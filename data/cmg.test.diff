init_failed : <nl> static gboolean <nl> plugin_init ( GstPlugin * plugin ) <nl> { <nl> - if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_MARGINAL , <nl> + if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_PRIMARY , <nl> GST_TYPE_MPEG2DEC )) <nl> return FALSE ; <nl> 
gst_rdt_depay_push ( GstRDTDepay * rdtdepay , GstBuffer * buffer ) <nl> rdtdepay -> need_newsegment = FALSE ; <nl> } <nl>  <nl> + buffer = gst_buffer_make_metadata_writable ( buffer ); <nl> gst_buffer_set_caps ( buffer , GST_PAD_CAPS ( rdtdepay -> srcpad )); <nl>  <nl> if ( rdtdepay -> discont ) {
main ( int argc , char * argv []) <nl> name = strrchr ( drvs -> a . filename , '/'); <nl> if ( name == NULL ) <nl> name = drvs -> a . filename ; <nl> + else <nl> + name ++; /* skip '/' character */ <nl> break ; <nl> case drv_d : <nl> name = drvs -> d . guest ;
krb5_dbe_def_cpw ( krb5_context context , <nl> krb5_boolean keepold , <nl> krb5_db_entry * db_entry ); <nl>  <nl> + krb5_error_code <nl> + krb5_def_promote_db ( krb5_context , char *, char **); <nl> + <nl> krb5_error_code <nl> krb5_db_create_policy ( krb5_context kcontext , <nl> osa_policy_ent_t policy );
void ECDH :: SetPrivateKey ( const FunctionCallbackInfo < Value >& args ) { <nl> if ( priv == nullptr ) <nl> return env -> ThrowError (" Failed to convert Buffer to BN "); <nl>  <nl> - if (! EC_KEY_set_private_key ( ecdh -> key_ , priv )) <nl> + int result = EC_KEY_set_private_key ( ecdh -> key_ , priv ); <nl> + BN_free ( priv ); <nl> + <nl> + if (! result ) { <nl> return env -> ThrowError (" Failed to convert BN to a private key "); <nl> + } <nl> } <nl>  <nl> 
static SRes SzFolder_Decode2 ( const CSzFolder * folder , const UInt64 * packSizes , <nl> else <nl> return SZ_ERROR_UNSUPPORTED ; <nl> } <nl> + if (! packSizes ) <nl> + return SZ_ERROR_FAIL ; <nl> offset = GetSum ( packSizes , si ); <nl> inSize = packSizes [ si ]; <nl> RINOK ( LookInStream_SeekTo ( inStream , startPos + offset ));
static int url_hash_match ( const struct regex_matcher * rlist , const char * inurl , <nl> size_t j , k , ji , ki ; <nl> int rc ; <nl>  <nl> - if (! rlist -> md5_hashes . bm_patterns ) { <nl> + if (! rlist || ! rlist -> md5_hashes . bm_patterns ) { <nl> return CL_SUCCESS ; <nl> } <nl> if (! inurl )
static int handle_stream ( client_conn_t * conn , struct fd_buf * buf , const struct o <nl> pthread_mutex_unlock (& exit_mutex ); <nl> } <nl> * error = 1 ; <nl> + return - 1 ; <nl> } else { <nl> pos = 4 ; <nl> memmove ( buf -> buffer , & buf -> buffer [ pos ], buf -> off - pos );
do_election_count_vote ( long long action , <nl> crm_devel (" Election fail : born_on "); <nl> we_loose = TRUE ; <nl>  <nl> - } else if ( your_node -> node_born_on < our_node -> node_born_on ) { <nl> + } else if ( your_node -> node_born_on > our_node -> node_born_on ) { <nl> crm_devel (" Election pass : born_on "); <nl>  <nl> } else if ( strcmp ( fsa_our_uname , vote_from ) > 0 ) {
prepare_cmd_parameters ( const char * rsc_type , const char * op_type , <nl> * Add the teminating NULL pointer . <nl> */ <nl> params_argv [ 2 ] = NULL ; <nl> - if ( ht_size != 0 ) { <nl> + if ( ( ht_size != 0 ) && ( 0 != STRNCMP_CONST ( op_type , " status ")) ) { <nl> cl_log ( LOG_WARNING , " For LSB init script , no additional " <nl> " parameters are needed ."); <nl> }
update_attr ( cib_t * the_cib , int call_options , <nl> CRM_CHECK ( set_name != NULL , return cib_missing ); <nl>  <nl> if ( attr_value == NULL ) { <nl> + free_xml ( xml_obj ); <nl> return cib_missing_data ; <nl> } <nl>  <nl> update_attr ( cib_t * the_cib , int call_options , <nl> xml_obj = create_xml_node ( xml_obj , XML_TAG_ATTRS ); <nl> crm_free ( local_set_name ); <nl> } else { <nl> + free_xml ( xml_obj ); <nl> xml_obj = NULL ; <nl> } <nl> 
static void impl__free ( git_odb_backend * _backend ) <nl> { <nl> struct memory_packer_db * db = ( struct memory_packer_db *) _backend ; <nl>  <nl> + git_mempack_reset ( _backend ); <nl> git_oidmap_free ( db -> objects ); <nl> git__free ( db ); <nl> }
int git_futils_mmap_ro_file ( git_map * out , const char * path ) <nl> if ( fd < 0 ) <nl> return fd ; <nl>  <nl> - len = git_futils_filesize ( fd ); <nl> + if (( len = git_futils_filesize ( fd )) < 0 ) <nl> + return - 1 ; <nl> + <nl> if (! git__is_sizet ( len )) { <nl> giterr_set ( GITERR_OS , " file `% s ` too large to mmap ", path ); <nl> return - 1 ;
int git_reference_rename ( git_reference * ref , const char * new_name , int force ) <nl> head_target = git_reference_target ( head ); <nl>  <nl> if ( head_target && ! strcmp ( head_target , ref -> name )) { <nl> + git_reference_free ( head ); <nl> + head = NULL ; <nl> + <nl> if ( git_reference_create_symbolic (& head , ref -> owner , " HEAD ", new_name , 1 ) < 0 ) { <nl> giterr_set ( GITERR_REFERENCE , <nl> " Failed to update HEAD after renaming reference ");
static struct child_process * git_connect_git ( int fd [ 2 ], char * hostandport , <nl> target_host = xstrdup ( hostandport ); <nl>  <nl> transport_check_allowed (" git "); <nl> + if ( strchr ( target_host , '\ n ') || strchr ( path , '\ n ')) <nl> + die ( _ (" newline is forbidden in git :// hosts and repo paths ")); <nl>  <nl> /* <nl> * These underlying connection commands die () if they
static int read_patches ( const char * range , struct string_list * list ) <nl> strbuf_addbuf (& buf , & line ); <nl> strbuf_addstr (& buf , "\ n \ n "); <nl> } else if ( starts_with ( line . buf , " ")) { <nl> + strbuf_rtrim (& line ); <nl> strbuf_addbuf (& buf , & line ); <nl> strbuf_addch (& buf , '\ n '); <nl> }
static int get_sha1_oneline ( const char * prefix , unsigned char * sha1 ) <nl> unsigned long size ; <nl>  <nl> commit = pop_most_recent_commit (& list , ONELINE_SEEN ); <nl> - parse_object ( commit -> object . sha1 ); <nl> + if (! parse_object ( commit -> object . sha1 )) <nl> + continue ; <nl> if ( temp_commit_buffer ) <nl> free ( temp_commit_buffer ); <nl> if ( commit -> buffer )
int fts_expunge_log_uid_count ( struct fts_expunge_log * log , <nl> { <nl> int ret ; <nl>  <nl> - if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) <nl> + if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) { <nl> + * expunges_r = 0 ; <nl> return ret ; <nl> + } <nl>  <nl> return fts_expunge_log_read_expunge_count ( log , expunges_r ); <nl> }
static void imapc_connection_set_state ( struct imapc_connection * conn , <nl>  <nl> conn -> selecting_box = NULL ; <nl> conn -> selected_box = NULL ; <nl> + <nl> + i_free ( conn -> ips ); <nl> + conn -> ips_count = 0 ; <nl> break ; <nl> default : <nl> break ;
void smtp_server_connection_data_chunk_init ( struct smtp_server_cmd_ctx * cmd ) <nl> command -> hook_replied = cmd_data_chunk_replied ; <nl> command -> hook_destroy = cmd_data_destroy ; <nl>  <nl> - if ( conn -> state . data_chain == NULL ) { <nl> + if (! conn -> state . data_failed && conn -> state . data_chain == NULL ) { <nl> i_assert ( data_cmd -> chunk_first ); <nl> i_assert ( conn -> state . data_chain_input == NULL ); <nl> conn -> state . data_chain_input =
static bool pop3_uidl_assign_by_size ( struct mailbox * box ) <nl> struct imap_msg_map * imap_map ; <nl> unsigned int i , pop3_count , imap_count , count ; <nl>  <nl> + if ( mstorage -> skip_size_check ) <nl> + return FALSE ; <nl> + <nl> pop3_map = array_get_modifiable (& mstorage -> pop3_uidl_map , & pop3_count ); <nl> imap_map = array_get_modifiable (& mbox -> imap_msg_map , & imap_count ); <nl> count = I_MIN ( pop3_count , imap_count );
dsync_mailbox_find_common_uid ( struct dsync_mailbox_importer * importer , <nl> } <nl> return ; <nl> } <nl> - if ( importer -> revert_local_changes ) { <nl> + if ( importer -> revert_local_changes && <nl> + change -> type != DSYNC_MAIL_CHANGE_TYPE_EXPUNGE ) { <nl> dsync_mailbox_revert_missing ( importer , change ); <nl> * result_r = " Reverting local change by deleting mailbox "; <nl> } else if ( dsync_mailbox_find_common_expunged_uid ( importer , change )) {
int index_storage_search_deinit ( struct mail_search_context * _ctx ) <nl> array_free (& ctx -> mail_ctx . results ); <nl> array_free (& ctx -> mail_ctx . module_contexts ); <nl>  <nl> - array_foreach_modifiable (& ctx -> mails , mailp ) <nl> + array_foreach_modifiable (& ctx -> mails , mailp ) { <nl> + struct index_mail * imail = ( struct index_mail *)* mailp ; <nl> + <nl> + imail -> search_mail = FALSE ; <nl> mail_free ( mailp ); <nl> + } <nl> array_free (& ctx -> mails ); <nl> i_free ( ctx ); <nl> return ret ;
int i_getpwnam ( const char * name , struct passwd * pwd_r ) <nl> errno = getpwnam_r ( name , pwd_r , pwbuf , pwbuf_size , & result ); <nl> if ( result != NULL ) <nl> return 1 ; <nl> + if ( errno == EINVAL ) { <nl> + /* FreeBSD fails here when name =" user @ domain " */ <nl> + return 0 ; <nl> + } <nl> return errno == 0 ? 0 : - 1 ; <nl> } <nl> 
imap_msgpart_crlf_seek ( struct mail * mail , struct istream * input , <nl> if ( message_skip_virtual ( input , virtual_skip , & cr_skipped ) < 0 ) { <nl> errinput = i_stream_create_error ( errno ); <nl> i_stream_set_name ( errinput , i_stream_get_name ( input )); <nl> + i_stream_unref (& input ); <nl> return errinput ; <nl> } <nl> 
void io_loop_set_current ( struct ioloop * ioloop ) <nl> io_switch_callback_t * const * callbackp ; <nl> struct ioloop * prev_ioloop = current_ioloop ; <nl>  <nl> + if ( ioloop == current_ioloop ) <nl> + return ; <nl> + <nl> current_ioloop = ioloop ; <nl> if ( array_is_created (& io_switch_callbacks )) { <nl> array_foreach (& io_switch_callbacks , callbackp )
void ssl_iostream_destroy ( struct ssl_iostream ** _ssl_io ) <nl> { <nl> struct ssl_iostream * ssl_io = * _ssl_io ; <nl>  <nl> + if ( _ssl_io == NULL || * _ssl_io == NULL ) <nl> + return ; <nl> + <nl> + ssl_io = * _ssl_io ; <nl> * _ssl_io = NULL ; <nl> ssl_vfuncs -> destroy ( ssl_io ); <nl> }
mail_storage_service_init_post ( struct mail_storage_service_ctx * ctx , <nl> if ( errno == EACCES ) { <nl> i_error ("% s ", eacces_error_get (" chdir ", <nl> t_strconcat ( home , "/", NULL ))); <nl> - } if ( errno != ENOENT ) <nl> + } else if ( errno != ENOENT ) <nl> i_error (" chdir (% s ) failed : % m ", home ); <nl> else if ( mail_set -> mail_debug ) <nl> i_debug (" Home dir not found : % s ", home );
ConnectionExists ( struct Curl_easy * data , <nl> if ( chosen ) { <nl> /* mark it as used before releasing the lock */ <nl> chosen -> inuse = TRUE ; <nl> + chosen -> data = data ; /* own it ! */ <nl> Curl_conncache_unlock ( needle ); <nl> * usethis = chosen ; <nl> return TRUE ; /* yes , we found one to use ! */
void curl_mime_free ( curl_mime * mime ) <nl> curl_mimepart * part ; <nl>  <nl> if ( mime ) { <nl> + mime_subparts_unbind ( mime ); /* Be sure it ' s not referenced anymore . */ <nl> while ( mime -> firstpart ) { <nl> part = mime -> firstpart ; <nl> mime -> firstpart = part -> nextpart ;
int main ( int argc , char ** argv ) <nl> /* HTTP PUT please */ <nl> curl_easy_setopt ( curl , CURLOPT_PUT , TRUE ); <nl>  <nl> - /* specify target */ <nl> + /* specify target URL , and note that this URL should include a file <nl> + name , not only a directory */ <nl> curl_easy_setopt ( curl , CURLOPT_URL , url ); <nl>  <nl> /* now specify which file to upload */
START_TEST ( test_should_resend_obit ) <nl> pjob . ji_obit_sent = time_now ; <nl>  <nl> // Running jobs shouldn ' t re - send their obits <nl> - pjob . ji_qs . ji_substate == JOB_SUBSTATE_RUNNING ; <nl> + pjob . ji_qs . ji_substate = JOB_SUBSTATE_RUNNING ; <nl> fail_unless ( should_resend_obit (& pjob , diff ) == false ); <nl> pjob . ji_obit_busy_time = time_now - ( 2 * diff ); <nl> fail_unless ( should_resend_obit (& pjob , diff ) == false );
char * pbse_to_txt ( int err ) <nl> exit ( 1 ); <nl> } <nl>  <nl> + int trq_cg_remove_process_from_accts ( job * pjob ) <nl> + { <nl> + return ( PBSE_NONE ); <nl> + }
void * queue_route ( void * vp ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - void acct_close ( void ) <nl> + void acct_close ( bool ) <nl> { <nl> fprintf ( stderr , " The call to acct_close needs to be mocked !!\ n "); <nl> exit ( 1 );
job * job_clone ( <nl> pnewjob -> ji_qs . ji_jobid [ PBS_MAXSVRJOBID ] = '\ 0 '; <nl> snprintf ( pnewjob -> ji_qs . ji_jobid , PBS_MAXSVRJOBID ,"% s -% d .% s ", <nl> oldid , taskid , hostname ); <nl> - <nl> + free ( oldid ); <nl> /* update the job filename <nl> * We could optimize the sub - jobs to all use the same file . We would need a <nl> * way to track the number of tasks still using the job file so we know when
int get_active_pbs_server ( <nl> } <nl> else if (( rc = socket_read_str ( local_socket , & read_buf , & read_buf_len )) != PBSE_NONE ) <nl> { <nl> + if ( read_buf != NULL ) <nl> + free ( read_buf ); <nl> + <nl> close ( local_socket ); <nl> return ( rc ); <nl> }
int th_msg_received ( void * data ) <nl> { <nl> sip_msg_t msg ; <nl> str * obuf ; <nl> - char * nbuf ; <nl> + char * nbuf = NULL ; <nl> int direction ; <nl> int dialog ; <nl>  <nl> int th_msg_received ( void * data ) <nl> obuf -> s [ obuf -> len ] = '\ 0 '; <nl>  <nl> done : <nl> + if ( nbuf != NULL ) <nl> + pkg_free ( nbuf ); <nl> free_sip_msg (& msg ); <nl> return 0 ; <nl> }
int db_postgres_store_result ( const db1_con_t * _con , db1_res_t ** _r ) <nl> } <nl>  <nl> done : <nl> - db_postgres_free_query ( _con ); <nl> return ( rc ); <nl> } <nl> 
static int child_init ( int _rank ) <nl> dlist_t * ptr ; <nl> int i ; <nl>  <nl> + if ( sruid_init (& _ul_sruid , '-', " ulcx ", SRUID_INC )< 0 ) <nl> + return - 1 ; <nl> + <nl> if ( _rank == PROC_MAIN && ul_timer_procs > 0 ) <nl> { <nl> for ( i = 0 ; i < ul_timer_procs ; i ++)
else <nl> { <nl> LM_ERR ("% sCall (% s ) REFER error (% d )", pfncname , <nl> pcall -> call_from , nreply ); <nl> - pcall -> call_state = CLSTA_INQUEUE ; <nl> - update_call_rec ( pcall ); <nl> + if ( nreply == 481 ) <nl> + { delete_call ( pcall ); } <nl> + else <nl> + { <nl> + pcall -> call_state = CLSTA_INQUEUE ; <nl> + update_call_rec ( pcall ); <nl> + } <nl> } <nl> return ; <nl> }
struct module_exports exports = { <nl> static int mod_init ( void ) { <nl> struct stat fs ; <nl>  <nl> + if ( register_mi_mod ( exports . name , mi_cmds )!= 0 ) <nl> + { <nl> + LM_ERR (" failed to register MI commands \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> subscriber_table . len = strlen ( subscriber_table . s ); <nl> subscriber_username_col . len = strlen ( subscriber_username_col . s ); <nl> subscriber_domain_col . len = strlen ( subscriber_domain_col . s );
static int control ( struct af_instance_s * af , int cmd , void * arg ) <nl> *( float *) arg = s -> scale ; <nl> return AF_OK ; <nl> case AF_CONTROL_COMMAND_LINE :{ <nl> - strarg_t speed ; <nl> + strarg_t speed = {}; <nl> opt_t subopts [] = { <nl> {" scale ", OPT_ARG_FLOAT , & s -> scale_nominal , NULL }, <nl> {" stride ", OPT_ARG_FLOAT , & s -> ms_stride , NULL },
static void pass_add_hook ( struct gl_video * p , struct tex_hook hook ) <nl> p -> tex_hooks [ p -> tex_hook_num ++] = hook ; <nl> } else { <nl> MP_ERR ( p , " Too many hooks ! Limit is % d .\ n ", MAX_TEXTURE_HOOKS ); <nl> + <nl> + if ( hook . free ) <nl> + hook . free (& hook ); <nl> } <nl> } <nl> 
const struct ao_driver audio_out_null = { <nl> . priv_size = sizeof ( struct priv ), <nl> . priv_defaults = &( const struct priv ) { <nl> . bufferlen = 0 . 2 , <nl> - . latency_sec = 0 . 5 , <nl> . outburst = 256 , <nl> . speed = 1 , <nl> },
int libztex_prepare_device ( struct libusb_device * dev , struct libztex_device ** z <nl> void libztex_destroy_device ( struct libztex_device * ztex ) { <nl> if ( ztex -> hndl != NULL ) { <nl> libusb_close ( ztex -> hndl ); <nl> + ztex -> hndl = NULL ; <nl> } <nl> if ( ztex -> bitFileName != NULL ) { <nl> free ( ztex -> bitFileName );
void knc_poll ( struct thr_info * const thr ) <nl> if ( KNC_REPLY_NONCE_FOUND == rtype ) <nl> { <nl> nonce = get_u32be (& rxbuf [ 4 ]); <nl> + nonce = le32toh ( nonce ); <nl> inc_hw_errors2 ( mythr , NULL , & nonce ); <nl> } <nl> else
AtHardwTest0010 ( void ) <nl> return ( Status ); <nl> } <nl>  <nl> - Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress ); <nl> + Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress , 0 ); <nl> if ( ACPI_FAILURE ( Status )) <nl> { <nl> AapiErrors ++; <nl> AtHardwTest0011 ( void ) <nl> return ( Status ); <nl> } <nl>  <nl> - Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress ); <nl> + Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress , 0 ); <nl> if ( Status != AE_NO_ACPI_TABLES ) <nl> { <nl> AapiErrors ++;
static void qemu_tcg_init_cpu_signals ( void ) <nl> } <nl> # endif /* _WIN32 */ <nl>  <nl> - QemuMutex qemu_global_mutex ; <nl> + static QemuMutex qemu_global_mutex ; <nl> static QemuCond qemu_io_proceeded_cond ; <nl> static bool iothread_requesting_mutex ; <nl> 
static int qemu_event_init ( void ) <nl>  <nl> static void qemu_event_increment ( void ) <nl> { <nl> - SetEvent ( qemu_event_handle ); <nl> + if (! SetEvent ( qemu_event_handle )) { <nl> + fprintf ( stderr , " qemu_event_increment : SetEvent failed : % d \ n ", <nl> + GetLastError ()); <nl> + exit ( 1 ); <nl> + } <nl> } <nl> # endif <nl> 
const VMStateDescription vmstate_ahci = { <nl> VMSTATE_UINT32 ( control_regs . impl , AHCIState ), <nl> VMSTATE_UINT32 ( control_regs . version , AHCIState ), <nl> VMSTATE_UINT32 ( idp_index , AHCIState ), <nl> - VMSTATE_INT32 ( ports , AHCIState ), <nl> + VMSTATE_INT32_EQUAL ( ports , AHCIState ), <nl> VMSTATE_END_OF_LIST () <nl> }, <nl> };
static int sd_snapshot_goto ( BlockDriverState * bs , const char * snapshot_id ) <nl> if ( snapid ) { <nl> tag [ 0 ] = 0 ; <nl> } else { <nl> - pstrcpy ( tag , sizeof ( tag ), s -> name ); <nl> + pstrcpy ( tag , sizeof ( tag ), snapshot_id ); <nl> } <nl>  <nl> ret = reload_inode ( s , snapid , tag );
static int kvm_sclp_service_call ( S390CPU * cpu , struct kvm_run * run , <nl> int r = 0 ; <nl>  <nl> cpu_synchronize_state ( CPU ( cpu )); <nl> + if ( env -> psw . mask & PSW_MASK_PSTATE ) { <nl> + enter_pgmcheck ( cpu , PGM_PRIVILEGED ); <nl> + return 0 ; <nl> + } <nl> sccb = env -> regs [ ipbh0 & 0xf ]; <nl> code = env -> regs [( ipbh0 & 0xf0 ) >> 4 ]; <nl> 
static CharDriverState * qemu_chr_open_udp ( QemuOpts * opts ) <nl>  <nl> fd = inet_dgram_opts ( opts , & local_err ); <nl> if ( fd < 0 ) { <nl> + qerror_report_err ( local_err ); <nl> + error_free ( local_err ); <nl> return NULL ; <nl> } <nl> return qemu_chr_open_udp_fd ( fd );
static void leon3_generic_hw_init ( MachineState * machine ) <nl> fprintf ( stderr , " Can ' t read bios image % s \ n ", filename ); <nl> exit ( 1 ); <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Can directly load an application . */ <nl> if ( kernel_filename != NULL ) {
static int qcow2_read_extensions ( BlockDriverState * bs , uint64_t start_offset , <nl> # ifdef DEBUG_EXT <nl> printf (" ext . magic = 0x % x \ n ", ext . magic ); <nl> # endif <nl> + if ( ext . len > end_offset - offset ) { <nl> + error_report (" Header extension too large "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> switch ( ext . magic ) { <nl> case QCOW2_EXT_MAGIC_END : <nl> return 0 ;
void qemu_opts_del ( QemuOpts * opts ) <nl> { <nl> QemuOpt * opt ; <nl>  <nl> + if ( opts == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> for (;;) { <nl> opt = QTAILQ_FIRST (& opts -> head ); <nl> if ( opt == NULL )
static int aio_flush_request ( void * opaque ) <nl> return ! QLIST_EMPTY (& s -> outstanding_aio_head ); <nl> } <nl>  <nl> -# ifdef _WIN32 <nl> +# if ! defined ( SOL_TCP ) || ! defined ( TCP_CORK ) <nl>  <nl> static int set_cork ( int fd , int v ) <nl> {
void * cli_jsonarray_nojson ( const char * key ) <nl> int cli_jsonint_array_nojson ( int32_t val ) <nl> { <nl> nojson_func (" nojson : % d \ n ", val ); <nl> + return CL_SUCCESS ; <nl> } <nl>  <nl> # endif
static int count_column_width ( struct libscols_table * tb , <nl>  <nl> cl -> width = 0 ; <nl>  <nl> - <nl> - if ( cl -> width_min ) { <nl> + if (! cl -> width_min ) { <nl> if ( cl -> width_hint < 1 && scols_table_is_maxout ( tb )) <nl> cl -> width_min = ( size_t ) ( cl -> width_hint * tb -> termwidth ) - ( is_last_column ( cl ) ? 0 : 1 ); <nl> if ( scols_cell_get_data (& cl -> header )) {
int read_hypervisor_dmi ( void ) <nl> if ( rc ) <nl> goto done ; <nl> free ( buf ); <nl> - <nl> + buf = NULL ; <nl> memory_scan : <nl> # if defined ( __x86_64__ ) || defined ( __i386__ ) <nl> /* Fallback to memory scan ( x86 , x86_64 ) */
details_only : <nl> /* <nl> * Gather PART_ENTRY_ * values if the current device is a partition . <nl> */ <nl> - if (( blkid_partitions_get_flags ( pr ) & BLKID_PARTS_ENTRY_DETAILS )) { <nl> + if (! chn -> binary && <nl> + ( blkid_partitions_get_flags ( pr ) & BLKID_PARTS_ENTRY_DETAILS )) { <nl> + <nl> if (! blkid_partitions_probe_partition ( pr )) <nl> rc = 0 ; <nl> }
display_summary ( void ) <nl> continue ; <nl> cn = canonicalize_path ( dev ); <nl> if ( cn ) <nl> - printf ("%- 40s % s ", cn , p ); <nl> + printf ("%- 39s % s ", cn , p ); <nl> free ( dev ); <nl> free ( cn ); <nl> }
void GeoIPManager :: configure () <nl> const bool enabled = Preferences :: instance ()-> resolvePeerCountries (); <nl> if ( m_enabled != enabled ) { <nl> m_enabled = enabled ; <nl> - if ( m_enabled && ! m_geoIPDatabase ) <nl> + if ( m_enabled && ! m_geoIPDatabase ) { <nl> loadDatabase (); <nl> + } <nl> + else if (! m_enabled && m_geoIPDatabase ) { <nl> + delete m_geoIPDatabase ; <nl> + m_geoIPDatabase = 0 ; <nl> + } <nl> } <nl> } <nl> 
void rpmProblemSetFree ( rpmProblemSet probs ); <nl> void rpmProblemSetFilter ( rpmProblemSet ps , int flags ); <nl> int rpmRunTransactions ( rpmTransactionSet ts , rpmCallbackFunction notify , <nl> void * notifyData , rpmProblemSet okProbs , <nl> - rpmProblemSet * newProbs , int flags ); <nl> + rpmProblemSet * newProbs , int flags , int ignoreSet ); <nl>  <nl> # define RPMPROB_FILTER_IGNOREOS ( 1 << 0 ) <nl> # define RPMPROB_FILTER_IGNOREARCH ( 1 << 1 )
int rpmRunTransactions ( rpmTransactionSet ts , rpmCallbackFunction notify , <nl> ( void *) NULL , & fi -> fc )) { <nl> fi -> fc = 0 ; <nl> fi -> h = alp -> h ; <nl> + hdrs [ alp - al -> list ] = headerLink ( fi -> h ); <nl> continue ; <nl> } <nl> 
# include " parseaddr . h " <nl> # include " xmalloc . h " <nl>  <nl> + static char parseaddr_unspecified_domain [] = " unspecified - domain "; <nl> + <nl> static void parseaddr_append (); <nl> static int parseaddr_phrase (); <nl> static int parseaddr_domain (); <nl> char ** freemep ; <nl> newaddr -> mailbox = mailbox ; <nl>  <nl> if ( domain && !* domain ) { <nl> - domain = " unspecified - domain "; <nl> + domain = parseaddr_unspecified_domain ; <nl> } <nl> newaddr -> domain = domain ; <nl> 
EXPORTED int carddav_writecard ( struct carddav_db * carddavdb , struct carddav_data <nl> else if (! strcmp ( name , " x - fm - otheraccount - member ")) { <nl> if ( strncmp ( propval , " urn : uuid :", 9 )) continue ; <nl> struct vparse_param * param = vparse_get_param ( ventry , " userid "); <nl> + if (! param ) continue ; <nl> strarray_append (& member_uids , propval + 9 ); <nl> strarray_append (& member_uids , param -> value ); <nl> }
int report_sync_col ( struct transaction_t * txn , <nl>  <nl> /* XXX Handle Depth ( cal - home - set at toplevel ) */ <nl>  <nl> + memset (& istate , 0 , sizeof ( struct index_state )); <nl> istate . map = NULL ; <nl>  <nl> /* Open mailbox for reading */
void latencyAddSample ( char * event , mstime_t latency ) { <nl> if ( ts -> samples [ prev ]. time == now ) { <nl> if ( latency > ts -> samples [ prev ]. latency ) <nl> ts -> samples [ prev ]. latency = latency ; <nl> + if ( latency > ts -> max ) ts -> max = latency ; <nl> return ; <nl> } <nl> 
static int fic_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " Could not allocate slice data .\ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( ctx -> slice_data , 0 , nslices * sizeof ( ctx -> slice_data [ 0 ])); <nl>  <nl> for ( slice = 0 ; slice < nslices ; slice ++) { <nl> unsigned slice_off = AV_RB32 ( src + tsize + FIC_HEADER_SIZE + slice * 4 );
static int decode_frame_adu ( AVCodecContext * avctx , void * data , <nl> /* update codec info */ <nl> avctx -> sample_rate = s -> sample_rate ; <nl> avctx -> channels = s -> nb_channels ; <nl> + avctx -> channel_layout = s -> nb_channels == 1 ? AV_CH_LAYOUT_MONO : AV_CH_LAYOUT_STEREO ; <nl> if (! avctx -> bit_rate ) <nl> avctx -> bit_rate = s -> bit_rate ; <nl> 
static int mov_write_udta_tag ( ByteIOContext * pb , MOVMuxContext * mov , <nl> put_be32 ( pb , size + 8 ); <nl> put_tag ( pb , " udta "); <nl> put_buffer ( pb , buf , size ); <nl> - av_free ( buf ); <nl> } <nl> + av_free ( buf ); <nl>  <nl> return 0 ; <nl> }
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> pics = 0 ; <nl> while ( h -> delayed_pic [ pics ]) pics ++; <nl> + <nl> + assert ( pics + 1 < sizeof ( h -> delayed_pic ) / sizeof ( h -> delayed_pic [ 0 ])); <nl> + <nl> h -> delayed_pic [ pics ++] = cur ; <nl> if ( cur -> reference == 0 ) <nl> cur -> reference = 1 ;
static av_cold int rl2_read_header ( AVFormatContext * s ) <nl> rate = avio_rl16 ( pb ); <nl> channels = avio_rl16 ( pb ); <nl> def_sound_size = avio_rl16 ( pb ); <nl> + if (! channels || channels > 42 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels : % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> /** setup video stream */ <nl> st = avformat_new_stream ( s , NULL );
static av_cold int adpcm_encode_init ( AVCodecContext * avctx ) <nl> } <nl>  <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> - avctx -> coded_frame -> key_frame = 1 ; <nl>  <nl> return 0 ; <nl> error :
static int mov_read_stsd ( MOVContext * c , ByteIOContext * pb , MOVAtom atom ) <nl> # endif <nl> /* no ifdef since parameters are always those */ <nl> case CODEC_ID_QCELP : <nl> + // force sample rate for qcelp when not stored in mov <nl> + if ( st -> codec -> codec_tag != MKTAG (' Q ',' c ',' l ',' p ')) <nl> + st -> codec -> sample_rate = 8000 ; <nl> st -> codec -> frame_size = 160 ; <nl> st -> codec -> channels = 1 ; /* really needed */ <nl> break ;
void mpeg4_encode_mb ( MpegEncContext * s , <nl> cbpy ^= 0xf ; <nl> put_bits ( pb2 , cbpy_tab [ cbpy ][ 1 ], cbpy_tab [ cbpy ][ 0 ]); <nl>  <nl> + if (! s -> progressive_sequence ){ <nl> + if ( cbp ) <nl> + put_bits ( pb2 , 1 , s -> interlaced_dct ); <nl> + } <nl> + <nl> if ( interleaved_stats ){ <nl> bits = get_bit_count (& s -> pb ); <nl> s -> misc_bits += bits - s -> last_bits ;
static int pva_read_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> flags = get_byte ( pb ); <nl> length = get_be16 ( pb ); <nl>  <nl> - pts_flag = ( flags & 0x10 ) >> 4 ; <nl> + pts_flag = flags & 0x10 ; <nl>  <nl> if ( syncword != PVA_MAGIC ) { <nl> av_log ( s , AV_LOG_ERROR , " invalid syncword \ n ");
static int vp9_handle_packet ( AVFormatContext * ctx , PayloadContext * rtp_vp9_ctx , <nl> return 0 ; <nl> } <nl>  <nl> + static void vp9_close_context ( PayloadContext * vp9 ) <nl> +{ <nl> + ffio_free_dyn_buf (& vp9 -> buf ); <nl> +} <nl> + <nl> RTPDynamicProtocolHandler ff_vp9_dynamic_handler = { <nl> . enc_name = " VP9 ", <nl> . codec_type = AVMEDIA_TYPE_VIDEO , <nl> . codec_id = AV_CODEC_ID_VP9 , <nl> . priv_data_size = sizeof ( PayloadContext ), <nl> . init = vp9_init , <nl> + . close = vp9_close_context , <nl> . parse_packet = vp9_handle_packet <nl> };
const CodecMime ff_id3v2_mime_tags [] = { <nl> {" image / jpg ", CODEC_ID_MJPEG }, <nl> {" image / png " , CODEC_ID_PNG }, <nl> {" image / tiff ", CODEC_ID_TIFF }, <nl> + {" image / bmp ", CODEC_ID_BMP }, <nl> {"", CODEC_ID_NONE }, <nl> }; <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Invalid combination - slices % d and - max_nal_size % d .\ n ", <nl> avctx -> slices , s -> max_nal_size ); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl>  <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } else { <nl> av_log ( avctx , AV_LOG_ERROR , " Invalid - max_nal_size , " <nl> " specify a valid max_nal_size to use - slice_mode dyn \ n "); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl> }
int ff_rtp_chain_mux_open ( AVFormatContext ** out , AVFormatContext * s , <nl> /* Get the payload type from the codec */ <nl> if ( st -> id < RTP_PT_PRIVATE ) <nl> rtpctx -> streams [ 0 ]-> id = <nl> - ff_rtp_get_payload_type ( rtpctx , st -> codec , idx ); <nl> + ff_rtp_get_payload_type ( s , st -> codec , idx ); <nl> else <nl> rtpctx -> streams [ 0 ]-> id = st -> id ; <nl> 
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + case NAL_IDR_SLICE : <nl> + case NAL_SLICE : <nl> nals_needed = nal_index ; <nl> } <nl> continue ;
static av_cold int twin_decode_init ( AVCodecContext * avctx ) <nl> default : avctx -> sample_rate = isampf * 1000 ; break ; <nl> } <nl>  <nl> - if ( avctx -> channels > CHANNELS_MAX ) { <nl> + if ( avctx -> channels <= 0 || avctx -> channels > CHANNELS_MAX ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unsupported number of channels : % i \ n ", <nl> avctx -> channels ); <nl> return - 1 ;
void estimate_motion ( MpegEncContext * s , <nl> if ( varc * 2 + 200 > vard ){ <nl> mb_type |= MB_TYPE_INTER ; <nl> halfpel_motion_search ( s , & mx , & my , dmin , xmin , ymin , xmax , ymax , pred_x , pred_y ); <nl> + } else { <nl> + mx = mx * 2 - mb_x * 32 ; <nl> + my = my * 2 - mb_y * 32 ; <nl> } <nl> } else { <nl> if ( vard <= 64 || vard < varc ) {
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> /* skip B - frames if we don ' t have reference frames */ <nl> if ( s -> last_picture_ptr == NULL && ( s -> pict_type == AV_PICTURE_TYPE_B || s -> droppable )) { <nl> - goto err ; <nl> + goto end ; <nl> } <nl> if (( avctx -> skip_frame >= AVDISCARD_NONREF && s -> pict_type == AV_PICTURE_TYPE_B ) || <nl> ( avctx -> skip_frame >= AVDISCARD_NONKEY && s -> pict_type != AV_PICTURE_TYPE_I ) ||
static int codebook_sanity_check_for_rate_quarter ( const uint8_t * cbgain ) <nl> * @ param gain array holding the 4 pitch subframe gain values <nl> * @ param cdn_vector array for the generated scaled codebook vector <nl> */ <nl> - static void compute_svector ( const QCELPContext * q , const float * gain , <nl> + static void compute_svector ( QCELPContext * q , const float * gain , <nl> float * cdn_vector ) <nl> { <nl> int i , j , k ;
static int64_t http_seek ( URLContext * h , int64_t off , int whence ) <nl>  <nl> if ( whence == AVSEEK_SIZE ) <nl> return s -> filesize ; <nl> + else if (( whence == SEEK_CUR && off == 0 ) || <nl> + ( whence == SEEK_SET && off == s -> off )) <nl> + return s -> off ; <nl> else if (( s -> filesize == - 1 && whence == SEEK_END ) || h -> is_streamed ) <nl> return AVERROR ( ENOSYS ); <nl> 
static int avi_read_header ( AVFormatContext * s ) <nl> ast = s -> streams [ 0 ]-> priv_data ; <nl> av_freep (& s -> streams [ 0 ]-> codec -> extradata ); <nl> av_freep (& s -> streams [ 0 ]-> codec ); <nl> + av_freep (& s -> streams [ 0 ]-> info ); <nl> av_freep (& s -> streams [ 0 ]); <nl> s -> nb_streams = 0 ; <nl> if ( CONFIG_DV_DEMUXER ) {
static int append_entry ( HLSContext * hls , uint64_t duration ) <nl> if (! en ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - av_get_frame_filename ( en -> name , sizeof ( en -> name ), hls -> basename , <nl> + av_get_frame_filename ( en -> name , sizeof ( en -> name ), <nl> + av_basename ( hls -> basename ), <nl> hls -> number - 1 ); <nl>  <nl> en -> duration = duration ;
static int handle_connection ( HTTPContext * c ) <nl> } <nl> if ( http_send_data ( c ) < 0 ) <nl> return - 1 ; <nl> + /* close connection if trailer sent */ <nl> + if ( c -> state == HTTPSTATE_SEND_DATA_TRAILER ) <nl> + return - 1 ; <nl> break ; <nl> case HTTPSTATE_RECEIVE_DATA : <nl> /* no need to read if no events */
static int sql_close ( SQLSOCK * sqlsocket , SQL_CONFIG * config ) { <nl>  <nl> rlm_sql_mysql_sock * mysql_sock = sqlsocket -> conn ; <nl>  <nl> - mysql_close ( mysql_sock -> sock ); <nl> + if ( mysql_sock -> sock ) <nl> + mysql_close ( mysql_sock -> sock ); <nl> mysql_sock -> sock = NULL ; <nl>  <nl> return 0 ;
struct groupchat * jabber_chat_join ( struct im_connection * ic , char * room , char * <nl> node = xt_new_node ( " x ", NULL , NULL ); <nl> xt_add_attr ( node , " xmlns ", XMLNS_MUC ); <nl> node = jabber_make_packet ( " presence ", NULL , roomjid , node ); <nl> + if ( password ) <nl> + xt_add_child ( node , xt_new_node ( " password ", password , NULL ) ); <nl> jabber_cache_add ( ic , node , jabber_chat_join_failed ); <nl>  <nl> if ( ! jabber_write_packet ( ic , node ) )
file_transfer_t * imcb_file_send_start ( struct im_connection * ic , char * handle , ch <nl> bee_t * bee = ic -> bee ; <nl> bee_user_t * bu = bee_user_by_handle ( bee , ic , handle ); <nl>  <nl> - if ( bee -> ui -> ft_in_start ) { <nl> + if ( bee -> ui -> ft_in_start && bu ) { <nl> return bee -> ui -> ft_in_start ( bee , bu , file_name , file_size ); <nl> } else { <nl> return NULL ;
WSLUA_METAMETHOD Dir__call ( lua_State * L ) { <nl> const gchar * filename ; <nl> const char * ext ; <nl>  <nl> - if (! dir ) <nl> + if (! dir ) { <nl> luaL_argerror ( L , 1 ," must be a Dir "); <nl> + return 0 ; <nl> + } <nl>  <nl> if (! dir -> dir ) { <nl> return 0 ;
PacketList :: PacketList ( QWidget * parent ) : <nl> decode_as_ ( NULL ), <nl> ctx_column_ (- 1 ), <nl> capture_in_progress_ ( false ), <nl> - tail_timer_id_ ( 0 ) <nl> + tail_timer_id_ ( 0 ), <nl> + rows_inserted_ ( false ) <nl> { <nl> QMenu * submenu , * subsubmenu ; <nl> QAction * action ;
add_content_type ( proto_tree * tree , tvbuff_t * tvb , guint32 val_start , <nl> So we have to disable that one and become " slow " by pretending that <nl> the tree is " visible ". <nl> */ <nl> - PTREE_DATA ( tree )-> visible = 1 ; <nl> + if ( tree ) <nl> + PTREE_DATA ( tree )-> visible = 1 ; <nl>  <nl> * textual_content = NULL ; <nl> * well_known_content = 0 ;
peektagged_read_packet ( wtap * wth , FILE_T fh , struct wtap_pkthdr * phdr , <nl>  <nl> case TAG_PEEKTAGGED_CENTER_FREQUENCY : <nl> /* XXX - also seen in an EtherPeek capture ; value unknown */ <nl> + ieee_802_11 . presence_flags |= PHDR_802_11_HAS_FREQUENCY ; <nl> + ieee_802_11 . frequency = pletoh32 (& tag_value [ 2 ]); <nl> break ; <nl>  <nl> case TAG_PEEKTAGGED_UNKNOWN_0x000E :
dissect_attribute_value_pairs ( proto_tree * tree , packet_info * pinfo , tvbuff_t * tv <nl>  <nl> avp_vsa_len -= avp_vsa_header_len ; <nl>  <nl> + memset (& vendor_type , 0 , sizeof ( vendor_type )); <nl> if ( avp_is_extended ) { <nl> vendor_type . u8_code [ 0 ] = avp_type . u8_code [ 0 ]; <nl> vendor_type . u8_code [ 1 ] = avp_vsa_type ;
bool InterfaceTreeWidgetItem :: operator < ( const QTreeWidgetItem & other ) const { <nl> # include < QComboBox > <nl>  <nl> InterfaceTreeDelegate :: InterfaceTreeDelegate ( QObject * parent ) <nl> - : QStyledItemDelegate ( parent ) <nl> + : QStyledItemDelegate ( parent ), tree_ ( NULL ) <nl> { <nl> } <nl> 
static char * substring_conf ( char * start , int len , char quote TSRMLS_DC ) <nl> # if HAVE_MBSTRING && ! defined ( COMPILE_DL_MBSTRING ) <nl> if ( php_mb_encoding_translation ( TSRMLS_C )) { <nl> size_t j = php_mb_mbchar_bytes ( start + i TSRMLS_CC ); <nl> - while ( j -- > 0 ) { <nl> + while ( j -- > 0 && i < len ) { <nl> * resp ++ = start [ i ++]; <nl> } <nl> -- i ;
static void _php_mb_regex_ereg_replace_exec ( INTERNAL_FUNCTION_PARAMETERS , OnigOp <nl> ! ZVAL_IS_UNDEF (& retval )) { <nl> convert_to_string_ex (& retval ); <nl> smart_str_appendl (& out_buf , Z_STRVAL ( retval ), Z_STRLEN ( retval )); <nl> - eval_buf . s -> len = 0 ; <nl> + if ( eval_buf . s ) { <nl> + eval_buf . s -> len = 0 ; <nl> + } <nl> zval_ptr_dtor (& retval ); <nl> } else { <nl> efree ( description );
PHP_FUNCTION ( imap_utf8 ) <nl> if ( dest . data ) { <nl> free ( dest . data ); <nl> } <nl> + if ( src . data ) { <nl> + free ( src . data ); <nl> + } <nl> } <nl> /* }}} */ <nl> 
SAPI_API size_t sapi_apply_default_charset ( char ** mimetype , size_t len TSRMLS_DC <nl> newtype = emalloc ( newlen + 1 ); <nl> PHP_STRLCPY ( newtype , * mimetype , newlen + 1 , len ); <nl> strlcat ( newtype , "; charset =", newlen + 1 ); <nl> + strlcat ( newtype , charset , newlen + 1 ); <nl> efree (* mimetype ); <nl> * mimetype = newtype ; <nl> return newlen ;
static int init_request_info ( TSRMLS_D ) <nl> SG ( request_info ). content_length = LSAPI_GetReqBodyLen (); <nl> SG ( request_info ). path_translated = estrdup ( LSAPI_GetScriptFileName ()); <nl>  <nl> - /* It is not reset by zend engine , set it to 0 . */ <nl> - SG ( sapi_headers ). http_response_code = 0 ; <nl> + /* It is not reset by zend engine , set it to 200 . */ <nl> + SG ( sapi_headers ). http_response_code = 200 ; <nl>  <nl> pAuth = LSAPI_GetHeader ( H_AUTHORIZATION ); <nl> php_handle_auth_data ( pAuth TSRMLS_CC );
php_sprintf_appenddouble ( char ** buffer , int * pos , <nl> if (( adjust & ADJ_PRECISION ) == 0 ) { <nl> precision = FLOAT_PRECISION ; <nl> } else if ( precision > MAX_FLOAT_PRECISION ) { <nl> + php_error_docref ( NULL TSRMLS_CC , E_NOTICE , " Requested precision of % d digits was truncated to PHP maximum of % d digits ", precision , MAX_FLOAT_PRECISION ); <nl> precision = MAX_FLOAT_PRECISION ; <nl> } <nl> 
main ( int argc , <nl> die_with_error (" Failed to make / slave "); <nl>  <nl> /* Create a tmpfs which we will use as / in the namespace */ <nl> - if ( mount ("", newroot , " tmpfs ", MS_NODEV | MS_NOEXEC | MS_NOSUID , NULL ) != 0 ) <nl> + if ( mount ("", newroot , " tmpfs ", MS_NODEV | MS_NOSUID , NULL ) != 0 ) <nl> die_with_error (" Failed to mount tmpfs "); <nl>  <nl> old_cwd = get_current_dir_name ();
end : <nl>  <nl> /* Removing empty dropin dirs */ <nl> if (! arg_full ) { <nl> - _cleanup_free_ char * dir = dirname_malloc (* original ); <nl> + _cleanup_free_ char * dir ; <nl> + <nl> + dir = dirname_malloc (* original ); <nl> + if (! dir ) <nl> + return log_oom (); <nl> + <nl> /* no need to check if the dir is empty , rmdir <nl> * does nothing if it is not the case . <nl> */
static int get_key ( struct udev * udev , char ** line , char ** key , enum operation_ty <nl> char * temp ; <nl>  <nl> linepos = * line ; <nl> - if ( linepos == NULL && linepos [ 0 ] == '\ 0 ') <nl> + if ( linepos == NULL || linepos [ 0 ] == '\ 0 ') <nl> return - 1 ; <nl>  <nl> /* skip whitespace */
int unit_file_get_list ( <nl> } <nl>  <nl> r = null_or_empty_path ( f -> path ); <nl> - if ( r < 0 ) { <nl> + if ( r < 0 && r != - ENOENT ) { <nl> free ( f -> path ); <nl> free ( f ); <nl> goto finish ;
static int unit_find_paths ( <nl> _cleanup_free_ char * template = NULL ; <nl>  <nl> r = unit_name_template ( unit_name , & template ); <nl> - if ( r != - EINVAL ) <nl> + if ( r < 0 && r != - EINVAL ) <nl> return log_error_errno ( r , " Failed to determine template name : % m "); <nl> if ( r >= 0 ) { <nl> r = unit_file_find_path ( lp , template , & path );
int dns_zone_put ( DnsZone * z , DnsScope * s , DnsResourceRecord * rr , bool probe ) { <nl> if ( established ) <nl> i -> state = DNS_ZONE_ITEM_ESTABLISHED ; <nl> else { <nl> + i -> state = DNS_ZONE_ITEM_PROBING ; <nl> + <nl> r = dns_zone_item_probe_start ( i ); <nl> if ( r < 0 ) { <nl> dns_zone_item_remove_and_free ( z , i ); <nl> i = NULL ; <nl> return r ; <nl> } <nl> - <nl> - i -> state = DNS_ZONE_ITEM_PROBING ; <nl> } <nl> } else <nl> i -> state = DNS_ZONE_ITEM_ESTABLISHED ;
static int unit_create_cgroups ( Unit * u , CGroupControllerMask mask ) { <nl> is_in_hash = true ; <nl>  <nl> if ( r < 0 ) { <nl> - free ( path ); <nl> log_error (" cgroup % s exists already : % s ", path , strerror (- r )); <nl> + free ( path ); <nl> return r ; <nl> } <nl> 
int bus_socket_read_message ( sd_bus * bus ) { <nl> return - EIO ; <nl> } <nl>  <nl> - f = realloc ( bus -> fds , sizeof ( int ) + ( bus -> n_fds + n )); <nl> + f = realloc ( bus -> fds , sizeof ( int ) * ( bus -> n_fds + n )); <nl> if (! f ) { <nl> close_many (( int *) CMSG_DATA ( cmsg ), n ); <nl> return - ENOMEM ;
static int add_string ( struct udev_rules * rules , const char * str ) <nl> unsigned short node_off ; <nl> unsigned char key ; <nl> size_t len ; <nl> - int depth ; <nl> + unsigned int depth ; <nl> unsigned int off ; <nl>  <nl> len = strlen ( str );
Client_Key_Exchange :: Client_Key_Exchange ( Handshake_IO & io , <nl>  <nl> DL_Group group ( p , g ); <nl>  <nl> - if (! group . verify_group ( rng , true )) <nl> - throw Internal_Error (" DH group failed validation , possible attack "); <nl> + if (! group . verify_group ( rng , false )) <nl> + throw TLS_Exception ( Alert :: INSUFFICIENT_SECURITY , <nl> + " DH group validation failed "); <nl>  <nl> DH_PublicKey counterparty_key ( group , Y ); <nl> 
SecureVector < byte > generate_dsa_primes ( BigInt & p , BigInt & q , u32bit pbits ) <nl> BigInt random_prime ( u32bit bits , const BigInt & coprime , <nl> u32bit equiv , u32bit modulo ) <nl> { <nl> - if ( bits <= 48 ) <nl> + if ( bits < 48 ) <nl> throw Invalid_Argument (" random_prime : Can ' t make a prime of " + <nl> to_string ( bits ) + " bits "); <nl> 
add_enc_ts_padata ( krb5_context context , <nl> if ( salt == NULL ) { <nl> /* default to standard salt */ <nl> ret = krb5_get_pw_salt ( context , client , & salt2 ); <nl> + if ( ret ) <nl> + return ret ; <nl> salt = & salt2 ; <nl> } <nl> if (! enctypes ) {
EVP_CipherInit_ex ( EVP_CIPHER_CTX * ctx , const EVP_CIPHER * c , ENGINE * engine , <nl> ctx -> cipher = c ; <nl> ctx -> key_len = c -> key_len ; <nl>  <nl> - ctx -> cipher_data = malloc ( c -> ctx_size ); <nl> + ctx -> cipher_data = calloc ( 1 , c -> ctx_size ); <nl> if ( ctx -> cipher_data == NULL && c -> ctx_size != 0 ) <nl> return 0 ; <nl> 
check ( void * opt , int argc , char ** argv ) <nl> p2 = strdup ( realm ); <nl> if ( p2 == NULL ) { <nl> krb5_warn ( context , errno , " malloc "); <nl> - free ( p ); <nl> goto fail ; <nl> } <nl> strlwr ( p2 );
del_enctype ( void * opt , int argc , char ** argv ) <nl> goto out2 ; <nl> } <nl>  <nl> + if ( kadm5_all_keys_are_bogus ( princ . n_key_data , princ . key_data )) { <nl> + krb5_warnx ( context , " user lacks get - keys privilege "); <nl> + goto out ; <nl> + } <nl> + <nl> new_key_data = malloc ( princ . n_key_data * sizeof (* new_key_data )); <nl> if ( new_key_data == NULL && princ . n_key_data != 0 ) { <nl> krb5_warnx ( context , " out of memory ");
void shadow_client_refresh_rect ( rdpShadowClient * client , BYTE count , RECTANGLE_1 <nl> wParam = ( SHADOW_MSG_IN_REFRESH_OUTPUT *) calloc ( 1 , sizeof ( SHADOW_MSG_IN_REFRESH_OUTPUT )); <nl>  <nl> if (! wParam || ! areas ) <nl> + { <nl> + if ( wParam ) <nl> + free ( wParam ); <nl> return ; <nl> + } <nl>  <nl> wParam -> numRects = ( UINT32 ) count ; <nl> 
BOOL xf_event_process ( freerdp * instance , XEvent * event ) <nl> if ( event -> xcookie . type == GenericEvent && <nl> event -> xcookie . extension == xfi -> XInputOpcode ) <nl> { <nl> - switch ( cookie . evtype ) <nl> + switch ( cookie -> evtype ) <nl> { <nl> case XI_ButtonPress : <nl> case XI_Motion :
void update_free ( rdpUpdate * update ) <nl>  <nl> xfree ( update -> bitmap_update . rectangles ); <nl> xfree ( update -> pointer ); <nl> + xfree ( update -> primary -> polyline . points ); <nl> + xfree ( update -> primary -> polygon_sc . points ); <nl> xfree ( update -> primary ); <nl> xfree ( update -> secondary ); <nl> xfree ( update -> altsec );
BOOL drive_file_set_information ( DRIVE_FILE * file , UINT32 FsInformationClass , UIN <nl> if (! fullpath ) <nl> { <nl> WLog_ERR ( TAG , " drive_file_combine_fullpath failed !"); <nl> + free ( s ); <nl> return FALSE ; <nl> } <nl> free ( s );
static BOOL autodetect_recv_bandwidth_measure_results ( rdpRdp * rdp , wStream * s , <nl> return FALSE ; <nl>  <nl> WLog_VRB ( AUTODETECT_TAG , " received Bandwidth Measure Results PDU "); <nl> + if ( Stream_GetRemainingLength ( s ) < 8 ) <nl> + return - 1 ; <nl> Stream_Read_UINT32 ( s , rdp -> autodetect -> bandwidthMeasureTimeDelta ); /* timeDelta ( 4 bytes ) */ <nl> Stream_Read_UINT32 ( s , rdp -> autodetect -> bandwidthMeasureByteCount ); /* byteCount ( 4 bytes ) */ <nl> 
boolean fastpath_send_update_pdu ( rdpFastPath * fastpath , uint8 updateCode , STREAM <nl> try_comp = rdp -> settings -> compression ; <nl> comp_update = stream_new ( 0 ); <nl>  <nl> - for ( fragment = 0 ; totalLength > 0 ; fragment ++) <nl> + for ( fragment = 0 ; totalLength > 0 || fragment == 0 ; fragment ++) <nl> { <nl> stream_get_mark ( s , holdp ); <nl> ls = s ;
INT32 progressive_decompress ( PROGRESSIVE_CONTEXT * progressive , <nl>  <nl> if ( progressive -> cTiles < surface -> gridSize ) <nl> { <nl> - progressive -> tiles = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> + BYTE * tmpBuf = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> surface -> gridSize * sizeof ( RFX_PROGRESSIVE_TILE *)); <nl> + if (! tmpBuf ) <nl> + return - 1025 ; <nl> + <nl> + progressive -> tiles = tmpBuf ; <nl> progressive -> cTiles = surface -> gridSize ; <nl> } <nl> 
again : <nl> return false ; <nl> } <nl> if ( x < 0 ) { <nl> - log () << " MessagingPort recv () error " << errno << ' ' << farEnd . toString ()<< endl ; <nl> + log () << " MessagingPort recv () error \"" << strerror ( errno ) << "\" (" << errno << ") " << farEnd . toString ()<< endl ; <nl> m . reset (); <nl> return false ; <nl> }
namespace mongo { <nl> class BalancingWindowUnitTest : public UnitTest { <nl> public : <nl> void run () { <nl> + <nl> + if ( ! cmdLine . isMongos () ) <nl> + return ; <nl> + <nl> // T0 < T1 < now < T2 < T3 and Error <nl> const string T0 = " 9 : 00 "; <nl> const string T1 = " 11 : 00 ";
public : <nl>  <nl> TTreeReader (): <nl> fDirectory ( 0 ), <nl> - fEntryStatus ( kEntryNoTree ) <nl> + fEntryStatus ( kEntryNoTree ), <nl> + fDirector ( 0 ) <nl> {} <nl>  <nl> TTreeReader ( TTree * tree );
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * insamplesref ) <nl> AVFilterBufferRef * outsamplesref = ff_get_audio_buffer ( outlink , AV_PERM_WRITE , n_out ); <nl> int ret ; <nl>  <nl> + if (! outsamplesref ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> avfilter_copy_buffer_ref_props ( outsamplesref , insamplesref ); <nl> outsamplesref -> format = outlink -> format ; <nl> outsamplesref -> audio -> channel_layout = outlink -> channel_layout ;
static void video_audio_display ( VideoState * s ) <nl> { <nl> int i , i_start , x , y1 , y , ys , delay , n , nb_display_channels ; <nl> int ch , channels , h , h2 , bgcolor , fgcolor ; <nl> - int16_t time_diff ; <nl> + int64_t time_diff ; <nl> int rdft_bits , nb_freq ; <nl>  <nl> for ( rdft_bits = 1 ; ( 1 << rdft_bits ) < 2 * s -> height ; rdft_bits ++)
static int yuv4_read_header ( AVFormatContext * s ) <nl> enum AVPixelFormat pix_fmt = AV_PIX_FMT_NONE , alt_pix_fmt = AV_PIX_FMT_NONE ; <nl> enum AVChromaLocation chroma_sample_location = AVCHROMA_LOC_UNSPECIFIED ; <nl> AVStream * st ; <nl> - enum AVFieldOrder field_order ; <nl> + enum AVFieldOrder field_order = AV_FIELD_UNKNOWN ; <nl>  <nl> for ( i = 0 ; i < MAX_YUV4_HEADER ; i ++) { <nl> header [ i ] = avio_r8 ( pb );
static void close_slaves ( AVFormatContext * avf ) <nl> } <nl> } <nl> av_freep (& tee -> slaves [ i ]. stream_map ); <nl> + av_freep (& tee -> slaves [ i ]. bsfs ); <nl>  <nl> avio_close ( avf2 -> pb ); <nl> avf2 -> pb = NULL ;
static int r3d_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> case MKTAG (' R ',' E ',' D ',' A '): <nl> if (! r3d -> audio_channels ) <nl> return - 1 ; <nl> - if ( s -> streams [ 1 ]-> discard == AVDISCARD_ALL ) <nl> + if ( s -> nb_streams >= 2 && s -> streams [ 1 ]-> discard == AVDISCARD_ALL ) <nl> goto skip ; <nl> if (!( err = r3d_read_reda ( s , pkt , & atom ))) <nl> return 0 ;
static void copy_bits ( PutBitContext * pb , <nl> rmn_bits = rmn_bytes = get_bits_left ( gb ); <nl> if ( rmn_bits < nbits ) <nl> return ; <nl> + if ( nbits > pb -> size_in_bits - put_bits_count ( pb )) <nl> + return ; <nl> rmn_bits &= 7 ; rmn_bytes >>= 3 ; <nl> if (( rmn_bits = FFMIN ( rmn_bits , nbits )) > 0 ) <nl> put_bits ( pb , rmn_bits , get_bits ( gb , rmn_bits ));
static int decode_iccp_chunk ( PNGDecContext * s , int length , AVFrame * f ) <nl> return ret ; <nl>  <nl> av_bprint_finalize (& bp , ( char **)& data ); <nl> + if (! data ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> sd = av_frame_new_side_data ( f , AV_FRAME_DATA_ICC_PROFILE , bp . len ); <nl> if (! sd ) {
static void mxf_free_metadataset ( MXFMetadataSet ** ctx , int freectx ) <nl> case MaterialPackage : <nl> av_freep (&(( MXFPackage *)* ctx )-> tracks_refs ); <nl> av_freep (&(( MXFPackage *)* ctx )-> name ); <nl> + av_freep (&(( MXFPackage *)* ctx )-> comment_refs ); <nl> break ; <nl> case TaggedValue : <nl> av_freep (&(( MXFTaggedValue *)* ctx )-> name );
static int read_interval_packets ( WriterContext * w , AVFormatContext * fmt_ctx , <nl> } <nl>  <nl> frame = av_frame_alloc (); <nl> + if (! frame ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> while (! av_read_frame ( fmt_ctx , & pkt )) { <nl> if ( selected_streams [ pkt . stream_index ]) { <nl> AVRational tb = fmt_ctx -> streams [ pkt . stream_index ]-> time_base ;
static void gif_copy_img_rect ( const uint32_t * src , uint32_t * dst , <nl> const uint32_t * src_px , * src_pr , <nl> * src_py = src + y_start , <nl> * dst_py = dst + y_start ; <nl> - const uint32_t * src_pb = src_py + t * linesize ; <nl> + const uint32_t * src_pb = src_py + h * linesize ; <nl> uint32_t * dst_px ; <nl>  <nl> for (; src_py < src_pb ; src_py += linesize , dst_py += linesize ) {
static int filter_packet ( void * log_ctx , AVPacket * pkt , <nl> AVFormatContext * fmt_ctx , AVBitStreamFilterContext * bsf_ctx ) <nl> { <nl> AVCodecContext * enc_ctx = fmt_ctx -> streams [ pkt -> stream_index ]-> codec ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> while ( bsf_ctx ) { <nl> AVPacket new_pkt = * pkt ;
void uninit_opts ( void ) <nl> av_freep (& avformat_opts -> key ); <nl> av_freep (& avformat_opts ); <nl> # if CONFIG_SWSCALE <nl> - av_freep (& sws_opts ); <nl> + sws_freeContext ( sws_opts ); <nl> + sws_opts = NULL ; <nl> # endif <nl> for ( i = 0 ; i < opt_name_count ; i ++) { <nl> // opt_values are only stored for codec - specific options in which case
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> start_subband += start_subband - 7 ; <nl> end_subband = get_bits ( gbc , 3 ) + 5 ; <nl> # if USE_FIXED <nl> - s -> spx_dst_end_freq = end_freq_inv_tab [ end_subband ]; <nl> + s -> spx_dst_end_freq = end_freq_inv_tab [ end_subband - 5 ]; <nl> # endif <nl> if ( end_subband > 7 ) <nl> end_subband += end_subband - 7 ;
static int mov_text_decode_close ( AVCodecContext * avctx ) <nl> { <nl> MovTextContext * m = avctx -> priv_data ; <nl> mov_text_cleanup_ftab ( m ); <nl> + mov_text_cleanup ( m ); <nl> return 0 ; <nl> } <nl> 
static const AVOption options [] = { <nl> { " safe ", " enable safe mode ", <nl> OFFSET ( safe ), AV_OPT_TYPE_INT , {. i64 = - 1 }, - 1 , 1 , DEC }, <nl> { " auto_convert ", " automatically convert bitstream format ", <nl> - OFFSET ( auto_convert ), AV_OPT_TYPE_INT , {. i64 = 0 }, 0 , 1 , DEC }, <nl> + OFFSET ( auto_convert ), AV_OPT_TYPE_INT , {. i64 = 1 }, 0 , 1 , DEC }, <nl> { NULL } <nl> }; <nl> 
int main ( int argc , char * argv []) <nl> /* keep ftyp atom */ <nl> if ( atom_type == FTYP_ATOM ) { <nl> ftyp_atom_size = atom_size ; <nl> + free ( ftyp_atom ); <nl> ftyp_atom = malloc ( ftyp_atom_size ); <nl> if (! ftyp_atom ) { <nl> printf (" could not allocate %" PRIu64 " byte for ftyp atom \ n ",
static int submit_packet ( PerThreadContext * p , AVPacket * avpkt ) <nl> } <nl>  <nl> fctx -> prev_thread = p ; <nl> + fctx -> next_decoding ++; <nl>  <nl> return 0 ; <nl> } <nl> int ff_thread_decode_frame ( AVCodecContext * avctx , <nl> err = submit_packet ( p , avpkt ); <nl> if ( err ) return err ; <nl>  <nl> - fctx -> next_decoding ++; <nl> - <nl> /* <nl> * If we ' re still receiving the initial packets , don ' t return a frame . <nl> */
static int read_header ( ShortenContext * s ) <nl> s -> blocksize = blocksize ; <nl>  <nl> maxnlpc = get_uint ( s , LPCQSIZE ); <nl> + if ( maxnlpc > 1024U ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " maxnlpc is : % d \ n ", maxnlpc ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> nmean = get_uint ( s , 0 ); <nl>  <nl> skip_bytes = get_uint ( s , NSKIPSIZE );
static int bmp_decode_frame ( AVCodecContext * avctx , <nl> BiCompression comp ; <nl> unsigned int ihsize ; <nl> int i , j , n , linesize , ret ; <nl> - uint32_t rgb [ 3 ]; <nl> + uint32_t rgb [ 3 ] = { 0 }; <nl> uint32_t alpha = 0 ; <nl> uint8_t * ptr ; <nl> int dsize ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> # else <nl> if ( s -> xch_present && ! s -> xch_disable ) { <nl> # endif <nl> + if ( avctx -> channel_layout & AV_CH_BACK_CENTER ) { <nl> + avpriv_request_sample ( avctx , " XCh with Back center channel "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> avctx -> channel_layout |= AV_CH_BACK_CENTER ; <nl> if ( s -> lfe ) { <nl> avctx -> channel_layout |= AV_CH_LOW_FREQUENCY ;
static inline int mpeg4_decode_block ( MpegEncContext * s , DCTELEM * block , <nl> int n , int coded , int intra , int rvlc ) <nl> { <nl> int level , i , last , run ; <nl> - int dc_pred_dir ; <nl> + int av_uninit ( dc_pred_dir ); <nl> RLTable * rl ; <nl> RL_VLC_ELEM * rl_vlc ; <nl> const uint8_t * scan_table ;
static int decode_ref_pic_list_reordering ( H264Context * h ){ <nl> const unsigned int abs_diff_pic_num = get_ue_golomb (& s -> gb ) + 1 ; <nl> int frame_num ; <nl>  <nl> - if ( abs_diff_pic_num >= h -> max_pic_num ){ <nl> + if ( abs_diff_pic_num > h -> max_pic_num ){ <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " abs_diff_pic_num overflow \ n "); <nl> return - 1 ; <nl> }
static void pre_process_video_frame ( InputStream * ist , AVPicture * picture , void * <nl>  <nl> /* create temporary picture */ <nl> size = avpicture_get_size ( dec -> pix_fmt , dec -> width , dec -> height ); <nl> + if ( size < 0 ) <nl> + return ; <nl> buf = av_malloc ( size ); <nl> if (! buf ) <nl> return ;
static int cinvideo_decode_frame ( AVCodecContext * avctx , <nl> bitmap_frame_size = buf_size - 4 ; <nl>  <nl> /* handle palette */ <nl> + if ( bitmap_frame_size < palette_colors_count * ( 3 + ( palette_type != 0 ))) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( palette_type == 0 ) { <nl> for ( i = 0 ; i < palette_colors_count ; ++ i ) { <nl> cin -> palette [ i ] = bytestream_get_le24 (& buf );
static int asf_read_close ( AVFormatContext * s ) <nl> av_dict_free (& asf -> asf_sd [ i ]. asf_met ); <nl> } <nl>  <nl> + asf -> nb_streams = 0 ; <nl> return 0 ; <nl> } <nl> 
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
typedef struct Jpeg2000TgtNode { <nl> } Jpeg2000TgtNode ; <nl>  <nl> typedef struct Jpeg2000CodingStyle { <nl> - uint8_t nreslevels ; // number of resolution levels <nl> - uint8_t nreslevels2decode ; // number of resolution levels to decode <nl> + int nreslevels ; // number of resolution levels <nl> + int nreslevels2decode ; // number of resolution levels to decode <nl> uint8_t log2_cblk_width , <nl> log2_cblk_height ; // exponent of codeblock size <nl> uint8_t transform ; // DWT type
static int mpeg_decode_slice ( MpegEncContext * s , int mb_y , <nl> } <nl> } <nl> eos : // end of slice <nl> + if ( get_bits_left (& s -> gb ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> * buf += ( get_bits_count (& s -> gb )- 1 )/ 8 ; <nl> av_dlog ( s , " y % d % d % d % d \ n ", s -> resync_mb_x , s -> resync_mb_y , s -> mb_x , s -> mb_y ); <nl> return 0 ;
static int alac_decode_frame ( AVCodecContext * avctx , void * data , <nl> avpkt -> size * 8 - get_bits_count (& alac -> gb )); <nl> } <nl>  <nl> - * got_frame_ptr = 1 ; <nl> + if ( alac -> channels == ch ) <nl> + * got_frame_ptr = 1 ; <nl>  <nl> return avpkt -> size ; <nl> }
static int config_props ( AVFilterLink * outlink ) <nl>  <nl> if (( ret = sws_init_context (* s , NULL , NULL )) < 0 ) <nl> return ret ; <nl> + if (! scale -> interlaced ) <nl> + break ; <nl> } <nl> } <nl> 
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl>  <nl> if ( x >= w ) { <nl> av_log ( NULL , AV_LOG_ERROR , " run overflow \ n "); <nl> + av_assert0 ( x <= w ); <nl> return ; <nl> } <nl> 
static int decode_frame_mp3on4 ( AVCodecContext * avctx , void * data , <nl>  <nl> avpriv_mpegaudio_decode_header (( MPADecodeHeader *) m , header ); <nl>  <nl> - if ( ch + m -> nb_channels > avctx -> channels ) { <nl> + if ( ch + m -> nb_channels > avctx -> channels || <nl> + s -> coff [ fr ] + m -> nb_channels > avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame channel count exceeds codec " <nl> " channel count \ n "); <nl> return AVERROR_INVALIDDATA ;
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl> vseq -> seq_fields . bits . direct_8x8_inference_flag = 1 ; <nl> vseq -> seq_fields . bits . log2_max_frame_num_minus4 = 4 ; <nl> vseq -> seq_fields . bits . pic_order_cnt_type = 0 ; <nl> + vseq -> seq_fields . bits . log2_max_pic_order_cnt_lsb_minus4 = <nl> + av_clip ( av_log2 ( avctx -> max_b_frames + 1 ) - 2 , 0 , 12 ); <nl>  <nl> if ( avctx -> width != ctx -> surface_width || <nl> avctx -> height != ctx -> surface_height ) {
int ff_alsa_get_device_list ( AVDeviceInfoList * device_list , snd_pcm_stream_t stre <nl> & device_list -> nb_devices , new_device )) < 0 ) { <nl> goto fail ; <nl> } <nl> + if (! strcmp ( new_device -> device_name , " default ")) <nl> + device_list -> default_device = device_list -> nb_devices - 1 ; <nl> new_device = NULL ; <nl> } <nl> fail :
static int ffm_write_packet ( AVFormatContext * s , int stream_index , <nl> /* packet size & key_frame */ <nl> header [ 0 ] = stream_index ; <nl> header [ 1 ] = 0 ; <nl> - if ( st -> codec . coded_picture -> key_frame ) <nl> + if ( st -> codec . coded_picture && st -> codec . coded_picture -> key_frame ) <nl> header [ 1 ] |= FLAG_KEY_FRAME ; <nl> header [ 2 ] = ( size >> 16 ) & 0xff ; <nl> header [ 3 ] = ( size >> 8 ) & 0xff ;
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl> } <nl> if ( ctx -> swfurl ) { <nl> av_strlcat ( filename , " swfUrl =", len ); <nl> - av_strlcat ( filename , ctx -> pageurl , len ); <nl> + av_strlcat ( filename , ctx -> swfurl , len ); <nl> } <nl> if ( ctx -> flashver ) { <nl> av_strlcat ( filename , " flashVer =", len );
static int flashsv2_prime ( FlashSVContext * s , uint8_t * src , int size ) <nl> z_stream zs ; <nl> int zret ; // Zlib return code <nl>  <nl> + if (! src ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> zs . zalloc = NULL ; <nl> zs . zfree = NULL ; <nl> zs . opaque = NULL ;
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
static int extract_extradata_h2645 ( AVBSFContext * ctx , AVPacket * pkt , <nl> ret = ff_h2645_packet_split (& h2645_pkt , pkt -> data , pkt -> size , <nl> ctx , 0 , 0 , ctx -> par_in -> codec_id , 1 ); <nl> if ( ret < 0 ) <nl> - return ret ; <nl> + goto fail ; <nl>  <nl> for ( i = 0 ; i < h2645_pkt . nb_nals ; i ++) { <nl> H2645NAL * nal = & h2645_pkt . nals [ i ];
static int initFilter ( int16_t ** outFilter , int16_t ** filterPos , int * outFilterSi <nl> int j ; <nl> (* filterPos )[ i ]= xx ; <nl> for ( j = 0 ; j < filterSize ; j ++) { <nl> - int64_t d = (( int64_t ) FFABS (( xx << 17 ) - xDstInSrc ))<< 13 ; <nl> + int64_t d = ( FFABS ((( int64_t ) xx << 17 ) - xDstInSrc ))<< 13 ; <nl> double floatd ; <nl> int64_t coeff ; <nl> 
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> goto fail ; <nl>  <nl> s -> cbf_luma = av_malloc ( sps -> min_tb_width * sps -> min_tb_height ); <nl> - s -> tab_ipm = av_malloc ( min_pu_size ); <nl> + s -> tab_ipm = av_mallocz ( min_pu_size ); <nl> s -> is_pcm = av_malloc ( min_pu_size ); <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ;
static int stream_component_open ( VideoState * is , int stream_index ) <nl> goto fail ; <nl> link = is -> out_audio_filter -> inputs [ 0 ]; <nl> sample_rate = link -> sample_rate ; <nl> - nb_channels = link -> channels ; <nl> + nb_channels = avfilter_link_get_channels ( link ); <nl> channel_layout = link -> channel_layout ; <nl> } <nl> # else
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_INIT_VIDEO_BUFFERS : <nl> av_dlog ( NULL , " initialize video buffers \ n "); <nl> - if (( opcode_version > 2 ) || ( opcode_size > 8 )) { <nl> + if (( opcode_version > 2 ) || ( opcode_size > 8 ) || opcode_size < 4 ) { <nl> av_dlog ( NULL , " bad init_video_buffers opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
typedef struct { <nl> static int probe ( AVProbeData * p ) <nl> { <nl> if ( AV_RL16 ( p -> buf ) == 0 && AV_RL16 ( p -> buf + 2 ) == 1 && AV_RL16 ( p -> buf + 4 )) <nl> - return AVPROBE_SCORE_MAX / 3 ; <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> return 0 ; <nl> } <nl> 
static int mjpeg_decode_scan ( MJpegDecodeContext * s , int nb_components , int Ah , <nl> uint8_t * data [ MAX_COMPONENTS ]; <nl> const uint8_t * reference_data [ MAX_COMPONENTS ]; <nl> int linesize [ MAX_COMPONENTS ]; <nl> - GetBitContext mb_bitmask_gb ; <nl> + GetBitContext mb_bitmask_gb = { 0 }; // initialize to silence gcc warning <nl> int bytes_per_pixel = 1 + ( s -> bits > 8 ); <nl>  <nl> if ( mb_bitmask ) {
static int h263p_decode_umotion ( MpegEncContext * s , int pred ) <nl> code += get_bits1 (& s -> gb ); <nl> if ( code >= 32768 ) { <nl> avpriv_request_sample ( s -> avctx , " Huge DMV "); <nl> - return AVERROR_INVALIDDATA ; <nl> + return 0xffff ; <nl> } <nl> } <nl> sign = code & 1 ;
static int read_thread ( void * arg ) <nl> stream_component_close ( is , is -> video_stream ); <nl> if ( is -> subtitle_stream >= 0 ) <nl> stream_component_close ( is , is -> subtitle_stream ); <nl> - if ( is -> ic ) { <nl> - avformat_close_input (& is -> ic ); <nl> + if ( ic ) { <nl> + avformat_close_input (& ic ); <nl> + is -> ic = NULL ; <nl> } <nl>  <nl> if ( ret != 0 ) {
static int twolame_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> if ( ret < 0 ) // twolame error <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame ) { <nl> + avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame -> pts != AV_NOPTS_VALUE ) <nl> avpkt -> pts = frame -> pts - ff_samples_to_time_base ( avctx , avctx -> initial_padding ); <nl> } else {
cl_program av_opencl_compile ( const char * program_name , const char * build_opts ) <nl> int i ; <nl> cl_int status , build_status ; <nl> int kernel_code_idx = 0 ; <nl> - const char * kernel_source ; <nl> + const char * kernel_source = NULL ; <nl> size_t kernel_code_len ; <nl> char * ptr = NULL ; <nl> cl_program program = NULL ;
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + case NAL_IDR_SLICE : <nl> + case NAL_SLICE : <nl> nals_needed = nal_index ; <nl> } <nl> continue ;
again : <nl>  <nl> if ( avctx -> bits_per_raw_sample != h -> sps . bit_depth_luma || <nl> h -> cur_chroma_format_idc != h -> sps . chroma_format_idc ) { <nl> - if ( s -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU <nl> + if ( s -> avctx -> codec && <nl> + s -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU <nl> && ( h -> sps . bit_depth_luma != 8 || <nl> h -> sps . chroma_format_idc > 1 )) { <nl> av_log ( avctx , AV_LOG_ERROR ,
static char * value_string ( char * buf , int buf_size , struct unit_value uv ) <nl> const char * prefix_string = ""; <nl> int l ; <nl>  <nl> - if ( use_value_prefix ) { <nl> + if ( use_value_prefix && vald > 1 ) { <nl> long long int index ; <nl>  <nl> if ( uv . unit == unit_byte_str && use_byte_value_binary_prefix ) {
static int get_video_frame ( VideoState * is , AVFrame * frame , int64_t * pts , AVPacke <nl> return 0 ; <nl> } <nl>  <nl> - avcodec_decode_video2 ( is -> video_st -> codec , frame , & got_picture , pkt ); <nl> + if ( avcodec_decode_video2 ( is -> video_st -> codec , frame , & got_picture , pkt ) < 0 ) <nl> + return - 1 ; <nl>  <nl> if ( got_picture ) { <nl> int ret = 1 ;
static int dnxhd_encode_rdo ( AVCodecContext * avctx , DNXHDEncContext * ctx ) <nl> last_higher = FFMAX ( lambda , last_higher ); <nl> if ( last_lower != INT_MAX ) <nl> lambda = ( lambda + last_lower )>> 1 ; <nl> + else if (( int64_t ) lambda + up_step > INT_MAX ) <nl> + return - 1 ; <nl> else <nl> lambda += up_step ; <nl> - up_step *= 5 ; <nl> + up_step = FFMIN (( int64_t ) up_step * 5 , INT_MAX ); <nl> down_step = 1 << LAMBDA_FRAC_BITS ; <nl> } <nl> }
int av_find_stream_info ( AVFormatContext * ic ) <nl> st -> codec -> frame_size = 0 ; <nl> st -> codec -> channels = 0 ; <nl> } <nl> - if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO ){ <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO || <nl> + st -> codec -> codec_type == AVMEDIA_TYPE_SUBTITLE ) { <nl> /* if (! st -> time_base . num ) <nl> st -> time_base = */ <nl> if (! st -> codec -> time_base . num )
static void event_loop ( VideoState * cur_stream ) <nl> } else { <nl> pos = get_master_clock ( cur_stream ); <nl> pos += incr ; <nl> + if ( cur_stream -> ic -> start_time != AV_NOPTS_VALUE && pos < cur_stream -> ic -> start_time / ( double ) AV_TIME_BASE ) <nl> + pos = cur_stream -> ic -> start_time / ( double ) AV_TIME_BASE ; <nl> stream_seek ( cur_stream , ( int64_t )( pos * AV_TIME_BASE ), ( int64_t )( incr * AV_TIME_BASE ), 0 ); <nl> } <nl> break ;
int ff_spatial_idwt_init2 ( DWTContext * d , IDWTELEM * buffer , int width , int height <nl> d -> vertical_compose_l0 = ( void *) vertical_compose_fidelityiL0 ; <nl> d -> vertical_compose_h0 = ( void *) vertical_compose_fidelityiH0 ; <nl> d -> horizontal_compose = horizontal_compose_fidelityi ; <nl> + d -> support = 0 ; // not really used <nl> break ; <nl> case DWT_DIRAC_DAUB9_7 : <nl> d -> spatial_compose = spatial_compose_daub97i_dy ;
int ff_mpeg_update_thread_context ( AVCodecContext * dst , <nl> # define UPDATE_PICTURE ( pic )\ <nl> do {\ <nl> ff_mpeg_unref_picture ( s , & s -> pic );\ <nl> - if ( s1 -> pic . f -> buf [ 0 ])\ <nl> + if ( s1 -> pic . f && s1 -> pic . f -> buf [ 0 ])\ <nl> ret = ff_mpeg_ref_picture ( s , & s -> pic , & s1 -> pic );\ <nl> else \ <nl> ret = update_picture_tables (& s -> pic , & s1 -> pic );\
static int64_t asf_read_pts ( AVFormatContext * s , int stream_index , <nl>  <nl> // assert (( asf_st -> packet_pos - s -> data_offset ) % s -> packet_size == 0 ); <nl> pos = asf_st -> packet_pos ; <nl> + av_assert1 ( pkt -> pos == asf_st -> packet_pos ); <nl>  <nl> av_add_index_entry ( s -> streams [ i ], pos , pts , pkt -> size , <nl> pos - start_pos [ i ] + 1 , AVINDEX_KEYFRAME );
av_cold int vaapi_device_init ( const char * device ) <nl> { <nl> int err ; <nl>  <nl> + av_buffer_unref (& hw_device_ctx ); <nl> + <nl> err = av_hwdevice_ctx_create (& hw_device_ctx , AV_HWDEVICE_TYPE_VAAPI , <nl> device , NULL , 0 ); <nl> if ( err < 0 ) {
int ff_jni_exception_get_summary ( JNIEnv * env , jthrowable exception , char ** error <nl> jclass exception_class = NULL ; <nl> jmethodID get_message_id = NULL ; <nl>  <nl> - jstring string ; <nl> + jstring string = NULL ; <nl>  <nl> av_bprint_init (& bp , 0 , AV_BPRINT_SIZE_AUTOMATIC ); <nl> 
int ff_raw_audio_read_header ( AVFormatContext * s , <nl> st -> codec -> codec_type = AVMEDIA_TYPE_AUDIO ; <nl> st -> codec -> codec_id = s -> iformat -> value ; <nl> st -> need_parsing = AVSTREAM_PARSE_FULL ; <nl> + st -> start_time = 0 ; <nl> /* the parameters will be extracted from the compressed bitstream */ <nl>  <nl> return 0 ;
static int sap_write_close ( AVFormatContext * s ) <nl> url_fclose ( rtpctx -> pb ); <nl> av_metadata_free (& rtpctx -> streams [ 0 ]-> metadata ); <nl> av_metadata_free (& rtpctx -> metadata ); <nl> + av_free ( rtpctx -> streams [ 0 ]-> info ); <nl> av_free ( rtpctx -> streams [ 0 ]); <nl> av_free ( rtpctx ); <nl> s -> streams [ i ]-> priv_data = NULL ;
exit_loop : <nl> } <nl> } <nl> ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> + ff_thread_report_progress (& s -> previous_picture , INT_MAX , 0 ); <nl>  <nl> av_frame_set_metadata ( p , metadata ); <nl> metadata = NULL ; <nl> exit_loop : <nl> fail : <nl> av_dict_free (& metadata ); <nl> ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> + ff_thread_report_progress (& s -> previous_picture , INT_MAX , 0 ); <nl> return ret ; <nl> } <nl> 
static void compute_chapters_end ( AVFormatContext * s ) <nl> unsigned int i , j ; <nl> int64_t max_time = 0 ; <nl>  <nl> - if ( s -> duration > 0 ) <nl> + if ( s -> duration > 0 && s -> start_time < INT64_MAX - s -> duration ) <nl> max_time = s -> duration + <nl> (( s -> start_time == AV_NOPTS_VALUE ) ? 0 : s -> start_time ); <nl> 
static int matroska_ebmlnum_uint ( MatroskaDemuxContext * matroska , <nl> { <nl> ByteIOContext pb ; <nl> init_put_byte (& pb , data , size , 0 , NULL , NULL , NULL , NULL ); <nl> - return ebml_read_num ( matroska , & pb , 8 , num ); <nl> + return ebml_read_num ( matroska , & pb , FFMIN ( size , 8 ), num ); <nl> } <nl>  <nl> /*
static inline int wv_unpack_stereo ( WavpackFrameContext * s , GetBitContext * gb , <nl> } <nl>  <nl> if ( type == AV_SAMPLE_FMT_S16P ) { <nl> - if ( FFABS ( L ) + FFABS ( R ) > ( 1 << 19 )) { <nl> + if ( FFABS ( L ) + ( unsigned ) FFABS ( R ) > ( 1 << 19 )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " sample % d % d too large \ n ", L , R ); <nl> return AVERROR_INVALIDDATA ; <nl> }
int ff_combine_frame ( ParseContext * pc , int next , const uint8_t ** buf , int * buf_s <nl> if (! new_buffer ) <nl> return AVERROR ( ENOMEM ); <nl> pc -> buffer = new_buffer ; <nl> + if ( FF_INPUT_BUFFER_PADDING_SIZE > - next ) <nl> memcpy (& pc -> buffer [ pc -> index ], * buf , next + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> pc -> index = 0 ; <nl> * buf = pc -> buffer ;
static int decode_exp_vlc ( WMACodecContext * s , int ch ) <nl> } <nl> /* NOTE : this offset is the same as MPEG4 AAC ! */ <nl> last_exp += code - 60 ; <nl> - if (( unsigned ) last_exp + 60 > FF_ARRAY_ELEMS ( pow_tab )) { <nl> + if (( unsigned ) last_exp + 60 >= FF_ARRAY_ELEMS ( pow_tab )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Exponent out of range : % d \ n ", <nl> last_exp ); <nl> return - 1 ;
static void sample_queue_push ( HintSampleQueue * queue , uint8_t * data , int size , <nl> return ; <nl> if (! queue -> samples || queue -> len >= queue -> size ) { <nl> HintSample * samples ; <nl> - samples = av_realloc ( queue -> samples , sizeof ( HintSample ) * ( queue -> size + 10 )); <nl> + samples = av_realloc_array ( queue -> samples , queue -> size + 10 , sizeof ( HintSample )); <nl> if (! samples ) <nl> return ; <nl> queue -> size += 10 ;
static void qdm2_fft_decode_tones ( QDM2Context * q , int duration , GetBitContext * <nl> return ; <nl>  <nl> local_int_14 = ( offset >> local_int_8 ); <nl> + if ( local_int_14 >= FF_ARRAY_ELEMS ( fft_level_index_table )) <nl> + return ; <nl>  <nl> if ( q -> nb_channels > 1 ) { <nl> channel = get_bits1 ( gb );
static int initFilter ( int16_t ** outFilter , int16_t ** filterPos , int * outFilterSi <nl> if ( xInc <= 1 << 16 ) filterSize = 1 + sizeFactor ; // upscale <nl> else filterSize = 1 + ( sizeFactor * srcW + dstW - 1 )/ dstW ; <nl>  <nl> - if ( filterSize > srcW - 2 ) filterSize = srcW - 2 ; <nl> + filterSize = av_clip ( filterSize , 1 , srcW - 2 ); <nl>  <nl> FF_ALLOC_OR_GOTO ( NULL , filter , dstW * sizeof (* filter )* filterSize , fail ); <nl> 
static int decode_lowdelay ( DiracContext * s ) <nl> slice_num ++; <nl>  <nl> buf += bytes ; <nl> - bufsize -= bytes * 8 ; <nl> + if ( bufsize / 8 >= bytes ) <nl> + bufsize -= bytes * 8 ; <nl> + else <nl> + bufsize = 0 ; <nl> } <nl>  <nl> avctx -> execute ( avctx , decode_lowdelay_slice , slices , NULL , slice_num ,
static int decode_subframe_fixed ( FLACContext * s , int channel , int pred_order ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> int32_t * decoded = s -> decoded [ channel ]; <nl> - int a , b , c , d , i ; <nl> + int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl>  <nl> /* warm up samples */ <nl> for ( i = 0 ; i < pred_order ; i ++) {
static int xan_huffman_decode ( uint8_t * dest , int dest_len , <nl> return ret ; <nl>  <nl> while ( val != 0x16 ) { <nl> - unsigned idx = val - 0x17 + get_bits1 (& gb ) * byte ; <nl> + unsigned idx ; <nl> + if ( get_bits_left (& gb ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + idx = val - 0x17 + get_bits1 (& gb ) * byte ; <nl> if ( idx >= 2 * byte ) <nl> return AVERROR_INVALIDDATA ; <nl> val = src [ idx ];
static int start_frame ( AVFilterLink * link , AVFilterBufferRef * picref ) <nl>  <nl> av_assert0 ( picref ); <nl>  <nl> + if ( picref -> video -> h < 3 || picref -> video -> w < 3 ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Video of less than 3 columns or lines is not supported \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> if ( yadif -> frame_pending ) <nl> return_frame ( ctx , 1 ); <nl> 
static int ftp_send_command ( FTPContext * s , const char * command , <nl> if ( response ) <nl> * response = NULL ; <nl>  <nl> + if (! s -> conn_control ) <nl> + return AVERROR ( EIO ); <nl> + <nl> if (( err = ffurl_write ( s -> conn_control , command , strlen ( command ))) < 0 ) <nl> return err ; <nl> if (! err )
static int segment_mux_init ( AVFormatContext * s ) <nl> oc -> opaque = s -> opaque ; <nl> oc -> io_close = s -> io_close ; <nl> oc -> io_open = s -> io_open ; <nl> + oc -> flags = s -> flags ; <nl>  <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> AVStream * st ;
static int idcin_read_packet ( AVFormatContext * s , <nl> } <nl>  <nl> chunk_size = avio_rl32 ( pb ); <nl> + if ( chunk_size < 4 || chunk_size > INT_MAX - 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " invalid chunk size : % u \ n ", chunk_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> /* skip the number of decoded bytes ( always equal to width * height ) */ <nl> avio_skip ( pb , 4 ); <nl> chunk_size -= 4 ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> if (! avctx -> rc_initial_buffer_occupancy ) <nl> avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3 / 4 ; <nl>  <nl> - if ( avctx -> ticks_per_frame && <nl> + if ( avctx -> ticks_per_frame && avctx -> time_base . num && <nl> avctx -> ticks_per_frame > INT_MAX / avctx -> time_base . num ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " ticks_per_frame % d too large for the timebase % d /% d .",
matroska_read_seek ( AVFormatContext * s , int stream_index , int64_t timestamp , <nl> matroska -> skip_to_keyframe = !( flags & AVSEEK_FLAG_ANY ); <nl> matroska -> skip_to_stream = st ; <nl> matroska -> peek_id = 0 ; <nl> + av_update_cur_dts ( s , st , st -> index_entries [ index ]. timestamp ); <nl> return 0 ; <nl> } <nl> 
static int oma_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> pkt -> stream_index = 0 ; <nl>  <nl> - if ( pos > 0 ) { <nl> + if ( pos > 0 && byte_rate > 0 ) { <nl> pkt -> pts = <nl> pkt -> dts = av_rescale ( pos , st -> time_base . den , <nl> byte_rate * ( int64_t ) st -> time_base . num );
static int decode_lt_rps ( HEVCContext * s , LongTermRPS * rps , GetBitContext * gb ) <nl> static int set_sps ( HEVCContext * s , const HEVCSPS * sps ) <nl> { <nl> int ret ; <nl> - int num = 0 , den = 0 ; <nl> + unsigned num = 0 , den = 0 ; <nl>  <nl> pic_arrays_free ( s ); <nl> ret = pic_arrays_init ( s , sps );
AV_WN32A ( v0 + i , r ); \ <nl> AV_WN32A ( v1 + i , r ); \ <nl> } \ <nl> - for ( i = 0 ; i < BUF_SIZE * 8 / 3 ; i += 4 ) { \ <nl> + for ( i = 0 ; i < width * 8 / 3 ; i += 4 ) { \ <nl> uint32_t r = rnd (); \ <nl> AV_WN32A ( dst0 + i , r ); \ <nl> AV_WN32A ( dst1 + i , r ); \
FFPsyChannelGroup * ff_psy_find_group ( FFPsyContext * ctx , int channel ) <nl>  <nl> av_cold void ff_psy_end ( FFPsyContext * ctx ) <nl> { <nl> - if ( ctx -> model -> end ) <nl> + if ( ctx -> model && ctx -> model -> end ) <nl> ctx -> model -> end ( ctx ); <nl> av_freep (& ctx -> bands ); <nl> av_freep (& ctx -> num_bands );
static inline uint8_t lag_get_rac ( lag_rac * l ) <nl> l -> range -= range_scaled * l -> prob [ 255 ]; <nl> } <nl>  <nl> + if (! l -> range ) <nl> + l -> range = 0x80 ; <nl> + <nl> l -> low -= range_scaled * l -> prob [ val ]; <nl>  <nl> return val ;
static void writer_close ( WriterContext ** wctx ) <nl> if ((* wctx )-> writer -> priv_class ) <nl> av_opt_free ((* wctx )-> priv ); <nl> av_freep (&((* wctx )-> priv )); <nl> + av_opt_free (* wctx ); <nl> av_freep ( wctx ); <nl> } <nl> 
static int read_braindead_odml_indx ( AVFormatContext * s , int frame_num ){ <nl> longs_pre_entry , index_type , entries_in_use , chunk_id , base ); <nl> # endif <nl>  <nl> - if ( stream_id > s -> nb_streams || stream_id < 0 ) <nl> + if ( stream_id >= s -> nb_streams || stream_id < 0 ) <nl> return - 1 ; <nl> st = s -> streams [ stream_id ]; <nl> ast = st -> priv_data ;
void ff_set_mpeg4_time ( MpegEncContext * s ) <nl>  <nl> static void mpeg4_encode_gop_header ( MpegEncContext * s ) <nl> { <nl> - int hours , minutes , seconds ; <nl> + int64_t hours , minutes , seconds ; <nl> int64_t time ; <nl>  <nl> put_bits (& s -> pb , 16 , 0 );
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> if (! get_bits1 (& ctx -> gb ) || ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> transform_id = get_bits (& ctx -> gb , 5 ); <nl> - if (! transforms [ transform_id ]. inv_trans ) { <nl> + if ( transform_id >= FF_ARRAY_ELEMS ( transforms ) || <nl> + ! transforms [ transform_id ]. inv_trans ) { <nl> av_log_ask_for_sample ( avctx , " Unimplemented transform : % d !\ n ", transform_id ); <nl> return AVERROR_PATCHWELCOME ; <nl> }
static int get_delayed_pic ( DiracContext * s , AVFrame * picture , int * got_frame ) <nl>  <nl> if ( out ) { <nl> out -> reference ^= DELAYED_PIC_REF ; <nl> - * got_frame = 1 ; <nl> if (( ret = av_frame_ref ( picture , out -> avframe )) < 0 ) <nl> return ret ; <nl> + * got_frame = 1 ; <nl> } <nl>  <nl> return 0 ;
int ff_h264_decode_ref_pic_list_reordering ( H264Context * h , H264SliceContext * sl ) <nl>  <nl> long_idx = pic_num_extract ( h , pic_id , & pic_structure ); <nl>  <nl> - if ( long_idx > 31 ) { <nl> + if ( long_idx > 31U ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> " long_term_pic_idx overflow \ n "); <nl> return AVERROR_INVALIDDATA ;
int64_t av_rescale_rnd ( int64_t a , int64_t b , int64_t c , enum AVRounding rnd ) <nl> else { <nl> int64_t ad = a / c ; <nl> int64_t a2 = ( a % c * b + r ) / c ; <nl> - if ( ad >= INT32_MAX && ad > ( INT64_MAX - a2 ) / b ) <nl> + if ( ad >= INT32_MAX && b && ad > ( INT64_MAX - a2 ) / b ) <nl> return INT64_MIN ; <nl> return ad * b + a2 ; <nl> }
static void ff_compute_band_indexes ( MPADecodeContext * s , GranuleDef * g ) <nl> else <nl> g -> long_end = 6 ; <nl>  <nl> - g -> short_start = 2 + ( s -> sample_rate_index != 8 ); <nl> + g -> short_start = 3 ; <nl> } else { <nl> g -> long_end = 0 ; <nl> g -> short_start = 0 ;
static int mxf_read_generic_descriptor ( void * arg , AVIOContext * pb , int tag , int <nl> default : <nl> /* Private uid used by SONY C0023S01 . mxf */ <nl> if ( IS_KLV_KEY ( uid , mxf_sony_mpeg4_extradata )) { <nl> - descriptor -> extradata = av_malloc ( size ); <nl> + descriptor -> extradata = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! descriptor -> extradata ) <nl> return - 1 ; <nl> descriptor -> extradata_size = size ;
static int Faac_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl>  <nl> /* add current frame to the queue */ <nl> if ( frame ) { <nl> - if (( ret = ff_af_queue_add (& s -> afq , frame ) < 0 )) <nl> + if (( ret = ff_af_queue_add (& s -> afq , frame )) < 0 ) <nl> return ret ; <nl> } <nl> 
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int ret , i , type , size , flags , is_audio ; <nl> int64_t next , pos ; <nl> int64_t dts , pts = AV_NOPTS_VALUE ; <nl> - int sample_rate , channels ; <nl> + int sample_rate = 0 , channels = 0 ; <nl> AVStream * st = NULL ; <nl>  <nl> for (;; avio_skip ( s -> pb , 4 )){ /* pkt size is repeated at end . skip it */
static int decode_blocks ( SnowContext * s ){ <nl>  <nl> for ( y = 0 ; y < h ; y ++){ <nl> for ( x = 0 ; x < w ; x ++){ <nl> + if ( s -> c . bytestream >= s -> c . bytestream_end ) <nl> + return AVERROR_INVALIDDATA ; <nl> if (( res = decode_q_branch ( s , 0 , x , y )) < 0 ) <nl> return res ; <nl> }
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> uint8_t * end = dst + ( width - 2 ); <nl>  <nl> avpriv_request_sample ( l -> avctx , " zero_run_line "); <nl> - return AVERROR_PATCHWELCOME ; <nl> + <nl> + memset ( dst , 0 , width ); <nl>  <nl> output_zeros : <nl> if ( l -> zeros_rem ) {
retry : <nl>  <nl> if ( next_pkt && next_pkt -> dts - scr > max_delay ) <nl> continue ; <nl> - <nl> + if ( stream -> predecode_packet <nl> + && stream -> predecode_packet -> size > stream -> buffer_index ) <nl> + rel_space += 1 << 28 ; <nl> if ( rel_space > best_score ){ <nl> best_score = rel_space ; <nl> best_i = i ;
static void show_packets ( AVFormatContext * fmt_ctx ) <nl>  <nl> av_init_packet (& pkt ); <nl> probe_array_header (" packets ", 0 ); <nl> - while (! av_read_frame ( fmt_ctx , & pkt )) <nl> + while (! av_read_frame ( fmt_ctx , & pkt )) { <nl> show_packet ( fmt_ctx , & pkt ); <nl> + av_packet_unref (& pkt ); <nl> + } <nl> probe_array_footer (" packets ", 0 ); <nl> } <nl> 
static enum CodecID vfw_codecid ( DWORD biCompression ) <nl> switch ( biCompression ) { <nl> case MKTAG (' d ', ' v ', ' s ', ' d '): <nl> return CODEC_ID_DVVIDEO ; <nl> + case MKTAG (' M ', ' J ', ' P ', ' G '): <nl> + case MKTAG (' m ', ' j ', ' p ', ' g '): <nl> + return CODEC_ID_MJPEG ; <nl> } <nl> return CODEC_ID_NONE ; <nl> }
static int read_len_table ( uint8_t * dst , GetBitContext * gb ){ <nl> if ( repeat == 0 ) <nl> repeat = get_bits ( gb , 8 ); <nl> // printf ("% d % d \ n ", val , repeat ); <nl> - if ( i + repeat > 256 ) { <nl> + if ( i + repeat > 256 || get_bits_left ( gb ) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error reading huffman table \ n "); <nl> return - 1 ; <nl> }
typedef struct MotionEstContext { <nl> int stride ; <nl> int uvstride ; <nl> /* temp variables for picture complexity calculation */ <nl> - int mc_mb_var_sum_temp ; <nl> - int mb_var_sum_temp ; <nl> + int64_t mc_mb_var_sum_temp ; <nl> + int64_t mb_var_sum_temp ; <nl> int scene_change_score ; <nl> /* cmp , chroma_cmp ;*/ <nl> op_pixels_func (* hpel_put )[ 4 ];
static int sami_paragraph_to_ass ( AVCodecContext * avctx , const char * src ) <nl> AVBPrint * dst_content = & sami -> encoded_content ; <nl> AVBPrint * dst_source = & sami -> encoded_source ; <nl>  <nl> + if (! dupsrc ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> av_bprint_clear (& sami -> encoded_content ); <nl> av_bprint_clear (& sami -> content ); <nl> av_bprint_clear (& sami -> encoded_source );
static int dsf_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> st -> codecpar -> block_align *= st -> codecpar -> channels ; <nl> + st -> codecpar -> bit_rate = st -> codecpar -> channels * st -> codecpar -> sample_rate * 8LL ; <nl> avio_skip ( pb , 4 ); <nl>  <nl> /* data chunk */
static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> av_log ( avctx , AV_LOG_ERROR , " Buffer contains IP frames !\ n "); <nl> } <nl>  <nl> + if ( ctx -> frame_type >= FRAMETYPE_NULL_FIRST ) <nl> + return buf_size ; <nl> + <nl> if ( ctx -> frame . data [ 0 ]) <nl> avctx -> release_buffer ( avctx , & ctx -> frame ); <nl> 
int ff_h2645_packet_split ( H2645Packet * pkt , const uint8_t * buf , int length , <nl>  <nl> buf += 3 ; <nl> length -= 3 ; <nl> - extract_length = length ; <nl> + extract_length = FFMIN ( length , next_avc - buf ); <nl>  <nl> if ( buf >= next_avc ) { <nl> /* skip to the start of the next NAL */
static int seq_fill_buffer ( SeqDemuxContext * seq , ByteIOContext * pb , int buffer_n <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> seq_buffer = & seq -> frame_buffers [ buffer_num ]; <nl> - if ( seq_buffer -> fill_size + data_size > seq_buffer -> data_size ) <nl> + if ( seq_buffer -> fill_size + data_size > seq_buffer -> data_size || data_size <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> url_fseek ( pb , seq -> current_frame_offs + data_offs , SEEK_SET );
int av_strerror ( int errnum , char * errbuf , size_t errbuf_size ) <nl> av_strlcpy ( errbuf , entry -> str , errbuf_size ); <nl> } else { <nl> # if HAVE_STRERROR_R <nl> - ret = strerror_r ( AVUNERROR ( errnum ), errbuf , errbuf_size ); <nl> + ret = AVERROR ( strerror_r ( AVUNERROR ( errnum ), errbuf , errbuf_size )); <nl> # else <nl> ret = - 1 ; <nl> # endif
static int iff_read_packet ( AVFormatContext * s , <nl> buf = pkt -> data ; <nl> bytestream_put_be16 (& buf , 2 ); <nl> ret = avio_read ( pb , buf , iff -> body_size ); <nl> + if ( ret >= 0 && ret < iff -> body_size ) <nl> + av_shrink_packet ( pkt , ret + 2 ); <nl> } else { <nl> av_assert0 ( 0 ); <nl> }
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> case XWD_GRAY_SCALE : <nl> if ( bpp != 1 && bpp != 8 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( pixdepth == 1 ) { <nl> + if ( bpp == 1 && pixdepth == 1 ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_MONOWHITE ; <nl> - } else if ( pixdepth == 8 ) { <nl> + } else if ( bpp == 8 && pixdepth == 8 ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_GRAY8 ; <nl> } <nl> break ;
typedef struct WmallDecodeCtx { <nl>  <nl> int8_t mclms_order ; <nl> int8_t mclms_scaling ; <nl> - int16_t mclms_coeffs [ 128 ]; <nl> - int16_t mclms_coeffs_cur [ 4 ]; <nl> + int16_t mclms_coeffs [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS * 32 ]; <nl> + int16_t mclms_coeffs_cur [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS ]; <nl> int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ;
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> S *= 1U << s -> float_shift ; <nl> sign = S < 0 ; <nl> if ( sign ) <nl> - S = - S ; <nl> - if ( S >= 0x1000000 ) { <nl> + S = -( unsigned ) S ; <nl> + if ( S >= 0x1000000U ) { <nl> if ( s -> got_extra_bits && get_bits1 (& s -> gb_extra_bits )) <nl> S = get_bits (& s -> gb_extra_bits , 23 ); <nl> else
static int mxf_get_sorted_table_segments ( MXFContext * mxf , int * nb_sorted_segment <nl> if ( mxf -> metadata_sets [ i ]-> type == IndexTableSegment ) <nl> nb_segments ++; <nl>  <nl> + if (! nb_segments ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> * sorted_segments = av_mallocz ( nb_segments * sizeof (** sorted_segments )); <nl> unsorted_segments = av_mallocz ( nb_segments * sizeof (* unsorted_segments )); <nl> if (! sorted_segments || ! unsorted_segments ) {
static int hls_write_trailer ( struct AVFormatContext * s ) <nl> av_freep (& vs -> baseurl ); <nl> } <nl>  <nl> + ff_format_io_close ( s , & hls -> m3u8_out ); <nl> + ff_format_io_close ( s , & hls -> sub_m3u8_out ); <nl> av_freep (& hls -> key_basename ); <nl> av_freep (& hls -> var_streams ); <nl> av_freep (& hls -> master_m3u8_url );
static int get_pcm ( HEVCContext * s , int x , int y ) <nl>  <nl> # define TC_CALC ( qp , bs ) \ <nl> tctable [ av_clip (( qp ) + DEFAULT_INTRA_TC_OFFSET * (( bs ) - 1 ) + \ <nl> - ( tc_offset >> 1 << 1 ), \ <nl> + ( tc_offset & - 2 ), \ <nl> 0 , MAX_QP + DEFAULT_INTRA_TC_OFFSET )] <nl>  <nl> static void deblocking_filter_CTB ( HEVCContext * s , int x0 , int y0 )
static int encode_apng ( AVCodecContext * avctx , AVPacket * pkt , <nl> return AVERROR ( ENOMEM ); <nl>  <nl> if ( avctx -> frame_number == 0 ) { <nl> + if (! pict ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> s -> bytestream = avctx -> extradata = av_malloc ( FF_MIN_BUFFER_SIZE ); <nl> if (! avctx -> extradata ) <nl> return AVERROR ( ENOMEM );
static int mp3_write_xing ( AVFormatContext * s ) <nl> } <nl>  <nl> /* dummy MPEG audio header */ <nl> - header = 0xff << 24 ; // sync <nl> + header = 0xffU << 24 ; // sync <nl> header |= ( 0x7 << 5 | ver << 3 | 0x1 << 1 | 0x1 ) << 16 ; // sync / audio - version / layer 3 / no crc */ <nl> header |= ( srate_idx << 2 ) << 8 ; <nl> header |= channels << 6 ;
static av_cold void rnd_table_init ( void ) { <nl>  <nl> static av_cold void init_noise_samples ( void ) { <nl> int i ; <nl> - int random_seed = 0 ; <nl> + unsigned random_seed = 0 ; <nl> float delta = 1 . 0 / 16384 . 0 ; <nl> for ( i = 0 ; i < 128 ; i ++) { <nl> random_seed = random_seed * 214013 + 2531011 ;
int ff_rtsp_open_transport_ctx ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl>  <nl> if (! rtsp_st -> transport_priv ) { <nl> return AVERROR ( ENOMEM ); <nl> - } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP ) { <nl> + } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP && <nl> + s -> iformat ) { <nl> RTPDemuxContext * rtpctx = rtsp_st -> transport_priv ; <nl> rtpctx -> ssrc = rtsp_st -> ssrc ; <nl> if ( rtsp_st -> dynamic_handler ) {
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i ); <nl> + if ( end - dst < count ) { <nl> + av_log ( l -> avctx , AV_LOG_ERROR , " Too many zeros remaining .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> memset ( dst , 0 , count ); <nl> l -> zeros_rem -= count ; <nl> dst += count ;
av_cold void ff_msmpeg4_encode_init ( MpegEncContext * s ) <nl>  <nl> for ( i = 0 ; i < NB_RL_TABLES ; i ++){ <nl> int level ; <nl> - for ( level = 0 ; level <= MAX_LEVEL ; level ++){ <nl> + for ( level = 1 ; level <= MAX_LEVEL ; level ++) { <nl> int run ; <nl> for ( run = 0 ; run <= MAX_RUN ; run ++){ <nl> int last ;
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl> vseq -> seq_fields . bits . direct_8x8_inference_flag = 1 ; <nl> vseq -> seq_fields . bits . log2_max_frame_num_minus4 = 4 ; <nl> vseq -> seq_fields . bits . pic_order_cnt_type = 0 ; <nl> + vseq -> seq_fields . bits . log2_max_pic_order_cnt_lsb_minus4 = <nl> + av_clip ( av_log2 ( avctx -> max_b_frames + 1 ) - 2 , 0 , 12 ); <nl>  <nl> if ( avctx -> width != ctx -> surface_width || <nl> avctx -> height != ctx -> surface_height ) {
static int poll_filters ( void ) <nl> } <nl> break ; <nl> } <nl> + frame_pts = AV_NOPTS_VALUE ; <nl> if ( ost -> enc -> type == AVMEDIA_TYPE_VIDEO ) <nl> filtered_frame -> pts = frame_pts = av_rescale_q ( picref -> pts , ist_pts_tb , AV_TIME_BASE_Q ); <nl> else if ( picref -> pts != AV_NOPTS_VALUE )
static int config_props ( AVFilterLink * outlink ) <nl>  <nl> scale -> input_is_pal = av_pix_fmt_descriptors [ inlink -> format ]. flags & PIX_FMT_PAL ; <nl>  <nl> + if ( scale -> sws ) <nl> + sws_freeContext ( scale -> sws ); <nl> scale -> sws = sws_getContext ( inlink -> w , inlink -> h , inlink -> format , <nl> outlink -> w , outlink -> h , outlink -> format , <nl> scale -> flags , NULL , NULL , NULL );
void ff_msmpeg4_encode_mb ( MpegEncContext * s , <nl> static void msmpeg4_encode_dc ( MpegEncContext * s , int level , int n , int * dir_ptr ) <nl> { <nl> int sign , code ; <nl> - int pred , extquant ; <nl> + int pred , av_uninit ( extquant ); <nl> int extrabits = 0 ; <nl>  <nl> int16_t * dc_val ;
static int swf_probe ( AVProbeData * p ) <nl> && AV_RB24 ( p -> buf ) != AV_RB24 (" FWS ")) <nl> return 0 ; <nl>  <nl> + if ( AV_RB24 ( p -> buf ) == AV_RB24 (" CWS ") <nl> + && p -> buf [ 3 ] <= 20 ) <nl> + return AVPROBE_SCORE_MAX / 4 + 1 ; <nl> + <nl> init_get_bits8 (& gb , p -> buf + 3 , p -> buf_size - 3 ); <nl>  <nl> skip_bits (& gb , 40 );
static int encode_thread ( AVCodecContext * c , void * arg ){ <nl> int mb_x , mb_y , pdif = 0 ; <nl> int chr_h = 16 >> s -> chroma_y_shift ; <nl> int i , j ; <nl> - MpegEncContext best_s , backup_s ; <nl> + MpegEncContext best_s = { 0 }, backup_s ; <nl> uint8_t bit_buf [ 2 ][ MAX_MB_BYTES ]; <nl> uint8_t bit_buf2 [ 2 ][ MAX_MB_BYTES ]; <nl> uint8_t bit_buf_tex [ 2 ][ MAX_MB_BYTES ];
static av_cold int X264_close ( AVCodecContext * avctx ) <nl> if ( x4 -> enc ) <nl> x264_encoder_close ( x4 -> enc ); <nl>  <nl> - av_free ( x4 -> preset ); <nl> - av_free ( x4 -> tune ); <nl> - av_free ( x4 -> profile ); <nl> - av_free ( x4 -> level ); <nl> - av_free ( x4 -> stats ); <nl> - av_free ( x4 -> weightp ); <nl> - av_free ( x4 -> x264opts ); <nl> - <nl> return 0 ; <nl> } <nl> 
static inline int set_options ( AVFilterContext * ctx , const char * args ) <nl>  <nl> hue -> hue_expr = NULL ; <nl> hue -> hue_deg_expr = NULL ; <nl> + hue -> saturation_expr = NULL ; <nl>  <nl> if (( ret = av_set_options_string ( hue , args , "=", ":")) < 0 ) <nl> return ret ;
static inline void mv_pred_direct ( AVSContext * h , cavs_vector * pmv_fw , <nl> cavs_vector * col_mv ) <nl> { <nl> cavs_vector * pmv_bw = pmv_fw + MV_BWD_OFFS ; <nl> - int den = h -> direct_den [ col_mv -> ref ]; <nl> + unsigned den = h -> direct_den [ col_mv -> ref ]; <nl> int m = FF_SIGNBIT ( col_mv -> x ); <nl>  <nl> pmv_fw -> dist = h -> dist [ 1 ];
static int mov_read_close ( AVFormatContext * s ) <nl> for ( i = 0 ; i < mov -> total_streams ; i ++) <nl> mov_free_stream_context ( mov -> streams [ i ]); <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) <nl> - av_free ( s -> streams [ i ]); <nl> + av_freep (& s -> streams [ i ]); <nl> return 0 ; <nl> } <nl> 
static int asf_write_header ( AVFormatContext * s ) <nl> * It is needed to use asf as a streamable format . */ <nl> if ( asf_write_header1 ( s , 0 , DATA_HEADER_SIZE ) < 0 ) { <nl> // av_free ( asf ); <nl> + av_freep (& asf -> index_ptr ); <nl> return - 1 ; <nl> } <nl> 
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( flv -> wrong_dts ) <nl> dts = AV_NOPTS_VALUE ; <nl> } <nl> - if ( type == 0 ) { <nl> + <nl> + if ( type == 0 && ! st -> codec -> extradata ) { <nl> if (( ret = flv_get_extradata ( s , st , size )) < 0 ) <nl> return ret ; <nl> if ( st -> codec -> codec_id == CODEC_ID_AAC ) {
av_cold int ffv1_common_init ( AVCodecContext * avctx ) <nl>  <nl> s -> picture . f = avcodec_alloc_frame (); <nl> s -> last_picture . f = av_frame_alloc (); <nl> + if (! s -> picture . f || ! s -> last_picture . f ) <nl> + return AVERROR ( ENOMEM ); <nl> ff_dsputil_init (& s -> dsp , avctx ); <nl>  <nl> s -> width = avctx -> width ;
static inline int read_huff_channels ( MLPDecodeContext * m , GetBitContext * gbp , <nl> result = ( result << lsb_bits ) + get_bits ( gbp , lsb_bits ); <nl>  <nl> result += cp -> sign_huff_offset ; <nl> - result <<= quant_step_size ; <nl> + result *= 1 << quant_step_size ; <nl>  <nl> m -> sample_buffer [ pos + s -> blockpos ][ channel ] = result ; <nl> }
static void search_for_quantizers_anmr ( AVCodecContext * avctx , AACEncContext * s , <nl> } <nl> while ( idx ) { <nl> sce -> sf_idx [ bandaddr [ idx ]] = minq + q0 ; <nl> - minq = paths [ idx ][ minq ]. prev ; <nl> + minq = FFMAX ( paths [ idx ][ minq ]. prev , 0 ); <nl> idx --; <nl> } <nl> // set the same quantizers inside window groups
static int mov_write_audio_tag ( AVFormatContext * s , AVIOContext * pb , MOVMuxContex <nl> uint32_t tag = track -> tag ; <nl>  <nl> if ( track -> mode == MODE_MOV ) { <nl> - if ( track -> timescale > UINT16_MAX ) { <nl> + if ( track -> timescale > UINT16_MAX || ! track -> par -> channels ) { <nl> if ( mov_get_lpcm_flags ( track -> par -> codec_id )) <nl> tag = AV_RL32 (" lpcm "); <nl> version = 2 ;
static int escape124_decode_frame ( AVCodecContext * avctx , <nl> // This codebook can be cut off at places other than <nl> // powers of 2 , leaving some of the entries undefined . <nl> cb_size = get_bits_long (& gb , 20 ); <nl> + if (! cb_size ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid codebook size 0 .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> cb_depth = av_log2 ( cb_size - 1 ) + 1 ; <nl> } else { <nl> cb_depth = get_bits (& gb , 4 );
ebml_read_ascii ( MatroskaDemuxContext * matroska , <nl> offset_t pos = url_ftell ( pb ); <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Read error at pos . %" PRIu64 " ( 0x %" PRIx64 ")\ n ", pos , pos ); <nl> + av_free (* str ); <nl> return AVERROR ( EIO ); <nl> } <nl> (* str )[ size ] = '\ 0 ';
static int swf_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> tag = get_swf_tag ( pb , & len ); <nl> if ( tag < 0 ) <nl> return tag ; <nl> + if ( len < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " len % d is invalid \ n ", len ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( tag == TAG_VIDEOSTREAM ) { <nl> int ch_id = avio_rl16 ( pb ); <nl> len -= 2 ;
static void vc1_mc_4mv_chroma4 ( VC1Context * v ) <nl> uvmy_field [ i ] = ( uvmy_field [ i ] & 3 ) << 1 ; <nl>  <nl> if ( fieldmv && !( uvsrc_y & 1 )) <nl> - v_edge_pos --; <nl> + v_edge_pos = ( s -> v_edge_pos >> 1 ) - 1 ; <nl> + <nl> if ( fieldmv && ( uvsrc_y & 1 ) && uvsrc_y < 2 ) <nl> uvsrc_y --; <nl> if (( v -> mv_mode == MV_PMODE_INTENSITY_COMP )
static av_cold void uninit ( AVFilterContext * ctx ) <nl> FrameRateContext * s = ctx -> priv ; <nl> int i ; <nl>  <nl> - for ( i = s -> frst + 1 ; i < s -> last ; i ++) { <nl> + for ( i = s -> frst ; i < s -> last ; i ++) { <nl> if ( s -> srce [ i ] && ( s -> srce [ i ] != s -> srce [ i + 1 ])) <nl> av_frame_free (& s -> srce [ i ]); <nl> }
static inline int dirac_get_arith_uint ( DiracArith * c , int follow_ctx , int data_c <nl> { <nl> int ret = 1 ; <nl> while (! dirac_get_arith_bit ( c , follow_ctx )) { <nl> + if ( ret >= 0x40000000 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " dirac_get_arith_uint overflow \ n "); <nl> + return - 1 ; <nl> + } <nl> ret <<= 1 ; <nl> ret += dirac_get_arith_bit ( c , data_ctx ); <nl> follow_ctx = ff_dirac_next_ctx [ follow_ctx ];
static int roq_read_packet ( AVFormatContext * s , <nl> pkt -> pos = avio_tell ( pb ); <nl> ret = avio_read ( pb , pkt -> data + RoQ_CHUNK_PREAMBLE_SIZE , <nl> chunk_size ); <nl> - if ( ret != chunk_size ) <nl> + if ( ret != chunk_size ) { <nl> + av_packet_unref ( pkt ); <nl> ret = AVERROR ( EIO ); <nl> + } <nl>  <nl> packet_read = 1 ; <nl> break ;
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> goto fail ; <nl> } <nl>  <nl> - if ( reply -> timeout > 0 ) <nl> + if ( rt -> nb_rtsp_streams && reply -> timeout > 0 ) <nl> rt -> timeout = reply -> timeout ; <nl>  <nl> if ( rt -> server_type == RTSP_SERVER_REAL )
int ff_rtsp_open_transport_ctx ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl>  <nl> if (! rtsp_st -> transport_priv ) { <nl> return AVERROR ( ENOMEM ); <nl> - } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP ) { <nl> + } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP && <nl> + s -> iformat ) { <nl> RTPDemuxContext * rtpctx = rtsp_st -> transport_priv ; <nl> rtpctx -> ssrc = rtsp_st -> ssrc ; <nl> if ( rtsp_st -> dynamic_handler ) {
static av_cold int amr_nb_encode_init ( AVCodecContext * avctx ) <nl>  <nl> avctx -> frame_size = 160 ; <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (! avctx -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> s -> enc_state = Encoder_Interface_init ( s -> enc_dtx ); <nl> if (! s -> enc_state ) {
reload : <nl> c -> end_of_segment = 1 ; <nl> c -> cur_seq_no = v -> cur_seq_no ; <nl>  <nl> - if ( v -> ctx ) { <nl> + if ( v -> ctx && v -> ctx -> nb_streams ) { <nl> v -> needed = 0 ; <nl> for ( i = v -> stream_offset ; i < v -> stream_offset + v -> ctx -> nb_streams ; <nl> i ++) {
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + if ( FAILED ( hr )) { <nl> + IDirect3D9Ex_Release ( d3d9ex ); <nl> + return AVERROR_UNKNOWN ; <nl> + } <nl>  <nl> d3dpp . BackBufferFormat = modeex . Format ; <nl> 
static int rm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> flags = ( seq ++ == 1 ) ? 2 : 0 ; <nl> } else { <nl> len = sync ( s , & timestamp , & flags , & i , & pos ); <nl> - st = s -> streams [ i ]; <nl> + if ( len > 0 ) <nl> + st = s -> streams [ i ]; <nl> } <nl>  <nl> if ( len < 0 || url_feof ( s -> pb ))
int ff_wma_end ( AVCodecContext * avctx ) <nl> free_vlc (& s -> coef_vlc [ i ]); <nl> av_free ( s -> run_table [ i ]); <nl> av_free ( s -> level_table [ i ]); <nl> + av_free ( s -> int_table [ i ]); <nl> } <nl>  <nl> return 0 ;
int main ( int argc , char ** argv ){ <nl>  <nl> selfTest ( src , stride , W , H ); <nl>  <nl> - return 123 ; <nl> + return 0 ; <nl> }
static int read_channel_params ( MLPDecodeContext * m , unsigned int substr , <nl>  <nl> if ( cp -> huff_lsbs > 24 ) { <nl> av_log ( m -> avctx , AV_LOG_ERROR , " Invalid huff_lsbs .\ n "); <nl> + cp -> huff_lsbs = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
typedef struct PanContext { <nl> static int parse_channel_name ( char ** arg , int * rchannel , int * rnamed ) <nl> { <nl> char buf [ 8 ]; <nl> - int len , i , channel_id ; <nl> + int len , i , channel_id = 0 ; <nl> int64_t layout , layout0 ; <nl>  <nl> if ( sscanf (* arg , " % 7 [ A - Z ] % n ", buf , & len )) {
enum dirac_subband { <nl> /* magic number division by 3 from schroedinger */ <nl> static inline int divide3 ( int x ) <nl> { <nl> - return (( x + 1 )* 21845 + 10922 ) >> 16 ; <nl> + return ( int )(( x + 1U )* 21845 + 10922 ) >> 16 ; <nl> } <nl>  <nl> static DiracFrame * remove_frame ( DiracFrame * framelist [], int picnum )
void avcodec_align_dimensions2 ( AVCodecContext * s , int * width , int * height , <nl> case AV_PIX_FMT_YUVJ411P : <nl> case AV_PIX_FMT_UYYVYY411 : <nl> w_align = 32 ; <nl> - h_align = 8 ; <nl> + h_align = 16 * 2 ; <nl> break ; <nl> case AV_PIX_FMT_YUV410P : <nl> if ( s -> codec_id == AV_CODEC_ID_SVQ1 ) {
static int64_t mxf_essence_container_end ( MXFContext * mxf , int body_sid ) <nl> static int mxf_edit_unit_absolute_offset ( MXFContext * mxf , MXFIndexTable * index_table , int64_t edit_unit , int64_t * edit_unit_out , int64_t * offset_out , int nag ) <nl> { <nl> int i ; <nl> - int offset_temp = 0 ; <nl> + int64_t offset_temp = 0 ; <nl>  <nl> for ( i = 0 ; i < index_table -> nb_segments ; i ++) { <nl> MXFIndexTableSegment * s = index_table -> segments [ i ];
static int tiff_decode_tag ( TiffContext * s ) <nl> " Samples per pixel requires a single value , many provided \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( value > 4U ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , <nl> + " Samples per pixel % d is too large \ n ", value ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( s -> bppcount == 1 ) <nl> s -> bpp *= value ; <nl> s -> bppcount = value ;
static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> av_buffer_realloc ( buf , size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> - if (! buf ) <nl> + if (!* buf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> memset ((* buf )-> data + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE );
static av_cold int dnxhd_decode_init ( AVCodecContext * avctx ) <nl>  <nl> ctx -> avctx = avctx ; <nl> ctx -> cid = - 1 ; <nl> - avctx -> colorspace = AVCOL_SPC_BT709 ; <nl> + if ( avctx -> colorspace == AVCOL_SPC_UNSPECIFIED ) { <nl> + avctx -> colorspace = AVCOL_SPC_BT709 ; <nl> + } <nl>  <nl> avctx -> coded_width = FFALIGN ( avctx -> width , 16 ); <nl> avctx -> coded_height = FFALIGN ( avctx -> height , 16 );
typedef struct TargaContext { <nl> } TargaContext ; <nl>  <nl> # define CHECK_BUFFER_SIZE ( buf , buf_end , needed , where ) \ <nl> - if ( buf + needed > buf_end ){ \ <nl> + if ( needed > buf_end - buf ){ \ <nl> av_log ( avctx , AV_LOG_ERROR , " Problem : unexpected end of data while reading " where "\ n "); \ <nl> return - 1 ; \ <nl> } \
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> avio_seek ( s -> pb , st -> index_entries [ st -> nb_index_entries - 1 ]. pos , SEEK_SET ); <nl> matroska -> current_id = 0 ; <nl> while (( index = av_index_search_timestamp ( st , timestamp , flags )) < 0 ) { <nl> + matroska -> prev_pkt = NULL ; <nl> matroska_clear_queue ( matroska ); <nl> if ( matroska_parse_cluster ( matroska ) < 0 ) <nl> break ;
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> output = s -> plane [ plane ]. subband [ 0 ]; <nl> for ( i = 0 ; i < lowpass_height * 2 ; i ++) { <nl> for ( j = 0 ; j < lowpass_width * 2 ; j ++) <nl> - output [ j ] <<= 2 ; <nl> + output [ j ] *= 4 ; <nl>  <nl> output += lowpass_width * 2 ; <nl> }
static int build_table ( VLC * vlc , int table_nb_bits , int nb_codes , <nl> /* note : realloc has been done , so reload tables */ <nl> table = & vlc -> table [ table_index ]; <nl> table [ j ][ 0 ] = index ; // code <nl> + av_assert0 ( table [ j ][ 0 ] == index ); <nl> i = k - 1 ; <nl> } <nl> }
static int decorrelate ( TAKDecContext * s , int c1 , int c2 , int length ) <nl> s -> residues [ i ] * s -> filter [ 0 ]; <nl> } <nl>  <nl> - v = ( av_clip_intp2 ( v >> 10 , 13 ) << dshift ) - * p1 ; <nl> + v = av_clip_intp2 ( v >> 10 , 13 ) * ( 1 << dshift ) - * p1 ; <nl> * p1 ++ = v ; <nl> } <nl> 
static int xbm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int number , len ; <nl>  <nl> ptr += strcspn ( ptr , "#"); <nl> - if ( sscanf ( ptr , "# define % 256s % u ", name , & number ) != 2 ) { <nl> + if ( sscanf ( ptr , "# define % 255s % u ", name , & number ) != 2 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unexpected preprocessor directive \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int hls_write_trailer ( struct AVFormatContext * s ) <nl> av_write_trailer ( oc ); <nl> hls -> size = avio_tell ( hls -> avf -> pb ) - hls -> start_pos ; <nl> avio_closep (& oc -> pb ); <nl> - avformat_free_context ( oc ); <nl> av_free ( hls -> basename ); <nl> hls_append_segment ( hls , hls -> duration , hls -> start_pos , hls -> size ); <nl> + avformat_free_context ( oc ); <nl> + hls -> avf = NULL ; <nl> hls_window ( s , 1 ); <nl>  <nl> hls_free_segments ( hls );
static int find_start_code ( const uint8_t * buf , int buf_size , <nl> buf [ buf_index + 2 ] == 1 ) <nl> break ; <nl>  <nl> - if ( buf_index + 3 >= buf_size ) <nl> + buf_index += 3 ; <nl> + <nl> + if ( buf_index >= buf_size ) <nl> return buf_size ; <nl>  <nl> - return buf_index + 3 ; <nl> + return buf_index ; <nl> } <nl>  <nl> static int get_avc_nalsize ( H264Context * h , const uint8_t * buf ,
static void cdxl_decode_rgb ( CDXLVideoContext * c ) <nl> { <nl> uint32_t * new_palette = ( uint32_t *) c -> frame . data [ 1 ]; <nl>  <nl> + memset ( c -> frame . data [ 1 ], 0 , AVPALETTE_SIZE ); <nl> import_palette ( c , new_palette ); <nl> import_format ( c , c -> frame . linesize [ 0 ], c -> frame . data [ 0 ]); <nl> }
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & gbc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
int ff_mp4_read_dec_config_descr ( AVFormatContext * fc , AVStream * st , AVIOContext <nl> return AVERROR ( ENOMEM ); <nl> avio_read ( pb , st -> codec -> extradata , len ); <nl> if ( st -> codec -> codec_id == AV_CODEC_ID_AAC ) { <nl> - MPEG4AudioConfig cfg ; <nl> + MPEG4AudioConfig cfg = { 0 }; <nl> avpriv_mpeg4audio_get_config (& cfg , st -> codec -> extradata , <nl> st -> codec -> extradata_size * 8 , 1 ); <nl> st -> codec -> channels = cfg . channels ;
static int decode_frame_mp3on4 ( AVCodecContext * avctx , void * data , <nl>  <nl> avpriv_mpegaudio_decode_header (( MPADecodeHeader *) m , header ); <nl>  <nl> - if ( ch + m -> nb_channels > avctx -> channels ) { <nl> + if ( ch + m -> nb_channels > avctx -> channels || s -> coff [ fr ] + m -> nb_channels > avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame channel count exceeds codec " <nl> " channel count \ n "); <nl> return AVERROR_INVALIDDATA ;
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> if ( ast -> sub_packet_size <= 0 || <nl> ast -> sub_packet_size > ast -> audio_framesize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( ast -> audio_framesize % ast -> sub_packet_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> break ; <nl> case DEINT_ID_SIPR : <nl> case DEINT_ID_INT0 :
static int fraps2_decode_plane ( FrapsContext * s , uint8_t * dst , int stride , int w , <nl> */ <nl> if ( j ) dst [ i ] += dst [ i - stride ]; <nl> else if ( Uoff ) dst [ i ] += 0x80 ; <nl> + if ( get_bits_left (& gb ) < 0 ) { <nl> + free_vlc (& vlc ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl> dst += stride ; <nl> }
 <nl> # include < stdarg . h > <nl> # include " avcodec . h " <nl> -# include " movtext . h " <nl> # include " libavutil / avstring . h " <nl> +# include " libavutil / intreadwrite . h " <nl> # include " ass_split . h " <nl> # include " ass . h " <nl> 
static void add_input_streams ( OptionsContext * o , AVFormatContext * ic ) <nl> if (! ist -> hwaccel_device ) <nl> exit_program ( 1 ); <nl> } <nl> + ist -> hwaccel_pix_fmt = AV_PIX_FMT_NONE ; <nl>  <nl> break ; <nl> case AVMEDIA_TYPE_AUDIO :
int av_open_input_file ( AVFormatContext ** ic_ptr , const char * filename , <nl> int err ; <nl> AVDictionary * opts = convert_format_parameters ( ap ); <nl>  <nl> - if (! ap -> prealloced_context ) <nl> + if (! ap || ! ap -> prealloced_context ) <nl> * ic_ptr = NULL ; <nl>  <nl> err = avformat_open_input ( ic_ptr , filename , fmt , & opts );
static int read_extra_header ( FFV1Context * f ) <nl> f -> quant_table_count = get_symbol ( c , state , 0 ); <nl> if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES || ! f -> quant_table_count ) { <nl> av_log ( f -> avctx , AV_LOG_ERROR , " quant table count % d is invalid \ n ", f -> quant_table_count ); <nl> + f -> quant_table_count = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int ivr_probe ( AVProbeData * p ) <nl> static int ivr_read_header ( AVFormatContext * s ) <nl> { <nl> unsigned tag , type , len , tlen , value ; <nl> - int i , j , n , count , nb_streams , ret ; <nl> + int i , j , n , count , nb_streams = 0 , ret ; <nl> uint8_t key [ 256 ], val [ 256 ]; <nl> AVIOContext * pb = s -> pb ; <nl> AVStream * st ;
static int mov_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> /* copy frame to create needed atoms */ <nl> trk -> vosLen = size ; <nl> trk -> vosData = av_malloc ( size ); <nl> + if (! trk -> vosData ) <nl> + return AVERROR ( ENOMEM ); <nl> memcpy ( trk -> vosData , pkt -> data , size ); <nl> } <nl> 
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> output = s -> plane [ plane ]. subband [ 0 ]; <nl> for ( i = 0 ; i < lowpass_height * 2 ; i ++) { <nl> for ( j = 0 ; j < lowpass_width * 2 ; j ++) <nl> - output [ j ] <<= 2 ; <nl> + output [ j ] *= 4 ; <nl>  <nl> output += lowpass_width * 2 ; <nl> }
int av_add_index_entry ( AVStream * st , <nl> memmove ( entries + index + 1 , entries + index , sizeof ( AVIndexEntry )*( st -> nb_index_entries - index )); <nl> } <nl> st -> nb_index_entries ++; <nl> + } else { <nl> + if ( ie -> pos == pos && distance < ie -> min_distance ) // dont reduce the distance <nl> + distance = ie -> min_distance ; <nl> } <nl> } else { <nl> index = st -> nb_index_entries ++;
static int unpack_bitstream ( G723_1_Context * p , const uint8_t * buf , <nl> /** <nl> * Bitexact implementation of sqrt ( val / 2 ). <nl> */ <nl> - static int16_t square_root ( int val ) <nl> + static int16_t square_root ( unsigned val ) <nl> { <nl> + av_assert2 (!( val & 0x80000000 )); <nl> + <nl> return ( ff_sqrt ( val << 1 ) >> 1 ) & (~ 1 ); <nl> } <nl> 
struct vfw_ctx { <nl> static enum PixelFormat vfw_pixfmt ( DWORD biCompression , WORD biBitCount ) <nl> { <nl> switch ( biCompression ) { <nl> + case MKTAG (' U ', ' Y ', ' V ', ' Y '): <nl> + return PIX_FMT_UYVY422 ; <nl> case MKTAG (' Y ', ' U ', ' Y ', ' 2 '): <nl> return PIX_FMT_YUYV422 ; <nl> case MKTAG (' I ', ' 4 ', ' 2 ', ' 0 '):
typedef struct SmcContext { <nl> row_ptr += stride * 4 ; \ <nl> } \ <nl> total_blocks --; \ <nl> - if ( total_blocks < 0 ) \ <nl> + if ( total_blocks < !! n_blocks ) \ <nl> { \ <nl> av_log ( s -> avctx , AV_LOG_INFO , " warning : block counter just went negative ( this should not happen )\ n "); \ <nl> return ; \
static int start_frame_overlay ( AVFilterLink * inlink , AVFilterBufferRef * inpicref <nl> OverlayContext * over = ctx -> priv ; <nl>  <nl> inlink -> cur_buf = NULL ; <nl> + avfilter_unref_bufferp (& over -> overpicref ); <nl> over -> overpicref = inpicref ; <nl> over -> overpicref -> pts = av_rescale_q ( inpicref -> pts , ctx -> inputs [ OVERLAY ]-> time_base , <nl> ctx -> outputs [ 0 ]-> time_base );
void ff_ivi_output_plane ( IVIPlaneDesc * plane , uint8_t * dst , int dst_pitch ) <nl> const int16_t * src = plane -> bands [ 0 ]. buf ; <nl> uint32_t pitch = plane -> bands [ 0 ]. pitch ; <nl>  <nl> + if (! src ) <nl> + return ; <nl> + <nl> for ( y = 0 ; y < plane -> height ; y ++) { <nl> for ( x = 0 ; x < plane -> width ; x ++) <nl> dst [ x ] = av_clip_uint8 ( src [ x ] + 128 );
static void print_report ( int is_last_report , int64_t timer_start , int64_t cur_ti <nl> AVCodecContext * enc ; <nl> int frame_number , vid , i ; <nl> double bitrate ; <nl> - int64_t pts = INT64_MIN ; <nl> + int64_t pts = INT64_MIN + 1 ; <nl> static int64_t last_time = - 1 ; <nl> static int qp_histogram [ 52 ]; <nl> int hours , mins , secs , us ;
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl>  <nl> if ( s -> v_max == 1 && s -> h_max == 1 && s -> lossless == 1 && nb_components == 3 ) <nl> s -> rgb = 1 ; <nl> + else if (! s -> lossless ) <nl> + s -> rgb = 0 ; <nl>  <nl> /* if different size , realloc / alloc picture */ <nl> if ( width != s -> width || height != s -> height
static int flashsv_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> if ( has_diff ) { <nl> + if (! s -> keyframe ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " inter frame without keyframe \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> diff_start = get_bits (& gb , 8 ); <nl> s -> diff_height = get_bits (& gb , 8 ); <nl> av_log ( avctx , AV_LOG_DEBUG ,
retry : <nl> /* If we need to reload the playlist again below ( if <nl> * there ' s still no more segments ), switch to a reload <nl> * interval of half the target duration . */ <nl> - reload_interval = s -> target_duration * 500000 ; <nl> + reload_interval = s -> target_duration * 500000LL ; <nl> } <nl> } <nl> if ( s -> cur_seq_no < s -> start_seq_no ) {
static inline int ape_decode_value_3900 ( APEContext * ctx , APERice * rice ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> x = range_decode_bits ( ctx , tmpk ); <nl> - } else if ( tmpk <= 32 ) { <nl> + } else if ( tmpk <= 31 ) { <nl> x = range_decode_bits ( ctx , 16 ); <nl> x |= ( range_decode_bits ( ctx , tmpk - 16 ) << 16 ); <nl> } else {
static int mov_read_stts ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> if ( entries >= UINT_MAX / sizeof (* sc -> stts_data )) <nl> return AVERROR ( EINVAL ); <nl>  <nl> + av_free ( sc -> stts_data ); <nl> sc -> stts_data = av_malloc ( entries * sizeof (* sc -> stts_data )); <nl> if (! sc -> stts_data ) <nl> return AVERROR ( ENOMEM );
static const QCELPBitmap qcelp_rate_octave_bitmap [] = { <nl> QCELP_OF ( lspv [ 8 ], 0 , 1 ), // 8 <nl> QCELP_OF ( cbsign [ 15 ], 0 , 1 ), // 7 <nl> QCELP_OF ( lspv [ 9 ], 0 , 1 ), // 6 <nl> - QCELP_OF ( cbgain [ 0 ], 0 , 2 ), // 7 <nl> + QCELP_OF ( cbgain [ 0 ], 0 , 2 ), // 5 <nl> QCELP_OF ( reserved , 0 , 4 ) // 3 <nl> }; <nl> 
static void picmemset ( PicContext * s , AVFrame * frame , int value , int run , <nl> if (* y < 0 ) { <nl> * y = s -> height - 1 ; <nl> * plane += 1 ; <nl> - value <<= bits_per_plane ; <nl> - mask <<= bits_per_plane ; <nl> if (* plane >= s -> nb_planes ) <nl> return ; <nl> + value <<= bits_per_plane ; <nl> + mask <<= bits_per_plane ; <nl> } <nl> } <nl> }
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> if ( s -> oformat -> init && ( ret = s -> oformat -> init ( s )) < 0 ) { <nl> - s -> oformat -> deinit ( s ); <nl> + if ( s -> oformat -> deinit ) <nl> + s -> oformat -> deinit ( s ); <nl> goto fail ; <nl> } <nl> 
static int decode_ics_info ( AACContext * ac , IndividualChannelStream * ics , <nl> if ( aot != AOT_ER_AAC_ELD ) { <nl> if ( get_bits1 ( gb )) { <nl> av_log ( ac -> avctx , AV_LOG_ERROR , " Reserved bit set .\ n "); <nl> - return AVERROR_INVALIDDATA ; <nl> + if ( ac -> avctx -> err_recognition & AV_EF_BITSTREAM ) <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl> ics -> window_sequence [ 1 ] = ics -> window_sequence [ 0 ]; <nl> ics -> window_sequence [ 0 ] = get_bits ( gb , 2 );
static AVFilterContext * create_filter_with_args ( const char * filt , void * opaque ) <nl> av_log ( NULL , AV_LOG_ERROR , <nl> " error creating filter \"% s \" with args \"% s \"\ n ", <nl> name , args ? args : "( none )"); <nl> - return NULL ; <nl> } <nl>  <nl> av_free ( filter );
static void read_tree ( GetBitContext * gb , Tree * tree ) <nl> tree -> syms [ i ] = get_bits ( gb , 4 ); <nl> tmp1 [ tree -> syms [ i ]] = 1 ; <nl> } <nl> - for ( i = 0 ; i < 16 ; i ++) <nl> + for ( i = 0 ; i < 16 && len < 16 - 1 ; i ++) <nl> if (! tmp1 [ i ]) <nl> tree -> syms [++ len ] = i ; <nl> } else {
# define _PATH_MAN "/ usr / share / man " <nl> # define _PATH_MEM "/ dev / mem " <nl> # define _PATH_NOLOGIN "/ etc / nologin " <nl> +# define _PATH_RSH "/ usr / bin / rsh " <nl> # define _PATH_SENDMAIL "/ usr / sbin / sendmail " <nl> # define _PATH_SHELLS "/ etc / shells " <nl> # define _PATH_TTY "/ dev / tty "
static int uhci_handle_td ( UHCIState * s , uint32_t addr , UHCI_TD * td , uint32_t * in <nl>  <nl> /* Allocate new packet */ <nl> async = uhci_async_alloc ( uhci_queue_get ( s , td ), addr ); <nl> - if (! async ) <nl> - return TD_RESULT_NEXT_QH ; <nl>  <nl> /* valid needs to be large enough to handle 10 frame delay <nl> * for initial isochronous requests
int unix_listen_opts ( QemuOpts * opts , Error ** errp ) <nl> qemu_opt_set ( opts , " path ", un . sun_path , & error_abort ); <nl> } <nl>  <nl> - if (( access ( un . sun_path , F_OK ) == 0 ) && <nl> - unlink ( un . sun_path ) < 0 ) { <nl> + if ( unlink ( un . sun_path ) < 0 && errno != ENOENT ) { <nl> error_setg_errno ( errp , errno , <nl> " Failed to unlink socket % s ", un . sun_path ); <nl> goto err ;
int qcow2_grow_l1_table ( BlockDriverState * bs , uint64_t min_size , <nl> } <nl> } <nl>  <nl> - if ( new_l1_size > INT_MAX ) { <nl> + if ( new_l1_size > INT_MAX / sizeof ( uint64_t )) { <nl> return - EFBIG ; <nl> } <nl> 
static void usb_msd_realize_bot ( USBDevice * dev , Error ** errp ) <nl> usb_desc_init ( dev ); <nl> scsi_bus_new (& s -> bus , sizeof ( s -> bus ), DEVICE ( dev ), <nl> & usb_msd_scsi_info_bot , NULL ); <nl> - s -> bus . qbus . allow_hotplug = 0 ; <nl> usb_msd_handle_reset ( dev ); <nl> } <nl> 
int qemu_acl_remove ( qemu_acl * acl , <nl> i ++; <nl> if ( strcmp ( entry -> match , match ) == 0 ) { <nl> QTAILQ_REMOVE (& acl -> entries , entry , next ); <nl> + acl -> nentries --; <nl> + g_free ( entry -> match ); <nl> + g_free ( entry ); <nl> return i ; <nl> } <nl> }
static uint64_t <nl> e1000e_io_read ( void * opaque , hwaddr addr , unsigned size ) <nl> { <nl> E1000EState * s = opaque ; <nl> - uint32_t idx ; <nl> + uint32_t idx = 0 ; <nl> uint64_t val ; <nl>  <nl> switch ( addr ) { <nl> e1000e_io_write ( void * opaque , hwaddr addr , <nl> uint64_t val , unsigned size ) <nl> { <nl> E1000EState * s = opaque ; <nl> - uint32_t idx ; <nl> + uint32_t idx = 0 ; <nl>  <nl> switch ( addr ) { <nl> case E1000_IOADDR :
static int blk_root_inactivate ( BdrvChild * child ) <nl> * this point because the VM is stopped ) and unattached monitor - owned <nl> * BlockBackends . If there is still any other user like a block job , then <nl> * we simply can ' t inactivate the image . */ <nl> - if (! blk -> dev && ! blk -> name [ 0 ]) { <nl> + if (! blk -> dev && ! blk_name ( blk )[ 0 ]) { <nl> return - EPERM ; <nl> } <nl> 
void qemu_opts_del ( QemuOpts * opts ) <nl> { <nl> QemuOpt * opt ; <nl>  <nl> + if ( opts == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> for (;;) { <nl> opt = QTAILQ_FIRST (& opts -> head ); <nl> if ( opt == NULL )
static void make_dirty ( uint8_t device ) <nl>  <nl> guest_buf = guest_alloc ( guest_malloc , len ); <nl> buf = g_malloc ( len ); <nl> + memset ( buf , rand () % 255 + 1 , len ); <nl> g_assert ( guest_buf ); <nl> g_assert ( buf ); <nl> 
static void realize ( DeviceState * d , Error ** errp ) <nl> error_free ( err ); <nl> object_unref ( OBJECT ( drc )); <nl> } <nl> + g_free ( child_name ); <nl> DPRINTFN (" drc realize complete "); <nl> } <nl> 
static void vhost_user_cleanup ( NetClientState * nc ) <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> s -> vhost_net = NULL ; <nl> } <nl> + if ( s -> chr ) { <nl> + qemu_chr_add_handlers ( s -> chr , NULL , NULL , NULL , NULL ); <nl> + qemu_chr_fe_release ( s -> chr ); <nl> + s -> chr = NULL ; <nl> + } <nl>  <nl> qemu_purge_queued_packets ( nc ); <nl> }
static void virtio_net_set_status ( struct VirtIODevice * vdev , uint8_t status ) <nl> qemu_bh_cancel ( q -> tx_bh ); <nl> } <nl> if (( n -> status & VIRTIO_NET_S_LINK_UP ) == 0 && <nl> - ( queue_status & VIRTIO_CONFIG_S_DRIVER_OK )) { <nl> + ( queue_status & VIRTIO_CONFIG_S_DRIVER_OK ) && <nl> + vdev -> vm_running ) { <nl> /* if tx is waiting we are likely have some packets in tx queue <nl> * and disabled notification */ <nl> q -> tx_waiting = 0 ;
int main ( int argc , char ** argv ) <nl>  <nl> process_requests ( sock ); <nl> error : <nl> + g_free ( rpath ); <nl> + g_free ( sock_name ); <nl> do_log ( LOG_INFO , " Done \ n "); <nl> closelog (); <nl> return 0 ;
static CharDriverState * create_eventfd_chr_device ( IVShmemState * s , <nl> int vector ) <nl> { <nl> /* create a event character device based on the passed eventfd */ <nl> - PCIDevice * pdev = PCI_DEVICE ( s ); <nl> int eventfd = event_notifier_get_fd ( n ); <nl> CharDriverState * chr ; <nl>  <nl> - s -> msi_vectors [ vector ]. pdev = pdev ; <nl> - <nl> chr = qemu_chr_open_eventfd ( eventfd ); <nl>  <nl> if ( chr == NULL ) {
BlockDriverAIOCB * dma_bdrv_io ( <nl> dbs -> sg_cur_index = 0 ; <nl> dbs -> sg_cur_byte = 0 ; <nl> dbs -> dir = dir ; <nl> + dbs -> in_cancel = false ; <nl> dbs -> io_func = io_func ; <nl> dbs -> bh = NULL ; <nl> qemu_iovec_init (& dbs -> iov , sg -> nsg );
static int vvfat_write ( BlockDriverState * bs , int64_t sector_num , <nl>  <nl> DLOG ( checkpoint ()); <nl>  <nl> + /* Check if we ' re operating in read - only mode */ <nl> + if ( s -> qcow == NULL ) { <nl> + return - EACCES ; <nl> + } <nl> + <nl> vvfat_close_current_file ( s ); <nl>  <nl> /*
typedef struct SCLPConsole { <nl> /* Return number of bytes that fit into iov buffer */ <nl> static int chr_can_read ( void * opaque ) <nl> { <nl> - int can_read ; <nl> SCLPConsole * scon = opaque ; <nl>  <nl> - can_read = SIZE_BUFFER_VT220 - scon -> iov_data_len ; <nl> - <nl> - return can_read ; <nl> + return scon -> iov ? SIZE_BUFFER_VT220 - scon -> iov_data_len : 0 ; <nl> } <nl>  <nl> /* Receive n bytes from character layer , save in iov buffer ,
static void handle_ti ( ESPState * s ) <nl> { <nl> uint32_t dmalen , minlen ; <nl>  <nl> + if ( s -> dma && ! s -> dma_enabled ) { <nl> + s -> dma_cb = handle_ti ; <nl> + return ; <nl> + } <nl> + <nl> dmalen = s -> rregs [ ESP_TCLO ] | ( s -> rregs [ ESP_TCMID ] << 8 ); <nl> if ( dmalen == 0 ) { <nl> dmalen = 0x10000 ;
int qemu_devtree_add_subnode ( void * fdt , const char * name ) <nl> int retval ; <nl>  <nl> if (! basename ) { <nl> + g_free ( dupname ); <nl> return - 1 ; <nl> } <nl> 
uint16_t pvpanic_port ( void ) <nl> if (! o ) { <nl> return 0 ; <nl> } <nl> - return object_property_get_int ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> + return object_property_get_uint ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> } <nl>  <nl> static Property pvpanic_isa_properties [] = {
int load_elf ( const char * filename , uint64_t (* translate_fn )( void *, uint64_t ), <nl> target_data_order = ELFDATA2LSB ; <nl> } <nl>  <nl> - if ( target_data_order != e_ident [ EI_DATA ]) <nl> - return - 1 ; <nl> + if ( target_data_order != e_ident [ EI_DATA ]) { <nl> + goto fail ; <nl> + } <nl>  <nl> lseek ( fd , 0 , SEEK_SET ); <nl> if ( e_ident [ EI_CLASS ] == ELFCLASS64 ) {
typedef struct RAMBlock { <nl> static inline void * ramblock_ptr ( RAMBlock * block , ram_addr_t offset ) <nl> { <nl> assert ( offset < block -> length ); <nl> + assert ( block -> host ); <nl> return ( char *) block -> host + offset ; <nl> } <nl> 
int qemu_file_rate_limit ( QEMUFile * f ) <nl>  <nl> size_t qemu_file_set_rate_limit ( QEMUFile * f , size_t new_rate ) <nl> { <nl> - if ( f -> set_rate_limit ) <nl> + /* any failed or completed migration keeps its state to allow probing of <nl> + * migration data , but has no associated file anymore */ <nl> + if ( f && f -> set_rate_limit ) <nl> return f -> set_rate_limit ( f -> opaque , new_rate ); <nl>  <nl> return 0 ;
static inline void bitmap_directory_to_be ( uint8_t * dir , size_t size ) <nl>  <nl> static void bitmap_free ( Qcow2Bitmap * bm ) <nl> { <nl> + if ( bm == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> g_free ( bm -> name ); <nl> g_free ( bm ); <nl> }
void acpi_setup ( PcGuestInfo * guest_info ) <nl> return ; <nl> } <nl>  <nl> + if (! acpi_enabled ) { <nl> + ACPI_BUILD_DPRINTF ( 3 , " ACPI disabled . Bailing out .\ n "); <nl> + return ; <nl> + } <nl> + <nl> build_state = g_malloc0 ( sizeof * build_state ); <nl>  <nl> build_state -> guest_info = guest_info ;
void qemu_iovec_destroy ( QEMUIOVector * qiov ) <nl> { <nl> assert ( qiov -> nalloc != - 1 ); <nl>  <nl> + qemu_iovec_reset ( qiov ); <nl> g_free ( qiov -> iov ); <nl> + qiov -> nalloc = 0 ; <nl> + qiov -> iov = NULL ; <nl> } <nl>  <nl> void qemu_iovec_reset ( QEMUIOVector * qiov )
void qbus_free ( BusState * bus ) <nl> QLIST_REMOVE ( bus , sibling ); <nl> bus -> parent -> num_child_bus --; <nl> } <nl> + qemu_free (( void *) bus -> name ); <nl> if ( bus -> qdev_allocated ) { <nl> qemu_free ( bus ); <nl> }
int64_t throttle_compute_wait ( LeakyBucket * bkt ) <nl> /* If the main bucket is not full yet we still have to check the <nl> * burst bucket in order to enforce the burst limit */ <nl> if ( bkt -> burst_length > 1 ) { <nl> + assert ( bkt -> max > 0 ); /* see throttle_is_valid () */ <nl> extra = bkt -> burst_level - burst_bucket_size ; <nl> if ( extra > 0 ) { <nl> return throttle_do_compute_wait ( bkt -> max , extra );
static target_ulong put_tce_emu ( sPAPRTCETable * tcet , target_ulong ioba , <nl> sPAPRTCE * tcep ; <nl>  <nl> if ( ioba >= tcet -> window_size ) { <nl> - hcall_dprintf (" spapr_vio_put_tce on out - of - boards IOBA 0x " <nl> + hcall_dprintf (" spapr_vio_put_tce on out - of - bounds IOBA 0x " <nl> TARGET_FMT_lx "\ n ", ioba ); <nl> return H_PARAMETER ; <nl> }
static void patch_instruction ( VAPICROMState * s , X86CPU * cpu , target_ulong ip ) <nl> CPUX86State * env = & cpu -> env ; <nl> VAPICHandlers * handlers ; <nl> uint8_t opcode [ 2 ]; <nl> - uint32_t imm32 ; <nl> + uint32_t imm32 = 0 ; <nl> target_ulong current_pc = 0 ; <nl> target_ulong current_cs_base = 0 ; <nl> uint32_t current_flags = 0 ;
static void gen_rot_rm_im ( DisasContext * s , int ot , int op1 , int op2 , <nl> if ( is_right ) { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask - 1 ); <nl> tcg_gen_shri_tl ( cpu_cc_dst , cpu_T [ 0 ], mask ); <nl> + tcg_gen_andi_tl ( cpu_cc_dst , cpu_cc_dst , 1 ); <nl> } else { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask ); <nl> tcg_gen_andi_tl ( cpu_cc_dst , cpu_T [ 0 ], 1 );
static void pc87312_class_init ( ObjectClass * klass , void * data ) <nl> dc -> reset = pc87312_reset ; <nl> dc -> vmsd = & vmstate_pc87312 ; <nl> dc -> props = pc87312_properties ; <nl> + /* Reason : Uses parallel_hds [ 0 ] in realize (), so it can ' t be used twice */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo pc87312_type_info = {
int main ( int argc , char ** argv ) <nl> " - device ipmi - bmc - extern , chardev = ipmi0 , id = bmc0 " <nl> " - device isa - ipmi - bt , bmc = bmc0 ", emu_port ); <nl> qtest_start ( cmdline ); <nl> + g_free ( cmdline ); <nl> qtest_irq_intercept_in ( global_qtest , " ioapic "); <nl> qtest_add_func ("/ ipmi / extern / connect ", test_connect ); <nl> qtest_add_func ("/ ipmi / extern / bt_base ", test_bt_base );
static int cpu_x86_find_by_name ( x86_def_t * x86_cpu_def , const char * cpu_model ) <nl>  <nl> tsc_freq = strtosz_suffix_unit ( val , & err , <nl> STRTOSZ_DEFSUFFIX_B , 1000 ); <nl> - if (!* val || * err ) { <nl> + if ( tsc_freq < 0 || * err ) { <nl> fprintf ( stderr , " bad numerical value % s \ n ", val ); <nl> goto error ; <nl> }
int load_image_targphys ( const char * filename , <nl> int size ; <nl>  <nl> size = get_image_size ( filename ); <nl> - if ( size > 0 ) <nl> + if ( size > max_sz ) { <nl> + return - 1 ; <nl> + } <nl> + if ( size > 0 ) { <nl> rom_add_file_fixed ( filename , addr , - 1 ); <nl> + } <nl> return size ; <nl> } <nl> 
static void aw_a10_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = aw_a10_realize ; <nl> + /* Reason : Uses serial_hds in realize and nd_table in instance_init */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo aw_a10_type_info = {
int spapr_populate_pci_devices ( sPAPRPHBState * phb , <nl> uint64_t child ; <nl> uint64_t parent ; <nl> uint64_t size ; <nl> - } __attribute__ (( packed )) ranges [] = { <nl> + } QEMU_PACKED ranges [] = { <nl> { <nl> cpu_to_be32 ( b_ss ( 1 )), cpu_to_be64 ( 0 ), <nl> cpu_to_be64 ( phb -> io_win_addr ),
uint32_t lm4549_write_samples ( lm4549_state * s , uint32_t left , uint32_t right ) <nl> This model supports 16 - bit playback . <nl> */ <nl>  <nl> - if ( s -> buffer_level >= LM4549_BUFFER_SIZE ) { <nl> + if ( s -> buffer_level > LM4549_BUFFER_SIZE - 2 ) { <nl> DPRINTF (" write_sample Buffer full \ n "); <nl> return 0 ; <nl> }
void * virtqueue_pop ( VirtQueue * vq , size_t sz ) <nl>  <nl> max = vq -> vring . num ; <nl>  <nl> + if ( vq -> inuse >= vq -> vring . num ) { <nl> + error_report (" Virtqueue size exceeded "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> i = head = virtqueue_get_head ( vq , vq -> last_avail_idx ++); <nl> if ( virtio_vdev_has_feature ( vdev , VIRTIO_RING_F_EVENT_IDX )) { <nl> vring_set_avail_event ( vq , vq -> last_avail_idx );
static void acpi_dsdt_add_cpus ( Aml * scope , int smp_cpus ) <nl> uint16_t i ; <nl>  <nl> for ( i = 0 ; i < smp_cpus ; i ++) { <nl> - Aml * dev = aml_device (" C % 03x ", i ); <nl> + Aml * dev = aml_device (" C %. 03X ", i ); <nl> aml_append ( dev , aml_name_decl (" _HID ", aml_string (" ACPI0007 "))); <nl> aml_append ( dev , aml_name_decl (" _UID ", aml_int ( i ))); <nl> aml_append ( scope , dev );
qemu_irq * i8259_init ( ISABus * bus , qemu_irq parent_irq ) <nl> ISADevice * isadev ; <nl> int i ; <nl>  <nl> - irq_set = g_malloc ( ISA_NUM_IRQS * sizeof ( qemu_irq )); <nl> + irq_set = g_new0 ( qemu_irq , ISA_NUM_IRQS ); <nl>  <nl> isadev = i8259_init_chip ( TYPE_I8259 , bus , true ); <nl> dev = DEVICE ( isadev );
static void external_snapshot_prepare ( BlkTransactionState * common , <nl> return ; <nl> } <nl>  <nl> - if ( has_snapshot_node_name && bdrv_find_node ( snapshot_node_name )) { <nl> - error_setg ( errp , " New snapshot node name already existing "); <nl> + if ( has_snapshot_node_name && <nl> + bdrv_lookup_bs ( snapshot_node_name , snapshot_node_name , NULL )) { <nl> + error_setg ( errp , " New snapshot node name already in use "); <nl> return ; <nl> } <nl> 
int qdev_device_help ( QemuOpts * opts ) <nl> return 0 ; <nl> } <nl>  <nl> - if (! object_class_by_name ( driver )) { <nl> - const char * typename = find_typename_by_alias ( driver ); <nl> - <nl> - if ( typename ) { <nl> - driver = typename ; <nl> - } <nl> + qdev_get_device_class (& driver , & local_err ); <nl> + if ( local_err ) { <nl> + goto error ; <nl> } <nl>  <nl> prop_list = qmp_device_list_properties ( driver , & local_err );
SCSIDevice * scsi_bus_legacy_add_drive ( SCSIBus * bus , BlockDriverState * bdrv , <nl> if ( object_property_find ( OBJECT ( dev ), " removable ", NULL )) { <nl> qdev_prop_set_bit ( dev , " removable ", removable ); <nl> } <nl> - if ( serial ) { <nl> + if ( serial && object_property_find ( OBJECT ( dev ), " serial ", NULL )) { <nl> qdev_prop_set_string ( dev , " serial ", serial ); <nl> } <nl> if ( qdev_prop_set_drive ( dev , " drive ", bdrv ) < 0 ) {
static void scsi_write_same_complete ( void * opaque , int ret ) <nl> data -> sector << BDRV_SECTOR_BITS , <nl> & data -> qiov , 0 , <nl> scsi_write_same_complete , data ); <nl> + aio_context_release ( blk_get_aio_context ( s -> qdev . conf . blk )); <nl> return ; <nl> } <nl> 
static void replay_save_event ( Event * event , int checkpoint ) <nl> replay_event_char_read_save ( event -> opaque ); <nl> break ; <nl> default : <nl> - error_report (" Unknown ID % d of replay event ", read_event_kind ); <nl> + error_report (" Unknown ID %" PRId64 " of replay event ", event -> id ); <nl> exit ( 1 ); <nl> } <nl> }
static void qvirtio_9p_pci_stop ( QVirtIO9P * v9p ) <nl> { <nl> qvirtqueue_cleanup ( v9p -> dev -> bus , v9p -> vq , v9p -> qs -> alloc ); <nl> qvirtio_pci_device_disable ( container_of ( v9p -> dev , QVirtioPCIDevice , vdev )); <nl> - g_free ( v9p -> dev ); <nl> + qvirtio_pci_device_free (( QVirtioPCIDevice *) v9p -> dev ); <nl> qvirtio_9p_stop ( v9p ); <nl> } <nl> 
static inline TCGv iwmmxt_load_creg ( int reg ) <nl> static inline void iwmmxt_store_creg ( int reg , TCGv var ) <nl> { <nl> tcg_gen_st_i32 ( var , cpu_env , offsetof ( CPUState , iwmmxt . cregs [ reg ])); <nl> + dead_tmp ( var ); <nl> } <nl>  <nl> static inline void gen_op_iwmmxt_movq_wRn_M0 ( int rn ) <nl> static int disas_iwmmxt_insn ( CPUState * env , DisasContext * s , uint32_t insn ) <nl> } <nl> } <nl> } <nl> + dead_tmp ( addr ); <nl> return 0 ; <nl> } <nl> 
int bdrv_pwrite_sync ( BlockDriverState * bs , int64_t offset , <nl> return ret ; <nl> } <nl>  <nl> - /* No flush needed for cache modes that already do it */ <nl> - if ( bs -> enable_write_cache ) { <nl> - bdrv_flush ( bs ); <nl> + ret = bdrv_flush ( bs ); <nl> + if ( ret < 0 ) { <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static int vhost_user_read ( struct vhost_dev * dev , VhostUserMsg * msg ) <nl>  <nl> r = qemu_chr_fe_read_all ( chr , p , size ); <nl> if ( r != size ) { <nl> - error_report (" Failed to read msg header . Read % d instead of % d .", r , <nl> - size ); <nl> + error_report (" Failed to read msg header . Read % d instead of % d ." <nl> + " Original request % d .", r , size , msg -> request ); <nl> goto fail ; <nl> } <nl> 
static void x86_cpu_realizefn ( DeviceState * dev , Error ** errp ) <nl> env -> cpuid_ext3_features &= TCG_EXT3_FEATURES ; <nl> env -> cpuid_svm_features &= TCG_SVM_FEATURES ; <nl> } else { <nl> -# ifdef CONFIG_KVM <nl> - filter_features_for_kvm ( cpu ); <nl> -# endif <nl> if ( check_cpuid && kvm_check_features_against_host ( cpu ) <nl> && enforce_cpuid ) { <nl> error_setg ( errp , " Host ' s CPU doesn ' t support requested features "); <nl> return ; <nl> } <nl> +# ifdef CONFIG_KVM <nl> + filter_features_for_kvm ( cpu ); <nl> +# endif <nl> } <nl>  <nl> # ifndef CONFIG_USER_ONLY
mips_mipssim_init ( MachineState * machine ) <nl> ! kernel_filename && ! qtest_enabled ()) { <nl> /* Bail out if we have neither a kernel image nor boot vector code . */ <nl> error_report (" Could not load MIPS bios '% s ', and no " <nl> - "- kernel argument was specified ", filename ); <nl> + "- kernel argument was specified ", bios_name ); <nl> exit ( 1 ); <nl> } else { <nl> /* We have a boot vector start address . */
static int iothread_stop ( Object * object , void * opaque ) <nl> IOThread * iothread ; <nl>  <nl> iothread = ( IOThread *) object_dynamic_cast ( object , TYPE_IOTHREAD ); <nl> - if (! iothread || ! iothread -> ctx ) { <nl> + if (! iothread || ! iothread -> ctx || iothread -> stopping ) { <nl> return 0 ; <nl> } <nl> iothread -> stopping = true ;
static void iothread_instance_finalize ( Object * obj ) <nl> iothread_stop ( obj , NULL ); <nl> qemu_cond_destroy (& iothread -> init_done_cond ); <nl> qemu_mutex_destroy (& iothread -> init_done_lock ); <nl> + if (! iothread -> ctx ) { <nl> + return ; <nl> + } <nl> aio_context_unref ( iothread -> ctx ); <nl> } <nl> 
int qdev_unplug ( DeviceState * dev ) <nl> dev -> parent_bus -> name ); <nl> return - 1 ; <nl> } <nl> + assert ( dev -> info -> unplug != NULL ); <nl> + <nl> return dev -> info -> unplug ( dev ); <nl> } <nl> 
int msix_init ( struct PCIDevice * dev , unsigned short nentries , <nl> return 0 ; <nl>  <nl> err_config : <nl> + dev -> msix_entries_nr = 0 ; <nl> cpu_unregister_io_memory ( dev -> msix_mmio_index ); <nl> err_index : <nl> qemu_free ( dev -> msix_table_page );
static int qemu_event_init ( void ) <nl>  <nl> static void qemu_event_increment ( void ) <nl> { <nl> - SetEvent ( qemu_event_handle ); <nl> + if (! SetEvent ( qemu_event_handle )) { <nl> + fprintf ( stderr , " qemu_event_increment : SetEvent failed : % d \ n ", <nl> + GetLastError ()); <nl> + exit ( 1 ); <nl> + } <nl> } <nl> # endif <nl> 
static int ehci_process_itd ( EHCIState * ehci , <nl> if ( off + len > 4096 ) { <nl> /* transfer crosses page border */ <nl> if ( pg == 6 ) { <nl> + qemu_sglist_destroy (& ehci -> isgl ); <nl> return - 1 ; /* avoid page pg + 1 */ <nl> } <nl> ptr2 = ( itd -> bufptr [ pg + 1 ] & ITD_BUFPTR_MASK );
static int blkverify_open ( BlockDriverState * bs , QDict * options , int flags , <nl>  <nl> ret = 0 ; <nl> fail : <nl> + qemu_opts_del ( opts ); <nl> return ret ; <nl> } <nl> 
static int ioreq_map ( struct ioreq * ioreq ) <nl> xen_be_printf (& ioreq -> blkdev -> xendev , 0 , <nl> " can ' t map grant ref % d (% s , % d maps )\ n ", <nl> refs [ i ], strerror ( errno ), ioreq -> blkdev -> cnt_map ); <nl> + ioreq -> mapped = 1 ; <nl> ioreq_unmap ( ioreq ); <nl> return - 1 ; <nl> }
void cpu_physical_memory_write_rom ( target_phys_addr_t addr , <nl> /* ROM / RAM case */ <nl> ptr = qemu_get_ram_ptr ( addr1 ); <nl> memcpy ( ptr , buf , l ); <nl> + if (! cpu_physical_memory_is_dirty ( addr1 )) { <nl> + /* invalidate code */ <nl> + tb_invalidate_phys_page_range ( addr1 , addr1 + l , 0 ); <nl> + /* set dirty bit */ <nl> + cpu_physical_memory_set_dirty_flags ( <nl> + addr1 , ( 0xff & ~ CODE_DIRTY_FLAG )); <nl> + } <nl> qemu_put_ram_ptr ( ptr ); <nl> } <nl> len -= l ;
int qemu_opts_foreach ( QemuOptsList * list , qemu_opts_loopfunc func , void * opaque , <nl> int rc = 0 ; <nl>  <nl> QTAILQ_FOREACH ( opts , & list -> head , next ) { <nl> - rc = func ( opts , opaque ); <nl> + rc |= func ( opts , opaque ); <nl> if ( abort_on_failure && rc != 0 ) <nl> break ; <nl> }
static int vscsi_srp_direct_data ( VSCSIState * s , vscsi_req * req , <nl> { <nl> struct srp_direct_buf * md = req -> cur_desc ; <nl> uint32_t llen ; <nl> - int rc ; <nl> + int rc = 0 ; <nl>  <nl> dprintf (" VSCSI : direct segment 0x % x bytes , va = 0x % llx desc len = 0x % x \ n ", <nl> len , ( unsigned long long ) md -> va , md -> len );
static int img_convert ( int argc , char ** argv ) <nl> ret = bdrv_parse_cache_flags ( cache , & flags ); <nl> if ( ret < 0 ) { <nl> error_report (" Invalid cache option : % s ", cache ); <nl> - return - 1 ; <nl> + goto out ; <nl> } <nl>  <nl> out_bs = bdrv_new_open (" target ", out_filename , out_fmt , flags , true , quiet );
static void fsl_imx25_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx25_realize ; <nl> - <nl> dc -> desc = " i . MX25 SOC "; <nl> + /* <nl> + * Reason : uses serial_hds in realize and the imx25 board does not <nl> + * support multiple CPUs <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx25_type_info = {
void mips_malta_init ( MachineState * machine ) <nl> /* Board ID = 0x420 ( Malta Board with CoreLV ) */ <nl> stl_p ( memory_region_get_ram_ptr ( bios_copy ) + 0x10 , 0x00000420 ); <nl>  <nl> - /* Init internal devices */ <nl> - cpu_mips_irq_init_cpu ( env ); <nl> - cpu_mips_clock_init ( env ); <nl> - <nl> /* <nl> * We have a circular dependency problem : pci_bus depends on isa_irq , <nl> * isa_irq is provided by i8259 , i8259 depends on ISA , ISA depends
int glue ( load_elf , SZ )( int fd , int64_t virt_to_phys_addend , <nl> data = NULL ; <nl> } <nl> } <nl> + qemu_free ( phdr ); <nl> return total_size ; <nl> fail : <nl> qemu_free ( data );
int kvm_init ( void ) <nl> } while ( ret == - EINTR ); <nl>  <nl> if ( ret < 0 ) { <nl> - fprintf ( stderr , " ioctl ( KVM_CREATE_VM ) failed : % d % s \ n ", - s -> vmfd , <nl> + fprintf ( stderr , " ioctl ( KVM_CREATE_VM ) failed : % d % s \ n ", - ret , <nl> strerror (- ret )); <nl>  <nl> # ifdef TARGET_S390X
int qdev_device_help ( QemuOpts * opts ) <nl> } <nl>  <nl> prop_list = qmp_device_list_properties ( driver , & local_err ); <nl> - if (! prop_list ) { <nl> + if ( local_err ) { <nl> error_printf ("% s \ n ", error_get_pretty ( local_err )); <nl> error_free ( local_err ); <nl> return 1 ;
int cpu_get_pic_interrupt ( CPUX86State * env ) <nl> /* Make sure everything is in a consistent state for calling fork (). */ <nl> void fork_start ( void ) <nl> { <nl> - cpu_list_lock (); <nl> - qemu_mutex_lock (& tb_ctx . tb_lock ); <nl> mmap_fork_start (); <nl> + qemu_mutex_lock (& tb_ctx . tb_lock ); <nl> + cpu_list_lock (); <nl> } <nl>  <nl> void fork_end ( int child )
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> } <nl>  <nl> ip_protocol = ip -> ip_p ; <nl> - ip_data_len = be16_to_cpu ( ip -> ip_len ) - hlen ; <nl> + <nl> + ip_data_len = be16_to_cpu ( ip -> ip_len ); <nl> + if ( ip_data_len < hlen || ip_data_len > eth_payload_len ) { <nl> + goto skip_offload ; <nl> + } <nl> + ip_data_len -= hlen ; <nl>  <nl> if ( txdw0 & CP_TX_IPCS ) <nl> {
static inline void gen_op_arith_compute_ov ( DisasContext * ctx , TCGv arg0 , <nl> { <nl> TCGv t0 = tcg_temp_new (); <nl>  <nl> - tcg_gen_xor_tl ( cpu_ov , arg0 , arg1 ); <nl> + tcg_gen_xor_tl ( cpu_ov , arg0 , arg2 ); <nl> tcg_gen_xor_tl ( t0 , arg1 , arg2 ); <nl> if ( sub ) { <nl> tcg_gen_and_tl ( cpu_ov , cpu_ov , t0 );
static void set_pixel_format ( VncState * vs , <nl> return ; <nl> } <nl>  <nl> + switch ( bits_per_pixel ) { <nl> + case 8 : <nl> + case 16 : <nl> + case 32 : <nl> + break ; <nl> + default : <nl> + vnc_client_error ( vs ); <nl> + return ; <nl> + } <nl> + <nl> vs -> client_pf . rmax = red_max ; <nl> vs -> client_pf . rbits = hweight_long ( red_max ); <nl> vs -> client_pf . rshift = red_shift ;
int paio_init ( void ) <nl> s -> first_aio = NULL ; <nl> if ( qemu_pipe ( fds ) == - 1 ) { <nl> fprintf ( stderr , " failed to create pipe \ n "); <nl> + g_free ( s ); <nl> return - 1 ; <nl> } <nl> 
e1000e_init_msix ( E1000EState * s ) <nl> static void <nl> e1000e_cleanup_msix ( E1000EState * s ) <nl> { <nl> - if ( msix_enabled ( PCI_DEVICE ( s ))) { <nl> + if ( msix_present ( PCI_DEVICE ( s ))) { <nl> e1000e_unuse_msix_vectors ( s , E1000E_MSIX_VEC_NUM ); <nl> msix_uninit ( PCI_DEVICE ( s ), & s -> msix , & s -> msix ); <nl> }
static int slirp_guestfwd ( SlirpState * s , const char * config_str , <nl> } <nl>  <nl> fwd = qemu_malloc ( sizeof ( struct GuestFwd )); <nl> - snprintf ( buf , sizeof ( buf ), " guestfwd . tcp :% d ", port ); <nl> + snprintf ( buf , sizeof ( buf ), " guestfwd . tcp .% d ", port ); <nl> fwd -> hd = qemu_chr_open ( buf , p , NULL ); <nl> if (! fwd -> hd ) { <nl> error_report (" could not open guest forwarding device '% s '", buf );
vubr_panic ( VuDev * dev , const char * msg ) <nl> vubr -> quit = 1 ; <nl> } <nl>  <nl> + static bool <nl> + vubr_queue_is_processed_in_order ( VuDev * dev , int qidx ) <nl> +{ <nl> + return true ; <nl> +} <nl> + <nl> static const VuDevIface vuiface = { <nl> . get_features = vubr_get_features , <nl> . set_features = vubr_set_features , <nl> . process_msg = vubr_process_msg , <nl> . queue_set_started = vubr_queue_set_started , <nl> + . queue_is_processed_in_order = vubr_queue_is_processed_in_order , <nl> }; <nl>  <nl> static void
typedef struct VirtIONet { <nl> uint8_t nobcast ; <nl> uint8_t vhost_started ; <nl> struct { <nl> - int in_use ; <nl> - int first_multi ; <nl> + uint32_t in_use ; <nl> + uint32_t first_multi ; <nl> uint8_t multi_overflow ; <nl> uint8_t uni_overflow ; <nl> uint8_t * macs ;
void qemu_input_event_send_key_qcode ( QemuConsole * src , QKeyCode q , bool down ) <nl>  <nl> void qemu_input_event_send_key_delay ( uint32_t delay_ms ) <nl> { <nl> + if (! runstate_is_running () && ! runstate_check ( RUN_STATE_SUSPENDED )) { <nl> + return ; <nl> + } <nl> + <nl> if (! kbd_timer ) { <nl> kbd_timer = timer_new_ms ( QEMU_CLOCK_VIRTUAL , qemu_input_queue_process , <nl> & kbd_queue );
static void aml_free ( gpointer data , gpointer user_data ) <nl> { <nl> Aml * var = data ; <nl> build_free_array ( var -> buf ); <nl> + g_free ( var ); <nl> } <nl>  <nl> Aml * init_aml_allocator ( void )
int qcow2_update_header ( BlockDriverState * bs ) <nl> ret = sizeof (* header ); <nl> break ; <nl> default : <nl> - return - EINVAL ; <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> } <nl>  <nl> buf += ret ;
static void fsl_imx6_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx6_realize ; <nl> - <nl> dc -> desc = " i . MX6 SOC "; <nl> + /* Reason : Uses serial_hds [] in the realize () function */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx6_type_info = {
static int connect_to_sdog ( const char * addr , const char * port ) <nl> if ( errno == EINTR ) { <nl> goto reconnect ; <nl> } <nl> + close ( fd ); <nl> break ; <nl> } <nl> 
static int pci_qdev_init ( DeviceState * qdev ) <nl> pci_dev -> romfile = g_strdup ( pc -> romfile ); <nl> is_default_rom = true ; <nl> } <nl> - pci_add_option_rom ( pci_dev , is_default_rom ); <nl> + <nl> + rc = pci_add_option_rom ( pci_dev , is_default_rom ); <nl> + if ( rc != 0 ) { <nl> + pci_unregister_device ( DEVICE ( pci_dev )); <nl> + return rc ; <nl> + } <nl>  <nl> return 0 ; <nl> }
struct BusState { <nl> struct Property { <nl> const char * name ; <nl> PropertyInfo * info ; <nl> - int offset ; <nl> + ptrdiff_t offset ; <nl> uint8_t bitnr ; <nl> qtype_code qtype ; <nl> int64_t defval ;
static void vnc_init_basic_info_from_server_addr ( QIOChannelSocket * ioc , <nl> { <nl> SocketAddress * addr = NULL ; <nl>  <nl> + if (! ioc ) { <nl> + error_setg ( errp , " No listener socket available "); <nl> + return ; <nl> + } <nl> + <nl> addr = qio_channel_socket_get_local_address ( ioc , errp ); <nl> if (! addr ) { <nl> return ;
int cpu_exec ( CPUState * env ) <nl> /* reset soft MMU for next block ( it can currently <nl> only be set by a memory fault ) */ <nl> } /* for (;;) */ <nl> + } else { <nl> + /* Reload env after longjmp - the compiler may have smashed all <nl> + * local variables as longjmp is marked ' noreturn '. */ <nl> + env = cpu_single_env ; <nl> } <nl> } /* for (;;) */ <nl> 
 <nl> typedef struct SuperIOConfig <nl> { <nl> - uint8_t config [ 0xff ]; <nl> + uint8_t config [ 0x100 ]; <nl> uint8_t index ; <nl> uint8_t data ; <nl> } SuperIOConfig ;
InetSocketAddress * inet_parse ( const char * str , Error ** errp ) <nl> { <nl> InetSocketAddress * addr ; <nl> const char * optstr , * h ; <nl> - char host [ 64 ]; <nl> + char host [ 65 ]; <nl> char port [ 33 ]; <nl> int to ; <nl> int pos ;
static void vscsi_report_luns ( VSCSIState * s , vscsi_req * req ) <nl> len = n + 8 ; <nl>  <nl> resp_data = g_malloc0 ( len ); <nl> - memset ( resp_data , 0 , len ); <nl> stl_be_p ( resp_data , n ); <nl> i = found_lun0 ? 8 : 16 ; <nl> QTAILQ_FOREACH ( kid , & s -> bus . qbus . children , sibling ) {
static void mmio_interface_realize ( DeviceState * dev , Error ** errp ) <nl>  <nl> if (! s -> host_ptr ) { <nl> error_setg ( errp , " host_ptr property must be set "); <nl> + return ; <nl> } <nl>  <nl> if (! s -> subregion ) { <nl> error_setg ( errp , " subregion property must be set "); <nl> + return ; <nl> } <nl>  <nl> memory_region_init_ram_ptr (& s -> ram_mem , OBJECT ( s ), " ram ",
static int vfio_msix_vector_do_use ( PCIDevice * pdev , unsigned int nr , <nl> vfio_update_kvm_msi_virq ( vector , * msg , pdev ); <nl> } <nl> } else { <nl> - vfio_add_kvm_msi_virq ( vdev , vector , nr , true ); <nl> + if ( msg ) { <nl> + vfio_add_kvm_msi_virq ( vdev , vector , nr , true ); <nl> + } <nl> } <nl>  <nl> /*
void socket_listen_cleanup ( int fd , Error ** errp ) <nl> SocketAddress * addr ; <nl>  <nl> addr = socket_local_address ( fd , errp ); <nl> + if (! addr ) { <nl> + return ; <nl> + } <nl>  <nl> if ( addr -> type == SOCKET_ADDRESS_TYPE_UNIX <nl> && addr -> u . q_unix . path ) {
static void spapr_cpu_core_realize_child ( Object * child , Error ** errp ) <nl> Object * obj ; <nl>  <nl> obj = object_new ( spapr -> icp_type ); <nl> - object_property_add_child ( OBJECT ( cpu ), " icp ", obj , NULL ); <nl> + object_property_add_child ( OBJECT ( cpu ), " icp ", obj , & error_abort ); <nl> + object_unref ( obj ); <nl> object_property_add_const_link ( obj , " xics ", OBJECT ( spapr ), & error_abort ); <nl> object_property_set_bool ( obj , true , " realized ", & local_err ); <nl> if ( local_err ) {
void qemu_tcg_configure ( QemuOpts * opts , Error ** errp ) <nl> } else if ( use_icount ) { <nl> error_setg ( errp , " No MTTCG when icount is enabled "); <nl> } else { <nl> +# ifndef TARGET_SUPPORT_MTTCG <nl> + error_report (" Guest not yet converted to MTTCG - " <nl> + " you may get unexpected results "); <nl> +# endif <nl> if (! check_tcg_memory_orders_compatible ()) { <nl> error_report (" Guest expects a stronger memory ordering " <nl> " than the host provides ");
static int vhdx_log_flush_desc ( BlockDriverState * bs , VHDXLogDescriptor * desc , <nl> /* write ' count ' sectors of sector */ <nl> memset ( buffer , 0 , VHDX_LOG_SECTOR_SIZE ); <nl> count = desc -> zero_length / VHDX_LOG_SECTOR_SIZE ; <nl> + } else { <nl> + error_report (" Invalid VHDX log descriptor entry signature 0x %" PRIx32 , <nl> + desc -> signature ); <nl> + ret = - EINVAL ; <nl> + goto exit ; <nl> } <nl>  <nl> file_offset = desc -> file_offset ;
static void spapr_add_lmbs ( DeviceState * dev , uint64_t addr , uint64_t size , <nl>  <nl> drck = SPAPR_DR_CONNECTOR_GET_CLASS ( drc ); <nl> drck -> attach ( drc , dev , fdt , fdt_offset , ! dev -> hotplugged , errp ); <nl> - spapr_hotplug_req_add_by_index ( drc ); <nl> addr += SPAPR_MEMORY_BLOCK_SIZE ; <nl> } <nl> + spapr_hotplug_req_add_by_count ( SPAPR_DR_CONNECTOR_TYPE_LMB , nr_lmbs ); <nl> } <nl>  <nl> static void spapr_memory_plug ( HotplugHandler * hotplug_dev , DeviceState * dev ,
int main ( int argc , char ** argv ) <nl> cmdline = g_strdup_printf ("- device ipmi - bmc - sim , id = bmc0 " <nl> " - device isa - ipmi - kcs , bmc = bmc0 "); <nl> qtest_start ( cmdline ); <nl> + g_free ( cmdline ); <nl> qtest_irq_intercept_in ( global_qtest , " ioapic "); <nl> qtest_add_func ("/ ipmi / local / kcs_base ", test_kcs_base ); <nl> qtest_add_func ("/ ipmi / local / kcs_abort ", test_kcs_abort );
bool address_space_access_valid ( AddressSpace * as , hwaddr addr , int len , bool is_ <nl> if (! memory_access_is_direct ( mr , is_write )) { <nl> l = memory_access_size ( mr , l , addr ); <nl> if (! memory_region_access_valid ( mr , xlat , l , is_write )) { <nl> + rcu_read_unlock (); <nl> return false ; <nl> } <nl> }
static void ivshmem_read ( void * opaque , const uint8_t * buf , int size ) <nl> if ( incoming_fd == - 1 ) { <nl> fprintf ( stderr , " could not allocate file descriptor % s \ n ", <nl> strerror ( errno )); <nl> + close ( tmp_fd ); <nl> return ; <nl> } <nl> 
static int grow_refcount_table ( BlockDriverState * bs , int min_size ) <nl> qcow2_free_clusters ( bs , old_table_offset , old_table_size * sizeof ( uint64_t )); <nl> return 0 ; <nl> fail : <nl> - qcow2_free_clusters ( bs , table_offset , new_table_size2 ); <nl> qemu_free ( new_table ); <nl> return - EIO ; <nl> }
static int parallels_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail_options ; <nl> } <nl>  <nl> - if (!( flags & BDRV_O_RESIZE ) || ! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> - bdrv_truncate ( bs -> file , bdrv_getlength ( bs -> file -> bs ), <nl> - PREALLOC_MODE_OFF , NULL ) != 0 ) { <nl> + if (! bdrv_has_zero_init ( bs -> file -> bs )) { <nl> s -> prealloc_mode = PRL_PREALLOC_MODE_FALLOCATE ; <nl> } <nl> 
int do_drive_del ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> */ <nl> if ( bdrv_get_attached_dev ( bs )) { <nl> bdrv_make_anon ( bs ); <nl> + <nl> + /* Further I / O must not pause the guest */ <nl> + bdrv_set_on_error ( bs , BLOCKDEV_ON_ERROR_REPORT , <nl> + BLOCKDEV_ON_ERROR_REPORT ); <nl> } else { <nl> drive_uninit ( drive_get_by_blockdev ( bs )); <nl> }
static TypeInfo arm_gic_info = { <nl> . parent = TYPE_ARM_GIC_COMMON , <nl> . instance_size = sizeof ( gic_state ), <nl> . class_init = arm_gic_class_init , <nl> + . class_size = sizeof ( ARMGICClass ), <nl> }; <nl>  <nl> static void arm_gic_register_types ( void )
static int dex_loadcode ( RBinFile * arch , RBinDexObj * bin ) { <nl> for ( i = 0 ; i < bin -> header . method_size ; i ++) { <nl> // RBinDexMethod * method = & bin -> methods [ i ]; <nl> if (! methods [ i ]) { <nl> + if ( i >= bin -> header . class_size ) continue ; <nl> struct dex_class_t * c = & bin -> classes [ i ]; <nl> char * class_name = dex_class_name ( bin , c ); <nl> if ( class_name ) {
struct uwsgi_stats * uwsgi_master_generate_stats () { <nl> uc = uc -> next ; <nl> } <nl>  <nl> + if ( uwsgi_stats_list_close ( us )) <nl> + goto end ; <nl> + <nl> if ( uwsgi_stats_comma ( us )) <nl> goto end ; <nl> }
void delete_all_wml_hotkeys () <nl> } <nl> } <nl>  <nl> -// retunrs weather a hotkey was deleted . <nl> +// Returns whether a hotkey was deleted . <nl> bool remove_wml_hotkey ( const std :: string & id ) <nl> { <nl> hotkey :: hotkey_command & command = get_hotkey_command ( id );
void display :: clear_redraw_observers () <nl>  <nl> void display :: draw ( bool update , bool force ) { <nl> // log_scope (" display :: draw "); <nl> - if ( screen_ . update_locked ()) { <nl> + if ( screen_ . update_locked () || ( SDL_GetAppState () & SDL_APPACTIVE ) == 0 ) { <nl> return ; <nl> } <nl> bool changed = draw_init ();
public : <nl> static void set_sunset ( const unsigned interval ) <nl> { sunset_ = interval ? interval : 5 ; } <nl>  <nl> + bool get_need_layout () const { return need_layout_ ; } <nl> + <nl> private : <nl>  <nl> /** Needed so we can change what ' s drawn on the screen . */
tdata_t <nl> _TIFFCheckMalloc ( TIFF * tif , size_t nmemb , size_t elem_size , const char * what ) <nl> { <nl> - tdata_t * cp = NULL ; <nl> + tdata_t cp = NULL ; <nl> tsize_t bytes = nmemb * elem_size ; <nl>  <nl> /*
EstimateStripByteCounts ( TIFF * tif , TIFFDirEntry * dir , uint16 dircount ) <nl> td -> td_stripbytecount = ( uint64 *) <nl> _TIFFCheckMalloc ( tif , td -> td_nstrips , sizeof ( uint64 ), <nl> " for \" StripByteCounts \" array "); <nl> + if ( td -> td_stripbytecount == NULL ) <nl> + return - 1 ; <nl> + <nl> if ( td -> td_compression != COMPRESSION_NONE ) { <nl> uint64 space ; <nl> uint64 filesize ;
int unit_file_get_list ( <nl> } <nl> } <nl>  <nl> - return r ; <nl> + return 0 ; <nl> } <nl>  <nl> static const char * const unit_file_state_table [ _UNIT_FILE_STATE_MAX ] = {
int bus_exec_context_set_transient_property ( <nl> } else { <nl> _cleanup_free_ char * joined = NULL ; <nl>  <nl> + r = strv_extend_strv (& c -> pass_environment , l , true ); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + <nl> /* We write just the new settings out to file , with unresolved specifiers . */ <nl> joined = unit_concat_strv ( l , UNIT_ESCAPE_SPECIFIERS ); <nl> if (! joined )
static int search_and_fopen_internal ( const char * path , const char * mode , const c <nl> _cleanup_free_ char * p = NULL ; <nl> FILE * f ; <nl>  <nl> - p = strjoin (* i , "/", path , NULL ); <nl> + if ( root ) <nl> + p = strjoin ( root , * i , "/", path , NULL ); <nl> + else <nl> + p = strjoin (* i , "/", path , NULL ); <nl> if (! p ) <nl> return - ENOMEM ; <nl> 
struct udev_device * udev_monitor_receive_device ( struct udev_monitor * udev_monito <nl> udev_device_unref ( udev_device ); <nl> return NULL ; <nl> } <nl> - udev_device_set_devnum ( udev_device , makedev ( maj , min )); <nl> + if ( maj > 0 ) <nl> + udev_device_set_devnum ( udev_device , makedev ( maj , min )); <nl> udev_device_set_info_loaded ( udev_device ); <nl> return udev_device ; <nl> }
int fstab_find_pri ( const char * options , int * ret ) { <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> + if (( int ) pri < 0 ) <nl> + return - ERANGE ; <nl> + <nl> * ret = ( int ) r ; <nl> return 1 ; <nl> }
int base_filesystem_create ( const char * root ) { <nl> const char * target = NULL ; <nl> const char * s ; <nl>  <nl> + if ( faccessat ( fd , table [ i ]. dir , F_OK , AT_SYMLINK_NOFOLLOW ) >= 0 ) <nl> + continue ; <nl> + <nl> /* check if one of the targets exists */ <nl> NULSTR_FOREACH ( s , table [ i ]. target ) { <nl> if ( faccessat ( fd , s , F_OK , AT_SYMLINK_NOFOLLOW ) < 0 )
int cg_pid_get_path ( const char * controller , pid_t pid , char ** path ) { <nl> if (! p ) <nl> return - ENOMEM ; <nl>  <nl> + /* Truncate suffix indicating the process is a zombie */ <nl> + e = endswith ( p , " ( deleted )"); <nl> + if ( e ) <nl> + * e = 0 ; <nl> + <nl> * path = p ; <nl> return 0 ; <nl> }
static int mount_load_proc_self_mountinfo ( Manager * m , bool set_flags ) { <nl> options = mnt_fs_get_options ( fs ); <nl> fstype = mnt_fs_get_fstype ( fs ); <nl>  <nl> + if (! device || ! path ) <nl> + continue ; <nl> + <nl> if ( cunescape ( device , UNESCAPE_RELAX , & d ) < 0 ) <nl> return log_oom (); <nl> 
static int output_cat ( sd_journal * j , OutputMode mode , unsigned line , <nl>  <nl> r = sd_journal_get_data ( j , " MESSAGE ", & data , & l ); <nl> if ( r < 0 ) { <nl> + /* An entry without MESSAGE =? */ <nl> + if ( r == - ENOENT ) <nl> + return 0 ; <nl> + <nl> log_error (" Failed to get data : % s ", strerror (- r )); <nl> return r ; <nl> }
int button_open ( Button * b ) { <nl> } <nl>  <nl> ( void ) button_set_mask ( b ); <nl> - <nl> + <nl> + b -> io_event_source = sd_event_source_unref ( b -> io_event_source ); <nl> r = sd_event_add_io ( b -> manager -> event , & b -> io_event_source , b -> fd , EPOLLIN , button_dispatch , b ); <nl> if ( r < 0 ) { <nl> log_error_errno ( r , " Failed to add button event : % m ");
DllMain ( HINSTANCE hinstDLL , <nl> static const gchar * <nl> _ev_win32_get_locale_dir ( HMODULE module ) <nl> { <nl> + if ( locale_dir ) <nl> + return locale_dir ; <nl> + <nl> gchar * install_dir = NULL , * utf8_locale_dir ; <nl> gchar * retval = NULL ; <nl> 
ev_view_presentation_transition_start ( EvViewPresentation * pview ) <nl>  <nl> duration = ev_document_transition_get_page_duration ( EV_DOCUMENT_TRANSITION ( pview -> document ), <nl> pview -> current_page ); <nl> - if ( duration > 0 ) { <nl> + if ( duration >= 0 ) { <nl> pview -> trans_timeout_id = <nl> g_timeout_add_seconds ( duration , <nl> ( GSourceFunc ) transition_next_page ,
gimp_device_manager_device_added ( GdkDeviceManager * gdk_manager , <nl> GdkDisplay * display ; <nl> GimpDeviceInfo * device_info ; <nl>  <nl> + if ( gdk_device_get_source ( device ) == GDK_SOURCE_KEYBOARD ) <nl> + return ; <nl> + <nl> display = gdk_device_manager_get_display ( gdk_manager ); <nl>  <nl> device_info =
gimp_config_path_expand_only ( const gchar * path , <nl> s = gimp_plug_in_directory (); <nl> else if ( strcmp ( token , " gimp_sysconf_dir ") == 0 ) <nl> s = gimp_sysconf_directory (); <nl> + else if ( strcmp ( token , " gimp_installation_dir ") == 0 ) <nl> + s = gimp_installation_directory (); <nl>  <nl> if (! s ) <nl> s = g_getenv ( token );
neon ( GimpDrawable * drawable , <nl> g_free ( val_p ); <nl> g_free ( val_m ); <nl> g_free ( src ); <nl> + g_free ( src2 ); <nl> g_free ( dest ); <nl> } <nl> 
gimp_draw_tool_control ( GimpTool * tool , <nl> static gboolean <nl> gimp_draw_tool_draw_timeout ( GimpDrawTool * draw_tool ) <nl> { <nl> + guint64 now = g_get_monotonic_time (); <nl> + <nl> + /* keep the timeout running if the last drawing just happened */ <nl> + if (( now - draw_tool -> last_draw_time ) <= MINIMUM_DRAW_INTERVAL ) <nl> + return FALSE ; <nl> + <nl> draw_tool -> draw_timeout = 0 ; <nl>  <nl> gimp_draw_tool_draw ( draw_tool );
load_image ( const gchar * filename , <nl> gint32 volatile image_ID = - 1 ; <nl> gint32 layer_ID ; <nl> int fd ; /* File descriptor */ <nl> - char buf [ BUFLEN ]; /* buffer for random things like scanning */ <nl> + char buf [ BUFLEN + 4 ]; /* buffer for random things like scanning */ <nl> PNMInfo * pnminfo ; <nl> PNMScanner * volatile scan ; <nl> int ctr ;
void <nl> _hb_ot_shape_complex_override_features_indic ( hb_ot_map_builder_t * map , <nl> const hb_segment_properties_t * props HB_UNUSED ) <nl> { <nl> + /* Uniscribe does not apply ' kern '. */ <nl> + if ( indic_options (). uniscribe_bug_compatible ) <nl> + map -> add_feature ( HB_TAG (' k ',' e ',' r ',' n '), 0 , true ); <nl> } <nl>  <nl> 
int ha_create_table_from_engine ( THD * thd , <nl> } <nl>  <nl> err_end : <nl> - my_free (( char *) frmblob , MYF ( MY_ALLOW_ZERO )); <nl> + my_free (( char *) frmblob , MYF ( MY_ALLOW_ZERO_PTR )); <nl> DBUG_RETURN ( error ); <nl> } <nl> 
btr_create ( <nl> PAGE_HEADER + PAGE_BTR_SEG_LEAF , mtr )) { <nl> /* Not enough space for new segment , free root <nl> segment before return . */ <nl> - fseg_free ( space , page_no , <nl> - PAGE_HEADER + PAGE_BTR_SEG_TOP ); <nl> + btr_free_root ( space , page_no , mtr ); <nl>  <nl> return ( FIL_NULL ); <nl> }
innobase_next_autoinc ( <nl> } else { <nl> next_value = current + increment ; <nl> } <nl> - } else { <nl> + } else if ( max_value > current ) { <nl> if ( current > offset ) { <nl> next_value = (( current - offset ) / increment ) + 1 ; <nl> } else { <nl> innobase_next_autoinc ( <nl> next_value += offset ; <nl> } <nl> } <nl> + } else { <nl> + next_value = max_value ; <nl> } <nl>  <nl> ut_a ( next_value <= max_value );
row_create_prebuilt ( <nl> prebuilt -> ins_node = NULL ; <nl>  <nl> prebuilt -> ins_upd_rec_buff = NULL ; <nl> - <nl> + <nl> + prebuilt -> hint_need_to_fetch_extra_cols = 0 ; <nl> + <nl> prebuilt -> upd_node = NULL ; <nl> prebuilt -> ins_graph = NULL ; <nl> prebuilt -> upd_graph = NULL ;
btr_cur_optimistic_insert ( <nl> if ( UNIV_UNLIKELY (! btr_page_reorganize ( block , index , mtr ))) { <nl> ut_a ( buf_block_get_page_zip ( block )); <nl>  <nl> + ibuf_reset_free_bits_with_type ( index -> type , block ); <nl> + <nl> goto fail ; <nl> } <nl> 
class base_list_iterator <nl> * new_list . last = current -> next ; <nl> current -> info = new_list . first -> info ; <nl> current -> next = new_list . first -> next ; <nl> + if (( list -> last == & current -> next ) && ( new_list . elements > 1 )) <nl> + list -> last = new_list . last ; <nl> list -> elements += new_list . elements - 1 ; <nl> } <nl> return ret_value ; // return old element
generate_v2_networkstatus ( void ) <nl> or_options_t * options = get_options (); <nl> char fingerprint [ FINGERPRINT_LEN + 1 ]; <nl> char ipaddr [ INET_NTOA_BUF_LEN + 1 ]; <nl> - char published [ ISO_TIME_LEN ]; <nl> + char published [ ISO_TIME_LEN + 1 ]; <nl> char digest [ DIGEST_LEN ]; <nl> struct in_addr in ; <nl> uint32_t addr ;
router_reload_networkstatus ( void ) <nl> entries = tor_listdir ( filename ); <nl> SMARTLIST_FOREACH ( entries , const char *, fn , { <nl> char buf [ DIGEST_LEN ]; <nl> + if ( fn [ 0 ] == '.') /* skip . and .. */ <nl> + continue ; <nl> if ( strlen ( fn ) != HEX_DIGEST_LEN || <nl> base16_decode ( buf , sizeof ( buf ), fn , strlen ( fn ))) { <nl> log_fn ( LOG_INFO ,
tor_zlib_process ( tor_zlib_state_t * state , <nl> return Z_OK ; <nl> return TOR_ZLIB_BUF_FULL ; <nl> case Z_OK : <nl> - if ( state -> stream . avail_out == 0 ) <nl> + if ( state -> stream . avail_out == 0 || finish ) <nl> return TOR_ZLIB_BUF_FULL ; <nl> return TOR_ZLIB_OK ; <nl> default :
# include < algorithm > <nl> # include < functional > <nl>  <nl> +// R - value references and std :: move <nl> +# if defined ( __cplusplus >= 201103L ) <nl> +# include < utility > <nl> +# endif <nl> + <nl> # ifdef CRYPTOPP_INCLUDE_VECTOR_CC <nl> // workaround needed on Sun Studio 12u1 Sun C ++ 5 . 10 SunOS_i386 128229 - 02 2009 / 09 / 21 <nl> # include < vector . cc >
void jsiSemiInit ( bool autoLoad ) { <nl> "| __ | _ -| . | _ | | | | | . |\ n " <nl> "| _____ | ___ | _ | _ | | ___ | _ | _ | _ | ___ |\ n " <nl> " | _ | http :// espruino . com \ n " <nl> - " " JS_VERSION " Copyright 2015 G . Williams \ n "); <nl> + " " JS_VERSION " Copyright 2016 G . Williams \ n "); <nl> # ifdef ESP8266 <nl> jshPrintBanner (); <nl> # endif
static int _move_account ( mysql_conn_t * mysql_conn , uint32_t lft , uint32_t rgt , <nl> } <nl> xfree ( query ); <nl> if (!( row = mysql_fetch_row ( result ))) { <nl> - error (" no row "); <nl> + debug4 (" Can ' t move a none existant association "); <nl> mysql_free_result ( result ); <nl> - return SLURM_ERROR ; <nl> + return SLURM_SUCCESS ; <nl> } <nl> par_left = atoi ( row [ 0 ]); <nl> mysql_free_result ( result );
extern int make_batch_job_cred ( batch_job_launch_msg_t * launch_msg_ptr , <nl> xassert ( job_ptr -> job_resrcs ); <nl> job_resrcs_ptr = job_ptr -> job_resrcs ; <nl>  <nl> + if ( job_ptr -> job_resrcs == NULL ) { <nl> + error ("% s : job % u is missing job_resrcs info ", <nl> + __func__ , job_ptr -> job_id ); <nl> + return SLURM_ERROR ; <nl> + } <nl> + <nl> memset (& cred_arg , 0 , sizeof ( slurm_cred_arg_t )); <nl>  <nl> cred_arg . jobid = launch_msg_ptr -> job_id ;
extern void gres_plugin_node_state_log ( List gres_list , char * node_name ) <nl> ListIterator gres_iter ; <nl> gres_node_state_t * gres_ptr ; <nl>  <nl> + if ( gres_list == NULL ) <nl> + return ; <nl> + <nl> ( void ) gres_plugin_init (); <nl>  <nl> slurm_mutex_lock (& gres_context_lock );
int msGetGDALGeoTransform ( GDALDatasetH hDS , mapObj * map , layerObj * layer , <nl> } <nl> /* fullPath has a filename included , so get the extension */ <nl> else { <nl> - fileExtension = strrchr ( szPath ,'.') + 1 ; <nl> + fileExtension = msStrdup ( strrchr ( szPath ,'.') + 1 ); <nl> } <nl> } <nl> /* common behaviour with worldfile generated from basename + . wld */
sgen_stop_world ( int generation ) <nl> { <nl> int count , dead ; <nl>  <nl> - /* XXX this is the right stop , thought might not be the nicest place to put it */ <nl> - sgen_process_togglerefs (); <nl> - <nl> mono_profiler_gc_event ( MONO_GC_EVENT_PRE_STOP_WORLD , generation ); <nl> MONO_GC_WORLD_STOP_BEGIN (); <nl> acquire_gc_locks (); <nl>  <nl> + /* We start to scan after locks are taking , this ensures we won ' t be interrupted . */ <nl> + sgen_process_togglerefs (); <nl> + <nl> update_current_thread_stack (& count ); <nl>  <nl> sgen_global_stop_count ++;
void check_object ( char * start ); <nl> */ <nl>  <nl> const char * descriptor_types [] = { <nl> + " INVALID ", <nl> " run_length ", <nl> " small_bitmap ", <nl> - " string ", <nl> " complex ", <nl> " vector ", <nl> - " array ", <nl> " large_bitmap ", <nl> - " complex_arr " <nl> + " complex_arr ", <nl> + " complex_ptrfree " <nl> }; <nl>  <nl> static char * describe_nursery_ptr ( char * ptr , gboolean need_setup );
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> token = read32 ( ip + 2 ); <nl> func = mono_method_get_wrapper_data ( method , token ); <nl> info = mono_find_jit_icall_by_addr ( func ); <nl> + if (! info ) <nl> + g_error (" Could not find icall address in wrapper % s ", mono_method_full_name ( method , 1 )); <nl> g_assert ( info ); <nl>  <nl> CHECK_STACK ( info -> sig -> param_count );
try <nl> rccS . send ( command ); <nl> string receive = rccS . recv (); <nl> cout << receive ; <nl> - exit ( 0 ); <nl> + return 0 ; <nl> } <nl> catch ( AhuException & ae ) <nl> { <nl> cerr <<" Fatal : "<< ae . reason <<"\ n "; <nl> - exit ( 1 ); <nl> + return 1 ; <nl> }
vector < std :: function < void ( void )>> setupLua ( bool client , const std :: string & confi <nl> return std :: shared_ptr < DNSAction >( new NoRecurseAction ); <nl> }); <nl>  <nl> + g_lua . writeFunction (" DropAction ", []() { <nl> + return std :: shared_ptr < DNSAction >( new DropAction ); <nl> + }); <nl> + <nl> + <nl> g_lua . writeFunction (" MaxQPSIPRule ", []( unsigned int qps ) { <nl> return std :: shared_ptr < DNSRule >( new MaxQPSIPRule ( qps )); <nl> });
HTTPserver :: HTTPserver ( const char * _docs_dir , const char * _scripts_dir ) { <nl> bool use_http = true ; <nl> struct stat statsBuf ; <nl> int stat_rc ; <nl> + struct timeval tv ; <nl>  <nl> + /* Randomize data */ <nl> + gettimeofday (& tv , NULL ); <nl> + srand ( tv . tv_sec + tv . tv_usec ); <nl> + <nl> static char * http_options [] = { <nl> ( char *)" listening_ports ", ports , <nl> ( char *)" enable_directory_listing ", ( char *)" no ",
location_cell_data_func ( GtkTreeViewColumn * column , <nl> NAUTILUS_LIST_MODEL_FILE_COLUMN , & file , <nl> - 1 ); <nl>  <nl> + /* The file might be NULL if we just toggled an expander <nl> + * and we ' re still loading the subdirectory . <nl> + */ <nl> + if ( file == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> if ( show_trash_orig && nautilus_file_is_in_trash ( file )) { <nl> NautilusFile * orig_file ; <nl> 
nautilus_window_slot_content_view_matches ( NautilusWindowSlot * self , <nl> return FALSE ; <nl> } <nl>  <nl> - if ( id != NAUTILUS_VIEW_INVALID_ID && NAUTILUS_IS_FILES_VIEW ( priv -> content_view )){ <nl> + if ( id != NAUTILUS_VIEW_INVALID_ID && NAUTILUS_IS_FILES_VIEW ( priv -> content_view )) { <nl> return nautilus_files_view_get_view_id ( NAUTILUS_FILES_VIEW ( priv -> content_view )) == id ; <nl> } else { <nl> return FALSE ;
static void TraverseCustomTexture ( GF_Node * node , void * rs , Bool is_destroy ) <nl>  <nl> static void CustomTexture_update ( GF_TextureHandler * txh ) <nl> { <nl> +# ifndef GPAC_DISABLE_3D <nl> char data [ 12 ]; <nl> +# endif <nl> CustomTextureStack * stack = gf_node_get_private ( txh -> owner ); <nl> // texture not setup , do it <nl> if (! txh -> tx_io ) {
GF_Err infe_Read ( GF_Box * s , GF_BitStream * bs ) <nl> } <nl> string_start += string_len ; <nl> string_len = 0 ; <nl> + if ( ptr -> content_encoding && ptr -> version == 1 ) { <nl> + break ; <nl> + } <nl> } <nl> string_len ++; <nl> }
parse_dir_record ( sc_card_t * card , u8 ** buf , size_t * buflen , int rec_nr ) <nl> else <nl> app -> label = NULL ; <nl>  <nl> - if ( asn1_dirrecord [ 2 ]. flags & SC_ASN1_PRESENT ) { <nl> + if ( asn1_dirrecord [ 2 ]. flags & SC_ASN1_PRESENT && path_len > 0 ) { <nl> /* application path present : ignore AID */ <nl> if ( path_len > SC_MAX_PATH_SIZE ) { <nl> free ( app );
int _sc_add_atr ( sc_context_t * ctx , struct sc_card_driver * driver , struct sc_atr_ <nl> driver -> atr_map = map ; <nl> dst = & driver -> atr_map [ driver -> natrs ++]; <nl> memset ( dst , 0 , sizeof (* dst )); <nl> + memset (& driver -> atr_map [ driver -> natrs ], 0 , sizeof ( struct sc_atr_table )); <nl> + dst -> atr = strdup ( src -> atr ); <nl> dst -> atr = strdup ( src -> atr ); <nl> if (! dst -> atr ) <nl> return SC_ERROR_OUT_OF_MEMORY ;
void sc_notify_id ( struct sc_context * ctx , struct sc_atr * atr , <nl>  <nl> switch ( id ) { <nl> case NOTIFY_CARD_INSERTED : <nl> - icon = " dialog - information "; <nl> + icon = " contact - new "; <nl> break ; <nl> case NOTIFY_CARD_REMOVED : <nl> - icon = " media - removed "; <nl> + icon = " media - eject "; <nl> break ; <nl> case NOTIFY_PIN_GOOD : <nl> icon = " changes - allow ";
static int westcos_sign_decipher ( int mode , sc_card_t * card , <nl> BIO * mem = BIO_new ( BIO_s_mem ()); <nl> # endif <nl>  <nl> - if ( card == NULL ) <nl> + if ( card == NULL ) { <nl> + if ( keyfile ) <nl> + sc_file_free ( keyfile ); <nl> return SC_ERROR_INVALID_ARGUMENTS ; <nl> + } <nl> sc_debug ( card -> ctx , SC_LOG_DEBUG_NORMAL , <nl> " westcos_sign_decipher outlen =% d \ n ", outlen ); <nl> 
CK_RV attr_extract ( CK_ATTRIBUTE_PTR pAttr , void * ptr , size_t * sizep ) <nl> size = sizeof ( CK_KEY_TYPE ); break ; <nl> case CKA_PRIVATE : <nl> size = sizeof ( CK_BBOOL ); break ; <nl> + case CKA_CERTIFICATE_TYPE : <nl> + size = sizeof ( CKA_CERTIFICATE_TYPE ); break ; <nl> default : <nl> return CKR_FUNCTION_FAILED ; <nl> }
void c_SimpleXMLElement :: t___construct ( CStrRef data , int64 options /* = 0 */, <nl> m_attributes = collect_attributes ( m_node , ns , is_prefix ); <nl> } <nl> } else { <nl> - raise_error (" String could not be parsed as XML "); <nl> + throw ( Object ) p_Exception ( NEW ( c_Exception )())-> create ( <nl> + " String could not be parsed as XML "); <nl> } <nl> } <nl> 
namespace XrdCl <nl> XRDCL_SMART_PTR_T < Message > msgPtr ( pResponse ); <nl> pResponse = 0 ; <nl>  <nl> - if ( rsp -> hdr . dlen < 4 ) <nl> + if ( rsp -> hdr . dlen <= 4 ) <nl> { <nl> log -> Error ( XRootDMsg , "[% s ] Got invalid redirect response .", <nl> pUrl . GetHostId (). c_str () );
static void do_chanuser_sync ( mychan_t * mc , chanuser_t * cu , chanacs_t * ca , <nl> } <nl>  <nl> try_kick ( chansvs . me -> me , mc -> chan , cu -> user , " You are not authorized to be on this channel "); <nl> + return ; <nl> } <nl> if ( fl & CA_AKICK && !( fl & CA_EXEMPT )) <nl> {
void write_accounts ( void ) <nl> for ( i = 0 ; i < 1024 ; i ++) { <nl> for ( nc = nclists [ i ]; nc ; nc = nc -> next ) { <nl> athemeflags = 0 ; <nl> + if ( nc -> aliases . count == 0 ) <nl> + continue ; <nl> na = nc -> aliases . list [ 0 ]; <nl> registered = na -> time_registered ; <nl> for ( ii = 1 ; ii < nc -> aliases . count ; ii ++)
void swTimer_node_insert ( swTimer_node ** root , swTimer_node * new_node ) <nl> swTimer_node * tmp = * root ; <nl> while ( 1 ) <nl> { <nl> - if ( tmp -> exec_msec >= new_node -> exec_msec ) <nl> + if ( tmp -> exec_msec > new_node -> exec_msec ) <nl> { <nl> new_node -> prev = tmp -> prev ; <nl> new_node -> next = tmp ;
class ModuleSSLGnuTLS : public Module <nl> // once a day , once a week or once a month . Depending on the <nl> // security requirements . <nl>  <nl> + if (! dh_alloc ) <nl> + return ; <nl> + <nl> int ret ; <nl>  <nl> if (( ret = gnutls_dh_params_generate2 ( dh_params , dh_bits )) < 0 )
private : <nl> */ <nl> char ibuf [ 16384 ]; <nl>  <nl> + /** <nl> + * The output buffer for this socket <nl> + */ <nl> + std :: string Buffer ; <nl> + <nl> /** <nl> * The IP address being connected <nl> * to stored in string form for
void NativeWindowViews :: SetParentWindow ( NativeWindow * parent ) { <nl>  <nl> void NativeWindowViews :: SetModal ( bool modal ) { <nl> # if defined ( USE_X11 ) <nl> + SetWindowType ( GetAcceleratedWidget (), modal ? " dialog " : " normal "); <nl> + Show (); <nl> SetWMSpecState ( GetAcceleratedWidget (), modal , <nl> GetAtom (" _NET_WM_STATE_MODAL ")); <nl> - SetWindowType ( GetAcceleratedWidget (), modal ? " dialog " : " normal "); <nl> # endif <nl> } <nl> 
rtadv_read ( struct thread * thread ) <nl> /* Register myself . */ <nl> rtadv_event ( zvrf , RTADV_READ , sock ); <nl>  <nl> - len = rtadv_recv_packet ( sock , buf , BUFSIZ , & from , & ifindex , & hoplimit ); <nl> + len = rtadv_recv_packet ( sock , buf , sizeof ( buf ), & from , & ifindex , & hoplimit ); <nl>  <nl> if ( len < 0 ) <nl> {
static const char * init_root ( const char * root0 ) { <nl> if ( root0 == NULL ) <nl> root0 = "/"; <nl> root = strdup ( root0 ); <nl> + if ( root == NULL ) <nl> + return NULL ; <nl> if ( root [ strlen ( root )- 1 ] != SEP ) { <nl> if ( REALLOC_N ( root , strlen ( root ) + 2 ) == - 1 ) { <nl> FREE ( root );
aggr_inI1_outR1_continue1 ( struct pluto_crypto_req_cont * pcrc <nl> /* unpack first calculation */ <nl> unpack_KE ( st , r , & st -> st_gr ); <nl>  <nl> + /* unpack nonce too */ <nl> + unpack_nonce (& st -> st_nr , r ); <nl> + <nl> /* NOTE : the " r " reply will get freed by our caller */ <nl>  <nl> /* set up second calculation */
static bool setup_half_ipsec_sa ( struct state * st , bool inbound ) <nl> case IKEv2_ENCR_AES_GCM_8 : <nl> case IKEv2_ENCR_AES_GCM_12 : <nl> case IKEv2_ENCR_AES_GCM_16 : <nl> + /* keymat contains 4 bytes of salt */ <nl> + enc_key_len += AES_GCM_SALT_BYTES ; <nl> + break ; <nl> case IKEv2_ENCR_AES_CCM_8 : <nl> case IKEv2_ENCR_AES_CCM_12 : <nl> case IKEv2_ENCR_AES_CCM_16 : <nl> - /* keymat contains 4 bytes of salt */ <nl> + /* keymat contains 3 bytes of salt */ <nl> enc_key_len += AES_CCM_SALT_BYTES ; <nl> break ; <nl> }
static bool ikev2_check_fragment ( struct msg_digest * md ) <nl> " discarding IKE encrypted fragment - fragmentation not allowed by local policy ( ike_frag = no )")); <nl> return FALSE ; <nl> } <nl> + if (!( st -> st_seen_fragvid )) { <nl> + DBG ( DBG_CONTROL , DBG_log ( <nl> + " discarding IKE encrypted fragment - remote never proposed fragmentation ")); <nl> + return FALSE ; <nl> + } <nl>  <nl> DBG ( DBG_CONTROL , DBG_log ( <nl> " received IKE encrypted fragment number '% u ', total number '% u ', next payload '% u '",
 <nl> # define LSW_NFDBITS ( 8 * sizeof ( long int )) <nl> # define LSW_FDELT ( d ) (( d ) / LSW_NFDBITS ) <nl> -# define LSW_FDMASK ( d ) (( long int ) 1 << (( d ) % LSW_NFDBITS )) <nl> +# define LSW_FDMASK ( d ) (( long int ) ( 1UL << (( d ) % LSW_NFDBITS ))) <nl> # define LSW_FD_SETCOUNT (( LSW_FD_SETSIZE + LSW_NFDBITS - 1 ) / LSW_NFDBITS ) <nl>  <nl> typedef struct {
static stf_status ikev2_child_out_tail ( struct msg_digest * md ) <nl>  <nl> DBG_log (" ikev2_child_sa_respond returned STF_FAIL with % s ", <nl> enum_name (& ikev2_notify_names , v2_notify_num )); <nl> + return ret ; /* abort building the response message */ <nl> } else if ( ret != STF_OK ) { <nl> DBG_log (" ikev2_child_sa_respond returned % s ", <nl> enum_name (& stfstatus_name , ret )); <nl> + return ret ; /* abort building the response message */ <nl> } <nl> + <nl> if (! ikev2_padup_pre_encrypt ( pst , & e_pbs_cipher )) <nl> return STF_INTERNAL_ERROR ; <nl> 
ExprResolveLhs ( struct xkb_context * ctx , const ExprDef * expr , <nl> * elem_rtrn = NULL ; <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> ident . ident ); <nl> * index_rtrn = NULL ; <nl> - return true ; <nl> + return (* field_rtrn != NULL ); <nl> case EXPR_FIELD_REF : <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> field_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> field_ref . field );
namespace Js <nl> const EquivalentPropertyEntry * refInfo = & properties [ pi ]; <nl> if (! this -> NullTypeHandlerBase :: IsObjTypeSpecEquivalent ( type , refInfo )) <nl> { <nl> + failedPropertyIndex = pi ; <nl> return false ; <nl> } <nl> }
time_t raptor_parse_date ( char * p , time_t * now ); <nl> int raptor_stringbuffer_append_turtle_string ( raptor_stringbuffer * stringbuffer , unsigned char * text , size_t len , int delim , raptor_simple_message_handler error_handler , void * error_data ); <nl>  <nl>  <nl> +/* raptor_xsd . c */ <nl> + raptor_identifier * raptor_new_identifier_from_double ( double d ); <nl> + <nl> /* end of RAPTOR_INTERNAL */ <nl> # endif <nl> 
struct evbuffer_overlapped { <nl> static inline struct evbuffer_overlapped * <nl> upcast_evbuffer ( struct evbuffer * buf ) <nl> { <nl> - if (! buf || buf -> is_overlapped ) <nl> + if (! buf || ! buf -> is_overlapped ) <nl> return NULL ; <nl> return EVUTIL_UPCAST ( buf , struct evbuffer_overlapped , buffer ); <nl> }
encode_int_internal ( ev_uint8_t * data , ev_uint32_t number ) <nl> { <nl> int off = 1 , nibbles = 0 ; <nl>  <nl> - memset ( data , 0 , sizeof ( uint32_t )+ 1 ); <nl> + memset ( data , 0 , sizeof ( ev_uint32_t )+ 1 ); <nl> while ( number ) { <nl> if ( off & 0x1 ) <nl> data [ off / 2 ] = ( data [ off / 2 ] & 0xf0 ) | ( number & 0x0f );
evhttp_parse_response_line ( struct evhttp_request * req , char * line ) <nl> return (- 1 ); <nl> } <nl>  <nl> + if ( req -> response_code_line != NULL ) <nl> + mm_free ( req -> response_code_line ); <nl> if (( req -> response_code_line = mm_strdup ( readable )) == NULL ) { <nl> event_warn ("% s : strdup ", __func__ ); <nl> return (- 1 );
static void huffman_decode_row ( x3f_info_t * I , x3f_directory_entry_t * DE , <nl> int col ; <nl> bit_state_t BS ; <nl>  <nl> + if ( HUF -> row_offsets . element [ row ] > ID -> data_size - 1 ) <nl> + throw LIBRAW_EXCEPTION_IO_CORRUPT ; <nl> set_bit_state (& BS , ( uint8_t *) ID -> data + HUF -> row_offsets . element [ row ]); <nl>  <nl> for ( col = 0 ; col < ID -> columns ; col ++)
static CURLcode ssh_block_statemach ( struct connectdata * conn , <nl>  <nl> while (( sshc -> state != SSH_STOP ) && ! result ) { <nl> bool block ; <nl> - time_t left ; <nl> + time_t left = 1000 ; <nl> struct timeval now = Curl_tvnow (); <nl>  <nl> result = ssh_statemach_act ( conn , & block );
CURLcode Curl_proxyCONNECT ( struct connectdata * conn , <nl> else <nl> for ( i = 0 ; i < gotbytes ; ptr ++, i ++) { <nl> perline ++; /* amount of bytes in this line so far */ <nl> - if (* ptr =='\ x0a ') { <nl> + if (* ptr == 0x0a ) { <nl> char letter ; <nl> int writetype ; <nl> 
int main ( int argc , char ** argv ) <nl> curl = curl_easy_init (); <nl> if ( curl ) { <nl> /* what call to write : */ <nl> - curl_easy_setopt ( curl , CURLOPT_URL , " HTTPS :// curl . haxx . se "); <nl> + curl_easy_setopt ( curl , CURLOPT_URL , " HTTPS :// your . favourite . ssl . site "); <nl> curl_easy_setopt ( curl , CURLOPT_WRITEHEADER , headerfile ); <nl>  <nl> while ( 1 ) /* do some ugly short cut ... */
public : <nl> inline Iterator end () const { return Iterator ( value , count ); } <nl>  <nl> inline size_t size () const { return count ; } <nl> + inline T operator []( ptrdiff_t ) const { return value ; } <nl>  <nl> private : <nl> T value ;
runtime  equal ( Type * t , ...) <nl> uintptr ret ; <nl>  <nl> x = ( byte *)(& t + 1 ); <nl> - y = x + ROUND ( t -> size , t -> align ); <nl> + y = x + t -> size ; <nl> ret = ( uintptr )( y + t -> size ); <nl> ret = ROUND ( ret , Structrnd ); <nl> t -> alg -> equal (( bool *) ret , t -> size , x , y );
int OBJ_obj2nid ( const ASN1_OBJECT * a ) <nl> if ( a -> nid != 0 ) <nl> return ( a -> nid ); <nl>  <nl> + if ( a -> length == 0 ) <nl> + return NID_undef ; <nl> + <nl> if ( added != NULL ) { <nl> ad . type = ADDED_DATA ; <nl> ad . obj = ( ASN1_OBJECT *) a ; /* XXX : ugly but harmless */
int PEM_read_bio ( BIO * bp , char ** name , char ** header , unsigned char ** data , <nl> dataB = BUF_MEM_new (); <nl> if (( nameB == NULL ) || ( headerB == NULL ) || ( dataB == NULL )) <nl> { <nl> + BUF_MEM_free ( nameB ); <nl> + BUF_MEM_free ( headerB ); <nl> + BUF_MEM_free ( dataB ); <nl> PEMerr ( PEM_F_PEM_READ_BIO , ERR_R_MALLOC_FAILURE ); <nl> return ( 0 ); <nl> }
# ifndef HEADER_STORE_H <nl> # define HEADER_STORE_H <nl>  <nl> +# include < openssl / opensslconf . h > <nl> + <nl> +# ifdef OPENSSL_NO_STORE <nl> +# error STORE is disabled . <nl> +# endif <nl> + <nl> # include < openssl / ossl_typ . h > <nl> # ifndef OPENSSL_NO_DEPRECATED <nl> # include < openssl / evp . h >
iperf_new_test () <nl> memset ( test , 0 , sizeof ( struct iperf_test )); <nl>  <nl> test -> settings = ( struct iperf_settings *) malloc ( sizeof ( struct iperf_settings )); <nl> + if (! test -> settings ) { <nl> + free ( test ); <nl> + i_errno = IENEWTEST ; <nl> + return NULL ; <nl> + } <nl> memset ( test -> settings , 0 , sizeof ( struct iperf_settings )); <nl>  <nl> return test ;
CBlock * CreateNewBlock ( CReserveKey & reservekey ) <nl> int64 nValueIn = coins . vout [ txin . prevout . n ]. nValue ; <nl> nTotalIn += nValueIn ; <nl>  <nl> - int nConf = pindexPrev -> nHeight - coins . nHeight ; <nl> + int nConf = pindexPrev -> nHeight - coins . nHeight + 1 ; <nl>  <nl> dPriority += ( double ) nValueIn * nConf ; <nl> }
scriptExec ( struct cnfstmt * root , msg_t * pMsg , wti_t * pWti ) <nl> struct cnfstmt * stmt ; <nl>  <nl> for ( stmt = root ; stmt != NULL ; stmt = stmt -> next ) { <nl> + if (* pWti -> pbShutdownImmediate ) { <nl> + DBGPRINTF (" scriptExec : ShutdownImmediate set , " <nl> + " force terminating \ n "); <nl> + goto done ; <nl> + } <nl> if ( Debug ) { <nl> cnfstmtPrintOnly ( stmt , 2 , 0 ); <nl> }
addListner ( instanceConf_t * inst ) <nl> lcnfLast = newlcnfinfo ; <nl> } <nl> } <nl> + } else { <nl> + errmsg . LogError ( 0 , NO_ERRCODE , " imudp : Could not create udp listener ," <nl> + " ignoring port % s bind - address % s .", <nl> + port , bindAddr ); <nl> } <nl>  <nl> finalize_it :
irc_server_msgq_flush () <nl> /* new_msg = plugin_modifier_exec ( PLUGIN_MODIFIER_IRC_IN , <nl> irc_recv_msgq -> server -> name , <nl> ptr_data );*/ <nl> + new_msg = NULL ; <nl> + <nl> /* no changes in new message */ <nl> if ( new_msg && ( strcmp ( ptr_data , new_msg ) == 0 )) <nl> {
Port * erts_get_heart_port () { <nl>  <nl> for ( ix = 0 ; ix < erts_max_ports ; ix ++) { <nl> port = & erts_port [ ix ]; <nl> - /* immediate compare */ <nl> + /* only examine undead or alive ports */ <nl> + if ( port -> status & ERTS_PORT_SFLGS_DEAD ) <nl> + continue ; <nl> + /* immediate atom compare */ <nl> if ( port -> reg && port -> reg -> name == am_heart_port ) { <nl> return port ; <nl> }
int main ( int argc , char ** argv ) <nl> int inotify_fd = inotify_init (); <nl> if ( inotify_fd == - 1 ) <nl> perror_msg_and_die (" inotify_init failed "); <nl> + close_on_exec_on ( inotify_fd ); <nl> if ( inotify_add_watch ( inotify_fd , DEBUG_DUMPS_DIR , IN_CREATE | IN_MOVED_TO ) == - 1 ) <nl> perror_msg_and_die (" inotify_add_watch failed on '% s '", DEBUG_DUMPS_DIR ); <nl> 
aiff_read_chanmap ( SF_PRIVATE * psf , unsigned dword ) <nl> psf_binheader_readf ( psf , " j ", dword - bytesread ) ; <nl>  <nl> if ( map_info -> channel_map != NULL ) <nl> - { size_t chanmap_size = psf -> sf . channels * sizeof ( psf -> channel_map [ 0 ]) ; <nl> + { size_t chanmap_size = SF_MIN ( psf -> sf . channels , layout_tag & 0xffff ) * sizeof ( psf -> channel_map [ 0 ]) ; <nl>  <nl> free ( psf -> channel_map ) ; <nl> 
void qtcDefaultSettings ( Options * opts ) <nl> opts -> shadeMenubarOnlyWhenActive = false ; <nl> opts -> thin = THIN_BUTTONS ; <nl> opts -> tbarBtns = TBTN_STANDARD ; <nl> +# ifdef _WIN32 <nl> + opts -> scrollbarType = SCROLLBAR_WINDOWS ; <nl> +# elif defined __APPLE__ <nl> + opts -> scrollbarType = SCROLLBAR_NONE ; <nl> +# else <nl> opts -> scrollbarType = SCROLLBAR_KDE ; <nl> +# endif <nl> opts -> buttonEffect = EFFECT_SHADOW ; <nl> opts -> focus = FOCUS_GLOW ; <nl> opts -> lvButton = false ;
static void write_unicode ( GByteArray * bplist , gunichar2 * val , uint64_t size ) <nl> for ( i = 0 ; i < size ; i ++) <nl> byte_convert ( buff + i * sizeof ( gunichar2 ), sizeof ( gunichar2 )); <nl> write_raw_data ( bplist , BPLIST_UNICODE , buff , size ); <nl> + free ( buff ); <nl> } <nl>  <nl> static void write_array ( GByteArray * bplist , GNode * node , GHashTable * ref_table , uint8_t dict_param_size )
static int view_scroll ( TEXT_BUFFER_VIEW_REC * view , GList ** lines , int * subline , <nl> { <nl> int linecount , realcount , scroll_visible ; <nl>  <nl> + if (* lines == NULL ) <nl> + return 0 ; <nl> + <nl> /* scroll down */ <nl> scroll_visible = lines == & view -> startline ; <nl> 
tls_ctx_personalise_random ( struct tls_root_ctx * ctx ) <nl> const md_kt_t * sha256_kt = md_kt_get (" SHA256 "); <nl> mbedtls_x509_crt * cert = ctx -> crt_chain ; <nl>  <nl> - if ( 0 != md_full ( sha256_kt , cert -> tbs . p , cert -> tbs . len , sha256_hash )) <nl> + if (! md_full ( sha256_kt , cert -> tbs . p , cert -> tbs . len , sha256_hash )) <nl> { <nl> msg ( M_WARN , " WARNING : failed to personalise random "); <nl> }
loop : <nl> client = accept_client ( fd , & addr . in , false ); <nl> } <nl>  <nl> - slog_debug ( client , " P : got connection : % s ", conninfo ( client )); <nl> + if ( client ) <nl> + slog_debug ( client , " P : got connection : % s ", conninfo ( client )); <nl>  <nl> /* <nl> * there may be several clients waiting ,
static void zone_timer ( int fd , short flg , void * arg ) <nl> z = container_of ( el , struct DNSZone , lnode ); <nl> ctx -> zone_state = 1 ; <nl> ctx -> cur_zone = z ; <nl> + ctx -> active ++; <nl> impl_query_soa_serial ( ctx , z -> zonename ); <nl> } <nl> 
static int inflate ( struct mszipd_stream * zip ) { <nl>  <nl> /* read in block type */ <nl> READ_BITS ( block_type , 2 ); <nl> - D ((" block_type =% u last_block =% u ", block_type , last_block )) <nl>  <nl> if ( block_type == 0 ) { <nl> /* uncompressed block */
void mg_send_digest_auth_request ( struct mg_connection * c ) { <nl> " realm =\"% s \", nonce =\"% lu \"\ r \ n \ r \ n ", <nl> conn -> server -> config_options [ AUTH_DOMAIN ], <nl> ( unsigned long ) time ( NULL )); <nl> + close_local_endpoint ( conn ); <nl> } <nl>  <nl> // Use the global passwords file , if specified by auth_gpass option ,
afc_file_read ( afc_client_t client , uint64_t handle , char * data , int length , uint <nl> const int MAXIMUM_READ_SIZE = 1 << 16 ; <nl> afc_error_t ret = AFC_E_SUCCESS ; <nl>  <nl> - if (! client || ! client -> afc_packet || ! client -> connection || handle == 0 ) <nl> + if (! client || ! client -> afc_packet || ! client -> connection || handle == 0 || ( length < 0 )) <nl> return AFC_E_INVALID_ARGUMENT ; <nl> log_debug_msg ("% s : called for length % i \ n ", __func__ , length ); <nl> 
int JPC_SEGPASSCNT ( int passno , int firstpassno , int numpasses , int bypass , int t <nl> } else { <nl> ret = JPC_PREC * 3 - 2 ; <nl> } <nl> - ret = JAS_MIN ( ret , numpasses - passno ); <nl> + if ( passno < numpasses ) <nl> + ret = JAS_MIN ( ret , numpasses - passno ); <nl> return ret ; <nl> } <nl> 
PyArray_FromString ( char * data , npy_intp slen , PyArray_Descr * dtype , <nl>  <nl> if ( dtype == NULL ) { <nl> dtype = PyArray_DescrFromType ( NPY_DEFAULT_TYPE ); <nl> + if ( dtype == NULL ) { <nl> + return NULL ; <nl> + } <nl> } <nl> if ( PyDataType_FLAGCHK ( dtype , NPY_ITEM_IS_POINTER ) || <nl> PyDataType_REFCHK ( dtype )) {
pid_t read_pid ( char * pidfile ) <nl> { <nl> FILE * f ; <nl> - long pid = 0 ; <nl> + long pid ; <nl>  <nl> if (!( f = fopen ( pidfile ," r "))) <nl> return 0 ; <nl> - fscanf ( f ,"% ld ", & pid ); <nl> + if ( fscanf ( f ,"% ld ", & pid ) != 1 ) <nl> + pid = 0 ; <nl> fclose ( f ); <nl> return pid ; <nl> }
int decodeInstruction ( struct InternalInstruction * insn , <nl>  <nl> insn -> length = ( size_t )( insn -> readerCursor - insn -> startLocation ); <nl>  <nl> + if ( insn -> length > 15 ) <nl> + return - 1 ; <nl> + <nl> // dbgprintf ( insn , " Read from 0x % llx to 0x % llx : length % zu ", <nl> // startLoc , insn -> readerCursor , insn -> length ); <nl> 
void Server :: handle_client_readdir ( MDRequest * mdr ) <nl> if (! dir -> is_complete ()) { <nl> if ( dir -> is_frozen ()) { <nl> dout ( 7 ) << " dir is frozen " << * dir << dendl ; <nl> + mds -> locker -> drop_locks ( mdr ); <nl> + mdr -> drop_local_auth_pins (); <nl> dir -> add_waiter ( CDir :: WAIT_UNFREEZE , new C_MDS_RetryRequest ( mdcache , mdr )); <nl> return ; <nl> }
void MDS :: replay_done () <nl> } <nl>  <nl> if ( continue_replay ) { <nl> - mdlog -> get_journaler ()-> set_writeable (); <nl> continue_replay = false ; <nl> standby_replay_restart (); <nl> return ; <nl> } <nl>  <nl> + mdlog -> get_journaler ()-> set_writeable (); <nl> + <nl> if ( g_conf . mds_wipe_sessions ) { <nl> dout ( 1 ) << " wiping out client sessions " << dendl ; <nl> sessionmap . wipe ();
Context * create_async_context_callback ( I & image_ctx , Context * on_finish ) { <nl> image_ctx . op_work_queue , on_finish ); <nl> } <nl>  <nl> + template < typename WQ > <nl> + Context * create_async_context_callback ( WQ * work_queue , Context * on_finish ) { <nl> + // use async callback to acquire a clean lock context <nl> + return new detail :: C_AsyncCallback < WQ >( work_queue , on_finish ); <nl> +} <nl> + <nl> // TODO : temporary until AioCompletion supports templated ImageCtx <nl> inline ImageCtx * get_image_ctx ( ImageCtx * image_ctx ) { <nl> return image_ctx ;
int rgw_bucket_init_index ( cls_method_context_t hctx , bufferlist * in , bufferlist <nl>  <nl> if ( header_bl . length () != 0 ) { <nl> CLS_LOG (" ERROR : index already initialized \ n "); <nl> + return - EINVAL ; <nl> } <nl>  <nl> rgw_bucket_dir dir ;
static int do_disk_usage ( librbd :: RBD & rbd , librados :: IoCtx & io_ctx , <nl> ++ count ; <nl> } <nl> } <nl> - if (! found ) { <nl> + if ( imgname != nullptr && ! found ) { <nl> std :: cerr << " specified image " << imgname << " is not found ." << std :: endl ; <nl> return - ENOENT ; <nl> }
int OSD :: mkfs ( const char * dev , ceph_fsid fsid , int whoami ) <nl> int OSD :: peek_super ( const char * dev , nstring & magic , ceph_fsid & fsid , int & whoami ) <nl> { <nl> ObjectStore * store = create_object_store ( dev ); <nl> + if (! store ) <nl> + return - ENODEV ; <nl> int err = store -> mount (); <nl> if ( err < 0 ) <nl> return err ;
void RGWZoneParams :: decode_json ( JSONObj * obj ) <nl> :: decode_json (" usage_log_pool ", usage_log_pool , obj ); <nl> :: decode_json (" user_keys_pool ", user_keys_pool , obj ); <nl> :: decode_json (" user_email_pool ", user_email_pool , obj ); <nl> + :: decode_json (" user_uid_pool ", user_uid_pool , obj ); <nl> :: decode_json (" user_swift_pool ", user_swift_pool , obj ); <nl> - :: decode_json (" user_uid_pool ", user_uid_pool , obj ); <nl> JSONDecoder :: decode_json (" system_key ", system_key , obj ); <nl> JSONDecoder :: decode_json (" placement_pools ", placement_pools , obj ); <nl> }
struct MOSDScrub : public Message { <nl> MOSDScrub () {} <nl> MOSDScrub ( ceph_fsid & f ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> - fsid ( f ) {} <nl> + fsid ( f ), repair ( false ) {} <nl> MOSDScrub ( ceph_fsid & f , vector < pg_t >& pgs , bool r ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> fsid ( f ), scrub_pgs ( pgs ), repair ( r ) {}
sr_t * CInode :: project_snaprealm ( snapid_t snapid ) <nl> } else { <nl> new_srnode = new sr_t (); <nl> new_srnode -> created = snapid ; <nl> - new_srnode -> current_parent_since = snapid ; <nl> + new_srnode -> current_parent_since = get_oldest_snap (); <nl> } <nl> dout ( 10 ) << " project_snaprealm " << new_srnode << dendl ; <nl> projected_nodes . back ()-> snapnode = new_srnode ;
const char * ceph_osd_flag_name ( unsigned flag ) <nl> case CEPH_OSD_FLAG_READ : return " read "; <nl> case CEPH_OSD_FLAG_WRITE : return " write "; <nl> case CEPH_OSD_FLAG_ORDERSNAP : return " ordersnap "; <nl> + case CEPH_OSD_FLAG_PEERSTAT_OLD : return " peerstat_old "; <nl> + case CEPH_OSD_FLAG_BALANCE_READS : return " balance_reads "; <nl> + case CEPH_OSD_FLAG_PARALLELEXEC : return " parallelexec "; <nl> case CEPH_OSD_FLAG_PGOP : return " pgop "; <nl> case CEPH_OSD_FLAG_EXEC : return " exec "; <nl> case CEPH_OSD_FLAG_EXEC_PUBLIC : return " exec_public ";
int ceph_setxattr ( struct dentry * dentry , const char * name , <nl> if ( strncmp ( name , " user .", 5 ) != 0 ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( _ceph_match_vir_xattr ( name ) != NULL ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> /* copy value into some pages */ <nl> nr_pages = calc_pages_for ( 0 , size ); <nl> if ( nr_pages ) {
static inline int ceph_fsid_compare ( const struct ceph_fsid * a , <nl> return memcmp ( a , b , sizeof (* a )); <nl> } <nl>  <nl> + static inline void ceph_fsid_set ( struct ceph_fsid * d , <nl> + const struct ceph_fsid * s ) <nl> +{ <nl> + memcpy ( d , s , sizeof (* d )); <nl> +} <nl> + <nl> /* <nl> * ino , object , etc . <nl> */
extern " C " int rados_pool_list ( rados_t cluster , char * buf , size_t len ) <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> - if (! buf ) <nl> + if ( len > 0 && ! buf ) <nl> return - EINVAL ; <nl>  <nl> char * b = buf ;
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> continue ; <nl> ci = ceph_inode ( inode ); <nl> spin_lock (& inode -> i_lock ); <nl> + if (! ci -> i_snap_realm ) <nl> + goto split_skip_inode ; <nl> ceph_put_snap_realm ( mdsc , ci -> i_snap_realm ); <nl> list_add (& ci -> i_snap_realm_item , <nl> & realm -> inodes_with_caps ); <nl> ci -> i_snap_realm = realm ; <nl> realm -> nref ++; <nl> + split_skip_inode : <nl> spin_unlock (& inode -> i_lock ); <nl> iput ( inode ); <nl> }
ONIG_EXTERN int onigenc_unicode_apply_all_case_fold P_ (( OnigCaseFoldType flag , O <nl> addr = OnigUnicodeFolds2 + ( buk )-> index ;\ <nl> else if (( buk )-> fold_len == 3 )\ <nl> addr = OnigUnicodeFolds3 + ( buk )-> index ;\ <nl> + else \ <nl> + addr = 0 ;\ <nl> } while ( 0 ) <nl>  <nl> extern OnigCodePoint OnigUnicodeFolds1 [];
void set_fat ( DOS_FS * fs , uint32_t cluster , int32_t new ) <nl> data [ 1 ] = new >> 4 ; <nl> } else { <nl> FAT_ENTRY subseqEntry ; <nl> - if ( cluster != fs -> clusters - 1 ) <nl> + if ( cluster != fs -> clusters + 1 ) <nl> get_fat (& subseqEntry , fs -> fat , cluster + 1 , fs ); <nl> else <nl> subseqEntry . value = 0 ;
typedef int int32_t ; <nl>  <nl> # endif /* HAVE_CONFIG_H */ <nl>  <nl> - int utf8_encode ( int codepoint , char * buffer , size_t * size ); <nl> + int utf8_encode ( int32_t codepoint , char * buffer , size_t * size ); <nl>  <nl> size_t utf8_check_first ( char byte ); <nl> size_t utf8_check_full ( const char * buffer , size_t size , int32_t * codepoint );
typedef double iw_tmpsample ; <nl>  <nl> # ifdef IW_64BIT <nl> -# define IW_DEFAULT_MAX_DIMENSION 1000000 <nl> -# define IW_DEFAULT_MAX_MALLOC 2000000000000 <nl> +# define IW_DEFAULT_MAX_DIMENSION 40000 <nl> +# define IW_DEFAULT_MAX_MALLOC 2000000000 <nl> # else <nl> # define IW_DEFAULT_MAX_DIMENSION 40000 // Must be less than sqrt ( 2 ^ 31 ). <nl> # define IW_DEFAULT_MAX_MALLOC 2000000000
static int iwgif_read_image ( struct iwgifrcontext * rctx ) <nl>  <nl> rctx -> image_width = ( int ) iw_get_ui16le (& rctx -> rbuf [ 4 ]); <nl> rctx -> image_height = ( int ) iw_get_ui16le (& rctx -> rbuf [ 6 ]); <nl> + if ( rctx -> image_width < 1 || rctx -> image_height < 1 ) { <nl> + iw_set_error ( rctx -> ctx , " Invalid image dimensions "); <nl> + goto done ; <nl> + } <nl>  <nl> rctx -> interlaced = ( int )(( rctx -> rbuf [ 8 ]>> 6 )& 0x01 ); <nl> 
char * argv [],* envp []; <nl> else <nl> program_name = argv [ 0 ]; <nl>  <nl> - if (!( eargv = ( char **) malloc (( argc + 3 ) * sizeof ( char *)))) <nl> + if (!( eargv = ( char **) malloc (( argc + 4 ) * sizeof ( char *)))) <nl> { <nl> fprintf ( stderr , "% s : Failed to obtain memory .\ n ", program_name ); <nl> exit ( 1 ); /* Trap core dump ! */
im_vips2dz ( IMAGE * in , const char * filename ) <nl> * p = '\ 0 '; <nl> im_strncpy ( mode , p + 1 , FILENAME_MAX ); <nl> } <nl> + else <nl> + strcpy ( mode , "" ); <nl>  <nl> strcpy ( buf , mode ); <nl> p = & buf [ 0 ];
ipf_extract_frags_from_batch ( struct ipf * ipf , struct dp_packet_batch * pb , <nl> ovs_mutex_lock (& ipf -> ipf_lock ); <nl> if (! ipf_handle_frag ( ipf , pkt , dl_type , zone , now , hash_basis )) { <nl> dp_packet_batch_refill ( pb , pkt , pb_idx ); <nl> + } else { <nl> + dp_packet_delete ( pkt ); <nl> } <nl> ovs_mutex_unlock (& ipf -> ipf_lock ); <nl> } else {
lock_table_locks_check ( <nl> ut_a ( table != NULL ); <nl> ut_ad ( lock_mutex_own ()); <nl>  <nl> + rw_lock_s_lock (& trx_sys -> lock ); <nl> + <nl> for ( trx = UT_LIST_GET_FIRST ( trx_sys -> trx_list ); <nl> trx != NULL ; <nl> trx = UT_LIST_GET_NEXT ( trx_list , trx )) { <nl> lock_table_locks_check ( <nl> } <nl> } <nl>  <nl> + rw_lock_s_unlock (& trx_sys -> lock ); <nl> + <nl> return ( NULL ); <nl> } <nl> # endif /* UNIV_DEBUG */
__weak_alias ( vis , _vis ) <nl> # define MAXEXTRAS 5 <nl>  <nl>  <nl> - char * MAKEEXTRALIST ( uint flag , const char * orig ) <nl> + char * MAKEEXTRALIST ( unsigned int flag , const char * orig ) <nl> { <nl> const char * o = orig ; <nl> char * e , * extra ;
static bool check_user ( THD * thd , enum_server_command command , const char * user , <nl> send_error ( net , ER_OUT_OF_RESOURCES ); <nl> return 1 ; <nl> } <nl> - strcpy ( thd -> priv_host , LOCAL_HOST ); <nl> + strmake ( thd -> priv_host , LOCAL_HOST , sizeof ( thd -> priv_host )- 1 ); <nl> thd -> master_access = acl_getroot ( thd , thd -> host , thd -> ip , thd -> user , <nl> passwd , thd -> scramble , <nl> & thd -> priv_user , thd -> priv_host ,
Prepared_statement :: execute_loop ( String * expanded_query , <nl> int reprepare_attempt = 0 ; <nl>  <nl> /* Check if we got an error when sending long data */ <nl> - if ( state == Query_arena :: ERROR ) <nl> + if ( state == Query_arena :: STMT_ERROR ) <nl> { <nl> my_message ( last_errno , last_error , MYF ( 0 )); <nl> return TRUE ;
void apply_paravirt ( struct paravirt_patch_site * start , <nl> unsigned int used ; <nl>  <nl> BUG_ON ( p -> len > MAX_PATCH_LEN ); <nl> + /* prep the buffer with the original instructions */ <nl> + memcpy ( insnbuf , p -> instr , p -> len ); <nl> used = paravirt_ops . patch ( p -> instrtype , p -> clobbers , insnbuf , <nl> ( unsigned long ) p -> instr , p -> len ); <nl> 
static void ati_remote_input_report ( struct urb * urb , struct pt_regs * regs ) <nl> input_regs ( dev , regs ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 1 ); <nl> + input_sync ( dev ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 0 ); <nl> input_sync ( dev );
static int nand_scan_bbt ( struct mtd_info * mtd , struct nand_bbt_descr * bd ) <nl> struct nand_bbt_descr * td = this -> bbt_td ; <nl> struct nand_bbt_descr * md = this -> bbt_md ; <nl>  <nl> - len = mtd -> size >> ( this -> bbt_erase_shift + 2 ); <nl> + len = ( mtd -> size >> ( this -> bbt_erase_shift + 2 )) ? : 1 ; <nl> /* <nl> * Allocate memory ( 2bit per block ) and clear the memory bad block <nl> * table .
radeon_atom_encoder_mode_set ( struct drm_encoder * encoder , <nl>  <nl> radeon_encoder -> pixel_clock = adjusted_mode -> clock ; <nl>  <nl> - if ( ASIC_IS_AVIVO ( rdev )) { <nl> + if ( ASIC_IS_AVIVO ( rdev ) && ! ASIC_IS_DCE4 ( rdev )) { <nl> if ( radeon_encoder -> active_device & ( ATOM_DEVICE_CV_SUPPORT | ATOM_DEVICE_TV_SUPPORT )) <nl> atombios_yuv_setup ( encoder , true ); <nl> else
con3270_init ( void ) <nl> return PTR_ERR ( rp ); <nl>  <nl> condev = kzalloc ( sizeof ( struct con3270 ), GFP_KERNEL | GFP_DMA ); <nl> + if (! condev ) <nl> + return - ENOMEM ; <nl> condev -> view . dev = rp ; <nl>  <nl> condev -> read = raw3270_request_alloc ( 0 );
int xen_pcibk_enable_msix ( struct xen_pcibk_device * pdev , <nl> /* <nl> * PCI_COMMAND_MEMORY must be enabled , otherwise we may not be able <nl> * to access the BARs where the MSI - X entries reside . <nl> + * But VF devices are unique in which the PF needs to be checked . <nl> */ <nl> - pci_read_config_word ( dev , PCI_COMMAND , & cmd ); <nl> + pci_read_config_word ( pci_physfn ( dev ), PCI_COMMAND , & cmd ); <nl> if ( dev -> msi_enabled || !( cmd & PCI_COMMAND_MEMORY )) <nl> return - ENXIO ; <nl> 
static long gntalloc_ioctl_alloc ( struct gntalloc_file_private_data * priv , <nl> goto out ; <nl> } <nl>  <nl> - gref_ids = kzalloc ( sizeof ( gref_ids [ 0 ]) * op . count , GFP_TEMPORARY ); <nl> + gref_ids = kcalloc ( op . count , sizeof ( gref_ids [ 0 ]), GFP_TEMPORARY ); <nl> if (! gref_ids ) { <nl> rc = - ENOMEM ; <nl> goto out ;
int regcache_sync_block ( struct regmap * map , void * block , <nl> unsigned int block_base , unsigned int start , <nl> unsigned int end ) <nl> { <nl> - if ( regmap_can_raw_write ( map )) <nl> + if ( regmap_can_raw_write ( map ) && ! map -> use_single_rw ) <nl> return regcache_sync_block_raw ( map , block , cache_present , <nl> block_base , start , end ); <nl> else
static void igmp_heard_query ( struct in_device * in_dev , struct sk_buff * skb , <nl> * to be intended in a v3 query . <nl> */ <nl> max_delay = IGMPV3_MRC ( ih3 -> code )*( HZ / IGMP_TIMER_SCALE ); <nl> + if (! max_delay ) <nl> + max_delay = 1 ; /* can ' t mod w / 0 */ <nl> } else { /* v3 */ <nl> if (! pskb_may_pull ( skb , sizeof ( struct igmpv3_query ))) <nl> return ;
static void zfcp_erp_rports_del ( struct zfcp_adapter * adapter ) <nl> { <nl> struct zfcp_port * port ; <nl> list_for_each_entry ( port , & adapter -> port_list_head , list ) { <nl> + if (! port -> rport ) <nl> + continue ; <nl> fc_remote_port_delete ( port -> rport ); <nl> port -> rport = NULL ; <nl> }
static struct platform_pwm_backlight_data zoom_backlight_data = { <nl> . max_brightness = 127 , <nl> . dft_brightness = 127 , <nl> . pwm_period_ns = 7812500 , <nl> + . enable_gpio = - 1 , <nl> }; <nl>  <nl> static struct platform_device zoom_backlight_pwm = {
static long logger_ioctl ( struct file * file , unsigned int cmd , unsigned long arg ) <nl> ret = - EBADF ; <nl> break ; <nl> } <nl> + if (!( in_egroup_p ( file -> f_dentry -> d_inode -> i_gid ) || <nl> + capable ( CAP_SYSLOG ))) { <nl> + ret = - EPERM ; <nl> + break ; <nl> + } <nl> list_for_each_entry ( reader , & log -> readers , list ) <nl> reader -> r_off = log -> w_off ; <nl> log -> head = log -> w_off ;
qla24xx_chip_diag ( scsi_qla_host_t * ha ) <nl> /* Perform RISC reset . */ <nl> qla24xx_reset_risc ( ha ); <nl>  <nl> - ha -> fw_transfer_size = REQUEST_ENTRY_SIZE * 1024 ; <nl> + ha -> fw_transfer_size = REQUEST_ENTRY_SIZE * ha -> request_q_length ; <nl>  <nl> rval = qla2x00_mbx_reg_test ( ha ); <nl> if ( rval ) {
setup_efi_state ( struct boot_params * params , unsigned long params_load_addr , <nl> if ( efi_enabled ( EFI_OLD_MEMMAP )) <nl> return 0 ; <nl>  <nl> + params -> secure_boot = boot_params . secure_boot ; <nl> ei -> efi_loader_signature = current_ei -> efi_loader_signature ; <nl> ei -> efi_systab = current_ei -> efi_systab ; <nl> ei -> efi_systab_hi = current_ei -> efi_systab_hi ;
static int cfg80211_netdev_notifier_call ( struct notifier_block * nb , <nl> kfree ( wdev -> wext . ie ); <nl> wdev -> wext . ie = NULL ; <nl> wdev -> wext . ie_len = 0 ; <nl> + wdev -> wext . connect . auth_type = NL80211_AUTHTYPE_AUTOMATIC ; <nl> # endif <nl> cfg80211_disconnect ( rdev , dev , <nl> WLAN_REASON_DEAUTH_LEAVING , true );
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl>  <nl> hw -> wiphy -> max_remain_on_channel_duration = 10000 ; <nl> hw -> max_listen_interval = IWL_CONN_MAX_LISTEN_INTERVAL ; <nl> + /* we can compensate an offset of up to 3 channels = 15 MHz */ <nl> + hw -> wiphy -> max_adj_channel_rssi_comp = 3 * 5 ; <nl>  <nl> /* Extract MAC address */ <nl> memcpy ( mvm -> addresses [ 0 ]. addr , mvm -> nvm_data -> hw_addr , ETH_ALEN );
static struct k_itimer * __lock_timer ( timer_t timer_id , unsigned long * flags ) <nl> { <nl> struct k_itimer * timr ; <nl>  <nl> + /* <nl> + * timer_t could be any type >= int and we want to make sure any <nl> + * @ timer_id outside positive int range fails lookup . <nl> + */ <nl> + if (( unsigned long long ) timer_id > INT_MAX ) <nl> + return NULL ; <nl> + <nl> rcu_read_lock (); <nl> timr = idr_find (& posix_timers_id , ( int ) timer_id ); <nl> if ( timr ) {
static void set_tracepoint ( struct tracepoint_entry ** entry , <nl> static void disable_tracepoint ( struct tracepoint * elem ) <nl> { <nl> elem -> state = 0 ; <nl> + rcu_assign_pointer ( elem -> funcs , NULL ); <nl> } <nl>  <nl> /**
static int __init parse_memopt ( char * p ) <nl>  <nl> userdef = 1 ; <nl> mem_size = memparse ( p , & p ); <nl> + /* don ' t remove all of memory when handling " mem ={ invalid }" param */ <nl> + if ( mem_size == 0 ) <nl> + return - EINVAL ; <nl> e820_remove_range ( mem_size , ULLONG_MAX - mem_size , E820_RAM , 1 ); <nl>  <nl> return 0 ;
static int vpbe_display_g_register ( struct file * file , void * priv , <nl> struct v4l2_dbg_register * reg ) <nl> { <nl> struct v4l2_dbg_match * match = & reg -> match ; <nl> + struct vpbe_fh * fh = file -> private_data ; <nl> + struct vpbe_device * vpbe_dev = fh -> disp_dev -> vpbe_dev ; <nl>  <nl> if ( match -> type >= 2 ) { <nl> v4l2_subdev_call ( vpbe_dev -> venc ,
static void hpet_msi_capability_lookup ( unsigned int start_timer ) <nl> continue ; <nl>  <nl> irq = hpet_assign_irq ( hpet_domain , hdev , hdev -> num ); <nl> - if ( irq < 0 ) <nl> + if ( irq <= 0 ) <nl> continue ; <nl>  <nl> sprintf ( hdev -> name , " hpet % d ", i );
xfs_file_last_byte ( <nl> * necessary . <nl> */ <nl> if ( ip -> i_df . if_flags & XFS_IFEXTENTS ) { <nl> + xfs_ilock ( ip , XFS_ILOCK_SHARED ); <nl> error = xfs_bmap_last_offset ( NULL , ip , & last_block , <nl> XFS_DATA_FORK ); <nl> + xfs_iunlock ( ip , XFS_ILOCK_SHARED ); <nl> if ( error ) { <nl> last_block = 0 ; <nl> }
void ath10k_thermal_set_throttling ( struct ath10k * ar ) <nl>  <nl> lockdep_assert_held (& ar -> conf_mutex ); <nl>  <nl> + if (! ar -> wmi . ops -> gen_pdev_set_quiet_mode ) <nl> + return ; <nl> + <nl> if ( ar -> state != ATH10K_STATE_ON ) <nl> return ; <nl> 
static int __block_prepare_write ( struct inode * inode , struct page * page , <nl> unmap_underlying_metadata ( bh -> b_bdev , <nl> bh -> b_blocknr ); <nl> if ( PageUptodate ( page )) { <nl> + clear_buffer_new ( bh ); <nl> set_buffer_uptodate ( bh ); <nl> + mark_buffer_dirty ( bh ); <nl> continue ; <nl> } <nl> if ( block_end > to || block_start < from ) {
store_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
bnad_cb_tx_resume ( struct bnad * bnad , struct bna_tcb * tcb ) <nl> static void <nl> bnad_cb_tx_cleanup ( struct bnad * bnad , struct bna_tcb * tcb ) <nl> { <nl> - struct bnad_unmap_q * unmap_q = tcb -> unmap_q ; <nl> + struct bnad_unmap_q * unmap_q ; <nl>  <nl> if (! tcb || (! tcb -> unmap_q )) <nl> return ; <nl>  <nl> + unmap_q = tcb -> unmap_q ; <nl> if (! unmap_q -> unmap_array ) <nl> return ; <nl> 
static int pch_phub_write_gbe_mac_addr ( struct pch_phub_reg * chip , u8 * data ) <nl> int retval ; <nl> int i ; <nl>  <nl> - if ( chip -> ioh_type == 1 ) /* EG20T */ <nl> + if (( chip -> ioh_type == 1 ) || ( chip -> ioh_type == 5 )) /* EG20T or ML7831 */ <nl> retval = pch_phub_gbe_serial_rom_conf ( chip ); <nl> else /* ML7223 */ <nl> retval = pch_phub_gbe_serial_rom_conf_mp ( chip );
static void pic_update_irq ( struct kvm_pic * s ) <nl> pic_set_irq1 (& s -> pics [ 0 ], 2 , 0 ); <nl> } <nl> irq = pic_get_irq (& s -> pics [ 0 ]); <nl> - if ( irq >= 0 ) <nl> - pic_irq_request ( s -> kvm , 1 ); <nl> - else <nl> - pic_irq_request ( s -> kvm , 0 ); <nl> + pic_irq_request ( s -> kvm , irq >= 0 ); <nl> } <nl>  <nl> void kvm_pic_update_irq ( struct kvm_pic * s )
struct ioat_dma_chan { <nl> struct delayed_work work ; <nl>  <nl> int pending ; <nl> - int dmacount ; <nl> - int desccount ; <nl> + u16 dmacount ; <nl> + u16 desccount ; <nl>  <nl> struct ioatdma_device * device ; <nl> struct dma_chan common ;
static int map_sg_data ( struct scsi_cmnd * cmd , <nl> sdev_printk ( KERN_ERR , cmd -> device , <nl> " Can ' t allocate memory " <nl> " for indirect table \ n "); <nl> + scsi_dma_unmap ( cmd ); <nl> return 0 ; <nl> } <nl> }
static int fb_deferred_io_mkwrite ( struct vm_area_struct * vma , <nl> deferred framebuffer IO . then if userspace touches a page <nl> again , we repeat the same scheme */ <nl>  <nl> + file_update_time ( vma -> vm_file ); <nl> + <nl> /* protect against the workqueue changing the page list */ <nl> mutex_lock (& fbdefio -> lock ); <nl> 
int beiscsi_cmd_mccq_create ( struct beiscsi_hba * phba , <nl> spin_lock (& phba -> ctrl . mbox_lock ); <nl> ctrl = & phba -> ctrl ; <nl> wrb = wrb_from_mbox (& ctrl -> mbox_mem ); <nl> + memset ( wrb , 0 , sizeof (* wrb )); <nl> req = embedded_payload ( wrb ); <nl> ctxt = & req -> context ; <nl> 
bnx2_test_loopback ( struct bnx2 * bp ) <nl>  <nl> pkt_size = 1514 ; <nl> skb = dev_alloc_skb ( pkt_size ); <nl> + if (! skb ) <nl> + return - ENOMEM ; <nl> packet = skb_put ( skb , pkt_size ); <nl> memcpy ( packet , bp -> mac_addr , 6 ); <nl> memset ( packet + 6 , 0x0 , 8 );
static int lowpan_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> if (! netif_running ( dev )) <nl> goto drop_skb ; <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto drop_skb ; <nl> + <nl> if ( dev -> type != ARPHRD_IEEE802154 ) <nl> goto drop_skb ; <nl> 
static int add_std_chmaps ( struct hda_codec * codec ) <nl> struct snd_pcm_chmap * chmap ; <nl> const struct snd_pcm_chmap_elem * elem ; <nl>  <nl> - if (! pcm || ! pcm -> pcm || pcm -> own_chmap || <nl> - ! hinfo -> substreams ) <nl> + if (! pcm -> pcm || pcm -> own_chmap || ! hinfo -> substreams ) <nl> continue ; <nl> elem = hinfo -> chmap ? hinfo -> chmap : snd_pcm_std_chmaps ; <nl> err = snd_pcm_add_chmap_ctls ( pcm -> pcm , str , elem ,
handle_transaction ( struct link_transaction * t ) <nl> struct subaction * sa ; <nl> int i ; <nl>  <nl> + if (! t -> request ) { <nl> + printf (" BUG in handle_transaction \ n "); <nl> + return ; <nl> + } <nl> + <nl> for ( i = 0 ; i < array_length ( protocol_decoders ); i ++) <nl> if ( protocol_decoders [ i ]. decode ( t )) <nl> break ;
static int eql_g_master_cfg ( struct net_device * dev , master_config_t __user * mcp ) <nl> equalizer_t * eql ; <nl> master_config_t mc ; <nl>  <nl> + memset (& mc , 0 , sizeof ( master_config_t )); <nl> + <nl> if ( eql_is_master ( dev )) { <nl> eql = netdev_priv ( dev ); <nl> mc . max_slaves = eql -> max_slaves ;
static void do_interrupt_requests ( struct kvm_vcpu * vcpu , <nl> vmx_update_window_states ( vcpu ); <nl>  <nl> if ( vcpu -> arch . nmi_pending && ! vcpu -> arch . nmi_injected ) { <nl> - if ( vcpu -> arch . nmi_window_open ) { <nl> + if ( vcpu -> arch . interrupt . pending ) { <nl> + enable_nmi_window ( vcpu ); <nl> + } else if ( vcpu -> arch . nmi_window_open ) { <nl> vcpu -> arch . nmi_pending = false ; <nl> vcpu -> arch . nmi_injected = true ; <nl> } else {
static int irda_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> struct sock * sk = sock -> sk ; <nl> struct irda_sock * self = irda_sk ( sk ); <nl>  <nl> + memset (& saddr , 0 , sizeof ( saddr )); <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED ) <nl> return - ENOTCONN ;
static int reiserfs_fill_super ( struct super_block * s , void * data , int silent ) <nl> if ( SB_AP_BITMAP ( s )) <nl> brelse ( SB_AP_BITMAP ( s )[ j ]. bh ); <nl> } <nl> - if ( SB_AP_BITMAP ( s )) <nl> - vfree ( SB_AP_BITMAP ( s )); <nl> + vfree ( SB_AP_BITMAP ( s )); <nl> } <nl> if ( SB_BUFFER_WITH_SB ( s )) <nl> brelse ( SB_BUFFER_WITH_SB ( s ));
static int __devinit cpmac_probe ( struct platform_device * pdev ) <nl> priv -> dev = dev ; <nl> priv -> ring_size = 64 ; <nl> priv -> msg_enable = netif_msg_init ( debug_level , 0xff ); <nl> - memcpy ( dev -> dev_addr , pdata -> dev_addr , sizeof ( dev -> dev_addr )); <nl> + memcpy ( dev -> dev_addr , pdata -> dev_addr , sizeof ( pdata -> dev_addr )); <nl>  <nl> snprintf ( priv -> phy_name , MII_BUS_ID_SIZE , PHY_ID_FMT , mdio_bus_id , phy_id ); <nl> 
ath5k_hw_set_antenna_mode ( struct ath5k_hw * ah , u8 ant_mode ) <nl> u8 def_ant , tx_ant , ee_mode ; <nl> u32 sta_id1 = 0 ; <nl>  <nl> + /* if channel is not initialized yet we can ' t set the antennas <nl> + * so just store the mode . it will be set on the next reset */ <nl> + if ( channel == NULL ) { <nl> + ah -> ah_ant_mode = ant_mode ; <nl> + return ; <nl> + } <nl> + <nl> def_ant = ah -> ah_def_ant ; <nl>  <nl> ATH5K_TRACE ( ah -> ah_sc );
static int cgroupstats_user_cmd ( struct sk_buff * skb , struct genl_info * info ) <nl> na = nla_reserve ( rep_skb , CGROUPSTATS_TYPE_CGROUP_STATS , <nl> sizeof ( struct cgroupstats )); <nl> if ( na == NULL ) { <nl> + nlmsg_free ( rep_skb ); <nl> rc = - EMSGSIZE ; <nl> goto err ; <nl> }
void * consistent_alloc ( gfp_t gfp , size_t size , dma_addr_t * handle ) <nl> split_page ( page , order ); <nl>  <nl> ret = page_address ( page ); <nl> + memset ( ret , 0 , size ); <nl> * handle = virt_to_phys ( ret ); <nl>  <nl> /*
static struct config_item * uvcg_frame_make ( struct config_group * group , <nl> h -> fmt_type = UVCG_MJPEG ; <nl> } else { <nl> mutex_unlock (& opts -> lock ); <nl> + kfree ( h ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> ++ fmt -> num_frames ;
static int iio_read_first_n_kfifo ( struct iio_buffer * r , <nl> int ret , copied ; <nl> struct iio_kfifo * kf = iio_to_kfifo ( r ); <nl>  <nl> - if ( n < r -> bytes_per_datum ) <nl> + if ( n < r -> bytes_per_datum || r -> bytes_per_datum == 0 ) <nl> return - EINVAL ; <nl>  <nl> ret = kfifo_to_user (& kf -> kf , buf , n , & copied ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> return copied ; <nl> }
nouveau_cli_destroy ( struct nouveau_cli * cli ) <nl> nvkm_vm_ref ( NULL , & nvxx_client (& cli -> base )-> vm , NULL ); <nl> nvif_client_fini (& cli -> base ); <nl> usif_client_fini ( cli ); <nl> + kfree ( cli ); <nl> } <nl>  <nl> static void
ath5k_deinit_softc ( struct ath5k_softc * sc ) <nl> * state and potentially want to use them . <nl> */ <nl> ath5k_hw_deinit ( sc -> ah ); <nl> + kfree ( sc -> ah ); <nl> free_irq ( sc -> irq , sc ); <nl> } <nl> 
int xhci_add_endpoint ( struct usb_hcd * hcd , struct usb_device * udev , <nl> * for usb_set_interface () and usb_set_configuration () claim ). <nl> */ <nl> if ( xhci_endpoint_init ( xhci , xhci -> devs [ udev -> slot_id ], <nl> - udev , ep , GFP_KERNEL ) < 0 ) { <nl> + udev , ep , GFP_NOIO ) < 0 ) { <nl> dev_dbg (& udev -> dev , "% s - could not initialize ep %# x \ n ", <nl> __func__ , ep -> desc . bEndpointAddress ); <nl> return - ENOMEM ;
static int perf_tp_filter_match ( struct perf_event * event , <nl> { <nl> void * record = data -> raw -> data ; <nl>  <nl> + /* only top level events have filters set */ <nl> + if ( event -> parent ) <nl> + event = event -> parent ; <nl> + <nl> if ( likely (! event -> filter ) || filter_match_preds ( event -> filter , record )) <nl> return 1 ; <nl> return 0 ;
__setup (" noclflush ", setup_noclflush ); <nl> void __cpuinit print_cpu_info ( struct cpuinfo_x86 * c ) <nl> { <nl> if ( c -> x86_model_id [ 0 ]) <nl> - printk ( KERN_INFO "% s ", c -> x86_model_id ); <nl> + printk ( KERN_CONT "% s ", c -> x86_model_id ); <nl>  <nl> if ( c -> x86_mask || c -> cpuid_level >= 0 ) <nl> printk ( KERN_CONT " stepping % 02x \ n ", c -> x86_mask );
static int cb_pcidas_attach ( struct comedi_device * dev , <nl>  <nl> /* 8255 */ <nl> s = dev -> subdevices + 2 ; <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + ret = subdev_8255_init ( dev , s , NULL , <nl> + devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* serial EEPROM , */ <nl> s = dev -> subdevices + 3 ;
static const struct of_device_id spear13xx_pcie_of_match [] = { <nl> }; <nl> MODULE_DEVICE_TABLE ( of , spear13xx_pcie_of_match ); <nl>  <nl> - static struct platform_driver spear13xx_pcie_driver = { <nl> + static struct platform_driver spear13xx_pcie_driver __initdata = { <nl> . probe = spear13xx_pcie_probe , <nl> . remove = spear13xx_pcie_remove , <nl> . driver = {
static int check_in_drive_lists ( ide_drive_t * drive , const char ** list ) <nl> static u8 svwks_ratemask ( ide_drive_t * drive ) <nl> { <nl> struct pci_dev * dev = HWIF ( drive )-> pci_dev ; <nl> - u8 mode ; <nl> + u8 mode = 0 ; <nl>  <nl> if (! svwks_revision ) <nl> pci_read_config_byte ( dev , PCI_REVISION_ID , & svwks_revision );
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , u32 features ) <nl> skb -> ip_summed = CHECKSUM_NONE ; <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> - if (( skb_headroom ( skb ) < frag_hdr_sz ) && <nl> + if (( skb_mac_header ( skb ) < skb -> head + frag_hdr_sz ) && <nl> pskb_expand_head ( skb , frag_hdr_sz , 0 , GFP_ATOMIC )) <nl> goto out ; <nl> 
static struct ath_buf * ath_clone_txbuf ( struct ath_softc * sc , struct ath_buf * bf ) <nl> tbf -> aphy = bf -> aphy ; <nl> tbf -> bf_mpdu = bf -> bf_mpdu ; <nl> tbf -> bf_buf_addr = bf -> bf_buf_addr ; <nl> - *( tbf -> bf_desc ) = *( bf -> bf_desc ); <nl> + memcpy ( tbf -> bf_desc , bf -> bf_desc , sc -> sc_ah -> caps . tx_desc_len ); <nl> tbf -> bf_state = bf -> bf_state ; <nl> tbf -> bf_dmacontext = bf -> bf_dmacontext ; <nl> 
static inline void __init ulite_console_of_find_device ( int id ) <nl> continue ; <nl>  <nl> ulite_ports [ id ]. mapbase = res . start ; <nl> + of_node_put ( np ); <nl> return ; <nl> } <nl> }
mpt_attach ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl> ioc -> msi_enable = 0 ; <nl> break ; <nl> } <nl> + <nl> + ioc -> fw_events_off = 1 ; <nl> + <nl> if ( ioc -> errata_flag_1064 ) <nl> pci_disable_io_access ( pdev ); <nl> 
int saa7134_input_init1 ( struct saa7134_dev * dev ) <nl> return - ENOMEM ; <nl> } <nl>  <nl> + ir -> dev = input_dev ; <nl> + <nl> /* init hardware - specific stuff */ <nl> ir -> mask_keycode = mask_keycode ; <nl> ir -> mask_keydown = mask_keydown ;
static int clone_backref_node ( struct btrfs_trans_handle * trans , <nl> new_node -> bytenr = dest -> node -> start ; <nl> new_node -> level = node -> level ; <nl> new_node -> lowest = node -> lowest ; <nl> + new_node -> checked = 1 ; <nl> new_node -> root = dest ; <nl>  <nl> if (! node -> lowest ) {
static int mwifiex_pcie_event_complete ( struct mwifiex_adapter * adapter , <nl>  <nl> if (! card -> evt_buf_list [ rdptr ]) { <nl> skb_push ( skb , INTF_HEADER_LEN ); <nl> + skb_put ( skb , MAX_EVENT_SIZE - skb -> len ); <nl> + memset ( skb -> data , 0 , MAX_EVENT_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , <nl> MAX_EVENT_SIZE , <nl> PCI_DMA_FROMDEVICE ))
static const struct nla_policy ifla_vf_policy [ IFLA_VF_MAX + 1 ] = { <nl> . len = sizeof ( struct ifla_vf_vlan ) }, <nl> [ IFLA_VF_TX_RATE ] = { . type = NLA_BINARY , <nl> . len = sizeof ( struct ifla_vf_tx_rate ) }, <nl> + [ IFLA_VF_SPOOFCHK ] = { . type = NLA_BINARY , <nl> + . len = sizeof ( struct ifla_vf_spoofchk ) }, <nl> }; <nl>  <nl> static const struct nla_policy ifla_port_policy [ IFLA_PORT_MAX + 1 ] = {
int ipoib_vlan_delete ( struct net_device * pdev , unsigned short pkey ) <nl> if ( priv -> pkey == pkey ) { <nl> unregister_netdev ( priv -> dev ); <nl> ipoib_dev_cleanup ( priv -> dev ); <nl> - <nl> list_del (& priv -> list ); <nl> - <nl> - kfree ( priv ); <nl> + free_netdev ( priv -> dev ); <nl>  <nl> ret = 0 ; <nl> break ;
static void vmbus_process_offer ( struct work_struct * work ) <nl> spin_lock_irqsave (& vmbus_connection . channel_lock , flags ); <nl> list_del (& newchannel -> listentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channel_lock , flags ); <nl> + kfree ( newchannel -> device_obj ); <nl>  <nl> free_channel ( newchannel ); <nl> } else {
void iwl_rx_reply_rx ( struct iwl_priv * priv , <nl> iwl_dbg_report_frame ( priv , rx_start , len , header , 1 ); <nl> # endif <nl> IWL_DEBUG_STATS_LIMIT ( priv , " Rssi % d , noise % d , qual % d , TSF % llu \ n ", <nl> - rx_status . signal , rx_status . noise , rx_status . signal , <nl> + rx_status . signal , rx_status . noise , rx_status . qual , <nl> ( unsigned long long ) rx_status . mactime ); <nl>  <nl> /*
out : <nl> full_bio -> bi_private = pe -> full_bio_private ; <nl> atomic_inc (& full_bio -> bi_remaining ); <nl> } <nl> - free_pending_exception ( pe ); <nl> - <nl> increment_pending_exceptions_done_count (); <nl>  <nl> up_write (& s -> lock ); <nl> out : <nl> } <nl>  <nl> retry_origin_bios ( s , origin_bios ); <nl> + <nl> + free_pending_exception ( pe ); <nl> } <nl>  <nl> static void commit_callback ( void * context , int success )
struct mmc_fixup { <nl> # define CID_OEMID_ANY (( unsigned short ) - 1 ) <nl> # define CID_NAME_ANY ( NULL ) <nl>  <nl> -# define END_FIXUP { 0 } <nl> +# define END_FIXUP { NULL } <nl>  <nl> # define _FIXUP_EXT ( _name , _manfid , _oemid , _rev_start , _rev_end , \ <nl> _cis_vendor , _cis_device , \
static pci_ers_result_t ixgbe_io_error_detected ( struct pci_dev * pdev , <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + if ( state == pci_channel_io_perm_failure ) <nl> + return PCI_ERS_RESULT_DISCONNECT ; <nl> + <nl> if ( netif_running ( netdev )) <nl> ixgbe_down ( adapter ); <nl> pci_disable_device ( pdev );
static int handle_invalid_guest_state ( struct kvm_vcpu * vcpu ) <nl> if ( intr_window_requested && vmx_interrupt_allowed ( vcpu )) <nl> return handle_interrupt_window (& vmx -> vcpu ); <nl>  <nl> + if ( test_bit ( KVM_REQ_EVENT , & vcpu -> requests )) <nl> + return 1 ; <nl> + <nl> err = emulate_instruction ( vcpu , 0 ); <nl>  <nl> if ( err == EMULATE_DO_MMIO ) {
static int at76_iw_handler_get_scan ( struct net_device * netdev , <nl> if (! iwe ) <nl> return - ENOMEM ; <nl>  <nl> - if ( priv -> scan_state != SCAN_COMPLETED ) <nl> + if ( priv -> scan_state != SCAN_COMPLETED ) { <nl> /* scan not yet finished */ <nl> + kfree ( iwe ); <nl> return - EAGAIN ; <nl> + } <nl>  <nl> spin_lock_irqsave (& priv -> bss_list_spinlock , flags ); <nl> 
static int ext4_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> EXT4_FEATURE_INCOMPAT_FILETYPE )) <nl> new_de -> file_type = old_de -> file_type ; <nl> new_dir -> i_version ++; <nl> + new_dir -> i_ctime = new_dir -> i_mtime = <nl> + ext4_current_time ( new_dir ); <nl> + ext4_mark_inode_dirty ( handle , new_dir ); <nl> BUFFER_TRACE ( new_bh , " call ext4_journal_dirty_metadata "); <nl> ext4_journal_dirty_metadata ( handle , new_bh ); <nl> brelse ( new_bh );
static void notify_ring ( struct drm_device * dev , <nl> struct intel_ring_buffer * ring ) <nl> { <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> - u32 seqno = ring -> get_seqno ( ring ); <nl> + u32 seqno ; <nl> + <nl> + if ( ring -> obj == NULL ) <nl> + return ; <nl>  <nl> + seqno = ring -> get_seqno ( ring ); <nl> trace_i915_gem_request_complete ( dev , seqno ); <nl>  <nl> ring -> irq_seqno = seqno ;
int twl4030_madc_conversion ( struct twl4030_madc_request * req ) <nl> u8 ch_msb , ch_lsb ; <nl> int ret ; <nl>  <nl> - if (! req ) <nl> + if (! req || ! twl4030_madc ) <nl> return - EINVAL ; <nl> + <nl> mutex_lock (& twl4030_madc -> lock ); <nl> if ( req -> method < TWL4030_MADC_RT || req -> method > TWL4030_MADC_SW2 ) { <nl> ret = - EINVAL ;
static int rocker_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl> goto err_probe_ports ; <nl> } <nl>  <nl> - dev_info (& pdev -> dev , " Rocker switch with id % 016llx \ n ", rocker -> hw . id ); <nl> + dev_info (& pdev -> dev , " Rocker switch with id %* phN \ n ", <nl> + ( int ) sizeof ( rocker -> hw . id ), & rocker -> hw . id ); <nl>  <nl> return 0 ; <nl> 
struct e820entry ; <nl> char * __init machine_specific_memory_setup ( void ); <nl> char * memory_setup ( void ); <nl>  <nl> - int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> - int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> + int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> + int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> void __init add_memory_region ( unsigned long long start , <nl> unsigned long long size , int type ); <nl> 
static int wm8731_set_bias_level ( struct snd_soc_codec * codec , <nl>  <nl> switch ( level ) { <nl> case SND_SOC_BIAS_ON : <nl> - if ( wm8731 -> mclk ) <nl> - clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( wm8731 -> mclk ) { <nl> + ret = clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( ret ) <nl> + return ret ; <nl> + } <nl> break ; <nl> case SND_SOC_BIAS_PREPARE : <nl> break ;
static int io_subchannel_sch_event ( struct subchannel * sch , int process ) <nl> goto out ; <nl> break ; <nl> case IO_SCH_UNREG_ATTACH : <nl> + if ( cdev -> private -> flags . resuming ) { <nl> + /* Device will be handled later . */ <nl> + rc = 0 ; <nl> + goto out ; <nl> + } <nl> /* Unregister ccw device . */ <nl> - if (! cdev -> private -> flags . resuming ) <nl> - ccw_device_unregister ( cdev ); <nl> + ccw_device_unregister ( cdev ); <nl> break ; <nl> default : <nl> break ;
static int machines__deliver_event ( struct machines * machines , <nl>  <nl> switch ( event -> header . type ) { <nl> case PERF_RECORD_SAMPLE : <nl> - dump_sample ( evsel , event , sample ); <nl> if ( evsel == NULL ) { <nl> ++ evlist -> stats . nr_unknown_id ; <nl> return 0 ; <nl> } <nl> + dump_sample ( evsel , event , sample ); <nl> if ( machine == NULL ) { <nl> ++ evlist -> stats . nr_unprocessable_samples ; <nl> return 0 ;
static int pwm_omap_dmtimer_probe ( struct platform_device * pdev ) <nl>  <nl> omap = devm_kzalloc (& pdev -> dev , sizeof (* omap ), GFP_KERNEL ); <nl> if (! omap ) { <nl> - omap -> pdata -> free ( dm_timer ); <nl> + pdata -> free ( dm_timer ); <nl> return - ENOMEM ; <nl> } <nl> 
static int ad198x_build_pcms ( struct hda_codec * codec ) <nl> if ( spec -> multiout . dig_out_nid ) { <nl> info ++; <nl> codec -> num_pcms ++; <nl> + codec -> spdif_status_reset = 1 ; <nl> info -> name = " AD198x Digital "; <nl> info -> pcm_type = HDA_PCM_TYPE_SPDIF ; <nl> info -> stream [ SNDRV_PCM_STREAM_PLAYBACK ] = ad198x_pcm_digital_playback ;
static int parse_raid_params ( struct raid_set * rs , char ** argv , <nl> rs -> ti -> error = " write_mostly option is only valid for RAID1 "; <nl> return - EINVAL ; <nl> } <nl> - if ( value > rs -> md . raid_disks ) { <nl> + if ( value >= rs -> md . raid_disks ) { <nl> rs -> ti -> error = " Invalid write_mostly drive index given "; <nl> return - EINVAL ; <nl> }
void __init at91_add_device_serial ( void ) <nl> printk ( KERN_INFO " AT91 : No default serial console defined .\ n "); <nl> } <nl> # else <nl> - void __init __deprecated at91_init_serial ( struct at91_uart_config * config ) {} <nl> void __init at91_register_uart ( unsigned id , unsigned portnr , unsigned pins ) {} <nl> void __init at91_set_serial_console ( unsigned portnr ) {} <nl> void __init at91_add_device_serial ( void ) {}
static int dma_set_runtime_config ( struct dma_chan * chan , <nl> u32 cctl = 0 ; <nl> int i ; <nl>  <nl> + if (! plchan -> slave ) <nl> + return - EINVAL ; <nl> + <nl> /* Transfer direction */ <nl> plchan -> runtime_direction = config -> direction ; <nl> if ( config -> direction == DMA_TO_DEVICE ) {
static int __devexit mxcnd_remove ( struct platform_device * pdev ) <nl> static struct platform_driver mxcnd_driver = { <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> + . owner = THIS_MODULE , <nl> }, <nl> . remove = __devexit_p ( mxcnd_remove ), <nl> };
static int hpb_dmae_chan_probe ( struct hpb_dmae_device * hpbdev , int id ) <nl> } <nl>  <nl> schan = & new_hpb_chan -> shdma_chan ; <nl> + schan -> max_xfer_len = HPB_DMA_TCR_MAX ; <nl> + <nl> shdma_chan_probe ( sdev , schan , id ); <nl>  <nl> if ( pdev -> id >= 0 )
static int pipe_buffer_setting ( struct m66592 * m66592 , <nl> break ; <nl> case M66592_BULK : <nl> /* isochronous pipes may be used as bulk pipes */ <nl> - if ( info -> pipe > M66592_BASE_PIPENUM_BULK ) <nl> + if ( info -> pipe >= M66592_BASE_PIPENUM_BULK ) <nl> bufnum = info -> pipe - M66592_BASE_PIPENUM_BULK ; <nl> else <nl> bufnum = info -> pipe - M66592_BASE_PIPENUM_ISOC ;
static int resizer_configure_output_win ( struct vpfe_resizer_device * resizer ) <nl>  <nl> outformat = & resizer -> resizer_a . formats [ RESIZER_PAD_SOURCE ]; <nl>  <nl> + memset (& output_specs , 0x0 , sizeof ( struct vpfe_rsz_output_spec )); <nl> output_specs . vst_y = param -> user_config . vst ; <nl> if ( outformat -> code == MEDIA_BUS_FMT_YDYUYDYV8_1X16 ) <nl> output_specs . vst_c = param -> user_config . vst ;
static int rcar_vin_videobuf_setup ( struct vb2_queue * vq , <nl> unsigned int bytes_per_line ; <nl> int ret ; <nl>  <nl> + if ( fmt -> fmt . pix . sizeimage < icd -> sizeimage ) <nl> + return - EINVAL ; <nl> + <nl> xlate = soc_camera_xlate_by_fourcc ( icd , <nl> fmt -> fmt . pix . pixelformat ); <nl> if (! xlate )
int wl1271_init_ieee80211 ( struct wl1271 * wl ) <nl> wl -> hw -> wiphy -> max_scan_ie_len = WL1271_CMD_TEMPL_DFLT_SIZE - <nl> sizeof ( struct ieee80211_header ); <nl>  <nl> + wl -> hw -> wiphy -> max_sched_scan_ie_len = WL1271_CMD_TEMPL_DFLT_SIZE - <nl> + sizeof ( struct ieee80211_header ); <nl> + <nl> wl -> hw -> wiphy -> flags |= WIPHY_FLAG_AP_UAPSD ; <nl>  <nl> /* make sure all our channels fit in the scanned_ch bitmask */
static inline void * __alloc_percpu ( size_t size , size_t align ) <nl> * percpu sections on SMP for which this path isn ' t used . <nl> */ <nl> WARN_ON_ONCE ( align > __alignof__ ( unsigned long long )); <nl> - return kzalloc ( size , gfp ); <nl> + return kzalloc ( size , GFP_KERNEL ); <nl> } <nl>  <nl> static inline void free_percpu ( void * p )
static int ir_lirc_unregister ( struct rc_dev * dev ) <nl>  <nl> lirc_unregister_driver ( lirc -> drv -> minor ); <nl> lirc_buffer_free ( lirc -> drv -> rbuf ); <nl> + kfree ( lirc -> drv -> rbuf ); <nl> kfree ( lirc -> drv ); <nl>  <nl> return 0 ;
static int k3_dma_probe ( struct platform_device * op ) <nl> d -> slave . device_issue_pending = k3_dma_issue_pending ; <nl> d -> slave . device_control = k3_dma_control ; <nl> d -> slave . copy_align = DMA_ALIGN ; <nl> - d -> slave . chancnt = d -> dma_requests ; <nl>  <nl> /* init virtual channel */ <nl> d -> chans = devm_kzalloc (& op -> dev ,
static int snd_asihpi_cmode_info ( struct snd_kcontrol * kcontrol , <nl> valid_modes ++; <nl> } <nl>  <nl> + if (! valid_modes ) <nl> + return - EINVAL ; <nl> + <nl> uinfo -> type = SNDRV_CTL_ELEM_TYPE_ENUMERATED ; <nl> uinfo -> count = 1 ; <nl> uinfo -> value . enumerated . items = valid_modes ;
int rt2x00mac_tx ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> */ <nl> if (! test_bit ( DEVICE_PRESENT , & rt2x00dev -> flags )) { <nl> ieee80211_stop_queues ( hw ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
static inline void update_sd_power_savings_stats ( struct sched_group * group , <nl> * capacity but still has some space to pick up some load <nl> * from other group and save more power <nl> */ <nl> - if ( sgs -> sum_nr_running > sgs -> group_capacity - 1 ) <nl> + if ( sgs -> sum_nr_running + 1 > sgs -> group_capacity ) <nl> return ; <nl>  <nl> if ( sgs -> sum_nr_running > sds -> leader_nr_running ||
static ssize_t set_pwm_mode ( struct device * dev , struct device_attribute * attr , <nl> if (!( val == 0 || val == 1 )) <nl> return - EINVAL ; <nl>  <nl> + /* F75373 does not support DC ( linear voltage ) fan control mode */ <nl> + if ( data -> kind == f75373 && val == 0 ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& data -> update_lock ); <nl> conf = f75375_read8 ( client , F75375_REG_CONFIG1 ); <nl> conf &= ~( 1 << FAN_CTRL_LINEAR ( nr ));
wlc_d11hdrs_mac80211 ( struct wlc_info * wlc , struct ieee80211_hw * hw , <nl>  <nl> /* add Broadcom tx descriptor header */ <nl> txh = ( d11txh_t *) skb_push ( p , D11_TXH_LEN ); <nl> - memset (( char *) txh , 0 , D11_TXH_LEN ); <nl> + memset ( txh , 0 , D11_TXH_LEN ); <nl>  <nl> /* setup frameid */ <nl> if ( tx_info -> flags & IEEE80211_TX_CTL_ASSIGN_SEQ ) {
out_err : <nl> * errors we try again until the max number of retries is reached . <nl> */ <nl> if ( result != - EHOSTUNREACH && result != - ENETUNREACH && <nl> - result != - ENETDOWN && result != EINVAL <nl> + result != - ENETDOWN && result != - EINVAL <nl> && result != - EPROTONOSUPPORT ) { <nl> lowcomms_connect_sock ( con ); <nl> result = 0 ;
static int mga_vram_init ( struct mga_device * mdev ) <nl> { <nl> void __iomem * mem ; <nl> struct apertures_struct * aper = alloc_apertures ( 1 ); <nl> + if (! aper ) <nl> + return - ENOMEM ; <nl>  <nl> /* BAR 0 is VRAM */ <nl> mdev -> mc . vram_base = pci_resource_start ( mdev -> dev -> pdev , 0 );
static int fc2580_set_params ( struct dvb_frontend * fe ) <nl> { <nl> struct fc2580_priv * priv = fe -> tuner_priv ; <nl> struct dtv_frontend_properties * c = & fe -> dtv_property_cache ; <nl> - int ret , i ; <nl> + int ret = 0 , i ; <nl> unsigned int r_val , n_val , k_val , k_val_reg , f_ref ; <nl> u8 tmp_val , r18_val ; <nl> u64 f_vco ;
static int btrfs_xattr_acl_set ( struct dentry * dentry , const char * name , <nl> int ret ; <nl> struct posix_acl * acl = NULL ; <nl>  <nl> + if (! is_owner_or_cap ( dentry -> d_inode )) <nl> + return - EPERM ; <nl> + <nl> if ( value ) { <nl> acl = posix_acl_from_xattr ( value , size ); <nl> if ( acl == NULL ) {
static void dwc2_hsotg_start_req ( struct dwc2_hsotg * hsotg , <nl> /* If endpoint is stalled , we will restart request later */ <nl> ctrl = dwc2_readl ( hsotg -> regs + epctrl_reg ); <nl>  <nl> - if ( ctrl & DXEPCTL_STALL ) { <nl> + if ( index && ctrl & DXEPCTL_STALL ) { <nl> dev_warn ( hsotg -> dev , "% s : ep % d is stalled \ n ", __func__ , index ); <nl> return ; <nl> }
static void gc_attach ( struct parport * pp ) <nl> pads = gc_cfg [ port_idx ]. args + 1 ; <nl> n_pads = gc_cfg [ port_idx ]. nargs - 1 ; <nl>  <nl> + memset (& gc_parport_cb , 0 , sizeof ( gc_parport_cb )); <nl> gc_parport_cb . flags = PARPORT_FLAG_EXCL ; <nl>  <nl> pd = parport_register_dev_model ( pp , " gamecon ", & gc_parport_cb ,
static int dma40_memcpy_channels [] = { <nl> }; <nl>  <nl> /* Default configuration for physcial memcpy */ <nl> - struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> + static struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> . mode = STEDMA40_MODE_PHYSICAL , <nl> . dir = DMA_MEM_TO_MEM , <nl>  <nl> struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> }; <nl>  <nl> /* Default configuration for logical memcpy */ <nl> - struct stedma40_chan_cfg dma40_memcpy_conf_log = { <nl> + static struct stedma40_chan_cfg dma40_memcpy_conf_log = { <nl> . mode = STEDMA40_MODE_LOGICAL , <nl> . dir = DMA_MEM_TO_MEM , <nl> 
static struct thread * __machine__findnew_thread ( struct machine * machine , <nl> * within thread__init_map_groups to find the thread <nl> * leader and that would screwed the rb tree . <nl> */ <nl> - if ( thread__init_map_groups ( th , machine )) <nl> + if ( thread__init_map_groups ( th , machine )) { <nl> + thread__delete ( th ); <nl> return NULL ; <nl> + } <nl> } <nl>  <nl> return th ;
static void rtl8150_disconnect ( struct usb_interface * intf ) <nl> if ( dev ) { <nl> set_bit ( RTL8150_UNPLUG , & dev -> flags ); <nl> tasklet_disable (& dev -> tl ); <nl> + tasklet_kill (& dev -> tl ); <nl> unregister_netdev ( dev -> netdev ); <nl> unlink_all_urbs ( dev ); <nl> free_all_urbs ( dev );
static int do_insnlist_ioctl ( struct comedi_device * dev , <nl> goto error ; <nl> } <nl>  <nl> + if ( sizeof ( struct comedi_insn ) * insnlist . n_insns < insnlist . n_insns ) { <nl> + ret = - EINVAL ; <nl> + goto error ; <nl> + } <nl> + <nl> insns = <nl> kmalloc ( sizeof ( struct comedi_insn ) * insnlist . n_insns , GFP_KERNEL ); <nl> if (! insns ) {
void ata_bmdma_error_handler ( struct ata_port * ap ) <nl> */ <nl> void ata_bmdma_post_internal_cmd ( struct ata_queued_cmd * qc ) <nl> { <nl> - ata_bmdma_stop ( qc ); <nl> + if ( qc -> ap -> ioaddr . bmdma_addr ) <nl> + ata_bmdma_stop ( qc ); <nl> } <nl>  <nl> # ifdef CONFIG_PCI
asmlinkage long sys_rt_sigreturn ( struct pt_regs * regs ) <nl>  <nl> /* It is more difficult to avoid calling this function than to <nl> call it and ignore errors . */ <nl> - if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> r1 )) <nl> + if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> r1 ) == - EFAULT ) <nl> goto badframe ; <nl>  <nl> return rval ;
iscsi_sendpage ( struct iscsi_conn * conn , struct iscsi_buf * buf , <nl> BUG_ON ( buf -> sent + size > buf -> sg . length ); <nl> if ( size > * count ) <nl> size = * count ; <nl> - if ( buf -> sent + size != buf -> sg . length ) <nl> + if ( buf -> sent + size != buf -> sg . length || * count != size ) <nl> flags |= MSG_MORE ; <nl>  <nl> res = iscsi_send ( sk , buf , size , flags );
static void slob_free ( void * block , int size ) <nl> sp -> units += units ; <nl>  <nl> if ( b < sp -> free ) { <nl> + if ( b + units == sp -> free ) { <nl> + units += slob_units ( sp -> free ); <nl> + sp -> free = slob_next ( sp -> free ); <nl> + } <nl> set_slob ( b , units , sp -> free ); <nl> sp -> free = b ; <nl> } else {
int iwl_mvm_tof_responder_cmd ( struct iwl_mvm * mvm , <nl> if (! fw_has_capa (& mvm -> fw -> ucode_capa , IWL_UCODE_TLV_CAPA_TOF_SUPPORT )) <nl> return - EINVAL ; <nl>  <nl> - if ( vif -> p2p || vif -> type != NL80211_IFTYPE_AP ) { <nl> + if ( vif -> p2p || vif -> type != NL80211_IFTYPE_AP || <nl> + ! mvmvif -> ap_ibss_active ) { <nl> IWL_ERR ( mvm , " Cannot start responder , not in AP mode \ n "); <nl> return - EIO ; <nl> }
extern struct page ** ceph_get_direct_page_vector ( const void __user * data , <nl> bool write_page ); <nl> extern void ceph_put_page_vector ( struct page ** pages , int num_pages , <nl> bool dirty ); <nl> - extern void ceph_release_page_vector ( struct page ** pages , int num_pages ); <nl> extern struct page ** ceph_alloc_page_vector ( int num_pages , gfp_t flags ); <nl> extern int ceph_copy_user_to_page_vector ( struct page ** pages , <nl> const void __user * data ,
static struct ib_mr * mthca_reg_phys_mr ( struct ib_pd * pd , <nl> convert_access ( acc ), mr ); <nl>  <nl> if ( err ) { <nl> + kfree ( page_list ); <nl> kfree ( mr ); <nl> return ERR_PTR ( err ); <nl> }
static int __init efi_rtc_probe ( struct platform_device * dev ) <nl> if ( IS_ERR ( rtc )) <nl> return PTR_ERR ( rtc ); <nl>  <nl> + rtc -> uie_unsupported = 1 ; <nl> platform_set_drvdata ( dev , rtc ); <nl>  <nl> return 0 ;
static struct platform_driver gef_wdt_driver = { <nl> . of_match_table = gef_wdt_ids , <nl> }, <nl> . probe = gef_wdt_probe , <nl> + . remove = gef_wdt_remove , <nl> }; <nl>  <nl> static int __init gef_wdt_init ( void )
int clockevents_unbind_device ( struct clock_event_device * ced , int cpu ) <nl> mutex_unlock (& clockevents_mutex ); <nl> return ret ; <nl> } <nl> - EXPORT_SYMBOL_GPL ( clockevents_unbind ); <nl> + EXPORT_SYMBOL_GPL ( clockevents_unbind_device ); <nl>  <nl> /** <nl> * clockevents_register_device - register a clock event device
void ath9k_htc_rx_msg ( struct htc_target * htc_handle , <nl> return ; <nl> } <nl>  <nl> - if ( epid >= ENDPOINT_MAX ) { <nl> + if ( epid < 0 || epid >= ENDPOINT_MAX ) { <nl> if ( pipe_id != USB_REG_IN_PIPE ) <nl> dev_kfree_skb_any ( skb ); <nl> else
static void _rtl8821ae_read_adapter_info ( struct ieee80211_hw * hw , bool b_pseudo_ <nl> if ( rtlefuse -> eeprom_channelplan == 0xff ) <nl> rtlefuse -> eeprom_channelplan = 0x7F ; <nl>  <nl> - /* set channel paln to world wide 13 */ <nl> - /* rtlefuse -> channel_plan = ( u8 ) rtlefuse -> eeprom_channelplan ; */ <nl> + /* set channel plan from efuse */ <nl> + rtlefuse -> channel_plan = rtlefuse -> eeprom_channelplan ; <nl>  <nl> /* parse xtal */ <nl> rtlefuse -> crystalcap = hwinfo [ EEPROM_XTAL_8821AE ];
int read_log ( struct tpm_bios_log * log ) <nl> log -> bios_event_log_end = log -> bios_event_log + len ; <nl>  <nl> virt = acpi_os_map_memory ( start , len ); <nl> + if (! virt ) { <nl> + kfree ( log -> bios_event_log ); <nl> + printk ("% s : ERROR - Unable to map memory \ n ", __func__ ); <nl> + return - EIO ; <nl> + } <nl>  <nl> memcpy ( log -> bios_event_log , virt , len ); <nl> 
static struct collection collections [] = { <nl>  <nl> /* Iterate over all benchmarks within a collection : */ <nl> # define for_each_bench ( coll , bench ) \ <nl> - for ( bench = coll -> benchmarks ; bench -> name ; bench ++) <nl> + for ( bench = coll -> benchmarks ; bench && bench -> name ; bench ++) <nl>  <nl> static void dump_benchmarks ( struct collection * coll ) <nl> {
static int pcnet32_open ( struct net_device * dev ) <nl> lp -> rx_dma_addr [ i ] = 0 ; <nl> } <nl>  <nl> - pcnet32_free_ring ( dev ); <nl> - <nl> /* <nl> * Switch back to 16bit mode to avoid problems with dumb <nl> * DOS packet driver after a warm reboot
static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> } <nl> logical = re -> logical ; <nl>  <nl> - reada_extent_put ( fs_info , re ); <nl> - <nl> atomic_inc (& dev -> reada_in_flight ); <nl> ret = reada_tree_block_flagged ( fs_info -> extent_root , logical , <nl> mirror_num , & eb ); <nl> static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> if ( eb ) <nl> free_extent_buffer ( eb ); <nl>  <nl> + reada_extent_put ( fs_info , re ); <nl> + <nl> return 1 ; <nl>  <nl> }
retry : <nl> ! atomic_inc_not_zero (& ctx -> refcount )) { <nl> raw_spin_unlock (& ctx -> lock ); <nl> ctx = NULL ; <nl> + } else { <nl> + WARN_ON_ONCE ( ctx -> task != task ); <nl> } <nl> - <nl> - WARN_ON_ONCE ( ctx -> task != task ); <nl> } <nl> rcu_read_unlock (); <nl> if (! ctx )
static IIO_DEV_ATTR_SAMP_FREQ ( S_IWUSR | S_IRUGO , <nl> adis16400_read_frequency , <nl> adis16400_write_frequency ); <nl>  <nl> - static IIO_CONST_ATTR_SAMP_FREQ_AVAIL (" 409 546 819 1638 "); <nl> - <nl> static const u8 adis16400_addresses [] = { <nl> [ ADIS16400_SCAN_GYRO_X ] = ADIS16400_XGYRO_OFF , <nl> [ ADIS16400_SCAN_GYRO_Y ] = ADIS16400_YGYRO_OFF , <nl> static const struct iio_chan_spec adis16334_channels [] = { <nl>  <nl> static struct attribute * adis16400_attributes [] = { <nl> & iio_dev_attr_sampling_frequency . dev_attr . attr , <nl> - & iio_const_attr_sampling_frequency_available . dev_attr . attr , <nl> NULL <nl> }; <nl> 
__xfrm4_bundle_create ( struct xfrm_policy * policy , struct xfrm_state ** xfrm , int <nl> afinfo = xfrm_state_get_afinfo ( dst_prev -> xfrm -> props . family ); <nl> if (! afinfo ) { <nl> dst = * dst_p ; <nl> + err = - EAFNOSUPPORT ; <nl> goto error ; <nl> } <nl> dst_prev -> output = afinfo -> output ;
static void pxa3xx_nand_cmdfunc ( struct mtd_info * mtd , unsigned command , <nl> /* disable HW ECC to get all the OOB data */ <nl> info -> buf_count = mtd -> writesize + mtd -> oobsize ; <nl> info -> buf_start = mtd -> writesize + column ; <nl> + memset ( info -> data_buff , 0xFF , info -> buf_count ); <nl>  <nl> if ( prepare_read_prog_cmd ( info , cmdset -> read1 , column , page_addr )) <nl> break ;
int sctp_sysctl_net_register ( struct net * net ) <nl> table [ i ]. data += ( char *)(& net -> sctp ) - ( char *)& init_net . sctp ; <nl>  <nl> net -> sctp . sysctl_header = register_net_sysctl ( net , " net / sctp ", table ); <nl> + if ( net -> sctp . sysctl_header == NULL ) { <nl> + kfree ( table ); <nl> + return - ENOMEM ; <nl> + } <nl> return 0 ; <nl> } <nl> 
struct kvm_vcpu_arch { <nl> struct kvm_mmu_memory_cache mmu_page_cache ; <nl>  <nl> /* Target CPU and feature flags */ <nl> - u32 target ; <nl> + int target ; <nl> DECLARE_BITMAP ( features , KVM_VCPU_MAX_FEATURES ); <nl>  <nl> /* Detect first run of a vcpu */
static void parse_dacl ( struct cifs_acl * pdacl , char * end_of_acl , <nl> umode_t group_mask = S_IRWXG ; <nl> umode_t other_mask = S_IRWXU | S_IRWXG | S_IRWXO ; <nl>  <nl> + if ( num_aces > ULONG_MAX / sizeof ( struct cifs_ace *)) <nl> + return ; <nl> ppace = kmalloc ( num_aces * sizeof ( struct cifs_ace *), <nl> GFP_KERNEL ); <nl> if (! ppace ) {
intel_pin_and_fence_fb_obj ( struct drm_device * dev , <nl> u32 alignment ; <nl> int ret ; <nl>  <nl> + WARN_ON (! mutex_is_locked (& dev -> struct_mutex )); <nl> + <nl> switch ( obj -> tiling_mode ) { <nl> case I915_TILING_NONE : <nl> if ( IS_BROADWATER ( dev ) || IS_CRESTLINE ( dev )) <nl> err_interruptible : <nl>  <nl> void intel_unpin_fb_obj ( struct drm_i915_gem_object * obj ) <nl> { <nl> + WARN_ON (! mutex_is_locked (& obj -> base . dev -> struct_mutex )); <nl> + <nl> i915_gem_object_unpin_fence ( obj ); <nl> i915_gem_object_unpin_from_display_plane ( obj ); <nl> }
int snd_pcm_status ( struct snd_pcm_substream * substream , <nl> runtime -> status -> audio_tstamp ; <nl> goto _tstamp_end ; <nl> } <nl> + } else { <nl> + /* get tstamp only in fallback mode and only if enabled */ <nl> + if ( runtime -> tstamp_mode == SNDRV_PCM_TSTAMP_ENABLE ) <nl> + snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> } <nl> - snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> _tstamp_end : <nl> status -> appl_ptr = runtime -> control -> appl_ptr ; <nl> status -> hw_ptr = runtime -> status -> hw_ptr ;
static irqreturn_t et131x_isr ( int irq , void * dev_id ) <nl> { <nl> bool handled = true ; <nl> bool enable_interrupts = true ; <nl> - struct net_device * netdev = ( struct net_device *) dev_id ; <nl> + struct net_device * netdev = dev_id ; <nl> struct et131x_adapter * adapter = netdev_priv ( netdev ); <nl> struct address_map __iomem * iomem = adapter -> regs ; <nl> struct rx_ring * rx_ring = & adapter -> rx_ring ;
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl> ! iwlwifi_mod_params . sw_crypto ) <nl> hw -> flags |= IEEE80211_HW_MFP_CAPABLE ; <nl>  <nl> - if ( mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> + if ( 0 && mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> hw -> flags |= IEEE80211_HW_SUPPORTS_UAPSD ; <nl> hw -> uapsd_queues = IWL_UAPSD_AC_INFO ; <nl> hw -> uapsd_max_sp_len = IWL_UAPSD_MAX_SP ;
static int generic_set_freq ( struct dvb_frontend * fe , <nl> goto err ; <nl>  <nl> rc = r820t_sysfreq_sel ( priv , freq , type , std , delsys ); <nl> + if ( rc < 0 ) <nl> + goto err ; <nl> + <nl> + tuner_dbg ("% s : PLL locked on frequency % d Hz , gain =% d \ n ", <nl> + __func__ , freq , r820t_read_gain ( priv )); <nl> + <nl> err : <nl>  <nl> if ( rc < 0 )
xfs_rename ( <nl> if ( unlikely (( target_dp -> i_d . di_flags & XFS_DIFLAG_PROJINHERIT ) && <nl> ( target_dp -> i_d . di_projid != src_ip -> i_d . di_projid ))) { <nl> error = XFS_ERROR ( EXDEV ); <nl> - xfs_rename_unlock4 ( inodes , XFS_ILOCK_SHARED ); <nl> + xfs_rename_unlock4 ( inodes , XFS_ILOCK_EXCL ); <nl> xfs_trans_cancel ( tp , cancel_flags ); <nl> goto std_return ; <nl> }
static ssize_t bonding_store_slaves ( struct device * d , <nl>  <nl> if ( command [ 0 ] == '-') { <nl> dev = NULL ; <nl> + original_mtu = 0 ; <nl> bond_for_each_slave ( bond , slave , i ) <nl> if ( strnicmp ( slave -> dev -> name , ifname , IFNAMSIZ ) == 0 ) { <nl> dev = slave -> dev ;
static void ni_660x_handle_gpct_interrupt ( struct comedi_device * dev , <nl> struct ni_gpct * counter = s -> private ; <nl>  <nl> ni_tio_handle_interrupt ( counter , s ); <nl> - cfc_handle_events ( dev , s ); <nl> + comedi_handle_events ( dev , s ); <nl> } <nl>  <nl> static irqreturn_t ni_660x_interrupt ( int irq , void * d )
free_interfaces : <nl> intf -> dev . bus_id , ret ); <nl> continue ; <nl> } <nl> - usb_create_sysfs_intf_files ( intf ); <nl> + <nl> + /* The driver ' s probe method can call usb_set_interface (), <nl> + * which would mean the interface ' s sysfs files are already <nl> + * created . Just in case , we ' ll remove them first . <nl> + */ <nl> + usb_remove_sysfs_intf_files ( intf ); <nl> + usb_create_sysfs_intf_files ( intf ); <nl> } <nl>  <nl> usb_autosuspend_device ( dev );
static int pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> free_press_irq : <nl> - free_irq ( key_press_irq , NULL ); <nl> + free_irq ( key_press_irq , pwrkey ); <nl> unreg_input_dev : <nl> input_unregister_device ( pwr ); <nl> pwr = NULL ;
enum pcpu_fc pcpu_chosen_fc __initdata = PCPU_FC_AUTO ; <nl>  <nl> static int __init percpu_alloc_setup ( char * str ) <nl> { <nl> + if (! str ) <nl> + return - EINVAL ; <nl> + <nl> if ( 0 ) <nl> /* nada */; <nl> # ifdef CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK
void led_trigger_unregister ( struct led_trigger * trigger ) <nl>  <nl> void led_trigger_unregister_simple ( struct led_trigger * trigger ) <nl> { <nl> - led_trigger_unregister ( trigger ); <nl> + if ( trigger ) <nl> + led_trigger_unregister ( trigger ); <nl> kfree ( trigger ); <nl> } <nl> 
static int pm860x_probe ( struct snd_soc_codec * codec ) <nl> } <nl> } <nl>  <nl> - pm860x_set_bias_level ( codec , SND_SOC_BIAS_STANDBY ); <nl> - <nl> return 0 ; <nl>  <nl> out : <nl> static int pm860x_remove ( struct snd_soc_codec * codec ) <nl>  <nl> for ( i = 3 ; i >= 0 ; i --) <nl> free_irq ( pm860x -> irq [ i ], pm860x ); <nl> - pm860x_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> return 0 ; <nl> } <nl> 
static void do_writes ( struct mirror_set * ms , struct bio_list * writes ) <nl> /* <nl> * Dispatch io . <nl> */ <nl> - if ( unlikely ( ms -> log_failure )) { <nl> + if ( unlikely ( ms -> log_failure ) && errors_handled ( ms )) { <nl> spin_lock_irq (& ms -> lock ); <nl> bio_list_merge (& ms -> failures , & sync ); <nl> spin_unlock_irq (& ms -> lock );
static DEFINE_MUTEX ( nb_smu_ind_mutex ); <nl> * Control ] <nl> */ <nl> # define F15H_M60H_REPORTED_TEMP_CTRL_OFFSET 0xd8200ca4 <nl> -# define PCI_DEVICE_ID_AMD_15H_M60H_NB_F3 0x1573 <nl>  <nl> static void amd_nb_smu_index_read ( struct pci_dev * pdev , unsigned int devfn , <nl> int offset , u32 * val )
void dump_trace ( struct task_struct * task , <nl> unsigned used = 0 ; <nl> struct thread_info * tinfo ; <nl> int graph = 0 ; <nl> + unsigned long dummy ; <nl> unsigned long bp ; <nl>  <nl> if (! task ) <nl> task = current ; <nl>  <nl> if (! stack ) { <nl> - unsigned long dummy ; <nl> stack = & dummy ; <nl> if ( task && task != current ) <nl> stack = ( unsigned long *) task -> thread . sp ;
void dynamic_irq_cleanup ( unsigned int irq ) <nl> desc -> chip_data = NULL ; <nl> desc -> handle_irq = handle_bad_irq ; <nl> desc -> chip = & no_irq_chip ; <nl> + desc -> name = NULL ; <nl> spin_unlock_irqrestore (& desc -> lock , flags ); <nl> } <nl> 
static int cpsw_poll ( struct napi_struct * napi , int budget ) <nl> cpdma_ctlr_eoi ( priv -> dma , CPDMA_EOI_RX ); <nl> prim_cpsw = cpsw_get_slave_priv ( priv , 0 ); <nl> if ( prim_cpsw -> irq_enabled == false ) { <nl> - cpsw_enable_irq ( priv ); <nl> prim_cpsw -> irq_enabled = true ; <nl> + cpsw_enable_irq ( priv ); <nl> } <nl> } <nl> 
int bnx2i_send_iscsi_nopout ( struct bnx2i_conn * bnx2i_conn , <nl> bnx2i_cmd = ( struct bnx2i_cmd *) task -> dd_data ; <nl> nopout_hdr = ( struct iscsi_nopout *) task -> hdr ; <nl> nopout_wqe = ( struct bnx2i_nop_out_request *) ep -> qp . sq_prod_qe ; <nl> + <nl> + memset ( nopout_wqe , 0x00 , sizeof ( struct bnx2i_nop_out_request )); <nl> + <nl> nopout_wqe -> op_code = nopout_hdr -> opcode ; <nl> nopout_wqe -> op_attr = ISCSI_FLAG_CMD_FINAL ; <nl> memcpy ( nopout_wqe -> lun , nopout_hdr -> lun , 8 );
struct Qdisc_ops <nl>  <nl> int (* enqueue )( struct sk_buff *, struct Qdisc *); <nl> struct sk_buff * (* dequeue )( struct Qdisc *); <nl> + struct sk_buff * (* peek )( struct Qdisc *); <nl> int (* requeue )( struct sk_buff *, struct Qdisc *); <nl> unsigned int (* drop )( struct Qdisc *); <nl> 
int pstore_register ( struct pstore_info * psi ) <nl> add_timer (& pstore_timer ); <nl> } <nl>  <nl> + /* <nl> + * Update the module parameter backend , so it is visible <nl> + * through / sys / module / pstore / parameters / backend <nl> + */ <nl> + backend = psi -> name ; <nl> + <nl> pr_info (" Registered % s as persistent store backend \ n ", psi -> name ); <nl>  <nl> return 0 ;
static int dasd_eer_open ( struct inode * inp , struct file * filp ) <nl> unsigned long flags ; <nl>  <nl> eerb = kzalloc ( sizeof ( struct eerbuffer ), GFP_KERNEL ); <nl> + if (! eerb ) <nl> + return - ENOMEM ; <nl> eerb -> buffer_page_count = eer_pages ; <nl> if ( eerb -> buffer_page_count < 1 || <nl> eerb -> buffer_page_count > INT_MAX / PAGE_SIZE ) {
static void vlv_display_power_well_deinit ( struct drm_i915_private * dev_priv ) <nl> valleyview_disable_display_irqs ( dev_priv ); <nl> spin_unlock_irq (& dev_priv -> irq_lock ); <nl>  <nl> + /* make sure we ' re done processing display irqs */ <nl> + synchronize_irq ( dev_priv -> dev -> irq ); <nl> + <nl> vlv_power_sequencer_reset ( dev_priv ); <nl> } <nl> 
struct extent_buffer * alloc_extent_buffer ( struct extent_io_tree * tree , <nl> spin_unlock (& tree -> buffer_lock ); <nl> goto free_eb ; <nl> } <nl> - spin_unlock (& tree -> buffer_lock ); <nl> - <nl> /* add one reference for the tree */ <nl> atomic_inc (& eb -> refs ); <nl> + spin_unlock (& tree -> buffer_lock ); <nl> return eb ; <nl>  <nl> free_eb :
int ide_device_add ( u8 idx [ 4 ]) <nl>  <nl> hwif = & ide_hwifs [ idx [ i ]]; <nl>  <nl> - if ( hwif -> present ) <nl> + if ( hwif -> present ) { <nl> + if ( hwif -> chipset == ide_unknown || <nl> + hwif -> chipset == ide_forced ) <nl> + hwif -> chipset = ide_generic ; <nl> hwif_register_devices ( hwif ); <nl> + } <nl> } <nl>  <nl> for ( i = 0 ; i < 4 ; i ++) {
static inline void check_for_tasks ( int cpu ) <nl>  <nl> write_lock_irq (& tasklist_lock ); <nl> for_each_process ( p ) { <nl> - if ( task_cpu ( p ) == cpu && <nl> + if ( task_cpu ( p ) == cpu && p -> state == TASK_RUNNING && <nl> (! cputime_eq ( p -> utime , cputime_zero ) || <nl> ! cputime_eq ( p -> stime , cputime_zero ))) <nl> printk ( KERN_WARNING " Task % s ( pid = % d ) is on cpu % d \
static void __devinit check_probe_mask ( struct azx * chip , int dev ) <nl> * white / black - list for enable_msi <nl> */ <nl> static struct snd_pci_quirk msi_black_list [] __devinitdata = { <nl> + SND_PCI_QUIRK ( 0x1043 , 0x81f2 , " ASUS ", 0 ), /* Athlon64 X2 + nvidia */ <nl> {} <nl> }; <nl> 
static ctl_table vm_table [] = { <nl> . extra2 = & one_hundred , <nl> }, <nl> # endif <nl> -# ifdef CONFIG_X86_32 <nl> +# if defined ( CONFIG_X86_32 ) || \ <nl> + ( defined ( CONFIG_SUPERH ) && defined ( CONFIG_VSYSCALL )) <nl> { <nl> . ctl_name = VM_VDSO_ENABLED , <nl> . procname = " vdso_enabled ",
static int iio_device_add_event ( struct iio_dev * indio_dev , <nl> & indio_dev -> event_interface -> dev_attr_list ); <nl> kfree ( postfix ); <nl>  <nl> + if (( ret == - EBUSY ) && ( shared_by != IIO_SEPARATE )) <nl> + continue ; <nl> + <nl> if ( ret ) <nl> return ret ; <nl> 
static void intel_dp_prepare ( struct drm_encoder * encoder ) <nl> uint32_t dp_reg = I915_READ ( intel_dp -> output_reg ); <nl>  <nl> if ( IS_eDP ( intel_dp ) || IS_PCH_eDP ( intel_dp )) { <nl> + ironlake_edp_panel_off ( dev ); <nl> ironlake_edp_backlight_off ( dev ); <nl> ironlake_edp_panel_vdd_on ( dev ); <nl> ironlake_edp_pll_on ( encoder );
bool i40e_dcb_need_reconfig ( struct i40e_pf * pf , <nl> /* Check if APP Table has changed */ <nl> if ( memcmp (& new_cfg -> app , <nl> & old_cfg -> app , <nl> - sizeof ( new_cfg -> app ))) <nl> + sizeof ( new_cfg -> app ))) { <nl> need_reconfig = true ; <nl> dev_info (& pf -> pdev -> dev , " APP Table change detected .\ n "); <nl> + } <nl>  <nl> return need_reconfig ; <nl> }
void __init clocksource_of_init ( void ) <nl> clocksource_of_init_fn init_func ; <nl>  <nl> for_each_matching_node_and_match ( np , __clksrc_of_table , & match ) { <nl> + if (! of_device_is_available ( np )) <nl> + continue ; <nl> + <nl> init_func = match -> data ; <nl> init_func ( np ); <nl> }
static int balloon ( void * _vballoon ) <nl> try_to_freeze (); <nl> wait_event_interruptible ( vb -> config_change , <nl> ( diff = towards_target ( vb )) != 0 <nl> - || kthread_should_stop ()); <nl> + || kthread_should_stop () <nl> + || freezing ( current )); <nl> if ( diff > 0 ) <nl> fill_balloon ( vb , diff ); <nl> else if ( diff < 0 )
static void mousevsc_on_channel_callback ( void * context ) <nl> static int mousevsc_connect_to_vsp ( struct hv_device * device ) <nl> { <nl> int ret = 0 ; <nl> - int t ; <nl> + unsigned long t ; <nl> struct mousevsc_dev * input_dev = hv_get_drvdata ( device ); <nl> struct mousevsc_prt_msg * request ; <nl> struct mousevsc_prt_msg * response ;
int radeon_atombios_init ( struct radeon_device * rdev ) <nl>  <nl> void radeon_atombios_fini ( struct radeon_device * rdev ) <nl> { <nl> - kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> - kfree ( rdev -> mode_info . atom_context ); <nl> + if ( rdev -> mode_info . atom_context ) { <nl> + kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> + kfree ( rdev -> mode_info . atom_context ); <nl> + } <nl> kfree ( rdev -> mode_info . atom_card_info ); <nl> } <nl> 
static int do_ip_getsockopt ( struct sock * sk , int level , int optname , <nl> case IP_HDRINCL : <nl> val = inet -> hdrincl ; <nl> break ; <nl> + case IP_NODEFRAG : <nl> + val = inet -> nodefrag ; <nl> + break ; <nl> case IP_MTU_DISCOVER : <nl> val = inet -> pmtudisc ; <nl> break ;
struct btrfs_root * btrfs_read_fs_root_no_radix ( struct btrfs_root * tree_root , <nl> } <nl> btrfs_free_path ( path ); <nl> if ( ret ) { <nl> + kfree ( root ); <nl> if ( ret > 0 ) <nl> ret = - ENOENT ; <nl> return ERR_PTR ( ret );
static int rt2800_init_registers ( struct rt2x00_dev * rt2x00dev ) <nl>  <nl> rt2800_register_read ( rt2x00dev , MM40_PROT_CFG , & reg ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_RATE , 0x4084 ); <nl> - rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , <nl> - ! rt2x00_is_usb ( rt2x00dev )); <nl> + rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , 0 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_NAV , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_CCK , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_OFDM , 1 );
int btrfs_orphan_add ( struct btrfs_trans_handle * trans , struct inode * inode ) <nl> /* insert an orphan item to track this unlinked / truncated file */ <nl> if ( insert >= 1 ) { <nl> ret = btrfs_insert_orphan_item ( trans , root , btrfs_ino ( inode )); <nl> - BUG_ON ( ret ); <nl> + BUG_ON ( ret && ret != - EEXIST ); <nl> } <nl>  <nl> /* insert an orphan item to track subvolume contains orphan files */
static enum BC_STATUS bc_cproc_download_fw ( struct crystalhd_cmd * ctx , <nl> sts = crystalhd_download_fw ( ctx -> adp , ( uint8_t *) idata -> add_cdata , <nl> idata -> add_cdata_sz ); <nl>  <nl> - if ( sts != BC_STS_SUCCESS ) { <nl> + if ( sts != BC_STS_SUCCESS ) <nl> BCMLOG_ERR (" Firmware Download Failure !! - % d \ n ", sts ); <nl> - } else <nl> + else <nl> ctx -> state |= BC_LINK_INIT ; <nl>  <nl> return sts ;
static int ibmvfc_map_sg_data ( struct scsi_cmnd * scmd , <nl> & evt -> ext_list_token ); <nl>  <nl> if (! evt -> ext_list ) { <nl> - scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> + scsi_dma_unmap ( scmd ); <nl> + if ( vhost -> log_level > IBMVFC_DEFAULT_LOG_LEVEL ) <nl> + scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> return - ENOMEM ; <nl> } <nl> }
static int uvc_v4l2_put_xu_mapping ( const struct uvc_xu_control_mapping * kp , <nl> __put_user ( kp -> menu_count , & up -> menu_count )) <nl> return - EFAULT ; <nl>  <nl> - __clear_user ( up -> reserved , sizeof ( up -> reserved )); <nl> + if ( __clear_user ( up -> reserved , sizeof ( up -> reserved ))) <nl> + return - EFAULT ; <nl>  <nl> if ( kp -> menu_count == 0 ) <nl> return 0 ;
int vb2_mmap ( struct vb2_queue * q , struct vm_area_struct * vma ) <nl> { <nl> unsigned long off = vma -> vm_pgoff << PAGE_SHIFT ; <nl> struct vb2_buffer * vb ; <nl> - unsigned int buffer , plane ; <nl> + unsigned int buffer = 0 , plane = 0 ; <nl> int ret ; <nl> unsigned long length ; <nl> 
static struct ieee80211_ops agnx_ops = { <nl> static void __devexit agnx_pci_remove ( struct pci_dev * pdev ) <nl> { <nl> struct ieee80211_hw * dev = pci_get_drvdata ( pdev ); <nl> - struct agnx_priv * priv = dev -> priv ; <nl> + struct agnx_priv * priv ; <nl> AGNX_TRACE ; <nl>  <nl> if (! dev ) <nl> return ; <nl> + priv = dev -> priv ; <nl> ieee80211_unregister_hw ( dev ); <nl> pci_iounmap ( pdev , priv -> ctl ); <nl> pci_iounmap ( pdev , priv -> data );
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
static const __u8 uvc_camera_guid [ 16 ] = UVC_GUID_UVC_CAMERA ; <nl> static const __u8 uvc_media_transport_input_guid [ 16 ] = <nl> UVC_GUID_UVC_MEDIA_TRANSPORT_INPUT ; <nl>  <nl> - static int uvc_entity_match_guid ( struct uvc_entity * entity , __u8 guid [ 16 ]) <nl> + static int uvc_entity_match_guid ( const struct uvc_entity * entity , <nl> + const __u8 guid [ 16 ]) <nl> { <nl> switch ( UVC_ENTITY_TYPE ( entity )) { <nl> case UVC_ITT_CAMERA :
int seq_bitmap ( struct seq_file * m , const unsigned long * bits , <nl> unsigned int nr_bits ); <nl> static inline int seq_cpumask ( struct seq_file * m , const struct cpumask * mask ) <nl> { <nl> - return seq_bitmap ( m , mask -> bits , NR_CPUS ); <nl> + return seq_bitmap ( m , mask -> bits , nr_cpu_ids ); <nl> } <nl>  <nl> static inline int seq_nodemask ( struct seq_file * m , nodemask_t * mask )
static int run ( mddev_t * mddev ) <nl> int i ; <nl>  <nl> conf_t * conf = kmalloc ( sizeof (* conf ), GFP_KERNEL ); <nl> + if (! conf ) <nl> + return - ENOMEM ; <nl>  <nl> for ( i = 0 ; i < Modes ; i ++) { <nl> atomic_set (& conf -> counters [ i ], 0 );
static int soc_hw_match_params ( struct snd_pcm_substream * substream , <nl> /* normalise cpu bfs div & codec const mult */ <nl> codec_bfs = soc_bfs_rate_to_div ( codec_dai_mode -> bfs , rate , <nl> mclk , rtd -> codec_dai -> dai_runtime . pcmfmt , chn ); <nl> - if ( codec_dai_mode -> bfs & codec_bfs ) { <nl> + if ( cpu_dai_mode -> bfs & codec_bfs ) { <nl> rtd -> cpu_dai -> dai_runtime . bfs = codec_bfs ; <nl> rtd -> codec_dai -> dai_runtime . bfs = codec_dai_mode -> bfs ; <nl> } else
static void w9966_term ( struct w9966 * cam ) <nl> parport_unregister_device ( cam -> pdev ); <nl> w9966_set_state ( cam , W9966_STATE_PDEV , 0 ); <nl> } <nl> + memset ( cam , 0 , sizeof (* cam )); <nl> } <nl>  <nl> 
static void dwc3_endpoint_interrupt ( struct dwc3 * dwc , <nl>  <nl> dep = dwc -> eps [ epnum ]; <nl>  <nl> + if (!( dep -> flags & DWC3_EP_ENABLED )) <nl> + return ; <nl> + <nl> dev_vdbg ( dwc -> dev , "% s : % s \ n ", dep -> name , <nl> dwc3_ep_event_string ( event -> endpoint_event )); <nl> 
void * __init alloc_large_system_hash ( const char * tablename , <nl> table = __vmalloc ( size , GFP_ATOMIC , PAGE_KERNEL ); <nl> else { <nl> unsigned long order = get_order ( size ); <nl> - table = ( void *) __get_free_pages ( GFP_ATOMIC , order ); <nl> + <nl> + if ( order < MAX_ORDER ) <nl> + table = ( void *) __get_free_pages ( GFP_ATOMIC , <nl> + order ); <nl> /* <nl> * If bucketsize is not a power - of - two , we may free <nl> * some pages at the end of hash table .
static inline void run_hrtimer_queue ( struct hrtimer_base * base ) <nl> { <nl> struct rb_node * node ; <nl>  <nl> + if (! base -> first ) <nl> + return ; <nl> + <nl> if ( base -> get_softirq_time ) <nl> base -> softirq_time = base -> get_softirq_time (); <nl> 
static struct sk_buff * be_insert_vlan_in_pkt ( struct be_adapter * adapter , <nl>  <nl> if ( vlan_tx_tag_present ( skb )) { <nl> vlan_tag = be_get_tx_vlan_tag ( adapter , skb ); <nl> - __vlan_put_tag ( skb , vlan_tag ); <nl> - skb -> vlan_tci = 0 ; <nl> + skb = __vlan_put_tag ( skb , vlan_tag ); <nl> + if ( skb ) <nl> + skb -> vlan_tci = 0 ; <nl> } <nl>  <nl> return skb ;
int ip6_del_rt ( struct rt6_info * rt , struct nlmsghdr * nlh , void * _rtattr , struct <nl> int err ; <nl> struct fib6_table * table ; <nl>  <nl> + if ( rt == & ip6_null_entry ) <nl> + return - ENOENT ; <nl> + <nl> table = rt -> rt6i_table ; <nl> write_lock_bh (& table -> tb6_lock ); <nl> 
int perf_event_init_context ( struct task_struct * child , int ctxn ) <nl> * swapped under us . <nl> */ <nl> parent_ctx = perf_pin_task_context ( parent , ctxn ); <nl> + if (! parent_ctx ) <nl> + return 0 ; <nl>  <nl> /* <nl> * No need to check if parent_ctx != NULL here ; since we saw
static int tg3_run_loopback ( struct tg3 * tp , int loopback_mode ) <nl> } <nl> mac_mode = ( tp -> mac_mode & ~ MAC_MODE_PORT_MODE_MASK ) | <nl> MAC_MODE_LINK_POLARITY | MAC_MODE_PORT_MODE_GMII ; <nl> - if (( tp -> phy_id & PHY_ID_MASK ) == PHY_ID_BCM5401 ) <nl> + if (( tp -> phy_id & PHY_ID_MASK ) == PHY_ID_BCM5401 ) { <nl> mac_mode &= ~ MAC_MODE_LINK_POLARITY ; <nl> + tg3_writephy ( tp , MII_TG3_EXT_CTRL , <nl> + MII_TG3_EXT_CTRL_LNK3_LED_MODE ); <nl> + } <nl> tw32 ( MAC_MODE , mac_mode ); <nl> } <nl> else
static struct crypto_instance * crypto_cts_alloc ( struct rtattr ** tb ) <nl> if (! is_power_of_2 ( alg -> cra_blocksize )) <nl> goto out_put_alg ; <nl>  <nl> + if ( strncmp ( alg -> cra_name , " cbc (", 4 )) <nl> + goto out_put_alg ; <nl> + <nl> inst = crypto_alloc_instance (" cts ", alg ); <nl> if ( IS_ERR ( inst )) <nl> goto out_put_alg ;
int bench_sched_messaging ( int argc , const char ** argv , <nl> break ; <nl> } <nl>  <nl> + free ( pth_tab ); <nl> + <nl> return 0 ; <nl> }
static int sh_eth_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> skb -> len + 2 ); <nl> txdesc -> addr = dma_map_single (& ndev -> dev , skb -> data , skb -> len , <nl> DMA_TO_DEVICE ); <nl> + if ( dma_mapping_error (& ndev -> dev , txdesc -> addr )) { <nl> + kfree_skb ( skb ); <nl> + return NETDEV_TX_OK ; <nl> + } <nl> txdesc -> buffer_length = skb -> len ; <nl>  <nl> if ( entry >= mdp -> num_tx_ring - 1 )
static int __init lart_flash_init ( void ) <nl> mtd . name = module_name ; <nl> mtd . type = MTD_NORFLASH ; <nl> mtd . writesize = 1 ; <nl> + mtd . writebufsize = 4 ; <nl> mtd . flags = MTD_CAP_NORFLASH ; <nl> mtd . size = FLASH_BLOCKSIZE_PARAM * FLASH_NUMBLOCKS_16m_PARAM + FLASH_BLOCKSIZE_MAIN * FLASH_NUMBLOCKS_16m_MAIN ; <nl> mtd . erasesize = FLASH_BLOCKSIZE_MAIN ;
static int __init usba_udc_probe ( struct platform_device * pdev ) <nl> usba_writel ( udc , CTRL , USBA_DISABLE_MASK ); <nl> clk_disable ( pclk ); <nl>  <nl> - usba_ep = kmalloc ( sizeof ( struct usba_ep ) * pdata -> num_ep , <nl> + usba_ep = kzalloc ( sizeof ( struct usba_ep ) * pdata -> num_ep , <nl> GFP_KERNEL ); <nl> if (! usba_ep ) <nl> goto err_alloc_ep ;
struct tracepoint { <nl> do { \ <nl> void ** it_func ; \ <nl> \ <nl> - rcu_read_lock_sched (); \ <nl> + rcu_read_lock_sched_notrace (); \ <nl> it_func = rcu_dereference (( tp )-> funcs ); \ <nl> if ( it_func ) { \ <nl> do { \ <nl> (( void (*)( proto ))(* it_func ))( args ); \ <nl> } while (*(++ it_func )); \ <nl> } \ <nl> - rcu_read_unlock_sched (); \ <nl> + rcu_read_unlock_sched_notrace (); \ <nl> } while ( 0 ) <nl>  <nl> /*
static int __devinit palmas_i2c_probe ( struct i2c_client * i2c , <nl> goto err ; <nl> } <nl>  <nl> + children [ PALMAS_PMIC_ID ]. platform_data = pdata -> pmic_pdata ; <nl> + children [ PALMAS_PMIC_ID ]. pdata_size = sizeof (* pdata -> pmic_pdata ); <nl> + <nl> ret = mfd_add_devices ( palmas -> dev , - 1 , <nl> children , ARRAY_SIZE ( palmas_children ), <nl> NULL , regmap_irq_chip_get_base ( palmas -> irq_data ));
static void exynos5_powerdown_conf ( enum sys_powerdown mode ) <nl> void exynos_sys_powerdown_conf ( enum sys_powerdown mode ) <nl> { <nl> unsigned int i ; <nl> + const struct exynos_pmu_data * pmu_data ; <nl> + <nl> + if (! pmu_context ) <nl> + return ; <nl>  <nl> - const struct exynos_pmu_data * pmu_data = pmu_context -> pmu_data ; <nl> + pmu_data = pmu_context -> pmu_data ; <nl>  <nl> if ( pmu_data -> powerdown_conf ) <nl> pmu_data -> powerdown_conf ( mode );
static const struct file_operations bm_register_operations = { <nl> static ssize_t <nl> bm_status_read ( struct file * file , char __user * buf , size_t nbytes , loff_t * ppos ) <nl> { <nl> - char * s = enabled ? " enabled " : " disabled "; <nl> + char * s = enabled ? " enabled \ n " : " disabled \ n "; <nl>  <nl> return simple_read_from_buffer ( buf , nbytes , ppos , s , strlen ( s )); <nl> }
static int radeon_uvd_cs_reloc ( struct radeon_cs_parser * p , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (( start >> 28 ) != ( end >> 28 )) { <nl> + if (( start >> 28 ) != (( end - 1 ) >> 28 )) { <nl> DRM_ERROR (" reloc % LX -% LX crossing 256MB boundary !\ n ", <nl> start , end ); <nl> return - EINVAL ;
static void i9xx_enable_pll ( struct intel_crtc * crtc ) <nl> I915_READ ( DPLL (! crtc -> pipe )) | DPLL_DVO_2X_MODE ); <nl> } <nl>  <nl> + I915_WRITE ( reg , dpll ); <nl> + <nl> /* Wait for the clocks to stabilize . */ <nl> POSTING_READ ( reg ); <nl> udelay ( 150 );
static size_t jffs2_trusted_listxattr ( struct dentry * dentry , char * list , <nl> { <nl> size_t retlen = XATTR_TRUSTED_PREFIX_LEN + name_len + 1 ; <nl>  <nl> + if (! capable ( CAP_SYS_ADMIN )) <nl> + return 0 ; <nl> + <nl> if ( list && retlen <= list_size ) { <nl> strcpy ( list , XATTR_TRUSTED_PREFIX ); <nl> strcpy ( list + XATTR_TRUSTED_PREFIX_LEN , name );
static struct drm_driver driver = { <nl> . minor = DRIVER_MINOR , <nl> . patchlevel = DRIVER_PATCHLEVEL , <nl>  <nl> - . gem_free_object = mgag200_gem_free_object , <nl> + . gem_free_object_unlocked = mgag200_gem_free_object , <nl> . dumb_create = mgag200_dumb_create , <nl> . dumb_map_offset = mgag200_dumb_mmap_offset , <nl> . dumb_destroy = drm_gem_dumb_destroy ,
out_device_destroy : <nl> scsi_device_set_state ( sdev , SDEV_DEL ); <nl> transport_destroy_device (& sdev -> sdev_gendev ); <nl> put_device (& sdev -> sdev_dev ); <nl> + scsi_free_queue ( sdev -> request_queue ); <nl> put_device (& sdev -> sdev_gendev ); <nl> out : <nl> if ( display_failure_msg )
xfs_destroy_ioend ( <nl> } <nl>  <nl> if ( ioend -> io_iocb ) { <nl> + inode_dio_done ( ioend -> io_inode ); <nl> if ( ioend -> io_isasync ) { <nl> aio_complete ( ioend -> io_iocb , ioend -> io_error ? <nl> ioend -> io_error : ioend -> io_result , 0 ); <nl> } <nl> - inode_dio_done ( ioend -> io_inode ); <nl> } <nl>  <nl> mempool_free ( ioend , xfs_ioend_pool );
static int llc_ui_create ( struct net * net , struct socket * sock , int protocol ) <nl> struct sock * sk ; <nl> int rc = - ESOCKTNOSUPPORT ; <nl>  <nl> + if (! capable ( CAP_NET_RAW )) <nl> + return - EPERM ; <nl> + <nl> if ( net != & init_net ) <nl> return - EAFNOSUPPORT ; <nl> 
static void wacom_i4_parse_pen_report ( struct wacom_data * wdata , <nl>  <nl> switch ( data [ 1 ]) { <nl> case 0x80 : /* Out of proximity report */ <nl> - wdata -> tool = 0 ; <nl> input_report_key ( input , BTN_TOUCH , 0 ); <nl> input_report_abs ( input , ABS_PRESSURE , 0 ); <nl> input_report_key ( input , wdata -> tool , 0 ); <nl> + wdata -> tool = 0 ; <nl> input_sync ( input ); <nl> break ; <nl> case 0xC2 : /* Tool report */
static bool new_idmap_permitted ( const struct file * file , <nl> u32 id = new_map -> extent [ 0 ]. lower_first ; <nl> if ( cap_setid == CAP_SETUID ) { <nl> kuid_t uid = make_kuid ( ns -> parent , id ); <nl> - if ( uid_eq ( uid , file -> f_cred -> fsuid )) <nl> + if ( uid_eq ( uid , file -> f_cred -> euid )) <nl> return true ; <nl> } <nl> }
static ssize_t tcmu_set_configfs_dev_params ( struct se_device * dev , <nl> default : <nl> break ; <nl> } <nl> + <nl> + if ( ret ) <nl> + break ; <nl> } <nl>  <nl> kfree ( orig );
static int p9_virtio_probe ( struct virtio_device * vdev ) <nl> int err ; <nl> struct virtio_chan * chan ; <nl>  <nl> + if (! vdev -> config -> get ) { <nl> + dev_err (& vdev -> dev , "% s failure : config access disabled \ n ", <nl> + __func__ ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> chan = kmalloc ( sizeof ( struct virtio_chan ), GFP_KERNEL ); <nl> if (! chan ) { <nl> pr_err (" Failed to allocate virtio 9P channel \ n ");
static int old_capi_manufacturer ( unsigned int cmd , void __user * data ) <nl> return - EFAULT ; <nl> } <nl> card = get_capi_ctr_by_nr ( ldef . contr ); <nl> + if (! card ) <nl> + return - EINVAL ; <nl> card = capi_ctr_get ( card ); <nl> if (! card ) <nl> return - ESRCH ;
static void set_times ( struct tca6507_chip * tca , int bank ) <nl> int result ; <nl>  <nl> result = choose_times ( tca -> bank [ bank ]. ontime , & c1 , & c2 ); <nl> + if ( result < 0 ) <nl> + return ; <nl> dev_dbg (& tca -> client -> dev , <nl> " Chose on times % d (% d ) % d (% d ) for % dms \ n ", <nl> c1 , time_codes [ c1 ],
static int snd_hdspm_playback_open ( struct snd_pcm_substream * substream ) <nl> snd_pcm_hw_constraint_minmax ( runtime , <nl> SNDRV_PCM_HW_PARAM_PERIOD_SIZE , <nl> 64 , 8192 ); <nl> + snd_pcm_hw_constraint_minmax ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS , <nl> + 2 , 2 ); <nl> break ; <nl> } <nl>  <nl> static int snd_hdspm_capture_open ( struct snd_pcm_substream * substream ) <nl> snd_pcm_hw_constraint_minmax ( runtime , <nl> SNDRV_PCM_HW_PARAM_PERIOD_SIZE , <nl> 64 , 8192 ); <nl> + snd_pcm_hw_constraint_minmax ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS , <nl> + 2 , 2 ); <nl> break ; <nl> } <nl> 
int arizona_set_fll ( struct arizona_fll * fll , int source , <nl> if ( ena ) <nl> pm_runtime_put_autosuspend ( arizona -> dev ); <nl>  <nl> + fll -> fref = Fref ; <nl> + fll -> fout = Fout ; <nl> + <nl> return 0 ; <nl> } <nl> 
create_hw_context ( struct drm_device * dev , <nl> return ERR_PTR (- ENOMEM ); <nl> } <nl>  <nl> + if ( INTEL_INFO ( dev )-> gen >= 7 ) { <nl> + ret = i915_gem_object_set_cache_level ( ctx -> obj , <nl> + I915_CACHE_LLC_MLC ); <nl> + if ( ret ) <nl> + goto err_out ; <nl> + } <nl> + <nl> /* The ring associated with the context object is handled by the normal <nl> * object tracking code . We give an initial ring value simple to pass an <nl> * assertion in the context switch code .
void radeon_compute_pll_avivo ( struct radeon_pll * pll , <nl> ref_div_min = pll -> reference_div ; <nl> else <nl> ref_div_min = pll -> min_ref_div ; <nl> - ref_div_max = pll -> max_ref_div ; <nl> + <nl> + if ( pll -> flags & RADEON_PLL_USE_FRAC_FB_DIV && <nl> + pll -> flags & RADEON_PLL_USE_REF_DIV ) <nl> + ref_div_max = pll -> reference_div ; <nl> + else <nl> + ref_div_max = pll -> max_ref_div ; <nl>  <nl> /* determine allowed post divider range */ <nl> if ( pll -> flags & RADEON_PLL_USE_POST_DIV ) {
int __init omap2_clk_provider_init ( struct device_node * parent , int index , <nl> clocks_node_ptr [ index ] = clocks ; <nl>  <nl> io = kzalloc ( sizeof (* io ), GFP_KERNEL ); <nl> + if (! io ) <nl> + return - ENOMEM ; <nl>  <nl> io -> regmap = syscon ; <nl> io -> mem = mem ;
intel_fill_fb_ggtt_view ( struct i915_ggtt_view * view , struct drm_framebuffer * fb , <nl> tile_size = intel_tile_size ( dev_priv ); <nl>  <nl> cpp = drm_format_plane_cpp ( fb -> pixel_format , 0 ); <nl> - tile_width = intel_tile_width ( dev_priv , cpp , fb -> modifier [ 0 ]); <nl> + tile_width = intel_tile_width ( dev_priv , fb -> modifier [ 0 ], cpp ); <nl> tile_height = tile_size / tile_width ; <nl>  <nl> info -> width_pages = DIV_ROUND_UP ( fb -> pitches [ 0 ], tile_width );
static int __devexit gpio_keys_remove ( struct platform_device * pdev ) <nl> } <nl>  <nl> input_unregister_device ( input ); <nl> + kfree ( ddata ); <nl>  <nl> return 0 ; <nl> }
nfsd_cross_mnt ( struct svc_rqst * rqstp , struct dentry ** dpp , <nl>  <nl> exp2 = rqst_exp_get_by_name ( rqstp , mnt , mounts ); <nl> if ( IS_ERR ( exp2 )) { <nl> - err = PTR_ERR ( exp2 ); <nl> + if ( PTR_ERR ( exp2 ) != - ENOENT ) <nl> + err = PTR_ERR ( exp2 ); <nl> dput ( mounts ); <nl> mntput ( mnt ); <nl> goto out ;
static void printl ( const char * fmt , ...) <nl>  <nl> kfifo_put ( tcpw . fifo , tbuf , len ); <nl> wake_up (& tcpw . wait ); <nl> -} <nl> +} __attribute__ (( format ( printf , 1 , 2 ))); <nl> + <nl>  <nl> /* <nl> * Hook inserted to be called before each receive packet .
static u32 __seccomp_phase1_filter ( int this_syscall , struct seccomp_data * sd ) <nl>  <nl> switch ( action ) { <nl> case SECCOMP_RET_ERRNO : <nl> - /* Set the low - order 16 - bits as a errno . */ <nl> + /* Set low - order bits as an errno , capped at MAX_ERRNO . */ <nl> + if ( data > MAX_ERRNO ) <nl> + data = MAX_ERRNO ; <nl> syscall_set_return_value ( current , task_pt_regs ( current ), <nl> - data , 0 ); <nl> goto skip ;
static size_t log_output ( int facility , int level , enum log_flags lflags , const c <nl> cont_flush (); <nl> } <nl>  <nl> + /* Skip empty continuation lines that couldn ' t be added - they just flush */ <nl> + if (! text_len && ( lflags & LOG_CONT )) <nl> + return 0 ; <nl> + <nl> /* If it doesn ' t end in a newline , try to buffer the current line */ <nl> if (!( lflags & LOG_NEWLINE )) { <nl> if ( cont_add ( facility , level , lflags , text , text_len ))
int ath9k_hw_init ( struct ath_hw * ah ) <nl> struct ath_common * common = ath9k_hw_common ( ah ); <nl> int r = 0 ; <nl>  <nl> - if (! ath9k_hw_devid_supported ( ah -> hw_version . devid )) <nl> + if (! ath9k_hw_devid_supported ( ah -> hw_version . devid )) { <nl> + ath_print ( common , ATH_DBG_FATAL , <nl> + " Unsupported device ID : 0x % 0x \ n ", <nl> + ah -> hw_version . devid ); <nl> return - EOPNOTSUPP ; <nl> + } <nl>  <nl> ath9k_hw_init_defaults ( ah ); <nl> ath9k_hw_init_config ( ah );
static int vtg_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl> } <nl> vtg -> regs = devm_ioremap_nocache ( dev , res -> start , resource_size ( res )); <nl> + if (! vtg -> regs ) { <nl> + DRM_ERROR (" failed to remap I / O memory \ n "); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> np = of_parse_phandle ( pdev -> dev . of_node , " st , slave ", 0 ); <nl> if ( np ) {
static int i915_context_status ( struct seq_file * m , void * unused ) <nl> } <nl>  <nl> list_for_each_entry ( ctx , & dev_priv -> context_list , link ) { <nl> + if ( ctx -> obj == NULL ) <nl> + continue ; <nl> + <nl> seq_puts ( m , " HW context "); <nl> describe_ctx ( m , ctx ); <nl> for_each_ring ( ring , dev_priv , i )
static struct tty_struct * receive_chars ( struct uart_port * port , struct pt_regs * <nl> break ; <nl>  <nl> if ( c == CON_BREAK ) { <nl> + if ( uart_handle_break ( port )) <nl> + continue ; <nl> saw_console_brk = 1 ; <nl> c = 0 ; <nl> }
static int i40e_suspend ( struct pci_dev * pdev , pm_message_t state ) <nl>  <nl> set_bit ( __I40E_SUSPENDED , & pf -> state ); <nl> set_bit ( __I40E_DOWN , & pf -> state ); <nl> + del_timer_sync (& pf -> service_timer ); <nl> + cancel_work_sync (& pf -> service_task ); <nl> rtnl_lock (); <nl> i40e_prep_for_reset ( pf ); <nl> rtnl_unlock ();
static int cp2112_read ( struct cp2112_device * dev , u8 * data , size_t size ) <nl> struct cp2112_force_read_report report ; <nl> int ret ; <nl>  <nl> + if ( size > sizeof ( dev -> read_data )) <nl> + size = sizeof ( dev -> read_data ); <nl> report . report = CP2112_DATA_READ_FORCE_SEND ; <nl> report . length = cpu_to_be16 ( size ); <nl> 
static void o2hb_region_release ( struct config_item * item ) <nl> debugfs_remove ( reg -> hr_debug_dir ); <nl> kfree ( reg -> hr_db_livenodes ); <nl> kfree ( reg -> hr_db_regnum ); <nl> - kfree ( reg -> hr_debug_elapsed_time ); <nl> - kfree ( reg -> hr_debug_pinned ); <nl> + kfree ( reg -> hr_db_elapsed_time ); <nl> + kfree ( reg -> hr_db_pinned ); <nl>  <nl> spin_lock (& o2hb_live_lock ); <nl> list_del (& reg -> hr_all_item );
static int alloc_spa ( struct cxl_afu * afu ) <nl>  <nl> static void release_spa ( struct cxl_afu * afu ) <nl> { <nl> + cxl_p1n_write ( afu , CXL_PSL_SPAP_An , 0 ); <nl> free_pages (( unsigned long ) afu -> spa , afu -> spa_order ); <nl> } <nl> 
static void mmci_post_request ( struct mmc_host * mmc , struct mmc_request * mrq , <nl> chan = host -> dma_tx_channel ; <nl> dmaengine_terminate_all ( chan ); <nl>  <nl> + if ( host -> dma_desc_current == next -> dma_desc ) <nl> + host -> dma_desc_current = NULL ; <nl> + <nl> + if ( host -> dma_current == next -> dma_chan ) <nl> + host -> dma_current = NULL ; <nl> + <nl> next -> dma_desc = NULL ; <nl> next -> dma_chan = NULL ; <nl> + data -> host_cookie = 0 ; <nl> } <nl> } <nl> 
# define OMAP4430_PRM_BASE 0x4a306000 <nl> # define OMAP44XX_GPMC_BASE 0x50000000 <nl> # define OMAP443X_SCM_BASE 0x4a002000 <nl> -# define OMAP443X_CTRL_BASE OMAP443X_SCM_BASE <nl> +# define OMAP443X_CTRL_BASE 0x4a100000 <nl> # define OMAP44XX_IC_BASE 0x48200000 <nl> # define OMAP44XX_IVA_INTC_BASE 0x40000000 <nl> # define IRQ_SIR_IRQ 0x0040
static int piix_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl>  <nl> static void piix_host_stop ( struct ata_host_set * host_set ) <nl> { <nl> + struct piix_host_priv * hpriv = host_set -> private_data ; <nl> + <nl> ata_host_stop ( host_set ); <nl> + <nl> + kfree ( hpriv ); <nl> } <nl>  <nl> static int __init piix_init ( void )
static int __devinit snd_hdspm_create_hwdep ( struct snd_card * card , <nl>  <nl> hw -> ops . open = snd_hdspm_hwdep_dummy_op ; <nl> hw -> ops . ioctl = snd_hdspm_hwdep_ioctl ; <nl> + hw -> ops . ioctl_compat = snd_hdspm_hwdep_ioctl ; <nl> hw -> ops . release = snd_hdspm_hwdep_dummy_op ; <nl>  <nl> return 0 ;
static int fman_init ( struct fman * fman ) <nl> /* allocate MURAM for FIFO according to total size */ <nl> fman -> fifo_offset = fman_muram_alloc ( fman -> muram , <nl> fman -> state -> total_fifo_size ); <nl> - if ( IS_ERR_VALUE ( fman -> cam_offset )) { <nl> + if ( IS_ERR_VALUE ( fman -> fifo_offset )) { <nl> free_init_resources ( fman ); <nl> dev_err ( fman -> dev , "% s : MURAM alloc for BMI FIFO failed \ n ", <nl> __func__ );
struct fib_table * fib_trie_unmerge ( struct fib_table * oldtb ) <nl> local_l = fib_find_node ( lt , & local_tp , l -> key ); <nl>  <nl> if ( fib_insert_alias ( lt , local_tp , local_l , new_fa , <nl> - NULL , l -> key )) <nl> + NULL , l -> key )) { <nl> + kmem_cache_free ( fn_alias_kmem , new_fa ); <nl> goto out ; <nl> + } <nl> } <nl>  <nl> /* stop loop if key wrapped back to 0 */
static int wl1271_init_ieee80211 ( struct wl1271 * wl ) <nl> wl -> hw -> wiphy -> max_sched_scan_ie_len = WL1271_CMD_TEMPL_MAX_SIZE - <nl> sizeof ( struct ieee80211_header ); <nl>  <nl> - wl -> hw -> wiphy -> max_remain_on_channel_duration = 5000 ; <nl> + wl -> hw -> wiphy -> max_remain_on_channel_duration = 30000 ; <nl>  <nl> wl -> hw -> wiphy -> flags |= WIPHY_FLAG_AP_UAPSD | <nl> WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL |
perf_callchain_kernel ( struct perf_callchain_entry_ctx * entry , struct pt_regs * re <nl> return ; <nl> } <nl>  <nl> - perf_callchain_store ( entry , regs -> ip ); <nl> + if ( perf_callchain_store ( entry , regs -> ip )) <nl> + return ; <nl>  <nl> dump_trace ( NULL , regs , NULL , 0 , & backtrace_ops , entry ); <nl> }
int scrub_enumerate_chunks ( struct scrub_ctx * sctx , <nl> btrfs_put_block_group ( cache ); <nl> if ( ret ) <nl> break ; <nl> - if ( atomic64_read (& dev_replace -> num_write_errors ) > 0 ) { <nl> + if ( is_dev_replace && <nl> + atomic64_read (& dev_replace -> num_write_errors ) > 0 ) { <nl> ret = - EIO ; <nl> break ; <nl> }
void irq_domain_free_irqs_common ( struct irq_domain * domain , unsigned int virq , <nl> } <nl> irq_domain_free_irqs_parent ( domain , virq , nr_irqs ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( irq_domain_free_irqs_common ); <nl>  <nl> /** <nl> * irq_domain_free_irqs_top - Clear handler and handler data , clear irqdata and free parent
static int soc15_common_set_clockgating_state ( void * handle , <nl> { <nl> struct amdgpu_device * adev = ( struct amdgpu_device *) handle ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> switch ( adev -> asic_type ) { <nl> case CHIP_VEGA10 : <nl> nbio_v6_1_update_medium_grain_clock_gating ( adev ,
int netvsc_send ( struct hv_device * device , <nl> if (! net_device ) <nl> return - ENODEV ; <nl>  <nl> + /* We may race with netvsc_connect_vsp ()/ netvsc_init_buf () and get <nl> + * here before the negotiation with the host is finished and <nl> + * send_section_map may not be allocated yet . <nl> + */ <nl> + if (! net_device -> send_section_map ) <nl> + return - EAGAIN ; <nl> + <nl> out_channel = net_device -> chn_table [ q_idx ]; <nl>  <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ;
static inline int unshare_utsname ( unsigned long unshare_flags , <nl>  <nl> static inline int copy_utsname ( int flags , struct task_struct * tsk ) <nl> { <nl> + if ( flags & CLONE_NEWUTS ) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> static inline void put_uts_ns ( struct uts_namespace * ns )
acpi_ex_load_op ( union acpi_operand_object * obj_desc , <nl> */ <nl> status = acpi_tb_add_table ( table_ptr , & table_index ); <nl> if ( ACPI_FAILURE ( status )) { <nl> - return_ACPI_STATUS ( status ); <nl> + goto cleanup ; <nl> } <nl>  <nl> status =
struct gen_pool * devm_gen_pool_create ( struct device * dev , int min_alloc_order , <nl> struct gen_pool ** ptr , * pool ; <nl>  <nl> ptr = devres_alloc ( devm_gen_pool_release , sizeof (* ptr ), GFP_KERNEL ); <nl> + if (! ptr ) <nl> + return NULL ; <nl>  <nl> pool = gen_pool_create ( min_alloc_order , nid ); <nl> if ( pool ) {
 <nl> int intel_sanitize_enable_execlists ( struct drm_device * dev , int enable_execlists ) <nl> { <nl> + WARN_ON ( i915 . enable_ppgtt == - 1 ); <nl> + <nl> if ( enable_execlists == 0 ) <nl> return 0 ; <nl> 
void __cfg80211_disconnected ( struct net_device * dev , const u8 * ie , <nl> memset (& wrqu , 0 , sizeof ( wrqu )); <nl> wrqu . ap_addr . sa_family = ARPHRD_ETHER ; <nl> wireless_send_event ( dev , SIOCGIWAP , & wrqu , NULL ); <nl> + wdev -> wext . connect . ssid_len = 0 ; <nl> # endif <nl> } <nl> 
static int stb0899_send_diseqc_msg ( struct dvb_frontend * fe , struct dvb_diseqc_ma <nl> struct stb0899_state * state = fe -> demodulator_priv ; <nl> u8 reg , i ; <nl>  <nl> - if ( cmd -> msg_len > 8 ) <nl> + if ( cmd -> msg_len > sizeof ( cmd -> msg )) <nl> return - EINVAL ; <nl>  <nl> /* enable FIFO precharge */
static ssize_t gpio_keys_attr_store_helper ( struct gpio_keys_drvdata * ddata , <nl> } <nl> } <nl>  <nl> + if ( i == ddata -> pdata -> nbuttons ) { <nl> + error = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> mutex_lock (& ddata -> disable_lock ); <nl>  <nl> for ( i = 0 ; i < ddata -> pdata -> nbuttons ; i ++) {
static void vmbus_process_offer ( struct work_struct * work ) <nl> spin_lock_irqsave (& vmbus_connection . channel_lock , flags ); <nl> list_del (& newchannel -> listentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channel_lock , flags ); <nl> + kfree ( newchannel -> device_obj ); <nl>  <nl> free_channel ( newchannel ); <nl> } else {
static int marvell_pre_reset ( struct ata_port * ap ) <nl> switch ( ap -> port_no ) <nl> { <nl> case 0 : <nl> - /* Might be backward , docs unclear */ <nl> if ( inb ( ap -> ioaddr . bmdma_addr + 1 ) & 1 ) <nl> - ap -> cbl = ATA_CBL_PATA80 ; <nl> - else <nl> ap -> cbl = ATA_CBL_PATA40 ; <nl> + else <nl> + ap -> cbl = ATA_CBL_PATA80 ; <nl> + break ; <nl>  <nl> case 1 : /* Legacy SATA port */ <nl> ap -> cbl = ATA_CBL_SATA ;
static int usbhsg_ep_disable ( struct usb_ep * ep ) <nl> struct usbhsg_uep * uep = usbhsg_ep_to_uep ( ep ); <nl> struct usbhs_pipe * pipe = usbhsg_uep_to_pipe ( uep ); <nl>  <nl> + if (! pipe ) <nl> + return - EINVAL ; <nl> + <nl> usbhsg_pipe_disable ( uep ); <nl> usbhs_pipe_free ( pipe ); <nl> 
static int malidp_bind ( struct device * dev ) <nl> if ( ret < 0 ) <nl> goto irq_init_fail ; <nl>  <nl> + drm -> irq_enabled = true ; <nl> + <nl> ret = drm_vblank_init ( drm , drm -> mode_config . num_crtc ); <nl> if ( ret < 0 ) { <nl> DRM_ERROR (" failed to initialise vblank \ n "); <nl> fbdev_fail : <nl> vblank_fail : <nl> malidp_se_irq_fini ( drm ); <nl> malidp_de_irq_fini ( drm ); <nl> + drm -> irq_enabled = false ; <nl> irq_init_fail : <nl> component_unbind_all ( dev , drm ); <nl> bind_fail :
int i965_reset ( struct drm_device * dev , u8 flags ) <nl> } <nl> } else { <nl> DRM_ERROR (" Error occurred . Don ' t know how to reset this chip .\ n "); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return - ENODEV ; <nl> } <nl> 
static const struct usb_device_id usb_quirk_list [] = { <nl> /* Creative SB Audigy 2 NX */ <nl> { USB_DEVICE ( 0x041e , 0x3020 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl>  <nl> + /* Microsoft LifeCam - VX700 v2 . 0 */ <nl> + { USB_DEVICE ( 0x045e , 0x0770 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl> + <nl> /* Logitech Quickcam Fusion */ <nl> { USB_DEVICE ( 0x046d , 0x08c1 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl> 
static void zfcp_fc_adisc_handler ( unsigned long data ) <nl> struct zfcp_port * port = adisc -> els . port ; <nl> struct zfcp_ls_adisc_acc * ls_adisc = & adisc -> ls_adisc_acc ; <nl>  <nl> - if (! adisc -> els . status ) { <nl> + if ( adisc -> els . status ) { <nl> /* request rejected or timed out */ <nl> zfcp_erp_port_forced_reopen ( port , 0 , 63 , NULL ); <nl> goto out ;
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> __le16 fc ; <nl> struct ieee80211_hdr hdr ; <nl> struct ieee80211s_hdr mesh_hdr __maybe_unused ; <nl> - struct mesh_path * mppath = NULL ; <nl> + struct mesh_path __maybe_unused * mppath = NULL ; <nl> const u8 * encaps_data ; <nl> int encaps_len , skip_header_bytes ; <nl> int nh_pos , h_pos ;
static int w1_attach_slave_device ( struct w1_master * dev , struct w1_reg_num * rn ) <nl> atomic_set (& sl -> refcnt , 0 ); <nl> init_completion (& sl -> released ); <nl>  <nl> + /* slave modules need to be loaded in a context with unlocked mutex */ <nl> + mutex_unlock (& dev -> mutex ); <nl> request_module (" w1 - family - 0x % 0x ", rn -> family ); <nl> + mutex_lock (& dev -> mutex ); <nl>  <nl> spin_lock (& w1_flock ); <nl> f = w1_family_registered ( rn -> family );
static void pci_dio_detach ( struct comedi_device * dev ) <nl> if ( devpriv ) { <nl> if ( devpriv -> valid ) <nl> pci_dio_reset ( dev ); <nl> + } <nl> + if ( dev -> subdevices ) { <nl> for ( i = 0 ; i < dev -> n_subdevices ; i ++) { <nl> s = dev -> subdevices + i ; <nl> if ( s -> type == COMEDI_SUBD_DIO )
void __devinit pci_read_bridge_bases ( struct pci_bus * child ) <nl> struct resource * res ; <nl> int i ; <nl>  <nl> - if (! child -> parent ) /* It ' s a host bus , nothing to read */ <nl> + if ( pci_is_root_bus ( child )) /* It ' s a host bus , nothing to read */ <nl> return ; <nl>  <nl> if ( dev -> transparent ) {
void __init create_boot_cache ( struct kmem_cache * s , const char * name , size_t siz <nl> err = __kmem_cache_create ( s , flags ); <nl>  <nl> if ( err ) <nl> - panic (" Creation of kmalloc slab % s size =% zd failed . Reason % d \ n ", <nl> + panic (" Creation of kmalloc slab % s size =% zu failed . Reason % d \ n ", <nl> name , size , err ); <nl>  <nl> s -> refcount = - 1 ; /* Exempt from merging for now */
static int __init bm2835_mmal_init ( void ) <nl> num_cameras = MAX_BCM2835_CAMERAS ; <nl>  <nl> for ( camera = 0 ; camera < num_cameras ; camera ++) { <nl> - dev = kzalloc ( sizeof ( struct bm2835_mmal_dev ), GFP_KERNEL ); <nl> + dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> if (! dev ) { <nl> ret = - ENOMEM ; <nl> goto cleanup_gdev ;
ccw_device_offline ( struct ccw_device * cdev ) <nl> ccw_device_done ( cdev , DEV_STATE_NOT_OPER ); <nl> return 0 ; <nl> } <nl> + if ( cdev -> private -> state == DEV_STATE_BOXED ) { <nl> + ccw_device_done ( cdev , DEV_STATE_BOXED ); <nl> + return 0 ; <nl> + } <nl> if ( ccw_device_is_orphan ( cdev )) { <nl> ccw_device_done ( cdev , DEV_STATE_OFFLINE ); <nl> return 0 ;
int cfg80211_wext_siwscan ( struct net_device * dev , <nl> wext_freq_not_found : ; <nl> } <nl> } <nl> + /* No channels found ? */ <nl> + if (! i ) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl>  <nl> /* Set real number of channels specified in creq -> channels [] */ <nl> creq -> n_channels = i ;
int diAlloc ( struct inode * pip , bool dir , struct inode * ip ) <nl> /* mask any prior bits for the starting words of the <nl> * summary map . <nl> */ <nl> - mask = ONES << ( EXTSPERSUM - bitno ); <nl> + mask = ( bitno == 0 ) ? 0 : ( ONES << ( EXTSPERSUM - bitno )); <nl> inosmap = le32_to_cpu ( iagp -> inosmap [ sword ]) | mask ; <nl> extsmap = le32_to_cpu ( iagp -> extsmap [ sword ]) | mask ; <nl> 
void ath9k_ps_wakeup ( struct ath_softc * sc ) <nl> spin_lock (& common -> cc_lock ); <nl> ath_hw_cycle_counters_update ( common ); <nl> memset (& common -> cc_survey , 0 , sizeof ( common -> cc_survey )); <nl> + memset (& common -> cc_ani , 0 , sizeof ( common -> cc_ani )); <nl> spin_unlock (& common -> cc_lock ); <nl> } <nl> 
static int ak8975_probe ( struct i2c_client * client , <nl> indio_dev -> channels = ak8975_channels ; <nl> indio_dev -> num_channels = ARRAY_SIZE ( ak8975_channels ); <nl> indio_dev -> info = & ak8975_info ; <nl> + indio_dev -> name = id -> name ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl>  <nl> err = iio_device_register ( indio_dev );
int btrfs_check_trunc_cache_free_space ( struct btrfs_root * root , <nl> else <nl> ret = 0 ; <nl> spin_unlock (& rsv -> lock ); <nl> - return 0 ; <nl> + return ret ; <nl> } <nl>  <nl> int btrfs_truncate_free_space_cache ( struct btrfs_root * root ,
static int rcar_vin_videobuf_setup ( struct vb2_queue * vq , <nl> unsigned int bytes_per_line ; <nl> int ret ; <nl>  <nl> + if ( fmt -> fmt . pix . sizeimage < icd -> sizeimage ) <nl> + return - EINVAL ; <nl> + <nl> xlate = soc_camera_xlate_by_fourcc ( icd , <nl> fmt -> fmt . pix . pixelformat ); <nl> if (! xlate )
static long ft1000_ChIoctl ( struct file * File , unsigned int Command , <nl> break ; <nl> case IOCTL_GET_DSP_STAT_CMD : <nl> // DEBUG (" FT1000 : ft1000_ChIoctl : IOCTL_FT1000_GET_DSP_STAT called \ n "); <nl> - <nl> + memset (& get_stat_data , 0 , sizeof ( get_stat_data )); <nl> memcpy ( get_stat_data . DspVer , info -> DspVer , DSPVERSZ ); <nl> memcpy ( get_stat_data . HwSerNum , info -> HwSerNum , HWSERNUMSZ ); <nl> memcpy ( get_stat_data . Sku , info -> Sku , SKUSZ );
int add_extent_mapping ( struct extent_map_tree * tree , <nl> rb = tree_insert (& tree -> map , em -> start , & em -> rb_node ); <nl> if ( rb ) { <nl> ret = - EEXIST ; <nl> - free_extent_map ( merge ); <nl> goto out ; <nl> } <nl> atomic_inc (& em -> refs );
long drm_ioctl ( struct file * filp , <nl> usize = asize = _IOC_SIZE ( cmd ); <nl> if ( drv_size > asize ) <nl> asize = drv_size ; <nl> + cmd = ioctl -> cmd_drv ; <nl> } <nl> else if (( nr >= DRM_COMMAND_END ) || ( nr < DRM_COMMAND_BASE )) { <nl> ioctl = & drm_ioctls [ nr ];
static void __init _set_omap_chip ( void ) <nl>  <nl> } <nl>  <nl> - void __init omap2_check_revision ( void ) <nl> + void __init omap24xx_check_revision ( void ) <nl> { <nl> int i , j ; <nl> u32 idcode ; <nl> void __init omap2_check_revision ( void ) <nl>  <nl> } <nl>  <nl> + void __init omap2_check_revision ( void ) <nl> +{ <nl> + omap24xx_check_revision (); <nl> +} <nl> + <nl> void __init omap2_set_globals_tap ( struct omap_globals * omap2_globals ) <nl> { <nl> class = omap2_globals -> class ;
static int spidev_release ( struct inode * inode , struct file * filp ) <nl> kfree ( spidev -> rx_buffer ); <nl> spidev -> rx_buffer = NULL ; <nl>  <nl> - spidev -> speed_hz = spidev -> spi -> max_speed_hz ; <nl> + if ( spidev -> spi ) <nl> + spidev -> speed_hz = spidev -> spi -> max_speed_hz ; <nl>  <nl> /* ... after we unbound from the underlying device ? */ <nl> spin_lock_irq (& spidev -> spi_lock );
void rt2x00link_update_stats ( struct rt2x00_dev * rt2x00dev , <nl> struct link_ant * ant = & rt2x00dev -> link . ant ; <nl> struct ieee80211_hdr * hdr = ( struct ieee80211_hdr *) skb -> data ; <nl>  <nl> + /* <nl> + * No need to update the stats for != STA interfaces <nl> + */ <nl> + if (! rt2x00dev -> intf_sta_count ) <nl> + return ; <nl> + <nl> /* <nl> * Frame was received successfully since non - succesfull <nl> * frames would have been dropped by the hardware .
static struct dma_chan * zx_of_dma_simple_xlate ( struct of_phandle_args * dma_spec , <nl> struct dma_chan * chan ; <nl> struct zx_dma_chan * c ; <nl>  <nl> - if ( request > d -> dma_requests ) <nl> + if ( request >= d -> dma_requests ) <nl> return NULL ; <nl>  <nl> chan = dma_get_any_slave_channel (& d -> slave );
static int load_twl4030_script ( struct twl4030_script * tscript , <nl> goto out ; <nl> } <nl> if ( tscript -> flags & TWL4030_WAKEUP12_SCRIPT ) { <nl> + /* Reset any existing sleep script to avoid hangs on reboot */ <nl> + err = twl_i2c_write_u8 ( TWL_MODULE_PM_MASTER , END_OF_SCRIPT , <nl> + R_SEQ_ADD_A2S ); <nl> + if ( err ) <nl> + goto out ; <nl> + <nl> err = twl4030_config_wakeup12_sequence ( address ); <nl> if ( err ) <nl> goto out ;
static void broxton_phy_init ( struct drm_i915_private * dev_priv , <nl> DRM_DEBUG_DRIVER (" DDI PHY % d already enabled , " <nl> " won ' t reprogram it \ n ", phy ); <nl> /* Still read out the GRC value for state verification */ <nl> - if ( phy == DPIO_PHY1 ) <nl> + if ( phy == DPIO_PHY0 ) <nl> dev_priv -> bxt_phy_grc = broxton_get_grc ( dev_priv , phy ); <nl>  <nl> return ;
retry : <nl> return UBI_IO_BITFLIPS ; <nl> } <nl>  <nl> - if ( read != len && retries ++ < UBI_IO_RETRIES ) { <nl> + if ( retries ++ < UBI_IO_RETRIES ) { <nl> dbg_io (" error % d % s while reading % d bytes from PEB % d :% d ," <nl> " read only % zd bytes , retry ", <nl> err , errstr , len , pnum , offset , read );
static int wm8731_register ( struct wm8731_priv * wm8731 ) <nl>  <nl> memcpy ( codec -> reg_cache , wm8731_reg , sizeof ( wm8731_reg )); <nl>  <nl> + ret = wm8731_reset ( codec ); <nl> + if ( ret < 0 ) { <nl> + dev_err ( codec -> dev , " Failed to issue reset \ n "); <nl> + return ret ; <nl> + } <nl> + <nl> wm8731_dai . dev = codec -> dev ; <nl>  <nl> - wm8731_reset ( codec ); <nl> wm8731_set_bias_level ( codec , SND_SOC_BIAS_STANDBY ); <nl>  <nl> /* Latch the update bits */
out_destroy : <nl> out_free : <nl> pr_info ("% s : failed to register PMU devices !\ n ", <nl> of_node_full_name ( node )); <nl> + kfree ( pmu -> irq_affinity ); <nl> kfree ( pmu ); <nl> return ret ; <nl> }
int vty_init ( const struct file_operations * console_fops ); <nl>  <nl> static inline bool vt_force_oops_output ( struct vc_data * vc ) <nl> { <nl> - if ( oops_in_progress && vc -> vc_panic_force_write ) <nl> + if ( oops_in_progress && vc -> vc_panic_force_write && panic_timeout >= 0 ) <nl> return true ; <nl> return false ; <nl> }
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> - free ( ptr ); <nl> + kmem_free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
void hid_sensor_remove_trigger ( struct iio_dev * indio_dev ) <nl> { <nl> iio_trigger_unregister ( indio_dev -> trig ); <nl> iio_trigger_free ( indio_dev -> trig ); <nl> + indio_dev -> trig = NULL ; <nl> } <nl> EXPORT_SYMBOL ( hid_sensor_remove_trigger ); <nl> 
static struct platform_device sdhi0_device = { <nl> static struct sh_mobile_sdhi_info sdhi1_info = { <nl> . dma_slave_tx = SHDMA_SLAVE_SDHI1_TX , <nl> . dma_slave_rx = SHDMA_SLAVE_SDHI1_RX , <nl> - . tmio_ocr_mask = MMC_VDD_165_195 , <nl> . tmio_flags = TMIO_MMC_WRPROTECT_DISABLE , <nl> . tmio_caps = MMC_CAP_SD_HIGHSPEED | MMC_CAP_SDIO_IRQ | <nl> MMC_CAP_NEEDS_POLL , <nl> static struct resource sh_mmcif_resources [] = { <nl>  <nl> static struct sh_mmcif_plat_data sh_mmcif_plat = { <nl> . sup_pclk = 0 , <nl> - . ocr = MMC_VDD_165_195 | MMC_VDD_32_33 | MMC_VDD_33_34 , <nl> . caps = MMC_CAP_4_BIT_DATA | <nl> MMC_CAP_8_BIT_DATA | <nl> MMC_CAP_NEEDS_POLL ,
# define DOC_ECCCONF1 0x1042 <nl> # define DOC_ECCPRESET 0x1044 <nl> # define DOC_HAMMINGPARITY 0x1046 <nl> -# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 1 )) <nl> +# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 0 )) <nl>  <nl> # define DOC_PROTECTION 0x1056 <nl> # define DOC_DPS0_ADDRLOW 0x1060
int kvm_dev_ioctl_check_extension ( long ext ) <nl> case KVM_CAP_SYNC_REGS : <nl> r = 1 ; <nl> break ; <nl> + case KVM_CAP_NR_VCPUS : <nl> + case KVM_CAP_MAX_VCPUS : <nl> + r = KVM_MAX_VCPUS ; <nl> + break ; <nl> default : <nl> r = 0 ; <nl> }
static void flush_tlb_others_ipi ( const struct cpumask * cpumask , <nl> * We have to send the IPI only to <nl> * CPUs affected . <nl> */ <nl> - send_IPI_mask ( cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl> + send_IPI_mask ( f -> flush_cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl>  <nl> while (! cpumask_empty ( to_cpumask ( f -> flush_cpumask ))) <nl> cpu_relax ();
static int snd_ctl_elem_user_tlv ( struct snd_kcontrol * kcontrol , <nl> ue -> tlv_data = new_data ; <nl> ue -> tlv_data_size = size ; <nl> } else { <nl> + if (! ue -> tlv_data_size || ! ue -> tlv_data ) <nl> + return - ENXIO ; <nl> if ( size < ue -> tlv_data_size ) <nl> return - ENOSPC ; <nl> if ( copy_to_user ( tlv , ue -> tlv_data , ue -> tlv_data_size ))
static void enqueue_cmd_and_start_io ( ctlr_info_t * h , <nl> spin_lock_irqsave (& h -> lock , flags ); <nl> addQ (& h -> reqQ , c ); <nl> h -> Qdepth ++; <nl> + if ( h -> Qdepth > h -> maxQsinceinit ) <nl> + h -> maxQsinceinit = h -> Qdepth ; <nl> start_io ( h ); <nl> spin_unlock_irqrestore (& h -> lock , flags ); <nl> }
out_attach : <nl> static void ocfs2_drop_dentry_lock ( struct ocfs2_super * osb , <nl> struct ocfs2_dentry_lock * dl ) <nl> { <nl> + iput ( dl -> dl_inode ); <nl> ocfs2_simple_drop_lockres ( osb , & dl -> dl_lockres ); <nl> ocfs2_lock_res_free (& dl -> dl_lockres ); <nl> - iput ( dl -> dl_inode ); <nl> kfree ( dl ); <nl> } <nl> 
int module_finalize ( const Elf32_Ehdr * hdr , const Elf_Shdr * sechdrs , <nl> # endif <nl> s = find_mod_section ( hdr , sechdrs , ". alt . smp . init "); <nl> if ( s && ! is_smp ()) <nl> +# ifdef CONFIG_SMP_ON_UP <nl> fixup_smp (( void *) s -> sh_addr , s -> sh_size ); <nl> +# else <nl> + return - EINVAL ; <nl> +# endif <nl> return 0 ; <nl> } <nl> 
static const struct pinmux_cfg_reg pinmux_config_regs [] = { <nl> FN_MSIOF0_SCK_B , 0 , <nl> /* IP5_23_21 [ 3 ] */ <nl> FN_WE1_N , FN_IERX , FN_CAN1_RX , FN_VI1_G4 , <nl> - FN_VI1_G4_B , FN_VI2_R6 , FN_SCIFA0_CTS_N_B , <nl> - FN_IERX_C , 0 , <nl> + FN_VI1_G4_B , FN_VI2_R6 , FN_SCIFA0_CTS_N_B , FN_IERX_C , <nl> /* IP5_20_18 [ 3 ] */ <nl> FN_WE0_N , FN_IECLK , FN_CAN_CLK , <nl> FN_VI2_VSYNC_N , FN_SCIFA0_TXD_B , FN_VI2_VSYNC_N_B , 0 , 0 ,
static struct async * alloc_async ( unsigned int numisoframes ) <nl> static void free_async ( struct async * as ) <nl> { <nl> put_pid ( as -> pid ); <nl> - put_cred ( as -> cred ); <nl> + if ( as -> cred ) <nl> + put_cred ( as -> cred ); <nl> kfree ( as -> urb -> transfer_buffer ); <nl> kfree ( as -> urb -> setup_packet ); <nl> usb_free_urb ( as -> urb );
static int __devexit powernowk8_cpu_exit ( struct cpufreq_policy * pol ) <nl>  <nl> static unsigned int powernowk8_get ( unsigned int cpu ) <nl> { <nl> - struct powernow_k8_data * data ; <nl> + struct powernow_k8_data * data = per_cpu ( powernow_data , cpu ); <nl> cpumask_t oldmask = current -> cpus_allowed ; <nl> unsigned int khz = 0 ; <nl> - unsigned int first ; <nl> - <nl> - first = cpumask_first ( cpu_core_mask ( cpu )); <nl> - data = per_cpu ( powernow_data , first ); <nl>  <nl> if (! data ) <nl> return - EINVAL ;
static bool nested_vmx_exit_handled_msr ( struct kvm_vcpu * vcpu , <nl> u32 msr_index = vcpu -> arch . regs [ VCPU_REGS_RCX ]; <nl> gpa_t bitmap ; <nl>  <nl> - if (! nested_cpu_has ( get_vmcs12 ( vcpu ), CPU_BASED_USE_MSR_BITMAPS )) <nl> + if (! nested_cpu_has ( vmcs12 , CPU_BASED_USE_MSR_BITMAPS )) <nl> return 1 ; <nl>  <nl> /*
static int bcm_parse_target_params ( PMINI_ADAPTER Adapter ) <nl> if (! buff ) <nl> return - ENOMEM ; <nl>  <nl> - if (( Adapter -> pstargetparams = kmalloc ( sizeof ( STARGETPARAMS ), GFP_KERNEL )) == NULL ) { <nl> + Adapter -> pstargetparams = kmalloc ( sizeof ( STARGETPARAMS ), GFP_KERNEL ); <nl> + if ( Adapter -> pstargetparams == NULL ) { <nl> kfree ( buff ); <nl> return - ENOMEM ; <nl> }
static int mmpcam_probe ( struct platform_device * pdev ) <nl>  <nl> out_unregister : <nl> mccic_shutdown ( mcam ); <nl> - mmpcam_power_down ( mcam ); <nl> out_gpio2 : <nl> + mmpcam_power_down ( mcam ); <nl> gpio_free ( pdata -> sensor_reset_gpio ); <nl> out_gpio : <nl> gpio_free ( pdata -> sensor_power_gpio );
static void ceph_x_destroy ( struct ceph_auth_client * ac ) <nl> remove_ticket_handler ( ac , th ); <nl> } <nl>  <nl> + if ( xi -> auth_authorizer . buf ) <nl> + ceph_buffer_put ( xi -> auth_authorizer . buf ); <nl> + <nl> kfree ( ac -> private ); <nl> ac -> private = NULL ; <nl> }
static int ltr501_write_event_config ( struct iio_dev * indio_dev , <nl> int ret ; <nl>  <nl> /* only 1 and 0 are valid inputs */ <nl> - if ( state != 1 || state != 0 ) <nl> + if ( state != 1 && state != 0 ) <nl> return - EINVAL ; <nl>  <nl> switch ( chan -> type ) {
int c4iw_register_device ( struct c4iw_dev * dev ) <nl> dev -> ibdev . iwcm -> add_ref = c4iw_qp_add_ref ; <nl> dev -> ibdev . iwcm -> rem_ref = c4iw_qp_rem_ref ; <nl> dev -> ibdev . iwcm -> get_qp = c4iw_get_qp ; <nl> + memcpy ( dev -> ibdev . iwcm -> ifname , dev -> rdev . lldi . ports [ 0 ]-> name , <nl> + sizeof ( dev -> ibdev . iwcm -> ifname )); <nl>  <nl> ret = ib_register_device (& dev -> ibdev , NULL ); <nl> if ( ret )
static void i40iw_cm_disconn_true ( struct i40iw_qp * iwqp ) <nl> /* Flush the queues */ <nl> i40iw_flush_wqes ( iwdev , iwqp ); <nl>  <nl> - if ( qp -> term_flags ) { <nl> + if ( qp -> term_flags && iwqp -> ibqp . event_handler ) { <nl> ibevent . device = iwqp -> ibqp . device ; <nl> ibevent . event = ( qp -> eventtype == TERM_EVENT_QP_FATAL ) ? <nl> IB_EVENT_QP_FATAL : IB_EVENT_QP_ACCESS_ERR ;
static int gb_uart_connection_init ( struct gb_connection * connection ) <nl> if ( minor == - ENOSPC ) { <nl> dev_err (& connection -> dev , <nl> " no more free minor numbers \ n "); <nl> - return - ENODEV ; <nl> + retval = - ENODEV ; <nl> + goto error_version ; <nl> } <nl> - return minor ; <nl> + retval = minor ; <nl> + goto error_version ; <nl> } <nl>  <nl> gb_tty -> minor = minor ;
static void igb_reuse_rx_page ( struct igb_ring * rx_ring , <nl> rx_ring -> next_to_alloc = ( nta < rx_ring -> count ) ? nta : 0 ; <nl>  <nl> /* transfer page from old buffer to new buffer */ <nl> - memcpy ( new_buff , old_buff , sizeof ( struct igb_rx_buffer )); <nl> + * new_buff = * old_buff ; <nl>  <nl> /* sync the buffer for use by the device */ <nl> dma_sync_single_range_for_device ( rx_ring -> dev , old_buff -> dma ,
static int exynos_drm_fbdev_update ( struct drm_fb_helper * helper , <nl>  <nl> fbi -> screen_base = buffer -> kvaddr + offset ; <nl> fbi -> screen_size = size ; <nl> + fbi -> fix . smem_len = size ; <nl>  <nl> return 0 ; <nl> }
static void vss_on_reset ( void ) <nl> int <nl> hv_vss_init ( struct hv_util_service * srv ) <nl> { <nl> + if ( vmbus_proto_version < VERSION_WIN8_1 ) { <nl> + pr_warn (" Integration service ' Backup ( volume snapshot )'" <nl> + " not supported on this host version .\ n "); <nl> + return - ENOTSUPP ; <nl> + } <nl> recv_buffer = srv -> recv_buffer ; <nl>  <nl> /*
TLan_FinishReset ( struct net_device * dev ) <nl> TLan_SetTimer ( dev , ( 10 * HZ ), TLAN_TIMER_FINISH_RESET ); <nl> return ; <nl> } <nl> + TLan_SetMulticastList ( dev ); <nl>  <nl> } /* TLan_FinishReset */ <nl> 
fastcall void __kprobes do_general_protection ( struct pt_regs * regs , <nl> tss -> io_bitmap_max - thread -> io_bitmap_max ); <nl> tss -> io_bitmap_max = thread -> io_bitmap_max ; <nl> tss -> io_bitmap_base = IO_BITMAP_OFFSET ; <nl> + tss -> io_bitmap_owner = thread ; <nl> put_cpu (); <nl> return ; <nl> }
static ssize_t yurex_write ( struct file * file , const char * user_buffer , size_t co <nl> goto error ; <nl>  <nl> mutex_lock (& dev -> io_mutex ); <nl> - if (! dev -> interface ) { /* alreaday disconnected */ <nl> + if (! dev -> interface ) { /* already disconnected */ <nl> mutex_unlock (& dev -> io_mutex ); <nl> retval = - ENODEV ; <nl> goto error ;
int hfsplus_get_block ( struct inode * inode , sector_t iblock , <nl> goto done ; <nl> } <nl>  <nl> + if ( inode -> i_ino == HFSPLUS_EXT_CNID ) <nl> + return - EIO ; <nl> + <nl> mutex_lock (& HFSPLUS_I ( inode ). extents_lock ); <nl> res = hfsplus_ext_read_extent ( inode , ablock ); <nl> if (! res ) {
int cvmx_usb_initialize ( struct cvmx_usb_state * state , int usb_port_number , <nl> } <nl> } <nl>  <nl> - memset ( usb , 0 , sizeof ( usb )); <nl> + memset ( usb , 0 , sizeof (* usb )); <nl> usb -> init_flags = flags ; <nl>  <nl> /* Initialize the USB state structure */
err_iounmap : <nl> err_free_mem_region : <nl> release_mem_region ( res -> start , resource_size ( res )); <nl> err_free_mem : <nl> - input_free_device ( kbc -> idev ); <nl> + input_free_device ( input_dev ); <nl> kfree ( kbc ); <nl>  <nl> return err ;
static void setup_rt_frame ( int sig , struct k_sigaction * ka , siginfo_t * info , <nl>  <nl> set_fs ( USER_DS ); <nl>  <nl> + /* the tracer may want to single - step inside the handler */ <nl> + if ( test_thread_flag ( TIF_SINGLESTEP )) <nl> + ptrace_notify ( SIGTRAP ); <nl> + <nl> # ifdef DEBUG_SIG <nl> printk ( KERN_INFO " SIG deliver (% s :% d ): sp =% p pc =% 08lx \ n ", <nl> current -> comm , current -> pid , frame , regs -> pc );
int cap_bprm_set_creds ( struct linux_binprm * bprm ) <nl> { <nl> const struct cred * old = current_cred (); <nl> struct cred * new = bprm -> cred ; <nl> - bool effective , has_cap ; <nl> + bool effective , has_cap = false ; <nl> int ret ; <nl>  <nl> effective = false ;
static int audio_set_pcm_format ( struct snd_pcm_hardware * pcm_hw , <nl> if ( cfg -> subbuffer_size != 1 ) <nl> goto error ; <nl> pr_info (" PCM format is 8 - bit mono \ n "); <nl> + pcm_hw -> channels_min = 1 ; <nl> + pcm_hw -> channels_max = 1 ; <nl> pcm_hw -> formats = SNDRV_PCM_FMTBIT_S8 ; <nl> } else if (! strcmp ( pcm_format , " 2x16 ")) { <nl> if ( cfg -> subbuffer_size != 4 )
static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath_softc * sc = hw -> priv ; <nl> int ret = 0 ; <nl>  <nl> - local_bh_disable (); <nl> + mutex_lock (& sc -> mutex ); <nl>  <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( sc -> sc_ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> - local_bh_enable (); <nl> + mutex_unlock (& sc -> mutex ); <nl>  <nl> return ret ; <nl> }
int sc_ioctl ( int card , scs_ioctl * data ) <nl>  <nl> case SCIOCSTART : <nl> { <nl> + kfree ( rcvmsg ); <nl> pr_debug ("% s : SCIOSTART : ioctl received \ n ", <nl> sc_adapter [ card ]-> devicename ); <nl> if ( sc_adapter [ card ]-> EngineUp ) {
drm_atomic_helper_wait_for_vblanks ( struct drm_device * dev , <nl> for_each_crtc_in_state ( old_state , crtc , old_crtc_state , i ) { <nl> struct drm_crtc_state * new_crtc_state = crtc -> state ; <nl>  <nl> - if (! new_crtc_state -> active ) <nl> - continue ; <nl> - <nl> - if (! drm_atomic_helper_framebuffer_changed ( dev , <nl> - old_state , crtc )) <nl> + if (! new_crtc_state -> active || ! new_crtc_state -> planes_changed ) <nl> continue ; <nl>  <nl> ret = drm_crtc_vblank_get ( crtc );
static int unix_dgram_recvmsg ( struct socket * sock , struct msghdr * msg , <nl> goto out_unlock ; <nl> } <nl>  <nl> - wake_up_interruptible_sync_poll (& u -> peer_wait , <nl> - POLLOUT | POLLWRNORM | POLLWRBAND ); <nl> + if ( wq_has_sleeper (& u -> peer_wait )) <nl> + wake_up_interruptible_sync_poll (& u -> peer_wait , <nl> + POLLOUT | POLLWRNORM | <nl> + POLLWRBAND ); <nl>  <nl> if ( msg -> msg_name ) <nl> unix_copy_addr ( msg , skb -> sk );
static int mv88e6xxx_set_port_state ( struct dsa_switch * ds , int port , u8 state ) <nl> mutex_lock (& ps -> smi_mutex ); <nl>  <nl> reg = _mv88e6xxx_reg_read ( ds , REG_PORT ( port ), PORT_CONTROL ); <nl> - if ( reg < 0 ) <nl> + if ( reg < 0 ) { <nl> + ret = reg ; <nl> goto abort ; <nl> + } <nl>  <nl> oldstate = reg & PORT_CONTROL_STATE_MASK ; <nl> if ( oldstate != state ) {
static int parse_scriptname ( const struct option * opt __used , <nl> script ++; <nl> } else { <nl> script = str ; <nl> - ext = strchr ( script , '.'); <nl> + ext = strrchr ( script , '.'); <nl> if (! ext ) { <nl> fprintf ( stderr , " invalid script extension "); <nl> return - 1 ;
static int bnx2x_populate_ext_phy ( struct bnx2x * bp , <nl> return - EINVAL ; <nl> default : <nl> * phy = phy_null ; <nl> + /* In case external PHY wasn ' t found */ <nl> + if (( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_DIRECT ) && <nl> + ( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_NOT_CONN )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
int fscrypt_ioctl_set_policy ( struct file * filp , const void __user * arg ) <nl> printk ( KERN_WARNING <nl> "% s : Policy inconsistent with encryption context \ n ", <nl> __func__ ); <nl> - ret = - EINVAL ; <nl> + ret = - EEXIST ; <nl> } <nl>  <nl> inode_unlock ( inode );
static void doc_delay ( struct docg3 * docg3 , int nbNOPs ) <nl> { <nl> int i ; <nl>  <nl> - doc_dbg (" NOP x % d \ n ", nbNOPs ); <nl> + doc_vdbg (" NOP x % d \ n ", nbNOPs ); <nl> for ( i = 0 ; i < nbNOPs ; i ++) <nl> doc_writeb ( docg3 , 0 , DOC_NOP ); <nl> }
static int digi_read_oob_callback ( struct urb * urb ) <nl> return - 1 ; <nl>  <nl> /* handle each oob command */ <nl> - for ( i = 0 ; i < urb -> actual_length - 4 ; i += 4 ) { <nl> + for ( i = 0 ; i < urb -> actual_length - 3 ; i += 4 ) { <nl> opcode = buf [ i ]; <nl> line = buf [ i + 1 ]; <nl> status = buf [ i + 2 ];
static int rt298_i2c_probe ( struct i2c_client * i2c , <nl>  <nl> /* enable jack combo mode on supported devices */ <nl> acpiid = acpi_match_device ( dev -> driver -> acpi_match_table , dev ); <nl> - if ( acpiid ) { <nl> + if ( acpiid && acpiid -> driver_data ) { <nl> rt298 -> pdata = *( struct rt298_platform_data *) <nl> acpiid -> driver_data ; <nl> }
i915_error_object_create_sized ( struct drm_i915_private * dev_priv , <nl> goto unwind ; <nl>  <nl> local_irq_save ( flags ); <nl> - if ( reloc_offset < dev_priv -> gtt . mappable_end && <nl> + if ( src -> cache_level == I915_CACHE_NONE && <nl> + reloc_offset < dev_priv -> gtt . mappable_end && <nl> src -> has_global_gtt_mapping && <nl> i915_is_ggtt ( vm )) { <nl> void __iomem * s ;
done : <nl> /* Timeouts occur when the device isn ' t connected , so they ' re <nl> * " normal " -- don ' t fill the kernel log with these */ <nl> if ( status & DP_AUX_CH_CTL_TIME_OUT_ERROR ) { <nl> - DRM_DEBUG_KMS (" dp_aux_ch timeout status 0x % 08x \ n ", status ); <nl> + DRM_DEBUG_KMS_RATELIMITED (" dp_aux_ch timeout status 0x % 08x \ n ", <nl> + status ); <nl> ret = - ETIMEDOUT ; <nl> goto out ; <nl> }
void x86_pci_root_bus_res_quirks ( struct pci_bus * b ) <nl> int j ; <nl> struct pci_root_info * info ; <nl>  <nl> + /* don ' t go for it if _CRS is used */ <nl> + if ( pci_probe & PCI_USE__CRS ) <nl> + return ; <nl> + <nl> /* if only one root bus , don ' t need to anything */ <nl> if ( pci_root_num < 2 ) <nl> return ;
u32 ft_get_task_tag ( struct se_cmd * se_cmd ) <nl> { <nl> struct ft_cmd * cmd = container_of ( se_cmd , struct ft_cmd , se_cmd ); <nl>  <nl> + if ( cmd -> aborted ) <nl> + return ~ 0 ; <nl> return fc_seq_exch ( cmd -> seq )-> rxid ; <nl> } <nl> 
next_file : <nl> mnt_drop_write_file ( dst_file ); <nl> next_loop : <nl> fdput ( dst_fd ); <nl> + <nl> + if ( fatal_signal_pending ( current )) <nl> + goto out ; <nl> } <nl>  <nl> out :
static int port_fops_open ( struct inode * inode , struct file * filp ) <nl>  <nl> /* We get the port with a kref here */ <nl> port = find_port_by_devt ( cdev -> dev ); <nl> + if (! port ) { <nl> + /* Port was unplugged before we could proceed */ <nl> + return - ENXIO ; <nl> + } <nl> filp -> private_data = port ; <nl>  <nl> /*
void __cpuinit generic_processor_info ( int apicid , int version ) <nl> num_processors ++; <nl> cpu = cpumask_next_zero (- 1 , cpu_present_mask ); <nl>  <nl> + if ( version != apic_version [ boot_cpu_physical_apicid ]) <nl> + WARN_ONCE ( 1 , <nl> + " ACPI : apic version mismatch , bootcpu : % x cpu % d : % x \ n ", <nl> + apic_version [ boot_cpu_physical_apicid ], cpu , version ); <nl> + <nl> physid_set ( apicid , phys_cpu_present_map ); <nl> if ( apicid == boot_cpu_physical_apicid ) { <nl> /*
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
static int tegra_dma_probe ( struct platform_device * pdev ) <nl> if ( ret ) { <nl> dev_err (& pdev -> dev , <nl> " request_irq failed with err % d channel % d \ n ", <nl> - i , ret ); <nl> + ret , i ); <nl> goto err_irq ; <nl> } <nl> 
static void rtl8169_init_phy ( struct net_device * dev , struct rtl8169_private * tp ) <nl> rtl8169_set_speed ( dev , AUTONEG_ENABLE , SPEED_1000 , DUPLEX_FULL , <nl> ADVERTISED_10baseT_Half | ADVERTISED_10baseT_Full | <nl> ADVERTISED_100baseT_Half | ADVERTISED_100baseT_Full | <nl> - tp -> mii . supports_gmii ? <nl> + ( tp -> mii . supports_gmii ? <nl> ADVERTISED_1000baseT_Half | <nl> - ADVERTISED_1000baseT_Full : 0 ); <nl> + ADVERTISED_1000baseT_Full : 0 )); <nl>  <nl> if ( RTL_R8 ( PHYstatus ) & TBI_Enable ) <nl> netif_info ( tp , link , dev , " TBI auto - negotiating \ n ");
static void irq_domain_disassociate_many ( struct irq_domain * domain , <nl> while ( count --) { <nl> int irq = irq_base + count ; <nl> struct irq_data * irq_data = irq_get_irq_data ( irq ); <nl> - irq_hw_number_t hwirq = irq_data -> hwirq ; <nl> + irq_hw_number_t hwirq ; <nl>  <nl> if ( WARN_ON (! irq_data || irq_data -> domain != domain )) <nl> continue ; <nl>  <nl> + hwirq = irq_data -> hwirq ; <nl> irq_set_status_flags ( irq , IRQ_NOREQUEST ); <nl>  <nl> /* remove chip and handler */
static void free_migration ( struct dm_cache_migration * mg ) <nl> wake_up (& cache -> migration_wait ); <nl>  <nl> mempool_free ( mg , cache -> migration_pool ); <nl> - wake_worker ( cache ); <nl> } <nl>  <nl> static int prealloc_data_structs ( struct cache * cache , struct prealloc * p )
static ssize_t chars_in_buffer ( struct tty_struct * tty ) <nl>  <nl> static ssize_t n_tty_chars_in_buffer ( struct tty_struct * tty ) <nl> { <nl> + WARN_ONCE ( 1 , "% s is deprecated and scheduled for removal .", __func__ ); <nl> return chars_in_buffer ( tty ); <nl> } <nl> 
static int tg3_init_one ( struct pci_dev * pdev , <nl>  <nl> tg3_timer_init ( tp ); <nl>  <nl> + tg3_carrier_off ( tp ); <nl> + <nl> err = register_netdev ( dev ); <nl> if ( err ) { <nl> dev_err (& pdev -> dev , " Cannot register net device , aborting \ n ");
static int xfrm_get_policy ( struct sk_buff * skb , struct nlmsghdr * nlh , void ** xfr <nl> MSG_DONTWAIT ); <nl> } <nl> } else { <nl> + c . data . byid = p -> index ; <nl> c . event = XFRM_SAP_DELETED ; <nl> c . seq = nlh -> nlmsg_seq ; <nl> c . pid = nlh -> nlmsg_pid ;
int msm_proc_comm ( unsigned cmd , unsigned * data1 , unsigned * data2 ) <nl> * and unknown state . This function should be called early to <nl> * wait on the ARM9 . <nl> */ <nl> - void __init proc_comm_boot_wait ( void ) <nl> + void __devinit proc_comm_boot_wait ( void ) <nl> { <nl> void __iomem * base = MSM_SHARED_RAM_BASE ; <nl> 
drm_display_mode_from_vic_index ( struct drm_connector * connector , <nl> return NULL ; <nl>  <nl> newmode = drm_mode_duplicate ( dev , & edid_cea_modes [ cea_mode ]); <nl> + if (! newmode ) <nl> + return NULL ; <nl> + <nl> newmode -> vrefresh = 0 ; <nl>  <nl> return newmode ;
static struct ad5755_platform_data * ad5755_parse_dt ( struct device * dev ) <nl>  <nl> devnr = 0 ; <nl> for_each_child_of_node ( np , pp ) { <nl> - if ( devnr > AD5755_NUM_CHANNELS ) { <nl> + if ( devnr >= AD5755_NUM_CHANNELS ) { <nl> dev_err ( dev , <nl> " There is to many channels defined in DT \ n "); <nl> goto error_out ;
int trace_get_user ( struct trace_parser * parser , const char __user * ubuf , <nl>  <nl> /* read the non - space input */ <nl> while ( cnt && ! isspace ( ch )) { <nl> - if ( parser -> idx < parser -> size ) <nl> + if ( parser -> idx < parser -> size - 1 ) <nl> parser -> buffer [ parser -> idx ++] = ch ; <nl> else { <nl> ret = - EINVAL ;
static int __devinit iic_probe ( struct ocp_device * ocp ){ <nl> strcpy ( adap -> name , " IBM IIC "); <nl> i2c_set_adapdata ( adap , dev ); <nl> adap -> id = I2C_HW_OCP ; <nl> + adap -> class = I2C_CLASS_HWMON ; <nl> adap -> algo = & iic_algo ; <nl> adap -> client_register = NULL ; <nl> adap -> client_unregister = NULL ;
static int tty_fasync ( int fd , struct file * filp , int on ) <nl> pid = task_pid ( current ); <nl> type = PIDTYPE_PID ; <nl> } <nl> - spin_unlock_irqrestore (& tty -> ctrl_lock , flags ); <nl> retval = __f_setown ( filp , pid , type , 0 ); <nl> + spin_unlock_irqrestore (& tty -> ctrl_lock , flags ); <nl> if ( retval ) <nl> goto out ; <nl> } else {
int mesh_allocated ; <nl> static struct kmem_cache * rm_cache ; <nl>  <nl> -# ifdef CONFIG_MAC80211_MESH <nl> bool mesh_action_is_path_sel ( struct ieee80211_mgmt * mgmt ) <nl> { <nl> return ( mgmt -> u . action . u . mesh_action . action_code == <nl> WLAN_MESH_ACTION_HWMP_PATH_SELECTION ); <nl> } <nl> -# else <nl> - bool mesh_action_is_path_sel ( struct ieee80211_mgmt * mgmt ) <nl> -{ return false ; } <nl> -# endif <nl>  <nl> void ieee80211s_init ( void ) <nl> {
static int __devinit sta2x11_mfd_probe ( struct pci_dev * pdev , <nl> sta2x11_mfd_setup ( pdev , setup_data ); <nl>  <nl> /* Record this pdev before mfd_add_devices : their probe looks for it */ <nl> - sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl> + if (! sta2x11_mfd_find ( pdev )) <nl> + sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl>  <nl> /* Just 2 bars for all mfd ' s at present */ <nl> for ( i = 0 ; i < 2 ; i ++) {
static int nokia_modem_probe ( struct device * dev ) <nl> dev_set_drvdata ( dev , modem ); <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> - if ( irq < 0 ) { <nl> + if (! irq ) { <nl> dev_err ( dev , " Invalid rst_ind interrupt (% d )\ n ", irq ); <nl> - return irq ; <nl> + return - EINVAL ; <nl> } <nl> modem -> nokia_modem_rst_ind_irq = irq ; <nl> pflags = irq_get_trigger_type ( irq );
static int __ixgbe_shutdown ( struct pci_dev * pdev , bool * enable_wake ) <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + rtnl_lock (); <nl> if ( netif_running ( netdev )) { <nl> - rtnl_lock (); <nl> ixgbe_down ( adapter ); <nl> ixgbe_free_irq ( adapter ); <nl> ixgbe_free_all_tx_resources ( adapter ); <nl> ixgbe_free_all_rx_resources ( adapter ); <nl> - rtnl_unlock (); <nl> } <nl> + rtnl_unlock (); <nl>  <nl> ixgbe_clear_interrupt_scheme ( adapter ); <nl> 
int __init ks_dw_pcie_host_init ( struct keystone_pcie * ks_pcie , <nl>  <nl> /* Index 0 is the config reg . space address */ <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - pci -> dbi_base = devm_ioremap_resource ( dev , res ); <nl> + pci -> dbi_base = devm_pci_remap_cfg_resource ( dev , res ); <nl> if ( IS_ERR ( pci -> dbi_base )) <nl> return PTR_ERR ( pci -> dbi_base ); <nl> 
static struct cphy * my3126_phy_create ( adapter_t * adapter , <nl> { <nl> struct cphy * cphy = kzalloc ( sizeof (* cphy ), GFP_KERNEL ); <nl>  <nl> - if ( cphy ) <nl> - cphy_init ( cphy , adapter , phy_addr , & my3126_ops , mdio_ops ); <nl> + if (! cphy ) <nl> + return NULL ; <nl>  <nl> + cphy_init ( cphy , adapter , phy_addr , & my3126_ops , mdio_ops ); <nl> INIT_DELAYED_WORK (& cphy -> phy_update , my3216_poll ); <nl> cphy -> bmsr = 0 ; <nl> 
static int gp8psk_fe_read_signal_strength ( struct dvb_frontend * fe , u16 * strength <nl>  <nl> static int gp8psk_fe_get_tune_settings ( struct dvb_frontend * fe , struct dvb_frontend_tune_settings * tune ) <nl> { <nl> - tune -> min_delay_ms = 200 ; <nl> + tune -> min_delay_ms = 800 ; <nl> return 0 ; <nl> } <nl> 
int max8998_irq_init ( struct max8998_dev * max8998 ) <nl>  <nl> void max8998_irq_exit ( struct max8998_dev * max8998 ) <nl> { <nl> + if ( max8998 -> ono ) <nl> + free_irq ( max8998 -> ono , max8998 ); <nl> + <nl> if ( max8998 -> irq ) <nl> free_irq ( max8998 -> irq , max8998 ); <nl> }
trace_print_graph_duration ( unsigned long long duration , struct trace_seq * s ) <nl>  <nl> /* Print nsecs ( we don ' t want to exceed 7 numbers ) */ <nl> if ( len < 7 ) { <nl> - snprintf ( nsecs_str , 8 - len , "% 03lu ", nsecs_rem ); <nl> + snprintf ( nsecs_str , min ( sizeof ( nsecs_str ), 8UL - len ), "% 03lu ", <nl> + nsecs_rem ); <nl> ret = trace_seq_printf ( s , ".% s ", nsecs_str ); <nl> if (! ret ) <nl> return TRACE_TYPE_PARTIAL_LINE ;
static ssize_t display_upd_mode_store ( struct device * dev , <nl> int val , r ; <nl> enum omap_dss_update_mode mode ; <nl>  <nl> + if (! dssdev -> driver -> set_update_mode ) <nl> + return - EINVAL ; <nl> + <nl> val = simple_strtoul ( buf , NULL , 10 ); <nl>  <nl> switch ( val ) {
static int macvtap_ioctl_set_queue ( struct file * file , unsigned int flags ) <nl> ret = macvtap_enable_queue ( vlan -> dev , file , q ); <nl> else if ( flags & IFF_DETACH_QUEUE ) <nl> ret = macvtap_disable_queue ( q ); <nl> + else <nl> + ret = - EINVAL ; <nl>  <nl> macvtap_put_vlan ( vlan ); <nl> return ret ;
ssize_t tcp_splice_read ( struct socket * sock , loff_t * ppos , <nl> ssize_t spliced ; <nl> int ret ; <nl>  <nl> + sock_rps_record_flow ( sk ); <nl> /* <nl> * We can ' t seek on a socket input <nl> */
static int arche_platform_probe ( struct platform_device * pdev ) <nl> arche_pdata -> wake_detect_gpio = of_get_named_gpio ( np , " svc , wake - detect - gpio ", 0 ); <nl> if ( arche_pdata -> wake_detect_gpio < 0 ) { <nl> dev_err ( dev , " failed to get wake detect gpio \ n "); <nl> - ret = arche_pdata -> wake_detect_gpio ; <nl> - return ret ; <nl> + return arche_pdata -> wake_detect_gpio ; <nl> } <nl>  <nl> ret = devm_gpio_request ( dev , arche_pdata -> wake_detect_gpio , " wake detect ");
int btrfs_open_devices ( struct btrfs_fs_devices * fs_devices , <nl> goto error_brelse ; <nl>  <nl> transid = btrfs_super_generation ( disk_super ); <nl> - if ( transid > latest_transid ) { <nl> + if (! latest_transid || transid > latest_transid ) { <nl> latest_devid = devid ; <nl> latest_transid = transid ; <nl> latest_bdev = bdev ;
static void __cpuinit put_core_offline ( unsigned int cpu ) <nl>  <nl> indx = TO_ATTR_NO ( cpu ); <nl>  <nl> + /* The core id is too big , just return */ <nl> + if ( indx > MAX_CORE_DATA - 1 ) <nl> + return ; <nl> + <nl> if ( pdata -> core_data [ indx ] && pdata -> core_data [ indx ]-> cpu == cpu ) <nl> coretemp_remove_core ( pdata , & pdev -> dev , indx ); <nl> 
nfqnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl>  <nl> if ( nfqa [ NFQA_CFG_PARAMS - 1 ]) { <nl> struct nfqnl_msg_config_params * params ; <nl> - params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl>  <nl> + if (! queue ) { <nl> + ret = - ENOENT ; <nl> + goto out_put ; <nl> + } <nl> + params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl> nfqnl_set_mode ( queue , params -> copy_mode , <nl> ntohl ( params -> copy_range )); <nl> }
static int snd_hdsp_hwdep_ioctl ( struct snd_hwdep * hw , struct file * file , unsigne <nl> if (( err = hdsp_get_iobox_version ( hdsp )) < 0 ) <nl> return err ; <nl> } <nl> + memset (& hdsp_version , 0 , sizeof ( hdsp_version )); <nl> hdsp_version . io_type = hdsp -> io_type ; <nl> hdsp_version . firmware_rev = hdsp -> firmware_rev ; <nl> if (( err = copy_to_user ( argp , & hdsp_version , sizeof ( hdsp_version ))))
static int do_garbage_collect ( struct f2fs_sb_info * sbi , <nl>  <nl> for ( segno = start_segno ; segno < end_segno ; segno ++) { <nl>  <nl> - if ( get_valid_blocks ( sbi , segno , 1 ) == 0 ) <nl> + if ( get_valid_blocks ( sbi , segno , 1 ) == 0 || <nl> + unlikely ( f2fs_cp_error ( sbi ))) <nl> goto next ; <nl>  <nl> /* find segment summary of victim */
static int log_one_block ( struct log_writes_c * lc , <nl> goto out ; <nl> sector ++; <nl>  <nl> + atomic_inc (& lc -> io_blocks ); <nl> bio = bio_alloc ( GFP_KERNEL , block -> vec_cnt ); <nl> if (! bio ) { <nl> DMERR (" Couldn ' t alloc log bio "); <nl> goto error ; <nl> } <nl> - atomic_inc (& lc -> io_blocks ); <nl> bio -> bi_iter . bi_size = 0 ; <nl> bio -> bi_iter . bi_sector = sector ; <nl> bio -> bi_bdev = lc -> logdev -> bdev ;
static int cgroup_rmdir ( struct kernfs_node * kn ) <nl> cgrp = cgroup_kn_lock_live ( kn ); <nl> if (! cgrp ) <nl> return 0 ; <nl> - cgroup_get ( cgrp ); /* for @ kn -> priv clearing */ <nl>  <nl> ret = cgroup_destroy_locked ( cgrp ); <nl>  <nl> cgroup_kn_unlock ( kn ); <nl> - <nl> - cgroup_put ( cgrp ); <nl> return ret ; <nl> } <nl> 
static struct xfrm_state * pfkey_msg2xfrm_state ( struct net * net , <nl> x -> aalg -> alg_key_len = key -> sadb_key_bits ; <nl> memcpy ( x -> aalg -> alg_key , key + 1 , keysize ); <nl> } <nl> + x -> aalg -> alg_trunc_len = a -> uinfo . auth . icv_truncbits ; <nl> x -> props . aalgo = sa -> sadb_sa_auth ; <nl> /* x -> algo . flags = sa -> sadb_sa_flags ; */ <nl> }
static int mwl8k_request_firmware ( struct mwl8k_priv * priv ) <nl> return 0 ; <nl> } <nl>  <nl> + MODULE_FIRMWARE (" mwl8k / helper_8687 . fw "); <nl> + MODULE_FIRMWARE (" mwl8k / fmimage_8687 . fw "); <nl> + <nl> struct mwl8k_cmd_pkt { <nl> __le16 code ; <nl> __le16 length ;
static int sh_eth_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> skb -> len + 2 ); <nl> txdesc -> addr = dma_map_single (& ndev -> dev , skb -> data , skb -> len , <nl> DMA_TO_DEVICE ); <nl> + if ( dma_mapping_error (& ndev -> dev , txdesc -> addr )) { <nl> + kfree_skb ( skb ); <nl> + return NETDEV_TX_OK ; <nl> + } <nl> txdesc -> buffer_length = skb -> len ; <nl>  <nl> if ( entry >= mdp -> num_tx_ring - 1 )
static int do_tcp_getsockopt ( struct sock * sk , int level , <nl> val = tp -> mss_cache ; <nl> if (! val && (( 1 << sk -> sk_state ) & ( TCPF_CLOSE | TCPF_LISTEN ))) <nl> val = tp -> rx_opt . user_mss ; <nl> + if ( tp -> repair ) <nl> + val = tp -> rx_opt . mss_clamp ; <nl> break ; <nl> case TCP_NODELAY : <nl> val = !!( tp -> nonagle & TCP_NAGLE_OFF );
static int pm8001_chip_sata_req ( struct pm8001_hba_info * pm8001_ha , <nl>  <nl> /* Check for read log for failed drive and return */ <nl> if ( sata_cmd . sata_fis . command == 0x2f ) { <nl> - if ( pm8001_ha_dev && (( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> + if ((( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_ABORT_ALL_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_2ND_RLE_FLAG ))) { <nl> struct task_status_struct * ts ;
static int wm8750_set_bias_level ( struct snd_soc_codec * codec , <nl> case SND_SOC_BIAS_PREPARE : <nl> break ; <nl> case SND_SOC_BIAS_STANDBY : <nl> - if ( codec -> dapm . bias_level == SND_SOC_BIAS_OFF ) { <nl> + if ( snd_soc_codec_get_bias_level ( codec ) == SND_SOC_BIAS_OFF ) { <nl> snd_soc_cache_sync ( codec ); <nl>  <nl> /* Set VMID to 5k */
struct virtio_device_id { <nl>  <nl> struct i2c_device_id { <nl> char name [ I2C_NAME_SIZE ]; <nl> - kernel_ulong_t driver_data ; /* Data private to the driver */ <nl> + kernel_ulong_t driver_data /* Data private to the driver */ <nl> + __attribute__ (( aligned ( sizeof ( kernel_ulong_t )))); <nl> }; <nl>  <nl> 
static int read_bus_info_block ( struct fw_device * device , int generation ) <nl> return - ENOMEM ; <nl>  <nl> stack = & rom [ READ_BIB_ROM_SIZE ]; <nl> + memset ( rom , 0 , sizeof (* rom ) * READ_BIB_ROM_SIZE ); <nl>  <nl> device -> max_speed = SCODE_100 ; <nl> 
out_disable_phy : <nl> out_unregister_bus : <nl> phy_exit ( host -> generic_phy ); <nl> out_host_free : <nl> - devm_kfree ( dev , host ); <nl> ufshcd_set_variant ( hba , NULL ); <nl> out : <nl> return err ;
static void ipsec_esp_decrypt_swauth_done ( struct device * dev , <nl> } else <nl> oicv = ( char *)& edesc -> link_tbl [ 0 ]; <nl>  <nl> - err = memcmp ( oicv , icv , authsize ) ? - EBADMSG : 0 ; <nl> + err = crypto_memneq ( oicv , icv , authsize ) ? - EBADMSG : 0 ; <nl> } <nl>  <nl> kfree ( edesc );
static void _rtl_usb_tx_preprocess ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> u8 tid = 0 ; <nl> u16 seq_number = 0 ; <nl>  <nl> + memset (& tcb_desc , 0 , sizeof ( struct rtl_tcb_desc )); <nl> if ( ieee80211_is_auth ( fc )) { <nl> RT_TRACE ( rtlpriv , COMP_SEND , DBG_DMESG , (" MAC80211_LINKING \ n ")); <nl> rtl_ips_nic_on ( hw );
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> + free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
static int ohci_init ( struct ohci_hcd * ohci ) <nl> return 0 ; <nl>  <nl> ohci -> hcca = dma_alloc_coherent ( hcd -> self . controller , <nl> - sizeof * ohci -> hcca , & ohci -> hcca_dma , 0 ); <nl> + sizeof (* ohci -> hcca ), & ohci -> hcca_dma , GFP_KERNEL ); <nl> if (! ohci -> hcca ) <nl> return - ENOMEM ; <nl> 
void __iomem * __ioremap ( unsigned long phys_addr , unsigned long size , unsigned l <nl> */ <nl> offset = phys_addr & ~ PAGE_MASK ; <nl> phys_addr &= PAGE_MASK ; <nl> - size = PAGE_ALIGN ( last_addr ) - phys_addr ; <nl> + size = PAGE_ALIGN ( last_addr + 1 ) - phys_addr ; <nl>  <nl> /* <nl> * Ok , go for it ..
static inline struct kvm_vcpu * kvm_get_vcpu_by_id ( struct kvm * kvm , int id ) <nl> struct kvm_vcpu * vcpu ; <nl> int i ; <nl>  <nl> + if ( id < 0 || id >= KVM_MAX_VCPUS ) <nl> + return NULL ; <nl> + vcpu = kvm_get_vcpu ( kvm , id ); <nl> + if ( vcpu && vcpu -> vcpu_id == id ) <nl> + return vcpu ; <nl> kvm_for_each_vcpu ( i , vcpu , kvm ) <nl> if ( vcpu -> vcpu_id == id ) <nl> return vcpu ;
static int set_wep_key ( struct airo_info * ai , u16 index , const char * key , <nl> WepKeyRid wkr ; <nl> int rc ; <nl>  <nl> - WARN_ON ( keylen == 0 ); <nl> + if ( WARN_ON ( keylen == 0 )) <nl> + return - 1 ; <nl>  <nl> memset (& wkr , 0 , sizeof ( wkr )); <nl> wkr . len = cpu_to_le16 ( sizeof ( wkr ));
long kernel_wait4 ( pid_t upid , int __user * stat_addr , int options , <nl> __WNOTHREAD | __WCLONE | __WALL )) <nl> return - EINVAL ; <nl>  <nl> + /* - INT_MIN is not defined */ <nl> + if ( upid == INT_MIN ) <nl> + return - ESRCH ; <nl> + <nl> if ( upid == - 1 ) <nl> type = PIDTYPE_MAX ; <nl> else if ( upid < 0 ) {
static struct platform_device_id armpmu_plat_device_ids [] = { <nl>  <nl> static int __devinit armpmu_device_probe ( struct platform_device * pdev ) <nl> { <nl> + if (! cpu_pmu ) <nl> + return - ENODEV ; <nl> + <nl> cpu_pmu -> plat_device = pdev ; <nl> return 0 ; <nl> }
static int ecryptfs_write_begin ( struct file * file , <nl> && ( pos != 0 )) <nl> zero_user ( page , 0 , PAGE_CACHE_SIZE ); <nl> out : <nl> + if ( unlikely ( rc )) { <nl> + unlock_page ( page ); <nl> + page_cache_release ( page ); <nl> + * pagep = NULL ; <nl> + } <nl> return rc ; <nl> } <nl> 
# define CR8_RESERVED_BITS (~( unsigned long ) X86_CR8_TPR ) <nl>  <nl> # define KVM_MAX_MCE_BANKS 32 <nl> -# define KVM_MCE_CAP_SUPPORTED MCG_CTL_P <nl> +# define KVM_MCE_CAP_SUPPORTED ( MCG_CTL_P | MCG_SER_P ) <nl>  <nl> /* EFER defaults : <nl> * - enable syscall per default because its emulated by KVM
static int cdrom_read_cdda_bpc ( struct cdrom_device_info * cdi , __u8 __user * ubuf , <nl> if (! q ) <nl> return - ENXIO ; <nl>  <nl> + if (! blk_queue_scsi_passthrough ( q )) { <nl> + WARN_ONCE ( true , <nl> + " Attempt read CDDA info through a non - SCSI queue \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> cdi -> last_sense = 0 ; <nl>  <nl> while ( nframes ) {
int fib_dump_info ( struct sk_buff * skb , u32 portid , u32 seq , int event , <nl> IN_DEV_IGNORE_ROUTES_WITH_LINKDOWN ( in_dev )) <nl> rtm -> rtm_flags |= RTNH_F_DEAD ; <nl> } <nl> + if ( fi -> fib_nh -> nh_flags & RTNH_F_OFFLOAD ) <nl> + rtm -> rtm_flags |= RTNH_F_OFFLOAD ; <nl> # ifdef CONFIG_IP_ROUTE_CLASSID <nl> if ( fi -> fib_nh [ 0 ]. nh_tclassid && <nl> nla_put_u32 ( skb , RTA_FLOW , fi -> fib_nh [ 0 ]. nh_tclassid ))
static int i2c_hid_hwreset ( struct i2c_client * client ) <nl> static void i2c_hid_get_input ( struct i2c_hid * ihid ) <nl> { <nl> int ret , ret_size ; <nl> - int size = le16_to_cpu ( ihid -> hdesc . wMaxInputLength ); <nl> + int size = ihid -> bufsize ; <nl>  <nl> ret = i2c_master_recv ( ihid -> client , ihid -> inbuf , size ); <nl> if ( ret != size ) {
static int mxs_lradc_probe ( struct platform_device * pdev ) <nl> * of the array . <nl> */ <nl> scale_uv = (( u64 ) lradc -> vref_mv [ i ] * 100000000 ) >> <nl> - ( iio -> channels [ i ]. scan_type . realbits - s ); <nl> + ( LRADC_RESOLUTION - s ); <nl> lradc -> scale_avail [ i ][ s ]. nano = <nl> do_div ( scale_uv , 100000000 ) * 10 ; <nl> lradc -> scale_avail [ i ][ s ]. integer = scale_uv ;
ff_layout_alloc_lseg ( struct pnfs_layout_hdr * lh , <nl> goto out_err_free ; <nl>  <nl> /* fh */ <nl> + rc = - EIO ; <nl> p = xdr_inline_decode (& stream , 4 ); <nl> if (! p ) <nl> goto out_err_free ;
static int match_session ( struct cifs_ses * ses , struct smb_vol * vol ) <nl> vol -> username ? vol -> username : "", <nl> CIFS_MAX_USERNAME_LEN )) <nl> return 0 ; <nl> - if ( strlen ( vol -> username ) != 0 && <nl> + if (( vol -> username && strlen ( vol -> username ) != 0 ) && <nl> ses -> password != NULL && <nl> strncmp ( ses -> password , <nl> vol -> password ? vol -> password : "",
int __init mon_text_init ( void ) <nl> { <nl> struct dentry * mondir ; <nl>  <nl> - mondir = debugfs_create_dir (" usbmon ", NULL ); <nl> + mondir = debugfs_create_dir (" usbmon ", usb_debug_root ); <nl> if ( IS_ERR ( mondir )) { <nl> printk ( KERN_NOTICE TAG ": debugfs is not available \ n "); <nl> return - ENODEV ;
static long kfd_ioctl_create_queue ( struct file * filep , struct kfd_process * p , <nl> p -> pasid , <nl> dev -> id ); <nl>  <nl> - err = pqm_create_queue (& p -> pqm , dev , filep , & q_properties , 0 , <nl> - KFD_QUEUE_TYPE_COMPUTE , & queue_id ); <nl> + err = pqm_create_queue (& p -> pqm , dev , filep , & q_properties , <nl> + 0 , q_properties . type , & queue_id ); <nl> if ( err != 0 ) <nl> goto err_create_queue ; <nl> 
bool dc_stream_set_cursor_position ( <nl> ! pipe_ctx -> ipp || ! pipe_ctx -> surface ) <nl> continue ; <nl>  <nl> + if ( pipe_ctx -> surface -> public . address . type <nl> + == PLN_ADDR_TYPE_VIDEO_PROGRESSIVE ) <nl> + pos_cpy . enable = false ; <nl> + <nl> if ( pipe_ctx -> top_pipe && pipe_ctx -> surface != pipe_ctx -> top_pipe -> surface ) <nl> pos_cpy . enable = false ; <nl> 
static int strip_open ( struct tty_struct * tty ) <nl> * We need a write method . <nl> */ <nl>  <nl> - if ( tty -> ops -> write == NULL ) <nl> + if ( tty -> ops -> write == NULL || tty -> ops -> set_termios == NULL ) <nl> return - EOPNOTSUPP ; <nl>  <nl> /*
static int agpioc_info_wrap ( struct agp_file_private * priv , void __user * arg ) <nl>  <nl> agp_copy_info ( agp_bridge , & kerninfo ); <nl>  <nl> + memset (& userinfo , 0 , sizeof ( userinfo )); <nl> userinfo . version . major = kerninfo . version . major ; <nl> userinfo . version . minor = kerninfo . version . minor ; <nl> userinfo . bridge_id = kerninfo . device -> vendor |
int __init omap_mux_init ( u32 mux_pbase , u32 mux_size , <nl> } <nl>  <nl> # ifdef CONFIG_OMAP_MUX <nl> - omap_mux_package_fixup ( package_subset , superset ); <nl> - omap_mux_package_init_balls ( package_balls , superset ); <nl> + if ( package_subset ) <nl> + omap_mux_package_fixup ( package_subset , superset ); <nl> + if ( package_balls ) <nl> + omap_mux_package_init_balls ( package_balls , superset ); <nl> omap_mux_set_cmdline_signals (); <nl> omap_mux_set_board_signals ( board_mux ); <nl> # endif
kvm_irqfd ( struct kvm * kvm , struct kvm_irqfd * args ) <nl> { <nl> if ( args -> flags & ~( KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE )) <nl> return - EINVAL ; <nl> + if ( args -> gsi >= KVM_MAX_IRQ_ROUTES ) <nl> + return - EINVAL ; <nl>  <nl> if ( args -> flags & KVM_IRQFD_FLAG_DEASSIGN ) <nl> return kvm_irqfd_deassign ( kvm , args );
static int ioat_pci_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> device -> version = readb ( device -> reg_base + IOAT_VER_OFFSET ); <nl> if ( device -> version >= IOAT_VER_3_0 ) { <nl> + if ( is_skx_ioat ( pdev )) <nl> + device -> version = IOAT_VER_3_2 ; <nl> err = ioat3_dma_probe ( device , ioat_dca_enabled ); <nl>  <nl> if ( device -> version >= IOAT_VER_3_3 )
static int mwifiex_pcie_event_complete ( struct mwifiex_adapter * adapter , <nl>  <nl> if (! card -> evt_buf_list [ rdptr ]) { <nl> skb_push ( skb , INTF_HEADER_LEN ); <nl> + skb_put ( skb , MAX_EVENT_SIZE - skb -> len ); <nl> + memset ( skb -> data , 0 , MAX_EVENT_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , <nl> MAX_EVENT_SIZE , <nl> PCI_DMA_FROMDEVICE ))
static int virtnet_set_channels ( struct net_device * dev , <nl> if ( channels -> rx_count || channels -> tx_count || channels -> other_count ) <nl> return - EINVAL ; <nl>  <nl> - if ( queue_pairs > vi -> max_queue_pairs ) <nl> + if ( queue_pairs > vi -> max_queue_pairs || queue_pairs == 0 ) <nl> return - EINVAL ; <nl>  <nl> get_online_cpus ();
static int als_wait_for_data_ready ( struct device * dev ) <nl> ret = i2c_smbus_read_byte_data ( client , 0x86 ); <nl> } while (!( ret & 0x80 ) && retry --); <nl>  <nl> - if (! retry ) { <nl> + if ( retry < 0 ) { <nl> dev_warn ( dev , " timeout waiting for data ready \ n "); <nl> return - ETIMEDOUT ; <nl> }
int sas_smp_get_phy_events ( struct sas_phy * phy ) <nl> phy -> phy_reset_problem_count = scsi_to_u32 (& resp [ 24 ]); <nl>  <nl> out : <nl> + kfree ( req ); <nl> kfree ( resp ); <nl> return res ; <nl> 
void usb_serial_generic_resubmit_read_urb ( struct usb_serial_port * port , <nl> (( serial -> type -> read_bulk_callback ) ? <nl> serial -> type -> read_bulk_callback : <nl> usb_serial_generic_read_bulk_callback ), port ); <nl> + <nl> result = usb_submit_urb ( urb , mem_flags ); <nl> - if ( result ) <nl> + if ( result && result != - EPERM ) { <nl> dev_err (& port -> dev , <nl> "% s - failed resubmitting read urb , error % d \ n ", <nl> __func__ , result ); <nl> + } <nl> } <nl> EXPORT_SYMBOL_GPL ( usb_serial_generic_resubmit_read_urb ); <nl> 
struct extent_buffer * read_tree_block ( struct btrfs_root * root , u64 bytenr , <nl> return NULL ; <nl>  <nl> ret = btree_read_extent_buffer_pages ( root , buf , 0 , parent_transid ); <nl> + if ( ret ) { <nl> + free_extent_buffer ( buf ); <nl> + return NULL ; <nl> + } <nl> return buf ; <nl>  <nl> }
int jbd2_journal_start_reserved ( handle_t * handle , unsigned int type , <nl> */ <nl> ret = start_this_handle ( journal , handle , GFP_NOFS ); <nl> if ( ret < 0 ) { <nl> + handle -> h_journal = journal ; <nl> jbd2_journal_free_reserved ( handle ); <nl> return ret ; <nl> }
static int rndis_wlan_bind ( struct usbnet * usbdev , struct usb_interface * intf ) <nl>  <nl> /* because rndis_command () sleeps we need to use workqueue */ <nl> priv -> workqueue = create_singlethread_workqueue (" rndis_wlan "); <nl> + if (! priv -> workqueue ) { <nl> + wiphy_free ( wiphy ); <nl> + return - ENOMEM ; <nl> + } <nl> INIT_WORK (& priv -> work , rndis_wlan_worker ); <nl> INIT_DELAYED_WORK (& priv -> dev_poller_work , rndis_device_poller ); <nl> INIT_DELAYED_WORK (& priv -> scan_work , rndis_get_scan_results );
int hugetlb_fault ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> * so no worry about deadlock . <nl> */ <nl> page = pte_page ( entry ); <nl> + get_page ( page ); <nl> if ( page != pagecache_page ) <nl> lock_page ( page ); <nl>  <nl> int hugetlb_fault ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> } <nl> if ( page != pagecache_page ) <nl> unlock_page ( page ); <nl> + put_page ( page ); <nl>  <nl> out_mutex : <nl> mutex_unlock (& hugetlb_instantiation_mutex );
lpfc_els_retry ( struct lpfc_hba * phba , struct lpfc_iocbq * cmdiocb , <nl> /* FLOGI retry policy */ <nl> retry = 1 ; <nl> /* retry FLOGI forever */ <nl> - maxretry = 0 ; <nl> + if ( phba -> link_flag != LS_LOOPBACK_MODE ) <nl> + maxretry = 0 ; <nl> + else <nl> + maxretry = 2 ; <nl> + <nl> if ( cmdiocb -> retry >= 100 ) <nl> delay = 5000 ; <nl> else if ( cmdiocb -> retry >= 32 )
static u32 atombios_adjust_pll ( struct drm_crtc * crtc , <nl> if ( radeon_crtc -> ss . refdiv ) { <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_REF_DIV ; <nl> radeon_crtc -> pll_reference_div = radeon_crtc -> ss . refdiv ; <nl> - if ( rdev -> family >= CHIP_RV770 ) <nl> + if ( ASIC_IS_AVIVO ( rdev ) && <nl> + rdev -> family != CHIP_RS780 && <nl> + rdev -> family != CHIP_RS880 ) <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_FRAC_FB_DIV ; <nl> } <nl> }
int fcoe_ctlr_els_send ( struct fcoe_ctlr * fip , struct fc_lport * lport , <nl> fip -> send ( fip , skb ); <nl> return - EINPROGRESS ; <nl> drop : <nl> - kfree_skb ( skb ); <nl> LIBFCOE_FIP_DBG ( fip , " drop els_send op % u d_id % x \ n ", <nl> op , ntoh24 ( fh -> fh_d_id )); <nl> + kfree_skb ( skb ); <nl> return - EINVAL ; <nl> } <nl> EXPORT_SYMBOL ( fcoe_ctlr_els_send );
struct tpm_chip * tpmm_chip_alloc ( struct device * dev , <nl>  <nl> device_initialize (& chip -> dev ); <nl>  <nl> - chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> cdev_init (& chip -> cdev , & tpm_fops ); <nl> + chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> + chip -> cdev . kobj . parent = & chip -> dev . kobj ; <nl>  <nl> return chip ; <nl> }
void update_mmu_cache ( struct vm_area_struct * vma , unsigned long address , <nl> * called with either mm -> page_table_lock held or ptl lock held <nl> */ <nl> unsigned long access = 0 , trap ; <nl> + if ( radix_enabled ()) <nl> + return ; <nl>  <nl> /* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */ <nl> if (! pte_young (* ptep ) || address >= TASK_SIZE )
void intel_display_power_get ( struct drm_i915_private * dev_priv , <nl> struct i915_power_well * power_well ; <nl> int i ; <nl>  <nl> + intel_runtime_pm_get ( dev_priv ); <nl> + <nl> power_domains = & dev_priv -> power_domains ; <nl>  <nl> mutex_lock (& power_domains -> lock ); <nl> void intel_display_power_put ( struct drm_i915_private * dev_priv , <nl> } <nl>  <nl> mutex_unlock (& power_domains -> lock ); <nl> + <nl> + intel_runtime_pm_put ( dev_priv ); <nl> } <nl>  <nl> static struct i915_power_domains * hsw_pwr ;
static int caif_seqpkt_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( m -> msg_flags & MSG_OOB ) <nl> goto read_error ; <nl>  <nl> + m -> msg_namelen = 0 ; <nl> + <nl> skb = skb_recv_datagram ( sk , flags , 0 , & ret ); <nl> if (! skb ) <nl> goto read_error ;
static void rtsx_pci_shutdown ( struct pci_dev * pcidev ) <nl> rtsx_pci_power_off ( pcr , HOST_ENTER_S1 ); <nl>  <nl> pci_disable_device ( pcidev ); <nl> + free_irq ( pcr -> irq , ( void *) pcr ); <nl> + if ( pcr -> msi_en ) <nl> + pci_disable_msi ( pcr -> pci ); <nl> } <nl>  <nl> # else /* CONFIG_PM */
static int get_dma_channel ( struct device_node * ssi_np , <nl> * dai -> platform name should already point to an allocated buffer . <nl> */ <nl> ret = of_address_to_resource ( dma_channel_np , 0 , & res ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + of_node_put ( dma_channel_np ); <nl> return ret ; <nl> + } <nl> snprintf (( char *) dai -> platform_name , DAI_NAME_SIZE , "% llx .% s ", <nl> ( unsigned long long ) res . start , dma_channel_np -> name ); <nl> 
beiscsi_create_wrb_rings ( struct beiscsi_hba * phba , <nl> if ( status != 0 ) { <nl> shost_printk ( KERN_ERR , phba -> shost , <nl> " wrbq create failed ."); <nl> + kfree ( pwrb_arr ); <nl> return status ; <nl> } <nl> phwi_ctrlr -> wrb_context [ i * 2 ]. cid = phwi_context -> be_wrbq [ i ].
static int udf_encode_fh ( struct inode * inode , __u32 * fh , int * lenp , <nl> * lenp = 3 ; <nl> fid -> udf . block = location . logicalBlockNum ; <nl> fid -> udf . partref = location . partitionReferenceNum ; <nl> + fid -> udf . parent_partref = 0 ; <nl> fid -> udf . generation = inode -> i_generation ; <nl>  <nl> if ( parent ) {
void reload_ucode_amd ( void ) <nl> if ( mc && rev < mc -> hdr . patch_id ) { <nl> if (! __apply_microcode_amd ( mc )) { <nl> ucode_new_rev = mc -> hdr . patch_id ; <nl> - pr_info (" microcode : reload patch_level = 0x % 08x \ n ", <nl> - ucode_new_rev ); <nl> + pr_info (" reload patch_level = 0x % 08x \ n ", ucode_new_rev ); <nl> } <nl> } <nl> }
static int __init kvmppc_e500_init ( void ) <nl> return kvm_init ( NULL , sizeof ( struct kvmppc_vcpu_e500 ), 0 , THIS_MODULE ); <nl> } <nl>  <nl> - static void __init kvmppc_e500_exit ( void ) <nl> + static void __exit kvmppc_e500_exit ( void ) <nl> { <nl> kvmppc_booke_exit (); <nl> }
static int dpi_check_timings ( struct omap_dss_device * dssdev , <nl> struct dpi_clk_calc_ctx ctx ; <nl> bool ok ; <nl>  <nl> + if ( timings -> x_res % 8 != 0 ) <nl> + return - EINVAL ; <nl> + <nl> if ( mgr && ! dispc_mgr_timings_ok ( mgr -> id , timings )) <nl> return - EINVAL ; <nl> 
static netdev_tx_t ip6erspan_tunnel_xmit ( struct sk_buff * skb , <nl> truncate = true ; <nl> } <nl>  <nl> + if ( skb_cow_head ( skb , dev -> needed_headroom )) <nl> + goto tx_err ; <nl> + <nl> t -> parms . o_flags &= ~ TUNNEL_KEY ; <nl> IPCB ( skb )-> flags = 0 ; <nl> 
static int s3c24xx_eint_init ( struct samsung_pinctrl_drv_data * d ) <nl> irq = bank -> eint_offset ; <nl> mask = bank -> eint_mask ; <nl> for ( pin = 0 ; mask ; ++ pin , mask >>= 1 ) { <nl> - if ( irq > NUM_EINT ) <nl> + if ( irq >= NUM_EINT ) <nl> break ; <nl> if (!( mask & 1 )) <nl> continue ;
static int rtw_wx_set_enc_ext ( struct net_device * dev , <nl> memset ( param , 0 , param_len ); <nl>  <nl> param -> cmd = IEEE_CMD_SET_ENCRYPTION ; <nl> - memset ( param -> sta_addr , 0xff , ETH_ALEN ); <nl> + eth_broadcast_addr ( param -> sta_addr ); <nl>  <nl> switch ( pext -> alg ) { <nl> case IW_ENCODE_ALG_NONE :
static long aio_read_events_ring ( struct kioctx * ctx , <nl> if ( head == tail ) <nl> goto out ; <nl>  <nl> + head %= ctx -> nr_events ; <nl> + tail %= ctx -> nr_events ; <nl> + <nl> while ( ret < nr ) { <nl> long avail ; <nl> struct io_event * ev ;
void xgbe_debugfs_init ( struct xgbe_prv_data * pdata ) <nl> pdata -> xgbe_debugfs = debugfs_create_dir ( buf , NULL ); <nl> if (! pdata -> xgbe_debugfs ) { <nl> netdev_err ( pdata -> netdev , " debugfs_create_dir failed \ n "); <nl> + kfree ( buf ); <nl> return ; <nl> } <nl> 
bool fw_download_code ( struct net_device * dev , u8 * code_virtual_address , u32 buff <nl> * add 4 to avoid packet appending overflow . <nl> * */ <nl> skb = dev_alloc_skb ( USB_HWDESC_HEADER_LEN + frag_length + 4 ); <nl> + if (! skb ) <nl> + return false ; <nl> memcpy (( unsigned char *)( skb -> cb ),& dev , sizeof ( dev )); <nl> tcb_desc = ( cb_desc *)( skb -> cb + MAX_DEV_ADDR_SIZE ); <nl> tcb_desc -> queue_index = TXCMD_QUEUE ;
int inet_recv_error ( struct sock * sk , struct msghdr * msg , int len , <nl>  <nl> static inline void inet_ctl_sock_destroy ( struct sock * sk ) <nl> { <nl> - sock_release ( sk -> sk_socket ); <nl> + if ( sk ) <nl> + sock_release ( sk -> sk_socket ); <nl> } <nl>  <nl> # endif
int dlm_migrate_request_handler ( struct o2net_msg * msg , u32 len , void * data , <nl> migrate -> new_master , <nl> migrate -> master ); <nl>  <nl> + if ( ret < 0 ) <nl> + kmem_cache_free ( dlm_mle_cache , mle ); <nl> + <nl> spin_unlock (& dlm -> master_lock ); <nl> unlock : <nl> spin_unlock (& dlm -> spinlock );
static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
static int tower_probe ( struct usb_interface * interface , const struct usb_device <nl> USB_MAJOR , dev -> minor ); <nl>  <nl> exit : <nl> + kfree ( get_version_reply ); <nl> return retval ; <nl>  <nl> error :
int evtchn_get ( unsigned int evtchn ) <nl> struct irq_info * info ; <nl> int err = - ENOENT ; <nl>  <nl> + if ( evtchn >= NR_EVENT_CHANNELS ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& irq_mapping_update_lock ); <nl>  <nl> irq = evtchn_to_irq [ evtchn ];
static void ieee80211_csa_connection_drop_work ( struct work_struct * work ) <nl> container_of ( work , struct ieee80211_sub_if_data , <nl> u . ibss . csa_connection_drop_work ); <nl>  <nl> + sdata_lock ( sdata ); <nl> + <nl> ieee80211_ibss_disconnect ( sdata ); <nl> synchronize_rcu (); <nl> skb_queue_purge (& sdata -> skb_queue ); <nl>  <nl> /* trigger a scan to find another IBSS network to join */ <nl> ieee80211_queue_work (& sdata -> local -> hw , & sdata -> work ); <nl> + <nl> + sdata_unlock ( sdata ); <nl> } <nl>  <nl> static void ieee80211_ibss_csa_mark_radar ( struct ieee80211_sub_if_data * sdata )
static struct omap_system_dma_plat_info dma_plat_info __initdata = { <nl> . dma_read = dma_read , <nl> }; <nl>  <nl> - static struct platform_device_info omap_dma_dev_info = { <nl> + static struct platform_device_info omap_dma_dev_info __initdata = { <nl> . name = " omap - dma - engine ", <nl> . id = - 1 , <nl> . dma_mask = DMA_BIT_MASK ( 32 ),
static int truncate_data_node ( const struct ubifs_info * c , const struct inode * in <nl> int err , dlen , compr_type , out_len , old_dlen ; <nl>  <nl> out_len = le32_to_cpu ( dn -> size ); <nl> - buf = kmalloc ( out_len * WORST_COMPR_FACTOR , GFP_NOFS ); <nl> + buf = kmalloc_array ( out_len , WORST_COMPR_FACTOR , GFP_NOFS ); <nl> if (! buf ) <nl> return - ENOMEM ; <nl> 
static void _tcpm_cc_change ( struct tcpm_port * port , enum typec_cc_status cc1 , <nl> break ; <nl>  <nl> case SRC_TRY : <nl> - tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> + if ( tcpm_port_is_source ( port )) <nl> + tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> break ; <nl> case SRC_TRY_DEBOUNCE : <nl> tcpm_set_state ( port , SRC_TRY , 0 );
static void destroy_eps ( struct ci_hdrc * ci ) <nl> for ( i = 0 ; i < ci -> hw_ep_max ; i ++) { <nl> struct ci_hw_ep * hwep = & ci -> ci_hw_ep [ i ]; <nl>  <nl> + if ( hwep -> pending_td ) <nl> + free_pending_td ( hwep ); <nl> dma_pool_free ( ci -> qh_pool , hwep -> qh . ptr , hwep -> qh . dma ); <nl> } <nl> }
int fscrypt_process_policy ( struct inode * inode , <nl> return - EINVAL ; <nl>  <nl> if (! inode_has_encryption_context ( inode )) { <nl> + if (! S_ISDIR ( inode -> i_mode )) <nl> + return - EINVAL ; <nl> if (! inode -> i_sb -> s_cop -> empty_dir ) <nl> return - EOPNOTSUPP ; <nl> if (! inode -> i_sb -> s_cop -> empty_dir ( inode ))
static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
static int pxa3xx_u2d_probe ( struct platform_device * pdev ) <nl> struct resource * r ; <nl> int err ; <nl>  <nl> - u2d = kzalloc ( sizeof ( struct pxa3xx_u2d_ulpi ), GFP_KERNEL ); <nl> + u2d = kzalloc ( sizeof (* u2d ), GFP_KERNEL ); <nl> if (! u2d ) { <nl> dev_err (& pdev -> dev , " failed to allocate memory \ n "); <nl> return - ENOMEM ;
static void ath9k_hw_get_def_gain_boundaries_pdadcs ( struct ath_hw * ah , <nl> vpdTableI [ i ][ sizeCurrVpdTable - 2 ]); <nl> vpdStep = ( int16_t )(( vpdStep < 1 ) ? 1 : vpdStep ); <nl>  <nl> - if ( tgtIndex > maxIndex ) { <nl> + if ( tgtIndex >= maxIndex ) { <nl> while (( ss <= tgtIndex ) && <nl> ( k < ( AR5416_NUM_PDADC_VALUES - 1 ))) { <nl> tmpVal = ( int16_t )(( vpdTableI [ i ][ sizeCurrVpdTable - 1 ] +
irqreturn_t uic_cascade ( int virq , void * data ) <nl> int subvirq ; <nl>  <nl> msr = mfdcr ( uic -> dcrbase + UIC_MSR ); <nl> + if (! msr ) /* spurious interrupt */ <nl> + return IRQ_HANDLED ; <nl> + <nl> src = 32 - ffs ( msr ); <nl>  <nl> subvirq = irq_linear_revmap ( uic -> irqhost , src );
int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl> if ( ret != - EPROBE_DEFER ) <nl> list_del (& driver -> pending ); <nl> if ( ret ) <nl> - goto err4 ; <nl> + goto err5 ; <nl> break ; <nl> } <nl> } <nl> int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl>  <nl> return 0 ; <nl>  <nl> + err5 : <nl> + device_del (& udc -> dev ); <nl> + <nl> err4 : <nl> list_del (& udc -> list ); <nl> mutex_unlock (& udc_lock );
static int labpc_ai_cmd ( struct comedi_device * dev , struct comedi_subdevice * s ) <nl> devpriv -> write_byte ( INTERVAL_LOAD_BITS , <nl> dev -> iobase + INTERVAL_LOAD_REG ); <nl>  <nl> - if ( cmd -> convert_src == TRIG_TIMER || cmd -> scan_begin_src == TRIG_TIMER ) { <nl> + if ( cmd -> convert_src == TRIG_TIMER || <nl> + cmd -> scan_begin_src == TRIG_TIMER ) { <nl> /* set up pacing */ <nl> labpc_adc_timing ( dev , cmd , mode ); <nl> /* load counter b0 in mode 3 */
static int bpa10x_probe ( struct usb_interface * intf , const struct usb_device_id * <nl> if ( ignore ) <nl> return - ENODEV ; <nl>  <nl> + if ( intf -> cur_altsetting -> desc . bInterfaceNumber > 0 ) <nl> + return - ENODEV ; <nl> + <nl> data = kmalloc ( sizeof (* data ), GFP_KERNEL ); <nl> if (! data ) { <nl> BT_ERR (" Can ' t allocate data structure ");
static int imx2_wdt_set_timeout ( struct watchdog_device * wdog , <nl> { <nl> struct imx2_wdt_device * wdev = watchdog_get_drvdata ( wdog ); <nl>  <nl> + wdog -> timeout = new_timeout ; <nl> + <nl> regmap_update_bits ( wdev -> regmap , IMX2_WDT_WCR , IMX2_WDT_WCR_WT , <nl> WDOG_SEC_TO_COUNT ( new_timeout )); <nl> return 0 ;
void spuctx_switch_state ( struct spu_context * ctx , <nl> node = spu -> node ; <nl> if ( old_state == SPU_UTIL_USER ) <nl> atomic_dec (& cbe_spu_info [ node ]. busy_spus ); <nl> - if ( new_state == SPU_UTIL_USER ); <nl> + if ( new_state == SPU_UTIL_USER ) <nl> atomic_inc (& cbe_spu_info [ node ]. busy_spus ); <nl> } <nl> }
int kvm_set_msr_common ( struct kvm_vcpu * vcpu , struct msr_data * msr_info ) <nl> /* ... but clean it before doing the actual write */ <nl> vcpu -> arch . time_offset = data & ~( PAGE_MASK | 1 ); <nl>  <nl> + /* Check that the address is 32 - byte aligned . */ <nl> + if ( vcpu -> arch . time_offset & <nl> + ( sizeof ( struct pvclock_vcpu_time_info ) - 1 )) <nl> + break ; <nl> + <nl> vcpu -> arch . time_page = <nl> gfn_to_page ( vcpu -> kvm , data >> PAGE_SHIFT ); <nl> 
static int do_mq_notify ( mqd_t mqdes , const struct sigevent * notification ) <nl>  <nl> timeo = MAX_SCHEDULE_TIMEOUT ; <nl> ret = netlink_attachskb ( sock , nc , & timeo , NULL ); <nl> - if ( ret == 1 ) <nl> + if ( ret == 1 ) { <nl> + sock = NULL ; <nl> goto retry ; <nl> + } <nl> if ( ret ) { <nl> sock = NULL ; <nl> nc = NULL ;
static long DAC960_gam_ioctl ( struct file * file , unsigned int Request , <nl> else <nl> ErrorCode = 0 ; <nl> } <nl> + break ; <nl> default : <nl> ErrorCode = - ENOTTY ; <nl> }
ecryptfs_decode_from_filename ( unsigned char * dst , size_t * dst_size , <nl> break ; <nl> case 2 : <nl> dst [ dst_byte_offset ++] |= ( src_byte ); <nl> - dst [ dst_byte_offset ] = 0 ; <nl> current_bit_offset = 0 ; <nl> break ; <nl> }
i915_gem_create ( struct drm_file * file , <nl> u32 handle ; <nl>  <nl> size = roundup ( size , PAGE_SIZE ); <nl> + if ( size == 0 ) <nl> + return - EINVAL ; <nl>  <nl> /* Allocate the new object */ <nl> obj = i915_gem_alloc_object ( dev , size );
static inline void get_page ( struct page * page ) <nl> page_ref_inc ( page ); <nl> } <nl>  <nl> + static inline __must_check bool try_get_page ( struct page * page ) <nl> +{ <nl> + page = compound_head ( page ); <nl> + if ( WARN_ON_ONCE ( page_ref_count ( page ) <= 0 )) <nl> + return false ; <nl> + page_ref_inc ( page ); <nl> + return true ; <nl> +} <nl> + <nl> static inline void put_page ( struct page * page ) <nl> { <nl> page = compound_head ( page );
enum xgbe_conn_type { <nl> XGBE_CONN_TYPE_NONE = 0 , <nl> XGBE_CONN_TYPE_SFP , <nl> XGBE_CONN_TYPE_MDIO , <nl> + XGBE_CONN_TYPE_RSVD1 , <nl> XGBE_CONN_TYPE_BACKPLANE , <nl> XGBE_CONN_TYPE_MAX , <nl> }; <nl> static int xgbe_phy_init ( struct xgbe_prv_data * pdata ) <nl> if ( xgbe_phy_conn_type_mismatch ( pdata )) { <nl> dev_err ( pdata -> dev , " phy mode / connection mismatch (%# x /%# x )\ n ", <nl> phy_data -> port_mode , phy_data -> conn_type ); <nl> + return - EINVAL ; <nl> } <nl>  <nl> /* Validate the mode requested */
static void binder_send_failed_reply ( struct binder_transaction * t , <nl> if ( target_thread -> return_error == BR_OK ) { <nl> binder_debug ( BINDER_DEBUG_FAILED_TRANSACTION , <nl> " send failed reply for transaction % d to % d :% d \ n ", <nl> - t -> debug_id , target_thread -> proc -> pid , <nl> + t -> debug_id , <nl> + target_thread -> proc -> pid , <nl> target_thread -> pid ); <nl>  <nl> binder_pop_transaction ( target_thread , t );
static int qat_hal_init_esram ( struct icp_qat_fw_loader_handle * handle ) <nl> unsigned int csr_val ; <nl> int times = 30 ; <nl>  <nl> + if ( handle -> pci_dev -> device == ADF_C3XXX_PCI_DEVICE_ID ) <nl> + return 0 ; <nl> + <nl> csr_val = ADF_CSR_RD ( csr_addr , 0 ); <nl> if (( csr_val & ESRAM_AUTO_TINIT ) && ( csr_val & ESRAM_AUTO_TINIT_DONE )) <nl> return 0 ;
int blk_mq_alloc_tag_set ( struct blk_mq_tag_set * set ) <nl> return - EINVAL ; <nl>  <nl>  <nl> - set -> tags = kmalloc_node ( set -> nr_hw_queues * sizeof ( struct blk_mq_tags ), <nl> + set -> tags = kmalloc_node ( set -> nr_hw_queues * <nl> + sizeof ( struct blk_mq_tags *), <nl> GFP_KERNEL , set -> numa_node ); <nl> if (! set -> tags ) <nl> goto out ;
static int davinci_pcm_open ( struct snd_pcm_substream * substream ) <nl> int ret = 0 ; <nl>  <nl> snd_soc_set_runtime_hwparams ( substream , & davinci_pcm_hardware ); <nl> + /* ensure that buffer size is a multiple of period size */ <nl> + ret = snd_pcm_hw_constraint_integer ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> prtd = kzalloc ( sizeof ( struct davinci_runtime_data ), GFP_KERNEL ); <nl> if ( prtd == NULL )
static int omap_pcm_open ( struct snd_pcm_substream * substream ) <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> - prtd = kzalloc ( sizeof ( prtd ), GFP_KERNEL ); <nl> + prtd = kzalloc ( sizeof (* prtd ), GFP_KERNEL ); <nl> if ( prtd == NULL ) { <nl> ret = - ENOMEM ; <nl> goto out ;
int dmar_set_interrupt ( struct intel_iommu * iommu ) <nl> return 0 ; <nl>  <nl> irq = create_irq (); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> pr_err (" IOMMU : no free vectors \ n "); <nl> return - EINVAL ; <nl> }
iwl_parse_nvm_data ( struct device * dev , const struct iwl_cfg * cfg , <nl> if (! nvm_calib ) { <nl> IWL_ERR_DEV ( dev , <nl> " Can ' t parse empty Calib NVM sections \ n "); <nl> + kfree ( data ); <nl> return NULL ; <nl> } <nl> /* in family 8000 Xtal calibration values moved to OTP */
static const struct ieee80211_channel ath10k_5ghz_channels [] = { <nl> CHAN5G ( 132 , 5660 , 0 ), <nl> CHAN5G ( 136 , 5680 , 0 ), <nl> CHAN5G ( 140 , 5700 , 0 ), <nl> + CHAN5G ( 144 , 5720 , 0 ), <nl> CHAN5G ( 149 , 5745 , 0 ), <nl> CHAN5G ( 153 , 5765 , 0 ), <nl> CHAN5G ( 157 , 5785 , 0 ),
static void mwifiex_recreate_adapter ( struct sdio_mmc_card * card ) <nl> mmc_hw_reset ( func -> card -> host ); <nl> sdio_release_host ( func ); <nl>  <nl> + /* Previous save_adapter won ' t be valid after this . We will cancel <nl> + * pending work requests . <nl> + */ <nl> + clear_bit ( MWIFIEX_IFACE_WORK_DEVICE_DUMP , & iface_work_flags ); <nl> + clear_bit ( MWIFIEX_IFACE_WORK_CARD_RESET , & iface_work_flags ); <nl> + <nl> mwifiex_sdio_probe ( func , device_id ); <nl> } <nl> 
static int sharpsl_nand_probe ( struct platform_device * pdev ) <nl> /* Register the partitions */ <nl> mtd -> name = " sharpsl - nand "; <nl>  <nl> - err = mtd_device_parse_register ( mtd , NULL , NULL , <nl> + err = mtd_device_parse_register ( mtd , data -> part_parsers , NULL , <nl> data -> partitions , data -> nr_partitions ); <nl> if ( err ) <nl> goto err_add ;
static int __devinit snd_miro_probe ( struct snd_card * card ) <nl>  <nl> error = snd_card_miro_aci_detect ( card , miro ); <nl> if ( error < 0 ) { <nl> - snd_card_free ( card ); <nl> snd_printk ( KERN_ERR " unable to detect aci chip \ n "); <nl> return - ENODEV ; <nl> }
static int tomoyo_mount_acl ( struct tomoyo_request_info * r , char * dev_name , <nl> } <nl> if ( need_dev ) { <nl> /* Get mount point or device file . */ <nl> - if ( kern_path ( dev_name , LOOKUP_FOLLOW , & path )) { <nl> + if (! dev_name || kern_path ( dev_name , LOOKUP_FOLLOW , & path )) { <nl> error = - ENOENT ; <nl> goto out ; <nl> }
static u16 ar9003_hw_get_max_edge_power ( struct ar9300_eeprom * eep , <nl> break ; <nl> } <nl> } <nl> + <nl> + if ( is2GHz && ! twiceMaxEdgePower ) <nl> + twiceMaxEdgePower = 60 ; <nl> + <nl> return twiceMaxEdgePower ; <nl> } <nl> 
int core_kernel_text ( unsigned long addr ) <nl> addr <= ( unsigned long ) _etext ) <nl> return 1 ; <nl>  <nl> - if ( addr >= ( unsigned long ) _sinittext && <nl> + if ( system_state == SYSTEM_BOOTING && <nl> + addr >= ( unsigned long ) _sinittext && <nl> addr <= ( unsigned long ) _einittext ) <nl> return 1 ; <nl> return 0 ;
int __mdiobus_register ( struct mii_bus * bus , struct module * owner ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> - put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
static void dpp1_cm_set_regamma_pwl ( <nl> struct dpp * dpp_base , const struct pwl_params * params , enum opp_regamma mode ) <nl> { <nl> struct dcn10_dpp * dpp = TO_DCN10_DPP ( dpp_base ); <nl> - uint32_t re_mode ; <nl> + uint32_t re_mode = 0 ; <nl>  <nl> switch ( mode ) { <nl> case OPP_REGAMMA_BYPASS :
static int pci_create_attr ( struct pci_dev * pdev , int num , int write_combine ) <nl> res_attr -> size = pci_resource_len ( pdev , num ); <nl> res_attr -> private = & pdev -> resource [ num ]; <nl> retval = sysfs_create_bin_file (& pdev -> dev . kobj , res_attr ); <nl> + if ( retval ) <nl> + kfree ( res_attr ); <nl> } else <nl> retval = - ENOMEM ; <nl> 
int intel_gvt_init ( struct drm_i915_private * dev_priv ) <nl> goto bail ; <nl> } <nl>  <nl> + if (! i915 . enable_execlists ) { <nl> + DRM_INFO (" GPU guest virtualisation [ GVT - g ] disabled due to disabled execlist submission [ i915 . enable_execlists module parameter ]\ n "); <nl> + goto bail ; <nl> + } <nl> + <nl> /* <nl> * We ' re not in host or fail to find a MPT module , disable GVT - g <nl> */
static int do_replace ( struct net * net , const void __user * user , <nl> if ( tmp . num_counters >= INT_MAX / sizeof ( struct ebt_counter )) <nl> return - ENOMEM ; <nl>  <nl> + tmp . name [ sizeof ( tmp . name ) - 1 ] = 0 ; <nl> + <nl> countersize = COUNTER_OFFSET ( tmp . nentries ) * nr_cpu_ids ; <nl> newinfo = vmalloc ( sizeof (* newinfo ) + countersize ); <nl> if (! newinfo )
static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
static int __devinit e100_probe ( struct pci_dev * pdev , <nl>  <nl> e100_get_defaults ( nic ); <nl>  <nl> + /* D100 MAC doesn ' t allow rx of vlan packets with normal MTU */ <nl> + if ( nic -> mac < mac_82558_D101_A4 ) <nl> + netdev -> features |= NETIF_F_VLAN_CHALLENGED ; <nl> + <nl> /* locks must be initialized before calling hw_reset */ <nl> spin_lock_init (& nic -> cb_lock ); <nl> spin_lock_init (& nic -> cmd_lock );
static int greybus_remove ( struct device * dev ) <nl> gb_connection_disable_rx ( connection ); <nl>  <nl> driver -> disconnect ( bundle ); <nl> + <nl> + /* Catch buggy drivers that fail to disable their connections . */ <nl> + list_for_each_entry ( connection , & bundle -> connections , bundle_links ) { <nl> + if ( WARN_ON ( connection -> state != GB_CONNECTION_STATE_DISABLED )) <nl> + gb_connection_disable ( connection ); <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static int __devinit mc13783_led_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - led = kzalloc ( sizeof (* led ) * pdata -> num_leds , GFP_KERNEL ); <nl> + led = kcalloc ( pdata -> num_leds , sizeof (* led ), GFP_KERNEL ); <nl> if ( led == NULL ) { <nl> dev_err (& pdev -> dev , " failed to alloc memory \ n "); <nl> return - ENOMEM ;
static int spi_imx_probe ( struct platform_device * pdev ) <nl> goto out_clk_put ; <nl> } <nl>  <nl> + if (! master -> cs_gpios ) { <nl> + dev_err (& pdev -> dev , " No CS GPIOs available \ n "); <nl> + goto out_clk_put ; <nl> + } <nl> + <nl> for ( i = 0 ; i < master -> num_chipselect ; i ++) { <nl> if (! gpio_is_valid ( master -> cs_gpios [ i ])) <nl> continue ;
int wilc_netdev_init ( void ) <nl>  <nl> /* create the common structure */ <nl> g_linux_wlan = kzalloc ( sizeof ( linux_wlan_t ), GFP_KERNEL ); <nl> + if (! g_linux_wlan ) <nl> + return - ENOMEM ; <nl>  <nl> /* Reset interrupt count debug */ <nl> int_rcvdU = 0 ;
static u16 xhci_calculate_lpm_timeout ( struct usb_hcd * hcd , <nl> if (! config ) <nl> return timeout ; <nl>  <nl> - for ( i = 0 ; i < USB_MAXINTERFACES ; i ++) { <nl> + for ( i = 0 ; i < config -> desc . bNumInterfaces ; i ++) { <nl> struct usb_driver * driver ; <nl> struct usb_interface * intf = config -> interface [ i ]; <nl> 
static int spi_gpio_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl>  <nl> status = devm_add_action_or_reset (& pdev -> dev , spi_gpio_put , master ); <nl> - if ( status ) <nl> + if ( status ) { <nl> + spi_master_put ( master ); <nl> return status ; <nl> + } <nl>  <nl> if ( of_id ) <nl> status = spi_gpio_probe_dt ( pdev , master );
int class_process_proc_param ( char * prefix , struct lprocfs_vars * lvars , <nl>  <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> - rc = var -> fops -> write (& fakefile , sval , <nl> + rc = var -> fops -> write (& fakefile , <nl> + ( const char __user *) sval , <nl> vallen , NULL ); <nl> set_fs ( oldfs ); <nl> }
static void s6000_pcm_enqueue_dma ( struct snd_pcm_substream * substream ) <nl> return ; <nl> } <nl>  <nl> - BUG_ON ( period_size & 15 ); <nl> + if ( WARN_ON ( period_size & 15 )) <nl> + return ; <nl> s6dmac_put_fifo ( DMA_MASK_DMAC ( channel ), DMA_INDEX_CHNL ( channel ), <nl> src , dst , period_size ); <nl> 
static int xhci_mtk_probe ( struct platform_device * pdev ) <nl> goto disable_ldos ; <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if ( irq < 0 ) <nl> + if ( irq < 0 ) { <nl> + ret = irq ; <nl> goto disable_clk ; <nl> + } <nl>  <nl> /* Initialize dma_mask and coherent_dma_mask to 32 - bits */ <nl> ret = dma_set_coherent_mask ( dev , DMA_BIT_MASK ( 32 ));
static int mpc5121_nfc_probe ( struct platform_device * op ) <nl> chip = & prv -> chip ; <nl>  <nl> mtd -> priv = chip ; <nl> + mtd -> dev . parent = dev ; <nl> chip -> priv = prv ; <nl> prv -> dev = dev ; <nl> 
static int __devinit sta2x11_mfd_probe ( struct pci_dev * pdev , <nl> sta2x11_mfd_setup ( pdev , setup_data ); <nl>  <nl> /* Record this pdev before mfd_add_devices : their probe looks for it */ <nl> - sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl> + if (! sta2x11_mfd_find ( pdev )) <nl> + sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl>  <nl> /* Just 2 bars for all mfd ' s at present */ <nl> for ( i = 0 ; i < 2 ; i ++) {
void ip_tunnel_xmit ( struct sk_buff * skb , struct net_device * dev , <nl> tunnel -> err_time + IPTUNNEL_ERR_TIMEO )) { <nl> tunnel -> err_count --; <nl>  <nl> + memset ( IPCB ( skb ), 0 , sizeof (* IPCB ( skb ))); <nl> dst_link_failure ( skb ); <nl> } else <nl> tunnel -> err_count = 0 ;
static void __init setup_bootmem ( void ) <nl> } <nl> memset ( pfnnid_map , 0xff , sizeof ( pfnnid_map )); <nl>  <nl> - for ( i = 0 ; i < npmem_ranges ; i ++) <nl> + for ( i = 0 ; i < npmem_ranges ; i ++) { <nl> + node_set_state ( i , N_NORMAL_MEMORY ); <nl> node_set_online ( i ); <nl> + } <nl> # endif <nl>  <nl> /*
static void channel_swdemux_tsklet ( unsigned long data ) <nl> writel ( channel -> back_buffer_busaddr , channel -> irec + <nl> DMA_PRDS_BUSRP_TP ( 0 )); <nl> else <nl> - writel ( wp , channel -> irec + DMA_PRDS_BUSWP_TP ( 0 )); <nl> + writel ( wp , channel -> irec + DMA_PRDS_BUSRP_TP ( 0 )); <nl> } <nl>  <nl> static int c8sectpfe_start_feed ( struct dvb_demux_feed * dvbdmxfeed )
static int overlay_set_addr ( struct mmp_overlay * overlay , struct mmp_addr * addr ) <nl> struct lcd_regs * regs = path_regs ( overlay -> path ); <nl>  <nl> /* FIXME : assert addr supported */ <nl> - memcpy (& overlay -> addr , addr , sizeof ( struct mmp_win )); <nl> + memcpy (& overlay -> addr , addr , sizeof ( struct mmp_addr )); <nl> writel ( addr -> phys [ 0 ], & regs -> g_0 ); <nl>  <nl> return overlay -> addr . phys [ 0 ];
int debug_log ( struct bat_priv * bat_priv , char * fmt , ...) <nl>  <nl> va_start ( args , fmt ); <nl> vscnprintf ( tmp_log_buf , sizeof ( tmp_log_buf ), fmt , args ); <nl> - fdebug_log ( bat_priv -> debug_log , "[% 10u ] % s ", <nl> + fdebug_log ( bat_priv -> debug_log , "[% 10lu ] % s ", <nl> ( jiffies / HZ ), tmp_log_buf ); <nl> va_end ( args ); <nl> 
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
int kvm_vm_ioctl_check_extension ( struct kvm * kvm , long ext ) <nl> break ; <nl> # endif <nl> case KVM_CAP_PPC_HTM : <nl> - r = cpu_has_feature ( CPU_FTR_TM_COMP ) && <nl> - is_kvmppc_hv_enabled ( kvm ); <nl> + r = cpu_has_feature ( CPU_FTR_TM_COMP ) && hv_enabled ; <nl> break ; <nl> default : <nl> r = 0 ;
static int fn_hash_insert ( struct fib_table * tb , struct fib_config * cfg ) <nl> struct fib_info * fi_drop ; <nl> u8 state ; <nl>  <nl> + if ( fi -> fib_treeref > 1 ) <nl> + goto out ; <nl> + <nl> write_lock_bh (& fib_hash_lock ); <nl> fi_drop = fa -> fa_info ; <nl> fa -> fa_info = fi ;
static void _rtl_usb_rx_process_noagg ( struct ieee80211_hw * hw , <nl> ieee80211_rx ( hw , skb ); <nl> else <nl> dev_kfree_skb_any ( skb ); <nl> + } else { <nl> + dev_kfree_skb_any ( skb ); <nl> } <nl> } <nl> 
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> - memset ( sax , 0 , sizeof ( sax )); <nl> + memset ( sax , 0 , sizeof (* sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
void __init kmem_cache_init ( void ) <nl> /* Allocate two kmem_caches from the page allocator */ <nl> kmalloc_size = ALIGN ( kmem_size , cache_line_size ()); <nl> order = get_order ( 2 * kmalloc_size ); <nl> - kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT , order ); <nl> + kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT | __GFP_ZERO , order ); <nl>  <nl> /* <nl> * Must first have the slab cache available for the allocations of the
static int iwl_mvm_free_inactive_queue ( struct iwl_mvm * mvm , int queue , <nl> spin_unlock_bh (& mvm -> queue_info_lock ); <nl>  <nl> mvmsta = iwl_mvm_sta_from_staid_protected ( mvm , sta_id ); <nl> + if ( WARN_ON (! mvmsta )) <nl> + return - EINVAL ; <nl>  <nl> disable_agg_tids = iwl_mvm_remove_sta_queue_marking ( mvm , queue ); <nl> /* Disable the queue */
static int replace_map_fd_with_map_ptr ( struct verifier_env * env ) <nl> if ( IS_ERR ( map )) { <nl> verbose (" fd % d is not pointing to valid bpf_map \ n ", <nl> insn -> imm ); <nl> - fdput ( f ); <nl> return PTR_ERR ( map ); <nl> } <nl> 
static int GLOB_SBD_init ( void ) <nl> int i ; <nl>  <nl> /* Set debug output level ( 0 ~ 3 ) here . 3 is most verbose */ <nl> - nand_debug_level = 0 ; <nl> - <nl> printk ( KERN_ALERT " Spectra : % s \ n ", GLOB_version ); <nl>  <nl> mutex_init (& spectra_lock );
static long gpio_ioctl ( struct file * filp , unsigned int cmd , unsigned long arg ) <nl> if ( cmd == GPIO_GET_CHIPINFO_IOCTL ) { <nl> struct gpiochip_info chipinfo ; <nl>  <nl> + memset (& chipinfo , 0 , sizeof ( chipinfo )); <nl> + <nl> strncpy ( chipinfo . name , dev_name (& gdev -> dev ), <nl> sizeof ( chipinfo . name )); <nl> chipinfo . name [ sizeof ( chipinfo . name )- 1 ] = '\ 0 ';
receive_buf ( struct tty_struct * tty , struct tty_buffer * head , int count ) <nl> count = disc -> ops -> receive_buf2 ( tty , p , f , count ); <nl> else { <nl> count = min_t ( int , count , tty -> receive_room ); <nl> - if ( count ) <nl> + if ( count && disc -> ops -> receive_buf ) <nl> disc -> ops -> receive_buf ( tty , p , f , count ); <nl> } <nl> return count ;
struct inet_connection_sock { <nl>  <nl> u32 probe_timestamp ; <nl> } icsk_mtup ; <nl> - u32 icsk_ca_priv [ 16 ]; <nl> u32 icsk_user_timeout ; <nl> -# define ICSK_CA_PRIV_SIZE ( 16 * sizeof ( u32 )) <nl> + <nl> + u64 icsk_ca_priv [ 64 / sizeof ( u64 )]; <nl> +# define ICSK_CA_PRIV_SIZE ( 8 * sizeof ( u64 )) <nl> }; <nl>  <nl> # define ICSK_TIME_RETRANS 1 /* Retransmit timer */
ath5k_deinit_softc ( struct ath5k_softc * sc ) <nl> * state and potentially want to use them . <nl> */ <nl> ath5k_hw_deinit ( sc -> ah ); <nl> + kfree ( sc -> ah ); <nl> free_irq ( sc -> irq , sc ); <nl> } <nl> 
static int s5c73m3_probe ( struct i2c_client * client , <nl> state -> oif_pads [ OIF_ISP_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_JPEG_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_SOURCE_PAD ]. flags = MEDIA_PAD_FL_SOURCE ; <nl> - oif_sd -> entity . function = MEDIA_ENT_F_V4L2_SUBDEV_UNKNOWN ; <nl> + oif_sd -> entity . function = MEDIA_ENT_F_PROC_VIDEO_SCALER ; <nl>  <nl> ret = media_entity_pads_init (& oif_sd -> entity , OIF_NUM_PADS , <nl> state -> oif_pads );
unsigned long perf_instruction_pointer ( struct pt_regs * regs ) <nl> bool use_siar = regs_use_siar ( regs ); <nl> unsigned long siar = mfspr ( SPRN_SIAR ); <nl>  <nl> - if ( ppmu -> flags & PPMU_P10_DD1 ) { <nl> + if ( ppmu && ( ppmu -> flags & PPMU_P10_DD1 )) { <nl> if ( siar ) <nl> return siar ; <nl> else
int vt_do_kdskled ( int console , int cmd , unsigned long arg , int perm ) <nl> kbd -> default_ledflagstate = (( arg >> 4 ) & 7 ); <nl> set_leds (); <nl> spin_unlock_irqrestore (& kbd_event_lock , flags ); <nl> - break ; <nl> + return 0 ; <nl>  <nl> /* the ioctls below only set the lights , not the functions */ <nl> /* for those , see KDGKBLED and KDSKBLED above */
int ip6_route_add ( struct fib6_config * cfg ) <nl> goto out ; <nl> lwtunnel_state_get ( lwtstate ); <nl> rt -> rt6i_lwtstate = lwtstate ; <nl> + rt -> dst . output = lwtunnel_output6 ; <nl> } <nl>  <nl> ipv6_addr_prefix (& rt -> rt6i_dst . addr , & cfg -> fc_dst , cfg -> fc_dst_len );
static int spi_qup_remove ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> ret = pm_runtime_get_sync (& pdev -> dev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> return ret ; <nl>  <nl> ret = spi_qup_set_state ( controller , QUP_STATE_RESET );
typhoon_request_firmware ( struct typhoon * tp ) <nl> err = - ENOMEM ; <nl> goto out_err ; <nl> } <nl> + memcpy ( typhoon_fw_image , typhoon_fw -> data , typhoon_fw -> size ); <nl>  <nl> return 0 ; <nl> 
build_unc_path_to_root ( const struct smb_vol * vol , <nl> pos = full_path + unc_len ; <nl>  <nl> if ( pplen ) { <nl> - * pos ++ = CIFS_DIR_SEP ( cifs_sb ); <nl> - strncpy ( pos , vol -> prepath , pplen ); <nl> + * pos = CIFS_DIR_SEP ( cifs_sb ); <nl> + strncpy ( pos + 1 , vol -> prepath , pplen ); <nl> pos += pplen ; <nl> } <nl> 
s32 host_int_del_station ( struct host_if_drv * hif_drv , const u8 * pu8MacAddr ) <nl> msg . drv = hif_drv ; <nl>  <nl> if (! pu8MacAddr ) <nl> - memset ( pstrDelStationMsg -> mac_addr , 255 , ETH_ALEN ); <nl> + eth_broadcast_addr ( pstrDelStationMsg -> mac_addr ); <nl> else <nl> memcpy ( pstrDelStationMsg -> mac_addr , pu8MacAddr , ETH_ALEN ); <nl> 
static struct stmmac_axi * stmmac_axi_setup ( struct platform_device * pdev ) <nl> if (! np ) <nl> return NULL ; <nl>  <nl> - axi = kzalloc ( sizeof ( axi ), GFP_KERNEL ); <nl> + axi = kzalloc ( sizeof (* axi ), GFP_KERNEL ); <nl> if (! axi ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
void intel_engines_mark_idle ( struct drm_i915_private * i915 ) <nl> for_each_engine ( engine , i915 , id ) { <nl> intel_engine_disarm_breadcrumbs ( engine ); <nl> i915_gem_batch_pool_fini (& engine -> batch_pool ); <nl> + tasklet_kill (& engine -> irq_tasklet ); <nl> engine -> no_priolist = false ; <nl> } <nl> }
static void perf_callchain_user_64 ( struct perf_callchain_entry * entry , <nl> sp = regs -> gpr [ 1 ]; <nl> perf_callchain_store ( entry , next_ip ); <nl>  <nl> - for (;;) { <nl> + while ( entry -> nr < PERF_MAX_STACK_DEPTH ) { <nl> fp = ( unsigned long __user *) sp ; <nl> if (! valid_user_sp ( sp , 1 ) || read_user_stack_64 ( fp , & next_sp )) <nl> return ;
static void * ixgbe_fwd_add ( struct net_device * pdev , struct net_device * vdev ) <nl> ( adapter -> num_rx_pools > IXGBE_MAX_MACVLANS )) <nl> return ERR_PTR (- EBUSY ); <nl>  <nl> - fwd_adapter = kcalloc ( 1 , sizeof ( struct ixgbe_fwd_adapter ), GFP_KERNEL ); <nl> + fwd_adapter = kzalloc ( sizeof (* fwd_adapter ), GFP_KERNEL ); <nl> if (! fwd_adapter ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
static int tegra_syncpt_wait ( struct drm_device * drm , void * data , <nl> if (! sp ) <nl> return - EINVAL ; <nl>  <nl> - return host1x_syncpt_wait ( sp , args -> thresh , args -> timeout , <nl> + return host1x_syncpt_wait ( sp , args -> thresh , <nl> + msecs_to_jiffies ( args -> timeout ), <nl> & args -> value ); <nl> } <nl> 
static int __init twl4030_configure_resource ( struct twl4030_resconfig * rconfig ) <nl> return err ; <nl> } <nl>  <nl> - if ( rconfig -> remap_off >= 0 ) { <nl> + if ( rconfig -> remap_off != TWL4030_RESCONFIG_UNDEF ) { <nl> remap &= ~ OFF_STATE_MASK ; <nl> remap |= rconfig -> remap_off << OFF_STATE_SHIFT ; <nl> } <nl>  <nl> - if ( rconfig -> remap_sleep >= 0 ) { <nl> + if ( rconfig -> remap_sleep != TWL4030_RESCONFIG_UNDEF ) { <nl> remap &= ~ SLEEP_STATE_MASK ; <nl> remap |= rconfig -> remap_off << SLEEP_STATE_SHIFT ; <nl> }
static void chase_port ( struct edgeport_port * port , unsigned long timeout , <nl> wait_queue_t wait ; <nl> unsigned long flags ; <nl>  <nl> + if (! tty ) <nl> + return ; <nl> + <nl> if (! timeout ) <nl> timeout = ( HZ * EDGE_CLOSING_WAIT )/ 100 ; <nl> 
static int ov965x_enum_frame_sizes ( struct v4l2_subdev * sd , <nl> { <nl> int i = ARRAY_SIZE ( ov965x_formats ); <nl>  <nl> - if ( fse -> index > ARRAY_SIZE ( ov965x_framesizes )) <nl> + if ( fse -> index >= ARRAY_SIZE ( ov965x_framesizes )) <nl> return - EINVAL ; <nl>  <nl> while (-- i )
static int ci_hdrc_remove ( struct platform_device * pdev ) <nl> dbg_remove_files ( ci ); <nl> free_irq ( ci -> irq , ci ); <nl> ci_role_destroy ( ci ); <nl> + kfree ( ci -> hw_bank . regmap ); <nl>  <nl> return 0 ; <nl> }
static int perf_copy_attr ( struct perf_event_attr __user * uattr , <nl> if ( ret ) <nl> return - EFAULT ; <nl>  <nl> + attr -> size = size ; <nl> + <nl> if ( attr -> __reserved_1 ) <nl> return - EINVAL ; <nl> 
i915_gem_shrink ( struct drm_i915_private * dev_priv , <nl> SINGLE_DEPTH_NESTING ); <nl> if (! obj -> mm . pages ) { <nl> __i915_gem_object_invalidate ( obj ); <nl> + list_del_init (& obj -> global_list ); <nl> count += obj -> base . size >> PAGE_SHIFT ; <nl> } <nl> mutex_unlock (& obj -> mm . lock );
static int mp_wait_modem_status ( struct sb_uart_state * state , unsigned long arg ) <nl>  <nl> static int mp_get_count ( struct sb_uart_state * state , struct serial_icounter_struct * icnt ) <nl> { <nl> - struct serial_icounter_struct icount ; <nl> + struct serial_icounter_struct icount = {}; <nl> struct sb_uart_icount cnow ; <nl> struct sb_uart_port * port = state -> port ; <nl> 
static int vhci_hub_status ( struct usb_hcd * hcd , char * buf ) <nl>  <nl> pr_info (" changed % d \ n ", changed ); <nl>  <nl> - if ( hcd -> state == HC_STATE_SUSPENDED ) <nl> + if (( hcd -> state == HC_STATE_SUSPENDED ) && ( changed == 1 )) <nl> usb_hcd_resume_root_hub ( hcd ); <nl>  <nl> done :
static int gbaudio_codec_probe ( struct gb_connection * connection ) <nl> kfree ( topology ); <nl> base_error : <nl> gbcodec -> mgmt_connection = NULL ; <nl> + gbaudio_free_codec ( dev , gbcodec ); <nl> return ret ; <nl> } <nl> 
EXPORT_SYMBOL ( vio_unregister_driver ); <nl> /* vio_dev refcount hit 0 */ <nl> static void __devinit vio_dev_release ( struct device * dev ) <nl> { <nl> - if ( dev -> archdata . of_node ) { <nl> - /* XXX should free TCE table */ <nl> - of_node_put ( dev -> archdata . of_node ); <nl> - } <nl> + /* XXX should free TCE table */ <nl> + of_node_put ( dev -> archdata . of_node ); <nl> kfree ( to_vio_dev ( dev )); <nl> } <nl> 
struct pci_dn * handle_eeh_events ( struct eeh_event * event ) <nl> } <nl>  <nl> /* All devices should claim they have recovered by now . */ <nl> - if ( result != PCI_ERS_RESULT_RECOVERED ) { <nl> + if (( result != PCI_ERS_RESULT_RECOVERED ) && <nl> + ( result != PCI_ERS_RESULT_NONE )) { <nl> printk ( KERN_WARNING " EEH : Not recovered \ n "); <nl> goto hard_fail ; <nl> }
static int pcie_find_smpss ( struct pci_dev * dev , void * data ) <nl> * will occur as normal . <nl> */ <nl> if ( dev -> is_hotplug_bridge && (! list_is_singular (& dev -> bus -> devices ) || <nl> - dev -> bus -> self -> pcie_type != PCI_EXP_TYPE_ROOT_PORT )) <nl> + ( dev -> bus -> self && <nl> + dev -> bus -> self -> pcie_type != PCI_EXP_TYPE_ROOT_PORT ))) <nl> * smpss = 0 ; <nl>  <nl> if (* smpss > dev -> pcie_mpss )
static void oz_usb_handle_ep_data ( struct oz_usb_ctx * usb_ctx , <nl> struct oz_multiple_fixed * body = <nl> ( struct oz_multiple_fixed *) data_hdr ; <nl> u8 * data = body -> data ; <nl> - int n = ( len - sizeof ( struct oz_multiple_fixed )+ 1 ) <nl> + int n ; <nl> + if (! body -> unit_size ) <nl> + break ; <nl> + n = ( len - sizeof ( struct oz_multiple_fixed )+ 1 ) <nl> / body -> unit_size ; <nl> while ( n --) { <nl> oz_hcd_data_ind ( usb_ctx -> hport , body -> endpoint ,
int smb2_tree_disconnect ( struct ksmbd_work * work ) <nl>  <nl> ksmbd_close_tree_conn_fds ( work ); <nl> ksmbd_tree_conn_disconnect ( sess , tcon ); <nl> + work -> tcon = NULL ; <nl> return 0 ; <nl> } <nl> 
static int treo_attach ( struct usb_serial * serial ) <nl> ( serial -> num_interrupt_in == 0 )) <nl> return 0 ; <nl>  <nl> + if ( serial -> num_bulk_in < 2 || serial -> num_interrupt_in < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> /* <nl> * It appears that Treos and Kyoceras want to use the <nl> * 1st bulk in endpoint to communicate with the 2nd bulk out endpoint ,
int of_get_fb_videomode ( struct device_node * np , struct fb_videomode * fb , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - fb_videomode_from_videomode (& vm , fb ); <nl> + ret = fb_videomode_from_videomode (& vm , fb ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> pr_debug ("% s : got % dx % d display mode from % s \ n ", <nl> of_node_full_name ( np ), vm . hactive , vm . vactive , np -> name );
static struct dma_async_tx_descriptor * sdma_prep_slave_sg ( <nl>  <nl> param = BD_DONE | BD_EXTD | BD_CONT ; <nl>  <nl> - if ( i + 1 == sg_len ) <nl> + if ( i + 1 == sg_len ) { <nl> param |= BD_INTR ; <nl> + param |= BD_LAST ; <nl> + param &= ~ BD_CONT ; <nl> + } <nl>  <nl> dev_dbg ( sdma -> dev , " entry % d : count : % d dma : 0x % 08x % s % s \ n ", <nl> i , count , sg -> dma_address ,
void __init tegra_super_clk_gen4_init ( void __iomem * clk_base , <nl> ARRAY_SIZE ( cclk_lp_parents ), <nl> CLK_SET_RATE_PARENT , <nl> clk_base + CCLKLP_BURST_POLICY , <nl> - 0 , 4 , 8 , 9 , NULL ); <nl> + TEGRA_DIVIDER_2 , 4 , 8 , 9 , NULL ); <nl> * dt_clk = clk ; <nl> } <nl> 
static int mxcnd_probe ( struct platform_device * pdev ) <nl> init_completion (& host -> op_completion ); <nl>  <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq < 0 ) <nl> + return host -> irq ; <nl>  <nl> /* <nl> * Use host -> devtype_data -> irq_control () here instead of irq_control ()
void iio_disable_all_buffers ( struct iio_dev * indio_dev ) <nl> indio_dev -> currentmode = INDIO_DIRECT_MODE ; <nl> if ( indio_dev -> setup_ops -> postdisable ) <nl> indio_dev -> setup_ops -> postdisable ( indio_dev ); <nl> + <nl> + if ( indio_dev -> available_scan_masks == NULL ) <nl> + kfree ( indio_dev -> active_scan_mask ); <nl> } <nl>  <nl> int iio_update_buffers ( struct iio_dev * indio_dev ,
static int virtnet_probe ( struct virtio_device * vdev ) <nl> dev -> mtu = mtu ; <nl> dev -> max_mtu = mtu ; <nl> } <nl> + <nl> + /* TODO : size buffers correctly in this case . */ <nl> + if ( dev -> mtu > ETH_DATA_LEN ) <nl> + vi -> big_packets = true ; <nl> } <nl>  <nl> if ( vi -> any_header_sg )
int intel_svm_bind_mm ( struct device * dev , int * pasid , int flags , struct svm_dev_ <nl> pasid_max - 1 , GFP_KERNEL ); <nl> if ( ret < 0 ) { <nl> kfree ( svm ); <nl> + kfree ( sdev ); <nl> goto out ; <nl> } <nl> svm -> pasid = ret ;
static int spear_smi_probe_config_dt ( struct platform_device * pdev , <nl> pdata -> board_flash_info = devm_kzalloc (& pdev -> dev , <nl> sizeof (* pdata -> board_flash_info ), <nl> GFP_KERNEL ); <nl> + if (! pdata -> board_flash_info ) <nl> + return - ENOMEM ; <nl>  <nl> /* Fill structs for each subnode ( flash device ) */ <nl> while (( pp = of_get_next_child ( np , pp ))) {
skl_tplg_init_pipe_modules ( struct skl * skl , struct skl_pipe * pipe ) <nl> if ( mconfig -> id . module_id < 0 ) { <nl> struct skl_dfw_module * dfw_config ; <nl>  <nl> - dfw_config = kzalloc ( sizeof ( dfw_config ), GFP_KERNEL ); <nl> + dfw_config = kzalloc ( sizeof (* dfw_config ), GFP_KERNEL ); <nl> if (! dfw_config ) <nl> return - ENOMEM ; <nl> 
sctp_disposition_t sctp_sf_eat_auth ( const struct sctp_endpoint * ep , <nl> struct sctp_chunk * err_chunk ; <nl> sctp_ierror_t error ; <nl>  <nl> + /* Make sure that the peer has AUTH capable */ <nl> + if (! asoc -> peer . auth_capable ) <nl> + return sctp_sf_unk_chunk ( ep , asoc , type , arg , commands ); <nl> + <nl> if (! sctp_vtag_verify ( chunk , asoc )) { <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_REPORT_BAD_TAG , <nl> SCTP_NULL ());
# include " commands . h " <nl> # include " power . h " <nl>  <nl> - static bool force_cam ; <nl> + static bool force_cam = true ; <nl> module_param ( force_cam , bool , 0644 ); <nl> MODULE_PARM_DESC ( force_cam , " force continuously aware mode ( no power saving at all )"); <nl> 
static bool msr_mtrr_valid ( unsigned msr ) <nl> case MSR_MTRRdefType : <nl> case MSR_IA32_CR_PAT : <nl> return true ; <nl> - case 0x2f8 : <nl> - return true ; <nl> } <nl> return false ; <nl> }
static void cache_set_flush ( struct closure * cl ) <nl> struct btree * b ; <nl> unsigned i ; <nl>  <nl> + if (! c ) <nl> + closure_return ( cl ); <nl> + <nl> bch_cache_accounting_destroy (& c -> accounting ); <nl>  <nl> kobject_put (& c -> internal );
static struct task_struct * select_bad_process ( unsigned long * ppoints ) <nl> unsigned long points ; <nl> int releasing ; <nl>  <nl> + /* skip kernel threads */ <nl> + if (! p -> mm ) <nl> + continue ; <nl> /* skip the init task with pid == 1 */ <nl> if ( p -> pid == 1 ) <nl> continue ;
static netdev_tx_t reg_vif_xmit ( struct sk_buff * skb , <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> dev -> stats . tx_bytes += skb -> len ;
bool kvm_irq_delivery_to_apic_fast ( struct kvm * kvm , struct kvm_lapic * src , <nl> * r = - 1 ; <nl>  <nl> if ( irq -> shorthand == APIC_DEST_SELF ) { <nl> + if ( KVM_BUG_ON (! src , kvm )) { <nl> + * r = 0 ; <nl> + return true ; <nl> + } <nl> * r = kvm_apic_set_irq ( src -> vcpu , irq , dest_map ); <nl> return true ; <nl> }
static struct kset * ipl_kset ; <nl>  <nl> static void __ipl_run ( void * unused ) <nl> { <nl> + if ( MACHINE_IS_LPAR && ipl_info . type == IPL_TYPE_CCW ) <nl> + diag308 ( DIAG308_LOAD_NORMAL_DUMP , NULL ); <nl> diag308 ( DIAG308_LOAD_CLEAR , NULL ); <nl> if ( MACHINE_IS_VM ) <nl> __cpcmd (" IPL ", NULL , 0 , NULL );
static struct afs_volume * afs_alloc_volume ( struct afs_mount_params * params , <nl> error_2 : <nl> afs_put_serverlist ( params -> net , slist ); <nl> error_1 : <nl> + afs_put_cell ( params -> net , volume -> cell ); <nl> kfree ( volume ); <nl> error_0 : <nl> return ERR_PTR ( ret );
struct dev_pm_domain omap_device_pm_domain = { <nl> USE_PLATFORM_PM_SLEEP_OPS <nl> . suspend_noirq = _od_suspend_noirq , <nl> . resume_noirq = _od_resume_noirq , <nl> + . freeze_noirq = _od_suspend_noirq , <nl> + . thaw_noirq = _od_resume_noirq , <nl> + . restore_noirq = _od_resume_noirq , <nl> } <nl> }; <nl> 
struct request_queue * __scsi_alloc_queue ( struct Scsi_Host * shost , <nl> request_fn_proc * request_fn ) <nl> { <nl> struct request_queue * q ; <nl> - struct device * dev = shost -> shost_gendev . parent ; <nl> + struct device * dev = shost -> dma_dev ; <nl>  <nl> q = blk_init_queue ( request_fn , NULL ); <nl> if (! q )
int ext4_expand_extra_isize_ea ( struct inode * inode , int new_extra_isize , <nl> goto cleanup ; <nl> kfree ( b_entry_name ); <nl> kfree ( buffer ); <nl> + b_entry_name = NULL ; <nl> + buffer = NULL ; <nl> brelse ( is -> iloc . bh ); <nl> kfree ( is ); <nl> kfree ( bs );
static int rtl8xxxu_submit_int_urb ( struct ieee80211_hw * hw ) <nl> ret = usb_submit_urb ( urb , GFP_KERNEL ); <nl> if ( ret ) { <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> goto error ; <nl> } <nl> 
int udp_ioctl ( struct sock * sk , int cmd , unsigned long arg ) <nl> { <nl> unsigned int amount = first_packet_length ( sk ); <nl>  <nl> - if ( amount ) <nl> - /* <nl> - * We will only return the amount <nl> - * of this packet since that is all <nl> - * that will be read . <nl> - */ <nl> return put_user ( amount , ( int __user *) arg ); <nl> } <nl> 
static void tcp_mtu_probing ( struct inet_connection_sock * icsk , struct sock * sk ) <nl> struct tcp_sock * tp = tcp_sk ( sk ); <nl> int mss ; <nl>  <nl> - mss = tcp_mtu_to_mss ( sk , icsk -> icsk_mtup . search_low )/ 2 ; <nl> + mss = tcp_mtu_to_mss ( sk , icsk -> icsk_mtup . search_low ) >> 1 ; <nl> mss = min ( sysctl_tcp_base_mss , mss ); <nl> mss = max ( mss , 68 - tp -> tcp_header_len ); <nl> icsk -> icsk_mtup . search_low = tcp_mss_to_mtu ( sk , mss );
static int mwifiex_pcie_init_evt_ring ( struct mwifiex_adapter * adapter ) <nl> skb_put ( skb , MAX_EVENT_SIZE ); <nl>  <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MAX_EVENT_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> + kfree ( card -> evtbd_ring_vbase ); <nl> return - 1 ; <nl> + } <nl>  <nl> buf_pa = MWIFIEX_SKB_DMA_ADDR ( skb ); <nl> 
static int tun_set_iff ( struct net * net , struct file * file , struct ifreq * ifr ) <nl>  <nl> err_detach : <nl> tun_detach_all ( dev ); <nl> + /* register_netdevice () already called tun_free_netdev () */ <nl> + goto err_free_dev ; <nl> + <nl> err_free_flow : <nl> tun_flow_uninit ( tun ); <nl> security_tun_dev_free_security ( tun -> security );
static int valid_kprobe_addr ( int template , int slot , unsigned long addr ) <nl> addr ); <nl> return - EINVAL ; <nl> } <nl> + <nl> + if ( slot == 1 && bundle_encoding [ template ][ 1 ] != L ) { <nl> + printk ( KERN_WARNING " Inserting kprobes on slot # 1 " <nl> + " is not supported \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static int __devinit ks8851_probe ( struct spi_device * spi ) <nl>  <nl>  <nl> err_netdev : <nl> - free_irq ( ndev -> irq , ndev ); <nl> + free_irq ( ndev -> irq , ks ); <nl>  <nl> err_id : <nl> err_irq :
static void f2fs_submit_discard_endio ( struct bio * bio ) <nl>  <nl> dc -> error = bio -> bi_error ; <nl> dc -> state = D_DONE ; <nl> - complete (& dc -> wait ); <nl> + complete_all (& dc -> wait ); <nl> bio_put ( bio ); <nl> } <nl> 
static int reset_one_sub_crq_queue ( struct ibmvnic_adapter * adapter , <nl> scrq -> irq = 0 ; <nl> } <nl>  <nl> - memset ( scrq -> msgs , 0 , 2 * PAGE_SIZE ); <nl> + memset ( scrq -> msgs , 0 , 4 * PAGE_SIZE ); <nl> scrq -> cur = 0 ; <nl>  <nl> rc = h_reg_sub_crq ( adapter -> vdev -> unit_address , scrq -> msg_token ,
static int wait_for_connected ( struct usb_device * udev , <nl> while ( delay_ms < 2000 ) { <nl> if ( status || * portstatus & USB_PORT_STAT_CONNECTION ) <nl> break ; <nl> + if (! port_is_power_on ( hub , * portstatus )) { <nl> + status = - ENODEV ; <nl> + break ; <nl> + } <nl> msleep ( 20 ); <nl> delay_ms += 20 ; <nl> status = hub_port_status ( hub , * port1 , portstatus , portchange );
static int fcoe_rcv ( struct sk_buff * skb , struct net_device * netdev , <nl> skb_tail_pointer ( skb ), skb_end_pointer ( skb ), <nl> skb -> csum , skb -> dev ? skb -> dev -> name : "< NULL >"); <nl>  <nl> + <nl> + skb = skb_share_check ( skb , GFP_ATOMIC ); <nl> + <nl> + if ( skb == NULL ) <nl> + return NET_RX_DROP ; <nl> + <nl> eh = eth_hdr ( skb ); <nl>  <nl> if ( is_fip_mode ( ctlr ) &&
static inline long snd_ctl_ioctl_compat ( struct file * file , unsigned int cmd , uns <nl> case SNDRV_CTL_IOCTL_POWER_STATE : <nl> case SNDRV_CTL_IOCTL_ELEM_LOCK : <nl> case SNDRV_CTL_IOCTL_ELEM_UNLOCK : <nl> + case SNDRV_CTL_IOCTL_ELEM_REMOVE : <nl> + case SNDRV_CTL_IOCTL_TLV_READ : <nl> + case SNDRV_CTL_IOCTL_TLV_WRITE : <nl> + case SNDRV_CTL_IOCTL_TLV_COMMAND : <nl> return snd_ctl_ioctl ( file , cmd , ( unsigned long ) argp ); <nl> case SNDRV_CTL_IOCTL_ELEM_LIST32 : <nl> return snd_ctl_elem_list_compat ( ctl -> card , argp );
void ath6kl_cfg80211_disconnect_event ( struct ath6kl * ar , u8 reason , <nl> NULL , 0 , <nl> WLAN_STATUS_UNSPECIFIED_FAILURE , <nl> GFP_KERNEL ); <nl> - } else { <nl> + } else if ( ar -> sme_state == SME_CONNECTED ) { <nl> cfg80211_disconnected ( ar -> net_dev , reason , <nl> NULL , 0 , GFP_KERNEL ); <nl> }
sctp_disposition_t sctp_sf_do_5_2_4_dupcook ( struct net * net , <nl> } <nl>  <nl> /* Delete the tempory new association . */ <nl> - sctp_add_cmd_sf ( commands , SCTP_CMD_NEW_ASOC , SCTP_ASOC ( new_asoc )); <nl> + sctp_add_cmd_sf ( commands , SCTP_CMD_SET_ASOC , SCTP_ASOC ( new_asoc )); <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_DELETE_TCB , SCTP_NULL ()); <nl>  <nl> /* Restore association pointer to provide SCTP command interpeter
static void collapse_huge_page ( struct mm_struct * mm , <nl> set_pmd_at ( mm , address , pmd , _pmd ); <nl> spin_unlock (& mm -> page_table_lock ); <nl> anon_vma_unlock ( vma -> anon_vma ); <nl> - mem_cgroup_uncharge_page ( new_page ); <nl> goto out ; <nl> } <nl>  <nl> static void collapse_huge_page ( struct mm_struct * mm , <nl> return ; <nl>  <nl> out : <nl> + mem_cgroup_uncharge_page ( new_page ); <nl> # ifdef CONFIG_NUMA <nl> put_page ( new_page ); <nl> # endif
# include < asm / bitops . h > <nl> # include < asm / uaccess . h > <nl>  <nl> -# include " mxser . h " <nl> +# include " mxser_new . h " <nl>  <nl> # define MXSER_VERSION " 1 . 8 " <nl> # define MXSERMAJOR 174
xfs_attr3_rmt_verify ( <nl> if ( be32_to_cpu ( rmt -> rm_bytes ) > fsbsize - sizeof (* rmt )) <nl> return false ; <nl> if ( be32_to_cpu ( rmt -> rm_offset ) + <nl> - be32_to_cpu ( rmt -> rm_bytes ) >= XATTR_SIZE_MAX ) <nl> + be32_to_cpu ( rmt -> rm_bytes ) > XATTR_SIZE_MAX ) <nl> return false ; <nl> if ( rmt -> rm_owner == 0 ) <nl> return false ;
static int get_bitmap_file ( struct mddev * mddev , void __user * arg ) <nl> char * ptr ; <nl> int err ; <nl>  <nl> - file = kmalloc ( sizeof (* file ), GFP_NOIO ); <nl> + file = kzalloc ( sizeof (* file ), GFP_NOIO ); <nl> if (! file ) <nl> return - ENOMEM ; <nl> 
static int stts751_read_chip_config ( struct stts751_priv * priv ) <nl> ret = i2c_smbus_read_byte_data ( priv -> client , STTS751_REG_RATE ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + if ( ret >= ARRAY_SIZE ( stts751_intervals )) { <nl> + dev_err ( priv -> dev , " Unrecognized conversion rate 0x % x \ n ", ret ); <nl> + return - ENODEV ; <nl> + } <nl> priv -> interval = ret ; <nl>  <nl> ret = stts751_read_reg16 ( priv , & priv -> event_max ,
int rtc_irq_set_freq ( struct rtc_device * rtc , struct rtc_task * task , int freq ) <nl> int err = 0 ; <nl> unsigned long flags ; <nl>  <nl> - if ( freq <= 0 ) <nl> + if ( freq <= 0 || freq > 5000 ) <nl> return - EINVAL ; <nl>  <nl> spin_lock_irqsave (& rtc -> irq_task_lock , flags );
static int nokia_modem_gpio_probe ( struct device * dev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - modem -> gpios = devm_kzalloc ( dev , gpio_count * <nl> - sizeof ( struct nokia_modem_gpio ), GFP_KERNEL ); <nl> + modem -> gpios = devm_kcalloc ( dev , gpio_count , sizeof (* modem -> gpios ), <nl> + GFP_KERNEL ); <nl> if (! modem -> gpios ) { <nl> dev_err ( dev , " Could not allocate memory for gpios \ n "); <nl> return - ENOMEM ;
static netdev_tx_t ipgre_tunnel_xmit ( struct sk_buff * skb , struct net_device * dev <nl> if ( skb_headroom ( skb ) < max_headroom || skb_shared ( skb )|| <nl> ( skb_cloned ( skb ) && ! skb_clone_writable ( skb , 0 ))) { <nl> struct sk_buff * new_skb = skb_realloc_headroom ( skb , max_headroom ); <nl> + if ( max_headroom > dev -> needed_headroom ) <nl> + dev -> needed_headroom = max_headroom ; <nl> if (! new_skb ) { <nl> ip_rt_put ( rt ); <nl> dev -> stats . tx_dropped ++;
MODULE_PARM_DESC ( cidmode , " Call - ID mode "); <nl> # define GIGASET_MODULENAME " usb_gigaset " <nl> # define GIGASET_DEVNAME " ttyGU " <nl>  <nl> -# define IF_WRITEBUF 2000 /* arbitrary limit */ <nl> +/* length limit according to Siemens 3070usb - protokoll . doc ch . 2 . 1 */ <nl> +# define IF_WRITEBUF 264 <nl>  <nl> /* Values for the Gigaset M105 Data */ <nl> # define USB_M105_VENDOR_ID 0x0681
void rt2x00ht_create_tx_descriptor ( struct queue_entry * entry , <nl> txdesc -> mpdu_density = 0 ; <nl>  <nl> txdesc -> ba_size = 7 ; /* FIXME : What value is needed ? */ <nl> - txdesc -> stbc = 0 ; /* FIXME : What value is needed ? */ <nl> + <nl> + txdesc -> stbc = <nl> + ( tx_info -> flags & IEEE80211_TX_CTL_STBC ) >> IEEE80211_TX_CTL_STBC_SHIFT ; <nl>  <nl> txdesc -> mcs = rt2x00_get_rate_mcs ( hwrate -> mcs ); <nl> if ( txrate -> flags & IEEE80211_TX_RC_USE_SHORT_PREAMBLE )
void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> bool return_now = false ; <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> + if (! pnfs_layout_is_valid ( lo )) { <nl> + spin_unlock (& inode -> i_lock ); <nl> + return ; <nl> + } <nl> pnfs_set_plh_return_info ( lo , range . iomode , 0 ); <nl> /* Block LAYOUTGET */ <nl> set_bit ( NFS_LAYOUT_RETURN , & lo -> plh_flags );
static void drm_cleanup ( struct drm_device * dev ) <nl> DRM_ERROR (" Cannot unload module \ n "); <nl> } <nl>  <nl> - int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> + static int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> { <nl> struct drm_minor * minor = ptr ; <nl> struct drm_device * dev ;
static int mga_vram_init ( struct mga_device * mdev ) <nl> { <nl> void __iomem * mem ; <nl> struct apertures_struct * aper = alloc_apertures ( 1 ); <nl> + if (! aper ) <nl> + return - ENOMEM ; <nl>  <nl> /* BAR 0 is VRAM */ <nl> mdev -> mc . vram_base = pci_resource_start ( mdev -> dev -> pdev , 0 );
void free_fib_info ( struct fib_info * fi ) <nl> # endif <nl> call_rcu (& fi -> rcu , free_fib_info_rcu ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( free_fib_info ); <nl>  <nl> void fib_release_info ( struct fib_info * fi ) <nl> {
static int rt5514_dsp_voice_wake_up_put ( struct snd_kcontrol * kcontrol , <nl> # else <nl> dev_err ( component -> dev , " There is no SPI driver for " <nl> " loading the firmware \ n "); <nl> + memset ( buf , 0 , sizeof ( buf )); <nl> # endif <nl> rt5514 -> pll3_cal_value = buf [ 0 ] | buf [ 1 ] << 8 | <nl> buf [ 2 ] << 16 | buf [ 3 ] << 24 ;
static int powermate_probe ( struct usb_interface * intf , const struct usb_device_i <nl> int error = - ENOMEM ; <nl>  <nl> interface = intf -> cur_altsetting ; <nl> + if ( interface -> desc . bNumEndpoints < 1 ) <nl> + return - EINVAL ; <nl> + <nl> endpoint = & interface -> endpoint [ 0 ]. desc ; <nl> if (! usb_endpoint_is_int_in ( endpoint )) <nl> return - EIO ;
static void mmu_pte_write_zap_pte ( struct kvm_vcpu * vcpu , <nl> mmu_page_remove_parent_pte ( child , spte ); <nl> } <nl> } <nl> - * spte = 0 ; <nl> + set_shadow_pte ( spte , 0 ); <nl> kvm_flush_remote_tlbs ( vcpu -> kvm ); <nl> } <nl> 
static int clie_5_attach ( struct usb_serial * serial ) <nl> */ <nl>  <nl> /* some sanity check */ <nl> - if ( serial -> num_ports < 2 ) <nl> - return - 1 ; <nl> + if ( serial -> num_bulk_out < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing bulk out endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> /* port 0 now uses the modified endpoint Address */ <nl> port = serial -> port [ 0 ];
static int process_ctrl_td ( struct xhci_hcd * xhci , struct xhci_td * td , <nl> case TRB_NORMAL : <nl> td -> urb -> actual_length = requested - remaining ; <nl> goto finish_td ; <nl> + case TRB_STATUS : <nl> + td -> urb -> actual_length = requested ; <nl> + goto finish_td ; <nl> default : <nl> xhci_warn ( xhci , " WARN : unexpected TRB Type % d \ n ", <nl> trb_type );
int mwifiex_bss_start ( struct mwifiex_private * priv , struct cfg80211_bss * bss , <nl> goto done ; <nl> } <nl>  <nl> - if ( priv -> bss_mode == NL80211_IFTYPE_STATION ) { <nl> + if ( priv -> bss_mode == NL80211_IFTYPE_STATION || <nl> + priv -> bss_mode == NL80211_IFTYPE_P2P_CLIENT ) { <nl> u8 config_bands ; <nl>  <nl> - /* Infra mode */ <nl> ret = mwifiex_deauthenticate ( priv , NULL ); <nl> if ( ret ) <nl> goto done ;
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . fuse = gm107_fuse_new , <nl> . gpio = gk104_gpio_new , <nl> + . i2c = gm200_i2c_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> . mmu = gf100_mmu_new ,
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> goto cleanup ; <nl> } <nl>  <nl> - if ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr ))) { <nl> + if (( fibsize < ( sizeof ( struct user_aac_srb ) - sizeof ( struct user_sgentry ))) || <nl> + ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr )))) { <nl> rcode = - EINVAL ; <nl> goto cleanup ; <nl> }
static int asoc_simple_card_dai_link_of ( struct device_node * node , <nl> strlen ( dai_link -> cpu_dai_name ) + <nl> strlen ( dai_link -> codec_dai_name ) + 2 , <nl> GFP_KERNEL ); <nl> + if (! name ) { <nl> + ret = - ENOMEM ; <nl> + goto dai_link_of_err ; <nl> + } <nl> + <nl> sprintf ( name , "% s -% s ", dai_link -> cpu_dai_name , <nl> dai_link -> codec_dai_name ); <nl> dai_link -> name = dai_link -> stream_name = name ;
static int asoc_simple_card_probe ( struct platform_device * pdev ) <nl> snd_soc_card_set_drvdata ( card , priv ); <nl>  <nl> ret = devm_snd_soc_register_card ( dev , card ); <nl> - if ( ret >= 0 ) <nl> - return ret ; <nl> + if ( ret < 0 ) <nl> + goto err ; <nl> + <nl> + return 0 ; <nl> err : <nl> asoc_simple_card_clean_reference ( card ); <nl> 
static int tc6393xb_resume ( struct platform_device * dev ) <nl> int ret ; <nl> int i ; <nl>  <nl> - clk_prepare_enable ( tc6393xb -> clk ); <nl> + ret = clk_prepare_enable ( tc6393xb -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = tcpd -> resume ( dev ); <nl> if ( ret )
static __init int hardware_setup ( void ) <nl> goto out ; <nl> } <nl>  <nl> - vmx_io_bitmap_b = ( unsigned long *) __get_free_page ( GFP_KERNEL ); <nl> memset ( vmx_vmread_bitmap , 0xff , PAGE_SIZE ); <nl> memset ( vmx_vmwrite_bitmap , 0xff , PAGE_SIZE ); <nl> 
static int alloc_reserved_tree_block ( struct btrfs_trans_handle * trans , <nl> ret = btrfs_insert_empty_item ( trans , fs_info -> extent_root , path , <nl> ins , size ); <nl> if ( ret ) { <nl> + btrfs_free_path ( path ); <nl> btrfs_free_and_pin_reserved_extent ( root , ins -> objectid , <nl> root -> nodesize ); <nl> - btrfs_free_path ( path ); <nl> return ret ; <nl> } <nl> 
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
int wpa_ioctl ( PSDevice pDevice , struct iw_point * p ) <nl> default : <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO " wpa_ioctl : unknown cmd =% d \ n ", <nl> param -> cmd ); <nl> + kfree ( param ); <nl> return - EOPNOTSUPP ; <nl> } <nl> 
static int mga_vram_init ( struct mga_device * mdev ) <nl> aper -> count = 1 ; <nl>  <nl> remove_conflicting_framebuffers ( aper , " mgafb ", true ); <nl> + kfree ( aper ); <nl>  <nl> if (! request_mem_region ( mdev -> mc . vram_base , mdev -> mc . vram_window , <nl> " mgadrmfb_vram ")) {
void ping_unhash ( struct sock * sk ) <nl> if ( sk_hashed ( sk )) { <nl> write_lock_bh (& ping_table . lock ); <nl> hlist_nulls_del (& sk -> sk_nulls_node ); <nl> + sk_nulls_node_init (& sk -> sk_nulls_node ); <nl> sock_put ( sk ); <nl> isk -> inet_num = 0 ; <nl> isk -> inet_sport = 0 ;
static enum print_line_t trace_ctxwake_bin ( struct trace_iterator * iter , <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_pid ); <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_prio ); <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_state ); <nl> + SEQ_PUT_FIELD_RET ( s , field -> next_cpu ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_pid ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_prio ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_state );
static inline u64 btrfs_ino ( struct inode * inode ) <nl> { <nl> u64 ino = BTRFS_I ( inode )-> location . objectid ; <nl>  <nl> - if ( ino <= BTRFS_FIRST_FREE_OBJECTID ) <nl> + /* <nl> + * ! ino : btree_inode <nl> + * type == BTRFS_ROOT_ITEM_KEY : subvol dir <nl> + */ <nl> + if (! ino || BTRFS_I ( inode )-> location . type == BTRFS_ROOT_ITEM_KEY ) <nl> ino = inode -> i_ino ; <nl> return ino ; <nl> }
static int ath10k_abort_scan ( struct ath10k * ar ) <nl> ret = ath10k_wmi_stop_scan ( ar , & arg ); <nl> if ( ret ) { <nl> ath10k_warn (" could not submit wmi stop scan (% d )\ n ", ret ); <nl> + spin_lock_bh (& ar -> data_lock ); <nl> + ar -> scan . in_progress = false ; <nl> + ath10k_offchan_tx_purge ( ar ); <nl> + spin_unlock_bh (& ar -> data_lock ); <nl> return - EIO ; <nl> } <nl> 
static sctp_xmit_t sctp_packet_bundle_auth ( struct sctp_packet * pkt , <nl> /* See if this is an auth chunk we are bundling or if <nl> * auth is already bundled . <nl> */ <nl> - if ( chunk -> chunk_hdr -> type == SCTP_CID_AUTH || pkt -> auth ) <nl> + if ( chunk -> chunk_hdr -> type == SCTP_CID_AUTH || pkt -> has_auth ) <nl> return retval ; <nl>  <nl> /* if the peer did not request this chunk to be authenticated ,
xfs_inumbers ( <nl> return error ; <nl>  <nl> bcount = MIN ( left , ( int )( PAGE_SIZE / sizeof (* buffer ))); <nl> - buffer = kmem_alloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> + buffer = kmem_zalloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> do { <nl> struct xfs_inobt_rec_incore r ; <nl> int stat ;
# define crisv10_mask_irq ( irq_nr ) (* R_VECT_MASK_CLR = 1 << ( irq_nr )); <nl> # define crisv10_unmask_irq ( irq_nr ) (* R_VECT_MASK_SET = 1 << ( irq_nr )); <nl>  <nl> + extern void kgdb_init ( void ); <nl> + extern void breakpoint ( void ); <nl> + <nl> /* don ' t use set_int_vector , it bypasses the linux interrupt handlers . it is <nl> * global just so that the kernel gdb can use it . <nl> */
static bool do_propagate_liveness ( const struct bpf_verifier_state * state , <nl> if ( parent -> spilled_regs [ i ]. live & REG_LIVE_READ ) <nl> continue ; <nl> if ( state -> spilled_regs [ i ]. live == REG_LIVE_READ ) { <nl> - parent -> regs [ i ]. live |= REG_LIVE_READ ; <nl> + parent -> spilled_regs [ i ]. live |= REG_LIVE_READ ; <nl> touched = true ; <nl> } <nl> }
static const struct fm10k_stats fm10k_gstrings_pf_stats [] = { <nl>  <nl> static const struct fm10k_stats fm10k_gstrings_mbx_stats [] = { <nl> FM10K_MBX_STAT (" mbx_tx_busy ", tx_busy ), <nl> - FM10K_MBX_STAT (" mbx_tx_oversized ", tx_dropped ), <nl> + FM10K_MBX_STAT (" mbx_tx_dropped ", tx_dropped ), <nl> FM10K_MBX_STAT (" mbx_tx_messages ", tx_messages ), <nl> FM10K_MBX_STAT (" mbx_tx_dwords ", tx_dwords ), <nl> FM10K_MBX_STAT (" mbx_tx_mbmem_pulled ", tx_mbmem_pulled ),
int copy_creds ( struct task_struct * p , unsigned long clone_flags ) <nl> struct cred * new ; <nl> int ret ; <nl>  <nl> + p -> replacement_session_keyring = NULL ; <nl> + <nl> if ( <nl> # ifdef CONFIG_KEYS <nl> ! p -> cred -> thread_keyring &&
static SQInteger thread_call ( HSQUIRRELVM v ) <nl> SQObjectPtr o = stack_get ( v , 1 ); <nl> if ( sq_type ( o ) == OT_THREAD ) { <nl> SQInteger nparams = sq_gettop ( v ); <nl> + sq_reservestack ( _thread ( o ), nparams + 3 ); <nl> _thread ( o )-> Push ( _thread ( o )-> _roottable ); <nl> for ( SQInteger i = 2 ; i <( nparams + 1 ); i ++) <nl> sq_move ( _thread ( o ), v , i );
crun_command_exec ( struct crun_global_arguments * global_args , int argc , char ** a <nl> capabilities -> effective = exec_options . cap ; <nl> capabilities -> effective_len = exec_options . cap_size ; <nl>  <nl> - capabilities -> inheritable = dup_array ( exec_options . cap , exec_options . cap_size ); <nl> - capabilities -> inheritable_len = exec_options . cap_size ; <nl> + capabilities -> inheritable = NULL ; <nl> + capabilities -> inheritable_len = 0 ; <nl>  <nl> capabilities -> bounding = dup_array ( exec_options . cap , exec_options . cap_size ); <nl> capabilities -> bounding_len = exec_options . cap_size ;
class McAsciiParserBase { <nl> const char * posStart , <nl> const char * posEnd ); <nl>  <nl> + // limit the value size . <nl> + static constexpr uint32_t maxValueBytes = 1 * 1024 * 1024 * 1024 ; // 1GB <nl> + <nl> std :: string currentErrorDescription_ ; <nl>  <nl> uint64_t currentUInt_ { 0 };
hermesBuiltinApply ( void *, Runtime * runtime , NativeArgs args ) { <nl>  <nl> ScopedNativeCallFrame newFrame { <nl> runtime , len , * fn , isConstructor , thisVal . getHermesValue ()}; <nl> + if ( LLVM_UNLIKELY ( newFrame . overflowed ())) <nl> + return runtime -> raiseStackOverflow ( Runtime :: StackOverflowKind :: NativeStack ); <nl> + <nl> for ( uint32_t i = 0 ; i < len ; ++ i ) { <nl> newFrame -> getArgRef ( i ) = argArray -> at ( runtime , i ); <nl> }
static int prctl_set_vma_anon_name ( unsigned long start , unsigned long end , <nl> tmp = end ; <nl>  <nl> /* Here vma -> vm_start <= start < tmp <= ( end | vma -> vm_end ). */ <nl> - error = prctl_update_vma_anon_name ( vma , & prev , start , end , <nl> + error = prctl_update_vma_anon_name ( vma , & prev , start , tmp , <nl> ( const char __user *) arg ); <nl> if ( error ) <nl> return error ;
int pam_sm_acct_mgmt ( pam_handle_t * pamh , int flags , <nl> int tac_fd ; <nl>  <nl> user = tty = r_addr = NULL ; <nl> + memset (& arep , 0 , sizeof ( arep )); <nl>  <nl> /* this also obtains service name for authorization <nl> this should be normally performed by pam_get_item ( PAM_SERVICE )
static bool check_allocations ( ASS_Shaper * shaper , size_t new_size ) <nl> ! ASS_REALLOC_ARRAY ( shaper -> emblevels , new_size ) || <nl> ! ASS_REALLOC_ARRAY ( shaper -> cmap , new_size )) <nl> return false ; <nl> + shaper -> n_glyphs = new_size ; <nl> } <nl> return true ; <nl> }
static void singlevar ( LexState * ls , expdesc * var ) { <nl> expdesc key ; <nl> singlevaraux ( fs , ls -> envn , var , 1 ); /* get environment variable */ <nl> lua_assert ( var -> k != VVOID ); /* this one must exist */ <nl> + luaK_exp2anyregup ( fs , var ); /* but could be a constant */ <nl> codestring (& key , varname ); /* key is variable name */ <nl> luaK_indexed ( fs , var , & key ); /* env [ varname ] */ <nl> }
static const unsigned char * parse_object ( cJSON * item , const unsigned char * value <nl> fail : <nl> if ( item -> child != NULL ) <nl> { <nl> - cJSON_Delete ( child ); <nl> + cJSON_Delete ( item -> child ); <nl> item -> child = NULL ; <nl> } <nl> 
native_handle * Parcel :: readNativeHandle () const <nl> if ( err != NO_ERROR ) return 0 ; <nl>  <nl> native_handle * h = native_handle_create ( numFds , numInts ); <nl> + if (! h ) { <nl> + return 0 ; <nl> + } <nl> + <nl> for ( int i = 0 ; err == NO_ERROR && i < numFds ; i ++) { <nl> h -> data [ i ] = dup ( readFileDescriptor ()); <nl> if ( h -> data [ i ] < 0 ) err = BAD_VALUE ;
status_t OMXCodec :: allocateBuffersOnPort ( OMX_U32 portIndex ) { <nl>  <nl> for ( OMX_U32 i = 0 ; i < def . nBufferCountActual ; ++ i ) { <nl> sp < IMemory > mem = mDealer [ portIndex ]-> allocate ( def . nBufferSize ); <nl> - CHECK ( mem . get () != NULL ); <nl> + if ( mem == NULL || mem -> pointer () == NULL ) { <nl> + return NO_MEMORY ; <nl> + } <nl>  <nl> BufferInfo info ; <nl> info . mData = NULL ;
class MapStageOp : public OpKernel { <nl> OP_REQUIRES_OK ( ctx , ctx -> input (" key ", & key_tensor )); <nl> OP_REQUIRES_OK ( ctx , ctx -> input (" indices ", & indices_tensor )); <nl> OP_REQUIRES_OK ( ctx , ctx -> input_list (" values ", & values_tensor )); <nl> + OP_REQUIRES ( ctx , key_tensor -> NumElements () > 0 , <nl> + errors :: InvalidArgument (" key must not be empty ")); <nl>  <nl> // Create copy for insertion into Staging Area <nl> Tensor key (* key_tensor );
TfLiteStatus Prepare ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_TYPES_EQ ( context , input -> type , output -> type ); <nl>  <nl> const int block_size = params -> block_size ; <nl> + TF_LITE_ENSURE ( context , block_size > 0 ); <nl> const int input_height = input -> dims -> data [ 1 ]; <nl> const int input_width = input -> dims -> data [ 2 ]; <nl> int output_height = input_height / block_size ;
TfLiteStatus Eval ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_OK ( context , GetInputSafe ( context , node , 2 , & value )); <nl>  <nl> const int num_rows = SizeOfDimension ( value , 0 ); <nl> + TF_LITE_ENSURE ( context , num_rows != 0 ); <nl> const int row_bytes = value -> bytes / num_rows ; <nl> void * pointer = nullptr ; <nl> DynamicBuffer buf ;
int ksmbd_decode_ntlmssp_auth_blob ( struct authenticate_message * authblob , <nl> dn_off = le32_to_cpu ( authblob -> DomainName . BufferOffset ); <nl> dn_len = le16_to_cpu ( authblob -> DomainName . Length ); <nl>  <nl> - if ( blob_len < ( u64 ) dn_off + dn_len || blob_len < ( u64 ) nt_off + nt_len ) <nl> + if ( blob_len < ( u64 ) dn_off + dn_len || blob_len < ( u64 ) nt_off + nt_len || <nl> + nt_len < CIFS_ENCPWD_SIZE ) <nl> return - EINVAL ; <nl>  <nl> # ifdef CONFIG_SMB_INSECURE_SERVER
BZIP3_API s32 bz3_decode_block ( struct bz3_state * state , u8 * buffer , s32 data_s <nl> } <nl>  <nl> if ( bwt_idx == - 1 ) { <nl> - if ( data_size - 8 > 64 ) { <nl> + if ( data_size - 8 > 64 || data_size < 8 ) { <nl> state -> last_error = BZ3_ERR_MALFORMED_HEADER ; <nl> return - 1 ; <nl> }
std :: string ProcessRawBytesWithSeparators ( const unsigned char * data , <nl> // except for the last byte . <nl> std :: string ret ; <nl> size_t kMin = 0U ; <nl> + <nl> + if (! data_length ) <nl> + return ""; <nl> + <nl> ret . reserve ( std :: max ( kMin , data_length * 3 - 1 )); <nl>  <nl> for ( size_t i = 0 ; i < data_length ; ++ i ) {
class ActionBoxTest : public InProcessBrowserTest , <nl> case content :: NOTIFICATION_WEB_CONTENTS_DESTROYED : <nl> case chrome :: NOTIFICATION_TAB_PARENTED : <nl> case chrome :: NOTIFICATION_AUTOCOMPLETE_CONTROLLER_RESULT_READY : <nl> - case chrome :: NOTIFICATION_BOOKMARK_MODEL_LOADED : <nl> case chrome :: NOTIFICATION_HISTORY_LOADED : <nl> case chrome :: NOTIFICATION_HISTORY_URLS_MODIFIED : <nl> case chrome :: NOTIFICATION_TEMPLATE_URL_SERVICE_LOADED :
void TabSpecificContentSettings :: OnContentBlocked ( <nl> const std :: string & resource_identifier ) { <nl> DCHECK ( type != CONTENT_SETTINGS_TYPE_GEOLOCATION ) <nl> << " Geolocation settings handled by OnGeolocationPermissionSet "; <nl> + if ( type < 0 || type >= CONTENT_SETTINGS_NUM_TYPES ) <nl> + return ; <nl> content_accessed_ [ type ] = true ; <nl> // Unless UI for resource content settings is enabled , ignore the resource <nl> // identifier .
int PDFiumEngine :: GetMostVisiblePage () { <nl> if ( in_flight_visible_page_ ) <nl> return * in_flight_visible_page_ ; <nl>  <nl> + // We can call GetMostVisiblePage through a callback from PDFium . We have <nl> + // to defer the page deletion otherwise we could potentially delete the page <nl> + // that originated the calling JS request and destroy the objects that are <nl> + // currently being used . <nl> + defer_page_unload_ = true ; <nl> CalculateVisiblePages (); <nl> + defer_page_unload_ = false ; <nl> return most_visible_page_ ; <nl> } <nl> 
void PaintLayerScrollableArea :: UpdateCompositingLayersAfterScroll () { <nl> DCHECK ( Layer ()-> HasCompositedLayerMapping ()); <nl> ScrollingCoordinator * scrolling_coordinator = GetScrollingCoordinator (); <nl> bool handled_scroll = <nl> - Layer ()-> IsRootLayer () && scrolling_coordinator && <nl> + ( Layer ()-> IsRootLayer () || <nl> + RuntimeEnabledFeatures :: BlinkGenPropertyTreesEnabled ()) && <nl> + scrolling_coordinator && <nl> scrolling_coordinator -> UpdateCompositedScrollOffset ( this ); <nl>  <nl> if (! handled_scroll ) {

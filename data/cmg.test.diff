do_ldmtool_diskgroup_volumes ( const char * diskgroup ) <nl> reply_with_error ("% s ", err ); <nl> return NULL ; <nl> } <nl> - free ( err ); <nl>  <nl> return parse_json_get_object_string_list ( out , " volumes ", <nl> __func__ , " ldmtool show diskgroup ");
static int rrd_shutdown ( void ) <nl> rrd_cache_flush (- 1 ); <nl> pthread_mutex_unlock (& cache_lock ); <nl>  <nl> + /* Wait for all the values to be written to disk before returning . */ <nl> + if ( queue_thread != 0 ) <nl> + { <nl> + pthread_join ( queue_thread , NULL ); <nl> + queue_thread = 0 ; <nl> + DEBUG (" rrdtool plugin : queue_thread exited ."); <nl> + } <nl> + <nl> pthread_mutex_lock (& queue_lock ); <nl> do_shutdown = 1 ; <nl> pthread_cond_signal (& queue_cond );
int uc_get_names ( char *** ret_names , cdtime_t ** ret_times , size_t * ret_number ) <nl> if ( status != 0 ) <nl> { <nl> size_t i ; <nl> - <nl> + <nl> for ( i = 0 ; i < number ; i ++) <nl> { <nl> sfree ( names [ i ]); <nl> } <nl> sfree ( names ); <nl> + sfree ( times ); <nl>  <nl> return (- 1 ); <nl> }
krb5_kdc_req * val ; <nl> krb5_free_principal ( val -> client ); <nl> if ( val -> server ) <nl> krb5_free_principal ( val -> server ); <nl> + if ( val -> etype ) <nl> + xfree ( val -> etype ); <nl> if ( val -> addresses ) <nl> krb5_free_address ( val -> addresses ); <nl> if ( val -> authorization_data . ciphertext . data )
Handle < Value > ReadFloatGeneric ( const Arguments & args ) { <nl> return ThrowTypeError (" offset is not uint "); <nl> size_t len = static_cast < size_t >( <nl> args . This ()-> GetIndexedPropertiesExternalArrayDataLength ()); <nl> - if ( offset + sizeof ( T ) > len ) <nl> + if ( offset + sizeof ( T ) > len || offset + sizeof ( T ) < offset ) <nl> return ThrowRangeError (" Trying to read beyond buffer length "); <nl> } <nl> 
void ECDH :: SetPrivateKey ( const FunctionCallbackInfo < Value >& args ) { <nl> if ( priv == nullptr ) <nl> return env -> ThrowError (" Failed to convert Buffer to BN "); <nl>  <nl> - if (! EC_KEY_set_private_key ( ecdh -> key_ , priv )) <nl> + int result = EC_KEY_set_private_key ( ecdh -> key_ , priv ); <nl> + BN_free ( priv ); <nl> + <nl> + if (! result ) { <nl> return env -> ThrowError (" Failed to convert BN to a private key "); <nl> + } <nl> } <nl>  <nl> 
int ASN1_STRING_to_UTF8 ( unsigned char ** out , ASN1_STRING * in ) <nl> mbflag = tag2nbyte [ type ]; <nl> if ( mbflag == - 1 ) return - 1 ; <nl> mbflag |= MBSTRING_FLAG ; <nl> + memset (& stmp , 0 , sizeof ( stmp )); <nl> stmp . data = NULL ; <nl> ret = ASN1_mbstring_copy (& str , in -> data , in -> length , mbflag , B_ASN1_UTF8STRING ); <nl> if ( ret < 0 ) return ret ;
do_cib_control ( long long action , <nl> clear_bit_inplace ( fsa_input_register , R_CIB_CONNECTED ); <nl> if ( fsa_cib_conn != NULL <nl> && fsa_cib_conn -> state != cib_disconnected ) { <nl> + fsa_cib_conn -> cmds -> set_slave ( <nl> + fsa_cib_conn , cib_scope_local ); <nl> fsa_cib_conn -> cmds -> signoff ( fsa_cib_conn ); <nl> } <nl> }
void master_rsc_colocation_rh ( <nl> clone_variant_data_t * clone_data = NULL ; <nl> get_clone_variant_data ( clone_data , rsc_rh ); <nl>  <nl> + CRM_CHECK ( rsc_rh != NULL , return ); <nl> if ( rsc_rh -> provisional ) { <nl> return ; <nl> 
do_election_count_vote ( long long action , <nl> crm_devel (" Election fail : born_on "); <nl> we_loose = TRUE ; <nl>  <nl> - } else if ( your_node -> node_born_on < our_node -> node_born_on ) { <nl> + } else if ( your_node -> node_born_on > our_node -> node_born_on ) { <nl> crm_devel (" Election pass : born_on "); <nl>  <nl> } else if ( strcmp ( fsa_our_uname , vote_from ) > 0 ) {
static xmlNode * inject_node_state ( cib_t * cib_conn , char * node ) <nl> rc = cib_conn -> cmds -> query ( cib_conn , xpath , & cib_object , cib_xpath | cib_sync_call | cib_scope_local ); <nl> } <nl>  <nl> + crm_free ( xpath ); <nl> CRM_ASSERT ( rc == cib_ok ); <nl> return cib_object ; <nl> }
crm_graph_functions_t te_graph_fns = { <nl> te_fence_node <nl> }; <nl>  <nl> + extern GMainLoop * mainloop ; <nl> + <nl> void <nl> notify_crmd ( crm_graph_t * graph ) <nl> { <nl> notify_crmd ( crm_graph_t * graph ) <nl>  <nl> case tg_shutdown : <nl> crm_info (" Exiting after transition "); <nl> + if ( mainloop != NULL && g_main_is_running ( mainloop )) { <nl> + g_main_quit ( mainloop ); <nl> + return ; <nl> + } <nl> exit ( LSB_EXIT_OK ); <nl> } <nl> 
git_oid_shorten * git_oid_shorten_new ( size_t min_length ) <nl>  <nl> void git_oid_shorten_free ( git_oid_shorten * os ) <nl> { <nl> + if ( os == NULL ) <nl> + return ; <nl> + <nl> git__free ( os -> nodes ); <nl> git__free ( os ); <nl> }
int git_futils_mkdir ( <nl> min_root_len = git_path_root ( make_path . ptr ); <nl> if ( root < min_root_len ) <nl> root = min_root_len ; <nl> - while ( make_path . ptr [ root ] == '/') <nl> + while ( root >= 0 && make_path . ptr [ root ] == '/') <nl> ++ root ; <nl>  <nl> /* clip root to make_path length */
static void credential_write_item ( FILE * fp , const char * key , const char * value ) <nl> { <nl> if (! value ) <nl> return ; <nl> + if ( strchr ( value , '\ n ')) <nl> + die (" credential value for % s contains newline ", key ); <nl> fprintf ( fp , "% s =% s \ n ", key , value ); <nl> } <nl> 
void mark_parents_uninteresting ( struct commit * commit ) <nl>  <nl> void add_pending_object ( struct rev_info * revs , struct object * obj , const char * name ) <nl> { <nl> + if ( revs -> no_walk && ( obj -> flags & UNINTERESTING )) <nl> + die (" object ranges do not make sense when not walking revisions "); <nl> add_object_array ( obj , name , & revs -> pending ); <nl> if ( revs -> reflog_info && obj -> type == OBJ_COMMIT ) <nl> add_reflog_for_walk ( revs -> reflog_info ,
static void wt_shortstatus_print_tracking ( struct wt_status * s ) <nl> base = shorten_unambiguous_ref ( base , 0 ); <nl> color_fprintf ( s -> fp , header_color , "..."); <nl> color_fprintf ( s -> fp , branch_color_remote , "% s ", base ); <nl> + free (( char *) base ); <nl>  <nl> if (! upstream_is_gone && ! num_ours && ! num_theirs ) { <nl> fputc ( s -> null_termination ? '\ 0 ' : '\ n ', s -> fp );
traverse_dnode ( traverse_data_t * td , const dnode_phys_t * dnp , <nl> break ; <nl> } <nl>  <nl> - if ( dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) { <nl> + if ( err == 0 && dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) { <nl> SET_BOOKMARK (& czb , objset , object , 0 , DMU_SPILL_BLKID ); <nl> err = traverse_visitbp ( td , dnp , & dnp -> dn_spill , & czb ); <nl> }
static const struct command imap4rev1_commands [] = { <nl> { " APPEND ", cmd_append , COMMAND_FLAG_BREAKS_SEQS }, <nl> { " EXAMINE ", cmd_examine , COMMAND_FLAG_BREAKS_MAILBOX }, <nl> { " CREATE ", cmd_create , 0 }, <nl> - { " DELETE ", cmd_delete , COMMAND_FLAG_USE_NONEXISTENT }, <nl> + { " DELETE ", cmd_delete , COMMAND_FLAG_BREAKS_MAILBOX | <nl> + COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " RENAME ", cmd_rename , COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " LIST ", cmd_list , 0 }, <nl> { " LSUB ", cmd_lsub , 0 },
static bool server_connection_input_one ( struct server_connection * conn ) <nl> return FALSE ; <nl>  <nl> /* check logs */ <nl> - ( void ) server_connection_print_log ( conn ); <nl> + if ( conn -> log_input != NULL ) <nl> + ( void ) server_connection_print_log ( conn ); <nl>  <nl> switch ( conn -> state ) { <nl> case SERVER_REPLY_STATE_DONE :
int mail_modifylog_mark_synced ( MailModifyLog * log ) <nl> { <nl> i_assert ( log -> index -> lock_type != MAIL_LOCK_UNLOCK ); <nl>  <nl> + if (! mmap_update ( log )) <nl> + return FALSE ; <nl> + <nl> if ( log -> header -> sync_id == SYNC_ID_FULL ) { <nl> /* log file is full , switch to next one */ <nl> return mail_modifylog_switch_file ( log );
DOVEADM_CMD_PARAMS_END <nl>  <nl> struct doveadm_cmd_ver2 doveadm_cmd_mailbox_delete_ver2 = { <nl> . name = " mailbox delete ", <nl> - . mail_cmd = cmd_mailbox_delete_alloc , <nl> - . usage = DOVEADM_CMD_MAIL_USAGE_PREFIX "[- s ] < mailbox > [...]", <nl> + . mail_cmd = cmd_mailbox_delete_alloc , <nl> + . usage = DOVEADM_CMD_MAIL_USAGE_PREFIX "[- s ] < mailbox > [...]", <nl> DOVEADM_CMD_PARAMS_START <nl> DOVEADM_CMD_MAIL_COMMON <nl> DOVEADM_CMD_PARAM (' s ', " subscriptions ", CMD_PARAM_BOOL , 0 )
const char * t_abspath ( const char * path ) <nl> { <nl> const char * dir ; <nl> + i_assert ( path != NULL ); <nl>  <nl> if (* path == '/') <nl> return path ; <nl> const char * t_abspath ( const char * path ) <nl>  <nl> const char * t_abspath_to ( const char * path , const char * root ) <nl> { <nl> + i_assert ( path != NULL ); <nl> + i_assert ( root != NULL ); <nl> + <nl> if (* path == '/') <nl> return path ; <nl> 
int mailbox_get_guid ( struct mailbox * box , uint8_t guid [ MAIL_GUID_128_SIZE ]) <nl> if ( box -> v . get_guid == NULL ) { <nl> mail_storage_set_error ( box -> storage , MAIL_ERROR_NOTPOSSIBLE , <nl> " Storage doesn ' t support mailbox GUIDs "); <nl> + return - 1 ; <nl> } <nl> if (! box -> opened ) { <nl> if ( mailbox_open ( box ) < 0 )
void auth_client_request_abort ( struct auth_client_request ** _request ) <nl>  <nl> auth_client_send_cancel ( request -> conn -> client , request -> id ); <nl> call_callback ( request , AUTH_REQUEST_STATUS_ABORT , NULL , NULL ); <nl> + pool_unref (& request -> pool ); <nl> } <nl>  <nl> unsigned int auth_client_request_get_id ( struct auth_client_request * request )
uint64_t mail_index_transaction_get_highest_modseq ( struct mail_index_transaction <nl> new_highest_modseq ++; <nl> } <nl> if ( array_is_created (& t -> updates ) && <nl> - transaction_flag_updates_have_non_internal ( t ) > 0 ) <nl> + transaction_flag_updates_have_non_internal ( t )) <nl> new_highest_modseq ++; <nl> if ( array_is_created (& t -> keyword_updates )) { <nl> new_highest_modseq +=
cmd_save_to_mailbox ( struct save_cmd_context * ctx , struct mailbox * box , <nl> i_error (" open (% s ) failed : % s ", <nl> i_stream_get_name ( input ), <nl> i_stream_get_error ( input )); <nl> + ctx -> ctx . exit_code = EX_TEMPFAIL ; <nl> return - 1 ; <nl> } <nl> 
int mailbox_list_delete_trash ( const char * path , const char ** error_r ) <nl> errno = ELOOP ; <nl> return - 1 ; <nl> } <nl> + return - 1 ; <nl> } <nl> return 0 ; <nl> }
authd_abort_client ( struct Client * client_p ) <nl>  <nl> /* XXX should we blindly allow like this ? */ <nl> authd_decide_client ( client_p , "*", "*", true , '\ 0 ', NULL , NULL ); <nl> - <nl> client_p -> preClient -> authd_cid = 0 ; <nl> } <nl> 
CURLcode Curl_sasl_create_digest_md5_message ( struct SessionHandle * data , <nl> " qop =% s ", <nl> userp , realm , nonce , <nl> cnonce , nonceCount , spn , resp_hash_hex , qop ); <nl> + Curl_safefree ( spn ); <nl> if (! response ) <nl> return CURLE_OUT_OF_MEMORY ; <nl>  <nl> CURLcode Curl_sasl_create_digest_md5_message ( struct SessionHandle * data , <nl> result = Curl_base64_encode ( data , response , 0 , outptr , outlen ); <nl>  <nl> Curl_safefree ( response ); <nl> - Curl_safefree ( spn ); <nl>  <nl> return result ; <nl> }
CURLcode Curl_setopt ( struct Curl_easy * data , CURLoption option , <nl> arg = va_arg ( param , long ); <nl> if ( arg < CURLSSH_AUTH_NONE ) <nl> return CURLE_BAD_FUNCTION_ARGUMENT ; <nl> - data -> set . ssh_auth_types = va_arg ( param , long ); <nl> + data -> set . ssh_auth_types = arg ; <nl> break ; <nl>  <nl> case CURLOPT_SSH_PUBLIC_KEYFILE :
struct pbsnode * find_fitting_node ( <nl> if (( pnode = check_node ( ln , needed )) != NULL ) <nl> { <nl> ln -> times_used ++; <nl> + free_resizable_array ( ordered ); <nl> return ( pnode ); <nl> } <nl> }
# include "../../ mem / mem . h " <nl> # include "../../ md5utils . h " <nl> # include "../../ ip_addr . h " <nl> +# include "../../ parser / parse_uri . h " <nl>  <nl> # include " config . h " <nl> # include " lock . h "
void clean_hdr_field ( struct hdr_field * hf ) <nl> break ; <nl>  <nl> case HDR_SESSIONEXPIRES_T : <nl> + if (* h_parsed ) { <nl> + (( hf_parsed_t *)(* h_parsed ))-> hfree (* h_parsed ); <nl> + * h_parsed = 0 ; <nl> + } <nl> + break ; <nl> + <nl> case HDR_MIN_SE_T : <nl> case HDR_ACCEPTCONTACT_T : <nl> case HDR_ALLOWEVENTS_T :
int convert_temporary_dialog ( ua_pres_t * dialog ) <nl> temp_dialog = get_temporary_dialog ( dialog , hash_code ); <nl> if ( temp_dialog ) <nl> delete_htable ( temp_dialog , hash_code ); <nl> - else <nl> + else { <nl> + lock_release (& HashT -> p_records [ hash_code ]. lock ); <nl> return - 1 ; <nl> + } <nl>  <nl> insert_htable ( dialog , hash_code ); <nl> 
inline static str * binrpc_val_conv_str ( struct binrpc_ctx * ctx , <nl> s = int2str ( v -> u . intval , & len ); <nl> ret = ctl_malloc ( sizeof (* ret )+ len + 1 ); <nl> if ( ret == 0 || binrpc_gc_track ( ctx , ret )!= 0 ){ <nl> + if ( ret != 0 ) ctl_free ( ret ); <nl> * err = E_BINRPC_OVERFLOW ; <nl> return 0 ; <nl> }
void tm_ctx_set_branch_index ( int v ); <nl>  <nl> # else <nl>  <nl> -# define tm_ctx_get () <nl> +# define tm_ctx_get () NULL <nl> # define tm_ctx_init () <nl> # define tm_ctx_set_branch_index ( v ) <nl> 
int build_path_vector ( struct sip_msg * _m , str * path , str ** received ) <nl> goto error ; <nl> } <nl> for (; params ; params = params -> next ) { <nl> - if ( params -> type == P_RECEIVED ) <nl> + if ( params -> type == P_RECEIVED ) { <nl> * received = & hooks . contact . received -> body ; <nl> + break ; <nl> + } <nl> } <nl> + free_params ( params ); <nl> } <nl> free_rr (& route ); <nl> }
static void mod_destroy ( void ) <nl> free_atom_fmt_buff (); <nl> free_list_fmt_buff (); <nl> free_xbuff_fmt_buff (); <nl> + free_pid_fmt_buff (); <nl> } <nl>  <nl> static int postprocess_request ( struct sip_msg * msg , unsigned int flags , void * _param ) <nl> static int postprocess_request ( struct sip_msg * msg , unsigned int flags , void * _p <nl> free_atom_fmt_buff (); <nl> free_list_fmt_buff (); <nl> free_xbuff_fmt_buff (); <nl> + free_pid_fmt_buff (); <nl> return 0 ; <nl> } <nl> 
sca_reply ( sca_mod * scam , int status_code , char * status_msg , <nl> SCA_STR_COPY_CSTR ( & extra_headers , " Expires : " ); <nl>  <nl> len = snprintf ( extra_headers . s + extra_headers . len , <nl> - sizeof ( hdr_buf - extra_headers . len ), <nl> + sizeof ( hdr_buf ) - extra_headers . len , <nl> "% d % s ", expires , CRLF ); <nl> extra_headers . len += len ; <nl> 
rs_set_warmth_auto ( RS_BLOB * rs ) <nl> gdouble dsum [ 8 ], dmax ; <nl> gfloat tint , warmth ; <nl>  <nl> + if ( unlikely (! rs -> in_use )) return ; <nl> + <nl> for ( row = 0 ; row < rs -> input -> h - 7 ; row += 8 ) <nl> for ( col = 0 ; col < rs -> input -> w - 7 ; col += 8 ) <nl> {
void mp_msg_va ( struct mp_log * log , int lev , const char * format , va_list va ) <nl>  <nl> set_msg_color ( stream , lev ); <nl> if ( header ) { <nl> - if ( lev >= MSGL_V || verbose || mp_msg_module ) { <nl> + if (( lev >= MSGL_V && lev != MSGL_SMODE ) || verbose || mp_msg_module ) { <nl> fprintf ( stream , "[% s ] ", log -> verbose_prefix ); <nl> } else if ( log -> prefix ) { <nl> fprintf ( stream , "[% s ] ", log -> prefix );
static int demux_mkv_read_tags ( demuxer_t * demuxer ) <nl> demux_info_add_bstr ( demuxer , tag . simple_tag [ j ]. tag_name , tag . simple_tag [ j ]. tag_string ); <nl> } <nl>  <nl> + talloc_free ( parse_ctx . talloc_ctx ); <nl> return 0 ; <nl> } <nl> 
static void print_status ( float a_pos , float a_v , float corr ) <nl> width = screen_width ; <nl> else <nl> width = 80 ; <nl> +# ifdef WIN32 <nl> + // windows command line is broken ( MinGW ' s rxvt works though , but we <nl> + // should not depend on that ). <nl> + width --; <nl> +# endif <nl> line = malloc ( width + 1 ); // one additional for terminating null <nl>  <nl> // Audio time
int mpcodecs_config_vo ( sh_video_t * sh , int w , int h , unsigned int preferred_outf <nl> } <nl> } <nl>  <nl> + if ( video_out -> get_info ) <nl> { const vo_info_t * info = video_out -> get_info (); <nl> mp_msg ( MSGT_CPLAYER , MSGL_INFO ," VO : [% s ] % dx % d => % dx % d % s % s % s % s % s \ n ", info -> short_name , <nl> sh -> disp_w , sh -> disp_h ,
bool load_bitstream_intelhex ( bytes_t * rv , const char * dname , const char * fn ) <nl> applog ( LOG_ERR , " Error reading '% s '", fn ); <nl> goto ihxerr ; <nl> } <nl> - fgets ( buf , sizeof ( buf ), F ); <nl> + if (! fgets ( buf , sizeof ( buf ), F )) <nl> + goto ihxerr ; <nl> if ( unlikely ( buf [ 0 ] != ':')) <nl> goto ihxerr ; <nl> if ( unlikely (!(
static void devdetail_an ( struct io_data * io_data , struct cgpu_info * cgpu , bool i <nl> if ( cgpu -> device_path ) <nl> root = api_add_string ( root , " Device Path ", cgpu -> device_path , false ); <nl>  <nl> - if ( cgpu -> drv -> get_api_extra_device_detail ) <nl> + if (( per_proc || cgpu -> procs <= 1 ) && cgpu -> drv -> get_api_extra_device_detail ) <nl> root = api_add_extra ( root , cgpu -> drv -> get_api_extra_device_detail ( cgpu )); <nl>  <nl> root = print_data ( root , buf , isjson , precom );
bool initiate_stratum ( struct pool * pool ) <nl> goto out ; <nl> } <nl>  <nl> + free ( pool -> nonce1 ); <nl> pool -> nonce1 = strdup ( json_string_value ( json_array_get ( res_val , 1 ))); <nl> if (! pool -> nonce1 ) { <nl> applog ( LOG_WARNING , " Failed to get nonce1 in initiate_stratum ");
static void * submit_work_thread ( __maybe_unused void * userdata ) <nl> } <nl> ++ wip ; <nl> } <nl> - else <nl> + else { <nl> -- total_submitting ; <nl> + free_work ( work ); <nl> + } <nl> } <nl> if ( unlikely ( shutting_down && ! wip )) <nl> break ;
AcpiGetHandle ( <nl> { <nl> ACPI_STATUS Status ; <nl> NAME_TABLE_ENTRY * ThisEntry ; <nl> - ACPI_HANDLE Scope = NULL ; <nl> + NAME_TABLE_ENTRY * Scope = NULL ; <nl>  <nl> if (! RetHandle || ! Pathname ) <nl> { <nl> AcpiGetHandle ( <nl> } <nl>  <nl> if ( Parent ) <nl> - Scope = Parent -> Scope ; <nl> + Scope = (( NAME_TABLE_ENTRY *) Parent )-> Scope ; <nl>  <nl> /* Special case for root , since we can ' t search for it */ <nl> 
ACPI_STATUS <nl> OsGetGlobalLock ( void ) <nl> { <nl> UINT32 GlobalLockReg ; <nl> - ACPI_STATUS Status ; <nl> + ACPI_STATUS Status = AE_OK ; <nl>  <nl>  <nl> if ( FACS )
AcpiHwClearAcpiStatus ( <nl>  <nl> Status = AcpiHwRegisterWrite ( ACPI_REGISTER_PM1_STATUS , <nl> ACPI_BITMASK_ALL_FIXED_STATUS ); <nl> + <nl> + AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + <nl> if ( ACPI_FAILURE ( Status )) <nl> { <nl> - goto UnlockAndExit ; <nl> + goto Exit ; <nl> } <nl>  <nl> /* Clear the GPE Bits in all GPE registers in all GPE blocks */ <nl>  <nl> Status = AcpiEvWalkGpeList ( AcpiHwClearGpeBlock , NULL ); <nl>  <nl> - UnlockAndExit : <nl> - AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + Exit : <nl> return_ACPI_STATUS ( Status ); <nl> } <nl> 
uint32_t HELPER ( neon_rshl_u32 )( uint32_t val , uint32_t shiftop ) <nl> uint64_t HELPER ( neon_rshl_u64 )( uint64_t val , uint64_t shiftop ) <nl> { <nl> int8_t shift = ( uint8_t ) shiftop ; <nl> - if ( shift >= 64 || shift < 64 ) { <nl> + if ( shift >= 64 || shift < - 64 ) { <nl> val = 0 ; <nl> } else if ( shift == - 64 ) { <nl> /* Rounding a 1 - bit result just preserves that bit . */
static int qemu_event_init ( void ) <nl>  <nl> static void qemu_event_increment ( void ) <nl> { <nl> - SetEvent ( qemu_event_handle ); <nl> + if (! SetEvent ( qemu_event_handle )) { <nl> + fprintf ( stderr , " qemu_event_increment : SetEvent failed : % d \ n ", <nl> + GetLastError ()); <nl> + exit ( 1 ); <nl> + } <nl> } <nl> # endif <nl> 
static void framebuffer_update_request ( VncState * vs , int incremental , <nl> return ; <nl> } <nl>  <nl> + vs -> force_update = 1 ; <nl> vnc_set_area_dirty ( vs -> dirty , width , height , x , y , w , h ); <nl> } <nl> 
static TCGArg * tcg_constant_folding ( TCGContext * s , uint16_t * tcg_opc_ptr , <nl> /* Simplify LT / GE comparisons vs zero to a single compare <nl> vs the high word of the input . */ <nl> s -> gen_opc_buf [ op_index ] = INDEX_op_setcond_i32 ; <nl> + reset_temp ( args [ 0 ]); <nl> gen_args [ 0 ] = args [ 0 ]; <nl> gen_args [ 1 ] = args [ 2 ]; <nl> gen_args [ 2 ] = args [ 4 ];
static int kvm_sclp_service_call ( S390CPU * cpu , struct kvm_run * run , <nl> int r = 0 ; <nl>  <nl> cpu_synchronize_state ( CPU ( cpu )); <nl> + if ( env -> psw . mask & PSW_MASK_PSTATE ) { <nl> + enter_pgmcheck ( cpu , PGM_PRIVILEGED ); <nl> + return 0 ; <nl> + } <nl> sccb = env -> regs [ ipbh0 & 0xf ]; <nl> code = env -> regs [( ipbh0 & 0xf0 ) >> 4 ]; <nl> 
static void coroutine_fn backup_run ( void * opaque ) <nl>  <nl> if ( job -> sync_bitmap ) { <nl> BdrvDirtyBitmap * bm ; <nl> - if ( ret < 0 ) { <nl> + if ( ret < 0 || block_job_is_cancelled (& job -> common )) { <nl> /* Merge the successor back into the parent , delete nothing . */ <nl> bm = bdrv_reclaim_dirty_bitmap ( bs , job -> sync_bitmap , NULL ); <nl> assert ( bm );
static void * qpa_thread_out ( void * arg ) <nl> return NULL ; <nl> } <nl>  <nl> + pa -> live = 0 ; <nl> pa -> rpos = rpos ; <nl> - pa -> live -= decr ; <nl> pa -> decr += decr ; <nl> } <nl> 
static int iscsi_open ( BlockDriverState * bs , QDict * options , int flags , <nl> QemuOpts * opts ; <nl> Error * local_err = NULL ; <nl> const char * filename ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> if (( BDRV_SECTOR_SIZE % 512 ) != 0 ) { <nl> error_setg ( errp , " iSCSI : Invalid BDRV_SECTOR_SIZE . "
static int decode_micromips_opc ( CPUMIPSState * env , DisasContext * ctx , int * is_b <nl> case LB32 : <nl> case LH32 : <nl> case DADDIU32 : <nl> - case POOL48A : /* ??? */ <nl> case LWC132 : <nl> case LDC132 : <nl> case LD32 :
static sd_rsp_type_t sd_normal_command ( SDState * sd , <nl> } <nl> break ; <nl>  <nl> + case 5 : /* CMD5 : reserved for SDIO cards */ <nl> + sd -> card_status |= ILLEGAL_COMMAND ; <nl> + return sd_r0 ; <nl> + <nl> case 6 : /* CMD6 : SWITCH_FUNCTION */ <nl> if ( sd -> spi ) <nl> goto bad_cmd ;
static int unpack ( const struct optstruct * opts ) <nl> name [ sizeof ( name )- 1 ]='\ 0 '; <nl> } <nl>  <nl> + if ( cl_cvdverify ( name ) != CL_SUCCESS ) { <nl> + mprintf ("! unpack : % s is not a valid CVD \ n ", name ); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( cli_cvdunpack ( name , ".") == - 1 ) { <nl> mprintf ("! unpack : Can ' t unpack file % s \ n ", name ); <nl> return - 1 ;
int cli_url_canon ( const char * inurl , size_t len , char * urlbuff , size_t dest_len , <nl> ++ host_begin ; <nl>  <nl> /* ignore username in URL */ <nl> - p = strchr ( host_begin , '@'); <nl> + while (( host_begin < urlend ) && * host_begin == '/') ++ host_begin ; <nl> + host_len = strcspn ( host_begin , ":/?"); <nl> + p = memchr ( host_begin , '@', host_len ); <nl> if ( p ) <nl> host_begin = p + 1 ; <nl> url = host_begin ;
int qtm_decompress ( struct qtm_stream * qtm , uint32_t out_bytes ) { <nl> if (( frame_start + QTM_FRAME_SIZE ) < frame_end ) { <nl> frame_end = frame_start + QTM_FRAME_SIZE ; <nl> } <nl> + if ( frame_end < window_posn ) { <nl> + cli_dbgmsg (" qtm_decompress : window position beyond end of frame \ n "); <nl> + return qtm -> error = CL_EFORMAT ; <nl> + } <nl>  <nl> while ( window_posn < frame_end ) { <nl> QTM_GET_SYMBOL ( qtm -> model7 , selector );
static int valid_pmbr ( struct fdisk_context * cxt ) <nl> goto check_hybrid ; <nl> } <nl> } <nl> - check_hybrid : <nl> + <nl> if ( ret != GPT_MBR_PROTECTIVE ) <nl> goto done ; <nl> + check_hybrid : <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> if (( pmbr -> partition_record [ i ]. os_type != EFI_PMBR_OSTYPE ) && <nl> ( pmbr -> partition_record [ i ]. os_type != 0x00 ))
static void get_sem_elements ( struct sem_data * p ) <nl> { <nl> size_t i ; <nl>  <nl> - if (! p || ! p -> sem_nsems || p -> sem_perm . id < 0 ) <nl> + if (! p || ! p -> sem_nsems || p -> sem_nsems > SIZE_MAX || p -> sem_perm . id < 0 ) <nl> return ; <nl>  <nl> p -> elements = xcalloc ( p -> sem_nsems , sizeof ( struct sem_elem ));
static void removeIndexEntry ( dbIndex * dbi , char * key , dbIndexRecord rec , <nl> case 2 : <nl> break ; /* error message already generated from dbindex . c */ <nl> } <nl> + <nl> + freeDBIndexRecord ( matches ); <nl> } <nl>  <nl> int rpmdbRemove ( rpmdb db , unsigned int offset , int tolerant ) { <nl> int rpmdbRemove ( rpmdb db , unsigned int offset , int tolerant ) { <nl>  <nl> unblockSignals (); <nl>  <nl> + freeHeader ( h ); <nl> + <nl> return 0 ; <nl> } <nl> 
int sync_crc_calc ( struct mailbox * mailbox , char * buf , int maxlen ) <nl> if ( mailbox_read_index_record ( mailbox , recno , & record )) <nl> continue ; <nl>  <nl> + /* always skip EXPUNGED flags , so we don ' t count the annots */ <nl> + if ( record . system_flags & FLAG_EXPUNGED ) <nl> + continue ; <nl> + <nl> sync_crc_algorithm -> addrecord ( mailbox , & record , sync_crc_covers ); <nl> if (( sync_crc_covers & SYNC_CRC_ANNOTATIONS ) && user_annot_db ) { <nl> r = read_annotations ( mailbox , & record , & annots );
static void cmd_search ( char * tag , int usinguid ) <nl> freesearchargs ( searchargs ); <nl> return ; <nl> } <nl> + if ( imapd_id . quirks & QUIRK_SEARCHFUZZY ) { <nl> + char * expr = search_expr_serialise ( searchargs -> root ); <nl> + syslog ( LOG_NOTICE , " fuzzy search % s ", expr ); <nl> + free ( expr ); <nl> + } <nl>  <nl> if ( c == '\ r ') c = prot_getc ( imapd_in ); <nl> if ( c != '\ n ') {
static int mboxlist_do_find ( struct find_rock * rock , const strarray_t * patterns ) <nl> if ( len ) len --; // trailing separator <nl>  <nl> if (! strncmp ( rock -> namespace -> prefix [ NAMESPACE_USER ], commonpat , MIN ( len , prefixlen ))) { <nl> - if ( prefixlen < len ) { <nl> + if ( prefixlen <= len ) { <nl> /* we match all users */ <nl> strlcpy ( domainpat + domainlen , " user .", sizeof ( domainpat )- domainlen ); <nl> }
static int expunge_userflags ( struct mailbox * mailbox , struct expire_rock * erock ) <nl> for ( i = 0 ; i < MAX_USER_FLAGS ; i ++) { <nl> if ( erock -> userflags [ i / 32 ] & 1 <<( i & 31 )) <nl> continue ; <nl> + if (! mailbox -> flagname [ i ]) <nl> + continue ; <nl> if ( verbose ) <nl> fprintf ( stderr , " Expunging userflag % u (% s ) from % s \ n ", <nl> i , mailbox -> flagname [ i ], mailbox -> name );
static int expire_conversations ( const mbentry_t * mbentry , void * rock ) <nl> if ( mbentry -> mbtype & MBTYPE_REMOTE ) <nl> goto done ; <nl>  <nl> + if ( mboxname_isdeletedmailbox ( mbentry -> name , NULL )) <nl> + goto done ; <nl> + <nl> filename = conversations_getmboxpath ( mbentry -> name ); <nl> if (! filename ) <nl> goto done ;
static int mailbox_open_advanced ( const char * name , <nl> goto done ; <nl> } <nl>  <nl> + if (! mbentry -> partition ) { <nl> + mboxlist_entry_free (& mbentry ); <nl> + r = IMAP_MAILBOX_NONEXISTENT ; <nl> + goto done ; <nl> + } <nl> + <nl> mailbox -> part = xstrdup ( mbentry -> partition ); <nl>  <nl> /* Note that the header does have the ACL information , but it is only
int meth_acl ( struct transaction_t * txn , void * params ) <nl> else if (! xmlStrcmp ( priv -> name , <nl> BAD_CAST " read ")) <nl> rights |= DACL_READ ; <nl> + else if (! xmlStrcmp ( priv -> name , <nl> + BAD_CAST " read - free - busy ")) <nl> + rights |= DACL_READFB ; <nl> else if (! xmlStrcmp ( priv -> name , <nl> BAD_CAST " write ")) <nl> rights |= DACL_WRITE ;
int clusterLoadConfig ( char * filename ) { <nl> while ( fgets ( line , maxline , fp ) != NULL ) { <nl> int argc ; <nl> sds * argv = sdssplitargs ( line ,& argc ); <nl> + if ( argv == NULL ) goto fmterr ; <nl> + <nl> clusterNode * n , * master ; <nl> char * p , * s ; <nl> 
static int read_header ( ShortenContext * s ) <nl> s -> channels = get_uint ( s , CHANSIZE ); <nl> if ( s -> channels <= 0 || s -> channels > MAX_CHANNELS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + s -> channels = 0 ; <nl> return - 1 ; <nl> } <nl> s -> avctx -> channels = s -> channels ;
static int audio_decode_frame ( VideoState * is , double * pts_ptr ) <nl> /* free the current packet */ <nl> if ( pkt -> data ) <nl> av_free_packet ( pkt ); <nl> + memset ( pkt_temp , 0 , sizeof (* pkt_temp )); <nl>  <nl> if ( is -> paused || is -> audioq . abort_request ) { <nl> return - 1 ;
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> break ; <nl> case MKTAG (' s ', ' t ', ' r ', ' f '): <nl> /* stream header */ <nl> - if ( stream_index >= s -> nb_streams || avi -> dv_demux ) { <nl> + if ( stream_index >= ( unsigned ) s -> nb_streams || avi -> dv_demux ) { <nl> url_fskip ( pb , size ); <nl> } else { <nl> st = s -> streams [ stream_index ];
static inline int put_bits_count ( PutBitContext * s ) <nl> static inline void flush_put_bits ( PutBitContext * s ) <nl> { <nl> # ifndef BITSTREAM_WRITER_LE <nl> - s -> bit_buf <<= s -> bit_left ; <nl> + if ( s -> bit_left < 32 ) <nl> + s -> bit_buf <<= s -> bit_left ; <nl> # endif <nl> while ( s -> bit_left < 32 ) { <nl> /* XXX : should test end of buffer */
static void svq1_write_header ( SVQ1Context * s , int frame_type ) <nl> # define QUALITY_THRESHOLD 100 <nl> # define THRESHOLD_MULTIPLIER 0 . 6 <nl>  <nl> +# if defined ( HAVE_ALTIVEC ) <nl> +# undef vector <nl> +# endif <nl>  <nl> static int encode_block ( SVQ1Context * s , uint8_t * src , uint8_t * ref , uint8_t * decoded , int stride , int level , int threshold , int lambda , int intra ){ <nl> int count , y , x , i , j , split , best_mean , best_score , best_count ;
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
static int mpegts_read_packet ( AVFormatContext * s , <nl> MpegTSContext * ts = s -> priv_data ; <nl> int ret , i ; <nl>  <nl> + pkt -> size = - 1 ; <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> static int mpegts_read_packet ( AVFormatContext * s , <nl> } <nl> } <nl>  <nl> + if (! ret && pkt -> size < 0 ) <nl> + ret = AVERROR ( EINTR ); <nl> return ret ; <nl> } <nl> 
static void set_codec_str ( AVFormatContext * s , AVCodecParameters * par , <nl> tags [ 0 ] = ff_mp4_obj_type ; <nl> oti = av_codec_get_tag ( tags , par -> codec_id ); <nl> if ( oti ) <nl> - av_strlcatf ( str , size , ".% 02x ", oti ); <nl> + av_strlcatf ( str , size , ".% 02 " SCNx32 , oti ); <nl> else <nl> return ; <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Invalid combination - slices % d and - max_nal_size % d .\ n ", <nl> avctx -> slices , s -> max_nal_size ); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl>  <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } else { <nl> av_log ( avctx , AV_LOG_ERROR , " Invalid - max_nal_size , " <nl> " specify a valid max_nal_size to use - slice_mode dyn \ n "); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl> }
static int mp3_read_probe ( AVProbeData * p ) <nl>  <nl> max_frames = 0 ; <nl> buf = buf0 ; <nl> - end = buf + p -> buf_size - sizeof ( uint32_t ); <nl> + end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl>  <nl> for (; buf < end ; buf = buf2 + 1 ) { <nl> buf2 = buf ;
static const char * filter_name ( void * p ) <nl> } <nl>  <nl> static const AVClass avfilter_class = { <nl> - " AVFilter ", <nl> - filter_name , <nl> - NULL , <nl> - LIBAVUTIL_VERSION_INT , <nl> + . class_name = " AVFilter ", <nl> + . item_name = filter_name , <nl> + . version = LIBAVUTIL_VERSION_INT , <nl> }; <nl>  <nl> int avfilter_open ( AVFilterContext ** filter_ctx , AVFilter * filter , const char * inst_name )
static av_cold int twin_decode_init ( AVCodecContext * avctx ) <nl> default : avctx -> sample_rate = isampf * 1000 ; break ; <nl> } <nl>  <nl> - if ( avctx -> channels > CHANNELS_MAX ) { <nl> + if ( avctx -> channels <= 0 || avctx -> channels > CHANNELS_MAX ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unsupported number of channels : % i \ n ", <nl> avctx -> channels ); <nl> return - 1 ;
static int64_t ism_seek ( void * opaque , int64_t offset , int whence ) <nl> os -> tail_out = NULL ; <nl> } <nl> if ( offset >= os -> cur_start_pos ) { <nl> - ffurl_seek ( os -> out , offset - os -> cur_start_pos , SEEK_SET ); <nl> + if ( os -> out ) <nl> + ffurl_seek ( os -> out , offset - os -> cur_start_pos , SEEK_SET ); <nl> os -> cur_pos = offset ; <nl> return offset ; <nl> }
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels == 0 || channels > 64 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
otp_hotp_mac ( const unsigned char counter [ 8 ], unsigned char output [ 7 ], <nl> /* 1 . hmac */ <nl> if (! HMAC ( EVP_sha1 (), keyblock , key_len , counter , 8 , hmac , & hmac_len ) || <nl> hmac_len != 20 ) { <nl> - otp_log ( OTP_LOG_ERR , "% s : HMAC failed : HMAC ", log_prefix ); <nl> + otp_log ( OTP_LOG_ERR , "% s : HMAC failed ", log_prefix ); <nl> return - 1 ; <nl> } <nl> 
static gboolean prplcb_xfer_new_send_cb ( gpointer data , gint fd , b_input_conditio <nl> /* TODO ( wilmer ): After spreading some more const goodness in BitlBee , <nl> remove the evil cast below . */ <nl> px -> ft = imcb_file_send_start ( ic , ( char *) who , xfer -> filename , xfer -> size ); <nl> + <nl> + if (! px -> ft ) { <nl> + return FALSE ; <nl> + } <nl> px -> ft -> data = px ; <nl>  <nl> px -> ft -> accept = prpl_xfer_accept ;
gboolean <nl> profile_exists ( const gchar * profilename , gboolean global ) <nl> { <nl> gchar * path = NULL , * global_path ; <nl> + if (! profilename ) <nl> + return FALSE ; <nl> if ( global ) { <nl> global_path = get_global_profiles_dir (); <nl> path = g_strdup_printf ("% s % s % s ", global_path ,
proto_register_sua ( void ) <nl> " This may affect TCAP ' s ability to recognize which messages belong to which TCAP session .", & set_addresses ); <nl>  <nl> heur_subdissector_list = register_heur_dissector_list (" sua "); <nl> - sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_NOT_ALLOW_DUPLICATE ); <nl> + sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_ALLOW_DUPLICATE ); <nl> sua_tap = register_tap (" sua "); <nl>  <nl> assocs = wmem_tree_new_autoreset ( wmem_epan_scope (), wmem_file_scope ());
rtp_streams_stat_draw ( void * arg _U_ ) <nl>  <nl> list = g_list_next ( list ); <nl>  <nl> - g_free ( payload_type ); <nl> wmem_free ( NULL , src_addr ); <nl> wmem_free ( NULL , dst_addr ); <nl> wmem_free ( NULL , payload_type );
WirelessFrame :: WirelessFrame ( QWidget * parent ) : <nl>  <nl> WirelessFrame ::~ WirelessFrame () <nl> { <nl> + ws80211_free_interfaces ( interfaces_ ); <nl> delete ui ; <nl> } <nl> 
dissect_vmlab ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint8 attributes ; <nl> guint8 portgroup ; <nl>  <nl> - guint16 encap_proto ; <nl> + volatile guint16 encap_proto ; <nl>  <nl> col_set_str ( pinfo -> cinfo , COL_PROTOCOL , " VMLAB "); <nl> col_clear ( pinfo -> cinfo , COL_INFO );
nextcontext : <nl> tvb_previous_offset = tvb_find_guint8 ( tvb , tvb_current_offset , <nl> tvb_len , '=')+ 1 ; <nl> tvb_previous_offset = tvb_skip_wsp ( tvb , tvb_previous_offset ); <nl> - tvb_current_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> + tvb_next_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> tvb_len , '{'); <nl> + if ( tvb_current_offset >= tvb_next_offset ) { <nl> + proto_tree_add_text ( megaco_tree , tvb , 0 , 0 , "[ Parse error : Invalid offset ]"); <nl> + return ; <nl> + } <nl> + tvb_current_offset = tvb_next_offset ; <nl>  <nl>  <nl> tokenlen = tvb_current_offset - tvb_previous_offset ;
PHP_FUNCTION ( str_repeat ) <nl> int l = 0 ; <nl> memcpy ( result , input_str , input_str_len ); <nl> s = result ; <nl> - e = result + input_str_len ; <nl> - ee = result + result_len ; <nl> + e = ( char *) result + input_str_len ; <nl> + ee = ( char *) result + result_len ; <nl>  <nl> while ( e < ee ) { <nl> l = ( e - s ) < ( ee - e ) ? ( e - s ) : ( ee - e );
static void _php_mb_regex_ereg_replace_exec ( INTERNAL_FUNCTION_PARAMETERS , OnigOp <nl> ! ZVAL_IS_UNDEF (& retval )) { <nl> convert_to_string_ex (& retval ); <nl> smart_str_appendl (& out_buf , Z_STRVAL ( retval ), Z_STRLEN ( retval )); <nl> - eval_buf . s -> len = 0 ; <nl> + if ( eval_buf . s ) { <nl> + eval_buf . s -> len = 0 ; <nl> + } <nl> zval_ptr_dtor (& retval ); <nl> } else { <nl> efree ( description );
static sdlParamPtr get_param ( sdlFunctionPtr function , char * param_name , int inde <nl> } else { <nl> ht = function -> responseParameters ; <nl> } <nl> + <nl> + if ( ht == NULL ) { <nl> + return NULL ; <nl> + } <nl>  <nl> if ( param_name != NULL ) { <nl> if ( zend_hash_find ( ht , param_name , strlen ( param_name ), ( void **)& tmp ) != FAILURE ) {
PHPAPI size_t php_strip_tags ( char * rbuf , int len , int * stateptr , char * allow , in <nl>  <nl> while ( i < len ) { <nl> switch ( c ) { <nl> + case '\ 0 ': <nl> + break ; <nl> case '<': <nl> if ( isspace (*( p + 1 ))) { <nl> goto reg_char ;
PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl>  <nl> if ( php_check_open_basedir ( path_copy TSRMLS_CC )) { <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> return NULL ; <nl> } <nl> PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl> * opened_path = estrdup ( path_copy ); <nl> } <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> path_copy = NULL ; <nl> 
void ThreadingMsgListModel :: setSourceModel ( QAbstractItemModel * sourceModel ) <nl> this , SLOT ( handleRowsAboutToBeInserted ( const QModelIndex &, int , int ) ) ); <nl> connect ( sourceModel , SIGNAL ( rowsInserted ( const QModelIndex &, int , int ) ), <nl> this , SLOT ( handleRowsInserted ( const QModelIndex &, int , int ) ) ); <nl> + resetMe (); <nl> } <nl>  <nl> void ThreadingMsgListModel :: handleDataChanged ( const QModelIndex & topLeft , const QModelIndex & bottomRight )
enum config_type { <nl> # define VALUE_SIZE 100 <nl> # define ID_SIZE 50 <nl> # define PLACE_SIZE 50 <nl> +# define PROGRAM_SIZE 100 <nl>  <nl> # define TYPE_LABEL " LABEL " <nl> # define TYPE_NUMBER " NUMBER " <nl> struct config_device { <nl> char id [ ID_SIZE ]; <nl> char place [ PLACE_SIZE ]; <nl> char kernel_name [ NAME_SIZE ]; <nl> - char exec_program [ FILE_SIZE ]; <nl> + char exec_program [ PROGRAM_SIZE ]; <nl> char name [ NAME_SIZE ]; <nl> char symlink [ NAME_SIZE ]; <nl> struct sysfs_pair sysfs_pair [ MAX_SYSFS_PAIRS ];
struct DnsTransaction { <nl>  <nl> uint16_t id ; <nl>  <nl> - bool initial_jitter_scheduled ; <nl> - bool initial_jitter_elapsed ; <nl> + bool initial_jitter_scheduled : 1 ; <nl> + bool initial_jitter_elapsed : 1 ; <nl>  <nl> DnsPacket * sent , * received ; <nl> 
static int show_one ( <nl> */ <nl> if ( info . pid_file && access ( info . pid_file , F_OK ) == 0 ) <nl> r = 1 ; <nl> + else if ( streq_ptr ( info . load_state , " not - found ") && streq_ptr ( info . active_state , " inactive ")) <nl> + r = 4 ; <nl> else <nl> r = 3 ; <nl> }
static int event_make_signal_data ( <nl> d -> priority = priority ; <nl>  <nl> r = hashmap_put ( e -> signal_data , & d -> priority , d ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( d ); <nl> return r ; <nl> + } <nl>  <nl> added = true ; <nl> }
struct udev * udev_new ( void ) <nl> } <nl>  <nl> if ( strcasecmp ( key , " udev_log ") == 0 ) { <nl> - udev -> log_priority = util_log_priority ( val ); <nl> + udev_set_log_priority ( udev , util_log_priority ( val )); <nl> continue ; <nl> } <nl> if ( strcasecmp ( key , " udev_root ") == 0 ) {
int link_config_apply ( link_config_ctx * ctx , link_config * config , <nl> if ( ctx -> enable_name_policy && config -> name_policy ) { <nl> NamePolicy * policy ; <nl>  <nl> - for ( policy = config -> name_policy ; ! respect_predictable && ! new_name && <nl> - * policy != _NAMEPOLICY_INVALID ; policy ++) { <nl> + for ( policy = config -> name_policy ; <nl> + ! new_name && * policy != _NAMEPOLICY_INVALID ; policy ++) { <nl> switch (* policy ) { <nl> case NAMEPOLICY_KERNEL : <nl> respect_predictable = true ;
static int cache_space_refresh ( Server * s , JournalStorage * storage ) { <nl>  <nl> ts = now ( CLOCK_MONOTONIC ); <nl>  <nl> - if ( space -> timestamp + RECHECK_SPACE_USEC > ts ) <nl> + if ( space -> timestamp != 0 && space -> timestamp + RECHECK_SPACE_USEC > ts ) <nl> return 0 ; <nl>  <nl> r = determine_path_usage ( s , storage -> path , & vfs_used , & vfs_avail );
int sd_rtnl_message_append_ether_addr ( sd_rtnl_message * m , unsigned short type , c <nl> return - ENOTSUP ; <nl> } <nl>  <nl> - r = add_rtattr ( m , type , data , sizeof ( data )); <nl> + r = add_rtattr ( m , type , data , ETH_ALEN ); <nl> if ( r < 0 ) <nl> return r ; <nl> 
int dns_zone_put ( DnsZone * z , DnsResourceRecord * rr ) { <nl> assert ( z ); <nl> assert ( rr ); <nl>  <nl> + if ( rr -> key -> class == DNS_CLASS_ANY ) <nl> + return - EINVAL ; <nl> + if ( rr -> key -> type == DNS_TYPE_ANY ) <nl> + return - EINVAL ; <nl> + <nl> existing = dns_zone_get ( z , rr ); <nl> if ( existing ) <nl> return 0 ;
static int parse_file ( const char * path , bool ignore_enoent ) { <nl>  <nl> free ( property ); <nl> free ( new_value ); <nl> - if ( r != - EEXIST ) <nl> + if ( r != 0 ) <nl> goto finish ; <nl> } <nl> }
Client_Key_Exchange :: Client_Key_Exchange ( Handshake_IO & io , <nl>  <nl> DL_Group group ( p , g ); <nl>  <nl> - if (! group . verify_group ( rng , true )) <nl> - throw Internal_Error (" DH group failed validation , possible attack "); <nl> + if (! group . verify_group ( rng , false )) <nl> + throw TLS_Exception ( Alert :: INSUFFICIENT_SECURITY , <nl> + " DH group validation failed "); <nl>  <nl> DH_PublicKey counterparty_key ( group , Y ); <nl> 
void xor_buf ( uint8_t out [], <nl> const uint8_t in2 [], <nl> size_t length ) <nl> { <nl> - while ( length >= 8 ) <nl> + while ( length >= 16 ) <nl> { <nl> out [ 0 ] = in [ 0 ] ^ in2 [ 0 ]; <nl> out [ 1 ] = in [ 1 ] ^ in2 [ 1 ];
EVP_CipherInit_ex ( EVP_CIPHER_CTX * ctx , const EVP_CIPHER * c , ENGINE * engine , <nl> ctx -> cipher = c ; <nl> ctx -> key_len = c -> key_len ; <nl>  <nl> - ctx -> cipher_data = malloc ( c -> ctx_size ); <nl> + ctx -> cipher_data = calloc ( 1 , c -> ctx_size ); <nl> if ( ctx -> cipher_data == NULL && c -> ctx_size != 0 ) <nl> return 0 ; <nl> 
static void * rdpei_schedule_thread ( void * arg ) <nl>  <nl> out : <nl>  <nl> - if ( error && rdpei -> rdpcontext ) <nl> + if ( error && rdpei && rdpei -> rdpcontext ) <nl> setChannelError ( rdpei -> rdpcontext , error , <nl> " rdpei_schedule_thread reported an error "); <nl> 
boolean nego_send_negotiation_response ( rdpNego * nego ) <nl> settings -> encryption_method = ENCRYPTION_METHOD_40BIT | ENCRYPTION_METHOD_128BIT | ENCRYPTION_METHOD_FIPS ; <nl> settings -> encryption_level = ENCRYPTION_LEVEL_CLIENT_COMPATIBLE ; <nl> } <nl> + if ( settings -> encryption && settings -> server_key == NULL && settings -> rdp_key_file == NULL ) <nl> + return false ; <nl> } <nl> else if ( settings -> selected_protocol == PROTOCOL_TLS ) <nl> {
DWORD GetTimeZoneInformation ( LPTIME_ZONE_INFORMATION lpTimeZoneInformation ) <nl>  <nl> /* 1 ... TIME_ZONE_ID_STANDARD <nl> * 2 ... TIME_ZONE_ID_DAYLIGHT */ <nl> - return dtz -> SupportsDST ? 2 : 1 ; <nl> + return local_time -> tm_isdst ? 2 : 1 ; <nl> } <nl> else <nl> {
BOOL freerdp_client_detect_command_line ( int argc , char ** argv , DWORD * flags ) <nl> return compatibility ; <nl>  <nl> /* Check , if this may be windows style syntax ... */ <nl> - if ( windows_cli_count && ( windows_cli_count >= posix_cli_count )) <nl> + if ( windows_cli_count && ( windows_cli_count >= posix_cli_count ) || ( windows_cli_status <= COMMAND_LINE_STATUS_PRINT )) <nl> { <nl> + windows_cli_count = 1 ; <nl> * flags = COMMAND_LINE_SEPARATOR_COLON ; <nl> * flags |= COMMAND_LINE_SIGIL_SLASH | COMMAND_LINE_SIGIL_PLUS_MINUS ; <nl> }
BGD_DECLARE ( gdImagePtr ) gdImageCropThreshold ( gdImagePtr im , const unsigned int c <nl> return NULL ; <nl> } <nl>  <nl> + if ( color < 0 || (! gdImageTrueColor ( im ) && color >= gdImageColorsTotal ( im ))) { <nl> + return NULL ; <nl> + } <nl> + <nl> /* TODO : Add gdImageGetRowPtr and works with ptr at the row level <nl> * for the true color and palette images <nl> * new formats will simply work with ptr
public : <nl> ROOT :: NewFunc_t GetNew () const ; <nl> ROOT :: NewArrFunc_t GetNewArray () const ; <nl> Int_t GetNmethods (); <nl> -# ifdef __CINT__ <nl> - TClass ** GetPersistentRef () const { return fPersistentRef ; } <nl> -# else <nl> TClass * const * GetPersistentRef () const { return fPersistentRef ; } <nl> -# endif <nl> TRealData * GetRealData ( const char * name ) const ; <nl> TVirtualRefProxy * GetReferenceProxy () const { return fRefProxy ; } <nl> const ROOT :: Detail :: TSchemaRuleSet * GetSchemaRules () const ;
public : <nl>  <nl> TTreeReader (): <nl> fDirectory ( 0 ), <nl> - fEntryStatus ( kEntryNoTree ) <nl> + fEntryStatus ( kEntryNoTree ), <nl> + fDirector ( 0 ) <nl> {} <nl>  <nl> TTreeReader ( TTree * tree );
static int asf_parse_packet ( AVFormatContext * s , AVIOContext * pb , AVPacket * pkt ) <nl> continue ; <nl> } <nl> asf -> asf_st = & asf -> streams [ s -> streams [ asf -> stream_index ]-> id ]; <nl> - asf -> asf_st -> skip_to_key = 0 ; <nl> + if (! asf -> packet_frag_offset ) <nl> + asf -> asf_st -> skip_to_key = 0 ; <nl> } <nl> asf_st = asf -> asf_st ; <nl> av_assert0 ( asf_st );
void ff_MPV_frame_end ( MpegEncContext * s ) <nl> s -> avctx -> coded_frame = & s -> current_picture_ptr -> f ; <nl>  <nl> if ( s -> codec_id != CODEC_ID_H264 && s -> current_picture . f . reference ) { <nl> - ff_thread_report_progress (& s -> current_picture_ptr -> f , <nl> - s -> mb_height - 1 , 0 ); <nl> + ff_thread_report_progress (& s -> current_picture_ptr -> f , INT_MAX , 0 ); <nl> } <nl> } <nl> 
static const int prof_h264_high [] = { FF_PROFILE_H264_CONSTRAINED_BASELINE , <nl> FF_PROFILE_UNKNOWN }; <nl> static const int prof_hevc_main [] = { FF_PROFILE_HEVC_MAIN , <nl> FF_PROFILE_UNKNOWN }; <nl> - static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN , <nl> - FF_PROFILE_HEVC_MAIN_10 , <nl> + static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN_10 , <nl> FF_PROFILE_UNKNOWN }; <nl>  <nl> static const dxva_mode dxva_modes [] = {
static void close_slaves ( AVFormatContext * avf ) <nl> } <nl> } <nl> av_freep (& tee -> slaves [ i ]. stream_map ); <nl> + av_freep (& tee -> slaves [ i ]. bsfs ); <nl>  <nl> avio_close ( avf2 -> pb ); <nl> avf2 -> pb = NULL ;
static int decode_frame ( AVCodecContext * avctx , void * data , <nl> out : <nl>  <nl> s -> current_picture_ptr = NULL ; <nl> + s -> first_field = 0 ; <nl>  <nl> // FIXME factorize this with the output code below <nl> out = h -> delayed_pic [ 0 ];
static av_always_inline void predict ( PredictorState * ps , int * coef , <nl> if ( shift > 0 ) { <nl> * coef += ( unsigned )(( pv . mant + ( 1 << ( shift - 1 ))) >> shift ); <nl> } else <nl> - * coef += ( unsigned )( pv . mant << - shift ); <nl> + * coef += ( unsigned ) pv . mant << - shift ; <nl> } <nl> } <nl> 
static void smc_decode_stream ( SmcContext * s ) <nl> } else <nl> color_table_index = CQUAD * s -> buf [ stream_ptr ++]; <nl>  <nl> - while ( n_blocks --) { <nl> + while ( n_blocks -- && stream_ptr + 3 < s -> size ) { <nl> color_flags = AV_RB32 (& s -> buf [ stream_ptr ]); <nl> stream_ptr += 4 ; <nl> /* flag mask actually acts as a bit shift count here */
int main ( int argc , char ** argv ){ <nl> FILE * f [ 2 ]; <nl> int i , pos ; <nl> int siglen , datlen ; <nl> - int bestpos ; <nl> + int bestpos = 0 ; <nl> double bestc = 0 ; <nl> double sigamp = 0 ; <nl> int16_t * signal , * data ;
int av_opt_set_dict ( void * obj , AVDictionary ** options ) <nl> AVDictionary * tmp = NULL ; <nl> int ret = 0 ; <nl>  <nl> + if (! options ) <nl> + return 0 ; <nl> + <nl> while (( t = av_dict_get (* options , "", t , AV_DICT_IGNORE_SUFFIX ))) { <nl> ret = av_opt_set ( obj , t -> key , t -> value , 0 ); <nl> if ( ret == AVERROR_OPTION_NOT_FOUND )
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterLink * outlink = inlink -> dst -> outputs [ 0 ]; <nl>  <nl> AVFilterBufferRef * out ; <nl> - int direct , c ; <nl> + int direct = 0 , c ; <nl>  <nl> if ( in -> perms & AV_PERM_WRITE ) { <nl> direct = 1 ;
static void hls_prediction_unit ( HEVCContext * s , int x0 , int y0 , <nl>  <nl> MvField * tab_mvf = s -> ref -> tab_mvf ; <nl> RefPicList * refPicList = s -> ref -> refPicList ; <nl> - HEVCFrame * ref0 , * ref1 ; <nl> + HEVCFrame * ref0 = NULL , * ref1 = NULL ; <nl> uint8_t * dst0 = POS ( 0 , x0 , y0 ); <nl> uint8_t * dst1 = POS ( 1 , x0 , y0 ); <nl> uint8_t * dst2 = POS ( 2 , x0 , y0 );
static av_cold int mpc8_decode_init ( AVCodecContext * avctx ) <nl> c -> frames = 1 << ( get_bits (& gb , 3 ) * 2 ); <nl>  <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ; <nl> - avctx -> channel_layout = ( avctx -> channels == 2 ) ? AV_CH_LAYOUT_STEREO : AV_CH_LAYOUT_MONO ; <nl> + avctx -> channel_layout = ( channels == 2 ) ? AV_CH_LAYOUT_STEREO : AV_CH_LAYOUT_MONO ; <nl> + avctx -> channels = channels ; <nl>  <nl> if ( vlc_initialized ) return 0 ; <nl> av_log ( avctx , AV_LOG_DEBUG , " Initing VLC \ n ");
static void mxf_free_metadataset ( MXFMetadataSet ** ctx , int freectx ) <nl> case MaterialPackage : <nl> av_freep (&(( MXFPackage *)* ctx )-> tracks_refs ); <nl> av_freep (&(( MXFPackage *)* ctx )-> name ); <nl> + av_freep (&(( MXFPackage *)* ctx )-> comment_refs ); <nl> break ; <nl> case TaggedValue : <nl> av_freep (&(( MXFTaggedValue *)* ctx )-> name );
static int mxf_read_close ( AVFormatContext * s ) <nl>  <nl> for ( i = 0 ; i < mxf -> nb_index_tables ; i ++) { <nl> av_freep (& mxf -> index_tables [ i ]. segments ); <nl> + av_freep (& mxf -> index_tables [ i ]. ptses ); <nl> av_freep (& mxf -> index_tables [ i ]. fake_index ); <nl> } <nl> av_freep (& mxf -> index_tables );
static int mpeg_mux_write_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> stream -> next_packet = & stream -> premux_packet ; <nl> * stream -> next_packet = <nl> pkt_desc = av_mallocz ( sizeof ( PacketDesc )); <nl> + if (! pkt_desc ) <nl> + return AVERROR ( ENOMEM ); <nl> pkt_desc -> pts = pts ; <nl> pkt_desc -> dts = dts ; <nl> pkt_desc -> unwritten_size =
int main ( int argc , char ** argv ) <nl> goto end ; <nl> } <nl> w_name = av_strtok ( print_format , "=", & buf ); <nl> + if (! w_name ) { <nl> + av_log ( NULL , AV_LOG_ERROR , <nl> + " No name specified for the output format \ n "); <nl> + ret = AVERROR ( EINVAL ); <nl> + goto end ; <nl> + } <nl> w_args = buf ; <nl>  <nl> if ( show_data_hash ) {
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> } <nl>  <nl> if ( S ) { <nl> - S <<= s -> float_shift ; <nl> + S *= 1 << s -> float_shift ; <nl> sign = S < 0 ; <nl> if ( sign ) <nl> S = - S ;
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> url_fseek ( s -> pb , st -> index_entries [ index ]. pos , SEEK_SET ); <nl> matroska -> skip_to_keyframe = !( flags & AVSEEK_FLAG_ANY ); <nl> matroska -> skip_to_stream = st ; <nl> + matroska -> done = 0 ; <nl> av_update_cur_dts ( s , st , st -> index_entries [ index ]. timestamp ); <nl> return 0 ; <nl> }
static int16_t g726_decode ( G726Context * c , int I ) <nl> c -> se += mult ( i2f ( c -> a [ i ] >> 2 , & f ), & c -> sr [ i ]); <nl> c -> se >>= 1 ; <nl>  <nl> - return av_clip ( re_signal << 2 , - 0xffff , 0xffff ); <nl> + return av_clip ( re_signal * 4 , - 0xffff , 0xffff ); <nl> } <nl>  <nl> static av_cold int g726_reset ( G726Context * c )
fixup_vorbis_headers ( AVFormatContext * as , struct oggvorbis_private * priv , <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> memcpy (& ptr [ offset ], priv -> packet [ i ], priv -> len [ i ]); <nl> offset += priv -> len [ i ]; <nl> + av_freep (& priv -> packet [ i ]); <nl> } <nl> * buf = av_realloc (* buf , offset + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> return offset ;
static int mov_text_decode_close ( AVCodecContext * avctx ) <nl> { <nl> MovTextContext * m = avctx -> priv_data ; <nl> mov_text_cleanup_ftab ( m ); <nl> + mov_text_cleanup ( m ); <nl> return 0 ; <nl> } <nl> 
static inline int mdec_decode_block_intra ( MDECContext * a , int16_t * block , int n ) <nl> if ( diff >= 0xffff ) <nl> return AVERROR_INVALIDDATA ; <nl> a -> last_dc [ component ] += diff ; <nl> - block [ 0 ] = a -> last_dc [ component ] << 3 ; <nl> + block [ 0 ] = a -> last_dc [ component ] * ( 1 << 3 ); <nl> } <nl>  <nl> i = 0 ;
static int hls_write_header ( AVFormatContext * s ) <nl> int ret , i ; <nl> char * p ; <nl> const char * pattern = "% d . ts "; <nl> - int basename_size = strlen ( s -> filename ) + strlen ( pattern ); <nl> + int basename_size = strlen ( s -> filename ) + strlen ( pattern ) + 1 ; <nl>  <nl> hls -> number = 0 ; <nl> 
 <nl> static av_cold int encode_init ( AVCodecContext * avctx ) <nl> { <nl> + if ( avctx -> width > 65535 || avctx -> height > 65535 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " Unsupported resolution % dx % d .\ n ", avctx -> width , avctx -> height ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> avctx -> coded_frame = av_frame_alloc (); <nl> if (! avctx -> coded_frame ) <nl> return AVERROR ( ENOMEM );
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> return - 1 ; <nl> } <nl>  <nl> + if ( wc -> ch_offset >= avctx -> channels ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " too many channels \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> memset ( s -> decorr , 0 , MAX_TERMS * sizeof ( Decorr )); <nl> memset ( s -> ch , 0 , sizeof ( s -> ch )); <nl> s -> extra_bits = 0 ;
static int submit_packet ( PerThreadContext * p , AVPacket * avpkt ) <nl> } <nl>  <nl> fctx -> prev_thread = p ; <nl> + fctx -> next_decoding ++; <nl>  <nl> return 0 ; <nl> } <nl> int ff_thread_decode_frame ( AVCodecContext * avctx , <nl> err = submit_packet ( p , avpkt ); <nl> if ( err ) return err ; <nl>  <nl> - fctx -> next_decoding ++; <nl> - <nl> /* <nl> * If we ' re still receiving the initial packets , don ' t return a frame . <nl> */
int av_packet_unpack_dictionary ( const uint8_t * data , int size , AVDictionary ** di <nl> const uint8_t * key = data ; <nl> const uint8_t * val = data + strlen ( key ) + 1 ; <nl>  <nl> - if ( val >= end ) <nl> + if ( val >= end || !* key ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> ret = av_dict_set ( dict , key , val , 0 );
static int smka_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " channels mismatch \ n "); <nl> return AVERROR ( EINVAL ); <nl> } <nl> - if ( bits && avctx -> sample_fmt == AV_SAMPLE_FMT_U8 ) { <nl> + if ( bits == ( avctx -> sample_fmt == AV_SAMPLE_FMT_U8 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " sample format mismatch \ n "); <nl> return AVERROR ( EINVAL ); <nl> }
static av_cold int wmv2_encode_init ( AVCodecContext * avctx ){ <nl> ff_wmv2_common_init ( w ); <nl>  <nl> avctx -> extradata_size = 4 ; <nl> - avctx -> extradata = av_mallocz ( avctx -> extradata_size + 10 ); <nl> + avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! avctx -> extradata ) <nl> + return AVERROR ( ENOMEM ); <nl> encode_ext_header ( w ); <nl>  <nl> return 0 ;
static attribute_align_arg void * frame_worker_thread ( void * arg ) <nl>  <nl> pthread_mutex_lock (& p -> progress_mutex ); <nl> for ( i = 0 ; i < MAX_BUFFERS ; i ++) <nl> - if ( p -> progress_used [ i ]) { <nl> + if ( p -> progress_used [ i ] && ( p -> got_frame || p -> result < 0 || avctx -> codec_id != CODEC_ID_H264 )) { <nl> p -> progress [ i ][ 0 ] = INT_MAX ; <nl> p -> progress [ i ][ 1 ] = INT_MAX ; <nl> }
static int decode_ref_pic_list_reordering ( H264Context * h ){ <nl> const unsigned int abs_diff_pic_num = get_ue_golomb (& s -> gb ) + 1 ; <nl> int frame_num ; <nl>  <nl> - if ( abs_diff_pic_num >= h -> max_pic_num ){ <nl> + if ( abs_diff_pic_num > h -> max_pic_num ){ <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " abs_diff_pic_num overflow \ n "); <nl> return - 1 ; <nl> }
static void pkt_dump_internal ( void * avcl , FILE * f , int level , const AVPacket * pk <nl> HEXDUMP_PRINT ("\ n "); <nl> HEXDUMP_PRINT (" size =% d \ n ", pkt -> size ); <nl> if ( dump_payload ) <nl> - av_hex_dump ( f , pkt -> data , pkt -> size ); <nl> + hex_dump_internal ( avcl , f , level , pkt -> data , pkt -> size ); <nl> } <nl>  <nl> void av_pkt_dump2 ( FILE * f , const AVPacket * pkt , int dump_payload , const AVStream * st )
int av_packet_ref ( AVPacket * dst , const AVPacket * src ) <nl> if ( ret < 0 ) <nl> goto fail ; <nl> memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl> - } else <nl> + } else { <nl> dst -> buf = av_buffer_ref ( src -> buf ); <nl> + if (! dst -> buf ) <nl> + goto fail ; <nl> + } <nl>  <nl> dst -> size = src -> size ; <nl> dst -> data = dst -> buf -> data ;
static int alac_decode_frame ( AVCodecContext * avctx , void * data , <nl> avpkt -> size * 8 - get_bits_count (& alac -> gb )); <nl> } <nl>  <nl> - * got_frame_ptr = 1 ; <nl> + if ( alac -> channels == ch ) <nl> + * got_frame_ptr = 1 ; <nl>  <nl> return avpkt -> size ; <nl> }
static int config_props ( AVFilterLink * outlink ) <nl>  <nl> if (( ret = sws_init_context (* s , NULL , NULL )) < 0 ) <nl> return ret ; <nl> + if (! scale -> interlaced ) <nl> + break ; <nl> } <nl> } <nl> 
static int decode_frame_mp3on4 ( AVCodecContext * avctx , void * data , <nl>  <nl> avpriv_mpegaudio_decode_header (( MPADecodeHeader *) m , header ); <nl>  <nl> - if ( ch + m -> nb_channels > avctx -> channels ) { <nl> + if ( ch + m -> nb_channels > avctx -> channels || <nl> + s -> coff [ fr ] + m -> nb_channels > avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame channel count exceeds codec " <nl> " channel count \ n "); <nl> return AVERROR_INVALIDDATA ;
static int rv20_decode_picture_header ( RVDecContext * rv ) <nl> return - 1 ; <nl> } <nl>  <nl> + if ( s -> low_delay && s -> pict_type == AV_PICTURE_TYPE_B ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " low delay B \ n "); <nl> + return - 1 ; <nl> + } <nl> if ( s -> last_picture_ptr == NULL && s -> pict_type == AV_PICTURE_TYPE_B ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " early B pix \ n "); <nl> return - 1 ;
int ff_pcm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int ret , size ; <nl>  <nl> size = RAW_SAMPLES * s -> streams [ 0 ]-> codec -> block_align ; <nl> + if ( size <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> ret = av_get_packet ( s -> pb , pkt , size ); <nl> 
static int Stagefright_decode_frame ( AVCodecContext * avctx , void * data , <nl> frame -> size = orig_size ; <nl> } <nl> memcpy ( frame -> buffer , ptr , orig_size ); <nl> + if ( avpkt == & pkt ) <nl> + av_free ( avpkt -> data ); <nl>  <nl> frame -> time = ++ s -> frame_index ; <nl> (* s -> ts_map )[ s -> frame_index ]. pts = avpkt -> pts ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl> } <nl>  <nl> + if ( avctx -> slices > 1 && <nl> + ( avctx -> codec_id == AV_CODEC_ID_FLV1 || avctx -> codec_id == AV_CODEC_ID_H261 )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Multiple slices are not supported by this codec \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> if ( s -> avctx -> thread_count > 1 && <nl> s -> codec_id != AV_CODEC_ID_MPEG4 && <nl> s -> codec_id != AV_CODEC_ID_MPEG1VIDEO &&
static int flashsv2_prime ( FlashSVContext * s , uint8_t * src , int size ) <nl> z_stream zs ; <nl> int zret ; // Zlib return code <nl>  <nl> + if (! src ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> zs . zalloc = NULL ; <nl> zs . zfree = NULL ; <nl> zs . opaque = NULL ;
static int extract_extradata_h2645 ( AVBSFContext * ctx , AVPacket * pkt , <nl> ret = ff_h2645_packet_split (& h2645_pkt , pkt -> data , pkt -> size , <nl> ctx , 0 , 0 , ctx -> par_in -> codec_id , 1 ); <nl> if ( ret < 0 ) <nl> - return ret ; <nl> + goto fail ; <nl>  <nl> for ( i = 0 ; i < h2645_pkt . nb_nals ; i ++) { <nl> H2645NAL * nal = & h2645_pkt . nals [ i ];
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> goto fail ; <nl>  <nl> s -> cbf_luma = av_malloc ( sps -> min_tb_width * sps -> min_tb_height ); <nl> - s -> tab_ipm = av_malloc ( min_pu_size ); <nl> + s -> tab_ipm = av_mallocz ( min_pu_size ); <nl> s -> is_pcm = av_malloc ( min_pu_size ); <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ;
int ff_dwt_encode ( DWTContext * s , void * t ) <nl>  <nl> int ff_dwt_decode ( DWTContext * s , void * t ) <nl> { <nl> + if ( s -> ndeclevels == 0 ) <nl> + return 0 ; <nl> + <nl> switch ( s -> type ) { <nl> case FF_DWT97 : <nl> dwt_decode97_float ( s , t );
static int mjpeg_decode_scan ( MJpegDecodeContext * s , int nb_components , int Ah , <nl> uint8_t * data [ MAX_COMPONENTS ]; <nl> const uint8_t * reference_data [ MAX_COMPONENTS ]; <nl> int linesize [ MAX_COMPONENTS ]; <nl> - GetBitContext mb_bitmask_gb ; <nl> + GetBitContext mb_bitmask_gb = { 0 }; // initialize to silence gcc warning <nl> int bytes_per_pixel = 1 + ( s -> bits > 8 ); <nl>  <nl> if ( mb_bitmask ) {
# include < stdlib . h > <nl> # include < stdio . h > <nl>  <nl> +# include " libavutil / common . h " <nl> # include " libavformat / avformat . h " <nl>  <nl> # undef exit <nl> int main ( int argc , char ** argv ) <nl>  <nl> for ( i = 0 ; ; i ++){ <nl> AVPacket pkt ; <nl> - AVStream * st ; <nl> + AVStream * av_uninit ( st ); <nl>  <nl> memset (& pkt , 0 , sizeof ( pkt )); <nl> if ( ret >= 0 ){
int avpriv_mpeg4audio_get_config ( MPEG4AudioConfig * c , const uint8_t * buf , <nl> GetBitContext gb ; <nl> int specific_config_bitindex ; <nl>  <nl> + if ( bit_size <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> init_get_bits (& gb , buf , bit_size ); <nl> c -> object_type = get_object_type (& gb ); <nl> c -> sample_rate = get_sample_rate (& gb , & c -> sampling_index );
static int msrle_decode_pal4 ( AVCodecContext * avctx , AVPicture * pic , <nl> unsigned int pixel_ptr = 0 ; <nl> int row_dec = pic -> linesize [ 0 ]; <nl> int row_ptr = ( avctx -> height - 1 ) * row_dec ; <nl> - int frame_size = row_dec * avctx -> height ; <nl> + int frame_size = FFABS ( row_dec ) * avctx -> height ; <nl> int i ; <nl>  <nl> while ( row_ptr >= 0 ) {
static int parse_tonal ( DCALbrDecoder * s , int group ) <nl> break ; // End of subframe <nl>  <nl> freq += diff - 2 ; <nl> - if ( freq >> ( 5 - group ) > s -> nsubbands * 4 - 5 ) { <nl> + if ( freq >> ( 5 - group ) > s -> nsubbands * 4 - 6 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid spectral line offset \ n "); <nl> return - 1 ; <nl> }
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> } else { <nl> pkt = add_to_pktbuf (& ic -> packet_buffer , & pkt1 , <nl> & ic -> packet_buffer_end ); <nl> + if (! pkt ) <nl> + goto find_stream_info_err ; <nl> if (( ret = av_dup_packet ( pkt )) < 0 ) <nl> goto find_stream_info_err ; <nl> }
static enum AVPixelFormat get_format ( HEVCContext * s , const HEVCSPS * sps ) <nl> * fmt ++ = sps -> pix_fmt ; <nl> * fmt = AV_PIX_FMT_NONE ; <nl>  <nl> - return ff_get_format ( s -> avctx , pix_fmts ); <nl> + return ff_thread_get_format ( s -> avctx , pix_fmts ); <nl> } <nl>  <nl> static int set_sps ( HEVCContext * s , const HEVCSPS * sps ,
int av_frame_copy ( AVFrame * dst , const AVFrame * src ) <nl>  <nl> if ( dst -> width > 0 && dst -> height > 0 ) <nl> return frame_copy_video ( dst , src ); <nl> - else if ( dst -> nb_samples > 0 && dst -> channel_layout ) <nl> + else if ( dst -> nb_samples > 0 && dst -> channels > 0 ) <nl> return frame_copy_audio ( dst , src ); <nl>  <nl> return AVERROR ( EINVAL );
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> ret = ff_vp8_decode_frame ( avctx , p , got_frame , & pkt ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + <nl> + update_canvas_size ( avctx , avctx -> width , avctx -> height ); <nl> + <nl> if ( s -> has_alpha ) { <nl> ret = vp8_lossy_decode_alpha ( avctx , p , s -> alpha_data , <nl> s -> alpha_data_size );
int ff_h2645_packet_split ( H2645Packet * pkt , const uint8_t * buf , int length , <nl> ret = hevc_parse_nal_header ( nal , logctx ); <nl> else <nl> ret = h264_parse_nal_header ( nal , logctx ); <nl> - if ( ret <= 0 ) { <nl> + if ( ret <= 0 || nal -> size <= 0 ) { <nl> if ( ret < 0 ) { <nl> av_log ( logctx , AV_LOG_ERROR , " Invalid NAL unit % d , skipping .\ n ", <nl> nal -> type );
static av_cold int common_init ( AVCodecContext * avctx ){ <nl> s -> avctx = avctx ; <nl> s -> flags = avctx -> flags ; <nl>  <nl> + avcodec_get_frame_defaults (& s -> picture ); <nl> + <nl> dsputil_init (& s -> dsp , avctx ); <nl>  <nl> s -> width = avctx -> width ;
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> INC_MED ( 2 ); <nl> } <nl> if (! c -> error_limit ) { <nl> + if ( add >= 0x2000000U ) { <nl> + av_log ( ctx -> avctx , AV_LOG_ERROR , " k % d is too large \ n ", add ); <nl> + goto error ; <nl> + } <nl> ret = base + get_tail ( gb , add ); <nl> if ( get_bits_left ( gb ) <= 0 ) <nl> goto error ;
again : <nl>  <nl> if ( avctx -> bits_per_raw_sample != h -> sps . bit_depth_luma || <nl> h -> cur_chroma_format_idc != h -> sps . chroma_format_idc ) { <nl> - if ( s -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU <nl> + if ( s -> avctx -> codec && <nl> + s -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU <nl> && ( h -> sps . bit_depth_luma != 8 || <nl> h -> sps . chroma_format_idc > 1 )) { <nl> av_log ( avctx , AV_LOG_ERROR ,
static av_cold int truemotion1_decode_end ( AVCodecContext * avctx ) <nl> TrueMotion1Context * s = avctx -> priv_data ; <nl>  <nl> av_frame_unref (& s -> frame ); <nl> - av_free ( s -> vert_pred ); <nl> + av_freep (& s -> vert_pred ); <nl>  <nl> return 0 ; <nl> }
static const AVClass writer_class = { <nl>  <nl> static void writer_close ( WriterContext ** wctx ) <nl> { <nl> - if (* wctx && (* wctx )-> writer -> uninit ) <nl> - (* wctx )-> writer -> uninit (* wctx ); <nl> + if (!* wctx ) <nl> + return ; <nl>  <nl> + if ((* wctx )-> writer -> uninit ) <nl> + (* wctx )-> writer -> uninit (* wctx ); <nl> av_freep (&((* wctx )-> priv )); <nl> av_freep ( wctx ); <nl> }
static void get_private_data ( OutputStream * os ) <nl> return ; <nl> os -> private_str = av_mallocz ( 2 * size + 1 ); <nl> if (! os -> private_str ) <nl> - return ; <nl> + goto fail ; <nl> for ( i = 0 ; i < size ; i ++) <nl> snprintf (& os -> private_str [ 2 * i ], 3 , "% 02x ", ptr [ i ]); <nl> + fail : <nl> if ( ptr != codec -> extradata ) <nl> av_free ( ptr ); <nl> }
static int decode_subframe ( WmallDecodeCtx * s ) <nl> else <nl> use_normal_update_speed ( s , i ); <nl> revert_cdlms ( s , i , 0 , subframe_len ); <nl> - } <nl> + } else <nl> + memset ( s -> channel_residues , 0 , sizeof ( s -> channel_residues )); <nl> } <nl> if ( s -> do_mclms ) <nl> revert_mclms ( s , subframe_len );
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl>  <nl> x265pic . pts = pic -> pts ; <nl> x265pic . bitDepth = av_pix_fmt_desc_get ( avctx -> pix_fmt )-> comp [ 0 ]. depth_minus1 + 1 ; <nl> + <nl> + x265pic . sliceType = pic -> pict_type == AV_PICTURE_TYPE_I ? X265_TYPE_I : <nl> + pic -> pict_type == AV_PICTURE_TYPE_P ? X265_TYPE_P : <nl> + pic -> pict_type == AV_PICTURE_TYPE_B ? X265_TYPE_B : <nl> + X265_TYPE_AUTO ; <nl> } <nl>  <nl> ret = x265_encoder_encode ( ctx -> encoder , & nal , & nnal ,
static int decode_init_thread_copy ( AVCodecContext * avctx ) <nl> memset ( h -> sps_buffers , 0 , sizeof ( h -> sps_buffers )); <nl> memset ( h -> pps_buffers , 0 , sizeof ( h -> pps_buffers )); <nl>  <nl> + h -> avctx = avctx ; <nl> h -> rbsp_buffer [ 0 ] = NULL ; <nl> h -> rbsp_buffer [ 1 ] = NULL ; <nl> h -> rbsp_buffer_size [ 0 ] = 0 ;
int ff_msmpeg4_decode_block ( MpegEncContext * s , int16_t * block , <nl> if ( level < 0 ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " dc overflow - block : % d qscale : % d //\ n ", n , s -> qscale ); <nl> if ( s -> inter_intra_pred ) level = 0 ; <nl> - else return - 1 ; <nl> } <nl> if ( n < 4 ) { <nl> rl = & ff_rl_table [ s -> rl_table_index ];
int av_image_check_sar ( unsigned int w , unsigned int h , AVRational sar ) <nl> { <nl> int64_t scaled_dim ; <nl>  <nl> - if (! sar . den ) <nl> + if ( sar . den <= 0 || sar . num < 0 ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> if (! sar . num || sar . num == sar . den )
static int parse_pixel_format ( AVCodecContext * avctx ) <nl> normal_map = flags & DDPF_NORMALMAP ; <nl> fourcc = bytestream2_get_le32 ( gbc ); <nl>  <nl> + if ( ctx -> compressed && ctx -> paletted ) { <nl> + av_log ( avctx , AV_LOG_WARNING , <nl> + " Disabling invalid palette flag for compressed dds .\ n "); <nl> + ctx -> paletted = 0 ; <nl> + } <nl> + <nl> bpp = bytestream2_get_le32 ( gbc ); // rgbbitcount <nl> r = bytestream2_get_le32 ( gbc ); // rbitmask <nl> g = bytestream2_get_le32 ( gbc ); // gbitmask
static int rtcp_parse_packet ( RTPDemuxContext * s , const unsigned char * buf , int l <nl> while ( len >= 2 ) { <nl> switch ( buf [ 1 ]) { <nl> case RTCP_SR : <nl> - if ( len < 16 ) { <nl> + if ( len < 20 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid length for RTCP SR packet \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> switch ( type ) { <nl> case AVS_PALETTE : <nl> + if ( size - 4 > sizeof ( palette )) <nl> + return AVERROR_INVALIDDATA ; <nl> ret = avio_read ( s -> pb , palette , size - 4 ); <nl> if ( ret < size - 4 ) <nl> return AVERROR ( EIO );
static int pcm_read_header ( AVFormatContext * s ) <nl> av_log ( s , AV_LOG_ERROR , <nl> " Invalid sample_rate found in mime_type \"% s \"\ n ", <nl> mime_type ); <nl> + av_freep (& mime_type ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> st -> codecpar -> sample_rate = rate ; <nl> static int pcm_read_header ( AVFormatContext * s ) <nl> st -> codecpar -> channels = channels ; <nl> } <nl> } <nl> + av_freep (& mime_type ); <nl>  <nl> st -> codecpar -> bits_per_coded_sample = <nl> av_get_bits_per_sample ( st -> codecpar -> codec_id );
static void start_children ( FFServerStream * feed ) <nl> feed -> pid = fork (); <nl> if ( feed -> pid < 0 ) { <nl> http_log (" Unable to create children : % s \ n ", strerror ( errno )); <nl> + av_free ( pathname ); <nl> exit ( EXIT_FAILURE ); <nl> } <nl> 
static int decode_studio_vop_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> return 0 ; <nl>  <nl> s -> partitioned_frame = 0 ; <nl> + s -> interlaced_dct = 0 ; <nl> s -> decode_mb = mpeg4_decode_studio_mb ; <nl>  <nl> decode_smpte_tc ( ctx , gb );
int ff_raw_audio_read_header ( AVFormatContext * s , <nl> st -> codec -> codec_type = AVMEDIA_TYPE_AUDIO ; <nl> st -> codec -> codec_id = s -> iformat -> value ; <nl> st -> need_parsing = AVSTREAM_PARSE_FULL ; <nl> + st -> start_time = 0 ; <nl> /* the parameters will be extracted from the compressed bitstream */ <nl>  <nl> return 0 ;
static void smc_decode_stream ( SmcContext * s ) <nl> row_ptr , image_size ); <nl> return ; <nl> } <nl> + if ( bytestream2_get_bytes_left (& s -> gb ) < 1 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " input too small \ n "); <nl> + return ; <nl> + } <nl>  <nl> opcode = bytestream2_get_byte (& s -> gb ); <nl> switch ( opcode & 0xF0 ) {
static int img_read_header ( AVFormatContext * s1 ) <nl> s -> img_last = last_index ; <nl> s -> img_number = first_index ; <nl> /* compute duration */ <nl> - st -> start_time = 0 ; <nl> - st -> duration = last_index - first_index + 1 ; <nl> + if (! s -> ts_from_file ) { <nl> + st -> start_time = 0 ; <nl> + st -> duration = last_index - first_index + 1 ; <nl> + } <nl> } <nl>  <nl> if ( s1 -> video_codec_id ) {
static int mxf_read_seek ( AVFormatContext * s , int stream_index , int64_t sample_ti <nl> sample_time = FFMIN ( sample_time , source_track -> original_duration - 1 ); <nl> } <nl>  <nl> - if (( ret = mxf_edit_unit_absolute_offset ( mxf , t , sample_time , & sample_time , & seekpos , 1 )) << 0 ) <nl> + if (( ret = mxf_edit_unit_absolute_offset ( mxf , t , sample_time , & sample_time , & seekpos , 1 )) < 0 ) <nl> return ret ; <nl>  <nl> ff_update_cur_dts ( s , st , sample_time );
# ifndef AVUTIL_MEM_INTERNAL_H <nl> # define AVUTIL_MEM_INTERNAL_H <nl>  <nl> +# include " avassert . h " <nl> +# include " mem . h " <nl> + <nl> static inline int ff_fast_malloc ( void * ptr , unsigned int * size , size_t min_size , int zero_realloc ) <nl> { <nl> void * val ;
static inline int wv_unpack_stereo ( WavpackFrameContext * s , GetBitContext * gb , <nl> } <nl>  <nl> if ( type == AV_SAMPLE_FMT_S16P ) { <nl> - if ( FFABS ( L ) + FFABS ( R ) > ( 1 << 19 )) { <nl> + if ( FFABS ( L ) + ( unsigned ) FFABS ( R ) > ( 1 << 19 )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " sample % d % d too large \ n ", L , R ); <nl> return AVERROR_INVALIDDATA ; <nl> }
the_end : <nl> } <nl> } <nl> } <nl> - if ( s -> flipped ) { <nl> + if ( s -> flipped && ! s -> rgb ) { <nl> int j ; <nl> avcodec_get_chroma_sub_sample ( s -> avctx -> pix_fmt , & hshift , & vshift ); <nl> av_assert0 ( s -> nb_components == av_pix_fmt_count_planes ( s -> picture_ptr -> format ));
static int mxf_parse_physical_source_package ( MXFContext * mxf , MXFTrack * source_t <nl> break ; <nl>  <nl> /* the name of physical source package is name of the reel or tape */ <nl> - if ( physical_package -> name [ 0 ]) <nl> + if ( physical_package -> name && physical_package -> name [ 0 ]) <nl> av_dict_set (& st -> metadata , " reel_name ", physical_package -> name , 0 ); <nl>  <nl> /* the source timecode is calculated by adding the start_position of the sourceclip from the file source package track
static int dca_decode_frame ( AVCodecContext * avctx , void * data , <nl> } else { <nl> s -> channel_order_tab = dca_channel_reorder_nolfe_xch [ s -> amode ]; <nl> } <nl> + if ( s -> channel_order_tab [ s -> xch_base_channel ] < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> } else { <nl> channels = num_core_channels + !! s -> lfe ; <nl> s -> xch_present = 0 ; /* disable further xch processing */
int opt_default ( void * optctx , const char * opt , const char * arg ) <nl> # endif <nl> # if CONFIG_AVRESAMPLE <nl> rc_class = avresample_get_class (); <nl> - if ( av_opt_find (& rc_class , opt , NULL , 0 , <nl> - AV_OPT_SEARCH_CHILDREN | AV_OPT_SEARCH_FAKE_OBJ )) { <nl> + if (( o = av_opt_find (& rc_class , opt , NULL , 0 , <nl> + AV_OPT_SEARCH_CHILDREN | AV_OPT_SEARCH_FAKE_OBJ ))) { <nl> av_dict_set (& resample_opts , opt , arg , FLAGS ); <nl> consumed = 1 ; <nl> }
static int ogg_build_opus_headers ( AVCodecContext * avctx , <nl> static int ogg_write_header ( AVFormatContext * s ) <nl> { <nl> OGGContext * ogg = s -> priv_data ; <nl> - OGGStreamContext * oggstream ; <nl> + OGGStreamContext * oggstream = NULL ; <nl> int i , j ; <nl>  <nl> if ( ogg -> pref_size )
static int hls_decode_entry_wpp ( AVCodecContext * avctxt , void * input_ctb_row , int <nl>  <nl> if ( more_data < 0 ) { <nl> s -> tab_slice_address [ ctb_addr_rs ] = - 1 ; <nl> + avpriv_atomic_int_set (& s1 -> wpp_err , 1 ); <nl> + ff_thread_report_progress2 ( s -> avctx , ctb_row , thread , SHIFT_CTB_WPP ); <nl> return more_data ; <nl> } <nl> 
static int encode_frame ( AVCodecContext * avctx , unsigned char * buf , <nl> return - 1 ; <nl> } <nl> if (! is_yuv ) <nl> - s -> bpp_tab_size = ( s -> bpp >> 3 ); <nl> + s -> bpp_tab_size = (( s -> bpp + 7 ) >> 3 ); <nl>  <nl> if ( s -> compr == TIFF_DEFLATE || s -> compr == TIFF_ADOBE_DEFLATE || s -> compr == TIFF_LZW ) <nl> // best choose for DEFLATE
static int tgv_decode_frame ( AVCodecContext * avctx , <nl> frame -> pict_type = AV_PICTURE_TYPE_I ; <nl>  <nl> if (! s -> frame_buffer && <nl> - !( s -> frame_buffer = av_malloc ( s -> width * s -> height ))) <nl> + !( s -> frame_buffer = av_mallocz ( s -> width * s -> height ))) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> if ( unpack ( buf , buf_end , s -> frame_buffer , s -> avctx -> width , s -> avctx -> height ) < 0 ) {
static int read_interval_packets ( WriterContext * w , AVFormatContext * fmt_ctx , <nl> AVPacket pkt , pkt1 ; <nl> AVFrame frame ; <nl> int ret = 0 , i = 0 , frame_count = 0 ; <nl> - int64_t start , end = interval -> end ; <nl> + int64_t start = - INT64_MAX , end = interval -> end ; <nl> int has_start = 0 , has_end = interval -> has_end && ! interval -> end_is_offset ; <nl>  <nl> av_init_packet (& pkt );
static int cache_read ( URLContext * h , unsigned char * buf , int size ) <nl> { <nl> Context * c = h -> priv_data ; <nl> CacheEntry * entry , * next [ 2 ] = { NULL , NULL }; <nl> - int r ; <nl> + int64_t r ; <nl>  <nl> entry = av_tree_find ( c -> root , & c -> logical_pos , cmp , ( void **) next ); <nl> 
static int compat_decode ( AVCodecContext * avctx , AVFrame * frame , <nl> int * got_frame , const AVPacket * pkt ) <nl> { <nl> AVCodecInternal * avci = avctx -> internal ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> av_assert0 ( avci -> compat_decode_consumed == 0 ); <nl> 
static void ra144_encode_subblock ( RA144Context * ractx , <nl> float zero [ BLOCKSIZE ], cba [ BLOCKSIZE ], cb1 [ BLOCKSIZE ], cb2 [ BLOCKSIZE ]; <nl> int16_t cba_vect [ BLOCKSIZE ]; <nl> int cba_idx , cb1_idx , cb2_idx , gain ; <nl> - int i , n , m [ 3 ]; <nl> + int i , n ; <nl> + unsigned m [ 3 ]; <nl> float g [ 3 ]; <nl> float error , best_error ; <nl> 
int check_stream_specifier ( AVFormatContext * s , AVStream * st , const char * spec ) <nl> case ' a ': type = AVMEDIA_TYPE_AUDIO ; break ; <nl> case ' s ': type = AVMEDIA_TYPE_SUBTITLE ; break ; <nl> case ' d ': type = AVMEDIA_TYPE_DATA ; break ; <nl> + default : abort (); // never reached , silence warning <nl> } <nl> if ( type != st -> codec -> codec_type ) <nl> return 0 ;
static int xan_decode_frame ( AVCodecContext * avctx , <nl> } <nl> buf_size = buf_end - buf ; <nl> } <nl> + if ( s -> palettes_count <= 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " No palette found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = avctx -> get_buffer ( avctx , & s -> current_frame ))) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return ret ;
static int process_output_surface ( AVCodecContext * avctx , AVPacket * pkt , NvencSur <nl> } <nl> slice_offsets = av_mallocz ( slice_mode_data * sizeof (* slice_offsets )); <nl>  <nl> - if (! slice_offsets ) <nl> + if (! slice_offsets ) { <nl> + res = AVERROR ( ENOMEM ); <nl> goto error ; <nl> + } <nl>  <nl> lock_params . version = NV_ENC_LOCK_BITSTREAM_VER ; <nl> 
static int segment_mux_init ( AVFormatContext * s ) <nl> oc -> opaque = s -> opaque ; <nl> oc -> io_close = s -> io_close ; <nl> oc -> io_open = s -> io_open ; <nl> + oc -> flags = s -> flags ; <nl>  <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> AVStream * st ;
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> tag = avio_rl32 ( pb ); <nl> size = avio_rl32 ( pb ); <nl>  <nl> + if ( size > avi -> fsize ){ <nl> + av_log ( s , AV_LOG_ERROR , " chunk size is too big during header parsing \ n "); <nl> + goto fail ; <nl> + } <nl> + <nl> print_tag (" tag ", tag , size ); <nl>  <nl> switch ( tag ) {
static int gxf_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> } <nl>  <nl> static int gxf_seek ( AVFormatContext * s , int stream_index , int64_t timestamp , int flags ) { <nl> - int res = 0 ; <nl> + int64_t res = 0 ; <nl> uint64_t pos ; <nl> uint64_t maxlen = 100 * 1024 * 1024 ; <nl> AVStream * st = s -> streams [ 0 ];
static av_cold int read_specific_config ( ALSDecContext * ctx ) <nl> GetBitContext gb ; <nl> uint64_t ht_size ; <nl> int i , config_offset ; <nl> - MPEG4AudioConfig m4ac ; <nl> + MPEG4AudioConfig m4ac = { 0 }; <nl> ALSSpecificConfig * sconf = & ctx -> sconf ; <nl> AVCodecContext * avctx = ctx -> avctx ; <nl> uint32_t als_id , header_size , trailer_size ;
static int mpeg_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( avctx -> extradata && ! avctx -> frame_number ) { <nl> int ret = decode_chunks ( avctx , picture , data_size , avctx -> extradata , avctx -> extradata_size ); <nl> + * data_size = 0 ; <nl> if ( ret < 0 && ( avctx -> err_recognition & AV_EF_EXPLODE )) <nl> return ret ; <nl> }
static av_cold int init ( AVCodecContext * avctx ) <nl> int dummy_int ; <nl>  <nl> /* Back up the extradata so it can be restored at close time . */ <nl> - priv -> orig_extradata = av_malloc ( avctx -> extradata_size ); <nl> + priv -> orig_extradata = av_malloc ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! priv -> orig_extradata ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Failed to allocate copy of extradata \ n ");
static int read_header ( AVFormatContext * s ) <nl> avio_skip ( s -> pb , 1 ); // padding <nl>  <nl> st -> codec -> sample_rate = bfstm ? read32 ( s ) : read16 ( s ); <nl> - if (! st -> codec -> sample_rate ) <nl> + if ( st -> codec -> sample_rate <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (! bfstm )
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> continue ; <nl> } <nl> v -> second_field = 1 ; <nl> - v -> blocks_off = s -> mb_width * s -> mb_height << 1 ; <nl> + v -> blocks_off = s -> b8_stride * ( s -> mb_height &~ 1 ); <nl> v -> mb_off = s -> mb_stride * s -> mb_height >> 1 ; <nl> } else { <nl> v -> second_field = 0 ;
static int vqf_read_header ( AVFormatContext * s ) <nl> break ; <nl> default : <nl> st -> codec -> sample_rate = rate_flag * 1000 ; <nl> + if ( st -> codec -> sample_rate <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " sample rate % d is invalid \ n ", st -> codec -> sample_rate ); <nl> + return - 1 ; <nl> + } <nl> break ; <nl> } <nl> 
AV_WN32A ( v0 + i , r ); \ <nl> AV_WN32A ( v1 + i , r ); \ <nl> } \ <nl> - for ( i = 0 ; i < BUF_SIZE * 8 / 3 ; i += 4 ) { \ <nl> + for ( i = 0 ; i < width * 8 / 3 ; i += 4 ) { \ <nl> uint32_t r = rnd (); \ <nl> AV_WN32A ( dst0 + i , r ); \ <nl> AV_WN32A ( dst1 + i , r ); \
static void writer_close ( WriterContext ** wctx ) <nl> if ((* wctx )-> writer -> priv_class ) <nl> av_opt_free ((* wctx )-> priv ); <nl> av_freep (&((* wctx )-> priv )); <nl> + av_opt_free (* wctx ); <nl> av_freep ( wctx ); <nl> } <nl> 
static int read_braindead_odml_indx ( AVFormatContext * s , int frame_num ){ <nl> longs_pre_entry , index_type , entries_in_use , chunk_id , base ); <nl> # endif <nl>  <nl> - if ( stream_id > s -> nb_streams || stream_id < 0 ) <nl> + if ( stream_id >= s -> nb_streams || stream_id < 0 ) <nl> return - 1 ; <nl> st = s -> streams [ stream_id ]; <nl> ast = st -> priv_data ;
static int decode_fctl_chunk ( AVFormatContext * s , APNGDemuxContext * ctx , AVPacket <nl> static int apng_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> APNGDemuxContext * ctx = s -> priv_data ; <nl> - int ret ; <nl> + int64_t ret ; <nl> int64_t size ; <nl> AVIOContext * pb = s -> pb ; <nl> uint32_t len , tag ;
int av_packet_ref ( AVPacket * dst , const AVPacket * src ) <nl> ret = packet_alloc (& dst -> buf , src -> size ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl> - memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl> + if ( src -> size ) <nl> + memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl>  <nl> dst -> data = dst -> buf -> data ; <nl> } else {
int attribute_align_arg avcodec_decode_audio4 ( AVCodecContext * avctx , <nl> * extended_data are doing it correctly */ <nl> if (* got_frame_ptr ) { <nl> planar = av_sample_fmt_is_planar ( frame -> format ); <nl> - channels = av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> + channels = frame -> channels ; <nl> if (!( planar && channels > AV_NUM_DATA_POINTERS )) <nl> frame -> extended_data = frame -> data ; <nl> } else {
static int process_video_header_vp6 ( AVFormatContext * s ) <nl> avio_skip ( pb , 4 ); <nl> ea -> time_base . den = avio_rl32 ( pb ); <nl> ea -> time_base . num = avio_rl32 ( pb ); <nl> + if ( ea -> time_base . den <= 0 || ea -> time_base . num <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Timebase is invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> ea -> video_codec = AV_CODEC_ID_VP6 ; <nl>  <nl> return 1 ;
void ff_estimate_b_frame_motion ( MpegEncContext * s , <nl> score = fbmin ; <nl> type = MB_TYPE_BIDIR ; <nl> } <nl> - score = ( score * score + 128 * 256 )>> 16 ; <nl> + score = (( unsigned )( score * score + 128 * 256 ))>> 16 ; <nl> s -> mc_mb_var_sum += score ; <nl> s -> mc_mb_var [ mb_y * s -> mb_width + mb_x ] = score ; // FIXME use SSD <nl> }
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> return res ; <nl> av_assert1 ( block_duration != AV_NOPTS_VALUE ); <nl>  <nl> - block_time = AV_RB16 ( data ); <nl> + block_time = sign_extend ( AV_RB16 ( data ), 16 ); <nl> data += 2 ; <nl> flags = * data ++; <nl> size -= 3 ;
static int dca_decode_frame ( AVCodecContext * avctx , <nl> } else <nl> s -> channel_order_tab = dca_channel_reorder_nolfe [ s -> amode ]; <nl>  <nl> + if ( s -> prim_channels > 0 && <nl> + s -> channel_order_tab [ s -> prim_channels - 1 ] < 0 ) <nl> + return - 1 ; <nl> + <nl> if ( avctx -> request_channels == 2 && s -> prim_channels > 2 ) { <nl> channels = 2 ; <nl> s -> output = DCA_STEREO ;
static int a64multi_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> } else { <nl> /* fill up mc_meta_charset with data until lifetime exceeds */ <nl> if ( c -> mc_frame_counter < c -> mc_lifetime ) { <nl> - * p = * pict ; <nl> + ret = av_frame_ref ( p , pict ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> p -> key_frame = 1 ; <nl> to_meta_with_crop ( avctx , p , meta + 32000 * c -> mc_frame_counter );
int av_reallocp ( void * ptr , size_t size ) <nl> void ** ptrptr = ptr ; <nl> void * ret ; <nl>  <nl> + if (! size ) { <nl> + av_freep ( ptr ); <nl> + return 0 ; <nl> + } <nl> ret = av_realloc (* ptrptr , size ); <nl>  <nl> if (! ret ) {
av_cold static int auto_matrix ( SwrContext * s ) <nl>  <nl> memset ( s -> matrix , 0 , sizeof ( s -> matrix )); <nl> for ( i = 0 ; i < 64 ; i ++){ <nl> - if ( in_ch_layout & out_ch_layout & ( 1LL << i )) <nl> + if ( in_ch_layout & out_ch_layout & ( 1ULL << i )) <nl> matrix [ i ][ i ]= 1 . 0 ; <nl> } <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> h -> ref_count [ 1 ] = get_ue_golomb (& s -> gb ) + 1 ; <nl> else <nl> // full range is spec - ok in this case , even for frames <nl> - max [ 1 ] = 31 ; <nl> + h -> ref_count [ 1 ] = 1 ; <nl> } <nl>  <nl> if ( h -> ref_count [ 0 ]- 1 > max [ 0 ] || h -> ref_count [ 1 ]- 1 > max [ 1 ]){
AV_WN32A ( v0 + i , r ); \ <nl> AV_WN32A ( v1 + i , r ); \ <nl> } \ <nl> - for ( i = 0 ; i < BUF_SIZE * 8 / 3 ; i += 4 ) { \ <nl> + for ( i = 0 ; i < width * 8 / 3 ; i += 4 ) { \ <nl> uint32_t r = rnd (); \ <nl> AV_WN32A ( dst0 + i , r ); \ <nl> AV_WN32A ( dst1 + i , r ); \
int ff_ivi_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> avctx -> release_buffer ( avctx , & ctx -> frame ); <nl>  <nl> ctx -> frame . reference = 0 ; <nl> + avcodec_set_dimensions ( avctx , ctx -> planes [ 0 ]. width , ctx -> planes [ 0 ]. height ); <nl> if (( result = avctx -> get_buffer ( avctx , & ctx -> frame )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return result ;
static int ff_filter_frame_framed ( AVFilterLink * link , AVFilterBufferRef * frame ) <nl> } else <nl> out = frame ; <nl>  <nl> - while ( cmd && cmd -> time <= frame -> pts * av_q2d ( link -> time_base )){ <nl> + while ( cmd && cmd -> time <= out -> pts * av_q2d ( link -> time_base )){ <nl> av_log ( link -> dst , AV_LOG_DEBUG , <nl> " Processing command time :% f command :% s arg :% s \ n ", <nl> cmd -> time , cmd -> command , cmd -> arg );
static int get_pcm ( HEVCContext * s , int x , int y ) <nl>  <nl> # define TC_CALC ( qp , bs ) \ <nl> tctable [ av_clip (( qp ) + DEFAULT_INTRA_TC_OFFSET * (( bs ) - 1 ) + \ <nl> - ( tc_offset >> 1 << 1 ), \ <nl> + ( tc_offset & - 2 ), \ <nl> 0 , MAX_QP + DEFAULT_INTRA_TC_OFFSET )] <nl>  <nl> static void deblocking_filter_CTB ( HEVCContext * s , int x0 , int y0 )
static int mp3_write_xing ( AVFormatContext * s ) <nl> } <nl>  <nl> /* dummy MPEG audio header */ <nl> - header = 0xff << 24 ; // sync <nl> + header = 0xffU << 24 ; // sync <nl> header |= ( 0x7 << 5 | ver << 3 | 0x1 << 1 | 0x1 ) << 16 ; // sync / audio - version / layer 3 / no crc */ <nl> header |= ( srate_idx << 2 ) << 8 ; <nl> header |= channels << 6 ;
static int read_header ( FFV1Context * f ) <nl> } else { <nl> const uint8_t * p = c -> bytestream_end ; <nl> for ( f -> slice_count = 0 ; <nl> - f -> slice_count < MAX_SLICES && 3 < p - c -> bytestream_start ; <nl> + f -> slice_count < MAX_SLICES && 3 + 5 *!! f -> ec < p - c -> bytestream_start ; <nl> f -> slice_count ++) { <nl> int trailer = 3 + 5 *!! f -> ec ; <nl> int size = AV_RB24 ( p - trailer );
static int vid_probe ( AVProbeData * p ) <nl> if ( AV_RL32 ( p -> buf ) != MKTAG (' V ', ' I ', ' D ', 0 )) <nl> return 0 ; <nl>  <nl> + if ( p -> buf [ 4 ] != 2 ) <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> printf (" Demuxing audio from file '% s ' into '% s '\ n ", src_filename , audio_dst_filename ); <nl>  <nl> /* read frames from the file */ <nl> - while ( av_read_frame ( fmt_ctx , & pkt ) >= 0 ) <nl> + while ( av_read_frame ( fmt_ctx , & pkt ) >= 0 ) { <nl> decode_packet (& got_frame , 0 ); <nl> + av_free_packet (& pkt ); <nl> + } <nl>  <nl> /* flush cached frames */ <nl> pkt . data = NULL ;
static int msrle_decode_frame ( AVCodecContext * avctx , <nl> uint8_t * buf = avpkt -> data + ( avctx -> height - 1 )* istride ; <nl> int i , j ; <nl>  <nl> + if ( linesize < 0 ) <nl> + return linesize ; <nl> + <nl> for ( i = 0 ; i < avctx -> height ; i ++) { <nl> if ( avctx -> bits_per_coded_sample == 4 ) { <nl> for ( j = 0 ; j < avctx -> width - 1 ; j += 2 ) {
static int mxf_read_header ( AVFormatContext * s ) <nl> } <nl> if ( res < 0 ) { <nl> av_log ( s , AV_LOG_ERROR , " error reading header metadata \ n "); <nl> - return res ; <nl> + ret = res ; <nl> + goto fail ; <nl> } <nl> break ; <nl> } else {
static int mp_decode_frame ( AVCodecContext * avctx , <nl> if ( sz == 0 ) <nl> goto end ; <nl>  <nl> + if ( mp -> max_codes_bits <= 0 ) <nl> + goto end ; <nl> if ( init_vlc (& mp -> vlc , mp -> max_codes_bits , mp -> codes_count , & mp -> codes [ 0 ]. size , sizeof ( HuffCode ), 1 , & mp -> codes [ 0 ]. code , sizeof ( HuffCode ), 4 , 0 )) <nl> goto end ; <nl> mp_decode_frame_helper ( mp , & gb );
static int vc1_decode_frame ( AVCodecContext * avctx , <nl> divider = find_next_marker ( buf , buf + buf_size ); <nl> if (( divider == ( buf + buf_size )) || AV_RB32 ( divider ) != VC1_CODE_FIELD ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Error in WVC1 interlaced frame \ n "); <nl> + av_free ( buf2 ); <nl> return - 1 ; <nl> } <nl> 
static int mxf_set_audio_pts ( MXFContext * mxf , AVCodecContext * codec , AVPacket * p <nl> { <nl> MXFTrack * track = mxf -> fc -> streams [ pkt -> stream_index ]-> priv_data ; <nl> pkt -> pts = track -> sample_count ; <nl> + if ( codec -> channels <= 0 || av_get_bits_per_sample ( codec -> codec_id ) <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl> track -> sample_count += pkt -> size / ( codec -> channels * av_get_bits_per_sample ( codec -> codec_id ) / 8 ); <nl> return 0 ; <nl> }
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> } else { <nl> frame -> key_frame = 0 ; <nl> frame -> pict_type = AV_PICTURE_TYPE_P ; <nl> + if ( c -> decomp_len < 2LL * (( c -> width + c -> bw - 1 ) / c -> bw ) * (( c -> height + c -> bh - 1 ) / c -> bh )) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( c -> decomp_len ) <nl> c -> decode_xor ( c ); <nl> }
static int ftp_status ( FTPContext * s , char ** line , const int response_codes []) <nl>  <nl> while (! code_found || dash ) { <nl> if (( err = ftp_get_line ( s , buf , sizeof ( buf ))) < 0 ) { <nl> - av_bprint_finalize (& line_buffer , NULL ); <nl> + if ( line ) <nl> + av_bprint_finalize (& line_buffer , NULL ); <nl> return err ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> for (; yq < slice_h && yq < h ; yq ++){ <nl> IDWTELEM * line = slice_buffer_get_line (& s -> sb , yq ); <nl> for ( x = 0 ; x < w ; x ++){ <nl> - line [ x ] <<= FRAC_BITS ; <nl> + line [ x ] *= 1 << FRAC_BITS ; <nl> } <nl> } <nl> }
int attribute_align_arg avcodec_decode_audio4 ( AVCodecContext * avctx , <nl> frame -> extended_buf = NULL ; <nl> frame -> nb_extended_buf = 0 ; <nl> } <nl> - } <nl> - <nl> - if ( ret < 0 && frame -> data [ 0 ]) <nl> + } else if ( frame -> data [ 0 ]) <nl> av_frame_unref ( frame ); <nl> } <nl> 
static int decode_vop_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> int time_incr , time_increment ; <nl> int64_t pts ; <nl>  <nl> + s -> mcsel = 0 ; <nl> s -> pict_type = get_bits ( gb , 2 ) + AV_PICTURE_TYPE_I ; /* pict type : I = 0 , P = 1 */ <nl> if ( s -> pict_type == AV_PICTURE_TYPE_B && s -> low_delay && <nl> ctx -> vol_control_parameters == 0 && !( s -> avctx -> flags & AV_CODEC_FLAG_LOW_DELAY )) {
static inline struct rgbvec lerp ( const struct rgbvec * v0 , const struct rgbvec * v <nl>  <nl> # define NEAR ( x ) (( int )(( x ) + . 5 )) <nl> # define PREV ( x ) (( int )( x )) <nl> -# define NEXT ( x ) (( int )( x ) + 1 ) <nl> +# define NEXT ( x ) ( FFMIN (( int )( x ) + 1 , lut3d -> lutsize - 1 )) <nl>  <nl> /** <nl> * Get the nearest defined point
static av_cold int init ( AVFilterContext * ctx , const char * args ) <nl>  <nl> aresample -> next_pts = AV_NOPTS_VALUE ; <nl> aresample -> swr = swr_alloc (); <nl> - if (! aresample -> swr ) <nl> - return AVERROR ( ENOMEM ); <nl> + if (! aresample -> swr ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl>  <nl> if ( args ) { <nl> char * ptr = argd , * token ;
int av_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , <nl> " Dropped corrupted packet ( stream = % d )\ n ", <nl> pkt -> stream_index ); <nl> + av_free_packet ( pkt ); <nl> continue ; <nl> } <nl> 
static int config_props ( AVFilterLink * outlink ) <nl>  <nl> scale -> input_is_pal = av_pix_fmt_descriptors [ inlink -> format ]. flags & PIX_FMT_PAL ; <nl>  <nl> + if ( scale -> sws ) <nl> + sws_freeContext ( scale -> sws ); <nl> scale -> sws = sws_getContext ( inlink -> w , inlink -> h , inlink -> format , <nl> outlink -> w , outlink -> h , outlink -> format , <nl> scale -> flags , NULL , NULL , NULL );
int ff_dirac_golomb_read_32bit ( DiracGolombLUT * lut_ctx , const uint8_t * buf , <nl> if (( c_idx + 1 ) > coeffs ) <nl> return c_idx ; <nl>  <nl> + if ( res_bits >= RSIZE_BITS ) <nl> + res_bits = res = 0 ; <nl> + <nl> /* res_bits is a hint for better branch prediction */ <nl> if ( res_bits && l -> sign ) { <nl> int32_t coeff = 1 ;
static int xwma_read_header ( AVFormatContext * s ) <nl>  <nl> /* Estimate the duration from the total number of output bytes . */ <nl> const uint64_t total_decoded_bytes = dpds_table [ dpds_table_size - 1 ]; <nl> + <nl> + if (! bytes_per_sample ) { <nl> + av_log ( s , AV_LOG_ERROR , " bytes_per_sample is 0 \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> st -> duration = total_decoded_bytes / bytes_per_sample ; <nl>  <nl> /* Use the dpds data to build a seek table . We can only do this after
static int asf_write_header ( AVFormatContext * s ) <nl> * It is needed to use asf as a streamable format . */ <nl> if ( asf_write_header1 ( s , 0 , DATA_HEADER_SIZE ) < 0 ) { <nl> // av_free ( asf ); <nl> + av_freep (& asf -> index_ptr ); <nl> return - 1 ; <nl> } <nl> 
static void stream_component_close ( VideoState * is , int stream_index ) <nl> if ( is -> rdft ) { <nl> av_rdft_end ( is -> rdft ); <nl> av_freep (& is -> rdft_data ); <nl> + is -> rdft = NULL ; <nl> + is -> rdft_bits = 0 ; <nl> } <nl> break ; <nl> case AVMEDIA_TYPE_VIDEO :
static void mpeg_er_decode_mb ( void * opaque , int ref , int mv_dir , int mv_type , <nl> s -> mb_skipped = mb_skipped ; <nl> s -> mb_x = mb_x ; <nl> s -> mb_y = mb_y ; <nl> + s -> mcsel = 0 ; <nl> memcpy ( s -> mv , mv , sizeof (* mv )); <nl>  <nl> ff_init_block_index ( s );
static int parse_playlist ( URLContext * h , const char * url ) <nl> return ret ; <nl>  <nl> read_chomp_line ( in , line , sizeof ( line )); <nl> - if ( strcmp ( line , "# EXTM3U ")) <nl> - return AVERROR_INVALIDDATA ; <nl> + if ( strcmp ( line , "# EXTM3U ")) { <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto fail ; <nl> + } <nl>  <nl> free_segment_list ( s ); <nl> s -> finished = 0 ;
static int find_slice_quant ( AVCodecContext * avctx , const AVFrame * pic , <nl> if ( ctx -> alpha_bits ) <nl> bits += estimate_alpha_plane ( ctx , & error , src , linesize [ 3 ], <nl> mbs_per_slice , q , td -> blocks [ 3 ]); <nl> - if ( bits > 65000 * 8 ) { <nl> + if ( bits > 65000 * 8 ) <nl> error = SCORE_LIMIT ; <nl> - break ; <nl> - } <nl> + <nl> slice_bits [ q ] = bits ; <nl> slice_score [ q ] = error ; <nl> }
yuv2mono_X_c_template ( SwsContext * c , const int16_t * lumFilter , <nl> const uint8_t * const d128 = dither_8x8_220 [ y & 7 ]; <nl> uint8_t * g = c -> table_gU [ 128 ] + c -> table_gV [ 128 ]; <nl> int i ; <nl> - int acc = 0 ; <nl> + unsigned acc = 0 ; <nl>  <nl> for ( i = 0 ; i < dstW - 1 ; i += 2 ) { <nl> int j ;
static int read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( size > 0 ) { <nl> - if ( pos + size < pos ) <nl> + if ( pos > INT64_MAX - size ) <nl> return AVERROR_INVALIDDATA ; <nl> avio_skip ( pb , FFMAX ( 0 , pos + size - avio_tell ( pb ))); <nl> }
static int thp_read_packet ( AVFormatContext * s , <nl> thp -> frame ++; <nl>  <nl> ret = av_get_packet ( pb , pkt , size ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( ret != size ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO );
static void write_frame ( AVFormatContext * s , AVPacket * pkt , OutputStream * ost ) <nl> * reordering , see do_video_out () <nl> */ <nl> if (!( avctx -> codec_type == AVMEDIA_TYPE_VIDEO && avctx -> codec )) { <nl> - if ( ost -> frame_number >= ost -> max_frames ) <nl> + if ( ost -> frame_number >= ost -> max_frames ) { <nl> + av_free_packet ( pkt ); <nl> return ; <nl> + } <nl> ost -> frame_number ++; <nl> } <nl> 
static int send_invoke_response ( URLContext * s , RTMPPacket * pkt ) <nl> { <nl> RTMPContext * rt = s -> priv_data ; <nl> double seqnum ; <nl> - char filename [ 64 ]; <nl> + char filename [ 128 ]; <nl> char command [ 64 ]; <nl> int stringlen ; <nl> char * pchar ;
static int load_ipmovie_packet ( IPMVEContext * s , AVIOContext * pb , <nl> int chunk_type ; <nl>  <nl> if ( s -> audio_chunk_offset ) { <nl> + if ( s -> audio_type == CODEC_ID_NONE ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " Can not read audio packet before " <nl> + " audio codec is known \ n "); <nl> + return CHUNK_BAD ; <nl> + } <nl>  <nl> /* adjust for PCM audio by skipping chunk header */ <nl> if ( s -> audio_type != CODEC_ID_INTERPLAY_DPCM ) {
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame_ptr , <nl> header = AV_RB32 ( buf ); <nl> if ( header >> 8 == AV_RB32 (" TAG ")>> 8 ) { <nl> av_log ( avctx , AV_LOG_DEBUG , " discarding ID3 tag \ n "); <nl> - return buf_size ; <nl> + return buf_size + skipped ; <nl> } <nl> ret = avpriv_mpegaudio_decode_header (( MPADecodeHeader *) s , header ); <nl> if ( ret < 0 ) {
static int decode_block ( MJpegDecodeContext * s , int16_t * block , int component , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> val = val * quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> + val = FFMIN ( val , 32767 ); <nl> s -> last_dc [ component ] = val ; <nl> block [ 0 ] = val ; <nl> /* AC coefs */
static void probe_codec ( AVFormatContext * s , AVStream * st , const AVPacket * pkt ) <nl> memset ( pd -> buf + pd -> buf_size , 0 , AVPROBE_PADDING_SIZE ); <nl> } else { <nl> st -> probe_packets = 0 ; <nl> + if (! pd -> buf_size ) { <nl> + av_log ( s , AV_LOG_ERROR , " nothing to probe for stream % d \ n ", <nl> + st -> index ); <nl> + return ; <nl> + } <nl> } <nl>  <nl> if (! st -> probe_packets ||
static int alloc_sequence_buffers ( DiracContext * s ) <nl> s -> mctmp = av_malloc (( w + 64 + MAX_BLOCKSIZE ) * ( h * MAX_BLOCKSIZE ) * sizeof (* s -> mctmp )); <nl> s -> mcscratch = av_malloc (( w + 64 )* MAX_BLOCKSIZE ); <nl>  <nl> - if (! s -> sbsplit || ! s -> blmotion ) <nl> + if (! s -> sbsplit || ! s -> blmotion || ! s -> mctmp || ! s -> mcscratch ) <nl> return AVERROR ( ENOMEM ); <nl> return 0 ; <nl> }
av_cold int ff_MPV_encode_init ( AVCodecContext * avctx ) <nl> av_log ( avctx , AV_LOG_ERROR , " b frames not supported by codec \ n "); <nl> return - 1 ; <nl> } <nl> + if ( s -> max_b_frames < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " max b frames must be 0 or postive for mpegvideo based encoders \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> if (( s -> codec_id == AV_CODEC_ID_MPEG4 || <nl> s -> codec_id == AV_CODEC_ID_H263 ||
int ff_rtsp_open_transport_ctx ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl>  <nl> if (! rtsp_st -> transport_priv ) { <nl> return AVERROR ( ENOMEM ); <nl> - } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP ) { <nl> + } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP && <nl> + s -> iformat ) { <nl> RTPDemuxContext * rtpctx = rtsp_st -> transport_priv ; <nl> rtpctx -> ssrc = rtsp_st -> ssrc ; <nl> if ( rtsp_st -> dynamic_handler ) {
int ff_flac_parse_picture ( AVFormatContext * s , uint8_t * buf , int buf_size ) <nl> ret = AVERROR_INVALIDDATA ; <nl> goto fail ; <nl> } <nl> - if (!( data = av_buffer_alloc ( len ))) { <nl> + if (!( data = av_buffer_alloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ))) { <nl> RETURN_ERROR ( AVERROR ( ENOMEM )); <nl> } <nl> if ( avio_read ( pb , data -> data , len ) != len ) {
static av_cold int amr_nb_encode_init ( AVCodecContext * avctx ) <nl>  <nl> avctx -> frame_size = 160 ; <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (! avctx -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> s -> enc_state = Encoder_Interface_init ( s -> enc_dtx ); <nl> if (! s -> enc_state ) {
static void matroska_add_index_entries ( MatroskaDemuxContext * matroska ) <nl> { <nl> EbmlList * index_list ; <nl> MatroskaIndex * index ; <nl> - int index_scale = 1 ; <nl> + uint64_t index_scale = 1 ; <nl> int i , j ; <nl>  <nl> if ( matroska -> ctx -> flags & AVFMT_FLAG_IGNIDX )
int ff_get_wav_header ( AVIOContext * pb , AVCodecContext * codec , int size ) <nl> codec -> sample_rate = 0 ; <nl> } <nl> /* override bits_per_coded_sample for G . 726 */ <nl> - if ( codec -> codec_id == AV_CODEC_ID_ADPCM_G726 ) <nl> + if ( codec -> codec_id == AV_CODEC_ID_ADPCM_G726 && codec -> sample_rate ) <nl> codec -> bits_per_coded_sample = codec -> bit_rate / codec -> sample_rate ; <nl>  <nl> return 0 ;
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_CREATE_TIMER : <nl> av_dlog ( NULL , " create timer \ n "); <nl> - if (( opcode_version > 0 ) || ( opcode_size > 6 )) { <nl> + if (( opcode_version > 0 ) || ( opcode_size != 6 )) { <nl> av_dlog ( NULL , " bad create_timer opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
int av_set_string3 ( void * obj , const char * name , const char * val , int alloc , cons <nl>  <nl> int av_opt_set ( void * obj , const char * name , const char * val , int search_flags ) <nl> { <nl> - int ret ; <nl> + int ret = 0 ; <nl> void * dst , * target_obj ; <nl> const AVOption * o = av_opt_find2 ( obj , name , NULL , 0 , search_flags , & target_obj ); <nl> if (! o || ! target_obj )
static av_cold void uninit ( AVFilterContext * ctx ) <nl>  <nl> av_freep (& s -> frame_data . input ); <nl> av_freep (& s -> frame_data . temp ); <nl> + av_freep (& s -> fdsp ); <nl> av_frame_free (& s -> second ); <nl> } <nl> 
static int r3d_read_reda ( AVFormatContext * s , AVPacket * pkt , Atom * atom ) <nl> dts = avio_rb32 ( s -> pb ); <nl>  <nl> st -> codec -> sample_rate = avio_rb32 ( s -> pb ); <nl> + if ( st -> codec -> sample_rate < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " negative sample rate \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> samples = avio_rb32 ( s -> pb ); <nl> 
static av_cold int pcm_encode_init ( AVCodecContext * avctx ) <nl>  <nl> avctx -> bits_per_coded_sample = av_get_bits_per_sample ( avctx -> codec -> id ); <nl> avctx -> block_align = avctx -> channels * avctx -> bits_per_coded_sample / 8 ; <nl> - avctx -> bit_rate = avctx -> block_align * avctx -> sample_rate * 8 ; <nl> + avctx -> bit_rate = avctx -> block_align * 8LL * avctx -> sample_rate ; <nl>  <nl> return 0 ; <nl> }
enum AVPixelFormat avcodec_find_best_pix_fmt_of_list ( const enum AVPixelFormat * p <nl> int loss ; <nl>  <nl> for ( i = 0 ; pix_fmt_list [ i ] != AV_PIX_FMT_NONE ; i ++) { <nl> - loss = * loss_ptr ; <nl> + loss = loss_ptr ? * loss_ptr : 0 ; <nl> best = avcodec_find_best_pix_fmt_of_2 ( best , pix_fmt_list [ i ], src_pix_fmt , has_alpha , & loss ); <nl> } <nl>  <nl> - * loss_ptr = loss ; <nl> + if ( loss_ptr ) <nl> + * loss_ptr = loss ; <nl> return best ; <nl> } <nl> 
static av_cold int vqa_decode_init ( AVCodecContext * avctx ) <nl> /* allocate decode buffer */ <nl> s -> decode_buffer_size = ( s -> width / s -> vector_width ) * <nl> ( s -> height / s -> vector_height ) * 2 ; <nl> - s -> decode_buffer = av_malloc ( s -> decode_buffer_size ); <nl> + s -> decode_buffer = av_mallocz ( s -> decode_buffer_size ); <nl> if (! s -> decode_buffer ) <nl> goto fail ; <nl> 
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> static int mov_probe ( AVProbeData * p ) <nl> { <nl> - unsigned int offset ; <nl> + int64_t offset ; <nl> uint32_t tag ; <nl> int score = 0 ; <nl> 
static void fill_table ( uint8_t * table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int <nl> } <nl> } <nl>  <nl> - static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int inc ) <nl> + static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int64_t inc ) <nl> { <nl> int i ; <nl> int off = -( inc >> 9 );
static int read_matrix_params ( MLPDecodeContext * m , unsigned int substr , GetBitCo <nl> av_log ( m -> avctx , AV_LOG_ERROR , <nl> " Number of primitive matrices cannot be greater than % d .\ n ", <nl> max_primitive_matrices ); <nl> + s -> num_primitive_matrices = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int mov_preroll_write_stbl_atoms ( AVIOContext * pb , MOVTrack * track ) <nl> } <nl> entries ++; <nl>  <nl> - if (! group ) <nl> + if (! group ) { <nl> + av_free ( sgpd_entries ); <nl> return 0 ; <nl> + } <nl>  <nl> /* Write sgpd tag */ <nl> avio_wb32 ( pb , 24 + ( group * 2 )); /* size */
end : <nl> frame -> height = avctx -> height ; <nl> } <nl>  <nl> + if ( ret < 0 ) <nl> + av_frame_unref ( frame ); <nl> + <nl> return ret ; <nl> } <nl> 
void ff_af_queue_remove ( AudioFrameQueue * afq , int nb_samples , int64_t * pts , <nl>  <nl> if ( nb_samples ){ <nl> av_assert0 (! afq -> frame_count ); <nl> - if ( afq -> frames [ 0 ]. pts != AV_NOPTS_VALUE ) <nl> + if ( afq -> frames && afq -> frames [ 0 ]. pts != AV_NOPTS_VALUE ) <nl> afq -> frames [ 0 ]. pts += nb_samples ; <nl> av_log ( afq -> avctx , AV_LOG_DEBUG , " Trying to remove % d more samples than are in the que \ n ", nb_samples ); <nl> }
static const struct URLProtocol * url_find_protocol ( const char * filename ) <nl> return up ; <nl> } <nl> } <nl> + av_freep (& protocols ); <nl>  <nl> return NULL ; <nl> }
static int build_table ( VLC * vlc , int table_nb_bits , int nb_codes , <nl> /* note : realloc has been done , so reload tables */ <nl> table = & vlc -> table [ table_index ]; <nl> table [ j ][ 0 ] = index ; // code <nl> + av_assert0 ( table [ j ][ 0 ] == index ); <nl> i = k - 1 ; <nl> } <nl> }
static int decorrelate ( TAKDecContext * s , int c1 , int c2 , int length ) <nl> s -> residues [ i ] * s -> filter [ 0 ]; <nl> } <nl>  <nl> - v = ( av_clip_intp2 ( v >> 10 , 13 ) << dshift ) - * p1 ; <nl> + v = av_clip_intp2 ( v >> 10 , 13 ) * ( 1 << dshift ) - * p1 ; <nl> * p1 ++ = v ; <nl> } <nl> 
static int hls_write_trailer ( struct AVFormatContext * s ) <nl> av_write_trailer ( oc ); <nl> hls -> size = avio_tell ( hls -> avf -> pb ) - hls -> start_pos ; <nl> avio_closep (& oc -> pb ); <nl> - avformat_free_context ( oc ); <nl> av_free ( hls -> basename ); <nl> hls_append_segment ( hls , hls -> duration , hls -> start_pos , hls -> size ); <nl> + avformat_free_context ( oc ); <nl> + hls -> avf = NULL ; <nl> hls_window ( s , 1 ); <nl>  <nl> hls_free_segments ( hls );
static void do_video_out ( AVFormatContext * s , <nl> if (! ost -> last_frame ) <nl> ost -> last_frame = av_frame_alloc (); <nl> av_frame_unref ( ost -> last_frame ); <nl> - if ( next_picture ) <nl> + if ( next_picture && ost -> last_frame ) <nl> av_frame_ref ( ost -> last_frame , next_picture ); <nl> else <nl> av_frame_free (& ost -> last_frame );
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> s -> channel_in_cpl [ ch ] = 0 ; <nl> s -> first_cpl_coords [ ch ] = 1 ; <nl> } <nl> - s -> first_cpl_leak = 1 ; <nl> + s -> first_cpl_leak = s -> eac3 ; <nl> s -> phase_flags_in_use = 0 ; <nl> } <nl> } else if (! s -> eac3 ) {
ImgReSampleContext * img_resample_full_init ( int owidth , int oheight , <nl> { <nl> ImgReSampleContext * s ; <nl>  <nl> + if (! owidth || ! oheight || ! iwidth || ! iheight ) <nl> + return NULL ; <nl> + <nl> s = av_mallocz ( sizeof ( ImgReSampleContext )); <nl> if (! s ) <nl> return NULL ;
static int cavs_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> if (! s -> low_delay && h -> DPB [ 0 ]. f . data [ 0 ]) { <nl> * data_size = sizeof ( AVPicture ); <nl> * picture = h -> DPB [ 0 ]. f ; <nl> + memset (& h -> DPB [ 0 ], 0 , sizeof ( h -> DPB [ 0 ])); <nl> } <nl> return 0 ; <nl> }
static int fdk_aac_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size , 0 ); <nl> + err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size / sizeof ( INT_PCM ), 0 ); <nl> if ( err == AAC_DEC_NOT_ENOUGH_BITS ) { <nl> ret = avpkt -> size - valid ; <nl> goto end ;
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & gbc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
void avcodec_flush_buffers ( AVCodecContext * avctx ) <nl> ff_thread_flush ( avctx ); <nl> else if ( avctx -> codec -> flush ) <nl> avctx -> codec -> flush ( avctx ); <nl> + <nl> + if (! avctx -> refcounted_frames ) <nl> + av_frame_unref (& avctx -> internal -> to_free ); <nl> } <nl>  <nl> int av_get_exact_bits_per_sample ( enum AVCodecID codec_id )
static int mpeg4_decode_header ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl> if ( avctx -> extradata_size && pc -> first_picture ) { <nl> init_get_bits ( gb , avctx -> extradata , avctx -> extradata_size * 8 ); <nl> ret = ff_mpeg4_decode_picture_header ( dec_ctx , gb ); <nl> + if ( ret < 0 ) <nl> + av_log ( avctx , AV_LOG_WARNING , " Failed to parse extradata \ n "); <nl> } <nl>  <nl> init_get_bits ( gb , buf , 8 * buf_size );
int av_find_best_stream ( AVFormatContext * ic , enum AVMediaType type , <nl> st -> disposition & ( AV_DISPOSITION_HEARING_IMPAIRED | <nl> AV_DISPOSITION_VISUAL_IMPAIRED )) <nl> continue ; <nl> - if ( type == AVMEDIA_TYPE_AUDIO && ! avctx -> channels ) <nl> + if ( type == AVMEDIA_TYPE_AUDIO && !( avctx -> channels && avctx -> sample_rate )) <nl> continue ; <nl> if ( decoder_ret ) { <nl> decoder = find_decoder ( ic , st , st -> codec -> codec_id );
int ff_mpeg4_frame_end ( AVCodecContext * avctx , const uint8_t * buf , int buf_size ) <nl> } <nl>  <nl> if ( startcode_found ) { <nl> - av_fast_malloc (& s -> bitstream_buffer , <nl> + av_fast_padded_malloc (& s -> bitstream_buffer , <nl> & s -> allocated_bitstream_buffer_size , <nl> - buf_size - current_pos + <nl> - FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + buf_size - current_pos ); <nl> if (! s -> bitstream_buffer ) <nl> return AVERROR ( ENOMEM ); <nl> memcpy ( s -> bitstream_buffer , buf + current_pos ,
static void exit_program ( void ) <nl> /* close files */ <nl> for ( i = 0 ; i < nb_output_files ; i ++) { <nl> AVFormatContext * s = output_files [ i ]-> ctx ; <nl> - if (!( s -> oformat -> flags & AVFMT_NOFILE ) && s -> pb ) <nl> + if ( s && s -> oformat && !( s -> oformat -> flags & AVFMT_NOFILE ) && s -> pb ) <nl> avio_close ( s -> pb ); <nl> avformat_free_context ( s ); <nl> av_dict_free (& output_files [ i ]-> opts );
static int mpegts_read_packet ( AVFormatContext * s , <nl> MpegTSContext * ts = s -> priv_data ; <nl> int ret , i ; <nl>  <nl> + pkt -> size = - 1 ; <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> static int mpegts_read_packet ( AVFormatContext * s , <nl> } <nl> } <nl>  <nl> + if (! ret && pkt -> size < 0 ) <nl> + ret = AVERROR ( EINTR ); <nl> return ret ; <nl> } <nl> 
typedef struct SmcContext { <nl> row_ptr += stride * 4 ; \ <nl> } \ <nl> total_blocks --; \ <nl> - if ( total_blocks < 0 ) \ <nl> + if ( total_blocks < !! n_blocks ) \ <nl> { \ <nl> av_log ( s -> avctx , AV_LOG_INFO , " warning : block counter just went negative ( this should not happen )\ n "); \ <nl> return ; \
static int start_frame_overlay ( AVFilterLink * inlink , AVFilterBufferRef * inpicref <nl> OverlayContext * over = ctx -> priv ; <nl>  <nl> inlink -> cur_buf = NULL ; <nl> + avfilter_unref_bufferp (& over -> overpicref ); <nl> over -> overpicref = inpicref ; <nl> over -> overpicref -> pts = av_rescale_q ( inpicref -> pts , ctx -> inputs [ OVERLAY ]-> time_base , <nl> ctx -> outputs [ 0 ]-> time_base );
static int vp56_size_changed ( VP56Context * s ) <nl> s -> plane_height [ 0 ] = s -> plane_height [ 3 ] = avctx -> coded_height ; <nl> s -> plane_height [ 1 ] = s -> plane_height [ 2 ] = avctx -> coded_height / 2 ; <nl>  <nl> + s -> have_undamaged_frame = 0 ; <nl> + <nl> for ( i = 0 ; i < 4 ; i ++) <nl> s -> stride [ i ] = s -> flip * s -> frames [ VP56_FRAME_CURRENT ]-> linesize [ i ]; <nl> 
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl>  <nl> display_matrix = ( uint32_t *) av_stream_get_side_data ( st , AV_PKT_DATA_DISPLAYMATRIX , <nl> & display_matrix_size ); <nl> - if ( display_matrix_size < 9 * sizeof (* display_matrix )) <nl> + if ( display_matrix && display_matrix_size < 9 * sizeof (* display_matrix )) <nl> display_matrix = NULL ; <nl> } <nl> 
static int apply_color_indexing_transform ( WebPContext * s ) <nl> uint8_t * line ; <nl> int pixel_bits = 8 >> pal -> size_reduction ; <nl>  <nl> - line = av_malloc ( img -> frame -> linesize [ 0 ]); <nl> + line = av_malloc ( img -> frame -> linesize [ 0 ] + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! line ) <nl> return AVERROR ( ENOMEM ); <nl> 
static void vc1_decode_i_blocks_adv ( VC1Context * v ) <nl> s -> mb_x = 0 ; <nl> ff_init_block_index ( s ); <nl> memset (& s -> coded_block [ s -> block_index [ 0 ]- s -> b8_stride ], 0 , <nl> - s -> b8_stride * sizeof (* s -> coded_block )); <nl> + ( 1 + s -> b8_stride ) * sizeof (* s -> coded_block )); <nl> } <nl> for (; s -> mb_y < s -> end_mb_y ; s -> mb_y ++) { <nl> s -> mb_x = 0 ;
static int wavpack_encode_block ( WavPackEncodeContext * s , <nl> uint8_t * out , int out_size ) <nl> { <nl> int block_size , start , end , data_size , tcount , temp , m = 0 ; <nl> - int i , j , ret , got_extra = 0 , nb_samples = s -> block_samples ; <nl> + int i , j , ret = 0 , got_extra = 0 , nb_samples = s -> block_samples ; <nl> uint32_t crc = 0xffffffffu ; <nl> struct Decorr * dpp ; <nl> PutByteContext pb ;
static void picmemset ( PicContext * s , AVFrame * frame , int value , int run , <nl> if (* y < 0 ) { <nl> * y = s -> height - 1 ; <nl> * plane += 1 ; <nl> - value <<= bits_per_plane ; <nl> - mask <<= bits_per_plane ; <nl> if (* plane >= s -> nb_planes ) <nl> return ; <nl> + value <<= bits_per_plane ; <nl> + mask <<= bits_per_plane ; <nl> } <nl> } <nl> }
static int flush_packet ( AVFormatContext * ctx , int stream_index , <nl>  <nl> if ( stuffing_size < 0 ) <nl> stuffing_size = 0 ; <nl> + <nl> + if ( startcode == PRIVATE_STREAM_1 && id >= 0xa0 ) { <nl> + if ( payload_size < av_fifo_size ( stream -> fifo )) <nl> + stuffing_size += payload_size % stream -> lpcm_align ; <nl> + } <nl> + <nl> if ( stuffing_size > 16 ) { /*<= 16 for MPEG - 1 , <= 32 for MPEG - 2 */ <nl> pad_packet_bytes += stuffing_size ; <nl> packet_size -= stuffing_size ;
static int aac_adtstoasc_filter ( AVBitStreamFilterContext * bsfc , <nl> buf_size -= get_bits_count (& gb )/ 8 ; <nl> buf += get_bits_count (& gb )/ 8 ; <nl> } <nl> + av_free ( avctx -> extradata ); <nl> avctx -> extradata_size = 2 + pce_size ; <nl> avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> 
static int qsv_decode ( AVCodecContext * avctx , QSVContext * q , <nl>  <nl> do { <nl> ret = get_surface ( avctx , q , & insurf ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& sync ); <nl> return ret ; <nl> + } <nl>  <nl> ret = MFXVideoDECODE_DecodeFrameAsync ( q -> session , avpkt -> size ? & bs : NULL , <nl> insurf , & outsurf , sync );
static int asf_read_metadata ( AVFormatContext * s , int64_t size ) <nl> } else { <nl> get_tag ( s , name , value_type , value_len , 16 ); <nl> } <nl> + av_freep (& name ); <nl> } <nl>  <nl> return 0 ;
static void vector_fmul_window_fixed_c ( int32_t * dst , const int32_t * src0 , <nl> AVFixedDSPContext * avpriv_alloc_fixed_dsp ( int bit_exact ) <nl> { <nl> AVFixedDSPContext * fdsp = av_malloc ( sizeof ( AVFixedDSPContext )); <nl> + <nl> + if (! fdsp ) <nl> + return NULL ; <nl> + <nl> fdsp -> vector_fmul_window_scaled = vector_fmul_window_fixed_scaled_c ; <nl> fdsp -> vector_fmul_window = vector_fmul_window_fixed_c ; <nl> 
static int open_output_file ( const char * filename ) <nl> || dec_ctx -> codec_type == AVMEDIA_TYPE_AUDIO ) { <nl> /* in this example , we choose transcoding to same codec */ <nl> encoder = avcodec_find_encoder ( dec_ctx -> codec_id ); <nl> + if (! encoder ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Neccessary encoder not found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> /* In this example , we transcode to same properties ( picture size , <nl> * sample rate etc .). These properties can be changed for output
static int pxr24_uncompress ( EXRContext * s , const uint8_t * src , <nl> in = ptr [ 2 ] + td -> xsize ; <nl>  <nl> for ( j = 0 ; j < td -> xsize ; ++ j ) { <nl> - uint32_t diff = (*( ptr [ 0 ]++) << 24 ) | <nl> + uint32_t diff = (( unsigned )*( ptr [ 0 ]++) << 24 ) | <nl> (*( ptr [ 1 ]++) << 16 ) | <nl> (*( ptr [ 2 ]++) << 8 ); <nl> pixel += diff ;
ksymsmmap ( dev , off , prot ) <nl> int off , prot ; <nl> { <nl> # define ksyms_btop ( x ) (( vm_offset_t )( x ) >> PGSHIFT <nl> + if ( off < 0 ) <nl> + return (- 1 ); <nl> if (( unsigned ) off >= ( unsigned )( esym - symtab ) + k1 -> a_text ) <nl> return (- 1 ); <nl> 
void feedlist_formaction :: process_operation ( operation op , bool automatic , std :: v <nl> case OP_OPENINBROWSER : { <nl> std :: tr1 :: shared_ptr < rss_feed > feed = v -> get_ctrl ()-> get_feed ( pos ); <nl> if ( feed ) { <nl> + LOG ( LOG_INFO , " feedlist_formaction : opening feed at position `% s ': % s ", feedpos . c_str (), feed -> link (). c_str ()); <nl> v -> open_in_browser ( feed -> link ()); <nl> } <nl> }
void kvm_irqchip_commit_routes ( KVMState * s ) <nl> { <nl> int ret ; <nl>  <nl> + if ( kvm_gsi_direct_mapping ()) { <nl> + return ; <nl> + } <nl> + <nl> + if (! kvm_gsi_routing_enabled ()) { <nl> + return ; <nl> + } <nl> + <nl> s -> irq_routes -> flags = 0 ; <nl> trace_kvm_irqchip_commit_routes (); <nl> ret = kvm_vm_ioctl ( s , KVM_SET_GSI_ROUTING , s -> irq_routes );
int qcow2_alloc_cluster_link_l2 ( BlockDriverState * bs , QCowL2Meta * m ) <nl> } <nl> qcow2_cache_entry_mark_dirty ( s -> l2_table_cache , l2_table ); <nl>  <nl> + assert ( l2_index + m -> nb_clusters <= s -> l2_size ); <nl> for ( i = 0 ; i < m -> nb_clusters ; i ++) { <nl> /* if two concurrent writes happen to the same unallocated cluster <nl> * each write allocates separate cluster and writes data concurrently .
opts_visitor_cleanup ( OptsVisitor * ov ) <nl> g_hash_table_destroy ( ov -> unprocessed_opts ); <nl> } <nl> g_free ( ov -> fake_id_opt ); <nl> - memset ( ov , '\ 0 ', sizeof * ov ); <nl> + g_free ( ov ); <nl> } <nl>  <nl> 
static inline int cpu_interrupts_enabled ( CPUSPARCState * env1 ) <nl> if ( env1 -> psret != 0 ) <nl> return 1 ; <nl> # else <nl> - if ( env1 -> pstate & PS_IE ) <nl> + if (( env1 -> pstate & PS_IE ) && ! cpu_hypervisor_mode ( env1 )) { <nl> return 1 ; <nl> + } <nl> # endif <nl>  <nl> return 0 ;
void qemu_input_event_send ( QemuConsole * src , InputEvent * evt ) <nl>  <nl> /* send event */ <nl> s = qemu_input_find_handler ( 1 << evt -> kind ); <nl> + if (! s ) { <nl> + return ; <nl> + } <nl> s -> handler -> event ( s -> dev , src , evt ); <nl> s -> events ++; <nl> }
int bdrv_open_image ( BlockDriverState ** pbs , const char * filename , <nl> bdref_key ); <nl> ret = - EINVAL ; <nl> } <nl> + QDECREF ( image_options ); <nl> goto done ; <nl> } <nl> 
static void test_dispatch_cmd_io ( void ) <nl>  <nl> ret3 = qobject_to_qint ( test_qmp_dispatch ( req )); <nl> assert ( qint_get_int ( ret3 ) == 66 ); <nl> - QDECREF ( ret ); <nl> + QDECREF ( ret3 ); <nl>  <nl> QDECREF ( req ); <nl> }
static uint64_t <nl> e1000e_io_read ( void * opaque , hwaddr addr , unsigned size ) <nl> { <nl> E1000EState * s = opaque ; <nl> - uint32_t idx ; <nl> + uint32_t idx = 0 ; <nl> uint64_t val ; <nl>  <nl> switch ( addr ) { <nl> e1000e_io_write ( void * opaque , hwaddr addr , <nl> uint64_t val , unsigned size ) <nl> { <nl> E1000EState * s = opaque ; <nl> - uint32_t idx ; <nl> + uint32_t idx = 0 ; <nl>  <nl> switch ( addr ) { <nl> case E1000_IOADDR :
static int iscsi_open ( BlockDriverState * bs , QDict * options , int flags , <nl> QemuOpts * opts ; <nl> Error * local_err = NULL ; <nl> const char * filename ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> if (( BDRV_SECTOR_SIZE % 512 ) != 0 ) { <nl> error_setg ( errp , " iSCSI : Invalid BDRV_SECTOR_SIZE . "
static uint32_t get_cmd ( ESPState * s , uint8_t * buf ) <nl> s -> ti_rptr = 0 ; <nl> s -> ti_wptr = 0 ; <nl>  <nl> - if ( s -> current_dev ) { <nl> + if ( s -> current_req ) { <nl> /* Started a new command before the old one finished . Cancel it . */ <nl> scsi_req_cancel ( s -> current_req ); <nl> s -> async_len = 0 ;
void hmp_drive_add_node ( Monitor * mon , const char * optstr ) <nl> qdict = qemu_opts_to_qdict ( opts , NULL ); <nl>  <nl> if (! qdict_get_try_str ( qdict , " node - name ")) { <nl> + QDECREF ( qdict ); <nl> error_report ("' node - name ' needs to be specified "); <nl> goto out ; <nl> }
static int img_check ( int argc , char ** argv ) <nl> static const struct option long_options [] = { <nl> {" help ", no_argument , 0 , ' h '}, <nl> {" format ", required_argument , 0 , ' f '}, <nl> - {" repair ", no_argument , 0 , ' r '}, <nl> + {" repair ", required_argument , 0 , ' r '}, <nl> {" output ", required_argument , 0 , OPTION_OUTPUT }, <nl> { 0 , 0 , 0 , 0 } <nl> };
static void vnc_async_encoding_end ( VncState * orig , VncState * local ) <nl> orig -> hextile = local -> hextile ; <nl> orig -> zrle = local -> zrle ; <nl> orig -> lossy_rect = local -> lossy_rect ; <nl> + <nl> + queue -> buffer = local -> output ; <nl> } <nl>  <nl> static int vnc_worker_thread_loop ( VncJobQueue * queue )
void init_paths ( const char * prefix ) <nl> base = new_entry ("", NULL , pref_buf ); <nl> base = add_dir_maybe ( base ); <nl> if ( base -> num_entries == 0 ) { <nl> - free ( base ); <nl> + g_free ( base -> pathname ); <nl> + free ( base -> name ); <nl> + free ( base ); <nl> base = NULL ; <nl> } else { <nl> set_parents ( base , base );
static void vscsi_process_login ( VSCSIState * s , vscsi_req * req ) <nl> struct srp_login_rsp * rsp = & iu -> srp . login_rsp ; <nl> uint64_t tag = iu -> srp . rsp . tag ; <nl>  <nl> - trace_spapr_vscsi__process_login (); <nl> + trace_spapr_vscsi_process_login (); <nl>  <nl> /* TODO handle case that requested size is wrong and <nl> * buffer format is wrong
static int qcow2_co_flush ( BlockDriverState * bs ) <nl> qemu_co_mutex_lock (& s -> lock ); <nl> ret = qcow2_cache_flush ( bs , s -> l2_table_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl>  <nl> ret = qcow2_cache_flush ( bs , s -> refcount_block_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl> qemu_co_mutex_unlock (& s -> lock );
static void disas_ldst_reg_imm9 ( DisasContext * s , uint32_t insn , <nl> post_index = false ; <nl> writeback = true ; <nl> break ; <nl> + default : <nl> + g_assert_not_reached (); <nl> } <nl>  <nl> if ( rn == 31 ) {
static bool cmd_smart ( IDEState * s , uint8_t cmd ) <nl> case 2 : /* extended self test */ <nl> s -> smart_selftest_count ++; <nl> if ( s -> smart_selftest_count > 21 ) { <nl> - s -> smart_selftest_count = 0 ; <nl> + s -> smart_selftest_count = 1 ; <nl> } <nl> n = 2 + ( s -> smart_selftest_count - 1 ) * 24 ; <nl> s -> smart_selftest_data [ n ] = s -> sector ;
void page_set_flags ( target_ulong start , target_ulong end , int flags ) <nl> guest address space . If this assert fires , it probably indicates <nl> a missing call to h2g_valid . */ <nl> # if TARGET_ABI_BITS > L1_MAP_ADDR_SPACE_BITS <nl> - assert ( end < (( target_ulong ) 1 << L1_MAP_ADDR_SPACE_BITS )); <nl> + assert ( end <= (( target_ulong ) 1 << L1_MAP_ADDR_SPACE_BITS )); <nl> # endif <nl> assert ( start < end ); <nl> assert_memory_lock ();
void replay_configure ( QemuOpts * opts ) <nl> rr = qemu_opt_get ( opts , " rr "); <nl> if (! rr ) { <nl> /* Just enabling icount */ <nl> - return ; <nl> + goto out ; <nl> } else if (! strcmp ( rr , " record ")) { <nl> mode = REPLAY_MODE_RECORD ; <nl> } else if (! strcmp ( rr , " replay ")) { <nl> void replay_configure ( QemuOpts * opts ) <nl>  <nl> replay_enable ( fname , mode ); <nl>  <nl> + out : <nl> loc_pop (& loc ); <nl> } <nl> 
static void tcp_chr_tls_handshake ( QIOTask * task , <nl> if ( qio_task_propagate_error ( task , NULL )) { <nl> tcp_chr_disconnect ( chr ); <nl> } else { <nl> - /* tn3270 does not support TLS yet */ <nl> - if ( s -> do_telnetopt && ! s -> is_tn3270 ) { <nl> + if ( s -> do_telnetopt ) { <nl> tcp_chr_telnet_init ( chr ); <nl> } else { <nl> tcp_chr_connect ( chr );
static int timebase_post_load ( void * opaque , int version_id ) <nl> host_ns = qemu_clock_get_ns ( QEMU_CLOCK_HOST ); <nl> ns_diff = MAX ( 0 , host_ns - tb_remote -> time_of_the_day_ns ); <nl> migration_duration_ns = MIN ( NANOSECONDS_PER_SECOND , ns_diff ); <nl> - migration_duration_tb = muldiv64 ( migration_duration_ns , freq , <nl> + migration_duration_tb = muldiv64 ( freq , migration_duration_ns , <nl> NANOSECONDS_PER_SECOND ); <nl> guest_tb = tb_remote -> guest_timebase + MIN ( 0 , migration_duration_tb ); <nl> 
static void bdrv_dirty_bitmap_truncate ( BlockDriverState * bs ) <nl> continue ; <nl> } <nl> hbitmap_truncate ( bitmap -> bitmap , size ); <nl> + bitmap -> size = size ; <nl> } <nl> } <nl> 
static void build_pci_bus_end ( PCIBus * bus , void * bus_state ) <nl>  <nl> build_append_byte ( notify , 0x7B ); /* AndOp */ <nl> build_append_byte ( notify , 0x68 ); /* Arg0Op */ <nl> - build_append_int ( notify , 0x1 << i ); <nl> + build_append_int ( notify , 0x1U << i ); <nl> build_append_byte ( notify , 0x00 ); /* NullName */ <nl> build_append_byte ( notify , 0x86 ); /* NotifyOp */ <nl> build_append_nameseg ( notify , " S %. 02X_ ", PCI_DEVFN ( i , 0 ));
static void loongarch_cpu_reset ( DeviceState * dev ) <nl>  <nl> # ifndef CONFIG_USER_ONLY <nl> env -> pc = 0x1c000000 ; <nl> + memset ( env -> tlb , 0 , sizeof ( env -> tlb )); <nl> # endif <nl>  <nl> restore_fp_status ( env );
static void vfio_put_device ( VFIOPCIDevice * vdev ) <nl> { <nl> g_free ( vdev -> vbasedev . name ); <nl> if ( vdev -> msix ) { <nl> + object_unparent ( OBJECT (& vdev -> msix -> mmap_mem )); <nl> g_free ( vdev -> msix ); <nl> vdev -> msix = NULL ; <nl> }
static void timer_enable ( struct xlx_timer * xt ) <nl> count = xt -> regs [ R_TLR ]; <nl> else <nl> count = ~ 0 - xt -> regs [ R_TLR ]; <nl> - ptimer_set_count ( xt -> ptimer , count ); <nl> + ptimer_set_limit ( xt -> ptimer , count , 1 ); <nl> ptimer_run ( xt -> ptimer , 1 ); <nl> } <nl> 
static void gen_sync ( DisasContext * ctx ) <nl> /* wait */ <nl> static void gen_wait ( DisasContext * ctx ) <nl> { <nl> - TCGv_i32 t0 = tcg_temp_new_i32 (); <nl> + TCGv_i32 t0 = tcg_const_i32 ( 1 ); <nl> tcg_gen_st_i32 ( t0 , cpu_env , <nl> - offsetof ( PowerPCCPU , env ) + offsetof ( CPUState , halted )); <nl> tcg_temp_free_i32 ( t0 );
int qemu_file_rate_limit ( QEMUFile * f ) <nl>  <nl> size_t qemu_file_set_rate_limit ( QEMUFile * f , size_t new_rate ) <nl> { <nl> - if ( f -> set_rate_limit ) <nl> + /* any failed or completed migration keeps its state to allow probing of <nl> + * migration data , but has no associated file anymore */ <nl> + if ( f && f -> set_rate_limit ) <nl> return f -> set_rate_limit ( f -> opaque , new_rate ); <nl>  <nl> return 0 ;
void coroutine_fn qemu_coroutine_yield ( void ) <nl> } <nl>  <nl> self -> caller = NULL ; <nl> - coroutine_swap ( self , to ); <nl> + qemu_coroutine_switch ( self , to , COROUTINE_YIELD ); <nl> }
static void tcg_liveness_analysis ( TCGContext * s ) <nl>  <nl> nb_ops = gen_opc_ptr - gen_opc_buf ; <nl>  <nl> - /* XXX : make it really dynamic */ <nl> - s -> op_dead_iargs = tcg_malloc ( OPC_BUF_SIZE * sizeof ( uint16_t )); <nl> + s -> op_dead_iargs = tcg_malloc ( nb_ops * sizeof ( uint16_t )); <nl>  <nl> dead_temps = tcg_malloc ( s -> nb_temps ); <nl> memset ( dead_temps , 1 , s -> nb_temps );
static void spapr_nvram_class_init ( ObjectClass * klass , void * data ) <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> props = spapr_nvram_properties ; <nl> dc -> vmsd = & vmstate_spapr_nvram ; <nl> + /* Reason : Internal device only , uses spapr_rtas_register () in realize () */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo spapr_nvram_type_info = {
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> if (( txdw0 & CP_TX_LGSEN ) && ip_protocol == IP_PROTO_TCP ) <nl> { <nl> + /* Large enough for the TCP header ? */ <nl> + if ( ip_data_len < sizeof ( tcp_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> int large_send_mss = ( txdw0 >> 16 ) & CP_TC_LGSEN_MSS_MASK ; <nl>  <nl> DPRINTF ("+++ C + mode offloaded task TSO MTU =% d IP data % d "
static sd_rsp_type_t sd_app_command ( SDState * sd , <nl> } <nl>  <nl> fprintf ( stderr , " SD : ACMD % i in a wrong state \ n ", req . cmd ); <nl> - return sd_r0 ; <nl> + return sd_illegal ; <nl> } <nl>  <nl> static int cmd_valid_while_locked ( SDState * sd , SDRequest * req )
void qemu_mutex_lock_iothread ( void ) <nl> * TCG code execution . <nl> */ <nl> if (! tcg_enabled () || qemu_in_vcpu_thread () || <nl> - ! first_cpu || ! first_cpu -> thread ) { <nl> + ! first_cpu || ! first_cpu -> created ) { <nl> qemu_mutex_lock (& qemu_global_mutex ); <nl> atomic_dec (& iothread_requesting_mutex ); <nl> } else {
void cuda_init ( int * cuda_mem_index , qemu_irq irq ) <nl>  <nl> s -> timers [ 1 ]. index = 1 ; <nl>  <nl> - qemu_get_timedate (& tm , RTC_OFFSET ); <nl> - s -> tick_offset = mktimegm (& tm ); <nl> + qemu_get_timedate (& tm , 0 ); <nl> + s -> tick_offset = ( uint32_t ) mktimegm (& tm ) + RTC_OFFSET ; <nl>  <nl> s -> adb_poll_timer = qemu_new_timer ( vm_clock , cuda_adb_poll , s ); <nl> * cuda_mem_index = cpu_register_io_memory ( 0 , cuda_read , cuda_write , s );
void AUD_del_capture ( CaptureVoiceOut * cap , void * cb_opaque ) <nl> sw = sw1 ; <nl> } <nl> QLIST_REMOVE ( cap , entries ); <nl> + g_free ( cap -> hw . mix_buf ); <nl> + g_free ( cap -> buf ); <nl> g_free ( cap ); <nl> } <nl> return ;
void object_property_set_qobject ( Object * obj , QObject * value , <nl> const char * name , Error ** errp ) <nl> { <nl> Visitor * v ; <nl> - /* TODO : Should we reject , rather than ignore , excess input ? */ <nl> - v = qobject_input_visitor_new ( value , false ); <nl> + <nl> + v = qobject_input_visitor_new ( value , true ); <nl> object_property_set ( obj , v , name , errp ); <nl> visit_free ( v ); <nl> }
void acpi_setup ( PcGuestInfo * guest_info ) <nl> return ; <nl> } <nl>  <nl> + if (! acpi_enabled ) { <nl> + ACPI_BUILD_DPRINTF ( 3 , " ACPI disabled . Bailing out .\ n "); <nl> + return ; <nl> + } <nl> + <nl> build_state = g_malloc0 ( sizeof * build_state ); <nl>  <nl> build_state -> guest_info = guest_info ;
void qemu_iovec_destroy ( QEMUIOVector * qiov ) <nl> { <nl> assert ( qiov -> nalloc != - 1 ); <nl>  <nl> + qemu_iovec_reset ( qiov ); <nl> g_free ( qiov -> iov ); <nl> + qiov -> nalloc = 0 ; <nl> + qiov -> iov = NULL ; <nl> } <nl>  <nl> void qemu_iovec_reset ( QEMUIOVector * qiov )
int64_t throttle_compute_wait ( LeakyBucket * bkt ) <nl> /* If the main bucket is not full yet we still have to check the <nl> * burst bucket in order to enforce the burst limit */ <nl> if ( bkt -> burst_length > 1 ) { <nl> + assert ( bkt -> max > 0 ); /* see throttle_is_valid () */ <nl> extra = bkt -> burst_level - burst_bucket_size ; <nl> if ( extra > 0 ) { <nl> return throttle_do_compute_wait ( bkt -> max , extra );
static void cpu_common_initfn ( Object * obj ) <nl>  <nl> cpu -> cpu_index = UNASSIGNED_CPU_INDEX ; <nl> cpu -> gdb_num_regs = cpu -> gdb_num_g_regs = cc -> gdb_num_core_regs ; <nl> + /* *- user doesn ' t have configurable SMP topology */ <nl> + /* the default value is changed by qemu_init_vcpu () for softmmu */ <nl> + cpu -> nr_cores = 1 ; <nl> + cpu -> nr_threads = 1 ; <nl> + <nl> qemu_mutex_init (& cpu -> work_mutex ); <nl> QTAILQ_INIT (& cpu -> breakpoints ); <nl> QTAILQ_INIT (& cpu -> watchpoints );
static void patch_instruction ( VAPICROMState * s , X86CPU * cpu , target_ulong ip ) <nl> CPUX86State * env = & cpu -> env ; <nl> VAPICHandlers * handlers ; <nl> uint8_t opcode [ 2 ]; <nl> - uint32_t imm32 ; <nl> + uint32_t imm32 = 0 ; <nl> target_ulong current_pc = 0 ; <nl> target_ulong current_cs_base = 0 ; <nl> uint32_t current_flags = 0 ;
static void serial_update_parameters ( SerialState * s ) <nl> int speed , parity , data_bits , stop_bits , frame_size ; <nl> QEMUSerialSetParams ssp ; <nl>  <nl> - if ( s -> divider == 0 ) <nl> + if ( s -> divider == 0 || s -> divider > s -> baudbase ) { <nl> return ; <nl> + } <nl>  <nl> /* Start bit . */ <nl> frame_size = 1 ;
static void pc87312_class_init ( ObjectClass * klass , void * data ) <nl> dc -> reset = pc87312_reset ; <nl> dc -> vmsd = & vmstate_pc87312 ; <nl> dc -> props = pc87312_properties ; <nl> + /* Reason : Uses parallel_hds [ 0 ] in realize (), so it can ' t be used twice */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo pc87312_type_info = {
X86RegisterInfo32 x86_reg_info_32 [ CPU_NB_REGS32 ] = { <nl>  <nl> const char * get_register_name_32 ( unsigned int reg ) <nl> { <nl> - if ( reg > CPU_NB_REGS32 ) { <nl> + if ( reg >= CPU_NB_REGS32 ) { <nl> return NULL ; <nl> } <nl> return x86_reg_info_32 [ reg ]. name ;
static void tcg_target_qemu_prologue ( TCGContext * s ) <nl> } <nl>  <nl> /* Call generated code */ <nl> - tcg_out_opc_reg ( s , OPC_JR , 0 , tcg_target_call_iarg_regs [ 1 ]), 0 ); <nl> + tcg_out_opc_reg ( s , OPC_JR , 0 , tcg_target_call_iarg_regs [ 1 ], 0 ); <nl> tcg_out_mov ( s , TCG_TYPE_PTR , TCG_AREG0 , tcg_target_call_iarg_regs [ 0 ]); <nl> tb_ret_addr = s -> code_ptr ; <nl> 
void usb_ehci_realize ( EHCIState * s , DeviceState * dev , Error ** errp ) <nl> NB_PORTS ); <nl> return ; <nl> } <nl> + if ( s -> maxframes < 8 || s -> maxframes > 512 ) { <nl> + error_setg ( errp , " maxframes % d out if range ( 8 .. 512 )", <nl> + s -> maxframes ); <nl> + return ; <nl> + } <nl>  <nl> usb_bus_new (& s -> bus , sizeof ( s -> bus ), s -> companion_enable ? <nl> & ehci_bus_ops_companion : & ehci_bus_ops_standalone , dev );
static ExitStatus op_ex ( DisasContext * s , DisasOps * o ) <nl> TCGv_i64 tmp ; <nl>  <nl> update_psw_addr ( s ); <nl> - update_cc_op ( s ); <nl> + gen_op_calc_cc ( s ); <nl>  <nl> tmp = tcg_const_i64 ( s -> next_pc ); <nl> gen_helper_ex ( cc_op , cpu_env , cc_op , o -> in1 , o -> in2 , tmp ); <nl> tcg_temp_free_i64 ( tmp ); <nl>  <nl> - set_cc_static ( s ); <nl> return NO_EXIT ; <nl> } <nl> 
static void dhcp_decode ( const struct bootp_t * bp , int * pmsg_type , <nl> if ( p >= p_end ) <nl> break ; <nl> len = * p ++; <nl> + if ( p + len > p_end ) { <nl> + break ; <nl> + } <nl> DPRINTF (" dhcp : tag =% d len =% d \ n ", tag , len ); <nl>  <nl> switch ( tag ) {
int spapr_populate_pci_devices ( sPAPRPHBState * phb , <nl> uint64_t child ; <nl> uint64_t parent ; <nl> uint64_t size ; <nl> - } __attribute__ (( packed )) ranges [] = { <nl> + } QEMU_PACKED ranges [] = { <nl> { <nl> cpu_to_be32 ( b_ss ( 1 )), cpu_to_be64 ( 0 ), <nl> cpu_to_be64 ( phb -> io_win_addr ),
static int64_t ivshmem_recv_msg ( IVShmemState * s , int * pfd , Error ** errp ) <nl> } while ( n < sizeof ( msg )); <nl>  <nl> * pfd = qemu_chr_fe_get_msgfd (& s -> server_chr ); <nl> - return msg ; <nl> + return le64_to_cpu ( msg ); <nl> } <nl>  <nl> static void ivshmem_recv_setup ( IVShmemState * s , Error ** errp )
static uint32_t get_cmd ( ESPState * s , uint8_t * buf , uint8_t buflen ) <nl> s -> dma_memory_read ( s -> dma_opaque , buf , dmalen ); <nl> } else { <nl> dmalen = s -> ti_size ; <nl> + if ( dmalen > TI_BUFSZ ) { <nl> + return 0 ; <nl> + } <nl> memcpy ( buf , s -> ti_buf , dmalen ); <nl> buf [ 0 ] = buf [ 2 ] >> 5 ; <nl> }
static TCGArg * tcg_constant_folding ( TCGContext * s , uint16_t * tcg_opc_ptr , <nl> switch ( op ) { <nl> CASE_OP_32_64 ( or ): <nl> CASE_OP_32_64 ( and ): <nl> - if ( args [ 1 ] == args [ 2 ]) { <nl> + if ( temps_are_copies ( args [ 1 ], args [ 2 ])) { <nl> if ( temps_are_copies ( args [ 0 ], args [ 1 ])) { <nl> gen_opc_buf [ op_index ] = INDEX_op_nop ; <nl> } else {
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static void machvirt_init ( MachineState * machine ) <nl> } <nl>  <nl> object_property_set_bool ( cpuobj , true , " realized ", NULL ); <nl> + object_unref ( cpuobj ); <nl> } <nl> fdt_add_timer_nodes ( vms ); <nl> fdt_add_cpu_nodes ( vms );
static void external_snapshot_prepare ( BlkTransactionState * common , <nl> return ; <nl> } <nl>  <nl> - if ( has_snapshot_node_name && bdrv_find_node ( snapshot_node_name )) { <nl> - error_setg ( errp , " New snapshot node name already existing "); <nl> + if ( has_snapshot_node_name && <nl> + bdrv_lookup_bs ( snapshot_node_name , snapshot_node_name , NULL )) { <nl> + error_setg ( errp , " New snapshot node name already in use "); <nl> return ; <nl> } <nl> 
static void scsi_write_same_complete ( void * opaque , int ret ) <nl> data -> sector << BDRV_SECTOR_BITS , <nl> & data -> qiov , 0 , <nl> scsi_write_same_complete , data ); <nl> + aio_context_release ( blk_get_aio_context ( s -> qdev . conf . blk )); <nl> return ; <nl> } <nl> 
static void replay_save_event ( Event * event , int checkpoint ) <nl> replay_event_char_read_save ( event -> opaque ); <nl> break ; <nl> default : <nl> - error_report (" Unknown ID % d of replay event ", read_event_kind ); <nl> + error_report (" Unknown ID %" PRId64 " of replay event ", event -> id ); <nl> exit ( 1 ); <nl> } <nl> }
static void blkverify_err ( BlkverifyAIOCB * acb , const char * fmt , ...) <nl> va_list ap ; <nl>  <nl> va_start ( ap , fmt ); <nl> - fprintf ( stderr , " blkverify : % s sector_num =% ld nb_sectors =% d ", <nl> + fprintf ( stderr , " blkverify : % s sector_num =%" PRId64 " nb_sectors =% d ", <nl> acb -> is_write ? " write " : " read ", acb -> sector_num , <nl> acb -> nb_sectors ); <nl> vfprintf ( stderr , fmt , ap );
void AcpiCpuHotplug_add ( ACPIGPE * gpe , AcpiCpuHotplug * g , CPUState * cpu ) <nl>  <nl> * gpe -> sts = * gpe -> sts | ACPI_CPU_HOTPLUG_STATUS ; <nl> cpu_id = k -> get_arch_id ( CPU ( cpu )); <nl> + g_assert (( cpu_id / 8 ) < ACPI_GPE_PROC_LEN ); <nl> g -> sts [ cpu_id / 8 ] |= ( 1 << ( cpu_id % 8 )); <nl> } <nl> 
static inline abi_long host_to_target_sockaddr ( abi_ulong target_addr , <nl> if ( len == 0 ) { <nl> return 0 ; <nl> } <nl> + assert ( addr ); <nl>  <nl> target_saddr = lock_user ( VERIFY_WRITE , target_addr , len , 0 ); <nl> if (! target_saddr )
static int vhost_user_read ( struct vhost_dev * dev , VhostUserMsg * msg ) <nl>  <nl> r = qemu_chr_fe_read_all ( chr , p , size ); <nl> if ( r != size ) { <nl> - error_report (" Failed to read msg header . Read % d instead of % d .", r , <nl> - size ); <nl> + error_report (" Failed to read msg header . Read % d instead of % d ." <nl> + " Original request % d .", r , size , msg -> request ); <nl> goto fail ; <nl> } <nl> 
void memory_mapping_filter ( MemoryMappingList * list , int64_t begin , <nl> if ( cur -> phys_addr >= begin + length || <nl> cur -> phys_addr + cur -> length <= begin ) { <nl> QTAILQ_REMOVE (& list -> head , cur , next ); <nl> + g_free ( cur ); <nl> list -> num --; <nl> continue ; <nl> }
static uint64_t uart_read ( void * opaque , hwaddr addr , <nl> r = s -> regs [ R_RXTX ]; <nl> s -> regs [ R_LSR ] &= ~ LSR_DR ; <nl> uart_update_irq ( s ); <nl> + qemu_chr_accept_input ( s -> chr ); <nl> break ; <nl> case R_IIR : <nl> case R_LSR :
static void build_pci_bus_end ( PCIBus * bus , void * bus_state ) <nl> * to make acpi tables compatible with legacy machine types . <nl> */ <nl> if (! child -> pcihp_bridge_en && bus -> parent_dev ) { <nl> + build_free_array ( bus_table ); <nl> + build_pci_bus_state_cleanup ( child ); <nl> + g_free ( child ); <nl> return ; <nl> } <nl> 
static int qdev_add_one_global ( QemuOpts * opts , void * opaque ) <nl> g -> driver = qemu_opt_get ( opts , " driver "); <nl> g -> property = qemu_opt_get ( opts , " property "); <nl> g -> value = qemu_opt_get ( opts , " value "); <nl> - oc = object_class_by_name ( g -> driver ); <nl> + oc = object_class_dynamic_cast ( object_class_by_name ( g -> driver ), <nl> + TYPE_DEVICE ); <nl> if ( oc ) { <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl> 
static void test_interface_impl ( const char * type ) <nl>  <nl> g_assert ( iobj ); <nl> g_assert ( ioc -> test == PATTERN ); <nl> + object_unref ( obj ); <nl> } <nl>  <nl> static void interface_direct_test ( void )
int net_init_vhost_user ( const NetClientOptions * opts , const char * name , <nl> } <nl>  <nl> queues = vhost_user_opts -> has_queues ? vhost_user_opts -> queues : 1 ; <nl> + if ( queues < 1 ) { <nl> + error_setg ( errp , <nl> + " vhost - user number of queues must be bigger than zero "); <nl> + return - 1 ; <nl> + } <nl>  <nl> return net_vhost_user_init ( peer , " vhost_user ", name , chr , queues ); <nl> }
static void multiwrite_cb ( void * opaque , int ret ) <nl> { <nl> MultiwriteCB * mcb = opaque ; <nl>  <nl> - if ( ret < 0 ) { <nl> + if ( ret < 0 && ! mcb -> error ) { <nl> mcb -> error = ret ; <nl> multiwrite_user_cb ( mcb ); <nl> }
static QTAILQ_HEAD (, Rom ) roms = QTAILQ_HEAD_INITIALIZER ( roms ); <nl>  <nl> static inline bool rom_order_compare ( Rom * rom , Rom * item ) <nl> { <nl> - return ( rom -> as > item -> as ) || <nl> + return (( uintptr_t )( void *) rom -> as > ( uintptr_t )( void *) item -> as ) || <nl> ( rom -> as == item -> as && rom -> addr >= item -> addr ); <nl> } <nl> 
static inline abi_long do_semctl ( int semid , int semnum , int cmd , <nl> { <nl> union semun arg ; <nl> struct semid_ds dsarg ; <nl> - unsigned short * array ; <nl> + unsigned short * array = NULL ; <nl> struct seminfo seminfo ; <nl> abi_long ret = - TARGET_EINVAL ; <nl> abi_long err ;
Coroutine * qemu_coroutine_new ( void ) <nl> stack_t oss ; <nl> sigset_t sigs ; <nl> sigset_t osigs ; <nl> - jmp_buf old_env ; <nl> + sigjmp_buf old_env ; <nl>  <nl> /* The way to manipulate stack is with the sigaltstack function . We <nl> * prepare a stack , with it delivering a signal to ourselves and then
static BlockBackend * img_open_opts ( const char * optstr , <nl> if ( qdict_haskey ( options , BDRV_OPT_FORCE_SHARE ) <nl> && ! qdict_get_bool ( options , BDRV_OPT_FORCE_SHARE )) { <nl> error_report ("-- force - share /- U conflicts with image options "); <nl> + QDECREF ( options ); <nl> return NULL ; <nl> } <nl> qdict_put ( options , BDRV_OPT_FORCE_SHARE , qbool_from_bool ( true ));
static uint64_t pci_host_data_read ( void * opaque , <nl> { <nl> PCIHostState * s = opaque ; <nl> uint32_t val ; <nl> - if (!( s -> config_reg & ( 1 << 31 ))) <nl> + if (!( s -> config_reg & ( 1U << 31 ))) { <nl> return 0xffffffff ; <nl> + } <nl> val = pci_data_read ( s -> bus , s -> config_reg | ( addr & 3 ), len ); <nl> PCI_DPRINTF (" read addr " TARGET_FMT_plx " len % d val % x \ n ", <nl> addr , len , val );
static int bdrv_open_common ( BlockDriverState * bs , BlockDriverState * file , <nl> ret = - EINVAL ; <nl> goto free_and_fail ; <nl> } <nl> - assert ( file != NULL ); <nl> bs -> file = file ; <nl> ret = drv -> bdrv_open ( bs , options , open_flags ); <nl> }
static void ivshmem_read ( void * opaque , const uint8_t * buf , int size ) <nl> if ( incoming_posn == - 1 ) { <nl> void * map_ptr ; <nl>  <nl> + if ( s -> shm_fd >= 0 ) { <nl> + error_report (" shm already initialized "); <nl> + close ( incoming_fd ); <nl> + return ; <nl> + } <nl> + <nl> if ( check_shm_size ( s , incoming_fd , & err ) == - 1 ) { <nl> error_report_err ( err ); <nl> close ( incoming_fd );
void tcg_gen_ld8u_i64 ( TCGv_i64 ret , TCGv_ptr arg2 , tcg_target_long offset ) <nl> void tcg_gen_ld8s_i64 ( TCGv_i64 ret , TCGv_ptr arg2 , tcg_target_long offset ) <nl> { <nl> tcg_gen_ld8s_i32 ( TCGV_LOW ( ret ), arg2 , offset ); <nl> - tcg_gen_sari_i32 ( TCGV_HIGH ( ret ), TCGV_HIGH ( ret ), 31 ); <nl> + tcg_gen_sari_i32 ( TCGV_HIGH ( ret ), TCGV_LOW ( ret ), 31 ); <nl> } <nl>  <nl> void tcg_gen_ld16u_i64 ( TCGv_i64 ret , TCGv_ptr arg2 , tcg_target_long offset )
static int htab_save_iterate ( QEMUFile * f , void * opaque ) <nl> /* Iteration header */ <nl> if (! spapr -> htab_shift ) { <nl> qemu_put_be32 ( f , - 1 ); <nl> - return 0 ; <nl> + return 1 ; <nl> } else { <nl> qemu_put_be32 ( f , 0 ); <nl> }
static void vhost_dev_sync_region ( struct vhost_dev * dev , <nl> log = __sync_fetch_and_and ( from , 0 ); <nl> while (( bit = sizeof ( log ) > sizeof ( int ) ? <nl> ffsll ( log ) : ffs ( log ))) { <nl> + ram_addr_t ram_addr ; <nl> bit -= 1 ; <nl> - cpu_physical_memory_set_dirty ( addr + bit * VHOST_LOG_PAGE ); <nl> + ram_addr = cpu_get_physical_page_desc ( addr + bit * VHOST_LOG_PAGE ); <nl> + cpu_physical_memory_set_dirty ( ram_addr ); <nl> log &= ~( 0x1ull << bit ); <nl> } <nl> addr += VHOST_LOG_CHUNK ;
char * socket_address_to_string ( struct SocketAddress * addr , Error ** errp ) <nl>  <nl> SocketAddress * socket_address_flatten ( SocketAddressLegacy * addr_legacy ) <nl> { <nl> - SocketAddress * addr = g_new ( SocketAddress , 1 ); <nl> + SocketAddress * addr ; <nl>  <nl> if (! addr_legacy ) { <nl> return NULL ; <nl> } <nl>  <nl> + addr = g_new ( SocketAddress , 1 ); <nl> + <nl> switch ( addr_legacy -> type ) { <nl> case SOCKET_ADDRESS_LEGACY_KIND_INET : <nl> addr -> type = SOCKET_ADDRESS_TYPE_INET ;
static void tap_cleanup ( VLANClientState * nc ) <nl>  <nl> if ( s -> vhost_net ) { <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> + s -> vhost_net = NULL ; <nl> } <nl>  <nl> qemu_purge_queued_packets ( nc );
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> /* libc does special remapping of the return value of <nl> * sys_getpriority () so it ' s just easiest to call <nl> * sys_getpriority () directly rather than through libc . */ <nl> - ret = sys_getpriority ( arg1 , arg2 ); <nl> + ret = get_errno ( sys_getpriority ( arg1 , arg2 )); <nl> break ; <nl> case TARGET_NR_setpriority : <nl> ret = get_errno ( setpriority ( arg1 , arg2 , arg3 ));
static void test_io_channel_tls ( const void * opaque ) <nl> mainloop = g_main_context_default (); <nl> do { <nl> g_main_context_iteration ( mainloop , TRUE ); <nl> - } while (! clientHandshake . finished && <nl> + } while (! clientHandshake . finished || <nl> ! serverHandshake . finished ); <nl>  <nl> g_assert ( clientHandshake . failed == data -> expectClientFail );
static int img_convert ( int argc , char ** argv ) <nl> ret = bdrv_parse_cache_flags ( cache , & flags ); <nl> if ( ret < 0 ) { <nl> error_report (" Invalid cache option : % s ", cache ); <nl> - return - 1 ; <nl> + goto out ; <nl> } <nl>  <nl> out_bs = bdrv_new_open (" target ", out_filename , out_fmt , flags , true , quiet );
static void xen_pci_passthrough_class_init ( ObjectClass * klass , void * data ) <nl> k -> exit = xen_pt_unregister_device ; <nl> k -> config_read = xen_pt_pci_read_config ; <nl> k -> config_write = xen_pt_pci_write_config ; <nl> + k -> is_express = 1 ; /* We might be */ <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> desc = " Assign an host PCI device with Xen "; <nl> dc -> props = xen_pci_passthrough_properties ;
bool bdrv_dev_is_medium_locked ( BlockDriverState * bs ) <nl> */ <nl> int bdrv_check ( BlockDriverState * bs , BdrvCheckResult * res , BdrvCheckMode fix ) <nl> { <nl> + if ( bs -> drv == NULL ) { <nl> + return - ENOMEDIUM ; <nl> + } <nl> if ( bs -> drv -> bdrv_check == NULL ) { <nl> return - ENOTSUP ; <nl> }
ram_addr_t qemu_ram_alloc_from_ptr ( ram_addr_t size , void * host , <nl>  <nl> qemu_ram_setup_dump ( new_block -> host , size ); <nl> qemu_madvise ( new_block -> host , size , QEMU_MADV_HUGEPAGE ); <nl> + qemu_madvise ( new_block -> host , size , QEMU_MADV_DONTFORK ); <nl>  <nl> if ( kvm_enabled ()) <nl> kvm_setup_guest_memory ( new_block -> host , size );
static void rc4030_write ( void * opaque , hwaddr addr , uint64_t data , <nl> break ; <nl> /* Interval timer reload */ <nl> case 0x0228 : <nl> - s -> itr = val ; <nl> + s -> itr = val & 0x01FF ; <nl> qemu_irq_lower ( s -> timer_irq ); <nl> set_next_tick ( s ); <nl> break ;
typedef struct QObject { <nl>  <nl> /* High - level interface for qobject_decref () */ <nl> # define QDECREF ( obj ) \ <nl> - qobject_decref ( QOBJECT ( obj )) <nl> + qobject_decref ( obj ? QOBJECT ( obj ) : NULL ) <nl>  <nl> /* Initialize an object to default values */ <nl> # define QOBJECT_INIT ( obj , qtype_type ) \
restart : <nl> QLIST_REMOVE ( elem , all ); <nl> /* Read state before ret . */ <nl> smp_rmb (); <nl> + <nl> + /* Schedule ourselves in case elem -> common . cb () calls aio_poll () to <nl> + * wait for another request that completed at the same time . <nl> + */ <nl> + qemu_bh_schedule ( pool -> completion_bh ); <nl> + <nl> elem -> common . cb ( elem -> common . opaque , elem -> ret ); <nl> qemu_aio_release ( elem ); <nl> goto restart ;
QEMUOptionParameter * append_option_parameters ( QEMUOptionParameter * dest , <nl> num_options += count_option_parameters ( list ); <nl>  <nl> dest = qemu_realloc ( dest , ( num_options + 1 ) * sizeof ( QEMUOptionParameter )); <nl> + dest [ num_dest_options ]. name = NULL ; <nl>  <nl> while ( list && list -> name ) { <nl> if ( get_option_parameter ( dest , list -> name ) == NULL ) {
IOCTL ( BLKFLSBUF , 0 , TYPE_NULL ) <nl> IOCTL ( BLKRASET , 0 , TYPE_INT ) <nl> IOCTL ( BLKRAGET , IOC_R , MK_PTR ( TYPE_LONG )) <nl> - IOCTL ( BLKSSZGET , IOC_R , MK_PTR ( TYPE_LONG )) <nl> + IOCTL ( BLKSSZGET , IOC_R , MK_PTR ( TYPE_INT )) <nl> IOCTL ( BLKBSZGET , IOC_R , MK_PTR ( TYPE_INT )) <nl> IOCTL_SPECIAL ( BLKPG , IOC_W , do_ioctl_blkpg , <nl> MK_PTR ( MK_STRUCT ( STRUCT_blkpg_ioctl_arg )))
static int usbredir_post_load ( void * priv , int version_id ) <nl> { <nl> USBRedirDevice * dev = priv ; <nl>  <nl> + if ( dev -> parser == NULL ) { <nl> + return 0 ; <nl> + } <nl> + <nl> switch ( dev -> device_info . speed ) { <nl> case usb_redir_speed_low : <nl> dev -> dev . speed = USB_SPEED_LOW ;
static int connect_to_ssh ( BDRVSSHState * s , QDict * options , <nl> /* Open the socket and connect . */ <nl> s -> sock = inet_connect ( s -> hostport , errp ); <nl> if ( s -> sock < 0 ) { <nl> - ret = - errno ; <nl> + ret = - EIO ; <nl> goto err ; <nl> } <nl> 
static void set_pixel_format ( VncState * vs , <nl> return ; <nl> } <nl>  <nl> + switch ( bits_per_pixel ) { <nl> + case 8 : <nl> + case 16 : <nl> + case 32 : <nl> + break ; <nl> + default : <nl> + vnc_client_error ( vs ); <nl> + return ; <nl> + } <nl> + <nl> vs -> client_pf . rmax = red_max ; <nl> vs -> client_pf . rbits = hweight_long ( red_max ); <nl> vs -> client_pf . rshift = red_shift ;
int paio_init ( void ) <nl> s -> first_aio = NULL ; <nl> if ( qemu_pipe ( fds ) == - 1 ) { <nl> fprintf ( stderr , " failed to create pipe \ n "); <nl> + g_free ( s ); <nl> return - 1 ; <nl> } <nl> 
ivshmem_server_handle_new_conn ( IvshmemServer * server ) <nl> } <nl> if ( i == G_MAXUINT16 ) { <nl> IVSHMEM_SERVER_DEBUG ( server , " cannot allocate new client id \ n "); <nl> - goto fail ; <nl> + close ( newfd ); <nl> + g_free ( peer ); <nl> + return - 1 ; <nl> } <nl> peer -> id = server -> cur_id ++; <nl> 
static void tcp_chr_tls_init ( CharDriverState * chr ) <nl> if ( tioc == NULL ) { <nl> error_free ( err ); <nl> tcp_chr_disconnect ( chr ); <nl> + return ; <nl> } <nl> object_unref ( OBJECT ( s -> ioc )); <nl> s -> ioc = QIO_CHANNEL ( tioc );
static inline void gen_illegal_opcode ( DisasContext * s ) <nl> gen_program_exception ( s , PGM_SPECIFICATION ); <nl> } <nl>  <nl> - static inline void check_privileged ( DisasContext * s ) <nl> +# ifndef CONFIG_USER_ONLY <nl> + static void check_privileged ( DisasContext * s ) <nl> { <nl> if ( s -> tb -> flags & ( PSW_MASK_PSTATE >> 32 )) { <nl> gen_program_exception ( s , PGM_PRIVILEGED ); <nl> } <nl> } <nl> +# endif <nl>  <nl> static TCGv_i64 get_address ( DisasContext * s , int x2 , int b2 , int d2 ) <nl> {
static void cleanup_infolist ( CommandLineParameterInfoList * head ) <nl> if (! strcmp ( pre_entry -> value -> name , cur -> next -> value -> name )) { <nl> del_entry = cur -> next ; <nl> cur -> next = cur -> next -> next ; <nl> - g_free ( del_entry ); <nl> + del_entry -> next = NULL ; <nl> + qapi_free_CommandLineParameterInfoList ( del_entry ); <nl> break ; <nl> } <nl> pre_entry = pre_entry -> next ;
 <nl> # define DRC_CONTAINER_PATH "/ dr - connector " <nl> # define DRC_INDEX_TYPE_SHIFT 28 <nl> -# define DRC_INDEX_ID_MASK (~(~ 0 << DRC_INDEX_TYPE_SHIFT )) <nl> +# define DRC_INDEX_ID_MASK (( 1ULL << DRC_INDEX_TYPE_SHIFT ) - 1 ) <nl>  <nl> static sPAPRDRConnectorTypeShift get_type_shift ( sPAPRDRConnectorType type ) <nl> {
static int kvm_physical_sync_dirty_bitmap ( MemoryRegionSection * section ) <nl>  <nl> d . slot = mem -> slot ; <nl>  <nl> - if ( kvm_vm_ioctl ( s , KVM_GET_DIRTY_LOG , & d ) == - 1 ) { <nl> + if ( kvm_vm_ioctl ( s , KVM_GET_DIRTY_LOG , & d ) < 0 ) { <nl> DPRINTF (" ioctl failed % d \ n ", errno ); <nl> ret = - 1 ; <nl> break ;
static int check_directory_consistency ( BDRVVVFATState * s , <nl>  <nl> if ( s -> used_clusters [ cluster_num ] & USED_ANY ) { <nl> fprintf ( stderr , " cluster % d used more than once \ n ", ( int ) cluster_num ); <nl> - return 0 ; <nl> + goto fail ; <nl> } <nl> s -> used_clusters [ cluster_num ] = USED_DIRECTORY ; <nl> 
static void fsl_imx6_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx6_realize ; <nl> - <nl> dc -> desc = " i . MX6 SOC "; <nl> + /* Reason : Uses serial_hds [] in the realize () function */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx6_type_info = {
static TRBCCode xhci_disable_ep ( XHCIState * xhci , unsigned int slotid , <nl> usb_packet_cleanup (& epctx -> transfers [ i ]. packet ); <nl> } <nl>  <nl> - xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + /* only touch guest RAM if we ' re not resetting the HC */ <nl> + if ( xhci -> dcbaap_low || xhci -> dcbaap_high ) { <nl> + xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + } <nl>  <nl> timer_free ( epctx -> kick_timer ); <nl> g_free ( epctx );
static int iscsi_truncate ( BlockDriverState * bs , int64_t offset ) <nl> if ( iscsilun -> allocationmap != NULL ) { <nl> g_free ( iscsilun -> allocationmap ); <nl> iscsilun -> allocationmap = <nl> - bitmap_new ( DIV_ROUND_UP ( bs -> total_sectors , <nl> + bitmap_new ( DIV_ROUND_UP ( sector_lun2qemu ( iscsilun -> num_blocks , <nl> + iscsilun ), <nl> iscsilun -> cluster_sectors )); <nl> } <nl> 
void cpu_x86_cpuid ( CPUX86State * env , uint32_t index , uint32_t count , <nl> index = env -> cpuid_xlevel ; <nl> } <nl> } else { <nl> - index = env -> cpuid_xlevel ; <nl> + /* Intel documentation states that invalid EAX input will <nl> + * return the same information as EAX = cpuid_level <nl> + * ( Intel SDM Vol . 2A - Instruction Set Reference - CPUID ) <nl> + */ <nl> + index = env -> cpuid_level ; <nl> } <nl> } <nl> } else {
static void vscsi_report_luns ( VSCSIState * s , vscsi_req * req ) <nl> len = n + 8 ; <nl>  <nl> resp_data = g_malloc0 ( len ); <nl> - memset ( resp_data , 0 , len ); <nl> stl_be_p ( resp_data , n ); <nl> i = found_lun0 ? 8 : 16 ; <nl> QTAILQ_FOREACH ( kid , & s -> bus . qbus . children , sibling ) {
static int kvm_set_user_memory_region ( KVMState * s , KVMSlot * slot ) <nl> if ( s -> migration_log ) { <nl> mem . flags |= KVM_MEM_LOG_DIRTY_PAGES ; <nl> } <nl> - if ( mem . flags & KVM_MEM_READONLY ) { <nl> + <nl> + if ( slot -> memory_size && mem . flags & KVM_MEM_READONLY ) { <nl> /* Set the slot size to 0 before setting the slot to the desired <nl> * value . This is needed based on KVM commit 75d61fbc . */ <nl> mem . memory_size = 0 ;
static int vfio_msix_vector_do_use ( PCIDevice * pdev , unsigned int nr , <nl> vfio_update_kvm_msi_virq ( vector , * msg , pdev ); <nl> } <nl> } else { <nl> - vfio_add_kvm_msi_virq ( vdev , vector , nr , true ); <nl> + if ( msg ) { <nl> + vfio_add_kvm_msi_virq ( vdev , vector , nr , true ); <nl> + } <nl> } <nl>  <nl> /*
void qemu_input_event_send_key ( QemuConsole * src , KeyValue * key , bool down ) <nl> } else if ( queue_count < queue_limit ) { <nl> qemu_input_queue_event (& kbd_queue , src , evt ); <nl> qemu_input_queue_sync (& kbd_queue ); <nl> + } else { <nl> + qapi_free_InputEvent ( evt ); <nl> } <nl> } <nl> 
typedef struct CPUX86State { <nl> uint8_t has_error_code ; <nl> uint32_t sipi_vector ; <nl> bool tsc_valid ; <nl> - int tsc_khz ; <nl> + int64_t tsc_khz ; <nl> void * kvm_xsave_buf ; <nl>  <nl> uint64_t mcg_cap ;
void block_job_set_speed ( BlockJob * job , int64_t speed , Error ** errp ) <nl> } <nl>  <nl> job -> speed = speed ; <nl> - if ( speed <= old_speed ) { <nl> + if ( speed && speed <= old_speed ) { <nl> return ; <nl> } <nl> 
CharDriverState * qemu_chr_new_from_opts ( QemuOpts * opts , <nl> if ( i == NULL ) { <nl> error_setg ( errp , " chardev : backend \"% s \" not found ", <nl> qemu_opt_get ( opts , " backend ")); <nl> - return NULL ; <nl> + goto err ; <nl> } <nl>  <nl> if (! cd -> open ) {
static void disas_arm_insn ( DisasContext * s , unsigned int insn ) <nl> ARCH ( 6T2 ); <nl> shift = ( insn >> 7 ) & 0x1f ; <nl> i = ( insn >> 16 ) & 0x1f ; <nl> + if ( i < shift ) { <nl> + /* UNPREDICTABLE ; we choose to UNDEF */ <nl> + goto illegal_op ; <nl> + } <nl> i = i + 1 - shift ; <nl> if ( rm == 15 ) { <nl> tmp = tcg_temp_new_i32 ();
static void spapr_add_lmbs ( DeviceState * dev , uint64_t addr , uint64_t size , <nl>  <nl> drck = SPAPR_DR_CONNECTOR_GET_CLASS ( drc ); <nl> drck -> attach ( drc , dev , fdt , fdt_offset , ! dev -> hotplugged , errp ); <nl> - spapr_hotplug_req_add_by_index ( drc ); <nl> addr += SPAPR_MEMORY_BLOCK_SIZE ; <nl> } <nl> + spapr_hotplug_req_add_by_count ( SPAPR_DR_CONNECTOR_TYPE_LMB , nr_lmbs ); <nl> } <nl>  <nl> static void spapr_memory_plug ( HotplugHandler * hotplug_dev , DeviceState * dev ,
static void become_daemon ( const char * pidfile ) <nl> goto fail ; <nl> } <nl>  <nl> - close ( STDIN_FILENO ); <nl> - close ( STDOUT_FILENO ); <nl> - close ( STDERR_FILENO ); <nl> + reopen_fd_to_null ( STDIN_FILENO ); <nl> + reopen_fd_to_null ( STDOUT_FILENO ); <nl> + reopen_fd_to_null ( STDERR_FILENO ); <nl> return ; <nl>  <nl> fail :
int ppc_compat_max_threads ( PowerPCCPU * cpu ); <nl> # define SPR_601_UDECR ( 0x006 ) <nl> # define SPR_LR ( 0x008 ) <nl> # define SPR_CTR ( 0x009 ) <nl> -# define SPR_UAMR ( 0x00C ) <nl> +# define SPR_UAMR ( 0x00D ) <nl> # define SPR_DSCR ( 0x011 ) <nl> # define SPR_DSISR ( 0x012 ) <nl> # define SPR_DAR ( 0x013 ) /* DAE for PowerPC 601 */
static void cpu_common_reset ( CPUState * cpu ) <nl> log_cpu_state ( cpu , cc -> reset_dump_flags ); <nl> } <nl>  <nl> - cpu -> exit_request = 0 ; <nl> cpu -> interrupt_request = 0 ; <nl> cpu -> current_tb = NULL ; <nl> cpu -> halted = 0 ;
static void ivshmem_read ( void * opaque , const uint8_t * buf , int size ) <nl> if ( incoming_fd == - 1 ) { <nl> fprintf ( stderr , " could not allocate file descriptor % s \ n ", <nl> strerror ( errno )); <nl> + close ( tmp_fd ); <nl> return ; <nl> } <nl> 
static int vmdk_snapshot_create ( const char * filename , const char * backing_file ) <nl> ret = - errno ; <nl> goto fail_rgd ; <nl> } <nl> - qemu_free ( rgd_buf ); <nl>  <nl> /* write GD */ <nl> gd_buf = qemu_malloc ( gd_size ); <nl> static int vmdk_snapshot_create ( const char * filename , const char * backing_file ) <nl> goto fail_gd ; <nl> } <nl> qemu_free ( gd_buf ); <nl> + qemu_free ( rgd_buf ); <nl>  <nl> close ( p_fd ); <nl> close ( snp_fd );
uint32_t gic_acknowledge_irq ( GICState * s , int cpu ) <nl> } <nl> s -> last_active [ irq ][ cpu ] = s -> running_irq [ cpu ]; <nl>  <nl> - if ( s -> revision == REV_11MPCORE ) { <nl> + if ( s -> revision == REV_11MPCORE || s -> revision == REV_NVIC ) { <nl> /* Clear pending flags for both level and edge triggered interrupts . <nl> * Level triggered IRQs will be reasserted once they become inactive . <nl> */
GEN_HANDLER ( tlbiva , 0x1F , 0x12 , 0x18 , 0x03FFF801 , PPC_TLBIVA ) <nl> GEN_EXCP_PRIVOPC ( ctx ); <nl> return ; <nl> } <nl> + t0 = tcg_temp_new (); <nl> gen_addr_reg_index ( t0 , ctx ); <nl> # if defined ( TARGET_PPC64 ) <nl> if (! ctx -> sf_mode )
mgt_run ( int dflag , const char * T_arg ) <nl>  <nl> setproctitle (" Varnish - Mgr % s ", heritage . name ); <nl>  <nl> + memset (& sac , 0 , sizeof sac ); <nl> sac . sa_handler = SIG_IGN ; <nl> sac . sa_flags = SA_RESTART ; <nl> 
struct mg_context * mg_start ( mg_callback_t user_callback , const char ** options ) { <nl> ctx = calloc ( 1 , sizeof (* ctx )); <nl> ctx -> user_callback = user_callback ; <nl>  <nl> - while (( name = * options ++) != NULL ) { <nl> + while ( options && ( name = * options ++) != NULL ) { <nl> if (( i = get_option_index ( name )) == - 1 ) { <nl> cry ( fc ( ctx ), " Invalid option : % s ", name ); <nl> free_context ( ctx );
struct uwsgi_stats * uwsgi_master_generate_stats () { <nl> uc = uc -> next ; <nl> } <nl>  <nl> + if ( uwsgi_stats_list_close ( us )) <nl> + goto end ; <nl> + <nl> if ( uwsgi_stats_comma ( us )) <nl> goto end ; <nl> }
void delete_all_wml_hotkeys () <nl> } <nl> } <nl>  <nl> -// retunrs weather a hotkey was deleted . <nl> +// Returns whether a hotkey was deleted . <nl> bool remove_wml_hotkey ( const std :: string & id ) <nl> { <nl> hotkey :: hotkey_command & command = get_hotkey_command ( id );
std :: vector < std :: string > get_text () { <nl> " _ " N_ ("+ Catalan Translation "), <nl> "- Carles Company ( brrr )", <nl> "- Dan Rosàs Garcia ( focks )", <nl> + "- Jonatan Alamà ( tin )", <nl> "- Jordà Polo ( ettin )", <nl> "- Mark Recasens ", <nl> "- Pau Rul · lan Ferragut ",
process ( register int code , unsigned char ** fill ) <nl> } <nl>  <nl> if ( oldcode == - 1 ) { <nl> + if ( code >= clear ) { <nl> + fprintf ( stderr , " bad input : code =% d is larger than clear =% d \ n ", code , clear ); <nl> + return 0 ; <nl> + } <nl> *(* fill )++ = suffix [ code ]; <nl> firstchar = oldcode = code ; <nl> return 1 ;
static int readContigStripsIntoBuffer ( TIFF * in , uint8 * buf ) <nl> ( unsigned long ) strip , ( unsigned long ) rows ); <nl> return 0 ; <nl> } <nl> - bufp += bytes_read ; <nl> + bufp += stripsize ; <nl> } <nl>  <nl> return 1 ;
static int unit_file_search ( <nl>  <nl> _cleanup_free_ char * template = NULL ; <nl> _cleanup_strv_free_ char ** dirs = NULL ; <nl> - _cleanup_free_ char ** files = NULL ; <nl> + _cleanup_strv_free_ char ** files = NULL ; <nl> const char * dropin_dir_name = NULL ; <nl> const char * dropin_template_dir_name = NULL ; <nl> 
_public_ int sd_pid_get_machine_name ( pid_t pid , char ** name ) { <nl>  <nl> _public_ int sd_pid_get_owner_uid ( pid_t pid , uid_t * uid ) { <nl> int r ; <nl> - _cleanup_free_ char * root = NULL , * cgroup = NULL , * p = NULL , * cc = NULL ; <nl> + _cleanup_free_ char * root = NULL , * cgroup = NULL , * cc = NULL ; <nl> + char * p ; <nl> struct stat st ; <nl>  <nl> if ( pid < 0 )
static void item_free ( Item * i ) { <nl> free ( i -> uid_path ); <nl> free ( i -> gid_path ); <nl> free ( i -> description ); <nl> + free ( i -> home ); <nl> free ( i ); <nl> } <nl> 
static int get_key ( struct udev * udev , char ** line , char ** key , enum operation_ty <nl> char * temp ; <nl>  <nl> linepos = * line ; <nl> - if ( linepos == NULL && linepos [ 0 ] == '\ 0 ') <nl> + if ( linepos == NULL || linepos [ 0 ] == '\ 0 ') <nl> return - 1 ; <nl>  <nl> /* skip whitespace */
int cunescape_length_with_prefix ( const char * s , size_t length , const char * prefi <nl> continue ; <nl> } <nl>  <nl> + free ( r ); <nl> return - EINVAL ; <nl> } <nl>  <nl> int cunescape_length_with_prefix ( const char * s , size_t length , const char * prefi <nl> continue ; <nl> } <nl>  <nl> + free ( r ); <nl> return k ; <nl> } <nl> 
static uint32_t term_color_to_argb32 ( const term_color * color , const term_attr * a <nl> case TERM_CCODE_BLACK ... TERM_CCODE_LIGHT_WHITE : <nl> t = color -> ccode - TERM_CCODE_BLACK ; <nl>  <nl> - /* bold causes light colors */ <nl> - if ( t < 8 && attr -> bold ) <nl> + /* bold causes light colors ( only for foreground colors ) */ <nl> + if ( t < 8 && attr -> bold && color == & attr -> fg ) <nl> t += 8 ; <nl>  <nl> r = palette [ t * 3 + 0 ];
static int str_compare ( const void * _a , const void * _b ) { <nl> } <nl>  <nl> char ** strv_sort ( char ** l ) { <nl> - <nl> - if ( strv_isempty ( l )) <nl> - return l ; <nl> - <nl> - qsort ( l , strv_length ( l ), sizeof ( char *), str_compare ); <nl> + qsort_safe ( l , strv_length ( l ), sizeof ( char *), str_compare ); <nl> return l ; <nl> } <nl> 
static struct node * bus_node_allocate ( sd_bus * bus , const char * path ) { <nl> e = strrchr ( path , '/'); <nl> assert ( e ); <nl>  <nl> - p = strndupa ( path , MAX ( 1 , path - e )); <nl> + p = strndupa ( path , MAX ( 1 , e - path )); <nl>  <nl> parent = bus_node_allocate ( bus , p ); <nl> if (! parent )
int fstab_find_pri ( const char * options , int * ret ) { <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> + if (( int ) pri < 0 ) <nl> + return - ERANGE ; <nl> + <nl> * ret = ( int ) r ; <nl> return 1 ; <nl> }
_public_ int sd_bus_try_close ( sd_bus * bus ) { <nl> assert_return (! bus_pid_changed ( bus ), - ECHILD ); <nl> assert_return ( bus -> is_kernel , - ENOTSUP ); <nl>  <nl> + if ( bus -> rqueue_size > 0 ) <nl> + return - EBUSY ; <nl> + <nl> r = bus_kernel_try_close ( bus ); <nl> if ( r < 0 ) <nl> return r ;
int dnssec_verify_rrset ( <nl> } <nl>  <nl> /* Collect all relevant RRs in a single array , so that we can look at the RRset */ <nl> - list = newa ( DnsResourceRecord *, a -> n_rrs ); <nl> + list = newa ( DnsResourceRecord *, dns_answer_size ( a )); <nl>  <nl> DNS_ANSWER_FOREACH ( rr , a ) { <nl> r = dns_resource_key_equal ( key , rr -> key );
static void sigusr2_handler ( int num ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; ais_service [ i ]; i ++) { <nl> - if ( ais_service [ i ]-> exec_dump_fn ) { <nl> + for ( i = 0 ; i < SERVICE_HANDLER_MAXIMUM_COUNT ; i ++) { <nl> + if ( ais_service [ i ] && ais_service [ i ]-> exec_dump_fn ) { <nl> ais_service [ i ]-> exec_dump_fn (); <nl> } <nl> }
wilber_get_extents ( cairo_t * cr ) <nl> cairo_fill_extents ( cr , & wilber_x1 , & wilber_y1 , & wilber_x2 , & wilber_y2 ); <nl>  <nl> wilber_cairo_path = cairo_copy_path ( cr ); <nl> + cairo_new_path ( cr ); <nl>  <nl> cairo_restore ( cr ); <nl> }
xcf_init ( Gimp * gimp ) <nl> " Filename ", <nl> " The name of the file " <nl> " to save the image in , " <nl> - " in the on - disk " <nl> - " character set and " <nl> - " encoding ", <nl> + " in URI format and " <nl> + " UTF - 8 encoding ", <nl> TRUE , FALSE , TRUE , <nl> NULL , <nl> GIMP_PARAM_READWRITE ));
load_image ( const gchar * filename , <nl> gint32 volatile image_ID = - 1 ; <nl> gint32 layer_ID ; <nl> int fd ; /* File descriptor */ <nl> - char buf [ BUFLEN ]; /* buffer for random things like scanning */ <nl> + char buf [ BUFLEN + 4 ]; /* buffer for random things like scanning */ <nl> PNMInfo * pnminfo ; <nl> PNMScanner * volatile scan ; <nl> int ctr ;
int merge_many_buff ( Sort_param * param , uchar * sort_buffer , <nl> ulong read_to_buffer ( IO_CACHE * fromfile , BUFFPEK * buffpek , <nl> uint rec_length ) <nl> { <nl> - register ulong count ; <nl> + ulong count ; <nl> ulong length = 0 ; <nl>  <nl> if (( count = ( ulong ) MY_MIN (( ha_rows ) buffpek -> max_keys , buffpek -> count )))
pstack_install_segv_action ( const char * path_format_ ) <nl> if (( abfd = load_bfd ( pid ))== 0 ) <nl> fprintf ( stderr , " BFD load failed ..\ n "); <nl> else { <nl> - long storage_needed = bfd_get_symtab_upper_bound ( abfd ); <nl> + long storage_needed = ( bfd_get_file_flags ( abfd ) & HAS_SYMS ) ? <nl> + bfd_get_symtab_upper_bound ( abfd ) : 0 ; <nl> long i ; <nl> ( void ) i ; <nl> 
ulong my_scan_8bit ( CHARSET_INFO * cs , const char * str , const char * end , int sq ) <nl> return 0 ; <nl>  <nl> case MY_SEQ_SPACES : <nl> - for ( str ++ ; str != end ; str ++) <nl> + for (; str != end ; str ++) <nl> { <nl> if (! my_isspace ( cs ,* str )) <nl> break ;
btr_create ( <nl> PAGE_HEADER + PAGE_BTR_SEG_LEAF , mtr )) { <nl> /* Not enough space for new segment , free root <nl> segment before return . */ <nl> - fseg_free ( space , page_no , <nl> - PAGE_HEADER + PAGE_BTR_SEG_TOP ); <nl> + btr_free_root ( space , page_no , mtr ); <nl>  <nl> return ( FIL_NULL ); <nl> }
TABLE * Delayed_insert :: get_local_table ( THD * client_thd ) <nl> goto error ; <nl> dfield_ptr = copy -> default_field ; <nl> } <nl> + copy -> expr_arena = NULL ; <nl>  <nl> /* Ensure we don ' t use the table list of the original table */ <nl> copy -> pos_in_table_list = 0 ;
void opj_lupSolve ( OPJ_FLOAT32 * pResult , <nl> lTmpMatrix = lLineMatrix ; <nl> u = *( lTmpMatrix ++); <nl> lCurrentPtr = lDestPtr --; <nl> - for ( j = k + 1 ; j < nb_compo ; ++ j ) { <nl> + for ( j = ( OPJ_UINT32 )( k + 1 ); j < nb_compo ; ++ j ) { <nl> /* sum += matrix [ k ][ j ] * x [ j ] */ <nl> sum += (*( lTmpMatrix ++)) * (*( lCurrentPtr ++)); <nl> }
parse_server_transport_line ( const char * line , int validate_only ) <nl> done : <nl> SMARTLIST_FOREACH ( items , char *, s , tor_free ( s )); <nl> smartlist_free ( items ); <nl> - SMARTLIST_FOREACH ( transport_list , char *, s , tor_free ( s )); <nl> - smartlist_free ( transport_list ); <nl> + if ( transport_list ) { <nl> + SMARTLIST_FOREACH ( transport_list , char *, s , tor_free ( s )); <nl> + smartlist_free ( transport_list ); <nl> + } <nl>  <nl> return r ; <nl> }
tor_tls_check_lifetime ( int severity , tor_tls_t * tls , <nl> * < b > future_tolerance </ b > seconds . If it is live , return 0 . If it is not <nl> * live , log a message and return - 1 . */ <nl> static int <nl> - check_cert_lifetime_internal ( int severity , const X509 * cert , int past_tolerance , <nl> - int future_tolerance ) <nl> + check_cert_lifetime_internal ( int severity , const X509 * cert , <nl> + int past_tolerance , int future_tolerance ) <nl> { <nl> time_t now , t ; <nl> 
NAMESPACE_BEGIN ( CryptoPP ) <nl> RandomPool :: RandomPool () <nl> : m_pCipher ( new AES :: Encryption ), m_keySet ( false ) <nl> { <nl> + memset ( m_key , 0 , m_key . SizeInBytes ()); <nl> + memset ( m_seed , 0 , m_seed . SizeInBytes ()); <nl> } <nl>  <nl> void RandomPool :: IncorporateEntropy ( const byte * input , size_t length )
static int _layouts_load_config_common ( layout_plugin_t * plugin , <nl> * calling the update_done layout callback */ <nl> updated_entities [ i ] = e ; <nl> } <nl> + xfree ( e_name ); <nl> + xfree ( e_type ); <nl>  <nl> /* ** Full load config only ( flags == 0 ) ** <nl> * post - read - and - build ( post stage 1 )
extern int make_batch_job_cred ( batch_job_launch_msg_t * launch_msg_ptr , <nl> xassert ( job_ptr -> job_resrcs ); <nl> job_resrcs_ptr = job_ptr -> job_resrcs ; <nl>  <nl> + if ( job_ptr -> job_resrcs == NULL ) { <nl> + error ("% s : job % u is missing job_resrcs info ", <nl> + __func__ , job_ptr -> job_id ); <nl> + return SLURM_ERROR ; <nl> + } <nl> + <nl> memset (& cred_arg , 0 , sizeof ( slurm_cred_arg_t )); <nl>  <nl> cred_arg . jobid = launch_msg_ptr -> job_id ;
static int _load_jobs ( slurm_msg_t * req_msg , job_info_msg_t ** job_info_msg_pptr , <nl> int i , j , rc = SLURM_SUCCESS ; <nl> int local_job_cnt ; <nl> slurm_msg_t resp_msg , resp_msg_fed ; <nl> - job_info_msg_t * orig_msg = NULL , * new_msg ; <nl> + job_info_msg_t * orig_msg = NULL , * new_msg = NULL ; <nl> uint32_t new_rec_cnt ; <nl> uint32_t hash_inx , * hash_tbl_size = NULL , ** hash_job_id = NULL ; <nl> slurmdb_cluster_rec_t * cluster ;
static void _purge_missing_jobs ( int node_inx , time_t now ) <nl> requeue = true ; <nl> info (" Batch JobId =% u missing from node 0 ", <nl> job_ptr -> job_id ); <nl> + job_ptr -> exit_code = 1 ; <nl> job_complete ( job_ptr -> job_id , 0 , requeue , true , NO_VAL ); <nl> } else { <nl> _notify_srun_missing_step ( job_ptr , node_inx ,
int initStyle ( styleObj * style ) { <nl> style -> sizescaled = style -> size = 1 ; // in SIZEUNITS ( layerObj ) <nl> style -> minsize = MS_MINSYMBOLSIZE ; <nl> style -> maxsize = MS_MAXSYMBOLSIZE ; <nl> - style -> offsetx = style -> offsety = - 1 ; <nl> + style -> offsetx = style -> offsety = 0 ; <nl>  <nl> return MS_SUCCESS ; <nl> }
mono_arch_create_specific_trampoline ( gpointer arg1 , MonoTrampolineType tramp_ty <nl>  <nl> code = buf = mono_domain_code_reserve_align ( domain , size , 1 ); <nl>  <nl> - if ((( gint64 ) tramp - ( gint64 ) code ) >> 31 != 0 ) { <nl> + if ((( gint64 ) tramp - ( gint64 ) code ) >> 31 != 0 && (( gint64 ) tramp - ( gint64 ) code ) >> 31 != - 1 ) { <nl> # ifndef MONO_ARCH_NOMAP32BIT <nl> g_assert_not_reached (); <nl> # endif
retry : <nl> * between 1 and 2 , the object is still live ) <nl> */ <nl> * objslot = NULL ; <nl> + SET_OWNER ( top , idx ); <nl> + SET_SP ( handles , top , idx ); <nl> mono_memory_write_barrier (); <nl> top -> size ++; <nl> mono_memory_write_barrier (); <nl> * objslot = obj ; <nl> - SET_OWNER ( top , idx ); <nl> - SET_SP ( handles , top , idx ); <nl> return objslot ; <nl> } <nl> if ( G_LIKELY ( top -> next )) {
load_aot_module ( MonoAssembly * assembly , gpointer user_data ) <nl> mono_trace ( G_LOG_LEVEL_INFO , MONO_TRACE_AOT , " AOT : image '% s ' not found : % s ", aot_name , err ); <nl> g_free ( err ); <nl> } <nl> + g_free ( aot_name ); <nl> } <nl> if (! sofile ) { <nl> GList * l ;
void check_object ( char * start ); <nl> */ <nl>  <nl> const char * descriptor_types [] = { <nl> + " INVALID ", <nl> " run_length ", <nl> " small_bitmap ", <nl> - " string ", <nl> " complex ", <nl> " vector ", <nl> - " array ", <nl> " large_bitmap ", <nl> - " complex_arr " <nl> + " complex_arr ", <nl> + " complex_ptrfree " <nl> }; <nl>  <nl> static char * describe_nursery_ptr ( char * ptr , gboolean need_setup );
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> token = read32 ( ip + 2 ); <nl> func = mono_method_get_wrapper_data ( method , token ); <nl> info = mono_find_jit_icall_by_addr ( func ); <nl> + if (! info ) <nl> + g_error (" Could not find icall address in wrapper % s ", mono_method_full_name ( method , 1 )); <nl> g_assert ( info ); <nl>  <nl> CHECK_STACK ( info -> sig -> param_count );
seq_point_info_add_seq_point ( MonoSeqPointInfo * info , SeqPoint * sp , SeqPoint * la <nl> guint8 buffer [ 4 ]; <nl> guint8 len ; <nl>  <nl> + if (! info -> has_debug_data && <nl> + ( sp -> il_offset == METHOD_ENTRY_IL_OFFSET || sp -> il_offset == METHOD_EXIT_IL_OFFSET )) <nl> + return FALSE ; <nl> + <nl> /* check that data can be added to the arrays */ <nl> g_assert ( info -> alloc_arrays ); <nl> 
mono_trace_set_printerr_handler ( MonoPrintCallback callback ) <nl> { <nl> g_assert ( callback ); <nl> printerr_callback = callback ; <nl> - g_set_print_handler ( printerr_handler ); <nl> + g_set_printerr_handler ( printerr_handler ); <nl> }
is_regsize_var ( MonoType * t ) { <nl> case MONO_TYPE_U4 : <nl> case MONO_TYPE_I : <nl> case MONO_TYPE_U : <nl> + case MONO_TYPE_PTR : <nl> + case MONO_TYPE_FNPTR : <nl> return TRUE ; <nl> case MONO_TYPE_OBJECT : <nl> case MONO_TYPE_STRING :
try <nl>  <nl> qtype = mdp . d_qtype ; <nl> qclass = mdp . d_qclass ; <nl> + <nl> + d_trc = TSIGRecordContent (); <nl> + <nl> return 0 ; <nl> } <nl> catch ( std :: exception & e ) {
int MySQLDB :: exec_sql_query ( MYSQL * conn , char * sql , <nl> // than a simple 0 <nl> if (( result = mysql_store_result (& mysql )) == NULL ) <nl> rc = 0 ; // unable to retrieve the result but still the query succeded <nl> - else <nl> + else { <nl> + mysql_free_result ( result ); <nl> rc = mysql_num_rows ( result ); <nl> + } <nl> } <nl>  <nl> if ( doLock && m ) m -> unlock ( __FILE__ , __LINE__ );
eazel_install_install_packages ( EazelInstall * service , GList * categories ) <nl> if ( install_new_packages ( service , categories )== FALSE ) { <nl> g_warning ( _ (" Install failed ")); <nl> } <nl> - eazel_install_emit_done ( service ); <nl> if ( eazel_install_emit_delete_files ( service )) { <nl> GList * item ; <nl> GList * cat ; <nl> eazel_install_install_packages ( EazelInstall * service , GList * categories ) <nl> } <nl> } <nl> } <nl> + eazel_install_emit_done ( service ); <nl> } <nl>  <nl> void
GF_Err import_file ( GF_ISOFile * dest , char * inName , u32 import_flags , Double forc <nl> } <nl> } <nl> } <nl> + else if (! strnicmp ( ext , " prog_id =", 8 )) { <nl> + prog_id = atoi ( ext + 8 ); <nl> + do_all = 0 ; <nl> + } <nl> else track_id = atoi ( ext ); <nl> } <nl> if ( do_audio || do_video || track_id ) do_all = 0 ;
GF_Err infe_Read ( GF_Box * s , GF_BitStream * bs ) <nl> } <nl> string_start += string_len ; <nl> string_len = 0 ; <nl> + if ( ptr -> content_encoding && ptr -> version == 1 ) { <nl> + break ; <nl> + } <nl> } <nl> string_len ++; <nl> }
sm_encrypt_des_cbc3 ( struct sc_context * ctx , unsigned char * key , <nl>  <nl> * out_len = data_len ; <nl> * out = malloc ( data_len + 8 ); <nl> - if (* out == NULL ) <nl> + if (* out == NULL ) { <nl> + free ( data ); <nl> LOG_TEST_RET ( ctx , SC_ERROR_OUT_OF_MEMORY , " SM encrypt_des_cbc3 : failure "); <nl> + } <nl>  <nl> memcpy (& kk , key , 8 ); <nl> memcpy (& k2 , key + 8 , 8 );
static int westcos_sign_decipher ( int mode , sc_card_t * card , <nl> BIO * mem = BIO_new ( BIO_s_mem ()); <nl> # endif <nl>  <nl> - if ( card == NULL ) <nl> + if ( card == NULL ) { <nl> + if ( keyfile ) <nl> + sc_file_free ( keyfile ); <nl> return SC_ERROR_INVALID_ARGUMENTS ; <nl> + } <nl> sc_debug ( card -> ctx , SC_LOG_DEBUG_NORMAL , <nl> " westcos_sign_decipher outlen =% d \ n ", outlen ); <nl> 
void c_SimpleXMLElement :: t___construct ( CStrRef data , int64 options /* = 0 */, <nl> m_attributes = collect_attributes ( m_node , ns , is_prefix ); <nl> } <nl> } else { <nl> - raise_error (" String could not be parsed as XML "); <nl> + throw ( Object ) p_Exception ( NEW ( c_Exception )())-> create ( <nl> + " String could not be parsed as XML "); <nl> } <nl> } <nl> 
void Translator :: handleAssertionEffects ( Tracelet & t , <nl> */ <nl> if ( tas . m_changeSet . count ( dl -> location )) { <nl> auto const src = findInputSrc ( tas . m_t -> m_instrStream . last , dl ); <nl> - if ( src -> outputPredicted ) src -> outputPredicted = false ; <nl> + if ( src && src -> outputPredicted ) src -> outputPredicted = false ; <nl> } <nl> } <nl> 
void NativeWindowViews :: SetParentWindow ( NativeWindow * parent ) { <nl>  <nl> void NativeWindowViews :: SetModal ( bool modal ) { <nl> # if defined ( USE_X11 ) <nl> + SetWindowType ( GetAcceleratedWidget (), modal ? " dialog " : " normal "); <nl> + Show (); <nl> SetWMSpecState ( GetAcceleratedWidget (), modal , <nl> GetAtom (" _NET_WM_STATE_MODAL ")); <nl> - SetWindowType ( GetAcceleratedWidget (), modal ? " dialog " : " normal "); <nl> # endif <nl> } <nl> 
DEFUN ( ipv6_ospf6_priority , <nl>  <nl> oi -> priority = strtol ( argv [ 0 ], NULL , 10 ); <nl>  <nl> - if ( oi -> area ) <nl> + if ( oi -> area && <nl> + ( oi -> state == OSPF6_INTERFACE_DROTHER || <nl> + oi -> state == OSPF6_INTERFACE_BDR || <nl> + oi -> state == OSPF6_INTERFACE_DR )) <nl> ospf6_interface_state_change ( dr_election ( oi ), oi ); <nl>  <nl> return CMD_SUCCESS ;
mrb_irep_free ( mrb_state * mrb , mrb_irep * irep ) <nl> } <nl> mrb_free ( mrb , irep -> pool ); <nl> mrb_free ( mrb , irep -> syms ); <nl> + mrb_free ( mrb , irep -> reps ); <nl> mrb_free ( mrb , ( void *) irep -> filename ); <nl> mrb_free ( mrb , irep -> lines ); <nl> mrb_debug_info_free ( mrb , irep -> debug_info );
size_t format_end ( char * buf , <nl> send_cert = "+ S = C "; <nl> break ; <nl> } <nl> - p = add_str ( endopts , sizeof ( endopts ), p , send_cert ); <nl> + add_str ( endopts , sizeof ( endopts ), p , send_cert ); <nl> } <nl> } <nl> 
skip_more_whitespace_and_comments : <nl>  <nl> /* LHS Keysym . */ <nl> if ( chr ( s , '<')) { <nl> - while ( peek ( s ) != '>' && ! eol ( s )) <nl> + while ( peek ( s ) != '>' && ! eol ( s ) && ! eof ( s )) <nl> buf_append ( s , next ( s )); <nl> if (! chr ( s , '>')) { <nl> scanner_err ( s , " unterminated keysym literal ");
LookupModMask ( struct xkb_context * ctx , const void * priv , xkb_atom_t field , <nl> return false ; <nl>  <nl> str = xkb_atom_text ( ctx , field ); <nl> + if (! str ) <nl> + return false ; <nl>  <nl> if ( istreq ( str , " all ")) { <nl> * val_rtrn = MOD_REAL_MASK_ALL ;
ExprAppendMultiKeysymList ( ExprDef * expr , ExprDef * append ) <nl> darray_append ( expr -> keysym_list . symsNumEntries , numEntries ); <nl> darray_concat ( expr -> keysym_list . syms , append -> keysym_list . syms ); <nl>  <nl> - FreeStmt (( ParseCommon *) & append ); <nl> + FreeStmt (( ParseCommon *) append ); <nl>  <nl> return expr ; <nl> }
static MagickBooleanType WriteOnePNGImage ( MngInfo * mng_info , <nl> image_colors =( int ) image -> colors ; <nl> image_matte = image -> alpha_trait == BlendPixelTrait ? MagickTrue : MagickFalse ; <nl>  <nl> - if ( mng_info -> write_png_colortype > 4 ) <nl> + if ( mng_info -> write_png_colortype < 5 ) <nl> mng_info -> IsPalette = image -> storage_class == PseudoClass && <nl> image_colors <= 256 && image -> colormap != NULL ; <nl> else
static Image * ReadOneJNGImage ( MngInfo * mng_info , <nl> ( void ) LogMagickEvent ( CoderEvent , GetMagickModule (), <nl> " Copying JDAT chunk data to color_blob ."); <nl>  <nl> - ( void ) WriteBlob ( color_image , length , chunk ); <nl> - <nl> if ( length != 0 ) <nl> - chunk =( unsigned char *) RelinquishMagickMemory ( chunk ); <nl> + { <nl> + ( void ) WriteBlob ( color_image , length , chunk ); <nl> + chunk =( unsigned char *) RelinquishMagickMemory ( chunk ); <nl> + } <nl>  <nl> continue ; <nl> }
void CServer :: ProcessClientPacket ( CNetChunk * pPacket ) <nl> return ; <nl>  <nl> int Chunk = Unpacker . GetInt (); <nl> - int ChunkSize = 1024 - 128 ; <nl> - int Offset = Chunk * ChunkSize ; <nl> + unsigned int ChunkSize = 1024 - 128 ; <nl> + unsigned int Offset = Chunk * ChunkSize ; <nl> int Last = 0 ; <nl>  <nl> // drop faulty map data requests
bool console_input_special_binds ( INPUT_EVENT e , void * user_data ) <nl>  <nl> bool console_input_normal_binds ( INPUT_EVENT e , void * user_data ) <nl> { <nl> + // need to be ingame for these binds <nl> + if ( client_state () != CLIENTSTATE_ONLINE ) <nl> + return false ; <nl> + <nl> // don ' t handle invalid events and keys that arn ' t set to anything <nl> if ( e . key <= 0 || e . key >= KEY_LAST || keybindings [ e . key ][ 0 ] == 0 ) <nl> return false ;
mmvJob :: mmvJob ( FileAccess * session , const ArgV * args , const char * t , FA :: open_mode <nl> { <nl> op . set ( args -> a0 ()); <nl> for ( int i = args -> getindex (); i < args -> count (); i ++) <nl> - wcd . push ( strdup ( args -> getarg ( i ))); <nl> + wcd . push ( xstrdup ( args -> getarg ( i ))); <nl> } <nl>  <nl> void mmvJob :: doOpen () const
# include < assert . h > <nl> # include < string . h > <nl>  <nl> -# if defined ( __linux__ ) <nl> -# include < ucontext . h > <nl> -# elif defined ( __APPLE__ ) <nl> +# if defined ( __APPLE__ ) <nl> # include < sys / ucontext . h > <nl> +# else <nl> +# include < ucontext . h > <nl> # endif <nl>  <nl> # include < limits . h >
_shell_app_remove_window ( ShellApp * app , <nl> g_object_unref ( window ); <nl> app -> windows = g_slist_remove ( app -> windows , window ); <nl>  <nl> + g_signal_emit ( app , shell_app_signals [ WINDOWS_CHANGED ], 0 ); <nl> + <nl> if ( app -> windows == NULL ) <nl> disconnect_workspace_switch ( app ); <nl> }
DetectPcreData * DetectPcreParse ( char * regexstr ) <nl> SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' I '"); <nl> goto error ; <nl> } <nl> + if ( pd -> flags & DETECT_PCRE_RAWBYTES ) { <nl> + SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' B '"); <nl> + goto error ; <nl> + } <nl> pd -> flags |= DETECT_PCRE_URI ; <nl> break ; <nl> case ' H ': /* snort ' s option */
CURLcode get_url_file_name ( char ** filename , const char * url ) <nl> Curl_safefree (* filename ); <nl> * filename = strdup ( buffer ); /* clone the buffer */ <nl> curl_free ( tdir ); <nl> + if (!* filename ) <nl> + return CURLE_OUT_OF_MEMORY ; <nl> } <nl> } <nl> # endif
int Curl_resolv_timeout ( struct connectdata * conn , <nl>  <nl> * entry = NULL ; <nl>  <nl> + if ( timeoutms < 0 ) <nl> + /* got an already expired timeout */ <nl> + return CURLRESOLV_TIMEDOUT ; <nl> + <nl> # ifdef USE_ALARM_TIMEOUT <nl> if ( data -> set . no_signal ) <nl> /* Ignore the timeout when signals are disabled */
static void GLimp_InitExtensions ( void ) <nl> // Find out how many general combiners they have . <nl> # define GL_MAX_GENERAL_COMBINERS_NV 0x854D <nl> GLint iNumGeneralCombiners = 0 ; <nl> - qglGetIntegerv ( GL_MAX_GENERAL_COMBINERS_NV , & iNumGeneralCombiners ); <nl> + if ( bNVRegisterCombiners ) <nl> + qglGetIntegerv ( GL_MAX_GENERAL_COMBINERS_NV , & iNumGeneralCombiners ); <nl>  <nl> // Only allow dynamic glows / flares if they have the hardware <nl> if ( bTexRectSupported && bARBVertexProgram && qglActiveTextureARB && glConfig . maxActiveTextures >= 4 &&
stats_dnsbl ( struct Client * source_p ) <nl> rb_dictionary_iter iter ; <nl> struct BlacklistStats * stats ; <nl>  <nl> + if ( bl_stats == NULL ) <nl> + return ; <nl> + <nl> RB_DICTIONARY_FOREACH ( stats , & iter , bl_stats ) <nl> { <nl> /* use RPL_STATSDEBUG for now -- jilles */
static INLINE OPJ_BOOL opj_dwt_encode_procedure ( opj_tcd_tilecomp_t * tilec , void <nl>  <nl> l_data_size = opj_dwt_max_resolution ( tilec -> resolutions , tilec -> numresolutions ) * ( OPJ_UINT32 ) sizeof ( OPJ_INT32 ); <nl> bj = ( OPJ_INT32 *) opj_malloc (( size_t ) l_data_size ); <nl> - if (! bj ) { <nl> + if ( l_data_size != 0 && ! bj ) { <nl> return OPJ_FALSE ; <nl> } <nl> i = l ;
GetOutboundPinholeTimeout ( struct upnphttp * h , const char * action , const char * <nl> rem_port = GetValueFromNameValueList (& data , " RemotePort "); <nl> protocol = GetValueFromNameValueList (& data , " Protocol "); <nl>  <nl> - if (! int_port || ! ext_port || ! protocol ) <nl> + if (! int_port || ! rem_port || ! protocol ) <nl> { <nl> ClearNameValueList (& data ); <nl> SoapError ( h , 402 , " Invalid Args ");
int ssl3_get_cert_verify ( SSL * s ) <nl> if ( s -> s3 -> tmp . message_type != SSL3_MT_CERTIFICATE_VERIFY ) <nl> { <nl> s -> s3 -> tmp . reuse_message = 1 ; <nl> - if (( peer != NULL ) && ( type & EVP_PKT_SIGN )) <nl> + if ( peer != NULL ) <nl> { <nl> al = SSL_AD_UNEXPECTED_MESSAGE ; <nl> SSLerr ( SSL_F_SSL3_GET_CERT_VERIFY , SSL_R_MISSING_VERIFY_MESSAGE );
int SSL_check_private_key ( SSL * ssl ) <nl> SSLerr ( SSL_F_SSL_CHECK_PRIVATE_KEY , ERR_R_PASSED_NULL_PARAMETER ); <nl> return ( 0 ); <nl> } <nl> + if ( ssl -> cert == NULL ) <nl> + return 0 ; <nl> if ( ssl -> cert -> key -> x509 == NULL ) <nl> { <nl> SSLerr ( SSL_F_SSL_CHECK_PRIVATE_KEY , SSL_R_NO_CERTIFICATE_ASSIGNED );
tplProcessCnf ( struct cnfobj * o ) <nl> pTpl -> optFormatEscape = JSON_ESCAPE ; <nl>  <nl> finalize_it : <nl> + free ( tplStr ); <nl> if ( pvals != NULL ) <nl> cnfparamvalsDestruct ( pvals , & pblk ); <nl> if ( iRet != RS_RET_OK ) {
setup_current_filesystem ( struct archive_read_disk * a ) <nl> if (! GetVolumePathName ( tree_current_access_path ( t ), vol , sizeof ( vol ))) { <nl> t -> current_filesystem -> remote = - 1 ; <nl> archive_set_error (& a -> archive , ARCHIVE_ERRNO_MISC , <nl> - " GetVolumePathName failed : % d ", GetLastError ()); <nl> + " GetVolumePathName failed : % d ", ( int ) GetLastError ()); <nl> return ( ARCHIVE_FAILED ); <nl> } <nl> switch ( GetDriveType ( vol )) {
int imap_fetch_message ( MESSAGE * msg , CONTEXT * ctx , int msgno ) <nl> } <nl> } <nl>  <nl> - mutt_message _ (" Fetching message ..."); <nl> + if (! isendwin ()) <nl> + mutt_message _ (" Fetching message ..."); <nl>  <nl> cache -> uid = HEADER_DATA ( h )-> uid ; <nl> mutt_mktemp ( path );
int main ( int argc , char * argv []) <nl> if (! use_curses ) <nl> early_quit ( 0 , " No servers could be used ! Exiting ."); <nl> # ifdef HAVE_CURSES <nl> + wrefresh ( logwin ); <nl> halfdelay ( 10 ); <nl> if ( getch () != ERR ) <nl> early_quit ( 0 , " No servers could be used ! Exiting .");
static void mcast () <nl>  <nl> count ++; <nl> came_from_siz = sizeof ( came_from ); <nl> - if ( SOCKETFAIL ( rep = recvfrom ( mcast_sock , buf , sizeof ( buf ), <nl> + if ( SOCKETFAIL ( rep = recvfrom ( mcast_sock , buf , sizeof ( buf ) - 1 , <nl> 0 , ( struct sockaddr *)(& came_from ), & came_from_siz ))) { <nl> applog ( LOG_DEBUG , " API mcast failed count =% d (% s ) (% d )", <nl> count , SOCKERRMSG , ( int ) mcast_sock );
static int open_user_core ( uid_t uid , uid_t fsuid , pid_t pid , char ** percent_valu <nl>  <nl> static bool dump_fd_info ( const char * dest_filename , char * source_filename , int source_base_ofs , uid_t uid , gid_t gid ) <nl> { <nl> - FILE * fp = fopen ( dest_filename , " w "); <nl> + FILE * fp = fopen ( dest_filename , " wx "); <nl> if (! fp ) <nl> return false ; <nl> 
static void server_recv_cb ( EV_P_ ev_io * w , int revents ) <nl>  <nl> if ( remote_ctx != NULL ) { <nl> if ( memcmp (& src_addr , & remote_ctx -> src_addr , sizeof ( src_addr )) <nl> - || strcmp ( addr_header , remote_ctx -> addr_header ) != 0 ) { <nl> + || remote_ctx -> addr_header_len != addr_header_len <nl> + || memcmp ( addr_header , remote_ctx -> addr_header , addr_header_len ) != 0 ) { <nl> remote_ctx = NULL ; <nl> } <nl> }
pcm_init ( SF_PRIVATE * psf ) <nl> { int chars = 0 ; <nl>  <nl> if ( psf -> bytewidth == 0 || psf -> sf . channels == 0 ) <nl> + { psf_log_printf ( psf , " pcm_init : internal error : bytewitdh = % d , channels = % d \ n ", psf -> bytewidth , psf -> sf . channels ) ; <nl> return SFE_INTERNAL ; <nl> + } ; <nl>  <nl> psf -> blockwidth = psf -> bytewidth * psf -> sf . channels ; <nl> 
OPJ_BOOL opj_jp2_read_boxhdr_char ( opj_jp2_box_t * box , <nl> opj_event_msg ( p_manager , EVT_ERROR , " Cannot handle box of undefined sizes \ n "); <nl> return OPJ_FALSE ; <nl> } <nl> - <nl> + if ( box -> length < * p_number_bytes_read ) { <nl> + opj_event_msg ( p_manager , EVT_ERROR , " Box length is inconsistent .\ n "); <nl> + return OPJ_FALSE ; <nl> + } <nl> return OPJ_TRUE ; <nl> } <nl> 
local size_t compressed_suffix ( char * nm ) <nl> if ( len > 4 ) { <nl> nm += len - 4 ; <nl> len = 4 ; <nl> - if ( strcmp ( nm , ". zip ") == 0 || strcmp ( nm , ". ZIP ") == 0 ) <nl> + if ( strcmp ( nm , ". zip ") == 0 || strcmp ( nm , ". ZIP ") == 0 || strcmp ( nm , ". spli ") == 0 ) <nl> return 4 ; <nl> } <nl> if ( len > 3 ) {
key_to_event ( guint keyval , gint mode ) { <nl> utf_conv = g_convert ( byte , - 1 , " UTF - 8 ", " ISO - 8859 - 1 ", NULL , NULL , NULL ); <nl>  <nl> send_event ( mode == GDK_KEY_PRESS ? KEY_PRESS : KEY_RELEASE , utf_conv ? utf_conv : byte , NULL ); <nl> + g_free ( utf_conv ); <nl> } <nl> else <nl> send_event ( mode == GDK_KEY_PRESS ? KEY_PRESS : KEY_RELEASE ,
set_interface_var ( const char * iface , <nl> if ( snprintf ( spath , sizeof ( spath ), var , iface ) >= sizeof ( spath )) <nl> return - 1 ; <nl>  <nl> + /* No path traversal */ <nl> + if ( strstr ( name , "..") || strchr ( name , '/')) <nl> + return - 1 ; <nl> + <nl> if ( access ( spath , F_OK ) != 0 ) <nl> return - 1 ; <nl> 
void mg_send_digest_auth_request ( struct mg_connection * c ) { <nl> " realm =\"% s \", nonce =\"% lu \"\ r \ n \ r \ n ", <nl> conn -> server -> config_options [ AUTH_DOMAIN ], <nl> ( unsigned long ) time ( NULL )); <nl> + close_local_endpoint ( conn ); <nl> } <nl>  <nl> // Use the global passwords file , if specified by auth_gpass option ,
static st_ret_t _st_db_put ( st_driver_t drv , const char * type , const char * owner , <nl> DB_TXN * t ; <nl> st_ret_t ret ; <nl>  <nl> + if ( dbd == NULL ) { <nl> + return st_FAILED ; <nl> + } <nl> + <nl> if ( os_count ( os ) == 0 ) <nl> return st_SUCCESS ; <nl> 
mobilesync_error_t mobilesync_ready_to_send_changes_from_computer ( mobilesync_cli <nl> err = MOBILESYNC_E_SUCCESS ; <nl>  <nl> out : <nl> + if ( response_type ) { <nl> + free ( response_type ); <nl> + response_type = NULL ; <nl> + } <nl> if ( msg ) { <nl> plist_free ( msg ); <nl> msg = NULL ;
int JPC_SEGPASSCNT ( int passno , int firstpassno , int numpasses , int bypass , int t <nl> } else { <nl> ret = JPC_PREC * 3 - 2 ; <nl> } <nl> - ret = JAS_MIN ( ret , numpasses - passno ); <nl> + if ( passno < numpasses ) <nl> + ret = JAS_MIN ( ret , numpasses - passno ); <nl> return ret ; <nl> } <nl> 
static DecodeStatus DecodeINSVE_DF_4 ( MCInst * MI , uint32_t insn , <nl> } // else llvm_unreachable (" Invalid encoding "); <nl>  <nl> // assert ( NSize != 0 && RegDecoder != nullptr ); <nl> + if ( NSize == 0 || RegDecoder == NULL ) <nl> + return MCDisassembler_Fail ; <nl>  <nl> if ( RegDecoder == NULL ) <nl> return MCDisassembler_Fail ;
static int read_all_chunked_input ( req_state * s , char ** pdata , int * plen ) <nl> int read_len = 0 , len = 0 ; <nl> do { <nl> int r = s -> cio -> read ( data + len , need_to_read , & read_len ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( data ); <nl> return r ; <nl> + } <nl>  <nl> len += read_len ; <nl> 
CInode * Server :: rdlock_path_pin_ref ( MDRequest * mdr , bool want_auth ) <nl> // open ref inode <nl> CInode * ref = 0 ; <nl> if ( trace . empty ()) <nl> - ref = mdcache -> get_root (); <nl> + ref = mdcache -> get_inode ( refpath . get_ino ()); <nl> else { <nl> CDentry * dn = trace [ trace . size ()- 1 ]; <nl> 
bool MDS :: _dispatch ( Message * m ) <nl> } <nl>  <nl> switch ( m -> get_type ()) { <nl> + <nl> + case CEPH_MSG_MON_MAP : <nl> + delete m ; <nl> + break ; <nl> + <nl> // MDS <nl> case CEPH_MSG_MDS_MAP : <nl> handle_mds_map (( MMDSMap *) m );
void RGWCopyObj_ObjStore_S3 :: send_partial_response ( off_t ofs ) <nl> dump_errno ( s ); <nl>  <nl> end_header ( s , this , " application / xml "); <nl> + dump_start ( s ); <nl> if ( op_ret == 0 ) { <nl> s -> formatter -> open_object_section_in_ns (" CopyObjectResult ", XMLNS_AWS_S3 ); <nl> }
static struct errno_http hterrs [] = { <nl> { EEXIST , " 409 ", " BucketAlreadyExists " }, <nl> { ENOTEMPTY , " 409 ", " BucketNotEmpty " }, <nl> { ERANGE , " 416 ", " InvalidRange " }, <nl> - { 0 , NULL }}; <nl> + { 0 , NULL , NULL }}; <nl>  <nl> void dump_errno ( struct req_state * s , int err , struct rgw_err * rgwerr ) <nl> {
void RGWAbortMultipart :: execute () <nl> // and also remove the metadata obj <nl> op_ret = del_op . delete_obj (); <nl> if ( op_ret == - ENOENT ) { <nl> - op_ret = - ERR_NO_SUCH_BUCKET ; <nl> + op_ret = - ERR_NO_SUCH_UPLOAD ; <nl> } <nl> } <nl> 
void ReplicatedPG :: sub_op_modify_ondisk ( MOSDSubOp * op , int ackerosd , eversion_t <nl> commit -> set_pg_complete_thru ( last_complete ); <nl> commit -> set_peer_stat ( osd -> get_my_stat_for ( g_clock . now (), ackerosd )); <nl> osd -> messenger -> send_message ( commit , osd -> osdmap -> get_inst ( ackerosd )); <nl> - delete op ; <nl> } <nl> + <nl> + delete op ; <nl> } <nl>  <nl> void ReplicatedPG :: sub_op_modify_reply ( MOSDSubOpReply * r )
OutputDataSocket :: OutputDataSocket ( CephContext * cct , uint64_t _backlog ) <nl> m_shutdown_rd_fd (- 1 ), <nl> m_shutdown_wr_fd (- 1 ), <nl> going_down ( false ), <nl> + data_size ( 0 ), <nl> m_lock (" OutputDataSocket :: m_lock ") <nl> { <nl> }
class MHeartbeat : public Message { <nl> this -> load = load ; <nl> this -> beat = beat ; <nl> } <nl> + private : <nl> + ~ MHeartbeat () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " HB "; } <nl>  <nl> void encode_payload () {
void Rank :: Pipe :: fault ( bool onconnect ) <nl>  <nl> if (! onconnect ) dout ( 2 ) << " fault " << errno << ": " << strerror ( errno ) << dendl ; <nl>  <nl> - if ( state == STATE_CLOSED ) { <nl> - dout ( 10 ) << " fault already closed " << dendl ; <nl> + if ( state == STATE_CLOSED || <nl> + state == STATE_CLOSING ) { <nl> + dout ( 10 ) << " fault already closed | closing " << dendl ; <nl> return ; <nl> } <nl> 
int OSD :: init_op_flags ( OpRequestRef op ) <nl> } <nl> } <nl>  <nl> + if ( op -> rmw_flags == 0 ) <nl> + return - EINVAL ; <nl> + <nl> return 0 ; <nl> }
OPTION ( cluster_network , OPT_STR , "") <nl> OPTION ( num_client , OPT_INT , 1 ) <nl> OPTION ( monmap , OPT_STR , "") <nl> OPTION ( mon_host , OPT_STR , "") <nl> + OPTION ( mon_dns_srv_name , OPT_STR , " ceph - mon ") <nl> OPTION ( lockdep , OPT_BOOL , false ) <nl> OPTION ( lockdep_force_backtrace , OPT_BOOL , false ) // always gather current backtrace at every lock <nl> OPTION ( run_dir , OPT_STR , "/ var / run / ceph ") // the "/ var / run / ceph " dir , created on daemon startup
EbmlElement * EbmlElement :: FindNextElement ( IOCallback & DataStream , const EbmlSe <nl> ReadIndex = SizeIdx - 1 ; <nl> memmove (& PossibleIdNSize [ 0 ], & PossibleIdNSize [ 1 ], ReadIndex ); <nl> UpperLevel = UpperLevel_original ; <nl> - } while ( MaxDataSize > DataStream . getFilePointer () - SizeIdx + PossibleID_Length ); <nl> + } while ( MaxDataSize >= ReadSize ); <nl>  <nl> return NULL ; <nl> }
l_uint32 * line ; <nl> pos = ( qpos + i ) % 8 ; <nl> npx = px + xpostab [ pos ]; <nl> npy = py + ypostab [ pos ]; <nl> + if ( npx < 0 || npx >= w || npy < 0 || npy >= h ) <nl> + continue ; <nl> line = data + npy * wpl ; <nl> val = GET_DATA_BIT ( line , npx ); <nl> if ( val ) {
rsvg_acquire_file_resource ( const char * filename , const char * base_uri , GError * <nl>  <nl> g_byte_array_append ( array , ( guint8 *) data , length ); <nl> g_free ( data ); <nl> + g_free ( path ); <nl>  <nl> return array ; <nl> }
rsvg_characters_impl ( RsvgHandle * ctx , const xmlChar * ch , int len ) <nl>  <nl> if ( ctx -> priv -> currentnode ) <nl> rsvg_node_add_child ( ctx -> priv -> currentnode , node ); <nl> + <nl> + node = rsvg_node_unref ( node ); <nl> } <nl>  <nl> static void
DialInstance :: DialResult DialInstance :: execute () <nl>  <nl> void DialInstance :: prepareAddress () <nl> { <nl> - if ( mTargetUri . scheme () == Symbols :: Sip ) { <nl> + if ( mTargetUri . scheme () == Symbols :: Sip || <nl> + mTargetUri . scheme () == Symbols :: Sips ) { <nl> mFullTarget = mTargetUri ; <nl> return ; <nl> }
int ares_init_options_with_socket_function ( ares_channel * channelptr , struct ares <nl> channel -> ndomains = - 1 ; <nl> channel -> nsort = - 1 ; <nl> channel -> lookups = NULL ; <nl> + channel -> servers = NULL ; <nl>  <nl> /* Initialize configuration by each of the four sources , from highest <nl> * precedence to lowest .
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
Guardian :: Guardian ( Thread_registry * thread_registry_arg , <nl> monitoring_interval ( monitoring_interval_arg ), <nl> thread_registry ( thread_registry_arg ), <nl> instance_map ( instance_map_arg ), <nl> + guarded_instances ( 0 ), <nl> shutdown_requested ( FALSE ) <nl> { <nl> pthread_mutex_init (& LOCK_guardian , 0 );
history_save ( History * h , const char * fname ) <nl> retval = HPREV ( h , & ev ), i ++) { <nl> len = strlen ( ev . str ) * 4 ; <nl> if ( len >= max_size ) { <nl> - max_size = ( len + 1023 ) & 1023 ; <nl> + max_size = ( len + 1023 ) & ~ 1023 ; <nl> ptr = h_realloc ( ptr , max_size ); <nl> } <nl> ( void ) strvis ( ptr , ev . str , VIS_WHITE );
pstack_install_segv_action ( const char * path_format_ ) <nl> if (( abfd = load_bfd ( pid ))== 0 ) <nl> fprintf ( stderr , " BFD load failed ..\ n "); <nl> else { <nl> - long storage_needed = bfd_get_symtab_upper_bound ( abfd ); <nl> + long storage_needed = ( bfd_get_file_flags ( abfd ) & HAS_SYMS ) ? <nl> + bfd_get_symtab_upper_bound ( abfd ) : 0 ; <nl> long i ; <nl> ( void ) i ; <nl> 
buf_get_latched_pages_number ( void ) <nl>  <nl> block = buf_pool_get_nth_block ( buf_pool , i ); <nl>  <nl> - if (( block -> buf_fix_count != 0 ) || ( block -> io_fix != 0 )) <nl> + if ((( block -> buf_fix_count != 0 ) || ( block -> io_fix != 0 )) && <nl> + block -> magic_n == BUF_BLOCK_MAGIC_N ) <nl> fixed_pages_number ++; <nl> } <nl> 
int SetCipherList ( Suites * s , const char * list ) <nl> byte b ; <nl> byte compression ; <nl> ProtocolVersion pv ; <nl> - word16 extSz ; <nl> word32 i = * inOutIdx ; <nl> word32 begin = i ; <nl> 
int SendCertificateRequest ( WOLFSSL * ssl ) <nl> /* write to output */ <nl> output [ i ++] = ( byte ) typeTotal ; /* # of types */ <nl> # ifdef HAVE_ECC <nl> - if ( ssl -> options . cipherSuite0 == ECC_BYTE && <nl> + if (( ssl -> options . cipherSuite0 == ECC_BYTE || <nl> + ssl -> options . cipherSuite0 == CHACHA_BYTE ) && <nl> ssl -> specs . sig_algo == ecc_dsa_sa_algo ) { <nl> output [ i ++] = ecdsa_sign ; <nl> } else
EXPORT_SYMBOL ( _find_next_zero_bit_be ); <nl> EXPORT_SYMBOL ( _find_first_bit_be ); <nl> EXPORT_SYMBOL ( _find_next_bit_be ); <nl> # endif <nl> + <nl> + EXPORT_SYMBOL ( copy_page );
void * dma_generic_alloc_coherent ( struct device * dev , size_t size , <nl> void * ret , * ret_nocache ; <nl> int order = get_order ( size ); <nl>  <nl> + gfp |= __GFP_ZERO ; <nl> + <nl> ret = ( void *) __get_free_pages ( gfp , order ); <nl> if (! ret ) <nl> return NULL ; <nl>  <nl> - memset ( ret , 0 , size ); <nl> /* <nl> * Pages from the page allocator may have data present in <nl> * cache . So flush the cache before using uncached memory .
again : <nl>  <nl> # ifdef CONFIG_MODULE_SIG <nl> mod -> sig_ok = info -> sig_ok ; <nl> - if (! mod -> sig_ok ) <nl> + if (! mod -> sig_ok ) { <nl> + printk_once ( KERN_NOTICE <nl> + "% s : module verification failed : signature and / or " <nl> + " required key missing - tainting kernel \ n ", <nl> + mod -> name ); <nl> add_taint_module ( mod , TAINT_FORCED_MODULE ); <nl> + } <nl> # endif <nl>  <nl> /* Now module is in final location , initialize linked lists , etc . */
static void xfrm4_dst_destroy ( struct dst_entry * dst ) <nl>  <nl> if ( likely ( xdst -> u . rt . idev )) <nl> in_dev_put ( xdst -> u . rt . idev ); <nl> + if ( likely ( xdst -> u . rt . peer )) <nl> + inet_putpeer ( xdst -> u . rt . peer ); <nl> xfrm_dst_destroy ( xdst ); <nl> } <nl> 
static int try_to_unmap_file ( struct page * page , enum ttu_flags flags ) <nl> unsigned long max_nl_size = 0 ; <nl> unsigned int mapcount ; <nl>  <nl> + if ( PageHuge ( page )) <nl> + pgoff = page -> index << compound_order ( page ); <nl> + <nl> mutex_lock (& mapping -> i_mmap_mutex ); <nl> vma_interval_tree_foreach ( vma , & mapping -> i_mmap , pgoff , pgoff ) { <nl> unsigned long address = vma_address ( page , vma );
static int nand_scan_bbt ( struct mtd_info * mtd , struct nand_bbt_descr * bd ) <nl> struct nand_bbt_descr * td = this -> bbt_td ; <nl> struct nand_bbt_descr * md = this -> bbt_md ; <nl>  <nl> - len = mtd -> size >> ( this -> bbt_erase_shift + 2 ); <nl> + len = ( mtd -> size >> ( this -> bbt_erase_shift + 2 )) ? : 1 ; <nl> /* <nl> * Allocate memory ( 2bit per block ) and clear the memory bad block <nl> * table .
static inline unsigned long long res_counter_margin ( struct res_counter * cnt ) <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& cnt -> lock , flags ); <nl> - margin = cnt -> limit - cnt -> usage ; <nl> + if ( cnt -> limit > cnt -> usage ) <nl> + margin = cnt -> limit - cnt -> usage ; <nl> + else <nl> + margin = 0 ; <nl> spin_unlock_irqrestore (& cnt -> lock , flags ); <nl> return margin ; <nl> }
static inline int local_sid_lookup ( struct id * entry ) <nl> return - 1 ; <nl> } <nl>  <nl> -/* Invalidate all id mappings on local core */ <nl> +/* Invalidate all id mappings on local core -- call with preempt disabled */ <nl> static inline void local_sid_destroy_all ( void ) <nl> { <nl> - preempt_disable (); <nl> __get_cpu_var ( pcpu_last_used_sid ) = 0 ; <nl> memset (& __get_cpu_var ( pcpu_sids ), 0 , sizeof ( __get_cpu_var ( pcpu_sids ))); <nl> - preempt_enable (); <nl> } <nl>  <nl> static void * kvmppc_e500_id_table_alloc ( struct kvmppc_vcpu_e500 * vcpu_e500 )
int xen_pcibk_enable_msix ( struct xen_pcibk_device * pdev , <nl> /* <nl> * PCI_COMMAND_MEMORY must be enabled , otherwise we may not be able <nl> * to access the BARs where the MSI - X entries reside . <nl> + * But VF devices are unique in which the PF needs to be checked . <nl> */ <nl> - pci_read_config_word ( dev , PCI_COMMAND , & cmd ); <nl> + pci_read_config_word ( pci_physfn ( dev ), PCI_COMMAND , & cmd ); <nl> if ( dev -> msi_enabled || !( cmd & PCI_COMMAND_MEMORY )) <nl> return - ENXIO ; <nl> 
static int do_lo_send_aops ( struct loop_device * lo , struct bio_vec * bvec , <nl> if ( ret ) <nl> goto fail ; <nl>  <nl> + file_update_time ( file ); <nl> + <nl> transfer_result = lo_do_transfer ( lo , WRITE , page , offset , <nl> bvec -> bv_page , bv_offs , size , IV ); <nl> copied = size ;
static int __init orion_nand_probe ( struct platform_device * pdev ) <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) { <nl> - err = - ENODEV ; <nl> + ret = - ENODEV ; <nl> goto no_res ; <nl> } <nl> 
cio_start_key ( struct subchannel * sch , /* subchannel structure */ <nl> CIO_TRACE_EVENT ( 4 , sch -> dev . bus_id ); <nl>  <nl> orb = & to_io_private ( sch )-> orb ; <nl> + memset ( orb , 0 , sizeof ( union orb )); <nl> /* sch is always under 2G . */ <nl> orb -> cmd . intparm = ( u32 )( addr_t ) sch ; <nl> orb -> cmd . fmt = 1 ;
static unsigned int iwl_hw_get_beacon_cmd ( struct iwl_priv * priv , <nl> sizeof ( frame -> u ) - sizeof (* tx_beacon_cmd )); <nl> if ( WARN_ON_ONCE ( frame_size > MAX_MPDU_SIZE )) <nl> return 0 ; <nl> + if (! frame_size ) <nl> + return 0 ; <nl>  <nl> /* Set up TX command fields */ <nl> tx_beacon_cmd -> tx . len = cpu_to_le16 (( u16 ) frame_size );
static int imx6ul_tsc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> adc_irq = platform_get_irq ( pdev , 1 ); <nl> - if ( adc_irq <= 0 ) { <nl> + if ( adc_irq < 0 ) { <nl> dev_err (& pdev -> dev , " no adc irq resource ?\ n "); <nl> return adc_irq ; <nl> }
static void brcmf_fws_dequeue_worker ( struct work_struct * worker ) <nl> fws = container_of ( worker , struct brcmf_fws_info , fws_dequeue_work ); <nl>  <nl> brcmf_fws_lock ( fws -> drvr , flags ); <nl> - for ( fifo = NL80211_NUM_ACS ; fifo >= 0 && ! fws -> bus_flow_blocked ; <nl> + for ( fifo = BRCMF_FWS_FIFO_BCMC ; fifo >= 0 && ! fws -> bus_flow_blocked ; <nl> fifo --) { <nl> while (( fws -> fifo_credit [ fifo ]) || ((! fws -> bcmc_credit_check ) && <nl> ( fifo == BRCMF_FWS_FIFO_BCMC ))) {
uint brcms_reset ( struct brcms_info * wl ) <nl> /* dpc will not be rescheduled */ <nl> wl -> resched = false ; <nl>  <nl> + /* inform publicly that interface is down */ <nl> + wl -> pub -> up = false ; <nl> + <nl> return 0 ; <nl> } <nl> 
lnet_ping ( lnet_process_id_t id , int timeout_ms , lnet_process_id_t * ids , int n_i <nl>  <nl> rc = - EFAULT ; /* If I SEGV ... */ <nl>  <nl> + memset (& tmpid , 0 , sizeof ( tmpid )); <nl> for ( i = 0 ; i < n_ids ; i ++) { <nl> tmpid . pid = info -> pi_pid ; <nl> tmpid . nid = info -> pi_ni [ i ]. ns_nid ;
static int pm2fb_check_var ( struct fb_var_screeninfo * var , struct fb_info * info ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + var -> transp . offset = 0 ; <nl> + var -> transp . length = 0 ; <nl> switch ( var -> bits_per_pixel ) { <nl> case 8 : <nl> var -> red . length = var -> green . length = var -> blue . length = 8 ;
static unsigned int bsg_poll ( struct file * file , poll_table * wait ) <nl> spin_lock_irq (& bd -> lock ); <nl> if (! list_empty (& bd -> done_list )) <nl> mask |= POLLIN | POLLRDNORM ; <nl> - if ( bd -> queued_cmds >= bd -> max_queue ) <nl> + if ( bd -> queued_cmds < bd -> max_queue ) <nl> mask |= POLLOUT ; <nl> spin_unlock_irq (& bd -> lock ); <nl> 
ia64_fault ( unsigned long vector , unsigned long isr , unsigned long ifa , <nl> printk ( KERN_ERR " iip - 0x % lx , ifa - 0x % lx , isr - 0x % lx \ n ", <nl> iip , ifa , isr ); <nl> force_sig ( SIGSEGV , current ); <nl> - break ; <nl> + return ; <nl>  <nl> case 46 : <nl> printk ( KERN_ERR " Unexpected IA - 32 intercept trap ( Trap 46 )\ n ");
 <nl> # define BOUNCE_SIZE ( 64 * 1024 ) <nl>  <nl> -# define PS3ROM_MAX_SECTORS ( BOUNCE_SIZE / CD_FRAMESIZE ) <nl> +# define PS3ROM_MAX_SECTORS ( BOUNCE_SIZE >> 9 ) <nl>  <nl>  <nl> struct ps3rom_private {
c4_add_dev ( hdw_info_t * hi , int brdno , unsigned long f0 , unsigned long f1 , <nl> hi -> devname , irq1 ); <nl> unregister_netdev ( ndev ); <nl> free_irq ( irq0 , ndev ); <nl> - OS_kfree ( ndev -> priv ); <nl> + OS_kfree ( netdev_priv ( ndev )); <nl> OS_kfree ( ndev ); <nl> error_flag = EIO ; <nl> return 0 ;
static int sirfsoc_gpio_probe ( struct device_node * np ) <nl> if ( err ) { <nl> dev_err (& pdev -> dev , <nl> " could not connect irqchip to gpiochip \ n "); <nl> - goto out ; <nl> + goto out_banks ; <nl> } <nl>  <nl> for ( i = 0 ; i < SIRFSOC_GPIO_NO_OF_BANKS ; i ++) {
void ath10k_thermal_set_throttling ( struct ath10k * ar ) <nl>  <nl> lockdep_assert_held (& ar -> conf_mutex ); <nl>  <nl> + if (! ar -> wmi . ops -> gen_pdev_set_quiet_mode ) <nl> + return ; <nl> + <nl> if ( ar -> state != ATH10K_STATE_ON ) <nl> return ; <nl> 
bool __init early_can_reuse_p2m_middle ( unsigned long set_pfn , unsigned long set_ <nl> if ( p2m_index ( set_pfn )) <nl> return false ; <nl>  <nl> - for ( pfn = 0 ; pfn <= MAX_DOMAIN_PAGES ; pfn += P2M_PER_PAGE ) { <nl> + for ( pfn = 0 ; pfn < MAX_DOMAIN_PAGES ; pfn += P2M_PER_PAGE ) { <nl> topidx = p2m_top_index ( pfn ); <nl>  <nl> if (! p2m_top [ topidx ])
void __init spear13xx_l2x0_init ( void ) <nl> * write alloc and ' Full line of zero ' options <nl> * <nl> */ <nl> + if (! IS_ENABLED ( CONFIG_CACHE_L2X0 )) <nl> + return ; <nl>  <nl> writel_relaxed ( 0x06 , VA_L2CC_BASE + L2X0_PREFETCH_CTRL ); <nl> 
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 81 " <nl> -# define DRV_MODULE_RELDATE " September 5 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 82 " <nl> +# define DRV_MODULE_RELDATE " October 5 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static struct file * do_last ( struct nameidata * nd , struct path * path , <nl> if ( error ) <nl> return ERR_PTR ( error ); <nl> error = - EISDIR ; <nl> - if ( S_ISDIR ( nd -> inode -> i_mode )) <nl> + if (( open_flag & O_CREAT ) && S_ISDIR ( nd -> inode -> i_mode )) <nl> goto exit ; <nl> error = - ENOTDIR ; <nl> if (( nd -> flags & LOOKUP_DIRECTORY ) && ! nd -> inode -> i_op -> lookup )
static void svm_get_segment ( struct kvm_vcpu * vcpu , <nl> if ( seg == VCPU_SREG_CS ) <nl> var -> g = s -> limit > 0xfffff ; <nl>  <nl> + /* <nl> + * Work around a bug where the busy flag in the tr selector <nl> + * isn ' t exposed <nl> + */ <nl> + if ( seg == VCPU_SREG_TR ) <nl> + var -> type |= 0x2 ; <nl> + <nl> var -> unusable = ! var -> present ; <nl> } <nl> 
vxge_hw_device_initialize ( <nl> __vxge_hw_device_pci_e_init ( hldev ); <nl>  <nl> status = __vxge_hw_device_reg_addr_get ( hldev ); <nl> - if ( status != VXGE_HW_OK ) <nl> + if ( status != VXGE_HW_OK ) { <nl> + vfree ( hldev ); <nl> goto exit ; <nl> + } <nl> __vxge_hw_device_id_get ( hldev ); <nl>  <nl> __vxge_hw_device_host_info_get ( hldev );
static int ath9k_sta_state ( struct ieee80211_hw * hw , <nl> } <nl>  <nl> if ( ath9k_is_chanctx_enabled ()) { <nl> - if ( old_state == IEEE80211_STA_ASSOC && <nl> - new_state == IEEE80211_STA_AUTHORIZED ) <nl> - ath_chanctx_event ( sc , vif , <nl> - ATH_CHANCTX_EVENT_AUTHORIZED ); <nl> + if ( vif -> type == NL80211_IFTYPE_STATION ) { <nl> + if ( old_state == IEEE80211_STA_ASSOC && <nl> + new_state == IEEE80211_STA_AUTHORIZED ) <nl> + ath_chanctx_event ( sc , vif , <nl> + ATH_CHANCTX_EVENT_AUTHORIZED ); <nl> + } <nl> } <nl>  <nl> return ret ;
static int chd_dec_fetch_cdata ( struct crystalhd_adp * adp , <nl> if ( rc ) { <nl> BCMLOG_ERR (" failed to pull add_cdata sz :% x ua_off :% x \ n ", <nl> io -> add_cdata_sz , ( unsigned int ) ua_off ); <nl> - kfree ( io -> add_cdata ); <nl> + vfree ( io -> add_cdata ); <nl> io -> add_cdata = NULL ; <nl> return - ENODATA ; <nl> }
ath5k_hw_set_antenna_mode ( struct ath5k_hw * ah , u8 ant_mode ) <nl> u8 def_ant , tx_ant , ee_mode ; <nl> u32 sta_id1 = 0 ; <nl>  <nl> + /* if channel is not initialized yet we can ' t set the antennas <nl> + * so just store the mode . it will be set on the next reset */ <nl> + if ( channel == NULL ) { <nl> + ah -> ah_ant_mode = ant_mode ; <nl> + return ; <nl> + } <nl> + <nl> def_ant = ah -> ah_def_ant ; <nl>  <nl> ATH5K_TRACE ( ah -> ah_sc );
static int FNAME ( fix_write_pf )( struct kvm_vcpu * vcpu , <nl> struct kvm_mmu_page * page ; <nl>  <nl> if ( is_writeble_pte (* shadow_ent )) <nl> - return 0 ; <nl> + return ! user || (* shadow_ent & PT_USER_MASK ); <nl>  <nl> writable_shadow = * shadow_ent & PT_SHADOW_WRITABLE_MASK ; <nl> if ( user ) {
static int perf_event__repipe_attr ( struct perf_tool * tool , <nl> union perf_event * event , <nl> struct perf_evlist ** pevlist ) <nl> { <nl> + struct perf_inject * inject = container_of ( tool , struct perf_inject , <nl> + tool ); <nl> int ret ; <nl>  <nl> ret = perf_event__process_attr ( tool , event , pevlist ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if (! inject -> pipe_output ) <nl> + return 0 ; <nl> + <nl> return perf_event__repipe_synth ( tool , event ); <nl> } <nl> 
struct dst_entry * fib6_rule_lookup ( struct net * net , struct flowi * fl , <nl> { <nl> struct fib_lookup_arg arg = { <nl> . lookup_ptr = lookup , <nl> + . flags = FIB_LOOKUP_NOREF , <nl> }; <nl>  <nl> fib_rules_lookup ( net -> ipv6 . fib6_rules_ops , fl , flags , & arg ); <nl> - if ( arg . rule ) <nl> - fib_rule_put ( arg . rule ); <nl>  <nl> if ( arg . result ) <nl> return arg . result ;
static int ceph_link ( struct dentry * old_dentry , struct inode * dir , <nl> req -> r_locked_dir = dir ; <nl> req -> r_dentry_drop = CEPH_CAP_FILE_SHARED ; <nl> req -> r_dentry_unless = CEPH_CAP_FILE_EXCL ; <nl> + /* release LINK_SHARED on source inode ( mds will lock it ) */ <nl> + req -> r_old_inode_drop = CEPH_CAP_LINK_SHARED ; <nl> err = ceph_mdsc_do_request ( mdsc , dir , req ); <nl> if ( err ) { <nl> d_drop ( dentry );
int xhci_add_endpoint ( struct usb_hcd * hcd , struct usb_device * udev , <nl> * for usb_set_interface () and usb_set_configuration () claim ). <nl> */ <nl> if ( xhci_endpoint_init ( xhci , xhci -> devs [ udev -> slot_id ], <nl> - udev , ep , GFP_KERNEL ) < 0 ) { <nl> + udev , ep , GFP_NOIO ) < 0 ) { <nl> dev_dbg (& udev -> dev , "% s - could not initialize ep %# x \ n ", <nl> __func__ , ep -> desc . bEndpointAddress ); <nl> return - ENOMEM ;
static int device_authorization ( struct hdpvr_device * dev ) <nl> hex_dump_to_buffer ( response , 8 , 16 , 1 , print_buf , 5 * buf_size + 1 , 0 ); <nl> v4l2_dbg ( MSG_INFO , hdpvr_debug , & dev -> v4l2_dev , " response : % s \ n ", <nl> print_buf ); <nl> + kfree ( print_buf ); <nl> # endif <nl>  <nl> msleep ( 100 );
static int hostap_enable_hostapd ( PSDevice pDevice , int rtnl_locked ) <nl> if ( ret ) { <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO "% s : register_netdevice ( AP ) failed !\ n ", <nl> dev -> name ); <nl> + free_netdev ( pDevice -> apdev ); <nl> + pDevice -> apdev = NULL ; <nl> return - 1 ; <nl> } <nl> 
static int __do_index ( struct sw842_param * p , u8 size , u8 bits , u64 fsize ) <nl> /* this is where the current fifo is */ <nl> u64 section = round_down ( total , fsize ); <nl> /* the current pos in the fifo */ <nl> - u64 pos = total % fsize ; <nl> + u64 pos = total - section ; <nl>  <nl> /* if the offset is past / at the pos , we need to <nl> * go back to the last fifo section
static const struct of_device_id spear13xx_pcie_of_match [] = { <nl> }; <nl> MODULE_DEVICE_TABLE ( of , spear13xx_pcie_of_match ); <nl>  <nl> - static struct platform_driver spear13xx_pcie_driver = { <nl> + static struct platform_driver spear13xx_pcie_driver __initdata = { <nl> . probe = spear13xx_pcie_probe , <nl> . remove = spear13xx_pcie_remove , <nl> . driver = {
static int md_open ( struct block_device * bdev , fmode_t mode ) <nl> struct mddev * mddev = mddev_find ( bdev -> bd_dev ); <nl> int err ; <nl>  <nl> + if (! mddev ) <nl> + return - ENODEV ; <nl> + <nl> if ( mddev -> gendisk != bdev -> bd_disk ) { <nl> /* we are racing with mddev_put which is discarding this <nl> * bd_disk .
static inline void __init ulite_console_of_find_device ( int id ) <nl> continue ; <nl>  <nl> ulite_ports [ id ]. mapbase = res . start ; <nl> + of_node_put ( np ); <nl> return ; <nl> } <nl> }
int saa7134_input_init1 ( struct saa7134_dev * dev ) <nl> return - ENOMEM ; <nl> } <nl>  <nl> + ir -> dev = input_dev ; <nl> + <nl> /* init hardware - specific stuff */ <nl> ir -> mask_keycode = mask_keycode ; <nl> ir -> mask_keydown = mask_keydown ;
static int wm8731_check_osc ( struct snd_soc_dapm_widget * source , <nl> { <nl> struct wm8731_priv * wm8731 = snd_soc_codec_get_drvdata ( source -> codec ); <nl>  <nl> - return wm8731 -> sysclk_type == WM8731_SYSCLK_MCLK ; <nl> + return wm8731 -> sysclk_type == WM8731_SYSCLK_XTAL ; <nl> } <nl>  <nl> static const struct snd_soc_dapm_route wm8731_intercon [] = {
int drbd_al_begin_io_nonblock ( struct drbd_conf * mdev , struct drbd_interval * i ) <nl> if ( unlikely ( tmp != NULL )) { <nl> struct bm_extent * bm_ext = lc_entry ( tmp , struct bm_extent , lce ); <nl> if ( test_bit ( BME_NO_WRITES , & bm_ext -> flags )) { <nl> - if (! test_and_set_bit ( BME_PRIORITY , & bm_ext -> flags )); <nl> + if (! test_and_set_bit ( BME_PRIORITY , & bm_ext -> flags )) <nl> return - EBUSY ; <nl> return - EWOULDBLOCK ; <nl> }
int ipoib_vlan_delete ( struct net_device * pdev , unsigned short pkey ) <nl> if ( priv -> pkey == pkey ) { <nl> unregister_netdev ( priv -> dev ); <nl> ipoib_dev_cleanup ( priv -> dev ); <nl> - <nl> list_del (& priv -> list ); <nl> - <nl> - kfree ( priv ); <nl> + free_netdev ( priv -> dev ); <nl>  <nl> ret = 0 ; <nl> break ;
static void fc_rport_plogi_resp ( struct fc_seq * sp , struct fc_frame * fp , <nl>  <nl> tov = ntohl ( plp -> fl_csp . sp_e_d_tov ); <nl> if ( ntohs ( plp -> fl_csp . sp_features ) & FC_SP_FT_EDTR ) <nl> - tov /= 1000 ; <nl> + tov /= 1000000 ; <nl> if ( tov > rdata -> e_d_tov ) <nl> rdata -> e_d_tov = tov ; <nl> csp_seq = ntohs ( plp -> fl_csp . sp_tot_seq );
static void prepare_dma ( struct s3c64xx_spi_dma_data * dma , <nl> struct scatterlist sg ; <nl> struct dma_async_tx_descriptor * desc ; <nl>  <nl> + memset (& config , 0 , sizeof ( config )); <nl> + <nl> if ( dma -> direction == DMA_DEV_TO_MEM ) { <nl> sdd = container_of (( void *) dma , <nl> struct s3c64xx_spi_driver_data , rx_dma );
static void stmmac_clk_csr_set ( struct stmmac_priv * priv ) <nl> priv -> clk_csr = STMMAC_CSR_100_150M ; <nl> else if (( clk_rate >= CSR_F_150M ) && ( clk_rate < CSR_F_250M )) <nl> priv -> clk_csr = STMMAC_CSR_150_250M ; <nl> - else if (( clk_rate >= CSR_F_250M ) && ( clk_rate < CSR_F_300M )) <nl> + else if (( clk_rate >= CSR_F_250M ) && ( clk_rate <= CSR_F_300M )) <nl> priv -> clk_csr = STMMAC_CSR_250_300M ; <nl> } <nl> }
static int afiucv_hs_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> break ; <nl> case 0 : <nl> /* plain data frame */ <nl> + memcpy ( CB_TRGCLS ( skb ), & trans_hdr -> iucv_hdr . class , <nl> + CB_TRGCLS_LEN ); <nl> err = afiucv_hs_callback_rx ( sk , skb ); <nl> break ; <nl> default :
mwifiex_config_scan ( struct mwifiex_private * priv , <nl> wildcard_ssid_tlv -> max_ssid_length = <nl> IEEE80211_MAX_SSID_LEN ; <nl>  <nl> + if (! memcmp ( user_scan_in -> ssid_list [ i ]. ssid , <nl> + " DIRECT -", 7 )) <nl> + wildcard_ssid_tlv -> max_ssid_length = 0xfe ; <nl> + <nl> memcpy ( wildcard_ssid_tlv -> ssid , <nl> user_scan_in -> ssid_list [ i ]. ssid , ssid_len ); <nl> 
void prune_icache_sb ( struct super_block * sb , int nr_to_scan ) <nl> * inode to the back of the list so we don ' t spin on it . <nl> */ <nl> if (! spin_trylock (& inode -> i_lock )) { <nl> - list_move (& inode -> i_lru , & sb -> s_inode_lru ); <nl> + list_move_tail (& inode -> i_lru , & sb -> s_inode_lru ); <nl> continue ; <nl> } <nl> 
static int kdb_ll ( int argc , const char ** argv ) <nl> while ( va ) { <nl> char buf [ 80 ]; <nl>  <nl> + if ( KDB_FLAG ( CMD_INTERRUPT )) <nl> + return 0 ; <nl> + <nl> sprintf ( buf , "% s " kdb_machreg_fmt "\ n ", command , va ); <nl> diag = kdb_parse ( buf ); <nl> if ( diag )
static int remove_and_add_spares ( struct mddev * mddev , <nl> ! test_bit ( Bitmap_sync , & rdev -> flags ))) <nl> continue ; <nl>  <nl> - rdev -> recovery_offset = 0 ; <nl> + if ( rdev -> saved_raid_disk < 0 ) <nl> + rdev -> recovery_offset = 0 ; <nl> if ( mddev -> pers -> <nl> hot_add_disk ( mddev , rdev ) == 0 ) { <nl> if ( sysfs_link_rdev ( mddev , rdev ))
static int f2fs_write_end ( struct file * file , <nl> if ( pos + copied > i_size_read ( inode )) { <nl> i_size_write ( inode , pos + copied ); <nl> mark_inode_dirty ( inode ); <nl> - update_inode_page ( inode ); <nl> } <nl>  <nl> f2fs_put_page ( page , 1 );
# undef __get_str <nl>  <nl> # undef TP_printk <nl> -# define TP_printk ( fmt , args ...) "% s , % s \ n ", # fmt , __stringify ( args ) <nl> +# define TP_printk ( fmt , args ...) "\"% s \", % s \ n ", fmt , __stringify ( args ) <nl>  <nl> # undef TP_fast_assign <nl> # define TP_fast_assign ( args ...) args
next_button : <nl> if ( dev -> num_button_polling_addresses ) { <nl> memset ( dev -> button_polling_last_values , 0 , <nl> EM28XX_NUM_BUTTON_ADDRESSES_MAX ); <nl> - INIT_DELAYED_WORK (& dev -> buttons_query_work , <nl> - em28xx_query_buttons ); <nl> schedule_delayed_work (& dev -> buttons_query_work , <nl> msecs_to_jiffies ( dev -> button_polling_interval )); <nl> } <nl> static int em28xx_ir_init ( struct em28xx * dev ) <nl> } <nl>  <nl> kref_get (& dev -> ref ); <nl> + INIT_DELAYED_WORK (& dev -> buttons_query_work , em28xx_query_buttons ); <nl>  <nl> if ( dev -> board . buttons ) <nl> em28xx_init_buttons ( dev );
static int nr_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> sax -> fsa_ax25 . sax25_family = AF_NETROM ; <nl> sax -> fsa_ax25 . sax25_ndigis = 1 ; <nl> sax -> fsa_ax25 . sax25_call = nr -> user_addr ; <nl> + memset ( sax -> fsa_digipeater , 0 , sizeof ( sax -> fsa_digipeater )); <nl> sax -> fsa_digipeater [ 0 ] = nr -> dest_addr ; <nl> * uaddr_len = sizeof ( struct full_sockaddr_ax25 ); <nl> } else {
int ip_recv_error ( struct sock * sk , struct msghdr * msg , int len , int * addr_len ) <nl> int err ; <nl> int copied ; <nl>  <nl> + WARN_ON_ONCE ( sk -> sk_family == AF_INET6 ); <nl> + <nl> err = - EAGAIN ; <nl> skb = sock_dequeue_err_skb ( sk ); <nl> if ( skb == NULL )
int snd_pcm_hw_refine ( struct snd_pcm_substream * substream , <nl> snd_mask_max (& params -> masks [ SNDRV_PCM_HW_PARAM_CHANNELS ])) { <nl> changed = substream -> ops -> ioctl ( substream , <nl> SNDRV_PCM_IOCTL1_FIFO_SIZE , params ); <nl> - if ( params < 0 ) <nl> + if ( changed < 0 ) <nl> return changed ; <nl> } <nl> }
again : <nl> key . offset = found_key . offset - 1 ; <nl> wc . replay_dest -> log_root = NULL ; <nl> free_extent_buffer ( log -> node ); <nl> + free_extent_buffer ( log -> commit_root ); <nl> kfree ( log ); <nl>  <nl> if ( found_key . offset == 0 )
int skb_gro_receive ( struct sk_buff ** head , struct sk_buff * skb ) <nl> * NAPI_GRO_CB ( nskb ) = * NAPI_GRO_CB ( p ); <nl> skb_shinfo ( nskb )-> frag_list = p ; <nl> skb_shinfo ( nskb )-> gso_size = pinfo -> gso_size ; <nl> + pinfo -> gso_size = 0 ; <nl> skb_header_release ( p ); <nl> nskb -> prev = p ; <nl> 
acpi_ns_lookup ( union acpi_generic_state * scope_info , <nl> * segments ). <nl> */ <nl> if ( this_node -> type == ACPI_TYPE_LOCAL_ALIAS ) { <nl> + if (! this_node -> object ) { <nl> + return_ACPI_STATUS ( AE_NOT_EXIST ); <nl> + } <nl> + <nl> if ( acpi_ns_opens_scope <nl> ((( struct acpi_namespace_node *) this_node -> <nl> object )-> type )) {
static int nortel_pci_cor_reset ( struct orinoco_private * priv ) <nl> return 0 ; <nl> } <nl>  <nl> - int nortel_pci_hw_init ( struct nortel_pci_card * card ) <nl> + static int nortel_pci_hw_init ( struct nortel_pci_card * card ) <nl> { <nl> int i ; <nl> u32 reg ;
out : <nl> if ( mem_tight != 0 ) <nl> cfs_memory_pressure_restore ( mpflag ); <nl>  <nl> - if ( crattr != NULL ) { <nl> - kfree ( crattr ); <nl> - } <nl> + kfree ( crattr ); <nl>  <nl> if ( rc != 0 ) { <nl> LASSERT ( req == NULL );
static int sst_platform_pcm_trigger ( struct snd_pcm_substream * substream , <nl> struct snd_soc_pcm_runtime * rtd = substream -> private_data ; <nl>  <nl> dev_dbg ( rtd -> dev , " sst_platform_pcm_trigger called \ n "); <nl> + if ( substream -> pcm -> internal ) <nl> + return 0 ; <nl> stream = substream -> runtime -> private_data ; <nl> str_id = stream -> stream_info . str_id ; <nl> switch ( cmd ) {
static int k3_dma_probe ( struct platform_device * op ) <nl> d -> slave . device_issue_pending = k3_dma_issue_pending ; <nl> d -> slave . device_control = k3_dma_control ; <nl> d -> slave . copy_align = DMA_ALIGN ; <nl> - d -> slave . chancnt = d -> dma_requests ; <nl>  <nl> /* init virtual channel */ <nl> d -> chans = devm_kzalloc (& op -> dev ,
static void __devinit pci_fixed_bar_fixup ( struct pci_dev * dev ) <nl> u32 size ; <nl> int i ; <nl>  <nl> + /* Must have extended configuration space */ <nl> + if ( dev -> cfg_size < PCIE_CAP_OFFSET + 4 ) <nl> + return ; <nl> + <nl> /* Fixup the BAR sizes for fixed BAR devices and make them unmoveable */ <nl> offset = fixed_bar_cap ( dev -> bus , dev -> devfn ); <nl> if (! offset || PCI_DEVFN ( 2 , 0 ) == dev -> devfn ||
static int sdhci_bcm_kona_probe ( struct platform_device * pdev ) <nl> kona_dev = sdhci_pltfm_priv ( pltfm_priv ); <nl> mutex_init (& kona_dev -> write_lock ); <nl>  <nl> - mmc_of_parse ( host -> mmc ); <nl> + ret = mmc_of_parse ( host -> mmc ); <nl> + if ( ret ) <nl> + goto err_pltfm_free ; <nl>  <nl> if (! host -> mmc -> f_max ) { <nl> dev_err (& pdev -> dev , " Missing max - freq for SDHCI cfg \ n ");
void rt2x00link_update_stats ( struct rt2x00_dev * rt2x00dev , <nl> struct link_ant * ant = & rt2x00dev -> link . ant ; <nl> struct ieee80211_hdr * hdr = ( struct ieee80211_hdr *) skb -> data ; <nl>  <nl> + /* <nl> + * No need to update the stats for != STA interfaces <nl> + */ <nl> + if (! rt2x00dev -> intf_sta_count ) <nl> + return ; <nl> + <nl> /* <nl> * Frame was received successfully since non - succesfull <nl> * frames would have been dropped by the hardware .
static inline void update_sd_power_savings_stats ( struct sched_group * group , <nl> * capacity but still has some space to pick up some load <nl> * from other group and save more power <nl> */ <nl> - if ( sgs -> sum_nr_running > sgs -> group_capacity - 1 ) <nl> + if ( sgs -> sum_nr_running + 1 > sgs -> group_capacity ) <nl> return ; <nl>  <nl> if ( sgs -> sum_nr_running > sds -> leader_nr_running ||
wlc_d11hdrs_mac80211 ( struct wlc_info * wlc , struct ieee80211_hw * hw , <nl>  <nl> /* add Broadcom tx descriptor header */ <nl> txh = ( d11txh_t *) skb_push ( p , D11_TXH_LEN ); <nl> - memset (( char *) txh , 0 , D11_TXH_LEN ); <nl> + memset ( txh , 0 , D11_TXH_LEN ); <nl>  <nl> /* setup frameid */ <nl> if ( tx_info -> flags & IEEE80211_TX_CTL_ASSIGN_SEQ ) {
error3 : atomic_dec (& cm_id_priv -> refcount ); <nl> cm_deref_id ( listen_cm_id_priv ); <nl> cm_cleanup_timewait ( cm_id_priv -> timewait_info ); <nl> error2 : kfree ( cm_id_priv -> timewait_info ); <nl> + cm_id_priv -> timewait_info = NULL ; <nl> error1 : ib_destroy_cm_id (& cm_id_priv -> id ); <nl> return ret ; <nl> }
static void md_update_sb ( mddev_t * mddev , int force_change ) <nl> int sync_req ; <nl> int nospares = 0 ; <nl>  <nl> + if ( mddev -> external ) <nl> + return ; <nl> repeat : <nl> spin_lock_irq (& mddev -> write_lock ); <nl> 
static int btrfs_remount ( struct super_block * sb , int * flags , char * data ) <nl> struct btrfs_root * root = btrfs_sb ( sb ); <nl> int ret ; <nl>  <nl> + ret = btrfs_parse_options ( root , data ); <nl> + if ( ret ) <nl> + return - EINVAL ; <nl> + <nl> if ((* flags & MS_RDONLY ) == ( sb -> s_flags & MS_RDONLY )) <nl> return 0 ; <nl> 
static int sh_eth_drv_probe ( struct platform_device * pdev ) <nl> } <nl> mdp -> tsu_addr = ioremap ( rtsu -> start , <nl> resource_size ( rtsu )); <nl> + if ( mdp -> tsu_addr == NULL ) { <nl> + ret = - ENOMEM ; <nl> + dev_err (& pdev -> dev , " TSU ioremap failed .\ n "); <nl> + goto out_release ; <nl> + } <nl> mdp -> port = devno % 2 ; <nl> ndev -> features = NETIF_F_HW_VLAN_FILTER ; <nl> }
static int aic3x_read ( struct snd_soc_codec * codec , unsigned int reg , <nl> if ( reg >= AIC3X_CACHEREGNUM ) <nl> return - 1 ; <nl>  <nl> - * value = codec -> hw_read ( codec , reg ); <nl> + codec -> cache_bypass = 1 ; <nl> + * value = snd_soc_read ( codec , reg ); <nl> + codec -> cache_bypass = 0 ; <nl> + <nl> cache [ reg ] = * value ; <nl>  <nl> return 0 ;
int mls_context_to_sid ( char oldc , <nl>  <nl> if (! selinux_mls_enabled ) { <nl> if ( def_sid != SECSID_NULL && oldc ) <nl> - * scontext += strlen (* scontext ); <nl> + * scontext += strlen (* scontext )+ 1 ; <nl> return 0 ; <nl> } <nl> 
static int __cpuinit comp_pool_callback ( struct notifier_block * nfb , <nl> ehca_gen_dbg (" CPU : % x ( CPU_PREPARE )", cpu ); <nl> if (! create_comp_task ( pool , cpu )) { <nl> ehca_gen_err (" Can ' t create comp_task for cpu : % x ", cpu ); <nl> - return NOTIFY_BAD ; <nl> + return notifier_from_errno (- ENOMEM ); <nl> } <nl> break ; <nl> case CPU_UP_CANCELED :
int eeh_dn_check_failure ( struct device_node * dn , struct pci_dev * dev ) <nl> no_dn ++; <nl> return 0 ; <nl> } <nl> + dn = find_device_pe ( dn ); <nl> pdn = PCI_DN ( dn ); <nl>  <nl> /* Access to IO BARs might get this far and still not want checking . */
static int fc2580_set_params ( struct dvb_frontend * fe ) <nl> { <nl> struct fc2580_priv * priv = fe -> tuner_priv ; <nl> struct dtv_frontend_properties * c = & fe -> dtv_property_cache ; <nl> - int ret , i ; <nl> + int ret = 0 , i ; <nl> unsigned int r_val , n_val , k_val , k_val_reg , f_ref ; <nl> u8 tmp_val , r18_val ; <nl> u64 f_vco ;
static int btrfs_xattr_acl_set ( struct dentry * dentry , const char * name , <nl> int ret ; <nl> struct posix_acl * acl = NULL ; <nl>  <nl> + if (! is_owner_or_cap ( dentry -> d_inode )) <nl> + return - EPERM ; <nl> + <nl> if ( value ) { <nl> acl = posix_acl_from_xattr ( value , size ); <nl> if ( acl == NULL ) {
bool rtl92cu_rx_query_desc ( struct ieee80211_hw * hw , <nl> ( bool ) GET_RX_DESC_PAGGR ( pdesc )); <nl> rx_status -> mactime = GET_RX_DESC_TSFL ( pdesc ); <nl> if ( phystatus ) { <nl> - p_drvinfo = ( struct rx_fwinfo_92c *)( pdesc + RTL_RX_DESC_SIZE ); <nl> + p_drvinfo = ( struct rx_fwinfo_92c *)( skb -> data + <nl> + stats -> rx_bufshift ); <nl> rtl92c_translate_rx_signal_stuff ( hw , skb , stats , pdesc , <nl> p_drvinfo ); <nl> }
static void dwc2_hsotg_start_req ( struct dwc2_hsotg * hsotg , <nl> /* If endpoint is stalled , we will restart request later */ <nl> ctrl = dwc2_readl ( hsotg -> regs + epctrl_reg ); <nl>  <nl> - if ( ctrl & DXEPCTL_STALL ) { <nl> + if ( index && ctrl & DXEPCTL_STALL ) { <nl> dev_warn ( hsotg -> dev , "% s : ep % d is stalled \ n ", __func__ , index ); <nl> return ; <nl> }
int fib_nh_match ( struct fib_config * cfg , struct fib_info * fi ) <nl> return 1 ; <nl>  <nl> attrlen = rtnh_attrlen ( rtnh ); <nl> - if ( attrlen < 0 ) { <nl> + if ( attrlen > 0 ) { <nl> struct nlattr * nla , * attrs = rtnh_attrs ( rtnh ); <nl>  <nl> nla = nla_find ( attrs , attrlen , RTA_GATEWAY );
static int gpio_setup_irq ( struct gpio_desc * desc , struct device * dev , <nl> return 0 ; <nl>  <nl> free_sd : <nl> - sysfs_put ( pdesc -> value_sd ); <nl> + if ( pdesc ) <nl> + sysfs_put ( pdesc -> value_sd ); <nl> free_id : <nl> idr_remove (& pdesc_idr , id ); <nl> desc -> flags &= GPIO_FLAGS_MASK ;
static int wm5100_set_sysclk ( struct snd_soc_codec * codec , int clk_id , <nl> fval = 1 ; <nl> break ; <nl> case 22579200 : <nl> - case 2457600 : <nl> + case 24576000 : <nl> fval = 2 ; <nl> break ; <nl> default : <nl> static int wm5100_set_sysclk ( struct snd_soc_codec * codec , int clk_id , <nl>  <nl> case 6144000 : <nl> case 12288000 : <nl> - case 2457600 : <nl> + case 24576000 : <nl> audio_rate = 48000 ; <nl> break ; <nl> 
handle_locking_key ( struct input_dev * visorinput_dev , <nl> int keycode , int desired_state ) <nl> { <nl> int led ; <nl> - char * sled ; <nl>  <nl> switch ( keycode ) { <nl> case KEY_CAPSLOCK : <nl> led = LED_CAPSL ; <nl> - sled = " CAP "; <nl> break ; <nl> case KEY_SCROLLLOCK : <nl> led = LED_SCROLLL ; <nl> - sled = " SCR "; <nl> break ; <nl> case KEY_NUMLOCK : <nl> led = LED_NUML ; <nl> - sled = " NUM "; <nl> break ; <nl> default : <nl> led = - 1 ;
skip_create_disk : <nl> blk_queue_max_hw_sectors ( dd -> queue , 0xffff ); <nl> blk_queue_max_segment_size ( dd -> queue , 0x400000 ); <nl> blk_queue_io_min ( dd -> queue , 4096 ); <nl> + blk_queue_bounce_limit ( dd -> queue , dd -> pdev -> dma_mask ); <nl>  <nl> /* <nl> * write back cache is not supported in the device . FUA depends on
static void _rtl8821ae_read_adapter_info ( struct ieee80211_hw * hw , bool b_pseudo_ <nl> if ( rtlefuse -> eeprom_channelplan == 0xff ) <nl> rtlefuse -> eeprom_channelplan = 0x7F ; <nl>  <nl> - /* set channel paln to world wide 13 */ <nl> - /* rtlefuse -> channel_plan = ( u8 ) rtlefuse -> eeprom_channelplan ; */ <nl> + /* set channel plan from efuse */ <nl> + rtlefuse -> channel_plan = rtlefuse -> eeprom_channelplan ; <nl>  <nl> /* parse xtal */ <nl> rtlefuse -> crystalcap = hwinfo [ EEPROM_XTAL_8821AE ];
void wl1271_tx_work_locked ( struct wl1271 * wl ) <nl>  <nl> /* if rates have changed , re - configure the rate policy */ <nl> if ( unlikely ( sta_rates )) { <nl> + ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + woken_up = true ; <nl> + <nl> wl -> rate_set = wl1271_tx_enabled_rates_get ( wl , sta_rates ); <nl> wl1271_acx_rate_policies ( wl ); <nl> }
static int pmic_irq_type ( unsigned irq , unsigned type ) <nl> u32 gpio = irq - pg -> irq_base ; <nl> unsigned long flags ; <nl>  <nl> - if ( gpio > pg -> chip . ngpio ) <nl> + if ( gpio >= pg -> chip . ngpio ) <nl> return - EINVAL ; <nl>  <nl> spin_lock_irqsave (& pg -> irqtypes . lock , flags );
static int pcnet32_open ( struct net_device * dev ) <nl> lp -> rx_dma_addr [ i ] = 0 ; <nl> } <nl>  <nl> - pcnet32_free_ring ( dev ); <nl> - <nl> /* <nl> * Switch back to 16bit mode to avoid problems with dumb <nl> * DOS packet driver after a warm reboot
static int sep_get_time_handler ( unsigned long arg ) <nl> struct sep_driver_get_time_t command_args ; <nl>  <nl> error = sep_set_time (& command_args . time_physical_address , & command_args . time_value ); <nl> - error = copy_to_user (( void *) arg , ( void *) & command_args , sizeof ( struct sep_driver_get_time_t )); <nl> + if ( error == 0 ) <nl> + error = copy_to_user (( void __user *) arg , <nl> + & command_args , sizeof ( struct sep_driver_get_time_t )); <nl> return error ; <nl>  <nl> }
void hists__output_recalc_col_len ( struct hists * hists , int max_rows ) <nl>  <nl> while ( next && row ++ < max_rows ) { <nl> n = rb_entry ( next , struct hist_entry , rb_node ); <nl> - hists__calc_col_len ( hists , n ); <nl> + if (! n -> filtered ) <nl> + hists__calc_col_len ( hists , n ); <nl> next = rb_next (& n -> rb_node ); <nl> } <nl> }
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> + memset ( sax , 0 , sizeof ( sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
static int ipw_up ( struct ipw_priv * priv ) <nl> if (!( priv -> config & CFG_CUSTOM_MAC )) <nl> eeprom_parse_mac ( priv , priv -> mac_addr ); <nl> memcpy ( priv -> net_dev -> dev_addr , priv -> mac_addr , ETH_ALEN ); <nl> + memcpy ( priv -> net_dev -> perm_addr , priv -> mac_addr , ETH_ALEN ); <nl>  <nl> for ( j = 0 ; j < ARRAY_SIZE ( ipw_geos ); j ++) { <nl> if (! memcmp (& priv -> eeprom [ EEPROM_COUNTRY_CODE ],
static void __init xen_rebuild_p2m_list ( unsigned long * p2m ) <nl> p2m_missing_pte : p2m_identity_pte ; <nl> for ( i = 0 ; i < PMDS_PER_MID_PAGE ; i ++) { <nl> pmdp = populate_extra_pmd ( <nl> - ( unsigned long )( p2m + pfn + i * PTRS_PER_PTE )); <nl> + ( unsigned long )( p2m + pfn ) + i * PMD_SIZE ); <nl> set_pmd ( pmdp , __pmd ( __pa ( ptep ) | _KERNPG_TABLE )); <nl> } <nl> }
static int validate_region_size ( struct raid_set * rs , unsigned long region_size ) <nl> static int validate_raid_redundancy ( struct raid_set * rs ) <nl> { <nl> unsigned i , rebuild_cnt = 0 ; <nl> - unsigned rebuilds_per_group , copies , d ; <nl> + unsigned rebuilds_per_group = 0 , copies , d ; <nl> unsigned group_size , last_group_start ; <nl>  <nl> for ( i = 0 ; i < rs -> md . raid_disks ; i ++)
static int __init wb_module_init ( void ) <nl>  <nl> err = map_bios (); <nl> if ( err ) <nl> - return err ; <nl> + goto err_free_keymap ; <nl>  <nl> err = platform_driver_register (& wistron_driver ); <nl> if ( err ) <nl> static int __init wb_module_init ( void ) <nl> platform_driver_unregister (& wistron_driver ); <nl> err_unmap_bios : <nl> unmap_bios (); <nl> + err_free_keymap : <nl> + kfree ( keymap ); <nl>  <nl> return err ; <nl> }
static int era_is_congested ( struct dm_target_callbacks * cb , int bdi_bits ) <nl>  <nl> static void era_destroy ( struct era * era ) <nl> { <nl> - metadata_close ( era -> md ); <nl> + if ( era -> md ) <nl> + metadata_close ( era -> md ); <nl>  <nl> if ( era -> wq ) <nl> destroy_workqueue ( era -> wq );
int sctp_sysctl_net_register ( struct net * net ) <nl> table [ i ]. data += ( char *)(& net -> sctp ) - ( char *)& init_net . sctp ; <nl>  <nl> net -> sctp . sysctl_header = register_net_sysctl ( net , " net / sctp ", table ); <nl> + if ( net -> sctp . sysctl_header == NULL ) { <nl> + kfree ( table ); <nl> + return - ENOMEM ; <nl> + } <nl> return 0 ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> + if ( ret != sizeof ( event )) { <nl> + printf (" Reading event failed !\ n "); <nl> + ret = - EIO ; <nl> + break ; <nl> + } <nl> + <nl> print_event (& event ); <nl> } <nl> 
void radeon_compute_pll ( struct radeon_pll * pll , <nl> * frac_fb_div_p = best_frac_feedback_div ; <nl> * ref_div_p = best_ref_div ; <nl> * post_div_p = best_post_div ; <nl> + DRM_DEBUG_KMS ("% d % d , pll dividers - fb : % d .% d ref : % d , post % d \ n ", <nl> + freq , best_freq / 1000 , best_feedback_div , best_frac_feedback_div , <nl> + best_ref_div , best_post_div ); <nl> + <nl> } <nl>  <nl> static void radeon_user_framebuffer_destroy ( struct drm_framebuffer * fb )
static int sn_hwperf_op_cpu ( struct sn_hwperf_op_info * op_info ) <nl> else { <nl> /* migrate the task before calling SAL */ <nl> save_allowed = current -> cpus_allowed ; <nl> - set_cpus_allowed ( current , cpumask_of_cpu ( cpu )); <nl> + set_cpus_allowed_ptr ( current , cpumask_of ( cpu )); <nl> sn_hwperf_call_sal ( op_info ); <nl> - set_cpus_allowed ( current , save_allowed ); <nl> + set_cpus_allowed_ptr ( current , & save_allowed ); <nl> } <nl> } <nl> r = op_info -> ret ;
static void bnx2fc_recv_frame ( struct sk_buff * skb ) <nl> break ; <nl> } <nl> } <nl> + <nl> + if ( fh -> fh_r_ctl == FC_RCTL_BA_ABTS ) { <nl> + /* Drop incoming ABTS */ <nl> + put_cpu (); <nl> + kfree_skb ( skb ); <nl> + return ; <nl> + } <nl> + <nl> if ( le32_to_cpu ( fr_crc ( fp )) != <nl> ~ crc32 (~ 0 , skb -> data , fr_len )) { <nl> if ( stats -> InvalidCRCCount < 5 )
# define SEEK_MAX SEEK_END <nl>  <nl> struct fstrim_range { <nl> - uint64_t start ; <nl> - uint64_t len ; <nl> - uint64_t minlen ; <nl> + __u64 start ; <nl> + __u64 len ; <nl> + __u64 minlen ; <nl> }; <nl>  <nl> /* And dynamically - tunable limits and defaults : */
out_unlock : <nl> * because the core might be gone away while we unlocked the mutex . */ <nl> static struct b43_wldev * b43_wireless_core_stop ( struct b43_wldev * dev ) <nl> { <nl> - struct b43_wl * wl = dev -> wl ; <nl> + struct b43_wl * wl ; <nl> struct b43_wldev * orig_dev ; <nl> u32 mask ; <nl>  <nl> + if (! dev ) <nl> + return NULL ; <nl> + wl = dev -> wl ; <nl> redo : <nl> if (! dev || b43_status ( dev ) < B43_STAT_STARTED ) <nl> return dev ;
static void balloon_up ( struct work_struct * dummy ) <nl> floor = compute_balloon_floor (); <nl>  <nl> /* Refuse to balloon below the floor , keep the 2M granularity . */ <nl> - if ( val . freeram - num_pages < floor ) { <nl> + if ( val . freeram < num_pages || val . freeram - num_pages < floor ) { <nl> num_pages = val . freeram > floor ? ( val . freeram - floor ) : 0 ; <nl> num_pages -= num_pages % PAGES_IN_2M ; <nl> }
int snd_hda_input_jack_add ( struct hda_codec * codec , hda_nid_t nid , int type , <nl> err = snd_jack_new ( codec -> bus -> card , name , type , & jack -> jack ); <nl> if ( err < 0 ) <nl> return err ; <nl> + jack -> type = type ; <nl> jack -> jack -> private_data = jack ; <nl> jack -> jack -> private_free = hda_free_jack_priv ; <nl> return 0 ;
static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
static int igb_set_eee ( struct net_device * netdev , <nl> ( hw -> phy . media_type != e1000_media_type_copper )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset (& eee_curr , 0 , sizeof ( struct ethtool_eee )); <nl> + <nl> ret_val = igb_get_eee ( netdev , & eee_curr ); <nl> if ( ret_val ) <nl> return ret_val ;
cputime_to_timeval ( const cputime_t cputime , struct timeval * value ) <nl> value -> tv_usec = rp . subreg . even / 4096 ; <nl> value -> tv_sec = rp . subreg . odd ; <nl> # else <nl> - value -> tv_usec = cputime % 4096000000ULL ; <nl> + value -> tv_usec = ( cputime % 4096000000ULL ) / 4096 ; <nl> value -> tv_sec = cputime / 4096000000ULL ; <nl> # endif <nl> }
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> /* Device config is special , because it must work on <nl> * an unconfigured device . */ <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> + if ( minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> + /* Device config not appropriate on non - board minors . */ <nl> + rc = - ENOTTY ; <nl> + goto done ; <nl> + } <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> if ( rc == 0 )
static irqreturn_t et131x_isr ( int irq , void * dev_id ) <nl> { <nl> bool handled = true ; <nl> bool enable_interrupts = true ; <nl> - struct net_device * netdev = ( struct net_device *) dev_id ; <nl> + struct net_device * netdev = dev_id ; <nl> struct et131x_adapter * adapter = netdev_priv ( netdev ); <nl> struct address_map __iomem * iomem = adapter -> regs ; <nl> struct rx_ring * rx_ring = & adapter -> rx_ring ;
spider_net_stop ( struct net_device * netdev ) <nl> /* release chains */ <nl> spider_net_release_tx_chain ( card , 1 ); <nl>  <nl> + spider_net_free_rx_chain_contents ( card ); <nl> + <nl> spider_net_free_chain ( card , & card -> tx_chain ); <nl> spider_net_free_chain ( card , & card -> rx_chain ); <nl> 
static bool hist_browser__toggle_fold ( struct hist_browser * browser ) <nl> struct callchain_list * cl = container_of ( ms , struct callchain_list , ms ); <nl> bool has_children ; <nl>  <nl> + if (! he || ! ms ) <nl> + return false ; <nl> + <nl> if ( ms == & he -> ms ) <nl> has_children = hist_entry__toggle_fold ( he ); <nl> else
xfs_rename ( <nl> if ( unlikely (( target_dp -> i_d . di_flags & XFS_DIFLAG_PROJINHERIT ) && <nl> ( target_dp -> i_d . di_projid != src_ip -> i_d . di_projid ))) { <nl> error = XFS_ERROR ( EXDEV ); <nl> - xfs_rename_unlock4 ( inodes , XFS_ILOCK_SHARED ); <nl> + xfs_rename_unlock4 ( inodes , XFS_ILOCK_EXCL ); <nl> xfs_trans_cancel ( tp , cancel_flags ); <nl> goto std_return ; <nl> }
int radeonfb_create ( struct drm_device * dev , <nl> goto out_unref ; <nl> } <nl>  <nl> + rdev -> fbdev_info = info ; <nl> rfbdev = info -> par ; <nl> rfbdev -> helper . funcs = & radeon_fb_helper_funcs ; <nl> rfbdev -> helper . dev = dev ;
static int dwc3_ep0_set_config ( struct dwc3 * dwc , struct usb_ctrlrequest * ctrl ) <nl> case DWC3_ADDRESS_STATE : <nl> ret = dwc3_ep0_delegate_req ( dwc , ctrl ); <nl> /* if the cfg matches and the cfg is non zero */ <nl> - if (! ret && cfg ) <nl> + if ( cfg && (! ret || ( ret == USB_GADGET_DELAYED_STATUS ))) <nl> dwc -> dev_state = DWC3_CONFIGURED_STATE ; <nl> break ; <nl> 
static int __devinit toshiba_acpi_setup_backlight ( struct toshiba_acpi_dev * dev ) <nl> ret = get_tr_backlight_status ( dev , & enabled ); <nl> dev -> tr_backlight_supported = ! ret ; <nl>  <nl> + memset (& props , 0 , sizeof ( props )); <nl> props . type = BACKLIGHT_PLATFORM ; <nl> props . max_brightness = HCI_LCD_BRIGHTNESS_LEVELS - 1 ; <nl> - memset (& props , 0 , sizeof ( props )); <nl>  <nl> /* adding an extra level and having 0 change to transflective mode */ <nl> if ( dev -> tr_backlight_supported )
int i2c_dw_probe ( struct dw_i2c_dev * dev ) <nl>  <nl> snprintf ( adap -> name , sizeof ( adap -> name ), <nl> " Synopsys DesignWare I2C adapter "); <nl> + adap -> retries = 3 ; <nl> adap -> algo = & i2c_dw_algo ; <nl> adap -> dev . parent = dev -> dev ; <nl> i2c_set_adapdata ( adap , dev );
static int ieee80211_prep_connection ( struct ieee80211_sub_if_data * sdata , <nl> chanctx_conf = rcu_dereference ( sdata -> vif . chanctx_conf ); <nl> if ( WARN_ON (! chanctx_conf )) { <nl> rcu_read_unlock (); <nl> + sta_info_free ( local , new_sta ); <nl> return - EINVAL ; <nl> } <nl> rate_flags = ieee80211_chandef_rate_flags (& chanctx_conf -> def );
static void __init of_omap2_apll_setup ( struct device_node * node ) <nl> const char * parent_name ; <nl> u32 val ; <nl>  <nl> - ad = kzalloc ( sizeof (* clk_hw ), GFP_KERNEL ); <nl> + ad = kzalloc ( sizeof (* ad ), GFP_KERNEL ); <nl> clk_hw = kzalloc ( sizeof (* clk_hw ), GFP_KERNEL ); <nl> init = kzalloc ( sizeof (* init ), GFP_KERNEL ); <nl> 
event_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_preds ( call ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl>  <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_subsystem_preds ( system ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl> 
void dump_trace ( struct task_struct * task , <nl> unsigned used = 0 ; <nl> struct thread_info * tinfo ; <nl> int graph = 0 ; <nl> + unsigned long dummy ; <nl> unsigned long bp ; <nl>  <nl> if (! task ) <nl> task = current ; <nl>  <nl> if (! stack ) { <nl> - unsigned long dummy ; <nl> stack = & dummy ; <nl> if ( task && task != current ) <nl> stack = ( unsigned long *) task -> thread . sp ;
static int __devinit cobalt_raq_led_probe ( struct platform_device * pdev ) <nl> if (! res ) <nl> return - EBUSY ; <nl>  <nl> - led_port = ioremap ( res -> start , res -> end - res -> start + 1 ); <nl> + led_port = ioremap ( res -> start , resource_size ( res )); <nl> if (! led_port ) <nl> return - ENOMEM ; <nl> 
batadv_purge_outstanding_packets ( struct batadv_priv * bat_priv , <nl> * we delete only packets belonging to the given interface <nl> */ <nl> if (( hard_iface ) && <nl> - ( forw_packet -> if_incoming != hard_iface )) <nl> + ( forw_packet -> if_incoming != hard_iface ) && <nl> + ( forw_packet -> if_outgoing != hard_iface )) <nl> continue ; <nl>  <nl> spin_unlock_bh (& bat_priv -> forw_bcast_list_lock );
static int dasd_eer_open ( struct inode * inp , struct file * filp ) <nl> unsigned long flags ; <nl>  <nl> eerb = kzalloc ( sizeof ( struct eerbuffer ), GFP_KERNEL ); <nl> + if (! eerb ) <nl> + return - ENOMEM ; <nl> eerb -> buffer_page_count = eer_pages ; <nl> if ( eerb -> buffer_page_count < 1 || <nl> eerb -> buffer_page_count > INT_MAX / PAGE_SIZE ) {
nouveau_bo_move_m2mf ( struct ttm_buffer_object * bo , int evict , bool intr , <nl> bool no_wait_gpu , struct ttm_mem_reg * new_mem ) <nl> { <nl> struct nouveau_drm * drm = nouveau_bdev ( bo -> bdev ); <nl> - struct nouveau_channel * chan = chan = drm -> ttm . chan ; <nl> + struct nouveau_channel * chan = drm -> ttm . chan ; <nl> struct nouveau_bo * nvbo = nouveau_bo ( bo ); <nl> struct ttm_mem_reg * old_mem = & bo -> mem ; <nl> int ret ;
static int vq_memory_access_ok ( void __user * log_base , struct vhost_memory * mem , <nl> int log_all ) <nl> { <nl> int i ; <nl> + <nl> + if (! mem ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < mem -> nregions ; ++ i ) { <nl> struct vhost_memory_region * m = mem -> regions + i ; <nl> unsigned long a = m -> userspace_addr ;
PyMODINIT_FUNC initperf ( void ) <nl> pyrf_cpu_map__setup_types () < 0 ) <nl> return ; <nl>  <nl> + page_size = sysconf ( _SC_PAGE_SIZE ); <nl> + <nl> Py_INCREF (& pyrf_evlist__type ); <nl> PyModule_AddObject ( module , " evlist ", ( PyObject *)& pyrf_evlist__type ); <nl> 
static void intel_dp_prepare ( struct drm_encoder * encoder ) <nl> uint32_t dp_reg = I915_READ ( intel_dp -> output_reg ); <nl>  <nl> if ( IS_eDP ( intel_dp ) || IS_PCH_eDP ( intel_dp )) { <nl> + ironlake_edp_panel_off ( dev ); <nl> ironlake_edp_backlight_off ( dev ); <nl> ironlake_edp_panel_vdd_on ( dev ); <nl> ironlake_edp_pll_on ( encoder );
static int perl_start_script ( const char * script , int argc , const char ** argv ) <nl> goto error ; <nl> } <nl>  <nl> - perl_run ( my_perl ); <nl> + if ( perl_run ( my_perl )) { <nl> + err = - 1 ; <nl> + goto error ; <nl> + } <nl> + <nl> if ( SvTRUE ( ERRSV )) { <nl> err = - 1 ; <nl> goto error ;
done : <nl> atomic_dec (& urb -> use_count ); <nl> if ( urb -> reject ) <nl> wake_up (& usb_kill_urb_queue ); <nl> - usb_put_urb ( urb ); <nl> usbmon_urb_submit_error (& hcd -> self , urb , status ); <nl> + usb_put_urb ( urb ); <nl> } <nl> return status ; <nl> }
static bool ixgbe_clean_tx_irq ( struct ixgbe_q_vector * q_vector , <nl> total_packets += tx_buffer -> gso_segs ; <nl>  <nl> /* free the skb */ <nl> - dev_kfree_skb_any ( tx_buffer -> skb ); <nl> + dev_consume_skb_any ( tx_buffer -> skb ); <nl>  <nl> /* unmap skb header data */ <nl> dma_unmap_single ( tx_ring -> dev ,
static void mousevsc_on_channel_callback ( void * context ) <nl> static int mousevsc_connect_to_vsp ( struct hv_device * device ) <nl> { <nl> int ret = 0 ; <nl> - int t ; <nl> + unsigned long t ; <nl> struct mousevsc_dev * input_dev = hv_get_drvdata ( device ); <nl> struct mousevsc_prt_msg * request ; <nl> struct mousevsc_prt_msg * response ;
store_dh_state ( struct device * dev , struct device_attribute * attr , <nl> struct scsi_device_handler * scsi_dh ; <nl> int err = - EINVAL ; <nl>  <nl> + if ( sdev -> sdev_state == SDEV_CANCEL || <nl> + sdev -> sdev_state == SDEV_DEL ) <nl> + return - ENODEV ; <nl> + <nl> if (! sdev -> scsi_dh_data ) { <nl> /* <nl> * Attach to a device handler
int tf_tonga_thermal_setup_fan_table ( struct pp_hwmgr * hwmgr , void * input , void * <nl> int res ; <nl> uint64_t tmp64 ; <nl>  <nl> + if (! phm_cap_enabled ( hwmgr -> platform_descriptor . platformCaps , PHM_PlatformCaps_MicrocodeFanControl )) <nl> + return 0 ; <nl> + <nl> if ( 0 == data -> fan_table_start ) { <nl> phm_cap_unset ( hwmgr -> platform_descriptor . platformCaps , PHM_PlatformCaps_MicrocodeFanControl ); <nl> return 0 ;
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
int usb_set_interface ( struct usb_device * dev , int interface , int alternate ) <nl> interface ); <nl> return - EINVAL ; <nl> } <nl> + if ( iface -> unregistering ) <nl> + return - ENODEV ; <nl>  <nl> alt = usb_altnum_to_altsetting ( iface , alternate ); <nl> if (! alt ) {
static const char * get_input_type ( struct hda_gnode * node , unsigned int * pinctl ) <nl> return " Front Aux "; <nl> return " Aux "; <nl> case AC_JACK_MIC_IN : <nl> - if ( node -> pin_caps & <nl> - ( AC_PINCAP_VREF_80 << AC_PINCAP_VREF_SHIFT )) <nl> + if ( pinctl && <nl> + ( node -> pin_caps & <nl> + ( AC_PINCAP_VREF_80 << AC_PINCAP_VREF_SHIFT ))) <nl> * pinctl |= AC_PINCTL_VREF_80 ; <nl> if (( location & 0x0f ) == AC_JACK_LOC_FRONT ) <nl> return " Front Mic ";
static int ibmvfc_map_sg_data ( struct scsi_cmnd * scmd , <nl> & evt -> ext_list_token ); <nl>  <nl> if (! evt -> ext_list ) { <nl> - scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> + scsi_dma_unmap ( scmd ); <nl> + if ( vhost -> log_level > IBMVFC_DEFAULT_LOG_LEVEL ) <nl> + scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> return - ENOMEM ; <nl> } <nl> }
mptctl_getiocinfo ( unsigned long arg , unsigned int data_size ) <nl> else <nl> karg -> adapterType = MPT_IOCTL_INTERFACE_SCSI ; <nl>  <nl> - if ( karg -> hdr . port > 1 ) <nl> + if ( karg -> hdr . port > 1 ) { <nl> + kfree ( karg ); <nl> return - EINVAL ; <nl> + } <nl> port = karg -> hdr . port ; <nl>  <nl> karg -> port = port ;
static int perf_exclude_event ( struct perf_event * event , <nl> struct pt_regs * regs ) <nl> { <nl> if ( event -> hw . state & PERF_HES_STOPPED ) <nl> - return 0 ; <nl> + return 1 ; <nl>  <nl> if ( regs ) { <nl> if ( event -> attr . exclude_user && user_mode ( regs ))
hw_perf_counter_init ( struct perf_counter * counter ) <nl> return NULL ; <nl> } <nl> events [ n ] = ev ; <nl> + ctrs [ n ] = counter ; <nl> if ( check_excludes ( ctrs , n , 1 )) <nl> return NULL ; <nl> if ( power_check_constraints ( events , n + 1 ))
static int scan ( struct wiphy * wiphy , struct cfg80211_scan_request * request ) <nl> kmalloc_array ( request -> n_ssids , <nl> sizeof ( struct hidden_network ), <nl> GFP_KERNEL ); <nl> + if (! strHiddenNetwork . net_info ) <nl> + return - ENOMEM ; <nl> strHiddenNetwork . n_ssids = request -> n_ssids ; <nl>  <nl> 
fail_put : <nl> ip -> i_gl -> gl_object = NULL ; <nl> gfs2_glock_put ( ip -> i_gl ); <nl> fail : <nl> - iput ( inode ); <nl> + iget_failed ( inode ); <nl> return ERR_PTR ( error ); <nl> } <nl> 
static int tda10071_get_frontend ( struct dvb_frontend * fe ) <nl> if ( ret ) <nl> goto error ; <nl>  <nl> - c -> symbol_rate = ( buf [ 0 ] << 16 ) | ( buf [ 1 ] << 8 ) | ( buf [ 2 ] << 0 ); <nl> + c -> symbol_rate = (( buf [ 0 ] << 16 ) | ( buf [ 1 ] << 8 ) | ( buf [ 2 ] << 0 )) * 1000 ; <nl>  <nl> return ret ; <nl> error :
do { \ <nl> if ( EXT4_FITS_IN_INODE ( raw_inode , einode , xtime )) \ <nl> ( einode )-> xtime . tv_sec = \ <nl> ( signed ) le32_to_cpu (( raw_inode )-> xtime ); \ <nl> + else \ <nl> + ( einode )-> xtime . tv_sec = 0 ; \ <nl> if ( EXT4_FITS_IN_INODE ( raw_inode , einode , xtime ## _extra )) \ <nl> ext4_decode_extra_time (&( einode )-> xtime , \ <nl> raw_inode -> xtime ## _extra ); \
static int snd_pcm_update_hw_ptr0 ( struct snd_pcm_substream * substream , <nl> new_hw_ptr = hw_base + pos ; <nl> } <nl> __delta : <nl> - delta = ( new_hw_ptr - old_hw_ptr ) % runtime -> boundary ; <nl> + delta = new_hw_ptr - old_hw_ptr ; <nl> + if ( delta < 0 ) <nl> + delta += runtime -> boundary ; <nl> if ( xrun_debug ( substream , in_interrupt ? <nl> XRUN_DEBUG_PERIODUPDATE : XRUN_DEBUG_HWPTRUPDATE )) { <nl> char name [ 16 ];
static int parse_opts ( char * params , struct p9_fd_opts * opts ) <nl> opts -> port = P9_PORT ; <nl> opts -> rfd = ~ 0 ; <nl> opts -> wfd = ~ 0 ; <nl> + opts -> privport = 0 ; <nl>  <nl> if (! params ) <nl> return 0 ;
mt7601u_extra_power_over_mac ( struct mt7601u_dev * dev ) <nl> static void <nl> mt7601u_set_power_rate ( struct power_per_rate * rate , s8 delta , u8 value ) <nl> { <nl> + /* Invalid ? Note : vendor driver does not handle this */ <nl> + if ( value == 0xff ) <nl> + return ; <nl> + <nl> rate -> raw = s6_validate ( value ); <nl> rate -> bw20 = s6_to_int ( value ); <nl> /* Note : vendor driver does cap the value to s6 right away */
void print2byte ( int input , struct iio_channel_info * info ) <nl> * Shift before conversion to avoid sign extension <nl> * of left aligned data <nl> */ <nl> - input = input >> info -> shift ; <nl> + input >>= info -> shift ; <nl> if ( info -> is_signed ) { <nl> int16_t val = input ; <nl> 
_xfs_buf_find ( <nl> * have to check that the buffer falls within the filesystem bounds . <nl> */ <nl> eofs = XFS_FSB_TO_BB ( btp -> bt_mount , btp -> bt_mount -> m_sb . sb_dblocks ); <nl> - if ( blkno >= eofs ) { <nl> + if ( blkno < 0 || blkno >= eofs ) { <nl> /* <nl> * XXX ( dgc ): we should really be returning - EFSCORRUPTED here , <nl> * but none of the higher level infrastructure supports
static void __cpuinit early_init_intel ( struct cpuinfo_x86 * c ) <nl> { <nl> /* Unmask CPUID levels if masked : */ <nl> - if ( c -> x86 == 6 && c -> x86_model >= 15 ) { <nl> + if ( c -> x86 > 6 || ( c -> x86 == 6 && c -> x86_model >= 0xd )) { <nl> u64 misc_enable ; <nl>  <nl> rdmsrl ( MSR_IA32_MISC_ENABLE , misc_enable );
static int __init omapdss_hdmihw_probe ( struct platform_device * pdev ) <nl> hdmi . ip_data . pll_offset = HDMI_PLLCTRL ; <nl> hdmi . ip_data . phy_offset = HDMI_PHY ; <nl>  <nl> + hdmi_init_output ( pdev ); <nl> + <nl> r = hdmi_panel_init (); <nl> if ( r ) { <nl> DSSERR (" can ' t init panel \ n "); <nl> static int __init omapdss_hdmihw_probe ( struct platform_device * pdev ) <nl>  <nl> dss_debugfs_create_file (" hdmi ", hdmi_dump_regs ); <nl>  <nl> - hdmi_init_output ( pdev ); <nl> - <nl> hdmi_probe_pdata ( pdev ); <nl>  <nl> return 0 ;
int ip6_del_rt ( struct rt6_info * rt , struct nlmsghdr * nlh , void * _rtattr , struct <nl> int err ; <nl> struct fib6_table * table ; <nl>  <nl> + if ( rt == & ip6_null_entry ) <nl> + return - ENOENT ; <nl> + <nl> table = rt -> rt6i_table ; <nl> write_lock_bh (& table -> tb6_lock ); <nl> 
static int ath9k_htc_aggr_oper ( struct ath9k_htc_priv * priv , <nl> int ret = 0 ; <nl> u8 cmd_rsp ; <nl>  <nl> - if ( tid > ATH9K_HTC_MAX_TID ) <nl> + if ( tid >= ATH9K_HTC_MAX_TID ) <nl> return - EINVAL ; <nl>  <nl> memset (& aggr , 0 , sizeof ( struct ath9k_htc_target_aggr ));
static int tg3_run_loopback ( struct tg3 * tp , int loopback_mode ) <nl> } <nl> mac_mode = ( tp -> mac_mode & ~ MAC_MODE_PORT_MODE_MASK ) | <nl> MAC_MODE_LINK_POLARITY | MAC_MODE_PORT_MODE_GMII ; <nl> - if (( tp -> phy_id & PHY_ID_MASK ) == PHY_ID_BCM5401 ) <nl> + if (( tp -> phy_id & PHY_ID_MASK ) == PHY_ID_BCM5401 ) { <nl> mac_mode &= ~ MAC_MODE_LINK_POLARITY ; <nl> + tg3_writephy ( tp , MII_TG3_EXT_CTRL , <nl> + MII_TG3_EXT_CTRL_LNK3_LED_MODE ); <nl> + } <nl> tw32 ( MAC_MODE , mac_mode ); <nl> } <nl> else
struct btrfs_root * btrfs_create_tree ( struct btrfs_trans_handle * trans , <nl> fail : <nl> if ( leaf ) { <nl> btrfs_tree_unlock ( leaf ); <nl> + free_extent_buffer ( root -> commit_root ); <nl> free_extent_buffer ( leaf ); <nl> } <nl> kfree ( root );
static void resample_shrink ( struct snd_pcm_plugin * plugin , <nl> while ( dst_frames1 > 0 ) { <nl> S1 = S2 ; <nl> if ( src_frames1 -- > 0 ) { <nl> - S1 = * src ; <nl> + S2 = * src ; <nl> src += src_step ; <nl> } <nl> if ( pos & ~ R_MASK ) {
int intel_framebuffer_init ( struct drm_device * dev , <nl> switch ( mode_cmd -> bpp ) { <nl> case 8 : <nl> case 16 : <nl> + /* Only pre - ILK can handle 5 : 5 : 5 */ <nl> + if ( mode_cmd -> depth == 15 && ! HAS_PCH_SPLIT ( dev )) <nl> + return - EINVAL ; <nl> + break ; <nl> + <nl> case 24 : <nl> case 32 : <nl> break ;
static int filter_ack ( struct ieee80211_hw * hw , struct ieee80211_hdr * rx_hdr , <nl> struct ieee80211_hdr * tx_hdr ; <nl>  <nl> tx_hdr = ( struct ieee80211_hdr *) skb -> data ; <nl> - if ( likely (! compare_ether_addr ( tx_hdr -> addr2 , rx_hdr -> addr1 ))) <nl> + if ( likely (! memcmp ( tx_hdr -> addr2 , rx_hdr -> addr1 , ETH_ALEN ))) <nl> { <nl> __skb_unlink ( skb , q ); <nl> tx_status ( hw , skb , IEEE80211_TX_STAT_ACK , stats -> signal , 1 );
__append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
static void mlx4_enable_msi_x ( struct mlx4_dev * dev ) <nl> nreq = err ; <nl> goto retry ; <nl> } <nl> - <nl> + kfree ( entries ); <nl> goto no_msi ; <nl> } <nl> 
static int __init amijoy_init ( void ) <nl> int i , j ; <nl> int err ; <nl>  <nl> + if (! MACH_IS_AMIGA ) <nl> + return - ENODEV ; <nl> + <nl> for ( i = 0 ; i < 2 ; i ++) { <nl> if (! amijoy [ i ]) <nl> continue ;
struct clk * imx_clk_fixup_mux ( const char * name , void __iomem * reg , <nl> init . ops = & clk_fixup_mux_ops ; <nl> init . parent_names = parents ; <nl> init . num_parents = num_parents ; <nl> + init . flags = 0 ; <nl>  <nl> fixup_mux -> mux . reg = reg ; <nl> fixup_mux -> mux . shift = shift ;
void nfs_inode_return_delegation_noreclaim ( struct inode * inode ) <nl>  <nl> delegation = nfs_inode_detach_delegation ( inode ); <nl> if ( delegation != NULL ) <nl> - nfs_do_return_delegation ( inode , delegation , 0 ); <nl> + nfs_do_return_delegation ( inode , delegation , 1 ); <nl> } <nl>  <nl> /**
static void ati_remote_input_report ( struct urb * urb , struct pt_regs * regs ) <nl> input_regs ( dev , regs ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 1 ); <nl> + input_sync ( dev ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 0 ); <nl> input_sync ( dev );
static int vb2_internal_streamon ( struct vb2_queue * q , enum v4l2_buf_type type ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if (! q -> num_buffers ) { <nl> + dprintk ( 1 , " streamon : no buffers have been allocated \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* <nl> * If any buffers were queued before streamon , <nl> * we can now pass them to driver for processing .
struct drm_gem_object * msm_gem_new ( struct drm_device * dev , <nl>  <nl> fail : <nl> if ( obj ) <nl> - drm_gem_object_unreference_unlocked ( obj ); <nl> + drm_gem_object_unreference ( obj ); <nl>  <nl> return ERR_PTR ( ret ); <nl> }
static struct drm_driver driver = { <nl> . minor = DRIVER_MINOR , <nl> . patchlevel = DRIVER_PATCHLEVEL , <nl>  <nl> - . gem_free_object = mgag200_gem_free_object , <nl> + . gem_free_object_unlocked = mgag200_gem_free_object , <nl> . dumb_create = mgag200_dumb_create , <nl> . dumb_map_offset = mgag200_dumb_mmap_offset , <nl> . dumb_destroy = drm_gem_dumb_destroy ,
static int snd_pcm_oss_open_file ( struct file * file , <nl> for ( idx = 0 ; idx < 2 ; idx ++) { <nl> if ( setup [ idx ]. disable ) <nl> continue ; <nl> + if (! pcm -> streams [ idx ]. substream_count ) <nl> + continue ; /* no matching substream */ <nl> if ( idx == SNDRV_PCM_STREAM_PLAYBACK ) { <nl> if (! ( f_mode & FMODE_WRITE )) <nl> continue ;
out_device_destroy : <nl> scsi_device_set_state ( sdev , SDEV_DEL ); <nl> transport_destroy_device (& sdev -> sdev_gendev ); <nl> put_device (& sdev -> sdev_dev ); <nl> + scsi_free_queue ( sdev -> request_queue ); <nl> put_device (& sdev -> sdev_gendev ); <nl> out : <nl> if ( display_failure_msg )
drm_do_get_edid ( struct drm_connector * connector , struct i2c_adapter * adapter ) <nl> break ; <nl> } <nl> } <nl> - if ( i == 4 ) <nl> + <nl> + if ( i == 4 && print_bad_edid ) { <nl> dev_warn ( connector -> dev -> dev , <nl> "% s : Ignoring invalid EDID block % d .\ n ", <nl> drm_get_connector_name ( connector ), j ); <nl> + <nl> + connector -> bad_edid_counter ++; <nl> + } <nl> } <nl>  <nl> if ( valid_extensions != block [ 0x7e ]) {
static int __cpufreq_set_policy ( struct cpufreq_policy * data , <nl> memcpy (& policy -> cpuinfo , & data -> cpuinfo , <nl> sizeof ( struct cpufreq_cpuinfo )); <nl>  <nl> - if ( policy -> min > data -> min && policy -> min > policy -> max ) { <nl> + if ( policy -> min > data -> max || policy -> max < data -> min ) { <nl> ret = - EINVAL ; <nl> goto error_out ; <nl> }
enum acer_wmi_event_ids { <nl> static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x01 , { KEY_WLAN } }, /* WiFi */ <nl> { KE_KEY , 0x03 , { KEY_WLAN } }, /* WiFi */ <nl> + { KE_KEY , 0x04 , { KEY_WLAN } }, /* WiFi */ <nl> { KE_KEY , 0x12 , { KEY_BLUETOOTH } }, /* BT */ <nl> { KE_KEY , 0x21 , { KEY_PROG1 } }, /* Backup */ <nl> { KE_KEY , 0x22 , { KEY_PROG2 } }, /* Arcade */
# include < linux / list . h > <nl> # include < linux / slab . h > <nl> # include < linux / export . h > <nl> +# include < linux / vmalloc . h > <nl> # include < net / net_namespace . h > <nl> # include < net / ip . h > <nl> # include < net / protocol . h >
static int lbs_spi_thread ( void * data ) <nl> up (& card -> spi_thread_terminated ); <nl> do_exit ( 0 ); <nl> } <nl> - } while ( err == EINTR ); <nl> + } while ( err == - EINTR ); <nl>  <nl> /* Read the host interrupt status register to see what we <nl> * can do . */
static u32 asle_set_backlight ( struct drm_device * dev , u32 bclp ) <nl> return ASLE_BACKLIGHT_FAILED ; <nl>  <nl> intel_panel_set_backlight ( dev , bclp , 255 ); <nl> - iowrite32 (( bclp * 0x64 )/ 0xff | ASLE_CBLV_VALID , & asle -> cblv ); <nl> + iowrite32 ( DIV_ROUND_UP ( bclp * 100 , 255 ) | ASLE_CBLV_VALID , & asle -> cblv ); <nl>  <nl> return 0 ; <nl> }
static void rt2800_config_channel_rf55xx ( struct rt2x00_dev * rt2x00dev , <nl> rt2800_rfcsr_write ( rt2x00dev , 49 , rfcsr ); <nl>  <nl> rt2800_rfcsr_read ( rt2x00dev , 50 , & rfcsr ); <nl> - if ( info -> default_power1 > power_bound ) <nl> + if ( info -> default_power2 > power_bound ) <nl> rt2x00_set_field8 (& rfcsr , RFCSR50_TX , power_bound ); <nl> else <nl> rt2x00_set_field8 (& rfcsr , RFCSR50_TX , info -> default_power2 );
static inline int rsi_create_kthread ( struct rsi_common * common , <nl> u8 * name ) <nl> { <nl> init_completion (& thread -> completion ); <nl> - thread -> task = kthread_run ( func_ptr , common , name ); <nl> + thread -> task = kthread_run ( func_ptr , common , "% s ", name ); <nl> if ( IS_ERR ( thread -> task )) <nl> return ( int ) PTR_ERR ( thread -> task ); <nl> 
static int pn533_target_found ( struct pn533 * dev , <nl> if ( resp -> tg != 1 ) <nl> return - EPROTO ; <nl>  <nl> + memset (& nfc_tgt , 0 , sizeof ( struct nfc_target )); <nl> + <nl> target_data_len = resp_len - sizeof ( struct pn533_poll_response ); <nl>  <nl> switch ( dev -> poll_mod_curr ) {
done : <nl> init_timer (& dpriv -> timer ); <nl> dpriv -> timer . expires = jiffies + 10 * HZ ; <nl> dpriv -> timer . data = ( unsigned long ) dev ; <nl> - dpriv -> timer . function = & dscc4_timer ; <nl> + dpriv -> timer . function = dscc4_timer ; <nl> add_timer (& dpriv -> timer ); <nl> netif_carrier_on ( dev ); <nl> 
static void usbhsh_pipe_detach ( struct usbhsh_hpriv * hpriv , <nl> struct device * dev = usbhs_priv_to_dev ( priv ); <nl> unsigned long flags ; <nl>  <nl> + if ( unlikely (! uep )) { <nl> + dev_err ( dev , " no uep \ n "); <nl> + return ; <nl> + } <nl> + <nl> /******************** spin lock ********************/ <nl> usbhs_lock ( priv , flags ); <nl> 
struct greybus_host_device * greybus_create_hd ( struct greybus_host_driver * driver <nl>  <nl> if ( buffer_size_max < GB_OPERATION_MESSAGE_SIZE_MIN ) { <nl> dev_err ( parent , " greybus host - device buffers too small \ n "); <nl> - return NULL ; <nl> + return ERR_PTR (- EINVAL ); <nl> } <nl>  <nl> if ( num_cports == 0 || num_cports > CPORT_ID_MAX ) {
static int start_afu ( struct cxlflash_cfg * cfg ) <nl>  <nl> init_pcr ( cfg ); <nl>  <nl> + /* After an AFU reset , RRQ entries are stale , clear them */ <nl> + memset (& afu -> rrq_entry , 0 , sizeof ( afu -> rrq_entry )); <nl> + <nl> /* Initialize RRQ pointers */ <nl> afu -> hrrq_start = & afu -> rrq_entry [ 0 ]; <nl> afu -> hrrq_end = & afu -> rrq_entry [ NUM_RRQ_ENTRY - 1 ];
int ath9k_hw_init ( struct ath_hw * ah ) <nl> struct ath_common * common = ath9k_hw_common ( ah ); <nl> int r = 0 ; <nl>  <nl> - if (! ath9k_hw_devid_supported ( ah -> hw_version . devid )) <nl> + if (! ath9k_hw_devid_supported ( ah -> hw_version . devid )) { <nl> + ath_print ( common , ATH_DBG_FATAL , <nl> + " Unsupported device ID : 0x % 0x \ n ", <nl> + ah -> hw_version . devid ); <nl> return - EOPNOTSUPP ; <nl> + } <nl>  <nl> ath9k_hw_init_defaults ( ah ); <nl> ath9k_hw_init_config ( ah );
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> * anyway ( it holds no special properties of the bond device ), <nl> * so we can change it without calling change_active_interface () <nl> */ <nl> - if (! bond -> curr_active_slave ) <nl> + if (! bond -> curr_active_slave && new_slave -> link == BOND_LINK_UP ) <nl> bond -> curr_active_slave = new_slave ; <nl>  <nl> break ;
int drbd_adm_resize ( struct sk_buff * skb , struct genl_info * info ) <nl> mutex_unlock (& device -> resource -> conf_update ); <nl> synchronize_rcu (); <nl> kfree ( old_disk_conf ); <nl> + new_disk_conf = NULL ; <nl> } <nl>  <nl> ddsf = ( rs . resize_force ? DDSF_FORCED : 0 ) | ( rs . no_resync ? DDSF_NO_RESYNC : 0 ); <nl> int drbd_adm_resize ( struct sk_buff * skb , struct genl_info * info ) <nl>  <nl> fail_ldev : <nl> put_ldev ( device ); <nl> + kfree ( new_disk_conf ); <nl> goto fail ; <nl> } <nl> 
void tipc_link_delete ( struct link * l_ptr ) <nl>  <nl> static void link_start ( struct link * l_ptr ) <nl> { <nl> + tipc_node_lock ( l_ptr -> owner ); <nl> link_state_event ( l_ptr , STARTING_EVT ); <nl> + tipc_node_unlock ( l_ptr -> owner ); <nl> } <nl>  <nl> /**
static int saa7134_try_get_set_fmt_vbi_cap ( struct file * file , void * priv , <nl> struct saa7134_dev * dev = fh -> dev ; <nl> struct saa7134_tvnorm * norm = dev -> tvnorm ; <nl>  <nl> + memset (& f -> fmt . vbi . reserved , 0 , sizeof ( f -> fmt . vbi . reserved )); <nl> f -> fmt . vbi . sampling_rate = 6750000 * 4 ; <nl> f -> fmt . vbi . samples_per_line = 2048 /* VBI_LINE_LENGTH */; <nl> f -> fmt . vbi . sample_format = V4L2_PIX_FMT_GREY ;
int kernfs_rename_ns ( struct kernfs_node * kn , struct kernfs_node * new_parent , <nl> if (! new_name ) <nl> goto out ; <nl>  <nl> - kfree ( kn -> name ); <nl> + if ( kn -> flags & KERNFS_STATIC_NAME ) <nl> + kn -> flags &= ~ KERNFS_STATIC_NAME ; <nl> + else <nl> + kfree ( kn -> name ); <nl> + <nl> kn -> name = new_name ; <nl> } <nl> 
static int mt8173_nor_write_single_byte ( struct mt8173_nor * mt8173_nor , <nl> mt8173_nor_set_addr ( mt8173_nor , addr ); <nl>  <nl> for ( i = 0 ; i < length ; i ++) { <nl> + writeb (* data ++, mt8173_nor -> base + MTK_NOR_WDATA_REG ); <nl> ret = mt8173_nor_execute_cmd ( mt8173_nor , MTK_NOR_PIO_WR_CMD ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> - writeb (* data ++, mt8173_nor -> base + MTK_NOR_WDATA_REG ); <nl> } <nl> return 0 ; <nl> }
static int alloc_spa ( struct cxl_afu * afu ) <nl>  <nl> static void release_spa ( struct cxl_afu * afu ) <nl> { <nl> + cxl_p1n_write ( afu , CXL_PSL_SPAP_An , 0 ); <nl> free_pages (( unsigned long ) afu -> spa , afu -> spa_order ); <nl> } <nl> 
static int __devinit xencons_probe ( struct xenbus_device * dev , <nl> if ( devid == 0 ) <nl> return - ENODEV ; <nl>  <nl> - info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL | __GFP_ZERO ); <nl> + info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL ); <nl> if (! info ) <nl> - goto error_nomem ; <nl> + return - ENOMEM ; <nl> dev_set_drvdata (& dev -> dev , info ); <nl> info -> xbdev = dev ; <nl> info -> vtermno = xenbus_devid_to_vtermno ( devid );
static void mmci_post_request ( struct mmc_host * mmc , struct mmc_request * mrq , <nl> chan = host -> dma_tx_channel ; <nl> dmaengine_terminate_all ( chan ); <nl>  <nl> + if ( host -> dma_desc_current == next -> dma_desc ) <nl> + host -> dma_desc_current = NULL ; <nl> + <nl> + if ( host -> dma_current == next -> dma_chan ) <nl> + host -> dma_current = NULL ; <nl> + <nl> next -> dma_desc = NULL ; <nl> next -> dma_chan = NULL ; <nl> + data -> host_cookie = 0 ; <nl> } <nl> } <nl> 
struct vm_area_struct * vb2_get_vma ( struct vm_area_struct * vma ) <nl>  <nl> return vma_copy ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( vb2_get_vma ); <nl>  <nl> /** <nl> * vb2_put_userptr () - release a userspace virtual memory area
static int dwc3_cleanup_done_reqs ( struct dwc3 * dwc , struct dwc3_ep * dep , <nl> for_each_sg ( sg , s , pending , i ) { <nl> trb = & dep -> trb_pool [ dep -> trb_dequeue ]; <nl>  <nl> + if ( trb -> ctrl & DWC3_TRB_CTRL_HWO ) <nl> + break ; <nl> + <nl> req -> sg = sg_next ( s ); <nl> req -> num_pending_sgs --; <nl> 
static struct resource * res_pci_find_mem ( u_long base , u_long num , <nl>  <nl> static int res_pci_init ( struct pcmcia_socket * s ) <nl> { <nl> - if (! s -> cb_dev || (! s -> features & SS_CAP_PAGE_REGS )) { <nl> + if (! s -> cb_dev || !( s -> features & SS_CAP_PAGE_REGS )) { <nl> dev_err (& s -> dev , " not supported by res_pci \ n "); <nl> return - EOPNOTSUPP ; <nl> }
static int pppoe_rcv_core ( struct sock * sk , struct sk_buff * skb ) <nl> * can ' t change . <nl> */ <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto abort_kfree ; <nl> + <nl> if ( sk -> sk_state & PPPOX_BOUND ) { <nl> ppp_input (& po -> chan , skb ); <nl> } else if ( sk -> sk_state & PPPOX_RELAY ) {
xfer_exit : <nl>  <nl> static void dma_pl330_rqcb ( struct dma_pl330_desc * desc , enum pl330_op_err err ) <nl> { <nl> - struct dma_pl330_chan * pch = desc -> pchan ; <nl> + struct dma_pl330_chan * pch ; <nl> unsigned long flags ; <nl>  <nl> + if (! desc ) <nl> + return ; <nl> + <nl> + pch = desc -> pchan ; <nl> + <nl> /* If desc aborted */ <nl> if (! pch ) <nl> return ;
static void __cpuinit init_amd ( struct cpuinfo_x86 * c ) <nl> level = cpuid_eax ( 1 ); <nl> if ( c -> x86 == 15 && (( level >= 0x0f48 && level < 0x0f50 ) || level >= 0x0f58 )) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl> - if ( c -> x86 == 0x10 ) <nl> + if ( c -> x86 == 0x10 || c -> x86 == 0x11 ) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl>  <nl> /* Enable workaround for FXSAVE leak */
perf_callchain_kernel ( struct perf_callchain_entry_ctx * entry , struct pt_regs * re <nl> return ; <nl> } <nl>  <nl> - perf_callchain_store ( entry , regs -> ip ); <nl> + if ( perf_callchain_store ( entry , regs -> ip )) <nl> + return ; <nl>  <nl> dump_trace ( NULL , regs , NULL , 0 , & backtrace_ops , entry ); <nl> }
data_sock_getname ( struct socket * sock , struct sockaddr * addr , <nl> lock_sock ( sk ); <nl>  <nl> * addr_len = sizeof (* maddr ); <nl> + maddr -> family = AF_ISDN ; <nl> maddr -> dev = _pms ( sk )-> dev -> id ; <nl> maddr -> channel = _pms ( sk )-> ch . nr ; <nl> maddr -> sapi = _pms ( sk )-> ch . addr & 0xff ;
static int rv3029c2_rtc_i2c_set_alarm ( struct i2c_client * client , <nl> dev_dbg (& client -> dev , " alarm IRQ armed \ n "); <nl> } else { <nl> /* disable AIE irq */ <nl> - ret = rv3029c2_rtc_i2c_alarm_set_irq ( client , 1 ); <nl> + ret = rv3029c2_rtc_i2c_alarm_set_irq ( client , 0 ); <nl> if ( ret ) <nl> return ret ; <nl> 
cfg80211_bss_update ( struct cfg80211_registered_device * dev , <nl> memcpy ( ies , res -> pub . information_elements , ielen ); <nl> found -> ies_allocated = true ; <nl> found -> pub . information_elements = ies ; <nl> + found -> pub . len_information_elements = ielen ; <nl> } <nl> } <nl> }
static void _rtl92s_phy_get_txpower_index ( struct ieee80211_hw * hw , u8 channel , <nl> /* Read HT 40 OFDM TX power */ <nl> ofdmpowerLevel [ 0 ] = rtlefuse -> txpwrlevel_ht40_2s [ 0 ][ index ]; <nl> ofdmpowerLevel [ 1 ] = rtlefuse -> txpwrlevel_ht40_2s [ 1 ][ index ]; <nl> + } else { <nl> + ofdmpowerLevel [ 0 ] = 0 ; <nl> + ofdmpowerLevel [ 1 ] = 0 ; <nl> } <nl> } <nl> 
int ieee80211_radiotap_iterator_init ( <nl> /* find payload start allowing for extended bitmap ( s ) */ <nl>  <nl> if ( iterator -> _bitmap_shifter & ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> + if (( unsigned long ) iterator -> _arg - <nl> + ( unsigned long ) iterator -> _rtheader + sizeof ( uint32_t ) > <nl> + ( unsigned long ) iterator -> _max_length ) <nl> + return - EINVAL ; <nl> while ( get_unaligned_le32 ( iterator -> _arg ) & <nl> ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> iterator -> _arg += sizeof ( uint32_t );
static int __init dm_bufio_init ( void ) <nl> { <nl> __u64 mem ; <nl>  <nl> + dm_bufio_allocated_kmem_cache = 0 ; <nl> + dm_bufio_allocated_get_free_pages = 0 ; <nl> + dm_bufio_allocated_vmalloc = 0 ; <nl> + dm_bufio_current_allocated = 0 ; <nl> + <nl> memset (& dm_bufio_caches , 0 , sizeof dm_bufio_caches ); <nl> memset (& dm_bufio_cache_names , 0 , sizeof dm_bufio_cache_names ); <nl> 
void ceph_handle_caps ( struct ceph_mds_session * session , <nl> cap -> cap_id = le64_to_cpu ( h -> cap_id ); <nl> cap -> mseq = mseq ; <nl> cap -> seq = seq ; <nl> + cap -> issue_seq = seq ; <nl> spin_lock (& session -> s_cap_lock ); <nl> list_add_tail (& cap -> session_caps , <nl> & session -> s_cap_releases );
acpi_ex_load_op ( union acpi_operand_object * obj_desc , <nl> */ <nl> status = acpi_tb_add_table ( table_ptr , & table_index ); <nl> if ( ACPI_FAILURE ( status )) { <nl> - return_ACPI_STATUS ( status ); <nl> + goto cleanup ; <nl> } <nl>  <nl> status =
static int mount_ubifs ( struct ubifs_info * c ) <nl> if ( err ) <nl> goto out_orphans ; <nl> err = ubifs_rcvry_gc_commit ( c ); <nl> + if ( err ) <nl> + goto out_orphans ; <nl> } else { <nl> err = take_gc_lnum ( c ); <nl> if ( err )
static unsigned int pxa2xx_ssp_get_clk_div ( struct driver_data * drv_data , <nl> switch ( drv_data -> ssp_type ) { <nl> case QUARK_X1000_SSP : <nl> clk_div = quark_x1000_get_clk_div ( rate , & chip -> dds_rate ); <nl> + break ; <nl> default : <nl> clk_div = ssp_get_clk_div ( drv_data , rate ); <nl> + break ; <nl> } <nl> return clk_div << 8 ; <nl> }
static int ieee802154_dev_ioctl ( struct sock * sk , struct ifreq __user * arg , <nl> dev_load ( sock_net ( sk ), ifr . ifr_name ); <nl> dev = dev_get_by_name ( sock_net ( sk ), ifr . ifr_name ); <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( dev -> type == ARPHRD_IEEE802154 && dev -> netdev_ops -> ndo_do_ioctl ) <nl> ret = dev -> netdev_ops -> ndo_do_ioctl ( dev , & ifr , cmd ); <nl> 
TRACE_EVENT ( iwlwifi_dev_tx , <nl> __entry -> framelen = buf0_len + buf1_len ; <nl> memcpy ( __get_dynamic_array ( tfd ), tfd , tfdlen ); <nl> memcpy ( __get_dynamic_array ( buf0 ), buf0 , buf0_len ); <nl> - memcpy ( __get_dynamic_array ( buf1 ), buf1 , buf0_len ); <nl> + memcpy ( __get_dynamic_array ( buf1 ), buf1 , buf1_len ); <nl> ), <nl> TP_printk ("[% p ] TX %. 2x (% zu bytes )", <nl> __entry -> priv ,
const struct imx_imx_ssi_data imx35_imx_ssi_data [] __initconst = { <nl> # ifdef CONFIG_SOC_IMX51 <nl> const struct imx_imx_ssi_data imx51_imx_ssi_data [] __initconst = { <nl> # define imx51_imx_ssi_data_entry ( _id , _hwid ) \ <nl> - imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_4K ) <nl> + imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_16K ) <nl> imx51_imx_ssi_data_entry ( 0 , 1 ), <nl> imx51_imx_ssi_data_entry ( 1 , 2 ), <nl> imx51_imx_ssi_data_entry ( 2 , 3 ),
int vmbus_open ( struct vmbus_channel * newchannel , u32 send_ringbuffer_size , <nl> ret = vmbus_post_msg ( open_msg , <nl> sizeof ( struct vmbus_channel_open_channel )); <nl>  <nl> - if ( ret != 0 ) <nl> + if ( ret != 0 ) { <nl> + err = ret ; <nl> goto error1 ; <nl> + } <nl>  <nl> t = wait_for_completion_timeout (& open_info -> waitevent , 5 * HZ ); <nl> if ( t == 0 ) {
static int net2280_stop ( struct usb_gadget * _gadget , <nl> device_remove_file (& dev -> pdev -> dev , & dev_attr_function ); <nl> device_remove_file (& dev -> pdev -> dev , & dev_attr_queues ); <nl>  <nl> - DEBUG ( dev , " unregistered driver '% s '\ n ", driver -> driver . name ); <nl> + DEBUG ( dev , " unregistered driver '% s '\ n ", <nl> + driver ? driver -> driver . name : ""); <nl> + <nl> return 0 ; <nl> } <nl> 
void blk_mq_delay_queue ( struct blk_mq_hw_ctx * hctx , unsigned long msecs ) <nl> if ( unlikely (! blk_mq_hw_queue_mapped ( hctx ))) <nl> return ; <nl>  <nl> + blk_mq_stop_hw_queue ( hctx ); <nl> kblockd_schedule_delayed_work_on ( blk_mq_hctx_next_cpu ( hctx ), <nl> & hctx -> delay_work , msecs_to_jiffies ( msecs )); <nl> }
static enum dvbfe_search cxd2820r_search ( struct dvb_frontend * fe ) <nl> /* frontend lock wait loop count */ <nl> switch ( priv -> delivery_system ) { <nl> case SYS_DVBT : <nl> + case SYS_DVBC_ANNEX_A : <nl> i = 20 ; <nl> break ; <nl> case SYS_DVBT2 :
static void sil_host_intr ( struct ata_port * ap , u32 bmdma2 ) <nl> u8 status ; <nl>  <nl> if ( unlikely ( bmdma2 & SIL_DMA_SATA_IRQ )) { <nl> - u32 serror ; <nl> + u32 serror = 0xffffffff ; <nl>  <nl> /* SIEN doesn ' t mask SATA IRQs on some 3112s . Those <nl> * controllers continue to assert IRQ as long as
static int pl330_dma_device_slave_caps ( struct dma_chan * dchan , <nl> caps -> directions = BIT ( DMA_DEV_TO_MEM ) | BIT ( DMA_MEM_TO_DEV ); <nl> caps -> cmd_pause = false ; <nl> caps -> cmd_terminate = true ; <nl> + caps -> residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR ; <nl>  <nl> return 0 ; <nl> }
static void bus_reset_work ( struct work_struct * work ) <nl> { <nl> struct fw_ohci * ohci = <nl> container_of ( work , struct fw_ohci , bus_reset_work ); <nl> - int self_id_count , i , j , reg ; <nl> - int generation , new_generation ; <nl> + int self_id_count , generation , new_generation , i , j ; <nl> + u32 reg ; <nl> unsigned long flags ; <nl> void * free_rom = NULL ; <nl> dma_addr_t free_rom_bus = 0 ;
static int loop_clr_fd ( struct loop_device * lo , struct block_device * bdev ) <nl> lo -> lo_state = Lo_unbound ; <nl> /* This is safe : open () is still holding a reference . */ <nl> module_put ( THIS_MODULE ); <nl> - if ( max_part > 0 ) <nl> + if ( max_part > 0 && bdev ) <nl> ioctl_by_bdev ( bdev , BLKRRPART , 0 ); <nl> mutex_unlock (& lo -> lo_ctl_mutex ); <nl> /*
static int hdmi_remove ( struct platform_device * pdev ) <nl>  <nl> pm_runtime_disable ( dev ); <nl>  <nl> - free_irq ( hdata -> irq , hdata ); <nl> + free_irq ( hdata -> irq , ctx ); <nl>  <nl>  <nl> /* hdmiphy i2c driver */
static int slic_ioctl ( struct net_device * dev , struct ifreq * rq , int cmd ) <nl> else <nl> duplex = 0 ; <nl> slic_link_config ( adapter , speed , duplex ); <nl> - slic_link_event_handler ( adapter ); <nl> + if ( slic_link_event_handler ( adapter )) <nl> + return - EFAULT ; <nl> } <nl> } <nl> return 0 ;
int intel_setup_gmbus ( struct drm_device * dev ) <nl> bus -> reg0 = i | GMBUS_RATE_100KHZ ; <nl>  <nl> /* XXX force bit banging until GMBUS is fully debugged */ <nl> - bus -> force_bit = intel_gpio_create ( dev_priv , i ); <nl> + if ( IS_GEN2 ( dev )) <nl> + bus -> force_bit = intel_gpio_create ( dev_priv , i ); <nl> } <nl>  <nl> intel_i2c_reset ( dev_priv -> dev );
static int skge_down ( struct net_device * dev ) <nl> struct skge_hw * hw = skge -> hw ; <nl> int port = skge -> port ; <nl>  <nl> - if ( skge -> mem == NULL ) <nl> + if (! skge -> mem ) <nl> return 0 ; <nl>  <nl> netif_info ( skge , ifdown , skge -> netdev , " disabling interface \ n ");
void BSP_gettod ( int * yearp , int * monp , int * dayp , <nl> { <nl> } <nl>  <nl> - int BSP_hwclk ( int op , struct hwclk_time * t ) <nl> + int BSP_hwclk ( int op , struct rtc_time * t ) <nl> { <nl> if (! op ) { <nl> /* read */
static int davinci_gpio_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& chips [ i ]. lock ); <nl>  <nl> regs = gpio2regs ( base ); <nl> + if (! regs ) <nl> + return - ENXIO ; <nl> chips [ i ]. regs = regs ; <nl> chips [ i ]. set_data = & regs -> set_data ; <nl> chips [ i ]. clr_data = & regs -> clr_data ;
static int w1_attach_slave_device ( struct w1_master * dev , struct w1_reg_num * rn ) <nl> atomic_set (& sl -> refcnt , 0 ); <nl> init_completion (& sl -> released ); <nl>  <nl> + /* slave modules need to be loaded in a context with unlocked mutex */ <nl> + mutex_unlock (& dev -> mutex ); <nl> request_module (" w1 - family - 0x % 0x ", rn -> family ); <nl> + mutex_lock (& dev -> mutex ); <nl>  <nl> spin_lock (& w1_flock ); <nl> f = w1_family_registered ( rn -> family );
static struct platform_driver axp288_fuel_gauge_driver = { <nl>  <nl> module_platform_driver ( axp288_fuel_gauge_driver ); <nl>  <nl> + MODULE_AUTHOR (" Ramakrishna Pallala < ramakrishna . pallala @ intel . com >"); <nl> MODULE_AUTHOR (" Todd Brandt < todd . e . brandt @ linux . intel . com >"); <nl> MODULE_DESCRIPTION (" Xpower AXP288 Fuel Gauge Driver "); <nl> MODULE_LICENSE (" GPL ");
void __init create_boot_cache ( struct kmem_cache * s , const char * name , size_t siz <nl> err = __kmem_cache_create ( s , flags ); <nl>  <nl> if ( err ) <nl> - panic (" Creation of kmalloc slab % s size =% zd failed . Reason % d \ n ", <nl> + panic (" Creation of kmalloc slab % s size =% zu failed . Reason % d \ n ", <nl> name , size , err ); <nl>  <nl> s -> refcount = - 1 ; /* Exempt from merging for now */
static int sierra_resume ( struct usb_serial * serial ) <nl> if ( err < 0 ) { <nl> intfdata -> in_flight --; <nl> usb_unanchor_urb ( urb ); <nl> - usb_scuttle_anchored_urbs (& portdata -> delayed ); <nl> - break ; <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> + spin_lock (& portdata -> lock ); <nl> + portdata -> outstanding_urbs --; <nl> + spin_unlock (& portdata -> lock ); <nl> + continue ; <nl> } <nl> } <nl> 
static struct sh_eth_cpu_data sh7734_data = { <nl> . tsu = 1 , <nl> . hw_checksum = 1 , <nl> . select_mii = 1 , <nl> + . magic = 1 , <nl> }; <nl>  <nl> /* SH7763 */
static int mcp23s08_irq_setup ( struct mcp23s08 * mcp ) <nl> return - ENODEV ; <nl>  <nl> err = devm_request_threaded_irq ( chip -> dev , mcp -> irq , NULL , mcp23s08_irq , <nl> - IRQF_TRIGGER_LOW | IRQF_ONESHOT , <nl> + IRQF_TRIGGER_LOW | IRQF_ONESHOT | <nl> + IRQF_SHARED , <nl> dev_name ( chip -> dev ), mcp ); <nl> if ( err != 0 ) { <nl> dev_err ( chip -> dev , " unable to request IRQ #% d : % d \ n ",
static void ad1884_fixup_thinkpad ( struct hda_codec * codec , <nl> { <nl> struct ad198x_spec * spec = codec -> spec ; <nl>  <nl> - if ( action == HDA_FIXUP_ACT_PRE_PROBE ) <nl> + if ( action == HDA_FIXUP_ACT_PRE_PROBE ) { <nl> spec -> gen . keep_eapd_on = 1 ; <nl> + spec -> gen . vmaster_mute . hook = ad_vmaster_eapd_hook ; <nl> + spec -> eapd_nid = 0x12 ; <nl> + } <nl> } <nl>  <nl> /* set magic COEFs for dmic */
int afu_register_irqs ( struct cxl_context * ctx , u32 count ) <nl> */ <nl> INIT_LIST_HEAD (& ctx -> irq_names ); <nl> for ( r = 1 ; r < CXL_IRQ_RANGES ; r ++) { <nl> - for ( i = 0 ; i < ctx -> irqs . range [ r ]; hwirq ++, i ++) { <nl> + for ( i = 0 ; i < ctx -> irqs . range [ r ]; i ++) { <nl> irq_name = kmalloc ( sizeof ( struct cxl_irq_name ), <nl> GFP_KERNEL ); <nl> if (! irq_name )
void radeon_audio_detect ( struct drm_connector * connector , <nl> return ; <nl>  <nl> rdev = connector -> encoder -> dev -> dev_private ; <nl> + <nl> + if (! radeon_audio_chipset_supported ( rdev )) <nl> + return ; <nl> + <nl> radeon_encoder = to_radeon_encoder ( connector -> encoder ); <nl> dig = radeon_encoder -> enc_priv ; <nl> 
static int ll_dir_setdirstripe ( struct inode * parent , struct lmv_user_md * lump , <nl> PFID ( ll_inode2fid ( parent )), parent , dirname , <nl> ( int ) lump -> lum_stripe_offset , lump -> lum_stripe_count ); <nl>  <nl> + if ( lump -> lum_stripe_count > 1 && <nl> + !( exp_connect_flags ( sbi -> ll_md_exp ) & OBD_CONNECT_DIR_STRIPE )) <nl> + return - EINVAL ; <nl> + <nl> if ( lump -> lum_magic != cpu_to_le32 ( LMV_USER_MAGIC )) <nl> lustre_swab_lmv_user_md ( lump ); <nl> 
void oz_apps_term ( void ) <nl> void oz_handle_app_elt ( struct oz_pd * pd , u8 app_id , struct oz_elt * elt ) <nl> { <nl> struct oz_app_if * ai ; <nl> - if ( app_id > OZ_APPID_MAX ) <nl> + if ( app_id == 0 || app_id > OZ_APPID_MAX ) <nl> return ; <nl> ai = & g_app_if [ app_id - 1 ]; <nl> ai -> rx ( pd , elt );
struct s3c2410_spigpio { <nl>  <nl> static inline struct s3c2410_spigpio * spidev_to_sg ( struct spi_device * spi ) <nl> { <nl> - return spi -> controller_data ; <nl> + return spi_master_get_devdata ( spi -> master ); <nl> } <nl>  <nl> static inline void setsck ( struct spi_device * dev , int on )
int btrfs_check_trunc_cache_free_space ( struct btrfs_root * root , <nl> else <nl> ret = 0 ; <nl> spin_unlock (& rsv -> lock ); <nl> - return 0 ; <nl> + return ret ; <nl> } <nl>  <nl> int btrfs_truncate_free_space_cache ( struct btrfs_root * root ,
static void igb_update_ring_itr ( struct igb_q_vector * q_vector ) <nl> else <nl> new_val = avg_wire_size / 2 ; <nl>  <nl> + /* when in itr mode 3 do not exceed 20K ints / sec */ <nl> + if ( adapter -> rx_itr_setting == 3 && new_val < 196 ) <nl> + new_val = 196 ; <nl> + <nl> set_itr_val : <nl> if ( new_val != q_vector -> itr_val ) { <nl> q_vector -> itr_val = new_val ;
static long ft1000_ChIoctl ( struct file * File , unsigned int Command , <nl> break ; <nl> case IOCTL_GET_DSP_STAT_CMD : <nl> // DEBUG (" FT1000 : ft1000_ChIoctl : IOCTL_FT1000_GET_DSP_STAT called \ n "); <nl> - <nl> + memset (& get_stat_data , 0 , sizeof ( get_stat_data )); <nl> memcpy ( get_stat_data . DspVer , info -> DspVer , DSPVERSZ ); <nl> memcpy ( get_stat_data . HwSerNum , info -> HwSerNum , HWSERNUMSZ ); <nl> memcpy ( get_stat_data . Sku , info -> Sku , SKUSZ );
static void ext3_put_super ( struct super_block * sb ) <nl> } <nl> sb -> s_fs_info = NULL ; <nl> kfree ( sbi -> s_blockgroup_lock ); <nl> + mutex_destroy (& sbi -> s_orphan_lock ); <nl> + mutex_destroy (& sbi -> s_resize_lock ); <nl> kfree ( sbi ); <nl> } <nl> 
int snd_hda_create_dig_out_ctls ( struct hda_codec * codec , <nl> return - EBUSY ; <nl> } <nl> spdif = snd_array_new (& codec -> spdif_out ); <nl> + if (! spdif ) <nl> + return - ENOMEM ; <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> if (! kctl )
static int _hardware_enqueue ( struct ci_hw_ep * hwep , struct ci_hw_req * hwreq ) <nl> rest -= count ; <nl> } <nl>  <nl> - if ( hwreq -> req . zero && hwreq -> req . length <nl> + if ( hwreq -> req . zero && hwreq -> req . length && hwep -> dir == TX <nl> && ( hwreq -> req . length % hwep -> ep . maxpacket == 0 )) <nl> add_td_to_list ( hwep , hwreq , 0 ); <nl> 
late_initcall ( geneve_init_module ); <nl> static void __exit geneve_cleanup_module ( void ) <nl> { <nl> destroy_workqueue ( geneve_wq ); <nl> + unregister_pernet_subsys (& geneve_net_ops ); <nl> } <nl> module_exit ( geneve_cleanup_module ); <nl> 
static void __init ati_bugs_contd ( int num , int slot , int func ) <nl> if ( rev >= 0x40 ) <nl> acpi_fix_pin2_polarity = 1 ; <nl>  <nl> - if ( rev > 0x13 ) <nl> + /* <nl> + * SB600 : revisions 0x11 , 0x12 , 0x13 , 0x14 , ... <nl> + * SB700 : revisions 0x39 , 0x3a , ... <nl> + * SB800 : revisions 0x40 , 0x41 , ... <nl> + */ <nl> + if ( rev >= 0x39 ) <nl> return ; <nl>  <nl> if ( acpi_use_timer_override )
int inet_recv_error ( struct sock * sk , struct msghdr * msg , int len , <nl>  <nl> static inline void inet_ctl_sock_destroy ( struct sock * sk ) <nl> { <nl> - sock_release ( sk -> sk_socket ); <nl> + if ( sk ) <nl> + sock_release ( sk -> sk_socket ); <nl> } <nl>  <nl> # endif
static int __rfcomm_dlc_close ( struct rfcomm_dlc * d , int err ) <nl> rfcomm_dlc_unlock ( d ); <nl>  <nl> skb_queue_purge (& d -> tx_queue ); <nl> - rfcomm_session_put ( s ); <nl> - <nl> rfcomm_dlc_unlink ( d ); <nl> } <nl>  <nl> static struct rfcomm_session * rfcomm_session_create ( bdaddr_t * src , bdaddr_t * dst <nl> goto failed ; <nl> } <nl>  <nl> - rfcomm_session_hold ( s ); <nl> - <nl> s -> initiator = 1 ; <nl>  <nl> bacpy (& addr . l2_bdaddr , dst );
static int em28xx_i2c_xfer ( struct i2c_adapter * i2c_adap , <nl> if ( dev -> disconnected ) <nl> return - ENODEV ; <nl>  <nl> - rc = rt_mutex_trylock (& dev -> i2c_bus_lock ); <nl> - if ( rc < 0 ) <nl> - return rc ; <nl> + if (! rt_mutex_trylock (& dev -> i2c_bus_lock )) <nl> + return - EAGAIN ; <nl>  <nl> /* Switch I2C bus if needed */ <nl> if ( bus != dev -> cur_i2c_bus &&
static struct sk_buff * fill_packet_ipv4 ( struct net_device * odev , <nl> /* Eth + IPh + UDPh + mpls */ <nl> datalen = pkt_dev -> cur_pkt_size - 14 - 20 - 8 - <nl> pkt_dev -> pkt_overhead ; <nl> - if ( datalen < sizeof ( struct pktgen_hdr )) <nl> + if ( datalen < 0 || datalen < sizeof ( struct pktgen_hdr )) <nl> datalen = sizeof ( struct pktgen_hdr ); <nl>  <nl> udph -> source = htons ( pkt_dev -> cur_udp_src );
static int slic_mcast_add_list ( struct adapter * adapter , char * address ) <nl> } <nl>  <nl> /* Doesn ' t already exist . Allocate a structure to hold it */ <nl> - mcaddr = kmalloc ( sizeof ( struct mcast_address ), GFP_ATOMIC ); <nl> + mcaddr = kmalloc ( sizeof (* mcaddr ), GFP_ATOMIC ); <nl> if ( mcaddr == NULL ) <nl> return 1 ; <nl> 
# define DOC_ECCCONF1 0x1042 <nl> # define DOC_ECCPRESET 0x1044 <nl> # define DOC_HAMMINGPARITY 0x1046 <nl> -# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 1 )) <nl> +# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 0 )) <nl>  <nl> # define DOC_PROTECTION 0x1056 <nl> # define DOC_DPS0_ADDRLOW 0x1060
int kvm_dev_ioctl_check_extension ( long ext ) <nl> case KVM_CAP_SYNC_REGS : <nl> r = 1 ; <nl> break ; <nl> + case KVM_CAP_NR_VCPUS : <nl> + case KVM_CAP_MAX_VCPUS : <nl> + r = KVM_MAX_VCPUS ; <nl> + break ; <nl> default : <nl> r = 0 ; <nl> }
static void virtcons_remove ( struct virtio_device * vdev ) <nl> /* Disable interrupts for vqs */ <nl> vdev -> config -> reset ( vdev ); <nl> /* Finish up work that ' s lined up */ <nl> - cancel_work_sync (& portdev -> control_work ); <nl> + if ( use_multiport ( portdev )) <nl> + cancel_work_sync (& portdev -> control_work ); <nl>  <nl> list_for_each_entry_safe ( port , port2 , & portdev -> ports , list ) <nl> unplug_port ( port );
static void pn_rx_complete ( struct usb_ep * ep , struct usb_request * req ) <nl> } <nl>  <nl> skb_add_rx_frag ( skb , skb_shinfo ( skb )-> nr_frags , page , <nl> - skb -> len == 0 , req -> actual ); <nl> + skb -> len <= 1 , req -> actual ); <nl> page = NULL ; <nl>  <nl> if ( req -> actual < req -> length ) { /* Last fragment */
qla2x00_probe_one ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> sht = & qla2x00_driver_template ; <nl> if ( pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2422 || <nl> - pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 ) <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5422 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5432 ) <nl> sht = & qla24xx_driver_template ; <nl> host = scsi_host_alloc ( sht , sizeof ( scsi_qla_host_t )); <nl> if ( host == NULL ) {
static int nsp_cs_config ( struct pcmcia_device * link ) <nl>  <nl> nsp_dbg ( NSP_DEBUG_INIT , " in "); <nl>  <nl> - cfg_mem = kzalloc ( sizeof ( cfg_mem ), GFP_KERNEL ); <nl> + cfg_mem = kzalloc ( sizeof (* cfg_mem ), GFP_KERNEL ); <nl> if (! cfg_mem ) <nl> return - ENOMEM ; <nl> cfg_mem -> data = data ;
# include < syslog . h > <nl> # endif <nl>  <nl> -# define S8 int8_t <nl> # define S16 int16_t <nl> # define S32 int32_t <nl> # define S64 int64_t
static void pci_acpi_cleanup ( struct device * dev ) <nl>  <nl> static bool pci_acpi_bus_match ( struct device * dev ) <nl> { <nl> - return dev -> bus == & pci_bus_type ; <nl> + return dev_is_pci ( dev ); <nl> } <nl>  <nl> static struct acpi_bus_type acpi_pci_bus = {
static int rt5645_irq_detection ( struct rt5645_priv * rt5645 ) <nl> { <nl> int val , btn_type , gpio_state = 0 , report = 0 ; <nl>  <nl> + if (! rt5645 -> codec ) <nl> + return - EINVAL ; <nl> + <nl> switch ( rt5645 -> pdata . jd_mode ) { <nl> case 0 : /* Not using rt5645 JD */ <nl> if ( rt5645 -> gpiod_hp_det ) {
static inline int phy_set_mode ( struct phy * phy , enum phy_mode mode ) <nl> return - ENOSYS ; <nl> } <nl>  <nl> + static inline int phy_reset ( struct phy * phy ) <nl> +{ <nl> + if (! phy ) <nl> + return 0 ; <nl> + return - ENOSYS ; <nl> +} <nl> + <nl> static inline int phy_get_bus_width ( struct phy * phy ) <nl> { <nl> return - ENOSYS ;
static unsigned int get_max_cost ( struct f2fs_sb_info * sbi , <nl> if ( p -> alloc_mode == SSR ) <nl> return sbi -> blocks_per_seg ; <nl> if ( p -> gc_mode == GC_GREEDY ) <nl> - return sbi -> blocks_per_seg * p -> ofs_unit ; <nl> + return 2 * sbi -> blocks_per_seg * p -> ofs_unit ; <nl> else if ( p -> gc_mode == GC_CB ) <nl> return UINT_MAX ; <nl> else /* No other gc_mode */
static int emi26_load_firmware ( struct usb_device * dev ) <nl>  <nl> /* De - assert reset ( let the CPU run ) */ <nl> err = emi26_set_reset ( dev , 0 ); <nl> + if ( err < 0 ) { <nl> + err ("% s - error loading firmware : error = % d ", __FUNCTION__ , err ); <nl> + goto wraperr ; <nl> + } <nl> msleep ( 250 ); /* let device settle */ <nl>  <nl> /* 2 . We upload the FPGA firmware into the EMI
static struct collection collections [] = { <nl>  <nl> /* Iterate over all benchmarks within a collection : */ <nl> # define for_each_bench ( coll , bench ) \ <nl> - for ( bench = coll -> benchmarks ; bench -> name ; bench ++) <nl> + for ( bench = coll -> benchmarks ; bench && bench -> name ; bench ++) <nl>  <nl> static void dump_benchmarks ( struct collection * coll ) <nl> {
static int qxl_palette_create_1bit ( struct qxl_bo * palette_bo , <nl> * correctly globaly , since that would require <nl> * tracking all of our palettes . */ <nl> ret = qxl_bo_kmap ( palette_bo , ( void **)& pal ); <nl> + if ( ret ) <nl> + return ret ; <nl> pal -> num_ents = 2 ; <nl> pal -> unique = unique ++; <nl> if ( visual == FB_VISUAL_TRUECOLOR || visual == FB_VISUAL_DIRECTCOLOR ) {
static void tcp_init_metrics ( struct sock * sk ) <nl> } <nl> if ( dst_metric ( dst , RTAX_RTTVAR ) > tp -> mdev ) { <nl> tp -> mdev = dst_metric ( dst , RTAX_RTTVAR ); <nl> - tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , TCP_RTO_MIN ); <nl> + tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , tcp_rto_min ( sk )); <nl> } <nl> tcp_set_rto ( sk ); <nl> tcp_bound_rto ( sk );
static int selinux_setprocattr ( struct task_struct * p , <nl> return error ; <nl>  <nl> /* Obtain a SID for the context , if one was specified . */ <nl> - if ( size && str [ 1 ] && str [ 1 ] != '\ n ') { <nl> + if ( size && str [ 0 ] && str [ 0 ] != '\ n ') { <nl> if ( str [ size - 1 ] == '\ n ') { <nl> str [ size - 1 ] = 0 ; <nl> size --;
static int gb_uart_connection_init ( struct gb_connection * connection ) <nl> if ( minor == - ENOSPC ) { <nl> dev_err (& connection -> dev , <nl> " no more free minor numbers \ n "); <nl> - return - ENODEV ; <nl> + retval = - ENODEV ; <nl> + goto error_version ; <nl> } <nl> - return minor ; <nl> + retval = minor ; <nl> + goto error_version ; <nl> } <nl>  <nl> gb_tty -> minor = minor ;
static int rohm_ts_load_firmware ( struct i2c_client * client , <nl> break ; <nl>  <nl> error = - EIO ; <nl> - } while (++ retry >= FIRMWARE_RETRY_MAX ); <nl> + } while (++ retry <= FIRMWARE_RETRY_MAX ); <nl>  <nl> out : <nl> error2 = i2c_smbus_write_byte_data ( client , INT_MASK , INT_ALL );
static int __devinit snd_ca0106_probe ( struct pci_dev * pci , <nl> snd_ca0106_proc_init ( chip ); <nl> # endif <nl>  <nl> + snd_card_set_dev ( card , & pci -> dev ); <nl> + <nl> if (( err = snd_card_register ( card )) < 0 ) { <nl> snd_card_free ( card ); <nl> return err ;
struct dnode_of_data { <nl> static inline void set_new_dnode ( struct dnode_of_data * dn , struct inode * inode , <nl> struct page * ipage , struct page * npage , nid_t nid ) <nl> { <nl> + memset ( dn , 0 , sizeof (* dn )); <nl> dn -> inode = inode ; <nl> dn -> inode_page = ipage ; <nl> dn -> node_page = npage ; <nl> dn -> nid = nid ; <nl> - dn -> inode_page_locked = 0 ; <nl> } <nl>  <nl> /*
retry : <nl> if ( local_flags & __GFP_WAIT ) <nl> local_irq_enable (); <nl> kmem_flagcheck ( cache , flags ); <nl> - obj = kmem_getpages ( cache , flags , - 1 ); <nl> + obj = kmem_getpages ( cache , local_flags , - 1 ); <nl> if ( local_flags & __GFP_WAIT ) <nl> local_irq_disable (); <nl> if ( obj ) {
static void atombios_crtc_program_ss ( struct drm_crtc * crtc , <nl> return ; <nl> } <nl> args . v2 . ucEnable = enable ; <nl> - if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK )) <nl> + if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK ) || ASIC_IS_DCE41 ( rdev )) <nl> args . v2 . ucEnable = ATOM_DISABLE ; <nl> } else if ( ASIC_IS_DCE3 ( rdev )) { <nl> args . v1 . usSpreadSpectrumPercentage = cpu_to_le16 ( ss -> percentage );
static int __devinit pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> unsigned int delay ; <nl> u8 pon_cntl ; <nl> struct pmic8xxx_pwrkey * pwrkey ; <nl> - const struct pm8xxx_pwrkey_platform_data * pdata = mfd_get_data ( pdev ); <nl> + const struct pm8xxx_pwrkey_platform_data * pdata = <nl> + dev_get_platdata (& pdev -> dev ); <nl>  <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " power key platform data not supplied \ n ");
struct rtnl_link_stats64 * e1000e_get_stats64 ( struct net_device * netdev , <nl> static int e1000_change_mtu ( struct net_device * netdev , int new_mtu ) <nl> { <nl> struct e1000_adapter * adapter = netdev_priv ( netdev ); <nl> - int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN ; <nl> + int max_frame = new_mtu + VLAN_HLEN + ETH_HLEN + ETH_FCS_LEN ; <nl>  <nl> /* Jumbo frame support */ <nl> if (( max_frame > ETH_FRAME_LEN + ETH_FCS_LEN ) &&
static ssize_t do_generic_file_read ( struct file * filp , loff_t * ppos , <nl>  <nl> cond_resched (); <nl> find_page : <nl> + if ( fatal_signal_pending ( current )) { <nl> + error = - EINTR ; <nl> + goto out ; <nl> + } <nl> + <nl> page = find_get_page ( mapping , index ); <nl> if (! page ) { <nl> page_cache_sync_readahead ( mapping ,
void drm_mm_remove_node ( struct drm_mm_node * node ) <nl> struct drm_mm * mm = node -> mm ; <nl> struct drm_mm_node * prev_node ; <nl>  <nl> + if ( WARN_ON (! node -> allocated )) <nl> + return ; <nl> + <nl> BUG_ON ( node -> scanned_block || node -> scanned_prev_free <nl> || node -> scanned_next_free ); <nl> 
__ieee80211_tx_prepare ( struct ieee80211_tx_data * tx , <nl> tx -> local = local ; <nl> tx -> sdata = IEEE80211_DEV_TO_SUB_IF ( dev ); <nl> tx -> channel = local -> hw . conf . channel ; <nl> + tx -> rate_idx = - 1 ; <nl> + tx -> last_frag_rate_idx = - 1 ; <nl> /* <nl> * Set this flag ( used below to indicate " automatic fragmentation "), <nl> * it will be cleared / left by radiotap as desired .
static int tiadc_read_raw ( struct iio_dev * indio_dev , <nl> return - EAGAIN ; <nl> } <nl> } <nl> - map_val = chan -> channel + TOTAL_CHANNELS ; <nl> + map_val = adc_dev -> channel_step [ chan -> scan_index ]; <nl>  <nl> /* <nl> * We check the complete FIFO . We programmed just one entry but in case
void cpuset_print_task_mems_allowed ( struct task_struct * tsk ) <nl>  <nl> struct cgroup * cgrp = task_cs ( tsk )-> css . cgroup ; <nl>  <nl> + rcu_read_lock (); <nl> spin_lock (& cpuset_buffer_lock ); <nl>  <nl> nodelist_scnprintf ( cpuset_nodelist , CPUSET_NODELIST_LEN , <nl> void cpuset_print_task_mems_allowed ( struct task_struct * tsk ) <nl> tsk -> comm , cgroup_name ( cgrp ), cpuset_nodelist ); <nl>  <nl> spin_unlock (& cpuset_buffer_lock ); <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> /*
static umode_t lm3533_attr_is_visible ( struct kobject * kobj , <nl> struct device_attribute * dattr = to_dev_attr ( attr ); <nl> struct lm3533_device_attribute * lattr = to_lm3533_dev_attr ( dattr ); <nl> enum lm3533_attribute_type type = lattr -> type ; <nl> - mode_t mode = attr -> mode ; <nl> + umode_t mode = attr -> mode ; <nl>  <nl> if (! lm3533 -> have_backlights && type == LM3533_ATTR_TYPE_BACKLIGHT ) <nl> mode = 0 ;
static const struct mfd_cell cros_devs [] = { <nl> . id = 2 , <nl> . of_compatible = " google , cros - ec - i2c - tunnel ", <nl> }, <nl> + { <nl> + . name = " cros - ec - ctl ", <nl> + . id = 3 , <nl> + }, <nl> }; <nl>  <nl> int cros_ec_register ( struct cros_ec_device * ec_dev )
static void dm_stat_free ( struct rcu_head * head ) <nl> int cpu ; <nl> struct dm_stat * s = container_of ( head , struct dm_stat , rcu_head ); <nl>  <nl> + kfree ( s -> histogram_boundaries ); <nl> kfree ( s -> program_id ); <nl> kfree ( s -> aux_data ); <nl> for_each_possible_cpu ( cpu ) {
static struct clk * cp110_register_gate ( const char * name , <nl> if (! gate ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> + memset (& init , 0 , sizeof ( init )); <nl> + <nl> init . name = name ; <nl> init . ops = & cp110_gate_ops ; <nl> init . parent_names = & parent_name ;
static int unix_dgram_recvmsg ( struct socket * sock , struct msghdr * msg , <nl> goto out_unlock ; <nl> } <nl>  <nl> - wake_up_interruptible_sync_poll (& u -> peer_wait , <nl> - POLLOUT | POLLWRNORM | POLLWRBAND ); <nl> + if ( wq_has_sleeper (& u -> peer_wait )) <nl> + wake_up_interruptible_sync_poll (& u -> peer_wait , <nl> + POLLOUT | POLLWRNORM | <nl> + POLLWRBAND ); <nl>  <nl> if ( msg -> msg_name ) <nl> unix_copy_addr ( msg , skb -> sk );
static inline struct sk_buff * sk_stream_alloc_pskb ( struct sock * sk , <nl> { <nl> struct sk_buff * skb ; <nl>  <nl> + /* The TCP header must be at least 32 - bit aligned . */ <nl> + size = ALIGN ( size , 4 ); <nl> + <nl> skb = alloc_skb_fclone ( size + sk -> sk_prot -> max_header , gfp ); <nl> if ( skb ) { <nl> skb -> truesize += mem ;
int pl320_ipc_unregister_notifier ( struct notifier_block * nb ) <nl> } <nl> EXPORT_SYMBOL_GPL ( pl320_ipc_unregister_notifier ); <nl>  <nl> - static int __init pl320_probe ( struct amba_device * adev , <nl> - const struct amba_id * id ) <nl> + static int pl320_probe ( struct amba_device * adev , const struct amba_id * id ) <nl> { <nl> int ret ; <nl> 
int drm_mode_page_flip_ioctl ( struct drm_device * dev , <nl> goto out ; <nl> crtc = obj_to_crtc ( obj ); <nl>  <nl> + if ( crtc -> fb == NULL ) { <nl> + /* The framebuffer is currently unbound , presumably <nl> + * due to a hotplug event , that userspace has not <nl> + * yet discovered . <nl> + */ <nl> + ret = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> if ( crtc -> funcs -> page_flip == NULL ) <nl> goto out ; <nl> 
static int max77686_clk_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + drv_data -> num_clks = num_clks ; <nl> drv_data -> max_clk_data = devm_kcalloc ( dev , num_clks , <nl> sizeof (* drv_data -> max_clk_data ), <nl> GFP_KERNEL );
static int acpi_cpufreq_cpu_init ( struct cpufreq_policy * policy ) <nl>  <nl> switch ( perf -> control_register . space_id ) { <nl> case ACPI_ADR_SPACE_SYSTEM_IO : <nl> + if ( boot_cpu_data . x86_vendor == X86_VENDOR_AMD && <nl> + boot_cpu_data . x86 == 0xf ) { <nl> + pr_debug (" AMD K8 systems must use native drivers .\ n "); <nl> + result = - ENODEV ; <nl> + goto err_unreg ; <nl> + } <nl> pr_debug (" SYSTEM IO addr space \ n "); <nl> data -> cpu_feature = SYSTEM_IO_CAPABLE ; <nl> break ;
static int rtl8152_close ( struct net_device * netdev ) <nl> netif_stop_queue ( netdev ); <nl>  <nl> res = usb_autopm_get_interface ( tp -> intf ); <nl> - if ( res < 0 ) { <nl> + if ( res < 0 || test_bit ( RTL8152_UNPLUG , & tp -> flags )) { <nl> rtl_drop_queued_tx ( tp ); <nl> rtl_stop_rx ( tp ); <nl> } else {
static void doc_delay ( struct docg3 * docg3 , int nbNOPs ) <nl> { <nl> int i ; <nl>  <nl> - doc_dbg (" NOP x % d \ n ", nbNOPs ); <nl> + doc_vdbg (" NOP x % d \ n ", nbNOPs ); <nl> for ( i = 0 ; i < nbNOPs ; i ++) <nl> doc_writeb ( docg3 , 0 , DOC_NOP ); <nl> }
ieee80211softmac_assoc_req ( struct ieee80211_assoc_request ** pkt , <nl> return 0 ; <nl> ieee80211softmac_hdr_3addr ( mac , &((* pkt )-> header ), IEEE80211_STYPE_ASSOC_REQ , net -> bssid , net -> bssid ); <nl>  <nl> + /* Fill in the capabilities */ <nl> + (* pkt )-> capability = ieee80211softmac_capabilities ( mac , net ); <nl> + <nl> /* Fill in Listen Interval (?) */ <nl> (* pkt )-> listen_interval = cpu_to_le16 ( 10 ); <nl> 
static int sep_prepare_input_output_dma_table_in_dcb ( struct sep_device * sep , <nl> } <nl> } <nl> if ( tail_size ) { <nl> + if ( tail_size > sizeof ( dcb_table_ptr -> tail_data )) <nl> + return - EINVAL ; <nl> if ( is_kva == true ) { <nl> memcpy ( dcb_table_ptr -> tail_data , <nl> ( void *)( app_in_address + data_in_size -
static int of_mpc8xxx_spi_get_chipselects ( struct device * dev ) <nl> gpio = of_get_gpio_flags ( np , i , & flags ); <nl> if (! gpio_is_valid ( gpio )) { <nl> dev_err ( dev , " invalid gpio #% d : % d \ n ", i , gpio ); <nl> + ret = gpio ; <nl> goto err_loop ; <nl> } <nl> 
static struct kvmppc_linear_info * kvm_alloc_linear ( int type ) <nl> break ; <nl> } <nl> spin_unlock (& linear_lock ); <nl> + memset ( ri -> base_virt , 0 , ri -> npages << PAGE_SHIFT ); <nl> return ri ; <nl> } <nl> 
static int ac97_write ( struct snd_soc_codec * codec , unsigned int reg , <nl> { <nl> u16 * cache = codec -> reg_cache ; <nl>  <nl> - soc_ac97_ops . write ( codec -> ac97 , reg , val ); <nl> + if ( reg < 0x7c ) <nl> + soc_ac97_ops . write ( codec -> ac97 , reg , val ); <nl> reg = reg >> 1 ; <nl> if ( reg < ( ARRAY_SIZE ( wm9712_reg ))) <nl> cache [ reg ] = val ;
extern void __qdisc_run ( struct Qdisc * q ); <nl>  <nl> static inline void qdisc_run ( struct Qdisc * q ) <nl> { <nl> - if (! test_and_set_bit ( __QDISC_STATE_RUNNING , & q -> state )) <nl> + struct netdev_queue * txq = q -> dev_queue ; <nl> + <nl> + if (! netif_tx_queue_stopped ( txq ) && <nl> + ! test_and_set_bit ( __QDISC_STATE_RUNNING , & q -> state )) <nl> __qdisc_run ( q ); <nl> } <nl> 
static int skl_probe ( struct pci_dev * pci , <nl> if ( err < 0 ) <nl> goto out_free ; <nl>  <nl> + skl -> nhlt = skl_nhlt_init ( bus -> dev ); <nl> + <nl> + if ( skl -> nhlt == NULL ) <nl> + goto out_free ; <nl> + <nl> pci_set_drvdata ( skl -> pci , ebus ); <nl>  <nl> /* check if dsp is there */
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> fail_workqueue : <nl> wiphy_unregister ( local -> hw . wiphy ); <nl> fail_wiphy_register : <nl> - kfree ( local -> int_scan_req -> channels ); <nl> + kfree ( local -> int_scan_req ); <nl> return result ; <nl> } <nl> EXPORT_SYMBOL ( ieee80211_register_hw );
void x86_pci_root_bus_res_quirks ( struct pci_bus * b ) <nl> int j ; <nl> struct pci_root_info * info ; <nl>  <nl> + /* don ' t go for it if _CRS is used */ <nl> + if ( pci_probe & PCI_USE__CRS ) <nl> + return ; <nl> + <nl> /* if only one root bus , don ' t need to anything */ <nl> if ( pci_root_num < 2 ) <nl> return ;
asmlinkage long sys_oabi_semtimedop ( int semid , <nl> long err ; <nl> int i ; <nl>  <nl> - if ( nsops < 1 ) <nl> + if ( nsops < 1 || nsops > SEMOPM ) <nl> return - EINVAL ; <nl> sops = kmalloc ( sizeof (* sops ) * nsops , GFP_KERNEL ); <nl> if (! sops )
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
static void iwlagn_bt_traffic_change_work ( struct work_struct * work ) <nl> priv -> cfg -> ops -> lib -> update_chain_flags ( priv ); <nl>  <nl> if ( smps_request != - 1 ) { <nl> + priv -> current_ht_config . smps = smps_request ; <nl> for_each_context ( priv , ctx ) { <nl> if ( ctx -> vif && ctx -> vif -> type == NL80211_IFTYPE_STATION ) <nl> ieee80211_request_smps ( ctx -> vif , smps_request );
int tm6000_reset ( struct tm6000_core * dev ) <nl>  <nl> msleep ( 5 ); <nl>  <nl> + /* <nl> + * Not all devices have int_in defined <nl> + */ <nl> + if (! dev -> int_in . endp ) <nl> + return 0 ; <nl> + <nl> err = usb_set_interface ( dev -> udev , dev -> isoc_in . bInterfaceNumber , 2 ); <nl> if ( err < 0 ) { <nl> tm6000_err (" failed to select interface % d , alt . setting 2 \ n ",
err : <nl> */ <nl> void fcoe_ctlr_recv ( struct fcoe_ctlr * fip , struct sk_buff * skb ) <nl> { <nl> + skb = skb_share_check ( skb , GFP_ATOMIC ); <nl> + if (! skb ) <nl> + return ; <nl> skb_queue_tail (& fip -> fip_recv_list , skb ); <nl> schedule_work (& fip -> recv_work ); <nl> }
d40_get_dev_addr ( struct d40_chan * chan , enum dma_data_direction direction ) <nl> { <nl> struct stedma40_platform_data * plat = chan -> base -> plat_data ; <nl> struct stedma40_chan_cfg * cfg = & chan -> dma_cfg ; <nl> - dma_addr_t addr ; <nl> + dma_addr_t addr = 0 ; <nl>  <nl> if ( chan -> runtime_addr ) <nl> return chan -> runtime_addr ;
static void mpc_push ( struct atm_vcc * vcc , struct sk_buff * skb ) <nl> eg -> packets_rcvd ++; <nl> mpc -> eg_ops -> put ( eg ); <nl>  <nl> - memset ( ATM_SKB ( skb ), 0 , sizeof ( struct atm_skb_data )); <nl> + memset ( ATM_SKB ( new_skb ), 0 , sizeof ( struct atm_skb_data )); <nl> netif_rx ( new_skb ); <nl> } <nl> 
int xfrm_init_replay ( struct xfrm_state * x ) <nl> replay_esn -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> return - EINVAL ; <nl>  <nl> + if (( x -> props . flags & XFRM_STATE_ESN ) && replay_esn -> replay_window == 0 ) <nl> + return - EINVAL ; <nl> + <nl> if (( x -> props . flags & XFRM_STATE_ESN ) && x -> replay_esn ) <nl> x -> repl = & xfrm_replay_esn ; <nl> else
int recover_inode_page ( struct f2fs_sb_info * sbi , struct page * page ) <nl> new_ni = old_ni ; <nl> new_ni . ino = ino ; <nl>  <nl> + if (! inc_valid_node_count ( sbi , NULL , 1 )) <nl> + WARN_ON ( 1 ); <nl> set_node_addr ( sbi , & new_ni , NEW_ADDR ); <nl> inc_valid_inode_count ( sbi ); <nl> 
static int tg3_init_one ( struct pci_dev * pdev , <nl>  <nl> tg3_timer_init ( tp ); <nl>  <nl> + tg3_carrier_off ( tp ); <nl> + <nl> err = register_netdev ( dev ); <nl> if ( err ) { <nl> dev_err (& pdev -> dev , " Cannot register net device , aborting \ n ");
static int cgroup_get_sb ( struct file_system_type * fs_type , <nl> BUG_ON ( root -> number_of_cgroups != 1 ); <nl>  <nl> cgroup_populate_dir ( root_cgrp ); <nl> - mutex_unlock (& inode -> i_mutex ); <nl> mutex_unlock (& cgroup_mutex ); <nl> + mutex_unlock (& inode -> i_mutex ); <nl> } <nl>  <nl> simple_set_mnt ( mnt , sb );
gen6_read ## x ( struct drm_i915_private * dev_priv , off_t reg , bool trace ) { \ <nl> static u ## x \ <nl> vlv_read ## x ( struct drm_i915_private * dev_priv , off_t reg , bool trace ) { \ <nl> unsigned fwengine = 0 ; \ <nl> - unsigned * fwcount = 0 ; \ <nl> + unsigned * fwcount ; \ <nl> REG_READ_HEADER ( x ); \ <nl> if ( FORCEWAKE_VLV_RENDER_RANGE_OFFSET ( reg )) { \ <nl> fwengine = FORCEWAKE_RENDER ; \
static struct ad5755_platform_data * ad5755_parse_dt ( struct device * dev ) <nl>  <nl> devnr = 0 ; <nl> for_each_child_of_node ( np , pp ) { <nl> - if ( devnr > AD5755_NUM_CHANNELS ) { <nl> + if ( devnr >= AD5755_NUM_CHANNELS ) { <nl> dev_err ( dev , <nl> " There is to many channels defined in DT \ n "); <nl> goto error_out ;
static struct dma_async_tx_descriptor * edma_prep_slave_sg ( <nl> edma_alloc_slot ( EDMA_CTLR ( echan -> ch_num ), <nl> EDMA_SLOT_ANY ); <nl> if ( echan -> slot [ i ] < 0 ) { <nl> + kfree ( edesc ); <nl> dev_err ( dev , " Failed to allocate slot \ n "); <nl> kfree ( edesc ); <nl> return NULL ;
int of_irq_map_one ( struct device_node * device , int index , struct of_irq * out_irq <nl> intsize = * tmp ; <nl>  <nl> /* Check index */ <nl> - if ( index * intsize >= intlen ) <nl> + if (( index + 1 ) * intsize > intlen ) <nl> return - EINVAL ; <nl>  <nl> /* Get new specifier and map it */
struct ulpi_info { <nl> /* ULPI hardcoded IDs , used for probing */ <nl> static struct ulpi_info ulpi_ids [] = { <nl> ULPI_INFO ( ULPI_ID ( 0x04cc , 0x1504 ), " NXP ISP1504 "), <nl> - ULPI_INFO ( ULPI_ID ( 0x0424 , 0x0006 ), " SMSC USB3319 "), <nl> + ULPI_INFO ( ULPI_ID ( 0x0424 , 0x0006 ), " SMSC USB331x "), <nl> }; <nl>  <nl> static int ulpi_set_otg_flags ( struct otg_transceiver * otg )
minstrel_aggr_check ( struct minstrel_priv * mp , struct ieee80211_sta * pubsta , stru <nl> if ( likely ( sta -> ampdu_mlme . tid_tx [ tid ])) <nl> return ; <nl>  <nl> + if ( skb_get_queue_mapping ( skb ) == IEEE80211_AC_VO ) <nl> + return ; <nl> + <nl> ieee80211_start_tx_ba_session ( pubsta , tid ); <nl> } <nl> 
static int __devinit bfin_lq035_probe ( struct platform_device * pdev ) <nl> i2c_add_driver (& ad5280_driver ); <nl>  <nl> memset (& props , 0 , sizeof ( props )); <nl> + props . type = BACKLIGHT_RAW ; <nl> props . max_brightness = MAX_BRIGHENESS ; <nl> bl_dev = backlight_device_register (" bf537 - bl ", NULL , NULL , <nl> & bfin_lq035fb_bl_ops , & props );
static struct cphy * my3126_phy_create ( adapter_t * adapter , <nl> { <nl> struct cphy * cphy = kzalloc ( sizeof (* cphy ), GFP_KERNEL ); <nl>  <nl> - if ( cphy ) <nl> - cphy_init ( cphy , adapter , phy_addr , & my3126_ops , mdio_ops ); <nl> + if (! cphy ) <nl> + return NULL ; <nl>  <nl> + cphy_init ( cphy , adapter , phy_addr , & my3126_ops , mdio_ops ); <nl> INIT_DELAYED_WORK (& cphy -> phy_update , my3216_poll ); <nl> cphy -> bmsr = 0 ; <nl> 
static int btrfs_parse_early_options ( const char * options , fmode_t flags , <nl> token = match_token ( p , tokens , args ); <nl> switch ( token ) { <nl> case Opt_subvol : <nl> + kfree (* subvol_name ); <nl> * subvol_name = match_strdup (& args [ 0 ]); <nl> break ; <nl> case Opt_subvolid :
static int rename_volumes ( struct ubi_device * ubi , <nl> req -> ents [ i ]. name [ req -> ents [ i ]. name_len ] = '\ 0 '; <nl> n = strlen ( req -> ents [ i ]. name ); <nl> if ( n != req -> ents [ i ]. name_len ) <nl> - err = - EINVAL ; <nl> + return - EINVAL ; <nl> } <nl>  <nl> /* Make sure volume IDs and names are unique */
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> b43_set_phytxctl_defaults ( dev ); <nl>  <nl> /* Minimum Contention Window */ <nl> - if ( phy -> type == B43_PHYTYPE_B ) { <nl> + if ( phy -> type == B43_PHYTYPE_B ) <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0x1F ); <nl> - } else { <nl> + else <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0xF ); <nl> - } <nl> /* Maximum Contention Window */ <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MAXCONT , 0x3FF ); <nl> 
int dm_split_args ( int * argc , char *** argvp , char * input ) <nl> unsigned array_size = 0 ; <nl>  <nl> * argc = 0 ; <nl> + <nl> + if (! input ) { <nl> + * argvp = NULL ; <nl> + return 0 ; <nl> + } <nl> + <nl> argv = realloc_argv (& array_size , argv ); <nl> if (! argv ) <nl> return - ENOMEM ;
safe_sig_queue_validate ( struct signal_queue_header * psafe_sqh , <nl> struct signal_queue_header * punsafe_sqh , <nl> u32 * phead , u32 * ptail ) <nl> { <nl> - if ((* phead >= psafe_sqh -> max_slots ) <nl> - || (* ptail >= psafe_sqh -> max_slots )) { <nl> + if ((* phead >= psafe_sqh -> max_slots ) || <nl> + (* ptail >= psafe_sqh -> max_slots )) { <nl> /* Choose 0 or max , maybe based on current tail value */ <nl> * phead = 0 ; <nl> * ptail = 0 ;
static ssize_t tun_chr_aio_read ( struct kiocb * iocb , const struct iovec * iv , <nl> ret = tun_do_read ( tun , tfile , iocb , iv , len , <nl> file -> f_flags & O_NONBLOCK ); <nl> ret = min_t ( ssize_t , ret , len ); <nl> + if ( ret > 0 ) <nl> + iocb -> ki_pos = ret ; <nl> out : <nl> tun_put ( tun ); <nl> return ret ;
static int __vhost_add_used_n ( struct vhost_virtqueue * vq , <nl>  <nl> start = vq -> last_used_idx % vq -> num ; <nl> used = vq -> used -> ring + start ; <nl> - if ( copy_to_user ( used , heads , count * sizeof * used )) { <nl> + if ( __copy_to_user ( used , heads , count * sizeof * used )) { <nl> vq_err ( vq , " Failed to write used "); <nl> return - EFAULT ; <nl> }
static int falcon_mtd_probe ( struct efx_nic * efx ) <nl>  <nl> /* Allocate space for maximum number of partitions */ <nl> parts = kcalloc ( 2 , sizeof (* parts ), GFP_KERNEL ); <nl> + if (! parts ) <nl> + return - ENOMEM ; <nl> n_parts = 0 ; <nl>  <nl> spi = & nic_data -> spi_flash ;
static struct tx_agg * r8152_get_tx_agg ( struct r8152 * tp ) <nl> struct tx_agg * agg = NULL ; <nl> unsigned long flags ; <nl>  <nl> + if ( list_empty (& tp -> tx_free )) <nl> + return NULL ; <nl> + <nl> spin_lock_irqsave (& tp -> tx_lock , flags ); <nl> if (! list_empty (& tp -> tx_free )) { <nl> struct list_head * cursor ;
static void napi_reuse_skb ( struct napi_struct * napi , struct sk_buff * skb ) <nl> __skb_pull ( skb , skb_headlen ( skb )); <nl> skb_reserve ( skb , NET_IP_ALIGN - skb_headroom ( skb )); <nl> skb -> vlan_tci = 0 ; <nl> + skb -> dev = napi -> dev ; <nl>  <nl> napi -> skb = skb ; <nl> }
void __init kmem_cache_init ( void ) <nl> /* Allocate two kmem_caches from the page allocator */ <nl> kmalloc_size = ALIGN ( kmem_size , cache_line_size ()); <nl> order = get_order ( 2 * kmalloc_size ); <nl> - kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT , order ); <nl> + kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT | __GFP_ZERO , order ); <nl>  <nl> /* <nl> * Must first have the slab cache available for the allocations of the
int vfs_quota_on_mount ( struct super_block * sb , char * qf_name , <nl> struct dentry * dentry ; <nl> int error ; <nl>  <nl> + mutex_lock (& sb -> s_root -> d_inode -> i_mutex ); <nl> dentry = lookup_one_len ( qf_name , sb -> s_root , strlen ( qf_name )); <nl> + mutex_unlock (& sb -> s_root -> d_inode -> i_mutex ); <nl> if ( IS_ERR ( dentry )) <nl> return PTR_ERR ( dentry ); <nl> 
static u32 * vgic_bytemap_get_reg ( struct vgic_bytemap * x , int cpuid , u32 offset ) <nl> { <nl> offset >>= 2 ; <nl> BUG_ON ( offset > ( VGIC_NR_IRQS / 4 )); <nl> - if ( offset < 4 ) <nl> + if ( offset < 8 ) <nl> return x -> percpu [ cpuid ] + offset ; <nl> else <nl> return x -> shared + offset - 8 ;
static int log_one_block ( struct log_writes_c * lc , <nl> goto out ; <nl> sector ++; <nl>  <nl> + atomic_inc (& lc -> io_blocks ); <nl> bio = bio_alloc ( GFP_KERNEL , block -> vec_cnt ); <nl> if (! bio ) { <nl> DMERR (" Couldn ' t alloc log bio "); <nl> goto error ; <nl> } <nl> - atomic_inc (& lc -> io_blocks ); <nl> bio -> bi_iter . bi_size = 0 ; <nl> bio -> bi_iter . bi_sector = sector ; <nl> bio -> bi_bdev = lc -> logdev -> bdev ;
static int elm_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> pm_runtime_enable (& pdev -> dev ); <nl> - if ( pm_runtime_get_sync (& pdev -> dev )) { <nl> + if ( pm_runtime_get_sync (& pdev -> dev ) < 0 ) { <nl> ret = - EINVAL ; <nl> pm_runtime_disable (& pdev -> dev ); <nl> dev_err (& pdev -> dev , " can ' t enable clock \ n ");
static void dwc2_hc_set_even_odd_frame ( struct dwc2_hsotg * hsotg , <nl> if ( chan -> ep_type == USB_ENDPOINT_XFER_INT || <nl> chan -> ep_type == USB_ENDPOINT_XFER_ISOC ) { <nl> /* 1 if _next_ frame is odd , 0 if it ' s even */ <nl> - if ( dwc2_hcd_get_frame_number ( hsotg ) & 0x1 ) <nl> + if (!( dwc2_hcd_get_frame_number ( hsotg ) & 0x1 )) <nl> * hcchar |= HCCHAR_ODDFRM ; <nl> } <nl> }
static int ade7854_set_irq ( struct device * dev , bool enable ) <nl> else <nl> irqen &= ~ BIT ( 17 ); <nl>  <nl> - ret = st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> - <nl> - return ret ; <nl> + return st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> } <nl>  <nl> static int ade7854_initial_setup ( struct iio_dev * indio_dev )
static int __ext4_ext_check ( const char * function , unsigned int line , <nl> error_msg = " invalid extent entries "; <nl> goto corrupted ; <nl> } <nl> + if ( unlikely ( depth > 32 )) { <nl> + error_msg = " too large eh_depth "; <nl> + goto corrupted ; <nl> + } <nl> /* Verify checksum on non - root extent tree nodes */ <nl> if ( ext_depth ( inode ) != depth && <nl> ! ext4_extent_block_csum_verify ( inode , eh )) {
static void add_new_bitmap ( struct btrfs_block_group_cache * block_group , <nl> BUG_ON ( block_group -> total_bitmaps >= max_bitmaps ); <nl>  <nl> info -> offset = offset_to_bitmap ( block_group , offset ); <nl> + info -> bytes = 0 ; <nl> link_free_space ( block_group , info ); <nl> block_group -> total_bitmaps ++; <nl> 
static int mxsfb_attach_endpoint ( struct drm_device * drm , <nl>  <nl> int mxsfb_create_output ( struct drm_device * drm ) <nl> { <nl> + struct mxsfb_drm_private * mxsfb = drm -> dev_private ; <nl> struct device_node * ep_np = NULL ; <nl> struct of_endpoint ep ; <nl> int ret ; <nl> int mxsfb_create_output ( struct drm_device * drm ) <nl> } <nl> } <nl>  <nl> + if (! mxsfb -> panel ) <nl> + return - EPROBE_DEFER ; <nl> + <nl> return 0 ; <nl> }
static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
static int child_wait_callback ( wait_queue_t * wait , unsigned mode , <nl> if (! eligible_child ( wo , p )) <nl> return 0 ; <nl>  <nl> + if (( wo -> wo_flags & __WNOTHREAD ) && wait -> private != p -> parent ) <nl> + return 0 ; <nl> + <nl> return default_wake_function ( wait , mode , sync , key ); <nl> } <nl> 
static int rtd520_probe_fifo_depth ( comedi_device * dev ) <nl> return - EIO ; <nl> } <nl> RtdAdcClearFifo ( dev ); <nl> - if ( fifo_size != 0x400 || fifo_size != 0x2000 ) <nl> + if ( fifo_size != 0x400 && fifo_size != 0x2000 ) <nl> { <nl> rt_printk ("\ ncomedi : % s : unexpected fifo size of % i , expected 1024 or 8192 .\ n ", <nl> DRV_NAME , fifo_size );
static int __init s3c2410fb_probe ( struct platform_device * pdev ) <nl>  <nl> info = fbinfo -> par ; <nl> info -> fb = fbinfo ; <nl> + info -> dev = & pdev -> dev ; <nl> + <nl> platform_set_drvdata ( pdev , fbinfo ); <nl>  <nl> dprintk (" devinit \ n ");
static void passdown_endio ( struct bio * bio ) <nl> * to unmap ( we ignore err ). <nl> */ <nl> queue_passdown_pt2 ( bio -> bi_private ); <nl> + bio_put ( bio ); <nl> } <nl>  <nl> static void process_prepared_discard_passdown_pt1 ( struct dm_thin_new_mapping * m )
static int xhci_plat_probe ( struct platform_device * pdev ) <nl> ret = clk_prepare_enable ( clk ); <nl> if ( ret ) <nl> goto put_hcd ; <nl> + } else if ( PTR_ERR ( clk ) == - EPROBE_DEFER ) { <nl> + ret = - EPROBE_DEFER ; <nl> + goto put_hcd ; <nl> } <nl>  <nl> xhci = hcd_to_xhci ( hcd );
void sun4c_update_mmu_cache ( struct vm_area_struct * vma , unsigned long address , p <nl> unsigned long flags ; <nl> int pseg ; <nl>  <nl> + if ( vma -> vm_mm -> context == NO_CONTEXT ) <nl> + return ; <nl> + <nl> local_irq_save ( flags ); <nl> address &= PAGE_MASK ; <nl> if (( pseg = sun4c_get_segmap ( address )) == invalid_segment ) {
static int ohci_init ( struct ohci_hcd * ohci ) <nl> return 0 ; <nl>  <nl> ohci -> hcca = dma_alloc_coherent ( hcd -> self . controller , <nl> - sizeof * ohci -> hcca , & ohci -> hcca_dma , 0 ); <nl> + sizeof (* ohci -> hcca ), & ohci -> hcca_dma , GFP_KERNEL ); <nl> if (! ohci -> hcca ) <nl> return - ENOMEM ; <nl> 
int activate_fd ( int irq , int fd , int type , void * dev_id ) <nl> . events = events , <nl> . current_events = 0 } ); <nl>  <nl> + err = - EBUSY ; <nl> spin_lock_irqsave (& irq_lock , flags ); <nl> for ( irq_fd = active_fds ; irq_fd != NULL ; irq_fd = irq_fd -> next ) { <nl> if (( irq_fd -> fd == fd ) && ( irq_fd -> type == type )) {
static enum dma_status rcar_dmac_tx_status ( struct dma_chan * chan , <nl> residue = rcar_dmac_chan_get_residue ( rchan , cookie ); <nl> spin_unlock_irqrestore (& rchan -> lock , flags ); <nl>  <nl> + /* if there ' s no residue , the cookie is complete */ <nl> + if (! residue ) <nl> + return DMA_COMPLETE ; <nl> + <nl> dma_set_residue ( txstate , residue ); <nl>  <nl> return status ;
static void free_pcppages_bulk ( struct zone * zone , int count , <nl> list = & pcp -> lists [ migratetype ]; <nl> } while ( list_empty ( list )); <nl>  <nl> + /* This is the only non - empty list . Free them all . */ <nl> + if ( batch_free == MIGRATE_PCPTYPES ) <nl> + batch_free = to_free ; <nl> + <nl> do { <nl> page = list_entry ( list -> prev , struct page , lru ); <nl> /* must delete as __free_one_page list manipulates */
static int xl_open ( struct net_device * dev ) <nl> if ( i == 0 ) { <nl> printk ( KERN_WARNING "% s : Not enough memory to allocate rx buffers . Adapter disabled \ n ", dev -> name ) ; <nl> free_irq ( dev -> irq , dev ) ; <nl> + kfree ( xl_priv -> xl_tx_ring ); <nl> + kfree ( xl_priv -> xl_rx_ring ); <nl> return - EIO ; <nl> } <nl> 
static int __do_request ( struct ceph_mds_client * mdsc , <nl> int mds = - 1 ; <nl> int err = - EAGAIN ; <nl>  <nl> - if ( req -> r_err || req -> r_got_result ) <nl> + if ( req -> r_err || req -> r_got_result ) { <nl> + if ( req -> r_aborted ) <nl> + __unregister_request ( mdsc , req ); <nl> goto out ; <nl> + } <nl>  <nl> if ( req -> r_timeout && <nl> time_after_eq ( jiffies , req -> r_started + req -> r_timeout )) {
static int usbvision_v4l2_close ( struct file * file ) <nl> usbvision_scratch_free ( usbvision ); <nl>  <nl> usbvision -> user --; <nl> + mutex_unlock (& usbvision -> v4l2_lock ); <nl>  <nl> if ( usbvision -> remove_pending ) { <nl> printk ( KERN_INFO "% s : Final disconnect \ n ", __func__ ); <nl> usbvision_release ( usbvision ); <nl> return 0 ; <nl> } <nl> - mutex_unlock (& usbvision -> v4l2_lock ); <nl>  <nl> PDEBUG ( DBG_IO , " success "); <nl> return v4l2_fh_release ( file );
static int set_wep_key ( struct airo_info * ai , u16 index , const char * key , <nl> WepKeyRid wkr ; <nl> int rc ; <nl>  <nl> - WARN_ON ( keylen == 0 ); <nl> + if ( WARN_ON ( keylen == 0 )) <nl> + return - 1 ; <nl>  <nl> memset (& wkr , 0 , sizeof ( wkr )); <nl> wkr . len = cpu_to_le16 ( sizeof ( wkr ));
static void clear_subscriber_list ( struct snd_seq_client * client , <nl> list_del (& subs -> dest_list ); <nl> else <nl> list_del (& subs -> src_list ); <nl> + up_write (& agrp -> list_mutex ); <nl> unsubscribe_port ( c , aport , agrp , & subs -> info , 1 ); <nl> kfree ( subs ); <nl> - up_write (& agrp -> list_mutex ); <nl> snd_seq_port_unlock ( aport ); <nl> snd_seq_client_unlock ( c ); <nl> }
static int __init net_ns_init ( void ) <nl>  <nl> register_pernet_subsys (& net_ns_ops ); <nl>  <nl> - rtnl_register ( PF_UNSPEC , RTM_NEWNSID , rtnl_net_newid , NULL , 0 ); <nl> + rtnl_register ( PF_UNSPEC , RTM_NEWNSID , rtnl_net_newid , NULL , <nl> + RTNL_FLAG_DOIT_UNLOCKED ); <nl> rtnl_register ( PF_UNSPEC , RTM_GETNSID , rtnl_net_getid , rtnl_net_dumpid , <nl> - 0 ); <nl> + RTNL_FLAG_DOIT_UNLOCKED ); <nl>  <nl> return 0 ; <nl> }
static int sof_set_get_large_ctrl_data ( struct snd_sof_dev * sdev , <nl> else <nl> err = sof_get_ctrl_copy_params ( cdata -> type , partdata , cdata , <nl> sparams ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree ( partdata ); <nl> return err ; <nl> + } <nl>  <nl> msg_bytes = sparams -> msg_bytes ; <nl> pl_size = sparams -> pl_size ;
ath5k_intr ( int irq , void * dev_id ) <nl> tasklet_schedule (& sc -> restq ); <nl> } else { <nl> if ( status & AR5K_INT_SWBA ) { <nl> - tasklet_schedule (& sc -> beacontq ); <nl> + tasklet_hi_schedule (& sc -> beacontq ); <nl> } <nl> if ( status & AR5K_INT_RXEOL ) { <nl> /*
# define CR8_RESERVED_BITS (~( unsigned long ) X86_CR8_TPR ) <nl>  <nl> # define KVM_MAX_MCE_BANKS 32 <nl> -# define KVM_MCE_CAP_SUPPORTED MCG_CTL_P <nl> +# define KVM_MCE_CAP_SUPPORTED ( MCG_CTL_P | MCG_SER_P ) <nl>  <nl> /* EFER defaults : <nl> * - enable syscall per default because its emulated by KVM
static int svm_check_intercept ( struct kvm_vcpu * vcpu , <nl> if ( info -> intercept == x86_intercept_in || <nl> info -> intercept == x86_intercept_ins ) { <nl> exit_info |= SVM_IOIO_TYPE_MASK ; <nl> - bytes = info -> src_bytes ; <nl> - } else { <nl> bytes = info -> dst_bytes ; <nl> + } else { <nl> + bytes = info -> src_bytes ; <nl> } <nl>  <nl> if ( info -> intercept == x86_intercept_outs ||
int fb_find_mode ( struct fb_var_screeninfo * var , <nl> "", ( margins ) ? " with margins " : "", ( interlace ) ? <nl> " interlaced " : ""); <nl>  <nl> + memset (& cvt_mode , 0 , sizeof ( cvt_mode )); <nl> cvt_mode . xres = xres ; <nl> cvt_mode . yres = yres ; <nl> cvt_mode . refresh = ( refresh ) ? refresh : 60 ;
static int pcnet32_phys_id ( struct net_device * dev , u32 data ) <nl> if ((! data ) || ( data > ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ))) <nl> data = ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ); <nl>  <nl> - schedule_timeout ( data * HZ ); <nl> + msleep_interruptible ( data * 1000 ); <nl> del_timer_sync (& lp -> blink_timer ); <nl>  <nl> /* Restore the original value of the bcrs */
static ssize_t sta_ht_capa_read ( struct file * file , char __user * userbuf , <nl> if ( _cond ) \ <nl> p += scnprintf ( p , sizeof ( buf )+ buf - p , "\ t " _str "\ n "); \ <nl> } while ( 0 ) <nl> - char buf [ 1024 ], * p = buf ; <nl> + char buf [ 512 ], * p = buf ; <nl> int i ; <nl> struct sta_info * sta = file -> private_data ; <nl> struct ieee80211_sta_ht_cap * htc = & sta -> sta . ht_cap ;
int omap3isp_csiphy_acquire ( struct isp_csiphy * phy ) <nl> if ( rval < 0 ) <nl> goto done ; <nl>  <nl> - omap3isp_csi2_reset ( phy -> csi2 ); <nl> + rval = omap3isp_csi2_reset ( phy -> csi2 ); <nl> + if ( rval < 0 ) <nl> + goto done ; <nl>  <nl> csiphy_dphy_config ( phy ); <nl> csiphy_lanes_config ( phy );
static const char * intercon [][ 3 ] = { <nl> {" HPRCOM ", NULL , " Right HP Com "}, <nl>  <nl> /* Mono Output */ <nl> - {" MONOLOUT ", NULL , " Mono Out "}, <nl> - {" MONOLOUT ", NULL , " Mono Out "}, <nl> + {" MONO_LOUT ", NULL , " Mono Out "}, <nl> + {" MONO_LOUT ", NULL , " Mono Out "}, <nl>  <nl> /* Left Input */ <nl> {" Left Line1L Mux ", " single - ended ", " LINE1L "},
static int musb_urb_enqueue ( <nl> * we only have work to do in the former case . <nl> */ <nl> spin_lock_irqsave (& musb -> lock , flags ); <nl> - if ( hep -> hcpriv ) { <nl> + if ( hep -> hcpriv || ! next_urb ( qh )) { <nl> /* some concurrent activity submitted another urb to hep ... <nl> * odd , rare , error prone , but legal . <nl> */
mwifiex_cmd_802_11_ad_hoc_start ( struct mwifiex_private * priv , <nl>  <nl> memset ( adhoc_start -> ssid , 0 , IEEE80211_MAX_SSID_LEN ); <nl>  <nl> + if ( req_ssid -> ssid_len > IEEE80211_MAX_SSID_LEN ) <nl> + req_ssid -> ssid_len = IEEE80211_MAX_SSID_LEN ; <nl> memcpy ( adhoc_start -> ssid , req_ssid -> ssid , req_ssid -> ssid_len ); <nl>  <nl> mwifiex_dbg ( adapter , INFO , " info : ADHOC_S_CMD : SSID = % s \ n ",
static void __devinit dfx_bus_init ( struct net_device * dev ) <nl> * Interrupts are disabled at the adapter bus - specific logic . <nl> */ <nl>  <nl> - static void __devinit dfx_bus_uninit ( struct net_device * dev ) <nl> + static void __devexit dfx_bus_uninit ( struct net_device * dev ) <nl> { <nl> DFX_board_t * bp = netdev_priv ( dev ); <nl> struct device * bdev = bp -> bus_dev ;
static unsigned char GetXG27FPBits ( struct vb_device_info * pVBInfo ) <nl> /* enable GPIOA / B / C read */ <nl> xgifb_reg_and_or ( pVBInfo -> P3d4 , 0x4A , ~ 0x03 , 0x03 ); <nl> temp = xgifb_reg_get ( pVBInfo -> P3d4 , 0x48 ); <nl> - if ( temp <= 2 ) <nl> - temp &= 0x03 ; <nl> - else <nl> + if ( temp > 2 ) <nl> temp = (( temp & 0x04 ) >> 1 ) | ((~ temp ) & 0x01 ); <nl>  <nl> xgifb_reg_set ( pVBInfo -> P3d4 , 0x4A , CR4A );
static int mwifiex_pcie_event_complete ( struct mwifiex_adapter * adapter , <nl>  <nl> if (! card -> evt_buf_list [ rdptr ]) { <nl> skb_push ( skb , INTF_HEADER_LEN ); <nl> + skb_put ( skb , MAX_EVENT_SIZE - skb -> len ); <nl> + memset ( skb -> data , 0 , MAX_EVENT_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , <nl> MAX_EVENT_SIZE , <nl> PCI_DMA_FROMDEVICE ))
void ieee80211_csa_finalize_work ( struct work_struct * work ) <nl> container_of ( work , struct ieee80211_sub_if_data , <nl> csa_finalize_work ); <nl> struct ieee80211_local * local = sdata -> local ; <nl> - int err , changed ; <nl> + int err , changed = 0 ; <nl>  <nl> if (! ieee80211_sdata_running ( sdata )) <nl> return ;
static void m_stop ( struct seq_file * m , void * v ) <nl> struct proc_maps_private * priv = m -> private ; <nl> struct vm_area_struct * vma = v ; <nl>  <nl> - vma_stop ( priv , vma ); <nl> + if (! IS_ERR ( vma )) <nl> + vma_stop ( priv , vma ); <nl> if ( priv -> task ) <nl> put_task_struct ( priv -> task ); <nl> }
void hpsb_packet_received ( struct hpsb_host * host , quadlet_t * data , size_t size , <nl> /* return the index ( within a minor number block ) of a file */ <nl> static inline unsigned char ieee1394_file_to_instance ( struct file * file ) <nl> { <nl> - return file -> f_dentry -> d_inode -> i_cindex ; <nl> + return file -> f_path . dentry -> d_inode -> i_cindex ; <nl> } <nl>  <nl> extern int hpsb_disable_irm ;
int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> cur_devices -> num_devices --; <nl> cur_devices -> total_devices --; <nl> + /* Update total_devices of the parent fs_devices if it ' s seed */ <nl> + if ( cur_devices != fs_devices ) <nl> + fs_devices -> total_devices --; <nl>  <nl> if ( test_bit ( BTRFS_DEV_STATE_MISSING , & device -> dev_state )) <nl> cur_devices -> missing_devices --;
static int remove_and_add_spares ( struct mddev * mddev , <nl> ! test_bit ( Bitmap_sync , & rdev -> flags ))) <nl> continue ; <nl>  <nl> - rdev -> recovery_offset = 0 ; <nl> + if ( rdev -> saved_raid_disk < 0 ) <nl> + rdev -> recovery_offset = 0 ; <nl> if ( mddev -> pers -> <nl> hot_add_disk ( mddev , rdev ) == 0 ) { <nl> if ( sysfs_link_rdev ( mddev , rdev ))
void usb_serial_generic_resubmit_read_urb ( struct usb_serial_port * port , <nl> (( serial -> type -> read_bulk_callback ) ? <nl> serial -> type -> read_bulk_callback : <nl> usb_serial_generic_read_bulk_callback ), port ); <nl> + <nl> result = usb_submit_urb ( urb , mem_flags ); <nl> - if ( result ) <nl> + if ( result && result != - EPERM ) { <nl> dev_err (& port -> dev , <nl> "% s - failed resubmitting read urb , error % d \ n ", <nl> __func__ , result ); <nl> + } <nl> } <nl> EXPORT_SYMBOL_GPL ( usb_serial_generic_resubmit_read_urb ); <nl> 
static int sym_prepare_setting ( struct Scsi_Host * shost , struct sym_hcb * np , stru <nl>  <nl> tp -> usrflags |= ( SYM_DISC_ENABLED | SYM_TAGS_ENABLED ); <nl> tp -> usrtags = SYM_SETUP_MAX_TAG ; <nl> + tp -> usr_width = np -> maxwide ; <nl> + tp -> usr_period = 9 ; <nl>  <nl> sym_nvram_setup_target ( tp , i , nvram ); <nl> 
extern struct debug_obj_descr rcuhead_debug_descr ; <nl>  <nl> static inline void debug_rcu_head_queue ( struct rcu_head * head ) <nl> { <nl> + WARN_ON_ONCE (( unsigned long ) head & 0x3 ); <nl> debug_object_activate ( head , & rcuhead_debug_descr ); <nl> debug_object_active_state ( head , & rcuhead_debug_descr , <nl> STATE_RCU_HEAD_READY ,
static struct btrfs_trans_handle * start_transaction ( struct btrfs_root * root , <nl> */ <nl> if ( type != TRANS_JOIN_NOLOCK && <nl> ! __sb_start_write ( root -> fs_info -> sb , SB_FREEZE_FS , false )) { <nl> - if ( type == TRANS_JOIN_FREEZE ) <nl> + if ( type == TRANS_JOIN_FREEZE ) { <nl> + kmem_cache_free ( btrfs_trans_handle_cachep , h ); <nl> return ERR_PTR (- EPERM ); <nl> + } <nl> sb_start_intwrite ( root -> fs_info -> sb ); <nl> } <nl> 
static struct proc_dir_entry * proc_sn2_ptc ; <nl>  <nl> static int __init sn2_ptc_init ( void ) <nl> { <nl> + if (! ia64_platform_is (" sn2 ")) <nl> + return - ENOSYS ; <nl> + <nl> if (!( proc_sn2_ptc = create_proc_entry ( PTC_BASENAME , 0444 , NULL ))) { <nl> printk ( KERN_ERR " unable to create % s proc entry ", PTC_BASENAME ); <nl> return - EINVAL ;
static void cciss_geometry_inquiry ( int ctlr , int logvol , <nl> " does not support reading geometry \ n "); <nl> drv -> heads = 255 ; <nl> drv -> sectors = 32 ; // Sectors per track <nl> + drv -> raid_level = RAID_UNKNOWN ; <nl> } else { <nl> drv -> heads = inq_buff -> data_byte [ 6 ]; <nl> drv -> sectors = inq_buff -> data_byte [ 7 ];
static void __split_and_process_bio ( struct mapped_device * md , <nl> } <nl>  <nl> /* drop the extra reference count */ <nl> - dec_pending ( ci . io , error ); <nl> + dec_pending ( ci . io , errno_to_blk_status ( error )); <nl> } <nl> /*----------------------------------------------------------------- <nl> * CRUD END
static int ascot2e_write_regs ( struct ascot2e_priv * priv , <nl> } <nl> }; <nl>  <nl> - if ( len + 1 >= sizeof ( buf )) { <nl> + if ( len + 1 > sizeof ( buf )) { <nl> dev_warn (& priv -> i2c -> dev ," wr reg =% 04x : len =% d is too big !\ n ", <nl> reg , len + 1 ); <nl> return - E2BIG ;
int ext4_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> /* ( user + group )*( old + new ) structure , inode write ( sb , <nl> * inode block , ? - but truncate inode update has it ) */ <nl> handle = ext4_journal_start ( inode , ( EXT4_MAXQUOTAS_INIT_BLOCKS ( inode -> i_sb )+ <nl> - EXT4_QUOTA_DEL_BLOCKS ( inode -> i_sb ))+ 3 ); <nl> + EXT4_MAXQUOTAS_DEL_BLOCKS ( inode -> i_sb ))+ 3 ); <nl> if ( IS_ERR ( handle )) { <nl> error = PTR_ERR ( handle ); <nl> goto err_out ;
static int snd_ctl_elem_add ( struct snd_ctl_file * file , <nl> if ( ue == NULL ) <nl> return - ENOMEM ; <nl> ue -> info = * info ; <nl> + ue -> info . access = 0 ; <nl> ue -> elem_data = ( char *) ue + sizeof (* ue ); <nl> ue -> elem_data_size = private_size ; <nl> kctl . private_free = snd_ctl_elem_user_free ;
static inline void __fsnotify_update_dcache_flags ( struct dentry * dentry ) <nl> assert_spin_locked (& dentry -> d_lock ); <nl>  <nl> parent = dentry -> d_parent ; <nl> - if ( fsnotify_inode_watches_children ( parent -> d_inode )) <nl> + if ( parent -> d_inode && fsnotify_inode_watches_children ( parent -> d_inode )) <nl> dentry -> d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED ; <nl> else <nl> dentry -> d_flags &= ~ DCACHE_FSNOTIFY_PARENT_WATCHED ;
static void __orinoco_ev_info ( struct net_device * dev , hermes_t * hw ) <nl> /* Read scan data */ <nl> err = hermes_bap_pread ( hw , IRQ_BAP , ( void *) buf , len , <nl> infofid , sizeof ( info )); <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree ( buf ); <nl> break ; <nl> + } <nl>  <nl> # ifdef ORINOCO_DEBUG <nl> {
int tipc_nl_publ_dump ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if (! attrs [ TIPC_NLA_SOCK ]) <nl> + return - EINVAL ; <nl> + <nl> err = nla_parse_nested ( sock , TIPC_NLA_SOCK_MAX , <nl> attrs [ TIPC_NLA_SOCK ], <nl> tipc_nl_sock_policy );
static void __ip_rt_update_pmtu ( struct rtable * rt , struct flowi4 * fl4 , u32 mtu ) <nl> if ( mtu < ip_rt_min_pmtu ) <nl> mtu = ip_rt_min_pmtu ; <nl>  <nl> + if ( rt -> rt_pmtu == mtu && <nl> + time_before ( jiffies , dst -> expires - ip_rt_mtu_expires / 2 )) <nl> + return ; <nl> + <nl> rcu_read_lock (); <nl> if ( fib_lookup ( dev_net ( dst -> dev ), fl4 , & res ) == 0 ) { <nl> struct fib_nh * nh = & FIB_RES_NH ( res );
static int complete_emulated_mmio ( struct kvm_vcpu * vcpu ) <nl> frag -> len -= len ; <nl> } <nl>  <nl> - if ( vcpu -> mmio_cur_fragment == vcpu -> mmio_nr_fragments ) { <nl> + if ( vcpu -> mmio_cur_fragment >= vcpu -> mmio_nr_fragments ) { <nl> vcpu -> mmio_needed = 0 ; <nl>  <nl> /* FIXME : return into emulator if single - stepping . */
static void spear_smi_hw_init ( struct spear_smi * dev ) <nl> val = HOLD1 | BANK_EN | DSEL_TIME | ( prescale << 8 ); <nl>  <nl> mutex_lock (& dev -> lock ); <nl> + /* clear all interrupt conditions */ <nl> + writel ( 0 , dev -> io_base + SMI_SR ); <nl> + <nl> writel ( val , dev -> io_base + SMI_CR1 ); <nl> mutex_unlock (& dev -> lock ); <nl> }
static void __wl1271_op_remove_interface ( struct wl1271 * wl , <nl> wl1271_free_ap_keys ( wl , wlvif ); <nl> } <nl>  <nl> + dev_kfree_skb ( wlvif -> probereq ); <nl> + wlvif -> probereq = NULL ; <nl> wl12xx_tx_reset_wlvif ( wl , wlvif ); <nl> if ( wl -> last_wlvif == wlvif ) <nl> wl -> last_wlvif = NULL ;
struct task_struct { <nl> int exit_state ; <nl> int exit_code , exit_signal ; <nl> int pdeath_signal ; /* The signal sent when the parent dies */ <nl> - unsigned int jobctl ; /* JOBCTL_ *, siglock protected */ <nl> + unsigned long jobctl ; /* JOBCTL_ *, siglock protected */ <nl>  <nl> /* Used for emulating ABI behavior of previous Linux versions */ <nl> unsigned int personality ;
int rtllib_rx ( struct rtllib_device * ieee , struct sk_buff * skb , <nl> return ret ; <nl>  <nl> rx_dropped : <nl> - ieee -> stats . rx_dropped ++; <nl> + if ( ieee ) <nl> + ieee -> stats . rx_dropped ++; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( rtllib_rx );
static netdev_tx_t ip6erspan_tunnel_xmit ( struct sk_buff * skb , <nl> truncate = true ; <nl> } <nl>  <nl> + if ( skb_cow_head ( skb , dev -> needed_headroom )) <nl> + goto tx_err ; <nl> + <nl> t -> parms . o_flags &= ~ TUNNEL_KEY ; <nl> IPCB ( skb )-> flags = 0 ; <nl> 
static int rtw_wx_set_enc_ext ( struct net_device * dev , <nl> memset ( param , 0 , param_len ); <nl>  <nl> param -> cmd = IEEE_CMD_SET_ENCRYPTION ; <nl> - memset ( param -> sta_addr , 0xff , ETH_ALEN ); <nl> + eth_broadcast_addr ( param -> sta_addr ); <nl>  <nl> switch ( pext -> alg ) { <nl> case IW_ENCODE_ALG_NONE :
static int find_fsync_dnodes ( struct f2fs_sb_info * sbi , struct list_head * head ) <nl> if ( IS_ERR ( entry -> inode )) { <nl> err = PTR_ERR ( entry -> inode ); <nl> kmem_cache_free ( fsync_entry_slab , entry ); <nl> - if ( err == - ENOENT ) <nl> + if ( err == - ENOENT ) { <nl> + err = 0 ; <nl> goto next ; <nl> + } <nl> break ; <nl> } <nl> list_add_tail (& entry -> list , head );
static int __floppy_read_block_0 ( struct block_device * bdev ) <nl> bio . bi_size = size ; <nl> bio . bi_bdev = bdev ; <nl> bio . bi_sector = 0 ; <nl> + bio . bi_flags = BIO_QUIET ; <nl> init_completion (& complete ); <nl> bio . bi_private = & complete ; <nl> bio . bi_end_io = floppy_rb0_complete ;
static int ext4_file_open ( struct inode * inode , struct file * filp ) <nl> path . dentry = mnt -> mnt_root ; <nl> cp = d_path (& path , buf , sizeof ( buf )); <nl> if (! IS_ERR ( cp )) { <nl> - memcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> - sizeof ( sbi -> s_es -> s_last_mounted )); <nl> + strlcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> + sizeof ( sbi -> s_es -> s_last_mounted )); <nl> ext4_mark_super_dirty ( sb ); <nl> } <nl> }
static int of_iommu_xlate ( struct device * dev , <nl> * a proper probe - ordering dependency mechanism in future . <nl> */ <nl> if (! ops ) <nl> - return - EPROBE_DEFER ; <nl> + return driver_deferred_probe_check_state ( dev ); <nl>  <nl> return ops -> of_xlate ( dev , iommu_spec ); <nl> }
void compact_pgdat ( pg_data_t * pgdat , int order ) <nl> . sync = false , <nl> }; <nl>  <nl> + if (! order ) <nl> + return ; <nl> + <nl> __compact_pgdat ( pgdat , & cc ); <nl> } <nl> 
get_futex_key ( u32 __user * uaddr , int fshared , union futex_key * key , int rw ) <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> + page = compound_head ( page ); <nl> lock_page ( page ); <nl> if (! page -> mapping ) { <nl> unlock_page ( page );
static void slic_card_cleanup ( struct sliccard * card ) <nl> { <nl> if ( card -> loadtimerset ) { <nl> card -> loadtimerset = 0 ; <nl> - del_timer (& card -> loadtimer ); <nl> + del_timer_sync (& card -> loadtimer ); <nl> } <nl>  <nl> slic_debug_card_destroy ( card );
int __cfs_fail_timeout_set ( __u32 id , __u32 value , int ms , int set ) <nl> int ret ; <nl>  <nl> ret = __cfs_fail_check_set ( id , value , set ); <nl> - if ( ret ) { <nl> + if ( ret && likely ( ms > 0 )) { <nl> CERROR (" cfs_fail_timeout id % x sleeping for % dms \ n ", <nl> id , ms ); <nl> set_current_state ( TASK_UNINTERRUPTIBLE );
void xgbe_debugfs_init ( struct xgbe_prv_data * pdata ) <nl> pdata -> xgbe_debugfs = debugfs_create_dir ( buf , NULL ); <nl> if (! pdata -> xgbe_debugfs ) { <nl> netdev_err ( pdata -> netdev , " debugfs_create_dir failed \ n "); <nl> + kfree ( buf ); <nl> return ; <nl> } <nl> 
static int uwbd ( void * param ) <nl> HZ ); <nl> if ( should_stop ) <nl> break ; <nl> - try_to_freeze (); <nl>  <nl> spin_lock_irqsave (& rc -> uwbd . event_list_lock , flags ); <nl> if (! list_empty (& rc -> uwbd . event_list )) {
int devm_request_irq ( struct device * dev , unsigned int irq , <nl>  <nl> rc = request_irq ( irq , handler , irqflags , devname , dev_id ); <nl> if ( rc ) { <nl> - kfree ( dr ); <nl> + devres_free ( dr ); <nl> return rc ; <nl> } <nl> 
void hostif_data_indication ( struct ks_wlan_private * priv ) <nl> { <nl> unsigned int rx_ind_size ; /* indicate data size */ <nl> struct sk_buff * skb ; <nl> - unsigned short auth_type ; <nl> + u16 auth_type ; <nl> unsigned char temp [ 256 ]; <nl> struct ether_hdr * eth_hdr ; <nl> unsigned short eth_proto ;
static struct page * get_partial ( struct kmem_cache * s , gfp_t flags , int node ) <nl> int searchnode = ( node == NUMA_NO_NODE ) ? numa_node_id () : node ; <nl>  <nl> page = get_partial_node ( get_node ( s , searchnode )); <nl> - if ( page || node != - 1 ) <nl> + if ( page || node != NUMA_NO_NODE ) <nl> return page ; <nl>  <nl> return get_any_partial ( s , flags );
nouveau_framebuffer_init ( struct drm_device * dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( nvbo -> tile_flags & NOUVEAU_GEM_TILE_NONCONTIG ) { <nl> + NV_ERROR ( drm , " framebuffer requires contiguous bo \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( nv_device ( drm -> device )-> chipset == 0x50 ) <nl> nv_fb -> r_format |= ( tile_flags << 8 ); <nl> 
getname_kernel ( const char * filename ) <nl> if ( len <= EMBEDDED_NAME_MAX ) { <nl> result -> name = ( char *) result -> iname ; <nl> } else if ( len <= PATH_MAX ) { <nl> + const size_t size = offsetof ( struct filename , iname [ 1 ]); <nl> struct filename * tmp ; <nl>  <nl> - tmp = kmalloc ( sizeof (* tmp ), GFP_KERNEL ); <nl> + tmp = kmalloc ( size , GFP_KERNEL ); <nl> if ( unlikely (! tmp )) { <nl> __putname ( result ); <nl> return ERR_PTR (- ENOMEM );
static void kick_requests ( struct ceph_osd_client * osdc , int force_resend ) <nl> __register_request ( osdc , req ); <nl> __unregister_linger_request ( osdc , req ); <nl> } <nl> + reset_changed_osds ( osdc ); <nl> mutex_unlock (& osdc -> request_mutex ); <nl>  <nl> if ( needmap ) { <nl> dout ("% d requests for down osds , need new map \ n ", needmap ); <nl> ceph_monc_request_next_osdmap (& osdc -> client -> monc ); <nl> } <nl> - reset_changed_osds ( osdc ); <nl> } <nl>  <nl> 
static int mcp3422_probe ( struct i2c_client * client , <nl> | MCP3422_CHANNEL_VALUE ( 0 ) <nl> | MCP3422_PGA_VALUE ( MCP3422_PGA_1 ) <nl> | MCP3422_SAMPLE_RATE_VALUE ( MCP3422_SRATE_240 )); <nl> - mcp3422_update_config ( adc , config ); <nl> + err = mcp3422_update_config ( adc , config ); <nl> + if ( err < 0 ) <nl> + return err ; <nl>  <nl> err = devm_iio_device_register (& client -> dev , indio_dev ); <nl> if ( err < 0 )
static int pxa3xx_u2d_probe ( struct platform_device * pdev ) <nl> struct resource * r ; <nl> int err ; <nl>  <nl> - u2d = kzalloc ( sizeof ( struct pxa3xx_u2d_ulpi ), GFP_KERNEL ); <nl> + u2d = kzalloc ( sizeof (* u2d ), GFP_KERNEL ); <nl> if (! u2d ) { <nl> dev_err (& pdev -> dev , " failed to allocate memory \ n "); <nl> return - ENOMEM ;
static int blan_mdlm_bind ( struct usbnet * dev , struct usb_interface * intf ) <nl> } <nl> /* expect bcdVersion 1 . 0 , ignore */ <nl> if ( memcmp (& desc -> bGUID , blan_guid , 16 ) <nl> - && memcmp (& desc -> bGUID , blan_guid , 16 ) ) { <nl> + && memcmp (& desc -> bGUID , safe_guid , 16 ) ) { <nl> /* hey , this one might _really_ be MDLM ! */ <nl> dev_dbg (& intf -> dev , " MDLM guid \ n "); <nl> goto bad_desc ;
static int sdio_read_cis ( struct mmc_card * card , struct sdio_func * func ) <nl> if ( tpl_code == 0xff ) <nl> break ; <nl>  <nl> + /* null entries have no link field or data */ <nl> + if ( tpl_code == 0x00 ) <nl> + continue ; <nl> + <nl> ret = mmc_io_rw_direct ( card , 0 , 0 , ptr ++, 0 , & tpl_link ); <nl> if ( ret ) <nl> break ;
# define MPT2SAS_DRIVER_NAME " mpt2sas " <nl> # define MPT2SAS_AUTHOR " LSI Corporation < DL - MPTFusionLinux @ lsi . com >" <nl> # define MPT2SAS_DESCRIPTION " LSI MPT Fusion SAS 2 . 0 Device Driver " <nl> -# define MPT2SAS_DRIVER_VERSION " 07 . 100 . 00 . 00 " <nl> -# define MPT2SAS_MAJOR_VERSION 07 <nl> +# define MPT2SAS_DRIVER_VERSION " 08 . 100 . 00 . 00 " <nl> +# define MPT2SAS_MAJOR_VERSION 08 <nl> # define MPT2SAS_MINOR_VERSION 100 <nl> # define MPT2SAS_BUILD_VERSION 00 <nl> # define MPT2SAS_RELEASE_VERSION 00
static int ubifs_get_sb ( struct file_system_type * fs_type , int flags , <nl> */ <nl> ubi = open_ubi ( name , UBI_READONLY ); <nl> if ( IS_ERR ( ubi )) { <nl> - ubifs_err (" cannot open \"% s \", error % d ", <nl> - name , ( int ) PTR_ERR ( ubi )); <nl> + dbg_err (" cannot open \"% s \", error % d ", <nl> + name , ( int ) PTR_ERR ( ubi )); <nl> return PTR_ERR ( ubi ); <nl> } <nl> ubi_get_volume_info ( ubi , & vi );
static int labpc_ai_cmd ( struct comedi_device * dev , struct comedi_subdevice * s ) <nl> devpriv -> write_byte ( INTERVAL_LOAD_BITS , <nl> dev -> iobase + INTERVAL_LOAD_REG ); <nl>  <nl> - if ( cmd -> convert_src == TRIG_TIMER || cmd -> scan_begin_src == TRIG_TIMER ) { <nl> + if ( cmd -> convert_src == TRIG_TIMER || <nl> + cmd -> scan_begin_src == TRIG_TIMER ) { <nl> /* set up pacing */ <nl> labpc_adc_timing ( dev , cmd , mode ); <nl> /* load counter b0 in mode 3 */
static void ssb_pmu_resources_init ( struct ssb_chipcommon * cc ) <nl>  <nl> switch ( bus -> chip_id ) { <nl> case 0x4312 : <nl> + min_msk = 0xCBB ; <nl> + break ; <nl> case 0x4322 : <nl> /* We keep the default settings : <nl> * min_msk = 0xCBB
static int soc_tplg_dapm_widget_create ( struct soc_tplg * tplg , <nl> widget -> dobj . type = SND_SOC_DOBJ_WIDGET ; <nl> widget -> dobj . ops = tplg -> ops ; <nl> widget -> dobj . index = tplg -> index ; <nl> + kfree ( template . sname ); <nl> + kfree ( template . name ); <nl> list_add (& widget -> dobj . list , & tplg -> comp -> dobj_list ); <nl> return 0 ; <nl> 
void rtsx_add_cmd ( struct rtsx_chip * chip , <nl> void rtsx_send_cmd_no_wait ( struct rtsx_chip * chip ); <nl> int rtsx_send_cmd ( struct rtsx_chip * chip , u8 card , int timeout ); <nl>  <nl> - extern inline u8 * rtsx_get_cmd_data ( struct rtsx_chip * chip ) <nl> + static inline u8 * rtsx_get_cmd_data ( struct rtsx_chip * chip ) <nl> { <nl> # ifdef CMD_USING_SG <nl> return ( u8 *)( chip -> host_sg_tbl_ptr );
static int ilk_compute_pipe_wm ( struct intel_crtc * intel_crtc , <nl> return PTR_ERR ( cstate ); <nl>  <nl> pipe_wm = & cstate -> wm . optimal . ilk ; <nl> + memset ( pipe_wm , 0 , sizeof (* pipe_wm )); <nl>  <nl> for_each_intel_plane_on_crtc ( dev , intel_crtc , intel_plane ) { <nl> ps = drm_atomic_get_plane_state ( state ,
static void cnl_display_core_init ( struct drm_i915_private * dev_priv , bool resume <nl>  <nl> /* 6 . Enable DBUF */ <nl> gen9_dbuf_enable ( dev_priv ); <nl> + <nl> + if ( resume && dev_priv -> csr . dmc_payload ) <nl> + intel_csr_load_program ( dev_priv ); <nl> } <nl>  <nl> # undef CNL_PROCMON_IDX
static int __devexit gpio_keys_remove ( struct platform_device * pdev ) <nl> } <nl>  <nl> input_unregister_device ( input ); <nl> + kfree ( ddata ); <nl>  <nl> return 0 ; <nl> }
static int tegra_output_hdmi_check_mode ( struct tegra_output * output , <nl> parent = clk_get_parent ( hdmi -> clk_parent ); <nl>  <nl> err = clk_round_rate ( parent , pclk * 4 ); <nl> - if ( err < 0 ) <nl> + if ( err <= 0 ) <nl> * status = MODE_NOCLOCK ; <nl> else <nl> * status = MODE_OK ;
static int do_mq_notify ( mqd_t mqdes , const struct sigevent * notification ) <nl>  <nl> timeo = MAX_SCHEDULE_TIMEOUT ; <nl> ret = netlink_attachskb ( sock , nc , & timeo , NULL ); <nl> - if ( ret == 1 ) <nl> + if ( ret == 1 ) { <nl> + sock = NULL ; <nl> goto retry ; <nl> + } <nl> if ( ret ) { <nl> sock = NULL ; <nl> nc = NULL ;
static void * skd_alloc_dma ( struct skd_device * skdev , struct kmem_cache * s , <nl> return NULL ; <nl> * dma_handle = dma_map_single ( dev , buf , s -> size , dir ); <nl> if ( dma_mapping_error ( dev , * dma_handle )) { <nl> - kfree ( buf ); <nl> + kmem_cache_free ( s , buf ); <nl> buf = NULL ; <nl> } <nl> return buf ;
static void nhmex_uncore_msr_enable_event ( struct intel_uncore_box * box , struct p <nl> { <nl> struct hw_perf_event * hwc = & event -> hw ; <nl>  <nl> - if ( hwc -> idx >= UNCORE_PMC_IDX_FIXED ) <nl> + if ( hwc -> idx == UNCORE_PMC_IDX_FIXED ) <nl> wrmsrl ( hwc -> config_base , NHMEX_PMON_CTL_EN_BIT0 ); <nl> else if ( box -> pmu -> type -> event_mask & NHMEX_PMON_CTL_EN_BIT0 ) <nl> wrmsrl ( hwc -> config_base , hwc -> config | NHMEX_PMON_CTL_EN_BIT22 );
static void pwmled_brightness ( struct led_classdev * cdev , enum led_brightness b ) <nl> * NOTE : we reuse the platform_data structure of GPIO leds , <nl> * but repurpose its " gpio " number as a PWM channel number . <nl> */ <nl> - static int __init pwmled_probe ( struct platform_device * pdev ) <nl> + static int __devinit pwmled_probe ( struct platform_device * pdev ) <nl> { <nl> const struct gpio_led_platform_data * pdata ; <nl> struct pwmled * leds ;
nfs4_preprocess_seqid_op ( struct svc_fh * current_fh , u32 seqid , stateid_t * statei <nl> printk (" NFSD : preprocess_seqid_op : old stateid !\ n "); <nl> goto out ; <nl> } <nl> - /* XXX renew the client lease here */ <nl> + renew_client ( sop -> so_client ); <nl> status = nfs_ok ; <nl>  <nl> out :
struct pci_bus * pci_acpi_scan_root ( struct acpi_pci_root * root ) <nl> return NULL ; <nl>  <nl> root_ops = kzalloc_node ( sizeof (* root_ops ), GFP_KERNEL , node ); <nl> - if (! root_ops ) <nl> + if (! root_ops ) { <nl> + kfree ( ri ); <nl> return NULL ; <nl> + } <nl>  <nl> ri -> cfg = pci_acpi_setup_ecam_mapping ( root ); <nl> if (! ri -> cfg ) {
int kvm_arch_vcpu_init ( struct kvm_vcpu * vcpu ) <nl> } <nl> vcpu -> arch . mcg_cap = KVM_MAX_MCE_BANKS ; <nl>  <nl> - if (! zalloc_cpumask_var (& vcpu -> arch . wbinvd_dirty_mask , GFP_KERNEL )) <nl> + if (! zalloc_cpumask_var (& vcpu -> arch . wbinvd_dirty_mask , GFP_KERNEL )) { <nl> + r = - ENOMEM ; <nl> goto fail_free_mce_banks ; <nl> + } <nl>  <nl> r = fx_init ( vcpu ); <nl> if ( r )
static long ppc_set_hwdebug ( struct task_struct * child , <nl>  <nl> brk . address = bp_info -> addr & ~ 7UL ; <nl> brk . type = HW_BRK_TYPE_TRANSLATE ; <nl> + brk . len = 8 ; <nl> if ( bp_info -> trigger_type & PPC_BREAKPOINT_TRIGGER_READ ) <nl> brk . type |= HW_BRK_TYPE_READ ; <nl> if ( bp_info -> trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE )
snd_nm256_mixer ( struct nm256 * chip ) <nl> . read = snd_nm256_ac97_read , <nl> }; <nl>  <nl> - chip -> ac97_regs = kcalloc ( sizeof ( short ), <nl> - ARRAY_SIZE ( nm256_ac97_init_val ), GFP_KERNEL ); <nl> + chip -> ac97_regs = kcalloc ( ARRAY_SIZE ( nm256_ac97_init_val ), <nl> + sizeof ( short ), GFP_KERNEL ); <nl> if (! chip -> ac97_regs ) <nl> return - ENOMEM ; <nl> 
static int pfkey_register ( struct sock * sk , struct sk_buff * skb , const struct sad <nl> pfk -> registered |= ( 1 << hdr -> sadb_msg_satype ); <nl> } <nl>  <nl> + mutex_lock (& pfkey_mutex ); <nl> xfrm_probe_algs (); <nl>  <nl> supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL | __GFP_ZERO ); <nl> + mutex_unlock (& pfkey_mutex ); <nl> + <nl> if (! supp_skb ) { <nl> if ( hdr -> sadb_msg_satype != SADB_SATYPE_UNSPEC ) <nl> pfk -> registered &= ~( 1 << hdr -> sadb_msg_satype );
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> btrfs_wait_ordered_range ( src , off , len ); <nl> } <nl>  <nl> + /* truncate page cache pages from target inode range */ <nl> + truncate_inode_pages_range (& inode -> i_data , off , <nl> + ALIGN ( off + len , PAGE_CACHE_SIZE ) - 1 ); <nl> + <nl> /* clone data */ <nl> key . objectid = btrfs_ino ( src ); <nl> key . type = BTRFS_EXTENT_DATA_KEY ;
static long tce_iommu_register_pages ( struct tce_container * container , <nl> return ret ; <nl>  <nl> tcemem = kzalloc ( sizeof (* tcemem ), GFP_KERNEL ); <nl> + if (! tcemem ) { <nl> + mm_iommu_put ( container -> mm , mem ); <nl> + return - ENOMEM ; <nl> + } <nl> + <nl> tcemem -> mem = mem ; <nl> list_add (& tcemem -> next , & container -> prereg_list ); <nl> 
static void pch_gpio_setup ( struct pch_gpio * chip ) <nl> static int pch_irq_type ( struct irq_data * d , unsigned int type ) <nl> { <nl> u32 im ; <nl> - u32 * im_reg ; <nl> + u32 __iomem * im_reg ; <nl> u32 ien ; <nl> u32 im_pos ; <nl> int ch ;
static int dm_update_crtcs_state ( struct dc * dc , <nl> } <nl> } <nl>  <nl> - if ( dc_is_stream_unchanged ( new_stream , dm_old_crtc_state -> stream ) && <nl> + if ( enable && dc_is_stream_unchanged ( new_stream , dm_old_crtc_state -> stream ) && <nl> dc_is_stream_scaling_unchanged ( new_stream , dm_old_crtc_state -> stream )) { <nl>  <nl> new_crtc_state -> mode_changed = false ;
static ssize_t iwl_dbgfs_fw_rx_stats_read ( struct file * file , <nl>  <nl> mutex_lock (& mvm -> mutex ); <nl>  <nl> + if ( iwl_mvm_firmware_running ( mvm )) <nl> + iwl_mvm_request_statistics ( mvm , false ); <nl> + <nl> pos += scnprintf ( buf + pos , bufsz - pos , fmt_header , <nl> " Statistics_Rx - OFDM "); <nl> if (! iwl_mvm_has_new_rx_stats_api ( mvm )) {
int snd_hda_multi_out_analog_open ( struct hda_codec * codec , <nl> if ( mout -> spdif_maxbps < hinfo -> maxbps ) <nl> hinfo -> maxbps = mout -> spdif_maxbps ; <nl> } <nl> + mutex_unlock (& codec -> spdif_mutex ); <nl> } <nl> - mutex_unlock (& codec -> spdif_mutex ); <nl> return snd_pcm_hw_constraint_step ( substream -> runtime , 0 , <nl> SNDRV_PCM_HW_PARAM_CHANNELS , 2 ); <nl> }
static u16 ar9003_hw_get_max_edge_power ( struct ar9300_eeprom * eep , <nl> break ; <nl> } <nl> } <nl> + <nl> + if ( is2GHz && ! twiceMaxEdgePower ) <nl> + twiceMaxEdgePower = 60 ; <nl> + <nl> return twiceMaxEdgePower ; <nl> } <nl> 
static int acp_dma_hw_params ( struct snd_pcm_substream * substream , <nl> if ( WARN_ON (! rtd )) <nl> return - EINVAL ; <nl>  <nl> - rtd -> i2s_instance = pinfo -> i2s_instance ; <nl> + if ( pinfo ) <nl> + rtd -> i2s_instance = pinfo -> i2s_instance ; <nl> if ( adata -> asic_type == CHIP_STONEY ) { <nl> val = acp_reg_read ( adata -> acp_mmio , <nl> mmACP_I2S_16BIT_RESOLUTION_EN );
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
static int ovl_copy_up_locked ( struct dentry * workdir , struct dentry * upperdir , <nl>  <nl> out_cleanup : <nl> ovl_cleanup ( wdir , newdentry ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> /*
vnet_select_queue ( struct net_device * dev , struct sk_buff * skb , <nl> struct vnet * vp = netdev_priv ( dev ); <nl> struct vnet_port * port = __tx_port_find ( vp , skb ); <nl>  <nl> + if ( port == NULL ) <nl> + return 0 ; <nl> return port -> q_index ; <nl> } <nl> 
static void proc_flush_task_mnt ( struct vfsmount * mnt , pid_t pid , pid_t tgid ) <nl> dput ( dentry ); <nl> } <nl>  <nl> + if ( pid == tgid ) <nl> + return ; <nl> + <nl> name . name = buf ; <nl> name . len = snprintf ( buf , sizeof ( buf ), "% d ", tgid ); <nl> leader = d_hash_and_lookup ( mnt -> mnt_root , & name );
static int prepare_for_handlers ( struct ieee80211_rx_data * rx , <nl> * and location updates . Note that mac80211 <nl> * itself never looks at these frames . <nl> */ <nl> + if (! multicast && <nl> + ! ether_addr_equal ( sdata -> vif . addr , hdr -> addr1 )) <nl> + return 0 ; <nl> if ( ieee80211_is_public_action ( hdr , skb -> len )) <nl> return 1 ; <nl> if (! ieee80211_is_beacon ( hdr -> frame_control ))
static void sm501_free_init_fb ( struct sm501fb_info * info , <nl> { <nl> struct fb_info * fbi = info -> fb [ head ]; <nl>  <nl> + if (! fbi ) <nl> + return ; <nl> + <nl> fb_dealloc_cmap (& fbi -> cmap ); <nl> } <nl> 
static void dpp1_cm_set_regamma_pwl ( <nl> struct dpp * dpp_base , const struct pwl_params * params , enum opp_regamma mode ) <nl> { <nl> struct dcn10_dpp * dpp = TO_DCN10_DPP ( dpp_base ); <nl> - uint32_t re_mode ; <nl> + uint32_t re_mode = 0 ; <nl>  <nl> switch ( mode ) { <nl> case OPP_REGAMMA_BYPASS :
int cx23885_dvb_unregister ( struct cx23885_tsport * port ) <nl> * implement MFE support . <nl> */ <nl> fe0 = videobuf_dvb_get_frontend (& port -> frontends , 1 ); <nl> - if ( fe0 -> dvb . frontend ) <nl> + if ( fe0 && fe0 -> dvb . frontend ) <nl> videobuf_dvb_unregister_bus (& port -> frontends ); <nl>  <nl> switch ( port -> dev -> board ) {
static int igb_find_enabled_vfs ( struct igb_adapter * adapter ) <nl> vf_devfn = pdev -> devfn + 0x80 ; <nl> pvfdev = pci_get_device ( hw -> vendor_id , device_id , NULL ); <nl> while ( pvfdev ) { <nl> - if ( pvfdev -> devfn == vf_devfn ) <nl> + if ( pvfdev -> devfn == vf_devfn && <nl> + ( pvfdev -> bus -> number >= pdev -> bus -> number )) <nl> vfs_found ++; <nl> vf_devfn += vf_stride ; <nl> pvfdev = pci_get_device ( hw -> vendor_id ,
void __init exynos5_init_irq ( void ) <nl> * Theses parameters should be NULL and 0 because EXYNOS4 <nl> * uses GIC instead of VIC . <nl> */ <nl> - s5p_init_irq ( NULL , 0 ); <nl> + if (! of_machine_is_compatible (" samsung , exynos5440 ")) <nl> + s5p_init_irq ( NULL , 0 ); <nl>  <nl> gic_arch_extn . irq_set_wake = s3c_irq_wake ; <nl> }
int dccp_rcv_state_process ( struct sock * sk , struct sk_buff * skb , <nl> if ( inet_csk ( sk )-> icsk_af_ops -> conn_request ( sk , <nl> skb ) < 0 ) <nl> return 1 ; <nl> - goto discard ; <nl> + consume_skb ( skb ); <nl> + return 0 ; <nl> } <nl> if ( dh -> dccph_type == DCCP_PKT_RESET ) <nl> goto discard ;
int dma_async_device_register ( struct dma_device * device ) <nl> } <nl> chan -> client_count = 0 ; <nl> } <nl> + <nl> + if (! chancnt ) { <nl> + dev_err ( device -> dev , "% s : device has no channels !\ n ", __func__ ); <nl> + rc = - ENODEV ; <nl> + goto err_out ; <nl> + } <nl> + <nl> device -> chancnt = chancnt ; <nl>  <nl> mutex_lock (& dma_list_mutex );
int acpi_bus_generate_proc_event4 ( const char * device_class , const char * bus_id , <nl> if (! event_is_open ) <nl> return 0 ; <nl>  <nl> - event = kmalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> + event = kzalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> if (! event ) <nl> return - ENOMEM ; <nl> 
static struct davinci_aemif_timing da830_evm_nandflash_timing = { <nl> }; <nl>  <nl> static struct davinci_nand_pdata da830_evm_nand_pdata = { <nl> + . core_chipsel = 1 , <nl> . parts = da830_evm_nand_partitions , <nl> . nr_parts = ARRAY_SIZE ( da830_evm_nand_partitions ), <nl> . ecc_mode = NAND_ECC_HW ,
__tree_mod_log_free_eb ( struct btrfs_fs_info * fs_info , struct extent_buffer * eb ) <nl> u32 nritems ; <nl> int ret ; <nl>  <nl> + if ( btrfs_header_level ( eb ) == 0 ) <nl> + return ; <nl> + <nl> nritems = btrfs_header_nritems ( eb ); <nl> for ( i = nritems - 1 ; i >= 0 ; i --) { <nl> ret = tree_mod_log_insert_key_locked ( fs_info , eb , i ,
static void __exit cmdq_drv_exit ( void ) <nl>  <nl> subsys_initcall ( cmdq_drv_init ); <nl> module_exit ( cmdq_drv_exit ); <nl> + <nl> + MODULE_LICENSE (" GPL v2 ");
static int sensor_hub_probe ( struct hid_device * hdev , <nl> if ( name == NULL ) { <nl> hid_err ( hdev , " Failed MFD device name \ n "); <nl> ret = - ENOMEM ; <nl> + kfree ( hsdev ); <nl> goto err_no_mem ; <nl> } <nl> sd -> hid_sensor_hub_client_devs [
static int orinoco_ioctl_set_auth ( struct net_device * dev , <nl> */ <nl> if ( param -> value ) { <nl> priv -> tkip_cm_active = 1 ; <nl> - ret = hermes_enable_port ( hw , 0 ); <nl> + ret = hermes_disable_port ( hw , 0 ); <nl> } else { <nl> priv -> tkip_cm_active = 0 ; <nl> - ret = hermes_disable_port ( hw , 0 ); <nl> + ret = hermes_enable_port ( hw , 0 ); <nl> } <nl> break ; <nl> 
static int _write_mirror ( struct ore_io_state * ios , int cur_comp ) <nl> struct bio * bio ; <nl>  <nl> if ( per_dev != master_dev ) { <nl> - bio = bio_clone_kmalloc ( master_dev -> bio , <nl> - GFP_KERNEL ); <nl> + bio = bio_clone_fast ( master_dev -> bio , <nl> + GFP_KERNEL , NULL ); <nl> if ( unlikely (! bio )) { <nl> ORE_DBGMSG ( <nl> " Failed to allocate BIO size =% u \ n ",
xfs_setattr_nonsize ( <nl>  <nl> out_cancel : <nl> xfs_trans_cancel ( tp ); <nl> + xfs_iunlock ( ip , XFS_ILOCK_EXCL ); <nl> out_dqrele : <nl> xfs_qm_dqrele ( udqp ); <nl> xfs_qm_dqrele ( gdqp );
static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> struct gfs2_holder * t_gh ) <nl> { <nl> struct gfs2_inode * ip ; <nl> - struct gfs2_holder ji_gh ; <nl> struct gfs2_jdesc * jd ; <nl> struct lfcc * lfcc ; <nl> LIST_HEAD ( list ); <nl> static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> gfs2_glock_dq_uninit (& lfcc -> gh ); <nl> kfree ( lfcc ); <nl> } <nl> - gfs2_glock_dq_uninit (& ji_gh ); <nl> return error ; <nl> } <nl> 
__cpuinit int unsynchronized_tsc ( void ) <nl> if ( boot_cpu_data . x86_vendor == X86_VENDOR_INTEL ) { <nl> # ifdef CONFIG_ACPI <nl> /* But TSC doesn ' t tick in C3 so don ' t use it there */ <nl> - if ( acpi_fadt . length > 0 && acpi_fadt . plvl3_lat < 100 ) <nl> + if ( acpi_fadt . length > 0 && acpi_fadt . plvl3_lat < 1000 ) <nl> return 1 ; <nl> # endif <nl> return 0 ;
snd_usb_audio_probe ( struct usb_device * dev , <nl> __error : <nl> if ( chip && ! chip -> num_interfaces ) <nl> snd_card_free ( chip -> card ); <nl> + chip -> probing = 0 ; <nl> mutex_unlock (& register_mutex ); <nl> __err_val : <nl> return NULL ;
static void __init setup_bootmem ( void ) <nl> } <nl> memset ( pfnnid_map , 0xff , sizeof ( pfnnid_map )); <nl>  <nl> - for ( i = 0 ; i < npmem_ranges ; i ++) <nl> + for ( i = 0 ; i < npmem_ranges ; i ++) { <nl> + node_set_state ( i , N_NORMAL_MEMORY ); <nl> node_set_online ( i ); <nl> + } <nl> # endif <nl>  <nl> /*
static int i2c_mux_reg_probe_dt ( struct regmux * mux , <nl> mux -> data . idle_in_use = true ; <nl>  <nl> /* map address from " reg " if exists */ <nl> - if ( of_address_to_resource ( np , 0 , & res )) { <nl> + if ( of_address_to_resource ( np , 0 , & res ) == 0 ) { <nl> mux -> data . reg_size = resource_size (& res ); <nl> mux -> data . reg = devm_ioremap_resource (& pdev -> dev , & res ); <nl> if ( IS_ERR ( mux -> data . reg ))
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> + if ( count > XFS_ACL_MAX_ENTRIES ) <nl> + return ERR_PTR (- EFSCORRUPTED ); <nl>  <nl> acl = posix_acl_alloc ( count , GFP_KERNEL ); <nl> if (! acl )
static void __reada_start_machine ( struct btrfs_fs_info * fs_info ) <nl>  <nl> do { <nl> enqueued = 0 ; <nl> + mutex_lock (& fs_devices -> device_list_mutex ); <nl> list_for_each_entry ( device , & fs_devices -> devices , dev_list ) { <nl> if ( atomic_read (& device -> reada_in_flight ) < <nl> MAX_IN_FLIGHT ) <nl> enqueued += reada_start_machine_dev ( fs_info , <nl> device ); <nl> } <nl> + mutex_unlock (& fs_devices -> device_list_mutex ); <nl> total += enqueued ; <nl> } while ( enqueued && total < 10000 ); <nl> 
static int xgifb_probe ( struct pci_dev * pdev , <nl>  <nl> if ( xgifb_info -> mode_idx < 0 ) { <nl> dev_err (& pdev -> dev , " No supported video mode found \ n "); <nl> + ret = - EINVAL ; <nl> goto error_1 ; <nl> } <nl> 
static void __init clk_sp810_of_setup ( struct device_node * node ) <nl>  <nl> if ( of_clk_parent_fill ( node , parent_names , num ) != num ) { <nl> pr_warn (" Failed to obtain parent clocks for SP810 !\ n "); <nl> + kfree ( sp810 ); <nl> return ; <nl> } <nl> 
static void ath9k_htc_send_buffered ( struct ath9k_htc_priv * priv , <nl> } <nl>  <nl> tx_slot = ath9k_htc_tx_get_slot ( priv ); <nl> - if ( tx_slot != 0 ) { <nl> + if ( tx_slot < 0 ) { <nl> ath_dbg ( common , ATH_DBG_XMIT , " No free CAB slot \ n "); <nl> dev_kfree_skb_any ( skb ); <nl> goto next ;
static ktime_t tick_nohz_stop_sched_tick ( struct tick_sched * ts , <nl> */ <nl> if ( delta == 0 ) { <nl> tick_nohz_restart ( ts , now ); <nl> + /* <nl> + * Make sure next tick stop doesn ' t get fooled by past <nl> + * clock deadline <nl> + */ <nl> + ts -> next_tick = 0 ; <nl> goto out ; <nl> } <nl> }
static int autofs4_tree_busy ( struct vfsmount * mnt , <nl> struct autofs_info * ino = autofs4_dentry_ino ( p ); <nl> unsigned int ino_count = atomic_read (& ino -> count ); <nl>  <nl> + /* <nl> + * Clean stale dentries below that have not been <nl> + * invalidated after a mount fail during lookup <nl> + */ <nl> + d_invalidate ( p ); <nl> + <nl> /* allow for dget above and top is already dgot */ <nl> if ( p == top ) <nl> ino_count += 2 ;
static struct comedi_driver das16_driver = { <nl> module_comedi_driver ( das16_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for DAS16 compatible boards "); <nl> MODULE_LICENSE (" GPL ");
static int mmci_probe ( struct amba_device * dev , <nl> dev_dbg ( mmc_dev ( mmc ), " clocking block at % u Hz \ n ", mmc -> f_max ); <nl>  <nl> /* Get regulators and the supported OCR mask */ <nl> - mmc_regulator_get_supply ( mmc ); <nl> + ret = mmc_regulator_get_supply ( mmc ); <nl> + if ( ret == - EPROBE_DEFER ) <nl> + goto clk_disable ; <nl> + <nl> if (! mmc -> ocr_avail ) <nl> mmc -> ocr_avail = plat -> ocr_mask ; <nl> else if ( plat -> ocr_mask )
static ssize_t hidraw_get_report ( struct file * file , char __user * buffer , size_t <nl> int ret = 0 , len ; <nl> unsigned char report_number ; <nl>  <nl> + if (! hidraw_table [ minor ] || ! hidraw_table [ minor ]-> exist ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl> + <nl> dev = hidraw_table [ minor ]-> hid ; <nl>  <nl> if (! dev -> ll_driver -> raw_request ) {
struct device * driver_find_device ( struct device_driver * drv , <nl> struct klist_iter i ; <nl> struct device * dev ; <nl>  <nl> - if (! drv ) <nl> + if (! drv || ! drv -> p ) <nl> return NULL ; <nl>  <nl> klist_iter_init_node (& drv -> p -> klist_devices , & i ,
static void tcm_loop_submission_work ( struct work_struct * work ) <nl> return ; <nl>  <nl> out_done : <nl> + kmem_cache_free ( tcm_loop_cmd_cache , tl_cmd ); <nl> sc -> scsi_done ( sc ); <nl> return ; <nl> }
void dlm_add_cb ( struct dlm_lkb * lkb , uint32_t flags , int mode , int status , <nl>  <nl> spin_lock (& dlm_cb_seq_spin ); <nl> new_seq = ++ dlm_cb_seq ; <nl> + if (! dlm_cb_seq ) <nl> + new_seq = ++ dlm_cb_seq ; <nl> spin_unlock (& dlm_cb_seq_spin ); <nl>  <nl> if ( lkb -> lkb_flags & DLM_IFL_USER ) {
receive_buf ( struct tty_struct * tty , struct tty_buffer * head , int count ) <nl> count = disc -> ops -> receive_buf2 ( tty , p , f , count ); <nl> else { <nl> count = min_t ( int , count , tty -> receive_room ); <nl> - if ( count ) <nl> + if ( count && disc -> ops -> receive_buf ) <nl> disc -> ops -> receive_buf ( tty , p , f , count ); <nl> } <nl> return count ;
xfs_dir2_leafn_remove ( <nl> /* <nl> * One less used entry in the free table . <nl> */ <nl> - free -> hdr . nused = cpu_to_be32 (- 1 ); <nl> + be32_add (& free -> hdr . nused , - 1 ); <nl> xfs_dir2_free_log_header ( tp , fbp ); <nl> /* <nl> * If this was the last entry in the table , we can
static int ibmvscsi_probe ( struct vio_dev * vdev , const struct vio_device_id * id ) <nl> host -> max_lun = 8 ; <nl> host -> max_id = max_id ; <nl> host -> max_channel = max_channel ; <nl> + host -> max_cmd_len = 16 ; <nl>  <nl> if ( scsi_add_host ( hostdata -> host , hostdata -> dev )) <nl> goto add_host_failed ;
void qla4xxx_free_ddb_index ( struct scsi_qla_host * ha ) <nl> ret = qla4xxx_get_fwddb_entry ( ha , idx , NULL , 0 , NULL , <nl> & next_idx , & state , & conn_err , <nl> NULL , NULL ); <nl> - if ( ret == QLA_ERROR ) <nl> + if ( ret == QLA_ERROR ) { <nl> + next_idx ++; <nl> continue ; <nl> + } <nl> if ( state == DDB_DS_NO_CONNECTION_ACTIVE || <nl> state == DDB_DS_SESSION_FAILED ) { <nl> DEBUG2 ( ql4_printk ( KERN_INFO , ha ,
static int alc_mux_select ( struct hda_codec * codec , unsigned int adc_idx , <nl> int i , type , num_conns ; <nl> hda_nid_t nid ; <nl>  <nl> + if (! spec -> input_mux ) <nl> + return 0 ; <nl> + <nl> mux_idx = adc_idx >= spec -> num_mux_defs ? 0 : adc_idx ; <nl> imux = & spec -> input_mux [ mux_idx ]; <nl> if (! imux -> num_items && mux_idx > 0 )
static int pch_dma_probe ( struct pci_dev * pdev , <nl>  <nl> if (!( pci_resource_flags ( pdev , 1 ) & IORESOURCE_MEM )) { <nl> dev_err (& pdev -> dev , " Cannot find proper base address \ n "); <nl> + err = - ENODEV ; <nl> goto err_disable_pdev ; <nl> } <nl> 
static void * etm_setup_aux ( int event_cpu , void ** pages , <nl> if (! sink_ops ( sink )-> alloc_buffer ) <nl> goto err ; <nl>  <nl> + cpu = cpumask_first ( mask ); <nl> /* Get the AUX specific data from the sink buffer */ <nl> event_data -> snk_config = <nl> sink_ops ( sink )-> alloc_buffer ( sink , cpu , pages ,
struct usb_function * ecm_alloc ( struct usb_function_instance * fi ) <nl> sizeof ( ecm -> ethaddr )); <nl> if ( status < 12 ) { <nl> kfree ( ecm ); <nl> + mutex_unlock (& opts -> lock ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> ecm_string_defs [ 1 ]. s = ecm -> ethaddr ;
static int do_md_run ( mddev_t * mddev ) <nl> if ( spares && mddev -> pers -> sync_request ) { <nl> mddev -> recovery = 0 ; <nl> set_bit ( MD_RECOVERY_RUNNING , & mddev -> recovery ); <nl> + set_bit ( MD_RECOVERY_RECOVER , & mddev -> recovery ); <nl> mddev -> sync_thread = md_register_thread ( md_do_sync , <nl> mddev , <nl> " resync ");
static int check_stack_boundary ( struct bpf_verifier_env * env , int regno , <nl> tnum_strn ( tn_buf , sizeof ( tn_buf ), regs [ regno ]. var_off ); <nl> verbose ( env , " invalid variable stack read R % d var_off =% s \ n ", <nl> regno , tn_buf ); <nl> + return - EACCES ; <nl> } <nl> off = regs [ regno ]. off + regs [ regno ]. var_off . value ; <nl> if ( off >= 0 || off < - MAX_BPF_STACK || off + access_size > 0 ||
typhoon_request_firmware ( struct typhoon * tp ) <nl> err = - ENOMEM ; <nl> goto out_err ; <nl> } <nl> + memcpy ( typhoon_fw_image , typhoon_fw -> data , typhoon_fw -> size ); <nl>  <nl> return 0 ; <nl> 
static int dcp_probe ( struct platform_device * pdev ) <nl>  <nl> r = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> dev -> dcp_regs_base = devm_ioremap_resource (& pdev -> dev , r ); <nl> + if ( IS_ERR ( dev -> dcp_regs_base )) <nl> + return PTR_ERR ( dev -> dcp_regs_base ); <nl>  <nl> dcp_set ( dev , DCP_CTRL_SFRST , DCP_REG_CTRL ); <nl> udelay ( 10 );
int mei_cl_disconnect ( struct mei_cl * cl ) <nl> cl_err ( dev , cl , " failed to disconnect .\ n "); <nl> goto free ; <nl> } <nl> + cl -> timer_count = MEI_CONNECT_TIMEOUT ; <nl> mdelay ( 10 ); /* Wait for hardware disconnection ready */ <nl> list_add_tail (& cb -> list , & dev -> ctrl_rd_list . list ); <nl> } else {
static __inline__ struct page * drm_do_vm_shm_nopage ( struct vm_area_struct * vma , <nl>  <nl> offset = address - vma -> vm_start ; <nl> i = ( unsigned long ) map -> handle + offset ; <nl> - page = vmalloc_to_page (( void *) i ); <nl> + page = ( map -> type == _DRM_CONSISTENT ) ? <nl> + virt_to_page (( void *) i ) : vmalloc_to_page (( void *) i ); <nl> if (! page ) <nl> return NOPAGE_OOM ; <nl> get_page ( page );
static int __devinit gab_probe ( struct platform_device * pdev ) <nl> ret = request_any_context_irq ( irq , gab_charged , <nl> IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING , <nl> " battery charged ", adc_bat ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_gpio ; <nl> } <nl> 
static int ismt_access ( struct i2c_adapter * adap , u16 addr , <nl>  <nl> case I2C_SMBUS_BLOCK_PROC_CALL : <nl> dev_dbg ( dev , " I2C_SMBUS_BLOCK_PROC_CALL \ n "); <nl> + if ( data -> block [ 0 ] > I2C_SMBUS_BLOCK_MAX ) <nl> + return - EINVAL ; <nl> + <nl> dma_size = I2C_SMBUS_BLOCK_MAX ; <nl> desc -> tgtaddr_rw = ISMT_DESC_ADDR_RW ( addr , 1 ); <nl> desc -> wr_len_cmd = data -> block [ 0 ] + 1 ;
static inline u32 skb_mstamp_us_delta ( const struct skb_mstamp * t1 , <nl> return delta_us ; <nl> } <nl>  <nl> + static inline bool skb_mstamp_after ( const struct skb_mstamp * t1 , <nl> + const struct skb_mstamp * t0 ) <nl> +{ <nl> + s32 diff = t1 -> stamp_jiffies - t0 -> stamp_jiffies ; <nl> + <nl> + if (! diff ) <nl> + diff = t1 -> stamp_us - t0 -> stamp_us ; <nl> + return diff > 0 ; <nl> +} <nl>  <nl> /** <nl> * struct sk_buff - socket buffer
static int cqspi_setup_flash ( struct cqspi_st * cqspi , struct device_node * np ) <nl> goto err ; <nl> } <nl>  <nl> - if ( cs > CQSPI_MAX_CHIPSELECT ) { <nl> + if ( cs >= CQSPI_MAX_CHIPSELECT ) { <nl> dev_err ( dev , " Chip select % d out of range .\ n ", cs ); <nl> goto err ; <nl> }
static struct net_device * fs_init_instance ( struct device * dev , <nl> if ( registered ) <nl> unregister_netdev ( ndev ); <nl>  <nl> - if ( fep != NULL ) { <nl> + if ( fep && fep -> ops ) { <nl> (* fep -> ops -> free_bd )( ndev ); <nl> (* fep -> ops -> cleanup_data )( ndev ); <nl> }
static int sgtl5000_i2c_probe ( struct i2c_client * client , <nl> if ( IS_ERR ( sgtl5000 -> mclk )) { <nl> ret = PTR_ERR ( sgtl5000 -> mclk ); <nl> dev_err (& client -> dev , " Failed to get mclock : % d \ n ", ret ); <nl> + /* Defer the probe to see if the clk will be provided later */ <nl> + if ( ret == - ENOENT ) <nl> + return - EPROBE_DEFER ; <nl> return ret ; <nl> } <nl> 
static ssize_t btrfs_label_store ( struct kobject * kobj , <nl> struct btrfs_fs_info * fs_info = to_fs_info ( kobj ); <nl> size_t p_len ; <nl>  <nl> + if (! fs_info ) <nl> + return - EPERM ; <nl> + <nl> if ( fs_info -> sb -> s_flags & MS_RDONLY ) <nl> return - EROFS ; <nl> 
static int uvesafb_setcmap ( struct fb_cmap * cmap , struct fb_info * info ) <nl> info -> cmap . len || cmap -> start < info -> cmap . start ) <nl> return - EINVAL ; <nl>  <nl> - entries = kmalloc ( sizeof (* entries ) * cmap -> len , GFP_KERNEL ); <nl> + entries = kmalloc_array ( cmap -> len , sizeof (* entries ), <nl> + GFP_KERNEL ); <nl> if (! entries ) <nl> return - ENOMEM ; <nl> 
struct timekeeper { <nl> u32 mult ; <nl> }; <nl>  <nl> - struct timekeeper timekeeper ; <nl> + static struct timekeeper timekeeper ; <nl>  <nl> /** <nl> * timekeeper_setup_internals - Set up internals to use clocksource clock . <nl> static struct timespec total_sleep_time ; <nl> /* <nl> * The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock . <nl> */ <nl> - struct timespec raw_time ; <nl> + static struct timespec raw_time ; <nl>  <nl> /* flag for if timekeeping is suspended */ <nl> int __read_mostly timekeeping_suspended ;
static int omap2_onenand_probe ( struct platform_device * pdev ) <nl> } <nl> } <nl>  <nl> - if ( pdata -> skip_initial_unlocking ) <nl> - this -> options |= ONENAND_SKIP_INITIAL_UNLOCKING ; <nl> - <nl> if (( r = onenand_scan (& c -> mtd , 1 )) < 0 ) <nl> goto err_release_dma ; <nl> 
int ubi_io_read ( const struct ubi_device * ubi , void * buf , int pnum , int offset , <nl> return UBI_IO_BITFLIPS ; <nl> } <nl>  <nl> - if ( read != len && retries ++ < UBI_IO_RETRIES ) { <nl> + if ( retries ++ < UBI_IO_RETRIES ) { <nl> dbg_io (" error % d % s while reading % d bytes from PEB % d :% d ," <nl> " read only % zd bytes , retry ", <nl> err , errstr , len , pnum , offset , read );
i915_gem_shrink ( struct drm_i915_private * dev_priv , <nl> SINGLE_DEPTH_NESTING ); <nl> if (! obj -> mm . pages ) { <nl> __i915_gem_object_invalidate ( obj ); <nl> + list_del_init (& obj -> global_list ); <nl> count += obj -> base . size >> PAGE_SHIFT ; <nl> } <nl> mutex_unlock (& obj -> mm . lock );
ieee80211_sta_process_chanswitch ( struct ieee80211_sub_if_data * sdata , <nl> case - 1 : <nl> cfg80211_chandef_create (& new_chandef , new_chan , <nl> NL80211_CHAN_NO_HT ); <nl> + /* keep width for 5 / 10 MHz channels */ <nl> + switch ( sdata -> vif . bss_conf . chandef . width ) { <nl> + case NL80211_CHAN_WIDTH_5 : <nl> + case NL80211_CHAN_WIDTH_10 : <nl> + new_chandef . width = sdata -> vif . bss_conf . chandef . width ; <nl> + break ; <nl> + default : <nl> + break ; <nl> + } <nl> break ; <nl> } <nl> 
int adis_update_scan_mode ( struct iio_dev * indio_dev , <nl> return - ENOMEM ; <nl>  <nl> adis -> buffer = kcalloc ( indio_dev -> scan_bytes , 2 , GFP_KERNEL ); <nl> - if (! adis -> buffer ) <nl> + if (! adis -> buffer ) { <nl> + kfree ( adis -> xfer ); <nl> + adis -> xfer = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> rx = adis -> buffer ; <nl> tx = rx + scan_count ;
e1000_up ( struct e1000_adapter * adapter ) <nl> return err ; <nl>  <nl> mod_timer (& adapter -> watchdog_timer , jiffies ); <nl> - e1000_irq_enable ( adapter ); <nl>  <nl> # ifdef CONFIG_E1000_NAPI <nl> netif_poll_enable ( netdev ); <nl> # endif <nl> + e1000_irq_enable ( adapter ); <nl> + <nl> return 0 ; <nl> } <nl> 
static struct dma_async_tx_descriptor * sdma_prep_slave_sg ( <nl>  <nl> param = BD_DONE | BD_EXTD | BD_CONT ; <nl>  <nl> - if ( i + 1 == sg_len ) <nl> + if ( i + 1 == sg_len ) { <nl> param |= BD_INTR ; <nl> + param |= BD_LAST ; <nl> + param &= ~ BD_CONT ; <nl> + } <nl>  <nl> dev_dbg ( sdma -> dev , " entry % d : count : % d dma : 0x % 08x % s % s \ n ", <nl> i , count , sg -> dma_address ,
void iio_disable_all_buffers ( struct iio_dev * indio_dev ) <nl> indio_dev -> currentmode = INDIO_DIRECT_MODE ; <nl> if ( indio_dev -> setup_ops -> postdisable ) <nl> indio_dev -> setup_ops -> postdisable ( indio_dev ); <nl> + <nl> + if ( indio_dev -> available_scan_masks == NULL ) <nl> + kfree ( indio_dev -> active_scan_mask ); <nl> } <nl>  <nl> int iio_update_buffers ( struct iio_dev * indio_dev ,
static int hpsa_eh_device_reset_handler ( struct scsi_cmnd * scsicmd ) <nl> return FAILED ; <nl> } <nl>  <nl> + if ( dev -> devtype == TYPE_ENCLOSURE ) <nl> + return SUCCESS ; <nl> + <nl> /* if controller locked up , we can guarantee command won ' t complete */ <nl> if ( lockup_detected ( h )) { <nl> snprintf ( msg , sizeof ( msg ),
static int ext4_quota_enable ( struct super_block * sb , int type , int format_id , <nl> return PTR_ERR ( qf_inode ); <nl> } <nl>  <nl> + /* Don ' t account quota for quota files to avoid recursion */ <nl> + qf_inode -> i_flags |= S_NOQUOTA ; <nl> err = dquot_enable ( qf_inode , type , format_id , flags ); <nl> iput ( qf_inode ); <nl> 
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
static int vesafb_setcolreg ( unsigned regno , unsigned red , unsigned green , <nl>  <nl> static void vesafb_destroy ( struct fb_info * info ) <nl> { <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> if ( info -> screen_base ) <nl> iounmap ( info -> screen_base ); <nl> release_mem_region ( info -> apertures -> ranges [ 0 ]. base , info -> apertures -> ranges [ 0 ]. size );
static int ext4_valid_extent ( struct inode * inode , struct ext4_extent * ext ) <nl> ext4_fsblk_t block = ext4_ext_pblock ( ext ); <nl> int len = ext4_ext_get_actual_len ( ext ); <nl>  <nl> + if ( len == 0 ) <nl> + return 0 ; <nl> return ext4_data_block_valid ( EXT4_SB ( inode -> i_sb ), block , len ); <nl> } <nl> 
int be_cmd_if_create ( struct be_adapter * adapter , u32 cap_flags , u32 en_flags , <nl> if (! status ) { <nl> struct be_cmd_resp_if_create * resp = embedded_payload ( wrb ); <nl> * if_handle = le32_to_cpu ( resp -> interface_id ); <nl> + <nl> + /* Hack to retrieve VF ' s pmac - id on BE3 */ <nl> + if ( BE3_chip ( adapter ) && ! be_physfn ( adapter )) <nl> + adapter -> pmac_id [ 0 ] = le32_to_cpu ( resp -> pmac_id ); <nl> } <nl>  <nl> err :
static struct sock * pep_sock_accept ( struct sock * sk , int flags , int * errp , <nl>  <nl> err = pep_accept_conn ( newsk , skb ); <nl> if ( err ) { <nl> + __sock_put ( sk ); <nl> sock_put ( newsk ); <nl> newsk = NULL ; <nl> goto drop ;
static inline int pmd_none_or_trans_huge_or_clear_bad ( pmd_t * pmd ) <nl> # ifdef CONFIG_TRANSPARENT_HUGEPAGE <nl> barrier (); <nl> # endif <nl> - if ( pmd_none ( pmdval )) <nl> + if ( pmd_none ( pmdval ) || pmd_trans_huge ( pmdval )) <nl> return 1 ; <nl> if ( unlikely ( pmd_bad ( pmdval ))) { <nl> - if (! pmd_trans_huge ( pmdval )) <nl> - pmd_clear_bad ( pmd ); <nl> + pmd_clear_bad ( pmd ); <nl> return 1 ; <nl> } <nl> return 0 ;
static void __init setup_lowcore ( void ) <nl> + PAGE_SIZE - STACK_FRAME_OVERHEAD - sizeof ( struct pt_regs ); <nl> lc -> current_task = ( unsigned long ) init_thread_union . thread_info . task ; <nl> lc -> thread_info = ( unsigned long ) & init_thread_union ; <nl> + lc -> lpp = LPP_MAGIC ; <nl> lc -> machine_flags = S390_lowcore . machine_flags ; <nl> lc -> stfl_fac_list = S390_lowcore . stfl_fac_list ; <nl> memcpy ( lc -> stfle_fac_list , S390_lowcore . stfle_fac_list ,
static int pinconf_dbg_config_write ( struct file * file , <nl> int i ; <nl>  <nl> /* Get userspace string and assure termination */ <nl> - buf_size = min ( count , ( sizeof ( buf )- 1 )); <nl> + buf_size = min ( count , ( size_t )( sizeof ( buf )- 1 )); <nl> if ( copy_from_user ( buf , user_buf , buf_size )) <nl> return - EFAULT ; <nl> buf [ buf_size ] = 0 ;
struct nvkm_pll_vals ; <nl>  <nl> struct nv04_devinit_priv { <nl> struct nvkm_devinit base ; <nl> - u8 owner ; <nl> + int owner ; <nl> }; <nl>  <nl> int nv04_devinit_ctor ( struct nvkm_object *, struct nvkm_object *,
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl> else <nl> hw -> wiphy -> flags &= ~ WIPHY_FLAG_PS_ON_BY_DEFAULT ; <nl>  <nl> - if ( mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_SCHED_SCAN ) { <nl> + if ( 0 && mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_SCHED_SCAN ) { <nl> hw -> wiphy -> flags |= WIPHY_FLAG_SUPPORTS_SCHED_SCAN ; <nl> hw -> wiphy -> max_sched_scan_ssids = PROBE_OPTION_MAX ; <nl> hw -> wiphy -> max_match_sets = IWL_SCAN_MAX_PROFILES ;
nv130_chipset = { <nl> static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> + . bios = nvkm_bios_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
static int wm8731_probe ( struct snd_soc_codec * codec ) <nl> err_regulator_get : <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8731 -> supplies ), wm8731 -> supplies ); <nl>  <nl> - kfree ( wm8731 ); <nl> return ret ; <nl> } <nl> 
static struct ext4_new_flex_group_data * alloc_flex_gd ( unsigned long flexbg_size ) <nl> if ( flex_gd == NULL ) <nl> goto out3 ; <nl>  <nl> - if ( flexbg_size >= UINT_MAX / sizeof ( struct ext4_new_flex_group_data )) <nl> + if ( flexbg_size >= UINT_MAX / sizeof ( struct ext4_new_group_data )) <nl> goto out2 ; <nl> flex_gd -> count = flexbg_size ; <nl> 
int inode_init_always ( struct super_block * sb , struct inode * inode ) <nl> mapping -> a_ops = & empty_aops ; <nl> mapping -> host = inode ; <nl> mapping -> flags = 0 ; <nl> + mapping -> wb_err = 0 ; <nl> atomic_set (& mapping -> i_mmap_writable , 0 ); <nl> mapping_set_gfp_mask ( mapping , GFP_HIGHUSER_MOVABLE ); <nl> mapping -> private_data = NULL ;
static int pit_ioport_read ( struct kvm_io_device * this , <nl> return - EOPNOTSUPP ; <nl>  <nl> addr &= KVM_PIT_CHANNEL_MASK ; <nl> + if ( addr == 3 ) <nl> + return 0 ; <nl> + <nl> s = & pit_state -> channels [ addr ]; <nl>  <nl> mutex_lock (& pit_state -> lock );
static int __devinit hvcs_initialize ( void ) <nl> num_ttys_to_alloc = hvcs_parm_num_devs ; <nl>  <nl> hvcs_tty_driver = alloc_tty_driver ( num_ttys_to_alloc ); <nl> - if (! hvcs_tty_driver ) <nl> + if (! hvcs_tty_driver ) { <nl> + mutex_unlock (& hvcs_init_mutex ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if ( hvcs_alloc_index_list ( num_ttys_to_alloc )) { <nl> rc = - ENOMEM ;
int max8998_irq_init ( struct max8998_dev * max8998 ) <nl>  <nl> void max8998_irq_exit ( struct max8998_dev * max8998 ) <nl> { <nl> + if ( max8998 -> ono ) <nl> + free_irq ( max8998 -> ono , max8998 ); <nl> + <nl> if ( max8998 -> irq ) <nl> free_irq ( max8998 -> irq , max8998 ); <nl> }
long kvm_vm_ioctl_create_spapr_tce ( struct kvm * kvm , <nl> int ret = - ENOMEM ; <nl> int i ; <nl>  <nl> - if (! args -> size ) <nl> + if (! args -> size || args -> page_shift < 12 || args -> page_shift > 34 || <nl> + ( args -> offset + args -> size > ( ULLONG_MAX >> args -> page_shift ))) <nl> return - EINVAL ; <nl>  <nl> size = _ALIGN_UP ( args -> size , PAGE_SIZE >> 3 );
void stmmac_remove_config_dt ( struct platform_device * pdev , <nl> if ( of_phy_is_fixed_link ( np )) <nl> of_phy_deregister_fixed_link ( np ); <nl> of_node_put ( plat -> phy_node ); <nl> + of_node_put ( plat -> mdio_node ); <nl> } <nl> # else <nl> struct plat_stmmacenet_data *
struct dev_pm_domain omap_device_pm_domain = { <nl> USE_PLATFORM_PM_SLEEP_OPS <nl> . suspend_noirq = _od_suspend_noirq , <nl> . resume_noirq = _od_resume_noirq , <nl> + . freeze_noirq = _od_suspend_noirq , <nl> + . thaw_noirq = _od_resume_noirq , <nl> + . restore_noirq = _od_resume_noirq , <nl> } <nl> }; <nl> 
# ifndef HPSB_DEBUG_TLABELS <nl> static <nl> # endif <nl> - spinlock_t hpsb_tlabel_lock = SPIN_LOCK_UNLOCKED ; <nl> + DEFINE_SPINLOCK ( hpsb_tlabel_lock ); <nl>  <nl> static DECLARE_WAIT_QUEUE_HEAD ( tlabel_wq ); <nl> 
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( sta -> sta . txq [ 0 ]) <nl> kfree ( to_txq_info ( sta -> sta . txq [ 0 ])); <nl> free : <nl> + free_percpu ( sta -> pcpu_rx_stats ); <nl> # ifdef CONFIG_MAC80211_MESH <nl> kfree ( sta -> mesh ); <nl> # endif
static int parse_mount_options ( struct ceph_mount_options ** pfsopt , <nl>  <nl> fsopt -> rsize = CEPH_MOUNT_RSIZE_DEFAULT ; <nl> fsopt -> snapdir_name = kstrdup ( CEPH_SNAPDIRNAME_DEFAULT , GFP_KERNEL ); <nl> + fsopt -> caps_wanted_delay_min = CEPH_CAPS_WANTED_DELAY_MIN_DEFAULT ; <nl> + fsopt -> caps_wanted_delay_max = CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT ; <nl> fsopt -> cap_release_safety = CEPH_CAP_RELEASE_SAFETY_DEFAULT ; <nl> fsopt -> max_readdir = CEPH_MAX_READDIR_DEFAULT ; <nl> fsopt -> max_readdir_bytes = CEPH_MAX_READDIR_BYTES_DEFAULT ;
static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> cnt += tcp_skb_pcount ( skb ); <nl>  <nl> if ( cnt > packets ) { <nl> - if ( tcp_is_sack ( tp ) || ( oldcnt >= packets )) <nl> + if (( tcp_is_sack ( tp ) && ! tcp_is_fack ( tp )) || <nl> + ( oldcnt >= packets )) <nl> break ; <nl>  <nl> mss = skb_shinfo ( skb )-> gso_size ;
char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> compose_mount_options_err : <nl> kfree ( mountdata ); <nl> mountdata = ERR_PTR ( rc ); <nl> + kfree (* devname ); <nl> + * devname = NULL ; <nl> goto compose_mount_options_out ; <nl> } <nl> 
static int __ipmi_bmc_register ( struct ipmi_smi * intf , <nl> bmc -> pdev . name = " ipmi_bmc "; <nl>  <nl> rv = ida_simple_get (& ipmi_bmc_ida , 0 , 0 , GFP_KERNEL ); <nl> - if ( rv < 0 ) <nl> + if ( rv < 0 ) { <nl> + kfree ( bmc ); <nl> goto out ; <nl> + } <nl> + <nl> bmc -> pdev . dev . driver = & ipmidriver . driver ; <nl> bmc -> pdev . id = rv ; <nl> bmc -> pdev . dev . release = release_bmc_device ;
void DisableVGA ( volatile STG4000REG __iomem * pSTGReg ) <nl> { <nl> u32 tmp ; <nl> - volatile u32 count , i ; <nl> + volatile u32 count = 0 , i ; <nl>  <nl> /* Reset the VGA registers */ <nl> tmp = STG_READ_REG ( SoftwareReset );
static void setup_mailboxes ( int base_io , struct Scsi_Host * shpnt ); <nl> static int aha1542_restart ( struct Scsi_Host * shost ); <nl> static void aha1542_intr_handle ( struct Scsi_Host * shost ); <nl>  <nl> -# define aha1542_intr_reset ( base ) outb ( IRST , CONTROL ( base )) <nl> + static inline void aha1542_intr_reset ( u16 base ) <nl> +{ <nl> + outb ( IRST , CONTROL ( base )); <nl> +} <nl>  <nl> # define WAIT ( port , mask , allof , noneof ) \ <nl> { register int WAITbits ; \
static bool lm3533_readable_register ( struct device * dev , unsigned int reg ) <nl> static bool lm3533_volatile_register ( struct device * dev , unsigned int reg ) <nl> { <nl> switch ( reg ) { <nl> - case 0x34 : /* zone */ <nl> + case 0x34 ... 0x36 : /* zone */ <nl> case 0x37 ... 0x38 : /* adc */ <nl> case 0xb0 ... 0xb1 : /* fault */ <nl> return true ;
static int fcoe_rcv ( struct sk_buff * skb , struct net_device * netdev , <nl> skb_tail_pointer ( skb ), skb_end_pointer ( skb ), <nl> skb -> csum , skb -> dev ? skb -> dev -> name : "< NULL >"); <nl>  <nl> + <nl> + skb = skb_share_check ( skb , GFP_ATOMIC ); <nl> + <nl> + if ( skb == NULL ) <nl> + return NET_RX_DROP ; <nl> + <nl> eh = eth_hdr ( skb ); <nl>  <nl> if ( is_fip_mode ( ctlr ) &&
int svc_register ( const struct svc_serv * serv , struct net * net , <nl> if ( vers -> vs_hidden ) <nl> continue ; <nl>  <nl> + /* <nl> + * Don ' t register a UDP port if we need congestion <nl> + * control . <nl> + */ <nl> + if ( vers -> vs_need_cong_ctrl && proto == IPPROTO_UDP ) <nl> + continue ; <nl> + <nl> error = __svc_register ( net , progp -> pg_name , progp -> pg_prog , <nl> i , family , proto , port ); <nl> 
static int __devinit max17042_probe ( struct i2c_client * client , <nl> reg |= CONFIG_ALRT_BIT_ENBL ; <nl> max17042_write_reg ( client , MAX17042_CONFIG , reg ); <nl> max17042_set_soc_threshold ( chip , 1 ); <nl> - } else <nl> + } else { <nl> + client -> irq = 0 ; <nl> dev_err (& client -> dev , "% s (): cannot get IRQ \ n ", <nl> __func__ ); <nl> + } <nl> } <nl>  <nl> reg = max17042_read_reg ( chip -> client , MAX17042_STATUS );
enum batadv_dbg_level { <nl> BATADV_DBG_NC = BIT ( 5 ), <nl> BATADV_DBG_MCAST = BIT ( 6 ), <nl> BATADV_DBG_TP_METER = BIT ( 7 ), <nl> - BATADV_DBG_ALL = 127 , <nl> + BATADV_DBG_ALL = 255 , <nl> }; <nl>  <nl> # ifdef CONFIG_BATMAN_ADV_DEBUG
int cx231xx_set_mode ( struct cx231xx * dev , enum cx231xx_mode set_mode ) <nl> } <nl> } <nl>  <nl> - return errCode ? - EINVAL : 0 ; <nl> + if ( errCode < 0 ) { <nl> + dev_err ( dev -> dev , " Failed to set devmode to % s : error : % i ", <nl> + dev -> mode == CX231XX_DIGITAL_MODE ? " digital " : " analog ", <nl> + errCode ); <nl> + return errCode ; <nl> + } <nl> + <nl> + return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( cx231xx_set_mode ); <nl> 
sctp_disposition_t sctp_sf_do_5_2_4_dupcook ( struct net * net , <nl> } <nl>  <nl> /* Delete the tempory new association . */ <nl> - sctp_add_cmd_sf ( commands , SCTP_CMD_NEW_ASOC , SCTP_ASOC ( new_asoc )); <nl> + sctp_add_cmd_sf ( commands , SCTP_CMD_SET_ASOC , SCTP_ASOC ( new_asoc )); <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_DELETE_TCB , SCTP_NULL ()); <nl>  <nl> /* Restore association pointer to provide SCTP command interpeter
void jfs_evict_inode ( struct inode * inode ) <nl> dquot_initialize ( inode ); <nl>  <nl> if ( JFS_IP ( inode )-> fileset == FILESYSTEM_I ) { <nl> + struct inode * ipimap = JFS_SBI ( inode -> i_sb )-> ipimap ; <nl> truncate_inode_pages_final (& inode -> i_data ); <nl>  <nl> if ( test_cflag ( COMMIT_Freewmap , inode )) <nl> jfs_free_zero_link ( inode ); <nl>  <nl> - if ( JFS_SBI ( inode -> i_sb )-> ipimap ) <nl> + if ( ipimap && JFS_IP ( ipimap )-> i_imap ) <nl> diFree ( inode ); <nl>  <nl> /*
static void collapse_huge_page ( struct mm_struct * mm , <nl> set_pmd_at ( mm , address , pmd , _pmd ); <nl> spin_unlock (& mm -> page_table_lock ); <nl> anon_vma_unlock ( vma -> anon_vma ); <nl> - mem_cgroup_uncharge_page ( new_page ); <nl> goto out ; <nl> } <nl>  <nl> static void collapse_huge_page ( struct mm_struct * mm , <nl> return ; <nl>  <nl> out : <nl> + mem_cgroup_uncharge_page ( new_page ); <nl> # ifdef CONFIG_NUMA <nl> put_page ( new_page ); <nl> # endif
static int xfrm_del_sa ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl>  <nl> static void copy_to_user_state ( struct xfrm_state * x , struct xfrm_usersa_info * p ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> id , & x -> id , sizeof ( p -> id )); <nl> memcpy (& p -> sel , & x -> sel , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & x -> lft , sizeof ( p -> lft ));
static int doDevConfig ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> int sdev = - 1 , nchans , tmp ; <nl> struct BondedDevice * bdev = NULL ; <nl>  <nl> - if ( minor < 0 || minor > COMEDI_NUM_BOARD_MINORS ) { <nl> + if ( minor < 0 || minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> ERROR (" Minor % d is invalid !\ n ", minor ); <nl> return 0 ; <nl> }
static int gdma_dma_remove ( struct platform_device * pdev ) <nl> struct gdma_dma_dev * dma_dev = platform_get_drvdata ( pdev ); <nl>  <nl> tasklet_kill (& dma_dev -> task ); <nl> - of_dma_controller_free ( pdev -> dev . of_node ); <nl> + of_dma_controller_free ( pdev -> dev . of_node ); <nl> dma_async_device_unregister (& dma_dev -> ddev ); <nl>  <nl> return 0 ;
static void cgroup_enable_task_cg_lists ( void ) <nl> } <nl>  <nl> void cgroup_iter_start ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __acquires ( css_set_lock ) <nl> { <nl> /* <nl> * The first time anyone tries to iterate across a cgroup , <nl> struct task_struct * cgroup_iter_next ( struct cgroup * cgrp , <nl> } <nl>  <nl> void cgroup_iter_end ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __releases ( css_set_lock ) <nl> { <nl> read_unlock (& css_set_lock ); <nl> }
static int __init mvebu_soc_id_init ( void ) <nl>  <nl> res_ioremap : <nl> clk_disable_unprepare ( clk ); <nl> + clk_put ( clk ); <nl>  <nl> clk_err : <nl> of_node_put ( child );
int uprobe_write_opcode ( struct mm_struct * mm , unsigned long vaddr , <nl>  <nl> retry : <nl> /* Read the page with vaddr into memory */ <nl> - ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , FOLL_FORCE , & old_page , <nl> - & vma , NULL ); <nl> + ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , <nl> + FOLL_FORCE | FOLL_SPLIT , & old_page , & vma , NULL ); <nl> if ( ret <= 0 ) <nl> return ret ; <nl> 
i915_gem_pwrite_ioctl ( struct drm_device * dev , void * data , <nl>  <nl> if ( obj -> gtt_space && <nl> obj -> cache_level == I915_CACHE_NONE && <nl> + obj -> tiling_mode == I915_TILING_NONE && <nl> obj -> map_and_fenceable && <nl> obj -> base . write_domain != I915_GEM_DOMAIN_CPU ) { <nl> ret = i915_gem_gtt_pwrite_fast ( dev , obj , args , file );
static int fc_lport_els_request ( struct fc_bsg_job * job , <nl> char * pp ; <nl> int len ; <nl>  <nl> - fp = fc_frame_alloc ( lport , sizeof ( struct fc_frame_header ) + <nl> - job -> request_payload . payload_len ); <nl> + fp = fc_frame_alloc ( lport , job -> request_payload . payload_len ); <nl> if (! fp ) <nl> return - ENOMEM ; <nl> 
static netdev_tx_t w83977af_hard_xmit ( struct sk_buff * skb , <nl>  <nl> mtt = irda_get_mtt ( skb ); <nl> pr_debug ("% s (% ld ), mtt =% d \ n ", __func__ , jiffies , mtt ); <nl> - if ( mtt ) <nl> + if ( mtt > 1000 ) <nl> + mdelay ( mtt / 1000 ); <nl> + else if ( mtt ) <nl> udelay ( mtt ); <nl>  <nl> /* Enable DMA interrupt */
static void cpufreq_stats_free_table ( unsigned int cpu ) <nl> static void cpufreq_stats_free_sysfs ( unsigned int cpu ) <nl> { <nl> struct cpufreq_policy * policy = cpufreq_cpu_get ( cpu ); <nl> + <nl> + if (! cpufreq_frequency_get_table ( cpu )) <nl> + return ; <nl> + <nl> if ( policy && ! policy_is_shared ( policy )) { <nl> pr_debug ("% s : Free sysfs stat \ n ", __func__ ); <nl> sysfs_remove_group (& policy -> kobj , & stats_attr_group );
int dss_mgr_enable ( struct omap_overlay_manager * mgr ) <nl> if (! mgr_manual_update ( mgr )) <nl> mp -> updating = true ; <nl>  <nl> + if (! dss_data . irq_enabled && need_isr ()) <nl> + dss_register_vsync_isr (); <nl> + <nl> spin_unlock_irqrestore (& data_lock , flags ); <nl>  <nl> if (! mgr_manual_update ( mgr ))
static int btree_gc_coalesce ( struct btree * b , struct btree_op * op , <nl> BUG_ON ( btree_bset_first ( new_nodes [ 0 ])-> keys ); <nl> btree_node_free ( new_nodes [ 0 ]); <nl> rw_unlock ( true , new_nodes [ 0 ]); <nl> + new_nodes [ 0 ] = NULL ; <nl>  <nl> for ( i = 0 ; i < nodes ; i ++) { <nl> if ( __bch_keylist_realloc (& keylist , bkey_u64s (& r [ i ]. b -> key )))
void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> bool return_now = false ; <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> + if (! pnfs_layout_is_valid ( lo )) { <nl> + spin_unlock (& inode -> i_lock ); <nl> + return ; <nl> + } <nl> pnfs_set_plh_return_info ( lo , range . iomode , 0 ); <nl> /* Block LAYOUTGET */ <nl> set_bit ( NFS_LAYOUT_RETURN , & lo -> plh_flags );
static int netvsc_init_buf ( struct hv_device * device ) <nl> net_device -> map_words = DIV_ROUND_UP ( net_device -> send_section_cnt , <nl> BITS_PER_LONG ); <nl>  <nl> - net_device -> send_section_map = <nl> - kzalloc ( net_device -> map_words * sizeof ( ulong ), GFP_KERNEL ); <nl> + net_device -> send_section_map = kcalloc ( net_device -> map_words , <nl> + sizeof ( ulong ), GFP_KERNEL ); <nl> if ( net_device -> send_section_map == NULL ) { <nl> ret = - ENOMEM ; <nl> goto cleanup ;
static int wm8903_probe ( struct snd_soc_codec * codec ) <nl> /* power down chip */ <nl> static int wm8903_remove ( struct snd_soc_codec * codec ) <nl> { <nl> + struct wm8903_priv * wm8903 = snd_soc_codec_get_drvdata ( codec ); <nl> + <nl> wm8903_free_gpio ( codec ); <nl> wm8903_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> + if ( wm8903 -> irq ) <nl> + free_irq ( wm8903 -> irq , codec ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int ioctl_send_response ( struct client * client , void * buffer ) <nl> if ( copy_from_user ( r -> data , u64_to_uptr ( request -> data ), <nl> r -> length )) { <nl> ret = - EFAULT ; <nl> + kfree ( r -> request ); <nl> goto out ; <nl> } <nl> fw_send_response ( client -> device -> card , r -> request ,
static void drm_cleanup ( struct drm_device * dev ) <nl> DRM_ERROR (" Cannot unload module \ n "); <nl> } <nl>  <nl> - int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> + static int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> { <nl> struct drm_minor * minor = ptr ; <nl> struct drm_device * dev ;
static int au_ide_probe ( struct device * dev ) <nl> goto out ; <nl> } <nl>  <nl> - /* FIXME : This might possibly break PCMCIA IDE devices */ <nl> - <nl> - hwif = & ide_hwifs [ pdev -> id ]; <nl> + hwif = ide_find_port (); <nl> + if ( hwif == NULL ) { <nl> + ret = - ENOENT ; <nl> + goto out ; <nl> + } <nl>  <nl> memset (& hw , 0 , sizeof ( hw )); <nl> auide_setup_ports (& hw , ahwif );
static int create_tracing_map_fields ( struct hist_trigger_data * hist_data ) <nl> struct tracing_map * map = hist_data -> map ; <nl> struct ftrace_event_field * field ; <nl> struct hist_field * hist_field ; <nl> - int i , idx ; <nl> + int i , idx = 0 ; <nl>  <nl> for_each_hist_field ( i , hist_data ) { <nl> hist_field = hist_data -> fields [ i ];
int skl_init_module ( struct skl_sst * ctx , <nl> return ret ; <nl> } <nl> mconfig -> m_state = SKL_MODULE_INIT_DONE ; <nl> - <nl> + kfree ( param_data ); <nl> return ret ; <nl> } <nl> 
void free_fib_info ( struct fib_info * fi ) <nl> # endif <nl> call_rcu (& fi -> rcu , free_fib_info_rcu ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( free_fib_info ); <nl>  <nl> void fib_release_info ( struct fib_info * fi ) <nl> {
static bool intel_sdvo_detect_hdmi_audio ( struct drm_connector * connector ) <nl> edid = intel_sdvo_get_edid ( connector ); <nl> if ( edid != NULL && edid -> input & DRM_EDID_INPUT_DIGITAL ) <nl> has_audio = drm_detect_monitor_audio ( edid ); <nl> + kfree ( edid ); <nl>  <nl> return has_audio ; <nl> }
DOT11D_GetMaxTxPwrInDbm ( <nl> netdev_info ( dev -> dev , " DOT11D_GetMaxTxPwrInDbm (): Invalid Channel \ n "); <nl> return MaxTxPwrInDbm ; <nl> } <nl> - if ( pDot11dInfo -> channel_map [ Channel ]) { <nl> + if ( pDot11dInfo -> channel_map [ Channel ]) <nl> MaxTxPwrInDbm = pDot11dInfo -> MaxTxPwrDbmList [ Channel ]; <nl> - } <nl>  <nl> return MaxTxPwrInDbm ; <nl> }
static int powermate_probe ( struct usb_interface * intf , const struct usb_device_i <nl> int error = - ENOMEM ; <nl>  <nl> interface = intf -> cur_altsetting ; <nl> + if ( interface -> desc . bNumEndpoints < 1 ) <nl> + return - EINVAL ; <nl> + <nl> endpoint = & interface -> endpoint [ 0 ]. desc ; <nl> if (! usb_endpoint_is_int_in ( endpoint )) <nl> return - EIO ;
static inline int ptr_ring_consume_batched_bh ( struct ptr_ring * r , <nl>  <nl> static inline void ** __ptr_ring_init_queue_alloc ( unsigned int size , gfp_t gfp ) <nl> { <nl> + if ( size * sizeof ( void *) > KMALLOC_MAX_SIZE ) <nl> + return NULL ; <nl> return kcalloc ( size , sizeof ( void *), gfp ); <nl> } <nl> 
static ssize_t input_dev_show_modalias ( struct class_device * dev , char * buf ) <nl>  <nl> len = input_print_modalias ( buf , PAGE_SIZE , id , 1 ); <nl>  <nl> - return max_t ( int , len , PAGE_SIZE ); <nl> + return min_t ( int , len , PAGE_SIZE ); <nl> } <nl> static CLASS_DEVICE_ATTR ( modalias , S_IRUGO , input_dev_show_modalias , NULL ); <nl> 
int rds_rdma_extra_size ( struct rds_rdma_args * args ) <nl>  <nl> local_vec = ( struct rds_iovec __user *)( unsigned long ) args -> local_vec_addr ; <nl>  <nl> + if ( args -> nr_local == 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* figure out the number of pages in the vector */ <nl> for ( i = 0 ; i < args -> nr_local ; i ++) { <nl> if ( copy_from_user (& vec , & local_vec [ i ],
static int llog_process_thread ( void * arg ) <nl> else <nl> last_index = LLOG_BITMAP_BYTES * 8 - 1 ; <nl>  <nl> + /* Record is not in this buffer . */ <nl> + if ( index > last_index ) <nl> + goto out ; <nl> + <nl> while ( rc == 0 ) { <nl> struct llog_rec_hdr * rec ; <nl> 
static __init int hardware_setup ( void ) <nl> goto out ; <nl> } <nl>  <nl> - vmx_io_bitmap_b = ( unsigned long *) __get_free_page ( GFP_KERNEL ); <nl> memset ( vmx_vmread_bitmap , 0xff , PAGE_SIZE ); <nl> memset ( vmx_vmwrite_bitmap , 0xff , PAGE_SIZE ); <nl> 
int amdgpu_device_ip_suspend ( struct amdgpu_device * adev ) <nl> if ( amdgpu_sriov_vf ( adev )) <nl> amdgpu_virt_request_full_gpu ( adev , false ); <nl>  <nl> + /* ungate SMC block powergating */ <nl> + if ( adev -> powerplay . pp_feature & PP_GFXOFF_MASK ) <nl> + amdgpu_device_ip_set_powergating_state ( adev , <nl> + AMD_IP_BLOCK_TYPE_SMC , <nl> + AMD_CG_STATE_UNGATE ); <nl> + <nl> /* ungate SMC block first */ <nl> r = amdgpu_device_ip_set_clockgating_state ( adev , AMD_IP_BLOCK_TYPE_SMC , <nl> AMD_CG_STATE_UNGATE );
static void ni6527_reset ( struct comedi_device * dev ) <nl> /* disable deglitch filters on all channels */ <nl> ni6527_set_filter_enable ( dev , 0 ); <nl>  <nl> + /* disable edge detection */ <nl> + ni6527_set_edge_detection ( dev , 0xffffffff , 0 , 0 ); <nl> + <nl> writeb ( NI6527_CLR_IRQS | NI6527_CLR_RESET_FILT , <nl> mmio + NI6527_CLR_REG ); <nl> writeb ( NI6527_CTRL_DISABLE_IRQS , mmio + NI6527_CTRL_REG );
int ip6_append_data ( struct sock * sk , int getfrag ( void * from , char * to , <nl> if ( WARN_ON ( np -> cork . opt )) <nl> return - EINVAL ; <nl>  <nl> - np -> cork . opt = kmalloc ( opt -> tot_len , sk -> sk_allocation ); <nl> + np -> cork . opt = kzalloc ( opt -> tot_len , sk -> sk_allocation ); <nl> if ( unlikely ( np -> cork . opt == NULL )) <nl> return - ENOBUFS ; <nl> 
static int setup_rt_frame ( int sig , struct k_sigaction * ka , siginfo_t * info , <nl> } else { <nl> regs -> gprs [ 14 ] = ( unsigned long ) <nl> frame -> retcode | PSW_ADDR_AMODE ; <nl> - err |= __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> - ( u16 __user *)( frame -> retcode )); <nl> + if ( __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> + ( u16 __user *)( frame -> retcode ))) <nl> + goto give_sigsegv ; <nl> } <nl>  <nl> /* Set up backchain . */
static int __devinit cas_init_one ( struct pci_dev * pdev , <nl> INIT_WORK (& cp -> reset_task , cas_reset_task ); <nl>  <nl> /* Default link parameters */ <nl> - if ( link_mode >= 0 && link_mode <= 6 ) <nl> + if ( link_mode >= 0 && link_mode < 6 ) <nl> cp -> link_cntl = link_modes [ link_mode ]; <nl> else <nl> cp -> link_cntl = BMCR_ANENABLE ;
static void kvm_vcpu_init ( struct kvm_vcpu * vcpu , struct kvm * kvm , unsigned id ) <nl>  <nl> static void kvm_vcpu_destroy ( struct kvm_vcpu * vcpu ) <nl> { <nl> - kvm_dirty_ring_free (& vcpu -> dirty_ring ); <nl> kvm_arch_vcpu_destroy ( vcpu ); <nl> + kvm_dirty_ring_free (& vcpu -> dirty_ring ); <nl>  <nl> /* <nl> * No need for rcu_read_lock as VCPU_RUN is the only place that changes
static int img_prl_out_set_fmt ( struct snd_soc_dai * dai , unsigned int fmt ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + pm_runtime_get_sync ( prl -> dev ); <nl> reg = img_prl_out_readl ( prl , IMG_PRL_OUT_CTL ); <nl> reg = ( reg & ~ IMG_PRL_OUT_CTL_EDGE_MASK ) | control_set ; <nl> img_prl_out_writel ( prl , reg , IMG_PRL_OUT_CTL ); <nl> + pm_runtime_put ( prl -> dev ); <nl>  <nl> return 0 ; <nl> }
static sctp_xmit_t sctp_packet_bundle_auth ( struct sctp_packet * pkt , <nl> /* See if this is an auth chunk we are bundling or if <nl> * auth is already bundled . <nl> */ <nl> - if ( chunk -> chunk_hdr -> type == SCTP_CID_AUTH || pkt -> auth ) <nl> + if ( chunk -> chunk_hdr -> type == SCTP_CID_AUTH || pkt -> has_auth ) <nl> return retval ; <nl>  <nl> /* if the peer did not request this chunk to be authenticated ,
extern void to_tm ( int tim , struct rtc_time * tm ); <nl> extern void tick_broadcast_ipi_handler ( void ); <nl>  <nl> extern void generic_calibrate_decr ( void ); <nl> + extern void hdec_interrupt ( struct pt_regs * regs ); <nl>  <nl> /* Some sane defaults : 125 MHz timebase , 1GHz processor */ <nl> extern unsigned long ppc_proc_freq ;
static int ade7854_set_irq ( struct device * dev , bool enable ) <nl> else <nl> irqen &= ~ BIT ( 17 ); <nl>  <nl> - ret = st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> - <nl> - return ret ; <nl> + return st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> } <nl>  <nl> static int ade7854_initial_setup ( struct iio_dev * indio_dev )
struct block_device * bdget ( dev_t dev ) <nl>  <nl> if ( inode -> i_state & I_NEW ) { <nl> bdev -> bd_contains = NULL ; <nl> + bdev -> bd_super = NULL ; <nl> bdev -> bd_inode = inode ; <nl> bdev -> bd_block_size = ( 1 << inode -> i_blkbits ); <nl> bdev -> bd_part_count = 0 ;
static int __devinit snd_hdspm_create ( struct snd_card * card , <nl> hdspm -> io_type = AES32 ; <nl> hdspm -> card_name = " RME AES32 "; <nl> hdspm -> midiPorts = 2 ; <nl> - } else if (( hdspm -> firmware_rev == 0xd5 ) || <nl> + } else if (( hdspm -> firmware_rev == 0xd2 ) || <nl> (( hdspm -> firmware_rev >= 0xc8 ) && <nl> ( hdspm -> firmware_rev <= 0xcf ))) { <nl> hdspm -> io_type = MADI ;
# include " viosrp . h " <nl>  <nl> # define IBMVFC_NAME " ibmvfc " <nl> -# define IBMVFC_DRIVER_VERSION " 1 . 0 . 3 " <nl> -# define IBMVFC_DRIVER_DATE "( October 28 , 2008 )" <nl> +# define IBMVFC_DRIVER_VERSION " 1 . 0 . 4 " <nl> +# define IBMVFC_DRIVER_DATE "( November 14 , 2008 )" <nl>  <nl> # define IBMVFC_DEFAULT_TIMEOUT 15 <nl> # define IBMVFC_INIT_TIMEOUT 30
int ip6_forward ( struct sk_buff * skb ) <nl> if ( mtu < IPV6_MIN_MTU ) <nl> mtu = IPV6_MIN_MTU ; <nl>  <nl> - if ( skb -> len > mtu ) { <nl> + if ( skb -> len > mtu && ! skb_is_gso ( skb )) { <nl> /* Again , force OUTPUT device used as source address */ <nl> skb -> dev = dst -> dev ; <nl> icmpv6_send ( skb , ICMPV6_PKT_TOOBIG , 0 , mtu );
static const struct fm10k_stats fm10k_gstrings_pf_stats [] = { <nl>  <nl> static const struct fm10k_stats fm10k_gstrings_mbx_stats [] = { <nl> FM10K_MBX_STAT (" mbx_tx_busy ", tx_busy ), <nl> - FM10K_MBX_STAT (" mbx_tx_oversized ", tx_dropped ), <nl> + FM10K_MBX_STAT (" mbx_tx_dropped ", tx_dropped ), <nl> FM10K_MBX_STAT (" mbx_tx_messages ", tx_messages ), <nl> FM10K_MBX_STAT (" mbx_tx_dwords ", tx_dwords ), <nl> FM10K_MBX_STAT (" mbx_tx_mbmem_pulled ", tx_mbmem_pulled ),
static int qe_ep_enable ( struct usb_ep * _ep , <nl> ep = container_of ( _ep , struct qe_ep , ep ); <nl>  <nl> /* catch various bogus parameters */ <nl> - if (! _ep || ! desc || ep -> ep . desc || _ep -> name == ep_name [ 0 ] || <nl> + if (! _ep || ! desc || _ep -> name == ep_name [ 0 ] || <nl> ( desc -> bDescriptorType != USB_DT_ENDPOINT )) <nl> return - EINVAL ; <nl> 
static int ptlrpc_main ( void * arg ) <nl> /* Process all incoming reqs before handling any */ <nl> if ( ptlrpc_server_request_incoming ( svcpt )) { <nl> lu_context_enter (& env -> le_ctx ); <nl> + env -> le_ses = NULL ; <nl> ptlrpc_server_handle_req_in ( svcpt , thread ); <nl> lu_context_exit (& env -> le_ctx ); <nl> 
static int beagle_twl_gpio_setup ( struct device * dev , <nl>  <nl> /* TWL4030_GPIO_MAX + 0 == ledA , EHCI nEN_USB_PWR ( out , active low ) */ <nl> gpio_request ( gpio + TWL4030_GPIO_MAX , " nEN_USB_PWR "); <nl> - gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 1 ); <nl> + gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 0 ); <nl>  <nl> /* TWL4030_GPIO_MAX + 1 == ledB , PMU_STAT ( out , active low LED ) */ <nl> gpio_leds [ 2 ]. gpio = gpio + TWL4030_GPIO_MAX + 1 ;
static int snd_timer_user_tselect ( struct file * file , <nl> if ( err < 0 ) <nl> goto __err ; <nl>  <nl> + tu -> qhead = tu -> qtail = tu -> qused = 0 ; <nl> kfree ( tu -> queue ); <nl> tu -> queue = NULL ; <nl> kfree ( tu -> tqueue );
int copy_creds ( struct task_struct * p , unsigned long clone_flags ) <nl> struct cred * new ; <nl> int ret ; <nl>  <nl> + p -> replacement_session_keyring = NULL ; <nl> + <nl> if ( <nl> # ifdef CONFIG_KEYS <nl> ! p -> cred -> thread_keyring &&
_gnutls_x509_dn_to_string ( const char * oid , void * value , <nl> if ( ret < 0 ) { <nl> gnutls_assert (); <nl> gnutls_free ( str -> data ); <nl> + str -> data = NULL ; <nl> return ret ; <nl> } <nl> str -> size = size ;
static bool check_allocations ( ASS_Shaper * shaper , size_t new_size ) <nl> ! ASS_REALLOC_ARRAY ( shaper -> emblevels , new_size ) || <nl> ! ASS_REALLOC_ARRAY ( shaper -> cmap , new_size )) <nl> return false ; <nl> + shaper -> n_glyphs = new_size ; <nl> } <nl> return true ; <nl> }
parserule ( struct scanner * s , struct environment * env ) <nl> var = scanname ( s ); <nl> parselet ( s , & val ); <nl> ruleaddvar ( r , var , val ); <nl> + if (! val ) <nl> + continue ; <nl> if ( strcmp ( var , " command ") == 0 ) <nl> hascommand = true ; <nl> else if ( strcmp ( var , " rspfile ") == 0 )
static void singlevar ( LexState * ls , expdesc * var ) { <nl> expdesc key ; <nl> singlevaraux ( fs , ls -> envn , var , 1 ); /* get environment variable */ <nl> lua_assert ( var -> k != VVOID ); /* this one must exist */ <nl> + luaK_exp2anyregup ( fs , var ); /* but could be a constant */ <nl> codestring (& key , varname ); /* key is variable name */ <nl> luaK_indexed ( fs , var , & key ); /* env [ varname ] */ <nl> }
static int cmd_handle_untagged ( IMAP_DATA * idata ) <nl> dprint ( 2 , ( debugfile , " Handling untagged NO \ n ")); <nl>  <nl> /* Display the warning message from the server */ <nl> - mutt_error ("% s ", s + 3 ); <nl> + mutt_error ("% s ", s + 2 ); <nl> mutt_sleep ( 2 ); <nl> } <nl> 
static char * clean_path ( char * path ) <nl> char * ch ; <nl> char * ch2 ; <nl> char * str ; <nl> - str = xmalloc ( strlen ( path )); <nl> + str = xmalloc ( strlen ( path ) + 1 ); <nl> ch = path ; <nl> ch2 = str ; <nl> while ( true ) {
int delete_sdp_line ( struct sip_msg * msg , char * s , struct sdp_stream_cell * str <nl>  <nl> while (* end != '\ n ' && end < ( stream -> body . s + stream -> body . len ) ) <nl> end ++; <nl> - end ++; <nl> + if ( * end == '\ n ') <nl> + end ++; <nl>  <nl> /* delete the entry */ <nl> if ( del_lump ( msg , start - msg -> buf , end - start , 0 ) == NULL )
WORD32 ih264d_video_decode ( iv_obj_t * dec_hdl , void * pv_api_ip , void * pv_api_op ) <nl> else <nl> prev_slice_err = 2 ; <nl>  <nl> + if ( ps_dec -> u4_first_slice_in_pic && ( ps_dec -> u2_total_mbs_coded == 0 )) <nl> + prev_slice_err = 1 ; <nl> + <nl> ret1 = ih264d_mark_err_slice_skip ( ps_dec , num_mb_skipped , ps_dec -> u1_nal_unit_type == IDR_SLICE_NAL , ps_dec -> ps_cur_slice -> u2_frame_num , <nl> & temp_poc , prev_slice_err ); <nl> 
static int MP4_ReadBox_String ( stream_t * p_stream , MP4_Box_t * p_box ) <nl> { <nl> MP4_READBOX_ENTER ( MP4_Box_data_string_t ); <nl>  <nl> + if ( p_box -> i_size < 8 || p_box -> i_size > SIZE_MAX ) <nl> + MP4_READBOX_EXIT ( 0 ); <nl> + <nl> p_box -> data . p_string -> psz_text = malloc ( p_box -> i_size + 1 - 8 ); /* +\ 0 , - name , - size */ <nl> if ( p_box -> data . p_string -> psz_text == NULL ) <nl> MP4_READBOX_EXIT ( 0 );
TfLiteStatus ExpandTensorDim ( TfLiteContext * context , const TfLiteTensor & input , <nl> axis = input_dims . size + 1 + axis ; <nl> } <nl> TF_LITE_ENSURE ( context , axis <= input_dims . size ); <nl> + TF_LITE_ENSURE ( context , axis >= 0 ); <nl>  <nl> TfLiteIntArray * output_dims = TfLiteIntArrayCreate ( input_dims . size + 1 ); <nl> for ( int i = 0 ; i < output_dims -> size ; ++ i ) {
int MatchingArraySize ( const ArrayType1 & array1 , int index1 , <nl> inline int MatchingDim ( const RuntimeShape & shape1 , int index1 , <nl> const RuntimeShape & shape2 , int index2 ) { <nl> TFLITE_DCHECK_EQ ( shape1 . Dims ( index1 ), shape2 . Dims ( index2 )); <nl> - return shape1 . Dims ( index1 ); <nl> + return std :: min ( shape1 . Dims ( index1 ), shape2 . Dims ( index2 )); <nl> } <nl>  <nl> template < typename ... Args >
void hexdump ( msg_info msg_info , const char * mem , unsigned int len ) <nl> } <nl> str [ c ++] = '\ n '; <nl> str [ c ++] = 0 ; <nl> - print_message ( msg_info , str ); <nl> + print_message ( msg_info , "% s ", str ); <nl> c = 0 ; <nl> } <nl> }
int PDFiumEngine :: GetMostVisiblePage () { <nl> if ( in_flight_visible_page_ ) <nl> return * in_flight_visible_page_ ; <nl>  <nl> + // We can call GetMostVisiblePage through a callback from PDFium . We have <nl> + // to defer the page deletion otherwise we could potentially delete the page <nl> + // that originated the calling JS request and destroy the objects that are <nl> + // currently being used . <nl> + defer_page_unload_ = true ; <nl> CalculateVisiblePages (); <nl> + defer_page_unload_ = false ; <nl> return most_visible_page_ ; <nl> } <nl> 
void AudioOutputController :: DoFlush () { <nl> if (! sync_reader_ ) { <nl> if ( state_ != kPaused ) <nl> return ; <nl> + AutoLock auto_lock ( lock_ ); <nl> buffer_ . Clear (); <nl> } <nl> }
bool SimplifiedBackwardsTextIterator :: handleTextNode () <nl> m_textLength = m_positionEndOffset - m_positionStartOffset ; <nl> m_textCharacters = text . characters () + ( m_positionStartOffset - offsetInNode ); <nl> ASSERT ( m_textCharacters >= text . characters ()); <nl> - ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl> + RELEASE_ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl>  <nl> m_lastCharacter = text [ m_positionEndOffset - 1 ]; <nl> 
void PaintLayerScrollableArea :: UpdateCompositingLayersAfterScroll () { <nl> DCHECK ( Layer ()-> HasCompositedLayerMapping ()); <nl> ScrollingCoordinator * scrolling_coordinator = GetScrollingCoordinator (); <nl> bool handled_scroll = <nl> - Layer ()-> IsRootLayer () && scrolling_coordinator && <nl> + ( Layer ()-> IsRootLayer () || <nl> + RuntimeEnabledFeatures :: BlinkGenPropertyTreesEnabled ()) && <nl> + scrolling_coordinator && <nl> scrolling_coordinator -> UpdateCompositedScrollOffset ( this ); <nl>  <nl> if (! handled_scroll ) {
static int ovs_events_plugin_config ( oconfig_item_t * ci ) { <nl> ovs_events_config_free (); <nl> return - 1 ; <nl> } <nl> - strncpy ( ovs_events_ctx . config . ovs_db_serv , service , <nl> - sizeof ( ovs_events_ctx . config . ovs_db_serv )); <nl> + sstrncpy ( ovs_events_ctx . config . ovs_db_serv , service , <nl> + sizeof ( ovs_events_ctx . config . ovs_db_serv )); <nl> sfree ( service ); <nl> } else if ( strcasecmp (" Socket ", child -> key ) == 0 ) { <nl> if ( cf_util_get_string_buffer (
int lcc_getval ( lcc_connection_t * c , lcc_identifier_t * ident , /* {{{ */ <nl> if ( ret_values_names != NULL ) <nl> * ret_values_names = values_names ; <nl>  <nl> + lcc_response_free (& res ); <nl> + <nl> return ( 0 ); <nl> } /* }}} int lcc_getval */ <nl> 
static int varnish_read ( user_data_t * ud ) /* {{{ */ <nl>  <nl> vd = VSM_New (); <nl> VSC_Setup ( vd ); <nl> + if ( VSM_n_Arg ( vd , conf -> instance ) == - 1 ) <nl> + { <nl> + ERROR (" Varnish plugin : unable to load statistics from instance "); <nl> + return (- 1 ); <nl> + } <nl> if ( VSC_Open ( vd , /* diag = */ 1 )) <nl> { <nl> ERROR (" varnish plugin : Unable to load statistics .");
krb5_lcc_close ( krb5_context context , krb5_ccache id ) <nl>  <nl> if ( data ) { <nl> LsaDeregisterLogonProcess ( data -> LogonHandle ); <nl> + if ( data -> cc_name ) <nl> + free ( data -> cc_name ); <nl> free ( data ); <nl> } <nl> free ( id );
strdup ( str ) <nl> int len ; <nl> char * copy ; <nl>  <nl> + if (! str ) <nl> + return (( char *) 0 ); <nl> len = strlen ( str ) + 1 ; <nl> if (!( copy = malloc (( u_int ) len ))) <nl> return (( char *) 0 );
static struct { <nl> } <nl>  <nl> void StopTracingAgent () { <nl> - tracing_agent_ -> Stop (); <nl> + if ( tracing_agent_ ) <nl> + tracing_agent_ -> Stop (); <nl> } <nl>  <nl> tracing :: Agent * GetTracingAgent () const {
class CipherBase : public BaseObject { <nl> private : <nl> EVP_CIPHER_CTX ctx_ ; /* coverity [ member_decl ] */ <nl> bool initialised_ ; <nl> - CipherKind kind_ ; <nl> + const CipherKind kind_ ; <nl> unsigned int auth_tag_len_ ; <nl> char auth_tag_ [ EVP_GCM_TLS_TAG_LEN ]; <nl> };
const char * cl_strerror ( int clerror ) <nl> return " Null argument passed while initialized is required "; <nl> case CL_EIO : <nl> return " Input / Output error "; <nl> + case CL_EFORMAT : <nl> + return " Bad format or broken data "; <nl> default : <nl> return " Unknown error code "; <nl> }
int cli_bm_scanbuff ( const unsigned char * buffer , uint32_t length , const char ** v <nl> memset (& info , 0 , sizeof ( info )); <nl> i = BM_MIN_LENGTH - BM_BLOCK_SIZE ; <nl> if ( offdata ) { <nl> + if (! offdata -> cnt ) <nl> + return CL_CLEAN ; <nl> for (; offdata -> pos && offdata -> offtab [ offdata -> pos ] > offset ; offdata -> pos --); <nl> if ( offdata -> offtab [ offdata -> pos ] < offset ) <nl> offdata -> pos ++;
do_election_count_vote ( long long action , <nl> crm_devel (" Election fail : born_on "); <nl> we_loose = TRUE ; <nl>  <nl> - } else if ( your_node -> node_born_on < our_node -> node_born_on ) { <nl> + } else if ( your_node -> node_born_on > our_node -> node_born_on ) { <nl> crm_devel (" Election pass : born_on "); <nl>  <nl> } else if ( strcmp ( fsa_our_uname , vote_from ) > 0 ) {
prepare_cmd_parameters ( const char * rsc_type , const char * op_type , <nl> * Add the teminating NULL pointer . <nl> */ <nl> params_argv [ 2 ] = NULL ; <nl> - if ( ht_size != 0 ) { <nl> + if ( ( ht_size != 0 ) && ( 0 != STRNCMP_CONST ( op_type , " status ")) ) { <nl> cl_log ( LOG_WARNING , " For LSB init script , no additional " <nl> " parameters are needed ."); <nl> }
bool ParseHDKeypath ( std :: string keypath_str , std :: vector < uint32_t >& keypath ) <nl> return false ; <nl> } <nl> uint32_t number ; <nl> - ParseUInt32 ( item , & number ); <nl> + if (! ParseUInt32 ( item , & number )) { <nl> + return false ; <nl> + } <nl> path |= number ; <nl>  <nl> keypath . push_back ( path );
int git_repository_config__weakptr ( git_config ** out , git_repository * repo ) <nl> git_buf_free (& global_buf ); <nl> git_buf_free (& xdg_buf ); <nl> git_buf_free (& system_buf ); <nl> + git_buf_free (& programdata_buf ); <nl> } <nl>  <nl> * out = repo -> _config ;
static void checkout_data_clear ( checkout_data * data ) <nl> git__free ( data -> pfx ); <nl> data -> pfx = NULL ; <nl>  <nl> + git_buf_free (& data -> last_mkdir ); <nl> git_buf_free (& data -> path ); <nl> git_buf_free (& data -> tmp ); <nl> 
int git_reference_foreach ( <nl> if ( list_flags & GIT_REF_PACKED ) { <nl> const char * ref_name ; <nl> void * ref ; <nl> + GIT_UNUSED ( ref ); <nl>  <nl> if ( packed_load ( repo ) < 0 ) <nl> return - 1 ;
static int find_common ( int fd [ 2 ], unsigned char * result_sha1 , <nl> retval = 0 ; <nl> in_vain = 0 ; <nl> got_continue = 1 ; <nl> + if ( ack == ACK_ready ) <nl> + rev_list = NULL ; <nl> break ; <nl> } <nl> }
static struct child_process * git_connect_git ( int fd [ 2 ], char * hostandport , <nl> target_host = xstrdup ( hostandport ); <nl>  <nl> transport_check_allowed (" git "); <nl> + if ( strchr ( target_host , '\ n ') || strchr ( path , '\ n ')) <nl> + die ( _ (" newline is forbidden in git :// hosts and repo paths ")); <nl>  <nl> /* <nl> * These underlying connection commands die () if they
static int prepare_lines ( struct scoreboard * sb ) <nl> bol = 1 ; <nl> } <nl> } <nl> + sb -> lineno = xrealloc ( sb -> lineno , <nl> + sizeof ( int * ) * ( num + incomplete + 1 )); <nl> + sb -> lineno [ num + incomplete ] = buf - sb -> final_buf ; <nl> sb -> num_lines = num + incomplete ; <nl> return sb -> num_lines ; <nl> }
static void credential_write_item ( FILE * fp , const char * key , const char * value ) <nl> { <nl> if (! value ) <nl> return ; <nl> + if ( strchr ( value , '\ n ')) <nl> + die (" credential value for % s contains newline ", key ); <nl> fprintf ( fp , "% s =% s \ n ", key , value ); <nl> } <nl> 
static void copy_templates ( const char * git_dir , int len , char * template_dir ) <nl> } <nl>  <nl> memcpy ( path , git_dir , len ); <nl> + path [ len ] = 0 ; <nl> copy_templates_1 ( path , len , <nl> template_path , template_len , <nl> dir );
arg_new_interval ( struct mail_search_build_context * ctx , <nl> } <nl> sarg -> value . search_flags = MAIL_SEARCH_ARG_FLAG_USE_TZ ; <nl> sarg -> value . time = ioloop_time - interval ; <nl> + sarg -> value . date_type = MAIL_SEARCH_DATE_TYPE_RECEIVED ; <nl> return sarg ; <nl> } <nl> 
struct ostream * fs_write_stream ( struct fs_file * file ) <nl>  <nl> int fs_write_stream_finish ( struct fs_file * file , struct ostream ** output ) <nl> { <nl> - i_assert (* output == file -> output ); <nl> + i_assert (* output == file -> output || * output == NULL ); <nl>  <nl> * output = NULL ; <nl> return file -> fs -> v . write_stream_finish ( file , TRUE );
int hash_format_init ( const char * format_string , struct hash_format ** format_r , <nl> } T_END ; <nl> if ( ret < 0 ) { <nl> * error_r = t_strdup (* error_r ); <nl> + pool_unref (& pool ); <nl> return - 1 ; <nl> } <nl> * format_r = format ;
static void sasl_callback ( struct client * _client , enum sasl_server_reply reply , <nl> data : AUTH_FAILED_MSG , NULL ); <nl> client_send_line ( client , msg ); <nl>  <nl> - if (! client -> destroyed ) { <nl> + if (! client -> destroyed && ! client -> auth_initializing ) { <nl> /* get back to normal client input . */ <nl> if ( client -> io != NULL ) <nl> io_remove (& client -> io );
DOVEADM_CMD_PARAMS_END <nl>  <nl> struct doveadm_cmd_ver2 doveadm_cmd_mailbox_delete_ver2 = { <nl> . name = " mailbox delete ", <nl> - . mail_cmd = cmd_mailbox_delete_alloc , <nl> - . usage = DOVEADM_CMD_MAIL_USAGE_PREFIX "[- s ] < mailbox > [...]", <nl> + . mail_cmd = cmd_mailbox_delete_alloc , <nl> + . usage = DOVEADM_CMD_MAIL_USAGE_PREFIX "[- s ] < mailbox > [...]", <nl> DOVEADM_CMD_PARAMS_START <nl> DOVEADM_CMD_MAIL_COMMON <nl> DOVEADM_CMD_PARAM (' s ', " subscriptions ", CMD_PARAM_BOOL , 0 )
static void tview_lookup_seq_range ( struct mail_index_view * view , <nl> if ( first_uid <= rec -> uid ) <nl> break ; <nl> } <nl> - if ( seq > tview -> t -> last_new_seq ) { <nl> + if ( seq > tview -> t -> last_new_seq || rec -> uid > last_uid ) { <nl> /* no messages in range */ <nl> return ; <nl> }
const struct stat * i_stream_stat ( struct istream * stream ) <nl> { <nl> struct _istream * _stream = stream -> real_stream ; <nl>  <nl> + if ( stream -> closed ) <nl> + return NULL ; <nl> + <nl> return _stream -> stat ( _stream ); <nl> } <nl> 
static bool outofmem = FALSE ; <nl>  <nl> static union { <nl> struct stack_block block ; <nl> - unsigned char data [ 128 ]; <nl> + unsigned char data [ 512 ]; <nl> } outofmem_area ; <nl>  <nl> static void data_stack_last_buffer_reset ( bool preserve_data ATTR_UNUSED ) <nl> static void free_blocks ( struct stack_block * block ) <nl> unused_block = block ; <nl> } else { <nl> # ifndef USE_GC <nl> - free ( block ); <nl> + if ( block != & outofmem_area . block ) <nl> + free ( block ); <nl> # endif <nl> } <nl> 
int shared_storage_get_namespace ( struct mail_namespace ** _ns , <nl> /* this user doesn ' t have a usable storage */ <nl> new_ns -> flags |= NAMESPACE_FLAG_UNUSABLE ; <nl> } <nl> + /* mark the shared namespace root as usable , since it now has <nl> + child namespaces */ <nl> + ns -> flags |= NAMESPACE_FLAG_USABLE ; <nl> * _name = mailbox_list_get_storage_name ( new_ns -> list , <nl> t_strconcat ( new_ns -> prefix , name , NULL )); <nl> * _ns = new_ns ;
void Curl_ossl_md5sum ( unsigned char * tmp , /* input */ <nl>  <nl> bool Curl_ossl_cert_status_request ( void ) <nl> { <nl> +# if ! defined ( HAVE_BORINGSSL ) && ! defined ( OPENSSL_NO_TLSEXT ) <nl> return TRUE ; <nl> +# else <nl> + return FALSE ; <nl> +# endif <nl> } <nl> # endif /* USE_SSLEAY */
static CURLcode imap_statemach_act ( struct connectdata * conn ) <nl> if ( result ) <nl> return result ; <nl>  <nl> + /* Was there an error parsing the response line ? */ <nl> + if ( imapcode == - 1 ) <nl> + return CURLE_FTP_WEIRD_SERVER_REPLY ; <nl> + <nl> if ( imapcode ) { <nl> /* We have now received a full IMAP server response */ <nl> switch ( imapc -> state ) {
int Curl_resolv_timeout ( struct connectdata * conn , <nl>  <nl> * entry = NULL ; <nl>  <nl> + if ( timeoutms < 0 ) <nl> + /* got an already expired timeout */ <nl> + return CURLRESOLV_TIMEDOUT ; <nl> + <nl> # ifdef USE_ALARM_TIMEOUT <nl> if ( data -> set . no_signal ) <nl> /* Ignore the timeout when signals are disabled */
static CURLcode smb_connect ( struct connectdata * conn , bool * done ) <nl>  <nl> ( void ) done ; <nl>  <nl> + /* Check we have a username and password to authenticate with */ <nl> + if (! conn -> bits . user_passwd ) <nl> + return CURLE_LOGIN_DENIED ; <nl> + <nl> /* Initialize the connection state */ <nl> memset ( smbc , 0 , sizeof (* smbc )); <nl> smbc -> state = SMB_CONNECTING ;
int main ( int argc , char ** argv ) <nl> /* HTTP PUT please */ <nl> curl_easy_setopt ( curl , CURLOPT_PUT , TRUE ); <nl>  <nl> - /* specify target */ <nl> + /* specify target URL , and note that this URL should include a file <nl> + name , not only a directory */ <nl> curl_easy_setopt ( curl , CURLOPT_URL , url ); <nl>  <nl> /* now specify which file to upload */
int req_stat_job ( <nl> */ <nl>  <nl> snprintf ( name , sizeof ( name ), "% s ", preq -> rq_ind . rq_status . rq_id ); <nl> - name [( PBS_MAXSVRJOBID > PBS_MAXDEST ? PBS_MAXSVRJOBID : PBS_MAXDEST )] = '\ 0 '; <nl> + name [ sizeof ( name ) - 1 ] = '\ 0 '; <nl>  <nl> if (( name [ 0 ] == '\ 0 ') || ( name [ 0 ] == '@')) <nl> {
START_TEST ( test_should_resend_obit ) <nl> pjob . ji_obit_sent = time_now ; <nl>  <nl> // Running jobs shouldn ' t re - send their obits <nl> - pjob . ji_qs . ji_substate == JOB_SUBSTATE_RUNNING ; <nl> + pjob . ji_qs . ji_substate = JOB_SUBSTATE_RUNNING ; <nl> fail_unless ( should_resend_obit (& pjob , diff ) == false ); <nl> pjob . ji_obit_busy_time = time_now - ( 2 * diff ); <nl> fail_unless ( should_resend_obit (& pjob , diff ) == false );
int get_active_pbs_server ( <nl> } <nl> else if (( rc = socket_read_str ( local_socket , & read_buf , & read_buf_len )) != PBSE_NONE ) <nl> { <nl> + if ( read_buf != NULL ) <nl> + free ( read_buf ); <nl> + <nl> close ( local_socket ); <nl> return ( rc ); <nl> }
int convert_temporary_dialog ( ua_pres_t * dialog ) <nl> temp_dialog = get_temporary_dialog ( dialog , hash_code ); <nl> if ( temp_dialog ) <nl> delete_htable ( temp_dialog , hash_code ); <nl> - else <nl> + else { <nl> + lock_release (& HashT -> p_records [ hash_code ]. lock ); <nl> return - 1 ; <nl> + } <nl>  <nl> insert_htable ( dialog , hash_code ); <nl> 
int th_msg_received ( void * data ) <nl> { <nl> sip_msg_t msg ; <nl> str * obuf ; <nl> - char * nbuf ; <nl> + char * nbuf = NULL ; <nl> int direction ; <nl> int dialog ; <nl>  <nl> int th_msg_received ( void * data ) <nl> obuf -> s [ obuf -> len ] = '\ 0 '; <nl>  <nl> done : <nl> + if ( nbuf != NULL ) <nl> + pkg_free ( nbuf ); <nl> free_sip_msg (& msg ); <nl> return 0 ; <nl> }
sl_api_t slb ; <nl> /** module variables */ <nl> str dmq_request_method = str_init (" KDMQ "); <nl> dmq_worker_t * workers ; <nl> - dmq_peer_list_t * peer_list ; <nl> + dmq_peer_list_t * peer_list = 0 ; <nl> /* the list of dmq servers */ <nl> dmq_node_list_t * node_list ; <nl> // the dmq module is a peer itself for receiving notifications regarding nodes
gui_save_file_callback ( gpointer callback_data , guint callback_action , GtkWidget <nl> if ( filetype_str ) <nl> if ( g_str_equal ( savers [ n ]. extension , filetype_str )) <nl> gtk_combo_box_set_active ( GTK_COMBO_BOX ( filetype ), n ); <nl> + g_free ( filetype_str ); <nl> n ++; <nl> } <nl> if ( gtk_combo_box_get_active ( GTK_COMBO_BOX ( filetype )) == - 1 )
static void set_format ( WAVEFORMATEXTENSIBLE * wformat , WORD bytepersample , <nl> wformat -> Format . nAvgBytesPerSec = samplerate * block_align ; <nl> wformat -> Format . nBlockAlign = block_align ; <nl> wformat -> Format . wBitsPerSample = bytepersample * 8 ; <nl> - wformat -> Format . cbSize = <nl> - 22 ; /* must be at least 22 for WAVE_FORMAT_EXTENSIBLE */ <nl> + wformat -> Format . cbSize = sizeof ( WAVEFORMATEXTENSIBLE ) - sizeof ( WAVEFORMATEX ); <nl> if ( bytepersample == 4 ) <nl> wformat -> SubFormat = mp_KSDATAFORMAT_SUBTYPE_IEEE_FLOAT ; <nl> else
static void handle_stream ( demuxer_t * demuxer , int i ) <nl> sh_sub -> frame_based = 23 . 976 ; <nl> } <nl> } <nl> + <nl> + if ( matches_avinputformat_name ( priv , " ass ")) <nl> + sh_sub -> is_utf8 = true ; <nl> + <nl> break ; <nl> } <nl> case AVMEDIA_TYPE_ATTACHMENT : {
static void pass_add_hook ( struct gl_video * p , struct tex_hook hook ) <nl> p -> tex_hooks [ p -> tex_hook_num ++] = hook ; <nl> } else { <nl> MP_ERR ( p , " Too many hooks ! Limit is % d .\ n ", MAX_TEXTURE_HOOKS ); <nl> + <nl> + if ( hook . free ) <nl> + hook . free (& hook ); <nl> } <nl> } <nl> 
void mp_msg_va ( struct mp_log * log , int lev , const char * format , va_list va ) <nl>  <nl> set_msg_color ( stream , lev ); <nl> if ( header ) { <nl> - if ( lev >= MSGL_V || verbose || mp_msg_module ) { <nl> + if (( lev >= MSGL_V && lev != MSGL_SMODE ) || verbose || mp_msg_module ) { <nl> fprintf ( stream , "[% s ] ", log -> verbose_prefix ); <nl> } else if ( log -> prefix ) { <nl> fprintf ( stream , "[% s ] ", log -> prefix );
static void vo_wayland_border ( struct vo * vo ) <nl> static void vo_wayland_fullscreen ( struct vo * vo ) <nl> { <nl> struct vo_wayland_state * wl = vo -> wayland ; <nl> - if (! wl -> display . shell ) <nl> + if (! wl -> display . shell || !! vo -> opts -> fullscreen == wl -> window . is_fullscreen ) <nl> return ; <nl>  <nl> struct wl_output * fs_output = wl -> display . fs_output ;
if ( verbose > 1 ){ <nl> mpi -> stride [ 0 ]= lavc_picture . linesize [ 0 ]; <nl> mpi -> stride [ 1 ]= lavc_picture . linesize [ 1 ]; <nl> mpi -> stride [ 2 ]= lavc_picture . linesize [ 2 ]; <nl> + if ( lavc_context . pix_fmt == PIX_FMT_YUV422P ){ <nl> + mpi -> stride [ 1 ]*= 2 ; <nl> + mpi -> stride [ 2 ]*= 2 ; <nl> + } <nl>  <nl> // stride [ 1 ]= stride [ 2 ]= 0 ; <nl> // stride [ 0 ]/= 2 ;
void vo_seek_reset ( struct vo * vo ) <nl> { <nl> vo_control ( vo , VOCTRL_RESET , NULL ); <nl> vo -> frame_loaded = false ; <nl> + vo -> hasframe = false ; <nl> mp_image_unrefp (& vo -> waiting_mpi ); <nl> } <nl> 
static void update_osd_msg ( void ) { <nl> # ifdef USE_OSD <nl> if ( sh_video ) vo_osd_changed ( OSDTYPE_OSD ); else <nl> # endif <nl> - if ( term_osd ) printf ("% s % s \ n ", term_osd_esc , msg -> msg ); <nl> + if ( term_osd ) mp_msg ( MSGT_CPLAYER , MSGL_STATUS ,"% s % s \ n ", term_osd_esc , msg -> msg ); <nl> } <nl> return ; <nl> }
bool load_bitstream_intelhex ( bytes_t * rv , const char * dname , const char * fn ) <nl> applog ( LOG_ERR , " Error reading '% s '", fn ); <nl> goto ihxerr ; <nl> } <nl> - fgets ( buf , sizeof ( buf ), F ); <nl> + if (! fgets ( buf , sizeof ( buf ), F )) <nl> + goto ihxerr ; <nl> if ( unlikely ( buf [ 0 ] != ':')) <nl> goto ihxerr ; <nl> if ( unlikely (!(
extern FILE * open_bitstream ( const char * dname , const char * filename ); <nl>  <nl> extern void close_device_fd ( struct thr_info *); <nl>  <nl> +# define for_each_managed_proc ( procvar , dev ) \ <nl> + for ( struct cgpu_info * procvar = dev ; procvar ; procvar = procvar -> next_proc ) <nl> +# define for_each_logical_proc ( procvar , dev ) \ <nl> + for ( struct cgpu_info * procvar = dev ; procvar -> proc_id < dev -> procs ; procvar = procvar -> next_proc ) <nl> + <nl> # endif
static void devdetail_an ( struct io_data * io_data , struct cgpu_info * cgpu , bool i <nl> if ( cgpu -> device_path ) <nl> root = api_add_string ( root , " Device Path ", cgpu -> device_path , false ); <nl>  <nl> - if ( cgpu -> drv -> get_api_extra_device_detail ) <nl> + if (( per_proc || cgpu -> procs <= 1 ) && cgpu -> drv -> get_api_extra_device_detail ) <nl> root = api_add_extra ( root , cgpu -> drv -> get_api_extra_device_detail ( cgpu )); <nl>  <nl> root = print_data ( root , buf , isjson , precom );
AcpiGetHandle ( <nl> { <nl> ACPI_STATUS Status ; <nl> NAME_TABLE_ENTRY * ThisEntry ; <nl> - ACPI_HANDLE Scope = NULL ; <nl> + NAME_TABLE_ENTRY * Scope = NULL ; <nl>  <nl> if (! RetHandle || ! Pathname ) <nl> { <nl> AcpiGetHandle ( <nl> } <nl>  <nl> if ( Parent ) <nl> - Scope = Parent -> Scope ; <nl> + Scope = (( NAME_TABLE_ENTRY *) Parent )-> Scope ; <nl>  <nl> /* Special case for root , since we can ' t search for it */ <nl> 
ACPI_STATUS <nl> OsGetGlobalLock ( void ) <nl> { <nl> UINT32 GlobalLockReg ; <nl> - ACPI_STATUS Status ; <nl> + ACPI_STATUS Status = AE_OK ; <nl>  <nl>  <nl> if ( FACS )
/****************************************************************************** <nl> * <nl> * Module Name : aeexec - Support routines for AcpiExec utility <nl> - * $ Revision : 1 . 107 $ <nl> + * $ Revision : 1 . 108 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AeMiscellaneousTests ( <nl> AcpiOsPrintf (" Could not get GlobalLock , % X \ n ", Status ); <nl> } <nl>  <nl> + Status = AcpiAcquireGlobalLock ( 0x5 , & LockHandle ); /* Should fail */ <nl> + <nl> Status = AcpiReleaseGlobalLock ( LockHandle ); <nl> if ( ACPI_FAILURE ( Status )) <nl> {
AcpiNsEvaluate ( <nl>  <nl> Status = AE_OK ; <nl> } <nl> + else if ( ACPI_FAILURE ( Status )) <nl> + { <nl> + /* If ReturnObject exists , delete it */ <nl> + <nl> + if ( Info -> ReturnObject ) <nl> + { <nl> + AcpiUtRemoveReference ( Info -> ReturnObject ); <nl> + Info -> ReturnObject = NULL ; <nl> + } <nl> + } <nl>  <nl> ACPI_DEBUG_PRINT (( ACPI_DB_NAMES , <nl> "*** Completed evaluation of object % s ***\ n ",
AcpiHwClearAcpiStatus ( <nl>  <nl> Status = AcpiHwRegisterWrite ( ACPI_REGISTER_PM1_STATUS , <nl> ACPI_BITMASK_ALL_FIXED_STATUS ); <nl> + <nl> + AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + <nl> if ( ACPI_FAILURE ( Status )) <nl> { <nl> - goto UnlockAndExit ; <nl> + goto Exit ; <nl> } <nl>  <nl> /* Clear the GPE Bits in all GPE registers in all GPE blocks */ <nl>  <nl> Status = AcpiEvWalkGpeList ( AcpiHwClearGpeBlock , NULL ); <nl>  <nl> - UnlockAndExit : <nl> - AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + Exit : <nl> return_ACPI_STATUS ( Status ); <nl> } <nl> 
/****************************************************************************** <nl> * <nl> * Module Name : exutils - interpreter / scanner utilities <nl> - * $ Revision : 1 . 116 $ <nl> + * $ Revision : 1 . 117 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiExEisaIdToString ( <nl> * <nl> * RETURN : None , string <nl> * <nl> - * DESCRIPTOIN : Convert a number to string representation . Assumes string <nl> + * DESCRIPTION : Convert a number to string representation . Assumes string <nl> * buffer is large enough to hold the string . <nl> * <nl> ******************************************************************************/
AmlDumpOperand ( <nl> break ; <nl>  <nl>  <nl> + case AML_NAMEPATH_OP : <nl> + DEBUG_PRINT_RAW ( ACPI_INFO , (" Lvalue . Nte -> Name % x \ n ", <nl> + EntryDesc -> Lvalue . Nte -> Name )); <nl> + break ; <nl> + <nl> default : <nl>  <nl> /* unknown opcode */
findso : <nl> so -> so_ti = ti ; <nl> tp -> t_timer [ TCPT_KEEP ] = TCPTV_KEEP_INIT ; <nl> tp -> t_state = TCPS_SYN_RECEIVED ; <nl> + tcp_template ( tp ); <nl> } <nl> return ; <nl> 
void ppce500_init ( PPCE500Params * params ) <nl>  <nl> /* Fixup Memory size on a alignment boundary */ <nl> ram_size &= ~( RAM_SIZES_ALIGN - 1 ); <nl> + params -> ram_size = ram_size ; <nl>  <nl> /* Register Memory */ <nl> memory_region_init_ram ( ram , " mpc8544ds . ram ", ram_size );
static int64_t coroutine_fn vmdk_co_get_block_status ( BlockDriverState * bs , <nl> break ; <nl> case VMDK_OK : <nl> ret = BDRV_BLOCK_DATA ; <nl> - if ( extent -> file == bs -> file ) { <nl> + if ( extent -> file == bs -> file && ! extent -> compressed ) { <nl> ret |= BDRV_BLOCK_OFFSET_VALID | offset ; <nl> } <nl> 
void qmp_migrate_set_cache_size ( int64_t value , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> + /* Cache should not be larger than guest ram size */ <nl> + if ( value > ram_bytes_total ()) { <nl> + error_set ( errp , QERR_INVALID_PARAMETER_VALUE , " cache size ", <nl> + " exceeds guest ram size "); <nl> + return ; <nl> + } <nl> + <nl> new_size = xbzrle_cache_resize ( value ); <nl> if ( new_size < 0 ) { <nl> error_set ( errp , QERR_INVALID_PARAMETER_VALUE , " cache size ",
void qemu_iovec_concat_iov ( QEMUIOVector * dst , <nl> { <nl> int i ; <nl> size_t done ; <nl> + <nl> + if (! sbytes ) { <nl> + return ; <nl> + } <nl> assert ( dst -> nalloc != - 1 ); <nl> for ( i = 0 , done = 0 ; done < sbytes && i < src_cnt ; i ++) { <nl> if ( soffset < src_iov [ i ]. iov_len ) {
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> - if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + if ( dev -> setup_index < 0 || <nl> + dev -> setup_len < 0 || <nl> + dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> }
int virtio_load ( VirtIODevice * vdev , QEMUFile * f ) <nl> qemu_get_8s ( f , & vdev -> status ); <nl> qemu_get_8s ( f , & vdev -> isr ); <nl> qemu_get_be16s ( f , & vdev -> queue_sel ); <nl> + if ( vdev -> queue_sel >= VIRTIO_PCI_QUEUE_MAX ) { <nl> + return - 1 ; <nl> + } <nl> qemu_get_be32s ( f , & features ); <nl>  <nl> if ( virtio_set_features ( vdev , features ) < 0 ) {
static inline void gen_neon_mull ( TCGv_i64 dest , TCGv a , TCGv b , int size , int u ) <nl> case 4 : <nl> tmp = gen_muls_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> case 5 : <nl> tmp = gen_mulu_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> default : abort (); <nl> }
static void * qpa_thread_out ( void * arg ) <nl> return NULL ; <nl> } <nl>  <nl> + pa -> live = 0 ; <nl> pa -> rpos = rpos ; <nl> - pa -> live -= decr ; <nl> pa -> decr += decr ; <nl> } <nl> 
static uint32_t pic_poll_read ( PicState * s ) <nl> pic_update_irq ( s -> pics_state ); <nl> } else { <nl> ret = 0x07 ; <nl> - pic_update_irq ( s -> pics_state ); <nl> } <nl>  <nl> return ret ;
static void vfio_msi_interrupt ( void * opaque ) <nl> MSIMessage msg ; <nl>  <nl> if ( vdev -> interrupt == VFIO_INT_MSIX ) { <nl> - msg = msi_get_message (& vdev -> pdev , nr ); <nl> - } else if ( vdev -> interrupt == VFIO_INT_MSI ) { <nl> msg = msix_get_message (& vdev -> pdev , nr ); <nl> + } else if ( vdev -> interrupt == VFIO_INT_MSI ) { <nl> + msg = msi_get_message (& vdev -> pdev , nr ); <nl> } else { <nl> abort (); <nl> }
static int vmdk_open_vmdk4 ( BlockDriverState * bs , <nl> } <nl> extent -> compressed = <nl> le16_to_cpu ( header . compressAlgorithm ) == VMDK4_COMPRESSION_DEFLATE ; <nl> + if ( extent -> compressed ) { <nl> + g_free ( s -> create_type ); <nl> + s -> create_type = g_strdup (" streamOptimized "); <nl> + } <nl> extent -> has_marker = le32_to_cpu ( header . flags ) & VMDK4_FLAG_MARKER ; <nl> extent -> version = le32_to_cpu ( header . version ); <nl> extent -> has_zero_grain = le32_to_cpu ( header . flags ) & VMDK4_FLAG_ZERO_GRAIN ;
static struct dconf_module modules [] = { <nl> { " PE ", " MD5SECT ", PE_CONF_MD5SECT , 1 }, <nl> { " PE ", " UPX ", PE_CONF_UPX , 1 }, <nl> { " PE ", " FSG ", PE_CONF_FSG , 1 }, <nl> - { " PE ", " SWIZZOR ", PE_CONF_SWIZZOR , 1 }, <nl> + { " PE ", " SWIZZOR ", PE_CONF_SWIZZOR , 0 }, <nl>  <nl> { " PE ", " PETITE ", PE_CONF_PETITE , 1 }, <nl> { " PE ", " PESPIN ", PE_CONF_PESPIN , 1 },
int cli_fmap_scandesc ( cli_ctx * ctx , cli_file_t ftype , uint8_t ftonly , struct cli <nl> type = ret ; <nl> } <nl>  <nl> - if ( hdb && ! SCAN_ALL ) { <nl> + if ( hdb ) { <nl> const void * data = buff + maxpatlen * ( offset != 0 ); <nl> uint32_t data_len = bytes - maxpatlen * ( offset != 0 ); <nl> 
long old_style_option ( int * argc , char ** argv ) <nl> lines = strtol_or_err ( argv [ i ] + 1 , <nl> _ (" failed to parse number of lines ")); <nl> nargs --; <nl> - memmove ( argv + i , argv + i + 1 , sizeof ( char *) * nargs ); <nl> + if ( nargs - i ) <nl> + memmove ( argv + i , argv + i + 1 , <nl> + sizeof ( char *) * ( nargs - i )); <nl> } else <nl> i ++; <nl> }
static void get_sem_elements ( struct sem_data * p ) <nl> { <nl> size_t i ; <nl>  <nl> - if (! p || ! p -> sem_nsems || p -> sem_perm . id < 0 ) <nl> + if (! p || ! p -> sem_nsems || p -> sem_nsems > SIZE_MAX || p -> sem_perm . id < 0 ) <nl> return ; <nl>  <nl> p -> elements = xcalloc ( p -> sem_nsems , sizeof ( struct sem_elem ));
void TorrentModel :: removeTorrent ( const QString & hash ) <nl> qDebug () << Q_FUNC_INFO << hash << row ; <nl> if ( row >= 0 ) { <nl> beginRemoveTorrent ( row ); <nl> + delete m_torrents [ row ]; <nl> m_torrents . removeAt ( row ); <nl> endRemoveTorrent (); <nl> }
static int readPackageHeaders ( FD_t fd , /*@ out @*/ struct rpmlead * leadPtr , <nl> int rpmReadPackageInfo ( FD_t fd , Header * sigp , Header * hdrp ) <nl> { <nl> int rc = readPackageHeaders ( fd , NULL , sigp , hdrp ); <nl> + if ( rc ) <nl> + return rc ; <nl> if ( hdrp && * hdrp && sigp && * sigp ) <nl> headerMergeLegacySigs (* hdrp , * sigp ); <nl> return rc ;
void rpmProblemSetFree ( rpmProblemSet probs ); <nl> void rpmProblemSetFilter ( rpmProblemSet ps , int flags ); <nl> int rpmRunTransactions ( rpmTransactionSet ts , rpmCallbackFunction notify , <nl> void * notifyData , rpmProblemSet okProbs , <nl> - rpmProblemSet * newProbs , int flags ); <nl> + rpmProblemSet * newProbs , int flags , int ignoreSet ); <nl>  <nl> # define RPMPROB_FILTER_IGNOREOS ( 1 << 0 ) <nl> # define RPMPROB_FILTER_IGNOREARCH ( 1 << 1 )
int rpmdbRebuild ( char * rootdir ) { <nl> " to recover ", dbpath , newdbpath ); <nl> return 1 ; <nl> } <nl> - rmdir ( newdbpath ); <nl> + if ( rmdir ( newdbpath )) <nl> + rpmMessage ( RPMERR_RMDIR , " failed to remove % s : % s \ n ", <nl> + newdbpath , strerror ( errno )); <nl> } <nl>  <nl> 
EXPORTED void conversations_rename_cidentry ( struct conversations_state * state , <nl> { <nl> struct rename_rock rrock ; <nl>  <nl> + if ( from == to ) return ; <nl> + <nl> memset (& rrock , 0 , sizeof ( rrock )); <nl> rrock . state = state ; <nl> rrock . from_cid = from ;
char ** reply ; <nl> r = connect ( s , ( struct sockaddr *)& srvaddr , sizeof ( srvaddr )); <nl> if ( r == - 1 ) { <nl> * reply = " cannot connect to pwcheck server "; <nl> - return 0 ; <nl> + return 1 ; <nl> } <nl>  <nl> iov [ 0 ]. iov_base = user ;
EXPORTED void jmap_closembox ( jmap_req_t * req , struct mailbox ** mboxp ) <nl> struct _mboxcache_rec * rec = NULL ; <nl> int i ; <nl>  <nl> + if (! mboxp || !* mboxp ) return ; <nl> + <nl> for ( i = 0 ; i < req -> mboxes -> count ; i ++) { <nl> rec = ( struct _mboxcache_rec *) ptrarray_nth ( req -> mboxes , i ); <nl> if ( rec -> mbox == * mboxp )
static int expunge_userflags ( struct mailbox * mailbox , struct expire_rock * erock ) <nl> for ( i = 0 ; i < MAX_USER_FLAGS ; i ++) { <nl> if ( erock -> userflags [ i / 32 ] & 1 <<( i & 31 )) <nl> continue ; <nl> + if (! mailbox -> flagname [ i ]) <nl> + continue ; <nl> if ( verbose ) <nl> fprintf ( stderr , " Expunging userflag % u (% s ) from % s \ n ", <nl> i , mailbox -> flagname [ i ], mailbox -> name );
EXPORTED void buf_ensure ( struct buf * buf , size_t n ) <nl> /* protect against wrap */ <nl> assert ( newlen >= buf -> len ); <nl>  <nl> + if ( buf -> alloc >= newlen ) <nl> + return ; <nl> + <nl> if ( buf -> alloc ) { <nl> buf -> s = xrealloc ( buf -> s , newlen ); <nl> }
static int expire_conversations ( const mbentry_t * mbentry , void * rock ) <nl> if ( mbentry -> mbtype & MBTYPE_REMOTE ) <nl> goto done ; <nl>  <nl> + if ( mboxname_isdeletedmailbox ( mbentry -> name , NULL )) <nl> + goto done ; <nl> + <nl> filename = conversations_getmboxpath ( mbentry -> name ); <nl> if (! filename ) <nl> goto done ;
static int mailbox_open_advanced ( const char * name , <nl> goto done ; <nl> } <nl>  <nl> + if (! mbentry -> partition ) { <nl> + mboxlist_entry_free (& mbentry ); <nl> + r = IMAP_MAILBOX_NONEXISTENT ; <nl> + goto done ; <nl> + } <nl> + <nl> mailbox -> part = xstrdup ( mbentry -> partition ); <nl>  <nl> /* Note that the header does have the ACL information , but it is only
void clusterCommand ( redisClient * c ) { <nl> addReplyError ( c ," Invalid CLUSTER SETSLOT action or number of arguments "); <nl> return ; <nl> } <nl> + clusterUpdateState (); <nl> clusterSaveConfigOrDie (); <nl> addReply ( c , shared . ok ); <nl> } else if (! strcasecmp ( c -> argv [ 1 ]-> ptr ," info ") && c -> argc == 2 ) {
size_t zmalloc_size ( void * ptr ) { <nl> return size + PREFIX_SIZE ; <nl> } <nl> size_t zmalloc_usable ( void * ptr ) { <nl> - return zmalloc_usable ( ptr )- PREFIX_SIZE ; <nl> + return zmalloc_size ( ptr )- PREFIX_SIZE ; <nl> } <nl> # endif <nl> 
static int setup_hwaccel ( AVCodecContext * avctx , <nl> return AVERROR ( ENOMEM ); <nl> } <nl>  <nl> + avctx -> hwaccel = hwa ; <nl> if ( hwa -> init ) { <nl> ret = hwa -> init ( avctx ); <nl> if ( ret < 0 ) { <nl> av_freep (& avctx -> internal -> hwaccel_priv_data ); <nl> + avctx -> hwaccel = NULL ; <nl> return ret ; <nl> } <nl> } <nl>  <nl> - avctx -> hwaccel = hwa ; <nl> - <nl> return 0 ; <nl> } <nl> 
static int mp3_write_header ( struct AVFormatContext * s ) <nl> char yeartxt [ 10 ]; <nl>  <nl> if ( s -> track ) <nl> - snprintf ( tracktxt , sizeof ( tracktxt ) - 1 , "% d ", s -> track ); <nl> + snprintf ( tracktxt , sizeof ( tracktxt ) , "% d ", s -> track ); <nl> if ( s -> year ) <nl> snprintf ( yeartxt , sizeof ( yeartxt ) , "% d ", s -> year ); <nl> 
int main ( int argc , char ** argv ) <nl>  <nl> ret = probe_file ( input_filename ); <nl>  <nl> + uninit_opts (); <nl> + av_dict_free (& fmt_entries_to_show ); <nl> + <nl> avformat_network_deinit (); <nl>  <nl> return ret ;
typedef struct FlacSubframe { <nl> int shift ; <nl> RiceContext rc ; <nl> int32_t samples [ FLAC_MAX_BLOCKSIZE ]; <nl> - int32_t residual [ FLAC_MAX_BLOCKSIZE ]; <nl> + int32_t residual [ FLAC_MAX_BLOCKSIZE + 1 ]; <nl> } FlacSubframe ; <nl>  <nl> typedef struct FlacFrame {
static av_cold int rl2_read_header ( AVFormatContext * s ) <nl> rate = avio_rl16 ( pb ); <nl> channels = avio_rl16 ( pb ); <nl> def_sound_size = avio_rl16 ( pb ); <nl> + if (! channels || channels > 42 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels : % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> /** setup video stream */ <nl> st = avformat_new_stream ( s , NULL );
static int mkv_write_header ( AVFormatContext * s ) <nl> // currently defined level 1 element <nl> mkv -> main_seekhead = mkv_start_seekhead ( pb , mkv -> segment_offset , 10 ); <nl> mkv -> cluster_seekhead = mkv_start_seekhead ( pb , mkv -> segment_offset , 0 ); <nl> + if ( mkv -> main_seekhead == NULL || mkv -> cluster_seekhead == NULL ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( mkv_add_seekhead_entry ( mkv -> main_seekhead , MATROSKA_ID_INFO , url_ftell ( pb )) < 0 ) <nl> return - 1 ;
static int mov_read_stsd ( MOVContext * c , ByteIOContext * pb , MOVAtom atom ) <nl> # endif <nl> /* no ifdef since parameters are always those */ <nl> case CODEC_ID_QCELP : <nl> + // force sample rate for qcelp when not stored in mov <nl> + if ( st -> codec -> codec_tag != MKTAG (' Q ',' c ',' l ',' p ')) <nl> + st -> codec -> sample_rate = 8000 ; <nl> st -> codec -> frame_size = 160 ; <nl> st -> codec -> channels = 1 ; /* really needed */ <nl> break ;
static int decode_channel_transform ( WMAProDecodeCtx * s ) <nl> if ( get_bits1 (& s -> gb )) { <nl> avpriv_request_sample ( s -> avctx , <nl> " Unknown channel transform type "); <nl> + return AVERROR_PATCHWELCOME ; <nl> } <nl> } else { <nl> chgroup -> transform = 1 ;
static av_cold int twin_decode_init ( AVCodecContext * avctx ) <nl> default : avctx -> sample_rate = isampf * 1000 ; break ; <nl> } <nl>  <nl> - if ( avctx -> channels > CHANNELS_MAX ) { <nl> + if ( avctx -> channels <= 0 || avctx -> channels > CHANNELS_MAX ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unsupported number of channels : % i \ n ", <nl> avctx -> channels ); <nl> return - 1 ;
static int mxf_get_sorted_table_segments ( MXFContext * mxf , int * nb_sorted_segment <nl> if ( mxf -> metadata_sets [ i ]-> type == IndexTableSegment ) <nl> nb_segments ++; <nl>  <nl> + if (! nb_segments ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> * sorted_segments = av_mallocz ( nb_segments * sizeof (** sorted_segments )); <nl> unsorted_segments = av_mallocz ( nb_segments * sizeof (* unsorted_segments )); <nl> if (! sorted_segments || ! unsorted_segments ) {
int av_metadata_set ( AVMetadata ** pm , const char * key , const char * value ) <nl> m -> elems [ m -> count ]. value = av_strdup ( value ); <nl> m -> count ++; <nl> } <nl> - if (! m -> count ) <nl> + if (! m -> count ) { <nl> + av_free ( m -> elems ); <nl> av_freep ( pm ); <nl> + } <nl>  <nl> return 0 ; <nl> }
cmd_connect ( gchar ** args , struct cmd_help_t help ) <nl> if ( stream ){ <nl> // Limit to READ_BUF_SIZE bytes to prevent overflows in the case of a poorly chosen command <nl> account -> password = g_malloc ( READ_BUF_SIZE ); <nl> + if (! account -> password ){ <nl> + log_error (" Failed to allocate enough memory to read eval_password output "); <nl> + return TRUE ; <nl> + } <nl> account -> password = fgets ( account -> password , READ_BUF_SIZE , stream ); <nl> pclose ( stream ); <nl> } else {
otp_hotp_mac ( const unsigned char counter [ 8 ], unsigned char output [ 7 ], <nl> /* 1 . hmac */ <nl> if (! HMAC ( EVP_sha1 (), keyblock , key_len , counter , 8 , hmac , & hmac_len ) || <nl> hmac_len != 20 ) { <nl> - otp_log ( OTP_LOG_ERR , "% s : HMAC failed : HMAC ", log_prefix ); <nl> + otp_log ( OTP_LOG_ERR , "% s : HMAC failed ", log_prefix ); <nl> return - 1 ; <nl> } <nl> 
static struct cmp * cmp ; <nl> /* <nl> * Compare 2 attributes . May call the attribute compare function . <nl> */ <nl> - int paircompare ( VALUE_PAIR * request , VALUE_PAIR * check , <nl> + static int paircompare ( VALUE_PAIR * request , VALUE_PAIR * check , <nl> VALUE_PAIR * check_pairs , VALUE_PAIR ** reply_pairs ) <nl> { <nl> int ret = - 2 ;
struct groupchat * jabber_chat_join ( struct im_connection * ic , char * room , char * <nl> node = xt_new_node ( " x ", NULL , NULL ); <nl> xt_add_attr ( node , " xmlns ", XMLNS_MUC ); <nl> node = jabber_make_packet ( " presence ", NULL , roomjid , node ); <nl> + if ( password ) <nl> + xt_add_child ( node , xt_new_node ( " password ", password , NULL ) ); <nl> jabber_cache_add ( ic , node , jabber_chat_join_failed ); <nl>  <nl> if ( ! jabber_write_packet ( ic , node ) )
int jabber_si_handle_request ( struct im_connection * ic , struct xt_node * node , st <nl> requestok = FALSE ; <nl> } <nl>  <nl> - * s = '/'; <nl> + if ( s ) <nl> + * s = '/'; <nl> } <nl> - else <nl> + <nl> + if ( ! requestok ) <nl> { <nl> reply = jabber_make_error_packet ( node , " item - not - found ", " cancel ", NULL ); <nl> if (! jabber_write_packet ( ic , reply ))
capture_start ( capture_options * capture_opts , capture_session * cap_session , void ( <nl> GString * source ; <nl>  <nl> cap_session -> state = CAPTURE_PREPARING ; <nl> + cap_session -> count = 0 ; <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_MESSAGE , " Capture Start ..."); <nl> source = get_iface_list_string ( capture_opts , IFLIST_SHOW_FILTER ); <nl> cf_set_tempfile_source (( capture_file *) cap_session -> cf , source -> str );
WSLUA_METAMETHOD Dir__call ( lua_State * L ) { <nl> const gchar * filename ; <nl> const char * ext ; <nl>  <nl> - if (! dir ) <nl> + if (! dir ) { <nl> luaL_argerror ( L , 1 ," must be a Dir "); <nl> + return 0 ; <nl> + } <nl>  <nl> if (! dir -> dir ) { <nl> return 0 ;
static void * mqtt_message_decode_copy_cb ( void * dest , const void * orig , size_t le <nl> const mqtt_message_decode_t * o = ( const mqtt_message_decode_t *) orig ; <nl> mqtt_message_decode_t * d = ( mqtt_message_decode_t *) dest ; <nl>  <nl> + d -> match_criteria = o -> match_criteria ; <nl> d -> topic_pattern = g_strdup ( o -> topic_pattern ); <nl> d -> payload_proto_name = g_strdup ( o -> payload_proto_name ); <nl> 
dissect_gryphon_message ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> /* <nl> * Indicate what kind of message this is . <nl> */ <nl> - col_set_str ( pinfo -> cinfo , COL_INFO , val_to_str ( frmtyp , frame_type , "- Invalid -")); <nl> + col_set_str ( pinfo -> cinfo , COL_INFO , val_to_str_const ( frmtyp , frame_type , "- Invalid -")); <nl> } <nl>  <nl> if ( tree == NULL )
static void init_wepkeys ( void ) { <nl>  <nl> # ifdef USE_ENV <nl> buf = ep_alloc ( 128 ); <nl> - sprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> + g_snprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> tmp = getenv ( buf ); <nl> # else <nl> tmp = wep_keystr [ i ];
clear_node_pr ( stat_node * n ) <nl> clear_node_pr ( c ); <nl> } <nl>  <nl> - if ( n -> pr -> iter ) { <nl> + if ( n -> pr && n -> pr -> iter ) { <nl> gtk_tree_store_remove ( n -> st -> pr -> store , n -> pr -> iter ); <nl> n -> pr -> iter = NULL ; <nl> }
file_open_error_message ( int err , gboolean for_writing ) <nl> break ; <nl> # endif <nl>  <nl> + case EINVAL : <nl> + errmsg = " The file \"% s \" could not be created because an invalid filename was specified ."; <nl> + break ; <nl> + <nl> default : <nl> g_snprintf ( errmsg_errno , sizeof ( errmsg_errno ), <nl> " The file \"%% s \" could not be % s : % s .",
peektagged_read_packet ( wtap * wth , FILE_T fh , struct wtap_pkthdr * phdr , <nl>  <nl> case TAG_PEEKTAGGED_CENTER_FREQUENCY : <nl> /* XXX - also seen in an EtherPeek capture ; value unknown */ <nl> + ieee_802_11 . presence_flags |= PHDR_802_11_HAS_FREQUENCY ; <nl> + ieee_802_11 . frequency = pletoh32 (& tag_value [ 2 ]); <nl> break ; <nl>  <nl> case TAG_PEEKTAGGED_UNKNOWN_0x000E :
decode_pdu_sns_delete ( build_info_t * bi ) { <nl> { NSIP_IE_IP4_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> { NSIP_IE_IP6_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> }; <nl> - decode_iei_transaction_id ( ies , bi , bi -> offset ); <nl> - decode_pdu_general (& ies [ 1 ], 3 , bi ); <nl> + decode_pdu_general ( ies , 1 , bi ); <nl> + decode_iei_transaction_id (& ies [ 1 ], bi , bi -> offset ); <nl> + decode_pdu_general (& ies [ 2 ], 3 , bi ); <nl> } <nl>  <nl> static void
void PacketList :: columnsChanged () <nl> setColumnVisibility (); <nl> create_far_overlay_ = true ; <nl> packet_list_model_ -> resetColumns (); <nl> + applyRecentColumnWidths (); <nl> columns_changed_ = false ; <nl> } <nl>  <nl> void PacketList :: setCaptureFile ( capture_file * cf ) <nl> cap_file_ = cf ; <nl> if ( cap_file_ && columns_changed_ ) { <nl> columnsChanged (); <nl> - applyRecentColumnWidths (); <nl> } <nl> packet_list_model_ -> setCaptureFile ( cf ); <nl> create_near_overlay_ = true ;
dissect_dcerpc_cn_bs_body ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> * it was just too short to tell and ask the TCP layer for more <nl> * data . */ <nl> pinfo -> desegment_offset = offset ; <nl> - pinfo -> desegment_len = sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset ); <nl> + pinfo -> desegment_len = ( guint32 )( sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset )); <nl> } else { <nl> /* Really not DCE - RPC */ <nl> break ;
PHP_FUNCTION ( str_repeat ) <nl> int l = 0 ; <nl> memcpy ( result , input_str , input_str_len ); <nl> s = result ; <nl> - e = result + input_str_len ; <nl> - ee = result + result_len ; <nl> + e = ( char *) result + input_str_len ; <nl> + ee = ( char *) result + result_len ; <nl>  <nl> while ( e < ee ) { <nl> l = ( e - s ) < ( ee - e ) ? ( e - s ) : ( ee - e );
PHPAPI php_stream_filter * php_stream_filter_create ( const char * filtername , zval <nl> /* try a wildcard */ <nl> char * wildname ; <nl>  <nl> - wildname = estrdup ( filtername ); <nl> + wildname = emalloc ( n + 3 ); <nl> + memcpy ( wildname , filtername , n + 1 ); <nl> period = wildname + ( period - filtername ); <nl> while ( period && ! filter ) { <nl> * period = '\ 0 ';
PHP_FUNCTION ( imap_utf8 ) <nl> if ( dest . data ) { <nl> free ( dest . data ); <nl> } <nl> + if ( src . data ) { <nl> + free ( src . data ); <nl> + } <nl> } <nl> /* }}} */ <nl> 
ZEND_API zend_mm_heap * zend_mm_startup ( void ) <nl> if ( zend_mm_low_bit ( seg_size ) != zend_mm_high_bit ( seg_size )) { <nl> fprintf ( stderr , " ZEND_MM_SEG_SIZE must be a power of two \ n "); <nl> exit ( 255 ); <nl> + } else if ( seg_size < ZEND_MM_ALIGNED_SEGMENT_SIZE + ZEND_MM_ALIGNED_HEADER_SIZE ) { <nl> + fprintf ( stderr , " ZEND_MM_SEG_SIZE is too small \ n "); <nl> + exit ( 255 ); <nl> } <nl> } else { <nl> seg_size = ZEND_MM_SEG_SIZE ;
PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl>  <nl> if ( php_check_open_basedir ( path_copy TSRMLS_CC )) { <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> return NULL ; <nl> } <nl> PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl> * opened_path = estrdup ( path_copy ); <nl> } <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> path_copy = NULL ; <nl> 
PHP_FUNCTION ( odbc_execute ) <nl>  <nl> /* Check for safe mode . */ <nl> if ( PG ( safe_mode ) && (! php_checkuid ( filename , NULL , CHECKUID_CHECK_FILE_AND_DIR ))) { <nl> - RETURN_FALSE ; <nl> - } <nl> + efree ( filename ); <nl> + efree ( params ); <nl> + RETURN_FALSE ; <nl> + } <nl>  <nl> /* Check the basedir */ <nl> if ( php_check_open_basedir ( filename TSRMLS_CC )) { <nl> + efree ( filename ); <nl> + efree ( params ); <nl> RETURN_FALSE ; <nl> } <nl> 
static zend_object * spl_filesystem_object_clone ( zval * zobject TSRMLS_DC ) <nl> old_object = Z_OBJ_P ( zobject ); <nl> source = ( spl_filesystem_object *) old_object ; <nl> new_object = spl_filesystem_object_new_ex ( old_object -> ce TSRMLS_CC ); <nl> + intern = ( spl_filesystem_object *) new_object ; <nl>  <nl> intern -> flags = source -> flags ; <nl> 
void gdi_Glyph_Free ( rdpContext * context , rdpGlyph * glyph ) <nl> gdi_SelectObject ( gdi_glyph -> hdc , ( HGDIOBJECT ) gdi_glyph -> org_bitmap ); <nl> gdi_DeleteObject (( HGDIOBJECT ) gdi_glyph -> bitmap ); <nl> gdi_DeleteDC ( gdi_glyph -> hdc ); <nl> - xfree ( gdi_glyph ); <nl> } <nl> } <nl> 
void ThreadingMsgListModel :: setSourceModel ( QAbstractItemModel * sourceModel ) <nl> this , SLOT ( handleRowsAboutToBeInserted ( const QModelIndex &, int , int ) ) ); <nl> connect ( sourceModel , SIGNAL ( rowsInserted ( const QModelIndex &, int , int ) ), <nl> this , SLOT ( handleRowsInserted ( const QModelIndex &, int , int ) ) ); <nl> + resetMe (); <nl> } <nl>  <nl> void ThreadingMsgListModel :: handleDataChanged ( const QModelIndex & topLeft , const QModelIndex & bottomRight )
int switch_root ( const char * new_root ) { <nl>  <nl> if ( fstat ( old_root_fd , & rb ) < 0 ) <nl> log_warning (" Failed to stat old root directory , leaving : % m "); <nl> - else <nl> + else { <nl> rm_rf_children ( old_root_fd , false , false , & rb ); <nl> + old_root_fd = - 1 ; <nl> + } <nl> } <nl>  <nl> r = 0 ;
int unit_file_get_list ( <nl> } <nl>  <nl> r = null_or_empty_path ( f -> path ); <nl> - if ( r < 0 ) { <nl> + if ( r < 0 && r != - ENOENT ) { <nl> free ( f -> path ); <nl> free ( f ); <nl> goto finish ;
int columns ( void ) { <nl> struct winsize ws ; <nl> zero ( ws ); <nl>  <nl> - if ( ioctl ( STDIN_FILENO , TIOCGWINSZ , & ws ) >= 0 ) <nl> + if ( ioctl ( STDOUT_FILENO , TIOCGWINSZ , & ws ) >= 0 ) <nl> parsed_columns = ws . ws_col ; <nl> } <nl> 
int udevdb_add_dev ( const char * path , const struct udevice * dev ) <nl> if (( path == NULL ) || ( dev == NULL )) <nl> return - ENODEV ; <nl>  <nl> - memset ( keystr , 0 , NAME_SIZE ); <nl> + memset ( keystr , 0 , SYSFS_PATH_MAX ); <nl> strfieldcpy ( keystr , path ); <nl> key . dptr = keystr ; <nl> key . dsize = strlen ( keystr ) + 1 ;
static int swap_add_device_links ( Swap * s ) { <nl> if (! s -> what ) <nl> return 0 ; <nl>  <nl> + if (! s -> from_fragment ) <nl> + return 0 ; <nl> + <nl> if ( is_device_path ( s -> what )) <nl> return unit_add_node_link ( UNIT ( s ), s -> what , UNIT ( s )-> manager -> running_as == SYSTEMD_SYSTEM ); <nl> else
int udev_device_set_syspath ( struct udev_device * udev_device , const char * syspath <nl> } <nl>  <nl> /* trailing number */ <nl> - while ( isdigit ( udev_device -> sysname [-- len ])) <nl> + while ( len > 0 && isdigit ( udev_device -> sysname [-- len ])) <nl> udev_device -> sysnum = & udev_device -> sysname [ len ]; <nl> + <nl> + /* sysname is completely numeric */ <nl> + if ( len == 0 ) <nl> + udev_device -> sysnum = NULL ; <nl> + <nl> return 0 ; <nl> } <nl> 
static char ** user_dirs ( <nl> const char * e ; <nl> _cleanup_strv_free_ char ** config_dirs = NULL , ** data_dirs = NULL ; <nl> _cleanup_free_ char * data_home = NULL ; <nl> - _cleanup_free_ char ** res = NULL ; <nl> + _cleanup_strv_free_ char ** res = NULL ; <nl> char ** tmp ; <nl> int r ; <nl> 
imath_rsa_public_decrypt ( int flen , const unsigned char * from , <nl> mp_int_clear (& us ); <nl>  <nl> /* head zero was skipped by mp_int_to_unsigned */ <nl> + if (* p == 0 ) <nl> + return - 7 ; <nl> if (* p != 1 ) <nl> return - 6 ; <nl> size --; p ++;
add_enc_ts_padata ( krb5_context context , <nl> if ( salt == NULL ) { <nl> /* default to standard salt */ <nl> ret = krb5_get_pw_salt ( context , client , & salt2 ); <nl> + if ( ret ) <nl> + return ret ; <nl> salt = & salt2 ; <nl> } <nl> if (! enctypes ) {
void shadow_client_refresh_rect ( rdpShadowClient * client , BYTE count , RECTANGLE_1 <nl> wParam = ( SHADOW_MSG_IN_REFRESH_OUTPUT *) calloc ( 1 , sizeof ( SHADOW_MSG_IN_REFRESH_OUTPUT )); <nl>  <nl> if (! wParam || ! areas ) <nl> + { <nl> + if ( wParam ) <nl> + free ( wParam ); <nl> return ; <nl> + } <nl>  <nl> wParam -> numRects = ( UINT32 ) count ; <nl> 
BOOL drive_file_set_information ( DRIVE_FILE * file , UINT32 FsInformationClass , UIN <nl> if (! fullpath ) <nl> { <nl> WLog_ERR ( TAG , " drive_file_combine_fullpath failed !"); <nl> + free ( s ); <nl> return FALSE ; <nl> } <nl> free ( s );
static BOOL autodetect_recv_bandwidth_measure_results ( rdpRdp * rdp , wStream * s , <nl> return FALSE ; <nl>  <nl> WLog_VRB ( AUTODETECT_TAG , " received Bandwidth Measure Results PDU "); <nl> + if ( Stream_GetRemainingLength ( s ) < 8 ) <nl> + return - 1 ; <nl> Stream_Read_UINT32 ( s , rdp -> autodetect -> bandwidthMeasureTimeDelta ); /* timeDelta ( 4 bytes ) */ <nl> Stream_Read_UINT32 ( s , rdp -> autodetect -> bandwidthMeasureByteCount ); /* byteCount ( 4 bytes ) */ <nl> 
BOOL tls_accept ( rdpTls * tls , const char * cert_file , const char * privatekey_file ) <nl>  <nl> BOOL tls_disconnect ( rdpTls * tls ) <nl> { <nl> + if (! tls ) <nl> + return FALSE ; <nl> + <nl> if ( tls -> ssl ) <nl> SSL_shutdown ( tls -> ssl ); <nl> 
DWORD GetTimeZoneInformation ( LPTIME_ZONE_INFORMATION lpTimeZoneInformation ) <nl>  <nl> /* 1 ... TIME_ZONE_ID_STANDARD <nl> * 2 ... TIME_ZONE_ID_DAYLIGHT */ <nl> - return dtz -> SupportsDST ? 2 : 1 ; <nl> + return local_time -> tm_isdst ? 2 : 1 ; <nl> } <nl> else <nl> {
BOOL freerdp_client_detect_command_line ( int argc , char ** argv , DWORD * flags ) <nl> return compatibility ; <nl>  <nl> /* Check , if this may be windows style syntax ... */ <nl> - if ( windows_cli_count && ( windows_cli_count >= posix_cli_count )) <nl> + if ( windows_cli_count && ( windows_cli_count >= posix_cli_count ) || ( windows_cli_status <= COMMAND_LINE_STATUS_PRINT )) <nl> { <nl> + windows_cli_count = 1 ; <nl> * flags = COMMAND_LINE_SEPARATOR_COLON ; <nl> * flags |= COMMAND_LINE_SIGIL_SLASH | COMMAND_LINE_SIGIL_PLUS_MINUS ; <nl> }
static void rfx_compose_message_frame_begin ( RFX_CONTEXT * context , STREAM * data_o <nl> stream_write_uint8 ( data_out , 0 ); /* CodecChannelT . channelId */ <nl> stream_write_uint32 ( data_out , context -> frame_idx ); /* frameIdx */ <nl> stream_write_uint16 ( data_out , 1 ); /* numRegions */ <nl> + <nl> + context -> frame_idx ++; <nl> } <nl>  <nl> static void rfx_compose_message_region ( RFX_CONTEXT * context , STREAM * data_out ,
gdImageScaleTwoPass ( const gdImagePtr src , const unsigned int new_width , <nl> }/* if */ <nl>  <nl> if ( src != tmp_im ) { <nl> - gdFree ( tmp_im ); <nl> + gdImageDestroy ( tmp_im ); <nl> }/* if */ <nl>  <nl> return dst ;
namespace cling { <nl> // Compile the wrapper code . <nl> // <nl> const llvm :: GlobalValue * GV = 0 ; <nl> + if (! getCodeGenerator ()) <nl> + return 0 ; <nl> + <nl> if ( ifUnique ) <nl> GV = getCodeGenerator ()-> GetModule ()-> getNamedValue ( name ); <nl> 
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * insamplesref ) <nl> AVFilterBufferRef * outsamplesref = ff_get_audio_buffer ( outlink , AV_PERM_WRITE , n_out ); <nl> int ret ; <nl>  <nl> + if (! outsamplesref ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> avfilter_copy_buffer_ref_props ( outsamplesref , insamplesref ); <nl> outsamplesref -> format = outlink -> format ; <nl> outsamplesref -> audio -> channel_layout = outlink -> channel_layout ;
static void close_slaves ( AVFormatContext * avf ) <nl> } <nl> } <nl> av_freep (& tee -> slaves [ i ]. stream_map ); <nl> + av_freep (& tee -> slaves [ i ]. bsfs ); <nl>  <nl> avio_close ( avf2 -> pb ); <nl> avf2 -> pb = NULL ;
header_fail : <nl> c -> height = 0 ; <nl> c -> tiles_x = <nl> c -> tiles_y = 0 ; <nl> + c -> tile_width = <nl> + c -> tile_height = 0 ; <nl> return ret ; <nl> } <nl> 
static int add_candidate_ref ( HEVCContext * s , RefPicList * list , <nl> { <nl> HEVCFrame * ref = find_ref_idx ( s , poc ); <nl>  <nl> - if ( ref == s -> ref ) <nl> + if ( ref == s -> ref || list -> nb_refs >= HEVC_MAX_REFS ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (! ref ) {
static int mov_read_stts ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> static void mov_update_dts_shift ( MOVStreamContext * sc , int duration ) <nl> { <nl> if ( duration < 0 ) { <nl> + if ( duration == INT_MIN ) { <nl> + av_log ( NULL , AV_LOG_WARNING , " mov_update_dts_shift (): dts_shift set to % d \ n ", INT_MAX ); <nl> + duration ++; <nl> + } <nl> sc -> dts_shift = FFMAX ( sc -> dts_shift , - duration ); <nl> } <nl> }
static int build_table ( VLC * vlc , int table_nb_bits , int nb_codes , <nl> VLC_TYPE (* table )[ 2 ]; <nl>  <nl> table_size = 1 << table_nb_bits ; <nl> + if ( table_nb_bits > 30 ) <nl> + return - 1 ; <nl> table_index = alloc_table ( vlc , table_size , flags & INIT_VLC_USE_NEW_STATIC ); <nl> av_dlog ( NULL , " new table index =% d size =% d \ n ", table_index , table_size ); <nl> if ( table_index < 0 )
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf_size = FFMIN ( buf_size , s -> max_framesize - s -> bitstream_size ); <nl> input_buf_size = buf_size ; <nl>  <nl> - if ( s -> bitstream_index + s -> bitstream_size + buf_size > <nl> + if ( s -> bitstream_index + s -> bitstream_size + buf_size + FF_INPUT_BUFFER_PADDING_SIZE > <nl> s -> allocated_bitstream_size ) { <nl> memmove ( s -> bitstream , & s -> bitstream [ s -> bitstream_index ], <nl> s -> bitstream_size );
int ff_MPV_lowest_referenced_row ( MpegEncContext * s , int dir ) <nl> int my_max = INT_MIN , my_min = INT_MAX , qpel_shift = ! s -> quarter_sample ; <nl> int my , off , i , mvs ; <nl>  <nl> - if ( s -> picture_structure != PICT_FRAME ) goto unhandled ; <nl> + if ( s -> picture_structure != PICT_FRAME || s -> mcsel ) goto unhandled ; <nl>  <nl> switch ( s -> mv_type ) { <nl> case MV_TYPE_16X16 :
void uninit_opts ( void ) <nl> av_freep (& avformat_opts -> key ); <nl> av_freep (& avformat_opts ); <nl> # if CONFIG_SWSCALE <nl> - av_freep (& sws_opts ); <nl> + sws_freeContext ( sws_opts ); <nl> + sws_opts = NULL ; <nl> # endif <nl> for ( i = 0 ; i < opt_name_count ; i ++) { <nl> // opt_values are only stored for codec - specific options in which case
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> url_fseek ( s -> pb , st -> index_entries [ index ]. pos , SEEK_SET ); <nl> matroska -> skip_to_keyframe = !( flags & AVSEEK_FLAG_ANY ); <nl> matroska -> skip_to_stream = st ; <nl> + matroska -> done = 0 ; <nl> av_update_cur_dts ( s , st , st -> index_entries [ index ]. timestamp ); <nl> return 0 ; <nl> }
static int decode_subframe_fixed ( FLACContext * s , int32_t * decoded , <nl> int pred_order , int bps ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> - int a , b , c , d , i ; <nl> + int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl>  <nl> /* warm up samples */ <nl> for ( i = 0 ; i < pred_order ; i ++) {
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> start_subband += start_subband - 7 ; <nl> end_subband = get_bits ( gbc , 3 ) + 5 ; <nl> # if USE_FIXED <nl> - s -> spx_dst_end_freq = end_freq_inv_tab [ end_subband ]; <nl> + s -> spx_dst_end_freq = end_freq_inv_tab [ end_subband - 5 ]; <nl> # endif <nl> if ( end_subband > 7 ) <nl> end_subband += end_subband - 7 ;
resync : <nl> /* for AC3 , needs to swap bytes */ <nl> if ( st -> codec -> codec_id == CODEC_ID_AC3 ) { <nl> ptr = pkt -> data ; <nl> - for ( j = 0 ; j < len ; j += 2 ) { <nl> + for ( j = 0 ; j < pkt -> size ; j += 2 ) { <nl> FFSWAP ( int , ptr [ 0 ], ptr [ 1 ]); <nl> ptr += 2 ; <nl> }
static int smka_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " channels mismatch \ n "); <nl> return AVERROR ( EINVAL ); <nl> } <nl> - if ( bits && avctx -> sample_fmt == AV_SAMPLE_FMT_U8 ) { <nl> + if ( bits == ( avctx -> sample_fmt == AV_SAMPLE_FMT_U8 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " sample format mismatch \ n "); <nl> return AVERROR ( EINVAL ); <nl> }
static int64_t get_bit_rate ( AVCodecContext * ctx ) <nl> break ; <nl> case AVMEDIA_TYPE_AUDIO : <nl> bits_per_sample = av_get_bits_per_sample ( ctx -> codec_id ); <nl> - bit_rate = bits_per_sample ? ctx -> sample_rate * ctx -> channels * bits_per_sample : ctx -> bit_rate ; <nl> + bit_rate = bits_per_sample ? ctx -> sample_rate * ( int64_t ) ctx -> channels * bits_per_sample : ctx -> bit_rate ; <nl> break ; <nl> default : <nl> bit_rate = 0 ;
static int execute_decode_slices ( H264Context * h , int context_count ) <nl> if ( context_count == 1 ) { <nl> return decode_slice ( avctx , & h ); <nl> } else { <nl> + av_assert0 ( context_count > 0 ); <nl> for ( i = 1 ; i < context_count ; i ++) { <nl> hx = h -> thread_context [ i ]; <nl> hx -> s . err_recognition = avctx -> err_recognition ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> # else <nl> if ( s -> xch_present && ! s -> xch_disable ) { <nl> # endif <nl> + if ( avctx -> channel_layout & AV_CH_BACK_CENTER ) { <nl> + avpriv_request_sample ( avctx , " XCh with Back center channel "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> avctx -> channel_layout |= AV_CH_BACK_CENTER ; <nl> if ( s -> lfe ) { <nl> avctx -> channel_layout |= AV_CH_LOW_FREQUENCY ;
int attribute_align_arg avcodec_open2 ( AVCodecContext * avctx , const AVCodec * code <nl> avctx -> time_base = av_inv_q ( av_mul_q ( avctx -> framerate , ( AVRational ){ avctx -> ticks_per_frame , 1 })); <nl> # endif <nl> } <nl> + if ( codec -> priv_data_size > 0 && avctx -> priv_data && codec -> priv_class ) { <nl> + av_assert0 (*( const AVClass **) avctx -> priv_data == codec -> priv_class ); <nl> + } <nl> + <nl> end : <nl> ff_unlock_avcodec (); <nl> if ( options ) {
static void id3v1_create_tag ( AVFormatContext * s , uint8_t * buf ) <nl>  <nl> static int mp3_read_probe ( AVProbeData * p ) <nl> { <nl> - int max_frames , first_frames ; <nl> + int max_frames , first_frames = 0 ; <nl> int fsize , frames , sample_rate ; <nl> uint32_t header ; <nl> uint8_t * buf , * buf2 , * end ;
int av_aes_init ( AVAES * a , const uint8_t * key , int key_bits , int decrypt ) { <nl> uint8_t log8 [ 256 ]; <nl> uint8_t alog8 [ 512 ]; <nl>  <nl> - if (! enc_multbl [ 0 ][ sizeof ( enc_multbl )/ sizeof ( enc_multbl [ 0 ][ 0 ])- 1 ]){ <nl> + if (! enc_multbl [ FF_ARRAY_ELEMS ( enc_multbl )- 1 ][ FF_ARRAY_ELEMS ( enc_multbl [ 0 ])- 1 ]){ <nl> j = 1 ; <nl> for ( i = 0 ; i < 255 ; i ++){ <nl> alog8 [ i ]=
static void vmd_decode ( VmdVideoContext * s , AVFrame * frame ) <nl> palette32 [ i ] = ( r << 16 ) | ( g << 8 ) | ( b ); <nl> } <nl> } <nl> - s -> size -= ( 256 * 3 + 2 ); <nl> + s -> size -= PALETTE_COUNT * 3 + 2 ; <nl> } <nl> if ( s -> size > 0 ) { <nl> /* originally UnpackFrame in VAG ' s code */
static int cinvideo_decode_frame ( AVCodecContext * avctx , <nl> bitmap_frame_size = buf_size - 4 ; <nl>  <nl> /* handle palette */ <nl> + if ( bitmap_frame_size < palette_colors_count * ( 3 + ( palette_type != 0 ))) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( palette_type == 0 ) { <nl> for ( i = 0 ; i < palette_colors_count ; ++ i ) { <nl> cin -> palette [ i ] = bytestream_get_le24 (& buf );
static void fill_coding_method_array ( sb_int8_array tone_level_idx , <nl> for ( j = 0 ; j < 64 ; j ++) <nl> acc += tone_level_idx_temp [ ch ][ sb ][ j ]; <nl>  <nl> - multres = 0x66666667 * ( acc * 10 ); <nl> + multres = 0x66666667LL * ( acc * 10 ); <nl> esp_40 = ( multres >> 32 ) / 8 + (( multres & 0xffffffff ) >> 31 ); <nl> for ( ch = 0 ; ch < nb_channels ; ch ++) <nl> for ( sb = 0 ; sb < 30 ; sb ++)
static int tak_decode_frame ( AVCodecContext * avctx , void * data , <nl> uint8_t * samples = ( uint8_t *) frame -> extended_data [ chan ]; <nl> int32_t * decoded = s -> decoded [ chan ]; <nl> for ( i = 0 ; i < s -> nb_samples ; i ++) <nl> - samples [ i ] = decoded [ i ] + 0x80 ; <nl> + samples [ i ] = decoded [ i ] + 0x80U ; <nl> } <nl> break ; <nl> case AV_SAMPLE_FMT_S16P :
int avio_close_dyn_buf ( AVIOContext * s , uint8_t ** pbuffer ) <nl> static const char padbuf [ FF_INPUT_BUFFER_PADDING_SIZE ] = { 0 }; <nl> int padding = 0 ; <nl>  <nl> + if (! s ) { <nl> + * pbuffer = NULL ; <nl> + return 0 ; <nl> + } <nl> + <nl> /* don ' t attempt to pad fixed - size packet buffers */ <nl> if (! s -> max_packet_size ) { <nl> avio_write ( s , padbuf , sizeof ( padbuf ));
typedef struct Jpeg2000TgtNode { <nl> } Jpeg2000TgtNode ; <nl>  <nl> typedef struct Jpeg2000CodingStyle { <nl> - uint8_t nreslevels ; // number of resolution levels <nl> - uint8_t nreslevels2decode ; // number of resolution levels to decode <nl> + int nreslevels ; // number of resolution levels <nl> + int nreslevels2decode ; // number of resolution levels to decode <nl> uint8_t log2_cblk_width , <nl> log2_cblk_height ; // exponent of codeblock size <nl> uint8_t transform ; // DWT type
static int alac_decode_frame ( AVCodecContext * avctx , void * data , <nl> avpkt -> size * 8 - get_bits_count (& alac -> gb )); <nl> } <nl>  <nl> - * got_frame_ptr = 1 ; <nl> + if ( alac -> channels == ch ) <nl> + * got_frame_ptr = 1 ; <nl>  <nl> return avpkt -> size ; <nl> }
static int opus_decode_packet ( AVCodecContext * avctx , void * data , <nl> memset ( frame -> extended_data [ i ], 0 , frame -> linesize [ 0 ]); <nl> } <nl>  <nl> - if ( c -> gain_i ) { <nl> + if ( c -> gain_i && decoded_samples > 0 ) { <nl> c -> fdsp -> vector_fmul_scalar (( float *) frame -> extended_data [ i ], <nl> ( float *) frame -> extended_data [ i ], <nl> c -> gain , FFALIGN ( decoded_samples , 8 ));
static inline void tm2_apply_deltas ( TM2Context * ctx , int * Y , int stride , int * de <nl> } <nl> } <nl>  <nl> - static inline void tm2_high_chroma ( int * data , int stride , int * last , int * CD , int * deltas ) <nl> + static inline void tm2_high_chroma ( int * data , int stride , int * last , unsigned * CD , int * deltas ) <nl> { <nl> int i , j ; <nl> for ( j = 0 ; j < 2 ; j ++) {
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl> vseq -> seq_fields . bits . direct_8x8_inference_flag = 1 ; <nl> vseq -> seq_fields . bits . log2_max_frame_num_minus4 = 4 ; <nl> vseq -> seq_fields . bits . pic_order_cnt_type = 0 ; <nl> + vseq -> seq_fields . bits . log2_max_pic_order_cnt_lsb_minus4 = <nl> + av_clip ( av_log2 ( avctx -> max_b_frames + 1 ) - 2 , 0 , 12 ); <nl>  <nl> if ( avctx -> width != ctx -> surface_width || <nl> avctx -> height != ctx -> surface_height ) {
int ff_raw_read_partial_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> - pkt -> size = ret ; <nl> + av_shrink_packet ( pkt , ret ); <nl> return ret ; <nl> } <nl> 
static int rv20_decode_picture_header ( RVDecContext * rv ) <nl> return - 1 ; <nl> } <nl>  <nl> + if ( s -> low_delay && s -> pict_type == AV_PICTURE_TYPE_B ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " low delay B \ n "); <nl> + return - 1 ; <nl> + } <nl> if ( s -> last_picture_ptr == NULL && s -> pict_type == AV_PICTURE_TYPE_B ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " early B pix \ n "); <nl> return - 1 ;
static int flv_set_video_codec ( AVFormatContext * s , AVStream * vstream , int flv_co <nl> vcodec -> codec_id = CODEC_ID_VP6A ; <nl> if ( vcodec -> extradata_size != 1 ) { <nl> vcodec -> extradata_size = 1 ; <nl> - vcodec -> extradata = av_malloc ( 1 ); <nl> + vcodec -> extradata = av_malloc ( 1 + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> } <nl> vcodec -> extradata [ 0 ] = avio_r8 ( s -> pb ); <nl> return 1 ; // 1 byte body size adjustment for flv_read_packet ()
int ff_h263_decode_mb ( MpegEncContext * s , <nl> } <nl>  <nl> if ( IS_DIRECT ( mb_type )){ <nl> + if (! s -> pp_time ) <nl> + return AVERROR_INVALIDDATA ; <nl> s -> mv_dir = MV_DIR_FORWARD | MV_DIR_BACKWARD | MV_DIRECT ; <nl> mb_type |= ff_mpeg4_set_direct_mv ( s , 0 , 0 ); <nl> } else {
static int set_hwframe_ctx ( AVCodecContext * ctx , AVBufferRef * hw_device_ctx ) <nl> if (( err = av_hwframe_ctx_init ( hw_frames_ref )) < 0 ) { <nl> fprintf ( stderr , " Failed to initialize VAAPI frame context ." <nl> " Error code : % s \ n ", av_err2str ( err )); <nl> + av_buffer_unref (& hw_frames_ref ); <nl> return err ; <nl> } <nl> ctx -> hw_frames_ctx = av_buffer_ref ( hw_frames_ref );
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> // update precincts size : 2 ^ n value <nl> reslevel -> log2_prec_width = codsty -> log2_prec_widths [ reslevelno ]; <nl> reslevel -> log2_prec_height = codsty -> log2_prec_heights [ reslevelno ]; <nl> + if (! reslevel -> log2_prec_width || ! reslevel -> log2_prec_height ) { <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> /* Number of bands for each resolution level */ <nl> if ( reslevelno == 0 )
fail : <nl> } <nl>  <nl> if ( ret < 0 ) { <nl> + if ( codec -> object ) { <nl> + (* env )-> DeleteGlobalRef ( env , codec -> object ); <nl> + } <nl> ff_jni_reset_jfields ( env , & codec -> jfields , jni_amediacodec_mapping , 1 , codec ); <nl> av_freep (& codec ); <nl> }
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
typedef struct { <nl> static const AVOption amerge_options [] = { <nl> { " inputs ", " specify the number of inputs ", OFFSET ( nb_inputs ), <nl> AV_OPT_TYPE_INT , { . dbl = 2 }, 2 , SWR_CH_MAX }, <nl> + { 0 } <nl> }; <nl>  <nl> static const AVClass amerge_class = {
static void vc1_decode_b_mb_intfi ( VC1Context * v ) <nl> int fwd ; <nl> int dmv_x [ 2 ], dmv_y [ 2 ], pred_flag [ 2 ]; <nl> int bmvtype = BMV_TYPE_BACKWARD ; <nl> - int idx_mbmode , interpmvp ; <nl> + int idx_mbmode ; <nl> + int av_uninit ( interpmvp ); <nl>  <nl> mquant = v -> pq ; /* Lossy initialization */ <nl> s -> mb_intra = 0 ;
int ff_dwt_encode ( DWTContext * s , void * t ) <nl>  <nl> int ff_dwt_decode ( DWTContext * s , void * t ) <nl> { <nl> + if ( s -> ndeclevels == 0 ) <nl> + return 0 ; <nl> + <nl> switch ( s -> type ) { <nl> case FF_DWT97 : <nl> dwt_decode97_float ( s , t );
static int videotoolbox_buffer_create ( AVCodecContext * avctx , AVFrame * frame ) <nl> vtctx -> cached_hw_frames_ctx = hw_frames_ctx ; <nl> } <nl>  <nl> - av_assert0 (! frame -> hw_frames_ctx ); <nl> + av_buffer_unref (& frame -> hw_frames_ctx ); <nl> frame -> hw_frames_ctx = av_buffer_ref ( vtctx -> cached_hw_frames_ctx ); <nl> if (! frame -> hw_frames_ctx ) <nl> return AVERROR ( ENOMEM );
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> av_log ( s , AV_LOG_ERROR , " No seek table at given position \ n "); <nl> return ; <nl> } <nl> + if ( size < 0 || size >= INT_MAX / 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Bad seek table size \ n "); <nl> + return ; <nl> + } <nl> if (!( buf = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ))) <nl> return ; <nl> avio_read ( s -> pb , buf , size );
static int raw_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> & pal_size ); <nl> int ret ; <nl>  <nl> - if ( pal_size != AVPALETTE_SIZE ) { <nl> + if ( pal && pal_size != AVPALETTE_SIZE ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Palette size % d is wrong \ n ", pal_size ); <nl> pal = NULL ; <nl> }
static int ipvideo_decode_block_opcode_0x9 ( IpvideoContext * s , AVFrame * frame ) <nl> int x , y ; <nl> unsigned char P [ 4 ]; <nl>  <nl> + if ( bytestream2_get_bytes_left (& s -> stream_ptr ) < 8 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too little data for opcode 0x9 \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* 4 - color encoding */ <nl> bytestream2_get_buffer (& s -> stream_ptr , P , 4 ); <nl> 
int ff_jpegls_decode_picture ( MJpegDecodeContext * s , int near , <nl> else <nl> shift = point_transform + ( 16 - s -> bits ); <nl>  <nl> + if ( shift >= 16 ) { <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto end ; <nl> + } <nl> + <nl> if ( s -> avctx -> debug & FF_DEBUG_PICT_INFO ) { <nl> av_log ( s -> avctx , AV_LOG_DEBUG , <nl> " JPEG - LS params : % ix % i NEAR =% i MV =% i T (% i ,% i ,% i ) "
static void vp56_decode_mb ( VP56Context * s , int row , int col , int is_alpha ) <nl>  <nl> frame_current = s -> framep [ VP56_FRAME_CURRENT ]; <nl> frame_ref = s -> framep [ ref_frame ]; <nl> + if ( mb_type != VP56_MB_INTRA && ! frame_ref -> data [ 0 ]) <nl> + return ; <nl>  <nl> ab = 6 * is_alpha ; <nl> b_max = 6 - 2 * is_alpha ;
static int config_input ( AVFilterLink * inlink ) <nl> int hsub = desc -> log2_chroma_w ; <nl> int vsub = desc -> log2_chroma_h ; <nl>  <nl> + av_freep (& s -> buf ); <nl> s -> buf = av_mallocz (( FFALIGN ( inlink -> w , 16 ) * ( s -> radius + 1 ) / 2 + 32 ) * sizeof ( uint16_t )); <nl> if (! s -> buf ) <nl> return AVERROR ( ENOMEM );
static char * value_string ( char * buf , int buf_size , struct unit_value uv ) <nl> const char * prefix_string = ""; <nl> int l ; <nl>  <nl> - if ( use_value_prefix ) { <nl> + if ( use_value_prefix && vald > 1 ) { <nl> long long int index ; <nl>  <nl> if ( uv . unit == unit_byte_str && use_byte_value_binary_prefix ) {
typedef struct Context { <nl>  <nl> static int cmp ( const void * key , const void * node ) <nl> { <nl> - return (*( const int64_t *) key ) - (( const CacheEntry *) node )-> logical_pos ; <nl> + return FFDIFFSIGN (*( const int64_t *) key , (( const CacheEntry *) node )-> logical_pos ); <nl> } <nl>  <nl> static int cache_open ( URLContext * h , const char * arg , int flags , AVDictionary ** options )
static int pcm_dvd_parse_header ( AVCodecContext * avctx , const uint8_t * header ) <nl> /* early exit if the header didn ' t change apart from the frame number */ <nl> if ( s -> last_header == header_int ) <nl> return 0 ; <nl> + s -> last_header = - 1 ; <nl>  <nl> if ( avctx -> debug & FF_DEBUG_PICT_INFO ) <nl> av_dlog ( avctx , " pcm_dvd_parse_header : header = % 02x % 02x % 02x \ n ",
static void qdm2_fft_decode_tones ( QDM2Context * q , int duration , GetBitContext * <nl> return ; <nl>  <nl> local_int_14 = ( offset >> local_int_8 ); <nl> + if ( local_int_14 >= FF_ARRAY_ELEMS ( fft_level_index_table )) <nl> + return ; <nl>  <nl> if ( q -> nb_channels > 1 ) { <nl> channel = get_bits1 ( gb );
static av_cold void RENAME ( sws_init_swScale )( SwsContext * c ) <nl> enum PixelFormat srcFormat = c -> srcFormat , <nl> dstFormat = c -> dstFormat ; <nl>  <nl> - if (! is16BPS ( dstFormat ) && ! is9_OR_10BPS ( dstFormat )) { <nl> + if (! is16BPS ( dstFormat ) && ! is9_OR_10BPS ( dstFormat ) && <nl> + dstFormat != PIX_FMT_NV12 && dstFormat != PIX_FMT_NV21 ) { <nl> if (!( c -> flags & SWS_BITEXACT )) { <nl> if ( c -> flags & SWS_ACCURATE_RND ) { <nl> c -> yuv2yuv1 = RENAME ( yuv2yuv1_ar );
static void event_loop ( VideoState * cur_stream ) <nl> } else { <nl> pos = get_master_clock ( cur_stream ); <nl> pos += incr ; <nl> + if ( cur_stream -> ic -> start_time != AV_NOPTS_VALUE && pos < cur_stream -> ic -> start_time / ( double ) AV_TIME_BASE ) <nl> + pos = cur_stream -> ic -> start_time / ( double ) AV_TIME_BASE ; <nl> stream_seek ( cur_stream , ( int64_t )( pos * AV_TIME_BASE ), ( int64_t )( incr * AV_TIME_BASE ), 0 ); <nl> } <nl> break ;
AVFilterFormats * avfilter_merge_formats ( AVFilterFormats * a , AVFilterFormats * b ) <nl> AVFilterFormats * ret ; <nl> unsigned i , j , k = 0 ; <nl>  <nl> + if ( a == b ) return a ; <nl> + <nl> ret = av_mallocz ( sizeof ( AVFilterFormats )); <nl>  <nl> /* merge list of formats */
static int64_t asf_read_pts ( AVFormatContext * s , int stream_index , <nl>  <nl> // assert (( asf_st -> packet_pos - s -> data_offset ) % s -> packet_size == 0 ); <nl> pos = asf_st -> packet_pos ; <nl> + av_assert1 ( pkt -> pos == asf_st -> packet_pos ); <nl>  <nl> av_add_index_entry ( s -> streams [ i ], pos , pts , pkt -> size , <nl> pos - start_pos [ i ] + 1 , AVINDEX_KEYFRAME );
static int read_connect ( URLContext * s , RTMPContext * rt ) <nl> return ret ; <nl>  <nl> // Chunk size <nl> - if (( ret = ff_rtmp_packet_create (& pkt , RTMP_SYSTEM_CHANNEL , <nl> + if (( ret = ff_rtmp_packet_create (& pkt , RTMP_NETWORK_CHANNEL , <nl> RTMP_PT_CHUNK_SIZE , 0 , 4 )) < 0 ) <nl> return ret ; <nl> 
static int flashsv2_prime ( FlashSVContext * s , uint8_t * src , <nl> s -> zstream . avail_out = s -> block_size * 3 ; <nl> inflate (& s -> zstream , Z_SYNC_FLUSH ); <nl>  <nl> - deflateInit (& zs , 0 ); <nl> + if ( deflateInit (& zs , 0 ) != Z_OK ) <nl> + return - 1 ; <nl> zs . next_in = s -> tmpblock ; <nl> zs . avail_in = s -> block_size * 3 - s -> zstream . avail_out ; <nl> zs . next_out = s -> deflate_block ;
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> * In complex cases , there are style descriptors appended to the string <nl> * so we can ' t just assume the packet size is the string size . <nl> */ <nl> - end = ptr + FFMAX ( 2 + AV_RB16 ( ptr ), avpkt -> size ); <nl> + end = ptr + FFMIN ( 2 + AV_RB16 ( ptr ), avpkt -> size ); <nl> ptr += 2 ; <nl>  <nl> ts_start = av_rescale_q ( avpkt -> pts ,
static int matroska_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> ret = matroska_parse_cluster ( matroska ); <nl> } <nl>  <nl> + if ( ret == AVERROR_INVALIDDATA ) { <nl> + pkt -> flags |= AV_PKT_FLAG_CORRUPT ; <nl> + return 0 ; <nl> + } <nl> + <nl> return ret ; <nl> } <nl> 
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> bytestream_put_le16 (& buf , 0 ); <nl> bytestream_put_le32 (& buf , 0 ); <nl>  <nl> - if (( ret = avio_read ( pb , buf , image -> size )) < 0 ) <nl> + if (( ret = avio_read ( pb , buf , image -> size )) < 0 ) { <nl> + av_packet_unref ( pkt ); <nl> return ret ; <nl> + } <nl>  <nl> st -> codecpar -> bits_per_coded_sample = AV_RL16 ( buf + 14 ); <nl> 
int ff_jni_exception_get_summary ( JNIEnv * env , jthrowable exception , char ** error <nl> jclass exception_class = NULL ; <nl> jmethodID get_message_id = NULL ; <nl>  <nl> - jstring string ; <nl> + jstring string = NULL ; <nl>  <nl> av_bprint_init (& bp , 0 , AV_BPRINT_SIZE_AUTOMATIC ); <nl> 
static inline void copy ( LZOContext * c , int cnt ) <nl> */ <nl> static inline void copy_backptr ( LZOContext * c , int back , int cnt ) <nl> { <nl> - register const uint8_t * src = & c -> out [- back ]; <nl> register uint8_t * dst = c -> out ; <nl> - if ( src < c -> out_start || src > dst ) { <nl> + if ( dst - c -> out_start < back ) { <nl> c -> error |= AV_LZO_INVALID_BACKPTR ; <nl> return ; <nl> }
static int bitpacked_decode_yuv422p10 ( AVCodecContext * avctx , AVFrame * frame , <nl> AVPacket * avpkt ) <nl> { <nl> uint64_t frame_size = ( uint64_t ) avctx -> width * ( uint64_t ) avctx -> height * 20 ; <nl> - uint64_t packet_size = avpkt -> size * 8 ; <nl> + uint64_t packet_size = ( uint64_t ) avpkt -> size * 8 ; <nl> GetBitContext bc ; <nl> uint16_t * y , * u , * v ; <nl> int ret , i ;
static int mov_finalize_stsd_codec ( MOVContext * c , AVIOContext * pb , <nl>  <nl> static int mov_skip_multiple_stsd ( MOVContext * c , AVIOContext * pb , <nl> int codec_tag , int format , <nl> - int size ) <nl> + int64_t size ) <nl> { <nl> int video_codec_id = ff_codec_get_id ( ff_codec_movvideo_tags , format ); <nl> 
static void compute_chapters_end ( AVFormatContext * s ) <nl> unsigned int i , j ; <nl> int64_t max_time = 0 ; <nl>  <nl> - if ( s -> duration > 0 ) <nl> + if ( s -> duration > 0 && s -> start_time < INT64_MAX - s -> duration ) <nl> max_time = s -> duration + <nl> (( s -> start_time == AV_NOPTS_VALUE ) ? 0 : s -> start_time ); <nl> 
static int ism_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> SmoothStreamingContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ pkt -> stream_index ]; <nl> - int64_t end_dts = ( c -> nb_fragments + 1 ) * c -> min_frag_duration ; <nl> + int64_t end_dts = ( c -> nb_fragments + 1LL ) * c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
int ff_combine_frame ( ParseContext * pc , int next , const uint8_t ** buf , int * buf_s <nl> if (! new_buffer ) <nl> return AVERROR ( ENOMEM ); <nl> pc -> buffer = new_buffer ; <nl> + if ( FF_INPUT_BUFFER_PADDING_SIZE > - next ) <nl> memcpy (& pc -> buffer [ pc -> index ], * buf , next + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> pc -> index = 0 ; <nl> * buf = pc -> buffer ;
static int rm_write_header ( AVFormatContext * s ) <nl> int n ; <nl> AVCodecContext * codec ; <nl>  <nl> + if ( s -> nb_streams > 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " At most 2 streams are currently supported for muxing in RM \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> for ( n = 0 ; n < s -> nb_streams ; n ++) { <nl> s -> streams [ n ]-> id = n ; <nl> codec = s -> streams [ n ]-> codec ;
ASSStyle * ff_ass_style_get ( ASSSplitContext * ctx , const char * style ) <nl> if (! style || !* style ) <nl> style = " Default "; <nl> for ( i = 0 ; i < ass -> styles_count ; i ++) <nl> - if (! strcmp ( ass -> styles [ i ]. name , style )) <nl> + if ( ass -> styles [ i ]. name && ! strcmp ( ass -> styles [ i ]. name , style )) <nl> return ass -> styles + i ; <nl> return NULL ; <nl> }
the_end : <nl> } <nl> } <nl> } <nl> - if ( s -> flipped ) { <nl> + if ( s -> flipped && ! s -> rgb ) { <nl> int j ; <nl> avcodec_get_chroma_sub_sample ( s -> avctx -> pix_fmt , & hshift , & vshift ); <nl> av_assert0 ( s -> nb_components == av_pix_fmt_count_planes ( s -> picture_ptr -> format ));
typedef struct AMRWBContext { <nl> } AMRWBContext ; <nl>  <nl> static const AVOption options [] = { <nl> - { " dtx ", " Allow DTX ( generate comfort noise )", offsetof ( AMRWBContext , allow_dtx ), FF_OPT_TYPE_INT , 0 , 0 , 1 , AV_OPT_FLAG_AUDIO_PARAM | AV_OPT_FLAG_ENCODING_PARAM }, <nl> + { " dtx ", " Allow DTX ( generate comfort noise )", offsetof ( AMRWBContext , allow_dtx ), FF_OPT_TYPE_INT , { 0 }, 0 , 1 , AV_OPT_FLAG_AUDIO_PARAM | AV_OPT_FLAG_ENCODING_PARAM }, <nl> { NULL } <nl> }; <nl> 
static int encode_frame ( AVCodecContext * avctx , unsigned char * buf , <nl> return - 1 ; <nl> } <nl> if (! is_yuv ) <nl> - s -> bpp_tab_size = ( s -> bpp >> 3 ); <nl> + s -> bpp_tab_size = (( s -> bpp + 7 ) >> 3 ); <nl>  <nl> if ( s -> compr == TIFF_DEFLATE || s -> compr == TIFF_ADOBE_DEFLATE || s -> compr == TIFF_LZW ) <nl> // best choose for DEFLATE
static int mov_write_header ( AVFormatContext * s ) <nl> AVStream * st = s -> streams [ i ]; <nl> if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO || <nl> st -> codec -> codec_type == AVMEDIA_TYPE_AUDIO ) { <nl> - ff_mov_init_hinting ( s , hint_track , i ); <nl> + if ( ff_mov_init_hinting ( s , hint_track , i ) < 0 ) <nl> + goto error ; <nl> hint_track ++; <nl> } <nl> }
static int update_frame_pool ( AVCodecContext * avctx , AVFrame * frame ) <nl> break ; <nl> } <nl> case AVMEDIA_TYPE_AUDIO : { <nl> - int ch = av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> + int ch = av_frame_get_channels ( frame ); // av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> int planar = av_sample_fmt_is_planar ( frame -> format ); <nl> int planes = planar ? ch : 1 ; <nl> 
static int start_frame ( AVFilterLink * link , AVFilterBufferRef * picref ) <nl>  <nl> av_assert0 ( picref ); <nl>  <nl> + if ( picref -> video -> h < 3 || picref -> video -> w < 3 ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Video of less than 3 columns or lines is not supported \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> if ( yadif -> frame_pending ) <nl> return_frame ( ctx , 1 ); <nl> 
int check_stream_specifier ( AVFormatContext * s , AVStream * st , const char * spec ) <nl> case ' a ': type = AVMEDIA_TYPE_AUDIO ; break ; <nl> case ' s ': type = AVMEDIA_TYPE_SUBTITLE ; break ; <nl> case ' d ': type = AVMEDIA_TYPE_DATA ; break ; <nl> + default : abort (); // never reached , silence warning <nl> } <nl> if ( type != st -> codec -> codec_type ) <nl> return 0 ;
static int opus_decode_frame ( OpusStreamContext * s , const uint8_t * data , int size <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Error resampling SILK data .\ n "); <nl> return samples ; <nl> } <nl> + av_assert2 (( samples & 7 ) == 0 ); <nl> s -> delayed_samples += s -> packet . frame_duration - samples ; <nl> } else <nl> ff_silk_flush ( s -> silk );
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl> else s -> avctx -> pix_fmt = AV_PIX_FMT_YUV420P16 ; <nl> s -> avctx -> color_range = s -> cs_itu601 ? AVCOL_RANGE_MPEG : AVCOL_RANGE_JPEG ; <nl> if ( pix_fmt_id == 0x42111100 ) { <nl> + if ( s -> bits > 8 ) <nl> + goto unk_pixfmt ; <nl> s -> upscale_h = 6 ; <nl> } else if ( pix_fmt_id == 0x24111100 ) { <nl> + if ( s -> bits > 8 ) <nl> + goto unk_pixfmt ; <nl> s -> upscale_v = 6 ; <nl> } <nl> break ;
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> offset = matroska_decode_buffer (& pkt_data ,& pkt_size , track ); <nl> if ( offset < 0 ) <nl> continue ; <nl> + av_assert0 ( offset + pkt_size >= pkt_size ); <nl> } <nl>  <nl> pkt = av_mallocz ( sizeof ( AVPacket ));
static int sbr_make_f_master ( AACContext * ac , SpectralBandReplication * sbr , <nl> max_qmf_subbands = 35 ; <nl> } else if ( sbr -> sample_rate >= 48000 ) <nl> max_qmf_subbands = 32 ; <nl> + else <nl> + av_assert0 ( 0 ); <nl>  <nl> if ( sbr -> k [ 2 ] - sbr -> k [ 0 ] > max_qmf_subbands ) { <nl> av_log ( ac -> avctx , AV_LOG_ERROR ,
static av_cold int read_specific_config ( ALSDecContext * ctx ) <nl> GetBitContext gb ; <nl> uint64_t ht_size ; <nl> int i , config_offset ; <nl> - MPEG4AudioConfig m4ac ; <nl> + MPEG4AudioConfig m4ac = { 0 }; <nl> ALSSpecificConfig * sconf = & ctx -> sconf ; <nl> AVCodecContext * avctx = ctx -> avctx ; <nl> uint32_t als_id , header_size , trailer_size ;
fixup_vorbis_headers ( AVFormatContext * as , struct oggvorbis_private * priv , <nl> len = priv -> len [ 0 ] + priv -> len [ 1 ] + priv -> len [ 2 ]; <nl> buf_len = len + len / 255 + 64 ; <nl> ptr = * buf = av_realloc ( NULL , buf_len ); <nl> + if (!* buf ) <nl> + return 0 ; <nl> memset (* buf , '\ 0 ', buf_len ); <nl>  <nl> ptr [ 0 ] = 2 ;
void mpeg_motion_internal ( MpegEncContext * s , <nl> { <nl> uint8_t * ptr_y , * ptr_cb , * ptr_cr ; <nl> int dxy , uvdxy , mx , my , src_x , src_y , <nl> - uvsrc_x , uvsrc_y , v_edge_pos , uvlinesize , linesize ; <nl> + uvsrc_x , uvsrc_y , v_edge_pos ; <nl> + ptrdiff_t uvlinesize , linesize ; <nl>  <nl> # if 0 <nl> if ( s -> quarter_sample )
static int read_header ( AVFormatContext * s ) <nl> avio_skip ( s -> pb , 1 ); // padding <nl>  <nl> st -> codec -> sample_rate = bfstm ? read32 ( s ) : read16 ( s ); <nl> - if (! st -> codec -> sample_rate ) <nl> + if ( st -> codec -> sample_rate <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (! bfstm )
FF_ENABLE_DEPRECATION_WARNINGS <nl> if ( avctx -> level > 0 ) <nl> x4 -> params . i_level_idc = avctx -> level ; <nl>  <nl> - x4 -> params . rc . f_rate_tolerance = <nl> - ( float ) avctx -> bit_rate_tolerance / avctx -> bit_rate ; <nl> + if ( avctx -> bit_rate > 0 ) <nl> + x4 -> params . rc . f_rate_tolerance = <nl> + ( float ) avctx -> bit_rate_tolerance / avctx -> bit_rate ; <nl>  <nl> if (( avctx -> rc_buffer_size ) && <nl> ( avctx -> rc_initial_buffer_occupancy <= avctx -> rc_buffer_size )) {
AV_WN32A ( v0 + i , r ); \ <nl> AV_WN32A ( v1 + i , r ); \ <nl> } \ <nl> - for ( i = 0 ; i < BUF_SIZE * 8 / 3 ; i += 4 ) { \ <nl> + for ( i = 0 ; i < width * 8 / 3 ; i += 4 ) { \ <nl> uint32_t r = rnd (); \ <nl> AV_WN32A ( dst0 + i , r ); \ <nl> AV_WN32A ( dst1 + i , r ); \
static void dmix_sub_c ( int32_t * dst , const int32_t * src , int coeff , ptrdiff_t le <nl> int i ; <nl>  <nl> for ( i = 0 ; i < len ; i ++) <nl> - dst [ i ] -= mul15 ( src [ i ], coeff ); <nl> + dst [ i ] -= ( unsigned ) mul15 ( src [ i ], coeff ); <nl> } <nl>  <nl> static void dmix_add_c ( int32_t * dst , const int32_t * src , int coeff , ptrdiff_t len )
static int dpcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl> case CODEC_ID_SOL_DPCM : <nl> if ( avctx -> codec_tag != 3 ) { <nl> - uint8_t * output_samples_u8 = data ; <nl> + uint8_t * output_samples_u8 = s -> frame . data [ 0 ]; <nl> while ( buf < buf_end ) { <nl> uint8_t n = * buf ++; <nl> 
static int xv_write_header ( AVFormatContext * s ) <nl> if ( XvQueryAdaptors ( xv -> display , DefaultRootWindow ( xv -> display ), & num_adaptors , & ai ) != Success ) <nl> return AVERROR_EXTERNAL ; <nl> xv -> xv_port = ai [ 0 ]. base_id ; <nl> + XvFreeAdaptorInfo ( ai ); <nl>  <nl> if ( encctx -> pix_fmt != AV_PIX_FMT_YUV420P ) { <nl> av_log ( s , AV_LOG_ERROR ,
static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> AVIOContext * pbx ; <nl> unsigned char * buffer = NULL ; <nl> int buffer_size = 0 ; <nl> - const ID3v2EMFunc * extra_func ; <nl> + const ID3v2EMFunc * extra_func = NULL ; <nl> unsigned char * compressed_buffer = NULL ; <nl> int compressed_buffer_size = 0 ; <nl> 
static int read_braindead_odml_indx ( AVFormatContext * s , int frame_num ){ <nl> longs_pre_entry , index_type , entries_in_use , chunk_id , base ); <nl> # endif <nl>  <nl> - if ( stream_id > s -> nb_streams || stream_id < 0 ) <nl> + if ( stream_id >= s -> nb_streams || stream_id < 0 ) <nl> return - 1 ; <nl> st = s -> streams [ stream_id ]; <nl> ast = st -> priv_data ;
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> + av_shrink_packet ( pkt , CDXL_HEADER_SIZE + ret ); <nl> pkt -> stream_index = cdxl -> video_stream_index ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> pkt -> pos = pos ;
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame_ptr , <nl> uint32_t header ; <nl> int out_size ; <nl>  <nl> - while ( buf_size && !* buf ) <nl> + while ( buf_size && !* buf ){ <nl> buf ++; <nl> + buf_size --; <nl> + } <nl>  <nl> if ( buf_size < HEADER_SIZE ) <nl> return AVERROR_INVALIDDATA ;
static enum CodecID vfw_codecid ( DWORD biCompression ) <nl> switch ( biCompression ) { <nl> case MKTAG (' d ', ' v ', ' s ', ' d '): <nl> return CODEC_ID_DVVIDEO ; <nl> + case MKTAG (' M ', ' J ', ' P ', ' G '): <nl> + case MKTAG (' m ', ' j ', ' p ', ' g '): <nl> + return CODEC_ID_MJPEG ; <nl> } <nl> return CODEC_ID_NONE ; <nl> }
int main ( int argc , char ** argv ) <nl> k = av_tree_find ( root , ( void *)( j + 1 ), cmp , NULL ); <nl> if ( k ) <nl> av_log ( NULL , AV_LOG_ERROR , " removal failure % d \ n ", i ); <nl> + av_free ( node2 ); <nl> } <nl> } <nl> + av_free ( node ); <nl>  <nl> av_tree_destroy ( root ); <nl> 
int ff_hevc_parse_sps ( HEVCSPS * sps , GetBitContext * gb , unsigned int * sps_id , <nl> return 0 ; <nl>  <nl> err : <nl> - return ret ; <nl> + return ret < 0 ? ret : AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> int ff_hevc_decode_nal_sps ( GetBitContext * gb , AVCodecContext * avctx ,
static int dss_read_close ( AVFormatContext * s ) <nl> { <nl> DSSDemuxContext * ctx = s -> priv_data ; <nl>  <nl> - av_free ( ctx -> dss_sp_buf ); <nl> + av_freep (& ctx -> dss_sp_buf ); <nl>  <nl> return 0 ; <nl> }
static int read_gab2_sub ( AVFormatContext * s , AVStream * st , AVPacket * pkt ) <nl> goto error ; <nl>  <nl> if (! avformat_open_input (& ast -> sub_ctx , "", sub_demuxer , NULL )) { <nl> + if ( ast -> sub_ctx -> nb_streams != 1 ) <nl> + goto error ; <nl> ff_read_packet ( ast -> sub_ctx , & ast -> sub_pkt ); <nl> avcodec_parameters_copy ( st -> codecpar , ast -> sub_ctx -> streams [ 0 ]-> codecpar ); <nl> time_base = ast -> sub_ctx -> streams [ 0 ]-> time_base ;
static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> av_log ( avctx , AV_LOG_ERROR , " Buffer contains IP frames !\ n "); <nl> } <nl>  <nl> + if ( ctx -> frame_type >= FRAMETYPE_NULL_FIRST ) <nl> + return buf_size ; <nl> + <nl> if ( ctx -> frame . data [ 0 ]) <nl> avctx -> release_buffer ( avctx , & ctx -> frame ); <nl> 
static int asfrtp_parse_packet ( AVFormatContext * s , PayloadContext * asf , <nl> int prev_len = out_len ; <nl> out_len += cur_len ; <nl> asf -> buf = av_realloc ( asf -> buf , out_len ); <nl> + if (! asf -> buf || FFMIN ( cur_len , len - off )< 0 ) <nl> + return - 1 ; <nl> memcpy ( asf -> buf + prev_len , buf + off , <nl> FFMIN ( cur_len , len - off )); <nl> avio_skip ( pb , cur_len );
static int seq_fill_buffer ( SeqDemuxContext * seq , ByteIOContext * pb , int buffer_n <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> seq_buffer = & seq -> frame_buffers [ buffer_num ]; <nl> - if ( seq_buffer -> fill_size + data_size > seq_buffer -> data_size ) <nl> + if ( seq_buffer -> fill_size + data_size > seq_buffer -> data_size || data_size <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> url_fseek ( pb , seq -> current_frame_offs + data_offs , SEEK_SET );
static int matroska_parse_laces ( MatroskaDemuxContext * matroska , uint8_t ** buf , <nl> } <nl>  <nl> case 0x2 : /* fixed - size lacing */ <nl> - if ( size != ( size / * laces ) * size ) { <nl> + if ( size % (* laces )) { <nl> res = AVERROR_INVALIDDATA ; <nl> break ; <nl> }
static int read_sbr_grid ( AACContext * ac , SpectralBandReplication * sbr , <nl> if ( ch_data -> bs_frame_class == FIXFIX ) { <nl> idx = ch_data -> bs_num_env >> 1 ; <nl> } else if ( ch_data -> bs_frame_class & 1 ) { // FIXVAR or VARVAR <nl> - idx = ch_data -> bs_num_env - FFMAX ( bs_pointer - 1 , 1 ); <nl> + idx = ch_data -> bs_num_env - FFMAX (( int ) bs_pointer - 1 , 1 ); <nl> } else { // VARFIX <nl> if (! bs_pointer ) <nl> idx = 1 ;
static int64_t read_ts ( char ** line , int * duration ) <nl> int64_t start , end ; <nl>  <nl> if ( sscanf (* line , "%" SCNd64 ",%" SCNd64 , & start , & end ) == 2 ) { <nl> - * line += strcspn (* line , "\"") + 1 ; <nl> + * line += strcspn (* line , "\""); <nl> + * line += !!** line ; <nl> * duration = end - start ; <nl> return start ; <nl> }
static int hls_write_trailer ( struct AVFormatContext * s ) <nl> av_freep (& vs -> baseurl ); <nl> } <nl>  <nl> + ff_format_io_close ( s , & hls -> m3u8_out ); <nl> + ff_format_io_close ( s , & hls -> sub_m3u8_out ); <nl> av_freep (& hls -> key_basename ); <nl> av_freep (& hls -> var_streams ); <nl> av_freep (& hls -> master_m3u8_url );
static int get_pcm ( HEVCContext * s , int x , int y ) <nl>  <nl> # define TC_CALC ( qp , bs ) \ <nl> tctable [ av_clip (( qp ) + DEFAULT_INTRA_TC_OFFSET * (( bs ) - 1 ) + \ <nl> - ( tc_offset >> 1 << 1 ), \ <nl> + ( tc_offset & - 2 ), \ <nl> 0 , MAX_QP + DEFAULT_INTRA_TC_OFFSET )] <nl>  <nl> static void deblocking_filter_CTB ( HEVCContext * s , int x0 , int y0 )
static int smvjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_siz <nl> s -> picture [ 1 ]-> linesize [ i ] = mjpeg_data -> linesize [ i ]; <nl>  <nl> ret = av_frame_ref ( data , s -> picture [ 1 ]); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> } <nl>  <nl> - return ret ; <nl> + return avpkt -> size ; <nl> } <nl>  <nl> static const AVClass smvjpegdec_class = {
static int encode_apng ( AVCodecContext * avctx , AVPacket * pkt , <nl> return AVERROR ( ENOMEM ); <nl>  <nl> if ( avctx -> frame_number == 0 ) { <nl> + if (! pict ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> s -> bytestream = avctx -> extradata = av_malloc ( FF_MIN_BUFFER_SIZE ); <nl> if (! avctx -> extradata ) <nl> return AVERROR ( ENOMEM );
static int aac_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> } <nl> start_ch += chans ; <nl> } <nl> - if (( ret = ff_alloc_packet2 ( avctx , avpkt , 768 * s -> channels ))) { <nl> + if (( ret = ff_alloc_packet2 ( avctx , avpkt , 8192 * s -> channels ))) { <nl> av_log ( avctx , AV_LOG_ERROR , " Error getting output packet \ n "); <nl> return ret ; <nl> }
static int mp3_write_xing ( AVFormatContext * s ) <nl> } <nl>  <nl> /* dummy MPEG audio header */ <nl> - header = 0xff << 24 ; // sync <nl> + header = 0xffU << 24 ; // sync <nl> header |= ( 0x7 << 5 | ver << 3 | 0x1 << 1 | 0x1 ) << 16 ; // sync / audio - version / layer 3 / no crc */ <nl> header |= ( srate_idx << 2 ) << 8 ; <nl> header |= channels << 6 ;
static int vid_probe ( AVProbeData * p ) <nl> if ( AV_RL32 ( p -> buf ) != MKTAG (' V ', ' I ', ' D ', 0 )) <nl> return 0 ; <nl>  <nl> + if ( p -> buf [ 4 ] != 2 ) <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
static int mxf_read_header ( AVFormatContext * s ) <nl> } <nl> if ( res < 0 ) { <nl> av_log ( s , AV_LOG_ERROR , " error reading header metadata \ n "); <nl> - return res ; <nl> + ret = res ; <nl> + goto fail ; <nl> } <nl> break ; <nl> } else {
int ff_jni_init_jfields ( JNIEnv * env , void * jfields , const struct FFJniField * jfi <nl>  <nl> last_clazz = *( jclass *)(( uint8_t *) jfields + jfields_mapping [ i ]. offset ) = <nl> global ? (* env )-> NewGlobalRef ( env , clazz ) : clazz ; <nl> + <nl> + if ( global ) { <nl> + (* env )-> DeleteLocalRef ( env , clazz ); <nl> + } <nl> + <nl> } else { <nl>  <nl> if (! last_clazz ) {
vbv_retry : <nl> if ( s -> mb_info ) <nl> av_packet_shrink_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , s -> mb_info_size ); <nl> } else { <nl> - assert (( put_bits_ptr (& s -> pb ) == s -> pb . buf )); <nl> s -> frame_bits = 0 ; <nl> } <nl> assert (( s -> frame_bits & 7 ) == 0 );
static void dump_stream_format ( AVFormatContext * ic , int i , <nl> int g = av_gcd ( st -> time_base . num , st -> time_base . den ); <nl> AVDictionaryEntry * lang = av_dict_get ( st -> metadata , " language ", NULL , 0 ); <nl>  <nl> + if (! g ) <nl> + g = 1 ; <nl> + <nl> avcodec_string ( buf , sizeof ( buf ), st -> codec , is_output ); <nl> av_log ( NULL , AV_LOG_INFO , " Stream #% d :% d ", index , i ); <nl> 
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * buf ) <nl>  <nl> merge -> frame_requested = 0 ; <nl> draw_frame ( ctx , main_buf , alpha_buf ); <nl> - ff_filter_frame ( ctx -> outputs [ 0 ], avfilter_ref_buffer ( main_buf , ~ 0 )); <nl> + ff_filter_frame ( ctx -> outputs [ 0 ], main_buf ); <nl> avfilter_unref_buffer ( alpha_buf ); <nl> } <nl> return 0 ;
matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , int size , <nl> memcpy ( pkt -> data , matroska -> tracks [ track ]-> encoding_settings , offset ); <nl> memcpy ( pkt -> data + offset , pkt_data , pkt_size ); <nl>  <nl> + if ( pkt_data != data ) <nl> + av_free ( pkt_data ); <nl> + <nl> if ( n == 0 ) <nl> pkt -> flags = is_keyframe ; <nl> pkt -> stream_index = stream_index ;
int attribute_align_arg avcodec_decode_audio4 ( AVCodecContext * avctx , <nl> frame -> extended_buf = NULL ; <nl> frame -> nb_extended_buf = 0 ; <nl> } <nl> - } <nl> - <nl> - if ( ret < 0 && frame -> data [ 0 ]) <nl> + } else if ( frame -> data [ 0 ]) <nl> av_frame_unref ( frame ); <nl> } <nl> 
static int libopenjpeg_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> case AV_PIX_FMT_GBRP14 : <nl> case AV_PIX_FMT_GBRP16 : <nl> gbrframe = av_frame_alloc (); <nl> + if (! gbrframe ) <nl> + return AVERROR ( ENOMEM ); <nl> av_frame_ref ( gbrframe , frame ); <nl> gbrframe -> data [ 0 ] = frame -> data [ 2 ]; // swap to be rgb <nl> gbrframe -> data [ 1 ] = frame -> data [ 0 ];
static int decode_frame ( WmallDecodeCtx * s ) <nl>  <nl> /* decode all subframes */ <nl> while (! s -> parsed_all_subframes ) { <nl> + int decoded_samples = s -> channel [ 0 ]. decoded_samples ; <nl> if ( decode_subframe ( s ) < 0 ) { <nl> s -> packet_loss = 1 ; <nl> + if ( s -> frame -> nb_samples ) <nl> + s -> frame -> nb_samples = decoded_samples ; <nl> return 0 ; <nl> } <nl> }
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl> vseq -> seq_fields . bits . direct_8x8_inference_flag = 1 ; <nl> vseq -> seq_fields . bits . log2_max_frame_num_minus4 = 4 ; <nl> vseq -> seq_fields . bits . pic_order_cnt_type = 0 ; <nl> + vseq -> seq_fields . bits . log2_max_pic_order_cnt_lsb_minus4 = <nl> + av_clip ( av_log2 ( avctx -> max_b_frames + 1 ) - 2 , 0 , 12 ); <nl>  <nl> if ( avctx -> width != ctx -> surface_width || <nl> avctx -> height != ctx -> surface_height ) {
int av_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , <nl> " Dropped corrupted packet ( stream = % d )\ n ", <nl> pkt -> stream_index ); <nl> + av_free_packet ( pkt ); <nl> continue ; <nl> } <nl> 
int ff_mjpeg_find_marker ( MJpegDecodeContext * s , <nl> put_bits (& pb , 8 , x ); <nl> if ( x == 0xFF ) { <nl> x = src [ b ++]; <nl> + if ( x & 0x80 ) { <nl> + av_log ( s -> avctx , AV_LOG_WARNING , " Invalid escape sequence \ n "); <nl> + x &= 0x7f ; <nl> + } <nl> put_bits (& pb , 7 , x ); <nl> bit_count --; <nl> }
static int decode_pivot ( MSS1Context * ctx , ArithCoder * acoder , int base ) <nl> val = arith_get_number ( acoder , ( base + 1 ) / 2 - 2 ) + 3 ; <nl> } <nl>  <nl> - if ( val == base ) { <nl> + if (( unsigned ) val >= base ) { <nl> ctx -> corrupted = 1 ; <nl> return 0 ; <nl> }
static int swf_probe ( AVProbeData * p ) <nl> && AV_RB24 ( p -> buf ) != AV_RB24 (" FWS ")) <nl> return 0 ; <nl>  <nl> + if ( AV_RB24 ( p -> buf ) == AV_RB24 (" CWS ") <nl> + && p -> buf [ 3 ] <= 20 ) <nl> + return AVPROBE_SCORE_MAX / 4 + 1 ; <nl> + <nl> init_get_bits8 (& gb , p -> buf + 3 , p -> buf_size - 3 ); <nl>  <nl> skip_bits (& gb , 40 );
static int asf_write_header ( AVFormatContext * s ) <nl> * It is needed to use asf as a streamable format . */ <nl> if ( asf_write_header1 ( s , 0 , DATA_HEADER_SIZE ) < 0 ) { <nl> // av_free ( asf ); <nl> + av_freep (& asf -> index_ptr ); <nl> return - 1 ; <nl> } <nl> 
static int raw_decode ( AVCodecContext * avctx , <nl> AVFrame * frame = ( AVFrame *) data ; <nl> AVPicture * picture = ( AVPicture *) data ; <nl>  <nl> + frame -> pict_type = avctx -> coded_frame -> pict_type ; <nl> frame -> interlaced_frame = avctx -> coded_frame -> interlaced_frame ; <nl> frame -> top_field_first = avctx -> coded_frame -> top_field_first ; <nl> frame -> reordered_opaque = avctx -> reordered_opaque ;
static int film_read_header ( AVFormatContext * s ) <nl> film -> audio_samplerate = AV_RB16 (& scratch [ 24 ]); <nl> film -> audio_channels = scratch [ 21 ]; <nl> film -> audio_bits = scratch [ 22 ]; <nl> - if ( scratch [ 23 ] == 2 ) <nl> + if ( scratch [ 23 ] == 2 && film -> audio_channels > 0 ) <nl> film -> audio_type = AV_CODEC_ID_ADPCM_ADX ; <nl> else if ( film -> audio_channels > 0 ) { <nl> if ( film -> audio_bits == 8 )
yuv2mono_X_c_template ( SwsContext * c , const int16_t * lumFilter , <nl> const uint8_t * const d128 = dither_8x8_220 [ y & 7 ]; <nl> uint8_t * g = c -> table_gU [ 128 ] + c -> table_gV [ 128 ]; <nl> int i ; <nl> - int acc = 0 ; <nl> + unsigned acc = 0 ; <nl>  <nl> for ( i = 0 ; i < dstW - 1 ; i += 2 ) { <nl> int j ;
void decode_mb_mode ( VP8Context * s , VP8Macroblock * mb , int mb_x , int mb_y , uint8_ <nl>  <nl> if ( s -> segmentation . update_map ) <nl> * segment = vp8_rac_get_tree ( c , vp8_segmentid_tree , s -> prob -> segmentid ); <nl> - else <nl> + else if ( s -> segmentation . enabled ) <nl> * segment = ref ? * ref : * segment ; <nl> s -> segment = * segment ; <nl> 
static int parse_picture_segment ( AVCodecContext * avctx , <nl> ctx -> pictures [ picture_id ]. w = width ; <nl> ctx -> pictures [ picture_id ]. h = height ; <nl>  <nl> - av_fast_malloc (& ctx -> pictures [ picture_id ]. rle , & ctx -> pictures [ picture_id ]. rle_buffer_size , rle_bitmap_len ); <nl> + av_fast_padded_malloc (& ctx -> pictures [ picture_id ]. rle , & ctx -> pictures [ picture_id ]. rle_buffer_size , rle_bitmap_len ); <nl>  <nl> if (! ctx -> pictures [ picture_id ]. rle ) <nl> return - 1 ;
static int mov_read_close ( AVFormatContext * s ) <nl> for ( i = 0 ; i < mov -> total_streams ; i ++) <nl> mov_free_stream_context ( mov -> streams [ i ]); <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) <nl> - av_free ( s -> streams [ i ]); <nl> + av_freep (& s -> streams [ i ]); <nl> return 0 ; <nl> } <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> if (! fifo -> root . next ) { <nl> if (( ret = ff_request_frame ( outlink -> src -> inputs [ 0 ])) < 0 ) <nl> return ret ; <nl> + av_assert0 ( fifo -> root . next ); <nl> } <nl>  <nl> /* by doing this , we give ownership of the reference to the next filter ,
static av_cold int ulti_decode_init ( AVCodecContext * avctx ) <nl> s -> width = avctx -> width ; <nl> s -> height = avctx -> height ; <nl> s -> blocks = ( s -> width / 8 ) * ( s -> height / 8 ); <nl> + if ( s -> blocks == 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV410P ; <nl> s -> ulti_codebook = ulti_codebook ; <nl> 
static int decode_frame_png ( AVCodecContext * avctx , <nl> } <nl>  <nl> if (( ret = av_frame_ref ( data , s -> picture . f )) < 0 ) <nl> - return ret ; <nl> + goto the_end ; <nl>  <nl> * got_frame = 1 ; <nl> 
static int mxf_read_primer_pack ( MXFContext * mxf ) <nl>  <nl> static int mxf_add_metadata_set ( MXFContext * mxf , void * metadata_set ) <nl> { <nl> + if ( mxf -> metadata_sets_count + 1 >= UINT_MAX / sizeof (* mxf -> metadata_sets )) <nl> + return AVERROR ( ENOMEM ); <nl> mxf -> metadata_sets = av_realloc ( mxf -> metadata_sets , ( mxf -> metadata_sets_count + 1 ) * sizeof (* mxf -> metadata_sets )); <nl> if (! mxf -> metadata_sets ) <nl> return - 1 ;
static int asf_write_header ( AVFormatContext * s ) <nl> * It is needed to use asf as a streamable format . */ <nl> if ( asf_write_header1 ( s , 0 , DATA_HEADER_SIZE ) < 0 ) { <nl> // av_free ( asf ); <nl> + av_freep (& asf -> index_ptr ); <nl> return - 1 ; <nl> } <nl> 
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> int i , t , seekd ; <nl> GetBitContext gb ; <nl>  <nl> + if ( s -> nb_streams <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " cannot parse stream table before stream header \ n "); <nl> + return ; <nl> + } <nl> + <nl> avio_seek ( s -> pb , off , SEEK_SET ); <nl> mpc8_get_chunk_header ( s -> pb , & tag , & size ); <nl> if ( tag != TAG_SEEKTABLE ){
typedef struct MpegDemuxContext { <nl> static int mpegps_read_header ( AVFormatContext * s ) <nl> { <nl> MpegDemuxContext * m = s -> priv_data ; <nl> - char buffer [ 7 ]; <nl> + char buffer [ 7 ] = { 0 }; <nl> int64_t last_pos = avio_tell ( s -> pb ); <nl>  <nl> m -> header_state = 0xff ;
static int64_t mxf_essence_container_end ( MXFContext * mxf , int body_sid ) <nl> static int mxf_edit_unit_absolute_offset ( MXFContext * mxf , MXFIndexTable * index_table , int64_t edit_unit , int64_t * edit_unit_out , int64_t * offset_out , int nag ) <nl> { <nl> int i ; <nl> - int offset_temp = 0 ; <nl> + int64_t offset_temp = 0 ; <nl>  <nl> for ( i = 0 ; i < index_table -> nb_segments ; i ++) { <nl> MXFIndexTableSegment * s = index_table -> segments [ i ];
static int mpegts_push_data ( MpegTSFilter * filter , <nl> pes -> st -> request_probe = 1 ; <nl> } <nl> } else { <nl> + pes -> pes_header_size = 6 ; <nl> pes -> state = MPEGTS_PAYLOAD ; <nl> pes -> data_index = 0 ; <nl> }
int ff_get_qtpalette ( int codec_id , AVIOContext * pb , uint32_t * palette ) <nl>  <nl> /* If the depth is 1 , 2 , 4 , or 8 bpp , file is palettized . */ <nl> if (( bit_depth == 1 || bit_depth == 2 || bit_depth == 4 || bit_depth == 8 )) { <nl> - int color_count , color_start , color_end ; <nl> + uint32_t color_count , color_start , color_end ; <nl> uint32_t a , r , g , b ; <nl>  <nl> /* Ignore the greyscale bit for 1 - bit video and sample
static int probe ( AVProbeData * p ) <nl> offset = AV_RL32 ( p -> buf + 18 + i * 16 ); <nl> if ( offset < 22 ) <nl> return FFMIN ( i , AVPROBE_SCORE_MAX / 4 ); <nl> - if ( offset + 8 > p -> buf_size ) <nl> + if ( offset > p -> buf_size - 8 ) <nl> continue ; <nl> if ( p -> buf [ offset ] != 40 && AV_RB64 ( p -> buf + offset ) != PNGSIG ) <nl> return FFMIN ( i , AVPROBE_SCORE_MAX / 4 );
static int old_codec47 ( SANMVideoContext * ctx , int top , <nl> if ( bytestream2_get_bytes_left (& ctx -> gb ) < width * height ) <nl> return AVERROR_INVALIDDATA ; <nl> for ( j = 0 ; j < height ; j ++) { <nl> - for ( i = 0 ; i < width ; i ++) <nl> - bytestream2_get_bufferu (& ctx -> gb , dst , width ); <nl> + bytestream2_get_bufferu (& ctx -> gb , dst , width ); <nl> dst += stride ; <nl> } <nl> break ;
static int rtp_mpegts_write_header ( AVFormatContext * s ) <nl> } <nl> rtp_ctx -> oformat = rtp_format ; <nl> st = avformat_new_stream ( rtp_ctx , NULL ); <nl> + if (! st ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl> st -> time_base . num = 1 ; <nl> st -> time_base . den = 90000 ; <nl> st -> codecpar -> codec_id = AV_CODEC_ID_MPEG2TS ;
int ff_vaapi_render_picture ( struct vaapi_context * vactx , VASurfaceID surface ) <nl> VABufferID va_buffers [ 3 ]; <nl> unsigned int n_va_buffers = 0 ; <nl>  <nl> + if (! vactx -> pic_param_buf_id ) <nl> + return 0 ; <nl> + <nl> vaUnmapBuffer ( vactx -> display , vactx -> pic_param_buf_id ); <nl> va_buffers [ n_va_buffers ++] = vactx -> pic_param_buf_id ; <nl> 
static int matroska_parse_frame ( MatroskaDemuxContext * matroska , <nl> /* XXX : prevent data copy ... */ <nl> if ( av_new_packet ( pkt , pkt_size + offset ) < 0 ) { <nl> av_free ( pkt ); <nl> + av_freep (& pkt_data ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
static int decode_block ( MJpegDecodeContext * s , int16_t * block , int component , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> val = val * quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> + val = FFMIN ( val , 32767 ); <nl> s -> last_dc [ component ] = val ; <nl> block [ 0 ] = val ; <nl> /* AC coefs */
av_cold int avcodec_close ( AVCodecContext * avctx ) <nl> avctx -> codec -> close ( avctx ); <nl> avcodec_default_free_buffers ( avctx ); <nl> avctx -> coded_frame = NULL ; <nl> - if ( avctx -> codec -> priv_class ) <nl> + if ( avctx -> codec && avctx -> codec -> priv_class ) <nl> av_opt_free ( avctx -> priv_data ); <nl> av_opt_free ( avctx ); <nl> av_freep (& avctx -> priv_data );
static int libquvi_read_header ( AVFormatContext * s ) <nl> if ( rc != QUVI_OK ) <nl> goto quvi_fail ; <nl>  <nl> + if (!( qc -> fmtctx = avformat_alloc_context ())) <nl> + goto quvi_fail ; <nl> + <nl> if (( ret = ff_copy_whitelists ( qc -> fmtctx , s )) < 0 ) <nl> goto end ; <nl> 
static int mov_read_header ( AVFormatContext * s ) <nl> } <nl> if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO && sc -> nb_frames_for_fps > 0 && sc -> duration_for_fps > 0 ) <nl> av_reduce (& st -> avg_frame_rate . num , & st -> avg_frame_rate . den , <nl> - sc -> time_scale * sc -> nb_frames_for_fps , sc -> duration_for_fps , INT_MAX ); <nl> + sc -> time_scale *( int64_t ) sc -> nb_frames_for_fps , sc -> duration_for_fps , INT_MAX ); <nl> } <nl>  <nl> if ( mov -> trex_data ) {
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> int lower_transport , const char * real_challenge ) <nl> { <nl> RTSPState * rt = s -> priv_data ; <nl> - int rtx , j , i , err , interleave = 0 ; <nl> + int rtx = 0 , j , i , err , interleave = 0 ; <nl> RTSPStream * rtsp_st ; <nl> RTSPMessageHeader reply1 , * reply = & reply1 ; <nl> char cmd [ 2048 ];
static void output_packet ( OutputFile * of , AVPacket * pkt , OutputStream * ost ) <nl> if ( ost -> nb_bitstream_filters ) { <nl> int idx ; <nl>  <nl> + av_packet_split_side_data ( pkt ); <nl> ret = av_bsf_send_packet ( ost -> bsf_ctx [ 0 ], pkt ); <nl> if ( ret < 0 ) <nl> goto finish ;
static void decode_lpc ( int32_t * coeffs , int mode , int length ) <nl> unsigned a1 = * coeffs ++; <nl> for ( i = 0 ; i < length - 1 >> 1 ; i ++) { <nl> * coeffs += a1 ; <nl> - coeffs [ 1 ] += * coeffs ; <nl> + coeffs [ 1 ] += ( unsigned )* coeffs ; <nl> a1 = coeffs [ 1 ]; <nl> coeffs += 2 ; <nl> }
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> goto fail ; <nl> } <nl>  <nl> - if ( reply -> timeout > 0 ) <nl> + if ( rt -> nb_rtsp_streams && reply -> timeout > 0 ) <nl> rt -> timeout = reply -> timeout ; <nl>  <nl> if ( rt -> server_type == RTSP_SERVER_REAL )
static int get_siz ( Jpeg2000DecoderContext * s ) <nl> avpriv_request_sample ( s -> avctx , " Support for image offsets "); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> + if ( s -> width > 32768U || s -> height > 32768U ) { <nl> + avpriv_request_sample ( s -> avctx , " Large Dimensions "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl>  <nl> if ( ncomponents <= 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid number of components : % d \ n ",
int ff_flac_parse_picture ( AVFormatContext * s , uint8_t * buf , int buf_size ) <nl> ret = AVERROR_INVALIDDATA ; <nl> goto fail ; <nl> } <nl> - if (!( data = av_buffer_alloc ( len ))) { <nl> + if (!( data = av_buffer_alloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ))) { <nl> RETURN_ERROR ( AVERROR ( ENOMEM )); <nl> } <nl> if ( avio_read ( pb , data -> data , len ) != len ) {
static int hds_write_header ( AVFormatContext * s ) <nl>  <nl> snprintf ( os -> temp_filename , sizeof ( os -> temp_filename ), <nl> "% s / stream % d_temp ", s -> filename , i ); <nl> - init_file ( s , os , 0 ); <nl> + ret = init_file ( s , os , 0 ); <nl> + if ( ret < 0 ) <nl> + goto fail ; <nl>  <nl> if (! os -> has_video && c -> min_frag_duration <= 0 ) { <nl> av_log ( s , AV_LOG_WARNING ,
static int init_filters ( const char * filters_descr ) <nl> abuffersink_params -> packing_fmts = packing_fmts ; <nl> ret = avfilter_graph_create_filter (& buffersink_ctx , abuffersink , " out ", <nl> NULL , abuffersink_params , filter_graph ); <nl> + av_free ( abuffersink_params ); <nl> if ( ret < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Cannot create audio buffer sink \ n "); <nl> return ret ;
static av_cold int amr_nb_encode_init ( AVCodecContext * avctx ) <nl>  <nl> avctx -> frame_size = 160 ; <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (! avctx -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> s -> enc_state = Encoder_Interface_init ( s -> enc_dtx ); <nl> if (! s -> enc_state ) {
static int dvdsub_parse ( AVCodecParserContext * s , <nl>  <nl> if ( pc -> packet_index == 0 ) { <nl> if ( buf_size < 2 ) <nl> - return 0 ; <nl> + return buf_size ; <nl> pc -> packet_len = AV_RB16 ( buf ); <nl> if ( pc -> packet_len == 0 ) /* HD - DVD subpicture packet */ <nl> pc -> packet_len = AV_RB32 ( buf + 2 );
AVCodec ff_wmv2_decoder = { <nl> wmv2_decode_end , <nl> ff_h263_decode_frame , <nl> CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 , <nl> - . max_lowres = 3 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" Windows Media Video 8 "), <nl> . pix_fmts = ff_pixfmt_list_420 , <nl> };
static int decode_mb_info ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> } <nl> } <nl> } else { <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> mb -> type = ref_mb -> type ; /* copy mb_type from corresponding reference mb */ <nl> } else if ( ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> mb -> type = 0 ; /* mb_type is always INTRA for intra - frames */
int av_opt_query_ranges_default ( AVOptionRanges ** ranges_arg , void * obj , const ch <nl> fail : <nl> av_free ( ranges ); <nl> av_free ( range ); <nl> + av_free ( range_array ); <nl> return ret ; <nl> } <nl> 
static int decode_thread ( void * arg ) <nl> if ( ret < 0 ) { <nl> if ( ret == AVERROR_EOF || url_feof ( ic -> pb )) <nl> eof = 1 ; <nl> - if ( ic -> pb -> error ) <nl> + if ( ic -> pb && ic -> pb -> error ) <nl> break ; <nl> SDL_Delay ( 100 ); /* wait for user event */ <nl> continue ;
static int vp9_superframe_filter ( AVBSFContext * ctx , AVPacket * out ) <nl> goto done ; <nl> } <nl>  <nl> - av_packet_move_ref ( s -> cache [ s -> n_cache ++], in ); <nl> + res = av_packet_ref ( s -> cache [ s -> n_cache ++], in ); <nl> + if ( res < 0 ) <nl> + goto done ; <nl>  <nl> if ( invisible ) { <nl> res = AVERROR ( EAGAIN );
static av_cold void uninit ( AVFilterContext * ctx ) <nl> avfilter_unref_buffer ( deshake -> ref ); <nl> if ( deshake -> fp ) <nl> fclose ( deshake -> fp ); <nl> - avcodec_close ( deshake -> avctx ); <nl> + if ( deshake -> avctx ) <nl> + avcodec_close ( deshake -> avctx ); <nl> av_freep (& deshake -> avctx ); <nl> } <nl> 
static int get_metadata ( AVFormatContext * s , <nl> if (! buf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - if ( avio_read ( s -> pb , buf , data_size ) < 0 ) { <nl> + if ( avio_read ( s -> pb , buf , data_size ) != data_size ) { <nl> av_free ( buf ); <nl> return AVERROR ( EIO ); <nl> }
static int ism_write_header ( AVFormatContext * s ) <nl> goto fail ; <nl> } <nl>  <nl> - c -> streams = av_mallocz ( sizeof (* c -> streams ) * s -> nb_streams ); <nl> + c -> streams = av_mallocz_array ( s -> nb_streams , sizeof (* c -> streams )); <nl> if (! c -> streams ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ;
static int unpack_modes ( Vp3DecodeContext * s , GetBitContext * gb ) <nl>  <nl> /* is it a custom coding scheme ? */ <nl> if ( scheme == 0 ) { <nl> + for ( i = 0 ; i < 8 ; i ++) <nl> + custom_mode_alphabet [ i ] = MODE_INTER_NO_MV ; <nl> for ( i = 0 ; i < 8 ; i ++) <nl> custom_mode_alphabet [ get_bits ( gb , 3 )] = i ; <nl> }
static int ivf_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> static int ivf_write_trailer ( AVFormatContext * s ) <nl> { <nl> AVIOContext * pb = s -> pb ; <nl> - if ( pb -> seekable ) { <nl> - IVFEncContext * ctx = s -> priv_data ; <nl> + IVFEncContext * ctx = s -> priv_data ; <nl> + <nl> + if ( pb -> seekable && ctx -> frame_cnt > 1 ) { <nl> size_t end = avio_tell ( pb ); <nl>  <nl> avio_seek ( pb , 24 , SEEK_SET );
static int flac_parse ( AVCodecParserContext * s , AVCodecContext * avctx , <nl> temp = curr -> next ; <nl> av_freep (& curr -> link_penalty ); <nl> av_free ( curr ); <nl> + fpc -> nb_headers_buffered --; <nl> } <nl> fpc -> headers = fpc -> best_header -> next ; <nl> av_freep (& fpc -> best_header -> link_penalty ); <nl> av_freep (& fpc -> best_header ); <nl> + fpc -> nb_headers_buffered --; <nl> } <nl>  <nl> /* Find and score new headers . */
static int decorrelate ( TAKDecContext * s , int c1 , int c2 , int length ) <nl> s -> residues [ i ] * s -> filter [ 0 ]; <nl> } <nl>  <nl> - v = ( av_clip_intp2 ( v >> 10 , 13 ) << dshift ) - * p1 ; <nl> + v = av_clip_intp2 ( v >> 10 , 13 ) * ( 1 << dshift ) - * p1 ; <nl> * p1 ++ = v ; <nl> } <nl> 
static int find_start_code ( const uint8_t * buf , int buf_size , <nl> buf [ buf_index + 2 ] == 1 ) <nl> break ; <nl>  <nl> - if ( buf_index + 3 >= buf_size ) <nl> + buf_index += 3 ; <nl> + <nl> + if ( buf_index >= buf_size ) <nl> return buf_size ; <nl>  <nl> - return buf_index + 3 ; <nl> + return buf_index ; <nl> } <nl>  <nl> static int get_avc_nalsize ( H264Context * h , const uint8_t * buf ,
static int dfa_probe ( AVProbeData * p ) <nl> if ( p -> buf_size < 4 || AV_RL32 ( p -> buf ) != MKTAG (' D ', ' F ', ' I ', ' A ')) <nl> return 0 ; <nl>  <nl> + if ( AV_RL32 ( p -> buf + 16 ) != 0x80 ) <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
LF_IFUNC ( v , chroma_intra , depth , avx ) <nl> LF_FUNCS ( uint8_t , 8 ) <nl> LF_FUNCS ( uint16_t , 10 ) <nl>  <nl> -# if ARCH_X86_32 <nl> +# if ARCH_X86_32 && HAVE_YASM <nl> LF_FUNC ( v8 , luma , 8 , mmxext ) <nl> static void ff_deblock_v_luma_8_mmxext ( uint8_t * pix , int stride , int alpha , int beta , int8_t * tc0 ) <nl> {
static int crypto_open ( URLContext * h , const char * uri , int flags ) <nl>  <nl> return 0 ; <nl> err : <nl> - av_free ( c -> key ); <nl> - av_free ( c -> iv ); <nl> + av_freep (& c -> key ); <nl> + av_freep (& c -> iv ); <nl> return ret ; <nl> } <nl> 
static int ape_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl> if ( s -> fileversion < 3950 ) // previous versions overread two bytes <nl> buf_size += 2 ; <nl> - av_fast_malloc (& s -> data , & s -> data_size , buf_size ); <nl> + av_fast_padded_malloc (& s -> data , & s -> data_size , buf_size ); <nl> if (! s -> data ) <nl> return AVERROR ( ENOMEM ); <nl> s -> dsp . bswap_buf (( uint32_t *) s -> data , ( const uint32_t *) buf , buf_size >> 2 );
int ff_dirac_golomb_read_16bit ( DiracGolombLUT * lut_ctx , const uint8_t * buf , <nl> if (( c_idx + 1 ) > coeffs ) <nl> return c_idx ; <nl>  <nl> + if ( res_bits >= RSIZE_BITS ) <nl> + res_bits = res = 0 ; <nl> + <nl> if ( res_bits && l -> sign ) { <nl> int32_t coeff = 1 ; <nl> APPEND_RESIDUE ( res , l -> preamble );
AVInputFormat * av_probe_input_format3 ( AVProbeData * pd , int is_opened , int * score <nl> AVProbeData lpd = * pd ; <nl> AVInputFormat * fmt1 = NULL , * fmt ; <nl> int score , nodat = 0 , score_max = 0 ; <nl> + const static uint8_t zerobuffer [ AVPROBE_PADDING_SIZE ]; <nl> + <nl> + if (! lpd . buf ) <nl> + lpd . buf = zerobuffer ; <nl>  <nl> if ( lpd . buf_size > 10 && ff_id3v2_match ( lpd . buf , ID3v2_DEFAULT_MAGIC )) { <nl> int id3len = ff_id3v2_tag_len ( lpd . buf );
int av_cold ff_wma_get_frame_len_bits ( int sample_rate , int version , <nl> } else if ( sample_rate <= 22050 || <nl> ( sample_rate <= 32000 && version == 1 )) { <nl> frame_len_bits = 10 ; <nl> - } else if ( sample_rate <= 48000 ) { <nl> + } else if ( sample_rate <= 48000 || version < 3 ) { <nl> frame_len_bits = 11 ; <nl> } else if ( sample_rate <= 96000 ) { <nl> frame_len_bits = 12 ;
void avcodec_free_context ( AVCodecContext ** pavctx ) <nl>  <nl> av_freep (& avctx -> extradata ); <nl> av_freep (& avctx -> subtitle_header ); <nl> + av_freep (& avctx -> intra_matrix ); <nl> + av_freep (& avctx -> inter_matrix ); <nl> + av_freep (& avctx -> rc_override ); <nl>  <nl> av_freep ( pavctx ); <nl> }
static inline int parse_command_line ( AVFormatContext * s , const char * line , <nl> RTSPState * rt = s -> priv_data ; <nl> const char * linept , * searchlinept ; <nl> linept = strchr ( line , ' '); <nl> + <nl> + if (! linept ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( linept - line > methodsize - 1 ) { <nl> av_log ( s , AV_LOG_ERROR , " Method string too long \ n "); <nl> return AVERROR ( EIO );
static int read_extra_header ( FFV1Context * f ) <nl> f -> quant_table_count = get_symbol ( c , state , 0 ); <nl> if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES || ! f -> quant_table_count ) { <nl> av_log ( f -> avctx , AV_LOG_ERROR , " quant table count % d is invalid \ n ", f -> quant_table_count ); <nl> + f -> quant_table_count = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> s -> buf = & ptr ; <nl> s -> buf_size = pkt -> size ; <nl>  <nl> - if ( check_size ( s , 8 )) <nl> + if ( check_size ( s , 8 )) { <nl> + ret = AVERROR ( EINVAL ); <nl> goto fail ; <nl> + } <nl>  <nl> // write header <nl> bytestream_put_le16 (& ptr , 0x4949 );
static void aw_pulse_set2 ( WMAVoiceContext * s , GetBitContext * gb , <nl> int excl_range = s -> aw_pulse_range ; // always 16 or 24 <nl> uint16_t * use_mask_ptr = & use_mask [ idx >> 4 ]; <nl> int first_sh = 16 - ( idx & 15 ); <nl> - * use_mask_ptr ++ &= 0xFFFF << first_sh ; <nl> + * use_mask_ptr ++ &= 0xFFFFu << first_sh ; <nl> excl_range -= first_sh ; <nl> if ( excl_range >= 16 ) { <nl> * use_mask_ptr ++ = 0 ;
static int adx_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> AVCodecContext * avctx = s -> streams [ 0 ]-> codec ; <nl> int ret , size ; <nl>  <nl> + if ( avctx -> channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " invalid number of channels % d \ n ", avctx -> channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> size = BLOCK_SIZE * avctx -> channels ; <nl>  <nl> pkt -> pos = avio_tell ( s -> pb );
static int idcin_read_packet ( AVFormatContext * s , <nl> return ret ; <nl> else if ( ret != chunk_size ) { <nl> av_log ( s , AV_LOG_ERROR , " incomplete packet \ n "); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( EIO ); <nl> } <nl> if ( command == 1 ) {
void ff_rtsp_close_streams ( AVFormatContext * s ) <nl> if ( rtsp_st -> dynamic_handler && rtsp_st -> dynamic_protocol_context ) <nl> rtsp_st -> dynamic_handler -> close ( <nl> rtsp_st -> dynamic_protocol_context ); <nl> + av_free ( rtsp_st ); <nl> } <nl> } <nl> av_free ( rt -> rtsp_streams );
static int v4l2_set_parameters ( AVFormatContext * s1 ) <nl> standard . index = i ; <nl> if ( v4l2_ioctl ( s -> fd , VIDIOC_ENUMSTD , & standard ) < 0 ) { <nl> ret = AVERROR ( errno ); <nl> - if ( ret == AVERROR ( EINVAL )) { <nl> + if ( ret == AVERROR ( EINVAL ) || ret == AVERROR ( ENODATA )) { <nl> tpf = & streamparm . parm . capture . timeperframe ; <nl> break ; <nl> }
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> else v = buf_p - c -> bytestream_start ; <nl> if ( buf_p - c -> bytestream_start < v ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Slice pointer chain broken \ n "); <nl> + ff_thread_report_progress (& f -> picture , INT_MAX , 0 ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> buf_p -= v ;
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i ); <nl> + if ( end - dst < count ) { <nl> + av_log ( l -> avctx , AV_LOG_ERROR , " too many zeros remaining \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> memset ( dst , 0 , count ); <nl> l -> zeros_rem -= count ; <nl> dst += count ;
static int opt_vstats ( void * optctx , const char * opt , const char * arg ) <nl> time_t today2 = time ( NULL ); <nl> struct tm * today = localtime (& today2 ); <nl>  <nl> + if (! today ) <nl> + return AVERROR ( errno ); <nl> + <nl> snprintf ( filename , sizeof ( filename ), " vstats_ % 02d % 02d % 02d . log ", today -> tm_hour , today -> tm_min , <nl> today -> tm_sec ); <nl> return opt_vstats_file ( NULL , opt , filename );
static int decode_slice_header ( H264Context * h ){ <nl> return - 1 ; <nl> } <nl>  <nl> - if ( h -> dequant_coeff_pps != ( int ) pps_id ){ <nl> - h -> dequant_coeff_pps = ( int ) pps_id ; <nl> + if ( h -> dequant_coeff_pps != pps_id ){ <nl> + h -> dequant_coeff_pps = pps_id ; <nl> init_dequant_tables ( h ); <nl> } <nl> 
static int check_tag ( AVIOContext * s , int offset , unsigned int len ) <nl>  <nl> if ( len > 4 || <nl> avio_seek ( s , offset , SEEK_SET ) < 0 || <nl> - avio_read ( s , tag , len ) < len ) <nl> + avio_read ( s , tag , len ) < ( int ) len ) <nl> return - 1 ; <nl> else if (! AV_RB32 ( tag ) || is_tag ( tag , len )) <nl> return 1 ;
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> current_dts -= sc -> dts_shift ; <nl>  <nl> - if (! sc -> sample_count ) <nl> + if (! sc -> sample_count || st -> nb_index_entries ) <nl> return ; <nl> if ( sc -> sample_count >= UINT_MAX / sizeof (* st -> index_entries )) <nl> return ;
int ff_j2k_init_component ( J2kComponent * comp , J2kCodingStyle * codsty , J2kQuantSt <nl> band -> cblk = av_malloc ( sizeof ( J2kCblk ) * band -> cblknx * band -> cblkny ); <nl> if (! band -> cblk ) <nl> return AVERROR ( ENOMEM ); <nl> - band -> prec = av_malloc ( reslevel -> num_precincts_x * reslevel -> num_precincts_y * sizeof ( J2kPrec )); <nl> + band -> prec = av_malloc ( sizeof ( J2kCblk ) * reslevel -> num_precincts_x * reslevel -> num_precincts_y ); <nl> if (! band -> prec ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int fourxm_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( fourcc_tag == std__TAG ) { <nl> + if ( header_size < i + 16 ) { <nl> + av_log ( s , AV_LOG_ERROR , " std TAG truncated \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> fourxm -> fps = av_int2float ( AV_RL32 (& header [ i + 12 ])); <nl> } else if ( fourcc_tag == vtrk_TAG ) { <nl> /* check that there is enough data */
static int parse_palette ( AVCodecContext * avctx , GetByteContext * gbc , <nl> bytestream2_skip ( gbc , 1 ); <nl> b = bytestream2_get_byte ( gbc ); <nl> bytestream2_skip ( gbc , 1 ); <nl> - pal [ idx ] = ( r << 16 ) | ( g << 8 ) | b ; <nl> + pal [ idx ] = ( 0xFFU << 24 ) | ( r << 16 ) | ( g << 8 ) | b ; <nl> } <nl> return 0 ; <nl> }
static void vector_fmul_window_fixed_c ( int32_t * dst , const int32_t * src0 , <nl> AVFixedDSPContext * avpriv_alloc_fixed_dsp ( int bit_exact ) <nl> { <nl> AVFixedDSPContext * fdsp = av_malloc ( sizeof ( AVFixedDSPContext )); <nl> + <nl> + if (! fdsp ) <nl> + return NULL ; <nl> + <nl> fdsp -> vector_fmul_window_scaled = vector_fmul_window_fixed_scaled_c ; <nl> fdsp -> vector_fmul_window = vector_fmul_window_fixed_c ; <nl> 
static av_cold void nvenc_setup_rate_control ( AVCodecContext * avctx ) <nl>  <nl> if ( ctx -> flags & NVENC_LOSSLESS ) { <nl> set_lossless ( avctx ); <nl> - } else if ( ctx -> rc > 0 ) { <nl> + } else if ( ctx -> rc >= 0 ) { <nl> nvenc_override_rate_control ( avctx ); <nl> } else { <nl> ctx -> encode_config . rcParams . rateControlMode = NV_ENC_PARAMS_RC_VBR ;
void av_packet_free ( AVPacket ** pkt ) <nl> static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> { <nl> int ret ; <nl> - if (( unsigned ) size >= ( unsigned ) size + AV_INPUT_BUFFER_PADDING_SIZE ) <nl> + if ( size < 0 || size >= INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> ret = av_buffer_realloc ( buf , size + AV_INPUT_BUFFER_PADDING_SIZE );
static int concat_read_packet ( AVFormatContext * avf , AVPacket * pkt ) <nl> ConcatStream * cs ; <nl> AVStream * st ; <nl>  <nl> + if (! cat -> avf ) <nl> + return AVERROR ( EIO ); <nl> + <nl> while ( 1 ) { <nl> ret = av_read_frame ( cat -> avf , pkt ); <nl> if ( ret == AVERROR_EOF ) {
void kvm_irqchip_commit_routes ( KVMState * s ) <nl> { <nl> int ret ; <nl>  <nl> + if ( kvm_gsi_direct_mapping ()) { <nl> + return ; <nl> + } <nl> + <nl> + if (! kvm_gsi_routing_enabled ()) { <nl> + return ; <nl> + } <nl> + <nl> s -> irq_routes -> flags = 0 ; <nl> trace_kvm_irqchip_commit_routes (); <nl> ret = kvm_vm_ioctl ( s , KVM_SET_GSI_ROUTING , s -> irq_routes );
int qcow2_grow_l1_table ( BlockDriverState * bs , uint64_t min_size , <nl> } <nl> } <nl>  <nl> - if ( new_l1_size > INT_MAX ) { <nl> + if ( new_l1_size > INT_MAX / sizeof ( uint64_t )) { <nl> return - EFBIG ; <nl> } <nl> 
static int decode_gusa ( DisasContext * ctx , CPUSH4State * env , int * pmax_insns ) <nl> } <nl>  <nl> /* If op_src is not a valid register , then op_arg was a constant . */ <nl> - if ( op_src < 0 ) { <nl> + if ( op_src < 0 && ! TCGV_IS_UNUSED ( op_arg )) { <nl> tcg_temp_free_i32 ( op_arg ); <nl> } <nl> 
static int blk_root_inactivate ( BdrvChild * child ) <nl> * this point because the VM is stopped ) and unattached monitor - owned <nl> * BlockBackends . If there is still any other user like a block job , then <nl> * we simply can ' t inactivate the image . */ <nl> - if (! blk -> dev && ! blk -> name [ 0 ]) { <nl> + if (! blk -> dev && ! blk_name ( blk )[ 0 ]) { <nl> return - EPERM ; <nl> } <nl> 
static void virtio_device_realize ( DeviceState * dev , Error ** errp ) <nl> virtio_bus_device_plugged ( vdev , & err ); <nl> if ( err != NULL ) { <nl> error_propagate ( errp , err ); <nl> + vdc -> unrealize ( dev , NULL ); <nl> return ; <nl> } <nl> 
void monitor_init ( CharDriverState * hd , int show_banner ) <nl> hide_banner = ! show_banner ; <nl>  <nl> qemu_chr_add_handlers ( hd , term_can_read , term_read , term_event , NULL ); <nl> + <nl> + readline_start ("", 0 , monitor_handle_command1 , NULL ); <nl> } <nl>  <nl> /* XXX : use threads ? */
static void realize ( DeviceState * d , Error ** errp ) <nl> error_free ( err ); <nl> object_unref ( OBJECT ( drc )); <nl> } <nl> + g_free ( child_name ); <nl> DPRINTFN (" drc realize complete "); <nl> } <nl> 
static const RunStateTransition runstate_transitions_def [] = { <nl> { RUN_STATE_WATCHDOG , RUN_STATE_FINISH_MIGRATE }, <nl>  <nl> { RUN_STATE_GUEST_PANICKED , RUN_STATE_PAUSED }, <nl> + { RUN_STATE_GUEST_PANICKED , RUN_STATE_FINISH_MIGRATE }, <nl>  <nl> { RUN_STATE_MAX , RUN_STATE_MAX }, <nl> };
static void do_dma_memory_set ( dma_addr_t addr , uint8_t c , dma_addr_t len ) <nl> while ( len > 0 ) { <nl> l = len < FILLBUF_SIZE ? len : FILLBUF_SIZE ; <nl> cpu_physical_memory_rw ( addr , fillbuf , l , true ); <nl> - len -= len ; <nl> - addr += len ; <nl> + len -= l ; <nl> + addr += l ; <nl> } <nl> } <nl> 
void * address_space_map ( AddressSpace * as , <nl> if ( bounce . buffer ) { <nl> return NULL ; <nl> } <nl> - bounce . buffer = qemu_memalign ( TARGET_PAGE_SIZE , TARGET_PAGE_SIZE ); <nl> + /* Avoid unbounded allocations */ <nl> + l = MIN ( l , TARGET_PAGE_SIZE ); <nl> + bounce . buffer = qemu_memalign ( TARGET_PAGE_SIZE , l ); <nl> bounce . addr = addr ; <nl> bounce . len = l ; <nl> 
static Qcow2BitmapList * bitmap_list_load ( BlockDriverState * bs , uint64_t offset , <nl> goto fail ; <nl> } <nl>  <nl> - bm = g_new ( Qcow2Bitmap , 1 ); <nl> + bm = g_new0 ( Qcow2Bitmap , 1 ); <nl> bm -> table . offset = e -> bitmap_table_offset ; <nl> bm -> table . size = e -> bitmap_table_size ; <nl> bm -> flags = e -> flags ;
void virtio_cleanup ( VirtIODevice * vdev ) <nl> if ( vdev -> config ) <nl> qemu_free ( vdev -> config ); <nl> qemu_free ( vdev -> vq ); <nl> + qemu_free ( vdev ); <nl> } <nl>  <nl> static void virtio_vmstate_change ( void * opaque , int running , int reason )
void do_compare_and_swap32 ( void * cpu_env , int num ) <nl> uint32_t * value = ( uint32_t *)(( CPUX86State *) cpu_env )-> regs [ R_ECX ]; <nl> DPRINTF (" commpage : compare_and_swap32 (% x , new ,% p )\ n ", old , value ); <nl>  <nl> - if ( value && old == tswap32 (* value )) <nl> + if ( old == tswap32 (* value )) <nl> { <nl> uint32_t new = (( CPUX86State *) cpu_env )-> regs [ R_EDX ]; <nl> * value = tswap32 ( new );
static void pmac_ide_atapi_transfer_cb ( void * opaque , int ret ) <nl> } <nl>  <nl> /* Calculate current offset */ <nl> - offset = ( int64_t )( s -> lba << 11 ) + s -> io_buffer_index ; <nl> + offset = (( int64_t ) s -> lba << 11 ) + s -> io_buffer_index ; <nl>  <nl> pmac_dma_read ( s -> blk , offset , io -> len , pmac_ide_atapi_transfer_cb , io ); <nl> return ;
static int vdi_create ( const char * filename , QEMUOptionParameter * options ) <nl> static void vdi_close ( BlockDriverState * bs ) <nl> { <nl> BDRVVdiState * s = bs -> opaque ; <nl> + <nl> + g_free ( s -> bmap ); <nl> + <nl> migrate_del_blocker ( s -> migration_blocker ); <nl> error_free ( s -> migration_blocker ); <nl> }
static void disas_ldst_reg_imm9 ( DisasContext * s , uint32_t insn , <nl> post_index = false ; <nl> writeback = true ; <nl> break ; <nl> + default : <nl> + g_assert_not_reached (); <nl> } <nl>  <nl> if ( rn == 31 ) {
static void virtio_net_set_status ( struct VirtIODevice * vdev , uint8_t status ) <nl> qemu_bh_cancel ( q -> tx_bh ); <nl> } <nl> if (( n -> status & VIRTIO_NET_S_LINK_UP ) == 0 && <nl> - ( queue_status & VIRTIO_CONFIG_S_DRIVER_OK )) { <nl> + ( queue_status & VIRTIO_CONFIG_S_DRIVER_OK ) && <nl> + vdev -> vm_running ) { <nl> /* if tx is waiting we are likely have some packets in tx queue <nl> * and disabled notification */ <nl> q -> tx_waiting = 0 ;
void page_set_flags ( target_ulong start , target_ulong end , int flags ) <nl> guest address space . If this assert fires , it probably indicates <nl> a missing call to h2g_valid . */ <nl> # if TARGET_ABI_BITS > L1_MAP_ADDR_SPACE_BITS <nl> - assert ( end < (( target_ulong ) 1 << L1_MAP_ADDR_SPACE_BITS )); <nl> + assert ( end <= (( target_ulong ) 1 << L1_MAP_ADDR_SPACE_BITS )); <nl> # endif <nl> assert ( start < end ); <nl> assert_memory_lock ();
void * qxl_phys2virt ( PCIQXLDevice * qxl , QXLPHYSICAL pqxl , int group_id ) <nl> case MEMSLOT_GROUP_HOST : <nl> return ( void *) offset ; <nl> case MEMSLOT_GROUP_GUEST : <nl> - PANIC_ON ( slot > NUM_MEMSLOTS ); <nl> + PANIC_ON ( slot >= NUM_MEMSLOTS ); <nl> PANIC_ON (! qxl -> guest_slots [ slot ]. active ); <nl> PANIC_ON ( offset < qxl -> guest_slots [ slot ]. delta ); <nl> offset -= qxl -> guest_slots [ slot ]. delta ;
static void cleanup_unknown_header_ext ( BlockDriverState * bs ) <nl> } <nl> } <nl>  <nl> - static void report_unsupported ( BlockDriverState * bs , const char * fmt , ...) <nl> + static void GCC_FMT_ATTR ( 2 , 3 ) report_unsupported ( BlockDriverState * bs , <nl> + const char * fmt , ...) <nl> { <nl> char msg [ 64 ]; <nl> va_list ap ;
static uint64_t msix_pba_mmio_read ( void * opaque , hwaddr addr , <nl> return pci_get_long ( dev -> msix_pba + addr ); <nl> } <nl>  <nl> + static void msix_pba_mmio_write ( void * opaque , hwaddr addr , <nl> + uint64_t val , unsigned size ) <nl> +{ <nl> +} <nl> + <nl> static const MemoryRegionOps msix_pba_mmio_ops = { <nl> . read = msix_pba_mmio_read , <nl> + . write = msix_pba_mmio_write , <nl> . endianness = DEVICE_LITTLE_ENDIAN , <nl> . valid = { <nl> . min_access_size = 4 ,
static inline void fimd_swap_data ( unsigned int swap_ctl , uint64_t * data ) <nl> if ( swap_ctl & FIMD_WINCON_SWAP_BITS ) { <nl> res = 0 ; <nl> for ( i = 0 ; i < 64 ; i ++) { <nl> - if ( x & ( 1ULL << ( 64 - i ))) { <nl> + if ( x & ( 1ULL << ( 63 - i ))) { <nl> res |= ( 1ULL << i ); <nl> } <nl> }
static void tcg_out_op ( TCGContext * s , TCGOpcode opc , const TCGArg * args , <nl> break ; <nl>  <nl> case INDEX_op_ext32u_i64 : <nl> - tcg_out_rld ( s , RLDICR , args [ 0 ], args [ 1 ], 0 , 32 ); <nl> + tcg_out_rld ( s , RLDICL , args [ 0 ], args [ 1 ], 0 , 32 ); <nl> break ; <nl>  <nl> case INDEX_op_setcond_i32 :
static uint64_t pci_read ( void * opaque , hwaddr addr , unsigned int size ) <nl> uint32_t val = 0 ; <nl> int bsel = s -> hotplug_select ; <nl>  <nl> - if ( bsel < 0 || bsel > ACPI_PCIHP_MAX_HOTPLUG_BUS ) { <nl> + if ( bsel < 0 || bsel >= ACPI_PCIHP_MAX_HOTPLUG_BUS ) { <nl> return 0 ; <nl> } <nl> 
static inline void setcc ( S390CPU * cpu , uint64_t cc ) <nl>  <nl> env -> psw . mask &= ~( 3ull << 44 ); <nl> env -> psw . mask |= ( cc & 3 ) << 44 ; <nl> + env -> cc_op = cc ; <nl> } <nl>  <nl> typedef struct LowCore
int coroutine_fn bdrv_is_allocated ( BlockDriverState * bs , int64_t sector_num , <nl> if ( ret < 0 ) { <nl> return ret ; <nl> } <nl> - return ( ret & BDRV_BLOCK_ALLOCATED ); <nl> + return !!( ret & BDRV_BLOCK_ALLOCATED ); <nl> } <nl>  <nl> /*
# endif <nl>  <nl> typedef struct IOHandlerRecord { <nl> - int fd ; <nl> IOCanReadHandler * fd_read_poll ; <nl> IOHandler * fd_read ; <nl> IOHandler * fd_write ; <nl> - int deleted ; <nl> void * opaque ; <nl> QLIST_ENTRY ( IOHandlerRecord ) next ; <nl> + int fd ; <nl> + bool deleted ; <nl> } IOHandlerRecord ; <nl>  <nl> static QLIST_HEAD (, IOHandlerRecord ) io_handlers =
static void vhost_dev_unassign_memory ( struct vhost_dev * dev , <nl> if ( start_addr <= reg -> guest_phys_addr && memlast >= reglast ) { <nl> -- dev -> mem -> nregions ; <nl> -- to ; <nl> - assert ( to >= 0 ); <nl> ++ overlap_middle ; <nl> continue ; <nl> }
int qcow2_alloc_cluster_offset ( BlockDriverState * bs , uint64_t offset , <nl>  <nl> again : <nl> start = offset ; <nl> - remaining = * num << BDRV_SECTOR_BITS ; <nl> + remaining = ( uint64_t )* num << BDRV_SECTOR_BITS ; <nl> cluster_offset = 0 ; <nl> * host_offset = 0 ; <nl> cur_bytes = 0 ;
typedef struct SCLPConsole { <nl> /* Return number of bytes that fit into iov buffer */ <nl> static int chr_can_read ( void * opaque ) <nl> { <nl> - int can_read ; <nl> SCLPConsole * scon = opaque ; <nl>  <nl> - can_read = SIZE_BUFFER_VT220 - scon -> iov_data_len ; <nl> - <nl> - return can_read ; <nl> + return scon -> iov ? SIZE_BUFFER_VT220 - scon -> iov_data_len : 0 ; <nl> } <nl>  <nl> /* Receive n bytes from character layer , save in iov buffer ,
static void virtio_net_guest_notifier_mask ( VirtIODevice * vdev , int idx , <nl> void virtio_net_set_config_size ( VirtIONet * n , uint32_t host_features ) <nl> { <nl> int i , config_size = 0 ; <nl> + host_features |= ( 1 << VIRTIO_NET_F_MAC ); <nl> for ( i = 0 ; feature_sizes [ i ]. flags != 0 ; i ++) { <nl> if ( host_features & feature_sizes [ i ]. flags ) { <nl> config_size = MAX ( feature_sizes [ i ]. end , config_size );
static void timer_enable ( struct xlx_timer * xt ) <nl> count = xt -> regs [ R_TLR ]; <nl> else <nl> count = ~ 0 - xt -> regs [ R_TLR ]; <nl> - ptimer_set_count ( xt -> ptimer , count ); <nl> + ptimer_set_limit ( xt -> ptimer , count , 1 ); <nl> ptimer_run ( xt -> ptimer , 1 ); <nl> } <nl> 
restart : <nl> aio_context_release ( pool -> ctx ); <nl> elem -> common . cb ( elem -> common . opaque , elem -> ret ); <nl> aio_context_acquire ( pool -> ctx ); <nl> + <nl> + /* We can safely cancel the completion_bh here regardless of someone <nl> + * else having scheduled it meanwhile because we reenter the <nl> + * completion function anyway ( goto restart ). <nl> + */ <nl> + qemu_bh_cancel ( pool -> completion_bh ); <nl> + <nl> qemu_aio_unref ( elem ); <nl> goto restart ; <nl> } else {
static int vmxnet3_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> + vmxnet3_validate_interrupts ( s ); <nl> + <nl> return 0 ; <nl> } <nl> 
static QemuOptsList qemu_vnc_opts = { <nl> },{ <nl> . name = " connections ", <nl> . type = QEMU_OPT_NUMBER , <nl> + },{ <nl> + . name = " to ", <nl> + . type = QEMU_OPT_NUMBER , <nl> + },{ <nl> + . name = " ipv4 ", <nl> + . type = QEMU_OPT_BOOL , <nl> + },{ <nl> + . name = " ipv6 ", <nl> + . type = QEMU_OPT_BOOL , <nl> },{ <nl> . name = " password ", <nl> . type = QEMU_OPT_BOOL ,
hwaddr s390_cpu_get_phys_page_debug ( CPUState * cs , vaddr vaddr ) <nl> vaddr &= 0x7fffffff ; <nl> } <nl>  <nl> - mmu_translate ( env , vaddr , MMU_INST_FETCH , asc , & raddr , & prot , false ); <nl> - <nl> + if ( mmu_translate ( env , vaddr , MMU_INST_FETCH , asc , & raddr , & prot , false )) { <nl> + return - 1 ; <nl> + } <nl> return raddr ; <nl> } <nl> 
void qemu_spice_create_host_primary ( SimpleSpiceDisplay * ssd ) <nl> { <nl> QXLDevSurfaceCreate surface ; <nl>  <nl> + memset (& surface , 0 , sizeof ( surface )); <nl> + <nl> dprint ( 1 , "% s : % dx % d \ n ", __FUNCTION__ , <nl> ds_get_width ( ssd -> ds ), ds_get_height ( ssd -> ds )); <nl> 
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> if (( txdw0 & CP_TX_LGSEN ) && ip_protocol == IP_PROTO_TCP ) <nl> { <nl> + /* Large enough for the TCP header ? */ <nl> + if ( ip_data_len < sizeof ( tcp_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> int large_send_mss = ( txdw0 >> 16 ) & CP_TC_LGSEN_MSS_MASK ; <nl>  <nl> DPRINTF ("+++ C + mode offloaded task TSO MTU =% d IP data % d "
static int64_t seek_to_sector ( BlockDriverState * bs , int64_t sector_num ) <nl> offset = sector_num % s -> tracks ; <nl>  <nl> /* not allocated */ <nl> - if (( index > s -> catalog_size ) || ( s -> catalog_bitmap [ index ] == 0 )) <nl> + if (( index >= s -> catalog_size ) || ( s -> catalog_bitmap [ index ] == 0 )) <nl> return - 1 ; <nl> return <nl> (( uint64_t ) s -> catalog_bitmap [ index ] * s -> off_multiplier + offset ) * 512 ;
static int scsi_disk_initfn ( SCSIDevice * dev ) <nl> } <nl> s -> bs = s -> qdev . conf . dinfo -> bdrv ; <nl>  <nl> + if ( bdrv_is_sg ( s -> bs )) { <nl> + qemu_error (" scsi - disk : unwanted / dev / sg *\ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( bdrv_get_type_hint ( s -> bs ) == BDRV_TYPE_CDROM ) { <nl> s -> cluster_size = 4 ; <nl> } else {
static int qcrypto_ivgen_essiv_init ( QCryptoIVGen * ivgen , <nl> & salt , & nhash , <nl> errp ) < 0 ) { <nl> g_free ( essiv ); <nl> + g_free ( salt ); <nl> return - 1 ; <nl> } <nl> 
void ppc_slb_invalidate_all ( CPUPPCState * env ) <nl>  <nl> do_invalidate = 0 ; <nl> sr_base = env -> spr [ SPR_ASR ]; <nl> - for ( n = 0 ; n < env -> slb_nr ; n ++) { <nl> + /* XXX : Warning : slbia never invalidates the first segment */ <nl> + for ( n = 1 ; n < env -> slb_nr ; n ++) { <nl> tmp64 = ldq_phys ( sr_base ); <nl> if ( slb_is_valid ( tmp64 )) { <nl> slb_invalidate (& tmp64 );
static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> ( s -> incompatible_features & QCOW2_INCOMPAT_DIRTY )) { <nl> BdrvCheckResult result = { 0 }; <nl>  <nl> - ret = qcow2_check ( bs , & result , BDRV_FIX_ERRORS ); <nl> + ret = qcow2_check ( bs , & result , BDRV_FIX_ERRORS | BDRV_FIX_LEAKS ); <nl> if ( ret < 0 ) { <nl> error_setg_errno ( errp , - ret , " Could not repair dirty image "); <nl> goto fail ;
static ssize_t mp_dacl_listxattr ( FsContext * ctx , const char * path , <nl> } <nl>  <nl> /* len includes the trailing NUL */ <nl> - memcpy ( value , ACL_ACCESS , len ); <nl> + memcpy ( value , ACL_DEFAULT , len ); <nl> return 0 ; <nl> } <nl> 
static void handle_rev16 ( DisasContext * s , unsigned int sf , <nl> tcg_gen_shli_i64 ( tcg_rd , tcg_rd , 8 ); <nl> tcg_gen_or_i64 ( tcg_rd , tcg_rd , tcg_tmp ); <nl>  <nl> + tcg_temp_free_i64 ( mask ); <nl> tcg_temp_free_i64 ( tcg_tmp ); <nl> } <nl> 
static int megasas_ctrl_get_info ( MegasasState * s , MegasasCmd * cmd ) <nl> BusChild * kid ; <nl> int num_pd_disks = 0 ; <nl>  <nl> - memset (& info , 0x0 , cmd -> iov_size ); <nl> + memset (& info , 0x0 , dcmd_size ); <nl> if ( cmd -> iov_size < dcmd_size ) { <nl> trace_megasas_dcmd_invalid_xfer_len ( cmd -> index , cmd -> iov_size , <nl> dcmd_size );
void vhost_dev_cleanup ( struct vhost_dev * hdev ) <nl> g_free ( hdev -> mem ); <nl> g_free ( hdev -> mem_sections ); <nl> hdev -> vhost_ops -> vhost_backend_cleanup ( hdev ); <nl> + assert (! hdev -> log ); <nl> QLIST_REMOVE ( hdev , entry ); <nl> } <nl> 
static void vnc_dpy_copy ( DisplayChangeListener * dcl , <nl> } <nl> } <nl>  <nl> + if (! vd -> server ) { <nl> + /* no client connected */ <nl> + return ; <nl> + } <nl> /* do bitblit op on the local surface too */ <nl> pitch = vnc_server_fb_stride ( vd ); <nl> src_row = vnc_server_fb_ptr ( vd , src_x , src_y );
const VMStateDescription vmstate_ahci = { <nl> VMSTATE_UINT32 ( control_regs . impl , AHCIState ), <nl> VMSTATE_UINT32 ( control_regs . version , AHCIState ), <nl> VMSTATE_UINT32 ( idp_index , AHCIState ), <nl> - VMSTATE_INT32 ( ports , AHCIState ), <nl> + VMSTATE_INT32_EQUAL ( ports , AHCIState ), <nl> VMSTATE_END_OF_LIST () <nl> }, <nl> };
void cpu_x86_load_seg ( CPUX86State * s , int seg_reg , int selector ) <nl> cpu_x86_load_seg_cache ( env , seg_reg , selector , <nl> ( selector << 4 ), 0xffff , 0 ); <nl> } else { <nl> - load_seg ( seg_reg , selector ); <nl> + helper_load_seg ( seg_reg , selector ); <nl> } <nl> env = saved_env ; <nl> }
again : <nl> fail : <nl> qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) & l2_table ); <nl> fail_put : <nl> - if ( nb_clusters > 0 ) { <nl> + if ( m -> nb_clusters > 0 ) { <nl> QLIST_REMOVE ( m , next_in_flight ); <nl> } <nl> return ret ;
static void dhcp_decode ( const struct bootp_t * bp , int * pmsg_type , <nl> if ( p >= p_end ) <nl> break ; <nl> len = * p ++; <nl> + if ( p + len > p_end ) { <nl> + break ; <nl> + } <nl> DPRINTF (" dhcp : tag =% d len =% d \ n ", tag , len ); <nl>  <nl> switch ( tag ) {
static void acpi_dsdt_add_cpus ( Aml * scope , int smp_cpus ) <nl> uint16_t i ; <nl>  <nl> for ( i = 0 ; i < smp_cpus ; i ++) { <nl> - Aml * dev = aml_device (" C % 03x ", i ); <nl> + Aml * dev = aml_device (" C %. 03X ", i ); <nl> aml_append ( dev , aml_name_decl (" _HID ", aml_string (" ACPI0007 "))); <nl> aml_append ( dev , aml_name_decl (" _UID ", aml_int ( i ))); <nl> aml_append ( scope , dev );
static bool scsi_block_is_passthrough ( SCSIDiskState * s , uint8_t * buf ) <nl> * for the number of logical blocks specified in the length <nl> * field ). For other modes , do not use scatter / gather operation . <nl> */ <nl> - if (( buf [ 1 ] & 6 ) != 2 ) { <nl> + if (( buf [ 1 ] & 6 ) == 2 ) { <nl> return false ; <nl> } <nl> break ;
int kvm_cpu_exec ( CPUState * cpu ) <nl> break ; <nl> case KVM_EXIT_MMIO : <nl> DPRINTF (" handle_mmio \ n "); <nl> - qemu_mutex_lock_iothread (); <nl> + /* Called outside BQL */ <nl> address_space_rw (& address_space_memory , <nl> run -> mmio . phys_addr , attrs , <nl> run -> mmio . data , <nl> run -> mmio . len , <nl> run -> mmio . is_write ); <nl> - qemu_mutex_unlock_iothread (); <nl> ret = 0 ; <nl> break ; <nl> case KVM_EXIT_IRQ_WINDOW_OPEN :
qio_channel_socket_accept ( QIOChannelSocket * ioc , <nl> cioc -> fd = qemu_accept ( ioc -> fd , ( struct sockaddr *)& cioc -> remoteAddr , <nl> & cioc -> remoteAddrLen ); <nl> if ( cioc -> fd < 0 ) { <nl> - trace_qio_channel_socket_accept_fail ( ioc ); <nl> if ( errno == EINTR ) { <nl> goto retry ; <nl> } <nl> + error_setg_errno ( errp , errno , " Unable to accept connection "); <nl> + trace_qio_channel_socket_accept_fail ( ioc ); <nl> goto error ; <nl> } <nl> 
static int usb_xhci_initfn ( struct PCIDevice * dev ) <nl> if ( xhci -> numintrs > MAXINTRS ) { <nl> xhci -> numintrs = MAXINTRS ; <nl> } <nl> + while ( xhci -> numintrs & ( xhci -> numintrs - 1 )) { /* ! power of 2 */ <nl> + xhci -> numintrs ++; <nl> + } <nl> if ( xhci -> numintrs < 1 ) { <nl> xhci -> numintrs = 1 ; <nl> }
static void handle_qmp_command ( JSONMessageParser * parser , QList * tokens ) <nl> obj = qdict_get ( input , " arguments "); <nl> if (! obj ) { <nl> args = qdict_new (); <nl> + } else if ( qobject_type ( obj ) != QTYPE_QDICT ) { <nl> + qerror_report ( QERR_QMP_BAD_INPUT_OBJECT_MEMBER , " arguments ", " object "); <nl> + goto err_input ; <nl> } else { <nl> args = qobject_to_qdict ( obj ); <nl> QINCREF ( args );
static inline uint32_t efststeq ( CPUPPCState * env , uint32_t op1 , uint32_t op2 ) <nl> # define HELPER_SINGLE_SPE_CMP ( name ) \ <nl> uint32_t helper_e ## name ( CPUPPCState * env , uint32_t op1 , uint32_t op2 ) \ <nl> { \ <nl> - return e ## name ( env , op1 , op2 ) << 2 ; \ <nl> + return e ## name ( env , op1 , op2 ); \ <nl> } <nl> /* efststlt */ <nl> HELPER_SINGLE_SPE_CMP ( fststlt );
vpc_co_pwritev ( BlockDriverState * bs , uint64_t offset , uint64_t bytes , <nl> int64_t image_offset ; <nl> int64_t n_bytes ; <nl> int64_t bytes_done = 0 ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> VHDFooter * footer = ( VHDFooter *) s -> footer_buf ; <nl> QEMUIOVector local_qiov ; <nl> 
static int blkverify_open ( BlockDriverState * bs , QDict * options , int flags , <nl>  <nl> ret = 0 ; <nl> fail : <nl> + if ( ret < 0 ) { <nl> + bdrv_unref_child ( bs , bs -> file ); <nl> + } <nl> qemu_opts_del ( opts ); <nl> return ret ; <nl> }
static SpiceChannelList * qmp_query_spice_channels ( void ) <nl> struct sockaddr * paddr ; <nl> socklen_t plen ; <nl>  <nl> - if (!( item -> info -> flags & SPICE_CHANNEL_EVENT_FLAG_ADDR_EXT )) { <nl> - error_report (" invalid channel event "); <nl> - return NULL ; <nl> - } <nl> + assert ( item -> info -> flags & SPICE_CHANNEL_EVENT_FLAG_ADDR_EXT ); <nl>  <nl> chan = g_malloc0 ( sizeof (* chan )); <nl> chan -> value = g_malloc0 ( sizeof (* chan -> value ));
ssize_t pcnet_receive ( NetClientState * nc , const uint8_t * buf , size_t size_ ) <nl> int pktcount = 0 ; <nl>  <nl> if (! s -> looptest ) { <nl> + if ( size > 4092 ) { <nl> +# ifdef PCNET_DEBUG_RMD <nl> + fprintf ( stderr , " pcnet : truncates rx packet .\ n "); <nl> +# endif <nl> + size = 4092 ; <nl> + } <nl> memcpy ( src , buf , size ); <nl> /* no need to compute the CRC */ <nl> src [ size ] = 0 ;
static inline abi_long host_to_target_sockaddr ( abi_ulong target_addr , <nl> if ( len == 0 ) { <nl> return 0 ; <nl> } <nl> + assert ( addr ); <nl>  <nl> target_saddr = lock_user ( VERIFY_WRITE , target_addr , len , 0 ); <nl> if (! target_saddr )
static void uart_write ( void * opaque , hwaddr offset , <nl>  <nl> DB_PRINT (" offset :% x data :% 08x \ n ", ( unsigned ) offset , ( unsigned ) value ); <nl> offset >>= 2 ; <nl> + if ( offset >= CADENCE_UART_R_MAX ) { <nl> + return ; <nl> + } <nl> switch ( offset ) { <nl> case R_IER : /* ier ( wts imr ) */ <nl> s -> r [ R_IMR ] |= value ;
static ssize_t gem_receive ( VLANClientState * nc , const uint8_t * buf , size_t size ) <nl> */ <nl>  <nl> memcpy ( rxbuf , buf , size ); <nl> - memset ( rxbuf + size , 0 , sizeof ( rxbuf - size )); <nl> + memset ( rxbuf + size , 0 , sizeof ( rxbuf ) - size ); <nl> rxbuf_ptr = rxbuf ; <nl> crc_val = cpu_to_le32 ( crc32 ( 0 , rxbuf , MAX ( size , 60 ))); <nl> if ( size < 60 ) {
int qemu_chr_fe_write ( CharDriverState * s , const uint8_t * buf , int len ) <nl> int qemu_chr_fe_write_all ( CharDriverState * s , const uint8_t * buf , int len ) <nl> { <nl> int offset = 0 ; <nl> - int res ; <nl> + int res = 0 ; <nl>  <nl> qemu_mutex_lock (& s -> chr_write_lock ); <nl> while ( offset < len ) {
static void iothread_instance_finalize ( Object * obj ) <nl> iothread_stop ( obj , NULL ); <nl> qemu_cond_destroy (& iothread -> init_done_cond ); <nl> qemu_mutex_destroy (& iothread -> init_done_lock ); <nl> + if (! iothread -> ctx ) { <nl> + return ; <nl> + } <nl> aio_context_unref ( iothread -> ctx ); <nl> } <nl> 
void do_device_add ( Monitor * mon , const QDict * qdict ) <nl>  <nl> opts = qemu_opts_parse (& qemu_device_opts , <nl> qdict_get_str ( qdict , " config "), " driver "); <nl> - if ( opts && ! qdev_device_help ( opts )) <nl> - qdev_device_add ( opts ); <nl> + if ( opts ) { <nl> + if ( qdev_device_help ( opts ) || qdev_device_add ( opts ) == NULL ) { <nl> + qemu_opts_del ( opts ); <nl> + } <nl> + } <nl> } <nl>  <nl> void do_device_del ( Monitor * mon , const QDict * qdict )
static void vmdk_free_last_extent ( BlockDriverState * bs ) <nl> static uint32_t vmdk_read_cid ( BlockDriverState * bs , int parent ) <nl> { <nl> char desc [ DESC_SIZE ]; <nl> - uint32_t cid ; <nl> + uint32_t cid = 0 ; <nl> const char * p_name , * cid_str ; <nl> size_t cid_str_size ; <nl> BDRVVmdkState * s = bs -> opaque ;
Coroutine * qemu_coroutine_new ( void ) <nl> stack_t oss ; <nl> sigset_t sigs ; <nl> sigset_t osigs ; <nl> - jmp_buf old_env ; <nl> + sigjmp_buf old_env ; <nl>  <nl> /* The way to manipulate stack is with the sigaltstack function . We <nl> * prepare a stack , with it delivering a signal to ourselves and then
static void ivshmem_read ( void * opaque , const uint8_t * buf , int size ) <nl> if ( incoming_posn == - 1 ) { <nl> void * map_ptr ; <nl>  <nl> + if ( s -> shm_fd >= 0 ) { <nl> + error_report (" shm already initialized "); <nl> + close ( incoming_fd ); <nl> + return ; <nl> + } <nl> + <nl> if ( check_shm_size ( s , incoming_fd , & err ) == - 1 ) { <nl> error_report_err ( err ); <nl> close ( incoming_fd );
static QString * read_line ( FILE * file , char * key ) <nl> { <nl> char value [ 128 ]; <nl>  <nl> - if ( fscanf ( file , "% s % s ", key , value ) == EOF ) <nl> + if ( fscanf ( file , "% 127s % 127s ", key , value ) == EOF ) { <nl> return NULL ; <nl> + } <nl> remove_dots ( key ); <nl> return qstring_from_str ( value ); <nl> }
void tcg_gen_ld8u_i64 ( TCGv_i64 ret , TCGv_ptr arg2 , tcg_target_long offset ) <nl> void tcg_gen_ld8s_i64 ( TCGv_i64 ret , TCGv_ptr arg2 , tcg_target_long offset ) <nl> { <nl> tcg_gen_ld8s_i32 ( TCGV_LOW ( ret ), arg2 , offset ); <nl> - tcg_gen_sari_i32 ( TCGV_HIGH ( ret ), TCGV_HIGH ( ret ), 31 ); <nl> + tcg_gen_sari_i32 ( TCGV_HIGH ( ret ), TCGV_LOW ( ret ), 31 ); <nl> } <nl>  <nl> void tcg_gen_ld16u_i64 ( TCGv_i64 ret , TCGv_ptr arg2 , tcg_target_long offset )
void replay_add_event ( ReplayAsyncEventKind event_kind , <nl>  <nl> void replay_bh_schedule_event ( QEMUBH * bh ) <nl> { <nl> - if ( replay_mode != REPLAY_MODE_NONE ) { <nl> + if ( replay_mode != REPLAY_MODE_NONE && events_enabled ) { <nl> uint64_t id = replay_get_current_step (); <nl> replay_add_event ( REPLAY_ASYNC_EVENT_BH , bh , NULL , id ); <nl> } else {
static void stm32f2xx_timer_write ( void * opaque , hwaddr offset , <nl> return ; <nl> case TIM_PSC : <nl> timer_val = stm32f2xx_ns_to_ticks ( s , now ) - s -> tick_offset ; <nl> - s -> tim_psc = value ; <nl> + s -> tim_psc = value & 0xFFFF ; <nl> value = timer_val ; <nl> break ; <nl> case TIM_CNT :
char * object_property_get_str ( Object * obj , const char * name , <nl> void object_property_set_link ( Object * obj , Object * value , <nl> const char * name , Error ** errp ) <nl> { <nl> - object_property_set_str ( obj , object_get_canonical_path ( value ), <nl> - name , errp ); <nl> + gchar * path = object_get_canonical_path ( value ); <nl> + object_property_set_str ( obj , path , name , errp ); <nl> + g_free ( path ); <nl> } <nl>  <nl> Object * object_property_get_link ( Object * obj , const char * name ,
static void spapr_phb_realize ( DeviceState * dev , Error ** errp ) <nl> } <nl> # endif <nl>  <nl> - memory_region_init_io (& sphb -> msiwindow , NULL , & spapr_msi_ops , spapr , <nl> + memory_region_init_io (& sphb -> msiwindow , OBJECT ( sphb ), & spapr_msi_ops , spapr , <nl> " msi ", msi_window_size ); <nl> memory_region_add_subregion (& sphb -> iommu_root , SPAPR_PCI_MSI_WINDOW , <nl> & sphb -> msiwindow );
static void taihu_405ep_init ( ram_addr_t ram_size , <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> if ( filename ) { <nl> bios_size = load_image ( filename , qemu_get_ram_ptr ( bios_offset )); <nl> + qemu_free ( filename ); <nl> } else { <nl> bios_size = - 1 ; <nl> }
static void test_io_channel_tls ( const void * opaque ) <nl> mainloop = g_main_context_default (); <nl> do { <nl> g_main_context_iteration ( mainloop , TRUE ); <nl> - } while (! clientHandshake . finished && <nl> + } while (! clientHandshake . finished || <nl> ! serverHandshake . finished ); <nl>  <nl> g_assert ( clientHandshake . failed == data -> expectClientFail );
static int sd_create ( const char * filename , QemuOpts * opts , <nl> bdrv_unref ( bs ); <nl> } <nl>  <nl> + s -> aio_context = qemu_get_aio_context (); <nl> ret = do_sd_create ( s , & vid , 0 , errp ); <nl> if ( ret ) { <nl> goto out ;
static int exynos4210_combiner_init ( SysBusDevice * sbd ) <nl> qdev_init_gpio_in ( dev , exynos4210_combiner_handler , IIC_NIRQ ); <nl>  <nl> /* Connect SysBusDev irqs to device specific irqs */ <nl> - for ( i = 0 ; i < IIC_NIRQ ; i ++) { <nl> + for ( i = 0 ; i < IIC_NGRP ; i ++) { <nl> sysbus_init_irq ( sbd , & s -> output_irq [ i ]); <nl> } <nl> 
static int vmxnet3_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> + vmxnet3_validate_queues ( s ); <nl> vmxnet3_validate_interrupts ( s ); <nl>  <nl> return 0 ;
static int vhost_kernel_memslots_limit ( struct vhost_dev * dev ) <nl> & s , NULL , NULL )) { <nl> uint64_t val = g_ascii_strtoull ( s , NULL , 10 ); <nl> if (!(( val == G_MAXUINT64 || ! val ) && errno )) { <nl> + g_free ( s ); <nl> return val ; <nl> } <nl> error_report (" ignoring invalid max_mem_regions value in vhost module :" <nl> " % s ", s ); <nl> } <nl> + g_free ( s ); <nl> return limit ; <nl> } <nl> 
void cpu_exec_init ( CPUArchState * env ) <nl> # ifndef CONFIG_USER_ONLY <nl> cpu -> as = & address_space_memory ; <nl> cpu -> thread_id = qemu_get_thread_id (); <nl> + cpu_reload_memory_map ( cpu ); <nl> # endif <nl> QTAILQ_INSERT_TAIL (& cpus , cpu , node ); <nl> # if defined ( CONFIG_USER_ONLY )
void tlb_fill ( CPUState * env1 , target_ulong addr , int is_write , int mmu_idx , <nl> int ret ; <nl>  <nl> saved_env = env ; <nl> + env = env1 ; <nl> ret = cpu_arm_handle_mmu_fault ( env , addr , is_write , mmu_idx ); <nl> if ( unlikely ( ret )) { <nl> if ( retaddr ) {
static void run_block_job ( BlockJob * job , Error ** errp ) <nl>  <nl> do { <nl> aio_poll ( aio_context , true ); <nl> - qemu_progress_print (( float ) job -> offset / job -> len * 100 . f , 0 ); <nl> + qemu_progress_print ( job -> len ? <nl> + (( float ) job -> offset / job -> len * 100 . f ) : 0 . 0f , 0 ); <nl> } while (! job -> ready ); <nl>  <nl> block_job_complete_sync ( job , errp );
static void kvm_ioapic_put ( IOAPICCommonState * s ) <nl>  <nl> void kvm_ioapic_dump_state ( Monitor * mon , const QDict * qdict ) <nl> { <nl> - IOAPICCommonState s ; <nl> + IOAPICCommonState * s = IOAPIC_COMMON ( object_resolve_path (" ioapic ", NULL )); <nl>  <nl> - kvm_ioapic_get (& s ); <nl> - <nl> - ioapic_print_redtbl ( mon , & s ); <nl> + assert ( s ); <nl> + kvm_ioapic_get ( s ); <nl> + ioapic_print_redtbl ( mon , s ); <nl> } <nl>  <nl> static void kvm_ioapic_reset ( DeviceState * dev )
static inline void gdb_continue ( GDBState * s ) <nl> # ifdef CONFIG_USER_ONLY <nl> s -> running_state = 1 ; <nl> # else <nl> - if ( runstate_check ( RUN_STATE_DEBUG )) { <nl> + if (! runstate_needs_reset ()) { <nl> vm_start (); <nl> } <nl> # endif
static int usb_xhci_post_load ( void * opaque , int version_id ) <nl> slot -> uport = xhci_lookup_uport ( xhci , slot_ctx ); <nl> assert ( slot -> uport && slot -> uport -> dev ); <nl>  <nl> - for ( epid = 1 ; epid <= 32 ; epid ++) { <nl> + for ( epid = 1 ; epid <= 31 ; epid ++) { <nl> pctx = slot -> ctx + 32 * epid ; <nl> xhci_dma_read_u32s ( xhci , pctx , ep_ctx , sizeof ( ep_ctx )); <nl> state = ep_ctx [ 0 ] & EP_STATE_MASK ;
static uint64_t get_migration_pass ( void ) <nl> } else { <nl> rsp_ram = qdict_get_qdict ( rsp_return , " ram "); <nl> result = qdict_get_try_int ( rsp_ram , " dirty - sync - count ", 0 ); <nl> - QDECREF ( rsp ); <nl> } <nl> + QDECREF ( rsp ); <nl> return result ; <nl> } <nl> 
ivshmem_server_handle_new_conn ( IvshmemServer * server ) <nl> } <nl> if ( i == G_MAXUINT16 ) { <nl> IVSHMEM_SERVER_DEBUG ( server , " cannot allocate new client id \ n "); <nl> - goto fail ; <nl> + close ( newfd ); <nl> + g_free ( peer ); <nl> + return - 1 ; <nl> } <nl> peer -> id = server -> cur_id ++; <nl> 
static void cleanup_infolist ( CommandLineParameterInfoList * head ) <nl> if (! strcmp ( pre_entry -> value -> name , cur -> next -> value -> name )) { <nl> del_entry = cur -> next ; <nl> cur -> next = cur -> next -> next ; <nl> - g_free ( del_entry ); <nl> + del_entry -> next = NULL ; <nl> + qapi_free_CommandLineParameterInfoList ( del_entry ); <nl> break ; <nl> } <nl> pre_entry = pre_entry -> next ;
void gen_intermediate_code ( CPUAlphaState * env , struct TranslationBlock * tb ) <nl> num_insns ++; <nl>  <nl> if ( unlikely ( cpu_breakpoint_test ( cs , ctx . pc , BP_ANY ))) { <nl> - gen_excp (& ctx , EXCP_DEBUG , 0 ); <nl> + ret = gen_excp (& ctx , EXCP_DEBUG , 0 ); <nl> /* The address covered by the breakpoint must be included in <nl> [ tb -> pc , tb -> pc + tb -> size ) in order to for it to be <nl> properly cleared -- thus we increment the PC here so that
static void qvirtio_scsi_pci_free ( QVirtIOSCSI * vs ) <nl> qvirtio_pci_device_disable ( container_of ( vs -> dev , QVirtioPCIDevice , vdev )); <nl> g_free ( vs -> dev ); <nl> qpci_free_pc ( vs -> bus ); <nl> + g_free ( vs ); <nl> } <nl>  <nl> static uint64_t qvirtio_scsi_alloc ( QVirtIOSCSI * vs , size_t alloc_size ,
int pci_add_capability ( PCIDevice * pdev , uint8_t cap_id , <nl> Error * local_err = NULL ; <nl>  <nl> ret = pci_add_capability2 ( pdev , cap_id , offset , size , & local_err ); <nl> - if ( local_err ) { <nl> - assert ( ret < 0 ); <nl> + if ( ret < 0 ) { <nl> error_report_err ( local_err ); <nl> - } else { <nl> - /* success implies a positive offset in config space */ <nl> - assert ( ret > 0 ); <nl> } <nl> return ret ; <nl> }
static void vga_draw_graphic ( VGACommonState * s , int full_update ) <nl> } else if ( is_buffer_shared ( surface ) && <nl> ( full_update || surface_data ( surface ) != s -> vram_ptr <nl> + ( s -> start_addr * 4 ))) { <nl> - DisplaySurface * surface ; <nl> surface = qemu_create_displaysurface_from ( disp_width , <nl> height , depth , s -> line_offset , <nl> s -> vram_ptr + ( s -> start_addr * 4 ), byteswap );
vubr_panic ( VuDev * dev , const char * msg ) <nl> vubr -> quit = 1 ; <nl> } <nl>  <nl> + static bool <nl> + vubr_queue_is_processed_in_order ( VuDev * dev , int qidx ) <nl> +{ <nl> + return true ; <nl> +} <nl> + <nl> static const VuDevIface vuiface = { <nl> . get_features = vubr_get_features , <nl> . set_features = vubr_set_features , <nl> . process_msg = vubr_process_msg , <nl> . queue_set_started = vubr_queue_set_started , <nl> + . queue_is_processed_in_order = vubr_queue_is_processed_in_order , <nl> }; <nl>  <nl> static void
static void usb_xhci_exit ( PCIDevice * dev ) <nl> /* destroy msix memory region */ <nl> if ( dev -> msix_table && dev -> msix_pba <nl> && dev -> msix_entry_used ) { <nl> - memory_region_del_subregion (& xhci -> mem , & dev -> msix_table_mmio ); <nl> - memory_region_del_subregion (& xhci -> mem , & dev -> msix_pba_mmio ); <nl> + msix_uninit ( dev , & xhci -> mem , & xhci -> mem ); <nl> } <nl>  <nl> usb_bus_release (& xhci -> bus );
BlockDriverAIOCB * laio_submit ( BlockDriverState * bs , void * aio_ctx , int fd , <nl> goto out_dec_count ; <nl> return & laiocb -> common ; <nl>  <nl> - out_free_aiocb : <nl> - qemu_aio_release ( laiocb ); <nl> out_dec_count : <nl> s -> count --; <nl> + out_free_aiocb : <nl> + qemu_aio_release ( laiocb ); <nl> return NULL ; <nl> } <nl> 
static void aml_free ( gpointer data , gpointer user_data ) <nl> { <nl> Aml * var = data ; <nl> build_free_array ( var -> buf ); <nl> + g_free ( var ); <nl> } <nl>  <nl> Aml * init_aml_allocator ( void )
static VncState * vnc_state ; /* needed for info vnc */ <nl>  <nl> void do_info_vnc ( void ) <nl> { <nl> - if ( vnc_state == NULL ) <nl> + if ( vnc_state == NULL || vnc_state -> display == NULL ) <nl> term_printf (" VNC server disabled \ n "); <nl> else { <nl> term_printf (" VNC server active on : ");
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> trace_qxl_enter_vga_mode ( d -> id ); <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> - memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + dpy_gfx_resize ( d -> ssd . ds ); <nl> vga_dirty_log_start (& d -> vga ); <nl> } <nl> 
static void disas_ldst_reg_imm9 ( DisasContext * s , uint32_t insn ) <nl> } <nl> } else { <nl> TCGv_i64 tcg_rt = cpu_reg ( s , rt ); <nl> - int memidx = is_unpriv ? 1 : get_mem_index ( s ); <nl> + int memidx = is_unpriv ? MMU_USER_IDX : get_mem_index ( s ); <nl>  <nl> if ( is_store ) { <nl> do_gpr_st_memidx ( s , tcg_rt , tcg_addr , size , memidx );
static TRBCCode xhci_disable_ep ( XHCIState * xhci , unsigned int slotid , <nl> usb_packet_cleanup (& epctx -> transfers [ i ]. packet ); <nl> } <nl>  <nl> - xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + /* only touch guest RAM if we ' re not resetting the HC */ <nl> + if ( xhci -> dcbaap_low || xhci -> dcbaap_high ) { <nl> + xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + } <nl>  <nl> timer_free ( epctx -> kick_timer ); <nl> g_free ( epctx );
static void set_pci_devfn ( Object * obj , Visitor * v , void * opaque , <nl>  <nl> visit_type_str ( v , & str , name , & local_err ); <nl> if ( local_err ) { <nl> + error_free ( local_err ); <nl> return set_int32 ( obj , v , opaque , name , errp ); <nl> } <nl> 
typedef struct USBNetState { <nl>  <nl> static int is_rndis ( USBNetState * s ) <nl> { <nl> - return s -> dev . config -> bConfigurationValue == DEV_RNDIS_CONFIG_VALUE ; <nl> + return s -> dev . config ? <nl> + s -> dev . config -> bConfigurationValue == DEV_RNDIS_CONFIG_VALUE : 0 ; <nl> } <nl>  <nl> static int ndis_query ( USBNetState * s , uint32_t oid ,
int kvm_init_vcpu ( CPUState * env ) <nl>  <nl> env -> kvm_fd = ret ; <nl> env -> kvm_state = s ; <nl> + env -> kvm_vcpu_dirty = 1 ; <nl>  <nl> mmap_size = kvm_ioctl ( s , KVM_GET_VCPU_MMAP_SIZE , 0 ); <nl> if ( mmap_size < 0 ) {
static int coroutine_fn bdrv_mirror_top_pdiscard ( BlockDriverState * bs , <nl>  <nl> static void bdrv_mirror_top_refresh_filename ( BlockDriverState * bs , QDict * opts ) <nl> { <nl> + if ( bs -> backing == NULL ) { <nl> + /* we can be here after failed bdrv_attach_child in <nl> + * bdrv_set_backing_hd */ <nl> + return ; <nl> + } <nl> bdrv_refresh_filename ( bs -> backing -> bs ); <nl> pstrcpy ( bs -> exact_filename , sizeof ( bs -> exact_filename ), <nl> bs -> backing -> bs -> filename );
void bdrv_image_info_specific_dump ( fprintf_function func_fprintf , void * f , <nl> assert ( qobject_type ( obj ) == QTYPE_QDICT ); <nl> data = qdict_get ( qobject_to_qdict ( obj ), " data "); <nl> dump_qobject ( func_fprintf , f , 1 , data ); <nl> + qobject_decref ( obj ); <nl> visit_free ( v ); <nl> } <nl> 
extern const PropertyInfo qdev_prop_link ; <nl> _arrayfield , _arrayprop , _arraytype ) { \ <nl> . name = ( PROP_ARRAY_LEN_PREFIX _name ), \ <nl> . info = &( qdev_prop_arraylen ), \ <nl> + . defval . u = 0 , \ <nl> . offset = offsetof ( _state , _field ) \ <nl> + type_check ( uint32_t , typeof_field ( _state , _field )), \ <nl> . arrayinfo = &( _arrayprop ), \
static void mmio_interface_realize ( DeviceState * dev , Error ** errp ) <nl>  <nl> if (! s -> host_ptr ) { <nl> error_setg ( errp , " host_ptr property must be set "); <nl> + return ; <nl> } <nl>  <nl> if (! s -> subregion ) { <nl> error_setg ( errp , " subregion property must be set "); <nl> + return ; <nl> } <nl>  <nl> memory_region_init_ram_ptr (& s -> ram_mem , OBJECT ( s ), " ram ",
struct ccw1 { <nl> __u8 flags ; <nl> __u16 count ; <nl> __u32 cda ; <nl> -} __attribute__ (( packed )); <nl> +} __attribute__ (( packed , aligned ( 8 ))); <nl>  <nl> # define CCW_FLAG_DC 0x80 <nl> # define CCW_FLAG_CC 0x40
static void pty_chr_state ( CharDriverState * chr , int connected ) <nl> s -> timer_tag = 0 ; <nl> } <nl> if (! s -> connected ) { <nl> - qemu_chr_be_generic_open ( chr ); <nl> s -> connected = 1 ; <nl> + qemu_chr_be_generic_open ( chr ); <nl> s -> fd_tag = io_add_watch_poll ( s -> fd , pty_chr_read_poll , pty_chr_read , chr ); <nl> } <nl> }
static inline void acpi_build_tables_init ( AcpiBuildTables * tables ) <nl> static inline void acpi_build_tables_cleanup ( AcpiBuildTables * tables , bool mfre ) <nl> { <nl> void * linker_data = bios_linker_loader_cleanup ( tables -> linker ); <nl> - if ( mfre ) { <nl> - g_free ( linker_data ); <nl> - } <nl> + g_free ( linker_data ); <nl> g_array_free ( tables -> rsdp , mfre ); <nl> - g_array_free ( tables -> table_data , mfre ); <nl> + g_array_free ( tables -> table_data , true ); <nl> g_array_free ( tables -> tcpalog , mfre ); <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> cmdline = g_strdup_printf ("- device ipmi - bmc - sim , id = bmc0 " <nl> " - device isa - ipmi - kcs , bmc = bmc0 "); <nl> qtest_start ( cmdline ); <nl> + g_free ( cmdline ); <nl> qtest_irq_intercept_in ( global_qtest , " ioapic "); <nl> qtest_add_func ("/ ipmi / local / kcs_base ", test_kcs_base ); <nl> qtest_add_func ("/ ipmi / local / kcs_abort ", test_kcs_abort );
static int do_readlink ( struct iovec * iovec , struct iovec * out_iovec ) <nl> } <nl> buffer = g_malloc ( size ); <nl> v9fs_string_init (& target ); <nl> - retval = readlink ( path . data , buffer , size ); <nl> + retval = readlink ( path . data , buffer , size - 1 ); <nl> if ( retval > 0 ) { <nl> buffer [ retval ] = '\ 0 '; <nl> v9fs_string_sprintf (& target , "% s ", buffer );
static int pci_add_option_rom ( PCIDevice * pdev , bool is_default_rom ) <nl> if ( size < 0 ) { <nl> error_report ("% s : failed to find romfile \"% s \"", <nl> __FUNCTION__ , pdev -> romfile ); <nl> + qemu_free ( path ); <nl> return - 1 ; <nl> } <nl> if ( size & ( size - 1 )) {
BlockBackend * blk_new_open ( const char * filename , const char * reference , <nl> } <nl>  <nl> blk -> root = bdrv_root_attach_child ( bs , " root ", & child_root , <nl> - perm , BLK_PERM_ALL , blk , & error_abort ); <nl> + perm , BLK_PERM_ALL , blk , errp ); <nl> + if (! blk -> root ) { <nl> + bdrv_unref ( bs ); <nl> + blk_unref ( blk ); <nl> + return NULL ; <nl> + } <nl>  <nl> return blk ; <nl> }
static int grow_refcount_table ( BlockDriverState * bs , int min_size ) <nl> qcow2_free_clusters ( bs , old_table_offset , old_table_size * sizeof ( uint64_t )); <nl> return 0 ; <nl> fail : <nl> - qcow2_free_clusters ( bs , table_offset , new_table_size2 ); <nl> qemu_free ( new_table ); <nl> return - EIO ; <nl> }
static void virtio_net_vhost_status ( VirtIONet * n , uint8_t status ) <nl> return ; <nl> } <nl>  <nl> - if (!! n -> vhost_started == virtio_net_started ( n , status ) && <nl> - ! nc -> peer -> link_down ) { <nl> + if (!! n -> vhost_started == <nl> + ( virtio_net_started ( n , status ) && ! nc -> peer -> link_down )) { <nl> return ; <nl> } <nl> if (! n -> vhost_started ) {
static void create_dummy_nodes ( RAGraph * g ) { <nl> const RListIter * it ; <nl> const RGraphEdge * e ; <nl>  <nl> - g -> long_edges = r_list_new (); <nl> + g -> long_edges = r_list_newf (( RListFree ) free ); <nl> dummy_vis . data = g -> long_edges ; <nl> dummy_vis . tree_edge = ( RGraphEdgeCallback ) view_dummy ; <nl> dummy_vis . fcross_edge = ( RGraphEdgeCallback ) view_dummy ;
R_API int r_debug_syscall ( RDebug * dbg , int num ) { <nl>  <nl> R_API int r_debug_kill ( RDebug * dbg , int pid , int tid , int sig ) { <nl> int ret = R_FALSE ; <nl> + if ( r_debug_is_dead ( dbg )) <nl> + return R_FALSE ; <nl> if ( dbg -> h && dbg -> h -> kill ) <nl> ret = dbg -> h -> kill ( dbg , pid , tid , sig ); <nl> else eprintf (" Backend does not implements kill ()\ n ");
struct r_bin_zimg_obj_t * r_bin_zimg_new_buf ( RBuffer * buf ) { <nl> goto fail ; <nl> } <nl>  <nl> + if ( r_buf_size ( bin -> b ) < sizeof ( struct zimg_header_t )) { <nl> + goto fail ; <nl> + } <nl> bin -> header = (*( struct zimg_header_t *) bin -> b -> buf ); <nl>  <nl> return bin ;
static int __plugin_open ( RIO * io , const char * file , ut8 many ) { <nl>  <nl> static RIODesc * __open ( RIO * io , const char * file , int rw , int mode ) { <nl> int ret , pid = getpid (); <nl> + if ( r_sandbox_enable ( 0 )) <nl> + return NULL ; <nl> io -> va = R_TRUE ; // nop <nl> ret = update_self_regions ( pid ); <nl> if ( ret ) {
static void get_chain_data ( proxy_data * pd , unsigned int * proxy_count , chain_typ <nl> } <nl> } <nl> fclose ( file ); <nl> + if (! count ) { <nl> + fprintf ( stderr , " error : no valid proxy found in config \ n "); <nl> + exit ( 1 ); <nl> + } <nl> * proxy_count = count ; <nl> proxychains_got_chain_data = 1 ; <nl> }
mgt_sigchld ( struct ev * e , int what ) <nl> ev_poker = NULL ; <nl>  <nl> r = wait4 (- 1 , & status , WNOHANG , NULL ); <nl> - if ( r != child_pid ) { <nl> + if ( r != child_pid || r == - 1 ) { <nl> fprintf ( stderr , " Unknown child died pid =% d status = 0x % x \ n ", <nl> r , status ); <nl> return ( 0 );
void logto ( char * logfile ) { <nl> uwsgi . logfile = logfile ; <nl>  <nl> if ( uwsgi . chmod_logfile_value ) { <nl> - if ( chmod ( uwsgi . logfile , uwsgi . chmod_logfile_value )) { <nl> - uwsgi_error (" chmod ()"); <nl> + if ( fchmod ( fd , uwsgi . chmod_logfile_value )) { <nl> + uwsgi_error (" fchmod ()"); <nl> } <nl> } <nl> }
void * uwsgi_python_tracebacker_thread ( void * foobar ) { <nl> uwsgi . no_defer_accept = current_defer_accept ; <nl>  <nl> PyObject * traceback_module = PyImport_ImportModule (" traceback "); <nl> - if (! traceback_module ) return NULL ; <nl> + if (! traceback_module ) { <nl> + free ( str_wid ); <nl> + free ( sock_path ); <nl> + close ( fd ); <nl> + return NULL ; <nl> + } <nl> PyObject * traceback_dict = PyModule_GetDict ( traceback_module ); <nl> PyObject * extract_stack = PyDict_GetItemString ( traceback_dict , " extract_stack "); <nl> 
void unit :: write ( config & cfg ) const <nl> break ; <nl> case unit_type :: LIMINAL : <nl> cfg [" alignment "] = " liminal "; <nl> + break ; <nl> default : <nl> cfg [" alignment "] = " neutral "; <nl> }
version_info :: version_info ( unsigned int major , unsigned int minor , unsigned int <nl> } <nl>  <nl> version_info :: version_info ( const std :: string & str ) <nl> - : special_ (""), special_separator_ ('\ 0 '), sane_ ( true ) <nl> + : nums_ () <nl> + , special_ ("") <nl> + , special_separator_ ('\ 0 ') <nl> + , sane_ ( true ) <nl> { <nl> const std :: vector < std :: string > string_parts = utils :: split ( str ,'.'); <nl> // first two components are required to be valid numbers , though
std :: vector < std :: string > get_text () { <nl> " _ " N_ ("+ Catalan Translation "), <nl> "- Carles Company ( brrr )", <nl> "- Dan Rosàs Garcia ( focks )", <nl> + "- Jonatan Alamà ( tin )", <nl> "- Jordà Polo ( ettin )", <nl> "- Mark Recasens ", <nl> "- Pau Rul · lan Ferragut ",
int config_parse_strv ( <nl> if (* sv ) <nl> for ( k = 0 ; (* sv )[ k ]; k ++) <nl> n [ k ] = (* sv )[ k ]; <nl> + else <nl> + k = 0 ; <nl>  <nl> FOREACH_WORD_QUOTED ( w , l , rvalue , state ) <nl> if (!( n [ k ++] = strndup ( w , l )))
static int dispatch_wqueue ( sd_bus * bus ) { <nl> * it got full , then all bets are off <nl> * anyway . */ <nl>  <nl> - sd_bus_message_unref ( bus -> wqueue [ 0 ]); <nl> bus -> wqueue_size --; <nl> + sd_bus_message_unref ( bus -> wqueue [ 0 ]); <nl> memmove ( bus -> wqueue , bus -> wqueue + 1 , sizeof ( sd_bus_message *) * bus -> wqueue_size ); <nl> bus -> windex = 0 ; <nl> 
int cunescape_length_with_prefix ( const char * s , size_t length , const char * prefi <nl> continue ; <nl> } <nl>  <nl> + free ( r ); <nl> return - EINVAL ; <nl> } <nl>  <nl> int cunescape_length_with_prefix ( const char * s , size_t length , const char * prefi <nl> continue ; <nl> } <nl>  <nl> + free ( r ); <nl> return k ; <nl> } <nl> 
int main ( int argc , char * argv []) { <nl>  <nl> umask ( 0022 ); <nl>  <nl> + /* Refuse to run unless we are in an initrd () */ <nl> + if (! in_initrd ()) <nl> + return EXIT_SUCCESS ; <nl> + <nl> device = argv [ 1 ]; <nl>  <nl> if ( stat ( device , & st ) < 0 ) {
int config_parse_exec_nice ( <nl> assert ( rvalue ); <nl> assert ( data ); <nl>  <nl> + if ( isempty ( rvalue )) { <nl> + c -> nice_set = false ; <nl> + return 0 ; <nl> + } <nl> + <nl> r = parse_nice ( rvalue , & priority ); <nl> if ( r < 0 ) { <nl> if ( r == - ERANGE )
struct udev_device * udev_monitor_receive_device ( struct udev_monitor * udev_monito <nl> udev_device_unref ( udev_device ); <nl> return NULL ; <nl> } <nl> - udev_device_set_devnum ( udev_device , makedev ( maj , min )); <nl> + if ( maj > 0 ) <nl> + udev_device_set_devnum ( udev_device , makedev ( maj , min )); <nl> udev_device_set_info_loaded ( udev_device ); <nl> return udev_device ; <nl> }
subst : <nl> cpos ++; <nl> while ( isspace ( cpos [ 0 ])) <nl> cpos ++; <nl> + if ( cpos [ 0 ] == '\ 0 ') <nl> + break ; <nl> } <nl> if ( i > 0 ) { <nl> log_error (" requested part of result string not found ");
int cg_pid_get_path ( const char * controller , pid_t pid , char ** path ) { <nl> if (! p ) <nl> return - ENOMEM ; <nl>  <nl> + /* Truncate suffix indicating the process is a zombie */ <nl> + e = endswith ( p , " ( deleted )"); <nl> + if ( e ) <nl> + * e = 0 ; <nl> + <nl> * path = p ; <nl> return 0 ; <nl> }
static int export_legacy_dbus_address ( <nl> _cleanup_free_ char * s = NULL ; <nl> int r ; <nl>  <nl> + /* skip export if kdbus is not active */ <nl> + if ( access ("/ dev / kdbus ", F_OK ) < 0 ) <nl> + return PAM_SUCCESS ; <nl> + <nl> if ( asprintf (& s , KERNEL_USER_BUS_FMT ";" UNIX_USER_BUS_FMT , <nl> ( unsigned long ) uid , runtime ) < 0 ) { <nl> pam_syslog ( handle , LOG_ERR , " Failed to set bus variable .");
static int mount_load_proc_self_mountinfo ( Manager * m , bool set_flags ) { <nl> options = mnt_fs_get_options ( fs ); <nl> fstype = mnt_fs_get_fstype ( fs ); <nl>  <nl> + if (! device || ! path ) <nl> + continue ; <nl> + <nl> if ( cunescape ( device , UNESCAPE_RELAX , & d ) < 0 ) <nl> return log_oom (); <nl> 
int dnssec_verify_rrset ( <nl> } <nl>  <nl> /* Collect all relevant RRs in a single array , so that we can look at the RRset */ <nl> - list = newa ( DnsResourceRecord *, a -> n_rrs ); <nl> + list = newa ( DnsResourceRecord *, dns_answer_size ( a )); <nl>  <nl> DNS_ANSWER_FOREACH ( rr , a ) { <nl> r = dns_resource_key_equal ( key , rr -> key );
static int service_collect_fds ( Service * s , int ** fds , unsigned * n_fds ) { <nl> p = manager_get_unit ( UNIT ( s )-> meta . manager , k ); <nl> free ( k ); <nl>  <nl> + if (! p ) <nl> + continue ; <nl> + <nl> if (( r = socket_collect_fds ( SOCKET ( p ), & cfds , & cn_fds )) < 0 ) <nl> goto fail ; <nl> 
gimp_device_manager_device_added ( GdkDeviceManager * gdk_manager , <nl> GdkDisplay * display ; <nl> GimpDeviceInfo * device_info ; <nl>  <nl> + if ( gdk_device_get_source ( device ) == GDK_SOURCE_KEYBOARD ) <nl> + return ; <nl> + <nl> display = gdk_device_manager_get_display ( gdk_manager ); <nl>  <nl> device_info =
layer_options_dialog_new ( GimpImage * image , <nl> GimpContainer * filters ; <nl> GtkWidget * view ; <nl>  <nl> - frame = gimp_frame_new (" Active Filters "); <nl> + frame = gimp_frame_new ( _ (" Active Filters ")); <nl> gtk_box_pack_start ( GTK_BOX ( left_vbox ), frame , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( frame ); <nl> 
unsigned int <nl> hb_ot_layout_table_get_lookup_count ( hb_face_t * face , <nl> hb_tag_t table_tag ) <nl> { <nl> + if ( unlikely (! hb_ot_shaper_face_data_ensure ( face ))) return 0 ; <nl> switch ( table_tag ) <nl> { <nl> case HB_OT_TAG_GSUB :
referenced_xids_note_snapshot_txn_end_iter ( OMTVALUE live_xidv , u_int32_t UU ( inde <nl> if (-- tuple -> references == 0 ) { <nl> r = toku_omt_delete_at ( referenced_xids , idx ); <nl> lazy_assert_zero ( r ); <nl> + toku_free ( tuple ); <nl> } <nl> done : <nl> return 0 ;
static void wsrep_mysql_parse ( THD * thd , char * rawbuf , uint length , <nl> } <nl> mysql_mutex_unlock (& thd -> LOCK_wsrep_thd ); <nl> } <nl> + <nl> + /* If retry is requested clean up explain structure */ <nl> + if ( thd -> wsrep_conflict_state == RETRY_AUTOCOMMIT && thd -> lex -> explain ) <nl> + delete_explain_query ( thd -> lex ); <nl> + <nl> } while ( thd -> wsrep_conflict_state == RETRY_AUTOCOMMIT ); <nl>  <nl> if ( thd -> wsrep_retry_query )
 <nl> # include < File . hpp > <nl>  <nl> +// alt use PATH_MAX <nl> +# ifndef MAXPATHLEN <nl> +# define MAXPATHLEN 1024 <nl> +# endif <nl> + <nl> // <nl> // PUBLIC <nl> //
build_index ( DB_INDEXER * indexer ) { <nl> else { <nl> invariant ( prov_info . le ); <nl> invariant ( prov_info . ule ); <nl> - LEAFENTRY le = prov_info . le ; <nl> ULEHANDLE ule = prov_info . ule ; <nl> for ( int which_db = 0 ; ( which_db < indexer -> i -> N ) && ( result == 0 ); which_db ++) { <nl> DB * db = indexer -> i -> dest_dbs [ which_db ];
btr_cur_optimistic_insert ( <nl> if ( UNIV_UNLIKELY (! btr_page_reorganize ( block , index , mtr ))) { <nl> ut_a ( buf_block_get_page_zip ( block )); <nl>  <nl> + ibuf_reset_free_bits_with_type ( index -> type , block ); <nl> + <nl> goto fail ; <nl> } <nl> 
__weak_alias ( vis , _vis ) <nl> # define MAXEXTRAS 5 <nl>  <nl>  <nl> - char * MAKEEXTRALIST ( uint flag , const char * orig ) <nl> + char * MAKEEXTRALIST ( unsigned int flag , const char * orig ) <nl> { <nl> const char * o = orig ; <nl> char * e , * extra ;
int subselect_indexsubquery_engine :: exec () <nl> null_finding = 1 ; <nl> /* Check if there exists a row with a null value in the index */ <nl> if (( error = safe_index_read ( tab ))) <nl> + { <nl> + if ( error < 0 ) <nl> + error = 0 ; // Key not found <nl> break ; <nl> + } <nl> } <nl> } <nl> }
static uint remove_key ( MI_KEYDEF * keyinfo , uint nod_flag , <nl> else <nl> get_key_length ( rest_length , keypos ); <nl>  <nl> - if ( next_length > prev_length ) <nl> + if ( next_length >= prev_length ) <nl> { /* Key after is based on deleted key */ <nl> uint pack_length , tmp ; <nl> bmove_upp (( char *) keypos ,( char *) ( lastkey + next_length ),
row_purge_remove_sec_if_poss_low ( <nl>  <nl> mtr_start (& mtr ); <nl>  <nl> - btr_cur -> thr = que_node_get_parent ( node ); <nl> - <nl> row_search_index_entry (& was_buffered , index , entry , <nl> BTR_MODIFY_LEAF | BTR_DELETE , & pcur , <nl> & mtr );
extern INLINE double U64_TO_DBL ( uint64_t x ) { <nl> # endif <nl>  <nl> /* GCC has several useful attributes . */ <nl> -# ifdef __GNUC__ <nl> +# ifdef __GNUC__ && __GNUC_MAJOR__ >= 3 <nl> # define ATTR_NORETURN __attribute__ (( noreturn )) <nl> # define ATTR_PURE __attribute__ (( pure )) <nl> # define ATTR_MALLOC __attribute__ (( malloc ))
circuit_expire_building ( time_t now ) <nl> /* c_rend_ready circs measure age since timestamp_dirty , <nl> * because that ' s set when they switch purposes <nl> */ <nl> - if (! victim -> rend_query [ 0 ] || victim -> timestamp_dirty <= cutoff ) <nl> + if (! victim -> rend_query [ 0 ] || victim -> timestamp_dirty > cutoff ) <nl> continue ; <nl> break ; <nl> case CIRCUIT_PURPOSE_C_REND_READY_INTRO_ACKED :
add_an_entry_guard ( const node_t * chosen , int reset_status , int prepend , <nl> entry -> can_retry = 1 ; <nl> } <nl> entry -> is_dir_cache = node_is_dir ( node ); <nl> + if ( get_options ()-> UseBridges && node_is_a_configured_bridge ( node )) <nl> + entry -> is_dir_cache = 1 ; <nl> + <nl> return NULL ; <nl> } <nl> } else if (! for_directory ) {
static int _layouts_load_config_common ( layout_plugin_t * plugin , <nl> * calling the update_done layout callback */ <nl> updated_entities [ i ] = e ; <nl> } <nl> + xfree ( e_name ); <nl> + xfree ( e_type ); <nl>  <nl> /* ** Full load config only ( flags == 0 ) ** <nl> * post - read - and - build ( post stage 1 )
extern char * ba_set_ionode_str ( bitstr_t * ionode_bitmap ) <nl> if ( hl ) { <nl> ionode_str = hostlist_ranged_string_xmalloc_dims ( <nl> hl , 5 , 0 ); <nl> - info (" iostring is % s ", ionode_str ); <nl> + // info (" iostring is % s ", ionode_str ); <nl> hostlist_destroy ( hl ); <nl> hl = NULL ; <nl> }
mono_profiler_init_log ( const char * desc ) <nl> mono_profiler_set_gc_event_callback ( handle , gc_event ); <nl>  <nl> mono_profiler_set_thread_started_callback ( handle , thread_start ); <nl> - mono_profiler_set_thread_stopped_callback ( handle , thread_end ); <nl> + mono_profiler_set_thread_exited_callback ( handle , thread_end ); <nl> mono_profiler_set_thread_name_callback ( handle , thread_name ); <nl>  <nl> mono_profiler_set_domain_loaded_callback ( handle , domain_loaded );
mono_threads_attach_coop_internal ( MonoDomain * domain , gpointer * cookie , MonoSta <nl> { <nl> MonoDomain * orig ; <nl> MonoThreadInfo * info ; <nl> - gboolean external ; <nl> + gboolean external = FALSE ; <nl>  <nl> orig = mono_domain_get (); <nl> 
int main ( int argc , char ** argv ) <nl> if (! dh -> qdcount ) <nl> continue ; <nl>  <nl> + if ( pr . d_len < sizeof ( dnsheader )) <nl> + continue ; <nl> + <nl> uint16_t qtype , qclass ; <nl> DNSName qname ; <nl> try {
M_API M_bool M_queue_remove ( M_queue_t * queue , void * member ); <nl> * \ param member User - supplied queue object ( pointer ) to find . <nl> * \ return M_TRUE if member exists , M_FALSE otherwise . <nl> */ <nl> - M_bool M_queue_exists ( M_queue_t * queue , void * member ); <nl> + M_API M_bool M_queue_exists ( M_queue_t * queue , void * member ); <nl>  <nl>  <nl> /*! Take control of a user - supplied queue object ( pointer ). This will remove the
static void handle_cgi_request ( struct mg_connection * conn , const char * prog ) { <nl> struct file fout = STRUCT_FILE_INITIALIZER ; <nl> pid_t pid ; <nl>  <nl> + memset (& ri , 0 , sizeof ( ri )); <nl> prepare_cgi_environment ( conn , prog , & blk ); <nl>  <nl> // CGI must be executed in its own directory . ' dir ' must point to the
nautilus_path_bar_scroll_down ( NautilusPathBar * path_bar ) <nl> * from the end , removing buttons until we get all the space we <nl> * need . */ <nl> gtk_widget_get_allocation ( BUTTON_DATA ( up_button -> data )-> button , & button_allocation ); <nl> - while ( space_available < space_needed ) { <nl> + while (( space_available < space_needed ) && <nl> + ( up_button != NULL )) { <nl> space_available += button_allocation . width + path_bar -> spacing ; <nl> up_button = up_button -> prev ; <nl> path_bar -> first_scrolled_button = up_button ;
nautilus_window_slot_switch_new_content_view ( NautilusWindowSlot * slot ) <nl> GtkWidget * widget ; <nl> gboolean reusing_view ; <nl>  <nl> - reusing_view = gtk_widget_get_parent ( GTK_WIDGET ( slot -> details -> new_content_view )) != NULL ; <nl> + reusing_view = slot -> details -> new_content_view && <nl> + gtk_widget_get_parent ( GTK_WIDGET ( slot -> details -> new_content_view )) != NULL ; <nl> /* We are either reusing the view , so new_content_view and content_view <nl> * are the same , or the new_content_view is invalid */ <nl> if ( slot -> details -> new_content_view == NULL || reusing_view )
void gf_mpd_url_free ( void * _item ) <nl> { <nl> GF_MPD_URL * ptr = ( GF_MPD_URL *) _item ; <nl> if ( ptr -> sourceURL ) gf_free ( ptr -> sourceURL ); <nl> + if ( ptr -> byte_range ) gf_free ( ptr -> byte_range ); <nl> gf_free ( ptr ); <nl> } <nl> void gf_mpd_string_free ( void * _item ) {
static GF_Err gf_media_export_filters ( GF_MediaExporter * dumper ) <nl> } <nl> esd = gf_media_map_esd ( dumper -> file , track_num , 0 ); <nl> sample_count = gf_isom_get_sample_count ( dumper -> file , dumper -> trackID ); <nl> - if ( esd ) { <nl> + if ( esd && esd -> decoderConfig ) { <nl> if ( esd -> decoderConfig -> objectTypeIndication < GF_CODECID_LAST_MPEG4_MAPPING ) { <nl> codec_id = gf_codecid_from_oti ( esd -> decoderConfig -> streamType , esd -> decoderConfig -> objectTypeIndication ); <nl> # ifndef GPAC_DISABLE_AV_PARSERS
void IMG_NetIO ( void * cbk , GF_NETIO_Parameter * param ) <nl> e = param -> error ; <nl> /* wait to get the whole file */ <nl> if (! e && ( param -> msg_type != GF_NETIO_DATA_TRANSFERED )) return ; <nl> + if (( e == GF_EOS ) && ( param -> msg_type == GF_NETIO_DATA_EXCHANGE )) return ; <nl>  <nl> if ( param -> msg_type == GF_NETIO_DATA_TRANSFERED ) { <nl> szCache = gf_dm_sess_get_cache_name ( read -> dnload );
static int asn1_decode_entry ( sc_context_t * ctx , struct sc_asn1_entry * entry , <nl>  <nl> /* Strip off padding zero */ <nl> if (( entry -> flags & SC_ASN1_UNSIGNED ) <nl> - && obj [ 0 ] == 0x00 && objlen > 1 ) { <nl> + && objlen > 1 && obj [ 0 ] == 0x00 ) { <nl> objlen --; <nl> obj ++; <nl> }
void sc_notify_id ( struct sc_context * ctx , struct sc_atr * atr , <nl>  <nl> switch ( id ) { <nl> case NOTIFY_CARD_INSERTED : <nl> - icon = " dialog - information "; <nl> + icon = " contact - new "; <nl> break ; <nl> case NOTIFY_CARD_REMOVED : <nl> - icon = " media - removed "; <nl> + icon = " media - eject "; <nl> break ; <nl> case NOTIFY_PIN_GOOD : <nl> icon = " changes - allow ";
namespace XrdCl <nl> pFileUrl ( 0 ), <nl> pDataServer ( 0 ), <nl> pLoadBalancer ( 0 ), <nl> + pStateRedirect ( 0 ), <nl> pFileHandle ( 0 ), <nl> pOpenMode ( 0 ), <nl> pOpenFlags ( 0 ),
static void mygroup_delete ( mygroup_t * mg ) <nl> } <nl>  <nl> metadata_delete_all ( mg ); <nl> + BlockHeapFree ( mygroup_heap , mg ); <nl> } <nl>  <nl> mygroup_t * mygroup_add ( const char * name ) <nl> mygroup_t * mygroup_find ( const char * name ) <nl> static void groupacs_des ( groupacs_t * ga ) <nl> { <nl> metadata_delete_all ( ga ); <nl> - /* XXX nothing */ <nl> + BlockHeapFree ( groupacs_heap , ga ); <nl> } <nl>  <nl> groupacs_t * groupacs_add ( mygroup_t * mg , myuser_t * mu , unsigned int flags )
msg_create ( mqueue_t * mq , const char * message ) <nl> } <nl>  <nl> mowgli_node_add ( msg , & msg -> node , & mq -> entries ); <nl> + mq -> last_used = CURRTIME ; <nl>  <nl> return msg ; <nl> }
class ModuleSSLGnuTLS : public Module <nl> // once a day , once a week or once a month . Depending on the <nl> // security requirements . <nl>  <nl> + if (! dh_alloc ) <nl> + return ; <nl> + <nl> int ret ; <nl>  <nl> if (( ret = gnutls_dh_params_generate2 ( dh_params , dh_bits )) < 0 )
class ModuleNickFlood : public Module <nl>  <nl> virtual int OnUserPreNick ( userrec * user , const std :: string & newnick ) <nl> { <nl> + if ( isdigit ( newnick [ 0 ])) /* allow switches to UID */ <nl> + return 0 ; <nl> + <nl> for ( UCListIter i = user -> chans . begin (); i != user -> chans . end (); i ++) <nl> { <nl> chanrec * channel = i -> first ;
ModeAction ModeChannelLimit :: OnModeChange ( userrec * source , userrec * dest , chanre <nl> return MODEACTION_DENY ; <nl> } <nl>  <nl> + parameter = ConvToStr ( limit ); <nl> + <nl> /* Set new limit */ <nl> channel -> limit = limit ; <nl> channel -> modes [ CM_LIMIT ] = 1 ;
void NativeWindowViews :: SetParentWindow ( NativeWindow * parent ) { <nl>  <nl> void NativeWindowViews :: SetModal ( bool modal ) { <nl> # if defined ( USE_X11 ) <nl> + SetWindowType ( GetAcceleratedWidget (), modal ? " dialog " : " normal "); <nl> + Show (); <nl> SetWMSpecState ( GetAcceleratedWidget (), modal , <nl> GetAtom (" _NET_WM_STATE_MODAL ")); <nl> - SetWindowType ( GetAcceleratedWidget (), modal ? " dialog " : " normal "); <nl> # endif <nl> } <nl> 
DEFUN ( ipv6_ospf6_priority , <nl>  <nl> oi -> priority = strtol ( argv [ 0 ], NULL , 10 ); <nl>  <nl> - if ( oi -> area ) <nl> + if ( oi -> area && <nl> + ( oi -> state == OSPF6_INTERFACE_DROTHER || <nl> + oi -> state == OSPF6_INTERFACE_BDR || <nl> + oi -> state == OSPF6_INTERFACE_DR )) <nl> ospf6_interface_state_change ( dr_election ( oi ), oi ); <nl>  <nl> return CMD_SUCCESS ;
 <nl> # define STREAM_VERIFY_SANE ( S ) \ <nl> do { \ <nl> - if ( !( GETP_VALID ( S , ( S )-> getp )) && ENDP_VALID ( S , ( S )-> endp ) ) \ <nl> + if ( !( GETP_VALID ( S , ( S )-> getp ) && ENDP_VALID ( S , ( S )-> endp )) ) \ <nl> STREAM_WARN_OFFSETS ( S ); \ <nl> assert ( GETP_VALID ( S , ( S )-> getp ) ); \ <nl> assert ( ENDP_VALID ( S , ( S )-> endp ) ); \
community_del_val ( struct community * com , u_int32_t * val ) <nl> c = com -> size - i - 1 ; <nl>  <nl> if ( c > 0 ) <nl> - memcpy ( com -> val + i , com -> val + ( i + 1 ), c * sizeof ( val )); <nl> + memcpy ( com -> val + i , com -> val + ( i + 1 ), c * sizeof (* val )); <nl>  <nl> com -> size --; <nl> 
genop_peep ( codegen_scope * s , mrb_code i , int val ) <nl> return 0 ; <nl> } <nl> } <nl> + if ( c0 == OP_LOADNIL ) { <nl> + if ( GETARG_B ( i ) == GETARG_A ( i0 )) { <nl> + s -> pc --; <nl> + return 0 ; <nl> + } <nl> + } <nl> break ; <nl> case OP_JMPIF : <nl> case OP_JMPNOT :
mrb_irep_free ( mrb_state * mrb , mrb_irep * irep ) <nl> } <nl> mrb_free ( mrb , irep -> pool ); <nl> mrb_free ( mrb , irep -> syms ); <nl> + mrb_free ( mrb , irep -> reps ); <nl> mrb_free ( mrb , ( void *) irep -> filename ); <nl> mrb_free ( mrb , irep -> lines ); <nl> mrb_debug_info_free ( mrb , irep -> debug_info );
read_section_debug ( mrb_state * mrb , const uint8_t * start , mrb_irep * irep ) <nl> result = read_debug_record ( mrb , bin , irep , & len , filenames , filenames_len ); <nl> if ( result != MRB_DUMP_OK ) goto debug_exit ; <nl>  <nl> + bin += len ; <nl> if (( bin - start ) != bin_to_uint32 ( header -> section_size )) { <nl> result = MRB_DUMP_GENERAL_FAILURE ; <nl> }
static stf_status ikev2_child_out_tail ( struct msg_digest * md ) <nl>  <nl> DBG_log (" ikev2_child_sa_respond returned STF_FAIL with % s ", <nl> enum_name (& ikev2_notify_names , v2_notify_num )); <nl> + return ret ; /* abort building the response message */ <nl> } else if ( ret != STF_OK ) { <nl> DBG_log (" ikev2_child_sa_respond returned % s ", <nl> enum_name (& stfstatus_name , ret )); <nl> + return ret ; /* abort building the response message */ <nl> } <nl> + <nl> if (! ikev2_padup_pre_encrypt ( pst , & e_pbs_cipher )) <nl> return STF_INTERNAL_ERROR ; <nl> 
int do_pam_authentication ( void * varg ) <nl>  <nl> retval = pam_start (" pluto ", arg -> name . ptr , & conv , & pamh ); <nl>  <nl> + /* Send the remote host address to PAM */ <nl> + if ( retval == PAM_SUCCESS ) <nl> + retval = pam_set_item ( pamh , PAM_RHOST , pluto_ip_str (& arg -> st -> st_remoteaddr )); <nl> /* Two factor authentication - Check that the user is valid , <nl> and then check if they are permitted access */ <nl> if ( retval == PAM_SUCCESS )
cherokee_validator_ldap_check ( cherokee_validator_ldap_t * ldap , <nl> /* Sanity checks <nl> */ <nl> if (( conn -> validator == NULL ) || <nl> - cherokee_buffer_is_empty (& conn -> validator -> user )) <nl> + cherokee_buffer_is_empty (& conn -> validator -> user ) || <nl> + cherokee_buffer_is_empty (& conn -> validator -> passwd )) <nl> return ret_error ; <nl>  <nl> size = cherokee_buffer_cnt_cspn (& conn -> validator -> user , 0 , "*()");
cherokee_buffer_cnt_cspn ( cherokee_buffer_t * buf , cuint_t offset , const char * st <nl> crc_t <nl> cherokee_buffer_crc32 ( cherokee_buffer_t * buf ) <nl> { <nl> + if ( cherokee_buffer_is_empty ( buf )) <nl> + return 0 ; <nl> + <nl> return crc32_sz ( buf -> buf , buf -> len ); <nl> } <nl> 
ExprResolveBoolean ( struct xkb_context * ctx , const ExprDef * expr , <nl>  <nl> case EXPR_INVERT : <nl> case EXPR_NOT : <nl> - ok = ExprResolveBoolean ( ctx , expr , set_rtrn ); <nl> + ok = ExprResolveBoolean ( ctx , expr -> unary . child , set_rtrn ); <nl> if ( ok ) <nl> * set_rtrn = !* set_rtrn ; <nl> return ok ;
bool console_input_special_binds ( INPUT_EVENT e , void * user_data ) <nl>  <nl> bool console_input_normal_binds ( INPUT_EVENT e , void * user_data ) <nl> { <nl> + // need to be ingame for these binds <nl> + if ( client_state () != CLIENTSTATE_ONLINE ) <nl> + return false ; <nl> + <nl> // don ' t handle invalid events and keys that arn ' t set to anything <nl> if ( e . key <= 0 || e . key >= KEY_LAST || keybindings [ e . key ][ 0 ] == 0 ) <nl> return false ;
mmvJob :: mmvJob ( FileAccess * session , const ArgV * args , const char * t , FA :: open_mode <nl> { <nl> op . set ( args -> a0 ()); <nl> for ( int i = args -> getindex (); i < args -> count (); i ++) <nl> - wcd . push ( strdup ( args -> getarg ( i ))); <nl> + wcd . push ( xstrdup ( args -> getarg ( i ))); <nl> } <nl>  <nl> void mmvJob :: doOpen () const
int uv_async_init ( uv_async_t * async , uv_async_cb async_cb ) { <nl>  <nl> int uv_async_send ( uv_async_t * async ) { <nl> ev_async_send ( EV_DEFAULT_UC_ & async -> async_watcher ); <nl> + return 0 ; <nl> } <nl>  <nl> 
static void simple_decode_row ( x3f_info_t * I , x3f_directory_entry_t * DE , <nl> x3f_image_data_t * ID = & DEH -> data_subsection . image_data ; <nl> x3f_huffman_t * HUF = ID -> huffman ; <nl>  <nl> + if ( row * row_stride > ID -> data_size - ( ID -> columns * sizeof ( uint32_t ))) <nl> + throw LIBRAW_EXCEPTION_IO_CORRUPT ; <nl> uint32_t * data = ( uint32_t *)(( unsigned char *) ID -> data + row * row_stride ); <nl>  <nl> uint16_t c [ 3 ] = { 0 , 0 , 0 };
update_focus_app ( ShellWindowTracker * self ) <nl> } <nl>  <nl> set_focus_app ( self , new_focus_app ); <nl> + <nl> + g_clear_object (& new_focus_app ); <nl> } <nl>  <nl> static void
static int DNSUDPResponseParse ( Flow * f , void * dstate , <nl>  <nl> tx -> replied = 1 ; <nl> } <nl> - if ( dns_state != NULL && f != NULL ) { <nl> + if ( f != NULL ) { <nl> dns_state -> last_resp = f -> lastts ; <nl> } <nl> SCReturnInt ( 1 );
static CURLcode pop3_doing ( struct connectdata * conn , bool * dophase_done ) <nl> CURLcode result ; <nl> result = pop3_multi_statemach ( conn , dophase_done ); <nl>  <nl> - if (* dophase_done ) { <nl> + if (! result && * dophase_done ) { <nl> result = pop3_dophase_done ( conn , FALSE /* not connected */); <nl>  <nl> DEBUGF ( infof ( conn -> data , " DO phase is complete \ n "));
CURLcode Curl_http2_switched ( struct connectdata * conn , <nl> " after upgrade : len =% zu \ n ", <nl> nread ); <nl>  <nl> - memcpy ( httpc -> inbuf , mem , nread ); <nl> + if ( nread ) <nl> + memcpy ( httpc -> inbuf , mem , nread ); <nl> httpc -> inbuflen = nread ; <nl>  <nl> nproc = nghttp2_session_mem_recv ( httpc -> h2 , ( const uint8_t *) httpc -> inbuf ,
int ssl_get_ciphersuite_id ( const char * ciphersuite_name ) <nl>  <nl> const char * ssl_get_ciphersuite ( const ssl_context * ssl ) <nl> { <nl> + if ( ssl == NULL || ssl -> session == NULL ) <nl> + return NULL ; <nl> + <nl> return ssl_get_ciphersuite_name ( ssl -> session -> ciphersuite ); <nl> } <nl> 
int init_aliases ( void ) <nl> ( tail -> dir = strdup ( dir )) == NULL ) { <nl> die_mem (); <nl> } <nl> - tail -> next = NULL ; <nl> } else { <nl> DirAlias * curr ; <nl>  <nl> int init_aliases ( void ) <nl> tail -> next = curr ; <nl> tail = curr ; <nl> } <nl> + tail -> next = NULL ; <nl> } <nl> fclose ( fp ); <nl> aliases_up ++;
char ** argv ; <nl>  <nl> void readin () <nl> { <nl> - skelout (); <nl> - <nl> - if ( ddebug ) <nl> - puts ( "# define FLEX_DEBUG " ); <nl> - <nl> if ( csize == 256 ) <nl> puts ( " typedef unsigned char YY_CHAR ;" ); <nl> else <nl> puts ( " typedef char YY_CHAR ;" ); <nl>  <nl> + skelout (); <nl> + <nl> + if ( ddebug ) <nl> + puts ( "# define FLEX_DEBUG " ); <nl> + <nl> line_directive_out ( stdout ); <nl>  <nl> if ( yyparse () )
reswitch : <nl> goto error ; <nl> switch ( t -> etype ) { <nl> default : <nl> - yyerror (" invalid operation : % N ( index of type % T )", n , t ); <nl> + yyerror (" invalid operation : % N ( type % T does not support indexing )", n , t ); <nl> goto error ; <nl>  <nl> 
int ssl3_get_cert_verify ( SSL * s ) <nl> if ( s -> s3 -> tmp . message_type != SSL3_MT_CERTIFICATE_VERIFY ) <nl> { <nl> s -> s3 -> tmp . reuse_message = 1 ; <nl> - if (( peer != NULL ) && ( type & EVP_PKT_SIGN )) <nl> + if ( peer != NULL ) <nl> { <nl> al = SSL_AD_UNEXPECTED_MESSAGE ; <nl> SSLerr ( SSL_F_SSL3_GET_CERT_VERIFY , SSL_R_MISSING_VERIFY_MESSAGE );
X509_VERIFY_PARAM * X509_VERIFY_PARAM_new ( void ) <nl>  <nl> void X509_VERIFY_PARAM_free ( X509_VERIFY_PARAM * param ) <nl> { <nl> + if ( param == NULL ) <nl> + return ; <nl> x509_verify_param_zero ( param ); <nl> OPENSSL_free ( param -> id ); <nl> OPENSSL_free ( param );
static int pkey_rsa_ctrl ( EVP_PKEY_CTX * ctx , int type , int p1 , void * p2 ) <nl> *( const EVP_MD **) p2 = rctx -> md ; <nl> } else { <nl> if ( rsa_pss_restricted ( rctx )) { <nl> - if ( EVP_MD_type ( rctx -> md ) == EVP_MD_type ( p2 )) <nl> + if ( EVP_MD_type ( rctx -> mgf1md ) == EVP_MD_type ( p2 )) <nl> return 1 ; <nl> RSAerr ( RSA_F_PKEY_RSA_CTRL , RSA_R_MGF1_DIGEST_NOT_ALLOWED ); <nl> return 0 ;
int tls1_setup_key_block ( SSL * s ) <nl>  <nl> if (( p2 = ( unsigned char *) OPENSSL_malloc ( num )) == NULL ) { <nl> SSLerr ( SSL_F_TLS1_SETUP_KEY_BLOCK , ERR_R_MALLOC_FAILURE ); <nl> + OPENSSL_free ( p1 ); <nl> goto err ; <nl> } <nl> # ifdef TLS_DEBUG
tplProcessCnf ( struct cnfobj * o ) <nl> pTpl -> optFormatEscape = JSON_ESCAPE ; <nl>  <nl> finalize_it : <nl> + free ( tplStr ); <nl> if ( pvals != NULL ) <nl> cnfparamvalsDestruct ( pvals , & pblk ); <nl> if ( iRet != RS_RET_OK ) {
split_binary_parameters ( uchar ** const szBinary , char *** const __restrict__ aPara <nl> (* aParams )[ iPrm ] = NULL ; /* NULL per argv [] convention */ <nl>  <nl> finalize_it : <nl> + if ( estrBinary != param_binary ) { <nl> + es_deleteStr ( estrBinary ); <nl> + } <nl> + if ( estrParams != NULL ) { <nl> + es_deleteStr ( estrParams ); <nl> + } <nl> RETiRet ; <nl> }
addListner ( instanceConf_t * inst ) <nl> lcnfLast = newlcnfinfo ; <nl> } <nl> } <nl> + } else { <nl> + errmsg . LogError ( 0 , NO_ERRCODE , " imudp : Could not create udp listener ," <nl> + " ignoring port % s bind - address % s .", <nl> + port , bindAddr ); <nl> } <nl>  <nl> finalize_it :
irc_server_msgq_flush () <nl> /* new_msg = plugin_modifier_exec ( PLUGIN_MODIFIER_IRC_IN , <nl> irc_recv_msgq -> server -> name , <nl> ptr_data );*/ <nl> + new_msg = NULL ; <nl> + <nl> /* no changes in new message */ <nl> if ( new_msg && ( strcmp ( ptr_data , new_msg ) == 0 )) <nl> {
gui_chat_draw ( struct t_gui_buffer * buffer , int clear_chat ) <nl>  <nl> if ( clear_chat ) <nl> { <nl> - snprintf ( format_empty , 32 , "%%-% ds ", ptr_win -> win_chat_width ); <nl> + snprintf ( format_empty , sizeof ( format_empty ), <nl> + "%%-% ds ", ptr_win -> win_chat_width ); <nl> for ( i = 0 ; i < ptr_win -> win_chat_height ; i ++) <nl> { <nl> mvwprintw ( GUI_WINDOW_OBJECTS ( ptr_win )-> win_chat , i , 0 ,
static void doXPathDump ( xmlXPathObjectPtr cur ) { <nl> # ifdef LIBXML_OUTPUT_ENABLED <nl> xmlSaveCtxtPtr ctxt ; <nl>  <nl> - if ( cur -> nodesetval -> nodeNr <= 0 ) { <nl> + if (( cur -> nodesetval == NULL ) || ( cur -> nodesetval -> nodeNr <= 0 )) { <nl> fprintf ( stderr , " XPath set is empty \ n "); <nl> progresult = XMLLINT_ERR_XPATH ; <nl> break ;
static size_t add_index_color ( char * buf , size_t buflen , format_flag flags , char <nl> if (!( flags & MUTT_FORMAT_INDEX )) <nl> return 0 ; <nl>  <nl> + /* this item is going to be passed to an external filter */ <nl> + if ( flags & MUTT_FORMAT_NOFILTER ) <nl> + return 0 ; <nl> + <nl> if ( color == MT_COLOR_INDEX ) <nl> { /* buf might be uninitialized other cases */ <nl> len = mutt_strlen ( buf );
int mutt_index_menu ( void ) <nl> char buf [ 128 ]; <nl>  <nl> buf [ 0 ] = '\ 0 '; <nl> - if (! mutt_get_field (" Enter macro stroke : ", buf , sizeof ( buf ), <nl> + if (! mutt_get_field ( _ (" Enter macro stroke : "), buf , sizeof ( buf ), <nl> MUTT_CLEAR ) && buf [ 0 ]) <nl> { <nl> snprintf ( str , sizeof ( str ), "% s % s ", MarkMacroPrefix , buf );
int main ( int argc , char * argv []) <nl> if (! use_curses ) <nl> early_quit ( 0 , " No servers could be used ! Exiting ."); <nl> # ifdef HAVE_CURSES <nl> + wrefresh ( logwin ); <nl> halfdelay ( 10 ); <nl> if ( getch () != ERR ) <nl> early_quit ( 0 , " No servers could be used ! Exiting .");
mm_answer_skeyrespond ( int sock , Buffer * m ) <nl> debug3 ("% s : sending authenticated : % d ", __func__ , authok ); <nl> mm_request_send ( sock , MONITOR_ANS_SKEYRESPOND , m ); <nl>  <nl> - auth_method = " skey "; <nl> + auth_method = " keyboard - interactive "; <nl> + auth_submethod = " skey "; <nl>  <nl> return ( authok != 0 ); <nl> }
on_unregister_handler ( TCMUService1HandlerManager1 * interface , <nl> gpointer user_data ) <nl> { <nl> struct tcmur_handler * handler = find_handler_by_subtype ( subtype ); <nl> - struct dbus_info * info = handler -> opaque ; <nl> + struct dbus_info * info = handler ? handler -> opaque : NULL ; <nl>  <nl> if (! handler ) { <nl> g_dbus_method_invocation_return_value ( invocation ,
int main ( int argc , char ** argv ) <nl> darray_foreach ( tmp_r_handler , g_runner_handlers ) { <nl> struct tcmulib_handler tmp_handler ; <nl>  <nl> + memset (& tmp_handler , 0 , sizeof ( tmp_handler )); <nl> tmp_handler . name = (* tmp_r_handler )-> name ; <nl> tmp_handler . subtype = (* tmp_r_handler )-> subtype ; <nl> tmp_handler . cfg_desc = (* tmp_r_handler )-> cfg_desc ;
void qtcDefaultSettings ( Options * opts ) <nl> opts -> shadeMenubarOnlyWhenActive = false ; <nl> opts -> thin = THIN_BUTTONS ; <nl> opts -> tbarBtns = TBTN_STANDARD ; <nl> +# ifdef _WIN32 <nl> + opts -> scrollbarType = SCROLLBAR_WINDOWS ; <nl> +# elif defined __APPLE__ <nl> + opts -> scrollbarType = SCROLLBAR_NONE ; <nl> +# else <nl> opts -> scrollbarType = SCROLLBAR_KDE ; <nl> +# endif <nl> opts -> buttonEffect = EFFECT_SHADOW ; <nl> opts -> focus = FOCUS_GLOW ; <nl> opts -> lvButton = false ;
void h2o_reprocess_request ( h2o_req_t * req , h2o_iovec_t method , const h2o_url_sch <nl> req -> path_normalized = h2o_url_normalize_path (& req -> pool , req -> path . base , req -> path . len , & req -> query_at , & req -> norm_indexes ); <nl> req -> overrides = overrides ; <nl> req -> res_is_delegated |= is_delegated ; <nl> + req -> reprocess_if_too_early = 0 ; <nl> reset_response ( req ); <nl>  <nl> /* check the delegation ( or reprocess ) counter */
LibreportError save_dump_dir_from_problem_data ( problem_data_t * problem_data , cha <nl> VERB2 log (" Renaming from '% s ' to '% s '", dd -> dd_dirname , new_path ); <nl> if ( dd_rename ( dd , new_path ) != 0 ) <nl> { <nl> + free ( new_path ); <nl> + dd_close ( dd ); <nl> + <nl> free (* problem_id ); <nl> * problem_id = NULL ; <nl> return LR_ERROR ; <nl> } <nl> - free ( new_path ); <nl>  <nl> + free ( new_path ); <nl> dd_close ( dd ); <nl>  <nl> return LR_OK ;
static plist_t parse_bin_node ( struct bplist_data * bplist , const char ** object ) <nl> return parse_string_node ( object , size ); <nl>  <nl> case BPLIST_UNICODE : <nl> + if ( size * 2 < size ) { <nl> + PLIST_BIN_ERR ("% s : Integer overflow when calculating BPLIST_UNICODE data size .\ n ", __func__ ); <nl> + return NULL ; <nl> + } <nl> if (* object + size * 2 > bplist -> offset_table ) { <nl> PLIST_BIN_ERR ("% s : BPLIST_UNICODE data bytes point outside of valid range \ n ", __func__ ); <nl> return NULL ;
static void node_from_xml ( parse_ctx ctx , plist_t * plist ) <nl> return ; <nl> } <nl> if (*( ctx -> pos - 1 ) == '/') { <nl> - tag [ ctx -> pos - p - 1 ] = '\ 0 '; <nl> + int idx = ctx -> pos - p - 1 ; <nl> + if ( idx < taglen ) <nl> + tag [ idx ] = '\ 0 '; <nl> is_empty = 1 ; <nl> } <nl> ctx -> pos ++;
* development environment . <nl> */ <nl>  <nl> +/* MSVC headers do not define this macro , so do it here */ <nl> +# ifndef IN6_ARE_ADDR_EQUAL <nl> +# define IN6_ARE_ADDR_EQUAL ( a , b ) \ <nl> + ( memcmp (( const void *)( a ), ( const void *)( b ), sizeof ( struct in6_addr )) == 0 ) <nl> +# endif <nl> + <nl> void init_win32 ( void ); <nl> void uninit_win32 ( void ); <nl> 
loop : <nl> client = accept_client ( fd , & addr . in , false ); <nl> } <nl>  <nl> - slog_debug ( client , " P : got connection : % s ", conninfo ( client )); <nl> + if ( client ) <nl> + slog_debug ( client , " P : got connection : % s ", conninfo ( client )); <nl>  <nl> /* <nl> * there may be several clients waiting ,
static int write_element_to_buffer ( bson_buffer * buffer , int type_byte , PyObject * <nl>  <nl> item_value = PySequence_GetItem ( value , i ); <nl> if (! write_element_to_buffer ( buffer , list_type_byte , item_value , check_keys )) { <nl> + Py_DECREF ( item_value ); <nl> return 0 ; <nl> } <nl> + Py_DECREF ( item_value ); <nl> } <nl>  <nl> /* write null byte and fill in length */
static void handle_propfind ( struct connection * conn , const char * path , <nl> struct dir_entry * de = & arr [ i ]; <nl> mg_url_encode ( de -> file_name , strlen ( de -> file_name ), buf , sizeof ( buf )); <nl> print_props ( conn , buf , & de -> st ); <nl> + free ( de -> file_name ); <nl> } <nl> + free ( arr ); <nl> } <nl> ns_send ( conn -> ns_conn , footer , sizeof ( footer ) - 1 ); <nl> }
idevice_error_t idevice_connection_enable_ssl ( idevice_connection_t connection ) <nl> return_me = SSL_do_handshake ( ssl ); <nl> if ( return_me != 1 ) { <nl> debug_info (" ERROR in SSL_do_handshake : % s ", errorstring ( SSL_get_error ( ssl , return_me ))); <nl> + SSL_free ( ssl ); <nl> SSL_CTX_free ( ssl_ctx ); <nl> } else { <nl> ssl_data_t ssl_data_loc = ( ssl_data_t ) malloc ( sizeof ( struct ssl_data_private ));
PyArray_FromString ( char * data , npy_intp slen , PyArray_Descr * dtype , <nl>  <nl> if ( dtype == NULL ) { <nl> dtype = PyArray_DescrFromType ( NPY_DEFAULT_TYPE ); <nl> + if ( dtype == NULL ) { <nl> + return NULL ; <nl> + } <nl> } <nl> if ( PyDataType_FLAGCHK ( dtype , NPY_ITEM_IS_POINTER ) || <nl> PyDataType_REFCHK ( dtype )) {
int createEKHandle ( TSS2_SYS_CONTEXT * sapi_context ) <nl> LOG_INFO (" EK create succ .. Handle : 0x % 8 . 8x ", handle2048ek ); <nl>  <nl> if (! ctx . non_persistent_read ) { <nl> + <nl> + if (! ctx . persistent_handle ) { <nl> + LOG_ERR (" Persistent handle for EK was not provided "); <nl> + return 1 ; <nl> + } <nl> + <nl> /* <nl> * To make EK persistent , use own auth <nl> */
opfunc_in ( opcode_t opdata __unused , /**< operation data */ <nl> const idx_t left_var_idx = opdata . data . in . var_left ; <nl> const idx_t right_var_idx = opdata . data . in . var_right ; <nl>  <nl> + int_data -> pos ++; <nl> + <nl> ecma_completion_value_t ret_value ; <nl>  <nl> ECMA_TRY_CATCH ( left_value , get_variable_value ( int_data , left_var_idx , false ), ret_value );
void librados :: IoCtxImpl :: C_NotifyComplete :: notify ( uint8_t opcode , <nl> uint64_t ver , <nl> bufferlist & bl ) <nl> { <nl> + lock -> Lock (); <nl> * done = true ; <nl> cond -> Signal (); <nl> + lock -> Unlock (); <nl> } <nl>  <nl> /////////////////////////// WatchContext ///////////////////////////////
void PG :: activate ( ObjectStore :: Transaction & t , <nl>  <nl> need_up_thru = false ; <nl>  <nl> + // clear prior set ( and dependency info )... we are done peering ! <nl> + clear_prior (); <nl> + <nl> // write pg info , log <nl> write_info ( t ); <nl> write_log ( t );
void MDS :: replay_done () <nl> } <nl>  <nl> if ( continue_replay ) { <nl> - mdlog -> get_journaler ()-> set_writeable (); <nl> continue_replay = false ; <nl> standby_replay_restart (); <nl> return ; <nl> } <nl>  <nl> + mdlog -> get_journaler ()-> set_writeable (); <nl> + <nl> if ( g_conf . mds_wipe_sessions ) { <nl> dout ( 1 ) << " wiping out client sessions " << dendl ; <nl> sessionmap . wipe ();
static int read_all_chunked_input ( req_state * s , char ** pdata , int * plen ) <nl> int read_len = 0 , len = 0 ; <nl> do { <nl> int r = s -> cio -> read ( data + len , need_to_read , & read_len ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( data ); <nl> return r ; <nl> + } <nl>  <nl> len += read_len ; <nl> 
int EventCenter :: process_events ( int timeout_microseconds ) <nl> external_lock . Unlock (); <nl> while (! cur_process . empty ()) { <nl> EventCallbackRef e = cur_process . front (); <nl> - cur_process . pop_front (); <nl> if ( e ) <nl> e -> do_request ( 0 ); <nl> + cur_process . pop_front (); <nl> } <nl> } <nl> return numevents ;
public : <nl>  <nl> MLogAck () : Message ( MSG_LOGACK ) {} <nl> MLogAck ( ceph_fsid_t & f , version_t l ) : Message ( MSG_LOGACK ), fsid ( f ), last ( l ) {} <nl> + private : <nl> + ~ MLogAck () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " log_ack "; } <nl> void print ( ostream & out ) { <nl> out << " log ( last " << last << ")";
int RGWRados :: prepare_atomic_for_write ( RGWRadosCtx * rctx , rgw_obj & obj , librados <nl> do { <nl> r = prepare_atomic_for_write_impl ( rctx , obj , io_ctx , actual_obj , op , pstate ); <nl> } while ( r == - ECANCELED ); <nl> + <nl> + return r ; <nl> } <nl>  <nl> /**
OSD :: OSD ( int id , Messenger * m , MonMap * mm , const char * dev ) : <nl>  <nl> state = STATE_BOOTING ; <nl>  <nl> + memset (& my_stat , 0 , sizeof ( my_stat )); <nl> + <nl> stat_ops = 0 ; <nl> stat_qlen = 0 ; <nl> stat_rd_ops = stat_rd_ops_shed_in = stat_rd_ops_shed_out = 0 ;
int CrushWrapper :: device_class_clone ( <nl> // pick a new shadow bucket id that is not used by the current map <nl> // * or * any previous shadow buckets . <nl> bno = - 1 ; <nl> - while ( crush -> buckets [- 1 - bno ] || <nl> + while (((- 1 - bno ) < crush -> max_buckets && crush -> buckets [- 1 - bno ]) || <nl> used_ids . count ( bno )) { <nl> -- bno ; <nl> }
void FileStore :: _set_replay_guard ( int fd , const SequencerPosition & spos ) <nl> // first make sure the previous operation commits <nl> :: fsync ( fd ); <nl>  <nl> + // sync object_map too . even if this object has a header or keys , <nl> + // it have had them in the past and then removed them , so always <nl> + // sync . <nl> + object_map -> sync (); <nl> + <nl> // then record that we did it <nl> bufferlist v ( 40 ); <nl> :: encode ( spos , v );
static inline int ceph_fsid_compare ( const struct ceph_fsid * a , <nl> return memcmp ( a , b , sizeof (* a )); <nl> } <nl>  <nl> + static inline void ceph_fsid_set ( struct ceph_fsid * d , <nl> + const struct ceph_fsid * s ) <nl> +{ <nl> + memcpy ( d , s , sizeof (* d )); <nl> +} <nl> + <nl> /* <nl> * ino , object , etc . <nl> */
void Rank :: Pipe :: fault ( bool onconnect ) <nl>  <nl> if (! onconnect ) dout ( 2 ) << " fault " << errno << ": " << strerror ( errno ) << dendl ; <nl>  <nl> - if ( state == STATE_CLOSED ) { <nl> - dout ( 10 ) << " fault already closed " << dendl ; <nl> + if ( state == STATE_CLOSED || <nl> + state == STATE_CLOSING ) { <nl> + dout ( 10 ) << " fault already closed | closing " << dendl ; <nl> return ; <nl> } <nl> 
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> continue ; <nl> ci = ceph_inode ( inode ); <nl> spin_lock (& inode -> i_lock ); <nl> + if (! ci -> i_snap_realm ) <nl> + goto split_skip_inode ; <nl> ceph_put_snap_realm ( mdsc , ci -> i_snap_realm ); <nl> list_add (& ci -> i_snap_realm_item , <nl> & realm -> inodes_with_caps ); <nl> ci -> i_snap_realm = realm ; <nl> realm -> nref ++; <nl> + split_skip_inode : <nl> spin_unlock (& inode -> i_lock ); <nl> iput ( inode ); <nl> }
tar_directory_for_file ( GsfInfileTar * dir , const char * name , gboolean last ) <nl> gsf_infile_child_by_name ( GSF_INFILE ( dir ), <nl> dirname ); <nl> if ( subdir ) { <nl> + dir = GSF_IS_INFILE_TAR ( subdir ) <nl> + ? GSF_INFILE_TAR ( subdir ) <nl> + : dir ; <nl> /* Undo the ref . */ <nl> g_object_unref ( subdir ); <nl> - dir = GSF_INFILE_TAR ( subdir ); <nl> } else <nl> dir = tar_create_dir ( dir , dirname ); <nl> }
rsvg_filter_primitive_gaussian_blur_render ( RsvgFilterPrimitive * self , RsvgFilt <nl> boundarys . x0 , boundarys . y0 , <nl> boundarys . x1 - boundarys . x0 , boundarys . y1 - boundarys . y0 ); <nl> cairo_fill ( cr ); <nl> + cairo_destroy ( cr ); <nl> } <nl>  <nl> op . surface = output ;
retry : <nl> usbmuxd_device_info_t * devinfo = device_info_from_device_record ( dev ); <nl> if (! devinfo ) { <nl> DEBUG ( 1 , "% s : can ' t create device info object \ n ", __func__ ); <nl> - free ( payload ); <nl> return - 1 ; <nl> } <nl> collection_add (& tmpdevs , devinfo );
Perl_re_op_compile ( pTHX_ SV ** const patternp , int pat_count , <nl>  <nl> /* We have that number in RExC_npar */ <nl> RExC_total_parens = RExC_npar ; <nl> + <nl> + /* XXX For backporting , use long jumps if there is any possibility of <nl> + * overflow */ <nl> + if ( RExC_size > U16_MAX && ! RExC_use_BRANCHJ ) { <nl> + RExC_use_BRANCHJ = TRUE ; <nl> + flags |= RESTART_PARSE ; <nl> + } <nl> } <nl> else if (! MUST_RESTART ( flags )) { <nl> ReREFCNT_dec ( Rx );
void rfbScaledScreenUpdateRect ( rfbScreenInfoPtr screen , rfbScreenInfoPtr ptr , in <nl> default : <nl> /* fixme : endianness problem ? */ <nl> for ( z = 0 ; z < bytesPerPixel ; z ++) <nl> - pixel_value += ( srcptr2 [ z ] << ( 8 * z )); <nl> + pixel_value += (( unsigned long ) srcptr2 [ z ] << ( 8 * z )); <nl> break ; <nl> } <nl> /*
virDomainGetTime ( virDomainPtr dom , <nl> virResetLastError (); <nl>  <nl> virCheckDomainReturn ( dom , - 1 ); <nl> + virCheckReadOnlyGoto ( dom -> conn -> flags , error ); <nl>  <nl> if ( dom -> conn -> driver -> domainGetTime ) { <nl> int ret = dom -> conn -> driver -> domainGetTime ( dom , seconds ,
uint32_t skip ( Protocol_ & prot , TType arg_type ) { <nl> result += prot . readListEnd (); <nl> return result ; <nl> } <nl> - default : <nl> - return 0 ; <nl> + default : { <nl> + TProtocolException :: throwInvalidSkipType ( arg_type ); <nl> + } <nl> } <nl> } <nl> 
innobase_start_or_create_for_mysql ( void ) <nl> sum_of_new_sizes += srv_data_file_sizes [ i ]; <nl> } <nl>  <nl> - if ( sum_of_new_sizes < 640 ) { <nl> + if ( sum_of_new_sizes < 10485760 / UNIV_PAGE_SIZE ) { <nl> fprintf ( stderr , <nl> " InnoDB : Error : tablespace size must be " <nl> " at least 10 MB \ n ");
static bool check_user ( THD * thd , enum_server_command command , const char * user , <nl> send_error ( net , ER_OUT_OF_RESOURCES ); <nl> return 1 ; <nl> } <nl> - strcpy ( thd -> priv_host , LOCAL_HOST ); <nl> + strmake ( thd -> priv_host , LOCAL_HOST , sizeof ( thd -> priv_host )- 1 ); <nl> thd -> master_access = acl_getroot ( thd , thd -> host , thd -> ip , thd -> user , <nl> passwd , thd -> scramble , <nl> & thd -> priv_user , thd -> priv_host ,
static int DoCertificate ( CYASSL * ssl , byte * input , word32 * inOutIdx , <nl> ret = KEYUSE_ENCIPHER_E ; <nl> } <nl> if (( ssl -> specs . sig_algo == rsa_sa_algo || <nl> - ssl -> specs . sig_algo == ecc_dsa_sa_algo ) && <nl> + ( ssl -> specs . sig_algo == ecc_dsa_sa_algo && <nl> + ! ssl -> specs . static_ecdh )) && <nl> ( dCert . extKeyUsage & KEYUSE_DIGITAL_SIG ) == 0 ) { <nl> CYASSL_MSG (" KeyUse Digital Sig not set "); <nl> ret = KEYUSE_SIGNATURE_E ;
void InitX509Name ( WOLFSSL_X509_NAME * name , int dynamicFlag ) <nl> # ifdef OPENSSL_EXTRA <nl> XMEMSET (& name -> fullName , 0 , sizeof ( DecodedName )); <nl> XMEMSET (& name -> cnEntry , 0 , sizeof ( WOLFSSL_X509_NAME_ENTRY )); <nl> + XMEMSET (& name -> extra , 0 , sizeof ( name -> extra )); <nl> name -> cnEntry . value = &( name -> cnEntry . data ); /* point to internal data */ <nl> name -> cnEntry . nid = ASN_COMMON_NAME ; <nl> name -> x509 = NULL ;
EXPORT_SYMBOL ( _find_next_zero_bit_be ); <nl> EXPORT_SYMBOL ( _find_first_bit_be ); <nl> EXPORT_SYMBOL ( _find_next_bit_be ); <nl> # endif <nl> + <nl> + EXPORT_SYMBOL ( copy_page );
int activate_fd ( int irq , int fd , int type , void * dev_id ) <nl> . events = events , <nl> . current_events = 0 } ); <nl>  <nl> + err = - EBUSY ; <nl> spin_lock_irqsave (& irq_lock , flags ); <nl> for ( irq_fd = active_fds ; irq_fd != NULL ; irq_fd = irq_fd -> next ) { <nl> if (( irq_fd -> fd == fd ) && ( irq_fd -> type == type )) {
int drm_fb_helper_init ( struct drm_device * dev , <nl> struct drm_crtc * crtc ; <nl> int i ; <nl>  <nl> + if (! max_conn_count ) <nl> + return - EINVAL ; <nl> + <nl> fb_helper -> dev = dev ; <nl>  <nl> INIT_LIST_HEAD (& fb_helper -> kernel_fb_list );
static int pci_dio_auto_attach ( struct comedi_device * dev , <nl> else <nl> dev -> iobase = pci_resource_start ( pcidev , 2 ); <nl>  <nl> + pci_dio_reset ( dev ); <nl> + <nl> ret = comedi_alloc_subdevices ( dev , board -> nsubdevs ); <nl> if ( ret ) <nl> return ret ; <nl> static int pci_dio_auto_attach ( struct comedi_device * dev , <nl> comedi_8254_subdevice_init ( s , dev -> pacer ); <nl> } <nl>  <nl> - pci_dio_reset ( dev ); <nl> - <nl> return 0 ; <nl> } <nl> 
flush_signal_handlers ( struct task_struct * t , int force_default ) <nl> if ( force_default || ka -> sa . sa_handler != SIG_IGN ) <nl> ka -> sa . sa_handler = SIG_DFL ; <nl> ka -> sa . sa_flags = 0 ; <nl> +# ifdef SA_RESTORER <nl> + ka -> sa . sa_restorer = NULL ; <nl> +# endif <nl> sigemptyset (& ka -> sa . sa_mask ); <nl> ka ++; <nl> }
static void ati_remote_input_report ( struct urb * urb , struct pt_regs * regs ) <nl> input_regs ( dev , regs ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 1 ); <nl> + input_sync ( dev ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 0 ); <nl> input_sync ( dev );
static struct omap2_hsmmc_info mmc [] = { <nl> { <nl> . mmc = 1 , <nl> . caps = MMC_CAP_4_BIT_DATA | MMC_CAP_8_BIT_DATA , <nl> + . gpio_cd = - EINVAL , <nl> . gpio_wp = - EINVAL , <nl> }, <nl> {} /* Terminator */
int ext4_group_add_blocks ( handle_t * handle , struct super_block * sb , <nl>  <nl> ext4_debug (" Adding block ( s ) % llu -% llu \ n ", block , block + count - 1 ); <nl>  <nl> + if ( count == 0 ) <nl> + return 0 ; <nl> + <nl> ext4_get_group_no_and_offset ( sb , block , & block_group , & bit ); <nl> /* <nl> * Check to see if we are freeing blocks across a group
cio_start_key ( struct subchannel * sch , /* subchannel structure */ <nl> CIO_TRACE_EVENT ( 4 , sch -> dev . bus_id ); <nl>  <nl> orb = & to_io_private ( sch )-> orb ; <nl> + memset ( orb , 0 , sizeof ( union orb )); <nl> /* sch is always under 2G . */ <nl> orb -> cmd . intparm = ( u32 )( addr_t ) sch ; <nl> orb -> cmd . fmt = 1 ;
static void igmp_heard_query ( struct in_device * in_dev , struct sk_buff * skb , <nl> * to be intended in a v3 query . <nl> */ <nl> max_delay = IGMPV3_MRC ( ih3 -> code )*( HZ / IGMP_TIMER_SCALE ); <nl> + if (! max_delay ) <nl> + max_delay = 1 ; /* can ' t mod w / 0 */ <nl> } else { /* v3 */ <nl> if (! pskb_may_pull ( skb , sizeof ( struct igmpv3_query ))) <nl> return ;
static void raise_dtr ( struct isi_port * port ) <nl> } <nl>  <nl> /* card -> lock HAS to be held */ <nl> - static inline void drop_dtr ( struct isi_port * port ) <nl> + static void drop_dtr ( struct isi_port * port ) <nl> { <nl> struct isi_board * card = port -> card ; <nl> unsigned long base = card -> base ;
static int lis3l02dq_read_raw ( struct iio_dev * indio_dev , <nl> ret = lis3l02dq_read_reg_s16 ( indio_dev , reg , val ); <nl> } <nl> mutex_unlock (& indio_dev -> mlock ); <nl> + if ( ret < 0 ) <nl> + goto error_ret ; <nl> return IIO_VAL_INT ; <nl> case IIO_CHAN_INFO_SCALE : <nl> * val = 0 ;
static struct platform_pwm_backlight_data zoom_backlight_data = { <nl> . max_brightness = 127 , <nl> . dft_brightness = 127 , <nl> . pwm_period_ns = 7812500 , <nl> + . enable_gpio = - 1 , <nl> }; <nl>  <nl> static struct platform_device zoom_backlight_pwm = {
ext3_set_acl ( handle_t * handle , struct inode * inode , int type , <nl> return error ; <nl> else { <nl> inode -> i_mode = mode ; <nl> + inode -> i_ctime = CURRENT_TIME_SEC ; <nl> ext3_mark_inode_dirty ( handle , inode ); <nl> if ( error == 0 ) <nl> acl = NULL ;
static int cfg80211_netdev_notifier_call ( struct notifier_block * nb , <nl> kfree ( wdev -> wext . ie ); <nl> wdev -> wext . ie = NULL ; <nl> wdev -> wext . ie_len = 0 ; <nl> + wdev -> wext . connect . auth_type = NL80211_AUTHTYPE_AUTOMATIC ; <nl> # endif <nl> cfg80211_disconnect ( rdev , dev , <nl> WLAN_REASON_DEAUTH_LEAVING , true );
void eth_header_cache_update ( struct hh_cache * hh , struct net_device * dev , <nl> static int eth_mac_addr ( struct net_device * dev , void * p ) <nl> { <nl> struct sockaddr * addr = p ; <nl> + <nl> if ( netif_running ( dev )) <nl> return - EBUSY ; <nl> + if (! is_valid_ether_addr ( addr -> sa_data )) <nl> + return - EADDRNOTAVAIL ; <nl> memcpy ( dev -> dev_addr , addr -> sa_data , dev -> addr_len ); <nl> return 0 ; <nl> }
static int iommu_no_mapping ( struct pci_dev * pdev ) <nl> if ( pdev -> dma_mask > DMA_BIT_MASK ( 32 )) { <nl> int ret ; <nl> ret = domain_add_dev_info ( si_domain , pdev ); <nl> + if ( ret ) <nl> + return 0 ; <nl> + ret = domain_context_mapping ( si_domain , pdev , CONTEXT_TT_MULTI_LEVEL ); <nl> if (! ret ) { <nl> printk ( KERN_INFO " 64bit % s uses identity mapping \ n ", <nl> pci_name ( pdev ));
static int imx6ul_tsc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> adc_irq = platform_get_irq ( pdev , 1 ); <nl> - if ( adc_irq <= 0 ) { <nl> + if ( adc_irq < 0 ) { <nl> dev_err (& pdev -> dev , " no adc irq resource ?\ n "); <nl> return adc_irq ; <nl> }
intel_fill_fb_ggtt_view ( struct i915_ggtt_view * view , struct drm_framebuffer * fb , <nl> tile_size = intel_tile_size ( dev_priv ); <nl>  <nl> cpp = drm_format_plane_cpp ( fb -> pixel_format , 0 ); <nl> - tile_width = intel_tile_width ( dev_priv , cpp , fb -> modifier [ 0 ]); <nl> + tile_width = intel_tile_width ( dev_priv , fb -> modifier [ 0 ], cpp ); <nl> tile_height = tile_size / tile_width ; <nl>  <nl> info -> width_pages = DIV_ROUND_UP ( fb -> pitches [ 0 ], tile_width );
static int emac_dev_open ( struct net_device * ndev ) <nl> struct emac_priv * priv = netdev_priv ( ndev ); <nl>  <nl> netif_carrier_off ( ndev ); <nl> - for ( cnt = 0 ; cnt <= ETH_ALEN ; cnt ++) <nl> + for ( cnt = 0 ; cnt < ETH_ALEN ; cnt ++) <nl> ndev -> dev_addr [ cnt ] = priv -> mac_addr [ cnt ]; <nl>  <nl> /* Configuration items */
struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
*/ <nl> typedef union <nl> { <nl> - __u32 a4 ; <nl> - __u32 a6 [ 4 ]; <nl> + __be32 a4 ; <nl> + __be32 a6 [ 4 ]; <nl> } xfrm_address_t ; <nl>  <nl> /* Ident of a specific xfrm_state . It is used on input to lookup
module_i2c_driver ( si2157_driver ); <nl> MODULE_DESCRIPTION (" Silicon Labs Si2157 / Si2158 silicon tuner driver "); <nl> MODULE_AUTHOR (" Antti Palosaari < crope @ iki . fi >"); <nl> MODULE_LICENSE (" GPL "); <nl> + MODULE_FIRMWARE ( SI2158_A20_FIRMWARE );
found : <nl>  <nl> static int openpromfs_readdir ( struct file * filp , void * dirent , filldir_t filldir ) <nl> { <nl> - struct inode * inode = filp -> f_dentry -> d_inode ; <nl> + struct inode * inode = filp -> f_path . dentry -> d_inode ; <nl> struct op_inode_info * oi = OP_I ( inode ); <nl> struct device_node * dp = oi -> u . node ; <nl> struct device_node * child ;
void scsi_eh_prep_cmnd ( struct scsi_cmnd * scmd , struct scsi_eh_save * ses , <nl> ses -> prot_op = scmd -> prot_op ; <nl>  <nl> scmd -> prot_op = SCSI_PROT_NORMAL ; <nl> + scmd -> eh_eflags = 0 ; <nl> scmd -> cmnd = ses -> eh_cmnd ; <nl> memset ( scmd -> cmnd , 0 , BLK_MAX_CDB ); <nl> memset (& scmd -> sdb , 0 , sizeof ( scmd -> sdb ));
static void disable_lapic_nmi_watchdog ( void ) <nl> wrmsr ( MSR_P6_EVNTSEL0 , 0 , 0 ); <nl> break ; <nl> case 15 : <nl> - if ( boot_cpu_data . x86_model > 0x3 ) <nl> + if ( boot_cpu_data . x86_model > 0x4 ) <nl> break ; <nl>  <nl> wrmsr ( MSR_P4_IQ_CCCR0 , 0 , 0 ); <nl> void setup_apic_nmi_watchdog ( void ) <nl> setup_p6_watchdog (); <nl> break ; <nl> case 15 : <nl> - if ( boot_cpu_data . x86_model > 0x3 ) <nl> + if ( boot_cpu_data . x86_model > 0x4 ) <nl> return ; <nl>  <nl> if (! setup_p4_watchdog ())
static void pic_update_irq ( struct kvm_pic * s ) <nl> pic_set_irq1 (& s -> pics [ 0 ], 2 , 0 ); <nl> } <nl> irq = pic_get_irq (& s -> pics [ 0 ]); <nl> - if ( irq >= 0 ) <nl> - pic_irq_request ( s -> kvm , 1 ); <nl> - else <nl> - pic_irq_request ( s -> kvm , 0 ); <nl> + pic_irq_request ( s -> kvm , irq >= 0 ); <nl> } <nl>  <nl> void kvm_pic_update_irq ( struct kvm_pic * s )
struct unw_frame_info { <nl> struct unw_ireg { <nl> unsigned long * loc ; <nl> struct unw_ireg_nat { <nl> - long type : 3 ; /* enum unw_nat_type */ <nl> + unsigned long type : 3 ; /* enum unw_nat_type */ <nl> signed long off : 61 ; /* NaT word is at loc + nat . off */ <nl> } nat ; <nl> } r4 , r5 , r6 , r7 ;
int beiscsi_cmd_mccq_create ( struct beiscsi_hba * phba , <nl> spin_lock (& phba -> ctrl . mbox_lock ); <nl> ctrl = & phba -> ctrl ; <nl> wrb = wrb_from_mbox (& ctrl -> mbox_mem ); <nl> + memset ( wrb , 0 , sizeof (* wrb )); <nl> req = embedded_payload ( wrb ); <nl> ctxt = & req -> context ; <nl> 
struct event_format * trace_find_next_event ( struct pevent * pevent , <nl> { <nl> static int idx ; <nl>  <nl> - if (! pevent -> events ) <nl> + if (! pevent || ! pevent -> events ) <nl> return NULL ; <nl>  <nl> if (! event ) {
static int iio_channel_read ( struct iio_channel * chan , int * val , int * val2 , <nl> if ( val2 == NULL ) <nl> val2 = & unused ; <nl>  <nl> + if (! iio_channel_has_info ( chan -> channel , info )) <nl> + return - EINVAL ; <nl> + <nl> if ( chan -> indio_dev -> info -> read_raw_multi ) { <nl> ret = chan -> indio_dev -> info -> read_raw_multi ( chan -> indio_dev , <nl> chan -> channel , INDIO_MAX_RAW_ELEMENTS ,
static void ar9003_hw_spur_mitigate_ofdm ( struct ath_hw * ah , <nl>  <nl> ar9003_hw_spur_ofdm_clear ( ah ); <nl>  <nl> - for ( i = 0 ; spurChansPtr [ i ] && i < 5 ; i ++) { <nl> + for ( i = 0 ; i < AR_EEPROM_MODAL_SPURS && spurChansPtr [ i ]; i ++) { <nl> freq_offset = FBIN2FREQ ( spurChansPtr [ i ], mode ) - synth_freq ; <nl> if ( abs ( freq_offset ) < range ) { <nl> ar9003_hw_spur_ofdm_work ( ah , chan , freq_offset );
static int lowpan_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> if (! netif_running ( dev )) <nl> goto drop_skb ; <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto drop_skb ; <nl> + <nl> if ( dev -> type != ARPHRD_IEEE802154 ) <nl> goto drop_skb ; <nl> 
static int reiserfs_fill_super ( struct super_block * s , void * data , int silent ) <nl> if ( SB_AP_BITMAP ( s )) <nl> brelse ( SB_AP_BITMAP ( s )[ j ]. bh ); <nl> } <nl> - if ( SB_AP_BITMAP ( s )) <nl> - vfree ( SB_AP_BITMAP ( s )); <nl> + vfree ( SB_AP_BITMAP ( s )); <nl> } <nl> if ( SB_BUFFER_WITH_SB ( s )) <nl> brelse ( SB_BUFFER_WITH_SB ( s ));
static int cgroupstats_user_cmd ( struct sk_buff * skb , struct genl_info * info ) <nl> na = nla_reserve ( rep_skb , CGROUPSTATS_TYPE_CGROUP_STATS , <nl> sizeof ( struct cgroupstats )); <nl> if ( na == NULL ) { <nl> + nlmsg_free ( rep_skb ); <nl> rc = - EMSGSIZE ; <nl> goto err ; <nl> }
static unsigned long super_cache_scan ( struct shrinker * shrink , <nl> inodes = list_lru_count_node (& sb -> s_inode_lru , sc -> nid ); <nl> dentries = list_lru_count_node (& sb -> s_dentry_lru , sc -> nid ); <nl> total_objects = dentries + inodes + fs_objects + 1 ; <nl> + if (! total_objects ) <nl> + total_objects = 1 ; <nl>  <nl> /* proportion the scan between the caches */ <nl> dentries = mult_frac ( sc -> nr_to_scan , dentries , total_objects );
static inline pte_t pfn_pte ( unsigned long pfn , pgprot_t prot ) <nl> sz_bits = 0UL ; <nl> if ( _PAGE_SZBITS_4U != 0UL || _PAGE_SZBITS_4V != 0UL ) { <nl> __asm__ __volatile__ ( <nl> - "\ n661 : sethi % uhi (% 1 ), % 0 \ n " <nl> + "\ n661 : sethi %% uhi (% 1 ), % 0 \ n " <nl> " sllx % 0 , 32 , % 0 \ n " <nl> " . section . sun4v_2insn_patch , \" ax \"\ n " <nl> " . word 661b \ n "
static int perf_event__repipe_attr ( struct perf_tool * tool , <nl> union perf_event * event , <nl> struct perf_evlist ** pevlist ) <nl> { <nl> + struct perf_inject * inject = container_of ( tool , struct perf_inject , <nl> + tool ); <nl> int ret ; <nl>  <nl> ret = perf_event__process_attr ( tool , event , pevlist ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if (! inject -> pipe_output ) <nl> + return 0 ; <nl> + <nl> return perf_event__repipe_synth ( tool , event ); <nl> } <nl> 
int bond_arp_rcv ( const struct sk_buff * skb , struct bonding * bond , <nl> __be32 sip , tip ; <nl> int alen ; <nl>  <nl> + slave -> last_arp_rx = jiffies ; <nl> + <nl> if ( skb -> protocol != __cpu_to_be16 ( ETH_P_ARP )) <nl> return RX_HANDLER_ANOTHER ; <nl> 
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl>  <nl> void oprofile_arch_exit ( void ) <nl> { <nl> - model -> exit (); <nl> + if ( model ) <nl> + model -> exit (); <nl> }
static int __devinit sdhci_probe_slot ( struct pci_dev * pdev , int slot ) <nl>  <nl> version = readw ( host -> ioaddr + SDHCI_HOST_VERSION ); <nl> version = ( version & SDHCI_SPEC_VER_MASK ) >> SDHCI_SPEC_VER_SHIFT ; <nl> - if ( version != 0 ) { <nl> + if ( version > 1 ) { <nl> printk ( KERN_ERR "% s : Unknown controller version (% d ). " <nl> " You may experience problems .\ n ", host -> slot_descr , <nl> version );
static struct ath_buf * ath_clone_txbuf ( struct ath_softc * sc , struct ath_buf * bf ) <nl> tbf -> aphy = bf -> aphy ; <nl> tbf -> bf_mpdu = bf -> bf_mpdu ; <nl> tbf -> bf_buf_addr = bf -> bf_buf_addr ; <nl> - *( tbf -> bf_desc ) = *( bf -> bf_desc ); <nl> + memcpy ( tbf -> bf_desc , bf -> bf_desc , sc -> sc_ah -> caps . tx_desc_len ); <nl> tbf -> bf_state = bf -> bf_state ; <nl> tbf -> bf_dmacontext = bf -> bf_dmacontext ; <nl> 
static int r820t_signal ( struct dvb_frontend * fe , u16 * strength ) <nl>  <nl> /* A higher gain at LNA means a lower signal strength */ <nl> * strength = ( 45 - rc ) << 4 | 0xff ; <nl> + if (* strength == 0xff ) <nl> + * strength = 0 ; <nl> } else { <nl> * strength = 0 ; <nl> }
int setup_one_line ( struct line * lines , int n , char * init , <nl> * error_out = " Failed to allocate memory "; <nl> return - ENOMEM ; <nl> } <nl> - if ( line -> valid ) <nl> + if ( line -> valid ) { <nl> tty_unregister_device ( driver , n ); <nl> + kfree ( line -> init_str ); <nl> + } <nl> line -> init_str = new ; <nl> line -> valid = 1 ; <nl> err = parse_chan_pair ( new , line , n , opts , error_out );
int saa7134_input_init1 ( struct saa7134_dev * dev ) <nl> return - ENOMEM ; <nl> } <nl>  <nl> + ir -> dev = input_dev ; <nl> + <nl> /* init hardware - specific stuff */ <nl> ir -> mask_keycode = mask_keycode ; <nl> ir -> mask_keydown = mask_keydown ;
int hid_input_report ( struct hid_device * hid , int type , u8 * data , int size , int i <nl>  <nl> if ( hdrv && hdrv -> raw_event && hid_match_report ( hid , report )) { <nl> ret = hdrv -> raw_event ( hid , report , data , size ); <nl> - if ( ret != 0 ) { <nl> + if ( ret < 0 ) { <nl> ret = ret < 0 ? ret : 0 ; <nl> goto unlock ; <nl> }
nfsd4_getdeviceinfo ( struct svc_rqst * rqstp , <nl> nfserr = ops -> proc_getdeviceinfo ( exp -> ex_path . mnt -> mnt_sb , gdp ); <nl>  <nl> gdp -> gd_notify_types &= ops -> notify_types ; <nl> - exp_put ( exp ); <nl> out : <nl> + exp_put ( exp ); <nl> return nfserr ; <nl> } <nl> 
static int t4_sge_init_soft ( struct adapter * adap ) <nl> # undef READ_FL_BUF <nl>  <nl> if ( fl_small_pg != PAGE_SIZE || <nl> - ( fl_large_pg != 0 && ( fl_large_pg <= fl_small_pg || <nl> + ( fl_large_pg != 0 && ( fl_large_pg < fl_small_pg || <nl> ( fl_large_pg & ( fl_large_pg - 1 )) != 0 ))) { <nl> dev_err ( adap -> pdev_dev , " bad SGE FL page buffer sizes [% d , % d ]\ n ", <nl> fl_small_pg , fl_large_pg );
static ssize_t dgrp_class_pollrate_store ( struct device * c , <nl> struct device_attribute * attr , <nl> const char * buf , size_t count ) <nl> { <nl> - sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ); <nl> + if ( sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ) != 1 ) <nl> + return - EINVAL ; <nl> + <nl> return count ; <nl> } <nl> static DEVICE_ATTR ( pollrate , 0600 , dgrp_class_pollrate_show ,
nouveau_abi16_ioctl_channel_alloc ( ABI16_IOCTL_ARGS ) <nl>  <nl> if ( unlikely (! abi16 )) <nl> return - ENOMEM ; <nl> + <nl> + if (! drm -> channel ) <nl> + return nouveau_abi16_put ( abi16 , - ENODEV ); <nl> + <nl> client = nv_client ( abi16 -> client ); <nl>  <nl> if ( init -> fb_ctxdma_handle == ~ 0 || init -> tt_ctxdma_handle == ~ 0 )
EXPORT_SYMBOL_GPL ( usbnet_resume ); <nl>  <nl> static int __init usbnet_init ( void ) <nl> { <nl> - /* compiler should optimize this out */ <nl> - BUILD_BUG_ON ( sizeof ((( struct sk_buff *) 0 )-> cb ) <nl> - < sizeof ( struct skb_data )); <nl> + /* Compiler should optimize this out . */ <nl> + BUILD_BUG_ON ( <nl> + FIELD_SIZEOF ( struct sk_buff , cb ) < sizeof ( struct skb_data )); <nl>  <nl> random_ether_addr ( node_id ); <nl> return 0 ;
static int __init mbcs_init ( void ) <nl> { <nl> int rv ; <nl>  <nl> + if (! ia64_platform_is (" sn2 ")) <nl> + return - ENODEV ; <nl> + <nl> // Put driver into chrdevs []. Get major number . <nl> rv = register_chrdev ( mbcs_major , DEVICE_NAME , & mbcs_ops ); <nl> if ( rv < 0 ) {
void xenbus_dev_changed ( const char * node , struct xen_bus_type * bus ) <nl>  <nl> kfree ( root ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( xenbus_dev_changed ); <nl>  <nl> static void frontend_changed ( struct xenbus_watch * watch , <nl> const char ** vec , unsigned int len )
int main ( int ac , char ** av ) <nl> single_menu_mode = 1 ; <nl> } <nl>  <nl> + initscr (); <nl> + <nl> getyx ( stdscr , saved_y , saved_x ); <nl> if ( init_dialog ( NULL )) { <nl> fprintf ( stderr , N_ (" Your display is too small to run Menuconfig !\ n "));
static unsigned int ip_vs_post_routing ( unsigned int hooknum , <nl> { <nl> if (!((* pskb )-> ipvs_property )) <nl> return NF_ACCEPT ; <nl> - <nl> /* The packet was sent from IPVS , exit this chain */ <nl> - (* okfn )(* pskb ); <nl> - <nl> - return NF_STOLEN ; <nl> + return NF_STOP ; <nl> } <nl>  <nl> u16 ip_vs_checksum_complete ( struct sk_buff * skb , int offset )
static int hdmi_remove ( struct platform_device * pdev ) <nl>  <nl> pm_runtime_disable ( dev ); <nl>  <nl> - free_irq ( hdata -> irq , hdata ); <nl> + free_irq ( hdata -> irq , ctx ); <nl>  <nl>  <nl> /* hdmiphy i2c driver */
static int kdb_ll ( int argc , const char ** argv ) <nl> while ( va ) { <nl> char buf [ 80 ]; <nl>  <nl> + if ( KDB_FLAG ( CMD_INTERRUPT )) <nl> + return 0 ; <nl> + <nl> sprintf ( buf , "% s " kdb_machreg_fmt "\ n ", command , va ); <nl> diag = kdb_parse ( buf ); <nl> if ( diag )
static long hcall_vphn ( unsigned long cpu , __be32 * associativity ) <nl> long retbuf [ PLPAR_HCALL9_BUFSIZE ] = { 0 }; <nl> u64 flags = 1 ; <nl> int hwcpu = get_hard_smp_processor_id ( cpu ); <nl> + int i ; <nl>  <nl> rc = plpar_hcall9 ( H_HOME_NODE_ASSOCIATIVITY , retbuf , flags , hwcpu ); <nl> + for ( i = 0 ; i < 6 ; i ++) <nl> + retbuf [ i ] = cpu_to_be64 ( retbuf [ i ]); <nl> vphn_unpack_associativity ( retbuf , associativity ); <nl>  <nl> return rc ;
postchange : <nl> current_multiplier ); <nl> } <nl> # endif <nl> + if ( err ) <nl> + freqs . new = freqs . old ; <nl> + <nl> cpufreq_notify_transition ( policy , & freqs , CPUFREQ_POSTCHANGE ); <nl> return err ; <nl> }
static int io_subchannel_sch_event ( struct subchannel * sch , int process ) <nl> goto out ; <nl> break ; <nl> case IO_SCH_UNREG_ATTACH : <nl> + if ( cdev -> private -> flags . resuming ) { <nl> + /* Device will be handled later . */ <nl> + rc = 0 ; <nl> + goto out ; <nl> + } <nl> /* Unregister ccw device . */ <nl> - if (! cdev -> private -> flags . resuming ) <nl> - ccw_device_unregister ( cdev ); <nl> + ccw_device_unregister ( cdev ); <nl> break ; <nl> default : <nl> break ;
static int __devinit cciss_init_one ( struct pci_dev * pdev , <nl> h -> scatter_list = kmalloc ( h -> max_commands * <nl> sizeof ( struct scatterlist *), <nl> GFP_KERNEL ); <nl> + if (! h -> scatter_list ) <nl> + goto clean4 ; <nl> + <nl> for ( k = 0 ; k < h -> nr_cmds ; k ++) { <nl> h -> scatter_list [ k ] = kmalloc ( sizeof ( struct scatterlist ) * <nl> h -> maxsgentries ,
static const char * alc_get_line_out_pfx ( struct alc_spec * spec , int ch , <nl> case AUTO_PIN_SPEAKER_OUT : <nl> if ( cfg -> line_outs == 1 ) <nl> return " Speaker "; <nl> + if ( cfg -> line_outs == 2 ) <nl> + return ch ? " Bass Speaker " : " Speaker "; <nl> break ; <nl> case AUTO_PIN_HP_OUT : <nl> /* for multi - io case , only the primary out */
static void macb_handle_link_change ( struct net_device * dev ) <nl>  <nl> if ( phydev -> duplex ) <nl> reg |= MACB_BIT ( FD ); <nl> - if ( phydev -> speed ) <nl> + if ( phydev -> speed == SPEED_100 ) <nl> reg |= MACB_BIT ( SPD ); <nl>  <nl> macb_writel ( bp , NCFGR , reg );
static int da9052_rtc_probe ( struct platform_device * pdev ) <nl> return ret ; <nl> } <nl>  <nl> + device_init_wakeup (& pdev -> dev , true ); <nl> + <nl> rtc -> rtc = devm_rtc_device_register (& pdev -> dev , pdev -> name , <nl> & da9052_rtc_ops , THIS_MODULE ); <nl> return PTR_ERR_OR_ZERO ( rtc -> rtc );
static int ad198x_build_pcms ( struct hda_codec * codec ) <nl> if ( spec -> multiout . dig_out_nid ) { <nl> info ++; <nl> codec -> num_pcms ++; <nl> + codec -> spdif_status_reset = 1 ; <nl> info -> name = " AD198x Digital "; <nl> info -> pcm_type = HDA_PCM_TYPE_SPDIF ; <nl> info -> stream [ SNDRV_PCM_STREAM_PLAYBACK ] = ad198x_pcm_digital_playback ;
int __init option_setup ( char * str ) <nl>  <nl> TRACE2 ((" option_setup () str % s \ n ", str ? str :" NULL ")); <nl>  <nl> - while ( cur && isdigit (* cur ) && i <= MAXHA ) { <nl> + while ( cur && isdigit (* cur ) && i < MAXHA ) { <nl> ints [ i ++] = simple_strtoul ( cur , NULL , 0 ); <nl> if (( cur = strchr ( cur , ',')) != NULL ) cur ++; <nl> }
static void stop_nop_trace ( struct trace_array * tr ) <nl>  <nl> static void nop_trace_init ( struct trace_array * tr ) <nl> { <nl> + int cpu ; <nl> ctx_trace = tr ; <nl>  <nl> + for_each_online_cpu ( cpu ) <nl> + tracing_reset ( tr -> data [ cpu ]); <nl> + <nl> if ( tr -> ctrl ) <nl> start_nop_trace ( tr ); <nl> }
static int zd1201_resume ( struct usb_interface * interface ) <nl> { <nl> struct zd1201 * zd = usb_get_intfdata ( interface ); <nl>  <nl> + if (! zd || ! zd -> dev ) <nl> + return - ENODEV ; <nl> + <nl> netif_device_attach ( zd -> dev ); <nl>  <nl> if ( zd -> was_enabled )
int __devinit wl18xx_probe ( struct platform_device * pdev ) <nl> { <nl> struct wl1271 * wl ; <nl> struct ieee80211_hw * hw ; <nl> + struct wl18xx_priv * priv ; <nl>  <nl> - hw = wlcore_alloc_hw ( 0 ); <nl> + hw = wlcore_alloc_hw ( sizeof (* priv )); <nl> if ( IS_ERR ( hw )) { <nl> wl1271_error (" can ' t allocate hw "); <nl> return PTR_ERR ( hw );
static int nortel_pci_cor_reset ( struct orinoco_private * priv ) <nl> return 0 ; <nl> } <nl>  <nl> - int nortel_pci_hw_init ( struct nortel_pci_card * card ) <nl> + static int nortel_pci_hw_init ( struct nortel_pci_card * card ) <nl> { <nl> int i ; <nl> u32 reg ;
static void oz_hcd_complete_set_interface ( struct oz_port * port , struct urb * urb , <nl> struct usb_hcd * hcd = port -> ozhcd -> hcd ; <nl> int rc = 0 ; <nl>  <nl> - if ( rcode == 0 ) { <nl> + if (( rcode == 0 ) && ( port -> config_num > 0 )) { <nl> struct usb_host_config * config ; <nl> struct usb_host_interface * intf ; <nl> oz_dbg ( ON , " Set interface % d alt % d \ n ", if_num , alt );
restart : <nl> mutex_unlock (& mutex ); <nl> } <nl>  <nl> +/* <nl> + * sync everything . Start out by waking pdflush , because that writes back <nl> + * all queues in parallel . <nl> + */ <nl> SYSCALL_DEFINE0 ( sync ) <nl> { <nl> + wakeup_pdflush ( 0 ); <nl> sync_filesystems ( 0 ); <nl> sync_filesystems ( 1 ); <nl> if ( unlikely ( laptop_mode ))
good_area : <nl> return handle_mm_fault ( mm , vma , addr & PAGE_MASK , flags ); <nl>  <nl> check_stack : <nl> - if ( vma -> vm_flags & VM_GROWSDOWN && ! expand_stack ( vma , addr )) <nl> + /* Don ' t allow expansion below FIRST_USER_ADDRESS */ <nl> + if ( vma -> vm_flags & VM_GROWSDOWN && <nl> + addr >= FIRST_USER_ADDRESS && ! expand_stack ( vma , addr )) <nl> goto good_area ; <nl> out : <nl> return fault ;
static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPW , <nl> . default_value = 720 , <nl> DEFREF ( cropw ), <nl> + DEFINT ( 0 , 864 ), <nl> . get_max_value = ctrl_cropw_max_get , <nl> . get_def_value = ctrl_get_cropcapdw , <nl> }, { <nl> static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPH , <nl> . default_value = 480 , <nl> DEFREF ( croph ), <nl> + DEFINT ( 0 , 576 ), <nl> . get_max_value = ctrl_croph_max_get , <nl> . get_def_value = ctrl_get_cropcapdh , <nl> }, {
static int mga_vram_init ( struct mga_device * mdev ) <nl> { <nl> void __iomem * mem ; <nl> struct apertures_struct * aper = alloc_apertures ( 1 ); <nl> + if (! aper ) <nl> + return - ENOMEM ; <nl>  <nl> /* BAR 0 is VRAM */ <nl> mdev -> mc . vram_base = pci_resource_start ( mdev -> dev -> pdev , 0 );
static int ioapic_service ( struct kvm_ioapic * ioapic , int irq , bool line_status ) <nl> BUG_ON ( ioapic -> rtc_status . pending_eoi != 0 ); <nl> ret = kvm_irq_delivery_to_apic ( ioapic -> kvm , NULL , & irqe , <nl> ioapic -> rtc_status . dest_map ); <nl> - ioapic -> rtc_status . pending_eoi = ret ; <nl> + ioapic -> rtc_status . pending_eoi = ( ret < 0 ? 0 : ret ); <nl> } else <nl> ret = kvm_irq_delivery_to_apic ( ioapic -> kvm , NULL , & irqe , NULL ); <nl> 
static int dwc3_gadget_init_hw_endpoints ( struct dwc3 * dwc , <nl>  <nl> dep -> endpoint . name = dep -> name ; <nl>  <nl> + dev_vdbg ( dwc -> dev , " initializing % s \ n ", dep -> name ); <nl> + <nl> if ( epnum == 0 || epnum == 1 ) { <nl> dep -> endpoint . maxpacket = 512 ; <nl> dep -> endpoint . maxburst = 1 ;
static int aic3x_read ( struct snd_soc_codec * codec , unsigned int reg , <nl> if ( reg >= AIC3X_CACHEREGNUM ) <nl> return - 1 ; <nl>  <nl> - * value = codec -> hw_read ( codec , reg ); <nl> + codec -> cache_bypass = 1 ; <nl> + * value = snd_soc_read ( codec , reg ); <nl> + codec -> cache_bypass = 0 ; <nl> + <nl> cache [ reg ] = * value ; <nl>  <nl> return 0 ;
static void __init mark_kpte_bitmap ( unsigned long start , unsigned long end ) <nl> while ( start < end ) { <nl> long remains ; <nl>  <nl> + remains = end - start ; <nl> + if ( remains < size_256MB ) <nl> + break ; <nl> + <nl> if ( start & mask_256MB ) { <nl> start = ( start + size_256MB ) & ~ mask_256MB ; <nl> continue ; <nl> } <nl>  <nl> - remains = end - start ; <nl> while ( remains >= size_256MB ) { <nl> unsigned long index = start >> shift_256MB ; <nl> 
static int btrfs_xattr_acl_set ( struct dentry * dentry , const char * name , <nl> int ret ; <nl> struct posix_acl * acl = NULL ; <nl>  <nl> + if (! is_owner_or_cap ( dentry -> d_inode )) <nl> + return - EPERM ; <nl> + <nl> if ( value ) { <nl> acl = posix_acl_from_xattr ( value , size ); <nl> if ( acl == NULL ) {
static void handle_critical_trips ( struct thermal_zone_device * tz , <nl> tz -> ops -> get_trip_temp ( tz , trip , & trip_temp ); <nl>  <nl> /* If we have not crossed the trip_temp , we do not care . */ <nl> - if ( tz -> temperature < trip_temp ) <nl> + if ( trip_temp <= 0 || tz -> temperature < trip_temp ) <nl> return ; <nl>  <nl> trace_thermal_zone_trip ( tz , trip , trip_type );
bool rtl92cu_rx_query_desc ( struct ieee80211_hw * hw , <nl> ( bool ) GET_RX_DESC_PAGGR ( pdesc )); <nl> rx_status -> mactime = GET_RX_DESC_TSFL ( pdesc ); <nl> if ( phystatus ) { <nl> - p_drvinfo = ( struct rx_fwinfo_92c *)( pdesc + RTL_RX_DESC_SIZE ); <nl> + p_drvinfo = ( struct rx_fwinfo_92c *)( skb -> data + <nl> + stats -> rx_bufshift ); <nl> rtl92c_translate_rx_signal_stuff ( hw , skb , stats , pdesc , <nl> p_drvinfo ); <nl> }
static struct thread * __machine__findnew_thread ( struct machine * machine , <nl> * within thread__init_map_groups to find the thread <nl> * leader and that would screwed the rb tree . <nl> */ <nl> - if ( thread__init_map_groups ( th , machine )) <nl> + if ( thread__init_map_groups ( th , machine )) { <nl> + thread__delete ( th ); <nl> return NULL ; <nl> + } <nl> } <nl>  <nl> return th ;
parse_init_table ( struct nvbios * bios , uint16_t offset , struct init_exec * iexec ) <nl> int count = 0 , i , ret ; <nl> uint8_t id ; <nl>  <nl> + /* catch NULL script pointers */ <nl> + if ( offset == 0 ) <nl> + return 0 ; <nl> + <nl> /* <nl> * Loop until INIT_DONE causes us to break out of the loop <nl> * ( or until offset > bios length just in case ... )
int l2tp_xmit_skb ( struct l2tp_session * session , struct sk_buff * skb , int hdr_len <nl> headroom = NET_SKB_PAD + sizeof ( struct iphdr ) + <nl> uhlen + hdr_len ; <nl> old_headroom = skb_headroom ( skb ); <nl> - if ( skb_cow_head ( skb , headroom )) <nl> + if ( skb_cow_head ( skb , headroom )) { <nl> + dev_kfree_skb ( skb ); <nl> goto abort ; <nl> + } <nl>  <nl> new_headroom = skb_headroom ( skb ); <nl> skb_orphan ( skb );
static struct inotify_watch * create_watch ( struct inotify_device * dev , <nl> return ERR_PTR ( ret ); <nl> } <nl>  <nl> - dev -> last_wd = ret ; <nl> + dev -> last_wd = watch -> wd ; <nl> watch -> mask = mask ; <nl> atomic_set (& watch -> count , 0 ); <nl> INIT_LIST_HEAD (& watch -> d_list );
static int btrfs_get_sb ( struct file_system_type * fs_type , int flags , <nl> mutex_unlock (& root -> d_inode -> i_mutex ); <nl>  <nl> if ( IS_ERR ( new_root )) { <nl> + dput ( root ); <nl> deactivate_locked_super ( s ); <nl> error = PTR_ERR ( new_root ); <nl> - dput ( root ); <nl> goto error_free_subvol_name ; <nl> } <nl> if (! new_root -> d_inode ) {
static int __devinit fealnx_init_one ( struct pci_dev * pdev , <nl> if ( np -> flags == HAS_MII_XCVR ) { <nl> int phy , phy_idx = 0 ; <nl>  <nl> - for ( phy = 1 ; phy < 32 && phy_idx < 4 ; phy ++) { <nl> + for ( phy = 1 ; phy < 32 && phy_idx < ARRAY_SIZE ( np -> phys ); <nl> + phy ++) { <nl> int mii_status = mdio_read ( dev , phy , 1 ); <nl>  <nl> if ( mii_status != 0xffff && mii_status != 0x0000 ) {
static void _rtl8821ae_read_adapter_info ( struct ieee80211_hw * hw , bool b_pseudo_ <nl> if ( rtlefuse -> eeprom_channelplan == 0xff ) <nl> rtlefuse -> eeprom_channelplan = 0x7F ; <nl>  <nl> - /* set channel paln to world wide 13 */ <nl> - /* rtlefuse -> channel_plan = ( u8 ) rtlefuse -> eeprom_channelplan ; */ <nl> + /* set channel plan from efuse */ <nl> + rtlefuse -> channel_plan = rtlefuse -> eeprom_channelplan ; <nl>  <nl> /* parse xtal */ <nl> rtlefuse -> crystalcap = hwinfo [ EEPROM_XTAL_8821AE ];
static int __devinit snd_cs423x_pnp_init_mpu ( int dev , struct pnp_dev * pdev ) <nl> static int __devinit snd_card_cs4232_pnp ( int dev , struct snd_card_cs4236 * acard , <nl> struct pnp_dev * pdev ) <nl> { <nl> + acard -> wss = pdev ; <nl> if ( snd_cs423x_pnp_init_wss ( dev , acard -> wss ) < 0 ) <nl> return - EBUSY ; <nl> cport [ dev ] = - 1 ;
static void parse_elf ( void * output ) <nl> default : /* Ignore other PT_ * */ break ; <nl> } <nl> } <nl> + <nl> + free ( phdrs ); <nl> } <nl>  <nl> asmlinkage void decompress_kernel ( void * rmode , memptr heap ,
static struct dmi_system_id __initdata acpisleep_dmi_table [] = { <nl> DMI_MATCH ( DMI_PRODUCT_NAME , " Macmini1 , 1 "), <nl> }, <nl> }, <nl> + { <nl> + . callback = init_old_suspend_ordering , <nl> + . ident = " Asus Pundit P1 - AH2 ( M2N8L motherboard )", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_BOARD_VENDOR , " ASUSTek Computer INC ."), <nl> + DMI_MATCH ( DMI_BOARD_NAME , " M2N8L "), <nl> + }, <nl> + }, <nl> {}, <nl> }; <nl> # endif /* CONFIG_SUSPEND */
i915_gem_object_bind_to_gtt ( struct drm_gem_object * obj , unsigned alignment ) <nl> bool retry_alloc = false ; <nl> int ret ; <nl>  <nl> - if ( dev_priv -> mm . suspended ) <nl> - return - EBUSY ; <nl> - <nl> if ( obj_priv -> madv != I915_MADV_WILLNEED ) { <nl> DRM_ERROR (" Attempting to bind a purgeable object \ n "); <nl> return - EINVAL ;
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> if (! dev -> suspend_in_progress ) <nl> b43_rng_init ( wl ); <nl>  <nl> + ieee80211_wake_queues ( dev -> wl -> hw ); <nl> + <nl> b43_set_status ( dev , B43_STAT_INITIALIZED ); <nl>  <nl> if (! dev -> suspend_in_progress )
static int sg_io ( struct file * file , request_queue_t * q , <nl>  <nl> rq -> cmd_type = REQ_TYPE_BLOCK_PC ; <nl>  <nl> - /* <nl> - * bounce this after holding a reference to the original bio , it ' s <nl> - * needed for proper unmapping <nl> - */ <nl> - if ( rq -> bio ) <nl> - blk_queue_bounce ( q , & rq -> bio ); <nl> - <nl> rq -> timeout = jiffies_to_msecs ( hdr -> timeout ); <nl> if (! rq -> timeout ) <nl> rq -> timeout = q -> sg_timeout ;
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> + memset ( sax , 0 , sizeof ( sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
static int easycap_usb_probe ( struct usb_interface * intf , <nl> /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */ <nl> pdata_urb = kzalloc ( sizeof ( struct data_urb ), GFP_KERNEL ); <nl> if (! pdata_urb ) { <nl> + usb_free_urb ( purb ); <nl> SAM (" ERROR : Could not allocate struct data_urb .\ n "); <nl> return - ENOMEM ; <nl> }
int do_sigtimedwait ( const sigset_t * which , siginfo_t * info , <nl> recalc_sigpending (); <nl> spin_unlock_irq (& tsk -> sighand -> siglock ); <nl>  <nl> - timeout = schedule_timeout_interruptible ( timeout ); <nl> + timeout = freezable_schedule_timeout_interruptible ( timeout ); <nl>  <nl> spin_lock_irq (& tsk -> sighand -> siglock ); <nl> __set_task_blocked ( tsk , & tsk -> real_blocked );
static int ir_do_setkeycode ( struct input_dev * dev , <nl> break ; <nl> } <nl>  <nl> - if ( old_keycode == KEY_RESERVED ) { <nl> + if ( old_keycode == KEY_RESERVED && keycode != KEY_RESERVED ) { <nl> /* No previous mapping found , we might need to grow the table */ <nl> if ( ir_resize_table ( rc_tab )) <nl> return - ENOMEM ;
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 84 " <nl> -# define DRV_MODULE_RELDATE " October 12 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 85 " <nl> +# define DRV_MODULE_RELDATE " October 18 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static int exynos_drm_fbdev_update ( struct drm_fb_helper * helper , <nl>  <nl> fbi -> screen_base = buffer -> kvaddr + offset ; <nl> fbi -> screen_size = size ; <nl> + fbi -> fix . smem_len = size ; <nl>  <nl> return 0 ; <nl> }
static void sonic_rx ( struct net_device * dev ) <nl> status = sonic_rda_get ( dev , entry , SONIC_RD_STATUS ); <nl> if ( status & SONIC_RCR_PRX ) { <nl> /* Malloc up new buffer . */ <nl> - new_skb = netdev_alloc_skb ( SONIC_RBSIZE + 2 ); <nl> + new_skb = netdev_alloc_skb ( dev , SONIC_RBSIZE + 2 ); <nl> if ( new_skb == NULL ) { <nl> printk ( KERN_ERR "% s : Memory squeeze , dropping packet .\ n ", dev -> name ); <nl> lp -> stats . rx_dropped ++;
unsigned long long sched_clock ( void ) <nl>  <nl> /* jiffies based sched_clock if no clocksource is installed */ <nl> if (! clocksource_sh . rating ) <nl> - return ( unsigned long long ) jiffies * ( NSEC_PER_SEC / HZ ); <nl> + return ( jiffies_64 - INITIAL_JIFFIES ) * ( NSEC_PER_SEC / HZ ); <nl>  <nl> cycles = clocksource_sh . read (& clocksource_sh ); <nl> return cyc2ns (& clocksource_sh , cycles );
probe_fail_cdrom_register : <nl> del_gendisk ( gd . disk ); <nl> probe_fail_no_disk : <nl> kfree ( gd . cd_info ); <nl> + probe_fail_no_mem : <nl> unregister_blkdev ( gdrom_major , GDROM_DEV_NAME ); <nl> gdrom_major = 0 ; <nl> - probe_fail_no_mem : <nl> pr_warning (" Probe failed - error is 0x % X \ n ", err ); <nl> return err ; <nl> }
int scsi_error_handler ( void * data ) <nl> set_current_state ( TASK_INTERRUPTIBLE ); <nl> } <nl>  <nl> + __set_current_state ( TASK_RUNNING ); <nl> + <nl> SCSI_LOG_ERROR_RECOVERY ( 1 , printk (" Error handler scsi_eh_ % d " <nl> " exiting \ n ", shost -> host_no )); <nl> 
static void ieee80211_enable_ps ( struct ieee80211_local * local , <nl> { <nl> struct ieee80211_conf * conf = & local -> hw . conf ; <nl>  <nl> + /* <nl> + * If we are scanning right now then the parameters will <nl> + * take effect when scan finishes . <nl> + */ <nl> + if ( local -> hw_scanning || local -> sw_scanning ) <nl> + return ; <nl> + <nl> if ( conf -> dynamic_ps_timeout > 0 && <nl> !( local -> hw . flags & IEEE80211_HW_SUPPORTS_DYNAMIC_PS )) { <nl> mod_timer (& local -> dynamic_ps_timer , jiffies +
static int change_memory_common ( unsigned long addr , int numpages , <nl> WARN_ON_ONCE ( 1 ); <nl> } <nl>  <nl> + if (! numpages ) <nl> + return 0 ; <nl> + <nl> if ( start < MODULES_VADDR || start >= MODULES_END ) <nl> return - EINVAL ; <nl> 
static int __devinit piix_init_one ( struct pci_dev * pdev , <nl> u8 tmp ; <nl> pci_read_config_byte ( pdev , PIIX_SCC , & tmp ); <nl> if ( tmp == PIIX_AHCI_DEVICE ) { <nl> - int rc = piix_disable_ahci ( pdev ); <nl> + rc = piix_disable_ahci ( pdev ); <nl> if ( rc ) <nl> return rc ; <nl> }
static bool compliance_mode_recovery_timer_quirk_check ( void ) <nl>  <nl> dmi_product_name = dmi_get_system_info ( DMI_PRODUCT_NAME ); <nl> dmi_sys_vendor = dmi_get_system_info ( DMI_SYS_VENDOR ); <nl> + if (! dmi_product_name || ! dmi_sys_vendor ) <nl> + return false ; <nl>  <nl> if (!( strstr ( dmi_sys_vendor , " Hewlett - Packard "))) <nl> return false ;
static void bnx2fc_recv_frame ( struct sk_buff * skb ) <nl> break ; <nl> } <nl> } <nl> + <nl> + if ( fh -> fh_r_ctl == FC_RCTL_BA_ABTS ) { <nl> + /* Drop incoming ABTS */ <nl> + put_cpu (); <nl> + kfree_skb ( skb ); <nl> + return ; <nl> + } <nl> + <nl> if ( le32_to_cpu ( fr_crc ( fp )) != <nl> ~ crc32 (~ 0 , skb -> data , fr_len )) { <nl> if ( stats -> InvalidCRCCount < 5 )
struct kvm_vcpu_arch { <nl> struct kvm_mmu_memory_cache mmu_page_cache ; <nl>  <nl> /* Target CPU and feature flags */ <nl> - u32 target ; <nl> + int target ; <nl> DECLARE_BITMAP ( features , KVM_VCPU_MAX_FEATURES ); <nl>  <nl> /* Detect first run of a vcpu */
static int __devinit e100_probe ( struct pci_dev * pdev , <nl>  <nl> e100_get_defaults ( nic ); <nl>  <nl> + /* D100 MAC doesn ' t allow rx of vlan packets with normal MTU */ <nl> + if ( nic -> mac < mac_82558_D101_A4 ) <nl> + netdev -> features |= NETIF_F_VLAN_CHALLENGED ; <nl> + <nl> /* locks must be initialized before calling hw_reset */ <nl> spin_lock_init (& nic -> cb_lock ); <nl> spin_lock_init (& nic -> cmd_lock );
static int init_fb_chan ( struct mx3fb_data * mx3fb , struct idmac_channel * ichan ) <nl> init_completion (& mx3fbi -> flip_cmpl ); <nl> disable_irq ( ichan -> eof_irq ); <nl> dev_dbg ( mx3fb -> dev , " disabling irq % d \ n ", ichan -> eof_irq ); <nl> - ret = mx3fb_set_par ( fbi ); <nl> - if ( ret < 0 ) <nl> - goto esetpar ; <nl> - <nl> - mx3fb_blank ( FB_BLANK_UNBLANK , fbi ); <nl>  <nl> dev_info ( dev , " registered , using mode % s \ n ", fb_mode ); <nl> 
static int xfrm6_fill_dst ( struct xfrm_dst * xdst , struct net_device * dev , <nl> dev_hold ( dev ); <nl>  <nl> xdst -> u . rt6 . rt6i_idev = in6_dev_get ( dev ); <nl> - if (! xdst -> u . rt6 . rt6i_idev ) <nl> + if (! xdst -> u . rt6 . rt6i_idev ) { <nl> + dev_put ( dev ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> rt6_transfer_peer (& xdst -> u . rt6 , rt ); <nl> 
static int run_perf_stat ( int argc __used , const char ** argv ) <nl> "\ t Consider tweaking " <nl> " / proc / sys / kernel / perf_event_paranoid or running as root .", <nl> system_wide ? " system - wide " : ""); <nl> + } else if ( errno == ENOENT ) { <nl> + error ("% s event is not supported . ", event_name ( counter )); <nl> } else { <nl> error (" open_counter returned with % d (% s ). " <nl> "/ bin / dmesg may provide additional information .\ n ",
static long rtc_dev_ioctl ( struct file * file , <nl> err = ops -> ioctl ( rtc -> dev . parent , cmd , arg ); <nl> if ( err == - ENOIOCTLCMD ) <nl> err = - ENOTTY ; <nl> - } <nl> + } else <nl> + err = - ENOTTY ; <nl> break ; <nl> } <nl> 
static irqreturn_t et131x_isr ( int irq , void * dev_id ) <nl> { <nl> bool handled = true ; <nl> bool enable_interrupts = true ; <nl> - struct net_device * netdev = ( struct net_device *) dev_id ; <nl> + struct net_device * netdev = dev_id ; <nl> struct et131x_adapter * adapter = netdev_priv ( netdev ); <nl> struct address_map __iomem * iomem = adapter -> regs ; <nl> struct rx_ring * rx_ring = & adapter -> rx_ring ;
static int genwqe_pin_mem ( struct genwqe_file * cfile , struct genwqe_mem * m ) <nl> if ( rc != 0 ) { <nl> dev_err (& pci_dev -> dev , <nl> "[% s ] genwqe_user_vmap rc =% d \ n ", __func__ , rc ); <nl> + kfree ( dma_map ); <nl> return rc ; <nl> } <nl> 
xfs_ioc_trim ( <nl>  <nl> if (! capable ( CAP_SYS_ADMIN )) <nl> return - XFS_ERROR ( EPERM ); <nl> + if (! blk_queue_discard ( q )) <nl> + return - XFS_ERROR ( EOPNOTSUPP ); <nl> if ( copy_from_user (& range , urange , sizeof ( range ))) <nl> return - XFS_ERROR ( EFAULT ); <nl> 
static void mv_chan_set_mode ( struct mv_xor_chan * chan , <nl> config &= ~ 0x7 ; <nl> config |= op_mode ; <nl>  <nl> - if ( IS_ENABLED ( __BIG_ENDIAN )) <nl> - config |= XOR_DESCRIPTOR_SWAP ; <nl> - else <nl> - config &= ~ XOR_DESCRIPTOR_SWAP ; <nl> +# if defined ( __BIG_ENDIAN ) <nl> + config |= XOR_DESCRIPTOR_SWAP ; <nl> +# else <nl> + config &= ~ XOR_DESCRIPTOR_SWAP ; <nl> +# endif <nl>  <nl> writel_relaxed ( config , XOR_CONFIG ( chan )); <nl> chan -> current_type = type ;
static void ppp_cp_parse_cr ( struct net_device * dev , u16 pid , u8 id , <nl> for ( opt = data ; len ; len -= opt [ 1 ], opt += opt [ 1 ]) { <nl> if ( len < 2 || len < opt [ 1 ]) { <nl> dev -> stats . rx_errors ++; <nl> + kfree ( out ); <nl> return ; /* bad packet , drop silently */ <nl> } <nl> 
static int __devinit toshiba_acpi_setup_backlight ( struct toshiba_acpi_dev * dev ) <nl> ret = get_tr_backlight_status ( dev , & enabled ); <nl> dev -> tr_backlight_supported = ! ret ; <nl>  <nl> + memset (& props , 0 , sizeof ( props )); <nl> props . type = BACKLIGHT_PLATFORM ; <nl> props . max_brightness = HCI_LCD_BRIGHTNESS_LEVELS - 1 ; <nl> - memset (& props , 0 , sizeof ( props )); <nl>  <nl> /* adding an extra level and having 0 change to transflective mode */ <nl> if ( dev -> tr_backlight_supported )
int i2c_dw_probe ( struct dw_i2c_dev * dev ) <nl>  <nl> snprintf ( adap -> name , sizeof ( adap -> name ), <nl> " Synopsys DesignWare I2C adapter "); <nl> + adap -> retries = 3 ; <nl> adap -> algo = & i2c_dw_algo ; <nl> adap -> dev . parent = dev -> dev ; <nl> i2c_set_adapdata ( adap , dev );
static int patch_stac922x ( struct hda_codec * codec ) <nl> */ <nl> printk ( KERN_INFO " hda_codec : STAC922x , Apple subsys_id =% x \ n ", codec -> subsystem_id ); <nl> switch ( codec -> subsystem_id ) { <nl> + case 0x106b0a00 : /* MacBook First generatoin */ <nl> + spec -> board_config = STAC_MACBOOK ; <nl> + break ; <nl> case 0x106b0200 : /* MacBook Pro first generation */ <nl> spec -> board_config = STAC_MACBOOK_PRO_V1 ; <nl> break ;
intel_crt_load_detect ( struct drm_crtc * crtc , struct intel_encoder * intel_encoder <nl> if ( IS_I9XX ( dev )) { <nl> uint32_t pipeconf = I915_READ ( pipeconf_reg ); <nl> I915_WRITE ( pipeconf_reg , pipeconf | PIPECONF_FORCE_BORDER ); <nl> + POSTING_READ ( pipeconf_reg ); <nl> /* Wait for next Vblank to substitue <nl> * border color for Color info */ <nl> intel_wait_for_vblank ( dev , pipe );
static int pcrypt_aead_init_tfm ( struct crypto_tfm * tfm ) <nl> return PTR_ERR ( cipher ); <nl>  <nl> ctx -> child = cipher ; <nl> - tfm -> crt_aead . reqsize = sizeof ( struct pcrypt_request ) <nl> - + sizeof ( struct aead_givcrypt_request ) <nl> - + crypto_aead_reqsize ( cipher ); <nl> + crypto_aead_set_reqsize ( __crypto_aead_cast ( tfm ), <nl> + sizeof ( struct pcrypt_request ) + <nl> + sizeof ( struct aead_givcrypt_request ) + <nl> + crypto_aead_reqsize ( cipher )); <nl>  <nl> return 0 ; <nl> }
static unsigned int xuartps_set_baud_rate ( struct uart_port * port , <nl> unsigned int baud ) <nl> { <nl> unsigned int calc_baud ; <nl> - u32 cd , bdiv ; <nl> + u32 cd = 0 , bdiv = 0 ; <nl> u32 mreg ; <nl> int div8 ; <nl> struct xuartps * xuartps = port -> private_data ;
hub_port_init ( struct usb_hub * hub , struct usb_device * udev , int port1 , <nl>  <nl> did_new_scheme = true ; <nl> retval = hub_enable_device ( udev ); <nl> - if ( retval < 0 ) <nl> + if ( retval < 0 ) { <nl> + dev_err (& udev -> dev , <nl> + " hub failed to enable device , error % d \ n ", <nl> + retval ); <nl> goto fail ; <nl> + } <nl>  <nl> # define GET_DESCRIPTOR_BUFSIZE 64 <nl> buf = kmalloc ( GET_DESCRIPTOR_BUFSIZE , GFP_NOIO );
event_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_preds ( call ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl>  <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_subsystem_preds ( system ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl> 
static int dn_shutdown ( struct socket * sock , int how ) <nl> if ( scp -> state == DN_O ) <nl> goto out ; <nl>  <nl> - if ( how != SHUTDOWN_MASK ) <nl> + if ( how != SHUT_RDWR ) <nl> goto out ; <nl>  <nl> - sk -> sk_shutdown = how ; <nl> + sk -> sk_shutdown = SHUTDOWN_MASK ; <nl> dn_destroy_sock ( sk ); <nl> err = 0 ; <nl> 
int comedi_device_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> " driver '% s ' does not support attach using comedi_config \ n ", <nl> driv -> driver_name ); <nl> module_put ( driv -> module ); <nl> - ret = - ENOSYS ; <nl> + ret = - EIO ; <nl> goto out ; <nl> } <nl> dev -> driver = driv ;
static void sc_kref_release ( struct kref * kref ) <nl> sc -> sc_node = NULL ; <nl>  <nl> r2net_debug_del_sc ( sc ); <nl> + <nl> + if ( sc -> sc_page ) <nl> + __free_page ( sc -> sc_page ); <nl> kfree ( sc ); <nl> } <nl> 
EXPORT_SYMBOL_GPL ( irq_of_parse_and_map ); <nl>  <nl> void irq_dispose_mapping ( unsigned int virq ) <nl> { <nl> - struct irq_host * host = irq_map [ virq ]. host ; <nl> + struct irq_host * host ; <nl> irq_hw_number_t hwirq ; <nl> unsigned long flags ; <nl>  <nl> + if ( virq == NO_IRQ ) <nl> + return ; <nl> + <nl> + host = irq_map [ virq ]. host ; <nl> WARN_ON ( host == NULL ); <nl> if ( host == NULL ) <nl> return ;
static int mxcnd_probe ( struct platform_device * pdev ) <nl> if ( err ) <nl> return err ; <nl>  <nl> - clk_prepare_enable ( host -> clk ); <nl> + err = clk_prepare_enable ( host -> clk ); <nl> + if ( err ) <nl> + return err ; <nl> host -> clk_act = 1 ; <nl>  <nl> /*
static int gfar_start_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> kfree_skb ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> + <nl> + /* Steal sock reference for processing TX time stamps */ <nl> + swap ( skb_new -> sk , skb -> sk ); <nl> + swap ( skb_new -> destructor , skb -> destructor ); <nl> kfree_skb ( skb ); <nl> skb = skb_new ; <nl> }
struct extent_buffer * alloc_extent_buffer ( struct extent_io_tree * tree , <nl> spin_unlock (& tree -> buffer_lock ); <nl> goto free_eb ; <nl> } <nl> - spin_unlock (& tree -> buffer_lock ); <nl> - <nl> /* add one reference for the tree */ <nl> atomic_inc (& eb -> refs ); <nl> + spin_unlock (& tree -> buffer_lock ); <nl> return eb ; <nl>  <nl> free_eb :
static bool ath_rx_edma_buf_link ( struct ath_softc * sc , <nl> static void ath_rx_addbuffer_edma ( struct ath_softc * sc , <nl> enum ath9k_rx_qtype qtype , int size ) <nl> { <nl> - struct ath_rx_edma * rx_edma ; <nl> struct ath_common * common = ath9k_hw_common ( sc -> sc_ah ); <nl> u32 nbuf = 0 ; <nl>  <nl> - rx_edma = & sc -> rx . rx_edma [ qtype ]; <nl> if ( list_empty (& sc -> rx . rxbuf )) { <nl> ath_print ( common , ATH_DBG_QUEUE , " No free rx buf available \ n "); <nl> return ;
static void __devinit check_probe_mask ( struct azx * chip , int dev ) <nl> * white / black - list for enable_msi <nl> */ <nl> static struct snd_pci_quirk msi_black_list [] __devinitdata = { <nl> + SND_PCI_QUIRK ( 0x1043 , 0x81f2 , " ASUS ", 0 ), /* Athlon64 X2 + nvidia */ <nl> {} <nl> }; <nl> 
static int iio_device_add_event ( struct iio_dev * indio_dev , <nl> & indio_dev -> event_interface -> dev_attr_list ); <nl> kfree ( postfix ); <nl>  <nl> + if (( ret == - EBUSY ) && ( shared_by != IIO_SEPARATE )) <nl> + continue ; <nl> + <nl> if ( ret ) <nl> return ret ; <nl> 
static const struct of_device_id spinand_dt [] = { <nl> { . compatible = " spinand , mt29f ", }, <nl> {} <nl> }; <nl> + MODULE_DEVICE_TABLE ( of , spinand_dt ); <nl>  <nl> /* <nl> * Device name structure description
asmlinkage long sys_uselib ( const char __user * library ) <nl> if ( error ) <nl> goto out ; <nl>  <nl> + error = - EACCES ; <nl> + if ( nd . mnt -> mnt_flags & MNT_NOEXEC ) <nl> + goto exit ; <nl> error = - EINVAL ; <nl> if (! S_ISREG ( nd . dentry -> d_inode -> i_mode )) <nl> goto exit ;
static inline void ata_tf_init ( struct ata_device * dev , struct ata_taskfile * tf ) <nl>  <nl> static inline void ata_qc_reinit ( struct ata_queued_cmd * qc ) <nl> { <nl> + qc -> dma_dir = DMA_NONE ; <nl> qc -> __sg = NULL ; <nl> qc -> flags = 0 ; <nl> qc -> cursect = qc -> cursg = qc -> cursg_ofs = 0 ;
void gfs2_inplace_release ( struct gfs2_inode * ip ) <nl> { <nl> struct gfs2_blkreserv * rs = ip -> i_res ; <nl>  <nl> - gfs2_blkrsv_put ( ip ); <nl> if ( rs -> rs_rgd_gh . gh_gl ) <nl> gfs2_glock_dq_uninit (& rs -> rs_rgd_gh ); <nl> + gfs2_blkrsv_put ( ip ); <nl> } <nl>  <nl> /**
int symbol__init ( void ) <nl> if ( symbol_conf . initialized ) <nl> return 0 ; <nl>  <nl> + symbol_conf . priv_size = ALIGN ( symbol_conf . priv_size , sizeof ( u64 )); <nl> + <nl> elf_version ( EV_CURRENT ); <nl> if ( symbol_conf . sort_by_name ) <nl> symbol_conf . priv_size += ( sizeof ( struct symbol_name_rb_node ) -
struct vpif_disp_buffer { <nl> }; <nl>  <nl> struct common_obj { <nl> - /* Buffer specific parameters */ <nl> - u8 * fbuffers [ VIDEO_MAX_FRAME ]; /* List of buffer pointers for <nl> - * storing frames */ <nl> u32 numbuffers ; /* number of buffers */ <nl> struct vpif_disp_buffer * cur_frm ; /* Pointer pointing to current <nl> * vb2_buffer */
static void default_idle ( void ) <nl> if ( test_thread_flag ( TIF_MCCK_PENDING )) { <nl> local_mcck_enable (); <nl> local_irq_enable (); <nl> - s390_handle_mcck (); <nl> return ; <nl> } <nl> trace_hardirqs_on (); <nl> void cpu_idle ( void ) <nl> for (;;) { <nl> tick_nohz_idle_enter (); <nl> rcu_idle_enter (); <nl> - while (! need_resched ()) <nl> + while (! need_resched () && ! test_thread_flag ( TIF_MCCK_PENDING )) <nl> default_idle (); <nl> rcu_idle_exit (); <nl> tick_nohz_idle_exit (); <nl> + if ( test_thread_flag ( TIF_MCCK_PENDING )) <nl> + s390_handle_mcck (); <nl> preempt_enable_no_resched (); <nl> schedule (); <nl> preempt_disable ();
void atari_kbd_leds ( unsigned int leds ) <nl>  <nl> static int atari_keyb_done = 0 ; <nl>  <nl> - int __init atari_keyb_init ( void ) <nl> + int atari_keyb_init ( void ) <nl> { <nl> if ( atari_keyb_done ) <nl> return 0 ; <nl> int __init atari_keyb_init ( void ) <nl> atari_keyb_done = 1 ; <nl> return 0 ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( atari_keyb_init ); <nl>  <nl> int atari_kbd_translate ( unsigned char keycode , unsigned char * keycodep , char raw_mode ) <nl> {
int __devinit of_mtd_parse_partitions ( struct device * dev , <nl> return nr_parts ; <nl> } <nl> EXPORT_SYMBOL ( of_mtd_parse_partitions ); <nl> + <nl> + MODULE_LICENSE (" GPL ");
qla2x00_do_dpc_vp ( scsi_qla_host_t * vha ) <nl> struct qla_hw_data * ha = vha -> hw ; <nl> scsi_qla_host_t * base_vha = pci_get_drvdata ( ha -> pdev ); <nl>  <nl> + if (!( ha -> current_topology & ISP_CFG_F )) <nl> + return 0 ; <nl> + <nl> if ( test_and_clear_bit ( VP_IDX_ACQUIRED , & vha -> vp_flags )) { <nl> /* VP acquired . complete port configuration */ <nl> if ( atomic_read (& base_vha -> loop_state ) == LOOP_READY ) {
static int scan ( struct wiphy * wiphy , struct cfg80211_scan_request * request ) <nl> kmalloc_array ( request -> n_ssids , <nl> sizeof ( struct hidden_network ), <nl> GFP_KERNEL ); <nl> + if (! strHiddenNetwork . net_info ) <nl> + return - ENOMEM ; <nl> strHiddenNetwork . n_ssids = request -> n_ssids ; <nl>  <nl> 
static ssize_t eisa_eeprom_read ( struct file * file , <nl> ssize_t ret ; <nl> int i ; <nl>  <nl> - if (* ppos >= HPEE_MAX_LENGTH ) <nl> + if (* ppos < 0 || * ppos >= HPEE_MAX_LENGTH ) <nl> return 0 ; <nl>  <nl> count = * ppos + count < HPEE_MAX_LENGTH ? count : HPEE_MAX_LENGTH - * ppos ;
static ssize_t w1_seq_show ( struct device * device , <nl> w1_write_8 ( sl -> master , W1_42_COND_READ ); <nl> rv = w1_read_block ( sl -> master , ( u8 *)& rn , 8 ); <nl> reg_num = ( struct w1_reg_num *) & rn ; <nl> - if (( char ) reg_num -> family == W1_42_FINISHED_BYTE ) <nl> + if ( reg_num -> family == W1_42_FINISHED_BYTE ) <nl> break ; <nl> if ( sl -> reg_num . id == reg_num -> id ) <nl> seq = i ;
static void iso_callback ( struct fw_iso_context * context , u32 cycle , <nl> struct client * client = data ; <nl> struct iso_interrupt_event * e ; <nl>  <nl> - e = kzalloc ( sizeof (* e ) + header_length , GFP_ATOMIC ); <nl> + e = kmalloc ( sizeof (* e ) + header_length , GFP_ATOMIC ); <nl> if ( e == NULL ) <nl> return ; <nl> 
static int do_dev_config ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> if (! devs ) { <nl> dev_err ( dev -> class_dev , <nl> " Could not allocate memory . Out of memory ?\ n "); <nl> + kfree ( bdev ); <nl> return - ENOMEM ; <nl> } <nl> devpriv -> devs = devs ;
static int run ( mddev_t * mddev ) <nl> int i ; <nl>  <nl> conf_t * conf = kmalloc ( sizeof (* conf ), GFP_KERNEL ); <nl> + if (! conf ) <nl> + return - ENOMEM ; <nl>  <nl> for ( i = 0 ; i < Modes ; i ++) { <nl> atomic_set (& conf -> counters [ i ], 0 );
static int rtl8169_rx_interrupt ( struct net_device * dev , <nl> pkt_size , PCI_DMA_FROMDEVICE ); <nl> rtl8169_mark_to_asic ( desc , tp -> rx_buf_sz ); <nl> } else { <nl> - pci_unmap_single ( pdev , addr , pkt_size , <nl> + pci_unmap_single ( pdev , addr , tp -> rx_buf_sz , <nl> PCI_DMA_FROMDEVICE ); <nl> tp -> Rx_skbuff [ entry ] = NULL ; <nl> }
static irqreturn_t snd_hdsp_interrupt ( int irq , void * dev_id ) <nl> midi0status = hdsp_read ( hdsp , HDSP_midiStatusIn0 ) & 0xff ; <nl> midi1status = hdsp_read ( hdsp , HDSP_midiStatusIn1 ) & 0xff ; <nl>  <nl> + if (!( hdsp -> state & HDSP_InitializationComplete )) <nl> + return IRQ_HANDLED ; <nl> + <nl> if ( audio ) { <nl> if ( hdsp -> capture_substream ) <nl> snd_pcm_period_elapsed ( hdsp -> pcm -> streams [ SNDRV_PCM_STREAM_CAPTURE ]. substream );
xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
static inline bool i40e_vc_isvalid_vsi_id ( struct i40e_vf * vf , u8 vsi_id ) <nl> { <nl> struct i40e_pf * pf = vf -> pf ; <nl>  <nl> + if ( vsi_id > pf -> num_alloc_vsi ) <nl> + return false ; <nl> return pf -> vsi [ vsi_id ]-> vf_id == vf -> vf_id ; <nl> } <nl> 
int __init acpi_ec_ecdt_probe ( void ) <nl> saved_ec = kmalloc ( sizeof ( struct acpi_ec ), GFP_KERNEL ); <nl> if (! saved_ec ) <nl> return - ENOMEM ; <nl> - memcpy (& saved_ec , boot_ec , sizeof ( saved_ec )); <nl> + memcpy ( saved_ec , boot_ec , sizeof (* saved_ec )); <nl> /* fall through */ <nl> } <nl> /* This workaround is needed only on some broken machines ,
static int ath10k_station_assoc ( struct ath10k * ar , struct ath10k_vif * arvif , <nl> return ret ; <nl> } <nl>  <nl> - if (! sta -> wme ) { <nl> + if (! sta -> wme && ! reassoc ) { <nl> arvif -> num_legacy_stations ++; <nl> ret = ath10k_recalc_rtscts_prot ( arvif ); <nl> if ( ret ) {
_xfs_buf_ioapply ( <nl> int size ; <nl> int i ; <nl>  <nl> + /* <nl> + * Make sure we capture only current IO errors rather than stale errors <nl> + * left over from previous use of the buffer ( e . g . failed readahead ). <nl> + */ <nl> + bp -> b_error = 0 ; <nl> + <nl> if ( bp -> b_flags & XBF_WRITE ) { <nl> if ( bp -> b_flags & XBF_SYNCIO ) <nl> rw = WRITE_SYNC ;
static void handle_callback ( struct gfs2_glock * gl , unsigned int state , int remot <nl> } <nl> return ; <nl> } <nl> - } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED ) { <nl> - gl -> gl_demote_state = state ; <nl> + } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED && <nl> + gl -> gl_demote_state != state ) { <nl> + gl -> gl_demote_state = LM_ST_UNLOCKED ; <nl> } <nl> spin_unlock (& gl -> gl_spin ); <nl> }
__append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
void free_initmem ( void ) <nl> if (! have_of ) <nl> FREESEC ( openfirmware ); <nl> printk ("\ n "); <nl> + ppc_md . progress = NULL ; <nl> # undef FREESEC <nl> } <nl> 
static void mlx4_enable_msi_x ( struct mlx4_dev * dev ) <nl> nreq = err ; <nl> goto retry ; <nl> } <nl> - <nl> + kfree ( entries ); <nl> goto no_msi ; <nl> } <nl> 
static void batadv_hash_init ( struct batadv_hashtable * hash ) <nl> { <nl> uint32_t i ; <nl>  <nl> - for ( i = 0 ; i < hash -> size ; i ++) { <nl> + for ( i = 0 ; i < hash -> size ; i ++) { <nl> INIT_HLIST_HEAD (& hash -> table [ i ]); <nl> spin_lock_init (& hash -> list_locks [ i ]); <nl> }
static void exynos5_powerdown_conf ( enum sys_powerdown mode ) <nl> void exynos_sys_powerdown_conf ( enum sys_powerdown mode ) <nl> { <nl> unsigned int i ; <nl> + const struct exynos_pmu_data * pmu_data ; <nl> + <nl> + if (! pmu_context ) <nl> + return ; <nl>  <nl> - const struct exynos_pmu_data * pmu_data = pmu_context -> pmu_data ; <nl> + pmu_data = pmu_context -> pmu_data ; <nl>  <nl> if ( pmu_data -> powerdown_conf ) <nl> pmu_data -> powerdown_conf ( mode );
static inline int open_arg ( int flags , int mask ) <nl>  <nl> static int audit_match_perm ( struct audit_context * ctx , int mask ) <nl> { <nl> + unsigned n ; <nl> if ( unlikely (! ctx )) <nl> return 0 ; <nl>  <nl> - unsigned n = ctx -> major ; <nl> + n = ctx -> major ; <nl> switch ( audit_classify_syscall ( ctx -> arch , n )) { <nl> case 0 : /* native */ <nl> if (( mask & AUDIT_PERM_WRITE ) &&
static const struct file_operations bm_register_operations = { <nl> static ssize_t <nl> bm_status_read ( struct file * file , char __user * buf , size_t nbytes , loff_t * ppos ) <nl> { <nl> - char * s = enabled ? " enabled " : " disabled "; <nl> + char * s = enabled ? " enabled \ n " : " disabled \ n "; <nl>  <nl> return simple_read_from_buffer ( buf , nbytes , ppos , s , strlen ( s )); <nl> }
static int vb2_internal_streamon ( struct vb2_queue * q , enum v4l2_buf_type type ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if (! q -> num_buffers ) { <nl> + dprintk ( 1 , " streamon : no buffers have been allocated \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* <nl> * If any buffers were queued before streamon , <nl> * we can now pass them to driver for processing .
static int radeon_uvd_cs_reloc ( struct radeon_cs_parser * p , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (( start >> 28 ) != ( end >> 28 )) { <nl> + if (( start >> 28 ) != (( end - 1 ) >> 28 )) { <nl> DRM_ERROR (" reloc % LX -% LX crossing 256MB boundary !\ n ", <nl> start , end ); <nl> return - EINVAL ;
nfqnl_build_packet_message ( struct nfqnl_instance * queue , <nl> struct net_device * indev ; <nl> struct net_device * outdev ; <nl>  <nl> - size = NLMSG_ALIGN ( sizeof ( struct nfgenmsg )) <nl> + size = NLMSG_SPACE ( sizeof ( struct nfgenmsg )) <nl> + nla_total_size ( sizeof ( struct nfqnl_msg_packet_hdr )) <nl> + nla_total_size ( sizeof ( u_int32_t )) /* ifindex */ <nl> + nla_total_size ( sizeof ( u_int32_t )) /* ifindex */
static const char * intel_pt_err_msgs [] = { <nl>  <nl> int intel_pt__strerror ( int code , char * buf , size_t buflen ) <nl> { <nl> - if ( code < 1 || code > INTEL_PT_ERR_MAX ) <nl> + if ( code < 1 || code >= INTEL_PT_ERR_MAX ) <nl> code = INTEL_PT_ERR_UNK ; <nl> strlcpy ( buf , intel_pt_err_msgs [ code ], buflen ); <nl> return 0 ;
int dm_suspend ( struct mapped_device * md , unsigned suspend_flags ) <nl> * requests are being added to md -> deferred list . <nl> */ <nl>  <nl> - dm_table_postsuspend_targets ( map ); <nl> - <nl> set_bit ( DMF_SUSPENDED , & md -> flags ); <nl>  <nl> + dm_table_postsuspend_targets ( map ); <nl> + <nl> out : <nl> dm_table_put ( map ); <nl> 
static int jfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> */ <nl> rc = dtSearch ( new_dir , & new_dname , & ino , & btstack , JFS_LOOKUP ); <nl> if (! rc ) { <nl> - if (( new_ip == 0 ) || ( ino != new_ip -> i_ino )) { <nl> + if ((! new_ip ) || ( ino != new_ip -> i_ino )) { <nl> rc = - ESTALE ; <nl> goto out3 ; <nl> }
static int __split_vma ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> return 0 ; <nl>  <nl> /* Clean everything up if vma_adjust failed . */ <nl> - new -> vm_ops -> close ( new ); <nl> + if ( new -> vm_ops && new -> vm_ops -> close ) <nl> + new -> vm_ops -> close ( new ); <nl> if ( new -> vm_file ) { <nl> if ( vma -> vm_flags & VM_EXECUTABLE ) <nl> removed_exe_file_vma ( mm );
static void sti_crtc_atomic_flush ( struct drm_crtc * crtc , <nl>  <nl> switch ( plane -> status ) { <nl> case STI_PLANE_UPDATED : <nl> + /* ignore update for other CRTC */ <nl> + if ( p -> state -> crtc != crtc ) <nl> + continue ; <nl> + <nl> /* update planes tag as updated */ <nl> DRM_DEBUG_DRIVER (" update plane % s \ n ", <nl> sti_plane_to_str ( plane ));
static struct intel_iommu * device_to_iommu ( struct device * dev , u8 * bus , u8 * devf <nl> * which we used for the IOMMU lookup . Strictly speaking <nl> * we could do this for all PCI devices ; we only need to <nl> * get the BDF # from the scope table for ACPI matches . */ <nl> - if ( pdev -> is_virtfn ) <nl> + if ( pdev && pdev -> is_virtfn ) <nl> goto got_pdev ; <nl>  <nl> * bus = drhd -> devices [ i ]. bus ;
static struct afs_server * afs_alloc_server ( struct afs_cell * cell , <nl>  <nl> memcpy (& server -> addr , addr , sizeof ( struct in_addr )); <nl> server -> addr . s_addr = addr -> s_addr ; <nl> + _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> + } else { <nl> + _leave (" = NULL [ nomem ]"); <nl> } <nl> - <nl> - _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> return server ; <nl> } <nl> 
struct greybus_host_device * greybus_create_hd ( struct greybus_host_driver * driver <nl>  <nl> if ( buffer_size_max < GB_OPERATION_MESSAGE_SIZE_MIN ) { <nl> dev_err ( parent , " greybus host - device buffers too small \ n "); <nl> - return NULL ; <nl> + return ERR_PTR (- EINVAL ); <nl> } <nl>  <nl> if ( num_cports == 0 || num_cports > CPORT_ID_MAX ) {
static __u32 get_ftdi_divisor ( struct tty_struct * tty , <nl> case FT2232H : /* FT2232H chip */ <nl> case FT4232H : /* FT4232H chip */ <nl> case FT232H : /* FT232H chip */ <nl> - if (( baud <= 12000000 ) & ( baud >= 1200 )) { <nl> + if (( baud <= 12000000 ) && ( baud >= 1200 )) { <nl> div_value = ftdi_2232h_baud_to_divisor ( baud ); <nl> } else if ( baud < 1200 ) { <nl> div_value = ftdi_232bm_baud_to_divisor ( baud );
static void printl ( const char * fmt , ...) <nl>  <nl> kfifo_put ( tcpw . fifo , tbuf , len ); <nl> wake_up (& tcpw . wait ); <nl> -} <nl> +} __attribute__ (( format ( printf , 1 , 2 ))); <nl> + <nl>  <nl> /* <nl> * Hook inserted to be called before each receive packet .
int ath9k_hw_init ( struct ath_hw * ah ) <nl> struct ath_common * common = ath9k_hw_common ( ah ); <nl> int r = 0 ; <nl>  <nl> - if (! ath9k_hw_devid_supported ( ah -> hw_version . devid )) <nl> + if (! ath9k_hw_devid_supported ( ah -> hw_version . devid )) { <nl> + ath_print ( common , ATH_DBG_FATAL , <nl> + " Unsupported device ID : 0x % 0x \ n ", <nl> + ah -> hw_version . devid ); <nl> return - EOPNOTSUPP ; <nl> + } <nl>  <nl> ath9k_hw_init_defaults ( ah ); <nl> ath9k_hw_init_config ( ah );
int drbd_adm_resize ( struct sk_buff * skb , struct genl_info * info ) <nl> mutex_unlock (& device -> resource -> conf_update ); <nl> synchronize_rcu (); <nl> kfree ( old_disk_conf ); <nl> + new_disk_conf = NULL ; <nl> } <nl>  <nl> ddsf = ( rs . resize_force ? DDSF_FORCED : 0 ) | ( rs . no_resync ? DDSF_NO_RESYNC : 0 ); <nl> int drbd_adm_resize ( struct sk_buff * skb , struct genl_info * info ) <nl>  <nl> fail_ldev : <nl> put_ldev ( device ); <nl> + kfree ( new_disk_conf ); <nl> goto fail ; <nl> } <nl> 
static int i40e_suspend ( struct pci_dev * pdev , pm_message_t state ) <nl>  <nl> set_bit ( __I40E_SUSPENDED , & pf -> state ); <nl> set_bit ( __I40E_DOWN , & pf -> state ); <nl> + del_timer_sync (& pf -> service_timer ); <nl> + cancel_work_sync (& pf -> service_task ); <nl> rtnl_lock (); <nl> i40e_prep_for_reset ( pf ); <nl> rtnl_unlock ();
static int __init atmel_nand_probe ( struct platform_device * pdev ) <nl> } <nl> } <nl> if ( use_dma ) <nl> - dev_info ( host -> dev , " Using DMA for NAND access .\ n "); <nl> + dev_info ( host -> dev , " Using % s for DMA transfers .\ n ", <nl> + dma_chan_name ( host -> dma_chan )); <nl> else <nl> dev_info ( host -> dev , " No DMA support for NAND access .\ n "); <nl> 
static int fb_deferred_io_mkwrite ( struct vm_area_struct * vma , <nl> deferred framebuffer IO . then if userspace touches a page <nl> again , we repeat the same scheme */ <nl>  <nl> + file_update_time ( vma -> vm_file ); <nl> + <nl> /* protect against the workqueue changing the page list */ <nl> mutex_lock (& fbdefio -> lock ); <nl> 
static void svm_get_segment ( struct kvm_vcpu * vcpu , <nl> if ( seg == VCPU_SREG_CS ) <nl> var -> g = s -> limit > 0xfffff ; <nl>  <nl> + /* <nl> + * Work around a bug where the busy flag in the tr selector <nl> + * isn ' t exposed <nl> + */ <nl> + if ( seg == VCPU_SREG_TR ) <nl> + var -> type |= 0x2 ; <nl> + <nl> var -> unusable = ! var -> present ; <nl> } <nl> 
struct gb_sdio_get_caps_response { <nl>  <nl> /* see possible values below at vdd */ <nl> __le32 ocr ; <nl> - __le16 max_blk_count ; <nl> - __le16 max_blk_size ; <nl> __le32 f_min ; <nl> __le32 f_max ; <nl> + __le16 max_blk_count ; <nl> + __le16 max_blk_size ; <nl> } __packed ; <nl>  <nl> /* set ios request : response has no payload */
void xhci_mem_cleanup ( struct xhci_hcd * xhci ) <nl>  <nl> if ( xhci -> lpm_command ) <nl> xhci_free_command ( xhci , xhci -> lpm_command ); <nl> + xhci -> lpm_command = NULL ; <nl> if ( xhci -> cmd_ring ) <nl> xhci_ring_free ( xhci , xhci -> cmd_ring ); <nl> xhci -> cmd_ring = NULL ;
static void fsg_unbind ( struct usb_configuration * c , struct usb_function * f ) <nl> fsg_common_put ( common ); <nl> usb_free_descriptors ( fsg -> function . descriptors ); <nl> usb_free_descriptors ( fsg -> function . hs_descriptors ); <nl> + usb_free_descriptors ( fsg -> function . ss_descriptors ); <nl> kfree ( fsg ); <nl> } <nl> 
extern void __chk_io_ptr ( void __iomem *); <nl> # define __deprecated /* unimplemented */ <nl> # endif <nl>  <nl> +# ifdef MODULE <nl> +# define __deprecated_for_modules __deprecated <nl> +# else <nl> +# define __deprecated_for_modules <nl> +# endif <nl> + <nl> # ifndef __must_check <nl> # define __must_check <nl> # endif
static ssize_t rng_dev_read ( struct file * filp , char __user * buf , size_t size , <nl> if ( filp -> f_flags & O_NONBLOCK ) <nl> return ret ? : - EAGAIN ; <nl>  <nl> - if ( need_resched ()){ <nl> - current -> state = TASK_INTERRUPTIBLE ; <nl> - schedule_timeout ( 1 ); <nl> - } <nl> + if ( need_resched ()) <nl> + schedule_timeout_interruptible ( 1 ); <nl> } <nl> else return n ; <nl> if ( signal_pending ( current ))
static inline void iounmap ( volatile void __iomem * addr ) <nl> { <nl> } <nl>  <nl> + static inline void __iomem * ioport_map ( unsigned long port , unsigned int nr ) <nl> +{ <nl> + return NULL ; <nl> +} <nl> + <nl> + static inline void ioport_unmap ( void __iomem * p ) <nl> +{ <nl> +} <nl> + <nl> /* <nl> * s390 needs a private implementation of pci_iomap since ioremap with its <nl> * offset parameter isn ' t sufficient . That ' s because BAR spaces are not
pnfs_layout_bulk_destroy_byserver_locked ( struct nfs_client * clp , <nl> struct inode * inode ; <nl>  <nl> list_for_each_entry_safe ( lo , next , & server -> layouts , plh_layouts ) { <nl> + if ( test_bit ( NFS_LAYOUT_INVALID_STID , & lo -> plh_flags )) <nl> + continue ; <nl> inode = igrab ( lo -> plh_inode ); <nl> if ( inode == NULL ) <nl> continue ;
static int fman_init ( struct fman * fman ) <nl> /* allocate MURAM for FIFO according to total size */ <nl> fman -> fifo_offset = fman_muram_alloc ( fman -> muram , <nl> fman -> state -> total_fifo_size ); <nl> - if ( IS_ERR_VALUE ( fman -> cam_offset )) { <nl> + if ( IS_ERR_VALUE ( fman -> fifo_offset )) { <nl> free_init_resources ( fman ); <nl> dev_err ( fman -> dev , "% s : MURAM alloc for BMI FIFO failed \ n ", <nl> __func__ );
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> } <nl> btrfs_release_path ( root , path ); <nl>  <nl> - if ( key . offset + datal < off || <nl> + if ( key . offset + datal <= off || <nl> key . offset >= off + len ) <nl> goto next ; <nl> 
static void cpufreq_offline_finish ( unsigned int cpu ) <nl> * since this is a core component , and is essential for the <nl> * subsequent light - weight -> init () to succeed . <nl> */ <nl> - if ( cpufreq_driver -> exit ) <nl> + if ( cpufreq_driver -> exit ) { <nl> cpufreq_driver -> exit ( policy ); <nl> + policy -> freq_table = NULL ; <nl> + } <nl> } <nl>  <nl> /**
struct radeon_device { <nl> struct radeon_object * fbdev_robj ; <nl> struct radeon_framebuffer * fbdev_rfb ; <nl> /* Register mmio */ <nl> - unsigned long rmmio_base ; <nl> - unsigned long rmmio_size ; <nl> + resource_size_t rmmio_base ; <nl> + resource_size_t rmmio_size ; <nl> void * rmmio ; <nl> radeon_rreg_t mm_rreg ; <nl> radeon_wreg_t mm_wreg ;
int rt28xx_sta_ioctl ( IN struct net_device * net_dev , <nl> Status = <nl> copy_to_user ( erq -> pointer , pAd -> nickname , <nl> erq -> length ); <nl> + if ( Status ) <nl> + Status = - EFAULT ; <nl> break ; <nl> } <nl> case SIOCGIWRATE : /* get default bit rate ( bps ) */
int drbd_merge_bvec ( struct request_queue * q , struct bvec_merge_data * bvm , struct <nl> struct request_queue * const b = <nl> device -> ldev -> backing_bdev -> bd_disk -> queue ; <nl> if ( b -> merge_bvec_fn ) { <nl> + bvm -> bi_bdev = device -> ldev -> backing_bdev ; <nl> backing_limit = b -> merge_bvec_fn ( b , bvm , bvec ); <nl> limit = min ( limit , backing_limit ); <nl> }
static irqreturn_t skge_intr ( int irq , void * dev_id ) <nl>  <nl> if ( status & IS_HW_ERR ) <nl> skge_error_irq ( hw ); <nl> - <nl> + out : <nl> skge_write32 ( hw , B0_IMSK , hw -> intr_mask ); <nl> skge_read32 ( hw , B0_IMSK ); <nl> - out : <nl> spin_unlock (& hw -> hw_lock ); <nl>  <nl> return IRQ_RETVAL ( handled );
static int __init twl4030_pwrbutton_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> free_irq : <nl> - free_irq ( irq , NULL ); <nl> + free_irq ( irq , pwr ); <nl> free_input_dev : <nl> input_free_device ( pwr ); <nl> return err ;
befs_find_brun_dblindirect ( struct super_block * sb , <nl> struct buffer_head * indir_block ; <nl> befs_block_run indir_run ; <nl> befs_disk_inode_addr * iaddr_array ; <nl> - struct befs_sb_info * befs_sb = BEFS_SB ( sb ); <nl>  <nl> befs_blocknr_t indir_start_blk = <nl> - data -> max_indirect_range >> befs_sb -> block_shift ; <nl> + data -> max_indirect_range >> BEFS_SB ( sb )-> block_shift ; <nl>  <nl> off_t dbl_indir_off = blockno - indir_start_blk ; <nl> 
int ieee80211_radiotap_iterator_init ( <nl> /* find payload start allowing for extended bitmap ( s ) */ <nl>  <nl> if ( iterator -> _bitmap_shifter & ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> + if (( unsigned long ) iterator -> _arg - <nl> + ( unsigned long ) iterator -> _rtheader + sizeof ( uint32_t ) > <nl> + ( unsigned long ) iterator -> _max_length ) <nl> + return - EINVAL ; <nl> while ( get_unaligned_le32 ( iterator -> _arg ) & <nl> ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> iterator -> _arg += sizeof ( uint32_t );
static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
static int s3c_fb_check_var ( struct fb_var_screeninfo * var , <nl>  <nl> default : <nl> dev_err ( sfb -> dev , " invalid bpp \ n "); <nl> + return - EINVAL ; <nl> } <nl>  <nl> dev_dbg ( sfb -> dev , "% s : verified parameters \ n ", __func__ );
static struct of_device * __init scan_one_device ( struct device_node * dp , <nl> if (! parent ) <nl> strcpy ( op -> dev . bus_id , " root "); <nl> else <nl> - strcpy ( op -> dev . bus_id , dp -> path_component_name ); <nl> + sprintf ( op -> dev . bus_id , "% s @% 08x ", dp -> name , dp -> node ); <nl>  <nl> if ( of_device_register ( op )) { <nl> printk ("% s : Could not register of device .\ n ",
static int f2fs_write_data_pages ( struct address_space * mapping , <nl> available_free_memory ( sbi , DIRTY_DENTS )) <nl> goto skip_write ; <nl>  <nl> + /* during POR , we don ' t need to trigger writepage at all . */ <nl> + if ( unlikely ( is_sbi_flag_set ( sbi , SBI_POR_DOING ))) <nl> + goto skip_write ; <nl> + <nl> diff = nr_pages_to_write ( sbi , DATA , wbc ); <nl>  <nl> if (! S_ISDIR ( inode -> i_mode )) {
static int __init dm_bufio_init ( void ) <nl> { <nl> __u64 mem ; <nl>  <nl> + dm_bufio_allocated_kmem_cache = 0 ; <nl> + dm_bufio_allocated_get_free_pages = 0 ; <nl> + dm_bufio_allocated_vmalloc = 0 ; <nl> + dm_bufio_current_allocated = 0 ; <nl> + <nl> memset (& dm_bufio_caches , 0 , sizeof dm_bufio_caches ); <nl> memset (& dm_bufio_cache_names , 0 , sizeof dm_bufio_cache_names ); <nl> 
svc_tcp_accept ( struct svc_sock * svsk ) <nl> serv -> sv_name ); <nl> printk ( KERN_NOTICE <nl> "% s : last TCP connect from % s \ n ", <nl> - serv -> sv_name , buf ); <nl> + serv -> sv_name , __svc_print_addr ( sin , <nl> + buf , sizeof ( buf ))); <nl> } <nl> /* <nl> * Always select the oldest socket . It ' s not fair ,
static void <nl> activate_substream ( struct snd_usb_caiaqdev * dev , <nl> struct snd_pcm_substream * sub ) <nl> { <nl> + spin_lock (& dev -> spinlock ); <nl> + <nl> if ( sub -> stream == SNDRV_PCM_STREAM_PLAYBACK ) <nl> dev -> sub_playback [ sub -> number ] = sub ; <nl> else <nl> dev -> sub_capture [ sub -> number ] = sub ; <nl> + <nl> + spin_unlock (& dev -> spinlock ); <nl> } <nl>  <nl> static void
extern struct mlog_bits mlog_and_bits , mlog_not_bits ; <nl> # define mlog_errno ( st ) do { \ <nl> int _st = ( st ); \ <nl> if ( _st != - ERESTARTSYS && _st != - EINTR && \ <nl> - _st != AOP_TRUNCATED_PAGE ) \ <nl> + _st != AOP_TRUNCATED_PAGE && _st != - ENOSPC ) \ <nl> mlog ( ML_ERROR , " status = % lld \ n ", ( long long ) _st ); \ <nl> } while ( 0 ) <nl> 
static int ds2760_battery_remove ( struct platform_device * pdev ) <nl> & di -> set_charged_work ); <nl> destroy_workqueue ( di -> monitor_wqueue ); <nl> power_supply_unregister (& di -> bat ); <nl> + kfree ( di ); <nl>  <nl> return 0 ; <nl> }
static bool check_device_tree ( struct ath6kl * ar ) <nl> board_filename , ret ); <nl> continue ; <nl> } <nl> + of_node_put ( node ); <nl> return true ; <nl> } <nl> return false ;
static int net2280_stop ( struct usb_gadget * _gadget , <nl> device_remove_file (& dev -> pdev -> dev , & dev_attr_function ); <nl> device_remove_file (& dev -> pdev -> dev , & dev_attr_queues ); <nl>  <nl> - DEBUG ( dev , " unregistered driver '% s '\ n ", driver -> driver . name ); <nl> + DEBUG ( dev , " unregistered driver '% s '\ n ", <nl> + driver ? driver -> driver . name : ""); <nl> + <nl> return 0 ; <nl> } <nl> 
int i40e_ndo_get_vf_config ( struct net_device * netdev , <nl> else <nl> ivi -> linkstate = IFLA_VF_LINK_STATE_DISABLE ; <nl> ivi -> spoofchk = vf -> spoofchk ; <nl> + ivi -> trusted = vf -> trusted ; <nl> ret = 0 ; <nl>  <nl> error_param :
struct ifmcaddr6 <nl> struct ip6_sf_list * mca_sources ; <nl> struct ip6_sf_list * mca_tomb ; <nl> unsigned int mca_sfmode ; <nl> + unsigned char mca_crcount ; <nl> unsigned long mca_sfcount [ 2 ]; <nl> struct timer_list mca_timer ; <nl> unsigned mca_flags ; <nl> int mca_users ; <nl> atomic_t mca_refcnt ; <nl> spinlock_t mca_lock ; <nl> - unsigned char mca_crcount ; <nl> unsigned long mca_cstamp ; <nl> unsigned long mca_tstamp ; <nl> };
static ssize_t gpio_keys_attr_store_helper ( struct gpio_keys_drvdata * ddata , <nl> } <nl> } <nl>  <nl> + if ( i == ddata -> pdata -> nbuttons ) { <nl> + error = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> mutex_lock (& ddata -> disable_lock ); <nl>  <nl> for ( i = 0 ; i < ddata -> pdata -> nbuttons ; i ++) {
static const struct ich_laptop ich_laptop [] = { <nl> { 0x27DF , 0x0005 , 0x0280 }, /* ICH7 on Acer 5602WLMi */ <nl> { 0x27DF , 0x1025 , 0x0110 }, /* ICH7 on Acer 3682WLMi */ <nl> { 0x27DF , 0x1043 , 0x1267 }, /* ICH7 on Asus W5F */ <nl> + { 0x27DF , 0x103C , 0x30A1 }, /* ICH7 on HP Compaq nc2400 */ <nl> { 0x24CA , 0x1025 , 0x0061 }, /* ICH4 on ACER Aspire 2023WLMi */ <nl> /* end marker */ <nl> { 0 , }
static int loop_clr_fd ( struct loop_device * lo , struct block_device * bdev ) <nl> lo -> lo_state = Lo_unbound ; <nl> /* This is safe : open () is still holding a reference . */ <nl> module_put ( THIS_MODULE ); <nl> - if ( max_part > 0 ) <nl> + if ( max_part > 0 && bdev ) <nl> ioctl_by_bdev ( bdev , BLKRRPART , 0 ); <nl> mutex_unlock (& lo -> lo_ctl_mutex ); <nl> /*
static int blkpg_ioctl ( struct block_device * bdev , struct blkpg_ioctl_arg __user <nl> || pstart < 0 || plength < 0 || partno > 65535 ) <nl> return - EINVAL ; <nl> } <nl> + /* check if partition is aligned to blocksize */ <nl> + if ( p . start & ( bdev_logical_block_size ( bdev ) - 1 )) <nl> + return - EINVAL ; <nl>  <nl> mutex_lock (& bdev -> bd_mutex ); <nl> 
static int radeon_atom_pick_pll ( struct drm_crtc * crtc ) <nl> if ( pll != ATOM_PPLL_INVALID ) <nl> return pll ; <nl> } <nl> - } else { <nl> + } else if (! ASIC_IS_DCE41 ( rdev )) { /* Don ' t share PLLs on DCE4 . 1 chips */ <nl> /* use the same PPLL for all monitors with the same clock */ <nl> pll = radeon_get_shared_nondp_ppll ( crtc ); <nl> if ( pll != ATOM_PPLL_INVALID )
static noinline size_t if_nlmsg_size ( const struct net_device * dev , <nl> + rtnl_link_get_af_size ( dev , ext_filter_mask ) /* IFLA_AF_SPEC */ <nl> + nla_total_size ( MAX_PHYS_ITEM_ID_LEN ) /* IFLA_PHYS_PORT_ID */ <nl> + nla_total_size ( MAX_PHYS_ITEM_ID_LEN ) /* IFLA_PHYS_SWITCH_ID */ <nl> + + nla_total_size ( IFNAMSIZ ) /* IFLA_PHYS_PORT_NAME */ <nl> + nla_total_size ( 1 ); /* IFLA_PROTO_DOWN */ <nl>  <nl> }
static int slic_ioctl ( struct net_device * dev , struct ifreq * rq , int cmd ) <nl> else <nl> duplex = 0 ; <nl> slic_link_config ( adapter , speed , duplex ); <nl> - slic_link_event_handler ( adapter ); <nl> + if ( slic_link_event_handler ( adapter )) <nl> + return - EFAULT ; <nl> } <nl> } <nl> return 0 ;
void tcp_fastopen_add_skb ( struct sock * sk , struct sk_buff * skb ) <nl> tp -> segs_in = 0 ; <nl> tcp_segs_in ( tp , skb ); <nl> __skb_pull ( skb , tcp_hdrlen ( skb )); <nl> + sk_forced_mem_schedule ( sk , skb -> truesize ); <nl> skb_set_owner_r ( skb , sk ); <nl>  <nl> TCP_SKB_CB ( skb )-> seq ++;
restart : <nl> * with multiple processes reclaiming pages , the total <nl> * freeing target can get unreasonably large . <nl> */ <nl> - if ( nr_reclaimed >= nr_to_reclaim && priority < DEF_PRIORITY ) <nl> + if ( nr_reclaimed >= nr_to_reclaim ) <nl> + nr_to_reclaim = 0 ; <nl> + else <nl> + nr_to_reclaim -= nr_reclaimed ; <nl> + <nl> + if (! nr_to_reclaim && priority < DEF_PRIORITY ) <nl> break ; <nl> } <nl> blk_finish_plug (& plug );
static int aspeed_gpio_set_config ( struct gpio_chip * chip , unsigned int offset , <nl> param == PIN_CONFIG_BIAS_PULL_DOWN || <nl> param == PIN_CONFIG_DRIVE_STRENGTH ) <nl> return pinctrl_gpio_set_config ( offset , config ); <nl> + else if ( param == PIN_CONFIG_DRIVE_OPEN_DRAIN || <nl> + param == PIN_CONFIG_DRIVE_OPEN_SOURCE ) <nl> + /* Return - ENOTSUPP to trigger emulation , as per datasheet */ <nl> + return - ENOTSUPP ; <nl>  <nl> return - ENOTSUPP ; <nl> }
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> __le16 fc ; <nl> struct ieee80211_hdr hdr ; <nl> struct ieee80211s_hdr mesh_hdr __maybe_unused ; <nl> - struct mesh_path * mppath = NULL ; <nl> + struct mesh_path __maybe_unused * mppath = NULL ; <nl> const u8 * encaps_data ; <nl> int encaps_len , skip_header_bytes ; <nl> int nh_pos , h_pos ;
static int batadv_tp_send ( void * arg ) <nl> primary_if = batadv_primary_if_get_selected ( bat_priv ); <nl> if ( unlikely (! primary_if )) { <nl> err = BATADV_TP_REASON_DST_UNREACHABLE ; <nl> + tp_vars -> reason = err ; <nl> goto out ; <nl> } <nl> 
static int amd64_check_ecc_enabled ( struct amd64_pvt * pvt ) <nl> " ECC is enabled by BIOS , Proceeding " <nl> " with EDAC module initialization \ n "); <nl>  <nl> + /* Signal good ECC status */ <nl> + ret = 0 ; <nl> + <nl> /* CLEAR the override , since BIOS controlled it */ <nl> ecc_enable_override = 0 ; <nl> }
static int ccw_uevent ( struct device * dev , char ** envp , int num_envp , <nl> snprint_alias ( modalias_buf , sizeof ( modalias_buf ), id , ""); <nl> ret = add_uevent_var ( envp , num_envp , & i , buffer , buffer_size , & len , <nl> " MODALIAS =% s ", modalias_buf ); <nl> - return ret ; <nl> + if ( ret ) <nl> + return ret ; <nl> + envp [ i ] = NULL ; <nl> + return 0 ; <nl> } <nl>  <nl> struct bus_type ccw_bus_type ;
static u32 slic_card_locate ( struct adapter * adapter ) <nl> if (! physcard ) { <nl> /* no structure allocated for this physical card yet */ <nl> physcard = kzalloc ( sizeof ( struct physcard ), GFP_ATOMIC ); <nl> - if (! physcard ) <nl> + if (! physcard ) { <nl> + if ( card_hostid == SLIC_HOSTID_DEFAULT ) <nl> + kfree ( card ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> physcard -> next = slic_global . phys_card ; <nl> slic_global . phys_card = physcard ;
struct ttm_tt * ttm_tt_create ( struct ttm_bo_device * bdev , unsigned long size , <nl> ttm -> dummy_read_page = dummy_read_page ; <nl>  <nl> ttm_tt_alloc_page_directory ( ttm ); <nl> - if (! ttm -> pages ) { <nl> + if (! ttm -> pages || ! ttm -> dma_address ) { <nl> ttm_tt_destroy ( ttm ); <nl> printk ( KERN_ERR TTM_PFX " Failed allocating page table \ n "); <nl> return NULL ;
static int __mlxsw_sp_port_fid_join ( struct mlxsw_sp_port * mlxsw_sp_port , <nl> { <nl> struct mlxsw_sp_fid * f ; <nl>  <nl> + if ( test_bit ( fid , mlxsw_sp_port -> active_vlans )) <nl> + return 0 ; <nl> + <nl> f = mlxsw_sp_fid_find ( mlxsw_sp_port -> mlxsw_sp , fid ); <nl> if (! f ) { <nl> f = mlxsw_sp_fid_create ( mlxsw_sp_port -> mlxsw_sp , fid );
static int pm8001_dev_found_notify ( struct domain_device * dev ) <nl> wait_for_completion (& completion ); <nl> if ( dev -> dev_type == SAS_END_DEV ) <nl> msleep ( 50 ); <nl> - pm8001_ha -> flags = PM8001F_RUN_TIME ; <nl> + pm8001_ha -> flags |= PM8001F_RUN_TIME ; <nl> return 0 ; <nl> found_out : <nl> spin_unlock_irqrestore (& pm8001_ha -> lock , flags );
void ath9k_ps_wakeup ( struct ath_softc * sc ) <nl> spin_lock (& common -> cc_lock ); <nl> ath_hw_cycle_counters_update ( common ); <nl> memset (& common -> cc_survey , 0 , sizeof ( common -> cc_survey )); <nl> + memset (& common -> cc_ani , 0 , sizeof ( common -> cc_ani )); <nl> spin_unlock (& common -> cc_lock ); <nl> } <nl> 
struct flex_array * flex_array_alloc ( int element_size , unsigned int total , <nl> ret -> element_size = element_size ; <nl> ret -> total_nr_elements = total ; <nl> if ( elements_fit_in_base ( ret ) && !( flags & __GFP_ZERO )) <nl> - memset ( ret -> parts [ 0 ], FLEX_ARRAY_FREE , <nl> + memset (& ret -> parts [ 0 ], FLEX_ARRAY_FREE , <nl> FLEX_ARRAY_BASE_BYTES_LEFT ); <nl> return ret ; <nl> }
void radeon_audio_detect ( struct drm_connector * connector , <nl> return ; <nl>  <nl> rdev = connector -> encoder -> dev -> dev_private ; <nl> + <nl> + if (! radeon_audio_chipset_supported ( rdev )) <nl> + return ; <nl> + <nl> radeon_encoder = to_radeon_encoder ( connector -> encoder ); <nl> dig = radeon_encoder -> enc_priv ; <nl> 
int hcd_buffer_create ( struct usb_hcd * hcd ) <nl> char name [ 16 ]; <nl> int i , size ; <nl>  <nl> + if (! hcd -> self . controller -> dma_mask ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < HCD_BUFFER_POOLS ; i ++) { <nl> if (!( size = pool_max [ i ])) <nl> continue ;
struct s3c2410_spigpio { <nl>  <nl> static inline struct s3c2410_spigpio * spidev_to_sg ( struct spi_device * spi ) <nl> { <nl> - return spi -> controller_data ; <nl> + return spi_master_get_devdata ( spi -> master ); <nl> } <nl>  <nl> static inline void setsck ( struct spi_device * dev , int on )
done : <nl>  <nl> out : <nl> if ( unlikely ( frozen_buffer )) /* It ' s usually NULL */ <nl> - kfree ( frozen_buffer ); <nl> + jbd_slab_free ( frozen_buffer , bh -> b_size ); <nl>  <nl> JBUFFER_TRACE ( jh , " exit "); <nl> return error ;
int skb_gro_receive ( struct sk_buff ** head , struct sk_buff * skb ) <nl> * NAPI_GRO_CB ( nskb ) = * NAPI_GRO_CB ( p ); <nl> skb_shinfo ( nskb )-> frag_list = p ; <nl> skb_shinfo ( nskb )-> gso_size = pinfo -> gso_size ; <nl> + pinfo -> gso_size = 0 ; <nl> skb_header_release ( p ); <nl> nskb -> prev = p ; <nl> 
int btrfs_check_trunc_cache_free_space ( struct btrfs_root * root , <nl> else <nl> ret = 0 ; <nl> spin_unlock (& rsv -> lock ); <nl> - return 0 ; <nl> + return ret ; <nl> } <nl>  <nl> int btrfs_truncate_free_space_cache ( struct btrfs_root * root ,
void kvm_exit ( void ) <nl> kvm_arch_hardware_unsetup (); <nl> kvm_arch_exit (); <nl> free_cpumask_var ( cpus_hardware_enabled ); <nl> + __free_page ( fault_page ); <nl> __free_page ( hwpoison_page ); <nl> __free_page ( bad_page ); <nl> }
struct pci_dev * of_create_pci_dev ( struct pci_pbm_info * pbm , <nl> const char * type ; <nl> u32 class ; <nl>  <nl> - dev = kzalloc ( sizeof ( struct pci_dev ), GFP_KERNEL ); <nl> + dev = alloc_pci_dev (); <nl> if (! dev ) <nl> return NULL ; <nl> 
static int e1000_change_mtu ( struct net_device * netdev , int new_mtu ) <nl> struct e1000_adapter * adapter = netdev_priv ( netdev ); <nl> int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN ; <nl>  <nl> - if (( max_frame < ETH_ZLEN + ETH_FCS_LEN ) || <nl> + if (( new_mtu < ETH_ZLEN + ETH_FCS_LEN + VLAN_HLEN ) || <nl> ( max_frame > MAX_JUMBO_FRAME_SIZE )) { <nl> e_err (" Invalid MTU setting \ n "); <nl> return - EINVAL ;
static int _hardware_enqueue ( struct ci_hw_ep * hwep , struct ci_hw_req * hwreq ) <nl> rest -= count ; <nl> } <nl>  <nl> - if ( hwreq -> req . zero && hwreq -> req . length <nl> + if ( hwreq -> req . zero && hwreq -> req . length && hwep -> dir == TX <nl> && ( hwreq -> req . length % hwep -> ep . maxpacket == 0 )) <nl> add_td_to_list ( hwep , hwreq , 0 ); <nl> 
int mei_cl_connect ( struct mei_cl * cl , struct file * file ) <nl> mutex_lock (& dev -> device_lock ); <nl>  <nl> if ( cl -> state != MEI_FILE_CONNECTED ) { <nl> + cl -> state = MEI_FILE_DISCONNECTED ; <nl> /* something went really wrong */ <nl> if (! cl -> status ) <nl> cl -> status = - EFAULT ;
static int stripe_ctr ( struct dm_target * ti , unsigned int argc , char ** argv ) <nl> sc -> stripes_shift = __ffs ( stripes ); <nl>  <nl> r = dm_set_target_max_io_len ( ti , chunk_size ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( sc ); <nl> return r ; <nl> + } <nl>  <nl> ti -> num_flush_bios = stripes ; <nl> ti -> num_discard_bios = stripes ;
static int shmem_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> i_size_write ( inode , newsize ); <nl> inode -> i_ctime = inode -> i_mtime = CURRENT_TIME ; <nl> } <nl> - if ( newsize < oldsize ) { <nl> + if ( newsize <= oldsize ) { <nl> loff_t holebegin = round_up ( newsize , PAGE_SIZE ); <nl> unmap_mapping_range ( inode -> i_mapping , holebegin , 0 , 1 ); <nl> shmem_truncate_range ( inode , newsize , ( loff_t )- 1 );
static int cxgb_extension_ioctl ( struct net_device * dev , void __user * useraddr ) <nl> case CHELSIO_GET_QSET_NUM :{ <nl> struct ch_reg edata ; <nl>  <nl> + memset (& edata , 0 , sizeof ( struct ch_reg )); <nl> + <nl> edata . cmd = CHELSIO_GET_QSET_NUM ; <nl> edata . val = pi -> nqsets ; <nl> if ( copy_to_user ( useraddr , & edata , sizeof ( edata )))
static int rx_intr_handler ( struct ring_info * ring_data , int budget ) <nl> struct RxD1 * rxdp1 ; <nl> struct RxD3 * rxdp3 ; <nl>  <nl> + if ( budget <= 0 ) <nl> + return napi_pkts ; <nl> + <nl> get_info = ring_data -> rx_curr_get_info ; <nl> get_block = get_info . block_index ; <nl> memcpy (& put_info , & ring_data -> rx_curr_put_info , sizeof ( put_info ));
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
static void gen3_init_clock_gating ( struct drm_device * dev ) <nl> dstate |= DSTATE_PLL_D3_OFF | DSTATE_GFX_CLOCK_GATING | <nl> DSTATE_DOT_CLOCK_GATING ; <nl> I915_WRITE ( D_STATE , dstate ); <nl> + <nl> + if ( IS_PINEVIEW ( dev )) <nl> + I915_WRITE ( ECOSKPD , _MASKED_BIT_ENABLE ( ECO_GATING_CX_ONLY )); <nl> } <nl>  <nl> static void i85x_init_clock_gating ( struct drm_device * dev )
static int davinci_spi_setup_transfer ( struct spi_device * spi , <nl> struct davinci_spi * dspi ; <nl> struct davinci_spi_config * spicfg ; <nl> u8 bits_per_word = 0 ; <nl> - u32 hz = 0 , spifmt = 0 , prescale = 0 ; <nl> + u32 hz = 0 , spifmt = 0 ; <nl> + int prescale ; <nl>  <nl> dspi = spi_master_get_devdata ( spi -> master ); <nl> spicfg = ( struct davinci_spi_config *) spi -> controller_data ;
int snd_hda_codec_reset ( struct hda_codec * codec ) <nl> codec -> num_pcms = 0 ; <nl> codec -> pcm_info = NULL ; <nl> codec -> preset = NULL ; <nl> + memset (& codec -> patch_ops , 0 , sizeof ( codec -> patch_ops )); <nl> + codec -> slave_dig_outs = NULL ; <nl> + codec -> spdif_status_reset = 0 ; <nl> module_put ( codec -> owner ); <nl> codec -> owner = NULL ; <nl> 
struct iwl_cfg iwl6000i_2bg_cfg = { <nl> . fw_name_pre = IWL6050_FW_PRE , \ <nl> . ucode_api_max = IWL6050_UCODE_API_MAX , \ <nl> . ucode_api_min = IWL6050_UCODE_API_MIN , \ <nl> + . valid_tx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> + . valid_rx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> . ops = & iwl6050_ops , \ <nl> . eeprom_ver = EEPROM_6050_EEPROM_VERSION , \ <nl> . eeprom_calib_ver = EEPROM_6050_TX_POWER_VERSION , \
static int rt2800_get_gain_calibration_delta ( struct rt2x00_dev * rt2x00dev ) <nl> u8 step ; <nl> int i ; <nl>  <nl> + /* <nl> + * First check if temperature compensation is supported . <nl> + */ <nl> + rt2800_eeprom_read ( rt2x00dev , EEPROM_NIC_CONF1 , & eeprom ); <nl> + if (! rt2x00_get_field16 ( eeprom , EEPROM_NIC_CONF1_EXTERNAL_TX_ALC )) <nl> + return 0 ; <nl> + <nl> /* <nl> * Read TSSI boundaries for temperature compensation from <nl> * the EEPROM .
typedef struct { <nl> uint8_t max_lun ; <nl>  <nl> uint32_t unique_id ; <nl> - uint8_t irq ; <nl> + int irq ; <nl> uint8_t ito ; <nl> caddr_t ibuf ; <nl> dma_addr_t ibuf_dma_h ;
struct device_type usb_device_type = { <nl>  <nl> static int ksuspend_usb_init ( void ) <nl> { <nl> - ksuspend_usb_wq = create_singlethread_workqueue (" ksuspend_usbd "); <nl> + /* This workqueue is supposed to be both freezable and <nl> + * singlethreaded . Its job doesn ' t justify running on more <nl> + * than one CPU . <nl> + */ <nl> + ksuspend_usb_wq = create_freezeable_workqueue (" ksuspend_usbd "); <nl> if (! ksuspend_usb_wq ) <nl> return - ENOMEM ; <nl> return 0 ;
int sirfsoc_uart_probe ( struct platform_device * pdev ) <nl>  <nl> if ( sirfport -> hw_flow_ctrl ) { <nl> sirfport -> p = pinctrl_get_select_default (& pdev -> dev ); <nl> - ret = IS_ERR ( sirfport -> p ); <nl> - if ( ret ) <nl> + if ( IS_ERR ( sirfport -> p )) { <nl> + ret = PTR_ERR ( sirfport -> p ); <nl> goto err ; <nl> + } <nl> } <nl>  <nl> sirfport -> clk = clk_get (& pdev -> dev , NULL );
static struct platform_device sdhi0_device = { <nl> static struct sh_mobile_sdhi_info sdhi1_info = { <nl> . dma_slave_tx = SHDMA_SLAVE_SDHI1_TX , <nl> . dma_slave_rx = SHDMA_SLAVE_SDHI1_RX , <nl> - . tmio_ocr_mask = MMC_VDD_165_195 , <nl> . tmio_flags = TMIO_MMC_WRPROTECT_DISABLE , <nl> . tmio_caps = MMC_CAP_SD_HIGHSPEED | MMC_CAP_SDIO_IRQ | <nl> MMC_CAP_NEEDS_POLL , <nl> static struct resource sh_mmcif_resources [] = { <nl>  <nl> static struct sh_mmcif_plat_data sh_mmcif_plat = { <nl> . sup_pclk = 0 , <nl> - . ocr = MMC_VDD_165_195 | MMC_VDD_32_33 | MMC_VDD_33_34 , <nl> . caps = MMC_CAP_4_BIT_DATA | <nl> MMC_CAP_8_BIT_DATA | <nl> MMC_CAP_NEEDS_POLL ,
static int lx_pipe_wait_for_state ( struct lx6464es * chip , u32 pipe , <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> - if ( current_state == state ) <nl> + if (! err && current_state == state ) <nl> return 0 ; <nl>  <nl> mdelay ( 1 );
int of_pci_range_to_resource ( struct of_pci_range * range , <nl> } <nl> res -> start = port ; <nl> } else { <nl> + if (( sizeof ( resource_size_t ) < 8 ) && <nl> + upper_32_bits ( range -> cpu_addr )) { <nl> + err = - EINVAL ; <nl> + goto invalid_range ; <nl> + } <nl> + <nl> res -> start = range -> cpu_addr ; <nl> } <nl> res -> end = res -> start + range -> size - 1 ;
static void flush_tlb_others_ipi ( const struct cpumask * cpumask , <nl> * We have to send the IPI only to <nl> * CPUs affected . <nl> */ <nl> - send_IPI_mask ( cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl> + send_IPI_mask ( f -> flush_cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl>  <nl> while (! cpumask_empty ( to_cpumask ( f -> flush_cpumask ))) <nl> cpu_relax ();
static void pn_rx_complete ( struct usb_ep * ep , struct usb_request * req ) <nl> } <nl>  <nl> skb_add_rx_frag ( skb , skb_shinfo ( skb )-> nr_frags , page , <nl> - skb -> len == 0 , req -> actual ); <nl> + skb -> len <= 1 , req -> actual ); <nl> page = NULL ; <nl>  <nl> if ( req -> actual < req -> length ) { /* Last fragment */
qla2x00_probe_one ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> sht = & qla2x00_driver_template ; <nl> if ( pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2422 || <nl> - pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 ) <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5422 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5432 ) <nl> sht = & qla24xx_driver_template ; <nl> host = scsi_host_alloc ( sht , sizeof ( scsi_qla_host_t )); <nl> if ( host == NULL ) {
ieee80211_sta_process_chanswitch ( struct ieee80211_sub_if_data * sdata , <nl> case - 1 : <nl> cfg80211_chandef_create (& new_chandef , new_chan , <nl> NL80211_CHAN_NO_HT ); <nl> + /* keep width for 5 / 10 MHz channels */ <nl> + switch ( sdata -> vif . bss_conf . chandef . width ) { <nl> + case NL80211_CHAN_WIDTH_5 : <nl> + case NL80211_CHAN_WIDTH_10 : <nl> + new_chandef . width = sdata -> vif . bss_conf . chandef . width ; <nl> + break ; <nl> + default : <nl> + break ; <nl> + } <nl> break ; <nl> } <nl> 
static bool vgic_update_irq_pending ( struct kvm * kvm , int cpuid , <nl> } else { <nl> vgic_dist_irq_clear_pending ( vcpu , irq_num ); <nl> } <nl> + <nl> + ret = false ; <nl> + goto out ; <nl> } <nl>  <nl> enabled = vgic_irq_is_enabled ( vcpu , irq_num );
static int fs_open ( struct atm_vcc * atm_vcc ) <nl> /* Docs are vague about this atm_hdr field . By the way , the FS <nl> * chip makes odd errors if lower bits are set .... -- REW */ <nl> tc -> atm_hdr = ( vpi << 20 ) | ( vci << 4 ); <nl> + tmc0 = 0 ; <nl> { <nl> int pcr = atm_pcr_goal ( txtp ); <nl> 
relookup : <nl> secure_ipv6_id ( daddr -> addr . a6 )); <nl> p -> metrics [ RTAX_LOCK - 1 ] = INETPEER_METRICS_NEW ; <nl> p -> rate_tokens = 0 ; <nl> - p -> rate_last = 0 ; <nl> + /* 60 * HZ is arbitrary , but chosen enough high so that the first <nl> + * calculation of tokens is at its maximum . <nl> + */ <nl> + p -> rate_last = jiffies - 60 * HZ ; <nl> INIT_LIST_HEAD (& p -> gc_list ); <nl>  <nl> /* Link the node . */
static bool shadow_walk_okay ( struct kvm_shadow_walk_iterator * iterator ) <nl> { <nl> if ( iterator -> level < PT_PAGE_TABLE_LEVEL ) <nl> return false ; <nl> + <nl> + if ( iterator -> level == PT_PAGE_TABLE_LEVEL ) <nl> + if ( is_large_pte (* iterator -> sptep )) <nl> + return false ; <nl> + <nl> iterator -> index = SHADOW_PT_INDEX ( iterator -> addr , iterator -> level ); <nl> iterator -> sptep = (( u64 *) __va ( iterator -> shadow_addr )) + iterator -> index ; <nl> return true ;
struct dvb_frontend * tda829x_attach ( struct dvb_frontend * fe , <nl> } <nl>  <nl> if ((!( cfg ) || ( TDA829X_PROBE_TUNER == cfg -> probe_tuner )) && <nl> - ( tda829x_find_tuner ( fe ) < 0 )) <nl> + ( tda829x_find_tuner ( fe ) < 0 )) { <nl> + memset (& fe -> ops . analog_ops , 0 , sizeof ( struct analog_demod_ops )); <nl> + <nl> goto fail ; <nl> + } <nl>  <nl> switch ( priv -> ver ) { <nl> case TDA8290 :
static bool is_zone_first_populated ( pg_data_t * pgdat , struct zone * zone ) <nl> return zone == compare ; <nl> } <nl>  <nl> - /* The zone must be somewhere ! */ <nl> - WARN_ON_ONCE ( 1 ); <nl> return false ; <nl> } <nl> 
static void ext4_free_data ( handle_t * handle , struct inode * inode , <nl> * block pointed to itself , it would have been detached when <nl> * the block was cleared . Check for this instead of OOPSing . <nl> */ <nl> - if ( bh2jh ( this_bh )) <nl> + if (( EXT4_JOURNAL ( inode ) == NULL ) || bh2jh ( this_bh )) <nl> ext4_handle_dirty_metadata ( handle , inode , this_bh ); <nl> else <nl> ext4_error ( inode -> i_sb , __func__ ,
static void clk_pllv2_unprepare ( struct clk_hw * hw ) <nl> __raw_writel ( reg , pllbase + MXC_PLL_DP_CTL ); <nl> } <nl>  <nl> - struct clk_ops clk_pllv2_ops = { <nl> + static struct clk_ops clk_pllv2_ops = { <nl> . prepare = clk_pllv2_prepare , <nl> . unprepare = clk_pllv2_unprepare , <nl> . recalc_rate = clk_pllv2_recalc_rate ,
static struct async * alloc_async ( unsigned int numisoframes ) <nl> static void free_async ( struct async * as ) <nl> { <nl> put_pid ( as -> pid ); <nl> - put_cred ( as -> cred ); <nl> + if ( as -> cred ) <nl> + put_cred ( as -> cred ); <nl> kfree ( as -> urb -> transfer_buffer ); <nl> kfree ( as -> urb -> setup_packet ); <nl> usb_free_urb ( as -> urb );
static int __devexit powernowk8_cpu_exit ( struct cpufreq_policy * pol ) <nl>  <nl> static unsigned int powernowk8_get ( unsigned int cpu ) <nl> { <nl> - struct powernow_k8_data * data ; <nl> + struct powernow_k8_data * data = per_cpu ( powernow_data , cpu ); <nl> cpumask_t oldmask = current -> cpus_allowed ; <nl> unsigned int khz = 0 ; <nl> - unsigned int first ; <nl> - <nl> - first = cpumask_first ( cpu_core_mask ( cpu )); <nl> - data = per_cpu ( powernow_data , first ); <nl>  <nl> if (! data ) <nl> return - EINVAL ;
struct sk_buff * __skb_recv_datagram ( struct sock * sk , unsigned int flags , <nl> skb_queue_walk ( queue , skb ) { <nl> * peeked = skb -> peeked ; <nl> if ( flags & MSG_PEEK ) { <nl> - if (* off >= skb -> len && skb -> len ) { <nl> + if (* off >= skb -> len && ( skb -> len || * off || <nl> + skb -> peeked )) { <nl> * off -= skb -> len ; <nl> continue ; <nl> }
static void ui_browser__hists_seek ( struct ui_browser * browser , <nl> * and stop when we printed enough lines to fill the screen . <nl> */ <nl> do_offset : <nl> + if (! nd ) <nl> + return ; <nl> + <nl> if ( offset > 0 ) { <nl> do { <nl> h = rb_entry ( nd , struct hist_entry , rb_node );
extern int blk_iopoll_enabled ; <nl> static int sixty = 60 ; <nl> # endif <nl>  <nl> + static int neg_one = - 1 ; <nl> static int zero ; <nl> static int __maybe_unused one = 1 ; <nl> static int __maybe_unused two = 2 ;
tracing_buffers_splice_read ( struct file * file , loff_t * ppos , <nl> if (! ref ) <nl> break ; <nl>  <nl> + ref -> ref = 1 ; <nl> ref -> buffer = info -> tr -> buffer ; <nl> ref -> page = ring_buffer_alloc_read_page ( ref -> buffer ); <nl> if (! ref -> page ) {
static int gb_uart_connection_init ( struct gb_connection * connection ) <nl> if ( minor == - ENOSPC ) { <nl> dev_err (& connection -> dev , <nl> " no more free minor numbers \ n "); <nl> - return - ENODEV ; <nl> + retval = - ENODEV ; <nl> + goto error_version ; <nl> } <nl> - return minor ; <nl> + retval = minor ; <nl> + goto error_version ; <nl> } <nl>  <nl> gb_tty -> minor = minor ;
static int __devinit snd_ca0106_probe ( struct pci_dev * pci , <nl> snd_ca0106_proc_init ( chip ); <nl> # endif <nl>  <nl> + snd_card_set_dev ( card , & pci -> dev ); <nl> + <nl> if (( err = snd_card_register ( card )) < 0 ) { <nl> snd_card_free ( card ); <nl> return err ;
static void do_checkpoint ( struct f2fs_sb_info * sbi , bool is_umount ) <nl> /* Here , we only have one bio having CP pack */ <nl> sync_meta_pages ( sbi , META_FLUSH , LONG_MAX ); <nl>  <nl> - if ( unlikely (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG ))) { <nl> + if (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG )) { <nl> clear_prefree_segments ( sbi ); <nl> release_dirty_inode ( sbi ); <nl> F2FS_RESET_SB_DIRT ( sbi );
static int read_exceptions ( struct pstore * ps , <nl> r = insert_exceptions ( ps , area , callback , callback_context , <nl> & full ); <nl>  <nl> + if (! full ) <nl> + memcpy ( ps -> area , area , ps -> store -> chunk_size << SECTOR_SHIFT ); <nl> + <nl> dm_bufio_release ( bp ); <nl>  <nl> dm_bufio_forget ( client , chunk );
struct usb_tt { <nl> struct usb_device * hub ; /* upstream highspeed hub */ <nl> int multi ; /* true means one TT per port */ <nl> unsigned think_time ; /* think time in ns */ <nl> + void * hcpriv ; /* HCD private data */ <nl>  <nl> /* for control / bulk error recovery ( CLEAR_TT_BUFFER ) */ <nl> spinlock_t lock ;
static void pic_clear_isr ( struct kvm_kpic_state * s , int irq ) <nl> void kvm_pic_clear_isr_ack ( struct kvm * kvm ) <nl> { <nl> struct kvm_pic * s = pic_irqchip ( kvm ); <nl> + pic_lock ( s ); <nl> s -> pics [ 0 ]. isr_ack = 0xff ; <nl> s -> pics [ 1 ]. isr_ack = 0xff ; <nl> + pic_unlock ( s ); <nl> } <nl>  <nl> /*
radeon_atom_encoder_dpms_dig ( struct drm_encoder * encoder , int mode ) <nl> * does the same thing and more . <nl> */ <nl> if (( rdev -> family != CHIP_RV710 ) && ( rdev -> family != CHIP_RV730 ) && <nl> - ( rdev -> family != CHIP_RS880 )) <nl> + ( rdev -> family != CHIP_RS780 ) && ( rdev -> family != CHIP_RS880 )) <nl> atombios_dig_transmitter_setup ( encoder , ATOM_TRANSMITTER_ACTION_ENABLE_OUTPUT , 0 , 0 ); <nl> } <nl> if ( ENCODER_MODE_IS_DP ( atombios_get_encoder_mode ( encoder )) && connector ) {
static void das16_interrupt ( struct comedi_device * dev ) <nl> cfc_write_array_to_buffer ( s , <nl> devpriv -> dma_buffer [ buffer_index ], num_bytes ); <nl>  <nl> - cfc_handle_events ( dev , s ); <nl> + comedi_handle_events ( dev , s ); <nl> } <nl>  <nl> static void das16_timer_interrupt ( unsigned long arg )
static void setup_rt_frame ( int sig , struct k_sigaction * ka , siginfo_t * info , <nl>  <nl> set_fs ( USER_DS ); <nl>  <nl> + /* the tracer may want to single - step inside the handler */ <nl> + if ( test_thread_flag ( TIF_SINGLESTEP )) <nl> + ptrace_notify ( SIGTRAP ); <nl> + <nl> # ifdef DEBUG_SIG <nl> printk ( KERN_INFO " SIG deliver (% s :% d ): sp =% p pc =% 08lx \ n ", <nl> current -> comm , current -> pid , frame , regs -> pc );
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl> ! iwlwifi_mod_params . sw_crypto ) <nl> hw -> flags |= IEEE80211_HW_MFP_CAPABLE ; <nl>  <nl> - if ( mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> + if ( 0 && mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> hw -> flags |= IEEE80211_HW_SUPPORTS_UAPSD ; <nl> hw -> uapsd_queues = IWL_UAPSD_AC_INFO ; <nl> hw -> uapsd_max_sp_len = IWL_UAPSD_MAX_SP ;
int qed_resc_alloc ( struct qed_dev * cdev ) <nl> DP_ERR ( p_hwfn , <nl> " Cannot allocate 0x % x EQ elements . The maximum of a u16 chain is 0x % x \ n ", <nl> n_eqes , 0xFFFF ); <nl> + rc = - EINVAL ; <nl> goto alloc_err ; <nl> } <nl> 
static void taal_esd_work ( struct work_struct * work ) <nl> } <nl> /* Self - diagnostics result is also shown on TE GPIO line . We need <nl> * to re - enable TE after self diagnostics */ <nl> - if ( td -> use_ext_te && td -> te_enabled ) <nl> - taal_enable_te ( dssdev , true ); <nl> + if ( td -> use_ext_te && td -> te_enabled ) { <nl> + r = taal_dcs_write_1 ( DCS_TEAR_ON , 0 ); <nl> + if ( r ) <nl> + goto err ; <nl> + } <nl>  <nl> dsi_bus_unlock (); <nl> 
int dlm_posix_lock ( dlm_lockspace_t * lockspace , u64 number , struct file * file , <nl> send_op ( op ); <nl>  <nl> if ( xop -> callback == NULL ) { <nl> - rv = wait_event_killable ( recv_wq , ( op -> done != 0 )); <nl> + rv = wait_event_interruptible ( recv_wq , ( op -> done != 0 )); <nl> if ( rv == - ERESTARTSYS ) { <nl> log_debug ( ls , " dlm_posix_lock : wait killed % llx ", <nl> ( unsigned long long ) number );
static int ide_diag_taskfile ( ide_drive_t * drive , ide_task_t * args , unsigned long <nl> struct request rq ; <nl>  <nl> memset (& rq , 0 , sizeof ( rq )); <nl> + rq . ref_count = 1 ; <nl> rq . cmd_type = REQ_TYPE_ATA_TASKFILE ; <nl> rq . buffer = buf ; <nl> 
void __kvm_migrate_pit_timer ( struct kvm_vcpu * vcpu ) <nl> return ; <nl>  <nl> timer = & pit -> pit_state . timer ; <nl> + mutex_lock (& pit -> pit_state . lock ); <nl> if ( hrtimer_cancel ( timer )) <nl> hrtimer_start_expires ( timer , HRTIMER_MODE_ABS ); <nl> + mutex_unlock (& pit -> pit_state . lock ); <nl> } <nl>  <nl> static void destroy_pit_timer ( struct kvm_pit * pit )
skl_ddi_pll_select ( struct intel_crtc * intel_crtc , <nl> DPLL_CFGCR2_KDIV ( wrpll_params . kdiv ) | <nl> DPLL_CFGCR2_PDIV ( wrpll_params . pdiv ) | <nl> wrpll_params . central_freq ; <nl> - } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT ) { <nl> + } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT || <nl> + intel_encoder -> type == INTEL_OUTPUT_DP_MST ) { <nl> switch ( crtc_state -> port_clock / 2 ) { <nl> case 81000 : <nl> ctrl1 |= DPLL_CTRL1_LINK_RATE ( DPLL_CTRL1_LINK_RATE_810 , 0 );
static int imx_es8328_dai_init ( struct snd_soc_pcm_runtime * rtd ) <nl>  <nl> /* Headphone jack detection */ <nl> if ( gpio_is_valid ( data -> jack_gpio )) { <nl> - ret = snd_soc_jack_new ( rtd -> codec , " Headphone ", <nl> - SND_JACK_HEADPHONE | SND_JACK_BTN_0 , <nl> - & headset_jack ); <nl> + ret = snd_soc_card_jack_new ( rtd -> card , " Headphone ", <nl> + SND_JACK_HEADPHONE | SND_JACK_BTN_0 , <nl> + & headset_jack , NULL , 0 ); <nl> if ( ret ) <nl> return ret ; <nl> 
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - counter_config = kmalloc ( sizeof ( struct op_counter_config ) * spec -> num_counters , <nl> + counter_config = kcalloc ( spec -> num_counters , sizeof ( struct op_counter_config ), <nl> GFP_KERNEL ); <nl> if (! counter_config ) <nl> return - ENOMEM ;
static void belkin_sa_set_termios ( struct usb_serial_port * port , struct ktermios <nl> } <nl>  <nl> baud = tty_get_baud_rate ( port -> tty ); <nl> + if ( baud == 0 ) { <nl> + dbg ("% s - tty_get_baud_rate says 0 baud ", __FUNCTION__ ); <nl> + return ; <nl> + } <nl> urb_value = BELKIN_SA_BAUD ( baud ); <nl> /* Clip to maximum speed */ <nl> if ( urb_value == 0 )
get_pll_register ( struct drm_device * dev , enum pll_types type ) <nl> else { <nl> u8 * plim = & bios -> data [ bios -> pll_limit_tbl_ptr ]; <nl>  <nl> - if ( plim [ 0 ] >= 0x40 ) { <nl> + if ( plim [ 0 ] >= 0x30 ) { <nl> u8 * entry = plim + plim [ 1 ]; <nl> for ( i = 0 ; i < plim [ 3 ]; i ++, entry += plim [ 2 ]) { <nl> if ( entry [ 0 ] == type )
int map_groups__clone ( struct map_groups * mg , <nl> if ( new == NULL ) <nl> goto out_unlock ; <nl> map_groups__insert ( mg , new ); <nl> + map__put ( new ); <nl> } <nl>  <nl> err = 0 ;
int __init init_hw_breakpoint ( void ) <nl> err_alloc : <nl> for_each_possible_cpu ( err_cpu ) { <nl> for ( i = 0 ; i < TYPE_MAX ; i ++) <nl> - kfree ( per_cpu ( nr_task_bp_pinned [ i ], cpu )); <nl> + kfree ( per_cpu ( nr_task_bp_pinned [ i ], err_cpu )); <nl> if ( err_cpu == cpu ) <nl> break ; <nl> }
static struct kvmppc_linear_info * kvm_alloc_linear ( int type ) <nl> break ; <nl> } <nl> spin_unlock (& linear_lock ); <nl> + memset ( ri -> base_virt , 0 , ri -> npages << PAGE_SHIFT ); <nl> return ri ; <nl> } <nl> 
static int receive_DataRequest ( struct drbd_conf * mdev , enum drbd_packets cmd , un <nl> " no local data .\ n "); <nl> drbd_send_ack_rp ( mdev , cmd == P_DATA_REQUEST ? P_NEG_DREPLY : <nl> P_NEG_RS_DREPLY , p ); <nl> - return TRUE ; <nl> + /* drain possibly payload */ <nl> + return drbd_drain_block ( mdev , digest_size ); <nl> } <nl>  <nl> /* GFP_NOIO , because we must not cause arbitrary write - out : in a DRBD
mwifiex_wmm_get_highest_priolist_ptr ( struct mwifiex_adapter * adapter , <nl> list_for_each_entry ( ptr , & tid_ptr -> ra_list , <nl> list ) { <nl>  <nl> - if (! skb_queue_empty (& ptr -> skb_head )) <nl> + if (! ptr -> tx_paused && <nl> + ! skb_queue_empty (& ptr -> skb_head )) <nl> /* holds both locks */ <nl> goto found ; <nl> }
struct fib6_node * fib6_lookup ( struct fib6_node * root , struct in6_addr * daddr , <nl> } <nl> }; <nl>  <nl> - fn = fib6_lookup_1 ( root , args ); <nl> + fn = fib6_lookup_1 ( root , daddr ? args : args + 1 ); <nl>  <nl> if ( fn == NULL || fn -> fn_flags & RTN_TL_ROOT ) <nl> fn = root ;
static int w1_f29_add_slave ( struct w1_slave * sl ) <nl> static void w1_f29_remove_slave ( struct w1_slave * sl ) <nl> { <nl> int i ; <nl> - for ( i = NB_SYSFS_BIN_FILES ; i <= 0 ; -- i ) <nl> + for ( i = NB_SYSFS_BIN_FILES - 1 ; i >= 0 ; -- i ) <nl> sysfs_remove_bin_file (& sl -> dev . kobj , <nl> &( w1_f29_sysfs_bin_files [ i ])); <nl> }
static int btrfs_finish_ordered_io ( struct btrfs_ordered_extent * ordered_extent ) <nl> EXTENT_DEFRAG , 1 , cached_state ); <nl> if ( ret ) { <nl> u64 last_snapshot = btrfs_root_last_snapshot (& root -> root_item ); <nl> - if ( last_snapshot >= BTRFS_I ( inode )-> generation ) <nl> + if ( 0 && last_snapshot >= BTRFS_I ( inode )-> generation ) <nl> /* the inode is shared */ <nl> new = record_old_file_extents ( inode , ordered_extent ); <nl> 
static void intel_find_plane_obj ( struct intel_crtc * intel_crtc , <nl> return ; <nl>  <nl> kfree ( intel_crtc -> base . fb ); <nl> + intel_crtc -> base . fb = NULL ; <nl>  <nl> /* <nl> * Failed to alloc the obj , check to see if we should share
void cfg80211_conn_work ( struct work_struct * work ) <nl> wdev_unlock ( wdev ); <nl> continue ; <nl> } <nl> - if ( wdev -> sme_state != CFG80211_SME_CONNECTING ) { <nl> + if ( wdev -> sme_state != CFG80211_SME_CONNECTING || ! wdev -> conn ) { <nl> wdev_unlock ( wdev ); <nl> continue ; <nl> }
static struct wiphy * wlan_create_wiphy ( struct device * dev , wlandevice_t * wlandev <nl> wiphy -> n_cipher_suites = PRISM2_NUM_CIPHER_SUITES ; <nl> wiphy -> cipher_suites = prism2_cipher_suites ; <nl>  <nl> - if ( wiphy_register ( wiphy ) < 0 ) <nl> + if ( wiphy_register ( wiphy ) < 0 ) { <nl> + wiphy_free ( wiphy ); <nl> return NULL ; <nl> + } <nl>  <nl> return wiphy ; <nl> }
void __init tegra_add_of_provider ( struct device_node * np ) <nl> of_clk_add_provider ( np , of_clk_src_onecell_get , & clk_data ); <nl>  <nl> rst_ctlr . of_node = np ; <nl> - rst_ctlr . nr_resets = clk_num * 32 ; <nl> + rst_ctlr . nr_resets = periph_banks * 32 ; <nl> reset_controller_register (& rst_ctlr ); <nl> } <nl> 
static inline void print_ipv6_addr ( struct audit_buffer * ab , <nl> char * name1 , char * name2 ) <nl> { <nl> if (! ipv6_addr_any ( addr )) <nl> - audit_log_format ( ab , " % s =% pI6 ", name1 , addr ); <nl> + audit_log_format ( ab , " % s =% pI6c ", name1 , addr ); <nl> if ( port ) <nl> audit_log_format ( ab , " % s =% d ", name2 , ntohs ( port )); <nl> }
static int move_tasks ( struct rq * this_rq , int this_cpu , struct rq * busiest , <nl> max_load_move - total_load_moved , <nl> sd , idle , all_pinned , & this_best_prio ); <nl> class = class -> next ; <nl> + <nl> + if ( idle == CPU_NEWLY_IDLE && this_rq -> nr_running ) <nl> + break ; <nl> + <nl> } while ( class && max_load_move > total_load_moved ); <nl>  <nl> return total_load_moved > 0 ;
cntrlEnd : <nl>  <nl> BCM_DEBUG_PRINT ( Adapter , DBG_TYPE_OTHERS , OSAL_DBG , DBG_LVL_ALL , " Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO \ n "); <nl>  <nl> + memset (& DevInfo , 0 , sizeof ( DevInfo )); <nl> DevInfo . MaxRDMBufferSize = BUFFER_4K ; <nl> DevInfo . u32DSDStartOffset = EEPROM_CALPARAM_START ; <nl> DevInfo . u32RxAlignmentCorrection = 0 ;
int t4vf_wr_mbox_core ( struct adapter * adapter , const void * cmd , int size , <nl> delay_idx = 0 ; <nl> ms = delay [ 0 ]; <nl>  <nl> - for ( i = 0 ; i < 500 ; i += ms ) { <nl> + for ( i = 0 ; i < FW_CMD_MAX_TIMEOUT ; i += ms ) { <nl> if ( sleep_ok ) { <nl> ms = delay [ delay_idx ]; <nl> if ( delay_idx < ARRAY_SIZE ( delay ) - 1 )
static void rtl8169_init_phy ( struct net_device * dev , struct rtl8169_private * tp ) <nl> rtl8169_set_speed ( dev , AUTONEG_ENABLE , SPEED_1000 , DUPLEX_FULL , <nl> ADVERTISED_10baseT_Half | ADVERTISED_10baseT_Full | <nl> ADVERTISED_100baseT_Half | ADVERTISED_100baseT_Full | <nl> - tp -> mii . supports_gmii ? <nl> + ( tp -> mii . supports_gmii ? <nl> ADVERTISED_1000baseT_Half | <nl> - ADVERTISED_1000baseT_Full : 0 ); <nl> + ADVERTISED_1000baseT_Full : 0 )); <nl>  <nl> if ( RTL_R8 ( PHYstatus ) & TBI_Enable ) <nl> netif_info ( tp , link , dev , " TBI auto - negotiating \ n ");
int cdc_ncm_bind_common ( struct usbnet * dev , struct usb_interface * intf , u8 data_ <nl> u8 iface_no ; <nl>  <nl> ctx = kzalloc ( sizeof (* ctx ), GFP_KERNEL ); <nl> - if ( ctx == NULL ) <nl> - return - ENODEV ; <nl> + if (! ctx ) <nl> + return - ENOMEM ; <nl>  <nl> hrtimer_init (& ctx -> tx_timer , CLOCK_MONOTONIC , HRTIMER_MODE_REL ); <nl> ctx -> tx_timer . function = & cdc_ncm_tx_timer_cb ;
struct img_hash_request_ctx { <nl> unsigned long op ; <nl>  <nl> size_t bufcnt ; <nl> - u8 buffer [ 0 ] __aligned ( sizeof ( u32 )); <nl> struct ahash_request fallback_req ; <nl> + <nl> + /* Zero length buffer must remain last member of struct */ <nl> + u8 buffer [ 0 ] __aligned ( sizeof ( u32 )); <nl> }; <nl>  <nl> struct img_hash_ctx {
static int ucode_init ( loader_block * lb , amb_dev * dev ) <nl> const struct firmware * fw ; <nl> unsigned long start_address ; <nl> const struct ihex_binrec * rec ; <nl> - const char * errmsg = 0 ; <nl> + const char * errmsg = NULL ; <nl> int res ; <nl>  <nl> res = request_ihex_firmware (& fw , " atmsar11 . fw ", & dev -> pci_dev -> dev );
v9fs_mux_rpc ( struct v9fs_mux_data * m , struct v9fs_fcall * tc , <nl> r . rcall || r . err ); <nl> } while (! r . rcall && ! r . err && err ==- ERESTARTSYS && <nl> m -> trans -> status == Connected && ! m -> err ); <nl> + <nl> + err = - ERESTARTSYS ; <nl> } <nl> sigpending = 1 ; <nl> }
static int tmio_probe ( struct platform_device * dev ) <nl> nand_chip = & tmio -> chip ; <nl> mtd -> priv = nand_chip ; <nl> mtd -> name = " tmio - nand "; <nl> + mtd -> dev . parent = & dev -> dev ; <nl>  <nl> tmio -> ccr = devm_ioremap (& dev -> dev , ccr -> start , resource_size ( ccr )); <nl> if (! tmio -> ccr )
static void * raid0_takeover_raid1 ( mddev_t * mddev ) <nl> mddev -> new_layout = 0 ; <nl> mddev -> new_chunk_sectors = 128 ; /* by default set chunk size to 64k */ <nl> mddev -> delta_disks = 1 - mddev -> raid_disks ; <nl> + mddev -> raid_disks = 1 ; <nl> /* make sure it will be not marked as dirty */ <nl> mddev -> recovery_cp = MaxSector ; <nl> 
TRACE_EVENT ( task_rename , <nl> TP_fast_assign ( <nl> __entry -> pid = task -> pid ; <nl> memcpy ( entry -> oldcomm , task -> comm , TASK_COMM_LEN ); <nl> - memcpy ( entry -> newcomm , comm , TASK_COMM_LEN ); <nl> + strlcpy ( entry -> newcomm , comm , TASK_COMM_LEN ); <nl> __entry -> oom_score_adj = task -> signal -> oom_score_adj ; <nl> ), <nl> 
static int __devinit bfin_lq035_probe ( struct platform_device * pdev ) <nl> i2c_add_driver (& ad5280_driver ); <nl>  <nl> memset (& props , 0 , sizeof ( props )); <nl> + props . type = BACKLIGHT_RAW ; <nl> props . max_brightness = MAX_BRIGHENESS ; <nl> bl_dev = backlight_device_register (" bf537 - bl ", NULL , NULL , <nl> & bfin_lq035fb_bl_ops , & props );
process_filter ( struct event_format * event , struct filter_arg ** parg , <nl> * parg = current_op ; <nl> else <nl> * parg = current_exp ; <nl> + free ( token ); <nl> return PEVENT_ERRNO__UNBALANCED_PAREN ; <nl> } <nl> break ; <nl> process_filter ( struct event_format * event , struct filter_arg ** parg , <nl>  <nl> * parg = current_op ; <nl>  <nl> + free ( token ); <nl> return 0 ; <nl>  <nl> fail_alloc :
static ssize_t tun_chr_aio_read ( struct kiocb * iocb , const struct iovec * iv , <nl> ret = tun_do_read ( tun , tfile , iocb , iv , len , <nl> file -> f_flags & O_NONBLOCK ); <nl> ret = min_t ( ssize_t , ret , len ); <nl> + if ( ret > 0 ) <nl> + iocb -> ki_pos = ret ; <nl> out : <nl> tun_put ( tun ); <nl> return ret ;
trace_print_graph_duration ( unsigned long long duration , struct trace_seq * s ) <nl>  <nl> /* Print nsecs ( we don ' t want to exceed 7 numbers ) */ <nl> if ( len < 7 ) { <nl> - snprintf ( nsecs_str , 8 - len , "% 03lu ", nsecs_rem ); <nl> + snprintf ( nsecs_str , min ( sizeof ( nsecs_str ), 8UL - len ), "% 03lu ", <nl> + nsecs_rem ); <nl> ret = trace_seq_printf ( s , ".% s ", nsecs_str ); <nl> if (! ret ) <nl> return TRACE_TYPE_PARTIAL_LINE ;
static ssize_t display_upd_mode_store ( struct device * dev , <nl> int val , r ; <nl> enum omap_dss_update_mode mode ; <nl>  <nl> + if (! dssdev -> driver -> set_update_mode ) <nl> + return - EINVAL ; <nl> + <nl> val = simple_strtoul ( buf , NULL , 10 ); <nl>  <nl> switch ( val ) {
int serial8250_em485_init ( struct uart_8250_port * p ) <nl> if ( p -> em485 != NULL ) <nl> return 0 ; <nl>  <nl> - p -> em485 = kmalloc ( sizeof ( struct uart_8250_em485 ), GFP_KERNEL ); <nl> + p -> em485 = kmalloc ( sizeof ( struct uart_8250_em485 ), GFP_ATOMIC ); <nl> if ( p -> em485 == NULL ) <nl> return - ENOMEM ; <nl> 
ibx_get_dpll ( struct intel_crtc * crtc , struct intel_crtc_state * crtc_state , <nl> DPLL_ID_PCH_PLL_B ); <nl> } <nl>  <nl> + if (! pll ) <nl> + return NULL ; <nl> + <nl> /* reference the pll */ <nl> intel_reference_shared_dpll ( pll , crtc_state ); <nl> 
static int mxs_gpio_set_wake_irq ( u32 irq , u32 enable ) <nl> } <nl>  <nl> static struct irq_chip gpio_irq_chip = { <nl> + . name = " mxs gpio ", <nl> . ack = mxs_gpio_ack_irq , <nl> . mask = mxs_gpio_mask_irq , <nl> . unmask = mxs_gpio_unmask_irq ,
static int s5m87xx_i2c_probe ( struct i2c_client * i2c , <nl> s5m87xx -> rtc = i2c_new_dummy ( i2c -> adapter , RTC_I2C_ADDR ); <nl> i2c_set_clientdata ( s5m87xx -> rtc , s5m87xx ); <nl>  <nl> - if ( pdata -> cfg_pmic_irq ) <nl> + if ( pdata && pdata -> cfg_pmic_irq ) <nl> pdata -> cfg_pmic_irq (); <nl>  <nl> s5m_irq_init ( s5m87xx );
static int xc5000_release ( struct dvb_frontend * fe ) <nl>  <nl> if ( priv ) { <nl> cancel_delayed_work (& priv -> timer_sleep ); <nl> - hybrid_tuner_release_state ( priv ); <nl> if ( priv -> firmware ) <nl> release_firmware ( priv -> firmware ); <nl> + hybrid_tuner_release_state ( priv ); <nl> } <nl>  <nl> mutex_unlock (& xc5000_list_mutex );
static void p54_pspoll_workaround ( struct p54_common * priv , struct sk_buff * skb ) <nl> return ; <nl>  <nl> /* only consider beacons from the associated BSSID */ <nl> - if (! ether_addr_equal ( hdr -> addr3 , priv -> bssid )) <nl> + if (! ether_addr_equal_64bits ( hdr -> addr3 , priv -> bssid )) <nl> return ; <nl>  <nl> tim = p54_find_ie ( skb , WLAN_EID_TIM );
int invalidate_inode_pages2_range ( struct address_space * mapping , <nl> pagevec_release (& pvec ); <nl> cond_resched (); <nl> } <nl> + WARN_ON_ONCE ( ret ); <nl> return ret ; <nl> } <nl> EXPORT_SYMBOL_GPL ( invalidate_inode_pages2_range );
static int hid_scan_report ( struct hid_device * hid ) <nl> item . type == HID_ITEM_TYPE_MAIN && <nl> item . tag == HID_MAIN_ITEM_TAG_BEGIN_COLLECTION && <nl> ( item_udata (& item ) & 0xff ) == HID_COLLECTION_PHYSICAL && <nl> - hid -> bus == BUS_USB ) <nl> + ( hid -> bus == BUS_USB || hid -> bus == BUS_I2C )) <nl> hid -> group = HID_GROUP_SENSOR_HUB ; <nl> } <nl> 
static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
snd_m3_enable_ints ( struct snd_m3 * chip ) <nl> val = ASSP_INT_ENABLE /*| MPU401_INT_ENABLE */; <nl> if ( chip -> hv_config & HV_CTRL_ENABLE ) <nl> val |= HV_INT_ENABLE ; <nl> + outb ( val , chip -> iobase + HOST_INT_STATUS ); <nl> outw ( val , io + HOST_INT_CTRL ); <nl> outb ( inb ( io + ASSP_CONTROL_C ) | ASSP_HOST_INT_ENABLE , <nl> io + ASSP_CONTROL_C );
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> loff_t file_size ; <nl> unsigned int num ; <nl> unsigned int offset ; <nl> - size_t total_len ; <nl> + size_t total_len = 0 ; <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req ))
static int stmmac_rx ( struct stmmac_priv * priv , int limit ) <nl>  <nl> frame_len = priv -> hw -> desc -> get_rx_frame_len ( p , coe ); <nl>  <nl> + /* check if frame_len fits the preallocated memory */ <nl> + if ( frame_len > priv -> dma_buf_sz ) { <nl> + priv -> dev -> stats . rx_length_errors ++; <nl> + break ; <nl> + } <nl> + <nl> /* ACS is set ; GMAC core strips PAD / FCS for IEEE 802 . 3 <nl> * Type frames ( LLC / LLC - SNAP ) <nl> */
static int pm8001_chip_sata_req ( struct pm8001_hba_info * pm8001_ha , <nl>  <nl> /* Check for read log for failed drive and return */ <nl> if ( sata_cmd . sata_fis . command == 0x2f ) { <nl> - if ( pm8001_ha_dev && (( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> + if ((( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_ABORT_ALL_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_2ND_RLE_FLAG ))) { <nl> struct task_status_struct * ts ;
static int read_bus_info_block ( struct fw_device * device , int generation ) <nl> return - ENOMEM ; <nl>  <nl> stack = & rom [ READ_BIB_ROM_SIZE ]; <nl> + memset ( rom , 0 , sizeof (* rom ) * READ_BIB_ROM_SIZE ); <nl>  <nl> device -> max_speed = SCODE_100 ; <nl> 
static unsigned long nid_range ( unsigned long start , unsigned long end , <nl> start += PAGE_SIZE ; <nl> } <nl>  <nl> + if ( start > end ) <nl> + start = end ; <nl> + <nl> return start ; <nl> } <nl> # else
static void _rtl_usb_tx_preprocess ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> u8 tid = 0 ; <nl> u16 seq_number = 0 ; <nl>  <nl> + memset (& tcb_desc , 0 , sizeof ( struct rtl_tcb_desc )); <nl> if ( ieee80211_is_auth ( fc )) { <nl> RT_TRACE ( rtlpriv , COMP_SEND , DBG_DMESG , (" MAC80211_LINKING \ n ")); <nl> rtl_ips_nic_on ( hw );
static int iwlagn_mac_sta_add ( struct ieee80211_hw * hw , <nl> { <nl> struct iwl_priv * priv = hw -> priv ; <nl> struct iwl_station_priv * sta_priv = ( void *) sta -> drv_priv ; <nl> - bool is_ap = priv -> iw_mode == NL80211_IFTYPE_STATION ; <nl> + bool is_ap = vif -> type == NL80211_IFTYPE_STATION ; <nl> int ret ; <nl> u8 sta_id ; <nl> 
struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
xfs_fs_geometry ( <nl> xfs_fsop_geom_t * geo , <nl> int new_version ) <nl> { <nl> + <nl> + memset ( geo , 0 , sizeof (* geo )); <nl> + <nl> geo -> blocksize = mp -> m_sb . sb_blocksize ; <nl> geo -> rtextsize = mp -> m_sb . sb_rextsize ; <nl> geo -> agblocks = mp -> m_sb . sb_agblocks ;
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
int mwifiex_ret_wmm_get_status ( struct mwifiex_private * priv , <nl> " WMM Parameter Set Count : % d \ n ", <nl> wmm_param_ie -> qos_info_bitmap & mask ); <nl>  <nl> + if ( wmm_param_ie -> vend_hdr . len + 2 > <nl> + sizeof ( struct ieee_types_wmm_parameter )) <nl> + break ; <nl> + <nl> memcpy (( u8 *) & priv -> curr_bss_params . bss_descriptor . <nl> wmm_ie , wmm_param_ie , <nl> wmm_param_ie -> vend_hdr . len + 2 );
static int skl_unload_module ( struct sst_dsp * ctx , u16 mod_id ) <nl> dev_err ( ctx -> dev , " Module bad usage cnt !:% d \ n ", usage_cnt ); <nl> return - EIO ; <nl> } <nl> + <nl> + /* if module is used by others return , no need to unload */ <nl> + if ( usage_cnt > 0 ) <nl> + return 0 ; <nl> + <nl> ret = skl_ipc_unload_modules (& skl -> ipc , <nl> SKL_NUM_MODULES , & mod_id ); <nl> if ( ret < 0 ) {
static char * res_strings [] = { <nl> " reserved 37 ", <nl> " reserved 38 ", <nl> " reserved 39 ", <nl> - " reseverd 40 ", <nl> + " reserved 40 ", <nl> " reserved 41 ", <nl> " reserved 42 ", <nl> " reserved 43 ",
static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
static int set_wep_key ( struct airo_info * ai , u16 index , const char * key , <nl> WepKeyRid wkr ; <nl> int rc ; <nl>  <nl> - WARN_ON ( keylen == 0 ); <nl> + if ( WARN_ON ( keylen == 0 )) <nl> + return - 1 ; <nl>  <nl> memset (& wkr , 0 , sizeof ( wkr )); <nl> wkr . len = cpu_to_le16 ( sizeof ( wkr ));
static int tegra_kbc_start ( struct tegra_kbc * kbc ) <nl> /* Reset the KBC controller to clear all previous status .*/ <nl> reset_control_assert ( kbc -> rst ); <nl> udelay ( 100 ); <nl> - reset_control_assert ( kbc -> rst ); <nl> + reset_control_deassert ( kbc -> rst ); <nl> udelay ( 100 ); <nl>  <nl> tegra_kbc_config_pins ( kbc );
static int __devexit ssm2602_i2c_remove ( struct i2c_client * client ) <nl>  <nl> static const struct i2c_device_id ssm2602_i2c_id [] = { <nl> { " ssm2602 ", SSM2602 }, <nl> + { " ssm2603 ", SSM2602 }, <nl> { " ssm2604 ", SSM2604 }, <nl> { } <nl> }; <nl> static void __exit ssm2602_exit ( void ) <nl> } <nl> module_exit ( ssm2602_exit ); <nl>  <nl> - MODULE_DESCRIPTION (" ASoC SSM2602 / SSM2604 driver "); <nl> + MODULE_DESCRIPTION (" ASoC SSM2602 / SSM2603 / SSM2604 driver "); <nl> MODULE_AUTHOR (" Cliff Cai "); <nl> MODULE_LICENSE (" GPL ");
qla24xx_report_id_acquisition ( scsi_qla_host_t * vha , <nl> if ( vp_idx == 0 && ( MSB ( stat ) != 1 )) <nl> goto reg_needed ; <nl>  <nl> - if ( MSB ( stat ) == 1 ) { <nl> + if ( MSB ( stat ) != 0 ) { <nl> ql_dbg ( ql_dbg_mbx , vha , 0x10ba , <nl> " Could not acquire ID for VP [% d ].\ n ", vp_idx ); <nl> return ;
static int sof_set_get_large_ctrl_data ( struct snd_sof_dev * sdev , <nl> else <nl> err = sof_get_ctrl_copy_params ( cdata -> type , partdata , cdata , <nl> sparams ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree ( partdata ); <nl> return err ; <nl> + } <nl>  <nl> msg_bytes = sparams -> msg_bytes ; <nl> pl_size = sparams -> pl_size ;
static struct aead_edesc * aead_giv_edesc_alloc ( struct aead_givcrypt_request <nl> assoc_nents = assoc_nents ? : 1 ; <nl> src_nents = src_nents ? : 1 ; <nl> sec4_sg_len += assoc_nents + 1 + src_nents ; <nl> - if ( likely ( req -> src == req -> dst )) <nl> + if ( req -> src == req -> dst && <nl> + ( src_nents || iv_dma + ivsize != sg_dma_address ( req -> src ))) <nl> contig &= ~ GIV_DST_CONTIG ; <nl> } <nl> 
static void e1000_get_ethtool_stats ( struct net_device * netdev , <nl> p = ( char *) adapter + <nl> e1000_gstrings_stats [ i ]. stat_offset ; <nl> break ; <nl> + default : <nl> + data [ i ] = 0 ; <nl> + continue ; <nl> } <nl>  <nl> data [ i ] = ( e1000_gstrings_stats [ i ]. sizeof_stat ==
ath5k_intr ( int irq , void * dev_id ) <nl> tasklet_schedule (& sc -> restq ); <nl> } else { <nl> if ( status & AR5K_INT_SWBA ) { <nl> - tasklet_schedule (& sc -> beacontq ); <nl> + tasklet_hi_schedule (& sc -> beacontq ); <nl> } <nl> if ( status & AR5K_INT_RXEOL ) { <nl> /*
static int __init fm10k_init_module ( void ) <nl> /* create driver workqueue */ <nl> fm10k_workqueue = alloc_workqueue ("% s ", WQ_MEM_RECLAIM , 0 , <nl> fm10k_driver_name ); <nl> + if (! fm10k_workqueue ) <nl> + return - ENOMEM ; <nl>  <nl> fm10k_dbg_init (); <nl> 
batadv_iv_ogm_orig_get ( struct batadv_priv * bat_priv , const uint8_t * addr ) <nl> free_bcast_own : <nl> kfree ( orig_node -> bat_iv . bcast_own ); <nl> free_orig_node : <nl> + /* free twice , as batadv_orig_node_new sets refcount to 2 */ <nl> + batadv_orig_node_free_ref ( orig_node ); <nl> batadv_orig_node_free_ref ( orig_node ); <nl>  <nl> return NULL ;
static bool hist_browser__toggle_fold ( struct hist_browser * browser ) <nl> struct callchain_list * cl = container_of ( ms , struct callchain_list , ms ); <nl> bool has_children ; <nl>  <nl> + if (! he || ! ms ) <nl> + return false ; <nl> + <nl> if ( ms == & he -> ms ) <nl> has_children = hist_entry__toggle_fold ( he ); <nl> else
predicate_parse ( const char * str , int nr_parens , int nr_preds , <nl>  <nl> switch (* next ) { <nl> case '(': /* # 2 */ <nl> - if ( top - op_stack > nr_parens ) <nl> - return ERR_PTR (- EINVAL ); <nl> + if ( top - op_stack > nr_parens ) { <nl> + ret = - EINVAL ; <nl> + goto out_free ; <nl> + } <nl> *(++ top ) = invert ; <nl> continue ; <nl> case '!': /* # 3 */
struct cfg80211_bss * cfg80211_get_bss ( struct wiphy * wiphy , <nl> continue ; <nl> if ( channel && bss -> pub . channel != channel ) <nl> continue ; <nl> + if (! is_valid_ether_addr ( bss -> pub . bssid )) <nl> + continue ; <nl> /* Don ' t get expired BSS structs */ <nl> if ( time_after ( now , bss -> ts + IEEE80211_SCAN_RESULT_EXPIRE ) && <nl> ! atomic_read (& bss -> hold ))
static int ci_hdrc_usb2_probe ( struct platform_device * pdev ) <nl>  <nl> if (! ci_pdata ) { <nl> ci_pdata = devm_kmalloc ( dev , sizeof (* ci_pdata ), GFP_KERNEL ); <nl> + if (! ci_pdata ) <nl> + return - ENOMEM ; <nl> * ci_pdata = ci_default_pdata ; /* struct copy */ <nl> } <nl> 
static int soc_cleanup_card_resources ( struct snd_soc_card * card ) <nl> if ( card -> remove ) <nl> card -> remove ( card ); <nl>  <nl> + snd_soc_dapm_free (& card -> dapm ); <nl> + <nl> kfree ( card -> rtd ); <nl> snd_card_free ( card -> snd_card ); <nl> return 0 ;
static int sh_veu_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> * vdev = sh_veu_videodev ; <nl> + vdev -> v4l2_dev = & veu -> v4l2_dev ; <nl> spin_lock_init (& veu -> lock ); <nl> mutex_init (& veu -> fop_lock ); <nl> vdev -> lock = & veu -> fop_lock ;
static int pcnet32_phys_id ( struct net_device * dev , u32 data ) <nl> if ((! data ) || ( data > ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ))) <nl> data = ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ); <nl>  <nl> - schedule_timeout ( data * HZ ); <nl> + msleep_interruptible ( data * 1000 ); <nl> del_timer_sync (& lp -> blink_timer ); <nl>  <nl> /* Restore the original value of the bcrs */
static const char * intercon [][ 3 ] = { <nl> {" HPRCOM ", NULL , " Right HP Com "}, <nl>  <nl> /* Mono Output */ <nl> - {" MONOLOUT ", NULL , " Mono Out "}, <nl> - {" MONOLOUT ", NULL , " Mono Out "}, <nl> + {" MONO_LOUT ", NULL , " Mono Out "}, <nl> + {" MONO_LOUT ", NULL , " Mono Out "}, <nl>  <nl> /* Left Input */ <nl> {" Left Line1L Mux ", " single - ended ", " LINE1L "},
static void snd_timer_user_tinterrupt ( struct snd_timer_instance * timeri , <nl> } <nl> if (( tu -> filter & ( 1 << SNDRV_TIMER_EVENT_RESOLUTION )) && <nl> tu -> last_resolution != resolution ) { <nl> + memset (& r1 , 0 , sizeof ( r1 )); <nl> r1 . event = SNDRV_TIMER_EVENT_RESOLUTION ; <nl> r1 . tstamp = tstamp ; <nl> r1 . val = resolution ;
void vmw_kms_helper_resource_finish ( struct vmw_validation_ctx * ctx , <nl> vmw_kms_helper_buffer_finish ( res -> dev_priv , NULL , ctx -> buf , <nl> out_fence , NULL ); <nl>  <nl> + vmw_dmabuf_unreference (& ctx -> buf ); <nl> vmw_resource_unreserve ( res , false , NULL , 0 ); <nl> mutex_unlock (& res -> dev_priv -> cmdbuf_mutex ); <nl> }
static void sba_process_received_request ( struct sba_device * sba , <nl>  <nl> WARN_ON ( tx -> cookie < 0 ); <nl> if ( tx -> cookie > 0 ) { <nl> + spin_lock_irqsave (& sba -> reqs_lock , flags ); <nl> dma_cookie_complete ( tx ); <nl> + spin_unlock_irqrestore (& sba -> reqs_lock , flags ); <nl> dmaengine_desc_get_callback_invoke ( tx , NULL ); <nl> dma_descriptor_unmap ( tx ); <nl> tx -> callback = NULL ;
static ssize_t rpmsg_eptdev_write_iter ( struct kiocb * iocb , <nl> if (! kbuf ) <nl> return - ENOMEM ; <nl>  <nl> - if (! copy_from_iter_full ( kbuf , len , from )) <nl> - return - EFAULT ; <nl> + if (! copy_from_iter_full ( kbuf , len , from )) { <nl> + ret = - EFAULT ; <nl> + goto free_kbuf ; <nl> + } <nl>  <nl> if ( mutex_lock_interruptible (& eptdev -> ept_lock )) { <nl> ret = - ERESTARTSYS ;
int iwl_trans_pcie_dyn_txq_alloc ( struct iwl_trans * trans , <nl> rsp = ( void *) hcmd . resp_pkt -> data ; <nl> qid = le16_to_cpu ( rsp -> queue_number ); <nl>  <nl> - if ( qid > ARRAY_SIZE ( trans_pcie -> txq )) { <nl> + if ( qid >= ARRAY_SIZE ( trans_pcie -> txq )) { <nl> WARN_ONCE ( 1 , " queue index % d unsupported ", qid ); <nl> ret = - EIO ; <nl> goto error_free_resp ;
int netvsc_send ( struct hv_device * device , <nl> if (! net_device ) <nl> return - ENODEV ; <nl>  <nl> + /* We may race with netvsc_connect_vsp ()/ netvsc_init_buf () and get <nl> + * here before the negotiation with the host is finished and <nl> + * send_section_map may not be allocated yet . <nl> + */ <nl> + if (! net_device -> send_section_map ) <nl> + return - EAGAIN ; <nl> + <nl> out_channel = net_device -> chn_table [ q_idx ]; <nl>  <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ;
int yama_task_prctl ( int option , unsigned long arg2 , unsigned long arg3 , <nl> if ( arg2 == 0 ) { <nl> yama_ptracer_del ( NULL , myself ); <nl> rc = 0 ; <nl> - } else if ( arg2 == PR_SET_PTRACER_ANY ) { <nl> + } else if ( arg2 == PR_SET_PTRACER_ANY || ( int ) arg2 == - 1 ) { <nl> rc = yama_ptracer_add ( NULL , myself ); <nl> } else { <nl> struct task_struct * tracer ;
EXPORT_SYMBOL_GPL ( crypto_init_spawn2 ); <nl>  <nl> void crypto_drop_spawn ( struct crypto_spawn * spawn ) <nl> { <nl> + if (! spawn -> alg ) <nl> + return ; <nl> + <nl> down_write (& crypto_alg_sem ); <nl> list_del (& spawn -> list ); <nl> up_write (& crypto_alg_sem );
static int __devinit rdc321x_wdt_probe ( struct platform_device * pdev ) <nl> struct resource * r ; <nl> struct rdc321x_wdt_pdata * pdata ; <nl>  <nl> - pdata = pdev -> dev . platform_data ; <nl> + pdata = platform_get_drvdata ( pdev ); <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " no platform data supplied \ n "); <nl> return - ENODEV ;
static int als_wait_for_data_ready ( struct device * dev ) <nl> ret = i2c_smbus_read_byte_data ( client , 0x86 ); <nl> } while (!( ret & 0x80 ) && retry --); <nl>  <nl> - if (! retry ) { <nl> + if ( retry < 0 ) { <nl> dev_warn ( dev , " timeout waiting for data ready \ n "); <nl> return - ETIMEDOUT ; <nl> }
static struct dentry * aio_mount ( struct file_system_type * fs_type , <nl> static const struct dentry_operations ops = { <nl> . d_dname = simple_dname , <nl> }; <nl> - return mount_pseudo ( fs_type , " aio :", NULL , & ops , AIO_RING_MAGIC ); <nl> + struct dentry * root = mount_pseudo ( fs_type , " aio :", NULL , & ops , <nl> + AIO_RING_MAGIC ); <nl> + <nl> + if (! IS_ERR ( root )) <nl> + root -> d_sb -> s_iflags |= SB_I_NOEXEC ; <nl> + return root ; <nl> } <nl>  <nl> /* aio_setup
static void iomd_get_next_sg ( struct scatterlist * sg , struct iomd_dma * idma ) <nl>  <nl> if ( idma -> dma . sg -> length == 0 ) { <nl> if ( idma -> dma . sgcount > 1 ) { <nl> - idma -> dma . sg ++; <nl> + idma -> dma . sg = sg_next ( idma -> dma . sg ); <nl> idma -> dma . sgcount --; <nl> } else { <nl> idma -> dma . sg = NULL ;
acpi_ps_get_next_arg ( struct acpi_walk_state * walk_state , <nl> ACPI_POSSIBLE_METHOD_CALL ); <nl>  <nl> if ( arg -> common . aml_opcode == AML_INT_METHODCALL_OP ) { <nl> + <nl> + /* Free method call op and corresponding namestring sub - ob */ <nl> + <nl> + acpi_ps_free_op ( arg -> common . value . arg ); <nl> acpi_ps_free_op ( arg ); <nl> arg = NULL ; <nl> walk_state -> arg_count = 1 ;
static int gb_uart_flush ( struct gb_tty * gb_tty , u8 flags ) <nl> & request , sizeof ( request ), NULL , 0 ); <nl> } <nl>  <nl> - static struct gb_tty * get_gb_by_minor ( unsigned minor ) <nl> + static struct gb_tty * get_gb_by_minor ( unsigned int minor ) <nl> { <nl> struct gb_tty * gb_tty ; <nl> 
int jbd2_journal_start_reserved ( handle_t * handle , unsigned int type , <nl> */ <nl> ret = start_this_handle ( journal , handle , GFP_NOFS ); <nl> if ( ret < 0 ) { <nl> + handle -> h_journal = journal ; <nl> jbd2_journal_free_reserved ( handle ); <nl> return ret ; <nl> }
static int gic_shared_irq_domain_map ( struct irq_domain * d , unsigned int virq , <nl>  <nl> spin_lock_irqsave (& gic_lock , flags ); <nl> gic_map_to_pin ( intr , gic_cpu_pin ); <nl> - gic_map_to_vpe ( intr , vpe ); <nl> + gic_map_to_vpe ( intr , mips_cm_vp_id ( vpe )); <nl> for ( i = 0 ; i < min ( gic_vpes , NR_CPUS ); i ++) <nl> clear_bit ( intr , pcpu_masks [ i ]. pcpu_mask ); <nl> set_bit ( intr , pcpu_masks [ vpe ]. pcpu_mask );
int hugetlb_fault ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> * so no worry about deadlock . <nl> */ <nl> page = pte_page ( entry ); <nl> + get_page ( page ); <nl> if ( page != pagecache_page ) <nl> lock_page ( page ); <nl>  <nl> int hugetlb_fault ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> } <nl> if ( page != pagecache_page ) <nl> unlock_page ( page ); <nl> + put_page ( page ); <nl>  <nl> out_mutex : <nl> mutex_unlock (& hugetlb_instantiation_mutex );
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> + if (! nodes_subset ( new , node_online_map )) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> err = security_task_movememory ( task ); <nl> if ( err ) <nl> goto out ;
static int eb_copy_relocations ( const struct i915_execbuffer * eb ) <nl> min_t ( u64 , BIT_ULL ( 31 ), size - copied ); <nl>  <nl> if ( __copy_from_user (( char *) relocs + copied , <nl> - ( char *) urelocs + copied , <nl> + ( char __user *) urelocs + copied , <nl> len )) { <nl> kvfree ( relocs ); <nl> err = - EFAULT ;
static unsigned long axi_clkgen_recalc_rate ( struct clk_hw * clk_hw , <nl> tmp = ( unsigned long long )( parent_rate / d ) * m ; <nl> do_div ( tmp , dout ); <nl>  <nl> - if ( tmp > ULONG_MAX ) <nl> - return ULONG_MAX ; <nl> - <nl> - return tmp ; <nl> + return min_t ( unsigned long long , tmp , ULONG_MAX ); <nl> } <nl>  <nl> static int axi_clkgen_enable ( struct clk_hw * clk_hw )
static void process_checks ( struct r1bio * r1_bio ) <nl> struct page ** ppages = get_resync_pages ( pbio )-> pages ; <nl> struct page ** spages = get_resync_pages ( sbio )-> pages ; <nl> struct bio_vec * bi ; <nl> - int page_len [ RESYNC_PAGES ]; <nl> + int page_len [ RESYNC_PAGES ] = { 0 }; <nl>  <nl> if ( sbio -> bi_end_io != end_sync_read ) <nl> continue ;
static int hi3660_stub_clk_probe ( struct platform_device * pdev ) <nl> return PTR_ERR ( stub_clk_chan . mbox ); <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> + if (! res ) <nl> + return - EINVAL ; <nl> freq_reg = devm_ioremap ( dev , res -> start , resource_size ( res )); <nl> if (! freq_reg ) <nl> return - ENOMEM ;
static void tilcdc_crtc_destroy ( struct drm_crtc * crtc ) <nl> struct tilcdc_crtc * tilcdc_crtc = to_tilcdc_crtc ( crtc ); <nl> struct tilcdc_drm_private * priv = crtc -> dev -> dev_private ; <nl>  <nl> + drm_modeset_lock_crtc ( crtc , NULL ); <nl> tilcdc_crtc_disable ( crtc ); <nl> + drm_modeset_unlock_crtc ( crtc ); <nl>  <nl> flush_workqueue ( priv -> wq ); <nl> 
int io_msg_ring ( struct io_kiocb * req , unsigned int issue_flags ) <nl> req_set_fail ( req ); <nl> io_req_set_res ( req , ret , 0 ); <nl> /* put file to avoid an attempt to IOPOLL the req */ <nl> - io_put_file ( req -> file ); <nl> + if (!( req -> flags & REQ_F_FIXED_FILE )) <nl> + io_put_file ( req -> file ); <nl> req -> file = NULL ; <nl> return IOU_OK ; <nl> }
int tipc_nl_publ_dump ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if (! attrs [ TIPC_NLA_SOCK ]) <nl> + return - EINVAL ; <nl> + <nl> err = nla_parse_nested ( sock , TIPC_NLA_SOCK_MAX , <nl> attrs [ TIPC_NLA_SOCK ], <nl> tipc_nl_sock_policy );
static void __ip_rt_update_pmtu ( struct rtable * rt , struct flowi4 * fl4 , u32 mtu ) <nl> if ( mtu < ip_rt_min_pmtu ) <nl> mtu = ip_rt_min_pmtu ; <nl>  <nl> + if ( rt -> rt_pmtu == mtu && <nl> + time_before ( jiffies , dst -> expires - ip_rt_mtu_expires / 2 )) <nl> + return ; <nl> + <nl> rcu_read_lock (); <nl> if ( fib_lookup ( dev_net ( dst -> dev ), fl4 , & res ) == 0 ) { <nl> struct fib_nh * nh = & FIB_RES_NH ( res );
static enum io_status ccwreq_status ( struct ccw_device * cdev , struct irb * lcirb ) <nl> /* Ask the driver what to do */ <nl> if ( cdev -> drv && cdev -> drv -> uc_handler ) { <nl> todo = cdev -> drv -> uc_handler ( cdev , lcirb ); <nl> + CIO_TRACE_EVENT ( 2 , " uc_response "); <nl> + CIO_HEX_EVENT ( 2 , & todo , sizeof ( todo )); <nl> switch ( todo ) { <nl> case UC_TODO_RETRY : <nl> return IO_STATUS_ERROR ;
static int __f2fs_setxattr ( struct inode * inode , int index , <nl> goto exit ; <nl> } <nl>  <nl> - if ( f2fs_xattr_value_same ( here , value , size )) <nl> + if ( value && f2fs_xattr_value_same ( here , value , size )) <nl> goto exit ; <nl> } else if (( flags & XATTR_REPLACE )) { <nl> error = - ENODATA ;
beiscsi_create_wrb_rings ( struct beiscsi_hba * phba , <nl> if ( status != 0 ) { <nl> shost_printk ( KERN_ERR , phba -> shost , <nl> " wrbq create failed ."); <nl> + kfree ( pwrb_arr ); <nl> return status ; <nl> } <nl> phwi_ctrlr -> wrb_context [ i * 2 ]. cid = phwi_context -> be_wrbq [ i ].
static int tegra_io_pad_prepare ( enum tegra_io_pad id , unsigned long * request , <nl> } <nl>  <nl> rate = clk_get_rate ( pmc -> clk ); <nl> + if (! rate ) <nl> + return - ENODEV ; <nl>  <nl> tegra_pmc_writel ( DPD_SAMPLE_ENABLE , DPD_SAMPLE ); <nl> 
static void __wl1271_op_remove_interface ( struct wl1271 * wl , <nl> wl1271_free_ap_keys ( wl , wlvif ); <nl> } <nl>  <nl> + dev_kfree_skb ( wlvif -> probereq ); <nl> + wlvif -> probereq = NULL ; <nl> wl12xx_tx_reset_wlvif ( wl , wlvif ); <nl> if ( wl -> last_wlvif == wlvif ) <nl> wl -> last_wlvif = NULL ;
static int cciss_ioctl32_big_passthru ( struct block_device * bdev , fmode_t mode , <nl> int err ; <nl> u32 cp ; <nl>  <nl> + memset (& arg64 , 0 , sizeof ( arg64 )); <nl> err = 0 ; <nl> err |= <nl> copy_from_user (& arg64 . LUN_info , & arg32 -> LUN_info ,
long keyctl_read_key ( key_serial_t keyid , char __user * buffer , size_t buflen ) <nl>  <nl> key = key_ref_to_ptr ( key_ref ); <nl>  <nl> + if ( test_bit ( KEY_FLAG_NEGATIVE , & key -> flags )) { <nl> + ret = - ENOKEY ; <nl> + goto error2 ; <nl> + } <nl> + <nl> /* see if we can read it directly */ <nl> ret = key_permission ( key_ref , KEY_NEED_READ ); <nl> if ( ret == 0 )
int vfio_pci_set_irqs_ioctl ( struct vfio_pci_device * vdev , uint32_t flags , <nl> func = vfio_pci_set_err_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> case VFIO_PCI_REQ_IRQ_INDEX : <nl> switch ( flags & VFIO_IRQ_SET_ACTION_TYPE_MASK ) { <nl> case VFIO_IRQ_SET_ACTION_TRIGGER : <nl> func = vfio_pci_set_req_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> } <nl>  <nl> if (! func )
static irqreturn_t sil24_interrupt ( int irq , void * dev_instance , struct pt_regs * <nl>  <nl> status = readl ( hpriv -> host_base + HOST_IRQ_STAT ); <nl>  <nl> + if ( status == 0xffffffff ) { <nl> + printk ( KERN_ERR DRV_NAME ": IRQ status == 0xffffffff , " <nl> + " PCI fault or device removal ?\ n "); <nl> + goto out ; <nl> + } <nl> + <nl> if (!( status & IRQ_STAT_4PORTS )) <nl> goto out ; <nl> 
static int rtw_wx_set_enc_ext ( struct net_device * dev , <nl> memset ( param , 0 , param_len ); <nl>  <nl> param -> cmd = IEEE_CMD_SET_ENCRYPTION ; <nl> - memset ( param -> sta_addr , 0xff , ETH_ALEN ); <nl> + eth_broadcast_addr ( param -> sta_addr ); <nl>  <nl> switch ( pext -> alg ) { <nl> case IW_ENCODE_ALG_NONE :
static ssize_t wait_for_direct_io ( enum ORANGEFS_io_type type , struct inode * inod <nl> */ <nl> if ( ret == - EAGAIN && op_state_purged ( new_op )) { <nl> orangefs_bufmap_put ( bufmap , buffer_index ); <nl> + buffer_index = - 1 ; <nl> gossip_debug ( GOSSIP_FILE_DEBUG , <nl> "% s : going to repopulate_shared_memory .\ n ", <nl> __func__ );
static void wl3501_free_tx_buffer ( struct wl3501_card * this , u16 ptr ) <nl>  <nl> static int wl3501_esbq_req_test ( struct wl3501_card * this ) <nl> { <nl> - u8 tmp ; <nl> + u8 tmp = 0 ; <nl>  <nl> wl3501_get_from_wla ( this , this -> esbq_req_head + 3 , & tmp , sizeof ( tmp )); <nl> return tmp & 0x80 ;
static int __init ptp_kvm_init ( void ) <nl> { <nl> long ret ; <nl>  <nl> + if (! kvm_para_available ()) <nl> + return - ENODEV ; <nl> + <nl> clock_pair_gpa = slow_virt_to_phys (& clock_pair ); <nl> hv_clock = pvclock_pvti_cpu0_va (); <nl> 
unsigned long __init lmb_alloc_base ( unsigned long size , unsigned long align , <nl>  <nl> alloc = __lmb_alloc_base ( size , align , max_addr ); <nl>  <nl> - if ( alloc < 0 ) <nl> + if ( alloc == 0 ) <nl> panic (" ERROR : Failed to allocate 0x % lx bytes below 0x % lx .\ n ", <nl> size , max_addr ); <nl> 
static struct console udbg_console = { <nl> . index = - 1 , <nl> }; <nl>  <nl> + static int early_console_initialized ; <nl> + <nl> void __init disable_early_printk ( void ) <nl> { <nl> + if (! early_console_initialized ) <nl> + return ; <nl> unregister_console (& udbg_console ); <nl> + early_console_initialized = 0 ; <nl> } <nl>  <nl> /* called by setup_system */ <nl> void register_early_udbg_console ( void ) <nl> { <nl> + early_console_initialized = 1 ; <nl> register_console (& udbg_console ); <nl> } <nl> 
static void qlcnic_get_ethtool_stats ( struct net_device * dev , <nl> memset ( data , 0 , stats -> n_stats * sizeof ( u64 )); <nl>  <nl> for ( ring = 0 , index = 0 ; ring < adapter -> drv_tx_rings ; ring ++) { <nl> - if ( test_bit ( __QLCNIC_DEV_UP , & adapter -> state )) { <nl> + if ( adapter -> is_up == QLCNIC_ADAPTER_UP_MAGIC ) { <nl> tx_ring = & adapter -> tx_ring [ ring ]; <nl> data = qlcnic_fill_tx_queue_stats ( data , tx_ring ); <nl> qlcnic_update_stats ( adapter );
unsigned long sctp_transport_timeout ( struct sctp_transport * trans ) <nl> trans -> state != SCTP_PF ) <nl> timeout += trans -> hbinterval ; <nl>  <nl> - return timeout ; <nl> + return max_t ( unsigned long , timeout , HZ / 5 ); <nl> } <nl>  <nl> /* Reset transport variables to their initial values */
int dlm_migrate_request_handler ( struct o2net_msg * msg , u32 len , void * data , <nl> migrate -> new_master , <nl> migrate -> master ); <nl>  <nl> + if ( ret < 0 ) <nl> + kmem_cache_free ( dlm_mle_cache , mle ); <nl> + <nl> spin_unlock (& dlm -> master_lock ); <nl> unlock : <nl> spin_unlock (& dlm -> spinlock );
static void ieee80211_csa_connection_drop_work ( struct work_struct * work ) <nl> container_of ( work , struct ieee80211_sub_if_data , <nl> u . ibss . csa_connection_drop_work ); <nl>  <nl> + sdata_lock ( sdata ); <nl> + <nl> ieee80211_ibss_disconnect ( sdata ); <nl> synchronize_rcu (); <nl> skb_queue_purge (& sdata -> skb_queue ); <nl>  <nl> /* trigger a scan to find another IBSS network to join */ <nl> ieee80211_queue_work (& sdata -> local -> hw , & sdata -> work ); <nl> + <nl> + sdata_unlock ( sdata ); <nl> } <nl>  <nl> static void ieee80211_ibss_csa_mark_radar ( struct ieee80211_sub_if_data * sdata )
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> * and we need some headroom for passing the frame to monitor <nl> * interfaces , but never both at the same time . <nl> */ <nl> - local -> tx_headroom = max ( local -> hw . extra_tx_headroom , <nl> - sizeof ( struct ieee80211_tx_status_rtap_hdr )); <nl> + local -> tx_headroom = max_t ( unsigned int , local -> hw . extra_tx_headroom , <nl> + sizeof ( struct ieee80211_tx_status_rtap_hdr )); <nl>  <nl> debugfs_hw_add ( local ); <nl> 
copy_from_user ( void * to , const void __user * from , unsigned long n ) <nl> __kernel_size_t __copy_size = ( __kernel_size_t ) n ; <nl>  <nl> if ( __copy_size && __access_ok ( __copy_from , __copy_size )) <nl> - return __copy_user ( to , from , __copy_size ); <nl> + __copy_size = __copy_user ( to , from , __copy_size ); <nl> + <nl> + if ( unlikely ( __copy_size )) <nl> + memset ( to + ( n - __copy_size ), 0 , __copy_size ); <nl>  <nl> return __copy_size ; <nl> }
static int stmmac_pci_probe ( struct pci_dev * pdev , <nl> priv = stmmac_dvr_probe (&( pdev -> dev ), & plat_dat , addr ); <nl> if (! priv ) { <nl> pr_err ("% s : main driver probe failed ", __func__ ); <nl> + ret = - ENODEV ; <nl> goto err_out ; <nl> } <nl> priv -> dev -> irq = pdev -> irq ;
static int igb_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> if ( hw -> flash_address ) <nl> iounmap ( hw -> flash_address ); <nl> err_sw_init : <nl> + kfree ( adapter -> shadow_vfta ); <nl> igb_clear_interrupt_scheme ( adapter ); <nl> pci_iounmap ( pdev , hw -> hw_addr ); <nl> err_ioremap :
static struct ctl_table sctp_net_table [] = { <nl> . mode = 0644 , <nl> . proc_handler = proc_sctp_do_auth , <nl> }, <nl> + { <nl> + . procname = " intl_enable ", <nl> + . data = & init_net . sctp . intl_enable , <nl> + . maxlen = sizeof ( int ), <nl> + . mode = 0644 , <nl> + . proc_handler = proc_dointvec , <nl> + }, <nl> { <nl> . procname = " addr_scope_policy ", <nl> . data = & init_net . sctp . scope_policy ,
static int tipc_l2_device_event ( struct notifier_block * nb , unsigned long evt , <nl> break ; <nl> case NETDEV_UNREGISTER : <nl> case NETDEV_CHANGENAME : <nl> - bearer_disable ( dev_net ( dev ), b ); <nl> + bearer_disable ( net , b ); <nl> break ; <nl> } <nl> return NOTIFY_OK ;
static int snd_imx_open ( struct snd_pcm_substream * substream ) <nl> dma_params = snd_soc_dai_get_dma_data ( rtd -> cpu_dai , substream ); <nl>  <nl> dma_data = kzalloc ( sizeof (* dma_data ), GFP_KERNEL ); <nl> + if (! dma_data ) <nl> + return - ENOMEM ; <nl> + <nl> dma_data -> peripheral_type = dma_params -> shared_peripheral ? <nl> IMX_DMATYPE_SSI_SP : IMX_DMATYPE_SSI ; <nl> dma_data -> priority = DMA_PRIO_HIGH ;
static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
static int pxa3xx_u2d_probe ( struct platform_device * pdev ) <nl> struct resource * r ; <nl> int err ; <nl>  <nl> - u2d = kzalloc ( sizeof ( struct pxa3xx_u2d_ulpi ), GFP_KERNEL ); <nl> + u2d = kzalloc ( sizeof (* u2d ), GFP_KERNEL ); <nl> if (! u2d ) { <nl> dev_err (& pdev -> dev , " failed to allocate memory \ n "); <nl> return - ENOMEM ;
static int mwifiex_pcie_alloc_cmdrsp_buf ( struct mwifiex_adapter * adapter ) <nl> } <nl> skb_put ( skb , MWIFIEX_UPLD_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MWIFIEX_UPLD_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> return - 1 ; <nl> + } <nl>  <nl> card -> cmdrsp_buf = skb ; <nl> 
static void ath9k_hw_get_def_gain_boundaries_pdadcs ( struct ath_hw * ah , <nl> vpdTableI [ i ][ sizeCurrVpdTable - 2 ]); <nl> vpdStep = ( int16_t )(( vpdStep < 1 ) ? 1 : vpdStep ); <nl>  <nl> - if ( tgtIndex > maxIndex ) { <nl> + if ( tgtIndex >= maxIndex ) { <nl> while (( ss <= tgtIndex ) && <nl> ( k < ( AR5416_NUM_PDADC_VALUES - 1 ))) { <nl> tmpVal = ( int16_t )(( vpdTableI [ i ][ sizeCurrVpdTable - 1 ] +
static void ixgbe_configure_dcb ( struct ixgbe_adapter * adapter ) <nl> if ( hw -> mac . type == ixgbe_mac_82598EB ) <nl> netif_set_gso_max_size ( adapter -> netdev , 32768 ); <nl>  <nl> - ixgbe_dcb_check_config (& adapter -> dcb_cfg ); <nl> ixgbe_dcb_calculate_tc_credits (& adapter -> dcb_cfg , DCB_TX_CONFIG ); <nl> ixgbe_dcb_calculate_tc_credits (& adapter -> dcb_cfg , DCB_RX_CONFIG ); <nl> 
static int labpc_ai_cmd ( struct comedi_device * dev , struct comedi_subdevice * s ) <nl> devpriv -> write_byte ( INTERVAL_LOAD_BITS , <nl> dev -> iobase + INTERVAL_LOAD_REG ); <nl>  <nl> - if ( cmd -> convert_src == TRIG_TIMER || cmd -> scan_begin_src == TRIG_TIMER ) { <nl> + if ( cmd -> convert_src == TRIG_TIMER || <nl> + cmd -> scan_begin_src == TRIG_TIMER ) { <nl> /* set up pacing */ <nl> labpc_adc_timing ( dev , cmd , mode ); <nl> /* load counter b0 in mode 3 */
static int __arm_v7s_map ( struct arm_v7s_io_pgtable * data , unsigned long iova , <nl> pte |= ARM_V7S_ATTR_NS_TABLE ; <nl>  <nl> __arm_v7s_set_pte ( ptep , pte , 1 , cfg ); <nl> - } else { <nl> + } else if ( ARM_V7S_PTE_IS_TABLE ( pte , lvl )) { <nl> cptep = iopte_deref ( pte , lvl ); <nl> + } else { <nl> + /* We require an unmap first */ <nl> + WARN_ON (! selftest_running ); <nl> + return - EEXIST ; <nl> } <nl>  <nl> /* Rinse , repeat */
int dw_mci_probe ( struct dw_mci * host ) <nl> } <nl> } <nl>  <nl> - if ( host -> pdata -> num_slots > 1 ) { <nl> + if ( host -> pdata -> num_slots < 1 ) { <nl> dev_err ( host -> dev , <nl> " Platform data must supply num_slots .\ n "); <nl> return - ENODEV ;
static int iucv_sock_sendmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> txmsg . class = 0 ; <nl> + memcpy (& txmsg . class , skb -> data , skb -> len >= 4 ? 4 : skb -> len ); <nl> txmsg . tag = iucv -> send_tag ++; <nl> memcpy ( skb -> cb , & txmsg . tag , 4 ); <nl> skb_queue_tail (& iucv -> send_skb_q , skb );
static void _qed_iscsi_get_tstats ( struct qed_hwfn * p_hwfn , <nl> HILO_64_REGPAIR ( tstats . iscsi_rx_bytes_cnt ); <nl> p_stats -> iscsi_rx_packet_cnt = <nl> HILO_64_REGPAIR ( tstats . iscsi_rx_packet_cnt ); <nl> + p_stats -> iscsi_rx_new_ooo_isle_events_cnt = <nl> + HILO_64_REGPAIR ( tstats . iscsi_rx_new_ooo_isle_events_cnt ); <nl> p_stats -> iscsi_cmdq_threshold_cnt = <nl> le32_to_cpu ( tstats . iscsi_cmdq_threshold_cnt ); <nl> p_stats -> iscsi_rq_threshold_cnt =
void Dot11d_Init ( struct ieee80211_device * ieee ) <nl>  <nl> pDot11dInfo -> State = DOT11D_STATE_NONE ; <nl> pDot11dInfo -> CountryIeLen = 0 ; <nl> - memset ( pDot11dInfo -> channel_map , 0 , MAX_CHANNEL_NUMBER + 1 ); <nl> + memset ( pDot11dInfo -> channel_map , 0 , MAX_CHANNEL_NUMBER + 1 ); <nl> memset ( pDot11dInfo -> MaxTxPwrDbmList , 0xFF , MAX_CHANNEL_NUMBER + 1 ); <nl> RESET_CIE_WATCHDOG ( ieee ); <nl> 
static int i915_display_info ( struct seq_file * m , void * unused ) <nl> x , y , crtc -> cursor_addr , <nl> yesno ( active )); <nl> } <nl> + <nl> + seq_printf ( m , "\ tunderrun reporting : cpu =% s pch =% s \ n ", <nl> + yesno (! crtc -> cpu_fifo_underrun_disabled ), <nl> + yesno (! crtc -> pch_fifo_underrun_disabled )); <nl> } <nl>  <nl> seq_printf ( m , "\ n ");
struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
xlog_finish_defer_ops ( <nl> 0 , XFS_TRANS_RESERVE , & tp ); <nl> if ( error ) <nl> return error ; <nl> + /* dfops is already populated so assign it manually */ <nl> + tp -> t_dfops = dfops ; <nl>  <nl> error = xfs_defer_finish (& tp , dfops ); <nl> if ( error )
static void do_boot_cpu ( __u8 cpuid ); <nl> static void do_quad_bootstrap ( void ); <nl>  <nl> int hard_smp_processor_id ( void ); <nl> + int safe_smp_processor_id ( void ); <nl>  <nl> /* Inline functions */ <nl> static inline void <nl> hard_smp_processor_id ( void ) <nl> return 0 ; <nl> } <nl>  <nl> + int <nl> + safe_smp_processor_id ( void ) <nl> +{ <nl> + return hard_smp_processor_id (); <nl> +} <nl> + <nl> /* broadcast a halt to all other CPUs */ <nl> void <nl> smp_send_stop ( void )
static int do_mq_notify ( mqd_t mqdes , const struct sigevent * notification ) <nl>  <nl> timeo = MAX_SCHEDULE_TIMEOUT ; <nl> ret = netlink_attachskb ( sock , nc , & timeo , NULL ); <nl> - if ( ret == 1 ) <nl> + if ( ret == 1 ) { <nl> + sock = NULL ; <nl> goto retry ; <nl> + } <nl> if ( ret ) { <nl> sock = NULL ; <nl> nc = NULL ;
static struct protection_domain * get_domain ( struct device * dev ) <nl> domain = to_pdomain ( io_domain ); <nl> attach_device ( dev , domain ); <nl> } <nl> + if ( domain == NULL ) <nl> + return ERR_PTR (- EBUSY ); <nl> + <nl> if (! dma_ops_domain ( domain )) <nl> return ERR_PTR (- EBUSY ); <nl> 
static int eql_g_master_cfg ( struct net_device * dev , master_config_t __user * mcp ) <nl> equalizer_t * eql ; <nl> master_config_t mc ; <nl>  <nl> + memset (& mc , 0 , sizeof ( master_config_t )); <nl> + <nl> if ( eql_is_master ( dev )) { <nl> eql = netdev_priv ( dev ); <nl> mc . max_slaves = eql -> max_slaves ;
static int tipc_connect ( struct socket * sock , struct sockaddr * dest , <nl> if ( dst -> family == AF_UNSPEC ) { <nl> memset (& tsk -> remote , 0 , sizeof ( struct sockaddr_tipc )); <nl> tsk -> connected = 0 ; <nl> + } else if ( destlen != sizeof ( struct sockaddr_tipc )) { <nl> + res = - EINVAL ; <nl> } else { <nl> memcpy (& tsk -> remote , dest , destlen ); <nl> tsk -> connected = 1 ;
static int cpufreq_online ( unsigned int cpu ) <nl> if ( new_policy ) { <nl> /* related_cpus should at least include policy -> cpus . */ <nl> cpumask_copy ( policy -> related_cpus , policy -> cpus ); <nl> - /* Clear mask of registered CPUs */ <nl> - cpumask_clear ( policy -> real_cpus ); <nl> } <nl>  <nl> /*
xfs_rtfree_range ( <nl> */ <nl> error = xfs_rtfind_forw ( mp , tp , end , mp -> m_sb . sb_rextents - 1 , <nl> & postblock ); <nl> + if ( error ) <nl> + return error ; <nl> /* <nl> * If there are blocks not being freed at the front of the <nl> * old extent , add summary data for them to be allocated .
static ssize_t field ## _show ( struct device * dev , \ <nl> char * buf ) \ <nl> { \ <nl> struct gb_interface * intf = to_gb_interface ( dev ); \ <nl> - return sprintf ( buf , "%"# type "\ n ", intf -> field ); \ <nl> + return scnprintf ( buf , PAGE_SIZE , "%"# type "\ n ", intf -> field ); \ <nl> } \ <nl> static DEVICE_ATTR_RO ( field ) <nl> 
static void bdw_load_gamma_lut ( struct drm_crtc_state * state , u32 offset ) <nl> } <nl>  <nl> /* Program the max register to clamp values > 1 . 0 . */ <nl> + i = lut_size - 1 ; <nl> I915_WRITE ( PREC_PAL_GC_MAX ( pipe , 0 ), <nl> drm_color_lut_extract ( lut [ i ]. red , 16 )); <nl> I915_WRITE ( PREC_PAL_GC_MAX ( pipe , 1 ),
static void etb_update_buffer ( struct coresight_device * csdev , <nl>  <nl> capacity = drvdata -> buffer_depth * ETB_FRAME_SIZE_WORDS ; <nl>  <nl> - CS_UNLOCK ( drvdata -> base ); <nl> etb_disable_hw ( drvdata ); <nl> + CS_UNLOCK ( drvdata -> base ); <nl>  <nl> /* unit is in words , not bytes */ <nl> read_ptr = readl_relaxed ( drvdata -> base + ETB_RAM_READ_POINTER );
int dmar_set_interrupt ( struct intel_iommu * iommu ) <nl> return 0 ; <nl>  <nl> irq = create_irq (); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> pr_err (" IOMMU : no free vectors \ n "); <nl> return - EINVAL ; <nl> }
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> struct oz_app_hdr * app_hdr ; <nl> struct oz_serial_ctx * ctx ; <nl>  <nl> + if ( count > sizeof ( ei -> data ) - sizeof (* elt ) - sizeof (* app_hdr )) <nl> + return - EINVAL ; <nl> + <nl> spin_lock_bh (& g_cdev . lock ); <nl> pd = g_cdev . active_pd ; <nl> if ( pd )
int drm_atomic_helper_setup_commit ( struct drm_atomic_state * state , <nl> ! try_wait_for_completion (& old_plane_state -> commit -> flip_done )) <nl> return - EBUSY ; <nl>  <nl> - commit = crtc_or_fake_commit ( state , old_plane_state -> crtc ); <nl> + commit = crtc_or_fake_commit ( state , new_plane_state -> crtc ?: old_plane_state -> crtc ); <nl> if (! commit ) <nl> return - ENOMEM ; <nl> 
struct sched_entity { <nl>  <nl> struct sched_rt_entity { <nl> struct list_head run_list ; <nl> - unsigned int time_slice ; <nl> unsigned long timeout ; <nl> + unsigned int time_slice ; <nl> int nr_cpus_allowed ; <nl>  <nl> struct sched_rt_entity * back ;
cifs_put_tcp_session ( struct TCP_Server_Info * server , int from_reconnect ) <nl> server -> session_key . response = NULL ; <nl> server -> session_key . len = 0 ; <nl> kfree ( server -> hostname ); <nl> + server -> hostname = NULL ; <nl>  <nl> task = xchg (& server -> tsk , NULL ); <nl> if ( task )
s32 fm10k_disable_queues_generic ( struct fm10k_hw * hw , u16 q_cnt ) <nl> /* clear tx_ready to prevent any false hits for reset */ <nl> hw -> mac . tx_ready = false ; <nl>  <nl> + if ( FM10K_REMOVED ( hw -> hw_addr )) <nl> + return 0 ; <nl> + <nl> /* clear the enable bit for all rings */ <nl> for ( i = 0 ; i < q_cnt ; i ++) { <nl> reg = fm10k_read_reg ( hw , FM10K_TXDCTL ( i ));
struct oz_pd * oz_pd_alloc ( const u8 * mac_addr ) <nl> /* <nl> * Context : softirq or process <nl> */ <nl> - void oz_pd_free ( struct work_struct * work ) <nl> + static void oz_pd_free ( struct work_struct * work ) <nl> { <nl> struct list_head * e ; <nl> struct oz_tx_frame * f ;
static void mwifiex_recreate_adapter ( struct sdio_mmc_card * card ) <nl> mmc_hw_reset ( func -> card -> host ); <nl> sdio_release_host ( func ); <nl>  <nl> + /* Previous save_adapter won ' t be valid after this . We will cancel <nl> + * pending work requests . <nl> + */ <nl> + clear_bit ( MWIFIEX_IFACE_WORK_DEVICE_DUMP , & iface_work_flags ); <nl> + clear_bit ( MWIFIEX_IFACE_WORK_CARD_RESET , & iface_work_flags ); <nl> + <nl> mwifiex_sdio_probe ( func , device_id ); <nl> } <nl> 
static int sierra_resume ( struct usb_serial * serial ) <nl> if ( err < 0 ) { <nl> intfdata -> in_flight --; <nl> usb_unanchor_urb ( urb ); <nl> - usb_scuttle_anchored_urbs (& portdata -> delayed ); <nl> - break ; <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> + spin_lock (& portdata -> lock ); <nl> + portdata -> outstanding_urbs --; <nl> + spin_unlock (& portdata -> lock ); <nl> + continue ; <nl> } <nl> } <nl> 
static int dm_update_crtcs_state ( struct dc * dc , <nl> } <nl> } <nl>  <nl> - if ( dc_is_stream_unchanged ( new_stream , dm_old_crtc_state -> stream ) && <nl> + if ( enable && dc_is_stream_unchanged ( new_stream , dm_old_crtc_state -> stream ) && <nl> dc_is_stream_scaling_unchanged ( new_stream , dm_old_crtc_state -> stream )) { <nl>  <nl> new_crtc_state -> mode_changed = false ;
ssize_t btrfs_listxattr ( struct dentry * dentry , char * buffer , size_t size ) <nl>  <nl> if (! buffer || ( name_len + 1 ) > size_left ) { <nl> ret = - ERANGE ; <nl> - break ; <nl> + goto err ; <nl> } <nl>  <nl> name_ptr = ( unsigned long )( di + 1 );
nv134_chipset = { <nl> . top = gk104_top_new , <nl> . disp = gp104_disp_new , <nl> . dma = gf119_dma_new , <nl> + . fifo = gp100_fifo_new , <nl> }; <nl>  <nl> static int
static void tmu_set_mode ( enum clock_event_mode mode , <nl> { <nl> switch ( mode ) { <nl> case CLOCK_EVT_MODE_PERIODIC : <nl> - ctrl_outl ( ctrl_inl ( TMU0_TCNT ), TMU0_TCOR ); <nl> + ctrl_outl ( tmu_latest_interval [ TMU0 ], TMU0_TCOR ); <nl> break ; <nl> case CLOCK_EVT_MODE_ONESHOT : <nl> ctrl_outl ( 0 , TMU0_TCOR );
static int ieee80211_update_mesh_config ( struct wiphy * wiphy , <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_TTL , mask )) <nl> conf -> dot11MeshTTL = nconf -> dot11MeshTTL ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_ELEMENT_TTL , mask )) <nl> - conf -> dot11MeshTTL = nconf -> element_ttl ; <nl> + conf -> element_ttl = nconf -> element_ttl ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_AUTO_OPEN_PLINKS , mask )) <nl> conf -> auto_open_plinks = nconf -> auto_open_plinks ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_SYNC_OFFSET_MAX_NEIGHBOR , mask ))
static int kvm_vm_ioctl_create_vcpu ( struct kvm * kvm , u32 id ) <nl> int r ; <nl> struct kvm_vcpu * vcpu , * v ; <nl>  <nl> + if ( id >= KVM_MAX_VCPUS ) <nl> + return - EINVAL ; <nl> + <nl> vcpu = kvm_arch_vcpu_create ( kvm , id ); <nl> if ( IS_ERR ( vcpu )) <nl> return PTR_ERR ( vcpu );
static int __exit fsl_udc_remove ( struct platform_device * pdev ) <nl> if (! udc_controller ) <nl> return - ENODEV ; <nl>  <nl> - usb_del_gadget_udc (& udc_controller -> gadget ); <nl> udc_controller -> done = & done ; <nl> + usb_del_gadget_udc (& udc_controller -> gadget ); <nl>  <nl> fsl_udc_clk_release (); <nl> 
static int sst_platform_open ( struct snd_pcm_substream * substream ) <nl> ret_val = register_sst_card ( stream -> sstdrv_ops ); <nl> if ( ret_val ) { <nl> pr_err (" sst : sst card registration failed \ n "); <nl> + kfree ( stream -> sstdrv_ops ); <nl> + kfree ( stream ); <nl> return ret_val ; <nl> } <nl> runtime -> private_data = stream ;
static struct pnp_driver tpm_inf_pnp_driver = { <nl> . probe = tpm_inf_pnp_probe , <nl> . suspend = tpm_inf_pnp_suspend , <nl> . resume = tpm_inf_pnp_resume , <nl> - . remove = __devexit_p ( tpm_inf_pnp_remove ) <nl> + . remove = tpm_inf_pnp_remove <nl> }; <nl>  <nl> static int __init init_inf ( void )
static int ima_lsm_rule_init ( struct ima_measure_rule_entry * entry , <nl> result = security_filter_rule_init ( entry -> lsm [ lsm_rule ]. type , <nl> Audit_equal , args , <nl> & entry -> lsm [ lsm_rule ]. rule ); <nl> + if (! entry -> lsm [ lsm_rule ]. rule ) <nl> + return - EINVAL ; <nl> return result ; <nl> } <nl> 
CIFSSMBSetAttrLegacy ( int xid , struct cifsTconInfo * tcon , char * fileName , <nl>  <nl> int <nl> CIFSSMBUnixSetInfo ( const int xid , struct cifsTconInfo * tcon , char * fileName , <nl> - const struct cifs_unix_set_info_args * args , <nl> + const struct cifs_unix_set_info_args * args , <nl> const struct nls_table * nls_codepage , int remap ) <nl> { <nl> TRANSACTION2_SPI_REQ * pSMB = NULL ;
void btrfs_update_iflags ( struct inode * inode ) <nl> */ <nl> void btrfs_inherit_iflags ( struct inode * inode , struct inode * dir ) <nl> { <nl> - unsigned int flags = BTRFS_I ( dir )-> flags ; <nl> + unsigned int flags ; <nl> + <nl> + if (! dir ) <nl> + return ; <nl> + <nl> + flags = BTRFS_I ( dir )-> flags ; <nl>  <nl> if ( S_ISREG ( inode -> i_mode )) <nl> flags &= ~ BTRFS_INODE_DIRSYNC ;
extern int blk_iopoll_enabled ; <nl> static int sixty = 60 ; <nl> # endif <nl>  <nl> + static int neg_one = - 1 ; <nl> static int zero ; <nl> static int __maybe_unused one = 1 ; <nl> static int __maybe_unused two = 2 ;
int ubifs_run_commit ( struct ubifs_info * c ) <nl>  <nl> spin_lock (& c -> cs_lock ); <nl> if ( c -> cmt_state == COMMIT_BROKEN ) { <nl> - err = - EINVAL ; <nl> + err = - EROFS ; <nl> goto out ; <nl> } <nl>  <nl> int ubifs_run_commit ( struct ubifs_info * c ) <nl> * re - check it . <nl> */ <nl> if ( c -> cmt_state == COMMIT_BROKEN ) { <nl> - err = - EINVAL ; <nl> + err = - EROFS ; <nl> goto out_cmt_unlock ; <nl> } <nl> 
static int xen_allocate_irq_gsi ( unsigned gsi ) <nl>  <nl> static void xen_free_irq ( unsigned irq ) <nl> { <nl> + /* Legacy IRQ descriptors are managed by the arch . */ <nl> + if ( irq < NR_IRQS_LEGACY ) <nl> + return ; <nl> + <nl> irq_free_desc ( irq ); <nl> } <nl> 
int dccp_rcv_state_process ( struct sock * sk , struct sk_buff * skb , <nl> if ( inet_csk ( sk )-> icsk_af_ops -> conn_request ( sk , <nl> skb ) < 0 ) <nl> return 1 ; <nl> - goto discard ; <nl> + consume_skb ( skb ); <nl> + return 0 ; <nl> } <nl> if ( dh -> dccph_type == DCCP_PKT_RESET ) <nl> goto discard ;
int sysfs_move_dir ( struct kobject * kobj , struct kobject * new_parent_kobj ) <nl> /* Remove from old parent ' s list and insert into new parent ' s list . */ <nl> sysfs_unlink_sibling ( sd ); <nl> sysfs_get ( new_parent_sd ); <nl> + drop_nlink ( old_parent -> d_inode ); <nl> sysfs_put ( sd -> s_parent ); <nl> sd -> s_parent = new_parent_sd ; <nl> + inc_nlink ( new_parent -> d_inode ); <nl> sysfs_link_sibling ( sd ); <nl>  <nl> out_unlock :
void init_tg_cfs_entry ( struct task_group * tg , struct cfs_rq * cfs_rq , <nl> se -> cfs_rq = parent -> my_q ; <nl>  <nl> se -> my_q = cfs_rq ; <nl> - update_load_set (& se -> load , 0 ); <nl> + /* guarantee group entities always have weight */ <nl> + update_load_set (& se -> load , NICE_0_LOAD ); <nl> se -> parent = parent ; <nl> } <nl> 
int __nvme_submit_sync_cmd ( struct request_queue * q , struct nvme_command * cmd , <nl> return PTR_ERR ( req ); <nl>  <nl> req -> cmd_type = REQ_TYPE_DRV_PRIV ; <nl> + req -> cmd_flags = REQ_FAILFAST_DRIVER ; <nl> req -> __data_len = 0 ; <nl> req -> __sector = ( sector_t ) - 1 ; <nl> req -> bio = req -> biotail = NULL ;
static int f2fs_ioc_defragment ( struct file * filp , unsigned long arg ) <nl> goto out ; <nl> } <nl>  <nl> + if ( unlikely (( range . start + range . len ) >> PAGE_SHIFT > <nl> + sbi -> max_file_blocks )) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> err = f2fs_defragment_range ( sbi , filp , & range ); <nl> f2fs_update_time ( sbi , REQ_TIME ); <nl> if ( err < 0 )
int acpi_bus_generate_proc_event4 ( const char * device_class , const char * bus_id , <nl> if (! event_is_open ) <nl> return 0 ; <nl>  <nl> - event = kmalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> + event = kzalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> if (! event ) <nl> return - ENOMEM ; <nl> 
static int spi_imx_probe ( struct platform_device * pdev ) <nl> goto out_clk_put ; <nl> } <nl>  <nl> + if (! master -> cs_gpios ) { <nl> + dev_err (& pdev -> dev , " No CS GPIOs available \ n "); <nl> + goto out_clk_put ; <nl> + } <nl> + <nl> for ( i = 0 ; i < master -> num_chipselect ; i ++) { <nl> if (! gpio_is_valid ( master -> cs_gpios [ i ])) <nl> continue ;
static int bnx2x_populate_ext_phy ( struct bnx2x * bp , <nl> return - EINVAL ; <nl> default : <nl> * phy = phy_null ; <nl> + /* In case external PHY wasn ' t found */ <nl> + if (( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_DIRECT ) && <nl> + ( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_NOT_CONN )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static int sensor_hub_probe ( struct hid_device * hdev , <nl> if ( name == NULL ) { <nl> hid_err ( hdev , " Failed MFD device name \ n "); <nl> ret = - ENOMEM ; <nl> + kfree ( hsdev ); <nl> goto err_no_mem ; <nl> } <nl> sd -> hid_sensor_hub_client_devs [
megasas_sysfs_set_dbg_lvl ( struct device_driver * dd , const char * buf , size_t coun <nl> return retval ; <nl> } <nl>  <nl> - static DRIVER_ATTR ( dbg_lvl , S_IRUGO | S_IWUGO , megasas_sysfs_show_dbg_lvl , <nl> + static DRIVER_ATTR ( dbg_lvl , S_IRUGO | S_IWUSR , megasas_sysfs_show_dbg_lvl , <nl> megasas_sysfs_set_dbg_lvl ); <nl>  <nl> static ssize_t
static int spi_gpio_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl>  <nl> status = devm_add_action_or_reset (& pdev -> dev , spi_gpio_put , master ); <nl> - if ( status ) <nl> + if ( status ) { <nl> + spi_master_put ( master ); <nl> return status ; <nl> + } <nl>  <nl> if ( of_id ) <nl> status = spi_gpio_probe_dt ( pdev , master );
int snd_hda_queue_unsol_event ( struct hda_bus * bus , u32 res , u32 res_ex ) <nl> struct hda_bus_unsolicited * unsol ; <nl> unsigned int wp ; <nl>  <nl> + if (! bus || ! bus -> workq ) <nl> + return 0 ; <nl> + <nl> trace_hda_unsol_event ( bus , res , res_ex ); <nl> unsol = bus -> unsol ; <nl> if (! unsol )
static struct pernet_operations unix_net_ops = { <nl> static int __init af_unix_init ( void ) <nl> { <nl> int rc = - 1 ; <nl> - struct sk_buff * dummy_skb ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> rc = proto_register (& unix_proto , 1 ); <nl> if ( rc != 0 ) {
void rt2x00lib_txdone ( struct queue_entry * entry , <nl> * Update TX statistics . <nl> */ <nl> rt2x00dev -> link . qual . tx_success += success ; <nl> - rt2x00dev -> link . qual . tx_failed += txdesc -> retry + fail ; <nl> + rt2x00dev -> link . qual . tx_failed += fail ; <nl>  <nl> /* <nl> * Initialize TX status
snd_usb_audio_probe ( struct usb_device * dev , <nl> __error : <nl> if ( chip && ! chip -> num_interfaces ) <nl> snd_card_free ( chip -> card ); <nl> + chip -> probing = 0 ; <nl> mutex_unlock (& register_mutex ); <nl> __err_val : <nl> return NULL ;
static void perf_addr_filters_adjust ( struct vm_area_struct * vma ) <nl> struct perf_event_context * ctx ; <nl> int ctxn ; <nl>  <nl> + /* <nl> + * Data tracing isn ' t supported yet and as such there is no need <nl> + * to keep track of anything that isn ' t related to executable code : <nl> + */ <nl> + if (!( vma -> vm_flags & VM_EXEC )) <nl> + return ; <nl> + <nl> rcu_read_lock (); <nl> for_each_task_context_nr ( ctxn ) { <nl> ctx = rcu_dereference ( current -> perf_event_ctxp [ ctxn ]);
static int proc_pid_permission ( struct inode * inode , int mask ) <nl> bool has_perms ; <nl>  <nl> task = get_proc_task ( inode ); <nl> + if (! task ) <nl> + return - ESRCH ; <nl> has_perms = has_pid_permissions ( pid , task , 1 ); <nl> put_task_struct ( task ); <nl> 
static void snd_als4000_configure ( struct snd_sb * chip ) <nl> /* SPECS_PAGE : 39 */ <nl> for ( i = ALS4K_GCR91_DMA0_ADDR ; i <= ALS4K_GCR96_DMA3_MODE_COUNT ; ++ i ) <nl> snd_als4k_gcr_write ( chip , i , 0 ); <nl> - <nl> + /* enable burst mode to prevent dropouts during high PCI bus usage */ <nl> snd_als4k_gcr_write ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL , <nl> - snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL )); <nl> + snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL ) | 0x04 ); <nl> spin_unlock_irq (& chip -> reg_lock ); <nl> } <nl> 
static int sr_init_command ( struct scsi_cmnd * SCpnt ) <nl> static int sr_block_open ( struct inode * inode , struct file * file ) <nl> { <nl> struct gendisk * disk = inode -> i_bdev -> bd_disk ; <nl> - struct scsi_cd * cd = scsi_cd ( inode -> i_bdev -> bd_disk ); <nl> + struct scsi_cd * cd ; <nl> int ret = 0 ; <nl>  <nl> if (!( cd = scsi_cd_get ( disk )))
int sst_hsw_stream_get_volume ( struct sst_hsw * hsw , struct sst_hsw_stream * stream <nl> return - EINVAL ; <nl>  <nl> sst_dsp_read ( hsw -> dsp , volume , <nl> - stream -> reply . volume_register_address [ channel ], sizeof ( volume )); <nl> + stream -> reply . volume_register_address [ channel ], <nl> + sizeof (* volume )); <nl>  <nl> return 0 ; <nl> }
int xfrm_policy_flush ( u8 type , struct xfrm_audit * audit_info ) <nl> continue ; <nl> hlist_del (& pol -> bydst ); <nl> hlist_del (& pol -> byidx ); <nl> + list_del (& pol -> walk . all ); <nl> write_unlock_bh (& xfrm_policy_lock ); <nl>  <nl> xfrm_audit_policy_delete ( pol , 1 , audit_info -> loginuid ,
static int i2c_mux_reg_probe_dt ( struct regmux * mux , <nl> mux -> data . idle_in_use = true ; <nl>  <nl> /* map address from " reg " if exists */ <nl> - if ( of_address_to_resource ( np , 0 , & res )) { <nl> + if ( of_address_to_resource ( np , 0 , & res ) == 0 ) { <nl> mux -> data . reg_size = resource_size (& res ); <nl> mux -> data . reg = devm_ioremap_resource (& pdev -> dev , & res ); <nl> if ( IS_ERR ( mux -> data . reg ))
static int overlay_set_addr ( struct mmp_overlay * overlay , struct mmp_addr * addr ) <nl> struct lcd_regs * regs = path_regs ( overlay -> path ); <nl>  <nl> /* FIXME : assert addr supported */ <nl> - memcpy (& overlay -> addr , addr , sizeof ( struct mmp_win )); <nl> + memcpy (& overlay -> addr , addr , sizeof ( struct mmp_addr )); <nl> writel ( addr -> phys [ 0 ], & regs -> g_0 ); <nl>  <nl> return overlay -> addr . phys [ 0 ];
static s32 Handle_Get_InActiveTime ( struct wilc_vif * vif , <nl> wid . type = WID_STR ; <nl> wid . size = ETH_ALEN ; <nl> wid . val = kmalloc ( wid . size , GFP_KERNEL ); <nl> + if (! wid . val ) <nl> + return - ENOMEM ; <nl>  <nl> stamac = wid . val ; <nl> ether_addr_copy ( stamac , strHostIfStaInactiveT -> mac );
static void sas_discover_domain ( struct work_struct * work ) <nl> case FANOUT_DEV : <nl> error = sas_discover_root_expander ( dev ); <nl> break ; <nl> -# ifdef CONFIG_SCSI_SAS_ATA <nl> case SATA_DEV : <nl> case SATA_PM : <nl> +# ifdef CONFIG_SCSI_SAS_ATA <nl> error = sas_discover_sata ( dev ); <nl> break ; <nl> +# else <nl> + SAS_DPRINTK (" ATA device seen but CONFIG_SCSI_SAS_ATA = N so cannot attach \ n "); <nl> + /* Fall through */ <nl> # endif <nl> default : <nl> error = - ENXIO ;
static int __sock_diag_rcv_msg ( struct sk_buff * skb , struct nlmsghdr * nlh ) <nl> if ( nlmsg_len ( nlh ) < sizeof (* req )) <nl> return - EINVAL ; <nl>  <nl> + if ( req -> sdiag_family >= AF_MAX ) <nl> + return - EINVAL ; <nl> + <nl> hndl = sock_diag_lock_handler ( req -> sdiag_family ); <nl> if ( hndl == NULL ) <nl> err = - ENOENT ;
static int cachefiles_mark_object_active ( struct cachefiles_cache * cache , <nl> /* an old object from a previous incarnation is hogging the slot - we <nl> * need to wait for it to be destroyed */ <nl> wait_for_old_object : <nl> - if ( fscache_object_is_live (& object -> fscache )) { <nl> + if ( fscache_object_is_live (& xobject -> fscache )) { <nl> pr_err ("\ n "); <nl> pr_err (" Error : Unexpected object collision \ n "); <nl> cachefiles_printk_object ( object , xobject );
static int radeon_uvd_cs_reloc ( struct radeon_cs_parser * p , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (( start >> 28 ) != ( end >> 28 )) { <nl> + if (( start >> 28 ) != (( end - 1 ) >> 28 )) { <nl> DRM_ERROR (" reloc % LX -% LX crossing 256MB boundary !\ n ", <nl> start , end ); <nl> return - EINVAL ;
void __init omap_detect_sram ( void ) <nl> if ( cpu_is_omap34xx ()) { <nl> omap_sram_base = OMAP3_SRAM_PUB_VA ; <nl> omap_sram_start = OMAP3_SRAM_PUB_PA ; <nl> - omap_sram_size = 0x8000 ; /* 32K */ <nl> + if (( omap_type () == OMAP2_DEVICE_TYPE_EMU ) || <nl> + ( omap_type () == OMAP2_DEVICE_TYPE_SEC )) { <nl> + omap_sram_size = 0x7000 ; /* 28K */ <nl> + } else { <nl> + omap_sram_size = 0x8000 ; /* 32K */ <nl> + } <nl> } else { <nl> omap_sram_base = OMAP2_SRAM_PUB_VA ; <nl> omap_sram_start = OMAP2_SRAM_PUB_PA ;
static long gntalloc_ioctl_alloc ( struct gntalloc_file_private_data * priv , <nl> goto out ; <nl> } <nl>  <nl> - gref_ids = kzalloc ( sizeof ( gref_ids [ 0 ]) * op . count , GFP_TEMPORARY ); <nl> + gref_ids = kcalloc ( op . count , sizeof ( gref_ids [ 0 ]), GFP_TEMPORARY ); <nl> if (! gref_ids ) { <nl> rc = - ENOMEM ; <nl> goto out ;
vc4_get_bcl ( struct drm_device * dev , struct vc4_exec_info * exec ) <nl> sizeof ( struct vc4_shader_state )) || <nl> temp_size < exec_size ) { <nl> DRM_ERROR (" overflow in exec arguments \ n "); <nl> + ret = - EINVAL ; <nl> goto fail ; <nl> } <nl> 
static void hpfs_write_failed ( struct address_space * mapping , loff_t to ) <nl> { <nl> struct inode * inode = mapping -> host ; <nl>  <nl> + hpfs_lock ( inode -> i_sb ); <nl> + <nl> if ( to > inode -> i_size ) { <nl> truncate_pagecache ( inode , to , inode -> i_size ); <nl> hpfs_truncate ( inode ); <nl> } <nl> + <nl> + hpfs_unlock ( inode -> i_sb ); <nl> } <nl>  <nl> static int hpfs_write_begin ( struct file * file , struct address_space * mapping ,
int iptunnel_xmit ( struct sock * sk , struct rtable * rt , struct sk_buff * skb , <nl> __be32 src , __be32 dst , __u8 proto , <nl> __u8 tos , __u8 ttl , __be16 df , bool xnet ) <nl> { <nl> - int pkt_len = skb -> len ; <nl> + int pkt_len = skb -> len - skb_inner_network_offset ( skb ); <nl> struct iphdr * iph ; <nl> int err ; <nl> 
long btrfs_ioctl_send ( struct file * mnt_file , void __user * arg_ ) <nl> goto out ; <nl> } <nl>  <nl> + if ( arg -> clone_sources_count > <nl> + ULLONG_MAX / sizeof (* arg -> clone_sources )) { <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> if (! access_ok ( VERIFY_READ , arg -> clone_sources , <nl> sizeof (* arg -> clone_sources ) * <nl> arg -> clone_sources_count )) {
static struct i2c_board_info __initdata snapper9260_i2c_devices [] = { <nl> { <nl> /* RTC */ <nl> I2C_BOARD_INFO (" isl1208 ", 0x6f ), <nl> + . irq = gpio_to_irq ( AT91_PIN_PA31 ), <nl> }, <nl> }; <nl> 
int beiscsi_cmd_mccq_create ( struct beiscsi_hba * phba , <nl> spin_lock (& phba -> ctrl . mbox_lock ); <nl> ctrl = & phba -> ctrl ; <nl> wrb = wrb_from_mbox (& ctrl -> mbox_mem ); <nl> + memset ( wrb , 0 , sizeof (* wrb )); <nl> req = embedded_payload ( wrb ); <nl> ctxt = & req -> context ; <nl> 
static int twl6040_vibra_probe ( struct platform_device * pdev ) <nl>  <nl> info -> input_dev -> name = " twl6040 : vibrator "; <nl> info -> input_dev -> id . version = 1 ; <nl> - info -> input_dev -> dev . parent = pdev -> dev . parent ; <nl> info -> input_dev -> close = twl6040_vibra_close ; <nl> __set_bit ( FF_RUMBLE , info -> input_dev -> ffbit ); <nl> 
int btrfs_read_chunk_tree ( struct btrfs_root * root ) <nl> key . type = 0 ; <nl> again : <nl> ret = btrfs_search_slot ( NULL , root , & key , path , 0 , 0 ); <nl> + if ( ret < 0 ) <nl> + goto error ; <nl> while ( 1 ) { <nl> leaf = path -> nodes [ 0 ]; <nl> slot = path -> slots [ 0 ];
lba_legacy_resources ( struct parisc_device * pa_dev , struct lba_device * lba_dev ) <nl> r -> name = " LBA PCI Busses "; <nl> r -> start = lba_num & 0xff ; <nl> r -> end = ( lba_num >> 8 ) & 0xff ; <nl> + r -> flags = IORESOURCE_BUS ; <nl>  <nl> /* Set up local PCI Bus resources - we don ' t need them for <nl> ** Legacy boxes but it ' s nice to see in / proc / iomem .
lpfc_els_flush_cmd ( struct lpfc_vport * vport ) <nl> */ <nl> spin_lock_irq (& phba -> hbalock ); <nl> pring = lpfc_phba_elsring ( phba ); <nl> + <nl> + /* Bail out if we ' ve no ELS wq , like in PCI error recovery case . */ <nl> + if ( unlikely (! pring )) { <nl> + spin_unlock_irq (& phba -> hbalock ); <nl> + return ; <nl> + } <nl> + <nl> if ( phba -> sli_rev == LPFC_SLI_REV4 ) <nl> spin_lock (& pring -> ring_lock ); <nl> 
static int blkif_release ( struct inode * inode , struct file * filep ) <nl> struct xenbus_device * dev = info -> xbdev ; <nl> enum xenbus_state state = xenbus_read_driver_state ( dev -> otherend ); <nl>  <nl> - if ( state == XenbusStateClosing ) <nl> + if ( state == XenbusStateClosing && info -> is_ready ) <nl> blkfront_closing ( dev ); <nl> } <nl> return 0 ;
void qla4xxx_free_ddb_index ( struct scsi_qla_host * ha ) <nl> ret = qla4xxx_get_fwddb_entry ( ha , idx , NULL , 0 , NULL , <nl> & next_idx , & state , & conn_err , <nl> NULL , NULL ); <nl> - if ( ret == QLA_ERROR ) <nl> + if ( ret == QLA_ERROR ) { <nl> + next_idx ++; <nl> continue ; <nl> + } <nl> if ( state == DDB_DS_NO_CONNECTION_ACTIVE || <nl> state == DDB_DS_SESSION_FAILED ) { <nl> DEBUG2 ( ql4_printk ( KERN_INFO , ha ,
static int clone_backref_node ( struct btrfs_trans_handle * trans , <nl> new_node -> bytenr = dest -> node -> start ; <nl> new_node -> level = node -> level ; <nl> new_node -> lowest = node -> lowest ; <nl> + new_node -> checked = 1 ; <nl> new_node -> root = dest ; <nl>  <nl> if (! node -> lowest ) {
static int do_md_run ( mddev_t * mddev ) <nl> if ( spares && mddev -> pers -> sync_request ) { <nl> mddev -> recovery = 0 ; <nl> set_bit ( MD_RECOVERY_RUNNING , & mddev -> recovery ); <nl> + set_bit ( MD_RECOVERY_RECOVER , & mddev -> recovery ); <nl> mddev -> sync_thread = md_register_thread ( md_do_sync , <nl> mddev , <nl> " resync ");
static void __init smp_init_package_map ( void ) <nl> * primary cores . <nl> */ <nl> ncpus = boot_cpu_data . x86_max_cores ; <nl> + if (! ncpus ) { <nl> + pr_warn (" x86_max_cores == zero !?!?"); <nl> + ncpus = 1 ; <nl> + } <nl> + <nl> __max_logical_packages = DIV_ROUND_UP ( total_cpus , ncpus ); <nl>  <nl> /*
typhoon_request_firmware ( struct typhoon * tp ) <nl> err = - ENOMEM ; <nl> goto out_err ; <nl> } <nl> + memcpy ( typhoon_fw_image , typhoon_fw -> data , typhoon_fw -> size ); <nl>  <nl> return 0 ; <nl> 
static void sdhci_prepare_data ( struct sdhci_host * host , struct mmc_command * cmd ) <nl> int sg_cnt ; <nl>  <nl> sg_cnt = sdhci_pre_dma_transfer ( host , data , NULL ); <nl> - if ( sg_cnt == 0 ) { <nl> + if ( sg_cnt <= 0 ) { <nl> /* <nl> * This only happens when someone fed <nl> * us an invalid request .
static void isd200_ata_command ( struct scsi_cmnd * srb , struct us_data * us ) <nl>  <nl> /* Make sure driver was initialized */ <nl>  <nl> - if ( us -> extra == NULL ) <nl> + if ( us -> extra == NULL ) { <nl> usb_stor_dbg ( us , " ERROR Driver not initialized \ n "); <nl> + srb -> result = DID_ERROR << 16 ; <nl> + return ; <nl> + } <nl>  <nl> scsi_set_resid ( srb , 0 ); <nl> /* scsi_bufflen might change in protocol translation to ata */
static enum page_references page_check_references ( struct page * page , <nl> */ <nl> SetPageReferenced ( page ); <nl>  <nl> - if ( referenced_page ) <nl> + if ( referenced_page || referenced_ptes > 1 ) <nl> return PAGEREF_ACTIVATE ; <nl>  <nl> return PAGEREF_KEEP ;
static int yurex_probe ( struct usb_interface * interface , const struct usb_device_ <nl> usb_rcvintpipe ( dev -> udev , dev -> int_in_endpointAddr ), <nl> dev -> int_buffer , YUREX_BUF_SIZE , yurex_interrupt , <nl> dev , 1 ); <nl> - dev -> cntl_urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> + dev -> urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> if ( usb_submit_urb ( dev -> urb , GFP_KERNEL )) { <nl> retval = - EIO ; <nl> err (" Could not submitting URB ");
void btrfs_invalidate_inodes ( struct btrfs_root * root ) <nl> struct inode * inode ; <nl> u64 objectid = 0 ; <nl>  <nl> - WARN_ON ( btrfs_root_refs (& root -> root_item ) != 0 ); <nl> + if (! test_bit ( BTRFS_FS_STATE_ERROR , & root -> fs_info -> fs_state )) <nl> + WARN_ON ( btrfs_root_refs (& root -> root_item ) != 0 ); <nl>  <nl> spin_lock (& root -> inode_lock ); <nl> again :
void bochs_fbdev_fini ( struct bochs_device * bochs ) <nl> if ( bochs -> fb . initialized ) <nl> bochs_fbdev_destroy ( bochs ); <nl>  <nl> - drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + if ( bochs -> fb . helper . fbdev ) <nl> + drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + <nl> bochs -> fb . initialized = false ; <nl> }
static int dim2_probe ( struct platform_device * pdev ) <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - dev -> disable_platform = pdata ? pdata -> disable : 0 ; <nl> + dev -> disable_platform = pdata ? pdata -> disable : NULL ; <nl>  <nl> dev_info (& pdev -> dev , " sync : num of frames per sub - buffer : % u \ n ", fcnt ); <nl> hal_ret = dim_startup ( dev -> io_base , dev -> clk_speed , fcnt );
static struct tnode * tnode_new ( t_key key , int pos , int bits ) <nl> } <nl>  <nl> pr_debug (" AT % p s =% zu % zu \ n ", tn , sizeof ( struct tnode ), <nl> - sizeof ( struct rt_trie_node ) << bits ); <nl> + sizeof ( struct rt_trie_node *) << bits ); <nl> return tn ; <nl> } <nl> 
static void ds1374_set_tlet ( ulong arg ) <nl> " can ' t confirm time set from rtc chip \ n "); <nl> } <nl>  <nl> - ulong new_time ; <nl> + static ulong new_time ; <nl>  <nl> DECLARE_TASKLET_DISABLED ( ds1374_tasklet , ds1374_set_tlet , ( ulong ) & new_time ); <nl> 
static int atmel_nand_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> nand_chip -> ecc . mode = host -> board . ecc_mode ; <nl> - nand_chip -> chip_delay = 20 ; /* 20us command delay time */ <nl> + nand_chip -> chip_delay = 40 ; /* 40us command delay time */ <nl>  <nl> if ( host -> board . bus_width_16 ) /* 16 - bit bus width */ <nl> nand_chip -> options |= NAND_BUSWIDTH_16 ;
enum siginfo_layout siginfo_layout ( int sig , int si_code ) <nl> [ SIGSEGV ] = { NSIGSEGV , SIL_FAULT }, <nl> [ SIGBUS ] = { NSIGBUS , SIL_FAULT }, <nl> [ SIGTRAP ] = { NSIGTRAP , SIL_FAULT }, <nl> -# if defined ( SIGMET ) && defined ( NSIGEMT ) <nl> +# if defined ( SIGEMT ) && defined ( NSIGEMT ) <nl> [ SIGEMT ] = { NSIGEMT , SIL_FAULT }, <nl> # endif <nl> [ SIGCHLD ] = { NSIGCHLD , SIL_CHLD },
static int crb_recv ( struct tpm_chip * chip , u8 * buf , size_t count ) <nl>  <nl> memcpy_fromio ( buf , priv -> rsp , 6 ); <nl> expected = be32_to_cpup (( __be32 *) & buf [ 2 ]); <nl> - <nl> - if ( expected > count ) <nl> + if ( expected > count || expected < 6 ) <nl> return - EIO ; <nl>  <nl> memcpy_fromio (& buf [ 6 ], & priv -> rsp [ 6 ], expected - 6 );
static inline void ipv6_store_devconf ( struct ipv6_devconf * cnf , <nl> # endif <nl> array [ DEVCONF_DISABLE_IPV6 ] = cnf -> disable_ipv6 ; <nl> array [ DEVCONF_ACCEPT_DAD ] = cnf -> accept_dad ; <nl> + array [ DEVCONF_FORCE_TLLAO ] = cnf -> force_tllao ; <nl> } <nl>  <nl> static inline size_t inet6_if_nlmsg_size ( void )
static void release_resources ( struct ibmvnic_adapter * adapter ) <nl> } <nl> } <nl> } <nl> + kfree ( adapter -> napi ); <nl> + adapter -> napi = NULL ; <nl>  <nl> release_login_rsp_buffer ( adapter ); <nl> }
static void pmf_gpio_set_ ## name ( struct gpio_runtime * rt , int on )\ <nl> \ <nl> if ( unlikely (! rt )) return ; \ <nl> rc = pmf_call_function ( rt -> node , # name "- mute ", & args ); \ <nl> - if ( rc ) \ <nl> + if ( rc && rc != - ENODEV ) \ <nl> printk ( KERN_WARNING " pmf_gpio_set_ " # name \ <nl> " failed , rc : % d \ n ", rc ); \ <nl> rt -> implementation_private &= ~( 1 << bit ); \
static int perf_copy_attr ( struct perf_event_attr __user * uattr , <nl> if ( ret ) <nl> return - EFAULT ; <nl>  <nl> + attr -> size = size ; <nl> + <nl> if ( attr -> __reserved_1 ) <nl> return - EINVAL ; <nl> 
static void intel_pt_insn_decoder ( struct insn * insn , <nl> enum intel_pt_insn_branch branch = INTEL_PT_BR_NO_BRANCH ; <nl> int ext ; <nl>  <nl> + intel_pt_insn -> rel = 0 ; <nl> + <nl> if ( insn_is_avx ( insn )) { <nl> intel_pt_insn -> op = INTEL_PT_OP_OTHER ; <nl> intel_pt_insn -> branch = INTEL_PT_BR_NO_BRANCH ;
static int iommu_map_page ( struct protection_domain * dom , <nl> count = PAGE_SIZE_PTE_COUNT ( page_size ); <nl> pte = alloc_pte ( dom , bus_addr , page_size , NULL , GFP_KERNEL ); <nl>  <nl> + if (! pte ) <nl> + return - ENOMEM ; <nl> + <nl> for ( i = 0 ; i < count ; ++ i ) <nl> if ( IOMMU_PTE_PRESENT ( pte [ i ])) <nl> return - EBUSY ;
struct iio_channel * iio_channel_get ( const char * name , const char * channel_name ) <nl> if ( c == NULL ) <nl> return ERR_PTR (- ENODEV ); <nl>  <nl> - channel = kmalloc ( sizeof (* channel ), GFP_KERNEL ); <nl> + channel = kzalloc ( sizeof (* channel ), GFP_KERNEL ); <nl> if ( channel == NULL ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
int adis_update_scan_mode ( struct iio_dev * indio_dev , <nl> return - ENOMEM ; <nl>  <nl> adis -> buffer = kcalloc ( indio_dev -> scan_bytes , 2 , GFP_KERNEL ); <nl> - if (! adis -> buffer ) <nl> + if (! adis -> buffer ) { <nl> + kfree ( adis -> xfer ); <nl> + adis -> xfer = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> rx = adis -> buffer ; <nl> tx = rx + scan_count ;
static int scmi_hwmon_probe ( struct scmi_device * sdev ) <nl> scmi_chip_info . info = ptr_scmi_ci ; <nl> chip_info = & scmi_chip_info ; <nl>  <nl> - for ( type = 0 ; type < hwmon_max && nr_count [ type ]; type ++) { <nl> + for ( type = 0 ; type < hwmon_max ; type ++) { <nl> + if (! nr_count [ type ]) <nl> + continue ; <nl> + <nl> scmi_hwmon_add_chan_info ( scmi_hwmon_chan , dev , nr_count [ type ], <nl> type , hwmon_attributes [ type ]); <nl> * ptr_scmi_ci ++ = scmi_hwmon_chan ++;
int vfs_quota_on_mount ( struct super_block * sb , char * qf_name , <nl> if ( IS_ERR ( dentry )) <nl> return PTR_ERR ( dentry ); <nl>  <nl> + if (! dentry -> d_inode ) { <nl> + error = - ENOENT ; <nl> + goto out ; <nl> + } <nl> + <nl> error = security_quota_on ( dentry ); <nl> if (! error ) <nl> error = vfs_quota_on_inode ( dentry -> d_inode , type , format_id ); <nl>  <nl> + out : <nl> dput ( dentry ); <nl> return error ; <nl> }
struct edac_pci_ctl_info * edac_pci_create_generic_ctl ( struct device * dev , <nl>  <nl> pci -> mod_name = mod_name ; <nl> pci -> ctl_name = EDAC_PCI_GENCTL_NAME ; <nl> - pci -> edac_check = edac_pci_generic_check ; <nl> + if ( edac_op_state == EDAC_OPSTATE_POLL ) <nl> + pci -> edac_check = edac_pci_generic_check ; <nl>  <nl> pdata -> edac_idx = edac_pci_idx ++; <nl> 
static struct irq_desc * __real_move_irq_desc ( struct irq_desc * old_desc , <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
rpcrdma_register_internal ( struct rpcrdma_ia * ia , void * va , int len , <nl> */ <nl> iov -> addr = ib_dma_map_single ( ia -> ri_id -> device , <nl> va , len , DMA_BIDIRECTIONAL ); <nl> + if ( ib_dma_mapping_error ( ia -> ri_id -> device , iov -> addr )) <nl> + return - ENOMEM ; <nl> + <nl> iov -> length = len ; <nl>  <nl> if ( ia -> ri_have_dma_lkey ) {
static int __get_data_block ( struct inode * inode , sector_t iblock , <nl> if (! err ) { <nl> map_bh ( bh , inode -> i_sb , map . m_pblk ); <nl> bh -> b_state = ( bh -> b_state & ~ F2FS_MAP_FLAGS ) | map . m_flags ; <nl> - bh -> b_size = map . m_len << inode -> i_blkbits ; <nl> + bh -> b_size = ( u64 ) map . m_len << inode -> i_blkbits ; <nl> } <nl> return err ; <nl> }
void rtw_alloc_hwxmits ( struct adapter * padapter ) <nl>  <nl> pxmitpriv -> hwxmit_entry = HWXMIT_ENTRY ; <nl>  <nl> - pxmitpriv -> hwxmits = kzalloc ( sizeof ( struct hw_xmit ) * pxmitpriv -> hwxmit_entry , GFP_KERNEL ); <nl> + pxmitpriv -> hwxmits = kcalloc ( pxmitpriv -> hwxmit_entry , <nl> + sizeof ( struct hw_xmit ), GFP_KERNEL ); <nl>  <nl> hwxmits = pxmitpriv -> hwxmits ; <nl> 
static int tvp5150_fill_fmt ( struct v4l2_subdev * sd , <nl> struct v4l2_mbus_framefmt * f ; <nl> struct tvp5150 * decoder = to_tvp5150 ( sd ); <nl>  <nl> - if (! format || format -> pad ) <nl> + if (! format || ( format -> pad != DEMOD_PAD_VID_OUT )) <nl> return - EINVAL ; <nl>  <nl> f = & format -> format ;
int snd_soc_dapm_device_event ( struct snd_soc_device * socdev , int event ) <nl> struct snd_soc_machine * machine = socdev -> machine ; <nl>  <nl> if ( machine -> dapm_event ) <nl> - machine -> dapm_event ( machine , event ); <nl> + machine -> dapm_event ( machine , event ); <nl> if ( codec -> dapm_event ) <nl> - codec -> dapm_event ( codec , event ); <nl> + codec -> dapm_event ( codec , event ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_device_event );
void * knav_pool_create ( const char * name , <nl> bool slot_found ; <nl> int ret ; <nl>  <nl> + if (! kdev ) <nl> + return ERR_PTR (- EPROBE_DEFER ); <nl> + <nl> if (! kdev -> dev ) <nl> return ERR_PTR (- ENODEV ); <nl> 
static void release_sub_crqs ( struct ibmvnic_adapter * adapter ) <nl> release_sub_crq_queue ( adapter , <nl> adapter -> tx_scrq [ i ]); <nl> } <nl> + kfree ( adapter -> tx_scrq ); <nl> adapter -> tx_scrq = NULL ; <nl> } <nl>  <nl> static void release_sub_crqs ( struct ibmvnic_adapter * adapter ) <nl> release_sub_crq_queue ( adapter , <nl> adapter -> rx_scrq [ i ]); <nl> } <nl> + kfree ( adapter -> rx_scrq ); <nl> adapter -> rx_scrq = NULL ; <nl> } <nl> }
void iio_disable_all_buffers ( struct iio_dev * indio_dev ) <nl> indio_dev -> currentmode = INDIO_DIRECT_MODE ; <nl> if ( indio_dev -> setup_ops -> postdisable ) <nl> indio_dev -> setup_ops -> postdisable ( indio_dev ); <nl> + <nl> + if ( indio_dev -> available_scan_masks == NULL ) <nl> + kfree ( indio_dev -> active_scan_mask ); <nl> } <nl>  <nl> int iio_update_buffers ( struct iio_dev * indio_dev ,
static int64_t _sort__sym_cmp ( struct symbol * sym_l , struct symbol * sym_r ) <nl> if ( sym_l == sym_r ) <nl> return 0 ; <nl>  <nl> + if ( sym_l -> inlined || sym_r -> inlined ) <nl> + return strcmp ( sym_l -> name , sym_r -> name ); <nl> + <nl> if ( sym_l -> start != sym_r -> start ) <nl> return ( int64_t )( sym_r -> start - sym_l -> start ); <nl> 
static int hpsa_eh_device_reset_handler ( struct scsi_cmnd * scsicmd ) <nl> return FAILED ; <nl> } <nl>  <nl> + if ( dev -> devtype == TYPE_ENCLOSURE ) <nl> + return SUCCESS ; <nl> + <nl> /* if controller locked up , we can guarantee command won ' t complete */ <nl> if ( lockup_detected ( h )) { <nl> snprintf ( msg , sizeof ( msg ),
static int copy_to_user_tmpl ( struct xfrm_policy * xp , struct sk_buff * skb ) <nl> struct xfrm_user_tmpl * up = & vec [ i ]; <nl> struct xfrm_tmpl * kp = & xp -> xfrm_vec [ i ]; <nl>  <nl> + memset ( up , 0 , sizeof (* up )); <nl> memcpy (& up -> id , & kp -> id , sizeof ( up -> id )); <nl> up -> family = kp -> encap_family ; <nl> memcpy (& up -> saddr , & kp -> saddr , sizeof ( up -> saddr ));
static void sl_tx_timeout ( struct net_device * dev , unsigned int txqueue ) <nl> spin_lock (& sl -> lock ); <nl>  <nl> if ( netif_queue_stopped ( dev )) { <nl> - if (! netif_running ( dev )) <nl> + if (! netif_running ( dev ) || ! sl -> tty ) <nl> goto out ; <nl>  <nl> /* May be we must check transmitter timeout here ?
long vhost_dev_ioctl ( struct vhost_dev * d , unsigned int ioctl , void __user * argp ) <nl> } <nl> if ( eventfp != d -> log_file ) { <nl> filep = d -> log_file ; <nl> + d -> log_file = eventfp ; <nl> ctx = d -> log_ctx ; <nl> d -> log_ctx = eventfp ? <nl> eventfd_ctx_fileget ( eventfp ) : NULL ;
static int spear_smi_probe_config_dt ( struct platform_device * pdev , <nl> pdata -> board_flash_info = devm_kzalloc (& pdev -> dev , <nl> sizeof (* pdata -> board_flash_info ), <nl> GFP_KERNEL ); <nl> + if (! pdata -> board_flash_info ) <nl> + return - ENOMEM ; <nl>  <nl> /* Fill structs for each subnode ( flash device ) */ <nl> while (( pp = of_get_next_child ( np , pp ))) {
decompress_fn __init decompress_method ( const unsigned char * inbuf , long len , <nl> { <nl> const struct compress_format * cf ; <nl>  <nl> - if ( len < 2 ) <nl> + if ( len < 2 ) { <nl> + if ( name ) <nl> + * name = NULL ; <nl> return NULL ; /* Need at least this much ... */ <nl> + } <nl>  <nl> pr_debug (" Compressed data magic : %#. 2x %#. 2x \ n ", inbuf [ 0 ], inbuf [ 1 ]); <nl> 
static void execlists_submission_tasklet ( unsigned long data ) <nl> trace_i915_request_out ( rq ); <nl> i915_request_put ( rq ); <nl>  <nl> + GEM_TRACE ("% s completed ctx =% d \ n ", <nl> + engine -> name , port -> context_id ); <nl> + <nl> execlists_port_complete ( execlists , port ); <nl> } else { <nl> port_set ( port , port_pack ( rq , count ));
intel_sdvo_tv_init ( struct intel_sdvo * intel_sdvo , int type ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl>  <nl> intel_sdvo_lvds_init ( struct intel_sdvo * intel_sdvo , int device ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl> 
static bool msr_mtrr_valid ( unsigned msr ) <nl> case MSR_MTRRdefType : <nl> case MSR_IA32_CR_PAT : <nl> return true ; <nl> - case 0x2f8 : <nl> - return true ; <nl> } <nl> return false ; <nl> }
static int nr_listen ( struct socket * sock , int backlog ) <nl> struct sock * sk = sock -> sk ; <nl>  <nl> lock_sock ( sk ); <nl> + if ( sock -> state != SS_UNCONNECTED ) { <nl> + release_sock ( sk ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( sk -> sk_state != TCP_LISTEN ) { <nl> memset (& nr_sk ( sk )-> user_addr , 0 , AX25_ADDR_LEN ); <nl> sk -> sk_max_ack_backlog = backlog ;
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> fail_workqueue : <nl> wiphy_unregister ( local -> hw . wiphy ); <nl> fail_wiphy_register : <nl> - kfree ( local -> int_scan_req -> channels ); <nl> + kfree ( local -> int_scan_req ); <nl> return result ; <nl> } <nl> EXPORT_SYMBOL ( ieee80211_register_hw );
static void cache_set_flush ( struct closure * cl ) <nl> struct btree * b ; <nl> unsigned i ; <nl>  <nl> + if (! c ) <nl> + closure_return ( cl ); <nl> + <nl> bch_cache_accounting_destroy (& c -> accounting ); <nl>  <nl> kobject_put (& c -> internal );
rio_dma_transfer ( struct file * filp , u32 transfer_mode , <nl> goto err_req ; <nl> } <nl>  <nl> - pinned = get_user_pages_unlocked ( <nl> + pinned = get_user_pages_fast ( <nl> ( unsigned long ) xfer -> loc_addr & PAGE_MASK , <nl> - nr_pages , <nl> - page_list , <nl> - dir == DMA_FROM_DEVICE ? FOLL_WRITE : 0 ); <nl> + nr_pages , dir == DMA_FROM_DEVICE , page_list ); <nl>  <nl> if ( pinned != nr_pages ) { <nl> if ( pinned < 0 ) {
static int set_gamma ( struct fbtft_par * par , u32 * curves ) <nl> * The masks are the same for both positive and negative voltage <nl> * gamma curves . <nl> */ <nl> - const u8 gamma_par_mask [] = { <nl> + static const u8 gamma_par_mask [] = { <nl> 0xFF , /* V63 [ 3 : 0 ], V0 [ 3 : 0 ]*/ <nl> 0x3F , /* V1 [ 5 : 0 ] */ <nl> 0x3F , /* V2 [ 5 : 0 ] */
static int rk_hw_params ( struct snd_pcm_substream * substream , <nl> case 96000 : <nl> mclk = 12288000 ; <nl> break ; <nl> + case 192000 : <nl> + mclk = 24576000 ; <nl> + break ; <nl> case 11025 : <nl> case 22050 : <nl> case 44100 :
static int snd_usb_copy_string_desc ( struct mixer_build * state , <nl> int index , char * buf , int maxlen ) <nl> { <nl> int len = usb_string ( state -> chip -> dev , index , buf , maxlen - 1 ); <nl> + <nl> + if ( len < 0 ) <nl> + return 0 ; <nl> + <nl> buf [ len ] = 0 ; <nl> return len ; <nl> }
static inline clock_t jiffies_delta_to_clock_t ( long delta ) <nl> return jiffies_to_clock_t ( max ( 0L , delta )); <nl> } <nl>  <nl> + static inline unsigned int jiffies_delta_to_msecs ( long delta ) <nl> +{ <nl> + return jiffies_to_msecs ( max ( 0L , delta )); <nl> +} <nl> + <nl> extern unsigned long clock_t_to_jiffies ( unsigned long x ); <nl> extern u64 jiffies_64_to_clock_t ( u64 x ); <nl> extern u64 nsec_to_clock_t ( u64 x );
static inline int virtqueue_add ( struct virtqueue * _vq , <nl> * host should service the ring ASAP . */ <nl> if ( out_sgs ) <nl> vq -> notify (& vq -> vq ); <nl> + if ( indirect ) <nl> + kfree ( desc ); <nl> END_USE ( vq ); <nl> return - ENOSPC ; <nl> }
int thermal_add_hwmon_sysfs ( struct thermal_zone_device * tz ) <nl>  <nl> INIT_LIST_HEAD (& hwmon -> tz_list ); <nl> strlcpy ( hwmon -> type , tz -> type , THERMAL_NAME_LENGTH ); <nl> - hwmon -> device = hwmon_device_register ( NULL ); <nl> + hwmon -> device = hwmon_device_register (& tz -> device ); <nl> if ( IS_ERR ( hwmon -> device )) { <nl> result = PTR_ERR ( hwmon -> device ); <nl> goto free_mem ;
int radeon_suspend_kms ( struct drm_device * dev , bool suspend , <nl> radeon_agp_suspend ( rdev ); <nl>  <nl> pci_save_state ( dev -> pdev ); <nl> - if ( freeze && rdev -> family >= CHIP_R600 ) { <nl> + if ( freeze && rdev -> family >= CHIP_CEDAR ) { <nl> rdev -> asic -> asic_reset ( rdev , true ); <nl> pci_restore_state ( dev -> pdev ); <nl> } else if ( suspend ) {
static int mlxsw_sp_ageing_set ( struct mlxsw_sp * mlxsw_sp , u32 ageing_time ) <nl>  <nl> static int mlxsw_sp_port_attr_br_ageing_set ( struct mlxsw_sp_port * mlxsw_sp_port , <nl> struct switchdev_trans * trans , <nl> - unsigned long ageing_jiffies ) <nl> + unsigned long ageing_clock_t ) <nl> { <nl> struct mlxsw_sp * mlxsw_sp = mlxsw_sp_port -> mlxsw_sp ; <nl> + unsigned long ageing_jiffies = clock_t_to_jiffies ( ageing_clock_t ); <nl> u32 ageing_time = jiffies_to_msecs ( ageing_jiffies ) / 1000 ; <nl>  <nl> if ( switchdev_trans_ph_prepare ( trans ))
static int __devinit hvcs_initialize ( void ) <nl> num_ttys_to_alloc = hvcs_parm_num_devs ; <nl>  <nl> hvcs_tty_driver = alloc_tty_driver ( num_ttys_to_alloc ); <nl> - if (! hvcs_tty_driver ) <nl> + if (! hvcs_tty_driver ) { <nl> + mutex_unlock (& hvcs_init_mutex ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if ( hvcs_alloc_index_list ( num_ttys_to_alloc )) { <nl> rc = - ENOMEM ;
int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
qeth_l3_add_mc_to_hash ( struct qeth_card * card , struct in_device * in4_dev ) <nl>  <nl> tmp -> u . a4 . addr = be32_to_cpu ( im4 -> multiaddr ); <nl> memcpy ( tmp -> mac , buf , sizeof ( tmp -> mac )); <nl> + tmp -> is_multicast = 1 ; <nl>  <nl> ipm = qeth_l3_ip_from_hash ( card , tmp ); <nl> if ( ipm ) {
static int parse_output ( struct hda_codec * codec ) <nl> memcpy ( cfg -> speaker_pins , cfg -> line_out_pins , <nl> sizeof ( cfg -> speaker_pins )); <nl> cfg -> line_outs = 0 ; <nl> + memset ( cfg -> line_out_pins , 0 , sizeof ( cfg -> line_out_pins )); <nl> } <nl>  <nl> return 0 ;
copy_user_handle_tail ( char * to , char * from , unsigned len , unsigned zerorest ) <nl> char c ; <nl> unsigned zero_len ; <nl>  <nl> - for (; len ; -- len ) { <nl> + for (; len ; -- len , to ++) { <nl> if ( __get_user_nocheck ( c , from ++, sizeof ( char ))) <nl> break ; <nl> - if ( __put_user_nocheck ( c , to ++, sizeof ( char ))) <nl> + if ( __put_user_nocheck ( c , to , sizeof ( char ))) <nl> break ; <nl> } <nl> 
int iio_buffer_register ( struct iio_dev * indio_dev , <nl> if ( channels ) { <nl> /* new magic */ <nl> for ( i = 0 ; i < num_channels ; i ++) { <nl> + if ( channels [ i ]. scan_index < 0 ) <nl> + continue ; <nl> + <nl> /* Establish necessary mask length */ <nl> if ( channels [ i ]. scan_index > <nl> ( int ) indio_dev -> masklength - 1 )
struct request_queue * __scsi_alloc_queue ( struct Scsi_Host * shost , <nl> request_fn_proc * request_fn ) <nl> { <nl> struct request_queue * q ; <nl> - struct device * dev = shost -> shost_gendev . parent ; <nl> + struct device * dev = shost -> dma_dev ; <nl>  <nl> q = blk_init_queue ( request_fn , NULL ); <nl> if (! q )
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( sta -> sta . txq [ 0 ]) <nl> kfree ( to_txq_info ( sta -> sta . txq [ 0 ])); <nl> free : <nl> + free_percpu ( sta -> pcpu_rx_stats ); <nl> # ifdef CONFIG_MAC80211_MESH <nl> kfree ( sta -> mesh ); <nl> # endif
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , u32 features ) <nl> skb -> ip_summed = CHECKSUM_NONE ; <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> - if (( skb_headroom ( skb ) < frag_hdr_sz ) && <nl> + if (( skb_mac_header ( skb ) < skb -> head + frag_hdr_sz ) && <nl> pskb_expand_head ( skb , frag_hdr_sz , 0 , GFP_ATOMIC )) <nl> goto out ; <nl> 
void parse_events__free_terms ( struct list_head * terms ) <nl>  <nl> list_for_each_entry_safe ( term , h , terms , list ) <nl> free ( term ); <nl> - <nl> - free ( terms ); <nl> }
static void hub_activate ( struct usb_hub * hub , enum hub_activation_type type ) <nl> PREPARE_DELAYED_WORK (& hub -> init_work , hub_init_func2 ); <nl> schedule_delayed_work (& hub -> init_work , <nl> msecs_to_jiffies ( delay )); <nl> + <nl> + /* Suppress autosuspend until init is done */ <nl> + to_usb_interface ( hub -> intfdev )-> pm_usage_cnt = 1 ; <nl> return ; /* Continues at init2 : below */ <nl> } else { <nl> hub_power_on ( hub , true );
static int find_first_block_group ( struct btrfs_root * root , <nl> } else { <nl> ret = 0 ; <nl> } <nl> + free_extent_map ( em ); <nl> goto out ; <nl> } <nl> path -> slots [ 0 ]++;
static byte connect_res ( dword Id , word Number , DIVA_CAPI_ADAPTER * a , <nl> add_ai ( plci , & parms [ 5 ]); <nl> sig_req ( plci , REJECT , 0 ); <nl> } <nl> - else if ( Reject == 1 || Reject > 9 ) <nl> + else if ( Reject == 1 || Reject >= 9 ) <nl> { <nl> add_ai ( plci , & parms [ 5 ]); <nl> sig_req ( plci , HANGUP , 0 );
static int start_afu ( struct cxlflash_cfg * cfg ) <nl>  <nl> init_pcr ( cfg ); <nl>  <nl> + /* After an AFU reset , RRQ entries are stale , clear them */ <nl> + memset (& afu -> rrq_entry , 0 , sizeof ( afu -> rrq_entry )); <nl> + <nl> /* Initialize RRQ pointers */ <nl> afu -> hrrq_start = & afu -> rrq_entry [ 0 ]; <nl> afu -> hrrq_end = & afu -> rrq_entry [ NUM_RRQ_ENTRY - 1 ];
qla25xx_process_bidir_status_iocb ( scsi_qla_host_t * vha , void * pkt , <nl> bsg_job -> reply_len = sizeof ( struct fc_bsg_reply ); <nl> /* Always return DID_OK , bsg will send the vendor specific response <nl> * in this case only */ <nl> - sp -> done ( sp , DID_OK << 6 ); <nl> + sp -> done ( sp , DID_OK << 16 ); <nl>  <nl> } <nl> 
sctp_disposition_t sctp_sf_do_5_2_4_dupcook ( struct net * net , <nl> } <nl>  <nl> /* Delete the tempory new association . */ <nl> - sctp_add_cmd_sf ( commands , SCTP_CMD_NEW_ASOC , SCTP_ASOC ( new_asoc )); <nl> + sctp_add_cmd_sf ( commands , SCTP_CMD_SET_ASOC , SCTP_ASOC ( new_asoc )); <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_DELETE_TCB , SCTP_NULL ()); <nl>  <nl> /* Restore association pointer to provide SCTP command interpeter
static void fcopy_send_data ( struct work_struct * dummy ) <nl> out_src = smsg_out ; <nl> break ; <nl>  <nl> + case WRITE_TO_FILE : <nl> + out_src = fcopy_transaction . fcopy_msg ; <nl> + out_len = sizeof ( struct hv_do_fcopy ); <nl> + break ; <nl> default : <nl> out_src = fcopy_transaction . fcopy_msg ; <nl> out_len = fcopy_transaction . recv_len ;
struct inode * ext2_new_inode ( struct inode * dir , umode_t mode , <nl>  <nl> for ( i = 0 ; i < sbi -> s_groups_count ; i ++) { <nl> gdp = ext2_get_group_desc ( sb , group , & bh2 ); <nl> + if (! gdp ) { <nl> + if (++ group == sbi -> s_groups_count ) <nl> + group = 0 ; <nl> + continue ; <nl> + } <nl> brelse ( bitmap_bh ); <nl> bitmap_bh = read_inode_bitmap ( sb , group ); <nl> if (! bitmap_bh ) {
static int sdhci_acpi_probe ( struct platform_device * pdev ) <nl> host -> hw_name = " ACPI "; <nl> host -> ops = & sdhci_acpi_ops_dflt ; <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq <= 0 ) { <nl> + err = - EINVAL ; <nl> + goto err_free ; <nl> + } <nl>  <nl> host -> ioaddr = devm_ioremap_nocache ( dev , iomem -> start , <nl> resource_size ( iomem ));
static int xfrm_del_sa ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl>  <nl> static void copy_to_user_state ( struct xfrm_state * x , struct xfrm_usersa_info * p ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> id , & x -> id , sizeof ( p -> id )); <nl> memcpy (& p -> sel , & x -> sel , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & x -> lft , sizeof ( p -> lft ));
static inline void x86_assign_hw_event ( struct perf_event * event , <nl> hwc -> event_base = 0 ; <nl> } else if ( hwc -> idx >= X86_PMC_IDX_FIXED ) { <nl> hwc -> config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL ; <nl> - hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 ; <nl> + hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + ( hwc -> idx - X86_PMC_IDX_FIXED ); <nl> } else { <nl> hwc -> config_base = x86_pmu_config_addr ( hwc -> idx ); <nl> hwc -> event_base = x86_pmu_event_addr ( hwc -> idx );
void __iomem * __ioremap ( unsigned long addr , unsigned long size , <nl> pa = addr & PAGE_MASK ; <nl> size = PAGE_ALIGN ( addr + size ) - pa ; <nl>  <nl> - if ( size == 0 ) <nl> + if (( size == 0 ) || ( pa == 0 )) <nl> return NULL ; <nl>  <nl> if ( mem_init_done ) {
static netdev_tx_t w83977af_hard_xmit ( struct sk_buff * skb , <nl>  <nl> mtt = irda_get_mtt ( skb ); <nl> pr_debug ("% s (% ld ), mtt =% d \ n ", __func__ , jiffies , mtt ); <nl> - if ( mtt ) <nl> + if ( mtt > 1000 ) <nl> + mdelay ( mtt / 1000 ); <nl> + else if ( mtt ) <nl> udelay ( mtt ); <nl>  <nl> /* Enable DMA interrupt */
void __init reserve_early_overlap_ok ( u64 start , u64 end , char * name ) <nl> */ <nl> void __init reserve_early ( u64 start , u64 end , char * name ) <nl> { <nl> + if ( start >= end ) <nl> + return ; <nl> + <nl> drop_overlaps_that_are_ok ( start , end ); <nl> __reserve_early ( start , end , name , 0 ); <nl> }
static void mcam_ctlr_image ( struct mcam_camera * cam ) <nl> mcam_reg_write_mask ( cam , REG_CTRL0 , <nl> C0_DF_RGB | C0_RGBF_565 | C0_RGB5_BGGR , C0_DF_MASK ); <nl> break ; <nl> + case V4L2_PIX_FMT_SBGGR8 : <nl> + mcam_reg_write_mask ( cam , REG_CTRL0 , <nl> + C0_DF_RGB | C0_RGB5_GRBG , C0_DF_MASK ); <nl> + break ; <nl> default : <nl> cam_err ( cam , " camera : unknown format : %# x \ n ", fmt -> pixelformat ); <nl> break ;
static int sched_read_attr ( struct sched_attr __user * uattr , <nl> attr -> size = usize ; <nl> } <nl>  <nl> - ret = copy_to_user ( uattr , attr , usize ); <nl> + ret = copy_to_user ( uattr , attr , attr -> size ); <nl> if ( ret ) <nl> return - EFAULT ; <nl> 
static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
static irqreturn_t ab8500_irq ( int irq , void * dev ) <nl> do { <nl> int bit = __ffs ( value ); <nl> int line = i * 8 + bit ; <nl> + int virq = ab8500_irq_get_virq ( ab8500 , line ); <nl>  <nl> - handle_nested_irq ( ab8500 -> irq_base + line ); <nl> + handle_nested_irq ( virq ); <nl> value &= ~( 1 << bit ); <nl>  <nl> } while ( value );
static int f2fs_move_file_range ( struct file * file_in , loff_t pos_in , <nl> if ( f2fs_encrypted_inode ( src ) || f2fs_encrypted_inode ( dst )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( src == dst ) { <nl> + if ( pos_in == pos_out ) <nl> + return 0 ; <nl> + if ( pos_out > pos_in && pos_out < pos_in + len ) <nl> + return - EINVAL ; <nl> + } <nl> + <nl> inode_lock ( src ); <nl> if ( src != dst ) { <nl> if (! inode_trylock ( dst )) {
void free_fib_info ( struct fib_info * fi ) <nl> # endif <nl> call_rcu (& fi -> rcu , free_fib_info_rcu ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( free_fib_info ); <nl>  <nl> void fib_release_info ( struct fib_info * fi ) <nl> {
static void __unregister_request ( struct ceph_osd_client * osdc , <nl> */ <nl> static void __cancel_request ( struct ceph_osd_request * req ) <nl> { <nl> - if ( req -> r_sent ) { <nl> + if ( req -> r_sent && req -> r_osd ) { <nl> ceph_con_revoke (& req -> r_osd -> o_con , req -> r_request ); <nl> req -> r_sent = 0 ; <nl> }
static bool intel_sdvo_detect_hdmi_audio ( struct drm_connector * connector ) <nl> edid = intel_sdvo_get_edid ( connector ); <nl> if ( edid != NULL && edid -> input & DRM_EDID_INPUT_DIGITAL ) <nl> has_audio = drm_detect_monitor_audio ( edid ); <nl> + kfree ( edid ); <nl>  <nl> return has_audio ; <nl> }
DOT11D_GetMaxTxPwrInDbm ( <nl> netdev_info ( dev -> dev , " DOT11D_GetMaxTxPwrInDbm (): Invalid Channel \ n "); <nl> return MaxTxPwrInDbm ; <nl> } <nl> - if ( pDot11dInfo -> channel_map [ Channel ]) { <nl> + if ( pDot11dInfo -> channel_map [ Channel ]) <nl> MaxTxPwrInDbm = pDot11dInfo -> MaxTxPwrDbmList [ Channel ]; <nl> - } <nl>  <nl> return MaxTxPwrInDbm ; <nl> }
static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl>  <nl> mutex_lock (& dev -> lock ); <nl>  <nl> - if ( dev -> open ++) <nl> + if ( dev -> open ) <nl> goto unlock ; <nl>  <nl> kref_get (& dev -> ref ); <nl> static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl> goto error_release ; <nl>  <nl> unlock : <nl> + dev -> open ++; <nl> mutex_unlock (& dev -> lock ); <nl> blktrans_dev_put ( dev ); <nl> return ret ;
int compat_get_timex ( struct timex * txc , const struct compat_timex __user * utp ) <nl> { <nl> struct compat_timex tx32 ; <nl>  <nl> + memset ( txc , 0 , sizeof ( struct timex )); <nl> if ( copy_from_user (& tx32 , utp , sizeof ( struct compat_timex ))) <nl> return - EFAULT ; <nl> 
int iio_sw_buffer_preenable ( struct iio_dev * indio_dev ) <nl> buffer -> scan_mask ); <nl> else <nl> indio_dev -> active_scan_mask = buffer -> scan_mask ; <nl> + <nl> + if ( indio_dev -> active_scan_mask == NULL ) <nl> + return - EINVAL ; <nl> + <nl> iio_update_demux ( indio_dev ); <nl>  <nl> if ( indio_dev -> info -> update_scan_mode )
int ipu_dp_init ( struct ipu_soc * ipu , struct device * dev , unsigned long base ) <nl> int i ; <nl>  <nl> priv = devm_kzalloc ( dev , sizeof (* priv ), GFP_KERNEL ); <nl> + if (! priv ) <nl> + return - ENOMEM ; <nl> priv -> dev = dev ; <nl> priv -> ipu = ipu ; <nl> 
static int conf_choice ( struct menu * menu ) <nl> } <nl> if (! child ) <nl> continue ; <nl> - if ( line [ strlen ( line ) - 1 ] == '?') { <nl> + if ( line [ 0 ] && line [ strlen ( line ) - 1 ] == '?') { <nl> print_help ( child ); <nl> continue ; <nl> }
static const char * ext4_decode_error ( struct super_block * sb , int errno , <nl> errstr = " Out of memory "; <nl> break ; <nl> case - EROFS : <nl> - if (! sb || EXT4_SB ( sb )-> s_journal -> j_flags & JBD2_ABORT ) <nl> + if (! sb || ( EXT4_SB ( sb )-> s_journal && <nl> + EXT4_SB ( sb )-> s_journal -> j_flags & JBD2_ABORT )) <nl> errstr = " Journal has aborted "; <nl> else <nl> errstr = " Readonly filesystem ";
static int vidioc_try_fmt_vid_cap ( struct file * file , void * priv , <nl> else <nl> f -> fmt . pix . field = dev -> interlaced ? <nl> V4L2_FIELD_INTERLACED : V4L2_FIELD_TOP ; <nl> + f -> fmt . pix . priv = 0 ; <nl>  <nl> return 0 ; <nl> }
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> goto cleanup ; <nl> } <nl>  <nl> - if ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr ))) { <nl> + if (( fibsize < ( sizeof ( struct user_aac_srb ) - sizeof ( struct user_sgentry ))) || <nl> + ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr )))) { <nl> rcode = - EINVAL ; <nl> goto cleanup ; <nl> }
xfs_da3_fixhashpath ( <nl> node = blk -> bp -> b_addr ; <nl> dp -> d_ops -> node_hdr_from_disk (& nodehdr , node ); <nl> btree = dp -> d_ops -> node_tree_p ( node ); <nl> - if ( be32_to_cpu ( btree -> hashval ) == lasthash ) <nl> + if ( be32_to_cpu ( btree [ blk -> index ]. hashval ) == lasthash ) <nl> break ; <nl> blk -> hashval = lasthash ; <nl> btree [ blk -> index ]. hashval = cpu_to_be32 ( lasthash );
static void btc8821a2ant_run_coexist_mechanism ( struct btc_coexist * btcoexist ) <nl> return ; <nl> } <nl>  <nl> + if ( coex_sta -> under_ips ) { <nl> + RT_TRACE ( rtlpriv , COMP_BT_COEXIST , DBG_LOUD , <nl> + "[ BTCoex ], wifi is under IPS !!!\ n "); <nl> + return ; <nl> + } <nl> + <nl> algorithm = btc8821a2ant_action_algorithm ( btcoexist ); <nl> if ( coex_sta -> c2h_bt_inquiry_page && <nl> ( BT_8821A_2ANT_COEX_ALGO_PANHS != algorithm )) {
asmlinkage int sys_rt_sigreturn ( struct pt_regs * regs ) <nl> if ( restore_sigcontext ( regs , & frame -> uc . uc_mcontext )) <nl> goto badframe ; <nl>  <nl> + if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> sp ) == - EFAULT ) <nl> + goto badframe ; <nl> + <nl> pr_debug (" Context restored : pc = % 08lx , lr = % 08lx , sp = % 08lx \ n ", <nl> regs -> pc , regs -> lr , regs -> sp ); <nl> 
static int mmc_ext_csd_open ( struct inode * inode , struct file * filp ) <nl> if ( err ) <nl> goto out_free ; <nl>  <nl> - for ( i = 511 ; i >= 0 ; i --) <nl> + for ( i = 0 ; i < 512 ; i ++) <nl> n += sprintf ( buf + n , "% 02x ", ext_csd [ i ]); <nl> n += sprintf ( buf + n , "\ n "); <nl> BUG_ON ( n != EXT_CSD_STR_LEN );
static __init int hardware_setup ( void ) <nl> goto out ; <nl> } <nl>  <nl> - vmx_io_bitmap_b = ( unsigned long *) __get_free_page ( GFP_KERNEL ); <nl> memset ( vmx_vmread_bitmap , 0xff , PAGE_SIZE ); <nl> memset ( vmx_vmwrite_bitmap , 0xff , PAGE_SIZE ); <nl> 
static inline int copy_linear_skb ( struct sk_buff * skb , int len , int off , <nl> { <nl> int n , copy = len - off ; <nl>  <nl> + if ( copy < 0 ) <nl> + return - EINVAL ; <nl> n = copy_to_iter ( skb -> data + off , copy , to ); <nl> if ( n == copy ) <nl> return 0 ;
static int ci_get_platdata ( struct device * dev , <nl> return ret ; <nl> } <nl>  <nl> + if ( of_find_property ( dev -> of_node , " non - zero - ttctrl - ttha ", NULL )) <nl> + platdata -> flags |= CI_HDRC_SET_NON_ZERO_TTHA ; <nl> + <nl> ext_id = ERR_PTR (- ENODEV ); <nl> ext_vbus = ERR_PTR (- ENODEV ); <nl> if ( of_property_read_bool ( dev -> of_node , " extcon ")) {
void drm_mode_config_reset ( struct drm_device * dev ) <nl> if ( encoder -> funcs -> reset ) <nl> encoder -> funcs -> reset ( encoder ); <nl>  <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> drm_for_each_connector ( connector , dev ) { <nl> connector -> status = connector_status_unknown ; <nl>  <nl> if ( connector -> funcs -> reset ) <nl> connector -> funcs -> reset ( connector ); <nl> } <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl> } <nl> EXPORT_SYMBOL ( drm_mode_config_reset ); <nl> 
static void mtk_plane_atomic_update ( struct drm_plane * plane , <nl> pitch = fb -> pitches [ 0 ]; <nl> format = fb -> pixel_format ; <nl>  <nl> - addr += ( plane -> state -> src . x1 >> 16 ) * 4 ; <nl> + addr += ( plane -> state -> src . x1 >> 16 ) * drm_format_plane_cpp ( format , 0 ); <nl> addr += ( plane -> state -> src . y1 >> 16 ) * pitch ; <nl>  <nl> state -> pending . enable = true ;
int ip6_append_data ( struct sock * sk , int getfrag ( void * from , char * to , <nl> if ( WARN_ON ( np -> cork . opt )) <nl> return - EINVAL ; <nl>  <nl> - np -> cork . opt = kmalloc ( opt -> tot_len , sk -> sk_allocation ); <nl> + np -> cork . opt = kzalloc ( opt -> tot_len , sk -> sk_allocation ); <nl> if ( unlikely ( np -> cork . opt == NULL )) <nl> return - ENOBUFS ; <nl> 
static int snd_compress_check_input ( struct snd_compr_params * params ) <nl> { <nl> /* first let ' s check the buffer parameter ' s */ <nl> if ( params -> buffer . fragment_size == 0 || <nl> - params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + params -> buffer . fragments > INT_MAX / params -> buffer . fragment_size ) <nl> return - EINVAL ; <nl>  <nl> /* now codec parameters */
static bool event_compare ( struct fsnotify_event * old , struct fsnotify_event * new <nl> /* remember , after old was put on the wait_q we aren ' t <nl> * allowed to look at the inode any more , only thing <nl> * left to check was if the file_name is the same */ <nl> - if ( old -> name_len && <nl> + if (! old -> name_len || <nl> ! strcmp ( old -> file_name , new -> file_name )) <nl> return true ; <nl> break ;
static void bmc150_accel_unregister_triggers ( struct bmc150_accel_data * data , <nl> { <nl> int i ; <nl>  <nl> - for ( i = from ; i >= 0 ; i ++) { <nl> + for ( i = from ; i >= 0 ; i --) { <nl> if ( data -> triggers [ i ]. indio_trig ) { <nl> iio_trigger_unregister ( data -> triggers [ i ]. indio_trig ); <nl> data -> triggers [ i ]. indio_trig = NULL ;
int bnx2i_send_iscsi_nopout ( struct bnx2i_conn * bnx2i_conn , <nl> bnx2i_cmd = ( struct bnx2i_cmd *) task -> dd_data ; <nl> nopout_hdr = ( struct iscsi_nopout *) task -> hdr ; <nl> nopout_wqe = ( struct bnx2i_nop_out_request *) ep -> qp . sq_prod_qe ; <nl> + <nl> + memset ( nopout_wqe , 0x00 , sizeof ( struct bnx2i_nop_out_request )); <nl> + <nl> nopout_wqe -> op_code = nopout_hdr -> opcode ; <nl> nopout_wqe -> op_attr = ISCSI_FLAG_CMD_FINAL ; <nl> memcpy ( nopout_wqe -> lun , nopout_hdr -> lun , 8 );
static int setup_rt_frame ( int sig , struct k_sigaction * ka , siginfo_t * info , <nl> } else { <nl> regs -> gprs [ 14 ] = ( unsigned long ) <nl> frame -> retcode | PSW_ADDR_AMODE ; <nl> - err |= __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> - ( u16 __user *)( frame -> retcode )); <nl> + if ( __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> + ( u16 __user *)( frame -> retcode ))) <nl> + goto give_sigsegv ; <nl> } <nl>  <nl> /* Set up backchain . */
nlmsvc_lock ( struct svc_rqst * rqstp , struct nlm_file * file , <nl> goto out ; <nl> case - EAGAIN : <nl> ret = nlm_lck_denied ; <nl> - goto out ; <nl> + break ; <nl> case FILE_LOCK_DEFERRED : <nl> if ( wait ) <nl> break ; <nl> nlmsvc_lock ( struct svc_rqst * rqstp , struct nlm_file * file , <nl> goto out ; <nl> } <nl>  <nl> + ret = nlm_lck_denied ; <nl> + if (! wait ) <nl> + goto out ; <nl> + <nl> ret = nlm_lck_blocked ; <nl>  <nl> /* Append to list of blocked */
static int ip_frag_reasm ( struct ipq * qp , struct sk_buff * prev , <nl> skb_morph ( head , qp -> q . fragments ); <nl> head -> next = qp -> q . fragments -> next ; <nl>  <nl> - kfree_skb ( qp -> q . fragments ); <nl> + consume_skb ( qp -> q . fragments ); <nl> qp -> q . fragments = head ; <nl> } <nl> 
static noinline void key_gc_unused_keys ( struct list_head * keys ) <nl> if ( test_bit ( KEY_FLAG_INSTANTIATED , & key -> flags )) <nl> atomic_dec (& key -> user -> nikeys ); <nl>  <nl> - key_user_put ( key -> user ); <nl> - <nl> /* now throw away the key memory */ <nl> if ( key -> type -> destroy ) <nl> key -> type -> destroy ( key ); <nl>  <nl> + key_user_put ( key -> user ); <nl> + <nl> kfree ( key -> description ); <nl>  <nl> # ifdef KEY_DEBUGGING
void __init at91_gpio_irq_setup ( void ) <nl> irq_set_chip_data ( id , this ); <nl> irq_set_chained_handler ( id , gpio_irq_handler ); <nl> } <nl> - pr_info (" AT91 : % d gpio irqs in % d banks \ n ", irq , gpio_banks ); <nl> + pr_info (" AT91 : % d gpio irqs in % d banks \ n ", irq - gpio_to_irq ( 0 ), gpio_banks ); <nl> } <nl>  <nl> /* gpiolib support */
static long bcm_char_ioctl ( struct file * filp , UINT cmd , ULONG arg ) <nl>  <nl> BCM_DEBUG_PRINT ( Adapter , DBG_TYPE_OTHERS , OSAL_DBG , DBG_LVL_ALL , " Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO \ n "); <nl>  <nl> + memset (& DevInfo , 0 , sizeof ( DevInfo )); <nl> DevInfo . MaxRDMBufferSize = BUFFER_4K ; <nl> DevInfo . u32DSDStartOffset = EEPROM_CALPARAM_START ; <nl> DevInfo . u32RxAlignmentCorrection = 0 ;
i915_gem_do_execbuffer ( struct drm_device * dev , void * data , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( args -> num_cliprects > UINT_MAX / sizeof (* cliprects )) { <nl> + DRM_DEBUG (" execbuf with % u cliprects \ n ", <nl> + args -> num_cliprects ); <nl> + return - EINVAL ; <nl> + } <nl> cliprects = kmalloc ( args -> num_cliprects * sizeof (* cliprects ), <nl> GFP_KERNEL ); <nl> if ( cliprects == NULL ) {
bool ieee80211_set_channel_type ( struct ieee80211_local * local , <nl> switch ( tmp -> vif . bss_conf . channel_type ) { <nl> case NL80211_CHAN_NO_HT : <nl> case NL80211_CHAN_HT20 : <nl> + if ( superchan > tmp -> vif . bss_conf . channel_type ) <nl> + break ; <nl> + <nl> superchan = tmp -> vif . bss_conf . channel_type ; <nl> break ; <nl> case NL80211_CHAN_HT40PLUS :
static int pcmciamtd_config ( struct pcmcia_device * link ) <nl> } <nl> dev_info (& dev -> p_dev -> dev , " mtd % d : % s \ n ", mtd -> index , mtd -> name ); <nl> return 0 ; <nl> - <nl> - dev_err (& dev -> p_dev -> dev , " CS Error , exiting \ n "); <nl> - pcmciamtd_release ( link ); <nl> - return - ENODEV ; <nl> } <nl>  <nl> 
static int zfcp_ccw_set_online ( struct ccw_device * ccw_device ) <nl> zfcp_erp_adapter_reopen ( adapter , ZFCP_STATUS_COMMON_ERP_FAILED , 85 , <nl> NULL ); <nl> zfcp_erp_wait ( adapter ); <nl> - goto out ; <nl> + up (& zfcp_data . config_sema ); <nl> + flush_work (& adapter -> scan_work ); <nl> + return 0 ; <nl>  <nl> out_scsi_register : <nl> zfcp_erp_thread_kill ( adapter );
static void set_truncation ( <nl> REG_UPDATE_3 ( FMT_BIT_DEPTH_CONTROL , <nl> FMT_TRUNCATE_EN , 1 , <nl> FMT_TRUNCATE_DEPTH , <nl> - params -> flags . TRUNCATE_MODE , <nl> + params -> flags . TRUNCATE_DEPTH , <nl> FMT_TRUNCATE_MODE , <nl> - params -> flags . TRUNCATE_DEPTH ); <nl> + params -> flags . TRUNCATE_MODE ); <nl> } <nl>  <nl> 
int tcp_disconnect ( struct sock * sk , int flags ) <nl> tcp_set_ca_state ( sk , TCP_CA_Open ); <nl> tcp_clear_retrans ( tp ); <nl> inet_csk_delack_init ( sk ); <nl> + /* Initialize rcv_mss to TCP_MIN_MSS to avoid division by 0 <nl> + * issue in __tcp_select_window () <nl> + */ <nl> + icsk -> icsk_ack . rcv_mss = TCP_MIN_MSS ; <nl> tcp_init_send_head ( sk ); <nl> memset (& tp -> rx_opt , 0 , sizeof ( tp -> rx_opt )); <nl> __sk_dst_reset ( sk );
int copy_creds ( struct task_struct * p , unsigned long clone_flags ) <nl> struct cred * new ; <nl> int ret ; <nl>  <nl> + p -> replacement_session_keyring = NULL ; <nl> + <nl> if ( <nl> # ifdef CONFIG_KEYS <nl> ! p -> cred -> thread_keyring &&
static void __init_discard_policy ( struct f2fs_sb_info * sbi , <nl> dpolicy -> min_interval = DEF_MIN_DISCARD_ISSUE_TIME ; <nl> dpolicy -> max_interval = DEF_MAX_DISCARD_ISSUE_TIME ; <nl> dpolicy -> io_aware = true ; <nl> + dpolicy -> sync = false ; <nl> if ( utilization ( sbi ) > DEF_DISCARD_URGENT_UTIL ) { <nl> dpolicy -> granularity = 1 ; <nl> dpolicy -> max_interval = DEF_MIN_DISCARD_ISSUE_TIME ;
gif_internal_decode_frame ( gif_animation * gif , <nl> unsigned int x , y , decode_y , burst_bytes ; <nl> register unsigned char colour ; <nl>  <nl> + /* If the GIF has no frame data , frame holders will not be allocated in <nl> + * gif_initialise () */ <nl> + if ( gif -> frames == NULL ) { <nl> + return GIF_INSUFFICIENT_DATA ; <nl> + } <nl> + <nl> /* Ensure this frame is supposed to be decoded */ <nl> if ( gif -> frames [ frame ]. display == false ) { <nl> return GIF_OK ;
int pam_sm_acct_mgmt ( pam_handle_t * pamh , int flags , <nl> int tac_fd ; <nl>  <nl> user = tty = r_addr = NULL ; <nl> + memset (& arep , 0 , sizeof ( arep )); <nl>  <nl> /* this also obtains service name for authorization <nl> this should be normally performed by pam_get_item ( PAM_SERVICE )
static void ProcessRadioRxDone ( void ) <nl> } <nl> } <nl>  <nl> + // Abort on empty radio frames <nl> + if ( size == 0 ) <nl> + { <nl> + MacCtx . McpsIndication . Status = LORAMAC_EVENT_INFO_STATUS_ERROR ; <nl> + PrepareRxDoneAbort ( ); <nl> + return ; <nl> + } <nl> + <nl> macHdr . Value = payload [ pktHeaderLen ++]; <nl>  <nl> // Accept frames of LoRaWAN Major Version 1 only
pspdf_prepare_outpages () <nl> chapter_outstarts [ c ] = num_outpages ; <nl>  <nl> for ( i = chapter_starts [ c ], j = 0 , nup = - 1 , page = pages + i ; <nl> - i <= chapter_ends [ c ]; <nl> + i <= chapter_ends [ c ] && num_outpages < num_pages ; <nl> i ++, page ++) <nl> { <nl> if ( nup != page -> nup )
OGRKMLLayer :: OGRKMLLayer ( const char * pszName , <nl> if ( poSRSIn != nullptr ) <nl> { <nl> poSRS_ -> SetWellKnownGeogCS ( " WGS84 " ); <nl> + poSRS_ -> SetAxisMappingStrategy ( OAMS_TRADITIONAL_GIS_ORDER ); <nl> if ( ! poSRS_ -> IsSame ( poSRSIn ) ) <nl> { <nl> poCT_ = OGRCreateCoordinateTransformation ( poSRSIn , poSRS_ );
IMPEG2D_ERROR_CODES_T impeg2d_dec_p_b_slice ( dec_state_t * ps_dec ) <nl>  <nl> if ( ret ) <nl> return IMPEG2D_MB_TEX_DECODE_ERR ; <nl> + <nl> + if ( 0 >= ps_dec -> u2_num_mbs_left ) <nl> + { <nl> + break ; <nl> + } <nl> + <nl> IMPEG2D_TRACE_MB_START ( ps_dec -> u2_mb_x , ps_dec -> u2_mb_y ); <nl>  <nl> u4_x_dst_offset = u4_frm_offset + ( ps_dec -> u2_mb_x << 4 );
status_t BnGraphicBufferProducer :: onTransact ( <nl> QueueBufferOutput * const output = <nl> reinterpret_cast < QueueBufferOutput *>( <nl> reply -> writeInplace ( sizeof ( QueueBufferOutput ))); <nl> + memset ( output , 0 , sizeof ( QueueBufferOutput )); <nl> status_t result = queueBuffer ( buf , input , output ); <nl> reply -> writeInt32 ( result ); <nl> return NO_ERROR ;
status_t OMXNodeInstance :: allocateBufferWithBackup ( <nl> } <nl>  <nl> CHECK_EQ ( header -> pAppPrivate , buffer_meta ); <nl> - memset ( header -> pBuffer , 0 , header -> nAllocLen ); <nl>  <nl> * buffer = makeBufferID ( header ); <nl> 
int modbus_reply ( modbus_t * ctx , const uint8_t * req , <nl> nb_write , nb , MODBUS_MAX_WR_WRITE_REGISTERS , MODBUS_MAX_WR_READ_REGISTERS ); <nl> } else if ( mapping_address < 0 || <nl> ( mapping_address + nb ) > mb_mapping -> nb_registers || <nl> - mapping_address < 0 || <nl> + mapping_address_write < 0 || <nl> ( mapping_address_write + nb_write ) > mb_mapping -> nb_registers ) { <nl> rsp_length = response_exception ( <nl> ctx , & sft , MODBUS_EXCEPTION_ILLEGAL_DATA_ADDRESS , rsp , FALSE ,
static int oidc_request_post_preserved_restore ( request_rec * r , <nl> " input . type = \" hidden \";\ n " <nl> " document . forms [ 0 ]. appendChild ( input );\ n " <nl> " }\ n " <nl> - " document . forms [ 0 ]. action = '% s ';\ n " <nl> + " document . forms [ 0 ]. action = \"% s \";\ n " <nl> " document . forms [ 0 ]. submit ();\ n " <nl> " }\ n " <nl> " </ script >\ n ", method , original_url );
int init_result ( RESULT & result , void *& data ) { <nl> log_messages . printf ( MSG_DEBUG , " Check result \ n "); <nl>  <nl> char buff [ 256 ]; <nl> - n = fscanf ( f , "% s ", buff ); <nl> + // n = fscanf ( f , "% s ", buff ); <nl> + fgets ( buff , 256 , f ); <nl> char * pch ; <nl> pch = strtok ( buff , " ,"); <nl> if ( pch != NULL ) {
TfLiteStatus ResizeOutput ( TfLiteContext * context , const TfLiteTensor * input , <nl> axis_value += NumDimensions ( input ); <nl> } <nl>  <nl> + TF_LITE_ENSURE ( context , axis_value >= 0 ); <nl> + TF_LITE_ENSURE ( context , axis_value < NumDimensions ( input )); <nl> + <nl> // Copy the input dimensions to output except the axis dimension . <nl> TfLiteIntArray * output_dims = TfLiteIntArrayCreate ( NumDimensions ( input ) - 1 ); <nl> int j = 0 ;
Status ImportGenericFunction ( <nl> // Import the function attributes with a ` tf .` prefix to match the current <nl> // infrastructure expectations . <nl> for ( const auto & namedAttr : func . attr ()) { <nl> + if ( namedAttr . first . empty ()) <nl> + return InvalidArgument (" Invalid function attribute name "); <nl> const std :: string & name = " tf ." + namedAttr . first ; <nl> const AttrValue & tf_attr = namedAttr . second ; <nl> TF_ASSIGN_OR_RETURN ( Attribute attr ,
int ksmbd_decode_ntlmssp_auth_blob ( struct authenticate_message * authblob , <nl> dn_off = le32_to_cpu ( authblob -> DomainName . BufferOffset ); <nl> dn_len = le16_to_cpu ( authblob -> DomainName . Length ); <nl>  <nl> - if ( blob_len < ( u64 ) dn_off + dn_len || blob_len < ( u64 ) nt_off + nt_len ) <nl> + if ( blob_len < ( u64 ) dn_off + dn_len || blob_len < ( u64 ) nt_off + nt_len || <nl> + nt_len < CIFS_ENCPWD_SIZE ) <nl> return - EINVAL ; <nl>  <nl> # ifdef CONFIG_SMB_INSECURE_SERVER
std :: string ResourceBundle :: LoadLocaleResources ( <nl>  <nl> std :: unique_ptr < DataPack > data_pack ( new DataPack ( SCALE_FACTOR_100P )); <nl> if (! data_pack -> LoadFromPath ( locale_file_path )) { <nl> - LOG ( ERROR ) << " failed to load locale . pak "; <nl> + LOG ( ERROR ) << " failed to load locale file : " << locale_file_path ; <nl> NOTREACHED (); <nl> return std :: string (); <nl> }
void MediaStreamDispatcherHost :: DoOpenDevice ( <nl> } <nl>  <nl> media_stream_manager_ -> OpenDevice ( <nl> - render_process_id_ , render_frame_id_ , page_request_id , requester_id_ , <nl> + render_process_id_ , render_frame_id_ , requester_id_ , page_request_id , <nl> device_id , type , std :: move ( salt_and_origin ), std :: move ( callback ), <nl> base :: BindRepeating (& MediaStreamDispatcherHost :: OnDeviceStopped , <nl> weak_factory_ . GetWeakPtr ()));
void TabSpecificContentSettings :: OnContentBlocked ( <nl> const std :: string & resource_identifier ) { <nl> DCHECK ( type != CONTENT_SETTINGS_TYPE_GEOLOCATION ) <nl> << " Geolocation settings handled by OnGeolocationPermissionSet "; <nl> + if ( type < 0 || type >= CONTENT_SETTINGS_NUM_TYPES ) <nl> + return ; <nl> content_accessed_ [ type ] = true ; <nl> // Unless UI for resource content settings is enabled , ignore the resource <nl> // identifier .
bool PlatformFontSkia :: InitDefaultFont () { <nl>  <nl> bool success = false ; <nl> std :: string family = kFallbackFontFamilyName ; <nl> - int size_pixels = 12 ; <nl> + int size_pixels = PlatformFont :: kDefaultBaseFontSize ; <nl> int style = Font :: NORMAL ; <nl> Font :: Weight weight = Font :: Weight :: NORMAL ; <nl> FontRenderParams params ;
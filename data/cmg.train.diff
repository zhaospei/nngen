gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl> frame -> system_frame_number , <nl> GST_TIME_ARGS ( frame -> pts ), GST_TIME_ARGS ( frame -> duration )); <nl>  <nl> + gst_buffer_ref ( buf ); <nl> if (! gst_buffer_map ( buf , & minfo , GST_MAP_READ )) { <nl> GST_ERROR_OBJECT ( mpeg2dec , " Failed to map input buffer "); <nl> return GST_FLOW_ERROR ; <nl> gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl>  <nl> done : <nl> gst_buffer_unmap ( buf , & minfo ); <nl> + gst_buffer_unref ( buf ); <nl> return ret ; <nl> } <nl> 
create_request_failed : <nl> { <nl> GST_ELEMENT_ERROR ( ctx , LIBRARY , INIT , <nl> (" Could not create request ."), ( NULL )); <nl> + g_free ( req_url ); <nl> goto reset ; <nl> } <nl> send_error :
free_tree ( struct tree * t ) <nl> { <nl> size_t i ; <nl>  <nl> + if ( t == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < t -> nr_files ; ++ i ) { <nl> free ( t -> files [ i ]. path ); <nl> guestfs_free_statns ( t -> files [ i ]. stat );
do_part_get_gpt_type ( const char * device , int partnum ) <nl> char * <nl> do_part_get_name ( const char * device , int partnum ) <nl> { <nl> - CLEANUP_FREE char * parttype = do_part_get_parttype ( device ); <nl> + CLEANUP_FREE char * parttype ; <nl> + <nl> + parttype = do_part_get_parttype ( device ); <nl> + if ( parttype == NULL ) <nl> + return NULL ; <nl>  <nl> if ( STREQ ( parttype , " gpt ")) { <nl> int parted_has_m_opt = test_parted_m_opt ();
inspect_mount_handle ( guestfs_h * g ) <nl> exit ( EXIT_FAILURE ); <nl> } <nl>  <nl> + /* Free old global if there is one . */ <nl> + free ( root ); <nl> + <nl> root = roots [ 0 ]; <nl> free ( roots ); <nl> 
static int ovs_events_plugin_config ( oconfig_item_t * ci ) { <nl> ovs_events_config_free (); <nl> return - 1 ; <nl> } <nl> - strncpy ( ovs_events_ctx . config . ovs_db_serv , service , <nl> - sizeof ( ovs_events_ctx . config . ovs_db_serv )); <nl> + sstrncpy ( ovs_events_ctx . config . ovs_db_serv , service , <nl> + sizeof ( ovs_events_ctx . config . ovs_db_serv )); <nl> sfree ( service ); <nl> } else if ( strcasecmp (" Socket ", child -> key ) == 0 ) { <nl> if ( cf_util_get_string_buffer (
static int mb_config_add_host ( oconfig_item_t * ci ) /* {{{ */ <nl>  <nl> status = cf_util_get_string_buffer ( ci , host -> host , sizeof ( host -> host )); <nl> if ( status != 0 ) <nl> + { <nl> + sfree ( host ); <nl> return ( status ); <nl> + } <nl> if ( host -> host [ 0 ] == 0 ) <nl> + { <nl> + sfree ( host ); <nl> return ( EINVAL ); <nl> + } <nl>  <nl> for ( i = 0 ; i < ci -> children_num ; i ++) <nl> {
static char * replace_str ( const char * str , const char * old , /* {{{ */ <nl> } else <nl> retlen = strlen ( str ); <nl>  <nl> - ret = malloc ( retlen + 1 ); <nl> + ret = calloc ( 1 , retlen + 1 ); <nl> if ( ret == NULL ) <nl> return NULL ; <nl> // added to original : not optimized , but keeps valgrind happy . <nl> - memset ( ret , 0 , retlen + 1 ); <nl>  <nl> r = ret ; <nl> p = str ;
static int bind_config ( oconfig_item_t * ci ) /* {{{ */ <nl> return (- 1 ); <nl> } <nl>  <nl> + sfree ( url ); <nl> url = strdup ( child -> values [ 0 ]. value . string ); <nl> } else if ( strcasecmp (" OpCodes ", child -> key ) == 0 ) <nl> bind_config_set_bool (" OpCodes ", & global_opcodes , child );
static int perl_config_includedir ( oconfig_item_t * ci ) <nl> || ( OCONFIG_TYPE_STRING != ci -> values [ 0 ]. type )) <nl> return 1 ; <nl>  <nl> + if ( NULL == aTHX ) { <nl> + log_warn (" EnableDebugger has no effects if used after LoadPlugin ."); <nl> + return 1 ; <nl> + } <nl> + <nl> value = ci -> values [ 0 ]. value . string ; <nl>  <nl> if ( NULL == perl ) {
c_avl_tree_t * c_avl_create ( int (* compare ) ( const void *, const void *)) <nl>  <nl> void c_avl_destroy ( c_avl_tree_t * t ) <nl> { <nl> + if ( t == NULL ) <nl> + return ; <nl> free_node ( t -> root ); <nl> free ( t ); <nl> }
int lcc_getval ( lcc_connection_t * c , lcc_identifier_t * ident , /* {{{ */ <nl> if ( ret_values_names != NULL ) <nl> * ret_values_names = values_names ; <nl>  <nl> + lcc_response_free (& res ); <nl> + <nl> return ( 0 ); <nl> } /* }}} int lcc_getval */ <nl> 
static int cpu_read ( void ) <nl> submit ( cpu , " wait ", wait ); <nl> submit ( cpu , " interrupt ", intr ); <nl> submit ( cpu , " softirq ", sitr ); <nl> + <nl> + if ( numfields >= 9 ) <nl> + submit ( cpu , " steal ", atoll ( fields [ 8 ])); <nl> } <nl> } <nl> 
static int tss2_add_vserver ( int vserver_port ) <nl> } <nl>  <nl> /* Allocate memory */ <nl> - entry = malloc ( sizeof (* entry )); <nl> + entry = calloc ( 1 , sizeof (* entry )); <nl> if ( entry == NULL ) <nl> { <nl> - ERROR (" teamspeak2 plugin : malloc failed ."); <nl> + ERROR (" teamspeak2 plugin : calloc failed ."); <nl> return (- 1 ); <nl> } <nl> - memset ( entry , 0 , sizeof ( vserver_list_t )); <nl>  <nl> /* Save data */ <nl> entry -> port = vserver_port ;
static int varnish_read ( user_data_t * ud ) /* {{{ */ <nl>  <nl> vd = VSM_New (); <nl> VSC_Setup ( vd ); <nl> + if ( VSM_n_Arg ( vd , conf -> instance ) == - 1 ) <nl> + { <nl> + ERROR (" Varnish plugin : unable to load statistics from instance "); <nl> + return (- 1 ); <nl> + } <nl> if ( VSC_Open ( vd , /* diag = */ 1 )) <nl> { <nl> ERROR (" varnish plugin : Unable to load statistics .");
refresh_lists ( void ) <nl>  <nl> /* Get list of domains . */ <nl> domids = malloc ( sizeof (* domids ) * n ); <nl> - if ( domids == 0 ) { <nl> + if ( domids == NULL ) { <nl> ERROR ( PLUGIN_NAME " plugin : malloc failed ."); <nl> return - 1 ; <nl> }
static int c_psql_config_query ( oconfig_item_t * ci ) <nl> else <nl> log_warn (" Ignoring unknown config key \"% s \".", c -> key ); <nl> } <nl> + <nl> + if ( NULL == query -> query ) { <nl> + log_err (" Query \"% s \" does not include an SQL query string - " <nl> + " please check your configuration .", query -> name ); <nl> + c_psql_query_delete ( query ); <nl> + -- queries_num ; <nl> + return 1 ; <nl> + } <nl> return 0 ; <nl> } /* c_psql_config_query */ <nl> 
int uc_get_names ( char *** ret_names , cdtime_t ** ret_times , size_t * ret_number ) <nl> if ( status != 0 ) <nl> { <nl> size_t i ; <nl> - <nl> + <nl> for ( i = 0 ; i < number ; i ++) <nl> { <nl> sfree ( names [ i ]); <nl> } <nl> sfree ( names ); <nl> + sfree ( times ); <nl>  <nl> return (- 1 ); <nl> }
int main ( int argc , char ** argv ) <nl> return 1 ; <nl> } <nl> krb5_klog_syslog ( LOG_INFO , " commencing operation "); <nl> + if ( nofork ) <nl> + fprintf ( stderr , "% s : starting ...\ n ", kdc_progname ); <nl> if (( retval = listen_and_process ())) { <nl> kdc_err ( kcontext , retval , " while processing network requests "); <nl> errout ++;
krb5_lcc_close ( krb5_context context , krb5_ccache id ) <nl>  <nl> if ( data ) { <nl> LsaDeregisterLogonProcess ( data -> LogonHandle ); <nl> + if ( data -> cc_name ) <nl> + free ( data -> cc_name ); <nl> free ( data ); <nl> } <nl> free ( id );
krb5_kdc_req * val ; <nl> krb5_free_principal ( val -> client ); <nl> if ( val -> server ) <nl> krb5_free_principal ( val -> server ); <nl> + if ( val -> etype ) <nl> + xfree ( val -> etype ); <nl> if ( val -> addresses ) <nl> krb5_free_address ( val -> addresses ); <nl> if ( val -> authorization_data . ciphertext . data )
strdup ( str ) <nl> int len ; <nl> char * copy ; <nl>  <nl> + if (! str ) <nl> + return (( char *) 0 ); <nl> len = strlen ( str ) + 1 ; <nl> if (!( copy = malloc (( u_int ) len ))) <nl> return (( char *) 0 );
kadm5_setkey_principal_3 ( void * server_handle , <nl> goto done ; <nl> } <nl> tptr = & kdb . key_data [ i ]; <nl> + tptr -> key_data_ver = tmp_key_data . key_data_ver ; <nl> + tptr -> key_data_kvno = tmp_key_data . key_data_kvno ; <nl> for ( k = 0 ; k < tmp_key_data . key_data_ver ; k ++) { <nl> tptr -> key_data_type [ k ] = tmp_key_data . key_data_type [ k ]; <nl> tptr -> key_data_length [ k ] = tmp_key_data . key_data_length [ k ];
try_one_princ ( krb5_context context , const krb5_ap_req * req , <nl> if ( ret ) <nl> return ret ; <nl> ret = try_one_entry ( context , req , & ent , keyblock_out ); <nl> + if ( ret == 0 ) <nl> + TRACE_RD_REQ_DECRYPT_SPECIFIC ( context , ent . principal , & ent . key ); <nl> ( void ) krb5_free_keytab_entry_contents ( context , & ent ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - TRACE_RD_REQ_DECRYPT_SPECIFIC ( context , ent . principal , & ent . key ); <nl> return 0 ; <nl> } <nl> 
template int SSLWrap < TLSCallbacks >:: TLSExtStatusCallback ( SSL * s , void * arg ); <nl>  <nl>  <nl> static void crypto_threadid_cb ( CRYPTO_THREADID * tid ) { <nl> - static_assert ( sizeof ( uv_thread_t ) <= sizeof ( void *), <nl> + static_assert ( sizeof ( uv_thread_t ) <= sizeof ( void *), // NOLINT ( runtime / sizeof ) <nl> " uv_thread_t does not fit in a pointer "); <nl> CRYPTO_THREADID_set_pointer ( tid , reinterpret_cast < void *>( uv_thread_self ())); <nl> }
static struct { <nl> } <nl>  <nl> void StopTracingAgent () { <nl> - tracing_agent_ -> Stop (); <nl> + if ( tracing_agent_ ) <nl> + tracing_agent_ -> Stop (); <nl> } <nl>  <nl> tracing :: Agent * GetTracingAgent () const {
Handle < Value > ReadFloatGeneric ( const Arguments & args ) { <nl> return ThrowTypeError (" offset is not uint "); <nl> size_t len = static_cast < size_t >( <nl> args . This ()-> GetIndexedPropertiesExternalArrayDataLength ()); <nl> - if ( offset + sizeof ( T ) > len ) <nl> + if ( offset + sizeof ( T ) > len || offset + sizeof ( T ) < offset ) <nl> return ThrowRangeError (" Trying to read beyond buffer length "); <nl> } <nl> 
static void GetStringWidth ( const FunctionCallbackInfo < Value >& args ) { <nl> TwoByteValue value ( env -> isolate (), args [ 0 ]); <nl> // reinterpret_cast is required by windows to compile <nl> UChar * str = reinterpret_cast < UChar *>(* value ); <nl> - UChar32 c ; <nl> + static_assert ( sizeof (* str ) == sizeof (** value ), <nl> + " sizeof (* str ) == sizeof (** value )"); <nl> + UChar32 c = 0 ; <nl> UChar32 p ; <nl> size_t n = 0 ; <nl> uint32_t width = 0 ;
class CipherBase : public BaseObject { <nl> private : <nl> EVP_CIPHER_CTX ctx_ ; /* coverity [ member_decl ] */ <nl> bool initialised_ ; <nl> - CipherKind kind_ ; <nl> + const CipherKind kind_ ; <nl> unsigned int auth_tag_len_ ; <nl> char auth_tag_ [ EVP_GCM_TLS_TAG_LEN ]; <nl> };
int ASN1_STRING_to_UTF8 ( unsigned char ** out , ASN1_STRING * in ) <nl> mbflag = tag2nbyte [ type ]; <nl> if ( mbflag == - 1 ) return - 1 ; <nl> mbflag |= MBSTRING_FLAG ; <nl> + memset (& stmp , 0 , sizeof ( stmp )); <nl> stmp . data = NULL ; <nl> ret = ASN1_mbstring_copy (& str , in -> data , in -> length , mbflag , B_ASN1_UTF8STRING ); <nl> if ( ret < 0 ) return ret ;
void SetupProcessObject ( Environment * env , <nl> READONLY_PROPERTY ( process , <nl> " _preload_modules ", <nl> array ); <nl> + <nl> + delete [] preload_modules ; <nl> + preload_modules = nullptr ; <nl> + preload_module_count = 0 ; <nl> } <nl>  <nl> // -- no - deprecation
void CipherBase :: Final ( const FunctionCallbackInfo < Value >& args ) { <nl>  <nl> args . GetReturnValue (). Set ( <nl> Buffer :: New ( env , reinterpret_cast < char *>( out_value ), out_len )); <nl> + delete [] out_value ; <nl> } <nl>  <nl> 
int unrar_open ( int fd , const char * dirname , unrar_state_t * state ) <nl> unrar_dbgmsg (" UNRAR : Offset : % x \ n ", offset ); <nl> if ( offset < 0 ){ <nl> unrar_dbgmsg (" UNRAR : Error Offset : % d \ n ", offset ); <nl> - offset = 0 ; <nl> + free ( main_hdr ); <nl> + free ( state -> comment_dir ); <nl> + free ( unpack_data ); <nl> + return UNRAR_ERR ; <nl> } <nl> comment_header = read_header ( fd , COMM_HEAD ); <nl> if ( comment_header ) {
static int pdf_extract_obj ( struct pdf_struct * pdf , struct pdf_obj * obj ) <nl> n -= q2 - q ; <nl> q = q2 ; <nl> } <nl> - } while ( n > 0 && q2 && q2 [- 1 ] == '\\'); <nl> + } while ( n > 0 && q2 && q2 [- 2 ] == '\\'); <nl> if ( q2 ) <nl> end = q2 - 1 ; <nl> n = end - out ;
abort : <nl> } <nl> if ( file_tmp_o1 ) { <nl> html_output_flush ( file_tmp_o1 ); <nl> - close ( file_tmp_o1 -> fd ); <nl> + if ( file_buff_text -> fd != - 1 ) <nl> + close ( file_tmp_o1 -> fd ); <nl> free ( file_tmp_o1 ); <nl> } <nl> return retval ;
static char * sha256file ( const char * file , unsigned int * size ) <nl> sha256_final (& ctx , digest ); <nl> sha = ( char *) malloc ( 65 ); <nl> if (! sha ) <nl> + { <nl> + fclose ( fh ); <nl> return NULL ; <nl> + } <nl> for ( i = 0 ; i < 32 ; i ++) <nl> sprintf ( sha + i * 2 , "% 02x ", digest [ i ]); <nl> + <nl> + fclose ( fh ); <nl> return sha ; <nl> } <nl> 
inline static int ac_special_altstr ( const char * hexpr , uint8_t sigopts , struct c <nl> /* allocate reusable subexpr */ <nl> if (!( subexpr = cli_calloc ( slen + 1 , sizeof ( char )))) { <nl> cli_errmsg (" ac_special_altstr : Can ' t allocate subexpr container \ n "); <nl> + free ( hexprcpy ); <nl> return CL_EMEM ; <nl> } <nl> 
int cli_bm_scanbuff ( const unsigned char * buffer , uint32_t length , const char ** v <nl> memset (& info , 0 , sizeof ( info )); <nl> i = BM_MIN_LENGTH - BM_BLOCK_SIZE ; <nl> if ( offdata ) { <nl> + if (! offdata -> cnt ) <nl> + return CL_CLEAN ; <nl> for (; offdata -> pos && offdata -> offtab [ offdata -> pos ] > offset ; offdata -> pos --); <nl> if ( offdata -> offtab [ offdata -> pos ] < offset ) <nl> offdata -> pos ++;
static int updatedb ( const char * dbname , const char * hostname , char * ip , int * sig <nl> } <nl>  <nl> if ( rename ( newfile , newdb ) == - 1 ) { <nl> - logg ("! Can ' t rename % s to % s \ n ", newfile , newdb ); <nl> + logg ("! Can ' t rename % s to % s : % s \ n ", newfile , newdb , strerror ( errno )); <nl> unlink ( newfile ); <nl> free ( newfile ); <nl> return 57 ;
static int cabd_read_headers ( struct mspack_system * sys , <nl> } <nl> else { <nl> /* ignore invalid file and continue parsing */ <nl> + if ( file -> filename ) { <nl> + sys -> free ( file -> filename ); <nl> + file -> filename = NULL ; <nl> + } <nl> sys -> free ( file ); <nl> sys -> message ( fh , " WARNING ; omitting file % d of % d from file list ", i , num_files ); <nl> }
char * pdf_finalize_string ( struct pdf_struct * pdf , struct pdf_obj * obj , const cha <nl> /* TODO : replace the escape sequences directly in the wrkstr */ <nl> if ( strchr ( wrkstr , '\\')) { <nl> output = cli_calloc ( wrklen + 1 , sizeof ( char )); <nl> - if (! output ) <nl> + if (! output ) { <nl> + free ( wrkstr ); <nl> return NULL ; <nl> + } <nl>  <nl> outlen = 0 ; <nl> for ( i = 0 ; i < wrklen ; ++ i ) {
cib_perform_op ( const char * op , int call_options , cib_op_t * fn , gboolean is_query <nl>  <nl> if ( rc == cib_ok && scratch ) { <nl> const char * new_version = crm_element_value ( scratch , XML_ATTR_CRM_VERSION ); <nl> - if ( new_version && compare_version ( new_version , CRM_FEATURE_SET ) < 0 ) { <nl> + if ( new_version && compare_version ( new_version , CRM_FEATURE_SET ) > 0 ) { <nl> crm_err (" Discarding update with feature set '% s ' greater than our own '% s '", <nl> new_version , CRM_FEATURE_SET ); <nl> rc = cib_NOTSUPPORTED ;
do_cl_join_finalize_respond ( long long action , <nl> erase_status_tag ( fsa_our_uname , XML_CIB_TAG_LRM ); <nl>  <nl> /* Just in case attrd was still around too */ <nl> - if ( is_not_set ( input_register , R_SHUTDOWN )) { <nl> + if ( is_not_set ( fsa_input_register , R_SHUTDOWN )) { <nl> update_attrd ( fsa_our_uname , " terminate ", NULL ); <nl> update_attrd ( fsa_our_uname , XML_CIB_ATTR_SHUTDOWN , NULL ); <nl> }
do_cib_control ( long long action , <nl> clear_bit_inplace ( fsa_input_register , R_CIB_CONNECTED ); <nl> if ( fsa_cib_conn != NULL <nl> && fsa_cib_conn -> state != cib_disconnected ) { <nl> + fsa_cib_conn -> cmds -> set_slave ( <nl> + fsa_cib_conn , cib_scope_local ); <nl> fsa_cib_conn -> cmds -> signoff ( fsa_cib_conn ); <nl> } <nl> }
lrmd_init_remote_tls_server () <nl> if ( rc != 0 ) { <nl> crm_warn (" A cluster connection will not be possible until the key is available "); <nl> } <nl> + gnutls_free ( psk_key . data ); <nl>  <nl> memset (& hints , 0 , sizeof ( struct addrinfo )); <nl> /* Bind to the wildcard address ( INADDR_ANY or IN6ADDR_ANY_INIT ).
void native_rsc_order_rh ( <nl> rsc -> actions , order -> rh_action_task , NULL ); <nl> } <nl>  <nl> - if ( rh_actions == NULL && lh_action != NULL ) { <nl> + if ( rh_actions == NULL ) { <nl> crm_debug_4 (" No RH - Side (% s /% s ) found for constraint ..." <nl> " ignoring ", rsc -> id , order -> rh_action_task ); <nl> if ( lh_action ) {
need_abort ( crm_data_t * update ) <nl> return NULL ; <nl> } <nl>  <nl> + xml_prop_iter ( update , name , value , <nl> + if ( safe_str_eq ( name , XML_ATTR_ID ) == FALSE ) { <nl> + return update ; <nl> + } <nl> + ); <nl> + <nl> section = XML_CIB_TAG_NODES ; <nl> section_xml = get_object_root ( section , update ); <nl> xml_child_iter ( section_xml , child ,
send_smtp_trap ( const char * node , const char * rsc , const char * task , int target_r <nl> char crm_mail_body [ BODY_MAX ]; <nl> char * crm_mail_subject = NULL ; <nl>  <nl> + memset (& sa , 0 , sizeof ( struct sigaction )); <nl> + <nl> if ( node == NULL ) { <nl> node = "-"; <nl> }
void master_rsc_colocation_rh ( <nl> clone_variant_data_t * clone_data = NULL ; <nl> get_clone_variant_data ( clone_data , rsc_rh ); <nl>  <nl> + CRM_CHECK ( rsc_rh != NULL , return ); <nl> if ( rsc_rh -> provisional ) { <nl> return ; <nl> 
cib_ha_peer_callback ( HA_Message * msg , void * private_data ) <nl> { <nl> xmlNode * xml = convert_ha_message ( NULL , msg , __FUNCTION__ ); <nl> cib_peer_callback ( xml , private_data ); <nl> + free_xml ( xml ); <nl> } <nl>  <nl> void
synthesize_lrmd_failure ( lrm_state_t * lrm_state , xmlNode * action , int rc ) <nl> lrmd_free_rsc_info ( rsc_info ); <nl> process_lrm_event ( lrm_state , op , NULL ); <nl>  <nl> - } else { <nl> + } else if ( controld_action_is_recordable ( op -> op_type )) { <nl> /* If we can ' t process the result normally , at least write it to the CIB <nl> * if possible , so the scheduler can act on it . <nl> */
crm_graph_functions_t te_graph_fns = { <nl> te_fence_node <nl> }; <nl>  <nl> + extern GMainLoop * mainloop ; <nl> + <nl> void <nl> notify_crmd ( crm_graph_t * graph ) <nl> { <nl> notify_crmd ( crm_graph_t * graph ) <nl>  <nl> case tg_shutdown : <nl> crm_info (" Exiting after transition "); <nl> + if ( mainloop != NULL && g_main_is_running ( mainloop )) { <nl> + g_main_quit ( mainloop ); <nl> + return ; <nl> + } <nl> exit ( LSB_EXIT_OK ); <nl> } <nl> 
write_cib_contents ( gpointer p ) <nl> crm_free ( tmp2 ); <nl> crm_free ( tmp1 ); <nl>  <nl> + free_xml ( local_cib ); <nl> + <nl> if ( p == NULL ) { <nl> /* exit () could potentially affect the parent by closing things it shouldn ' t <nl> * Use _exit instead
process_pe_message ( xmlNode * msg , xmlNode * xml_data , qb_ipcs_connection_t * send <nl> crm_xml_add_int ( data_set . graph , " transition_id ", 0 ); <nl> crm_xml_add_int ( data_set . graph , " cluster - delay ", 0 ); <nl> process = FALSE ; <nl> + free ( digest ); <nl>  <nl> } else if ( safe_str_eq ( digest , last_digest )) { <nl> crm_info (" Input has not changed since last time , not saving to disk "); <nl> is_repoke = TRUE ; <nl> + free ( digest ); <nl>  <nl> } else { <nl> free ( last_digest );
attrd_local_callback ( xmlNode * msg ) <nl> goto set_unexpanded ; <nl> } <nl>  <nl> - int_value = char2score ( value ); <nl> + int_value = char2score ( hash_entry -> value ); <nl> if ( value [ plus_plus_len + 1 ] != '+') { <nl> const char * offset_s = value +( plus_plus_len + 2 ); <nl> offset = char2score ( offset_s );
void BitcoinGUI :: message ( const QString & title , const QString & message , unsigned <nl>  <nl> showNormalIfMinimized (); <nl> QMessageBox mBox ( static_cast < QMessageBox :: Icon >( nMBoxIcon ), strTitle , message , buttons , this ); <nl> + mBox . setTextFormat ( Qt :: PlainText ); <nl> int r = mBox . exec (); <nl> if ( ret != nullptr ) <nl> * ret = r == QMessageBox :: Ok ;
void ServiceConnection ( AcceptedConnection * conn ) <nl> // Read HTTP message headers and body <nl> ReadHTTPMessage ( conn -> stream (), mapHeaders , strRequest , nProto ); <nl>  <nl> + if ( strURI != "/") { <nl> + conn -> stream () << HTTPReply ( HTTP_NOT_FOUND , "", false ) << std :: flush ; <nl> + break ; <nl> + } <nl> + <nl> // Check authorization <nl> if ( mapHeaders . count (" authorization ") == 0 ) <nl> {
int git_libgit2_opts ( int key , ...) <nl> void git_strarray_free ( git_strarray * array ) <nl> { <nl> size_t i ; <nl> + <nl> + if ( array == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < array -> count ; ++ i ) <nl> git__free ( array -> strings [ i ]); <nl> 
int git_repository_config__weakptr ( git_config ** out , git_repository * repo ) <nl> git_buf_free (& global_buf ); <nl> git_buf_free (& xdg_buf ); <nl> git_buf_free (& system_buf ); <nl> + git_buf_free (& programdata_buf ); <nl> } <nl>  <nl> * out = repo -> _config ;
static int config_delete ( git_config_file * cfg , const char * name ) <nl> pos = git_strmap_lookup_index ( b -> values , key ); <nl> git__free ( key ); <nl>  <nl> - if (! git_strmap_valid_index ( b -> values , pos )) <nl> + if (! git_strmap_valid_index ( b -> values , pos )) { <nl> + giterr_set ( GITERR_CONFIG , " Could not find key '% s ' to delete ", name ); <nl> return GIT_ENOTFOUND ; <nl> + } <nl>  <nl> var = git_strmap_value_at ( b -> values , pos ); <nl> 
int git_remote_update_tips ( git_remote * remote , int (* cb )( const char * refname , co <nl> for (; i < refs -> length ; ++ i ) { <nl> head = refs -> contents [ i ]; <nl>  <nl> + /* Skip tag annotations */ <nl> + if (! git__suffixcmp ( head -> name , "^{}")) <nl> + continue ; <nl> + <nl> if ( git_refspec_transform_r (& refname , spec , head -> name ) < 0 ) <nl> goto on_error ; <nl> 
git_oid_shorten * git_oid_shorten_new ( size_t min_length ) <nl>  <nl> void git_oid_shorten_free ( git_oid_shorten * os ) <nl> { <nl> + if ( os == NULL ) <nl> + return ; <nl> + <nl> git__free ( os -> nodes ); <nl> git__free ( os ); <nl> }
int git_futils_mkdir ( <nl> min_root_len = git_path_root ( make_path . ptr ); <nl> if ( root < min_root_len ) <nl> root = min_root_len ; <nl> - while ( make_path . ptr [ root ] == '/') <nl> + while ( root >= 0 && make_path . ptr [ root ] == '/') <nl> ++ root ; <nl>  <nl> /* clip root to make_path length */
int git_path_diriter_init ( <nl> unsigned int flags ) <nl> { <nl> git_win32_path path_filter ; <nl> - git_buf hack = { 0 }; <nl>  <nl> static int is_win7_or_later = - 1 ; <nl> if ( is_win7_or_later < 0 )
static void checkout_data_clear ( checkout_data * data ) <nl> git__free ( data -> pfx ); <nl> data -> pfx = NULL ; <nl>  <nl> + git_buf_free (& data -> last_mkdir ); <nl> git_buf_free (& data -> path ); <nl> git_buf_free (& data -> tmp ); <nl> 
int git_reference_foreach ( <nl> if ( list_flags & GIT_REF_PACKED ) { <nl> const char * ref_name ; <nl> void * ref ; <nl> + GIT_UNUSED ( ref ); <nl>  <nl> if ( packed_load ( repo ) < 0 ) <nl> return - 1 ;
int git_repository_config__weakptr ( git_config ** out , git_repository * repo ) <nl> res = load_config (& repo -> _config , repo , global_config_path , xdg_config_path , system_config_path ); <nl>  <nl> git_buf_free (& global_buf ); <nl> + git_buf_free (& xdg_buf ); <nl> git_buf_free (& system_buf ); <nl>  <nl> if ( res < 0 )
static int limit_list ( git_commit_list ** out , git_revwalk * walk , git_commit_list <nl> break ; <nl> } <nl>  <nl> - if (! commit -> uninteresting && walk -> hide_cb && walk -> hide_cb (& commit -> oid , walk -> hide_cb_payload )) <nl> - continue ; <nl> + if ( walk -> hide_cb && walk -> hide_cb (& commit -> oid , walk -> hide_cb_payload )) <nl> + continue ; <nl>  <nl> time = commit -> time ; <nl> p = & git_commit_list_insert ( commit , p )-> next ;
void git_revwalk_reset ( git_revwalk * walk ) <nl> git_commit_list_free (& walk -> iterator_rand ); <nl> git_commit_list_free (& walk -> iterator_reverse ); <nl> git_commit_list_free (& walk -> user_input ); <nl> + walk -> first_parent = 0 ; <nl> walk -> walking = 0 ; <nl> walk -> did_push = walk -> did_hide = 0 ; <nl> }
int git_openssl_stream_global_init ( void ) <nl> * to speak TLSv1 to perform the encryption itself . <nl> */ <nl> git__ssl_ctx = SSL_CTX_new ( SSLv23_method ()); <nl> + if (! git__ssl_ctx ) { <nl> + return - 1 ; <nl> + } <nl> + <nl> SSL_CTX_set_options ( git__ssl_ctx , ssl_opts ); <nl> SSL_CTX_set_mode ( git__ssl_ctx , SSL_MODE_AUTO_RETRY ); <nl> SSL_CTX_set_verify ( git__ssl_ctx , SSL_VERIFY_NONE , NULL );
uint32_t git_pool__system_page_size ( void ) <nl> size_t page_size ; <nl> if ( git__page_size (& page_size ) < 0 ) <nl> page_size = 4096 ; <nl> - size = page_size - 2 * sizeof ( void *); /* allow space for malloc overhead */ <nl> + /* allow space for malloc overhead */ <nl> + size = page_size - ( 2 * sizeof ( void *)) - sizeof ( git_pool_page ); <nl> } <nl>  <nl> return size ;
int git_repository_head_unborn ( git_repository * repo ) <nl> error = git_repository_head (& ref , repo ); <nl> git_reference_free ( ref ); <nl>  <nl> - if ( error == GIT_EUNBORNBRANCH ) <nl> + if ( error == GIT_EUNBORNBRANCH ) { <nl> + giterr_clear (); <nl> return 1 ; <nl> + } <nl>  <nl> if ( error < 0 ) <nl> return - 1 ;
static int wait_while_ack ( gitno_buffer * buf ) <nl> ( pkt -> status != GIT_ACK_CONTINUE || <nl> pkt -> status != GIT_ACK_COMMON )) { <nl> git__free ( pkt ); <nl> - break ; <nl> + return 0 ; <nl> } <nl> } <nl> 
cmsHANDLE CMSEXPORT cmsIT8LoadFromMem ( cmsContext ContextID , const void * Ptr , cm <nl>  <nl> it8 = ( cmsIT8 *) hIT8 ; <nl> it8 -> MemoryBlock = ( char *) _cmsMalloc ( ContextID , len + 1 ); <nl> + if ( it8 -> MemoryBlock == NULL ) <nl> + { <nl> + cmsIT8Free ( hIT8 ); <nl> + return FALSE ; <nl> + } <nl>  <nl> strncpy ( it8 -> MemoryBlock , ( const char *) Ptr , len ); <nl> it8 -> MemoryBlock [ len ] = 0 ;
void * Type_MLU_Read ( struct _cms_typehandler_struct * self , cmsIOHANDLER * io , cmsU <nl>  <nl> // Check for overflow <nl> if ( Offset < ( SizeOfHeader + 8 )) goto Error ; <nl> + if (( Offset + Len ) > SizeOfTag + 8 ) goto Error ; <nl>  <nl> // True begin of the string <nl> BeginOfThisString = Offset - SizeOfHeader - 8 ;
void HandleSwitches ( int argc , char * argv []) <nl> case ' d ': <nl> case ' D ': { <nl> cmsFloat64Number ObserverAdaptationState = atof ( xoptarg ); <nl> - if ( ObserverAdaptationState < 0 && <nl> + if ( ObserverAdaptationState < 0 || <nl> ObserverAdaptationState > 1 . 0 ) <nl> FatalError (" Adaptation states should be between 0 and 1 "); <nl> 
const char * make_absolute_path ( const char * path ) <nl> char * last_elem = NULL ; <nl> struct stat st ; <nl>  <nl> + /* We ' ve already done it */ <nl> + if ( path == buf || path == next_buf ) <nl> + return path ; <nl> + <nl> if ( strlcpy ( buf , path , PATH_MAX ) >= PATH_MAX ) <nl> die (" Too long path : %.* s ", 60 , path ); <nl> 
# define BLOCKSIZE ( RECORDSIZE * 20 ) <nl>  <nl> static const char tar_tree_usage [] = <nl> -" git - tar - tree [-- remote =< repo >] < ent > [ basedir ]"; <nl> +" git - tar - tree [-- remote =< repo >] < tree - ish > [ basedir ]"; <nl>  <nl> static char block [ BLOCKSIZE ]; <nl> static unsigned long offset ;
static struct ref_entry * search_ref_array ( struct ref_array * array , const char * n <nl> if ( name == NULL ) <nl> return NULL ; <nl>  <nl> + if (! array -> nr ) <nl> + return NULL ; <nl> + <nl> len = strlen ( name ) + 1 ; <nl> e = xmalloc ( sizeof ( struct ref_entry ) + len ); <nl> memcpy ( e -> name , name , len );
static const char * format_time ( unsigned long time , const char * tz_str , <nl> int tz ; <nl>  <nl> if ( show_raw_time ) { <nl> - sprintf ( time_buf , "% lu % s ", time , tz_str ); <nl> + snprintf ( time_buf , sizeof ( time_buf ), "% lu % s ", time , tz_str ); <nl> } <nl> else { <nl> tz = atoi ( tz_str );
*/ <nl>  <nl> typedef struct { <nl> + unsigned long long size ; <nl> unsigned int H [ 5 ]; <nl> unsigned int W [ 16 ]; <nl> - unsigned long long size ; <nl> } blk_SHA_CTX ; <nl>  <nl> void blk_SHA1_Init ( blk_SHA_CTX * ctx );
void diff_setup ( struct diff_options * options ) <nl>  <nl> int diff_setup_done ( struct diff_options * options ) <nl> { <nl> - if (( options -> find_copies_harder && <nl> - options -> detect_rename != DIFF_DETECT_COPY ) || <nl> - ( 0 <= options -> rename_limit && ! options -> detect_rename )) <nl> + if ( options -> find_copies_harder ) <nl> + options -> detect_rename = DIFF_DETECT_COPY ; <nl> + <nl> + if (( 0 <= options -> rename_limit && ! options -> detect_rename ) <nl> return - 1 ; <nl>  <nl> if ( options -> output_format & ( DIFF_FORMAT_NAME |
static int find_common ( int fd [ 2 ], unsigned char * result_sha1 , <nl> retval = 0 ; <nl> in_vain = 0 ; <nl> got_continue = 1 ; <nl> + if ( ack == ACK_ready ) <nl> + rev_list = NULL ; <nl> break ; <nl> } <nl> }
static int prepare_lines ( struct scoreboard * sb ) <nl> bol = 1 ; <nl> } <nl> } <nl> + sb -> lineno = xrealloc ( sb -> lineno , <nl> + sizeof ( int * ) * ( num + incomplete + 1 )); <nl> + sb -> lineno [ num + incomplete ] = buf - sb -> final_buf ; <nl> sb -> num_lines = num + incomplete ; <nl> return sb -> num_lines ; <nl> }
static void setup_progress_signal ( void ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> SHA_CTX ctx ; <nl> - char line [ PATH_MAX + 20 ]; <nl> + char line [ 40 + 1 + PATH_MAX + 2 ]; <nl> int window = 10 , depth = 10 , pack_to_stdout = 0 ; <nl> struct object_entry ** list ; <nl> int num_preferred_base = 0 ;
static void credential_write_item ( FILE * fp , const char * key , const char * value ) <nl> { <nl> if (! value ) <nl> return ; <nl> + if ( strchr ( value , '\ n ')) <nl> + die (" credential value for % s contains newline ", key ); <nl> fprintf ( fp , "% s =% s \ n ", key , value ); <nl> } <nl> 
struct http_pack_request * new_http_pack_request ( <nl> return preq ; <nl>  <nl> abort : <nl> - free ( filename ); <nl> free ( preq -> url ); <nl> free ( preq ); <nl> return NULL ;
void mark_parents_uninteresting ( struct commit * commit ) <nl>  <nl> void add_pending_object ( struct rev_info * revs , struct object * obj , const char * name ) <nl> { <nl> + if ( revs -> no_walk && ( obj -> flags & UNINTERESTING )) <nl> + die (" object ranges do not make sense when not walking revisions "); <nl> add_object_array ( obj , name , & revs -> pending ); <nl> if ( revs -> reflog_info && obj -> type == OBJ_COMMIT ) <nl> add_reflog_for_walk ( revs -> reflog_info ,
static void prune_directory ( struct dir_struct * dir , const char ** pathspec , int p <nl> free ( entry ); <nl> continue ; <nl> } <nl> + if ( entry -> ignored_entry ) <nl> + fprintf ( stderr , " warning : '% s ' is an ignored path .\ n ", <nl> + entry -> name ); <nl> * dst ++ = entry ; <nl> } <nl> dir -> nr = dst - dir -> entries ;
static int write_pseudoref ( const char * pseudoref , const struct object_id * oid , <nl> struct strbuf buf = STRBUF_INIT ; <nl> int ret = - 1 ; <nl>  <nl> + if (! oid ) <nl> + return 0 ; <nl> + <nl> strbuf_addf (& buf , "% s \ n ", oid_to_hex ( oid )); <nl>  <nl> filename = git_path ("% s ", pseudoref );
extern int cmd_main ( int , const char **); <nl> */ <nl> # ifdef SUPPRESS_ANNOTATED_LEAKS <nl> extern void unleak_memory ( const void * ptr , size_t len ); <nl> -# define UNLEAK ( var ) unleak_memory (&( var ), sizeof ( var )); <nl> +# define UNLEAK ( var ) unleak_memory (&( var ), sizeof ( var )) <nl> # else <nl> -# define UNLEAK ( var ) <nl> +# define UNLEAK ( var ) do {} while ( 0 ) <nl> # endif <nl>  <nl> # endif
unsigned long count_delta ( void * delta_buf , unsigned long delta_size ) <nl> /* delete size is what was _not_ copied from source . <nl> * edit size is that and literal additions . <nl> */ <nl> + if ( src_size + added_literal < copied_from_source ) <nl> + /* we ended up overcounting and underflowed */ <nl> + return 0 ; <nl> return ( src_size - copied_from_source ) + added_literal ; <nl> }
int xmkstemp ( char * template ) <nl> int saved_errno = errno ; <nl> const char * nonrelative_template ; <nl>  <nl> - if (! template [ 0 ]) <nl> + if ( strlen ( template ) != strlen ( origtemplate )) <nl> template = origtemplate ; <nl>  <nl> nonrelative_template = absolute_path ( template );
const char * const local_repo_env [ LOCAL_REPO_ENV_SIZE + 1 ] = { <nl> static void setup_git_env ( void ) <nl> { <nl> git_dir = getenv ( GIT_DIR_ENVIRONMENT ); <nl> - if (! git_dir ) <nl> + if (! git_dir ) { <nl> git_dir = read_gitfile_gently ( DEFAULT_GIT_DIR_ENVIRONMENT ); <nl> + git_dir = git_dir ? xstrdup ( git_dir ) : NULL ; <nl> + } <nl> if (! git_dir ) <nl> git_dir = DEFAULT_GIT_DIR_ENVIRONMENT ; <nl> git_object_dir = getenv ( DB_ENVIRONMENT );
static void wt_shortstatus_print_tracking ( struct wt_status * s ) <nl> base = shorten_unambiguous_ref ( base , 0 ); <nl> color_fprintf ( s -> fp , header_color , "..."); <nl> color_fprintf ( s -> fp , branch_color_remote , "% s ", base ); <nl> + free (( char *) base ); <nl>  <nl> if (! upstream_is_gone && ! num_ours && ! num_theirs ) { <nl> fputc ( s -> null_termination ? '\ 0 ' : '\ n ', s -> fp );
static int setup_index ( unsigned char * sha1 ) <nl> return - 1 ; <nl>  <nl> new_pack = parse_pack_index ( sha1 ); <nl> + if (! new_pack ) <nl> + return - 1 ; /* parse_pack_index () already issued error message */ <nl> new_pack -> next = repo -> packs ; <nl> repo -> packs = new_pack ; <nl> return 0 ;
static void copy_templates ( const char * git_dir , int len , char * template_dir ) <nl> } <nl>  <nl> memcpy ( path , git_dir , len ); <nl> + path [ len ] = 0 ; <nl> copy_templates_1 ( path , len , <nl> template_path , template_len , <nl> dir );
static int fetch_object ( struct alt_base * repo , unsigned char * sha1 ) <nl> curl_result = curl_easy_perform ( curl ); <nl> curl_easy_setopt ( curl , CURLOPT_HTTPHEADER , no_range_header ); <nl> if ( curl_result != 0 ) { <nl> - unlink ( tmpfile ); <nl> return error ("% s ", curl_errorstr ); <nl> } <nl> 
traverse_dnode ( traverse_data_t * td , const dnode_phys_t * dnp , <nl> break ; <nl> } <nl>  <nl> - if ( dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) { <nl> + if ( err == 0 && dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) { <nl> SET_BOOKMARK (& czb , objset , object , 0 , DMU_SPILL_BLKID ); <nl> err = traverse_visitbp ( td , dnp , & dnp -> dn_spill , & czb ); <nl> }
arg_new_interval ( struct mail_search_build_context * ctx , <nl> } <nl> sarg -> value . search_flags = MAIL_SEARCH_ARG_FLAG_USE_TZ ; <nl> sarg -> value . time = ioloop_time - interval ; <nl> + sarg -> value . date_type = MAIL_SEARCH_DATE_TYPE_RECEIVED ; <nl> return sarg ; <nl> } <nl> 
static void cmd_user_ver2 ( struct doveadm_cmd_context * cctx ) <nl> ( void ) doveadm_cmd_param_str ( cctx , " field ", & show_field ); <nl> ( void ) doveadm_cmd_param_bool ( cctx , " userdb - only ", & userdb_only ); <nl>  <nl> + memset (& input , 0 , sizeof ( input )); <nl> if ( doveadm_cmd_param_array ( cctx , " auth - info ", & optval )) <nl> for (;* optval != NULL ; optval ++) <nl> auth_user_info_parse (& input . info , * optval );
static const struct command imap4rev1_commands [] = { <nl> { " APPEND ", cmd_append , COMMAND_FLAG_BREAKS_SEQS }, <nl> { " EXAMINE ", cmd_examine , COMMAND_FLAG_BREAKS_MAILBOX }, <nl> { " CREATE ", cmd_create , 0 }, <nl> - { " DELETE ", cmd_delete , COMMAND_FLAG_USE_NONEXISTENT }, <nl> + { " DELETE ", cmd_delete , COMMAND_FLAG_BREAKS_MAILBOX | <nl> + COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " RENAME ", cmd_rename , COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " LIST ", cmd_list , 0 }, <nl> { " LSUB ", cmd_lsub , 0 },
bool passdb_get_credentials ( struct auth_request * auth_request , <nl> /* anything goes . change the credentials_scheme to what we <nl> actually got , so blocking passdbs work . */ <nl> auth_request -> credentials_scheme = <nl> - p_strdup ( auth_request -> pool , input_scheme ); <nl> + p_strdup ( auth_request -> pool , t_strcut ( input_scheme , '.')); <nl> return TRUE ; <nl> } <nl> 
struct ostream * fs_write_stream ( struct fs_file * file ) <nl>  <nl> int fs_write_stream_finish ( struct fs_file * file , struct ostream ** output ) <nl> { <nl> - i_assert (* output == file -> output ); <nl> + i_assert (* output == file -> output || * output == NULL ); <nl>  <nl> * output = NULL ; <nl> return file -> fs -> v . write_stream_finish ( file , TRUE );
int mail_modifylog_mark_synced ( MailModifyLog * log ) <nl> { <nl> i_assert ( log -> index -> lock_type != MAIL_LOCK_UNLOCK ); <nl>  <nl> + if (! mmap_update ( log )) <nl> + return FALSE ; <nl> + <nl> if ( log -> header -> sync_id == SYNC_ID_FULL ) { <nl> /* log file is full , switch to next one */ <nl> return mail_modifylog_switch_file ( log );
imapc_sync_send_commands ( struct imapc_sync_context * ctx , uint32_t first_uid ) <nl> { <nl> string_t * cmd = t_str_new ( 64 ); <nl>  <nl> + if ( ctx -> mbox -> exists_count == 0 ) { <nl> + /* empty mailbox - no point in fetching anything */ <nl> + return ; <nl> + } <nl> + <nl> str_printfa ( cmd , " UID FETCH % u :* ( FLAGS ", first_uid ); <nl> if ( imapc_mailbox_has_modseqs ( ctx -> mbox )) { <nl> str_append ( cmd , " MODSEQ ");
int hash_format_init ( const char * format_string , struct hash_format ** format_r , <nl> } T_END ; <nl> if ( ret < 0 ) { <nl> * error_r = t_strdup (* error_r ); <nl> + pool_unref (& pool ); <nl> return - 1 ; <nl> } <nl> * format_r = format ;
static void o_stream_metawrap_call_callback ( struct metawrap_ostream * mstream ) <nl> if ( write_callback != NULL ) { <nl> mstream -> write_callback = NULL ; <nl> write_callback ( mstream -> context ); <nl> + /* metadata headers aren ' t counted as part of the offset */ <nl> + mstream -> ostream . ostream . offset = 0 ; <nl> } <nl> } <nl> 
squat_trie_lookup_real ( struct squat_trie * trie , const char * str , <nl> unsigned int i , start , bytes , str_bytelen , str_charlen ; <nl> int ret = 0 ; <nl>  <nl> + array_clear ( definite_uids ); <nl> + array_clear ( maybe_uids ); <nl> + <nl> memset (& ctx , 0 , sizeof ( ctx )); <nl> ctx . trie = trie ; <nl> ctx . type = type ;
static int dbox_sync_index_rebuild_dir ( struct dbox_sync_rebuild_context * ctx , <nl> dir = opendir ( path ); <nl> if ( dir == NULL ) { <nl> if ( errno == ENOENT ) { <nl> + if (! primary ) { <nl> + /* alt directory doesn ' t exist , ignore */ <nl> + return 0 ; <nl> + } <nl> mailbox_set_deleted (& ctx -> mbox -> ibox . box ); <nl> return - 1 ; <nl> }
void imapc_connection_connect ( struct imapc_connection * conn , <nl> unsigned int ips_count ; <nl> int ret ; <nl>  <nl> - if ( conn -> fd != - 1 ) { <nl> + if ( conn -> fd != - 1 || conn -> dns_lookup != NULL ) { <nl> i_assert ( login_callback == NULL ); <nl> return ; <nl> }
static void sasl_callback ( struct client * _client , enum sasl_server_reply reply , <nl> data : AUTH_FAILED_MSG , NULL ); <nl> client_send_line ( client , msg ); <nl>  <nl> - if (! client -> destroyed ) { <nl> + if (! client -> destroyed && ! client -> auth_initializing ) { <nl> /* get back to normal client input . */ <nl> if ( client -> io != NULL ) <nl> io_remove (& client -> io );
static void smtp_server_connection_input ( struct connection * _conn ) <nl> bool smtp_server_connection_pending_command_data ( <nl> struct smtp_server_connection * conn ) <nl> { <nl> + if ( conn -> smtp_parser == NULL ) <nl> + return FALSE ; <nl> return smtp_command_parser_pending_data ( conn -> smtp_parser ); <nl> } <nl> 
fs_list_get_path ( struct mailbox_list * _list , const char * name , <nl> i_assert ( mailbox_list_is_valid_name ( _list , name , & error )); <nl>  <nl> if ( mailbox_list_try_get_absolute_path ( _list , & name )) { <nl> + if ( type == MAILBOX_LIST_PATH_TYPE_INDEX && <nl> + * set -> index_dir == '\ 0 ') <nl> + return 0 ; <nl> * path_r = name ; <nl> return 1 ; <nl> }
DOVEADM_CMD_PARAMS_END <nl>  <nl> struct doveadm_cmd_ver2 doveadm_cmd_mailbox_delete_ver2 = { <nl> . name = " mailbox delete ", <nl> - . mail_cmd = cmd_mailbox_delete_alloc , <nl> - . usage = DOVEADM_CMD_MAIL_USAGE_PREFIX "[- s ] < mailbox > [...]", <nl> + . mail_cmd = cmd_mailbox_delete_alloc , <nl> + . usage = DOVEADM_CMD_MAIL_USAGE_PREFIX "[- s ] < mailbox > [...]", <nl> DOVEADM_CMD_PARAMS_START <nl> DOVEADM_CMD_MAIL_COMMON <nl> DOVEADM_CMD_PARAM (' s ', " subscriptions ", CMD_PARAM_BOOL , 0 )
static void tview_lookup_seq_range ( struct mail_index_view * view , <nl> if ( first_uid <= rec -> uid ) <nl> break ; <nl> } <nl> - if ( seq > tview -> t -> last_new_seq ) { <nl> + if ( seq > tview -> t -> last_new_seq || rec -> uid > last_uid ) { <nl> /* no messages in range */ <nl> return ; <nl> }
const struct stat * i_stream_stat ( struct istream * stream ) <nl> { <nl> struct _istream * _stream = stream -> real_stream ; <nl>  <nl> + if ( stream -> closed ) <nl> + return NULL ; <nl> + <nl> return _stream -> stat ( _stream ); <nl> } <nl> 
fts_backend_solr_update_deinit ( struct fts_backend_update_context * _ctx ) <nl> visible to the following search */ <nl> if ( ctx -> expunges ) <nl> fts_backend_solr_expunge_flush ( ctx ); <nl> - str = t_strdup_printf ("< commit waitFlush =\" false \" " <nl> - " waitSearcher =\"% s \"/>", <nl> + str = t_strdup_printf ("< commit waitSearcher =\"% s \"/>", <nl> ctx -> documents_added ? " true " : " false "); <nl> if ( solr_connection_post ( solr_conn , str ) < 0 ) <nl> ret = - 1 ;
static void driver_pgsql_close ( struct pgsql_db * db ) <nl> db -> io_dir = 0 ; <nl> db -> fatal_error = FALSE ; <nl>  <nl> + driver_pgsql_stop_io ( db ); <nl> + <nl> PQfinish ( db -> pg ); <nl> db -> pg = NULL ; <nl>  <nl> - driver_pgsql_stop_io ( db ); <nl> if ( db -> to_connect != NULL ) <nl> timeout_remove (& db -> to_connect ); <nl> 
void io_loop_handler_init ( struct ioloop * ioloop ) <nl> data -> fd_index = p_new ( ioloop -> pool , struct io_list *, data -> idx_size ); <nl>  <nl> data -> epfd = epoll_create ( INITIAL_EPOLL_EVENTS ); <nl> + if ( data -> epfd < 0 ) <nl> + i_panic (" epoll_create (): % m "); <nl> } <nl>  <nl> void io_loop_handler_deinit ( struct ioloop * ioloop )
static int snarf ( struct mailbox * srcbox , struct mailbox * destbox ) <nl> enum mail_error error ; <nl> int ret ; <nl>  <nl> + /* make sure the destination mailbox has been opened */ <nl> + if ( mailbox_open ( destbox ) < 0 ) <nl> + return - 1 ; <nl> + <nl> if ( mailbox_sync ( srcbox , MAILBOX_SYNC_FLAG_FULL_READ ) < 0 ) <nl> return - 1 ; <nl> 
static void db_ldap_get_fd ( struct ldap_connection * conn ) <nl> i_fatal (" LDAP : Can ' t get connection fd : % s ", <nl> ldap_err2string ( ret )); <nl> } <nl> + if ( conn -> fd <= CLIENT_LISTEN_FD ) { <nl> + /* Solaris LDAP library seems to be broken */ <nl> + i_fatal (" LDAP : Buggy LDAP library returned wrong fd : % d ", <nl> + conn -> fd ); <nl> + } <nl> i_assert ( conn -> fd != - 1 ); <nl> net_set_nonblock ( conn -> fd , TRUE ); <nl> }
db_oauth2_introspect_continue ( struct oauth2_introspection_result * result , <nl>  <nl> if (! result -> success ) { <nl> /* fail here */ <nl> + req -> result = PASSDB_RESULT_INTERNAL_FAILURE ; <nl> req -> failed = TRUE ; <nl> db_oauth2_callback ( req , FALSE , result -> error ); <nl> return ;
penalty_bump_checksum ( struct penalty_rec * rec , unsigned int checksum ) <nl> for ( i = 0 ; i < count ; i ++) { <nl> if ( checksums [ i ] == checksum ) { <nl> if ( i > 0 ) { <nl> - memcpy ( checksums + 1 , checksums , <nl> - sizeof ( checksums [ 0 ]) * i ); <nl> + memmove ( checksums + 1 , checksums , <nl> + sizeof ( checksums [ 0 ]) * i ); <nl> checksums [ 0 ] = checksum ; <nl> } <nl> return TRUE ;
static int <nl> imapc_mailbox_exists ( struct mailbox * box , bool auto_boxes ATTR_UNUSED , <nl> enum mailbox_existence * existence_r ) <nl> { <nl> + if ( strcmp ( box -> list -> name , MAILBOX_LIST_NAME_IMAPC ) != 0 ) { <nl> + if ( box -> inbox_any ) <nl> + * existence_r = MAILBOX_EXISTENCE_SELECT ; <nl> + else <nl> + * existence_r = MAILBOX_EXISTENCE_NONE ; <nl> + return 0 ; <nl> + } <nl> + <nl> enum mailbox_info_flags flags ; <nl>  <nl> struct imapc_mailbox_list * list = ( struct imapc_mailbox_list *) box -> list ;
struct file_dict_transaction_context { <nl> }; <nl>  <nl> static struct dotlock_settings file_dict_dotlock_settings = { <nl> - . timeout = 30 , <nl> - . stale_timeout = 5 <nl> + . timeout = 60 * 2 , <nl> + . stale_timeout = 60 , <nl> + . use_io_notify = TRUE <nl> }; <nl>  <nl> static struct dict * file_dict_init ( struct dict * driver , const char * uri ,
int openssl_cert_match_name ( SSL * ssl , const char * verify_name ) <nl> } <nl> } <nl> sk_GENERAL_NAME_pop_free ( gnames , GENERAL_NAME_free ); <nl> + X509_free ( cert ); <nl> + <nl> /* verify against CommonName only when there wasn ' t any DNS <nl> SubjectAltNames */ <nl> if ( dns_names )
const char * t_abspath ( const char * path ) <nl> { <nl> const char * dir ; <nl> + i_assert ( path != NULL ); <nl>  <nl> if (* path == '/') <nl> return path ; <nl> const char * t_abspath ( const char * path ) <nl>  <nl> const char * t_abspath_to ( const char * path , const char * root ) <nl> { <nl> + i_assert ( path != NULL ); <nl> + i_assert ( root != NULL ); <nl> + <nl> if (* path == '/') <nl> return path ; <nl> 
int shared_storage_get_namespace ( struct mail_namespace ** _ns , <nl> /* this user doesn ' t have a usable storage */ <nl> new_ns -> flags |= NAMESPACE_FLAG_UNUSABLE ; <nl> } <nl> + /* mark the shared namespace root as usable , since it now has <nl> + child namespaces */ <nl> + ns -> flags |= NAMESPACE_FLAG_USABLE ; <nl> * _name = mailbox_list_get_storage_name ( new_ns -> list , <nl> t_strconcat ( new_ns -> prefix , name , NULL )); <nl> * _ns = new_ns ;
static void mail_transaction_log_2_unlink_old ( struct mail_transaction_log * log ) <nl> return ; <nl> } <nl>  <nl> - if ( st . st_mtime + log -> index -> log_rotate_log2_stale_secs <= ioloop_time && <nl> + if ( ioloop_time - st . st_mtime >= ( time_t ) log -> index -> log_rotate_log2_stale_secs && <nl> ! log -> index -> readonly ) <nl> i_unlink_if_exists ( log -> filepath2 ); <nl> }
ssize_t i_stream_decrypt_read_header_v1 ( struct decrypt_istream * stream , <nl> buffer_t * key = buffer_create_dynamic ( pool_datastack_create (), 256 ); <nl>  <nl> hdr_len = (( data [ 0 ] << 8 ) | data [ 1 ]) + 12 ; <nl> - if ( mlen < hdr_len ) { <nl> + <nl> + if ( mlen < hdr_len - pos ) { <nl> /* try to read more */ <nl> return 0 ; <nl> }
int mailbox_get_guid ( struct mailbox * box , uint8_t guid [ MAIL_GUID_128_SIZE ]) <nl> if ( box -> v . get_guid == NULL ) { <nl> mail_storage_set_error ( box -> storage , MAIL_ERROR_NOTPOSSIBLE , <nl> " Storage doesn ' t support mailbox GUIDs "); <nl> + return - 1 ; <nl> } <nl> if (! box -> opened ) { <nl> if ( mailbox_open ( box ) < 0 )
static int auth_master_run_cmd ( struct auth_master_connection * conn , <nl> io_loop_run ( conn -> ioloop ); <nl> } <nl>  <nl> - auth_master_unset_io ( conn , prev_ioloop ); <nl> + if ( prev_ioloop != NULL ) <nl> + auth_master_unset_io ( conn , prev_ioloop ); <nl> if ( conn -> aborted ) { <nl> conn -> aborted = FALSE ; <nl> auth_connection_close ( conn );
ssize_t i_stream_read ( struct istream * stream ) <nl> errno = stream -> stream_errno ; <nl> } else { <nl> i_assert ( stream -> eof ); <nl> + i_assert ( old_size == _stream -> pos - _stream -> skip ); <nl> } <nl> break ; <nl> case 0 :
fts_search_arg_create_or ( const struct mail_search_arg * orig_arg , pool_t pool , <nl> array_foreach ( tokens , tokenp ) { <nl> arg = p_new ( pool , struct mail_search_arg , 1 ); <nl> * arg = * orig_arg ; <nl> + arg -> match_not = FALSE ; /* we copied this to the parent SUB */ <nl> arg -> next = NULL ; <nl> arg -> value . str = p_strdup ( pool , * tokenp ); <nl> 
uid_range_to_seqs ( struct fts_search_context * fctx , <nl> if (! array_is_created ( seq_range )) <nl> p_array_init ( seq_range , fctx -> result_pool , count ); <nl> for ( i = 0 ; i < count ; i ++) { <nl> + if ( range [ i ]. seq1 > range [ i ]. seq2 ) <nl> + continue ; <nl> mailbox_get_seq_range ( fctx -> box , range [ i ]. seq1 , range [ i ]. seq2 , <nl> & seq1 , & seq2 ); <nl> if ( seq1 != 0 )
mail_index_sync_ext_atomic_inc ( struct mail_index_sync_map_ctx * ctx , <nl> ext -> record_size ); <nl> return - 1 ; <nl> } <nl> - if ( u -> diff < 0 && ( uint32_t )(- u -> diff ) > orig_num ) { <nl> + if ( u -> diff < 0 && ( uint64_t )(- u -> diff ) > orig_num ) { <nl> mail_index_sync_set_corrupted ( ctx , <nl> " Extension record inc drops number below zero " <nl> "( uid =% u , diff =% d , orig =% llu )",
void buffer_verify_pool ( buffer_t * _buf ) <nl> const struct real_buffer * buf = ( const struct real_buffer *) _buf ; <nl> void * ret ; <nl>  <nl> - if ( buf -> pool != NULL && buf -> pool -> datastack_pool ) { <nl> + if ( buf -> pool != NULL && buf -> pool -> datastack_pool && buf -> alloc > 0 ) { <nl> /* this doesn ' t really do anything except verify the <nl> stack frame */ <nl> ret = p_realloc ( buf -> pool , buf -> w_buffer ,
static void push_notification_event_mailboxunsubscribe_event ( <nl>  <nl> data = p_new ( ptxn -> pool , <nl> struct push_notification_event_mailboxunsubscribe_data , 1 ); <nl> - data -> subscribe = TRUE ; <nl> + data -> subscribe = FALSE ; <nl>  <nl> push_notification_txn_mbox_set_eventdata ( ptxn , mbox , ec , data ); <nl> }
static void list_send ( struct list_send_context * ctx , struct list_node * node , <nl> name = node -> name ; <nl> send_name = name ; <nl>  <nl> - if ( node -> flags != MAILBOX_PLACEHOLDER ) <nl> + if ( node -> flags != MAILBOX_PLACEHOLDER && <nl> + node -> flags != MAILBOX_NOSELECT ) <nl> match = IMAP_MATCH_YES ; <nl> else { <nl> /* make sure the placeholder matches . */
quota_mailbox_delete_shrink_quota ( struct mailbox * box ) <nl> struct mail * mail ; <nl> struct mail_search_args * search_args ; <nl>  <nl> + if ( mailbox_mark_index_deleted ( box , TRUE ) < 0 ) <nl> + return - 1 ; <nl> + <nl> t = mailbox_transaction_begin ( box , 0 ); <nl> qt = quota_transaction_begin ( box ); <nl> 
password_scheme_detect ( const char * plain_password , const char * crypted_password , <nl> break ; <nl> key = NULL ; <nl> } <nl> + hash_table_iterate_deinit (& ctx ); <nl> return key ; <nl> } <nl> 
int http_header_parse_next_field ( struct http_header_parser * parser , <nl> const uoff_t max_size = parser -> limits . max_size ; <nl> const uoff_t max_field_size = parser -> limits . max_field_size ; <nl> const unsigned char * data ; <nl> - uoff_t size ; <nl> + size_t size ; <nl> int ret ; <nl>  <nl> * error_r = NULL ;
acl_backend_vfile_get_local_dir ( struct acl_backend * backend , const char * name ) <nl> dir = mailbox_list_get_path ( ns -> list , name , <nl> MAILBOX_LIST_PATH_TYPE_MAILBOX ); <nl> } <nl> - if ( name == NULL ) { <nl> + if ( name == NULL && dir != NULL ) { <nl> /* verify that the directory isn ' t same as INBOX ' s directory . <nl> this is mainly for Maildir . */ <nl> inbox = mailbox_list_get_path ( ns -> list , " INBOX ",
imapc_mail_get_stream ( struct mail * _mail , bool get_body , <nl> mail_set_aborted ( _mail ); <nl> return - 1 ; <nl> } <nl> + if ( _mail -> expunged ) { <nl> + /* We already detected that the mail is expunged . <nl> + Don ' t spend time trying to FETCH it again . */ <nl> + mail_set_expunged ( _mail ); <nl> + return - 1 ; <nl> + } <nl> fetch_field = get_body || <nl> ( data -> access_part & READ_BODY ) != 0 ? <nl> MAIL_FETCH_STREAM_BODY : MAIL_FETCH_STREAM_HEADER ;
void i_set_failure_prefix ( const char * prefix ) <nl> { <nl> i_free ( log_prefix ); <nl> log_prefix = i_strdup ( prefix ); <nl> - i_warning (" new prefix =% s ", prefix ); <nl> } <nl>  <nl> static int ATTR_FORMAT ( 2 , 0 )
client_deliver ( struct client * client , const struct mail_recipient * rcpt , <nl> input = mail_storage_service_user_get_input ( rcpt -> service_user ); <nl> username = t_strdup ( input -> username ); <nl>  <nl> - if ( rcpt -> parallel_count >= client -> lmtp_set -> lmtp_user_concurrency_limit ) { <nl> + if ( client -> lmtp_set -> lmtp_user_concurrency_limit > 0 && <nl> + rcpt -> parallel_count >= client -> lmtp_set -> lmtp_user_concurrency_limit ) { <nl> client_send_line ( client , ERRSTR_TEMP_USERDB_FAIL_PREFIX <nl> " Too many concurrent deliveries for user ", <nl> rcpt -> address );
static int mail_index_open_init ( MailIndex * index , int update_recent , <nl> index -> set_flags |= MAIL_INDEX_FLAG_REBUILD ; <nl> } <nl>  <nl> - return TRUE ; <nl> + /* finally reset the modify log marks , fsck or syncing might have <nl> + deleted some messages , and since we ' re only just opening the <nl> + index , there ' s no need to remember them */ <nl> + return mail_modifylog_mark_synced ( index -> modifylog ); <nl> } <nl>  <nl> static int mail_index_open_file ( MailIndex * index , const char * filename ,
static bool remote_ip_is_usable ( const struct ip_addr * ip ) <nl> return FALSE ; /* 10 / 8 */ <nl> if ( addr >= 3232235520 && addr <= 3232301055 ) <nl> return FALSE ; /* 192 . 168 / 16 */ <nl> + if ( addr >= 2886729728 && addr <= 2887778303 ) <nl> + return FALSE ; /* 172 . 16 / 12 */ <nl> if ( addr >= 2130706432 && addr <= 2147483647 ) <nl> return FALSE ; /* 127 / 8 */ <nl> }
cmd_save_to_mailbox ( struct save_cmd_context * ctx , struct mailbox * box , <nl> i_error (" open (% s ) failed : % s ", <nl> i_stream_get_name ( input ), <nl> i_stream_get_error ( input )); <nl> + ctx -> ctx . exit_code = EX_TEMPFAIL ; <nl> return - 1 ; <nl> } <nl> 
int mailbox_list_delete_trash ( const char * path , const char ** error_r ) <nl> errno = ELOOP ; <nl> return - 1 ; <nl> } <nl> + return - 1 ; <nl> } <nl> return 0 ; <nl> }
authd_abort_client ( struct Client * client_p ) <nl>  <nl> /* XXX should we blindly allow like this ? */ <nl> authd_decide_client ( client_p , "*", "*", true , '\ 0 ', NULL , NULL ); <nl> - <nl> client_p -> preClient -> authd_cid = 0 ; <nl> } <nl> 
lookup_ip ( const char * addr , int aftype , DNSCB callback , void * data ) <nl> return ( rid ); <nl> } <nl>  <nl> - uint32_t <nl> + static uint32_t <nl> get_nameservers ( DNSLISTCB callback , void * data ) <nl> { <nl> struct dnsstatreq * req = rb_malloc ( sizeof ( struct dnsstatreq ));
add_conf_item ( const char * topconf , const char * name , int type , void (* func ) ( voi <nl> if (( tc = find_top_conf ( topconf )) == NULL ) <nl> return - 1 ; <nl>  <nl> - if (( cf = find_conf_item ( tc , name )) != NULL ) <nl> + if ( find_conf_item ( tc , name ) != NULL ) <nl> return - 1 ; <nl>  <nl> cf = rb_malloc ( sizeof ( struct ConfEntry ));
rb_get_ssl_certfp ( rb_fde_t * F , uint8_t certfp [ RB_SSL_CERTFP_LEN ]) <nl> res == X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT ) <nl> { <nl> memcpy ( certfp , cert -> sha1_hash , RB_SSL_CERTFP_LEN ); <nl> + X509_free ( cert ); <nl> return 1 ; <nl> } <nl> X509_free ( cert );
initialize_global_set_options ( void ) <nl>  <nl> GlobalSetOptions . maxclients = ServerInfo . default_max_clients ; <nl>  <nl> - if ( GlobalSetOptions . maxclients > ( maxconnections - MAX_BUFFER )) <nl> + if ( GlobalSetOptions . maxclients > ( maxconnections - MAX_BUFFER ) || ( GlobalSetOptions . maxclients <= 0 )) <nl> GlobalSetOptions . maxclients = maxconnections - MAX_BUFFER ; <nl>  <nl> GlobalSetOptions . autoconn = 1 ;
void Curl_ossl_md5sum ( unsigned char * tmp , /* input */ <nl>  <nl> bool Curl_ossl_cert_status_request ( void ) <nl> { <nl> +# if ! defined ( HAVE_BORINGSSL ) && ! defined ( OPENSSL_NO_TLSEXT ) <nl> return TRUE ; <nl> +# else <nl> + return FALSE ; <nl> +# endif <nl> } <nl> # endif /* USE_SSLEAY */
static int ldap_win_bind_auth ( LDAP * server , const char * user , <nl> const char * passwd , unsigned long authflags ) <nl> { <nl> ULONG method = 0 ; <nl> - SEC_WINNT_AUTH_IDENTITY cred = { 0 , }; <nl> + SEC_WINNT_AUTH_IDENTITY cred ; <nl> int rc = LDAP_AUTH_METHOD_NOT_SUPPORTED ; <nl>  <nl> + memset (& cred , 0 , sizeof ( cred )); <nl> + <nl> # if defined ( USE_SPNEGO ) <nl> if ( authflags & CURLAUTH_NEGOTIATE ) { <nl> method = LDAP_AUTH_NEGOTIATE ;
static CURLcode file_range ( struct connectdata * conn ) <nl> else { <nl> /* X - Y */ <nl> totalsize = to - from ; <nl> + if ( totalsize == CURL_OFF_T_MAX ) <nl> + /* this is too big to increase , so bail out */ <nl> + return CURLE_RANGE_ERROR ; <nl> data -> req . maxdownload = totalsize + 1 ; /* include last byte */ <nl> data -> state . resume_from = from ; <nl> DEBUGF ( infof ( data , " RANGE from %" CURL_FORMAT_CURL_OFF_T
static CURLcode imap_statemach_act ( struct connectdata * conn ) <nl> if ( result ) <nl> return result ; <nl>  <nl> + /* Was there an error parsing the response line ? */ <nl> + if ( imapcode == - 1 ) <nl> + return CURLE_FTP_WEIRD_SERVER_REPLY ; <nl> + <nl> if ( imapcode ) { <nl> /* We have now received a full IMAP server response */ <nl> switch ( imapc -> state ) {
static CURLcode tftp_rx ( tftp_state_data_t * state , tftp_event_t event ) <nl> } <nl>  <nl> /* Check if completed ( That is , a less than full packet is received ) */ <nl> - if ( state -> rbytes < sizeof ( state -> spacket )){ <nl> + if ( state -> rbytes < ( ssize_t ) sizeof ( state -> spacket )){ <nl> state -> state = TFTP_STATE_FIN ; <nl> } <nl> else {
int Curl_resolv_timeout ( struct connectdata * conn , <nl>  <nl> * entry = NULL ; <nl>  <nl> + if ( timeoutms < 0 ) <nl> + /* got an already expired timeout */ <nl> + return CURLRESOLV_TIMEDOUT ; <nl> + <nl> # ifdef USE_ALARM_TIMEOUT <nl> if ( data -> set . no_signal ) <nl> /* Ignore the timeout when signals are disabled */
Curl_cookie_add ( struct Curl_easy * data , <nl> /* too long individual name or contents , or too long combination of <nl> name + contents . Chrome and Firefox support 4095 or 4096 bytes <nl> combo . */ <nl> - free ( co ); <nl> + freecookie ( co ); <nl> infof ( data , " oversized cookie dropped , name / val % d + % d bytes \ n ", <nl> nlen , len ); <nl> return NULL ;
int Curl_select ( int nfds , <nl> SET_SOCKERRNO ( EINVAL ); <nl> return - 1 ; <nl> } <nl> - timeout_ms = ( timeout -> tv_sec * 1000 ) + ( timeout -> tv_usec / 1000 ); <nl> + timeout_ms = ( int )( timeout -> tv_sec * 1000 ) + ( int )( timeout -> tv_usec / 1000 ); <nl> } <nl> else { <nl> timeout_ms = - 1 ;
_CURL_WARNING ( _curl_easy_getinfo_err_curl_socket , <nl> # endif <nl>  <nl> /* evaluates to true if expr is of type FILE * */ <nl> -# define _curl_is_FILE ( expr ) \ <nl> - ( __builtin_types_compatible_p ( __typeof__ ( expr ), FILE *)) <nl> +# define _curl_is_FILE ( expr ) \ <nl> + ( _curl_is_NULL ( expr ) || \ <nl> + ( __builtin_types_compatible_p ( __typeof__ ( expr ), FILE *))) <nl>  <nl> /* evaluates to true if expr can be passed as POST data ( void * or char *) */ <nl> # define _curl_is_postfields ( expr ) \
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> if ( res == CURLE_OK ) { <nl> bool retry = Curl_retry_request ( conn , & newurl ); <nl>  <nl> - if ( retry ) <nl> + if ( retry ) { <nl> follow = FOLLOW_RETRY ; <nl> + if (! newurl ) <nl> + res = CURLE_OUT_OF_MEMORY ; <nl> + } <nl> else { <nl> /* <nl> * We must duplicate the new URL here as the connection data may
CURLcode Curl_ntlm_create_type1_message ( const char * userp , <nl> *( dup_domain . tchar_ptr + domlen ) = TEXT ('\ 0 '); <nl> ntlm -> identity . Domain = dup_domain . tbyte_ptr ; <nl> ntlm -> identity . DomainLength = curlx_uztoul ( domlen ); <nl> - free ( dup_domain . tchar_ptr ); <nl> dup_domain . tchar_ptr = NULL ; <nl>  <nl> Curl_unicodefree ( useranddomain . tchar_ptr );
static CURLcode AddFormData ( struct FormData ** formp , <nl> file */ <nl> if (! strequal ("-", newform -> line )) { <nl> struct_stat file ; <nl> - if (! stat ( newform -> line , & file ) && S_ISREG ( file . st_mode )) <nl> + if (! stat ( newform -> line , & file ) && ! S_ISDIR ( file . st_mode )) <nl> * size += file . st_size ; <nl> else <nl> return CURLE_BAD_FUNCTION_ARGUMENT ;
CURLcode Curl_sasl_create_digest_md5_message ( struct SessionHandle * data , <nl> " qop =% s ", <nl> userp , realm , nonce , <nl> cnonce , nonceCount , spn , resp_hash_hex , qop ); <nl> + Curl_safefree ( spn ); <nl> if (! response ) <nl> return CURLE_OUT_OF_MEMORY ; <nl>  <nl> CURLcode Curl_sasl_create_digest_md5_message ( struct SessionHandle * data , <nl> result = Curl_base64_encode ( data , response , 0 , outptr , outlen ); <nl>  <nl> Curl_safefree ( response ); <nl> - Curl_safefree ( spn ); <nl>  <nl> return result ; <nl> }
# define OS " AmigaOS " <nl>  <nl> # define PACKAGE " curl " <nl> -# define PACKAGE_BUGREPORT " a suitable curl mailing list : https :// curl . haxx . se / mail /" <nl> +# define PACKAGE_BUGREPORT " a suitable mailing list : https :// curl . haxx . se / mail /" <nl> # define PACKAGE_NAME " curl " <nl> # define PACKAGE_STRING " curl -" <nl> # define PACKAGE_TARNAME " curl "
# undef ssize_t <nl>  <nl> /* Define this to ' int ' if socklen_t is not an available typedefed type */ <nl> -# undef socklen_t size_t <nl> +# define socklen_t size_t <nl>  <nl> /* Define this as a suitable file to read random data from */ <nl> # undef RANDOM_FILE
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> * an error , use the strerror () string or if things are so bad that not <nl> * even that is good , set a bad string that mentions the error code . <nl> */ <nl> - char * str = curl_easy_strerror ( res ); <nl> + const char * str = curl_easy_strerror ( res ); <nl> if (! str ) <nl> failf ( data , " unspecified error % d ", ( int ) res ); <nl> else
CURLcode Curl_conncache_add_conn ( struct conncache * connc , <nl> return result ; <nl>  <nl> key = hashkey ( conn ); <nl> - if (! key ) <nl> + if (! key ) { <nl> + bundle_destroy ( new_bundle ); <nl> return CURLE_OUT_OF_MEMORY ; <nl> + } <nl>  <nl> rc = conncache_add_bundle ( data -> state . conn_cache , key , new_bundle ); <nl> free ( key );
CURLMcode Curl_pipeline_set_site_blacklist ( char ** sites , <nl> bool Curl_pipeline_server_blacklisted ( struct SessionHandle * handle , <nl> char * server_name ) <nl> { <nl> - if ( handle -> multi ) { <nl> + if ( handle -> multi && server_name ) { <nl> struct curl_llist * blacklist = <nl> Curl_multi_pipelining_server_bl ( handle -> multi ); <nl> 
CURLcode Curl_setopt ( struct Curl_easy * data , CURLoption option , <nl> arg = va_arg ( param , long ); <nl> if ( arg < CURLSSH_AUTH_NONE ) <nl> return CURLE_BAD_FUNCTION_ARGUMENT ; <nl> - data -> set . ssh_auth_types = va_arg ( param , long ); <nl> + data -> set . ssh_auth_types = arg ; <nl> break ; <nl>  <nl> case CURLOPT_SSH_PUBLIC_KEYFILE :
CURLcode Curl_is_resolved ( struct connectdata * conn , <nl> if ( conn -> async . done ) { <nl> /* we ' re done , kill the ares handle */ <nl> if (! conn -> async . dns ) { <nl> - failf ( data , " Could not resolve host : % s (% s )", conn -> name , <nl> + failf ( data , " Could not resolve host : % s (% s )", conn -> host . dispname , <nl> ares_strerror ( conn -> async . status )); <nl> return CURLE_COULDNT_RESOLVE_HOST ; <nl> }
void Curl_updateconninfo ( struct connectdata * conn , curl_socket_t sockfd ) <nl> struct SessionHandle * data = conn -> data ; <nl> struct PureInfo * info = & conn -> data -> info ; <nl>  <nl> + if ( conn -> bits . reuse ) <nl> + /* reusing same connection */ <nl> + return ; <nl> + <nl> len = sizeof ( struct Curl_sockaddr_storage ); <nl> if ( getpeername ( sockfd , ( struct sockaddr *) & ssrem , & len )) { <nl> error = SOCKERRNO ;
mbed_connect_step3 ( struct connectdata * conn , <nl>  <nl> ret = mbedtls_ssl_get_session (& connssl -> ssl , our_ssl_sessionid ); <nl> if ( ret ) { <nl> + free ( our_ssl_sessionid ); <nl> failf ( data , " mbedtls_ssl_get_session returned - 0x % x ", - ret ); <nl> return CURLE_SSL_CONNECT_ERROR ; <nl> }
CURLcode Curl_output_ntlm_wb ( struct connectdata * conn , <nl> conn -> response_header = NULL ; <nl> break ; <nl> case NTLMSTATE_TYPE2 : <nl> - input = aprintf (" TT % s ", conn -> challenge_header ); <nl> + input = aprintf (" TT % s \ n ", conn -> challenge_header ); <nl> if (! input ) <nl> return CURLE_OUT_OF_MEMORY ; <nl> res = ntlm_wb_response ( conn , input , ntlm -> state );
static GlobCode glob_range ( URLGlob * glob , char ** patternp , <nl> } <nl> else <nl> step_n = 1 ; <nl> - if (* endp == ']') { <nl> + if ( endp && (* endp == ']')) { <nl> pattern = endp + 1 ; <nl> } <nl> else
_hb_ot_layout_set_glyph_class ( hb_face_t * face , <nl> unsigned char * new_klasses ; <nl>  <nl> new_len = len == 0 ? 120 : 2 * len ; <nl> - if ( new_len > 65535 ) <nl> - new_len = 65535 ; <nl> + if ( new_len > 65536 ) <nl> + new_len = 65536 ; <nl> new_klasses = ( unsigned char *) realloc ( layout -> new_gdef . klasses , new_len * sizeof ( unsigned char )); <nl>  <nl> if ( HB_UNLIKELY (! new_klasses ))
_pango_emoji_iter_next ( PangoEmojiIter * iter ) <nl> if ( iter -> is_emoji == PANGO_EMOJI_TYPE_IS_EMOJI ( current_emoji_type )) <nl> { <nl> iter -> is_emoji = ! PANGO_EMOJI_TYPE_IS_EMOJI ( current_emoji_type ); <nl> + <nl> + /* Make sure we make progress . Weird sequences , like a VC15 followed <nl> + * by VC16 , can trick us into stalling otherwise . */ <nl> + if ( iter -> start == iter -> end ) <nl> + iter -> end = g_utf8_next_char ( iter -> end ); <nl> + <nl> return TRUE ; <nl> } <nl> }
int TMomFinalizeChild ( <nl> /* Put the script ' s arguments on the command line ( see configure option -- enable - shell - use - argv ). */ <nl> if ( TJE -> is_interactive == FALSE ) <nl> { <nl> - arg [ aindex ] = calloc ( 1 , <nl> + arg [ aindex ] = ( char *) calloc ( 1 , <nl> strlen ( path_jobs ) + <nl> strlen ( pjob -> ji_qs . ji_fileprefix ) + <nl> strlen ( JOB_SCRIPT_SUFFIX ) + 6 );
dynamic_string * prepare_mom_hierarchy () <nl> int fds ; <nl> dynamic_string * send_format = NULL ; <nl>  <nl> + mh = initialize_mom_hierarchy (); <nl> + <nl> if (( fds = open ( path_mom_hierarchy , O_RDONLY , 0 )) < 0 ) <nl> { <nl> if ( errno == ENOENT )
int req_stat_job ( <nl> */ <nl>  <nl> snprintf ( name , sizeof ( name ), "% s ", preq -> rq_ind . rq_status . rq_id ); <nl> - name [( PBS_MAXSVRJOBID > PBS_MAXDEST ? PBS_MAXSVRJOBID : PBS_MAXDEST )] = '\ 0 '; <nl> + name [ sizeof ( name ) - 1 ] = '\ 0 '; <nl>  <nl> if (( name [ 0 ] == '\ 0 ') || ( name [ 0 ] == '@')) <nl> {
void check_busy ( double mla ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - void mom_is_request ( struct tcp_chan * chan , int version , int * cmdp ) <nl> + void mom_is_request ( struct tcp_chan * chan , int version , int * cmdp , struct sockaddr_in * pSockAddr ) <nl> { <nl> fprintf ( stderr , " The call to mom_is_request needs to be mocked !!\ n "); <nl> exit ( 1 );
int trq_main ( <nl> { <nl> printf (" Daemon exit requested \ n "); <nl> } <nl> - if ( trq_server_ip != NULL ) <nl> - free ( trq_server_ip ); <nl> if ( the_key != NULL ) <nl> free ( the_key ); <nl> return rc ;
bool Chip :: spread_place_cores ( <nl>  <nl> if ( fits == true ) <nl> { <nl> - int step_count = 1 ; <nl> + int step_count = step ; <nl> + <nl> + if ( lprocs_per_task_remaining == 1 ) <nl> + step_count = 1 ; <nl> + <nl>  <nl> /* cores_placed and cores_to_fill are used because we only want to make sure we <nl> fill the number of cores for this task */
int delete_cpuset ( <nl> */ <nl> else if (! strcmp ( pdirent -> d_name , " tasks ")) <nl> { <nl> + slept = 0 ; <nl> + <nl> do <nl> { <nl> npids = 0 ; <nl> - slept = 0 ; <nl> if (( fd = fopen ( path , " r ")) != NULL ) <nl> { <nl> while (( fgets ( tid , sizeof ( tid ), fd )) != NULL )
void Chip :: calculateStepCounts ( <nl> int & place_count_remaining ) <nl>  <nl> { <nl> + if ( lprocs_per_task == 0 ) <nl> + { <nl> + step = 0 ; <nl> + step_remainder = processing_units_per_task ; <nl> + place_count = 0 ; <nl> + return ; <nl> + } <nl> + <nl> if ( lprocs_per_task == 1 ) <nl> { <nl> step = ( processing_units_per_task / 2 ) + 1 ;
int procs_requested ( <nl> if ( proplist (& str , & prop , & num_procs , & num_gpus , & num_mics )) <nl> { <nl> free ( tmp_spec ); <nl> + if ( prop != NULL ) <nl> + free_prop ( prop ); <nl> return (- 1 ); <nl> } <nl> } <nl> int procs_requested ( <nl> { <nl> /* must be a prop list with no number in front */ <nl> free ( tmp_spec ); <nl> + if ( prop != NULL ) <nl> + free_prop ( prop ); <nl>  <nl> return (- 1 ); <nl> }
 <nl> START_TEST ( test_one ) <nl> { <nl> + /* As this is site specific , there is no implementation in this function */ <nl> + char * user = NULL ; <nl> + char * host = NULL ; <nl> + int rc = - 1 ; <nl> + rc = site_allow_u ( user , host ); <nl> + fail_unless ( rc == 0 , " The return value has changed !!!"); <nl> } <nl> END_TEST <nl> 
work_task * next_task ( <nl> pthread_mutex_lock ( at -> alltasks_mutex ); <nl>  <nl> wt = next_thing ( at -> ra , iter ); <nl> + if ( wt != NULL ) <nl> + pthread_mutex_lock ( wt -> wt_mutex ); <nl>  <nl> pthread_mutex_unlock ( at -> alltasks_mutex ); <nl>  <nl> if ( wt != NULL ) <nl> { <nl> - pthread_mutex_lock ( wt -> wt_mutex ); <nl> - <nl> if ( wt -> wt_being_recycled == TRUE ) <nl> { <nl> pthread_mutex_unlock ( wt -> wt_mutex );
 <nl> # define MAX_UPDATES_BEFORE_SENDING 20 <nl> # define PMOMTCPTIMEOUT 60 /* duration in seconds mom TCP requests will block */ <nl> +# define TCP_READ_PROTO_TIMEOUT 2 <nl>  <nl> /* Global Data Items */ <nl>  <nl> int tcp_read_proto_version ( <nl>  <nl> tmpT = pbs_tcp_timeout ; <nl>  <nl> - pbs_tcp_timeout = 0 ; <nl> + pbs_tcp_timeout = TCP_READ_PROTO_TIMEOUT ; <nl>  <nl> * proto = disrsi ( chan , & rc ); <nl> 
struct pbsnode * find_fitting_node ( <nl> if (( pnode = check_node ( ln , needed )) != NULL ) <nl> { <nl> ln -> times_used ++; <nl> + free_resizable_array ( ordered ); <nl> return ( pnode ); <nl> } <nl> }
void main_func ( <nl> script_tmp , /* O */ <nl> & ji )) != 0 ) <nl> { <nl> + fclose ( script_fp ); <nl> unlink ( script_tmp ); <nl>  <nl> exit ( 1 );
static char * active_pbs_server ; <nl> pbs_net_t trq_server_addr ; <nl> char trq_hostname [ PBS_MAXSERVERNAME + 1 ]; <nl>  <nl> +/* Get the name of the active pbs_server */ <nl> int load_trqauthd_config ( <nl>  <nl> char ** default_server_name ,
int pres_process_body ( publ_info_t * publ , str ** fin_body , int ver , str ** tuple_pa <nl> { <nl> if ( tuple == NULL ) <nl> { <nl> + if ( strlen ( tuple_id )>= 50 ) { <nl> + LM_ERR (" tuple id is too long : % s \ n ", tuple_id ); <nl> + goto error ; <nl> + } <nl> strcpy ( buf , tuple_id ); <nl> xmlFree ( tuple_id ); <nl> tuple_id = buf ;
__dialog_created ( struct dlg_cell * dlg , int type , struct dlg_cb_params * _params ) <nl> ( include_req_uri )?&( dlg -> req_uri ):&( dlg -> to_uri ), <nl> &( dlg -> callid ), 1 , dlginfo -> lifetime , <nl> 0 , 0 , 0 , 0 , ( send_publish_flag ==- 1 )? 1 : 0 ); <nl> - free_dlginfo_cell ( dlginfo ); <nl>  <nl> } <nl> 
int rx_send_str ( str * rx_session_id ) { <nl> // so just wait for STA or for Grace Timout to happen <nl> LM_DBG (" Hmmm , auth session already in disconnected state \ n "); <nl> cdpb . AAASessionsUnlock ( auth -> hash ); <nl> - CSCF_RETURN_FALSE ; <nl> + return CSCF_RETURN_FALSE ; <nl> } <nl>  <nl> LM_DBG (" Creating STR \ n ");
void clean_hdr_field ( struct hdr_field * hf ) <nl> break ; <nl>  <nl> case HDR_SESSIONEXPIRES_T : <nl> + if (* h_parsed ) { <nl> + (( hf_parsed_t *)(* h_parsed ))-> hfree (* h_parsed ); <nl> + * h_parsed = 0 ; <nl> + } <nl> + break ; <nl> + <nl> case HDR_MIN_SE_T : <nl> case HDR_ACCEPTCONTACT_T : <nl> case HDR_ALLOWEVENTS_T :
static int rtpproxy_set_store ( modparam_t type , void * val ){ <nl> return - 1 ; <nl> } <nl> } else {/* realloc to make room for the current set */ <nl> - rtpp_strings = ( char **) pkg_realloc ( rtpp_strings , <nl> + rtpp_strings = ( char **) pkg_reallocxf ( rtpp_strings , <nl> ( rtpp_sets + 1 )* sizeof ( char *)); <nl> if (! rtpp_strings ){ <nl> LM_ERR (" no pkg memory left \ n ");
int convert_temporary_dialog ( ua_pres_t * dialog ) <nl> temp_dialog = get_temporary_dialog ( dialog , hash_code ); <nl> if ( temp_dialog ) <nl> delete_htable ( temp_dialog , hash_code ); <nl> - else <nl> + else { <nl> + lock_release (& HashT -> p_records [ hash_code ]. lock ); <nl> return - 1 ; <nl> + } <nl>  <nl> insert_htable ( dialog , hash_code ); <nl> 
inline static str * binrpc_val_conv_str ( struct binrpc_ctx * ctx , <nl> s = int2str ( v -> u . intval , & len ); <nl> ret = ctl_malloc ( sizeof (* ret )+ len + 1 ); <nl> if ( ret == 0 || binrpc_gc_track ( ctx , ret )!= 0 ){ <nl> + if ( ret != 0 ) ctl_free ( ret ); <nl> * err = E_BINRPC_OVERFLOW ; <nl> return 0 ; <nl> }
void tm_ctx_set_branch_index ( int v ); <nl>  <nl> # else <nl>  <nl> -# define tm_ctx_get () <nl> +# define tm_ctx_get () NULL <nl> # define tm_ctx_init () <nl> # define tm_ctx_set_branch_index ( v ) <nl> 
dlg_cell_t * dlg_lookup ( unsigned int h_entry , unsigned int h_id ) <nl> dlg_cell_t * dlg ; <nl> dlg_entry_t * d_entry ; <nl>  <nl> + if ( d_table == NULL ) <nl> + return 0 ; <nl> + <nl> if ( h_entry >= d_table -> size ) <nl> goto not_found ; <nl> 
int build_path_vector ( struct sip_msg * _m , str * path , str ** received ) <nl> goto error ; <nl> } <nl> for (; params ; params = params -> next ) { <nl> - if ( params -> type == P_RECEIVED ) <nl> + if ( params -> type == P_RECEIVED ) { <nl> * received = & hooks . contact . received -> body ; <nl> + break ; <nl> + } <nl> } <nl> + free_params ( params ); <nl> } <nl> free_rr (& route ); <nl> }
if ( rtplen != prtpstat -> len ) <nl> LM_ERR (" Unable to find RTPSTAT pv !\ n "); <nl> goto initerr ; <nl> } <nl> - prtp_pv = pv_cache_get (& prtpstat [ 0 ]); <nl> + prtp_pv = pv_cache_get ( prtpstat ); <nl> if (! prtp_pv ) <nl> { <nl> LM_ERR (" Unable to find pv spec for RTPSTAT !\ n ");
kz_amqp_zone_ptr kz_primary_zone = NULL ; <nl> amqp_exchange_declare_ok_t * AMQP_CALL kz_amqp_exchange_declare ( amqp_connection_state_t state , amqp_channel_t channel , <nl> amqp_bytes_t exchange , amqp_bytes_t type , <nl> amqp_boolean_t passive , amqp_boolean_t durable , amqp_table_t arguments ) { <nl> -# if AMQP_VERSION_MINOR == 5 <nl> +# if AMQP_VERSION_MAJOR == 0 && AMQP_VERSION_MINOR < 6 <nl> return amqp_exchange_declare ( state , channel , exchange , type , passive , durable , arguments ); <nl> # else <nl> return amqp_exchange_declare ( state , channel , exchange , type , passive , durable , 0 , 0 , arguments );
sl_api_t slb ; <nl> /** module variables */ <nl> str dmq_request_method = str_init (" KDMQ "); <nl> dmq_worker_t * workers ; <nl> - dmq_peer_list_t * peer_list ; <nl> + dmq_peer_list_t * peer_list = 0 ; <nl> /* the list of dmq servers */ <nl> dmq_node_list_t * node_list ; <nl> // the dmq module is a peer itself for receiving notifications regarding nodes
static void mod_destroy ( void ) <nl> free_atom_fmt_buff (); <nl> free_list_fmt_buff (); <nl> free_xbuff_fmt_buff (); <nl> + free_pid_fmt_buff (); <nl> } <nl>  <nl> static int postprocess_request ( struct sip_msg * msg , unsigned int flags , void * _param ) <nl> static int postprocess_request ( struct sip_msg * msg , unsigned int flags , void * _p <nl> free_atom_fmt_buff (); <nl> free_list_fmt_buff (); <nl> free_xbuff_fmt_buff (); <nl> + free_pid_fmt_buff (); <nl> return 0 ; <nl> } <nl> 
static int init_mi_uptime ( void ) <nl> { <nl> char * p ; <nl>  <nl> + if ( kmi_up_since_ctime . s != 0 ) <nl> + return 0 ; <nl> time (& kmi_up_since ); <nl> p = ctime (& kmi_up_since ); <nl> kmi_up_since_ctime . len = strlen ( p )- 1 ;
int t_load_contacts ( struct sip_msg * msg , char * key , char * value ) <nl> return - 1 ; <nl> } <nl>  <nl> + memset ( next , 0 , sizeof ( struct contact )); <nl> next -> uri . s = branch -> uri ; <nl> next -> uri . len = branch -> len ; <nl> next -> dst_uri . s = branch -> dst_uri ;
static int mi_child_init ( void ) <nl> return - 1 ; <nl> } <nl> } <nl> + <nl> + if ( sruid_init (& _ul_sruid , '-', " ulcx ", SRUID_INC )< 0 ) <nl> + return - 1 ; <nl> done = 1 ; <nl>  <nl> return 0 ;
void rpc_shv_set ( rpc_t * rpc , void * c ) <nl> rpc -> fault ( c , 500 , " Cannot set shared variable value "); <nl> LM_ERR (" cannot set shv value \ n "); <nl> } else { <nl> - rpc -> printf ( c , " Ok . Variable set to new value ."); <nl> + rpc -> rpl_printf ( c , " Ok . Variable set to new value ."); <nl> } <nl>  <nl> unlock_shvar ( shv );
sca_reply ( sca_mod * scam , int status_code , char * status_msg , <nl> SCA_STR_COPY_CSTR ( & extra_headers , " Expires : " ); <nl>  <nl> len = snprintf ( extra_headers . s + extra_headers . len , <nl> - sizeof ( hdr_buf - extra_headers . len ), <nl> + sizeof ( hdr_buf ) - extra_headers . len , <nl> "% d % s ", expires , CRLF ); <nl> extra_headers . len += len ; <nl> 
static int do_load_gws ( struct sip_msg * _m , int grp_id ) <nl> } <nl> from_uri = get_from ( _m )-> uri ; <nl> } <nl> - if ( from_uri . len < MAX_FROM_URI_LEN ) { <nl> + if ( from_uri . len <= MAX_FROM_URI_LEN ) { <nl> strncpy ( from_uri_str , from_uri . s , from_uri . len ); <nl> from_uri_str [ from_uri . len ] = '\ 0 '; <nl> } else {
int tmx_check_pretran ( sip_msg_t * msg ) <nl> if ( likely ( vbr != NULL )) { <nl> svbranch = vbr -> value ; <nl> trim (& svbranch ); <nl> - dsize += svbranch . len ; <nl> + dsize += svbranch . len + 1 ; <nl> } <nl> if ( dsize < 256 ) dsize = 256 ; <nl> 
static int load_gws ( struct sip_msg * _m , int argc , action_u_t argv []) <nl> } <nl> /* Do not look further if this matching rule was stopper */ <nl> if ( rule -> stopper == 1 ) goto done ; <nl> + } else { <nl> + LM_DBG (" from uri <%.* s > did not match to from regex <%.* s >", <nl> + from_uri . len , from_uri . s , rule -> from_uri_len , <nl> + rule -> from_uri ); <nl> } <nl> } <nl> rule = rule -> next ;
rs_set_warmth_auto ( RS_BLOB * rs ) <nl> gdouble dsum [ 8 ], dmax ; <nl> gfloat tint , warmth ; <nl>  <nl> + if ( unlikely (! rs -> in_use )) return ; <nl> + <nl> for ( row = 0 ; row < rs -> input -> h - 7 ; row += 8 ) <nl> for ( col = 0 ; col < rs -> input -> w - 7 ; col += 8 ) <nl> {
rs_cms_get_intent ( RS_CMS * cms ) <nl> void * <nl> rs_cms_get_transform ( RS_CMS * cms , CMS_TRANSFORM transform ) <nl> { <nl> + if (! cms -> enabled ) return NULL ; <nl> if ( transform > ( TRANSFORMS - 1 )) return ( NULL ); <nl> return ( cms -> transforms [ transform ]); <nl> }
batch_queue_save ( RS_QUEUE * queue ) <nl> xmlTextWriterWriteFormatElement ( writer , BAD_CAST " filename ", "% s ", filename ); <nl> xmlTextWriterWriteFormatElement ( writer , BAD_CAST " snapshot ", "% d ", setting_id ); <nl> xmlTextWriterEndElement ( writer ); <nl> + g_free ( filename ); <nl> } while ( gtk_tree_model_iter_next ( queue -> list , & iter )); <nl>  <nl> xmlTextWriterEndDocument ( writer );
makernote_nikon ( RAWFILE * rawfile , guint offset , RSMetadata * meta ) <nl> || g_str_equal ( meta -> model_ascii , " NIKON 1 V1 ") <nl> || g_str_equal ( meta -> model_ascii , " NIKON 1 V2 ") <nl> || g_str_equal ( meta -> model_ascii , " NIKON D7000 ") <nl> + || g_str_equal ( meta -> model_ascii , " NIKON D7100 ") <nl> || g_str_equal ( meta -> model_ascii , " COOLPIX P7700 ")) <nl> { <nl> meta -> cam_mul [ 0 ] = get_rational ( rawfile , offset );
gui_drawingarea_motion_callback ( GtkWidget * widget , GdkEventMotion * event , RS_BLO <nl> gint y = ( gint ) event -> y ; <nl> gushort * pixel ; <nl>  <nl> + if (! rs -> photo ) return FALSE ; <nl> + <nl> /* Draw RGB - values at bottom of screen */ <nl> gui_set_values ( rs , x , y ); <nl> 
ACTION ( copy_settings ) <nl>  <nl> ACTION ( paste_settings ) <nl> { <nl> - gint mask ; <nl> + gint mask = 0xffffff ; <nl>  <nl> GtkWidget * dialog , * cb_box ; <nl> GtkWidget * cb_exposure , * cb_saturation , * cb_hue , * cb_contrast , * cb_whitebalance , * cb_curve , * cb_sharpen ;
GtkWidget * <nl> gui_make_scale_from_adj ( RS_BLOB * rs , GCallback cb , GtkObject * adj , gint mask ) <nl> { <nl> GtkWidget * hscale , * box , * rimage , * revent ; <nl> - struct reset_carrier * rc = malloc ( sizeof ( struct reset_carrier )); <nl> + struct reset_carrier * rc = g_malloc ( sizeof ( struct reset_carrier )); <nl> rc -> rs = rs ; <nl> rc -> mask = mask ; <nl> 
gui_save_file_callback ( gpointer callback_data , guint callback_action , GtkWidget <nl> if ( filetype_str ) <nl> if ( g_str_equal ( savers [ n ]. extension , filetype_str )) <nl> gtk_combo_box_set_active ( GTK_COMBO_BOX ( filetype ), n ); <nl> + g_free ( filetype_str ); <nl> n ++; <nl> } <nl> if ( gtk_combo_box_get_active ( GTK_COMBO_BOX ( filetype )) == - 1 )
static void dump_index ( demuxer_t * demuxer , int stream_id ) <nl> if ( verbose <= 1 ) <nl> return ; <nl>  <nl> - if ( stream_id > MAX_STREAMS ) <nl> + if ( stream_id >= MAX_STREAMS ) <nl> return ; <nl>  <nl> index = priv -> index_table [ stream_id ];
void uninit_player ( struct MPContext * mpctx , unsigned int mask ){ <nl> mpctx -> timeline = NULL ; <nl> mpctx -> num_timeline_parts = 0 ; <nl> talloc_free ( mpctx -> chapters ); <nl> + mpctx -> chapters = NULL ; <nl> mpctx -> num_chapters = 0 ; <nl> mpctx -> video_offset = 0 ; <nl> if ( mpctx -> demuxer ){
# include " config . h " <nl> # include " mp_msg . h " <nl>  <nl> -# include " fastmemcpy . h " <nl> +# include "../../ libvo / fastmemcpy . h " <nl>  <nl> # include " libmpdemux / nuppelvideo . h " <nl> # include " RTjpegN . h "
int mp_header_process_sequence_header ( mp_mpeg_header_t * picture , const unsigne <nl> picture -> mpeg1 = 1 ; <nl> picture -> picture_structure = 3 ; // FRAME_PICTURE ; <nl> picture -> display_time = 100 ; <nl> + picture -> frame_rate_extension_n = 1 ; <nl> + picture -> frame_rate_extension_d = 1 ; <nl> return 0 ; <nl> } <nl> 
static void drm_egl_uninit ( MPGLContext * ctx ) <nl>  <nl> static int drm_egl_init ( struct MPGLContext * ctx , int flags ) <nl> { <nl> + if ( ctx -> vo -> probing ) { <nl> + MP_VERBOSE ( ctx -> vo , " DRM EGL backend can be activated only manually .\ n "); <nl> + return - 1 ; <nl> + } <nl> struct priv * p = ctx -> priv ; <nl> p -> kms = NULL ; <nl> p -> old_crtc = NULL ;
void mp_msg_va ( struct mp_log * log , int lev , const char * format , va_list va ) <nl>  <nl> set_msg_color ( stream , lev ); <nl> if ( header ) { <nl> - if ( lev >= MSGL_V || verbose || mp_msg_module ) { <nl> + if (( lev >= MSGL_V && lev != MSGL_SMODE ) || verbose || mp_msg_module ) { <nl> fprintf ( stream , "[% s ] ", log -> verbose_prefix ); <nl> } else if ( log -> prefix ) { <nl> fprintf ( stream , "[% s ] ", log -> prefix );
static void pass_prepare_src_tex ( struct gl_video * p ) <nl> static void render_pass_quad ( struct gl_video * p , int vp_w , int vp_h , <nl> const struct mp_rect * dst , int flags ) <nl> { <nl> - struct vertex va [ 4 ]; <nl> + struct vertex va [ 4 ] = { 0 }; <nl>  <nl> struct gl_transform t ; <nl> gl_transform_ortho (& t , 0 , vp_w , 0 , vp_h );
static mp_cmd_t * interpret_key ( struct input_ctx * ictx , int code ) <nl> code &= ~ KEY_MODIFIER_SHIFT ; <nl>  <nl> if ( code & MP_KEY_DOWN ) { <nl> - if ( ictx -> num_key_down > MP_MAX_KEY_DOWN ) { <nl> + if ( ictx -> num_key_down >= MP_MAX_KEY_DOWN ) { <nl> mp_tmsg ( MSGT_INPUT , MSGL_ERR , " Too many key down events " <nl> " at the same time \ n "); <nl> return NULL ;
static int demux_mkv_read_tags ( demuxer_t * demuxer ) <nl> demux_info_add_bstr ( demuxer , tag . simple_tag [ j ]. tag_name , tag . simple_tag [ j ]. tag_string ); <nl> } <nl>  <nl> + talloc_free ( parse_ctx . talloc_ctx ); <nl> return 0 ; <nl> } <nl> 
static int control ( stream_t * stream , int cmd , void * arg ) <nl> int n = dvdnav_describe_title_chapters ( dvdnav , tit , & parts , & duration ); <nl> if (! parts ) <nl> break ; <nl> - if ( chapter < 0 || chapter + 1 >= n ) <nl> + if ( chapter < 0 || chapter + 1 > n ) <nl> break ; <nl> * ch = chapter > 0 ? parts [ chapter - 1 ] / 90000 . 0 : 0 ; <nl> free ( parts );
static void ao_chain_uninit ( struct ao_chain * ao_c ) <nl> talloc_free ( ao_c -> conv ); <nl> talloc_free ( ao_c -> input_frame ); <nl> talloc_free ( ao_c -> input_format ); <nl> + talloc_free ( ao_c -> output_frame ); <nl> talloc_free ( ao_c -> filter_input_format ); <nl> talloc_free ( ao_c -> ao_buffer ); <nl> talloc_free ( ao_c );
static int find_entrypoint ( int format , VAEntrypoint * ep , int num_ep ) <nl>  <nl> static int is_direct_mapping ( VADisplay display ) <nl> { <nl> - VADisplayAttribute attr ; <nl> + VADisplayAttribute attr = { 0 }; <nl> VAStatus status ; <nl>  <nl> # if VA_CHECK_VERSION ( 0 , 34 , 0 )
void demux_seek_mpg ( demuxer_t * demuxer , float rel_seek_secs , float audio_delay , in <nl> continue ; <nl> } <nl> } <nl> + if (! sh_video ) break ; <nl> i = sync_video_packet ( d_video ); <nl> if ( sh_video -> format == 0x10000004 ) { // mpeg4 <nl> if ( i == 0x1B6 ) { // vop ( frame ) startcode
static void vo_wayland_border ( struct vo * vo ) <nl> static void vo_wayland_fullscreen ( struct vo * vo ) <nl> { <nl> struct vo_wayland_state * wl = vo -> wayland ; <nl> - if (! wl -> display . shell ) <nl> + if (! wl -> display . shell || !! vo -> opts -> fullscreen == wl -> window . is_fullscreen ) <nl> return ; <nl>  <nl> struct wl_output * fs_output = wl -> display . fs_output ;
static int control ( sh_video_t * sh , int cmd , void * arg ,...){ <nl> va_start ( ap , arg ); <nl> value = va_arg ( ap , int ); <nl> va_end ( ap ); <nl> - if ( DS_VideoDecoder_SetValue ( sh -> context , arg , value )== 0 ) <nl> + if ( DS_VideoDecoder_SetValue ( sh -> context , arg , 50 + value / 2 )== 0 ) <nl> return CONTROL_OK ; <nl> return CONTROL_FALSE ; <nl> }
struct mp_csp_equalizer * gl_video_eq_ptr ( struct gl_video * p ) <nl> // Call when the mp_csp_equalizer returned by gl_video_eq_ptr () was changed . <nl> void gl_video_eq_update ( struct gl_video * p ) <nl> { <nl> + gl_video_reset_surfaces ( p ); <nl> } <nl>  <nl> static int validate_scaler_opt ( struct mp_log * log , const m_option_t * opt ,
static const struct gl_functions gl_functions [] = { <nl> // uniform buffer object extensions , requires OpenGL 3 . 1 . <nl> { <nl> . ver_core = 310 , <nl> - . extension = " ARB_uniform_buffer_object ", <nl> + . extension = " GL_ARB_uniform_buffer_object ", <nl> . functions = ( const struct gl_function []) { <nl> DEF_FN ( GetUniformBlockIndex ), <nl> DEF_FN ( UniformBlockBinding ),
static void print_status ( float a_pos , float a_v , float corr ) <nl> width = screen_width ; <nl> else <nl> width = 80 ; <nl> +# ifdef WIN32 <nl> + // windows command line is broken ( MinGW ' s rxvt works though , but we <nl> + // should not depend on that ). <nl> + width --; <nl> +# endif <nl> line = malloc ( width + 1 ); // one additional for terminating null <nl>  <nl> // Audio time
struct exports exp_msvcr80 []={ <nl> FF ( _initterm_e , - 1 ) <nl> FF ( _initterm , - 1 ) <nl> FF ( _decode_pointer , - 1 ) <nl> +/* needed by KGV1 - VFW . dll */ <nl> + {"?? 2 @ YAPAXI @ Z ", - 1 , expnew }, <nl> + {"?? 3 @ YAXPAX @ Z ", - 1 , expdelete } <nl> }; <nl>  <nl> struct exports exp_msvcp60 []={
d_dvdsub = demuxer -> sub ; <nl> sh_audio = d_audio -> sh ; <nl> sh_video = d_video -> sh ; <nl>  <nl> + if (! sh_video ) <nl> + { <nl> + mp_msg ( MSGT_CPLAYER , MSGL_FATAL ," Video stream is mandatory !\ n "); <nl> + mencoder_exit ( 1 , NULL ); <nl> + } <nl> + <nl> if (! video_read_properties ( sh_video )){ <nl> printf ( MSGTR_CannotReadVideoProperties ); <nl> mencoder_exit ( 1 , NULL );
if ( verbose > 1 ){ <nl> mpi -> stride [ 0 ]= lavc_picture . linesize [ 0 ]; <nl> mpi -> stride [ 1 ]= lavc_picture . linesize [ 1 ]; <nl> mpi -> stride [ 2 ]= lavc_picture . linesize [ 2 ]; <nl> + if ( lavc_context . pix_fmt == PIX_FMT_YUV422P ){ <nl> + mpi -> stride [ 1 ]*= 2 ; <nl> + mpi -> stride [ 2 ]*= 2 ; <nl> + } <nl>  <nl> // stride [ 1 ]= stride [ 2 ]= 0 ; <nl> // stride [ 0 ]/= 2 ;
struct demux_packet * demux_read_packet ( struct sh_stream * sh ) <nl> // packets from the queue . <nl> double demux_get_next_pts ( struct sh_stream * sh ) <nl> { <nl> - if ( sh ) { <nl> + if ( sh && sh -> ds -> selected ) { <nl> ds_get_packets ( sh ); <nl> if ( sh -> ds -> head ) <nl> return sh -> ds -> head -> pts ;
void gl_video_upload_image ( struct gl_video * p , struct mp_image * mpi ) <nl> p -> osd_pts = mpi -> pts ; <nl>  <nl> if ( p -> hwdec_active ) { <nl> + talloc_free ( vimg -> hwimage ); <nl> vimg -> hwimage = mpi ; <nl> p -> have_image = true ; <nl> return ;
static HRESULT init_session_display ( struct wasapi_state * state ) { <nl> exit_label : <nl> MP_ERR ( state , " Error setting audio session display name : % s ( 0x %" PRIx32 ")\ n ", <nl> wasapi_explain_err ( hr ), ( uint32_t ) hr ); <nl> - return hr ; <nl> + // No reason to abort initialization . <nl> + return S_OK ; <nl> } <nl>  <nl> static HRESULT fix_format ( struct ao * ao )
dvb_config_t * dvb_get_config ( void ) <nl> } <nl>  <nl> if (( access ( conf_file , F_OK | R_OK ) != 0 )) <nl> + { <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> conf_file = get_path (" channels . conf "); <nl> + } <nl>  <nl> list = dvb_get_channels ( conf_file , type ); <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> if ( list == NULL ) <nl> continue ; <nl> 
int streaming_bufferize ( streaming_ctrl_t * streaming_ctrl , char * buffer , int siz <nl> int nop_streaming_read ( int fd , char * buffer , int size , streaming_ctrl_t * stream_ctrl ); <nl> int nop_streaming_seek ( int fd , off_t pos , streaming_ctrl_t * stream_ctrl ); <nl>  <nl> + int connect2Server ( char * host , int port ); <nl> + <nl> # endif
 <nl> static int av_log_level_to_mp_level ( int av_level ) <nl> { <nl> + if ( av_level > AV_LOG_VERBOSE ) <nl> + return MSGL_DBG2 ; <nl> if ( av_level > AV_LOG_INFO ) <nl> return MSGL_V ; <nl> if ( av_level > AV_LOG_WARNING )
void vo_seek_reset ( struct vo * vo ) <nl> { <nl> vo_control ( vo , VOCTRL_RESET , NULL ); <nl> vo -> frame_loaded = false ; <nl> + vo -> hasframe = false ; <nl> mp_image_unrefp (& vo -> waiting_mpi ); <nl> } <nl> 
void add_subtitles ( char * filename , float fps , int silent ) <nl> # ifdef USE_ASS <nl> if ( ass_enabled ) <nl> asst = ass_read_file ( filename ); <nl> - if ( ass_enabled && ! asst ) <nl> + if ( ass_enabled && subd && ! asst ) <nl> asst = ass_read_subdata ( subd , fps ); <nl>  <nl> if (! asst && ! subd && ! silent )
static void update_osd_msg ( void ) { <nl> # ifdef USE_OSD <nl> if ( sh_video ) vo_osd_changed ( OSDTYPE_OSD ); else <nl> # endif <nl> - if ( term_osd ) printf ("% s % s \ n ", term_osd_esc , msg -> msg ); <nl> + if ( term_osd ) mp_msg ( MSGT_CPLAYER , MSGL_STATUS ,"% s % s \ n ", term_osd_esc , msg -> msg ); <nl> } <nl> return ; <nl> }
static bool get_screenshot_from_texture ( d3d_priv * priv , mp_image_t * image ) <nl> struct texplane * plane = & priv -> planes [ n ]; <nl>  <nl> int width = priv -> src_width >> plane -> shift_x ; <nl> - int height = priv -> src_height >> plane -> shift_x ; <nl> + int height = priv -> src_height >> plane -> shift_y ; <nl>  <nl> memcpy_pic ( image -> planes [ n ], plane -> locked_rect . pBits , <nl> width * plane -> bytes_per_pixel , height ,
static int af_find_output_conversion ( struct af_stream * s , struct mp_audio * cfg ) <nl> ! mp_chmap_equals_reordered (& af -> fmt_in . channels , & af -> fmt_out . channels )) <nl> return AF_ERROR ; <nl> } <nl> + // And not if it ' s the only filter . <nl> + if ( conv -> prev == s -> first && conv -> next == s -> last ) <nl> + return AF_ERROR ; <nl>  <nl> * cfg = s -> output ; <nl> return AF_OK ;
case 0 : <nl> break ; <nl> case VCODEC_FRAMENO : <nl> mux_v -> buffer =& decoded_frameno ; // tricky <nl> - aviwrite_write_chunk ( muxer , mux_v , muxer_f , sizeof ( int ), 0 ); <nl> + if ( skip_flag <= 0 ) aviwrite_write_chunk ( muxer , mux_v , muxer_f , sizeof ( int ), 0 ); <nl> break ; <nl> case VCODEC_DIVX4 : <nl> blit_frame = decode_video (& video_out , sh_video , start , in_size , 0 );
struct vo * init_best_video_out ( struct MPOpts * opts , <nl> // continue ... <nl> free ( name ); <nl> ++ vo_list ; <nl> - if (!( vo_list [ 0 ])) <nl> + if (!( vo_list [ 0 ])) { <nl> + talloc_free ( vo ); <nl> return NULL ; // do NOT fallback to others <nl> + } <nl> } <nl> // now try the rest ... <nl> vo_subdevice = NULL ;
double get_current_time ( struct MPContext * mpctx ) <nl> return 0 ; <nl> if ( demuxer -> stream_pts != MP_NOPTS_VALUE ) <nl> return demuxer -> stream_pts ; <nl> + if ( mpctx -> hrseek_active ) <nl> + return mpctx -> hrseek_pts ; <nl> double apts = playing_audio_pts ( mpctx ); <nl> if ( apts != MP_NOPTS_VALUE ) <nl> return apts ;
static int seek_to_chapter ( stream_t * stream , ifo_handle_t * vts_file , tt_srpt_t * <nl> if (! vts_file || ! tt_srpt ) <nl> return 0 ; <nl>  <nl> - if ( chapter < 0 || chapter > vts_file -> vts_ptt_srpt -> title [ title_no ]. nr_of_ptts ) // no such chapter <nl> + if ( chapter < 0 || chapter > vts_file -> vts_ptt_srpt -> title [ title_no ]. nr_of_ptts - 1 ) // no such chapter <nl> return 0 ; <nl>  <nl> ptt = vts_file -> vts_ptt_srpt -> title [ title_no ]. ptt [ chapter ];
static void draw_image ( struct vo * vo , struct mp_image * mpi ) <nl> return ; <nl> } <nl> mp_image_copy_attributes ( dst , mpi ); <nl> - mpi = dst ; <nl> + talloc_free ( mpi ); <nl> + mpi = mp_image_new_ref ( dst ); <nl> } <nl>  <nl> talloc_free ( p -> output_surfaces [ p -> output_surface ]);
bool ca_asbd_equals ( const AudioStreamBasicDescription * a , <nl> a -> mBitsPerChannel == b -> mBitsPerChannel && <nl> ca_normalize_formatid ( a -> mFormatID ) == <nl> ca_normalize_formatid ( b -> mFormatID ) && <nl> - a -> mBytesPerPacket == b -> mBytesPerPacket ; <nl> + a -> mBytesPerPacket == b -> mBytesPerPacket && <nl> + a -> mChannelsPerFrame == b -> mChannelsPerFrame && <nl> + a -> mSampleRate == b -> mSampleRate ; <nl> } <nl>  <nl> // Return the AF_FORMAT_ * ( AF_FORMAT_S16 etc .) corresponding to the asbd .
static enum check_result libztex_checkDevice ( struct libusb_device * dev ) <nl> ret = CHECK_RESCAN ; <nl>  <nl> done : <nl> + free ( fw_buf ); <nl> if ( fp ) <nl> fclose ( fp ); <nl> if ( hndl )
void ft232r_close ( struct ft232r_device_handle * dev ) <nl> libusb_release_interface ( dev -> h , 0 ); <nl> libusb_reset_device ( dev -> h ); <nl> libusb_close ( dev -> h ); <nl> + free ( dev ); <nl> } <nl>  <nl> bool ft232r_purge_buffers ( struct ft232r_device_handle * dev , enum ft232r_reset_purge purge )
static bool avalon_detect_one ( libusb_device * dev , struct usb_find_devices * found <nl> info -> temp_sum = 0 ; <nl> info -> temp_old = 0 ; <nl>  <nl> - ret = avalon_reset ( avalon , true ); <nl> - if ( ret && ! configured ) <nl> + if (! add_cgpu ( avalon )) <nl> goto unshin ; <nl>  <nl> - if (! add_cgpu ( avalon )) <nl> + ret = avalon_reset ( avalon , true ); <nl> + if ( ret && ! configured ) <nl> goto unshin ; <nl>  <nl> update_usb_stats ( avalon );
void hashmeter2 ( struct thr_info * thr ) <nl>  <nl> gettimeofday (& tv_now , NULL ); <nl> timersub (& tv_now , & thr -> tv_lastupdate , & tv_elapsed ); <nl> - if ( tv_elapsed . tv_sec >= opt_log_interval ) { <nl> + /* Update the hashmeter at most 5 times per second */ <nl> + if ( tv_elapsed . tv_sec > 0 || tv_elapsed . tv_usec > 200 ) { <nl> hashmeter ( thr -> id , & tv_elapsed , thr -> hashes_done ); <nl> thr -> hashes_done = 0 ; <nl> thr -> tv_lastupdate = tv_now ;
static bool setup_stratum_curl ( struct pool * pool ) <nl> quit ( 1 , " Failed to curl_easy_init in initiate_stratum "); <nl> if ( pool -> sockbuf ) <nl> pool -> sockbuf [ 0 ] = '\ 0 '; <nl> - mutex_unlock (& pool -> stratum_lock ); <nl>  <nl> curl = pool -> stratum_curl ; <nl>  <nl> static bool setup_stratum_curl ( struct pool * pool ) <nl> pool -> cgminer_pool_stats . times_sent ++; <nl> pool -> cgminer_pool_stats . times_received ++; <nl>  <nl> + mutex_unlock (& pool -> stratum_lock ); <nl> + <nl> return true ; <nl> } <nl> 
void decay_time ( double * f , double fadd ) <nl> ratio = 1 / ratio ; <nl> } <nl>  <nl> - if ( ratio > 0 . 9 ) <nl> + if ( ratio > 0 . 95 ) <nl> * f = ( fadd * 0 . 1 + * f ) / 1 . 1 ; <nl> else <nl> * f = ( fadd + * f * 0 . 1 ) / 1 . 1 ;
void zero_stats ( void ) <nl> total_ro = 0 ; <nl> total_secs = 1 . 0 ; <nl> total_diff1 = 0 ; <nl> + total_bad_nonces = 0 ; <nl> found_blocks = 0 ; <nl> total_diff_accepted = 0 ; <nl> total_diff_rejected = 0 ;
bool load_bitstream_intelhex ( bytes_t * rv , const char * dname , const char * fn ) <nl> applog ( LOG_ERR , " Error reading '% s '", fn ); <nl> goto ihxerr ; <nl> } <nl> - fgets ( buf , sizeof ( buf ), F ); <nl> + if (! fgets ( buf , sizeof ( buf ), F )) <nl> + goto ihxerr ; <nl> if ( unlikely ( buf [ 0 ] != ':')) <nl> goto ihxerr ; <nl> if ( unlikely (!(
extern FILE * open_bitstream ( const char * dname , const char * filename ); <nl>  <nl> extern void close_device_fd ( struct thr_info *); <nl>  <nl> +# define for_each_managed_proc ( procvar , dev ) \ <nl> + for ( struct cgpu_info * procvar = dev ; procvar ; procvar = procvar -> next_proc ) <nl> +# define for_each_logical_proc ( procvar , dev ) \ <nl> + for ( struct cgpu_info * procvar = dev ; procvar -> proc_id < dev -> procs ; procvar = procvar -> next_proc ) <nl> + <nl> # endif
static inline void string_elist_del ( struct string_elist * item ) <nl> if ( item -> free_me ) <nl> free ( item -> string ); <nl> list_del (& item -> list ); <nl> + free ( item ); <nl> } <nl>  <nl> 
static void avalon_parse_results ( struct cgpu_info * avalon , struct avalon_info * i <nl> } <nl>  <nl> if (! found ) <nl> - spare = * offset - AVALON_READ_SIZE - 1 ; <nl> + spare = * offset - AVALON_READ_SIZE ; <nl> else <nl> spare = AVALON_READ_SIZE + i ; <nl> applog ( LOG_WARNING , " Avalon : Discarding % d bytes from buffer ", spare );
static void devdetail_an ( struct io_data * io_data , struct cgpu_info * cgpu , bool i <nl> if ( cgpu -> device_path ) <nl> root = api_add_string ( root , " Device Path ", cgpu -> device_path , false ); <nl>  <nl> - if ( cgpu -> drv -> get_api_extra_device_detail ) <nl> + if (( per_proc || cgpu -> procs <= 1 ) && cgpu -> drv -> get_api_extra_device_detail ) <nl> root = api_add_extra ( root , cgpu -> drv -> get_api_extra_device_detail ( cgpu )); <nl>  <nl> root = print_data ( root , buf , isjson , precom );
bool hashbusterusb_vrm_unlock ( struct cgpu_info * const proc , const char * const <nl> hex2bin (& buf [ 1 ], code , size ); <nl>  <nl> hashbusterusb_io ( h , buf , buf ); <nl> - return memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> + return ! memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> } <nl>  <nl> static
void bfg_waddstr ( WINDOW * win , const char * s ) <nl>  <nl> while ( true ) <nl> { <nl> - while ( likely ( p [ 0 ] >= 0x20 && p [ 0 ] < stop_ascii )) <nl> + while ( likely ( p [ 0 ] == '\ n ' || ( p [ 0 ] >= 0x20 && p [ 0 ] < stop_ascii ))) <nl> { <nl> // Printable ASCII <nl> ++ p ;
bool initiate_stratum ( struct pool * pool ) <nl> goto out ; <nl> } <nl>  <nl> + free ( pool -> nonce1 ); <nl> pool -> nonce1 = strdup ( json_string_value ( json_array_get ( res_val , 1 ))); <nl> if (! pool -> nonce1 ) { <nl> applog ( LOG_WARNING , " Failed to get nonce1 in initiate_stratum ");
static void * submit_work_thread ( __maybe_unused void * userdata ) <nl> } <nl> ++ wip ; <nl> } <nl> - else <nl> + else { <nl> -- total_submitting ; <nl> + free_work ( work ); <nl> + } <nl> } <nl> if ( unlikely ( shutting_down && ! wip )) <nl> break ;
AcpiGetHandle ( <nl> { <nl> ACPI_STATUS Status ; <nl> NAME_TABLE_ENTRY * ThisEntry ; <nl> - ACPI_HANDLE Scope = NULL ; <nl> + NAME_TABLE_ENTRY * Scope = NULL ; <nl>  <nl> if (! RetHandle || ! Pathname ) <nl> { <nl> AcpiGetHandle ( <nl> } <nl>  <nl> if ( Parent ) <nl> - Scope = Parent -> Scope ; <nl> + Scope = (( NAME_TABLE_ENTRY *) Parent )-> Scope ; <nl>  <nl> /* Special case for root , since we can ' t search for it */ <nl> 
ACPI_STATUS <nl> OsGetGlobalLock ( void ) <nl> { <nl> UINT32 GlobalLockReg ; <nl> - ACPI_STATUS Status ; <nl> + ACPI_STATUS Status = AE_OK ; <nl>  <nl>  <nl> if ( FACS )
/****************************************************************************** <nl> * <nl> * Module Name : aeexec - Support routines for AcpiExec utility <nl> - * $ Revision : 1 . 107 $ <nl> + * $ Revision : 1 . 108 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AeMiscellaneousTests ( <nl> AcpiOsPrintf (" Could not get GlobalLock , % X \ n ", Status ); <nl> } <nl>  <nl> + Status = AcpiAcquireGlobalLock ( 0x5 , & LockHandle ); /* Should fail */ <nl> + <nl> Status = AcpiReleaseGlobalLock ( LockHandle ); <nl> if ( ACPI_FAILURE ( Status )) <nl> {
AcpiNsEvaluate ( <nl>  <nl> Status = AE_OK ; <nl> } <nl> + else if ( ACPI_FAILURE ( Status )) <nl> + { <nl> + /* If ReturnObject exists , delete it */ <nl> + <nl> + if ( Info -> ReturnObject ) <nl> + { <nl> + AcpiUtRemoveReference ( Info -> ReturnObject ); <nl> + Info -> ReturnObject = NULL ; <nl> + } <nl> + } <nl>  <nl> ACPI_DEBUG_PRINT (( ACPI_DB_NAMES , <nl> "*** Completed evaluation of object % s ***\ n ",
AcpiHwClearAcpiStatus ( <nl>  <nl> Status = AcpiHwRegisterWrite ( ACPI_REGISTER_PM1_STATUS , <nl> ACPI_BITMASK_ALL_FIXED_STATUS ); <nl> + <nl> + AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + <nl> if ( ACPI_FAILURE ( Status )) <nl> { <nl> - goto UnlockAndExit ; <nl> + goto Exit ; <nl> } <nl>  <nl> /* Clear the GPE Bits in all GPE registers in all GPE blocks */ <nl>  <nl> Status = AcpiEvWalkGpeList ( AcpiHwClearGpeBlock , NULL ); <nl>  <nl> - UnlockAndExit : <nl> - AcpiOsReleaseLock ( AcpiGbl_HardwareLock , LockFlags ); <nl> + Exit : <nl> return_ACPI_STATUS ( Status ); <nl> } <nl> 
AmlExecCreateField ( <nl> FieldDesc -> FieldUnit . UpdateRule = ( UINT8 ) UPDATE_Preserve ; <nl> FieldDesc -> FieldUnit . Length = BitCount ; <nl> FieldDesc -> FieldUnit . BitOffset = ( UINT8 ) ( BitOffset % 8 ); <nl> - FieldDesc -> FieldUnit . Offset = BitOffset / 8 ; <nl> + FieldDesc -> FieldUnit . Offset = DIV_8 ( BitOffset ); <nl> FieldDesc -> FieldUnit . Container = SrcDesc ; <nl> FieldDesc -> FieldUnit . Sequence = SrcDesc -> Buffer . Sequence ; <nl> 
/****************************************************************************** <nl> * <nl> * Name : acwin . h - OS specific defines , etc . <nl> - * $ Revision : 1 . 27 $ <nl> + * $ Revision : 1 . 28 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> typedef COMPILER_DEPENDENT_UINT64 u64 ; <nl> # define ACPI_SIMPLE_RETURN_MACROS <nl> # endif <nl>  <nl> +/*! [ End ] no source code translation !*/ <nl> + <nl> /* <nl> * Global Lock acquire / release code <nl> *
ApIsValidHeader ( <nl>  <nl> /* Check for minimum table length */ <nl>  <nl> - if ( Table -> Length <= sizeof ( ACPI_TABLE_HEADER )) <nl> + if ( Table -> Length < sizeof ( ACPI_TABLE_HEADER )) <nl> { <nl> fprintf ( stderr , " Table length ( 0x % 8 . 8X ) is invalid \ n ", <nl> Table -> Length );
/****************************************************************************** <nl> * <nl> * Name : acglobal . h - Declarations for global variables <nl> - * $ Revision : 1 . 153 $ <nl> + * $ Revision : 1 . 154 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> # define ACPI_INIT_GLOBAL ( a , b ) a <nl> # endif <nl>  <nl> -/* <nl> +/* <nl> * Keep local copies of these FADT - based registers . NOTE : These globals <nl> * are first in this file for alignment reasons on 64 - bit systems . <nl> */
/****************************************************************************** <nl> * <nl> * Module Name : exutils - interpreter / scanner utilities <nl> - * $ Revision : 1 . 116 $ <nl> + * $ Revision : 1 . 117 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiExEisaIdToString ( <nl> * <nl> * RETURN : None , string <nl> * <nl> - * DESCRIPTOIN : Convert a number to string representation . Assumes string <nl> + * DESCRIPTION : Convert a number to string representation . Assumes string <nl> * buffer is large enough to hold the string . <nl> * <nl> ******************************************************************************/
/****************************************************************************** <nl> * <nl> * Module Name : evregion - ACPI AddressSpace / OpRegion handler dispatch <nl> - * $ Revision : 1 . 79 $ <nl> + * $ Revision : 1 . 80 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiEvInstallDefaultAddressSpaceHandlers ( <nl>  <nl> FUNCTION_TRACE (" EvInstallDefaultAddressSpaceHandlers "); <nl>  <nl> - /* <nl> + /* <nl> * All address spaces ( PCI Config , EC , SMBus ) are scope dependent <nl> * and registration must occur for a specific device . In the case <nl> * system memory and IO address spaces there is currently no device
AmlDumpOperand ( <nl> break ; <nl>  <nl>  <nl> + case AML_NAMEPATH_OP : <nl> + DEBUG_PRINT_RAW ( ACPI_INFO , (" Lvalue . Nte -> Name % x \ n ", <nl> + EntryDesc -> Lvalue . Nte -> Name )); <nl> + break ; <nl> + <nl> default : <nl>  <nl> /* unknown opcode */
INT32 <nl> AcpiModeCapabilities ( <nl> void ); <nl>  <nl> - <nl> /* <nl> * Event / System interfaces <nl> */
findso : <nl> so -> so_ti = ti ; <nl> tp -> t_timer [ TCPT_KEEP ] = TCPTV_KEEP_INIT ; <nl> tp -> t_state = TCPS_SYN_RECEIVED ; <nl> + tcp_template ( tp ); <nl> } <nl> return ; <nl> 
static int scsi_disk_initfn ( SCSIDevice * dev ) <nl> } <nl> s -> bs = s -> qdev . conf . dinfo -> bdrv ; <nl>  <nl> + if ( bdrv_is_sg ( s -> bs )) { <nl> + qemu_error (" scsi - disk : unwanted / dev / sg *\ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( bdrv_get_type_hint ( s -> bs ) == BDRV_TYPE_CDROM ) { <nl> s -> cluster_size = 4 ; <nl> } else {
void ppce500_init ( PPCE500Params * params ) <nl>  <nl> /* Fixup Memory size on a alignment boundary */ <nl> ram_size &= ~( RAM_SIZES_ALIGN - 1 ); <nl> + params -> ram_size = ram_size ; <nl>  <nl> /* Register Memory */ <nl> memory_region_init_ram ( ram , " mpc8544ds . ram ", ram_size );
void tlb_fill ( CPUCRISState * env , target_ulong addr , int is_write , int mmu_idx , <nl> int ret ; <nl>  <nl> D_LOG ("% s pc =% x tpc =% x ra =% p \ n ", __func__ , <nl> - env -> pc , env -> debug1 , ( void *) retaddr ); <nl> + env -> pc , env -> pregs [ PR_EDA ], ( void *) retaddr ); <nl> ret = cpu_cris_handle_mmu_fault ( env , addr , is_write , mmu_idx ); <nl> if ( unlikely ( ret )) { <nl> if ( retaddr ) {
void usb_packet_complete ( USBDevice * dev , USBPacket * p ) <nl>  <nl> while (! QTAILQ_EMPTY (& ep -> queue )) { <nl> p = QTAILQ_FIRST (& ep -> queue ); <nl> + if ( p -> state == USB_PACKET_ASYNC ) { <nl> + break ; <nl> + } <nl> assert ( p -> state == USB_PACKET_QUEUED ); <nl> ret = usb_process_one ( p ); <nl> if ( ret == USB_RET_ASYNC ) {
uint32_t HELPER ( neon_rshl_u32 )( uint32_t val , uint32_t shiftop ) <nl> uint64_t HELPER ( neon_rshl_u64 )( uint64_t val , uint64_t shiftop ) <nl> { <nl> int8_t shift = ( uint8_t ) shiftop ; <nl> - if ( shift >= 64 || shift < 64 ) { <nl> + if ( shift >= 64 || shift < - 64 ) { <nl> val = 0 ; <nl> } else if ( shift == - 64 ) { <nl> /* Rounding a 1 - bit result just preserves that bit . */
typedef struct { <nl>  <nl> static inline int num_effective_busses ( XilinxSPIPS * s ) <nl> { <nl> - return ( s -> regs [ R_LQSPI_STS ] & LQSPI_CFG_SEP_BUS && <nl> - s -> regs [ R_LQSPI_STS ] & LQSPI_CFG_TWO_MEM ) ? s -> num_busses : 1 ; <nl> + return ( s -> regs [ R_LQSPI_CFG ] & LQSPI_CFG_SEP_BUS && <nl> + s -> regs [ R_LQSPI_CFG ] & LQSPI_CFG_TWO_MEM ) ? s -> num_busses : 1 ; <nl> } <nl>  <nl> static void xilinx_spips_update_cs_lines ( XilinxSPIPS * s )
static int mode_sense_page ( SCSIDiskState * s , int page , uint8_t ** p_outbuf , <nl>  <nl> case MODE_PAGE_R_W_ERROR : <nl> length = 10 ; <nl> + if ( page_control == 1 ) { /* Changeable Values */ <nl> + break ; <nl> + } <nl> p [ 0 ] = 0x80 ; /* Automatic Write Reallocation Enabled */ <nl> if ( s -> qdev . type == TYPE_ROM ) { <nl> p [ 1 ] = 0x20 ; /* Read Retry Count */
static int raw_open ( BlockDriverState * bs , const char * filename , int flags ) <nl> } <nl> # endif <nl> # ifdef CONFIG_COCOA <nl> - u_int32_t blockSize = 512 ; <nl> + uint32_t blockSize = 512 ; <nl> if ( ! ioctl ( fd , DKIOCGETBLOCKSIZE , & blockSize ) && blockSize > bufsize ) { <nl> bufsize = blockSize ; <nl> }
BlockDriverAIOCB * paio_ioctl ( BlockDriverState * bs , int fd , <nl> acb -> aio_type = QEMU_AIO_IOCTL ; <nl> acb -> aio_fildes = fd ; <nl> acb -> ev_signo = SIGUSR2 ; <nl> + acb -> async_context_id = get_async_context_id (); <nl> acb -> aio_offset = 0 ; <nl> acb -> aio_ioctl_buf = buf ; <nl> acb -> aio_ioctl_cmd = req ;
out : <nl> static void usb_host_req_abort ( USBHostRequest * r ) <nl> { <nl> USBHostDevice * s = r -> host ; <nl> - bool inflight = ( r -> p && r -> p -> state == USB_RET_ASYNC ); <nl> + bool inflight = ( r -> p && r -> p -> state == USB_PACKET_ASYNC ); <nl>  <nl> if ( inflight ) { <nl> r -> p -> status = USB_RET_NODEV ;
static int send_sub_rect_jpeg ( VncState * vs , int x , int y , int w , int h , <nl> } else { <nl> ret = send_palette_rect ( vs , x , y , w , h , palette ); <nl> } <nl> + } else { <nl> + ret = 0 ; <nl> } <nl> return ret ; <nl> }
bool aio_wait ( AioContext * ctx ) <nl> * Otherwise , if there are no AIO requests , qemu_aio_wait () would <nl> * wait indefinitely . <nl> */ <nl> - if ( node -> io_flush ) { <nl> + if (! node -> deleted && node -> io_flush ) { <nl> if ( node -> io_flush ( node -> opaque ) == 0 ) { <nl> continue ; <nl> }
static void smc91c111_writeb ( void * opaque , hwaddr offset , <nl> return ; <nl> case 12 : /* Early receive . */ <nl> s -> ercv = value & 0x1f ; <nl> + return ; <nl> case 13 : <nl> /* Ignore . */ <nl> return ;
tight_detect_smooth_image ( VncState * vs , int w , int h ) <nl> } else { <nl> errors = tight_detect_smooth_image16 ( vs , w , h ); <nl> } <nl> - if ( quality != - 1 ) { <nl> + if ( quality != ( uint8_t )- 1 ) { <nl> return ( errors < tight_conf [ quality ]. jpeg_threshold ); <nl> } <nl> return ( errors < tight_conf [ compression ]. gradient_threshold );
DMAContext * spapr_tce_new_dma_context ( uint32_t liobn , size_t window_size ) <nl> { <nl> sPAPRTCETable * tcet ; <nl>  <nl> + if ( spapr_tce_find_by_liobn ( liobn )) { <nl> + fprintf ( stderr , " Attempted to create TCE table with duplicate " <nl> + " LIOBN 0x % x \ n ", liobn ); <nl> + return NULL ; <nl> + } <nl> + <nl> if (! window_size ) { <nl> return NULL ; <nl> }
static int alloc_refcount_block ( BlockDriverState * bs , <nl> uint64_t last_table_size ; <nl> uint64_t blocks_clusters ; <nl> do { <nl> - uint64_t table_clusters = size_to_clusters ( s , table_size ); <nl> + uint64_t table_clusters = <nl> + size_to_clusters ( s , table_size * sizeof ( uint64_t )); <nl> blocks_clusters = 1 + <nl> (( table_clusters + refcount_block_clusters - 1 ) <nl> / refcount_block_clusters );
int paio_init ( void ) <nl> s -> first_aio = NULL ; <nl> if ( qemu_pipe ( fds ) == - 1 ) { <nl> fprintf ( stderr , " failed to create pipe \ n "); <nl> + g_free ( s ); <nl> return - 1 ; <nl> } <nl> 
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> + qemu_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
petalogix_ml605_init ( QEMUMachineInitArgs * args ) <nl> dma = qdev_create ( NULL , " xlnx . axi - dma "); <nl>  <nl> /* FIXME : attach to the sysbus instead */ <nl> + object_property_add_child ( qdev_get_machine (), " xilinx - eth ", OBJECT ( eth0 ), <nl> + NULL ); <nl> object_property_add_child ( qdev_get_machine (), " xilinx - dma ", OBJECT ( dma ), <nl> NULL ); <nl> 
static void hda_audio_set_amp ( HDAAudioStream * st ) <nl> left = left * 255 / QEMU_HDA_AMP_STEPS ; <nl> right = right * 255 / QEMU_HDA_AMP_STEPS ; <nl>  <nl> + if (! st -> state -> mixer ) { <nl> + return ; <nl> + } <nl> if ( st -> output ) { <nl> AUD_set_volume_out ( st -> voice . out , muted , left , right ); <nl> } else {
static void vring_init ( struct vring * vr , unsigned int num , void * p , <nl> vr -> used -> flags = VRING_USED_F_NO_NOTIFY ; <nl> vr -> used -> idx = 0 ; <nl> vr -> used_idx = 0 ; <nl> + vr -> next_idx = 0 ; <nl>  <nl> debug_print_addr (" init vr ", vr ); <nl> }
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static int64_t coroutine_fn vmdk_co_get_block_status ( BlockDriverState * bs , <nl> break ; <nl> case VMDK_OK : <nl> ret = BDRV_BLOCK_DATA ; <nl> - if ( extent -> file == bs -> file ) { <nl> + if ( extent -> file == bs -> file && ! extent -> compressed ) { <nl> ret |= BDRV_BLOCK_OFFSET_VALID | offset ; <nl> } <nl> 
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> { <nl> DPRINTF ("+++ C + mode offloaded task checksum \ n "); <nl>  <nl> + /* Large enough for Ethernet and IP headers ? */ <nl> + if ( saved_size < ETH_HLEN + sizeof ( ip_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ip packet header */ <nl> ip_header * ip = NULL ; <nl> int hlen = 0 ;
const VMStateDescription vmstate_ide_atapi_gesn_state = { <nl> . fields = ( VMStateField []) { <nl> VMSTATE_BOOL ( events . new_media , IDEState ), <nl> VMSTATE_BOOL ( events . eject_request , IDEState ), <nl> + VMSTATE_END_OF_LIST () <nl> } <nl> }; <nl> 
void virtio_cleanup ( VirtIODevice * vdev ) <nl> if ( vdev -> config ) <nl> qemu_free ( vdev -> config ); <nl> qemu_free ( vdev -> vq ); <nl> + qemu_free ( vdev ); <nl> } <nl>  <nl> static void virtio_vmstate_change ( void * opaque , int running , int reason )
void qmp_migrate_set_cache_size ( int64_t value , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> + /* Cache should not be larger than guest ram size */ <nl> + if ( value > ram_bytes_total ()) { <nl> + error_set ( errp , QERR_INVALID_PARAMETER_VALUE , " cache size ", <nl> + " exceeds guest ram size "); <nl> + return ; <nl> + } <nl> + <nl> new_size = xbzrle_cache_resize ( value ); <nl> if ( new_size < 0 ) { <nl> error_set ( errp , QERR_INVALID_PARAMETER_VALUE , " cache size ",
void cpu_loop ( CPUMIPSState * env ) <nl> syscall_num = env -> active_tc . gpr [ 2 ] - 4000 ; <nl> env -> active_tc . PC += 4 ; <nl> if ( syscall_num >= sizeof ( mips_syscall_args )) { <nl> - ret = - ENOSYS ; <nl> + ret = - TARGET_ENOSYS ; <nl> } else { <nl> int nb_args ; <nl> abi_ulong sp_reg ;
static void gen_compute_branch ( DisasContext * ctx , uint32_t opc , int r1 , <nl> break ; <nl> case OPCM_32_BRR_LOOP : <nl> if ( MASK_OP_BRR_OP2 ( ctx -> opcode ) == OPC2_32_BRR_LOOP ) { <nl> - gen_loop ( ctx , r1 , offset * 2 ); <nl> + gen_loop ( ctx , r2 , offset * 2 ); <nl> } else { <nl> /* OPC2_32_BRR_LOOPU */ <nl> gen_goto_tb ( ctx , 0 , ctx -> pc + offset * 2 );
static int sd_create ( const char * filename , QemuOpts * opts , <nl> bdrv_unref ( bs ); <nl> } <nl>  <nl> + s -> aio_context = qemu_get_aio_context (); <nl> ret = do_sd_create ( s , & vid , 0 , errp ); <nl> if ( ret ) { <nl> goto out ;
static void cirrus_init_common ( CirrusVGAState * s , int device_id , int is_pci ) <nl> s -> vga . cursor_draw_line = cirrus_cursor_draw_line ; <nl>  <nl> qemu_register_reset ( cirrus_reset , s ); <nl> - cirrus_reset ( s ); <nl> } <nl>  <nl> /***************************************
static TCGArg * tcg_constant_folding ( TCGContext * s , uint16_t * tcg_opc_ptr , <nl> /* Simplify LT / GE comparisons vs zero to a single compare <nl> vs the high word of the input . */ <nl> s -> gen_opc_buf [ op_index ] = INDEX_op_setcond_i32 ; <nl> + reset_temp ( args [ 0 ]); <nl> gen_args [ 0 ] = args [ 0 ]; <nl> gen_args [ 1 ] = args [ 2 ]; <nl> gen_args [ 2 ] = args [ 4 ];
static bool cpu_thread_is_idle ( CPUArchState * env ) <nl> if ( env -> stopped || ! runstate_is_running ()) { <nl> return true ; <nl> } <nl> - if (! env -> halted || qemu_cpu_has_work ( env ) || <nl> - ( kvm_enabled () && kvm_irqchip_in_kernel ())) { <nl> + if (! env -> halted || qemu_cpu_has_work ( env ) || kvm_irqchip_in_kernel ()) { <nl> return false ; <nl> } <nl> return true ;
static int refresh_total_sectors ( BlockDriverState * bs , int64_t hint ) <nl> { <nl> BlockDriver * drv = bs -> drv ; <nl>  <nl> + /* Do not attempt drv -> bdrv_getlength () on scsi - generic devices */ <nl> + if ( bs -> sg ) <nl> + return 0 ; <nl> + <nl> /* query actual device if possible , otherwise just trust the hint */ <nl> if ( drv -> bdrv_getlength ) { <nl> int64_t length = drv -> bdrv_getlength ( bs );
static uint32_t parse_enumeration ( char * str , <nl> { <nl> uint32_t ret = not_found_value ; <nl>  <nl> + if ( str == NULL ) <nl> + return 0 ; <nl> + <nl> while ( table -> name != NULL ) { <nl> if ( strcmp ( table -> name , str ) == 0 ) { <nl> ret = table -> value ;
fail : <nl> qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) table ); <nl> } <nl> s -> l1_table [ l1_index ] = old_l2_offset ; <nl> + if ( l2_offset > 0 ) { <nl> + qcow2_free_clusters ( bs , l2_offset , s -> l2_size * sizeof ( uint64_t ), <nl> + QCOW2_DISCARD_ALWAYS ); <nl> + } <nl> return ret ; <nl> } <nl> 
void qemu_iovec_concat_iov ( QEMUIOVector * dst , <nl> { <nl> int i ; <nl> size_t done ; <nl> + <nl> + if (! sbytes ) { <nl> + return ; <nl> + } <nl> assert ( dst -> nalloc != - 1 ); <nl> for ( i = 0 , done = 0 ; done < sbytes && i < src_cnt ; i ++) { <nl> if ( soffset < src_iov [ i ]. iov_len ) {
void vnc_display_open ( const char * id , Error ** errp ) <nl> /* listen for connects */ <nl> if ( strncmp ( vnc , " unix :", 5 ) == 0 ) { <nl> vs -> lsock = unix_listen ( vnc + 5 , NULL , 0 , errp ); <nl> + if ( vs -> lsock < 0 ) { <nl> + goto fail ; <nl> + } <nl> vs -> is_unix = true ; <nl> } else { <nl> vs -> lsock = inet_listen_opts ( sopts , 5900 , errp );
static const char * const tcg_target_reg_names [ TCG_TARGET_NB_REGS ] = { <nl> # endif <nl>  <nl> static const int tcg_target_reg_alloc_order [] = { <nl> - TCG_REG_EAX , <nl> - TCG_REG_EDX , <nl> - TCG_REG_ECX , <nl> TCG_REG_EBX , <nl> TCG_REG_ESI , <nl> TCG_REG_EDI , <nl> TCG_REG_EBP , <nl> + TCG_REG_ECX , <nl> + TCG_REG_EDX , <nl> + TCG_REG_EAX , <nl> }; <nl>  <nl> static const int tcg_target_call_iarg_regs [ 3 ] = { TCG_REG_EAX , TCG_REG_EDX , TCG_REG_ECX };
static void coroutine_fn backup_run ( void * opaque ) <nl>  <nl> if ( job -> sync_bitmap ) { <nl> BdrvDirtyBitmap * bm ; <nl> - if ( ret < 0 ) { <nl> + if ( ret < 0 || block_job_is_cancelled (& job -> common )) { <nl> /* Merge the successor back into the parent , delete nothing . */ <nl> bm = bdrv_reclaim_dirty_bitmap ( bs , job -> sync_bitmap , NULL ); <nl> assert ( bm );
 <nl> typedef struct SuperIOConfig <nl> { <nl> - uint8_t config [ 0xff ]; <nl> + uint8_t config [ 0x100 ]; <nl> uint8_t index ; <nl> uint8_t data ; <nl> } SuperIOConfig ;
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> - if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + if ( dev -> setup_index < 0 || <nl> + dev -> setup_len < 0 || <nl> + dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> }
bool bdrv_dev_is_medium_locked ( BlockDriverState * bs ) <nl> */ <nl> int bdrv_check ( BlockDriverState * bs , BdrvCheckResult * res , BdrvCheckMode fix ) <nl> { <nl> + if ( bs -> drv == NULL ) { <nl> + return - ENOMEDIUM ; <nl> + } <nl> if ( bs -> drv -> bdrv_check == NULL ) { <nl> return - ENOTSUP ; <nl> }
int virtio_load ( VirtIODevice * vdev , QEMUFile * f ) <nl> qemu_get_8s ( f , & vdev -> status ); <nl> qemu_get_8s ( f , & vdev -> isr ); <nl> qemu_get_be16s ( f , & vdev -> queue_sel ); <nl> + if ( vdev -> queue_sel >= VIRTIO_PCI_QUEUE_MAX ) { <nl> + return - 1 ; <nl> + } <nl> qemu_get_be32s ( f , & features ); <nl>  <nl> if ( virtio_set_features ( vdev , features ) < 0 ) {
int net_init_vhost_user ( const NetClientOptions * opts , const char * name , <nl> } <nl>  <nl> queues = vhost_user_opts -> has_queues ? vhost_user_opts -> queues : 1 ; <nl> + if ( queues < 1 ) { <nl> + error_setg ( errp , <nl> + " vhost - user number of queues must be bigger than zero "); <nl> + return - 1 ; <nl> + } <nl>  <nl> return net_vhost_user_init ( peer , " vhost_user ", name , chr , queues ); <nl> }
static inline void gen_neon_mull ( TCGv_i64 dest , TCGv a , TCGv b , int size , int u ) <nl> case 4 : <nl> tmp = gen_muls_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> case 5 : <nl> tmp = gen_mulu_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> default : abort (); <nl> }
static void * qpa_thread_out ( void * arg ) <nl> return NULL ; <nl> } <nl>  <nl> + pa -> live = 0 ; <nl> pa -> rpos = rpos ; <nl> - pa -> live -= decr ; <nl> pa -> decr += decr ; <nl> } <nl> 
static void disas_s390_insn ( DisasContext * s ) <nl> store_reg32 ( r1 , tmp32_1 ); <nl> tcg_gen_trunc_i64_i32 ( tmp32_2 , tmp2 ); <nl> store_reg32 ( r1 + 1 , tmp32_2 ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> + tcg_temp_free_i64 ( tmp2 ); <nl> break ; <nl> case 0x98 : /* LM R1 , R3 , D2 ( B2 ) [ RS ] */ <nl> case 0x90 : /* STM R1 , R3 , D2 ( B2 ) [ RS ] */
static uint32_t pic_poll_read ( PicState * s ) <nl> pic_update_irq ( s -> pics_state ); <nl> } else { <nl> ret = 0x07 ; <nl> - pic_update_irq ( s -> pics_state ); <nl> } <nl>  <nl> return ret ;
void qcow2_free_any_clusters ( BlockDriverState * bs , uint64_t l2_entry , <nl> } <nl> break ; <nl> case QCOW2_CLUSTER_NORMAL : <nl> - qcow2_free_clusters ( bs , l2_entry & L2E_OFFSET_MASK , <nl> - nb_clusters << s -> cluster_bits , type ); <nl> + case QCOW2_CLUSTER_ZERO : <nl> + if ( l2_entry & L2E_OFFSET_MASK ) { <nl> + qcow2_free_clusters ( bs , l2_entry & L2E_OFFSET_MASK , <nl> + nb_clusters << s -> cluster_bits , type ); <nl> + } <nl> break ; <nl> case QCOW2_CLUSTER_UNALLOCATED : <nl> - case QCOW2_CLUSTER_ZERO : <nl> break ; <nl> default : <nl> abort ();
ram_addr_t qemu_ram_alloc_from_ptr ( ram_addr_t size , void * host , <nl>  <nl> qemu_ram_setup_dump ( new_block -> host , size ); <nl> qemu_madvise ( new_block -> host , size , QEMU_MADV_HUGEPAGE ); <nl> + qemu_madvise ( new_block -> host , size , QEMU_MADV_DONTFORK ); <nl>  <nl> if ( kvm_enabled ()) <nl> kvm_setup_guest_memory ( new_block -> host , size );
static void xhci_doorbell_write ( void * ptr , hwaddr reg , <nl> } <nl> } <nl>  <nl> + static void xhci_cap_write ( void * opaque , hwaddr addr , uint64_t val , <nl> + unsigned width ) <nl> +{ <nl> + /* nothing */ <nl> +} <nl> + <nl> static const MemoryRegionOps xhci_cap_ops = { <nl> . read = xhci_cap_read , <nl> + . write = xhci_cap_write , <nl> . valid . min_access_size = 1 , <nl> . valid . max_access_size = 4 , <nl> . impl . min_access_size = 4 ,
# ifndef FW_CFG_H <nl> # define FW_CFG_H <nl>  <nl> +# ifndef NO_QEMU_PROTOS <nl> +# include < stdint . h > <nl> +# include < stddef . h > <nl> + <nl> +# include " exec / hwaddr . h " <nl> +# endif <nl> + <nl> # define FW_CFG_SIGNATURE 0x00 <nl> # define FW_CFG_ID 0x01 <nl> # define FW_CFG_UUID 0x02
static void vfio_msi_interrupt ( void * opaque ) <nl> MSIMessage msg ; <nl>  <nl> if ( vdev -> interrupt == VFIO_INT_MSIX ) { <nl> - msg = msi_get_message (& vdev -> pdev , nr ); <nl> - } else if ( vdev -> interrupt == VFIO_INT_MSI ) { <nl> msg = msix_get_message (& vdev -> pdev , nr ); <nl> + } else if ( vdev -> interrupt == VFIO_INT_MSI ) { <nl> + msg = msi_get_message (& vdev -> pdev , nr ); <nl> } else { <nl> abort (); <nl> }
static int iscsi_open ( BlockDriverState * bs , QDict * options , int flags , <nl> QemuOpts * opts ; <nl> Error * local_err = NULL ; <nl> const char * filename ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> if (( BDRV_SECTOR_SIZE % 512 ) != 0 ) { <nl> error_setg ( errp , " iSCSI : Invalid BDRV_SECTOR_SIZE . "
static int ioreq_map ( struct ioreq * ioreq ) <nl> xen_be_printf (& ioreq -> blkdev -> xendev , 0 , <nl> " can ' t map grant ref % d (% s , % d maps )\ n ", <nl> refs [ i ], strerror ( errno ), ioreq -> blkdev -> cnt_map ); <nl> + ioreq -> mapped = 1 ; <nl> ioreq_unmap ( ioreq ); <nl> return - 1 ; <nl> }
static int decode_micromips_opc ( CPUMIPSState * env , DisasContext * ctx , int * is_b <nl> case LB32 : <nl> case LH32 : <nl> case DADDIU32 : <nl> - case POOL48A : /* ??? */ <nl> case LWC132 : <nl> case LDC132 : <nl> case LD32 :
struct m_hdr { <nl> struct mbuf { <nl> struct m_hdr m_hdr ; <nl> Slirp * slirp ; <nl> + bool arp_requested ; <nl> + uint64_t expiration_date ; <nl> + /* start of dynamic buffer area , must be last element */ <nl> union M_dat { <nl> char m_dat_ [ 1 ]; /* ANSI don ' t like 0 sized arrays */ <nl> char * m_ext_ ; <nl> } M_dat ; <nl> - bool arp_requested ; <nl> - uint64_t expiration_date ; <nl> }; <nl>  <nl> # define m_next m_hdr . mh_next
static sd_rsp_type_t sd_normal_command ( SDState * sd , <nl> } <nl> break ; <nl>  <nl> + case 5 : /* CMD5 : reserved for SDIO cards */ <nl> + sd -> card_status |= ILLEGAL_COMMAND ; <nl> + return sd_r0 ; <nl> + <nl> case 6 : /* CMD6 : SWITCH_FUNCTION */ <nl> if ( sd -> spi ) <nl> goto bad_cmd ;
static void ehci_async_complete_packet ( USBPort * port , USBPacket * packet ) <nl> assert ( p -> async == EHCI_ASYNC_INFLIGHT ); <nl> p -> async = EHCI_ASYNC_FINISHED ; <nl> p -> usb_status = packet -> result ; <nl> + <nl> + if ( p -> queue -> async ) { <nl> + qemu_bh_schedule ( p -> queue -> ehci -> async_bh ); <nl> + } <nl> } <nl>  <nl> static void ehci_execute_complete ( EHCIQueue * q )
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> if (( txdw0 & CP_TX_LGSEN ) && ip_protocol == IP_PROTO_TCP ) <nl> { <nl> + /* Large enough for the TCP header ? */ <nl> + if ( ip_data_len < sizeof ( tcp_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> int large_send_mss = ( txdw0 >> 16 ) & CP_TC_LGSEN_MSS_MASK ; <nl>  <nl> DPRINTF ("+++ C + mode offloaded task TSO MTU =% d IP data % d "
uint32_t gic_acknowledge_irq ( GICState * s , int cpu ) <nl> } <nl> s -> last_active [ irq ][ cpu ] = s -> running_irq [ cpu ]; <nl>  <nl> - if ( s -> revision == REV_11MPCORE ) { <nl> + if ( s -> revision == REV_11MPCORE || s -> revision == REV_NVIC ) { <nl> /* Clear pending flags for both level and edge triggered interrupts . <nl> * Level triggered IRQs will be reasserted once they become inactive . <nl> */
int msix_init ( struct PCIDevice * dev , unsigned short nentries , <nl> return 0 ; <nl>  <nl> err_config : <nl> + dev -> msix_entries_nr = 0 ; <nl> cpu_unregister_io_memory ( dev -> msix_mmio_index ); <nl> err_index : <nl> qemu_free ( dev -> msix_table_page );
enum { <nl>  <nl> static inline void flush_icache_range ( unsigned long start , unsigned long stop ) <nl> { <nl> -# if QEMU_GNUC_PREREQ ( 4 , 1 ) <nl> - __builtin___clear_cache (( char *) start , ( char *) stop ); <nl> -# else <nl> -# error not implemented <nl> -# endif <nl> }
static int vmdk_open_vmdk4 ( BlockDriverState * bs , <nl> } <nl> extent -> compressed = <nl> le16_to_cpu ( header . compressAlgorithm ) == VMDK4_COMPRESSION_DEFLATE ; <nl> + if ( extent -> compressed ) { <nl> + g_free ( s -> create_type ); <nl> + s -> create_type = g_strdup (" streamOptimized "); <nl> + } <nl> extent -> has_marker = le32_to_cpu ( header . flags ) & VMDK4_FLAG_MARKER ; <nl> extent -> version = le32_to_cpu ( header . version ); <nl> extent -> has_zero_grain = le32_to_cpu ( header . flags ) & VMDK4_FLAG_ZERO_GRAIN ;
* significant half of a uint64_t struct member . <nl> */ <nl> # ifdef HOST_WORDS_BIGENDIAN <nl> -# define offsetoflow32 ( S , M ) offsetof ( S , M + sizeof ( uint32_t )) <nl> +# define offsetoflow32 ( S , M ) ( offsetof ( S , M ) + sizeof ( uint32_t )) <nl> # else <nl> # define offsetoflow32 ( S , M ) offsetof ( S , M ) <nl> # endif
static int sysbus_device_init ( DeviceState * dev ) <nl> SysBusDevice * sd = SYS_BUS_DEVICE ( dev ); <nl> SysBusDeviceClass * sbc = SYS_BUS_DEVICE_GET_CLASS ( sd ); <nl>  <nl> + if (! sbc -> init ) { <nl> + return 0 ; <nl> + } <nl> return sbc -> init ( sd ); <nl> } <nl> 
static int htab_load ( QEMUFile * f , void * opaque , int version_id ) <nl> if ( section_hdr ) { <nl> /* First section , just the hash shift */ <nl> if ( spapr -> htab_shift != section_hdr ) { <nl> + error_report (" htab_shift mismatch : source % d target % d ", <nl> + section_hdr , spapr -> htab_shift ); <nl> return - EINVAL ; <nl> } <nl> return 0 ;
void * __lzma_wrap_alloc ( void * unused , size_t size ) { <nl> return NULL ; <nl> } <nl>  <nl> - return cli_malloc ( size ); <nl> + return cli_calloc ( 1 , size ); <nl> } <nl> void __lzma_wrap_free ( void * unused , void * freeme ) { <nl> UNUSEDPARAM ( unused );
int cli_url_canon ( const char * inurl , size_t len , char * urlbuff , size_t dest_len , <nl> ++ host_begin ; <nl>  <nl> /* ignore username in URL */ <nl> - p = strchr ( host_begin , '@'); <nl> + while (( host_begin < urlend ) && * host_begin == '/') ++ host_begin ; <nl> + host_len = strcspn ( host_begin , ":/?"); <nl> + p = memchr ( host_begin , '@', host_len ); <nl> if ( p ) <nl> host_begin = p + 1 ; <nl> url = host_begin ;
static struct dconf_module modules [] = { <nl> { " PE ", " MD5SECT ", PE_CONF_MD5SECT , 1 }, <nl> { " PE ", " UPX ", PE_CONF_UPX , 1 }, <nl> { " PE ", " FSG ", PE_CONF_FSG , 1 }, <nl> - { " PE ", " SWIZZOR ", PE_CONF_SWIZZOR , 1 }, <nl> + { " PE ", " SWIZZOR ", PE_CONF_SWIZZOR , 0 }, <nl>  <nl> { " PE ", " PETITE ", PE_CONF_PETITE , 1 }, <nl> { " PE ", " PESPIN ", PE_CONF_PESPIN , 1 },
void cache_add ( unsigned char * md5 , size_t size , cli_ctx * ctx ) { <nl> uint32_t level ; <nl> struct CACHE * c ; <nl>  <nl> - if (! ctx || ! ctx -> engine || ! ctx -> engine -> cache ) <nl> + if (! ctx || ! ctx -> engine || ! ctx -> engine -> cache || ctx -> found_possibly_unwanted ) <nl> return ; <nl>  <nl> level = (* ctx -> fmap && (* ctx -> fmap )-> dont_cache_flag ) ? ctx -> recursion : 0 ;
int qtm_decompress ( struct qtm_stream * qtm , uint32_t out_bytes ) { <nl> if (( frame_start + QTM_FRAME_SIZE ) < frame_end ) { <nl> frame_end = frame_start + QTM_FRAME_SIZE ; <nl> } <nl> + if ( frame_end < window_posn ) { <nl> + cli_dbgmsg (" qtm_decompress : window position beyond end of frame \ n "); <nl> + return qtm -> error = CL_EFORMAT ; <nl> + } <nl>  <nl> while ( window_posn < frame_end ) { <nl> QTM_GET_SYMBOL ( qtm -> model7 , selector );
int cli_fmap_scandesc ( cli_ctx * ctx , cli_file_t ftype , uint8_t ftonly , struct cli <nl> type = ret ; <nl> } <nl>  <nl> - if ( hdb && ! SCAN_ALL ) { <nl> + if ( hdb ) { <nl> const void * data = buff + maxpatlen * ( offset != 0 ); <nl> uint32_t data_len = bytes - maxpatlen * ( offset != 0 ); <nl> 
int cli_scanhwp3 ( cli_ctx * ctx ) <nl>  <nl> offset += HWP3_DOCSUMMARY_SIZE ; <nl>  <nl> + /* password - protected document - cannot parse */ <nl> + if ( docinfo . di_passwd ) { <nl> + cli_dbgmsg (" HWP3 . x : password - protected file , skip parsing \ n "); <nl> + return CL_SUCCESS ; <nl> + } <nl> + <nl> if ( docinfo . di_infoblksize ) { <nl> /* OPTIONAL TODO : HANDLE OPTIONAL INFORMATION BLOCK # 0 ' s FOR PRECLASS */ <nl> offset += docinfo . di_infoblksize ;
bool read_sequence ( u8 const * & src , u8 const * const end , u8 const * & literal , u <nl> literal = src ; <nl> src += literal_len ; <nl>  <nl> - if ( src > end - 2 ) <nl> + if ( src > end - 2 || src < literal ) <nl> return false ; <nl>  <nl> match_dist = * src ++;
GlyphCache :: GlyphCache ( const Face & face , const uint32 face_options ) <nl> } <nl> delete _glyph_loader ; <nl> _glyph_loader = 0 ; <nl> + // coverity [ leaked_storage : FALSE ] - calling read_glyph on index 0 saved <nl> + // glyphs as _glyphs [ 0 ]. Setting _glyph_loader to nullptr here flags that <nl> + // the dtor needs to call delete [] on _glyphs [ 0 ] to release what was allocated <nl> + // as glyphs <nl> } <nl>  <nl> if ( _glyphs && glyph ( 0 ) == 0 )
read_extended_partition ( int fd , struct partition * ep , <nl> if (++ loopct > 100 ) <nl> return n ; <nl>  <nl> - bp = getblock ( fd , here ); <nl> + bp = getblock ( fd , here * ssf ); /* in 512 blocks */ <nl> if ( bp == NULL ) <nl> return n ; <nl> 
int main ( int argc , char ** argv ) <nl> break ; <nl> case ' h ': <nl> usage ( stdout ); <nl> - return 0 ; <nl> + return EXIT_SUCCESS ; <nl> default : <nl> usage ( stderr ); <nl> /* Do not exit ! */ <nl> int main ( int argc , char ** argv ) <nl> /* <nl> * User pressed Control - D . <nl> */ <nl> - return 0 ; <nl> + return EXIT_SUCCESS ; <nl> }
static FILE * dump ( FILE * in , const char * filename , int follow , FILE * out ) <nl> struct utmp ut ; <nl>  <nl> if ( follow ) <nl> - fseek ( in , - 10 * sizeof ( ut ), SEEK_END ); <nl> + ignore_result ( fseek ( in , - 10 * sizeof ( ut ), SEEK_END ) ); <nl>  <nl> while ( fread (& ut , sizeof ( ut ), 1 , in ) == 1 ) <nl> print_utline ( ut , out );
 <nl> # if defined ( __i386__ ) || defined ( __sparc__ ) || defined ( __arm__ ) || \ <nl> defined ( __mips__ ) || defined ( __s390__ ) || defined ( __sh__ ) || \ <nl> - defined ( __aarch64__ ) || \ <nl> + defined ( __aarch64__ ) || defined ( __xtensa__ ) || \ <nl> defined ( __x86_64__ ) || defined ( __avr32__ ) || defined ( __cris__ ) <nl> # define BSD_LABELSECTOR 1 <nl> # define BSD_LABELOFFSET 0
static int probe_minix ( blkid_probe pr , <nl>  <nl> if ( version <= 2 ) { <nl> struct minix_super_block * sb = ( struct minix_super_block *) data ; <nl> - int zones , ninodes , imaps , zmaps , firstz ; <nl> + unsigned long zones , ninodes , imaps , zmaps ; <nl> + off_t firstz ; <nl>  <nl> if ( sb -> s_imap_blocks == 0 || sb -> s_zmap_blocks == 0 || <nl> sb -> s_log_zone_size != 0 )
static int print_udev_ambivalent ( blkid_probe pr ) <nl>  <nl> if ( count > 1 ) { <nl> *( val + valsz - 1 ) = '\ 0 '; /* rem tailing whitespace */ <nl> - printf (" ID_FS_AMBIVALEN =% s \ n ", val ); <nl> + printf (" ID_FS_AMBIVALENT =% s \ n ", val ); <nl> rc = 0 ; <nl> } <nl> done :
looplist_open ( struct looplist * ll , int flag ) <nl> static void <nl> looplist_close ( struct looplist * ll ) <nl> { <nl> - if ( ll -> minors ) <nl> - free ( ll -> minors ); <nl> + free ( ll -> minors ); <nl> if ( ll -> proc ) <nl> fclose ( ll -> proc ); <nl> ll -> minors = NULL ;
int setpwnam ( struct passwd * pwd ) <nl> } <nl>  <nl> /* Set up the limits so that we ' re not foiled */ <nl> - static void pw_init () <nl> + static void pw_init ( void ) <nl> { <nl> struct rlimit rlim ; <nl> 
static char * replace_u ( char * str , char * username ) <nl> } <nl> sz = strlen ( str ); <nl>  <nl> - if ( p == str && sz == 2 ) <nl> + if ( p == str && sz == 2 ) { <nl> /* ' str ' contains only '\ u ' */ <nl> + free ( old ); <nl> return username ; <nl> + } <nl>  <nl> tp = entry = malloc ( sz + usz ); <nl> if (! tp )
int proc_next_tid ( struct proc_tasks * tasks , pid_t * tid ) <nl> struct dirent * d ; <nl> char * end ; <nl>  <nl> + if (! tasks || ! tid ) <nl> + return - 1 ; <nl> + <nl> * tid = 0 ; <nl> errno = 0 ; <nl> 
long old_style_option ( int * argc , char ** argv ) <nl> lines = strtol_or_err ( argv [ i ] + 1 , <nl> _ (" failed to parse number of lines ")); <nl> nargs --; <nl> - memmove ( argv + i , argv + i + 1 , sizeof ( char *) * nargs ); <nl> + if ( nargs - i ) <nl> + memmove ( argv + i , argv + i + 1 , <nl> + sizeof ( char *) * ( nargs - i )); <nl> } else <nl> i ++; <nl> }
static void get_sem_elements ( struct sem_data * p ) <nl> { <nl> size_t i ; <nl>  <nl> - if (! p || ! p -> sem_nsems || p -> sem_perm . id < 0 ) <nl> + if (! p || ! p -> sem_nsems || p -> sem_nsems > SIZE_MAX || p -> sem_perm . id < 0 ) <nl> return ; <nl>  <nl> p -> elements = xcalloc ( p -> sem_nsems , sizeof ( struct sem_elem ));
blkid_partition blkid_partlist_devno_to_partition ( blkid_partlist ls , dev_t devno <nl> if ( blkid_partition_get_start ( par ) == start && <nl> blkid_partition_get_size ( par ) == size ) <nl> return par ; <nl> + <nl> + /* exception for extended dos partitions */ <nl> + if ( blkid_partition_get_start ( par ) == start && <nl> + blkid_partition_is_extended ( par ) && size <= 1024 ) <nl> + return par ; <nl> + <nl> } <nl> return NULL ; <nl> }
wchar_t * buf ; <nl>  <nl> static void sig_handler ( int signo __attribute__ (( __unused__ ))) <nl> { <nl> - free ( buf ); <nl> _exit ( EXIT_SUCCESS ); <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> if (( pwd = getrootpwent ( opt_e )) == NULL ) { <nl> warnx ( _ (" cannot open password database .")); <nl> sleep ( 2 ); <nl> + return EXIT_FAILURE ; <nl> } <nl>  <nl> /*
makemsg ( fname ) <nl> putc ('\ n ', fp ); <nl> cnt = 0 ; <nl> } <nl> - carefulputc ( ch , fp ); <nl> + if ( ch != '\ n ') <nl> + carefulputc ( ch , fp ); <nl> } <nl> } <nl> fprintf ( fp , "% 79s \ r \ n ", " ");
*/ <nl> blkid_loff_t blkid_get_dev_size ( int fd ) <nl> { <nl> - struct stat st ; <nl> unsigned long long bytes ; <nl>  <nl> - if ( fstat ( fd , & st ) == 0 && S_ISREG ( st . st_mode )) <nl> - return st . st_size ; <nl> - <nl> if ( blkdev_get_size ( fd , & bytes )) <nl> return 0 ; <nl> 
void TorrentModel :: removeTorrent ( const QString & hash ) <nl> qDebug () << Q_FUNC_INFO << hash << row ; <nl> if ( row >= 0 ) { <nl> beginRemoveTorrent ( row ); <nl> + delete m_torrents [ row ]; <nl> m_torrents . removeAt ( row ); <nl> endRemoveTorrent (); <nl> }
void MainWindow :: serverDisconnected ( QAbstractSocket :: SocketError err , QString re <nl> if (! Database :: getDigest ( host , port ). isNull ()) { <nl> basereason = tr ("< b > WARNING :</ b > The server presented a certificate that was different from the stored one ."); <nl> } else { <nl> - basereason = tr (" Sever presented a certificate which failed verification ."); <nl> + basereason = tr (" Server presented a certificate which failed verification ."); <nl> } <nl> QStringList qsl ; <nl> foreach ( QSslError e , g . sh -> qlErrors )
void Settings :: save () { <nl> SAVELOAD ( bSuppressMacEventTapWarning , " shortcut / mac / suppresswarning "); <nl> SAVELOAD ( bEnableEvdev , " shortcut / linux / evdev / enable "); <nl> SAVELOAD ( bEnableXInput2 , " shortcut / x11 / xinput2 / enable "); <nl> + SAVELOAD ( bEnableGKey , " shortcut / gkey "); <nl> SAVELOAD ( bEnableXboxInput , " shortcut / windows / xbox / enable "); <nl> SAVELOAD ( bEnableWinHooks , " winhooks "); <nl> SAVELOAD ( bDirectInputVerboseLogging , " shortcut / windows / directinput / verboselogging ");
static int readPackageHeaders ( FD_t fd , /*@ out @*/ struct rpmlead * leadPtr , <nl> int rpmReadPackageInfo ( FD_t fd , Header * sigp , Header * hdrp ) <nl> { <nl> int rc = readPackageHeaders ( fd , NULL , sigp , hdrp ); <nl> + if ( rc ) <nl> + return rc ; <nl> if ( hdrp && * hdrp && sigp && * sigp ) <nl> headerMergeLegacySigs (* hdrp , * sigp ); <nl> return rc ;
static int doSign ( poptContext optCon ) <nl> } <nl>  <nl> exit : <nl> + free ( passPhrase ); <nl> free ( name ); <nl> return rc ; <nl> }
static int rdToken ( ParseState state ) <nl> size_t ts ; <nl>  <nl> p ++; <nl> - for ( ts = 1 ; p [ ts ] && p [ ts ] != '\"'; ts ++); <nl> + for ( ts = 0 ; p [ ts ] && p [ ts ] != '\"'; ts ++); <nl> temp = xmalloc ( ts + 1 ); <nl> memcpy ( temp , p , ts ); <nl> p += ts - 1 ;
int rpmVerifyScript ( char * root , Header h , int err ) { <nl> exit (- 1 ); <nl> } <nl>  <nl> - close ( out ); <nl> - close ( err ); <nl> + if ( out > 2 ) close ( out ); <nl> + if ( err > 2 ) close ( err ); <nl> close ( fd ); <nl> if (! rpmIsVerbose ()) close ( out ); <nl> 
int rpmdbRebuild ( char * rootdir ) { <nl> " to recover ", dbpath , newdbpath ); <nl> return 1 ; <nl> } <nl> - rmdir ( newdbpath ); <nl> + if ( rmdir ( newdbpath )) <nl> + rpmMessage ( RPMERR_RMDIR , " failed to remove % s : % s \ n ", <nl> + newdbpath , strerror ( errno )); <nl> } <nl>  <nl> 
char * headerSprintf ( Header h , const char * origFmt , <nl>  <nl> strcat ( answer , piece ); <nl> answerLength += pieceLength ; <nl> + free ( piece ); <nl> } <nl> } <nl> 
static int makeHDRDigest ( Header sigh , const char * file , rpmTagVal sigTag ) <nl> ( void ) rpmDigestUpdate ( ctx , utd . data , utd . count ); <nl> ( void ) rpmDigestFinal ( ctx , ( void **)& SHA1 , NULL , 1 ); <nl> rpmtdFreeData (& utd ); <nl> + } else { <nl> + rpmlog ( RPMLOG_ERR , _ (" Cannot sign RPM v3 packages \ n ")); <nl> + goto exit ; <nl> } <nl>  <nl> if ( SHA1 == NULL )
envelope_err : <nl> void unwrap_flaglist ( strarray_t * strlist , strarray_t ** flaglist , <nl> variable_list_t * variables ) <nl> { <nl> + if (! strlist ) return ; <nl> + <nl> int len = strarray_size ( strlist ); <nl>  <nl> if ( len ) {
EXPORTED void conversations_rename_cidentry ( struct conversations_state * state , <nl> { <nl> struct rename_rock rrock ; <nl>  <nl> + if ( from == to ) return ; <nl> + <nl> memset (& rrock , 0 , sizeof ( rrock )); <nl> rrock . state = state ; <nl> rrock . from_cid = from ;
int sync_crc_calc ( struct mailbox * mailbox , char * buf , int maxlen ) <nl> if ( mailbox_read_index_record ( mailbox , recno , & record )) <nl> continue ; <nl>  <nl> + /* always skip EXPUNGED flags , so we don ' t count the annots */ <nl> + if ( record . system_flags & FLAG_EXPUNGED ) <nl> + continue ; <nl> + <nl> sync_crc_algorithm -> addrecord ( mailbox , & record , sync_crc_covers ); <nl> if (( sync_crc_covers & SYNC_CRC_ANNOTATIONS ) && user_annot_db ) { <nl> r = read_annotations ( mailbox , & record , & annots );
void config_read_file ( const char * filename ) <nl> infile = fopen ( filename , " r "); <nl>  <nl> if (! infile ) { <nl> - snprintf ( buf , bufsize , " can ' t open configuration file % s : % m ", <nl> - filename ); <nl> + snprintf ( buf , bufsize , " can ' t open configuration file % s : % s ", <nl> + filename , strerror ( errno )); <nl> fatal ( buf , EC_CONFIG ); <nl> } <nl> 
char ** reply ; <nl> r = connect ( s , ( struct sockaddr *)& srvaddr , sizeof ( srvaddr )); <nl> if ( r == - 1 ) { <nl> * reply = " cannot connect to pwcheck server "; <nl> - return 0 ; <nl> + return 1 ; <nl> } <nl>  <nl> iov [ 0 ]. iov_base = user ;
static int jmap_write_calendarevent ( json_t * event , <nl> } <nl> jmap_calendarevent_to_ical ( comp , event , & rock ); <nl> jmap_timezones_to_ical ( ical , & rock ); <nl> + if ( rock . oldcomp ) { <nl> + icalcomponent_free ( rock . oldcomp ); <nl> + } <nl> calevent_rock_free (& rock ); <nl>  <nl> /* Handle any property errors and bail out . */
EXPORTED void jmap_closembox ( jmap_req_t * req , struct mailbox ** mboxp ) <nl> struct _mboxcache_rec * rec = NULL ; <nl> int i ; <nl>  <nl> + if (! mboxp || !* mboxp ) return ; <nl> + <nl> for ( i = 0 ; i < req -> mboxes -> count ; i ++) { <nl> rec = ( struct _mboxcache_rec *) ptrarray_nth ( req -> mboxes , i ); <nl> if ( rec -> mbox == * mboxp )
static int caldav_delete_cal ( struct transaction_t * txn , <nl> sched_reply ( userid , strarray_nth (& schedule_addresses , 0 ), ical , NULL ); <nl> } <nl>  <nl> + free ( userid ); <nl> strarray_fini (& schedule_addresses ); <nl> } <nl> 
static int setCalendarEvents ( struct jmap_req * req ) <nl>  <nl> json_incref ( set ); <nl> json_t * item = json_pack ("[]"); <nl> - json_array_append_new ( item , json_string (" calendarsEventsSet ")); <nl> + json_array_append_new ( item , json_string (" calendarEventsSet ")); <nl> json_array_append_new ( item , set ); <nl> json_array_append_new ( item , json_string ( req -> tag )); <nl> json_array_append_new ( req -> response , item );
static void cmd_search ( char * tag , int usinguid ) <nl> freesearchargs ( searchargs ); <nl> return ; <nl> } <nl> + if ( imapd_id . quirks & QUIRK_SEARCHFUZZY ) { <nl> + char * expr = search_expr_serialise ( searchargs -> root ); <nl> + syslog ( LOG_NOTICE , " fuzzy search % s ", expr ); <nl> + free ( expr ); <nl> + } <nl>  <nl> if ( c == '\ r ') c = prot_getc ( imapd_in ); <nl> if ( c != '\ n ') {
static int mboxlist_do_find ( struct find_rock * rock , const strarray_t * patterns ) <nl> if ( len ) len --; // trailing separator <nl>  <nl> if (! strncmp ( rock -> namespace -> prefix [ NAMESPACE_USER ], commonpat , MIN ( len , prefixlen ))) { <nl> - if ( prefixlen < len ) { <nl> + if ( prefixlen <= len ) { <nl> /* we match all users */ <nl> strlcpy ( domainpat + domainlen , " user .", sizeof ( domainpat )- domainlen ); <nl> }
static json_t * jmap_mailbox_from_mbox ( struct mailbox * mbox , <nl> } <nl> } <nl> json_object_set_new ( obj , " sortOrder ", json_integer ( sortOrder )); <nl> + buf_free (& attrib ); <nl> } <nl> if ( _wantprop ( props , " parentId ")) { <nl> json_object_set_new ( obj , " parentId ", parent && parent != inbox ?
static int expunge_userflags ( struct mailbox * mailbox , struct expire_rock * erock ) <nl> for ( i = 0 ; i < MAX_USER_FLAGS ; i ++) { <nl> if ( erock -> userflags [ i / 32 ] & 1 <<( i & 31 )) <nl> continue ; <nl> + if (! mailbox -> flagname [ i ]) <nl> + continue ; <nl> if ( verbose ) <nl> fprintf ( stderr , " Expunging userflag % u (% s ) from % s \ n ", <nl> i , mailbox -> flagname [ i ], mailbox -> name );
static int mboxlist_do_find ( struct find_rock * rock , const strarray_t * patterns ) <nl> int i ; <nl> const char * p ; <nl>  <nl> + if ( patterns -> count < 1 ) return 0 ; /* nothing to do */ <nl> + <nl> for ( i = 0 ; i < patterns -> count ; i ++) { <nl> glob * g = glob_init ( strarray_nth ( patterns , i ), rock -> namespace -> hier_sep ); <nl> ptrarray_append (& rock -> globs , g );
int sync_update_mailbox ( struct sync_folder * local , <nl> flags |= SYNC_FLAG_ISREPEAT ; <nl>  <nl> if ( r == IMAP_AGAIN ) { <nl> + local -> batchsize = 0 ; /* don ' t batch the re - update , means sync to 2 . 4 will still work after fullsync */ <nl> r = mailbox_full_update ( local , reserve_list , sync_be , flags ); <nl> if (! r ) r = update_mailbox_once ( local , remote , topart , <nl> reserve_list , sync_be , flags );
int deliver ( message_data_t * msgdata , char * authuser , <nl> proxy_adddest (& dlist , recip , n , mbentry -> server , authuser ); <nl> status [ n ] = nosieve ; <nl> } <nl> - else { <nl> + else if (! r ) { <nl> /* local mailbox */ <nl> mydata . cur_rcpt = n ; <nl> # ifdef USE_SIEVE
EXPORTED int index_snippets ( struct index_state * state , <nl> int nmatches = 0 ; <nl> struct snippet_rock srock ; <nl>  <nl> + /* reload index */ <nl> + r = index_refresh ( state ); <nl> + if ( r ) return r ; <nl> + <nl> bx = search_begin_search ( state -> mailbox , SEARCH_MULTIPLE ); <nl> if (! bx ) { <nl> r = IMAP_INTERNAL ;
int mailbox_create ( const char * name , <nl> mailbox -> i . options = options ; <nl> mailbox -> i . highestmodseq = 1 ; <nl>  <nl> + /* initialise header size field so appends calculate the <nl> + * correct map size */ <nl> + mailbox -> index_size = INDEX_HEADER_SIZE ; <nl> + <nl> mailbox -> header_dirty = 1 ; <nl> if (! uniqueid ) { <nl> mailbox_make_uniqueid ( mailbox );
int meth_delete ( struct transaction_t * txn , void * params ) <nl> else if ( r == IMAP_MAILBOX_NONEXISTENT ) ret = HTTP_NOT_FOUND ; <nl> else if ( r ) ret = HTTP_SERVER_ERROR ; <nl>  <nl> - dparams -> davdb . close_db ( davdb ); <nl> - <nl> goto done ; <nl> } <nl> 
EXPORTED void mbname_free ( mbname_t ** mbnamep ) <nl> free ( mbname -> intname ); <nl> free ( mbname -> extname ); <nl> free ( mbname -> extuserid ); <nl> + free ( mbname -> recipient ); <nl>  <nl> /* thing itself */ <nl> free ( mbname );
int service_main ( int argc , char ** argv , <nl> struct io_count * io_count_stop ; <nl>  <nl> if ( config_iolog ) { <nl> - io_count_start = malloc ( sizeof ( struct io_count )); <nl> - io_count_stop = malloc ( sizeof ( struct io_count )); <nl> + io_count_start = xmalloc ( sizeof ( struct io_count )); <nl> + io_count_stop = xmalloc ( sizeof ( struct io_count )); <nl> read_io_count ( io_count_start ); <nl> } <nl> 
EXPORTED void buf_ensure ( struct buf * buf , size_t n ) <nl> /* protect against wrap */ <nl> assert ( newlen >= buf -> len ); <nl>  <nl> + if ( buf -> alloc >= newlen ) <nl> + return ; <nl> + <nl> if ( buf -> alloc ) { <nl> buf -> s = xrealloc ( buf -> s , newlen ); <nl> }
static int chkchildren ( char * name , <nl> int r ; <nl>  <nl> r = mboxlist_lookup ( name , & mbentry , 0 ); <nl> + /* deleted mailboxes don ' t count as children */ <nl> + if ( r == IMAP_MAILBOX_NONEXISTENT ) return 0 ; <nl> if ( r ) return r ; <nl>  <nl> if (! strcmp ( part , mbentry -> partition ))
static int expire_conversations ( const mbentry_t * mbentry , void * rock ) <nl> if ( mbentry -> mbtype & MBTYPE_REMOTE ) <nl> goto done ; <nl>  <nl> + if ( mboxname_isdeletedmailbox ( mbentry -> name , NULL )) <nl> + goto done ; <nl> + <nl> filename = conversations_getmboxpath ( mbentry -> name ); <nl> if (! filename ) <nl> goto done ;
static void add_attendees ( icalcomponent * ical , <nl> prop = icalcomponent_get_next_invitee ( comp )) { <nl>  <nl> const char * attendee = icalproperty_get_invitee ( prop ); <nl> + if (! attendee ) continue ; <nl> + <nl> if (! strncasecmp ( attendee , " mailto :", 7 )) attendee += 7 ; <nl>  <nl> /* Skip where attendee == organizer */
int mailbox_ensure_cache ( struct mailbox * mailbox , unsigned offset ) <nl> mailbox -> cache_fd = open ( fname , openflags , 0 ); <nl> if ( mailbox -> cache_fd == - 1 ) <nl> goto fail ; <nl> + <nl> + if ( mailbox -> cache_buf . s ) <nl> + map_free (( const char **)& mailbox -> cache_buf . s , & mailbox -> cache_len ); <nl> + mailbox -> cache_buf . len = 0 ; <nl> } <nl>  <nl> if ( offset >= mailbox -> cache_buf . len ) {
EXPORTED int mboxlist_renamemailbox ( const char * oldname , const char * newname , <nl>  <nl> /* log the rename */ <nl> sync_log_mailbox_double ( oldname , newname ); <nl> + /* and log an append so that squatter indexes it */ <nl> + sync_log_append ( newname ); <nl> } <nl>  <nl> /* free memory */
static int mailbox_open_advanced ( const char * name , <nl> goto done ; <nl> } <nl>  <nl> + if (! mbentry -> partition ) { <nl> + mboxlist_entry_free (& mbentry ); <nl> + r = IMAP_MAILBOX_NONEXISTENT ; <nl> + goto done ; <nl> + } <nl> + <nl> mailbox -> part = xstrdup ( mbentry -> partition ); <nl>  <nl> /* Note that the header does have the ACL information , but it is only
static int is_incompressible ( const char * p , size_t n ) <nl> */ <nl> EXPORTED int prot_data_boundary ( struct protstream * s ) <nl> { <nl> - s -> boundary = 1 ; <nl> + // XXX - appears to be broken , so just don ' t set the boundary . We ' ll <nl> + // spend trivially more CPU when transferring binary parts . Boo hoo <nl> + // re - enable this once the bug is fixed <nl> + // s -> boundary = 1 ; <nl> return 0 ; <nl> } <nl> 
static int caldav_put ( struct transaction_t * txn , <nl> } <nl> else { <nl> /* Attendee scheduling object resource */ <nl> + if (! oldical ) { <nl> + /* Can ' t reply to a non - existent invitation */ <nl> + ret = HTTP_FORBIDDEN ; <nl> + goto done ; <nl> + } <nl> sched_reply ( userid , oldical , ical ); <nl> } <nl> 
void do_zonedir ( const char * dir , struct hash_table * tzentries , <nl> ical = icalparser_parse_string ( base ); <nl> map_free (& base , & len ); <nl>  <nl> + if (! ical ) continue ; /* skip non - iCalendar files */ <nl> + <nl> comp = icalcomponent_get_first_component ( ical , <nl> ICAL_VTIMEZONE_COMPONENT ); <nl> prop = icalcomponent_get_first_property ( comp , ICAL_TZID_PROPERTY );
static int usersubs_cb ( void * rock , const char * key , size_t keylen , <nl> mboxname_userownsmailbox ( mbrock -> userid , mboxname )) return 0 ; <nl>  <nl> r = mboxlist_lookup ( mboxname , & mbrock -> mbentry , NULL ); <nl> + if ( r == IMAP_MAILBOX_NONEXISTENT ) return 0 ; <nl> + <nl> if ( r ) { <nl> syslog ( LOG_INFO , " mboxlist_lookup (% s ) failed : % s ", <nl> mboxname , error_message ( r ));
int meth_mkcol ( struct transaction_t * txn , void * params ) <nl> /* Start construction of our mkcol / mkcalendar response */ <nl> buf_appendcstr (& txn -> buf , "- response "); <nl> root = init_xml_response ( buf_cstring (& txn -> buf ), NS_REQ_ROOT , root , ns ); <nl> + buf_reset (& txn -> buf ); <nl> if (! root ) { <nl> ret = HTTP_SERVER_ERROR ; <nl> txn -> error . desc = " Unable to create XML response \ r \ n ";
int cacheScheduleIOPushJobs ( int flags ) { <nl>  <nl> if ( op -> type != REDIS_IO_LOAD && flags & REDIS_IO_ONLYLOADS ) break ; <nl>  <nl> - if (!( flags & REDIS_IO_ASAP ) && <nl> + /* Don ' t execute SAVE before the scheduled time for completion */ <nl> + if ( op -> type == REDIS_IO_SAVE && !( flags & REDIS_IO_ASAP ) && <nl> ( now - op -> ctime ) < server . cache_flush_delay ) break ; <nl>  <nl> /* Don ' t add a SAVE job in the IO thread queue if there is already
void sendReplyToClient ( aeEventLoop * el , int fd , void * privdata , int mask ) { <nl>  <nl> /* If the buffer was sent , set bufpos to zero to continue with <nl> * the remainder of the reply . */ <nl> - if ( c -> sentlen == c -> bufpos ) { <nl> + if (( int ) c -> sentlen == c -> bufpos ) { <nl> c -> bufpos = 0 ; <nl> c -> sentlen = 0 ; <nl> }
static void repl ( void ) { <nl> } <nl>  <nl> elapsed = mstime ()- start_time ; <nl> - if ( elapsed >= 500 ) { <nl> + if ( elapsed >= 500 && <nl> + config . output == OUTPUT_STANDARD ) <nl> + { <nl> printf ("(%. 2fs )\ n ",( double ) elapsed / 1000 ); <nl> } <nl> }
void xreadCommand ( client * c ) { <nl> } <nl>  <nl> if ( strcmp ( c -> argv [ i ]-> ptr ,"$") == 0 ) { <nl> + if ( xreadgroup ) { <nl> + addReplyError ( c ," The $ ID can be specified only when calling " <nl> + " XREAD without GROUP option ."); <nl> + goto cleanup ; <nl> + } <nl> if ( o ) { <nl> stream * s = o -> ptr ; <nl> ids [ id_idx ] = s -> last_id ;
void clusterCommand ( redisClient * c ) { <nl> addReplyError ( c ," Invalid CLUSTER SETSLOT action or number of arguments "); <nl> return ; <nl> } <nl> + clusterUpdateState (); <nl> clusterSaveConfigOrDie (); <nl> addReply ( c , shared . ok ); <nl> } else if (! strcasecmp ( c -> argv [ 1 ]-> ptr ," info ") && c -> argc == 2 ) {
size_t zmalloc_size ( void * ptr ) { <nl> return size + PREFIX_SIZE ; <nl> } <nl> size_t zmalloc_usable ( void * ptr ) { <nl> - return zmalloc_usable ( ptr )- PREFIX_SIZE ; <nl> + return zmalloc_size ( ptr )- PREFIX_SIZE ; <nl> } <nl> # endif <nl> 
int clusterLoadConfig ( char * filename ) { <nl> while ( fgets ( line , maxline , fp ) != NULL ) { <nl> int argc ; <nl> sds * argv = sdssplitargs ( line ,& argc ); <nl> + if ( argv == NULL ) goto fmterr ; <nl> + <nl> clusterNode * n , * master ; <nl> char * p , * s ; <nl> 
int luaRedisGenericCommand ( lua_State * lua , int raise_error ) { <nl> luaSortArray ( lua ); <nl> } <nl> sdsfree ( reply ); <nl> + c -> reply_bytes = 0 ; <nl>  <nl> cleanup : <nl> /* Clean up . Command code may have changed argv / argc so we use the
int rdbSaveDoubleValue ( rio * rdb , double val ) { <nl>  <nl> /* For information about double serialization check rdbSaveDoubleValue () */ <nl> int rdbLoadDoubleValue ( rio * rdb , double * val ) { <nl> - char buf [ 128 ]; <nl> + char buf [ 256 ]; <nl> unsigned char len ; <nl>  <nl> if ( rioRead ( rdb ,& len , 1 ) == 0 ) return - 1 ;
static int read_header ( ShortenContext * s ) <nl> s -> channels = get_uint ( s , CHANSIZE ); <nl> if ( s -> channels <= 0 || s -> channels > MAX_CHANNELS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + s -> channels = 0 ; <nl> return - 1 ; <nl> } <nl> s -> avctx -> channels = s -> channels ;
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> break ; <nl> case MKTAG (' s ', ' t ', ' r ', ' f '): <nl> /* stream header */ <nl> - if ( stream_index >= s -> nb_streams || avi -> dv_demux ) { <nl> + if ( stream_index >= ( unsigned ) s -> nb_streams || avi -> dv_demux ) { <nl> url_fskip ( pb , size ); <nl> } else { <nl> st = s -> streams [ stream_index ];
int avfilter_graph_add_filter ( AVFilterGraph * graph , AVFilterContext * filter ) <nl> graph -> filters = filters ; <nl> graph -> filters [ graph -> nb_filters ++] = filter ; <nl>  <nl> +# if FF_API_FOO_COUNT <nl> + graph -> filter_count = graph -> nb_filters ; <nl> +# endif <nl> + <nl> return 0 ; <nl> } <nl> 
static int mp3_write_header ( struct AVFormatContext * s ) <nl> char yeartxt [ 10 ]; <nl>  <nl> if ( s -> track ) <nl> - snprintf ( tracktxt , sizeof ( tracktxt ) - 1 , "% d ", s -> track ); <nl> + snprintf ( tracktxt , sizeof ( tracktxt ) , "% d ", s -> track ); <nl> if ( s -> year ) <nl> snprintf ( yeartxt , sizeof ( yeartxt ) , "% d ", s -> year ); <nl> 
static int udp_open ( URLContext * h , const char * uri , int flags ) <nl> goto fail ; <nl> } <nl>  <nl> - if (( s -> is_multicast || ! s -> local_port ) && ( h -> flags & AVIO_FLAG_READ )) <nl> + if (( s -> is_multicast || s -> local_port < 0 ) && ( h -> flags & AVIO_FLAG_READ )) <nl> s -> local_port = port ; <nl>  <nl> if ( localaddr [ 0 ])
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> if (! get_bits1 (& ctx -> gb ) || ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> transform_id = get_bits (& ctx -> gb , 5 ); <nl> - if (! transforms [ transform_id ]. inv_trans ) { <nl> + if ( transform_id >= FF_ARRAY_ELEMS ( transforms ) || <nl> + ! transforms [ transform_id ]. inv_trans ) { <nl> av_log_ask_for_sample ( avctx , " Unimplemented transform : % d !\ n ", transform_id ); <nl> return AVERROR_PATCHWELCOME ; <nl> }
int ffurl_close ( URLContext * h ) <nl> # if CONFIG_NETWORK <nl> ff_network_close (); <nl> # endif <nl> - if ( h -> prot -> priv_data_size ) <nl> + if ( h -> prot -> priv_data_size ) { <nl> + if ( h -> prot -> priv_data_class ) <nl> + av_opt_free ( h -> priv_data ); <nl> av_free ( h -> priv_data ); <nl> + } <nl> av_free ( h ); <nl> return ret ; <nl> }
int main ( int argc , char ** argv ) <nl>  <nl> ret = probe_file ( input_filename ); <nl>  <nl> + uninit_opts (); <nl> + av_dict_free (& fmt_entries_to_show ); <nl> + <nl> avformat_network_deinit (); <nl>  <nl> return ret ;
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> if ( st -> codec -> bit_rate > 0 ) { <nl> - if ( INT_MAX - st -> codec -> bit_rate > bit_rate ) { <nl> + if ( INT_MAX - st -> codec -> bit_rate < bit_rate ) { <nl> bit_rate = 0 ; <nl> break ; <nl> }
static int mjpeg_decode_frame ( AVCodecContext * avctx , <nl> *( dst ++) = x ; <nl> if ( x == 0xff ) <nl> { <nl> - while (* src == 0xff ) src ++; <nl> + while ( src < buf_end && x == 0xff ) <nl> + x = *( src ++); <nl>  <nl> - x = *( src ++); <nl> if ( x >= 0xd0 && x <= 0xd7 ) <nl> *( dst ++) = x ; <nl> else if ( x )
static int video_thread ( void * arg ) <nl> filt_out = is -> out_video_filter ; <nl> # endif <nl>  <nl> + if (! frame ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> for (;;) { <nl> # if CONFIG_AVFILTER <nl> AVRational tb ;
static inline int put_bits_count ( PutBitContext * s ) <nl> static inline void flush_put_bits ( PutBitContext * s ) <nl> { <nl> # ifndef BITSTREAM_WRITER_LE <nl> - s -> bit_buf <<= s -> bit_left ; <nl> + if ( s -> bit_left < 32 ) <nl> + s -> bit_buf <<= s -> bit_left ; <nl> # endif <nl> while ( s -> bit_left < 32 ) { <nl> /* XXX : should test end of buffer */
static int mov_read_stss ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> av_dlog ( c -> fc , " keyframe_count = % d \ n ", entries ); <nl>  <nl> + if (! entries ) <nl> + return 0 ; <nl> if ( entries >= UINT_MAX / sizeof ( int )) <nl> return AVERROR_INVALIDDATA ; <nl> sc -> keyframes = av_malloc ( entries * sizeof ( int ));
static void stream_component_close ( VideoState * is , int stream_index ) <nl> SDL_CloseAudio (); <nl>  <nl> packet_queue_end (& is -> audioq ); <nl> + av_free_packet (& is -> audio_pkt ); <nl> if ( is -> reformat_ctx ) <nl> av_audio_convert_free ( is -> reformat_ctx ); <nl> is -> reformat_ctx = NULL ;
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> * q ++ = 0xe0 ; <nl> } else if ( st -> codec -> codec_type == AVMEDIA_TYPE_AUDIO && <nl> ( st -> codec -> codec_id == CODEC_ID_MP2 || <nl> - st -> codec -> codec_id == CODEC_ID_MP3 )) { <nl> + st -> codec -> codec_id == CODEC_ID_MP3 || <nl> + st -> codec -> codec_id == CODEC_ID_AAC )) { <nl> * q ++ = 0xc0 ; <nl> } else { <nl> * q ++ = 0xbd ;
static void svq1_write_header ( SVQ1Context * s , int frame_type ) <nl> # define QUALITY_THRESHOLD 100 <nl> # define THRESHOLD_MULTIPLIER 0 . 6 <nl>  <nl> +# if defined ( HAVE_ALTIVEC ) <nl> +# undef vector <nl> +# endif <nl>  <nl> static int encode_block ( SVQ1Context * s , uint8_t * src , uint8_t * ref , uint8_t * decoded , int stride , int level , int threshold , int lambda , int intra ){ <nl> int count , y , x , i , j , split , best_mean , best_score , best_count ;
decode_intra_mb : <nl> sl -> intra4x4_pred_mode_cache [ scan8 [ i ]] = decode_cabac_mb_intra4x4_pred_mode ( sl , pred ); <nl>  <nl> ff_dlog ( h -> avctx , " i4x4 pred =% d mode =% d \ n ", pred , <nl> - h -> intra4x4_pred_mode_cache [ scan8 [ i ]]); <nl> + sl -> intra4x4_pred_mode_cache [ scan8 [ i ]]); <nl> } <nl> } <nl> write_back_intra_pred_mode ( h , sl );
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> for ( i = 0 ; i < sc -> chunk_count ; i ++) { <nl> current_offset = sc -> chunk_offsets [ i ]; <nl> - if ( stsc_index + 1 < sc -> stsc_count && <nl> + while ( stsc_index + 1 < sc -> stsc_count && <nl> i + 1 == sc -> stsc_data [ stsc_index + 1 ]. first ) <nl> stsc_index ++; <nl> for ( j = 0 ; j < sc -> stsc_data [ stsc_index ]. count ; j ++) {
static int set_channel_layout ( AVCodecContext * avctx , int channels , int num_core_ <nl> s -> channel_order_tab = ff_dca_channel_reorder_nolfe [ s -> amode ]; <nl> } <nl>  <nl> + if ( channels < ff_dca_channels [ s -> amode ]) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( channels > !! s -> lfe && <nl> s -> channel_order_tab [ channels - 1 - !! s -> lfe ] < 0 ) <nl> return AVERROR_INVALIDDATA ;
typedef struct FlacSubframe { <nl> int shift ; <nl> RiceContext rc ; <nl> int32_t samples [ FLAC_MAX_BLOCKSIZE ]; <nl> - int32_t residual [ FLAC_MAX_BLOCKSIZE ]; <nl> + int32_t residual [ FLAC_MAX_BLOCKSIZE + 1 ]; <nl> } FlacSubframe ; <nl>  <nl> typedef struct FlacFrame {
static void decode_frame ( SiprContext * ctx , SiprParameters * params , <nl> memcpy ( ctx -> postfilter_syn5k0 , ctx -> postfilter_syn5k0 + frame_size , <nl> LP_FILTER_ORDER * sizeof ( float )); <nl> } <nl> - memcpy ( ctx -> excitation , excitation - PITCH_DELAY_MAX - L_INTERPOL , <nl> + memmove ( ctx -> excitation , excitation - PITCH_DELAY_MAX - L_INTERPOL , <nl> ( PITCH_DELAY_MAX + L_INTERPOL ) * sizeof ( float )); <nl>  <nl> ff_acelp_apply_order_2_transfer_function ( out_data , synth ,
int ff_rtmp_packet_create ( RTMPPacket * pkt , int channel_id , RTMPPacketType type , <nl> int timestamp , int size ) <nl> { <nl> if ( size ) { <nl> - pkt -> data = av_malloc ( size ); <nl> + pkt -> data = av_realloc ( NULL , size ); <nl> if (! pkt -> data ) <nl> return AVERROR ( ENOMEM ); <nl> }
static int xa_read_packet ( AVFormatContext * s , <nl> unsigned int packet_size ; <nl> int ret ; <nl>  <nl> - if ( xa -> sent_bytes > xa -> out_size ) <nl> - return AVERROR ( EIO ); <nl> + if ( xa -> sent_bytes >= xa -> out_size ) <nl> + return AVERROR_EOF ; <nl> /* 1 byte header and 14 bytes worth of samples * number channels per block */ <nl> packet_size = 15 * st -> codec -> channels ; <nl> 
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> break ; <nl>  <nl> arraylen = avio_rb32 ( ioc ); <nl> + if ( arraylen >> 28 ) <nl> + break ; <nl> + <nl> /* <nl> * Expect only ' times ' or ' filepositions ' sub - arrays in other case refuse to use such metadata <nl> * for indexing
static int mkv_write_header ( AVFormatContext * s ) <nl> // currently defined level 1 element <nl> mkv -> main_seekhead = mkv_start_seekhead ( pb , mkv -> segment_offset , 10 ); <nl> mkv -> cluster_seekhead = mkv_start_seekhead ( pb , mkv -> segment_offset , 0 ); <nl> + if ( mkv -> main_seekhead == NULL || mkv -> cluster_seekhead == NULL ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( mkv_add_seekhead_entry ( mkv -> main_seekhead , MATROSKA_ID_INFO , url_ftell ( pb )) < 0 ) <nl> return - 1 ;
static av_cold int qsv_enc_init ( AVCodecContext * avctx ) <nl>  <nl> if ( q -> load_plugin != LOAD_PLUGIN_NONE ) { <nl> static const char * uid_hevcenc_sw = " 2fca99749fdb49aeb121a5b63ef568f7 "; <nl> - static const char * uid_hevcenc_hw = " e5400a06c74d41f5b12d430bbaa23d0b "; <nl> + static const char * uid_hevcenc_hw = " 6fadc791a0c2eb479ab6dcd5ea9da347 "; <nl>  <nl> if ( q -> qsv . load_plugins [ 0 ]) { <nl> av_log ( avctx , AV_LOG_WARNING ,
static int aac_adtstoasc_filter ( AVBitStreamFilterContext * bsfc , <nl> buf += get_bits_count (& gb )/ 8 ; <nl> } <nl> avctx -> extradata_size = 2 + pce_size ; <nl> - avctx -> extradata = av_malloc ( avctx -> extradata_size ); <nl> + avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> init_put_bits (& pb , avctx -> extradata , avctx -> extradata_size ); <nl> put_bits (& pb , 5 , hdr . object_type );
static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> FLACContext * s = avctx -> priv_data ; <nl> s -> avctx = avctx ; <nl>  <nl> + avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> + <nl> if ( avctx -> extradata_size > 4 ) { <nl> /* initialize based on the demuxer - supplied streamdata header */ <nl> if ( avctx -> extradata_size == FLAC_STREAMINFO_SIZE ) { <nl> static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> } <nl> } <nl>  <nl> - avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> return 0 ; <nl> } <nl> 
static int ffm_write_header ( AVFormatContext * s ) <nl> static int ffm_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> int64_t dts ; <nl> - uint8_t header [ FRAME_HEADER_SIZE ]; <nl> + uint8_t header [ FRAME_HEADER_SIZE + 4 ]; <nl> int header_size = FRAME_HEADER_SIZE ; <nl>  <nl> dts = s -> timestamp + pkt -> dts ;
static int decode_wdlt ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> segments = bytestream2_get_le16 ( gb ); <nl> } <nl> line_ptr = frame ; <nl> + if ( frame_end - frame < width ) <nl> + return AVERROR_INVALIDDATA ; <nl> frame += width ; <nl> y ++; <nl> while ( segments --) {
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
static void vc1_mc_4mv_chroma4 ( VC1Context * v ) <nl> mquant = v -> altpq ; \ <nl> if (( edges & 8 ) && s -> mb_y == ( s -> mb_height - 1 )) \ <nl> mquant = v -> altpq ; \ <nl> + if (! mquant || mquant > 31 ) { \ <nl> + av_log ( v -> s . avctx , AV_LOG_ERROR , \ <nl> + " Overriding invalid mquant % d \ n ", mquant ); \ <nl> + mquant = 1 ; \ <nl> + } \ <nl> } <nl>  <nl> /**
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> break ; <nl> } <nl>  <nl> - if ( last ){ <nl> + if ( last && s -> current_picture_ptr ){ <nl> if ( r -> loop_filter ) <nl> r -> loop_filter ( r , s -> mb_height - 1 ); <nl> ff_er_frame_end ( s );
static int mpegts_read_packet ( AVFormatContext * s , <nl> MpegTSContext * ts = s -> priv_data ; <nl> int ret , i ; <nl>  <nl> + pkt -> size = - 1 ; <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> static int mpegts_read_packet ( AVFormatContext * s , <nl> } <nl> } <nl>  <nl> + if (! ret && pkt -> size < 0 ) <nl> + ret = AVERROR ( EINTR ); <nl> return ret ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> } else if ( s -> bit_depth == 16 && <nl> s -> color_type == PNG_COLOR_TYPE_GRAY_ALPHA ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_YA16BE ; <nl> + } else if ( s -> bit_depth == 16 && <nl> + s -> color_type == PNG_COLOR_TYPE_RGB_ALPHA ) { <nl> + avctx -> pix_fmt = AV_PIX_FMT_RGBA64BE ; <nl> } else { <nl> avpriv_report_missing_feature ( avctx , <nl> " Bit depth % d color type % d ",
static int find_slice_quant ( AVCodecContext * avctx , const AVFrame * pic , <nl> if ( ctx -> alpha_bits ) <nl> bits += estimate_alpha_plane ( ctx , & error , src , linesize [ 3 ], <nl> mbs_per_slice , q , td -> blocks [ 3 ]); <nl> - if ( bits > 65000 * 8 ) { <nl> + if ( bits > 65000 * 8 ) <nl> error = SCORE_LIMIT ; <nl> - break ; <nl> - } <nl> + <nl> slice_bits [ q ] = bits ; <nl> slice_score [ q ] = error ; <nl> }
# define XAVS_PART_B8X8 0x100 /* Analyze b16x8 , b */ <nl>  <nl> typedef struct XavsContext { <nl> + AVClass * class ; <nl> xavs_param_t params ; <nl> xavs_t * enc ; <nl> xavs_picture_t pic ;
static void set_codec_str ( AVFormatContext * s , AVCodecParameters * par , <nl> tags [ 0 ] = ff_mp4_obj_type ; <nl> oti = av_codec_get_tag ( tags , par -> codec_id ); <nl> if ( oti ) <nl> - av_strlcatf ( str , size , ".% 02x ", oti ); <nl> + av_strlcatf ( str , size , ".% 02 " SCNx32 , oti ); <nl> else <nl> return ; <nl> 
static inline int op ( uint8_t ** dst , const uint8_t * dst_end , <nl> int striplen = FFMIN ( count , remaining ); <nl> if ( buf ) { <nl> striplen = FFMIN ( striplen , buf_end - * buf ); <nl> + if (* buf >= buf_end ) <nl> + goto exhausted ; <nl> memcpy (* dst , * buf , striplen ); <nl> * buf += striplen ; <nl> } else if ( pixel >= 0 )
int ff_adts_decode_extradata ( AVFormatContext * s , ADTSContext * adts , uint8_t * buf <nl> av_log ( s , AV_LOG_ERROR , " Scalable configurations are not allowed in ADTS \ n "); <nl> return - 1 ; <nl> } <nl> + if ( get_bits (& gb , 1 )) { <nl> + av_log ( s , AV_LOG_ERROR , " Extension flag is not allowed in ADTS \ n "); <nl> + return - 1 ; <nl> + } <nl> if (! adts -> channel_conf ) { <nl> init_put_bits (& pb , adts -> pce_data , MAX_PCE_SIZE ); <nl> 
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl>  <nl> log = av_log2 ( buf ); <nl>  <nl> - if ( log > 31 - 11 ){ <nl> + if ( log - k >= 32 - MIN_CACHE_BITS ){ <nl> buf >>= log - k ; <nl> buf += ( 30 - log )<< k ; <nl> LAST_SKIP_BITS ( re , gb , 32 + k - log );
static int xan_unpack ( uint8_t * dest , const int dest_len , <nl> if ( size + size2 > dest_end - dest ) <nl> break ; <nl> } <nl> - if ( src + size > src_end || dest + size + size2 > dest_end ) <nl> + if ( src + size > src_end || dest + size + size2 > dest_end || <nl> + dest - orig_dest + size < back ) <nl> return - 1 ; <nl> bytestream_get_buffer (& src , dest , size ); <nl> dest += size ;
static int a64_write_header ( AVFormatContext * s ) <nl> 0x00 , // charset_lifetime ( multi only ) <nl> 0x00 // fps in 50 / fps ; <nl> }; <nl> + <nl> + if ( avctx -> extradata_size < 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Missing extradata \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> switch ( avctx -> codec -> id ) { <nl> case AV_CODEC_ID_A64_MULTI : <nl> header [ 2 ] = 0x00 ;
static int decode_channel_transform ( WMAProDecodeCtx * s ) <nl> if ( get_bits1 (& s -> gb )) { <nl> avpriv_request_sample ( s -> avctx , <nl> " Unknown channel transform type "); <nl> + return AVERROR_PATCHWELCOME ; <nl> } <nl> } else { <nl> chgroup -> transform = 1 ;
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> /* 248DCT setup */ <nl> s -> fdct [ 1 ] = dsp . fdct248 ; <nl> s -> idct_put [ 1 ] = ff_simple_idct248_put ; // FIXME : need to add it to DSP <nl> - memcpy ( s -> dv_zigzag [ 1 ], ff_dv_zigzag248_direct , 64 ); <nl> + memcpy ( s -> dv_zigzag [ 1 ], ff_dv_zigzag248_direct , sizeof ( s -> dv_zigzag [ 1 ])); <nl>  <nl> s -> avctx = avctx ; <nl> avctx -> chroma_sample_location = AVCHROMA_LOC_TOPLEFT ;
static void unpack_alpha ( GetBitContext * gb , uint16_t * dst , int num_coeffs , <nl> dst [ idx ++] = alpha_val >> 6 ; <nl> else <nl> dst [ idx ++] = ( alpha_val << 2 ) | ( alpha_val >> 6 ); <nl> - if ( idx == num_coeffs - 1 ) <nl> + if ( idx >= num_coeffs - 1 ) <nl> break ; <nl> } while ( get_bits1 ( gb )); <nl> val = get_bits ( gb , 4 );
static int flush_packet ( AVFormatContext * ctx , int stream_index , <nl>  <nl> if ( stuffing_size < 0 ) <nl> stuffing_size = 0 ; <nl> + <nl> + if ( startcode == PRIVATE_STREAM_1 && id >= 0xa0 ) { <nl> + if ( payload_size < av_fifo_size ( stream -> fifo )) <nl> + stuffing_size += payload_size % stream -> lpcm_align ; <nl> + } <nl> + <nl> if ( stuffing_size > 16 ) { /*<= 16 for MPEG - 1 , <= 32 for MPEG - 2 */ <nl> pad_packet_bytes += stuffing_size ; <nl> packet_size -= stuffing_size ;
int ffurl_open ( URLContext ** puc , const char * filename , int flags , <nl> int ret = ffurl_alloc ( puc , filename , flags , int_cb , protocols ); <nl> if ( ret ) <nl> return ret ; <nl> + if ( options && <nl> + ( ret = av_opt_set_dict (* puc , options )) < 0 ) <nl> + goto fail ; <nl> if ( options && (* puc )-> prot -> priv_data_class && <nl> ( ret = av_opt_set_dict ((* puc )-> priv_data , options )) < 0 ) <nl> goto fail ;
static int probe_buf_write ( void * opaque , uint8_t * buf , int buf_size ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> int ret ; <nl> - uint8_t * buffer = av_malloc ( AVP_BUFFSIZE ); <nl> + uint8_t * buffer = av_mallocz ( AVP_BUFFSIZE ); <nl>  <nl> if (! buffer ) <nl> exit ( 1 );
static int cyuv_decode_init ( AVCodecContext * avctx ) <nl>  <nl> s -> avctx = avctx ; <nl> s -> width = avctx -> width ; <nl> + /* width needs to be divisible by 4 for this codec to work */ <nl> + if ( s -> width & 0x3 ) <nl> + return - 1 ; <nl> s -> height = avctx -> height ; <nl> avctx -> pix_fmt = PIX_FMT_YUV411P ; <nl> avctx -> has_b_frames = 0 ;
static void truemotion1_decode_24bit ( TrueMotion1Context * s ) <nl> int index ; <nl>  <nl> /* clean out the line buffer */ <nl> - memset ( s -> vert_pred , 0 , s -> avctx -> width * sizeof ( unsigned short )); <nl> + memset ( s -> vert_pred , 0 , s -> avctx -> width * sizeof ( unsigned int )); <nl>  <nl> GET_NEXT_INDEX (); <nl> 
static int mov_read_stsd ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> sc -> stsd_count = entries ; <nl> - sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof ( sc -> extradata_size )); <nl> + sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof (* sc -> extradata_size )); <nl> if (! sc -> extradata_size ) <nl> return AVERROR ( ENOMEM ); <nl> 
int attribute_align_arg avcodec_encode_audio2 ( AVCodecContext * avctx , <nl> avctx -> time_base ); <nl> } <nl> avpkt -> dts = avpkt -> pts ; <nl> + } else { <nl> + avpkt -> size = 0 ; <nl> } <nl> } else { <nl> /* for compatibility with encoders not supporting encode2 (), we need to
static inline void put_cabac_ueg ( CABACContext * c , uint8_t * state , int v , int ma <nl> } <nl>  <nl> static void refill ( CABACContext * c ){ <nl> - if ( c -> bytestream < c -> bytestream_end ) <nl> + if ( c -> bytestream <= c -> bytestream_end ) <nl> # if CABAC_BITS == 16 <nl> c -> low += (( c -> bytestream [ 0 ]<< 9 ) + ( c -> bytestream [ 1 ])<< 1 ); <nl> # else
static int sol_read_packet ( AVFormatContext * s , <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR ( EIO ); <nl> ret = av_get_packet ( s -> pb , pkt , MAX_SIZE ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> pkt -> stream_index = 0 ; <nl>  <nl> /* note : we need to modify the packet size here to handle the last
static int decode_vol_header ( MpegEncContext * s , GetBitContext * gb ){ <nl>  <nl> s -> progressive_sequence = <nl> s -> progressive_frame = get_bits1 ( gb )^ 1 ; <nl> + s -> interlaced_dct = 0 ; <nl> if (! get_bits1 ( gb ) && ( s -> avctx -> debug & FF_DEBUG_PICT_INFO )) <nl> av_log ( s -> avctx , AV_LOG_INFO , " MPEG4 OBMC not supported ( very likely buggy encoder )\ n "); /* OBMC Disable */ <nl> if ( vo_ver_id == 1 ) {
static const char * filter_name ( void * p ) <nl> } <nl>  <nl> static const AVClass avfilter_class = { <nl> - " AVFilter ", <nl> - filter_name , <nl> - NULL , <nl> - LIBAVUTIL_VERSION_INT , <nl> + . class_name = " AVFilter ", <nl> + . item_name = filter_name , <nl> + . version = LIBAVUTIL_VERSION_INT , <nl> }; <nl>  <nl> int avfilter_open ( AVFilterContext ** filter_ctx , AVFilter * filter , const char * inst_name )
static int ffm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> FFMContext * ffm = s -> priv_data ; <nl> int duration ; <nl>  <nl> + if ( url_fsize ( s -> pb ) == FFM_PACKET_SIZE ) <nl> + return - 1 ; <nl> + <nl> switch ( ffm -> read_state ) { <nl> case READ_HEADER : <nl> if (! ffm_is_avail_data ( s , FRAME_HEADER_SIZE + 4 )) {
static int mpegts_read_header ( AVFormatContext * s , <nl> /* normal demux */ <nl>  <nl> /* first do a scaning to get all the services */ <nl> - if ( avio_seek ( pb , pos , SEEK_SET ) < 0 ) <nl> + if ( pb -> seekable && avio_seek ( pb , pos , SEEK_SET ) < 0 ) <nl> av_log ( s , AV_LOG_ERROR , " Unable to seek back to the start \ n "); <nl>  <nl> mpegts_open_section_filter ( ts , SDT_PID , sdt_cb , ts , 1 );
static int vaapi_encode_h264_write_extra_header ( AVCodecContext * avctx , <nl>  <nl> if ( priv -> sei_needed ) { <nl> if ( priv -> aud_needed ) { <nl> - vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + err = vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + if ( err < 0 ) <nl> + goto fail ; <nl> priv -> aud_needed = 0 ; <nl> } <nl> 
static int ac3_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> } <nl>  <nl> /* check for crc mismatch */ <nl> - if ( avctx -> error_resilience > 0 ) { <nl> + if ( avctx -> error_resilience >= FF_ER_CAREFUL ) { <nl> if ( av_crc ( av_crc_get_table ( AV_CRC_16_ANSI ), 0 , & buf [ 2 ], s -> frame_size - 2 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame CRC mismatch \ n "); <nl> return - 1 ;
static int cbs_h2645_write_nal_unit ( CodedBitstreamContext * ctx , <nl> // Overflow but we didn ' t notice . <nl> av_assert0 ( put_bits_count (& pbc ) <= 8 * priv -> write_buffer_size ); <nl>  <nl> + if ( err < 0 ) { <nl> + // Write failed for some other reason . <nl> + return err ; <nl> + } <nl> + <nl> if ( put_bits_count (& pbc ) % 8 ) <nl> unit -> data_bit_padding = 8 - put_bits_count (& pbc ) % 8 ; <nl> else
void avcodec_pix_fmt_string ( char * buf , int buf_size , enum PixelFormat pix_fmt ) <nl> char is_alpha_char = info . is_alpha ? ' y ' : ' n '; <nl>  <nl> snprintf ( buf , buf_size , <nl> - "%- 11s " " % 1d " " % 2d " " % c ", <nl> + "%- 11s % 5d % 9d % 6c ", <nl> info . name , <nl> info . nb_channels , <nl> info . depth ,
static int64_t ism_seek ( void * opaque , int64_t offset , int whence ) <nl> os -> tail_out = NULL ; <nl> } <nl> if ( offset >= os -> cur_start_pos ) { <nl> - ffurl_seek ( os -> out , offset - os -> cur_start_pos , SEEK_SET ); <nl> + if ( os -> out ) <nl> + ffurl_seek ( os -> out , offset - os -> cur_start_pos , SEEK_SET ); <nl> os -> cur_pos = offset ; <nl> return offset ; <nl> }
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels == 0 || channels > 64 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
int av_metadata_set ( AVMetadata ** pm , const char * key , const char * value ) <nl> m -> elems [ m -> count ]. value = av_strdup ( value ); <nl> m -> count ++; <nl> } <nl> - if (! m -> count ) <nl> + if (! m -> count ) { <nl> + av_free ( m -> elems ); <nl> av_freep ( pm ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void opt_output_file ( const char * filename ) <nl> fprintf ( stderr , " Not overwriting - exiting \ n "); <nl> av_exit ( 1 ); <nl> } <nl> + while ( c != '\ n ' && c != EOF ) <nl> + c = getchar (); <nl> } <nl> else { <nl> fprintf ( stderr ," File '% s ' already exists . Exiting .\ n ", filename );
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & bc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
cmd_connect ( gchar ** args , struct cmd_help_t help ) <nl> if ( stream ){ <nl> // Limit to READ_BUF_SIZE bytes to prevent overflows in the case of a poorly chosen command <nl> account -> password = g_malloc ( READ_BUF_SIZE ); <nl> + if (! account -> password ){ <nl> + log_error (" Failed to allocate enough memory to read eval_password output "); <nl> + return TRUE ; <nl> + } <nl> account -> password = fgets ( account -> password , READ_BUF_SIZE , stream ); <nl> pclose ( stream ); <nl> } else {
otp_hotp_mac ( const unsigned char counter [ 8 ], unsigned char output [ 7 ], <nl> /* 1 . hmac */ <nl> if (! HMAC ( EVP_sha1 (), keyblock , key_len , counter , 8 , hmac , & hmac_len ) || <nl> hmac_len != 20 ) { <nl> - otp_log ( OTP_LOG_ERR , "% s : HMAC failed : HMAC ", log_prefix ); <nl> + otp_log ( OTP_LOG_ERR , "% s : HMAC failed ", log_prefix ); <nl> return - 1 ; <nl> } <nl> 
static struct cmp * cmp ; <nl> /* <nl> * Compare 2 attributes . May call the attribute compare function . <nl> */ <nl> - int paircompare ( VALUE_PAIR * request , VALUE_PAIR * check , <nl> + static int paircompare ( VALUE_PAIR * request , VALUE_PAIR * check , <nl> VALUE_PAIR * check_pairs , VALUE_PAIR ** reply_pairs ) <nl> { <nl> int ret = - 2 ;
static int dhcp_process ( REQUEST * request ) <nl> break ; <nl> } <nl>  <nl> + /* <nl> + * Releases don ' t get replies . <nl> + */ <nl> + if ( request -> packet -> code == PW_DHCP_RELEASE ) { <nl> + request -> reply -> code = 0 ; <nl> + } <nl> + <nl> return 1 ; <nl> } <nl> 
static int common_socket_parse ( CONF_SECTION * cs , rad_listen_t * this ) <nl> /* <nl> * Try IPv4 first <nl> */ <nl> + memset (& ipaddr , 0 , sizeof ( ipaddr )); <nl> ipaddr . ipaddr . ip4addr . s_addr = htonl ( INADDR_NONE ); <nl> rcode = cf_item_parse ( cs , " ipaddr ", PW_TYPE_IPADDR , <nl> & ipaddr . ipaddr . ip4addr , NULL );
static gboolean prplcb_xfer_new_send_cb ( gpointer data , gint fd , b_input_conditio <nl> /* TODO ( wilmer ): After spreading some more const goodness in BitlBee , <nl> remove the evil cast below . */ <nl> px -> ft = imcb_file_send_start ( ic , ( char *) who , xfer -> filename , xfer -> size ); <nl> + <nl> + if (! px -> ft ) { <nl> + return FALSE ; <nl> + } <nl> px -> ft -> data = px ; <nl>  <nl> px -> ft -> accept = prpl_xfer_accept ;
dissect_smpp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint32 offset = 0 ; <nl> while ( tvb_reported_length_remaining ( tvb , offset ) > 0 ) { <nl> guint16 pdu_len = tvb_get_ntohl ( tvb , offset ); <nl> + if ( pdu_len < 1 ) <nl> + THROW ( ReportedBoundsError ); <nl> gint pdu_real_len = tvb_length_remaining ( tvb , offset ); <nl> tvbuff_t * pdu_tvb ; <nl> 
proto_reg_handoff_e100 ( void ) <nl> /* Check all UDP traffic , as the specific UDP port is configurable */ <nl> heur_dissector_add (" udp ", dissect_e100 , " E100 over UDP ", " e100_udp ", proto_e100 , HEURISTIC_ENABLE ); <nl> /* e100 traffic encapsulates traffic from the ethernet frame on */ <nl> - eth_handle = find_dissector (" eth "); <nl> + eth_handle = find_dissector (" eth_withoutfcs "); <nl> } <nl>  <nl> /*
capture_start ( capture_options * capture_opts , capture_session * cap_session , void ( <nl> GString * source ; <nl>  <nl> cap_session -> state = CAPTURE_PREPARING ; <nl> + cap_session -> count = 0 ; <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_MESSAGE , " Capture Start ..."); <nl> source = get_iface_list_string ( capture_opts , IFLIST_SHOW_FILTER ); <nl> cf_set_tempfile_source (( capture_file *) cap_session -> cf , source -> str );
capture_all_filter_check_syntax_cb ( GtkWidget * w _U_ , gpointer user_data _U_ ) <nl> } <nl> # ifdef HAVE_EXTCAP <nl> /* Can ' t verify extcap capture filters */ <nl> - if ( device . if_info . extcap != NULL ) <nl> + if ( device . if_info . extcap != NULL && strlen ( device . if_info . extcap ) > 0 ) <nl> continue ; <nl> # endif <nl> filter_text = gtk_combo_box_text_get_active_text ( GTK_COMBO_BOX_TEXT ( filter_cm ));
static void dissect_HEARTBEAT_VIRTUAL ( tvbuff_t * tvb , packet_info * pinfo _U_ , gin <nl> if (!( flags & FLAG_VIRTUAL_HEARTBEAT_N )) { <nl> proto_tree_add_item ( sil_tree_writer , hf_rtps_virtual_heartbeat_num_virtual_guids , tvb , <nl> offset , 4 , little_endian ? ENC_LITTLE_ENDIAN : ENC_BIG_ENDIAN ); <nl> + num_virtual_guids = NEXT_guint32 ( tvb , offset , little_endian ); <nl> offset += 4 ; <nl> } else { <nl> num_virtual_guids = 0 ;
static void * mqtt_message_decode_copy_cb ( void * dest , const void * orig , size_t le <nl> const mqtt_message_decode_t * o = ( const mqtt_message_decode_t *) orig ; <nl> mqtt_message_decode_t * d = ( mqtt_message_decode_t *) dest ; <nl>  <nl> + d -> match_criteria = o -> match_criteria ; <nl> d -> topic_pattern = g_strdup ( o -> topic_pattern ); <nl> d -> payload_proto_name = g_strdup ( o -> payload_proto_name ); <nl> 
color_global_cb ( GtkWidget * widget _U_ , gpointer data ) <nl>  <nl> gtk_file_chooser_select_filename ( GTK_FILE_CHOOSER ( fs_widget ), path ); <nl>  <nl> - g_free (( gchar *) path ); <nl> + g_free ( path ); <nl> } <nl>  <nl> /* Import color filters */
csv_handle ( GtkTreeModel * model , GtkTreePath * path _U_ , GtkTreeIter * iter , <nl> i == PERCENT_COLUMN || i == PROTECTION_COLUMN ) { <nl> gtk_tree_model_get ( model , iter , i , & table_text , - 1 ); <nl> g_string_append ( CSV_str , table_text ); <nl> + g_free ( table_text ); <nl> } else { <nl> gtk_tree_model_get ( model , iter , i , & table_value , - 1 ); <nl> g_string_append_printf ( CSV_str , "% u ", table_value );
dissect_data_chunk ( tvbuff_t * chunk_tvb , <nl> */ <nl> if ( b_bit ) <nl> { <nl> - gboolean retval ; <nl> + gboolean retval = FALSE ; <nl>  <nl> /* <nl> * If this particular fragment happens to get a ReportedBoundsError
UAT_CSTRING_CB_DEF ( uat_ue_keys_records , rrcIntegrityKeyString , uat_ue_keys_reco <nl>  <nl> static gboolean global_pdcp_decipher_signalling = FALSE ; <nl> static gboolean global_pdcp_decipher_userplane = FALSE ; <nl> - static gboolean global_pdcp_check_integrity = FALSE ; <nl> # endif <nl> + static gboolean global_pdcp_check_integrity = FALSE ; <nl>  <nl> static const value_string direction_vals [] = <nl> {
void <nl> frame_data_reset ( frame_data * fdata ) <nl> { <nl> fdata -> flags . visited = 0 ; <nl> + fdata -> subnum = 0 ; <nl>  <nl> if ( fdata -> pfd ) { <nl> g_slist_free ( fdata -> pfd );
dissect_gtpv2_mm_context_utms_qq ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tr <nl> proto_tree_add_item ( tree , hf_gtpv2_ik , tvb , offset , 16 , ENC_NA ); <nl> offset += 16 ; <nl>  <nl> - if ( nr_qua ) <nl> - { <nl> - offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qui ); <nl> + if ( nr_qua ) { <nl> + offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qua ); <nl> } <nl>  <nl> if ( nr_qui ) {
save_decode_as_entries ( gchar ** err ) <nl>  <nl> dissector_all_tables_foreach_changed ( decode_as_write_entry , da_file ); <nl> fclose ( da_file ); <nl> + g_free ( daf_path ); <nl> return 0 ; <nl> } <nl> 
static void init_wepkeys ( void ) { <nl>  <nl> # ifdef USE_ENV <nl> buf = ep_alloc ( 128 ); <nl> - sprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> + g_snprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> tmp = getenv ( buf ); <nl> # else <nl> tmp = wep_keystr [ i ];
gboolean <nl> profile_exists ( const gchar * profilename , gboolean global ) <nl> { <nl> gchar * path = NULL , * global_path ; <nl> + if (! profilename ) <nl> + return FALSE ; <nl> if ( global ) { <nl> global_path = get_global_profiles_dir (); <nl> path = g_strdup_printf ("% s % s % s ", global_path ,
clear_node_pr ( stat_node * n ) <nl> clear_node_pr ( c ); <nl> } <nl>  <nl> - if ( n -> pr -> iter ) { <nl> + if ( n -> pr && n -> pr -> iter ) { <nl> gtk_tree_store_remove ( n -> st -> pr -> store , n -> pr -> iter ); <nl> n -> pr -> iter = NULL ; <nl> }
file_open_error_message ( int err , gboolean for_writing ) <nl> break ; <nl> # endif <nl>  <nl> + case EINVAL : <nl> + errmsg = " The file \"% s \" could not be created because an invalid filename was specified ."; <nl> + break ; <nl> + <nl> default : <nl> g_snprintf ( errmsg_errno , sizeof ( errmsg_errno ), <nl> " The file \"%% s \" could not be % s : % s .",
static void parse_outhdr_string ( const guchar * outhdr_string , gint outhdr_string_ <nl> guint d ; <nl>  <nl> /* Find digits */ <nl> - for ( ; n < outhdr_string_len ; n ++) { <nl> + for ( ; ( n < outhdr_string_len ) && ( number_digits < MAX_OUTHDR_VALUES ); n ++) { <nl> if (! g_ascii_isdigit ( outhdr_string [ n ])) { <nl> break ; <nl> }
dissect_sip_contact_item ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , gi <nl> gint current_offset ; <nl> gint queried_offset ; <nl> gint contact_params_start_offset = - 1 ; <nl> - gint contact_param_end_offset = - 1 ; <nl> + /* gint contact_param_end_offset = - 1 ;*/ <nl> uri_offset_info uri_offsets ; <nl>  <nl> /* skip Spaces and Tabs */
dissect_dcm_pdv_fragmented ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> pdv_body_len , <nl> !( pdv -> is_last_fragment )); <nl>  <nl> - if (( head && ( head -> next == NULL )) || pdv -> is_last_fragment ) { <nl> + if ( head && ( head -> next == NULL )) { <nl> /* Was not really fragmented , therefore use ' conventional ' decoding <nl> fragment_add_seq_next () won ' t add any items to the list , when last fragment only <nl> */
ssl_association_remove ( GTree * associations , SslAssociation * assoc ) <nl> if ( assoc -> handle ) <nl> dissector_delete (( assoc -> tcp )?" tcp . port ":" udp . port ", assoc -> ssl_port , assoc -> handle ); <nl>  <nl> + g_free ( assoc -> info ); <nl> + <nl> g_tree_remove ( associations , assoc ); <nl> g_free ( assoc ); <nl> }
epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_da <nl> register_cb cb , <nl> gpointer client_data ) <nl> { <nl> - gboolean status = TRUE ; <nl> + volatile gboolean status = TRUE ; <nl>  <nl> /* initialize memory allocation subsystem */ <nl> wmem_init ();
fill_list ( GtkWidget * main_w ) <nl> * and use it later without any crashes . This may not be a <nl> * valid assumption . <nl> */ <nl> + g_free ( l_select ); <nl> l_select = ( GtkTreeIter *) g_memdup (& iter , sizeof ( iter )); <nl> } <nl> fl_entry = g_list_next ( fl_entry );
Dot11DecryptDecryptWPABroadcastKey ( const EAPOL_RSN_KEY * pEAPKey , guint8 * decrypt <nl> DEBUG_DUMP (" FullDecrKey :", new_key , 32 ); <nl>  <nl> if ( gcry_cipher_open (& rc4_handle , GCRY_CIPHER_ARCFOUR , GCRY_CIPHER_MODE_STREAM , 0 )) { <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> if ( gcry_cipher_setkey ( rc4_handle , new_key , sizeof ( new_key ))) { <nl> gcry_cipher_close ( rc4_handle ); <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> 
proto_register_sua ( void ) <nl> " This may affect TCAP ' s ability to recognize which messages belong to which TCAP session .", & set_addresses ); <nl>  <nl> heur_subdissector_list = register_heur_dissector_list (" sua "); <nl> - sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_NOT_ALLOW_DUPLICATE ); <nl> + sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_ALLOW_DUPLICATE ); <nl> sua_tap = register_tap (" sua "); <nl>  <nl> assocs = wmem_tree_new_autoreset ( wmem_epan_scope (), wmem_file_scope ());
void OverlayScrollBar :: paintEvent ( QPaintEvent * event ) <nl> pm_painter . setPen ( border_color ); <nl> pm_painter . drawLine ( near_dest . topLeft (), near_dest . bottomLeft ()); <nl> pm_painter . drawLine ( near_dest . topRight (), near_dest . bottomRight ()); <nl> + pm_painter . drawLine ( near_dest . bottomLeft (), near_dest . bottomRight ()); <nl> pm_painter . restore (); <nl>  <nl> // Draw the map .
typedef struct _ext_toolbar_update_list_t <nl> GList * entries ; <nl> } ext_toolbar_update_list_t ; <nl>  <nl> - extern gint <nl> + static gint <nl> ext_toolbar_find_item ( gconstpointer a , gconstpointer b ) <nl> { <nl> if ( a == 0 || b == 0 )
static int decode_bgp_link_nlri_prefix_descriptors ( tvbuff_t * tvb , <nl> break ; <nl>  <nl> case BGP_NLRI_TLV_IP_REACHABILITY_INFORMATION : <nl> - decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> - tvb , offset + 4 , 0 , " Reachability "); <nl> + if ( decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> + tvb , offset + 4 , 0 , " Reachability ") == - 1 ) <nl> + return diss_length ; <nl> break ; <nl> } <nl> 
dissect_esp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> /* Copy back the Authentication which was not encrypted */ <nl> if ( decrypted_len >= esp_auth_len ) <nl> { <nl> - tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , sizeof ( struct newesp )+ decrypted_len - esp_auth_len , esp_auth_len ); <nl> + tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , ( gint )( sizeof ( struct newesp )+ decrypted_len - esp_auth_len ), esp_auth_len ); <nl> } <nl>  <nl> /* Decryption has finished */
rtp_streams_stat_draw ( void * arg _U_ ) <nl>  <nl> list = g_list_next ( list ); <nl>  <nl> - g_free ( payload_type ); <nl> wmem_free ( NULL , src_addr ); <nl> wmem_free ( NULL , dst_addr ); <nl> wmem_free ( NULL , payload_type );
do_simple_message_box ( ESD_TYPE_E type , gboolean * notagain , <nl> if ( notagain != NULL ) { <nl> checkbox = gtk_check_button_new_with_label (" Don ' t show this message again ."); <nl> gtk_container_set_border_width ( GTK_CONTAINER ( checkbox ), 12 ); <nl> - gtk_box_pack_start ( GTK_BOX ( gtk_message_dialog_get_message_area ( GTK_MESSAGE_DIALOG ( msg_dialog ))), checkbox , <nl> - TRUE , TRUE , 0 ); <nl> + gtk_box_pack_start ( GTK_BOX ( gtk_dialog_get_content_area ( GTK_DIALOG ( msg_dialog ))), <nl> + checkbox , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( checkbox ); <nl> } <nl> 
static nghttp2_hd_entry * add_hd_table_incremental ( nghttp2_hd_context * context , <nl>  <nl> if ( rv != 0 ) { <nl> -- new_ent -> ref ; <nl> + <nl> + /* nv -> name and nv -> value are managed by caller . */ <nl> + new_ent -> nv . name = NULL ; <nl> + new_ent -> nv . namelen = 0 ; <nl> + new_ent -> nv . value = NULL ; <nl> + new_ent -> nv . valuelen = 0 ; <nl> + <nl> nghttp2_hd_entry_free ( new_ent ); <nl> free ( new_ent ); <nl> 
const int custom_occurrence_col_ = 4 ; <nl> ColumnPreferencesFrame :: ColumnPreferencesFrame ( QWidget * parent ) : <nl> QFrame ( parent ), <nl> ui ( new Ui :: ColumnPreferencesFrame ), <nl> + cur_column_ ( 0 ), <nl> cur_line_edit_ ( NULL ), <nl> cur_combo_box_ ( NULL ), <nl> + saved_combo_idx_ ( 0 ), <nl> saved_custom_combo_idx_ (- 1 ) <nl> { <nl> ui -> setupUi ( this );
extcap_register_preferences_callback ( gpointer key , gpointer value _U_ , gpointer <nl>  <nl> void extcap_register_preferences ( void ) <nl> { <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> module_t * dev_module = prefs_find_module (" extcap "); <nl>  <nl> if (! dev_module ) <nl> extcap_load_interface_list ( void ) <nl> gchar * argv ; <nl> gchar * error ; <nl>  <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> if ( _toolbars ) <nl> { <nl> // Remove existing interface toolbars here instead of in extcap_clear_interfaces ()
decode_pdu_sns_delete ( build_info_t * bi ) { <nl> { NSIP_IE_IP4_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> { NSIP_IE_IP6_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> }; <nl> - decode_iei_transaction_id ( ies , bi , bi -> offset ); <nl> - decode_pdu_general (& ies [ 1 ], 3 , bi ); <nl> + decode_pdu_general ( ies , 1 , bi ); <nl> + decode_iei_transaction_id (& ies [ 1 ], bi , bi -> offset ); <nl> + decode_pdu_general (& ies [ 2 ], 3 , bi ); <nl> } <nl>  <nl> static void
ws_pipe_wait_for_pipe ( HANDLE * pipe_handles , int num_pipe_handles , HANDLE pid ) <nl> int num_waiting_to_connect = 0 ; <nl> int num_handles = num_pipe_handles + 1 ; // PID handle is also added to list of handles . <nl>  <nl> + SecureZeroMemory ( pipeinsts , sizeof ( pipeinsts )); <nl> + <nl> if ( num_pipe_handles == 0 || num_pipe_handles > 3 ) <nl> { <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_DEBUG , " Invalid number of pipes given as argument .");
dissect_qnet6 ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * dat <nl> /* <nl> * data after header <nl> */ <nl> - if ( cklen != 0 ) <nl> + if ( cklen > 0 ) <nl> { <nl> crc = crc32_mpeg2_seed ( tvb_get_ptr ( tvb , 36 + 2 , cklen ), cklen , ~ crc ); <nl> crc = ~ crc ;
nextcontext : <nl> tvb_previous_offset = tvb_find_guint8 ( tvb , tvb_current_offset , <nl> tvb_len , '=')+ 1 ; <nl> tvb_previous_offset = tvb_skip_wsp ( tvb , tvb_previous_offset ); <nl> - tvb_current_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> + tvb_next_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> tvb_len , '{'); <nl> + if ( tvb_current_offset >= tvb_next_offset ) { <nl> + proto_tree_add_text ( megaco_tree , tvb , 0 , 0 , "[ Parse error : Invalid offset ]"); <nl> + return ; <nl> + } <nl> + tvb_current_offset = tvb_next_offset ; <nl>  <nl>  <nl> tokenlen = tvb_current_offset - tvb_previous_offset ;
void PacketList :: columnsChanged () <nl> setColumnVisibility (); <nl> create_far_overlay_ = true ; <nl> packet_list_model_ -> resetColumns (); <nl> + applyRecentColumnWidths (); <nl> columns_changed_ = false ; <nl> } <nl>  <nl> void PacketList :: setCaptureFile ( capture_file * cf ) <nl> cap_file_ = cf ; <nl> if ( cap_file_ && columns_changed_ ) { <nl> columnsChanged (); <nl> - applyRecentColumnWidths (); <nl> } <nl> packet_list_model_ -> setCaptureFile ( cf ); <nl> create_near_overlay_ = true ;
private Q_SLOTS : <nl> }; <nl>  <nl> Q_DECLARE_METATYPE ( ExtcapArgument ) <nl> + Q_DECLARE_METATYPE ( ExtcapArgument *) <nl>  <nl> class ExtArgText : public ExtcapArgument <nl> {
write_recent ( void ) <nl> g_free ( rf_path ); <nl> return FALSE ; <nl> } <nl> + g_free ( rf_path ); <nl>  <nl> fputs ("# Recent settings file for Wireshark " VERSION ".\ n " <nl> "#\ n "
dissect_rtp_hext_rfc5215_onebyte ( tvbuff_t * tvb , packet_info * pinfo , <nl> return ; <nl>  <nl> ext_length = ( ext_hdr_hdr & 0x0F ) + 1 ; <nl> + <nl> + /* Exit on malformed extension headers */ <nl> + if ( ext_offset + ext_length + 1 > tvb_captured_length ( tvb )) { <nl> + return ; <nl> + } <nl> + <nl> if ( rtp_hext_tree ) { <nl> rtp_hext_rfc5285_tree = proto_tree_add_subtree ( rtp_hext_tree , tvb , ext_offset , ext_length + 1 , <nl> ett_hdr_ext_rfc5285 , NULL , " RFC 5285 Header Extension ( One - Byte Header )");
dissect_dcerpc_cn_bs_body ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> * it was just too short to tell and ask the TCP layer for more <nl> * data . */ <nl> pinfo -> desegment_offset = offset ; <nl> - pinfo -> desegment_len = sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset ); <nl> + pinfo -> desegment_len = ( guint32 )( sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset )); <nl> } else { <nl> /* Really not DCE - RPC */ <nl> break ;
void MulticastStatisticsDialog :: updateMulticastParameters () <nl>  <nl> param = buffer_alarm_threshold_le_ -> text (). toInt (& ok ); <nl> if ( ok && param > 0 ) { <nl> - mcast_stream_trigger = param ; <nl> + mcast_stream_bufferalarm = param ; <nl> } <nl>  <nl> param = stream_empty_speed_le_ -> text (). toInt (& ok );
pcapng_read_section_header_block ( FILE_T fh , pcapng_block_header_t * bh , <nl> bytes_read = pcapng_read_option ( fh , pn , & oh , option_content , opt_cont_buf_len , to_read , err , err_info , " section_header "); <nl> if ( bytes_read <= 0 ) { <nl> pcapng_debug (" pcapng_read_section_header_block : failed to read option "); <nl> + g_free ( option_content ); <nl> return PCAPNG_BLOCK_ERROR ; <nl> } <nl> to_read -= bytes_read ;
int ieee80211_radiotap_iterator_init ( <nl>  <nl> /* find payload start allowing for extended bitmap ( s ) */ <nl> if ( iterator -> _bitmap_shifter & ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> + if (! ITERATOR_VALID ( iterator , sizeof ( guint32 ))) <nl> + return - EINVAL ; <nl> while ( get_unaligned_le32 ( iterator -> _arg ) & <nl> ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> iterator -> _arg += sizeof ( guint32 );
ZEND_API int zend_register_constant ( zend_constant * c ELS_DC ) <nl>  <nl> zend_str_tolower ( lowercase_name , c -> name_len ); <nl> if ( zend_hash_add ( EG ( zend_constants ), lowercase_name , c -> name_len , ( void *) c , sizeof ( zend_constant ), NULL )== FAILURE ) { <nl> + free ( c -> name ); <nl> zval_dtor (& c -> value ); <nl> zend_error ( E_NOTICE ," Constant % s already defined ", lowercase_name ); <nl> ret = FAILURE ;
ZEND_API int zend_hash_update_current_key_ex ( HashTable * ht , int key_type , const <nl> p -> h = num_index ; <nl> } else { <nl> p -> h = h ; <nl> + p -> nKeyLength = str_length ; <nl> if ( IS_INTERNED ( str_index )) { <nl> p -> arKey = str_index ; <nl> } else {
PHPAPI void php_fgetcsv ( php_stream * stream , /* {{{ */ <nl> /* Types converted , free storage */ <nl> efree ( delim ); <nl> efree ( enc ); <nl> - efree ( buffer ); <nl> } else { <nl> /* Binary stream with binary delimiter / enclosures / prefetch */ <nl> php_fgetcsv_ex ( stream , delim , delim_len , enc , enc_len , "\\", 1 , buffer , buffer_len , return_value TSRMLS_CC );
static php_iconv_err_t _php_iconv_strpos ( unsigned int * pretval , <nl> ndl_buf_left -= GENERIC_SUPERSET_NBYTES ; <nl> if ( ndl_buf_left == 0 ) { <nl> * pretval = match_ofs ; <nl> + ndl_buf_p = ndl_buf ; <nl> + ndl_buf_left = ndl_buf_len ; <nl> + match_ofs = - 1 ; <nl> } <nl> } else { <nl> unsigned int i , j , lim ;
static php_stream * php_stream_url_wrap_rfc2397 ( php_stream_wrapper * wrapper , cha <nl> ts -> mode = mode && mode [ 0 ] == ' r ' ? TEMP_STREAM_READONLY : 0 ; <nl> ts -> meta = meta ; <nl> } <nl> + efree ( comma ); <nl>  <nl> return stream ; <nl> }
PHP_FUNCTION ( str_repeat ) <nl> int l = 0 ; <nl> memcpy ( result , input_str , input_str_len ); <nl> s = result ; <nl> - e = result + input_str_len ; <nl> - ee = result + result_len ; <nl> + e = ( char *) result + input_str_len ; <nl> + ee = ( char *) result + result_len ; <nl>  <nl> while ( e < ee ) { <nl> l = ( e - s ) < ( ee - e ) ? ( e - s ) : ( ee - e );
PHP_FILEINFO_API zend_object_value finfo_objects_new ( zend_class_entry * class_typ <nl> intern = ecalloc ( 1 , sizeof ( struct finfo_object )); <nl> intern -> zo . ce = class_type ; <nl> intern -> zo . properties = NULL ; <nl> -# if ZEND_EXTENSION_API_NO > 220050000 <nl> +# if ZEND_MODULE_API_NO >= 20050922 <nl> intern -> zo . guards = NULL ; <nl> # else <nl> intern -> zo . in_get = 0 ;
static int create_segments ( size_t requested_size , zend_shared_segment *** shared_ <nl> /* creating segment here */ <nl> * shared_segments_count = 1 ; <nl> * shared_segments_p = ( zend_shared_segment **) calloc ( 1 , sizeof ( zend_shared_segment )+ sizeof ( void *)); <nl> + if (!* shared_segments_p ) { <nl> + zend_win_error_message ( ACCEL_LOG_FATAL , " calloc () failed "); <nl> + * error_in = " calloc "; <nl> + return ALLOC_FAILURE ; <nl> + } <nl> shared_segment = ( zend_shared_segment *)(( char *)(* shared_segments_p ) + sizeof ( void *)); <nl> (* shared_segments_p )[ 0 ] = shared_segment ; <nl> 
PS_READ_FUNC ( files ) <nl> return FAILURE ; <nl>  <nl> data -> st_size = * vallen = sbuf . st_size ; <nl> + <nl> + if ( sbuf . st_size == 0 ) { <nl> + * val = STR_EMPTY_ALLOC (); <nl> + return SUCCESS ; <nl> + } <nl> + <nl> * val = emalloc ( sbuf . st_size ); <nl>  <nl> # if defined ( HAVE_PREAD )
ZEND_METHOD ( reflection_class , isSubclassOf ) <nl> case IS_UNICODE : <nl> if ( zend_u_lookup_class ( Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name ), Z_UNILEN_P ( class_name ), & pce TSRMLS_CC ) == FAILURE ) { <nl> zend_throw_exception_ex ( reflection_exception_ptr , 0 TSRMLS_CC , <nl> - " Interface % R does not exist ", Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name )); <nl> + " Class % R does not exist ", Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name )); <nl> return ; <nl> } <nl> class_ce = * pce ;
PHP_FUNCTION ( simplexml_import_dom ) <nl>  <nl> if ( object -> node && object -> node -> node ) { <nl> nodep = object -> node -> node ; <nl> + if ( nodep -> doc == NULL ) { <nl> + php_error ( E_WARNING , " Imported Node must have associated Document "); <nl> + RETURN_NULL (); <nl> + } <nl> if ( nodep -> type == XML_DOCUMENT_NODE || nodep -> type == XML_HTML_DOCUMENT_NODE ) { <nl> nodep = xmlDocGetRootElement (( xmlDocPtr ) nodep ); <nl> }
PHPAPI int _php_stream_copy_to_stream_ex ( php_stream * src , php_stream * dest , size <nl>  <nl> * len = didwrite ; <nl>  <nl> - /* read bytes match written */ <nl> - if ( mapped == didwrite ) { <nl> + /* we ' ve got at least 1 byte to read <nl> + * less than 1 is an error <nl> + * AND read bytes match written */ <nl> + if ( mapped > 0 && mapped == didwrite ) { <nl> return SUCCESS ; <nl> } <nl> return FAILURE ;
static void _free_mysql_result ( zend_rsrc_list_entry * rsrc TSRMLS_DC ) <nl> MYSQL_RES * mysql_result = ( MYSQL_RES *) rsrc -> ptr ; <nl>  <nl> mysql_free_result ( mysql_result ); <nl> + MySG ( result_allocated )--; <nl> } <nl> /* }}} */ <nl> 
int zend_register_functions ( zend_class_entry * scope , zend_function_entry * functi <nl> char * lowercase_name ; <nl> int fname_len ; <nl>  <nl> + memset ( internal_function , 0 , sizeof ( zend_function )); <nl> if ( type == MODULE_PERSISTENT ) { <nl> error_type = E_CORE_WARNING ; <nl> } else {
static void ps_files_open ( ps_files * data , const char * key ) <nl> data -> basedir = NULL ; <nl> data -> basedir_len = 0 ; <nl> } <nl> + efree ( data ); <nl> php_error_docref ( NULL , E_WARNING , " The session id is too long or contains illegal characters , valid characters are a - z , A - Z , 0 - 9 and '-,'"); <nl> return ; <nl> }
ZEND_API int zend_restore_ini_entry ( char * name , uint name_length , int stage ) /* <nl> } <nl>  <nl> if ( EG ( modified_ini_directives )) { <nl> - zend_restore_ini_entry_cb ( ini_entry , stage TSRMLS_CC ); <nl> - zend_hash_del ( EG ( modified_ini_directives ), name , name_length ); <nl> + if ( zend_restore_ini_entry_cb ( ini_entry , stage TSRMLS_CC ) == 0 ) { <nl> + zend_hash_del ( EG ( modified_ini_directives ), name , name_length ); <nl> + } else { <nl> + return FAILURE ; <nl> + } <nl> } <nl>  <nl> return SUCCESS ;
typedef int32_t zend_off_t ; <nl> # define ZEND_STRTOUL ( s0 , s1 , base ) strtoull (( s0 ), ( s1 ), ( base )) <nl> # define ZEND_STRTOL_PTR strtoll <nl> # define ZEND_STRTOUL_PTR strtoull <nl> -# define ZEND_ABS llabs <nl> +# define ZEND_ABS imaxabs <nl> # endif <nl> # else <nl> # define ZEND_STRTOL ( s0 , s1 , base ) strtol (( s0 ), ( s1 ), ( base ))
apprentice_map ( struct magic_set * ms , const char * fn ) <nl> if ( dbname == NULL ) <nl> goto error ; <nl>  <nl> - stream = php_stream_open_wrapper (( char *) fn , " rb ", REPORT_ERRORS , NULL ); <nl> + stream = php_stream_open_wrapper (( char *) fn , " rb ", REPORT_ERRORS , NULL ); <nl>  <nl> if (! stream ) { <nl> goto error ;
PHP_FUNCTION ( fd_set ) <nl> FD_SET ( fd , & readfd ); <nl> if ( fd > max_fd ) max_fd = fd ; <nl> } <nl> + efree ( args ); <nl> } <nl> RETURN_LONG ( 1 ); <nl> }
PHPAPI php_stream_filter * php_stream_filter_create ( const char * filtername , zval <nl> /* try a wildcard */ <nl> char * wildname ; <nl>  <nl> - wildname = estrdup ( filtername ); <nl> + wildname = emalloc ( n + 3 ); <nl> + memcpy ( wildname , filtername , n + 1 ); <nl> period = wildname + ( period - filtername ); <nl> while ( period && ! filter ) { <nl> * period = '\ 0 ';
PHPAPI size_t php_strip_tags ( char * rbuf , int len , int * stateptr , char * allow , in <nl>  <nl> while ( i < len ) { <nl> switch ( c ) { <nl> + case '\ 0 ': <nl> + break ; <nl> case '<': <nl> if ( isspace (*( p + 1 ))) { <nl> goto reg_char ;
static int php_cli_server_poller_iter_on_active ( php_cli_server_poller * poller , v <nl> SOCKET fd ; <nl> int events ; <nl> } entries [ FD_SETSIZE * 2 ]; <nl> - php_socket_t fd = 0 ; <nl> size_t i ; <nl> struct socket_entry * n = entries , * m ; <nl> 
PHPAPI extern const char php_sig_gif [ 3 ]; <nl> PHPAPI extern const char php_sig_jpg [ 3 ]; <nl> PHPAPI extern const char php_sig_png [ 3 ]; <nl> - PHPAPI const char php_sig_gd2 [ 3 ] = {' g ', ' d ', ' 2 '}; <nl> + PHPAPI extern const char php_sig_gd2 [ 3 ] = {' g ', ' d ', ' 2 '}; <nl>  <nl> extern zend_module_entry gd_module_entry ; <nl> # define phpext_gd_ptr & gd_module_entry
void php_mysqli_fetch_into_hash ( INTERNAL_FUNCTION_PARAMETERS , int override_flags <nl> { <nl> MYSQL_RES * result ; <nl> zval * mysql_result ; <nl> - int fetchtype ; <nl> + long fetchtype ; <nl> unsigned int i ; <nl> MYSQL_FIELD * fields ; <nl> MYSQL_ROW row ;
static inline zval * zend_assign_to_variable ( zval * variable_ptr , zval * value TSRM <nl> value = Z_REFVAL_P ( value ); <nl> } <nl> if ( Z_REFCOUNTED_P ( value )) { <nl> + if ( UNEXPECTED ( variable_ptr == value )) { <nl> + return variable_ptr ; <nl> + } <nl> Z_ADDREF_P ( value ); <nl> } <nl> }
ZEND_API zend_mm_heap * zend_mm_startup ( void ) <nl> if ( zend_mm_low_bit ( seg_size ) != zend_mm_high_bit ( seg_size )) { <nl> fprintf ( stderr , " ZEND_MM_SEG_SIZE must be a power of two \ n "); <nl> exit ( 255 ); <nl> + } else if ( seg_size < ZEND_MM_ALIGNED_SEGMENT_SIZE + ZEND_MM_ALIGNED_HEADER_SIZE ) { <nl> + fprintf ( stderr , " ZEND_MM_SEG_SIZE is too small \ n "); <nl> + exit ( 255 ); <nl> } <nl> } else { <nl> seg_size = ZEND_MM_SEG_SIZE ;
static inline int php_tcp_sockop_connect ( php_stream * stream , php_netstream_data_ <nl> if ( xparam -> want_errortext ) { <nl> spprintf (& xparam -> outputs . error_text , 0 , " local_addr context option is not a string ."); <nl> } <nl> + efree ( host ); <nl> return - 1 ; <nl> } <nl> bindto = parse_ip_address_ex ( Z_STRVAL_PP ( tmpzval ), Z_STRLEN_PP ( tmpzval ), & bindport , xparam -> want_errortext , & xparam -> outputs . error_text TSRMLS_CC );
static PHP_INI_MH ( OnTypeLibFileUpdate ) <nl> char * strtok_buf = NULL ; <nl> int cached ; <nl>  <nl> - if (! new_value || ( typelib_file = VCWD_FOPEN ( new_value , " r "))== NULL ) { <nl> + if (! new_value || ! new_value [ 0 ] || ( typelib_file = VCWD_FOPEN ( new_value , " r "))== NULL ) { <nl> return FAILURE ; <nl> } <nl> 
SPL_METHOD ( SplFileInfo , getExtension ) <nl>  <nl> p = zend_memrchr ( ret -> val , '.', ret -> len ); <nl> if ( p ) { <nl> - idx = p - fname ; <nl> + idx = p - ret -> val ; <nl> RETVAL_STRINGL ( ret -> val + idx + 1 , ret -> len - idx - 1 ); <nl> STR_RELEASE ( ret ); <nl> return ;
static int php_zip_extract_file ( struct zip * za , char * dest , char * file , int fil <nl> * safemode status as its parent folder ? <nl> */ <nl> if ( OPENBASEDIR_CHECKPATH ( fullpath )) { <nl> + efree ( fullpath ); <nl> efree ( file_dirname_fullpath ); <nl> efree ( file_basename ); <nl> return 0 ;
static inline int _php_stream_path_param_encode ( zval ** ppzval , char ** ppath , int <nl> if ( FAILURE == php_stream_path_encode ( NULL , & path , & path_len , Z_USTRVAL_PP ( ppzval ), Z_USTRLEN_PP ( ppzval ), options , context )) { <nl> return FAILURE ; <nl> } <nl> + Z_ADDREF_PP ( ppzval ); /* the conversion removes a refcount */ <nl> MAKE_STD_ZVAL ( zpath ); <nl> ZVAL_STRINGL ( zpath , path , path_len , 0 ); <nl> Z_UNSET_ISREF_P ( zpath );
PHPAPI void php_pcre_match_impl ( pcre_cache_entry * pce , char * subject , int subjec <nl> if ( pcre_get_substring_list ( subject , offsets , count , & stringlist ) < 0 ) { <nl> efree ( subpat_names ); <nl> efree ( offsets ); <nl> + if ( match_sets ) efree ( match_sets ); <nl> php_error_docref ( NULL TSRMLS_CC , E_WARNING , " Get subpatterns list failed "); <nl> RETURN_FALSE ; <nl> }
PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl>  <nl> if ( php_check_open_basedir ( path_copy TSRMLS_CC )) { <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> return NULL ; <nl> } <nl> PHP_BZ2_API php_stream * _php_stream_bz2open ( php_stream_wrapper * wrapper , <nl> * opened_path = estrdup ( path_copy ); <nl> } <nl> # ifdef VIRTUAL_DIR <nl> - efree ( path_copy ); <nl> + free ( path_copy ); <nl> # endif <nl> path_copy = NULL ; <nl> 
static HashTable * zend_closure_get_debug_info ( zval * object , int * is_temp TSRMLS_ <nl> } <nl> info_len = zend_spprintf (& info , 0 , "% s ", <nl> i >= required ? "< optional >" : "< required >"); <nl> - add_assoc_stringl_ex (& val , name , name_len , info , info_len , 0 ); <nl> +//??? TODO : avoid reallocation <nl> + add_assoc_stringl_ex (& val , name , name_len , info , info_len , 1 ); <nl> + efree ( info ); <nl> efree ( name ); <nl> arg_info ++; <nl> }
struct _php_stream { <nl> char * orig_path ; <nl>  <nl> zend_resource * ctx ; <nl> - int flags ; /* PHP_STREAM_FLAG_XXX */ <nl> + uint32_t flags ; /* PHP_STREAM_FLAG_XXX */ <nl>  <nl> int eof ; <nl> 
PHPAPI void php_pcre_match_impl ( pcre_cache_entry * pce , char * subject , int subjec <nl> if ( pcre_get_substring_list ( subject , offsets , count , & stringlist ) < 0 ) { <nl> efree ( subpat_names ); <nl> efree ( offsets ); <nl> + if ( match_sets ) efree ( match_sets ); <nl> php_error_docref ( NULL TSRMLS_CC , E_WARNING , " Get subpatterns list failed "); <nl> RETURN_FALSE ; <nl> }
static DWORD tls_key ; <nl>  <nl> # elif defined ( BETHREADS ) <nl> static int32 tls_key ; <nl> -# define tsrm_tls_set ( what ) tls_set ( tls_key , ( void *)( what ) <nl> +# define tsrm_tls_set ( what ) tls_set ( tls_key , ( void *)( what )) <nl> # define tsrm_tls_get () ( tsrm_tls_entry *) tls_get ( tls_key ) <nl>  <nl> # else
# define _FTP_H <nl>  <nl> # include < stdio . h > <nl> -# if HAVE_UINSTD_H <nl> +# if ! PHP_WIN32 <nl> # include < netinet / in . h > <nl> # endif <nl> 
PHP_FUNCTION ( file_put_contents ) <nl> RETURN_FALSE ; <nl> } <nl> switch ( Z_TYPE_P ( data )) { <nl> + case IS_RESOURCE : <nl> + { <nl> + php_stream * srcstream ; <nl> + php_stream_from_zval ( srcstream , & data ); <nl> + <nl> + numbytes = php_stream_copy_to_stream ( srcstream , stream , PHP_STREAM_COPY_ALL ); <nl> + <nl> + break ; <nl> + } <nl> case IS_NULL : <nl> case IS_LONG : <nl> case IS_DOUBLE :
PHP_FUNCTION ( odbc_execute ) <nl>  <nl> /* Check for safe mode . */ <nl> if ( PG ( safe_mode ) && (! php_checkuid ( filename , NULL , CHECKUID_CHECK_FILE_AND_DIR ))) { <nl> - RETURN_FALSE ; <nl> - } <nl> + efree ( filename ); <nl> + efree ( params ); <nl> + RETURN_FALSE ; <nl> + } <nl>  <nl> /* Check the basedir */ <nl> if ( php_check_open_basedir ( filename TSRMLS_CC )) { <nl> + efree ( filename ); <nl> + efree ( params ); <nl> RETURN_FALSE ; <nl> } <nl> 
static zend_object * spl_filesystem_object_clone ( zval * zobject TSRMLS_DC ) <nl> old_object = Z_OBJ_P ( zobject ); <nl> source = ( spl_filesystem_object *) old_object ; <nl> new_object = spl_filesystem_object_new_ex ( old_object -> ce TSRMLS_CC ); <nl> + intern = ( spl_filesystem_object *) new_object ; <nl>  <nl> intern -> flags = source -> flags ; <nl> 
export_desktop_file ( const char * app , <nl> g_string_append_printf ( new_exec , " ", arg ); <nl> else if ( strcasecmp ( arg , "% u ") == 0 ) <nl> g_string_append_printf ( new_exec , " ", arg ); <nl> - else if ( strcmp ( arg , " u ") == 0 ) <nl> + else if ( g_str_has_prefix ( arg , "@@")) <nl> g_print ( _ (" Skipping invalid Exec argument % s \ n "), arg ); <nl> else <nl> g_string_append_printf ( new_exec , " % s ", arg );
flatpak_installation_drop_caches ( FlatpakInstallation * self , <nl> { <nl> priv -> dir_unlocked = clone ; <nl> g_object_unref ( old ); <nl> + res = TRUE ; <nl> } <nl>  <nl> G_UNLOCK ( dir );
setup_seccomp ( FlatpakBwrap * bwrap , <nl>  <nl> /* Don ' t allow subnamespace setups : */ <nl> { SCMP_SYS ( unshare ), EPERM }, <nl> + { SCMP_SYS ( setns ), EPERM }, <nl> { SCMP_SYS ( mount ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ )
flatpak_run_add_environment_args ( GPtrArray * argv_array , <nl> "/ dev / dri ", <nl> /* mali */ <nl> "/ dev / mali ", <nl> + "/ dev / mali0 ", <nl> "/ dev / umplock ", <nl> /* nvidia */ <nl> "/ dev / nvidiactl ",
xdg_app_dir_update_appstream ( XdgAppDir * self , <nl> if (! ostree_repo_resolve_rev ( self -> repo , remote_and_branch , TRUE , & new_checksum , error )) <nl> return FALSE ; <nl>  <nl> + if ( new_checksum == NULL ) <nl> + { <nl> + g_warning (" No appstream branch in remote % s \ n ", remote ); <nl> + return TRUE ; <nl> + } <nl> + <nl> appstream_dir = g_file_get_child ( xdg_app_dir_get_path ( self ), " appstream "); <nl> remote_dir = g_file_get_child ( appstream_dir , remote ); <nl> arch_dir = g_file_get_child ( remote_dir , arch );
setup_seccomp ( FlatpakBwrap * bwrap , <nl> { SCMP_SYS ( umount ), EPERM }, <nl> { SCMP_SYS ( umount2 ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> + { SCMP_SYS ( chroot ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ ) <nl> /* Architectures with CONFIG_CLONE_BACKWARDS2 : the child stack <nl> * and flags arguments are reversed so the flags come second */
flatpak_number_prompt ( int min , int max , const char * prompt , ...) <nl>  <nl> va_start ( var_args , prompt ); <nl> s = g_strdup_vprintf ( prompt , var_args ); <nl> + va_end ( var_args ); <nl>  <nl> while ( TRUE ) <nl> {
setup_seccomp ( FlatpakBwrap * bwrap , <nl> { SCMP_SYS ( unshare ), EPERM }, <nl> { SCMP_SYS ( setns ), EPERM }, <nl> { SCMP_SYS ( mount ), EPERM }, <nl> + { SCMP_SYS ( umount ), EPERM }, <nl> + { SCMP_SYS ( umount2 ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ ) <nl> /* Architectures with CONFIG_CLONE_BACKWARDS2 : the child stack
add_related ( GHashTable * all_refs , <nl> g_hash_table_insert ( all_refs , g_steal_pointer (& ext_collection_ref ), c_s ); <nl> } <nl>  <nl> + g_list_free_full ( extensions , ( GDestroyNotify ) flatpak_extension_free ); <nl> + <nl> return TRUE ; <nl> } <nl> 
xdp_fuse_init ( GError ** error ) <nl>  <nl> path = xdp_fuse_get_mountpoint (); <nl> if (( stat ( path , & st ) == - 1 && errno == ENOTCONN ) || <nl> - (( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN || <nl> + ((( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN ) || <nl> ( statfs_res == 0 && stfs . f_type == 0x65735546 /* fuse */))) <nl> { <nl> int count ;
static void f_synIDattr ( typval_T * argvars , typval_T * rettv ) <nl> modec = TOLOWER_ASC ( mode [ 0 ]); <nl> if ( modec != ' c ' && modec != ' g ') <nl> modec = 0 ; /* replace invalid with current */ <nl> + } else if ( ui_rgb_attached ()) { <nl> + modec = ' g '; <nl> } else { <nl> modec = ' c '; <nl> }
static void qf_free_stack ( win_T * wp , qf_info_T * qi ) <nl> // If the location list window is open , then create a new empty location <nl> // list <nl> qf_info_T * new_ll = ll_new_list (); <nl> + <nl> + // first free the list reference in the location list window <nl> + ll_free_all (& orig_wp -> w_llist_ref ); <nl> + <nl> orig_wp -> w_llist_ref = new_ll ; <nl> if ( llwin != NULL ) { <nl> llwin -> w_llist = new_ll ;
static inline void add_search_pattern ( PossiblyFreedShadaEntry * const ret_pse , <nl> ? defaults . data . search_pattern . place_cursor_at_end <nl> : pat . off . end ), <nl> . offset = ( is_substitute_pattern <nl> - ? pat . off . off <nl> - : defaults . data . search_pattern . offset ), <nl> + ? defaults . data . search_pattern . offset <nl> + : pat . off . off ), <nl> . is_last_used = ( is_substitute_pattern ^ search_last_used ), <nl> . is_substitute_pattern = is_substitute_pattern , <nl> . highlighted = (( is_substitute_pattern ^ search_last_used )
 <nl> # include " nvim / api / private / defs . h " <nl> # include " nvim / func_attr . h " <nl> +# include " nvim / eval / typval . h " <nl> +# include " nvim / ex_cmds_defs . h " <nl>  <nl> // Generated by msgpack - gen . lua <nl> void nlua_add_api_functions ( lua_State * lstate ) REAL_FATTR_NONNULL_ALL ;
BOOL CloseHandle ( HANDLE hObject ) <nl> if ( pipe -> serverfd != - 1 ) <nl> close ( pipe -> serverfd ); <nl>  <nl> - free ( Object ); <nl> + free ( pipe -> lpFileName ); <nl> + free ( pipe -> lpFilePath ); <nl> + free ( pipe -> name ); <nl> + free ( pipe ); <nl>  <nl> return TRUE ; <nl> }
char * crypto_print_name ( X509_NAME * name ) <nl> if ( X509_NAME_print_ex ( outBIO , name , 0 , XN_FLAG_ONELINE ) > 0 ) <nl> { <nl> unsigned long size = BIO_number_written ( outBIO ); <nl> - buffer = xzalloc ( size ); <nl> - memset ( buffer , 0 , size ); <nl> + buffer = xzalloc ( size + 1 ); <nl> + memset ( buffer , 0 , size + 1 ); <nl> BIO_read ( outBIO , buffer , size ); <nl> } <nl> 
void gdi_Glyph_Free ( rdpContext * context , rdpGlyph * glyph ) <nl> gdi_SelectObject ( gdi_glyph -> hdc , ( HGDIOBJECT ) gdi_glyph -> org_bitmap ); <nl> gdi_DeleteObject (( HGDIOBJECT ) gdi_glyph -> bitmap ); <nl> gdi_DeleteDC ( gdi_glyph -> hdc ); <nl> - xfree ( gdi_glyph ); <nl> } <nl> } <nl> 
void update_free ( rdpUpdate * update ) <nl>  <nl> xfree ( update -> bitmap_update . rectangles ); <nl> xfree ( update -> pointer ); <nl> + xfree ( update -> primary -> polyline . points ); <nl> + xfree ( update -> primary -> polygon_sc . points ); <nl> xfree ( update -> primary ); <nl> xfree ( update -> secondary ); <nl> xfree ( update -> altsec );
int send_arp ( u_char type , struct ip_addr * sip , u_int8 * smac , struct ip_addr * tip <nl>  <nl> SEND_LOCK ; <nl>  <nl> + // FIXME <nl> + // why without clearing again the packet I get issue # 245 ? <nl> + libnet_clear_packet ( GBL_IFACE -> lnet ); <nl> + <nl> /* ARP uses 00 : 00 : 00 : 00 : 00 : 00 broadcast */ <nl> if ( type == ARPOP_REQUEST && tmac == MEDIA_BROADCAST ) <nl> tmac = ARP_BROADCAST ;
IODeviceSocket ::~ IODeviceSocket () <nl> proc -> kill (); <nl> } <nl>  <nl> - delete d ; <nl> + d -> deleteLater (); <nl> } <nl>  <nl> bool IODeviceSocket :: canReadLine ()
List :: List ( const Kind _kind , const QByteArray & line , int & start ): <nl>  <nl> ++ start ; <nl>  <nl> - if ( start >= line . size ()) <nl> + if ( start >= line . size () - 2 ) <nl> throw NoData ( line , start ); // no mailbox <nl>  <nl> mailbox = LowLevelParser :: getMailbox ( line , start );
void ThreadingMsgListModel :: setSourceModel ( QAbstractItemModel * sourceModel ) <nl> this , SLOT ( handleRowsAboutToBeInserted ( const QModelIndex &, int , int ) ) ); <nl> connect ( sourceModel , SIGNAL ( rowsInserted ( const QModelIndex &, int , int ) ), <nl> this , SLOT ( handleRowsInserted ( const QModelIndex &, int , int ) ) ); <nl> + resetMe (); <nl> } <nl>  <nl> void ThreadingMsgListModel :: handleDataChanged ( const QModelIndex & topLeft , const QModelIndex & bottomRight )
int main ( int argc , char * argv []) { <nl> * the variable name we only support ptys here . */ <nl>  <nl> r = getenv_for_pid ( 1 , " container_ttys ", & container_ttys ); <nl> - if ( r >= 0 ) { <nl> + if ( r > 0 ) { <nl> char * w , * state ; <nl> size_t l ; <nl> 
int devnode_acl_all ( struct udev * udev , <nl> if ( r < 0 ) <nl> goto finish ; <nl>  <nl> - r = udev_enumerate_add_match_tag ( e , seat ); <nl> - if ( r < 0 ) <nl> - goto finish ; <nl> + if (! streq ( seat , " seat0 ")) { <nl> + r = udev_enumerate_add_match_tag ( e , seat ); <nl> + if ( r < 0 ) <nl> + goto finish ; <nl> + } <nl>  <nl> r = udev_enumerate_scan_devices ( e ); <nl> if ( r < 0 )
int main ( int argc , char * argv []) { <nl> log_error_errno ( r , " Failed to iterate through journal : % m "); <nl> goto finish ; <nl> } <nl> + if ( r == 0 ) { <nl> + printf ("-- No entries --\ n "); <nl> + goto finish ; <nl> + } <nl>  <nl> if (! arg_follow ) <nl> pager_open_if_enabled ();
int main ( int argc , char * argv []) { <nl> const void * data ; <nl> size_t size ; <nl>  <nl> + r = sd_journal_set_data_threshold ( j , 0 ); <nl> + if ( r < 0 ) { <nl> + log_error (" Failed to unset data size threshold "); <nl> + return EXIT_FAILURE ; <nl> + } <nl> + <nl> r = sd_journal_query_unique ( j , arg_field ); <nl> if ( r < 0 ) { <nl> log_error (" Failed to query unique data objects : % s ", strerror (- r ));
int switch_root ( const char * new_root ) { <nl>  <nl> if ( fstat ( old_root_fd , & rb ) < 0 ) <nl> log_warning (" Failed to stat old root directory , leaving : % m "); <nl> - else <nl> + else { <nl> rm_rf_children ( old_root_fd , false , false , & rb ); <nl> + old_root_fd = - 1 ; <nl> + } <nl> } <nl>  <nl> r = 0 ;
read_again : <nl> return - EINVAL ; <nl> } <nl>  <nl> - transitions = malloc0 ( total_size + tzspec_len ); <nl> + /* leave space for additional zone_names zero terminator */ <nl> + transitions = malloc0 ( total_size + tzspec_len + 1 ); <nl> if ( transitions == NULL ) <nl> return - EINVAL ; <nl> 
static int bus_message_setup_kmsg ( sd_bus_message * m ) { <nl> if (! m -> kdbus ) <nl> return - ENOMEM ; <nl>  <nl> + memset ( m -> kdbus , 0 , sz ); <nl> + <nl> m -> kdbus -> flags = <nl> (( m -> header -> flags & SD_BUS_MESSAGE_NO_REPLY_EXPECTED ) ? 0 : KDBUS_MSG_FLAGS_EXPECT_REPLY ) | <nl> (( m -> header -> flags & SD_BUS_MESSAGE_NO_AUTO_START ) ? KDBUS_MSG_FLAGS_NO_AUTO_START : 0 );
int main ( int argc , char * argv [], char * envp []) <nl> dbg (" error fcntl on write pipe : % s ", strerror ( errno )); <nl>  <nl> /* set signal handlers */ <nl> + memset (& act , 0x00 , sizeof ( struct sigaction )); <nl> act . sa_handler = ( void (*) ( int )) sig_handler ; <nl> sigemptyset (& act . sa_mask ); <nl> act . sa_flags = SA_RESTART ;
int main ( int argc , char * argv []) { <nl> goto finish ; <nl> } <nl>  <nl> + sd_event_get_exit_code ( m -> event , & r ); <nl> + <nl> finish : <nl> sd_notify ( false , " STATUS = Shutting down ..."); <nl> 
enum config_type { <nl> # define VALUE_SIZE 100 <nl> # define ID_SIZE 50 <nl> # define PLACE_SIZE 50 <nl> +# define PROGRAM_SIZE 100 <nl>  <nl> # define TYPE_LABEL " LABEL " <nl> # define TYPE_NUMBER " NUMBER " <nl> struct config_device { <nl> char id [ ID_SIZE ]; <nl> char place [ PLACE_SIZE ]; <nl> char kernel_name [ NAME_SIZE ]; <nl> - char exec_program [ FILE_SIZE ]; <nl> + char exec_program [ PROGRAM_SIZE ]; <nl> char name [ NAME_SIZE ]; <nl> char symlink [ NAME_SIZE ]; <nl> struct sysfs_pair sysfs_pair [ MAX_SYSFS_PAIRS ];
static int create_item ( Item * i ) { <nl>  <nl> case CREATE_FILE : <nl> case TRUNCATE_FILE : <nl> + r = write_one_file ( i , i -> path ); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + break ; <nl> case WRITE_FILE : <nl> r = glob_item ( i , write_one_file ); <nl> if ( r < 0 )
struct DnsTransaction { <nl>  <nl> uint16_t id ; <nl>  <nl> - bool initial_jitter_scheduled ; <nl> - bool initial_jitter_elapsed ; <nl> + bool initial_jitter_scheduled : 1 ; <nl> + bool initial_jitter_elapsed : 1 ; <nl>  <nl> DnsPacket * sent , * received ; <nl> 
finish : <nl>  <nl> if ( n_arguments > 3 ) { <nl> arguments [ n_arguments ] = NULL ; <nl> + strv_uniq ( arguments ); <nl> execv ("/ sbin / modprobe ", arguments ); <nl>  <nl> log_error (" Failed to execute / sbin / modprobe : % m ");
static int show_one ( <nl> */ <nl> if ( info . pid_file && access ( info . pid_file , F_OK ) == 0 ) <nl> r = 1 ; <nl> + else if ( streq_ptr ( info . load_state , " not - found ") && streq_ptr ( info . active_state , " inactive ")) <nl> + r = 4 ; <nl> else <nl> r = 3 ; <nl> }
static int method_set_vc_keyboard ( sd_bus_message * m , void * userdata , sd_bus_erro <nl> } <nl>  <nl> # ifdef HAVE_XKBCOMMON <nl> + _printf_ ( 3 , 0 ) <nl> static void log_xkb ( struct xkb_context * ctx , enum xkb_log_level lvl , const char * format , va_list args ) { <nl> const char * fmt ; <nl> 
static int systemctl_parse_argv ( int argc , char * argv []) { <nl> size_t size ; <nl>  <nl> FOREACH_WORD_SEPARATOR ( word , size , optarg , ",", state ) { <nl> - char * s ; <nl> + _cleanup_free_ char * s = NULL ; <nl>  <nl> s = strndup ( word , size ); <nl> if (! s )
int main ( int argc , char ** argv ) { <nl> log_parse_environment (); <nl>  <nl> r = parse_config (); <nl> - if ( r <= 0 ) <nl> + if ( r < 0 ) <nl> goto finish ; <nl>  <nl> r = parse_argv ( argc , argv );
static int event_make_signal_data ( <nl> d -> priority = priority ; <nl>  <nl> r = hashmap_put ( e -> signal_data , & d -> priority , d ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( d ); <nl> return r ; <nl> + } <nl>  <nl> added = true ; <nl> }
_public_ int sd_bus_path_decode_many ( const char * path , const char * path_template <nl> } <nl> va_end ( list ); <nl>  <nl> - free ( labels ); <nl> - labels = NULL ; <nl> + labels = mfree ( labels ); <nl> return 1 ; <nl> } <nl> 
static void output_unit_file_list ( const UnitFileList * units , unsigned c ) { <nl> } else <nl> id_cols = max_id_len ; <nl>  <nl> - if (! arg_no_legend ) <nl> + if (! arg_no_legend && c > 0 ) <nl> printf ("%-* s %-* s \ n ", <nl> id_cols , " UNIT FILE ", <nl> state_cols , " STATE ");
int columns ( void ) { <nl> struct winsize ws ; <nl> zero ( ws ); <nl>  <nl> - if ( ioctl ( STDIN_FILENO , TIOCGWINSZ , & ws ) >= 0 ) <nl> + if ( ioctl ( STDOUT_FILENO , TIOCGWINSZ , & ws ) >= 0 ) <nl> parsed_columns = ws . ws_col ; <nl> } <nl> 
static int list_machines ( int argc , char * argv [], void * userdata ) { <nl> while (( r = sd_bus_message_read ( reply , "( ssso )", & name , & class , & service , & object )) > 0 ) { <nl> size_t l ; <nl>  <nl> + if ( name [ 0 ] == '.' && ! arg_all ) <nl> + continue ; <nl> + <nl> if (! GREEDY_REALLOC ( machines , n_allocated , n_machines + 1 )) <nl> return log_oom (); <nl> 
static void print_status_info ( <nl>  <nl> printf (" CGroup : % s \ n ", i -> control_group ); <nl>  <nl> - if ( arg_transport == BUS_TRANSPORT_LOCAL ) { <nl> + if ( arg_transport == BUS_TRANSPORT_LOCAL || arg_transport == BUS_TRANSPORT_CONTAINER ) { <nl> unsigned k = 0 ; <nl> pid_t extra [ 2 ]; <nl> char prefix [] = " ";
static int manager_sigusr2 ( sd_event_source * s , const struct signalfd_siginfo * si <nl> assert ( m ); <nl>  <nl> manager_flush_caches ( m ); <nl> - log_info (" Flushed all caches ."); <nl>  <nl> return 0 ; <nl> } <nl> void manager_flush_caches ( Manager * m ) { <nl>  <nl> LIST_FOREACH ( scopes , scope , m -> dns_scopes ) <nl> dns_cache_flush (& scope -> cache ); <nl> + <nl> + log_info (" Flushed all caches ."); <nl> }
int session_set_controller ( Session * s , const char * sender , bool force ) { <nl> * If logind crashes / restarts , we restore the controller during restart <nl> * or reset the VT in case it crashed / exited , too . */ <nl> r = session_prepare_vt ( s ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( t ); <nl> return r ; <nl> + } <nl>  <nl> session_swap_controller ( s , t ); <nl> 
struct udev * udev_new ( void ) <nl> } <nl>  <nl> if ( strcasecmp ( key , " udev_log ") == 0 ) { <nl> - udev -> log_priority = util_log_priority ( val ); <nl> + udev_set_log_priority ( udev , util_log_priority ( val )); <nl> continue ; <nl> } <nl> if ( strcasecmp ( key , " udev_root ") == 0 ) {
int udevdb_add_dev ( const char * path , const struct udevice * dev ) <nl> if (( path == NULL ) || ( dev == NULL )) <nl> return - ENODEV ; <nl>  <nl> - memset ( keystr , 0 , NAME_SIZE ); <nl> + memset ( keystr , 0 , SYSFS_PATH_MAX ); <nl> strfieldcpy ( keystr , path ); <nl> key . dptr = keystr ; <nl> key . dsize = strlen ( keystr ) + 1 ;
bool socket_address_equal ( const SocketAddress * a , const SocketAddress * b ) { <nl> break ; <nl>  <nl> case AF_UNIX : <nl> + if ( a -> size <= offsetof ( struct sockaddr_un , sun_path ) || <nl> + b -> size <= offsetof ( struct sockaddr_un , sun_path )) <nl> + return false ; <nl> + <nl> if (( a -> sockaddr . un . sun_path [ 0 ] == 0 ) != ( b -> sockaddr . un . sun_path [ 0 ] == 0 )) <nl> return false ; <nl> 
static void bus_free ( sd_bus * b ) { <nl>  <nl> sd_bus_detach_event ( b ); <nl>  <nl> + if ( b -> default_bus_ptr ) <nl> + * b -> default_bus_ptr = NULL ; <nl> + <nl> bus_close_fds ( b ); <nl>  <nl> if ( b -> kdbus_buffer )
static DBusHandlerResult locale_message_handler ( <nl> " Locale \ 0 "); <nl> if (! changed ) <nl> goto oom ; <nl> - } <nl> + } else <nl> + strv_free ( l ); <nl> + <nl> } else if ( dbus_message_is_method_call ( message , " org . freedesktop . locale1 ", " SetVConsoleKeyboard ")) { <nl>  <nl> const char * keymap , * keymap_toggle ;
int link_config_apply ( link_config_ctx * ctx , link_config * config , <nl> if ( ctx -> enable_name_policy && config -> name_policy ) { <nl> NamePolicy * policy ; <nl>  <nl> - for ( policy = config -> name_policy ; ! respect_predictable && ! new_name && <nl> - * policy != _NAMEPOLICY_INVALID ; policy ++) { <nl> + for ( policy = config -> name_policy ; <nl> + ! new_name && * policy != _NAMEPOLICY_INVALID ; policy ++) { <nl> switch (* policy ) { <nl> case NAMEPOLICY_KERNEL : <nl> respect_predictable = true ;
static int cache_space_refresh ( Server * s , JournalStorage * storage ) { <nl>  <nl> ts = now ( CLOCK_MONOTONIC ); <nl>  <nl> - if ( space -> timestamp + RECHECK_SPACE_USEC > ts ) <nl> + if ( space -> timestamp != 0 && space -> timestamp + RECHECK_SPACE_USEC > ts ) <nl> return 0 ; <nl>  <nl> r = determine_path_usage ( s , storage -> path , & vfs_used , & vfs_avail );
int base_filesystem_create ( const char * root ) { <nl> const char * target = NULL ; <nl> const char * s ; <nl>  <nl> + if ( faccessat ( fd , table [ i ]. dir , F_OK , AT_SYMLINK_NOFOLLOW ) >= 0 ) <nl> + continue ; <nl> + <nl> /* check if one of the targets exists */ <nl> NULSTR_FOREACH ( s , table [ i ]. target ) { <nl> if ( faccessat ( fd , s , F_OK , AT_SYMLINK_NOFOLLOW ) < 0 )
int message_append_basic ( sd_bus_message * m , char type , const void * p , const void <nl> void * a ; <nl> char * e = NULL ; <nl> int fd = - 1 ; <nl> - uint32_t fdi ; <nl> + uint32_t fdi = 0 ; <nl> int r ; <nl>  <nl> if (! m )
int sd_rtnl_message_append_ether_addr ( sd_rtnl_message * m , unsigned short type , c <nl> return - ENOTSUP ; <nl> } <nl>  <nl> - r = add_rtattr ( m , type , data , sizeof ( data )); <nl> + r = add_rtattr ( m , type , data , ETH_ALEN ); <nl> if ( r < 0 ) <nl> return r ; <nl> 
subst : <nl> cpos ++; <nl> while ( isspace ( cpos [ 0 ])) <nl> cpos ++; <nl> + if ( cpos [ 0 ] == '\ 0 ') <nl> + break ; <nl> } <nl> if ( i > 0 ) { <nl> log_error (" requested part of result string not found ");
int udev_device_set_syspath ( struct udev_device * udev_device , const char * syspath <nl> } <nl>  <nl> /* trailing number */ <nl> - while ( isdigit ( udev_device -> sysname [-- len ])) <nl> + while ( len > 0 && isdigit ( udev_device -> sysname [-- len ])) <nl> udev_device -> sysnum = & udev_device -> sysname [ len ]; <nl> + <nl> + /* sysname is completely numeric */ <nl> + if ( len == 0 ) <nl> + udev_device -> sysnum = NULL ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int import_program_into_properties ( struct udev_device * dev , const char * p <nl> { <nl> struct udev * udev = udev_device_get_udev ( dev ); <nl> char ** envp ; <nl> - char result [ 2048 ]; <nl> + char result [ 4096 ]; <nl> size_t reslen ; <nl> char * line ; <nl> 
static int builtin_kmod ( struct udev_device * dev , int argc , char * argv [], bool te <nl> struct udev * udev = udev_device_get_udev ( dev ); <nl> int i ; <nl>  <nl> - if ( ctx ) <nl> + if (! ctx ) <nl> return 0 ; <nl>  <nl> if ( argc < 3 || strcmp ( argv [ 1 ], " load ")) {
void seat_evict_position ( Seat * s , Session * session ) { <nl> * position ( eg ., during gdm -> session transition ), so let ' s look <nl> * for it and set it on the free slot . */ <nl> LIST_FOREACH ( sessions_by_seat , iter , s -> sessions ) { <nl> - if ( iter -> position == pos ) { <nl> + if ( iter -> position == pos && session_get_state ( iter ) != SESSION_CLOSING ) { <nl> s -> positions [ pos ] = iter ; <nl> break ; <nl> }
int dns_zone_put ( DnsZone * z , DnsResourceRecord * rr ) { <nl> assert ( z ); <nl> assert ( rr ); <nl>  <nl> + if ( rr -> key -> class == DNS_CLASS_ANY ) <nl> + return - EINVAL ; <nl> + if ( rr -> key -> type == DNS_TYPE_ANY ) <nl> + return - EINVAL ; <nl> + <nl> existing = dns_zone_get ( z , rr ); <nl> if ( existing ) <nl> return 0 ;
static int parse_password ( const char * filename , char ** wall ) { <nl> } <nl> } <nl>  <nl> + if ( pid > 0 && <nl> + kill ( pid , 0 ) < 0 && <nl> + errno == ESRCH ) { <nl> + r = 0 ; <nl> + goto finish ; <nl> + } <nl> + <nl> if ( arg_action == ACTION_LIST ) <nl> printf ("'% s ' ( PID % u )\ n ", message , pid ); <nl> else if ( arg_action == ACTION_WALL ) {
static int json_parse_tokens ( JsonVariant ** tokens , size_t ntokens , JsonVariant * <nl> size_t it = 0 ; <nl> int r ; <nl> JsonVariant * e ; <nl> - _cleanup_jsonunref_ JsonVariant * p ; <nl> + _cleanup_jsonunref_ JsonVariant * p = NULL ; <nl>  <nl> assert ( tokens ); <nl> assert ( ntokens );
static int detect_vm_xen ( void ) { <nl> r = read_one_line_file ("/ proc / xen / capabilities ", & domcap ); <nl> if ( r == - ENOENT ) <nl> return VIRTUALIZATION_NONE ; <nl> + if ( r < 0 ) <nl> + return r ; <nl>  <nl> i = domcap ; <nl> while (( cap = strsep (& i , ",")))
int sd_rtnl_call ( sd_rtnl * rtnl , <nl> r = rtnl_poll ( rtnl , true , left ); <nl> if ( r < 0 ) <nl> return r ; <nl> + else if ( r == 0 ) <nl> + return - ETIMEDOUT ; <nl>  <nl> r = dispatch_wqueue ( rtnl ); <nl> if ( r < 0 )
void seat_claim_position ( Seat * s , Session * session , unsigned int pos ) { <nl> seat_evict_position ( s , session ); <nl>  <nl> session -> position = pos ; <nl> - if ( pos > 0 && ! s -> positions [ pos ]) <nl> + if ( pos > 0 ) <nl> s -> positions [ pos ] = session ; <nl> } <nl> 
static char ** user_dirs ( <nl> const char * e ; <nl> _cleanup_strv_free_ char ** config_dirs = NULL , ** data_dirs = NULL ; <nl> _cleanup_free_ char * data_home = NULL ; <nl> - _cleanup_free_ char ** res = NULL ; <nl> + _cleanup_strv_free_ char ** res = NULL ; <nl> char ** tmp ; <nl> int r ; <nl> 
int selinux_setup ( bool * loaded_policy ) { <nl> * loaded_policy = true ; <nl>  <nl> } else { <nl> + log_open (); <nl> + <nl> if ( enforce > 0 ) { <nl> - log_error (" Failed to load SELinux policy ."); <nl> + log_error (" Failed to load SELinux policy . Freezing ."); <nl> return - EIO ; <nl> } else <nl> - log_debug (" Unable to load SELinux policy ."); <nl> + log_debug (" Unable to load SELinux policy . Ignoring ."); <nl> } <nl> # endif <nl> 
static int parse_file ( const char * path , bool ignore_enoent ) { <nl>  <nl> free ( property ); <nl> free ( new_value ); <nl> - if ( r != - EEXIST ) <nl> + if ( r != 0 ) <nl> goto finish ; <nl> } <nl> }
static int parse_request ( uint8_t code , uint8_t len , const void * option , void * us <nl>  <nl> break ; <nl> case SD_DHCP_OPTION_MAXIMUM_MESSAGE_SIZE : <nl> - if ( len == 2 ) <nl> + <nl> + if ( len == 2 && unaligned_read_be16 ( option ) >= sizeof ( DHCPPacket )) <nl> req -> max_optlen = unaligned_read_be16 ( option ) - sizeof ( DHCPPacket ); <nl>  <nl> break ;
static int server_parse_proc_cmdline ( Server * s ) { <nl>  <nl> p = line ; <nl> for (;;) { <nl> - _cleanup_free_ char * word ; <nl> + _cleanup_free_ char * word = NULL ; <nl>  <nl> r = extract_first_word (& p , & word , NULL , 0 ); <nl> if ( r < 0 )
static int transaction_verify_order_one ( Transaction * tr , Job * j , Job * from , unsi <nl> " Found dependency on % s /% s ", <nl> k -> unit -> id , job_type_to_string ( k -> type )); <nl>  <nl> - if (! delete && <nl> + if (! delete && hashmap_get ( tr -> jobs , k -> unit ) && <nl> ! unit_matters_to_anchor ( k -> unit , k )) { <nl> /* Ok , we can drop this one , so let ' s <nl> * do so . */
void xor_buf ( uint8_t out [], <nl> const uint8_t in2 [], <nl> size_t length ) <nl> { <nl> - while ( length >= 8 ) <nl> + while ( length >= 16 ) <nl> { <nl> out [ 0 ] = in [ 0 ] ^ in2 [ 0 ]; <nl> out [ 1 ] = in [ 1 ] ^ in2 [ 1 ];
gss_verify_mic ( OM_uint32 * minor_status , <nl> gss_qop_t * qop_state ) <nl> { <nl> struct _gss_context * ctx = ( struct _gss_context *) context_handle ; <nl> - gssapi_mech_interface m = ctx -> gc_mech ; <nl> + gssapi_mech_interface m ; <nl>  <nl> if ( qop_state ) <nl> * qop_state = 0 ; <nl> gss_verify_mic ( OM_uint32 * minor_status , <nl> return GSS_S_NO_CONTEXT ; <nl> } <nl>  <nl> + m = ctx -> gc_mech ; <nl> + <nl> return ( m -> gm_verify_mic ( minor_status , ctx -> gc_ctx , <nl> message_buffer , token_buffer , qop_state )); <nl> }
KRB5_LIB_FUNCTION krb5_error_code KRB5_LIB_CALL <nl> krb5_ret_int16 ( krb5_storage * sp , <nl> int16_t * value ) <nl> { <nl> - int32_t v ; <nl> + int32_t v = 0 ; <nl> int ret ; <nl> ret = krb5_ret_int ( sp , & v , 2 ); <nl> if ( ret )
roken_detach_prep ( int argc , char ** argv , char * special_arg ) <nl> do { <nl> bytes = read ( pipefds [ 0 ], buf , sizeof ( buf )); <nl> } while ( bytes == - 1 && errno == EINTR ); <nl> + ( void ) close ( pipefds [ 0 ]); <nl> + pipefds [ 0 ] = - 1 ; <nl> if ( bytes == - 1 ) { <nl> /* <nl> * No need to wait for the process . We ' ve killed it . If it
gss_pseudo_random ( OM_uint32 * minor_status , <nl> gss_buffer_t prf_out ) <nl> { <nl> struct _gss_context * ctx = ( struct _gss_context *) context ; <nl> - gssapi_mech_interface m = ctx -> gc_mech ; <nl> + gssapi_mech_interface m ; <nl> OM_uint32 major_status ; <nl>  <nl> _mg_buffer_zero ( prf_out ); <nl> gss_pseudo_random ( OM_uint32 * minor_status , <nl> return GSS_S_NO_CONTEXT ; <nl> } <nl>  <nl> + m = ctx -> gc_mech ; <nl> + <nl> if ( m -> gm_pseudo_random == NULL ) <nl> return GSS_S_UNAVAILABLE ; <nl> 
 <nl> extern require_preauth ; <nl> extern sig_atomic_t exit_flag ; <nl> + extern char * keyfile ; <nl>  <nl> extern struct timeval now ; <nl> # define kdc_time ( now . tv_sec ) <nl> void loop ( krb5_context ); <nl>  <nl> void kdc_log ( int , const char * fmt , ...); <nl>  <nl> + Key * unseal_key ( Key * key ); <nl> + <nl> # define ALLOC ( X ) (( X ) = malloc ( sizeof (*( X )))) <nl>  <nl> # endif /* __KDC_LOCL_H__ */
cms_create_sd ( struct cms_create_sd_options * opt , int argc , char ** argv ) <nl> if ( ret ) <nl> hx509_err ( context , 1 , ret , " hx509_certs_find "); <nl> } <nl> + if (! opt -> embedded_certs_flag ) <nl> + flags |= HX509_CMS_SIGNATURE_NO_CERTS ; <nl> + if ( opt -> embed_leaf_only_flag ) <nl> + flags |= HX509_CMS_SIGNATURE_LEAF_ONLY ; <nl>  <nl> ret = rk_undumpdata ( infile , & p , & sz ); <nl> if ( ret )
verify_checksum ( krb5_context context , <nl> if ( keyed_checksum && crypto == NULL ) { <nl> krb5_set_error_message ( context , KRB5_PROG_SUMTYPE_NOSUPP , <nl> N_ (" Checksum type % s is keyed but no " <nl> - " crypto context ( key ) was passed in ", "") <nl> + " crypto context ( key ) was passed in ", ""), <nl> ct -> name ); <nl> return KRB5_PROG_SUMTYPE_NOSUPP ; /* XXX */ <nl> }
# ifndef KRB5_DEPRECATED <nl> # if defined ( __GNUC__ ) && (( __GNUC__ > 3 ) || (( __GNUC__ == 3 ) && ( __GNUC_MINOR__ >= 1 ))) <nl> # define KRB5_DEPRECATED __attribute__ (( deprecated )) <nl> -# elif defined ( _MSC_VER ) <nl> +# elif defined ( _MSC_VER ) && ( _MSC_VER > 1200 ) <nl> # define KRB5_DEPRECATED __declspec ( deprecated ) <nl> # else <nl> # define KRB5_DEPRECATED
get_creds ( krb5_context context , const char * keytab_str , <nl>  <nl> ret = krb5_cc_store_cred ( context , * cache , & creds ); <nl> if ( ret ) krb5_err ( context , 1 , ret , " krb5_cc_store_cred "); <nl> + <nl> + krb5_free_cred_contents ( context , & creds ); <nl> + krb5_free_principal ( context , client ); <nl> } <nl>  <nl> static krb5_error_code
main ( int argc , char ** argv ) <nl>  <nl> setprogname ( argv [ 0 ]); <nl>  <nl> + setlocale ( LC_ALL , ""); <nl> + bindtextdomain (" heimdal_kuser ", HEIMDAL_LOCALEDIR ); <nl> + textdomain (" heimdal_kuser "); <nl> + <nl> ret = krb5_init_context (& context ); <nl> if ( ret == KRB5_CONFIG_BADFORMAT ) <nl> errx ( 1 , " krb5_init_context failed to parse configuration file ");
EVP_BytesToKey ( const EVP_CIPHER * type , <nl> void * keydata , <nl> void * ivdata ) <nl> { <nl> + return - 1 ; <nl> } <nl> 
tgs_build_reply ( astgs_request_t priv , <nl>  <nl> s = & adtkt . cname ; <nl> r = adtkt . crealm ; <nl> + } else if ( s == NULL ) { <nl> + ret = KRB5KDC_ERR_S_PRINCIPAL_UNKNOWN ; <nl> + _kdc_set_e_text ( r , " No server in request "); <nl> + goto out ; <nl> } <nl>  <nl> _krb5_principalname2krb5_principal ( context , & sp , * s , r );
stats_config ( <nl> rawstats . fp = NULL ; <nl> filegen_setup (& rawstats , now . l_ui ); <nl> } <nl> +# ifdef OPENSSL <nl> + if ( cryptostats . prefix == & statsdir [ 0 ] && <nl> + cryptostats . fp != NULL ) { <nl> + fclose ( cryptostats . fp ); <nl> + cryptostats . fp = NULL ; <nl> + filegen_setup (& cryptostats , now . l_ui ); <nl> + } <nl> +# endif /* OPENSSL */ <nl> } <nl> break ; <nl> 
int rdp_redirection_apply_settings ( rdpRdp * rdp ) <nl> settings -> TargetNetAddresses [ i ] = _strdup ( redirection -> TargetNetAddresses [ i ]); <nl> if (! settings -> TargetNetAddresses [ i ]) <nl> { <nl> - for (; i > 0 ; -- i ) <nl> - free ( settings -> TargetNetAddresses [ i ]); <nl> + UINT32 j ; <nl> + <nl> + for ( j = 0 ; j < i ; j ++) <nl> + free ( settings -> TargetNetAddresses [ j ]); <nl> return - 1 ; <nl> } <nl> }
rdpSettings * freerdp_settings_new ( DWORD flags ) <nl> settings -> SaltedChecksum = TRUE ; <nl> settings -> ServerPort = 3389 ; <nl> settings -> GatewayPort = 443 ; <nl> + settings -> GatewayBypassLocal = TRUE ; <nl> settings -> DesktopResize = TRUE ; <nl> settings -> ToggleFullscreen = TRUE ; <nl> settings -> DesktopPosX = 0 ;
static int cliprdr_server_receive_format_list ( CliprdrServerContext * context , wSt <nl>  <nl> for ( index = 0 ; index < formatList . numFormats ; index ++) <nl> { <nl> - if ( formats [ index ]. formatName ) <nl> - free ( formats [ index ]. formatName ); <nl> + if ( formatList . formats [ index ]. formatName ) <nl> + free ( formatList . formats [ index ]. formatName ); <nl> } <nl>  <nl> - free ( formats ); <nl> + free ( formatList . formats ); <nl>  <nl> return 1 ; <nl> }
int makecert_context_process ( MAKECERT_CONTEXT * context , int argc , char ** argv ) <nl> if (! rsa ) <nl> return - 1 ; <nl>  <nl> + context -> rsa = RSA_new (); <nl> + if (! context -> rsa ) <nl> + { <nl> + BN_clear_free ( rsa ); <nl> + return - 1 ; <nl> + } <nl> BN_set_word ( rsa , RSA_F4 ); <nl> rc = RSA_generate_key_ex ( context -> rsa , key_length , rsa , NULL ); <nl> BN_clear_free ( rsa );
static void * rdpei_schedule_thread ( void * arg ) <nl>  <nl> out : <nl>  <nl> - if ( error && rdpei -> rdpcontext ) <nl> + if ( error && rdpei && rdpei -> rdpcontext ) <nl> setChannelError ( rdpei -> rdpcontext , error , <nl> " rdpei_schedule_thread reported an error "); <nl> 
int rdtk_font_parse_descriptor_buffer ( rdtkFont * font , BYTE * buffer , int size ) <nl> } <nl>  <nl> font -> glyphCount = count ; <nl> - font -> glyphs = ( rdtkGlyph *) calloc ( font -> glyphCount , sizeof ( rdtkGlyph )); <nl> + font -> glyphs = NULL ; <nl> + if ( count > 0 ) <nl> + font -> glyphs = ( rdtkGlyph *) calloc ( font -> glyphCount , sizeof ( rdtkGlyph )); <nl>  <nl> if (! font -> glyphs ) <nl> return - 1 ;
static void set_hints ( primitives_hints_t * hints ) <nl>  <nl> # elif defined ( _M_ARM ) <nl>  <nl> - static UINT32 androidNeon ( void ) <nl> + static UINT32 getNeonSupport ( void ) <nl> { <nl> # ifdef __ANDROID__ <nl> if ( android_getCpuFamily () != ANDROID_CPU_FAMILY_ARM ) return 0 ; <nl> static UINT32 androidNeon ( void ) <nl> static void set_hints ( primitives_hints_t * hints ) <nl> { <nl> /* ARM : TODO */ <nl> - hints -> arm_flags |= androidNeon (); <nl> + hints -> arm_flags |= getNeonSupport (); <nl> } <nl>  <nl> # else
void transport_free ( rdpTransport * transport ) <nl> { <nl> if ( transport ) <nl> { <nl> + if ( transport -> async ) <nl> + { <nl> + assert (! transport -> thread ); <nl> + assert (! transport -> stopEvent ); <nl> + } <nl> + <nl> if ( transport -> ReceiveBuffer ) <nl> Stream_Release ( transport -> ReceiveBuffer ); <nl> 
void tsmf_playback_ack ( IWTSVirtualChannelCallback * pChannelCallback , <nl> if (! callback || ! callback -> channel || ! callback -> channel -> Write ) <nl> { <nl> WLog_ERR ( TAG , " callback =% p , channel =% p , write =% p ", callback , <nl> - callback -> channel , callback -> channel -> Write ); <nl> + callback ? callback -> channel : NULL , <nl> + ( callback && callback -> channel ) ? callback -> channel -> Write : NULL ); <nl> } <nl> else <nl> {
wStream * transport_send_stream_init ( rdpTransport * transport , int size ) <nl>  <nl> void transport_attach ( rdpTransport * transport , int sockfd ) <nl> { <nl> + if (! transport -> TcpIn ) <nl> + transport -> TcpIn = freerdp_tcp_new ( transport -> settings ); <nl> + <nl> freerdp_tcp_attach ( transport -> TcpIn , sockfd ); <nl> + <nl> transport -> SplitInputOutput = FALSE ; <nl> transport -> frontBio = transport -> TcpIn -> bufferedBio ; <nl> }
boolean security_establish_keys ( uint8 * client_random , rdpRdp * rdp ) <nl>  <nl> memcpy ( rdp -> decrypt_update_key , rdp -> decrypt_key , 16 ); <nl> memcpy ( rdp -> encrypt_update_key , rdp -> encrypt_key , 16 ); <nl> + rdp -> decrypt_use_count = 0 ; <nl> + rdp -> decrypt_checksum_use_count = 0 ; <nl> + rdp -> encrypt_use_count = 0 ; <nl> + rdp -> encrypt_checksum_use_count = 0 ; <nl>  <nl> return true ; <nl> }
static CACHE_BITMAP_V3_ORDER * update_read_cache_bitmap_v3_order ( rdpUpdate * updat <nl> Stream_Read_UINT16 ( s , bitmapData -> height ); /* height ( 2 bytes ) */ <nl> Stream_Read_UINT32 ( s , new_len ); /* length ( 4 bytes ) */ <nl>  <nl> - if ( Stream_GetRemainingLength ( s ) < new_len ) <nl> + if (( new_len == 0 ) || ( Stream_GetRemainingLength ( s ) < new_len )) <nl> goto fail ; <nl>  <nl> new_data = ( BYTE *) realloc ( bitmapData -> data , new_len );
boolean nego_send_negotiation_response ( rdpNego * nego ) <nl> settings -> encryption_method = ENCRYPTION_METHOD_40BIT | ENCRYPTION_METHOD_128BIT | ENCRYPTION_METHOD_FIPS ; <nl> settings -> encryption_level = ENCRYPTION_LEVEL_CLIENT_COMPATIBLE ; <nl> } <nl> + if ( settings -> encryption && settings -> server_key == NULL && settings -> rdp_key_file == NULL ) <nl> + return false ; <nl> } <nl> else if ( settings -> selected_protocol == PROTOCOL_TLS ) <nl> {
int freerdp_parse_args ( rdpSettings * settings , int argc , char ** argv , <nl> } <nl> else if ( strcmp ("-- plugin ", argv [ index ]) == 0 ) <nl> { <nl> - t = index ; <nl> index ++; <nl> + t = index ; <nl> if ( index == argc ) <nl> { <nl> printf (" missing plugin name \ n ");
static BOOL update_read_bitmap_data ( rdpUpdate * update , wStream * s , BITMAP_DATA * <nl> { <nl> if (!( bitmapData -> flags & NO_BITMAP_COMPRESSION_HDR )) <nl> { <nl> + if ( Stream_GetRemainingLength ( s ) < 8 ) <nl> + return FALSE ; <nl> + <nl> Stream_Read_UINT16 ( s , <nl> bitmapData -> cbCompFirstRowSize ); /* cbCompFirstRowSize ( 2 bytes ) */ <nl> Stream_Read_UINT16 ( s ,
SECURITY_STATUS ntlm_read_NegotiateMessage ( NTLM_CONTEXT * context , PSecBuffer buf <nl> return SEC_E_INVALID_TOKEN ; <nl> } <nl>  <nl> + if ( Stream_GetRemainingLength ( s ) < 4 ) <nl> + { <nl> + Stream_Free ( s , FALSE ); <nl> + return SEC_E_INVALID_TOKEN ; <nl> + } <nl> Stream_Read_UINT32 ( s , message -> NegotiateFlags ); /* NegotiateFlags ( 4 bytes ) */ <nl>  <nl> if (!(( message -> NegotiateFlags & NTLMSSP_REQUEST_TARGET ) &&
BOOL security_fips_decrypt ( BYTE * data , size_t length , rdpRdp * rdp ) <nl> { <nl> size_t olen ; <nl>  <nl> + if (! rdp || ! rdp -> fips_decrypt ) <nl> + return FALSE ; <nl> + <nl> if (! winpr_Cipher_Update ( rdp -> fips_decrypt , data , length , data , & olen )) <nl> return FALSE ; <nl> 
static UINT drive_process_irp_query_directory ( DRIVE_DEVICE * drive , IRP * irp ) <nl> Stream_Read_UINT32 ( irp -> input , PathLength ); <nl> Stream_Seek ( irp -> input , 23 ); /* Padding */ <nl> path = ( WCHAR *) Stream_Pointer ( irp -> input ); <nl> + if (! Stream_CheckAndLogRequiredLength ( TAG , irp -> input , PathLength )) <nl> + return ERROR_INVALID_DATA ; <nl> + <nl> file = drive_get_file_by_id ( drive , irp -> FileId ); <nl>  <nl> if ( file == NULL )
static void rdpsnd_process_receive ( rdpSvcPlugin * plugin , STREAM * data_in ) <nl> if ( rdpsnd -> expectingWave ) <nl> { <nl> rdpsnd_process_message_wave ( rdpsnd , data_in ); <nl> + stream_free ( data_in ); <nl> return ; <nl> } <nl>  <nl> static void rdpsnd_process_receive ( rdpSvcPlugin * plugin , STREAM * data_in ) <nl> DEBUG_WARN (" unknown msgType % d ", msgType ); <nl> break ; <nl> } <nl> + <nl> + stream_free ( data_in ); <nl> } <nl>  <nl> static void rdpsnd_register_device_plugin ( rdpsndPlugin * rdpsnd , rdpsndDevicePlugin * device )
DWORD GetTimeZoneInformation ( LPTIME_ZONE_INFORMATION lpTimeZoneInformation ) <nl>  <nl> /* 1 ... TIME_ZONE_ID_STANDARD <nl> * 2 ... TIME_ZONE_ID_DAYLIGHT */ <nl> - return dtz -> SupportsDST ? 2 : 1 ; <nl> + return local_time -> tm_isdst ? 2 : 1 ; <nl> } <nl> else <nl> {
int freerdp_assistance_parse_file ( rdpAssistanceFile * file , const char * name ) <nl> FILE * fp = NULL ; <nl> size_t readSize ; <nl> INT64 fileSize ; <nl> + <nl> + if (! name ) <nl> + return - 1 ; <nl> + <nl> fp = fopen ( name , " r "); <nl>  <nl> if (! fp )
static BOOL rdp_print_input_capability_set ( wStream * s , UINT16 length ) <nl> static BOOL rdp_read_font_capability_set ( wStream * s , UINT16 length , rdpSettings * settings ) <nl> { <nl> WINPR_UNUSED ( settings ); <nl> - if ( length > 4 ) <nl> + if ( length > 5 ) <nl> Stream_Seek_UINT16 ( s ); /* fontSupportFlags ( 2 bytes ) */ <nl>  <nl> - if ( length > 6 ) <nl> + if ( length > 7 ) <nl> Stream_Seek_UINT16 ( s ); /* pad2Octets ( 2 bytes ) */ <nl>  <nl> return TRUE ;
struct rdp_freerdp <nl> Must be set to NULL if not needed . */ <nl> UINT64 paddingC [ 47 - 35 ]; /* 35 */ <nl>  <nl> - ALIGN64 ConnectionCallbackState ; /* 48 */ <nl> + ALIGN64 UINT ConnectionCallbackState ; /* 47 */ <nl>  <nl> ALIGN64 pPreConnect PreConnect ; /**< ( offset 48 ) <nl> Callback for pre - connect operations .
rdpSettings * freerdp_settings_new ( DWORD flags ) <nl> settings -> NoBitmapCompressionHeader = TRUE ; <nl> settings -> RefreshRect = TRUE ; <nl> settings -> SuppressOutput = TRUE ; <nl> - settings -> GlyphSupportLevel = GLYPH_SUPPORT_FULL ; <nl> + settings -> GlyphSupportLevel = GLYPH_SUPPORT_NONE ; <nl> settings -> GlyphCache = malloc ( sizeof ( GLYPH_CACHE_DEFINITION ) * 10 ); <nl>  <nl> if (! settings -> GlyphCache )
BOOL glyph_cache_put ( rdpGlyphCache * glyphCache , UINT32 id , UINT32 index , rdpGlyp <nl> return FALSE ; <nl> } <nl>  <nl> - if ( index > glyphCache -> glyphCache [ id ]. number ) <nl> + if ( index >= glyphCache -> glyphCache [ id ]. number ) <nl> { <nl> WLog_ERR ( TAG , " invalid glyph cache index : %" PRIu32 " in cache id : %" PRIu32 "", index , id ); <nl> return FALSE ;
static void rfx_compose_message_frame_begin ( RFX_CONTEXT * context , STREAM * data_o <nl> stream_write_uint8 ( data_out , 0 ); /* CodecChannelT . channelId */ <nl> stream_write_uint32 ( data_out , context -> frame_idx ); /* frameIdx */ <nl> stream_write_uint16 ( data_out , 1 ); /* numRegions */ <nl> + <nl> + context -> frame_idx ++; <nl> } <nl>  <nl> static void rfx_compose_message_region ( RFX_CONTEXT * context , STREAM * data_out ,
int transport_write ( rdpTransport * transport , wStream * s ) <nl> int status = - 1 ; <nl> int writtenlength = 0 ; <nl>  <nl> + if (! transport ) <nl> + return - 1 ; <nl> + <nl> + if (! transport -> frontBio ) <nl> + { <nl> + transport -> layer = TRANSPORT_LAYER_CLOSED ; <nl> + return - 1 ; <nl> + } <nl> + <nl> EnterCriticalSection (&( transport -> WriteLock )); <nl>  <nl> length = Stream_GetPosition ( s );
public : <nl> return CONTINUE ; <nl> } <nl>  <nl> + void OnNick ( const CNick & Nick , const CString & sNewNick , const std :: vector < CChan *>& vChans ) override { <nl> + for ( CChan * pChan : vChans ) { <nl> + Message (* pChan ); <nl> + } <nl> + } <nl> + <nl> void ShowCommand ( const CString & sLine ) { <nl> PutModule (" Current limit is " + CString ( m_iThresholdMsgs ) + " lines " <nl> " in " + CString ( m_iThresholdSecs ) + " secs .");
BGD_DECLARE ( gdImagePtr ) gdImageCropThreshold ( gdImagePtr im , const unsigned int c <nl> return NULL ; <nl> } <nl>  <nl> + if ( color < 0 || (! gdImageTrueColor ( im ) && color >= gdImageColorsTotal ( im ))) { <nl> + return NULL ; <nl> + } <nl> + <nl> /* TODO : Add gdImageGetRowPtr and works with ptr at the row level <nl> * for the true color and palette images <nl> * new formats will simply work with ptr
namespace mongo { <nl>  <nl> virtual void startRequest () {} <nl>  <nl> + virtual void onAddAuthorizedPrincipal ( Principal *) {} <nl> + <nl> + virtual void onLogoutDatabase ( const std :: string & dbname ) {} <nl> + <nl> private : <nl> bool _returnValue ; <nl> };
namespace mongo { <nl> kill_wrapper ( pid , signal , port ); <nl>  <nl> int i = 0 ; <nl> - for ( ; i < 65 ; ++ i ) { <nl> - if ( i == 5 ) { <nl> + for ( ; i < 130 ; ++ i ) { <nl> + if ( i == 30 ) { <nl> char now [ 64 ]; <nl> time_t_to_String ( time ( 0 ), now ); <nl> now [ 20 ] = 0 ;
namespace mongo { <nl> ) , result ) ); <nl> conn . done (); <nl>  <nl> - return result . getObjectField ( " median " ); <nl> + return result . getObjectField ( " median " ). getOwned (); <nl> } <nl>  <nl> Shard * Shard :: split (){
namespace mongo { <nl> void DBConfig :: enableSharding () { <nl> if ( _shardingEnabled ) <nl> return ; <nl> + <nl> + assert ( _name != " config " ); <nl> + <nl> scoped_lock lk ( _lock ); <nl> _shardingEnabled = true ; <nl> _save ();
namespace cling { <nl> // Compile the wrapper code . <nl> // <nl> const llvm :: GlobalValue * GV = 0 ; <nl> + if (! getCodeGenerator ()) <nl> + return 0 ; <nl> + <nl> if ( ifUnique ) <nl> GV = getCodeGenerator ()-> GetModule ()-> getNamedValue ( name ); <nl> 
namespace cling { <nl> Interpreter :: Interpreter ( int argc , const char * const * argv , <nl> const char * llvmdir /*= 0 */) : <nl> m_UniqueCounter ( 0 ), m_PrintDebug ( false ), <nl> - m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ) { <nl> + m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ), <nl> + m_LastCustomPragmaDiagPopPoint (){ <nl>  <nl> m_LLVMContext . reset ( new llvm :: LLVMContext ); <nl> std :: vector < unsigned > LeftoverArgsIdx ;
namespace cling { <nl> ///\ param [ in ] file1 - A file to diff <nl> ///\ param [ in ] file2 - A file to diff <nl> ///\ param [ in ] differences - The differences if any between file1 and file2 <nl> + ///\ param [ in ] ignores - A list of differences to ignore . <nl> ///\ returns true if there is difference in the contents . <nl> /// <nl> bool differentContent ( const std :: string & file1 , const std :: string & file2 ,
namespace cling { <nl> if (! CI -> hasTarget ()) { <nl> return 0 ; <nl> } <nl> - CI -> getTarget (). setForcedLangOptions ( CI -> getLangOpts ()); <nl> + CI -> getTarget (). adjust ( CI -> getLangOpts ()); <nl> SetClingTargetLangOpts ( CI -> getLangOpts (), CI -> getTarget ()); <nl> if ( CI -> getTarget (). getTriple (). getOS () == llvm :: Triple :: Cygwin ) { <nl> // clang " forgets " the basic arch part needed by winnt . h :
namespace utils { <nl> newBody . insert ( newBody . begin () + indexOfLastExpr , DRE ); <nl>  <nl> // Attach the new body ( note : it does dealloc / alloc of all nodes ) <nl> - CS -> setStmts ( S -> getASTContext (), newBody . data (), newBody . size ()); <nl> + CS -> setStmts ( S -> getASTContext (), & newBody . front (), newBody . size ()); <nl> if ( FoundAt ) <nl> * FoundAt = indexOfLastExpr ; <nl> return DRE ;
static int asf_parse_packet ( AVFormatContext * s , AVIOContext * pb , AVPacket * pkt ) <nl> continue ; <nl> } <nl> asf -> asf_st = & asf -> streams [ s -> streams [ asf -> stream_index ]-> id ]; <nl> - asf -> asf_st -> skip_to_key = 0 ; <nl> + if (! asf -> packet_frag_offset ) <nl> + asf -> asf_st -> skip_to_key = 0 ; <nl> } <nl> asf_st = asf -> asf_st ; <nl> av_assert0 ( asf_st );
void ff_MPV_frame_end ( MpegEncContext * s ) <nl> s -> avctx -> coded_frame = & s -> current_picture_ptr -> f ; <nl>  <nl> if ( s -> codec_id != CODEC_ID_H264 && s -> current_picture . f . reference ) { <nl> - ff_thread_report_progress (& s -> current_picture_ptr -> f , <nl> - s -> mb_height - 1 , 0 ); <nl> + ff_thread_report_progress (& s -> current_picture_ptr -> f , INT_MAX , 0 ); <nl> } <nl> } <nl> 
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> } <nl>  <nl> /* Generic DSP setup */ <nl> + memset (& dsp , 0 , sizeof ( dsp )); <nl> ff_dsputil_init (& dsp , avctx ); <nl> ff_set_cmp (& dsp , dsp . ildct_cmp , avctx -> ildct_cmp ); <nl> s -> get_pixels = dsp . get_pixels ;
int ff_mov_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> else <nl> samples_in_chunk = 1 ; <nl>  <nl> + if ( samples_in_chunk < 1 ) { <nl> + av_log ( s , AV_LOG_ERROR , " fatal error , input packet contains no samples \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> /* copy extradata if it exists */ <nl> if ( trk -> vos_len == 0 && par -> extradata_size > 0 && <nl> ! TAG_IS_AVCI ( trk -> tag ) &&
int av_get_audio_frame_duration ( AVCodecContext * avctx , int frame_bytes ) <nl> return frame_bytes * 8 / bps ; <nl> } <nl>  <nl> - if ( ch > 0 ) { <nl> + if ( ch > 0 && ch < INT_MAX / 16 ) { <nl> /* calc from frame_bytes and channels */ <nl> switch ( id ) { <nl> case AV_CODEC_ID_ADPCM_AFC :
static int decode_pic_hdr ( IVI5DecContext * ctx , AVCodecContext * avctx ) <nl> ctx -> gop_invalid = 0 ; <nl> } <nl>  <nl> + if ( ctx -> frame_type == FRAMETYPE_INTER_SCAL && ! ctx -> is_scalable ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Scalable inter frame in non scaleable stream \ n "); <nl> + ctx -> frame_type = FRAMETYPE_INTER ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( ctx -> frame_type != FRAMETYPE_NULL ) { <nl> ctx -> frame_flags = get_bits (& ctx -> gb , 8 ); <nl> 
static const int prof_h264_high [] = { FF_PROFILE_H264_CONSTRAINED_BASELINE , <nl> FF_PROFILE_UNKNOWN }; <nl> static const int prof_hevc_main [] = { FF_PROFILE_HEVC_MAIN , <nl> FF_PROFILE_UNKNOWN }; <nl> - static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN , <nl> - FF_PROFILE_HEVC_MAIN_10 , <nl> + static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN_10 , <nl> FF_PROFILE_UNKNOWN }; <nl>  <nl> static const dxva_mode dxva_modes [] = {
static int mpeg_decode_slice ( Mpeg1Context * s1 , int mb_y , <nl> break ; <nl> } <nl> } <nl> + if ( s -> mb_x >= ( unsigned ) s -> mb_width ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " initial skip overflow \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> s -> resync_mb_x = s -> mb_x ; <nl> s -> resync_mb_y = s -> mb_y = mb_y ;
decode_intra_mb : <nl>  <nl> // We assume these blocks are very rare so we do not optimize it . <nl> h -> intra_pcm_ptr = align_get_bits (& h -> gb ); <nl> + if ( get_bits_left (& h -> gb ) < mb_size ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , " Not enough data for an intra PCM block .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> skip_bits_long (& h -> gb , mb_size ); <nl>  <nl> // In deblocking , the quantizer is 0
int avcodec_copy_context ( AVCodecContext * dest , const AVCodecContext * src ) <nl> memcpy ( dest , src , sizeof (* dest )); <nl>  <nl> dest -> priv_data = orig_priv_data ; <nl> + <nl> + if ( orig_priv_data ) <nl> + av_opt_copy ( orig_priv_data , src -> priv_data ); <nl> + <nl> dest -> codec = orig_codec ; <nl>  <nl> /* set values specific to opened codecs back to their default state */
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
static int cdg_decode_frame ( AVCodecContext * avctx , <nl> int buf_size = avpkt -> size ; <nl> int ret ; <nl> uint8_t command , inst ; <nl> - uint8_t cdg_data [ CDG_DATA_SIZE ]; <nl> + uint8_t cdg_data [ CDG_DATA_SIZE ] = { 0 }; <nl> AVFrame * frame = data ; <nl> CDGraphicsContext * cc = avctx -> priv_data ; <nl> 
static int process_audio_header_elements ( AVFormatContext * s ) <nl> } <nl>  <nl> switch ( compression_type ) { <nl> + case 0 : ea -> audio_codec = CODEC_ID_PCM_S16LE ; break ; <nl> case 7 : ea -> audio_codec = CODEC_ID_ADPCM_EA ; break ; <nl> default : <nl> av_log ( s , AV_LOG_ERROR , " unsupported stream type ; compression_type =% i \ n ", compression_type );
struct dshow_ctx { <nl> HANDLE event ; <nl> AVPacketList * pktl ; <nl>  <nl> - unsigned int curbufsize ; <nl> + int64_t curbufsize ; <nl> unsigned int video_frame_num ; <nl>  <nl> IMediaControl * control ;
static void compute_pkt_fields ( AVFormatContext * s , AVStream * st , <nl> if ( pkt -> dts != AV_NOPTS_VALUE && <nl> pkt -> pts == AV_NOPTS_VALUE && <nl> st -> last_IP_duration > 0 && <nl> - ( st -> cur_dts - next_dts ) <= 1 && <nl> + (( uint64_t ) st -> cur_dts - ( uint64_t ) next_dts + 1 ) <= 2 && <nl> next_dts != next_pts && <nl> next_pts != AV_NOPTS_VALUE ) <nl> pkt -> pts = next_dts ;
static const int prof_h264_high [] = { FF_PROFILE_H264_CONSTRAINED_BASELINE , <nl> FF_PROFILE_UNKNOWN }; <nl> static const int prof_hevc_main [] = { FF_PROFILE_HEVC_MAIN , <nl> FF_PROFILE_UNKNOWN }; <nl> - static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN , <nl> - FF_PROFILE_HEVC_MAIN_10 , <nl> + static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN_10 , <nl> FF_PROFILE_UNKNOWN }; <nl>  <nl> static const dxva_mode dxva_modes [] = {
int ff_vorbis_len2vlc ( uint8_t * bits , uint32_t * codes , unsigned num ) <nl> exit_at_level [ i ] = 0 ; <nl> // construct code ( append 0s to end ) and introduce new exits <nl> for ( j = i + 1 ; j <= bits [ p ]; ++ j ) <nl> - exit_at_level [ j ] = code + ( 1 << ( j - 1 )); <nl> + exit_at_level [ j ] = code + ( 1u << ( j - 1 )); <nl> codes [ p ] = code ; <nl> } <nl> 
static void dca_exss_parse_header ( DCAContext * s ) <nl> } <nl> } <nl>  <nl> + av_assert0 ( num_assets > 0 ); // silence a warning <nl> + <nl> for ( i = 0 ; i < num_assets ; i ++) <nl> asset_size [ i ] = get_bits_long (& s -> gb , 16 + 4 * blownup ); <nl> 
int ff_frame_thread_init ( AVCodecContext * avctx ) <nl> p -> frame = av_frame_alloc (); <nl> if (! p -> frame ) { <nl> err = AVERROR ( ENOMEM ); <nl> + av_freep (& copy ); <nl> goto error ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , const uint8_t * databuf , <nl>  <nl>  <nl> /* set the bitstream reader at the start of the second Sound Unit */ <nl> - init_get_bits8 (& q -> gb , <nl> + ret = init_get_bits8 (& q -> gb , <nl> ptr1 , q -> decoded_bytes_buffer + js_block_align - ptr1 ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> /* Fill the Weighting coeffs delay buffer */ <nl> memmove ( q -> weighting_delay [ js_pair ], & q -> weighting_delay [ js_pair ][ 2 ],
static int rm_write_audio ( AVFormatContext * s , const uint8_t * buf , int size , int <nl>  <nl> /* XXX : suppress this malloc */ <nl> buf1 = av_malloc ( size * sizeof ( uint8_t )); <nl> + if (! buf1 ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> write_packet_header ( s , stream , size , !!( flags & AV_PKT_FLAG_KEY )); <nl> 
av_cold int ff_MPV_encode_init ( AVCodecContext * avctx ) <nl> if ( avctx -> max_b_frames > MAX_B_FRAMES ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Too many B - frames requested , maximum " <nl> " is % d .\ n ", MAX_B_FRAMES ); <nl> + avctx -> max_b_frames = MAX_B_FRAMES ; <nl> } <nl> s -> max_b_frames = avctx -> max_b_frames ; <nl> s -> codec_id = avctx -> codec -> id ;
static int read_header ( AVFormatContext * s , <nl> jvf -> audio_size = avio_rl32 ( pb ); <nl> jvf -> video_size = avio_rl32 ( pb ); <nl> jvf -> palette_size = avio_r8 ( pb ) ? 768 : 0 ; <nl> + jvf -> video_size = FFMIN ( FFMAX ( jvf -> video_size , 0 ), <nl> + INT_MAX - JV_PREAMBLE_SIZE - jvf -> palette_size ); <nl> if ( avio_r8 ( pb )) <nl> av_log ( s , AV_LOG_WARNING , " unsupported audio codec \ n "); <nl> jvf -> video_type = avio_r8 ( pb );
int ff_rate_control_init ( MpegEncContext * s ) <nl> rcc -> pass1_rc_eq_output_sum = 0 . 001 ; <nl> rcc -> pass1_wanted_bits = 0 . 001 ; <nl>  <nl> + if ( s -> avctx -> qblur > 1 . 0 ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " qblur too large \ n "); <nl> + return - 1 ; <nl> + } <nl> /* init stuff with the user specified complexity */ <nl> if ( s -> avctx -> rc_initial_cplx ){ <nl> for ( i = 0 ; i < 60 * 30 ; i ++){
static int decode_frame ( AVCodecContext * avctx , void * data , <nl> out : <nl>  <nl> s -> current_picture_ptr = NULL ; <nl> + s -> first_field = 0 ; <nl>  <nl> // FIXME factorize this with the output code below <nl> out = h -> delayed_pic [ 0 ];
static inline int get_duration ( AVIStream * ast , int len ) <nl> static int get_riff ( AVFormatContext * s , AVIOContext * pb ) <nl> { <nl> AVIContext * avi = s -> priv_data ; <nl> - char header [ 8 ]; <nl> + char header [ 8 ] = { 0 }; <nl> int i ; <nl>  <nl> /* check RIFF header */
static int parse_video_var ( AVFormatContext * avctx , AVStream * st , const char * nam <nl> st -> nb_frames = st -> duration = var_read_int ( pb , size ); <nl> } else if (! strcmp ( name , " COMPRESSION ")) { <nl> char * str = var_read_string ( pb , size ); <nl> + if (! str ) <nl> + return AVERROR_INVALIDDATA ; <nl> if (! strcmp ( str , " 1 ")) { <nl> st -> codec -> codec_id = AV_CODEC_ID_MVC1 ; <nl> } else if (! strcmp ( str , " 2 ")) {
static av_always_inline void predict ( PredictorState * ps , int * coef , <nl> if ( shift > 0 ) { <nl> * coef += ( unsigned )(( pv . mant + ( 1 << ( shift - 1 ))) >> shift ); <nl> } else <nl> - * coef += ( unsigned )( pv . mant << - shift ); <nl> + * coef += ( unsigned ) pv . mant << - shift ; <nl> } <nl> } <nl> 
extern void idct_add_altivec ( uint8_t * dest , int line_size , int16_t * block ); <nl>  <nl> void MPV_common_init_altivec ( MpegEncContext * s ) <nl> { <nl> + if ( mm_flags & MM_ALTIVEC == 0 ) return ; <nl> + <nl> if ( s -> avctx -> lowres == 0 ) <nl> { <nl> if (( s -> avctx -> idct_algo == FF_IDCT_AUTO ) ||
int ff_rtmp_packet_create ( RTMPPacket * pkt , int channel_id , RTMPPacketType type , <nl> int timestamp , int size ) <nl> { <nl> if ( size ) { <nl> - pkt -> data = av_malloc ( size ); <nl> + pkt -> data = av_realloc ( NULL , size ); <nl> if (! pkt -> data ) <nl> return AVERROR ( ENOMEM ); <nl> }
static int read_header ( AVFormatContext * s ) <nl>  <nl> b -> data_start = avio_tell ( s -> pb ); <nl>  <nl> - if (( major != 1 || minor ) && ! bfstm ) <nl> + if (! bfstm && ( major != 1 || minor )) <nl> avpriv_request_sample ( s , " Version % d .% d ", major , minor ); <nl>  <nl> return 0 ;
static void smc_decode_stream ( SmcContext * s ) <nl> } else <nl> color_table_index = CQUAD * s -> buf [ stream_ptr ++]; <nl>  <nl> - while ( n_blocks --) { <nl> + while ( n_blocks -- && stream_ptr + 3 < s -> size ) { <nl> color_flags = AV_RB32 (& s -> buf [ stream_ptr ]); <nl> stream_ptr += 4 ; <nl> /* flag mask actually acts as a bit shift count here */
int av_read_frame ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> ret = add_to_pktbuf (& s -> internal -> packet_buffer , pkt , <nl> & s -> internal -> packet_buffer_end , 1 ); <nl> + av_packet_unref ( pkt ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> }
av_cold void ff_fft_fixed_init_arm ( FFTContext * s ) <nl> s -> fft_calc = ff_fft_fixed_calc_neon ; <nl>  <nl> # if CONFIG_MDCT <nl> - if (! s -> inverse && s -> mdct_bits >= 5 ) { <nl> + if (! s -> inverse && s -> nbits >= 3 ) { <nl> s -> mdct_permutation = FF_MDCT_PERM_INTERLEAVE ; <nl> s -> mdct_calc = ff_mdct_fixed_calc_neon ; <nl> s -> mdct_calcw = ff_mdct_fixed_calcw_neon ;
void ff_frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> av_freep (& p -> avctx -> slice_offset ); <nl> } <nl>  <nl> + av_buffer_unref (& p -> avctx -> hw_frames_ctx ); <nl> + <nl> av_freep (& p -> avctx -> internal ); <nl> av_freep (& p -> avctx ); <nl> }
static int decode_frame ( NUTContext * nut , AVPacket * pkt , int frame_code ) <nl> if ( ret != size ) { <nl> if ( ret < 0 ) <nl> return ret ; <nl> - av_shrink_packet ( pkt , nut -> header_len [ header_idx ] + size ); <nl> } <nl> + av_shrink_packet ( pkt , nut -> header_len [ header_idx ] + ret ); <nl>  <nl> pkt -> stream_index = stream_id ; <nl> if ( stc -> last_flags & FLAG_KEY )
static int tcp_write_packet ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl> interleave_header [ 0 ] = '$'; <nl> interleave_header [ 1 ] = id ; <nl> AV_WB16 ( interleave_header + 2 , packet_len ); <nl> - url_write ( rt -> rtsp_hd , interleaved_packet , 4 + packet_len ); <nl> + url_write ( rt -> rtsp_hd_out , interleaved_packet , 4 + packet_len ); <nl> ptr += packet_len ; <nl> size -= packet_len ; <nl> }
int av_opt_set_dict ( void * obj , AVDictionary ** options ) <nl> AVDictionary * tmp = NULL ; <nl> int ret = 0 ; <nl>  <nl> + if (! options ) <nl> + return 0 ; <nl> + <nl> while (( t = av_dict_get (* options , "", t , AV_DICT_IGNORE_SUFFIX ))) { <nl> ret = av_opt_set ( obj , t -> key , t -> value , 0 ); <nl> if ( ret == AVERROR_OPTION_NOT_FOUND )
static int vorbis_packet ( AVFormatContext * s , int idx ) <nl> s -> streams [ idx ]-> start_time = os -> lastpts + first_duration ; <nl> if ( s -> streams [ idx ]-> duration ) <nl> s -> streams [ idx ]-> duration -= s -> streams [ idx ]-> start_time ; <nl> - s -> streams [ idx ]-> cur_dts = AV_NOPTS_VALUE ; <nl> priv -> final_pts = AV_NOPTS_VALUE ; <nl> avpriv_vorbis_parse_reset (& priv -> vp ); <nl> }
static int tscc2_decode_mb ( TSCC2Context * c , int * q , int vlc_set , <nl> if ( ac == 0x1000 ) <nl> ac = get_bits ( gb , 12 ); <nl> bpos += ac & 0xF ; <nl> - if ( bpos >= 64 ) <nl> + if ( bpos >= 16 ) <nl> return AVERROR_INVALIDDATA ; <nl> val = sign_extend ( ac >> 4 , 8 ); <nl> c -> block [ tscc2_zigzag [ bpos ++]] = val ;
void av_opt_freep_ranges ( AVOptionRanges ** rangesp ) <nl>  <nl> for ( i = 0 ; i < ranges -> nb_ranges * ranges -> nb_components ; i ++) { <nl> AVOptionRange * range = ranges -> range [ i ]; <nl> - av_freep (& range -> str ); <nl> - av_freep (& ranges -> range [ i ]); <nl> + if ( range ) { <nl> + av_freep (& range -> str ); <nl> + av_freep (& ranges -> range [ i ]); <nl> + } <nl> } <nl> av_freep (& ranges -> range ); <nl> av_freep ( rangesp );
static int parse_read_interval ( const char * interval_spec , <nl> } <nl> interval -> end = lli ; <nl> } else { <nl> + interval -> duration_frames = 0 ; <nl> ret = av_parse_time (& us , p , 1 ); <nl> if ( ret < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid interval end / duration specification '% s '\ n ", p );
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> LclDecContext * const c = avctx -> priv_data ; <nl> unsigned int pixel_ptr ; <nl> int row , col ; <nl> - unsigned char * encoded , * outptr ; <nl> + unsigned char * encoded = avpkt -> data , * outptr ; <nl> uint8_t * y_out , * u_out , * v_out ; <nl> unsigned int width = avctx -> width ; // Real image width <nl> unsigned int height = avctx -> height ; // Real image height
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterLink * outlink = inlink -> dst -> outputs [ 0 ]; <nl>  <nl> AVFilterBufferRef * out ; <nl> - int direct , c ; <nl> + int direct = 0 , c ; <nl>  <nl> if ( in -> perms & AV_PERM_WRITE ) { <nl> direct = 1 ;
static void hls_prediction_unit ( HEVCContext * s , int x0 , int y0 , <nl>  <nl> MvField * tab_mvf = s -> ref -> tab_mvf ; <nl> RefPicList * refPicList = s -> ref -> refPicList ; <nl> - HEVCFrame * ref0 , * ref1 ; <nl> + HEVCFrame * ref0 = NULL , * ref1 = NULL ; <nl> uint8_t * dst0 = POS ( 0 , x0 , y0 ); <nl> uint8_t * dst1 = POS ( 1 , x0 , y0 ); <nl> uint8_t * dst2 = POS ( 2 , x0 , y0 );
static int cine_read_header ( AVFormatContext * avctx ) <nl>  <nl> /* parse image offsets */ <nl> avio_seek ( pb , offImageOffsets , SEEK_SET ); <nl> - for ( i = 0 ; i < st -> duration ; i ++) <nl> + for ( i = 0 ; i < st -> duration ; i ++) { <nl> + if ( avio_feof ( pb )) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> av_add_index_entry ( st , avio_rl64 ( pb ), i , 0 , 0 , AVINDEX_KEYFRAME ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static av_cold int mpc8_decode_init ( AVCodecContext * avctx ) <nl> c -> frames = 1 << ( get_bits (& gb , 3 ) * 2 ); <nl>  <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ; <nl> - avctx -> channel_layout = ( avctx -> channels == 2 ) ? AV_CH_LAYOUT_STEREO : AV_CH_LAYOUT_MONO ; <nl> + avctx -> channel_layout = ( channels == 2 ) ? AV_CH_LAYOUT_STEREO : AV_CH_LAYOUT_MONO ; <nl> + avctx -> channels = channels ; <nl>  <nl> if ( vlc_initialized ) return 0 ; <nl> av_log ( avctx , AV_LOG_DEBUG , " Initing VLC \ n ");
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ; <nl>  <nl> - s -> filter_slice_edges = av_malloc ( ctb_count ); <nl> + s -> filter_slice_edges = av_mallocz ( ctb_count ); <nl> s -> tab_slice_address = av_malloc_array ( pic_size_in_ctb , <nl> sizeof (* s -> tab_slice_address )); <nl> s -> qp_y_tab = av_malloc_array ( pic_size_in_ctb ,
typedef struct WmallDecodeCtx { <nl> int8_t mclms_scaling ; <nl> int16_t mclms_coeffs [ 128 ]; <nl> int16_t mclms_coeffs_cur [ 4 ]; <nl> - int16_t mclms_prevvalues [ 64 ]; <nl> - int16_t mclms_updates [ 64 ]; <nl> + int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> + int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ; <nl>  <nl> int movave_scaling ;
static int decode_frame ( AVCodecContext * avctx , void * data , <nl> buf = av_realloc ( s -> packet_buffer , avpkt -> size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! buf ) <nl> return AVERROR ( ENOMEM ); <nl> + memset ( buf + avpkt -> size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> s -> packet_buffer = buf ; <nl> memcpy ( s -> packet_buffer , avpkt -> data , avpkt -> size ); <nl> if (( ret = init_get_bits8 ( gb , s -> packet_buffer , avpkt -> size )) < 0 )
header_fail : <nl> c -> height = 0 ; <nl> c -> tiles_x = <nl> c -> tiles_y = 0 ; <nl> + c -> tile_width = <nl> + c -> tile_height = 0 ; <nl> return ret ; <nl> } <nl> 
int ff_lpc_calc_coefs ( DSPContext * s , <nl> ref [ i ] = fabs ( lpc [ i ][ i ]); <nl> } else { <nl> LLSModel m [ 2 ]; <nl> - double var [ MAX_LPC_ORDER + 1 ], weight ; <nl> + double var [ MAX_LPC_ORDER + 1 ], av_uninit ( weight ); <nl>  <nl> for ( pass = 0 ; pass < use_lpc - 1 ; pass ++){ <nl> av_init_lls (& m [ pass & 1 ], max_order );
void checkasm_stack_clobber ( uint64_t clobber , ...); <nl> }\ <nl> } while ( 0 ) <nl> # else <nl> -# define bench_new (...) <nl> +# define bench_new (...) while ( 0 ) <nl> # endif <nl>  <nl> # endif
static int vqf_read_header ( AVFormatContext * s ) <nl>  <nl> header_size -= len ; <nl>  <nl> - } while ( header_size >= 0 ); <nl> + } while ( header_size >= 0 && ! url_feof ( s -> pb )); <nl>  <nl> switch ( rate_flag ) { <nl> case - 1 :
static int add_candidate_ref ( HEVCContext * s , RefPicList * list , <nl> { <nl> HEVCFrame * ref = find_ref_idx ( s , poc ); <nl>  <nl> - if ( ref == s -> ref ) <nl> + if ( ref == s -> ref || list -> nb_refs >= HEVC_MAX_REFS ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (! ref ) {
static void free_tables ( H264Context * h ){ <nl> av_freep (& h -> mb2b_xy ); <nl> av_freep (& h -> mb2b8_xy ); <nl>  <nl> - for ( i = 0 ; i < h -> s . avctx -> thread_count ; i ++) { <nl> + for ( i = 0 ; i < MAX_THREADS ; i ++) { <nl> hx = h -> thread_context [ i ]; <nl> if (! hx ) continue ; <nl> av_freep (& hx -> top_borders [ 1 ]);
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl>  <nl> h -> b_stride = 4 * s -> mb_width ; <nl>  <nl> - ff_h264_alloc_tables ( h ); <nl> + if ( ff_h264_alloc_tables ( h ) < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " svq3 memory allocation failed \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> return 0 ;
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> end = ptr + FFMIN ( 2 + text_length , avpkt -> size ); <nl> ptr += 2 ; <nl>  <nl> + mov_text_cleanup ( m ); <nl> + <nl> tsmb_size = 0 ; <nl> m -> tracksize = 2 + text_length ; <nl> m -> style_entries = 0 ;
void ff_h264_direct_ref_list_init ( H264Context * const h ){ <nl> h -> col_fieldoff = 2 * h -> ref_list [ 1 ][ 0 ]. f . reference - 3 ; <nl> } <nl>  <nl> - if ( cur -> f . pict_type != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> + if ( h -> slice_type_nos != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> return ; <nl>  <nl> for ( list = 0 ; list < 2 ; list ++){
static uint32_t softfloat_mul ( uint32_t x , uint64_t mantissa ) <nl>  <nl> static uint8_t lag_calc_zero_run ( int8_t x ) <nl> { <nl> - return ( x << 1 ) ^ ( x >> 7 ); <nl> + return ( x * 2 ) ^ ( x >> 7 ); <nl> } <nl>  <nl> static int lag_decode_prob ( GetBitContext * gb , uint32_t * value )
AVFilterBufferRef * ff_default_get_audio_buffer ( AVFilterLink * link , int perms , <nl> if (! samplesref ) <nl> goto fail ; <nl>  <nl> + samplesref -> audio -> sample_rate = link -> sample_rate ; <nl> + <nl> av_freep (& data ); <nl>  <nl> fail :
static int url_alloc_for_protocol ( URLContext ** puc , struct URLProtocol * up , <nl> av_log ( uc , AV_LOG_ERROR , " Error parsing options string % s \ n ", start ); <nl> av_freep (& uc -> priv_data ); <nl> av_freep (& uc ); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl> memmove ( start , key + 1 , strlen ( key ));
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> avio_read ( pb , buf , 4 ); <nl> buf [ 4 ] = 0 ; <nl> } else { <nl> + AV_WL32 ( buf , 0 ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */ <nl> ast -> deint_id = AV_RL32 ( buf ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */
void avsubtitle_free ( AVSubtitle * sub ) <nl>  <nl> static int do_decode ( AVCodecContext * avctx , AVPacket * pkt ) <nl> { <nl> - int got_frame ; <nl> + int got_frame = 0 ; <nl> int ret ; <nl>  <nl> av_assert0 (! avctx -> internal -> buffer_frame -> buf [ 0 ]);
static int mov_read_stts ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> static void mov_update_dts_shift ( MOVStreamContext * sc , int duration ) <nl> { <nl> if ( duration < 0 ) { <nl> + if ( duration == INT_MIN ) { <nl> + av_log ( NULL , AV_LOG_WARNING , " mov_update_dts_shift (): dts_shift set to % d \ n ", INT_MAX ); <nl> + duration ++; <nl> + } <nl> sc -> dts_shift = FFMAX ( sc -> dts_shift , - duration ); <nl> } <nl> }
static int mpeg_mux_write_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> stream -> next_packet = & stream -> premux_packet ; <nl> * stream -> next_packet = <nl> pkt_desc = av_mallocz ( sizeof ( PacketDesc )); <nl> + if (! pkt_desc ) <nl> + return AVERROR ( ENOMEM ); <nl> pkt_desc -> pts = pts ; <nl> pkt_desc -> dts = dts ; <nl> pkt_desc -> unwritten_size =
reload : <nl>  <nl> return ret ; <nl> } <nl> - if ( c -> http_persistent ) { <nl> + if ( c -> http_persistent && av_strstart ( seg -> url , " http ", NULL )) { <nl> v -> input_read_done = 1 ; <nl> } else { <nl> ff_format_io_close ( v -> parent , & v -> input );
int main ( int argc , char ** argv ) <nl> goto end ; <nl> } <nl> w_name = av_strtok ( print_format , "=", & buf ); <nl> + if (! w_name ) { <nl> + av_log ( NULL , AV_LOG_ERROR , <nl> + " No name specified for the output format \ n "); <nl> + ret = AVERROR ( EINVAL ); <nl> + goto end ; <nl> + } <nl> w_args = buf ; <nl>  <nl> if ( show_data_hash ) {
static int v410_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 * avctx -> height * avctx -> width ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> pic -> reference = 0 ; <nl>  <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) {
static int build_table ( VLC * vlc , int table_nb_bits , int nb_codes , <nl> VLC_TYPE (* table )[ 2 ]; <nl>  <nl> table_size = 1 << table_nb_bits ; <nl> + if ( table_nb_bits > 30 ) <nl> + return - 1 ; <nl> table_index = alloc_table ( vlc , table_size , flags & INIT_VLC_USE_NEW_STATIC ); <nl> av_dlog ( NULL , " new table index =% d size =% d \ n ", table_index , table_size ); <nl> if ( table_index < 0 )
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf_size = FFMIN ( buf_size , s -> max_framesize - s -> bitstream_size ); <nl> input_buf_size = buf_size ; <nl>  <nl> - if ( s -> bitstream_index + s -> bitstream_size + buf_size > <nl> + if ( s -> bitstream_index + s -> bitstream_size + buf_size + FF_INPUT_BUFFER_PADDING_SIZE > <nl> s -> allocated_bitstream_size ) { <nl> memmove ( s -> bitstream , & s -> bitstream [ s -> bitstream_index ], <nl> s -> bitstream_size );
static av_cold int truemotion1_decode_init ( AVCodecContext * avctx ) <nl> /* there is a vertical predictor for each pixel in a line ; each vertical <nl> * predictor is 0 to start with */ <nl> av_fast_malloc (& s -> vert_pred , & s -> vert_pred_size , s -> avctx -> width * sizeof ( unsigned int )); <nl> - if (! s -> vert_pred ) <nl> + if (! s -> vert_pred ) { <nl> + av_frame_free (& s -> frame ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int handle_packet ( MpegTSContext * ts , const uint8_t * packet ) <nl> return 0 ; <nl>  <nl> pos = avio_tell ( ts -> stream -> pb ); <nl> - av_assert0 ( pos >= TS_PACKET_SIZE ); <nl> - ts -> pos47_full = pos - TS_PACKET_SIZE ; <nl> + if ( pos >= 0 ) { <nl> + av_assert0 ( pos >= TS_PACKET_SIZE ); <nl> + ts -> pos47_full = pos - TS_PACKET_SIZE ; <nl> + } <nl>  <nl> if ( tss -> type == MPEGTS_SECTION ) { <nl> if ( is_start ) {
static void apply_independent_coupling_fixed ( AACContext * ac , <nl> else { <nl> for ( i = 0 ; i < len ; i ++) { <nl> tmp = ( int )((( int64_t ) src [ i ] * c + ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ i ] += tmp << shift ; <nl> + dest [ i ] += tmp * ( 1 << shift ); <nl> } <nl> } <nl> }
static int v410_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 * avctx -> height * avctx -> width ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> pic -> reference = 0 ; <nl>  <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) {
static int config_props ( AVFilterLink * inlink ) <nl> double res ; <nl>  <nl> /* create the parsed expression */ <nl> + av_expr_free ( s -> comp_expr [ comp ]); <nl> + s -> comp_expr [ comp ] = NULL ; <nl> ret = av_expr_parse (& s -> comp_expr [ comp ], s -> comp_expr_str [ comp ], <nl> var_names , funcs1_names , funcs1 , NULL , NULL , 0 , ctx ); <nl> if ( ret < 0 ) {
static int matroska_parse_rm_audio ( MatroskaDemuxContext * matroska , <nl> } <nl> memcpy ( track -> audio . buf + y * w , data , w ); <nl> } else { <nl> - if ( size < sps * w / sps || h <= 0 ) { <nl> + if ( size < sps * w / sps || h <= 0 || w % sps ) { <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Corrupt generic RM - style audio packet size \ n "); <nl> return AVERROR_INVALIDDATA ;
static int matroska_decode_buffer ( uint8_t ** buf , int * buf_size , <nl> int result = 0 ; <nl> int olen ; <nl>  <nl> + if ( pkt_size >= 10000000 ) <nl> + return - 1 ; <nl> + <nl> switch ( encodings [ 0 ]. compression . algo ) { <nl> case MATROSKA_TRACK_ENCODING_COMP_HEADERSTRIP : <nl> return encodings [ 0 ]. compression . settings . size ;
av_cold int ff_ivi_decode_close ( AVCodecContext * avctx ) <nl> if ( ctx -> mb_vlc . cust_tab . table ) <nl> ff_free_vlc (& ctx -> mb_vlc . cust_tab ); <nl>  <nl> + if ( ctx -> blk_vlc . cust_tab . table ) <nl> + ff_free_vlc (& ctx -> blk_vlc . cust_tab ); <nl> + <nl> av_frame_free (& ctx -> p_frame ); <nl>  <nl> return 0 ;
typedef struct { <nl>  <nl> int bit_index ; <nl>  <nl> - int16_t curtileno ; <nl> + int curtileno ; <nl>  <nl> J2kTile * tile ; <nl> } J2kDecoderContext ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> return 0 ; <nl>  <nl> failed_alloc : <nl> - av_destruct_packet ( pkt ); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl>  <nl> int av_copy_packet_side_data ( AVPacket * pkt , AVPacket * src ) <nl> return 0 ; <nl>  <nl> failed_alloc : <nl> - av_destruct_packet ( pkt ); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
pp_mode * pp_get_mode_by_name_and_quality ( const char * name , int quality ) <nl> if ( filterToken == NULL ) break ; <nl> p += strlen ( filterToken ) + 1 ; // p points to next filterToken <nl> filterName = strtok ( filterToken , optionDelimiters ); <nl> + if ( filterName == NULL ) { <nl> + ppMode -> error ++; <nl> + break ; <nl> + } <nl> av_log ( NULL , AV_LOG_DEBUG , " pp : % s ::% s \ n ", filterToken , filterName ); <nl>  <nl> if (* filterName == '-'){
AVCodec ff_jpegls_encoder = { <nl> AV_PIX_FMT_GRAY8 , AV_PIX_FMT_GRAY16 , <nl> AV_PIX_FMT_NONE <nl> }, <nl> + . caps_internal = FF_CODEC_CAP_INIT_THREADSAFE | <nl> + FF_CODEC_CAP_INIT_CLEANUP , <nl> };
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> av_log ( avctx , AV_LOG_ERROR , " Custom quant matrix encountered !\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( band -> quant_mat > 21 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid quant matrix encountered !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> /* decode block huffman codebook */
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> sub_type = avio_r8 ( s -> pb ); <nl> type = avio_r8 ( s -> pb ); <nl> size = avio_rl16 ( s -> pb ); <nl> + if ( size < 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> avs -> remaining_frame_size -= size ; <nl>  <nl> switch ( type ) {
static void update_stream_timings ( AVFormatContext * ic ) <nl> end_time1 = av_rescale_q_rnd ( st -> duration , st -> time_base , <nl> AV_TIME_BASE_Q , <nl> AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX ); <nl> - if ( end_time1 != AV_NOPTS_VALUE ) { <nl> + if ( end_time1 != AV_NOPTS_VALUE && start_time1 <= INT64_MAX - end_time1 ) { <nl> end_time1 += start_time1 ; <nl> end_time = FFMAX ( end_time , end_time1 ); <nl> }
static int parse_video_info ( AVIOContext * pb , AVStream * st ) <nl> st -> codecpar -> codec_id = ff_codec_get_id ( ff_codec_bmp_tags , tag ); <nl> size_bmp = FFMAX ( size_asf , size_bmp ); <nl>  <nl> - if ( size_bmp > BMP_HEADER_SIZE ) { <nl> + if ( size_bmp > BMP_HEADER_SIZE && <nl> + size_bmp < INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) { <nl> int ret ; <nl> st -> codecpar -> extradata_size = size_bmp - BMP_HEADER_SIZE ; <nl> if (!( st -> codecpar -> extradata = av_malloc ( st -> codecpar -> extradata_size +
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> } <nl>  <nl> if ( S ) { <nl> - S <<= s -> float_shift ; <nl> + S *= 1 << s -> float_shift ; <nl> sign = S < 0 ; <nl> if ( sign ) <nl> S = - S ;
int ff_h264_frame_start ( H264Context * h ) <nl>  <nl> if (( ret = alloc_picture ( h , pic )) < 0 ) <nl> return ret ; <nl> - if (! h -> sync && ! h -> avctx -> hwaccel ) <nl> + if (! h -> sync && ! h -> avctx -> hwaccel && <nl> + !( h -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU )) <nl> avpriv_color_frame (& pic -> f , c ); <nl>  <nl> h -> cur_pic_ptr = pic ;
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> switch ( type ) { <nl> case AVS_PALETTE : <nl> + if ( size - 4 > sizeof ( palette )) <nl> + return AVERROR_INVALIDDATA ; <nl> ret = avio_read ( s -> pb , palette , size - 4 ); <nl> if ( ret < size - 4 ) <nl> return AVERROR ( EIO );
static int pcx_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> ptr = p -> data [ 0 ]; <nl> stride = p -> linesize [ 0 ]; <nl>  <nl> - scanline = av_malloc ( bytes_per_scanline ); <nl> + scanline = av_malloc ( bytes_per_scanline + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! scanline ) <nl> return AVERROR ( ENOMEM ); <nl> 
int ff_MPV_lowest_referenced_row ( MpegEncContext * s , int dir ) <nl> int my_max = INT_MIN , my_min = INT_MAX , qpel_shift = ! s -> quarter_sample ; <nl> int my , off , i , mvs ; <nl>  <nl> - if ( s -> picture_structure != PICT_FRAME ) goto unhandled ; <nl> + if ( s -> picture_structure != PICT_FRAME || s -> mcsel ) goto unhandled ; <nl>  <nl> switch ( s -> mv_type ) { <nl> case MV_TYPE_16X16 :
static int decode_subframe_fixed ( FLACContext * s , int32_t * decoded , <nl> int pred_order , int bps ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> - int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl> + unsigned av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ); <nl> + int i ; <nl> int ret ; <nl>  <nl> /* warm up samples */
static int gxf_write_header ( AVFormatContext * s ) <nl> if ( ff_audio_interleave_init ( s , GXF_samples_per_frame , ( AVRational ){ 1 , 48000 }) < 0 ) <nl> return - 1 ; <nl>  <nl> - if ( tcr ) <nl> + if ( tcr && vsc ) <nl> gxf_init_timecode ( s , & gxf -> tc , tcr -> value , vsc -> fields ); <nl>  <nl> gxf_init_timecode_track (& gxf -> timecode_track , vsc );
static void opt_output_file ( const char * filename ) <nl>  <nl> audio_enc -> bit_rate = audio_bit_rate ; <nl> audio_enc -> sample_rate = audio_sample_rate ; <nl> + audio_enc -> strict_std_compliance = strict ; <nl> /* For audio codecs other than AC3 we limit */ <nl> /* the number of coded channels to stereo */ <nl> if ( audio_channels > 2 && codec_id != CODEC_ID_AC3 ) {
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> url_fseek ( s -> pb , st -> index_entries [ index ]. pos , SEEK_SET ); <nl> matroska -> skip_to_keyframe = !( flags & AVSEEK_FLAG_ANY ); <nl> matroska -> skip_to_stream = st ; <nl> + matroska -> done = 0 ; <nl> av_update_cur_dts ( s , st , st -> index_entries [ index ]. timestamp ); <nl> return 0 ; <nl> }
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl>  <nl> av_freep (& h -> rbsp_buffer [ 0 ]); <nl> av_freep (& h -> rbsp_buffer [ 1 ]); <nl> + ff_h264_unref_picture ( h , & h -> last_pic_for_ec ); <nl> memcpy ( h , h1 , offsetof ( H264Context , intra_pcm_ptr )); <nl> memcpy (& h -> cabac , & h1 -> cabac , <nl> sizeof ( H264Context ) - offsetof ( H264Context , cabac ));
static int16_t g726_decode ( G726Context * c , int I ) <nl> c -> se += mult ( i2f ( c -> a [ i ] >> 2 , & f ), & c -> sr [ i ]); <nl> c -> se >>= 1 ; <nl>  <nl> - return av_clip ( re_signal << 2 , - 0xffff , 0xffff ); <nl> + return av_clip ( re_signal * 4 , - 0xffff , 0xffff ); <nl> } <nl>  <nl> static av_cold int g726_reset ( G726Context * c )
static int fraps2_decode_plane ( FrapsContext * s , uint8_t * dst , int stride , int w , <nl> else if ( Uoff ) dst [ i ] += 0x80 ; <nl> } <nl> dst += stride ; <nl> + if ( get_bits_left (& gb ) < 0 ) <nl> + return - 1 ; <nl> } <nl> free_vlc (& vlc ); <nl> return 0 ;
static int write_adaptation_set ( AVFormatContext * s , int as_index ) <nl> ret = write_representation ( s , s -> streams [ as -> streams [ i ]], <nl> representation_id , ! width_in_as , <nl> ! height_in_as , ! sample_rate_in_as ); <nl> - if ( ret ) return ret ; <nl> av_free ( representation_id ); <nl> + if ( ret ) return ret ; <nl> } <nl> avio_printf ( s -> pb , "</ AdaptationSet >\ n "); <nl> return 0 ;
static int encode_block ( SVQ1Context * s , uint8_t * src , uint8_t * ref , uint8_t * dec <nl> } <nl>  <nl> best_count = 0 ; <nl> - best_score -= (( block_sum [ 0 ]* block_sum [ 0 ])>>( level + 3 )); <nl> + best_score -= ( int )((( unsigned ) block_sum [ 0 ]* block_sum [ 0 ])>>( level + 3 )); <nl> best_mean = ( block_sum [ 0 ] + ( size >> 1 )) >> ( level + 3 ); <nl>  <nl> if ( level < 4 ){
static int vorbis_parse ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl>  <nl> if (! s -> vp && avctx -> extradata && avctx -> extradata_size ) { <nl> s -> vp = av_vorbis_parse_init ( avctx -> extradata , avctx -> extradata_size ); <nl> - if (! s -> vp ) <nl> - goto end ; <nl> } <nl> + if (! s -> vp ) <nl> + goto end ; <nl>  <nl> if (( duration = av_vorbis_parse_frame ( s -> vp , buf , buf_size )) >= 0 ) <nl> s1 -> duration = duration ;
static double bessel ( double x ){ <nl> lastv = v ; <nl> t *= x * inv [ i ]; <nl> v += t ; <nl> + av_assert2 ( i < 99 ); <nl> } <nl> return v ; <nl> }
av_cold struct FFPsyPreprocessContext * ff_psy_preprocess_init ( AVCodecContext * av <nl> if (! cutoff_coeff && avctx -> codec_id == AV_CODEC_ID_AAC ) <nl> cutoff_coeff = 2 . 0 * AAC_CUTOFF ( avctx ) / avctx -> sample_rate ; <nl>  <nl> - if ( cutoff_coeff ) <nl> + if ( cutoff_coeff && cutoff_coeff < 0 . 98 ) <nl> ctx -> fcoeffs = ff_iir_filter_init_coeffs ( avctx , FF_FILTER_TYPE_BUTTERWORTH , <nl> FF_FILTER_MODE_LOWPASS , FILT_ORDER , <nl> cutoff_coeff , 0 . 0 , 0 . 0 );
int ff_wms_parse_sdp_a_line ( AVFormatContext * s , const char * p ) <nl> { <nl> int ret = 0 ; <nl> if ( av_strstart ( p , " pgmpu : data : application / vnd . ms . wms - hdr . asfv1 ; base64 ,", & p )) { <nl> - AVIOContext pb ; <nl> + AVIOContext pb = { 0 }; <nl> RTSPState * rt = s -> priv_data ; <nl> AVDictionary * opts = NULL ; <nl> int len = strlen ( p ) * 6 / 8 ;
redo_frame : <nl> || !( height >>( s -> chroma_v_shift + s -> spatial_decomposition_count ))) <nl> s -> spatial_decomposition_count --; <nl>  <nl> + if ( s -> spatial_decomposition_count <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Resolution too low \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> s -> m . pict_type = pic -> pict_type ; <nl> s -> qbias = pic -> pict_type == AV_PICTURE_TYPE_P ? 2 : 0 ; <nl> 
static int get_cookies ( HTTPContext * s , char ** cookies , const char * path , <nl> } <nl>  <nl> done_cookie : <nl> - av_free ( cdomain ); <nl> - av_free ( cpath ); <nl> - av_free ( cvalue ); <nl> + av_freep (& cdomain ); <nl> + av_freep (& cpath ); <nl> + av_freep (& cvalue ); <nl> if ( ret < 0 ) { <nl> if (* cookies ) av_freep ( cookies ); <nl> av_free ( cset_cookies );
typedef struct Jpeg2000Component { <nl> /* misc tools */ <nl> static inline int ff_jpeg2000_ceildivpow2 ( int a , int b ) <nl> { <nl> - return -((( int64_t )(- a )) >> b ); <nl> + return -((-( int64_t ) a ) >> b ); <nl> } <nl>  <nl> static inline int ff_jpeg2000_ceildiv ( int a , int b )
static int decode_subframe_fixed ( FLACContext * s , int32_t * decoded , <nl> int pred_order , int bps ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> - int a , b , c , d , i ; <nl> + int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl>  <nl> /* warm up samples */ <nl> for ( i = 0 ; i < pred_order ; i ++) {
static int set_params ( AVFilterContext * ctx , const char * params ) <nl> Frei0rContext * frei0r = ctx -> priv ; <nl> int i ; <nl>  <nl> + if (! params ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < frei0r -> plugin_info . num_params ; i ++) { <nl> f0r_param_info_t info ; <nl> char * param ;
int ff_mpeg_update_thread_context ( AVCodecContext * dst , <nl> s -> input_picture_number = s1 -> input_picture_number ; <nl>  <nl> av_assert0 (! s -> picture || s -> picture != s1 -> picture ); <nl> + if ( s -> picture ) <nl> for ( i = 0 ; i < MAX_PICTURE_COUNT ; i ++) { <nl> ff_mpeg_unref_picture ( s , & s -> picture [ i ]); <nl> if ( s1 -> picture [ i ]. f . data [ 0 ] &&
static void filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * samplesref ) <nl> for ( i = 0 ; i < ctx -> nb_outputs ; i ++) <nl> ff_filter_samples ( inlink -> dst -> outputs [ i ], <nl> avfilter_ref_buffer ( samplesref , ~ AV_PERM_WRITE )); <nl> + avfilter_unref_buffer ( samplesref ); <nl> } <nl>  <nl> AVFilter avfilter_af_asplit = {
static inline int mdec_decode_block_intra ( MDECContext * a , int16_t * block , int n ) <nl> if ( diff >= 0xffff ) <nl> return AVERROR_INVALIDDATA ; <nl> a -> last_dc [ component ] += diff ; <nl> - block [ 0 ] = a -> last_dc [ component ] << 3 ; <nl> + block [ 0 ] = a -> last_dc [ component ] * ( 1 << 3 ); <nl> } <nl>  <nl> i = 0 ;
void av_frame_unref ( AVFrame * frame ) <nl> { <nl> int i ; <nl>  <nl> + if (! frame ) <nl> + return ; <nl> + <nl> wipe_side_data ( frame ); <nl>  <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( frame -> buf ); i ++)
static int matroska_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> ret = matroska_parse_cluster ( matroska ); <nl> } <nl>  <nl> - if ( ret == AVERROR_INVALIDDATA ) { <nl> + if ( ret == AVERROR_INVALIDDATA && pkt -> data ) { <nl> pkt -> flags |= AV_PKT_FLAG_CORRUPT ; <nl> return 0 ; <nl> }
static av_cold int bktr_init ( const char * video_device , int width , int height , <nl> long ioctl_frequency ; <nl> char * arg ; <nl> int c ; <nl> - struct sigaction act = { 0 }, old ; <nl> + struct sigaction act = { { 0 } }, old ; <nl>  <nl> if ( idev < 0 || idev > 4 ) <nl> {
static int hls_write_header ( AVFormatContext * s ) <nl> int ret , i ; <nl> char * p ; <nl> const char * pattern = "% d . ts "; <nl> - int basename_size = strlen ( s -> filename ) + strlen ( pattern ); <nl> + int basename_size = strlen ( s -> filename ) + strlen ( pattern ) + 1 ; <nl>  <nl> hls -> number = 0 ; <nl> 
static int encode_audio_frame ( AVFormatContext * s , OutputStream * ost , <nl> pkt . data = NULL ; <nl> pkt . size = 0 ; <nl>  <nl> - if ( buf ) { <nl> + if ( buf && buf_size ) { <nl> if (! ost -> output_frame ) { <nl> ost -> output_frame = avcodec_alloc_frame (); <nl> if (! ost -> output_frame ) {
static void start_children ( FFServerStream * feed ) <nl> av_free ( pathname ); <nl> _exit ( 1 ); <nl> } <nl> + av_free ( pathname ); <nl> } <nl>  <nl> /* open a listening socket */
static inline int l3_unscale ( int value , int exponent ) <nl> if ( e < 1 ) <nl> av_log ( NULL , AV_LOG_WARNING , " l3_unscale : e is % d \ n ", e ); <nl> # endif <nl> - if ( e > 31 ) <nl> + if ( e > ( SUINT ) 31 ) <nl> return 0 ; <nl> m = ( m + ( 1 << ( e - 1 ))) >> e ; <nl> 
 <nl> static av_cold int encode_init ( AVCodecContext * avctx ) <nl> { <nl> + if ( avctx -> width > 65535 || avctx -> height > 65535 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " Unsupported resolution % dx % d .\ n ", avctx -> width , avctx -> height ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> avctx -> coded_frame = av_frame_alloc (); <nl> if (! avctx -> coded_frame ) <nl> return AVERROR ( ENOMEM );
static int guess_ni_flag ( AVFormatContext * s ){ <nl> if ( last_start > first_end ) <nl> return 1 ; <nl> idx = av_mallocz ( sizeof (* idx ) * s -> nb_streams ); <nl> - for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1 ) { <nl> + for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1LU ) { <nl> int64_t max_dts = INT64_MIN / 2 , min_dts = INT64_MAX / 2 ; <nl> min_pos = INT64_MAX ; <nl> 
int ff_h264_decode_sei ( H264Context * h ){ <nl> size += show_bits (& s -> gb , 8 ); <nl> } while ( get_bits (& s -> gb , 8 ) == 255 ); <nl>  <nl> + if ( s -> avctx -> debug & FF_DEBUG_STARTCODE ) <nl> + av_log ( h -> s . avctx , AV_LOG_DEBUG , " SEI % d len :% d \ n ", type , size ); <nl> + <nl> switch ( type ){ <nl> case SEI_TYPE_PIC_TIMING : // Picture timing SEI <nl> if ( decode_picture_timing ( h ) < 0 )
static inline void dxt1_decode_pixels ( const uint8_t * s , uint32_t * d , <nl> unsigned int qstride , unsigned int flag , <nl> uint64_t alpha ) { <nl> - unsigned int x , y , c0 , c1 , a = (! flag * 255 ) << 24 ; <nl> + unsigned int x , y , c0 , c1 , a = (! flag * 255u ) << 24 ; <nl> unsigned int rb0 , rb1 , rb2 , rb3 , g0 , g1 , g2 , g3 ; <nl> uint32_t colors [ 4 ], pixels ; <nl> 
static int imc_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> IMCContext * q = avctx -> priv_data ; <nl>  <nl> - LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 ]); <nl> + LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 + FF_INPUT_BUFFER_PADDING_SIZE / 2 ]); <nl>  <nl> if ( buf_size < IMC_BLOCK_SIZE * avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame too small !\ n ");
int av_samples_get_buffer_size ( int * linesize , int nb_channels , int nb_samples , <nl>  <nl> /* auto - select alignment if not specified */ <nl> if (! align ) { <nl> + if ( nb_samples > INT_MAX - 31 ) <nl> + return AVERROR ( EINVAL ); <nl> align = 1 ; <nl> nb_samples = FFALIGN ( nb_samples , 32 ); <nl> }
static int decompress_i ( AVCodecContext * avctx , uint32_t * dst , int linesize ) <nl> clr = ( b << 16 ) + ( g << 8 ) + r ; <nl> k += run ; <nl> while ( run -- > 0 ) { <nl> + if ( y >= avctx -> height ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> dst [ y * linesize + x ] = clr ; <nl> lx = x ; <nl> ly = y ;
static ResampleContext * resample_init ( ResampleContext * c , int out_rate , int in_r <nl> av_assert0 ( 0 ); <nl> } <nl>  <nl> + if ( filter_size / factor > INT32_MAX / 256 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " Filter length too large \ n "); <nl> + goto error ; <nl> + } <nl> + <nl> c -> phase_shift = phase_shift ; <nl> c -> phase_mask = phase_count - 1 ; <nl> c -> linear = linear ;
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> return - 1 ; <nl> } <nl>  <nl> + if ( wc -> ch_offset >= avctx -> channels ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " too many channels \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> memset ( s -> decorr , 0 , MAX_TERMS * sizeof ( Decorr )); <nl> memset ( s -> ch , 0 , sizeof ( s -> ch )); <nl> s -> extra_bits = 0 ;
int ff_mpeg1_find_frame_end ( ParseContext * pc , const uint8_t * buf , int buf_size , <nl> pc -> frame_start_found = 4 ; <nl> } <nl> if ( state == SEQ_END_CODE ) { <nl> + pc -> frame_start_found = 0 ; <nl> pc -> state =- 1 ; <nl> return i + 1 ; <nl> }
int avpriv_adx_decode_header ( AVCodecContext * avctx , const uint8_t * buf , <nl>  <nl> /* channels */ <nl> avctx -> channels = buf [ 7 ]; <nl> - if ( avctx -> channels > 2 ) <nl> + if ( avctx -> channels <= 0 || avctx -> channels > 2 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> /* sample rate */
static void decode_nal_sei_decoded_picture_hash ( HEVCContext * s ) <nl> static void decode_nal_sei_frame_packing_arrangement ( HEVCContext * s ) <nl> { <nl> GetBitContext * gb = & s -> HEVClc -> gb ; <nl> - int cancel , type , quincunx , content ; <nl> + int cancel ; <nl> + int quincunx = 0 ; <nl> + int content = - 1 ; <nl> + int type = - 1 ; <nl>  <nl> get_ue_golomb ( gb ); // frame_packing_arrangement_id <nl> cancel = get_bits1 ( gb ); // frame_packing_cancel_flag
static void mp_decode_frame_helper ( MotionPixelsContext * mp , GetBitContext * gb ) <nl> YuvPixel p ; <nl> int y , y0 ; <nl>  <nl> + av_assert1 ( mp -> changes_map [ 0 ]); <nl> + <nl> for ( y = 0 ; y < mp -> avctx -> height ; ++ y ) { <nl> if ( mp -> changes_map [ y * mp -> avctx -> width ] != 0 ) { <nl> memset ( mp -> gradient_scale , 1 , sizeof ( mp -> gradient_scale ));
static int mov_read_close ( AVFormatContext * s ) <nl> av_freep (& sc -> rap_group ); <nl> av_freep (& sc -> display_matrix ); <nl>  <nl> - for ( j = 0 ; j < sc -> stsd_count ; j ++) <nl> - av_free ( sc -> extradata [ j ]); <nl> + if ( sc -> extradata ) <nl> + for ( j = 0 ; j < sc -> stsd_count ; j ++) <nl> + av_free ( sc -> extradata [ j ]); <nl> av_freep (& sc -> extradata ); <nl> av_freep (& sc -> extradata_size ); <nl> 
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> ThreadFrame tframe = { . f = frame }; <nl> WavpackFrameContext * s ; <nl> GetByteContext gb ; <nl> - void * samples_l , * samples_r ; <nl> + void * samples_l = NULL , * samples_r = NULL ; <nl> int ret ; <nl> int got_terms = 0 , got_weights = 0 , got_samples = 0 , <nl> got_entropy = 0 , got_bs = 0 , got_float = 0 , got_hybrid = 0 ;
static int get_cookies ( HTTPContext * s , char ** cookies , const char * path , <nl> snprintf (* cookies , str_size , "% s ; % s =% s ", tmp , cookie_entry -> key , cookie_entry -> value ); <nl> av_free ( tmp ); <nl> } <nl> + av_dict_free (& cookie_params ); <nl> } <nl>  <nl> av_free ( set_cookies );
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> ADD_METADATA ( count , " ModelTiepointTag ", NULL ); <nl> break ; <nl> case TIFF_GEO_KEY_DIRECTORY : <nl> + if ( s -> geotag_count ) { <nl> + avpriv_request_sample ( s -> avctx , " Multiple geo key directories \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> ADD_METADATA ( 1 , " GeoTIFF_Version ", NULL ); <nl> ADD_METADATA ( 2 , " GeoTIFF_Key_Revision ", "."); <nl> s -> geotag_count = ff_tget_short (& s -> gb , s -> le );
static AVIOContext * wtvfile_open_sector ( int first_sector , uint64_t length , int <nl> return NULL ; <nl> } <nl>  <nl> - if ( wf -> sectors [ wf -> nb_sectors - 1 ] << WTV_SECTOR_BITS > avio_tell ( s -> pb )) <nl> + if (( int64_t ) wf -> sectors [ wf -> nb_sectors - 1 ] << WTV_SECTOR_BITS > avio_tell ( s -> pb )) <nl> av_log ( s , AV_LOG_WARNING , " truncated file \ n "); <nl>  <nl> /* check length */
int av_packet_unpack_dictionary ( const uint8_t * data , int size , AVDictionary ** di <nl> const uint8_t * key = data ; <nl> const uint8_t * val = data + strlen ( key ) + 1 ; <nl>  <nl> - if ( val >= end ) <nl> + if ( val >= end || !* key ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> ret = av_dict_set ( dict , key , val , 0 );
static int smka_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " channels mismatch \ n "); <nl> return AVERROR ( EINVAL ); <nl> } <nl> - if ( bits && avctx -> sample_fmt == AV_SAMPLE_FMT_U8 ) { <nl> + if ( bits == ( avctx -> sample_fmt == AV_SAMPLE_FMT_U8 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " sample format mismatch \ n "); <nl> return AVERROR ( EINVAL ); <nl> }
static void process_client ( AVIOContext * client , const char * in_uri ) <nl> // may return empty string . <nl> if ( resource && strlen ( resource )) <nl> break ; <nl> + av_freep (& resource ); <nl> } <nl> if ( ret < 0 ) <nl> goto end ; <nl> end : <nl> avio_close ( client ); <nl> fprintf ( stderr , " Closing input \ n "); <nl> avio_close ( input ); <nl> + av_freep (& resource ); <nl> } <nl>  <nl> int main ( int argc , char ** argv )
static int64_t get_bit_rate ( AVCodecContext * ctx ) <nl> break ; <nl> case AVMEDIA_TYPE_AUDIO : <nl> bits_per_sample = av_get_bits_per_sample ( ctx -> codec_id ); <nl> - bit_rate = bits_per_sample ? ctx -> sample_rate * ctx -> channels * bits_per_sample : ctx -> bit_rate ; <nl> + bit_rate = bits_per_sample ? ctx -> sample_rate * ( int64_t ) ctx -> channels * bits_per_sample : ctx -> bit_rate ; <nl> break ; <nl> default : <nl> bit_rate = 0 ;
static int execute_decode_slices ( H264Context * h , int context_count ) <nl> if ( context_count == 1 ) { <nl> return decode_slice ( avctx , & h ); <nl> } else { <nl> + av_assert0 ( context_count > 0 ); <nl> for ( i = 1 ; i < context_count ; i ++) { <nl> hx = h -> thread_context [ i ]; <nl> hx -> s . err_recognition = avctx -> err_recognition ;
static int parse_bsfs ( void * log_ctx , const char * bsfs_spec , <nl> AVBitStreamFilterContext ** bsfs ) <nl> { <nl> char * bsf_name , * buf , * saveptr ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> if (!( buf = av_strdup ( bsfs_spec ))) <nl> return AVERROR ( ENOMEM );
static int ipvideo_decode_block_opcode_0xA ( IpvideoContext * s , AVFrame * frame ) <nl> unsigned char P [ 8 ]; <nl> int flags = 0 ; <nl>  <nl> + if ( bytestream2_get_bytes_left (& s -> stream_ptr ) < 16 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too little data for opcode 0xA \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> bytestream2_get_buffer (& s -> stream_ptr , P , 4 ); <nl>  <nl> /* 4 - color encoding for each 4x4 quadrant , or 4 - color encoding on
static int asf_read_frame_header ( AVFormatContext * s , AVIOContext * pb ){ <nl> case 0x54 : <nl> aspect . num = avio_r8 ( pb ); <nl> aspect . den = avio_r8 ( pb ); <nl> - if ( aspect . num > 0 && aspect . den > 0 ) { <nl> + if ( aspect . num > 0 && aspect . den > 0 && asf -> stream_index >= 0 ) { <nl> s -> streams [ asf -> stream_index ]-> sample_aspect_ratio = aspect ; <nl> } <nl> break ;
static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> next = avio_tell ( s -> pb ) + tlen ; <nl>  <nl> if ( tflags & ID3v2_FLAG_DATALEN ) { <nl> + if ( tlen < 4 ) <nl> + break ; <nl> avio_rb32 ( s -> pb ); <nl> tlen -= 4 ; <nl> }
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> align_get_bits (& ctx -> gb ); <nl>  <nl> + if (! band -> scan ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " band -> scan not set \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
int ff_flac_parse_picture ( AVFormatContext * s , uint8_t * buf , int buf_size ) <nl> if (!( data = av_buffer_alloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ))) { <nl> RETURN_ERROR ( AVERROR ( ENOMEM )); <nl> } <nl> + memset ( data -> data + len , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if ( avio_read ( pb , data -> data , len ) != len ) { <nl> av_log ( s , AV_LOG_ERROR , " Error reading attached picture data .\ n "); <nl> if ( s -> error_recognition & AV_EF_EXPLODE )
static int find_image_range ( int * pfirst_index , int * plast_index , <nl>  <nl> static int image_probe ( AVProbeData * p ) <nl> { <nl> - if ( av_str2id ( img_tags , p -> filename )) { <nl> + if ( p -> filename && av_str2id ( img_tags , p -> filename )) { <nl> if ( av_filename_number_test ( p -> filename )) <nl> return AVPROBE_SCORE_MAX ; <nl> else
static int ea_read_header ( AVFormatContext * s , <nl> ea -> audio_codec = 0 ; <nl> return 1 ; <nl> } <nl> + if ( ea -> bytes <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of bytes per sample : % d \ n ", ea -> bytes ); <nl> + ea -> audio_codec = CODEC_ID_NONE ; <nl> + return 1 ; <nl> + } <nl>  <nl> /* initialize the audio decoder stream */ <nl> st = avformat_new_stream ( s , NULL );
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , " Stream discovered after head already parsed \ n "); <nl> st = create_stream ( s , <nl> ( int []){ AVMEDIA_TYPE_VIDEO , AVMEDIA_TYPE_AUDIO , AVMEDIA_TYPE_DATA }[ stream_type ]); <nl> + if (! st ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> } <nl> av_dlog ( s , "% d % X % d \ n ", stream_type , flags , st -> discard );
static int convert_sub_to_old_ass_form ( AVSubtitle * sub , const AVPacket * pkt , AVR <nl> int ts_start , ts_duration = - 1 ; <nl> long int layer ; <nl>  <nl> - if ( rect -> type != SUBTITLE_ASS || ! strncmp ( rect -> ass , " Dialogue ", 10 )) <nl> + if ( rect -> type != SUBTITLE_ASS || ! strncmp ( rect -> ass , " Dialogue : ", 10 )) <nl> continue ; <nl>  <nl> av_bprint_clear (& buf );
static av_cold int twin_decode_init ( AVCodecContext * avctx ) <nl> default : avctx -> sample_rate = isampf * 1000 ; break ; <nl> } <nl>  <nl> - if ( avctx -> channels > CHANNELS_MAX ) { <nl> + if ( avctx -> channels <= 0 || avctx -> channels > CHANNELS_MAX ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unsupported number of channels : % i \ n ", <nl> avctx -> channels ); <nl> return - 1 ;
static int tqi_decode_frame ( AVCodecContext * avctx , <nl> for ( s -> mb_x = 0 ; s -> mb_x <( avctx -> width + 15 )/ 16 ; s -> mb_x ++) <nl> { <nl> if ( tqi_decode_mb ( s , t -> block ) < 0 ) <nl> - break ; <nl> + goto end ; <nl> tqi_idct_put ( t , t -> block ); <nl> } <nl> + end : <nl>  <nl> * data_size = sizeof ( AVFrame ); <nl> *( AVFrame *) data = t -> frame ;
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl>  <nl> /* first slice */ <nl> if ( si . start == 0 ) { <nl> - if ( s -> mb_num_left > 0 ) { <nl> + if ( s -> mb_num_left > 0 && s -> current_picture_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " New frame but still % d MB left .\ n ", <nl> s -> mb_num_left ); <nl> ff_er_frame_end (& s -> er );
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl>  <nl> if ( frames_hwctx -> texture ) <nl> ID3D11Texture2D_Release ( frames_hwctx -> texture ); <nl> + frames_hwctx -> texture = NULL ; <nl>  <nl> if ( s -> staging_texture ) <nl> ID3D11Texture2D_Release ( s -> staging_texture ); <nl> + s -> staging_texture = NULL ; <nl> } <nl>  <nl> static void free_texture ( void * opaque , uint8_t * data )
static attribute_align_arg void * frame_worker_thread ( void * arg ) <nl>  <nl> pthread_mutex_lock (& p -> progress_mutex ); <nl> for ( i = 0 ; i < MAX_BUFFERS ; i ++) <nl> - if ( p -> progress_used [ i ]) { <nl> + if ( p -> progress_used [ i ] && ( p -> got_frame || p -> result < 0 || avctx -> codec_id != CODEC_ID_H264 )) { <nl> p -> progress [ i ][ 0 ] = INT_MAX ; <nl> p -> progress [ i ][ 1 ] = INT_MAX ; <nl> }
static int decode_vol_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> else <nl> s -> quarter_sample = 0 ; <nl>  <nl> + if ( get_bits_left ( gb ) < 4 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " VOL Header truncated \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (! get_bits1 ( gb )) { <nl> int pos = get_bits_count ( gb ); <nl> int estimation_method = get_bits ( gb , 2 );
static int pcx_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> nplanes = buf [ 65 ]; <nl> bytes_per_scanline = nplanes * bytes_per_line ; <nl>  <nl> - if ( bytes_per_scanline < w * bits_per_pixel * nplanes / 8 || <nl> + if ( bytes_per_scanline < ( w * bits_per_pixel * nplanes + 7 ) / 8 || <nl> (! compressed && bytes_per_scanline > buf_size / h )) { <nl> av_log ( avctx , AV_LOG_ERROR , " PCX data is corrupted \ n "); <nl> return AVERROR_INVALIDDATA ;
static int sap_write_close ( AVFormatContext * s ) <nl> url_fclose ( rtpctx -> pb ); <nl> av_metadata_free (& rtpctx -> streams [ 0 ]-> metadata ); <nl> av_metadata_free (& rtpctx -> metadata ); <nl> + av_free ( rtpctx -> streams [ 0 ]-> info ); <nl> av_free ( rtpctx -> streams [ 0 ]); <nl> av_free ( rtpctx ); <nl> s -> streams [ i ]-> priv_data = NULL ;
static av_cold int hevc_decode_free ( AVCodecContext * avctx ) <nl> av_freep (& s -> sList [ i ]); <nl> } <nl> } <nl> + if ( s -> HEVClc == s -> HEVClcList [ 0 ]) <nl> + s -> HEVClc = NULL ; <nl> av_freep (& s -> HEVClcList [ 0 ]); <nl>  <nl> for ( i = 0 ; i < s -> nals_allocated ; i ++)
static int vp8_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> VP8Context * s = avctx -> priv_data ; <nl> int ret , mb_x , mb_y , i , y , referenced ; <nl> enum AVDiscard skip_thresh ; <nl> - AVFrame * curframe ; <nl> + AVFrame * curframe = NULL ; <nl>  <nl> if (( ret = decode_frame_header ( s , avpkt -> data , avpkt -> size )) < 0 ) <nl> return ret ;
static int dvbsub_parse_page_segment ( AVCodecContext * avctx , <nl> region_id = * buf ++; <nl> buf += 1 ; <nl>  <nl> + display = ctx -> display_list ; <nl> + while ( display && display -> region_id != region_id ) { <nl> + display = display -> next ; <nl> + } <nl> + if ( display ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " duplicate region \ n "); <nl> + break ; <nl> + } <nl> + <nl> display = tmp_display_list ; <nl> tmp_ptr = & tmp_display_list ; <nl> 
int ff_wma_init ( AVCodecContext * avctx , int flags2 ) <nl> int coef_vlc_table ; <nl>  <nl> if ( avctx -> sample_rate <= 0 || avctx -> sample_rate > 50000 <nl> - || avctx -> channels <= 0 || avctx -> channels > 8 <nl> + || avctx -> channels <= 0 || avctx -> channels > 2 <nl> || avctx -> bit_rate <= 0 ) <nl> return - 1 ; <nl> 
static void id3v1_create_tag ( AVFormatContext * s , uint8_t * buf ) <nl>  <nl> static int mp3_read_probe ( AVProbeData * p ) <nl> { <nl> - int max_frames , first_frames ; <nl> + int max_frames , first_frames = 0 ; <nl> int fsize , frames , sample_rate ; <nl> uint32_t header ; <nl> uint8_t * buf , * buf2 , * end ;
int av_aes_init ( AVAES * a , const uint8_t * key , int key_bits , int decrypt ) { <nl> uint8_t log8 [ 256 ]; <nl> uint8_t alog8 [ 512 ]; <nl>  <nl> - if (! enc_multbl [ 0 ][ sizeof ( enc_multbl )/ sizeof ( enc_multbl [ 0 ][ 0 ])- 1 ]){ <nl> + if (! enc_multbl [ FF_ARRAY_ELEMS ( enc_multbl )- 1 ][ FF_ARRAY_ELEMS ( enc_multbl [ 0 ])- 1 ]){ <nl> j = 1 ; <nl> for ( i = 0 ; i < 255 ; i ++){ <nl> alog8 [ i ]=
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl> cnt1 = get_bits ( b , nbits ); <nl> } else { <nl> pfx = 14 + (((( uint64_t )( value - 14 )) >> 32 ) & ( value - 14 )); <nl> + if ( pfx < 1 || pfx > 25 ) <nl> + return AVERROR_INVALIDDATA ; <nl> cnt1 *= ( 1 << pfx ) - 1 ; <nl> shbits = show_bits ( b , pfx ); <nl> if ( shbits <= 1 ) {
static void vmd_decode ( VmdVideoContext * s , AVFrame * frame ) <nl> palette32 [ i ] = ( r << 16 ) | ( g << 8 ) | ( b ); <nl> } <nl> } <nl> - s -> size -= ( 256 * 3 + 2 ); <nl> + s -> size -= PALETTE_COUNT * 3 + 2 ; <nl> } <nl> if ( s -> size > 0 ) { <nl> /* originally UnpackFrame in VAG ' s code */
av_cold int ff_mjpeg_decode_init ( AVCodecContext * avctx ) <nl> s -> first_picture = 1 ; <nl> s -> org_height = avctx -> coded_height ; <nl> avctx -> chroma_sample_location = AVCHROMA_LOC_CENTER ; <nl> + avctx -> colorspace = AVCOL_SPC_BT470BG ; <nl>  <nl> build_basic_mjpeg_vlc ( s ); <nl> 
static void pkt_dump_internal ( void * avcl , FILE * f , int level , const AVPacket * pk <nl> HEXDUMP_PRINT ("\ n "); <nl> HEXDUMP_PRINT (" size =% d \ n ", pkt -> size ); <nl> if ( dump_payload ) <nl> - av_hex_dump ( f , pkt -> data , pkt -> size ); <nl> + hex_dump_internal ( avcl , f , level , pkt -> data , pkt -> size ); <nl> } <nl>  <nl> void av_pkt_dump2 ( FILE * f , const AVPacket * pkt , int dump_payload , const AVStream * st )
static int init ( AVFilterContext * ctx , const char * args ) <nl> eval -> class = & aevalsrc_class ; <nl> av_opt_set_defaults ( eval ); <nl>  <nl> + if (! args1 ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Argument is empty \ n "); <nl> + ret = args ? AVERROR ( ENOMEM ) : AVERROR ( EINVAL ); <nl> + goto end ; <nl> + } <nl> + <nl> /* parse expressions */ <nl> buf = args1 ; <nl> i = 0 ;
static int ac3_parse_audio_block ( AC3DecodeContext * s , int blk ) <nl> /* coupling in use */ <nl> int cpl_begin_freq , cpl_end_freq ; <nl>  <nl> + if ( channel_mode < AC3_CHMODE_STEREO ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " coupling not allowed in mono or dual - mono \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> /* determine which channels are coupled */ <nl> for ( ch = 1 ; ch <= fbw_channels ; ch ++) <nl> s -> channel_in_cpl [ ch ] = get_bits1 ( gbc );
static int vqf_probe ( AVProbeData * probe_packet ) <nl> if (! memcmp ( probe_packet -> buf + 4 , " 00052200 ", 8 )) <nl> return AVPROBE_SCORE_MAX ; <nl>  <nl> + if ( AV_RL32 ( probe_packet -> buf + 12 ) > ( 1 << 27 )) <nl> + return AVPROBE_SCORE_EXTENSION / 2 ; <nl> + <nl> return AVPROBE_SCORE_EXTENSION ; <nl> } <nl> 
AVFormatContext * avformat_alloc_context ( void ) <nl> return NULL ; <nl> } <nl> ic -> internal -> offset = AV_NOPTS_VALUE ; <nl> + ic -> internal -> raw_packet_buffer_remaining_size = RAW_PACKET_BUFFER_SIZE ; <nl>  <nl> return ic ; <nl> }
int ff_wma_init ( AVCodecContext * avctx , int flags2 ) <nl>  <nl> /* compute MDCT block size */ <nl> s -> frame_len_bits = ff_wma_get_frame_len_bits ( s -> sample_rate , s -> version , 0 ); <nl> + s -> next_block_len_bits = s -> frame_len_bits ; <nl> + s -> prev_block_len_bits = s -> frame_len_bits ; <nl> + s -> block_len_bits = s -> frame_len_bits ; <nl>  <nl> s -> frame_len = 1 << s -> frame_len_bits ; <nl> if ( s -> use_variable_block_len ) {
static int g2m_init_buffers ( G2MContext * c ) <nl> if (! c -> synth_tile || ! c -> jpeg_tile || <nl> c -> old_tile_w < c -> tile_width || <nl> c -> old_tile_h < c -> tile_height ) { <nl> - c -> tile_stride = FFALIGN ( c -> tile_width * 3 , 16 ); <nl> + c -> tile_stride = FFALIGN ( c -> tile_width , 16 ) * 3 ; <nl> aligned_height = FFALIGN ( c -> tile_height , 16 ); <nl> av_free ( c -> synth_tile ); <nl> av_free ( c -> jpeg_tile );
static inline void codeblock ( DiracContext * s , SubBand * b , <nl> } \ <nl>  <nl> INTRA_DC_PRED ( 8 , int16_t ) <nl> - INTRA_DC_PRED ( 10 , int32_t ) <nl> + INTRA_DC_PRED ( 10 , uint32_t ) <nl>  <nl> /** <nl> * Dirac Specification ->
static int oma_read_seek ( struct AVFormatContext * s , <nl> int stream_index , int64_t timestamp , int flags ) <nl> { <nl> OMAContext * oc = s -> priv_data ; <nl> - int err = ff_pcm_read_seek ( s , stream_index , timestamp , flags ); <nl> + int64_t err = ff_pcm_read_seek ( s , stream_index , timestamp , flags ); <nl>  <nl> if (! oc -> encrypted ) <nl> return err ;
static int mov_write_audio_tag ( AVFormatContext * s , AVIOContext * pb , MOVMuxContex <nl> uint32_t tag = track -> tag ; <nl>  <nl> if ( track -> mode == MODE_MOV ) { <nl> - if ( track -> timescale > UINT16_MAX ) { <nl> + if ( track -> timescale > UINT16_MAX || ! track -> par -> channels ) { <nl> if ( mov_get_lpcm_flags ( track -> par -> codec_id )) <nl> tag = AV_RL32 (" lpcm "); <nl> version = 2 ;
static int tak_decode_frame ( AVCodecContext * avctx , void * data , <nl> uint8_t * samples = ( uint8_t *) frame -> extended_data [ chan ]; <nl> int32_t * decoded = s -> decoded [ chan ]; <nl> for ( i = 0 ; i < s -> nb_samples ; i ++) <nl> - samples [ i ] = decoded [ i ] + 0x80 ; <nl> + samples [ i ] = decoded [ i ] + 0x80U ; <nl> } <nl> break ; <nl> case AV_SAMPLE_FMT_S16P :
static int sp5x_decode_frame ( AVCodecContext * avctx , <nl> recoded [ j ++] = 0xFF ; <nl> recoded [ j ++] = 0xD9 ; <nl>  <nl> - avctx -> flags &= ~ CODEC_FLAG_EMU_EDGE ; <nl> av_init_packet (& avpkt_recoded ); <nl> avpkt_recoded . data = recoded ; <nl> avpkt_recoded . size = j ; <nl> AVCodec ff_amv_decoder = { <nl> NULL , <nl> ff_mjpeg_decode_end , <nl> sp5x_decode_frame , <nl> - CODEC_CAP_DR1 , <nl> + 0 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" AMV Video "), <nl> };
int avio_close_dyn_buf ( AVIOContext * s , uint8_t ** pbuffer ) <nl> static const char padbuf [ FF_INPUT_BUFFER_PADDING_SIZE ] = { 0 }; <nl> int padding = 0 ; <nl>  <nl> + if (! s ) { <nl> + * pbuffer = NULL ; <nl> + return 0 ; <nl> + } <nl> + <nl> /* don ' t attempt to pad fixed - size packet buffers */ <nl> if (! s -> max_packet_size ) { <nl> avio_write ( s , padbuf , sizeof ( padbuf ));
static int asf_read_metadata_obj ( AVFormatContext * s , const GUIDParseTable * g ) <nl> if (( ret = process_metadata ( s , name , name_len , val_len , type , <nl> & asf -> asf_sd [ st_num ]. asf_met )) < 0 ) <nl> break ; <nl> - } <nl> + } else <nl> + av_freep (& name ); <nl> } <nl> } <nl> 
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> break ; <nl>  <nl> arraylen = avio_rb32 ( ioc ); <nl> + if ( arraylen >> 28 ) <nl> + break ; <nl> + <nl> /* <nl> * Expect only ' times ' or ' filepositions ' sub - arrays in other case refuse to use such metadata <nl> * for indexing
int ff_h264_ref_picture ( H264Context * h , H264Picture * dst , H264Picture * src ) <nl> dst -> poc = src -> poc ; <nl> dst -> frame_num = src -> frame_num ; <nl> dst -> mmco_reset = src -> mmco_reset ; <nl> - dst -> pic_id = src -> pic_id ; <nl> dst -> long_ref = src -> long_ref ; <nl> dst -> mbaff = src -> mbaff ; <nl> dst -> field_picture = src -> field_picture ;
static int matroska_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> return - 1 ; <nl> matroska_execute_seekhead ( matroska ); <nl>  <nl> + if (! matroska -> time_scale ) <nl> + matroska -> time_scale = 1000000 ; <nl> if ( matroska -> duration ) <nl> matroska -> ctx -> duration = matroska -> duration * matroska -> time_scale <nl> * 1000 / AV_TIME_BASE ;
static int kalman_smoothen ( WMAVoiceContext * s , int pitch , <nl> float optimal_gain = 0 , dot ; <nl> const float * ptr = & in [- FFMAX ( s -> min_pitch_val , pitch - 3 )], <nl> * end = & in [- FFMIN ( s -> max_pitch_val , pitch + 3 )], <nl> - * best_hist_ptr ; <nl> + * best_hist_ptr = NULL ; <nl>  <nl> /* find best fitting point in history */ <nl> do {
static int read_gab2_sub ( AVFormatContext * s , AVStream * st , AVPacket * pkt ) <nl> if (! sub_demuxer ) <nl> goto error ; <nl>  <nl> + if ( strcmp ( sub_demuxer -> name , " srt ") && strcmp ( sub_demuxer -> name , " ass ")) <nl> + goto error ; <nl> + <nl> if (!( ast -> sub_ctx = avformat_alloc_context ())) <nl> goto error ; <nl> 
static int rm_read_multi ( AVFormatContext * s , AVIOContext * pb , <nl>  <nl> size2 = avio_rb32 ( pb ); <nl> ret = ff_rm_read_mdpr_codecdata ( s , s -> pb , st2 , st2 -> priv_data , <nl> - size2 , mime ); <nl> + size2 , NULL ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> }
enum AVCodecID av_guess_codec ( AVOutputFormat * fmt , const char * short_name , <nl> enum AVMediaType type ) <nl> { <nl> if ( av_match_name (" segment ", fmt -> name ) || av_match_name (" ssegment ", fmt -> name )) { <nl> - fmt = av_guess_format ( NULL , filename , NULL ); <nl> + AVOutputFormat * fmt2 = av_guess_format ( NULL , filename , NULL ); <nl> + if ( fmt2 ) <nl> + fmt = fmt2 ; <nl> } <nl>  <nl> if ( type == AVMEDIA_TYPE_VIDEO ) {
static int ea_read_header ( AVFormatContext * s , <nl> ea -> audio_codec = 0 ; <nl> } <nl>  <nl> + if ( ea -> sample_rate <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Unsupported sample rate : % d \ n ", ea -> sample_rate ); <nl> + ea -> audio_codec = 0 ; <nl> + } <nl> + <nl> if ( ea -> audio_codec ) { <nl> /* initialize the audio decoder stream */ <nl> st = av_new_stream ( s , 0 );
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> s -> picture_structure = last_pic_structure ; <nl> s -> dropable = last_pic_dropable ; <nl> return AVERROR_INVALIDDATA ; <nl> + } else if (! s -> current_picture_ptr ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , <nl> + " unset current_picture_ptr on % d . slice \ n ", <nl> + h0 -> current_slice + 1 ); <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl> } else { <nl> /* Shorten frame num gaps so we don ' t have to allocate reference
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl>  <nl> if ( hdr -> substreamid == info -> num_ind_sub + 1 ) { <nl> // info -> num_ind_sub ++; <nl> - avpriv_request_sample ( track -> par , " Multiple independent substreams "); <nl> + avpriv_request_sample ( mov -> fc , " Multiple independent substreams "); <nl> ret = AVERROR_PATCHWELCOME ; <nl> goto end ; <nl> } else if ( hdr -> substreamid < info -> num_ind_sub ||
static int decode_cblk ( Jpeg2000DecoderContext * s , Jpeg2000CodingStyle * codsty , <nl> ff_mqc_initdec (& t1 -> mqc , cblk -> data ); <nl>  <nl> while ( passno --) { <nl> + if ( bpno < 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " bpno invalid \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> switch ( pass_t ) { <nl> case 0 : <nl> decode_sigpass ( t1 , width , height , bpno + 1 , bandpos ,
static int opus_decode_packet ( AVCodecContext * avctx , void * data , <nl> memset ( frame -> extended_data [ i ], 0 , frame -> linesize [ 0 ]); <nl> } <nl>  <nl> - if ( c -> gain_i ) { <nl> + if ( c -> gain_i && decoded_samples > 0 ) { <nl> c -> fdsp -> vector_fmul_scalar (( float *) frame -> extended_data [ i ], <nl> ( float *) frame -> extended_data [ i ], <nl> c -> gain , FFALIGN ( decoded_samples , 8 ));
static int vdpau_vc1_start_frame ( AVCodecContext * avctx , <nl> else <nl> info -> picture_type = s -> pict_type - 1 + s -> pict_type / 3 ; <nl>  <nl> - info -> frame_coding_mode = v -> fcm ; <nl> + info -> frame_coding_mode = v -> fcm ? v -> fcm + 1 : 0 ; <nl> info -> postprocflag = v -> postprocflag ; <nl> info -> pulldown = v -> broadcast ; <nl> info -> interlace = v -> interlace ;
static int qdm2_parse_packet ( AVFormatContext * s , PayloadContext * qdm , <nl> * to the decoder that it is OK to initialize . */ <nl> st -> codec -> codec_id = CODEC_ID_QDM2 ; <nl> } <nl> + if ( st -> codec -> codec_id == CODEC_ID_NONE ) <nl> + return AVERROR ( EAGAIN ); <nl>  <nl> /* subpackets */ <nl> while ( end - p >= 4 ) {
int ff_raw_read_partial_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> - pkt -> size = ret ; <nl> + av_shrink_packet ( pkt , ret ); <nl> return ret ; <nl> } <nl> 
static int rv20_decode_picture_header ( RVDecContext * rv ) <nl> return - 1 ; <nl> } <nl>  <nl> + if ( s -> low_delay && s -> pict_type == AV_PICTURE_TYPE_B ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " low delay B \ n "); <nl> + return - 1 ; <nl> + } <nl> if ( s -> last_picture_ptr == NULL && s -> pict_type == AV_PICTURE_TYPE_B ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " early B pix \ n "); <nl> return - 1 ;
int ff_pcm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int ret , size ; <nl>  <nl> size = RAW_SAMPLES * s -> streams [ 0 ]-> codec -> block_align ; <nl> + if ( size <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> ret = av_get_packet ( s -> pb , pkt , size ); <nl> 
int ff_hevc_output_frame ( HEVCContext * s , AVFrame * out , int flush ) <nl> if (( frame -> flags & HEVC_FRAME_FLAG_OUTPUT ) && <nl> frame -> sequence == s -> seq_output ) { <nl> nb_output ++; <nl> - if ( frame -> poc < min_poc ) { <nl> + if ( frame -> poc < min_poc || nb_output == 1 ) { <nl> min_poc = frame -> poc ; <nl> min_idx = i ; <nl> }
static int flv_set_video_codec ( AVFormatContext * s , AVStream * vstream , int flv_co <nl> vcodec -> codec_id = CODEC_ID_VP6A ; <nl> if ( vcodec -> extradata_size != 1 ) { <nl> vcodec -> extradata_size = 1 ; <nl> - vcodec -> extradata = av_malloc ( 1 ); <nl> + vcodec -> extradata = av_malloc ( 1 + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> } <nl> vcodec -> extradata [ 0 ] = avio_r8 ( s -> pb ); <nl> return 1 ; // 1 byte body size adjustment for flv_read_packet ()
static int truemotion2rt_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if ( avctx -> width / s -> hscale * avctx -> height * s -> delta_size > avpkt -> size * 8LL * 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> ret = init_get_bits8 ( gb , avpkt -> data + ret , avpkt -> size - ret ); <nl> if ( ret < 0 ) <nl> return ret ;
static int amovie_request_frame ( AVFilterLink * outlink ) <nl>  <nl> if ( movie -> is_done ) <nl> return AVERROR_EOF ; <nl> - if (( ret = amovie_get_samples ( outlink )) < 0 ) <nl> - return ret ; <nl> + do { <nl> + if (( ret = amovie_get_samples ( outlink )) < 0 ) <nl> + return ret ; <nl> + } while (! movie -> samplesref ); <nl>  <nl> avfilter_filter_samples ( outlink , avfilter_ref_buffer ( movie -> samplesref , ~ 0 )); <nl> avfilter_unref_buffer ( movie -> samplesref );
static int decode_nal_units ( H264Context * h , uint8_t * buf , int buf_size ){ <nl> if ( ptr == NULL || dst_length < 0 ){ <nl> return - 1 ; <nl> } <nl> - while ( ptr [ dst_length - 1 ] == 0 && dst_length > 1 ) <nl> + while ( ptr [ dst_length - 1 ] == 0 && dst_length > 0 ) <nl> dst_length --; <nl> bit_length = 8 * dst_length - decode_rbsp_trailing ( h , ptr + dst_length - 1 ); <nl> 
static int encode_superframe ( AVCodecContext * avctx , <nl> } <nl> } <nl>  <nl> + if ( buf_size < 2 * MAX_CODED_SUPERFRAME_SIZE ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " output buffer size is too small \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> # if 1 <nl> total_gain = 128 ; <nl> for ( i = 64 ; i ; i >>= 1 ){
static inline void skip_put_bits ( PutBitContext * s , int n ) <nl> */ <nl> static inline void set_put_bits_buffer_size ( PutBitContext * s , int size ) <nl> { <nl> + av_assert0 ( size <= INT_MAX / 8 - 32 ); <nl> s -> buf_end = s -> buf + size ; <nl> s -> size_in_bits = 8 * size ; <nl> }
static int mpeg_mux_write_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> s -> preload *= 2 ; <nl> } <nl> preload = av_rescale ( s -> preload , 90000 , AV_TIME_BASE ); <nl> + av_log ( ctx , AV_LOG_DEBUG , " First SCR : %" PRId64 " First DTS : %" PRId64 "\ n ", s -> last_scr , dts + preload ); <nl> } <nl>  <nl> if ( dts != AV_NOPTS_VALUE ) dts += preload ;
static int ape_read_header ( AVFormatContext * s ) <nl> return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < ape -> seektablelength / sizeof ( uint32_t ); i ++) <nl> ape -> seektable [ i ] = avio_rl32 ( pb ); <nl> + } else { <nl> + av_log ( s , AV_LOG_ERROR , " Missing seektable \ n "); <nl> + return - 1 ; <nl> } <nl>  <nl> ape -> frames [ 0 ]. pos = ape -> firstframe ;
static int Stagefright_decode_frame ( AVCodecContext * avctx , void * data , <nl> frame -> size = orig_size ; <nl> } <nl> memcpy ( frame -> buffer , ptr , orig_size ); <nl> + if ( avpkt == & pkt ) <nl> + av_free ( avpkt -> data ); <nl>  <nl> frame -> time = ++ s -> frame_index ; <nl> (* s -> ts_map )[ s -> frame_index ]. pts = avpkt -> pts ;
static int mpc8_probe ( AVProbeData * p ) <nl> size = bs_get_v (& bs ); <nl> if ( size < 2 ) <nl> return 0 ; <nl> - if ( bs + size - 2 >= bs_end ) <nl> + if ( size >= bs_end - bs + 2 ) <nl> return AVPROBE_SCORE_EXTENSION - 1 ; // seems to be valid MPC but no header yet <nl> if ( header_found ) { <nl> if ( size < 11 || size > 28 )
static int read_extra_header ( FFV1Context * f ) <nl> } <nl>  <nl> f -> quant_table_count = get_symbol ( c , state , 0 ); <nl> - if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES ) <nl> + if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES || ! f -> quant_table_count ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> for ( i = 0 ; i < f -> quant_table_count ; i ++) {
int ff_h264_execute_ref_pic_marking ( H264Context * h , MMCO * mmco , int mmco_count ) <nl> */ <nl> if ( h -> short_ref_count && h -> short_ref [ 0 ] == h -> cur_pic_ptr ) { <nl> /* Just mark the second field valid */ <nl> - h -> cur_pic_ptr -> reference = PICT_FRAME ; <nl> + h -> cur_pic_ptr -> reference |= h -> picture_structure ; <nl> } else if ( h -> cur_pic_ptr -> long_ref ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " illegal short term reference " <nl> " assignment for second field "
static int set_hwframe_ctx ( AVCodecContext * ctx , AVBufferRef * hw_device_ctx ) <nl> if (( err = av_hwframe_ctx_init ( hw_frames_ref )) < 0 ) { <nl> fprintf ( stderr , " Failed to initialize VAAPI frame context ." <nl> " Error code : % s \ n ", av_err2str ( err )); <nl> + av_buffer_unref (& hw_frames_ref ); <nl> return err ; <nl> } <nl> ctx -> hw_frames_ctx = av_buffer_ref ( hw_frames_ref );
fail : <nl> } <nl>  <nl> if ( ret < 0 ) { <nl> + if ( codec -> object ) { <nl> + (* env )-> DeleteGlobalRef ( env , codec -> object ); <nl> + } <nl> ff_jni_reset_jfields ( env , & codec -> jfields , jni_amediacodec_mapping , 1 , codec ); <nl> av_freep (& codec ); <nl> }
static inline int get_len ( LZOContext * c , int x , int mask ) <nl> { <nl> int cnt = x & mask ; <nl> if (! cnt ) { <nl> - while (!( x = get_byte ( c ))) <nl> + while (!( x = get_byte ( c ))) { <nl> + if ( cnt >= INT_MAX - 1000 ) { <nl> + c -> error |= AV_LZO_ERROR ; <nl> + break ; <nl> + } <nl> cnt += 255 ; <nl> + } <nl> cnt += mask + x ; <nl> } <nl> return cnt ;
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> } <nl> } <nl>  <nl> -# define DURATION_MAX_READ_SIZE 250000 <nl> +# define DURATION_MAX_READ_SIZE 250000LL <nl> # define DURATION_MAX_RETRY 4 <nl>  <nl> /* only usable for MPEG - PS streams */
static int read_matrix_params ( MLPDecodeContext * m , unsigned int substr , GetBitCo <nl> if ( get_bits1 ( gbp )) <nl> coeff_val = get_sbits ( gbp , frac_bits + 2 ); <nl>  <nl> - s -> matrix_coeff [ mat ][ ch ] = coeff_val << ( 14 - frac_bits ); <nl> + s -> matrix_coeff [ mat ][ ch ] = coeff_val * ( 1 << ( 14 - frac_bits )); <nl> } <nl>  <nl> if ( s -> noise_type )
static void adpcm_compress_trellis ( AVCodecContext * avctx , <nl> uint8_t * h ;\ <nl> dec_sample = av_clip_int16 ( dec_sample );\ <nl> d = sample - dec_sample ;\ <nl> - ssd = nodes [ j ]-> ssd + d * d ;\ <nl> + ssd = nodes [ j ]-> ssd + d *( unsigned ) d ;\ <nl> /* Check for wraparound , skip such samples completely . \ <nl> * Note , changing ssd to a 64 bit variable would be \ <nl> * simpler , avoiding this check , but it ' s slower on \
int avformat_open_input ( AVFormatContext ** ps , const char * filename , AVInputForma <nl> { <nl> AVFormatContext * s = * ps ; <nl> int ret = 0 ; <nl> - AVFormatParameters ap = { 0 }; <nl> + AVFormatParameters ap = { { 0 } }; <nl> AVDictionary * tmp = NULL ; <nl>  <nl> if (! s && !( s = avformat_alloc_context ()))
int ff_lzw_decode ( LZWState * p , uint8_t * buf , int len ){ <nl> if ((-- l ) == 0 ) <nl> goto the_end ; <nl> } <nl> + if ( s -> ebuf < s -> pbuf ) { <nl> + av_log ( 0 , AV_LOG_ERROR , " lzw overread \ n "); <nl> + goto the_end ; <nl> + } <nl> c = lzw_get_code ( s ); <nl> if ( c == s -> end_code ) { <nl> break ;
typedef struct { <nl> static const AVOption amerge_options [] = { <nl> { " inputs ", " specify the number of inputs ", OFFSET ( nb_inputs ), <nl> AV_OPT_TYPE_INT , { . dbl = 2 }, 2 , SWR_CH_MAX }, <nl> + { 0 } <nl> }; <nl>  <nl> static const AVClass amerge_class = {
static av_cold void uninit ( AVFilterContext * ctx ) <nl>  <nl> if ( select -> do_scene_detect ) { <nl> avfilter_unref_bufferp (& select -> prev_picref ); <nl> - avcodec_close ( select -> avctx ); <nl> - av_freep (& select -> avctx ); <nl> + if ( select -> avctx ) { <nl> + avcodec_close ( select -> avctx ); <nl> + av_freep (& select -> avctx ); <nl> + } <nl> } <nl> } <nl> 
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> uint8_t mask2 = -( esc_count < 3 ); <nl> uint8_t * end = dst + ( width - 2 ); <nl>  <nl> + avpriv_request_sample ( l -> avctx , " zero_run_line "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i );
static void mov_parse_stsd_video ( MOVContext * c , AVIOContext * pb , <nl> if (( color_depth == 2 ) || ( color_depth == 4 ) || ( color_depth == 8 )) { <nl> /* for palette traversal */ <nl> unsigned int color_start , color_count , color_end ; <nl> - unsigned char a , r , g , b ; <nl> + unsigned int a , r , g , b ; <nl>  <nl> if ( color_greyscale ) { <nl> int color_index , color_dec ;
av_cold int ff_h264_decode_init ( AVCodecContext * avctx ) <nl>  <nl> if ( avctx -> extradata_size > 0 && avctx -> extradata ) { <nl> ret = ff_h264_decode_extradata ( h ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + ff_h264_free_context ( h ); <nl> return ret ; <nl> + } <nl> } <nl>  <nl> if ( h -> sps . bitstream_restriction_flag &&
static void frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> pthread_cond_signal (& p -> input_cond ); <nl> pthread_mutex_unlock (& p -> mutex ); <nl>  <nl> - pthread_join ( p -> thread , NULL ); <nl> + if ( p -> thread ) <nl> + pthread_join ( p -> thread , NULL ); <nl>  <nl> if ( codec -> close ) <nl> codec -> close ( p -> avctx );
static void vc1_decode_b_mb_intfi ( VC1Context * v ) <nl> int fwd ; <nl> int dmv_x [ 2 ], dmv_y [ 2 ], pred_flag [ 2 ]; <nl> int bmvtype = BMV_TYPE_BACKWARD ; <nl> - int idx_mbmode , interpmvp ; <nl> + int idx_mbmode ; <nl> + int av_uninit ( interpmvp ); <nl>  <nl> mquant = v -> pq ; /* Lossy initialization */ <nl> s -> mb_intra = 0 ;
static void alac_linear_predictor ( AlacEncodeContext * s , int ch ) <nl>  <nl> sum >>= lpc . lpc_quant ; <nl> sum += samples [ 0 ]; <nl> - residual [ i ] = samples [ lpc . lpc_order + 1 ] - sum ; <nl> + residual [ i ] = ( samples [ lpc . lpc_order + 1 ] - sum ) << ( 32 - s -> write_sample_size ) >> <nl> + ( 32 - s -> write_sample_size ); <nl> res_val = residual [ i ]; <nl>  <nl> if ( res_val ) {
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl> } else { <nl> int i ; <nl> for ( i = 0 ; SHOW_UBITS ( re , gb , 1 ) == 0 ; i ++){ <nl> - if ( get_bits_left ( gb )<= 0 ) <nl> + if ( gb -> size_in_bits <= re_index ) <nl> return - 1 ; <nl> LAST_SKIP_BITS ( re , gb , 1 ); <nl> UPDATE_CACHE ( re , gb );
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> if (! avctx -> rc_initial_buffer_occupancy ) <nl> - avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3 / 4 ; <nl> + avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3LL / 4 ; <nl>  <nl> if ( avctx -> ticks_per_frame && avctx -> time_base . num && <nl> avctx -> ticks_per_frame > INT_MAX / avctx -> time_base . num ) {
int ff_dwt_encode ( DWTContext * s , void * t ) <nl>  <nl> int ff_dwt_decode ( DWTContext * s , void * t ) <nl> { <nl> + if ( s -> ndeclevels == 0 ) <nl> + return 0 ; <nl> + <nl> switch ( s -> type ) { <nl> case FF_DWT97 : <nl> dwt_decode97_float ( s , t );
static int mxf_read_close ( AVFormatContext * s ) <nl> case MaterialPackage : <nl> av_freep (&(( MXFPackage *) mxf -> metadata_sets [ i ])-> tracks_refs ); <nl> break ; <nl> + case Track : <nl> + mxf -> metadata_sets [ i ] = NULL ; /* will be freed later */ <nl> + break ; <nl> default : <nl> break ; <nl> }
static int rtp_write ( URLContext * h , const uint8_t * buf , int size ) <nl> int ret ; <nl> URLContext * hd ; <nl>  <nl> + if ( size < 2 ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> if ( RTP_PT_IS_RTCP ( buf [ 1 ])) { <nl> /* RTCP payload type */ <nl> hd = s -> rtcp_hd ;
# include < stdlib . h > <nl> # include < stdio . h > <nl>  <nl> +# include " libavutil / common . h " <nl> # include " libavformat / avformat . h " <nl>  <nl> # undef exit <nl> int main ( int argc , char ** argv ) <nl>  <nl> for ( i = 0 ; ; i ++){ <nl> AVPacket pkt ; <nl> - AVStream * st ; <nl> + AVStream * av_uninit ( st ); <nl>  <nl> memset (& pkt , 0 , sizeof ( pkt )); <nl> if ( ret >= 0 ){
static int alac_set_info ( ALACContext * alac ) <nl>  <nl> alac -> max_samples_per_frame = bytestream2_get_be32u (& gb ); <nl> if (! alac -> max_samples_per_frame || <nl> - alac -> max_samples_per_frame > INT_MAX / sizeof ( int32_t )) { <nl> + alac -> max_samples_per_frame > 4096 * 4096 ) { <nl> av_log ( alac -> avctx , AV_LOG_ERROR , <nl> " max samples per frame invalid : %" PRIu32 "\ n ", <nl> alac -> max_samples_per_frame );
int avpriv_mpeg4audio_get_config ( MPEG4AudioConfig * c , const uint8_t * buf , <nl> GetBitContext gb ; <nl> int specific_config_bitindex ; <nl>  <nl> + if ( bit_size <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> init_get_bits (& gb , buf , bit_size ); <nl> c -> object_type = get_object_type (& gb ); <nl> c -> sample_rate = get_sample_rate (& gb , & c -> sampling_index );
static inline void encode_vlc_codeword ( PutBitContext * pb , unsigned codebook , int <nl> exponent = av_log2 ( val ); <nl>  <nl> put_bits ( pb , exponent - exp_order + switch_bits , 0 ); <nl> - put_bits ( pb , 1 , 1 ); <nl> - put_bits ( pb , exponent , val ); <nl> + put_bits ( pb , exponent + 1 , val ); <nl> } else { <nl> exponent = val >> rice_order ; <nl> 
static av_cold int libwebp_anim_encode_init ( AVCodecContext * avctx ) <nl> int ret = ff_libwebp_encode_init_common ( avctx ); <nl> if (! ret ) { <nl> LibWebPAnimContext * s = avctx -> priv_data ; <nl> - WebPAnimEncoderOptions enc_options ; <nl> + WebPAnimEncoderOptions enc_options = { 0 }; <nl> WebPAnimEncoderOptionsInit (& enc_options ); <nl> // TODO ( urvang ): Expose some options on command - line perhaps . <nl> s -> enc = WebPAnimEncoderNew ( avctx -> width , avctx -> height , & enc_options );
static int aiff_read_packet ( AVFormatContext * s , <nl> if ( max_size <= 0 ) <nl> return AVERROR_EOF ; <nl>  <nl> + if (! st -> codecpar -> block_align ) { <nl> + av_log ( s , AV_LOG_ERROR , " block_align not set \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* Now for that packet */ <nl> switch ( st -> codecpar -> codec_id ) { <nl> case AV_CODEC_ID_ADPCM_IMA_QT :
int ff_thread_init ( AVCodecContext * s ){ <nl> return 0 ; <nl> } <nl>  <nl> - s -> active_thread_type = FF_THREAD_SLICE ; <nl> - <nl> if ( s -> thread_count <= 1 ) <nl> return 0 ; <nl>  <nl> + s -> active_thread_type = FF_THREAD_SLICE ; <nl> + <nl> assert (! s -> thread_opaque ); <nl> c = av_mallocz ( sizeof ( ThreadContext )* s -> thread_count ); <nl> s -> thread_opaque = c ;
static int fileTest ( uint8_t * ref [ 4 ], int refStride [ 4 ], int w , int h , FILE * fp , <nl> struct Results r ; <nl> enum AVPixelFormat srcFormat ; <nl> char srcStr [ 12 ]; <nl> - int srcW , srcH ; <nl> + int srcW = 0 , srcH = 0 ; <nl> enum AVPixelFormat dstFormat ; <nl> char dstStr [ 12 ]; <nl> - int dstW , dstH ; <nl> + int dstW = 0 , dstH = 0 ; <nl> int flags ; <nl> int ret ; <nl> 
static void mov_fix_index ( MOVContext * mov , AVStream * st ) <nl> int first_non_zero_audio_edit = - 1 ; <nl> int packet_skip_samples = 0 ; <nl>  <nl> - if (! msc -> elst_data || msc -> elst_count <= 0 ) { <nl> + if (! msc -> elst_data || msc -> elst_count <= 0 || nb_old <= 0 ) { <nl> return ; <nl> } <nl> // Clean AVStream from traces of old index
static int msrle_decode_pal4 ( AVCodecContext * avctx , AVPicture * pic , <nl> unsigned int pixel_ptr = 0 ; <nl> int row_dec = pic -> linesize [ 0 ]; <nl> int row_ptr = ( avctx -> height - 1 ) * row_dec ; <nl> - int frame_size = row_dec * avctx -> height ; <nl> + int frame_size = FFABS ( row_dec ) * avctx -> height ; <nl> int i ; <nl>  <nl> while ( row_ptr >= 0 ) {
AVFilterFormats * avfilter_merge_formats ( AVFilterFormats * a , AVFilterFormats * b ) <nl> AVFilterFormats * ret ; <nl> unsigned i , j , k = 0 , m_count ; <nl>  <nl> + if ( a == b ) <nl> + return a ; <nl> + <nl> ret = av_mallocz ( sizeof (* ret )); <nl>  <nl> /* merge list of formats */
static int amv_encode_picture ( AVCodecContext * avctx , AVPacket * pkt , <nl> return - 1 ; <nl>  <nl> pic = av_frame_alloc (); <nl> + if (! pic ) <nl> + return AVERROR ( ENOMEM ); <nl> av_frame_ref ( pic , pic_arg ); <nl> // picture should be flipped upside - down <nl> for ( i = 0 ; i < 3 ; i ++) {
static void av_update_stream_timings ( AVFormatContext * ic ) <nl> duration = INT64_MIN ; <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> - if ( st -> start_time != AV_NOPTS_VALUE ) { <nl> + if ( st -> start_time != AV_NOPTS_VALUE && st -> time_base . den ) { <nl> start_time1 = av_rescale_q ( st -> start_time , st -> time_base , AV_TIME_BASE_Q ); <nl> if ( start_time1 < start_time ) <nl> start_time = start_time1 ;
static int videotoolbox_buffer_create ( AVCodecContext * avctx , AVFrame * frame ) <nl> vtctx -> cached_hw_frames_ctx = hw_frames_ctx ; <nl> } <nl>  <nl> - av_assert0 (! frame -> hw_frames_ctx ); <nl> + av_buffer_unref (& frame -> hw_frames_ctx ); <nl> frame -> hw_frames_ctx = av_buffer_ref ( vtctx -> cached_hw_frames_ctx ); <nl> if (! frame -> hw_frames_ctx ) <nl> return AVERROR ( ENOMEM );
static int decode_slice ( struct AVCodecContext * avctx , void * arg ) <nl> return 0 ; <nl> } else { <nl> ff_er_add_slice ( s , s -> resync_mb_x , s -> resync_mb_y , <nl> - s -> mb_x , s -> mb_y , <nl> + s -> mb_x - 1 , s -> mb_y , <nl> ER_MB_END & part_mask ); <nl>  <nl> return - 1 ;
static int ism_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> SmoothStreamingContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ pkt -> stream_index ]; <nl> - int64_t end_dts = ( c -> nb_fragments + 1 ) * c -> min_frag_duration ; <nl> + int64_t end_dts = ( c -> nb_fragments + 1 ) * ( int64_t ) c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int mpegps_read_packet ( AVFormatContext * s , <nl> MpegDemuxContext * m = s -> priv_data ; <nl> AVStream * st ; <nl> int len , startcode , i , es_type , ret ; <nl> - int lpcm_header_len ; <nl> + int lpcm_header_len = - 1 ; // Init to supress warning <nl> int request_probe = 0 ; <nl> enum AVCodecID codec_id = AV_CODEC_ID_NONE ; <nl> enum AVMediaType type ;
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> } else { <nl> pkt = add_to_pktbuf (& ic -> packet_buffer , & pkt1 , <nl> & ic -> packet_buffer_end ); <nl> + if (! pkt ) <nl> + goto find_stream_info_err ; <nl> if (( ret = av_dup_packet ( pkt )) < 0 ) <nl> goto find_stream_info_err ; <nl> }
static int read_var_block_data ( ALSDecContext * ctx , ALSBlockData * bd ) <nl>  <nl>  <nl> if (! sconf -> rlslms ) { <nl> - if ( sconf -> adapt_order ) { <nl> + if ( sconf -> adapt_order && sconf -> max_order ) { <nl> int opt_order_length = av_ceil_log2 ( av_clip (( bd -> block_length >> 3 ) - 1 , <nl> 2 , sconf -> max_order + 1 )); <nl> * bd -> opt_order = get_bits ( gb , opt_order_length );
static int raw_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> & pal_size ); <nl> int ret ; <nl>  <nl> - if ( pal_size != AVPALETTE_SIZE ) { <nl> + if ( pal && pal_size != AVPALETTE_SIZE ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Palette size % d is wrong \ n ", pal_size ); <nl> pal = NULL ; <nl> }
static int get_channel ( char ** map , uint64_t * ch , char delim ) <nl> static av_cold int channelmap_init ( AVFilterContext * ctx ) <nl> { <nl> ChannelMapContext * s = ctx -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> char * mapping , separator = '|'; <nl> int map_entries = 0 ; <nl> char buf [ 256 ];
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> AVFrame * const p = data ; <nl> int mb_x , mb_y , ret ; <nl>  <nl> + if ( buf_size * 8LL < a -> mb_height2 * a -> mb_width2 * 13LL ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 ) <nl> return ret ; <nl> p -> pict_type = AV_PICTURE_TYPE_I ;
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl>  <nl> if ( frames_hwctx -> texture ) <nl> ID3D11Texture2D_Release ( frames_hwctx -> texture ); <nl> + frames_hwctx -> texture = NULL ; <nl>  <nl> if ( s -> staging_texture ) <nl> ID3D11Texture2D_Release ( s -> staging_texture ); <nl> + s -> staging_texture = NULL ; <nl> } <nl>  <nl> static void free_texture ( void * opaque , uint8_t * data )
static void add_pid_to_pmt ( MpegTSContext * ts , unsigned int programid , <nl> if ( p -> nb_pids >= MAX_PIDS_PER_PROGRAM ) <nl> return ; <nl>  <nl> - for ( i = 0 ; i < MAX_PIDS_PER_PROGRAM ; i ++) <nl> + for ( i = 0 ; i < p -> nb_pids ; i ++) <nl> if ( p -> pids [ i ] == pid ) <nl> return ; <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl>  <nl> if ( mxg -> soi_ptr - mxg -> buffer > mxg -> cache_size ) { <nl> if ( mxg -> cache_size > 0 ) { <nl> - memcpy ( mxg -> buffer , mxg -> buffer_ptr , mxg -> cache_size ); <nl> + memmove ( mxg -> buffer , mxg -> buffer_ptr , mxg -> cache_size ); <nl> } <nl>  <nl> mxg -> buffer_ptr = mxg -> buffer ;
static int vaapi_encode_h264_write_extra_header ( AVCodecContext * avctx , <nl>  <nl> if ( priv -> sei_needed ) { <nl> if ( priv -> aud_needed ) { <nl> - vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + err = vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + if ( err < 0 ) <nl> + goto fail ; <nl> priv -> aud_needed = 0 ; <nl> } <nl> 
static enum AVPixelFormat get_format ( HEVCContext * s , const HEVCSPS * sps ) <nl> * fmt ++ = sps -> pix_fmt ; <nl> * fmt = AV_PIX_FMT_NONE ; <nl>  <nl> - return ff_get_format ( s -> avctx , pix_fmts ); <nl> + return ff_thread_get_format ( s -> avctx , pix_fmts ); <nl> } <nl>  <nl> static int set_sps ( HEVCContext * s , const HEVCSPS * sps ,
int av_frame_copy ( AVFrame * dst , const AVFrame * src ) <nl>  <nl> if ( dst -> width > 0 && dst -> height > 0 ) <nl> return frame_copy_video ( dst , src ); <nl> - else if ( dst -> nb_samples > 0 && dst -> channel_layout ) <nl> + else if ( dst -> nb_samples > 0 && dst -> channels > 0 ) <nl> return frame_copy_audio ( dst , src ); <nl>  <nl> return AVERROR ( EINVAL );
static int ipvideo_decode_block_opcode_0x9 ( IpvideoContext * s , AVFrame * frame ) <nl> int x , y ; <nl> unsigned char P [ 4 ]; <nl>  <nl> + if ( bytestream2_get_bytes_left (& s -> stream_ptr ) < 8 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too little data for opcode 0x9 \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* 4 - color encoding */ <nl> bytestream2_get_buffer (& s -> stream_ptr , P , 4 ); <nl> 
static int decode_bmv_frame ( const uint8_t * source , int src_len , uint8_t * frame , <nl> mode += 1 + advance_mode ; <nl> if ( mode >= 4 ) <nl> mode -= 3 ; <nl> - if ( FFABS ( dst_end - dst ) < len ) <nl> + if ( len <= 0 || FFABS ( dst_end - dst ) < len ) <nl> return AVERROR_INVALIDDATA ; <nl> switch ( mode ) { <nl> case 1 :
int av_cold ff_ivi_init_tiles ( IVIPlaneDesc * planes , int tile_width , int tile_hei <nl> t_width >>= 1 ; <nl> t_height >>= 1 ; <nl> } <nl> + if ( t_width <= 0 || t_height <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> for ( b = 0 ; b < planes [ p ]. num_bands ; b ++) { <nl> band = & planes [ p ]. bands [ b ];
static int v4l2_set_parameters ( AVFormatContext * s1 ) <nl> standard . index = i ; <nl> if ( v4l2_ioctl ( s -> fd , VIDIOC_ENUMSTD , & standard ) < 0 ) { <nl> ret = AVERROR ( errno ); <nl> + if ( ret == AVERROR ( EINVAL )) { <nl> + tpf = & streamparm . parm . capture . timeperframe ; <nl> + break ; <nl> + } <nl> av_log ( s1 , AV_LOG_ERROR , " ioctl ( VIDIOC_ENUMSTD ): % s \ n ", av_err2str ( ret )); <nl> return ret ; <nl> }
static int img_read_header ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl>  <nl> st = av_new_stream ( s1 , 0 ); <nl> if (! st ) { <nl> - av_free ( s ); <nl> return - ENOMEM ; <nl> } <nl>  <nl> static int img_read_header ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl> if (! s -> is_pipe ) <nl> url_fclose ( f ); <nl> fail : <nl> - av_free ( s ); <nl> return AVERROR_IO ; <nl> } <nl> 
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> ret = ff_vp8_decode_frame ( avctx , p , got_frame , & pkt ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + <nl> + update_canvas_size ( avctx , avctx -> width , avctx -> height ); <nl> + <nl> if ( s -> has_alpha ) { <nl> ret = vp8_lossy_decode_alpha ( avctx , p , s -> alpha_data , <nl> s -> alpha_data_size );
int main ( int argc , char ** argv ) <nl> avio_flush ( probe_out ); <nl> av_freep (& probe_out ); <nl> av_freep (& buffer ); <nl> - <nl> + uninit_opts (); <nl> avformat_network_deinit (); <nl>  <nl> return ret ;
again : <nl> hx -> inter_gb_ptr = & hx -> inter_gb ; <nl>  <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " Partitioned H . 264 support is incomplete \ n "); <nl> - return AVERROR_PATCHWELCOME ; <nl> + break ; <nl>  <nl> if ( hx -> redundant_pic_count == 0 && <nl> hx -> intra_gb_ptr &&
static int rsd_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> par -> channels = avio_rl32 ( pb ); <nl> - if (! par -> channels ) <nl> + if ( par -> channels <= 0 || par -> channels > INT_MAX / 36 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels : % d \ n ", par -> channels ); <nl> return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> avio_skip ( pb , 4 ); // Bit depth <nl> par -> sample_rate = avio_rl32 ( pb );
int ff_h2645_packet_split ( H2645Packet * pkt , const uint8_t * buf , int length , <nl> ret = hevc_parse_nal_header ( nal , logctx ); <nl> else <nl> ret = h264_parse_nal_header ( nal , logctx ); <nl> - if ( ret <= 0 ) { <nl> + if ( ret <= 0 || nal -> size <= 0 ) { <nl> if ( ret < 0 ) { <nl> av_log ( logctx , AV_LOG_ERROR , " Invalid NAL unit % d , skipping .\ n ", <nl> nal -> type );
static void ffmpeg_cleanup ( int ret ) <nl> av_freep (& ost -> audio_channels_map ); <nl> ost -> audio_channels_mapped = 0 ; <nl>  <nl> + av_dict_free (& ost -> sws_dict ); <nl> + <nl> avcodec_free_context (& ost -> enc_ctx ); <nl>  <nl> av_freep (& output_streams [ i ]);
int ff_h264_queue_decode_slice ( H264Context * h , const H2645NAL * nal ) <nl> return ret ; <nl>  <nl> // discard redundant pictures <nl> - if ( sl -> redundant_pic_count > 0 ) <nl> + if ( sl -> redundant_pic_count > 0 ) { <nl> + sl -> ref_count [ 0 ] = sl -> ref_count [ 1 ] = 0 ; <nl> return 0 ; <nl> + } <nl>  <nl> if ( sl -> first_mb_addr == 0 || ! h -> current_slice ) { <nl> if ( h -> setup_finished ) {
static int filter_packet ( void * log_ctx , AVPacket * pkt , <nl> } <nl>  <nl> if ( ret > 0 ) { <nl> + pkt -> side_data = NULL ; <nl> + pkt -> side_data_elems = 0 ; <nl> av_packet_unref ( pkt ); <nl> new_pkt . buf = av_buffer_create ( new_pkt . data , new_pkt . size , <nl> av_buffer_default_free , NULL , 0 );
static av_cold int common_init ( AVCodecContext * avctx ){ <nl> s -> avctx = avctx ; <nl> s -> flags = avctx -> flags ; <nl>  <nl> + avcodec_get_frame_defaults (& s -> picture ); <nl> + <nl> dsputil_init (& s -> dsp , avctx ); <nl>  <nl> s -> width = avctx -> width ;
static int dca_parse ( AVCodecParserContext * s , AVCodecContext * avctx , <nl> /* read the duration and sample rate from the frame header */ <nl> if (! dca_parse_params ( buf , buf_size , & duration , & sample_rate , & pc1 -> framesize )) { <nl> s -> duration = duration ; <nl> - avctx -> sample_rate = sample_rate ; <nl> } else <nl> s -> duration = 0 ; <nl> 
static int encode_init ( AVCodecContext * avctx ){ <nl> if ( avctx -> channels > MAX_CHANNELS ) <nl> return - 1 ; <nl>  <nl> + if ( avctx -> bit_rate < 24 * 1000 ) <nl> + return - 1 ; <nl> + <nl> /* extract flag infos */ <nl> flags1 = 0 ; <nl> flags2 = 1 ;
# define FELEM_MIN INT16_MIN <nl> # define WINDOW_TYPE 9 <nl> # elif ! defined ( CONFIG_RESAMPLE_AUDIOPHILE_KIDDY_MODE ) <nl> -# define FILTER_SHIFT 30 <nl> +# define FILTER_SHIFT 22 <nl>  <nl> # define FELEM int32_t <nl> # define FELEM2 int64_t
end : <nl> free_and_end : <nl> av_dict_free (& tmp ); <nl> av_freep (& avctx -> priv_data ); <nl> - if ( avctx -> internal ) <nl> + if ( avctx -> internal ) { <nl> + av_frame_free (& avctx -> internal -> to_free ); <nl> av_freep (& avctx -> internal -> pool ); <nl> + } <nl> av_freep (& avctx -> internal ); <nl> avctx -> codec = NULL ; <nl> goto end ;
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> INC_MED ( 2 ); <nl> } <nl> if (! c -> error_limit ) { <nl> + if ( add >= 0x2000000U ) { <nl> + av_log ( ctx -> avctx , AV_LOG_ERROR , " k % d is too large \ n ", add ); <nl> + goto error ; <nl> + } <nl> ret = base + get_tail ( gb , add ); <nl> if ( get_bits_left ( gb ) <= 0 ) <nl> goto error ;
unsigned int avpriv_toupper4 ( unsigned int x ) <nl> return av_toupper ( x & 0xFF ) + <nl> ( av_toupper (( x >> 8 ) & 0xFF ) << 8 ) + <nl> ( av_toupper (( x >> 16 ) & 0xFF ) << 16 ) + <nl> - ( av_toupper (( x >> 24 ) & 0xFF ) << 24 ); <nl> +(( unsigned ) av_toupper (( x >> 24 ) & 0xFF ) << 24 ); <nl> } <nl>  <nl> int ff_thread_ref_frame ( ThreadFrame * dst , ThreadFrame * src )
static int pmp_header ( AVFormatContext * s ) <nl> avio_skip ( pb , 10 ); <nl> srate = avio_rl32 ( pb ); <nl> channels = avio_rl32 ( pb ) + 1 ; <nl> - pos = avio_tell ( pb ) + 4 * index_cnt ; <nl> + pos = avio_tell ( pb ) + 4LL * index_cnt ; <nl> for ( i = 0 ; i < index_cnt ; i ++) { <nl> uint32_t size = avio_rl32 ( pb ); <nl> int flags = size & 1 ? AVINDEX_KEYFRAME : 0 ;
static int init_input ( AVFormatContext * s , const char * filename ) <nl> int ret ; <nl> AVProbeData pd = { filename , NULL , 0 }; <nl>  <nl> + if ( s -> iformat && ! strlen ( filename )) <nl> + return 0 ; <nl> + <nl> if ( s -> pb ) { <nl> s -> flags |= AVFMT_FLAG_CUSTOM_IO ; <nl> if (! s -> iformat )
static int read_packet ( AVFormatContext * s , uint8_t * buf , int raw_packet_size ) <nl> static int handle_packets ( MpegTSContext * ts , int nb_packets ) <nl> { <nl> AVFormatContext * s = ts -> stream ; <nl> - uint8_t packet [ TS_PACKET_SIZE ]; <nl> + uint8_t packet [ TS_PACKET_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> int packet_num , ret = 0 ; <nl>  <nl> if ( avio_tell ( s -> pb ) != ts -> last_pos ) {
static int ape_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl>  <nl> if ( ape -> seektablelength > 0 ) { <nl> ape -> seektable = av_malloc ( ape -> seektablelength ); <nl> + if (! ape -> seektable ) <nl> + return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < ape -> seektablelength / sizeof ( uint32_t ); i ++) <nl> ape -> seektable [ i ] = avio_rl32 ( pb ); <nl> }
static int concat_read_packet ( AVFormatContext * avf , AVPacket * pkt ) <nl> ( ret = open_next_file ( avf )) < 0 ) <nl> break ; <nl> } <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> delta = av_rescale_q ( cat -> cur_file -> start_time - cat -> avf -> start_time , <nl> AV_TIME_BASE_Q , <nl> cat -> avf -> streams [ pkt -> stream_index ]-> time_base );
static int find_video_stream_info ( AVFormatContext * fmt_ctx , int decode ) <nl> end : <nl> av_packet_unref (& pkt ); <nl>  <nl> + /* close all codecs opened in try_decode_video_frame */ <nl> + for ( i = 0 ; i < fmt_ctx -> nb_streams ; i ++) { <nl> + AVStream * st = fmt_ctx -> streams [ i ]; <nl> + avcodec_close ( st -> codec ); <nl> + } <nl> + <nl> return ret < 0 ; <nl> } <nl> 
static int mjpeg_decode_scan_progressive_ac ( MJpegDecodeContext * s , int ss , <nl> } <nl>  <nl> if (! Al ) { <nl> - s -> coefs_finished [ c ] |= ( 2LL << se ) - ( 1LL << ss ); <nl> + s -> coefs_finished [ c ] |= ( 2ULL << se ) - ( 1ULL << ss ); <nl> last_scan = !~ s -> coefs_finished [ c ]; <nl> } <nl> 
static int xvid_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> xvid_enc_frame_t xvid_enc_frame = { 0 }; <nl> xvid_enc_stats_t xvid_enc_stats = { 0 }; <nl>  <nl> - if (( ret = ff_alloc_packet2 ( avctx , pkt , mb_width * mb_height * MAX_MB_BYTES + FF_MIN_BUFFER_SIZE )) < 0 ) <nl> + if (( ret = ff_alloc_packet2 ( avctx , pkt , mb_width *( int64_t ) mb_height * MAX_MB_BYTES + FF_MIN_BUFFER_SIZE )) < 0 ) <nl> return ret ; <nl>  <nl> /* Start setting up the frame */
static av_cold int truemotion1_decode_end ( AVCodecContext * avctx ) <nl> TrueMotion1Context * s = avctx -> priv_data ; <nl>  <nl> av_frame_unref (& s -> frame ); <nl> - av_free ( s -> vert_pred ); <nl> + av_freep (& s -> vert_pred ); <nl>  <nl> return 0 ; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> s -> frames_out ++; <nl> if ( j > 0 ) s -> dup ++; <nl> } <nl> + av_frame_free (& buf ); <nl> } else { <nl> /* for delta less or equal to 0 , we should drop the frame , <nl> * otherwise , we will have one or more extra frames */
static void vp56_decode_mb ( VP56Context * s , int row , int col , int is_alpha ) <nl>  <nl> frame_current = s -> framep [ VP56_FRAME_CURRENT ]; <nl> frame_ref = s -> framep [ ref_frame ]; <nl> + if ( mb_type != VP56_MB_INTRA && ! frame_ref -> data [ 0 ]) <nl> + return ; <nl>  <nl> ab = 6 * is_alpha ; <nl> b_max = 6 - 2 * is_alpha ;
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> break ; <nl> default : <nl> av_log ( c , AV_LOG_ERROR , " Unsupported pixel format .\ n "); <nl> + av_free ( config ); <nl> return NULL ; <nl> } <nl> 
static int config_input ( AVFilterLink * inlink ) <nl> int hsub = desc -> log2_chroma_w ; <nl> int vsub = desc -> log2_chroma_h ; <nl>  <nl> + av_freep (& s -> buf ); <nl> s -> buf = av_mallocz (( FFALIGN ( inlink -> w , 16 ) * ( s -> radius + 1 ) / 2 + 32 ) * sizeof ( uint16_t )); <nl> if (! s -> buf ) <nl> return AVERROR ( ENOMEM );
static int xpm_decode_frame ( AVCodecContext * avctx , void * data , <nl> avctx -> pix_fmt = AV_PIX_FMT_BGRA ; <nl>  <nl> end = avpkt -> data + avpkt -> size ; <nl> - while ( memcmp ( ptr , "/* XPM */\ n ", 10 ) && ptr < end - 10 ) <nl> + while ( memcmp ( ptr , "/* XPM */", 9 ) && ptr < end - 9 ) <nl> ptr ++; <nl>  <nl> if ( ptr >= end ) {
static int adpcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> /* Each EA ADPCM frame has a 12 - byte header followed by 30 - byte pieces , <nl> each coding 28 stereo samples . */ <nl>  <nl> + if ( avctx -> channels != 2 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> src += 4 ; // skip sample count ( already read ) <nl>  <nl> current_left_sample = ( int16_t ) bytestream_get_le16 (& src );
typedef struct Context { <nl>  <nl> static int cmp ( const void * key , const void * node ) <nl> { <nl> - return (*( const int64_t *) key ) - (( const CacheEntry *) node )-> logical_pos ; <nl> + return FFDIFFSIGN (*( const int64_t *) key , (( const CacheEntry *) node )-> logical_pos ); <nl> } <nl>  <nl> static int cache_open ( URLContext * h , const char * arg , int flags , AVDictionary ** options )
static int amr_read_packet ( AVFormatContext * s , <nl> AVPacket * pkt ) <nl> { <nl> AVCodecContext * enc = s -> streams [ 0 ]-> codec ; <nl> - int read , size , toc , mode ; <nl> + int read , size = 0 , toc , mode ; <nl>  <nl> if ( url_feof (& s -> pb )) <nl> {
static int pcm_dvd_parse_header ( AVCodecContext * avctx , const uint8_t * header ) <nl> /* early exit if the header didn ' t change apart from the frame number */ <nl> if ( s -> last_header == header_int ) <nl> return 0 ; <nl> + s -> last_header = - 1 ; <nl>  <nl> if ( avctx -> debug & FF_DEBUG_PICT_INFO ) <nl> av_dlog ( avctx , " pcm_dvd_parse_header : header = % 02x % 02x % 02x \ n ",
static const AVClass writer_class = { <nl>  <nl> static void writer_close ( WriterContext ** wctx ) <nl> { <nl> - if (* wctx && (* wctx )-> writer -> uninit ) <nl> - (* wctx )-> writer -> uninit (* wctx ); <nl> + if (!* wctx ) <nl> + return ; <nl>  <nl> + if ((* wctx )-> writer -> uninit ) <nl> + (* wctx )-> writer -> uninit (* wctx ); <nl> av_freep (&((* wctx )-> priv )); <nl> av_freep ( wctx ); <nl> }
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> + av_shrink_packet ( pkt , CDXL_HEADER_SIZE + ret ); <nl> pkt -> stream_index = cdxl -> video_stream_index ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> pkt -> pos = pos ;
static void get_private_data ( OutputStream * os ) <nl> return ; <nl> os -> private_str = av_mallocz ( 2 * size + 1 ); <nl> if (! os -> private_str ) <nl> - return ; <nl> + goto fail ; <nl> for ( i = 0 ; i < size ; i ++) <nl> snprintf (& os -> private_str [ 2 * i ], 3 , "% 02x ", ptr [ i ]); <nl> + fail : <nl> if ( ptr != codec -> extradata ) <nl> av_free ( ptr ); <nl> }
static int rtp_mpegts_write_header ( AVFormatContext * s ) <nl> st -> time_base . num = 1 ; <nl> st -> time_base . den = 90000 ; <nl> st -> codec -> codec_id = AV_CODEC_ID_MPEG2TS ; <nl> - chain -> rtp_ctx = rtp_ctx ; <nl> rtp_ctx -> pb = s -> pb ; <nl> if (( ret = avformat_write_header ( rtp_ctx , NULL )) < 0 ) <nl> goto fail ; <nl> - rtp_ctx = NULL ; <nl> + chain -> rtp_ctx = rtp_ctx ; <nl>  <nl> return 0 ; <nl> 
void ff_rtp_send_h263 ( AVFormatContext * s1 , const uint8_t * buf1 , int size ) <nl>  <nl> while ( size > 0 ) { <nl> q = s -> buf ; <nl> - if (( buf1 [ 0 ] == 0 ) && ( buf1 [ 1 ] == 0 )) { <nl> + if ( size >= 2 && ( buf1 [ 0 ] == 0 ) && ( buf1 [ 1 ] == 0 )) { <nl> * q ++ = 0x04 ; <nl> buf1 += 2 ; <nl> size -= 2 ;
static void qdm2_fft_decode_tones ( QDM2Context * q , int duration , GetBitContext * <nl> return ; <nl>  <nl> local_int_14 = ( offset >> local_int_8 ); <nl> + if ( local_int_14 >= FF_ARRAY_ELEMS ( fft_level_index_table )) <nl> + return ; <nl>  <nl> if ( q -> nb_channels > 1 ) { <nl> channel = get_bits1 ( gb );
static av_cold void RENAME ( sws_init_swScale )( SwsContext * c ) <nl> enum PixelFormat srcFormat = c -> srcFormat , <nl> dstFormat = c -> dstFormat ; <nl>  <nl> - if (! is16BPS ( dstFormat ) && ! is9_OR_10BPS ( dstFormat )) { <nl> + if (! is16BPS ( dstFormat ) && ! is9_OR_10BPS ( dstFormat ) && <nl> + dstFormat != PIX_FMT_NV12 && dstFormat != PIX_FMT_NV21 ) { <nl> if (!( c -> flags & SWS_BITEXACT )) { <nl> if ( c -> flags & SWS_ACCURATE_RND ) { <nl> c -> yuv2yuv1 = RENAME ( yuv2yuv1_ar );
static av_cold int truespeech_decode_init ( AVCodecContext * avctx ) <nl> { <nl> // TSContext * c = avctx -> priv_data ; <nl>  <nl> + if ( avctx -> channels != 1 ) { <nl> + av_log_ask_for_sample ( avctx , " Unsupported channel count : % d \ n ", avctx -> channels ); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ; <nl> return 0 ; <nl> }
static inline int wnv1_get_code ( WNV1Context * w , int base_value ) <nl> if ( v == 15 ) <nl> return ff_reverse [ get_bits (& w -> gb , 8 - w -> shift )]; <nl> else <nl> - return base_value + (( v - 7 ) << w -> shift ); <nl> + return base_value + (( v - 7U ) << w -> shift ); <nl> } <nl>  <nl> static int decode_frame ( AVCodecContext * avctx ,
static int decode_subframe ( WmallDecodeCtx * s ) <nl> else <nl> use_normal_update_speed ( s , i ); <nl> revert_cdlms ( s , i , 0 , subframe_len ); <nl> - } <nl> + } else <nl> + memset ( s -> channel_residues , 0 , sizeof ( s -> channel_residues )); <nl> } <nl> if ( s -> do_mclms ) <nl> revert_mclms ( s , subframe_len );
static int bfi_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> return ret ; <nl>  <nl> pkt -> pts = bfi -> video_frame ; <nl> - bfi -> video_frame += ret / bfi -> video_size ; <nl> + bfi -> video_frame += bfi -> video_size ? ret / bfi -> video_size : 1 ; <nl>  <nl> /* One less frame to read . A cursory decrement . */ <nl> bfi -> nframes --;
static int init_image ( TiffContext * s , AVFrame * frame ) <nl> case 161 : <nl> s -> avctx -> pix_fmt = s -> le ? AV_PIX_FMT_GRAY16LE : AV_PIX_FMT_GRAY16BE ; <nl> break ; <nl> + case 162 : <nl> + s -> avctx -> pix_fmt = AV_PIX_FMT_YA8 ; <nl> + break ; <nl> case 322 : <nl> s -> avctx -> pix_fmt = s -> le ? AV_PIX_FMT_YA16LE : AV_PIX_FMT_YA16BE ; <nl> break ;
static void update_stream_timings ( AVFormatContext * ic ) <nl> end_time1 = av_rescale_q_rnd ( st -> duration , st -> time_base , <nl> AV_TIME_BASE_Q , <nl> AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX ); <nl> - if ( end_time1 != AV_NOPTS_VALUE && start_time1 <= INT64_MAX - end_time1 ) { <nl> + if ( end_time1 != AV_NOPTS_VALUE && ( end_time1 > 0 ? start_time1 <= INT64_MAX - end_time1 : start_time1 >= INT64_MIN - end_time1 )) { <nl> end_time1 += start_time1 ; <nl> end_time = FFMAX ( end_time , end_time1 ); <nl> }
static int opus_header ( AVFormatContext * avf , int idx ) <nl> /* gain = AV_RL16 ( packet + 16 );*/ <nl> /* channel_map = AV_RL8 ( packet + 18 );*/ <nl>  <nl> + av_freep (& st -> codecpar -> extradata ); <nl> if ( ff_alloc_extradata ( st -> codecpar , os -> psize )) <nl> return AVERROR ( ENOMEM ); <nl> 
static int load_input_picture ( MpegEncContext * s , const AVFrame * pic_arg ) <nl> EDGE_BOTTOM ); <nl> } <nl> } <nl> + emms_c (); <nl> } <nl> } <nl> ret = av_frame_copy_props ( pic -> f , pic_arg );
static void vc1_decode_p_blocks ( VC1Context * v ) <nl> if ( s -> mb_y != s -> start_mb_y ) ff_draw_horiz_band ( s , ( s -> mb_y - 1 ) * 16 , 16 ); <nl> s -> first_slice_line = 0 ; <nl> } <nl> - if ( apply_loop_filter ) { <nl> + if ( apply_loop_filter && v -> fcm == PROGRESSIVE ) { <nl> s -> mb_x = 0 ; <nl> ff_init_block_index ( s ); <nl> for (; s -> mb_x < s -> mb_width ; s -> mb_x ++) {
static int mxf_read_generic_descriptor ( void * arg , AVIOContext * pb , int tag , int <nl> default : <nl> /* Private uid used by SONY C0023S01 . mxf */ <nl> if ( IS_KLV_KEY ( uid , mxf_sony_mpeg4_extradata )) { <nl> + av_free ( descriptor -> extradata ); <nl> + descriptor -> extradata_size = 0 ; <nl> descriptor -> extradata = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! descriptor -> extradata ) <nl> return AVERROR ( ENOMEM );
int ff_vc1_parse_frame_header ( VC1Context * v , GetBitContext * gb ) <nl> { <nl> int pqindex , lowquant , status ; <nl>  <nl> + v -> field_mode = 0 ; <nl> + v -> fcm = 0 ; <nl> if ( v -> finterpflag ) <nl> v -> interpfrm = get_bits1 ( gb ); <nl> if (! v -> s . avctx -> codec )
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl> memset (& h -> mb , 0 , sizeof ( h -> mb )); <nl> memset (& h -> mb_luma_dc , 0 , sizeof ( h -> mb_luma_dc )); <nl> memset (& h -> mb_padding , 0 , sizeof ( h -> mb_padding )); <nl> + memset (& h -> cur_pic , 0 , sizeof ( h -> cur_pic )); <nl>  <nl> h -> avctx = dst ; <nl> h -> DPB = NULL ;
AVFilterFormats * avfilter_merge_formats ( AVFilterFormats * a , AVFilterFormats * b ) <nl> AVFilterFormats * ret ; <nl> unsigned i , j , k = 0 ; <nl>  <nl> + if ( a == b ) return a ; <nl> + <nl> ret = av_mallocz ( sizeof ( AVFilterFormats )); <nl>  <nl> /* merge list of formats */
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl>  <nl> x265pic . pts = pic -> pts ; <nl> x265pic . bitDepth = av_pix_fmt_desc_get ( avctx -> pix_fmt )-> comp [ 0 ]. depth_minus1 + 1 ; <nl> + <nl> + x265pic . sliceType = pic -> pict_type == AV_PICTURE_TYPE_I ? X265_TYPE_I : <nl> + pic -> pict_type == AV_PICTURE_TYPE_P ? X265_TYPE_P : <nl> + pic -> pict_type == AV_PICTURE_TYPE_B ? X265_TYPE_B : <nl> + X265_TYPE_AUTO ; <nl> } <nl>  <nl> ret = x265_encoder_encode ( ctx -> encoder , & nal , & nnal ,
# include < stddef . h > <nl> # include < stdint . h > <nl>  <nl> - typedef int16_t dwtcoef ; <nl> + typedef int32_t dwtcoef ; <nl>  <nl> enum VC2TransformType { <nl> VC2_TRANSFORM_9_7 = 0 , /* Deslauriers - Dubuc ( 9 , 7 ) */
av_cold void ff_cavsdsp_init_x86 ( CAVSDSPContext * c , AVCodecContext * avctx ) <nl> { <nl> av_unused int cpu_flags = av_get_cpu_flags (); <nl>  <nl> - cavsdsp_init_mmx ( c , avctx ); <nl> + if ( X86_MMX ( cpu_flags )) <nl> + cavsdsp_init_mmx ( c , avctx ); <nl> + <nl> # if HAVE_AMD3DNOW_INLINE <nl> if ( INLINE_AMD3DNOW ( cpu_flags )) <nl> cavsdsp_init_3dnow ( c , avctx );
static int read_tfra ( MOVContext * mov , AVIOContext * f ) <nl> } <nl> for ( i = 0 ; i < index -> item_count ; i ++) { <nl> int64_t time , offset ; <nl> + <nl> + if ( avio_feof ( f )) { <nl> + index -> item_count = 0 ; <nl> + av_freep (& index -> items ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( version == 1 ) { <nl> time = avio_rb64 ( f ); <nl> offset = avio_rb64 ( f );
static int decode_init_thread_copy ( AVCodecContext * avctx ) <nl> memset ( h -> sps_buffers , 0 , sizeof ( h -> sps_buffers )); <nl> memset ( h -> pps_buffers , 0 , sizeof ( h -> pps_buffers )); <nl>  <nl> + h -> avctx = avctx ; <nl> h -> rbsp_buffer [ 0 ] = NULL ; <nl> h -> rbsp_buffer [ 1 ] = NULL ; <nl> h -> rbsp_buffer_size [ 0 ] = 0 ;
start : <nl> c -> seek_timestamp = AV_NOPTS_VALUE ; <nl> break ; <nl> } <nl> + av_free_packet (& var -> pkt ); <nl> + reset_packet (& var -> pkt ); <nl> } <nl> } <nl> /* Check if this stream still is on an earlier segment number , or
static int unpack_vlcs ( Vp3DecodeContext * s , GetBitContext * gb , <nl> if ( blocks_ended ) <nl> dct_tokens [ j ++] = blocks_ended << 2 ; <nl>  <nl> - while ( coeff_i < num_coeffs ) { <nl> + while ( coeff_i < num_coeffs && get_bits_left ( gb ) > 0 ) { <nl> /* decode a VLC into a token */ <nl> token = get_vlc2 ( gb , vlc_table , 5 , 3 ); <nl> /* use the token to get a zero run , a coefficient , and an eob run */
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> av_log ( s , AV_LOG_ERROR , " No seek table at given position \ n "); <nl> return ; <nl> } <nl> + if ( size > INT_MAX / 10 || size <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Seek table size is invalid \ n "); <nl> + return ; <nl> + } <nl> if (!( buf = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ))) <nl> return ; <nl> avio_read ( s -> pb , buf , size );
static int query_formats ( AVFilterContext * ctx ) <nl> EvalContext * eval = ctx -> priv ; <nl> enum AVSampleFormat sample_fmts [] = { AV_SAMPLE_FMT_DBL , AV_SAMPLE_FMT_NONE }; <nl> int64_t chlayouts [] = { eval -> chlayout , - 1 }; <nl> + int sample_rates [] = { eval -> sample_rate , - 1 }; <nl>  <nl> avfilter_set_common_sample_formats ( ctx , avfilter_make_format_list ( sample_fmts )); <nl> ff_set_common_channel_layouts ( ctx , avfilter_make_format64_list ( chlayouts )); <nl> + ff_set_common_samplerates ( ctx , avfilter_make_format_list ( sample_rates )); <nl>  <nl> return 0 ; <nl> }
void ff_ivi_process_empty_tile ( AVCodecContext * avctx , IVIBandDesc * band , <nl> if ( band -> inherit_qdelta && ref_mb ) <nl> mb -> q_delta = ref_mb -> q_delta ; <nl>  <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> /* motion vector inheritance */ <nl> if ( mv_scale ) { <nl> mb -> mv_x = ivi_scale_mv ( ref_mb -> mv_x , mv_scale );
ogm_header ( AVFormatContext * s , int idx ) <nl> size -= 52 ; <nl> if ( bytestream2_get_bytes_left (& p ) < size ) <nl> return AVERROR_INVALIDDATA ; <nl> - ff_alloc_extradata ( st -> codecpar , size ); <nl> + if ( ff_alloc_extradata ( st -> codecpar , size ) < 0 ) <nl> + return AVERROR ( ENOMEM ); <nl> bytestream2_get_buffer (& p , st -> codecpar -> extradata , st -> codecpar -> extradata_size ); <nl> } <nl> }
void ffserver_parse_acl_row ( FFServerStream * stream , FFServerStream * feed , <nl> } <nl>  <nl> nacl = av_mallocz ( sizeof (* nacl )); <nl> + if (! nacl ) { <nl> + fprintf ( stderr , " Failed to allocate FFServerIPAddressACL \ n "); <nl> + goto bail ; <nl> + } <nl> + <nl> naclp = 0 ; <nl>  <nl> acl . next = 0 ;
static int wavpack_encode_block ( WavPackEncodeContext * s , <nl> block_size = bytestream2_tell_p (& pb ); <nl> AV_WL32 ( out + 4 , block_size - 8 ); <nl>  <nl> + av_assert0 ( put_bits_left (& s -> pb ) > 0 ); <nl> + <nl> return block_size ; <nl> } <nl> 
static void check_luma_dc_wht ( void ) <nl> } <nl>  <nl> # define SRC_BUF_STRIDE 32 <nl> -# define SRC_BUF_SIZE (( size + 5 ) * SRC_BUF_STRIDE ) <nl> +# define SRC_BUF_SIZE ((( size << ( size < 16 )) + 5 ) * SRC_BUF_STRIDE ) <nl> // The mc subpixel interpolation filter needs the 2 previous pixels in either <nl> // direction , the + 1 is to make sure the actual load addresses always are <nl> // unaligned .
static int vp3_decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> last_frame = s -> golden_frame ; <nl> s -> last_frame . type = FF_BUFFER_TYPE_COPY ; <nl> + ff_thread_report_progress (& s -> last_frame , INT_MAX , 0 ); <nl> } <nl> } <nl> 
static inline int decode_subframe ( FLACContext * s , int channel ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( wasted ) { <nl> + if ( wasted && wasted < 32 ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> decoded [ i ] = ( unsigned ) decoded [ i ] << wasted ;
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl>  <nl> flag = 0 ; <nl>  <nl> - if ( state * 4ULL > 0xFF || i >= size ) <nl> + if (( uint64_t ) state > 0xFF / 4 || i >= size ) <nl> continue ; <nl>  <nl> pfx = (( state + 8 ) >> 5 ) + ( state ? ff_clz ( state ): 32 ) - 24 ;
int ff_msmpeg4_decode_block ( MpegEncContext * s , int16_t * block , <nl> if ( level < 0 ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " dc overflow - block : % d qscale : % d //\ n ", n , s -> qscale ); <nl> if ( s -> inter_intra_pred ) level = 0 ; <nl> - else return - 1 ; <nl> } <nl> if ( n < 4 ) { <nl> rl = & ff_rl_table [ s -> rl_table_index ];
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> } <nl> st -> info -> duration_count ++; <nl> // ignore the first 4 values , they might have some random jitter <nl> - if ( st -> info -> duration_count > 3 ) <nl> + if ( st -> info -> duration_count > 3 && is_relative ( pkt -> dts ) == is_relative ( last )) <nl> st -> info -> duration_gcd = av_gcd ( st -> info -> duration_gcd , duration ); <nl> } <nl> if ( pkt -> dts != AV_NOPTS_VALUE )
int av_image_check_sar ( unsigned int w , unsigned int h , AVRational sar ) <nl> { <nl> int64_t scaled_dim ; <nl>  <nl> - if (! sar . den ) <nl> + if ( sar . den <= 0 || sar . num < 0 ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> if (! sar . num || sar . num == sar . den )
static int svq1_decode_frame ( AVCodecContext * avctx , <nl> av_dlog ( s -> avctx , " Error in svq1_decode_frame_header % i \ n ", result ); <nl> return result ; <nl> } <nl> + avcodec_set_dimensions ( avctx , s -> width , s -> height ); <nl>  <nl> // FIXME this avoids some confusion for " B frames " without 2 references <nl> // this should be removed after libavcodec can handle more flexible picture types & ordering
decode_intra_mb : <nl> } <nl>  <nl> // The pixels are stored in the same order as levels in h -> mb array . <nl> + if (( int ) ( h -> cabac . bytestream_end - ptr ) < mb_size ) <nl> + return - 1 ; <nl> memcpy ( h -> mb , ptr , mb_size ); ptr += mb_size ; <nl>  <nl> ff_init_cabac_decoder (& h -> cabac , ptr , h -> cabac . bytestream_end - ptr );
static int parse_pixel_format ( AVCodecContext * avctx ) <nl> normal_map = flags & DDPF_NORMALMAP ; <nl> fourcc = bytestream2_get_le32 ( gbc ); <nl>  <nl> + if ( ctx -> compressed && ctx -> paletted ) { <nl> + av_log ( avctx , AV_LOG_WARNING , <nl> + " Disabling invalid palette flag for compressed dds .\ n "); <nl> + ctx -> paletted = 0 ; <nl> + } <nl> + <nl> bpp = bytestream2_get_le32 ( gbc ); // rgbbitcount <nl> r = bytestream2_get_le32 ( gbc ); // rbitmask <nl> g = bytestream2_get_le32 ( gbc ); // gbitmask
static void free_geotags ( TiffContext * const s ) <nl> av_freep (& s -> geotags [ i ]. val ); <nl> } <nl> av_freep (& s -> geotags ); <nl> + s -> geotag_count = 0 ; <nl> } <nl>  <nl> # define RET_GEOKEY ( TYPE , array , element )\
static int caf_write_header ( AVFormatContext * s ) <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl>  <nl> + if ( par -> codec_id == AV_CODEC_ID_OPUS && par -> channels > 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Only mono and stereo are supported for Opus \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (! codec_tag ) { <nl> av_log ( s , AV_LOG_ERROR , " unsupported codec \ n "); <nl> return AVERROR_INVALIDDATA ;
static int smacker_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> smk -> cur_frame ++; <nl> smk -> nextpos = avio_tell ( s -> pb ); <nl> } else { <nl> - if ( smk -> stream_id [ smk -> curstream ] < 0 ) <nl> + if ( smk -> stream_id [ smk -> curstream ] < 0 || ! smk -> bufs [ smk -> curstream ]) <nl> return AVERROR_INVALIDDATA ; <nl> if ( av_new_packet ( pkt , smk -> buf_sizes [ smk -> curstream ])) <nl> return AVERROR ( ENOMEM );
const uint8_t ff_png_pass_mask [ NB_PASSES ] = { <nl>  <nl> void * ff_png_zalloc ( void * opaque , unsigned int items , unsigned int size ) <nl> { <nl> - if ( items >= UINT_MAX / size ) <nl> - return NULL ; <nl> - return av_malloc ( items * size ); <nl> + return av_mallocz_array ( items , size ); <nl> } <nl>  <nl> void ff_png_zfree ( void * opaque , void * ptr )
static int rtcp_parse_packet ( RTPDemuxContext * s , const unsigned char * buf , int l <nl> while ( len >= 2 ) { <nl> switch ( buf [ 1 ]) { <nl> case RTCP_SR : <nl> - if ( len < 16 ) { <nl> + if ( len < 20 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid length for RTCP SR packet \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int lavfi_read_packet ( AVFormatContext * avctx , AVPacket * pkt ) <nl> int stream_idx , min_pts_sink_idx = 0 ; <nl> AVFilterBufferRef * ref ; <nl> AVPicture pict ; <nl> - int ret , i , size ; <nl> + int ret , i ; <nl> + int size = 0 ; <nl>  <nl> /* iterate through all the graph sinks . Select the sink with the <nl> * minimum PTS */
reconnect : <nl> // audio or video packet arrives . <nl> while (! rt -> has_audio && ! rt -> has_video && ! rt -> received_metadata ) { <nl> if (( ret = get_packet ( s , 0 )) < 0 ) <nl> - return ret ; <nl> + goto fail ; <nl> } <nl>  <nl> // Either after we have read the metadata or ( if there is none ) the
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> switch ( type ) { <nl> case AVS_PALETTE : <nl> + if ( size - 4 > sizeof ( palette )) <nl> + return AVERROR_INVALIDDATA ; <nl> ret = avio_read ( s -> pb , palette , size - 4 ); <nl> if ( ret < size - 4 ) <nl> return AVERROR ( EIO );
static void set_frag_stream ( MOVFragmentIndex * frag_index , int id ) <nl> static MOVFragmentStreamInfo * get_current_frag_stream_info ( <nl> MOVFragmentIndex * frag_index ) <nl> { <nl> + if ( frag_index -> current < 0 || <nl> + frag_index -> current >= frag_index -> nb_items ) <nl> + return NULL ; <nl> + <nl> MOVFragmentIndexItem * item = & frag_index -> item [ frag_index -> current ]; <nl> if ( item -> current >= 0 && item -> current < item -> nb_stream_info ) <nl> return & item -> stream_info [ item -> current ];
static inline int l3_unscale ( int value , int exponent ) <nl> # endif <nl> if ( e > ( SUINT ) 31 ) <nl> return 0 ; <nl> - m = ( m + ( 1 << ( e - 1 ))) >> e ; <nl> + m = ( m + (( 1U << e )>> 1 )) >> e ; <nl>  <nl> return m ; <nl> }
static int pcm_read_header ( AVFormatContext * s ) <nl> av_log ( s , AV_LOG_ERROR , <nl> " Invalid sample_rate found in mime_type \"% s \"\ n ", <nl> mime_type ); <nl> + av_freep (& mime_type ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> st -> codecpar -> sample_rate = rate ; <nl> static int pcm_read_header ( AVFormatContext * s ) <nl> st -> codecpar -> channels = channels ; <nl> } <nl> } <nl> + av_freep (& mime_type ); <nl>  <nl> st -> codecpar -> bits_per_coded_sample = <nl> av_get_bits_per_sample ( st -> codecpar -> codec_id );
static int read_random ( uint32_t * dst , const char * file ) <nl>  <nl> if ( fd == - 1 ) <nl> return - 1 ; <nl> -# if HAVE_FCNTL && defined ( O_NONBLOCK ) <nl> - if ( fcntl ( fd , F_SETFL , fcntl ( fd , F_GETFL ) | O_NONBLOCK ) != - 1 ) <nl> -# endif <nl> err = read ( fd , dst , sizeof (* dst )); <nl> close ( fd ); <nl> 
static OutputStream * new_video_stream ( OptionsContext * o , AVFormatContext * oc , in <nl> } <nl> /* FIXME realloc failure */ <nl> video_enc -> rc_override = <nl> - av_realloc ( video_enc -> rc_override , <nl> - sizeof ( RcOverride ) * ( i + 1 )); <nl> + av_realloc_array ( video_enc -> rc_override , <nl> + i + 1 , sizeof ( RcOverride )); <nl> video_enc -> rc_override [ i ]. start_frame = start ; <nl> video_enc -> rc_override [ i ]. end_frame = end ; <nl> if ( q > 0 ) {
static void unpack_alpha ( GetBitContext * gb , uint16_t * dst , int num_coeffs , <nl> dst [ idx ++] = alpha_val >> 6 ; <nl> else <nl> dst [ idx ++] = ( alpha_val << 2 ) | ( alpha_val >> 6 ); <nl> - if ( idx == num_coeffs - 1 ) <nl> + if ( idx >= num_coeffs - 1 ) <nl> break ; <nl> } while ( get_bits1 ( gb )); <nl> val = get_bits ( gb , 4 );
static int mov_read_keys ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> avio_skip ( pb , 4 ); <nl> count = avio_rb32 ( pb ); <nl> - if ( count > UINT_MAX / sizeof (* c -> meta_keys )) { <nl> + if ( count > UINT_MAX / sizeof (* c -> meta_keys ) - 1 ) { <nl> av_log ( c -> fc , AV_LOG_ERROR , <nl> " The ' keys ' atom with the invalid key count : % d \ n ", count ); <nl> return AVERROR_INVALIDDATA ;
static int get_audio_frame_size ( AVCodecContext * enc , int size ) <nl> /* used for example by ADPCM codecs */ <nl> if ( enc -> bit_rate == 0 ) <nl> return - 1 ; <nl> - frame_size = ( size * 8 * enc -> sample_rate ) / enc -> bit_rate ; <nl> + frame_size = (( int64_t ) size * 8 * enc -> sample_rate ) / enc -> bit_rate ; <nl> } <nl> } else { <nl> frame_size = enc -> frame_size ;
static int smacker_decode_header_tree ( SmackVContext * smk , GetBitContext * gb , int <nl> huff . maxlength = 0 ; <nl> huff . current = 0 ; <nl> huff . values = av_mallocz ( huff . length * sizeof ( int )); <nl> + if (! huff . values ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( smacker_decode_bigtree ( gb , & huff , & ctx ) < 0 ) <nl> err = - 1 ;
static int decode_entropy_coded_image ( WebPContext * s , enum ImageRole role , <nl> length = offset + get_bits (& s -> gb , extra_bits ) + 1 ; <nl> } <nl> prefix_code = huff_reader_get_symbol (& hg [ HUFF_IDX_DIST ], & s -> gb ); <nl> - if ( prefix_code > 39 ) { <nl> + if ( prefix_code > 39U ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , <nl> " distance prefix code too large : % d \ n ", prefix_code ); <nl> return AVERROR_INVALIDDATA ;
static int flashsv2_prime ( FlashSVContext * s , uint8_t * src , <nl> s -> zstream . avail_out = s -> block_size * 3 ; <nl> inflate (& s -> zstream , Z_SYNC_FLUSH ); <nl>  <nl> - deflateInit (& zs , 0 ); <nl> + if ( deflateInit (& zs , 0 ) != Z_OK ) <nl> + return - 1 ; <nl> zs . next_in = s -> tmpblock ; <nl> zs . avail_in = s -> block_size * 3 - s -> zstream . avail_out ; <nl> zs . next_out = s -> deflate_block ;
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> * In complex cases , there are style descriptors appended to the string <nl> * so we can ' t just assume the packet size is the string size . <nl> */ <nl> - end = ptr + FFMAX ( 2 + AV_RB16 ( ptr ), avpkt -> size ); <nl> + end = ptr + FFMIN ( 2 + AV_RB16 ( ptr ), avpkt -> size ); <nl> ptr += 2 ; <nl>  <nl> ts_start = av_rescale_q ( avpkt -> pts ,
static int matroska_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> ret = matroska_parse_cluster ( matroska ); <nl> } <nl>  <nl> + if ( ret == AVERROR_INVALIDDATA ) { <nl> + pkt -> flags |= AV_PKT_FLAG_CORRUPT ; <nl> + return 0 ; <nl> + } <nl> + <nl> return ret ; <nl> } <nl> 
static void start_children ( FFServerStream * feed ) <nl> feed -> pid = fork (); <nl> if ( feed -> pid < 0 ) { <nl> http_log (" Unable to create children : % s \ n ", strerror ( errno )); <nl> + av_free ( pathname ); <nl> exit ( EXIT_FAILURE ); <nl> } <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> for ( i = 0 ; i < SUBFRAMES ; i ++) <nl> put_subframe ( c , i ); <nl>  <nl> + <nl> + for ( i = put_bits_count (& c -> pb ); i < 8 * c -> frame_size ; i ++) <nl> + put_bits (& c -> pb , 1 , 0 ); <nl> + <nl> flush_put_bits (& c -> pb ); <nl>  <nl> avpkt -> pts = frame -> pts ;
int main ( int argc , char ** argv ) <nl> return 1 ; <nl> } <nl>  <nl> - frame -> pts = 0 ; <nl> + if ( frame ) <nl> + frame -> pts = 0 ; <nl> for (;;) { <nl> /* Compute current audio and video time . */ <nl> if ( audio_st )
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> bytestream_put_le16 (& buf , 0 ); <nl> bytestream_put_le32 (& buf , 0 ); <nl>  <nl> - if (( ret = avio_read ( pb , buf , image -> size )) < 0 ) <nl> + if (( ret = avio_read ( pb , buf , image -> size )) < 0 ) { <nl> + av_packet_unref ( pkt ); <nl> return ret ; <nl> + } <nl>  <nl> st -> codecpar -> bits_per_coded_sample = AV_RL16 ( buf + 14 ); <nl> 
static void mov_text_cleanup_ftab ( MovTextContext * m ) <nl>  <nl> static int mov_text_tx3g ( AVCodecContext * avctx , MovTextContext * m ) <nl> { <nl> - char * tx3g_ptr = avctx -> extradata ; <nl> + uint8_t * tx3g_ptr = avctx -> extradata ; <nl> int i , box_size , font_length ; <nl> int8_t v_align , h_align ; <nl> int style_fontID ;
# include " cabac_functions . h " <nl> # include " hevc . h " <nl>  <nl> -# define CABAC_MAX_BIN 100 <nl> +# define CABAC_MAX_BIN 31 <nl>  <nl> /** <nl> * number of bin by SyntaxElement .
static int decode_studio_vop_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> return 0 ; <nl>  <nl> s -> partitioned_frame = 0 ; <nl> + s -> interlaced_dct = 0 ; <nl> s -> decode_mb = mpeg4_decode_studio_mb ; <nl>  <nl> decode_smpte_tc ( ctx , gb );
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> h = ( buf [ 1 ] + 1 ) * 8 ; <nl> buf += 2 ; <nl>  <nl> + if ( avpkt -> size < 2 + w * h / 513 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( w != avctx -> width || h != avctx -> height ) { <nl> av_freep (& c -> frame_buffer ); <nl> av_freep (& c -> last_frame_buffer );
static av_cold int rl2_decode_end ( AVCodecContext * avctx ) <nl> { <nl> Rl2Context * s = avctx -> priv_data ; <nl>  <nl> - av_free ( s -> back_frame ); <nl> + av_freep (& s -> back_frame ); <nl>  <nl> return 0 ; <nl> }
static int mp3_read_probe ( AVProbeData * p ) <nl>  <nl> max_frames = 0 ; <nl> buf = buf0 ; <nl> - end = buf + p -> buf_size - sizeof ( uint32_t ); <nl> + end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl>  <nl> for (; buf < end ; buf = buf2 + 1 ) { <nl> buf2 = buf ;
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl>  <nl> int res = avcodec_open2 ( ctx , c , NULL ); <nl> if ( res < 0 ) <nl> - return res ; <nl> + return 0 ; // Failure of avcodec_open2 () does not imply that a issue was found <nl>  <nl> FDBCreate (& buffer ); <nl> int got_frame ;
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> memcpy ( c , h -> s . thread_context [ i ], sizeof ( MpegEncContext )); <nl> memset (& c -> s + 1 , 0 , sizeof ( H264Context ) - sizeof ( MpegEncContext )); <nl> c -> h264dsp = h -> h264dsp ; <nl> + c -> h264qpel = h -> h264qpel ; <nl> c -> sps = h -> sps ; <nl> c -> pps = h -> pps ; <nl> c -> pixel_shift = h -> pixel_shift ;
static int video_thread ( void * arg ) <nl> frame -> opaque = picref ; <nl> } <nl>  <nl> - if ( av_cmp_q ( tb , is -> video_st -> time_base )) { <nl> + if ( ret >= 0 && av_cmp_q ( tb , is -> video_st -> time_base )) { <nl> av_unused int64_t pts1 = pts_int ; <nl> pts_int = av_rescale_q ( pts_int , tb , is -> video_st -> time_base ); <nl> av_dlog ( NULL , " video_thread (): "
skip : <nl> // sign extension <nl> int32_t cts = ( avio_rb24 ( s -> pb ) + 0xff800000 ) ^ 0xff800000 ; <nl> pts = dts + cts ; <nl> - if ( cts < 0 ) { // dts are wrong <nl> + if ( cts < 0 && ! flv -> wrong_dts ) { // dts might be wrong <nl> flv -> wrong_dts = 1 ; <nl> av_log ( s , AV_LOG_WARNING , <nl> " Negative cts , previous timestamps might be wrong .\ n ");
int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , SwsFilter * dstFilter ) <nl> /* precalculate vertical scaler filter coefficients */ <nl> { <nl> const int filterAlign = <nl> - ( HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX ) && ( flags & SWS_ACCURATE_RND ) ? 2 : <nl> + ( HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX ) ? 2 : <nl> ( HAVE_ALTIVEC && cpu_flags & AV_CPU_FLAG_ALTIVEC ) ? 8 : <nl> 1 ; <nl> 
static int vorbis_parse_setup_hdr_codebooks ( vorbis_context * vc ) <nl> } <nl>  <nl> // Initialize VLC table <nl> + if ( entries <= 0 ) { <nl> + av_log ( vc -> avctx , AV_LOG_ERROR , " Invalid codebook entry count \ n "); <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto error ; <nl> + } <nl> if ( ff_vorbis_len2vlc ( tmp_vlc_bits , tmp_vlc_codes , entries )) { <nl> av_log ( vc -> avctx , AV_LOG_ERROR , " Invalid code lengths while generating vlcs . \ n "); <nl> ret = AVERROR_INVALIDDATA ;
end : <nl> frame -> height = avctx -> height ; <nl> } <nl>  <nl> + if ( ret < 0 ) <nl> + av_frame_unref ( frame ); <nl> + <nl> return ret ; <nl> } <nl> 
static int twolame_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> if ( ret < 0 ) // twolame error <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame ) { <nl> + avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame -> pts != AV_NOPTS_VALUE ) <nl> avpkt -> pts = frame -> pts - ff_samples_to_time_base ( avctx , avctx -> delay ); <nl> } else {
static int bitpacked_decode_yuv422p10 ( AVCodecContext * avctx , AVFrame * frame , <nl> AVPacket * avpkt ) <nl> { <nl> uint64_t frame_size = ( uint64_t ) avctx -> width * ( uint64_t ) avctx -> height * 20 ; <nl> - uint64_t packet_size = avpkt -> size * 8 ; <nl> + uint64_t packet_size = ( uint64_t ) avpkt -> size * 8 ; <nl> GetBitContext bc ; <nl> uint16_t * y , * u , * v ; <nl> int ret , i ;
static void vda_decoder_callback ( void * vda_hw_ctx , <nl> vda_frame * new_frame ; <nl> vda_frame * queue_walker ; <nl>  <nl> - if (!( new_frame = av_mallocz ( sizeof ( vda_frame )))) <nl> + if (!( new_frame = av_mallocz ( sizeof (* new_frame )))) <nl> return ; <nl> new_frame -> next_frame = NULL ; <nl> new_frame -> cv_buffer = CVPixelBufferRetain ( image_buffer );
int ff_thread_get_buffer ( AVCodecContext * avctx , AVFrame * f ) <nl> p -> requested_frame = f ; <nl> p -> state = STATE_GET_BUFFER ; <nl> pthread_mutex_lock (& p -> progress_mutex ); <nl> - pthread_cond_signal (& p -> progress_cond ); <nl> + pthread_cond_broadcast (& p -> progress_cond ); <nl>  <nl> while ( p -> state != STATE_SETTING_UP ) <nl> pthread_cond_wait (& p -> progress_cond , & p -> progress_mutex );
# ifndef AVUTIL_MEM_INTERNAL_H <nl> # define AVUTIL_MEM_INTERNAL_H <nl>  <nl> +# include " avassert . h " <nl> +# include " mem . h " <nl> + <nl> static inline int ff_fast_malloc ( void * ptr , unsigned int * size , size_t min_size , int zero_realloc ) <nl> { <nl> void * val ;
static int decode_picture_header ( AVCodecContext * avctx , const uint8_t * buf , cons <nl> \ <nl> if ( q > switch_bits ) { /* exp golomb */ \ <nl> bits = exp_order - switch_bits + ( q << 1 ); \ <nl> - if ( bits > MIN_CACHE_BITS ) \ <nl> + if ( bits > FFMIN ( MIN_CACHE_BITS , 31 )) \ <nl> return AVERROR_INVALIDDATA ; \ <nl> val = SHOW_UBITS ( re , gb , bits ) - ( 1 << exp_order ) + \ <nl> (( switch_bits + 1 ) << rice_order ); \
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> } <nl> if ( len > 0xffff ) <nl> len = 0 ; <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO ) { <nl> + len = 0 ; <nl> + } <nl> * q ++ = len >> 8 ; <nl> * q ++ = len ; <nl> val = 0x80 ;
static int X264_frame ( AVCodecContext * ctx , uint8_t * buf , <nl> } <nl>  <nl> x4 -> out_pic . key_frame = pic_out . b_keyframe ; <nl> - x4 -> out_pic . quality = ( pic_out . i_qpplus1 - 1 ) * FF_QP2LAMBDA ; <nl> + if ( bufsize ) <nl> + x4 -> out_pic . quality = ( pic_out . i_qpplus1 - 1 ) * FF_QP2LAMBDA ; <nl>  <nl> return bufsize ; <nl> }
static int ism_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> SmoothStreamingContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ pkt -> stream_index ]; <nl> - int64_t end_dts = ( c -> nb_fragments + 1 ) * c -> min_frag_duration ; <nl> + int64_t end_dts = ( c -> nb_fragments + 1LL ) * c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int find_headers_search_validate ( FLACParseContext * fpc , int offset ) <nl> (* end_handle )-> offset = offset ; <nl> (* end_handle )-> link_penalty = av_malloc ( sizeof ( int ) * <nl> FLAC_MAX_SEQUENTIAL_HEADERS ); <nl> + if (!(* end_handle )-> link_penalty ) { <nl> + av_freep ( end_handle ); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> + <nl> for ( i = 0 ; i < FLAC_MAX_SEQUENTIAL_HEADERS ; i ++) <nl> (* end_handle )-> link_penalty [ i ] = FLAC_HEADER_NOT_PENALIZED_YET ; <nl> 
static int mxf_write_footer ( AVFormatContext * s ) <nl> AVIOContext * pb = s -> pb ; <nl> int err = 0 ; <nl>  <nl> + if (! mxf -> header_written || <nl> + ( s -> oformat == & ff_mxf_opatom_muxer && ! mxf -> body_partition_offset )) { <nl> + /* reason could be invalid options / not supported codec / out of memory */ <nl> + err = AVERROR_UNKNOWN ; <nl> + goto end ; <nl> + } <nl> + <nl> mxf -> duration = mxf -> last_indexed_edit_unit + mxf -> edit_units_count ; <nl>  <nl> mxf_write_klv_fill ( s );
int av_asrc_buffer_add_buffer ( AVFilterContext * ctx , <nl> int sample_fmt , int64_t channel_layout , int planar , <nl> int64_t pts , int av_unused flags ) <nl> { <nl> - uint8_t * data [ 8 ]; <nl> + uint8_t * data [ 8 ] = { 0 }; <nl> int linesize [ 8 ]; <nl> int nb_channels = av_get_channel_layout_nb_channels ( channel_layout ), <nl> nb_samples = buf_size / nb_channels / av_get_bytes_per_sample ( sample_fmt );
void ff_h264_direct_ref_list_init ( H264Context * const h ){ <nl> h -> col_fieldoff = 2 * h -> ref_list [ 1 ][ 0 ]. f . reference - 3 ; <nl> } <nl>  <nl> - if ( cur -> f . pict_type != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> + if ( h -> slice_type_nos != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> return ; <nl>  <nl> for ( list = 0 ; list < 2 ; list ++){
av_cold int ff_dcaadpcm_init ( DCAADPCMEncContext * s ) <nl> return - 1 ; <nl>  <nl> s -> private_data = av_malloc ( sizeof ( premultiplied_coeffs ) * DCA_ADPCM_VQCODEBOOK_SZ ); <nl> + if (! s -> private_data ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> precalc ( s -> private_data ); <nl> return 0 ; <nl> }
static int read_header ( AVFormatContext * s ) <nl> return ret ; <nl> } <nl>  <nl> - avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + if ( vst -> index_entries ) <nl> + avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + else <nl> + avio_skip ( pb , 4 ); <nl>  <nl> bink -> current_track = - 1 ; <nl> return 0 ;
void avcodec_get_frame_defaults ( AVFrame * frame ) <nl>  <nl> AVFrame * avcodec_alloc_frame ( void ) <nl> { <nl> - AVFrame * frame = av_malloc ( sizeof ( AVFrame )); <nl> + AVFrame * frame = av_mallocz ( sizeof ( AVFrame )); <nl>  <nl> if ( frame == NULL ) <nl> return NULL ;
static void asfrtp_close_context ( PayloadContext * asf ) <nl> { <nl> ffio_free_dyn_buf (& asf -> pktbuf ); <nl> av_freep (& asf -> buf ); <nl> - av_free ( asf ); <nl> } <nl>  <nl> # define RTP_ASF_HANDLER ( n , s , t ) \
int vc1_parse_frame_header_adv ( VC1Context * v , GetBitContext * gb ) <nl> int scale , shift , i ; /* for initializing LUT for intensity compensation */ <nl>  <nl> v -> numref = 0 ; <nl> + v -> fcm = 0 ; <nl> + v -> field_mode = 0 ; <nl> v -> p_frame_skipped = 0 ; <nl> if ( v -> second_field ) { <nl> v -> s . pict_type = ( v -> fptype & 1 ) ? AV_PICTURE_TYPE_P : AV_PICTURE_TYPE_I ;
static int mov_write_udta_tag ( ByteIOContext * pb , MOVMuxContext * mov , <nl> put_be32 ( pb , size + 8 ); <nl> put_tag ( pb , " udta "); <nl> put_buffer ( pb , buf , size ); <nl> - av_free ( buf ); <nl> } <nl> + av_free ( buf ); <nl>  <nl> return 0 ; <nl> }
static void apply_unsharp ( uint8_t * dst , int dst_stride , <nl>  <nl> int32_t res ; <nl> int x , y , z ; <nl> - const uint8_t * src2 ; <nl> + const uint8_t * src2 = NULL ; // silence a warning <nl>  <nl> if (! fp -> amount ) { <nl> if ( dst_stride == src_stride )
static int rm_write_header ( AVFormatContext * s ) <nl> int n ; <nl> AVCodecContext * codec ; <nl>  <nl> + if ( s -> nb_streams > 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " At most 2 streams are currently supported for muxing in RM \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> for ( n = 0 ; n < s -> nb_streams ; n ++) { <nl> s -> streams [ n ]-> id = n ; <nl> codec = s -> streams [ n ]-> codec ;
int ff_mjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> /* EOF */ <nl> if ( start_code < 0 ) { <nl> goto the_end ; <nl> - } else if ( unescaped_buf_size > ( 1U << 29 )) { <nl> + } else if ( unescaped_buf_size > ( 1U << 28 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " MJPEG packet 0x % x too big ( 0x % x / 0x % x ), corrupt data ?\ n ", <nl> start_code , unescaped_buf_size , buf_size ); <nl> return AVERROR_INVALIDDATA ;
static int v4l2_set_parameters ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl> struct v4l2_streamparm streamparm = { 0 }; <nl> struct v4l2_fract * tpf = & streamparm . parm . capture . timeperframe ; <nl> int i , ret ; <nl> - AVRational framerate_q ; <nl> + AVRational framerate_q ={ 0 }; <nl>  <nl> streamparm . type = V4L2_BUF_TYPE_VIDEO_CAPTURE ; <nl> 
int attribute_align_arg av_buffersink_get_frame_flags ( AVFilterContext * ctx , AVFr <nl>  <nl> if ( flags & AV_BUFFERSINK_FLAG_PEEK ) { <nl> cur_frame = *(( AVFrame **) av_fifo_peek2 ( buf -> fifo , 0 )); <nl> - av_frame_ref ( frame , cur_frame ); /* TODO check failure */ <nl> + if (( ret = av_frame_ref ( frame , cur_frame )) < 0 ) <nl> + return ret ; <nl> } else { <nl> av_fifo_generic_read ( buf -> fifo , & cur_frame , sizeof ( cur_frame ), NULL ); <nl> av_frame_move_ref ( frame , cur_frame );
void ff_lzw_decode_tail ( LZWState * p ) <nl>  <nl> if ( s -> mode == FF_LZW_GIF ) { <nl> while ( s -> bs > 0 ) { <nl> - if ( s -> pbuf + s -> bs >= s -> ebuf ) { <nl> + if ( s -> bs >= s -> ebuf - s -> pbuf ) { <nl> s -> pbuf = s -> ebuf ; <nl> break ; <nl> } else {
static inline int mxf_read_utf16_string ( AVIOContext * pb , int size , char ** str , i <nl> int ret ; <nl> size_t buf_size ; <nl>  <nl> - if ( size < 0 ) <nl> + if ( size < 0 || size > INT_MAX / 2 ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> buf_size = size + size / 2 + 1 ;
static opj_image_t * mj2_create_image ( AVCodecContext * avctx , opj_cparameters_t * p <nl>  <nl> img = opj_image_create ( numcomps , cmptparm , color_space ); <nl>  <nl> + if (! img ) <nl> + return NULL ; <nl> + <nl> // x0 , y0 is the top left corner of the image <nl> // x1 , y1 is the width , height of the reference grid <nl> img -> x0 = 0 ;
static int mxf_read_primer_pack ( void * arg , AVIOContext * pb , int tag , int size , U <nl> avpriv_request_sample ( pb , " Primer pack item length % d ", item_len ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> - if ( item_num > 65536 ) { <nl> + if ( item_num > 65536 || item_num < 0 ) { <nl> av_log ( mxf -> fc , AV_LOG_ERROR , " item_num % d is too large \ n ", item_num ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> // we just ignore it <nl> bytestream_get_le16 (& buf ); <nl>  <nl> + if ( buf_end - buf < h + 3 * 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> // allocate sub and set values <nl> sub -> rects = av_mallocz ( sizeof (* sub -> rects )); <nl> if (! sub -> rects )
ASSStyle * ff_ass_style_get ( ASSSplitContext * ctx , const char * style ) <nl> if (! style || !* style ) <nl> style = " Default "; <nl> for ( i = 0 ; i < ass -> styles_count ; i ++) <nl> - if (! strcmp ( ass -> styles [ i ]. name , style )) <nl> + if ( ass -> styles [ i ]. name && ! strcmp ( ass -> styles [ i ]. name , style )) <nl> return ass -> styles + i ; <nl> return NULL ; <nl> }
static int ebml_read_binary ( AVIOContext * pb , int length , EbmlBin * bin ) <nl> bin -> pos = avio_tell ( pb ); <nl> if ( avio_read ( pb , bin -> data , length ) != length ) { <nl> av_freep (& bin -> data ); <nl> + bin -> size = 0 ; <nl> return AVERROR ( EIO ); <nl> } <nl> 
static void flush_dpb ( AVCodecContext * avctx ) <nl> h -> parse_context . overread_index = 0 ; <nl> h -> parse_context . index = 0 ; <nl> h -> parse_context . last_index = 0 ; <nl> + <nl> + free_tables ( h , 1 ); <nl> + h -> context_initialized = 0 ; <nl> } <nl>  <nl> int ff_init_poc ( H264Context * h , int pic_field_poc [ 2 ], int * pic_poc )
static int decode_block ( MJpegDecodeContext * s , int16_t * block , int component , <nl> av_log ( s -> avctx , AV_LOG_ERROR , " error dc \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> - val = val * quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> + val = val * ( unsigned ) quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> val = av_clip_int16 ( val ); <nl> s -> last_dc [ component ] = val ; <nl> block [ 0 ] = val ;
static int mv_read_packet ( AVFormatContext * avctx , AVPacket * pkt ) <nl> AVStream * st = avctx -> streams [ mv -> stream_index ]; <nl> const AVIndexEntry * index ; <nl> int frame = mv -> frame [ mv -> stream_index ]; <nl> - int ret ; <nl> + int64_t ret ; <nl> uint64_t pos ; <nl>  <nl> if ( frame < st -> nb_index_entries ) {
static int get_siz ( J2kDecoderContext * s ) <nl> s -> tile_offset_y = bytestream_get_be32 (& s -> buf ); // YT0Siz <nl> s -> ncomponents = bytestream_get_be16 (& s -> buf ); // CSiz <nl>  <nl> + if ( s -> tile_width <= 0 || s -> tile_height <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> if ( s -> buf_end - s -> buf < 2 * s -> ncomponents ) <nl> return AVERROR ( EINVAL ); <nl> 
static int mxf_parse_physical_source_package ( MXFContext * mxf , MXFTrack * source_t <nl> break ; <nl>  <nl> /* the name of physical source package is name of the reel or tape */ <nl> - if ( physical_package -> name [ 0 ]) <nl> + if ( physical_package -> name && physical_package -> name [ 0 ]) <nl> av_dict_set (& st -> metadata , " reel_name ", physical_package -> name , 0 ); <nl>  <nl> /* the source timecode is calculated by adding the start_position of the sourceclip from the file source package track
static int read_header ( AVFormatContext * s , <nl> jvf -> audio_size = avio_rl32 ( pb ); <nl> jvf -> video_size = avio_rl32 ( pb ); <nl> jvf -> palette_size = avio_r8 ( pb ) ? 768 : 0 ; <nl> + jvf -> video_size = FFMIN ( FFMAX ( jvf -> video_size , 0 ), <nl> + INT_MAX - JV_PREAMBLE_SIZE - jvf -> palette_size ); <nl> if ( avio_r8 ( pb )) <nl> av_log ( s , AV_LOG_WARNING , " unsupported audio codec \ n "); <nl> jvf -> video_type = avio_r8 ( pb );
static void draw_char ( AVCodecContext * avctx , int c , int a ) <nl> s -> frame . linesize [ 0 ], s -> font , s -> font_height , c , <nl> a & 0x0F , a >> 4 ); <nl> s -> x += FONT_WIDTH ; <nl> - if ( s -> x >= avctx -> width ) { <nl> + if ( s -> x > avctx -> width - FONT_WIDTH ) { <nl> s -> x = 0 ; <nl> s -> y += s -> font_height ; <nl> }
static int nprobe ( AVFormatContext * s , uint8_t * enc_header , unsigned size , <nl> taglen = AV_RB32 (& enc_header [ pos + 32 ]); <nl> datalen = AV_RB32 (& enc_header [ pos + 36 ]) >> 4 ; <nl>  <nl> - pos += 44 + taglen ; <nl> + pos += 44 ; <nl> + if ( size - pos < taglen ) <nl> + return - 1 ; <nl> + <nl> + pos += taglen ; <nl>  <nl> if ( datalen << 4 > size - pos ) <nl> return - 1 ;
static float voice_factor ( float * p_vector , float p_gain , <nl> AMRWB_SFR_SIZE ) * <nl> f_gain * f_gain ; <nl>  <nl> - return ( p_ener - f_ener ) / ( p_ener + f_ener ); <nl> + return ( p_ener - f_ener ) / ( p_ener + f_ener + 0 . 01 ); <nl> } <nl>  <nl> /**
static int dca_decode_frame ( AVCodecContext * avctx , void * data , <nl> } else { <nl> s -> channel_order_tab = dca_channel_reorder_nolfe_xch [ s -> amode ]; <nl> } <nl> + if ( s -> channel_order_tab [ s -> xch_base_channel ] < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> } else { <nl> channels = num_core_channels + !! s -> lfe ; <nl> s -> xch_present = 0 ; /* disable further xch processing */
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size , <nl> h -> workaround_bugs |= FF_BUG_TRUNCATED ; <nl>  <nl> if (!( h -> workaround_bugs & FF_BUG_TRUNCATED )) <nl> - while ( ptr [ dst_length - 1 ] == 0 && dst_length > 0 ) <nl> + while ( dst_length > 0 && ptr [ dst_length - 1 ] == 0 ) <nl> dst_length --; <nl> bit_length = ! dst_length ? 0 <nl> : ( 8 * dst_length -
int opt_default ( void * optctx , const char * opt , const char * arg ) <nl> # endif <nl> # if CONFIG_AVRESAMPLE <nl> rc_class = avresample_get_class (); <nl> - if ( av_opt_find (& rc_class , opt , NULL , 0 , <nl> - AV_OPT_SEARCH_CHILDREN | AV_OPT_SEARCH_FAKE_OBJ )) { <nl> + if (( o = av_opt_find (& rc_class , opt , NULL , 0 , <nl> + AV_OPT_SEARCH_CHILDREN | AV_OPT_SEARCH_FAKE_OBJ ))) { <nl> av_dict_set (& resample_opts , opt , arg , FLAGS ); <nl> consumed = 1 ; <nl> }
static void pop_output_configuration ( AACContext * ac ) { <nl> ac -> oc [ 1 ] = ac -> oc [ 0 ]; <nl> ac -> avctx -> channels = ac -> oc [ 1 ]. channels ; <nl> ac -> avctx -> channel_layout = ac -> oc [ 1 ]. channel_layout ; <nl> - } else { <nl> - ac -> avctx -> channels = 0 ; <nl> - ac -> avctx -> channel_layout = 0 ; <nl> } <nl> } <nl> }
static int daala_header ( AVFormatContext * s , int idx ) <nl> if ( hdr -> gpshift >= 32 ) { <nl> av_log ( s , AV_LOG_ERROR , " Too large gpshift % d (>= 32 ).\ n ", <nl> hdr -> gpshift ); <nl> + hdr -> gpshift = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> hdr -> gpmask = ( 1U << hdr -> gpshift ) - 1 ;
static int decode_frame ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( rbuf + buf_size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 ) { <nl> av_free ( rbuf );
static int ogg_build_opus_headers ( AVCodecContext * avctx , <nl> static int ogg_write_header ( AVFormatContext * s ) <nl> { <nl> OGGContext * ogg = s -> priv_data ; <nl> - OGGStreamContext * oggstream ; <nl> + OGGStreamContext * oggstream = NULL ; <nl> int i , j ; <nl>  <nl> if ( ogg -> pref_size )
typedef struct AMRWBContext { <nl> } AMRWBContext ; <nl>  <nl> static const AVOption options [] = { <nl> - { " dtx ", " Allow DTX ( generate comfort noise )", offsetof ( AMRWBContext , allow_dtx ), FF_OPT_TYPE_INT , 0 , 0 , 1 , AV_OPT_FLAG_AUDIO_PARAM | AV_OPT_FLAG_ENCODING_PARAM }, <nl> + { " dtx ", " Allow DTX ( generate comfort noise )", offsetof ( AMRWBContext , allow_dtx ), FF_OPT_TYPE_INT , { 0 }, 0 , 1 , AV_OPT_FLAG_AUDIO_PARAM | AV_OPT_FLAG_ENCODING_PARAM }, <nl> { NULL } <nl> }; <nl> 
static int hls_decode_entry_wpp ( AVCodecContext * avctxt , void * input_ctb_row , int <nl>  <nl> if ( more_data < 0 ) { <nl> s -> tab_slice_address [ ctb_addr_rs ] = - 1 ; <nl> + avpriv_atomic_int_set (& s1 -> wpp_err , 1 ); <nl> + ff_thread_report_progress2 ( s -> avctx , ctb_row , thread , SHIFT_CTB_WPP ); <nl> return more_data ; <nl> } <nl> 
# include " mlz . h " <nl>  <nl> av_cold void ff_mlz_init_dict ( void * context , MLZ * mlz ) { <nl> - mlz -> dict = av_malloc_array ( TABLE_SIZE , sizeof (* mlz -> dict )); <nl> + mlz -> dict = av_mallocz_array ( TABLE_SIZE , sizeof (* mlz -> dict )); <nl>  <nl> mlz -> flush_code = FLUSH_CODE ; <nl> mlz -> current_dic_index_max = DIC_INDEX_INIT ;
static int dirac_decode_picture_header ( DiracContext * s ) <nl> get_buffer_with_edge ( s -> avctx , s -> ref_pics [ i ]-> avframe , AV_GET_BUFFER_FLAG_REF ); <nl> break ; <nl> } <nl> + <nl> + if (! s -> ref_pics [ i ]) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Reference could not be allocated \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> } <nl>  <nl> /* retire the reference frames that are not used anymore */
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> } <nl> break ; <nl> case MKTAG (' s ', ' t ', ' r ', ' d '): <nl> + st = s -> streams [ stream_index ]; <nl> if ( stream_index >= ( unsigned ) s -> nb_streams || st -> codec -> extradata_size ) { <nl> avio_skip ( pb , size ); <nl> } else {
static int encode_frame ( AVCodecContext * avctx , unsigned char * buf , <nl> return - 1 ; <nl> } <nl> if (! is_yuv ) <nl> - s -> bpp_tab_size = ( s -> bpp >> 3 ); <nl> + s -> bpp_tab_size = (( s -> bpp + 7 ) >> 3 ); <nl>  <nl> if ( s -> compr == TIFF_DEFLATE || s -> compr == TIFF_ADOBE_DEFLATE || s -> compr == TIFF_LZW ) <nl> // best choose for DEFLATE
 <nl> FFTContext * av_fft_init ( int nbits , int inverse ) <nl> { <nl> - FFTContext * s = av_malloc ( sizeof (* s )); <nl> + FFTContext * s = av_mallocz ( sizeof (* s )); <nl>  <nl> if ( s && ff_fft_init ( s , nbits , inverse )) <nl> av_freep (& s );
static int lag_decode_prob ( GetBitContext * gb , uint32_t * value ) <nl> } <nl>  <nl> val = get_bits_long ( gb , bits ); <nl> - val |= 1 << bits ; <nl> + val |= 1U << bits ; <nl>  <nl> * value = val - 1 ; <nl> 
static av_cold int vtenc_close ( AVCodecContext * avctx ) <nl>  <nl> if (! vtctx -> session ) return 0 ; <nl>  <nl> - VTCompressionSessionInvalidate ( vtctx -> session ); <nl> pthread_cond_destroy (& vtctx -> cv_sample_sent ); <nl> pthread_mutex_destroy (& vtctx -> lock ); <nl> CFRelease ( vtctx -> session );
static int decode_codestream ( J2kDecoderContext * s ) <nl> } <nl>  <nl> marker = bytestream_get_be16 (& s -> buf ); <nl> + if ( s -> avctx -> debug & FF_DEBUG_STARTCODE ) <nl> + av_log ( s -> avctx , AV_LOG_DEBUG , " marker 0x %. 4X at pos 0x % x \ n ", marker , s -> buf - s -> buf_start - 4 ); <nl> oldbuf = s -> buf ; <nl>  <nl> if ( marker == J2K_SOD ){
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> int err ; <nl>  <nl> if ( buf_index >= next_avc ) { <nl> - if ( buf_index >= buf_size ) break ; <nl> + if ( buf_index >= buf_size - h -> nal_length_size ) break ; <nl> nalsize = 0 ; <nl> for ( i = 0 ; i < h -> nal_length_size ; i ++) <nl> nalsize = ( nalsize << 8 ) | buf [ buf_index ++];
int av_tempfile ( const char * prefix , char ** filename , int log_offset , void * log_c <nl> if ( fd < 0 ) { <nl> int err = AVERROR ( errno ); <nl> av_log (& file_log_ctx , AV_LOG_ERROR , " ff_tempfile : Cannot open temporary file % s \ n ", * filename ); <nl> + av_freep ( filename ); <nl> return err ; <nl> } <nl> return fd ; /* success */
static int tgv_decode_frame ( AVCodecContext * avctx , <nl> frame -> pict_type = AV_PICTURE_TYPE_I ; <nl>  <nl> if (! s -> frame_buffer && <nl> - !( s -> frame_buffer = av_malloc ( s -> width * s -> height ))) <nl> + !( s -> frame_buffer = av_mallocz ( s -> width * s -> height ))) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> if ( unpack ( buf , buf_end , s -> frame_buffer , s -> avctx -> width , s -> avctx -> height ) < 0 ) {
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> bytestream2_init (& gbc , avpkt -> data , avpkt -> size ); <nl> if ( bytestream2_get_bytes_left (& gbc ) >= 552 <nl> - && ! check_header ( gbc . buffer , bytestream2_get_bytes_left (& gbc )) <nl> && check_header ( gbc . buffer + 512 , bytestream2_get_bytes_left (& gbc ) - 512 ) <nl> ) <nl> bytestream2_skip (& gbc , 512 );
static int decode_dds1 ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> return AVERROR_INVALIDDATA ; <nl> frame += v ; <nl> } else { <nl> - if ( frame_end - frame < width + 3 ) <nl> + if ( frame_end - frame < width + 4 ) <nl> return AVERROR_INVALIDDATA ; <nl> frame [ 0 ] = frame [ 1 ] = <nl> frame [ width ] = frame [ width + 1 ] = bytestream2_get_byte ( gb );
static int mov_write_header ( AVFormatContext * s ) <nl> AVStream * st = s -> streams [ i ]; <nl> if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO || <nl> st -> codec -> codec_type == AVMEDIA_TYPE_AUDIO ) { <nl> - ff_mov_init_hinting ( s , hint_track , i ); <nl> + if ( ff_mov_init_hinting ( s , hint_track , i ) < 0 ) <nl> + goto error ; <nl> hint_track ++; <nl> } <nl> }
static int decode_sei ( H264Context * h ){ <nl>  <nl> switch ( type ){ <nl> case 5 : <nl> - if ( decode_unregistered_user_data ( h , size ) < 0 ); <nl> + if ( decode_unregistered_user_data ( h , size ) < 0 ) <nl> return - 1 ; <nl> break ; <nl> default :
static int read_interval_packets ( WriterContext * w , AVFormatContext * fmt_ctx , <nl> AVPacket pkt , pkt1 ; <nl> AVFrame frame ; <nl> int ret = 0 , i = 0 , frame_count = 0 ; <nl> - int64_t start , end = interval -> end ; <nl> + int64_t start = - INT64_MAX , end = interval -> end ; <nl> int has_start = 0 , has_end = interval -> has_end && ! interval -> end_is_offset ; <nl>  <nl> av_init_packet (& pkt );
typedef struct SmcContext { <nl> row_ptr += stride * 4 ; \ <nl> } \ <nl> total_blocks --; \ <nl> - if ( total_blocks < 0 ) \ <nl> + if ( total_blocks < 0 + !! n_blocks ) \ <nl> { \ <nl> av_log ( s -> avctx , AV_LOG_INFO , " warning : block counter just went negative ( this should not happen )\ n "); \ <nl> return ; \
int ff_hevc_cu_qp_delta_abs ( HEVCContext * s ) <nl> suffix_val += 1 << k ; <nl> k ++; <nl> } <nl> - if ( k == CABAC_MAX_BIN ) <nl> + if ( k == CABAC_MAX_BIN ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " CABAC_MAX_BIN : % d \ n ", k ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> while ( k --) <nl> suffix_val += get_cabac_bypass (& s -> HEVClc -> cc ) << k ;
static int ffm_seek ( AVFormatContext * s , int stream_index , int64_t wanted_pts , in <nl> while ( pos_min <= pos_max ) { <nl> pts_min = get_dts ( s , pos_min ); <nl> pts_max = get_dts ( s , pos_max ); <nl> - if ( pts_min > wanted_pts || pts_max < wanted_pts ) { <nl> + if ( pts_min > wanted_pts || pts_max <= wanted_pts ) { <nl> pos = pts_min > wanted_pts ? pos_min : pos_max ; <nl> goto found ; <nl> }
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> return ret ; <nl> } <nl> } else { <nl> - if (( ret = ff_MPV_common_init ( s ) < 0 )) { <nl> + if (( ret = ff_MPV_common_init ( s )) < 0 ) { <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " ff_MPV_common_init () failed .\ n "); <nl> return ret ; <nl> }
typedef struct A64Context { <nl> AVLFG randctx ; <nl> int mc_lifetime ; <nl> int mc_use_5col ; <nl> - int mc_frame_counter ; <nl> + unsigned mc_frame_counter ; <nl> int * mc_meta_charset ; <nl> int * mc_charmap ; <nl> int * mc_best_cb ;
static int jpeg2000_read_main_headers ( Jpeg2000DecoderContext * s ) <nl> if ( marker == JPEG2000_EOC ) <nl> break ; <nl>  <nl> - if ( bytestream2_get_bytes_left (& s -> g ) < 2 ) <nl> - return AVERROR_INVALIDDATA ; <nl> len = bytestream2_get_be16u (& s -> g ); <nl> + if ( len < 2 || bytestream2_get_bytes_left (& s -> g ) < len - 2 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> switch ( marker ) { <nl> case JPEG2000_SIZ : <nl> ret = get_siz ( s );
static int lag_read_prob_header ( lag_rac * rac , GetBitContext * gb ) <nl> } <nl>  <nl> scale_factor ++; <nl> - cumulative_target = 1 << scale_factor ; <nl> + if ( scale_factor >= 32U ) <nl> + return AVERROR_INVALIDDATA ; <nl> + cumulative_target = 1U << scale_factor ; <nl>  <nl> if ( scaled_cumul_prob > cumulative_target ) { <nl> av_log ( rac -> avctx , AV_LOG_ERROR ,
static int xan_decode_frame_type0 ( AVCodecContext * avctx ) <nl> int dec_size ; <nl>  <nl> bytestream2_seek (& s -> gb , 8 + corr_off , SEEK_SET ); <nl> - dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size ); <nl> + dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size / 2 ); <nl> if ( dec_size < 0 ) <nl> dec_size = 0 ; <nl> for ( i = 0 ; i < dec_size ; i ++)
static int read_thread ( void * arg ) <nl> } <nl> if ( is -> queue_attachments_req ) { <nl> if ( is -> video_st && is -> video_st -> disposition & AV_DISPOSITION_ATTACHED_PIC ) { <nl> - AVPacket copy ; <nl> + AVPacket copy = { 0 }; <nl> if (( ret = av_copy_packet (& copy , & is -> video_st -> attached_pic )) < 0 ) <nl> goto fail ; <nl> packet_queue_put (& is -> videoq , & copy );
FF_ENABLE_DEPRECATION_WARNINGS <nl> /* Encode a dummy frame to get the extradata immediately */ <nl> if ( x -> quicktime_format ) { <nl> AVFrame * picture ; <nl> - AVPacket packet ; <nl> + AVPacket packet = { 0 }; <nl> int size , got_packet , ret ; <nl>  <nl> av_init_packet (& packet );
static int update_frame_pool ( AVCodecContext * avctx , AVFrame * frame ) <nl> break ; <nl> } <nl> case AVMEDIA_TYPE_AUDIO : { <nl> - int ch = av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> + int ch = av_frame_get_channels ( frame ); // av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> int planar = av_sample_fmt_is_planar ( frame -> format ); <nl> int planes = planar ? ch : 1 ; <nl> 
static av_cold int adx_encode_init ( AVCodecContext * avctx ) <nl> avctx -> frame_size = BLOCK_SAMPLES ; <nl>  <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (! avctx -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> /* the cutoff can be adjusted , but this seems to work pretty well */ <nl> c -> cutoff = 500 ;
static int vmd_read_header ( AVFormatContext * s ) <nl> vst -> codec -> width >>= 1 ; <nl> vst -> codec -> height >>= 1 ; <nl> } <nl> - vst -> codec -> extradata_size = VMD_HEADER_SIZE ; <nl> vst -> codec -> extradata = av_mallocz ( VMD_HEADER_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! vst -> codec -> extradata ) <nl> + return AVERROR ( ENOMEM ); <nl> + vst -> codec -> extradata_size = VMD_HEADER_SIZE ; <nl> memcpy ( vst -> codec -> extradata , vmd -> vmd_header , VMD_HEADER_SIZE ); <nl> } <nl> 
static int compat_decode ( AVCodecContext * avctx , AVFrame * frame , <nl> int * got_frame , const AVPacket * pkt ) <nl> { <nl> AVCodecInternal * avci = avctx -> internal ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> av_assert0 ( avci -> compat_decode_consumed == 0 ); <nl> 
static int msf_probe ( AVProbeData * p ) <nl> if ( AV_RB32 ( p -> buf + 16 ) <= 0 ) <nl> return 0 ; <nl>  <nl> + if ( AV_RB32 ( p -> buf + 4 ) > 16 ) <nl> + return AVPROBE_SCORE_MAX / 5 ; // unsupported / unknown codec <nl> + <nl> return AVPROBE_SCORE_MAX / 3 * 2 ; <nl> } <nl> 
static int mv_read_header ( AVFormatContext * avctx ) <nl> { <nl> MvContext * mv = avctx -> priv_data ; <nl> AVIOContext * pb = avctx -> pb ; <nl> - AVStream * ast , * vst ; <nl> + AVStream * ast = NULL , * vst = NULL ; // initialization to suppress warning <nl> int version , i ; <nl>  <nl> avio_skip ( pb , 4 );
static void ra144_encode_subblock ( RA144Context * ractx , <nl> float zero [ BLOCKSIZE ], cba [ BLOCKSIZE ], cb1 [ BLOCKSIZE ], cb2 [ BLOCKSIZE ]; <nl> int16_t cba_vect [ BLOCKSIZE ]; <nl> int cba_idx , cb1_idx , cb2_idx , gain ; <nl> - int i , n , m [ 3 ]; <nl> + int i , n ; <nl> + unsigned m [ 3 ]; <nl> float g [ 3 ]; <nl> float error , best_error ; <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> int prev_y = 0 , prev_u = 0 , prev_v = 0 ; <nl> uint8_t * rbuf ; <nl>  <nl> + if ( buf_size <= 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " buf_size % d is too small \ n ", buf_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> rbuf = av_malloc ( buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! rbuf ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n ");
AVIOContext * avio_alloc_context ( <nl> int64_t (* seek )( void * opaque , int64_t offset , int whence )) <nl> { <nl> AVIOContext * s = av_mallocz ( sizeof ( AVIOContext )); <nl> + if (! s ) <nl> + return NULL ; <nl> ffio_init_context ( s , buffer , buffer_size , write_flag , opaque , <nl> read_packet , write_packet , seek ); <nl> return s ;
int check_stream_specifier ( AVFormatContext * s , AVStream * st , const char * spec ) <nl> case ' a ': type = AVMEDIA_TYPE_AUDIO ; break ; <nl> case ' s ': type = AVMEDIA_TYPE_SUBTITLE ; break ; <nl> case ' d ': type = AVMEDIA_TYPE_DATA ; break ; <nl> + default : abort (); // never reached , silence warning <nl> } <nl> if ( type != st -> codec -> codec_type ) <nl> return 0 ;
error : <nl> static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , <nl> int S ) <nl> { <nl> - int bit ; <nl> + unsigned bit ; <nl>  <nl> if ( s -> extra_bits ) { <nl> S <<= s -> extra_bits ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> frame_len = 1 << frame_len_bits ; <nl>  <nl> - if ( s -> channels > MAX_CHANNELS ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + if ( avctx -> channels > MAX_CHANNELS ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", avctx -> channels ); <nl> return - 1 ; <nl> } <nl> 
int ff_img_read_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> ret = avio_read ( f [ 0 ], header , PROBE_BUF_MIN ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + memset ( header + ret , 0 , sizeof ( header ) - ret ); <nl> avio_skip ( f [ 0 ], - ret ); <nl> pd . buf = header ; <nl> pd . buf_size = ret ;
static int open_file ( AVFormatContext * avf , unsigned fileno ) <nl> if (! cat -> avf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - cat -> avf -> flags |= avf -> flags ; <nl> + cat -> avf -> flags |= avf -> flags & ~ AVFMT_FLAG_CUSTOM_IO ; <nl> cat -> avf -> interrupt_callback = avf -> interrupt_callback ; <nl>  <nl> if (( ret = ff_copy_whiteblacklists ( cat -> avf , avf )) < 0 )
static int xan_decode_frame ( AVCodecContext * avctx , <nl> } <nl> buf_size = buf_end - buf ; <nl> } <nl> + if ( s -> palettes_count <= 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " No palette found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = avctx -> get_buffer ( avctx , & s -> current_frame ))) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return ret ;
static int opus_decode_frame ( OpusStreamContext * s , const uint8_t * data , int size <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Error resampling SILK data .\ n "); <nl> return samples ; <nl> } <nl> + av_assert2 (( samples & 7 ) == 0 ); <nl> s -> delayed_samples += s -> packet . frame_duration - samples ; <nl> } else <nl> ff_silk_flush ( s -> silk );
static void compute_default_clut ( AVSubtitleRect * rect , int w , int h ) <nl> list_inv [ i ] = bestv ; <nl> } <nl>  <nl> - count = i - 1 ; <nl> + count = FFMAX ( i - 1 , 1 ); <nl> for ( i --; i >= 0 ; i --) { <nl> int v = i * 255 / count ; <nl> AV_WN32 ( rect -> data [ 1 ] + 4 * list_inv [ i ], RGBA ( v / 2 , v , v / 2 , v ));
static void show_packets ( WriterContext * w , AVFormatContext * fmt_ctx ) <nl> if ( do_show_frames && <nl> get_video_frame ( fmt_ctx , & frame , & pkt )) { <nl> show_frame ( w , & frame , fmt_ctx -> streams [ pkt . stream_index ]); <nl> - av_destruct_packet (& pkt ); <nl> + av_free_packet (& pkt ); <nl> } <nl> } <nl> av_init_packet (& pkt );
static int process_output_surface ( AVCodecContext * avctx , AVPacket * pkt , NvencSur <nl> } <nl> slice_offsets = av_mallocz ( slice_mode_data * sizeof (* slice_offsets )); <nl>  <nl> - if (! slice_offsets ) <nl> + if (! slice_offsets ) { <nl> + res = AVERROR ( ENOMEM ); <nl> goto error ; <nl> + } <nl>  <nl> lock_params . version = NV_ENC_LOCK_BITSTREAM_VER ; <nl> 
static int ivi_init_tiles ( IVIBandDesc * band , IVITile * ref_tile , <nl> band -> mb_size ); <nl>  <nl> av_freep (& tile -> mbs ); <nl> - tile -> mbs = av_malloc ( tile -> num_MBs * sizeof ( IVIMbInfo )); <nl> + tile -> mbs = av_mallocz ( tile -> num_MBs * sizeof ( IVIMbInfo )); <nl> if (! tile -> mbs ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl> } <nl> } else { <nl> - avpriv_request_sample ( s , " Uncompressed image "); <nl> + avpriv_request_sample ( avctx , " Uncompressed image "); <nl> return avpkt -> size ; <nl> } <nl> finish :
av_cold int ff_msmpeg4_decode_init ( AVCodecContext * avctx ) <nl> int i ; <nl> MVTable * mv ; <nl>  <nl> + if ( avctx -> width <= 0 || avctx -> height <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " invalid dimensions \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( ff_h263_decode_init ( avctx ) < 0 ) <nl> return - 1 ; <nl> 
static int g2m_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( got_header ) <nl> c -> got_header = 1 ; <nl>  <nl> - if ( c -> width && c -> height ) { <nl> + if ( c -> width && c -> height && c -> framebuf ) { <nl> if (( ret = ff_get_buffer ( avctx , pic , 0 )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return ret ;
static int ogg_write_trailer ( AVFormatContext * s ) <nl> av_free ( oggstream -> header [ 0 ]); <nl> av_free ( oggstream -> header [ 1 ]); <nl> } <nl> + else <nl> + av_free ( oggstream -> header [ 1 ]); <nl> av_freep (& st -> priv_data ); <nl> } <nl> return 0 ;
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> flv_same_video_codec ( st -> codec , flags )) { <nl> break ; <nl> } <nl> - } else if ( st -> id == stream_type ) { <nl> - break ; <nl> + } else if ( stream_type == FLV_STREAM_TYPE_DATA ) { <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_DATA ) <nl> + break ; <nl> } <nl> } <nl> if ( i == s -> nb_streams ){
static int h261_decode_mb ( H261Context * h ){ <nl>  <nl> // Read mtype <nl> h -> mtype = get_vlc2 (& s -> gb , h261_mtype_vlc . table , H261_MTYPE_VLC_BITS , 2 ); <nl> + if ( h -> mtype < 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " illegal mtype % d \ n ", h -> mtype ); <nl> + return SLICE_ERROR ; <nl> + } <nl> h -> mtype = h261_mtype_map [ h -> mtype ]; <nl>  <nl> // Read mquant
static int encode_picture_ls ( AVCodecContext * avctx , AVPacket * pkt , <nl>  <nl> ls_store_lse ( state , & pb ); <nl>  <nl> - zero = av_mallocz ( p -> linesize [ 0 ]); <nl> + zero = av_mallocz ( FFABS ( p -> linesize [ 0 ])); <nl> + if (! zero ) <nl> + return AVERROR ( ENOMEM ); <nl> last = zero ; <nl> cur = p -> data [ 0 ]; <nl> if ( avctx -> pix_fmt == PIX_FMT_GRAY8 ){
static void ff_compute_band_indexes ( MPADecodeContext * s , GranuleDef * g ) <nl> else <nl> g -> long_end = 6 ; <nl>  <nl> - g -> short_start = 2 + ( s -> sample_rate_index != 8 ); <nl> + g -> short_start = 3 ; <nl> } else { <nl> g -> long_end = 0 ; <nl> g -> short_start = 0 ;
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> offset = matroska_decode_buffer (& pkt_data ,& pkt_size , track ); <nl> if ( offset < 0 ) <nl> continue ; <nl> + av_assert0 ( offset + pkt_size >= pkt_size ); <nl> } <nl>  <nl> pkt = av_mallocz ( sizeof ( AVPacket ));
static int create_filter ( AVFilterContext ** filt_ctx , AVFilterGraph * ctx , int ind <nl> return ret ; <nl> } <nl>  <nl> - if (! strcmp ( filt_name , " scale ") && ! strstr ( args , " flags ")) { <nl> + if (! strcmp ( filt_name , " scale ") && args && ! strstr ( args , " flags ")) { <nl> snprintf ( tmp_args , sizeof ( tmp_args ), "% s :% s ", <nl> args , ctx -> scale_sws_opts ); <nl> args = tmp_args ;
static void fix_bitshift ( ShortenContext * s , int32_t * buffer ) <nl>  <nl> if ( s -> bitshift != 0 ) <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - buffer [ s -> nwrap + i ] <<= s -> bitshift ; <nl> + buffer [ i ] <<= s -> bitshift ; <nl> } <nl>  <nl> 
static inline int read_line ( AVFormatContext * s , char * rbuf , const int rbufsize , <nl>  <nl> do { <nl> ret = ffurl_read_complete ( rt -> rtsp_hd , rbuf + idx , 1 ); <nl> - if ( ret < 0 ) <nl> - return ret ; <nl> + if ( ret <= 0 ) <nl> + return ret ? ret : AVERROR_EOF ; <nl> if ( rbuf [ idx ] == '\ r ') { <nl> /* Ignore */ <nl> } else if ( rbuf [ idx ] == '\ n ') {
static int mov_read_stsz ( MOVContext * c , ByteIOContext * pb , MOVAtom atom ) <nl> return - 1 ; <nl> } <nl>  <nl> - if ( entries >= UINT_MAX / sizeof ( int )) <nl> + if ( entries >= UINT_MAX / sizeof ( int ) || entries >= ( UINT_MAX - 4 ) / field_size ) <nl> return - 1 ; <nl> sc -> sample_sizes = av_malloc ( entries * sizeof ( int )); <nl> if (! sc -> sample_sizes )
static int X264_frame ( AVCodecContext * ctx , AVPacket * pkt , const AVFrame * frame , <nl> X264Context * x4 = ctx -> priv_data ; <nl> x264_nal_t * nal ; <nl> int nnal , i , ret ; <nl> - x264_picture_t pic_out ; <nl> + x264_picture_t pic_out = { 0 }; <nl>  <nl> x264_picture_init ( & x4 -> pic ); <nl> x4 -> pic . img . i_csp = x4 -> params . i_csp ;
static int ra144_decode_frame ( AVCodecContext * avctx , void * data , <nl> do_output_subblock ( ractx , block_coefs [ i ], refl_rms [ i ], & gb ); <nl>  <nl> for ( j = 0 ; j < BLOCKSIZE ; j ++) <nl> - * samples ++ = av_clip_int16 ( ractx -> curr_sblock [ j + 10 ] << 2 ); <nl> + * samples ++ = av_clip_int16 ( ractx -> curr_sblock [ j + 10 ] * ( 1 << 2 )); <nl> } <nl>  <nl> ractx -> old_energy = energy ;
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> tag = avio_rl32 ( pb ); <nl> size = avio_rl32 ( pb ); <nl>  <nl> + if ( size > avi -> fsize ){ <nl> + av_log ( s , AV_LOG_ERROR , " chunk size is too big during header parsing \ n "); <nl> + goto fail ; <nl> + } <nl> + <nl> print_tag (" tag ", tag , size ); <nl>  <nl> switch ( tag ) {
static int gxf_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> } <nl>  <nl> static int gxf_seek ( AVFormatContext * s , int stream_index , int64_t timestamp , int flags ) { <nl> - int res = 0 ; <nl> + int64_t res = 0 ; <nl> uint64_t pos ; <nl> uint64_t maxlen = 100 * 1024 * 1024 ; <nl> AVStream * st = s -> streams [ 0 ];
static av_cold int read_specific_config ( ALSDecContext * ctx ) <nl> GetBitContext gb ; <nl> uint64_t ht_size ; <nl> int i , config_offset ; <nl> - MPEG4AudioConfig m4ac ; <nl> + MPEG4AudioConfig m4ac = { 0 }; <nl> ALSSpecificConfig * sconf = & ctx -> sconf ; <nl> AVCodecContext * avctx = ctx -> avctx ; <nl> uint32_t als_id , header_size , trailer_size ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> for ( x = 0 ; x < num_possible_block_sizes ; x ++) { <nl> int v = 0 ; <nl> while ( s -> sfb_offsets [ x ][ v + 1 ] << x < offset ) <nl> - ++ v ; <nl> + if (++ v >= MAX_BANDS ) <nl> + return AVERROR_INVALIDDATA ; <nl> s -> sf_offsets [ i ][ x ][ b ] = v ; <nl> } <nl> }
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> goto fail ; <nl>  <nl> s -> cbf_luma = av_malloc ( sps -> min_tb_width * sps -> min_tb_height ); <nl> - s -> tab_ipm = av_malloc ( min_pu_size ); <nl> + s -> tab_ipm = av_mallocz ( min_pu_size ); <nl> s -> is_pcm = av_malloc ( min_pu_size ); <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ;
 <nl> static av_always_inline int even ( uint64_t layout ) <nl> { <nl> - return (! layout || ( layout & ( layout - 1 ))); <nl> + return (! layout || !!( layout & ( layout - 1 ))); <nl> } <nl>  <nl> static int sane_layout ( uint64_t layout )
int av_reallocp_array ( void * ptr , size_t nmemb , size_t size ) <nl> { <nl> void ** ptrptr = ptr ; <nl> * ptrptr = av_realloc_f (* ptrptr , nmemb , size ); <nl> - if (!* ptrptr && !( nmemb && size )) <nl> + if (!* ptrptr && nmemb && size ) <nl> return AVERROR ( ENOMEM ); <nl> return 0 ; <nl> }
static int video_thread ( void * arg ) <nl> ret = queue_picture ( is , frame , pts , duration , frame -> pkt_pos , is -> viddec . pkt_serial ); <nl> av_frame_unref ( frame ); <nl> # if CONFIG_AVFILTER <nl> + if ( is -> videoq . serial != is -> viddec . pkt_serial ) <nl> + break ; <nl> } <nl> # endif <nl> 
fixup_vorbis_headers ( AVFormatContext * as , struct oggvorbis_private * priv , <nl> len = priv -> len [ 0 ] + priv -> len [ 1 ] + priv -> len [ 2 ]; <nl> buf_len = len + len / 255 + 64 ; <nl> ptr = * buf = av_realloc ( NULL , buf_len ); <nl> + if (!* buf ) <nl> + return 0 ; <nl> memset (* buf , '\ 0 ', buf_len ); <nl>  <nl> ptr [ 0 ] = 2 ;
static int64_t find_best_filter ( const DCAADPCMEncContext * s , const int32_t * in , <nl> { <nl> const premultiplied_coeffs * precalc_data = s -> private_data ; <nl> int i , j , k = 0 ; <nl> - int vq ; <nl> + int vq = - 1 ; <nl> int64_t err ; <nl> int64_t min_err = 1ll << 62 ; <nl> int64_t corr [ 15 ];
static int hnm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int ret ; <nl> uint16_t chunk_id ; <nl>  <nl> + if ( avpkt -> size < 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " packet too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = ff_get_buffer ( avctx , frame , 0 )) < 0 ) <nl> return ret ; <nl> 
int ff_init_vlc_sparse ( VLC * vlc , int nb_bits , int nb_codes , <nl> av_dlog ( NULL , " build table nb_codes =% d \ n ", nb_codes ); <nl>  <nl> buf = av_malloc (( nb_codes + 1 ) * sizeof ( VLCcode )); <nl> + if (! buf ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> av_assert0 ( symbols_size <= 2 || ! symbols ); <nl> j = 0 ;
static int mpeg_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( avctx -> extradata && ! avctx -> frame_number ) { <nl> int ret = decode_chunks ( avctx , picture , data_size , avctx -> extradata , avctx -> extradata_size ); <nl> + * data_size = 0 ; <nl> if ( ret < 0 && ( avctx -> err_recognition & AV_EF_EXPLODE )) <nl> return ret ; <nl> }
static int dxv_decompress_raw ( AVCodecContext * avctx ) <nl> DXVContext * ctx = avctx -> priv_data ; <nl> GetByteContext * gbc = & ctx -> gbc ; <nl>  <nl> + if ( bytestream2_get_bytes_left ( gbc ) < ctx -> tex_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> bytestream2_get_buffer ( gbc , ctx -> tex_data , ctx -> tex_size ); <nl> return 0 ; <nl> }
static int gdv_decode_frame ( AVCodecContext * avctx , void * data , <nl> unsigned r = bytestream2_get_byte ( gb ); <nl> unsigned g = bytestream2_get_byte ( gb ); <nl> unsigned b = bytestream2_get_byte ( gb ); <nl> - gdv -> pal [ i ] = 0xFF << 24 | r << 18 | g << 10 | b << 2 ; <nl> + gdv -> pal [ i ] = 0xFFU << 24 | r << 18 | g << 10 | b << 2 ; <nl> } <nl> break ; <nl> case 3 :
static int filter_frame ( AVFilterLink * inlink , AVFrame * insamples ) <nl> break ; <nl> av_assert1 ( input_number < am -> nb_inputs ); <nl> if ( ff_bufqueue_is_full (& am -> in [ input_number ]. queue )) { <nl> - av_log ( ctx , AV_LOG_ERROR , " Buffer queue overflow \ n "); <nl> av_frame_free (& insamples ); <nl> return AVERROR ( ENOMEM ); <nl> }
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
pp_context * pp_get_context ( int width , int height , int cpuCaps ){ <nl> int stride = FFALIGN ( width , 16 ); // assumed / will realloc if needed <nl> int qpStride = ( width + 15 )/ 16 + 2 ; // assumed / will realloc if needed <nl>  <nl> + if (! c ) <nl> + return NULL ; <nl> + <nl> c -> av_class = & av_codec_context_class ; <nl> if ( cpuCaps & PP_FORMAT ){ <nl> c -> hChromaSubSample = cpuCaps & 0x3 ;
void mpeg_motion_internal ( MpegEncContext * s , <nl> { <nl> uint8_t * ptr_y , * ptr_cb , * ptr_cr ; <nl> int dxy , uvdxy , mx , my , src_x , src_y , <nl> - uvsrc_x , uvsrc_y , v_edge_pos , uvlinesize , linesize ; <nl> + uvsrc_x , uvsrc_y , v_edge_pos ; <nl> + ptrdiff_t uvlinesize , linesize ; <nl>  <nl> # if 0 <nl> if ( s -> quarter_sample )
static av_cold void init_cplscales_table ( COOKContext * q ) <nl> static inline int decode_bytes ( const uint8_t * inbuffer , uint8_t * out , int bytes ) <nl> { <nl> static const uint32_t tab [ 4 ] = { <nl> - AV_BE2NE32C ( 0x37c511f2 ), AV_BE2NE32C ( 0xf237c511 ), <nl> - AV_BE2NE32C ( 0x11f237c5 ), AV_BE2NE32C ( 0xc511f237 ), <nl> + AV_BE2NE32C ( 0x37c511f2U ), AV_BE2NE32C ( 0xf237c511U ), <nl> + AV_BE2NE32C ( 0x11f237c5U ), AV_BE2NE32C ( 0xc511f237U ), <nl> }; <nl> int i , off ; <nl> uint32_t c ;
static int idcin_read_packet ( AVFormatContext * s , <nl> chunk_size = avio_rl32 ( pb ); <nl> /* skip the number of decoded bytes ( always equal to width * height ) */ <nl> avio_skip ( pb , 4 ); <nl> + if ( chunk_size < 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> chunk_size -= 4 ; <nl> ret = av_get_packet ( pb , pkt , chunk_size ); <nl> if ( ret < 0 )
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> INC_MED ( 1 ); <nl> DEC_MED ( 2 ); <nl> } else { <nl> - base = GET_MED ( 0 ) + GET_MED ( 1 ) + GET_MED ( 2 ) * ( t - 2 ); <nl> + base = GET_MED ( 0 ) + GET_MED ( 1 ) + GET_MED ( 2 ) * ( t - 2U ); <nl> add = GET_MED ( 2 ) - 1 ; <nl> INC_MED ( 0 ); <nl> INC_MED ( 1 );
static av_cold int init ( AVFilterContext * ctx , const char * args ) <nl> av_log ( ctx , AV_LOG_ERROR , " Invalid parameter .\ n "); <nl> return AVERROR ( EINVAL ); <nl> } <nl> - args += strlen ( name )+ 1 ; <nl> + args += strlen ( name ); <nl> + if ( args [ 0 ] == '=') <nl> + args ++; <nl>  <nl> for ( i = 0 ; ; i ++){ <nl> if (! filters [ i ] || ! strcmp ( name , filters [ i ]-> name ))
static void dwt_decode97_int ( DWTContext * s , int32_t * t ) <nl> line += 5 ; <nl>  <nl> for ( i = 0 ; i < w * h ; i ++) <nl> - data [ i ] <<= I_PRESHIFT ; <nl> + data [ i ] *= 1 << I_PRESHIFT ; <nl>  <nl> for ( lev = 0 ; lev < s -> ndeclevels ; lev ++) { <nl> int lh = s -> linelen [ lev ][ 0 ],
static av_cold int init ( AVCodecContext * avctx ) <nl> int dummy_int ; <nl>  <nl> /* Back up the extradata so it can be restored at close time . */ <nl> - priv -> orig_extradata = av_malloc ( avctx -> extradata_size ); <nl> + priv -> orig_extradata = av_malloc ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! priv -> orig_extradata ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Failed to allocate copy of extradata \ n ");
int av_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , <nl> " Dropped corrupted packet ( stream = % d )\ n ", <nl> pkt -> stream_index ); <nl> + av_free_packet ( pkt ); <nl> continue ; <nl> } <nl> 
# define CODE_UNSET - 1 <nl> # define CODE_BIT_INIT 9 <nl> # define DIC_INDEX_INIT 512 // 2 ^ 9 <nl> -# define DIC_INDEX_MAX 32768l // 2 ^ 15 <nl> +# define DIC_INDEX_MAX 32768 // 2 ^ 15 <nl> # define FLUSH_CODE 256 <nl> # define FREEZE_CODE 257 <nl> # define FIRST_CODE 258 <nl> -# define MAX_CODE 32767l <nl> -# define TABLE_SIZE 35023l // TABLE_SIZE must be a prime number <nl> +# define MAX_CODE 32767 <nl> +# define TABLE_SIZE 35023 // TABLE_SIZE must be a prime number <nl>  <nl> /** Dictionary structure for mlz decompression <nl> */
static int read_header ( AVFormatContext * s ) <nl> avio_skip ( s -> pb , 1 ); // padding <nl>  <nl> st -> codec -> sample_rate = bfstm ? read32 ( s ) : read16 ( s ); <nl> - if (! st -> codec -> sample_rate ) <nl> + if ( st -> codec -> sample_rate <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (! bfstm )
# include " libavcodec / bytestream . h " <nl> # include " libavformat / avformat . h " <nl>  <nl> -# include < FuzzerInterface . h > <nl> + int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ); <nl>  <nl> static void error ( const char * err ) <nl> {
FF_ENABLE_DEPRECATION_WARNINGS <nl> if ( avctx -> level > 0 ) <nl> x4 -> params . i_level_idc = avctx -> level ; <nl>  <nl> - x4 -> params . rc . f_rate_tolerance = <nl> - ( float ) avctx -> bit_rate_tolerance / avctx -> bit_rate ; <nl> + if ( avctx -> bit_rate > 0 ) <nl> + x4 -> params . rc . f_rate_tolerance = <nl> + ( float ) avctx -> bit_rate_tolerance / avctx -> bit_rate ; <nl>  <nl> if (( avctx -> rc_buffer_size ) && <nl> ( avctx -> rc_initial_buffer_occupancy <= avctx -> rc_buffer_size )) {
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> continue ; <nl> } <nl> v -> second_field = 1 ; <nl> - v -> blocks_off = s -> mb_width * s -> mb_height << 1 ; <nl> + v -> blocks_off = s -> b8_stride * ( s -> mb_height &~ 1 ); <nl> v -> mb_off = s -> mb_stride * s -> mb_height >> 1 ; <nl> } else { <nl> v -> second_field = 0 ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> frame_len = 1 << frame_len_bits ; <nl>  <nl> - if ( s -> channels > MAX_CHANNELS ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + if ( avctx -> channels > MAX_CHANNELS ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", avctx -> channels ); <nl> return - 1 ; <nl> } <nl> 
static av_always_inline av_const int32_t av_clipl_int32_c ( int64_t a ) <nl> */ <nl> static av_always_inline av_const int av_clip_intp2_c ( int a , int p ) <nl> { <nl> - if (( a + ( 1 << p )) & ~(( 2 << p ) - 1 )) <nl> + if ((( unsigned ) a + ( 1 << p )) & ~(( 2 << p ) - 1 )) <nl> return ( a >> 31 ) ^ (( 1 << p ) - 1 ); <nl> else <nl> return a ;
static int apng_read_header ( AVFormatContext * s ) <nl> AVIOContext * pb = s -> pb ; <nl> uint32_t len , tag ; <nl> AVStream * st ; <nl> - int ret = AVERROR_INVALIDDATA , acTL_found = 0 ; <nl> + int acTL_found = 0 ; <nl> + int64_t ret = AVERROR_INVALIDDATA ; <nl>  <nl> /* verify PNGSIG */ <nl> if ( avio_rb64 ( pb ) != PNGSIG )
static void dmix_sub_c ( int32_t * dst , const int32_t * src , int coeff , ptrdiff_t le <nl> int i ; <nl>  <nl> for ( i = 0 ; i < len ; i ++) <nl> - dst [ i ] -= mul15 ( src [ i ], coeff ); <nl> + dst [ i ] -= ( unsigned ) mul15 ( src [ i ], coeff ); <nl> } <nl>  <nl> static void dmix_add_c ( int32_t * dst , const int32_t * src , int coeff , ptrdiff_t len )
static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , <nl> unsigned bit ; <nl>  <nl> if ( s -> extra_bits ) { <nl> - S <<= s -> extra_bits ; <nl> + S *= 1 << s -> extra_bits ; <nl>  <nl> if ( s -> got_extra_bits && <nl> get_bits_left (& s -> gb_extra_bits ) >= s -> extra_bits ) {
static int thp_read_packet ( AVFormatContext * s , <nl> pkt -> stream_index = thp -> video_stream_index ; <nl> } else { <nl> ret = av_get_packet ( pb , pkt , thp -> audiosize ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( ret != thp -> audiosize ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO );
static int v410_encode_frame ( AVCodecContext * avctx , uint8_t * buf , <nl> int i , j ; <nl> int output_size = 0 ; <nl>  <nl> - if ( buf_size < avctx -> width * avctx -> height * 3 ) { <nl> + if ( buf_size < avctx -> width * avctx -> height * 4 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Out buffer is too small .\ n "); <nl> return AVERROR ( ENOMEM ); <nl> }
static int vp3_decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> last_frame = s -> golden_frame ; <nl> s -> last_frame . type = FF_BUFFER_TYPE_COPY ; <nl> + ff_thread_report_progress (& s -> last_frame , INT_MAX , 0 ); <nl> } <nl> } <nl> 
static int dpcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl> case CODEC_ID_SOL_DPCM : <nl> if ( avctx -> codec_tag != 3 ) { <nl> - uint8_t * output_samples_u8 = data ; <nl> + uint8_t * output_samples_u8 = s -> frame . data [ 0 ]; <nl> while ( buf < buf_end ) { <nl> uint8_t n = * buf ++; <nl> 
static int http_handshake ( URLContext * c ) <nl> av_log ( c , AV_LOG_TRACE , " Lower protocol \ n "); <nl> if (( ret = ffurl_handshake ( cl )) > 0 ) <nl> return 2 + ret ; <nl> - if (( ret < 0 )) <nl> + if ( ret < 0 ) <nl> return ret ; <nl> ch -> handshake_step = READ_HEADERS ; <nl> ch -> is_connected_server = 1 ;
static int xv_write_header ( AVFormatContext * s ) <nl> if ( XvQueryAdaptors ( xv -> display , DefaultRootWindow ( xv -> display ), & num_adaptors , & ai ) != Success ) <nl> return AVERROR_EXTERNAL ; <nl> xv -> xv_port = ai [ 0 ]. base_id ; <nl> + XvFreeAdaptorInfo ( ai ); <nl>  <nl> if ( encctx -> pix_fmt != AV_PIX_FMT_YUV420P ) { <nl> av_log ( s , AV_LOG_ERROR ,
static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> AVIOContext * pbx ; <nl> unsigned char * buffer = NULL ; <nl> int buffer_size = 0 ; <nl> - const ID3v2EMFunc * extra_func ; <nl> + const ID3v2EMFunc * extra_func = NULL ; <nl> unsigned char * compressed_buffer = NULL ; <nl> int compressed_buffer_size = 0 ; <nl> 
static int ac3_decode_frame ( AVCodecContext * avctx , void * data , <nl> avctx -> audio_service_type = AV_AUDIO_SERVICE_TYPE_KARAOKE ; <nl>  <nl> /* get output buffer */ <nl> + avctx -> channels = s -> out_channels ; <nl> s -> frame . nb_samples = s -> num_blocks * 256 ; <nl> if (( ret = avctx -> get_buffer ( avctx , & s -> frame )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n ");
static int mov_write_single_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int64_t frag_duration = 0 ; <nl> int size = pkt -> size ; <nl>  <nl> + int ret = check_pkt ( s , pkt ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> if ( mov -> flags & FF_MOV_FLAG_FRAG_DISCONT ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> nb_streams ; i ++)
static int parse_cookie ( HTTPContext * s , const char * p , AVDictionary ** cookies ) <nl> } <nl> } <nl> } <nl> + av_dict_free (& new_params ); <nl>  <nl> // duplicate the cookie name ( dict will dupe the value ) <nl> if (!( eql = strchr ( p , '='))) return AVERROR ( EINVAL );
static int decode_frame ( AVCodecContext * avctx , <nl> buf_size - offset ); <nl> if ( t < 0 ) { <nl> int j = tm2_stream_order [ i ]; <nl> - memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> + if ( l -> tok_lens [ j ]) <nl> + memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> return t ; <nl> } <nl> offset += t ;
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> int reslevelno , bandno , gbandno = 0 , ret , i , j ; <nl> uint32_t csize = 1 ; <nl>  <nl> + if (! codsty -> nreslevels2decode ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " nreslevels2decode uninitialized \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( ret = ff_jpeg2000_dwt_init (& comp -> dwt , comp -> coord , <nl> codsty -> nreslevels2decode - 1 , <nl> codsty -> transform ))
static int filter_frame ( AVFilterLink * inlink , AVFrame * buf ) <nl> av_frame_free (& buf ); <nl>  <nl> end : <nl> - vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += buf -> nb_samples ; <nl> + vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += out_buf -> nb_samples ; <nl> return ff_filter_frame ( outlink , out_buf ); <nl> } <nl> 
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> value = UINT_MAX ; <nl> } <nl> } else { <nl> - if ( type_sizes [ type ] * count > 4 ) { <nl> - off = bytestream2_tell (& s -> gb ); <nl> - } <nl> + off = bytestream2_tell (& s -> gb ); <nl> } <nl>  <nl> switch ( tag ) {
static void qtrle_decode_8bpp ( QtrleContext * s ) <nl> int header ; <nl> int start_line ; <nl> int lines_to_change ; <nl> - signed char rle_code ; <nl> + int rle_code ; <nl> int row_ptr , pixel_ptr ; <nl> int row_inc = s -> frame . linesize [ 0 ]; <nl> unsigned char pi1 , pi2 , pi3 , pi4 ; /* 4 palette indices */
static inline uint64_t v4l2_get_pts ( V4L2Buffer * avbuf ) <nl> int64_t v4l2_pts ; <nl>  <nl> /* convert pts back to encoder timebase */ <nl> - v4l2_pts = avbuf -> buf . timestamp . tv_sec * USEC_PER_SEC + avbuf -> buf . timestamp . tv_usec ; <nl> + v4l2_pts = ( int64_t ) avbuf -> buf . timestamp . tv_sec * USEC_PER_SEC + <nl> + avbuf -> buf . timestamp . tv_usec ; <nl>  <nl> return av_rescale_q ( v4l2_pts , v4l2_timebase , s -> avctx -> time_base ); <nl> }
static int check_fps ( int fps ) <nl>  <nl> static int check_timecode ( void * log_ctx , AVTimecode * tc ) <nl> { <nl> - if ( tc -> fps <= 0 ) { <nl> + if (( int ) tc -> fps <= 0 ) { <nl> av_log ( log_ctx , AV_LOG_ERROR , " Timecode frame rate must be specified \ n "); <nl> return AVERROR ( EINVAL ); <nl> }
vbv_retry : <nl> if ( s -> mb_info ) <nl> av_packet_shrink_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , s -> mb_info_size ); <nl> } else { <nl> - assert (( put_bits_ptr (& s -> pb ) == s -> pb . buf )); <nl> s -> frame_bits = 0 ; <nl> } <nl> assert (( s -> frame_bits & 7 ) == 0 );
static int decode_init_thread_copy ( AVCodecContext * avctx ) <nl> memset ( h -> sps_buffers , 0 , sizeof ( h -> sps_buffers )); <nl> memset ( h -> pps_buffers , 0 , sizeof ( h -> pps_buffers )); <nl>  <nl> + h -> rbsp_buffer [ 0 ] = NULL ; <nl> + h -> rbsp_buffer [ 1 ] = NULL ; <nl> + h -> rbsp_buffer_size [ 0 ] = 0 ; <nl> + h -> rbsp_buffer_size [ 1 ] = 0 ; <nl> h -> context_initialized = 0 ; <nl>  <nl> return 0 ;
static int svag_read_header ( AVFormatContext * s ) <nl> if ( st -> codec -> sample_rate <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl> st -> codec -> channels = avio_rl32 ( s -> pb ); <nl> - if ( st -> codec -> channels <= 0 ) <nl> + if ( st -> codec -> channels <= 0 || st -> codec -> channels > 8 ) <nl> return AVERROR_INVALIDDATA ; <nl> st -> duration = size / ( 16 * st -> codec -> channels ) * 28 ; <nl> align = avio_rl32 ( s -> pb );
int64_t av_gcd ( int64_t a , int64_t b ) { <nl> v -= u ; <nl> v >>= ff_ctzll ( v ); <nl> } <nl> - return u << k ; <nl> + return ( uint64_t ) u << k ; <nl> } <nl>  <nl> int64_t av_rescale_rnd ( int64_t a , int64_t b , int64_t c , enum AVRounding rnd )
static int ljpeg_decode_yuv_scan ( MJpegDecodeContext * s , int predictor , <nl> if ( s -> interlaced && s -> bottom_field ) <nl> ptr16 += linesize >> 1 ; <nl> pred &= mask ; <nl> - * ptr16 = pred + ( dc << point_transform ); <nl> + * ptr16 = pred + (( unsigned ) dc << point_transform ); <nl> } <nl> if (++ x == h ) { <nl> x = 0 ;
static int recode_subtitle ( AVCodecContext * avctx , <nl> goto end ; <nl> } <nl> outpkt -> size -= outl ; <nl> - outpkt -> data [ outpkt -> size - 1 ] = '\ 0 '; <nl> + memset ( outpkt -> data + outpkt -> size , 0 , outl ); <nl>  <nl> end : <nl> if ( cd != ( iconv_t )- 1 )
static av_cold int encode_init ( AVCodecContext * avctx ) <nl> } <nl> } <nl> gob_count = strtol ( p , & next , 0 ); <nl> - if ( next == p || gob_count < 0 ){ <nl> + if ( next == p || gob_count <= 0 ){ <nl> av_log ( avctx , AV_LOG_ERROR , " 2Pass file invalid \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int pmp_header ( AVFormatContext * s ) <nl> uint32_t index_cnt ; <nl> int audio_codec_id = AV_CODEC_ID_NONE ; <nl> int srate , channels ; <nl> - int i ; <nl> + unsigned i ; <nl> uint64_t pos ; <nl> int64_t fsize = avio_size ( pb ); <nl> 
static void apply_tns ( float coef [ 1024 ], TemporalNoiseShaping * tns , <nl> int w , filt , m , i ; <nl> int bottom , top , order , start , end , size , inc ; <nl> float lpc [ TNS_MAX_ORDER ]; <nl> - float tmp [ TNS_MAX_ORDER ]; <nl> + float tmp [ TNS_MAX_ORDER + 1 ]; <nl>  <nl> for ( w = 0 ; w < ics -> num_windows ; w ++) { <nl> bottom = ics -> num_swb ;
ogm_dshow_header ( AVFormatContext * s , int idx ) <nl> if (* p != 1 ) <nl> return 1 ; <nl>  <nl> + if ( os -> psize < 100 ) <nl> + return AVERROR_INVALIDDATA ; <nl> t = AV_RL32 ( p + 96 ); <nl>  <nl> if ( t == 0x05589f80 ){
static int read_seek ( AVFormatContext * s , int stream_index , <nl> next_node [ 1 ]-> pos , next_node [ 1 ]-> pos , <nl> next_node [ 0 ]-> ts , next_node [ 1 ]-> ts , <nl> AVSEEK_FLAG_BACKWARD , & ts , nut_read_timestamp ); <nl> + if ( pos < 0 ) <nl> + return pos ; <nl>  <nl> if (!( flags & AVSEEK_FLAG_BACKWARD )) { <nl> dummy . pos = pos + 16 ;
static int decode_fctl_chunk ( AVFormatContext * s , APNGDemuxContext * ctx , AVPacket <nl> static int apng_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> APNGDemuxContext * ctx = s -> priv_data ; <nl> - int ret ; <nl> + int64_t ret ; <nl> int64_t size ; <nl> AVIOContext * pb = s -> pb ; <nl> uint32_t len , tag ;
static int update_context_from_thread ( AVCodecContext * dst , AVCodecContext * src , <nl> { <nl> int err = 0 ; <nl>  <nl> - if ( dst != src ) { <nl> + if ( dst != src && ( for_user || !( av_codec_get_codec_descriptor ( src )-> props & AV_CODEC_PROP_INTRA_ONLY ))) { <nl> dst -> time_base = src -> time_base ; <nl> dst -> framerate = src -> framerate ; <nl> dst -> width = src -> width ;
end : <nl> free_and_end : <nl> av_dict_free (& tmp ); <nl> av_freep (& avctx -> priv_data ); <nl> - if ( avctx -> internal ) <nl> + if ( avctx -> internal ) { <nl> av_freep (& avctx -> internal -> pool ); <nl> + av_frame_free (& avctx -> internal -> to_free ); <nl> + } <nl> av_freep (& avctx -> internal ); <nl> avctx -> codec = NULL ; <nl> goto end ;
static av_always_inline int wp_exp2 ( int16_t val ) <nl> return neg ? - res : res ; <nl> } <nl>  <nl> - static av_always_inline int wp_log2 ( int32_t val ) <nl> + static av_always_inline int wp_log2 ( uint32_t val ) <nl> { <nl> int bits ; <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> return ret ; <nl> fail : <nl> av_dict_free (& metadata ); <nl> + ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> ret = AVERROR_INVALIDDATA ; <nl> - ff_thread_release_buffer ( avctx , & s -> picture ); <nl> goto the_end ; <nl> } <nl> 
av_cold static int auto_matrix ( SwrContext * s ) <nl> } else <nl> maxval = INT_MAX ; <nl>  <nl> - if ( maxcoef > maxval ){ <nl> + if ( maxcoef > maxval || s -> rematrix_volume < 0 ){ <nl> maxcoef /= maxval ; <nl> for ( i = 0 ; i < SWR_CH_MAX ; i ++) <nl> for ( j = 0 ; j < SWR_CH_MAX ; j ++){
int av_packet_ref ( AVPacket * dst , const AVPacket * src ) <nl> ret = packet_alloc (& dst -> buf , src -> size ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl> - memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl> + if ( src -> size ) <nl> + memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl>  <nl> dst -> data = dst -> buf -> data ; <nl> } else {
int main ( int argc , char * argv []) <nl> } <nl> } <nl>  <nl> - max = ( 1 << ( 8 * len )) - 1 ; <nl> + max = ( 1LL << ( 8 * len )) - 1 ; <nl>  <nl> f [ 0 ] = fopen ( argv [ 1 ], " rb "); <nl> f [ 1 ] = fopen ( argv [ 2 ], " rb ");
av_cold int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , <nl> } <nl> } <nl>  <nl> -# define USE_MMAP ( HAVE_MMAP && HAVE_MPROTECT && defined MAP_ANONYMOUS ) <nl> +# if HAVE_MMAP && HAVE_MPROTECT && defined ( MAP_ANONYMOUS ) <nl> +# define USE_MMAP 1 <nl> +# else <nl> +# define USE_MMAP 0 <nl> +# endif <nl>  <nl> /* precalculate horizontal scaler filter coefficients */ <nl> {
int ff_wma_run_level_decode ( AVCodecContext * avctx , GetBitContext * gb , <nl> } <nl> /** NOTE : EOB can be omitted */ <nl> if ( offset > num_coefs ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " overflow in spectral RLE , ignoring \ n "); <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " overflow (% d > % d ) in spectral RLE , ignoring \ n ", <nl> + offset , <nl> + num_coefs <nl> + ); <nl> return - 1 ; <nl> } <nl> 
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
static void read_ttag ( AVFormatContext * s , AVIOContext * pb , int taglen , const cha <nl> } <nl> else if (* dst ) <nl> dict_flags |= AV_DICT_DONT_STRDUP_VAL ; <nl> + else <nl> + av_freep (& dst ); <nl>  <nl> if ( dst ) <nl> av_dict_set (& s -> metadata , key , dst , dict_flags );
leave : <nl> av_log ( s , AV_LOG_ERROR , " Packet mismatch % d % d \ n ", last , orig_size + 11 ); <nl> avio_seek ( s -> pb , pos + 1 , SEEK_SET ); <nl> ret = resync ( s ); <nl> + av_free_packet ( pkt ); <nl> if ( ret >= 0 ) { <nl> - av_free_packet ( pkt ); <nl> goto retry ; <nl> } <nl> }
static void dvbsub_parse_page_segment ( AVCodecContext * avctx , <nl>  <nl> av_dlog ( avctx , " Page time out % ds , state % d \ n ", ctx -> time_out , page_state ); <nl>  <nl> - if ( page_state == 2 ) { <nl> + if ( page_state == 1 || page_state == 2 ) { <nl> delete_regions ( ctx ); <nl> delete_objects ( ctx ); <nl> delete_cluts ( ctx );
# define NDEBUG <nl> # endif <nl>  <nl> -# if defined ( DEBUG ) && ! defined ( CHECKED ) <nl> -# define CHECKED <nl> -# endif <nl> +// This can be enabled to allow detection of additional integer overflows with ubsan <nl> +//# define CHECKED <nl>  <nl> # include < limits . h > <nl> # include < stdint . h >
for examples see get_bits , show_bits , skip_bits , get_vlc <nl>  <nl> # define OPEN_READER ( name , gb ) \ <nl> unsigned int name ## _index = ( gb )-> index ; \ <nl> - int name ## _cache = 0 <nl> + unsigned int name ## _cache = 0 <nl>  <nl> # define CLOSE_READER ( name , gb ) ( gb )-> index = name ## _index <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> s -> current_picture -> linesize [ 2 ], w >> s -> chroma_h_shift , h >> s -> chroma_v_shift , <nl> EDGE_WIDTH >> s -> chroma_h_shift , EDGE_WIDTH >> s -> chroma_v_shift , EDGE_TOP | EDGE_BOTTOM ); <nl> } <nl> + emms_c (); <nl> } <nl>  <nl> ff_snow_frame_start ( s ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> } <nl> + emms_c (); <nl>  <nl> update_last_header_values ( s ); <nl> 
static char * get_content_url ( xmlNodePtr * baseurl_nodes , <nl> return NULL ; <nl> } <nl> av_strlcpy ( tmp_str , url , sizeof ( tmp_str )); <nl> - av_free ( url ); <nl> } <nl> if ( rep_bandwidth_val && tmp_str [ 0 ] != '\ 0 ') { <nl> + // free any previously assigned url before reassigning <nl> + av_free ( url ); <nl> url = av_strireplace ( tmp_str , "$ Bandwidth $", ( const char *) rep_bandwidth_val ); <nl> if (! url ) { <nl> return NULL ;
int ff_ass_add_rect ( AVSubtitle * sub , const char * dialog , <nl> sub -> rects = rects ; <nl> sub -> end_display_time = FFMAX ( sub -> end_display_time , 10 * duration ); <nl> rects [ sub -> num_rects ] = av_mallocz ( sizeof (* rects [ 0 ])); <nl> + if (! rects [ sub -> num_rects ]) <nl> + goto errnomem ; <nl> rects [ sub -> num_rects ]-> type = SUBTITLE_ASS ; <nl> ret = av_bprint_finalize (& buf , & rects [ sub -> num_rects ]-> ass ); <nl> if ( ret < 0 )
static int process_video_header_vp6 ( AVFormatContext * s ) <nl> avio_skip ( pb , 4 ); <nl> ea -> time_base . den = avio_rl32 ( pb ); <nl> ea -> time_base . num = avio_rl32 ( pb ); <nl> + if ( ea -> time_base . den <= 0 || ea -> time_base . num <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Timebase is invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> ea -> video_codec = AV_CODEC_ID_VP6 ; <nl>  <nl> return 1 ;
static int transcode ( OutputFile * output_files , <nl> int64_t ipts_min ; <nl> double opts_min ; <nl>  <nl> - redo : <nl> ipts_min = INT64_MAX ; <nl> opts_min = 1e100 ; <nl> /* if ' q ' pressed , exits */ <nl> static int transcode ( OutputFile * output_files , <nl> if ( exit_on_error ) <nl> exit_program ( 1 ); <nl> av_free_packet (& pkt ); <nl> - goto redo ; <nl> + continue ; <nl> } <nl>  <nl> discard_packet :
static int select_reference_stream ( AVFormatContext * s ) <nl> ret = avformat_match_stream_specifier ( s , s -> streams [ i ], <nl> seg -> reference_stream_specifier ); <nl> if ( ret < 0 ) <nl> - break ; <nl> + return ret ; <nl> if ( ret > 0 ) { <nl> seg -> reference_stream_index = i ; <nl> break ;
static int decode_end ( AVCodecContext * avctx ) <nl> common_end ( s ); <nl> av_freep (& s -> bitstream_buffer ); <nl>  <nl> - for ( i = 0 ; i < 3 ; i ++){ <nl> + for ( i = 0 ; i < 6 ; i ++){ <nl> free_vlc (& s -> vlc [ i ]); <nl> } <nl> 
static int rtmp_packet_read_one_chunk ( URLContext * h , RTMPPacket * p , <nl> prev -> data = p -> data ; <nl> prev -> read = p -> read ; <nl> prev -> offset = p -> offset ; <nl> + p -> data = NULL ; <nl> return AVERROR ( EAGAIN ); <nl> } <nl> 
int img_convert ( AVPicture * dst , int dst_pix_fmt , <nl> else <nl> int_pix_fmt = PIX_FMT_RGB24 ; <nl> } <nl> + if ( src_pix_fmt == int_pix_fmt ) <nl> + return - 1 ; <nl> if ( avpicture_alloc ( tmp , int_pix_fmt , dst_width , dst_height ) < 0 ) <nl> return - 1 ; <nl> ret = - 1 ;
static void compute_stereo ( MPADecodeContext * s , GranuleDef * g0 , GranuleDef * g1 ) <nl> { <nl> int i , j , k , l ; <nl> int sf_max , sf , len , non_zero_found ; <nl> - INTFLOAT (* is_tab )[ 16 ], * tab0 , * tab1 , tmp0 , tmp1 , v1 , v2 ; <nl> + INTFLOAT (* is_tab )[ 16 ], * tab0 , * tab1 , v1 , v2 ; <nl> + SUINTFLOAT tmp0 , tmp1 ; <nl> int non_zero_found_short [ 3 ]; <nl>  <nl> /* intensity stereo */
static av_always_inline av_const float roundf ( float x ) <nl> } <nl> # endif /* HAVE_ROUNDF */ <nl>  <nl> +# if ! HAVE_TRUNC <nl> + static av_always_inline av_const double trunc ( double x ) <nl> +{ <nl> + return ( x > 0 ) ? floor ( x ) : ceil ( x ); <nl> +} <nl> +# endif /* HAVE_TRUNC */ <nl> + <nl> # if ! HAVE_TRUNCF <nl> static av_always_inline av_const float truncf ( float x ) <nl> {
void ff_estimate_b_frame_motion ( MpegEncContext * s , <nl> score = fbmin ; <nl> type = MB_TYPE_BIDIR ; <nl> } <nl> - score = ( score * score + 128 * 256 )>> 16 ; <nl> + score = (( unsigned )( score * score + 128 * 256 ))>> 16 ; <nl> s -> mc_mb_var_sum += score ; <nl> s -> mc_mb_var [ mb_y * s -> mb_width + mb_x ] = score ; // FIXME use SSD <nl> }
static int configure_output_video_filter ( FilterGraph * fg , OutputFilter * ofilter , <nl> snprintf ( name , sizeof ( name ), " output stream % d :% d ", ost -> file_index , ost -> index ); <nl> ret = avfilter_graph_create_filter (& ofilter -> filter , <nl> avfilter_get_by_name (" buffersink "), <nl> - name , NULL , pix_fmts , fg -> graph ); <nl> + name , NULL , NULL , fg -> graph ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
static FilterGraph * init_simple_filtergraph ( InputStream * ist , OutputStream * ost ) <nl>  <nl> static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> { <nl> - InputStream * ist ; <nl> + InputStream * ist = NULL ; <nl> enum AVMediaType type = in -> filter_ctx -> input_pads [ in -> pad_idx ]. type ; <nl> int i ; <nl> 
static int decode_frame_headers ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl>  <nl> if ( ctx -> data_size == 16 ) <nl> return 4 ; <nl> - if ( ctx -> data_size > buf_size ) <nl> - ctx -> data_size = buf_size ; <nl> + ctx -> data_size = FFMIN ( ctx -> data_size , buf_size - 16 ); <nl>  <nl> bytestream2_skip (& gb , 3 ); // skip reserved byte and checksum <nl> 
static int tta_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> return - 1 ; <nl> } <nl> st -> codec -> extradata = av_mallocz ( st -> codec -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! st -> codec -> extradata ) { <nl> + st -> codec -> extradata_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> avio_seek ( s -> pb , start_offset , SEEK_SET ); <nl> avio_read ( s -> pb , st -> codec -> extradata , st -> codec -> extradata_size ); <nl> 
int av_grow_packet ( AVPacket * pkt , int grow_by ) <nl> pkt -> buf = av_buffer_alloc ( new_size ); <nl> if (! pkt -> buf ) <nl> return AVERROR ( ENOMEM ); <nl> - memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> + if ( pkt -> size > 0 ) <nl> + memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> pkt -> data = pkt -> buf -> data ; <nl> } <nl> pkt -> size += grow_by ;
static int cbs_jpeg_split_fragment ( CodedBitstreamContext * ctx , <nl> if ( marker == JPEG_MARKER_SOS ) { <nl> length = AV_RB16 ( frag -> data + start ); <nl>  <nl> + if ( length > end - start ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> data_ref = NULL ; <nl> data = av_malloc ( end - start + <nl> AV_INPUT_BUFFER_PADDING_SIZE );
static int remove_decoded_packets ( AVFormatContext * ctx , int64_t scr ){ <nl> if ( stream -> buffer_index < pkt_desc -> size || <nl> stream -> predecode_packet == stream -> premux_packet ){ <nl> av_log ( ctx , AV_LOG_ERROR , <nl> - " buffer underflow i =% d bufi =% d size =% d \ n ", <nl> + " buffer underflow st =% d bufi =% d size =% d \ n ", <nl> i , stream -> buffer_index , pkt_desc -> size ); <nl> break ; <nl> }
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterBufferRef * out ; <nl> int hsub0 = desc -> log2_chroma_w ; <nl> int vsub0 = desc -> log2_chroma_h ; <nl> - int direct ; <nl> + int direct = 0 ; <nl> int plane ; <nl>  <nl> if (( in -> perms & AV_PERM_WRITE ) && !( in -> perms & AV_PERM_PRESERVE )) {
static int mxf_read_primer_pack ( void * arg , AVIOContext * pb , int tag , int size , U <nl> avpriv_request_sample ( pb , " Primer pack item length % d ", item_len ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> - if ( item_num > 65536 ) { <nl> + if ( item_num > 65536 || item_num < 0 ) { <nl> av_log ( mxf -> fc , AV_LOG_ERROR , " item_num % d is too large \ n ", item_num ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl> while ( x < w ) { <nl> int err , pred ; <nl>  <nl> + if ( get_bits_left (& s -> gb ) <= 0 ) <nl> + return ; <nl> + <nl> /* compute gradients */ <nl> Ra = x ? R ( dst , x - stride ) : R ( last , x ); <nl> Rb = R ( last , x );
static void amr_decode_fix_avctx ( AVCodecContext * avctx ) <nl> { <nl> const int is_amr_wb = 1 + ( avctx -> codec_id == AV_CODEC_ID_AMR_WB ); <nl>  <nl> - avctx -> sample_rate = 8000 * is_amr_wb ; <nl> + if (! avctx -> sample_rate ) <nl> + avctx -> sample_rate = 8000 * is_amr_wb ; <nl>  <nl> if ( avctx -> channels > 1 ) { <nl> av_log_missing_feature ( avctx , " multi - channel AMR ", 0 );
static int decode_frame ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " The answer to life , universe and everything is not correct !\ n "); <nl> return - 1 ; <nl> } <nl> + // Reset these pointers so we can tell if they were set this frame <nl> + s -> stripsizes = s -> stripdata = NULL ; <nl> /* parse image file directory */ <nl> off = tget_long (& buf , le ); <nl> if ( off >= UINT_MAX - 14 || end_buf - orig_buf < off + 14 ) {
static int svq3_decode_frame ( AVCodecContext * avctx , void * data , <nl> h -> mb_x = h -> mb_y = h -> mb_xy = 0 ; <nl>  <nl> if ( s -> watermark_key ) { <nl> - av_fast_malloc (& s -> buf , & s -> buf_size , <nl> - buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + av_fast_padded_malloc (& s -> buf , & s -> buf_size , buf_size ); <nl> if (! s -> buf ) <nl> return AVERROR ( ENOMEM ); <nl> memcpy ( s -> buf , avpkt -> data , buf_size );
int avresample_convert ( AVAudioResampleContext * avr , void ** output , <nl> } <nl> } <nl>  <nl> - return handle_buffered_output ( avr , & output_buffer , current_buffer ); <nl> + return handle_buffered_output ( avr , output ? & output_buffer : NULL , <nl> + current_buffer ); <nl> } <nl>  <nl> int avresample_available ( AVAudioResampleContext * avr )
static void mov_metadata_creation_time ( AVDictionary ** metadata , int64_t time ) <nl> if ( time ) { <nl> if ( time >= 2082844800 ) <nl> time -= 2082844800 ; /* seconds between 1904 - 01 - 01 and Epoch */ <nl> + <nl> + if (( int64_t )( time * 1000000ULL ) / 1000000 != time ) { <nl> + av_log ( NULL , AV_LOG_DEBUG , " creation_time is not representable \ n "); <nl> + return ; <nl> + } <nl> + <nl> avpriv_dict_set_timestamp ( metadata , " creation_time ", time * 1000000 ); <nl> } <nl> }
int ff_rm_read_mdpr_codecdata ( AVFormatContext * s , AVIOContext * pb , <nl> skip : <nl> /* skip codec info */ <nl> size = avio_tell ( pb ) - codec_pos ; <nl> - avio_skip ( pb , codec_data_size - size ); <nl> + if ( codec_data_size >= size ) { <nl> + avio_skip ( pb , codec_data_size - size ); <nl> + } else { <nl> + av_log ( s , AV_LOG_WARNING , " codec_data_size % u < size % d \ n ", codec_data_size , size ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void update_stream_timings ( AVFormatContext * ic ) <nl> if ( end_time != INT64_MIN ) <nl> duration = FFMAX ( duration , end_time - start_time ); <nl> } <nl> - if ( duration != INT64_MIN && ic -> duration == AV_NOPTS_VALUE ) { <nl> + if ( duration != INT64_MIN && duration > 0 && ic -> duration == AV_NOPTS_VALUE ) { <nl> ic -> duration = duration ; <nl> } <nl> if ( ic -> pb && ( filesize = avio_size ( ic -> pb )) > 0 && ic -> duration != AV_NOPTS_VALUE ) {
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> return res ; <nl> av_assert1 ( block_duration != AV_NOPTS_VALUE ); <nl>  <nl> - block_time = AV_RB16 ( data ); <nl> + block_time = sign_extend ( AV_RB16 ( data ), 16 ); <nl> data += 2 ; <nl> flags = * data ++; <nl> size -= 3 ;
static av_cold int adpcm_decode_init ( AVCodecContext * avctx ) <nl> max_channels = 6 ; <nl> break ; <nl> } <nl> - if ( avctx -> channels > max_channels ){ <nl> - return - 1 ; <nl> + if ( avctx -> channels <= 0 || avctx -> channels > max_channels ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR ( EINVAL ); <nl> } <nl>  <nl> switch ( avctx -> codec -> id ) {
int ff_img_read_header ( AVFormatContext * s1 ) <nl> break ; <nl> } <nl> } <nl> - ffio_rewind_with_probe_data ( s1 -> pb , & probe_buffer , probe_buffer_size ); <nl> + if ( s1 -> flags & AVFMT_FLAG_CUSTOM_IO ) { <nl> + avio_seek ( s1 -> pb , 0 , SEEK_SET ); <nl> + } else <nl> + ffio_rewind_with_probe_data ( s1 -> pb , & probe_buffer , probe_buffer_size ); <nl> } <nl> if ( st -> codec -> codec_id == AV_CODEC_ID_NONE ) <nl> st -> codec -> codec_id = ff_guess_image2_codec ( s -> path );
void ff_parse_specific_params ( AVStream * st , int * au_rate , <nl>  <nl> void ff_riff_write_info_tag ( AVIOContext * pb , const char * tag , const char * str ) <nl> { <nl> - int len = strlen ( str ); <nl> - if ( len > 0 ) { <nl> + size_t len = strlen ( str ); <nl> + if ( len > 0 && len < UINT32_MAX ) { <nl> len ++; <nl> ffio_wfourcc ( pb , tag ); <nl> avio_wl32 ( pb , len );
int64_t ff_ape_parse_tag ( AVFormatContext * s ) <nl> return 0 ; <nl> } <nl>  <nl> - tag_start = file_size - tag_bytes - APE_TAG_FOOTER_BYTES ; <nl> - if ( tag_start < 0 ) { <nl> + if ( tag_bytes > file_size - APE_TAG_FOOTER_BYTES ) { <nl> av_log ( s , AV_LOG_ERROR , " Invalid tag size % u .\ n ", tag_bytes ); <nl> return 0 ; <nl> } <nl> + tag_start = file_size - tag_bytes - APE_TAG_FOOTER_BYTES ; <nl>  <nl> fields = avio_rl32 ( pb ); /* number of fields */ <nl> if ( fields > 65536 ) {
static int dca_decode_frame ( AVCodecContext * avctx , <nl> } else <nl> s -> channel_order_tab = dca_channel_reorder_nolfe [ s -> amode ]; <nl>  <nl> + if ( s -> prim_channels > 0 && <nl> + s -> channel_order_tab [ s -> prim_channels - 1 ] < 0 ) <nl> + return - 1 ; <nl> + <nl> if ( avctx -> request_channels == 2 && s -> prim_channels > 2 ) { <nl> channels = 2 ; <nl> s -> output = DCA_STEREO ;
static int theora_header ( AVFormatContext * s , int idx ) <nl> st -> codec -> extradata_size = 0 ; <nl> return err ; <nl> } <nl> + memset ( st -> codec -> extradata + cds , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + <nl> cdp = st -> codec -> extradata + st -> codec -> extradata_size ; <nl> * cdp ++ = os -> psize >> 8 ; <nl> * cdp ++ = os -> psize & 0xff ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> * of get_sr_golomb_shorten (). */ <nl> if ( s -> version == 0 ) <nl> residual_size --; <nl> + if ( residual_size > 30U ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " residual size unsupportd : % d \ n ", residual_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> /* calculate sample offset using means from previous blocks */
static int mpeg_mux_init ( AVFormatContext * ctx ) <nl> if (! s -> mux_rate ) { <nl> /* we increase slightly the bitrate to take into account the <nl> headers . XXX : compute it exactly */ <nl> - bitrate += bitrate * 5 / 100 ; <nl> + bitrate += bitrate * 5LL / 100 ; <nl> bitrate += 10000 ; <nl> s -> mux_rate = ( bitrate + ( 8 * 50 ) - 1 ) / ( 8 * 50 ); <nl> }
int main ( void ){ <nl> put_cabac (& c , state , r [ i ]& 1 ); <nl> } <nl>  <nl> - put_cabac_terminate (& c , 1 ); <nl> + i = put_cabac_terminate (& c , 1 ); <nl> + b [ i ++] = av_lfg_get (& prng ); <nl> + b [ i ] = av_lfg_get (& prng ); <nl>  <nl> ff_init_cabac_decoder (& c , b , SIZE ); <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame_ptr , <nl> uint32_t header ; <nl> int out_size ; <nl>  <nl> - while ( buf_size && !* buf ) <nl> + while ( buf_size && !* buf ){ <nl> buf ++; <nl> + buf_size --; <nl> + } <nl>  <nl> if ( buf_size < HEADER_SIZE ) <nl> return AVERROR_INVALIDDATA ;
static int mxf_write_header ( AVFormatContext * s ) <nl> mxf -> edit_unit_byte_count += klv_fill_size ( mxf -> edit_unit_byte_count ); <nl>  <nl> sc -> signal_standard = 1 ; <nl> + sc -> color_siting = 0 ; <nl> } <nl> if ( mxf -> signal_standard >= 0 ) <nl> sc -> signal_standard = mxf -> signal_standard ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> cases . */ <nl> static void imdct12 ( INTFLOAT * out , INTFLOAT * in ) <nl> { <nl> - INTFLOAT in0 , in1 , in2 , in3 , in4 , in5 , t1 , t2 ; <nl> + SUINTFLOAT in0 , in1 , in2 , in3 , in4 , in5 , t1 , t2 ; <nl>  <nl> in0 = in [ 0 * 3 ]; <nl> in1 = in [ 1 * 3 ] + in [ 0 * 3 ];
static int get_stats ( AVCodecContext * avctx , int eos ) <nl> // libtheora generates a summary header at the end <nl> memcpy ( h -> stats , buf , bytes ); <nl> avctx -> stats_out = av_malloc ( b64_size ); <nl> + if (! avctx -> stats_out ) <nl> + return AVERROR ( ENOMEM ); <nl> av_base64_encode ( avctx -> stats_out , b64_size , h -> stats , h -> stats_offset ); <nl> } <nl> return 0 ;
static int a64multi_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> } else { <nl> /* fill up mc_meta_charset with data until lifetime exceeds */ <nl> if ( c -> mc_frame_counter < c -> mc_lifetime ) { <nl> - * p = * pict ; <nl> + ret = av_frame_ref ( p , pict ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> p -> key_frame = 1 ; <nl> to_meta_with_crop ( avctx , p , meta + 32000 * c -> mc_frame_counter );
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> if ( atom . size < 0 ) <nl> atom . size = INT64_MAX ; <nl> - while ( total_size + 8 <= atom . size && ! avio_feof ( pb )) { <nl> + while ( total_size <= atom . size - 8 && ! avio_feof ( pb )) { <nl> int (* parse )( MOVContext *, AVIOContext *, MOVAtom ) = NULL ; <nl> a . size = atom . size ; <nl> a . type = 0 ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> void * tmp_ptr ; <nl> s -> max_framesize = 1024 ; // should hopefully be enough for the first header <nl> tmp_ptr = av_fast_realloc ( s -> bitstream , & s -> allocated_bitstream_size , <nl> - s -> max_framesize ); <nl> + s -> max_framesize + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! tmp_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM );
static int swScale ( SwsContext * c , const uint8_t * src [], <nl> || yuv2planeX == yuv2planeX_9LE_c <nl> || yuv2planeX == yuv2planeX_16BE_c <nl> || yuv2planeX == yuv2planeX_16LE_c <nl> - || yuv2planeX == yuv2planeX_8_c )); <nl> + || yuv2planeX == yuv2planeX_8_c ) || ! ARCH_X86 ); <nl> + <nl> if ( use_mmx_vfilter ){ <nl> vLumFilter = c -> lumMmxFilter ; <nl> vChrFilter = c -> chrMmxFilter ;
static av_cold void init_cplscales_table ( COOKContext * q ) <nl> static inline int decode_bytes ( const uint8_t * inbuffer , uint8_t * out , int bytes ) <nl> { <nl> static const uint32_t tab [ 4 ] = { <nl> - AV_BE2NE32C ( 0x37c511f2 ), AV_BE2NE32C ( 0xf237c511 ), <nl> - AV_BE2NE32C ( 0x11f237c5 ), AV_BE2NE32C ( 0xc511f237 ), <nl> + AV_BE2NE32C ( 0x37c511f2u ), AV_BE2NE32C ( 0xf237c511u ), <nl> + AV_BE2NE32C ( 0x11f237c5u ), AV_BE2NE32C ( 0xc511f237u ), <nl> }; <nl> int i , off ; <nl> uint32_t c ;
static void show_packets ( WriterContext * w , AVFormatContext * fmt_ctx ) <nl> if ( do_show_frames && <nl> get_video_frame ( fmt_ctx , & frame , & pkt )) { <nl> show_frame ( w , & frame , fmt_ctx -> streams [ pkt . stream_index ]); <nl> - av_free_packet (& pkt ); <nl> } <nl> + av_free_packet (& pkt ); <nl> } <nl> av_init_packet (& pkt ); <nl> pkt . data = NULL ;
static int flv_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> unsigned ts ; <nl> int size = pkt -> size ; <nl> uint8_t * data = NULL ; <nl> - int flags , flags_size ; <nl> + int flags = 0 , flags_size ; <nl>  <nl> // av_log ( s , AV_LOG_DEBUG , " type :% d pts : %" PRId64 " size :% d \ n ", <nl> // enc -> codec_type , timestamp , size );
static int writer_open ( WriterContext ** wctx , const Writer * writer , const char * a <nl> { <nl> int i , ret = 0 ; <nl>  <nl> - if (!(* wctx = av_malloc ( sizeof ( WriterContext )))) { <nl> + if (!(* wctx = av_mallocz ( sizeof ( WriterContext )))) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ; <nl> }
static int read_quant_tables ( RangeCoder * c , <nl> int context_count = 1 ; <nl>  <nl> for ( i = 0 ; i < 5 ; i ++) { <nl> - context_count *= read_quant_table ( c , quant_table [ i ], context_count ); <nl> + int ret = read_quant_table ( c , quant_table [ i ], context_count ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + context_count *= ret ; <nl> if ( context_count > 32768U ) { <nl> return AVERROR_INVALIDDATA ; <nl> }
static int execute_decode_slices ( H264Context * h , int context_count ) <nl> H264Context * hx ; <nl> int i ; <nl>  <nl> + if ( h -> mb_y >= h -> mb_height ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , <nl> + " Input contains more MB rows than the frame height .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( h -> avctx -> hwaccel ) <nl> return 0 ; <nl> if ( context_count == 1 ) {
static int sonic_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( buf_size == 0 ) return 0 ; <nl>  <nl> - s -> frame . nb_samples = s -> frame_size ; <nl> + s -> frame . nb_samples = s -> frame_size / avctx -> channels ; <nl> if (( ret = ff_get_buffer ( avctx , & s -> frame , 0 )) < 0 ) <nl> return ret ; <nl> samples = ( int16_t *) s -> frame . data [ 0 ];
static inline int asym_quant ( int c , int e , int qbits ) <nl> { <nl> int m ; <nl>  <nl> - c = ((( c << e ) >> ( 24 - qbits )) + 1 ) >> 1 ; <nl> + c = ((( c * ( 1 << e )) >> ( 24 - qbits )) + 1 ) >> 1 ; <nl> m = ( 1 << ( qbits - 1 )); <nl> if ( c >= m ) <nl> c = m - 1 ;
av_cold int ff_snow_common_init ( AVCodecContext * avctx ){ <nl> ff_scale_mv_ref [ i ][ j ] = 256 *( i + 1 )/( j + 1 ); <nl>  <nl> s -> avctx -> get_buffer ( s -> avctx , & s -> mconly_picture ); <nl> - s -> scratchbuf = av_malloc ( s -> mconly_picture . linesize [ 0 ]* 7 * MB_SIZE ); <nl> + s -> scratchbuf = av_mallocz ( s -> mconly_picture . linesize [ 0 ]* 7 * MB_SIZE ); <nl>  <nl> return 0 ; <nl> }
static av_cold int tiff_end ( AVCodecContext * avctx ) <nl>  <nl> ff_lzw_decode_close (& s -> lzw ); <nl> av_freep (& s -> deinvert_buf ); <nl> + s -> deinvert_buf_size = 0 ; <nl> av_freep (& s -> fax_buffer ); <nl> s -> fax_buffer_size = 0 ; <nl> return 0 ;
int main ( int argc , char ** argv ) <nl> k = av_tree_find ( root , ( void *)( j + 1 ), cmp , NULL ); <nl> if ( k ) <nl> av_log ( NULL , AV_LOG_ERROR , " removal failure % d \ n ", i ); <nl> + av_free ( node2 ); <nl> } <nl> } <nl> + av_free ( node ); <nl>  <nl> av_tree_destroy ( root ); <nl> 
typedef struct ListEntry { <nl>  <nl> typedef struct HLSContext { <nl> const AVClass * class ; // Class for private options . <nl> - int number ; <nl> + unsigned number ; <nl> int64_t sequence ; <nl> AVOutputFormat * oformat ; <nl> AVFormatContext * avf ;
int ff_hevc_parse_sps ( HEVCSPS * sps , GetBitContext * gb , unsigned int * sps_id , <nl> return 0 ; <nl>  <nl> err : <nl> - return ret ; <nl> + return ret < 0 ? ret : AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> int ff_hevc_decode_nal_sps ( GetBitContext * gb , AVCodecContext * avctx ,
static void decode_channel_map ( uint8_t layout_map [][ 3 ], <nl> case AAC_CHANNEL_LFE : <nl> syn_ele = TYPE_LFE ; <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl> layout_map [ 0 ][ 0 ] = syn_ele ; <nl> layout_map [ 0 ][ 1 ] = get_bits ( gb , 4 );
static int dss_read_close ( AVFormatContext * s ) <nl> { <nl> DSSDemuxContext * ctx = s -> priv_data ; <nl>  <nl> - av_free ( ctx -> dss_sp_buf ); <nl> + av_freep (& ctx -> dss_sp_buf ); <nl>  <nl> return 0 ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> buf += 5 ; <nl>  <nl> if ( video_size ) { <nl> + if ( video_size < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " video size % d invalid \ n ", video_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( avctx -> reget_buffer ( avctx , & s -> frame ) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return - 1 ;
static int prompeg_write ( URLContext * h , const uint8_t * buf , int size ) { <nl>  <nl> // FEC ( column ) send block - aligned <nl> if (! s -> first && s -> packet_idx % s -> d == 0 ) { <nl> - col_out_idx = s -> packet_idx / s -> l ; <nl> + col_out_idx = s -> packet_idx / s -> d ; <nl> if (( ret = prompeg_write_fec ( h , s -> fec_col [ col_out_idx ], PROMPEG_FEC_COL )) < 0 ) <nl> goto end ; <nl> written += ret ;
static int prepare_packet ( AVPacket * pkt , const FailingMuxerPacketData * pkt_data , <nl> { <nl> int ret ; <nl> FailingMuxerPacketData * data = av_malloc ( sizeof (* data )); <nl> + if (! data ) { <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> memcpy ( data , pkt_data , sizeof ( FailingMuxerPacketData )); <nl> ret = av_packet_from_data ( pkt , ( uint8_t *) data , sizeof (* data )); <nl> 
static int dnxhd_probe ( AVProbeData * p ) <nl> if (! w || ! h ) <nl> return 0 ; <nl> compression_id = AV_RB32 ( p -> buf + 0x28 ); <nl> - if ( compression_id < 1235 || compression_id > 1258 ) <nl> + if ( compression_id < 1235 || compression_id > 1260 ) <nl> return 0 ; <nl> return AVPROBE_SCORE_MAX ; <nl> }
static void sub_hfyu_median_prediction_int16_c ( uint16_t * dst , const uint16_t * sr <nl> * left_top = lt ; <nl> } <nl>  <nl> - static int add_hfyu_left_prediction_int16_c ( uint16_t * dst , const uint16_t * src , unsigned mask , int w , int acc ){ <nl> + static int add_hfyu_left_prediction_int16_c ( uint16_t * dst , const uint16_t * src , unsigned mask , int w , unsigned acc ){ <nl> int i ; <nl>  <nl> for ( i = 0 ; i < w - 1 ; i ++){
static int read_gab2_sub ( AVFormatContext * s , AVStream * st , AVPacket * pkt ) <nl> goto error ; <nl>  <nl> if (! avformat_open_input (& ast -> sub_ctx , "", sub_demuxer , NULL )) { <nl> + if ( ast -> sub_ctx -> nb_streams != 1 ) <nl> + goto error ; <nl> ff_read_packet ( ast -> sub_ctx , & ast -> sub_pkt ); <nl> avcodec_parameters_copy ( st -> codecpar , ast -> sub_ctx -> streams [ 0 ]-> codecpar ); <nl> time_base = ast -> sub_ctx -> streams [ 0 ]-> time_base ;
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 4 ] = jvf -> video_type ; <nl> if (( size = avio_read ( pb , pkt -> data + JV_PREAMBLE_SIZE , size )) < 0 ) <nl> return AVERROR ( EIO ); <nl> + memset ( pkt -> data + JV_PREAMBLE_SIZE + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> pkt -> size = size + JV_PREAMBLE_SIZE ; <nl> pkt -> stream_index = 1 ;
static av_cold int tdsc_init ( AVCodecContext * avctx ) <nl> ctx -> jpeg_avctx -> flags2 = avctx -> flags2 ; <nl> ctx -> jpeg_avctx -> dct_algo = avctx -> dct_algo ; <nl> ctx -> jpeg_avctx -> idct_algo = avctx -> idct_algo ;; <nl> - ret = avcodec_open2 ( ctx -> jpeg_avctx , codec , NULL ); <nl> + ret = ff_codec_open2_recursive ( ctx -> jpeg_avctx , codec , NULL ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
static int decode_slice ( AVCodecContext * avctx , void * tdata ) <nl>  <nl> /* if V or alpha component size is negative that means that previous <nl> component sizes are too large */ <nl> - if ( v_data_size < 0 || a_data_size < 0 || hdr_size < 6 ) { <nl> + if ( v_data_size < 0 || a_data_size < 0 || hdr_size < 6 || coff [ 3 ] > slice_data_size ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid data size \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame_header ( AVCodecContext * ctx , <nl> s -> lf_delta . ref [ 3 ] = - 1 ; <nl> s -> lf_delta . mode [ 0 ] = 0 ; <nl> s -> lf_delta . mode [ 1 ] = 0 ; <nl> + memset ( s -> segmentation . feat , 0 , sizeof ( s -> segmentation . feat )); <nl> } <nl> s -> filter . level = get_bits (& s -> gb , 6 ); <nl> sharp = get_bits (& s -> gb , 3 );
static void mxf_read_pixel_layout ( AVIOContext * pb , MXFDescriptor * descriptor ) <nl> if ( ofs <= 14 ) { <nl> layout [ ofs ++] = code ; <nl> layout [ ofs ++] = value ; <nl> - } <nl> + } else <nl> + break ; /* don ' t read byte by byte on sneaky files filled with lots of non - zeroes */ <nl> } while ( code != 0 ); /* SMPTE 377M E . 2 . 46 */ <nl>  <nl> ff_mxf_decode_pixel_layout ( layout , & descriptor -> pix_fmt );
static av_always_inline int vorbis_residue_decode_internal ( vorbis_context * vc , <nl> } <nl>  <nl> } else if ( vr_type == 2 ) { <nl> - unsigned voffs_div = FASTDIV ( voffset , ch ); <nl> + unsigned voffs_div = ch == 1 ? voffset : FASTDIV ( voffset , ch ); <nl> unsigned voffs_mod = voffset - voffs_div * ch ; <nl>  <nl> for ( k = 0 ; k < step ; ++ k ) {
static int check ( AVIOContext * pb , int64_t pos , uint32_t * ret_header ) <nl> return CHECK_SEEK_FAILED ; <nl>  <nl> ret = avio_read ( pb , & header_buf [ 0 ], 4 ); <nl> - if ( ret < 0 ) <nl> + /* We should always find four bytes for a valid mpa header . */ <nl> + if ( ret < 4 ) <nl> return CHECK_SEEK_FAILED ; <nl>  <nl> header = AV_RB32 (& header_buf [ 0 ]);
enum BandType { <nl> INTENSITY_BT = 15 , ///< Scalefactor data are intensity stereo positions . <nl> }; <nl>  <nl> -# define IS_CODEBOOK_UNSIGNED ( x ) (( x - 1 ) & 10 ) <nl> +# define IS_CODEBOOK_UNSIGNED ( x ) ((( x ) - 1 ) & 10 ) <nl>  <nl> enum ChannelPosition { <nl> AAC_CHANNEL_OFF = 0 ,
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl>  <nl> memcpy (& h -> poc , & h1 -> poc , sizeof ( h -> poc )); <nl>  <nl> - memcpy ( h -> default_ref , h1 -> default_ref , sizeof ( h -> default_ref )); <nl> memcpy ( h -> short_ref , h1 -> short_ref , sizeof ( h -> short_ref )); <nl> memcpy ( h -> long_ref , h1 -> long_ref , sizeof ( h -> long_ref )); <nl> memcpy ( h -> delayed_pic , h1 -> delayed_pic , sizeof ( h -> delayed_pic ));
static int vqf_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 1 ] = c -> last_frame_bits ; <nl> ret = avio_read ( s -> pb , pkt -> data + 2 , size ); <nl>  <nl> - if ( ret <= 0 ) { <nl> + if ( ret != size ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO ); <nl> }
static int parse_playlist ( AppleHTTPContext * c , const char * url , <nl> enum KeyType key_type = KEY_NONE ; <nl> uint8_t iv [ 16 ] = ""; <nl> int has_iv = 0 ; <nl> - char key [ MAX_URL_SIZE ]; <nl> + char key [ MAX_URL_SIZE ] = ""; <nl> char line [ 1024 ]; <nl> const char * ptr ; <nl> int close_in = 0 ;
static int read_highpass ( AVCodecContext * avctx , uint8_t * ptr , int plane , AVFrame <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( a == INT32_MIN ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> ret = read_high_coeffs ( avctx , ptr + bytestream2_tell (& ctx -> gb ), dest , size , <nl> c , ( b >= FFABS ( a )) ? b : a , d , <nl> ctx -> band [ plane ][ i + 1 ]. width , stride );
int ff_hevc_annexb2mp4 ( AVIOContext * pb , const uint8_t * buf_in , <nl> } <nl>  <nl> end : <nl> - free ( start ); <nl> + av_free ( start ); <nl> if ( ps_count ) <nl> * ps_count = num_ps ; <nl> return ret ; <nl> int ff_hevc_annexb2mp4_buf ( const uint8_t * buf_in , uint8_t ** buf_out , <nl> * size = avio_close_dyn_buf ( pb , buf_out ); <nl>  <nl> end : <nl> - free ( start ); <nl> + av_free ( start ); <nl> if ( ps_count ) <nl> * ps_count = num_ps ; <nl> return ret ;
int ff_mov_read_esds ( AVFormatContext * fc , ByteIOContext * pb , MOVAtom atom ) <nl> dprintf ( fc , " Specific MPEG4 header len =% d \ n ", len ); <nl> if (( uint64_t ) len > ( 1 << 30 )) <nl> return - 1 ; <nl> + av_free ( st -> codec -> extradata ); <nl> st -> codec -> extradata = av_mallocz ( len + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! st -> codec -> extradata ) <nl> return AVERROR ( ENOMEM );
static int ape_read_header ( AVFormatContext * s ) <nl> ape -> seektablelength / sizeof (* ape -> seektable ), ape -> totalframes ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> - ape -> frames = av_malloc ( ape -> totalframes * sizeof ( APEFrame )); <nl> + ape -> frames = av_malloc_array ( ape -> totalframes , sizeof ( APEFrame )); <nl> if (! ape -> frames ) <nl> return AVERROR ( ENOMEM ); <nl> ape -> firstframe = ape -> junklength + ape -> descriptorlength + ape -> headerlength + ape -> seektablelength + ape -> wavheaderlength ;
again : <nl> break ; <nl>  <nl> if ( h -> sei_recovery_frame_cnt >= 0 && ( h -> frame_num != h -> sei_recovery_frame_cnt || hx -> slice_type_nos != AV_PICTURE_TYPE_I )) <nl> - h -> valid_recovery_point ++; <nl> + h -> valid_recovery_point = 1 ; <nl>  <nl> if ( h -> sei_recovery_frame_cnt >= 0 <nl> && ( h -> recovery_frame < 0
unsigned avutil_version ( void ) <nl> av_assert0 ( LIBAVUTIL_VERSION_MICRO >= 100 ); <nl> av_assert0 ( HAVE_MMX2 == HAVE_MMXEXT ); <nl>  <nl> + if ( av_sat_dadd32 ( 1 , 2 ) != 5 ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Libavutil has been build with a broken binutils , please upgrade binutils and rebuild \ n "); <nl> + abort (); <nl> + } <nl> + <nl> return LIBAVUTIL_VERSION_INT ; <nl> } <nl> 
static float dca_dmix_code ( unsigned code ) <nl> static int scan_for_extensions ( AVCodecContext * avctx ) <nl> { <nl> DCAContext * s = avctx -> priv_data ; <nl> - int core_ss_end , ret ; <nl> + int core_ss_end , ret = 0 ; <nl>  <nl> core_ss_end = FFMIN ( s -> frame_size , s -> dca_buffer_size ) * 8 ; <nl> 
av_cold static int auto_matrix ( SwrContext * s ) <nl>  <nl> memset ( s -> matrix , 0 , sizeof ( s -> matrix )); <nl> for ( i = 0 ; i < 64 ; i ++){ <nl> - if ( in_ch_layout & out_ch_layout & ( 1LL << i )) <nl> + if ( in_ch_layout & out_ch_layout & ( 1ULL << i )) <nl> matrix [ i ][ i ]= 1 . 0 ; <nl> } <nl> 
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl> AVCodecContext * ctx = avcodec_alloc_context3 ( NULL ); <nl> if (! ctx ) <nl> error (" Failed memory allocation "); <nl> + <nl> + ctx -> max_pixels = 4096 * 4096 ; // To reduce false positive OOM and hangs <nl> + <nl> int res = avcodec_open2 ( ctx , c , NULL ); <nl> if ( res < 0 ) <nl> return res ;
reload : <nl>  <nl> static int hls_read_header ( AVFormatContext * s ) <nl> { <nl> - URLContext * u = s -> pb -> opaque ; <nl> + URLContext * u = ( s -> flags & AVFMT_FLAG_CUSTOM_IO ) ? NULL : s -> pb -> opaque ; <nl> HLSContext * c = s -> priv_data ; <nl> int ret = 0 , i , j , stream_offset = 0 ; <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl>  <nl> if ( h -> ref_count [ 0 ] > max_refs || h -> ref_count [ 1 ] > max_refs ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " reference overflow \ n "); <nl> - h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 1 ; <nl> + h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int matroska_parse_laces ( MatroskaDemuxContext * matroska , uint8_t ** buf , <nl> } <nl>  <nl> case 0x2 : /* fixed - size lacing */ <nl> - if ( size != ( size / * laces ) * size ) { <nl> + if ( size % (* laces )) { <nl> res = AVERROR_INVALIDDATA ; <nl> break ; <nl> }
static int latm_decode_audio_specific_config ( struct LATMContext * latmctx , <nl> " config not byte aligned .\ n ", 1 ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( asclen <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> bits_consumed = decode_audio_specific_config ( NULL , avctx , & m4ac , <nl> gb -> buffer + ( config_start_bit / 8 ), <nl> asclen , sync_extension );
static int output_frame ( AVFilterLink * outlink , int nb_samples ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> in_buf = ff_get_audio_buffer ( outlink , AV_PERM_WRITE , nb_samples ); <nl> - if (! in_buf ) <nl> + if (! in_buf ) { <nl> + avfilter_unref_buffer ( out_buf ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> for ( i = 0 ; i < s -> nb_inputs ; i ++) { <nl> if ( s -> input_state [ i ] == INPUT_ON ) {
static void opt_list ( void * obj , void * av_log_obj , const char * unit , <nl> av_log ( av_log_obj , AV_LOG_INFO , " ( default "); <nl> switch ( opt -> type ) { <nl> case AV_OPT_TYPE_FLAGS : <nl> - av_log ( av_log_obj , AV_LOG_INFO , "% 0llX ", opt -> default_val . i64 ); <nl> + av_log ( av_log_obj , AV_LOG_INFO , "%" PRIX64 , opt -> default_val . i64 ); <nl> break ; <nl> case AV_OPT_TYPE_DURATION : <nl> case AV_OPT_TYPE_INT :
int vp56_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> int is_alpha , av_uninit ( alpha_offset ); <nl>  <nl> if ( s -> has_alpha ) { <nl> + if ( remaining_buf_size < 3 ) <nl> + return - 1 ; <nl> alpha_offset = bytestream_get_be24 (& buf ); <nl> remaining_buf_size -= 3 ; <nl> + if ( remaining_buf_size < alpha_offset ) <nl> + return - 1 ; <nl> } <nl>  <nl> for ( is_alpha = 0 ; is_alpha < 1 + s -> has_alpha ; is_alpha ++) {
static int mov_read_sidx ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> MOVFragmentStreamInfo * si ; <nl> si = & item -> stream_info [ j ]; <nl> if ( si -> sidx_pts != AV_NOPTS_VALUE ) { <nl> - ref_st = c -> fc -> streams [ i ]; <nl> + ref_st = c -> fc -> streams [ j ]; <nl> ref_sc = ref_st -> priv_data ; <nl> break ; <nl> }
static int wma_decode_superframe ( AVCodecContext * avctx , void * data , <nl>  <nl> /* read each frame starting from bit_offset */ <nl> pos = bit_offset + 4 + 4 + s -> byte_offset_bits + 3 ; <nl> + if ( pos >= MAX_CODED_SUPERFRAME_SIZE * 8 ) <nl> + return AVERROR_INVALIDDATA ; <nl> init_get_bits (& s -> gb , buf + ( pos >> 3 ), ( MAX_CODED_SUPERFRAME_SIZE - ( pos >> 3 ))* 8 ); <nl> len = pos & 7 ; <nl> if ( len > 0 )
static av_cold int wavpack_encode_init ( AVCodecContext * avctx ) <nl> s -> avctx = avctx ; <nl>  <nl> if ( avctx -> channels > 255 ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " Too many channels \ n ", avctx -> channels ); <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid channel count : % d \ n ", avctx -> channels ); <nl> return AVERROR ( EINVAL ); <nl> } <nl> 
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl> escape = av_mod_uintp2 ( 16383 , pfx ); <nl> cnt1 = get_unary ( b , 0 , 8 ); <nl> if ( cnt1 < 8 ) { <nl> + if ( pfx < 1 || pfx > 25 ) <nl> + return AVERROR_INVALIDDATA ; <nl> value = show_bits ( b , pfx ); <nl> if ( value > 1 ) { <nl> skip_bits ( b , pfx );
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> h -> ref_count [ 1 ] = get_ue_golomb (& s -> gb ) + 1 ; <nl> else <nl> // full range is spec - ok in this case , even for frames <nl> - max [ 1 ] = 31 ; <nl> + h -> ref_count [ 1 ] = 1 ; <nl> } <nl>  <nl> if ( h -> ref_count [ 0 ]- 1 > max [ 0 ] || h -> ref_count [ 1 ]- 1 > max [ 1 ]){
void ff_ivi_output_plane ( IVIPlaneDesc * plane , uint8_t * dst , int dst_pitch ) <nl> const int16_t * src = plane -> bands [ 0 ]. buf ; <nl> uint32_t pitch = plane -> bands [ 0 ]. pitch ; <nl>  <nl> + if (! src ) <nl> + return ; <nl> + <nl> for ( y = 0 ; y < plane -> height ; y ++) { <nl> for ( x = 0 ; x < plane -> width ; x ++) <nl> dst [ x ] = av_clip_uint8 ( src [ x ] + 128 );
retry : <nl> is -> frame_timer += delay * FFMAX ( 1 , floor (( time - is -> frame_timer ) / delay )); <nl>  <nl> SDL_LockMutex ( is -> pictq_mutex ); <nl> - if (! isnan ( vp -> pts )) <nl> + if (! redisplay && ! isnan ( vp -> pts )) <nl> update_video_pts ( is , vp -> pts , vp -> pos , vp -> serial ); <nl> SDL_UnlockMutex ( is -> pictq_mutex ); <nl> 
av_cold void ff_ac3dsp_init_x86 ( AC3DSPContext * c , int bit_exact ) <nl> c -> ac3_rshift_int32 = ff_ac3_rshift_int32_mmx ; <nl> } <nl> if ( EXTERNAL_AMD3DNOW ( mm_flags )) { <nl> - c -> extract_exponents = ff_ac3_extract_exponents_3dnow ; <nl> if (! bit_exact ) { <nl> c -> float_to_fixed24 = ff_float_to_fixed24_3dnow ; <nl> }
mp_image_t * vf_get_image ( vf_instance_t * vf , unsigned int outfmt , int mp_imgtype , <nl> } <nl>  <nl> mpi -> qscale = NULL ; <nl> - } <nl> mpi -> usage_count ++; <nl> + } <nl> // printf ("\ rVF_MPI : % p % p % p % d % d % d \ n ", <nl> // mpi -> planes [ 0 ], mpi -> planes [ 1 ], mpi -> planes [ 2 ], <nl> // mpi -> stride [ 0 ], mpi -> stride [ 1 ], mpi -> stride [ 2 ]);
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> tracks [ i ]. stream , st -> index_entries [ index ]. timestamp , <nl> AVSEEK_FLAG_BACKWARD ); <nl> while ( index_sub >= 0 && <nl> - index_min >= 0 && <nl> + index_min > 0 && <nl> tracks [ i ]. stream -> index_entries [ index_sub ]. pos < st -> index_entries [ index_min ]. pos && <nl> st -> index_entries [ index ]. timestamp - tracks [ i ]. stream -> index_entries [ index_sub ]. timestamp < 30000000000 / matroska -> time_scale ) <nl> index_min --;
int attribute_align_arg avcodec_encode_audio2 ( AVCodecContext * avctx , <nl> implement encode2 () */ <nl> buf_size = 2 * avctx -> frame_size * avctx -> channels * <nl> av_get_bytes_per_sample ( avctx -> sample_fmt ); <nl> - buf_size += FF_MIN_BUFFER_SIZE ; <nl> + buf_size += 2 * FF_MIN_BUFFER_SIZE ; <nl> } <nl> } <nl> if (( ret = ff_alloc_packet ( avpkt , buf_size )))
fail : <nl> if ( pkt -> stream_index == seg -> reference_stream_index ) <nl> seg -> frame_count ++; <nl>  <nl> - if ( ret < 0 ) { <nl> - if ( seg -> list ) <nl> - avio_close ( seg -> list_pb ); <nl> - avformat_free_context ( oc ); <nl> - } <nl> - <nl> return ret ; <nl> } <nl> 
static int cinepak_decode_vectors ( CinepakContext * s , cvid_strip * strip , <nl> const uint8_t * eod = ( data + size ); <nl> uint32_t flag , mask ; <nl> uint8_t * cb0 , * cb1 , * cb2 , * cb3 ; <nl> - unsigned int x , y ; <nl> + int x , y ; <nl> char * ip0 , * ip1 , * ip2 , * ip3 ; <nl>  <nl> flag = 0 ;
static int hevc_frame_start ( HEVCContext * s ) <nl>  <nl> fail : <nl> if ( s -> ref ) <nl> - ff_thread_report_progress (& s -> ref -> tf , INT_MAX , 0 ); <nl> + ff_hevc_unref_frame ( s , s -> ref , ~ 0 ); <nl> s -> ref = NULL ; <nl> return ret ; <nl> }
static void put_ebml_num ( ByteIOContext * pb , uint64_t num , int bytes ) <nl> static void put_ebml_uint ( ByteIOContext * pb , unsigned int elementid , uint64_t val ) <nl> { <nl> int i , bytes = 1 ; <nl> - while ( val >> bytes * 8 && bytes < 8 ) bytes ++; <nl> + while ( bytes < 8 && val >> bytes * 8 ) bytes ++; <nl>  <nl> put_ebml_id ( pb , elementid ); <nl> put_ebml_num ( pb , bytes , 0 );
AV_WN32A ( v0 + i , r ); \ <nl> AV_WN32A ( v1 + i , r ); \ <nl> } \ <nl> - for ( i = 0 ; i < BUF_SIZE * 8 / 3 ; i += 4 ) { \ <nl> + for ( i = 0 ; i < width * 8 / 3 ; i += 4 ) { \ <nl> uint32_t r = rnd (); \ <nl> AV_WN32A ( dst0 + i , r ); \ <nl> AV_WN32A ( dst1 + i , r ); \
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl>  <nl> if ( hdr -> substreamid == info -> num_ind_sub + 1 ) { <nl> // info -> num_ind_sub ++; <nl> - avpriv_request_sample ( track -> par , " Multiple independent substreams "); <nl> + avpriv_request_sample ( mov -> fc , " Multiple independent substreams "); <nl> return AVERROR_PATCHWELCOME ; <nl> } else if ( hdr -> substreamid < info -> num_ind_sub || <nl> hdr -> substreamid == 0 && info -> substream [ 0 ]. bsid ) {
static inline int convert_frame ( AVAudioResampleContext * avr , <nl>  <nl> static inline int available_samples ( AVFrame * out ) <nl> { <nl> + int samples ; <nl> int bytes_per_sample = av_get_bytes_per_sample ( out -> format ); <nl> - int samples = out -> linesize [ 0 ] / bytes_per_sample ; <nl> + if (! bytes_per_sample ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> + samples = out -> linesize [ 0 ] / bytes_per_sample ; <nl> if ( av_sample_fmt_is_planar ( out -> format )) { <nl> return samples ; <nl> } else {
static int decode_motion ( GetBitContext * gb ) <nl> static int decode_mb ( MadContext * s , AVFrame * frame , int inter ) <nl> { <nl> int mv_map = 0 ; <nl> - int mv_x , mv_y ; <nl> + int av_uninit ( mv_x ), av_uninit ( mv_y ); <nl> int j ; <nl>  <nl> if ( inter ) {
static int wma_decode_superframe ( AVCodecContext * avctx , <nl> return 0 ; <nl> } <nl> if ( buf_size < s -> block_align ) <nl> - return 0 ; <nl> + return AVERROR ( EINVAL ); <nl> buf_size = s -> block_align ; <nl>  <nl> samples = data ;
int ff_ivi_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> avctx -> release_buffer ( avctx , & ctx -> frame ); <nl>  <nl> ctx -> frame . reference = 0 ; <nl> + avcodec_set_dimensions ( avctx , ctx -> planes [ 0 ]. width , ctx -> planes [ 0 ]. height ); <nl> if (( result = avctx -> get_buffer ( avctx , & ctx -> frame )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return result ;
 <nl> typedef struct Hnm4VideoContext { <nl> uint8_t version ; <nl> - uint16_t width ; <nl> - uint16_t height ; <nl> + int width ; <nl> + int height ; <nl> uint8_t * current ; <nl> uint8_t * previous ; <nl> uint8_t * buffer1 ;
static int ebml_parse_elem ( MatroskaDemuxContext * matroska , <nl> data = ( char *) data + syntax -> data_offset ; <nl> if ( syntax -> list_elem_size ) { <nl> EbmlList * list = data ; <nl> - newelem = av_realloc ( list -> elem , ( list -> nb_elem + 1 )* syntax -> list_elem_size ); <nl> + newelem = av_realloc_array ( list -> elem , list -> nb_elem + 1 , syntax -> list_elem_size ); <nl> if (! newelem ) <nl> return AVERROR ( ENOMEM ); <nl> list -> elem = newelem ;
static av_cold int init ( AVFilterContext * ctx , const char * args , void * opaque ) <nl> static av_cold void uninit ( AVFilterContext * ctx ) <nl> { <nl> BufferSourceContext * s = ctx -> priv ; <nl> - while ( av_fifo_size ( s -> fifo )) { <nl> + while ( s -> fifo && av_fifo_size ( s -> fifo )) { <nl> AVFilterBufferRef * buf ; <nl> av_fifo_generic_read ( s -> fifo , & buf , sizeof ( buf ), NULL ); <nl> avfilter_unref_buffer ( buf );
static void compute_scale_factors ( unsigned char scale_code [ SBLIMIT ], <nl> vmax = v ; <nl> } <nl> /* compute the scale factor index using log 2 computations */ <nl> - if ( vmax > 0 ) { <nl> + if ( vmax > 1 ) { <nl> n = av_log2 ( vmax ); <nl> /* n is the position of the MSB of vmax . now <nl> use at most 2 compares to find the index */
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> pkt . size = data_size ; <nl>  <nl> ret = ff_vp8_decode_frame ( avctx , p , got_frame , & pkt ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( s -> has_alpha ) { <nl> ret = vp8_lossy_decode_alpha ( avctx , p , s -> alpha_data , <nl> s -> alpha_data_size );
static int ff_filter_frame_framed ( AVFilterLink * link , AVFilterBufferRef * frame ) <nl> } else <nl> out = frame ; <nl>  <nl> - while ( cmd && cmd -> time <= frame -> pts * av_q2d ( link -> time_base )){ <nl> + while ( cmd && cmd -> time <= out -> pts * av_q2d ( link -> time_base )){ <nl> av_log ( link -> dst , AV_LOG_DEBUG , <nl> " Processing command time :% f command :% s arg :% s \ n ", <nl> cmd -> time , cmd -> command , cmd -> arg );
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> avio_skip ( pb , 16 ); <nl>  <nl> for ( type = 0 ; type != - 1 && avio_tell ( pb ) < next ; ) { <nl> + if ( url_feof ( pb )) <nl> + return AVERROR ( EOF ); <nl> type = avio_rb16 ( pb ); <nl> len = avio_rb16 ( pb ); <nl> av_log ( c -> fc , AV_LOG_DEBUG , " type % d , len % d \ n ", type , len );
static int64_t read_ts ( char ** line , int * duration ) <nl> int64_t start , end ; <nl>  <nl> if ( sscanf (* line , "%" SCNd64 ",%" SCNd64 , & start , & end ) == 2 ) { <nl> - * line += strcspn (* line , "\"") + 1 ; <nl> + * line += strcspn (* line , "\""); <nl> + * line += !!** line ; <nl> * duration = end - start ; <nl> return start ; <nl> }
avs_decode_frame ( AVCodecContext * avctx , <nl> int i , j , x , y , stride , vect_w = 3 , vect_h = 3 ; <nl> AvsVideoSubType sub_type ; <nl> AvsBlockType type ; <nl> - GetBitContext change_map ; <nl> + GetBitContext change_map = { 0 }; // init to silence warning <nl>  <nl> if ( avctx -> reget_buffer ( avctx , p )) { <nl> av_log ( avctx , AV_LOG_ERROR , " reget_buffer () failed \ n ");
static void ebml_free ( EbmlSyntax * syntax , void * data ) <nl> j ++, ptr += syntax [ i ]. list_elem_size ) <nl> ebml_free ( syntax [ i ]. def . n , ptr ); <nl> av_freep (& list -> elem ); <nl> + list -> nb_elem = 0 ; <nl> } else <nl> ebml_free ( syntax [ i ]. def . n , data_off ); <nl> default :
static void park_frame_worker_threads ( FrameThreadContext * fctx , int thread_count <nl> pthread_cond_wait (& p -> output_cond , & p -> progress_mutex ); <nl> pthread_mutex_unlock (& p -> progress_mutex ); <nl> } <nl> + p -> got_frame = 0 ; <nl> } <nl> } <nl> 
static void conv411 ( uint8_t * dst , int dst_wrap , <nl> int w , c ; <nl> uint8_t * s1 , * s2 , * d ; <nl>  <nl> + width >>= 1 ; <nl> + <nl> for (; height > 0 ; height --) { <nl> s1 = src ; <nl> s2 = src + src_wrap ;
static int common_end ( AVCodecContext * avctx ){ <nl> PlaneContext * p = & s -> plane [ i ]; <nl>  <nl> av_freep (& p -> state ); <nl> + av_freep (& p -> vlc_state ); <nl> } <nl>  <nl> return 0 ;
void av_dump_format ( AVFormatContext * ic , int index , <nl> av_log ( NULL , AV_LOG_INFO , " Duration : "); <nl> if ( ic -> duration != AV_NOPTS_VALUE ) { <nl> int hours , mins , secs , us ; <nl> - int64_t duration = ic -> duration + 5000 ; <nl> + int64_t duration = ic -> duration + ( ic -> duration <= INT64_MAX - 5000 ? 5000 : 0 ); <nl> secs = duration / AV_TIME_BASE ; <nl> us = duration % AV_TIME_BASE ; <nl> mins = secs / 60 ;
static int mxf_read_content_storage ( void * arg , AVIOContext * pb , int tag , int siz <nl> MXFContext * mxf = arg ; <nl> switch ( tag ) { <nl> case 0x1901 : <nl> + if ( mxf -> packages_refs ) <nl> + av_log ( mxf -> fc , AV_LOG_VERBOSE , " Multiple packages_refs \ n "); <nl> + av_free ( mxf -> packages_refs ); <nl> mxf -> packages_count = avio_rb32 ( pb ); <nl> mxf -> packages_refs = av_calloc ( mxf -> packages_count , sizeof ( UID )); <nl> if (! mxf -> packages_refs )
static inline int get_ue_golomb ( GetBitContext * gb ) <nl> int log = 2 * av_log2 ( buf ) - 31 ; <nl> LAST_SKIP_BITS ( re , gb , 32 - log ); <nl> CLOSE_READER ( re , gb ); <nl> - if ( CONFIG_FTRAPV && log < 0 ) { <nl> + if ( log < 7 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid UE golomb code \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int smvjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_siz <nl> s -> picture [ 1 ]-> linesize [ i ] = mjpeg_data -> linesize [ i ]; <nl>  <nl> ret = av_frame_ref ( data , s -> picture [ 1 ]); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> } <nl>  <nl> - return ret ; <nl> + return avpkt -> size ; <nl> } <nl>  <nl> static const AVClass smvjpegdec_class = {
static int decode_rle ( uint8_t * bitmap , int linesize , int w , int h , <nl> if ( start >= buf_size ) <nl> return - 1 ; <nl>  <nl> + if ( w <= 0 || h <= 0 ) <nl> + return - 1 ; <nl> + <nl> bit_len = ( buf_size - start ) * 8 ; <nl> init_get_bits (& gb , buf + start , bit_len ); <nl> 
static int idcin_read_seek ( AVFormatContext * s , int stream_index , <nl> IdcinDemuxContext * idcin = s -> priv_data ; <nl>  <nl> if ( idcin -> first_pkt_pos > 0 ) { <nl> - int ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> + int64_t ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> ff_update_cur_dts ( s , s -> streams [ idcin -> video_stream_index ], 0 );
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> + modeex . Size = sizeof ( D3DDISPLAYMODEEX ); <nl> hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> if ( FAILED ( hr )) { <nl> IDirect3D9Ex_Release ( d3d9ex );
static int aac_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> } <nl> start_ch += chans ; <nl> } <nl> - if (( ret = ff_alloc_packet2 ( avctx , avpkt , 768 * s -> channels ))) { <nl> + if (( ret = ff_alloc_packet2 ( avctx , avpkt , 8192 * s -> channels ))) { <nl> av_log ( avctx , AV_LOG_ERROR , " Error getting output packet \ n "); <nl> return ret ; <nl> }
int ff_hevc_decode_nal_sps ( HEVCContext * s ) <nl> if ( s -> sps_list [ sps_id ] && s -> sps == ( HEVCSPS *) s -> sps_list [ sps_id ]-> data ) { <nl> av_buffer_unref (& s -> current_sps ); <nl> s -> current_sps = av_buffer_ref ( s -> sps_list [ sps_id ]); <nl> + if (! s -> current_sps ) <nl> + s -> sps = NULL ; <nl> } <nl> av_buffer_unref (& s -> sps_list [ sps_id ]); <nl> s -> sps_list [ sps_id ] = sps_buf ;
static int read_header ( FFV1Context * f ) <nl> } else { <nl> const uint8_t * p = c -> bytestream_end ; <nl> for ( f -> slice_count = 0 ; <nl> - f -> slice_count < MAX_SLICES && 3 < p - c -> bytestream_start ; <nl> + f -> slice_count < MAX_SLICES && 3 + 5 *!! f -> ec < p - c -> bytestream_start ; <nl> f -> slice_count ++) { <nl> int trailer = 3 + 5 *!! f -> ec ; <nl> int size = AV_RB24 ( p - trailer );
static int read_code_table ( CLLCContext * ctx , GetBitContext * gb , VLC * vlc ) <nl>  <nl> count ++; <nl> } <nl> + if ( prefix > ( 65535 - 256 )/ 2 ) { <nl> + vlc -> table = NULL ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> prefix <<= 1 ; <nl> }
int main ( int argc , char ** argv ) <nl> printf (" Demuxing audio from file '% s ' into '% s '\ n ", src_filename , audio_dst_filename ); <nl>  <nl> /* read frames from the file */ <nl> - while ( av_read_frame ( fmt_ctx , & pkt ) >= 0 ) <nl> + while ( av_read_frame ( fmt_ctx , & pkt ) >= 0 ) { <nl> decode_packet (& got_frame , 0 ); <nl> + av_free_packet (& pkt ); <nl> + } <nl>  <nl> /* flush cached frames */ <nl> pkt . data = NULL ;
static int scale_vector ( int16_t * dst , const int16_t * vector , int length ) <nl> for ( i = 0 ; i < length ; i ++) <nl> max |= FFABS ( vector [ i ]); <nl>  <nl> - max = FFMIN ( max , 0x7FFF ); <nl> bits = normalize_bits ( max , 15 ); <nl>  <nl> if ( bits == 15 )
int ff_audio_interleave_init ( AVFormatContext * s , <nl> aic -> time_base = time_base ; <nl>  <nl> aic -> fifo_size = 100 * * aic -> samples ; <nl> - aic -> fifo = av_fifo_alloc_array ( 100 , * aic -> samples ); <nl> + if (!( aic -> fifo = av_fifo_alloc_array ( 100 , * aic -> samples ))) <nl> + return AVERROR ( ENOMEM ); <nl> } <nl> } <nl> 
static int msrle_decode_frame ( AVCodecContext * avctx , <nl> uint8_t * buf = avpkt -> data + ( avctx -> height - 1 )* istride ; <nl> int i , j ; <nl>  <nl> + if ( linesize < 0 ) <nl> + return linesize ; <nl> + <nl> for ( i = 0 ; i < avctx -> height ; i ++) { <nl> if ( avctx -> bits_per_coded_sample == 4 ) { <nl> for ( j = 0 ; j < avctx -> width - 1 ; j += 2 ) {
static int mxf_read_header ( AVFormatContext * s ) <nl> } <nl> if ( res < 0 ) { <nl> av_log ( s , AV_LOG_ERROR , " error reading header metadata \ n "); <nl> - return res ; <nl> + ret = res ; <nl> + goto fail ; <nl> } <nl> break ; <nl> } else {
static int mov_read_close ( AVFormatContext * s ) <nl> av_freep (& mov -> fragment_index_data ); <nl>  <nl> av_freep (& mov -> aes_decrypt ); <nl> + av_freep (& mov -> chapter_tracks ); <nl>  <nl> return 0 ; <nl> }
static int read_header ( AVFormatContext * s ) <nl> uint8_t root [ WTV_SECTOR_SIZE ]; <nl> AVIOContext * pb ; <nl> int64_t timeline_pos ; <nl> - int ret ; <nl> + int64_t ret ; <nl>  <nl> wtv -> epoch = <nl> wtv -> pts =
static int fourxm_read_packet ( AVFormatContext * s , <nl>  <nl> if ( ret < 0 ) { <nl> av_free_packet ( pkt ); <nl> - } else <nl> + } else { <nl> packet_read = 1 ; <nl> + av_shrink_packet ( pkt , ret + 8 ); <nl> + } <nl> break ; <nl>  <nl> case snd__TAG :
AVStream * avformat_new_stream ( AVFormatContext * s , const AVCodec * c ) <nl> st -> info -> last_dts = AV_NOPTS_VALUE ; <nl>  <nl> st -> codec = avcodec_alloc_context3 ( c ); <nl> + if (! st -> codec ) { <nl> + av_free ( st -> info ); <nl> + av_free ( st ); <nl> + return NULL ; <nl> + } <nl> if ( s -> iformat ) { <nl> /* no default bitrate if decoding */ <nl> st -> codec -> bit_rate = 0 ;
int avformat_seek_file ( AVFormatContext * s , int stream_index , int64_t min_ts , int <nl> { <nl> if ( min_ts > ts || max_ts < ts ) <nl> return - 1 ; <nl> + if ( stream_index < - 1 || stream_index >= ( int ) s -> nb_streams ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> if ( s -> seek2any > 0 ) <nl> flags |= AVSEEK_FLAG_ANY ;
static int mp_decode_frame ( AVCodecContext * avctx , <nl> if ( sz == 0 ) <nl> goto end ; <nl>  <nl> + if ( mp -> max_codes_bits <= 0 ) <nl> + goto end ; <nl> if ( init_vlc (& mp -> vlc , mp -> max_codes_bits , mp -> codes_count , & mp -> codes [ 0 ]. size , sizeof ( HuffCode ), 1 , & mp -> codes [ 0 ]. code , sizeof ( HuffCode ), 4 , 0 )) <nl> goto end ; <nl> mp_decode_frame_helper ( mp , & gb );
av_cold int ff_vp8_decode_free ( AVCodecContext * avctx ) <nl> VP8Context * s = avctx -> priv_data ; <nl> int i ; <nl>  <nl> + if (! s ) <nl> + return 0 ; <nl> + <nl> vp8_decode_flush_impl ( avctx , 1 ); <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( s -> frames ); i ++) <nl> av_frame_free (& s -> frames [ i ]. tf . f );
static int decode_frame ( AVCodecContext * avctx , <nl> int size , offset , start = 0 ; <nl>  <nl> offset = bytestream2_get_le16 ( gb ); <nl> - if ( offset > s -> nb_blocks ) <nl> + if ( offset >= s -> nb_blocks ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> size = bytestream2_get_le16 ( gb );
static int transcode_init ( void ) <nl> AVFormatContext * oc ; <nl> OutputStream * ost ; <nl> InputStream * ist ; <nl> - char error [ 1024 ]; <nl> + char error [ 1024 ] = { 0 }; <nl> int want_sdp = 1 ; <nl>  <nl> for ( i = 0 ; i < nb_filtergraphs ; i ++) {
int ff_jni_init_jfields ( JNIEnv * env , void * jfields , const struct FFJniField * jfi <nl>  <nl> last_clazz = *( jclass *)(( uint8_t *) jfields + jfields_mapping [ i ]. offset ) = <nl> global ? (* env )-> NewGlobalRef ( env , clazz ) : clazz ; <nl> + <nl> + if ( global ) { <nl> + (* env )-> DeleteLocalRef ( env , clazz ); <nl> + } <nl> + <nl> } else { <nl>  <nl> if (! last_clazz ) {
int main ( void ){ <nl> AVPixFmtDescriptor * desc = av_pix_fmt_desc_get ( i ); <nl> if (! desc ) <nl> continue ; <nl> - av_log ( 0 , AV_LOG_INFO , " pix fmt % s % d \ n ", desc -> name , is_yuv_planar ( i )); <nl> + av_log ( 0 , AV_LOG_INFO , " pix fmt % s yuv_plan :% d avg_bpp :% d \ n ", desc -> name , is_yuv_planar ( i ), avg_bits_per_pixel ( i )); <nl> } <nl> return 0 ; <nl> }
FF_ENABLE_DEPRECATION_WARNINGS <nl> wipe_side_data ( dst ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + if ( sd_src -> buf ) { <nl> sd_dst -> buf = av_buffer_ref ( sd_src -> buf ); <nl> if (! sd_dst -> buf ) { <nl> wipe_side_data ( dst ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl> sd_dst -> data = sd_dst -> buf -> data ; <nl> sd_dst -> size = sd_dst -> buf -> size ; <nl> + } <nl> } <nl> av_dict_copy (& sd_dst -> metadata , sd_src -> metadata , 0 ); <nl> }
void ff_aac_update_ltp ( AACEncContext * s , SingleChannelElement * sce ) <nl> lag = i ; <nl> } <nl> } <nl> - lag = av_clip ( lag , 0 , 2048 ); /* 11 bits => 2 ^ 11 = 2048 */ <nl> + lag = av_clip ( lag , 0 , 2047 ); /* 11 bits => 2 ^ 11 = 0 -> 2047 */ <nl>  <nl> if (! lag ) { <nl> sce -> ics . ltp . lag = lag ;
static int pmp_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( pmp -> cur_stream == 0 ) { <nl> int num_packets ; <nl> pmp -> audio_packets = avio_r8 ( pb ); <nl> + if (! pmp -> audio_packets ) { <nl> + av_log_ask_for_sample ( s , " 0 audio packets \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> num_packets = ( pmp -> num_streams - 1 ) * pmp -> audio_packets + 1 ; <nl> avio_skip ( pb , 8 ); <nl> pmp -> current_packet = 0 ;
vbv_retry : <nl> if ( s -> mb_info ) <nl> av_packet_shrink_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , s -> mb_info_size ); <nl> } else { <nl> - assert (( put_bits_ptr (& s -> pb ) == s -> pb . buf )); <nl> s -> frame_bits = 0 ; <nl> } <nl> assert (( s -> frame_bits & 7 ) == 0 );
static int vc1_decode_frame ( AVCodecContext * avctx , <nl> divider = find_next_marker ( buf , buf + buf_size ); <nl> if (( divider == ( buf + buf_size )) || AV_RB32 ( divider ) != VC1_CODE_FIELD ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Error in WVC1 interlaced frame \ n "); <nl> + av_free ( buf2 ); <nl> return - 1 ; <nl> } <nl> 
int attribute_align_arg avcodec_encode_audio ( AVCodecContext * avctx , <nl> const short * samples ) <nl> { <nl> AVPacket pkt ; <nl> - AVFrame frame0 ; <nl> + AVFrame frame0 = { 0 }; <nl> AVFrame * frame ; <nl> int ret , samples_size , got_packet ; <nl> 
static int configure_output_audio_filter ( FilterGraph * fg , OutputFilter * ofilter , <nl> pad_idx = 0 ; \ <nl> } while ( 0 ) <nl>  <nl> - if ( audio_sync_method > 0 ) { <nl> + if ( audio_sync_method > 0 && 0 ) { <nl> char args [ 256 ] = { 0 }; <nl>  <nl> av_strlcatf ( args , sizeof ( args ), " min_comp = 0 . 001 : min_hard_comp =% f ", audio_drift_threshold );
static int get_qcd ( Jpeg2000DecoderContext * s , int n , Jpeg2000QuantStyle * q , <nl> Jpeg2000QuantStyle tmp ; <nl> int compno , ret ; <nl>  <nl> + memset (& tmp , 0 , sizeof ( tmp )); <nl> + <nl> if (( ret = get_qcx ( s , n , & tmp )) < 0 ) <nl> return ret ; <nl> for ( compno = 0 ; compno < s -> ncomponents ; compno ++)
uint64_t time = rdtsc (); <nl> s -> workaround_bugs = avctx -> workaround_bugs ; <nl> s -> flags = avctx -> flags ; <nl>  <nl> - /* no supplementary picture */ <nl> + * data_size = 0 ; <nl> + <nl> + /* no supplementary picture */ <nl> if ( buf_size == 0 ) { <nl> - * data_size = 0 ; <nl> return 0 ; <nl> } <nl> 
static void dump_stream_format ( AVFormatContext * ic , int i , <nl> int g = av_gcd ( st -> time_base . num , st -> time_base . den ); <nl> AVDictionaryEntry * lang = av_dict_get ( st -> metadata , " language ", NULL , 0 ); <nl>  <nl> + if (! g ) <nl> + g = 1 ; <nl> + <nl> avcodec_string ( buf , sizeof ( buf ), st -> codec , is_output ); <nl> av_log ( NULL , AV_LOG_INFO , " Stream #% d :% d ", index , i ); <nl> 
static int config_props ( AVFilterLink * inlink ) <nl> s -> hsub = pixdesc -> log2_chroma_w ; <nl> s -> vsub = pixdesc -> log2_chroma_h ; <nl>  <nl> - s -> bpp = av_get_bits_per_pixel ( pixdesc ) >> 3 ; <nl> + s -> bpp = pixdesc -> flags & AV_PIX_FMT_FLAG_PLANAR ? <nl> + 1 : <nl> + av_get_bits_per_pixel ( pixdesc ) >> 3 ; <nl> s -> alpha &= !!( pixdesc -> flags & AV_PIX_FMT_FLAG_ALPHA ); <nl> s -> is_packed_rgb = ff_fill_rgba_map ( s -> rgba_map , inlink -> format ) >= 0 ; <nl> 
static int mxf_set_audio_pts ( MXFContext * mxf , AVCodecContext * codec , AVPacket * p <nl> { <nl> MXFTrack * track = mxf -> fc -> streams [ pkt -> stream_index ]-> priv_data ; <nl> pkt -> pts = track -> sample_count ; <nl> + if ( codec -> channels <= 0 || av_get_bits_per_sample ( codec -> codec_id ) <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl> track -> sample_count += pkt -> size / ( codec -> channels * av_get_bits_per_sample ( codec -> codec_id ) / 8 ); <nl> return 0 ; <nl> }
static int parse_header ( OutputStream * os , const uint8_t * buf , int buf_size ) <nl> if ( size > buf_size ) <nl> return AVERROR_INVALIDDATA ; <nl> if ( type == 8 || type == 9 ) { <nl> - if ( os -> nb_extra_packets > FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> + if ( os -> nb_extra_packets >= FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> return AVERROR_INVALIDDATA ; <nl> os -> extra_packet_sizes [ os -> nb_extra_packets ] = size ; <nl> os -> extra_packets [ os -> nb_extra_packets ] = av_malloc ( size );
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * insamples ) <nl> av_rescale_q (( p - ( int16_t *) insamples -> data [ 0 ]) / nb_channels , <nl> ( AVRational ){ 1 , inlink -> sample_rate }, <nl> outlink -> time_base ); <nl> - outlink -> out_buf = outpicref ; <nl> linesize = outpicref -> linesize [ 0 ]; <nl> memset ( outpicref -> data [ 0 ], 0 , showwaves -> h * linesize ); <nl> }
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> int best_fps = 0 ; <nl> double best_error = 0 . 01 ; <nl>  <nl> + if ( delta_dts >= INT64_MAX / st -> time_base . num || <nl> + delta_packets >= INT64_MAX / st -> time_base . den ) <nl> + continue ; <nl> av_reduce (& st -> avg_frame_rate . num , & st -> avg_frame_rate . den , <nl> delta_packets *( int64_t ) st -> time_base . den , <nl> delta_dts *( int64_t ) st -> time_base . num , 60000 );
static int dca_convert_bitstream ( uint8_t * src , int src_size , uint8_t * dst , <nl> uint16_t * ssrc = ( uint16_t *) src , * sdst = ( uint16_t *) dst ; <nl> PutBitContext pb ; <nl>  <nl> + if (( unsigned ) src_size > ( unsigned ) max_size ) <nl> + return - 1 ; <nl> + <nl> mrk = AV_RB32 ( src ); <nl> switch ( mrk ) { <nl> case DCA_MARKER_RAW_BE :
static AVFilterContext * create_filter ( AVFilterGraph * ctx , int index , <nl> AVFilter * filt ; <nl> char inst_name [ 30 ]; <nl>  <nl> - snprintf ( inst_name , sizeof ( inst_name ), " Parsed filter % d ", index ); <nl> + snprintf ( inst_name , sizeof ( inst_name ), " Filter % d % s ", index , filt_name ); <nl>  <nl> filt = avfilter_get_by_name ( filt_name ); <nl> 
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * buf ) <nl>  <nl> merge -> frame_requested = 0 ; <nl> draw_frame ( ctx , main_buf , alpha_buf ); <nl> - ff_filter_frame ( ctx -> outputs [ 0 ], avfilter_ref_buffer ( main_buf , ~ 0 )); <nl> + ff_filter_frame ( ctx -> outputs [ 0 ], main_buf ); <nl> avfilter_unref_buffer ( alpha_buf ); <nl> } <nl> return 0 ;
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * insamples ) <nl> } <nl> if ( showwaves -> buf_idx == showwaves -> w ) <nl> push_frame ( outlink ); <nl> + outpicref = showwaves -> outpicref ; <nl> } <nl>  <nl> avfilter_unref_buffer ( insamples );
static void fix_bitshift ( ShortenContext * s , int32_t * buffer ) <nl>  <nl> if ( s -> bitshift != 0 ) <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - buffer [ s -> nwrap + i ] <<= s -> bitshift ; <nl> + buffer [ i ] <<= s -> bitshift ; <nl> } <nl>  <nl> 
pp_mode * pp_get_mode_by_name_and_quality ( const char * name , int quality ) <nl> } <nl>  <nl> ppMode = av_malloc ( sizeof ( PPMode )); <nl> + if (! ppMode ) <nl> + return NULL ; <nl>  <nl> ppMode -> lumMode = 0 ; <nl> ppMode -> chromMode = 0 ;
static const struct { <nl>  <nl> static int webvtt_event_to_ass ( AVBPrint * buf , const char * p ) <nl> { <nl> - int i , again , skip = 0 ; <nl> + int i , again = 0 , skip = 0 ; <nl>  <nl> while (* p ) { <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> } else { <nl> frame -> key_frame = 0 ; <nl> frame -> pict_type = AV_PICTURE_TYPE_P ; <nl> + if ( c -> decomp_len < 2LL * (( c -> width + c -> bw - 1 ) / c -> bw ) * (( c -> height + c -> bh - 1 ) / c -> bh )) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( c -> decomp_len ) <nl> c -> decode_xor ( c ); <nl> }
matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , int size , <nl> memcpy ( pkt -> data , matroska -> tracks [ track ]-> encoding_settings , offset ); <nl> memcpy ( pkt -> data + offset , pkt_data , pkt_size ); <nl>  <nl> + if ( pkt_data != data ) <nl> + av_free ( pkt_data ); <nl> + <nl> if ( n == 0 ) <nl> pkt -> flags = is_keyframe ; <nl> pkt -> stream_index = stream_index ;
static int ftp_status ( FTPContext * s , char ** line , const int response_codes []) <nl>  <nl> while (! code_found || dash ) { <nl> if (( err = ftp_get_line ( s , buf , sizeof ( buf ))) < 0 ) { <nl> - av_bprint_finalize (& line_buffer , NULL ); <nl> + if ( line ) <nl> + av_bprint_finalize (& line_buffer , NULL ); <nl> return err ; <nl> } <nl> 
int64_t ff_gen_search ( AVFormatContext * s , int stream_index , int64_t target_ts , <nl> } <nl>  <nl> if ( ts_max == AV_NOPTS_VALUE ){ <nl> - int step = 1024 ; <nl> + int64_t step = 1024 ; <nl> filesize = avio_size ( s -> pb ); <nl> pos_max = filesize - 1 ; <nl> do {
static int tm2_read_stream ( TM2Context * ctx , const uint8_t * buf , int stream_id , i <nl> buf += 4 ; cur += 4 ; <nl> buf += 4 ; cur += 4 ; /* unused by decoder */ <nl>  <nl> + if ( skip < cur ) <nl> + return - 1 ; <nl> init_get_bits (& ctx -> gb , buf , ( skip - cur ) * 8 ); <nl> if ( tm2_build_huff_table ( ctx , & codes ) == - 1 ) <nl> return - 1 ;
static void do_subtitle_out ( AVFormatContext * s , <nl>  <nl> if (! subtitle_out ) { <nl> subtitle_out = av_malloc ( subtitle_out_max_size ); <nl> + if (! subtitle_out ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Failed to allocate subtitle_out \ n "); <nl> + exit_program ( 1 ); <nl> + } <nl> } <nl>  <nl> /* Note : DVB subtitle need one packet to draw them and one other
static int decode_frame ( WmallDecodeCtx * s ) <nl> /* decode tile information */ <nl> if (( ret = decode_tilehdr ( s ))) { <nl> s -> packet_loss = 1 ; <nl> + av_frame_unref ( s -> frame ); <nl> return ret ; <nl> } <nl> 
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + avctx -> height * lsize ) { <nl> + if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + ( uint64_t ) avctx -> height * lsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " input buffer too small \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl>  <nl> + if ( avpkt -> size < 20 + avctx -> width * avctx -> height / 16 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Input packet too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( s -> format != format ) { <nl> if ( ret < 0 ) <nl> return ret ;
static int decodeTonalComponents ( GetBitContext * gb , tonal_component * pComponent <nl>  <nl> for ( k = 0 ; k < coded_components ; k ++) { <nl> sfIndx = get_bits ( gb , 6 ); <nl> + if ( component_count >= 64 ) <nl> + return AVERROR_INVALIDDATA ; <nl> pComponent [ component_count ]. pos = j * 64 + ( get_bits ( gb , 6 )); <nl> max_coded_values = SAMPLES_PER_FRAME - pComponent [ component_count ]. pos ; <nl> coded_values = coded_values_per_component + 1 ;
static void read_apic ( AVFormatContext * s , AVIOContext * pb , int taglen , char * tag <nl> goto fail ; <nl> } <nl>  <nl> - apic -> buf = av_buffer_alloc ( taglen ); <nl> + apic -> buf = av_buffer_alloc ( taglen + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + apic -> buf -> size -= FF_INPUT_BUFFER_PADDING_SIZE ; <nl> if (! apic -> buf || ! taglen || avio_read ( pb , apic -> buf -> data , taglen ) != taglen ) <nl> goto fail ; <nl> 
static int decode_header ( SnowContext * s ){ <nl> s -> block_max_depth = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( FFABS ( s -> qbias ) > 127 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " qbias % d is too large \ n ", s -> qbias ); <nl> + s -> qbias = 0 ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> return 0 ; <nl> }
struct ff_timecode { <nl> char * str ; ///< string following the hh : mm : ss [:;.] ff format <nl> int start ; ///< timecode frame start <nl> int drop ; ///< drop flag ( 1 if drop , else 0 ) <nl> - AVRational rate ; ///< Frame rate in rationnal form <nl> + AVRational rate ; ///< Frame rate in rational form <nl> }; <nl>  <nl> /**
int attribute_align_arg avcodec_decode_audio4 ( AVCodecContext * avctx , <nl> frame -> extended_buf = NULL ; <nl> frame -> nb_extended_buf = 0 ; <nl> } <nl> - } <nl> - <nl> - if ( ret < 0 && frame -> data [ 0 ]) <nl> + } else if ( frame -> data [ 0 ]) <nl> av_frame_unref ( frame ); <nl> } <nl> 
static int decode_vop_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> int time_incr , time_increment ; <nl> int64_t pts ; <nl>  <nl> + s -> mcsel = 0 ; <nl> s -> pict_type = get_bits ( gb , 2 ) + AV_PICTURE_TYPE_I ; /* pict type : I = 0 , P = 1 */ <nl> if ( s -> pict_type == AV_PICTURE_TYPE_B && s -> low_delay && <nl> ctx -> vol_control_parameters == 0 && !( s -> avctx -> flags & AV_CODEC_FLAG_LOW_DELAY )) {
static inline struct rgbvec lerp ( const struct rgbvec * v0 , const struct rgbvec * v <nl>  <nl> # define NEAR ( x ) (( int )(( x ) + . 5 )) <nl> # define PREV ( x ) (( int )( x )) <nl> -# define NEXT ( x ) (( int )( x ) + 1 ) <nl> +# define NEXT ( x ) ( FFMIN (( int )( x ) + 1 , lut3d -> lutsize - 1 )) <nl>  <nl> /** <nl> * Get the nearest defined point
static int sdp_probe ( AVProbeData * p1 ) <nl>  <nl> /* we look for a line beginning " c = IN IP " */ <nl> while ( p < p_end && * p != '\ 0 ') { <nl> - if ( p + sizeof (" c = IN IP ") - 1 < p_end && <nl> + if ( sizeof (" c = IN IP ") - 1 < p_end - p && <nl> av_strstart ( p , " c = IN IP ", NULL )) <nl> return AVPROBE_SCORE_EXTENSION ; <nl> 
void ff_set_mpeg4_time ( MpegEncContext * s ) <nl>  <nl> static void mpeg4_encode_gop_header ( MpegEncContext * s ) <nl> { <nl> - int hours , minutes , seconds ; <nl> + int64_t hours , minutes , seconds ; <nl> int64_t time ; <nl>  <nl> put_bits (& s -> pb , 16 , 0 );
static av_cold int init ( AVFilterContext * ctx , const char * args ) <nl>  <nl> aresample -> next_pts = AV_NOPTS_VALUE ; <nl> aresample -> swr = swr_alloc (); <nl> - if (! aresample -> swr ) <nl> - return AVERROR ( ENOMEM ); <nl> + if (! aresample -> swr ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl>  <nl> if ( args ) { <nl> char * ptr = argd , * token ;
static void revert_cdlms ( WmallDecodeCtx * s , int ch , <nl> s -> channel_residues [ ch ][ icoef ] = input ; <nl> } <nl> } <nl> + emms_c (); <nl> } <nl>  <nl> static void revert_inter_ch_decorr ( WmallDecodeCtx * s , int tile_size )
static int libopenjpeg_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> case AV_PIX_FMT_GBRP14 : <nl> case AV_PIX_FMT_GBRP16 : <nl> gbrframe = av_frame_alloc (); <nl> + if (! gbrframe ) <nl> + return AVERROR ( ENOMEM ); <nl> av_frame_ref ( gbrframe , frame ); <nl> gbrframe -> data [ 0 ] = frame -> data [ 2 ]; // swap to be rgb <nl> gbrframe -> data [ 1 ] = frame -> data [ 0 ];
static int compat_decode ( AVCodecContext * avctx , AVFrame * frame , <nl> int * got_frame , AVPacket * pkt ) <nl> { <nl> AVCodecInternal * avci = avctx -> internal ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> av_assert0 ( avci -> compat_decode_consumed == 0 ); <nl> 
static int decode_frame ( WmallDecodeCtx * s ) <nl>  <nl> /* decode all subframes */ <nl> while (! s -> parsed_all_subframes ) { <nl> + int decoded_samples = s -> channel [ 0 ]. decoded_samples ; <nl> if ( decode_subframe ( s ) < 0 ) { <nl> s -> packet_loss = 1 ; <nl> + if ( s -> frame -> nb_samples ) <nl> + s -> frame -> nb_samples = decoded_samples ; <nl> return 0 ; <nl> } <nl> }
static int FUNC ( extension_data )( CodedBitstreamContext * ctx , RWContext * rw , <nl> BitstreamContext start ; <nl> uint8_t bit ; <nl> start = * rw ; <nl> - for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++); <nl> + for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++) <nl> + bitstream_skip ( rw , 1 ); <nl> current -> bit_length = k ; <nl> if ( k > 0 ) { <nl> * rw = start ;
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> if (! s -> context_initialized ) { <nl> if ( ff_msmpeg4_decode_init ( avctx ) < 0 || ff_vc1_decode_init_alloc_tables ( v ) < 0 ) <nl> - return - 1 ; <nl> + goto err ; <nl>  <nl> s -> low_delay = ! avctx -> has_b_frames || v -> res_sprite ; <nl> 
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl> int offset = get_bits_count (& gb ) + 7 >> 3 ; <nl> uint8_t * buf ; <nl>  <nl> - if (( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> + if ( watermark_height > 0 && <nl> + ( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> return - 1 ; <nl>  <nl> buf = av_malloc ( buf_len );
static char * choose_pix_fmts ( OutputStream * ost ) <nl> } <nl> if ( ost -> st -> codec -> pix_fmt != PIX_FMT_NONE ) { <nl> return av_strdup ( av_get_pix_fmt_name ( choose_pixel_fmt ( ost -> st , ost -> enc , ost -> st -> codec -> pix_fmt ))); <nl> - } else if ( ost -> enc -> pix_fmts ) { <nl> + } else if ( ost -> enc && ost -> enc -> pix_fmts ) { <nl> const enum PixelFormat * p ; <nl> AVIOContext * s = NULL ; <nl> uint8_t * ret ;
typedef struct { <nl>  <nl> static const AVOption options [] = { <nl> { " oggpagesize ", " Set preferred Ogg page size .", <nl> - offsetof ( OGGContext , pref_size ), FF_OPT_TYPE_INT , 0 , 0 , MAX_PAGE_SIZE , AV_OPT_FLAG_ENCODING_PARAM }, <nl> + offsetof ( OGGContext , pref_size ), FF_OPT_TYPE_INT , {. dbl = 0 }, 0 , MAX_PAGE_SIZE , AV_OPT_FLAG_ENCODING_PARAM }, <nl> { NULL }, <nl> }; <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ){ <nl>  <nl> } else { <nl> if ( s0 -> current_picture_ptr -> frame_num != h -> frame_num ) { <nl> + ff_thread_report_progress (( AVFrame *) s0 -> current_picture_ptr , INT_MAX , <nl> + s0 -> picture_structure == PICT_BOTTOM_FIELD ); <nl> /* <nl> * This and previous field had <nl> * different frame_nums . Consider this field first in
static int h264_mp4toannexb_filter ( AVBitStreamFilterContext * bsfc , <nl> buf += ctx -> length_size ; <nl> unit_type = * buf & 0x1f ; <nl>  <nl> - if ( buf + nal_size > buf_end || nal_size < 0 ) <nl> + if ( nal_size > buf_end - buf || nal_size < 0 ) <nl> goto fail ; <nl>  <nl> if ( unit_type == 7 )
void avcodec_align_dimensions2 ( AVCodecContext * s , int * width , int * height , <nl> case AV_PIX_FMT_YUV422P9BE : <nl> case AV_PIX_FMT_YUV422P10LE : <nl> case AV_PIX_FMT_YUV422P10BE : <nl> + case AV_PIX_FMT_YUVA422P10LE : <nl> + case AV_PIX_FMT_YUVA422P10BE : <nl> case AV_PIX_FMT_YUV444P9LE : <nl> case AV_PIX_FMT_YUV444P9BE : <nl> case AV_PIX_FMT_YUV444P10LE : <nl> case AV_PIX_FMT_YUV444P10BE : <nl> + case AV_PIX_FMT_YUVA444P10LE : <nl> + case AV_PIX_FMT_YUVA444P10BE : <nl> case AV_PIX_FMT_GBRP9LE : <nl> case AV_PIX_FMT_GBRP9BE : <nl> case AV_PIX_FMT_GBRP10LE :
static int mpc7_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> bits = av_malloc ((( buf_size - 1 ) & ~ 3 ) + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! bits ) <nl> + return AVERROR ( ENOMEM ); <nl> c -> dsp . bswap_buf (( uint32_t *) bits , ( const uint32_t *)( buf + 4 ), ( buf_size - 4 ) >> 2 ); <nl> init_get_bits (& gb , bits , ( buf_size - 4 )* 8 ); <nl> skip_bits_long (& gb , buf [ 0 ]);
matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , int size , <nl> pkt = av_mallocz ( sizeof ( AVPacket )); <nl> /* XXX : prevent data copy ... */ <nl> if ( av_new_packet ( pkt , pkt_size + offset ) < 0 ) { <nl> + av_free ( pkt ); <nl> res = AVERROR ( ENOMEM ); <nl> n = laces - 1 ; <nl> break ;
static int Stagefright_decode_frame ( AVCodecContext * avctx , void * data , <nl> AVFrame * ret_frame ; <nl>  <nl> if (! s -> thread_started ) { <nl> - pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx ); <nl> + if ( pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx )) <nl> + return AVERROR ( ENOMEM ); <nl> s -> thread_started = true ; <nl> } <nl> 
int av_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , <nl> " Dropped corrupted packet ( stream = % d )\ n ", <nl> pkt -> stream_index ); <nl> + av_free_packet ( pkt ); <nl> continue ; <nl> } <nl> 
static av_cold int vc1_decode_init ( AVCodecContext * avctx ) <nl> avctx -> idct_algo = FF_IDCT_WMV2 ; <nl> } <nl>  <nl> - if ( ff_h263_decode_init ( avctx ) < 0 ) <nl> + if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) <nl> return - 1 ; <nl> if ( vc1_init_common ( v ) < 0 ) return - 1 ; <nl> - // only for ff_msmp4_mb_i_table <nl> - if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) return - 1 ; <nl>  <nl> avctx -> coded_width = avctx -> width ; <nl> avctx -> coded_height = avctx -> height ;
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> if (! comp -> i_data ) <nl> return AVERROR ( ENOMEM ); <nl> } <nl> - comp -> reslevel = av_malloc_array ( codsty -> nreslevels , sizeof (* comp -> reslevel )); <nl> + comp -> reslevel = av_calloc ( codsty -> nreslevels , sizeof (* comp -> reslevel )); <nl> if (! comp -> reslevel ) <nl> return AVERROR ( ENOMEM ); <nl> /* LOOP on resolution levels */
static void ac3_bit_alloc_calc_bap_c ( int16_t * mask , int16_t * psd , <nl> static void ac3_update_bap_counts_c ( uint16_t mant_cnt [ 16 ], uint8_t * bap , <nl> int len ) <nl> { <nl> - while ( len -- >= 0 ) <nl> + while ( len -- > 0 ) <nl> mant_cnt [ bap [ len ]]++; <nl> } <nl> 
static int msf_read_header ( AVFormatContext * s ) <nl> st -> codec -> codec_type = AVMEDIA_TYPE_AUDIO ; <nl> codec = avio_rb32 ( s -> pb ); <nl> st -> codec -> channels = avio_rb32 ( s -> pb ); <nl> - if ( st -> codec -> channels <= 0 ) <nl> + if ( st -> codec -> channels <= 0 || st -> codec -> channels >= INT_MAX / 1024 ) <nl> return AVERROR_INVALIDDATA ; <nl> size = avio_rb32 ( s -> pb ); <nl> st -> codec -> sample_rate = avio_rb32 ( s -> pb );
static int sdp_parse_fmtp_config_h264 ( AVStream * stream , <nl> } <nl> } else if (! strcmp ( attr , " sprop - parameter - sets ")) { <nl> codec -> extradata_size = 0 ; <nl> - codec -> extradata = NULL ; <nl> + av_freep (& codec -> extradata ); <nl>  <nl> while (* value ) { <nl> char base64packet [ 1024 ];
static int http_prepare_data ( HTTPContext * c ) <nl>  <nl> av_freep (& c -> pb_buffer ); <nl> len = avio_close_dyn_buf ( ctx -> pb , & c -> pb_buffer ); <nl> + ctx -> pb = NULL ; <nl> c -> cur_frame_bytes = len ; <nl> c -> buffer_ptr = c -> pb_buffer ; <nl> c -> buffer_end = c -> pb_buffer + len ;
int ff_mjpeg_find_marker ( MJpegDecodeContext * s , <nl> put_bits (& pb , 8 , x ); <nl> if ( x == 0xFF ) { <nl> x = src [ b ++]; <nl> + if ( x & 0x80 ) { <nl> + av_log ( s -> avctx , AV_LOG_WARNING , " Invalid escape sequence \ n "); <nl> + x &= 0x7f ; <nl> + } <nl> put_bits (& pb , 7 , x ); <nl> bit_count --; <nl> }
static int decode_pivot ( MSS1Context * ctx , ArithCoder * acoder , int base ) <nl> val = arith_get_number ( acoder , ( base + 1 ) / 2 - 2 ) + 3 ; <nl> } <nl>  <nl> - if ( val == base ) { <nl> + if (( unsigned ) val >= base ) { <nl> ctx -> corrupted = 1 ; <nl> return 0 ; <nl> }
int ff_dca_lbr_parse ( DCALbrDecoder * s , uint8_t * data , DCAExssAsset * asset ) <nl> LBRChunk hr_grid [ DCA_LBR_CHANNELS / 2 ]; <nl> LBRChunk ts1 [ DCA_LBR_CHANNELS / 2 ]; <nl> LBRChunk ts2 [ DCA_LBR_CHANNELS / 2 ]; <nl> - } chunk = { 0 }; <nl> + } chunk = { { 0 } }; <nl>  <nl> GetByteContext gb ; <nl> 
static int decode_audio_specific_config ( AACContext * ac , <nl> * <nl> * @ return Returns a 32 - bit pseudorandom integer <nl> */ <nl> - static av_always_inline int lcg_random ( int previous_val ) <nl> + static av_always_inline int lcg_random ( unsigned previous_val ) <nl> { <nl> return previous_val * 1664525 + 1013904223 ; <nl> }
typedef struct ScalingList { <nl> } ScalingList ; <nl>  <nl> typedef struct HEVCSPS { <nl> - int vps_id ; <nl> + unsigned vps_id ; <nl> int chroma_format_idc ; <nl> uint8_t separate_colour_plane_flag ; <nl>  <nl> typedef struct HEVCSPS { <nl> } HEVCSPS ; <nl>  <nl> typedef struct HEVCPPS { <nl> - int sps_id ; ///< seq_parameter_set_id <nl> + unsigned sps_id ; ///< seq_parameter_set_id <nl>  <nl> uint8_t sign_data_hiding_flag ; <nl> 
static int mpeg_mux_init ( AVFormatContext * ctx ) <nl> if (! s -> mux_rate ) { <nl> /* we increase slightly the bitrate to take into account the <nl> headers . XXX : compute it exactly */ <nl> - bitrate += bitrate * 5 / 100 ; <nl> + bitrate += bitrate / 20 ; <nl> bitrate += 10000 ; <nl> s -> mux_rate = ( bitrate + ( 8 * 50 ) - 1 ) / ( 8 * 50 ); <nl> }
static int h264_slice_header_parse ( const H264Context * h , H264SliceContext * sl , <nl> } <nl>  <nl> sl -> last_qscale_diff = 0 ; <nl> - tmp = pps -> init_qp + get_se_golomb (& sl -> gb ); <nl> + tmp = pps -> init_qp + ( unsigned ) get_se_golomb (& sl -> gb ); <nl> if ( tmp > 51 + 6 * ( sps -> bit_depth_luma - 8 )) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " QP % u out of range \ n ", tmp ); <nl> return AVERROR_INVALIDDATA ;
static int rtsp_read_packet ( AVFormatContext * s , <nl> case RTSP_PROTOCOL_RTP_UDP : <nl> case RTSP_PROTOCOL_RTP_UDP_MULTICAST : <nl> len = udp_read_packet ( s , & rtsp_st , buf , sizeof ( buf )); <nl> - if ( rtsp_st -> rtp_ctx ) <nl> + if ( len >= 0 && rtsp_st -> rtp_ctx ) <nl> rtp_check_and_send_back_rr ( rtsp_st -> rtp_ctx , len ); <nl> break ; <nl> }
int32_t ff_mlp_pack_output ( int32_t lossless_check_data , <nl> ( 1U << output_shift [ mat_ch ]); <nl> lossless_check_data ^= ( sample & 0xffffff ) << mat_ch ; <nl> if ( is32 ) <nl> - * data_32 ++ = sample * 256 ; <nl> + * data_32 ++ = sample * 256U ; <nl> else <nl> * data_16 ++ = sample >> 8 ; <nl> }
resync : <nl> } <nl> ast -> frame_offset += get_duration ( ast , pkt -> size ); <nl> } <nl> - ast -> remaining -= size ; <nl> + ast -> remaining -= err ; <nl> if (! ast -> remaining ){ <nl> avi -> stream_index = - 1 ; <nl> ast -> packet_size = 0 ;
static av_always_inline int coeff_abs_level_remaining_decode ( HEVCContext * s , int <nl> } else { <nl> int prefix_minus3 = prefix - 3 ; <nl>  <nl> - if ( prefix == CABAC_MAX_BIN ) { <nl> + if ( prefix == CABAC_MAX_BIN || prefix_minus3 + rc_rice_param >= 31 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " CABAC_MAX_BIN : % d \ n ", prefix ); <nl> return 0 ; <nl> }
SwsFunc ff_yuv2rgb_get_func_ptr ( SwsContext * c ) <nl> } <nl>  <nl> static void fill_table ( uint8_t * table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , <nl> - const int inc , void * y_tab ) <nl> + const int64_t inc , void * y_tab ) <nl> { <nl> int i ; <nl> uint8_t * y_table = y_tab ;
void av_image_copy ( uint8_t * dst_data [ 4 ], int dst_linesizes [ 4 ], <nl> for ( i = 0 ; i < planes_nb ; i ++) { <nl> int h = height ; <nl> int bwidth = av_image_get_linesize ( pix_fmt , width , i ); <nl> + if ( bwidth < 0 ) { <nl> + av_log ( 0 , AV_LOG_ERROR , " av_image_get_linesize failed \ n "); <nl> + return ; <nl> + } <nl> if ( i == 1 || i == 2 ) { <nl> h = -((- height )>> desc -> log2_chroma_h ); <nl> }
int ff_dirac_golomb_read_32bit ( DiracGolombLUT * lut_ctx , const uint8_t * buf , <nl> if (( c_idx + 1 ) > coeffs ) <nl> return c_idx ; <nl>  <nl> + if ( res_bits >= RSIZE_BITS ) <nl> + res_bits = res = 0 ; <nl> + <nl> /* res_bits is a hint for better branch prediction */ <nl> if ( res_bits && l -> sign ) { <nl> int32_t coeff = 1 ;
static void start_frame ( AVFilterLink * inlink , AVFilterBufferRef * inpicref ) <nl> ) <nl> break ; <nl> } <nl> - pad -> needs_copy = plane < 4 && outpicref -> data [ plane ]; <nl> + pad -> needs_copy = plane < 4 && outpicref -> data [ plane ] || !( outpicref -> perms & AV_PERM_WRITE ); <nl> if ( pad -> needs_copy ){ <nl> av_log ( inlink -> dst , AV_LOG_DEBUG , " Direct padding impossible allocating new frame \ n "); <nl> avfilter_unref_buffer ( outpicref );
int ff_hevc_decode_nal_pps ( HEVCContext * s ) <nl> int pps_range_extensions_flag = get_bits1 ( gb ); <nl> /* int pps_extension_7bits = */ get_bits ( gb , 7 ); <nl> if ( sps -> ptl . general_ptl . profile_idc == FF_PROFILE_HEVC_REXT && pps_range_extensions_flag ) { <nl> - pps_range_extensions ( s , pps , sps ); <nl> + if (( ret = pps_range_extensions ( s , pps , sps )) < 0 ) <nl> + goto err ; <nl> } <nl> } <nl> 
static inline int put_bits_count ( PutBitContext * s ) <nl> static inline void flush_put_bits ( PutBitContext * s ) <nl> { <nl> # ifndef BITSTREAM_WRITER_LE <nl> - s -> bit_buf <<= s -> bit_left ; <nl> + if ( s -> bit_left < 32 ) <nl> + s -> bit_buf <<= s -> bit_left ; <nl> # endif <nl> while ( s -> bit_left < 32 ) { <nl> /* XXX : should test end of buffer */
void av_packet_free ( AVPacket ** pkt ) <nl> static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> { <nl> int ret ; <nl> - if (( unsigned ) size >= ( unsigned ) size + AV_INPUT_BUFFER_PADDING_SIZE ) <nl> + if ( size < 0 || size >= INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> ret = av_buffer_realloc ( buf , size + AV_INPUT_BUFFER_PADDING_SIZE );
static int decode_channel_residues ( WmallDecodeCtx * s , int ch , int tile_size ) <nl> residue = quo ; <nl> else { <nl> rem_bits = av_ceil_log2 ( ave_mean ); <nl> - rem = rem_bits ? get_bits (& s -> gb , rem_bits ) : 0 ; <nl> + rem = rem_bits ? get_bits_long (& s -> gb , rem_bits ) : 0 ; <nl> residue = ( quo << rem_bits ) + rem ; <nl> } <nl> 
int av_write_frame ( AVFormatContext * s , AVPacket * pkt ) <nl> return 1 ; <nl> } <nl>  <nl> + ret = do_packet_auto_bsf ( s , pkt ); <nl> + if ( ret <= 0 ) <nl> + return ret ; <nl> + <nl> # if FF_API_COMPUTE_PKT_FIELDS2 && FF_API_LAVF_AVCTX <nl> ret = compute_muxer_pkt_fields ( s , s -> streams [ pkt -> stream_index ], pkt ); <nl> 
static int xwma_read_header ( AVFormatContext * s ) <nl>  <nl> /* Estimate the duration from the total number of output bytes . */ <nl> const uint64_t total_decoded_bytes = dpds_table [ dpds_table_size - 1 ]; <nl> + <nl> + if (! bytes_per_sample ) { <nl> + av_log ( s , AV_LOG_ERROR , " bytes_per_sample is 0 \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> st -> duration = total_decoded_bytes / bytes_per_sample ; <nl>  <nl> /* Use the dpds data to build a seek table . We can only do this after
static int decode_dvd_subtitles ( DVDSubContext * ctx , AVSubtitle * sub_header , <nl> w = x2 - x1 + 1 ; <nl> if ( w < 0 ) <nl> w = 0 ; <nl> - h = y2 - y1 ; <nl> + h = y2 - y1 + 1 ; <nl> if ( h < 0 ) <nl> h = 0 ; <nl> if ( w > 0 && h > 0 ) {
static inline int decode_subframe ( FLACContext * s , int channel ) <nl> if ( wasted ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - decoded [ i ] <<= wasted ; <nl> + decoded [ i ] = ( unsigned ) decoded [ i ] << wasted ; <nl> } <nl>  <nl> return 0 ;
static int huf_uncompress ( GetByteContext * gb , <nl>  <nl> fail : <nl> for ( i = 0 ; i < HUF_DECSIZE ; i ++) { <nl> - if ( hdec [ i ]. p ) <nl> + if ( hdec ) <nl> av_freep (& hdec [ i ]. p ); <nl> } <nl> 
static int asf_write_header ( AVFormatContext * s ) <nl> * It is needed to use asf as a streamable format . */ <nl> if ( asf_write_header1 ( s , 0 , DATA_HEADER_SIZE ) < 0 ) { <nl> // av_free ( asf ); <nl> + av_freep (& asf -> index_ptr ); <nl> return - 1 ; <nl> } <nl> 
static int cin_read_frame_header ( CinDemuxContext * cin , AVIOContext * pb ) { <nl>  <nl> if ( avio_rl32 ( pb ) != 0xAA55AA55 ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( hdr -> video_frame_size < 0 || hdr -> audio_frame_size < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> return 0 ; <nl> }
static int vp9_raw_reorder_make_output ( AVBSFContext * bsf , <nl> "(%" PRId64 ") from slot % d .\ n ", <nl> frame -> sequence , frame -> pts , s ); <nl>  <nl> - frame -> packet = av_packet_alloc (); <nl> - if (! frame -> packet ) <nl> - return AVERROR ( ENOMEM ); <nl> - <nl> err = av_new_packet ( out , 2 ); <nl> if ( err < 0 ) <nl> return err ;
static int vqf_read_seek ( AVFormatContext * s , <nl> { <nl> VqfContext * c = s -> priv_data ; <nl> AVStream * st ; <nl> - int ret ; <nl> + int64_t ret ; <nl> int64_t pos ; <nl>  <nl> st = s -> streams [ stream_index ];
static int fdk_aac_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size , 0 ); <nl> + err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size / sizeof ( INT_PCM ), 0 ); <nl> if ( err == AAC_DEC_NOT_ENOUGH_BITS ) { <nl> ret = avpkt -> size - valid ; <nl> goto end ;
static void png_handle_row ( PNGDecContext * s ) <nl> } <nl> s -> y ++; <nl> if ( s -> y == s -> height ) { <nl> + memset ( s -> last_row , 0 , s -> row_size ); <nl> for (;;) { <nl> if ( s -> pass == NB_PASSES - 1 ) { <nl> s -> state |= PNG_ALLIMAGE ;
static int skeleton_header ( AVFormatContext * s , int idx ) <nl> start_num = AV_RL64 ( buf + 12 ); <nl> start_den = AV_RL64 ( buf + 20 ); <nl>  <nl> - if ( start_den ) { <nl> + if ( start_den > 0 && start_num > 0 ) { <nl> int base_den ; <nl> av_reduce (& start_time , & base_den , start_num , start_den , INT_MAX ); <nl> avpriv_set_pts_info ( st , 64 , 1 , base_den );
static void mpeg_er_decode_mb ( void * opaque , int ref , int mv_dir , int mv_type , <nl> s -> mb_skipped = mb_skipped ; <nl> s -> mb_x = mb_x ; <nl> s -> mb_y = mb_y ; <nl> + s -> mcsel = 0 ; <nl> memcpy ( s -> mv , mv , sizeof (* mv )); <nl>  <nl> ff_init_block_index ( s );
static int raw_decode ( AVCodecContext * avctx , <nl> AVFrame * frame = ( AVFrame *) data ; <nl> AVPicture * picture = ( AVPicture *) data ; <nl>  <nl> + frame -> pict_type = avctx -> coded_frame -> pict_type ; <nl> frame -> interlaced_frame = avctx -> coded_frame -> interlaced_frame ; <nl> frame -> top_field_first = avctx -> coded_frame -> top_field_first ; <nl> frame -> reordered_opaque = avctx -> reordered_opaque ;
static int film_read_header ( AVFormatContext * s ) <nl> film -> audio_samplerate = AV_RB16 (& scratch [ 24 ]); <nl> film -> audio_channels = scratch [ 21 ]; <nl> film -> audio_bits = scratch [ 22 ]; <nl> - if ( scratch [ 23 ] == 2 ) <nl> + if ( scratch [ 23 ] == 2 && film -> audio_channels > 0 ) <nl> film -> audio_type = AV_CODEC_ID_ADPCM_ADX ; <nl> else if ( film -> audio_channels > 0 ) { <nl> if ( film -> audio_bits == 8 )
static int read_header ( ShortenContext * s ) <nl> s -> channels = get_uint ( s , CHANSIZE ); <nl> if ( s -> channels <= 0 || s -> channels > MAX_CHANNELS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + s -> channels = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> s -> avctx -> channels = s -> channels ;
static int cmv_decode_frame ( AVCodecContext * avctx , <nl> CmvContext * s = avctx -> priv_data ; <nl> const uint8_t * buf_end = buf + buf_size ; <nl>  <nl> + if ( buf_end - buf < EA_PREAMBLE_SIZE ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( AV_RL32 ( buf )== MVIh_TAG || AV_RB32 ( buf )== MVIh_TAG ) { <nl> cmv_process_header ( s , buf + EA_PREAMBLE_SIZE , buf_end ); <nl> return buf_size ;
typedef struct TargaContext { <nl> } TargaContext ; <nl>  <nl> # define CHECK_BUFFER_SIZE ( buf , buf_end , needed , where ) \ <nl> - if ( buf + needed > buf_end ){ \ <nl> + if ( needed > buf_end - buf ){ \ <nl> av_log ( avctx , AV_LOG_ERROR , " Problem : unexpected end of data while reading " where "\ n "); \ <nl> return - 1 ; \ <nl> } \
static int ape_tag_read_field ( AVFormatContext * s ) <nl> if (! value ) <nl> return AVERROR ( ENOMEM ); <nl> c = avio_read ( pb , value , size ); <nl> + if ( c < 0 ) <nl> + return c ; <nl> value [ c ] = 0 ; <nl> av_dict_set (& s -> metadata , key , value , AV_DICT_DONT_STRDUP_VAL ); <nl> }
void decode_mb_mode ( VP8Context * s , VP8Macroblock * mb , int mb_x , int mb_y , uint8_ <nl>  <nl> if ( s -> segmentation . update_map ) <nl> * segment = vp8_rac_get_tree ( c , vp8_segmentid_tree , s -> prob -> segmentid ); <nl> - else <nl> + else if ( s -> segmentation . enabled ) <nl> * segment = ref ? * ref : * segment ; <nl> s -> segment = * segment ; <nl> 
static int dca_exss_parse_asset_header ( DCAContext * s ) <nl> { <nl> int header_pos = get_bits_count (& s -> gb ); <nl> int header_size ; <nl> - int channels ; <nl> + int channels = 0 ; <nl> int embedded_stereo = 0 ; <nl> int embedded_6ch = 0 ; <nl> int drc_code_present ; <nl> - int extensions_mask ; <nl> + int av_uninit ( extensions_mask ); <nl> int i , j ; <nl>  <nl> if ( get_bits_left (& s -> gb ) < 16 )
static int mkv_parse_video_projection ( AVStream * st , const MatroskaTrack * track ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> break ; <nl> + case MATROSKA_VIDEO_PROJECTION_TYPE_RECTANGULAR : <nl> + /* No Spherical metadata */ <nl> + return 0 ; <nl> default : <nl> av_log ( NULL , AV_LOG_WARNING , <nl> " Unknown spherical metadata type %" PRIu64 "\ n ",
static int film_probe ( AVProbeData * p ) <nl> if ( AV_RB32 (& p -> buf [ 0 ]) != FILM_TAG ) <nl> return 0 ; <nl>  <nl> + if ( AV_RB32 (& p -> buf [ 16 ]) != FDSC_TAG ) <nl> + return 0 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
static int read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( size > 0 ) { <nl> - if ( pos + size < pos ) <nl> + if ( pos > INT64_MAX - size ) <nl> return AVERROR_INVALIDDATA ; <nl> avio_skip ( pb , FFMAX ( 0 , pos + size - avio_tell ( pb ))); <nl> }
end : <nl>  <nl> return ret ; <nl> free_and_end : <nl> - if ( avctx -> codec && <nl> + if ( avctx -> codec && avctx -> codec -> close && <nl> ( codec_init_ok || <nl> ( avctx -> codec -> caps_internal & FF_CODEC_CAP_INIT_CLEANUP ))) <nl> avctx -> codec -> close ( avctx );
static int expand_rle_row ( SgiState * s , uint8_t * out_buf , <nl> } <nl>  <nl> /* Check for buffer overflow . */ <nl> - if ( out_buf + pixelstride * ( count - 1 ) >= out_end ) <nl> + if ( out_end - out_buf <= pixelstride * ( count - 1 )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if ( pixel & 0x80 ) {
static int ff_asf_parse_packet ( AVFormatContext * s , AVIOContext * pb , AVPacket * pk <nl> // printf (" packet % d % d \ n ", asf_st -> pkt . size , asf -> packet_frag_size ); <nl> asf_st -> pkt . size = 0 ; <nl> asf_st -> pkt . data = 0 ; <nl> + asf_st -> pkt . side_data_elems = 0 ; <nl> + asf_st -> pkt . side_data = NULL ; <nl> break ; // packet completed <nl> } <nl> }
static int pcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR ( EINVAL ); <nl> } <nl>  <nl> + if ( avctx -> channels == 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> n = avctx -> channels * sample_size ; <nl>  <nl> if ( n && buf_size % n ) {
static int sol_read_packet ( AVFormatContext * s , <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR ( EIO ); <nl> ret = av_get_packet ( s -> pb , pkt , MAX_SIZE ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> pkt -> stream_index = 0 ; <nl>  <nl> /* note : we need to modify the packet size here to handle the last
static int iff_read_header ( AVFormatContext * s ) <nl> break ; <nl>  <nl> case ID_CMAP : <nl> + if ( data_size < 3 || data_size > 768 || data_size % 3 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid CMAP chunk size % d \ n ", <nl> + data_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> st -> codec -> extradata_size = data_size ; <nl> st -> codec -> extradata = av_malloc ( data_size ); <nl> if (! st -> codec -> extradata )
int main ( int argc , char * argv []) <nl> end : <nl> avformat_close_input (& fmt_ctx ); <nl> /* note : the internal buffer could have changed , and be != avio_ctx_buffer */ <nl> - av_freep (& avio_ctx -> buffer ); <nl> - av_freep (& avio_ctx ); <nl> + if ( avio_ctx ) { <nl> + av_freep (& avio_ctx -> buffer ); <nl> + av_freep (& avio_ctx ); <nl> + } <nl> av_file_unmap ( buffer , buffer_size ); <nl>  <nl> if ( ret < 0 ) {
rdt_new_context ( void ) <nl> { <nl> PayloadContext * rdt = av_mallocz ( sizeof ( PayloadContext )); <nl>  <nl> - avformat_open_input (& rdt -> rmctx , "", & ff_rdt_demuxer , NULL ); <nl> + int ret = avformat_open_input (& rdt -> rmctx , "", & ff_rdt_demuxer , NULL ); <nl> + if ( ret < 0 ) { <nl> + av_free ( rdt ); <nl> + return NULL ; <nl> + } <nl>  <nl> return rdt ; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> if (! fifo -> root . next ) { <nl> if (( ret = ff_request_frame ( outlink -> src -> inputs [ 0 ])) < 0 ) <nl> return ret ; <nl> + av_assert0 ( fifo -> root . next ); <nl> } <nl>  <nl> /* by doing this , we give ownership of the reference to the next filter ,
static av_cold int ulti_decode_init ( AVCodecContext * avctx ) <nl> s -> width = avctx -> width ; <nl> s -> height = avctx -> height ; <nl> s -> blocks = ( s -> width / 8 ) * ( s -> height / 8 ); <nl> + if ( s -> blocks == 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV410P ; <nl> s -> ulti_codebook = ulti_codebook ; <nl> 
static int decode_frame_png ( AVCodecContext * avctx , <nl> } <nl>  <nl> if (( ret = av_frame_ref ( data , s -> picture . f )) < 0 ) <nl> - return ret ; <nl> + goto the_end ; <nl>  <nl> * got_frame = 1 ; <nl> 
static int ape_tag_read_field ( AVFormatContext * s ) <nl> { <nl> AVIOContext * pb = s -> pb ; <nl> uint8_t key [ 1024 ], * value ; <nl> - uint32_t size , flags ; <nl> + int64_t size , flags ; <nl> int i , c ; <nl>  <nl> size = avio_rl32 ( pb ); /* field size */
static int thp_read_packet ( AVFormatContext * s , <nl> thp -> frame ++; <nl>  <nl> ret = av_get_packet ( pb , pkt , size ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( ret != size ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO );
static int transcode_init ( void ) <nl> ost -> filter -> filter -> inputs [ 0 ]-> sample_aspect_ratio ; <nl> codec -> pix_fmt = ost -> filter -> filter -> inputs [ 0 ]-> format ; <nl>  <nl> - if ( codec -> width != icodec -> width || <nl> + if (! icodec || <nl> + codec -> width != icodec -> width || <nl> codec -> height != icodec -> height || <nl> codec -> pix_fmt != icodec -> pix_fmt ) { <nl> codec -> bits_per_raw_sample = frame_bits_per_raw_sample ;
static void hScale16_c ( SwsContext * c , int16_t * _dst , int dstW , const uint8_t * _s <nl> for ( i = 0 ; i < dstW ; i ++) { <nl> int j ; <nl> int srcPos = filterPos [ i ]; <nl> - unsigned int val = 0 ; <nl> + int val = 0 ; <nl>  <nl> for ( j = 0 ; j < filterSize ; j ++) { <nl> val += src [ srcPos + j ] * filter [ filterSize * i + j ];
int ff_img_read_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> } <nl>  <nl> if ( codec -> codec_id == AV_CODEC_ID_NONE ) { <nl> - AVProbeData pd ; <nl> + AVProbeData pd = { 0 }; <nl> AVInputFormat * ifmt ; <nl> uint8_t header [ PROBE_BUF_MIN + AVPROBE_PADDING_SIZE ]; <nl> int ret ;
static int lrc_read_header ( AVFormatContext * s ) <nl> } <nl> ff_subtitles_queue_finalize ( s , & lrc -> q ); <nl> ff_metadata_conv_ctx ( s , NULL , ff_lrc_metadata_conv ); <nl> + av_bprint_finalize (& line , NULL ); <nl> return 0 ; <nl> } <nl> 
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels == 0 || channels > 64 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
static int libopus_encode ( AVCodecContext * avctx , AVPacket * avpkt , <nl> int ret ; <nl>  <nl> if ( frame ) { <nl> - ff_af_queue_add (& opus -> afq , frame ); <nl> + ret = ff_af_queue_add (& opus -> afq , frame ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( frame -> nb_samples < opus -> opts . packet_size ) { <nl> audio = opus -> samples ; <nl> memcpy ( audio , frame -> data [ 0 ], frame -> nb_samples * sample_size );
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> ff_dv_rl_vlc [ i ]. run = run ; <nl> } <nl> ff_free_vlc (& dv_vlc ); <nl> - <nl> - dv_vlc_map_tableinit (); <nl> } <nl>  <nl> /* Generic DSP setup */ <nl> static av_cold int dvvideo_init_encoder ( AVCodecContext * avctx ) <nl> return AVERROR ( EINVAL ); <nl> } <nl>  <nl> + dv_vlc_map_tableinit (); <nl> + <nl> return ff_dvvideo_init ( avctx ); <nl> } <nl> 
static void write_frame ( AVFormatContext * s , AVPacket * pkt , OutputStream * ost ) <nl> * reordering , see do_video_out () <nl> */ <nl> if (!( avctx -> codec_type == AVMEDIA_TYPE_VIDEO && avctx -> codec )) { <nl> - if ( ost -> frame_number >= ost -> max_frames ) <nl> + if ( ost -> frame_number >= ost -> max_frames ) { <nl> + av_free_packet ( pkt ); <nl> return ; <nl> + } <nl> ost -> frame_number ++; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> + p -> key_frame = 1 ; <nl> * got_frame = 1 ; <nl>  <nl> return buf_size ;
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> int i , t , seekd ; <nl> GetBitContext gb ; <nl>  <nl> + if ( s -> nb_streams <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " cannot parse stream table before stream header \ n "); <nl> + return ; <nl> + } <nl> + <nl> avio_seek ( s -> pb , off , SEEK_SET ); <nl> mpc8_get_chunk_header ( s -> pb , & tag , & size ); <nl> if ( tag != TAG_SEEKTABLE ){
static int dvbsub_decode ( AVCodecContext * avctx , <nl> break ; <nl> case DVBSUB_DISPLAYDEFINITION_SEGMENT : <nl> dvbsub_parse_display_definition_segment ( avctx , p , segment_length ); <nl> + break ; <nl> case DVBSUB_DISPLAY_SEGMENT : <nl> * data_size = dvbsub_display_end_segment ( avctx , p , segment_length , sub ); <nl> break ;
static int encode_mode ( CinepakEncContext * s , int h , AVPicture * scratch_pict , AVP <nl> int needs_extra_bit , should_write_temp ; <nl> unsigned char temp [ 64 ]; // 32 / 2 = 16 V4 blocks at 4 B each -> 64 B <nl> mb_info * mb ; <nl> - AVPicture sub_scratch , sub_last ; <nl> + AVPicture sub_scratch = {{ 0 }}, sub_last = {{ 0 }}; <nl>  <nl> // encode codebooks <nl> ////// MacOS vintage decoder compatibility dictates the presence of
static void write_section_data ( MpegTSContext * ts , MpegTSFilter * tss1 , <nl> } else <nl> crc_valid = 2 ; <nl> } <nl> - if ( crc_valid ) <nl> + if ( crc_valid ) { <nl> tss -> section_cb ( tss1 , tss -> section_buf , tss -> section_h_size ); <nl> + if ( crc_valid != 1 ) <nl> + tss -> last_ver = - 1 ; <nl> + } <nl> } <nl> } <nl> 
static int sap_write_header ( AVFormatContext * s ) <nl> freeaddrinfo ( ai ); <nl> } <nl>  <nl> - contexts = av_mallocz ( sizeof ( AVFormatContext *) * s -> nb_streams ); <nl> + contexts = av_mallocz_array ( s -> nb_streams , sizeof ( AVFormatContext *)); <nl> if (! contexts ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ;
static int64_t mxf_essence_container_end ( MXFContext * mxf , int body_sid ) <nl> static int mxf_edit_unit_absolute_offset ( MXFContext * mxf , MXFIndexTable * index_table , int64_t edit_unit , int64_t * edit_unit_out , int64_t * offset_out , int nag ) <nl> { <nl> int i ; <nl> - int offset_temp = 0 ; <nl> + int64_t offset_temp = 0 ; <nl>  <nl> for ( i = 0 ; i < index_table -> nb_segments ; i ++) { <nl> MXFIndexTableSegment * s = index_table -> segments [ i ];
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl>  <nl> if ( wc -> ch_offset + s -> stereo >= avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_WARNING , " Too many channels coded in a packet .\ n "); <nl> - return ( avctx -> err_recognition & AV_EF_EXPLODE ) ? AVERROR_INVALIDDATA : 0 ; <nl> + return (( avctx -> err_recognition & AV_EF_EXPLODE ) || ! wc -> ch_offset ) ? AVERROR_INVALIDDATA : 0 ; <nl> } <nl>  <nl> samples_l = frame -> extended_data [ wc -> ch_offset ];
static int normalize_bits ( int num , int width ) <nl> if ( num < 0 ) <nl> num = ~ num ; <nl>  <nl> - return width - av_log2 ( num ); <nl> + return width - av_log2 ( num ) - 1 ; <nl> } <nl>  <nl> /**
static int au_read_header ( AVFormatContext * s ) <nl> st -> codec -> channels = channels ; <nl> st -> codec -> sample_rate = rate ; <nl> if ( data_size != AU_UNKNOWN_SIZE ) <nl> - st -> duration = ((( int64_t ) data_size )<< 3 ) / ( st -> codec -> channels * bps ); <nl> + st -> duration = ((( int64_t ) data_size )<< 3 ) / ( st -> codec -> channels * ( int64_t ) bps ); <nl> avpriv_set_pts_info ( st , 64 , 1 , rate ); <nl> return 0 ; <nl> }
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> uint32_t u ; <nl> } value ; <nl>  <nl> - int sign ; <nl> + unsigned int sign ; <nl> int exp = s -> float_max_exp ; <nl>  <nl> if ( s -> got_extra_bits ) {
fail : <nl> fail_at_end : <nl> av_freep (& pic -> codec_picture_params ); <nl> av_frame_free (& pic -> recon_image ); <nl> + av_buffer_unref (& pic -> output_buffer_ref ); <nl> + pic -> output_buffer = VA_INVALID_ID ; <nl> return err ; <nl> } <nl> 
static int opt_show_format_entry ( void * optctx , const char * opt , const char * arg ) <nl> char * buf = av_asprintf (" format =% s ", arg ); <nl> int ret ; <nl>  <nl> + if (! buf ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> av_log ( NULL , AV_LOG_WARNING , <nl> " Option '% s ' is deprecated , use '- show_entries format =% s ' instead \ n ", <nl> opt , arg );
static int mpegts_push_data ( MpegTSFilter * filter , <nl> pes -> st -> request_probe = 1 ; <nl> } <nl> } else { <nl> + pes -> pes_header_size = 6 ; <nl> pes -> state = MPEGTS_PAYLOAD ; <nl> pes -> data_index = 0 ; <nl> }
static int libshine_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> SHINEContext * s = avctx -> priv_data ; <nl> MPADecodeHeader hdr ; <nl> unsigned char * data ; <nl> - long written ; <nl> + int written ; <nl> int ret , len ; <nl>  <nl> if ( frame )
static int svq1_decode_frame ( AVCodecContext * avctx , <nl> if ( s -> f_code != 0x20 ) { <nl> uint32_t * src = ( uint32_t *) ( buf + 4 ); <nl>  <nl> + if ( buf_size < 36 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> src [ i ] = (( src [ i ] << 16 ) | ( src [ i ] >> 16 )) ^ src [ 7 - i ]; <nl> }
static int decode_frame_packing ( H264Context * h , int size ){ <nl>  <nl> int ff_h264_decode_sei ( H264Context * h ){ <nl> while ( get_bits_left (& h -> gb ) > 16 ) { <nl> - int size , type ; <nl> + int type ; <nl> + unsigned size ; <nl>  <nl> type = 0 ; <nl> do {
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> } <nl> } <nl> total_size += 8 ; <nl> - if ( a . size == 1 ) { /* 64 bit extended size */ <nl> + if ( a . size == 1 && total_size + 8 <= atom . size ) { /* 64 bit extended size */ <nl> a . size = avio_rb64 ( pb ) - 8 ; <nl> total_size += 8 ; <nl> }
static AVStream * init_stream ( AVFormatContext * s ) <nl> avpriv_set_pts_info ( st , 60 , bin -> framerate . den , bin -> framerate . num ); <nl>  <nl> /* simulate tty display speed */ <nl> - bin -> chars_per_frame = FFMAX ( av_q2d ( st -> time_base ) * bin -> chars_per_frame , 1 ); <nl> + bin -> chars_per_frame = av_clip ( av_q2d ( st -> time_base ) * bin -> chars_per_frame , 1 , INT_MAX ); <nl>  <nl> return st ; <nl> }
av_cold int ff_snow_common_init ( AVCodecContext * avctx ){ <nl>  <nl> s -> avctx = avctx ; <nl> s -> max_ref_frames = 1 ; // just make sure it ' s not an invalid value in case of no initial keyframe <nl> + s -> spatial_decomposition_count = 1 ; <nl>  <nl> ff_me_cmp_init (& s -> mecc , avctx ); <nl> ff_hpeldsp_init (& s -> hdsp , avctx -> flags );
static int load_ipmovie_packet ( IPMVEContext * s , AVIOContext * pb , <nl> int chunk_type ; <nl>  <nl> if ( s -> audio_chunk_offset ) { <nl> + if ( s -> audio_type == CODEC_ID_NONE ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " Can not read audio packet before " <nl> + " audio codec is known \ n "); <nl> + return CHUNK_BAD ; <nl> + } <nl>  <nl> /* adjust for PCM audio by skipping chunk header */ <nl> if ( s -> audio_type != CODEC_ID_INTERPLAY_DPCM ) {
static int probe ( AVProbeData * p ) <nl> offset = AV_RL32 ( p -> buf + 18 + i * 16 ); <nl> if ( offset < 22 ) <nl> return FFMIN ( i , AVPROBE_SCORE_MAX / 4 ); <nl> - if ( offset + 8 > p -> buf_size ) <nl> + if ( offset > p -> buf_size - 8 ) <nl> continue ; <nl> if ( p -> buf [ offset ] != 40 && AV_RB64 ( p -> buf + offset ) != PNGSIG ) <nl> return FFMIN ( i , AVPROBE_SCORE_MAX / 4 );
int attribute_align_arg avresample_convert ( AVAudioResampleContext * avr , <nl> resample_out = & output_buffer ; <nl> else <nl> resample_out = avr -> resample_out_buffer ; <nl> - av_dlog ( avr , "[ resample ] % s to % s \ n ", current_buffer -> name , <nl> + av_dlog ( avr , "[ resample ] % s to % s \ n ", <nl> + current_buffer ? current_buffer -> name : " null ", <nl> resample_out -> name ); <nl> ret = ff_audio_resample ( avr -> resample , resample_out , <nl> current_buffer );
int ff_get_cpu_flags_x86 ( void ) <nl>  <nl> if ( max_ext_level >= 0x80000001 ){ <nl> cpuid ( 0x80000001 , eax , ebx , ecx , ext_caps ); <nl> - if ( ext_caps & ( 1 << 31 )) <nl> + if ( ext_caps & ( 1U << 31 )) <nl> rval |= AV_CPU_FLAG_3DNOW ; <nl> if ( ext_caps & ( 1 << 30 )) <nl> rval |= AV_CPU_FLAG_3DNOWEXT ;
static int old_codec47 ( SANMVideoContext * ctx , int top , <nl> if ( bytestream2_get_bytes_left (& ctx -> gb ) < width * height ) <nl> return AVERROR_INVALIDDATA ; <nl> for ( j = 0 ; j < height ; j ++) { <nl> - for ( i = 0 ; i < width ; i ++) <nl> - bytestream2_get_bufferu (& ctx -> gb , dst , width ); <nl> + bytestream2_get_bufferu (& ctx -> gb , dst , width ); <nl> dst += stride ; <nl> } <nl> break ;
static int filter_frame ( AVFilterLink * inlink , AVFrame * inpicref ) <nl>  <nl> inpicref -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> ret = ff_filter_frame ( outlink , inpicref ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_frame_free (& second ); <nl> return ret ; <nl> + } <nl>  <nl> second -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> return ff_filter_frame ( outlink , second );
static int rtp_mpegts_write_header ( AVFormatContext * s ) <nl> } <nl> rtp_ctx -> oformat = rtp_format ; <nl> st = avformat_new_stream ( rtp_ctx , NULL ); <nl> + if (! st ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl> st -> time_base . num = 1 ; <nl> st -> time_base . den = 90000 ; <nl> st -> codecpar -> codec_id = AV_CODEC_ID_MPEG2TS ;
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> for ( ch = 1 ; ch <= s -> channels ; ch ++) { <nl> float gain = s -> mul_bias / 4194304 . 0f ; <nl> if ( s -> channel_mode == AC3_CHMODE_DUALMONO ) { <nl> - gain *= s -> dynamic_range [ ch - 1 ]; <nl> + gain *= s -> dynamic_range [ 2 - ch ]; <nl> } else { <nl> gain *= s -> dynamic_range [ 0 ]; <nl> }
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int i ; <nl>  <nl> + x265_picture_init ( ctx -> params , & x265pic ); <nl> + <nl> if ( pic ) { <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> x265pic . planes [ i ] = pic -> data [ i ];
static int decode_pic_hdr ( IVI45DecContext * ctx , AVCodecContext * avctx ) <nl> /* skip picture header extension if any */ <nl> while ( get_bits1 (& ctx -> gb )) { <nl> ff_dlog ( avctx , " Pic hdr extension encountered !\ n "); <nl> + if ( get_bits_left (& ctx -> gb ) < 10 ) <nl> + return AVERROR_INVALIDDATA ; <nl> skip_bits (& ctx -> gb , 8 ); <nl> } <nl> 
static int ac3_sync ( uint64_t state , AACAC3ParseContext * hdr_info , <nl> int err ; <nl> union { <nl> uint64_t u64 ; <nl> - uint8_t u8 [ 8 ]; <nl> + uint8_t u8 [ 8 + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> } tmp = { av_be2ne64 ( state ) }; <nl> AC3HeaderInfo hdr ; <nl> GetBitContext gbc ;
static int aac_sync ( uint64_t state , AACAC3ParseContext * hdr_info , <nl> int size ; <nl> union { <nl> uint64_t u64 ; <nl> - uint8_t u8 [ 8 ]; <nl> + uint8_t u8 [ 8 + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> } tmp ; <nl>  <nl> tmp . u64 = av_be2ne64 ( state );
static int read_quant_table ( RangeCoder * c , int16_t * quant_table , int scale ) <nl> memset ( state , 128 , sizeof ( state )); <nl>  <nl> for ( v = 0 ; i < 128 ; v ++) { <nl> - unsigned len = get_symbol ( c , state , 0 ) + 1 ; <nl> + unsigned len = get_symbol ( c , state , 0 ) + 1U ; <nl>  <nl> if ( len > 128 - i || ! len ) <nl> return AVERROR_INVALIDDATA ;
static av_cold int vc2_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> int64_t max_frame_bytes , r_bitrate = avctx -> bit_rate >> ( s -> interlaced ); <nl>  <nl> s -> avctx = avctx ; <nl> - s -> size_scaler = 1 ; <nl> + s -> size_scaler = 2 ; <nl> s -> prefix_bytes = 0 ; <nl> s -> last_parse_code = 0 ; <nl> s -> next_parse_offset = 0 ;
static int kempf_decode_tile ( G2MContext * c , int tile_x , int tile_y , <nl> src += 3 ; <nl> } <nl> npal = * src ++ + 1 ; <nl> + if ( src_end - src < npal * 3 ) <nl> + return AVERROR_INVALIDDATA ; <nl> memcpy ( pal , src , npal * 3 ); src += npal * 3 ; <nl> if ( sub_type != 2 ) { <nl> for ( i = 0 ; i < npal ; i ++) {
static int mov_read_udta_string ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> if (! key ) <nl> return 0 ; <nl> - if ( atom . size < 0 ) <nl> + if ( atom . size < 0 || str_size >= INT_MAX / 2 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> // worst - case requirement for output string in case of utf8 coded input
int ff_vaapi_render_picture ( struct vaapi_context * vactx , VASurfaceID surface ) <nl> VABufferID va_buffers [ 3 ]; <nl> unsigned int n_va_buffers = 0 ; <nl>  <nl> + if (! vactx -> pic_param_buf_id ) <nl> + return 0 ; <nl> + <nl> vaUnmapBuffer ( vactx -> display , vactx -> pic_param_buf_id ); <nl> va_buffers [ n_va_buffers ++] = vactx -> pic_param_buf_id ; <nl> 
static int decodeTonalComponents ( GetBitContext * gb , tonal_component * pComponent <nl>  <nl> for ( k = 0 ; k < coded_components ; k ++) { <nl> sfIndx = get_bits ( gb , 6 ); <nl> + if ( component_count >= 64 ) <nl> + return AVERROR_INVALIDDATA ; <nl> pComponent [ component_count ]. pos = j * 64 + ( get_bits ( gb , 6 )); <nl> max_coded_values = SAMPLES_PER_FRAME - pComponent [ component_count ]. pos ; <nl> coded_values = coded_values_per_component + 1 ;
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> if (! reslevel -> band ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> + if ( reslevel -> num_precincts_x * ( uint64_t ) reslevel -> num_precincts_y * reslevel -> nbands > avctx -> max_pixels / sizeof (* reslevel -> band -> prec )) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> for ( bandno = 0 ; bandno < reslevel -> nbands ; bandno ++, gbandno ++) { <nl> ret = init_band ( avctx , reslevel , <nl> comp , codsty , qntsty ,
static int lag_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if (! l -> rgb_planes ) { <nl> l -> rgb_stride = FFALIGN ( avctx -> width , 16 ); <nl> - l -> rgb_planes = av_malloc ( l -> rgb_stride * avctx -> height * planes ); <nl> + l -> rgb_planes = av_malloc ( l -> rgb_stride * avctx -> height * planes + 1 ); <nl> if (! l -> rgb_planes ) { <nl> av_log ( avctx , AV_LOG_ERROR , " cannot allocate temporary buffer \ n "); <nl> return AVERROR ( ENOMEM );
int av_find_stream_info ( AVFormatContext * ic ) <nl> } <nl>  <nl> pkt = add_to_pktbuf (& ic -> packet_buffer , & pkt1 ); <nl> - if ( av_dup_packet ( pkt ) < 0 ) <nl> + if ( av_dup_packet ( pkt ) < 0 ) { <nl> + av_free ( duration_error ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> read_size += pkt -> size ; <nl> 
static int read_frame ( BVID_DemuxContext * vid , AVIOContext * pb , AVPacket * pkt , <nl> if ( vid -> palette ) { <nl> uint8_t * pdata = av_packet_new_side_data ( pkt , AV_PKT_DATA_PALETTE , <nl> BVID_PALETTE_SIZE ); <nl> - memcpy ( pdata , vid -> palette , BVID_PALETTE_SIZE ); <nl> + if ( pdata ) <nl> + memcpy ( pdata , vid -> palette , BVID_PALETTE_SIZE ); <nl> av_freep (& vid -> palette ); <nl> } <nl> 
static int mov_write_header ( AVFormatContext * s ) <nl> else if (! TAG_IS_AVCI ( track -> tag )){ <nl> track -> vos_len = st -> codec -> extradata_size ; <nl> track -> vos_data = av_malloc ( track -> vos_len ); <nl> - if (! track -> vos_data ) <nl> + if (! track -> vos_data ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> goto error ; <nl> + } <nl> memcpy ( track -> vos_data , st -> codec -> extradata , track -> vos_len ); <nl> } <nl> }
static int ape_tag_read_field ( AVFormatContext * s ) <nl> if (! value ) <nl> return AVERROR ( ENOMEM ); <nl> c = avio_read ( pb , value , size ); <nl> - if ( c < 0 ) <nl> + if ( c < 0 ) { <nl> + av_free ( value ); <nl> return c ; <nl> + } <nl> value [ c ] = 0 ; <nl> av_dict_set (& s -> metadata , key , value , AV_DICT_DONT_STRDUP_VAL ); <nl> }
static int matroska_parse_frame ( MatroskaDemuxContext * matroska , <nl> /* XXX : prevent data copy ... */ <nl> if ( av_new_packet ( pkt , pkt_size + offset ) < 0 ) { <nl> av_free ( pkt ); <nl> + av_freep (& pkt_data ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
static inline int ff_fast_malloc ( void * ptr , unsigned int * size , size_t min_size , <nl> void ** p = ptr ; <nl> if ( min_size <= * size && * p ) <nl> return 0 ; <nl> - min_size = FFMAX ( 17 * min_size / 16 + 32 , min_size ); <nl> + min_size = FFMAX ( min_size + min_size / 16 + 32 , min_size ); <nl> av_free (* p ); <nl> * p = zero_realloc ? av_mallocz ( min_size ) : av_malloc ( min_size ); <nl> if (!* p )
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl>  <nl> display_matrix = ( uint32_t *) av_stream_get_side_data ( st , AV_PKT_DATA_DISPLAYMATRIX , <nl> & display_matrix_size ); <nl> - if ( display_matrix_size < 9 * sizeof (* display_matrix )) <nl> + if ( display_matrix && display_matrix_size < 9 * sizeof (* display_matrix )) <nl> display_matrix = NULL ; <nl> } <nl> 
static int opus_packet ( AVFormatContext * avf , int idx ) <nl>  <nl> if (! os -> psize ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( os -> granule > INT64_MAX - UINT32_MAX ) { <nl> + if ( os -> granule > ( 1LL << 62 )) { <nl> av_log ( avf , AV_LOG_ERROR , " Unsupported huge granule pos %" PRId64 "\ n ", os -> granule ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> bytestream2_init (& gbc , avpkt -> data , avpkt -> size ); <nl>  <nl> + /* PICT images start with a 512 bytes empty header */ <nl> + if ( bytestream2_peek_be32 (& gbc ) == 0 ) <nl> + bytestream2_skip (& gbc , 512 ); <nl> + <nl> /* smallest PICT header */ <nl> if ( bytestream2_get_bytes_left (& gbc ) < 40 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Frame is too small % d \ n ",
av_cold int avcodec_close ( AVCodecContext * avctx ) <nl> avctx -> codec -> close ( avctx ); <nl> avcodec_default_free_buffers ( avctx ); <nl> avctx -> coded_frame = NULL ; <nl> - if ( avctx -> codec -> priv_class ) <nl> + if ( avctx -> codec && avctx -> codec -> priv_class ) <nl> av_opt_free ( avctx -> priv_data ); <nl> av_opt_free ( avctx ); <nl> av_freep (& avctx -> priv_data );
int vp56_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> VP56Context * s = avctx -> priv_data ; <nl> AVFrame * const p = s -> framep [ VP56_FRAME_CURRENT ]; <nl> int remaining_buf_size = buf_size ; <nl> - int is_alpha , alpha_offset ; <nl> + int is_alpha , av_uninit ( alpha_offset ); <nl>  <nl> if ( s -> has_alpha ) { <nl> alpha_offset = bytestream_get_be24 (& buf );
ogg_gptopts ( AVFormatContext * s , int i , uint64_t gp , int64_t * dts ) <nl> if ( dts ) <nl> * dts = pts ; <nl> } <nl> + if ( pts > INT64_MAX && pts != AV_NOPTS_VALUE ) { <nl> + // The return type is unsigned , we thus cannot return negative pts <nl> + av_log ( s , AV_LOG_ERROR , " invalid pts %" PRId64 "\ n ", pts ); <nl> + pts = AV_NOPTS_VALUE ; <nl> + } <nl>  <nl> return pts ; <nl> }
void avformat_free_context ( AVFormatContext * s ) <nl> av_opt_free ( s ); <nl> if ( s -> iformat && s -> iformat -> priv_class && s -> priv_data ) <nl> av_opt_free ( s -> priv_data ); <nl> + if ( s -> oformat && s -> oformat -> priv_class && s -> priv_data ) <nl> + av_opt_free ( s -> priv_data ); <nl>  <nl> for ( i = s -> nb_streams - 1 ; i >= 0 ; i --) { <nl> ff_free_stream ( s , s -> streams [ i ]);
static void opt_output_file ( const char * filename ) <nl> video_enc -> rc_max_rate = video_rc_max_rate ; <nl> video_enc -> rc_min_rate = video_rc_min_rate ; <nl> video_enc -> rc_buffer_size = video_rc_buffer_size ; <nl> + video_enc -> rc_initial_buffer_occupancy = video_rc_buffer_size * 3 / 4 ; <nl> video_enc -> rc_buffer_aggressivity = video_rc_buffer_aggressivity ; <nl> video_enc -> rc_initial_cplx = video_rc_initial_cplx ; <nl> video_enc -> i_quant_factor = video_i_qfactor ;
static int libquvi_read_header ( AVFormatContext * s ) <nl> if ( rc != QUVI_OK ) <nl> goto quvi_fail ; <nl>  <nl> + if (!( qc -> fmtctx = avformat_alloc_context ())) <nl> + goto quvi_fail ; <nl> + <nl> if (( ret = ff_copy_whitelists ( qc -> fmtctx , s )) < 0 ) <nl> goto end ; <nl> 
static int join_request_frame ( AVFilterLink * outlink ) <nl>  <nl> ret = ff_filter_frame ( outlink , frame ); <nl>  <nl> - memset ( s -> input_frames , 0 , sizeof (* s -> input_frames ) * ctx -> nb_inputs ); <nl> + for ( i = 0 ; i < ctx -> nb_inputs ; i ++) <nl> + av_frame_free (& s -> input_frames [ i ]); <nl>  <nl> return ret ; <nl> 
static int probe_buf_write ( void * opaque , uint8_t * buf , int buf_size ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> int ret ; <nl> - uint8_t * buffer = av_malloc ( AVP_BUFFSIZE ); <nl> + uint8_t * buffer = av_mallocz ( AVP_BUFFSIZE ); <nl>  <nl> if (! buffer ) <nl> exit ( 1 );
retry : <nl> StreamInfo * stream = st -> priv_data ; <nl> const int avail_data = av_fifo_size ( stream -> fifo ); <nl> const int space = stream -> max_buffer_size - stream -> buffer_index ; <nl> - int rel_space = 1024 * space / stream -> max_buffer_size ; <nl> + int rel_space = 1024LL * space / stream -> max_buffer_size ; <nl> PacketDesc * next_pkt = stream -> premux_packet ; <nl>  <nl> /* for subtitle , a single PES packet must be generated ,
static int decode_frame ( AVCodecContext * avctx , <nl> buf += 5 ; <nl>  <nl> if ( video_size ) { <nl> - if ( video_size < 0 ) { <nl> + if ( video_size < 0 || video_size > buf_size ) { <nl> av_log ( avctx , AV_LOG_ERROR , " video size % d invalid \ n ", video_size ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int cllc_decode_frame ( AVCodecContext * avctx , void * data , <nl> coding_type = ( AV_RL32 ( src ) >> 8 ) & 0xFF ; <nl> av_log ( avctx , AV_LOG_DEBUG , " Frame coding type : % d \ n ", coding_type ); <nl>  <nl> + if ( get_bits_left (& gb ) < avctx -> height * avctx -> width ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> switch ( coding_type ) { <nl> case 0 : <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV422P ;
static int rtp_write_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> const uint8_t * mb_info = <nl> av_packet_get_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , <nl> & mb_info_size ); <nl> + if (! mb_info ) { <nl> + av_log ( s1 , AV_LOG_ERROR , " failed to allocate side data \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> ff_rtp_send_h263_rfc2190 ( s1 , pkt -> data , size , mb_info , mb_info_size ); <nl> break ; <nl> }
static int mov_read_header ( AVFormatContext * s ) <nl> } <nl> if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO && sc -> nb_frames_for_fps > 0 && sc -> duration_for_fps > 0 ) <nl> av_reduce (& st -> avg_frame_rate . num , & st -> avg_frame_rate . den , <nl> - sc -> time_scale * sc -> nb_frames_for_fps , sc -> duration_for_fps , INT_MAX ); <nl> + sc -> time_scale *( int64_t ) sc -> nb_frames_for_fps , sc -> duration_for_fps , INT_MAX ); <nl> } <nl>  <nl> if ( mov -> trex_data ) {
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> int lower_transport , const char * real_challenge ) <nl> { <nl> RTSPState * rt = s -> priv_data ; <nl> - int rtx , j , i , err , interleave = 0 ; <nl> + int rtx = 0 , j , i , err , interleave = 0 ; <nl> RTSPStream * rtsp_st ; <nl> RTSPMessageHeader reply1 , * reply = & reply1 ; <nl> char cmd [ 2048 ];
retry : <nl> es_size -= stream -> premux_packet -> unwritten_size ; <nl> stream -> premux_packet = stream -> premux_packet -> next ; <nl> } <nl> - if ( es_size ) <nl> + if ( stream -> premux_packet && es_size ) <nl> stream -> premux_packet -> unwritten_size -= es_size ; <nl>  <nl> if ( remove_decoded_packets ( ctx , s -> last_scr ) < 0 )
static int mp_decode_frame ( AVCodecContext * avctx , <nl> if ( sz == 0 ) <nl> goto end ; <nl>  <nl> + if ( mp -> max_codes_bits <= 0 ) <nl> + goto end ; <nl> if ( init_vlc (& mp -> vlc , mp -> max_codes_bits , mp -> codes_count , & mp -> codes [ 0 ]. size , sizeof ( HuffCode ), 1 , & mp -> codes [ 0 ]. code , sizeof ( HuffCode ), 4 , 0 )) <nl> goto end ; <nl> mp_decode_frame_helper ( mp , & gb );
static void output_packet ( OutputFile * of , AVPacket * pkt , OutputStream * ost ) <nl> if ( ost -> nb_bitstream_filters ) { <nl> int idx ; <nl>  <nl> + av_packet_split_side_data ( pkt ); <nl> ret = av_bsf_send_packet ( ost -> bsf_ctx [ 0 ], pkt ); <nl> if ( ret < 0 ) <nl> goto finish ;
static int crypto_open ( URLContext * h , const char * uri , int flags ) <nl>  <nl> return 0 ; <nl> err : <nl> - av_free ( c -> key ); <nl> - av_free ( c -> iv ); <nl> + av_freep (& c -> key ); <nl> + av_freep (& c -> iv ); <nl> return ret ; <nl> } <nl> 
int ff_h264_frame_start ( H264Context * h ) <nl>  <nl> if (( ret = alloc_picture ( h , pic )) < 0 ) <nl> return ret ; <nl> - if (! h -> sync ) <nl> + if (! h -> sync && ! h -> avctx -> hwaccel ) <nl> avpriv_color_frame (& pic -> f , c ); <nl>  <nl> h -> cur_pic_ptr = pic ;
AVCodec ff_mjpeg_encoder = { <nl> . type = AVMEDIA_TYPE_VIDEO , <nl> . id = AV_CODEC_ID_MJPEG , <nl> . priv_data_size = sizeof ( MpegEncContext ), <nl> - . priv_class = & mjpeg_class , <nl> . init = ff_mpv_encode_init , <nl> . encode2 = ff_mpv_encode_picture , <nl> . close = ff_mpv_encode_end ,
static int udp_close ( URLContext * h ) <nl> ret = pthread_join ( s -> circular_buffer_thread , NULL ); <nl> if ( ret != 0 ) <nl> av_log ( h , AV_LOG_ERROR , " pthread_join (): % s \ n ", strerror ( ret )); <nl> + pthread_mutex_destroy (& s -> mutex ); <nl> + pthread_cond_destroy (& s -> cond ); <nl> } <nl> - <nl> - pthread_mutex_destroy (& s -> mutex ); <nl> - pthread_cond_destroy (& s -> cond ); <nl> # endif <nl> av_fifo_free ( s -> fifo ); <nl> return 0 ;
void ff_h264_free_tables ( H264Context * h , int free_rbsp ) <nl> if ( free_rbsp && h -> DPB ) { <nl> for ( i = 0 ; i < H264_MAX_PICTURE_COUNT ; i ++) <nl> ff_h264_unref_picture ( h , & h -> DPB [ i ]); <nl> + memset ( h -> delayed_pic , 0 , sizeof ( h -> delayed_pic )); <nl> av_freep (& h -> DPB ); <nl> } else if ( h -> DPB ) { <nl> for ( i = 0 ; i < H264_MAX_PICTURE_COUNT ; i ++)
static int bmp_decode_frame ( AVCodecContext * avctx , <nl>  <nl> hsize = bytestream_get_le32 (& buf ); /* header size */ <nl> ihsize = bytestream_get_le32 (& buf ); /* more header size */ <nl> - if ( ihsize + 14 > hsize ) { <nl> + if ( ihsize + 14LL > hsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid header size % u \ n ", hsize ); <nl> return AVERROR_INVALIDDATA ; <nl> }
typedef struct VP9Context { <nl> DECLARE_ALIGNED ( 32 , int16_t , uvblock )[ 2 ][ 1024 ]; <nl> uint8_t eob [ 256 ]; <nl> uint8_t uveob [ 2 ][ 64 ]; <nl> - VP56mv min_mv , max_mv ; <nl> + struct { int x , y ; } min_mv , max_mv ; <nl> DECLARE_ALIGNED ( 32 , uint8_t , tmp_y )[ 64 * 64 ]; <nl> DECLARE_ALIGNED ( 32 , uint8_t , tmp_uv )[ 2 ][ 32 * 32 ]; <nl> } VP9Context ;
typedef struct WmallDecodeCtx { <nl>  <nl> int8_t mclms_order ; <nl> int8_t mclms_scaling ; <nl> - int16_t mclms_coeffs [ 128 ]; <nl> - int16_t mclms_coeffs_cur [ 4 ]; <nl> + int16_t mclms_coeffs [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS * 32 ]; <nl> + int16_t mclms_coeffs_cur [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS ]; <nl> int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ;
static int config_input ( AVFilterLink * inlink ) <nl> const AVPixFmtDescriptor * desc = av_pix_fmt_desc_get ( inlink -> format ); <nl> int i ; <nl>  <nl> + uninit ( inlink -> dst ); <nl> + <nl> s -> hsub = desc -> log2_chroma_w ; <nl> s -> vsub = desc -> log2_chroma_h ; <nl> s -> depth = desc -> comp [ 0 ]. depth_minus1 + 1 ;
int avcodec_copy_context ( AVCodecContext * dest , const AVCodecContext * src ) <nl> av_opt_free ( dest ); <nl>  <nl> memcpy ( dest , src , sizeof (* dest )); <nl> + av_opt_copy ( dest , src ); <nl>  <nl> dest -> priv_data = orig_priv_data ; <nl> 
static int handle_metadata ( RTMPContext * rt , RTMPPacket * pkt ) <nl> pts = cts ; <nl> ts += cts - pts ; <nl> pts = cts ; <nl> + if ( size + 3 + 4 > pkt -> data + pkt -> size - next ) <nl> + break ; <nl> bytestream_put_byte (& p , type ); <nl> bytestream_put_be24 (& p , size ); <nl> bytestream_put_be24 (& p , ts );
static void decode_lpc ( int32_t * coeffs , int mode , int length ) <nl> unsigned a1 = * coeffs ++; <nl> for ( i = 0 ; i < length - 1 >> 1 ; i ++) { <nl> * coeffs += a1 ; <nl> - coeffs [ 1 ] += * coeffs ; <nl> + coeffs [ 1 ] += ( unsigned )* coeffs ; <nl> a1 = coeffs [ 1 ]; <nl> coeffs += 2 ; <nl> }
static int dirac_unpack_prediction_parameters ( DiracContext * s ) <nl> s -> globalmc [ ref ]. perspective [ 0 ] = dirac_get_se_golomb ( gb ); <nl> s -> globalmc [ ref ]. perspective [ 1 ] = dirac_get_se_golomb ( gb ); <nl> } <nl> + if ( s -> globalmc [ ref ]. perspective_exp + ( uint64_t ) s -> globalmc [ ref ]. zrs_exp > 30 ) { <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> } <nl> } <nl> 
int main ( int argc , char ** argv ) <nl>  <nl> if (! print_format ) <nl> print_format = av_strdup (" default "); <nl> + if (! print_format ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> w_name = av_strtok ( print_format , "=", & buf ); <nl> w_args = buf ; <nl> 
static int alloc_sequence_buffers ( DiracContext * s ) <nl> s -> mctmp = av_malloc (( w + 64 + MAX_BLOCKSIZE ) * ( h * MAX_BLOCKSIZE ) * sizeof (* s -> mctmp )); <nl> s -> mcscratch = av_malloc (( w + 64 )* MAX_BLOCKSIZE ); <nl>  <nl> - if (! s -> sbsplit || ! s -> blmotion ) <nl> + if (! s -> sbsplit || ! s -> blmotion || ! s -> mctmp || ! s -> mcscratch ) <nl> return AVERROR ( ENOMEM ); <nl> return 0 ; <nl> }
av_cold int ff_MPV_encode_init ( AVCodecContext * avctx ) <nl> av_log ( avctx , AV_LOG_ERROR , " b frames not supported by codec \ n "); <nl> return - 1 ; <nl> } <nl> + if ( s -> max_b_frames < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " max b frames must be 0 or postive for mpegvideo based encoders \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> if (( s -> codec_id == AV_CODEC_ID_MPEG4 || <nl> s -> codec_id == AV_CODEC_ID_H263 ||
static int vp3_update_thread_context ( AVCodecContext * dst , const AVCodecContext * <nl> } <nl>  <nl> if ( s != s1 ) { <nl> + if (! s -> current_frame . f ) <nl> + return AVERROR ( ENOMEM ); <nl> // init tables if the first frame hasn ' t been decoded <nl> if (! s -> current_frame . f -> data [ 0 ]) { <nl> int y_fragment_count , c_fragment_count ;
static int mpegts_read_packet ( AVFormatContext * s , <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> + av_free_packet ( ts -> pkt ); <nl> /* flush pes data left */ <nl> for ( i = 0 ; i < NB_PID_MAX ; i ++) { <nl> if ( ts -> pids [ i ] && ts -> pids [ i ]-> type == MPEGTS_PES ) {
static int vp3_decode_init ( AVCodecContext * avctx ) <nl> & superblock_run_length_vlc_table [ 0 ][ 1 ], 4 , 2 , <nl> & superblock_run_length_vlc_table [ 0 ][ 0 ], 4 , 2 , 0 ); <nl>  <nl> - init_vlc (& s -> fragment_run_length_vlc , 5 , 31 , <nl> + init_vlc (& s -> fragment_run_length_vlc , 5 , 30 , <nl> & fragment_run_length_vlc_table [ 0 ][ 1 ], 4 , 2 , <nl> & fragment_run_length_vlc_table [ 0 ][ 0 ], 4 , 2 , 0 ); <nl> 
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int i ; <nl>  <nl> + x265_picture_init ( ctx -> params , & x265pic ); <nl> + <nl> if ( pic ) { <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> x265pic . planes [ i ] = pic -> data [ i ];
static int get_siz ( Jpeg2000DecoderContext * s ) <nl> avpriv_request_sample ( s -> avctx , " Support for image offsets "); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> + if ( s -> width > 32768U || s -> height > 32768U ) { <nl> + avpriv_request_sample ( s -> avctx , " Large Dimensions "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl>  <nl> if ( ncomponents <= 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid number of components : % d \ n ",
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> sfb_offsets [ i ][ band - 1 ] = subframe_len ; <nl> s -> num_sfb [ i ] = band - 1 ; <nl> + if ( s -> num_sfb [ i ] <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " num_sfb invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> 
static int vqf_read_header ( AVFormatContext * s ) <nl> rate_flag = AV_RB32 ( comm_chunk + 8 ); <nl> avio_skip ( s -> pb , len - 12 ); <nl>  <nl> + if ( st -> codec -> channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> st -> codec -> bit_rate = read_bitrate * 1000 ; <nl> break ; <nl> case MKTAG (' D ',' S ',' I ',' Z '): // size of compressed data
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> switch ( c -> codec_id ) { <nl> case AV_CODEC_ID_H264 : { <nl> int mode = 1 ; <nl> - if ( fmt && fmt -> oformat -> priv_class && <nl> + if ( fmt && fmt -> oformat && fmt -> oformat -> priv_class && <nl> av_opt_flag_is_set ( fmt -> priv_data , " rtpflags ", " h264_mode0 ")) <nl> mode = 0 ; <nl> if ( c -> extradata_size ) {
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> lace_size [ n ] = lace_size [ n - 1 ] + snum ; <nl> total += lace_size [ n ]; <nl> } <nl> - lace_size [ n ] = size - total ; <nl> + lace_size [ laces - 1 ] = size - total ; <nl> break ; <nl> } <nl> }
static void compute_chapters_end ( AVFormatContext * s ) <nl> if ( j != i && next_start > ch -> start && next_start < end ) <nl> end = next_start ; <nl> } <nl> - ch -> end = ( end == INT64_MAX ) ? ch -> start : end ; <nl> + ch -> end = ( end == INT64_MAX || end < ch -> start ) ? ch -> start : end ; <nl> } <nl> } <nl> 
int ff_flac_parse_picture ( AVFormatContext * s , uint8_t * buf , int buf_size ) <nl> ret = AVERROR_INVALIDDATA ; <nl> goto fail ; <nl> } <nl> - if (!( data = av_buffer_alloc ( len ))) { <nl> + if (!( data = av_buffer_alloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ))) { <nl> RETURN_ERROR ( AVERROR ( ENOMEM )); <nl> } <nl> if ( avio_read ( pb , data -> data , len ) != len ) {
static void apply_dependent_coupling_fixed ( AACContext * ac , <nl> for ( k = offsets [ i ]; k < offsets [ i + 1 ]; k ++) { <nl> tmp = ( int )((( int64_t ) src [ group * 128 + k ] * c + \ <nl> ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ group * 128 + k ] += tmp * ( 1 << shift ); <nl> + dest [ group * 128 + k ] += tmp * ( 1U << shift ); <nl> } <nl> } <nl> }
static void update_stream_timings ( AVFormatContext * ic ) <nl> st = ic -> streams [ i ]; <nl> if ( st -> start_time != AV_NOPTS_VALUE && st -> time_base . den ) { <nl> start_time1 = av_rescale_q ( st -> start_time , st -> time_base , AV_TIME_BASE_Q ); <nl> - if ( st -> codec -> codec_id == CODEC_ID_DVB_TELETEXT ) { <nl> + if ( st -> codec -> codec_id == CODEC_ID_DVB_TELETEXT || st -> codec -> codec_type == AVMEDIA_TYPE_SUBTITLE ) { <nl> if ( start_time1 < start_time_text ) <nl> start_time_text = start_time1 ; <nl> } else
static inline void xan_wc3_copy_pixel_run ( XanContext * s , <nl>  <nl> palette_plane = s -> current_frame . data [ 0 ]; <nl> prev_palette_plane = s -> last_frame . data [ 0 ]; <nl> + if (! prev_palette_plane ) <nl> + prev_palette_plane = palette_plane ; <nl> stride = s -> current_frame . linesize [ 0 ]; <nl> line_inc = stride - width ; <nl> curframe_index = y * stride + x ;
void ff_frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> av_freep (& p -> avctx -> slice_offset ); <nl> } <nl>  <nl> - if ( p -> avctx ) <nl> + if ( p -> avctx ) { <nl> av_freep (& p -> avctx -> internal ); <nl> + av_buffer_unref (& p -> avctx -> hw_frames_ctx ); <nl> + } <nl> + <nl> av_freep (& p -> avctx ); <nl> } <nl> 
static inline void drawbox ( AVFilterBufferRef * picref , unsigned int x , unsigned i <nl> static int draw_glyphs ( DrawTextContext * dtext , AVFilterBufferRef * picref , <nl> int width , int height , const uint8_t rgbcolor [ 4 ], const uint8_t yuvcolor [ 4 ], int x , int y ) <nl> { <nl> - char * text = dtext -> text ; <nl> + char * text = HAVE_LOCALTIME_R ? dtext -> expanded_text : dtext -> text ; <nl> uint32_t code = 0 ; <nl> int i ; <nl> uint8_t * p ;
static int init_filters ( const char * filters_descr ) <nl> abuffersink_params -> packing_fmts = packing_fmts ; <nl> ret = avfilter_graph_create_filter (& buffersink_ctx , abuffersink , " out ", <nl> NULL , abuffersink_params , filter_graph ); <nl> + av_free ( abuffersink_params ); <nl> if ( ret < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Cannot create audio buffer sink \ n "); <nl> return ret ;
ogg_get_length ( AVFormatContext * s ) <nl> url_fseek (& s -> pb , end , SEEK_SET ); <nl>  <nl> while (! ogg_read_page ( s , & i )){ <nl> - if ( ogg -> streams [ i ]. granule != - 1 && ogg -> streams [ i ]. granule != 0 ) <nl> + if ( ogg -> streams [ i ]. granule != - 1 && ogg -> streams [ i ]. granule != 0 && <nl> + ogg -> streams [ i ]. codec ) <nl> idx = i ; <nl> } <nl> 
static int mp2_write_trailer ( struct AVFormatContext * s ) <nl> return 0 ; <nl> } <nl>  <nl> - static int query_codec ( enum CodecID id , int std_compliance ) <nl> + static int query_codec ( enum AVCodecID id , int std_compliance ) <nl> { <nl> CodecMime * cm = ff_id3v2_mime_tags ; <nl> - while ( cm -> id != CODEC_ID_NONE ) { <nl> + while ( cm -> id != AV_CODEC_ID_NONE ) { <nl> if ( id == cm -> id ) <nl> return MKTAG (' A ', ' P ', ' I ', ' C '); <nl> cm ++;
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_INIT_AUDIO_BUFFERS : <nl> av_dlog ( NULL , " initialize audio buffers \ n "); <nl> - if (( opcode_version > 1 ) || ( opcode_size > 10 )) { <nl> + if (( opcode_version > 1 ) || ( opcode_size > 10 ) || opcode_size < 6 ) { <nl> av_dlog ( NULL , " bad init_audio_buffers opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static void matroska_add_index_entries ( MatroskaDemuxContext * matroska ) <nl> { <nl> EbmlList * index_list ; <nl> MatroskaIndex * index ; <nl> - int index_scale = 1 ; <nl> + uint64_t index_scale = 1 ; <nl> int i , j ; <nl>  <nl> if ( matroska -> ctx -> flags & AVFMT_FLAG_IGNIDX )
int ff_get_wav_header ( AVIOContext * pb , AVCodecContext * codec , int size ) <nl> codec -> sample_rate = 0 ; <nl> } <nl> /* override bits_per_coded_sample for G . 726 */ <nl> - if ( codec -> codec_id == AV_CODEC_ID_ADPCM_G726 ) <nl> + if ( codec -> codec_id == AV_CODEC_ID_ADPCM_G726 && codec -> sample_rate ) <nl> codec -> bits_per_coded_sample = codec -> bit_rate / codec -> sample_rate ; <nl>  <nl> return 0 ;
static int read_ffserver_streams ( OptionsContext * o , AVFormatContext * s , const ch <nl> { <nl> int i , err ; <nl> AVFormatContext * ic = avformat_alloc_context (); <nl> + if (! ic ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> ic -> interrupt_callback = int_cb ; <nl> err = avformat_open_input (& ic , filename , NULL , NULL );
void ff_rtsp_close_streams ( AVFormatContext * s ) <nl> if ( rtsp_st -> dynamic_handler && rtsp_st -> dynamic_protocol_context ) <nl> rtsp_st -> dynamic_handler -> close ( <nl> rtsp_st -> dynamic_protocol_context ); <nl> + av_free ( rtsp_st ); <nl> } <nl> } <nl> av_free ( rt -> rtsp_streams );
static void ipvideo_decode_opcodes ( IpvideoContext * s , AVFrame * frame ) <nl> init_get_bits (& gb , s -> decoding_map , s -> decoding_map_size * 8 ); <nl> for ( y = 0 ; y < s -> avctx -> height ; y += 8 ) { <nl> for ( x = 0 ; x < s -> avctx -> width ; x += 8 ) { <nl> + if ( get_bits_left (& gb ) < 4 ) <nl> + return ; <nl> opcode = get_bits (& gb , 4 ); <nl>  <nl> ff_tlog ( s -> avctx ,
static int dvdsub_parse ( AVCodecParserContext * s , <nl>  <nl> if ( pc -> packet_index == 0 ) { <nl> if ( buf_size < 2 ) <nl> - return 0 ; <nl> + return buf_size ; <nl> pc -> packet_len = AV_RB16 ( buf ); <nl> if ( pc -> packet_len == 0 ) /* HD - DVD subpicture packet */ <nl> pc -> packet_len = AV_RB32 ( buf + 2 );
static int yop_read_header ( AVFormatContext * s ) <nl>  <nl> audio_stream = avformat_new_stream ( s , NULL ); <nl> video_stream = avformat_new_stream ( s , NULL ); <nl> + if (! audio_stream || ! video_stream ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> // Extra data that will be passed to the decoder <nl> video_stream -> codec -> extradata_size = 8 ;
static int parse_pixel_format ( AVCodecContext * avctx ) <nl> normal_map = flags & DDPF_NORMALMAP ; <nl> fourcc = bytestream2_get_le32 ( gbc ); <nl>  <nl> + if ( ctx -> compressed && ctx -> paletted ) { <nl> + av_log ( avctx , AV_LOG_WARNING , <nl> + " Disabling invalid palette flag for compressed dds .\ n "); <nl> + ctx -> paletted = 0 ; <nl> + } <nl> + <nl> bpp = bytestream2_get_le32 ( gbc ); // rgbbitcount <nl> r = bytestream2_get_le32 ( gbc ); // rbitmask <nl> g = bytestream2_get_le32 ( gbc ); // gbitmask
static OSStatus ffat_decode_callback ( AudioConverterRef converter , UInt32 * nb_pac <nl> return 0 ; <nl> } <nl>  <nl> + av_packet_unref (& at -> in_pkt ); <nl> av_packet_move_ref (& at -> in_pkt , & at -> new_in_pkt ); <nl> at -> new_in_pkt . data = 0 ; <nl> at -> new_in_pkt . size = 0 ;
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + nals_needed = nal_index ; <nl> + break ; <nl> case NAL_IDR_SLICE : <nl> case NAL_SLICE : <nl> - nals_needed = nal_index ; <nl> + init_get_bits (& hx -> s . gb , ptr , bit_length ); <nl> + if (! get_ue_golomb (& hx -> s . gb )) <nl> + nals_needed = nal_index ; <nl> } <nl> continue ; <nl> }
static int mov_codec_id ( AVStream * st , uint32_t format ) <nl> static void mov_parse_stsd_video ( MOVContext * c , AVIOContext * pb , <nl> AVStream * st , MOVStreamContext * sc ) <nl> { <nl> - uint8_t codec_name [ 32 ]; <nl> + uint8_t codec_name [ 32 ] = { 0 }; <nl> int64_t stsd_start ; <nl> unsigned int len ; <nl> 
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_CREATE_TIMER : <nl> av_dlog ( NULL , " create timer \ n "); <nl> - if (( opcode_version > 0 ) || ( opcode_size > 6 )) { <nl> + if (( opcode_version > 0 ) || ( opcode_size != 6 )) { <nl> av_dlog ( NULL , " bad create_timer opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static av_cold int hevc_decode_free ( AVCodecContext * avctx ) <nl>  <nl> pic_arrays_free ( s ); <nl>  <nl> - av_freep (& lc -> edge_emu_buffer ); <nl> + if ( lc ) <nl> + av_freep (& lc -> edge_emu_buffer ); <nl> av_freep (& s -> md5_ctx ); <nl>  <nl> for ( i = 0 ; i < s -> nals_allocated ; i ++) {
static av_cold void dsputil_init_sse2 ( DSPContext * c , AVCodecContext * avctx , <nl> # if HAVE_SSE2_INLINE <nl> const int high_bit_depth = avctx -> bits_per_raw_sample > 8 ; <nl>  <nl> - if (! high_bit_depth && avctx -> idct_algo == FF_IDCT_XVIDMMX ) { <nl> + if (! high_bit_depth && avctx -> idct_algo == FF_IDCT_XVIDMMX && avctx -> lowres == 0 ) { <nl> c -> idct_put = ff_idct_xvid_sse2_put ; <nl> c -> idct_add = ff_idct_xvid_sse2_add ; <nl> c -> idct = ff_idct_xvid_sse2 ;
static int mov_read_extradata ( MOVContext * c , AVIOContext * pb , MOVAtom atom , <nl> av_log ( c -> fc , AV_LOG_WARNING , " truncated extradata \ n "); <nl> st -> codec -> extradata_size -= atom . size - err ; <nl> } <nl> + memset ( buf + 8 + err , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> return 0 ; <nl> } <nl> 
retry_duration : <nl> flv -> last_channels = <nl> channels = st -> codec -> channels ; <nl> } else { <nl> - AVCodecContext ctx ; <nl> + AVCodecContext ctx = { 0 }; <nl> ctx . sample_rate = sample_rate ; <nl> flv_set_audio_codec ( s , st , & ctx , flags & FLV_AUDIO_CODECID_MASK ); <nl> sample_rate = ctx . sample_rate ;
static int rv30_decode_mb_info ( RV34DecContext * r ) <nl> GetBitContext * gb = & s -> gb ; <nl> int code = svq3_get_ue_golomb ( gb ); <nl>  <nl> - if ( code > 11 ){ <nl> + if ( code < 0 || code > 11 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Incorrect MB type code \ n "); <nl> return - 1 ; <nl> }
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> } <nl> } <nl>  <nl> + if ( s -> pb -> eof_reached ) <nl> + return AVERROR_EOF ; <nl> + <nl> return AVERROR ( EIO ); <nl> } <nl> 
typedef struct MmContext { <nl> AVCodecContext * avctx ; <nl> AVFrame * frame ; <nl> - int palette [ AVPALETTE_COUNT ]; <nl> + unsigned int palette [ AVPALETTE_COUNT ]; <nl> GetByteContext gb ; <nl> } MmContext ; <nl> 
static av_cold int XAVS_init ( AVCodecContext * avctx ) <nl> if (! x4 -> enc ) <nl> return - 1 ; <nl>  <nl> - if (!( x4 -> pts_buffer = av_mallocz (( avctx -> max_b_frames + 1 ) * sizeof (* x4 -> pts_buffer )))) <nl> + if (!( x4 -> pts_buffer = av_mallocz_array (( avctx -> max_b_frames + 1 ), sizeof (* x4 -> pts_buffer )))) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> avctx -> coded_frame = av_frame_alloc ();
static int mpeg_decode_mb ( MpegEncContext * s , int16_t block [ 12 ][ 64 ]) <nl>  <nl> cbp = get_vlc2 (& s -> gb , ff_mb_pat_vlc . table , MB_PAT_VLC_BITS , 1 ); <nl> if ( mb_block_count > 6 ) { <nl> - cbp <<= mb_block_count - 6 ; <nl> + cbp *= 1 << mb_block_count - 6 ; <nl> cbp |= get_bits (& s -> gb , mb_block_count - 6 ); <nl> s -> bdsp . clear_blocks ( s -> block [ 6 ]); <nl> }
AVCodec ff_wmv2_decoder = { <nl> wmv2_decode_end , <nl> ff_h263_decode_frame , <nl> CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 , <nl> - . max_lowres = 3 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" Windows Media Video 8 "), <nl> . pix_fmts = ff_pixfmt_list_420 , <nl> };
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> break ; <nl> } <nl>  <nl> - if ( last ){ <nl> + if ( last && s -> current_picture_ptr ){ <nl> if ( r -> loop_filter ) <nl> r -> loop_filter ( r , s -> mb_height - 1 ); <nl> ff_er_frame_end ( s );
static int decode_mb_info ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> } <nl> } <nl> } else { <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> mb -> type = ref_mb -> type ; /* copy mb_type from corresponding reference mb */ <nl> } else if ( ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> mb -> type = 0 ; /* mb_type is always INTRA for intra - frames */
static int rtp_parse_packet_internal ( RTPDemuxContext * s , AVPacket * pkt , <nl> if ( ret < 0 ) <nl> return AVERROR ( EAGAIN ); <nl> if ( ret < len ) { <nl> - s -> read_buf_size = len - ret ; <nl> + s -> read_buf_size = FFMIN ( len - ret , sizeof ( s -> buf )); <nl> memcpy ( s -> buf , buf + ret , s -> read_buf_size ); <nl> s -> read_buf_index = 0 ; <nl> return 1 ;
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> else { <nl> switch ( type ) { <nl> case TIFF_BYTE : <nl> - s -> bpp = ( off & 0xFF ) + (( off >> 8 ) & 0xFF ) + <nl> - (( off >> 16 ) & 0xFF ) + (( off >> 24 ) & 0xFF ); <nl> - break ; <nl> case TIFF_SHORT : <nl> case TIFF_LONG : <nl> s -> bpp = 0 ;
static int r3d_read_reda ( AVFormatContext * s , AVPacket * pkt , Atom * atom ) <nl> dts = avio_rb32 ( s -> pb ); <nl>  <nl> st -> codec -> sample_rate = avio_rb32 ( s -> pb ); <nl> + if ( st -> codec -> sample_rate < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " negative sample rate \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> samples = avio_rb32 ( s -> pb ); <nl> 
static av_cold int pcm_encode_init ( AVCodecContext * avctx ) <nl>  <nl> avctx -> bits_per_coded_sample = av_get_bits_per_sample ( avctx -> codec -> id ); <nl> avctx -> block_align = avctx -> channels * avctx -> bits_per_coded_sample / 8 ; <nl> - avctx -> bit_rate = avctx -> block_align * avctx -> sample_rate * 8 ; <nl> + avctx -> bit_rate = avctx -> block_align * 8LL * avctx -> sample_rate ; <nl>  <nl> return 0 ; <nl> }
int av_opt_query_ranges_default ( AVOptionRanges ** ranges_arg , void * obj , const ch <nl> fail : <nl> av_free ( ranges ); <nl> av_free ( range ); <nl> + av_free ( range_array ); <nl> return ret ; <nl> } <nl> 
static int set_string_number ( void * obj , void * target_obj , const AVOption * o , con <nl> } <nl>  <nl> { <nl> - const AVOption * o_named = av_opt_find ( target_obj , buf , o -> unit , 0 , 0 ); <nl> + const AVOption * o_named = av_opt_find ( target_obj , i ? buf : val , o -> unit , 0 , 0 ); <nl> int res ; <nl> int ci = 0 ; <nl> double const_values [ 64 ];
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> return 0 ; <nl> } <nl>  <nl> + if ( vstream -> nb_index_entries > 0 ){ <nl> + av_log ( s , AV_LOG_WARNING , " Skiping duplicate index \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> while ( avio_tell ( ioc ) < max_pos - 2 && amf_get_string ( ioc , str_val , sizeof ( str_val )) > 0 ) { <nl> int64_t ** current_array ; <nl> unsigned int arraylen ;
static int read_access_unit ( AVCodecContext * avctx , void * data , <nl> substr_header_size += 2 ; <nl> } <nl>  <nl> + if ( length < header_size + substr_header_size ) { <nl> + av_log ( m -> avctx , AV_LOG_ERROR , " Insuffient data for headers \ n "); <nl> + goto error ; <nl> + } <nl> + <nl> if (!( nonrestart_substr ^ m -> is_major_sync_unit )) { <nl> av_log ( m -> avctx , AV_LOG_ERROR , " Invalid nonrestart_substr .\ n "); <nl> goto error ;
int ff_xvid_rate_control_init ( MpegEncContext * s ){ <nl>  <nl> if ( write ( fd , tmp , strlen ( tmp )) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error % s writing 2pass logfile \ n ", strerror ( errno )); <nl> + av_free ( tmp_name ); <nl> + close ( fd ); <nl> return AVERROR ( errno ); <nl> } <nl> }
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl> static void free_texture ( void * opaque , uint8_t * data ) <nl> { <nl> ID3D11Texture2D_Release (( ID3D11Texture2D *) opaque ); <nl> + av_free ( data ); <nl> } <nl>  <nl> static AVBufferRef * wrap_texture_buf ( ID3D11Texture2D * tex , int index )
enum AVPixelFormat avcodec_find_best_pix_fmt_of_list ( const enum AVPixelFormat * p <nl> int loss ; <nl>  <nl> for ( i = 0 ; pix_fmt_list [ i ] != AV_PIX_FMT_NONE ; i ++) { <nl> - loss = * loss_ptr ; <nl> + loss = loss_ptr ? * loss_ptr : 0 ; <nl> best = avcodec_find_best_pix_fmt_of_2 ( best , pix_fmt_list [ i ], src_pix_fmt , has_alpha , & loss ); <nl> } <nl>  <nl> - * loss_ptr = loss ; <nl> + if ( loss_ptr ) <nl> + * loss_ptr = loss ; <nl> return best ; <nl> } <nl> 
static int hds_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> HDSContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ s -> streams [ pkt -> stream_index ]-> id ]; <nl> - int64_t end_dts = ( os -> fragment_index ) * c -> min_frag_duration ; <nl> + int64_t end_dts = os -> fragment_index * ( int64_t ) c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int decode_thread ( void * arg ) <nl> if ( ret < 0 ) { <nl> if ( ret == AVERROR_EOF || url_feof ( ic -> pb )) <nl> eof = 1 ; <nl> - if ( ic -> pb -> error ) <nl> + if ( ic -> pb && ic -> pb -> error ) <nl> break ; <nl> SDL_Delay ( 100 ); /* wait for user event */ <nl> continue ;
int ff_lpc_calc_coefs ( LPCContext * s , <nl> LLSModel m [ 2 ]; <nl> double var [ MAX_LPC_ORDER + 1 ], av_uninit ( weight ); <nl>  <nl> + if ( lpc_passes <= 0 ) <nl> + lpc_passes = 2 ; <nl> + <nl> for ( pass = 0 ; pass < lpc_passes ; pass ++){ <nl> av_init_lls (& m [ pass & 1 ], max_order ); <nl> 
static inline int read_line ( AVFormatContext * s , char * rbuf , const int rbufsize , <nl>  <nl> do { <nl> ret = ffurl_read_complete ( rt -> rtsp_hd , rbuf + idx , 1 ); <nl> - if ( ret < 0 ) <nl> - return ret ; <nl> + if ( ret <= 0 ) <nl> + return ret ? ret : AVERROR_EOF ; <nl> if ( rbuf [ idx ] == '\ r ') { <nl> /* Ignore */ <nl> } else if ( rbuf [ idx ] == '\ n ') {
static int decode_frame ( AVCodecContext * avctx , <nl> AVFrame * const p = & s -> picture ; <nl> uint8_t * ptr ; <nl>  <nl> - int magic_num , offset , endian ; <nl> + unsigned int offset ; <nl> + int magic_num , endian ; <nl> int x , y ; <nl> int w , h , stride , bits_per_color , descriptor , elements , target_packet_size , source_packet_size ; <nl> 
void avcodec_register_all ( void ) <nl> REGISTER_ENCDEC ( XSUB , xsub ); <nl>  <nl> /* external libraries */ <nl> - REGISTER_ENCODER ( LIBAACPLUS , libaacplus ); <nl> REGISTER_DECODER ( LIBCELT , libcelt ); <nl> REGISTER_DECODER ( LIBDIRAC , libdirac ); <nl> REGISTER_ENCODER ( LIBFAAC , libfaac ); <nl> void avcodec_register_all ( void ) <nl> REGISTER_ENCODER ( LIBX264RGB , libx264rgb ); <nl> REGISTER_ENCODER ( LIBXAVS , libxavs ); <nl> REGISTER_ENCODER ( LIBXVID , libxvid ); <nl> + REGISTER_ENCODER ( LIBAACPLUS , libaacplus ); <nl>  <nl> /* text */ <nl> REGISTER_DECODER ( BINTEXT , bintext );
static int process_output_surface ( AVCodecContext * avctx , AVPacket * pkt , NvencSur <nl> NV_ENCODE_API_FUNCTION_LIST * p_nvenc = & dl_fn -> nvenc_funcs ; <nl>  <nl> uint32_t slice_mode_data ; <nl> - uint32_t * slice_offsets ; <nl> + uint32_t * slice_offsets = NULL ; <nl> NV_ENC_LOCK_BITSTREAM lock_params = { 0 }; <nl> NVENCSTATUS nv_status ; <nl> int res = 0 ;
static int vp9_superframe_filter ( AVBSFContext * ctx , AVPacket * out ) <nl> goto done ; <nl> } <nl>  <nl> - av_packet_move_ref ( s -> cache [ s -> n_cache ++], in ); <nl> + res = av_packet_ref ( s -> cache [ s -> n_cache ++], in ); <nl> + if ( res < 0 ) <nl> + goto done ; <nl>  <nl> if ( invisible ) { <nl> res = AVERROR ( EAGAIN );
static av_cold int g726_decode_init ( AVCodecContext * avctx ) <nl> { <nl> G726Context * c = avctx -> priv_data ; <nl>  <nl> + if ( avctx -> channels > 1 ){ <nl> + avpriv_request_sample ( avctx , " Decoding more than one channel "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> avctx -> channels = 1 ; <nl> avctx -> channel_layout = AV_CH_LAYOUT_MONO ; <nl> 
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> if ( entries >= UINT_MAX / sizeof (* sc -> drefs )) <nl> return AVERROR_INVALIDDATA ; <nl> av_free ( sc -> drefs ); <nl> + sc -> drefs_count = 0 ; <nl> sc -> drefs = av_mallocz ( entries * sizeof (* sc -> drefs )); <nl> if (! sc -> drefs ) <nl> return AVERROR ( ENOMEM );
static int ea_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( ea -> audio_codec ) { <nl> - if ( ea -> num_channels <= 0 ) { <nl> + if ( ea -> num_channels <= 0 || ea -> num_channels > 2 ) { <nl> av_log ( s , AV_LOG_WARNING , <nl> " Unsupported number of channels : % d \ n ", ea -> num_channels ); <nl> ea -> audio_codec = 0 ;
static av_cold void uninit ( AVFilterContext * ctx ) <nl> avfilter_unref_buffer ( deshake -> ref ); <nl> if ( deshake -> fp ) <nl> fclose ( deshake -> fp ); <nl> - avcodec_close ( deshake -> avctx ); <nl> + if ( deshake -> avctx ) <nl> + avcodec_close ( deshake -> avctx ); <nl> av_freep (& deshake -> avctx ); <nl> } <nl> 
static int txd_read_header ( AVFormatContext * s , AVFormatParameters * ap ) { <nl> st -> codec -> time_base . den = 5 ; <nl> st -> codec -> time_base . num = 1 ; <nl> /* the parameters will be extracted from the compressed bitstream */ <nl> + <nl> + s -> pb -> maxsize = avio_size ( s -> pb ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int http_open_cnx ( URLContext * h ) <nl> { <nl> const char * path , * proxy_path , * lower_proto = " tcp ", * local_path ; <nl> char hostname [ 1024 ], hoststr [ 1024 ], proto [ 10 ]; <nl> - char auth [ 1024 ], proxyauth [ 1024 ]; <nl> + char auth [ 1024 ], proxyauth [ 1024 ] = ""; <nl> char path1 [ 1024 ]; <nl> char buf [ 1024 ], urlbuf [ 1024 ]; <nl> int port , use_proxy , err , location_changed = 0 , redirects = 0 ;
static int vorbis_parse_audio_packet ( vorbis_context * vc ) <nl> ch_left -= ch ; <nl> } <nl>  <nl> + if ( ch_left > 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> // Inverse coupling <nl>  <nl> for ( i = mapping -> coupling_steps - 1 ; i >= 0 ; -- i ) { // warning : i has to be signed
static int bit_allocation ( IMCContext * q , IMCChannel * chctx , <nl> iacc += chctx -> bandWidthT [ i ]; <nl> summa += chctx -> bandWidthT [ i ] * chctx -> flcoeffs4 [ i ]; <nl> } <nl> + <nl> + if (! iacc ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> chctx -> bandWidthT [ BANDS - 1 ] = 0 ; <nl> summa = ( summa * 0 . 5 - freebits ) / iacc ; <nl> 
static av_cold int opus_decode_init ( AVCodecContext * avctx ) <nl>  <nl> /* find out the channel configuration */ <nl> ret = ff_opus_parse_extradata ( avctx , c ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& c -> channel_maps ); <nl> + av_freep (& c -> fdsp ); <nl> return ret ; <nl> + } <nl>  <nl> /* allocate and init each independent decoder */ <nl> c -> streams = av_mallocz_array ( c -> nb_streams , sizeof (* c -> streams ));
* MPEG Audio decoder <nl> */ <nl>  <nl> -# define UNCHECKED_BITSTREAM_READER 1 <nl> - <nl> # include " libavutil / audioconvert . h " <nl> # include " avcodec . h " <nl> # include " get_bits . h "
int ff_jpegls_decode_picture ( MJpegDecodeContext * s , int near , <nl> avpriv_report_missing_feature ( s -> avctx , " Sample interleaved images "); <nl> ret = AVERROR_PATCHWELCOME ; <nl> goto end ; <nl> + } else { /* unknown interleaving */ <nl> + avpriv_report_missing_feature ( s -> avctx , " Unknown interleaved images "); <nl> + ret = AVERROR_PATCHWELCOME ; <nl> + goto end ; <nl> } <nl>  <nl> if ( s -> xfrm && s -> nb_components == 3 ) {
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> static int mov_probe ( AVProbeData * p ) <nl> { <nl> - unsigned int offset ; <nl> + int64_t offset ; <nl> uint32_t tag ; <nl> int score = 0 ; <nl> 
static int resolve_content_path ( AVFormatContext * s , const char * url , int * max_ur <nl> if (!( node = baseurl_nodes [ rootId ])) { <nl> continue ; <nl> } <nl> - if ( ishttp ( xmlNodeGetContent ( node ))) { <nl> + text = xmlNodeGetContent ( node ); <nl> + if ( ishttp ( text )) { <nl> + xmlFree ( text ); <nl> break ; <nl> } <nl> + xmlFree ( text ); <nl> } <nl>  <nl> node = baseurl_nodes [ rootId ];
static int get_metadata ( AVFormatContext * s , <nl> if (! buf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - if ( avio_read ( s -> pb , buf , data_size ) < 0 ) { <nl> + if ( avio_read ( s -> pb , buf , data_size ) != data_size ) { <nl> av_free ( buf ); <nl> return AVERROR ( EIO ); <nl> }
resync : <nl> pkt -> data , pkt -> size ); <nl> pkt -> destruct = dstr ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> + if ( size < 0 ) <nl> + av_free_packet ( pkt ); <nl> } else { <nl> /* XXX : How to handle B - frames in AVI ? */ <nl> pkt -> dts = ast -> frame_offset ;
static int cinvideo_decode_frame ( AVCodecContext * avctx , <nl> if ( bitmap_frame_size < palette_colors_count * ( 3 + ( palette_type != 0 ))) <nl> return AVERROR_INVALIDDATA ; <nl> if ( palette_type == 0 ) { <nl> + if ( palette_colors_count > 256 ) <nl> + return AVERROR_INVALIDDATA ; <nl> for ( i = 0 ; i < palette_colors_count ; ++ i ) { <nl> cin -> palette [ i ] = bytestream_get_le24 (& buf ); <nl> bitmap_frame_size -= 3 ;
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + avctx -> height * lsize ) { <nl> + if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + ( uint64_t ) avctx -> height * lsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " input buffer too small \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
int ff_vda_destroy_decoder ( struct vda_context * vda_ctx ) <nl> static int vda_h264_uninit ( AVCodecContext * avctx ) <nl> { <nl> VDAContext * vda = avctx -> internal -> hwaccel_priv_data ; <nl> - av_freep (& vda -> bitstream ); <nl> - if ( vda -> frame ) <nl> - CVPixelBufferRelease ( vda -> frame ); <nl> + if ( vda ) { <nl> + av_freep (& vda -> bitstream ); <nl> + if ( vda -> frame ) <nl> + CVPixelBufferRelease ( vda -> frame ); <nl> + } <nl> return 0 ; <nl> } <nl> 
static int ea_read_header ( AVFormatContext * s , <nl> st -> codec -> height = ea -> height ; <nl> } <nl>  <nl> + if ( ea -> num_channels <= 0 ) { <nl> + av_log ( s , AV_LOG_WARNING , " Unsupported number of channels : % d \ n ", ea -> num_channels ); <nl> + ea -> audio_codec = 0 ; <nl> + } <nl> + <nl> if ( ea -> audio_codec ) { <nl> /* initialize the audio decoder stream */ <nl> st = av_new_stream ( s , 0 );
uint8_t * av_packet_new_side_data ( AVPacket * pkt , enum AVPacketSideDataType type , <nl> if (! pkt -> side_data ) <nl> return NULL ; <nl>  <nl> - pkt -> side_data [ elems ]. data = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + pkt -> side_data [ elems ]. data = av_mallocz ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! pkt -> side_data [ elems ]. data ) <nl> return NULL ; <nl> pkt -> side_data [ elems ]. size = size ;
av_cold int ff_ac3_encode_close ( AVCodecContext * avctx ) <nl> AC3EncodeContext * s = avctx -> priv_data ; <nl>  <nl> av_freep (& s -> windowed_samples ); <nl> + if ( s -> planar_samples ) <nl> for ( ch = 0 ; ch < s -> channels ; ch ++) <nl> av_freep (& s -> planar_samples [ ch ]); <nl> av_freep (& s -> planar_samples );
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & bc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
typedef struct HEVCPPS { <nl> uint8_t chroma_qp_offset_list_enabled_flag ; <nl> uint8_t diff_cu_chroma_qp_offset_depth ; <nl> uint8_t chroma_qp_offset_list_len_minus1 ; <nl> - int8_t cb_qp_offset_list [ 5 ]; <nl> - int8_t cr_qp_offset_list [ 5 ]; <nl> + int8_t cb_qp_offset_list [ 6 ]; <nl> + int8_t cr_qp_offset_list [ 6 ]; <nl> uint8_t log2_sao_offset_scale_luma ; <nl> uint8_t log2_sao_offset_scale_chroma ; <nl> 
static int gdv_decode_frame ( AVCodecContext * avctx , void * data , <nl> case 1 : <nl> memset ( gdv -> frame + PREAMBLE_SIZE , 0 , gdv -> frame_size - PREAMBLE_SIZE ); <nl> case 0 : <nl> + if ( bytestream2_get_bytes_left ( gb ) < 256 * 3 ) <nl> + return AVERROR_INVALIDDATA ; <nl> for ( i = 0 ; i < 256 ; i ++) { <nl> unsigned r = bytestream2_get_byte ( gb ); <nl> unsigned g = bytestream2_get_byte ( gb );
static int64_t pva_read_timestamp ( struct AVFormatContext * s , int stream_index , <nl> ByteIOContext * pb = s -> pb ; <nl> PVAContext * pvactx = s -> priv_data ; <nl> int length , streamid ; <nl> - int64_t res ; <nl> + int64_t res = AV_NOPTS_VALUE ; <nl>  <nl> pos_limit = FFMIN (* pos + PVA_MAX_PAYLOAD_LENGTH * 8 , ( uint64_t )* pos + pos_limit ); <nl> 
char * av_strdup ( const char * s ) <nl> char * ptr = NULL ; <nl> if ( s ) { <nl> int len = strlen ( s ) + 1 ; <nl> - ptr = av_malloc ( len ); <nl> + ptr = av_realloc ( NULL , len ); <nl> if ( ptr ) <nl> memcpy ( ptr , s , len ); <nl> }
static int adx_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf_size -= BLOCK_SIZE ; <nl> buf += BLOCK_SIZE ; <nl> } <nl> - samples_offset += BLOCK_SAMPLES ; <nl> + if (! c -> eof ) <nl> + samples_offset += BLOCK_SAMPLES ; <nl> } <nl>  <nl> + frame -> nb_samples = samples_offset ; <nl> * got_frame_ptr = 1 ; <nl>  <nl> return buf - avpkt -> data ;
int ff_audio_mix ( AudioMix * am , AudioData * src ) <nl>  <nl> if ( am -> in_matrix_channels && am -> out_matrix_channels ) { <nl> uint8_t ** data ; <nl> - uint8_t * data0 [ AVRESAMPLE_MAX_CHANNELS ]; <nl> + uint8_t * data0 [ AVRESAMPLE_MAX_CHANNELS ] = { NULL }; <nl>  <nl> if ( am -> out_matrix_channels < am -> out_channels || <nl> am -> in_matrix_channels < am -> in_channels ) {
static void fill_table ( uint8_t * table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int <nl> } <nl> } <nl>  <nl> - static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int inc ) <nl> + static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int64_t inc ) <nl> { <nl> int i ; <nl> int off = -( inc >> 9 );
static int mkv_write_attachments ( AVFormatContext * s ) <nl>  <nl> mkv -> attachments = av_mallocz ( sizeof (* mkv -> attachments )); <nl> if (! mkv -> attachments ) <nl> - return ret ; <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> av_lfg_init (& c , av_get_random_seed ()); <nl> 
static int ism_write_header ( AVFormatContext * s ) <nl> goto fail ; <nl> } <nl>  <nl> - c -> streams = av_mallocz ( sizeof (* c -> streams ) * s -> nb_streams ); <nl> + c -> streams = av_mallocz_array ( s -> nb_streams , sizeof (* c -> streams )); <nl> if (! c -> streams ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ;
static int decode_lowdelay ( DiracContext * s ) <nl> s -> slice_params_buf = av_realloc_f ( s -> slice_params_buf , s -> num_x * s -> num_y , sizeof ( DiracSlice )); <nl> if (! s -> slice_params_buf ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " slice params buffer allocation failure \ n "); <nl> + s -> slice_params_num_buf = 0 ; <nl> return AVERROR ( ENOMEM ); <nl> } <nl> s -> slice_params_num_buf = s -> num_x * s -> num_y ;
static int smush_read_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> case MKBETAG (' W ', ' a ', ' v ', ' e '): <nl> if ( size < 13 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( av_get_packet ( pb , pkt , size ) < 0 ) <nl> + if ( av_get_packet ( pb , pkt , size ) < 13 ) <nl> return AVERROR ( EIO ); <nl>  <nl> pkt -> stream_index = smush -> audio_stream_index ;
static int encode_thread ( AVCodecContext * c , void * arg ){ <nl> av_fast_padded_malloc (& new_buffer , & new_buffer_size , new_size ); <nl> if ( new_buffer ) { <nl> memcpy ( new_buffer , s -> avctx -> internal -> byte_buffer , s -> avctx -> internal -> byte_buffer_size ); <nl> + av_free ( s -> avctx -> internal -> byte_buffer ); <nl> s -> avctx -> internal -> byte_buffer = new_buffer ; <nl> s -> avctx -> internal -> byte_buffer_size = new_buffer_size ; <nl> rebase_put_bits (& s -> pb , new_buffer , new_buffer_size );
static int unpack_modes ( Vp3DecodeContext * s , GetBitContext * gb ) <nl>  <nl> /* is it a custom coding scheme ? */ <nl> if ( scheme == 0 ) { <nl> + for ( i = 0 ; i < 8 ; i ++) <nl> + custom_mode_alphabet [ i ] = MODE_INTER_NO_MV ; <nl> for ( i = 0 ; i < 8 ; i ++) <nl> custom_mode_alphabet [ get_bits ( gb , 3 )] = i ; <nl> }
void * av_malloc ( size_t size ) <nl> long diff ; <nl> # endif <nl>  <nl> + assert ( size ); <nl> + <nl> /* let ' s disallow possible ambiguous cases */ <nl> - if ( size > ( INT_MAX - 32 ) ) <nl> + if ( size > ( INT_MAX - 32 ) || ! size ) <nl> return NULL ; <nl>  <nl> # if CONFIG_MEMALIGN_HACK
typedef struct { <nl>  <nl>  <nl> static const AVOption options [] = { <nl> - { " use_odml ", " use odml index ", offsetof ( AVIContext , use_odml ), FF_OPT_TYPE_INT , 1 , - 1 , 1 , AV_OPT_FLAG_DECODING_PARAM }, <nl> + { " use_odml ", " use odml index ", offsetof ( AVIContext , use_odml ), FF_OPT_TYPE_INT , {. dbl = 1 }, - 1 , 1 , AV_OPT_FLAG_DECODING_PARAM }, <nl> { NULL }, <nl> }; <nl> 
static int vdpau_hevc_start_frame ( AVCodecContext * avctx , <nl> const HEVCFrame * frame = & h -> DPB [ i ]; <nl> if ( frame != h -> ref && ( frame -> flags & ( HEVC_FRAME_FLAG_LONG_REF | <nl> HEVC_FRAME_FLAG_SHORT_REF ))) { <nl> - if ( j > 16 ) { <nl> + if ( j > 15 ) { <nl> av_log ( avctx , AV_LOG_WARNING , <nl> " VDPAU only supports up to 16 references in the DPB . " <nl> " This frame may not be decoded correctly .\ n ");
static int swf_write_trailer ( AVFormatContext * s ) <nl> put_le32 ( pb , file_size ); <nl> url_fseek ( pb , swf -> duration_pos , SEEK_SET ); <nl> put_le16 ( pb , video_enc -> frame_number ); <nl> + url_fseek ( pb , file_size , SEEK_SET ); <nl> } <nl>  <nl> av_free ( swf -> audio_fifo );
static int read_matrix_params ( MLPDecodeContext * m , unsigned int substr , GetBitCo <nl> av_log ( m -> avctx , AV_LOG_ERROR , <nl> " Number of primitive matrices cannot be greater than % d .\ n ", <nl> max_primitive_matrices ); <nl> + s -> num_primitive_matrices = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
int ff_pnm_decode_header ( AVCodecContext * avctx , PNMContext * const s ) <nl>  <nl> avctx -> width = w ; <nl> avctx -> height = h ; <nl> + s -> maxval = maxval ; <nl> if ( depth == 1 ) { <nl> if ( maxval == 1 ) <nl> avctx -> pix_fmt = PIX_FMT_MONOWHITE ;
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> s -> slices [ i ][ j ]. start = offset + header_size ; <nl> - s -> slices [ i ][ j ]. size = avpkt -> size - offset ; <nl> + s -> slices [ i ][ j ]. size = avpkt -> size - s -> slices [ i ][ j ]. start ; <nl> } <nl>  <nl> if ( bytestream2_get_byte (& gb ) != s -> planes )
static int ivf_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> static int ivf_write_trailer ( AVFormatContext * s ) <nl> { <nl> AVIOContext * pb = s -> pb ; <nl> - if ( pb -> seekable ) { <nl> - IVFEncContext * ctx = s -> priv_data ; <nl> + IVFEncContext * ctx = s -> priv_data ; <nl> + <nl> + if ( pb -> seekable && ctx -> frame_cnt > 1 ) { <nl> size_t end = avio_tell ( pb ); <nl>  <nl> avio_seek ( pb , 24 , SEEK_SET );
static av_cold int vtenc_init ( AVCodecContext * avctx ) <nl> kCFAllocatorDefault , <nl> & has_b_frames_cfbool ); <nl>  <nl> - if (! status ) { <nl> + if (! status && has_b_frames_cfbool ) { <nl> // Some devices don ' t output B - frames for main profile , even if requested . <nl> vtctx -> has_b_frames = CFBooleanGetValue ( has_b_frames_cfbool ); <nl> CFRelease ( has_b_frames_cfbool );
static int mov_write_sidx_tag ( AVIOContext * pb , <nl> } <nl> } else { <nl> entries = track -> nb_frag_info ; <nl> + if ( entries <= 0 ) <nl> + return 0 ; <nl> presentation_time = track -> frag_info [ 0 ]. time ; <nl> } <nl> 
static int tcp_open ( URLContext * h , const char * uri , int flags ) <nl> } <nl> /* test error */ <nl> optlen = sizeof ( ret ); <nl> - getsockopt ( fd , SOL_SOCKET , SO_ERROR , & ret , & optlen ); <nl> + if ( getsockopt ( fd , SOL_SOCKET , SO_ERROR , & ret , & optlen )) <nl> + ret = AVUNERROR ( ff_neterrno ()); <nl> if ( ret != 0 ) { <nl> char errbuf [ 100 ]; <nl> ret = AVERROR ( ret );
static inline int init_get_bits ( GetBitContext * s , const uint8_t * buffer , <nl> int buffer_size ; <nl> int ret = 0 ; <nl>  <nl> - if ( bit_size > INT_MAX - 7 || bit_size <= 0 ) { <nl> + if ( bit_size > INT_MAX - 7 || bit_size < 0 || ! buffer ) { <nl> buffer_size = bit_size = 0 ; <nl> buffer = NULL ; <nl> ret = AVERROR_INVALIDDATA ;
end : <nl> frame -> height = avctx -> height ; <nl> } <nl>  <nl> + if ( ret < 0 ) <nl> + av_frame_unref ( frame ); <nl> + <nl> return ret ; <nl> } <nl> 
static int flac_parse ( AVCodecParserContext * s , AVCodecContext * avctx , <nl> temp = curr -> next ; <nl> av_freep (& curr -> link_penalty ); <nl> av_free ( curr ); <nl> + fpc -> nb_headers_buffered --; <nl> } <nl> fpc -> headers = fpc -> best_header -> next ; <nl> av_freep (& fpc -> best_header -> link_penalty ); <nl> av_freep (& fpc -> best_header ); <nl> + fpc -> nb_headers_buffered --; <nl> } <nl>  <nl> /* Find and score new headers . */
int av_expr_parse ( AVExpr ** expr , const char * s , <nl> goto end ; <nl> } <nl> e -> var = av_mallocz ( sizeof ( double ) * VARS ); <nl> + if (! e -> var ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> * expr = e ; <nl> e = NULL ; <nl> end :
void ff_af_queue_remove ( AudioFrameQueue * afq , int nb_samples , int64_t * pts , <nl>  <nl> if ( nb_samples ){ <nl> av_assert0 (! afq -> frame_count ); <nl> - if ( afq -> frames [ 0 ]. pts != AV_NOPTS_VALUE ) <nl> + if ( afq -> frames && afq -> frames [ 0 ]. pts != AV_NOPTS_VALUE ) <nl> afq -> frames [ 0 ]. pts += nb_samples ; <nl> av_log ( afq -> avctx , AV_LOG_DEBUG , " Trying to remove % d more samples than are in the que \ n ", nb_samples ); <nl> }
typedef struct FlacSubframe { <nl> int shift ; <nl> RiceContext rc ; <nl> int32_t samples [ FLAC_MAX_BLOCKSIZE ]; <nl> - int32_t residual [ FLAC_MAX_BLOCKSIZE ]; <nl> + int32_t residual [ FLAC_MAX_BLOCKSIZE + 1 ]; <nl> } FlacSubframe ; <nl>  <nl> typedef struct FlacFrame {
static int read_frame_data ( ALSDecContext * ctx , unsigned int ra_frame ) <nl>  <nl> // TODO : read_diff_float_data <nl>  <nl> + if ( get_bits_left ( gb ) < 0 ) { <nl> + av_log ( ctx -> avctx , AV_LOG_ERROR , " Overread % d \ n ", - get_bits_left ( gb )); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static int h261_probe ( AVProbeData * p ) <nl>  <nl> static int ac3_probe ( AVProbeData * p ) <nl> { <nl> - int max_frames , first_frames , frames ; <nl> + int max_frames , first_frames = 0 , frames ; <nl> uint8_t * buf , * buf2 , * end ; <nl> AC3HeaderInfo hdr ; <nl> 
static int cbs_jpeg_split_fragment ( CodedBitstreamContext * ctx , <nl> if ( marker == JPEG_MARKER_SOS ) { <nl> length = AV_RB16 ( frag -> data + start ); <nl>  <nl> + if ( length > end - start ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> data_ref = NULL ; <nl> data = av_malloc ( end - start + <nl> AV_INPUT_BUFFER_PADDING_SIZE );
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl> info -> ec3_done = 1 ; <nl> goto concatenate ; <nl> } <nl> + } else { <nl> + if ( hdr -> substreamid != 0 ) { <nl> + avpriv_request_sample ( mov -> fc , " Multiple non EAC3 independent substreams "); <nl> + ret = AVERROR_PATCHWELCOME ; <nl> + goto end ; <nl> + } <nl> } <nl>  <nl> /* fill the info needed for the " dec3 " atom */
static const struct URLProtocol * url_find_protocol ( const char * filename ) <nl> return up ; <nl> } <nl> } <nl> + av_freep (& protocols ); <nl>  <nl> return NULL ; <nl> }
int64_t avio_seek ( AVIOContext * s , int64_t offset , int whence ) <nl> offset1 = pos + ( s -> buf_ptr - s -> buffer ); <nl> if ( offset == 0 ) <nl> return offset1 ; <nl> + if ( offset > INT64_MAX - offset1 ) <nl> + return AVERROR ( EINVAL ); <nl> offset += offset1 ; <nl> } <nl> if ( offset < 0 )
static int flashsv_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log_missing_feature ( avctx , " zlibprime_curr ", 1 ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> + if (! s -> blocks && ( s -> zlibprime_curr || s -> zlibprime_prev )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " no data available for zlib " <nl> + " priming \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> size --; // account for flags byte <nl> } <nl> 
static av_cold int mp_decode_init ( AVCodecContext * avctx ) <nl> int w4 = ( avctx -> width + 3 ) & ~ 3 ; <nl> int h4 = ( avctx -> height + 3 ) & ~ 3 ; <nl>  <nl> + if ( avctx -> extradata_size < 2 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " extradata too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> motionpixels_tableinit (); <nl> mp -> avctx = avctx ; <nl> ff_dsputil_init (& mp -> dsp , avctx );
ogm_header ( AVFormatContext * s , int idx ) <nl> if ( size > 52 ) { <nl> av_assert0 ( AV_INPUT_BUFFER_PADDING_SIZE <= 52 ); <nl> size -= 52 ; <nl> + if ( bytestream2_get_bytes_left (& p ) < size ) <nl> + return AVERROR_INVALIDDATA ; <nl> ff_alloc_extradata ( st -> codecpar , size ); <nl> bytestream2_get_buffer (& p , st -> codecpar -> extradata , st -> codecpar -> extradata_size ); <nl> }
static int find_image_range ( int * pfirst_index , int * plast_index , <nl> if ( avio_check ( buf , AVIO_FLAG_READ ) > 0 ) <nl> break ; <nl> } <nl> - if ( first_index == 5 ) <nl> + if ( first_index == start_index + 5 ) <nl> goto fail ; <nl>  <nl> /* find the last image */
int ff_h264_decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> } <nl> } <nl>  <nl> - if ( h == h0 && h -> dequant_coeff_pps != pps_id ) { <nl> + if ( first_slice && h -> dequant_coeff_pps != pps_id ) { <nl> h -> dequant_coeff_pps = pps_id ; <nl> h264_init_dequant_tables ( h ); <nl> }
void ff_rtsp_undo_setup ( AVFormatContext * s ) <nl> avformat_free_context ( rtpctx ); <nl> } else if ( rt -> transport == RTSP_TRANSPORT_RDT && CONFIG_RTPDEC ) <nl> ff_rdt_parse_close ( rtsp_st -> transport_priv ); <nl> - else if ( rt -> transport == RTSP_TRANSPORT_RAW && CONFIG_RTPDEC ) <nl> + else if ( rt -> transport == RTSP_TRANSPORT_RTP && CONFIG_RTPDEC ) <nl> ff_rtp_parse_close ( rtsp_st -> transport_priv ); <nl> } <nl> rtsp_st -> transport_priv = NULL ;
int ff_raw_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> st -> codec -> width = width ; <nl> st -> codec -> height = height ; <nl> st -> codec -> pix_fmt = pix_fmt ; <nl> - break ; <nl> fail : <nl> av_freep (& s1 -> video_size ); <nl> av_freep (& s1 -> pixel_format );
static void read_tree ( GetBitContext * gb , Tree * tree ) <nl> tree -> syms [ i ] = get_bits ( gb , 4 ); <nl> tmp1 [ tree -> syms [ i ]] = 1 ; <nl> } <nl> - for ( i = 0 ; i < 16 ; i ++) <nl> + for ( i = 0 ; i < 16 && len < 16 - 1 ; i ++) <nl> if (! tmp1 [ i ]) <nl> tree -> syms [++ len ] = i ; <nl> } else {
static void do_video_out ( AVFormatContext * s , <nl> if (! ost -> last_frame ) <nl> ost -> last_frame = av_frame_alloc (); <nl> av_frame_unref ( ost -> last_frame ); <nl> - if ( next_picture ) <nl> + if ( next_picture && ost -> last_frame ) <nl> av_frame_ref ( ost -> last_frame , next_picture ); <nl> else <nl> av_frame_free (& ost -> last_frame );
static int mkv_write_header ( AVFormatContext * s ) <nl> // initialize stream_duration fields <nl> mkv -> stream_durations = av_mallocz ( s -> nb_streams * sizeof ( int64_t )); <nl> mkv -> stream_duration_offsets = av_mallocz ( s -> nb_streams * sizeof ( int64_t )); <nl> + if (! mkv -> stream_durations || ! mkv -> stream_duration_offsets ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl>  <nl> ret = mkv_write_tracks ( s ); <nl> if ( ret < 0 )
fail : <nl> fail_at_end : <nl> av_freep (& pic -> codec_picture_params ); <nl> av_frame_free (& pic -> recon_image ); <nl> + av_buffer_unref (& pic -> output_buffer_ref ); <nl> + pic -> output_buffer = VA_INVALID_ID ; <nl> return err ; <nl> } <nl> 
static int yop_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> s -> low_nibble = NULL ; <nl>  <nl> is_odd_frame = avpkt -> data [ 0 ]; <nl> + if ( is_odd_frame > 1 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " frame is too odd % d \ n ", is_odd_frame ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> firstcolor = s -> first_color [ is_odd_frame ]; <nl> palette = ( uint32_t *) s -> frame . data [ 1 ]; <nl> 
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> s -> channel_in_cpl [ ch ] = 0 ; <nl> s -> first_cpl_coords [ ch ] = 1 ; <nl> } <nl> - s -> first_cpl_leak = 1 ; <nl> + s -> first_cpl_leak = s -> eac3 ; <nl> s -> phase_flags_in_use = 0 ; <nl> } <nl> } else if (! s -> eac3 ) {
static int dfa_probe ( AVProbeData * p ) <nl> if ( p -> buf_size < 4 || AV_RL32 ( p -> buf ) != MKTAG (' D ', ' F ', ' I ', ' A ')) <nl> return 0 ; <nl>  <nl> + if ( AV_RL32 ( p -> buf + 16 ) != 0x80 ) <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
static void imc_get_coeffs ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_WARNING , <nl> " Potential problem on band % i , coefficient % i " <nl> ": cw_len =% i \ n ", i , j , cw_len ); <nl> - } <nl> - <nl> - cw = get_bits (& q -> gb , cw_len ); <nl> + } else <nl> + cw = get_bits (& q -> gb , cw_len ); <nl> } <nl>  <nl> chctx -> codewords [ j ] = cw ;
ImgReSampleContext * img_resample_full_init ( int owidth , int oheight , <nl> { <nl> ImgReSampleContext * s ; <nl>  <nl> + if (! owidth || ! oheight || ! iwidth || ! iheight ) <nl> + return NULL ; <nl> + <nl> s = av_mallocz ( sizeof ( ImgReSampleContext )); <nl> if (! s ) <nl> return NULL ;
static int qsv_decode_init ( AVCodecContext * avctx , QSVContext * q ) <nl> const AVPixFmtDescriptor * desc ; <nl> mfxSession session = NULL ; <nl> int iopattern = 0 ; <nl> - mfxVideoParam param = { { 0 } }; <nl> + mfxVideoParam param = { 0 }; <nl> int frame_width = avctx -> coded_width ; <nl> int frame_height = avctx -> coded_height ; <nl> int ret ;
static int apng_write_packet ( AVFormatContext * format_context , AVPacket * packet ) <nl> int ret ; <nl>  <nl> if (! apng -> prev_packet ) { <nl> - apng -> prev_packet = av_malloc ( sizeof (* apng -> prev_packet )); <nl> + apng -> prev_packet = av_packet_alloc (); <nl> if (! apng -> prev_packet ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int decode_byterun ( uint8_t * dst , int dst_size , <nl> } <nl> x += length ; <nl> } <nl> + if ( x < dst_size ) { <nl> + av_log ( NULL , AV_LOG_WARNING , " decode_byterun ended before plane size \ n "); <nl> + memset ( dst + x , 0 , dst_size - x ); <nl> + } <nl> return buf - buf_start ; <nl> } <nl> 
LF_IFUNC ( v , chroma_intra , depth , avx ) <nl> LF_FUNCS ( uint8_t , 8 ) <nl> LF_FUNCS ( uint16_t , 10 ) <nl>  <nl> -# if ARCH_X86_32 <nl> +# if ARCH_X86_32 && HAVE_YASM <nl> LF_FUNC ( v8 , luma , 8 , mmxext ) <nl> static void ff_deblock_v_luma_8_mmxext ( uint8_t * pix , int stride , int alpha , int beta , int8_t * tc0 ) <nl> {
static int cavs_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> if (! s -> low_delay && h -> DPB [ 0 ]. f . data [ 0 ]) { <nl> * data_size = sizeof ( AVPicture ); <nl> * picture = h -> DPB [ 0 ]. f ; <nl> + memset (& h -> DPB [ 0 ], 0 , sizeof ( h -> DPB [ 0 ])); <nl> } <nl> return 0 ; <nl> }
SchroFrame * ff_create_schro_frame ( AVCodecContext * avccontext , <nl> uv_height = y_height >> ( SCHRO_FRAME_FORMAT_V_SHIFT ( schro_frame_fmt )); <nl>  <nl> p_pic = av_mallocz ( sizeof ( AVPicture )); <nl> - avpicture_alloc ( p_pic , avccontext -> pix_fmt , y_width , y_height ); <nl> + if (! p_pic || avpicture_alloc ( p_pic , avccontext -> pix_fmt , y_width , y_height ) < 0 ) { <nl> + av_free ( p_pic ); <nl> + return NULL ; <nl> + } <nl>  <nl> p_frame = schro_frame_new (); <nl> p_frame -> format = schro_frame_fmt ;
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> payload_type , config ? config : ""); <nl> break ; <nl> case CODEC_ID_AAC : <nl> - if ( fmt && fmt -> oformat -> priv_class && <nl> + if ( fmt && fmt -> oformat && fmt -> oformat -> priv_class && <nl> av_opt_flag_is_set ( fmt -> priv_data , " rtpflags ", " latm ")) { <nl> config = latm_context2config ( c ); <nl> if (! config )
static int fdk_aac_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size , 0 ); <nl> + err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size / sizeof ( INT_PCM ), 0 ); <nl> if ( err == AAC_DEC_NOT_ENOUGH_BITS ) { <nl> ret = avpkt -> size - valid ; <nl> goto end ;
static int get_dimension ( GetBitContext * gb , const int * dim ) <nl> val = dim [ get_bits1 ( gb ) - val ]; <nl> if (! val ){ <nl> do { <nl> + if ( get_bits_left ( gb ) < 8 ) <nl> + return AVERROR_INVALIDDATA ; <nl> t = get_bits ( gb , 8 ); <nl> val += t << 2 ; <nl> } while ( t == 0xFF );
static int crypto_open ( URLContext * h , const char * uri , int flags ) <nl>  <nl> return 0 ; <nl> err : <nl> - av_free ( c -> key ); <nl> - av_free ( c -> iv ); <nl> + av_freep (& c -> key ); <nl> + av_freep (& c -> iv ); <nl> return ret ; <nl> } <nl> 
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> least one frame of codec data , this makes sure the codec initializes <nl> the channel configuration and does not only trust the values from the container . <nl> */ <nl> - try_decode_frame ( st , pkt , ( options && i <= orig_nb_streams )? & options [ i ] : NULL ); <nl> + try_decode_frame ( st , pkt , ( options && i < orig_nb_streams )? & options [ i ] : NULL ); <nl>  <nl> st -> codec_info_nb_frames ++; <nl> count ++;
void ff_mpeg_unref_picture ( MpegEncContext * s , Picture * pic ) <nl>  <nl> av_buffer_unref (& pic -> hwaccel_priv_buf ); <nl>  <nl> + if ( pic -> needs_realloc ) <nl> + free_picture_tables ( pic ); <nl> + <nl> memset (( uint8_t *) pic + off , 0 , sizeof (* pic ) - off ); <nl> } <nl> 
static int cmv_decode_frame ( AVCodecContext * avctx , <nl> CmvContext * s = avctx -> priv_data ; <nl> const uint8_t * buf_end = buf + buf_size ; <nl>  <nl> + if ( buf_end - buf < EA_PREAMBLE_SIZE ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( AV_RL32 ( buf )== MVIh_TAG || AV_RB32 ( buf )== MVIh_TAG ) { <nl> cmv_process_header ( s , buf + EA_PREAMBLE_SIZE , buf_end ); <nl> return buf_size ;
static int ape_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl> if ( s -> fileversion < 3950 ) // previous versions overread two bytes <nl> buf_size += 2 ; <nl> - av_fast_malloc (& s -> data , & s -> data_size , buf_size ); <nl> + av_fast_padded_malloc (& s -> data , & s -> data_size , buf_size ); <nl> if (! s -> data ) <nl> return AVERROR ( ENOMEM ); <nl> s -> dsp . bswap_buf (( uint32_t *) s -> data , ( const uint32_t *) buf , buf_size >> 2 );
static void init_uni_ac_vlc ( RLTable * rl , uint8_t * uni_ac_vlc_len ){ <nl> for ( i = 0 ; i < 128 ; i ++){ <nl> int level = i - 64 ; <nl> int run ; <nl> + if (! level ) <nl> + continue ; <nl> for ( run = 0 ; run < 64 ; run ++){ <nl> int len , bits , code ; <nl> 
int ff_dirac_golomb_read_16bit ( DiracGolombLUT * lut_ctx , const uint8_t * buf , <nl> if (( c_idx + 1 ) > coeffs ) <nl> return c_idx ; <nl>  <nl> + if ( res_bits >= RSIZE_BITS ) <nl> + res_bits = res = 0 ; <nl> + <nl> if ( res_bits && l -> sign ) { <nl> int32_t coeff = 1 ; <nl> APPEND_RESIDUE ( res , l -> preamble );
decode_intra_mb : <nl>  <nl> dquant = get_se_golomb (& sl -> gb ); <nl>  <nl> - sl -> qscale += dquant ; <nl> + sl -> qscale += ( unsigned ) dquant ; <nl>  <nl> if ((( unsigned ) sl -> qscale ) > max_qp ){ <nl> if ( sl -> qscale < 0 ) sl -> qscale += max_qp + 1 ;
enum AVCodecID av_codec_get_id ( const AVCodecTag * const * tags , unsigned int tag ) <nl> static void compute_chapters_end ( AVFormatContext * s ) <nl> { <nl> unsigned int i , j ; <nl> - int64_t max_time = s -> duration + <nl> + int64_t max_time = 0 ; <nl> + <nl> + if ( s -> duration > 0 ) <nl> + max_time = s -> duration + <nl> (( s -> start_time == AV_NOPTS_VALUE ) ? 0 : s -> start_time ); <nl>  <nl> for ( i = 0 ; i < s -> nb_chapters ; i ++)
static void apply_channel_coupling ( AC3EncodeContext * s ) <nl> # else <nl> int32_t (* fixed_cpl_coords )[ AC3_MAX_CHANNELS ][ 16 ] = cpl_coords ; <nl> # endif <nl> - int blk , ch , bnd , i , j ; <nl> + int av_uninit ( blk ), ch , bnd , i , j ; <nl> CoefSumType energy [ AC3_MAX_BLOCKS ][ AC3_MAX_CHANNELS ][ 16 ] = {{{ 0 }}}; <nl> int cpl_start , num_cpl_coefs ; <nl> 
static int submit_stats ( AVCodecContext * avctx ) <nl> } <nl> h -> stats_size = strlen ( avctx -> stats_in ) * 3 / 4 ; <nl> h -> stats = av_malloc ( h -> stats_size ); <nl> + if (! h -> stats ) { <nl> + h -> stats_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> h -> stats_size = av_base64_decode ( h -> stats , avctx -> stats_in , h -> stats_size ); <nl> } <nl> while ( h -> stats_size - h -> stats_offset > 0 ) {
static int open_slave ( AVFormatContext * avf , char * slave , TeeSlave * tee_slave ) <nl>  <nl> end : <nl> av_free ( format ); <nl> + av_free ( select ); <nl> av_dict_free (& options ); <nl> return ret ; <nl> }
static int load_apply_palette ( FFFrameSync * fs ) <nl>  <nl> error : <nl> av_frame_free (& master ); <nl> - av_frame_free (& second ); <nl> return ret ; <nl> } <nl> 
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( avctx -> codec_id == AV_CODEC_ID_VC1 || avctx -> codec_id == AV_CODEC_ID_VC1IMAGE ) { <nl> int buf_size2 = 0 ; <nl> buf2 = av_mallocz ( buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! buf2 ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( IS_MARKER ( AV_RB32 ( buf ))) { /* frame starts with marker and needs to be parsed */ <nl> const uint8_t * start , * end , * next ;
av_cold int ff_yuv2rgb_c_init_tables ( SwsContext * c , const int inv_table [ 4 ], int <nl> uint8_t * y_table ; <nl> uint16_t * y_table16 ; <nl> uint32_t * y_table32 ; <nl> - int i , base , rbase , gbase , bbase , abase , needAlpha ; <nl> + int i , base , rbase , gbase , bbase , av_uninit ( abase ), needAlpha ; <nl> const int yoffs = fullRange ? 384 : 326 ; <nl>  <nl> int64_t crv = inv_table [ 0 ];
static av_cold int cook_decode_init ( AVCodecContext * avctx ) <nl> int extradata_size = avctx -> extradata_size ; <nl> int s = 0 ; <nl> unsigned int channel_mask = 0 ; <nl> - int samples_per_frame ; <nl> + int samples_per_frame = 0 ; <nl> int ret ; <nl> q -> avctx = avctx ; <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> flush_dpb ( s -> avctx ); <nl> ff_MPV_common_end ( s ); <nl> h -> list_count = 0 ; <nl> + h -> current_slice = 0 ; <nl> } <nl> if (! s -> context_initialized ) { <nl> if ( h != h0 ) {
AVInputFormat ff_ivr_demuxer = { <nl> . read_probe = ivr_probe , <nl> . read_header = ivr_read_header , <nl> . read_packet = ivr_read_packet , <nl> + . read_close = rm_read_close , <nl> . extensions = " ivr ", <nl> };
void avcodec_flush_buffers ( AVCodecContext * avctx ) <nl> ff_thread_flush ( avctx ); <nl> else if ( avctx -> codec -> flush ) <nl> avctx -> codec -> flush ( avctx ); <nl> + <nl> + if (! avctx -> refcounted_frames ) <nl> + av_frame_unref (& avctx -> internal -> to_free ); <nl> } <nl>  <nl> int av_get_exact_bits_per_sample ( enum AVCodecID codec_id )
end : <nl> int ff_get_buffer ( AVCodecContext * avctx , AVFrame * frame , int flags ) <nl> { <nl> int ret = get_buffer_internal ( avctx , frame , flags ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> + frame -> width = frame -> height = 0 ; <nl> + } <nl> return ret ; <nl> } <nl> 
static int query_formats ( AVFilterContext * ctx ) <nl>  <nl> static int glyph_enu_free ( void * opaque , void * elem ) <nl> { <nl> + Glyph * glyph = elem ; <nl> + <nl> + FT_Done_Glyph (* glyph -> glyph ); <nl> + av_freep (& glyph -> glyph ); <nl> av_free ( elem ); <nl> return 0 ; <nl> }
static int decode_lt_rps ( HEVCContext * s , LongTermRPS * rps , GetBitContext * gb ) <nl> nb_sps = get_ue_golomb_long ( gb ); <nl> nb_sh = get_ue_golomb_long ( gb ); <nl>  <nl> - if ( nb_sh + nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> + if ( nb_sh + ( uint64_t ) nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> rps -> nb_refs = nb_sh + nb_sps ;
AVStream * avformat_new_stream ( AVFormatContext * s , const AVCodec * c ) <nl> } <nl>  <nl> st -> codec = avcodec_alloc_context3 ( c ); <nl> + if (! st -> codec ) { <nl> + av_free ( st -> info ); <nl> + av_free ( st ); <nl> + return NULL ; <nl> + } <nl> if ( s -> iformat ) { <nl> /* no default bitrate if decoding */ <nl> st -> codec -> bit_rate = 0 ;
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> { <nl> libx265Context * ctx = avctx -> priv_data ; <nl> x265_picture x265pic ; <nl> - x265_picture x265pic_out = { { 0 } }; <nl> + x265_picture x265pic_out = { 0 }; <nl> x265_nal * nal ; <nl> uint8_t * dst ; <nl> int payload = 0 ;
AVInputFormat * av_probe_input_format3 ( AVProbeData * pd , int is_opened , int * score <nl> AVProbeData lpd = * pd ; <nl> AVInputFormat * fmt1 = NULL , * fmt ; <nl> int score , nodat = 0 , score_max = 0 ; <nl> + const static uint8_t zerobuffer [ AVPROBE_PADDING_SIZE ]; <nl> + <nl> + if (! lpd . buf ) <nl> + lpd . buf = zerobuffer ; <nl>  <nl> if ( lpd . buf_size > 10 && ff_id3v2_match ( lpd . buf , ID3v2_DEFAULT_MAGIC )) { <nl> int id3len = ff_id3v2_tag_len ( lpd . buf );
void avformat_free_context ( AVFormatContext * s ) <nl> av_dict_free (& s -> metadata ); <nl> av_dict_free (& s -> internal -> id3v2_meta ); <nl> av_freep (& s -> streams ); <nl> - av_freep (& s -> internal ); <nl> flush_packet_queue ( s ); <nl> + av_freep (& s -> internal ); <nl> av_free ( s ); <nl> } <nl> 
static int decode_5 ( SANMVideoContext * ctx ) <nl> # if HAVE_BIGENDIAN <nl> npixels = ctx -> npixels ; <nl> frm = ctx -> frm0 ; <nl> - while ( npixels --) <nl> - * frm ++ = av_bswap16 (* frm ); <nl> + while ( npixels --) { <nl> + * frm = av_bswap16 (* frm ); <nl> + frm ++; <nl> + } <nl> # endif <nl>  <nl> return 0 ;
static int decode_mime_header ( AMRWBContext * ctx , const uint8_t * buf ) <nl> { <nl> /* Decode frame header ( 1st octet ) */ <nl> ctx -> fr_cur_mode = buf [ 0 ] >> 3 & 0x0F ; <nl> - ctx -> fr_quality = ( buf [ 0 ] & 0x4 ) != 0x4 ; <nl> + ctx -> fr_quality = ( buf [ 0 ] & 0x4 ) == 0x4 ; <nl>  <nl> return 1 ; <nl> }
static int ljpeg_decode_rgb_scan ( MJpegDecodeContext * s , int nb_components , int p <nl> int resync_mb_y = 0 ; <nl> int resync_mb_x = 0 ; <nl>  <nl> + if ( s -> nb_components != 3 && s -> nb_components != 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + if ( s -> v_max != 1 || s -> h_max != 1 || ! s -> lossless ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> + <nl> s -> restart_count = s -> restart_interval ; <nl>  <nl> av_fast_malloc (& s -> ljpeg_buffer , & s -> ljpeg_buffer_size ,
static av_cold int encode_init ( AVCodecContext * avc_context ) <nl>  <nl> /* Set up the output AVFrame */ <nl> avc_context -> coded_frame = av_frame_alloc (); <nl> + if (! avc_context -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> return 0 ; <nl> }
int avcodec_default_reget_buffer ( AVCodecContext * s , AVFrame * pic ){ <nl> return s -> get_buffer ( s , pic ); <nl> } <nl>  <nl> + assert ( s -> pix_fmt == pic -> pix_fmt ); <nl> + <nl> /* If internal buffer type return the same buffer */ <nl> if ( pic -> type == FF_BUFFER_TYPE_INTERNAL ) { <nl> if ( s -> pkt ) pic -> pkt_pts = s -> pkt -> pts ;
static int mm_decode_intra ( MmContext * s , int half_horiz , int half_vert ) <nl> */ <nl> static int mm_decode_inter ( MmContext * s , int half_horiz , int half_vert ) <nl> { <nl> - int data_off = bytestream2_get_le16 (& s -> gb ), y ; <nl> + int data_off = bytestream2_get_le16 (& s -> gb ); <nl> + int y = 0 ; <nl> GetByteContext data_ptr ; <nl>  <nl> if ( bytestream2_get_bytes_left (& s -> gb ) < data_off )
static int rtsp_read_header ( AVFormatContext * s , <nl> rt -> real_setup_cache = av_mallocz ( 2 * s -> nb_streams * sizeof (* rt -> real_setup_cache )); <nl> if (! rt -> real_setup_cache ) <nl> return AVERROR ( ENOMEM ); <nl> - rt -> real_setup = rt -> real_setup_cache + s -> nb_streams * sizeof (* rt -> real_setup ); <nl> + rt -> real_setup = rt -> real_setup_cache + s -> nb_streams ; <nl>  <nl> if ( ap -> initial_pause ) { <nl> /* do not start immediately */
static int mpeg4_decode_header ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl> if ( avctx -> extradata_size && pc -> first_picture ) { <nl> init_get_bits ( gb , avctx -> extradata , avctx -> extradata_size * 8 ); <nl> ret = ff_mpeg4_decode_picture_header ( dec_ctx , gb ); <nl> + if ( ret < 0 ) <nl> + av_log ( avctx , AV_LOG_WARNING , " Failed to parse extradata \ n "); <nl> } <nl>  <nl> init_get_bits ( gb , buf , 8 * buf_size );
retry : <nl> if ( snprintf ( str , str_size_alloc , "% f ", val ) >= str_size_alloc ) { <nl> av_log ( c -> fc , AV_LOG_ERROR , <nl> " Failed to store the float32 number (% f ) in string .\ n ", val ); <nl> + av_free ( str ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> } else {
int vc1_parse_frame_header_adv ( VC1Context * v , GetBitContext * gb ) <nl> v -> rnd = get_bits1 ( gb ); <nl> if ( v -> interlace ) <nl> v -> uvsamp = get_bits1 ( gb ); <nl> + if (! ff_vc1_bfraction_vlc . table ) <nl> + return 0 ; // parsing only , vlc tables havnt been allocated <nl> if ( v -> field_mode ) { <nl> if (! v -> refdist_flag ) <nl> v -> refdist = 0 ;
void ff_ivi_recompose53 ( const IVIPlaneDesc * plane , uint8_t * dst , <nl> b3_ptr = plane -> bands [ 3 ]. buf ; <nl>  <nl> for ( y = 0 ; y < plane -> height ; y += 2 ) { <nl> + <nl> + if ( y + 2 >= plane -> height ) <nl> + pitch = 0 ; <nl> /* load storage variables with values */ <nl> if ( num_bands > 0 ) { <nl> b0_1 = b0_ptr [ 0 ];
int av_find_best_stream ( AVFormatContext * ic , enum AVMediaType type , <nl> st -> disposition & ( AV_DISPOSITION_HEARING_IMPAIRED | <nl> AV_DISPOSITION_VISUAL_IMPAIRED )) <nl> continue ; <nl> - if ( type == AVMEDIA_TYPE_AUDIO && ! avctx -> channels ) <nl> + if ( type == AVMEDIA_TYPE_AUDIO && !( avctx -> channels && avctx -> sample_rate )) <nl> continue ; <nl> if ( decoder_ret ) { <nl> decoder = find_decoder ( ic , st , st -> codec -> codec_id );
int ff_mpeg4_frame_end ( AVCodecContext * avctx , const uint8_t * buf , int buf_size ) <nl> } <nl>  <nl> if ( startcode_found ) { <nl> - av_fast_malloc (& s -> bitstream_buffer , <nl> + av_fast_padded_malloc (& s -> bitstream_buffer , <nl> & s -> allocated_bitstream_buffer_size , <nl> - buf_size - current_pos + <nl> - FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + buf_size - current_pos ); <nl> if (! s -> bitstream_buffer ) <nl> return AVERROR ( ENOMEM ); <nl> memcpy ( s -> bitstream_buffer , buf + current_pos ,
static av_always_inline void decode_bgr_1 ( HYuvContext * s , int count , <nl> index = SHOW_UBITS ( re , & s -> gb , VLC_BITS ); <nl> VLC_INTERN ( s -> temp [ 0 ][ 4 * i + A ], s -> vlc [ 2 ]. table , <nl> & s -> gb , re , VLC_BITS , 3 ); <nl> - } <nl> + } else <nl> + s -> temp [ 0 ][ 4 * i + A ] = 0 ; <nl> } <nl> } <nl> CLOSE_READER ( re , & s -> gb );
static int ape_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR_EOF ; <nl> - if ( ape -> currentframe > ape -> totalframes ) <nl> + if ( ape -> currentframe >= ape -> totalframes ) <nl> return AVERROR_EOF ; <nl>  <nl> if ( avio_seek ( s -> pb , ape -> frames [ ape -> currentframe ]. pos , SEEK_SET ) < 0 )
static void force_codec_ids ( AVFormatContext * s , AVStream * st ) <nl> break ; <nl> case AVMEDIA_TYPE_DATA : <nl> if ( s -> data_codec_id ) <nl> - st -> codec -> codec_id = s -> data_codec_id ; <nl> + st -> codecpar -> codec_id = s -> data_codec_id ; <nl> break ; <nl> } <nl> }
static av_always_inline SoftFloat autocorr_calc ( int64_t accu ) <nl>  <nl> round = 1U << ( nz - 1 ); <nl> mant = ( int )(( accu + round ) >> nz ); <nl> - mant = ( mant + 0x40 )>> 7 ; <nl> + mant = ( mant + 0x40LL )>> 7 ; <nl> mant *= 64 ; <nl> expo = nz + 15 ; <nl> return av_int2sf ( mant , 30 - expo );
static int mov_read_header ( AVFormatContext * s ) <nl> if ( s -> streams [ j ]-> id == sc -> timecode_track ) <nl> tmcd_st_id = j ; <nl>  <nl> - if ( tmcd_st_id < 0 ) <nl> + if ( tmcd_st_id < 0 || tmcd_st_id == i ) <nl> continue ; <nl> tcr = av_dict_get ( s -> streams [ tmcd_st_id ]-> metadata , " timecode ", NULL , 0 ); <nl> if ( tcr )
static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> char * p ; <nl> int file_idx = strtol ( in -> name , & p , 0 ); <nl>  <nl> - if ( file_idx < 0 || file_idx > nb_input_files ) { <nl> + if ( file_idx < 0 || file_idx >= nb_input_files ) { <nl> av_log ( NULL , AV_LOG_FATAL , " Invalid file index % d in filtegraph description % s .\ n ", <nl> file_idx , fg -> graph_desc ); <nl> exit_program ( 1 );
static int qsv_decode ( AVCodecContext * avctx , QSVContext * q , <nl>  <nl> do { <nl> ret = get_surface ( avctx , q , & insurf ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& sync ); <nl> return ret ; <nl> + } <nl>  <nl> ret = MFXVideoDECODE_DecodeFrameAsync ( q -> session , avpkt -> size ? & bs : NULL , <nl> insurf , & outsurf , sync );
static int parse_playlist ( HLSContext * c , const char * url , <nl> uint8_t iv [ 16 ] = ""; <nl> int has_iv = 0 ; <nl> char key [ MAX_URL_SIZE ] = ""; <nl> - char line [ 1024 ]; <nl> + char line [ MAX_URL_SIZE ]; <nl> const char * ptr ; <nl> int close_in = 0 ; <nl> 
static int bit_allocation ( IMCContext * q , IMCChannel * chctx , <nl> iacc += chctx -> bandWidthT [ i ]; <nl> summa += chctx -> bandWidthT [ i ] * chctx -> flcoeffs4 [ i ]; <nl> } <nl> + <nl> + if (! iacc ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> chctx -> bandWidthT [ BANDS - 1 ] = 0 ; <nl> summa = ( summa * 0 . 5 - freebits ) / iacc ; <nl> 
static int decode_hq_slice ( DiracContext * s , DiracSlice * slice , uint8_t * tmp_buf ) <nl> skip_bits_long ( gb , 8 * s -> highquality . prefix_bytes ); <nl> quant_idx = get_bits ( gb , 8 ); <nl>  <nl> - if ( quant_idx > DIRAC_MAX_QUANT_INDEX ) { <nl> + if ( quant_idx > DIRAC_MAX_QUANT_INDEX - 1 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid quantization index - % i \ n ", quant_idx ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static inline int msmpeg4_decode_block ( MpegEncContext * s , DCTELEM * block , <nl> if ( i > 62 ){ <nl> i -= 192 ; <nl> if ( i &(~ 63 )){ <nl> - if ( s -> error_resilience < 0 ){ <nl> + if (( i + 192 == 64 && level / qmul ==- 1 ) || s -> error_resilience < 0 ){ <nl> fprintf ( stderr , " ignoring overflow at % d % d \ n ", s -> mb_x , s -> mb_y ); <nl> break ; <nl> } else {
av_cold int ffv1_init_slice_contexts ( FFV1Context * f ) <nl> int i ; <nl>  <nl> f -> slice_count = f -> num_h_slices * f -> num_v_slices ; <nl> + if ( f -> slice_count <= 0 ) { <nl> + av_log ( f -> avctx , AV_LOG_ERROR , " Invalid number of slices \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl>  <nl> for ( i = 0 ; i < f -> slice_count ; i ++) { <nl> FFV1Context * fs = av_mallocz ( sizeof (* fs ));
static int dirac_decode_picture_header ( DiracContext * s ) <nl> if (! s -> all_frames [ j ]. avframe . data [ 0 ]) { <nl> s -> ref_pics [ i ] = & s -> all_frames [ j ]; <nl> s -> avctx -> get_buffer ( s -> avctx , & s -> ref_pics [ i ]-> avframe ); <nl> + break ; <nl> } <nl> } <nl> 
void avcodec_free_context ( AVCodecContext ** pavctx ) <nl>  <nl> av_freep (& avctx -> extradata ); <nl> av_freep (& avctx -> subtitle_header ); <nl> + av_freep (& avctx -> intra_matrix ); <nl> + av_freep (& avctx -> inter_matrix ); <nl> + av_freep (& avctx -> rc_override ); <nl>  <nl> av_freep ( pavctx ); <nl> }
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl> fail : <nl> if ( filename != s -> filename ) <nl> av_freep (& filename ); <nl> + if ( rc ) <nl> + RTMP_Close ( r ); <nl> + <nl> return rc ; <nl> } <nl> 
static int expand_rle_row16 ( SgiState * s , uint16_t * out_buf , <nl> break ; <nl>  <nl> /* Check for buffer overflow . */ <nl> - if ( pixelstride * ( count - 1 ) >= len ) { <nl> + if ( out_end - out_buf <= pixelstride * ( count - 1 )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid pixel count .\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static inline int parse_command_line ( AVFormatContext * s , const char * line , <nl> RTSPState * rt = s -> priv_data ; <nl> const char * linept , * searchlinept ; <nl> linept = strchr ( line , ' '); <nl> + <nl> + if (! linept ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( linept - line > methodsize - 1 ) { <nl> av_log ( s , AV_LOG_ERROR , " Method string too long \ n "); <nl> return AVERROR ( EIO );
static int film_read_packet ( AVFormatContext * s , <nl> av_free ( film -> stereo_buffer ); <nl> film -> stereo_buffer_size = sample -> sample_size ; <nl> film -> stereo_buffer = av_malloc ( film -> stereo_buffer_size ); <nl> + if (! film -> stereo_buffer ) { <nl> + film -> stereo_buffer_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> pkt -> pos = avio_tell ( pb );
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> s -> buf = & ptr ; <nl> s -> buf_size = pkt -> size ; <nl>  <nl> - if ( check_size ( s , 8 )) <nl> + if ( check_size ( s , 8 )) { <nl> + ret = AVERROR ( EINVAL ); <nl> goto fail ; <nl> + } <nl>  <nl> // write header <nl> bytestream_put_le16 (& ptr , 0x4949 );
static av_cold int decode_end ( AVCodecContext * avctx ) <nl> { <nl> MadContext * t = avctx -> priv_data ; <nl> av_frame_free (& t -> last_frame ); <nl> - av_free ( t -> bitstream_buf ); <nl> + av_freep (& t -> bitstream_buf ); <nl> return 0 ; <nl> } <nl> 
static void exit_program ( void ) <nl> /* close files */ <nl> for ( i = 0 ; i < nb_output_files ; i ++) { <nl> AVFormatContext * s = output_files [ i ]-> ctx ; <nl> - if (!( s -> oformat -> flags & AVFMT_NOFILE ) && s -> pb ) <nl> + if ( s && s -> oformat && !( s -> oformat -> flags & AVFMT_NOFILE ) && s -> pb ) <nl> avio_close ( s -> pb ); <nl> avformat_free_context ( s ); <nl> av_dict_free (& output_files [ i ]-> opts );
int ff_mjpeg_find_marker ( MJpegDecodeContext * s , <nl> while ( b < t ) { <nl> uint8_t x = src [ b ++]; <nl> put_bits (& pb , 8 , x ); <nl> - if ( x == 0xFF ) { <nl> + if ( x == 0xFF && b < t ) { <nl> x = src [ b ++]; <nl> if ( x & 0x80 ) { <nl> av_log ( s -> avctx , AV_LOG_WARNING , " Invalid escape sequence \ n ");
int ff_h264_decode_slice_header ( H264Context * h , H264SliceContext * sl ) <nl> } <nl>  <nl> if ( h -> context_initialized && needs_reinit ) { <nl> + h -> context_initialized = 0 ; <nl> if ( sl != h -> slice_ctx ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> " changing width % d -> % d / height % d -> % d on "
SwsContext * sws_alloc_context ( void ) <nl> { <nl> SwsContext * c = av_mallocz ( sizeof ( SwsContext )); <nl>  <nl> - c -> av_class = & sws_context_class ; <nl> - av_opt_set_defaults ( c ); <nl> + if ( c ) { <nl> + c -> av_class = & sws_context_class ; <nl> + av_opt_set_defaults ( c ); <nl> + } <nl>  <nl> return c ; <nl> }
const char * av_opencl_errstr ( cl_int status ) <nl> static void free_device_list ( AVOpenCLDeviceList * device_list ) <nl> { <nl> int i , j ; <nl> - if (! device_list ) <nl> + if (! device_list || ! device_list -> platform_node ) <nl> return ; <nl> for ( i = 0 ; i < device_list -> platform_num ; i ++) { <nl> if (! device_list -> platform_node [ i ])
static int get_stream_info ( AVCodecContext * avctx ) <nl>  <nl> for ( i = 0 ; i < info -> numChannels ; i ++) { <nl> AUDIO_CHANNEL_TYPE ctype = info -> pChannelType [ i ]; <nl> - if ( ctype <= ACT_NONE || ctype > FF_ARRAY_ELEMS ( channel_counts )) { <nl> + if ( ctype <= ACT_NONE || ctype >= FF_ARRAY_ELEMS ( channel_counts )) { <nl> av_log ( avctx , AV_LOG_WARNING , " unknown channel type \ n "); <nl> break ; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
void decode_mvs ( VP8Context * s , VP8Macroblock * mb , int mb_x , int mb_y ) <nl>  <nl> AV_ZERO32 (& near_mv [ 0 ]); <nl> AV_ZERO32 (& near_mv [ 1 ]); <nl> + AV_ZERO32 (& near_mv [ 2 ]); <nl>  <nl> /* Process MB on top , left and top - left */ <nl> # define MV_EDGE_CHECK ( n )\
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( cdxl -> framerate ) <nl> st -> duration = frames ; <nl> else <nl> - st -> duration = frames * audio_size ; <nl> + st -> duration = frames * ( int64_t ) audio_size ; <nl> } <nl> st -> start_time = 0 ; <nl> cdxl -> video_stream_index = st -> index ;
static int mp3_read_probe ( AVProbeData * p ) <nl> const uint8_t * buf , * buf0 , * buf2 , * end ; <nl> AVCodecContext * avctx = avcodec_alloc_context3 ( NULL ); <nl>  <nl> + if (! avctx ) <nl> + return 0 ; <nl> + <nl> buf0 = p -> buf ; <nl> end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl> while ( buf0 < end && !* buf0 )
static int matroska_parse_cluster_incremental ( MatroskaDemuxContext * matroska ) <nl> } <nl> } <nl>  <nl> - if ( res < 0 ) matroska -> done = 1 ; <nl> return res ; <nl> } <nl> 
static int mpegts_read_packet ( AVFormatContext * s , <nl> MpegTSContext * ts = s -> priv_data ; <nl> int ret , i ; <nl>  <nl> + pkt -> size = - 1 ; <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> static int mpegts_read_packet ( AVFormatContext * s , <nl> } <nl> } <nl>  <nl> + if (! ret && pkt -> size < 0 ) <nl> + ret = AVERROR ( EINTR ); <nl> return ret ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> t = tm2_read_stream ( l , l -> buffer + offset , tm2_stream_order [ i ], <nl> buf_size - offset ); <nl> if ( t < 0 ) { <nl> + int j = tm2_stream_order [ i ]; <nl> + memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> return t ; <nl> } <nl> offset += t ;
static int decode_cell ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> /* of the predicted cell in order to avoid overflows . */ <nl> if ( vq_index >= 8 && ref_block ) { <nl> for ( x = 0 ; x < cell -> width << 2 ; x ++) <nl> - ref_block [ x ] = requant_tab [ vq_index & 7 ][ ref_block [ x ]]; <nl> + ref_block [ x ] = requant_tab [ vq_index & 7 ][ ref_block [ x ] & 127 ]; <nl> } <nl>  <nl> error = IV3_NOERR ;
static av_cold int MPA_encode_init ( AVCodecContext * avctx ) <nl> s -> freq_index = i ; <nl>  <nl> /* encoding bitrate & frequency */ <nl> - for ( i = 0 ; i < 15 ; i ++) { <nl> + for ( i = 1 ; i < 15 ; i ++) { <nl> if ( avpriv_mpa_bitrate_tab [ s -> lsf ][ 1 ][ i ] == bitrate ) <nl> break ; <nl> }
static inline uint32_t celt_icwrsi ( uint32_t N , uint32_t K , const int * y ) <nl> idx += CELT_PVQ_U ( N - i , sum ) + ( y [ i ] < 0 )* i_s ; <nl> sum += FFABS ( y [ i ]); <nl> } <nl> - av_assert0 ( sum == K ); <nl> return idx ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> AVFrame * const p = data ; <nl> int mb_x , mb_y , ret ; <nl>  <nl> - if ( buf_size * 8LL < a -> mb_height2 * a -> mb_width2 * 13LL ) <nl> + if ( buf_size * 8LL < a -> mb_height * a -> mb_width * 13LL ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 )
read_header : <nl>  <nl> // XXX FIXME factorize , this looks very similar to the EOI code <nl>  <nl> + if (! s -> got_picture ) { <nl> + av_log ( avctx , AV_LOG_WARNING , " no picture \ n "); <nl> + return buf_size ; <nl> + } <nl> + <nl> * picture = * s -> picture_ptr ; <nl> * data_size = sizeof ( AVFrame ); <nl> 
static void ffmpeg_cleanup ( int ret ) <nl> avcodec_free_context (& ost -> enc_ctx ); <nl> avcodec_parameters_free (& ost -> ref_par ); <nl>  <nl> - while ( av_fifo_size ( ost -> muxing_queue )) { <nl> + while ( ost -> muxing_queue && av_fifo_size ( ost -> muxing_queue )) { <nl> AVPacket pkt ; <nl> av_fifo_generic_read ( ost -> muxing_queue , & pkt , sizeof ( pkt ), NULL ); <nl> av_packet_unref (& pkt );
static int zerocodec_decode_frame ( AVCodecContext * avctx , void * data , <nl> pic -> key_frame = 1 ; <nl> pic -> pict_type = AV_PICTURE_TYPE_I ; <nl> } else { <nl> + if ( prev == NULL ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " No previous frame !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> pic -> key_frame = 0 ; <nl> pic -> pict_type = AV_PICTURE_TYPE_P ; <nl> }
unsigned ff_els_decode_unsigned ( ElsDecCtx * ctx , ElsUnsignedRung * ur ) <nl>  <nl> /* handle the error / overflow case */ <nl> if ( ctx -> err || n >= ELS_EXPGOLOMB_LEN ) { <nl> - ctx -> err = AVERROR ( EOVERFLOW ); <nl> + ctx -> err = AVERROR_INVALIDDATA ; <nl> return 0 ; <nl> } <nl> 
retry : <nl> } <nl> buf = c -> decomp_buf ; <nl> buf_size = c -> decomp_size - FFMAX ( FF_INPUT_BUFFER_PADDING_SIZE , AV_LZO_OUTPUT_PADDING ) - outlen ; <nl> + memset ( c -> decomp_buf + buf_size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> } <nl> if ( c -> codec_frameheader ) { <nl> int w , h , q ;
static void aw_pulse_set2 ( WMAVoiceContext * s , GetBitContext * gb , <nl> int excl_range = s -> aw_pulse_range ; // always 16 or 24 <nl> uint16_t * use_mask_ptr = & use_mask [ idx >> 4 ]; <nl> int first_sh = 16 - ( idx & 15 ); <nl> - * use_mask_ptr ++ &= 0xFFFF << first_sh ; <nl> + * use_mask_ptr ++ &= 0xFFFFu << first_sh ; <nl> excl_range -= first_sh ; <nl> if ( excl_range >= 16 ) { <nl> * use_mask_ptr ++ = 0 ;
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (!* got_frame ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> update_canvas_size ( avctx , avctx -> width , avctx -> height ); <nl>  <nl> if ( s -> has_alpha ) {
static int mxf_read_close ( AVFormatContext * s ) <nl>  <nl> for ( i = 0 ; i < mxf -> metadata_sets_count ; i ++) { <nl> switch ( mxf -> metadata_sets [ i ]-> type ) { <nl> + case Descriptor : <nl> + av_freep (&(( MXFDescriptor *) mxf -> metadata_sets [ i ])-> extradata ); <nl> + break ; <nl> case MultipleDescriptor : <nl> av_freep (&(( MXFDescriptor *) mxf -> metadata_sets [ i ])-> sub_descriptors_refs ); <nl> break ;
ogm_header ( AVFormatContext * s , int idx ) <nl>  <nl> time_unit = bytestream2_get_le64 (& p ); <nl> spu = bytestream2_get_le64 (& p ); <nl> + if (! time_unit || ! spu ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid timing values .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> bytestream2_skip (& p , 4 ); /* default_len */ <nl> bytestream2_skip (& p , 8 ); /* buffersize + bits_per_sample */ <nl> 
AVFilter avfilter_af_asyncts = { <nl> . config_props = config_props , <nl> . request_frame = request_frame }, <nl> { NULL }}, <nl> - . priv_size = & asyncts_class , <nl> + . priv_class = & asyncts_class , <nl> };
static int transcode_init ( void ) <nl> FilterGraph * fg = filtergraphs [ i ]; <nl> for ( j = 0 ; j < fg -> nb_outputs ; j ++) { <nl> OutputFilter * ofilter = fg -> outputs [ j ]; <nl> - if ( ofilter -> ost -> source_index >= 0 ) <nl> + if (! ofilter -> ost || ofilter -> ost -> source_index >= 0 ) <nl> continue ; <nl> if ( fg -> nb_inputs != 1 ) <nl> continue ;
static av_cold int tta_decode_init ( AVCodecContext * avctx ) <nl> s -> data_length = get_bits_long (& s -> gb , 32 ); <nl> skip_bits (& s -> gb , 32 ); // CRC32 of header <nl>  <nl> + if ( s -> channels == 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> switch ( s -> bps ) { <nl> case 2 : <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ;
int ff_vaapi_render_picture ( struct vaapi_context * vactx , VASurfaceID surface ) <nl> VABufferID va_buffers [ 3 ]; <nl> unsigned int n_va_buffers = 0 ; <nl>  <nl> + if (! vactx -> pic_param_buf_id ) <nl> + return 0 ; <nl> + <nl> vaUnmapBuffer ( vactx -> display , vactx -> pic_param_buf_id ); <nl> va_buffers [ n_va_buffers ++] = vactx -> pic_param_buf_id ; <nl> 
static av_cold int init ( AVFilterContext * ctx ) <nl> int nb_formats = 1 ; <nl> int i ; <nl>  <nl> + if (! s -> pix_fmts ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Empty output format string .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> /* count the formats */ <nl> cur = s -> pix_fmts ; <nl> while (( cur = strchr ( cur , '|'))) {
static void dwt_decode97_int ( DWTContext * s , int32_t * t ) <nl> line += 5 ; <nl>  <nl> for ( i = 0 ; i < w * h ; i ++) <nl> - data [ i ] *= 1 << I_PRESHIFT ; <nl> + data [ i ] *= 1LL << I_PRESHIFT ; <nl>  <nl> for ( lev = 0 ; lev < s -> ndeclevels ; lev ++) { <nl> int lh = s -> linelen [ lev ][ 0 ],
static int read_packet ( AVFormatContext * s , <nl> return 0 ; <nl>  <nl> case MM_TYPE_AUDIO : <nl> + if ( s -> nb_streams != 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , <nl> + " Unexpected audio packet , skipping \ n "); <nl> + avio_skip ( pb , length ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( av_get_packet ( s -> pb , pkt , length )< 0 ) <nl> return AVERROR ( ENOMEM ); <nl> pkt -> size = length ;
static int hls_read_header ( AVFormatContext * s ) <nl>  <nl> return 0 ; <nl> fail : <nl> - free_playlist_list ( c ); <nl> - free_variant_list ( c ); <nl> - free_rendition_list ( c ); <nl> + hls_close ( s ); <nl> return ret ; <nl> } <nl> 
void ff_convert_matrix ( MpegEncContext * s , int (* qmat )[ 64 ], <nl> qmat16 [ qscale ][ 0 ][ i ] == 128 * 256 ) <nl> qmat16 [ qscale ][ 0 ][ i ] = 128 * 256 - 1 ; <nl> qmat16 [ qscale ][ 1 ][ i ] = <nl> - ROUNDED_DIV ( bias << ( 16 - QUANT_BIAS_SHIFT ), <nl> + ROUNDED_DIV ( bias * ( 1 <<( 16 - QUANT_BIAS_SHIFT )), <nl> qmat16 [ qscale ][ 0 ][ i ]); <nl> } <nl> }
static int adx_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> AVCodecContext * avctx = s -> streams [ 0 ]-> codec ; <nl> int ret , size ; <nl>  <nl> + if ( avctx -> channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " invalid number of channels % d \ n ", avctx -> channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> size = BLOCK_SIZE * avctx -> channels ; <nl>  <nl> pkt -> pos = avio_tell ( s -> pb );
static AVCRC av_crc_table [ AV_CRC_MAX ][ 257 ]; <nl> * @ return < 0 on failure <nl> */ <nl> int av_crc_init ( AVCRC * ctx , int le , int bits , uint32_t poly , int ctx_size ){ <nl> - int i , j ; <nl> + unsigned i , j ; <nl> uint32_t c ; <nl>  <nl> if ( bits < 8 || bits > 32 || poly >= ( 1LL << bits ))
static int vp56_size_changed ( VP56Context * s ) <nl> s -> plane_height [ 0 ] = s -> plane_height [ 3 ] = avctx -> coded_height ; <nl> s -> plane_height [ 1 ] = s -> plane_height [ 2 ] = avctx -> coded_height / 2 ; <nl>  <nl> + s -> have_undamaged_frame = 0 ; <nl> + <nl> for ( i = 0 ; i < 4 ; i ++) <nl> s -> stride [ i ] = s -> flip * s -> frames [ VP56_FRAME_CURRENT ]-> linesize [ i ]; <nl> 
av_cold int ff_h264_decode_init ( AVCodecContext * avctx ) <nl> s -> low_delay = 0 ; <nl> } <nl>  <nl> + ff_init_cabac_states (); <nl> + <nl> return 0 ; <nl> } <nl>  <nl> static int decode_slice ( struct AVCodecContext * avctx , void * arg ) <nl> align_get_bits (& s -> gb ); <nl>  <nl> /* init cabac */ <nl> - ff_init_cabac_states (); <nl> ff_init_cabac_decoder (& h -> cabac , <nl> s -> gb . buffer + get_bits_count (& s -> gb ) / 8 , <nl> ( get_bits_left (& s -> gb ) + 7 ) / 8 );
static int mov_read_mac_string ( MOVContext * c , AVIOContext * pb , int len , <nl> uint8_t t , c = avio_r8 ( pb ); <nl> if ( c < 0x80 && p < end ) <nl> * p ++ = c ; <nl> - else <nl> + else if ( p < end ) <nl> PUT_UTF8 ( mac_to_unicode [ c - 0x80 ], t , if ( p < end ) * p ++ = t ;); <nl> } <nl> * p = 0 ;
static int imc_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> IMCContext * q = avctx -> priv_data ; <nl>  <nl> - LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 ]); <nl> + LOCAL_ALIGNED_16 ( uint16_t , buf16 , [( IMC_BLOCK_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ) / 2 ]); <nl>  <nl> if ( buf_size < IMC_BLOCK_SIZE * avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame too small !\ n ");
static int idcin_read_packet ( AVFormatContext * s , <nl> return ret ; <nl> else if ( ret != chunk_size ) { <nl> av_log ( s , AV_LOG_ERROR , " incomplete packet \ n "); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( EIO ); <nl> } <nl> if ( command == 1 ) {
void ff_rtsp_close_streams ( AVFormatContext * s ) <nl> if ( rtsp_st -> dynamic_handler && rtsp_st -> dynamic_protocol_context ) <nl> rtsp_st -> dynamic_handler -> close ( <nl> rtsp_st -> dynamic_protocol_context ); <nl> + av_free ( rtsp_st ); <nl> } <nl> } <nl> av_free ( rt -> rtsp_streams );
static int paf_video_decode ( AVCodecContext * avctx , void * data , <nl> bytestream2_init (& c -> gb , pkt -> data , pkt -> size ); <nl>  <nl> code = bytestream2_get_byte (& c -> gb ); <nl> - if (( code & 0xF ) > 4 ) { <nl> + if (( code & 0xF ) > 4 || ( code & 0xF ) == 3 ) { <nl> avpriv_request_sample ( avctx , " unknown / invalid code "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int mxf_compute_sample_count ( MXFContext * mxf , int stream_index , uint64_t <nl>  <nl> av_assert2 ( size ); <nl>  <nl> - * sample_count = ( mxf -> current_edit_unit / size ) * total ; <nl> + * sample_count = ( mxf -> current_edit_unit / size ) * ( uint64_t ) total ; <nl> for ( i = 0 ; i < mxf -> current_edit_unit % size ; i ++) { <nl> * sample_count += spf -> samples_per_frame [ i ]; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> inbuf_ptr = s -> inbuf ; <nl> s -> frame_size = 0 ; <nl> - * data_size = out_size ; <nl> + if ( out_size >= 0 ) <nl> + * data_size = out_size ; <nl> + else <nl> + av_log ( avctx , AV_LOG_DEBUG , " Error while decoding mpeg audio frame \ n "); // FIXME return - 1 / but also return the number of bytes consumed <nl> break ; <nl> } <nl> }
static int v4l2_set_parameters ( AVFormatContext * s1 ) <nl> standard . index = i ; <nl> if ( v4l2_ioctl ( s -> fd , VIDIOC_ENUMSTD , & standard ) < 0 ) { <nl> ret = AVERROR ( errno ); <nl> - if ( ret == AVERROR ( EINVAL )) { <nl> + if ( ret == AVERROR ( EINVAL ) || ret == AVERROR ( ENODATA )) { <nl> tpf = & streamparm . parm . capture . timeperframe ; <nl> break ; <nl> }
static int dnxhd_decode_header ( DNXHDContext * ctx , AVFrame * frame , <nl> frame -> top_field_first = first_field ^ ctx -> cur_field ; <nl> av_log ( ctx -> avctx , AV_LOG_DEBUG , <nl> " interlaced % d , cur field % d \ n ", buf [ 5 ] & 3 , ctx -> cur_field ); <nl> + } else { <nl> + ctx -> cur_field = 0 ; <nl> } <nl>  <nl> ctx -> height = AV_RB16 ( buf + 0x18 );
static av_cold int init ( AVFilterContext * ctx ) <nl> OCVContext * ocv = ctx -> priv ; <nl> int i ; <nl>  <nl> + if (! ocv -> name ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " No libopencv filter name specified \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( ocv_filter_entries ); i ++) { <nl> OCVFilterEntry * entry = & ocv_filter_entries [ i ]; <nl> if (! strcmp ( ocv -> name , entry -> name )) {
void ff_h264_direct_ref_list_init ( const H264Context * const h , H264SliceContext * <nl> memcpy ( cur -> ref_poc [ 1 ], cur -> ref_poc [ 0 ], sizeof ( cur -> ref_poc [ 0 ])); <nl> } <nl>  <nl> - cur -> mbaff = FRAME_MBAFF ( h ); <nl> + if ( h -> current_slice == 0 ) { <nl> + cur -> mbaff = FRAME_MBAFF ( h ); <nl> + } else { <nl> + av_assert0 ( cur -> mbaff == FRAME_MBAFF ( h )); <nl> + } <nl>  <nl> sl -> col_fieldoff = 0 ; <nl> 
static av_cold int vc2_encode_init ( AVCodecContext * avctx ) <nl> s -> num_x = s -> plane [ 0 ]. dwt_width / s -> slice_width ; <nl> s -> num_y = s -> plane [ 0 ]. dwt_height / s -> slice_height ; <nl>  <nl> - s -> slice_args = av_malloc ( s -> num_x * s -> num_y * sizeof ( SliceArgs )); <nl> + s -> slice_args = av_calloc ( s -> num_x * s -> num_y , sizeof ( SliceArgs )); <nl> if (! s -> slice_args ) <nl> goto alloc_fail ; <nl> 
static void ogg_free ( AVFormatContext * s ) <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> AVStream * st = s -> streams [ i ]; <nl> OGGStreamContext * oggstream = st -> priv_data ; <nl> + if (! oggstream ) <nl> + continue ; <nl> if ( st -> codecpar -> codec_id == AV_CODEC_ID_FLAC || <nl> st -> codecpar -> codec_id == AV_CODEC_ID_SPEEX || <nl> st -> codecpar -> codec_id == AV_CODEC_ID_OPUS ||
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> else v = buf_p - c -> bytestream_start ; <nl> if ( buf_p - c -> bytestream_start < v ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Slice pointer chain broken \ n "); <nl> + ff_thread_report_progress (& f -> picture , INT_MAX , 0 ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> buf_p -= v ;
static void FUNC ( dequant )( int16_t * coeffs , int16_t log2_size ) <nl> } else { <nl> for ( y = 0 ; y < size ; y ++) { <nl> for ( x = 0 ; x < size ; x ++) { <nl> - * coeffs = * coeffs << - shift ; <nl> + * coeffs = *( uint16_t *) coeffs << - shift ; <nl> coeffs ++; <nl> } <nl> }
static void apply_dependent_coupling_fixed ( AACContext * ac , <nl> for ( k = offsets [ i ]; k < offsets [ i + 1 ]; k ++) { <nl> tmp = ( int )((( int64_t ) src [ group * 128 + k ] * c + \ <nl> ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ group * 128 + k ] += tmp << shift ; <nl> + dest [ group * 128 + k ] += tmp * ( 1 << shift ); <nl> } <nl> } <nl> }
static int64_t run_opencl_bench ( AVOpenCLExternalEnv * ext_opencl_env ) <nl> cl_int status ; <nl> size_t kernel_len ; <nl> char * inbuf ; <nl> - int * mask ; <nl> + int * mask = NULL ; <nl> int buf_size = width * height * sizeof ( char ); <nl> int mask_size = sizeof ( uint32_t ) * 128 ; <nl> 
static int init_input ( AVFormatContext * s , const char * filename , AVDictionary ** o <nl> int ret ; <nl> AVProbeData pd = { filename , NULL , 0 }; <nl>  <nl> - if ( s -> iformat && ! strlen ( filename )) <nl> - return 0 ; <nl> - <nl> if ( s -> pb ) { <nl> s -> flags |= AVFMT_FLAG_CUSTOM_IO ; <nl> if (! s -> iformat )
return - 1 ; <nl> } <nl> # endif <nl> s -> qscale = get_bits (& s -> gb , 5 ); <nl> + if ( s -> qscale == 0 ){ <nl> + fprintf ( stderr , " invalid qscale \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> if ( s -> pict_type == I_TYPE ) { <nl> code = get_bits (& s -> gb , 5 );
static int hevc_init ( AVCodecParserContext * s ) <nl> { <nl> HEVCContext * h = &(( HEVCParseContext *) s -> priv_data )-> h ; <nl> h -> HEVClc = av_mallocz ( sizeof ( HEVCLocalContext )); <nl> + if (! h -> HEVClc ) <nl> + return AVERROR ( ENOMEM ); <nl> h -> skipped_bytes_pos_size = INT_MAX ; <nl>  <nl> return 0 ;
av_cold int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , <nl> } <nl> } <nl>  <nl> -# define USE_MMAP ( HAVE_MMAP && HAVE_MPROTECT && defined MAP_ANONYMOUS ) <nl> +# if HAVE_MMAP && HAVE_MPROTECT && defined ( MAP_ANONYMOUS ) <nl> +# define USE_MMAP 1 <nl> +# else <nl> +# define USE_MMAP 0 <nl> +# endif <nl>  <nl> /* precalculate horizontal scaler filter coefficients */ <nl> {
retry : <nl> return psize ; <nl> fail : <nl> av_free_packet ( pkt ); <nl> - av_free ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
static int smvjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_siz <nl>  <nl> cur_frame = avpkt -> pts % s -> frames_per_jpeg ; <nl>  <nl> + /* cur_frame is later used to calculate the buffer offset , so it mustn ' t be negative */ <nl> + if ( cur_frame < 0 ) <nl> + cur_frame += s -> frames_per_jpeg ; <nl> + <nl> /* Are we at the start of a block ? */ <nl> if (! cur_frame ) { <nl> av_frame_unref ( mjpeg_data );
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl>  <nl> display_matrix = ( uint32_t *) av_stream_get_side_data ( st , AV_PKT_DATA_DISPLAYMATRIX , <nl> & display_matrix_size ); <nl> - if ( display_matrix_size < 9 * sizeof (* display_matrix )) <nl> + if ( display_matrix && display_matrix_size < 9 * sizeof (* display_matrix )) <nl> display_matrix = NULL ; <nl> } <nl> 
static int commit_bitstream_and_slice_buffer ( AVCodecContext * avctx , <nl> const H264Picture * current_picture = h -> cur_pic_ptr ; <nl> struct dxva2_picture_context * ctx_pic = current_picture -> hwaccel_picture_private ; <nl> DXVA_Slice_H264_Short * slice = NULL ; <nl> - void * dxva_data_ptr ; <nl> + void * dxva_data_ptr = NULL ; <nl> uint8_t * dxva_data , * current , * end ; <nl> - unsigned dxva_size ; <nl> + unsigned dxva_size = 0 ; <nl> void * slice_data ; <nl> unsigned slice_size ; <nl> unsigned padding ;
ERROR <nl> # endif <nl>  <nl> void RENAME ( swri_noise_shaping )( SwrContext * s , AudioData * dsts , const AudioData * srcs , const AudioData * noises , int count ){ <nl> - int i , j , pos , ch ; <nl> + int pos = s -> dither . ns_pos ; <nl> + int i , j , ch ; <nl> int taps = s -> dither . ns_taps ; <nl> float S = s -> dither . ns_scale ; <nl> float S_1 = s -> dither . ns_scale_1 ;
static const AVProfile mpeg2_video_profiles [] = { <nl> { FF_PROFILE_MPEG2_SIMPLE , " Simple " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> + { FF_PROFILE_UNKNOWN }, <nl> }; <nl>  <nl> 
static void celt_pvq_search ( float * X , int * y , int K , int N ) <nl> for ( i = 0 ; i < N ; i ++) <nl> res += FFABS ( X [ i ]); <nl>  <nl> - res = K / res ; <nl> + res = K /( res + FLT_EPSILON ); <nl>  <nl> for ( i = 0 ; i < N ; i ++) { <nl> y [ i ] = lrintf ( res * X [ i ]);
return - 1 ; <nl> # endif <nl>  <nl> if ( s -> msmpeg4_version == 1 ){ <nl> - int start_code ; <nl> - start_code = ( get_bits (& s -> gb , 16 )<< 16 ) | get_bits (& s -> gb , 16 ); <nl> + int start_code = get_bits_long (& s -> gb , 32 ); <nl> if ( start_code != 0x00000100 ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " invalid startcode \ n "); <nl> return - 1 ;
static int apply_color_indexing_transform ( WebPContext * s ) <nl> uint8_t * line ; <nl> int pixel_bits = 8 >> pal -> size_reduction ; <nl>  <nl> - line = av_malloc ( img -> frame -> linesize [ 0 ]); <nl> + line = av_malloc ( img -> frame -> linesize [ 0 ] + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! line ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> TestSourceContext * test = outlink -> src -> priv ; <nl> AVFilterBufferRef * picref ; <nl>  <nl> - if ( test -> max_pts >= 0 && test -> pts > test -> max_pts ) <nl> + if ( test -> max_pts >= 0 && test -> pts >= test -> max_pts ) <nl> return AVERROR_EOF ; <nl> picref = avfilter_get_video_buffer ( outlink , AV_PERM_WRITE , <nl> test -> w , test -> h );
static int webm_dash_manifest_cues ( AVFormatContext * s , int64_t init_range ) <nl> "%" PRId64 , s -> streams [ 0 ]-> index_entries [ i ]. timestamp ); <nl> if ( ret <= 0 || ( ret == 20 && i == s -> streams [ 0 ]-> nb_index_entries - 1 )) { <nl> av_log ( s , AV_LOG_ERROR , " timestamp too long .\ n "); <nl> + av_free ( buf ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> end += ret ;
static int aac_decode_frame_int ( AVCodecContext * avctx , void * data , <nl>  <nl> if ( samples ) <nl> ac -> frame -> nb_samples = samples ; <nl> + else <nl> + av_frame_unref ( ac -> frame ); <nl> * got_frame_ptr = !! samples ; <nl>  <nl> if ( is_dmono ) {
static int expand_tseq ( void * log , struct sbg_script * s , int * nb_ev_max , <nl> } else { <nl> ev = alloc_array_elem (( void **)& s -> events , sizeof (* ev ), <nl> & s -> nb_events , nb_ev_max ); <nl> + if (! ev ) <nl> + return AVERROR ( ENOMEM ); <nl> ev -> ts = tseq -> ts . t ; <nl> ev -> elements = def -> elements ; <nl> ev -> nb_elements = def -> nb_elements ;
void checkasm_stack_clobber ( uint64_t clobber , ...); <nl> }\ <nl> } while ( 0 ) <nl> # else <nl> -# define bench_new (...) <nl> +# define bench_new (...) while ( 0 ) <nl> # endif <nl>  <nl> # endif
static int opt_vstats ( void * optctx , const char * opt , const char * arg ) <nl> time_t today2 = time ( NULL ); <nl> struct tm * today = localtime (& today2 ); <nl>  <nl> + if (! today ) <nl> + return AVERROR ( errno ); <nl> + <nl> snprintf ( filename , sizeof ( filename ), " vstats_ % 02d % 02d % 02d . log ", today -> tm_hour , today -> tm_min , <nl> today -> tm_sec ); <nl> return opt_vstats_file ( NULL , opt , filename );
AVBitStreamFilterContext * av_bitstream_filter_init ( const char * name ){ <nl> if (! strcmp ( name , bsf -> name )){ <nl> AVBitStreamFilterContext * bsfc = av_mallocz ( sizeof ( AVBitStreamFilterContext )); <nl> bsfc -> filter = bsf ; <nl> - bsfc -> priv_data = av_mallocz ( bsf -> priv_data_size ); <nl> + bsfc -> priv_data = bsf -> priv_data_size ? av_mallocz ( bsf -> priv_data_size ) : NULL ; <nl> return bsfc ; <nl> } <nl> bsf = bsf -> next ;
static int decode_slice_header ( H264Context * h ){ <nl> return - 1 ; <nl> } <nl>  <nl> - if ( h -> dequant_coeff_pps != ( int ) pps_id ){ <nl> - h -> dequant_coeff_pps = ( int ) pps_id ; <nl> + if ( h -> dequant_coeff_pps != pps_id ){ <nl> + h -> dequant_coeff_pps = pps_id ; <nl> init_dequant_tables ( h ); <nl> } <nl> 
resync : <nl> err = av_get_packet ( pb , pkt , size ); <nl> if ( err < 0 ) <nl> return err ; <nl> + size = err ; <nl>  <nl> if ( ast -> has_pal && pkt -> data && pkt -> size <( unsigned ) INT_MAX / 2 ){ <nl> uint8_t * pal ;
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> ret = 0 ; <nl>  <nl> the_end : <nl> - av_free ( crow_base ); <nl> - av_free ( progressive_buf ); <nl> - av_free ( top_buf ); <nl> + av_freep (& crow_base ); <nl> + av_freep (& progressive_buf ); <nl> + av_freep (& top_buf ); <nl> deflateEnd (& s -> zstream ); <nl> return ret ; <nl> fail :
static void vc1_decode_i_blocks_adv ( VC1Context * v ) <nl> s -> mb_x = 0 ; <nl> ff_init_block_index ( s ); <nl> memset (& s -> coded_block [ s -> block_index [ 0 ]- s -> b8_stride ], 0 , <nl> - s -> b8_stride * sizeof (* s -> coded_block )); <nl> + ( 1 + s -> b8_stride ) * sizeof (* s -> coded_block )); <nl> } <nl> for (; s -> mb_y < s -> end_mb_y ; s -> mb_y ++) { <nl> s -> mb_x = 0 ;
uint8_t * av_packet_new_side_data ( AVPacket * pkt , enum AVPacketSideDataType type , <nl> data = av_malloc ( size + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! data ) <nl> return NULL ; <nl> + memset ( data + size , 0 , AV_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> ret = av_packet_add_side_data ( pkt , type , data , size ); <nl> if ( ret < 0 ) {
fail_kernel_arg : <nl> kernel_arg , cle ); <nl> err = AVERROR ( EIO ); <nl> fail : <nl> + av_frame_free (& output ); <nl> return err ; <nl> } <nl> 
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> } else { <nl> t = get_unary_0_33 ( gb ); <nl> if ( t >= 2 ) { <nl> - if ( get_bits_left ( gb ) < t - 1 ) <nl> + if ( t >= 32 || get_bits_left ( gb ) < t - 1 ) <nl> goto error ; <nl> t = get_bits_long ( gb , t - 1 ) | ( 1 << ( t - 1 )); <nl> } else {
static int check_tag ( AVIOContext * s , int offset , unsigned int len ) <nl>  <nl> if ( len > 4 || <nl> avio_seek ( s , offset , SEEK_SET ) < 0 || <nl> - avio_read ( s , tag , len ) < len ) <nl> + avio_read ( s , tag , len ) < ( int ) len ) <nl> return - 1 ; <nl> else if (! AV_RB32 ( tag ) || is_tag ( tag , len )) <nl> return 1 ;
static void vp8_decode_flush ( AVCodecContext * avctx ) <nl> memset ( s -> framep , 0 , sizeof ( s -> framep )); <nl>  <nl> av_freep (& s -> macroblocks_base ); <nl> + av_freep (& s -> filter_strength ); <nl> av_freep (& s -> intra4x4_pred_mode_base ); <nl> av_freep (& s -> top_nnz ); <nl> av_freep (& s -> edge_emu_buffer );
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> s -> level ++; <nl> av_log ( avctx , AV_LOG_DEBUG , " Subband number %" PRIu16 "\ n ", data ); <nl> s -> subband_num = data ; <nl> - if ( s -> level > DWT_LEVELS ) { <nl> + if ( s -> level >= DWT_LEVELS ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Invalid level \ n "); <nl> ret = AVERROR ( EINVAL ); <nl> break ;
static int libschroedinger_decode_frame ( AVCodecContext * avctx , <nl> /* Grab next frame to be returned from the top of the queue . */ <nl> framewithpts = ff_schro_queue_pop (& p_schro_params -> dec_frame_queue ); <nl>  <nl> - if ( framewithpts && framewithpts -> frame ) { <nl> + if ( framewithpts && framewithpts -> frame && framewithpts -> frame -> components [ 0 ]. stride ) { <nl> int ret ; <nl>  <nl> if (( ret = ff_get_buffer ( avctx , avframe , 0 )) < 0 )
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> current_dts -= sc -> dts_shift ; <nl>  <nl> - if (! sc -> sample_count ) <nl> + if (! sc -> sample_count || st -> nb_index_entries ) <nl> return ; <nl> if ( sc -> sample_count >= UINT_MAX / sizeof (* st -> index_entries )) <nl> return ;
static void formant_postfilter ( G723_1_Context * p , int16_t * lpc , int16_t * buf ) <nl>  <nl> /* Compensation filter */ <nl> for ( j = 0 ; j < SUBFRAME_LEN ; j ++) { <nl> - buf_ptr [ j ] = av_clipl_int32 ( signal_ptr [ j ] + <nl> + buf_ptr [ j ] = av_clipl_int32 (( int64_t ) signal_ptr [ j ] + <nl> (( signal_ptr [ j - 1 ] >> 16 ) * <nl> temp << 1 )) >> 16 ; <nl> }
static void asf_build_simple_index ( AVFormatContext * s , int stream_index ) <nl> last_pos = pos ; <nl> } <nl> } <nl> - asf -> index_read = 1 ; <nl> + asf -> index_read = ict > 0 ; <nl> } <nl> avio_seek ( s -> pb , current_pos , SEEK_SET ); <nl> }
static int aac_adtstoasc_filter ( AVBitStreamFilterContext * bsfc , <nl> buf_size -= get_bits_count (& gb )/ 8 ; <nl> buf += get_bits_count (& gb )/ 8 ; <nl> } <nl> + av_free ( avctx -> extradata ); <nl> avctx -> extradata_size = 2 + pce_size ; <nl> avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> # endif <nl> { <nl> ret = decode_tiles ( avctx , data , size ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + ff_thread_report_progress (& s -> s . frames [ CUR_FRAME ]. tf , INT_MAX , 0 ); <nl> return ret ; <nl> + } <nl> } <nl>  <nl> // Sum all counts fields into td [ 0 ]. counts for tile threading
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size , <nl> continue ; <nl>  <nl> again : <nl> - if ( !( avctx -> active_thread_type & FF_THREAD_FRAME ) <nl> - || nals_needed >= nal_index ) <nl> + if ( (!( avctx -> active_thread_type & FF_THREAD_FRAME ) || nals_needed >= nal_index ) <nl> + && ! h -> current_slice ) <nl> h -> au_pps_id = - 1 ; <nl> /* Ignore per frame NAL unit type during extradata <nl> * parsing . Decoding slices is not possible in codec init
static int mm_decode_intra ( MmContext * s , int half_horiz , int half_vert ) <nl> */ <nl> static int mm_decode_inter ( MmContext * s , int half_horiz , int half_vert ) <nl> { <nl> - int data_off = bytestream2_get_le16 (& s -> gb ), y ; <nl> + int data_off = bytestream2_get_le16 (& s -> gb ), y = 0 ; <nl> GetByteContext data_ptr ; <nl>  <nl> if ( bytestream2_get_bytes_left (& s -> gb ) < data_off )
static int dxv_decompress_dxt5 ( AVCodecContext * avctx ) <nl> AV_WL32 ( ctx -> tex_data + 4 * pos , prev ); <nl> pos ++; <nl> } else { <nl> + if ( bytestream2_get_bytes_left ( gbc ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( state == 0 ) { <nl> value = bytestream2_get_le32 ( gbc ); <nl> state = 16 ;
int ff_j2k_init_component ( J2kComponent * comp , J2kCodingStyle * codsty , J2kQuantSt <nl> band -> cblk = av_malloc ( sizeof ( J2kCblk ) * band -> cblknx * band -> cblkny ); <nl> if (! band -> cblk ) <nl> return AVERROR ( ENOMEM ); <nl> - band -> prec = av_malloc ( reslevel -> num_precincts_x * reslevel -> num_precincts_y * sizeof ( J2kPrec )); <nl> + band -> prec = av_malloc ( sizeof ( J2kCblk ) * reslevel -> num_precincts_x * reslevel -> num_precincts_y ); <nl> if (! band -> prec ) <nl> return AVERROR ( ENOMEM ); <nl> 
enum AVCodecID ff_codec_guid_get_id ( const AVCodecGuid * guids , ff_asf_guid guid ) <nl> static void parse_waveformatex ( AVIOContext * pb , AVCodecParameters * par ) <nl> { <nl> ff_asf_guid subformat ; <nl> - par -> bits_per_coded_sample = avio_rl16 ( pb ); <nl> + int bps ; <nl> + <nl> + bps = avio_rl16 ( pb ); <nl> + if ( bps ) <nl> + par -> bits_per_coded_sample = bps ; <nl> par -> channel_layout = avio_rl32 ( pb ); /* dwChannelMask */ <nl>  <nl> ff_get_guid ( pb , & subformat );
int main ( int argc , char ** argv ) <nl>  <nl> if ( got_frame ) { <nl> /* push the audio data from decoded frame into the filtergraph */ <nl> - if ( av_buffersrc_add_frame_flags ( buffersrc_ctx , frame , AV_BUFFERSRC_FLAG_KEEP_REF ) < 0 ) { <nl> + if ( av_buffersrc_add_frame_flags ( buffersrc_ctx , frame , 0 ) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error while feeding the audio filtergraph \ n "); <nl> break ; <nl> }
static int fourxm_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( fourcc_tag == std__TAG ) { <nl> + if ( header_size < i + 16 ) { <nl> + av_log ( s , AV_LOG_ERROR , " std TAG truncated \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> fourxm -> fps = av_int2float ( AV_RL32 (& header [ i + 12 ])); <nl> } else if ( fourcc_tag == vtrk_TAG ) { <nl> /* check that there is enough data */
static int qsv_decode ( AVCodecContext * avctx , QSVContext * q , <nl>  <nl> do { <nl> ret = get_surface ( avctx , q , & insurf ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& sync ); <nl> return ret ; <nl> + } <nl>  <nl> ret = MFXVideoDECODE_DecodeFrameAsync ( q -> session , avpkt -> size ? & bs : NULL , <nl> insurf , & outsurf , sync );
static void create_adapt_vect ( float * vect , const int16_t * cb , int lag ) <nl> static int adaptive_cb_search ( const int16_t * adapt_cb , float * work , <nl> const float * coefs , float * data ) <nl> { <nl> - int i , best_vect ; <nl> - float score , gain , best_score , best_gain ; <nl> + int i , av_uninit ( best_vect ); <nl> + float score , gain , best_score , av_uninit ( best_gain ); <nl> float exc [ BLOCKSIZE ]; <nl>  <nl> gain = best_score = 0 ;
static int asf_read_metadata ( AVFormatContext * s , int64_t size ) <nl> } else { <nl> get_tag ( s , name , value_type , value_len , 16 ); <nl> } <nl> + av_freep (& name ); <nl> } <nl>  <nl> return 0 ;
static int decode_subframe_length ( WMAProDecodeCtx * s , int offset ) <nl> if ( offset == s -> samples_per_frame - s -> min_samples_per_subframe ) <nl> return s -> min_samples_per_subframe ; <nl>  <nl> + if ( get_bits_left (& s -> gb ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /** 1 bit indicates if the subframe is of maximum length */ <nl> if ( s -> max_subframe_len_bit ) { <nl> if ( get_bits1 (& s -> gb ))
static int init_pass2 ( MpegEncContext * s ) <nl> double rate_factor = 0 ; <nl> double step ; <nl> const int filter_size = ( int )( a -> qblur * 4 ) | 1 ; <nl> - double expected_bits ; <nl> + double expected_bits = 0 ; // init to silence gcc warning <nl> double * qscale , * blurred_qscale , qscale_sum ; <nl>  <nl> /* find complexity & const_bits & decide the pict_types */
static void update_initial_timestamps ( AVFormatContext * s , int stream_index , <nl> if ( st -> first_dts != AV_NOPTS_VALUE || <nl> dts == AV_NOPTS_VALUE || <nl> st -> cur_dts == AV_NOPTS_VALUE || <nl> + st -> cur_dts < INT_MIN + RELATIVE_TS_BASE || <nl> is_relative ( dts )) <nl> return ; <nl> 
static int applehttp_read_seek ( AVFormatContext * s , int stream_index , <nl> int64_t timestamp , int flags ) <nl> { <nl> AppleHTTPContext * c = s -> priv_data ; <nl> - int pos = 0 , i ; <nl> + int64_t pos = 0 ; <nl> + int i ; <nl> struct variant * var = c -> variants [ 0 ]; <nl>  <nl> if (( flags & AVSEEK_FLAG_BYTE ) || ! c -> finished )
static void vector_fmul_window_fixed_c ( int32_t * dst , const int32_t * src0 , <nl> AVFixedDSPContext * avpriv_alloc_fixed_dsp ( int bit_exact ) <nl> { <nl> AVFixedDSPContext * fdsp = av_malloc ( sizeof ( AVFixedDSPContext )); <nl> + <nl> + if (! fdsp ) <nl> + return NULL ; <nl> + <nl> fdsp -> vector_fmul_window_scaled = vector_fmul_window_fixed_scaled_c ; <nl> fdsp -> vector_fmul_window = vector_fmul_window_fixed_c ; <nl> 
static int opus_packet ( AVFormatContext * avf , int idx ) <nl>  <nl> if (! os -> psize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( os -> granule > INT64_MAX - UINT32_MAX ) { <nl> + av_log ( avf , AV_LOG_ERROR , " Unsupported huge granule pos %" PRId64 "\ n ", os -> granule ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> if ((! os -> lastpts || os -> lastpts == AV_NOPTS_VALUE ) && !( os -> flags & OGG_FLAG_EOS )) { <nl> int seg , d ;
static int try_decode_video_frame ( AVCodecContext * codec_ctx , AVPacket * pkt , int <nl> goto end ; <nl> } <nl>  <nl> - if (! decode && codec_ctx -> codec -> caps_internal & FF_CODEC_CAP_SKIP_FRAME_FILL_PARAM ) { <nl> + if (! decode && avpriv_codec_get_cap_skip_frame_fill_param ( codec_ctx -> codec )) { <nl> codec_ctx -> skip_frame = AVDISCARD_ALL ; <nl> } <nl> 
static int mpc8_decode_frame ( AVCodecContext * avctx , <nl> maxband = c -> last_max_band + get_vlc2 ( gb , band_vlc . table , MPC8_BANDS_BITS , 2 ); <nl> if ( maxband > 32 ) maxband -= 33 ; <nl> } <nl> + if ( maxband > c -> maxbands ) <nl> + return AVERROR_INVALIDDATA ; <nl> c -> last_max_band = maxband ; <nl>  <nl> /* read subband indexes */
static int sp5x_decode_frame ( AVCodecContext * avctx , <nl> recoded [ j ++] = 0xFF ; <nl> recoded [ j ++] = 0xD9 ; <nl>  <nl> - avctx -> flags &= ~ CODEC_FLAG_EMU_EDGE ; <nl> av_init_packet (& avpkt_recoded ); <nl> avpkt_recoded . data = recoded ; <nl> avpkt_recoded . size = j ; <nl> AVCodec ff_amv_decoder = { <nl> NULL , <nl> ff_mjpeg_decode_end , <nl> sp5x_decode_frame , <nl> - CODEC_CAP_DR1 , <nl> + 0 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" AMV Video "), <nl> };
static void decode_postinit ( H264Context * h , int setup_finished ){ <nl>  <nl> if ( s -> avctx -> strict_std_compliance >= FF_COMPLIANCE_STRICT <nl> && ! h -> sps . bitstream_restriction_flag ){ <nl> - s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT ; <nl> + s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT - 1 ; <nl> s -> low_delay = 0 ; <nl> } <nl> 
int ff_ac3_bit_alloc_calc_mask ( AC3BitAllocParameters * s , int16_t * band_psd , <nl> int band_start , band_end , begin , end1 ; <nl> int lowcomp , fastleak , slowleak ; <nl>  <nl> + if ( end <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /* excitation function */ <nl> band_start = ff_ac3_bin_to_band_tab [ start ]; <nl> band_end = ff_ac3_bin_to_band_tab [ end - 1 ] + 1 ;
static int smacker_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> } <nl> flags >>= 1 ; <nl> } <nl> - if ( frame_size < 0 ) <nl> + if ( frame_size < 0 || frame_size >= INT_MAX / 2 ) <nl> return AVERROR_INVALIDDATA ; <nl> if ( av_new_packet ( pkt , frame_size + 769 )) <nl> return AVERROR ( ENOMEM );
static const char * type_string ( int type ) <nl> return "< LINK >"; <nl> case AVIO_ENTRY_SOCKET : <nl> return "< SOCKET >"; <nl> + case AVIO_ENTRY_SERVER : <nl> + return "< SERVER >"; <nl> + case AVIO_ENTRY_SHARE : <nl> + return "< SHARE >"; <nl> + case AVIO_ENTRY_WORKGROUP : <nl> + return "< WORKGROUP >"; <nl> case AVIO_ENTRY_UNKNOWN : <nl> default : <nl> break ;
static int matroska_parse_rm_audio ( MatroskaDemuxContext * matroska , <nl> } <nl> memcpy ( track -> audio . buf + y * w , data , w ); <nl> } else { <nl> - if ( size < sps * w / sps ) { <nl> + if ( size < sps * w / sps || h <= 0 ) { <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Corrupt generic RM - style audio packet size \ n "); <nl> return AVERROR_INVALIDDATA ;
static int avi_read_header ( AVFormatContext * s ) <nl> codec_type = AVMEDIA_TYPE_VIDEO ; <nl>  <nl> ast -> sample_size = 0 ; <nl> + st -> avg_frame_rate = av_inv_q ( st -> time_base ); <nl> break ; <nl> case MKTAG (' a ', ' u ', ' d ', ' s '): <nl> codec_type = AVMEDIA_TYPE_AUDIO ;
static int decode_seq_header ( AVSContext * h ) { <nl> av_log_missing_feature ( s , " Width / height changing in CAVS is ", 0 ); <nl> return - 1 ; <nl> } <nl> + if ( width <= 0 || height <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Dimensions invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> width = width ; <nl> s -> height = height ; <nl> skip_bits (& s -> gb , 2 ); // chroma format
* <nl> */ <nl>  <nl> -# define WTV_SECTOR_BITS 12 <nl> +# define WTV_SECTOR_BITS INT64_C ( 12 ) <nl> # define WTV_SECTOR_SIZE ( 1 << WTV_SECTOR_BITS ) <nl> # define WTV_BIGSECTOR_BITS 18 <nl> 
static int http_open_cnx ( URLContext * h ) <nl>  <nl> if (! strcmp ( proto , " https ")) { <nl> lower_proto = " tls "; <nl> + use_proxy = 0 ; <nl> if ( port < 0 ) <nl> port = 443 ; <nl> }
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - s -> version_b = avctx -> extradata && avctx -> extradata [ 3 ] == ' b '; <nl> + s -> version_b = avctx -> extradata_size >= 4 && avctx -> extradata [ 3 ] == ' b '; <nl>  <nl> if ( avctx -> codec -> id == CODEC_ID_BINKAUDIO_RDFT ) { <nl> // audio is already interleaved for the RDFT format variant
static int has_codec_parameters ( AVStream * st , const char ** errmsg_ptr ) <nl> FAIL (" unspecified sample rate "); <nl> if (! avctx -> channels ) <nl> FAIL (" unspecified number of channels "); <nl> + if ( st -> info -> found_decoder >= 0 && ! st -> nb_decoded_frames && avctx -> codec_id == AV_CODEC_ID_DTS ) <nl> + FAIL (" no decodable DTS frames "); <nl> break ; <nl> case AVMEDIA_TYPE_VIDEO : <nl> if (! avctx -> width )
static av_cold void nvenc_setup_rate_control ( AVCodecContext * avctx ) <nl>  <nl> if ( ctx -> flags & NVENC_LOSSLESS ) { <nl> set_lossless ( avctx ); <nl> - } else if ( ctx -> rc > 0 ) { <nl> + } else if ( ctx -> rc >= 0 ) { <nl> nvenc_override_rate_control ( avctx ); <nl> } else { <nl> ctx -> encode_config . rcParams . rateControlMode = NV_ENC_PARAMS_RC_VBR ;
void av_packet_free ( AVPacket ** pkt ) <nl> static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> { <nl> int ret ; <nl> - if (( unsigned ) size >= ( unsigned ) size + AV_INPUT_BUFFER_PADDING_SIZE ) <nl> + if ( size < 0 || size >= INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> ret = av_buffer_realloc ( buf , size + AV_INPUT_BUFFER_PADDING_SIZE );
static void encode_frame ( MpegAudioContext * s , <nl> q1 += 1 << P ; <nl> if ( q1 < 0 ) <nl> q1 = 0 ; <nl> - q [ m ] = ( unsigned )( q1 * steps ) >> ( P + 1 ); <nl> + q [ m ] = ( q1 * ( unsigned ) steps ) >> ( P + 1 ); <nl> } <nl> # endif <nl> if ( q [ m ] >= steps )
av_cold void ff_init_range_decoder ( RangeCoder * c , const uint8_t * buf , <nl> c -> bytestream += 2 ; <nl> if ( c -> low >= 0xFF00 ) { <nl> c -> low = 0xFF00 ; <nl> - c -> bytestream_end = c -> bytestream + 2 ; <nl> + c -> bytestream_end = c -> bytestream ; <nl> } <nl> } <nl> 
RMStream * ff_rm_alloc_rmstream ( void ) <nl>  <nl> void ff_rm_free_rmstream ( RMStream * rms ) <nl> { <nl> - av_free ( rms -> videobuf ); <nl> - av_free ( rms -> audiobuf ); <nl> + av_freep (& rms -> videobuf ); <nl> + av_freep (& rms -> audiobuf ); <nl> } <nl>  <nl> static int rm_read_audio_stream_info ( AVFormatContext * s , ByteIOContext * pb ,
static int xv_write_trailer ( AVFormatContext * s ) <nl> XShmDetach ( xv -> display , & xv -> yuv_shminfo ); <nl> shmdt ( xv -> yuv_image -> data ); <nl> XFree ( xv -> yuv_image ); <nl> + XFreeGC ( xv -> display , xv -> gc ); <nl> XCloseDisplay ( xv -> display ); <nl> return 0 ; <nl> }
static int asf_write_header ( AVFormatContext * s ) <nl> asf -> nb_packets = 0 ; <nl>  <nl> asf -> index_ptr = av_malloc ( sizeof ( ASFIndex ) * ASF_INDEX_BLOCK ); <nl> + if (! asf -> index_ptr ) <nl> + return AVERROR ( ENOMEM ); <nl> asf -> nb_index_memory_alloc = ASF_INDEX_BLOCK ; <nl> asf -> maximum_packet = 0 ; <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> ret = calc_active_inputs ( s ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + } <nl>  <nl> + if ( s -> active_inputs > 1 ) { <nl> available_samples = get_available_samples ( s ); <nl> if (! available_samples ) <nl> return AVERROR ( EAGAIN );
static int concat_read_packet ( AVFormatContext * avf , AVPacket * pkt ) <nl> ConcatStream * cs ; <nl> AVStream * st ; <nl>  <nl> + if (! cat -> avf ) <nl> + return AVERROR ( EIO ); <nl> + <nl> while ( 1 ) { <nl> ret = av_read_frame ( cat -> avf , pkt ); <nl> if ( ret == AVERROR_EOF ) {
static int hls_slice_header ( HEVCContext * s ) <nl>  <nl> if ( s -> pps -> slice_header_extension_present_flag ) { <nl> unsigned int length = get_ue_golomb_long ( gb ); <nl> + if ( length * 8LL > get_bits_left ( gb )) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many slice_header_extension_data_bytes \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> for ( i = 0 ; i < length ; i ++) <nl> skip_bits ( gb , 8 ); // slice_header_extension_data_byte <nl> }
static int webm_dash_manifest_read_header ( AVFormatContext * s ) <nl> av_log ( s , AV_LOG_ERROR , " Failed to read file headers \ n "); <nl> return - 1 ; <nl> } <nl> + if (! s -> nb_streams ) { <nl> + matroska_read_close ( s ); <nl> + av_log ( s , AV_LOG_ERROR , " No streams found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> if (! matroska -> is_live ) { <nl> buf = av_asprintf ("% g ", matroska -> duration );
not_extra : <nl> if (!( s -> flags2 & CODEC_FLAG2_CHUNKS ) && ! s -> current_picture_ptr ){ <nl> if ( avctx -> skip_frame >= AVDISCARD_NONREF || <nl> buf_size >= 4 && ! memcmp (" Q264 ", buf , 4 )) <nl> - return 0 ; <nl> + return buf_size ; <nl> av_log ( avctx , AV_LOG_ERROR , " no frame !\ n "); <nl> return - 1 ; <nl> }
void ff_mpeg_set_erpic ( ERPicture * dst , Picture * src ) <nl> { <nl> int i ; <nl>  <nl> - if (! src ) <nl> + if (! src ) { <nl> + dst -> f = NULL ; <nl> + dst -> tf = NULL ; <nl> return ; <nl> + } <nl>  <nl> dst -> f = src -> f ; <nl> dst -> tf = & src -> tf ;
static int pxr24_uncompress ( EXRContext * s , const uint8_t * src , <nl> in = ptr [ 2 ] + td -> xsize ; <nl>  <nl> for ( j = 0 ; j < td -> xsize ; ++ j ) { <nl> - uint32_t diff = (*( ptr [ 0 ]++) << 24 ) | <nl> + uint32_t diff = (( unsigned )*( ptr [ 0 ]++) << 24 ) | <nl> (*( ptr [ 1 ]++) << 16 ) | <nl> (*( ptr [ 2 ]++) << 8 ); <nl> pixel += diff ;
static int vp3_update_thread_context ( AVCodecContext * dst , const AVCodecContext * <nl> || s -> width != s1 -> width <nl> || s -> height != s1 -> height ) { <nl> if ( s != s1 ) <nl> - copy_fields ( s , s1 , golden_frame , current_frame ); <nl> + copy_fields ( s , s1 , golden_frame , keyframe ); <nl> return - 1 ; <nl> } <nl> 
static av_cold int xvid_encode_close ( AVCodecContext * avctx ) { <nl> xvid_encore ( x -> encoder_handle , XVID_ENC_DESTROY , NULL , NULL ); <nl>  <nl> if ( avctx -> extradata != NULL ) <nl> - av_free ( avctx -> extradata ); <nl> + av_freep (& avctx -> extradata ); <nl> if ( x -> twopassbuffer != NULL ) { <nl> av_free ( x -> twopassbuffer ); <nl> av_free ( x -> old_twopassbuffer );
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl> s -> avctx -> bits_per_raw_sample = <nl> bits = get_bits (& s -> gb , 8 ); <nl>  <nl> + if ( bits > 16 || bits < 1 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " bits % d is invalid \ n ", bits ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( s -> pegasus_rct ) <nl> bits = 9 ; <nl> if ( bits == 9 && ! s -> pegasus_rct )
typedef struct XMVAudioPacket { <nl> uint16_t bits_per_sample ; ///< Bits per compressed sample . <nl> uint32_t bit_rate ; ///< Bits of compressed data per second . <nl> uint16_t flags ; ///< Flags <nl> - uint16_t block_align ; ///< Bytes per compressed block . <nl> + unsigned block_align ; ///< Bytes per compressed block . <nl> uint16_t block_samples ; ///< Decompressed samples per compressed block . <nl>  <nl> enum AVCodecID codec_id ; ///< The codec ID of the compression scheme .
static int mpeg4_update_thread_context ( AVCodecContext * dst , <nl> s -> time_increment_bits = s1 -> time_increment_bits ; <nl> s -> vol_sprite_usage = s1 -> vol_sprite_usage ; <nl> s -> rvlc = s1 -> rvlc ; <nl> + s -> divx_version = s1 -> divx_version ; <nl> + s -> divx_build = s1 -> divx_build ; <nl> + s -> xvid_build = s1 -> xvid_build ; <nl> + s -> lavc_build = s1 -> lavc_build ; <nl>  <nl> return 0 ; <nl> }
copy_oid ( const oid * from , oid * to ) <nl> to -> components = malloc ( to -> length * sizeof (* to -> components )); <nl> if ( to -> length != 0 && to -> components == NULL ) <nl> return ENOMEM ; <nl> - memcpy ( to -> components , from -> components , to -> length ); <nl> + memcpy ( to -> components , from -> components , to -> length * sizeof (* to -> components )); <nl> return 0 ; <nl> }
API_EXPORT ( int ) ap_call_exec ( request_rec * r , child_info * pinfo , char * argv0 , <nl> else if (( conf -> cgi_command_args == AP_FLAG_OFF ) <nl> || (! r -> args ) || (! r -> args [ 0 ]) <nl> || strchr ( r -> args , '=')) { <nl> - execle ( r -> filename , argv0 , NULL , env ); <nl> + execle ( r -> filename , argv0 , ( void *) NULL , env ); <nl> } <nl>  <nl> else {
hvn_nvs_cmd ( struct hvn_softc * sc , void * cmd , size_t cmdsize , uint64_t tid , <nl> return ( rv ); <nl> } <nl>  <nl> + if ( timo == 0 ) <nl> + return ( 0 ); <nl> + <nl> do { <nl> if ( cold ) <nl> delay ( 1000 );
output_data ( const char * format , ...) <nl>  <nl> va_start ( args , format ); <nl> remaining = BUFSIZ - ( nfrontp - netobuf ); <nl> + if ( remaining == 0 ) <nl> + return remaining ; <nl> if (( n = vsnprintf ( nfrontp , remaining , format , args )) >= remaining || n < 0 ) <nl> n = strlen ( nfrontp ); <nl> nfrontp += n ;
-/* $ OpenBSD : file_media . c , v 1 . 13 2016 / 01 / 11 07 : 57 : 54 jasper Exp $ */ <nl> +/* $ OpenBSD : file_media . c , v 1 . 14 2016 / 01 / 11 14 : 27 : 29 jasper Exp $ */ <nl>  <nl> /* <nl> * file_media . c - <nl> compute_block_size ( int fd ) <nl> } <nl> } <nl> } <nl> + free ( buffer ); <nl> return 0 ; <nl> } <nl> 
ksymsmmap ( dev , off , prot ) <nl> int off , prot ; <nl> { <nl> # define ksyms_btop ( x ) (( vm_offset_t )( x ) >> PGSHIFT <nl> + if ( off < 0 ) <nl> + return (- 1 ); <nl> if (( unsigned ) off >= ( unsigned )( esym - symtab ) + k1 -> a_text ) <nl> return (- 1 ); <nl> 
# ifndef TRACE__EVENT_INTERNAL_H <nl> # define TRACE__EVENT_INTERNAL_H <nl>  <nl> -# include " trace / generated - events . h " <nl> - <nl> - <nl> /** <nl> * TraceEvent : <nl> * @ id : Unique event identifier . <nl> * Opaque generic description of a tracing event . <nl> */ <nl> typedef struct TraceEvent { <nl> - TraceEventID id ; <nl> - TraceEventVCPUID vcpu_id ; <nl> + uint32_t id ; <nl> + uint32_t vcpu_id ; <nl> const char * name ; <nl> const bool sstate ; <nl> uint16_t * dstate ;
void kvm_irqchip_commit_routes ( KVMState * s ) <nl> { <nl> int ret ; <nl>  <nl> + if ( kvm_gsi_direct_mapping ()) { <nl> + return ; <nl> + } <nl> + <nl> + if (! kvm_gsi_routing_enabled ()) { <nl> + return ; <nl> + } <nl> + <nl> s -> irq_routes -> flags = 0 ; <nl> trace_kvm_irqchip_commit_routes (); <nl> ret = kvm_vm_ioctl ( s , KVM_SET_GSI_ROUTING , s -> irq_routes );
static int parallels_open ( BlockDriverState * bs , QDict * options , int flags , <nl> if ( local_err != NULL ) { <nl> goto fail_options ; <nl> } <nl> - if (! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> + <nl> + if (!( flags & BDRV_O_RESIZE ) || ! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> bdrv_truncate ( bs -> file , bdrv_getlength ( bs -> file -> bs )) != 0 ) { <nl> s -> prealloc_mode = PRL_PREALLOC_MODE_FALLOCATE ; <nl> }
int qcow2_alloc_cluster_link_l2 ( BlockDriverState * bs , QCowL2Meta * m ) <nl> } <nl> qcow2_cache_entry_mark_dirty ( s -> l2_table_cache , l2_table ); <nl>  <nl> + assert ( l2_index + m -> nb_clusters <= s -> l2_size ); <nl> for ( i = 0 ; i < m -> nb_clusters ; i ++) { <nl> /* if two concurrent writes happen to the same unallocated cluster <nl> * each write allocates separate cluster and writes data concurrently .
 <nl> # include " virtio - net . h " <nl> # include " vhost_net . h " <nl> +# include " qemu - error . h " <nl>  <nl> # include " config . h " <nl>  <nl> void vhost_net_cleanup ( struct vhost_net * net ) <nl> struct vhost_net * vhost_net_init ( VLANClientState * backend , int devfd , <nl> bool force ) <nl> { <nl> + error_report (" vhost - net support is not compiled in "); <nl> return NULL ; <nl> } <nl> 
int vhost_set_vring_enable ( NetClientState * nc , int enable ) <nl>  <nl> nc -> vring_enable = enable ; <nl>  <nl> - if ( vhost_ops -> vhost_set_vring_enable ) { <nl> + if ( vhost_ops && vhost_ops -> vhost_set_vring_enable ) { <nl> return vhost_ops -> vhost_set_vring_enable (& net -> dev , enable ); <nl> } <nl> 
static int qed_create ( const char * filename , uint32_t cluster_size , <nl> return ret ; <nl> } <nl>  <nl> + /* File must start empty and grow , check truncate is supported */ <nl> + ret = bdrv_truncate ( bs , 0 ); <nl> + if ( ret < 0 ) { <nl> + goto out ; <nl> + } <nl> + <nl> if ( backing_file ) { <nl> header . features |= QED_F_BACKING_FILE ; <nl> header . backing_filename_offset = sizeof ( le_header );
opts_visitor_cleanup ( OptsVisitor * ov ) <nl> g_hash_table_destroy ( ov -> unprocessed_opts ); <nl> } <nl> g_free ( ov -> fake_id_opt ); <nl> - memset ( ov , '\ 0 ', sizeof * ov ); <nl> + g_free ( ov ); <nl> } <nl>  <nl> 
static void slavio_check_interrupts ( SLAVIO_INTCTLState * s , int set_irqs ) <nl> CPU_IRQ_TIMER_IN ; <nl> if ( i == s -> target_cpu ) { <nl> for ( j = 0 ; j < 32 ; j ++) { <nl> - if (( s -> intregm_pending & ( 1 << j )) && intbit_to_level [ j ]) { <nl> + if (( s -> intregm_pending & ( 1U << j )) && intbit_to_level [ j ]) { <nl> s -> slaves [ i ]. intreg_pending |= 1 << intbit_to_level [ j ]; <nl> } <nl> }
# include < sys / resource . h > <nl> # endif <nl>  <nl> -# if ( defined ( _WIN32 ) || defined RUSAGE_THREAD ) <nl> +# if ( defined ( _WIN32 ) || defined RUSAGE_THREAD ) && \ <nl> + ( defined ( CONFIG_NETTLE_KDF ) || defined ( CONFIG_GCRYPT_KDF )) <nl> # define TEST_LUKS <nl> # else <nl> # undef TEST_LUKS
static int usb_serial_initfn ( USBDevice * dev ) <nl> USBSerialState * s = DO_UPCAST ( USBSerialState , dev , dev ); <nl> s -> dev . speed = USB_SPEED_FULL ; <nl>  <nl> + if (! s -> cs ) { <nl> + error_report (" Property chardev is required "); <nl> + return - 1 ; <nl> + } <nl> + <nl> qemu_chr_add_handlers ( s -> cs , usb_serial_can_read , usb_serial_read , <nl> usb_serial_event , s ); <nl> usb_serial_handle_reset ( dev );
DMAContext * spapr_tce_new_dma_context ( uint32_t liobn , size_t window_size ) <nl> { <nl> sPAPRTCETable * tcet ; <nl>  <nl> + if ( spapr_tce_find_by_liobn ( liobn )) { <nl> + fprintf ( stderr , " Attempted to create TCE table with duplicate " <nl> + " LIOBN 0x % x \ n ", liobn ); <nl> + return NULL ; <nl> + } <nl> + <nl> if (! window_size ) { <nl> return NULL ; <nl> }
static inline int cpu_interrupts_enabled ( CPUSPARCState * env1 ) <nl> if ( env1 -> psret != 0 ) <nl> return 1 ; <nl> # else <nl> - if ( env1 -> pstate & PS_IE ) <nl> + if (( env1 -> pstate & PS_IE ) && ! cpu_hypervisor_mode ( env1 )) { <nl> return 1 ; <nl> + } <nl> # endif <nl>  <nl> return 0 ;
static int decode_gusa ( DisasContext * ctx , CPUSH4State * env , int * pmax_insns ) <nl> } <nl>  <nl> /* If op_src is not a valid register , then op_arg was a constant . */ <nl> - if ( op_src < 0 ) { <nl> + if ( op_src < 0 && ! TCGV_IS_UNUSED ( op_arg )) { <nl> tcg_temp_free_i32 ( op_arg ); <nl> } <nl> 
void qemu_input_event_send ( QemuConsole * src , InputEvent * evt ) <nl>  <nl> /* send event */ <nl> s = qemu_input_find_handler ( 1 << evt -> kind ); <nl> + if (! s ) { <nl> + return ; <nl> + } <nl> s -> handler -> event ( s -> dev , src , evt ); <nl> s -> events ++; <nl> }
int bdrv_open_image ( BlockDriverState ** pbs , const char * filename , <nl> bdref_key ); <nl> ret = - EINVAL ; <nl> } <nl> + QDECREF ( image_options ); <nl> goto done ; <nl> } <nl> 
static void test_dispatch_cmd_io ( void ) <nl>  <nl> ret3 = qobject_to_qint ( test_qmp_dispatch ( req )); <nl> assert ( qint_get_int ( ret3 ) == 66 ); <nl> - QDECREF ( ret ); <nl> + QDECREF ( ret3 ); <nl>  <nl> QDECREF ( req ); <nl> }
int64_t bdrv_getlength ( BlockDriverState * bs ) <nl> { <nl> int64_t ret = bdrv_nb_sectors ( bs ); <nl>  <nl> + ret = ret > INT64_MAX / BDRV_SECTOR_SIZE ? - EFBIG : ret ; <nl> return ret < 0 ? ret : ret * BDRV_SECTOR_SIZE ; <nl> } <nl> 
static void ide_dma_cb ( void * opaque , int ret ) <nl> } <nl> if ( ret < 0 ) { <nl> if ( ide_handle_rw_error ( s , - ret , ide_dma_cmd_to_retry ( s -> dma_cmd ))) { <nl> + s -> bus -> dma -> aiocb = NULL ; <nl> return ; <nl> } <nl> }
static void qvirtio_scsi_pci_free ( QVirtIOSCSI * vs ) <nl> qvirtqueue_cleanup ( vs -> dev -> bus , vs -> vq [ i ], vs -> qs -> alloc ); <nl> } <nl> qvirtio_pci_device_disable ( container_of ( vs -> dev , QVirtioPCIDevice , vdev )); <nl> - g_free ( vs -> dev ); <nl> + qvirtio_pci_device_free (( QVirtioPCIDevice *) vs -> dev ); <nl> qvirtio_scsi_stop ( vs -> qs ); <nl> g_free ( vs ); <nl> }
static inline int array_ensure_allocated ( array_t * array , int index ) <nl> array -> pointer = g_realloc ( array -> pointer , new_size ); <nl> if (! array -> pointer ) <nl> return - 1 ; <nl> + memset ( array -> pointer + array -> size , 0 , new_size - array -> size ); <nl> array -> size = new_size ; <nl> array -> next = index + 1 ; <nl> }
static int img_amend ( int argc , char ** argv ) <nl> } <nl>  <nl> if ( optind != argc - 1 ) { <nl> - error_exit (" Expecting one image file name "); <nl> + error_report (" Expecting one image file name "); <nl> + ret = - 1 ; <nl> + goto out ; <nl> } <nl>  <nl> flags = BDRV_O_FLAGS | BDRV_O_RDWR ;
void tb_invalidate_phys_addr ( target_phys_addr_t addr ) <nl>  <nl> static void breakpoint_invalidate ( CPUArchState * env , target_ulong pc ) <nl> { <nl> - tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( env , pc )); <nl> + tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( env , pc ) | <nl> + ( pc & ~ TARGET_PAGE_MASK )); <nl> } <nl> # endif <nl> # endif /* TARGET_HAS_ICE */
PropertyInfo qdev_prop_bit = { <nl> static uint64_t qdev_get_prop_mask64 ( Property * prop ) <nl> { <nl> assert ( prop -> info == & qdev_prop_bit ); <nl> - return 0x1 << prop -> bitnr ; <nl> + return 0x1ull << prop -> bitnr ; <nl> } <nl>  <nl> static void bit64_prop_set ( DeviceState * dev , Property * props , bool val )
static target_ulong disas_insn ( DisasContext * s , target_ulong pc_start ) <nl> break ; <nl> case 5 : /* lfence */ <nl> case 6 : /* mfence */ <nl> - if (( modrm & 0xc7 ) != 0xc0 || !( s -> cpuid_features & CPUID_SSE )) <nl> + if (( modrm & 0xc7 ) != 0xc0 || !( s -> cpuid_features & CPUID_SSE2 )) <nl> goto illegal_op ; <nl> break ; <nl> case 7 : /* sfence / clflush */
static void leon3_generic_hw_init ( MachineState * machine ) <nl> fprintf ( stderr , " Can ' t read bios image % s \ n ", filename ); <nl> exit ( 1 ); <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Can directly load an application . */ <nl> if ( kernel_filename != NULL ) {
void ppce500_init ( MachineState * machine , PPCE500Params * params ) <nl> exit ( 1 ); <nl> } <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Reserve space for dtb */ <nl> dt_base = ( loadaddr + bios_size + DTC_LOAD_PAD ) & ~ DTC_PAD_MASK ;
static void * qpa_thread_out ( void * arg ) <nl> return NULL ; <nl> } <nl>  <nl> + pa -> live = 0 ; <nl> pa -> rpos = rpos ; <nl> - pa -> live -= decr ; <nl> pa -> decr += decr ; <nl> } <nl> 
static void ehci_detach ( USBPort * port ) <nl> ehci_queues_rip_device ( s , port -> dev , 0 ); <nl> ehci_queues_rip_device ( s , port -> dev , 1 ); <nl>  <nl> - * portsc &= ~( PORTSC_CONNECT | PORTSC_PED ); <nl> + * portsc &= ~( PORTSC_CONNECT | PORTSC_PED | PORTSC_SUSPEND ); <nl> * portsc |= PORTSC_CSC ; <nl>  <nl> ehci_raise_irq ( s , USBSTS_PCD );
glue ( cirrus_bitblt_rop_fwd_ , ROP_NAME )( CirrusVGAState * s , <nl> dstpitch -= bltwidth ; <nl> srcpitch -= bltwidth ; <nl>  <nl> - if ( dstpitch < 0 || srcpitch < 0 ) { <nl> - /* is 0 valid ? srcpitch == 0 could be useful */ <nl> + if ( bltheight > 1 && ( dstpitch < 0 || srcpitch < 0 )) { <nl> return ; <nl> } <nl> 
file_backend_instance_init ( Object * o ) <nl> set_mem_path , NULL ); <nl> } <nl>  <nl> + static void file_backend_instance_finalize ( Object * o ) <nl> +{ <nl> + HostMemoryBackendFile * fb = MEMORY_BACKEND_FILE ( o ); <nl> + <nl> + g_free ( fb -> mem_path ); <nl> +} <nl> + <nl> static const TypeInfo file_backend_info = { <nl> . name = TYPE_MEMORY_BACKEND_FILE , <nl> . parent = TYPE_MEMORY_BACKEND , <nl> . class_init = file_backend_class_init , <nl> . instance_init = file_backend_instance_init , <nl> + . instance_finalize = file_backend_instance_finalize , <nl> . instance_size = sizeof ( HostMemoryBackendFile ), <nl> }; <nl> 
int vhost_dev_init ( struct vhost_dev * hdev , void * opaque , <nl> if (!( hdev -> features & ( 0x1ULL << VHOST_F_LOG_ALL ))) { <nl> error_setg (& hdev -> migration_blocker , <nl> " Migration disabled : vhost lacks VHOST_F_LOG_ALL feature ."); <nl> - } else if (! qemu_memfd_check ()) { <nl> + } else if ( vhost_dev_log_is_shared ( hdev ) && ! qemu_memfd_check ()) { <nl> error_setg (& hdev -> migration_blocker , <nl> " Migration disabled : failed to allocate shared memory "); <nl> }
int virtio_load ( VirtIODevice * vdev , QEMUFile * f ) <nl> qemu_get_8s ( f , & vdev -> status ); <nl> qemu_get_8s ( f , & vdev -> isr ); <nl> qemu_get_be16s ( f , & vdev -> queue_sel ); <nl> + if ( vdev -> queue_sel >= VIRTIO_PCI_QUEUE_MAX ) { <nl> + return - 1 ; <nl> + } <nl> qemu_get_be32s ( f , & features ); <nl>  <nl> if ( virtio_set_features ( vdev , features ) < 0 ) {
static int iscsi_open ( BlockDriverState * bs , QDict * options , int flags , <nl> QemuOpts * opts ; <nl> Error * local_err = NULL ; <nl> const char * filename ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> if (( BDRV_SECTOR_SIZE % 512 ) != 0 ) { <nl> error_setg ( errp , " iSCSI : Invalid BDRV_SECTOR_SIZE . "
void restore_boot_order ( void * opaque ) <nl> return ; <nl> } <nl>  <nl> - qemu_boot_set ( normal_boot_order , NULL ); <nl> + if ( boot_set_handler ) { <nl> + qemu_boot_set ( normal_boot_order , & error_abort ); <nl> + } <nl>  <nl> qemu_unregister_reset ( restore_boot_order , normal_boot_order ); <nl> g_free ( normal_boot_order );
static int get_cluster_offset ( BlockDriverState * bs , <nl> uint32_t min_count , * l2_table ; <nl> bool zeroed = false ; <nl> int64_t ret ; <nl> - int32_t cluster_sector ; <nl> + int64_t cluster_sector ; <nl>  <nl> if ( m_data ) { <nl> m_data -> valid = 0 ;
static void qemu_cleanup_net_client ( NetClientState * nc ) <nl> { <nl> QTAILQ_REMOVE (& net_clients , nc , next ); <nl>  <nl> - nc -> info -> cleanup ( nc ); <nl> + if ( nc -> info -> cleanup ) { <nl> + nc -> info -> cleanup ( nc ); <nl> + } <nl> } <nl>  <nl> static void qemu_free_net_client ( NetClientState * nc )
static uint32_t vmxnet3_get_interrupt_config ( VMXNET3State * s ) <nl> static void vmxnet3_fill_stats ( VMXNET3State * s ) <nl> { <nl> int i ; <nl> + <nl> + if (! s -> device_active ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < s -> txq_num ; i ++) { <nl> cpu_physical_memory_write ( s -> txq_descr [ i ]. tx_stats_pa , <nl> & s -> txq_descr [ i ]. txq_stats ,
int net_init_l2tpv3 ( const NetClientOptions * opts , <nl> if ( fd == - 1 ) { <nl> fd = - errno ; <nl> error_report (" l2tpv3_open : socket creation failed , errno = % d ", - fd ); <nl> - freeaddrinfo ( result ); <nl> goto outerr ; <nl> } <nl> if ( bind ( fd , ( struct sockaddr *) result -> ai_addr , result -> ai_addrlen )) {
int mmu_translate ( CPUS390XState * env , target_ulong vaddr , int rw , uint64_t asc , <nl> /* Convert real address -> absolute address */ <nl> * raddr = mmu_real2abs ( env , * raddr ); <nl>  <nl> - if (* raddr <= ram_size ) { <nl> + if (* raddr < ram_size ) { <nl> sk = & env -> storage_keys [* raddr / TARGET_PAGE_SIZE ]; <nl> if (* flags & PAGE_READ ) { <nl> * sk |= SK_R ;
static void virtio_device_realize ( DeviceState * dev , Error ** errp ) <nl> virtio_bus_device_plugged ( vdev , & err ); <nl> if ( err != NULL ) { <nl> error_propagate ( errp , err ); <nl> + vdc -> unrealize ( dev , NULL ); <nl> return ; <nl> } <nl> 
static int ioreq_runio_qemu_aio ( struct ioreq * ioreq ) <nl> break ; <nl> case BLKIF_OP_WRITE : <nl> case BLKIF_OP_WRITE_BARRIER : <nl> - ioreq -> aio_inflight ++; <nl> if (! ioreq -> req . nr_segments ) <nl> break ; <nl> + ioreq -> aio_inflight ++; <nl> bdrv_aio_writev ( blkdev -> bs , ioreq -> start / BLOCK_SIZE , <nl> & ioreq -> v , ioreq -> v . size / BLOCK_SIZE , <nl> qemu_aio_complete , ioreq );
static int handle_instruction ( CPUState * env , struct kvm_run * run ) <nl> if ( r < 0 ) { <nl> enter_pgmcheck ( env , 0x0001 ); <nl> } <nl> - return r ; <nl> + return 0 ; <nl> } <nl>  <nl> static int handle_intercept ( CPUState * env )
void monitor_init ( CharDriverState * hd , int show_banner ) <nl> hide_banner = ! show_banner ; <nl>  <nl> qemu_chr_add_handlers ( hd , term_can_read , term_read , term_event , NULL ); <nl> + <nl> + readline_start ("", 0 , monitor_handle_command1 , NULL ); <nl> } <nl>  <nl> /* XXX : use threads ? */
static uint32_t get_cmd ( ESPState * s , uint8_t * buf ) <nl> s -> ti_rptr = 0 ; <nl> s -> ti_wptr = 0 ; <nl>  <nl> - if ( s -> current_dev ) { <nl> + if ( s -> current_req ) { <nl> /* Started a new command before the old one finished . Cancel it . */ <nl> scsi_req_cancel ( s -> current_req ); <nl> s -> async_len = 0 ;
static int print_block_option_help ( const char * filename , const char * fmt ) <nl> proto_drv = bdrv_find_protocol ( filename , true ); <nl> if (! proto_drv ) { <nl> error_report (" Unknown protocol '% s '", filename ); <nl> + free_option_parameters ( create_options ); <nl> return 1 ; <nl> } <nl> create_options = append_option_parameters ( create_options ,
static int floppy_probe_device ( const char * filename ) <nl> struct stat st ; <nl>  <nl> if ( strstart ( filename , "/ dev / fd ", NULL ) && <nl> - ! strstart ( filename , "/ dev / fdset /", NULL )) { <nl> + ! strstart ( filename , "/ dev / fdset /", NULL ) && <nl> + ! strstart ( filename , "/ dev / fd /", NULL )) { <nl> prio = 50 ; <nl> } <nl> 
static void esp_pci_dma_memory_rw ( PCIESPState * pci , uint8_t * buf , int len , <nl> /* update status registers */ <nl> pci -> dma_regs [ DMA_WBC ] -= len ; <nl> pci -> dma_regs [ DMA_WAC ] += len ; <nl> + if ( pci -> dma_regs [ DMA_WBC ] == 0 ) <nl> + pci -> dma_regs [ DMA_STAT ] |= DMA_STAT_DONE ; <nl> } <nl>  <nl> static void esp_pci_dma_memory_read ( void * opaque , uint8_t * buf , int len )
static const RunStateTransition runstate_transitions_def [] = { <nl> { RUN_STATE_WATCHDOG , RUN_STATE_FINISH_MIGRATE }, <nl>  <nl> { RUN_STATE_GUEST_PANICKED , RUN_STATE_PAUSED }, <nl> + { RUN_STATE_GUEST_PANICKED , RUN_STATE_FINISH_MIGRATE }, <nl>  <nl> { RUN_STATE_MAX , RUN_STATE_MAX }, <nl> };
static void calxeda_init ( MachineState * machine , enum cxmachines machine_id ) <nl> if ( bios_name != NULL ) { <nl> sysboot_filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> if ( sysboot_filename != NULL ) { <nl> - uint32_t filesize = get_image_size ( sysboot_filename ); <nl> - if ( load_image_targphys (" sysram . bin ", 0xfff88000 , filesize ) < 0 ) { <nl> + if ( load_image_targphys ( sysboot_filename , 0xfff88000 , 0x8000 ) < 0 ) { <nl> hw_error (" Unable to load % s \ n ", bios_name ); <nl> } <nl> g_free ( sysboot_filename );
static inline void hwsetup_add_tag ( HWSetup * hw , enum hwsetup_tag t ) <nl>  <nl> static inline void hwsetup_add_str ( HWSetup * hw , const char * str ) <nl> { <nl> - strncpy ( hw -> ptr , str , 31 ); /* make sure last byte is zero */ <nl> + pstrcpy ( hw -> ptr , 32 , str ); <nl> hw -> ptr += 32 ; <nl> } <nl> 
static void raise_mmu_exception ( CPUMIPSState * env , target_ulong address , <nl> env -> CP0_Context = ( env -> CP0_Context & ~ 0x007fffff ) | <nl> (( address >> 9 ) & 0x007ffff0 ); <nl> env -> CP0_EntryHi = ( env -> CP0_EntryHi & env -> CP0_EntryHi_ASID_mask ) | <nl> + ( env -> CP0_EntryHi & ( 1 << CP0EnHi_EHINV )) | <nl> ( address & ( TARGET_PAGE_MASK << 1 )); <nl> # if defined ( TARGET_MIPS64 ) <nl> env -> CP0_EntryHi &= env -> SEGMask ;
void hmp_drive_add_node ( Monitor * mon , const char * optstr ) <nl> qdict = qemu_opts_to_qdict ( opts , NULL ); <nl>  <nl> if (! qdict_get_try_str ( qdict , " node - name ")) { <nl> + QDECREF ( qdict ); <nl> error_report ("' node - name ' needs to be specified "); <nl> goto out ; <nl> }
static inline void elf_core_copy_regs ( target_elf_gregset_t * regs , <nl> int i ; <nl>  <nl> for ( i = 0 ; i < 16 ; i ++) { <nl> - (* regs [ i ]) = tswapreg ( env -> gregs [ i ]); <nl> + (* regs )[ i ] = tswapreg ( env -> gregs [ i ]); <nl> } <nl>  <nl> (* regs )[ TARGET_REG_PC ] = tswapreg ( env -> pc );
static void do_dma_memory_set ( dma_addr_t addr , uint8_t c , dma_addr_t len ) <nl> while ( len > 0 ) { <nl> l = len < FILLBUF_SIZE ? len : FILLBUF_SIZE ; <nl> cpu_physical_memory_rw ( addr , fillbuf , l , true ); <nl> - len -= len ; <nl> - addr += len ; <nl> + len -= l ; <nl> + addr += l ; <nl> } <nl> } <nl> 
static void pmac_ide_atapi_transfer_cb ( void * opaque , int ret ) <nl> s -> io_buffer_size = MIN ( s -> io_buffer_size , io -> len ); <nl> dma_memory_write (& address_space_memory , io -> addr , s -> io_buffer , <nl> s -> io_buffer_size ); <nl> + io -> len = 0 ; <nl> ide_atapi_cmd_ok ( s ); <nl> m -> dma_active = false ; <nl> goto done ;
static void sdhci_send_command ( SDHCIState * s ) <nl> ( s -> cmdreg & SDHC_CMD_RESPONSE ) == SDHC_CMD_RSP_WITH_BUSY ) { <nl> s -> norintsts |= SDHC_NIS_TRSCMP ; <nl> } <nl> - } else if ( rlen != 0 && ( s -> errintstsen & SDHC_EISEN_CMDIDX )) { <nl> - s -> errintsts |= SDHC_EIS_CMDIDX ; <nl> - s -> norintsts |= SDHC_NIS_ERR ; <nl> } <nl>  <nl> if ( s -> norintstsen & SDHC_NISEN_CMDCMP ) {
void qmp_block_resize ( bool has_device , const char * device , <nl> goto out ; <nl> } <nl>  <nl> - /* complete all in - flight operations before resizing the device */ <nl> - bdrv_drain_all (); <nl> - <nl> + bdrv_drained_begin ( bs ); <nl> ret = blk_truncate ( blk , size , errp ); <nl> + bdrv_drained_end ( bs ); <nl>  <nl> out : <nl> blk_unref ( blk );
static int load_refcount_block ( BlockDriverState * bs , <nl> static int get_refcount ( BlockDriverState * bs , int64_t cluster_index ) <nl> { <nl> BDRVQcowState * s = bs -> opaque ; <nl> - int refcount_table_index , block_index ; <nl> + uint64_t refcount_table_index , block_index ; <nl> int64_t refcount_block_offset ; <nl> int ret ; <nl> uint16_t * refcount_block ;
static void vnc_async_encoding_end ( VncState * orig , VncState * local ) <nl> orig -> hextile = local -> hextile ; <nl> orig -> zrle = local -> zrle ; <nl> orig -> lossy_rect = local -> lossy_rect ; <nl> + <nl> + queue -> buffer = local -> output ; <nl> } <nl>  <nl> static int vnc_worker_thread_loop ( VncJobQueue * queue )
void ptimer_set_limit ( ptimer_state * s , uint64_t limit , int reload ) <nl> * on the current generation of host machines . <nl> */ <nl>  <nl> - if ( limit * s -> period < 10000 && s -> period ) { <nl> + if (! use_icount && limit * s -> period < 10000 && s -> period ) { <nl> limit = 10000 / s -> period ; <nl> } <nl> 
static int vhost_user_cleanup ( struct vhost_dev * dev ) <nl>  <nl> u = dev -> opaque ; <nl> if ( u -> slave_fd >= 0 ) { <nl> + qemu_set_fd_handler ( u -> slave_fd , NULL , NULL , NULL ); <nl> close ( u -> slave_fd ); <nl> u -> slave_fd = - 1 ; <nl> }
static void qio_channel_websock_handshake_send_res_ok ( QIOChannelWebsock * ioc , <nl> char combined_key [ QIO_CHANNEL_WEBSOCK_CLIENT_KEY_LEN + <nl> QIO_CHANNEL_WEBSOCK_GUID_LEN + 1 ]; <nl> char * accept = NULL ; <nl> - char * date = qio_channel_websock_date_str (); <nl> + char * date = NULL ; <nl>  <nl> g_strlcpy ( combined_key , key , QIO_CHANNEL_WEBSOCK_CLIENT_KEY_LEN + 1 ); <nl> g_strlcat ( combined_key , QIO_CHANNEL_WEBSOCK_GUID , <nl> static void qio_channel_websock_handshake_send_res_ok ( QIOChannelWebsock * ioc , <nl> return ; <nl> } <nl>  <nl> + date = qio_channel_websock_date_str (); <nl> qio_channel_websock_handshake_send_res ( <nl> ioc , QIO_CHANNEL_WEBSOCK_HANDSHAKE_RES_OK , date , accept ); <nl> 
Object * object_resolve_path_component ( Object * parent , const gchar * part ) <nl> } <nl>  <nl> if ( object_property_is_link ( prop )) { <nl> - return *( Object **) prop -> opaque ; <nl> + LinkProperty * lprop = prop -> opaque ; <nl> + return * lprop -> child ; <nl> } else if ( object_property_is_child ( prop )) { <nl> return prop -> opaque ; <nl> } else {
write_refblocks : <nl> * this will leak that range , but we can easily fix that by running <nl> * a leak - fixing check after this rebuild operation */ <nl> reftable_offset = - 1 ; <nl> + } else { <nl> + assert ( on_disk_reftable ); <nl> } <nl> on_disk_reftable [ refblock_index ] = refblock_offset ; <nl>  <nl> write_refblocks : <nl> goto write_refblocks ; <nl> } <nl>  <nl> - assert ( on_disk_reftable ); <nl> - <nl> for ( refblock_index = 0 ; refblock_index < reftable_size ; refblock_index ++) { <nl> cpu_to_be64s (& on_disk_reftable [ refblock_index ]); <nl> }
void virtio_cleanup ( VirtIODevice * vdev ) <nl> if ( vdev -> config ) <nl> qemu_free ( vdev -> config ); <nl> qemu_free ( vdev -> vq ); <nl> + qemu_free ( vdev ); <nl> } <nl>  <nl> static void virtio_vmstate_change ( void * opaque , int running , int reason )
int v9fs_co_st_gen ( V9fsPDU * pdu , V9fsPath * path , mode_t st_mode , <nl> }); <nl> v9fs_path_unlock ( s ); <nl> } <nl> + /* The ioctl may not be supported depending on the path */ <nl> + if ( err == - ENOTTY ) { <nl> + err = 0 ; <nl> + } <nl> return err ; <nl> } <nl> 
void do_compare_and_swap32 ( void * cpu_env , int num ) <nl> uint32_t * value = ( uint32_t *)(( CPUX86State *) cpu_env )-> regs [ R_ECX ]; <nl> DPRINTF (" commpage : compare_and_swap32 (% x , new ,% p )\ n ", old , value ); <nl>  <nl> - if ( value && old == tswap32 (* value )) <nl> + if ( old == tswap32 (* value )) <nl> { <nl> uint32_t new = (( CPUX86State *) cpu_env )-> regs [ R_EDX ]; <nl> * value = tswap32 ( new );
static struct iovec * lock_iovec ( int type , abi_ulong target_addr , <nl> errno = 0 ; <nl> return NULL ; <nl> } <nl> - if ( count > IOV_MAX ) { <nl> + if ( count < 0 || count > IOV_MAX ) { <nl> errno = EINVAL ; <nl> return NULL ; <nl> }
void init_paths ( const char * prefix ) <nl> base = new_entry ("", NULL , pref_buf ); <nl> base = add_dir_maybe ( base ); <nl> if ( base -> num_entries == 0 ) { <nl> - free ( base ); <nl> + g_free ( base -> pathname ); <nl> + free ( base -> name ); <nl> + free ( base ); <nl> base = NULL ; <nl> } else { <nl> set_parents ( base , base );
void do_unassigned_access ( target_phys_addr_t addr , int is_write , int is_exec , <nl> else <nl> raise_exception ( TT_DATA_ACCESS ); <nl> } <nl> - env = saved_env ; <nl>  <nl> /* flush neverland mappings created during no - fault mode , <nl> so the sequential MMU faults report proper fault types */ <nl> if ( env -> mmuregs [ 0 ] & MMU_NF ) { <nl> tlb_flush ( env , 1 ); <nl> } <nl> + <nl> + env = saved_env ; <nl> } <nl> # else <nl> void do_unassigned_access ( target_phys_addr_t addr , int is_write , int is_exec ,
int pci_bridge_initfn ( PCIDevice * dev ) <nl> br -> bus_name ); <nl> sec_bus -> parent_dev = dev ; <nl> sec_bus -> map_irq = br -> map_irq ; <nl> + /* TODO : use memory API to perform memory filtering . */ <nl> + sec_bus -> address_space_mem = parent -> address_space_mem ; <nl> + sec_bus -> address_space_io = parent -> address_space_io ; <nl>  <nl> QLIST_INIT (& sec_bus -> child ); <nl> QLIST_INSERT_HEAD (& parent -> child , sec_bus , sibling );
static int vmdk_write_extent ( VmdkExtent * extent , int64_t cluster_offset , <nl> goto out ; <nl> } <nl>  <nl> - data -> lba = offset >> BDRV_SECTOR_BITS ; <nl> - data -> size = buf_len ; <nl> + data -> lba = cpu_to_le64 ( offset >> BDRV_SECTOR_BITS ); <nl> + data -> size = cpu_to_le32 ( buf_len ); <nl>  <nl> n_bytes = buf_len + sizeof ( VmdkGrainMarker ); <nl> iov = ( struct iovec ) {
int socket_dgram ( SocketAddress * remote , SocketAddress * local , Error ** errp ) <nl>  <nl> default : <nl> error_setg ( errp , " socket type unsupported for datagram "); <nl> - return - 1 ; <nl> + fd = - 1 ; <nl> } <nl> qemu_opts_del ( opts ); <nl> return fd ;
PCIDevice * virtio_net_init ( PCIBus * bus , NICInfo * nd , int devfn ) <nl> n -> promisc = 1 ; /* for compatibility */ <nl>  <nl> n -> mac_table . macs = qemu_mallocz ( MAC_TABLE_ENTRIES * ETH_ALEN ); <nl> - if (! n -> mac_table . macs ) <nl> - return NULL ; <nl>  <nl> n -> vlans = qemu_mallocz ( MAX_VLAN >> 3 ); <nl> - if (! n -> vlans ) <nl> - return NULL ; <nl>  <nl> register_savevm (" virtio - net ", virtio_net_id ++, VIRTIO_NET_VM_VERSION , <nl> virtio_net_save , virtio_net_load , n );
static void vscsi_process_login ( VSCSIState * s , vscsi_req * req ) <nl> struct srp_login_rsp * rsp = & iu -> srp . login_rsp ; <nl> uint64_t tag = iu -> srp . rsp . tag ; <nl>  <nl> - trace_spapr_vscsi__process_login (); <nl> + trace_spapr_vscsi_process_login (); <nl>  <nl> /* TODO handle case that requested size is wrong and <nl> * buffer format is wrong
static void timer_update_irq ( struct fs_timer_t * t ) <nl> qemu_irq_lower ( t -> irq [ 0 ]); <nl> } <nl>  <nl> - static void timer_hit ( struct fs_timer_t * t ) <nl> + static void timer_hit ( void * opaque ) <nl> { <nl> + struct fs_timer_t * t = opaque ; <nl> t -> r_intr |= 1 ; <nl> timer_update_irq ( t ); <nl> }
static void pmac_ide_atapi_transfer_cb ( void * opaque , int ret ) <nl> } <nl>  <nl> /* Calculate current offset */ <nl> - offset = ( int64_t )( s -> lba << 11 ) + s -> io_buffer_index ; <nl> + offset = (( int64_t ) s -> lba << 11 ) + s -> io_buffer_index ; <nl>  <nl> pmac_dma_read ( s -> blk , offset , io -> len , pmac_ide_atapi_transfer_cb , io ); <nl> return ;
static int inc_refcounts ( BlockDriverState * bs , <nl> if ( refcount == s -> refcount_max ) { <nl> fprintf ( stderr , " ERROR : overflow cluster offset = 0x %" PRIx64 <nl> "\ n ", cluster_offset ); <nl> + fprintf ( stderr , " Use qemu - img amend to increase the refcount entry " <nl> + " width or qemu - img convert to create a clean copy if the " <nl> + " image cannot be opened for writing \ n "); <nl> res -> corruptions ++; <nl> continue ; <nl> }
static void dump_qobject ( fprintf_function func_fprintf , void * f , <nl> case QTYPE_QERROR : { <nl> QString * value = qerror_human (( QError *) obj ); <nl> func_fprintf ( f , "% s ", qstring_get_str ( value )); <nl> + QDECREF ( value ); <nl> break ; <nl> } <nl> case QTYPE_NONE :
static int qcow2_co_flush ( BlockDriverState * bs ) <nl> qemu_co_mutex_lock (& s -> lock ); <nl> ret = qcow2_cache_flush ( bs , s -> l2_table_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl>  <nl> ret = qcow2_cache_flush ( bs , s -> refcount_block_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl> qemu_co_mutex_unlock (& s -> lock );
static void vnc_dpy_copy ( DisplayChangeListener * dcl , <nl> int i , x , y , pitch , inc , w_lim , s ; <nl> int cmp_bytes ; <nl>  <nl> + if (! vd -> server ) { <nl> + /* no client connected */ <nl> + return ; <nl> + } <nl> + <nl> vnc_refresh_server_surface ( vd ); <nl> QTAILQ_FOREACH_SAFE ( vs , & vd -> clients , next , vn ) { <nl> if ( vnc_has_feature ( vs , VNC_FEATURE_COPYRECT )) {
static void fsl_imx31_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx31_realize ; <nl> - <nl> dc -> desc = " i . MX31 SOC "; <nl> + /* <nl> + * Reason : uses serial_hds in realize and the kzm board does not <nl> + * support multiple CPUs <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx31_type_info = {
int qemu_spice_set_pw_expire ( time_t expires ); <nl> int qemu_spice_migrate_info ( const char * hostname , int port , int tls_port , <nl> const char * subject ); <nl>  <nl> -# define SPICE_NEEDS_SET_MM_TIME \ <nl> - (! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 )) <nl> +# if ! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 ) <nl> +# define SPICE_NEEDS_SET_MM_TIME 1 <nl> +# else <nl> +# define SPICE_NEEDS_SET_MM_TIME 0 <nl> +# endif <nl>  <nl> # if SPICE_SERVER_VERSION >= 0x000c02 <nl> void qemu_spice_register_ports ( void );
static int vdi_create ( const char * filename , QEMUOptionParameter * options ) <nl> static void vdi_close ( BlockDriverState * bs ) <nl> { <nl> BDRVVdiState * s = bs -> opaque ; <nl> + <nl> + g_free ( s -> bmap ); <nl> + <nl> migrate_del_blocker ( s -> migration_blocker ); <nl> error_free ( s -> migration_blocker ); <nl> }
static void spr_write_excp_prefix ( void * opaque , int sprn , int gprn ) <nl> tcg_gen_and_tl ( t0 , t0 , cpu_gpr [ gprn ]); <nl> tcg_gen_st_tl ( t0 , cpu_env , offsetof ( CPUState , excp_prefix )); <nl> gen_store_spr ( sprn , t0 ); <nl> + tcg_temp_free ( t0 ); <nl> } <nl>  <nl> static void spr_write_excp_vector ( void * opaque , int sprn , int gprn )
static void disas_ldst_reg_imm9 ( DisasContext * s , uint32_t insn , <nl> post_index = false ; <nl> writeback = true ; <nl> break ; <nl> + default : <nl> + g_assert_not_reached (); <nl> } <nl>  <nl> if ( rn == 31 ) {
int main ( int argc , char ** argv ) <nl>  <nl> path = g_strdup_printf (" e1000 /% s ", models [ i ]); <nl> qtest_add_data_func ( path , models [ i ], test_device ); <nl> + g_free ( path ); <nl> } <nl>  <nl> return g_test_run ();
static int write_refcount_block_entries ( BlockDriverState * bs , <nl> return 0 ; <nl> } <nl>  <nl> + if ( first_index < 0 ) { <nl> + return 0 ; <nl> + } <nl> + <nl> first_index &= ~( REFCOUNTS_PER_SECTOR - 1 ); <nl> last_index = ( last_index + REFCOUNTS_PER_SECTOR ) <nl> & ~( REFCOUNTS_PER_SECTOR - 1 );
static uint64_t do_cvttq ( CPUAlphaState * env , uint64_t a , int roundmode ) <nl> frac = a & 0xfffffffffffffull ; <nl>  <nl> if ( exp == 0 ) { <nl> - if ( unlikely ( frac != 0 )) { <nl> + if ( unlikely ( frac != 0 ) && ! env -> fp_status . flush_inputs_to_zero ) { <nl> goto do_underflow ; <nl> } <nl> } else if ( exp == 0x7ff ) {
static void isa_irq_handler ( void * opaque , int n , int level ) <nl> if ( n < 16 ) { <nl> qemu_set_irq ( isa -> i8259 [ n ], level ); <nl> } <nl> - qemu_set_irq ( isa -> ioapic [ n ], level ); <nl> + if ( isa -> ioapic ) <nl> + qemu_set_irq ( isa -> ioapic [ n ], level ); <nl> }; <nl>  <nl> static void ioport80_write ( void * opaque , uint32_t addr , uint32_t data )
static void openrisc_pic_cpu_handler ( void * opaque , int irq , int level ) <nl> { <nl> OpenRISCCPU * cpu = ( OpenRISCCPU *) opaque ; <nl> CPUState * cs = CPU ( cpu ); <nl> - uint32_t irq_bit = 1 << irq ; <nl> + uint32_t irq_bit ; <nl>  <nl> if ( irq > 31 || irq < 0 ) { <nl> return ; <nl> } <nl>  <nl> + irq_bit = 1U << irq ; <nl> + <nl> if ( level ) { <nl> cpu -> env . picsr |= irq_bit ; <nl> } else {
void qcow2_free_clusters ( BlockDriverState * bs , <nl> ret = update_refcount ( bs , offset , size , - 1 ); <nl> if ( ret < 0 ) { <nl> fprintf ( stderr , " qcow2_free_clusters failed : % s \ n ", strerror (- ret )); <nl> - abort (); <nl> + /* TODO Remember the clusters to free them later and avoid leaking */ <nl> } <nl> } <nl> 
static bool cmd_smart ( IDEState * s , uint8_t cmd ) <nl> case 2 : /* extended self test */ <nl> s -> smart_selftest_count ++; <nl> if ( s -> smart_selftest_count > 21 ) { <nl> - s -> smart_selftest_count = 0 ; <nl> + s -> smart_selftest_count = 1 ; <nl> } <nl> n = 2 + ( s -> smart_selftest_count - 1 ) * 24 ; <nl> s -> smart_selftest_data [ n ] = s -> sector ;
print_insn_sparc ( bfd_vma memaddr , disassemble_info * info ) <nl> } <nl>  <nl> info -> insn_type = dis_noninsn ; /* Mark as non - valid instruction . */ <nl> - (* info -> fprintf_func ) ( stream , _ (" unknown ")); <nl> + (* info -> fprintf_func ) ( stream , ". long %# 08lx ", insn ); <nl> return sizeof ( buffer ); <nl> }
void hmp_info_local_apic ( Monitor * mon , const QDict * qdict ) <nl>  <nl> void hmp_info_io_apic ( Monitor * mon , const QDict * qdict ) <nl> { <nl> - if ( kvm_irqchip_in_kernel ()) { <nl> + if ( kvm_irqchip_in_kernel () && <nl> + ! kvm_irqchip_is_split ()) { <nl> kvm_ioapic_dump_state ( mon , qdict ); <nl> } else { <nl> ioapic_dump_state ( mon , qdict );
void page_set_flags ( target_ulong start , target_ulong end , int flags ) <nl> guest address space . If this assert fires , it probably indicates <nl> a missing call to h2g_valid . */ <nl> # if TARGET_ABI_BITS > L1_MAP_ADDR_SPACE_BITS <nl> - assert ( end < (( target_ulong ) 1 << L1_MAP_ADDR_SPACE_BITS )); <nl> + assert ( end <= (( target_ulong ) 1 << L1_MAP_ADDR_SPACE_BITS )); <nl> # endif <nl> assert ( start < end ); <nl> assert_memory_lock ();
void * qxl_phys2virt ( PCIQXLDevice * qxl , QXLPHYSICAL pqxl , int group_id ) <nl> case MEMSLOT_GROUP_HOST : <nl> return ( void *) offset ; <nl> case MEMSLOT_GROUP_GUEST : <nl> - PANIC_ON ( slot > NUM_MEMSLOTS ); <nl> + PANIC_ON ( slot >= NUM_MEMSLOTS ); <nl> PANIC_ON (! qxl -> guest_slots [ slot ]. active ); <nl> PANIC_ON ( offset < qxl -> guest_slots [ slot ]. delta ); <nl> offset -= qxl -> guest_slots [ slot ]. delta ;
static void cleanup_unknown_header_ext ( BlockDriverState * bs ) <nl> } <nl> } <nl>  <nl> - static void report_unsupported ( BlockDriverState * bs , const char * fmt , ...) <nl> + static void GCC_FMT_ATTR ( 2 , 3 ) report_unsupported ( BlockDriverState * bs , <nl> + const char * fmt , ...) <nl> { <nl> char msg [ 64 ]; <nl> va_list ap ;
static int coroutine_fn bdrv_aligned_preadv ( BlockDriverState * bs , <nl> } <nl>  <nl> max_bytes = ROUND_UP ( MAX ( 0 , total_bytes - offset ), align ); <nl> - if ( bytes < max_bytes ) { <nl> + if ( bytes <= max_bytes ) { <nl> ret = bdrv_driver_preadv ( bs , offset , bytes , qiov , 0 ); <nl> } else if ( max_bytes > 0 ) { <nl> QEMUIOVector local_qiov ;
void replay_configure ( QemuOpts * opts ) <nl> rr = qemu_opt_get ( opts , " rr "); <nl> if (! rr ) { <nl> /* Just enabling icount */ <nl> - return ; <nl> + goto out ; <nl> } else if (! strcmp ( rr , " record ")) { <nl> mode = REPLAY_MODE_RECORD ; <nl> } else if (! strcmp ( rr , " replay ")) { <nl> void replay_configure ( QemuOpts * opts ) <nl>  <nl> replay_enable ( fname , mode ); <nl>  <nl> + out : <nl> loc_pop (& loc ); <nl> } <nl> 
static inline void fimd_swap_data ( unsigned int swap_ctl , uint64_t * data ) <nl> if ( swap_ctl & FIMD_WINCON_SWAP_BITS ) { <nl> res = 0 ; <nl> for ( i = 0 ; i < 64 ; i ++) { <nl> - if ( x & ( 1ULL << ( 64 - i ))) { <nl> + if ( x & ( 1ULL << ( 63 - i ))) { <nl> res |= ( 1ULL << i ); <nl> } <nl> }
void watchdog_perform_action ( void ) <nl> exit ( 0 ); <nl>  <nl> case WDT_PAUSE : /* same as ' stop ' command in monitor */ <nl> + /* In a timer callback , when vm_stop calls qemu_clock_enable <nl> + * you would get a deadlock . Bypass the problem . <nl> + */ <nl> + qemu_system_vmstop_request_prepare (); <nl> qapi_event_send_watchdog ( WATCHDOG_EXPIRATION_ACTION_PAUSE , & error_abort ); <nl> - vm_stop ( RUN_STATE_WATCHDOG ); <nl> + qemu_system_vmstop_request ( RUN_STATE_WATCHDOG ); <nl> break ; <nl>  <nl> case WDT_DEBUG :
static void ncq_cb ( void * opaque , int ret ) <nl> NCQTransferState * ncq_tfs = ( NCQTransferState *) opaque ; <nl> IDEState * ide_state = & ncq_tfs -> drive -> port . ifs [ 0 ]; <nl>  <nl> + ncq_tfs -> aiocb = NULL ; <nl> if ( ret == - ECANCELED ) { <nl> return ; <nl> }
static int qcow2_amend_options ( BlockDriverState * bs , QemuOpts * opts , <nl> } <nl>  <nl> if ( new_size ) { <nl> - ret = bdrv_truncate ( bs , new_size ); <nl> + BlockBackend * blk = blk_new (); <nl> + blk_insert_bs ( blk , bs ); <nl> + ret = blk_truncate ( blk , new_size ); <nl> + blk_unref ( blk ); <nl> + <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
static PixelFormat sdl_to_qemu_pixelformat ( SDL_PixelFormat * sdl_pf ) <nl> static DisplaySurface * sdl_create_displaysurface ( int width , int height ) <nl> { <nl> DisplaySurface * surface = ( DisplaySurface *) g_malloc0 ( sizeof ( DisplaySurface )); <nl> - if ( surface == NULL ) { <nl> - fprintf ( stderr , " sdl_create_displaysurface : malloc failed \ n "); <nl> - exit ( 1 ); <nl> - } <nl>  <nl> surface -> width = width ; <nl> surface -> height = height ;
static void tcg_out_op ( TCGContext * s , TCGOpcode opc , const TCGArg * args , <nl> break ; <nl>  <nl> case INDEX_op_ext32u_i64 : <nl> - tcg_out_rld ( s , RLDICR , args [ 0 ], args [ 1 ], 0 , 32 ); <nl> + tcg_out_rld ( s , RLDICL , args [ 0 ], args [ 1 ], 0 , 32 ); <nl> break ; <nl>  <nl> case INDEX_op_setcond_i32 :
void usb_packet_complete ( USBDevice * dev , USBPacket * p ) <nl> { <nl> /* Note : p -> owner != dev is possible in case dev is a hub */ <nl> assert ( p -> owner != NULL ); <nl> - dev -> port -> ops -> complete ( dev -> port , p ); <nl> p -> owner = NULL ; <nl> + dev -> port -> ops -> complete ( dev -> port , p ); <nl> } <nl>  <nl> /* Cancel an active packet . The packed must have been deferred by
static int vnc_display_listen_addr ( VncDisplay * vd , <nl> qio_channel_set_name ( QIO_CHANNEL ( sioc ), name ); <nl> if ( qio_channel_socket_listen_sync ( <nl> sioc , rawaddrs [ i ], listenerr == NULL ? & listenerr : NULL ) < 0 ) { <nl> + object_unref ( OBJECT ( sioc )); <nl> continue ; <nl> } <nl> listening = true ;
static int cpu_post_load ( void * opaque , int version_id ) <nl>  <nl> # if defined ( TARGET_PPC64 ) <nl> if ( cpu -> compat_pvr ) { <nl> + uint32_t compat_pvr = cpu -> compat_pvr ; <nl> Error * local_err = NULL ; <nl>  <nl> - ppc_set_compat ( cpu , cpu -> compat_pvr , & local_err ); <nl> + cpu -> compat_pvr = 0 ; <nl> + ppc_set_compat ( cpu , compat_pvr , & local_err ); <nl> if ( local_err ) { <nl> error_report_err ( local_err ); <nl> return - 1 ;
static void digic_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = digic_realize ; <nl> + /* Reason : Uses serial_hds in the realize function --> not usable twice */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo digic_type_info = {
static uint64_t pci_read ( void * opaque , hwaddr addr , unsigned int size ) <nl> uint32_t val = 0 ; <nl> int bsel = s -> hotplug_select ; <nl>  <nl> - if ( bsel < 0 || bsel > ACPI_PCIHP_MAX_HOTPLUG_BUS ) { <nl> + if ( bsel < 0 || bsel >= ACPI_PCIHP_MAX_HOTPLUG_BUS ) { <nl> return 0 ; <nl> } <nl> 
static target_ulong disas_insn ( CPUX86State * env , DisasContext * s , <nl> } <nl> /* fallthru */ <nl> case 0xf9 ... 0xff : /* sfence */ <nl> + if (!( s -> cpuid_features & CPUID_SSE ) <nl> + || ( prefixes & PREFIX_LOCK )) { <nl> + goto illegal_op ; <nl> + } <nl> + break ; <nl> case 0xe8 ... 0xef : /* lfence */ <nl> case 0xf0 ... 0xf7 : /* mfence */ <nl> if (!( s -> cpuid_features & CPUID_SSE2 )
static void build_guest_fsinfo_for_virtual_device ( char const * syspath , <nl> dirpath = g_strdup_printf ("% s / slaves ", syspath ); <nl> dir = opendir ( dirpath ); <nl> if (! dir ) { <nl> - error_setg_errno ( errp , errno , " opendir (\"% s \")", dirpath ); <nl> + if ( errno != ENOENT ) { <nl> + error_setg_errno ( errp , errno , " opendir (\"% s \")", dirpath ); <nl> + } <nl> g_free ( dirpath ); <nl> return ; <nl> }
static int spapr_vty_init ( VIOsPAPRDevice * sdev ) <nl> { <nl> VIOsPAPRVTYDevice * dev = ( VIOsPAPRVTYDevice *) sdev ; <nl>  <nl> + if (! dev -> chardev ) { <nl> + fprintf ( stderr , " spapr - vty : Can ' t create vty without a chardev !\ n "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> qemu_chr_add_handlers ( dev -> chardev , vty_can_receive , <nl> vty_receive , NULL , dev ); <nl> 
static void free_test_data ( test_data * data ) <nl> g_free ( temp -> asl_file ); <nl> } <nl>  <nl> - g_array_free ( data -> tables , false ); <nl> + g_array_free ( data -> tables , true ); <nl> } <nl>  <nl> static uint8_t acpi_checksum ( const uint8_t * data , int len )
static void ahci_reg_init ( AHCIState * s ) <nl> s -> control_regs . cap = ( s -> ports - 1 ) | <nl> ( AHCI_NUM_COMMAND_SLOTS << 8 ) | <nl> ( AHCI_SUPPORTED_SPEED_GEN1 << AHCI_SUPPORTED_SPEED ) | <nl> - HOST_CAP_NCQ | HOST_CAP_AHCI ; <nl> + HOST_CAP_NCQ | HOST_CAP_AHCI | HOST_CAP_64 ; <nl>  <nl> s -> control_regs . impl = ( 1 << s -> ports ) - 1 ; <nl> 
static void hmp_migrate_status_cb ( void * opaque ) <nl> MigrationInfo * info ; <nl>  <nl> info = qmp_query_migrate ( NULL ); <nl> - if (! info -> has_status || strcmp ( info -> status , " active ") == 0 ) { <nl> + if (! info -> has_status || strcmp ( info -> status , " active ") == 0 || <nl> + strcmp ( info -> status , " setup ") == 0 ) { <nl> if ( info -> has_disk ) { <nl> int progress ; <nl> 
int coroutine_fn bdrv_is_allocated ( BlockDriverState * bs , int64_t sector_num , <nl> if ( ret < 0 ) { <nl> return ret ; <nl> } <nl> - return ( ret & BDRV_BLOCK_ALLOCATED ); <nl> + return !!( ret & BDRV_BLOCK_ALLOCATED ); <nl> } <nl>  <nl> /*
long do_rt_sigreturn ( CPUM68KState * env ) <nl> { <nl> struct target_rt_sigframe * frame ; <nl> abi_ulong frame_addr = env -> aregs [ 7 ] - 4 ; <nl> - target_sigset_t target_set ; <nl> sigset_t set ; <nl>  <nl> trace_user_do_rt_sigreturn ( env , frame_addr ); <nl> if (! lock_user_struct ( VERIFY_READ , frame , frame_addr , 1 )) <nl> goto badframe ; <nl>  <nl> - target_to_host_sigset_internal (& set , & target_set ); <nl> + target_to_host_sigset (& set , & frame -> uc . tuc_sigmask ); <nl> set_sigmask (& set ); <nl>  <nl> /* restore registers */
long do_sigreturn ( CPUPPCState * env ) <nl> { <nl> struct target_sigcontext * sc = NULL ; <nl> struct target_mcontext * sr = NULL ; <nl> - target_ulong sr_addr , sc_addr ; <nl> + target_ulong sr_addr = 0 , sc_addr ; <nl> sigset_t blocked ; <nl> target_sigset_t set ; <nl> 
static int timebase_post_load ( void * opaque , int version_id ) <nl> host_ns = qemu_clock_get_ns ( QEMU_CLOCK_HOST ); <nl> ns_diff = MAX ( 0 , host_ns - tb_remote -> time_of_the_day_ns ); <nl> migration_duration_ns = MIN ( NANOSECONDS_PER_SECOND , ns_diff ); <nl> - migration_duration_tb = muldiv64 ( migration_duration_ns , freq , <nl> + migration_duration_tb = muldiv64 ( freq , migration_duration_ns , <nl> NANOSECONDS_PER_SECOND ); <nl> guest_tb = tb_remote -> guest_timebase + MIN ( 0 , migration_duration_tb ); <nl> 
static void usb_ohci_init ( OHCIState * ohci , DeviceState * dev , <nl>  <nl> ohci -> as = as ; <nl>  <nl> + if ( num_ports > OHCI_MAX_PORTS ) { <nl> + error_setg ( errp , " OHCI num - ports =% d is too big ( limit is % d ports )", <nl> + num_ports , OHCI_MAX_PORTS ); <nl> + return ; <nl> + } <nl> + <nl> if ( usb_frame_time == 0 ) { <nl> # ifdef OHCI_TIME_WARP <nl> usb_frame_time = NANOSECONDS_PER_SECOND ;
static int read_cpuinfo ( const char * field , char * value , int len ) <nl> break ; <nl> } <nl> if (! strncmp ( line , field , field_len )) { <nl> - strncpy ( value , line , len ); <nl> + pstrcpy ( value , len , line ); <nl> ret = 0 ; <nl> break ; <nl> }
static void bdrv_dirty_bitmap_truncate ( BlockDriverState * bs ) <nl> continue ; <nl> } <nl> hbitmap_truncate ( bitmap -> bitmap , size ); <nl> + bitmap -> size = size ; <nl> } <nl> } <nl> 
int qemu_create_pidfile ( const char * filename ) <nl> return - 1 ; <nl> } <nl>  <nl> - close ( fd ); <nl> + /* keep pidfile open & locked forever */ <nl> return 0 ; <nl> }
ssize_t ne2000_receive ( NetClientState * nc , const uint8_t * buf , size_t size_ ) <nl> if ( index <= s -> stop ) <nl> avail = s -> stop - index ; <nl> else <nl> - avail = 0 ; <nl> + break ; <nl> len = size ; <nl> if ( len > avail ) <nl> len = avail ;
static void spapr_rng_class_init ( ObjectClass * oc , void * data ) <nl> dc -> realize = spapr_rng_realize ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> props = spapr_rng_properties ; <nl> + dc -> hotpluggable = false ; <nl> } <nl>  <nl> static const TypeInfo spapr_rng_info = {
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> update_displaychangelistener (& d -> ssd . dcl , GUI_REFRESH_INTERVAL_DEFAULT ); <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> + qemu_spice_display_switch (& d -> ssd , d -> ssd . ds ); <nl> vga_dirty_log_start (& d -> vga ); <nl> graphic_hw_update ( d -> vga . con ); <nl> }
# endif <nl>  <nl> typedef struct IOHandlerRecord { <nl> - int fd ; <nl> IOCanReadHandler * fd_read_poll ; <nl> IOHandler * fd_read ; <nl> IOHandler * fd_write ; <nl> - int deleted ; <nl> void * opaque ; <nl> QLIST_ENTRY ( IOHandlerRecord ) next ; <nl> + int fd ; <nl> + bool deleted ; <nl> } IOHandlerRecord ; <nl>  <nl> static QLIST_HEAD (, IOHandlerRecord ) io_handlers =
void vfio_region_finalize ( VFIORegion * region ) <nl> g_free ( region -> mmaps ); <nl>  <nl> trace_vfio_region_finalize ( region -> vbasedev -> name , region -> nr ); <nl> + <nl> + region -> mem = NULL ; <nl> + region -> mmaps = NULL ; <nl> + region -> nr_mmaps = 0 ; <nl> + region -> size = 0 ; <nl> + region -> flags = 0 ; <nl> + region -> nr = 0 ; <nl> } <nl>  <nl> void vfio_region_mmaps_set_enabled ( VFIORegion * region , bool enabled )
static int aio_write_f ( BlockBackend * blk , int argc , char ** argv ) <nl> int64_t count = cvtnum ( argv [ optind ]); <nl> if ( count < 0 ) { <nl> print_cvtnum_err ( count , argv [ optind ]); <nl> + g_free ( ctx ); <nl> return 0 ; <nl> } <nl> 
static void build_pci_bus_end ( PCIBus * bus , void * bus_state ) <nl>  <nl> build_append_byte ( notify , 0x7B ); /* AndOp */ <nl> build_append_byte ( notify , 0x68 ); /* Arg0Op */ <nl> - build_append_int ( notify , 0x1 << i ); <nl> + build_append_int ( notify , 0x1U << i ); <nl> build_append_byte ( notify , 0x00 ); /* NullName */ <nl> build_append_byte ( notify , 0x86 ); /* NotifyOp */ <nl> build_append_nameseg ( notify , " S %. 02X_ ", PCI_DEVFN ( i , 0 ));
static void cuda_receive_packet ( CUDAState * s , <nl> } <nl> break ; <nl> default : <nl> + obuf [ 0 ] = ERROR_PACKET ; <nl> + obuf [ 1 ] = 0x2 ; <nl> + obuf [ 2 ] = CUDA_PACKET ; <nl> + obuf [ 3 ] = data [ 0 ]; <nl> + cuda_send_packet_to_host ( s , obuf , 4 ); <nl> break ; <nl> } <nl> }
static void win_stdio_close ( CharDriverState * chr ) <nl> } <nl>  <nl> g_free ( chr -> opaque ); <nl> - g_free ( chr ); <nl> } <nl>  <nl> static CharDriverState * qemu_chr_open_stdio ( const char * id ,
void microblaze_load_kernel ( MicroBlazeCPU * cpu , hwaddr ddr_base , <nl> big_endian , ELF_MACHINE , 0 ); <nl> } <nl> /* Always boot into physical ram . */ <nl> - boot_info . bootstrap_pc = ddr_base + ( entry & 0x0fffffff ); <nl> + boot_info . bootstrap_pc = ( uint32_t ) entry ; <nl>  <nl> /* If it wasn ' t an ELF image , try an u - boot image . */ <nl> if ( kernel_size < 0 ) {
static int ohci_bus_start ( OHCIState * ohci ) <nl> /* Stop sending SOF tokens on the bus */ <nl> static void ohci_bus_stop ( OHCIState * ohci ) <nl> { <nl> - if ( ohci -> eof_timer ) <nl> + if ( ohci -> eof_timer ) { <nl> timer_del ( ohci -> eof_timer ); <nl> + timer_free ( ohci -> eof_timer ); <nl> + } <nl> ohci -> eof_timer = NULL ; <nl> } <nl> 
static void setup_frame ( int sig , struct target_sigaction * ka , <nl>  <nl> long do_rt_sigreturn ( CPUARMState * env ) <nl> { <nl> - struct target_rt_sigframe * frame ; <nl> + struct target_rt_sigframe * frame = NULL ; <nl> abi_ulong frame_addr = env -> xregs [ 31 ]; <nl>  <nl> if ( frame_addr & 15 ) {
static bool ga_open_pidfile ( const char * pidfile ) <nl> int pidfd ; <nl> char pidstr [ 32 ]; <nl>  <nl> - pidfd = open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> + pidfd = qemu_open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> if ( pidfd == - 1 || lockf ( pidfd , F_TLOCK , 0 )) { <nl> g_critical (" Cannot lock pid file , % s ", strerror ( errno )); <nl> if ( pidfd != - 1 ) {
static void vhost_dev_unassign_memory ( struct vhost_dev * dev , <nl> if ( start_addr <= reg -> guest_phys_addr && memlast >= reglast ) { <nl> -- dev -> mem -> nregions ; <nl> -- to ; <nl> - assert ( to >= 0 ); <nl> ++ overlap_middle ; <nl> continue ; <nl> }
int qcow2_alloc_cluster_offset ( BlockDriverState * bs , uint64_t offset , <nl>  <nl> again : <nl> start = offset ; <nl> - remaining = * num << BDRV_SECTOR_BITS ; <nl> + remaining = ( uint64_t )* num << BDRV_SECTOR_BITS ; <nl> cluster_offset = 0 ; <nl> * host_offset = 0 ; <nl> cur_bytes = 0 ;
static void error_exit ( int err , const char * msg ) <nl> void qemu_mutex_init ( QemuMutex * mutex ) <nl> { <nl> int err ; <nl> - pthread_mutexattr_t mutexattr ; <nl>  <nl> - pthread_mutexattr_init (& mutexattr ); <nl> - pthread_mutexattr_settype (& mutexattr , PTHREAD_MUTEX_ERRORCHECK ); <nl> - err = pthread_mutex_init (& mutex -> lock , & mutexattr ); <nl> - pthread_mutexattr_destroy (& mutexattr ); <nl> + err = pthread_mutex_init (& mutex -> lock , NULL ); <nl> if ( err ) <nl> error_exit ( err , __func__ ); <nl> }
static target_long monitor_get_ccr ( const struct MonitorDef * md , int val ) <nl>  <nl> u = 0 ; <nl> for ( i = 0 ; i < 8 ; i ++) <nl> - u |= env -> crf [ i ] << ( 32 - ( 4 * i )); <nl> + u |= env -> crf [ i ] << ( 32 - ( 4 * ( i + 1 ))); <nl>  <nl> return u ; <nl> }
static void virtio_net_guest_notifier_mask ( VirtIODevice * vdev , int idx , <nl> void virtio_net_set_config_size ( VirtIONet * n , uint32_t host_features ) <nl> { <nl> int i , config_size = 0 ; <nl> + host_features |= ( 1 << VIRTIO_NET_F_MAC ); <nl> for ( i = 0 ; feature_sizes [ i ]. flags != 0 ; i ++) { <nl> if ( host_features & feature_sizes [ i ]. flags ) { <nl> config_size = MAX ( feature_sizes [ i ]. end , config_size );
static uint64_t pit_ioport_read ( void * opaque , hwaddr addr , <nl> PITChannelState * s ; <nl>  <nl> addr &= 3 ; <nl> + <nl> + if ( addr == 3 ) { <nl> + /* Mode / Command register is write only , read is ignored */ <nl> + return 0 ; <nl> + } <nl> + <nl> s = & pit -> channels [ addr ]; <nl> if ( s -> status_latched ) { <nl> s -> status_latched = 0 ;
TranslationBlock * tb_gen_code ( CPUState * cpu , <nl> /* flush must be done */ <nl> tb_flush ( cpu ); <nl> mmap_unlock (); <nl> + /* Make the execution loop process the flush as soon as possible . */ <nl> + cpu -> exception_index = EXCP_INTERRUPT ; <nl> cpu_loop_exit ( cpu ); <nl> } <nl> 
static void loongarch_cpu_reset ( DeviceState * dev ) <nl>  <nl> # ifndef CONFIG_USER_ONLY <nl> env -> pc = 0x1c000000 ; <nl> + memset ( env -> tlb , 0 , sizeof ( env -> tlb )); <nl> # endif <nl>  <nl> restore_fp_status ( env );
static void cg3_realizefn ( DeviceState * dev , Error ** errp ) <nl> if ( fcode_filename ) { <nl> ret = load_image_targphys ( fcode_filename , s -> prom_addr , <nl> FCODE_MAX_ROM_SIZE ); <nl> + g_free ( fcode_filename ); <nl> if ( ret < 0 || ret > FCODE_MAX_ROM_SIZE ) { <nl> error_report (" cg3 : could not load prom '% s '", CG3_ROM_FILE ); <nl> }
static int pfpu_decode_insn ( MilkymistPFPUState * s ) <nl> uint32_t reg_b = ( insn >> 11 ) & 0x7f ; <nl> uint32_t op = ( insn >> 7 ) & 0xf ; <nl> uint32_t reg_d = insn & 0x7f ; <nl> - uint32_t r ; <nl> + uint32_t r = 0 ; <nl> int latency = 0 ; <nl>  <nl> switch ( op ) {
static void vfio_put_device ( VFIOPCIDevice * vdev ) <nl> { <nl> g_free ( vdev -> vbasedev . name ); <nl> if ( vdev -> msix ) { <nl> + object_unparent ( OBJECT (& vdev -> msix -> mmap_mem )); <nl> g_free ( vdev -> msix ); <nl> vdev -> msix = NULL ; <nl> }
static int ppce500_load_device_tree ( MachineState * machine , <nl> } <nl>  <nl> fdt = load_device_tree ( filename , & fdt_size ); <nl> + g_free ( filename ); <nl> if (! fdt ) { <nl> goto out ; <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> break ; <nl> } <nl> # endif <nl> + case PR_GET_SECCOMP : <nl> + case PR_SET_SECCOMP : <nl> + /* Disable seccomp to prevent the target disabling syscalls we <nl> + * need . */ <nl> + ret = - TARGET_EINVAL ; <nl> + break ; <nl> default : <nl> /* Most prctl options have no pointer arguments */ <nl> ret = get_errno ( prctl ( arg1 , arg2 , arg3 , arg4 , arg5 ));
static void timer_enable ( struct xlx_timer * xt ) <nl> count = xt -> regs [ R_TLR ]; <nl> else <nl> count = ~ 0 - xt -> regs [ R_TLR ]; <nl> - ptimer_set_count ( xt -> ptimer , count ); <nl> + ptimer_set_limit ( xt -> ptimer , count , 1 ); <nl> ptimer_run ( xt -> ptimer , 1 ); <nl> } <nl> 
static void net_slirp_cleanup ( NetClientState * nc ) <nl> SlirpState * s = DO_UPCAST ( SlirpState , nc , nc ); <nl>  <nl> slirp_cleanup ( s -> slirp ); <nl> - qemu_remove_exit_notifier (& s -> exit_notifier ); <nl> + if ( s -> exit_notifier . notify ) { <nl> + qemu_remove_exit_notifier (& s -> exit_notifier ); <nl> + } <nl> slirp_smb_cleanup ( s ); <nl> QTAILQ_REMOVE (& slirp_stacks , s , entry ); <nl> }
static int count_contiguous_clusters ( uint64_t nb_clusters , int cluster_size , <nl> uint64_t * l2_table , uint64_t stop_flags ) <nl> { <nl> int i ; <nl> - uint64_t mask = stop_flags | L2E_OFFSET_MASK | QCOW2_CLUSTER_COMPRESSED ; <nl> + uint64_t mask = stop_flags | L2E_OFFSET_MASK | QCOW_OFLAG_COMPRESSED ; <nl> uint64_t first_entry = be64_to_cpu ( l2_table [ 0 ]); <nl> uint64_t offset = first_entry & mask ; <nl> 
TPMVersion tpm_tis_get_tpm_version ( Object * obj ) <nl> { <nl> TPMState * s = TPM ( obj ); <nl>  <nl> + if ( tpm_backend_had_startup_error ( s -> be_driver )) { <nl> + return TPM_VERSION_UNSPEC ; <nl> + } <nl> + <nl> return tpm_backend_get_tpm_version ( s -> be_driver ); <nl> } <nl> 
restart : <nl> aio_context_release ( pool -> ctx ); <nl> elem -> common . cb ( elem -> common . opaque , elem -> ret ); <nl> aio_context_acquire ( pool -> ctx ); <nl> + <nl> + /* We can safely cancel the completion_bh here regardless of someone <nl> + * else having scheduled it meanwhile because we reenter the <nl> + * completion function anyway ( goto restart ). <nl> + */ <nl> + qemu_bh_cancel ( pool -> completion_bh ); <nl> + <nl> qemu_aio_unref ( elem ); <nl> goto restart ; <nl> } else {
static int bad_mode_switch ( CPUARMState * env , int mode ) <nl> return ! arm_feature ( env , ARM_FEATURE_EL2 ) <nl> || arm_current_el ( env ) < 2 || arm_is_secure ( env ); <nl> case ARM_CPU_MODE_MON : <nl> - return ! arm_is_secure ( env ); <nl> + return arm_current_el ( env ) < 3 ; <nl> default : <nl> return 1 ; <nl> }
static int vmxnet3_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> + vmxnet3_validate_interrupts ( s ); <nl> + <nl> return 0 ; <nl> } <nl> 
DeviceState * qdev_device_add ( QemuOpts * opts ) <nl> } <nl> } <nl>  <nl> - if (! oc ) { <nl> + if (! object_class_dynamic_cast ( oc , TYPE_DEVICE )) { <nl> qerror_report ( QERR_INVALID_PARAMETER_VALUE , " driver ", " device type "); <nl> return NULL ; <nl> }
static void do_pci_unregister_device ( PCIDevice * pci_dev ) <nl> pci_dev -> bus -> devices [ pci_dev -> devfn ] = NULL ; <nl> pci_config_free ( pci_dev ); <nl>  <nl> - memory_region_del_subregion (& pci_dev -> bus_master_container_region , <nl> - & pci_dev -> bus_master_enable_region ); <nl> + if ( memory_region_is_mapped (& pci_dev -> bus_master_enable_region )) { <nl> + memory_region_del_subregion (& pci_dev -> bus_master_container_region , <nl> + & pci_dev -> bus_master_enable_region ); <nl> + } <nl> address_space_destroy (& pci_dev -> bus_master_as ); <nl> } <nl> 
static void s390_init ( ram_addr_t ram_size , <nl>  <nl> bios_filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> bios_size = load_image ( bios_filename , qemu_get_ram_ptr ( ZIPL_LOAD_ADDR )); <nl> + qemu_free ( bios_filename ); <nl>  <nl> if (( long ) bios_size < 0 ) { <nl> hw_error (" could not load bootloader '% s '\ n ", bios_name );
static bool ga_open_pidfile ( const char * pidfile ) <nl> pidfd = open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> if ( pidfd == - 1 || lockf ( pidfd , F_TLOCK , 0 )) { <nl> g_critical (" Cannot lock pid file , % s ", strerror ( errno )); <nl> + if ( pidfd != - 1 ) { <nl> + close ( pidfd ); <nl> + } <nl> return false ; <nl> } <nl> 
static void qemu_chr_free_common ( CharDriverState * chr ) <nl> if ( chr -> logfd != - 1 ) { <nl> close ( chr -> logfd ); <nl> } <nl> + qemu_mutex_destroy (& chr -> chr_write_lock ); <nl> g_free ( chr ); <nl> } <nl> 
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl> /* The snapshot list position has not yet been updated , so these clusters <nl> * must indeed be completely free */ <nl> ret = qcow2_pre_write_overlap_check ( bs , QCOW2_OL_DEFAULT , offset , <nl> - s -> snapshots_size ); <nl> + snapshots_size ); <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
bool sysbus_has_irq ( SysBusDevice * dev , int n ) <nl> ObjectProperty * r ; <nl>  <nl> r = object_property_find ( OBJECT ( dev ), prop , NULL ); <nl> + g_free ( prop ); <nl> + <nl> return ( r != NULL ); <nl> } <nl> 
static int css_interpret_ccw ( SubchDev * sch , hwaddr ccw_addr , <nl> if (! ccw_addr ) { <nl> return - EIO ; <nl> } <nl> + /* Check doubleword aligned and 31 or 24 ( fmt 0 ) bit addressable . */ <nl> + if ( ccw_addr & ( sch -> ccw_fmt_1 ? 0x80000007 : 0xff000007 )) { <nl> + return - EINVAL ; <nl> + } <nl>  <nl> /* Translate everything to format - 1 ccws - the information is the same . */ <nl> ccw = copy_ccw_from_guest ( ccw_addr , sch -> ccw_fmt_1 );
static void ipmi_init_sensors_from_sdrs ( IPMIBmcSim * s ) <nl> static int ipmi_register_netfn ( IPMIBmcSim * s , unsigned int netfn , <nl> const IPMINetfn * netfnd ) <nl> { <nl> - if (( netfn & 1 ) || ( netfn > MAX_NETFNS ) || ( s -> netfns [ netfn / 2 ])) { <nl> + if (( netfn & 1 ) || ( netfn >= MAX_NETFNS ) || ( s -> netfns [ netfn / 2 ])) { <nl> return - 1 ; <nl> } <nl> s -> netfns [ netfn / 2 ] = netfnd ;
char * vnc_display_local_addr ( const char * id ) <nl> { <nl> VncDisplay * vs = vnc_display_find ( id ); <nl>  <nl> + assert ( vs ); <nl> return vnc_socket_local_addr ("% s :% s ", vs -> lsock ); <nl> } <nl> 
static QemuOptsList qemu_vnc_opts = { <nl> },{ <nl> . name = " connections ", <nl> . type = QEMU_OPT_NUMBER , <nl> + },{ <nl> + . name = " to ", <nl> + . type = QEMU_OPT_NUMBER , <nl> + },{ <nl> + . name = " ipv4 ", <nl> + . type = QEMU_OPT_BOOL , <nl> + },{ <nl> + . name = " ipv6 ", <nl> + . type = QEMU_OPT_BOOL , <nl> },{ <nl> . name = " password ", <nl> . type = QEMU_OPT_BOOL ,
hwaddr s390_cpu_get_phys_page_debug ( CPUState * cs , vaddr vaddr ) <nl> vaddr &= 0x7fffffff ; <nl> } <nl>  <nl> - mmu_translate ( env , vaddr , MMU_INST_FETCH , asc , & raddr , & prot , false ); <nl> - <nl> + if ( mmu_translate ( env , vaddr , MMU_INST_FETCH , asc , & raddr , & prot , false )) { <nl> + return - 1 ; <nl> + } <nl> return raddr ; <nl> } <nl> 
static void vnc_display_close ( VncDisplay * vs ) <nl> vs -> subauth = VNC_AUTH_INVALID ; <nl> if ( vs -> tlscreds ) { <nl> object_unparent ( OBJECT ( vs -> tlscreds )); <nl> + vs -> tlscreds = NULL ; <nl> } <nl> g_free ( vs -> tlsaclname ); <nl> vs -> tlsaclname = NULL ;
void nand_setio ( DeviceState * dev , uint32_t value ) <nl>  <nl> if ( s -> ale ) { <nl> unsigned int shift = s -> addrlen * 8 ; <nl> - unsigned int mask = ~( 0xff << shift ); <nl> - unsigned int v = value << shift ; <nl> + uint64_t mask = ~( 0xffull << shift ); <nl> + uint64_t v = ( uint64_t ) value << shift ; <nl>  <nl> s -> addr = ( s -> addr & mask ) | v ; <nl> s -> addrlen ++;
static void tap_cleanup ( VLANClientState * nc ) <nl>  <nl> if ( s -> vhost_net ) { <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> + s -> vhost_net = NULL ; <nl> } <nl>  <nl> qemu_purge_queued_packets ( nc );
static void disas_s390_insn ( DisasContext * s ) <nl> store_reg32 ( r1 , tmp32_1 ); <nl> tcg_gen_trunc_i64_i32 ( tmp32_2 , tmp2 ); <nl> store_reg32 ( r1 + 1 , tmp32_2 ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> + tcg_temp_free_i64 ( tmp2 ); <nl> break ; <nl> case 0x98 : /* LM R1 , R3 , D2 ( B2 ) [ RS ] */ <nl> case 0x90 : /* STM R1 , R3 , D2 ( B2 ) [ RS ] */
void fork_end ( int child ) <nl> Discard information about the parent threads . */ <nl> CPU_FOREACH_SAFE ( cpu , next_cpu ) { <nl> if ( cpu != thread_cpu ) { <nl> - QTAILQ_REMOVE (& cpus , thread_cpu , node ); <nl> + QTAILQ_REMOVE (& cpus , cpu , node ); <nl> } <nl> } <nl> pending_cpus = 0 ;
static void entropy_available ( void * opaque ) <nl> ssize_t len ; <nl>  <nl> len = read ( s -> fd , buffer , s -> size ); <nl> + if ( len < 0 && errno == EAGAIN ) { <nl> + return ; <nl> + } <nl> g_assert ( len != - 1 ); <nl>  <nl> s -> receive_func ( s -> opaque , buffer , len );
int main ( int argc , char ** argv ) <nl>  <nl> path = g_strdup_printf (" eepro100 /% s ", models [ i ]); <nl> qtest_add_data_func ( path , models [ i ], test_device ); <nl> + g_free ( path ); <nl> } <nl>  <nl> return g_test_run ();
static always_inline void gen_store_gpr64 ( int reg , TCGv t ) { <nl> tcg_gen_mov_i64 ( cpu_gpr [ reg ], t ); <nl> # else <nl> tcg_gen_trunc_i64_i32 ( cpu_gpr [ reg ], t ); <nl> - TCGv tmp = tcg_temp_local_new ( TCG_TYPE_I64 ); <nl> + TCGv tmp = tcg_temp_new ( TCG_TYPE_I64 ); <nl> tcg_gen_shri_i64 ( tmp , t , 32 ); <nl> tcg_gen_trunc_i64_i32 ( cpu_gprh [ reg ], tmp ); <nl> tcg_temp_free ( tmp );
void coroutine_fn qemu_coroutine_yield ( void ) <nl> } <nl>  <nl> self -> caller = NULL ; <nl> - coroutine_swap ( self , to ); <nl> + qemu_coroutine_switch ( self , to , COROUTINE_YIELD ); <nl> }
static void tcg_liveness_analysis ( TCGContext * s ) <nl>  <nl> nb_ops = gen_opc_ptr - gen_opc_buf ; <nl>  <nl> - /* XXX : make it really dynamic */ <nl> - s -> op_dead_iargs = tcg_malloc ( OPC_BUF_SIZE * sizeof ( uint16_t )); <nl> + s -> op_dead_iargs = tcg_malloc ( nb_ops * sizeof ( uint16_t )); <nl>  <nl> dead_temps = tcg_malloc ( s -> nb_temps ); <nl> memset ( dead_temps , 1 , s -> nb_temps );
static int enable_write_target ( BDRVVVFATState * s , Error ** errp ) <nl> set_option_parameter ( options , BLOCK_OPT_BACKING_FILE , " fat :"); <nl>  <nl> ret = bdrv_create ( bdrv_qcow , s -> qcow_filename , options , errp ); <nl> + free_option_parameters ( options ); <nl> if ( ret < 0 ) { <nl> goto err ; <nl> }
static void vhost_scsi_unrealize ( DeviceState * dev , Error ** errp ) <nl> /* This will stop vhost backend . */ <nl> vhost_scsi_set_status ( vdev , 0 ); <nl>  <nl> + vhost_dev_cleanup (& s -> dev ); <nl> g_free ( s -> dev . vqs ); <nl>  <nl> virtio_scsi_common_unrealize ( dev , errp );
static void spapr_nvram_class_init ( ObjectClass * klass , void * data ) <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> props = spapr_nvram_properties ; <nl> dc -> vmsd = & vmstate_spapr_nvram ; <nl> + /* Reason : Internal device only , uses spapr_rtas_register () in realize () */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo spapr_nvram_type_info = {
static int xen_platform_initfn ( PCIDevice * dev ) <nl> PCIXenPlatformState * d = XEN_PLATFORM ( dev ); <nl> uint8_t * pci_conf ; <nl>  <nl> + /* Device will crash on reset if xen is not initialized */ <nl> + assert ( xen_enabled ()); <nl> + <nl> pci_conf = dev -> config ; <nl>  <nl> pci_set_word ( pci_conf + PCI_COMMAND , PCI_COMMAND_IO | PCI_COMMAND_MEMORY );
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> if (( txdw0 & CP_TX_LGSEN ) && ip_protocol == IP_PROTO_TCP ) <nl> { <nl> + /* Large enough for the TCP header ? */ <nl> + if ( ip_data_len < sizeof ( tcp_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> int large_send_mss = ( txdw0 >> 16 ) & CP_TC_LGSEN_MSS_MASK ; <nl>  <nl> DPRINTF ("+++ C + mode offloaded task TSO MTU =% d IP data % d "
static void spapr_tce_reset ( DeviceState * dev ) <nl> sPAPRTCETable * tcet = SPAPR_TCE_TABLE ( dev ); <nl> size_t table_size = tcet -> nb_table * sizeof ( uint64_t ); <nl>  <nl> - memset ( tcet -> table , 0 , table_size ); <nl> + if ( tcet -> nb_table ) { <nl> + memset ( tcet -> table , 0 , table_size ); <nl> + } <nl> } <nl>  <nl> static target_ulong put_tce_emu ( sPAPRTCETable * tcet , target_ulong ioba ,
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> + if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> + return - EINVAL ; <nl> + } <nl> return 0 ; <nl> } <nl> 
void qemu_mutex_lock_iothread ( void ) <nl> * TCG code execution . <nl> */ <nl> if (! tcg_enabled () || qemu_in_vcpu_thread () || <nl> - ! first_cpu || ! first_cpu -> thread ) { <nl> + ! first_cpu || ! first_cpu -> created ) { <nl> qemu_mutex_lock (& qemu_global_mutex ); <nl> atomic_dec (& iothread_requesting_mutex ); <nl> } else {
CharDriverState * qemu_chr_open ( const char * label , const char * filename , void (* i <nl> if ( chr && qemu_opt_get_bool ( opts , " mux ", 0 )) { <nl> monitor_init ( chr , MONITOR_USE_READLINE ); <nl> } <nl> + qemu_opts_del ( opts ); <nl> return chr ; <nl> } <nl> 
void cuda_init ( int * cuda_mem_index , qemu_irq irq ) <nl>  <nl> s -> timers [ 1 ]. index = 1 ; <nl>  <nl> - qemu_get_timedate (& tm , RTC_OFFSET ); <nl> - s -> tick_offset = mktimegm (& tm ); <nl> + qemu_get_timedate (& tm , 0 ); <nl> + s -> tick_offset = ( uint32_t ) mktimegm (& tm ) + RTC_OFFSET ; <nl>  <nl> s -> adb_poll_timer = qemu_new_timer ( vm_clock , cuda_adb_poll , s ); <nl> * cuda_mem_index = cpu_register_io_memory ( 0 , cuda_read , cuda_write , s );
static int32_t scsi_send_command ( SCSIDevice * d , uint32_t tag , <nl> uint8_t * cmd , int lun ) <nl> { <nl> SCSIDeviceState * s = d -> state ; <nl> - uint32_t len ; <nl> - int cmdlen ; <nl> + uint32_t len = 0 ; <nl> + int cmdlen = 0 ; <nl> SCSIRequest * r ; <nl> int ret ; <nl> 
static int scsi_disk_initfn ( SCSIDevice * dev ) <nl> } <nl> s -> bs = s -> qdev . conf . dinfo -> bdrv ; <nl>  <nl> + if ( bdrv_is_sg ( s -> bs )) { <nl> + qemu_error (" scsi - disk : unwanted / dev / sg *\ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( bdrv_get_type_hint ( s -> bs ) == BDRV_TYPE_CDROM ) { <nl> s -> cluster_size = 4 ; <nl> } else {
int qdev_device_help ( QemuOpts * opts ) <nl> return 1 ; <nl> } <nl>  <nl> - if (! qemu_opt_get ( opts , "?")) { <nl> + if (! driver || ! qemu_opt_get ( opts , "?")) { <nl> return 0 ; <nl> } <nl> 
static VncServerInfo * vnc_server_info_get ( VncDisplay * vd ) <nl> VncServerInfo * info ; <nl> Error * err = NULL ; <nl>  <nl> - info = g_malloc ( sizeof (* info )); <nl> + info = g_malloc0 ( sizeof (* info )); <nl> vnc_init_basic_info_from_server_addr ( vd -> lsock , <nl> qapi_VncServerInfo_base ( info ), & err ); <nl> info -> has_auth = true ;
void AUD_del_capture ( CaptureVoiceOut * cap , void * cb_opaque ) <nl> sw = sw1 ; <nl> } <nl> QLIST_REMOVE ( cap , entries ); <nl> + g_free ( cap -> hw . mix_buf ); <nl> + g_free ( cap -> buf ); <nl> g_free ( cap ); <nl> } <nl> return ;
static void test_smram_lock ( void ) <nl> g_assert ( smram_test_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN ) == false ); <nl> smram_set_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN , true ); <nl> g_assert ( smram_test_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN ) == true ); <nl> + <nl> + g_free ( pcidev ); <nl> + qpci_free_pc ( pcibus ); <nl> } <nl>  <nl> int main ( int argc , char ** argv )
static int megasas_dcmd_ld_get_info ( MegasasState * s , MegasasCmd * cmd ) <nl>  <nl> static int megasas_dcmd_cfg_read ( MegasasState * s , MegasasCmd * cmd ) <nl> { <nl> - uint8_t data [ 4096 ]; <nl> + uint8_t data [ 4096 ] = { 0 }; <nl> struct mfi_config_data * info ; <nl> int num_pd_disks = 0 , array_offset , ld_offset ; <nl> BusChild * kid ;
void object_property_set_qobject ( Object * obj , QObject * value , <nl> const char * name , Error ** errp ) <nl> { <nl> Visitor * v ; <nl> - /* TODO : Should we reject , rather than ignore , excess input ? */ <nl> - v = qobject_input_visitor_new ( value , false ); <nl> + <nl> + v = qobject_input_visitor_new ( value , true ); <nl> object_property_set ( obj , v , name , errp ); <nl> visit_free ( v ); <nl> }
static void tci_out_label ( TCGContext * s , TCGArg arg ) <nl> assert ( label -> u . value ); <nl> } else { <nl> tcg_out_reloc ( s , s -> code_ptr , sizeof ( tcg_target_ulong ), arg , 0 ); <nl> - tcg_out_i ( s , 0 ); <nl> + s -> code_ptr += sizeof ( tcg_target_ulong ); <nl> } <nl> } <nl> 
static void ccw_machine_class_init ( ObjectClass * oc , void * data ) <nl> mc -> no_parallel = 1 ; <nl> mc -> no_sdcard = 1 ; <nl> mc -> use_sclp = 1 ; <nl> - mc -> max_cpus = 255 ; <nl> + mc -> max_cpus = 248 ; <nl> mc -> get_hotplug_handler = s390_get_hotplug_handler ; <nl> hc -> plug = s390_machine_device_plug ; <nl> nc -> nmi_monitor_handler = s390_nmi ;
retry : <nl> goto retry ; <nl> } <nl> } <nl> + <nl> + /* Make sure that all offsets in the " allocated " range are representable <nl> + * in an int64_t */ <nl> + if ( s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) { <nl> + return - EFBIG ; <nl> + } <nl> + <nl> # ifdef DEBUG_ALLOC2 <nl> fprintf ( stderr , " alloc_clusters : size =%" PRId64 " -> %" PRId64 "\ n ", <nl> size ,
void qmp_drive_mirror ( const char * device , const char * target , <nl> if (! source && sync == MIRROR_SYNC_MODE_TOP ) { <nl> sync = MIRROR_SYNC_MODE_FULL ; <nl> } <nl> + if ( sync == MIRROR_SYNC_MODE_NONE ) { <nl> + source = bs ; <nl> + } <nl>  <nl> size = bdrv_getlength ( bs ); <nl> if ( size < 0 ) {
SCSIRequest * scsi_req_new ( SCSIDevice * d , uint32_t tag , uint32_t lun , <nl> } else { <nl> trace_scsi_req_parsed ( d -> id , lun , tag , buf [ 0 ], <nl> cmd . mode , cmd . xfer ); <nl> - if ( req -> cmd . lba != - 1 ) { <nl> + if ( cmd . lba != - 1 ) { <nl> trace_scsi_req_parsed_lba ( d -> id , lun , tag , buf [ 0 ], <nl> cmd . lba ); <nl> }
void hid_reset ( HIDState * hs ) <nl> memset ( hs -> kbd . keycodes , 0 , sizeof ( hs -> kbd . keycodes )); <nl> memset ( hs -> kbd . key , 0 , sizeof ( hs -> kbd . key )); <nl> hs -> kbd . keys = 0 ; <nl> + hs -> kbd . modifiers = 0 ; <nl> break ; <nl> case HID_MOUSE : <nl> case HID_TABLET :
void qdev_init_nofail ( DeviceState * dev ) <nl>  <nl> assert (! dev -> realized ); <nl>  <nl> + object_ref ( OBJECT ( dev )); <nl> object_property_set_bool ( OBJECT ( dev ), true , " realized ", & err ); <nl> if ( err ) { <nl> error_reportf_err ( err , " Initialization of device % s failed : ", <nl> object_get_typename ( OBJECT ( dev ))); <nl> exit ( 1 ); <nl> } <nl> + object_unref ( OBJECT ( dev )); <nl> } <nl>  <nl> void qdev_machine_creation_done ( void )
int qemu_chr_be_can_write ( CharDriverState * s ) <nl>  <nl> void qemu_chr_be_write ( CharDriverState * s , uint8_t * buf , int len ) <nl> { <nl> - s -> chr_read ( s -> handler_opaque , buf , len ); <nl> + if ( s -> chr_read ) { <nl> + s -> chr_read ( s -> handler_opaque , buf , len ); <nl> + } <nl> } <nl>  <nl> int qemu_chr_fe_get_msgfd ( CharDriverState * s )
static void cpu_common_initfn ( Object * obj ) <nl>  <nl> cpu -> cpu_index = UNASSIGNED_CPU_INDEX ; <nl> cpu -> gdb_num_regs = cpu -> gdb_num_g_regs = cc -> gdb_num_core_regs ; <nl> + /* *- user doesn ' t have configurable SMP topology */ <nl> + /* the default value is changed by qemu_init_vcpu () for softmmu */ <nl> + cpu -> nr_cores = 1 ; <nl> + cpu -> nr_threads = 1 ; <nl> + <nl> qemu_mutex_init (& cpu -> work_mutex ); <nl> QTAILQ_INIT (& cpu -> breakpoints ); <nl> QTAILQ_INIT (& cpu -> watchpoints );
static void do_test_equality ( bool expected , int _ , ...) <nl> g_assert ( qobject_is_equal ( args [ i ], args [ j ]) == expected ); <nl> } <nl> } <nl> + <nl> + g_free ( args ); <nl> } <nl>  <nl> # define check_equal (...) \
static int rtl8139_can_receive ( VLANClientState * nc ) <nl> } else { <nl> avail = MOD2 ( s -> RxBufferSize + s -> RxBufPtr - s -> RxBufAddr , <nl> s -> RxBufferSize ); <nl> - return ( avail == 0 || avail >= 1514 ); <nl> + return ( avail == 0 || avail >= 1514 || ( s -> IntrMask & RxOverflow )); <nl> } <nl> } <nl> 
static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail ; <nl> } <nl>  <nl> - s -> pagetable = qemu_blockalign ( bs , s -> max_table_entries * 4 ); <nl> + s -> pagetable = qemu_try_blockalign ( bs -> file , s -> max_table_entries * 4 ); <nl> + if ( s -> pagetable == NULL ) { <nl> + ret = - ENOMEM ; <nl> + goto fail ; <nl> + } <nl>  <nl> s -> bat_offset = be64_to_cpu ( dyndisk_header -> table_offset ); <nl> 
static void pci_qdev_unrealize ( DeviceState * dev , Error ** errp ) <nl> pc -> exit ( pci_dev ); <nl> } <nl>  <nl> + pci_device_deassert_intx ( pci_dev ); <nl> do_pci_unregister_device ( pci_dev ); <nl> } <nl> 
static void serial_update_parameters ( SerialState * s ) <nl> int speed , parity , data_bits , stop_bits , frame_size ; <nl> QEMUSerialSetParams ssp ; <nl>  <nl> - if ( s -> divider == 0 ) <nl> + if ( s -> divider == 0 || s -> divider > s -> baudbase ) { <nl> return ; <nl> + } <nl>  <nl> /* Start bit . */ <nl> frame_size = 1 ;
static void v9fs_post_lcreate ( V9fsState * s , V9fsLcreateState * vs , int err ) <nl> err = vs -> offset ; <nl> } else { <nl> vs -> fidp -> fid_type = P9_FID_NONE ; <nl> - close ( vs -> fidp -> fs . fd ); <nl> err = - errno ; <nl> + if ( vs -> fidp -> fs . fd > 0 ) { <nl> + close ( vs -> fidp -> fs . fd ); <nl> + } <nl> } <nl>  <nl> complete_pdu ( s , vs -> pdu , err );
static int qcrypto_ivgen_essiv_init ( QCryptoIVGen * ivgen , <nl> & salt , & nhash , <nl> errp ) < 0 ) { <nl> g_free ( essiv ); <nl> + g_free ( salt ); <nl> return - 1 ; <nl> } <nl> 
void ppc_slb_invalidate_all ( CPUPPCState * env ) <nl>  <nl> do_invalidate = 0 ; <nl> sr_base = env -> spr [ SPR_ASR ]; <nl> - for ( n = 0 ; n < env -> slb_nr ; n ++) { <nl> + /* XXX : Warning : slbia never invalidates the first segment */ <nl> + for ( n = 1 ; n < env -> slb_nr ; n ++) { <nl> tmp64 = ldq_phys ( sr_base ); <nl> if ( slb_is_valid ( tmp64 )) { <nl> slb_invalidate (& tmp64 );
static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> ( s -> incompatible_features & QCOW2_INCOMPAT_DIRTY )) { <nl> BdrvCheckResult result = { 0 }; <nl>  <nl> - ret = qcow2_check ( bs , & result , BDRV_FIX_ERRORS ); <nl> + ret = qcow2_check ( bs , & result , BDRV_FIX_ERRORS | BDRV_FIX_LEAKS ); <nl> if ( ret < 0 ) { <nl> error_setg_errno ( errp , - ret , " Could not repair dirty image "); <nl> goto fail ;
QGuestAllocator * pc_alloc_init ( void ) <nl> /* Respect PCI hole */ <nl> s -> end = MIN ( ram_size , 0xE0000000 ); <nl>  <nl> + /* clean - up */ <nl> + g_free ( fw_cfg ); <nl> + <nl> return & s -> alloc ; <nl> }
static QObject * qmp_output_pop ( QmpOutputVisitor * qov ) <nl> static QObject * qmp_output_first ( QmpOutputVisitor * qov ) <nl> { <nl> QStackEntry * e = QTAILQ_LAST (& qov -> stack , QStack ); <nl> + <nl> + /* FIXME - find a better way to deal with NULL values */ <nl> + if (! e ) { <nl> + return NULL ; <nl> + } <nl> + <nl> return e -> value ; <nl> } <nl> 
vvfat_co_pwritev ( BlockDriverState * bs , uint64_t offset , uint64_t bytes , <nl> static int64_t coroutine_fn vvfat_co_get_block_status ( BlockDriverState * bs , <nl> int64_t sector_num , int nb_sectors , int * n , BlockDriverState ** file ) <nl> { <nl> - BDRVVVFATState * s = bs -> opaque ; <nl> - * n = s -> sector_count - sector_num ; <nl> + * n = bs -> total_sectors - sector_num ; <nl> if (* n > nb_sectors ) { <nl> * n = nb_sectors ; <nl> } else if (* n < 0 ) {
static void pc_fw_add_pflash_drv ( void ) <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl>  <nl> opts = drive_add ( IF_PFLASH , - 1 , filename , " readonly = on "); <nl> + <nl> + g_free ( filename ); <nl> + <nl> if ( opts == NULL ) { <nl> return ; <nl> }
int unix_connect_opts ( QemuOpts * opts ) <nl> snprintf ( un . sun_path , sizeof ( un . sun_path ), "% s ", path ); <nl> if ( connect ( sock , ( struct sockaddr *) & un , sizeof ( un )) < 0 ) { <nl> fprintf ( stderr , " connect ( unix :% s ): % s \ n ", path , strerror ( errno )); <nl> + close ( sock ); <nl> return - 1 ; <nl> } <nl> 
static ssize_t mp_dacl_listxattr ( FsContext * ctx , const char * path , <nl> } <nl>  <nl> /* len includes the trailing NUL */ <nl> - memcpy ( value , ACL_ACCESS , len ); <nl> + memcpy ( value , ACL_DEFAULT , len ); <nl> return 0 ; <nl> } <nl> 
static void handle_rev16 ( DisasContext * s , unsigned int sf , <nl> tcg_gen_shli_i64 ( tcg_rd , tcg_rd , 8 ); <nl> tcg_gen_or_i64 ( tcg_rd , tcg_rd , tcg_tmp ); <nl>  <nl> + tcg_temp_free_i64 ( mask ); <nl> tcg_temp_free_i64 ( tcg_tmp ); <nl> } <nl> 
static int img_bench ( int argc , char ** argv ) <nl> BlockBackend * blk = NULL ; <nl> BenchData data = {}; <nl> int flags = 0 ; <nl> - bool writethrough ; <nl> + bool writethrough = false ; <nl> struct timeval t1 , t2 ; <nl> int i ; <nl> 
static QemuOptsList nbd_runtime_opts = { <nl> . type = QEMU_OPT_STRING , <nl> . help = " ID of the TLS credentials to use ", <nl> }, <nl> + { /* end of list */ } <nl> }, <nl> }; <nl> 
static ssize_t rtl8139_do_receive ( NetClientState * nc , const uint8_t * buf , size_t <nl> s -> IntrStatus |= RxOverflow ; <nl> ++ s -> RxMissed ; <nl> rtl8139_update_irq ( s ); <nl> - return size_ ; <nl> + return 0 ; <nl> } <nl>  <nl> packet_header |= RxStatusOK ;
pixman_format_code_t qemu_default_pixman_format ( int bpp , bool native_endian ) <nl> break ; <nl> } <nl> } <nl> - g_assert_not_reached (); <nl> + return 0 ; <nl> } <nl>  <nl> int qemu_pixman_get_type ( int rshift , int gshift , int bshift )
fork_exec ( struct socket * so , const char * ex , int do_pty ) <nl> bind ( s , ( struct sockaddr *)& addr , addrlen ) < 0 || <nl> listen ( s , 1 ) < 0 ) { <nl> error_report (" Error : inet socket : % s ", strerror ( errno )); <nl> - closesocket ( s ); <nl> + if ( s >= 0 ) { <nl> + closesocket ( s ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static char * SocketAddress_to_str ( const char * prefix , SocketAddress * addr , <nl> return g_strdup_printf ("% sfd :% s % s ", prefix , addr -> u . fd . data -> str , <nl> is_listen ? ", server " : ""); <nl> break ; <nl> + case SOCKET_ADDRESS_KIND_VSOCK : <nl> + return g_strdup_printf ("% svsock :% s :% s ", prefix , <nl> + addr -> u . vsock . data -> cid , <nl> + addr -> u . vsock . data -> port ); <nl> default : <nl> abort (); <nl> }
# define TT_DPROT 0x6c <nl> # define TT_SPILL 0x80 <nl> # define TT_FILL 0xc0 <nl> -# define TT_WOTHER 0x10 <nl> +# define TT_WOTHER ( 1 << 5 ) <nl> # define TT_TRAP 0x100 <nl> # endif <nl> 
void HELPER ( mtspr )( CPUOpenRISCState * env , target_ulong spr , target_ulong rb ) <nl> } <nl> break ; <nl> case TO_SPR ( 9 , 0 ): /* PICMR */ <nl> - env -> picmr |= rb ; <nl> + env -> picmr = rb ; <nl> break ; <nl> case TO_SPR ( 9 , 2 ): /* PICSR */ <nl> env -> picsr &= ~ rb ;
int mips_cpu_gdb_write_register ( CPUState * cs , uint8_t * mem_buf , int n ) <nl> return sizeof ( target_ulong ); <nl> } <nl> if ( env -> CP0_Config1 & ( 1 << CP0C1_FP ) <nl> - && n >= 38 && n < 73 ) { <nl> + && n >= 38 && n < 72 ) { <nl> if ( n < 70 ) { <nl> if ( env -> CP0_Status & ( 1 << CP0St_FR )) { <nl> env -> active_fpu . fpr [ n - 38 ]. d = tmp ;
int qemu_acl_insert ( qemu_acl * acl , <nl>  <nl> if ( index <= 0 ) <nl> return - 1 ; <nl> - if ( index >= acl -> nentries ) <nl> + if ( index > acl -> nentries ) { <nl> return qemu_acl_append ( acl , deny , match ); <nl> - <nl> + } <nl>  <nl> entry = g_malloc ( sizeof (* entry )); <nl> entry -> match = g_strdup ( match );
vmxnet3_indicate_packet ( VMXNET3State * s ) <nl> struct Vmxnet3_RxDesc rxd ; <nl> bool is_head = true ; <nl> uint32_t rxd_idx ; <nl> - uint32_t rx_ridx ; <nl> + uint32_t rx_ridx = 0 ; <nl>  <nl> struct Vmxnet3_RxCompDesc rxcd ; <nl> uint32_t new_rxcd_gen = VMXNET3_INIT_GEN ;
void qpci_device_foreach ( QPCIBus * bus , int vendor_id , int device_id , <nl>  <nl> if ( vendor_id != - 1 && <nl> qpci_config_readw ( dev , PCI_VENDOR_ID ) != vendor_id ) { <nl> + g_free ( dev ); <nl> continue ; <nl> } <nl>  <nl> if ( device_id != - 1 && <nl> qpci_config_readw ( dev , PCI_DEVICE_ID ) != device_id ) { <nl> + g_free ( dev ); <nl> continue ; <nl> } <nl> 
fail : <nl> QDECREF ( bs -> options ); <nl> QDECREF ( options ); <nl> bs -> options = NULL ; <nl> + bs -> explicit_options = NULL ; <nl> bdrv_unref ( bs ); <nl> error_propagate ( errp , local_err ); <nl> return NULL ; <nl> static void bdrv_close ( BlockDriverState * bs ) <nl> QDECREF ( bs -> options ); <nl> QDECREF ( bs -> explicit_options ); <nl> bs -> options = NULL ; <nl> + bs -> explicit_options = NULL ; <nl> QDECREF ( bs -> full_open_options ); <nl> bs -> full_open_options = NULL ; <nl> }
static int megasas_ctrl_get_info ( MegasasState * s , MegasasCmd * cmd ) <nl> BusChild * kid ; <nl> int num_pd_disks = 0 ; <nl>  <nl> - memset (& info , 0x0 , cmd -> iov_size ); <nl> + memset (& info , 0x0 , dcmd_size ); <nl> if ( cmd -> iov_size < dcmd_size ) { <nl> trace_megasas_dcmd_invalid_xfer_len ( cmd -> index , cmd -> iov_size , <nl> dcmd_size );
void ppc_tb_set_jmp_target ( unsigned long jmp_addr , unsigned long addr ); <nl> static inline void tb_set_jmp_target1 ( uintptr_t jmp_addr , uintptr_t addr ) <nl> { <nl> /* patch the branch destination */ <nl> - *( uint32_t *) jmp_addr = addr - ( jmp_addr + 4 ); <nl> + stl_p (( void *) jmp_addr , addr - ( jmp_addr + 4 )); <nl> /* no need to flush icache explicitly */ <nl> } <nl> # elif defined ( __aarch64__ )
void bdrv_detach_dev ( BlockDriverState * bs , void * dev ) <nl> bs -> dev = NULL ; <nl> bs -> dev_ops = NULL ; <nl> bs -> dev_opaque = NULL ; <nl> + bs -> buffer_alignment = 512 ; <nl> } <nl>  <nl> /* TODO change to return DeviceState * when all users are qdevified */
static int mmu_translate_region ( CPUS390XState * env , target_ulong vaddr , <nl> __func__ , origin , offs , new_entry ); <nl>  <nl> if (( new_entry & _REGION_ENTRY_INV ) != 0 ) { <nl> - /* XXX different regions have different faults */ <nl> DPRINTF ("% s : invalid region \ n ", __func__ ); <nl> - trigger_page_fault ( env , vaddr , PGM_SEGMENT_TRANS , asc , rw , exc ); <nl> + trigger_page_fault ( env , vaddr , pchks [ level / 4 ], asc , rw , exc ); <nl> return - 1 ; <nl> } <nl> 
static int get_S2prot ( CPUARMState * env , int s2ap , int xn ) <nl> prot |= PAGE_WRITE ; <nl> } <nl> if (! xn ) { <nl> - prot |= PAGE_EXEC ; <nl> + if ( arm_el_is_aa64 ( env , 2 ) || prot & PAGE_READ ) { <nl> + prot |= PAGE_EXEC ; <nl> + } <nl> } <nl> return prot ; <nl> }
static int usbnet_can_receive ( NetClientState * nc ) <nl> { <nl> USBNetState * s = qemu_get_nic_opaque ( nc ); <nl>  <nl> + if (! s -> dev . config ) { <nl> + return 0 ; <nl> + } <nl> + <nl> if ( is_rndis ( s ) && s -> rndis_state != RNDIS_DATA_INITIALIZED ) { <nl> return 1 ; <nl> }
void qmp_blockdev_change_medium ( const char * device , const char * filename , <nl> } <nl>  <nl> bdrv_flags = blk_get_open_flags_from_root_state ( blk ); <nl> + bdrv_flags &= ~( BDRV_O_TEMPORARY | BDRV_O_SNAPSHOT | BDRV_O_NO_BACKING | <nl> + BDRV_O_PROTOCOL ); <nl>  <nl> if (! has_read_only ) { <nl> read_only = BLOCKDEV_CHANGE_READ_ONLY_MODE_RETAIN ;
void vhost_dev_cleanup ( struct vhost_dev * hdev ) <nl> g_free ( hdev -> mem ); <nl> g_free ( hdev -> mem_sections ); <nl> hdev -> vhost_ops -> vhost_backend_cleanup ( hdev ); <nl> + assert (! hdev -> log ); <nl> QLIST_REMOVE ( hdev , entry ); <nl> } <nl> 
static int proxy_init ( FsContext * ctx ) <nl> sock_id = atoi ( ctx -> fs_root ); <nl> if ( sock_id < 0 ) { <nl> fprintf ( stderr , " socket descriptor not initialized \ n "); <nl> + g_free ( proxy ); <nl> return - 1 ; <nl> } <nl> } <nl> g_free ( ctx -> fs_root ); <nl> + ctx -> fs_root = NULL ; <nl>  <nl> proxy -> in_iovec . iov_base = g_malloc ( PROXY_MAX_IO_SZ + PROXY_HDR_SZ ); <nl> proxy -> in_iovec . iov_len = PROXY_MAX_IO_SZ + PROXY_HDR_SZ ;
X86RegisterInfo32 x86_reg_info_32 [ CPU_NB_REGS32 ] = { <nl>  <nl> const char * get_register_name_32 ( unsigned int reg ) <nl> { <nl> - if ( reg > CPU_NB_REGS32 ) { <nl> + if ( reg >= CPU_NB_REGS32 ) { <nl> return NULL ; <nl> } <nl> return x86_reg_info_32 [ reg ]. name ;
static void gen_add_A0_ds_seg ( DisasContext * s ) <nl> if ( s -> override >= 0 ) { <nl> override = s -> override ; <nl> must_add_seg = 1 ; <nl> - } else { <nl> - override = R_DS ; <nl> } <nl> if ( must_add_seg ) { <nl> # ifdef TARGET_X86_64
static int ide_dev_initfn ( IDEDevice * dev , IDEDriveKind kind ) <nl> return - 1 ; <nl> } <nl>  <nl> + if ( dev -> conf . logical_block_size != 512 ) { <nl> + error_report (" logical_block_size must be 512 for IDE "); <nl> + return - 1 ; <nl> + } <nl> + <nl> blkconf_serial (& dev -> conf , & dev -> serial ); <nl> if ( kind != IDE_CD ) { <nl> blkconf_geometry (& dev -> conf , & dev -> chs_trans , 65536 , 16 , 255 , & err );
static void vnc_dpy_copy ( DisplayChangeListener * dcl , <nl> } <nl> } <nl>  <nl> + if (! vd -> server ) { <nl> + /* no client connected */ <nl> + return ; <nl> + } <nl> /* do bitblit op on the local surface too */ <nl> pitch = vnc_server_fb_stride ( vd ); <nl> src_row = vnc_server_fb_ptr ( vd , src_x , src_y );
static int slirp_smb ( SlirpState * s , const char * exported_dir , <nl> " state directory =% s \ n " <nl> " log file =% s / log . smbd \ n " <nl> " smb passwd file =% s / smbpasswd \ n " <nl> - " security = share \ n " <nl> + " security = user \ n " <nl> + " map to guest = Bad User \ n " <nl> "[ qemu ]\ n " <nl> " path =% s \ n " <nl> " read only = no \ n "
static void tcg_target_qemu_prologue ( TCGContext * s ) <nl> } <nl>  <nl> /* Call generated code */ <nl> - tcg_out_opc_reg ( s , OPC_JR , 0 , tcg_target_call_iarg_regs [ 1 ]), 0 ); <nl> + tcg_out_opc_reg ( s , OPC_JR , 0 , tcg_target_call_iarg_regs [ 1 ], 0 ); <nl> tcg_out_mov ( s , TCG_TYPE_PTR , TCG_AREG0 , tcg_target_call_iarg_regs [ 0 ]); <nl> tb_ret_addr = s -> code_ptr ; <nl> 
const VMStateDescription vmstate_ahci = { <nl> VMSTATE_UINT32 ( control_regs . impl , AHCIState ), <nl> VMSTATE_UINT32 ( control_regs . version , AHCIState ), <nl> VMSTATE_UINT32 ( idp_index , AHCIState ), <nl> - VMSTATE_INT32 ( ports , AHCIState ), <nl> + VMSTATE_INT32_EQUAL ( ports , AHCIState ), <nl> VMSTATE_END_OF_LIST () <nl> }, <nl> };
void cpu_x86_load_seg ( CPUX86State * s , int seg_reg , int selector ) <nl> cpu_x86_load_seg_cache ( env , seg_reg , selector , <nl> ( selector << 4 ), 0xffff , 0 ); <nl> } else { <nl> - load_seg ( seg_reg , selector ); <nl> + helper_load_seg ( seg_reg , selector ); <nl> } <nl> env = saved_env ; <nl> }
build_ssdt ( GArray * table_data , GArray * linker , <nl>  <nl> patch_pci_windows ( pci , ssdt_ptr , sizeof ( ssdp_misc_aml )); <nl>  <nl> - *( uint16_t *)( ssdt_ptr + * ssdt_isa_pest ) = <nl> - cpu_to_le16 ( misc -> pvpanic_port ); <nl> + ACPI_BUILD_SET_LE ( ssdt_ptr , sizeof ( ssdp_misc_aml ), <nl> + ssdt_isa_pest [ 0 ], 16 , misc -> pvpanic_port ); <nl>  <nl> { <nl> GArray * sb_scope = build_alloc_array ();
static int xen_pt_bar_reg_read ( XenPCIPassthroughState * s , XenPTReg * cfg_entry , <nl>  <nl> /* get BAR index */ <nl> index = xen_pt_bar_offset_to_index ( reg -> offset ); <nl> - if ( index < 0 || index >= PCI_NUM_REGIONS ) { <nl> + if ( index < 0 || index >= PCI_NUM_REGIONS - 1 ) { <nl> XEN_PT_ERR (& s -> dev , " Internal error : Invalid BAR index [% d ].\ n ", index ); <nl> return - 1 ; <nl> }
void usb_ehci_realize ( EHCIState * s , DeviceState * dev , Error ** errp ) <nl> NB_PORTS ); <nl> return ; <nl> } <nl> + if ( s -> maxframes < 8 || s -> maxframes > 512 ) { <nl> + error_setg ( errp , " maxframes % d out if range ( 8 .. 512 )", <nl> + s -> maxframes ); <nl> + return ; <nl> + } <nl>  <nl> usb_bus_new (& s -> bus , sizeof ( s -> bus ), s -> companion_enable ? <nl> & ehci_bus_ops_companion : & ehci_bus_ops_standalone , dev );
static int tpm_passthrough_unix_write ( int fd , const uint8_t * buf , uint32_t len ) <nl> int ret , remain ; <nl>  <nl> remain = len ; <nl> - while ( len > 0 ) { <nl> + while ( remain > 0 ) { <nl> ret = write ( fd , buf , remain ); <nl> if ( ret < 0 ) { <nl> if ( errno != EINTR && errno != EAGAIN ) {
static void ps2_reset_keyboard ( PS2KbdState * s ) <nl> trace_ps2_reset_keyboard ( s ); <nl> s -> scan_enabled = 1 ; <nl> s -> scancode_set = 2 ; <nl> + ps2_reset_queue (& s -> common ); <nl> ps2_set_ledstate ( s , 0 ); <nl> } <nl> 
static void breakpoint_invalidate ( CPUState * cpu , target_ulong pc ) <nl> # else <nl> static void breakpoint_invalidate ( CPUState * cpu , target_ulong pc ) <nl> { <nl> - tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( cpu , pc ) | <nl> - ( pc & ~ TARGET_PAGE_MASK )); <nl> + hwaddr phys = cpu_get_phys_page_debug ( cpu , pc ); <nl> + if ( phys != - 1 ) { <nl> + tb_invalidate_phys_addr ( phys | ( pc & ~ TARGET_PAGE_MASK )); <nl> + } <nl> } <nl> # endif <nl> # endif /* TARGET_HAS_ICE */
static ExitStatus op_ex ( DisasContext * s , DisasOps * o ) <nl> TCGv_i64 tmp ; <nl>  <nl> update_psw_addr ( s ); <nl> - update_cc_op ( s ); <nl> + gen_op_calc_cc ( s ); <nl>  <nl> tmp = tcg_const_i64 ( s -> next_pc ); <nl> gen_helper_ex ( cc_op , cpu_env , cc_op , o -> in1 , o -> in2 , tmp ); <nl> tcg_temp_free_i64 ( tmp ); <nl>  <nl> - set_cc_static ( s ); <nl> return NO_EXIT ; <nl> } <nl> 
static int blk_send_response_one ( struct ioreq * ioreq ) <nl> break ; <nl> default : <nl> dst = NULL ; <nl> + return 0 ; <nl> } <nl> memcpy ( dst , & resp , sizeof ( resp )); <nl> blkdev -> rings . common . rsp_prod_pvt ++;
void ahci_realize ( AHCIState * s , DeviceState * qdev , AddressSpace * as , int ports ) <nl> ad -> port . dma -> ops = & ahci_dma_ops ; <nl> ide_register_restart_cb (& ad -> port ); <nl> } <nl> + g_free ( irqs ); <nl> } <nl>  <nl> void ahci_uninit ( AHCIState * s )
void pcie_aer_root_init ( PCIDevice * dev ) <nl> PCI_ERR_ROOT_CMD_EN_MASK ); <nl> pci_set_long ( dev -> w1cmask + pos + PCI_ERR_ROOT_STATUS , <nl> PCI_ERR_ROOT_STATUS_REPORT_MASK ); <nl> + /* PCI_ERR_ROOT_IRQ is RO but devices change it using a <nl> + * device - specific method . <nl> + */ <nl> + pci_set_long ( dev -> cmask + pos + PCI_ERR_ROOT_STATUS , <nl> + ~ PCI_ERR_ROOT_IRQ ); <nl> } <nl>  <nl> void pcie_aer_root_reset ( PCIDevice * dev )
static void gen_load_fp ( DisasContext * s , int opsize , TCGv addr , TCGv_ptr fp ) <nl> case OS_DOUBLE : <nl> tcg_gen_qemu_ld64 ( t64 , addr , index ); <nl> gen_helper_extf64 ( cpu_env , fp , t64 ); <nl> - tcg_temp_free_i64 ( t64 ); <nl> break ; <nl> case OS_EXTENDED : <nl> if ( m68k_feature ( s -> env , M68K_FEATURE_CF_FPU )) {
static void ioapic_class_init ( ObjectClass * klass , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( klass ); <nl>  <nl> k -> realize = ioapic_realize ; <nl> + /* <nl> + * If APIC is in kernel , we need to update the kernel cache after <nl> + * migration , otherwise first 24 gsi routes will be invalid . <nl> + */ <nl> + k -> post_load = ioapic_update_kvm_routes ; <nl> dc -> reset = ioapic_reset_common ; <nl> dc -> props = ioapic_properties ; <nl> }
void test_clone ( void ) <nl> CLONE_VM | CLONE_FS | CLONE_FILES | SIGCHLD , " hello2 ")); <nl>  <nl> while ( waitpid ( pid1 , & status1 , 0 ) != pid1 ); <nl> + free ( stack1 ); <nl> while ( waitpid ( pid2 , & status2 , 0 ) != pid2 ); <nl> + free ( stack2 ); <nl> if ( thread1_res != 5 || <nl> thread2_res != 6 ) <nl> error (" clone ");
int64_t qcow2_alloc_bytes ( BlockDriverState * bs , int size ) <nl> return new_cluster ; <nl> } <nl>  <nl> + if ( new_cluster == 0 ) { <nl> + qcow2_signal_corruption ( bs , true , - 1 , - 1 , " Preventing invalid " <nl> + " allocation of compressed cluster " <nl> + " at offset 0 "); <nl> + return - EIO ; <nl> + } <nl> + <nl> if (! offset || ROUND_UP ( offset , s -> cluster_size ) != new_cluster ) { <nl> offset = new_cluster ; <nl> free_in_cluster = s -> cluster_size ;
static void zynq_xadc_write ( void * opaque , hwaddr offset , uint64_t val , <nl> break ; <nl> } <nl>  <nl> - if ( xadc_reg > ZYNQ_XADC_NUM_ADC_REGS && xadc_cmd != CMD_NOP ) { <nl> + if ( xadc_reg >= ZYNQ_XADC_NUM_ADC_REGS && xadc_cmd != CMD_NOP ) { <nl> qemu_log_mask ( LOG_GUEST_ERROR , " read / write op to invalid xadc " <nl> " reg 0x % x \ n ", xadc_reg ); <nl> break ;
static inline int kvmppc_remove_spapr_tce ( void * table , int pfd , <nl>  <nl> static inline int kvmppc_reset_htab ( int shift_hint ) <nl> { <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl>  <nl> static inline uint64_t kvmppc_rma_size ( uint64_t current_size ,
static inline void gen_bcond ( DisasContext * ctx , int type ) <nl> gen_update_nip ( ctx , ctx -> nip ); <nl> tcg_gen_exit_tb ( 0 ); <nl> } <nl> - if ( type == BCOND_LR || type == BCOND_CTR ) { <nl> + if ( type == BCOND_LR || type == BCOND_CTR || type == BCOND_TAR ) { <nl> tcg_temp_free ( target ); <nl> } <nl> }
DeviceState * qdev_device_add ( QemuOpts * opts ) <nl> return NULL ; <nl> } <nl>  <nl> + if ( object_class_is_abstract ( oc )) { <nl> + qerror_report ( QERR_INVALID_PARAMETER_VALUE , " driver ", <nl> + " non - abstract device type "); <nl> + return NULL ; <nl> + } <nl> + <nl> dc = DEVICE_CLASS ( oc ); <nl>  <nl> /* find bus */
int qdev_device_help ( QemuOpts * opts ) <nl> } <nl> } <nl>  <nl> - if (! klass ) { <nl> + if (! object_class_dynamic_cast ( klass , TYPE_DEVICE )) { <nl> return 0 ; <nl> } <nl> do {
static int aio_write_f ( int argc , char ** argv ) <nl> case ' P ': <nl> pattern = parse_pattern ( optarg ); <nl> if ( pattern < 0 ) { <nl> + free ( ctx ); <nl> return 0 ; <nl> } <nl> break ;
static inline int vfp_exceptbits_from_host ( int host_bits ) <nl> target_bits |= 2 ; <nl> if ( host_bits & float_flag_overflow ) <nl> target_bits |= 4 ; <nl> - if ( host_bits & float_flag_underflow ) <nl> + if ( host_bits & ( float_flag_underflow | float_flag_output_denormal )) <nl> target_bits |= 8 ; <nl> if ( host_bits & float_flag_inexact ) <nl> target_bits |= 0x10 ;
int qcow2_pre_write_overlap_check ( BlockDriverState * bs , int ign , int64_t offset , <nl> offset , <nl> true , <nl> size , <nl> + true , <nl> & error_abort ); <nl> g_free ( message ); <nl> 
again : <nl> fail : <nl> qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) & l2_table ); <nl> fail_put : <nl> - if ( nb_clusters > 0 ) { <nl> + if ( m -> nb_clusters > 0 ) { <nl> QLIST_REMOVE ( m , next_in_flight ); <nl> } <nl> return ret ;
static int usb_msd_initfn_storage ( USBDevice * dev ) <nl> s -> conf . bootindex , dev -> serial , <nl> & err ); <nl> if (! scsi_dev ) { <nl> + error_report ("% s ", error_get_pretty ( err )); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> s -> bus . qbus . allow_hotplug = 0 ;
int qcow2_get_cluster_offset ( BlockDriverState * bs , uint64_t offset , <nl> break ; <nl> case QCOW2_CLUSTER_ZERO : <nl> if ( s -> qcow_version < 3 ) { <nl> + qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) & l2_table ); <nl> return - EIO ; <nl> } <nl> c = count_contiguous_clusters ( nb_clusters , s -> cluster_size ,
void virtio_reset ( void * opaque ) <nl> vdev -> vq [ i ]. signalled_used_valid = false ; <nl> vdev -> vq [ i ]. notification = true ; <nl> vdev -> vq [ i ]. vring . num = vdev -> vq [ i ]. vring . num_default ; <nl> + vdev -> vq [ i ]. inuse = 0 ; <nl> } <nl> } <nl> 
void s390_machine_reset ( void ) <nl> { <nl> S390CPU * ipl_cpu = S390_CPU ( qemu_get_cpu ( 0 )); <nl>  <nl> - qemu_devices_reset (); <nl> s390_cmma_reset (); <nl> + qemu_devices_reset (); <nl> s390_crypto_reset (); <nl>  <nl> /* all cpus are stopped - configure and start the ipl cpu only */
static void dhcp_decode ( const struct bootp_t * bp , int * pmsg_type , <nl> if ( p >= p_end ) <nl> break ; <nl> len = * p ++; <nl> + if ( p + len > p_end ) { <nl> + break ; <nl> + } <nl> DPRINTF (" dhcp : tag =% d len =% d \ n ", tag , len ); <nl>  <nl> switch ( tag ) {
static uint32_t cc_calc_abs_64 ( int64_t dst ) <nl> if (( uint64_t ) dst == 0x8000000000000000ULL ) { <nl> return 3 ; <nl> } else if ( dst ) { <nl> - return 1 ; <nl> + return 2 ; <nl> } else { <nl> return 0 ; <nl> } <nl> static uint32_t cc_calc_abs_32 ( int32_t dst ) <nl> if (( uint32_t ) dst == 0x80000000UL ) { <nl> return 3 ; <nl> } else if ( dst ) { <nl> - return 1 ; <nl> + return 2 ; <nl> } else { <nl> return 0 ; <nl> }
static void gd_menu_grab_input ( GtkMenuItem * item , void * opaque ) <nl> VirtualConsole * vc = gd_vc_find_current ( s ); <nl>  <nl> if ( gd_is_grab_active ( s )) { <nl> - gd_grab_keyboard ( vc ); <nl> + if (! gd_grab_on_hover ( s )) { <nl> + gd_grab_keyboard ( vc ); <nl> + } <nl> gd_grab_pointer ( vc ); <nl> } else { <nl> gd_ungrab_keyboard ( s );
static int tftp_session_allocate ( Slirp * slirp , struct sockaddr_storage * srcsas , <nl>  <nl> found : <nl> memset ( spt , 0 , sizeof (* spt )); <nl> - spt -> client_addr = * srcsas ; <nl> + memcpy (& spt -> client_addr , srcsas , sockaddr_size ( srcsas )); <nl> spt -> fd = - 1 ; <nl> spt -> block_size = 512 ; <nl> spt -> client_port = tp -> udp . uh_sport ;
static void x86_cpuid_set_model_id ( CPUX86State * env , const char * model_id ) <nl> model_id = ""; <nl> } <nl> len = strlen ( model_id ); <nl> + memset ( env -> cpuid_model , 0 , 48 ); <nl> for ( i = 0 ; i < 48 ; i ++) { <nl> if ( i >= len ) { <nl> c = '\ 0 ';
void s390_init_cpus ( MachineState * machine ) <nl> machine -> cpu_model = " host "; <nl> } <nl>  <nl> - cpu_states = g_malloc0 ( sizeof ( S390CPU *) * max_cpus ); <nl> + cpu_states = g_new0 ( S390CPU *, max_cpus ); <nl>  <nl> for ( i = 0 ; i < max_cpus ; i ++) { <nl> name = g_strdup_printf (" cpu [% i ]", i );
static inline bool cptype_valid ( int cptype ) <nl> */ <nl> static inline int arm_current_el ( CPUARMState * env ) <nl> { <nl> + if ( arm_feature ( env , ARM_FEATURE_M )) { <nl> + return !(( env -> v7m . exception == 0 ) && ( env -> v7m . control & 1 )); <nl> + } <nl> + <nl> if ( is_a64 ( env )) { <nl> return extract32 ( env -> pstate , 2 , 2 ); <nl> }
void tcg_add_target_add_op_defs ( const TCGTargetOpDef * tdefs ) <nl> if ( tdefs -> op == ( TCGOpcode )- 1 ) <nl> break ; <nl> op = tdefs -> op ; <nl> - assert ( op >= 0 && op < NB_OPS ); <nl> + assert (( unsigned ) op < NB_OPS ); <nl> def = & tcg_op_defs [ op ]; <nl> # if defined ( CONFIG_DEBUG_TCG ) <nl> /* Duplicate entry in op definitions ? */
static inline int test_and_change_bit ( int nr , volatile unsigned long * addr ) <nl> { <nl> unsigned long mask = BIT_MASK ( nr ); <nl> unsigned long * p = (( unsigned long *) addr ) + BIT_WORD ( nr ); <nl> - unsigned long old ; <nl> + unsigned long old = * p ; <nl>  <nl> * p = old ^ mask ; <nl> return ( old & mask ) != 0 ;
int qemu_devtree_add_subnode ( void * fdt , const char * name ) <nl> } <nl>  <nl> retval = fdt_add_subnode ( fdt , parent , basename ); <nl> +# if 0 <nl> if ( retval < 0 ) { <nl> fprintf ( stderr , " FDT : Failed to create subnode % s : % s \ n ", name , <nl> fdt_strerror ( retval )); <nl> exit ( 1 ); <nl> } <nl> +# endif <nl>  <nl> g_free ( dupname ); <nl> return retval ;
static void aspeed_soc_class_init ( ObjectClass * oc , void * data ) <nl>  <nl> sc -> info = ( AspeedSoCInfo *) data ; <nl> dc -> realize = aspeed_soc_realize ; <nl> + /* Reason : Uses serial_hds and nd_table in realize () directly */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo aspeed_soc_type_info = {
static void register_subpage ( MemoryRegionSection * section ) <nl> subpage = container_of ( existing -> mr , subpage_t , iomem ); <nl> } <nl> start = section -> offset_within_address_space & ~ TARGET_PAGE_MASK ; <nl> - end = start + section -> size ; <nl> + end = start + section -> size - 1 ; <nl> subpage_register ( subpage , start , end , phys_section_add ( section )); <nl> } <nl> 
static int qcow2_read_extensions ( BlockDriverState * bs , uint64_t start_offset , <nl> # ifdef DEBUG_EXT <nl> printf (" ext . magic = 0x % x \ n ", ext . magic ); <nl> # endif <nl> - if ( ext . len > end_offset - offset ) { <nl> + if ( offset > end_offset || ext . len > end_offset - offset ) { <nl> error_setg ( errp , " Header extension too large "); <nl> return - EINVAL ; <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> # endif <nl> # ifdef TARGET_NR_pause /* not on alpha */ <nl> case TARGET_NR_pause : <nl> - ret = get_errno ( pause ()); <nl> + if (! block_signals ()) { <nl> + sigsuspend (&(( TaskState *) cpu -> opaque )-> signal_mask ); <nl> + } <nl> + ret = - TARGET_EINTR ; <nl> break ; <nl> # endif <nl> # ifdef TARGET_NR_utime
static int64_t ivshmem_recv_msg ( IVShmemState * s , int * pfd , Error ** errp ) <nl> } while ( n < sizeof ( msg )); <nl>  <nl> * pfd = qemu_chr_fe_get_msgfd (& s -> server_chr ); <nl> - return msg ; <nl> + return le64_to_cpu ( msg ); <nl> } <nl>  <nl> static void ivshmem_recv_setup ( IVShmemState * s , Error ** errp )
static uint32_t get_cmd ( ESPState * s , uint8_t * buf , uint8_t buflen ) <nl> s -> dma_memory_read ( s -> dma_opaque , buf , dmalen ); <nl> } else { <nl> dmalen = s -> ti_size ; <nl> + if ( dmalen > TI_BUFSZ ) { <nl> + return 0 ; <nl> + } <nl> memcpy ( buf , s -> ti_buf , dmalen ); <nl> buf [ 0 ] = buf [ 2 ] >> 5 ; <nl> }
static void cpu_class_init ( ObjectClass * oc , void * data ) <nl> k -> get_receive_mask = receive_mask ; <nl> k -> read_event_data = read_event_data ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> + /* <nl> + * Reason : raise_irq_cpu_hotplug () depends on an unique <nl> + * TYPE_SCLP_CPU_HOTPLUG device , which is already created <nl> + * by the sclp event facility <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo sclp_cpu_info = {
void vhost_dev_stop ( struct vhost_dev * hdev , VirtIODevice * vdev ) <nl>  <nl> hdev -> started = false ; <nl> qemu_free ( hdev -> log ); <nl> + hdev -> log = NULL ; <nl> hdev -> log_size = 0 ; <nl> }
static void build_guest_fsinfo_for_real_device ( char const * syspath , <nl> break ; <nl> } <nl>  <nl> + g_free ( driver ); <nl> if ( sscanf ( p , "/% x :% x :% x .% x % n ", <nl> pci , pci + 1 , pci + 2 , pci + 3 , & pcilen ) == 4 ) { <nl> p += pcilen ;
static TCGArg * tcg_constant_folding ( TCGContext * s , uint16_t * tcg_opc_ptr , <nl> switch ( op ) { <nl> CASE_OP_32_64 ( or ): <nl> CASE_OP_32_64 ( and ): <nl> - if ( args [ 1 ] == args [ 2 ]) { <nl> + if ( temps_are_copies ( args [ 1 ], args [ 2 ])) { <nl> if ( temps_are_copies ( args [ 0 ], args [ 1 ])) { <nl> gen_opc_buf [ op_index ] = INDEX_op_nop ; <nl> } else {
static void qxl_destroy_primary ( PCIQXLDevice * d ) <nl> dprint ( d , 1 , "% s \ n ", __FUNCTION__ ); <nl>  <nl> d -> mode = QXL_MODE_UNDEFINED ; <nl> + qemu_mutex_unlock_iothread (); <nl> d -> ssd . worker -> destroy_primary_surface ( d -> ssd . worker , 0 ); <nl> + qemu_mutex_lock_iothread (); <nl> } <nl>  <nl> static void qxl_set_mode ( PCIQXLDevice * d , int modenr , int loadvm )
static inline void futex_wait ( QemuEvent * ev , unsigned val ) <nl> # else <nl> static inline void futex_wake ( QemuEvent * ev , int n ) <nl> { <nl> + pthread_mutex_lock (& ev -> lock ); <nl> if ( n == 1 ) { <nl> pthread_cond_signal (& ev -> cond ); <nl> } else { <nl> pthread_cond_broadcast (& ev -> cond ); <nl> } <nl> + pthread_mutex_unlock (& ev -> lock ); <nl> } <nl>  <nl> static inline void futex_wait ( QemuEvent * ev , unsigned val )
# define MAX_TOKEN_COUNT ( 2ULL << 20 ) <nl> # define MAX_NESTING ( 1ULL << 10 ) <nl>  <nl> + static void json_message_free_token ( void * token , void * opaque ) <nl> +{ <nl> + g_free ( token ); <nl> +} <nl> + <nl> static void json_message_free_tokens ( JSONMessageParser * parser ) <nl> { <nl> if ( parser -> tokens ) { <nl> + g_queue_foreach ( parser -> tokens , json_message_free_token , NULL ); <nl> g_queue_free ( parser -> tokens ); <nl> parser -> tokens = NULL ; <nl> }
int kvm_cpu_exec ( CPUState * cpu ) <nl> break ; <nl> case KVM_EXIT_MMIO : <nl> DPRINTF (" handle_mmio \ n "); <nl> - qemu_mutex_lock_iothread (); <nl> + /* Called outside BQL */ <nl> address_space_rw (& address_space_memory , <nl> run -> mmio . phys_addr , attrs , <nl> run -> mmio . data , <nl> run -> mmio . len , <nl> run -> mmio . is_write ); <nl> - qemu_mutex_unlock_iothread (); <nl> ret = 0 ; <nl> break ; <nl> case KVM_EXIT_IRQ_WINDOW_OPEN :
static int vhost_user_write ( struct vhost_dev * dev , VhostUserMsg * msg , <nl> return 0 ; <nl> } <nl>  <nl> - qemu_chr_fe_set_msgfds ( chr , fds , fd_num ); <nl> + if ( qemu_chr_fe_set_msgfds ( chr , fds , fd_num ) < 0 ) { <nl> + return - 1 ; <nl> + } <nl>  <nl> return qemu_chr_fe_write_all ( chr , ( const uint8_t *) msg , size ) == size ? <nl> 0 : - 1 ;
void stream_start ( const char * job_id , BlockDriverState * bs , <nl>  <nl> fail : <nl> if ( orig_bs_flags != bdrv_get_flags ( bs )) { <nl> - bdrv_reopen ( bs , s -> bs_flags , NULL ); <nl> + bdrv_reopen ( bs , orig_bs_flags , NULL ); <nl> } <nl> }
void virtqueue_discard ( VirtQueue * vq , const VirtQueueElement * elem , <nl> unsigned int len ) <nl> { <nl> vq -> last_avail_idx --; <nl> + vq -> inuse --; <nl> virtqueue_unmap_sg ( vq , elem , len ); <nl> } <nl> 
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static void scsi_cmd_xfer_mode ( SCSICommand * cmd ) <nl> case WRITE_LONG_10 : <nl> case WRITE_SAME_10 : <nl> case WRITE_SAME_16 : <nl> + case UNMAP : <nl> case SEARCH_HIGH_12 : <nl> case SEARCH_EQUAL_12 : <nl> case SEARCH_LOW_12 : <nl> static void scsi_cmd_xfer_mode ( SCSICommand * cmd ) <nl> case SEND_DVD_STRUCTURE : <nl> case PERSISTENT_RESERVE_OUT : <nl> case MAINTENANCE_OUT : <nl> + case ATA_PASSTHROUGH : <nl> cmd -> mode = SCSI_XFER_TO_DEV ; <nl> break ; <nl> default :
qio_channel_socket_accept ( QIOChannelSocket * ioc , <nl> cioc -> fd = qemu_accept ( ioc -> fd , ( struct sockaddr *)& cioc -> remoteAddr , <nl> & cioc -> remoteAddrLen ); <nl> if ( cioc -> fd < 0 ) { <nl> - trace_qio_channel_socket_accept_fail ( ioc ); <nl> if ( errno == EINTR ) { <nl> goto retry ; <nl> } <nl> + error_setg_errno ( errp , errno , " Unable to accept connection "); <nl> + trace_qio_channel_socket_accept_fail ( ioc ); <nl> goto error ; <nl> } <nl> 
struct ICSState { <nl>  <nl> static inline bool ics_valid_irq ( ICSState * ics , uint32_t nr ) <nl> { <nl> - return ( nr >= ics -> offset ) <nl> + return ( ics -> offset != 0 ) && ( nr >= ics -> offset ) <nl> && ( nr < ( ics -> offset + ics -> nr_irqs )); <nl> } <nl> 
static void machvirt_init ( MachineState * machine ) <nl> } <nl>  <nl> object_property_set_bool ( cpuobj , true , " realized ", NULL ); <nl> + object_unref ( cpuobj ); <nl> } <nl> fdt_add_timer_nodes ( vms ); <nl> fdt_add_cpu_nodes ( vms );
int spapr_ovec_populate_dt ( void * fdt , int fdt_offset , <nl> } <nl> } <nl>  <nl> - return fdt_setprop ( fdt , fdt_offset , name , vec , vec_len ); <nl> + return fdt_setprop ( fdt , fdt_offset , name , vec , vec_len + 1 ); <nl> }
static DriveInfo * blockdev_init ( QemuOpts * all_opts , <nl>  <nl> drv = bdrv_find_whitelisted_format ( buf , ro ); <nl> if (! drv ) { <nl> - error_report ("'% s ' invalid format ", buf ); <nl> + if (! ro && bdrv_find_whitelisted_format ( buf , ! ro )) { <nl> + error_report ("'% s ' can be only used as read - only device .", buf ); <nl> + } else { <nl> + error_report ("'% s ' invalid format ", buf ); <nl> + } <nl> return NULL ; <nl> } <nl> }
coroutine_fn iscsi_co_write_zeroes ( BlockDriverState * bs , int64_t sector_num , <nl> nb_blocks = sector_qemu2lun ( nb_sectors , iscsilun ); <nl>  <nl> if ( iscsilun -> zeroblock == NULL ) { <nl> - iscsilun -> zeroblock = g_malloc0 ( iscsilun -> block_size ); <nl> + iscsilun -> zeroblock = g_try_malloc0 ( iscsilun -> block_size ); <nl> + if ( iscsilun -> zeroblock == NULL ) { <nl> + return - ENOMEM ; <nl> + } <nl> } <nl>  <nl> iscsi_co_init_iscsitask ( iscsilun , & iTask );
static int bdrv_check_byte_request ( BlockDriverState * bs , int64_t offset , <nl> { <nl> int64_t len ; <nl>  <nl> + if ( size > INT_MAX ) { <nl> + return - EIO ; <nl> + } <nl> + <nl> if (! bdrv_is_inserted ( bs )) <nl> return - ENOMEDIUM ; <nl> 
static void prop_get_fdt ( Object * obj , Visitor * v , const char * name , <nl> void * fdt ; <nl>  <nl> if (! drc -> fdt ) { <nl> - visit_start_struct ( v , name , NULL , 0 , & err ); <nl> - if (! err ) { <nl> - visit_end_struct ( v , & err ); <nl> - } <nl> - error_propagate ( errp , err ); <nl> + visit_type_null ( v , NULL , errp ); <nl> return ; <nl> } <nl> 
static int bdrv_check_byte_request ( BlockDriverState * bs , int64_t offset , <nl> static int bdrv_check_request ( BlockDriverState * bs , int64_t sector_num , <nl> int nb_sectors ) <nl> { <nl> + if ( nb_sectors > INT_MAX / BDRV_SECTOR_SIZE ) { <nl> + return - EIO ; <nl> + } <nl> + <nl> return bdrv_check_byte_request ( bs , sector_num * BDRV_SECTOR_SIZE , <nl> nb_sectors * BDRV_SECTOR_SIZE ); <nl> }
DisplayState * init_displaystate ( void ) <nl> gchar * name ; <nl> int i ; <nl>  <nl> - if (! display_state ) { <nl> - display_state = g_new0 ( DisplayState , 1 ); <nl> - } <nl> - <nl> + get_alloc_displaystate (); <nl> for ( i = 0 ; i < nb_consoles ; i ++) { <nl> if ( consoles [ i ]-> console_type != GRAPHIC_CONSOLE && <nl> consoles [ i ]-> ds == NULL ) {
static FeatureWordInfo feature_word_info [ FEATURE_WORDS ] = { <nl> " ibpb ", NULL , NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> - NULL , " virt - ssbd ", NULL , NULL , <nl> + " amd - ssbd ", " virt - ssbd ", NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> }, <nl> . cpuid_eax = 0x80000008 ,
static void handle_qmp_command ( JSONMessageParser * parser , QList * tokens ) <nl> obj = qdict_get ( input , " arguments "); <nl> if (! obj ) { <nl> args = qdict_new (); <nl> + } else if ( qobject_type ( obj ) != QTYPE_QDICT ) { <nl> + qerror_report ( QERR_QMP_BAD_INPUT_OBJECT_MEMBER , " arguments ", " object "); <nl> + goto err_input ; <nl> } else { <nl> args = qobject_to_qdict ( obj ); <nl> QINCREF ( args );
static void ahci_reset_port ( AHCIState * s , int port ) <nl> ncq_tfs -> aiocb = NULL ; <nl> } <nl>  <nl> + /* Maybe we just finished the request thanks to bdrv_aio_cancel () */ <nl> + if (! ncq_tfs -> used ) { <nl> + continue ; <nl> + } <nl> + <nl> qemu_sglist_destroy (& ncq_tfs -> sglist ); <nl> ncq_tfs -> used = 0 ; <nl> }
static uint64_t pl011_read ( void * opaque , target_phys_addr_t offset , <nl> if ( s -> read_count == s -> read_trigger - 1 ) <nl> s -> int_level &= ~ PL011_INT_RX ; <nl> pl011_update ( s ); <nl> - qemu_chr_accept_input ( s -> chr ); <nl> + if ( s -> chr ) { <nl> + qemu_chr_accept_input ( s -> chr ); <nl> + } <nl> return c ; <nl> case 1 : /* UARTCR */ <nl> return 0 ;
static inline uint32_t efststeq ( CPUPPCState * env , uint32_t op1 , uint32_t op2 ) <nl> # define HELPER_SINGLE_SPE_CMP ( name ) \ <nl> uint32_t helper_e ## name ( CPUPPCState * env , uint32_t op1 , uint32_t op2 ) \ <nl> { \ <nl> - return e ## name ( env , op1 , op2 ) << 2 ; \ <nl> + return e ## name ( env , op1 , op2 ); \ <nl> } <nl> /* efststlt */ <nl> HELPER_SINGLE_SPE_CMP ( fststlt );
vpc_co_pwritev ( BlockDriverState * bs , uint64_t offset , uint64_t bytes , <nl> int64_t image_offset ; <nl> int64_t n_bytes ; <nl> int64_t bytes_done = 0 ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> VHDFooter * footer = ( VHDFooter *) s -> footer_buf ; <nl> QEMUIOVector local_qiov ; <nl> 
static void blkverify_err ( BlkverifyAIOCB * acb , const char * fmt , ...) <nl> va_list ap ; <nl>  <nl> va_start ( ap , fmt ); <nl> - fprintf ( stderr , " blkverify : % s sector_num =% ld nb_sectors =% d ", <nl> + fprintf ( stderr , " blkverify : % s sector_num =%" PRId64 " nb_sectors =% d ", <nl> acb -> is_write ? " write " : " read ", acb -> sector_num , <nl> acb -> nb_sectors ); <nl> vfprintf ( stderr , fmt , ap );
void helper_svm_check_intercept_param ( uint32_t type , uint64_t param ) <nl> switch (( uint32_t ) ECX ) { <nl> case 0 ... 0x1fff : <nl> t0 = ( ECX * 2 ) % 8 ; <nl> - t1 = ECX / 8 ; <nl> + t1 = ( ECX * 2 ) / 8 ; <nl> break ; <nl> case 0xc0000000 ... 0xc0001fff : <nl> t0 = ( 8192 + ECX - 0xc0000000 ) * 2 ;
static int blkverify_open ( BlockDriverState * bs , QDict * options , int flags , <nl>  <nl> ret = 0 ; <nl> fail : <nl> + if ( ret < 0 ) { <nl> + bdrv_unref_child ( bs , bs -> file ); <nl> + } <nl> qemu_opts_del ( opts ); <nl> return ret ; <nl> }
static SpiceChannelList * qmp_query_spice_channels ( void ) <nl> struct sockaddr * paddr ; <nl> socklen_t plen ; <nl>  <nl> - if (!( item -> info -> flags & SPICE_CHANNEL_EVENT_FLAG_ADDR_EXT )) { <nl> - error_report (" invalid channel event "); <nl> - return NULL ; <nl> - } <nl> + assert ( item -> info -> flags & SPICE_CHANNEL_EVENT_FLAG_ADDR_EXT ); <nl>  <nl> chan = g_malloc0 ( sizeof (* chan )); <nl> chan -> value = g_malloc0 ( sizeof (* chan -> value ));
static int kvmppc_read_host_property ( const char * node_path , const char * prop , <nl> { <nl> char * path ; <nl> FILE * f ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> int pathlen ; <nl>  <nl> pathlen = snprintf ( NULL , 0 , "% s /% s /% s ", PROC_DEVTREE_PATH , node_path , prop )
void unregister_displaychangelistener ( DisplayChangeListener * dcl ) <nl> dcl -> con -> dcls --; <nl> } <nl> QLIST_REMOVE ( dcl , next ); <nl> + dcl -> ds = NULL ; <nl> gui_setup_refresh ( ds ); <nl> } <nl> 
ssize_t pcnet_receive ( NetClientState * nc , const uint8_t * buf , size_t size_ ) <nl> int pktcount = 0 ; <nl>  <nl> if (! s -> looptest ) { <nl> + if ( size > 4092 ) { <nl> +# ifdef PCNET_DEBUG_RMD <nl> + fprintf ( stderr , " pcnet : truncates rx packet .\ n "); <nl> +# endif <nl> + size = 4092 ; <nl> + } <nl> memcpy ( src , buf , size ); <nl> /* no need to compute the CRC */ <nl> src [ size ] = 0 ;
static size_t qcrypto_hash_alg_size [ QCRYPTO_HASH_ALG__MAX ] = { <nl>  <nl> size_t qcrypto_hash_digest_len ( QCryptoHashAlgorithm alg ) <nl> { <nl> - if ( alg >= G_N_ELEMENTS ( qcrypto_hash_alg_size )) { <nl> - return 0 ; <nl> - } <nl> + assert ( alg < G_N_ELEMENTS ( qcrypto_hash_alg_size )); <nl> return qcrypto_hash_alg_size [ alg ]; <nl> } <nl> 
static void mirror_iteration_done ( MirrorOp * op , int ret ) <nl> bitmap_set ( s -> cow_bitmap , chunk_num , nb_chunks ); <nl> } <nl>  <nl> + qemu_iovec_destroy (& op -> qiov ); <nl> g_slice_free ( MirrorOp , op ); <nl> qemu_coroutine_enter ( s -> common . co , NULL ); <nl> }
void AcpiCpuHotplug_add ( ACPIGPE * gpe , AcpiCpuHotplug * g , CPUState * cpu ) <nl>  <nl> * gpe -> sts = * gpe -> sts | ACPI_CPU_HOTPLUG_STATUS ; <nl> cpu_id = k -> get_arch_id ( CPU ( cpu )); <nl> + g_assert (( cpu_id / 8 ) < ACPI_GPE_PROC_LEN ); <nl> g -> sts [ cpu_id / 8 ] |= ( 1 << ( cpu_id % 8 )); <nl> } <nl> 
void bdrv_drain_all ( void ) <nl> BlockDriverState * bs ; <nl>  <nl> while ( busy ) { <nl> - /* FIXME : We do not have timer support here , so this is effectively <nl> - * a busy wait . <nl> - */ <nl> QTAILQ_FOREACH ( bs , & bdrv_states , list ) { <nl> - if ( bdrv_start_throttled_reqs ( bs )) { <nl> - busy = true ; <nl> - } <nl> + bdrv_start_throttled_reqs ( bs ); <nl> } <nl>  <nl> busy = bdrv_requests_pending_all ();
static inline abi_long host_to_target_sockaddr ( abi_ulong target_addr , <nl> if ( len == 0 ) { <nl> return 0 ; <nl> } <nl> + assert ( addr ); <nl>  <nl> target_saddr = lock_user ( VERIFY_WRITE , target_addr , len , 0 ); <nl> if (! target_saddr )
static void bmdma_irq ( void * opaque , int n , int level ) <nl> return ; <nl> } <nl>  <nl> - if ( bm ) { <nl> - bm -> status |= BM_STATUS_INT ; <nl> - } <nl> + bm -> status |= BM_STATUS_INT ; <nl>  <nl> /* trigger the real irq */ <nl> qemu_set_irq ( bm -> irq , level );
static int coroutine_fn copy_sectors ( BlockDriverState * bs , <nl> BLKDBG_EVENT ( bs -> file , BLKDBG_COW_READ ); <nl>  <nl> if (! bs -> drv ) { <nl> - return - ENOMEDIUM ; <nl> + ret = - ENOMEDIUM ; <nl> + goto out ; <nl> } <nl>  <nl> /* Call . bdrv_co_readv () directly instead of using the public block - layer
static always_inline void gen_bcond ( DisasContext * ctx , int type ) <nl> # endif <nl> gen_op_btest_T1 ( ctx -> nip ); <nl> no_test : <nl> - if ( ctx -> singlestep_enabled & GDBSTUB_SINGLE_STEP ) { <nl> - gen_update_nip ( ctx , ctx -> nip ); <nl> - gen_op_debug (); <nl> - } <nl> tcg_gen_exit_tb ( 0 ); <nl> } <nl> }
static void json_message_process_token ( JSONLexer * lexer , QString * token , JSONTok <nl> parser -> bracket_count == 0 )) { <nl> goto out_emit ; <nl> } else if ( parser -> token_size > MAX_TOKEN_SIZE || <nl> - parser -> bracket_count > MAX_NESTING || <nl> - parser -> brace_count > MAX_NESTING ) { <nl> + parser -> bracket_count + parser -> brace_count > MAX_NESTING ) { <nl> /* Security consideration , we limit total memory allocated per object <nl> * and the maximum recursion depth that a message can force . <nl> */
void kvm_set_phys_mem ( target_phys_addr_t start_addr , <nl>  <nl> mem = kvm_lookup_slot ( s , start_addr ); <nl> if ( mem ) { <nl> - if ( flags == IO_MEM_UNASSIGNED ) { <nl> + if (( flags == IO_MEM_UNASSIGNED ) || ( flags >= TLB_MMIO )) { <nl> mem -> memory_size = 0 ; <nl> mem -> guest_phys_addr = start_addr ; <nl> mem -> userspace_addr = 0 ;
static void pci_info_device ( PCIBus * bus , PCIDevice * d ) <nl> base , limit ); <nl>  <nl> base = pci_bridge_get_base ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> - limit = pci_config_get_memory_base ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> + limit = pci_bridge_get_limit ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> monitor_printf ( mon , <nl> " memory range [ 0x % 08 " PRIx64 ", 0x % 08 " PRIx64 "]\ n ", <nl> base , limit );
int kvm_init ( int smp_cpus ) <nl> int ret ; <nl> int i ; <nl>  <nl> - if ( smp_cpus > 1 ) <nl> + if ( smp_cpus > 1 ) { <nl> + fprintf ( stderr , " No SMP KVM support , use '- smp 1 '\ n "); <nl> return - EINVAL ; <nl> + } <nl>  <nl> s = qemu_mallocz ( sizeof ( KVMState )); <nl> 
void hmp_info_block_jobs ( Monitor * mon , const QDict * qdict ) <nl> } <nl> list = list -> next ; <nl> } <nl> + <nl> + qapi_free_BlockJobInfoList ( list ); <nl> } <nl>  <nl> void hmp_info_tpm ( Monitor * mon , const QDict * qdict )
static void hid_keyboard_process_keycode ( HIDState * hs ) <nl> slot = hs -> head & QUEUE_MASK ; QUEUE_INCR ( hs -> head ); hs -> n --; <nl> keycode = hs -> kbd . keycodes [ slot ]; <nl>  <nl> + if (! hs -> n ) { <nl> + trace_hid_kbd_queue_empty (); <nl> + } <nl> + <nl> key = keycode & 0x7f ; <nl> index = key | (( hs -> kbd . modifiers & ( 1 << 8 )) >> 1 ); <nl> hid_code = hid_usage_keys [ index ];
static int vfio_populate_device ( VFIODevice * vbasedev ) <nl> return ret ; <nl> } <nl>  <nl> - vdev -> regions = g_malloc0_n ( vbasedev -> num_regions , <nl> - sizeof ( VFIORegion *)); <nl> + vdev -> regions = g_new0 ( VFIORegion *, vbasedev -> num_regions ); <nl>  <nl> for ( i = 0 ; i < vbasedev -> num_regions ; i ++) { <nl> struct vfio_region_info reg_info = { . argsz = sizeof ( reg_info ) };
static void vhost_region_del ( MemoryListener * listener , <nl> == section -> offset_within_address_space ) { <nl> -- dev -> n_mem_sections ; <nl> memmove (& dev -> mem_sections [ i ], & dev -> mem_sections [ i + 1 ], <nl> - dev -> n_mem_sections - i ); <nl> + ( dev -> n_mem_sections - i ) * sizeof (* dev -> mem_sections )); <nl> break ; <nl> } <nl> }
static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> } <nl> } <nl>  <nl> + if ( header . backing_file_offset > s -> cluster_size ) { <nl> + error_setg ( errp , " Invalid backing file offset "); <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> + } <nl> + <nl> if ( header . backing_file_offset ) { <nl> ext_end = header . backing_file_offset ; <nl> } else {
static int object_create ( QemuOpts * opts , void * opaque ) <nl>  <nl> obj = object_new ( type ); <nl> if ( qemu_opt_foreach ( opts , object_set_property , obj , 1 ) < 0 ) { <nl> + object_unref ( obj ); <nl> return - 1 ; <nl> } <nl>  <nl> object_property_add_child ( container_get ( object_get_root (), "/ objects "), <nl> id , obj , NULL ); <nl> - <nl> + object_unref ( obj ); <nl> return 0 ; <nl> } <nl> 
static void build_pci_bus_end ( PCIBus * bus , void * bus_state ) <nl> * to make acpi tables compatible with legacy machine types . <nl> */ <nl> if (! child -> pcihp_bridge_en && bus -> parent_dev ) { <nl> + build_free_array ( bus_table ); <nl> + build_pci_bus_state_cleanup ( child ); <nl> + g_free ( child ); <nl> return ; <nl> } <nl> 
static void spapr_dr_connector_class_init ( ObjectClass * k , void * data ) <nl> drck -> attach = attach ; <nl> drck -> detach = detach ; <nl> drck -> release_pending = release_pending ; <nl> + /* <nl> + * Reason : it crashes FIXME find and document the real reason <nl> + */ <nl> + dk -> cannot_instantiate_with_device_add_yet = true ; <nl> } <nl>  <nl> static const TypeInfo spapr_dr_connector_info = {
static void shpc_interrupt_update ( PCIDevice * d ) <nl> for ( slot = 0 ; slot < shpc -> nslots ; ++ slot ) { <nl> uint8_t event = shpc -> config [ SHPC_SLOT_EVENT_LATCH ( slot )]; <nl> uint8_t disable = shpc -> config [ SHPC_SLOT_EVENT_SERR_INT_DIS ( d , slot )]; <nl> - uint32_t mask = 1 << SHPC_IDX_TO_LOGICAL ( slot ); <nl> + uint32_t mask = 1U << SHPC_IDX_TO_LOGICAL ( slot ); <nl> if ( event & ~ disable ) { <nl> int_locator |= mask ; <nl> }
static void set_cfg_value ( bool is_max , int index , int value ) <nl> { <nl> if ( is_max ) { <nl> cfg . buckets [ index ]. max = value ; <nl> + /* If max is set , avg should never be 0 */ <nl> + cfg . buckets [ index ]. avg = MAX ( cfg . buckets [ index ]. avg , 1 ); <nl> } else { <nl> cfg . buckets [ index ]. avg = value ; <nl> }
udp_listen ( Slirp * slirp , uint32_t haddr , u_int hport , uint32_t laddr , <nl> return NULL ; <nl> } <nl> so -> s = qemu_socket ( AF_INET , SOCK_DGRAM , 0 ); <nl> + if ( so -> s < 0 ) { <nl> + sofree ( so ); <nl> + return NULL ; <nl> + } <nl> so -> so_expire = curtime + SO_EXPIRE ; <nl> insque ( so , & slirp -> udb ); <nl> 
static int get_real_id ( const char * devpath , const char * idname , uint16_t * val ) <nl> if ( fscanf ( f , "% li \ n ", & id ) == 1 ) { <nl> * val = id ; <nl> } else { <nl> + fclose ( f ); <nl> return - 1 ; <nl> } <nl> fclose ( f );
static inline int thunk_type_size ( const argtype * type_ptr , int is_host ) <nl> defined ( HOST_PARISC ) || defined ( HOST_SPARC64 ) <nl> return 4 ; <nl> # elif defined ( HOST_PPC ) <nl> - return TARGET_ABI_BITS / 8 ; <nl> + return sizeof ( void *); <nl> # else <nl> return 2 ; <nl> # endif
static void uart_write ( void * opaque , hwaddr offset , <nl>  <nl> DB_PRINT (" offset :% x data :% 08x \ n ", ( unsigned ) offset , ( unsigned ) value ); <nl> offset >>= 2 ; <nl> + if ( offset >= CADENCE_UART_R_MAX ) { <nl> + return ; <nl> + } <nl> switch ( offset ) { <nl> case R_IER : /* ier ( wts imr ) */ <nl> s -> r [ R_IMR ] |= value ;
static void test_interface_impl ( const char * type ) <nl>  <nl> g_assert ( iobj ); <nl> g_assert ( ioc -> test == PATTERN ); <nl> + object_unref ( obj ); <nl> } <nl>  <nl> static void interface_direct_test ( void )
static void acpi_get_pm_info ( AcpiPmInfo * pm ) <nl> Object * obj = NULL ; <nl> QObject * o ; <nl>  <nl> + pm -> cpu_hp_io_base = 0 ; <nl> pm -> pcihp_io_base = 0 ; <nl> pm -> pcihp_io_len = 0 ; <nl> if ( piix ) {
int net_init_vhost_user ( const NetClientOptions * opts , const char * name , <nl> } <nl>  <nl> queues = vhost_user_opts -> has_queues ? vhost_user_opts -> queues : 1 ; <nl> + if ( queues < 1 ) { <nl> + error_setg ( errp , <nl> + " vhost - user number of queues must be bigger than zero "); <nl> + return - 1 ; <nl> + } <nl>  <nl> return net_vhost_user_init ( peer , " vhost_user ", name , chr , queues ); <nl> }
static uint64_t arm_ldq_ptw ( CPUState * cs , hwaddr addr , bool is_secure , <nl> MemTxAttrs attrs = {}; <nl> MemTxResult result = MEMTX_OK ; <nl> AddressSpace * as ; <nl> - uint32_t data ; <nl> + uint64_t data ; <nl>  <nl> attrs . secure = is_secure ; <nl> as = arm_addressspace ( cs , attrs );
int main ( int argc , char ** argv ) <nl> qtest_add_func ("/ fdc / media_change ", test_media_change ); <nl> qtest_add_func ("/ fdc / sense_interrupt ", test_sense_interrupt ); <nl> qtest_add_func ("/ fdc / relative_seek ", test_relative_seek ); <nl> + qtest_add_func ("/ fdc / media_insert ", test_media_insert ); <nl> qtest_add_func ("/ fdc / fuzz - registers ", fuzz_registers ); <nl>  <nl> ret = g_test_run ();
static void vhost_iommu_region_add ( MemoryListener * listener , <nl> struct vhost_iommu * iommu ; <nl> Int128 end ; <nl> int iommu_idx ; <nl> - IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + IOMMUMemoryRegion * iommu_mr ; <nl>  <nl> if (! memory_region_is_iommu ( section -> mr )) { <nl> return ; <nl> } <nl>  <nl> + iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + <nl> iommu = g_malloc0 ( sizeof (* iommu )); <nl> end = int128_add ( int128_make64 ( section -> offset_within_region ), <nl> section -> size );
Error * error_copy ( const Error * err ) <nl> err_new = g_malloc0 ( sizeof (* err )); <nl> err_new -> msg = g_strdup ( err -> msg ); <nl> err_new -> err_class = err -> err_class ; <nl> + err_new -> src = err -> src ; <nl> + err_new -> line = err -> line ; <nl> + err_new -> func = err -> func ; <nl> if ( err -> hint ) { <nl> err_new -> hint = g_string_new ( err -> hint -> str ); <nl> }
static gboolean ga_channel_open ( GAChannel * c , const gchar * path , GAChannelMethod <nl> ret = ga_channel_client_add ( c , fd ); <nl> if ( ret ) { <nl> g_critical (" error adding channel to main loop "); <nl> + close ( fd ); <nl> return false ; <nl> } <nl> break ;
guint qemu_chr_fe_add_watch ( CharBackend * be , GIOCondition cond , <nl> } <nl>  <nl> g_source_set_callback ( src , ( GSourceFunc ) func , user_data , NULL ); <nl> - tag = g_source_attach ( src , NULL ); <nl> + tag = g_source_attach ( src , s -> gcontext ); <nl> g_source_unref ( src ); <nl>  <nl> return tag ;
void do_device_add ( Monitor * mon , const QDict * qdict ) <nl>  <nl> opts = qemu_opts_parse (& qemu_device_opts , <nl> qdict_get_str ( qdict , " config "), " driver "); <nl> - if ( opts && ! qdev_device_help ( opts )) <nl> - qdev_device_add ( opts ); <nl> + if ( opts ) { <nl> + if ( qdev_device_help ( opts ) || qdev_device_add ( opts ) == NULL ) { <nl> + qemu_opts_del ( opts ); <nl> + } <nl> + } <nl> } <nl>  <nl> void do_device_del ( Monitor * mon , const QDict * qdict )
static int xen_pt_bar_reg_write ( XenPCIPassthroughState * s , XenPTReg * cfg_entry , <nl> bar_ro_mask = XEN_PT_BAR_IO_RO_MASK | ( r_size - 1 ); <nl> break ; <nl> case XEN_PT_BAR_FLAG_UPPER : <nl> + assert ( index > 0 ); <nl> + r_size = d -> io_regions [ index - 1 ]. size >> 32 ; <nl> bar_emu_mask = XEN_PT_BAR_ALLF ; <nl> bar_ro_mask = r_size ? r_size - 1 : 0 ; <nl> break ;
static const QEMUFileOps rdma_write_ops = { <nl>  <nl> static void * qemu_fopen_rdma ( RDMAContext * rdma , const char * mode ) <nl> { <nl> - QEMUFileRDMA * r = g_malloc0 ( sizeof ( QEMUFileRDMA )); <nl> + QEMUFileRDMA * r ; <nl>  <nl> if ( qemu_file_mode_is_not_valid ( mode )) { <nl> return NULL ; <nl> } <nl>  <nl> + r = g_malloc0 ( sizeof ( QEMUFileRDMA )); <nl> r -> rdma = rdma ; <nl>  <nl> if ( mode [ 0 ] == ' w ') {
USBPacket * usb_ep_find_packet_by_id ( USBDevice * dev , int pid , int ep , <nl> struct USBEndpoint * uep = usb_ep_get ( dev , pid , ep ); <nl> USBPacket * p ; <nl>  <nl> - while (( p = QTAILQ_FIRST (& uep -> queue )) != NULL ) { <nl> + QTAILQ_FOREACH ( p , & uep -> queue , queue ) { <nl> if ( p -> id == id ) { <nl> return p ; <nl> }
static int img_convert ( int argc , char ** argv ) <nl>  <nl> if ( options && ! strcmp ( options , "?")) { <nl> print_option_help ( drv -> create_options ); <nl> + free ( bs ); <nl> return 0 ; <nl> } <nl> 
DeviceState * qdev_try_create ( BusState * bus , const char * name ) <nl> { <nl> DeviceState * dev ; <nl>  <nl> + if ( object_class_by_name ( name ) == NULL ) { <nl> + return NULL ; <nl> + } <nl> dev = DEVICE ( object_new ( name )); <nl> if (! dev ) { <nl> return NULL ;
static void multiwrite_cb ( void * opaque , int ret ) <nl> { <nl> MultiwriteCB * mcb = opaque ; <nl>  <nl> - if ( ret < 0 ) { <nl> + if ( ret < 0 && ! mcb -> error ) { <nl> mcb -> error = ret ; <nl> multiwrite_user_cb ( mcb ); <nl> }
static QTAILQ_HEAD (, Rom ) roms = QTAILQ_HEAD_INITIALIZER ( roms ); <nl>  <nl> static inline bool rom_order_compare ( Rom * rom , Rom * item ) <nl> { <nl> - return ( rom -> as > item -> as ) || <nl> + return (( uintptr_t )( void *) rom -> as > ( uintptr_t )( void *) item -> as ) || <nl> ( rom -> as == item -> as && rom -> addr >= item -> addr ); <nl> } <nl> 
static void vmdk_free_last_extent ( BlockDriverState * bs ) <nl> static uint32_t vmdk_read_cid ( BlockDriverState * bs , int parent ) <nl> { <nl> char desc [ DESC_SIZE ]; <nl> - uint32_t cid ; <nl> + uint32_t cid = 0 ; <nl> const char * p_name , * cid_str ; <nl> size_t cid_str_size ; <nl> BDRVVmdkState * s = bs -> opaque ;
static inline void gdb_continue ( GDBState * s ) <nl> # ifdef CONFIG_USER_ONLY <nl> s -> running_state = 1 ; <nl> # else <nl> - vm_start (); <nl> + if ( runstate_check ( RUN_STATE_DEBUG )) { <nl> + vm_start (); <nl> + } <nl> # endif <nl> } <nl> 
static void arm_cpu_reset ( CPUState * s ) <nl> */ <nl> env -> v7m . ccr = R_V7M_CCR_STKALIGN_MASK ; <nl>  <nl> + /* Unlike A / R profile , M profile defines the reset LR value */ <nl> + env -> regs [ 14 ] = 0xffffffff ; <nl> + <nl> /* Load the initial SP and PC from the vector table at address 0 */ <nl> rom = rom_ptr ( 0 ); <nl> if ( rom ) {
int main ( int argc , char ** argv , char ** envp ) <nl> /* init remote displays */ <nl> qemu_opts_foreach ( qemu_find_opts (" vnc "), vnc_init_func , NULL , 0 ); <nl> if ( show_vnc_port ) { <nl> - printf (" VNC server running on `% s '\ n ", <nl> - vnc_display_local_addr (" default ")); <nl> + char * ret = vnc_display_local_addr (" default "); <nl> + printf (" VNC server running on `% s '\ n ", ret ); <nl> + g_free ( ret ); <nl> } <nl> # endif <nl> # ifdef CONFIG_SPICE
size_t qlist_size ( const QList * qlist ) <nl> */ <nl> QList * qobject_to_qlist ( const QObject * obj ) <nl> { <nl> - if ( qobject_type ( obj ) != QTYPE_QLIST ) { <nl> + if (! obj || qobject_type ( obj ) != QTYPE_QLIST ) { <nl> return NULL ; <nl> } <nl> - <nl> return container_of ( obj , QList , base ); <nl> } <nl> 
static BlockBackend * img_open_opts ( const char * optstr , <nl> if ( qdict_haskey ( options , BDRV_OPT_FORCE_SHARE ) <nl> && ! qdict_get_bool ( options , BDRV_OPT_FORCE_SHARE )) { <nl> error_report ("-- force - share /- U conflicts with image options "); <nl> + QDECREF ( options ); <nl> return NULL ; <nl> } <nl> qdict_put ( options , BDRV_OPT_FORCE_SHARE , qbool_from_bool ( true ));
static uint64_t pci_host_data_read ( void * opaque , <nl> { <nl> PCIHostState * s = opaque ; <nl> uint32_t val ; <nl> - if (!( s -> config_reg & ( 1 << 31 ))) <nl> + if (!( s -> config_reg & ( 1U << 31 ))) { <nl> return 0xffffffff ; <nl> + } <nl> val = pci_data_read ( s -> bus , s -> config_reg | ( addr & 3 ), len ); <nl> PCI_DPRINTF (" read addr " TARGET_FMT_plx " len % d val % x \ n ", <nl> addr , len , val );
static int bdrv_open_common ( BlockDriverState * bs , BlockDriverState * file , <nl> ret = - EINVAL ; <nl> goto free_and_fail ; <nl> } <nl> - assert ( file != NULL ); <nl> bs -> file = file ; <nl> ret = drv -> bdrv_open ( bs , options , open_flags ); <nl> }
int kvm_arch_init_vcpu ( CPUState * cs ) <nl> cpuid_data . cpuid . nent = cpuid_i ; <nl>  <nl> if ((( env -> cpuid_version >> 8 )& 0xF ) >= 6 <nl> - && ( env -> cpuid_features &( CPUID_MCE | CPUID_MCA )) == ( CPUID_MCE | CPUID_MCA ) <nl> + && ( env -> cpuid_features & ( CPUID_MCE | CPUID_MCA )) == <nl> + ( CPUID_MCE | CPUID_MCA ) <nl> && kvm_check_extension ( cs -> kvm_state , KVM_CAP_MCE ) > 0 ) { <nl> uint64_t mcg_cap ; <nl> int banks ;
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl> return 0 ; <nl>  <nl> fail : <nl> + if ( snapshots_offset > 0 ) { <nl> + qcow2_free_clusters ( bs , snapshots_offset , snapshots_size , <nl> + QCOW2_DISCARD_ALWAYS ); <nl> + } <nl> return ret ; <nl> } <nl> 
static int oss_open ( int in , struct oss_params * req , <nl> goto err ; <nl> } <nl>  <nl> + if (! abinfo . fragstotal || ! abinfo . fragsize ) { <nl> + AUD_log ( AUDIO_CAP , " Returned bogus buffer information (% d , % d ) for % s \ n ", <nl> + abinfo . fragstotal , abinfo . fragsize , typ ); <nl> + goto err ; <nl> + } <nl> + <nl> obt -> fmt = fmt ; <nl> obt -> nchannels = nchannels ; <nl> obt -> freq = freq ;
static void spapr_finalize_fdt ( sPAPREnvironment * spapr , <nl>  <nl> cpu_physical_memory_write ( fdt_addr , fdt , fdt_totalsize ( fdt )); <nl>  <nl> + g_free ( bootlist ); <nl> g_free ( fdt ); <nl> } <nl> 
static void xenfb_handle_events ( struct XenFB * xenfb ) <nl>  <nl> prod = page -> out_prod ; <nl> out_cons = page -> out_cons ; <nl> - if ( prod - out_cons >= XENFB_OUT_RING_LEN ) { <nl> + if ( prod - out_cons > XENFB_OUT_RING_LEN ) { <nl> return ; <nl> } <nl> xen_rmb (); /* ensure we see ring contents up to prod */
DriveInfo * drive_init ( QemuOpts * opts , int default_to_scsi , int * fatal_error ) <nl> dinfo -> on_write_error = on_write_error ; <nl> dinfo -> opts = opts ; <nl> if ( serial ) <nl> - strncpy ( dinfo -> serial , serial , sizeof ( serial )); <nl> + strncpy ( dinfo -> serial , serial , sizeof ( dinfo -> serial ) - 1 ); <nl> QTAILQ_INSERT_TAIL (& drives , dinfo , next ); <nl>  <nl> switch ( type ) {
static void vmmouse_reset ( DeviceState * d ) <nl>  <nl> s -> status = 0xffff ; <nl> s -> queue_size = VMMOUSE_QUEUE_SIZE ; <nl> + <nl> + vmmouse_disable ( s ); <nl> } <nl>  <nl> static int vmmouse_initfn ( ISADevice * dev )
static int connect_namedsocket ( const char * path ) <nl> size = strlen ( helper . sun_path ) + sizeof ( helper . sun_family ); <nl> if ( connect ( sockfd , ( struct sockaddr *)& helper , size ) < 0 ) { <nl> fprintf ( stderr , " socket error \ n "); <nl> + close ( sockfd ); <nl> return - 1 ; <nl> } <nl> 
GEN_HANDLER ( dcbtst , 0x1F , 0x16 , 0x07 , 0x03E00001 , PPC_CACHE ) <nl> # define op_dcbz () (* gen_op_dcbz [ ctx -> mem_idx ])() <nl> static GenOpFunc * gen_op_dcbz [] = { <nl> & gen_op_dcbz_user , <nl> + & gen_op_dcbz_user , <nl> + & gen_op_dcbz_kernel , <nl> & gen_op_dcbz_kernel , <nl> }; <nl> # endif
static QString * read_line ( FILE * file , char * key ) <nl> { <nl> char value [ 128 ]; <nl>  <nl> - if ( fscanf ( file , "% s % s ", key , value ) == EOF ) <nl> + if ( fscanf ( file , "% 127s % 127s ", key , value ) == EOF ) { <nl> return NULL ; <nl> + } <nl> remove_dots ( key ); <nl> return qstring_from_str ( value ); <nl> }
static int send_sub_rect_nojpeg ( VncState * vs , int x , int y , int w , int h , <nl> ret = send_mono_rect ( vs , x , y , w , h , bg , fg ); <nl> } else if ( colors <= 256 ) { <nl> ret = send_palette_rect ( vs , x , y , w , h , palette ); <nl> + } else { <nl> + ret = 0 ; <nl> } <nl> return ret ; <nl> }
static int htab_save_iterate ( QEMUFile * f , void * opaque ) <nl> /* Iteration header */ <nl> if (! spapr -> htab_shift ) { <nl> qemu_put_be32 ( f , - 1 ); <nl> - return 0 ; <nl> + return 1 ; <nl> } else { <nl> qemu_put_be32 ( f , 0 ); <nl> }
static void tcx_realizefn ( DeviceState * dev , Error ** errp ) <nl> if ( fcode_filename ) { <nl> ret = load_image_targphys ( fcode_filename , s -> prom_addr , <nl> FCODE_MAX_ROM_SIZE ); <nl> + g_free ( fcode_filename ); <nl> if ( ret < 0 || ret > FCODE_MAX_ROM_SIZE ) { <nl> error_report (" tcx : could not load prom '% s '", TCX_ROM_FILE ); <nl> }
static void ppc_spapr_init ( QEMUMachineInitArgs * args ) <nl> NULL , & lowaddr , NULL , 0 , ELF_MACHINE , 0 ); <nl> kernel_le = kernel_size > 0 ; <nl> } <nl> - if ( kernel_size < 0 ) { <nl> - kernel_size = load_image_targphys ( kernel_filename , <nl> - KERNEL_LOAD_ADDR , <nl> - load_limit - KERNEL_LOAD_ADDR ); <nl> - } <nl> if ( kernel_size < 0 ) { <nl> fprintf ( stderr , " qemu : could not load kernel '% s '\ n ", <nl> kernel_filename );
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> s -> current_mapping -> path = buffer ; <nl> s -> current_mapping -> read_only = <nl> ( st . st_mode & ( S_IWUSR | S_IWGRP | S_IWOTH )) == 0 ; <nl> - } <nl> + } else { <nl> + g_free ( buffer ); <nl> + } <nl> } <nl> closedir ( dir ); <nl> 
static void vhost_dev_sync_region ( struct vhost_dev * dev , <nl> log = __sync_fetch_and_and ( from , 0 ); <nl> while (( bit = sizeof ( log ) > sizeof ( int ) ? <nl> ffsll ( log ) : ffs ( log ))) { <nl> + ram_addr_t ram_addr ; <nl> bit -= 1 ; <nl> - cpu_physical_memory_set_dirty ( addr + bit * VHOST_LOG_PAGE ); <nl> + ram_addr = cpu_get_physical_page_desc ( addr + bit * VHOST_LOG_PAGE ); <nl> + cpu_physical_memory_set_dirty ( ram_addr ); <nl> log &= ~( 0x1ull << bit ); <nl> } <nl> addr += VHOST_LOG_CHUNK ;
aio_read_f ( int argc , char ** argv ) <nl> case ' P ': <nl> ctx -> Pflag = 1 ; <nl> ctx -> pattern = parse_pattern ( optarg ); <nl> - if ( ctx -> pattern < 0 ) <nl> + if ( ctx -> pattern < 0 ) { <nl> + free ( ctx ); <nl> return 0 ; <nl> + } <nl> break ; <nl> case ' q ': <nl> ctx -> qflag = 1 ;
char * socket_address_to_string ( struct SocketAddress * addr , Error ** errp ) <nl>  <nl> SocketAddress * socket_address_flatten ( SocketAddressLegacy * addr_legacy ) <nl> { <nl> - SocketAddress * addr = g_new ( SocketAddress , 1 ); <nl> + SocketAddress * addr ; <nl>  <nl> if (! addr_legacy ) { <nl> return NULL ; <nl> } <nl>  <nl> + addr = g_new ( SocketAddress , 1 ); <nl> + <nl> switch ( addr_legacy -> type ) { <nl> case SOCKET_ADDRESS_LEGACY_KIND_INET : <nl> addr -> type = SOCKET_ADDRESS_TYPE_INET ;
static void tcg_reg_alloc_mov ( TCGContext * s , const TCGOpDef * def , <nl> } <nl> ots -> val_type = TEMP_VAL_CONST ; <nl> ots -> val = ts -> val ; <nl> + if ( IS_DEAD_ARG ( 1 )) { <nl> + temp_dead ( s , args [ 1 ]); <nl> + } <nl> } else { <nl> /* The code in the first if block should have moved the <nl> temp to a register . */
* THE SOFTWARE . <nl> */ <nl>  <nl> -# include " sysemu / sysemu . h " <nl> -# include " monitor / monitor . h " <nl> -# include " ui / console . h " <nl> - <nl> -# include " hw / hw . h " <nl> - <nl> +# include " qemu / main - loop . h " <nl> # include " qemu / timer . h " <nl> + <nl> # ifdef CONFIG_POSIX <nl> # include < pthread . h > <nl> # endif
static void x86_cpu_apic_create ( X86CPU * cpu , Error ** errp ) <nl>  <nl> object_property_add_child ( OBJECT ( cpu ), " lapic ", <nl> OBJECT ( cpu -> apic_state ), & error_abort ); <nl> + object_unref ( OBJECT ( cpu -> apic_state )); <nl>  <nl> qdev_prop_set_uint8 ( cpu -> apic_state , " id ", cpu -> apic_id ); <nl> /* TODO : convert to link <> */
void replay_add_event ( ReplayAsyncEventKind event_kind , <nl>  <nl> void replay_bh_schedule_event ( QEMUBH * bh ) <nl> { <nl> - if ( replay_mode != REPLAY_MODE_NONE ) { <nl> + if ( replay_mode != REPLAY_MODE_NONE && events_enabled ) { <nl> uint64_t id = replay_get_current_step (); <nl> replay_add_event ( REPLAY_ASYNC_EVENT_BH , bh , NULL , id ); <nl> } else {
void s390_cpu_do_interrupt ( CPUState * cs ) <nl> do_mchk_interrupt ( env ); <nl> break ; <nl> } <nl> + <nl> + /* WAIT PSW during interrupt injection */ <nl> + if ( cs -> exception_index == EXCP_HLT ) { <nl> + /* don ' t trigger a cpu_loop_exit (), use an interrupt instead */ <nl> + cpu_interrupt ( CPU ( cpu ), CPU_INTERRUPT_HALT ); <nl> + } <nl> cs -> exception_index = - 1 ; <nl>  <nl> /* we might still have pending interrupts , but not deliverable */
static void stm32f2xx_timer_write ( void * opaque , hwaddr offset , <nl> return ; <nl> case TIM_PSC : <nl> timer_val = stm32f2xx_ns_to_ticks ( s , now ) - s -> tick_offset ; <nl> - s -> tim_psc = value ; <nl> + s -> tim_psc = value & 0xFFFF ; <nl> value = timer_val ; <nl> break ; <nl> case TIM_CNT :
char * object_property_get_str ( Object * obj , const char * name , <nl> void object_property_set_link ( Object * obj , Object * value , <nl> const char * name , Error ** errp ) <nl> { <nl> - object_property_set_str ( obj , object_get_canonical_path ( value ), <nl> - name , errp ); <nl> + gchar * path = object_get_canonical_path ( value ); <nl> + object_property_set_str ( obj , path , name , errp ); <nl> + g_free ( path ); <nl> } <nl>  <nl> Object * object_property_get_link ( Object * obj , const char * name ,
static void spapr_phb_realize ( DeviceState * dev , Error ** errp ) <nl> } <nl> # endif <nl>  <nl> - memory_region_init_io (& sphb -> msiwindow , NULL , & spapr_msi_ops , spapr , <nl> + memory_region_init_io (& sphb -> msiwindow , OBJECT ( sphb ), & spapr_msi_ops , spapr , <nl> " msi ", msi_window_size ); <nl> memory_region_add_subregion (& sphb -> iommu_root , SPAPR_PCI_MSI_WINDOW , <nl> & sphb -> msiwindow );
int cpu_get_dump_info ( ArchDumpInfo * info , <nl> } else { <nl> info -> d_endian = ELFDATA2LSB ; <nl> } <nl> + /* 64KB is the max page size for pseries kernel */ <nl> + if ( strncmp ( object_get_typename ( qdev_get_machine ()), <nl> + " pseries -", 8 ) == 0 ) { <nl> + info -> page_size = ( 1U << 16 ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int find_and_clear_dirty_height ( VncState * vs , <nl> static int vnc_update_client ( VncState * vs , int has_dirty , bool sync ) <nl> { <nl> vs -> has_dirty += has_dirty ; <nl> - if ( vs -> need_update && vs -> ioc != NULL ) { <nl> + if ( vs -> need_update && ! vs -> disconnecting ) { <nl> VncDisplay * vd = vs -> vd ; <nl> VncJob * job ; <nl> int y ;
static void taihu_405ep_init ( ram_addr_t ram_size , <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> if ( filename ) { <nl> bios_size = load_image ( filename , qemu_get_ram_ptr ( bios_offset )); <nl> + qemu_free ( filename ); <nl> } else { <nl> bios_size = - 1 ; <nl> }
static void blk_delete ( BlockBackend * blk ) <nl> assert (! blk -> refcnt ); <nl> assert (! blk -> name ); <nl> assert (! blk -> dev ); <nl> + if ( blk -> public . throttle_state ) { <nl> + blk_io_limits_disable ( blk ); <nl> + } <nl> if ( blk -> root ) { <nl> blk_remove_bs ( blk ); <nl> }
static GIOStatus ga_channel_write ( GAChannel * c , const char * buf , size_t size , <nl> GIOStatus ga_channel_write_all ( GAChannel * c , const char * buf , size_t size ) <nl> { <nl> GIOStatus status = G_IO_STATUS_NORMAL ; <nl> - size_t count ; <nl> + size_t count = 0 ; <nl>  <nl> while ( size ) { <nl> status = ga_channel_write ( c , buf , size , & count );
static void tap_cleanup ( VLANClientState * nc ) <nl>  <nl> if ( s -> vhost_net ) { <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> + s -> vhost_net = NULL ; <nl> } <nl>  <nl> qemu_purge_queued_packets ( nc );
static int htab_load ( QEMUFile * f , void * opaque , int version_id ) <nl> section_hdr = qemu_get_be32 ( f ); <nl>  <nl> if ( section_hdr ) { <nl> - Error * local_err ; <nl> + Error * local_err = NULL ; <nl>  <nl> /* First section gives the htab size */ <nl> spapr_reallocate_hpt ( spapr , section_hdr , & local_err );
static bool run_poll_handlers_once ( AioContext * ctx ) <nl>  <nl> QLIST_FOREACH_RCU ( node , & ctx -> aio_handlers , node ) { <nl> if (! node -> deleted && node -> io_poll && <nl> - node -> io_poll ( node -> opaque )) { <nl> + aio_node_check ( ctx , node -> is_external ) && <nl> + node -> io_poll ( node -> opaque )) { <nl> progress = true ; <nl> } <nl> 
void qmp_drive_mirror ( const char * device , const char * target , <nl> return ; <nl> } <nl>  <nl> - if ( sync == MIRROR_SYNC_MODE_FULL && mode != NEW_IMAGE_MODE_EXISTING ) { <nl> + if (( sync == MIRROR_SYNC_MODE_FULL || ! source ) <nl> + && mode != NEW_IMAGE_MODE_EXISTING ) <nl> + { <nl> /* create new image w / o backing file */ <nl> assert ( format && drv ); <nl> bdrv_img_create ( target , format ,
tcp_listen ( Slirp * slirp , u_int32_t haddr , u_int hport , u_int32_t laddr , <nl> struct socket * so ; <nl> int s , opt = 1 ; <nl> socklen_t addrlen = sizeof ( addr ); <nl> + memset (& addr , 0 , addrlen ); <nl>  <nl> DEBUG_CALL (" tcp_listen "); <nl> DEBUG_ARG (" haddr = % x ", haddr );
reply_maybe_async : <nl> reply_async -> IOCLogInfo = count ; <nl> return ; <nl> } <nl> + g_free ( reply_async ); <nl> reply . TerminationCount = count ; <nl> break ; <nl> 
static int qcrypto_ivgen_essiv_calculate ( QCryptoIVGen * ivgen , <nl> uint8_t * data = g_new ( uint8_t , ndata ); <nl>  <nl> sector = cpu_to_le64 ( sector ); <nl> - memcpy ( data , ( uint8_t *)& sector , ndata ); <nl> + memcpy ( data , ( uint8_t *)& sector , MIN ( sizeof ( sector ), ndata )); <nl> if ( sizeof ( sector ) < ndata ) { <nl> memset ( data + sizeof ( sector ), 0 , ndata - sizeof ( sector )); <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> /* libc does special remapping of the return value of <nl> * sys_getpriority () so it ' s just easiest to call <nl> * sys_getpriority () directly rather than through libc . */ <nl> - ret = sys_getpriority ( arg1 , arg2 ); <nl> + ret = get_errno ( sys_getpriority ( arg1 , arg2 )); <nl> break ; <nl> case TARGET_NR_setpriority : <nl> ret = get_errno ( setpriority ( arg1 , arg2 , arg3 ));
static void nbd_refresh_filename ( BlockDriverState * bs , QDict * options ) <nl> ov = qobject_output_visitor_new (& saddr_qdict ); <nl> visit_type_SocketAddress ( ov , NULL , & s -> saddr , & error_abort ); <nl> visit_complete ( ov , & saddr_qdict ); <nl> + visit_free ( ov ); <nl> assert ( qobject_type ( saddr_qdict ) == QTYPE_QDICT ); <nl>  <nl> qdict_put_obj ( opts , " server ", saddr_qdict );
static void test_io_channel_tls ( const void * opaque ) <nl> mainloop = g_main_context_default (); <nl> do { <nl> g_main_context_iteration ( mainloop , TRUE ); <nl> - } while (! clientHandshake . finished && <nl> + } while (! clientHandshake . finished || <nl> ! serverHandshake . finished ); <nl>  <nl> g_assert ( clientHandshake . failed == data -> expectClientFail );
static void process_event ( JSONMessageParser * parser , QList * tokens ) <nl> error_free ( err ); <nl> } <nl> ret = send_response ( s , QOBJECT ( qdict )); <nl> - if ( ret ) { <nl> - g_warning (" error sending error response : % s ", strerror ( ret )); <nl> + if ( ret < 0 ) { <nl> + g_warning (" error sending error response : % s ", strerror (- ret )); <nl> } <nl> } <nl> 
static CharDriverState * gd_vc_handler ( ChardevVC * vc , Error ** errp ) <nl> chr -> chr_set_echo = gd_vc_chr_set_echo ; <nl>  <nl> /* Temporary , until gd_vc_vte_init runs . */ <nl> - chr -> opaque = g_new ( VirtualConsole , 1 ); <nl> + chr -> opaque = g_new0 ( VirtualConsole , 1 ); <nl>  <nl> /* defer OPENED events until our vc is fully initialized */ <nl> chr -> explicit_be_open = true ;
static void win32_aio_process_completion ( QEMUWin32AIOState * s , <nl> memcpy ( qiov -> iov [ i ]. iov_base , p , qiov -> iov [ i ]. iov_len ); <nl> p += qiov -> iov [ i ]. iov_len ; <nl> } <nl> - qemu_vfree ( waiocb -> buf ); <nl> } <nl> + qemu_vfree ( waiocb -> buf ); <nl> } <nl>  <nl> 
static void sch_handle_start_func ( SubchDev * sch , ORB * orb ) <nl> path = 0x80 ; <nl>  <nl> if (!( s -> ctrl & SCSW_ACTL_SUSP )) { <nl> + s -> cstat = 0 ; <nl> + s -> dstat = 0 ; <nl> /* Look at the orb and try to execute the channel program . */ <nl> assert ( orb != NULL ); /* resume does not pass an orb */ <nl> p -> intparm = orb -> intparm ;
static inline void powerpc_excp ( CPUPPCState * env , int excp_model , int excp ) <nl> if ( asrr1 != - 1 ) <nl> env -> spr [ asrr1 ] = env -> spr [ srr1 ]; <nl> /* If we disactivated any translation , flush TLBs */ <nl> - if ( new_msr & (( 1 << MSR_IR ) | ( 1 << MSR_DR ))) <nl> + if ( msr & (( 1 << MSR_IR ) | ( 1 << MSR_DR ))) <nl> tlb_flush ( env , 1 ); <nl>  <nl> if ( msr_ile ) {
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> - g_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
SCSIRequest * scsi_req_ref ( SCSIRequest * req ) <nl>  <nl> void scsi_req_unref ( SCSIRequest * req ) <nl> { <nl> + assert ( req -> refcount > 0 ); <nl> if (-- req -> refcount == 0 ) { <nl> if ( req -> ops -> free_req ) { <nl> req -> ops -> free_req ( req );
static QemuOptsList parallels_runtime_opts = { <nl> . name = PARALLELS_OPT_PREALLOC_SIZE , <nl> . type = QEMU_OPT_SIZE , <nl> . help = " Preallocation size on image expansion ", <nl> - . def_value_str = " 128MiB ", <nl> + . def_value_str = " 128M ", <nl> }, <nl> { <nl> . name = PARALLELS_OPT_PREALLOC_MODE ,
BlockDirtyInfoList * bdrv_query_dirty_bitmaps ( BlockDriverState * bs ) <nl> QLIST_FOREACH ( bm , & bs -> dirty_bitmaps , list ) { <nl> BlockDirtyInfo * info = g_new0 ( BlockDirtyInfo , 1 ); <nl> BlockDirtyInfoList * entry = g_new0 ( BlockDirtyInfoList , 1 ); <nl> - info -> count = bdrv_get_dirty_count ( bm ); <nl> + info -> count = bdrv_get_dirty_count ( bm ) << BDRV_SECTOR_BITS ; <nl> info -> granularity = bdrv_dirty_bitmap_granularity ( bm ); <nl> info -> has_name = !! bm -> name ; <nl> info -> name = g_strdup ( bm -> name );
static void glue ( audio_pcm_hw_gc_ , TYPE ) ( HW ** hwp ) <nl> audio_detach_capture ( hw ); <nl> # endif <nl> QLIST_REMOVE ( hw , entries ); <nl> + glue ( hw -> pcm_ops -> fini_ , TYPE ) ( hw ); <nl> glue ( s -> nb_hw_voices_ , TYPE ) += 1 ; <nl> glue ( audio_pcm_hw_free_resources_ , TYPE ) ( hw ); <nl> - glue ( hw -> pcm_ops -> fini_ , TYPE ) ( hw ); <nl> g_free ( hw ); <nl> * hwp = NULL ; <nl> }
static void drive_uninit ( DriveInfo * dinfo ) <nl> { <nl> qemu_opts_del ( dinfo -> opts ); <nl> bdrv_delete ( dinfo -> bdrv ); <nl> + qemu_free ( dinfo -> id ); <nl> QTAILQ_REMOVE (& drives , dinfo , next ); <nl> qemu_free ( dinfo ); <nl> }
static int exynos4210_combiner_init ( SysBusDevice * sbd ) <nl> qdev_init_gpio_in ( dev , exynos4210_combiner_handler , IIC_NIRQ ); <nl>  <nl> /* Connect SysBusDev irqs to device specific irqs */ <nl> - for ( i = 0 ; i < IIC_NIRQ ; i ++) { <nl> + for ( i = 0 ; i < IIC_NGRP ; i ++) { <nl> sysbus_init_irq ( sbd , & s -> output_irq [ i ]); <nl> } <nl> 
static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl> if ( ret < 0 ) { <nl> error_report (" failed to create inode for snapshot : % s ", <nl> error_get_pretty ( local_err )); <nl> + error_free ( local_err ); <nl> goto cleanup ; <nl> } <nl> 
static void xen_pci_passthrough_class_init ( ObjectClass * klass , void * data ) <nl> k -> exit = xen_pt_unregister_device ; <nl> k -> config_read = xen_pt_pci_read_config ; <nl> k -> config_write = xen_pt_pci_write_config ; <nl> + k -> is_express = 1 ; /* We might be */ <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> desc = " Assign an host PCI device with Xen "; <nl> dc -> props = xen_pci_passthrough_properties ;
# ifndef BITMAP_H <nl> # define BITMAP_H <nl>  <nl> -# include " qemu - common . h " <nl> +# include < glib . h > <nl> +# include < string . h > <nl> +# include < stdlib . h > <nl> + <nl> +# include " qemu / osdep . h " <nl> # include " qemu / bitops . h " <nl>  <nl> /*
bool bdrv_dev_is_medium_locked ( BlockDriverState * bs ) <nl> */ <nl> int bdrv_check ( BlockDriverState * bs , BdrvCheckResult * res , BdrvCheckMode fix ) <nl> { <nl> + if ( bs -> drv == NULL ) { <nl> + return - ENOMEDIUM ; <nl> + } <nl> if ( bs -> drv -> bdrv_check == NULL ) { <nl> return - ENOTSUP ; <nl> }
print_insn ( bfd_vma pc , disassemble_info * info ) <nl> } <nl> } <nl>  <nl> - if ( putop ( dp -> name , sizeflag ) == 0 ) <nl> + if ( dp -> name != NULL && putop ( dp -> name , sizeflag ) == 0 ) <nl> { <nl> for ( i = 0 ; i < MAX_OPERANDS ; ++ i ) <nl> {
static bool ga_open_pidfile ( const char * pidfile ) <nl> goto fail ; <nl> } <nl>  <nl> + /* keep pidfile open & locked forever */ <nl> return true ; <nl>  <nl> fail : <nl> unlink ( pidfile ); <nl> + close ( pidfd ); <nl> return false ; <nl> } <nl> # else /* _WIN32 */
get_net_error_message ( gint error ) <nl> if ( msg != NULL ) { <nl> nchars = wcslen ( msg ); <nl>  <nl> - if ( nchars > 2 && <nl> + if ( nchars >= 2 && <nl> msg [ nchars - 1 ] == L '\ n ' && <nl> msg [ nchars - 2 ] == L '\ r ') { <nl> msg [ nchars - 2 ] = L '\ 0 ';
static void numa_add ( const char * optarg ) <nl> node_mem [ nodenr ] = 0 ; <nl> } else { <nl> int64_t sval ; <nl> - sval = strtosz ( option , NULL ); <nl> - if ( sval < 0 ) { <nl> + sval = strtosz ( option , & endptr ); <nl> + if ( sval < 0 || * endptr ) { <nl> fprintf ( stderr , " qemu : invalid numa mem size : % s \ n ", optarg ); <nl> exit ( 1 ); <nl> }
void dpy_gfx_replace_surface ( QemuConsole * con , <nl> DisplaySurface * old_surface = con -> surface ; <nl> DisplayChangeListener * dcl ; <nl>  <nl> + assert ( old_surface != surface ); <nl> + <nl> con -> surface = surface ; <nl> QLIST_FOREACH ( dcl , & s -> listeners , next ) { <nl> if ( con != ( dcl -> con ? dcl -> con : active_console )) {
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> + qemu_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
static int vmxnet3_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> + vmxnet3_validate_queues ( s ); <nl> vmxnet3_validate_interrupts ( s ); <nl>  <nl> return 0 ;
void ide_dma_cb ( void * opaque , int ret ) <nl> sector_num , n , s -> dma_cmd ); <nl> # endif <nl>  <nl> - if (! ide_sect_range_ok ( s , sector_num , n )) { <nl> + if (( s -> dma_cmd == IDE_DMA_READ || s -> dma_cmd == IDE_DMA_WRITE ) && <nl> + ! ide_sect_range_ok ( s , sector_num , n )) { <nl> dma_buf_commit ( s ); <nl> ide_dma_error ( s ); <nl> return ;
static void test_visitor_in_fuzz ( TestInputVisitorData * data , <nl>  <nl> v = visitor_input_test_init ( data , buf ); <nl> visit_type_intList ( v , NULL , & ilres , NULL ); <nl> + qapi_free_intList ( ilres ); <nl> visitor_input_teardown ( data , NULL ); <nl>  <nl> v = visitor_input_test_init ( data , buf );
static int local_symlink ( FsContext * fs_ctx , const char * oldpath , <nl> return fd ; <nl> } <nl> /* Write the oldpath ( target ) to the file . */ <nl> - oldpath_size = strlen ( oldpath ) + 1 ; <nl> + oldpath_size = strlen ( oldpath ); <nl> do { <nl> write_size = write ( fd , ( void *) oldpath , oldpath_size ); <nl> } while ( write_size == - 1 && errno == EINTR );
static const char * stream_video_names [] = { <nl> [ SPICE_STREAM_VIDEO_FILTER ] = " filter ", <nl> }; <nl> # define parse_stream_video ( _name ) \ <nl> - name2enum ( _name , stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl> + parse_name ( _name , " stream video control ", \ <nl> + stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl>  <nl> static const char * compression_names [] = { <nl> [ SPICE_IMAGE_COMPRESS_OFF ] = " off ",
void ide_exec_cmd ( IDEBus * bus , uint32_t val ) <nl> lba48 = 1 ; <nl> /* fall through */ <nl> case WIN_READ_NATIVE_MAX : <nl> + /* Refuse if no sectors are addressable ( e . g . medium not inserted ) */ <nl> + if ( s -> nb_sectors == 0 ) { <nl> + goto abort_cmd ; <nl> + } <nl> ide_cmd_lba48_transform ( s , lba48 ); <nl> ide_set_sector ( s , s -> nb_sectors - 1 ); <nl> s -> status = READY_STAT | SEEK_STAT ;
static void curses_refresh ( DisplayChangeListener * dcl ) <nl> qemu_input_event_send_key_delay ( 0 ); <nl> } <nl> } else { <nl> - keysym = curses2qemu [ chr ]; <nl> + keysym = - 1 ; <nl> + if ( chr < CURSES_KEYS ) { <nl> + keysym = curses2qemu [ chr ]; <nl> + } <nl> if ( keysym == - 1 ) <nl> keysym = chr ; <nl> 
static void gen_msa ( CPUMIPSState * env , DisasContext * ctx ) <nl> case OPC_LD_H : <nl> case OPC_LD_W : <nl> case OPC_LD_D : <nl> + save_cpu_state ( ctx , 1 ); <nl> gen_helper_msa_ld_df ( cpu_env , tdf , twd , trs , ts10 ); <nl> break ; <nl> case OPC_ST_B : <nl> case OPC_ST_H : <nl> case OPC_ST_W : <nl> case OPC_ST_D : <nl> + save_cpu_state ( ctx , 1 ); <nl> gen_helper_msa_st_df ( cpu_env , tdf , twd , trs , ts10 ); <nl> break ; <nl> }
void cpu_alpha_store_fpcr ( CPUState * env , uint64_t val ) <nl> round_mode = float_round_nearest_even ; <nl> break ; <nl> case 3 : <nl> + default : /* this avoids a gcc (< 4 . 4 ) warning */ <nl> round_mode = float_round_up ; <nl> break ; <nl> }
static void ccid_handle_data ( USBDevice * dev , USBPacket * p ) <nl> " handle_data : int_in : notify_slot_change % X , " <nl> " requested len % zd \ n ", <nl> s -> bmSlotICCState , p -> iov . size ); <nl> + } else { <nl> + p -> status = USB_RET_NAK ; <nl> } <nl> break ; <nl> default :
static int kvm_physical_sync_dirty_bitmap ( MemoryRegionSection * section ) <nl> { <nl> KVMState * s = kvm_state ; <nl> unsigned long size , allocated_size = 0 ; <nl> - KVMDirtyLog d ; <nl> + KVMDirtyLog d = {}; <nl> KVMSlot * mem ; <nl> int ret = 0 ; <nl> hwaddr start_addr = section -> offset_within_address_space ;
static int target_pread ( int fd , abi_ulong ptr , abi_ulong len , <nl> int ret ; <nl>  <nl> buf = lock_user ( VERIFY_WRITE , ptr , len , 0 ); <nl> + if (! buf ) { <nl> + return - EFAULT ; <nl> + } <nl> ret = pread ( fd , buf , len , offset ); <nl> + if ( ret < 0 ) { <nl> + ret = - errno ; <nl> + } <nl> unlock_user ( buf , ptr , len ); <nl> return ret ; <nl> }
static void test_dummy_createcmdl ( void ) <nl> g_assert ( err == NULL ); <nl> error_free ( err ); <nl>  <nl> + object_unref ( OBJECT ( dobj )); <nl> + <nl> /* <nl> * cmdline - parsing via qemu_opts_parse () results in a QemuOpts entry <nl> * corresponding to the Object ' s ID to be added to the QemuOptsList
static void cpu_openrisc_load_kernel ( ram_addr_t ram_size , <nl> kernel_filename ); <nl> exit ( 1 ); <nl> } <nl> + cpu -> env . pc = entry ; <nl> } <nl> - <nl> - cpu -> env . pc = entry ; <nl> } <nl>  <nl> static void openrisc_sim_init ( QEMUMachineInitArgs * args )
void socket_listen_cleanup ( int fd , Error ** errp ) <nl> } <nl> } <nl>  <nl> - g_free ( addr ); <nl> + qapi_free_SocketAddress ( addr ); <nl> } <nl>  <nl> int socket_dgram ( SocketAddress * remote , SocketAddress * local , Error ** errp )
static void handle_keydown ( SDL_Event * ev ) <nl> case SDL_SCANCODE_7 : <nl> case SDL_SCANCODE_8 : <nl> case SDL_SCANCODE_9 : <nl> + if ( gui_grab ) { <nl> + sdl_grab_end ( scon ); <nl> + } <nl> + <nl> win = ev -> key . keysym . scancode - SDL_SCANCODE_1 ; <nl> if ( win < sdl2_num_outputs ) { <nl> sdl2_console [ win ]. hidden = ! sdl2_console [ win ]. hidden ;
static void rc4030_write ( void * opaque , hwaddr addr , uint64_t data , <nl> break ; <nl> /* Interval timer reload */ <nl> case 0x0228 : <nl> - s -> itr = val ; <nl> + s -> itr = val & 0x01FF ; <nl> qemu_irq_lower ( s -> timer_irq ); <nl> set_next_tick ( s ); <nl> break ;
static void ehci_frame_timer ( void * opaque ) <nl> int need_timer = 0 ; <nl> int64_t expire_time , t_now ; <nl> uint64_t ns_elapsed ; <nl> - int uframes , skipped_uframes ; <nl> + uint64_t uframes , skipped_uframes ; <nl> int i ; <nl>  <nl> t_now = qemu_clock_get_ns ( QEMU_CLOCK_VIRTUAL );
static int init_directory ( BDRVVVFATState * s , const char * dirname ) <nl> memset (&( s -> first_sectors [ 0 ]), 0 , 0x40 * 0x200 ); <nl>  <nl> /* TODO : if FAT32 , this is probably wrong */ <nl> - s -> sectors_per_fat = 0xfc ; <nl> + s -> sectors_per_fat = 0xec ; <nl> s -> sectors_per_cluster = 0x10 ; <nl> s -> cluster_size = s -> sectors_per_cluster * 0x200 ; <nl> s -> cluster = malloc ( s -> cluster_size );
static int pci_add_option_rom ( PCIDevice * pdev , bool is_default_rom ) <nl> pci_patch_ids ( pdev , ptr , size ); <nl> } <nl>  <nl> + qemu_put_ram_ptr ( ptr ); <nl> + <nl> pci_register_bar ( pdev , PCI_ROM_SLOT , size , <nl> 0 , pci_map_option_rom ); <nl> 
static BlockDriverAIOCB * rbd_start_aio ( BlockDriverState * bs , <nl> } <nl>  <nl> if ( r < 0 ) { <nl> - goto failed ; <nl> + goto failed_completion ; <nl> } <nl>  <nl> return & acb -> common ; <nl>  <nl> + failed_completion : <nl> + rbd_aio_release ( c ); <nl> failed : <nl> g_free ( rcb ); <nl> + qemu_vfree ( acb -> bounce ); <nl> qemu_aio_release ( acb ); <nl> return NULL ; <nl> }
SDState * sd_init ( BlockDriverState * bs , bool is_spi ) <nl> { <nl> SDState * sd ; <nl>  <nl> - if ( bdrv_is_read_only ( bs )) { <nl> + if ( bs && bdrv_is_read_only ( bs )) { <nl> fprintf ( stderr , " sd_init : Cannot use read - only drive \ n "); <nl> return NULL ; <nl> }
typedef struct QObject { <nl>  <nl> /* High - level interface for qobject_decref () */ <nl> # define QDECREF ( obj ) \ <nl> - qobject_decref ( QOBJECT ( obj )) <nl> + qobject_decref ( obj ? QOBJECT ( obj ) : NULL ) <nl>  <nl> /* Initialize an object to default values */ <nl> # define QOBJECT_INIT ( obj , qtype_type ) \
__org_qemu_x_Union1 * qmp___org_qemu_x_command ( __org_qemu_x_EnumList * a , <nl> ret -> type = ORG_QEMU_X_UNION1_KIND___ORG_QEMU_X_BRANCH ; <nl> ret -> u . __org_qemu_x_branch = strdup (" blah1 "); <nl>  <nl> + /* Also test that ' wchar - t ' was munged to ' q_wchar_t ' */ <nl> + if ( b && b -> value && ! b -> value -> has_q_wchar_t ) { <nl> + b -> value -> q_wchar_t = 1 ; <nl> + } <nl> return ret ; <nl> } <nl> 
restart : <nl> QLIST_REMOVE ( elem , all ); <nl> /* Read state before ret . */ <nl> smp_rmb (); <nl> + <nl> + /* Schedule ourselves in case elem -> common . cb () calls aio_poll () to <nl> + * wait for another request that completed at the same time . <nl> + */ <nl> + qemu_bh_schedule ( pool -> completion_bh ); <nl> + <nl> elem -> common . cb ( elem -> common . opaque , elem -> ret ); <nl> qemu_aio_release ( elem ); <nl> goto restart ;
static struct iovec * lock_iovec ( int type , abi_ulong target_addr , <nl> return vec ; <nl>  <nl> fail : <nl> + while (-- i >= 0 ) { <nl> + if ( tswapal ( target_vec [ i ]. iov_len ) > 0 ) { <nl> + unlock_user ( vec [ i ]. iov_base , tswapal ( target_vec [ i ]. iov_base ), 0 ); <nl> + } <nl> + } <nl> unlock_user ( target_vec , target_addr , 0 ); <nl> fail2 : <nl> free ( vec );
QObject * json_parser_parse ( QList * tokens , va_list * ap ) <nl> QObject * json_parser_parse_err ( QList * tokens , va_list * ap , Error ** errp ) <nl> { <nl> JSONParserContext ctxt = {}; <nl> - QList * working = qlist_copy ( tokens ); <nl> + QList * working ; <nl> QObject * result ; <nl>  <nl> + if (! tokens ) { <nl> + return NULL ; <nl> + } <nl> + working = qlist_copy ( tokens ); <nl> result = parse_value (& ctxt , & working , ap ); <nl>  <nl> QDECREF ( working );
void cpu_exec_init ( CPUArchState * env ) <nl> # ifndef CONFIG_USER_ONLY <nl> cpu -> as = & address_space_memory ; <nl> cpu -> thread_id = qemu_get_thread_id (); <nl> + cpu_reload_memory_map ( cpu ); <nl> # endif <nl> QTAILQ_INSERT_TAIL (& cpus , cpu , node ); <nl> # if defined ( CONFIG_USER_ONLY )
int bdrv_aio_multiwrite ( BlockDriverState * bs , BlockRequest * reqs , int num_reqs ) <nl> reqs [ i ]. error = - EIO ; <nl> goto fail ; <nl> } else { <nl> - mcb -> error = - EIO ; <nl> + mcb -> num_requests ++; <nl> + multiwrite_cb ( mcb , - EIO ); <nl> break ; <nl> } <nl> } else {
static int open_self_cmdline ( void * cpu_env , int fd ) <nl>  <nl> if ( word_skipped ) { <nl> if ( write ( fd , cp_buf , nb_read ) != nb_read ) { <nl> + close ( fd_orig ); <nl> return - 1 ; <nl> } <nl> }
static void tcg_out_qemu_st ( TCGContext * s , const TCGArg * args , int opc ) <nl> # else <nl> tcg_out_mov ( s , TCG_TYPE_I32 , 3 , addr_reg2 ); <nl> tcg_out_mov ( s , TCG_TYPE_I32 , 4 , addr_reg ); <nl> -# ifdef TCG_TARGET_CALL_ALIGN_ARGS <nl> ir = 5 ; <nl> -# else <nl> - ir = 4 ; <nl> -# endif <nl> # endif <nl>  <nl> switch ( opc ) {
static void qemu_input_queue_process ( void * opaque ) <nl> item = QTAILQ_FIRST ( queue ); <nl> g_assert ( item -> type == QEMU_INPUT_QUEUE_DELAY ); <nl> QTAILQ_REMOVE ( queue , item , node ); <nl> + queue_count --; <nl> g_free ( item ); <nl>  <nl> while (! QTAILQ_EMPTY ( queue )) {
void tlb_fill ( CPUState * env1 , target_ulong addr , int is_write , int mmu_idx , <nl> int ret ; <nl>  <nl> saved_env = env ; <nl> + env = env1 ; <nl> ret = cpu_arm_handle_mmu_fault ( env , addr , is_write , mmu_idx ); <nl> if ( unlikely ( ret )) { <nl> if ( retaddr ) {
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> if ( st . st_size > 0x7fffffff ) { <nl> fprintf ( stderr , " File % s is larger than 2GB \ n ", buffer ); <nl> free ( buffer ); <nl> + closedir ( dir ); <nl> return - 2 ; <nl> } <nl> direntry -> size = cpu_to_le32 ( S_ISDIR ( st . st_mode )? 0 : st . st_size );
static void print_block_info ( Monitor * mon , BlockInfo * info , <nl> inserted -> iops_size ); <nl> } <nl>  <nl> - if ( verbose ) { <nl> + /* TODO : inserted -> image should never be null */ <nl> + if ( verbose && inserted -> image ) { <nl> monitor_printf ( mon , "\ nImages :\ n "); <nl> image_info = inserted -> image ; <nl> while ( 1 ) {
static void set_year_20xx ( void ) <nl> g_assert_cmpint ( cmos_read ( RTC_YEAR ), ==, 0x11 ); <nl> g_assert_cmpint ( cmos_read ( RTC_CENTURY ), ==, 0x20 ); <nl>  <nl> + if ( sizeof ( time_t ) == 4 ) { <nl> + return ; <nl> + } <nl> + <nl> /* Set a date in 2080 to ensure there is no year - 2038 overflow . */ <nl> cmos_write ( RTC_REG_A , 0x76 ); <nl> cmos_write ( RTC_YEAR , 0x80 );
static int vmdk_init_tables ( BlockDriverState * bs , VmdkExtent * extent , <nl> Error ** errp ) <nl> { <nl> int ret ; <nl> - int l1_size , i ; <nl> + size_t l1_size ; <nl> + int i ; <nl>  <nl> /* read the L1 table */ <nl> l1_size = extent -> l1_size * sizeof ( uint32_t );
static void do_sdl_resize ( int new_width , int new_height , int bpp ) <nl>  <nl> // printf (" resizing to % d % d \ n ", w , h ); <nl>  <nl> - flags = SDL_HWSURFACE | SDL_ASYNCBLIT | SDL_HWACCEL | SDL_RESIZABLE ; <nl> - if ( gui_fullscreen ) <nl> + flags = SDL_HWSURFACE | SDL_ASYNCBLIT | SDL_HWACCEL ; <nl> + if ( gui_fullscreen ) { <nl> flags |= SDL_FULLSCREEN ; <nl> + } else { <nl> + flags |= SDL_RESIZABLE ; <nl> + } <nl> if ( gui_noframe ) <nl> flags |= SDL_NOFRAME ; <nl> 
static void gen_sse ( CPUX86State * env , DisasContext * s , int b , <nl> break ; <nl> # ifdef TARGET_X86_64 <nl> case MO_64 : <nl> - tcg_gen_mulu2_i64 ( cpu_regs [ s -> vex_v ], cpu_regs [ reg ], <nl> + tcg_gen_mulu2_i64 ( cpu_T [ 0 ], cpu_T [ 1 ], <nl> cpu_T [ 0 ], cpu_regs [ R_EDX ]); <nl> + tcg_gen_mov_i64 ( cpu_regs [ s -> vex_v ], cpu_T [ 0 ]); <nl> + tcg_gen_mov_i64 ( cpu_regs [ reg ], cpu_T [ 1 ]); <nl> break ; <nl> # endif <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl>  <nl> if (! lock_user_struct ( VERIFY_WRITE , target_st , arg2 , 0 )) <nl> goto efault ; <nl> + memset ( target_st , 0 , sizeof (* target_st )); <nl> __put_user ( st . st_dev , & target_st -> st_dev ); <nl> __put_user ( st . st_ino , & target_st -> st_ino ); <nl> __put_user ( st . st_mode , & target_st -> st_mode );
void slirp_input ( const uint8_t * pkt , int pkt_len ) <nl> if (! m ) <nl> return ; <nl> /* Note : we add to align the IP header */ <nl> + if ( M_FREEROOM ( m ) < pkt_len + 2 ) { <nl> + m_inc ( m , pkt_len + 2 ); <nl> + } <nl> m -> m_len = pkt_len + 2 ; <nl> memcpy ( m -> m_data + 2 , pkt , pkt_len ); <nl> 
static void qxl_reset_state ( PCIQXLDevice * d ) <nl> d -> num_free_res = 0 ; <nl> d -> last_release = NULL ; <nl> memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + qxl_update_irq ( d ); <nl> } <nl>  <nl> static void qxl_soft_reset ( PCIQXLDevice * d )
static uint32_t parse_enumeration ( char * str , <nl> { <nl> uint32_t ret = not_found_value ; <nl>  <nl> + if ( str == NULL ) <nl> + return 0 ; <nl> + <nl> while ( table -> name != NULL ) { <nl> if ( strcmp ( table -> name , str ) == 0 ) { <nl> ret = table -> value ;
static void kvm_ioapic_put ( IOAPICCommonState * s ) <nl>  <nl> void kvm_ioapic_dump_state ( Monitor * mon , const QDict * qdict ) <nl> { <nl> - IOAPICCommonState s ; <nl> + IOAPICCommonState * s = IOAPIC_COMMON ( object_resolve_path (" ioapic ", NULL )); <nl>  <nl> - kvm_ioapic_get (& s ); <nl> - <nl> - ioapic_print_redtbl ( mon , & s ); <nl> + assert ( s ); <nl> + kvm_ioapic_get ( s ); <nl> + ioapic_print_redtbl ( mon , s ); <nl> } <nl>  <nl> static void kvm_ioapic_reset ( DeviceState * dev )
if ( cmd == val ) { \ <nl> output_cmd ( IPC_STAT ); <nl> output_cmd ( IPC_INFO ); <nl> /* msgctl () commands */ <nl> - # ifdef __USER_MISC <nl> output_cmd ( MSG_STAT ); <nl> output_cmd ( MSG_INFO ); <nl> - # endif <nl> /* shmctl () commands */ <nl> output_cmd ( SHM_LOCK ); <nl> output_cmd ( SHM_UNLOCK );
void qemu_iovec_concat_iov ( QEMUIOVector * dst , <nl> { <nl> int i ; <nl> size_t done ; <nl> + <nl> + if (! sbytes ) { <nl> + return ; <nl> + } <nl> assert ( dst -> nalloc != - 1 ); <nl> for ( i = 0 , done = 0 ; done < sbytes && i < src_cnt ; i ++) { <nl> if ( soffset < src_iov [ i ]. iov_len ) {
IOCTL ( BLKFLSBUF , 0 , TYPE_NULL ) <nl> IOCTL ( BLKRASET , 0 , TYPE_INT ) <nl> IOCTL ( BLKRAGET , IOC_R , MK_PTR ( TYPE_LONG )) <nl> - IOCTL ( BLKSSZGET , IOC_R , MK_PTR ( TYPE_LONG )) <nl> + IOCTL ( BLKSSZGET , IOC_R , MK_PTR ( TYPE_INT )) <nl> IOCTL ( BLKBSZGET , IOC_R , MK_PTR ( TYPE_INT )) <nl> IOCTL_SPECIAL ( BLKPG , IOC_W , do_ioctl_blkpg , <nl> MK_PTR ( MK_STRUCT ( STRUCT_blkpg_ioctl_arg )))
static int virtio_net_handle_mac ( VirtIONet * n , uint8_t cmd , <nl> goto error ; <nl> } <nl>  <nl> - if ( in_use + mac_data . entries <= MAC_TABLE_ENTRIES ) { <nl> + if ( mac_data . entries <= MAC_TABLE_ENTRIES - in_use ) { <nl> s = iov_to_buf ( iov , iov_cnt , 0 , & macs [ in_use * ETH_ALEN ], <nl> mac_data . entries * ETH_ALEN ); <nl> if ( s != mac_data . entries * ETH_ALEN ) {
static void vfio_ccw_register_io_notifier ( VFIOCCWDevice * vcdev , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> - argsz = sizeof (* irq_set ); <nl> + argsz = sizeof (* irq_info ); <nl> irq_info = g_malloc0 ( argsz ); <nl> irq_info -> index = VFIO_CCW_IO_IRQ_INDEX ; <nl> irq_info -> argsz = argsz ;
void configure_icount ( const char * option ) <nl>  <nl> void qemu_run_all_timers ( void ) <nl> { <nl> + alarm_timer -> pending = 0 ; <nl> + <nl> /* rearm timer , if not periodic */ <nl> if ( alarm_timer -> expired ) { <nl> alarm_timer -> expired = 0 ; <nl> qemu_rearm_alarm_timer ( alarm_timer ); <nl> } <nl>  <nl> - alarm_timer -> pending = 0 ; <nl> - <nl> /* vm time timers */ <nl> if ( vm_running ) { <nl> qemu_run_timers ( vm_clock );
static uint64_t alloc_cluster_offset ( BlockDriverState * bs , <nl> /* how many free clusters ? */ <nl>  <nl> while ( i < nb_clusters ) { <nl> - cluster_offset = l2_table [ l2_index + i ]; <nl> + cluster_offset = be64_to_cpu ( l2_table [ l2_index + i ]); <nl> if ( cluster_offset != 0 ) <nl> break ; <nl> i ++;
int qcow2_snapshot_load_tmp ( BlockDriverState * bs , <nl> sn = & s -> snapshots [ snapshot_index ]; <nl>  <nl> /* Allocate and read in the snapshot ' s L1 table */ <nl> - new_l1_bytes = s -> l1_size * sizeof ( uint64_t ); <nl> + new_l1_bytes = sn -> l1_size * sizeof ( uint64_t ); <nl> new_l1_table = g_malloc0 ( align_offset ( new_l1_bytes , 512 )); <nl>  <nl> ret = bdrv_pread ( bs -> file , sn -> l1_table_offset , new_l1_table , new_l1_bytes );
static GIOStatus ga_channel_write ( GAChannel * c , const char * buf , size_t size , <nl> * count = written ; <nl> } <nl>  <nl> + if ( ov . hEvent ) { <nl> + CloseHandle ( ov . hEvent ); <nl> + ov . hEvent = NULL ; <nl> + } <nl> return status ; <nl> } <nl> 
static uint64_t uart_read ( void * opaque , target_phys_addr_t offset , <nl> uint32_t c = 0 ; <nl>  <nl> offset >>= 2 ; <nl> - if ( offset > R_MAX ) { <nl> + if ( offset >= R_MAX ) { <nl> return 0 ; <nl> } else if ( offset == R_TX_RX ) { <nl> uart_read_rx_fifo ( s , & c );
static int usbredir_post_load ( void * priv , int version_id ) <nl> { <nl> USBRedirDevice * dev = priv ; <nl>  <nl> + if ( dev -> parser == NULL ) { <nl> + return 0 ; <nl> + } <nl> + <nl> switch ( dev -> device_info . speed ) { <nl> case usb_redir_speed_low : <nl> dev -> dev . speed = USB_SPEED_LOW ;
static void qxl_spice_monitors_config_async ( PCIQXLDevice * qxl , int replay ) <nl> } else { <nl> # if SPICE_SERVER_VERSION >= 0x000c06 /* release 0 . 12 . 6 */ <nl> if ( qxl -> max_outputs ) { <nl> - spice_qxl_set_monitors_config_limit (& qxl -> ssd . qxl , <nl> - qxl -> max_outputs ); <nl> + spice_qxl_set_max_monitors (& qxl -> ssd . qxl , qxl -> max_outputs ); <nl> } <nl> # endif <nl> qxl -> guest_monitors_config = qxl -> ram -> monitors_config ;
static coroutine_fn int qcow2_handle_l2meta ( BlockDriverState * bs , <nl> while ( l2meta != NULL ) { <nl> QCowL2Meta * next ; <nl>  <nl> - if (! ret && link_l2 ) { <nl> + if ( link_l2 ) { <nl> ret = qcow2_alloc_cluster_link_l2 ( bs , l2meta ); <nl> if ( ret ) { <nl> goto out ;
static inline uint64_t muldiv64 ( uint64_t a , uint32_t b , uint32_t c ) <nl> return res . ll ; <nl> } <nl>  <nl> +/* Round number down to multiple */ <nl> +# define QEMU_ALIGN_DOWN ( n , m ) (( n ) / ( m ) * ( m )) <nl> + <nl> +/* Round number up to multiple */ <nl> +# define QEMU_ALIGN_UP ( n , m ) QEMU_ALIGN_DOWN (( n ) + ( m ) - 1 , ( m )) <nl> + <nl> # include " module . h " <nl>  <nl> # endif
static int connect_to_ssh ( BDRVSSHState * s , QDict * options , <nl> /* Open the socket and connect . */ <nl> s -> sock = inet_connect ( s -> hostport , errp ); <nl> if ( s -> sock < 0 ) { <nl> - ret = - errno ; <nl> + ret = - EIO ; <nl> goto err ; <nl> } <nl> 
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } <nl> if ( dev -> setup_index < 0 || <nl> dev -> setup_len < 0 || <nl> - dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> - dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> + dev -> setup_index > dev -> setup_len || <nl> + dev -> setup_len > sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> } <nl> return 0 ;
static void create_flash ( const VirtBoardInfo * vbi ) <nl> error_report (" Could not load ROM image '% s '", bios_name ); <nl> exit ( 1 ); <nl> } <nl> - g_free ( fn ); <nl> } <nl>  <nl> create_one_flash (" virt . flash0 ", flashbase , flashsize );
static inline void gdb_continue ( GDBState * s ) <nl> # ifdef CONFIG_USER_ONLY <nl> s -> running_state = 1 ; <nl> # else <nl> - if ( runstate_check ( RUN_STATE_DEBUG )) { <nl> + if (! runstate_needs_reset ()) { <nl> vm_start (); <nl> } <nl> # endif
int bdrv_all_delete_snapshot ( const char * name , BlockDriverState ** first_bad_bs , <nl> if ( bdrv_can_snapshot ( bs ) && <nl> bdrv_snapshot_find ( bs , snapshot , name ) >= 0 ) { <nl> ret = bdrv_snapshot_delete_by_id_or_name ( bs , name , err ); <nl> - if ( ret < 0 ) { <nl> - goto fail ; <nl> - } <nl> } <nl> aio_context_release ( ctx ); <nl> if ( ret < 0 ) {
int kvm_cpu_exec ( CPUState * cpu ) <nl> qemu_system_reset_request (); <nl> ret = EXCP_INTERRUPT ; <nl> break ; <nl> + case KVM_SYSTEM_EVENT_CRASH : <nl> + qemu_mutex_lock_iothread (); <nl> + qemu_system_guest_panicked (); <nl> + qemu_mutex_unlock_iothread (); <nl> + ret = 0 ; <nl> + break ; <nl> default : <nl> DPRINTF (" kvm_arch_handle_exit \ n "); <nl> ret = kvm_arch_handle_exit ( cpu , run );
static int cpudef_setfield ( const char * name , const char * str , void * opaque ) <nl> int err = 0 ; <nl>  <nl> if (! strcmp ( name , " name ")) { <nl> + g_free (( void *) def -> name ); <nl> def -> name = g_strdup ( str ); <nl> } else if (! strcmp ( name , " model_id ")) { <nl> strncpy ( def -> model_id , str , sizeof ( def -> model_id ));
int s390_virtio_hypercall ( CPUState * env , uint64_t mem , uint64_t hypercall ) <nl>  <nl> dev = s390_virtio_bus_find_mem ( s390_bus , mem ); <nl> virtio_reset ( dev -> vdev ); <nl> + stb_phys ( dev -> dev_offs + VIRTIO_DEV_OFFS_STATUS , 0 ); <nl> s390_virtio_device_sync ( dev ); <nl> break ; <nl> }
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> - if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + if ( dev -> setup_index < 0 || <nl> + dev -> setup_len < 0 || <nl> + dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> }
static int usb_xhci_post_load ( void * opaque , int version_id ) <nl> slot -> uport = xhci_lookup_uport ( xhci , slot_ctx ); <nl> assert ( slot -> uport && slot -> uport -> dev ); <nl>  <nl> - for ( epid = 1 ; epid <= 32 ; epid ++) { <nl> + for ( epid = 1 ; epid <= 31 ; epid ++) { <nl> pctx = slot -> ctx + 32 * epid ; <nl> xhci_dma_read_u32s ( xhci , pctx , ep_ctx , sizeof ( ep_ctx )); <nl> state = ep_ctx [ 0 ] & EP_STATE_MASK ;
void laio_cleanup ( void * s_ ) <nl> struct qemu_laio_state * s = s_ ; <nl>  <nl> event_notifier_cleanup (& s -> e ); <nl> + <nl> + if ( io_destroy ( s -> ctx ) != 0 ) { <nl> + fprintf ( stderr , "% s : destroy AIO context % p failed \ n ", <nl> + __func__ , & s -> ctx ); <nl> + } <nl> g_free ( s ); <nl> }
static int cpu_post_load ( void * opaque , int version_id ) <nl> ppc_set_compat ( cpu , cpu -> compat_pvr , & local_err ); <nl> if ( local_err ) { <nl> error_report_err ( local_err ); <nl> - error_free ( local_err ); <nl> return - 1 ; <nl> } <nl> } else
static void format_string ( StringOutputVisitor * sov , Range * r , bool next , <nl> { <nl> if ( r -> end - r -> begin > 1 ) { <nl> if ( human ) { <nl> - g_string_append_printf ( sov -> string , " 0x %" PRIx64 "-%" PRIx64 , <nl> + g_string_append_printf ( sov -> string , " 0x %" PRIx64 "- 0x %" PRIx64 , <nl> r -> begin , r -> end - 1 ); <nl>  <nl> } else {
int load_snapshot ( const char * name , Error ** errp ) <nl>  <nl> aio_context_acquire ( aio_context ); <nl> ret = qemu_loadvm_state ( f ); <nl> - qemu_fclose ( f ); <nl> aio_context_release ( aio_context ); <nl>  <nl> migration_incoming_state_destroy ();
static void machvirt_init ( MachineState * machine ) <nl> " secure - memory ", & error_abort ); <nl> } <nl>  <nl> - object_property_set_bool ( cpuobj , true , " realized ", NULL ); <nl> + object_property_set_bool ( cpuobj , true , " realized ", & error_fatal ); <nl> object_unref ( cpuobj ); <nl> } <nl> fdt_add_timer_nodes ( vms );
bool timerlist_expired ( QEMUTimerList * timer_list ) <nl> expire_time = timer_list -> active_timers -> expire_time ; <nl> qemu_mutex_unlock (& timer_list -> active_timers_lock ); <nl>  <nl> - return expire_time < qemu_clock_get_ns ( timer_list -> clock -> type ); <nl> + return expire_time <= qemu_clock_get_ns ( timer_list -> clock -> type ); <nl> } <nl>  <nl> bool qemu_clock_expired ( QEMUClockType type )
static uint64_t get_migration_pass ( void ) <nl> } else { <nl> rsp_ram = qdict_get_qdict ( rsp_return , " ram "); <nl> result = qdict_get_try_int ( rsp_ram , " dirty - sync - count ", 0 ); <nl> - QDECREF ( rsp ); <nl> } <nl> + QDECREF ( rsp ); <nl> return result ; <nl> } <nl> 
uint64_t pc_dimm_get_free_addr ( uint64_t address_space_start , <nl> uint64_t new_addr , ret = 0 ; <nl> uint64_t address_space_end = address_space_start + address_space_size ; <nl>  <nl> - assert ( address_space_end > address_space_size ); <nl> + if (! address_space_size ) { <nl> + error_setg ( errp , " memory hotplug is not enabled , " <nl> + " please add maxmem option "); <nl> + goto out ; <nl> + } <nl> + <nl> + assert ( address_space_end > address_space_start ); <nl> object_child_foreach ( qdev_get_machine (), pc_dimm_built_list , & list ); <nl>  <nl> if ( hint ) {
ivshmem_server_handle_new_conn ( IvshmemServer * server ) <nl> } <nl> if ( i == G_MAXUINT16 ) { <nl> IVSHMEM_SERVER_DEBUG ( server , " cannot allocate new client id \ n "); <nl> - goto fail ; <nl> + close ( newfd ); <nl> + g_free ( peer ); <nl> + return - 1 ; <nl> } <nl> peer -> id = server -> cur_id ++; <nl> 
static KeyValue * copy_key_value ( KeyValue * src ) <nl> { <nl> KeyValue * dst = g_new ( KeyValue , 1 ); <nl> memcpy ( dst , src , sizeof (* src )); <nl> + if ( dst -> type == KEY_VALUE_KIND_NUMBER ) { <nl> + QKeyCode code = qemu_input_key_number_to_qcode ( dst -> u . number . data ); <nl> + dst -> type = KEY_VALUE_KIND_QCODE ; <nl> + dst -> u . qcode . data = code ; <nl> + } <nl> return dst ; <nl> } <nl> 
static void close_guest_eventfds ( IVShmemState * s , int posn ) <nl> { <nl> int i , guest_curr_max ; <nl>  <nl> + if (! ivshmem_has_feature ( s , IVSHMEM_IOEVENTFD )) { <nl> + return ; <nl> + } <nl> + <nl> guest_curr_max = s -> peers [ posn ]. nb_eventfds ; <nl>  <nl> memory_region_transaction_begin ();
static void do_ext_interrupt ( CPUS390XState * env ) <nl>  <nl> static void do_io_interrupt ( CPUS390XState * env ) <nl> { <nl> - uint64_t mask , addr ; <nl> + uint64_t mask = 0 , addr = 0 ; <nl> LowCore * lowcore ; <nl> IOIntQueue * q ; <nl> uint8_t isc ;
static void tcp_chr_tls_init ( CharDriverState * chr ) <nl> if ( tioc == NULL ) { <nl> error_free ( err ); <nl> tcp_chr_disconnect ( chr ); <nl> + return ; <nl> } <nl> object_unref ( OBJECT ( s -> ioc )); <nl> s -> ioc = QIO_CHANNEL ( tioc );
static CharDriverState * qemu_chr_open_tty ( QemuOpts * opts ) <nl> } <nl> tty_serial_init ( fd , 115200 , ' N ', 8 , 1 ); <nl> chr = qemu_chr_open_fd ( fd , fd ); <nl> - if (! chr ) { <nl> - close ( fd ); <nl> - return NULL ; <nl> - } <nl> chr -> chr_ioctl = tty_serial_ioctl ; <nl> chr -> chr_close = qemu_chr_close_tty ; <nl> return chr ;
static void coroutine_fn mirror_run ( void * opaque ) <nl>  <nl> s -> common . len = bdrv_getlength ( bs ); <nl> if ( s -> common . len <= 0 ) { <nl> - block_job_completed (& s -> common , s -> common . len ); <nl> - return ; <nl> + ret = s -> common . len ; <nl> + goto immediate_exit ; <nl> } <nl>  <nl> length = DIV_ROUND_UP ( s -> common . len , s -> granularity );
static inline void gen_illegal_opcode ( DisasContext * s ) <nl> gen_program_exception ( s , PGM_SPECIFICATION ); <nl> } <nl>  <nl> - static inline void check_privileged ( DisasContext * s ) <nl> +# ifndef CONFIG_USER_ONLY <nl> + static void check_privileged ( DisasContext * s ) <nl> { <nl> if ( s -> tb -> flags & ( PSW_MASK_PSTATE >> 32 )) { <nl> gen_program_exception ( s , PGM_PRIVILEGED ); <nl> } <nl> } <nl> +# endif <nl>  <nl> static TCGv_i64 get_address ( DisasContext * s , int x2 , int b2 , int d2 ) <nl> {
int rom_copy ( uint8_t * dest , hwaddr addr , size_t size ) <nl> if ( rom -> addr + rom -> romsize < addr ) { <nl> continue ; <nl> } <nl> - if ( rom -> addr > end ) { <nl> + if ( rom -> addr > end || rom -> addr < addr ) { <nl> break ; <nl> } <nl> 
int vnc_display_pw_expire ( DisplayState * ds , time_t expires ) <nl> { <nl> VncDisplay * vs = ds ? ( VncDisplay *) ds -> opaque : vnc_display ; <nl>  <nl> + if (! vs ) { <nl> + return - EINVAL ; <nl> + } <nl> + <nl> vs -> expires = expires ; <nl> return 0 ; <nl> }
static void cleanup_infolist ( CommandLineParameterInfoList * head ) <nl> if (! strcmp ( pre_entry -> value -> name , cur -> next -> value -> name )) { <nl> del_entry = cur -> next ; <nl> cur -> next = cur -> next -> next ; <nl> - g_free ( del_entry ); <nl> + del_entry -> next = NULL ; <nl> + qapi_free_CommandLineParameterInfoList ( del_entry ); <nl> break ; <nl> } <nl> pre_entry = pre_entry -> next ;
static uint64_t acpi_pm_tmr_read ( void * opaque , hwaddr addr , unsigned width ) <nl> return acpi_pm_tmr_get ( opaque ); <nl> } <nl>  <nl> + static void acpi_pm_tmr_write ( void * opaque , hwaddr addr , uint64_t val , <nl> + unsigned width ) <nl> +{ <nl> + /* nothing */ <nl> +} <nl> + <nl> static const MemoryRegionOps acpi_pm_tmr_ops = { <nl> . read = acpi_pm_tmr_read , <nl> + . write = acpi_pm_tmr_write , <nl> . valid . min_access_size = 4 , <nl> . valid . max_access_size = 4 , <nl> . endianness = DEVICE_LITTLE_ENDIAN ,
typedef struct VirtQueueElement <nl> struct iovec out_sg [ VIRTQUEUE_MAX_SIZE ]; <nl> } VirtQueueElement ; <nl>  <nl> -# define VIRTIO_QUEUE_MAX 64 <nl> +# define VIRTIO_QUEUE_MAX 1024 <nl>  <nl> # define VIRTIO_NO_VECTOR 0xffff <nl> 
static void wdt_diag288_class_init ( ObjectClass * klass , void * data ) <nl> dc -> realize = wdt_diag288_realize ; <nl> dc -> unrealize = wdt_diag288_unrealize ; <nl> dc -> reset = wdt_diag288_reset ; <nl> + dc -> hotpluggable = false ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> vmsd = & vmstate_diag288 ; <nl> diag288 -> handle_timer = wdt_diag288_handle_timer ;
static void * oss_audio_init ( void ) <nl>  <nl> if ( access ( conf -> devpath_in , R_OK | W_OK ) < 0 || <nl> access ( conf -> devpath_out , R_OK | W_OK ) < 0 ) { <nl> + g_free ( conf ); <nl> return NULL ; <nl> } <nl> return conf ;
static int parse_pci_devfn ( DeviceState * dev , Property * prop , const char * str ) <nl> return - EINVAL ; <nl> if ( fn > 7 ) <nl> return - EINVAL ; <nl> + if ( slot > 31 ) <nl> + return - EINVAL ; <nl> * ptr = slot << 3 | fn ; <nl> return 0 ; <nl> }
void gen_intermediate_code ( CPUAlphaState * env , struct TranslationBlock * tb ) <nl> num_insns ++; <nl>  <nl> if ( unlikely ( cpu_breakpoint_test ( cs , ctx . pc , BP_ANY ))) { <nl> - gen_excp (& ctx , EXCP_DEBUG , 0 ); <nl> + ret = gen_excp (& ctx , EXCP_DEBUG , 0 ); <nl> /* The address covered by the breakpoint must be included in <nl> [ tb -> pc , tb -> pc + tb -> size ) in order to for it to be <nl> properly cleared -- thus we increment the PC here so that
static void vhost_scsi_stop ( VHostSCSI * s ) <nl> VirtioBusClass * k = VIRTIO_BUS_GET_CLASS ( qbus ); <nl> int ret = 0 ; <nl>  <nl> - if (! k -> set_guest_notifiers ) { <nl> + if ( k -> set_guest_notifiers ) { <nl> ret = k -> set_guest_notifiers ( qbus -> parent , s -> dev . nvqs , false ); <nl> if ( ret < 0 ) { <nl> error_report (" vhost guest notifier cleanup failed : % d \ n ", ret );
static void rtas_ibm_os_term ( PowerPCCPU * cpu , <nl> target_ulong args , <nl> uint32_t nret , target_ulong rets ) <nl> { <nl> - target_ulong ret = 0 ; <nl> + qemu_system_guest_panicked ( NULL ); <nl>  <nl> - qapi_event_send_guest_panicked ( GUEST_PANIC_ACTION_PAUSE , false , NULL , <nl> - & error_abort ); <nl> - <nl> - rtas_st ( rets , 0 , ret ); <nl> + rtas_st ( rets , 0 , RTAS_OUT_SUCCESS ); <nl> } <nl>  <nl> static void rtas_set_power_level ( PowerPCCPU * cpu , sPAPRMachineState * spapr ,
extern int ram_size ; <nl> void cpu_reset ( CPUSPARCState * env ) <nl> { <nl> memset ( env , 0 , sizeof (* env )); <nl> + tlb_flush ( env , 1 ); <nl> env -> cwp = 0 ; <nl> env -> wim = 1 ; <nl> env -> regwptr = env -> regbase + ( env -> cwp * 16 );
int qdev_simple_unplug_cb ( DeviceState * dev ) <nl> way is somewhat unclean , and best avoided . */ <nl> void qdev_init_nofail ( DeviceState * dev ) <nl> { <nl> + const char * typename = object_get_typename ( OBJECT ( dev )); <nl> + <nl> if ( qdev_init ( dev ) < 0 ) { <nl> - error_report (" Initialization of device % s failed ", <nl> - object_get_typename ( OBJECT ( dev ))); <nl> + error_report (" Initialization of device % s failed ", typename ); <nl> exit ( 1 ); <nl> } <nl> }
static void qvirtio_scsi_pci_free ( QVirtIOSCSI * vs ) <nl> qvirtio_pci_device_disable ( container_of ( vs -> dev , QVirtioPCIDevice , vdev )); <nl> g_free ( vs -> dev ); <nl> qpci_free_pc ( vs -> bus ); <nl> + g_free ( vs ); <nl> } <nl>  <nl> static uint64_t qvirtio_scsi_alloc ( QVirtIOSCSI * vs , size_t alloc_size ,
Object * container_get ( Object * root , const char * path ) <nl> if (! child ) { <nl> child = object_new (" container "); <nl> object_property_add_child ( obj , parts [ i ], child , NULL ); <nl> + object_unref ( child ); <nl> } <nl> } <nl> 
static void baum_chr_open ( Chardev * chr , <nl> error_setg ( errp , " brlapi__openConnection : % s ", <nl> brlapi_strerror ( brlapi_error_location ())); <nl> g_free ( handle ); <nl> + baum -> brlapi = NULL ; <nl> return ; <nl> } <nl> baum -> deferred_init = 0 ;
static bool is_zero_cluster ( BlockDriverState * bs , int64_t start ) <nl> BlockDriverState * file ; <nl> int64_t res = bdrv_get_block_status_above ( bs , NULL , start , <nl> s -> cluster_sectors , & nr , & file ); <nl> - return res >= 0 && (( res & BDRV_BLOCK_ZERO ) || !( res & BDRV_BLOCK_DATA )); <nl> + return res >= 0 && ( res & BDRV_BLOCK_ZERO ); <nl> } <nl>  <nl> static bool is_zero_cluster_top_locked ( BlockDriverState * bs , int64_t start )
static int kvm_physical_sync_dirty_bitmap ( MemoryRegionSection * section ) <nl>  <nl> d . slot = mem -> slot ; <nl>  <nl> - if ( kvm_vm_ioctl ( s , KVM_GET_DIRTY_LOG , & d ) == - 1 ) { <nl> + if ( kvm_vm_ioctl ( s , KVM_GET_DIRTY_LOG , & d ) < 0 ) { <nl> DPRINTF (" ioctl failed % d \ n ", errno ); <nl> ret = - 1 ; <nl> break ;
m_free ( struct mbuf * m ) <nl> * Either free () it or put it on the free list <nl> */ <nl> if ( m -> m_flags & M_DOFREE ) { <nl> - free ( m ); <nl> m -> slirp -> mbuf_alloced --; <nl> + free ( m ); <nl> } else if (( m -> m_flags & M_FREELIST ) == 0 ) { <nl> insque ( m ,& m -> slirp -> m_freelist ); <nl> m -> m_flags = M_FREELIST ; /* Clobber other flags */
static void spapr_tce_table_class_init ( ObjectClass * klass , void * data ) <nl> dc -> init = spapr_tce_table_realize ; <nl> dc -> reset = spapr_tce_reset ; <nl> dc -> unrealize = spapr_tce_table_unrealize ; <nl> + /* Reason : This is just an internal device for handling the hypercalls */ <nl> + dc -> user_creatable = false ; <nl>  <nl> QLIST_INIT (& spapr_tce_tables ); <nl> 
static void checkpoint ( void ) { <nl> return ; <nl> /* avoid compiler warnings : */ <nl> hexdump ( NULL , 100 ); <nl> - remove_mapping ( vvv , NULL ); <nl> + remove_mapping ( vvv , 0 ); <nl> print_mapping ( NULL ); <nl> print_direntry ( NULL ); <nl> }
void cache_insert ( PageCache * cache , uint64_t addr , uint8_t * pdata ) <nl> /* actual update of entry */ <nl> it = cache_get_by_addr ( cache , addr ); <nl>  <nl> + /* free old cached data if any */ <nl> + g_free ( it -> it_data ); <nl> + <nl> if (! it -> it_data ) { <nl> cache -> num_items ++; <nl> }
static void vga_draw_graphic ( VGACommonState * s , int full_update ) <nl> } else if ( is_buffer_shared ( surface ) && <nl> ( full_update || surface_data ( surface ) != s -> vram_ptr <nl> + ( s -> start_addr * 4 ))) { <nl> - DisplaySurface * surface ; <nl> surface = qemu_create_displaysurface_from ( disp_width , <nl> height , depth , s -> line_offset , <nl> s -> vram_ptr + ( s -> start_addr * 4 ), byteswap );
static int check_directory_consistency ( BDRVVVFATState * s , <nl>  <nl> if ( s -> used_clusters [ cluster_num ] & USED_ANY ) { <nl> fprintf ( stderr , " cluster % d used more than once \ n ", ( int ) cluster_num ); <nl> - return 0 ; <nl> + goto fail ; <nl> } <nl> s -> used_clusters [ cluster_num ] = USED_DIRECTORY ; <nl> 
static const RunStateTransition runstate_transitions_def [] = { <nl>  <nl> { RUN_STATE_PAUSED , RUN_STATE_RUNNING }, <nl> { RUN_STATE_PAUSED , RUN_STATE_FINISH_MIGRATE }, <nl> + { RUN_STATE_PAUSED , RUN_STATE_POSTMIGRATE }, <nl> { RUN_STATE_PAUSED , RUN_STATE_PRELAUNCH }, <nl> { RUN_STATE_PAUSED , RUN_STATE_COLO }, <nl>  <nl> static const RunStateTransition runstate_transitions_def [] = { <nl> { RUN_STATE_PRELAUNCH , RUN_STATE_INMIGRATE }, <nl>  <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_RUNNING }, <nl> + { RUN_STATE_FINISH_MIGRATE , RUN_STATE_PAUSED }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_POSTMIGRATE }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_PRELAUNCH }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_COLO },
static void * file_ram_alloc ( RAMBlock * block , <nl> } <nl>  <nl> /* MAP_POPULATE silently ignores failures */ <nl> - for ( i = 0 ; i < ( memory / hpagesize )- 1 ; i ++) { <nl> + for ( i = 0 ; i < ( memory / hpagesize ); i ++) { <nl> memset ( area + ( hpagesize * i ), 0 , 1 ); <nl> } <nl> 
static BlockMeasureInfo * qcow2_measure ( QemuOpts * opts , BlockDriverState * in_bs , <nl> for ( sector_num = 0 ; <nl> sector_num < ssize / BDRV_SECTOR_SIZE ; <nl> sector_num += pnum ) { <nl> - int nb_sectors = MAX ( ssize / BDRV_SECTOR_SIZE - sector_num , <nl> - INT_MAX ); <nl> + int nb_sectors = MIN ( ssize / BDRV_SECTOR_SIZE - sector_num , <nl> + BDRV_REQUEST_MAX_SECTORS ); <nl> BlockDriverState * file ; <nl> int64_t ret ; <nl> 
static void xio3130_downstream_realize ( PCIDevice * d , Error ** errp ) <nl> pcie_chassis_create ( s -> chassis ); <nl> rc = pcie_chassis_add_slot ( s ); <nl> if ( rc < 0 ) { <nl> + error_setg ( errp , " Can ' t add chassis slot , error % d ", rc ); <nl> goto err_pcie_cap ; <nl> } <nl> 
static void usb_xhci_exit ( PCIDevice * dev ) <nl> /* destroy msix memory region */ <nl> if ( dev -> msix_table && dev -> msix_pba <nl> && dev -> msix_entry_used ) { <nl> - memory_region_del_subregion (& xhci -> mem , & dev -> msix_table_mmio ); <nl> - memory_region_del_subregion (& xhci -> mem , & dev -> msix_pba_mmio ); <nl> + msix_uninit ( dev , & xhci -> mem , & xhci -> mem ); <nl> } <nl>  <nl> usb_bus_release (& xhci -> bus );
BlockDriverAIOCB * laio_submit ( BlockDriverState * bs , void * aio_ctx , int fd , <nl> goto out_dec_count ; <nl> return & laiocb -> common ; <nl>  <nl> - out_free_aiocb : <nl> - qemu_aio_release ( laiocb ); <nl> out_dec_count : <nl> s -> count --; <nl> + out_free_aiocb : <nl> + qemu_aio_release ( laiocb ); <nl> return NULL ; <nl> } <nl> 
void usb_desc_create_serial ( USBDevice * dev ) <nl> } <nl> dst += snprintf ( serial + dst , sizeof ( serial )- dst , "-% s ", dev -> port -> path ); <nl> usb_desc_set_string ( dev , index , serial ); <nl> + g_free ( path ); <nl> } <nl>  <nl> const char * usb_desc_get_string ( USBDevice * dev , uint8_t index )
static USBDevice * usb_try_create_simple ( USBBus * bus , const char * name , <nl> object_property_set_bool ( OBJECT ( dev ), true , " realized ", & err ); <nl> if ( err ) { <nl> error_propagate ( errp , err ); <nl> - error_prepend ( errp , " Failed to initialize USB device '% s ': ", <nl> - name ); <nl> - object_unparent ( OBJECT ( dev )); <nl> + error_prepend ( errp , " Failed to initialize USB device '% s ': ", name ); <nl> return NULL ; <nl> } <nl> return dev ;
int socket_connect ( SocketAddress * addr , Error ** errp , <nl> case SOCKET_ADDRESS_KIND_FD : <nl> fd = monitor_get_fd ( cur_mon , addr -> fd -> str , errp ); <nl> if ( callback ) { <nl> + qemu_set_nonblock ( fd ); <nl> callback ( fd , opaque ); <nl> } <nl> break ;
static void pc_init1 ( ram_addr_t ram_size , <nl> pci_bus = i440fx_init (& i440fx_state , & piix3_devfn , isa_irq , ram_size ); <nl> } else { <nl> pci_bus = NULL ; <nl> + i440fx_state = NULL ; <nl> isa_bus_new ( NULL ); <nl> } <nl> isa_bus_irqs ( isa_irq );
static int qcow2_do_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail ; <nl> } <nl>  <nl> + if ( header . refcount_table_clusters == 0 && !( flags & BDRV_O_CHECK )) { <nl> + error_setg ( errp , " Image does not contain a reference count table "); <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> + } <nl> + <nl> ret = validate_table_offset ( bs , s -> refcount_table_offset , <nl> s -> refcount_table_size , sizeof ( uint64_t )); <nl> if ( ret < 0 ) {
static int gicv3_gicd_no_migration_shift_bug_post_load ( void * opaque , <nl> return 0 ; <nl> } <nl>  <nl> + static bool needed_always ( void * opaque ) <nl> +{ <nl> + return true ; <nl> +} <nl> + <nl> const VMStateDescription vmstate_gicv3_gicd_no_migration_shift_bug = { <nl> . name = " arm_gicv3 / gicd_no_migration_shift_bug ", <nl> . version_id = 1 , <nl> . minimum_version_id = 1 , <nl> + . needed = needed_always , <nl> . pre_load = gicv3_gicd_no_migration_shift_bug_pre_load , <nl> . post_load = gicv3_gicd_no_migration_shift_bug_post_load , <nl> . fields = ( VMStateField []) {
void helper_pmon ( int function ) <nl> break ; <nl> case 158 : <nl> { <nl> - unsigned char * fmt = ( void *)( unsigned long ) env -> active_tc . gpr [ 4 ]; <nl> + unsigned char * fmt = ( void *)( uintptr_t ) env -> active_tc . gpr [ 4 ]; <nl> printf ("% s ", fmt ); <nl> } <nl> break ;
static void pci_host_config_write ( void * opaque , target_phys_addr_t addr , <nl>  <nl> PCI_DPRINTF ("% s addr " TARGET_FMT_plx " len % d val %" PRIx64 "\ n ", <nl> __func__ , addr , len , val ); <nl> + if ( addr != 0 || len != 4 ) { <nl> + return ; <nl> + } <nl> s -> config_reg = val ; <nl> } <nl> 
static int qemu_gluster_parse_json ( BlockdevOptionsGluster * gconf , <nl> Error * local_err = NULL ; <nl> char * str = NULL ; <nl> const char * ptr ; <nl> - size_t num_servers ; <nl> - int i , type ; <nl> + int i , type , num_servers ; <nl>  <nl> /* create opts info from runtime_json_opts list */ <nl> opts = qemu_opts_create (& runtime_json_opts , NULL , 0 , & error_abort );
VirtIODevice * virtio_common_init ( const char * name , uint16_t device_id , <nl> vdev -> queue_sel = 0 ; <nl> vdev -> config_vector = VIRTIO_NO_VECTOR ; <nl> vdev -> vq = qemu_mallocz ( sizeof ( VirtQueue ) * VIRTIO_PCI_QUEUE_MAX ); <nl> + vdev -> vm_running = vm_running ; <nl> for ( i = 0 ; i < VIRTIO_PCI_QUEUE_MAX ; i ++) { <nl> vdev -> vq [ i ]. vector = VIRTIO_NO_VECTOR ; <nl> vdev -> vq [ i ]. vdev = vdev ;
static int qcow2_check ( BlockDriverState * bs , BdrvCheckResult * result , <nl> } <nl>  <nl> if ( fix && result -> check_errors == 0 && result -> corruptions == 0 ) { <nl> - return qcow2_mark_clean ( bs ); <nl> + ret = qcow2_mark_clean ( bs ); <nl> + if ( ret < 0 ) { <nl> + return ret ; <nl> + } <nl> + return qcow2_mark_consistent ( bs ); <nl> } <nl> return ret ; <nl> }
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> trace_qxl_enter_vga_mode ( d -> id ); <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> - memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + dpy_gfx_resize ( d -> ssd . ds ); <nl> vga_dirty_log_start (& d -> vga ); <nl> } <nl> 
static int ehci_init_transfer ( EHCIPacket * p ) <nl> while ( bytes > 0 ) { <nl> if ( cpage > 4 ) { <nl> fprintf ( stderr , " cpage out of range (% d )\ n ", cpage ); <nl> + qemu_sglist_destroy (& p -> sgl ); <nl> return - 1 ; <nl> } <nl> 
static void disas_ldst_reg_imm9 ( DisasContext * s , uint32_t insn ) <nl> } <nl> } else { <nl> TCGv_i64 tcg_rt = cpu_reg ( s , rt ); <nl> - int memidx = is_unpriv ? 1 : get_mem_index ( s ); <nl> + int memidx = is_unpriv ? MMU_USER_IDX : get_mem_index ( s ); <nl>  <nl> if ( is_store ) { <nl> do_gpr_st_memidx ( s , tcg_rt , tcg_addr , size , memidx );
Object * container_get ( Object * root , const char * path ) <nl> } <nl> } <nl>  <nl> + g_strfreev ( parts ); <nl> + <nl> return obj ; <nl> } <nl> 
static int ata_passthrough_12_xfer_size ( SCSIDevice * dev , uint8_t * buf ) <nl> switch ( length ) { <nl> case 0 : <nl> case 3 : /* USB - specific . */ <nl> + default : <nl> xfer = 0 ; <nl> break ; <nl> case 1 : <nl> static int ata_passthrough_16_xfer_size ( SCSIDevice * dev , uint8_t * buf ) <nl> switch ( length ) { <nl> case 0 : <nl> case 3 : /* USB - specific . */ <nl> + default : <nl> xfer = 0 ; <nl> break ; <nl> case 1 :
void qmp_transaction ( TransactionActionList * dev_list , Error ** errp ) <nl> assert ( dev_info -> kind < ARRAY_SIZE ( actions )); <nl>  <nl> ops = & actions [ dev_info -> kind ]; <nl> + assert ( ops -> instance_size > 0 ); <nl> + <nl> state = g_malloc0 ( ops -> instance_size ); <nl> state -> ops = ops ; <nl> state -> action = dev_info ;
static void vnc_init_timer ( VncDisplay * vd ) <nl> vd -> timer_interval = VNC_REFRESH_INTERVAL_BASE ; <nl> if ( vd -> timer == NULL && ! QTAILQ_EMPTY (& vd -> clients )) { <nl> vd -> timer = qemu_new_timer ( rt_clock , vnc_refresh , vd ); <nl> + vnc_dpy_resize ( vd -> ds ); <nl> vnc_refresh ( vd ); <nl> } <nl> }
static inline void cpu_x86_load_seg_cache ( CPUX86State * env , <nl> } <nl>  <nl> static inline void cpu_x86_load_seg_cache_sipi ( X86CPU * cpu , <nl> - int sipi_vector ) <nl> + uint8_t sipi_vector ) <nl> { <nl> CPUState * cs = CPU ( cpu ); <nl> CPUX86State * env = & cpu -> env ;
static int64_t try_fiemap ( BlockDriverState * bs , off_t start , off_t * data , <nl>  <nl> f . fm . fm_start = start ; <nl> f . fm . fm_length = ( int64_t ) nb_sectors * BDRV_SECTOR_SIZE ; <nl> - f . fm . fm_flags = 0 ; <nl> + f . fm . fm_flags = FIEMAP_FLAG_SYNC ; <nl> f . fm . fm_extent_count = 1 ; <nl> f . fm . fm_reserved = 0 ; <nl> if ( ioctl ( s -> fd , FS_IOC_FIEMAP , & f ) == - 1 ) {
static TRBCCode xhci_disable_ep ( XHCIState * xhci , unsigned int slotid , <nl> usb_packet_cleanup (& epctx -> transfers [ i ]. packet ); <nl> } <nl>  <nl> - xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + /* only touch guest RAM if we ' re not resetting the HC */ <nl> + if ( xhci -> dcbaap_low || xhci -> dcbaap_high ) { <nl> + xhci_set_ep_state ( xhci , epctx , NULL , EP_DISABLED ); <nl> + } <nl>  <nl> timer_free ( epctx -> kick_timer ); <nl> g_free ( epctx );
static int disas_coproc_insn ( DisasContext * s , uint32_t insn ) <nl> break ; <nl> } <nl>  <nl> - gen_set_pc_im ( s , s -> pc ); <nl> + gen_set_pc_im ( s , s -> pc - 4 ); <nl> tmpptr = tcg_const_ptr ( ri ); <nl> tcg_syn = tcg_const_i32 ( syndrome ); <nl> gen_helper_access_check_cp_reg ( cpu_env , tmpptr , tcg_syn );
static void spapr_rtc_class_init ( ObjectClass * oc , void * data ) <nl>  <nl> dc -> realize = spapr_rtc_realize ; <nl> dc -> vmsd = & vmstate_spapr_rtc ; <nl> + /* Reason : This is an internal device only for handling the hypercalls */ <nl> + dc -> user_creatable = false ; <nl>  <nl> spapr_rtas_register ( RTAS_GET_TIME_OF_DAY , " get - time - of - day ", <nl> rtas_get_time_of_day );
static int raw_read_options ( QDict * options , BlockDriverState * bs , <nl>  <nl> /* Make sure size is multiple of BDRV_SECTOR_SIZE to prevent rounding <nl> * up and leaking out of the specified area . */ <nl> - if (! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> + if ( s -> has_size && ! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> error_setg ( errp , " Specified size is not multiple of % llu ", <nl> BDRV_SECTOR_SIZE ); <nl> ret = - EINVAL ;
static void v9fs_attach ( void * opaque ) <nl> s -> root_fid = fid ; <nl> /* disable migration */ <nl> error_set (& s -> migration_blocker , QERR_VIRTFS_FEATURE_BLOCKS_MIGRATION , <nl> - s -> ctx . fs_root , s -> tag ); <nl> + s -> ctx . fs_root ? s -> ctx . fs_root : " NULL ", s -> tag ); <nl> migrate_add_blocker ( s -> migration_blocker ); <nl> out : <nl> put_fid ( pdu , fidp );
static void spapr_memory_pre_plug ( HotplugHandler * hotplug_dev , DeviceState * dev , <nl> if ( mem_dev && ! kvmppc_is_mem_backend_page_size_ok ( mem_dev )) { <nl> error_setg ( errp , " Memory backend has bad page size . " <nl> " Use ' memory - backend - file ' with correct mem - path ."); <nl> - return ; <nl> + goto out ; <nl> } <nl> + <nl> + out : <nl> + g_free ( mem_dev ); <nl> } <nl>  <nl> struct sPAPRDIMMState {
bool aio_poll ( AioContext * ctx , bool blocking ) <nl> int count ; <nl> int timeout ; <nl>  <nl> - if ( aio_prepare ( ctx )) { <nl> + have_select_revents = aio_prepare ( ctx ); <nl> + if ( have_select_revents ) { <nl> blocking = false ; <nl> - have_select_revents = true ; <nl> } <nl>  <nl> was_dispatching = ctx -> dispatching ;
static void coroutine_fn mirror_run ( void * opaque ) <nl> } <nl>  <nl> end = s -> common . len >> BDRV_SECTOR_BITS ; <nl> - s -> buf = qemu_blockalign ( bs , s -> buf_size ); <nl> + s -> buf = qemu_try_blockalign ( bs , s -> buf_size ); <nl> + if ( s -> buf == NULL ) { <nl> + ret = - ENOMEM ; <nl> + goto immediate_exit ; <nl> + } <nl> + <nl> sectors_per_chunk = s -> granularity >> BDRV_SECTOR_BITS ; <nl> mirror_free_init ( s ); <nl> 
int kvm_arch_handle_exit ( CPUPPCState * env , struct kvm_run * run ) <nl> dprintf (" handle PAPR hypercall \ n "); <nl> run -> papr_hcall . ret = spapr_hypercall ( env , run -> papr_hcall . nr , <nl> run -> papr_hcall . args ); <nl> - ret = 1 ; <nl> + ret = 0 ; <nl> break ; <nl> # endif <nl> default :
static void set_pci_devfn ( Object * obj , Visitor * v , void * opaque , <nl>  <nl> visit_type_str ( v , & str , name , & local_err ); <nl> if ( local_err ) { <nl> + error_free ( local_err ); <nl> return set_int32 ( obj , v , opaque , name , errp ); <nl> } <nl> 
static void handle_windowevent ( SDL_Event * ev ) <nl> { <nl> struct sdl2_console * scon = get_scon_from_window ( ev -> window . windowID ); <nl>  <nl> + if (! scon ) { <nl> + return ; <nl> + } <nl> + <nl> switch ( ev -> window . event ) { <nl> case SDL_WINDOWEVENT_RESIZED : <nl> {
static void usbredir_handle_destroy ( USBDevice * udev ) <nl> USBRedirDevice * dev = DO_UPCAST ( USBRedirDevice , dev , udev ); <nl>  <nl> qemu_chr_delete ( dev -> cs ); <nl> + dev -> cs = NULL ; <nl> /* Note must be done after qemu_chr_close , as that causes a close event */ <nl> qemu_bh_delete ( dev -> chardev_close_bh ); <nl> 
static void report_unavailable_features ( FeatureWord w , uint32_t mask ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < 32 ; ++ i ) { <nl> - if ( 1 << i & mask ) { <nl> + if (( 1UL << i ) & mask ) { <nl> const char * reg = get_register_name_32 ( f -> cpuid_reg ); <nl> assert ( reg ); <nl> fprintf ( stderr , " warning : % s doesn ' t support requested feature : "
typedef struct USBNetState { <nl>  <nl> static int is_rndis ( USBNetState * s ) <nl> { <nl> - return s -> dev . config -> bConfigurationValue == DEV_RNDIS_CONFIG_VALUE ; <nl> + return s -> dev . config ? <nl> + s -> dev . config -> bConfigurationValue == DEV_RNDIS_CONFIG_VALUE : 0 ; <nl> } <nl>  <nl> static int ndis_query ( USBNetState * s , uint32_t oid ,
int kvm_init_vcpu ( CPUState * env ) <nl>  <nl> env -> kvm_fd = ret ; <nl> env -> kvm_state = s ; <nl> + env -> kvm_vcpu_dirty = 1 ; <nl>  <nl> mmap_size = kvm_ioctl ( s , KVM_GET_VCPU_MMAP_SIZE , 0 ); <nl> if ( mmap_size < 0 ) {
static int vmdk_write ( BlockDriverState * bs , int64_t sector_num , <nl> { <nl> BDRVVmdkState * s = bs -> opaque ; <nl> VmdkExtent * extent = NULL ; <nl> - int n , ret ; <nl> - int64_t index_in_cluster ; <nl> + int ret ; <nl> + int64_t index_in_cluster , n ; <nl> uint64_t extent_begin_sector , extent_relative_sector_num ; <nl> uint64_t cluster_offset ; <nl> VmdkMetaData m_data ;
static int coroutine_fn bdrv_mirror_top_pdiscard ( BlockDriverState * bs , <nl>  <nl> static void bdrv_mirror_top_refresh_filename ( BlockDriverState * bs , QDict * opts ) <nl> { <nl> + if ( bs -> backing == NULL ) { <nl> + /* we can be here after failed bdrv_attach_child in <nl> + * bdrv_set_backing_hd */ <nl> + return ; <nl> + } <nl> bdrv_refresh_filename ( bs -> backing -> bs ); <nl> pstrcpy ( bs -> exact_filename , sizeof ( bs -> exact_filename ), <nl> bs -> backing -> bs -> filename );
static int pty_chr_write ( CharDriverState * chr , const uint8_t * buf , int len ) <nl> if (! s -> connected ) { <nl> /* guest sends data , check for ( re -) connect */ <nl> pty_chr_update_read_handler_locked ( chr ); <nl> - return 0 ; <nl> + if (! s -> connected ) { <nl> + return 0 ; <nl> + } <nl> } <nl> return io_channel_send ( s -> fd , buf , len ); <nl> }
static int raw_create ( const char * filename , QemuOpts * opts , Error ** errp ) <nl> goto out ; <nl> } <nl>  <nl> - fd = qemu_open ( filename , O_WRONLY | O_CREAT | O_TRUNC | O_BINARY , <nl> + fd = qemu_open ( filename , O_RDWR | O_CREAT | O_TRUNC | O_BINARY , <nl> 0644 ); <nl> if ( fd < 0 ) { <nl> result = - errno ;
udp_input ( register struct mbuf * m , int iphlen ) <nl> * Locate pcb for datagram . <nl> */ <nl> so = slirp -> udp_last_so ; <nl> - if ( so -> so_lport != uh -> uh_sport || <nl> + if ( so == & slirp -> udb || so -> so_lport != uh -> uh_sport || <nl> so -> so_laddr . s_addr != ip -> ip_src . s_addr ) { <nl> struct socket * tmp ; <nl> 
static void pc_fw_add_pflash_drv ( void ) <nl> bios_name = BIOS_FILENAME ; <nl> } <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> + if (! filename ) { <nl> + error_report (" Can ' t open BIOS image % s ", bios_name ); <nl> + exit ( 1 ); <nl> + } <nl>  <nl> opts = drive_add ( IF_PFLASH , - 1 , filename , " readonly = on "); <nl> 
static int check_directory_consistency ( BDRVVVFATState * s , <nl>  <nl> long_file_name lfn ; <nl> int path_len = strlen ( path ); <nl> - char path2 [ PATH_MAX ]; <nl> + char path2 [ PATH_MAX + 1 ]; <nl>  <nl> assert ( path_len < PATH_MAX ); /* len was tested before ! */ <nl> pstrcpy ( path2 , sizeof ( path2 ), path );
static int spapr_fixup_cpu_smt_dt ( void * fdt , int offset , PowerPCCPU * cpu , <nl> int index = ppc_get_vcpu_dt_id ( cpu ); <nl>  <nl> if ( cpu -> cpu_version ) { <nl> - ret = fdt_setprop ( fdt , offset , " cpu - version ", <nl> - & cpu -> cpu_version , sizeof ( cpu -> cpu_version )); <nl> + ret = fdt_setprop_cell ( fdt , offset , " cpu - version ", cpu -> cpu_version ); <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
FWCfgState * pc_memory_init ( MachineState * machine , <nl> exit ( EXIT_FAILURE ); <nl> } <nl>  <nl> + if ( QEMU_ALIGN_UP ( machine -> maxram_size , <nl> + TARGET_PAGE_SIZE ) != machine -> maxram_size ) { <nl> + error_report (" maximum memory size must by aligned to multiple of " <nl> + "% d bytes ", TARGET_PAGE_SIZE ); <nl> + exit ( EXIT_FAILURE ); <nl> + } <nl> + <nl> pcms -> hotplug_memory_base = <nl> ROUND_UP ( 0x100000000ULL + above_4g_mem_size , 1ULL << 30 ); <nl> 
static inline void gen_op_arith_compute_ov ( DisasContext * ctx , TCGv arg0 , <nl> if ( is_isa300 ( ctx )) { <nl> tcg_gen_extract_tl ( cpu_ov32 , cpu_ov , 31 , 1 ); <nl> } <nl> - tcg_gen_extract_tl ( cpu_ov , cpu_ov , 63 , 1 ); <nl> + tcg_gen_extract_tl ( cpu_ov , cpu_ov , TARGET_LONG_BITS - 1 , 1 ); <nl> } <nl> tcg_gen_or_tl ( cpu_so , cpu_so , cpu_ov ); <nl> }
static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> ret = - EINVAL ; <nl> goto fail ; <nl> } <nl> - if ( s -> max_table_entries > ( VHD_MAX_SECTORS * 512 ) / s -> block_size ) { <nl> - ret = - EINVAL ; <nl> - goto fail ; <nl> - } <nl>  <nl> computed_size = ( uint64_t ) s -> max_table_entries * s -> block_size ; <nl> if ( computed_size < bs -> total_sectors * 512 ) {
ip_input ( struct mbuf * m ) <nl> DEBUG_ARG (" m_len = % d ", m -> m_len ); <nl>  <nl> if ( m -> m_len < sizeof ( struct ip )) { <nl> - return ; <nl> + goto bad ; <nl> } <nl>  <nl> ip = mtod ( m , struct ip *);
static void vfio_map_bar ( VFIOPCIDevice * vdev , int nr ) <nl> if ( vdev -> msix && vdev -> msix -> table_bar == nr ) { <nl> uint64_t start ; <nl>  <nl> - start = HOST_PAGE_ALIGN ( vdev -> msix -> table_offset + <nl> + start = HOST_PAGE_ALIGN (( uint64_t ) vdev -> msix -> table_offset + <nl> ( vdev -> msix -> entries * PCI_MSIX_ENTRY_SIZE )); <nl>  <nl> size = start < bar -> region . size ? bar -> region . size - start : 0 ;
fail : <nl> /* refcount checking functions */ <nl>  <nl>  <nl> - static size_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> + static uint64_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> { <nl> /* This assertion holds because there is no way we can address more than <nl> * 2 ^( 64 - 9 ) clusters at once ( with cluster size 512 = 2 ^ 9 , and because
static int vmdk_open_vmfs_sparse ( BlockDriverState * bs , <nl> } <nl> ret = vmdk_add_extent ( bs , file , false , <nl> le32_to_cpu ( header . disk_sectors ), <nl> - le32_to_cpu ( header . l1dir_offset ) << 9 , <nl> + ( int64_t ) le32_to_cpu ( header . l1dir_offset ) << 9 , <nl> 0 , <nl> le32_to_cpu ( header . l1dir_size ), <nl> 4096 ,
static int scsi_req_length ( SCSICommand * cmd , SCSIDevice * dev , uint8_t * buf ) <nl> case VERIFY_16 : <nl> if (( buf [ 1 ] & 2 ) == 0 ) { <nl> cmd -> xfer = 0 ; <nl> - } else if (( buf [ 1 ] & 4 ) == 1 ) { <nl> + } else if (( buf [ 1 ] & 4 ) != 0 ) { <nl> cmd -> xfer = 1 ; <nl> } <nl> cmd -> xfer *= dev -> blocksize ;
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> void * obj ; <nl> int i , j ; <nl>  <nl> + if (! object_dynamic_cast ( qdev_get_machine (), TYPE_SPAPR_MACHINE )) { <nl> + error_setg ( errp , " spapr - cpu - core needs a pseries machine "); <nl> + return ; <nl> + } <nl> + <nl> sc -> threads = g_malloc0 ( size * cc -> nr_threads ); <nl> for ( i = 0 ; i < cc -> nr_threads ; i ++) { <nl> char id [ 32 ];
extern const PropertyInfo qdev_prop_link ; <nl> _arrayfield , _arrayprop , _arraytype ) { \ <nl> . name = ( PROP_ARRAY_LEN_PREFIX _name ), \ <nl> . info = &( qdev_prop_arraylen ), \ <nl> + . defval . u = 0 , \ <nl> . offset = offsetof ( _state , _field ) \ <nl> + type_check ( uint32_t , typeof_field ( _state , _field )), \ <nl> . arrayinfo = &( _arrayprop ), \
found : <nl> QTAILQ_REMOVE (& s -> discards , p , next ); <nl> d -> offset = MIN ( d -> offset , p -> offset ); <nl> d -> bytes += p -> bytes ; <nl> + g_free ( p ); <nl> } <nl> } <nl> 
static void coroutine_fn verify_entered_step_2 ( void * opaque ) <nl> /* Once more to check it still works after yielding */ <nl> g_assert ( qemu_coroutine_entered ( caller )); <nl> g_assert ( qemu_coroutine_entered ( qemu_coroutine_self ())); <nl> - qemu_coroutine_yield (); <nl> } <nl>  <nl> static void coroutine_fn verify_entered_step_1 ( void * opaque )
static inline void cpu_physical_memory_set_dirty_lebitmap ( unsigned long * bitmap , <nl> unsigned long page = BIT_WORD ( start >> TARGET_PAGE_BITS ); <nl>  <nl> /* start address is aligned at the start of a word ? */ <nl> - if ((( page * BITS_PER_LONG ) << TARGET_PAGE_BITS ) == start ) { <nl> + if (((( page * BITS_PER_LONG ) << TARGET_PAGE_BITS ) == start ) && <nl> + ( hpratio == 1 )) { <nl> long k ; <nl> long nr = BITS_TO_LONGS ( pages ); <nl> 
static int iscsi_truncate ( BlockDriverState * bs , int64_t offset ) <nl> if ( iscsilun -> allocationmap != NULL ) { <nl> g_free ( iscsilun -> allocationmap ); <nl> iscsilun -> allocationmap = <nl> - bitmap_new ( DIV_ROUND_UP ( bs -> total_sectors , <nl> + bitmap_new ( DIV_ROUND_UP ( sector_lun2qemu ( iscsilun -> num_blocks , <nl> + iscsilun ), <nl> iscsilun -> cluster_sectors )); <nl> } <nl> 
static void update_irq ( struct xlx_pic * p ) <nl>  <nl> /* Update the vector register . */ <nl> for ( i = 0 ; i < 32 ; i ++) { <nl> - if ( p -> regs [ R_IPR ] & ( 1 << i )) <nl> + if ( p -> regs [ R_IPR ] & ( 1U << i )) { <nl> break ; <nl> + } <nl> } <nl> if ( i == 32 ) <nl> i = ~ 0 ;
static void write_bootloader ( uint8_t * base , int64_t run_addr , <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> stl_p ( p ++, 0x0ff0021c ); /* jal 870 */ <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> - stl_p ( p ++, 0x08000205 ); /* j 814 */ <nl> + stl_p ( p ++, 0x1000fff9 ); /* b 814 */ <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> stl_p ( p ++, 0x01a00009 ); /* jalr t5 */ <nl> stl_p ( p ++, 0x01602021 ); /* move a0 , t3 */
static void co_read_response ( void * opaque ) <nl> s -> co_recv = qemu_coroutine_create ( aio_read_response , opaque ); <nl> } <nl>  <nl> - aio_co_wake ( s -> co_recv ); <nl> + aio_co_enter ( s -> aio_context , s -> co_recv ); <nl> } <nl>  <nl> static void co_write_request ( void * opaque )
struct target_sigcontext { <nl> /* A Sparc stack frame */ <nl> struct sparc_stackf { <nl> abi_ulong locals [ 8 ]; <nl> - abi_ulong ins [ 6 ]; <nl> - struct sparc_stackf * fp ; <nl> - abi_ulong callers_pc ; <nl> + abi_ulong ins [ 8 ]; <nl> + /* It ' s simpler to treat fp and callers_pc as elements of ins [] <nl> + * since we never need to access them ourselves . <nl> + */ <nl> char * structptr ; <nl> abi_ulong xargs [ 6 ]; <nl> abi_ulong xxargs [ 1 ];
int main ( int argc , char ** argv ) <nl> return 0 ; <nl> } <nl> argv += optind ; <nl> - optind = 1 ; <nl> + optind = 0 ; <nl>  <nl> if (! trace_init_backends ()) { <nl> exit ( 1 );
static int bdrv_rw_co ( BlockDriverState * bs , int64_t sector_num , uint8_t * buf , <nl> . iov_len = nb_sectors * BDRV_SECTOR_SIZE , <nl> }; <nl>  <nl> + if ( nb_sectors < 0 || nb_sectors > INT_MAX / BDRV_SECTOR_SIZE ) { <nl> + return - EINVAL ; <nl> + } <nl> + <nl> qemu_iovec_init_external (& qiov , & iov , 1 ); <nl> return bdrv_prwv_co ( bs , sector_num << BDRV_SECTOR_BITS , <nl> & qiov , is_write , flags );
static coroutine_fn void nbd_read_reply_entry ( void * opaque ) <nl> { <nl> NBDClientSession * s = opaque ; <nl> uint64_t i ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> Error * local_err = NULL ; <nl>  <nl> while (! s -> quit ) {
static int usb_msd_handle_control ( USBDevice * dev , USBPacket * p , <nl> static void usb_msd_cancel_io ( USBDevice * dev , USBPacket * p ) <nl> { <nl> MSDState * s = DO_UPCAST ( MSDState , dev , dev ); <nl> - scsi_req_cancel ( s -> req ); <nl> + <nl> + if ( s -> req ) { <nl> + scsi_req_cancel ( s -> req ); <nl> + } <nl> } <nl>  <nl> static int usb_msd_handle_data ( USBDevice * dev , USBPacket * p )
static int kvm_set_user_memory_region ( KVMState * s , KVMSlot * slot ) <nl> if ( s -> migration_log ) { <nl> mem . flags |= KVM_MEM_LOG_DIRTY_PAGES ; <nl> } <nl> - if ( mem . flags & KVM_MEM_READONLY ) { <nl> + <nl> + if ( slot -> memory_size && mem . flags & KVM_MEM_READONLY ) { <nl> /* Set the slot size to 0 before setting the slot to the desired <nl> * value . This is needed based on KVM commit 75d61fbc . */ <nl> mem . memory_size = 0 ;
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + vga_dirty_log_start (& d -> vga ); <nl> } <nl>  <nl> static void qxl_exit_vga_mode ( PCIQXLDevice * d ) <nl> static void qxl_exit_vga_mode ( PCIQXLDevice * d ) <nl> return ; <nl> } <nl> trace_qxl_exit_vga_mode ( d -> id ); <nl> + vga_dirty_log_stop (& d -> vga ); <nl> qxl_destroy_primary ( d , QXL_SYNC ); <nl> } <nl> 
struct ccw1 { <nl> __u8 flags ; <nl> __u16 count ; <nl> __u32 cda ; <nl> -} __attribute__ (( packed )); <nl> +} __attribute__ (( packed , aligned ( 8 ))); <nl>  <nl> # define CCW_FLAG_DC 0x80 <nl> # define CCW_FLAG_CC 0x40
static ram_addr_t qxl_rom_size ( void ) <nl> sizeof ( qxl_modes ); <nl> uint32_t rom_size = 8192 ; /* two pages */ <nl>  <nl> - required_rom_size = MAX ( required_rom_size , TARGET_PAGE_SIZE ); <nl> - required_rom_size = msb_mask ( required_rom_size * 2 - 1 ); <nl> - assert ( required_rom_size <= rom_size ); <nl> + QEMU_BUILD_BUG_ON ( required_rom_size > rom_size ); <nl> return rom_size ; <nl> } <nl> 
static void pty_chr_state ( CharDriverState * chr , int connected ) <nl> s -> timer_tag = 0 ; <nl> } <nl> if (! s -> connected ) { <nl> - qemu_chr_be_generic_open ( chr ); <nl> s -> connected = 1 ; <nl> + qemu_chr_be_generic_open ( chr ); <nl> s -> fd_tag = io_add_watch_poll ( s -> fd , pty_chr_read_poll , pty_chr_read , chr ); <nl> } <nl> }
float64 HELPER ( recpe_f64 )( float64 input , void * fpstp ) <nl> } else { <nl> return float64_set_sign ( float64_maxnorm , float64_is_neg ( f64 )); <nl> } <nl> - } else if ( f64_exp >= 1023 && fpst -> flush_to_zero ) { <nl> + } else if ( f64_exp >= 2045 && fpst -> flush_to_zero ) { <nl> float_raise ( float_flag_underflow , fpst ); <nl> return float64_set_sign ( float64_zero , float64_is_neg ( f64 )); <nl> }
void qemu_input_event_send_key ( QemuConsole * src , KeyValue * key , bool down ) <nl> } else if ( queue_count < queue_limit ) { <nl> qemu_input_queue_event (& kbd_queue , src , evt ); <nl> qemu_input_queue_sync (& kbd_queue ); <nl> + } else { <nl> + qapi_free_InputEvent ( evt ); <nl> } <nl> } <nl> 
S390CPU * s390x_new_cpu ( const char * typename , uint32_t core_id , Error ** errp ) <nl> object_property_set_bool ( OBJECT ( cpu ), true , " realized ", & err ); <nl>  <nl> out : <nl> + object_unref ( OBJECT ( cpu )); <nl> if ( err ) { <nl> error_propagate ( errp , err ); <nl> - object_unref ( OBJECT ( cpu )); <nl> cpu = NULL ; <nl> } <nl> return cpu ;
static void pc_isa_bios_init ( MemoryRegion * rom_memory , <nl> flash_size = memory_region_size ( flash_mem ); <nl>  <nl> /* map the last 128KB of the BIOS in ISA space */ <nl> - isa_bios_size = flash_size ; <nl> - if ( isa_bios_size > ( 128 * 1024 )) { <nl> - isa_bios_size = 128 * 1024 ; <nl> - } <nl> + isa_bios_size = MIN ( flash_size , 128 * 1024 ); <nl> isa_bios = g_malloc ( sizeof (* isa_bios )); <nl> memory_region_init_ram ( isa_bios , NULL , " isa - bios ", isa_bios_size ); <nl> vmstate_register_ram_global ( isa_bios );
typedef struct CPUX86State { <nl> uint8_t has_error_code ; <nl> uint32_t sipi_vector ; <nl> bool tsc_valid ; <nl> - int tsc_khz ; <nl> + int64_t tsc_khz ; <nl> void * kvm_xsave_buf ; <nl>  <nl> uint64_t mcg_cap ;
fail : <nl> qemu_co_mutex_unlock (& s -> lock ); <nl>  <nl> qemu_iovec_destroy (& hd_qiov ); <nl> + g_free ( cluster_data ); <nl>  <nl> return ret ; <nl> } <nl> fail : <nl> qemu_co_mutex_unlock (& s -> lock ); <nl>  <nl> qemu_iovec_destroy (& hd_qiov ); <nl> + g_free ( cluster_data ); <nl>  <nl> return ret ; <nl> }
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> { <nl> DPRINTF ("+++ C + mode offloaded task checksum \ n "); <nl>  <nl> + /* Large enough for Ethernet and IP headers ? */ <nl> + if ( saved_size < ETH_HLEN + sizeof ( ip_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ip packet header */ <nl> ip_header * ip = NULL ; <nl> int hlen = 0 ;
static BlockDriverAIOCB * bdrv_aio_rw_vector ( BlockDriverState * bs , <nl> acb -> is_write = is_write ; <nl> acb -> qiov = qiov ; <nl> acb -> bounce = qemu_blockalign ( bs , qiov -> size ); <nl> - <nl> - if (! acb -> bh ) <nl> - acb -> bh = qemu_bh_new ( bdrv_aio_bh_cb , acb ); <nl> + acb -> bh = qemu_bh_new ( bdrv_aio_bh_cb , acb ); <nl>  <nl> if ( is_write ) { <nl> qemu_iovec_to_buffer ( acb -> qiov , acb -> bounce );
typedef struct MirrorBlockJob { <nl>  <nl> unsigned long * in_flight_bitmap ; <nl> int in_flight ; <nl> - int sectors_in_flight ; <nl> + int64_t sectors_in_flight ; <nl> int ret ; <nl> bool unmap ; <nl> bool waiting_for_io ;
static int apic_init_common ( SysBusDevice * dev ) <nl>  <nl> sysbus_init_mmio ( dev , & s -> io_memory ); <nl>  <nl> - if (! vapic && s -> vapic_control & VAPIC_ENABLE_MASK ) { <nl> + /* Note : We need at least 1M to map the VAPIC option ROM */ <nl> + if (! vapic && s -> vapic_control & VAPIC_ENABLE_MASK && <nl> + ram_size >= 1024 * 1024 ) { <nl> vapic = sysbus_create_simple (" kvmvapic ", - 1 , NULL ); <nl> } <nl> s -> vapic = vapic ;
static void xlnx_ep108_init ( MachineState * machine ) <nl> machine -> ram_size = EP108_MAX_RAM_SIZE ; <nl> } <nl>  <nl> - if ( machine -> ram_size <= 0x08000000 ) { <nl> + if ( machine -> ram_size < 0x08000000 ) { <nl> qemu_log (" WARNING : RAM size " RAM_ADDR_FMT " is small for EP108 ", <nl> machine -> ram_size ); <nl> }
static target_ulong h_client_architecture_support ( PowerPCCPU * cpu , <nl> error_report_err ( local_err ); <nl> return H_HARDWARE ; <nl> } <nl> + error_free ( local_err ); <nl> local_err = NULL ; <nl> } <nl> }
e1000_link_up ( E1000State * s ) <nl> { <nl> s -> mac_reg [ STATUS ] |= E1000_STATUS_LU ; <nl> s -> phy_reg [ PHY_STATUS ] |= MII_SR_LINK_STATUS ; <nl> + <nl> + /* E1000_STATUS_LU is tested by e1000_can_receive () */ <nl> + qemu_flush_queued_packets ( qemu_get_queue ( s -> nic )); <nl> } <nl>  <nl> static bool
CharDriverState * qemu_chr_new_from_opts ( QemuOpts * opts , <nl> if ( i == NULL ) { <nl> error_setg ( errp , " chardev : backend \"% s \" not found ", <nl> qemu_opt_get ( opts , " backend ")); <nl> - return NULL ; <nl> + goto err ; <nl> } <nl>  <nl> if (! cd -> open ) {
ObjectClass * object_class_dynamic_cast ( ObjectClass * class , <nl> TypeImpl * type = class -> type ; <nl> ObjectClass * ret = NULL ; <nl>  <nl> + if (! target_type ) { <nl> + /* target class type unknown , so fail the cast */ <nl> + return NULL ; <nl> + } <nl> + <nl> if ( type -> class -> interfaces && <nl> type_is_ancestor ( target_type , type_interface )) { <nl> int found = 0 ;
static void ccid_card_vscard_handle_message ( PassthruState * card , <nl> error_report (" ATR size exceeds spec , ignoring "); <nl> ccid_card_vscard_send_error ( card , scr_msg_header -> reader_id , <nl> VSC_GENERAL_ERROR ); <nl> + break ; <nl> } <nl> memcpy ( card -> atr , data , scr_msg_header -> length ); <nl> card -> atr_length = scr_msg_header -> length ;
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> - if ( chdir ("/") < 0 ) { <nl> - do_perror (" chdir "); <nl> - goto error ; <nl> - } <nl> if ( chroot ( rpath ) < 0 ) { <nl> do_perror (" chroot "); <nl> goto error ; <nl> } <nl> + if ( chdir ("/") < 0 ) { <nl> + do_perror (" chdir "); <nl> + goto error ; <nl> + } <nl>  <nl> get_version = false ; <nl> # ifdef FS_IOC_GETVERSION
static int img_amend ( int argc , char ** argv ) <nl> if (! is_valid_option_list ( optarg )) { <nl> error_report (" Invalid option list : % s ", optarg ); <nl> ret = - 1 ; <nl> - goto out ; <nl> + goto out_no_progress ; <nl> } <nl> if (! options ) { <nl> options = g_strdup ( optarg ); <nl> static int img_amend ( int argc , char ** argv ) <nl> out : <nl> qemu_progress_end (); <nl>  <nl> + out_no_progress : <nl> blk_unref ( blk ); <nl> qemu_opts_del ( opts ); <nl> qemu_opts_free ( create_opts );
static void QEMU_NORETURN force_sig ( int target_sig ) <nl> * it to arrive . */ <nl> sigfillset (& act . sa_mask ); <nl> act . sa_handler = SIG_DFL ; <nl> + act . sa_flags = 0 ; <nl> sigaction ( host_sig , & act , NULL ); <nl>  <nl> /* For some reason raise ( host_sig ) doesn ' t send the signal when
static void disas_arm_insn ( DisasContext * s , unsigned int insn ) <nl> ARCH ( 6T2 ); <nl> shift = ( insn >> 7 ) & 0x1f ; <nl> i = ( insn >> 16 ) & 0x1f ; <nl> + if ( i < shift ) { <nl> + /* UNPREDICTABLE ; we choose to UNDEF */ <nl> + goto illegal_op ; <nl> + } <nl> i = i + 1 - shift ; <nl> if ( rm == 15 ) { <nl> tmp = tcg_temp_new_i32 ();
static void virtqueue_map_desc ( unsigned int * p_num_sg , hwaddr * addr , struct iove <nl> } <nl>  <nl> iov [ num_sg ]. iov_base = cpu_physical_memory_map ( pa , & len , is_write ); <nl> + if (! iov [ num_sg ]. iov_base ) { <nl> + error_report (" virtio : bogus descriptor or out of resources "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> iov [ num_sg ]. iov_len = len ; <nl> addr [ num_sg ] = pa ; <nl> 
static inline void gen_neon_mull ( TCGv_i64 dest , TCGv a , TCGv b , int size , int u ) <nl> case 4 : <nl> tmp = gen_muls_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> case 5 : <nl> tmp = gen_mulu_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> default : abort (); <nl> }
static int qxl_track_command ( PCIQXLDevice * qxl , struct QXLCommandExt * ext ) <nl> qxl -> guest_cursor = ext -> cmd . data ; <nl> qemu_mutex_unlock (& qxl -> track_lock ); <nl> } <nl> + if ( cmd -> type == QXL_CURSOR_HIDE ) { <nl> + qemu_mutex_lock (& qxl -> track_lock ); <nl> + qxl -> guest_cursor = 0 ; <nl> + qemu_mutex_unlock (& qxl -> track_lock ); <nl> + } <nl> break ; <nl> } <nl> }
const char * bdrv_get_device_name ( BlockDriverState * bs ) <nl>  <nl> void bdrv_flush ( BlockDriverState * bs ) <nl> { <nl> + if (! bs -> drv ) <nl> + return ; <nl> if ( bs -> drv -> bdrv_flush ) <nl> bs -> drv -> bdrv_flush ( bs ); <nl> if ( bs -> backing_hd )
void ahci_uninit ( AHCIState * s ) <nl>  <nl> ide_exit ( s ); <nl> } <nl> + object_unparent ( OBJECT (& ad -> port )); <nl> } <nl>  <nl> g_free ( s -> dev );
static void gen_pusha ( DisasContext * s ) <nl> { <nl> int i ; <nl> gen_op_movl_A0_reg ( R_ESP ); <nl> - gen_op_addl_A0_im (- 8 << s -> dflag ); <nl> + gen_op_addl_A0_im (-( 8 << s -> dflag )); <nl> if (! s -> ss32 ) <nl> tcg_gen_ext16u_tl ( cpu_A0 , cpu_A0 ); <nl> tcg_gen_mov_tl ( cpu_T [ 1 ], cpu_A0 );
static inline void acpi_build_tables_init ( AcpiBuildTables * tables ) <nl> static inline void acpi_build_tables_cleanup ( AcpiBuildTables * tables , bool mfre ) <nl> { <nl> void * linker_data = bios_linker_loader_cleanup ( tables -> linker ); <nl> - if ( mfre ) { <nl> - g_free ( linker_data ); <nl> - } <nl> + g_free ( linker_data ); <nl> g_array_free ( tables -> rsdp , mfre ); <nl> - g_array_free ( tables -> table_data , mfre ); <nl> + g_array_free ( tables -> table_data , true ); <nl> g_array_free ( tables -> tcpalog , mfre ); <nl> } <nl> 
int main ( int argc , char * argv []) <nl> const char * arch = qtest_get_arch (); <nl> FILE * f = fopen ( disk , " w "); <nl> int ret ; <nl> + <nl> + if (! f ) { <nl> + fprintf ( stderr , " Couldn ' t open \"% s \": % s ", disk , strerror ( errno )); <nl> + return 1 ; <nl> + } <nl> fwrite ( boot_sector , 1 , sizeof boot_sector , f ); <nl> fclose ( f ); <nl> 
static int do_readlink ( struct iovec * iovec , struct iovec * out_iovec ) <nl> } <nl> buffer = g_malloc ( size ); <nl> v9fs_string_init (& target ); <nl> - retval = readlink ( path . data , buffer , size ); <nl> + retval = readlink ( path . data , buffer , size - 1 ); <nl> if ( retval > 0 ) { <nl> buffer [ retval ] = '\ 0 '; <nl> v9fs_string_sprintf (& target , "% s ", buffer );
int kvm_arch_release_virq_post ( int virq ) <nl> if ( entry -> virq == virq ) { <nl> trace_kvm_x86_remove_msi_route ( virq ); <nl> QLIST_REMOVE ( entry , list ); <nl> + g_free ( entry ); <nl> break ; <nl> } <nl> }
static int execute_command ( BlockDriverState * bdrv , <nl> r -> io_header . flags |= SG_FLAG_DIRECT_IO ; <nl>  <nl> r -> req . aiocb = bdrv_aio_ioctl ( bdrv , SG_IO , & r -> io_header , complete , r ); <nl> + if ( r -> req . aiocb == NULL ) { <nl> + return - EIO ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static void nvdimm_dsm_set_label_data ( NVDIMMDevice * nvdimm , NvdimmDsmIn * in , <nl> return ; <nl> } <nl>  <nl> - assert ( sizeof (* in ) + sizeof (* set_label_data ) + set_label_data -> length <= <nl> - 4096 ); <nl> + assert ( offsetof ( NvdimmDsmIn , arg3 ) + <nl> + sizeof (* set_label_data ) + set_label_data -> length <= 4096 ); <nl>  <nl> nvc -> write_label_data ( nvdimm , set_label_data -> in_buf , <nl> set_label_data -> length , set_label_data -> offset );
static void become_daemon ( const char * pidfile ) <nl> goto fail ; <nl> } <nl>  <nl> - close ( STDIN_FILENO ); <nl> - close ( STDOUT_FILENO ); <nl> - close ( STDERR_FILENO ); <nl> + reopen_fd_to_null ( STDIN_FILENO ); <nl> + reopen_fd_to_null ( STDOUT_FILENO ); <nl> + reopen_fd_to_null ( STDERR_FILENO ); <nl> return ; <nl>  <nl> fail :
static unsigned hpte_page_shift ( const struct ppc_one_seg_page_size * sps , <nl>  <nl> mask = (( 1ULL << ps -> page_shift ) - 1 ) & HPTE64_R_RPN ; <nl>  <nl> - if (( pte1 & mask ) == ( ps -> pte_enc << HPTE64_R_RPN_SHIFT )) { <nl> + if (( pte1 & mask ) == (( uint64_t ) ps -> pte_enc << HPTE64_R_RPN_SHIFT )) { <nl> return ps -> page_shift ; <nl> } <nl> }
void framebuffer_update_display ( <nl>  <nl> i = * first_row ; <nl> * first_row = - 1 ; <nl> - src_len = src_width * rows ; <nl> + src_len = ( hwaddr ) src_width * rows ; <nl>  <nl> mem = mem_section -> mr ; <nl> if (! mem ) {
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl>  <nl> id_str_size = strlen ( sn -> id_str ); <nl> name_size = strlen ( sn -> name ); <nl> + assert ( id_str_size <= UINT16_MAX && name_size <= UINT16_MAX ); <nl> h . id_str_size = cpu_to_be16 ( id_str_size ); <nl> h . name_size = cpu_to_be16 ( name_size ); <nl> offset = align_offset ( offset , 8 );
static int qio_channel_buffer_close ( QIOChannel * ioc , <nl> QIOChannelBuffer * bioc = QIO_CHANNEL_BUFFER ( ioc ); <nl>  <nl> g_free ( bioc -> data ); <nl> + bioc -> data = NULL ; <nl> bioc -> capacity = bioc -> usage = bioc -> offset = 0 ; <nl>  <nl> return 0 ;
void cpu_dump_state ( CPUPPCState * env , FILE * f , fprintf_function cpu_fprintf , <nl>  <nl> int i ; <nl>  <nl> + cpu_synchronize_state ( env ); <nl> + <nl> cpu_fprintf ( f , " NIP " TARGET_FMT_lx " LR " TARGET_FMT_lx " CTR " <nl> TARGET_FMT_lx " XER " TARGET_FMT_lx "\ n ", <nl> env -> nip , env -> lr , env -> ctr , env -> xer );
VIOsPAPRDevice * vty_lookup ( sPAPRMachineState * spapr , target_ulong reg ) <nl> return spapr_vty_get_default ( spapr -> vio_bus ); <nl> } <nl>  <nl> + if (! object_dynamic_cast ( OBJECT ( sdev ), TYPE_VIO_SPAPR_VTY_DEVICE )) { <nl> + return NULL ; <nl> + } <nl> + <nl> return sdev ; <nl> } <nl> 
void machine_register_compat_props ( MachineState * machine ) <nl>  <nl> for ( i = 0 ; i < mc -> compat_props -> len ; i ++) { <nl> p = g_array_index ( mc -> compat_props , GlobalProperty *, i ); <nl> + /* Machine compat_props must never cause errors : */ <nl> + p -> errp = & error_abort ; <nl> qdev_prop_register_global ( p ); <nl> } <nl> }
static GuestPCIAddress * get_pci_info ( char * guid , Error ** errp ) <nl> } else { <nl> error_setg_win32 ( errp , GetLastError (), <nl> " failed to get device name "); <nl> - goto out ; <nl> + goto free_dev_info ; <nl> } <nl> } <nl>  <nl> static GuestPCIAddress * get_pci_info ( char * guid , Error ** errp ) <nl> pci -> bus = bus ; <nl> break ; <nl> } <nl> + <nl> + free_dev_info : <nl> + SetupDiDestroyDeviceInfoList ( dev_info ); <nl> out : <nl> g_free ( buffer ); <nl> g_free ( name );
int hvf_vcpu_exec ( CPUState * cpu ) <nl> macvm_set_rip ( cpu , rip + ins_len ); <nl> break ; <nl> case VMX_REASON_VMCALL : <nl> - /* TODO : inject # GP fault */ <nl> + env -> exception_injected = EXCP0D_GPF ; <nl> + env -> has_error_code = true ; <nl> + env -> error_code = 0 ; <nl> break ; <nl> default : <nl> error_report ("% llx : unhandled exit % llx \ n ", rip , exit_reason );
int ppc_compat_max_threads ( PowerPCCPU * cpu ); <nl> # define SPR_601_UDECR ( 0x006 ) <nl> # define SPR_LR ( 0x008 ) <nl> # define SPR_CTR ( 0x009 ) <nl> -# define SPR_UAMR ( 0x00C ) <nl> +# define SPR_UAMR ( 0x00D ) <nl> # define SPR_DSCR ( 0x011 ) <nl> # define SPR_DSISR ( 0x012 ) <nl> # define SPR_DAR ( 0x013 ) /* DAE for PowerPC 601 */
void qio_channel_test_run_reader ( QIOChannelTest * test , <nl>  <nl> void qio_channel_test_validate ( QIOChannelTest * test ) <nl> { <nl> + g_assert ( test -> readerr == NULL ); <nl> + g_assert ( test -> writeerr == NULL ); <nl> g_assert_cmpint ( memcmp ( test -> input , <nl> test -> output , <nl> test -> len ), ==, 0 ); <nl> - g_assert ( test -> readerr == NULL ); <nl> - g_assert ( test -> writeerr == NULL ); <nl>  <nl> g_free ( test -> inputv ); <nl> g_free ( test -> outputv );
retry : <nl>  <nl> /* Make sure that all offsets in the " allocated " range are representable <nl> * in an int64_t */ <nl> - if ( s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) { <nl> + if ( s -> free_cluster_index > 0 && <nl> + s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) <nl> + { <nl> return - EFBIG ; <nl> } <nl> 
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> if ( local_err ) { <nl> goto err ; <nl> } <nl> + object_unref ( obj ); <nl> } <nl> object_child_foreach ( OBJECT ( dev ), spapr_cpu_core_realize_child , & local_err ); <nl> if ( local_err ) {
static void ppc_spapr_init ( MachineState * machine ) <nl>  <nl> /* Set up Interrupt Controller before we create the VCPUs */ <nl> spapr -> icp = xics_system_init ( machine , <nl> - smp_cpus * kvmppc_smt_threads () / smp_threads , <nl> + DIV_ROUND_UP ( smp_cpus * kvmppc_smt_threads (), <nl> + smp_threads ), <nl> XICS_IRQS ); <nl>  <nl> /* init CPUs */
static int iscsi_truncate ( BlockDriverState * bs , int64_t offset ) <nl> if ( iscsilun -> allocationmap != NULL ) { <nl> g_free ( iscsilun -> allocationmap ); <nl> iscsilun -> allocationmap = <nl> - bitmap_new ( DIV_ROUND_UP ( bs -> total_sectors , <nl> + bitmap_new ( DIV_ROUND_UP ( sector_lun2qemu ( iscsilun -> num_blocks , <nl> + iscsilun ), <nl> iscsilun -> cluster_sectors )); <nl> } <nl> 
static void apic_timer_update ( APICState * s , int64_t current_time ) <nl> d = ( current_time - s -> initial_count_load_time ) >> <nl> s -> count_shift ; <nl> if ( s -> lvt [ APIC_LVT_TIMER ] & APIC_LVT_TIMER_PERIODIC ) { <nl> + if (! s -> initial_count ) <nl> + goto no_timer ; <nl> d = (( d / (( uint64_t ) s -> initial_count + 1 )) + 1 ) * (( uint64_t ) s -> initial_count + 1 ); <nl> } else { <nl> if ( d >= s -> initial_count )
const char * path ( const char * name ) <nl> { <nl> /* Only do absolute paths : quick and dirty , but should mostly be OK . <nl> Could do relative by tracking cwd . */ <nl> - if (! base || name [ 0 ] != '/') <nl> + if (! base || ! name || name [ 0 ] != '/') <nl> return name ; <nl>  <nl> return follow_path ( base , name ) ?: name ;
BlockBackend * blk_new_open ( const char * filename , const char * reference , <nl> } <nl>  <nl> blk -> root = bdrv_root_attach_child ( bs , " root ", & child_root , <nl> - perm , BLK_PERM_ALL , blk , & error_abort ); <nl> + perm , BLK_PERM_ALL , blk , errp ); <nl> + if (! blk -> root ) { <nl> + bdrv_unref ( bs ); <nl> + blk_unref ( blk ); <nl> + return NULL ; <nl> + } <nl>  <nl> return blk ; <nl> }
static void hpet_ram_writel ( void * opaque , target_phys_addr_t addr , <nl> ( timer -> config & HPET_TN_SETVAL )) <nl> timer -> cmp = ( timer -> cmp & 0xffffffff00000000ULL ) <nl> | new_val ; <nl> - else { <nl> + if ( timer_is_periodic ( timer )) { <nl> /* <nl> * FIXME : Clamp period to reasonable min value ? <nl> * Clamp period to reasonable max value
static void unix_process_msgfd ( CharDriverState * chr , struct msghdr * msg ) <nl> if ( fd < 0 ) <nl> continue ; <nl>  <nl> + /* O_NONBLOCK is preserved across SCM_RIGHTS so reset it */ <nl> + qemu_set_block ( fd ); <nl> + <nl> # ifndef MSG_CMSG_CLOEXEC <nl> qemu_set_cloexec ( fd ); <nl> # endif
tcp_sockclosed ( struct tcpcb * tp ) <nl> DEBUG_CALL (" tcp_sockclosed "); <nl> DEBUG_ARG (" tp = % p ", tp ); <nl>  <nl> + if (! tp ) { <nl> + return ; <nl> + } <nl> + <nl> switch ( tp -> t_state ) { <nl>  <nl> case TCPS_CLOSED : <nl> tcp_sockclosed ( struct tcpcb * tp ) <nl> tp -> t_state = TCPS_LAST_ACK ; <nl> break ; <nl> } <nl> - if ( tp ) <nl> - tcp_output ( tp ); <nl> + tcp_output ( tp ); <nl> } <nl>  <nl> /*
static void tcg_out_movi ( TCGContext * s , TCGType type , <nl> { <nl> tcg_target_long hi , lo = ( int32_t ) arg ; <nl>  <nl> + /* Make sure we test 32 - bit constants for imm13 properly . */ <nl> + if ( type == TCG_TYPE_I32 ) { <nl> + arg = lo ; <nl> + } <nl> + <nl> /* A 13 - bit constant sign - extended to 64 - bits . */ <nl> if ( check_fit_tl ( arg , 13 )) { <nl> tcg_out_movi_imm13 ( s , ret , arg );
static void dec_barrel ( DisasContext * dc ) <nl> tcg_gen_shr_tl ( cpu_R [ dc -> rd ], cpu_R [ dc -> ra ], t0 ); <nl> } <nl> } <nl> + tcg_temp_free ( t0 ); <nl> } <nl>  <nl> static void dec_bit ( DisasContext * dc )
static void virtio_net_vhost_status ( VirtIONet * n , uint8_t status ) <nl> return ; <nl> } <nl>  <nl> - if (!! n -> vhost_started == virtio_net_started ( n , status ) && <nl> - ! nc -> peer -> link_down ) { <nl> + if (!! n -> vhost_started == <nl> + ( virtio_net_started ( n , status ) && ! nc -> peer -> link_down )) { <nl> return ; <nl> } <nl> if (! n -> vhost_started ) {
int main ( int argc , char ** argv ) <nl> root = tmpfs ; <nl> } <nl>  <nl> - socket_path = g_strdup_printf ("/ tmp / vhost -% d . sock ", getpid ()); <nl> + socket_path = g_strdup_printf ("% s / vhost . sock ", tmpfs ); <nl>  <nl> /* create char dev and add read handlers */ <nl> qemu_add_opts (& qemu_chardev_opts );
static int img_convert ( int argc , char ** argv ) <nl> goto out ; <nl> } <nl>  <nl> - out_bs = bdrv_new_open ( out_filename , out_fmt , BDRV_O_FLAGS | BDRV_O_RDWR ); <nl> + out_bs = bdrv_new_open ( out_filename , out_fmt , <nl> + BDRV_O_FLAGS | BDRV_O_RDWR | BDRV_O_NO_FLUSH ); <nl> if (! out_bs ) { <nl> ret = - 1 ; <nl> goto out ;
PCIBus * pci_get_bus_devfn ( int * devfnp , PCIBus * root , const char * devaddr ) <nl> int dom , bus ; <nl> unsigned slot ; <nl>  <nl> - assert (! root -> parent_dev ); <nl> - <nl> if (! root ) { <nl> fprintf ( stderr , " No primary PCI bus \ n "); <nl> return NULL ; <nl> } <nl>  <nl> + assert (! root -> parent_dev ); <nl> + <nl> if (! devaddr ) { <nl> * devfnp = - 1 ; <nl> return pci_find_bus_nr ( root , 0 );
static int send_status ( int sockfd , struct iovec * iovec , int status ) <nl> */ <nl> msg_size = proxy_marshal ( iovec , 0 , " ddd ", header . type , <nl> header . size , status ); <nl> + if ( msg_size < 0 ) { <nl> + return msg_size ; <nl> + } <nl> retval = socket_write ( sockfd , iovec -> iov_base , msg_size ); <nl> if ( retval < 0 ) { <nl> return retval ;
int qemu_fsdev_add ( QemuOpts * opts ) <nl>  <nl> if ( fsle -> fse . ops -> parse_opts ) { <nl> if ( fsle -> fse . ops -> parse_opts ( opts , & fsle -> fse )) { <nl> + g_free ( fsle -> fse . fsdev_id ); <nl> + g_free ( fsle ); <nl> return - 1 ; <nl> } <nl> }
static int vmdk_snapshot_create ( const char * filename , const char * backing_file ) <nl> ret = - errno ; <nl> goto fail_rgd ; <nl> } <nl> - qemu_free ( rgd_buf ); <nl>  <nl> /* write GD */ <nl> gd_buf = qemu_malloc ( gd_size ); <nl> static int vmdk_snapshot_create ( const char * filename , const char * backing_file ) <nl> goto fail_gd ; <nl> } <nl> qemu_free ( gd_buf ); <nl> + qemu_free ( rgd_buf ); <nl>  <nl> close ( p_fd ); <nl> close ( snp_fd );
static int stdio_pclose ( void * opaque ) <nl> QEMUFileStdio * s = opaque ; <nl> int ret ; <nl> ret = pclose ( s -> stdio_file ); <nl> + if ( ret == - 1 ) { <nl> + ret = - errno ; <nl> + } <nl> g_free ( s ); <nl> return ret ; <nl> }
uint64_t helper_msub64_q_ssov ( CPUTriCoreState * env , uint64_t r1 , uint32_t r2 , <nl> } else { <nl> result = INT64_MIN ; <nl> } <nl> + } else { <nl> + env -> PSW_USB_V = 0 ; <nl> } <nl> } else { <nl> if ( ovf < 0 ) {
uint32_t gic_acknowledge_irq ( GICState * s , int cpu ) <nl> } <nl> s -> last_active [ irq ][ cpu ] = s -> running_irq [ cpu ]; <nl>  <nl> - if ( s -> revision == REV_11MPCORE ) { <nl> + if ( s -> revision == REV_11MPCORE || s -> revision == REV_NVIC ) { <nl> /* Clear pending flags for both level and edge triggered interrupts . <nl> * Level triggered IRQs will be reasserted once they become inactive . <nl> */
static int create_dynamic_disk ( BlockBackend * blk , uint8_t * buf , <nl> num_bat_entries = ( total_sectors + block_size / 512 ) / ( block_size / 512 ); <nl>  <nl> ret = blk_pwrite ( blk , offset , buf , HEADER_SIZE ); <nl> - if ( ret ) { <nl> + if ( ret < 0 ) { <nl> goto fail ; <nl> } <nl> 
* significant half of a uint64_t struct member . <nl> */ <nl> # ifdef HOST_WORDS_BIGENDIAN <nl> -# define offsetoflow32 ( S , M ) offsetof ( S , M + sizeof ( uint32_t )) <nl> +# define offsetoflow32 ( S , M ) ( offsetof ( S , M ) + sizeof ( uint32_t )) <nl> # else <nl> # define offsetoflow32 ( S , M ) offsetof ( S , M ) <nl> # endif
static int check_refblocks ( BlockDriverState * bs , BdrvCheckResult * res , <nl> * nb_clusters ); <nl> memset (&(* refcount_table )[ old_nb_clusters ], 0 , <nl> (* nb_clusters - old_nb_clusters ) * <nl> - sizeof ( uint16_t )); <nl> + sizeof (** refcount_table )); <nl> } <nl> (* refcount_table )[ cluster ]--; <nl> inc_refcounts ( bs , res , * refcount_table , * nb_clusters ,
GEN_HANDLER ( tlbiva , 0x1F , 0x12 , 0x18 , 0x03FFF801 , PPC_TLBIVA ) <nl> GEN_EXCP_PRIVOPC ( ctx ); <nl> return ; <nl> } <nl> + t0 = tcg_temp_new (); <nl> gen_addr_reg_index ( t0 , ctx ); <nl> # if defined ( TARGET_PPC64 ) <nl> if (! ctx -> sf_mode )
int main ( int argc , char ** argv ) { <nl> datalen = 0 ; <nl> } <nl> code = malloc ( strlen ( p )+ 1 ); <nl> + if (! code ) { <nl> + return 1 ; <nl> + } <nl> codelen = r_hex_str2bin ( p , code ); <nl> if (! arch ) arch = " x86 "; <nl> if (! bits ) bits = 32 ;
R_API RList * r_io_map_get_maps_in_range ( RIO * io , ut64 addr , ut64 endaddr ) { <nl> RIOMap * map ; <nl> RListIter * iter ; <nl> RList * maps = r_list_new (); <nl> + maps -> free = NULL ; <nl> r_list_foreach ( io -> maps , iter , map ) { <nl> if ( map -> from <= addr && addr < map -> to ) r_list_append ( maps , map ); <nl> // if ( map -> from == addr && endaddr == map -> to ) r_list_append ( maps , map );
R_API void r_core_anal_autoname_all_fcns ( RCore * core ) { <nl> r_flag_rename ( core -> flags , r_flag_get ( core -> flags , fcn -> name ), name ); <nl> free ( fcn -> name ); <nl> fcn -> name = name ; <nl> + } else { <nl> + free ( name ); <nl> } <nl> } <nl> }
static int r_core_search_rop ( RCore * core , ut64 from , ut64 to , int opt , const cha <nl> } <nl> if ( mode == ' j ') <nl> r_cons_printf ("]\ n "); <nl> + free ( buf ); <nl> return R_TRUE ; <nl> } <nl> 
R_API bool r_sign_save ( RAnal * a , const char * file ) { <nl> if (! a || ! file ) { <nl> return false ; <nl> } <nl> + <nl> + if ( sdb_count ( a -> sdb_zigns ) == 0 ) { <nl> + eprintf (" WARNING : no zignatures to save \ n "); <nl> + return false ; <nl> + } <nl>  <nl> Sdb * db = sdb_new ( NULL , file , 0 ); <nl> if (! db ) {
static void bin_mach0_versioninfo ( RCore * r ) { <nl> static int bin_versioninfo ( RCore * r , int mode ) { <nl> const RBinInfo * info = r_bin_get_info ( r -> bin ); <nl>  <nl> + if (!( info && info -> rclass )) return false ; <nl> + <nl> if (! strncmp (" pe ", info -> rclass , 2 )) { <nl> bin_pe_versioninfo ( r ); <nl> } else if (! strncmp (" elf ", info -> rclass , 3 )) {
static int cmd_print ( void * data , const char * input ) { <nl> free ( loc_buf ); <nl> } <nl> r_list_foreach ( f -> bbs , locs_it , b ) { <nl> - if (! first ) { <nl> + if ( first ) { <nl> first = false ; <nl> } else { <nl> r_cons_print (",");
R_API int r_bp_size ( RBreakpoint * bp ) { <nl> int i , bpsize = 8 ; <nl> for ( i = 0 ; bp -> cur -> bps [ i ]. bytes ; i ++) { <nl> bpa = & bp -> cur -> bps [ i ]; <nl> - if ( bpa -> bits != bp -> bits ) { <nl> + if ( bpa -> bits && bpa -> bits != bp -> bits ) { <nl> continue ; <nl> } <nl> if ( bpa -> length < bpsize ) {
R_API int r_debug_syscall ( RDebug * dbg , int num ) { <nl>  <nl> R_API int r_debug_kill ( RDebug * dbg , int pid , int tid , int sig ) { <nl> int ret = R_FALSE ; <nl> + if ( r_debug_is_dead ( dbg )) <nl> + return R_FALSE ; <nl> if ( dbg -> h && dbg -> h -> kill ) <nl> ret = dbg -> h -> kill ( dbg , pid , tid , sig ); <nl> else eprintf (" Backend does not implements kill ()\ n ");
static char * r_debug_bf_reg_profile ( RDebug * dbg ) { <nl> ); <nl> } <nl>  <nl> - static int r_debug_bf_breakpoint ( void * bp , RBreakpointItem * b , bool set ) { <nl> + static int r_debug_bf_breakpoint ( struct r_bp_t * bp , RBreakpointItem * b , bool set ) { <nl> // r_io_system ( dbg -> iob . io , " db "); <nl> return false ; <nl> }
static int formatDisassembledOperand ( char * strOperand , int operandNum , const dis <nl> char binary [ 9 ]; <nl> int retVal ; <nl>  <nl> + if ( operandNum >= AVR_MAX_NUM_OPERANDS ) <nl> + return 0 ; <nl> + <nl> switch ( dInstruction . instruction -> operandTypes [ operandNum ]) { <nl> case OPERAND_NONE : <nl> case OPERAND_REGISTER_GHOST :
static Sdb * store_versioninfo_gnu_verneed ( struct Elf_ ( r_bin_elf_obj_t ) * bin , Elf <nl> i += entry -> vn_next ; <nl> snprintf ( key , sizeof ( key ), " version % d ", cnt ); <nl> sdb_ns_set ( sdb , key , sdb_version ); <nl> + // if entry -> vn_next is 0 it iterate infinitely <nl> + if (! entry -> vn_next ) break ; <nl> } <nl> free ( need ); <nl> return sdb ;
static void r_core_debug_kill ( RCore * core , const char * input ) { <nl> // - trace <nl> // - stop <nl> if ( signum < 1 ) signum = r_debug_signal_resolve ( core -> dbg , name ); <nl> - if ( signal > 0 ) { <nl> + if ( signum > 0 ) { <nl> int sigopt = 0 ; <nl> if ( strchr ( p , ' s ')) sigopt |= R_DBG_SIGNAL_SKIP ; <nl> if ( strchr ( p , ' c ')) sigopt |= R_DBG_SIGNAL_CONT ;
struct r_bin_zimg_obj_t * r_bin_zimg_new_buf ( RBuffer * buf ) { <nl> goto fail ; <nl> } <nl>  <nl> + if ( r_buf_size ( bin -> b ) < sizeof ( struct zimg_header_t )) { <nl> + goto fail ; <nl> + } <nl> bin -> header = (*( struct zimg_header_t *) bin -> b -> buf ); <nl>  <nl> return bin ;
static void r_bin_file_free ( void /* RBinFile */ * bf_ ) { <nl> if ( a -> curxtr && a -> curxtr -> destroy ) <nl> a -> curxtr -> free_xtr (( void *) ( a -> xtr_obj )); <nl>  <nl> - r_bin_object_free ( a -> o ); <nl> + r_list_free ( a -> objs ); <nl> a -> o = NULL ; <nl> r_buf_free ( a -> buf ); <nl> // TODO : unset related sdb namespaces
riscv_dis ( RAsm * a , RAsmOp * rop , const ut8 * buf , ut64 len ) { <nl> memcpy (& insn , buf , 4 ); <nl> riscv_disassemble ( a , rop , insn , a -> bits ); <nl>  <nl> - return 4 ; <nl> + return riscv_insn_length ( insn ); <nl> }
static int __plugin_open ( RIO * io , const char * file , ut8 many ) { <nl>  <nl> static RIODesc * __open ( RIO * io , const char * file , int rw , int mode ) { <nl> int ret , pid = getpid (); <nl> + if ( r_sandbox_enable ( 0 )) <nl> + return NULL ; <nl> io -> va = R_TRUE ; // nop <nl> ret = update_self_regions ( pid ); <nl> if ( ret ) {
static void get_chain_data ( proxy_data * pd , unsigned int * proxy_count , chain_typ <nl> } <nl> } <nl> fclose ( file ); <nl> + if (! count ) { <nl> + fprintf ( stderr , " error : no valid proxy found in config \ n "); <nl> + exit ( 1 ); <nl> + } <nl> * proxy_count = count ; <nl> proxychains_got_chain_data = 1 ; <nl> }
mgt_sigchld ( struct ev * e , int what ) <nl> ev_poker = NULL ; <nl>  <nl> r = wait4 (- 1 , & status , WNOHANG , NULL ); <nl> - if ( r != child_pid ) { <nl> + if ( r != child_pid || r == - 1 ) { <nl> fprintf ( stderr , " Unknown child died pid =% d status = 0x % x \ n ", <nl> r , status ); <nl> return ( 0 );
mgt_run ( int dflag , const char * T_arg ) <nl>  <nl> setproctitle (" Varnish - Mgr % s ", heritage . name ); <nl>  <nl> + memset (& sac , 0 , sizeof sac ); <nl> sac . sa_handler = SIG_IGN ; <nl> sac . sa_flags = SA_RESTART ; <nl> 
dispatch_f ( struct VSL_data * vsl , struct VSL_transaction * const pt [], <nl> for ( t = pt [ 0 ]; t != NULL ; t = *++ pt ) { <nl> CTX . gen ++; <nl> if ( t -> type != VSL_t_req ) <nl> + /* Only look at client requests */ <nl> + continue ; <nl> + if ( t -> reason == VSL_r_esi ) <nl> + /* Skip ESI requests */ <nl> continue ; <nl> CTX . hitmiss = "-"; <nl> CTX . handling = "-";
parse_new ( struct vcc * tl ) <nl> vcc_ErrWhere ( tl , tl -> t ); <nl> return ; <nl> } <nl> - XXXAZ ( sy1 ); <nl>  <nl> sy1 = VCC_AddSymbolTok ( tl , tl -> t , SYM_NONE ); // XXX : NONE ? <nl> XXXAN ( sy1 );
SES_RefSrcAddr ( struct sess * sp ) <nl> c3 = c ; <nl> continue ; <nl> } <nl> - TAILQ_REMOVE ( ch , c2 , list ); <nl> - free ( c2 ); <nl> + TAILQ_REMOVE ( ch , c , list ); <nl> + free ( c ); <nl> VSL_stats -> n_srcaddr --; <nl> } <nl> if ( c3 == NULL ) {
do_list ( struct cli * cli , struct director * d , void * priv ) <nl> if ( d -> vdir -> admin_health == VDI_AH_DELETED ) <nl> return ( 0 ); <nl>  <nl> + // XXX admin health " probe " for the no - probe case is confusing <nl> VCLI_Out ( cli , "\ n %- 30s %- 7s ", d -> vdir -> cli_name , VDI_Ahealth ( d )); <nl>  <nl> if ( d -> vdir -> methods -> list != NULL )
read_tmo ( int fd , char * ptr , unsigned len , double tmo ) <nl> pfd . events = POLLIN ; <nl> for ( j = 0 ; len > 0 ; ) { <nl> i = poll (& pfd , 1 , to ); <nl> + if ( i < 0 ) { <nl> + errno = EINTR ; <nl> + return (- 1 ); <nl> + } <nl> if ( i == 0 ) { <nl> errno = ETIMEDOUT ; <nl> return (- 1 );
struct mg_context * mg_start ( mg_callback_t user_callback , const char ** options ) { <nl> ctx = calloc ( 1 , sizeof (* ctx )); <nl> ctx -> user_callback = user_callback ; <nl>  <nl> - while (( name = * options ++) != NULL ) { <nl> + while ( options && ( name = * options ++) != NULL ) { <nl> if (( i = get_option_index ( name )) == - 1 ) { <nl> cry ( fc ( ctx ), " Invalid option : % s ", name ); <nl> free_context ( ctx );
struct mg_request_info { <nl> int remote_port ; // Client ' s port <nl> int is_ssl ; // 1 if SSL - ed , 0 if not <nl> void * user_data ; // User data pointer passed to mg_start () <nl> + void * conn_data ; // Connection - specific user data <nl>  <nl> int num_headers ; // Number of HTTP headers <nl> struct mg_header {
static char * amqp_simple_get_frame ( int fd , struct amqp_frame_header * fh ) { <nl> while ( len < fh -> size + 1 ) { <nl> rlen = recv ( fd , ptr , ( fh -> size + 1 )- len , 0 ); <nl> if ( rlen <= 0 ) { <nl> - if ( rlen < 0 ) <nl> + if ( rlen < 0 ) { <nl> uwsgi_error (" recv ()"); <nl> + } <nl> + free ( frame ); <nl> return NULL ; <nl> } <nl> len += rlen ;
next : <nl> } <nl>  <nl> // this must be called only by the master !!! <nl> - if ( uwsgi . mywid > 0 ) return ; <nl> + if (! uwsgi . workers ) return ; <nl> + if ( uwsgi . workers [ 0 ]. pid != getpid ()) return ; <nl> uwsgi_legion_announce_death (); <nl> } <nl> 
static void uwsgi_alarm_thread_loop ( struct uwsgi_thread * ut ) { <nl> long ptr = 0 ; <nl> memcpy (& ptr , buf , sizeof ( long )); <nl> struct uwsgi_alarm_instance * uai = ( struct uwsgi_alarm_instance *) ptr ; <nl> - if (! uai ) return ; <nl> + if (! uai ) <nl> + break ; <nl> uwsgi_alarm_run ( uai , msg , msg_size ); <nl> } <nl> } <nl> } <nl> + free ( buf ); <nl> } <nl>  <nl> // initialize alarms , instances and log regexps
int http_parse ( struct http_session * h_session ) { <nl> hv = hv -> next ; <nl> } <nl>  <nl> + // security check <nl> + if ( c >= MAX_HTTP_VEC - 4 ) { <nl> + uwsgi_log (" too much headers in request . skipping it .\ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> return c ; <nl>  <nl> }
void reap_them_all ( int signum ) { <nl> } <nl>  <nl> for ( i = 0 ; i < uwsgi . mules_cnt ; i ++) { <nl> + if (! uwsgi . mules ) break ; <nl> if ( uwsgi . mules [ i ]. pid > 0 ) <nl> kill ( uwsgi . mules [ i ]. pid , SIGKILL ); <nl> }
char * uwsgi_cgi_get_docroot ( char * path_info , uint16_t path_info_len , int * need_f <nl> } <nl>  <nl> if ( choosen_udd -> status == 0 ) { <nl> - char * tmp_udd = realpath ( path , NULL ); <nl> - if (! tmp_udd ) { <nl> + char * tmp_udd = uwsgi_malloc ( PATH_MAX + 1 ); <nl> + if (! realpath ( path , tmp_udd )) { <nl> return NULL ; <nl> } <nl> 
int uwsgi_start ( void * v_argv ) { <nl> # ifndef __OpenBSD__ <nl>  <nl> if ( uwsgi . rl . rlim_max > 0 ) { <nl> + uwsgi . rl . rlim_cur = uwsgi . rl . rlim_max ; <nl> uwsgi_log (" limiting address space of processes ...\ n "); <nl> if ( setrlimit ( RLIMIT_AS , & uwsgi . rl )) { <nl> uwsgi_error (" setrlimit ()");
void uwsgi_python_harakiri ( int wid ) { <nl> char * address = uwsgi_concat2 ( up . tracebacker , uwsgi_num2str ( wid )); <nl>  <nl> int fd = uwsgi_connect ( address , - 1 , 0 ); <nl> - for (;;) { <nl> + while ( fd >= 0 ) { <nl> int ret = uwsgi_waitfd ( fd , uwsgi . shared -> options [ UWSGI_OPTION_SOCKET_TIMEOUT ]); <nl> if ( ret <= 0 ) { <nl> break ;
void uwsgi_opt_set_placeholder ( char * opt , char * value , void * none ) { <nl>  <nl> p [ 0 ] = 0 ; <nl> add_exported_option ( uwsgi_str ( value ), p + 1 , 1 ); <nl> - p [ 1 ] = '='; <nl> + p [ 0 ] = '='; <nl>  <nl> } <nl> 
static v8 :: Handle < v8 :: Value > uwsgi_v8_commonjs_require ( const v8 :: Arguments & <nl> free ( tmp_filename ); <nl> return ret ; <nl> } <nl> + free ( tmp_filename ); <nl> } <nl> - free ( tmp_filename ); <nl> usl = usl -> next ; <nl> } <nl> }
static void uwsgi_pypy_onload () { <nl> # ifdef UWSGI_PYPY_HOME <nl> upypy . home = UWSGI_PYPY_HOME ; <nl> # endif <nl> + uwsgi . has_threads = 1 ; <nl> } <nl>  <nl> static int uwsgi_pypy_mule ( char * opt ) {
static void mongrel2_connect () { <nl> } <nl> char * responder = strchr ( uwsgi_sock -> name , ','); <nl> if (! responder ) { <nl> - uwsgi_log (" invalid zeromq address \ n "); <nl> + uwsgi_log (" invalid zeromq address : % s \ n ", uwsgi_sock -> name ); <nl> exit ( 1 ); <nl> } <nl> uwsgi_sock -> receiver = uwsgi_concat2n ( uwsgi_sock -> name , responder - uwsgi_sock -> name , "", 0 );
error : <nl> if ( urcc -> key ) free ( urcc -> key ); <nl> if ( urcc -> name ) free ( urcc -> name ); <nl> if ( urcc -> expires_str ) free ( urcc -> expires_str ); <nl> + free ( urcc ); <nl> return - 1 ; <nl> } <nl> 
void * uwsgi_python_tracebacker_thread ( void * foobar ) { <nl> uwsgi . no_defer_accept = current_defer_accept ; <nl>  <nl> PyObject * traceback_module = PyImport_ImportModule (" traceback "); <nl> - if (! traceback_module ) return NULL ; <nl> + if (! traceback_module ) { <nl> + free ( str_wid ); <nl> + free ( sock_path ); <nl> + close ( fd ); <nl> + return NULL ; <nl> + } <nl> PyObject * traceback_dict = PyModule_GetDict ( traceback_module ); <nl> PyObject * extract_stack = PyDict_GetItemString ( traceback_dict , " extract_stack "); <nl> 
void uwsgi_python_reset_random_seed () { <nl> void uwsgi_python_atexit () { <nl>  <nl> // if hijacked do not run atexit hooks <nl> + if ( uwsgi . workers [ uwsgi . mywid ]. hijacked ) <nl> + return ; <nl>  <nl> // this time we use this higher level function <nl> // as this code can be executed in a signal handler
PyObject * py_uwsgi_gevent_graceful ( PyObject * self , PyObject * args ) { <nl>  <nl> void uwsgi_gevent_gbcw () { <nl>  <nl> - uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> - <nl> py_uwsgi_gevent_graceful ( NULL , NULL ); <nl> + <nl> + uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> + exit ( 0 ); <nl> } <nl>  <nl> struct wsgi_request * uwsgi_gevent_current_wsgi_req ( void ) {
void unit_attack ( <nl> int damage_left = damage ; <nl> while ( damage_left > 0 && ! animator . would_end ()) { <nl> int step_left = ( animator . get_end_time () - animator . get_animation_time () )/ 50 ; <nl> - int removed_hp = damage_left / step_left ; <nl> + int removed_hp = step_left ? damage_left / step_left : 1 ; <nl> if ( removed_hp < 1 ) removed_hp = 1 ; <nl> if ( step_left < 1 ) step_left = 1 ; <nl> defender . take_hit ( removed_hp );
SYNCED_COMMAND_HANDLER_FUNCTION ( attack , child , /* use_undo */, show , error_handler <nl> } <nl> } <nl>  <nl> - if ( size_t ( weapon_num ) >= u -> attacks (). size ()) { <nl> + if ( static_cast < unsigned >( weapon_num ) >= u -> attacks (). size ()) { <nl> error_handler (" illegal weapon type in attack \ n ", true ); <nl> return false ; <nl> }
public : <nl> size_t size () const { return lmap_ . size (); } <nl> size_t num_iters () const ; <nl>  <nl> + bool empty () const { return lmap_ . empty (); } <nl> + <nl> void clear ( bool force = false ); <nl>  <nl> /**
void team :: team_info :: write ( config & cfg ) const <nl> cfg [" hidden "] = hidden ; <nl> cfg [" suppress_end_turn_confirmation "] = no_turn_confirmation ; <nl> cfg [" scroll_to_leader "] = scroll_to_leader ; <nl> - cfg [" controller "] = controller_string (); <nl> + cfg [" controller "] = ( controller == IDLE ? " human " : controller_string ()); <nl>  <nl> std :: stringstream can_recruit_str ; <nl> for ( std :: set < std :: string >:: const_iterator cr = can_recruit . begin (); cr != can_recruit . end (); ++ cr ) {
void show_about ( display & disp ) <nl> text . push_back ("+ Developers "); <nl> text . push_back ("- Alfredo Beaumont ( ziberpunk )"); <nl> text . push_back ("- Cyril Bouthors ( CyrilB )"); <nl> - text . push_back ("- Guillaume Duwelz - Rebert "); <nl> text . push_back ("- Isaac Clerencia "); <nl> text . push_back ("- J . R . Blain ( Cowboy )"); <nl> text . push_back ("- Justin Zaun ( jzaun )");
void textbox :: set_text ( std :: string text ) <nl> { <nl> text_ = string_to_wstring ( text ); <nl> cursor_ = text_ . size (); <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> } <nl> void textbox :: clear () <nl> cursor_ = 0 ; <nl> cursor_pos_ = 0 ; <nl> text_pos_ = 0 ; <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> }
game_launcher :: game_launcher ( const commandline_options & cmdline_opts , const char <nl> const std :: string app_basename = filesystem :: base_name ( appname ); <nl> jump_to_editor_ = app_basename . find (" editor ") != std :: string :: npos ; <nl>  <nl> + if ( cmdline_opts_ . core_id ) { <nl> + preferences :: set_core_id (* cmdline_opts_ . core_id ); <nl> + } <nl> if ( cmdline_opts_ . campaign ) { <nl> jump_to_campaign_ . jump_ = true ; <nl> jump_to_campaign_ . campaign_id_ = * cmdline_opts_ . campaign ;
bool loadgame :: load_multiplayer_game () <nl> return false ; <nl> } <nl>  <nl> + if ( is_replay_save ( summary_ )) { <nl> + gui2 :: show_transient_message ( video_ , _ (" Load Game "), _ (" Replays are not supported in multiplayer mode .")); <nl> + return false ; <nl> + } <nl> + <nl> if ( gamestate_ . classification (). campaign_type != game_classification :: CAMPAIGN_TYPE :: MULTIPLAYER ) { <nl> gui2 :: show_transient_error_message ( video_ , _ (" This is not a multiplayer save .")); <nl> return false ;
void unit :: write ( config & cfg ) const <nl> break ; <nl> case unit_type :: LIMINAL : <nl> cfg [" alignment "] = " liminal "; <nl> + break ; <nl> default : <nl> cfg [" alignment "] = " neutral "; <nl> }
WML_HANDLER_FUNCTION ( replace_schedule , /* event_info */, cfg ) <nl> ERR_NG << " attempted to to replace ToD schedule with empty schedule \ n "; <nl> } else { <nl> resources :: tod_manager -> replace_schedule ( cfg . get_parsed_config ()); <nl> + resources :: screen -> new_turn (); <nl> LOG_NG << " replaced ToD schedule \ n "; <nl> } <nl> }
namespace { <nl> net_manager_ ( min_thread , max_thread ), <nl> server_manager_ ( load_config ()), <nl> hooks_ (), <nl> - input_ ( 0 ) <nl> + input_ ( 0 ), <nl> + compress_level_ ( 0 ) <nl> { <nl> if ( cfg_ . child (" campaigns ") == NULL ) { <nl> cfg_ . add_child (" campaigns ");
namespace game_config <nl> namespace sounds { <nl> const std :: string turn_bell = " bell . wav ", <nl> timer_bell = " timer . wav ", <nl> - receive_message = " chat - 3 . ogg ", <nl> + receive_message = " chat - 1 . ogg , chat - 2 . ogg , chat - 3 . ogg , chat - 4 . ogg ", <nl> receive_message_highlight = " chat - highlight . ogg ", <nl> receive_message_friend = " chat - 4 . ogg ", <nl> receive_message_server = " receive . wav ",
namespace <nl> char * endptr ; <nl> int res = strtol ( index_str , & endptr , 10 ); <nl>  <nl> - if (* endptr != ']' || res > int ( game_config :: max_loop )) <nl> + if (* endptr != ']' || res > int ( game_config :: max_loop ) || endptr == index_str ) <nl> { <nl> throw invalid_variablename_exception (); <nl> }
loadscreen :: loadscreen ( CVideo & screen , const int & percent ): <nl> setconfig_counter ( 0 ), <nl> parser_counter ( 0 ), <nl> screen_ ( screen ), <nl> + textarea_ (), <nl> + logo_surface_ ( NULL ), <nl> logo_drawn_ ( false ), <nl> pby_offset_ ( 0 ), <nl> prcnt_ ( percent )
void room_manager :: load_config ( const config & cfg ) <nl> { <nl> filename_ = cfg [" room_save_file "]; <nl> compress_stored_rooms_ = utils :: string_bool ( cfg [" compress_stored_rooms "], true ); <nl> - new_room_policy_ = pp_from_string ( cfg [" new_room_policy "]); <nl> + PRIVILEGE_POLICY pp = pp_from_string ( cfg [" new_room_policy "]); <nl> + if ( pp != PP_COUNT ) new_room_policy_ = pp ; <nl> } <nl>  <nl> const std :: string & room_manager :: storage_filename () const
void wait :: start_game () <nl>  <nl> LOG_NW << " starting game \ n "; <nl> sound :: play_UI_sound ( game_config :: sounds :: mp_game_begins ); <nl> + game_display :: get_singleton ()-> send_notification ( _ (" Wesnoth "), _ (" Game has begun !")); <nl> } <nl>  <nl> void wait :: layout_children ( const SDL_Rect & rect )
void play_multiplayer ( display & disp , game_data & units_data , config cfg , <nl> } else if ( result < int ( choices . size ()/ 3 )* 2 ) { <nl> controller = " ai "; <nl> result -= choices . size ()/ 3 ; <nl> + sides [ res ]-> values [" description "] = ""; <nl> } else { <nl> controller = " network "; <nl> result -= ( choices . size ()/ 3 )* 2 ;
version_info :: version_info ( unsigned int major , unsigned int minor , unsigned int <nl> } <nl>  <nl> version_info :: version_info ( const std :: string & str ) <nl> - : special_ (""), special_separator_ ('\ 0 '), sane_ ( true ) <nl> + : nums_ () <nl> + , special_ ("") <nl> + , special_separator_ ('\ 0 ') <nl> + , sane_ ( true ) <nl> { <nl> const std :: vector < std :: string > string_parts = utils :: split ( str ,'.'); <nl> // first two components are required to be valid numbers , though
void widget :: set_visible ( const visibility visible ) <nl> visible_ = visible ; <nl>  <nl> if ( need_resize ) { <nl> - if ( new_widgets ) { <nl> + if ( visible == visibility :: visible && new_widgets ) { <nl> event :: message message ; <nl> fire ( event :: REQUEST_PLACEMENT , * this , message ); <nl> } else {
namespace { <nl> bool terrain_matches_filter ( const gamemap & map , const gamemap :: location & loc , const vconfig & cfg , <nl> const gamestatus & game_status , const unit_map & units , const bool flat_tod , <nl> const size_t max_loop ) <nl> -{ <nl> +{ <nl> + if (! map . on_board ( loc )) return false ; <nl> + <nl> // handle radius <nl> const size_t radius = minimum < size_t >( max_loop , <nl> lexical_cast_default < size_t >( cfg [" radius "], 0 ));
int battle_context :: choose_attacker_weapon ( const unit & attacker , <nl> attacker_combatant_ = new combatant (* attacker_stats_ ); <nl> defender_combatant_ = new combatant (* defender_stats_ , prev_def ); <nl> attacker_combatant_ -> fight (* defender_combatant_ ); <nl> + } else { <nl> + if ( attacker_stats_ -> disable ) { <nl> + delete attacker_stats_ ; <nl> + attacker_stats_ = nullptr ; <nl> + continue ; <nl> + } <nl> } <nl> if (! best_att_comb || better_combat (* attacker_combatant_ , * defender_combatant_ , <nl> * best_att_comb , * best_def_comb , harm_weight )) {
std :: vector < std :: string > get_text () { <nl> " _ " N_ ("+ Catalan Translation "), <nl> "- Carles Company ( brrr )", <nl> "- Dan Rosàs Garcia ( focks )", <nl> + "- Jonatan Alamà ( tin )", <nl> "- Jordà Polo ( ettin )", <nl> "- Mark Recasens ", <nl> "- Pau Rul · lan Ferragut ",
time_of_day :: time_of_day ( const config & cfg ) <nl> time_of_day :: time_of_day () <nl> : lawful_bonus ( 0 ) <nl> , bonus_modified ( 0 ) <nl> +, image () <nl> , name (" NULL_TOD ") <nl> , id (" nulltod ") <nl> +, image_mask () <nl> , red ( 0 ) <nl> , green ( 0 ) <nl> , blue ( 0 ) <nl> +, sounds () <nl> { <nl> } <nl> 
struct tiff { <nl> */ <nl> # ifndef ReadOK <nl> # define ReadOK ( tif , buf , size ) \ <nl> - ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t ) size ) == ( tsize_t ) size ) <nl> + ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t )( size )) == ( tsize_t )( size )) <nl> # endif <nl> # ifndef SeekOK <nl> # define SeekOK ( tif , off ) \
tsize_t t2p_readwrite_pdf_image_tile ( T2P * t2p , TIFF * input , TIFF * output , ttile_ <nl> return ( 0 ); <nl> } <nl> if ( TIFFGetField ( input , TIFFTAG_JPEGTABLES , & count , & jpt ) != 0 ) { <nl> - if ( count >= 4 ) { <nl> + if ( count > 4 ) { <nl> int retTIFFReadRawTile ; <nl> /* Ignore EOI marker of JpegTables */ <nl> _TIFFmemcpy ( buffer , jpt , count - 2 );
static int lxc_spawn ( struct lxc_handler * handler ) <nl> int preserve_mask = 0 , i ; <nl> int netpipepair [ 2 ], nveths ; <nl>  <nl> + netpipe = - 1 ; <nl> + <nl> for ( i = 0 ; i < LXC_NS_MAX ; i ++) <nl> if ( handler -> conf -> inherit_ns_fd [ i ] != - 1 ) <nl> preserve_mask |= ns_info [ i ]. clone_flag ;
static void print_usage ( const struct option longopts []) <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_version () <nl> + static void print_version ( void ) <nl> { <nl> printf ("% s \ n ", LXC_VERSION ); <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_help () <nl> + static void print_help ( void ) <nl> { <nl> fprintf ( stderr , "\ <nl> Usage : lxc - init -- name = NAME -- COMMAND \ n \
int lxc_abstract_unix_connect ( const char * path ) <nl>  <nl> if ( connect ( fd , ( struct sockaddr *)& addr , offsetof ( struct sockaddr_un , sun_path ) + len )) { <nl> int tmp = errno ; <nl> + /* special case to connect to older containers */ <nl> + if ( connect ( fd , ( struct sockaddr *)& addr , sizeof ( addr )) == 0 ) <nl> + return fd ; <nl> process_lock (); <nl> close ( fd ); <nl> process_unlock ();
static int keyboard_feed_evdev ( idev_keyboard * k , idev_data * data ) { <nl> /* TODO : update LEDs */ <nl> } <nl>  <nl> - if ( num < 0 ) <nl> + if ( num < 0 ) { <nl> + r = num ; <nl> goto error ; <nl> + } <nl>  <nl> r = keyboard_fill ( k , & k -> evdata , data -> resync , ev -> code , ev -> value , num , keysyms ); <nl> if ( r < 0 )
int dhcp_packet_verify_headers ( DHCPPacket * packet , size_t len , bool checksum ) { <nl>  <nl> /* UDP */ <nl>  <nl> + if ( packet -> ip . protocol != IPPROTO_UDP ) { <nl> + log_dhcp_client ( client , " ignoring packet : not UDP "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( len < DHCP_IP_UDP_SIZE ) { <nl> log_dhcp_client ( client , " ignoring packet : packet (% zu bytes ) " <nl> " smaller than IP + UDP header (% u bytes )", len ,
int dhcp6_option_parse_ip6addrs ( uint8_t * optval , uint16_t optlen , <nl>  <nl> int dhcp6_option_parse_domainname ( const uint8_t * optval , uint16_t optlen , char *** str_arr ) { <nl> size_t pos = 0 , idx = 0 ; <nl> - _cleanup_free_ char ** names = NULL ; <nl> + _cleanup_strv_free_ char ** names = NULL ; <nl> int r ; <nl>  <nl> assert_return ( optlen > 1 , - ENODATA );
int main ( int argc , char * argv [], char * envp []) <nl> udev_init_config (); <nl>  <nl> /* set signal handlers */ <nl> + memset (& act , 0x00 , sizeof ( act )); <nl> act . sa_handler = ( void (*) ( int )) sig_handler ; <nl> sigemptyset (& act . sa_mask ); <nl> act . sa_flags = 0 ;
static int load_group_database ( void ) { <nl> static int make_backup ( const char * target , const char * x ) { <nl> _cleanup_close_ int src = - 1 ; <nl> _cleanup_fclose_ FILE * dst = NULL ; <nl> - char * backup , * temp ; <nl> + _cleanup_free_ char * temp = NULL ; <nl> + char * backup ; <nl> struct timespec ts [ 2 ]; <nl> struct stat st ; <nl> int r ;
static int create_symlink ( const char * verb , const char * old_path , const char * ne <nl> return 1 ; <nl> } <nl>  <nl> + free ( dest ); <nl> return 0 ; <nl> } <nl> 
static int unit_file_search ( <nl>  <nl> _cleanup_free_ char * template = NULL ; <nl> _cleanup_strv_free_ char ** dirs = NULL ; <nl> - _cleanup_free_ char ** files = NULL ; <nl> + _cleanup_strv_free_ char ** files = NULL ; <nl> const char * dropin_dir_name = NULL ; <nl> const char * dropin_template_dir_name = NULL ; <nl> 
int config_parse_strv ( <nl> if (* sv ) <nl> for ( k = 0 ; (* sv )[ k ]; k ++) <nl> n [ k ] = (* sv )[ k ]; <nl> + else <nl> + k = 0 ; <nl>  <nl> FOREACH_WORD_QUOTED ( w , l , rvalue , state ) <nl> if (!( n [ k ++] = strndup ( w , l )))
int main ( int argc , char * argv [], char * envp []) <nl>  <nl> reload_config = 1 ; <nl> buf = malloc ( nbytes ); <nl> - if ( buf != NULL ) { <nl> + if ( buf == NULL ) { <nl> err (" error getting buffer for inotify , disable watching "); <nl> close ( inotify_fd ); <nl> inotify_fd = - 1 ;
static void item_free ( Item * i ) { <nl> free ( i -> uid_path ); <nl> free ( i -> gid_path ); <nl> free ( i -> description ); <nl> + free ( i -> home ); <nl> free ( i ); <nl> } <nl> 
static int get_key ( struct udev * udev , char ** line , char ** key , enum operation_ty <nl> char * temp ; <nl>  <nl> linepos = * line ; <nl> - if ( linepos == NULL && linepos [ 0 ] == '\ 0 ') <nl> + if ( linepos == NULL || linepos [ 0 ] == '\ 0 ') <nl> return - 1 ; <nl>  <nl> /* skip whitespace */
int main ( int argc , char * argv []) { <nl> return EXIT_FAILURE ; <nl> } <nl>  <nl> - if ( streq ( argv [ 1 ], " load ") && shall_restore_state ()) { <nl> + if ( streq ( argv [ 1 ], " load ")) { <nl> _cleanup_free_ char * value = NULL ; <nl>  <nl> + if (! shall_restore_state ()) <nl> + return EXIT_SUCCESS ; <nl> + <nl> r = read_one_line_file ( saved , & value ); <nl> if ( r < 0 ) { <nl> 
static int dispatch_wqueue ( sd_bus * bus ) { <nl> * it got full , then all bets are off <nl> * anyway . */ <nl>  <nl> - sd_bus_message_unref ( bus -> wqueue [ 0 ]); <nl> bus -> wqueue_size --; <nl> + sd_bus_message_unref ( bus -> wqueue [ 0 ]); <nl> memmove ( bus -> wqueue , bus -> wqueue + 1 , sizeof ( sd_bus_message *) * bus -> wqueue_size ); <nl> bus -> windex = 0 ; <nl> 
static int service_dispatch_timer ( sd_event_source * source , usec_t usec , void * us <nl>  <nl> case SERVICE_RELOAD : <nl> log_unit_warning ( UNIT ( s ), " Reload operation timed out . Stopping ."); <nl> + service_unwatch_control_pid ( s ); <nl> + service_kill_control_processes ( s ); <nl> s -> reload_result = SERVICE_FAILURE_TIMEOUT ; <nl> service_enter_running ( s , SERVICE_SUCCESS ); <nl> break ;
int main ( int argc , char * argv []) { <nl>  <nl> umask ( 0022 ); <nl>  <nl> + /* Refuse to run unless we are in an initrd () */ <nl> + if (! in_initrd ()) <nl> + return EXIT_SUCCESS ; <nl> + <nl> device = argv [ 1 ]; <nl>  <nl> if ( stat ( device , & st ) < 0 ) {
int config_parse_exec_nice ( <nl> assert ( rvalue ); <nl> assert ( data ); <nl>  <nl> + if ( isempty ( rvalue )) { <nl> + c -> nice_set = false ; <nl> + return 0 ; <nl> + } <nl> + <nl> r = parse_nice ( rvalue , & priority ); <nl> if ( r < 0 ) { <nl> if ( r == - ERANGE )
static int parse_line ( const char * fname , unsigned line , const char * buffer , bool <nl> } <nl> } else { <nl> existing = new0 ( ItemArray , 1 ); <nl> + if (! existing ) <nl> + return log_oom (); <nl> + <nl> r = ordered_hashmap_put ( h , i . path , existing ); <nl> if ( r < 0 ) <nl> return log_oom ();
static uint32_t term_color_to_argb32 ( const term_color * color , const term_attr * a <nl> case TERM_CCODE_BLACK ... TERM_CCODE_LIGHT_WHITE : <nl> t = color -> ccode - TERM_CCODE_BLACK ; <nl>  <nl> - /* bold causes light colors */ <nl> - if ( t < 8 && attr -> bold ) <nl> + /* bold causes light colors ( only for foreground colors ) */ <nl> + if ( t < 8 && attr -> bold && color == & attr -> fg ) <nl> t += 8 ; <nl>  <nl> r = palette [ t * 3 + 0 ];
static int str_compare ( const void * _a , const void * _b ) { <nl> } <nl>  <nl> char ** strv_sort ( char ** l ) { <nl> - <nl> - if ( strv_isempty ( l )) <nl> - return l ; <nl> - <nl> - qsort ( l , strv_length ( l ), sizeof ( char *), str_compare ); <nl> + qsort_safe ( l , strv_length ( l ), sizeof ( char *), str_compare ); <nl> return l ; <nl> } <nl> 
static int event_make_signal_data ( <nl> d -> priority = priority ; <nl>  <nl> r = hashmap_put ( e -> signal_data , & d -> priority , d ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( d ); <nl> return r ; <nl> + } <nl>  <nl> added = true ; <nl> }
int manager_handle_action ( <nl>  <nl> n = manager_count_displays ( m ); <nl> if ( n != 1 ) { <nl> - log_debug (" Ignoring lid switch request , % s displays connected ."); <nl> + log_debug (" Ignoring lid switch request , % i displays connected .", n ); <nl> return 0 ; <nl> } <nl> }
static void output_unit_file_list ( const UnitFileList * units , unsigned c ) { <nl> } else <nl> id_cols = max_id_len ; <nl>  <nl> - if (! arg_no_legend ) <nl> + if (! arg_no_legend && c > 0 ) <nl> printf ("%-* s %-* s \ n ", <nl> id_cols , " UNIT FILE ", <nl> state_cols , " STATE ");
int main ( int argc , char ** argv ) { <nl> } <nl> printf ("% s ... ", name ); <nl> fflush ( stdout ); <nl> - ( void ) LLVMFuzzerTestOneInput (( uint8_t *) buf , size ); <nl> + ( void ) LLVMFuzzerTestOneInput (( uint8_t *) buf , size ); <nl> printf (" ok \ n "); <nl> } <nl> return EXIT_SUCCESS ;
static int get_file_to_edit ( <nl> return log_oom (); <nl>  <nl> if ( arg_runtime ) { <nl> - run = strjoin ( paths -> runtime_config , name , NULL ); <nl> + run = strjoin ( paths -> runtime_config , "/", name , NULL ); <nl> if (! run ) <nl> return log_oom (); <nl> }
char * <nl> utf8_prev_char ( const char * p ) <nl> { <nl> - while ( 1 ) <nl> + for (;;) <nl> { <nl> p --; <nl> if ((* p & 0xc0 ) != 0x80 )
static int logind_check_inhibitors ( enum action a ) { <nl> if (! on_tty ()) <nl> return 0 ; <nl>  <nl> + if ( arg_transport != BUS_TRANSPORT_LOCAL ) <nl> + return 0 ; <nl> + <nl> r = acquire_bus ( BUS_FULL , & bus ); <nl> if ( r < 0 ) <nl> return r ;
static int client_receive_message_udp ( <nl> if ( buflen < 0 ) <nl> return buflen ; <nl>  <nl> + if ( buflen == 0 ) <nl> + buflen = 1 ; <nl> + <nl> message = malloc0 ( buflen ); <nl> if (! message ) <nl> return - ENOMEM ;
fallback : <nl> return - errno ; <nl> } <nl>  <nl> + free ( parent ); <nl> + parent = NULL ; <nl> + <nl> r = path_get_parent ( t , & parent ); <nl> if ( r < 0 ) <nl> return r ;
int bus_message_print_all_properties ( <nl> return log_oom (); <nl>  <nl> r = set_put (* found_properties , name ); <nl> - if ( r < 0 && r != EEXIST ) <nl> + if ( r < 0 && r != - EEXIST ) <nl> return log_oom (); <nl> } <nl> 
static struct node * bus_node_allocate ( sd_bus * bus , const char * path ) { <nl> e = strrchr ( path , '/'); <nl> assert ( e ); <nl>  <nl> - p = strndupa ( path , MAX ( 1 , path - e )); <nl> + p = strndupa ( path , MAX ( 1 , e - path )); <nl>  <nl> parent = bus_node_allocate ( bus , p ); <nl> if (! parent )
static void mount_set_state ( Mount * m , MountState state ) { <nl> state == MOUNT_REMOUNTING_SIGKILL || <nl> state == MOUNT_UNMOUNTING_SIGTERM || <nl> state == MOUNT_UNMOUNTING_SIGKILL || <nl> - state == MOUNT_FAILED ) <nl> - mount_notify_automount ( m , - ENODEV ); <nl> + state == MOUNT_FAILED ) { <nl> + if ( state != old_state ) <nl> + mount_notify_automount ( m , - ENODEV ); <nl> + } <nl>  <nl> if ( state != old_state ) <nl> log_debug ("% s changed % s -> % s ",
int main ( int argc , char * argv []) { <nl> } <nl> } <nl>  <nl> + free ( arg_root_what ); <nl> + <nl> return r < 0 ? EXIT_FAILURE : EXIT_SUCCESS ; <nl> }
int fopen_temporary ( const char * path , FILE ** _f , char ** _temp_path ) { <nl>  <nl> f = fdopen ( fd , " we "); <nl> if (! f ) { <nl> - unlink ( t ); <nl> + unlink_noerrno ( t ); <nl> free ( t ); <nl> safe_close ( fd ); <nl> return - errno ;
static int adm_settle ( struct udev * udev , int argc , char * argv []) { <nl> break ; <nl> } <nl>  <nl> - if ( timeout > 0 && now ( CLOCK_MONOTONIC ) >= deadline ) <nl> + if ( now ( CLOCK_MONOTONIC ) >= deadline ) <nl> break ; <nl>  <nl> /* wake up when queue is empty */
int cg_is_empty_recursive ( const char * controller , const char * path ) { <nl>  <nl> assert ( path ); <nl>  <nl> + /* The root cgroup is always populated */ <nl> + if ( controller && ( isempty ( path ) || path_equal ( path , "/"))) <nl> + return 0 ; <nl> + <nl> r = cg_is_empty ( controller , path ); <nl> if ( r <= 0 ) <nl> return r ;
_public_ int sd_bus_try_close ( sd_bus * bus ) { <nl> assert_return (! bus_pid_changed ( bus ), - ECHILD ); <nl> assert_return ( bus -> is_kernel , - ENOTSUP ); <nl>  <nl> + if ( bus -> rqueue_size > 0 ) <nl> + return - EBUSY ; <nl> + <nl> r = bus_kernel_try_close ( bus ); <nl> if ( r < 0 ) <nl> return r ;
static void output_draw ( Output * o , bool menu , term_screen * screen ) { <nl> */ <nl>  <nl> static void terminal_dirty ( Terminal * t ) { <nl> - uint64_t usec ; <nl> + usec_t usec ; <nl> int r ; <nl>  <nl> assert ( t );
subst : <nl> cpos ++; <nl> while ( isspace ( cpos [ 0 ])) <nl> cpos ++; <nl> + if ( cpos [ 0 ] == '\ 0 ') <nl> + break ; <nl> } <nl> if ( i > 0 ) { <nl> log_error (" requested part of result string not found ");
static int map_netif ( sd_bus * bus , const char * member , sd_bus_message * m , sd_bus_ <nl> r = sd_bus_message_read_array ( m , SD_BUS_TYPE_INT32 , & v , & l ); <nl> if ( r < 0 ) <nl> return r ; <nl> + if ( r == 0 ) <nl> + return - EBADMSG ; <nl>  <nl> i -> n_netif = l / sizeof ( int32_t ); <nl> i -> netif = memdup ( v , l );
int switch_root ( const char * new_root ) { <nl> snprintf ( new_mount , sizeof ( new_mount ), "% s % s ", new_root , i ); <nl> char_array_0 ( new_mount ); <nl>  <nl> - mkdir_parents ( new_mount , 0755 ); <nl> + mkdir_p ( new_mount , 0755 ); <nl>  <nl> if (( stat ( new_mount , & sb ) < 0 ) || <nl> sb . st_dev != new_root_stat . st_dev ) {
static int export_legacy_dbus_address ( <nl> _cleanup_free_ char * s = NULL ; <nl> int r ; <nl>  <nl> + /* skip export if kdbus is not active */ <nl> + if ( access ("/ dev / kdbus ", F_OK ) < 0 ) <nl> + return PAM_SUCCESS ; <nl> + <nl> if ( asprintf (& s , KERNEL_USER_BUS_FMT ";" UNIX_USER_BUS_FMT , <nl> ( unsigned long ) uid , runtime ) < 0 ) { <nl> pam_syslog ( handle , LOG_ERR , " Failed to set bus variable .");
bool path_is_safe ( const char * p ) { <nl> if ( streq ( p , "..") || startswith ( p , "../") || endswith ( p , "/..") || strstr ( p , "/../")) <nl> return false ; <nl>  <nl> - if ( strlen ( p ) > PATH_MAX ) <nl> + if ( strlen ( p )+ 1 > PATH_MAX ) <nl> return false ; <nl>  <nl> /* The following two checks are not really dangerous , but hey , they still are confusing */
int dnssec_verify_rrset ( <nl> } <nl>  <nl> /* Collect all relevant RRs in a single array , so that we can look at the RRset */ <nl> - list = newa ( DnsResourceRecord *, a -> n_rrs ); <nl> + list = newa ( DnsResourceRecord *, dns_answer_size ( a )); <nl>  <nl> DNS_ANSWER_FOREACH ( rr , a ) { <nl> r = dns_resource_key_equal ( key , rr -> key );
int server_flush_to_var ( Server * s , bool require_flag_file ) { <nl> r = 0 ; <nl>  <nl> finish : <nl> - journal_file_post_change ( s -> system_journal ); <nl> + if ( s -> system_journal ) <nl> + journal_file_post_change ( s -> system_journal ); <nl>  <nl> s -> runtime_journal = journal_file_close ( s -> runtime_journal ); <nl> 
void initialize_srand ( void ) { <nl>  <nl> auxv = ( void *) getauxval ( AT_RANDOM ); <nl> if ( auxv ) { <nl> - assert_cc ( sizeof ( x ) < 16 ); <nl> + assert_cc ( sizeof ( x ) <= 16 ); <nl> memcpy (& x , auxv , sizeof ( x )); <nl> } else <nl> # endif
static void manager_clear_jobs_and_units ( Manager * m ) { <nl>  <nl> m -> n_on_console = 0 ; <nl> m -> n_running_jobs = 0 ; <nl> + m -> n_installed_jobs = 0 ; <nl> + m -> n_failed_jobs = 0 ; <nl> } <nl>  <nl> Manager * manager_free ( Manager * m ) {
int dns_packet_is_reply_for ( DnsPacket * p , const DnsResourceKey * key ) { <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> + if (! p -> question ) <nl> + return 0 ; <nl> + <nl> if ( p -> question -> n_keys != 1 ) <nl> return 0 ; <nl> 
static int parse_date ( const char ** p , CalendarSpec * c ) { <nl> c -> month = first ; <nl> c -> day = second ; <nl> return 0 ; <nl> - } else if ( c -> end_of_month ) <nl> + } else if ( c -> end_of_month ) { <nl> + free_chain ( first ); <nl> + free_chain ( second ); <nl> return - EINVAL ; <nl> + } <nl>  <nl> if (* t == '~') <nl> c -> end_of_month = true ;
static int service_collect_fds ( Service * s , int ** fds , unsigned * n_fds ) { <nl> p = manager_get_unit ( UNIT ( s )-> meta . manager , k ); <nl> free ( k ); <nl>  <nl> + if (! p ) <nl> + continue ; <nl> + <nl> if (( r = socket_collect_fds ( SOCKET ( p ), & cfds , & cn_fds )) < 0 ) <nl> goto fail ; <nl> 
static int client_initialize_time_events ( sd_dhcp_client * client ) { <nl>  <nl> r = sd_event_source_set_priority ( client -> timeout_resend , <nl> client -> event_priority ); <nl> + if ( r < 0 ) <nl> + goto error ; <nl>  <nl> r = sd_event_source_set_description ( client -> timeout_resend , " dhcp4 - resend - timer "); <nl> if ( r < 0 )
static int handle_response ( sd_resolve * resolve , const Packet * packet , size_t len <nl>  <nl> if ( ni_resp -> hostlen > DNS_HOSTNAME_MAX || <nl> ni_resp -> servlen > DNS_HOSTNAME_MAX || <nl> - sizeof ( NameInfoResponse ) + ni_resp -> hostlen + ni_resp -> servlen > length + 2 ) <nl> + sizeof ( NameInfoResponse ) + ni_resp -> hostlen + ni_resp -> servlen > length ) <nl> ASSIGN_ERRNO ( q , EAI_SYSTEM , EIO , 0 ); <nl>  <nl> else {
int bus_event_loop_with_idle ( <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> - if ( r == 0 && ! exiting ) { <nl> + if ( r == 0 && ! exiting && idle ) { <nl>  <nl> r = sd_bus_try_close ( bus ); <nl> if ( r == - EBUSY )
static int set_usb_mass_storage_ifsubtype ( char * to , const char * from , size_t len <nl> type = " floppy "; <nl> break ; <nl> case 1 : /* RBC devices */ <nl> + type = " rbc "; <nl> + break ; <nl> case 6 : /* Transparent SPC - 2 devices */ <nl> - type = " disk "; <nl> + type = " scsi "; <nl> break ; <nl> default : <nl> break ;
void _logsys_log_printf ( <nl> subsysid = LOGSYS_MAX_SUBSYS_COUNT ; <nl> } <nl>  <nl> + if (( level > logsys_loggers [ subsysid ]. syslog_priority ) && <nl> + ( level > logsys_loggers [ subsysid ]. logfile_priority )) { <nl> + return ; <nl> + } <nl> + <nl> va_start ( ap , format ); <nl> len = vsprintf ( logsys_print_buffer , format , ap ); <nl> va_end ( ap );
void main_deliver_fn ( <nl> log_printf ( instance -> totemsrp_log_level_security , " Received message is too short ... ignoring % d .\ n ", msg_len ); <nl> return ; <nl> } <nl> - <nl> + <nl> + if (( int ) message_header -> type >= totemsrp_message_handlers . count ) { <nl> + log_printf ( instance -> totemsrp_log_level_security , " Type of received message is wrong ... ignoring % d .\ n ", ( int ) message_header -> type ); <nl> + return ; <nl> + } <nl> + <nl> /* <nl> * Handle incoming message <nl> */
static void sigusr2_handler ( int num ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; ais_service [ i ]; i ++) { <nl> - if ( ais_service [ i ]-> exec_dump_fn ) { <nl> + for ( i = 0 ; i < SERVICE_HANDLER_MAXIMUM_COUNT ; i ++) { <nl> + if ( ais_service [ i ] && ais_service [ i ]-> exec_dump_fn ) { <nl> ais_service [ i ]-> exec_dump_fn (); <nl> } <nl> }
poll_handle poll_create ( <nl> poll_instance -> poll_entries = 0 ; <nl> poll_instance -> ufds = 0 ; <nl> poll_instance -> poll_entry_count = 0 ; <nl> - poll_instance -> serialize_lock_fn = serialize_unlock_fn ; <nl> + poll_instance -> serialize_lock_fn = serialize_lock_fn ; <nl> poll_instance -> serialize_unlock_fn = serialize_unlock_fn ; <nl> timerlist_init (& poll_instance -> timerlist ); <nl> 
static void unlink_all_completed ( void ) <nl> * here <nl> */ <nl> serialize_unlock (); <nl> + api -> timer_delete ( corosync_stats_timer_handle ); <nl> poll_stop ( corosync_poll_handle ); <nl> totempg_finalize (); <nl> 
static int init_nss_hash ( struct crypto_instance * instance ) <nl> } <nl>  <nl> hash_param . type = siBuffer ; <nl> - hash_param . data = 0 ; <nl> - hash_param . len = 0 ; <nl> + hash_param . data = instance -> private_key ; <nl> + hash_param . len = instance -> private_key_len ; <nl>  <nl> hash_slot = PK11_GetBestSlot ( hash_to_nss [ instance -> crypto_hash_type ], NULL ); <nl> if ( hash_slot == NULL ) {
static void memb_state_gather_enter ( <nl>  <nl> instance -> memb_state = MEMB_STATE_GATHER ; <nl> instance -> stats . gather_entered ++; <nl> - instance -> stats . continuous_gather ++; <nl> + <nl> + if ( gather_from == 3 ) { <nl> + /* <nl> + * State 3 means gather , so we are continuously gathering . <nl> + */ <nl> + instance -> stats . continuous_gather ++; <nl> + } <nl>  <nl> if ( instance -> stats . continuous_gather > MAX_NO_CONT_GATHER ) { <nl> log_printf ( instance -> totemsrp_log_level_warning ,
ev_document_misc_get_screen_dpi ( GdkScreen * screen ) <nl>  <nl> /* diagonal in pixels */ <nl> dp = hypot ( gdk_screen_get_width ( screen ), gdk_screen_get_height ( screen )); <nl> + if ( dp == 0 ) <nl> + return 96 ; <nl>  <nl> /* diagonal in inches */ <nl> di = hypot ( gdk_screen_get_width_mm ( screen ), gdk_screen_get_height_mm ( screen )) / 25 . 4 ; <nl> + if ( di == 0 ) <nl> + return 96 ; <nl>  <nl> return ( dp / di ); <nl> }
ev_view_motion_notify_event ( GtkWidget * widget , <nl> } <nl>  <nl> if ( view -> scroll_info . autoscrolling ) { <nl> - view -> scroll_info . last_y = y ; <nl> + if ( y >= 0 ) <nl> + view -> scroll_info . last_y = y ; <nl> return TRUE ; <nl> } <nl> 
ev_view_accessible_focus_changed ( GtkWidget * widget , <nl> g_return_val_if_fail ( EV_IS_VIEW ( widget ), FALSE ); <nl> g_return_val_if_fail ( EV_IS_VIEW_ACCESSIBLE ( self ), FALSE ); <nl>  <nl> - if ( self -> priv -> children == NULL ) <nl> + if ( self -> priv -> children == NULL || self -> priv -> children -> len == 0 ) <nl> return FALSE ; <nl>  <nl> page_accessible = g_ptr_array_index ( self -> priv -> children ,
ev_view_button_release_event ( GtkWidget * widget , <nl> view -> pressed_button = - 1 ; <nl>  <nl> return TRUE ; <nl> - } <nl> + } <nl> + <nl> + if ( view -> pressed_button == 1 && event -> state & GDK_CONTROL_MASK ) { <nl> + view -> pressed_button = - 1 ; <nl> + return TRUE ; <nl> + } <nl>  <nl> if ( view -> drag_info . in_drag ) { <nl> view -> drag_info . release_timeout_id =
ev_view_select_all ( EvView * view ) <nl> } <nl>  <nl> merge_selection_region ( view , g_list_reverse ( selections )); <nl> - gtk_widget_queue_draw ( GTK_WIDGET ( view )); <nl> } <nl>  <nl> gboolean
end_element_handler ( GMarkupParseContext * markup_context , <nl> case STATE_INSIDE_ALT : <nl> case STATE_INSIDE_BAG : <nl> case STATE_INSIDE_SEQ : <nl> + if ( context -> property && context -> prop_cur_value < 0 ) <nl> + { <nl> + g_free ( context -> property ); <nl> + context -> property = NULL ; <nl> + } <nl> context -> state = STATE_INSIDE_PROPERTY ; <nl> break ; <nl> 
gimp_prop_widget_new_from_pspec ( GObject * config , <nl>  <nl> buffer = gimp_prop_text_buffer_new ( config , pspec -> name , - 1 ); <nl> view = gtk_text_view_new_with_buffer ( buffer ); <nl> + g_object_unref ( buffer ); <nl>  <nl> widget = gtk_scrolled_window_new ( NULL , NULL ); <nl> gtk_scrolled_window_set_shadow_type ( GTK_SCROLLED_WINDOW ( widget ),
choose_format ( GeglBuffer * buffer , <nl> break ; <nl>  <nl> case GIMP_SELECT_CRITERION_LCH_L : <nl> + format = babl_format (" CIE L alpha float "); <nl> + break ; <nl> + <nl> case GIMP_SELECT_CRITERION_LCH_C : <nl> case GIMP_SELECT_CRITERION_LCH_H : <nl> format = babl_format (" CIE LCH ( ab ) alpha float ");
wilber_get_extents ( cairo_t * cr ) <nl> cairo_fill_extents ( cr , & wilber_x1 , & wilber_y1 , & wilber_x2 , & wilber_y2 ); <nl>  <nl> wilber_cairo_path = cairo_copy_path ( cr ); <nl> + cairo_new_path ( cr ); <nl>  <nl> cairo_restore ( cr ); <nl> }
d_load_object ( gchar * desc , <nl> if ( sscanf ( buf , "% d ", & new_obj -> type_data ) != 1 ) <nl> { <nl> g_message (" Error while loading object ( no type data )"); <nl> + g_free ( new_obj ); <nl> return NULL ; <nl> } <nl>  <nl> d_load_object ( gchar * desc , <nl> if ( strcmp ("</ EXTRA >", buf )) <nl> { <nl> g_message (" Syntax error while loading object "); <nl> + g_free ( new_obj ); <nl> return NULL ; <nl> } <nl> /* Go around and read the last line */
gimp_levels_tool_config_notify ( GimpFilterTool * filter_tool , <nl> GIMP_FILTER_TOOL_CLASS ( parent_class )-> config_notify ( filter_tool , <nl> config , pspec ); <nl>  <nl> - if (! levels_tool -> channel_menu ) <nl> + if (! levels_tool -> channel_menu || <nl> + ! levels_tool -> histogram_view ) <nl> return ; <nl>  <nl> if (! strcmp ( pspec -> name , " linear "))
save_layer ( const gchar * filename , <nl> gimp_filename_to_utf8 ( filename )); <nl>  <nl> /* Attempt to open the output file */ <nl> - if (( outfile = g_fopen ( filename , " wb +")) == NULL ) <nl> + if (( outfile = g_fopen ( filename , " w + b ")) == NULL ) <nl> { <nl> g_set_error ( error , G_FILE_ERROR , <nl> g_file_error_from_errno ( errno ),
xcf_init ( Gimp * gimp ) <nl> " Filename ", <nl> " The name of the file " <nl> " to save the image in , " <nl> - " in the on - disk " <nl> - " character set and " <nl> - " encoding ", <nl> + " in URI format and " <nl> + " UTF - 8 encoding ", <nl> TRUE , FALSE , TRUE , <nl> NULL , <nl> GIMP_PARAM_READWRITE ));
gimp_container_tree_view_constructor ( GType type , <nl> tree_view -> main_column = gtk_tree_view_column_new (); <nl> gtk_tree_view_insert_column ( tree_view -> view , tree_view -> main_column , 0 ); <nl>  <nl> + gtk_tree_view_set_expander_column ( tree_view -> view , tree_view -> main_column ); <nl> + <nl> tree_view -> renderer_cell = gimp_cell_renderer_viewable_new (); <nl> gtk_tree_view_column_pack_start ( tree_view -> main_column , <nl> tree_view -> renderer_cell ,
gimp_image_map_tool_response ( GtkWidget * widget , <nl>  <nl> gimp_image_flush ( gimp_display_get_image ( tool -> display )); <nl>  <nl> - if ( image_map_tool -> config ) <nl> + if ( image_map_tool -> config && image_map_tool -> settings_box ) <nl> gimp_settings_box_add_current ( GIMP_SETTINGS_BOX ( image_map_tool -> settings_box ), <nl> GIMP_GUI_CONFIG ( tool -> tool_info -> gimp -> config )-> image_map_tool_max_recent ); <nl> }
gimp_tile_backend_plugin_command ( GeglTileSource * tile_store , <nl> break ; <nl>  <nl> default : <nl> - g_assert ( command < GEGL_TILE_LAST_COMMAND && command >= 0 ); <nl> + /* g_assert ( command < GEGL_TILE_LAST_COMMAND && command >= 0 ); */ <nl> + break ; <nl> } <nl>  <nl> return result ;
p_vertical_bend ( BenderDialog * cd , <nl> } <nl> } <nl> } <nl> + <nl> + g_free ( last_arr ); <nl> + g_free ( first_arr ); <nl> } <nl>  <nl> /* ============================================================================
metadata_message_dialog ( GtkMessageType type , <nl> { <nl> GtkWidget * dlg ; <nl>  <nl> - dlg = gtk_message_dialog_new ( parent , 0 , type , GTK_BUTTONS_OK , message ); <nl> + dlg = gtk_message_dialog_new ( parent , 0 , type , GTK_BUTTONS_OK , "% s ", message ); <nl>  <nl> if ( title ) <nl> gtk_window_set_title ( GTK_WINDOW ( dlg ), title );
gimp_histogram_view_notify ( GimpHistogram * histogram , <nl> static void <nl> gimp_histogram_view_update_bins ( GimpHistogramView * view ) <nl> { <nl> - gint new_bins ; <nl> + gint new_bins = 256 ; <nl>  <nl> if ( view -> histogram ) <nl> new_bins = gimp_histogram_n_bins ( view -> histogram );
gui_unique_win32_idle_open ( IdleOpenData * data ) <nl> if ( data -> file ) <nl> { <nl> file_open_from_command_line ( unique_gimp , data -> file , <nl> - data -> as_new , NULL , 0 ); <nl> + data -> as_new , NULL ); <nl> } <nl> else <nl> { <nl> gui_unique_quartz_idle_open ( GFile * file ) <nl>  <nl> if ( file ) <nl> { <nl> - file_open_from_command_line ( unique_gimp , file , FALSE , NULL , 0 ); <nl> + file_open_from_command_line ( unique_gimp , file , FALSE , NULL ); <nl> } <nl>  <nl> return FALSE ;
gimp_drawable_sync_fs_filter ( GimpDrawable * drawable , <nl> GeglNode * fs_source ; <nl>  <nl> private -> fs_filter = gimp_filter_new (" Floating Selection "); <nl> + gimp_viewable_set_stock_id ( GIMP_VIEWABLE ( private -> fs_filter ), <nl> + " gimp - floating - selection "); <nl>  <nl> node = gimp_filter_get_node ( private -> fs_filter ); <nl> 
save_dialog ( void ) <nl> g_signal_connect ( toggle , " toggled ", <nl> G_CALLBACK ( gimp_toggle_button_update ), <nl> & jsvals . save_xmp ); <nl> + g_signal_connect ( toggle , " toggled ", <nl> + G_CALLBACK ( make_preview ), <nl> + NULL ); <nl>  <nl> gtk_toggle_button_set_active ( GTK_TOGGLE_BUTTON ( toggle ), <nl> jsvals . save_xmp && has_metadata );
layer_options_dialog_new ( GimpImage * image , <nl> GimpContainer * filters ; <nl> GtkWidget * view ; <nl>  <nl> - frame = gimp_frame_new (" Active Filters "); <nl> + frame = gimp_frame_new ( _ (" Active Filters ")); <nl> gtk_box_pack_start ( GTK_BOX ( left_vbox ), frame , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( frame ); <nl> 
gimp_rectangle_select_tool_cursor_update ( GimpTool * tool , <nl> { <nl> gimp_tool_widget_get_cursor ( private -> widget , coords , state , <nl> & cursor , NULL , & modifier ); <nl> - <nl> - gimp_tool_control_set_cursor ( tool -> control , cursor ); <nl> - gimp_tool_control_set_cursor_modifier ( tool -> control , modifier ); <nl> } <nl>  <nl> + gimp_tool_control_set_cursor ( tool -> control , cursor ); <nl> + gimp_tool_control_set_cursor_modifier ( tool -> control , modifier ); <nl> + <nl> /* override the previous if shift or ctrl are down */ <nl> if ( state & ( gimp_get_extend_selection_mask () | <nl> gimp_get_modify_selection_mask ()))
gimp_prop_kelvin_presets_new ( GObject * config , <nl> menu = gtk_menu_new (); <nl> gtk_menu_attach_to_widget ( GTK_MENU ( menu ), button , NULL ); <nl>  <nl> + gimp_help_set_help_data ( button , <nl> + _ (" Choose from a list of common " <nl> + " color temperatures "), NULL ); <nl> + <nl> g_signal_connect ( button , " button - press - event ", <nl> G_CALLBACK ( gimp_prop_kelvin_presets_button_press ), <nl> menu );
gimp_image_map_tool_initialize ( GimpTool * tool , <nl>  <nl> window = gimp_display_shell_get_window ( display_shell ); <nl>  <nl> - image_map_tool -> overlay = gimp_image_window_get_fullscreen ( window ); <nl> + /* disabled for at least GIMP 2 . 8 */ <nl> + image_map_tool -> overlay = FALSE ; <nl>  <nl> if ( image_map_tool -> overlay ) <nl> {
gimp_transform_tool_commit ( GimpTransformTool * tr_tool ) <nl> if ( tr_tool -> gui ) <nl> gimp_tool_gui_hide ( tr_tool -> gui ); <nl>  <nl> - if ( gimp_matrix3_is_identity (& tr_tool -> transform )) <nl> + if ( GIMP_TRANSFORM_TOOL_GET_CLASS ( tr_tool )-> recalc_matrix && <nl> + gimp_matrix3_is_identity (& tr_tool -> transform )) <nl> { <nl> /* No need to commit an identity transformation ! */ <nl> return ;
unsigned int <nl> hb_ot_layout_table_get_lookup_count ( hb_face_t * face , <nl> hb_tag_t table_tag ) <nl> { <nl> + if ( unlikely (! hb_ot_shaper_face_data_ensure ( face ))) return 0 ; <nl> switch ( table_tag ) <nl> { <nl> case HB_OT_TAG_GSUB :
struct hb_ot_face_metrics_accelerator_t <nl>  <nl> this -> blob = OT :: Sanitizer < OT :: _mtx >:: sanitize ( face -> reference_table ( _mtx_tag )); <nl> if ( unlikely (! this -> num_advances || <nl> - 2 * ( this -> num_advances + this -> num_metrics ) < hb_blob_get_length ( this -> blob ))) <nl> + 2 * ( this -> num_advances + this -> num_metrics ) > hb_blob_get_length ( this -> blob ))) <nl> { <nl> this -> num_metrics = this -> num_advances = 0 ; <nl> hb_blob_destroy ( this -> blob );
static HB_Error Load_Mark2Array ( HB_Mark2Array * m2a , <nl>  <nl> FORGET_Frame (); <nl>  <nl> + if ( new_offset == base_offset ) { <nl> + /* Anchor table not provided . Skip loading . <nl> + * Some versions of FreeSans hit this . */ <nl> + m2an [ n ]. PosFormat = 0 ; <nl> + continue ; <nl> + } <nl> + <nl> cur_offset = FILE_Pos (); <nl> if ( FILE_Seek ( new_offset ) || <nl> ( error = Load_Anchor ( & m2an [ n ], stream ) ) != HB_Err_Ok )
int merge_many_buff ( Sort_param * param , uchar * sort_buffer , <nl> ulong read_to_buffer ( IO_CACHE * fromfile , BUFFPEK * buffpek , <nl> uint rec_length ) <nl> { <nl> - register ulong count ; <nl> + ulong count ; <nl> ulong length = 0 ; <nl>  <nl> if (( count = ( ulong ) MY_MIN (( ha_rows ) buffpek -> max_keys , buffpek -> count )))
btr_search_guess_on_hash ( <nl> } <nl>  <nl> block = buf_block_align ( rec ); <nl> - page = buf_block_get_frame ( block ); <nl> + page = page_align ( rec ); <nl>  <nl> if ( UNIV_LIKELY (! has_search_latch )) { <nl> 
static int run_query ( const char * query , DYNAMIC_STRING * ds_res , <nl> NULL ); <nl>  <nl> my_close ( fd , MYF ( 0 )); <nl> + my_delete ( query_file_path , MYF ( 0 )); <nl>  <nl> DBUG_RETURN ( ret ); <nl> }
lock_update_discard ( <nl> lock_rec_reset_and_release_wait ( block , heap_no ); <nl>  <nl> rec = page + rec_get_next_offs ( rec , TRUE ); <nl> - } while ( heap_no != PAGE_NEW_SUPREMUM ); <nl> + } while ( heap_no != PAGE_HEAP_NO_SUPREMUM ); <nl> } else { <nl> rec = page + PAGE_OLD_INFIMUM ; <nl>  <nl> lock_update_discard ( <nl> lock_rec_reset_and_release_wait ( block , heap_no ); <nl>  <nl> rec = page + rec_get_next_offs ( rec , FALSE ); <nl> - } while ( heap_no != PAGE_OLD_SUPREMUM ); <nl> + } while ( heap_no != PAGE_HEAP_NO_SUPREMUM ); <nl> } <nl>  <nl> lock_rec_free_all_from_discard_page ( block );
int serialize_brtnode_to ( int fd , diskoff off , diskoff size , BRTNODE node ) { <nl>  <nl> // printf ("% s :% d wrote % d bytes for % lld size =% lld \ n ", __FILE__ , __LINE__ , w . ndone , off , size ); <nl> assert ( w . ndone <= size ); <nl> - toku_free ( w . buf ); <nl> return 0 ; <nl> } <nl> 
_rl_fix_last_undo_of_type ( type , start , end ) <nl>  <nl> for ( rl = rl_undo_list ; rl ; rl = rl -> next ) <nl> { <nl> - if ( rl -> what == ( uint ) type ) <nl> + if ( rl -> what == ( unsigned int ) type ) <nl> { <nl> rl -> start = start ; <nl> rl -> end = end ;
static void wsrep_mysql_parse ( THD * thd , char * rawbuf , uint length , <nl> } <nl> mysql_mutex_unlock (& thd -> LOCK_wsrep_thd ); <nl> } <nl> + <nl> + /* If retry is requested clean up explain structure */ <nl> + if ( thd -> wsrep_conflict_state == RETRY_AUTOCOMMIT && thd -> lex -> explain ) <nl> + delete_explain_query ( thd -> lex ); <nl> + <nl> } while ( thd -> wsrep_conflict_state == RETRY_AUTOCOMMIT ); <nl>  <nl> if ( thd -> wsrep_retry_query )
ha_innobase :: add_index ( <nl>  <nl> func_exit : <nl> mem_heap_free ( heap ); <nl> + innobase_commit_low ( trx );/* work around a bug in mysql_alter_table () */ <nl>  <nl> /* There might be work for utility threads .*/ <nl> srv_active_wake_master_thread ();
btr_cur_optimistic_insert ( <nl> buf_block_get_page_no ( block ), max_size , <nl> rec_size + PAGE_DIR_SLOT_SIZE , index -> type ); <nl> # endif <nl> - if ( leaf <nl> - && ! dict_index_is_clust ( index ) <nl> - && ! dict_index_is_ibuf ( index )) { <nl> + if ( leaf && ! dict_index_is_clust ( index )) { <nl> /* Update the free bits of the B - tree page in the <nl> insert buffer bitmap . */ <nl> 
mysql_init ( MYSQL * mysql ) <nl> bzero (( char *) ( mysql ), sizeof (*( mysql ))); <nl> mysql -> options . connect_timeout = CONNECT_TIMEOUT ; <nl> mysql -> last_used_con = mysql -> next_slave = mysql -> master = mysql ; <nl> + mysql -> charset = default_charset_info ; <nl> strmov ( mysql -> net . sqlstate , not_error_sqlstate ); <nl> /* <nl> By default , we are a replication pivot . The caller must reset it
pstack_install_segv_action ( const char * path_format_ ) <nl> if (( abfd = load_bfd ( pid ))== 0 ) <nl> fprintf ( stderr , " BFD load failed ..\ n "); <nl> else { <nl> - long storage_needed = bfd_get_symtab_upper_bound ( abfd ); <nl> + long storage_needed = ( bfd_get_file_flags ( abfd ) & HAS_SYMS ) ? <nl> + bfd_get_symtab_upper_bound ( abfd ) : 0 ; <nl> long i ; <nl> ( void ) i ; <nl> 
sync_array_signal_object ( <nl>  <nl> if ( cell_count == cell_max_count ) { <nl> sync_cell_t ** old_cell_ptr = cell_ptr ; <nl> - size_t old_size = cell_max_count * <nl> + size_t old_size , new_size ; <nl> + <nl> + old_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl> cell_max_count *= 2 ; <nl> - <nl> - size_t new_size = cell_max_count * <nl> + new_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl>  <nl> cell_ptr = malloc ( new_size );
row_sel_sec_rec_is_for_clust_rec ( <nl> clust_index -> table ))) { <nl> goto inequal ; <nl> } <nl> + <nl> + continue ; <nl> } <nl> } <nl> 
i_s_zip_fill_low ( <nl>  <nl> DBUG_ENTER (" i_s_zip_fill_low "); <nl>  <nl> + /* deny access to non - superusers */ <nl> + if ( check_global_access ( thd , SUPER_ACL )) { <nl> + <nl> + DBUG_RETURN ( 0 ); <nl> + } <nl> + <nl> /* Determine log2 ( PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ). */ <nl> for ( uint r = PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ; r >>= 1 ; y ++); <nl> 
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
sync_array_signal_object ( <nl>  <nl> if ( cell_count == cell_max_count ) { <nl> sync_cell_t ** old_cell_ptr = cell_ptr ; <nl> - size_t old_size = cell_max_count * <nl> + size_t old_size , new_size ; <nl> + <nl> + old_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl> cell_max_count *= 2 ; <nl> - <nl> - size_t new_size = cell_max_count * <nl> + new_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl>  <nl> cell_ptr = malloc ( new_size );
bool check_change_password ( THD * thd , const char * host , const char * user , <nl> return ( 1 ); <nl> } <nl> uint len = strlen ( new_password ); <nl> - if ( len != SCRAMBLED_PASSWORD_CHAR_LENGTH && <nl> + if ( len && len != SCRAMBLED_PASSWORD_CHAR_LENGTH && <nl> len != SCRAMBLED_PASSWORD_CHAR_LENGTH_323 ) <nl> { <nl> net_printf ( thd , 0 ,
extern void bitmap_set_prefix ( MY_BITMAP * map , uint prefix_size ); <nl> extern void bitmap_intersect ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> extern void bitmap_subtract ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> extern void bitmap_union ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> + extern void bitmap_xor ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> + extern void bitmap_invert ( MY_BITMAP * map ); <nl>  <nl> extern uint bitmap_lock_set_next ( MY_BITMAP * map ); <nl> extern void bitmap_lock_clear_bit ( MY_BITMAP * map , uint bitmap_bit );
bool rollback_log_node_cache :: give_rollback_log_node ( TOKUTXN txn , ROLLBACK_LOG_N <nl> if ( m_num_avail < m_max_num_avail ) { <nl> retval = true ; <nl> uint32_t index = m_first + m_num_avail ; <nl> - if ( index > m_max_num_avail ) { <nl> + if ( index >= m_max_num_avail ) { <nl> index -= m_max_num_avail ; <nl> } <nl> m_avail_blocknums [ index ]. b = log -> blocknum . b ;
int create_cachetable ( CACHETABLE * result , int n_entries ) { <nl> int i ; <nl> t -> n_in_table = 0 ; <nl> t -> table_size = n_entries ; <nl> - t -> table = toku_calloc ( t -> table_size , sizeof ( struct ctpair )); <nl> + MALLOC_N ( t -> table_size , t -> table ); <nl> assert ( t -> table ); <nl> t -> head = t -> tail = 0 ; <nl> for ( i = 0 ; i < t -> table_size ; i ++) {
toku_minicron_change_period ( struct minicron * p , u_int32_t new_period ) <nl> int <nl> toku_minicron_shutdown ( struct minicron * p ) { <nl> int r = toku_pthread_mutex_lock (& p -> mutex ); assert ( r == 0 ); <nl> + assert (! p -> do_shutdown ); <nl> p -> do_shutdown = TRUE ; <nl> // printf ("% s :% d signalling \ n ", __FILE__ , __LINE__ ); <nl> r = toku_pthread_cond_signal (& p -> condvar ); assert ( r == 0 );
build_index ( DB_INDEXER * indexer ) { <nl> else { <nl> invariant ( prov_info . le ); <nl> invariant ( prov_info . ule ); <nl> - LEAFENTRY le = prov_info . le ; <nl> ULEHANDLE ule = prov_info . ule ; <nl> for ( int which_db = 0 ; ( which_db < indexer -> i -> N ) && ( result == 0 ); which_db ++) { <nl> DB * db = indexer -> i -> dest_dbs [ which_db ];
sp_head :: check_unresolved_goto () <nl> if ( m_backpatch_goto . elements > 0 ) <nl> { <nl> List_iterator_fast < bp_t > li ( m_backpatch_goto ); <nl> - bp_t * bp ; <nl> - while (( bp = li ++)) <nl> + while ( bp_t * bp = li ++) <nl> { <nl> - if (( bp -> instr_type == GOTO )) <nl> + if ( bp -> instr_type == GOTO ) <nl> { <nl> my_error ( ER_SP_LILABEL_MISMATCH , MYF ( 0 ), " GOTO ", bp -> lab -> name . str ); <nl> has_unresolved_label = true ;
buf_LRU_block_remove_hashed_page ( <nl> void * data = bpage -> zip . data ; <nl> bpage -> zip . data = NULL ; <nl>  <nl> + ut_ad (! bpage -> in_free_list ); <nl> + ut_ad (! bpage -> in_flush_list ); <nl> + ut_ad (! bpage -> in_LRU_list ); <nl> mutex_exit (&(( buf_block_t *) bpage )-> mutex ); <nl> buf_pool_mutex_exit_forbid (); <nl> buf_buddy_free ( data , page_zip_get_size (& bpage -> zip ));
trx_undo_parse_page_header ( <nl> mtr_t * mtr ) /*!< in : mtr or NULL */ <nl> { <nl> trx_id_t trx_id ; <nl> + /* Silence a GCC warning about possibly uninitialized variable <nl> + when mach_ull_parse_compressed () is not inlined . */ <nl> + ut_d ( trx_id = 0 ); <nl> + /* Declare the variable uninitialized in Valgrind , so that the <nl> + above initialization will not mask any bugs . */ <nl> + UNIV_MEM_INVALID (& trx_id , sizeof trx_id ); <nl>  <nl> ptr = mach_ull_parse_compressed ( ptr , end_ptr , & trx_id ); <nl> 
int ha_tokudb :: delete_all_rows () { <nl> txn <nl> ); <nl> if ( error ) { goto cleanup ; } <nl> + error = share -> key_file [ i ]-> pre_acquire_table_lock ( <nl> + share -> key_file [ i ], <nl> + txn <nl> + ); <nl> + if ( error ) { goto cleanup ; } <nl> } <nl> for ( uint i = 0 ; i < curr_num_DBs ; i ++) { <nl> error = truncate_dictionary ( i , txn );
bool ZIPUTIL :: WildMatch ( PSZ pat , PSZ str ) { <nl> if (!*++ pat ) return TRUE ; <nl> goto loopStart ; <nl> default : <nl> - if ( mapCaseTable [( unsigned )* s ] != mapCaseTable [( unsigned )* p ]) <nl> + if ( mapCaseTable [( uchar )* s ] != mapCaseTable [( uchar )* p ]) <nl> goto starCheck ; <nl> break ; <nl> } /* endswitch */
int toku_cachefile_fd ( CACHEFILE cf ) { <nl> } <nl>  <nl> int toku_cachefile_truncate0 ( CACHEFILE cf ) { <nl> - int r = toku_graceful_dirty ( cachefile ); <nl> + int r ; <nl> + r = toku_graceful_dirty ( cf ); <nl> if ( r != 0 ) return r ; <nl> - int r = ftruncate ( cf -> fd , 0 ); <nl> + r = ftruncate ( cf -> fd , 0 ); <nl> if ( r != 0 ) <nl> r = errno ; <nl> return r ;
referenced_xids_note_snapshot_txn_end_iter ( OMTVALUE live_xidv , u_int32_t UU ( inde <nl> if (-- tuple -> references == 0 ) { <nl> r = toku_omt_delete_at ( referenced_xids , idx ); <nl> lazy_assert_zero ( r ); <nl> + toku_free ( tuple ); <nl> } <nl> done : <nl> return 0 ;
dict_index_add_to_cache ( <nl>  <nl> if (! dict_index_find_cols ( table , index )) { <nl>  <nl> + dict_mem_index_free ( index ); <nl> return ( DB_CORRUPTION ); <nl> } <nl> 
__weak_alias ( vis , _vis ) <nl> # define MAXEXTRAS 5 <nl>  <nl>  <nl> - char * MAKEEXTRALIST ( uint flag , const char * orig ) <nl> + char * MAKEEXTRALIST ( unsigned int flag , const char * orig ) <nl> { <nl> const char * o = orig ; <nl> char * e , * extra ;
int subselect_indexsubquery_engine :: exec () <nl> null_finding = 1 ; <nl> /* Check if there exists a row with a null value in the index */ <nl> if (( error = safe_index_read ( tab ))) <nl> + { <nl> + if ( error < 0 ) <nl> + error = 0 ; // Key not found <nl> break ; <nl> + } <nl> } <nl> } <nl> }
enum tablespace_op_type <nl> e ||+-------------------------+ || <nl> V | neighbor | V | <nl> unit1 . 1 <+==================> unit1 . 2 unit2 . 1 <nl> - fake1 . 1 fake2 . 1 <nl> - select1 . 1 . 1 select 1 . 1 . 2 select1 . 2 . 1 select2 . 1 . 1 select2 . 1 . 2 <nl> + fake1 . 1 <nl> + select1 . 1 . 1 select 1 . 1 . 2 select1 . 2 . 1 select2 . 1 . 1 <nl> |^ <nl> || <nl> V |
void mysql_client_binlog_statement ( THD * thd ) <nl>  <nl> const char * error = 0 ; <nl> char * buf = ( char *) my_malloc ( event_len , MYF ( MY_WME )); <nl> - Log_event * ev ; <nl> + Log_event * ev = 0 ; <nl> int res ; <nl>  <nl> /*
static uint remove_key ( MI_KEYDEF * keyinfo , uint nod_flag , <nl> else <nl> get_key_length ( rest_length , keypos ); <nl>  <nl> - if ( next_length > prev_length ) <nl> + if ( next_length >= prev_length ) <nl> { /* Key after is based on deleted key */ <nl> uint pack_length , tmp ; <nl> bmove_upp (( char *) keypos ,( char *) ( lastkey + next_length ),
int decimal_round ( decimal * from , decimal * to , int scale , decimal_round_mode mode <nl> error = E_DEC_TRUNCATED ; <nl> } <nl>  <nl> - if ( scale + from -> intg < 0 ) <nl> + if ( scale + from -> intg <= 0 ) <nl> { <nl> decimal_make_zero ( to ); <nl> return E_DEC_OK ;
TABLE * Delayed_insert :: get_local_table ( THD * client_thd ) <nl> goto error ; <nl> dfield_ptr = copy -> default_field ; <nl> } <nl> + copy -> expr_arena = NULL ; <nl>  <nl> /* Ensure we don ' t use the table list of the original table */ <nl> copy -> pos_in_table_list = 0 ;
row_purge_remove_sec_if_poss_low ( <nl>  <nl> mtr_start (& mtr ); <nl>  <nl> - btr_cur -> thr = que_node_get_parent ( node ); <nl> - <nl> row_search_index_entry (& was_buffered , index , entry , <nl> BTR_MODIFY_LEAF | BTR_DELETE , & pcur , <nl> & mtr );
void OPJ_CALLCONV opj_stream_set_skip_function ( opj_stream_t * p_stream , opj_strea <nl> void OPJ_CALLCONV opj_stream_set_user_data ( opj_stream_t * p_stream , void * p_data ) <nl> { <nl> opj_stream_private_t * l_stream = ( opj_stream_private_t *) p_stream ; <nl> + if (! l_stream ) <nl> + return ; <nl> l_stream -> m_user_data = p_data ; <nl> } <nl>  <nl> void OPJ_CALLCONV opj_stream_set_user_data_length ( opj_stream_t * p_stream , OPJ_UINT64 data_length ) <nl> { <nl> opj_stream_private_t * l_stream = ( opj_stream_private_t *) p_stream ; <nl> + if (! l_stream ) <nl> + return ; <nl> l_stream -> m_user_data_length = data_length ; <nl> } <nl> 
static int tga_readheader ( FILE * fp , unsigned int * bits_per_pixel , <nl> if ( fread ( tga , TGA_HEADER_SIZE , 1 , fp ) != 1 ) { <nl> fprintf ( stderr , <nl> "\ nError : fread return a number of element different from the expected .\ n "); <nl> + free ( tga ); <nl> return 0 ; <nl> } <nl> id_len = ( unsigned char ) tga [ 0 ];
void opj_lupSolve ( OPJ_FLOAT32 * pResult , <nl> lTmpMatrix = lLineMatrix ; <nl> u = *( lTmpMatrix ++); <nl> lCurrentPtr = lDestPtr --; <nl> - for ( j = k + 1 ; j < nb_compo ; ++ j ) { <nl> + for ( j = ( OPJ_UINT32 )( k + 1 ); j < nb_compo ; ++ j ) { <nl> /* sum += matrix [ k ][ j ] * x [ j ] */ <nl> sum += (*( lTmpMatrix ++)) * (*( lCurrentPtr ++)); <nl> }
static opj_bool t2_read_packet_data ( <nl>  <nl> # endif /* USE_JPWL */ <nl>  <nl> + if (( l_cblk -> len + l_seg -> newlen ) > 8192 ) { <nl> + return OPJ_FALSE ; <nl> + } <nl> + <nl> memcpy ( l_cblk -> data + l_cblk -> len , l_current_data , l_seg -> newlen ); <nl>  <nl> if ( l_seg -> numpasses == 0 ) {
buf_shrink_freelists ( int free_all ) <nl> -- n_to_free ; <nl> } <nl> tor_assert (! n_to_free ); <nl> - freelists [ i ]. lowest_length = freelists [ i ]. cur_length = n_to_skip ; <nl> + freelists [ i ]. cur_length = n_to_skip ; <nl> } <nl> + freelists [ i ]. lowest_length = freelists [ i ]. cur_length ; <nl> assert_freelist_ok (& freelists [ i ]); <nl> } <nl> }
int connection_dns_finished_flushing ( connection_t * conn ) { <nl> int connection_dns_reached_eof ( connection_t * conn ) { <nl> log_fn ( LOG_WARN ," Read eof . Worker died unexpectedly ."); <nl> if ( conn -> state == DNSWORKER_STATE_BUSY ) { <nl> - dns_cancel_pending_resolve ( conn -> address ); <nl> + /* don ' t cancel the resolve here -- it would be cancelled in <nl> + * connection_about_to_close_connection (), since conn is still <nl> + * in state BUSY <nl> + */ <nl> num_dnsworkers_busy --; <nl> } <nl> num_dnsworkers --;
rend_service_set_connection_addr_port ( connection_t * conn , circuit_t * circ ) <nl> serviceid , circ -> n_circ_id ); <nl> circuit_mark_for_close ( circ ); <nl> connection_mark_for_close ( conn , 0 /* XXX */); <nl> + return - 1 ; <nl> } <nl> for ( i = 0 ; i < smartlist_len ( service -> ports ); ++ i ) { <nl> p = smartlist_get ( service -> ports , i );
extern INLINE double U64_TO_DBL ( uint64_t x ) { <nl> # endif <nl>  <nl> /* GCC has several useful attributes . */ <nl> -# ifdef __GNUC__ <nl> +# ifdef __GNUC__ && __GNUC_MAJOR__ >= 3 <nl> # define ATTR_NORETURN __attribute__ (( noreturn )) <nl> # define ATTR_PURE __attribute__ (( pure )) <nl> # define ATTR_MALLOC __attribute__ (( malloc ))
entry_guard_register_connect_status ( const char * digest , int succeeded , <nl> entry -> nickname , buf , tbuf ); <nl> entry -> last_attempted = now ; <nl> } <nl> - entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> + if ( entry ) <nl> + entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> } <nl>  <nl> if ( first_contact ) {
tor_tls_check_lifetime ( int severity , tor_tls_t * tls , <nl> * < b > future_tolerance </ b > seconds . If it is live , return 0 . If it is not <nl> * live , log a message and return - 1 . */ <nl> static int <nl> - check_cert_lifetime_internal ( int severity , const X509 * cert , int past_tolerance , <nl> - int future_tolerance ) <nl> + check_cert_lifetime_internal ( int severity , const X509 * cert , <nl> + int past_tolerance , int future_tolerance ) <nl> { <nl> time_t now , t ; <nl> 
router_dump_router_to_string ( char * s , size_t maxlen , routerinfo_t * router , <nl> if ( result < 0 ) <nl> return - 1 ; <nl> written += result ; <nl> - if ( written < maxlen + 2 ) <nl> + if ( written + 2 > maxlen ) <nl> return - 1 ; <nl> s [ written ++] = '\ n '; <nl> }
directory_remove_invalid ( void ) <nl> directory_set_dirty (); <nl>  <nl> routerlist_assert_ok ( rl ); <nl> + smartlist_free ( nodes ); <nl> } <nl>  <nl> /** Mark the directory as < b > dirty </ b > -- when we ' re next asked for a
consensus_queue_compression_work ( const char * consensus , <nl> config_line_prepend (& job -> labels_in , LABEL_SIGNATORIES , signers ); <nl> tor_free ( signers ); <nl> SMARTLIST_FOREACH ( hexvoters , char *, cp , tor_free ( cp )); <nl> + smartlist_free ( hexvoters ); <nl> } <nl>  <nl> if ( background_compression ) {
circuit_expire_building ( time_t now ) <nl> /* c_rend_ready circs measure age since timestamp_dirty , <nl> * because that ' s set when they switch purposes <nl> */ <nl> - if (! victim -> rend_query [ 0 ] || victim -> timestamp_dirty <= cutoff ) <nl> + if (! victim -> rend_query [ 0 ] || victim -> timestamp_dirty > cutoff ) <nl> continue ; <nl> break ; <nl> case CIRCUIT_PURPOSE_C_REND_READY_INTRO_ACKED :
add_an_entry_guard ( routerinfo_t * chosen ) <nl>  <nl> again : <nl> if (-- tries_left <= 0 ) { <nl> - log_warn ( LD_CIRC , " Tried finding a new entry , but failed . Bad news . XXX ."); <nl> + log_warn ( LD_CIRC , " Tried finding a new entry guard , but failed . " <nl> + " Can you reach the Tor network ?"); <nl> return NULL ; <nl> } <nl> if ( chosen )
add_an_entry_guard ( const node_t * chosen , int reset_status , int prepend , <nl> entry -> can_retry = 1 ; <nl> } <nl> entry -> is_dir_cache = node_is_dir ( node ); <nl> + if ( get_options ()-> UseBridges && node_is_a_configured_bridge ( node )) <nl> + entry -> is_dir_cache = 1 ; <nl> + <nl> return NULL ; <nl> } <nl> } else if (! for_directory ) {
static int parse_redirect_line ( or_options_t * options , <nl> tor_assert ( line ); <nl>  <nl> r = tor_malloc_zero ( sizeof ( exit_redirect_t )); <nl> + elements = smartlist_create (); <nl> smartlist_split_string ( elements , line -> value , " ", <nl> SPLIT_SKIP_SPACE | SPLIT_IGNORE_BLANK , 0 ); <nl> if ( smartlist_len ( elements ) != 2 ) {
tor_spawn_background ( const char * const filename , const char ** argv , <nl> process_environment_t * env , <nl> process_handle_t ** process_handle_out ) <nl> { <nl> - if ( may_spawn_background_process == 0 ) <nl> + if ( BUG ( may_spawn_background_process == 0 )) { <nl> + /* We should never reach this point if we ' re forbidden to spawn <nl> + * processes . Instead we should have caught the attempt earlier . */ <nl> return PROCESS_STATUS_ERROR ; <nl> + } <nl>  <nl> # ifdef _WIN32 <nl> HANDLE stdout_pipe_read = NULL ;
imalloc ( size_t size ) <nl> ptralloc = 1 ; <nl> size = malloc_pagesize ; <nl> } <nl> - if (( size + malloc_pagesize ) < size ) { /* Check for overflow */ <nl> + if ( size > SIZE_MAX - malloc_pagesize ) { /* Check for overflow */ <nl> result = NULL ; <nl> errno = ENOMEM ; <nl> } else if ( size <= malloc_maxsize )
NAMESPACE_BEGIN ( CryptoPP ) <nl> RandomPool :: RandomPool () <nl> : m_pCipher ( new AES :: Encryption ), m_keySet ( false ) <nl> { <nl> + memset ( m_key , 0 , m_key . SizeInBytes ()); <nl> + memset ( m_seed , 0 , m_seed . SizeInBytes ()); <nl> } <nl>  <nl> void RandomPool :: IncorporateEntropy ( const byte * input , size_t length )
typedef unsigned int word32 ; <nl> # if defined ( _MSC_VER ) || defined ( __BORLANDC__ ) <nl> typedef unsigned __int64 word64 ; <nl> # define W64LIT ( x ) x ## ui64 <nl> -# elif ( _LP64 || __LP64__ ) && ! defined ( __SUNPRO_CC ) <nl> +# elif ( _LP64 || __LP64__ ) <nl> typedef unsigned long word64 ; <nl> # define W64LIT ( x ) x ## UL <nl> # else
size_t jsvGetString ( const JsVar * v , char * str , size_t len ) { <nl> * want to pad the entire buffer with zeros */ <nl> len --; <nl> int l = 0 ; <nl> - while (* s && l < len ) { <nl> + while ( s [ l ] && l < len ) { <nl> str [ l ] = s [ l ]; <nl> l ++; <nl> }
JsVar * jspeFactor () { <nl> return jspeFactorTypeOf (); <nl> } else if ( execInfo . lex -> tk == LEX_R_VOID ) { <nl> JSP_MATCH ( LEX_R_VOID ); <nl> - jsvUnLock ( jspeBase ()); <nl> + jsvUnLock ( jspeFactor ()); <nl> return 0 ; <nl> } <nl> // Nothing we can do here ... just hope it ' s the end ...
void jswrap_wifi_restore ( void ) { <nl> ap_config . ssid_hidden = jsvGetInteger ( v ); <nl> jsvUnLock ( v ); <nl>  <nl> - v = jsvObjectGetChild ( o ," ssid ", 0 ); <nl> + v = jsvObjectGetChild ( o ," ssidAP ", 0 ); <nl> jsvGetString ( v , ( char *) ap_config . ssid , sizeof ( ap_config . ssid )); <nl>  <nl> ap_config . ssid_len = jsvGetStringLength ( v );
static int _layouts_load_config_common ( layout_plugin_t * plugin , <nl> * calling the update_done layout callback */ <nl> updated_entities [ i ] = e ; <nl> } <nl> + xfree ( e_name ); <nl> + xfree ( e_type ); <nl>  <nl> /* ** Full load config only ( flags == 0 ) ** <nl> * post - read - and - build ( post stage 1 )
slurmd_task_info_t * task_info_create ( int taskid , int gtaskid , <nl> static inline slurmd_task_info_t * <nl> job_task_info_by_pid ( slurmd_job_t * job , pid_t pid ) <nl> { <nl> - int i ; <nl> + uint32_t i ; <nl> for ( i = 0 ; i < job -> node_tasks ; i ++) { <nl> if ( job -> task [ i ]-> pid == pid ) <nl> return ( job -> task [ i ]);
int slurm_step_launch ( slurm_step_ctx ctx , <nl> char ** env = NULL ; <nl>  <nl> debug (" Entering slurm_step_launch "); <nl> + memset (& launch , 0 , sizeof ( launch )); <nl> + <nl> if ( ctx == NULL || ctx -> magic != STEP_CTX_MAGIC ) { <nl> error (" Not a valid slurm_step_ctx !"); <nl> 
static void log_msg ( log_level_t level , const char * fmt , va_list args ) <nl>  <nl> if ( level > SYSLOG_LEVEL && <nl> level > LOGFILE_LEVEL && <nl> - level > STDERR_LEVEL ) <nl> + level > STDERR_LEVEL ) { <nl> + pthread_mutex_unlock (& log_lock ); <nl> return ; <nl> + } <nl>  <nl> if ( log -> opt . prefix_level || SYSLOG_LEVEL > level ) { <nl> switch ( level ) {
void slurm_step_launch_wait_finish ( slurm_step_ctx_t * ctx ) <nl>  <nl> /* Then shutdown the message handler thread */ <nl> eio_signal_shutdown ( sls -> msg_handle ); <nl> + pthread_mutex_unlock (& sls -> lock ); <nl> pthread_join ( sls -> msg_thread , NULL ); <nl> + pthread_mutex_lock (& sls -> lock ); <nl> eio_handle_destroy ( sls -> msg_handle ); <nl>  <nl> /* Then wait for the IO thread to finish */ <nl> void slurm_step_launch_wait_finish ( slurm_step_ctx_t * ctx ) <nl> } <nl>  <nl> mpi_hook_client_fini ( sls -> mpi_state ); <nl> - <nl> pthread_mutex_unlock (& sls -> lock ); <nl> } <nl> 
static int _load_job_state ( Buf buffer ) <nl> jobacct_storage_g_job_complete ( acct_db_conn , job_ptr ); <nl> } <nl>  <nl> - if ( job_ptr -> qos ) { <nl> + if ( job_ptr -> qos && ( accounting_enforce & ACCOUNTING_ENFORCE_ASSOCS )) { <nl> memset (& qos_rec , 0 , sizeof ( acct_qos_rec_t )); <nl> qos_rec . id = job_ptr -> qos ; <nl> if ( _determine_and_validate_qos ( job_ptr , & qos_rec )
extern char * ba_set_ionode_str ( bitstr_t * ionode_bitmap ) <nl> if ( hl ) { <nl> ionode_str = hostlist_ranged_string_xmalloc_dims ( <nl> hl , 5 , 0 ); <nl> - info (" iostring is % s ", ionode_str ); <nl> + // info (" iostring is % s ", ionode_str ); <nl> hostlist_destroy ( hl ); <nl> hl = NULL ; <nl> }
extern int assoc_mgr_fill_in_assoc ( void * db_conn , <nl> ret_assoc = found_assoc ; <nl> debug3 (" found association " <nl> " for no partition "); <nl> + continue ; <nl> } else if ( strcasecmp ( assoc -> partition , <nl> - found_assoc -> partition )) <nl> + found_assoc -> partition )) { <nl> debug3 (" not the right partition "); <nl> - continue ; <nl> + continue ; <nl> + } <nl> } <nl> } <nl> ret_assoc = found_assoc ;
static int _load_jobs ( slurm_msg_t * req_msg , job_info_msg_t ** job_info_msg_pptr , <nl> int i , j , rc = SLURM_SUCCESS ; <nl> int local_job_cnt ; <nl> slurm_msg_t resp_msg , resp_msg_fed ; <nl> - job_info_msg_t * orig_msg = NULL , * new_msg ; <nl> + job_info_msg_t * orig_msg = NULL , * new_msg = NULL ; <nl> uint32_t new_rec_cnt ; <nl> uint32_t hash_inx , * hash_tbl_size = NULL , ** hash_job_id = NULL ; <nl> slurmdb_cluster_rec_t * cluster ;
extern void addto_qos_char_list ( List char_list , List qos_list , char * names ) <nl> bad : <nl> i ++; <nl> start = i ; <nl> + if (! names [ i ]) { <nl> + info (" There is a problem with " <nl> + " your line . It appears you " <nl> + " have spaces inside your list ."); <nl> + break ; <nl> + } <nl> } <nl> i ++; <nl> }
static void * _agent ( void * x ) <nl> free_buf ( buffer ); <nl> fail_time = 0 ; <nl> } else { <nl> + /* We still need to free a mult_msg even if we <nl> + got a failure . <nl> + */ <nl> + if ( list_msg . my_list ) { <nl> + list_msg . my_list = NULL ; <nl> + free_buf ( buffer ); <nl> + } <nl> + <nl> fail_time = time ( NULL ); <nl> } <nl> slurm_mutex_unlock (& agent_lock );
static uint32_t _update_weighted_freq ( struct jobacctinfo * jobacct , <nl> jobacct -> current_weighted_freq = <nl> jobacct -> current_weighted_freq + <nl> ( uint32_t ) jobacct -> this_sampled_cputime * thisfreq ; <nl> - if ( jobacct -> tot_cpu ) { <nl> + if ( jobacct -> tot_cpu >= 1 ) { <nl> return ( jobacct -> current_weighted_freq / <nl> ( uint32_t ) jobacct -> tot_cpu ); <nl> } else
static void _purge_missing_jobs ( int node_inx , time_t now ) <nl> requeue = true ; <nl> info (" Batch JobId =% u missing from node 0 ", <nl> job_ptr -> job_id ); <nl> + job_ptr -> exit_code = 1 ; <nl> job_complete ( job_ptr -> job_id , 0 , requeue , true , NO_VAL ); <nl> } else { <nl> _notify_srun_missing_step ( job_ptr , node_inx ,
bool slurm_container_has_pid ( uint32_t cont_id , pid_t pid ) <nl>  <nl> int slurm_container_wait ( uint32_t id ) <nl> { <nl> - if ( _job_waitjid (( jid_t ) id , NULL , 0 ) == ( jid_t )- 1 ) <nl> + int status ; <nl> + if ( _job_waitjid (( jid_t ) id , & status , 0 ) == ( jid_t )- 1 ) <nl> return SLURM_ERROR ; <nl>  <nl> return SLURM_SUCCESS ;
geocache_cfg * geocache_configuration_create ( apr_pool_t * pool ) { <nl> grid -> srs = apr_pstrdup ( pool ," epsg : 4326 "); <nl> grid -> unit = GEOCACHE_UNIT_DEGREES ; <nl> grid -> tile_sx = grid -> tile_sy = 256 ; <nl> - grid -> nlevels = 16 ; <nl> + grid -> nlevels = 19 ; <nl> grid -> extent [ 0 ] = wgs84_extent [ 0 ]; <nl> grid -> extent [ 1 ] = wgs84_extent [ 1 ]; <nl> grid -> extent [ 2 ] = wgs84_extent [ 2 ];
void msWriteErrorImage ( mapObj * map , char * filename , int blank ) <nl> ls . size = i ; <nl> MS_INIT_COLOR (* ls . color , 0 , 0 , 0 , 255 ); <nl> MS_INIT_COLOR (* ls . outlinecolor , 255 , 255 , 255 , 255 ); <nl> + ls . outlinewidth = 1 ; <nl> break ; <nl> } <nl> }
int loadLayer ( layerObj * layer , mapObj * map ) <nl> if ( getString (& layer -> tileindex ) == MS_FAILURE ) return (- 1 ); <nl> break ; <nl> case ( TILEITEM ): <nl> + free ( layer -> tileitem ); layer -> tileitem = NULL ; // erase default <nl> if ( getString (& layer -> tileitem ) == MS_FAILURE ) return (- 1 ); <nl> break ; <nl> case ( TOLERANCE ):
MS_DLL_EXPORT int freeClass ( classObj * ); <nl> MS_DLL_EXPORT void initLabel ( labelObj * label ); <nl> MS_DLL_EXPORT void resetClassStyle ( classObj * _class ); <nl> MS_DLL_EXPORT int initStyle ( styleObj * style ); <nl> + MS_DLL_EXPORT int freeStyle ( styleObj * style ); <nl> MS_DLL_EXPORT void initReferenceMap ( referenceMapObj * ref ); <nl> MS_DLL_EXPORT void initScalebar ( scalebarObj * scalebar ); <nl> MS_DLL_EXPORT void initGrid ( graticuleObj * pGraticule );
int msDumpLayer ( mapObj * map , layerObj * lp , int nVersion , const char * script_url_ <nl> free ( nestedGroups ); <nl> free ( numNestedGroups ); <nl> free ( isUsedInNestedGroup ); <nl> + free ( group_layers ); <nl> } <nl> } <nl> }
int initStyle ( styleObj * style ) { <nl> style -> sizescaled = style -> size = 1 ; // in SIZEUNITS ( layerObj ) <nl> style -> minsize = MS_MINSYMBOLSIZE ; <nl> style -> maxsize = MS_MAXSYMBOLSIZE ; <nl> - style -> offsetx = style -> offsety = - 1 ; <nl> + style -> offsetx = style -> offsety = 0 ; <nl>  <nl> return MS_SUCCESS ; <nl> }
static gboolean wsq_tlskey_inited = FALSE ; <nl> void <nl> mono_wsq_init () <nl> { <nl> + if ( wsq_tlskey_inited ) <nl> + return ; <nl> + <nl> mono_native_tls_alloc ( wsq_tlskey , NULL ); <nl> + wsq_tlskey_inited = TRUE ; <nl> } <nl>  <nl> void
mono_threads_summarize ( MonoContext * ctx , gchar ** out , MonoStackHash * hashes ) <nl> int count = 4 ; <nl>  <nl> while ( old_num_summarized == num_threads_summarized && count > 0 ) { <nl> + if ( thread -> state & ( ThreadState_Unstarted | ThreadState_Aborted | ThreadState_Stopped )) <nl> + break ; <nl> + <nl> sleep ( 1 ); <nl> mono_memory_barrier (); <nl> const char * name = thread_summary_state_to_str ( summarizing_thread_state );
read_pipes ( int outfd , gchar ** out_str , int errfd , gchar ** err_str ) <nl> err_closed = ( nread <= 0 ); <nl> } <nl> } <nl> - <nl> - } while ( res == - 1 && errno == EINTR ); <nl> + } while ( res > 0 || ( res == - 1 && errno == EINTR )); <nl>  <nl> g_free ( buffer ); <nl> if ( out_str )
mono_handle_stack_free ( HandleStack * stack ) <nl> c = next ; <nl> } <nl> g_free ( c ); <nl> + g_free ( stack ); <nl> } <nl>  <nl> void
mono_arch_create_specific_trampoline ( gpointer arg1 , MonoTrampolineType tramp_ty <nl>  <nl> code = buf = mono_domain_code_reserve_align ( domain , size , 1 ); <nl>  <nl> - if ((( gint64 ) tramp - ( gint64 ) code ) >> 31 != 0 ) { <nl> + if ((( gint64 ) tramp - ( gint64 ) code ) >> 31 != 0 && (( gint64 ) tramp - ( gint64 ) code ) >> 31 != - 1 ) { <nl> # ifndef MONO_ARCH_NOMAP32BIT <nl> g_assert_not_reached (); <nl> # endif
mono_local_emulate_ops ( MonoCompile * cfg ) <nl> * at IR level , instead of inlining the icall wrapper . FIXME <nl> */ <nl> if ( inlined_wrapper ) { <nl> - mono_decompose_long_opts ( cfg ); <nl> + if (! COMPILE_LLVM ( cfg )) <nl> + mono_decompose_long_opts ( cfg ); <nl> if ( cfg -> opt & ( MONO_OPT_CONSPROP | MONO_OPT_COPYPROP )) <nl> mono_local_cprop ( cfg ); <nl> }
continuation_store ( MonoContinuation * cont , int state , MonoException ** e ) <nl> /* clear to avoid GC retention */ <nl> if ( num_bytes < cont -> stack_used_size ) { <nl> memset (( char *) cont -> saved_stack + num_bytes , 0 , cont -> stack_used_size - num_bytes ); <nl> - cont -> stack_used_size = num_bytes ; <nl> } <nl> + cont -> stack_used_size = num_bytes ; <nl> } else { <nl> tasklets_lock (); <nl> internal_init ();
mono_profiler_init_log ( const char * desc ) <nl> mono_profiler_set_gc_event_callback ( handle , gc_event ); <nl>  <nl> mono_profiler_set_thread_started_callback ( handle , thread_start ); <nl> - mono_profiler_set_thread_stopped_callback ( handle , thread_end ); <nl> + mono_profiler_set_thread_exited_callback ( handle , thread_end ); <nl> mono_profiler_set_thread_name_callback ( handle , thread_name ); <nl>  <nl> mono_profiler_set_domain_loaded_callback ( handle , domain_loaded );
mono_jit_walk_stack_from_ctx_in_thread ( MonoJitStackWalk func , MonoDomain * domai <nl> /* <nl> * FIXME : These frames show up twice , and ctx could refer to native code . <nl> */ <nl> + ctx = new_ctx ; <nl> continue ; <nl> } <nl> frame . actual_method = get_method_from_stack_frame ( frame . ji , get_generic_info_from_stack_frame ( frame . ji , & ctx ));
emit_method_info ( MonoAotCompile * acfg , MonoCompile * cfg ) <nl>  <nl> encode_patch_list ( acfg , patches , n_patches , cfg -> compile_llvm , first_got_offset , p , & p ); <nl>  <nl> + g_ptr_array_free ( patches , TRUE ); <nl> + <nl> acfg -> stats . info_size += p - buf ; <nl>  <nl> g_assert ( p - buf < buf_size );
retry : <nl> * between 1 and 2 , the object is still live ) <nl> */ <nl> * objslot = NULL ; <nl> + SET_OWNER ( top , idx ); <nl> + SET_SP ( handles , top , idx ); <nl> mono_memory_write_barrier (); <nl> top -> size ++; <nl> mono_memory_write_barrier (); <nl> * objslot = obj ; <nl> - SET_OWNER ( top , idx ); <nl> - SET_SP ( handles , top , idx ); <nl> return objslot ; <nl> } <nl> if ( G_LIKELY ( top -> next )) {
load_aot_module ( MonoAssembly * assembly , gpointer user_data ) <nl> mono_trace ( G_LOG_LEVEL_INFO , MONO_TRACE_AOT , " AOT : image '% s ' not found : % s ", aot_name , err ); <nl> g_free ( err ); <nl> } <nl> + g_free ( aot_name ); <nl> } <nl> if (! sofile ) { <nl> GList * l ;
 <nl> typedef struct _MSBlockInfo MSBlockInfo ; <nl> struct _MSBlockInfo { <nl> - int obj_size ; <nl> - int obj_size_index ; <nl> + guint16 obj_size ; <nl> + /* <nl> + * FIXME : Do we even need this ? It ' s only used during sweep and might be worth <nl> + * recalculating to save the space . <nl> + */ <nl> + guint16 obj_size_index ; <nl> unsigned int pinned : 1 ; <nl> unsigned int has_references : 1 ; <nl> unsigned int has_pinned : 1 ; /* means cannot evacuate */
acfg_free ( MonoAotCompile * acfg ) <nl> g_hash_table_destroy ( acfg -> image_hash ); <nl> g_hash_table_destroy ( acfg -> unwind_info_offsets ); <nl> g_hash_table_destroy ( acfg -> method_label_hash ); <nl> - if (! acfg -> typespec_classes ) <nl> + if ( acfg -> typespec_classes ) <nl> g_hash_table_destroy ( acfg -> typespec_classes ); <nl> g_hash_table_destroy ( acfg -> export_names ); <nl> g_hash_table_destroy ( acfg -> plt_entry_debug_sym_cache );
mono_llvm_emit_method ( MonoCompile * cfg ) <nl> if ( cfg -> method -> save_lmf && ! cfg -> llvm_only ) <nl> LLVM_FAILURE ( ctx , " lmf "); <nl>  <nl> - if ( sig -> pinvoke && cfg -> method -> wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE ) <nl> + if ( sig -> pinvoke && cfg -> method -> wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE && ! cfg -> llvm_only ) <nl> LLVM_FAILURE ( ctx , " pinvoke signature "); <nl>  <nl> header = cfg -> header ;
coverage_filter ( MonoProfiler * prof , MonoMethod * method ) <nl> MonoLockFreeQueue * image_methods , * class_methods ; <nl> MonoLockFreeQueueNode * node ; <nl>  <nl> - if (! coverage_initialized ) <nl> - return FALSE ; <nl> + g_assert ( coverage_initialized && " Why are we being asked for coverage filter info when we ' re not doing coverage ?"); <nl>  <nl> COVERAGE_DEBUG ( fprintf ( stderr , " Coverage filter for % s \ n ", mono_method_get_name ( method ));) <nl> 
mini_method_compile ( MonoMethod * method , guint32 opts , MonoDomain * domain , JitFl <nl> // g_free ( nm ); <nl> } <nl> if ( cfg -> llvm_only ) { <nl> + g_free ( cfg -> exception_message ); <nl> cfg -> disable_aot = TRUE ; <nl> return cfg ; <nl> }
mono_mempool_alloc ( MonoMemPool * pool , guint size ) <nl>  <nl> # ifdef MALLOC_ALLOCATION <nl> { <nl> - Chunk * c = g_malloc ( size ); <nl> + Chunk * c = g_malloc ( size + sizeof ( Chunk )); <nl>  <nl> c -> next = pool -> chunks ; <nl> pool -> chunks = c ;
void <nl> mono_llvm_free_domain_info ( MonoDomain * domain ) <nl> { <nl> /* This is called even when llvm is not enabled */ <nl> - if ( mono_llvm_free_domain_info_fptr ) <nl> + if ( backend . free_domain_info ) <nl> backend . free_domain_info ( domain ); <nl> } <nl> 
mono_trace_set_printerr_handler ( MonoPrintCallback callback ) <nl> { <nl> g_assert ( callback ); <nl> printerr_callback = callback ; <nl> - g_set_print_handler ( printerr_handler ); <nl> + g_set_printerr_handler ( printerr_handler ); <nl> }
is_regsize_var ( MonoType * t ) { <nl> case MONO_TYPE_U4 : <nl> case MONO_TYPE_I : <nl> case MONO_TYPE_U : <nl> + case MONO_TYPE_PTR : <nl> + case MONO_TYPE_FNPTR : <nl> return TRUE ; <nl> case MONO_TYPE_OBJECT : <nl> case MONO_TYPE_STRING :
add_wrappers ( MonoAotCompile * acfg ) <nl> if ( export_name ) <nl> g_hash_table_insert ( acfg -> export_names , wrapper , export_name ); <nl> } <nl> + g_free ( cattr ); <nl> } <nl>  <nl> if (( method -> flags & METHOD_ATTRIBUTE_PINVOKE_IMPL ) ||
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> } <nl>  <nl> /* Add a sequence point for method entry / exit events */ <nl> - if ( cfg -> gen_seq_points_debug_data ) { <nl> + if ( seq_points && cfg -> gen_seq_points_debug_data ) { <nl> NEW_SEQ_POINT ( cfg , ins , METHOD_ENTRY_IL_OFFSET , FALSE ); <nl> MONO_ADD_INS ( init_localsbb , ins ); <nl> NEW_SEQ_POINT ( cfg , ins , METHOD_EXIT_IL_OFFSET , FALSE );
mono_allocate_stack_slots2 ( MonoCompile * cfg , gboolean backward , guint32 * stack_ <nl> if ( cfg -> disable_reuse_stack_slots ) <nl> reuse_slot = FALSE ; <nl>  <nl> + t = mini_get_underlying_type ( cfg , t ); <nl> switch ( t -> type ) { <nl> case MONO_TYPE_GENERICINST : <nl> if (! mono_type_generic_inst_is_valuetype ( t )) {
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> MonoMethod * cil_method ; <nl>  <nl> if ( method -> wrapper_type != MONO_WRAPPER_NONE ) { <nl> - if ( cfg -> verbose_level > 2 ) <nl> + if ( constrained_call && cfg -> verbose_level > 2 ) <nl> printf (" DM Constrained call to % s \ n ", mono_type_get_full_name ( constrained_call )); <nl> cmethod = ( MonoMethod *) mono_method_get_wrapper_data ( method , token ); <nl> cil_method = cmethod ;
decode_exception_debug_info ( MonoAotModule * amodule , MonoDomain * domain , <nl> mono_error_cleanup (& error ); /* FIXME don ' t swallow the error */ <nl> } <nl>  <nl> - gi -> generic_sharing_context = g_new0 ( MonoGenericSharingContext , 1 ); <nl> + gi -> generic_sharing_context = alloc0_jit_info_data ( domain , sizeof ( MonoGenericSharingContext ), async ); <nl> if ( decode_value ( p , & p )) { <nl> /* gsharedvt */ <nl> MonoGenericSharingContext * gsctx = gi -> generic_sharing_context ;
void GeoIPBackend :: initialize () { <nl> } <nl> } <nl>  <nl> - tmp_domains . push_back ( dom ); <nl> + tmp_domains . push_back ( std :: move ( dom )); <nl> } <nl>  <nl> s_domains . clear ();
bool isEDNSOptionInOpt ( const std :: string & packet , const size_t optStart , const s <nl> size_t p = optStart + 9 ; <nl> uint16_t rdLen = ( 0x100 * packet . at ( p ) + packet . at ( p + 1 )); <nl> p += sizeof ( rdLen ); <nl> - if ( 11 + rdLen > optLen ) { <nl> + if ( rdLen > ( optLen - 11 )) { <nl> return false ; <nl> } <nl> 
void doClient ( ComboAddress server , const std :: string & command ) <nl> msg . assign ( resp . get (), len ); <nl> msg = sodDecryptSym ( msg , g_key , theirs ); <nl> cout << msg ; <nl> + cout . flush (); <nl> } <nl> } <nl> else {
int main ( int argc , char ** argv ) <nl> if (! dh -> qdcount ) <nl> continue ; <nl>  <nl> + if ( pr . d_len < sizeof ( dnsheader )) <nl> + continue ; <nl> + <nl> uint16_t qtype , qclass ; <nl> DNSName qname ; <nl> try {
try <nl>  <nl> qtype = mdp . d_qtype ; <nl> qclass = mdp . d_qclass ; <nl> + <nl> + d_trc = TSIGRecordContent (); <nl> + <nl> return 0 ; <nl> } <nl> catch ( std :: exception & e ) {
static void handle_cgi_request ( struct mg_connection * conn , const char * prog ) { <nl> struct file fout = STRUCT_FILE_INITIALIZER ; <nl> pid_t pid ; <nl>  <nl> + memset (& ri , 0 , sizeof ( ri )); <nl> prepare_cgi_environment ( conn , prog , & blk ); <nl>  <nl> // CGI must be executed in its own directory . ' dir ' must point to the
int MySQLDB :: exec_sql_query ( MYSQL * conn , char * sql , <nl> // than a simple 0 <nl> if (( result = mysql_store_result (& mysql )) == NULL ) <nl> rc = 0 ; // unable to retrieve the result but still the query succeded <nl> - else <nl> + else { <nl> + mysql_free_result ( result ); <nl> rc = mysql_num_rows ( result ); <nl> + } <nl> } <nl>  <nl> if ( doLock && m ) m -> unlock ( __FILE__ , __LINE__ );
int pem_read_buffer ( pem_context * ctx , char * header , char * footer , const unsigne <nl> return ( POLARSSL_ERR_PEM_PASSWORD_MISMATCH ); <nl> } <nl> # else <nl> + free ( buf ); <nl> return ( POLARSSL_ERR_PEM_FEATURE_UNAVAILABLE ); <nl> # endif <nl> }
int ssl_get_ciphersuite_id ( const char * ciphersuite_name ) <nl>  <nl> const char * ssl_get_ciphersuite ( const ssl_context * ssl ) <nl> { <nl> + if ( ssl == NULL || ssl -> session == NULL ) <nl> + return NULL ; <nl> + <nl> return ssl_get_ciphersuite_name ( ssl -> session -> ciphersuite ); <nl> } <nl> 
nautilus_path_bar_scroll_down ( NautilusPathBar * path_bar ) <nl> * from the end , removing buttons until we get all the space we <nl> * need . */ <nl> gtk_widget_get_allocation ( BUTTON_DATA ( up_button -> data )-> button , & button_allocation ); <nl> - while ( space_available < space_needed ) { <nl> + while (( space_available < space_needed ) && <nl> + ( up_button != NULL )) { <nl> space_available += button_allocation . width + path_bar -> spacing ; <nl> up_button = up_button -> prev ; <nl> path_bar -> first_scrolled_button = up_button ;
nautilus_window_slot_switch_new_content_view ( NautilusWindowSlot * slot ) <nl> GtkWidget * widget ; <nl> gboolean reusing_view ; <nl>  <nl> - reusing_view = gtk_widget_get_parent ( GTK_WIDGET ( slot -> details -> new_content_view )) != NULL ; <nl> + reusing_view = slot -> details -> new_content_view && <nl> + gtk_widget_get_parent ( GTK_WIDGET ( slot -> details -> new_content_view )) != NULL ; <nl> /* We are either reusing the view , so new_content_view and content_view <nl> * are the same , or the new_content_view is invalid */ <nl> if ( slot -> details -> new_content_view == NULL || reusing_view )
on_widget_destroyed ( GtkWidget * widget , <nl> self -> details -> change_idle_id = 0 ; <nl> } <nl>  <nl> + free_fade ( self ); <nl> self -> details -> widget = NULL ; <nl> } <nl> 
nautilus_toolbar_dispose ( GObject * obj ) <nl> toolbar_update_appearance , self ); <nl> unschedule_menu_popup_timeout ( self ); <nl>  <nl> + g_clear_object (& self -> priv -> zoom_adjustment_grid ); <nl> + g_clear_object (& self -> priv -> zoom_adjustment_list ); <nl> + <nl> G_OBJECT_CLASS ( nautilus_toolbar_parent_class )-> dispose ( obj ); <nl> } <nl> 
eazel_install_install_packages ( EazelInstall * service , GList * categories ) <nl> if ( install_new_packages ( service , categories )== FALSE ) { <nl> g_warning ( _ (" Install failed ")); <nl> } <nl> - eazel_install_emit_done ( service ); <nl> if ( eazel_install_emit_delete_files ( service )) { <nl> GList * item ; <nl> GList * cat ; <nl> eazel_install_install_packages ( EazelInstall * service , GList * categories ) <nl> } <nl> } <nl> } <nl> + eazel_install_emit_done ( service ); <nl> } <nl>  <nl> void
struct _NautilusPathBarClass <nl> { <nl> GtkContainerClass parent_class ; <nl>  <nl> - void (* path_clicked ) ( NautilusPathBar * path_bar , <nl> - GFile * location ); <nl> - void (* path_event ) ( NautilusPathBar * path_bar , <nl> - GdkEventButton * event , <nl> - GFile * location ); <nl> + void (* path_clicked ) ( NautilusPathBar * path_bar , <nl> + GFile * location ); <nl> + gboolean (* path_event ) ( NautilusPathBar * path_bar , <nl> + GdkEventButton * event , <nl> + GFile * location ); <nl> }; <nl>  <nl> GType nautilus_path_bar_get_type ( void ) G_GNUC_CONST ;
clicked_within_double_click_interval ( NautilusIconContainer * container ) <nl> /* Stash time for next compare */ <nl> last_click_time = current_time ; <nl>  <nl> - return ( click_count > 0 ); <nl> + /* Only allow double click */ <nl> + return ( click_count == 1 ); <nl> } <nl>  <nl> static void
pk_proxy_appeared_cb ( GObject * source , <nl>  <nl> /* show an unhelpful dialog */ <nl> show_unhandled_type_error ( parameters_install ); <nl> - /* The callback wasn ' t started , so we have to free the parameters */ <nl> - activate_parameters_install_free ( parameters_install ); <nl>  <nl> return ; <nl> }
done_loading ( NautilusFilesView * view , <nl> } else if ( pending_selection != NULL && all_files_seen ) { <nl> view -> details -> pending_selection = NULL ; <nl>  <nl> - nautilus_files_view_call_set_selection ( view , selection ); <nl> + nautilus_files_view_call_set_selection ( view , pending_selection ); <nl> do_reveal = TRUE ; <nl> } <nl> 
nautilus_search_hit_compute_scores ( NautilusSearchHit * hit , <nl> proximity_bonus , recent_bonus , match_bonus ); <nl>  <nl> g_date_time_unref ( now ); <nl> + g_free ( query_uri ); <nl> } <nl>  <nl> const char *
notebook_create_window_cb ( GtkNotebook * notebook , <nl> g_object_set_data ( G_OBJECT ( slot ), " dnd - window - slot ", <nl> GINT_TO_POINTER ( TRUE )); <nl>  <nl> + gtk_window_set_position ( GTK_WINDOW ( new_window ), GTK_WIN_POS_MOUSE ); <nl> + <nl> return GTK_NOTEBOOK ( new_window -> details -> notebook ); <nl> } <nl> 
GF_Err import_file ( GF_ISOFile * dest , char * inName , u32 import_flags , Double forc <nl> } <nl> } <nl> } <nl> + else if (! strnicmp ( ext , " prog_id =", 8 )) { <nl> + prog_id = atoi ( ext + 8 ); <nl> + do_all = 0 ; <nl> + } <nl> else track_id = atoi ( ext ); <nl> } <nl> if ( do_audio || do_video || track_id ) do_all = 0 ;
void gf_mpd_url_free ( void * _item ) <nl> { <nl> GF_MPD_URL * ptr = ( GF_MPD_URL *) _item ; <nl> if ( ptr -> sourceURL ) gf_free ( ptr -> sourceURL ); <nl> + if ( ptr -> byte_range ) gf_free ( ptr -> byte_range ); <nl> gf_free ( ptr ); <nl> } <nl> void gf_mpd_string_free ( void * _item ) {
GF_Err gf_bin128_parse ( const char * string , bin128 value ) <nl> break ; <nl> sprintf ( szV , "% c % c ", string [ j ], string [ j + 1 ]); <nl> sscanf ( szV , "% x ", & v ); <nl> + if ( i > 15 ) { <nl> + // force error check below <nl> + i ++; <nl> + break ; <nl> + } <nl> value [ i ] = v ; <nl> i ++; <nl> + <nl> } <nl> } <nl> if ( i != 16 ) {
static GF_Err ft_set_font ( GF_FontReader * dr , const char * OrigFontName , u32 style <nl> opt = gf_modules_get_option (( GF_BaseInterface *) dr , " FontEngine ", fname ); <nl>  <nl> if ( opt ) { <nl> - gf_free ( fname ); <nl> FT_Face face ; <nl> + gf_free ( fname ); <nl> if ( FT_New_Face ( ftpriv -> library , opt , 0 , & face )) return GF_IO_ERR ; <nl> if (! face ) return GF_IO_ERR ; <nl> gf_list_add ( ftpriv -> loaded_fonts , face );
static GF_Err gf_media_export_filters ( GF_MediaExporter * dumper ) <nl> } <nl> esd = gf_media_map_esd ( dumper -> file , track_num , 0 ); <nl> sample_count = gf_isom_get_sample_count ( dumper -> file , dumper -> trackID ); <nl> - if ( esd ) { <nl> + if ( esd && esd -> decoderConfig ) { <nl> if ( esd -> decoderConfig -> objectTypeIndication < GF_CODECID_LAST_MPEG4_MAPPING ) { <nl> codec_id = gf_codecid_from_oti ( esd -> decoderConfig -> streamType , esd -> decoderConfig -> objectTypeIndication ); <nl> # ifndef GPAC_DISABLE_AV_PARSERS
s32 AVC_ReadSeqInfo ( char * sps_data , u32 sps_size , AVCState * avc , u32 subseq_sps , <nl>  <nl> pcomp = gf_bs_read_int ( bs , 8 ); <nl> /* sanity checks */ <nl> - if ( pcomp && 0x3 ) <nl> - goto exit ; <nl> + // JLF commented - breaks SVC import and no time to investigate <nl> +// if ( pcomp && 0x3 ) goto exit ; <nl>  <nl> level_idc = gf_bs_read_int ( bs , 8 ); <nl> 
void IMG_NetIO ( void * cbk , GF_NETIO_Parameter * param ) <nl> e = param -> error ; <nl> /* wait to get the whole file */ <nl> if (! e && ( param -> msg_type != GF_NETIO_DATA_TRANSFERED )) return ; <nl> + if (( e == GF_EOS ) && ( param -> msg_type == GF_NETIO_DATA_EXCHANGE )) return ; <nl>  <nl> if ( param -> msg_type == GF_NETIO_DATA_TRANSFERED ) { <nl> szCache = gf_dm_sess_get_cache_name ( read -> dnload );
static Bool enum_dir_fct ( void * cbck , char * file_name , char * file_path ) <nl> JSObject * obj ; <nl> enum_dir_cbk * cbk = ( enum_dir_cbk *) cbck ; <nl>  <nl> + if ( file_name && ( file_name [ 0 ]=='.')) return 0 ; <nl> + <nl> obj = JS_NewObject ( cbk -> c , 0 , 0 , 0 ); <nl> s = JS_NewStringCopyZ ( cbk -> c , file_name ); <nl> JS_DefineProperty ( cbk -> c , obj , " name ", STRING_TO_JSVAL ( s ), 0 , 0 , JSPROP_READONLY | JSPROP_PERMANENT );
GF_RTPHinter * gf_hinter_track_new ( GF_ISOFile * file , u32 TrackNum , <nl> max_ptime = ( u32 ) ( max_ptime * my_sl . timestampResolution / 1000 ); <nl>  <nl> my_sl . AUSeqNumLength = gf_get_bit_size ( gf_isom_get_sample_count ( file , TrackNum )); <nl> + if ( my_sl . AUSeqNumLength > 16 ) my_sl . AUSeqNumLength = 16 ; <nl> + <nl> my_sl . CUDuration = const_dur ; <nl>  <nl> if ( gf_isom_has_sync_points ( file , TrackNum )) {
GF_Box * gf_isom_box_new ( u32 boxType ) <nl>  <nl> void gf_isom_box_add_for_dump_mode ( GF_Box * parent , GF_Box * a ) <nl> { <nl> - if ( use_dump_mode && (! parent -> other_boxes || ( gf_list_find ( parent -> other_boxes , a )< 0 ) ) ) <nl> + if ( use_dump_mode && a && (! parent -> other_boxes || ( gf_list_find ( parent -> other_boxes , a )< 0 ) ) ) <nl> gf_isom_box_add_default ( parent , a ); <nl> } <nl> 
GF_Err adts_dmx_process ( GF_Filter * filter ) <nl> break ; <nl> } <nl>  <nl> + if ( ctx -> hdr . frame_size < ctx -> hdr . hdr_size ) { <nl> + GF_LOG ( GF_LOG_WARNING , GF_LOG_PARSER , ("[ ADTSDmx ] Corrupted ADTS frame header , resyncing \ n ")); <nl> + ctx -> nb_frames = 0 ; <nl> + goto drop_byte ; <nl> + } <nl> + <nl> adts_dmx_check_pid ( filter , ctx ); <nl>  <nl> if (! ctx -> is_playing ) {
static void parse_sec_attr_44 ( sc_file_t * file , const u8 * buf , size_t len ) <nl> } <nl>  <nl> /* Encryption key present ? */ <nl> - iPinCount = iACLen - 1 ; <nl> + iPinCount = iACLen > 0 ? iACLen - 1 : 0 ; <nl>  <nl> if ( buf [ iOffset ] & 0x20 ) { <nl> int iSC ;
static int jcop_set_security_env ( sc_card_t * card , <nl> tmp . algorithm_ref |= 0x10 ; <nl> if ( tmp . algorithm_flags & SC_ALGORITHM_RSA_HASH_MD5 ) <nl> tmp . algorithm_ref |= 0x20 ; <nl> - env =& tmp ; <nl> + <nl> + memcpy ( env , & tmp , sizeof ( struct sc_security_env )); <nl> } <nl>  <nl> sc_format_apdu ( card , & apdu , SC_APDU_CASE_3_SHORT , 0x22 , 0xC1 , 0 );
jpki_finish ( sc_card_t * card ) <nl> struct jpki_private_data * drvdata = JPKI_DRVDATA ( card ); <nl>  <nl> LOG_FUNC_CALLED ( card -> ctx ); <nl> - <nl> + if ( drvdata -> mf ) { <nl> + free ( drvdata -> mf ); <nl> + drvdata -> mf = NULL ; <nl> + } <nl> if ( drvdata ) { <nl> free ( drvdata ); <nl> card -> drv_data = NULL ;
pgp_finish ( sc_card_t * card ) <nl> pgp_iterate_blobs ( priv -> mf , 99 , pgp_free_blob ); <nl>  <nl> free ( priv ); <nl> + card -> drv_data = NULL ; <nl> return 0 ; <nl> } <nl> 
sm_encrypt_des_cbc3 ( struct sc_context * ctx , unsigned char * key , <nl>  <nl> * out_len = data_len ; <nl> * out = malloc ( data_len + 8 ); <nl> - if (* out == NULL ) <nl> + if (* out == NULL ) { <nl> + free ( data ); <nl> LOG_TEST_RET ( ctx , SC_ERROR_OUT_OF_MEMORY , " SM encrypt_des_cbc3 : failure "); <nl> + } <nl>  <nl> memcpy (& kk , key , 8 ); <nl> memcpy (& k2 , key + 8 , 8 );
static int part10_build_modify_pin_block ( struct sc_reader * reader , u8 * buf , siz <nl> pin_modify -> bInsertionOffsetNew = 0x00 ; <nl> } <nl>  <nl> - if (! data -> pin1 . min_length || ! data -> pin1 . max_length ) <nl> + if (!( data -> flags & SC_PIN_CMD_IMPLICIT_CHANGE ) <nl> + && (! data -> pin1 . min_length || ! data -> pin1 . max_length )) <nl> return SC_ERROR_INVALID_ARGUMENTS ; <nl>  <nl> tmp16 = ( data -> pin1 . min_length << 8 ) + data -> pin1 . max_length ;
iso7816_select_file ( struct sc_card * card , const struct sc_path * in_path , struct <nl> pathlen = in_path -> len ; <nl> pathtype = in_path -> type ; <nl>  <nl> + if ( file_out != NULL ) { <nl> + * file_out = NULL ; <nl> + } <nl> if ( in_path -> aid . len ) { <nl> if (! pathlen ) { <nl> memcpy ( path , in_path -> aid . value , in_path -> aid . len );
sc_keycache_forget_key ( const sc_path_t * path , int type , int ref ) <nl> while (( s = * prev ) != NULL ) { <nl> if ( __match_entry ( s , type , ref , path , 1 )) { <nl> * prev = s -> next ; <nl> + if ( s -> named_pin != - 1 && s -> ref == - 1 ) <nl> + named_pin [ s -> named_pin ] = NULL ; <nl> memset ( s , 0 , sizeof (* s )); <nl> free ( s ); <nl> } else {
static int asn1_decode_entry ( sc_context_t * ctx , struct sc_asn1_entry * entry , <nl>  <nl> /* Strip off padding zero */ <nl> if (( entry -> flags & SC_ASN1_UNSIGNED ) <nl> - && obj [ 0 ] == 0x00 && objlen > 1 ) { <nl> + && objlen > 1 && obj [ 0 ] == 0x00 ) { <nl> objlen --; <nl> obj ++; <nl> }
int sc_pkcs15_parse_tokeninfo ( sc_context_t * ctx , <nl> sprintf ( byte , "% 02X ", serial [ ii ]); <nl> strcat ( ti -> serial_number , byte ); <nl> } <nl> + sc_log ( ctx , " TokenInfo . serialNunmber '% s '", ti -> serial_number ); <nl> } <nl>  <nl> if ( ti -> manufacturer_id == NULL ) {
static int transform_pace_output ( u8 * rbuf , size_t rbuflen , <nl> if ( parsed + 2 > rbuflen ) <nl> return SC_ERROR_UNKNOWN_DATA_RECEIVED ; <nl> pace_output -> mse_set_at_sw1 = rbuf [ parsed + 0 ]; <nl> - pace_output -> mse_set_at_sw1 = rbuf [ parsed + 1 ]; <nl> + pace_output -> mse_set_at_sw2 = rbuf [ parsed + 1 ]; <nl> parsed += 2 ; <nl>  <nl> /* length_CardAccess */
static int tcos_decipher ( sc_card_t * card , const u8 * crgram , size_t crgram_len , <nl> apdu . data = sbuf ; <nl> apdu . lc = apdu . datalen = crgram_len + 1 ; <nl> sbuf [ 0 ] = tcos3 ? 0x00 : (( data -> pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1 ) ? 0x81 : 0x02 ); <nl> + if ( sizeof sbuf - 1 < crgram_len ) <nl> + return SC_ERROR_INVALID_ARGUMENTS ; <nl> memcpy ( sbuf + 1 , crgram , crgram_len ); <nl>  <nl> r = sc_transmit_apdu ( card , & apdu );
print_aligned_vertical ( const printTableContent * cont , FILE * fout ) <nl> if ( cont -> cells [ 0 ] == NULL && cont -> opt -> start_table && <nl> cont -> opt -> stop_table ) <nl> { <nl> - if (! opt_tuples_only ) <nl> + if (! opt_tuples_only && cont -> opt -> default_footer ) <nl> fprintf ( fout , _ ("( No rows )\ n ")); <nl> return ; <nl> }
pltcl_returnnext ( ClientData cdata , Tcl_Interp * interp , <nl> Datum retval ; <nl> bool isNull = false ; <nl>  <nl> + /* for paranoia ' s sake , check that tupdesc has exactly one column */ <nl> + if ( call_state -> ret_tupdesc -> natts != 1 ) <nl> + elog ( ERROR , " wrong result type supplied in return_next "); <nl> + <nl> retval = InputFunctionCall (& prodesc -> result_in_func , <nl> utf_u2e (( char *) Tcl_GetString ( objv [ 1 ])), <nl> prodesc -> result_typioparam ,
initHyperLogLog ( hyperLogLogState * cState , uint8 bwidth ) <nl> elog ( ERROR , " bit width must be between 4 and 16 inclusive "); <nl>  <nl> cState -> registerWidth = bwidth ; <nl> - cState -> nRegisters = 1 << bwidth ; <nl> + cState -> nRegisters = ( Size ) 1 << bwidth ; <nl> cState -> arrSize = sizeof ( uint8 ) * cState -> nRegisters + 1 ; <nl>  <nl> /*
void Translator :: handleAssertionEffects ( Tracelet & t , <nl> */ <nl> if ( tas . m_changeSet . count ( dl -> location )) { <nl> auto const src = findInputSrc ( tas . m_t -> m_instrStream . last , dl ); <nl> - if ( src -> outputPredicted ) src -> outputPredicted = false ; <nl> + if ( src && src -> outputPredicted ) src -> outputPredicted = false ; <nl> } <nl> } <nl> 
static int exif_scan_JPEG_header ( image_info_type * ImageInfo ) { <nl> case M_SOF13 : <nl> case M_SOF14 : <nl> case M_SOF15 : <nl> + if (( itemlen - 2 ) < 6 ) { <nl> + return 0 ; <nl> + } <nl> + <nl> exif_process_SOFn ( Data , marker , & sof_info ); <nl> ImageInfo -> Width = sof_info . width ; <nl> ImageInfo -> Height = sof_info . height ;
void LibEventTransport :: sendImpl ( const void * data , int size , int code , <nl> } else { <nl> if ( m_method != HEAD ) { <nl> evbuffer_add ( m_request -> output_buffer , data , size ); <nl> + } else { <nl> + char buf [ 11 ]; <nl> + snprintf ( buf , sizeof ( buf ), "% d ", size ); <nl> + addHeaderImpl (" Content - Length ", buf ); <nl> } <nl> m_server -> onResponse ( m_workerId , m_request , code ); <nl> m_sendEnded = true ;
TranslatorX64 :: smash ( X64Assembler & a , TCA src , TCA dest , bool isCall ) { <nl> */ <nl> CodeCursor cg ( a , src ); <nl> assert ( isSmashable ( a . code . frontier , kJmpLen )); <nl> - if ( dest > src && dest - src <= 7 ) { <nl> + if ( dest > src && dest - src <= kJmpLen ) { <nl> assert (! isCall ); <nl> a . emitNop ( dest - src ); <nl> } else if (! isCall ) {
SAL_DLLPUBLIC void SAL_CALL rtl_uString_newReplaceAllAsciiL ( <nl> @ param to pointer to the replacing substring ; must not be null and must <nl> point to memory of at least \ p toLength ASCII bytes <nl>  <nl> - @ param fromLength the length of the \ p to substring ; must be non - negative <nl> + @ param toLength the length of the \ p to substring ; must be non - negative <nl>  <nl> @ since LibreOffice 5 . 1 <nl> */
static LibreOfficeKit * lok_init_2 ( const char * install_path , const char * user_p <nl> LokHookFunction2 * pSym2 ; <nl>  <nl> dlhandle = lok_dlopen ( install_path , & imp_lib ); <nl> + if (! dlhandle ) <nl> + return NULL ; <nl>  <nl> pSym2 = ( LokHookFunction2 *) lok_dlsym ( dlhandle , " libreofficekit_hook_2 "); <nl> if (! pSym2 )
namespace XrdCl <nl>  <nl> XrdSysPwd pwdHandler ; <nl> passwd * pwd = pwdHandler . Get ( getuid () ); <nl> + if ( ! pwd ) return ; <nl> std :: string userPlugIns = pwd -> pw_dir ; <nl> userPlugIns += "/. xrootd / client . plugins . d "; <nl> ProcessConfigDir ( userPlugIns );
int XrdHttpReq :: PostProcessHTTPReq ( bool final_ ) { <nl>  <nl> } else <nl> for ( int i = 0 ; i < iovN ; i ++) { <nl> - prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len ); <nl> + if ( prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len )) return 1 ; <nl> writtenbytes += iovP [ i ]. iov_len ; <nl> } <nl> 
namespace XrdCl <nl> pFileUrl ( 0 ), <nl> pDataServer ( 0 ), <nl> pLoadBalancer ( 0 ), <nl> + pStateRedirect ( 0 ), <nl> pFileHandle ( 0 ), <nl> pOpenMode ( 0 ), <nl> pOpenFlags ( 0 ),
void DoIt () { myMutex . Lock (); <nl> virtual void Finished ( XrdSsiRequest & rqstR , <nl> const XrdSsiRespInfo & rInfo , <nl> bool cancel = false ) <nl> - { myMutex . Lock (); <nl> + { UnBindRequest (); <nl> + myMutex . Lock (); <nl> if (! isActive ) delete this ; <nl> else { isActive = false ; <nl> myMutex . UnLock ();
static void mygroup_delete ( mygroup_t * mg ) <nl> } <nl>  <nl> metadata_delete_all ( mg ); <nl> + BlockHeapFree ( mygroup_heap , mg ); <nl> } <nl>  <nl> mygroup_t * mygroup_add ( const char * name ) <nl> mygroup_t * mygroup_find ( const char * name ) <nl> static void groupacs_des ( groupacs_t * ga ) <nl> { <nl> metadata_delete_all ( ga ); <nl> - /* XXX nothing */ <nl> + BlockHeapFree ( groupacs_heap , ga ); <nl> } <nl>  <nl> groupacs_t * groupacs_add ( mygroup_t * mg , myuser_t * mu , unsigned int flags )
void help_display_as_subcmd ( sourceinfo_t * si , service_t * service , const char * su <nl> if (! help_file ) <nl> { <nl> command_fail ( si , fault_nosuch_target , _ (" Could not get help file for \ 2 % s \ 2 ."), command ); <nl> + free ( ccommand ); <nl> return ; <nl> } <nl> 
static void can_register ( hook_channel_register_check_t * req ) <nl>  <nl> return_if_fail ( req != NULL ); <nl>  <nl> + /* no point in moderating registrations from those who have PRIV_CHAN_ADMIN since they can <nl> + * approve them anyway . -- nenolod <nl> + */ <nl> + if ( has_priv ( req -> si , PRIV_CHAN_ADMIN )) <nl> + return ; <nl> + <nl> req -> approved ++; <nl>  <nl> cs = csreq_create ( req -> name , entity ( req -> si -> smu ));
PHP_FUNCTION ( swoole_server_sendfile ) <nl> memcpy ( buffer , filename , send_data . info . len ); <nl> buffer [ send_data . info . len ] = 0 ; <nl> send_data . info . len ++; <nl> + send_data . length = 0 ; <nl>  <nl> send_data . data = buffer ; <nl> SW_CHECK_RETURN ( serv -> factory . finish (& serv -> factory , & send_data ));
PHP_MINFO_FUNCTION ( swoole ) <nl> # ifdef HAVE_PTHREAD_BARRIER <nl> php_info_print_table_row ( 2 , " pthread_barrier ", " enabled "); <nl> # endif <nl> - <nl> +# ifdef SW_USE_JEMALLOC <nl> + php_info_print_table_row ( 2 , " jemalloc ", " enabled "); <nl> +# endif <nl> +# ifdef SW_USE_TCMALLOC <nl> + php_info_print_table_row ( 2 , " tcmalloc ", " enabled "); <nl> +# endif <nl> +# ifdef SW_USE_HUGEPAGE <nl> + php_info_print_table_row ( 2 , " hugepage ", " enabled "); <nl> +# endif <nl> php_info_print_table_end (); <nl>  <nl> DISPLAY_INI_ENTRIES ();
int swServer_master_onAccept ( swReactor * reactor , swEvent * event ) <nl> { <nl> if ( swSSL_create ( conn , 0 ) < 0 ) <nl> { <nl> - conn -> active = 0 ; <nl> + bzero ( conn , sizeof ( swConnection )); <nl> close ( new_fd ); <nl> } <nl> } <nl> int swServer_master_onAccept ( swReactor * reactor , swEvent * event ) <nl>  <nl> if ( ret < 0 ) <nl> { <nl> + bzero ( conn , sizeof ( swConnection )); <nl> close ( new_fd ); <nl> return SW_OK ; <nl> }
class ModuleSSLGnuTLS : public Module <nl>  <nl> // This may be on a large ( once a day or week ) timer eventually . <nl> GenerateDHParams (); <nl> + <nl> + delete Conf ; <nl> } <nl>  <nl> void GenerateDHParams () <nl> class ModuleSSLGnuTLS : public Module <nl>  <nl> virtual ~ ModuleSSLGnuTLS () <nl> { <nl> - delete Conf ; <nl> gnutls_dh_params_deinit ( dh_params ); <nl> gnutls_certificate_free_credentials ( x509_cred ); <nl> gnutls_global_deinit ();
class SilenceMessage : public ClientProtocol :: Message <nl> : ClientProtocol :: Message (" SILENCE ") <nl> { <nl> PushParam ( mask ); <nl> - PushParamRef ( flags ); <nl> + PushParam ( flags ); <nl> } <nl> }; <nl> 
int InspIRCd :: Run () <nl>  <nl> int main ( int argc , char ** argv ) <nl> { <nl> - InspIRCd TittyBiscuits = new InspIRCd ( argc , argv ); <nl> + InspIRCd * TittyBiscuits = new InspIRCd ( argc , argv ); <nl> TittyBiscuits -> Run (); <nl> delete TittyBiscuits ; <nl> return 0 ;
class CoreExport XLine : public classbase <nl> free ( reason ); <nl> free ( source ); <nl> } <nl> + <nl> + /** Returns true whether or not the given user is covered by this line . <nl> + */ <nl> + virtual bool Matches ( User * u ); <nl> + <nl> /** The time the line was added . <nl> */ <nl> time_t set_time ;
class ModuleNickFlood : public Module <nl>  <nl> virtual int OnUserPreNick ( userrec * user , const std :: string & newnick ) <nl> { <nl> + if ( isdigit ( newnick [ 0 ])) /* allow switches to UID */ <nl> + return 0 ; <nl> + <nl> for ( UCListIter i = user -> chans . begin (); i != user -> chans . end (); i ++) <nl> { <nl> chanrec * channel = i -> first ;
class cmd_shun : public Command <nl>  <nl> if ( pcnt == 1 ) <nl> { <nl> - if ( ServerInstance -> XLines -> DelLine ( parameters [ 0 ], " S ", user )) <nl> + if ( ServerInstance -> XLines -> DelLine ( parameters [ 0 ], " SHUN ", user )) <nl> { <nl> ServerInstance -> SNO -> WriteToSnoMask (' x ',"% s Removed shun on % s .", user -> nick , parameters [ 0 ]); <nl> }
class cmd_spynames : public command_t <nl> return CMD_FAILURE ; <nl> } <nl>  <nl> - if ( ServerInstance -> Parser -> LoopCall ( user , this , parameters , pcnt , 1 )) <nl> + if ( ServerInstance -> Parser -> LoopCall ( user , this , parameters , pcnt , 0 )) <nl> return CMD_FAILURE ; <nl>  <nl> c = ServerInstance -> FindChan ( parameters [ 0 ]);
bool LoadConf ( const char * filename , std :: stringstream * target , std :: stringstream <nl> std :: string newstuff = merge . str (); <nl> * target << newstuff ; <nl> } <nl> + else <nl> + { <nl> + // the error propogates up to its parent recursively <nl> + // causing the config reader to bail at the top level . <nl> + fclose ( conf ); <nl> + return false ; <nl> + } <nl> } <nl> else <nl> {
ModeAction ModeChannelLimit :: OnModeChange ( userrec * source , userrec * dest , chanre <nl> return MODEACTION_DENY ; <nl> } <nl>  <nl> + parameter = ConvToStr ( limit ); <nl> + <nl> /* Set new limit */ <nl> channel -> limit = limit ; <nl> channel -> modes [ CM_LIMIT ] = 1 ;
void TreeSocket :: OnTimeout () <nl>  <nl> void TreeSocket :: Close () <nl> { <nl> - if ( fd != - 1 ) <nl> - ServerInstance -> GlobalCulls . AddItem ( this ); <nl> + if ( fd < 0 ) <nl> + return ; <nl> + <nl> + ServerInstance -> GlobalCulls . AddItem ( this ); <nl> this -> BufferedSocket :: Close (); <nl> SetError (" Remote host closed connection "); <nl> 
NativeWindowWin :: NativeWindowWin ( content :: WebContents * web_contents , <nl> OnViewWasResized (); <nl>  <nl> if ( g_exe_icon == NULL ) <nl> - g_exe_icon = :: LoadImage ( GetModuleHandle ( NULL ), L " IDR_MAINFRAME ", <nl> + g_exe_icon = :: LoadImage ( GetModuleHandle ( NULL ), MAKEINTRESOURCE ( 1 ), <nl> IMAGE_ICON , 0 , 0 , 0 ); <nl> :: SendMessage ( window_ -> GetNativeWindow (), <nl> WM_SETICON ,
gfx :: Image Clipboard :: ReadImage ( mate :: Arguments * args ) { <nl> void Clipboard :: WriteImage ( const gfx :: Image & image , mate :: Arguments * args ) { <nl> ui :: ScopedClipboardWriter writer ( GetClipboardType ( args )); <nl> SkBitmap bmp ; <nl> + // TODO ( ferreus ): Replace with sk_tools_utils :: copy_to ( chrome60 ) <nl> if ( image . AsBitmap (). deepCopyTo (& bmp )) { <nl> writer . WriteImage ( bmp ); <nl> } else {
Archive ::~ Archive () { <nl> file_ . TakePlatformFile (); <nl> } <nl> # endif <nl> - base :: PostTaskWithTraits ( <nl> - FROM_HERE , <nl> - { base :: MayBlock (), base :: TaskPriority :: BACKGROUND , <nl> - base :: TaskShutdownBehavior :: CONTINUE_ON_SHUTDOWN }, <nl> - base :: Bind ([]( base :: File file ) { file . Close (); }, Passed (& file_ ))); <nl> + base :: ThreadRestrictions :: ScopedAllowIO allow_io ; <nl> + file_ . Close (); <nl> } <nl>  <nl> bool Archive :: Init () {
/* unzip . c -- IO for uncompress . zip files using zlib <nl> + <nl> + Modified for Quake III Arena to use the Z_Malloc () memory pool ; <nl> + this means a system copy of minizip is not a suitable replacement . <nl> + <nl> + Based on minizip : <nl> + <nl> Version 1 . 01e , February 12th , 2005 <nl>  <nl> Copyright ( C ) 1998 - 2005 Gilles Vollant
void AsmCall ( void ) { <nl> " doret : \ n \ t " \ <nl> " ret \ n \ t " \ <nl> : "= rm " ( callSyscallNum ), "= rm " ( callProgramStack ), "= rm " ( callOpStack ) \ <nl> - : " rm " ( instructionPointers ) \ <nl> + : " m " ( instructionPointers ) \ <nl> : " ax ", " di ", " si ", " cx " \ <nl> ); <nl> }
void compute_subject_downtime_times ( time_t start_time , time_t end_time , avail_su <nl> } <nl> saved_status = temp_as -> entry_type ; <nl> saved_stamp = temp_as -> time_stamp ; <nl> + <nl> + /* check if first time is before schedule downtime */ <nl> + if ( saved_stamp < start_time ) <nl> + saved_stamp = start_time ; <nl> + <nl> } <nl> } <nl> 
DEFUN ( ipv6_ospf6_priority , <nl>  <nl> oi -> priority = strtol ( argv [ 0 ], NULL , 10 ); <nl>  <nl> - if ( oi -> area ) <nl> + if ( oi -> area && <nl> + ( oi -> state == OSPF6_INTERFACE_DROTHER || <nl> + oi -> state == OSPF6_INTERFACE_BDR || <nl> + oi -> state == OSPF6_INTERFACE_DR )) <nl> ospf6_interface_state_change ( dr_election ( oi ), oi ); <nl>  <nl> return CMD_SUCCESS ;
static struct static_route * static_route_alloc () <nl>  <nl> s_route = XCALLOC ( MTYPE_PIM_STATIC_ROUTE , sizeof (* s_route )); <nl> if (! s_route ) { <nl> - zlog_err (" PIM XCALLOC (% u ) failure ", sizeof (* s_route )); <nl> + zlog_err (" PIM XCALLOC (% zu ) failure ", sizeof (* s_route )); <nl> return 0 ; <nl> } <nl> return s_route ;
community_del_val ( struct community * com , u_int32_t * val ) <nl> c = com -> size - i - 1 ; <nl>  <nl> if ( c > 0 ) <nl> - memcpy ( com -> val + i , com -> val + ( i + 1 ), c * sizeof ( val )); <nl> + memcpy ( com -> val + i , com -> val + ( i + 1 ), c * sizeof (* val )); <nl>  <nl> com -> size --; <nl> 
ospf_recv_packet ( int fd , struct interface ** ifp , struct stream * ibuf ) <nl>  <nl> ip_len = iph -> ip_len ; <nl>  <nl> -# if ! defined ( GNU_LINUX ) && ( OpenBSD < 200311 ) <nl> +# if ! defined ( GNU_LINUX ) && ( OpenBSD < 200311 ) && ( __FreeBSD_version < 1000000 ) <nl> /* <nl> * Kernel network code touches incoming IP header parameters , <nl> * before protocol specific processing .
int transform_save ( struct augeas * aug , struct tree * xfm , <nl> goto done ; <nl> } <nl>  <nl> + text = append_newline ( text , strlen ( text )); <nl> + <nl> // FIXME : We might have to create intermediary directories <nl> // to be able to write augnew , but we have no idea what permissions <nl> // etc . they should get . Just the process default ?
static void dump_ctx ( struct ctx * ctx ) { <nl> * Values <nl> */ <nl> static void print_tree ( FILE * out , int indent , struct tree * tree ) { <nl> + if ( tree == NULL ) { <nl> + fprintf ( out , "( null tree )\ n "); <nl> + return ; <nl> + } <nl> list_for_each ( t , tree ) { <nl> for ( int i = 0 ; i < indent ; i ++) fputc (' ', out ); <nl> fprintf ( out , "{ ");
int main ( int argc , char ** argv ) { <nl> } <nl> } <nl> putchar ('\ n '); <nl> + free ( rx ); <nl> } <nl>  <nl> return 0 ;
# include " fish_version . h " <nl>  <nl> /** Command used to start fishd */ <nl> -# define FISHD_CMD L " fishd ^ / tmp / fishd . log .% s " <nl> +# define FISHD_CMD L " fishd ^ / dev / null " <nl>  <nl> // Version for easier debugging <nl> //# define FISHD_CMD L " fishd "
universal_notifier_t :: notifier_strategy_t universal_notifier_t :: resolve_default_ <nl> } <nl> # if FISH_NOTIFYD_AVAILABLE <nl> return strategy_notifyd ; <nl> +# elif defined ( __CYGWIN__ ) <nl> + return strategy_shmem_polling ; <nl> # else <nl> return strategy_named_pipe ; <nl> # endif
static int indent ( string_buffer_t * out , wchar_t * in , int flags ) <nl> { <nl> indent --; <nl> } <nl> + /* case should have the same indent level as switch */ <nl> + else if ( wcscmp ( unesc , L " case " ) == 0 ) <nl> + { <nl> + indent --; <nl> + } <nl> else if ( wcscmp ( unesc , L " end " ) == 0 ) <nl> { <nl> indent --;
ecall ( mrb_state * mrb , int i ) <nl> mrb_value * self = mrb -> c -> stack ; <nl> struct RObject * exc ; <nl>  <nl> + if ( i < 0 ) return ; <nl> p = mrb -> c -> ensure [ i ]; <nl> if (! p ) return ; <nl> if ( mrb -> c -> ci -> eidx > i )
genop_peep ( codegen_scope * s , mrb_code i , int val ) <nl> return 0 ; <nl> } <nl> } <nl> + if ( c0 == OP_LOADNIL ) { <nl> + if ( GETARG_B ( i ) == GETARG_A ( i0 )) { <nl> + s -> pc --; <nl> + return 0 ; <nl> + } <nl> + } <nl> break ; <nl> case OP_JMPIF : <nl> case OP_JMPNOT :
mrb_init_class ( mrb_state * mrb ) <nl> mrb_define_method ( mrb , mod , " define_method ", mod_define_method , ARGS_REQ ( 1 )); <nl>  <nl> mrb_define_method ( mrb , mod , "===", mrb_mod_eqq , ARGS_REQ ( 1 )); <nl> + mrb_undef_method ( mrb , cls , " append_features "); <nl> }
main ( int argc , char ** argv ) <nl> } <nl>  <nl> mrb = mrb_open (); <nl> + if ( mrb == NULL ) { <nl> + fputs (" Invalid mrb_state , exiting mruby - strip \ n ", stderr ); <nl> + return EXIT_FAILURE ; <nl> + } <nl>  <nl> ireps = ( mrb_irep **) malloc ( sizeof ( mrb_irep *) * argc ); <nl> for ( i = args_result ; i < argc ; ++ i ) {
main ( int argc , char ** argv ) <nl> char_index = 0 ; <nl> while (( last_char = getchar ()) != '\ n ') { <nl> if ( last_char == EOF ) break ; <nl> - if ( char_index > sizeof ( last_code_line )- 2 ) { <nl> + if ( char_index >= sizeof ( last_code_line )- 2 ) { <nl> fputs (" input string too long \ n ", stderr ); <nl> continue ; <nl> }
mrb_irep_free ( mrb_state * mrb , mrb_irep * irep ) <nl> } <nl> mrb_free ( mrb , irep -> pool ); <nl> mrb_free ( mrb , irep -> syms ); <nl> + mrb_free ( mrb , irep -> reps ); <nl> mrb_free ( mrb , ( void *) irep -> filename ); <nl> mrb_free ( mrb , irep -> lines ); <nl> mrb_debug_info_free ( mrb , irep -> debug_info );
read_section_debug ( mrb_state * mrb , const uint8_t * start , mrb_irep * irep ) <nl> result = read_debug_record ( mrb , bin , irep , & len , filenames , filenames_len ); <nl> if ( result != MRB_DUMP_OK ) goto debug_exit ; <nl>  <nl> + bin += len ; <nl> if (( bin - start ) != bin_to_uint32 ( header -> section_size )) { <nl> result = MRB_DUMP_GENERAL_FAILURE ; <nl> }
mrb_str_inspect ( mrb_state * mrb , mrb_value str ) <nl> buf [ i ] = p [ i ]; <nl> } <nl> mrb_str_cat ( mrb , result , buf , clen ); <nl> - p += clen ; <nl> + p += clen - 1 ; <nl> continue ; <nl> } <nl> # endif
size_t format_end ( char * buf , <nl> send_cert = "+ S = C "; <nl> break ; <nl> } <nl> - p = add_str ( endopts , sizeof ( endopts ), p , send_cert ); <nl> + add_str ( endopts , sizeof ( endopts ), p , send_cert ); <nl> } <nl> } <nl> 
static char ** new_list ( const char * value ) <nl>  <nl> /* avoid damaging original string */ <nl> val = clone_str ( value , " new_list value "); <nl> - end = val + strlen ( val ); <nl> + if ( val != NULL ) /* silence a coverity warning */ <nl> + end = val + strlen ( val ); <nl>  <nl> /* count number of items in string */ <nl> for ( b = val , count = 0 ; b < end ; ) {
static lsw_cert_ret pluto_process_certs ( struct state * st , <nl> add_crl_fetch_request_nss (& fdn , end_cert_dp ); <nl> wake_fetch_thread ( __FUNCTION__ ); <nl> } <nl> + DBGF ( DBG_X509 , " releasing end_cert_dp sent to crl fetch "); <nl> + free_generalNames ( end_cert_dp , false /* shallow */); <nl> } <nl> # endif <nl> 
ipsec_xmit_sanity_check_dev ( struct ipsec_xmit_state * ixs ) <nl> } <nl>  <nl> ixs -> physmtu = ixs -> physdev -> mtu ; <nl> - ixs -> cur_mtu = ixs -> physdev -> mtu ; <nl> + ixs -> cur_mtu = ixs -> dev -> mtu ; <nl>  <nl> ixs -> stats = ( struct net_device_stats *) &( ixs -> prv -> mystats ); <nl> 
int do_pam_authentication ( void * varg ) <nl>  <nl> retval = pam_start (" pluto ", arg -> name . ptr , & conv , & pamh ); <nl>  <nl> + /* Send the remote host address to PAM */ <nl> + if ( retval == PAM_SUCCESS ) <nl> + retval = pam_set_item ( pamh , PAM_RHOST , pluto_ip_str (& arg -> st -> st_remoteaddr )); <nl> /* Two factor authentication - Check that the user is valid , <nl> and then check if they are permitted access */ <nl> if ( retval == PAM_SUCCESS )
cherokee_validator_ldap_check ( cherokee_validator_ldap_t * ldap , <nl> /* Sanity checks <nl> */ <nl> if (( conn -> validator == NULL ) || <nl> - cherokee_buffer_is_empty (& conn -> validator -> user )) <nl> + cherokee_buffer_is_empty (& conn -> validator -> user ) || <nl> + cherokee_buffer_is_empty (& conn -> validator -> passwd )) <nl> return ret_error ; <nl>  <nl> size = cherokee_buffer_cnt_cspn (& conn -> validator -> user , 0 , "*()");
entry_free ( cherokee_nonce_table_t * nonces , <nl> { <nl> cherokee_list_del (& entry -> listed ); <nl> cherokee_avl_del (& nonces -> table , & entry -> nonce , NULL ); <nl> + cherokee_buffer_mrproper (& entry -> nonce ); <nl> free ( entry ); <nl> } <nl> 
parse_x_real_ip ( cherokee_logger_t * logger , cherokee_connection_t * conn ) <nl> } <nl>  <nl> p = val ; <nl> - while (* p ) { <nl> + while (* p && ( p - val < len )) { <nl> if ((* p == ' ') || (* p == ',')) { <nl> len = p - val ; <nl> break ;
cherokee_buffer_cnt_cspn ( cherokee_buffer_t * buf , cuint_t offset , const char * st <nl> crc_t <nl> cherokee_buffer_crc32 ( cherokee_buffer_t * buf ) <nl> { <nl> + if ( cherokee_buffer_is_empty ( buf )) <nl> + return 0 ; <nl> + <nl> return crc32_sz ( buf -> buf , buf -> len ); <nl> } <nl> 
local_file_exists ( cherokee_rule_extensions_t * rule , <nl> ret = cherokee_io_stat ( srv -> iocache , tmp , rule -> use_iocache , <nl> & nocache_info , & io_entry , & info ); <nl>  <nl> - is_file = S_ISREG ( info -> st_mode ); <nl> + if ( ret == ret_ok ) { <nl> + is_file = S_ISREG ( info -> st_mode ); <nl> + } <nl>  <nl> if ( io_entry ) { <nl> cherokee_iocache_entry_unref (& io_entry );
ExprResolveBoolean ( struct xkb_context * ctx , const ExprDef * expr , <nl>  <nl> case EXPR_INVERT : <nl> case EXPR_NOT : <nl> - ok = ExprResolveBoolean ( ctx , expr , set_rtrn ); <nl> + ok = ExprResolveBoolean ( ctx , expr -> unary . child , set_rtrn ); <nl> if ( ok ) <nl> * set_rtrn = !* set_rtrn ; <nl> return ok ;
skip_more_whitespace_and_comments : <nl>  <nl> /* LHS Keysym . */ <nl> if ( chr ( s , '<')) { <nl> - while ( peek ( s ) != '>' && ! eol ( s )) <nl> + while ( peek ( s ) != '>' && ! eol ( s ) && ! eof ( s )) <nl> buf_append ( s , next ( s )); <nl> if (! chr ( s , '>')) { <nl> scanner_err ( s , " unterminated keysym literal ");
get_controls ( struct xkb_keymap * keymap , xcb_connection_t * conn , <nl> xcb_xkb_get_controls_reply ( conn , cookie , NULL ); <nl>  <nl> FAIL_IF_BAD_REPLY ( reply , " XkbGetControls "); <nl> + FAIL_UNLESS ( reply -> numGroups > 0 && reply -> numGroups <= 4 ); <nl>  <nl> keymap -> enabled_ctrls = translate_controls_mask ( reply -> enabledControls ); <nl> keymap -> num_groups = reply -> numGroups ;
bool <nl> map_file ( FILE * file , char ** string_out , size_t * size_out ) <nl> { <nl> struct stat stat_buf ; <nl> - const int fd = fileno ( file ); <nl> + int fd ; <nl> char * string ; <nl>  <nl> /* Make sure to keep the errno on failure ! */ <nl> + fd = fileno ( file ); <nl> + if ( fd < 0 ) <nl> + return false ; <nl>  <nl> if ( fstat ( fd , & stat_buf ) != 0 ) <nl> return false ;
LookupModMask ( struct xkb_context * ctx , const void * priv , xkb_atom_t field , <nl> return false ; <nl>  <nl> str = xkb_atom_text ( ctx , field ); <nl> + if (! str ) <nl> + return false ; <nl>  <nl> if ( istreq ( str , " all ")) { <nl> * val_rtrn = MOD_REAL_MASK_ALL ;
ExprAppendMultiKeysymList ( ExprDef * expr , ExprDef * append ) <nl> darray_append ( expr -> keysym_list . symsNumEntries , numEntries ); <nl> darray_concat ( expr -> keysym_list . syms , append -> keysym_list . syms ); <nl>  <nl> - FreeStmt (( ParseCommon *) & append ); <nl> + FreeStmt (( ParseCommon *) append ); <nl>  <nl> return expr ; <nl> }
extern char * ptsname ( int ); <nl> extern int ptsname_r ( int , char *, size_t ); <nl> extern int getpt ( void ); <nl>  <nl> - static __inline__ int grantpt ( int __fd ) <nl> + static __inline__ int grantpt ( int __fd __attribute (( unused ))) <nl> { <nl> ( void ) __fd ; <nl> return 0 ; /* devpts does this all for us ! */
void * mmap64 ( void * addr , size_t size , int prot , int flags , int fd , off64_t offse <nl>  <nl> // prevent allocations large enough for ` end - start ` to overflow <nl> size_t rounded = BIONIC_ALIGN ( size , PAGE_SIZE ); <nl> - if ( rounded < size || size > PTRDIFF_MAX ) { <nl> + if ( rounded < size || rounded > PTRDIFF_MAX ) { <nl> errno = ENOMEM ; <nl> return MAP_FAILED ; <nl> }
void get_malloc_leak_info ( uint8_t ** info , size_t * overallSize , <nl> // debug_log (" info = % p \ n ", info ); <nl> if (* info == NULL ) { <nl> * overallSize = 0 ; <nl> - goto done ; <nl> + goto out_nomem_info ; <nl> } <nl>  <nl> // debug_log (" sorting list ...\ n "); <nl> void get_malloc_leak_info ( uint8_t ** info , size_t * overallSize , <nl> head += * infoSize ; <nl> } <nl>  <nl> + out_nomem_info : <nl> dlfree ( list ); <nl>  <nl> done :
struct android_namespace_link_t { <nl> } <nl>  <nl> bool is_accessible ( const char * soname ) const { <nl> + if ( soname == nullptr ) { <nl> + return false ; <nl> + } <nl> return allow_all_shared_libs_ || shared_lib_sonames_ . find ( soname ) != shared_lib_sonames_ . end (); <nl> } <nl> 
HRESULT ChakraRTInterface :: ParseConfigFlags () <nl> else <nl> { <nl> hr = WideStringToNarrowDynamic ( fileNameWide , & m_argInfo -> filename ); <nl> + SysFreeString ( fileNameWide ); <nl> if ( FAILED ( hr )) <nl> { <nl> Assert ( hr == E_OUTOFMEMORY );
namespace Js <nl> Output :: Print ( _u (" ObjectHeaderInlining : Moving inlined properties out of the object header .\ n ")); <nl> Output :: Flush (); <nl> } <nl> - Var * const newInlineSlots = reinterpret_cast < Var *>( object + 1 ); <nl> + Field ( Var ) * const newInlineSlots = reinterpret_cast < Field ( Var ) *>( object + 1 ); <nl> PropertyIndex i = newInlineSlotCapacity ; <nl> do <nl> {
 <nl> int Icon :: stdSize ( int v ) <nl> { <nl> - if ( v < 20 ) { <nl> + if ( v < 19 ) { <nl> return 16 ; <nl> } else if ( v < 28 ) { <nl> return 22 ;
static Image * ReadTIFFImage ( const ImageInfo * image_info , <nl> /* <nl> Convert stripped TIFF image . <nl> */ <nl> - extent = 4 *( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> + extent = 4 *(( image -> depth + 7 )/ 8 )*( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> strip_pixels =( unsigned char *) AcquireQuantumMemory ( extent , <nl> sizeof (* strip_pixels )); <nl> if ( strip_pixels == ( unsigned char *) NULL )
static Image * ReadMPCImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> if ( LocaleCompare ( keyword ," number - meta - channels ") == 0 ) <nl> { <nl> image -> number_meta_channels = StringToUnsignedLong ( options ); <nl> + if ( image -> number_meta_channels > MaxPixelChannels ) <nl> + ThrowReaderException ( CorruptImageError , <nl> + " ImproperImageHeader "); <nl> break ; <nl> } <nl> break ;
static MagickBooleanType DrawStrokePolygon ( Image * image , <nl> for ( p = primitive_info ; p -> primitive != UndefinedPrimitive ; p += p -> coordinates ) <nl> { <nl> stroke_polygon = TraceStrokePolygon ( draw_info , p ); <nl> + if ( stroke_polygon == ( PrimitiveInfo *) NULL ) <nl> + { <nl> + status = 0 ; <nl> + break ; <nl> + } <nl> status &= DrawPolygonPrimitive ( image , clone_info , stroke_polygon , exception ); <nl> if ( status == 0 ) <nl> break ;
static void WriteTo8BimProfile ( Image * image , const char * name , <nl> count =( ssize_t ) value ; <nl> if (( count & 0x01 ) != 0 ) <nl> count ++; <nl> - if (( p > ( datum + length - count )) || ( count > ( ssize_t ) length )) <nl> + if (( count < 0 ) || ( p > ( datum + length - count )) || <nl> + ( count > ( ssize_t ) length )) <nl> break ; <nl> if ( id != profile_id ) <nl> p += count ;
void Magick :: Image :: read ( MagickCore :: Image * image , <nl> if (! quiet ()) <nl> throwExceptionExplicit ( MagickCore :: ImageWarning , <nl> " No image was loaded ."); <nl> + return ; <nl> } <nl> ThrowImageException ; <nl> }
MagickExport Image * WaveletDenoiseImage ( const Image * image , <nl> difference ; <nl>  <nl> difference = pixels [ low_pass ]- pixels [ high_pass ]; <nl> - pixels [ i ]+= copysignf ( fmaxf ( fabsf ( difference )- magnitude , 0 . 0f ), <nl> - difference ); <nl> + pixels [ i ]+= copysignf ( fmaxf ( fabsf ( difference )- magnitude - <nl> + softness * magnitude , 0 . 0f ), difference ); <nl> } <nl> } <nl> /*
static void CL_API_CALL DestroyMagickCLCacheInfoAndPixels ( <nl> } <nl> } <nl> pixels = info -> pixels ; <nl> + RelinquishMagickResource ( MemoryResource , info -> length ); <nl> DestroyMagickCLCacheInfo ( info ); <nl> ( void ) RelinquishAlignedMemory ( pixels ); <nl> }
static Image * ReadYCBCRImage ( const ImageInfo * image_info , <nl> if ( status == MagickFalse ) <nl> { <nl> quantum_info = DestroyQuantumInfo ( quantum_info ); <nl> + canvas_image = DestroyImage ( canvas_image ); <nl> return ( DestroyImageList ( image )); <nl> } <nl> SetImageColorspace ( image , YCbCrColorspace , exception );
static Image * ReadOneJNGImage ( MngInfo * mng_info , <nl> ( void ) LogMagickEvent ( CoderEvent , GetMagickModule (), <nl> " Copying JDAT chunk data to color_blob ."); <nl>  <nl> - ( void ) WriteBlob ( color_image , length , chunk ); <nl> - <nl> if ( length != 0 ) <nl> - chunk =( unsigned char *) RelinquishMagickMemory ( chunk ); <nl> + { <nl> + ( void ) WriteBlob ( color_image , length , chunk ); <nl> + chunk =( unsigned char *) RelinquishMagickMemory ( chunk ); <nl> + } <nl>  <nl> continue ; <nl> }
MagickExport Image * CloneImage ( const Image * image , const size_t columns , <nl> sizeof (* clone_image -> colormap )); <nl> if ( clone_image -> colormap == ( PixelInfo *) NULL ) <nl> { <nl> - clone_image = DestroyImage ( clone_image ); <nl> + image =( Image *) RelinquishMagickMemory ( image ); <nl> ThrowImageException ( ResourceLimitError ," MemoryAllocationFailed "); <nl> } <nl> ( void ) CopyMagickMemory ( clone_image -> colormap , image -> colormap , length *
ModuleExport MagickBooleanType ReadPSDLayers ( Image * image , <nl> if ( image -> debug != MagickFalse ) <nl> ( void ) LogMagickEvent ( CoderEvent , GetMagickModule (), <nl> " layer data is empty "); <nl> + if ( layer_info [ i ]. info != ( StringInfo *) NULL ) <nl> + layer_info [ i ]. info = DestroyStringInfo ( layer_info [ i ]. info ); <nl> continue ; <nl> } <nl> 
static Image * ReadPSDImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> image -> alpha_trait = UndefinedPixelTrait ; <nl> } <nl> } <nl> + if (( image -> depth == 1 ) && ( image -> storage_class != PseudoClass )) <nl> + ThrowReaderException ( CorruptImageError , " ImproperImageHeader "); <nl> has_merged_image = MagickTrue ; <nl> length = ReadBlobMSBLong ( image ); <nl> if ( length != 0 )
void CServer :: ProcessClientPacket ( CNetChunk * pPacket ) <nl> return ; <nl>  <nl> int Chunk = Unpacker . GetInt (); <nl> - int ChunkSize = 1024 - 128 ; <nl> - int Offset = Chunk * ChunkSize ; <nl> + unsigned int ChunkSize = 1024 - 128 ; <nl> + unsigned int Offset = Chunk * ChunkSize ; <nl> int Last = 0 ; <nl>  <nl> // drop faulty map data requests
bool console_input_special_binds ( INPUT_EVENT e , void * user_data ) <nl>  <nl> bool console_input_normal_binds ( INPUT_EVENT e , void * user_data ) <nl> { <nl> + // need to be ingame for these binds <nl> + if ( client_state () != CLIENTSTATE_ONLINE ) <nl> + return false ; <nl> + <nl> // don ' t handle invalid events and keys that arn ' t set to anything <nl> if ( e . key <= 0 || e . key >= KEY_LAST || keybindings [ e . key ][ 0 ] == 0 ) <nl> return false ;
evrpc_resume_request ( void * vbase , void * ctx , enum EVRPC_HOOK_RESULT res ) <nl>  <nl> (* pause -> cb )( pause -> ctx , res ); <nl> TAILQ_REMOVE ( head , pause , next ); <nl> + mm_free ( pause ); <nl> return ( 0 ); <nl> } <nl> 
evhttp_send_reply_start ( struct evhttp_request * req , int code , <nl> evhttp_add_header ( req -> output_headers , " Transfer - Encoding ", <nl> " chunked "); <nl> req -> chunked = 1 ; <nl> + } else { <nl> + req -> chunked = 0 ; <nl> } <nl> evhttp_make_header ( req -> evcon , req ); <nl> evhttp_write_buffer ( req -> evcon , NULL , NULL );
epoll_init ( struct event_base * base ) <nl> fd = epollop -> timerfd = timerfd_create ( CLOCK_MONOTONIC , TFD_NONBLOCK | TFD_CLOEXEC ); <nl> if ( epollop -> timerfd >= 0 ) { <nl> struct epoll_event epev ; <nl> + memset (& epev , 0 , sizeof ( epev )); <nl> epev . data . fd = epollop -> timerfd ; <nl> epev . events = EPOLLIN ; <nl> if ( epoll_ctl ( epollop -> epfd , EPOLL_CTL_ADD , fd , & epev ) < 0 ) {
event_set ( struct event * ev , int fd , short events , <nl> ev -> ev_arg = arg ; <nl> ev -> ev_fd = fd ; <nl> ev -> ev_events = events ; <nl> + ev -> ev_res = 0 ; <nl> ev -> ev_flags = EVLIST_INIT ; <nl> ev -> ev_ncalls = 0 ; <nl> ev -> ev_pncalls = NULL ;
int Ftp :: Do () <nl> if ( state != CONNECTED_STATE || Error ()) <nl> return MOVED ; <nl>  <nl> - if ( expect -> Has ( Expect :: FEAT )) <nl> + if ( expect -> Has ( Expect :: FEAT ) || conn -> quit_sent ) <nl> goto usual_return ; <nl>  <nl> # if USE_SSL
mmvJob :: mmvJob ( FileAccess * session , const ArgV * args , const char * t , FA :: open_mode <nl> { <nl> op . set ( args -> a0 ()); <nl> for ( int i = args -> getindex (); i < args -> count (); i ++) <nl> - wcd . push ( strdup ( args -> getarg ( i ))); <nl> + wcd . push ( xstrdup ( args -> getarg ( i ))); <nl> } <nl>  <nl> void mmvJob :: doOpen () const
public : <nl> if ( o || ! n ) <nl> return ; <nl> downloader = new const TorrentPeer *[ blk_count ]; <nl> - for ( int i = 0 ; i < blk_count ; i ++) <nl> + for ( unsigned i = 0 ; i < blk_count ; i ++) <nl> downloader [ i ]= 0 ; <nl> } <nl> const TorrentPeer *& d = downloader [ block ];
int uv__stdio_create ( uv_loop_t * loop , <nl>  <nl> case FILE_TYPE_PIPE : <nl> CHILD_STDIO_CRT_FLAGS ( buffer , i ) = FOPEN | FPIPE ; <nl> + break ; <nl>  <nl> case FILE_TYPE_CHAR : <nl> case FILE_TYPE_REMOTE :
static void uv_loop_init ( uv_loop_t * loop ) { <nl> loop -> active_udp_streams = 0 ; <nl>  <nl> loop -> last_err = uv_ok_ ; <nl> + <nl> + memset (& loop -> counters , 0 , sizeof loop -> counters ); <nl> } <nl>  <nl> 
int uv_run ( uv_loop_t * loop , uv_run_mode mode ) { <nl>  <nl> uv__update_time ( loop ); <nl> uv__run_timers ( loop ); <nl> + uv__run_pending ( loop ); <nl> uv__run_idle ( loop ); <nl> uv__run_prepare ( loop ); <nl> - uv__run_pending ( loop ); <nl>  <nl> timeout = 0 ; <nl> if (( mode & UV_RUN_NOWAIT ) == 0 )
void uv__pipe_close ( uv_pipe_t * handle ) { <nl> */ <nl> unlink ( handle -> pipe_fname ); <nl> free (( void *) handle -> pipe_fname ); <nl> + handle -> pipe_fname = NULL ; <nl> } <nl>  <nl> uv__stream_close (( uv_stream_t *) handle );
int uv_write ( uv_write_t * req , uv_stream_t * stream , uv_buf_t bufs [], int bufcnt , <nl> req -> type = UV_WRITE ; <nl> ngx_queue_init (& req -> queue ); <nl>  <nl> - if ( bufcnt < UV_REQ_BUFSML_SIZE ) { <nl> + if ( bufcnt <= UV_REQ_BUFSML_SIZE ) { <nl> req -> bufs = req -> bufsml ; <nl> } <nl> else {
int uv_async_init ( uv_async_t * async , uv_async_cb async_cb ) { <nl>  <nl> int uv_async_send ( uv_async_t * async ) { <nl> ev_async_send ( EV_DEFAULT_UC_ & async -> async_watcher ); <nl> + return 0 ; <nl> } <nl>  <nl> 
start : <nl> } <nl>  <nl> if ( n < 0 ) { <nl> - if ( errno != EAGAIN ) { <nl> + if ( errno != EAGAIN && errno != EWOULDBLOCK ) { <nl> /* Error */ <nl> req -> error = errno ; <nl> stream -> write_queue_size -= uv__write_req_size ( req );
static void simple_decode_row ( x3f_info_t * I , x3f_directory_entry_t * DE , <nl> x3f_image_data_t * ID = & DEH -> data_subsection . image_data ; <nl> x3f_huffman_t * HUF = ID -> huffman ; <nl>  <nl> + if ( row * row_stride > ID -> data_size - ( ID -> columns * sizeof ( uint32_t ))) <nl> + throw LIBRAW_EXCEPTION_IO_CORRUPT ; <nl> uint32_t * data = ( uint32_t *)(( unsigned char *) ID -> data + row * row_stride ); <nl>  <nl> uint16_t c [ 3 ] = { 0 , 0 , 0 };
int MK_EXPORT _mkp_network_io_create_socket ( int domain , int type , int protocol ); <nl> int MK_EXPORT _mkp_network_io_bind ( int socket_fd , const struct sockaddr * addr , <nl> socklen_t addrlen , int backlog ); <nl> int MK_EXPORT _mkp_network_io_server ( int port , char * listen_addr , int reuse_port ); <nl> + int MK_EXPORT _mkp_network_io_buffer_size (); <nl> int MK_EXPORT _mkp_event_read ( int sockfd ); <nl> int MK_EXPORT _mkp_event_write ( int sockfd ); <nl> int MK_EXPORT _mkp_event_error ( int sockfd );
static int get_port_by_socket ( int fd ) <nl> socklen_t len = sizeof ( struct sockaddr_in ); <nl> struct sockaddr_in m_addr ; <nl>  <nl> - getpeername ( fd , ( struct sockaddr *) & m_addr , & len ); <nl> + int ret = getpeername ( fd , ( struct sockaddr *) & m_addr , & len ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> return ( int ) m_addr . sin_port ; <nl> } <nl> 
# include < assert . h > <nl> # include < string . h > <nl>  <nl> -# if defined ( __linux__ ) <nl> -# include < ucontext . h > <nl> -# elif defined ( __APPLE__ ) <nl> +# if defined ( __APPLE__ ) <nl> # include < sys / ucontext . h > <nl> +# else <nl> +# include < ucontext . h > <nl> # endif <nl>  <nl> # include < limits . h >
time_t mk_utils_gmt2utime ( char * date ) <nl> { <nl> time_t new_unix_time ; <nl> struct tm t_data ; <nl> + memset (& t_data , 0 , sizeof ( struct tm )); <nl>  <nl> if (! strptime ( date , GMT_DATEFORMAT , ( struct tm *) & t_data )) { <nl> return - 1 ;
int mk_http_parser ( struct mk_http_request * req , struct mk_http_parser * p , <nl> p -> chars += 7 ; <nl>  <nl> request_set (& req -> protocol_p , p , buffer ); <nl> + req -> protocol_p . len = 8 ; <nl> mk_http_set_minor_version ( buffer [ tmp + 7 ]); <nl> continue ; <nl> }
int mk_sched_check_timeouts ( struct sched_list_node * sched ) <nl>  <nl> mk_sched_remove_client ( sched , cs_node -> socket ); <nl> mk_session_remove ( cs_node -> socket ); <nl> + <nl> + /* This removal invalidated our iterator . Start over from the beginning . */ <nl> + node = rb_first ( cs_list ); <nl> + if (! node ) break ; <nl> } <nl> } <nl> }
static inline void mk_stream_set ( struct mk_stream * stream , int type , <nl> * performance and aim to make things easier . The COPYBUF type is not <nl> * used by Monkey core , at the moment the only caller is the CGI plugin . <nl> */ <nl> - if (! stream && type == MK_STREAM_COPYBUF ) { <nl> + if ( type == MK_STREAM_COPYBUF ) { <nl> stream = mk_mem_malloc ( sizeof ( struct mk_stream )); <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> char * sites_conf_dir = NULL ; <nl> char * plugins_conf_dir = NULL ; <nl> char * mimes_conf_file = NULL ; <nl> + struct mk_server * server ; <nl>  <nl> static const struct option long_opts [] = { <nl> { " configdir ", required_argument , NULL , ' c ' },
update_focus_app ( ShellWindowTracker * self ) <nl> } <nl>  <nl> set_focus_app ( self , new_focus_app ); <nl> + <nl> + g_clear_object (& new_focus_app ); <nl> } <nl>  <nl> static void
shell_doc_system_open ( ShellDocSystem * system , <nl> app_exec_quoted = g_regex_replace ( regex , app_exec , - 1 , 0 , "%%", 0 , NULL ); <nl> g_regex_unref ( regex ); <nl>  <nl> - app_info = g_app_info_create_from_commandline ( app_exec , NULL , 0 , NULL ); <nl> + app_info = g_app_info_create_from_commandline ( app_exec_quoted , NULL , 0 , NULL ); <nl> + g_free ( app_exec_quoted ); <nl>  <nl> /* The point of passing an app launch context to <nl> launch () is mostly to get startup notification and
recorder_record_frame ( ShellRecorder * recorder ) <nl>  <nl> size = recorder -> area . width * recorder -> area . height * 4 ; <nl>  <nl> - data = g_malloc ( recorder -> area . width * 4 * recorder -> area . height ); <nl> + data = g_malloc ( size ); <nl> cogl_framebuffer_read_pixels ( cogl_get_draw_framebuffer (), <nl> recorder -> area . x , <nl> recorder -> area . y ,
st_box_layout_get_paint_volume ( ClutterActor * actor , <nl> ClutterActorBox content_box ; <nl> ClutterVertex origin ; <nl>  <nl> + /* Setting the paint volume does not make sense when we don ' t have any allocation */ <nl> + if (! clutter_actor_has_allocation ( actor )) <nl> + return FALSE ; <nl> + <nl> /* When have an adjustment we are clipped to the content box , so base <nl> * our paint volume on that . */ <nl> if ( priv -> hadjustment || priv -> vadjustment )
NPP_GetValue ( NPP instance , <nl>  <nl> *( NPObject **) value = funcs . createobject ( instance , & plugin_class ); <nl> break ; <nl> + <nl> + case NPPVpluginNeedsXEmbed : <nl> + *( bool *) value = TRUE ; <nl> + break ; <nl> + <nl> default : <nl> ; <nl> }
_shell_app_remove_window ( ShellApp * app , <nl> g_object_unref ( window ); <nl> app -> windows = g_slist_remove ( app -> windows , window ); <nl>  <nl> + g_signal_emit ( app , shell_app_signals [ WINDOWS_CHANGED ], 0 ); <nl> + <nl> if ( app -> windows == NULL ) <nl> disconnect_workspace_switch ( app ); <nl> }
static inline int SCSigGetFlowintType ( Signature * sig ) <nl> fi -> modifier == FLOWINT_MODIFIER_NE || <nl> fi -> modifier == FLOWINT_MODIFIER_GE || <nl> fi -> modifier == FLOWINT_MODIFIER_GT || <nl> + fi -> modifier == FLOWINT_MODIFIER_NOTSET || <nl> fi -> modifier == FLOWINT_MODIFIER_ISSET ) { <nl> read ++; <nl> } else {
void StreamTcpReassembleMemuseCounter ( ThreadVars * tv , TcpReassemblyThreadCtx * rt <nl> * \ retval 0 if not in bounds <nl> */ <nl> int StreamTcpReassembleCheckMemcap ( uint32_t size ) { <nl> - if ( stream_config . reassembly_memcap == 0 || size + SC_ATOMIC_GET ( ra_memuse ) <= stream_config . reassembly_memcap ) <nl> + if ( stream_config . reassembly_memcap == 0 || <nl> + ( uint64_t )(( uint64_t ) size + SC_ATOMIC_GET ( ra_memuse )) <= stream_config . reassembly_memcap ) <nl> return 1 ; <nl> return 0 ; <nl> }
int DetectEngineInspectHttpStatCode ( DetectEngineCtx * de_ctx , <nl> } <nl>  <nl> # ifdef DEBUG <nl> - SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSMDMATCH ]; <nl> + SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSCDMATCH ]; <nl> DetectContentData * co = ( DetectContentData *) sm -> ctx ; <nl> SCLogDebug (" co -> id %" PRIu32 , co -> id ); <nl> # endif
static int DNP3CheckStartBytes ( const DNP3LinkHeader * header ) <nl> */ <nl> static int DNP3ContainsBanner ( const uint8_t * input , uint32_t len ) <nl> { <nl> - return memmem ( input , len , banner , strlen ( banner )) != NULL ; <nl> + return BasicSearch ( input , len , ( uint8_t *) banner , strlen ( banner )) != NULL ; <nl> } <nl>  <nl> /**
insert : <nl>  <nl> Frag * frag ; <nl> TAILQ_FOREACH ( frag , & tracker -> frags , next ) { <nl> - if ( frag_offset < frag -> offset ) <nl> + if ( new -> offset < frag -> offset ) <nl> break ; <nl> } <nl> if ( frag == NULL ) {
static int StatsOutput ( ThreadVars * tv ) <nl> const StatsCounter * pc = NULL ; <nl> void * td = stats_thread_data ; <nl>  <nl> + if ( counters_global_id == 0 ) <nl> + return - 1 ; <nl> + <nl> if ( stats_table . nstats == 0 ) { <nl> StatsThreadRegister (" Global ", & stats_ctx -> global_counter_ctx ); <nl> 
int DetectHttpHeaderMatch ( ThreadVars * t , DetectEngineThreadCtx * det_ctx , <nl>  <nl> SCMutexLock (& f -> m ); <nl>  <nl> - if ( htp_state == NULL ) { <nl> + if ( htp_state == NULL || htp_state -> connp == NULL || <nl> + htp_state -> connp -> conn == NULL ) { <nl> SCLogDebug (" No htp state , no match at http header data "); <nl> goto end ; <nl> }
static int DetectFlowvarSetup ( DetectEngineCtx * de_ctx , Signature * s , char * raws <nl> fd = SCMalloc ( sizeof ( DetectFlowvarData )); <nl> if ( unlikely ( fd == NULL )) <nl> goto error ; <nl> + memset ( fd , 0x00 , sizeof (* fd )); <nl>  <nl> fd -> content = SCMalloc ( contentlen ); <nl> if ( unlikely ( fd -> content == NULL ))
int SigAddressPrepareStage4 ( DetectEngineCtx * de_ctx ) <nl>  <nl> SigGroupHeadBuildNonPrefilterArray ( de_ctx , sgh ); <nl>  <nl> + SigGroupHeadInitDataFree ( sgh -> init ); <nl> + sgh -> init = NULL ; <nl> + <nl> sgh -> id = idx ; <nl> cnt ++; <nl> }
static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> uint8_t flags , <nl> uint32_t * buffer_len ) <nl> { <nl> + uint8_t * headers_buffer = NULL ; <nl> int index = 0 ; <nl> * buffer_len = 0 ; <nl>  <nl> static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> goto end ; <nl>  <nl> htp_header_t * h = NULL ; <nl> - uint8_t * headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> + headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> size_t headers_buffer_len = 0 ; <nl>  <nl> table_iterator_reset ( headers );
DetectPcreData * DetectPcreParse ( char * regexstr ) <nl> SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' I '"); <nl> goto error ; <nl> } <nl> + if ( pd -> flags & DETECT_PCRE_RAWBYTES ) { <nl> + SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' B '"); <nl> + goto error ; <nl> + } <nl> pd -> flags |= DETECT_PCRE_URI ; <nl> break ; <nl> case ' H ': /* snort ' s option */
static int DNSUDPResponseParse ( Flow * f , void * dstate , <nl>  <nl> tx -> replied = 1 ; <nl> } <nl> - if ( dns_state != NULL && f != NULL ) { <nl> + if ( f != NULL ) { <nl> dns_state -> last_resp = f -> lastts ; <nl> } <nl> SCReturnInt ( 1 );
IMPORTED_FUNCTION * pe_parse_import_descriptor ( <nl> // I ' ve seen binaries where OriginalFirstThunk is zero . In this case <nl> // use FirstThunk . <nl>  <nl> - if ( offset < 0 ) <nl> + if ( offset <= 0 ) <nl> offset = pe_rva_to_offset ( pe , import_descriptor -> FirstThunk ); <nl>  <nl> if ( offset < 0 )
static ssize_t data_source_read_callback ( nghttp2_session * session , <nl> memcpy ( buf , stream -> upload_mem , nread ); <nl> stream -> upload_mem += nread ; <nl> stream -> upload_len -= nread ; <nl> - stream -> upload_left -= nread ; <nl> + if ( data_s -> state . infilesize != - 1 ) <nl> + stream -> upload_left -= nread ; <nl> } <nl>  <nl> if ( stream -> upload_left == 0 )
CURLcode get_url_file_name ( char ** filename , const char * url ) <nl> Curl_safefree (* filename ); <nl> * filename = strdup ( buffer ); /* clone the buffer */ <nl> curl_free ( tdir ); <nl> + if (!* filename ) <nl> + return CURLE_OUT_OF_MEMORY ; <nl> } <nl> } <nl> # endif
static CURLcode pop3_doing ( struct connectdata * conn , bool * dophase_done ) <nl> CURLcode result ; <nl> result = pop3_multi_statemach ( conn , dophase_done ); <nl>  <nl> - if (* dophase_done ) { <nl> + if (! result && * dophase_done ) { <nl> result = pop3_dophase_done ( conn , FALSE /* not connected */); <nl>  <nl> DEBUGF ( infof ( conn -> data , " DO phase is complete \ n "));
static CURLMcode multi_runsingle ( struct Curl_multi * multi , <nl>  <nl> case CURLM_STATE_TOOFAST : /* limit - rate exceeded in either direction */ <nl> /* if both rates are within spec , resume transfer */ <nl> + Curl_pgrsUpdate ( easy -> easy_conn ); <nl> if ( ( ( data -> set . max_send_speed == 0 ) || <nl> ( data -> progress . ulspeed < data -> set . max_send_speed )) && <nl> ( ( data -> set . max_recv_speed == 0 ) ||
static bool imap_endofresp ( struct connectdata * conn , char * line , size_t len , <nl> wordlen ++; <nl>  <nl> /* Does the server support the STARTTLS capability ? */ <nl> - if ( len >= 8 && ! memcmp ( line , " STARTTLS ", 8 )) <nl> + if ( wordlen >= 8 && ! memcmp ( line , " STARTTLS ", 8 )) <nl> imapc -> tls_supported = TRUE ; <nl>  <nl> /* Has the server explicitly disabled clear text authentication ? */
void curl_mime_free ( curl_mime * mime ) <nl> curl_mimepart * part ; <nl>  <nl> if ( mime ) { <nl> + mime_subparts_unbind ( mime ); /* Be sure it ' s not referenced anymore . */ <nl> while ( mime -> firstpart ) { <nl> part = mime -> firstpart ; <nl> mime -> firstpart = part -> nextpart ;
void ourWriteOut ( CURL * curl , struct OutStruct * outs , const char * writeinfo ) <nl> double doubleinfo ; <nl>  <nl> while ( ptr && * ptr ) { <nl> - if ('%' == * ptr ) { <nl> + if ('%' == * ptr && ptr [ 1 ]) { <nl> if ('%' == ptr [ 1 ]) { <nl> /* an escaped %- letter */ <nl> fputc ('%', stream );
gtls_connect_step3 ( struct connectdata * conn , <nl> infof ( data , "\ t common name : WARNING couldn ' t obtain \ n "); <nl> } <nl>  <nl> - if ( data -> set . ssl . certinfo ) { <nl> + if ( data -> set . ssl . certinfo && chainp ) { <nl> unsigned int i ; <nl>  <nl> result = Curl_ssl_init_certinfo ( data , cert_list_size );
CURLcode Curl_http2_switched ( struct connectdata * conn , <nl> " after upgrade : len =% zu \ n ", <nl> nread ); <nl>  <nl> - memcpy ( httpc -> inbuf , mem , nread ); <nl> + if ( nread ) <nl> + memcpy ( httpc -> inbuf , mem , nread ); <nl> httpc -> inbuflen = nread ; <nl>  <nl> nproc = nghttp2_session_mem_recv ( httpc -> h2 , ( const uint8_t *) httpc -> inbuf ,
libc5 - based Linux systems . Only include it on system that are known to <nl> require it ! */ <nl> # if defined ( _AIX ) || defined ( __NOVELL_LIBC__ ) || defined ( __NetBSD__ ) || \ <nl> - defined ( __minix ) || defined ( __SYMBIAN32__ ) || defined ( __INTEGRITY ) <nl> + defined ( __minix ) || defined ( __SYMBIAN32__ ) || defined ( __INTEGRITY ) || \ <nl> + defined ( ANDROID ) <nl> # include < sys / select . h > <nl> # endif <nl> 
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> * an error , use the strerror () string or if things are so bad that not <nl> * even that is good , set a bad string that mentions the error code . <nl> */ <nl> - char * str = curl_easy_strerror ( res ); <nl> + const char * str = curl_easy_strerror ( res ); <nl> if (! str ) <nl> failf ( data , " unspecified error % d ", ( int ) res ); <nl> else
int main ( void ) <nl> /* the DEBUGFUNCTION has no effect until we enable VERBOSE */ <nl> curl_easy_setopt ( curl , CURLOPT_VERBOSE , 1L ); <nl>  <nl> + /* example . com is redirected , so we tell libcurl to follow redirection */ <nl> + curl_easy_setopt ( curl , CURLOPT_FOLLOWLOCATION , 1L ); <nl> + <nl> curl_easy_setopt ( curl , CURLOPT_URL , " http :// example . com /"); <nl> res = curl_easy_perform ( curl ); <nl> /* Check for errors */
static void voutf ( struct GlobalConfig * config , <nl> ( void ) fwrite ( ptr , cut + 1 , 1 , config -> errors ); <nl> fputs ("\ n ", config -> errors ); <nl> ptr += cut + 1 ; /* skip the space too */ <nl> - len -= cut ; <nl> + len -= cut + 1 ; <nl> } <nl> else { <nl> fputs ( ptr , config -> errors );
static GlobCode glob_range ( URLGlob * glob , char ** patternp , <nl> } <nl> else <nl> step_n = 1 ; <nl> - if (* endp == ']') { <nl> + if ( endp && (* endp == ']')) { <nl> pattern = endp + 1 ; <nl> } <nl> else
static ParameterError str2double ( double * val , const char * str , long max ) <nl> num = strtod ( str , & endptr ); <nl> if ( errno == ERANGE ) <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> - if (( long ) num > max ) { <nl> + if ( num > max ) { <nl> /* too large */ <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> }
Ghoul2 Insert End <nl> return 0 ; <nl>  <nl> case CG_OPENJK_MENU_PAINT : <nl> - Menu_Paint ( ( menuDef_t *) VMA ( 1 ), ( intptr_t ) VMA ( 2 ) ); <nl> + Menu_Paint ( ( menuDef_t *) VMA ( 1 ), args [ 2 ] ); <nl> return 0 ; <nl>  <nl> case CG_OPENJK_GETMENU_BYNAME :
qboolean Sys_LowPhysicalMemory () <nl> if (! bAsked ) // just in case it takes a little time for GlobalMemoryStatus () to gather stats on <nl> { // stuff we don ' t care about such as virtual mem etc . <nl> bAsked = qtrue ; <nl> + <nl> + stat . dwLength = sizeof ( stat ); <nl> GlobalMemoryStatusEx (& stat ); <nl> } <nl> if ( sys_lowmem -> integer )
void CQuickSpriteSystem :: Add ( float * pointdata , color4ub_t color , vec2_t fog ) <nl> } <nl>  <nl> curcoord = mVerts [ mNextVert ]; <nl> - memcpy ( curcoord , pointdata , 4 * sizeof ( vec4_t )); <nl> + memcpy ( curcoord , pointdata , sizeof ( vec4_t )); <nl>  <nl> // Set up color <nl> curcolor = & mColors [ mNextVert ];
void Field_VariableSizeDraw ( field_t * edit , int x , int y , int width , int size , q <nl> drawLen = len - prestep ; <nl> } <nl>  <nl> + if ( drawLen < 0 ) <nl> + return ; <nl> + <nl> // extract < drawLen > characters from the field at < prestep > <nl> if ( drawLen >= MAX_STRING_CHARS ) { <nl> Com_Error ( ERR_DROP , " drawLen >= MAX_STRING_CHARS " );
void NPC_ChoosePainAnimation ( gentity_t * self , gentity_t * other , vec3_t point , i <nl> || PM_RollingAnim ( self -> client -> ps . legsAnim ) <nl> || ( BG_FlippingAnim ( self -> client -> ps . legsAnim )&&! PM_InCartwheel ( self -> client -> ps . legsAnim )) ) <nl> {// strong attacks , rolls , knockdowns , flips and spins cannot be interrupted by pain <nl> + return ; <nl> } <nl> else <nl> {// play an anim
static void GLimp_InitExtensions ( void ) <nl> // Find out how many general combiners they have . <nl> # define GL_MAX_GENERAL_COMBINERS_NV 0x854D <nl> GLint iNumGeneralCombiners = 0 ; <nl> - qglGetIntegerv ( GL_MAX_GENERAL_COMBINERS_NV , & iNumGeneralCombiners ); <nl> + if ( bNVRegisterCombiners ) <nl> + qglGetIntegerv ( GL_MAX_GENERAL_COMBINERS_NV , & iNumGeneralCombiners ); <nl>  <nl> // Only allow dynamic glows / flares if they have the hardware <nl> if ( bTexRectSupported && bARBVertexProgram && qglActiveTextureARB && glConfig . maxActiveTextures >= 4 &&
# define GLOBAL_METHOD_CACHE_SIZE 0x800 <nl> # endif <nl> # define LSB_ONLY ( x ) (( x ) & ~(( x ) - 1 )) <nl> -# define POWOR_OF_2_P ( x ) (( x ) == LSB_ONLY ( x )) <nl> -# if ! POWOR_OF_2_P ( GLOBAL_METHOD_CACHE_SIZE ) <nl> +# define POWER_OF_2_P ( x ) (( x ) == LSB_ONLY ( x )) <nl> +# if ! POWER_OF_2_P ( GLOBAL_METHOD_CACHE_SIZE ) <nl> # error GLOBAL_METHOD_CACHE_SIZE must be power of 2 <nl> # endif <nl> # ifndef GLOBAL_METHOD_CACHE_MASK
onig_vsnprintf_with_pattern ( UChar buf [], int bufsize , OnigEncoding enc , <nl> need = ( pat_end - pat ) * 4 + 4 ; <nl>  <nl> if ( n + need < ( size_t ) bufsize ) { <nl> - xstrcat (( char * ) buf , ": /", bufsize ); <nl> + static const char sep [] = ": /"; <nl> + memcpy (( char * ) buf + n , sep , sizeof ( sep )); <nl> s = buf + onigenc_str_bytelen_null ( ONIG_ENCODING_ASCII , buf ); <nl>  <nl> p = pat ;
syck_hdlr_add_alias ( SyckParser * p , char * a ) <nl> { <nl> SyckNode * n ; <nl>  <nl> - if ( st_lookup ( p -> anchors , ( st_data_t ) a , & n ) ) <nl> + if ( st_lookup ( p -> anchors , ( st_data_t ) a , ( st_data_t *)& n ) ) <nl> { <nl> return n ; <nl> }
zstream_run_func ( void * ptr ) <nl> struct zstream * z = args -> z ; <nl> uInt n ; <nl>  <nl> + err = Z_OK ; <nl> while (! args -> interrupt ) { <nl> n = z -> stream . avail_out ; <nl> err = z -> func -> run (& z -> stream , flush );
unix_recv_io ( int argc , VALUE * argv , VALUE sock ) <nl> enum { <nl> GC_REASON_EMSGSIZE = 0x1 , <nl> GC_REASON_TRUNCATE = 0x2 , <nl> - GC_REASON_ENOMEM = 0x4 , <nl> + GC_REASON_ENOMEM = 0x4 <nl> }; <nl>  <nl> int fd ;
rb_str_cat_conv_enc_opts ( VALUE newstr , long ofs , const char * ptr , long len , <nl> long olen ; <nl>  <nl> olen = RSTRING_LEN ( newstr ); <nl> - if ( ofs < - olen || olen <= ofs ) <nl> + if ( ofs < - olen || olen < ofs ) <nl> rb_raise ( rb_eIndexError , " index % ld out of string ", ofs ); <nl> if ( ofs < 0 ) ofs += olen ; <nl> if (! from ) {
str_byte_substr ( VALUE str , long beg , long len , int empty ) <nl> beg += n ; <nl> if ( beg < 0 ) return Qnil ; <nl> } <nl> - if ( beg + len > n ) <nl> + if ( len > n - beg ) <nl> len = n - beg ; <nl> if ( len <= 0 ) { <nl> if (! empty ) return Qnil ;
condvar_ptr ( VALUE self ) <nl>  <nl> /* forked children can ' t reach into parent thread stacks */ <nl> if ( cv -> fork_gen != fork_gen ) { <nl> + cv -> fork_gen = fork_gen ; <nl> list_head_init (& cv -> waitq ); <nl> } <nl> 
st_insert2 ( st_table * tab , st_data_t key , st_data_t value , <nl> if ( tab -> bins == NULL ) { <nl> bin = find_entry ( tab , hash_value , key ); <nl> new_p = bin == UNDEFINED_ENTRY_IND ; <nl> + if ( new_p ) <nl> + tab -> num_entries ++; <nl> bin_ind = UNDEFINED_BIN_IND ; <nl> } <nl> else {
valid_hostname ( const char * hostname ) <nl> if ( hostname == NULL ) <nl> return NO ; <nl>  <nl> + if (! strcmp ( hostname , " localhost ")) <nl> + return YES ; <nl> + <nl> if ('.' == * p || ':' == * p || '/' == * p ) <nl> return NO ; <nl> 
load_a_module ( const char * path , int warn , int core ) <nl> } <nl> } <nl> } <nl> + <nl> + break ; <nl> default : <nl> ilog ( L_MAIN , " Module % s has unknown / unsupported MAPI version % d .", <nl> mod_basename , MAPI_VERSION (* mapi_version ));
stats_dnsbl ( struct Client * source_p ) <nl> rb_dictionary_iter iter ; <nl> struct BlacklistStats * stats ; <nl>  <nl> + if ( bl_stats == NULL ) <nl> + return ; <nl> + <nl> RB_DICTIONARY_FOREACH ( stats , & iter , bl_stats ) <nl> { <nl> /* use RPL_STATSDEBUG for now -- jilles */
blacklist_dns_callback ( const char * result , bool status , query_type type , void * d <nl> { <nl> /* Done here */ <nl> notice_client ( auth -> cid , "*** IP not found in DNS blacklist % s ", <nl> - rb_dlink_list_length (& blacklist_list ) > 1 : " s " : ""); <nl> + rb_dlink_list_length (& blacklist_list ) > 1 ? " s " : ""); <nl> rb_free ( bluser ); <nl> auth -> data [ PROVIDER_BLACKLIST ] = NULL ; <nl> provider_done ( auth , PROVIDER_BLACKLIST );
delete_opm_scanner ( const char * key __unused , int parc __unused , const char ** par <nl>  <nl> rb_dlinkDelete (& proxy -> node , & proxy_scanners ); <nl> rb_free ( proxy ); <nl> + <nl> + if (! rb_dlink_list_length ( proxy_scanners )) <nl> + opm_enable = false ; <nl> } <nl>  <nl> static void <nl> delete_opm_scanner_all ( const char * key __unused , int parc __unused , const char * <nl> { <nl> opm_cancel ( auth ); <nl> } <nl> + <nl> + opm_enable = false ; <nl> } <nl>  <nl> 
int CloseUnreal ( HWND hWnd ) <nl> return 0 ; <nl> else <nl> { <nl> - DestroyWindow ( hWnd ); <nl> - exit ( 0 ); <nl> + DestroyWindow ( hWnd ); <nl> + TerminateProcess ( GetCurrentProcess (), 0 ); <nl> + exit ( 0 ); /* in case previous fails ( possible ?) */ <nl> } <nl> } <nl> 
static INLINE OPJ_BOOL opj_dwt_encode_procedure ( opj_tcd_tilecomp_t * tilec , void <nl>  <nl> l_data_size = opj_dwt_max_resolution ( tilec -> resolutions , tilec -> numresolutions ) * ( OPJ_UINT32 ) sizeof ( OPJ_INT32 ); <nl> bj = ( OPJ_INT32 *) opj_malloc (( size_t ) l_data_size ); <nl> - if (! bj ) { <nl> + if ( l_data_size != 0 && ! bj ) { <nl> return OPJ_FALSE ; <nl> } <nl> i = l ;
opj_pi_iterator_t * opj_pi_create_decode ( opj_image_t * p_image , <nl> l_current_pi -> include = 00 ; <nl> if ( l_step_l <= ( SIZE_MAX / ( l_tcp -> numlayers + 1U ))) <nl> { <nl> - l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( l_tcp -> numlayers + 1 ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> + l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( size_t )( l_tcp -> numlayers + 1U ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> } <nl>  <nl> if
int ssl_get_ciphersuite_id ( const char * ciphersuite_name ) <nl>  <nl> const char * ssl_get_ciphersuite ( const ssl_context * ssl ) <nl> { <nl> + if ( ssl == NULL || ssl -> session == NULL ) <nl> + return NULL ; <nl> + <nl> return ssl_get_ciphersuite_name ( ssl -> session -> ciphersuite ); <nl> } <nl> 
GetOutboundPinholeTimeout ( struct upnphttp * h , const char * action , const char * <nl> rem_port = GetValueFromNameValueList (& data , " RemotePort "); <nl> protocol = GetValueFromNameValueList (& data , " Protocol "); <nl>  <nl> - if (! int_port || ! ext_port || ! protocol ) <nl> + if (! int_port || ! rem_port || ! protocol ) <nl> { <nl> ClearNameValueList (& data ); <nl> SoapError ( h , 402 , " Invalid Args ");
void pureftpd_register_simple_auth_callback ( int (* callback )( const char * account , <nl> static AuthResult embedded_simple_pw_check ( const char * account , const char * password ) <nl> { <nl> AuthResult authresult ; <nl> - <nl> + <nl> + memset (& authresult , 0 , sizeof authresult ); <nl> if ( simple_auth_callback == NULL || <nl> account == NULL || * account == 0 || password == NULL ) { <nl> authresult . auth_ok = 0 ;
int init_aliases ( void ) <nl> ( tail -> dir = strdup ( dir )) == NULL ) { <nl> die_mem (); <nl> } <nl> - tail -> next = NULL ; <nl> } else { <nl> DirAlias * curr ; <nl>  <nl> int init_aliases ( void ) <nl> tail -> next = curr ; <nl> tail = curr ; <nl> } <nl> + tail -> next = NULL ; <nl> } <nl> fclose ( fp ); <nl> aliases_up ++;
char ** argv ; <nl>  <nl> void readin () <nl> { <nl> - skelout (); <nl> - <nl> - if ( ddebug ) <nl> - puts ( "# define FLEX_DEBUG " ); <nl> - <nl> if ( csize == 256 ) <nl> puts ( " typedef unsigned char YY_CHAR ;" ); <nl> else <nl> puts ( " typedef char YY_CHAR ;" ); <nl>  <nl> + skelout (); <nl> + <nl> + if ( ddebug ) <nl> + puts ( "# define FLEX_DEBUG " ); <nl> + <nl> line_directive_out ( stdout ); <nl>  <nl> if ( yyparse () )
Dconv ( Fmt * fp ) <nl> break ; <nl>  <nl> case D_BRANCH : <nl> - snprint ( str , sizeof ( str ), "% d ", a -> u . branch -> loc ); <nl> + if ( a -> u . branch == nil ) <nl> + snprint ( str , sizeof ( str ), "< nil >"); <nl> + else <nl> + snprint ( str , sizeof ( str ), "% d ", a -> u . branch -> loc ); <nl> break ; <nl>  <nl> case D_EXTERN :
reswitch : <nl> goto error ; <nl> switch ( t -> etype ) { <nl> default : <nl> - yyerror (" invalid operation : % N ( index of type % T )", n , t ); <nl> + yyerror (" invalid operation : % N ( type % T does not support indexing )", n , t ); <nl> goto error ; <nl>  <nl> 
static void list_type ( FUNC_TYPE ft , int one ) <nl> { <nl> FUNCTION * fp ; <nl> int i = 0 ; <nl> - DISPLAY_COLUMNS dc ; <nl> + DISPLAY_COLUMNS dc = { 0 }; <nl>  <nl> if (! one ) <nl> calculate_columns (& dc );
int ssl3_get_cert_verify ( SSL * s ) <nl> if ( s -> s3 -> tmp . message_type != SSL3_MT_CERTIFICATE_VERIFY ) <nl> { <nl> s -> s3 -> tmp . reuse_message = 1 ; <nl> - if (( peer != NULL ) && ( type & EVP_PKT_SIGN )) <nl> + if ( peer != NULL ) <nl> { <nl> al = SSL_AD_UNEXPECTED_MESSAGE ; <nl> SSLerr ( SSL_F_SSL3_GET_CERT_VERIFY , SSL_R_MISSING_VERIFY_MESSAGE );
DECLARE_ASN1_FUNCTIONS ( X509_CINF ) <nl> DECLARE_ASN1_FUNCTIONS ( X509 ) <nl> DECLARE_ASN1_FUNCTIONS ( X509_CERT_AUX ) <nl>  <nl> -# define X509_new_index ( l , p , newf , dupf , freef ) \ <nl> +# define X509_get_ex_new_index ( l , p , newf , dupf , freef ) \ <nl> CRYPTO_get_ex_new_index ( CRYPTO_EX_INDEX_X509 , l , p , newf , dupf , freef ) <nl> int X509_set_ex_data ( X509 * r , int idx , void * arg ); <nl> void * X509_get_ex_data ( X509 * r , int idx );
int tls1_mac ( SSL * ssl , SSL3_RECORD * rec , unsigned char * md , int sending ) <nl> mac_ctx = hash ; <nl> } else { <nl> hmac = EVP_MD_CTX_new (); <nl> - if ( hmac == NULL || ! EVP_MD_CTX_copy ( hmac , hash )) <nl> + if ( hmac == NULL || ! EVP_MD_CTX_copy ( hmac , hash )) { <nl> + EVP_MD_CTX_free ( hmac ); <nl> return 0 ; <nl> + } <nl> mac_ctx = hmac ; <nl> } <nl> 
X509_VERIFY_PARAM * X509_VERIFY_PARAM_new ( void ) <nl>  <nl> void X509_VERIFY_PARAM_free ( X509_VERIFY_PARAM * param ) <nl> { <nl> + if ( param == NULL ) <nl> + return ; <nl> x509_verify_param_zero ( param ); <nl> OPENSSL_free ( param -> id ); <nl> OPENSSL_free ( param );
static int pkey_rsa_ctrl ( EVP_PKEY_CTX * ctx , int type , int p1 , void * p2 ) <nl> *( const EVP_MD **) p2 = rctx -> md ; <nl> } else { <nl> if ( rsa_pss_restricted ( rctx )) { <nl> - if ( EVP_MD_type ( rctx -> md ) == EVP_MD_type ( p2 )) <nl> + if ( EVP_MD_type ( rctx -> mgf1md ) == EVP_MD_type ( p2 )) <nl> return 1 ; <nl> RSAerr ( RSA_F_PKEY_RSA_CTRL , RSA_R_MGF1_DIGEST_NOT_ALLOWED ); <nl> return 0 ;
int ossl_policy_cache_set_mapping ( X509 * x , POLICY_MAPPINGS * maps ) <nl>  <nl> ret = 1 ; <nl> bad_mapping : <nl> - if ( ret == - 1 && CRYPTO_THREAD_write_lock ( x -> lock )) { <nl> - x -> ex_flags |= EXFLAG_INVALID_POLICY ; <nl> - CRYPTO_THREAD_unlock ( x -> lock ); <nl> - } <nl> sk_POLICY_MAPPING_pop_free ( maps , POLICY_MAPPING_free ); <nl> return ret ; <nl> 
int PKCS12_PBE_keyivgen ( EVP_CIPHER_CTX * ctx , const char * pass , int passlen , <nl> unsigned char * salt ; <nl> unsigned char key [ EVP_MAX_KEY_LENGTH ], iv [ EVP_MAX_IV_LENGTH ]; <nl>  <nl> + if ( cipher == NULL ) <nl> + return 0 ; <nl> + <nl> /* Extract useful info from parameter */ <nl>  <nl> pbe = ASN1_TYPE_unpack_sequence ( ASN1_ITEM_rptr ( PBEPARAM ), param );
int dsa_builtin_paramgen2 ( DSA * ret , size_t L , size_t N , <nl> } else { <nl> p = BN_CTX_get ( ctx ); <nl> q = BN_CTX_get ( ctx ); <nl> + if ( q == NULL ) <nl> + goto err ; <nl> } <nl>  <nl> if (! BN_lshift ( test , BN_value_one (), L - 1 ))
static int check_suiteb_cipher_list ( const SSL_METHOD * meth , CERT * c , <nl> * prule_str = " ECDHE - ECDSA - AES256 - GCM - SHA384 "; <nl> break ; <nl> } <nl> + /* Set auto ECDH parameter determination */ <nl> + c -> ecdh_tmp_auto = 1 ; <nl> return 1 ; <nl> } <nl> 
int RSA_padding_check_PKCS1_OAEP ( unsigned char * to , int tlen , <nl> } <nl>  <nl> lzero = num - flen ; <nl> + if ( lzero < 0 ) <nl> + { <nl> + RSAerr ( RSA_F_RSA_PADDING_CHECK_PKCS1_OAEP , RSA_R_OAEP_DECODING_ERROR ); <nl> + return (- 1 ); <nl> + } <nl> maskeddb = from - lzero + SHA_DIGEST_LENGTH ; <nl>  <nl> MGF1 ( seed , SHA_DIGEST_LENGTH , maskeddb , dblen );
int OCSP_parse_url ( char * url , char ** phost , char ** pport , char ** ppath , int * pss <nl>  <nl>  <nl> err : <nl> + if ( buf ) OPENSSL_free ( buf ); <nl> if (* ppath ) OPENSSL_free (* ppath ); <nl> if (* pport ) OPENSSL_free (* pport ); <nl> if (* phost ) OPENSSL_free (* phost );
static int init_ssl_connection ( SSL * con ) <nl> BIO_ADDR_free ( client ); <nl> dtlslisten = 0 ; <nl> i = SSL_accept ( con ); <nl> + } else { <nl> + BIO_ADDR_free ( client ); <nl> } <nl> } else <nl> # endif
int tls1_setup_key_block ( SSL * s ) <nl>  <nl> if (( p2 = ( unsigned char *) OPENSSL_malloc ( num )) == NULL ) { <nl> SSLerr ( SSL_F_TLS1_SETUP_KEY_BLOCK , ERR_R_MALLOC_FAILURE ); <nl> + OPENSSL_free ( p1 ); <nl> goto err ; <nl> } <nl> # ifdef TLS_DEBUG
int SSL_check_private_key ( SSL * ssl ) <nl> SSLerr ( SSL_F_SSL_CHECK_PRIVATE_KEY , ERR_R_PASSED_NULL_PARAMETER ); <nl> return ( 0 ); <nl> } <nl> + if ( ssl -> cert == NULL ) <nl> + return 0 ; <nl> if ( ssl -> cert -> key -> x509 == NULL ) <nl> { <nl> SSLerr ( SSL_F_SSL_CHECK_PRIVATE_KEY , SSL_R_NO_CERTIFICATE_ASSIGNED );
static struct file_st * win32_splitter ( DSO * dso , const char * filename , <nl> DSOerr ( DSO_F_WIN32_SPLITTER , <nl> DSO_R_INCORRECT_FILE_SYNTAX ); <nl> /* goto err ;*/ <nl> + OPENSSL_free ( result ); <nl> return ( NULL ); <nl> } <nl> result -> device = start ; <nl> static char * win32_merger ( DSO * dso , const char * filespec1 , const char * filespec2 <nl>  <nl> merged = win32_joiner ( dso , filespec1_split ); <nl> } <nl> + OPENSSL_free ( filespec1_split ); <nl> + OPENSSL_free ( filespec2_split ); <nl> return ( merged ); <nl> } <nl> 
static void newstats ( struct cgpu_info * cgpu ) <nl> void update_usb_stats ( __maybe_unused struct cgpu_info * cgpu ) <nl> { <nl> # if DO_USB_STATS <nl> + if ( cgpu -> usbstat < 1 ) <nl> + newstats ( cgpu ); <nl> + <nl> // we don ' t know the device_id until after add_cgpu () <nl> usb_stats [ cgpu -> usbstat - 1 ]. device_id = cgpu -> device_id ; <nl> # endif
static bool klondike_get_stats ( struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= slaves ; dev ++) { <nl> char * reply = SendCmdGetReply ( klncgpu , ' S ', dev , 0 , NULL ); <nl> if ( reply != NULL ) <nl> - klninfo -> status [ dev ] = *( WORKSTATUS *)( reply + 2 ); <nl> + memcpy (( void *)(&( klninfo -> status [ dev ])), reply + 2 , sizeof ( klninfo -> status [ dev ])); <nl> } <nl> wr_unlock (&( klninfo -> stat_lock )); <nl> 
static int file_to_data ( const char * path , char ** data , size_t * len ) <nl> path , strerror ( errno )); <nl> goto err ; <nl> } <nl> + if (! sb . st_size ) { <nl> + * len = 0 ; <nl> + return 0 ; <nl> + } <nl>  <nl> * data = mmap ( NULL , sb . st_size , PROT_READ , MAP_PRIVATE , fd , 0 ); <nl> if (* data == MAP_FAILED ) {
char * sepol_av_to_string ( policydb_t * policydbp , uint32_t tclass , <nl> int rc ; <nl> int avlen = 0 , len ; <nl>  <nl> + memset ( avbuf , 0 , sizeof avbuf ); <nl> cladatum = policydbp -> class_val_to_struct [ tclass - 1 ]; <nl> p = avbuf ; <nl> for ( i = 0 ; i < cladatum -> permissions . nprim ; i ++) {
static void cil_reset_class ( struct cil_class * class ) <nl>  <nl> static void cil_reset_perm ( struct cil_perm * perm ) <nl> { <nl> - cil_reset_classperms_list ( perm -> classperms ); <nl> + cil_list_destroy (& perm -> classperms , CIL_FALSE ); <nl> } <nl>  <nl> static inline void cil_reset_classperms ( struct cil_classperms * cp )
static void cil_reset_classpermission ( struct cil_classpermission * cp ) <nl> return ; <nl> } <nl>  <nl> - cil_reset_classperms_list ( cp -> classperms ); <nl> + cil_list_destroy (& cp -> classperms , CIL_FALSE ); <nl> } <nl>  <nl> static void cil_reset_classperms_set ( struct cil_classperms_set * cp_set )
static void cil_reset_classpermission ( struct cil_classpermission * cp ) <nl>  <nl> static void cil_reset_classperms_set ( struct cil_classperms_set * cp_set ) <nl> { <nl> - cil_reset_classpermission ( cp_set -> set ); <nl> + if ( cp_set == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> + cp_set -> set = NULL ; <nl> } <nl>  <nl> static inline void cil_reset_classperms_list ( struct cil_list * cp_list )
rsRetVal actionDestruct ( action_t * pThis ) <nl> pthread_mutex_destroy (& pThis -> mutActExec ); <nl> d_free ( pThis -> pszName ); <nl> d_free ( pThis -> ppTpl ); <nl> + d_free ( pThis -> peParamPassing ); <nl>  <nl> finalize_it : <nl> d_free ( pThis );
BEGINrunInput <nl> # endif <nl>  <nl> CODESTARTrunInput <nl> + CHKmalloc ( pReadfds ); <nl> if ( runModConf -> bOmitLocalLogging && nfd == 1 ) <nl> ABORT_FINALIZE ( RS_RET_OK ); <nl> /* this is an endless loop - it is terminated when the thread is
buildSeverityMapping ( instanceData * pData ) <nl> uchar pszSevCode [ 512 ]; <nl> int sevCode ; <nl> uchar * mapping ; <nl> - struct severMap_s * node ; <nl> + struct severMap_s * node = NULL ; <nl> DEFiRet ; <nl>  <nl> mapping = cs . pszSeverityMapping ; <nl> buildSeverityMapping ( instanceData * pData ) <nl> } <nl>  <nl> finalize_it : <nl> + if ( iRet != RS_RET_OK ) { <nl> + if ( node != NULL ) <nl> + free ( node ); <nl> + } <nl> RETiRet ; <nl> } <nl> 
tplProcessCnf ( struct cnfobj * o ) <nl> pTpl -> optFormatEscape = JSON_ESCAPE ; <nl>  <nl> finalize_it : <nl> + free ( tplStr ); <nl> if ( pvals != NULL ) <nl> cnfparamvalsDestruct ( pvals , & pblk ); <nl> if ( iRet != RS_RET_OK ) {
split_binary_parameters ( uchar ** const szBinary , char *** const __restrict__ aPara <nl> (* aParams )[ iPrm ] = NULL ; /* NULL per argv [] convention */ <nl>  <nl> finalize_it : <nl> + if ( estrBinary != param_binary ) { <nl> + es_deleteStr ( estrBinary ); <nl> + } <nl> + if ( estrParams != NULL ) { <nl> + es_deleteStr ( estrParams ); <nl> + } <nl> RETiRet ; <nl> }
CODESTARTfreeWrkrInstance <nl> pWrkrData -> curlHandle = NULL ; <nl> } <nl> free ( pWrkrData -> restURL ); <nl> + es_deleteStr ( pWrkrData -> batch . data ); <nl> ENDfreeWrkrInstance <nl>  <nl> BEGINdbgPrintInstInfo
gui_chat_draw ( struct t_gui_buffer * buffer , int clear_chat ) <nl>  <nl> if ( clear_chat ) <nl> { <nl> - snprintf ( format_empty , 32 , "%%-% ds ", ptr_win -> win_chat_width ); <nl> + snprintf ( format_empty , sizeof ( format_empty ), <nl> + "%%-% ds ", ptr_win -> win_chat_width ); <nl> for ( i = 0 ; i < ptr_win -> win_chat_height ; i ++) <nl> { <nl> mvwprintw ( GUI_WINDOW_OBJECTS ( ptr_win )-> win_chat , i , 0 ,
irc_ctcp_dcc_filename_without_quotes ( const char * filename ) <nl> int length ; <nl>  <nl> length = strlen ( filename ); <nl> - if ( length > 0 ) <nl> + if ( length > 1 ) <nl> { <nl> if (( filename [ 0 ] == '\"') && ( filename [ length - 1 ] == '\"')) <nl> return weechat_strndup ( filename + 1 , length - 2 );
relay_irc_recv ( struct t_relay_client * client , const char * data ) <nl> /* server capabilities */ <nl> if ( irc_command && ( weechat_strcasecmp ( irc_command , " cap ") == 0 )) <nl> { <nl> - if (( irc_argc > 0 ) && irc_argv ) <nl> + if ( irc_argc > 0 ) <nl> { <nl> relay_irc_recv_command_capab ( client , <nl> irc_argc , irc_argv , irc_argv_eol );
parse_codes ( struct archive_read * a ) <nl> new_size = DICTIONARY_MAX_SIZE ; <nl> else <nl> new_size = rar_fls (( unsigned int ) rar -> unp_size ) << 1 ; <nl> + if ( new_size == 0 ) { <nl> + archive_set_error (& a -> archive , ARCHIVE_ERRNO_FILE_FORMAT , <nl> + " Zero window size is invalid ."); <nl> + return ( ARCHIVE_FATAL ); <nl> + } <nl> new_window = realloc ( rar -> lzss . window , new_size ); <nl> if ( new_window == NULL ) { <nl> archive_set_error (& a -> archive , ENOMEM ,
parse_codes ( struct archive_read * a ) <nl> rar -> range_dec . Stream = & rar -> bytein ; <nl> __archive_ppmd7_functions . Ppmd7_Construct (& rar -> ppmd7_context ); <nl>  <nl> + if ( rar -> dictionary_size == 0 ) { <nl> + archive_set_error (& a -> archive , ARCHIVE_ERRNO_FILE_FORMAT , <nl> + " Invalid zero dictionary size "); <nl> + return ( ARCHIVE_FATAL ); <nl> + } <nl> + <nl> if (! __archive_ppmd7_functions . Ppmd7_Alloc (& rar -> ppmd7_context , <nl> rar -> dictionary_size , & g_szalloc )) <nl> {
_warc_read ( struct archive_read * a , const void ** buf , size_t * bsz , int64_t * off ) <nl> return ( ARCHIVE_EOF ); <nl> } <nl>  <nl> + if ( w -> unconsumed ) { <nl> + __archive_read_consume ( a , w -> unconsumed ); <nl> + w -> unconsumed = 0U ; <nl> + } <nl> + <nl> rab = __archive_read_ahead ( a , 1U , & nrd ); <nl> if ( nrd < 0 ) { <nl> * bsz = 0U ;
setup_current_filesystem ( struct archive_read_disk * a ) <nl> if (! GetVolumePathName ( tree_current_access_path ( t ), vol , sizeof ( vol ))) { <nl> t -> current_filesystem -> remote = - 1 ; <nl> archive_set_error (& a -> archive , ARCHIVE_ERRNO_MISC , <nl> - " GetVolumePathName failed : % d ", GetLastError ()); <nl> + " GetVolumePathName failed : % d ", ( int ) GetLastError ()); <nl> return ( ARCHIVE_FAILED ); <nl> } <nl> switch ( GetDriveType ( vol )) {
static void doXPathDump ( xmlXPathObjectPtr cur ) { <nl> # ifdef LIBXML_OUTPUT_ENABLED <nl> xmlSaveCtxtPtr ctxt ; <nl>  <nl> - if ( cur -> nodesetval -> nodeNr <= 0 ) { <nl> + if (( cur -> nodesetval == NULL ) || ( cur -> nodesetval -> nodeNr <= 0 )) { <nl> fprintf ( stderr , " XPath set is empty \ n "); <nl> progresult = XMLLINT_ERR_XPATH ; <nl> break ;
static EC_KEY * ec_key_new ( ErlNifEnv * env , ERL_NIF_TERM curve_arg ) <nl> } else <nl> goto out_err ; <nl>  <nl> + if (! group ) <nl> + goto out_err ; <nl> + <nl> if ( enif_inspect_binary ( env , prime [ 2 ], & seed )) { <nl> EC_GROUP_set_seed ( group , seed . data , seed . size ); <nl> }
# define HAVE_EC <nl> # endif <nl>  <nl> -// ( test for == 1 . 1 . 1pre8 ) <nl> -# if OPENSSL_VERSION_NUMBER == ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> +// ( test for >= 1 . 1 . 1pre8 ) <nl> +# if OPENSSL_VERSION_NUMBER >= ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> && ! defined ( HAS_LIBRESSL ) \ <nl> && defined ( HAVE_EC ) <nl> // EXPERIMENTAL :
gen_select_val ( LoaderState * stp , GenOpArg S , GenOpArg Fail , <nl> op -> a [ j + size ] = Fail ; <nl>  <nl> # ifdef DEBUG <nl> - for ( i = 0 ; i < size ; i ++) { <nl> + for ( i = 0 ; i < size - 1 ; i ++) { <nl> ASSERT ( op -> a [ i + 3 ]. val <= op -> a [ i + 4 ]. val ); <nl> } <nl> # endif
static size_t add_index_color ( char * buf , size_t buflen , format_flag flags , char <nl> if (!( flags & MUTT_FORMAT_INDEX )) <nl> return 0 ; <nl>  <nl> + /* this item is going to be passed to an external filter */ <nl> + if ( flags & MUTT_FORMAT_NOFILTER ) <nl> + return 0 ; <nl> + <nl> if ( color == MT_COLOR_INDEX ) <nl> { /* buf might be uninitialized other cases */ <nl> len = mutt_strlen ( buf );
bool set_default_value ( const char * name , intptr_t value ) <nl> return false ; <nl>  <nl> int idx = mutt_option_index ( name ); <nl> - if (! idx ) <nl> + if ( idx < 0 ) <nl> return false ; <nl>  <nl> MuttVars [ idx ]. initial = value ;
static int cmd_handle_untagged ( struct ImapData * idata ) <nl> mutt_debug ( 2 , " Handling untagged NO \ n "); <nl>  <nl> /* Display the warning message from the server */ <nl> - mutt_error ("% s ", s + 3 ); <nl> + mutt_error ("% s ", s + 2 ); <nl> } <nl>  <nl> return 0 ;
static int label_message ( HEADER * hdr , char * new ) <nl> mutt_free_list (& hdr -> env -> labels ); <nl> } <nl>  <nl> - if ( new == NULL ) <nl> - hdr -> env -> labels = NULL ; <nl> - else <nl> + if (( new != NULL ) && (* new != '\ 0 ')) <nl> { <nl> char * last , * label ; <nl> 
int mutt_index_menu ( void ) <nl> imap_allow_reopen ( Context ); <nl> # endif <nl>  <nl> - index_hint = ( Context -> vcount && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl> + index_hint = ( Context -> vcount && menu -> current >= 0 && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl>  <nl> if (( check = mx_check_mailbox ( Context , & index_hint , 0 )) < 0 ) <nl> {
void imap_quote_string ( char * dest , size_t dlen , const char * src , bool quote_back <nl> const char * s = src ; <nl>  <nl> * pt ++ = '"'; <nl> - /* save room for trailing quote - char */ <nl> - dlen -= 2 ; <nl> + /* save room for quote - chars */ <nl> + dlen -= 3 ; <nl>  <nl> for (; * s && dlen ; s ++) <nl> {
int imap_fetch_message ( MESSAGE * msg , CONTEXT * ctx , int msgno ) <nl> } <nl> } <nl>  <nl> - mutt_message _ (" Fetching message ..."); <nl> + if (! isendwin ()) <nl> + mutt_message _ (" Fetching message ..."); <nl>  <nl> cache -> uid = HEADER_DATA ( h )-> uid ; <nl> mutt_mktemp ( path );
static void * klondike_get_replies ( void * userdata ) <nl> case KLN_CMD_ABORT : <nl> // We can ' t do / check this until it ' s initialised <nl> if ( klninfo -> initialised ) { <nl> + isc = 0 ; <nl> dev = kitem -> kline . ws . dev ; <nl> wr_lock (&( klninfo -> stat_lock )); <nl> klninfo -> jobque [ dev ]. workqc = ( int )( kitem -> kline . ws . workqc );
serial_open ( const char * devpath , unsigned long baud , signed short timeout , bool p <nl> switch ( baud ) { <nl> case 0 : <nl> break ; <nl> + case 57600 : <nl> + cfsetispeed ( & my_termios , B57600 ); <nl> + cfsetospeed ( & my_termios , B57600 ); <nl> + break ; <nl> case 115200 : <nl> cfsetispeed ( & my_termios , B115200 ); <nl> cfsetospeed ( & my_termios , B115200 );
static void set_work_target ( struct work * work , int diff ) <nl> free ( htarget ); <nl> } <nl> } <nl> - memcpy ( work -> target , target , 256 ); <nl> + memcpy ( work -> target , target , 32 ); <nl> } <nl>  <nl> static void gen_stratum_work ( struct pool * pool , struct work * work )
static void mcast () <nl>  <nl> count ++; <nl> came_from_siz = sizeof ( came_from ); <nl> - if ( SOCKETFAIL ( rep = recvfrom ( mcast_sock , buf , sizeof ( buf ), <nl> + if ( SOCKETFAIL ( rep = recvfrom ( mcast_sock , buf , sizeof ( buf ) - 1 , <nl> 0 , ( struct sockaddr *)(& came_from ), & came_from_siz ))) { <nl> applog ( LOG_DEBUG , " API mcast failed count =% d (% s ) (% d )", <nl> count , SOCKERRMSG , ( int ) mcast_sock );
retry : <nl> struct stat statbuf ; <nl>  <nl> strcpy ( filename , str ); <nl> + free ( str ); <nl> if (! stat ( filename , & statbuf )) { <nl> wlogprint (" File exists , overwrite ?\ n "); <nl> input = getch (); <nl> retry : <nl> goto retry ; <nl> } <nl> } <nl> + else <nl> + free ( str ); <nl> fcfg = fopen ( filename , " w "); <nl> if (! fcfg ) { <nl> wlogprint (" Cannot open or create file \ n ");
bool initiate_stratum ( struct pool * pool ) <nl> if ( pool -> sessionid ) <nl> applog ( LOG_DEBUG , " Pool % d stratum session id : % s ", pool -> pool_no , pool -> sessionid ); <nl> else <nl> - applog ( LOG_DEBUG , " Pool % d stratum session id does not exist "); <nl> + applog ( LOG_DEBUG , " Pool % d stratum session id does not exist ", pool -> pool_no ); <nl>  <nl> ret = true ; <nl> out :
void _simplelog ( int prio , const char * str , bool force ) <nl> { <nl> # ifdef HAVE_SYSLOG_H <nl> if ( use_syslog ) { <nl> - syslog ( prio , "% s ", str ); <nl> + syslog ( LOG_LOCAL0 | prio , "% s ", str ); <nl> } <nl> # else <nl> if ( 0 ) {}
struct cg_usb_tmo { <nl> struct cg_usb_info { <nl> uint8_t bus_number ; <nl> uint8_t device_address ; <nl> - int which_intinfo ; <nl> int usbstat ; <nl> bool nodev ; <nl> int nodev_count ;
static bool klondike_get_stats ( struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= slaves ; dev ++) { <nl> char * reply = SendCmdGetReply ( klncgpu , ' S ', dev , 0 , NULL ); <nl> if ( reply != NULL ) <nl> - klninfo -> status [ dev ] = *( WORKSTATUS *)( reply + 2 ); <nl> + memcpy (( void *)(&( klninfo -> status [ dev ])), reply + 2 , sizeof ( klninfo -> status [ dev ])); <nl> } <nl> wr_unlock (&( klninfo -> stat_lock )); <nl> 
mm_answer_skeyrespond ( int sock , Buffer * m ) <nl> debug3 ("% s : sending authenticated : % d ", __func__ , authok ); <nl> mm_request_send ( sock , MONITOR_ANS_SKEYRESPOND , m ); <nl>  <nl> - auth_method = " skey "; <nl> + auth_method = " keyboard - interactive "; <nl> + auth_submethod = " skey "; <nl>  <nl> return ( authok != 0 ); <nl> }
bindresvport_sa ( int sd , struct sockaddr * sa ) <nl> if ( sa == NULL ) { <nl> memset (& myaddr , 0 , sizeof ( myaddr )); <nl> sa = ( struct sockaddr *)& myaddr ; <nl> + salen = sizeof ( myaddr ); <nl>  <nl> if ( getsockname ( sd , sa , & salen ) == - 1 ) <nl> return - 1 ; /* errno is correctly set */
static int tcmu_rbd_lock_break ( struct tcmu_device * dev , char ** orig_owner ) <nl> tcmu_dev_err ( dev , " Could not break lock from % s . ( Err % d )\ n ", <nl> owners [ 0 ], ret ); <nl> if ( ret == - ETIMEDOUT ) <nl> - return ret ; <nl> + goto free_owners ; <nl>  <nl> ret = - EAGAIN ; <nl> if (!* orig_owner ) {
struct tcmulib_context * tcmulib_initialize ( <nl> teardown_netlink ( ctx -> nl_sock ); <nl> darray_free ( ctx -> handlers ); <nl> darray_free ( ctx -> devices ); <nl> + genl_unregister_family (& tcmu_ops ); <nl> + free ( ctx ); <nl> return NULL ; <nl> } <nl> 
on_unregister_handler ( TCMUService1HandlerManager1 * interface , <nl> gpointer user_data ) <nl> { <nl> struct tcmur_handler * handler = find_handler_by_subtype ( subtype ); <nl> - struct dbus_info * info = handler -> opaque ; <nl> + struct dbus_info * info = handler ? handler -> opaque : NULL ; <nl>  <nl> if (! handler ) { <nl> g_dbus_method_invocation_return_value ( invocation ,
int main ( int argc , char ** argv ) <nl> darray_foreach ( tmp_r_handler , g_runner_handlers ) { <nl> struct tcmulib_handler tmp_handler ; <nl>  <nl> + memset (& tmp_handler , 0 , sizeof ( tmp_handler )); <nl> tmp_handler . name = (* tmp_r_handler )-> name ; <nl> tmp_handler . subtype = (* tmp_r_handler )-> subtype ; <nl> tmp_handler . cfg_desc = (* tmp_r_handler )-> cfg_desc ;
int main ( int argc , char ** argv ) <nl> { <nl> char * rootdir = get_rootdir ( pid ); <nl>  <nl> - dd_create_basic_files ( dd , fsuid , ( rootdir && strcmp ( rootdir , "/") != 0 ) ? rootdir : NULL ); <nl> + dd_create_basic_files ( dd , fsuid , NULL ); <nl>  <nl> char source_filename [ sizeof ("/ proc /% lu / somewhat_long_name ") + sizeof ( long )* 3 ]; <nl> int source_base_ofs = sprintf ( source_filename , "/ proc /% lu / smaps ", ( long ) pid );
static off_t copyfd_sparse ( int src_fd , int dst_fd1 , int dst_fd2 , off_t size2 ) <nl> size2 -= rd ; <nl> if ( size2 < 0 ) <nl> dst_fd2 = - 1 ; <nl> +// TODO : truncate to 0 or even delete the second file <nl> +//( currently we delete the file later ) <nl> } <nl> out : <nl> 
GtkWidget * create_main_window ( void ) <nl> gtk_container_add ( GTK_CONTAINER ( halign ), hbox_report_delete ); <nl>  <nl> GtkWidget * hbox_help_close = gtk_hbutton_box_new (); <nl> - GtkWidget * btn_online_help = gtk_button_new_with_mnemonic ( _ (" _Online Help ")); <nl> + GtkWidget * btn_online_help = gtk_button_new_with_mnemonic ( _ (" Online _Help ")); <nl> GtkWidget * btn_close = gtk_button_new_from_stock ( GTK_STOCK_CLOSE ); <nl> gtk_box_pack_end ( GTK_BOX ( hbox_help_close ), btn_online_help , false , false , 0 ); <nl> gtk_box_pack_end ( GTK_BOX ( hbox_help_close ), btn_close , false , false , 0 );
static void remote_recv_cb ( EV_P_ ev_io * w , int revents ) { <nl>  <nl> ssize_t r = recv ( remote -> fd , server -> buf , BUF_SIZE , 0 ); <nl>  <nl> - if ( verbose ) { <nl> - LOGD (" remote recv : % d byte ", ( int ) r ); <nl> - } <nl> - <nl> if ( r == 0 ) { <nl> // connection closed <nl> close_and_free_remote ( EV_A_ remote );
void Client :: setDisconnectedFromCore () <nl> _ignoreListManager -> deleteLater (); <nl> _ignoreListManager = 0 ; <nl> } <nl> + <nl> + if ( _transferManager ) { <nl> + _transferManager -> deleteLater (); <nl> + _transferManager = 0 ; <nl> + } <nl> + <nl> // we probably don ' t want to save pending input for reconnect <nl> _userInputBuffer . clear (); <nl> 
void MultiLineEdit :: keyPressEvent ( QKeyEvent * event ) { <nl> case Qt :: Key_Greater : <nl> moveCursor ( QTextCursor :: End ); <nl> return ; <nl> + <nl> + // modify <nl> + case Qt :: Key_D : <nl> + moveCursor ( QTextCursor :: WordRight , QTextCursor :: KeepAnchor ); <nl> + cut (); <nl> + return ; <nl> } <nl> } <nl> }
QString MultiLineEdit :: convertMircCodesToHtml ( const QString & text ) { <nl> words [ i ] = "< span style =\"" + style + "\">" + words [ i ] + "</ span >"; <nl> } <nl> } <nl> - return words . join (""); <nl> + return words . join (""). replace ("\ n ","< br />"); <nl> } <nl>  <nl> void MultiLineEdit :: on_returnPressed () {
# include " common / xX . h " <nl> # include " common / tcpdump . h " <nl> # include " common / timer . h " <nl> + <nl> + extern char SVN_Version []; <nl> + const char * svn_version ( void ); /* svn_version . c */ <nl> + <nl> # endif
exif_subchunk_parse ( SF_PRIVATE * psf , uint32_t length ) <nl> case olym_MARKER : <nl> bytesread += psf_binheader_readf ( psf , " 4 ", & dword ) ; <nl> psf_log_printf ( psf , "% M : % u \ n ", marker , dword ) ; <nl> + if ( bytesread + dword > length ) <nl> + break ; <nl> dword += ( dword & 1 ) ; <nl> bytesread += psf_binheader_readf ( psf , " j ", dword ) ; <nl> break ;
pcm_init ( SF_PRIVATE * psf ) <nl> { int chars = 0 ; <nl>  <nl> if ( psf -> bytewidth == 0 || psf -> sf . channels == 0 ) <nl> + { psf_log_printf ( psf , " pcm_init : internal error : bytewitdh = % d , channels = % d \ n ", psf -> bytewidth , psf -> sf . channels ) ; <nl> return SFE_INTERNAL ; <nl> + } ; <nl>  <nl> psf -> blockwidth = psf -> bytewidth * psf -> sf . channels ; <nl> 
wavlike_subchunk_parse ( SF_PRIVATE * psf , int chunk , uint32_t chunk_length ) <nl>  <nl> case exif_MARKER : <nl> psf_log_printf ( psf , " % M \ n ", chunk ) ; <nl> - bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> + if ( chunk_length > bytesread ) <nl> + bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> continue ; <nl>  <nl> case data_MARKER :
aiff_read_header ( SF_PRIVATE * psf , COMM_CHUNK * comm_fmt ) <nl> } ; <nl> } ; <nl>  <nl> + if ( psf -> sf . channels < 1 ) <nl> + return SFE_CHANNEL_COUNT_ZERO ; <nl> + <nl> + if ( psf -> sf . channels >= SF_MAX_CHANNELS ) <nl> + return SFE_CHANNEL_COUNT ; <nl> + <nl> if (! ( found_chunk & HAVE_FORM )) <nl> return SFE_AIFF_NO_FORM ; <nl> 
OPJ_BOOL opj_jp2_read_boxhdr_char ( opj_jp2_box_t * box , <nl> opj_event_msg ( p_manager , EVT_ERROR , " Cannot handle box of undefined sizes \ n "); <nl> return OPJ_FALSE ; <nl> } <nl> - <nl> + if ( box -> length < * p_number_bytes_read ) { <nl> + opj_event_msg ( p_manager , EVT_ERROR , " Box length is inconsistent .\ n "); <nl> + return OPJ_FALSE ; <nl> + } <nl> return OPJ_TRUE ; <nl> } <nl> 
OPJ_BOOL opj_j2k_update_image_data ( opj_tcd_t * p_tcd , OPJ_BYTE * p_data , opj_im <nl> if ( ( l_offset_x0_src < 0 ) || ( l_offset_y0_src < 0 ) || ( l_offset_x1_src < 0 ) || ( l_offset_y1_src < 0 ) ){ <nl> return OPJ_FALSE ; <nl> } <nl> + /* testcase 2977 . pdf . asan . 67 . 2198 */ <nl> + if (( OPJ_INT32 ) l_width_dest < 0 || ( OPJ_INT32 ) l_height_dest < 0 ) { <nl> + return OPJ_FALSE ; <nl> + } <nl> /*-----*/ <nl>  <nl> /* Compute the input buffer offset */
int main ( int argc , char ** argv ) <nl>  <nl> io_sources . Register ( thread_mgr , true ); <nl>  <nl> - if ( io_sources . Size () > 0 || have_pending_timers ) <nl> + if ( io_sources . Size () > 0 || have_pending_timers || BifConst :: exit_only_after_terminate ) <nl> { <nl> if ( profiling_logger ) <nl> profiling_logger -> Log ();
void h2o_reprocess_request ( h2o_req_t * req , h2o_iovec_t method , const h2o_url_sch <nl> req -> path_normalized = h2o_url_normalize_path (& req -> pool , req -> path . base , req -> path . len , & req -> query_at , & req -> norm_indexes ); <nl> req -> overrides = overrides ; <nl> req -> res_is_delegated |= is_delegated ; <nl> + req -> reprocess_if_too_early = 0 ; <nl> reset_response ( req ); <nl>  <nl> /* check the delegation ( or reprocess ) counter */
xmlrpc_value * rhbz_get_member ( const char * member , xmlrpc_value * xml ) <nl> xmlrpc_struct_find_value (& env , xml , member , & value ); <nl> if ( env . fault_occurred ) <nl> abrt_xmlrpc_error (& env ); <nl> + if (! value ) <nl> + error_msg_and_die (" fatal : There is no member named '% s '", member ); <nl>  <nl> return value ; <nl> }
char * strtrimch ( char * str , int ch ) <nl> while (* tmp == ch ) <nl> ++ tmp ; <nl>  <nl> - memmove ( str , tmp , strlen ( str )); <nl> + memmove ( str , tmp , strlen ( tmp )); <nl>  <nl> // Remove trailing spaces . <nl> int i = strlen ( str );
static problem_data_t * load_problem_data_if_not_yet ( problem_data_t * problem_data <nl> struct dump_dir * dd = dd_opendir ( dump_dir_name , /* flags :*/ 0 ); <nl> if (! dd ) <nl> { <nl> - problem_data_free ( problem_data ); <nl> return NULL ; <nl> } <nl> problem_data = create_problem_data_from_dump_dir ( dd );
static plist_t parse_bin_node ( struct bplist_data * bplist , const char ** object ) <nl> return parse_string_node ( object , size ); <nl>  <nl> case BPLIST_UNICODE : <nl> + if ( size * 2 < size ) { <nl> + PLIST_BIN_ERR ("% s : Integer overflow when calculating BPLIST_UNICODE data size .\ n ", __func__ ); <nl> + return NULL ; <nl> + } <nl> if (* object + size * 2 > bplist -> offset_table ) { <nl> PLIST_BIN_ERR ("% s : BPLIST_UNICODE data bytes point outside of valid range \ n ", __func__ ); <nl> return NULL ;
static void node_from_xml ( parse_ctx ctx , plist_t * plist ) <nl> return ; <nl> } <nl> if (*( ctx -> pos - 1 ) == '/') { <nl> - tag [ ctx -> pos - p - 1 ] = '\ 0 '; <nl> + int idx = ctx -> pos - p - 1 ; <nl> + if ( idx < taglen ) <nl> + tag [ idx ] = '\ 0 '; <nl> is_empty = 1 ; <nl> } <nl> ctx -> pos ++;
usershape_t * gvusershape_find ( char * name ) <nl> { <nl> usershape_t probe ; <nl>  <nl> + if (! ImageDict ) <nl> + return NULL ; <nl> + <nl> probe . name = name ; <nl> return ( dtsearch ( ImageDict , & probe )); <nl> }
static void mp_resolve_color ( GVJ_t * job , gvcolor_t * color ) <nl> color -> u . rgba [ 2 ]); <nl> color -> u . index = i ; <nl> break ; <nl> + case HSVA_DOUBLE : /* TODO : implement color conversion */ <nl> + color -> u . index = 0 ; <nl> + break ; <nl> default : <nl> assert ( 0 ); /* internal error */ <nl> }
static int sig_autoremove ( void ) <nl>  <nl> /* Close only logs with private messages */ <nl> logitem = log -> items -> data ; <nl> + if ( logitem -> servertag == NULL ) <nl> + continue ; <nl> + <nl> server = server_find_tag ( logitem -> servertag ); <nl> if ( logitem -> type == LOG_ITEM_TARGET && <nl> server != NULL && ! server -> ischannel (* logitem -> name ))
verify_callback ( int preverify_ok , X509_STORE_CTX * ctx ) <nl> if ( opt -> verify_export_cert ) <nl> { <nl> gc = gc_new (); <nl> - if ( tmp_file = get_peer_cert ( ctx , opt -> verify_export_cert ,& gc )) <nl> + if (( tmp_file = get_peer_cert ( ctx , opt -> verify_export_cert ,& gc ))) <nl> { <nl> setenv_str ( opt -> es , " peer_cert ", tmp_file ); <nl> }
* development environment . <nl> */ <nl>  <nl> +/* MSVC headers do not define this macro , so do it here */ <nl> +# ifndef IN6_ARE_ADDR_EQUAL <nl> +# define IN6_ARE_ADDR_EQUAL ( a , b ) \ <nl> + ( memcmp (( const void *)( a ), ( const void *)( b ), sizeof ( struct in6_addr )) == 0 ) <nl> +# endif <nl> + <nl> void init_win32 ( void ); <nl> void uninit_win32 ( void ); <nl> 
ecma_create_arguments_object ( ecma_object_t * func_obj_p , /**< callee function */ <nl> prop_desc , <nl> false ); <nl> JERRY_ASSERT ( ecma_is_completion_value_normal_true ( completion )); <nl> + ecma_deref_ecma_string ( length_magic_string_p ); <nl>  <nl> ecma_dealloc_number ( len_p ); <nl> 
run_int_from_pos ( struct __int_data * int_data ) <nl> { <nl> const OPCODE * curr = & __program [ int_data -> pos ]; <nl> completion = __opfuncs [ curr -> op_idx ](* curr , int_data ); <nl> + <nl> + JERRY_ASSERT ( ! ecma_is_completion_value_normal ( completion ) <nl> + || ecma_is_completion_value_normal_simple_value ( completion , <nl> + ECMA_SIMPLE_VALUE_EMPTY ) ); <nl> } while ( completion . type == ECMA_COMPLETION_TYPE_NORMAL ); <nl>  <nl> if ( completion . type == ECMA_COMPLETION_TYPE_BREAK )
ieee802_11_print ( netdissect_options * ndo , <nl> hdrlen = roundup2 ( hdrlen , 4 ); <nl> if ( ndo -> ndo_Hflag && FC_TYPE ( fc ) == T_DATA && <nl> DATA_FRAME_IS_QOS ( FC_SUBTYPE ( fc ))) { <nl> + if (! ND_TTEST_1 ( p + hdrlen )) { <nl> + nd_print_trunc ( ndo ); <nl> + return hdrlen ; <nl> + } <nl> meshdrlen = extract_mesh_header_length ( p + hdrlen ); <nl> hdrlen += meshdrlen ; <nl> } else
icmp6_nodeinfo_print ( netdissect_options * ndo , u_int icmp6len , const u_char * bp , <nl> } else <nl> dnsname_print ( ndo , cp , ep ); <nl> if (( EXTRACT_16BITS (& ni6 -> ni_flags ) & 0x01 ) != 0 ) <nl> - ND_PRINT (( ndo ," [ TTL =% u ]", *( uint32_t *)( ni6 + 1 ))); <nl> + ND_PRINT (( ndo ," [ TTL =% u ]", EXTRACT_32BITS ( ni6 + 1 ))); <nl> break ; <nl> case NI_QTYPE_NODEADDR : <nl> if ( needcomma )
handle_pap ( netdissect_options * ndo , <nl>  <nl> switch ( code ) { <nl> case PAP_AREQ : <nl> + /* A valid Authenticate - Request is 6 or more octets long . */ <nl> + if ( len < 6 ) <nl> + goto trunc ; <nl> if ( length - ( p - p0 ) < 1 ) <nl> return ; <nl> ND_TCHECK (* p );
public : <nl> // base methods <nl> virtual ~ AP4_AtomParent (); <nl> AP4_List < AP4_Atom >& GetChildren () { return m_Children ; } <nl> + AP4_Result CopyChildren ( AP4_AtomParent & destination ) const ; <nl> virtual AP4_Result AddChild ( AP4_Atom * child , int position = - 1 ); <nl> virtual AP4_Result RemoveChild ( AP4_Atom * child ); <nl> virtual AP4_Result DeleteChild ( AP4_Atom :: Type type , AP4_Ordinal index = 0 );
static void start_auth_request ( PgSocket * client , const char * username ) <nl> int res ; <nl> PktBuf * buf ; <nl>  <nl> - client -> auth_user = client -> db -> auth_user ; <nl> /* have to fetch user info from db */ <nl> client -> pool = get_pool ( client -> db , client -> db -> auth_user ); <nl> if (! find_server ( client )) {
static bool handle_client_startup ( PgSocket * client , PktHdr * pkt ) <nl> } <nl> break ; <nl> case ' p ': /* PasswordMessage */ <nl> + /* too early */ <nl> + if (! client -> auth_user ) { <nl> + disconnect_client ( client , true , " client password pkt before startup packet "); <nl> + return false ; <nl> + } <nl> + <nl> /* haven ' t requested it */ <nl> if ( cf_auth_type <= AUTH_TRUST ) { <nl> disconnect_client ( client , true , " unrequested passwd pkt ");
static bool check_client_passwd ( PgSocket * client , const char * passwd ) <nl> const char * correct ; <nl> PgUser * user = client -> auth_user ; <nl>  <nl> + /* auth_user may be missing */ <nl> + if (! user ) { <nl> + slog_error ( client , " Password packet before auth packet ?"); <nl> + return false ; <nl> + } <nl> + <nl> /* disallow empty passwords */ <nl> if (!* passwd || !* user -> passwd ) <nl> return false ;
close_uzbl ( WebKitWebView * page , GArray * argv , GString * result ) { <nl> if ( uzbl . gui . main_window ) <nl> gtk_widget_destroy ( uzbl . gui . main_window ); <nl> else if ( uzbl . gui . plug ) <nl> - gtk_widget_destroy ( uzbl . gui . plug ); <nl> + gtk_widget_destroy ( GTK_WIDGET ( uzbl . gui . plug )); <nl>  <nl> gtk_main_quit (); <nl> }
find_existing_file ( gchar * path_list ) { <nl> i ++; <nl> } <nl>  <nl> + g_free ( executable ); <nl> g_strfreev ( split ); <nl> return NULL ; <nl> }
key_to_event ( guint keyval , gint mode ) { <nl> utf_conv = g_convert ( byte , - 1 , " UTF - 8 ", " ISO - 8859 - 1 ", NULL , NULL , NULL ); <nl>  <nl> send_event ( mode == GDK_KEY_PRESS ? KEY_PRESS : KEY_RELEASE , utf_conv ? utf_conv : byte , NULL ); <nl> + g_free ( utf_conv ); <nl> } <nl> else <nl> send_event ( mode == GDK_KEY_PRESS ? KEY_PRESS : KEY_RELEASE ,
request_starting_cb ( WebKitWebView * view , WebKitWebFrame * frame , WebKitWebResour <nl>  <nl> g_object_ref ( decision -> request ); <nl> request_decision ( uri , decision ); <nl> + <nl> + g_free ( decision ); <nl> } <nl>  <nl> void
static int write_element_to_buffer ( bson_buffer * buffer , int type_byte , PyObject * <nl>  <nl> item_value = PySequence_GetItem ( value , i ); <nl> if (! write_element_to_buffer ( buffer , list_type_byte , item_value , check_keys )) { <nl> + Py_DECREF ( item_value ); <nl> return 0 ; <nl> } <nl> + Py_DECREF ( item_value ); <nl> } <nl>  <nl> /* write null byte and fill in length */
static int send_ra ( int sock , struct Interface * iface , struct in6_addr const * des <nl> flog ( LOG_WARNING , " sendmsg : % s ", strerror ( errno )); <nl> else <nl> dlog ( LOG_DEBUG , 3 , " sendmsg : % s ", strerror ( errno )); <nl> + return - 1 ; <nl> } <nl>  <nl> return 0 ;
set_interface_var ( const char * iface , <nl> if ( snprintf ( spath , sizeof ( spath ), var , iface ) >= sizeof ( spath )) <nl> return - 1 ; <nl>  <nl> + /* No path traversal */ <nl> + if ( strstr ( name , "..") || strchr ( name , '/')) <nl> + return - 1 ; <nl> + <nl> if ( access ( spath , F_OK ) != 0 ) <nl> return - 1 ; <nl> 
public : <nl> y += secVnc . height (); <nl> secPlain . move ( xPad , y ); <nl> y += secPlain . height (); <nl> + <nl> + xPad -= SECOND_COL_XPAD ; <nl> # endif <nl>  <nl> /* Render " OK " and " Cancel " buttons */
static void handle_propfind ( struct connection * conn , const char * path , <nl> struct dir_entry * de = & arr [ i ]; <nl> mg_url_encode ( de -> file_name , strlen ( de -> file_name ), buf , sizeof ( buf )); <nl> print_props ( conn , buf , & de -> st ); <nl> + free ( de -> file_name ); <nl> } <nl> + free ( arr ); <nl> } <nl> ns_send ( conn -> ns_conn , footer , sizeof ( footer ) - 1 ); <nl> }
enum { <nl>  <nl> // Macros for enabling compiler - specific checks for printf - like arguments . <nl> # undef PRINTF_FORMAT_STRING <nl> -# if _MSC_VER >= 1400 <nl> +# if defined ( _MSC_VER ) && _MSC_VER >= 1400 <nl> # include < sal . h > <nl> -# if _MSC_VER > 1400 <nl> +# if defined ( _MSC_VER ) && _MSC_VER > 1400 <nl> # define PRINTF_FORMAT_STRING ( s ) _Printf_format_string_ s <nl> # else <nl> # define PRINTF_FORMAT_STRING ( s ) __format_string s
inf_text_fixline_buffer_buffer_insert_text ( InfTextBuffer * buffer , <nl>  <nl> priv -> keep = g_realloc ( <nl> priv -> keep , <nl> - priv -> n_keep + inf_text_chunk_get_length ( chunk ) <nl> + ( priv -> n_keep + inf_text_chunk_get_length ( chunk )) * sizeof ( guint ) <nl> ); <nl>  <nl> if ( pos - buf_len < priv -> n_keep )
static st_ret_t _st_db_put ( st_driver_t drv , const char * type , const char * owner , <nl> DB_TXN * t ; <nl> st_ret_t ret ; <nl>  <nl> + if ( dbd == NULL ) { <nl> + return st_FAILED ; <nl> + } <nl> + <nl> if ( os_count ( os ) == 0 ) <nl> return st_SUCCESS ; <nl> 
getfont ( PyObject * self_ , PyObject * args , PyObject * kw ) <nl> } <nl>  <nl> if ( filename && font_bytes_size <= 0 ) { <nl> + self -> font_bytes = NULL ; <nl> error = FT_New_Face ( library , filename , index , & self -> face ); <nl> } else { <nl> /* need to have allocated storage for font_bytes for the life of the object .*/
files_out : <nl> afc_file_lock ( afc , lockfile , AFC_LOCK_UN ); <nl> afc_file_close ( afc , lockfile ); <nl> lockfile = 0 ; <nl> - do_post_notification ( NP_SYNC_DID_FINISH ); <nl> + if ( cmd == CMD_BACKUP ) <nl> + do_post_notification ( NP_SYNC_DID_FINISH ); <nl> } <nl> } else { <nl> printf (" ERROR : Could not start service % s .\ n ", MOBILEBACKUP2_SERVICE_NAME );
idevice_error_t idevice_connection_enable_ssl ( idevice_connection_t connection ) <nl> return_me = SSL_do_handshake ( ssl ); <nl> if ( return_me != 1 ) { <nl> debug_info (" ERROR in SSL_do_handshake : % s ", errorstring ( SSL_get_error ( ssl , return_me ))); <nl> + SSL_free ( ssl ); <nl> SSL_CTX_free ( ssl_ctx ); <nl> } else { <nl> ssl_data_t ssl_data_loc = ( ssl_data_t ) malloc ( sizeof ( struct ssl_data_private ));
char * format_string ( const char * buf , int cols , int depth ) <nl> int i = 0 ; <nl> int j = 0 ; <nl>  <nl> - assert ( cols >= 0 ); <nl> + assert ( cols > 0 ); <nl> assert ( depth >= 0 ); <nl>  <nl> // Inserts new lines and tabs at appropriate locations
mobilesync_error_t mobilesync_ready_to_send_changes_from_computer ( mobilesync_cli <nl> err = MOBILESYNC_E_SUCCESS ; <nl>  <nl> out : <nl> + if ( response_type ) { <nl> + free ( response_type ); <nl> + response_type = NULL ; <nl> + } <nl> if ( msg ) { <nl> plist_free ( msg ); <nl> msg = NULL ;
static int jpc_dec_tileinit ( jpc_dec_t * dec , jpc_dec_tile_t * tile ) <nl> uint_fast32_t tmpxend ; <nl> uint_fast32_t tmpyend ; <nl> jpc_dec_cp_t * cp ; <nl> - jpc_tsfb_band_t bnds [ 64 ]; <nl> + jpc_tsfb_band_t bnds [ JPC_MAXBANDS ]; <nl> jpc_pchg_t * pchg ; <nl> int pchgno ; <nl> jpc_dec_cmpt_t * cmpt ;
int oidc_handle_redirect_uri_request ( request_rec * r , oidc_cfg * c , <nl> /* something went wrong */ <nl> return oidc_util_html_send_error ( r , c -> error_template , " Invalid Request ", <nl> apr_psprintf ( r -> pool , <nl> - " The OpenID Connect callback URL received an invalid request : % s ", <nl> - r -> args ), HTTP_INTERNAL_SERVER_ERROR ); <nl> + " The OpenID Connect callback URL received an invalid request "), <nl> + HTTP_INTERNAL_SERVER_ERROR ); <nl> } <nl>  <nl> /*
namespace SDDM { <nl> if ( index != - 1 ) <nl> env . insert ( s . left ( index ), s . mid ( index + 1 )); <nl> } <nl> +# else <nl> + // we strdup ' d the string before in this branch <nl> + free ( mapped ); <nl> # endif <nl> env . insert (" HOME ", pw -> pw_dir ); <nl> env . insert (" PWD ", pw -> pw_dir );
namespace SDDM { <nl> } <nl>  <nl> bool PamBackend :: closeSession () { <nl> - if ( m_pam -> isOpen ()) <nl> + if ( m_pam -> isOpen ()) { <nl> + qDebug () << "[ PAM ] Closing session "; <nl> return m_pam -> closeSession (); <nl> + } <nl> + qWarning () << "[ PAM ] Asked to close the session but it wasn ' t previously open "; <nl> return Backend :: closeSession (); <nl> } <nl> 
array_index ( PyArrayObject * v ) <nl> " one element can be converted to an index "); <nl> return NULL ; <nl> } <nl> + if ( PyArray_NDIM ( v ) != 0 ) { <nl> + if ( DEPRECATE (" converting an array with ndim > 0 to an index " <nl> + " will result in an error in the future ") < 0 ) { <nl> + return NULL ; <nl> + } <nl> + } <nl> return PyArray_DESCR ( v )-> f -> getitem ( PyArray_DATA ( v ), v ); <nl> } <nl> # endif
arr_insert ( PyObject * NPY_UNUSED ( self ), PyObject * args , PyObject * kwdict ) <nl> } else { <nl> Py_XDECREF ( values ); <nl> Py_XDECREF ( mask ); <nl> + Py_XDECREF ( array ); <nl> Py_RETURN_NONE ; <nl> } <nl> }
prepare_index ( PyArrayObject * self , PyObject * index , <nl> if ( indices [ i ]. value != PyArray_DIM ( self , used_ndim )) { <nl> static PyObject * warning ; <nl>  <nl> - char * err_msg [ 174 ]; <nl> - sprintf ( err_msg , <nl> + char err_msg [ 174 ]; <nl> + PyOS_snprintf ( err_msg , sizeof ( err_msg ), <nl> " boolean index did not match indexed array along " <nl> " dimension % d ; dimension is %" NPY_INTP_FMT <nl> " but corresponding boolean dimension is %" NPY_INTP_FMT ,
const TSS2_TCTI_INFO * tpm2_tcti_ldr_getinfo ( void ) { <nl> bool tpm2_tcti_ldr_is_tcti_present ( const char * name ) { <nl>  <nl> char path [ PATH_MAX ]; <nl> - snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so ", name ); <nl> + snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so . 0 ", name ); <nl>  <nl> void * handle = dlopen ( path , RTLD_LAZY ); <nl> if ( handle ) {
bool X86_getInstruction ( csh ud , const uint8_t * code , size_t code_len , <nl> insn . prefixPresent [ 0x64 ] = 0 ; <nl> insn . prefixPresent [ 0x65 ] = 0 ; <nl> insn . prefixPresent [ 0x66 ] = 0 ; <nl> + insn . prefixPresent [ 0x67 ] = 0 ; <nl> insn . prefixPresent [ 0xf0 ] = 0 ; <nl> insn . prefixPresent [ 0xf2 ] = 0 ; <nl> insn . prefixPresent [ 0xf3 ] = 0 ;
bool M68K_getInstruction ( csh ud , const uint8_t * code , size_t code_len , MCInst * i <nl> cs_struct * handle = instr -> csh ; <nl> m68k_info * info ; <nl>  <nl> - if ( inst_info == NULL ) { <nl> + if ( handle -> printer_info == NULL ) { <nl> info = cs_mem_malloc ( sizeof ( m68k_info )); <nl> if (! info ) { <nl> handle -> errnum = CS_ERR_MEM ;
static void printPCRelOperand ( MCInst * MI , int OpNum , SStream * O ) <nl> SStream_concat ( O , "% u ", imm ); <nl> } else { <nl> if ( imm < - HEX_THRESHOLD ) <nl> - SStream_concat ( O , "- 0x % x ", - imm ); <nl> + SStream_concat ( O , "- 0x % x ", ( unsigned int )- imm ); <nl> else <nl> SStream_concat ( O , "-% u ", - imm ); <nl> }
static DecodeStatus DecodeINSVE_DF_4 ( MCInst * MI , uint32_t insn , <nl> } // else llvm_unreachable (" Invalid encoding "); <nl>  <nl> // assert ( NSize != 0 && RegDecoder != nullptr ); <nl> + if ( NSize == 0 || RegDecoder == NULL ) <nl> + return MCDisassembler_Fail ; <nl>  <nl> if ( RegDecoder == NULL ) <nl> return MCDisassembler_Fail ;
opfunc_in ( opcode_t opdata __unused , /**< operation data */ <nl> const idx_t left_var_idx = opdata . data . in . var_left ; <nl> const idx_t right_var_idx = opdata . data . in . var_right ; <nl>  <nl> + int_data -> pos ++; <nl> + <nl> ecma_completion_value_t ret_value ; <nl>  <nl> ECMA_TRY_CATCH ( left_value , get_variable_value ( int_data , left_var_idx , false ), ret_value );
static inline void drop_trailing_newlines ( char * s ) <nl> static void dorealloc ( char ** mem , size_t oldlen , size_t newlen ) <nl> { <nl> int batches ; <nl> - if ( newlen % BATCH_SIZE <= oldlen % BATCH_SIZE ) <nl> + if ( newlen <= oldlen ) <nl> return ; <nl> - batches = ( newlen % BATCH_SIZE ) + 1 ; <nl> + batches = ( newlen / BATCH_SIZE ) + 1 ; <nl> if (!* mem ) { <nl> do { <nl> * mem = malloc ( batches * BATCH_SIZE );
HwProbe :: hd2value ( hd_t * hd ) <nl> out -> add ( YCPString (" sub_vendor "), YCPString ( s )); <nl> } <nl>  <nl> + // HAL udi <nl> + <nl> + s = hd -> udi ; <nl> + if ( s ) <nl> + { <nl> + out -> add ( YCPString (" udi "), YCPString ( s )); <nl> + } <nl> + <nl> // unique key <nl>  <nl> s = hd -> unique_id ;
YEBuiltin :: toStream ( std :: ostream & str ) const <nl> std :: ostream & <nl> YEBuiltin :: toXml ( std :: ostream & str , int indent ) const <nl> { <nl> - str << "< builtin name =\"" << m_decl -> name << "\""; <nl> + str << "< builtin name =\"" << StaticDeclaration :: Decl2String ( m_decl ) << "\""; <nl>  <nl> if ( m_parameterblock != 0 ) <nl> {
static int net_get_nopacket ( struct priv_net * pn , void * arg , int * len ) <nl> while ( 1 ) { <nl> l = sizeof ( buf ); <nl> c = net_get ( pn -> pn_s , buf , & l ); <nl> + if ( c < 0 ) <nl> + return c ; <nl>  <nl> if ( c != NET_PACKET && c > 0 ) <nl> break ;
static int resolve_deps ( const char * src ) <nl> if ( strstr ( buf , " not regular file ")) <nl> break ; <nl>  <nl> + if ( strstr ( buf , " cannot read header ")) <nl> + break ; <nl> + <nl> + if ( strstr ( buf , destrootdir )) <nl> + break ; <nl> + <nl> p = strstr ( buf , "/"); <nl> if ( p ) { <nl> int r ;
extern " C " int ceph_create ( struct ceph_mount_info ** cmount , const char * const i <nl> extern " C " void ceph_shutdown ( struct ceph_mount_info * cmount ) <nl> { <nl> cmount -> shutdown (); <nl> + delete cmount ; <nl> } <nl>  <nl> extern " C " int ceph_conf_read_file ( struct ceph_mount_info * cmount , const char * path )
bool Monitor :: ms_get_authorizer ( int dest_type , AuthAuthorizer ** authorizer , bool <nl> } <nl>  <nl> CephXTicketBlob blob ; <nl> - ret = cephx_build_service_ticket_blob ( info , blob ); <nl> - if ( ret < 0 ) <nl> + if (! cephx_build_service_ticket_blob ( info , blob )) <nl> return false ; <nl> bufferlist ticket_data ; <nl> :: encode ( blob , ticket_data );
public : <nl> set_tid ( rtid ); <nl> } <nl> MOSDSubOp () {} <nl> + private : <nl> + ~ MOSDSubOp () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " osd_sub_op "; } <nl> void print ( ostream & out ) { <nl> out << " osd_sub_op (" << reqid
private : <nl> item_dirty_dirfrag_nest ( this ), <nl> item_dirty_dirfrag_dirfragtree ( this ), <nl> auth_pins ( 0 ), nested_auth_pins ( 0 ), <nl> + auth_pin_freeze_allowance ( 0 ), <nl> nested_anchors ( 0 ), <nl> pop ( ceph_clock_now ( g_ceph_context )), <nl> versionlock ( this , & versionlock_type ),
void Elector :: start () <nl> return ; <nl> } <nl> dout ( 5 ) << " start -- can i be leader ?" << dendl ; <nl> + <nl> + acked_me . clear (); <nl>  <nl> // start by trying to elect me <nl> if ( epoch % 2 == 0 )
public : <nl> uint64_t get_required_features () const { <nl> return required_features ; <nl> } <nl> + mon_feature_t get_required_mon_features () const { <nl> + return monmap -> get_required_features (); <nl> + } <nl> void apply_quorum_to_compatset_features (); <nl> void apply_compatset_features_to_quorum_requirements (); <nl> 
unsigned ceph_str_hash_rjenkins ( const char * str , unsigned length ) <nl> unsigned ceph_str_hash_linux ( const char * str , unsigned length ) <nl> { <nl> unsigned long hash = 0 ; <nl> - unsigned char c ; <nl>  <nl> while ( length --) { <nl> - c = * str ++; <nl> + unsigned char c = * str ++; <nl> hash = ( hash + ( c << 4 ) + ( c >> 4 )) * 11 ; <nl> } <nl> return hash ;
int validate_pool ( IoCtx & io_ctx , CephContext * cct ) { <nl> snap_name ), <nl> boost :: bind (& ImageWatcher :: notify_snap_remove , <nl> ictx -> image_watcher , snap_name )); <nl> - if ( r < 0 && r != - EEXIST ) { <nl> + if ( r < 0 && r != - ENOENT ) { <nl> return r ; <nl> } <nl> } else {
void librados :: IoCtxImpl :: C_NotifyComplete :: notify ( uint8_t opcode , <nl> uint64_t ver , <nl> bufferlist & bl ) <nl> { <nl> + lock -> Lock (); <nl> * done = true ; <nl> cond -> Signal (); <nl> + lock -> Unlock (); <nl> } <nl>  <nl> /////////////////////////// WatchContext ///////////////////////////////
void PG :: activate ( ObjectStore :: Transaction & t , <nl>  <nl> need_up_thru = false ; <nl>  <nl> + // clear prior set ( and dependency info )... we are done peering ! <nl> + clear_prior (); <nl> + <nl> // write pg info , log <nl> write_info ( t ); <nl> write_log ( t );
ssize_t AsyncConnection :: _try_send ( bool send , bool more ) <nl> // trim already sent for outcoming_bl <nl> if ( sent_bytes ) { <nl> if ( sent_bytes < outcoming_bl . length ()) { <nl> - bufferlist bl ; <nl> - outcoming_bl . splice ( sent_bytes , outcoming_bl . length ()- sent_bytes , & bl ); <nl> - bl . swap ( outcoming_bl ); <nl> + outcoming_bl . splice ( 0 , sent_bytes ); <nl> } else { <nl> outcoming_bl . clear (); <nl> }
class MonMap { <nl> void calc_ranks () { <nl> rank_name . resize ( mon_addr . size ()); <nl> rank_addr . resize ( mon_addr . size ()); <nl> + addr_name . clear (); <nl> for ( map < string , entity_addr_t >:: iterator p = mon_addr . begin (); <nl> p != mon_addr . end (); <nl> p ++) {
int FileStore :: mount () <nl> } <nl>  <nl> dout ( 5 ) << " mount op_seq is " << initial_op_seq << dendl ; <nl> + if ( initial_op_seq == 0 ) { <nl> + derr << " mount initial op seq is 0 ; something is wrong " << dendl ; <nl> + ret = - EINVAL ; <nl> + goto close_current_fd ; <nl> + } <nl>  <nl> if (! btrfs_stable_commits ) { <nl> // mark current / as non - snapshotted so that we don ' t rollback away
void JournalingObjectStore :: _op_journal_transactions ( list < ObjectStore :: Transacti <nl> data_len = t -> get_data_length (); <nl> data_align = ( t -> get_data_alignment () - tbl . length ()) & ~ CEPH_PAGE_MASK ; <nl> } <nl> - t -> encode ( tbl ); <nl> + :: encode (* t , tbl ); <nl> } <nl> journal -> submit_entry ( op , tbl , data_align , onjournal ); <nl> } else if ( onjournal )
static int read_all_chunked_input ( req_state * s , char ** pdata , int * plen ) <nl> int read_len = 0 , len = 0 ; <nl> do { <nl> int r = s -> cio -> read ( data + len , need_to_read , & read_len ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( data ); <nl> return r ; <nl> + } <nl>  <nl> len += read_len ; <nl> 
CInode * Server :: rdlock_path_pin_ref ( MDRequest * mdr , bool want_auth ) <nl> // open ref inode <nl> CInode * ref = 0 ; <nl> if ( trace . empty ()) <nl> - ref = mdcache -> get_root (); <nl> + ref = mdcache -> get_inode ( refpath . get_ino ()); <nl> else { <nl> CDentry * dn = trace [ trace . size ()- 1 ]; <nl> 
int FileJournal :: prepare_single_write ( bufferlist & bl , off64_t & queue_pos , uint64 <nl>  <nl> // add it this entry <nl> entry_header_t h ; <nl> + memset (& h , 0 , sizeof ( h )); <nl> h . seq = seq ; <nl> h . pre_pad = pre_pad ; <nl> h . len = ebl . length ();
int lockdep_will_lock ( const char * name , int id ) <nl> * _dout << "\ npreviously locked at \ n "; <nl> p -> second -> print (* _dout ); <nl> } <nl> + delete bt ; <nl> * _dout << dendl ; <nl> assert ( 0 ); <nl> }
void Server :: early_reply ( MDRequest * mdr , CInode * tracei , CDentry * tracedn ) <nl> mds -> logger -> inc ( l_mds_reply ); <nl> double lat = g_clock . now () - mdr -> client_request -> get_recv_stamp (); <nl> mds -> logger -> favg ( l_mds_replyl , lat ); <nl> - dout ( 0 ) << " lat " << lat << dendl ; <nl> + dout ( 20 ) << " lat " << lat << dendl ; <nl> } <nl>  <nl> /*
reprotect_and_return_err : <nl> req_comp ); <nl> } else { <nl> if ( ictx -> cct -> _conf -> rbd_skip_partial_discard ) { <nl> + delete req_comp ; <nl> continue ; <nl> } <nl> req = new AioZero ( ictx , p -> oid . name , p -> objectno , p -> offset , p -> length ,
PG :: RecoveryState :: Peering :: react ( const AdvMap & advmap ) { <nl> dout ( 10 ) << " Peering advmap " << dendl ; <nl> if ( pg -> prior_set_affected (* prior_set . get (), & advmap . osdmap )) { <nl> dout ( 1 ) << " Peering , priors_set_affected , going to Reset " << dendl ; <nl> + pg -> state_clear ( PG_STATE_PEERING ); <nl> post_event ( advmap ); <nl> return transit < Reset >(); <nl> }
namespace ceph { <nl> assert ( s == SECSuccess ); <nl> } <nl> void Update ( const byte * input , size_t length ) { <nl> - SECStatus s ; <nl> - s = PK11_DigestOp ( ctx , input , length ); <nl> - assert ( s == SECSuccess ); <nl> + if ( length ) { <nl> + SECStatus s ; <nl> + s = PK11_DigestOp ( ctx , input , length ); <nl> + assert ( s == SECSuccess ); <nl> + } <nl> } <nl> void Final ( byte * digest ) { <nl> SECStatus s ;
void PrimaryLogPG :: on_shutdown () <nl> cancel_log_updates (); <nl> // we must remove PGRefs , so do this this prior to release_backoffs () callers <nl> clear_backoffs (); <nl> + // clean up snap trim references <nl> + snap_trimmer_machine . process_event ( Reset ()); <nl>  <nl> pgbackend -> on_change (); <nl> 
void MgrMonitor :: drop_active () <nl> pending_map . active_gid = 0 ; <nl> pending_map . available = false ; <nl> pending_map . active_addr = entity_addr_t (); <nl> + <nl> + // So that when new active mgr subscribes to mgrdigest , it will <nl> + // get an immediate response instead of waiting for next timer <nl> + cancel_timer (); <nl> } <nl>  <nl> void MgrMonitor :: drop_standby ( uint64_t gid )
int RGWPostObj_ObjStore :: read_form_part_header ( struct post_form_part * const part <nl> } <nl>  <nl> r = read_line ( bl , chunk_size , reached_boundary , done ); <nl> + if ( r < 0 ) { <nl> + return r ; <nl> + } <nl> } <nl>  <nl> return 0 ;
int namespace_add ( cls_method_context_t hctx , bufferlist * in , bufferlist * out ) <nl> } <nl>  <nl> /** <nl> - * Add a namespace to the namespace directory . <nl> + * Remove a namespace from the namespace directory . <nl> * <nl> * Input : <nl> * @ param name the name of the namespace
int buffer :: list :: read_file ( const char * fn ) <nl> :: fstat ( fd , & st ); <nl> int s = ROUND_UP_TO ( st . st_size , PAGE_SIZE ); <nl> bufferptr bp = buffer :: create_page_aligned ( s ); <nl> + bp . set_length ( st . st_size ); <nl> append ( bp ); <nl> :: read ( fd , ( void *) c_str (), length ()); <nl> :: close ( fd );
int Journal < I >:: remove ( librados :: IoCtx & io_ctx , const std :: string & image_id ) { <nl> return r ; <nl> } <nl>  <nl> - r = journaler . remove ( false ); <nl> + r = journaler . remove ( true ); <nl> if ( r < 0 ) { <nl> lderr ( cct ) << " failed to remove journal : " << cpp_strerror ( r ) << dendl ; <nl> return r ;
string render_log_object_name ( const string & format , <nl> break ; <nl>  <nl> case ' i ': <nl> - sprintf ( buf , "% lld ", bucket_id ); <nl> + sprintf ( buf , "% lld ", ( long long ) bucket_id ); <nl> break ; <nl> case ' n ': <nl> o += bucket_name ;
extern " C " int rados_conf_parse_argv_remainder ( rados_t cluster , int argc , <nl> conf -> apply_changes ( NULL ); <nl> assert ( args . size () <= ( unsigned int ) argc ); <nl> unsigned int i ; <nl> - for ( i = 0 ; i < argc ; ++ i ) { <nl> + for ( i = 0 ; i < ( unsigned int ) argc ; ++ i ) { <nl> if ( i < args . size ()) <nl> remargv [ i ] = args [ i ]; <nl> else
void PG :: do_peer ( ObjectStore :: Transaction & t , list < Context *>& tfin , <nl> osd -> queue_generate_backlog ( this ); <nl> return ; <nl> } <nl> - for ( unsigned i = 0 ; i < acting . size (); i ++) { <nl> + for ( unsigned i = 1 ; i < acting . size (); i ++) { <nl> int o = acting [ i ]; <nl> Info & pi = peer_info [ o ]; <nl> if ( pi . last_complete < pi . log_tail && ! pi . log_backlog &&
bool MDS :: _dispatch ( Message * m ) <nl> } <nl>  <nl> switch ( m -> get_type ()) { <nl> + <nl> + case CEPH_MSG_MON_MAP : <nl> + delete m ; <nl> + break ; <nl> + <nl> // MDS <nl> case CEPH_MSG_MDS_MAP : <nl> handle_mds_map (( MMDSMap *) m );
void RGWCopyObj_ObjStore_S3 :: send_partial_response ( off_t ofs ) <nl> dump_errno ( s ); <nl>  <nl> end_header ( s , this , " application / xml "); <nl> + dump_start ( s ); <nl> if ( op_ret == 0 ) { <nl> s -> formatter -> open_object_section_in_ns (" CopyObjectResult ", XMLNS_AWS_S3 ); <nl> }
int EventCenter :: process_events ( int timeout_microseconds ) <nl> external_lock . Unlock (); <nl> while (! cur_process . empty ()) { <nl> EventCallbackRef e = cur_process . front (); <nl> - cur_process . pop_front (); <nl> if ( e ) <nl> e -> do_request ( 0 ); <nl> + cur_process . pop_front (); <nl> } <nl> } <nl> return numevents ;
static struct errno_http hterrs [] = { <nl> { EEXIST , " 409 ", " BucketAlreadyExists " }, <nl> { ENOTEMPTY , " 409 ", " BucketNotEmpty " }, <nl> { ERANGE , " 416 ", " InvalidRange " }, <nl> - { 0 , NULL }}; <nl> + { 0 , NULL , NULL }}; <nl>  <nl> void dump_errno ( struct req_state * s , int err , struct rgw_err * rgwerr ) <nl> {
PG :: RecoveryState :: Stray :: Stray ( my_context ctx ) <nl> assert (! pg -> is_active ()); <nl> assert (! pg -> is_peering ()); <nl> assert (! pg -> is_primary ()); <nl> - pg -> state_set ( PG_STATE_PEERING ); <nl> } <nl>  <nl> boost :: statechart :: result
public : <nl> RBD (); <nl> ~ RBD (); <nl>  <nl> + // This must be dynamically allocated with new , and <nl> + // must be released with release (). <nl> + // Do not use delete . <nl> struct AioCompletion { <nl> void * pc ; <nl> AioCompletion ( void * cb_arg , callback_t complete_cb );
bool CInode :: encode_inodestat ( bufferlist & bl , Session * session , <nl> int allowed = get_caps_allowed_for_client ( client ); <nl> int issue = ( cap -> wanted () | likes ) & allowed ; <nl> cap -> issue_norevoke ( issue ); <nl> + issue = cap -> pending (); <nl> cap -> set_last_issue_stamp ( g_clock . recent_now ()); <nl> cap -> touch (); // move to back of session cap LRU <nl> e . cap . caps = issue ;
protected : <nl> public : <nl> Message () { <nl> memset (& header , 0 , sizeof ( header )); <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> }; <nl> Message ( int t ) { <nl> memset (& header , 0 , sizeof ( header )); <nl> header . type = t ; <nl> header . priority = 0 ; // undef <nl> header . data_off = 0 ; <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> } <nl> virtual ~ Message () { } <nl> 
void RGWAbortMultipart :: execute () <nl> // and also remove the metadata obj <nl> op_ret = del_op . delete_obj (); <nl> if ( op_ret == - ENOENT ) { <nl> - op_ret = - ERR_NO_SUCH_BUCKET ; <nl> + op_ret = - ERR_NO_SUCH_UPLOAD ; <nl> } <nl> } <nl> 
void ReplicatedPG :: handle_watch_timeout ( WatchRef watch ) <nl>  <nl> // obc ref swallowed by repop ! <nl> simple_repop_submit ( repop ); <nl> + <nl> + // apply new object state . <nl> + ctx -> obc -> obs = ctx -> new_obs ; <nl> } <nl>  <nl> ObjectContextRef ReplicatedPG :: create_object_context ( const object_info_t & oi ,
int RGWRados :: flush_read_list ( struct get_obj_data * d ) <nl> bufferlist & bl = * iter ; <nl> r = d -> client_cb -> handle_data ( bl , 0 , bl . length ()); <nl> if ( r < 0 ) { <nl> - dout ( 0 ) << " ERROR : flush_read_list (): d -> client_c -> handle_data () returned " << r << dendl ; <nl> + dout ( 0 ) << " ERROR : flush_read_list (): d -> client_cb -> handle_data () returned " << r << dendl ; <nl> break ; <nl> } <nl> }
public : <nl>  <nl> MLogAck () : Message ( MSG_LOGACK ) {} <nl> MLogAck ( ceph_fsid_t & f , version_t l ) : Message ( MSG_LOGACK ), fsid ( f ), last ( l ) {} <nl> + private : <nl> + ~ MLogAck () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " log_ack "; } <nl> void print ( ostream & out ) { <nl> out << " log ( last " << last << ")";
void OSD :: start_recovery_op ( PG * pg , const hobject_t & soid ) <nl>  <nl> void OSD :: finish_recovery_op ( PG * pg , const hobject_t & soid , bool dequeue ) <nl> { <nl> + recovery_wq . lock (); <nl> dout ( 10 ) << " finish_recovery_op " << * pg << " " << soid <nl> << " dequeue =" << dequeue <nl> << " (" << recovery_ops_active << "/" << g_conf -> osd_recovery_max_active << " rops )" <nl> << dendl ; <nl> - recovery_wq . lock (); <nl>  <nl> // adjust count <nl> recovery_ops_active --;
void ObjectCopyRequest < I >:: send_update_object_map () { <nl> m_local_image_ctx -> snap_lock . put_read (); <nl> finish ( 0 ); <nl> return ; <nl> + } else if ( m_local_image_ctx -> object_map == nullptr ) { <nl> + // possible that exclusive lock was lost in background <nl> + derr << ": object map is not initialized " << dendl ; <nl> + <nl> + m_local_image_ctx -> snap_lock . put_read (); <nl> + finish (- EINVAL ); <nl> + return ; <nl> } <nl>  <nl> assert ( m_local_image_ctx -> object_map != nullptr );
void ReplicatedPG :: sub_op_modify_ondisk ( MOSDSubOp * op , int ackerosd , eversion_t <nl> commit -> set_pg_complete_thru ( last_complete ); <nl> commit -> set_peer_stat ( osd -> get_my_stat_for ( g_clock . now (), ackerosd )); <nl> osd -> messenger -> send_message ( commit , osd -> osdmap -> get_inst ( ackerosd )); <nl> - delete op ; <nl> } <nl> + <nl> + delete op ; <nl> } <nl>  <nl> void ReplicatedPG :: sub_op_modify_reply ( MOSDSubOpReply * r )
void take_min_markers ( IterIn first , IterIn last , IterOut dest ) <nl> } <nl> } <nl>  <nl> +} // anonymous namespace <nl> + <nl> class DataLogTrimCR : public RGWCoroutine { <nl> RGWRados * store ; <nl> RGWHTTPManager * http ; <nl> int DataLogTrimPollCR :: operate () <nl> return 0 ; <nl> } <nl>  <nl> -} // anonymous namespace <nl> - <nl> RGWCoroutine * create_data_log_trim_cr ( RGWRados * store , <nl> RGWHTTPManager * http , <nl> int num_shards , utime_t interval )
OutputDataSocket :: OutputDataSocket ( CephContext * cct , uint64_t _backlog ) <nl> m_shutdown_rd_fd (- 1 ), <nl> m_shutdown_wr_fd (- 1 ), <nl> going_down ( false ), <nl> + data_size ( 0 ), <nl> m_lock (" OutputDataSocket :: m_lock ") <nl> { <nl> }
struct C_MDC_TruncateFinish : public Context { <nl>  <nl> void MDCache :: _truncate_inode ( CInode * in , LogSegment * ls ) <nl> { <nl> - inode_t * pi = in -> get_projected_inode (); <nl> + inode_t * pi = & in -> inode ; <nl> dout ( 10 ) << " _truncate_inode " <nl> << pi -> truncate_from << " -> " << pi -> truncate_size <nl> << " on " << * in << dendl ;
class MHeartbeat : public Message { <nl> this -> load = load ; <nl> this -> beat = beat ; <nl> } <nl> + private : <nl> + ~ MHeartbeat () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " HB "; } <nl>  <nl> void encode_payload () {
void ceph_handle_caps ( struct ceph_mds_client * mdsc , <nl> up_write (& mdsc -> snap_rwsem ); <nl> check_caps = 1 ; /* we may have sent a RELEASE to the old auth */ <nl> goto done ; <nl> - <nl> } <nl>  <nl> /* preallocate space for xattrs ? */ <nl> bad : <nl> return ; <nl>  <nl> release : <nl> + up_write (& mdsc -> snap_rwsem ); <nl> send_cap_msg ( mdsc , vino . ino , CEPH_CAP_OP_RELEASE , <nl> 0 , 0 , 0 , <nl> seq , 0 ,
public : <nl> monc_lock (" MonClient :: monc_lock "), <nl> timer ( monc_lock ), <nl> hunting ( false ), <nl> + want_monmap ( false ), <nl> + want_keys ( 0 ), <nl> mounting ( 0 ), mount_err ( 0 ), <nl> auth ( NULL ) { } <nl> ~ MonClient () {
static int do_bench_write ( librbd :: Image & image , uint64_t io_size , <nl>  <nl> printf (" SEC OPS OPS / SEC BYTES / SEC \ n "); <nl> uint64_t off ; <nl> - for ( off = 0 ; off < io_bytes ; off += io_size ) { <nl> + for ( off = 0 ; off < io_bytes ; ) { <nl> b . wait_for ( io_threads - 1 ); <nl> i = 0 ; <nl> while ( i < io_threads && off < io_bytes &&
OSD :: OSD ( int id , Messenger * m , MonMap * mm , const char * dev ) : <nl>  <nl> state = STATE_BOOTING ; <nl>  <nl> + memset (& my_stat , 0 , sizeof ( my_stat )); <nl> + <nl> stat_ops = 0 ; <nl> stat_qlen = 0 ; <nl> stat_rd_ops = stat_rd_ops_shed_in = stat_rd_ops_shed_out = 0 ;
OPTION ( osd_mon_shutdown_timeout , OPT_DOUBLE , 5 ) <nl> OPTION ( osd_max_object_size , OPT_U64 , 100 * 1024L * 1024L * 1024L ) // OSD ' s maximum object size <nl> OPTION ( osd_max_attr_size , OPT_U64 , 0 ) <nl>  <nl> - OPTION ( filestore , OPT_BOOL , false ) <nl> - <nl> /// filestore wb throttle limits <nl> OPTION ( filestore_wbthrottle_enable , OPT_BOOL , true ) <nl> OPTION ( filestore_wbthrottle_btrfs_bytes_start_flusher , OPT_U64 , 41943040 )
void CDir :: _omap_fetched ( bufferlist & hdrbl , map < string , bufferlist >& omap , <nl>  <nl> void CDir :: go_bad () <nl> { <nl> + if ( get_version () == 0 ) <nl> + set_version ( 1 ); <nl> state_set ( STATE_BADFRAG ); <nl> // mark complete , ! fetching <nl> mark_complete (); <nl> state_clear ( STATE_FETCHING ); <nl> auth_unpin ( this ); <nl> - <nl> + <nl> // kick waiters <nl> finish_waiting ( WAIT_COMPLETE , 0 ); <nl> }
class MMDSMap : public Message { <nl> epoch = mm -> get_epoch (); <nl> mm -> encode ( encoded ); <nl> } <nl> + private : <nl> + ~ MMDSMap () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " mdsmap "; } <nl> void print ( ostream & out ) { <nl> out << " mdsmap ( e " << epoch << ")";
int execute ( const po :: variables_map & vm ) { <nl> return r ; <nl> } <nl>  <nl> + io_ctx . set_osdmap_full_try (); <nl> + <nl> librbd :: RBD rbd ; <nl> r = do_delete ( rbd , io_ctx , image_name . c_str (), <nl> vm [ at :: NO_PROGRESS ]. as < bool >());
class FileLock : public SimpleLock { <nl> } <nl> bool can_rdlock_soon () { <nl> if ( parent -> is_auth ()) <nl> - return ( state == LOCK_GLOCKL ); <nl> + return <nl> + ( state == LOCK_GLOCKL ) || <nl> + ( state == LOCK_LOCK && xlock_by ); <nl> else <nl> return false ; <nl> }
void Mgr :: init () <nl> cluster_state . notify_osdmap ( osd_map ); <nl> }); <nl>  <nl> + // Subscribe to OSDMap update to pass on to ClusterState <nl> + objecter -> maybe_request_map (); <nl> + <nl> monc -> sub_want (" mgrdigest ", 0 , 0 ); <nl>  <nl> // Prepare to receive FSMap and request it <nl> bool Mgr :: ms_dispatch ( Message * m ) <nl> m -> put (); <nl> break ; <nl> case CEPH_MSG_OSD_MAP : <nl> - <nl> handle_osd_map (); <nl>  <nl> py_modules . notify_all (" osd_map ", "");
int FileStore :: _detect_fs () <nl> dout ( 0 ) << " mount FIEMAP ioctl is disabled via ' filestore fiemap ' config option " << dendl ; <nl> ioctl_fiemap = false ; <nl> } <nl> + free ( fiemap ); <nl>  <nl> struct statfs st ; <nl> r = :: fstatfs ( fd , & st );
int RGWRados :: bi_list ( rgw_bucket & bucket , const string & obj_name , const string & <nl> } <nl>  <nl> ret = cls_rgw_bi_list ( bs . index_ctx , bs . bucket_obj , obj_name , marker , max , entries , is_truncated ); <nl> + if ( ret == - ENOENT ) { <nl> + * is_truncated = false ; <nl> + } <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
int CrushWrapper :: device_class_clone ( <nl> // pick a new shadow bucket id that is not used by the current map <nl> // * or * any previous shadow buckets . <nl> bno = - 1 ; <nl> - while ( crush -> buckets [- 1 - bno ] || <nl> + while (((- 1 - bno ) < crush -> max_buckets && crush -> buckets [- 1 - bno ]) || <nl> used_ids . count ( bno )) { <nl> -- bno ; <nl> }
void FileStore :: _set_replay_guard ( int fd , const SequencerPosition & spos ) <nl> // first make sure the previous operation commits <nl> :: fsync ( fd ); <nl>  <nl> + // sync object_map too . even if this object has a header or keys , <nl> + // it have had them in the past and then removed them , so always <nl> + // sync . <nl> + object_map -> sync (); <nl> + <nl> // then record that we did it <nl> bufferlist v ( 40 ); <nl> :: encode ( spos , v );
void Rank :: Pipe :: fault ( bool onconnect ) <nl>  <nl> if (! onconnect ) dout ( 2 ) << " fault " << errno << ": " << strerror ( errno ) << dendl ; <nl>  <nl> - if ( state == STATE_CLOSED ) { <nl> - dout ( 10 ) << " fault already closed " << dendl ; <nl> + if ( state == STATE_CLOSED || <nl> + state == STATE_CLOSING ) { <nl> + dout ( 10 ) << " fault already closed | closing " << dendl ; <nl> return ; <nl> } <nl> 
void Server :: handle_client_rename ( MDRequestRef & mdr ) <nl> & remote_wrlocks , auth_pin_freeze )) <nl> return ; <nl>  <nl> + if (! check_access ( mdr , srcdn -> get_dir ()-> get_inode (), MAY_WRITE )) <nl> + return ; <nl> + <nl> + if (! check_access ( mdr , destdn -> get_dir ()-> get_inode (), MAY_WRITE )) <nl> + return ; <nl> + <nl> + if (! check_access ( mdr , srci , MAY_WRITE )) <nl> + return ; <nl> + <nl> if ( oldin && <nl> oldin -> is_dir () && <nl> _dir_is_nonempty ( mdr , oldin )) {
static inline std :: string pg_state_string ( int state ) <nl> if ( state & PG_STATE_SCANNING ) <nl> oss << " scanning +"; <nl> string ret ( oss . str ()); <nl> - return ( ret . length () == 0 ) ? " inactive " : ret ; <nl> + if ( ret . length () > 0 ) <nl> + ret . resize ( ret . length () - 1 ); <nl> + else <nl> + ret = " inactive "; <nl> + return ret ; <nl> } <nl>  <nl> 
struct ObjectOperation { <nl> string mname = " filter "; <nl> :: encode ( cname , osd_op . indata ); <nl> :: encode ( mname , osd_op . indata ); <nl> - :: encode ( cookie , osd_op . indata ); <nl> osd_op . indata . append ( filter ); <nl> + :: encode ( cookie , osd_op . indata ); <nl> } <nl> void add_alloc_hint ( int op , uint64_t expected_object_size , <nl> uint64_t expected_write_size ) {
public : <nl> case CEPH_LOCK_IXATTR : return 8 + 6 * SimpleLock :: WAIT_BITS ; <nl> case CEPH_LOCK_ISNAP : return 8 + 7 * SimpleLock :: WAIT_BITS ; <nl> case CEPH_LOCK_INEST : return 8 + 8 * SimpleLock :: WAIT_BITS ; <nl> + case CEPH_LOCK_IFLOCK : return 8 + 9 * SimpleLock :: WAIT_BITS ; <nl> default : <nl> assert ( 0 ); <nl> }
void send_mds_reconnect ( struct ceph_mds_client * mdsc , int mds ) <nl> session = __get_session ( mdsc , mds ); <nl> if ( session ) { <nl> session -> s_state = CEPH_MDS_SESSION_RECONNECTING ; <nl> + session -> s_cap_seq = 0 ; <nl>  <nl> /* estimate needed space */ <nl> len += session -> s_nr_caps *
void OSD :: build_initial_pg_history ( <nl> h -> last_epoch_split = e ; <nl> } <nl> lastmap = osdmap ; <nl> + up_primary = new_up_primary ; <nl> + acting_primary = new_acting_primary ; <nl> + up = new_up ; <nl> + acting = new_acting ; <nl> } <nl> dout ( 20 ) << __func__ << " " << debug . str () << dendl ; <nl> dout ( 10 ) << __func__ << " " << * h << " " << * pi
public : <nl> pg_temp . reset ( new map < pg_t , vector < int32_t > >(* o . pg_temp )); <nl> osd_uuid . reset ( new vector < uuid_d >(* o . osd_uuid )); <nl>  <nl> + if ( o . osd_primary_affinity ) <nl> + osd_primary_affinity . reset ( new vector < __u32 >(* o . osd_primary_affinity )); <nl> + <nl> // NOTE : this still references shared entity_addr_t ' s . <nl> osd_addrs . reset ( new addrs_s (* o . osd_addrs )); <nl> 
void OSD :: handle_osd_map ( MOSDMap * m ) <nl> << " but failed to encode full with correct crc ; requesting " <nl> << dendl ; <nl> clog -> warn () << " failed to encode map e " << e << " with expected crc \ n "; <nl> + delete o ; <nl> MMonGetOSDMap * req = new MMonGetOSDMap ; <nl> req -> request_full ( e , last ); <nl> monc -> send_mon_message ( req );
void Monitor :: handle_probe ( MonOpRequestRef op ) <nl> } <nl> } <nl>  <nl> -/** <nl> - * @ todo fix this . This is going to cause trouble . <nl> - */ <nl> void Monitor :: handle_probe_probe ( MonOpRequestRef op ) <nl> { <nl> MMonProbe * m = static_cast < MMonProbe *>( op -> get_req ());
int ReplicatedPG :: do_osd_ops ( OpContext * ctx , vector < OSDOp >& ops ) <nl> case CEPH_OSD_OP_WATCH : <nl> ++ ctx -> num_write ; <nl> { <nl> + if (! obs . exists ) { <nl> + result = - ENOENT ; <nl> + break ; <nl> + } <nl> uint64_t cookie = op . watch . cookie ; <nl> bool do_watch = op . watch . flag & 1 ; <nl> entity_name_t entity = ctx -> reqid . name ;
extern " C " int rados_pool_list ( rados_t cluster , char * buf , size_t len ) <nl> std :: list < std :: string > pools ; <nl> client -> pool_list ( pools ); <nl>  <nl> + if (! buf ) <nl> + return - EINVAL ; <nl> + <nl> char * b = buf ; <nl> if ( b ) <nl> memset ( b , 0 , len );
struct MClientLease : public Message { <nl> h . first = sf ; <nl> h . last = sl ; <nl> } <nl> + private : <nl> + ~ MClientLease () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " client_lease "; } <nl> void print ( ostream & out ) { <nl> out << " client_lease ( a =" << ceph_lease_op_name ( get_action ())
OPTION ( cluster_network , OPT_STR , "") <nl> OPTION ( num_client , OPT_INT , 1 ) <nl> OPTION ( monmap , OPT_STR , "") <nl> OPTION ( mon_host , OPT_STR , "") <nl> + OPTION ( mon_dns_srv_name , OPT_STR , " ceph - mon ") <nl> OPTION ( lockdep , OPT_BOOL , false ) <nl> OPTION ( lockdep_force_backtrace , OPT_BOOL , false ) // always gather current backtrace at every lock <nl> OPTION ( run_dir , OPT_STR , "/ var / run / ceph ") // the "/ var / run / ceph " dir , created on daemon startup
void virtio_delete_queues ( VirtIODevice * vdev ) <nl> struct virtqueue * vq ; <nl> unsigned i ; <nl>  <nl> + if ( vdev -> info == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < vdev -> maxQueues ; i ++) { <nl> vq = vdev -> info [ i ]. vq ; <nl> if ( vq != NULL ) {
extern <nl> /* Sadly we can ' t include < malloc . h > as it causes a redefinition error */ <nl> size_t malloc_usable_size ( void *); <nl> # elif defined ( __APPLE__ ) <nl> - # include < malloc . h > <nl> + # if TARGET_OS_IPHONE <nl> + # include < malloc / malloc . h > <nl> + # else <nl> + # include < malloc . h > <nl> + # endif <nl> # else <nl> # error Do not know what to do here <nl> # endif
static int jpeg_size ( unsigned char * data , unsigned int data_size , <nl> return 0 ; <nl> } <nl> i += 2 ; <nl> - block_length = data [ i ] * 256 + data [ i + 1 ]; <nl> + if ( i + 1 < data_size ) <nl> + block_length = data [ i ] * 256 + data [ i + 1 ]; <nl> } <nl> } <nl> }
+/* vim : set tabstop = 8 shiftwidth = 4 softtabstop = 4 smarttab expandtab autoindent : */ <nl> /** <nl> * The following sites have various bits & pieces about PDF document <nl> * generation
EbmlElement * EbmlElement :: FindNextElement ( IOCallback & DataStream , const EbmlSe <nl> ReadIndex = SizeIdx - 1 ; <nl> memmove (& PossibleIdNSize [ 0 ], & PossibleIdNSize [ 1 ], ReadIndex ); <nl> UpperLevel = UpperLevel_original ; <nl> - } while ( MaxDataSize > DataStream . getFilePointer () - SizeIdx + PossibleID_Length ); <nl> + } while ( MaxDataSize >= ReadSize ); <nl>  <nl> return NULL ; <nl> }
jv jq_next ( jq_state * jq ) { <nl> if ( opcode != ON_BACKTRACK ( DESTRUCTURE_ALT )) { <nl> jv_free ( stack_pop ( jq )); // free the input <nl> stack_push ( jq , jv_invalid_get_msg ( jq -> error )); // push the error ' s message <nl> + } else { <nl> + jv_free ( jq -> error ); <nl> } <nl> jq -> error = jv_null (); <nl> uint16_t offset = * pc ++;
tar_directory_for_file ( GsfInfileTar * dir , const char * name , gboolean last ) <nl> gsf_infile_child_by_name ( GSF_INFILE ( dir ), <nl> dirname ); <nl> if ( subdir ) { <nl> + dir = GSF_IS_INFILE_TAR ( subdir ) <nl> + ? GSF_INFILE_TAR ( subdir ) <nl> + : dir ; <nl> /* Undo the ref . */ <nl> g_object_unref ( subdir ); <nl> - dir = GSF_INFILE_TAR ( subdir ); <nl> } else <nl> dir = tar_create_dir ( dir , dirname ); <nl> }
int32_t ByteArray :: SetFilledLength ( int32_t filled_length ) { <nl> } <nl>  <nl> int32_t ByteArray :: Get ( int32_t index ) { <nl> + if ( index < 0 || index >= Length ()) <nl> + return - 1 ; <nl> return InternalGet ( index ) & 0xff ; <nl> } <nl> 
l_uint32 * line ; <nl> pos = ( qpos + i ) % 8 ; <nl> npx = px + xpostab [ pos ]; <nl> npy = py + ypostab [ pos ]; <nl> + if ( npx < 0 || npx >= w || npy < 0 || npy >= h ) <nl> + continue ; <nl> line = data + npy * wpl ; <nl> val = GET_DATA_BIT ( line , npx ); <nl> if ( val ) {
void * poller (){ <nl> cmd_stdout = popen ( entry -> command , " r "); <nl> if ( cmd_stdout != NULL ) fgets ( cmd_result , 64 , cmd_stdout ); <nl> if ( is_number ( cmd_result )) result = atoll ( cmd_result ); <nl> - break ; <nl> + pclose ( cmd_stdout ); <nl> + break ; <nl> default : <nl> printf (" Unknown Action !\ n "); <nl> result = 0 ;
rsvg_acquire_file_resource ( const char * filename , const char * base_uri , GError * <nl>  <nl> g_byte_array_append ( array , ( guint8 *) data , length ); <nl> g_free ( data ); <nl> + g_free ( path ); <nl>  <nl> return array ; <nl> }
rsvg_filter_primitive_free ( gpointer impl ) <nl> { <nl> RsvgFilterPrimitive * primitive = impl ; <nl>  <nl> - g_string_free ( primitive -> in , TRUE ); <nl> - g_string_free ( primitive -> result , TRUE ); <nl> + if ( primitive -> in ) { <nl> + g_string_free ( primitive -> in , TRUE ); <nl> + } <nl> + <nl> + if ( primitive -> result ) { <nl> + g_string_free ( primitive -> result , TRUE ); <nl> + } <nl>  <nl> g_free ( primitive ); <nl> }
rsvg_filter_primitive_gaussian_blur_render ( RsvgFilterPrimitive * self , RsvgFilt <nl> boundarys . x0 , boundarys . y0 , <nl> boundarys . x1 - boundarys . x0 , boundarys . y1 - boundarys . y0 ); <nl> cairo_fill ( cr ); <nl> + cairo_destroy ( cr ); <nl> } <nl>  <nl> op . surface = output ;
int main ( int argc , char * argv []) <nl> " dosfslabel : labels can be no longer than 11 characters \ n "); <nl> exit ( 1 ); <nl> } <nl> - for ( i = 0 ; i < 11 ; i ++) <nl> + for ( i = 0 ; label [ i ] && i < 11 ; i ++) <nl> /* don ' t know if here should be more strict ! uppercase ( label [ i ])*/ <nl> if ( islower ( label [ i ])) { <nl> fprintf ( stderr ,
struct iw_rr_ctx { <nl> }; <nl>  <nl>  <nl> - static IW_INLINE double iw_sinc ( double x ) <nl> + static double iw_sinc ( double x ) <nl> { <nl> - if ( x == 0 . 0 ) return 1 . 0 ; <nl> + if ( x <= 0 . 000000005 ) return 1 . 0 ; <nl> return sin ( M_PI * x )/( M_PI * x ); <nl> } <nl> 
IW_IMPL ( int ) iw_read_bmp_file ( struct iw_context * ctx , struct iw_iodescr * iodescr <nl> done : <nl> if (! retval ) { <nl> iw_set_error ( ctx ," BMP read failed "); <nl> + // If we didn ' t call iw_set_input_image , ' img ' still belongs to us , <nl> + // so free its contents . <nl> + iw_free ( ctx , img . pixels ); <nl> } <nl> return retval ; <nl> }
DialInstance :: DialResult DialInstance :: execute () <nl>  <nl> void DialInstance :: prepareAddress () <nl> { <nl> - if ( mTargetUri . scheme () == Symbols :: Sip ) { <nl> + if ( mTargetUri . scheme () == Symbols :: Sip || <nl> + mTargetUri . scheme () == Symbols :: Sips ) { <nl> mFullTarget = mTargetUri ; <nl> return ; <nl> }
int ares_parse_a_reply ( const unsigned char * abuf , int alen , <nl> rr_class = DNS_RR_CLASS ( aptr ); <nl> rr_len = DNS_RR_LEN ( aptr ); <nl> aptr += RRFIXEDSZ ; <nl> + if ( aptr + rr_len > abuf + alen ) <nl> + { <nl> + free ( rr_name ); <nl> + status = ARES_EBADRESP ; <nl> + break ; <nl> + } <nl>  <nl> if ( rr_class == C_IN && rr_type == T_A <nl> && rr_len == sizeof ( struct in_addr )
int main ( gint argc , gchar ** argv ) { <nl> bind ("/", prefix ); <nl>  <nl> fail_if ( chroot ( prefix )); <nl> + fail_if ( chdir ("/")); <nl> fail_if ( execvp (* argv , argv )); <nl> } <nl> 
check_user_token ( const char * authfile , <nl> { <nl> if ( verbose ) <nl> D ( debug_file , " Match user / token as % s /% s ", username , otp_id ); <nl> + <nl> + fclose ( opwfile ); <nl> return AUTH_FOUND ; <nl> } <nl> }
static int create_zone_index ( const char * directory , timelib_tzdb * db ) <nl> db_index [ index_next ]. pos = data_size ; <nl> data_size += length ; <nl> free ( tzfile_data ); <nl> + <nl> + index_next ++; <nl> + } else { <nl> + free ( db_index [ index_next ]. id ); <nl> } <nl> } <nl> - <nl> - index_next ++; <nl> } <nl> } <nl> 
exit_on_inactivity ( gpointer user_data ) <nl> extern gboolean in_shutdown ; <nl>  <nl> if (! in_shutdown ) <nl> - exit ( 0 ); <nl> + { <nl> + GDBusConnection * session_bus ; <nl> + <nl> + session_bus = g_bus_get_sync ( G_BUS_TYPE_SESSION , NULL , NULL ); <nl> + g_dbus_connection_flush_sync ( session_bus , NULL , NULL ); <nl> + g_object_unref ( session_bus ); <nl> + <nl> + exit ( 0 ); <nl> + } <nl>  <nl> return FALSE ; <nl> }
retry : <nl> usbmuxd_device_info_t * devinfo = device_info_from_device_record ( dev ); <nl> if (! devinfo ) { <nl> DEBUG ( 1 , "% s : can ' t create device info object \ n ", __func__ ); <nl> - free ( payload ); <nl> return - 1 ; <nl> } <nl> collection_add (& tmpdevs , devinfo );
Perl_re_op_compile ( pTHX_ SV ** const patternp , int pat_count , <nl>  <nl> /* We have that number in RExC_npar */ <nl> RExC_total_parens = RExC_npar ; <nl> + <nl> + /* XXX For backporting , use long jumps if there is any possibility of <nl> + * overflow */ <nl> + if ( RExC_size > U16_MAX && ! RExC_use_BRANCHJ ) { <nl> + RExC_use_BRANCHJ = TRUE ; <nl> + flags |= RESTART_PARSE ; <nl> + } <nl> } <nl> else if (! MUST_RESTART ( flags )) { <nl> ReREFCNT_dec ( Rx );
S_study_chunk ( pTHX_ RExC_state_t * pRExC_state , regnode ** scanp , <nl> RExC_precomp ))); <nl> } <nl>  <nl> + if ( ( minnext > 0 && mincount >= SSize_t_MAX / minnext ) <nl> + || min >= SSize_t_MAX - minnext * mincount ) <nl> + { <nl> + FAIL (" Regexp out of space "); <nl> + } <nl> + <nl> min += minnext * mincount ; <nl> is_inf_internal |= deltanext == SSize_t_MAX <nl> || ( maxcount == REG_INFTY && minnext + deltanext > 0 );
int charconv_buffer ( const char * tocode , const char * fromcode , <nl> * result_len = result_length ; <nl> } <nl> } <nl> + else { <nl> + mmap_string_free ( mmapstr ); <nl> + } <nl> } <nl> + else { <nl> + mmap_string_free ( mmapstr ); <nl> + } <nl> return res ; <nl> } <nl> /* else , let ' s try with iconv , if available */
void rfbScaledScreenUpdateRect ( rfbScreenInfoPtr screen , rfbScreenInfoPtr ptr , in <nl> default : <nl> /* fixme : endianness problem ? */ <nl> for ( z = 0 ; z < bytesPerPixel ; z ++) <nl> - pixel_value += ( srcptr2 [ z ] << ( 8 * z )); <nl> + pixel_value += (( unsigned long ) srcptr2 [ z ] << ( 8 * z )); <nl> break ; <nl> } <nl> /*
HandleRFBServerMessage ( rfbClient * client ) <nl> return FALSE ; <nl> } <nl>  <nl> - buffer = malloc (( uint64_t ) msg . sct . length + 1 ); <nl> + buffer = malloc ( msg . sct . length + 1 ); <nl>  <nl> if (! ReadFromRFBServer ( client , buffer , msg . sct . length )) { <nl> free ( buffer );
rfbSendServerCutText ( rfbScreenInfoPtr rfbScreen , char * str , int len ) <nl> rfbServerCutTextMsg sct ; <nl> rfbClientIteratorPtr iterator ; <nl>  <nl> + memset (( char *)& sct , 0 , sizeof ( sct )); <nl> + <nl> iterator = rfbGetClientIterator ( rfbScreen ); <nl> while (( cl = rfbClientIteratorNext ( iterator )) != NULL ) { <nl> sct . type = rfbServerCutText ;
void rfbClientCleanup ( rfbClient * client ) { <nl> client -> clientData = next ; <nl> } <nl>  <nl> + free ( client -> vncRec ); <nl> + <nl> if ( client -> sock != RFB_INVALID_SOCKET ) <nl> rfbCloseSocket ( client -> sock ); <nl> if ( client -> listenSock != RFB_INVALID_SOCKET )
ConnectClientToUnixSock ( const char * sockFile ) <nl> int sock ; <nl> struct sockaddr_un addr ; <nl> addr . sun_family = AF_UNIX ; <nl> + if ( strlen ( sockFile ) + 1 > sizeof ( addr . sun_path )) { <nl> + rfbClientErr (" ConnectToUnixSock : socket file name too long \ n "); <nl> + return - 1 ; <nl> + } <nl> strcpy ( addr . sun_path , sockFile ); <nl>  <nl> sock = socket ( AF_UNIX , SOCK_STREAM , 0 );
static void ctrycatchfinally ( JF , js_Ast * trystm , js_Ast * catchvar , js_Ast * catch <nl> emitstring ( J , F , OP_CATCH , catchvar -> string ); <nl> cstm ( J , F , catchstm ); <nl> emit ( J , F , OP_ENDCATCH ); <nl> + emit ( J , F , OP_ENDTRY ); <nl> L3 = emitjump ( J , F , OP_JUMP ); /* skip past the try block to the finally block */ <nl> } <nl> label ( J , F , L1 );
qemuProcessHandleMonitorEOF ( qemuMonitorPtr mon , <nl> /* We don ' t want this EOF handler to be called over and over while the <nl> * thread is waiting for a job . <nl> */ <nl> + virObjectLock ( mon ); <nl> qemuMonitorUnregister ( mon ); <nl> + virObjectUnlock ( mon ); <nl>  <nl> /* We don ' t want any cleanup from EOF handler ( or any other <nl> * thread ) to enter qemu namespace . */
status WAVEFile :: parseFormat ( const Tag & id , uint32_t size ) <nl>  <nl> /* numCoefficients should be at least 7 . */ <nl> assert ( numCoefficients >= 7 && numCoefficients <= 255 ); <nl> + if ( numCoefficients < 7 || numCoefficients > 255 ) <nl> + { <nl> + _af_error ( AF_BAD_HEADER , <nl> + " Bad number of coefficients "); <nl> + return AF_FAIL ; <nl> + } <nl>  <nl> m_msadpcmNumCoefficients = numCoefficients ; <nl> 
_rsvg_io_get_file_path ( const gchar * filename , <nl> { <nl> gchar * absolute_filename ; <nl>  <nl> - if ( g_file_test ( filename , G_FILE_TEST_EXISTS ) || g_path_is_absolute ( filename )) { <nl> + if ( g_path_is_absolute ( filename )) { <nl> absolute_filename = g_strdup ( filename ); <nl> } else { <nl> gchar * tmpcdir ;
hfs_cat_traverse ( HFS_INFO * hfs , <nl> size_t rec_off ; <nl> hfs_btree_key_cat * key ; <nl> uint8_t retval ; <nl> - uint16_t keylen ; <nl> + int keylen ; <nl>  <nl> // get the record offset in the node <nl> rec_off = <nl> hfs_cat_traverse ( HFS_INFO * hfs , <nl> size_t rec_off ; <nl> hfs_btree_key_cat * key ; <nl> uint8_t retval ; <nl> - uint16_t keylen ; <nl> + int keylen ; <nl>  <nl> // get the record offset in the node <nl> rec_off =
NS_ASSUME_NONNULL_BEGIN <nl> @ interface OTRXMPPMessageYapStroage : XMPPModule <nl>  <nl> @ property ( nonatomic , strong , readonly ) YapDatabaseConnection * databaseConnection ; <nl> +@ property ( nonatomic , readonly ) dispatch_queue_t moduleDelegateQueue ; <nl>  <nl> /** This connection is only used for readWrites */ <nl> - ( instancetype ) initWithDatabaseConnection :( YapDatabaseConnection *) databaseConnection ;
int fs_mount ( spiffs * spf , uint32_t addr , uint32_t size , uint8_t * workbuf , <nl> cfg . hal_erase_f = esp_spiffs_erase ; <nl>  <nl> if ( SPIFFS_mount ( spf , & cfg , workbuf , fds , fds_size , 0 , 0 , 0 ) != SPIFFS_OK ) { <nl> + LOG ( LL_ERROR , (" SPIFFS_mount failed : % d ", SPIFFS_errno ( spf ))); <nl> return SPIFFS_errno ( spf ); <nl> } <nl> 
static const char * stbvox_vertex_program = <nl> " uniform vec3 normal_table [ 32 ];\ n " <nl>  <nl> # ifndef STBVOX_CONFIG_OPENGL_MODELVIEW <nl> - " uniform mat44 model_view ;\ n " <nl> + " uniform mat4x4 model_view ;\ n " <nl> # endif <nl>  <nl> // fragment output data
void stb_leakcheck_free ( void * ptr ) <nl> if ( mi -> next ) <nl> mi -> next -> prev = mi -> prev ; <nl> # endif <nl> + free ( ptr ); <nl> } <nl> } <nl> 
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
static int ndbcluster_reset_logs ( THD * thd ) <nl> if (! ndb_binlog_running ) <nl> return 0 ; <nl>  <nl> + /* only reset master should reset logs */ <nl> + if (!( thd -> lex -> type & REFRESH_MASTER )) <nl> + return 0 ; <nl> + <nl> DBUG_ENTER (" ndbcluster_reset_logs "); <nl>  <nl> /*
innobase_next_autoinc ( <nl> } else { <nl> next_value = current + increment ; <nl> } <nl> - } else { <nl> + } else if ( max_value > current ) { <nl> if ( current > offset ) { <nl> next_value = (( current - offset ) / increment ) + 1 ; <nl> } else { <nl> innobase_next_autoinc ( <nl> next_value += offset ; <nl> } <nl> } <nl> + } else { <nl> + next_value = max_value ; <nl> } <nl>  <nl> ut_a ( next_value <= max_value );
innodb_initialize ( <nl>  <nl> my_eng_config = ( eng_config_info_t *) config_str ; <nl>  <nl> + /* If no call back function registered ( InnoDB engine failed to load ), <nl> + load InnoDB Memcached engine should fail too */ <nl> + if (! my_eng_config -> cb_ptr ) { <nl> + return ( ENGINE_TMPFAIL ); <nl> + } <nl> + <nl> /* Register the call back function */ <nl> register_innodb_cb (( void *) my_eng_config -> cb_ptr ); <nl> 
bool st_table_list :: prepare_security ( THD * thd ) <nl> { <nl> List_iterator_fast < TABLE_LIST > tb (* view_tables ); <nl> TABLE_LIST * tbl ; <nl> + DBUG_ENTER (" st_table_list :: prepare_security "); <nl> # ifndef NO_EMBEDDED_ACCESS_CHECKS <nl> Security_context * save_security_ctx = thd -> security_ctx ; <nl> - DBUG_ENTER (" st_table_list :: prepare_security "); <nl>  <nl> DBUG_ASSERT (! prelocking_placeholder ); <nl> if ( prepare_view_securety_context ( thd ))
Ndb :: internalize_index_name ( const NdbTableImpl * table , <nl> if (! table ) <nl> { <nl> DBUG_PRINT (" error ", ("! table ")); <nl> - return ret ; <nl> + DBUG_RETURN ( ret ); <nl> } <nl>  <nl> if ( fullyQualifiedNames )
TABLE_COUNTER_TYPE Query_cache :: is_cacheable ( THD * thd , uint32 query_len , <nl> ( tables_used -> db_length == 5 && <nl> # ifdef FN_NO_CASE_SENCE <nl> // TODO : latin1 charset should be replaced with system charset <nl> - my_strncasecmp ( my_charset_latin1 , tables_used -> db ," mysql ", 5 ) == 0 <nl> + my_strncasecmp (& my_charset_latin1 , <nl> + tables_used -> db , <nl> + " mysql ", 5 ) == 0 <nl> # else <nl> tables_used -> db [ 0 ]==' m ' && <nl> tables_used -> db [ 1 ]==' y ' &&
Guardian :: Guardian ( Thread_registry * thread_registry_arg , <nl> monitoring_interval ( monitoring_interval_arg ), <nl> thread_registry ( thread_registry_arg ), <nl> instance_map ( instance_map_arg ), <nl> + guarded_instances ( 0 ), <nl> shutdown_requested ( FALSE ) <nl> { <nl> pthread_mutex_init (& LOCK_guardian , 0 );
inline void setup_table_map ( TABLE * table , TABLE_LIST * table_list , uint tablenr ) <nl> table -> tablenr = tablenr ; <nl> table -> map = ( table_map ) 1 << tablenr ; <nl> table -> force_index = table_list -> force_index ; <nl> + table -> force_index_order = table -> force_index_group = 0 ; <nl> table -> covering_keys = table -> s -> keys_for_keyread ; <nl> table -> merge_keys . clear_all (); <nl> }
history_save ( History * h , const char * fname ) <nl> retval = HPREV ( h , & ev ), i ++) { <nl> len = strlen ( ev . str ) * 4 ; <nl> if ( len >= max_size ) { <nl> - max_size = ( len + 1023 ) & 1023 ; <nl> + max_size = ( len + 1023 ) & ~ 1023 ; <nl> ptr = h_realloc ( ptr , max_size ); <nl> } <nl> ( void ) strvis ( ptr , ev . str , VIS_WHITE );
pstack_install_segv_action ( const char * path_format_ ) <nl> if (( abfd = load_bfd ( pid ))== 0 ) <nl> fprintf ( stderr , " BFD load failed ..\ n "); <nl> else { <nl> - long storage_needed = bfd_get_symtab_upper_bound ( abfd ); <nl> + long storage_needed = ( bfd_get_file_flags ( abfd ) & HAS_SYMS ) ? <nl> + bfd_get_symtab_upper_bound ( abfd ) : 0 ; <nl> long i ; <nl> ( void ) i ; <nl> 
static void trace_table_dependencies ( Opt_trace_context * trace , <nl> } <nl> Opt_trace_array depends_on ( trace , " depends_on_map_bits "); <nl> // RAND_TABLE_BIT may be in join_tabs [ i ]. dependent , so we test all 64 bits <nl> - compile_time_assert ( sizeof ( TABLE :: map ) <= 64 ); <nl> + compile_time_assert ( sizeof ( table -> map ) <= 64 ); <nl> for ( uint j = 0 ; j < 64 ; j ++) <nl> { <nl> if ( join_tabs [ i ]. dependent & ( 1ULL << j ))
void Dblqh :: writeSinglePage ( Signal * signal , Uint32 pageNo , <nl> signal -> theData [ 7 ] = pageNo ; <nl> sendSignal ( NDBFS_REF , GSN_FSWRITEREQ , signal , 8 , JBA ); <nl>  <nl> + if ( logFilePtr . p -> fileRef == RNIL ) <nl> + { <nl> + signal -> theData [ 0 ] = 2305 ; <nl> + execDUMP_STATE_ORD ( signal ); <nl> + } <nl> ndbrequire ( logFilePtr . p -> fileRef != RNIL ); <nl>  <nl> logPartPtr . p -> m_io_tracker . send_io ( 32768 );
MgmtSrvr :: dumpStateSelf ( const Uint32 args [], Uint32 no ) <nl> if ( no < 1 ) <nl> return - 1 ; <nl>  <nl> +# ifdef ERROR_INSERT <nl> switch ( args [ 0 ]) <nl> { <nl> case 9994 : <nl> MgmtSrvr :: dumpStateSelf ( const Uint32 args [], Uint32 no ) <nl> break ; <nl> } <nl>  <nl> -# ifdef ERROR_INSERT <nl> case 9996 : <nl> { <nl> /* Sendbuffer consumption */
int mysql_create_function ( THD * thd , udf_func * udf ) <nl> } <nl>  <nl> rw_wrlock (& THR_LOCK_udf ); <nl> - if (( hash_search (& udf_hash ,( byte *) & udf -> name . str , udf -> name . length ))) <nl> + if (( hash_search (& udf_hash ,( byte *) udf -> name . str , udf -> name . length ))) <nl> { <nl> net_printf ( thd , ER_UDF_EXISTS , udf -> name ); <nl> goto err ;
buf_get_latched_pages_number ( void ) <nl>  <nl> block = buf_pool_get_nth_block ( buf_pool , i ); <nl>  <nl> - if (( block -> buf_fix_count != 0 ) || ( block -> io_fix != 0 )) <nl> + if ((( block -> buf_fix_count != 0 ) || ( block -> io_fix != 0 )) && <nl> + block -> magic_n == BUF_BLOCK_MAGIC_N ) <nl> fixed_pages_number ++; <nl> } <nl> 
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
int SetCipherList ( Suites * s , const char * list ) <nl> byte b ; <nl> byte compression ; <nl> ProtocolVersion pv ; <nl> - word16 extSz ; <nl> word32 i = * inOutIdx ; <nl> word32 begin = i ; <nl> 
void AesCtrEncrypt ( Aes * aes , byte * out , const byte * in , word32 sz ) <nl> word32 blocks = sz / AES_BLOCK_SIZE ; <nl>  <nl> while ( blocks --) { <nl> - AesEncrypt ( aes , aes -> reg , out ); <nl> + AesEncrypt ( aes , ( byte *) aes -> reg , out ); <nl> IncrementAesCounter (( byte *) aes -> reg ); <nl> xorbuf ( out , in , AES_BLOCK_SIZE ); <nl> 
static int DoCertificate ( CYASSL * ssl , byte * input , word32 * inOutIdx , <nl> ret = KEYUSE_ENCIPHER_E ; <nl> } <nl> if (( ssl -> specs . sig_algo == rsa_sa_algo || <nl> - ssl -> specs . sig_algo == ecc_dsa_sa_algo ) && <nl> + ( ssl -> specs . sig_algo == ecc_dsa_sa_algo && <nl> + ! ssl -> specs . static_ecdh )) && <nl> ( dCert . extKeyUsage & KEYUSE_DIGITAL_SIG ) == 0 ) { <nl> CYASSL_MSG (" KeyUse Digital Sig not set "); <nl> ret = KEYUSE_SIGNATURE_E ;
int wolfSSL_EVP_MD_type ( const WOLFSSL_EVP_MD * md ) <nl> return WOLFSSL_FAILURE ; <nl> # endif <nl> } <nl> - <nl> +# ifdef SESSION_CERTS <nl> + ssl -> session . chain . count = 0 ; <nl> +# endif <nl> # ifdef KEEP_PEER_CERT <nl> FreeX509 (& ssl -> peerCert ); <nl> InitX509 (& ssl -> peerCert , 0 , ssl -> heap );
int SendCertificateRequest ( WOLFSSL * ssl ) <nl> /* write to output */ <nl> output [ i ++] = ( byte ) typeTotal ; /* # of types */ <nl> # ifdef HAVE_ECC <nl> - if ( ssl -> options . cipherSuite0 == ECC_BYTE && <nl> + if (( ssl -> options . cipherSuite0 == ECC_BYTE || <nl> + ssl -> options . cipherSuite0 == CHACHA_BYTE ) && <nl> ssl -> specs . sig_algo == ecc_dsa_sa_algo ) { <nl> output [ i ++] = ecdsa_sign ; <nl> } else
static int DecodePolicyOID ( char * out , word32 outSz , byte * in , word32 inSz ) <nl> # endif <nl> } <nl> idx += policy_length ; <nl> - } while (( int ) idx < total_length && cert -> extCertPoliciesNb < MAX_CERTPOL_NB ); <nl> + } while (( int ) idx < total_length <nl> + # if defined ( WOLFSSL_CERT_EXT ) <nl> + && cert -> extCertPoliciesNb < MAX_CERTPOL_NB <nl> + # endif <nl> + ); <nl>  <nl> WOLFSSL_LEAVE (" DecodeCertPolicy ", 0 ); <nl> return 0 ;
void * memchr_inv ( const void * start , int c , size_t bytes ) <nl>  <nl> value64 = value ; <nl> # if defined ( CONFIG_ARCH_HAS_FAST_MULTIPLIER ) && BITS_PER_LONG == 64 <nl> - value64 *= 0x0101010101010101 ; <nl> + value64 *= 0x0101010101010101ULL ; <nl> # elif defined ( CONFIG_ARCH_HAS_FAST_MULTIPLIER ) <nl> value64 *= 0x01010101 ; <nl> value64 |= value64 << 32 ;
int activate_fd ( int irq , int fd , int type , void * dev_id ) <nl> . events = events , <nl> . current_events = 0 } ); <nl>  <nl> + err = - EBUSY ; <nl> spin_lock_irqsave (& irq_lock , flags ); <nl> for ( irq_fd = active_fds ; irq_fd != NULL ; irq_fd = irq_fd -> next ) { <nl> if (( irq_fd -> fd == fd ) && ( irq_fd -> type == type )) {
static int btree_readpage_end_io_hook ( struct page * page , u64 start , u64 end , <nl> goto err ; <nl> } <nl> found_level = btrfs_header_level ( eb ); <nl> + if ( found_level >= BTRFS_MAX_LEVEL ) { <nl> + btrfs_info ( root -> fs_info , " bad tree block level % d \ n ", <nl> + ( int ) btrfs_header_level ( eb )); <nl> + ret = - EIO ; <nl> + goto err ; <nl> + } <nl>  <nl> btrfs_set_buffer_lockdep_class ( btrfs_header_owner ( eb ), <nl> eb , found_level );
static void mga_connector_destroy ( struct drm_connector * connector ) <nl> kfree ( connector ); <nl> } <nl>  <nl> - struct drm_connector_helper_funcs mga_vga_connector_helper_funcs = { <nl> + static const struct drm_connector_helper_funcs mga_vga_connector_helper_funcs = { <nl> . get_modes = mga_vga_get_modes , <nl> . mode_valid = mga_vga_mode_valid , <nl> . best_encoder = mga_connector_best_encoder , <nl> }; <nl>  <nl> - struct drm_connector_funcs mga_vga_connector_funcs = { <nl> + static const struct drm_connector_funcs mga_vga_connector_funcs = { <nl> . dpms = drm_helper_connector_dpms , <nl> . detect = mga_vga_detect , <nl> . fill_modes = drm_helper_probe_single_connector_modes ,
void * dma_generic_alloc_coherent ( struct device * dev , size_t size , <nl> void * ret , * ret_nocache ; <nl> int order = get_order ( size ); <nl>  <nl> + gfp |= __GFP_ZERO ; <nl> + <nl> ret = ( void *) __get_free_pages ( gfp , order ); <nl> if (! ret ) <nl> return NULL ; <nl>  <nl> - memset ( ret , 0 , size ); <nl> /* <nl> * Pages from the page allocator may have data present in <nl> * cache . So flush the cache before using uncached memory .
static int tpm_binary_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 5th : delimiter */ <nl> seq_putc ( m , '\ 0 '); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl>  <nl> static int tpm_ascii_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 4th : eventname <= max + \' 0 ' delimiter */ <nl> seq_printf ( m , " % s \ n ", eventname ); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl> 
int drm_fb_helper_init ( struct drm_device * dev , <nl> struct drm_crtc * crtc ; <nl> int i ; <nl>  <nl> + if (! max_conn_count ) <nl> + return - EINVAL ; <nl> + <nl> fb_helper -> dev = dev ; <nl>  <nl> INIT_LIST_HEAD (& fb_helper -> kernel_fb_list );
static int pci_dio_auto_attach ( struct comedi_device * dev , <nl> else <nl> dev -> iobase = pci_resource_start ( pcidev , 2 ); <nl>  <nl> + pci_dio_reset ( dev ); <nl> + <nl> ret = comedi_alloc_subdevices ( dev , board -> nsubdevs ); <nl> if ( ret ) <nl> return ret ; <nl> static int pci_dio_auto_attach ( struct comedi_device * dev , <nl> comedi_8254_subdevice_init ( s , dev -> pacer ); <nl> } <nl>  <nl> - pci_dio_reset ( dev ); <nl> - <nl> return 0 ; <nl> } <nl> 
static void xiic_fill_tx_fifo ( struct xiic_i2c * i2c ) <nl> /* last message in transfer -> STOP */ <nl> data |= XIIC_TX_DYN_STOP_MASK ; <nl> dev_dbg ( i2c -> adap . dev . parent , "% s TX STOP \ n ", __func__ ); <nl> - <nl> - xiic_setreg16 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> - } else <nl> - xiic_setreg8 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> + } <nl> + xiic_setreg16 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> } <nl> } <nl> 
accept_err : <nl> return result ; <nl> } <nl>  <nl> - int sctp_accept_from_sock ( struct connection * con ) <nl> + static int sctp_accept_from_sock ( struct connection * con ) <nl> { <nl> /* Check that the new node is in the lockspace */ <nl> struct sctp_prim prim ;
again : <nl>  <nl> # ifdef CONFIG_MODULE_SIG <nl> mod -> sig_ok = info -> sig_ok ; <nl> - if (! mod -> sig_ok ) <nl> + if (! mod -> sig_ok ) { <nl> + printk_once ( KERN_NOTICE <nl> + "% s : module verification failed : signature and / or " <nl> + " required key missing - tainting kernel \ n ", <nl> + mod -> name ); <nl> add_taint_module ( mod , TAINT_FORCED_MODULE ); <nl> + } <nl> # endif <nl>  <nl> /* Now module is in final location , initialize linked lists , etc . */
int __init init_dmars ( void ) <nl> deferred_flush = kzalloc ( g_num_of_iommus * <nl> sizeof ( struct deferred_flush_tables ), GFP_KERNEL ); <nl> if (! deferred_flush ) { <nl> - kfree ( g_iommus ); <nl> ret = - ENOMEM ; <nl> goto error ; <nl> }
static void <nl> intel_disable_cursor_plane ( struct drm_plane * plane , <nl> struct drm_crtc * crtc ) <nl> { <nl> + struct intel_crtc * intel_crtc = to_intel_crtc ( crtc ); <nl> + <nl> + intel_crtc -> cursor_addr = 0 ; <nl> intel_crtc_update_cursor ( crtc , false ); <nl> } <nl> 
static struct usb_function * geth_alloc ( struct usb_function_instance * fi ) <nl> sizeof ( geth -> ethaddr )); <nl> if ( status < 12 ) { <nl> kfree ( geth ); <nl> + mutex_unlock (& opts -> lock ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> geth_string_defs [ 1 ]. s = geth -> ethaddr ;
flush_signal_handlers ( struct task_struct * t , int force_default ) <nl> if ( force_default || ka -> sa . sa_handler != SIG_IGN ) <nl> ka -> sa . sa_handler = SIG_DFL ; <nl> ka -> sa . sa_flags = 0 ; <nl> +# ifdef SA_RESTORER <nl> + ka -> sa . sa_restorer = NULL ; <nl> +# endif <nl> sigemptyset (& ka -> sa . sa_mask ); <nl> ka ++; <nl> }
static const struct file_operations bm_register_operations = { <nl> static ssize_t <nl> bm_status_read ( struct file * file , char __user * buf , size_t nbytes , loff_t * ppos ) <nl> { <nl> - char * s = enabled ? " enabled " : " disabled "; <nl> + char * s = enabled ? " enabled \ n " : " disabled \ n "; <nl>  <nl> return simple_read_from_buffer ( buf , nbytes , ppos , s , strlen ( s )); <nl> }
static void switched_to_dl ( struct rq * rq , struct task_struct * p ) <nl> if ( unlikely ( p -> dl . dl_throttled )) <nl> return ; <nl>  <nl> - if ( p -> on_rq || rq -> curr != p ) { <nl> + if ( p -> on_rq && rq -> curr != p ) { <nl> # ifdef CONFIG_SMP <nl> if ( rq -> dl . overloaded && push_dl_task ( rq ) && rq != task_rq ( p )) <nl> /* Only reschedule if pushing failed */
static int __init fcoe_init ( void ) <nl> /* Setup link change notification */ <nl> fcoe_dev_setup (); <nl>  <nl> - fcoe_if_init (); <nl> + rc = fcoe_if_init (); <nl> + if ( rc ) <nl> + goto out_free ; <nl>  <nl> return 0 ; <nl> 
static int mptsas_smp_handler ( struct Scsi_Host * shost , struct sas_rphy * rphy , <nl> smprep = ( SmpPassthroughReply_t *) ioc -> sas_mgmt . reply ; <nl> memcpy ( req -> sense , smprep , sizeof (* smprep )); <nl> req -> sense_len = sizeof (* smprep ); <nl> + req -> data_len = 0 ; <nl> + rsp -> data_len -= smprep -> ResponseDataLength ; <nl> } else { <nl> printk ( MYIOC_s_ERR_FMT "% s : smp passthru reply failed to be returned \ n ", <nl> ioc -> name , __FUNCTION__ );
static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
struct page * kmap_to_page ( void * vaddr ) <nl>  <nl> return virt_to_page ( addr ); <nl> } <nl> + EXPORT_SYMBOL ( kmap_to_page ); <nl>  <nl> static void flush_all_zero_pkmaps ( void ) <nl> {
static void xfrm4_dst_destroy ( struct dst_entry * dst ) <nl>  <nl> if ( likely ( xdst -> u . rt . idev )) <nl> in_dev_put ( xdst -> u . rt . idev ); <nl> + if ( likely ( xdst -> u . rt . peer )) <nl> + inet_putpeer ( xdst -> u . rt . peer ); <nl> xfrm_dst_destroy ( xdst ); <nl> } <nl> 
static int initialize_usbh1_port ( struct platform_device * pdev ) <nl>  <nl> mdelay ( 10 ); <nl>  <nl> - return mx51_initialize_usb_hw ( 0 , MXC_EHCI_ITC_NO_THRESHOLD ); <nl> + return mx51_initialize_usb_hw ( pdev -> id , MXC_EHCI_ITC_NO_THRESHOLD ); <nl> } <nl>  <nl> static struct mxc_usbh_platform_data usbh1_config = {
void rtl92e_set_key ( struct net_device * dev , u8 EntryNo , u8 KeyIndex , <nl> } <nl> } <nl> priv -> rtllib -> is_set_key = true ; <nl> - if ( EntryNo >= TOTAL_CAM_ENTRY ) <nl> + if ( EntryNo >= TOTAL_CAM_ENTRY ) { <nl> netdev_info ( dev , "% s (): Invalid CAM entry \ n ", __func__ ); <nl> + return ; <nl> + } <nl>  <nl> RT_TRACE ( COMP_SEC , <nl> "====> to rtl92e_set_key (), dev :% p , EntryNo :% d , KeyIndex :% d , KeyType :% d , MacAddr % pM \ n ",
e1000_set_tso ( struct net_device * netdev , u32 data ) <nl> else <nl> netdev -> features &= ~ NETIF_F_TSO ; <nl>  <nl> - if ( data ) <nl> + if ( data && ( adapter -> hw . mac_type > e1000_82547_rev_2 )) <nl> netdev -> features |= NETIF_F_TSO6 ; <nl> else <nl> netdev -> features &= ~ NETIF_F_TSO6 ;
static int try_to_unmap_file ( struct page * page , enum ttu_flags flags ) <nl> unsigned long max_nl_size = 0 ; <nl> unsigned int mapcount ; <nl>  <nl> + if ( PageHuge ( page )) <nl> + pgoff = page -> index << compound_order ( page ); <nl> + <nl> mutex_lock (& mapping -> i_mmap_mutex ); <nl> vma_interval_tree_foreach ( vma , & mapping -> i_mmap , pgoff , pgoff ) { <nl> unsigned long address = vma_address ( page , vma );
void free_user_ns ( struct kref * kref ) <nl> struct user_namespace * ns ; <nl>  <nl> ns = container_of ( kref , struct user_namespace , kref ); <nl> + free_uid ( ns -> root_user ); <nl> kfree ( ns ); <nl> } <nl> 
static void * arm64_swiotlb_alloc_coherent ( struct device * dev , size_t size , <nl> if ( IS_ENABLED ( CONFIG_DMA_CMA )) { <nl> struct page * page ; <nl>  <nl> + size = PAGE_ALIGN ( size ); <nl> page = dma_alloc_from_contiguous ( dev , size >> PAGE_SHIFT , <nl> get_order ( size )); <nl> if (! page )
static inline bool ipv4_is_local_multicast ( __be32 addr ) <nl> static inline bool ipv4_is_lbcast ( __be32 addr ) <nl> { <nl> /* limited broadcast */ <nl> - return addr == INADDR_BROADCAST ; <nl> + return addr == htonl ( INADDR_BROADCAST ); <nl> } <nl>  <nl> static inline bool ipv4_is_zeronet ( __be32 addr )
static void kvm_mmu_remove_some_alloc_mmu_pages ( struct kvm * kvm , <nl> { <nl> struct kvm_mmu_page * page ; <nl>  <nl> + if ( list_empty (& kvm -> arch . active_mmu_pages )) <nl> + return ; <nl> + <nl> page = container_of ( kvm -> arch . active_mmu_pages . prev , <nl> struct kvm_mmu_page , link ); <nl> kvm_mmu_prepare_zap_page ( kvm , page , invalid_list );
static int quota_setinfo ( struct super_block * sb , int type , void __user * addr ) <nl>  <nl> static void copy_to_if_dqblk ( struct if_dqblk * dst , struct fs_disk_quota * src ) <nl> { <nl> + memset ( dst , 0 , sizeof (* dst )); <nl> dst -> dqb_bhardlimit = src -> d_blk_hardlimit ; <nl> dst -> dqb_bsoftlimit = src -> d_blk_softlimit ; <nl> dst -> dqb_curspace = src -> d_bcount ;
static int __init sm_it87_init ( void ) <nl>  <nl> static void __exit sm_it87_exit ( void ) <nl> { <nl> - i2c_isa_del_driver (& it87_isa_driver ); <nl> + if ( isa_address ) <nl> + i2c_isa_del_driver (& it87_isa_driver ); <nl> i2c_del_driver (& it87_driver ); <nl> } <nl> 
static int omap_hdmi_dai_hw_params ( struct snd_pcm_substream * substream , <nl> /* <nl> * fill the IEC - 60958 channel status word <nl> */ <nl> + /* initialize the word bytes */ <nl> + memset ( iec -> status , 0 , sizeof ( iec -> status )); <nl>  <nl> /* specify IEC - 60958 - 3 ( commercial use ) */ <nl> iec -> status [ 0 ] &= ~ IEC958_AES0_PROFESSIONAL ;
static inline unsigned long long res_counter_margin ( struct res_counter * cnt ) <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& cnt -> lock , flags ); <nl> - margin = cnt -> limit - cnt -> usage ; <nl> + if ( cnt -> limit > cnt -> usage ) <nl> + margin = cnt -> limit - cnt -> usage ; <nl> + else <nl> + margin = 0 ; <nl> spin_unlock_irqrestore (& cnt -> lock , flags ); <nl> return margin ; <nl> }
static struct drm_display_mode * drm_mode_detailed ( struct drm_device * dev , <nl> return NULL ; <nl> } <nl>  <nl> + /* Some EDIDs have bogus h / vtotal values */ <nl> + if ( mode -> hsync_end > mode -> htotal ) <nl> + mode -> htotal = mode -> hsync_end + 1 ; <nl> + if ( mode -> vsync_end > mode -> vtotal ) <nl> + mode -> vtotal = mode -> vsync_end + 1 ; <nl> + <nl> drm_mode_set_name ( mode ); <nl>  <nl> if ( pt -> misc & DRM_EDID_PT_INTERLACED )
static inline int local_sid_lookup ( struct id * entry ) <nl> return - 1 ; <nl> } <nl>  <nl> -/* Invalidate all id mappings on local core */ <nl> +/* Invalidate all id mappings on local core -- call with preempt disabled */ <nl> static inline void local_sid_destroy_all ( void ) <nl> { <nl> - preempt_disable (); <nl> __get_cpu_var ( pcpu_last_used_sid ) = 0 ; <nl> memset (& __get_cpu_var ( pcpu_sids ), 0 , sizeof ( __get_cpu_var ( pcpu_sids ))); <nl> - preempt_enable (); <nl> } <nl>  <nl> static void * kvmppc_e500_id_table_alloc ( struct kvmppc_vcpu_e500 * vcpu_e500 )
static int do_lo_send_aops ( struct loop_device * lo , struct bio_vec * bvec , <nl> if ( ret ) <nl> goto fail ; <nl>  <nl> + file_update_time ( file ); <nl> + <nl> transfer_result = lo_do_transfer ( lo , WRITE , page , offset , <nl> bvec -> bv_page , bv_offs , size , IV ); <nl> copied = size ;
static int __init orion_nand_probe ( struct platform_device * pdev ) <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) { <nl> - err = - ENODEV ; <nl> + ret = - ENODEV ; <nl> goto no_res ; <nl> } <nl> 
static int iwl_read_ucode ( struct iwl_priv * priv ) <nl> priv -> ucode_data_backup . len = data_size ; <nl> iwl_alloc_fw_desc ( priv -> pci_dev , & priv -> ucode_data_backup ); <nl>  <nl> + if (! priv -> ucode_code . v_addr || ! priv -> ucode_data . v_addr || <nl> + ! priv -> ucode_data_backup . v_addr ) <nl> + goto err_pci_alloc ; <nl> + <nl> /* Initialization instructions and data */ <nl> if ( init_size && init_data_size ) { <nl> priv -> ucode_init . len = init_size ;
static void br_multicast_port_query_expired ( unsigned long data ) <nl> struct net_bridge * br = port -> br ; <nl>  <nl> spin_lock (& br -> multicast_lock ); <nl> - if ( port && ( port -> state == BR_STATE_DISABLED || <nl> - port -> state == BR_STATE_BLOCKING )) <nl> + if ( port -> state == BR_STATE_DISABLED || <nl> + port -> state == BR_STATE_BLOCKING ) <nl> goto out ; <nl>  <nl> if ( port -> multicast_startup_queries_sent <
static void atmel_sha_get_cap ( struct atmel_sha_dev * dd ) <nl>  <nl> /* keep only major version number */ <nl> switch ( dd -> hw_version & 0xff0 ) { <nl> + case 0x420 : <nl> + dd -> caps . has_dma = 1 ; <nl> + dd -> caps . has_dualbuff = 1 ; <nl> + dd -> caps . has_sha224 = 1 ; <nl> + dd -> caps . has_sha_384_512 = 1 ; <nl> + break ; <nl> case 0x410 : <nl> dd -> caps . has_dma = 1 ; <nl> dd -> caps . has_dualbuff = 1 ;
cio_start_key ( struct subchannel * sch , /* subchannel structure */ <nl> CIO_TRACE_EVENT ( 4 , sch -> dev . bus_id ); <nl>  <nl> orb = & to_io_private ( sch )-> orb ; <nl> + memset ( orb , 0 , sizeof ( union orb )); <nl> /* sch is always under 2G . */ <nl> orb -> cmd . intparm = ( u32 )( addr_t ) sch ; <nl> orb -> cmd . fmt = 1 ;
int rxrpc_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( copy > len - copied ) <nl> copy = len - copied ; <nl>  <nl> - if ( skb -> ip_summed == CHECKSUM_UNNECESSARY ) { <nl> + if ( skb -> ip_summed == CHECKSUM_UNNECESSARY || <nl> + skb -> ip_summed == CHECKSUM_PARTIAL ) { <nl> ret = skb_copy_datagram_iovec ( skb , offset , <nl> msg -> msg_iov , copy ); <nl> } else {
static unsigned int iwl_hw_get_beacon_cmd ( struct iwl_priv * priv , <nl> sizeof ( frame -> u ) - sizeof (* tx_beacon_cmd )); <nl> if ( WARN_ON_ONCE ( frame_size > MAX_MPDU_SIZE )) <nl> return 0 ; <nl> + if (! frame_size ) <nl> + return 0 ; <nl>  <nl> /* Set up TX command fields */ <nl> tx_beacon_cmd -> tx . len = cpu_to_le16 (( u16 ) frame_size );
static void raise_dtr ( struct isi_port * port ) <nl> } <nl>  <nl> /* card -> lock HAS to be held */ <nl> - static inline void drop_dtr ( struct isi_port * port ) <nl> + static void drop_dtr ( struct isi_port * port ) <nl> { <nl> struct isi_board * card = port -> card ; <nl> unsigned long base = card -> base ;
static void vmw_user_surface_base_release ( struct ttm_base_object ** p_base ) <nl> struct vmw_resource * res = & user_srf -> srf . res ; <nl>  <nl> * p_base = NULL ; <nl> - ttm_base_object_unref (& user_srf -> backup_base ); <nl> + if ( user_srf -> backup_base ) <nl> + ttm_base_object_unref (& user_srf -> backup_base ); <nl> vmw_resource_unreference (& res ); <nl> } <nl> 
static int lis3l02dq_read_raw ( struct iio_dev * indio_dev , <nl> ret = lis3l02dq_read_reg_s16 ( indio_dev , reg , val ); <nl> } <nl> mutex_unlock (& indio_dev -> mlock ); <nl> + if ( ret < 0 ) <nl> + goto error_ret ; <nl> return IIO_VAL_INT ; <nl> case IIO_CHAN_INFO_SCALE : <nl> * val = 0 ;
static int radeon_surface_free ( DRM_IOCTL_ARGS ) <nl> return DRM_ERR ( EINVAL ); <nl> } <nl>  <nl> - DRM_COPY_FROM_USER_IOCTL ( memfree , ( drm_radeon_mem_free_t __user *) data , <nl> + DRM_COPY_FROM_USER_IOCTL ( memfree , ( drm_radeon_surface_free_t __user *) data , <nl> sizeof ( memfree )); <nl>  <nl> if ( free_surface ( filp , dev_priv , memfree . address ))
static void incoming_packet ( struct sasem_context * context , <nl> } <nl>  <nl> if ( debug ) { <nl> - printk ( KERN_INFO " Incoming data : "); <nl> + pr_info (" Incoming data : "); <nl> for ( i = 0 ; i < 8 ; ++ i ) <nl> - printk ( KERN_CONT "% 02x ", buf [ i ]); <nl> - printk ( KERN_CONT "\ n "); <nl> + pr_cont ("% 02x ", buf [ i ]); <nl> + pr_cont ("\ n "); <nl> } <nl>  <nl> /*
i915_gem_object_finish_gpu ( struct drm_i915_gem_object * obj ) <nl> return ret ; <nl> } <nl>  <nl> + ret = i915_gem_object_wait_rendering ( obj ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* Ensure that we invalidate the GPU ' s caches and TLBs . */ <nl> obj -> base . read_domains &= ~ I915_GEM_GPU_DOMAINS ; <nl> - <nl> - return i915_gem_object_wait_rendering ( obj ); <nl> + return 0 ; <nl> } <nl>  <nl> /**
int dwc2_hcd_init ( struct dwc2_hsotg * hsotg , int irq , <nl> if (! hcd ) <nl> goto error1 ; <nl>  <nl> + if ( hsotg -> core_params -> dma_enable <= 0 ) <nl> + hcd -> self . uses_dma = 0 ; <nl> + <nl> hcd -> has_tt = 1 ; <nl>  <nl> spin_lock_init (& hsotg -> lock );
int __devinit mthca_init_eq_table ( struct mthca_dev * dev ) <nl> dev -> eq_table . clr_mask = <nl> swab32 ( 1 << ( dev -> eq_table . inta_pin & 31 )); <nl> dev -> eq_table . clr_int = dev -> clr_base + <nl> - ( dev -> eq_table . inta_pin < 31 ? 4 : 0 ); <nl> + ( dev -> eq_table . inta_pin < 32 ? 4 : 0 ); <nl> } <nl>  <nl> dev -> eq_table . arm_mask = 0 ;
static int ds1374_probe ( struct i2c_adapter * adap , int addr , int kind ) <nl> client -> driver = & ds1374_driver ; <nl>  <nl> ds1374_workqueue = create_singlethread_workqueue (" ds1374 "); <nl> + if (! ds1374_workqueue ) { <nl> + kfree ( client ); <nl> + return - ENOMEM ; /* most expected reason */ <nl> + } <nl>  <nl> if (( rc = i2c_attach_client ( client )) != 0 ) { <nl> kfree ( client );
static int dw_mci_data_complete ( struct dw_mci * host , struct mmc_data * data ) <nl> data -> error = - EIO ; <nl> } <nl>  <nl> - dev_err ( host -> dev , " data error , status 0x % 08x \ n ", status ); <nl> + dev_dbg ( host -> dev , " data error , status 0x % 08x \ n ", status ); <nl>  <nl> /* <nl> * After an error , there may be data lingering
static int __devinit twl_rtc_probe ( struct platform_device * pdev ) <nl> if ( ret < 0 ) <nl> goto out1 ; <nl>  <nl> - ret = request_irq ( irq , twl_rtc_interrupt , <nl> + ret = request_threaded_irq ( irq , NULL , twl_rtc_interrupt , <nl> IRQF_TRIGGER_RISING , <nl> dev_name (& rtc -> dev ), rtc ); <nl> if ( ret < 0 ) {
static LIST_HEAD ( pinctrl_maps ); <nl> list_for_each_entry ( _maps_node_ , & pinctrl_maps , node ) \ <nl> for ( _i_ = 0 , _map_ = & _maps_node_ -> maps [ _i_ ]; \ <nl> _i_ < _maps_node_ -> num_maps ; \ <nl> - i ++, _map_ = & _maps_node_ -> maps [ _i_ ]) <nl> + _i_ ++, _map_ = & _maps_node_ -> maps [ _i_ ]) <nl>  <nl> /** <nl> * pinctrl_provide_dummies () - indicate if pinctrl provides dummy state support
static void um_new_card ( DESCRIPTOR * d ) <nl> } else { <nl> DBG_ERR ((" could not create user mode idi card % d ", <nl> adapter_nr )); <nl> + diva_os_free ( 0 , card ); <nl> } <nl> } <nl> 
bnx2_init_5709_context ( struct bnx2 * bp ) <nl> for ( i = 0 ; i < bp -> ctx_pages ; i ++) { <nl> int j ; <nl>  <nl> + if ( bp -> ctx_blk [ i ]) <nl> + memset ( bp -> ctx_blk [ i ], 0 , BCM_PAGE_SIZE ); <nl> + else <nl> + return - ENOMEM ; <nl> + <nl> REG_WR ( bp , BNX2_CTX_HOST_PAGE_TBL_DATA0 , <nl> ( bp -> ctx_blk_mapping [ i ] & 0xffffffff ) | <nl> BNX2_CTX_HOST_PAGE_TBL_DATA0_VALID );
enum node_states { <nl> # else <nl> N_HIGH_MEMORY = N_NORMAL_MEMORY , <nl> # endif <nl> + N_MEMORY = N_HIGH_MEMORY , <nl> N_CPU , /* The node has one or more cpus */ <nl> NR_NODE_STATES <nl> };
void hw_cursor_setData ( struct lynx_cursor * cursor , <nl> iowrite16 ( data , pbuffer ); <nl>  <nl> /* assume pitch is 1 , 2 , 4 , 8 ,...*/ <nl> - if (( i + 1 ) % pitch == 0 ) <nl> - { <nl> + if (( i + 1 ) % pitch == 0 ) { <nl> /* need a return */ <nl> pstart += offset ; <nl> pbuffer = pstart ;
static int pagemap_pte_range ( pmd_t * pmd , unsigned long addr , unsigned long end , <nl>  <nl> /* find the first VMA at or above ' addr ' */ <nl> vma = find_vma ( walk -> mm , addr ); <nl> - if ( pmd_trans_huge_lock ( pmd , vma ) == 1 ) { <nl> + if ( vma && pmd_trans_huge_lock ( pmd , vma ) == 1 ) { <nl> for (; addr != end ; addr += PAGE_SIZE ) { <nl> unsigned long offset ; <nl> 
bool iwl_mvm_bt_coex_is_shared_ant_avail ( struct iwl_mvm * mvm ) <nl> if (!( mvm -> fw -> ucode_capa . api [ 0 ] & IWL_UCODE_TLV_API_BT_COEX_SPLIT )) <nl> return iwl_mvm_bt_coex_is_shared_ant_avail_old ( mvm ); <nl>  <nl> - return le32_to_cpu ( mvm -> last_bt_notif . bt_activity_grading ) == BT_OFF ; <nl> + return le32_to_cpu ( mvm -> last_bt_notif . bt_activity_grading ) < BT_HIGH_TRAFFIC ; <nl> } <nl>  <nl> bool iwl_mvm_bt_coex_is_tpc_allowed ( struct iwl_mvm * mvm ,
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> * queued ( again ) by ceph_update_snap_trace () <nl> * below . Queue it _now_ , under the old context . <nl> */ <nl> + spin_lock (& realm -> inodes_with_caps_lock ); <nl> list_del_init (& ci -> i_snap_realm_item ); <nl> + spin_unlock (& realm -> inodes_with_caps_lock ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> ceph_queue_cap_snap ( ci ,
void eth_header_cache_update ( struct hh_cache * hh , struct net_device * dev , <nl> static int eth_mac_addr ( struct net_device * dev , void * p ) <nl> { <nl> struct sockaddr * addr = p ; <nl> + <nl> if ( netif_running ( dev )) <nl> return - EBUSY ; <nl> + if (! is_valid_ether_addr ( addr -> sa_data )) <nl> + return - EADDRNOTAVAIL ; <nl> memcpy ( dev -> dev_addr , addr -> sa_data , dev -> addr_len ); <nl> return 0 ; <nl> }
void arch_release_hugepage ( struct page * page ) <nl> ptep = ( pte_t *) page [ 1 ]. index ; <nl> if (! ptep ) <nl> return ; <nl> + clear_table (( unsigned long *) ptep , _PAGE_TYPE_EMPTY , <nl> + PTRS_PER_PTE * sizeof ( pte_t )); <nl> page_table_free (& init_mm , ( unsigned long *) ptep ); <nl> page [ 1 ]. index = 0 ; <nl> }
mext_check_arguments ( struct inode * orig_inode , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( IS_IMMUTABLE ( donor_inode ) || IS_APPEND ( donor_inode )) <nl> + return - EPERM ; <nl> + <nl> /* Ext4 move extent does not support swapfile */ <nl> if ( IS_SWAPFILE ( orig_inode ) || IS_SWAPFILE ( donor_inode )) { <nl> ext4_debug (" ext4 move extent : The argument files should "
static s32 igb_get_invariants_82575 ( struct e1000_hw * hw ) <nl> * for setting word_size . <nl> */ <nl> size += NVM_WORD_SIZE_BASE_SHIFT ; <nl> + <nl> + /* EEPROM access above 16k is unsupported */ <nl> + if ( size > 14 ) <nl> + size = 14 ; <nl> nvm -> word_size = 1 << size ; <nl>  <nl> /* setup PHY parameters */
static int wm9713_soc_probe ( struct snd_soc_codec * codec ) <nl> if ( IS_ERR ( wm9713 -> ac97 )) <nl> return PTR_ERR ( wm9713 -> ac97 ); <nl>  <nl> - regmap = devm_regmap_init_ac97 ( wm9713 -> ac97 , & wm9713_regmap_config ); <nl> + regmap = regmap_init_ac97 ( wm9713 -> ac97 , & wm9713_regmap_config ); <nl> if ( IS_ERR ( regmap )) { <nl> snd_soc_free_ac97_codec ( wm9713 -> ac97 ); <nl> return PTR_ERR ( regmap );
static int iommu_no_mapping ( struct pci_dev * pdev ) <nl> if ( pdev -> dma_mask > DMA_BIT_MASK ( 32 )) { <nl> int ret ; <nl> ret = domain_add_dev_info ( si_domain , pdev ); <nl> + if ( ret ) <nl> + return 0 ; <nl> + ret = domain_context_mapping ( si_domain , pdev , CONTEXT_TT_MULTI_LEVEL ); <nl> if (! ret ) { <nl> printk ( KERN_INFO " 64bit % s uses identity mapping \ n ", <nl> pci_name ( pdev ));
static int ks8695_poll ( struct napi_struct * napi , int budget ) <nl> if ( work_done < budget ) { <nl> unsigned long flags ; <nl> spin_lock_irqsave (& ksp -> rx_lock , flags ); <nl> + __napi_complete ( napi ); <nl> /* enable rx interrupt */ <nl> writel ( isr | mask_bit , KS8695_IRQ_VA + KS8695_INTEN ); <nl> - __napi_complete ( napi ); <nl> spin_unlock_irqrestore (& ksp -> rx_lock , flags ); <nl> } <nl> return work_done ;
xfrm_state_find ( const xfrm_address_t * daddr , const xfrm_address_t * saddr , <nl> xfrm_state_look_at ( pol , x , fl , encap_family , <nl> & best , & acquire_in_progress , & error ); <nl> } <nl> - if ( best ) <nl> + if ( best || acquire_in_progress ) <nl> goto found ; <nl>  <nl> h_wildcard = xfrm_dst_hash ( net , daddr , & saddr_wildcard , tmpl -> reqid , encap_family );
static int mthca_alloc_qp_common ( struct mthca_dev * dev , <nl> int i ; <nl>  <nl> atomic_set (& qp -> refcount , 1 ); <nl> + init_waitqueue_head (& qp -> wait ); <nl> qp -> state = IB_QPS_RESET ; <nl> qp -> atomic_rd_en = 0 ; <nl> qp -> resp_depth = 0 ;
special_insn : <nl> case 0x88 ... 0x8b : /* mov */ <nl> goto mov ; <nl> case 0x8d : /* lea r16 / r32 , m */ <nl> - c -> dst . val = c -> modrm_val ; <nl> + c -> dst . val = c -> modrm_ea ; <nl> break ; <nl> case 0x8f : /* pop ( sole member of Grp1a ) */ <nl> rc = emulate_grp1a ( ctxt , ops );
static void vmid_reference ( struct snd_soc_codec * codec ) <nl> WM8994_LINEOUT2_DISCH , <nl> WM8994_LINEOUT_VMID_BUF_ENA ); <nl>  <nl> + wm_hubs_vmid_ena ( codec ); <nl> + <nl> /* Startup bias , VMID ramp & buffer */ <nl> snd_soc_update_bits ( codec , WM8994_ANTIPOP_2 , <nl> WM8994_BIAS_SRC | <nl> static void vmid_reference ( struct snd_soc_codec * codec ) <nl> WM8994_VMID_BUF_ENA | <nl> ( 0x2 << WM8994_VMID_RAMP_SHIFT )); <nl>  <nl> - wm_hubs_vmid_ena ( codec ); <nl> - <nl> /* Main bias enable , VMID = 2x40k */ <nl> snd_soc_update_bits ( codec , WM8994_POWER_MANAGEMENT_1 , <nl> WM8994_BIAS_ENA |
static int __devinit gen_74x164_probe ( struct spi_device * spi ) <nl> } <nl>  <nl> chip -> gpio_chip . ngpio = GEN_74X164_NUMBER_GPIOS * chip -> registers ; <nl> - chip -> buffer = devm_kzalloc (& spi -> dev , chip -> gpio_chip . ngpio , GFP_KERNEL ); <nl> + chip -> buffer = devm_kzalloc (& spi -> dev , chip -> registers , GFP_KERNEL ); <nl> if (! chip -> buffer ) { <nl> ret = - ENOMEM ; <nl> goto exit_destroy ;
int usb_alloc_streams ( struct usb_interface * interface , <nl> return - EINVAL ; <nl> if ( dev -> speed != USB_SPEED_SUPER ) <nl> return - EINVAL ; <nl> + if ( dev -> state < USB_STATE_CONFIGURED ) <nl> + return - ENODEV ; <nl>  <nl> for ( i = 0 ; i < num_eps ; i ++) { <nl> /* Streams only apply to bulk endpoints . */
static void reg_process_pending_hints ( void ) <nl> } <nl>  <nl> reg_process_hint ( reg_request ); <nl> + <nl> + lr = get_last_request (); <nl> + <nl> + spin_lock (& reg_requests_lock ); <nl> + if (! list_empty (& reg_requests_list ) && lr && lr -> processed ) <nl> + schedule_work (& reg_work ); <nl> + spin_unlock (& reg_requests_lock ); <nl> } <nl>  <nl> /* Processes beacon hints -- this has nothing to do with country IEs */
static DEFINE_PCI_DEVICE_TABLE ( rt2800pci_device_table ) = { <nl> { PCI_DEVICE ( 0x1814 , 0x5390 ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x5392 ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x539a ) }, <nl> + { PCI_DEVICE ( 0x1814 , 0x539b ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x539f ) }, <nl> # endif <nl> { 0 , }
static enum blk_eh_timer_return mtip_cmd_timeout ( struct request * req , <nl> bool reserved ) <nl> { <nl> struct driver_data * dd = req -> q -> queuedata ; <nl> - int ret = BLK_EH_RESET_TIMER ; <nl>  <nl> if ( reserved ) <nl> goto exit_handler ; <nl> static enum blk_eh_timer_return mtip_cmd_timeout ( struct request * req , <nl>  <nl> wake_up_interruptible (& dd -> port -> svc_wait ); <nl> exit_handler : <nl> - return ret ; <nl> + return BLK_EH_RESET_TIMER ; <nl> } <nl>  <nl> static struct blk_mq_ops mtip_mq_ops = {
static void qeth_clear_output_buffer ( struct qeth_qdio_out_q * queue , <nl> buf -> buffer -> element [ i ]. addr = NULL ; <nl> buf -> buffer -> element [ i ]. flags = 0 ; <nl> } <nl> + buf -> buffer -> element [ 15 ]. flags = 0 ; <nl> buf -> next_element_to_fill = 0 ; <nl> atomic_set (& buf -> state , QETH_QDIO_BUF_EMPTY ); <nl> }
static int unqueue_me ( struct futex_q * q ) <nl> /* In the common case we don ' t take the spinlock , which is nice . */ <nl> retry : <nl> lock_ptr = q -> lock_ptr ; <nl> + barrier (); <nl> if ( lock_ptr != 0 ) { <nl> spin_lock ( lock_ptr ); <nl> /*
more : <nl> goto out_unlock ; <nl> } <nl> if (! d_unhashed ( dentry ) && dentry -> d_inode && <nl> + ceph_snap ( dentry -> d_inode ) != CEPH_SNAPDIR && <nl> filp -> f_pos <= di -> offset ) <nl> break ; <nl> dout (" skipping % p %.* s at % llu (% llu )% s % s \ n ", dentry ,
int wusbhc_chid_set ( struct wusbhc * wusbhc , const struct wusb_ckhdid * chid ) <nl> { <nl> int result = 0 ; <nl>  <nl> - if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof ( chid )) == 0 ) <nl> + if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof (* chid )) == 0 ) <nl> chid = NULL ; <nl>  <nl> mutex_lock (& wusbhc -> mutex );
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) { <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) { <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& priv -> lock , flags );
static unsigned int br_nf_post_routing ( unsigned int hook , struct sk_buff * skb , <nl> if (! nf_bridge ) <nl> return NF_ACCEPT ; <nl>  <nl> + if (!( nf_bridge -> mask & ( BRNF_BRIDGED | BRNF_BRIDGED_DNAT ))) <nl> + return NF_ACCEPT ; <nl> + <nl> if (! realoutdev ) <nl> return NF_DROP ; <nl> 
void __init ehv_pic_init ( void ) <nl>  <nl> if (! ehv_pic -> irqhost ) { <nl> of_node_put ( np ); <nl> + kfree ( ehv_pic ); <nl> return ; <nl> } <nl> 
static inline int gred_change_vq ( struct Qdisc * sch , int dp , <nl> struct gred_sched_data * q ; <nl>  <nl> if ( table -> tab [ dp ] == NULL ) { <nl> - table -> tab [ dp ] = kzalloc ( sizeof (* q ), GFP_KERNEL ); <nl> + table -> tab [ dp ] = kzalloc ( sizeof (* q ), GFP_ATOMIC ); <nl> if ( table -> tab [ dp ] == NULL ) <nl> return - ENOMEM ; <nl> }
struct drm_mm_node * drm_mm_search_free_in_range ( const struct drm_mm * mm , <nl> wasted += alignment - tmp ; <nl> } <nl>  <nl> - if ( entry -> size >= size + wasted ) { <nl> + if ( entry -> size >= size + wasted && <nl> + ( entry -> start + wasted + size ) <= end ) { <nl> if (! best_match ) <nl> return entry ; <nl> if ( entry -> size < best_size ) {
static struct net_device * setup_pre_routing ( struct sk_buff * skb ) <nl> else if ( skb -> protocol == htons ( ETH_P_PPP_SES )) <nl> nf_bridge -> mask |= BRNF_PPPoE ; <nl>  <nl> + /* Must drop socket now because of tproxy . */ <nl> + skb_orphan ( skb ); <nl> return skb -> dev ; <nl> } <nl> 
static int usbduxsub_probe ( struct usb_interface * uinterf , <nl> usbduxsub [ index ]. dux_commands = kzalloc ( SIZEOFDUXBUFFER , GFP_KERNEL ); <nl> if (! usbduxsub [ index ]. dux_commands ) { <nl> dev_err ( dev , " comedi_ : usbdux : " <nl> - " error alloc space for dac commands \ n "); <nl> + " error alloc space for dux commands \ n "); <nl> tidy_up (&( usbduxsub [ index ])); <nl> up (& start_stop_sem ); <nl> return - ENOMEM ;
uint brcms_reset ( struct brcms_info * wl ) <nl> /* dpc will not be rescheduled */ <nl> wl -> resched = false ; <nl>  <nl> + /* inform publicly that interface is down */ <nl> + wl -> pub -> up = false ; <nl> + <nl> return 0 ; <nl> } <nl> 
lnet_ping ( lnet_process_id_t id , int timeout_ms , lnet_process_id_t * ids , int n_i <nl>  <nl> rc = - EFAULT ; /* If I SEGV ... */ <nl>  <nl> + memset (& tmpid , 0 , sizeof ( tmpid )); <nl> for ( i = 0 ; i < n_ids ; i ++) { <nl> tmpid . pid = info -> pi_pid ; <nl> tmpid . nid = info -> pi_ni [ i ]. ns_nid ;
static struct vmap_area * alloc_vmap_area ( unsigned long size , <nl>  <nl> BUG_ON ( size & ~ PAGE_MASK ); <nl>  <nl> - addr = ALIGN ( vstart , align ); <nl> - <nl> va = kmalloc_node ( sizeof ( struct vmap_area ), <nl> gfp_mask & GFP_RECLAIM_MASK , node ); <nl> if ( unlikely (! va )) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> retry : <nl> + addr = ALIGN ( vstart , align ); <nl> + <nl> spin_lock (& vmap_area_lock ); <nl> /* XXX : could have a last_hole cache */ <nl> n = vmap_area_root . rb_node ;
int radeon_vm_bo_update_pte ( struct radeon_device * rdev , <nl> return - ENOMEM ; <nl>  <nl> r = radeon_ib_get ( rdev , R600_RING_TYPE_DMA_INDEX , & ib , NULL , ndw * 4 ); <nl> + if ( r ) <nl> + return r ; <nl> ib . length_dw = 0 ; <nl>  <nl> r = radeon_vm_update_pdes ( rdev , vm , & ib , bo_va -> soffset , bo_va -> eoffset );
rcu_torture_init ( void ) <nl> writer_task = NULL ; <nl> goto unwind ; <nl> } <nl> - reader_tasks = kmalloc ( nrealreaders * sizeof ( reader_tasks [ 0 ]), <nl> + reader_tasks = kzalloc ( nrealreaders * sizeof ( reader_tasks [ 0 ]), <nl> GFP_KERNEL ); <nl> if ( reader_tasks == NULL ) { <nl> VERBOSE_PRINTK_ERRSTRING (" out of memory ");
int wm8350_device_init ( struct wm8350 * wm8350 , int irq , <nl> return ret ; <nl> } <nl>  <nl> - if ( pdata -> init ) { <nl> + if ( pdata && pdata -> init ) { <nl> ret = pdata -> init ( wm8350 ); <nl> if ( ret != 0 ) { <nl> dev_err ( wm8350 -> dev , " Platform init () failed : % d \ n ",
static inline int fls64 ( unsigned long x ) <nl> { <nl> unsigned long t , a , r ; <nl>  <nl> - t = __kernel_cmpbge ( x , 0x0101010101010101 ); <nl> + t = __kernel_cmpbge ( x , 0x0101010101010101UL ); <nl> a = __flsm1_tab [ t ]; <nl> t = __kernel_extbl ( x , a ); <nl> r = a * 8 + __flsm1_tab [ t ] + ( x != 0 );
intel_fill_fb_ggtt_view ( struct i915_ggtt_view * view , struct drm_framebuffer * fb , <nl> tile_size = intel_tile_size ( dev_priv ); <nl>  <nl> cpp = drm_format_plane_cpp ( fb -> pixel_format , 0 ); <nl> - tile_width = intel_tile_width ( dev_priv , cpp , fb -> modifier [ 0 ]); <nl> + tile_width = intel_tile_width ( dev_priv , fb -> modifier [ 0 ], cpp ); <nl> tile_height = tile_size / tile_width ; <nl>  <nl> info -> width_pages = DIV_ROUND_UP ( fb -> pitches [ 0 ], tile_width );
static void __init read_obp_translations ( void ) <nl> for ( i = 0 ; i < prom_trans_ents ; i ++) <nl> prom_trans [ i ]. data &= ~ 0x0003fe0000000000UL ; <nl> } <nl> + <nl> + /* Force execute bit on . */ <nl> + for ( i = 0 ; i < prom_trans_ents ; i ++) <nl> + prom_trans [ i ]. data |= ( tlb_type == hypervisor ? <nl> + _PAGE_EXEC_4V : _PAGE_EXEC_4U ); <nl> } <nl>  <nl> static void __init hypervisor_tlb_lock ( unsigned long vaddr ,
int sst_block_alloc_scratch ( struct sst_dsp * dsp ) <nl> ret = block_list_prepare ( dsp , & dsp -> scratch_block_list ); <nl> if ( ret < 0 ) { <nl> dev_err ( dsp -> dev , " error : scratch block prepare failed \ n "); <nl> + mutex_unlock (& dsp -> mutex ); <nl> return ret ; <nl> } <nl> 
bfa_ioc_mbox_isr ( struct bfa_ioc_s * ioc ) <nl> return ; <nl> } <nl>  <nl> - if (( mc > BFI_MC_MAX ) || ( mod -> mbhdlr [ mc ]. cbfn == NULL )) <nl> + if (( mc >= BFI_MC_MAX ) || ( mod -> mbhdlr [ mc ]. cbfn == NULL )) <nl> return ; <nl>  <nl> mod -> mbhdlr [ mc ]. cbfn ( mod -> mbhdlr [ mc ]. cbarg , & m );
bool ROUTEbRelay ( PSDevice pDevice , unsigned char * pbySkbData , unsigned int uData <nl> pHeadTD = pHeadTD -> next ; <nl> } <nl>  <nl> - pLastTD -> pTDInfo -> skb = 0 ; <nl> + pLastTD -> pTDInfo -> skb = NULL ; <nl> pLastTD -> pTDInfo -> byFlags = 0 ; <nl>  <nl> pDevice -> apCurrTD [ TYPE_AC0DMA ] = pHeadTD ;
static irqreturn_t pcf8563_irq ( int irq , void * dev_id ) <nl>  <nl> err = pcf8563_get_alarm_mode ( pcf8563 -> client , NULL , & pending ); <nl> if ( err ) <nl> - return err ; <nl> + return IRQ_NONE ; <nl>  <nl> if ( pending ) { <nl> rtc_update_irq ( pcf8563 -> rtc , 1 , RTC_IRQF | RTC_AF );
unlock : <nl>  <nl> for ( skb = segs ; skb ; skb = skb -> next ) { <nl> ipv6h = skb -> nh . ipv6h ; <nl> - ipv6h -> payload_len = htons ( skb -> len - skb -> mac_len ); <nl> + ipv6h -> payload_len = htons ( skb -> len - skb -> mac_len - <nl> + sizeof (* ipv6h )); <nl> } <nl>  <nl> out :
static int pm2fb_check_var ( struct fb_var_screeninfo * var , struct fb_info * info ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + var -> transp . offset = 0 ; <nl> + var -> transp . length = 0 ; <nl> switch ( var -> bits_per_pixel ) { <nl> case 8 : <nl> var -> red . length = var -> green . length = var -> blue . length = 8 ;
int iwl_power_update_mode ( struct iwl_priv * priv , bool force ) <nl> if ( priv -> cfg -> ops -> lib -> update_chain_flags && <nl> update_chains ) <nl> priv -> cfg -> ops -> lib -> update_chain_flags ( priv ); <nl> - else <nl> + else if ( priv -> cfg -> ops -> lib -> update_chain_flags ) <nl> IWL_DEBUG_POWER ( priv , <nl> " Cannot update the power , chain noise " <nl> " calibration running : % d \ n ",
static unsigned int bsg_poll ( struct file * file , poll_table * wait ) <nl> spin_lock_irq (& bd -> lock ); <nl> if (! list_empty (& bd -> done_list )) <nl> mask |= POLLIN | POLLRDNORM ; <nl> - if ( bd -> queued_cmds >= bd -> max_queue ) <nl> + if ( bd -> queued_cmds < bd -> max_queue ) <nl> mask |= POLLOUT ; <nl> spin_unlock_irq (& bd -> lock ); <nl> 
ia64_fault ( unsigned long vector , unsigned long isr , unsigned long ifa , <nl> printk ( KERN_ERR " iip - 0x % lx , ifa - 0x % lx , isr - 0x % lx \ n ", <nl> iip , ifa , isr ); <nl> force_sig ( SIGSEGV , current ); <nl> - break ; <nl> + return ; <nl>  <nl> case 46 : <nl> printk ( KERN_ERR " Unexpected IA - 32 intercept trap ( Trap 46 )\ n ");
 <nl> # define BOUNCE_SIZE ( 64 * 1024 ) <nl>  <nl> -# define PS3ROM_MAX_SECTORS ( BOUNCE_SIZE / CD_FRAMESIZE ) <nl> +# define PS3ROM_MAX_SECTORS ( BOUNCE_SIZE >> 9 ) <nl>  <nl>  <nl> struct ps3rom_private {
device_receive_frame ( <nl> } <nl>  <nl> ev . src_addr . sa_family = ARPHRD_ETHER ; <nl> - memcpy ( ev . src_addr . sa_data , pMACHeader -> abyAddr2 , ETH_ALEN ); <nl> + ether_addr_copy ( ev . src_addr . sa_data , <nl> + pMACHeader -> abyAddr2 ); <nl> memset (& wrqu , 0 , sizeof ( wrqu )); <nl> wrqu . data . length = sizeof ( ev ); <nl> wireless_send_event ( pDevice -> dev , IWEVMICHAELMICFAILURE , & wrqu , ( char *)& ev );
static int emac_dev_open ( struct net_device * ndev ) <nl> struct emac_priv * priv = netdev_priv ( ndev ); <nl>  <nl> netif_carrier_off ( ndev ); <nl> - for ( cnt = 0 ; cnt <= ETH_ALEN ; cnt ++) <nl> + for ( cnt = 0 ; cnt < ETH_ALEN ; cnt ++) <nl> ndev -> dev_addr [ cnt ] = priv -> mac_addr [ cnt ]; <nl>  <nl> /* Configuration items */
struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
static int cy_put_char ( struct tty_struct * tty , unsigned char ch ) <nl> return 0 ; <nl>  <nl> if (! info -> xmit_buf ) <nl> - return ; <nl> + return 0 ; <nl>  <nl> local_irq_save ( flags ); <nl> if ( info -> xmit_cnt >= PAGE_SIZE - 1 ) {
int mdiobus_register ( struct mii_bus * bus ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> + put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
*/ <nl> typedef union <nl> { <nl> - __u32 a4 ; <nl> - __u32 a6 [ 4 ]; <nl> + __be32 a4 ; <nl> + __be32 a6 [ 4 ]; <nl> } xfrm_address_t ; <nl>  <nl> /* Ident of a specific xfrm_state . It is used on input to lookup
c4_add_dev ( hdw_info_t * hi , int brdno , unsigned long f0 , unsigned long f1 , <nl> hi -> devname , irq1 ); <nl> unregister_netdev ( ndev ); <nl> free_irq ( irq0 , ndev ); <nl> - OS_kfree ( ndev -> priv ); <nl> + OS_kfree ( netdev_priv ( ndev )); <nl> OS_kfree ( ndev ); <nl> error_flag = EIO ; <nl> return 0 ;
# define CR8_RESERVED_BITS (~( unsigned long ) X86_CR8_TPR ) <nl>  <nl> # define KVM_MAX_MCE_BANKS 32 <nl> -# define KVM_MCE_CAP_SUPPORTED MCG_CTL_P <nl> +# define KVM_MCE_CAP_SUPPORTED ( MCG_CTL_P | MCG_SER_P ) <nl>  <nl> /* EFER defaults : <nl> * - enable syscall per default because its emulated by KVM
static inline long snd_ctl_ioctl_compat ( struct file * file , unsigned int cmd , uns <nl> case SNDRV_CTL_IOCTL_POWER_STATE : <nl> case SNDRV_CTL_IOCTL_ELEM_LOCK : <nl> case SNDRV_CTL_IOCTL_ELEM_UNLOCK : <nl> + case SNDRV_CTL_IOCTL_ELEM_REMOVE : <nl> + case SNDRV_CTL_IOCTL_TLV_READ : <nl> + case SNDRV_CTL_IOCTL_TLV_WRITE : <nl> + case SNDRV_CTL_IOCTL_TLV_COMMAND : <nl> return snd_ctl_ioctl ( file , cmd , ( unsigned long ) argp ); <nl> case SNDRV_CTL_IOCTL_ELEM_LIST32 : <nl> return snd_ctl_elem_list_compat ( ctl -> card , argp );
static ssize_t enable_store ( <nl> struct timed_output_dev * tdev = dev_get_drvdata ( dev ); <nl> int value ; <nl>  <nl> - sscanf ( buf , "% d ", & value ); <nl> + if ( sscanf ( buf , "% d ", & value ) != 1 ) <nl> + return - EINVAL ; <nl> + <nl> tdev -> enable ( tdev , value ); <nl>  <nl> return size ;
static void rtl8180_beacon_work ( struct work_struct * work ) <nl>  <nl> /* grab a fresh beacon */ <nl> skb = ieee80211_beacon_get ( dev , vif ); <nl> + if (! skb ) <nl> + goto resched ; <nl>  <nl> /* <nl> * update beacon timestamp w / TSF value
vhost_scsi_handle_vq ( struct vhost_scsi * vs , struct vhost_virtqueue * vq ) <nl> break ; <nl> } <nl>  <nl> + /* virtio - scsi spec requires byte 0 of the lun to be 1 */ <nl> + if ( unlikely ( v_req . lun [ 0 ] != 1 )) { <nl> + vhost_scsi_send_bad_target ( vs , vq , head , out ); <nl> + continue ; <nl> + } <nl> + <nl> /* Extract the tpgt */ <nl> target = v_req . lun [ 1 ]; <nl> tpg = ACCESS_ONCE ( vs_tpg [ target ]);
u64 perf_evsel__intval ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> value = *( u32 *) ptr ; <nl> break ; <nl> case 8 : <nl> - value = *( u64 *) ptr ; <nl> + memcpy (& value , ptr , sizeof ( u64 )); <nl> break ; <nl> default : <nl> return 0 ;
MODULE_DEVICE_TABLE ( pnp , smsc_ircc_pnp_table ); <nl> static int pnp_driver_registered ; <nl>  <nl> # ifdef CONFIG_PNP <nl> - static int __init smsc_ircc_pnp_probe ( struct pnp_dev * dev , <nl> + static int __devinit smsc_ircc_pnp_probe ( struct pnp_dev * dev , <nl> const struct pnp_device_id * dev_id ) <nl> { <nl> unsigned int firbase , sirbase ;
static int exynos_dsi_parse_dt ( struct exynos_dsi * dsi ) <nl>  <nl> ep = of_graph_get_next_endpoint ( node , NULL ); <nl> if (! ep ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl>  <nl> dsi -> bridge_node = of_graph_get_remote_port_parent ( ep ); <nl> if (! dsi -> bridge_node ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl> end :
do_last : <nl> goto exit ; <nl> } <nl>  <nl> + if ( IS_ERR ( nd -> intent . open . file )) { <nl> + mutex_unlock (& dir -> d_inode -> i_mutex ); <nl> + error = PTR_ERR ( nd -> intent . open . file ); <nl> + goto exit_dput ; <nl> + } <nl> + <nl> /* Negative dentry , just create the file */ <nl> if (! path . dentry -> d_inode ) { <nl> if (! IS_POSIXACL ( dir -> d_inode ))
int iwl_mvm_scan_request ( struct iwl_mvm * mvm , <nl> else <nl> cmd -> type = cpu_to_le32 ( SCAN_TYPE_FORCED ); <nl>  <nl> + /* <nl> + * TODO : This is a WA due to a bug in the FW AUX framework that does not <nl> + * properly handle time events that fail to be scheduled <nl> + */ <nl> + cmd -> type = cpu_to_le32 ( SCAN_TYPE_FORCED ); <nl> + <nl> cmd -> repeats = cpu_to_le32 ( 1 ); <nl>  <nl> /*
int mmc_send_if_cond ( struct mmc_host * host , u32 ocr ) <nl> static const u8 test_pattern = 0xAA ; <nl> u8 result_pattern ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( struct mmc_command )); <nl> + <nl> /* <nl> * To support SD 2 . 0 cards , we must always invoke SD_SEND_IF_COND <nl> * before SD_APP_OP_COND . This command will harmlessly fail for
static int sep_construct_dma_tables_from_lli ( <nl> table_data_size ); <nl>  <nl> /* If info entry is null - this is the first table built */ <nl> - if ( info_in_entry_ptr == NULL ) { <nl> + if ( info_in_entry_ptr == NULL || info_out_entry_ptr == NULL ) { <nl> /* Set the output parameters to physical addresses */ <nl> * lli_table_in_ptr = <nl> sep_shared_area_virt_to_bus ( sep , dma_in_lli_table_ptr );
u64 gfs2_ri_total ( struct gfs2_sbd * sdp ) <nl> for ( rgrps = 0 ;; rgrps ++) { <nl> loff_t pos = rgrps * sizeof ( struct gfs2_rindex ); <nl>  <nl> - if ( pos + sizeof ( struct gfs2_rindex ) >= i_size_read ( inode )) <nl> + if ( pos + sizeof ( struct gfs2_rindex ) > i_size_read ( inode )) <nl> break ; <nl> error = gfs2_internal_read ( ip , & ra_state , buf , & pos , <nl> sizeof ( struct gfs2_rindex ));
static void picolcd_debug_out_report ( struct picolcd_data * data , <nl> report -> id , raw_size ); <nl> hid_debug_event ( hdev , buff ); <nl> if ( raw_size + 5 > sizeof ( raw_data )) { <nl> + kfree ( buff ); <nl> hid_debug_event ( hdev , " TOO BIG \ n "); <nl> return ; <nl> } else {
pid_t pid_vnr ( struct pid * pid ); <nl> hlist_for_each_entry_rcu (( task ), pos___ , \ <nl> & pid -> tasks [ type ], pids [ type ]. node ) { <nl>  <nl> + /* <nl> + * Both old and new leaders may be attached to <nl> + * the same pid in the middle of de_thread (). <nl> + */ <nl> # define while_each_pid_task ( pid , type , task ) \ <nl> + if ( type == PIDTYPE_PID ) \ <nl> + break ; \ <nl> } \ <nl> } while ( 0 ) <nl> 
i830_dispatch_execbuffer ( struct intel_engine_cs * ring , <nl> */ <nl> intel_ring_emit ( ring , SRC_COPY_BLT_CMD | BLT_WRITE_RGBA ); <nl> intel_ring_emit ( ring , BLT_DEPTH_32 | BLT_ROP_SRC_COPY | 4096 ); <nl> - intel_ring_emit ( ring , DIV_ROUND_UP ( len , 4096 ) << 16 | 1024 ); <nl> + intel_ring_emit ( ring , DIV_ROUND_UP ( len , 4096 ) << 16 | 4096 ); <nl> intel_ring_emit ( ring , cs_offset ); <nl> intel_ring_emit ( ring , 4096 ); <nl> intel_ring_emit ( ring , offset );
bool __init early_can_reuse_p2m_middle ( unsigned long set_pfn , unsigned long set_ <nl> if ( p2m_index ( set_pfn )) <nl> return false ; <nl>  <nl> - for ( pfn = 0 ; pfn <= MAX_DOMAIN_PAGES ; pfn += P2M_PER_PAGE ) { <nl> + for ( pfn = 0 ; pfn < MAX_DOMAIN_PAGES ; pfn += P2M_PER_PAGE ) { <nl> topidx = p2m_top_index ( pfn ); <nl>  <nl> if (! p2m_top [ topidx ])
retry : <nl> printk (" locked it .\ n "); <nl>  <nl> do_each_thread ( g , p ) { <nl> + /* <nl> + * It ' s not reliable to print a task ' s held locks <nl> + * if it ' s not sleeping ( or if it ' s not the current <nl> + * task ): <nl> + */ <nl> + if ( p -> state == TASK_RUNNING && p != current ) <nl> + continue ; <nl> if ( p -> lockdep_depth ) <nl> lockdep_print_held_locks ( p ); <nl> if (! unlock )
static int hih6130_probe ( struct i2c_client * client , <nl> hih6130 -> client = client ; <nl> mutex_init (& hih6130 -> lock ); <nl>  <nl> + if (! i2c_check_functionality ( client -> adapter , I2C_FUNC_SMBUS_QUICK )) <nl> + hih6130 -> write_length = 1 ; <nl> + <nl> hwmon_dev = devm_hwmon_device_register_with_groups ( dev , client -> name , <nl> hih6130 , <nl> hih6130_groups );
tvp514x_probe ( struct i2c_client * client , const struct i2c_device_id * id ) <nl> if ( ret < 0 ) { <nl> v4l2_err ( sd , "% s decoder driver failed to register !!\ n ", <nl> sd -> name ); <nl> - kfree ( decoder ); <nl> return ret ; <nl> } <nl> # endif
static int sbus_do_settimeofday ( struct timespec * tv ) <nl> static int set_rtc_mmss ( unsigned long secs ) <nl> { <nl> struct rtc_device * rtc = rtc_class_open (" rtc0 "); <nl> + int err = - 1 ; <nl>  <nl> - if ( rtc ) <nl> - return rtc_set_mmss ( rtc , secs ); <nl> + if ( rtc ) { <nl> + err = rtc_set_mmss ( rtc , secs ); <nl> + rtc_class_close ( rtc ); <nl> + } <nl>  <nl> - return - 1 ; <nl> + return err ; <nl> }
module_i2c_driver ( si2157_driver ); <nl> MODULE_DESCRIPTION (" Silicon Labs Si2157 / Si2158 silicon tuner driver "); <nl> MODULE_AUTHOR (" Antti Palosaari < crope @ iki . fi >"); <nl> MODULE_LICENSE (" GPL "); <nl> + MODULE_FIRMWARE ( SI2158_A20_FIRMWARE );
offset_store ( struct md_rdev * rdev , const char * buf , size_t len ) <nl> * can be sane */ <nl> return - EBUSY ; <nl> rdev -> data_offset = offset ; <nl> + rdev -> new_data_offset = offset ; <nl> return len ; <nl> } <nl> 
found : <nl>  <nl> static int openpromfs_readdir ( struct file * filp , void * dirent , filldir_t filldir ) <nl> { <nl> - struct inode * inode = filp -> f_dentry -> d_inode ; <nl> + struct inode * inode = filp -> f_path . dentry -> d_inode ; <nl> struct op_inode_info * oi = OP_I ( inode ); <nl> struct device_node * dp = oi -> u . node ; <nl> struct device_node * child ;
static inline struct device_node * nand_get_flash_node ( struct nand_chip * chip ) <nl>  <nl> static inline struct nand_chip * mtd_to_nand ( struct mtd_info * mtd ) <nl> { <nl> - return mtd -> priv ; <nl> + return container_of ( mtd , struct nand_chip , mtd ); <nl> } <nl>  <nl> static inline struct mtd_info * nand_to_mtd ( struct nand_chip * chip )
static int bcm2835_dma_terminate_all ( struct dma_chan * chan ) <nl> * c -> desc is NULL and exit .) <nl> */ <nl> if ( c -> desc ) { <nl> + bcm2835_dma_desc_free (& c -> desc -> vd ); <nl> c -> desc = NULL ; <nl> bcm2835_dma_abort ( c -> chan_base ); <nl> 
bool ai_deviceremoved ( struct si_pub * sih ) <nl>  <nl> sii = ( struct si_info *) sih ; <nl>  <nl> + if ( sii -> icbus -> hosttype != BCMA_HOSTTYPE_PCI ) <nl> + return false ; <nl> + <nl> pci_read_config_dword ( sii -> pcibus , PCI_VENDOR_ID , & w ); <nl> if (( w & 0xFFFF ) != PCI_VENDOR_ID_BROADCOM ) <nl> return true ;
static struct file * do_last ( struct nameidata * nd , struct path * path , <nl> if ( error ) <nl> return ERR_PTR ( error ); <nl> error = - EISDIR ; <nl> - if ( S_ISDIR ( nd -> inode -> i_mode )) <nl> + if (( open_flag & O_CREAT ) && S_ISDIR ( nd -> inode -> i_mode )) <nl> goto exit ; <nl> error = - ENOTDIR ; <nl> if (( nd -> flags & LOOKUP_DIRECTORY ) && ! nd -> inode -> i_op -> lookup )
static int tun_chr_open ( struct inode * inode , struct file * file ) <nl> set_bit ( SOCK_EXTERNALLY_ALLOCATED , & tfile -> socket . flags ); <nl> INIT_LIST_HEAD (& tfile -> next ); <nl>  <nl> + sock_set_flag (& tfile -> sk , SOCK_ZEROCOPY ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int setup ( struct spi_device * spi ) <nl> if (( chip -> chip_select_num > 0 ) <nl> && ( chip -> chip_select_num <= spi -> master -> num_chipselect )) <nl> peripheral_request ( ssel [ spi -> master -> bus_num ] <nl> - [ chip -> chip_select_num - 1 ], DRV_NAME ); <nl> + [ chip -> chip_select_num - 1 ], spi -> modalias ); <nl>  <nl> cs_deactive ( drv_data , chip ); <nl> 
void scsi_eh_prep_cmnd ( struct scsi_cmnd * scmd , struct scsi_eh_save * ses , <nl> ses -> prot_op = scmd -> prot_op ; <nl>  <nl> scmd -> prot_op = SCSI_PROT_NORMAL ; <nl> + scmd -> eh_eflags = 0 ; <nl> scmd -> cmnd = ses -> eh_cmnd ; <nl> memset ( scmd -> cmnd , 0 , BLK_MAX_CDB ); <nl> memset (& scmd -> sdb , 0 , sizeof ( scmd -> sdb ));
EXPORT_SYMBOL ( genphy_config_advert ); <nl> */ <nl> int genphy_setup_forced ( struct phy_device * phydev ) <nl> { <nl> - int ctl = BMCR_RESET ; <nl> + int ctl = 0 ; <nl>  <nl> phydev -> pause = phydev -> asym_pause = 0 ; <nl> 
void uf_send_pkt_to_encrypt ( struct work_struct * work ) <nl>  <nl> if ( pktBulkDataLength > 0 ) { <nl> pktBulkData = kmalloc ( pktBulkDataLength , GFP_KERNEL ); <nl> - memset ( pktBulkData , 0 , pktBulkDataLength ); <nl> } else { <nl> unifi_error ( priv , " uf_send_pkt_to_encrypt () : invalid buffer \ n "); <nl> return ;
int rtw_tkip_encrypt23a ( struct rtw_adapter * padapter , <nl> arcfour_encrypt (& mycontext , payload , payload , length ); <nl> arcfour_encrypt (& mycontext , payload + length , crc , 4 ); <nl>  <nl> - pframe += pxmitpriv -> frag_len ; <nl> - pframe = PTR_ALIGN ( pframe , 4 ); <nl> + pframe += pxmitpriv -> frag_len ; <nl> + pframe = PTR_ALIGN ( pframe , 4 ); <nl> } <nl> } <nl> 
void __init qe_ic_init ( struct device_node * node , unsigned int flags ) <nl> return ; <nl>  <nl> memset ( qe_ic , 0 , sizeof ( struct qe_ic )); <nl> - qe_ic -> of_node = node ? of_node_get ( node ) : NULL ; <nl> + qe_ic -> of_node = of_node_get ( node ); <nl>  <nl> qe_ic -> irqhost = irq_alloc_host ( IRQ_HOST_MAP_LINEAR , <nl> NR_QE_IC_INTS , & qe_ic_host_ops , 0 );
static void disable_lapic_nmi_watchdog ( void ) <nl> wrmsr ( MSR_P6_EVNTSEL0 , 0 , 0 ); <nl> break ; <nl> case 15 : <nl> - if ( boot_cpu_data . x86_model > 0x3 ) <nl> + if ( boot_cpu_data . x86_model > 0x4 ) <nl> break ; <nl>  <nl> wrmsr ( MSR_P4_IQ_CCCR0 , 0 , 0 ); <nl> void setup_apic_nmi_watchdog ( void ) <nl> setup_p6_watchdog (); <nl> break ; <nl> case 15 : <nl> - if ( boot_cpu_data . x86_model > 0x3 ) <nl> + if ( boot_cpu_data . x86_model > 0x4 ) <nl> return ; <nl>  <nl> if (! setup_p4_watchdog ())
 <nl> # define DRV_MODULE_NAME " bnx2 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 1 . 6 . 7 " <nl> -# define DRV_MODULE_RELDATE " October 10 , 2007 " <nl> +# define DRV_MODULE_VERSION " 1 . 6 . 8 " <nl> +# define DRV_MODULE_RELDATE " October 17 , 2007 " <nl>  <nl> # define RUN_AT ( x ) ( jiffies + ( x )) <nl> 
xfs_qm_reset_dqcounts ( <nl> */ <nl> xfs_dqcheck ( mp , ddq , id + j , type , XFS_QMOPT_DQREPAIR , <nl> " xfs_quotacheck "); <nl> + /* <nl> + * Reset type in case we are reusing group quota file for <nl> + * project quotas or vice versa <nl> + */ <nl> + ddq -> d_flags = type ; <nl> ddq -> d_bcount = 0 ; <nl> ddq -> d_icount = 0 ; <nl> ddq -> d_rtbcount = 0 ;
static void kfd_process_destroy_delayed ( struct rcu_head * rcu ) <nl> mmdrop ( p -> mm ); <nl>  <nl> work = ( struct kfd_process_release_work *) <nl> - kmalloc ( sizeof ( struct kfd_process_release_work ), GFP_KERNEL ); <nl> + kmalloc ( sizeof ( struct kfd_process_release_work ), GFP_ATOMIC ); <nl>  <nl> if ( work ) { <nl> INIT_WORK (( struct work_struct *) work , kfd_process_wq_release );
static int stmmac_init_phy ( struct net_device * dev ) <nl> interface ); <nl> } <nl>  <nl> - if ( IS_ERR ( phydev )) { <nl> + if ( IS_ERR_OR_NULL ( phydev )) { <nl> pr_err ("% s : Could not attach to PHY \ n ", dev -> name ); <nl> + if (! phydev ) <nl> + return - ENODEV ; <nl> + <nl> return PTR_ERR ( phydev ); <nl> } <nl> 
static void btrfs_submit_direct ( int rw , struct bio * bio , struct inode * inode , <nl> if (! skip_sum ) { <nl> dip -> csums = kmalloc ( sizeof ( u32 ) * bio -> bi_vcnt , GFP_NOFS ); <nl> if (! dip -> csums ) { <nl> + kfree ( dip ); <nl> ret = - ENOMEM ; <nl> goto free_ordered ; <nl> }
int drm_resctx ( struct inode * inode , struct file * filp , <nl> for ( i = 0 ; i < DRM_RESERVED_CONTEXTS ; i ++ ) { <nl> ctx . handle = i ; <nl> if ( copy_to_user ( & res . contexts [ i ], <nl> - & i , sizeof ( i ) ) ) <nl> + & ctx , sizeof ( ctx ) ) ) <nl> return - EFAULT ; <nl> } <nl> }
static struct snd_soc_codec_driver soc_codec_dev_wm8962 = { <nl> . remove = wm8962_remove , <nl> . resume = wm8962_resume , <nl> . set_bias_level = wm8962_set_bias_level , <nl> - . reg_cache_size = WM8962_MAX_REGISTER , <nl> + . reg_cache_size = WM8962_MAX_REGISTER + 1 , <nl> . reg_word_size = sizeof ( u16 ), <nl> . reg_cache_default = wm8962_reg , <nl> . volatile_register = wm8962_volatile_register ,
mv64xxx_of_config ( struct mv64xxx_i2c_data * drv_data , <nl> } <nl> tclk = clk_get_rate ( drv_data -> clk ); <nl>  <nl> - rc = of_property_read_u32 ( np , " clock - frequency ", & bus_freq ); <nl> - if ( rc ) <nl> + if ( of_property_read_u32 ( np , " clock - frequency ", & bus_freq )) <nl> bus_freq = 100000 ; /* 100kHz by default */ <nl>  <nl> if (! mv64xxx_find_baud_factors ( bus_freq , tclk ,
static int em_gio_probe ( struct platform_device * pdev ) <nl> gpio_chip -> request = em_gio_request ; <nl> gpio_chip -> free = em_gio_free ; <nl> gpio_chip -> label = name ; <nl> + gpio_chip -> dev = & pdev -> dev ; <nl> gpio_chip -> owner = THIS_MODULE ; <nl> gpio_chip -> base = pdata -> gpio_base ; <nl> gpio_chip -> ngpio = pdata -> number_of_pins ;
extern void integrator_secondary_startup ( void ); <nl> * control for which core is the next to come out of the secondary <nl> * boot " holding pen " <nl> */ <nl> - volatile int __initdata pen_release = - 1 ; <nl> - unsigned long __initdata phys_pen_release = 0 ; <nl> + volatile int __cpuinitdata pen_release = - 1 ; <nl> + unsigned long __cpuinitdata phys_pen_release = 0 ; <nl>  <nl> static DEFINE_SPINLOCK ( boot_lock ); <nl> 
# include " myri10ge_mcp . h " <nl> # include " myri10ge_mcp_gen_header . h " <nl>  <nl> -# define MYRI10GE_VERSION_STR " 1 . 0 . 0 " <nl> +# define MYRI10GE_VERSION_STR " 1 . 1 . 0 " <nl>  <nl> MODULE_DESCRIPTION (" Myricom 10G driver ( 10GbE )"); <nl> MODULE_AUTHOR (" Maintainer : help @ myri . com ");
void read_persistent_clock ( struct timespec * ts ) <nl> year += 100 ; <nl>  <nl> ts -> tv_sec = mktime ( year , mon , day , hour , min , sec ); <nl> + ts -> tv_nsec = 0 ; <nl> } <nl>  <nl> 
static int ams_delta_led_remove ( struct platform_device * pdev ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < ARRAY_SIZE ( ams_delta_leds ); i --) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( ams_delta_leds ); i ++) <nl> led_classdev_unregister (& ams_delta_leds [ i ]. cdev ); <nl>  <nl> return 0 ;
static void mt_feature_mapping ( struct hid_device * hdev , <nl> td -> is_buttonpad = true ; <nl>  <nl> break ; <nl> + case 0xff0000c5 : <nl> + /* Retrieve the Win8 blob once to enable some devices */ <nl> + if ( usage -> usage_index == 0 ) <nl> + mt_get_feature ( hdev , field -> report ); <nl> + break ; <nl> } <nl> } <nl> 
cifs_ucs2_bytes ( const __le16 * from , int maxbytes , <nl> int maxwords = maxbytes / 2 ; <nl> char tmp [ NLS_MAX_CHARSET_SIZE ]; <nl>  <nl> - for ( i = 0 ; from [ i ] && i < maxwords ; i ++) { <nl> + for ( i = 0 ; i < maxwords && from [ i ]; i ++) { <nl> charlen = codepage -> uni2char ( le16_to_cpu ( from [ i ]), tmp , <nl> NLS_MAX_CHARSET_SIZE ); <nl> if ( charlen > 0 )
int mc13xxx_common_init ( struct mc13xxx * mc13xxx , <nl> err_mask : <nl> err_revision : <nl> mc13xxx_unlock ( mc13xxx ); <nl> - kfree ( mc13xxx ); <nl> return ret ; <nl> } <nl> 
struct unw_frame_info { <nl> struct unw_ireg { <nl> unsigned long * loc ; <nl> struct unw_ireg_nat { <nl> - long type : 3 ; /* enum unw_nat_type */ <nl> + unsigned long type : 3 ; /* enum unw_nat_type */ <nl> signed long off : 61 ; /* NaT word is at loc + nat . off */ <nl> } nat ; <nl> } r4 , r5 , r6 , r7 ;
vxge_hw_device_initialize ( <nl> __vxge_hw_device_pci_e_init ( hldev ); <nl>  <nl> status = __vxge_hw_device_reg_addr_get ( hldev ); <nl> - if ( status != VXGE_HW_OK ) <nl> + if ( status != VXGE_HW_OK ) { <nl> + vfree ( hldev ); <nl> goto exit ; <nl> + } <nl> __vxge_hw_device_id_get ( hldev ); <nl>  <nl> __vxge_hw_device_host_info_get ( hldev );
struct event_format * trace_find_next_event ( struct pevent * pevent , <nl> { <nl> static int idx ; <nl>  <nl> - if (! pevent -> events ) <nl> + if (! pevent || ! pevent -> events ) <nl> return NULL ; <nl>  <nl> if (! event ) {
static int br_multicast_ipv6_rcv ( struct net_bridge * br , <nl> ip6h -> payload_len == 0 ) <nl> return 0 ; <nl>  <nl> - len = ntohs ( ip6h -> payload_len ); <nl> + len = ntohs ( ip6h -> payload_len ) + sizeof (* ip6h ); <nl> if ( skb -> len < len ) <nl> return - EINVAL ; <nl> 
mapping_unwind : <nl> mapping_error : <nl> if ( net_ratelimit ()) <nl> dev_warn (& hw -> pdev -> dev , "% s : tx mapping error \ n ", dev -> name ); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
asmlinkage long sys_ioprio_set ( int which , int who , int ioprio ) <nl> continue ; <nl> ret = set_task_ioprio ( p , ioprio ); <nl> if ( ret ) <nl> - break ; <nl> + goto free_uid ; <nl> } while_each_thread ( g , p ); <nl> - <nl> + free_uid : <nl> if ( who ) <nl> free_uid ( user ); <nl> break ;
static void ar9003_hw_spur_mitigate_ofdm ( struct ath_hw * ah , <nl>  <nl> ar9003_hw_spur_ofdm_clear ( ah ); <nl>  <nl> - for ( i = 0 ; spurChansPtr [ i ] && i < 5 ; i ++) { <nl> + for ( i = 0 ; i < AR_EEPROM_MODAL_SPURS && spurChansPtr [ i ]; i ++) { <nl> freq_offset = FBIN2FREQ ( spurChansPtr [ i ], mode ) - synth_freq ; <nl> if ( abs ( freq_offset ) < range ) { <nl> ar9003_hw_spur_ofdm_work ( ah , chan , freq_offset );
u16 ieee80211_select_queue ( struct ieee80211_sub_if_data * sdata , <nl> return IEEE80211_AC_BE ; <nl> } <nl>  <nl> + if ( skb -> protocol == sdata -> control_port_protocol ) { <nl> + skb -> priority = 7 ; <nl> + return ieee80211_downgrade_queue ( sdata , skb ); <nl> + } <nl> + <nl> /* use the data classifier to determine what 802 . 1d tag the <nl> * data frame has */ <nl> rcu_read_lock ();
static int lmv_iocontrol ( unsigned int cmd , struct obd_export * exp , <nl> __u32 index ; <nl>  <nl> memcpy (& index , data -> ioc_inlbuf2 , sizeof ( __u32 )); <nl> - if (( index >= count )) <nl> + if ( index >= count ) <nl> return - ENODEV ; <nl>  <nl> if ( lmv -> tgts [ index ] == NULL ||
static void kvm_write_wall_clock ( struct kvm * kvm , gpa_t wall_clock ) <nl> */ <nl> getboottime (& boot ); <nl>  <nl> + if ( kvm -> arch . kvmclock_offset ) { <nl> + struct timespec ts = ns_to_timespec ( kvm -> arch . kvmclock_offset ); <nl> + boot = timespec_sub ( boot , ts ); <nl> + } <nl> wc . sec = boot . tv_sec ; <nl> wc . nsec = boot . tv_nsec ; <nl> wc . version = version ;
static void fsg_unbind ( struct usb_configuration * c , struct usb_function * f ) <nl> fsg_common_put ( common ); <nl> usb_free_descriptors ( fsg -> function . descriptors ); <nl> usb_free_descriptors ( fsg -> function . hs_descriptors ); <nl> + usb_free_descriptors ( fsg -> function . ss_descriptors ); <nl> kfree ( fsg ); <nl> } <nl> 
static bool tcp_fastopen_create_child ( struct sock * sk , <nl> struct dst_entry * dst , <nl> struct request_sock * req ) <nl> { <nl> - struct tcp_sock * tp = tcp_sk ( sk ); <nl> + struct tcp_sock * tp ; <nl> struct request_sock_queue * queue = & inet_csk ( sk )-> icsk_accept_queue ; <nl> struct sock * child ; <nl> 
allocate_trace_buffer ( struct trace_array * tr , struct trace_buffer * buf , int size <nl>  <nl> rb_flags = trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0 ; <nl>  <nl> + buf -> tr = tr ; <nl> + <nl> buf -> buffer = ring_buffer_alloc ( size , rb_flags ); <nl> if (! buf -> buffer ) <nl> return - ENOMEM ;
int snd_soc_dapm_set_endpoint ( struct snd_soc_codec * codec , <nl> list_for_each_entry ( w , & codec -> dapm_widgets , list ) { <nl> if (! strcmp ( w -> name , endpoint )) { <nl> w -> connected = status ; <nl> + return 0 ; <nl> } <nl> } <nl>  <nl> - return 0 ; <nl> + return - ENODEV ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_set_endpoint ); <nl> 
static cycle_t sb1250_hpt_read ( void ) <nl> } <nl>  <nl> struct clocksource bcm1250_clocksource = { <nl> - . name = " MIPS ", <nl> + . name = " bcm1250 - counter - 3 ", <nl> . rating = 200 , <nl> . read = sb1250_hpt_read , <nl> . mask = CLOCKSOURCE_MASK ( 23 ),
static int match_session ( struct cifs_ses * ses , struct smb_vol * vol ) <nl> vol -> username ? vol -> username : "", <nl> CIFS_MAX_USERNAME_LEN )) <nl> return 0 ; <nl> - if ( strlen ( vol -> username ) != 0 && <nl> + if (( vol -> username && strlen ( vol -> username ) != 0 ) && <nl> ses -> password != NULL && <nl> strncmp ( ses -> password , <nl> vol -> password ? vol -> password : "",
struct fsnotify_group * fsnotify_obtain_group ( unsigned int group_num , __u32 mask , <nl> struct fsnotify_group * group , * tgroup ; <nl>  <nl> /* very low use , simpler locking if we just always alloc */ <nl> - group = kmalloc ( sizeof ( struct fsnotify_group ), GFP_KERNEL ); <nl> + group = kzalloc ( sizeof ( struct fsnotify_group ), GFP_KERNEL ); <nl> if (! group ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
int tipc_node_get_linkname ( u32 bearer_id , u32 addr , char * linkname , size_t len ) <nl> struct tipc_link * link ; <nl> struct tipc_node * node = tipc_node_find ( addr ); <nl>  <nl> - if (( bearer_id > MAX_BEARERS ) || ! node ) <nl> + if (( bearer_id >= MAX_BEARERS ) || ! node ) <nl> return - EINVAL ; <nl> tipc_node_lock ( node ); <nl> link = node -> links [ bearer_id ];
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> */ <nl> if ( skb_shared ( skb )) { <nl> tmp_skb = skb ; <nl> - skb = skb_copy ( skb , GFP_ATOMIC ); <nl> + skb = skb_clone ( skb , GFP_ATOMIC ); <nl> kfree_skb ( tmp_skb ); <nl>  <nl> if (! skb ) {
static int egalax_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> struct egalax_data * td ; <nl> struct hid_report * report ; <nl>  <nl> - td = kmalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> + td = kzalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> if (! td ) { <nl> dev_err (& hdev -> dev , " cannot allocate eGalax data \ n "); <nl> return - ENOMEM ;
static int sdhci_esdhc_imx_probe ( struct platform_device * pdev ) <nl> * to something insane . Change it back here . <nl> */ <nl> if ( esdhc_is_usdhc ( imx_data )) { <nl> - writel ( 0x08100810 , host -> ioaddr + ESDHC_WTMK_LVL ); <nl> + writel ( 0x10401040 , host -> ioaddr + ESDHC_WTMK_LVL ); <nl> + <nl> host -> quirks2 |= SDHCI_QUIRK2_PRESET_VALUE_BROKEN ; <nl> host -> mmc -> caps |= MMC_CAP_1_8V_DDR ; <nl> 
int i915_vma_unbind ( struct i915_vma * vma ) <nl> * cause memory corruption through use - after - free . <nl> */ <nl>  <nl> + /* Throw away the active reference before moving to the unbound list */ <nl> + i915_gem_object_retire ( obj ); <nl> + <nl> if ( i915_is_ggtt ( vma -> vm )) { <nl> i915_gem_object_finish_gtt ( obj ); <nl> 
static int map_lookup_elem ( union bpf_attr * attr ) <nl> if ( copy_from_user ( key , ukey , map -> key_size ) != 0 ) <nl> goto free_key ; <nl>  <nl> - err = - ESRCH ; <nl> + err = - ENOENT ; <nl> rcu_read_lock (); <nl> value = map -> ops -> map_lookup_elem ( map , key ); <nl> if (! value )
static int __exit usba_udc_remove ( struct platform_device * pdev ) <nl> usba_ep_cleanup_debugfs (& usba_ep [ i ]); <nl> usba_cleanup_debugfs ( udc ); <nl>  <nl> - if ( gpio_is_valid ( udc -> vbus_pin )) <nl> + if ( gpio_is_valid ( udc -> vbus_pin )) { <nl> + free_irq ( gpio_to_irq ( udc -> vbus_pin ), udc ); <nl> gpio_free ( udc -> vbus_pin ); <nl> + } <nl>  <nl> free_irq ( udc -> irq , udc ); <nl> kfree ( usba_ep );
static void ath_ant_comb_scan ( struct ath_softc * sc , struct ath_rx_status * rs ) <nl> main_ant_conf = ( rs -> rs_rssi_ctl2 >> ATH_ANT_RX_MAIN_SHIFT ) & <nl> ATH_ANT_RX_MASK ; <nl>  <nl> - /* Record packet only when alt_rssi is positive */ <nl> - if ( alt_rssi > 0 ) { <nl> + /* Record packet only when both main_rssi and alt_rssi is positive */ <nl> + if ( main_rssi > 0 && alt_rssi > 0 ) { <nl> antcomb -> total_pkt_count ++; <nl> antcomb -> main_total_rssi += main_rssi ; <nl> antcomb -> alt_total_rssi += alt_rssi ;
static int chd_dec_fetch_cdata ( struct crystalhd_adp * adp , <nl> if ( rc ) { <nl> BCMLOG_ERR (" failed to pull add_cdata sz :% x ua_off :% x \ n ", <nl> io -> add_cdata_sz , ( unsigned int ) ua_off ); <nl> - kfree ( io -> add_cdata ); <nl> + vfree ( io -> add_cdata ); <nl> io -> add_cdata = NULL ; <nl> return - ENODATA ; <nl> }
static struct svc_xprt * svc_rdma_create ( struct svc_serv * serv , <nl> int ret ; <nl>  <nl> dprintk (" svcrdma : Creating RDMA socket \ n "); <nl> - <nl> + if ( sa -> sa_family != AF_INET ) { <nl> + dprintk (" svcrdma : Address family % d is not supported .\ n ", sa -> sa_family ); <nl> + return ERR_PTR (- EAFNOSUPPORT ); <nl> + } <nl> cma_xprt = rdma_create_xprt ( serv , 1 ); <nl> if (! cma_xprt ) <nl> return ERR_PTR (- ENOMEM );
static void collapse_huge_page ( struct mm_struct * mm , <nl> set_pmd_at ( mm , address , pmd , _pmd ); <nl> spin_unlock (& mm -> page_table_lock ); <nl> anon_vma_unlock ( vma -> anon_vma ); <nl> - mem_cgroup_uncharge_page ( new_page ); <nl> goto out ; <nl> } <nl>  <nl> out_up_write : <nl> return ; <nl>  <nl> out : <nl> + mem_cgroup_uncharge_page ( new_page ); <nl> # ifdef CONFIG_NUMA <nl> put_page ( new_page ); <nl> # endif
int wm8350_device_init ( struct wm8350 * wm8350 , int irq , <nl> wm8350 -> power . rev_g_coeff = 1 ; <nl> break ; <nl>  <nl> + case 1 : <nl> + dev_info ( wm8350 -> dev , " WM8351 Rev B \ n "); <nl> + wm8350 -> power . rev_g_coeff = 1 ; <nl> + break ; <nl> + <nl> default : <nl> dev_err ( wm8350 -> dev , " Unknown WM8351 CHIP_REV \ n "); <nl> ret = - ENODEV ;
iscsi_get_host_stats ( struct iscsi_transport * transport , struct nlmsghdr * nlh ) <nl> memset ( buf , 0 , host_stats_size ); <nl>  <nl> err = transport -> get_host_stats ( shost , buf , host_stats_size ); <nl> + if ( err ) { <nl> + kfree ( skbhost_stats ); <nl> + goto exit_host_stats ; <nl> + } <nl>  <nl> actual_size = nlmsg_total_size ( sizeof (* ev ) + host_stats_size ); <nl> skb_trim ( skbhost_stats , NLMSG_ALIGN ( actual_size ));
static struct pci_controller cobalt_pci_controller = { <nl> . mem_resource = & cobalt_mem_resource , <nl> . io_resource = & cobalt_io_resource , <nl> . io_offset = 0 - GT_DEF_PCI0_IO_BASE , <nl> + . io_map_base = CKSEG1ADDR ( GT_DEF_PCI0_IO_BASE ), <nl> }; <nl>  <nl> static int __init cobalt_pci_init ( void )
void machine_shutdown ( void ) <nl> */ <nl> void machine_halt ( void ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> local_irq_disable (); <nl> void machine_halt ( void ) <nl> */ <nl> void machine_power_off ( void ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> if ( pm_power_off ) <nl> void machine_power_off ( void ) <nl> */ <nl> void machine_restart ( char * cmd ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> arm_pm_restart ( reboot_mode , cmd );
static struct usb_request * isp1760_ep_alloc_request ( struct usb_ep * ep , <nl> struct isp1760_request * req ; <nl>  <nl> req = kzalloc ( sizeof (* req ), gfp_flags ); <nl> + if (! req ) <nl> + return NULL ; <nl>  <nl> return & req -> req ; <nl> }
int tcp_disconnect ( struct sock * sk , int flags ) <nl> tp -> snd_ssthresh = TCP_INFINITE_SSTHRESH ; <nl> tp -> snd_cwnd_cnt = 0 ; <nl> tp -> bytes_acked = 0 ; <nl> + tp -> window_clamp = 0 ; <nl> tcp_set_ca_state ( sk , TCP_CA_Open ); <nl> tcp_clear_retrans ( tp ); <nl> inet_csk_delack_init ( sk );
int inet6_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl> */ <nl> v4addr = LOOPBACK4_IPV6 ; <nl> if (!( addr_type & IPV6_ADDR_MULTICAST )) { <nl> - if (! inet -> transparent && <nl> + if (!( inet -> freebind || inet -> transparent ) && <nl> ! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> dev , 0 )) { <nl> err = - EADDRNOTAVAIL ;
# define VI6_DISP_IRQ_ENB 0x0078 <nl> # define VI6_DISP_IRQ_ENB_DSTE ( 1 << 8 ) <nl> # define VI6_DISP_IRQ_ENB_MAEE ( 1 << 5 ) <nl> -# define VI6_DISP_IRQ_ENB_LNEE ( n ) ( 1 << (( n ) + 4 )) <nl> +# define VI6_DISP_IRQ_ENB_LNEE ( n ) ( 1 << ( n )) <nl>  <nl> # define VI6_DISP_IRQ_STA 0x007c <nl> # define VI6_DISP_IRQ_STA_DSE ( 1 << 8 )
static int fc_user_scan ( struct Scsi_Host * shost , uint channel , <nl> if ( rport -> scsi_target_id == - 1 ) <nl> continue ; <nl>  <nl> + if ( rport -> port_state != FC_PORTSTATE_ONLINE ) <nl> + continue ; <nl> + <nl> if (( channel == SCAN_WILD_CARD || channel == rport -> channel ) && <nl> ( id == SCAN_WILD_CARD || id == rport -> scsi_target_id )) { <nl> scsi_scan_target (& rport -> dev , rport -> channel ,
static int atmel_prepare_rx_dma ( struct uart_port * port ) <nl> BUG_ON (! PAGE_ALIGNED ( ring -> buf )); <nl> sg_set_page (& atmel_port -> sg_rx , <nl> virt_to_page ( ring -> buf ), <nl> - ATMEL_SERIAL_RINGSIZE , <nl> + sizeof ( struct atmel_uart_char ) * ATMEL_SERIAL_RINGSIZE , <nl> ( int ) ring -> buf & ~ PAGE_MASK ); <nl> nent = dma_map_sg ( port -> dev , <nl> & atmel_port -> sg_rx ,
struct oz_urb_link { <nl> /* Holds state information about a USB endpoint . <nl> */ <nl> # define OZ_EP_BUFFER_SIZE_ISOC ( 1024 * 24 ) <nl> +# define OZ_EP_BUFFER_SIZE_INT 512 <nl> struct oz_endpoint { <nl> struct list_head urb_list ; /* List of oz_urb_link items . */ <nl> struct list_head link ; /* For isoc ep , links in to isoc <nl> static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> buffer_size = OZ_EP_BUFFER_SIZE_ISOC ; <nl> break ; <nl> case USB_ENDPOINT_XFER_INT : <nl> - buffer_size = 128 ; <nl> + buffer_size = OZ_EP_BUFFER_SIZE_INT ; <nl> break ; <nl> } <nl> }
static unsigned long super_cache_scan ( struct shrinker * shrink , <nl> inodes = list_lru_count_node (& sb -> s_inode_lru , sc -> nid ); <nl> dentries = list_lru_count_node (& sb -> s_dentry_lru , sc -> nid ); <nl> total_objects = dentries + inodes + fs_objects + 1 ; <nl> + if (! total_objects ) <nl> + total_objects = 1 ; <nl>  <nl> /* proportion the scan between the caches */ <nl> dentries = mult_frac ( sc -> nr_to_scan , dentries , total_objects );
static int FNAME ( fix_write_pf )( struct kvm_vcpu * vcpu , <nl> struct kvm_mmu_page * page ; <nl>  <nl> if ( is_writeble_pte (* shadow_ent )) <nl> - return 0 ; <nl> + return ! user || (* shadow_ent & PT_USER_MASK ); <nl>  <nl> writable_shadow = * shadow_ent & PT_SHADOW_WRITABLE_MASK ; <nl> if ( user ) {
static inline pte_t pfn_pte ( unsigned long pfn , pgprot_t prot ) <nl> sz_bits = 0UL ; <nl> if ( _PAGE_SZBITS_4U != 0UL || _PAGE_SZBITS_4V != 0UL ) { <nl> __asm__ __volatile__ ( <nl> - "\ n661 : sethi % uhi (% 1 ), % 0 \ n " <nl> + "\ n661 : sethi %% uhi (% 1 ), % 0 \ n " <nl> " sllx % 0 , 32 , % 0 \ n " <nl> " . section . sun4v_2insn_patch , \" ax \"\ n " <nl> " . word 661b \ n "
extern int ext4_init_inode_table ( struct super_block * sb , ext4_group_t group , <nl> group , used_blks , <nl> ext4_itable_unused_count ( sb , gdp )); <nl> ret = 1 ; <nl> - goto out ; <nl> + goto err_out ; <nl> } <nl>  <nl> blk = ext4_inode_table ( sb , gdp ) + used_blks ;
asmlinkage void math_emulate ( long arg ) <nl> entry_sel_off . offset = FPU_ORIG_EIP ; <nl> entry_sel_off . selector = FPU_CS ; <nl> entry_sel_off . opcode = ( byte1 << 8 ) | FPU_modrm ; <nl> + entry_sel_off . empty = 0 ; <nl>  <nl> FPU_rm = FPU_modrm & 7 ; <nl> 
void falcon_remove_nic ( struct efx_nic * efx ) <nl> struct falcon_nic_data * nic_data = efx -> nic_data ; <nl> int rc ; <nl>  <nl> + /* Remove I2C adapter and clear it in preparation for a retry */ <nl> rc = i2c_del_adapter (& efx -> i2c_adap ); <nl> BUG_ON ( rc ); <nl> + memset (& efx -> i2c_adap , 0 , sizeof ( efx -> i2c_adap )); <nl>  <nl> falcon_remove_spi_devices ( efx ); <nl> falcon_free_buffer ( efx , & efx -> irq_status );
enum { <nl> ATA_REG_IRQ = ATA_REG_NSECT , <nl>  <nl> /* ATA device commands */ <nl> + ATA_CMD_DEV_RESET = 0x08 , /* ATAPI device reset */ <nl> ATA_CMD_CHK_POWER = 0xE5 , /* check power mode */ <nl> ATA_CMD_STANDBY = 0xE2 , /* place in standby power mode */ <nl> ATA_CMD_IDLE = 0xE3 , /* place in idle power mode */
static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
/* <nl> * Literals <nl> */ <nl> -# define IPR_DRIVER_VERSION " 2 . 1 . 2 " <nl> -# define IPR_DRIVER_DATE "( February 8 , 2006 )" <nl> +# define IPR_DRIVER_VERSION " 2 . 1 . 3 " <nl> +# define IPR_DRIVER_DATE "( March 29 , 2006 )" <nl>  <nl> /* <nl> * IPR_MAX_CMD_PER_LUN : This defines the maximum number of outstanding
static void __init stp_reset ( void ) <nl>  <nl> stp_page = alloc_bootmem_pages ( PAGE_SIZE ); <nl> rc = chsc_sstpc ( stp_page , STP_OP_CTRL , 0x0000 ); <nl> - if ( rc == 1 ) <nl> + if ( rc == 0 ) <nl> set_bit ( CLOCK_SYNC_HAS_STP , & clock_sync_flags ); <nl> else if ( stp_online ) { <nl> printk ( KERN_WARNING " Running on non STP capable machine .\ n ");
retry_snap : <nl> goto retry_snap ; <nl> } <nl> } else { <nl> + loff_t old_size = inode -> i_size ; <nl> /* <nl> * No need to acquire the i_truncate_mutex . Because <nl> * the MDS revokes Fwb caps before sending truncate <nl> retry_snap : <nl> written = generic_file_buffered_write ( iocb , iov , nr_segs , <nl> pos , & iocb -> ki_pos , <nl> count , 0 ); <nl> + if ( inode -> i_size > old_size ) <nl> + ceph_fscache_update_objectsize ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> } <nl> 
static void iwl_ucode_callback ( const struct firmware * ucode_raw , void * context ) <nl> const struct iwl_op_mode_ops * ops = op -> ops ; <nl> drv -> op_mode = ops -> start ( drv -> trans , drv -> cfg , & drv -> fw ); <nl>  <nl> - if (! drv -> op_mode ) <nl> + if (! drv -> op_mode ) { <nl> + mutex_unlock (& iwlwifi_opmode_table_mtx ); <nl> goto out_unbind ; <nl> + } <nl> } else { <nl> load_module = true ; <nl> }
static int perf_event__repipe_attr ( struct perf_tool * tool , <nl> union perf_event * event , <nl> struct perf_evlist ** pevlist ) <nl> { <nl> + struct perf_inject * inject = container_of ( tool , struct perf_inject , <nl> + tool ); <nl> int ret ; <nl>  <nl> ret = perf_event__process_attr ( tool , event , pevlist ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if (! inject -> pipe_output ) <nl> + return 0 ; <nl> + <nl> return perf_event__repipe_synth ( tool , event ); <nl> } <nl> 
struct dst_entry * fib6_rule_lookup ( struct net * net , struct flowi * fl , <nl> { <nl> struct fib_lookup_arg arg = { <nl> . lookup_ptr = lookup , <nl> + . flags = FIB_LOOKUP_NOREF , <nl> }; <nl>  <nl> fib_rules_lookup ( net -> ipv6 . fib6_rules_ops , fl , flags , & arg ); <nl> - if ( arg . rule ) <nl> - fib_rule_put ( arg . rule ); <nl>  <nl> if ( arg . result ) <nl> return arg . result ;
int kvm_ioapic_set_irq ( struct kvm_ioapic * ioapic , int irq , int level ) <nl> if (( edge && old_irr != ioapic -> irr ) || <nl> (! edge && ! entry . fields . remote_irr )) <nl> ret = ioapic_service ( ioapic , irq ); <nl> + else <nl> + ret = 0 ; /* report coalesced interrupt */ <nl> } <nl> trace_kvm_ioapic_set_irq ( entry . bits , irq , ret == 0 ); <nl> }
int bond_arp_rcv ( const struct sk_buff * skb , struct bonding * bond , <nl> __be32 sip , tip ; <nl> int alen ; <nl>  <nl> + slave -> last_arp_rx = jiffies ; <nl> + <nl> if ( skb -> protocol != __cpu_to_be16 ( ETH_P_ARP )) <nl> return RX_HANDLER_ANOTHER ; <nl> 
static int ceph_link ( struct dentry * old_dentry , struct inode * dir , <nl> req -> r_locked_dir = dir ; <nl> req -> r_dentry_drop = CEPH_CAP_FILE_SHARED ; <nl> req -> r_dentry_unless = CEPH_CAP_FILE_EXCL ; <nl> + /* release LINK_SHARED on source inode ( mds will lock it ) */ <nl> + req -> r_old_inode_drop = CEPH_CAP_LINK_SHARED ; <nl> err = ceph_mdsc_do_request ( mdsc , dir , req ); <nl> if ( err ) { <nl> d_drop ( dentry );
try_again : <nl> * based cpu - clock - tick sw counter , which <nl> * is always available even if no PMU support : <nl> */ <nl> - if ( attr -> type == PERF_TYPE_HARDWARE <nl> + if ( err == ENOENT && attr -> type == PERF_TYPE_HARDWARE <nl> && attr -> config == PERF_COUNT_HW_CPU_CYCLES ) { <nl>  <nl> if ( verbose )
int wlan_unsetup ( wlandevice_t * wlandev ) <nl>  <nl> if ( wlandev -> netdev ) { <nl> wdev = netdev_priv ( wlandev -> netdev ); <nl> - if ( wdev -> wiphy ) wlan_free_wiphy ( wdev -> wiphy ); <nl> + if ( wdev -> wiphy ) <nl> + wlan_free_wiphy ( wdev -> wiphy ); <nl> free_netdev ( wlandev -> netdev ); <nl> wlandev -> netdev = NULL ; <nl> }
static inline void x86_assign_hw_event ( struct perf_event * event , <nl> } else { <nl> hwc -> config_base = x86_pmu_config_addr ( hwc -> idx ); <nl> hwc -> event_base = x86_pmu_event_addr ( hwc -> idx ); <nl> - hwc -> event_base_rdpmc = x86_pmu_addr_offset ( hwc -> idx ); <nl> + hwc -> event_base_rdpmc = hwc -> idx ; <nl> } <nl> } <nl> 
void dump_stack ( void ) <nl> EXPORT_SYMBOL ( dump_stack ); <nl>  <nl> /* Stolen from arch / i386 / kernel / traps . c */ <nl> - static int kstack_depth_to_print = 24 ; <nl> + static const int kstack_depth_to_print = 24 ; <nl>  <nl> /* This recently started being used in arch - independent code too , as in <nl> * kernel / sched . c .*/
static int device_authorization ( struct hdpvr_device * dev ) <nl> hex_dump_to_buffer ( response , 8 , 16 , 1 , print_buf , 5 * buf_size + 1 , 0 ); <nl> v4l2_dbg ( MSG_INFO , hdpvr_debug , & dev -> v4l2_dev , " response : % s \ n ", <nl> print_buf ); <nl> + kfree ( print_buf ); <nl> # endif <nl>  <nl> msleep ( 100 );
static void __cpuinit init_amd ( struct cpuinfo_x86 * c ) <nl> level = cpuid_eax ( 1 ); <nl> if ( c -> x86 == 15 && (( level >= 0x0f48 && level < 0x0f50 ) || level >= 0x0f58 )) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl> - if ( c -> x86 == 0x10 ) <nl> + if ( c -> x86 == 0x10 || c -> x86 == 0x11 ) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl>  <nl> /* Enable workaround for FXSAVE leak */
int tpm_open ( struct inode * inode , struct file * file ) <nl> return - EBUSY ; <nl> } <nl>  <nl> - chip -> data_buffer = kmalloc ( TPM_BUFSIZE * sizeof ( u8 ), GFP_KERNEL ); <nl> + chip -> data_buffer = kzalloc ( TPM_BUFSIZE , GFP_KERNEL ); <nl> if ( chip -> data_buffer == NULL ) { <nl> clear_bit ( 0 , & chip -> is_open ); <nl> put_device ( chip -> dev );
static int create_i2c_bus ( struct i2c_adapter * adapter , <nl> algo -> setscl = via_i2c_setscl ; <nl> algo -> getsda = via_i2c_getsda ; <nl> algo -> getscl = via_i2c_getscl ; <nl> - algo -> udelay = 40 ; <nl> - algo -> timeout = 20 ; <nl> + algo -> udelay = 10 ; <nl> + algo -> timeout = 2 ; <nl> algo -> data = adap_cfg ; <nl>  <nl> sprintf ( adapter -> name , " viafb i2c io_port idx 0x % 02x ",
void afu_release_irqs ( struct cxl_context * ctx , void * cookie ) <nl>  <nl> afu_irq_name_free ( ctx ); <nl> cxl_release_irq_ranges (& ctx -> irqs , ctx -> afu -> adapter ); <nl> + <nl> + kfree ( ctx -> irq_bitmap ); <nl> + ctx -> irq_bitmap = NULL ; <nl> + ctx -> irq_count = 0 ; <nl> }
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl>  <nl> void oprofile_arch_exit ( void ) <nl> { <nl> - model -> exit (); <nl> + if ( model ) <nl> + model -> exit (); <nl> }
static void deallocate_vmid ( struct device_queue_manager * dqm , <nl> { <nl> int bit = qpd -> vmid - KFD_VMID_START_OFFSET ; <nl>  <nl> + /* Release the vmid mapping */ <nl> + set_pasid_vmid_mapping ( dqm , 0 , qpd -> vmid ); <nl> + <nl> set_bit ( bit , ( unsigned long *)& dqm -> vmid_bitmap ); <nl> qpd -> vmid = 0 ; <nl> q -> properties . vmid = 0 ;
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
int __init omap_mux_init ( u32 mux_pbase , u32 mux_size , <nl> } <nl>  <nl> # ifdef CONFIG_OMAP_MUX <nl> - omap_mux_package_fixup ( package_subset , superset ); <nl> - omap_mux_package_init_balls ( package_balls , superset ); <nl> + if ( package_subset ) <nl> + omap_mux_package_fixup ( package_subset , superset ); <nl> + if ( package_balls ) <nl> + omap_mux_package_init_balls ( package_balls , superset ); <nl> omap_mux_set_cmdline_signals (); <nl> omap_mux_set_board_signals ( board_mux ); <nl> # endif
static void __init_memblock memblock_merge_regions ( struct memblock_type * type ) <nl> } <nl>  <nl> this -> size += next -> size ; <nl> - memmove ( next , next + 1 , ( type -> cnt - ( i + 1 )) * sizeof (* next )); <nl> + /* move forward from next + 1 , index of which is i + 2 */ <nl> + memmove ( next , next + 1 , ( type -> cnt - ( i + 2 )) * sizeof (* next )); <nl> type -> cnt --; <nl> } <nl> }
static int __devinit ab8500_ponkey_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_free_dbr_irq : <nl> - free_irq ( ponkey -> irq_dbf , ponkey ); <nl> + free_irq ( ponkey -> irq_dbr , ponkey ); <nl> err_free_dbf_irq : <nl> free_irq ( ponkey -> irq_dbf , ponkey ); <nl> err_free_mem :
static int cfg80211_netdev_notifier_call ( struct notifier_block * nb , <nl> * Configure power management to the driver here so that its <nl> * correctly set also after interface type changes etc . <nl> */ <nl> - if ( wdev -> iftype == NL80211_IFTYPE_STATION && <nl> + if (( wdev -> iftype == NL80211_IFTYPE_STATION || <nl> + wdev -> iftype == NL80211_IFTYPE_P2P_CLIENT ) && <nl> rdev -> ops -> set_power_mgmt ) <nl> if ( rdev -> ops -> set_power_mgmt ( wdev -> wiphy , dev , <nl> wdev -> ps ,
static void ixgbe_reset_subtask ( struct ixgbe_adapter * adapter ) <nl> netdev_err ( adapter -> netdev , " Reset adapter \ n "); <nl> adapter -> tx_timeout_count ++; <nl>  <nl> + rtnl_lock (); <nl> ixgbe_reinit_locked ( adapter ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> /**
int genwqe_device_create ( struct genwqe_dev * cd ) <nl> genwqe_attribute_groups , <nl> GENWQE_DEVNAME "% u_card ", <nl> cd -> card_idx ); <nl> - if ( cd -> dev == NULL ) { <nl> - rc = - ENODEV ; <nl> + if ( IS_ERR ( cd -> dev )) { <nl> + rc = PTR_ERR ( cd -> dev ); <nl> goto err_cdev ; <nl> } <nl> 
static int md_open ( struct block_device * bdev , fmode_t mode ) <nl> struct mddev * mddev = mddev_find ( bdev -> bd_dev ); <nl> int err ; <nl>  <nl> + if (! mddev ) <nl> + return - ENODEV ; <nl> + <nl> if ( mddev -> gendisk != bdev -> bd_disk ) { <nl> /* we are racing with mddev_put which is discarding this <nl> * bd_disk .
static struct clk_lookup lookups [] = { <nl>  <nl> /* MSTP32 clocks */ <nl> CLKDEV_DEV_ID (" sh_mmcif ", & mstp_clks [ MSTP331 ]), /* MMC */ <nl> + CLKDEV_DEV_ID (" ffe4e000 . mmcif ", & mstp_clks [ MSTP331 ]), /* MMC */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 0 ", & mstp_clks [ MSTP323 ]), /* SDHI0 */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 1 ", & mstp_clks [ MSTP322 ]), /* SDHI1 */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 2 ", & mstp_clks [ MSTP321 ]), /* SDHI2 */
static int imx6q_revision ( void ) <nl> } <nl> } <nl>  <nl> - void imx6q_restart ( char mode , const char * cmd ) <nl> + static void imx6q_restart ( char mode , const char * cmd ) <nl> { <nl> struct device_node * np ; <nl> void __iomem * wdog_base ; <nl> put_node : <nl> of_node_put ( np ); <nl> } <nl>  <nl> - struct platform_device imx6q_cpufreq_pdev = { <nl> + static struct platform_device imx6q_cpufreq_pdev = { <nl> . name = " imx6q - cpufreq ", <nl> }; <nl> 
int ieee80211_master_start_xmit ( struct sk_buff * skb , <nl> return 0 ; <nl> } <nl>  <nl> + if ( osdata -> vif . type == NL80211_IFTYPE_AP_VLAN ) <nl> + osdata = container_of ( osdata -> bss , <nl> + struct ieee80211_sub_if_data , <nl> + u . ap ); <nl> info -> control . vif = & osdata -> vif ; <nl> ret = ieee80211_tx ( odev , skb ); <nl> dev_put ( odev );
void btrfsic_unmount ( struct btrfs_root * root , <nl> btrfsic_block_link_free ( l ); <nl> } <nl>  <nl> - if ( b_all -> is_iodone ) <nl> + if ( b_all -> is_iodone || b_all -> never_written ) <nl> btrfsic_block_free ( b_all ); <nl> else <nl> printk ( KERN_INFO " btrfs : attempt to free % c - block "
static int snd_timer_s_stop ( struct snd_timer * timer ) <nl> timer -> sticks = priv -> last_expires - jiff ; <nl> else <nl> timer -> sticks = 1 ; <nl> + priv -> correction = 0 ; <nl> return 0 ; <nl> } <nl> 
# define gadget_is_musbhsfc ( g ) 0 <nl> # endif <nl>  <nl> -/* Mentor high speed " dual role " controller , peripheral mode */ <nl> -# ifdef CONFIG_USB_GADGET_MUSBHDRC <nl> -# define gadget_is_musbhdrc ( g ) ! strcmp (" musbhdrc_udc ", ( g )-> name ) <nl> +/* Mentor high speed " dual role " controller , in peripheral role */ <nl> +# ifdef CONFIG_USB_GADGET_MUSB_HDRC <nl> +# define gadget_is_musbhdrc ( g ) ! strcmp (" musb_hdrc ", ( g )-> name ) <nl> # else <nl> # define gadget_is_musbhdrc ( g ) 0 <nl> # endif
s32 e1000e_setup_fiber_serdes_link ( struct e1000_hw * hw ) <nl> e_dbg (" No signal detected \ n "); <nl> } <nl>  <nl> - return 0 ; <nl> + return ret_val ; <nl> } <nl>  <nl> /**
static int __devinit adm8211_probe ( struct pci_dev * pdev , <nl>  <nl> priv -> channel = 1 ; <nl>  <nl> + dev -> wiphy -> bands [ IEEE80211_BAND_2GHZ ] = & priv -> band ; <nl> + <nl> err = ieee80211_register_hw ( dev ); <nl> if ( err ) { <nl> printk ( KERN_ERR "% s ( adm8211 ): Cannot register device \ n ",
static void __devinit pci_read_bridge_io ( struct pci_bus * child ) <nl>  <nl> if ( base && base <= limit ) { <nl> res -> flags = ( io_base_lo & PCI_IO_RANGE_TYPE_MASK ) | IORESOURCE_IO ; <nl> + res2 . flags = res -> flags ; <nl> region . start = base ; <nl> region . end = limit + 0xfff ; <nl> pcibios_bus_to_resource ( dev , & res2 , & region );
ret_orig : <nl> kfree_skb ( clone ); <nl> return skb ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( nf_ct_frag6_gather ); <nl>  <nl> void nf_ct_frag6_consume_orig ( struct sk_buff * skb ) <nl> {
static int bug_handler ( struct pt_regs * regs , unsigned int esr ) <nl> break ; <nl>  <nl> case BUG_TRAP_TYPE_WARN : <nl> + /* Ideally , report_bug () should backtrace for us ... but no . */ <nl> + dump_backtrace ( regs , NULL ); <nl> break ; <nl>  <nl> default :
void (* mach_beep )( unsigned int , unsigned int ); <nl> # if defined ( CONFIG_ISA ) && defined ( MULTI_ISA ) <nl> int isa_type ; <nl> int isa_sex ; <nl> + EXPORT_SYMBOL ( isa_type ); <nl> + EXPORT_SYMBOL ( isa_sex ); <nl> # endif <nl>  <nl> extern int amiga_parse_bootinfo ( const struct bi_record *);
int setup_one_line ( struct line * lines , int n , char * init , <nl> * error_out = " Failed to allocate memory "; <nl> return - ENOMEM ; <nl> } <nl> - if ( line -> valid ) <nl> + if ( line -> valid ) { <nl> tty_unregister_device ( driver , n ); <nl> + kfree ( line -> init_str ); <nl> + } <nl> line -> init_str = new ; <nl> line -> valid = 1 ; <nl> err = parse_chan_pair ( new , line , n , opts , error_out );
void speakup_remove_virtual_keyboard ( void ) <nl> { <nl> if ( virt_keyboard != NULL ) { <nl> input_unregister_device ( virt_keyboard ); <nl> - input_free_device ( virt_keyboard ); <nl> virt_keyboard = NULL ; <nl> } <nl> }
static int zcache_new_pool ( uint16_t cli_id , uint32_t flags ) <nl> if ( cli == NULL ) <nl> goto out ; <nl> atomic_inc (& cli -> refcount ); <nl> - pool = kmalloc ( sizeof ( struct tmem_pool ), GFP_KERNEL ); <nl> + pool = kmalloc ( sizeof ( struct tmem_pool ), GFP_ATOMIC ); <nl> if ( pool == NULL ) { <nl> pr_info (" zcache : pool creation failed : out of memory \ n "); <nl> goto out ;
static u32 __init allocate_aperture ( void ) <nl> printk (" Cannot allocate aperture memory hole (% p ,% uK )\ n ", <nl> p , aper_size >> 10 ); <nl> if ( p ) <nl> - free_bootmem_node ( nd0 , ( unsigned long ) p , aper_size ); <nl> + free_bootmem_node ( nd0 , __pa ( p ), aper_size ); <nl> return 0 ; <nl> } <nl> printk (" Mapping aperture over % d KB of RAM @ % lx \ n ",
static void __init ima_add_boot_aggregate ( void ) <nl> result = ima_calc_boot_aggregate (& hash . hdr ); <nl> if ( result < 0 ) { <nl> audit_cause = " hashing_error "; <nl> - kfree ( entry ); <nl> goto err_out ; <nl> } <nl> }
static int wm8731_check_osc ( struct snd_soc_dapm_widget * source , <nl> { <nl> struct wm8731_priv * wm8731 = snd_soc_codec_get_drvdata ( source -> codec ); <nl>  <nl> - return wm8731 -> sysclk_type == WM8731_SYSCLK_MCLK ; <nl> + return wm8731 -> sysclk_type == WM8731_SYSCLK_XTAL ; <nl> } <nl>  <nl> static const struct snd_soc_dapm_route wm8731_intercon [] = {
static int mt9t112_probe ( struct i2c_client * client , <nl> v4l2_i2c_subdev_init (& priv -> subdev , client , & mt9t112_subdev_ops ); <nl>  <nl> ret = mt9t112_camera_probe ( client ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> kfree ( priv ); <nl> + return ret ; <nl> + } <nl>  <nl> /* Cannot fail : using the default supported pixel code */ <nl> mt9t112_set_params ( priv , & rect , V4L2_MBUS_FMT_UYVY8_2X8 );
SYSCALL_DEFINE5 ( perf_event_open , <nl> } <nl> } <nl>  <nl> - if ( pid != - 1 ) <nl> + if ( pid != - 1 ) { <nl> task = find_lively_task_by_vpid ( pid ); <nl> + if ( IS_ERR ( task )) { <nl> + err = PTR_ERR ( task ); <nl> + goto err_group_fd ; <nl> + } <nl> + } <nl>  <nl> /* <nl> * Get the target context ( task or percpu ):
static int omap_pcm_open ( struct snd_pcm_substream * substream ) <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> - prtd = kzalloc ( sizeof ( prtd ), GFP_KERNEL ); <nl> + prtd = kzalloc ( sizeof (* prtd ), GFP_KERNEL ); <nl> if ( prtd == NULL ) { <nl> ret = - ENOMEM ; <nl> goto out ;
static int parse_audio_selector_unit ( struct mixer_build * state , int unitid , void <nl> if (! len && check_input_term ( state , desc -> baSourceID [ i ], & iterm ) >= 0 ) <nl> len = get_term_name ( state , & iterm , namelist [ i ], MAX_ITEM_NAME_LEN , 0 ); <nl> if (! len ) <nl> - sprintf ( namelist [ i ], " Input % d ", i ); <nl> + sprintf ( namelist [ i ], " Input % u ", i ); <nl> } <nl>  <nl> kctl = snd_ctl_new1 (& mixer_selectunit_ctl , cval );
int datagram_send_ctl ( struct net * net , struct sock * sk , <nl>  <nl> if ( addr_type != IPV6_ADDR_ANY ) { <nl> int strict = __ipv6_addr_src_scope ( addr_type ) <= IPV6_ADDR_SCOPE_LINKLOCAL ; <nl> - if (! inet_sk ( sk )-> transparent && <nl> + if (!( inet_sk ( sk )-> freebind || inet_sk ( sk )-> transparent ) && <nl> ! ipv6_chk_addr ( net , & src_info -> ipi6_addr , <nl> strict ? dev : NULL , 0 )) <nl> err = - EINVAL ;
iscsi_tcp_state_change ( struct sock * sk ) <nl> conn = ( struct iscsi_conn *) sk -> sk_user_data ; <nl> session = conn -> session ; <nl>  <nl> - if ( sk -> sk_state == TCP_CLOSE_WAIT || <nl> - sk -> sk_state == TCP_CLOSE ) { <nl> + if (( sk -> sk_state == TCP_CLOSE_WAIT || <nl> + sk -> sk_state == TCP_CLOSE ) && <nl> + ! atomic_read (& sk -> sk_rmem_alloc )) { <nl> debug_tcp (" iscsi_tcp_state_change : TCP_CLOSE | TCP_CLOSE_WAIT \ n "); <nl> iscsi_conn_failure ( conn , ISCSI_ERR_CONN_FAILED ); <nl> }
int hid_input_report ( struct hid_device * hid , int type , u8 * data , int size , int i <nl>  <nl> if ( hdrv && hdrv -> raw_event && hid_match_report ( hid , report )) { <nl> ret = hdrv -> raw_event ( hid , report , data , size ); <nl> - if ( ret != 0 ) { <nl> + if ( ret < 0 ) { <nl> ret = ret < 0 ? ret : 0 ; <nl> goto unlock ; <nl> }
static int omap_sham_finup ( struct ahash_request * req ) <nl> ctx -> flags |= FLAGS_FINUP ; <nl>  <nl> err1 = omap_sham_update ( req ); <nl> - if ( err1 == - EINPROGRESS ) <nl> + if ( err1 == - EINPROGRESS || err1 == - EBUSY ) <nl> return err1 ; <nl> /* <nl> * final () has to be always called to cleanup resources
void batadv_gw_node_update ( struct batadv_priv * bat_priv , <nl> * gets dereferenced . <nl> */ <nl> spin_lock_bh (& bat_priv -> gw . list_lock ); <nl> - hlist_del_init_rcu (& gw_node -> list ); <nl> + if (! hlist_unhashed (& gw_node -> list )) { <nl> + hlist_del_init_rcu (& gw_node -> list ); <nl> + batadv_gw_node_free_ref ( gw_node ); <nl> + } <nl> spin_unlock_bh (& bat_priv -> gw . list_lock ); <nl>  <nl> - batadv_gw_node_free_ref ( gw_node ); <nl> - <nl> curr_gw = batadv_gw_get_selected_gw_node ( bat_priv ); <nl> if ( gw_node == curr_gw ) <nl> batadv_gw_reselect ( bat_priv );
static int perf_session_deliver_event ( struct perf_session * session , <nl> dump_sample ( session , event , sample ); <nl> if ( evsel == NULL ) { <nl> ++ session -> hists . stats . nr_unknown_id ; <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl> if ( machine == NULL ) { <nl> ++ session -> hists . stats . nr_unprocessable_samples ; <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl> return tool -> sample ( tool , event , sample , evsel , machine ); <nl> case PERF_RECORD_MMAP :
static void scsi_sysfs_add_devices ( struct Scsi_Host * shost ) <nl> /* target removed before the device could be added */ <nl> if ( sdev -> sdev_state == SDEV_DEL ) <nl> continue ; <nl> + /* If device is already visible , skip adding it to sysfs */ <nl> + if ( sdev -> is_visible ) <nl> + continue ; <nl> if (! scsi_host_scan_allowed ( shost ) || <nl> scsi_sysfs_add_sdev ( sdev ) != 0 ) <nl> __scsi_remove_device ( sdev );
__rb_reserve_next ( struct ring_buffer_per_cpu * cpu_buffer , <nl> write &= RB_WRITE_MASK ; <nl> tail = write - length ; <nl>  <nl> + /* <nl> + * If this is the first commit on the page , then it has the same <nl> + * timestamp as the page itself . <nl> + */ <nl> + if (! tail ) <nl> + delta = 0 ; <nl> + <nl> /* See if we shot pass the end of this buffer page */ <nl> if ( unlikely ( write > BUF_PAGE_SIZE )) <nl> return rb_move_tail ( cpu_buffer , length , tail ,
alloc_new_skb : <nl> * because we have no idea what fragment will be <nl> * the last . <nl> */ <nl> - if ( datalen == length ) <nl> + if ( datalen == length + fraggap ) <nl> alloclen += rt -> u . dst . trailer_len ; <nl>  <nl> if ( transhdrlen ) {
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( ieee80211_vif_is_mesh (& sdata -> vif ) && <nl> ! sdata -> u . mesh . user_mpm ) <nl> init_timer (& sta -> plink_timer ); <nl> + sta -> nonpeer_pm = NL80211_MESH_POWER_ACTIVE ; <nl> # endif <nl>  <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN );
nfsd4_getdeviceinfo ( struct svc_rqst * rqstp , <nl> nfserr = ops -> proc_getdeviceinfo ( exp -> ex_path . mnt -> mnt_sb , gdp ); <nl>  <nl> gdp -> gd_notify_types &= ops -> notify_types ; <nl> - exp_put ( exp ); <nl> out : <nl> + exp_put ( exp ); <nl> return nfserr ; <nl> } <nl> 
static void paging_new_cr3 ( struct kvm_vcpu * vcpu ) <nl> { <nl> pgprintk ("% s : cr3 % lx \ n ", __FUNCTION__ , vcpu -> cr3 ); <nl> mmu_free_roots ( vcpu ); <nl> + if ( unlikely ( vcpu -> kvm -> n_free_mmu_pages < KVM_MIN_FREE_MMU_PAGES )) <nl> + kvm_mmu_free_some_pages ( vcpu ); <nl> mmu_alloc_roots ( vcpu ); <nl> kvm_mmu_flush_tlb ( vcpu ); <nl> kvm_arch_ops -> set_cr3 ( vcpu , vcpu -> mmu . root_hpa );
int drbd_al_begin_io_nonblock ( struct drbd_conf * mdev , struct drbd_interval * i ) <nl> if ( unlikely ( tmp != NULL )) { <nl> struct bm_extent * bm_ext = lc_entry ( tmp , struct bm_extent , lce ); <nl> if ( test_bit ( BME_NO_WRITES , & bm_ext -> flags )) { <nl> - if (! test_and_set_bit ( BME_PRIORITY , & bm_ext -> flags )); <nl> + if (! test_and_set_bit ( BME_PRIORITY , & bm_ext -> flags )) <nl> return - EBUSY ; <nl> return - EWOULDBLOCK ; <nl> }
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> tnl_hlen = skb_tnl_header_len ( skb ); <nl> - if ( skb_headroom ( skb ) < ( tnl_hlen + frag_hdr_sz )) { <nl> + if ( skb -> mac_header < ( tnl_hlen + frag_hdr_sz )) { <nl> if ( gso_pskb_expand_head ( skb , tnl_hlen + frag_hdr_sz )) <nl> goto out ; <nl> }
static int pm860x_led_dt_init ( struct platform_device * pdev , <nl> of_property_read_u32 ( np , " marvell , 88pm860x - iset ", <nl> & iset ); <nl> data -> iset = PM8606_LED_CURRENT ( iset ); <nl> + of_node_put ( np ); <nl> break ; <nl> } <nl> }
int ceph_osdc_create_event ( struct ceph_osd_client * osdc , <nl> event -> data = data ; <nl> event -> osdc = osdc ; <nl> INIT_LIST_HEAD (& event -> osd_node ); <nl> + RB_CLEAR_NODE (& event -> node ); <nl> kref_init (& event -> kref ); /* one ref for us */ <nl> kref_get (& event -> kref ); /* one ref for the caller */ <nl> init_completion (& event -> completion );
static int virtnet_set_channels ( struct net_device * dev , <nl> if ( channels -> rx_count || channels -> tx_count || channels -> other_count ) <nl> return - EINVAL ; <nl>  <nl> - if ( queue_pairs > vi -> max_queue_pairs ) <nl> + if ( queue_pairs > vi -> max_queue_pairs || queue_pairs == 0 ) <nl> return - EINVAL ; <nl>  <nl> get_online_cpus ();
const struct imx_imx_ssi_data imx35_imx_ssi_data [] __initconst = { <nl> # ifdef CONFIG_SOC_IMX51 <nl> const struct imx_imx_ssi_data imx51_imx_ssi_data [] __initconst = { <nl> # define imx51_imx_ssi_data_entry ( _id , _hwid ) \ <nl> - imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_4K ) <nl> + imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_16K ) <nl> imx51_imx_ssi_data_entry ( 0 , 1 ), <nl> imx51_imx_ssi_data_entry ( 1 , 2 ), <nl> imx51_imx_ssi_data_entry ( 2 , 3 ),
static int t4_sge_init_soft ( struct adapter * adap ) <nl> # undef READ_FL_BUF <nl>  <nl> if ( fl_small_pg != PAGE_SIZE || <nl> - ( fl_large_pg != 0 && ( fl_large_pg <= fl_small_pg || <nl> + ( fl_large_pg != 0 && ( fl_large_pg < fl_small_pg || <nl> ( fl_large_pg & ( fl_large_pg - 1 )) != 0 ))) { <nl> dev_err ( adap -> pdev_dev , " bad SGE FL page buffer sizes [% d , % d ]\ n ", <nl> fl_small_pg , fl_large_pg );
static void put_pages ( struct drm_gem_object * obj ) <nl>  <nl> if ( iommu_present (& platform_bus_type )) <nl> drm_gem_put_pages ( obj , msm_obj -> pages , true , false ); <nl> - else <nl> + else { <nl> drm_mm_remove_node ( msm_obj -> vram_node ); <nl> + drm_free_large ( msm_obj -> pages ); <nl> + } <nl>  <nl> msm_obj -> pages = NULL ; <nl> }
static bool linkwatch_urgent_event ( struct net_device * dev ) <nl> if ( dev -> ifindex != dev -> iflink ) <nl> return true ; <nl>  <nl> + if ( dev -> priv_flags & IFF_TEAM_PORT ) <nl> + return true ; <nl> + <nl> return netif_carrier_ok ( dev ) && qdisc_tx_changing ( dev ); <nl> } <nl> 
static void fc_rport_plogi_resp ( struct fc_seq * sp , struct fc_frame * fp , <nl>  <nl> tov = ntohl ( plp -> fl_csp . sp_e_d_tov ); <nl> if ( ntohs ( plp -> fl_csp . sp_features ) & FC_SP_FT_EDTR ) <nl> - tov /= 1000 ; <nl> + tov /= 1000000 ; <nl> if ( tov > rdata -> e_d_tov ) <nl> rdata -> e_d_tov = tov ; <nl> csp_seq = ntohs ( plp -> fl_csp . sp_tot_seq );
static int md_notify_reboot ( struct notifier_block * this , <nl> if ( mddev_trylock ( mddev )) { <nl> if ( mddev -> pers ) <nl> __md_stop_writes ( mddev ); <nl> - mddev -> safemode = 2 ; <nl> + if ( mddev -> persistent ) <nl> + mddev -> safemode = 2 ; <nl> mddev_unlock ( mddev ); <nl> } <nl> need_delay = 1 ;
UNUSUAL_DEV ( 0x090a , 0x1200 , 0x0000 , 0x9999 , <nl> USB_SC_RBC , USB_PR_BULK , NULL , <nl> 0 ), <nl>  <nl> +/* Feiya QDI U2 DISK , reported by Hans de Goede < hdegoede @ redhat . com > */ <nl> + UNUSUAL_DEV ( 0x090c , 0x1000 , 0x0000 , 0xffff , <nl> + " Feiya ", <nl> + " QDI U2 DISK ", <nl> + USB_SC_DEVICE , USB_PR_DEVICE , NULL , <nl> + US_FL_NO_READ_CAPACITY_16 ), <nl> + <nl> /* aeb */ <nl> UNUSUAL_DEV ( 0x090c , 0x1132 , 0x0000 , 0xffff , <nl> " Feiya ",
static void do_reads ( struct mirror_set * ms , struct bio_list * reads ) <nl> /* <nl> * We can only read balance if the region is in sync . <nl> */ <nl> - if ( rh_in_sync (& ms -> rh , region , 0 )) <nl> + if ( rh_in_sync (& ms -> rh , region , 1 )) <nl> m = choose_mirror ( ms , bio -> bi_sector ); <nl> else <nl> m = ms -> default_mirror ;
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> struct oz_app_hdr * app_hdr ; <nl> struct oz_serial_ctx * ctx ; <nl>  <nl> + if ( count > sizeof ( ei -> data ) - sizeof (* elt ) - sizeof (* app_hdr )) <nl> + return - EINVAL ; <nl> + <nl> spin_lock_bh (& g_cdev . lock ); <nl> pd = g_cdev . active_pd ; <nl> if ( pd )
static int wm8994_device_init ( struct wm8994 * wm8994 , int irq ) <nl> struct regmap_config * regmap_config ; <nl> const struct reg_default * regmap_patch = NULL ; <nl> const char * devname ; <nl> - int ret , i , patch_regs ; <nl> + int ret , i , patch_regs = 0 ; <nl> int pulls = 0 ; <nl>  <nl> if ( dev_get_platdata ( wm8994 -> dev )) {
int ath9k_hw_fill_cap_info ( struct ath_hw * ah ) <nl>  <nl> if ( AR_SREV_9485 ( ah ) || AR_SREV_9285 ( ah ) || AR_SREV_9330 ( ah )) <nl> chip_chainmask = 1 ; <nl> + else if ( AR_SREV_9462 ( ah )) <nl> + chip_chainmask = 3 ; <nl> else if (! AR_SREV_9280_20_OR_LATER ( ah )) <nl> chip_chainmask = 7 ; <nl> else if (! AR_SREV_9300_20_OR_LATER ( ah ) || AR_SREV_9340 ( ah ))
static void prepare_dma ( struct s3c64xx_spi_dma_data * dma , <nl> struct scatterlist sg ; <nl> struct dma_async_tx_descriptor * desc ; <nl>  <nl> + memset (& config , 0 , sizeof ( config )); <nl> + <nl> if ( dma -> direction == DMA_DEV_TO_MEM ) { <nl> sdd = container_of (( void *) dma , <nl> struct s3c64xx_spi_driver_data , rx_dma );
int btrfs_drop_snapshot ( struct btrfs_root * root , <nl> int level ; <nl>  <nl> path = btrfs_alloc_path (); <nl> - BUG_ON (! path ); <nl> + if (! path ) <nl> + return - ENOMEM ; <nl>  <nl> wc = kzalloc ( sizeof (* wc ), GFP_NOFS ); <nl> - BUG_ON (! wc ); <nl> + if (! wc ) { <nl> + btrfs_free_path ( path ); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> trans = btrfs_start_transaction ( tree_root , 0 ); <nl> BUG_ON ( IS_ERR ( trans ));
static void stmmac_clk_csr_set ( struct stmmac_priv * priv ) <nl> priv -> clk_csr = STMMAC_CSR_100_150M ; <nl> else if (( clk_rate >= CSR_F_150M ) && ( clk_rate < CSR_F_250M )) <nl> priv -> clk_csr = STMMAC_CSR_150_250M ; <nl> - else if (( clk_rate >= CSR_F_250M ) && ( clk_rate < CSR_F_300M )) <nl> + else if (( clk_rate >= CSR_F_250M ) && ( clk_rate <= CSR_F_300M )) <nl> priv -> clk_csr = STMMAC_CSR_250_300M ; <nl> } <nl> }
static int sky2_rx_start ( struct sky2_port * sky2 ) <nl> rx_set_checksum ( sky2 ); <nl>  <nl> /* Space needed for frame data + headers rounded up */ <nl> - size = ALIGN ( sky2 -> netdev -> mtu + ETH_HLEN + VLAN_HLEN , 8 ) <nl> - + 8 ; <nl> + size = roundup ( sky2 -> netdev -> mtu + ETH_HLEN + VLAN_HLEN , 8 ); <nl>  <nl> /* Stopping point for hardware truncation */ <nl> thresh = ( size - 8 ) / sizeof ( u32 );
static int __cpuinit iucv_cpu_notify ( struct notifier_block * self , <nl> return NOTIFY_BAD ; <nl> iucv_param [ cpu ] = kmalloc_node ( sizeof ( union iucv_param ), <nl> GFP_KERNEL | GFP_DMA , cpu_to_node ( cpu )); <nl> - if (! iucv_param [ cpu ]) <nl> + if (! iucv_param [ cpu ]) { <nl> + kfree ( iucv_irq_data [ cpu ]); <nl> + iucv_irq_data [ cpu ] = NULL ; <nl> return NOTIFY_BAD ; <nl> + } <nl> break ; <nl> case CPU_UP_CANCELED : <nl> case CPU_UP_CANCELED_FROZEN :
static ssize_t dgrp_class_pollrate_store ( struct device * c , <nl> struct device_attribute * attr , <nl> const char * buf , size_t count ) <nl> { <nl> - sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ); <nl> + if ( sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ) != 1 ) <nl> + return - EINVAL ; <nl> + <nl> return count ; <nl> } <nl> static DEVICE_ATTR ( pollrate , 0600 , dgrp_class_pollrate_show ,
static void dmaengine_pcm_request_chan_of ( struct dmaengine_pcm * pcm , <nl> return ; <nl>  <nl> if ( pcm -> flags & SND_DMAENGINE_PCM_FLAG_HALF_DUPLEX ) { <nl> - pcm -> chan [ 0 ] = of_dma_request_slave_channel ( of_node , " tx_rx "); <nl> + pcm -> chan [ 0 ] = of_dma_request_slave_channel ( of_node , " rx - tx "); <nl> pcm -> chan [ 1 ] = pcm -> chan [ 0 ]; <nl> } else { <nl> for ( i = SNDRV_PCM_STREAM_PLAYBACK ; i <= SNDRV_PCM_STREAM_CAPTURE ; i ++) {
int __kvm_set_memory_region ( struct kvm * kvm , <nl> goto out ; <nl> if ( mem -> guest_phys_addr & ( PAGE_SIZE - 1 )) <nl> goto out ; <nl> + if ( mem -> userspace_addr & ( PAGE_SIZE - 1 )) <nl> + goto out ; <nl> if ( mem -> slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS ) <nl> goto out ; <nl> if ( mem -> guest_phys_addr + mem -> memory_size < mem -> guest_phys_addr )
static int snd_sb_csp_ioctl ( struct snd_hwdep * hw , struct file * file , unsigned i <nl> switch ( cmd ) { <nl> /* get information */ <nl> case SNDRV_SB_CSP_IOCTL_INFO : <nl> + memset (& info , 0 , sizeof ( info )); <nl> * info . codec_name = * p -> codec_name ; <nl> info . func_nr = p -> func_nr ; <nl> info . acc_format = p -> acc_format ;
static void ovfx2_pkt_scan ( struct gspca_dev * gspca_dev , <nl> gspca_frame_add ( gspca_dev , INTER_PACKET , data , len ); <nl>  <nl> /* A short read signals EOF */ <nl> - if ( len < OVFX2_BULK_SIZE ) { <nl> + if ( len < gspca_dev -> cam . bulk_size ) { <nl> /* If the frame is short , and it is one of the first ones <nl> the sensor and bridge are still syncing , so drop it . */ <nl> if ( sd -> first_frame ) {
nv_printk_ ( struct nouveau_object * object , int level , const char * fmt , ...) <nl> char obuf [ 64 ], * ofmt = ""; <nl>  <nl> if ( object -> engine ) { <nl> - snprintf ( obuf , sizeof ( obuf ), "[ 0x % 08x ][% p ]", <nl> - nv_hclass ( object ), object ); <nl> + snprintf ( obuf , sizeof ( obuf ), "[ 0x % 08x ]", <nl> + nv_hclass ( object )); <nl> ofmt = obuf ; <nl> subdev = object -> engine ; <nl> device = object -> engine ;
restart : <nl> if ( radix_tree_deref_retry ( entry )) <nl> goto restart ; <nl>  <nl> - irq = create_irq (); <nl> + irq = irq_alloc_desc ( numa_node_id ()); <nl> if ( unlikely ( irq < 0 )) { <nl> pr_err (" no more free IRQs , bailing ..\ n "); <nl> break ; <nl> } <nl>  <nl> + activate_irq ( irq ); <nl> + <nl> pr_info (" Setting up a chained VIRQ from % d -> % d \ n ", <nl> irq , entry -> pirq ); <nl> 
build_unc_path_to_root ( const struct smb_vol * vol , <nl> pos = full_path + unc_len ; <nl>  <nl> if ( pplen ) { <nl> - * pos ++ = CIFS_DIR_SEP ( cifs_sb ); <nl> - strncpy ( pos , vol -> prepath , pplen ); <nl> + * pos = CIFS_DIR_SEP ( cifs_sb ); <nl> + strncpy ( pos + 1 , vol -> prepath , pplen ); <nl> pos += pplen ; <nl> } <nl> 
static int __exit usba_udc_remove ( struct platform_device * pdev ) <nl> gpio_free ( udc -> vbus_pin ); <nl>  <nl> free_irq ( udc -> irq , udc ); <nl> + kfree ( usba_ep ); <nl> iounmap ( udc -> fifo ); <nl> iounmap ( udc -> regs ); <nl> clk_put ( udc -> hclk );
static inline bool __rpc_copy_addr6 ( struct sockaddr * dst , <nl>  <nl> dsin6 -> sin6_family = ssin6 -> sin6_family ; <nl> dsin6 -> sin6_addr = ssin6 -> sin6_addr ; <nl> + dsin6 -> sin6_scope_id = ssin6 -> sin6_scope_id ; <nl> return true ; <nl> } <nl> # else /* !( IS_ENABLED ( CONFIG_IPV6 ) */
static int ttm_buffer_object_transfer ( struct ttm_buffer_object * bo , <nl> INIT_LIST_HEAD (& fbo -> lru ); <nl> INIT_LIST_HEAD (& fbo -> swap ); <nl> fbo -> vm_node = NULL ; <nl> + atomic_set (& fbo -> cpu_writers , 0 ); <nl>  <nl> fbo -> sync_obj = driver -> sync_obj_ref ( bo -> sync_obj ); <nl> kref_init (& fbo -> list_kref );
static bool igbvf_clean_rx_irq ( struct igbvf_adapter * adapter , <nl> dma_unmap_single (& pdev -> dev , buffer_info -> dma , <nl> adapter -> rx_ps_hdr_size , <nl> DMA_FROM_DEVICE ); <nl> + buffer_info -> dma = 0 ; <nl> skb_put ( skb , hlen ); <nl> } <nl> 
static netdev_tx_t tg3_start_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> struct iphdr * iph ; <nl> u32 tcp_opt_len , hdr_len ; <nl>  <nl> - if ( skb_header_cloned ( skb ) && <nl> - pskb_expand_head ( skb , 0 , 0 , GFP_ATOMIC )) <nl> + if ( skb_cow_head ( skb , 0 )) <nl> goto drop ; <nl>  <nl> iph = ip_hdr ( skb );
int snd_soc_dapm_stream_event ( struct snd_soc_codec * codec , <nl> { <nl> struct snd_soc_dapm_widget * w ; <nl>  <nl> + if ( stream == NULL ) <nl> + return 0 ; <nl> + <nl> mutex_lock (& codec -> mutex ); <nl> list_for_each_entry ( w , & codec -> dapm_widgets , list ) <nl> {
retry : <nl> * and pretend the write failed ... */ <nl> ext3_truncate_failed_direct_write ( inode ); <nl> ret = PTR_ERR ( handle ); <nl> + if ( inode -> i_nlink ) <nl> + ext3_orphan_del ( NULL , inode ); <nl> goto out ; <nl> } <nl> if ( inode -> i_nlink )
static int __fimc_md_create_flite_source_links ( struct fimc_md * fmd ) <nl> { <nl> struct media_entity * source , * sink ; <nl> unsigned int flags = MEDIA_LNK_FL_ENABLED ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> for ( i = 0 ; i < FIMC_LITE_MAX_DEVS ; i ++) { <nl> struct fimc_lite * fimc = fmd -> fimc_lite [ i ];
static int gfs2_write_end ( struct file * file , struct address_space * mapping , <nl> } <nl>  <nl> brelse ( dibh ); <nl> - gfs2_trans_end ( sdp ); <nl> failed : <nl> + gfs2_trans_end ( sdp ); <nl> if ( al ) { <nl> gfs2_inplace_release ( ip ); <nl> gfs2_quota_unlock ( ip );
void __init paging_init ( void ) <nl> map_mem (); <nl> fixup_executable (); <nl>  <nl> - /* <nl> - * Finally flush the caches and tlb to ensure that we ' re in a <nl> - * consistent state . <nl> - */ <nl> - flush_cache_all (); <nl> - flush_tlb_all (); <nl> - <nl> /* allocate the zero page . */ <nl> zero_page = early_alloc ( PAGE_SIZE ); <nl> 
static int bnx2x_issue_dmae_with_comp ( struct bnx2x * bp , <nl> struct dmae_command * dmae ) <nl> { <nl> u32 * wb_comp = bnx2x_sp ( bp , wb_comp ); <nl> - int cnt = CHIP_REV_IS_SLOW ( bp ) ? ( 400000 ) : 40 ; <nl> + int cnt = CHIP_REV_IS_SLOW ( bp ) ? ( 400000 ) : 4000 ; <nl> int rc = 0 ; <nl>  <nl> DP ( BNX2X_MSG_OFF , " data before [ 0x % 08x 0x % 08x 0x % 08x 0x % 08x ]\ n ",
void sb1250_time_init ( void ) <nl> /* Disable the timer and set up the count */ <nl> __raw_writeq ( 0 , IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_CFG ))); <nl> # ifdef CONFIG_SIMULATION <nl> - __raw_writeq ( 50000 / HZ , <nl> + __raw_writeq (( 50000 / HZ ) - 1 , <nl> IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_INIT ))); <nl> # else <nl> - __raw_writeq ( 1000000 / HZ , <nl> + __raw_writeq (( V_SCD_TIMER_FREQ / HZ ) - 1 , <nl> IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_INIT ))); <nl> # endif <nl> 
mwifiex_config_scan ( struct mwifiex_private * priv , <nl> wildcard_ssid_tlv -> max_ssid_length = <nl> IEEE80211_MAX_SSID_LEN ; <nl>  <nl> + if (! memcmp ( user_scan_in -> ssid_list [ i ]. ssid , <nl> + " DIRECT -", 7 )) <nl> + wildcard_ssid_tlv -> max_ssid_length = 0xfe ; <nl> + <nl> memcpy ( wildcard_ssid_tlv -> ssid , <nl> user_scan_in -> ssid_list [ i ]. ssid , ssid_len ); <nl> 
static int __init mbcs_init ( void ) <nl> { <nl> int rv ; <nl>  <nl> + if (! ia64_platform_is (" sn2 ")) <nl> + return - ENODEV ; <nl> + <nl> // Put driver into chrdevs []. Get major number . <nl> rv = register_chrdev ( mbcs_major , DEVICE_NAME , & mbcs_ops ); <nl> if ( rv < 0 ) {
static struct mtd_partition * newpart ( char * s , <nl> s ++; <nl> } else { <nl> size = memparse ( s , & s ); <nl> - if ( size < PAGE_SIZE ) { <nl> - printk ( KERN_ERR ERRP " partition size too small (% llx )\ n ", <nl> - size ); <nl> + if (! size ) { <nl> + printk ( KERN_ERR ERRP " partition has size 0 \ n "); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> }
int i915_gpu_idle ( struct drm_device * dev ) <nl> /* Is the device fubar ? */ <nl> if ( WARN_ON (! list_empty (& ring -> gpu_write_list ))) <nl> return - EBUSY ; <nl> + <nl> + ret = i915_switch_context ( ring , NULL , DEFAULT_CONTEXT_ID ); <nl> + if ( ret ) <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static int fuse_dentry_revalidate ( struct dentry * entry , struct nameidata * nd ) <nl> { <nl> struct inode * inode ; <nl>  <nl> - if ( nd -> flags & LOOKUP_RCU ) <nl> + if ( nd && nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl>  <nl> inode = entry -> d_inode ;
static int submit_extent_page ( int rw , struct extent_io_tree * tree , <nl> return 0 ; <nl> } <nl> } <nl> - nr = min_t ( int , max_pages , bio_get_nr_vecs ( bdev )); <nl> + nr = bio_get_nr_vecs ( bdev ); <nl> bio = extent_bio_alloc ( bdev , sector , nr , GFP_NOFS | __GFP_HIGH ); <nl> if (! bio ) { <nl> printk (" failed to allocate bio nr % d \ n ", nr );
static int ep93xx_gpio_irq_type ( struct irq_data * d , unsigned int type ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - __irq_set_handler_locked ( d -> irq , handler ); <nl> + irq_set_handler_locked ( d , handler ); <nl>  <nl> gpio_int_enabled [ port ] |= port_mask ; <nl> 
static int img_spfi_start_dma ( struct spi_master * master , <nl> dma_async_issue_pending ( spfi -> rx_ch ); <nl> } <nl>  <nl> + spfi_start ( spfi ); <nl> + <nl> if ( xfer -> tx_buf ) { <nl> spfi -> tx_dma_busy = true ; <nl> dmaengine_submit ( txdesc ); <nl> dma_async_issue_pending ( spfi -> tx_ch ); <nl> } <nl>  <nl> - spfi_start ( spfi ); <nl> - <nl> return 1 ; <nl>  <nl> stop_dma :
static unsigned int ip_vs_post_routing ( unsigned int hooknum , <nl> { <nl> if (!((* pskb )-> ipvs_property )) <nl> return NF_ACCEPT ; <nl> - <nl> /* The packet was sent from IPVS , exit this chain */ <nl> - (* okfn )(* pskb ); <nl> - <nl> - return NF_STOLEN ; <nl> + return NF_STOP ; <nl> } <nl>  <nl> u16 ip_vs_checksum_complete ( struct sk_buff * skb , int offset )
extern long __put_user_asm_l ( void *, long ); <nl> extern long __put_user_asm_q ( void *, long ); <nl> extern void __put_user_unknown ( void ); <nl>  <nl> + extern long __strnlen_user ( const char * __s , long __n ); <nl> + <nl> # endif /* __ASM_SH_UACCESS_64_H */
static int hdmi_remove ( struct platform_device * pdev ) <nl>  <nl> pm_runtime_disable ( dev ); <nl>  <nl> - free_irq ( hdata -> irq , hdata ); <nl> + free_irq ( hdata -> irq , ctx ); <nl>  <nl>  <nl> /* hdmiphy i2c driver */
void rt2x00rfkill_allocate ( struct rt2x00_dev * rt2x00dev ) <nl>  <nl> void rt2x00rfkill_free ( struct rt2x00_dev * rt2x00dev ) <nl> { <nl> - if (! test_bit ( RFKILL_STATE_ALLOCATED , & rt2x00dev -> flags )) <nl> + if (! test_bit ( RFKILL_STATE_ALLOCATED , & rt2x00dev -> rfkill_state )) <nl> return ; <nl>  <nl> cancel_delayed_work_sync (& rt2x00dev -> rfkill_work );
static int kdb_ll ( int argc , const char ** argv ) <nl> while ( va ) { <nl> char buf [ 80 ]; <nl>  <nl> + if ( KDB_FLAG ( CMD_INTERRUPT )) <nl> + return 0 ; <nl> + <nl> sprintf ( buf , "% s " kdb_machreg_fmt "\ n ", command , va ); <nl> diag = kdb_parse ( buf ); <nl> if ( diag )
static int clk_fetch_parent_index ( struct clk_core * core , <nl> { <nl> int i ; <nl>  <nl> + if (! parent ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * find index of new parent clock using cached parent ptrs , <nl> * or if not yet cached , use string name comparison and cache
static int gprs_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> dev -> name , err ); <nl> dev -> stats . tx_aborted_errors ++; <nl> dev -> stats . tx_errors ++; <nl> - dev_kfree_skb ( skb ); <nl> } else { <nl> dev -> stats . tx_packets ++; <nl> dev -> stats . tx_bytes += len ;
static struct perf_pmu * pmu_lookup ( const char * name ) <nl> LIST_HEAD ( aliases ); <nl> __u32 type ; <nl>  <nl> + /* No support for intel_bts or intel_pt so disallow them */ <nl> + if (! strcmp ( name , " intel_bts ") || ! strcmp ( name , " intel_pt ")) <nl> + return NULL ; <nl> + <nl> /* <nl> * The pmu data we store & need consists of the pmu <nl> * type value and format definitions . Load both right
static int remove_and_add_spares ( struct mddev * mddev , <nl> ! test_bit ( Bitmap_sync , & rdev -> flags ))) <nl> continue ; <nl>  <nl> - rdev -> recovery_offset = 0 ; <nl> + if ( rdev -> saved_raid_disk < 0 ) <nl> + rdev -> recovery_offset = 0 ; <nl> if ( mddev -> pers -> <nl> hot_add_disk ( mddev , rdev ) == 0 ) { <nl> if ( sysfs_link_rdev ( mddev , rdev ))
static int wacom_set_device_mode ( struct hid_device * hdev , int report_id , <nl> if ( error >= 0 ) <nl> error = wacom_get_report ( hdev , HID_FEATURE_REPORT , <nl> rep_data , length , 1 ); <nl> - } while (( error < 0 || rep_data [ 1 ] != mode ) && limit ++ < WAC_MSG_RETRIES ); <nl> + } while ( error >= 0 && rep_data [ 1 ] != mode && limit ++ < WAC_MSG_RETRIES ); <nl>  <nl> kfree ( rep_data ); <nl> 
SYSCALL_DEFINE3 ( sched_setattr , pid_t , pid , struct sched_attr __user *, uattr , <nl> if ( retval ) <nl> return retval ; <nl>  <nl> + if ( attr . sched_policy < 0 ) <nl> + return - EINVAL ; <nl> + <nl> rcu_read_lock (); <nl> retval = - ESRCH ; <nl> p = find_process_by_pid ( pid );
static int dlfb_realloc_framebuffer ( struct dlfb_data * dev , struct fb_info * info ) <nl> int new_len ; <nl> unsigned char * old_fb = info -> screen_base ; <nl> unsigned char * new_fb ; <nl> - unsigned char * new_back = 0 ; <nl> + unsigned char * new_back = NULL ; <nl>  <nl> pr_warn (" Reallocating framebuffer . Addresses will change !\ n "); <nl> 
static long hcall_vphn ( unsigned long cpu , __be32 * associativity ) <nl> long retbuf [ PLPAR_HCALL9_BUFSIZE ] = { 0 }; <nl> u64 flags = 1 ; <nl> int hwcpu = get_hard_smp_processor_id ( cpu ); <nl> + int i ; <nl>  <nl> rc = plpar_hcall9 ( H_HOME_NODE_ASSOCIATIVITY , retbuf , flags , hwcpu ); <nl> + for ( i = 0 ; i < 6 ; i ++) <nl> + retbuf [ i ] = cpu_to_be64 ( retbuf [ i ]); <nl> vphn_unpack_associativity ( retbuf , associativity ); <nl>  <nl> return rc ;
postchange : <nl> current_multiplier ); <nl> } <nl> # endif <nl> + if ( err ) <nl> + freqs . new = freqs . old ; <nl> + <nl> cpufreq_notify_transition ( policy , & freqs , CPUFREQ_POSTCHANGE ); <nl> return err ; <nl> }
int hists__browse ( struct hists * self , const char * helpline , const char * ev_name ) <nl>  <nl> switch ( key ) { <nl> case ' a ': <nl> - if ( browser -> selection -> map == NULL && <nl> + if ( browser -> selection -> map == NULL || <nl> browser -> selection -> map -> dso -> annotate_warned ) <nl> continue ; <nl> goto do_annotate ;
static void request_key_auth_destroy ( struct key * key ) <nl> kenter ("{% d }", key -> serial ); <nl>  <nl> key_put ( rka -> target_key ); <nl> + kfree ( rka ); <nl>  <nl> } /* end request_key_auth_destroy () */ <nl> 
# undef __get_str <nl>  <nl> # undef TP_printk <nl> -# define TP_printk ( fmt , args ...) "% s , % s \ n ", # fmt , __stringify ( args ) <nl> +# define TP_printk ( fmt , args ...) "\"% s \", % s \ n ", fmt , __stringify ( args ) <nl>  <nl> # undef TP_fast_assign <nl> # define TP_fast_assign ( args ...) args
hpet_ioctl_common ( struct hpet_dev * devp , int cmd , unsigned long arg , <nl> break ; <nl> case HPET_INFO : <nl> { <nl> + memset ( info , 0 , sizeof (* info )); <nl> if ( devp -> hd_ireqfreq ) <nl> info -> hi_ireqfreq = <nl> hpet_time_div ( hpetp , devp -> hd_ireqfreq ); <nl> - else <nl> - info -> hi_ireqfreq = 0 ; <nl> info -> hi_flags = <nl> readq (& timer -> hpet_config ) & Tn_PER_INT_CAP_MASK ; <nl> info -> hi_hpet = hpetp -> hp_which ;
static int __devinit cciss_init_one ( struct pci_dev * pdev , <nl> h -> scatter_list = kmalloc ( h -> max_commands * <nl> sizeof ( struct scatterlist *), <nl> GFP_KERNEL ); <nl> + if (! h -> scatter_list ) <nl> + goto clean4 ; <nl> + <nl> for ( k = 0 ; k < h -> nr_cmds ; k ++) { <nl> h -> scatter_list [ k ] = kmalloc ( sizeof ( struct scatterlist ) * <nl> h -> maxsgentries ,
int ext4_mb_find_by_goal ( struct ext4_allocation_context * ac , <nl> int max ; <nl> int err ; <nl> struct ext4_sb_info * sbi = EXT4_SB ( ac -> ac_sb ); <nl> + struct ext4_group_info * grp = ext4_get_group_info ( ac -> ac_sb , group ); <nl> struct ext4_free_extent ex ; <nl>  <nl> if (!( ac -> ac_flags & EXT4_MB_HINT_TRY_GOAL )) <nl> return 0 ; <nl> + if ( grp -> bb_free == 0 ) <nl> + return 0 ; <nl>  <nl> err = ext4_mb_load_buddy ( ac -> ac_sb , group , e4b ); <nl> if ( err )
static int adp5588_gpio_probe ( struct i2c_client * client , <nl> } <nl>  <nl> dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> - if ( dev == NULL ) { <nl> - dev_err (& client -> dev , " failed to alloc memory \ n "); <nl> + if ( dev == NULL ) <nl> return - ENOMEM ; <nl> - } <nl>  <nl> dev -> client = client ; <nl> 
static int ab8500_usb_probe ( struct platform_device * pdev ) <nl> return err ; <nl> } <nl>  <nl> - /* Phy tuning values for AB8500 */ <nl> - if (! is_ab8500_2p0_or_earlier ( ab -> ab8500 )) { <nl> + /* Phy tuning values for AB8500 > v2 . 0 */ <nl> + if ( is_ab8500 ( ab -> ab8500 ) && ! is_ab8500_2p0_or_earlier ( ab -> ab8500 )) { <nl> /* Enable the PBT / Bank 0x12 access */ <nl> err = abx500_set_register_interruptible ( ab -> dev , <nl> AB8500_DEVELOPMENT , AB8500_BANK12_ACCESS , 0x01 );
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
long drm_ioctl ( struct file * filp , <nl> goto err_i1 ; <nl> } <nl> } <nl> + if ( asize > usize ) <nl> + memset ( kdata + usize , 0 , asize - usize ); <nl> } <nl>  <nl> if ( cmd & IOC_IN ) {
static const char * alc_get_line_out_pfx ( struct alc_spec * spec , int ch , <nl> case AUTO_PIN_SPEAKER_OUT : <nl> if ( cfg -> line_outs == 1 ) <nl> return " Speaker "; <nl> + if ( cfg -> line_outs == 2 ) <nl> + return ch ? " Bass Speaker " : " Speaker "; <nl> break ; <nl> case AUTO_PIN_HP_OUT : <nl> /* for multi - io case , only the primary out */
static void macb_handle_link_change ( struct net_device * dev ) <nl>  <nl> if ( phydev -> duplex ) <nl> reg |= MACB_BIT ( FD ); <nl> - if ( phydev -> speed ) <nl> + if ( phydev -> speed == SPEED_100 ) <nl> reg |= MACB_BIT ( SPD ); <nl>  <nl> macb_writel ( bp , NCFGR , reg );
next_button : <nl> if ( dev -> num_button_polling_addresses ) { <nl> memset ( dev -> button_polling_last_values , 0 , <nl> EM28XX_NUM_BUTTON_ADDRESSES_MAX ); <nl> - INIT_DELAYED_WORK (& dev -> buttons_query_work , <nl> - em28xx_query_buttons ); <nl> schedule_delayed_work (& dev -> buttons_query_work , <nl> msecs_to_jiffies ( dev -> button_polling_interval )); <nl> } <nl> static int em28xx_ir_init ( struct em28xx * dev ) <nl> } <nl>  <nl> kref_get (& dev -> ref ); <nl> + INIT_DELAYED_WORK (& dev -> buttons_query_work , em28xx_query_buttons ); <nl>  <nl> if ( dev -> board . buttons ) <nl> em28xx_init_buttons ( dev );
exit_snd_soc : <nl> exit_free_irq : <nl> free_irq ( irq , master ); <nl> exit_fsib : <nl> + pm_runtime_disable (& pdev -> dev ); <nl> fsi_stream_remove (& master -> fsib ); <nl> exit_fsia : <nl> fsi_stream_remove (& master -> fsia ); <nl> exit_iounmap : <nl> iounmap ( master -> base ); <nl> - pm_runtime_disable (& pdev -> dev ); <nl> exit_kfree : <nl> kfree ( master ); <nl> master = NULL ;
static int t7l66xb_probe ( struct platform_device * dev ) <nl> t7l66xb_cells [ T7L66XB_CELL_NAND ]. data_size = <nl> sizeof ( t7l66xb_cells [ T7L66XB_CELL_NAND ]); <nl>  <nl> + t7l66xb_cells [ T7L66XB_CELL_MMC ]. platform_data = <nl> + & t7l66xb_cells [ T7L66XB_CELL_MMC ]; <nl> + t7l66xb_cells [ T7L66XB_CELL_MMC ]. data_size = <nl> + sizeof ( t7l66xb_cells [ T7L66XB_CELL_MMC ]); <nl> + <nl> ret = mfd_add_devices (& dev -> dev , dev -> id , <nl> t7l66xb_cells , ARRAY_SIZE ( t7l66xb_cells ), <nl> iomem , t7l66xb -> irq_base );
unsigned int qe_get_num_of_snums ( void ) <nl> if (( num_of_snums < 28 ) || ( num_of_snums > QE_NUM_OF_SNUM )) { <nl> /* No QE ever has fewer than 28 SNUMs */ <nl> pr_err (" QE : number of snum is invalid \ n "); <nl> + of_node_put ( qe ); <nl> return - EINVAL ; <nl> } <nl> }
static int __init fusb300_probe ( struct platform_device * pdev ) <nl>  <nl> fusb300 -> ep0_req = fusb300_alloc_request (& fusb300 -> ep [ 0 ]-> ep , <nl> GFP_KERNEL ); <nl> - if ( fusb300 -> ep0_req == NULL ) <nl> + if ( fusb300 -> ep0_req == NULL ) { <nl> + ret = - ENOMEM ; <nl> goto clean_up3 ; <nl> + } <nl>  <nl> init_controller ( fusb300 ); <nl> ret = usb_add_gadget_udc (& pdev -> dev , & fusb300 -> gadget );
static inline int get_ni_value ( int mclk , int rate ) <nl> if ( ni_div [ i ]. mclk >= mclk ) <nl> break ; <nl> } <nl> + if ( i == ARRAY_SIZE ( ni_div )) <nl> + return - EINVAL ; <nl>  <nl> switch ( rate ) { <nl> case 8000 :
static void atombios_crtc_program_ss ( struct drm_crtc * crtc , <nl> case ATOM_PPLL_INVALID : <nl> return ; <nl> } <nl> - args . v2 . ucEnable = enable ; <nl> + args . v3 . ucEnable = enable ; <nl> if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK )) <nl> args . v3 . ucEnable = ATOM_DISABLE ; <nl> } else if ( ASIC_IS_DCE4 ( rdev )) {
static int setup_routing_entry ( struct kvm_irq_routing_table * rt , <nl> */ <nl> hlist_for_each_entry ( ei , n , & rt -> map [ ue -> gsi ], link ) <nl> if ( ei -> type == KVM_IRQ_ROUTING_MSI || <nl> + ue -> type == KVM_IRQ_ROUTING_MSI || <nl> ue -> u . irqchip . irqchip == ei -> irqchip . irqchip ) <nl> return r ; <nl> 
void btrfs_add_ordered_operation ( struct btrfs_trans_handle * trans , <nl> * if this file hasn ' t been changed since the last transaction <nl> * commit , we can safely return without doing anything <nl> */ <nl> - if ( last_mod < root -> fs_info -> last_trans_committed ) <nl> + if ( last_mod <= root -> fs_info -> last_trans_committed ) <nl> return ; <nl>  <nl> spin_lock (& root -> fs_info -> ordered_root_lock );
int __init option_setup ( char * str ) <nl>  <nl> TRACE2 ((" option_setup () str % s \ n ", str ? str :" NULL ")); <nl>  <nl> - while ( cur && isdigit (* cur ) && i <= MAXHA ) { <nl> + while ( cur && isdigit (* cur ) && i < MAXHA ) { <nl> ints [ i ++] = simple_strtoul ( cur , NULL , 0 ); <nl> if (( cur = strchr ( cur , ',')) != NULL ) cur ++; <nl> }
int __init init_dmars ( void ) <nl> deferred_flush = kzalloc ( g_num_of_iommus * <nl> sizeof ( struct deferred_flush_tables ), GFP_KERNEL ); <nl> if (! deferred_flush ) { <nl> - kfree ( g_iommus ); <nl> ret = - ENOMEM ; <nl> goto error ; <nl> }
static int pxa2xx_pcm_hw_free ( struct snd_pcm_substream * substream ) <nl> return 0 ; <nl> } <nl>  <nl> - struct snd_pcm_ops pxa2xx_pcm_ops = { <nl> + static struct snd_pcm_ops pxa2xx_pcm_ops = { <nl> . open = __pxa2xx_pcm_open , <nl> . close = __pxa2xx_pcm_close , <nl> . ioctl = snd_pcm_lib_ioctl ,
static int ata_dev_read_id ( struct ata_port * ap , struct ata_device * dev , <nl> err_out : <nl> printk ( KERN_WARNING " ata % u : dev % u failed to IDENTIFY (% s )\ n ", <nl> ap -> id , dev -> devno , reason ); <nl> - kfree ( id ); <nl> return rc ; <nl> } <nl> 
static int arizona_dai_set_sysclk ( struct snd_soc_dai * dai , <nl> routes [ 1 ]. source = arizona_dai_clk_str ( clk_id ); <nl> snd_soc_dapm_add_routes (& codec -> dapm , routes , ARRAY_SIZE ( routes )); <nl>  <nl> + dai_priv -> clk = clk_id ; <nl> + <nl> return snd_soc_dapm_sync (& codec -> dapm ); <nl> } <nl> 
void of_gpiochip_add ( struct gpio_chip * chip ) <nl> void of_gpiochip_remove ( struct gpio_chip * chip ) <nl> { <nl> gpiochip_remove_pin_ranges ( chip ); <nl> - <nl> - if ( chip -> of_node ) <nl> - of_node_put ( chip -> of_node ); <nl> + of_node_put ( chip -> of_node ); <nl> }
static void stop_nop_trace ( struct trace_array * tr ) <nl>  <nl> static void nop_trace_init ( struct trace_array * tr ) <nl> { <nl> + int cpu ; <nl> ctx_trace = tr ; <nl>  <nl> + for_each_online_cpu ( cpu ) <nl> + tracing_reset ( tr -> data [ cpu ]); <nl> + <nl> if ( tr -> ctrl ) <nl> start_nop_trace ( tr ); <nl> }
DECLARE_PCI_FIXUP_HEADER ( PCI_VENDOR_ID_ATI , PCI_DEVICE_ID_ATI_SBX00_SMBUS , <nl>  <nl> # if defined ( CONFIG_PCI ) && defined ( CONFIG_NUMA ) <nl> /* Set correct numa_node information for AMD NB functions */ <nl> - static void __init quirk_amd_nb_node ( struct pci_dev * dev ) <nl> + static void __devinit quirk_amd_nb_node ( struct pci_dev * dev ) <nl> { <nl> struct pci_dev * nb_ht ; <nl> unsigned int devfn ;
static void __init sanity_check_meminfo ( void ) <nl> bank -> size = VMALLOC_MIN - __va ( bank -> start ); <nl> } <nl> # else <nl> + bank -> highmem = highmem ; <nl> + <nl> /* <nl> * Check whether this memory bank would entirely overlap <nl> * the vmalloc area .
void handle_ra_miss ( struct address_space * mapping , <nl> { <nl> ra -> flags |= RA_FLAG_MISS ; <nl> ra -> flags &= ~ RA_FLAG_INCACHE ; <nl> + ra -> cache_hit = 0 ; <nl> } <nl>  <nl> /*
static inline u32 skb_mstamp_us_delta ( const struct skb_mstamp * t1 , <nl> return delta_us ; <nl> } <nl>  <nl> + static inline bool skb_mstamp_after ( const struct skb_mstamp * t1 , <nl> + const struct skb_mstamp * t0 ) <nl> +{ <nl> + s32 diff = t1 -> stamp_jiffies - t0 -> stamp_jiffies ; <nl> + <nl> + if (! diff ) <nl> + diff = t1 -> stamp_us - t0 -> stamp_us ; <nl> + return diff > 0 ; <nl> +} <nl>  <nl> /** <nl> * struct sk_buff - socket buffer
static inline pgtable_t pte_alloc_one ( struct mm_struct * mm , unsigned long addres <nl> { <nl> struct page * pte ; <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO , 0 ); <nl> + if (! pte ) <nl> + return NULL ; <nl> pgtable_page_ctor ( pte ); <nl> return pte ; <nl> }
static int mmc_blk_ioctl_cmd ( struct block_device * bdev , <nl> md = mmc_blk_get ( bdev -> bd_disk ); <nl> if (! md ) { <nl> err = - EINVAL ; <nl> - goto cmd_done ; <nl> + goto cmd_err ; <nl> } <nl>  <nl> card = md -> queue . card ; <nl> cmd_rel_host : <nl>  <nl> cmd_done : <nl> mmc_blk_put ( md ); <nl> + cmd_err : <nl> kfree ( idata -> buf ); <nl> kfree ( idata ); <nl> return err ;
static int __init d40_lcla_allocate ( struct d40_base * base ) <nl>  <nl> d40_err ( base -> dev , " Failed to allocate % d pages .\ n ", <nl> base -> lcla_pool . pages ); <nl> + ret = - ENOMEM ; <nl>  <nl> for ( j = 0 ; j < i ; j ++) <nl> free_pages ( page_list [ j ], base -> lcla_pool . pages );
static int gpmc_probe_dt ( struct platform_device * pdev ) <nl> of_node_cmp ( child -> name , " nor ") == 0 ) <nl> ret = gpmc_probe_generic_child ( pdev , child ); <nl>  <nl> - if ( ret < 0 ) { <nl> + if ( WARN ( ret < 0 , "% s : probing gpmc child % s failed \ n ", <nl> + __func__ , child -> full_name )) <nl> of_node_put ( child ); <nl> - return ret ; <nl> - } <nl> } <nl>  <nl> return 0 ;
static int wm8995_probe ( struct snd_soc_codec * codec ) <nl>  <nl> if ( ret != 0x8995 ) { <nl> dev_err ( codec -> dev , " Invalid device ID : %# x \ n ", ret ); <nl> + ret = - EINVAL ; <nl> goto err_reg_enable ; <nl> } <nl> 
ohci_enable_phys_dma ( struct fw_card * card , int node_id , int generation ) <nl> 1 << ( node_id - 32 )); <nl> } <nl> flush_writes ( ohci ); <nl> - <nl> - spin_unlock_irqrestore (& ohci -> lock , flags ); <nl> - <nl> out : <nl> + spin_unlock_irqrestore (& ohci -> lock , flags ); <nl> return retval ; <nl> } <nl> 
int rt2800_load_firmware ( struct rt2x00_dev * rt2x00dev , <nl> */ <nl> rt2800_register_write ( rt2x00dev , H2M_BBP_AGENT , 0 ); <nl> rt2800_register_write ( rt2x00dev , H2M_MAILBOX_CSR , 0 ); <nl> + if ( rt2x00_is_usb ( rt2x00dev )) <nl> + rt2800_register_write ( rt2x00dev , H2M_INT_SRC , 0 ); <nl> msleep ( 1 ); <nl>  <nl> return 0 ;
sctp_disposition_t sctp_sf_backbeat_8_3 ( const struct sctp_endpoint * ep , <nl> commands ); <nl>  <nl> hbinfo = ( sctp_sender_hb_info_t *) chunk -> skb -> data ; <nl> + /* Make sure that the length of the parameter is what we expect */ <nl> + if ( ntohs ( hbinfo -> param_hdr . length ) != <nl> + sizeof ( sctp_sender_hb_info_t )) { <nl> + return SCTP_DISPOSITION_DISCARD ; <nl> + } <nl> + <nl> from_addr = hbinfo -> daddr ; <nl> link = sctp_assoc_lookup_paddr ( asoc , & from_addr ); <nl> 
int usb_serial_register_drivers ( struct usb_serial_driver * const serial_drivers [] <nl>  <nl> /* we only set the reset_resume field if the serial_driver has one */ <nl> for ( sd = serial_drivers ; * sd ; ++ sd ) { <nl> - if ((* sd )-> reset_resume ) <nl> + if ((* sd )-> reset_resume ) { <nl> udriver -> reset_resume = usb_serial_reset_resume ; <nl> break ; <nl> + } <nl> } <nl>  <nl> rc = usb_register ( udriver );
static int nr_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> sax -> fsa_ax25 . sax25_family = AF_NETROM ; <nl> sax -> fsa_ax25 . sax25_ndigis = 1 ; <nl> sax -> fsa_ax25 . sax25_call = nr -> user_addr ; <nl> + memset ( sax -> fsa_digipeater , 0 , sizeof ( sax -> fsa_digipeater )); <nl> sax -> fsa_digipeater [ 0 ] = nr -> dest_addr ; <nl> * uaddr_len = sizeof ( struct full_sockaddr_ax25 ); <nl> } else {
int ip_recv_error ( struct sock * sk , struct msghdr * msg , int len , int * addr_len ) <nl> int err ; <nl> int copied ; <nl>  <nl> + WARN_ON_ONCE ( sk -> sk_family == AF_INET6 ); <nl> + <nl> err = - EAGAIN ; <nl> skb = sock_dequeue_err_skb ( sk ); <nl> if ( skb == NULL )
struct ieee80211_tx_info { <nl> } control ; <nl> struct { <nl> struct ieee80211_tx_rate rates [ IEEE80211_TX_MAX_RATES ]; <nl> - int ack_signal ; <nl> + s32 ack_signal ; <nl> u8 ampdu_ack_len ; <nl> u8 ampdu_len ; <nl> u8 antenna ; <nl> - /* 21 bytes free */ <nl> + void * status_driver_data [ 21 / sizeof ( void *)]; <nl> } status ; <nl> struct { <nl> struct ieee80211_tx_rate driver_rates [
no_firmware : <nl> "% s : please contact support @ connecttech . com \ n ", <nl> serial -> type -> description ); <nl> kfree ( result ); <nl> + kfree ( command ); <nl> return - ENODEV ; <nl>  <nl> no_command_private :
int ath9k_cmn_rx_skb_preprocess ( struct ath_common * common , <nl> { <nl> struct ath_hw * ah = common -> ah ; <nl>  <nl> + memset ( rx_status , 0 , sizeof ( struct ieee80211_rx_status )); <nl> if (! ath9k_rx_accept ( common , skb , rx_status , rx_stats , decrypt_error )) <nl> return - EINVAL ; <nl> 
match1 : <nl> ndoms_cur = 0 ; <nl> doms_new = & fallback_doms ; <nl> cpus_andnot ( doms_new [ 0 ], cpu_online_map , cpu_isolated_map ); <nl> - dattr_new = NULL ; <nl> + WARN_ON_ONCE ( dattr_new ); <nl> } <nl>  <nl> /* Build new domains */
static int wl1271_plt_init ( struct wl1271 * wl ) <nl> if ( ret < 0 ) <nl> goto out_free_memmap ; <nl>  <nl> + ret = wl1271_acx_sta_mem_cfg ( wl ); <nl> + if ( ret < 0 ) <nl> + goto out_free_memmap ; <nl> + <nl> /* Default fragmentation threshold */ <nl> ret = wl1271_acx_frag_threshold ( wl , wl -> conf . tx . frag_threshold ); <nl> if ( ret < 0 )
static int ax_probe ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_IRQ , 0 ); <nl> if ( res == NULL ) { <nl> dev_err (& pdev -> dev , " no IRQ specified \ n "); <nl> + ret = - ENXIO ; <nl> goto exit_mem ; <nl> } <nl> 
static int ext4_mb_init_per_dev_proc ( struct super_block * sb ) <nl> struct proc_dir_entry * proc ; <nl> char devname [ 64 ]; <nl>  <nl> + if ( proc_root_ext4 == NULL ) { <nl> + sbi -> s_mb_proc = NULL ; <nl> + return - EINVAL ; <nl> + } <nl> bdevname ( sb -> s_bdev , devname ); <nl> sbi -> s_mb_proc = proc_mkdir ( devname , proc_root_ext4 ); <nl> 
iwl_op_mode_mvm_start ( struct iwl_trans * trans , const struct iwl_cfg * cfg , <nl> } <nl> mvm -> sf_state = SF_UNINIT ; <nl> mvm -> low_latency_agg_frame_limit = 6 ; <nl> + mvm -> cur_ucode = IWL_UCODE_INIT ; <nl>  <nl> mutex_init (& mvm -> mutex ); <nl> mutex_init (& mvm -> d0i3_suspend_mutex );
static int zd1201_resume ( struct usb_interface * interface ) <nl> { <nl> struct zd1201 * zd = usb_get_intfdata ( interface ); <nl>  <nl> + if (! zd || ! zd -> dev ) <nl> + return - ENODEV ; <nl> + <nl> netif_device_attach ( zd -> dev ); <nl>  <nl> if ( zd -> was_enabled )
static const u32 cipher_suites [] = { <nl> }; <nl>  <nl> static const struct ieee80211_txrx_stypes <nl> - wilc_wfi_cfg80211_mgmt_types [ NL80211_IFTYPE_MAX ] = { <nl> + wilc_wfi_cfg80211_mgmt_types [ NUM_NL80211_IFTYPES ] = { <nl> [ NL80211_IFTYPE_STATION ] = { <nl> . tx = 0xffff , <nl> . rx = BIT ( IEEE80211_STYPE_ACTION >> 4 ) |
static int ioctl_send_response ( struct client * client , void * buffer ) <nl> if ( copy_from_user ( r -> data , u64_to_uptr ( request -> data ), <nl> r -> length )) { <nl> ret = - EFAULT ; <nl> + kfree ( r -> request ); <nl> goto out ; <nl> } <nl> fw_send_response ( client -> device -> card , r -> request ,
static int lpc18xx_pconf_set_i2c0 ( struct pinctrl_dev * pctldev , <nl> break ; <nl>  <nl> case PIN_CONFIG_INPUT_SCHMITT_ENABLE : <nl> - if ( param ) <nl> + if ( param_val ) <nl> * reg &= ~( LPC18XX_SCU_I2C0_ZIF << shift ); <nl> else <nl> * reg |= ( LPC18XX_SCU_I2C0_ZIF << shift ); <nl> static int lpc18xx_pconf_set_pin ( struct pinctrl_dev * pctldev , <nl> break ; <nl>  <nl> case PIN_CONFIG_INPUT_SCHMITT_ENABLE : <nl> - if ( param ) <nl> + if ( param_val ) <nl> * reg &= ~ LPC18XX_SCU_PIN_ZIF ; <nl> else <nl> * reg |= LPC18XX_SCU_PIN_ZIF ;
int snd_pcm_hw_refine ( struct snd_pcm_substream * substream , <nl> snd_mask_max (& params -> masks [ SNDRV_PCM_HW_PARAM_CHANNELS ])) { <nl> changed = substream -> ops -> ioctl ( substream , <nl> SNDRV_PCM_IOCTL1_FIFO_SIZE , params ); <nl> - if ( params < 0 ) <nl> + if ( changed < 0 ) <nl> return changed ; <nl> } <nl> }
static int sysfs_add_link ( struct dentry * parent , const char * name , struct kobj <nl> if (! error ) <nl> return 0 ; <nl>  <nl> + kobject_put ( target ); <nl> kfree ( sl -> link_name ); <nl> exit2 : <nl> kfree ( sl );
int drm_vblank_get ( struct drm_device * dev , int crtc ) <nl> unsigned long irqflags ; <nl> int ret = 0 ; <nl>  <nl> + if (! dev -> num_crtcs ) <nl> + return - EINVAL ; <nl> + <nl> if ( WARN_ON ( crtc >= dev -> num_crtcs )) <nl> return - EINVAL ; <nl> 
static struct config_item_type printer_func_type = { <nl>  <nl> static inline int gprinter_get_minor ( void ) <nl> { <nl> - return ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + int ret ; <nl> + <nl> + ret = ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + if ( ret >= PRINTER_MINORS ) { <nl> + ida_simple_remove (& printer_ida , ret ); <nl> + ret = - ENODEV ; <nl> + } <nl> + <nl> + return ret ; <nl> } <nl>  <nl> static inline void gprinter_put_minor ( int minor )
static int ath9k_start ( struct ieee80211_hw * hw ) <nl> DPRINTF ( sc , ATH_DBG_CONFIG , "% s : Starting driver with " <nl> " initial channel : % d MHz \ n ", __func__ , curchan -> center_freq ); <nl>  <nl> + memset (& sc -> sc_ht_info , 0 , sizeof ( struct ath_ht_info )); <nl> + <nl> /* setup initial channel */ <nl>  <nl> pos = ath_get_channel ( sc , curchan );
static int __devinit rtsx_probe ( struct pci_dev * pci , <nl> th = kthread_create ( rtsx_scan_thread , dev , " rtsx - scan "); <nl> if ( IS_ERR ( th )) { <nl> printk ( KERN_ERR " Unable to start the device - scanning thread \ n "); <nl> + complete (& dev -> scanning_done ); <nl> quiesce_and_remove_host ( dev ); <nl> err = PTR_ERR ( th ); <nl> goto errout ;
get_a_page : <nl> i_size_write ( inode , size ); <nl> inode -> i_mtime = inode -> i_atime = CURRENT_TIME ; <nl> mark_inode_dirty ( inode ); <nl> + set_bit ( QDF_REFRESH , & qd -> qd_flags ); <nl> return 0 ; <nl>  <nl> unlock_out :
static __init int samsung_gpiolib_init ( void ) <nl> s5p_register_gpioint_bank ( IRQ_GPIO_XA , 0 , IRQ_GPIO1_NR_GROUPS ); <nl> s5p_register_gpioint_bank ( IRQ_GPIO_XB , IRQ_GPIO1_NR_GROUPS , IRQ_GPIO2_NR_GROUPS ); <nl> # endif <nl> + } else { <nl> + WARN ( 1 , " Unknown SoC in gpio - samsung , no GPIOs added \ n "); <nl> + return - ENODEV ; <nl> } <nl>  <nl> return 0 ;
restart : <nl> goto out_free ; <nl> } <nl>  <nl> + if ( sk_filter ( other , skb ) < 0 ) { <nl> + /* Toss the packet but do not return any error to the sender */ <nl> + err = len ; <nl> + goto out_free ; <nl> + } <nl> + <nl> unix_state_lock ( other ); <nl> err = - EPERM ; <nl> if (! unix_may_send ( sk , other ))
int __devinit wl18xx_probe ( struct platform_device * pdev ) <nl> { <nl> struct wl1271 * wl ; <nl> struct ieee80211_hw * hw ; <nl> + struct wl18xx_priv * priv ; <nl>  <nl> - hw = wlcore_alloc_hw ( 0 ); <nl> + hw = wlcore_alloc_hw ( sizeof (* priv )); <nl> if ( IS_ERR ( hw )) { <nl> wl1271_error (" can ' t allocate hw "); <nl> return PTR_ERR ( hw );
again : <nl> key . offset = found_key . offset - 1 ; <nl> wc . replay_dest -> log_root = NULL ; <nl> free_extent_buffer ( log -> node ); <nl> + free_extent_buffer ( log -> commit_root ); <nl> kfree ( log ); <nl>  <nl> if ( found_key . offset == 0 )
static int pvscsi_queue_ring ( struct pvscsi_adapter * adapter , <nl> memcpy ( e -> cdb , cmd -> cmnd , e -> cdbLen ); <nl>  <nl> e -> tag = SIMPLE_QUEUE_TAG ; <nl> - if ( sdev -> tagged_supported && <nl> - ( cmd -> tag == HEAD_OF_QUEUE_TAG || <nl> - cmd -> tag == ORDERED_QUEUE_TAG )) <nl> - e -> tag = cmd -> tag ; <nl>  <nl> if ( cmd -> sc_data_direction == DMA_FROM_DEVICE ) <nl> e -> flags = PVSCSI_FLAG_CMD_DIR_TOHOST ;
done : <nl>  <nl> out : <nl> if ( unlikely ( frozen_buffer )) /* It ' s usually NULL */ <nl> - kfree ( frozen_buffer ); <nl> + jbd_slab_free ( frozen_buffer , bh -> b_size ); <nl>  <nl> JBUFFER_TRACE ( jh , " exit "); <nl> return error ;
int amdgpu_gem_userptr_ioctl ( struct drm_device * dev , void * data , <nl> AMDGPU_GEM_USERPTR_REGISTER )) <nl> return - EINVAL ; <nl>  <nl> - if (!( args -> flags & AMDGPU_GEM_USERPTR_ANONONLY ) || <nl> - !( args -> flags & AMDGPU_GEM_USERPTR_REGISTER )) { <nl> + if (!( args -> flags & AMDGPU_GEM_USERPTR_READONLY ) && ( <nl> + !( args -> flags & AMDGPU_GEM_USERPTR_ANONONLY ) || <nl> + !( args -> flags & AMDGPU_GEM_USERPTR_REGISTER ))) { <nl>  <nl> /* if we want to write to it we must require anonymous <nl> memory and install a MMU notifier */
pca963x_dt_init ( struct i2c_client * client , struct pca963x_chipdef * chip ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> for_each_child_of_node ( np , child ) { <nl> - struct led_info led ; <nl> + struct led_info led = {}; <nl> u32 reg ; <nl> int res ; <nl> 
static int f2fs_xattr_advise_set ( struct dentry * dentry , const char * name , <nl> return - EINVAL ; <nl>  <nl> F2FS_I ( inode )-> i_advise |= *( char *) value ; <nl> + mark_inode_dirty ( inode ); <nl> return 0 ; <nl> } <nl> 
asmlinkage long compat_sys_ppoll ( struct pollfd __user * ufds , <nl> } <nl>  <nl> if ( sigmask ) { <nl> - if ( sigsetsize |= sizeof ( compat_sigset_t )) <nl> + if ( sigsetsize != sizeof ( compat_sigset_t )) <nl> return - EINVAL ; <nl> if ( copy_from_user (& ss32 , sigmask , sizeof ( ss32 ))) <nl> return - EFAULT ;
int __init musb_platform_init ( struct musb * musb , void * board_data ) <nl>  <nl> usb_nop_xceiv_register (); <nl> musb -> xceiv = otg_get_transceiver (); <nl> - if (! musb -> xceiv ) <nl> + if (! musb -> xceiv ) { <nl> + gpio_free ( musb -> config -> gpio_vrsel ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> if ( ANOMALY_05000346 ) { <nl> bfin_write_USB_APHY_CALIB ( ANOMALY_05000346_value );
again : <nl> */ <nl> if ( type != TRANS_JOIN_NOLOCK && <nl> ! __sb_start_write ( root -> fs_info -> sb , SB_FREEZE_FS , false )) { <nl> - if ( type == TRANS_JOIN_FREEZE ) <nl> + if ( type == TRANS_JOIN_FREEZE ) { <nl> + kmem_cache_free ( btrfs_trans_handle_cachep , h ); <nl> return ERR_PTR (- EPERM ); <nl> + } <nl> sb_start_intwrite ( root -> fs_info -> sb ); <nl> } <nl> 
static void ext4_mb_group_or_file ( struct ext4_allocation_context * ac ) <nl> return ; <nl> } <nl>  <nl> + if ( sbi -> s_mb_group_prealloc <= 0 ) { <nl> + ac -> ac_flags |= EXT4_MB_STREAM_ALLOC ; <nl> + return ; <nl> + } <nl> + <nl> /* don ' t use group allocation for large files */ <nl> size = max ( size , isize ); <nl> if ( size > sbi -> s_mb_stream_request ) {
int skb_gro_receive ( struct sk_buff ** head , struct sk_buff * skb ) <nl> * NAPI_GRO_CB ( nskb ) = * NAPI_GRO_CB ( p ); <nl> skb_shinfo ( nskb )-> frag_list = p ; <nl> skb_shinfo ( nskb )-> gso_size = pinfo -> gso_size ; <nl> + pinfo -> gso_size = 0 ; <nl> skb_header_release ( p ); <nl> nskb -> prev = p ; <nl> 
static int nortel_pci_cor_reset ( struct orinoco_private * priv ) <nl> return 0 ; <nl> } <nl>  <nl> - int nortel_pci_hw_init ( struct nortel_pci_card * card ) <nl> + static int nortel_pci_hw_init ( struct nortel_pci_card * card ) <nl> { <nl> int i ; <nl> u32 reg ;
static ssize_t ac_read ( struct file * filp , char __user * buf , size_t count , loff_ <nl> struct mailbox mailbox ; <nl>  <nl> /* Got a packet for us */ <nl> + memset (& st_loc , 0 , sizeof ( st_loc )); <nl> ret = do_ac_read ( i , buf , & st_loc , & mailbox ); <nl> spin_unlock_irqrestore (& apbs [ i ]. mutex , flags ); <nl> set_current_state ( TASK_RUNNING );
out : <nl> if ( mem_tight != 0 ) <nl> cfs_memory_pressure_restore ( mpflag ); <nl>  <nl> - if ( crattr != NULL ) { <nl> - kfree ( crattr ); <nl> - } <nl> + kfree ( crattr ); <nl>  <nl> if ( rc != 0 ) { <nl> LASSERT ( req == NULL );
static void * raid5_takeover ( mddev_t * mddev ) <nl>  <nl> if ( mddev -> level == 1 ) <nl> return raid5_takeover_raid1 ( mddev ); <nl> + if ( mddev -> level == 4 ) { <nl> + mddev -> new_layout = ALGORITHM_PARITY_N ; <nl> + mddev -> new_level = 5 ; <nl> + return setup_conf ( mddev ); <nl> + } <nl>  <nl> return ERR_PTR (- EINVAL ); <nl> }
static int nokia_modem_probe ( struct device * dev ) <nl> return - ENOMEM ; <nl> } <nl> dev_set_drvdata ( dev , modem ); <nl> + modem -> device = dev ; <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> if (! irq ) {
nvc0_fifo_init ( struct nouveau_object * object ) <nl> nv_wr32 ( priv , 0x002a00 , 0xffffffff ); /* clears PFIFO . INTR bit 30 */ <nl> nv_wr32 ( priv , 0x002100 , 0xffffffff ); <nl> nv_wr32 ( priv , 0x002140 , 0x3fffffff ); <nl> + nv_wr32 ( priv , 0x002628 , 0x00000001 ); /* makes mthd 0x20 work */ <nl> return 0 ; <nl> } <nl> 
static int ext4_fill_super ( struct super_block * sb , void * data , int silent ) <nl> * of the filesystem . <nl> */ <nl> if ( le32_to_cpu ( es -> s_first_data_block ) >= ext4_blocks_count ( es )) { <nl> - ext4_msg ( sb , KERN_WARNING , " bad geometry : first data " <nl> + ext4_msg ( sb , KERN_WARNING , " bad geometry : first data " <nl> " block % u is beyond end of filesystem (% llu )", <nl> le32_to_cpu ( es -> s_first_data_block ), <nl> ext4_blocks_count ( es ));
int add_extent_mapping ( struct extent_map_tree * tree , <nl> rb = tree_insert (& tree -> map , em -> start , & em -> rb_node ); <nl> if ( rb ) { <nl> ret = - EEXIST ; <nl> - free_extent_map ( merge ); <nl> goto out ; <nl> } <nl> atomic_inc (& em -> refs );
static const struct snd_pci_quirk alc269_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x103c , 0x228a , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228b , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228c , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> + SND_PCI_QUIRK ( 0x103c , 0x228d , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22c5 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22c6 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ),
bte_result_t bte_unaligned_copy ( u64 src , u64 dest , u64 len , u64 mode ) <nl> } <nl>  <nl> /* temporary buffer used during unaligned transfers */ <nl> - bteBlock_unaligned = kmalloc ( len + 3 * L1_CACHE_BYTES , <nl> - GFP_KERNEL | GFP_DMA ); <nl> + bteBlock_unaligned = kmalloc ( len + 3 * L1_CACHE_BYTES , GFP_KERNEL ); <nl> if ( bteBlock_unaligned == NULL ) { <nl> return BTEFAIL_NOTAVAIL ; <nl> }
void local_touch_nmi ( void ) <nl> { <nl> __this_cpu_write ( last_nmi_rip , 0 ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( local_touch_nmi );
int snd_hda_create_dig_out_ctls ( struct hda_codec * codec , <nl> return - EBUSY ; <nl> } <nl> spdif = snd_array_new (& codec -> spdif_out ); <nl> + if (! spdif ) <nl> + return - ENOMEM ; <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> if (! kctl )
int kvm_vgic_create ( struct kvm * kvm , u32 type ) <nl> * emulation . So check this here again . KVM_CREATE_DEVICE does <nl> * the proper checks already . <nl> */ <nl> - if ( type == KVM_DEV_TYPE_ARM_VGIC_V2 && ! vgic -> can_emulate_gicv2 ) <nl> - return - ENODEV ; <nl> + if ( type == KVM_DEV_TYPE_ARM_VGIC_V2 && ! vgic -> can_emulate_gicv2 ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl>  <nl> /* <nl> * Any time a vcpu is run , vcpu_load is called which tries to grab the
unsigned int solo_eeprom_ewen ( struct solo_dev * solo_dev , int w_en ) <nl> __be16 solo_eeprom_read ( struct solo_dev * solo_dev , int loc ) <nl> { <nl> int read_cmd = loc | ( EE_READ_CMD << ADDR_LEN ); <nl> - unsigned short retval = 0 ; <nl> + u16 retval = 0 ; <nl> int i ; <nl>  <nl> solo_eeprom_cmd ( solo_dev , read_cmd );
static int mwifiex_cfg80211_start_ap ( struct wiphy * wiphy , <nl> case NL80211_HIDDEN_SSID_ZERO_CONTENTS : <nl> /* firmware doesn ' t support this type of hidden SSID */ <nl> default : <nl> + kfree ( bss_cfg ); <nl> return - EINVAL ; <nl> } <nl> 
struct simple_xattr * simple_xattr_alloc ( const void * value , size_t size ) <nl>  <nl> /* wrap around ? */ <nl> len = sizeof (* new_xattr ) + size ; <nl> - if ( len <= sizeof (* new_xattr )) <nl> + if ( len < sizeof (* new_xattr )) <nl> return NULL ; <nl>  <nl> new_xattr = kmalloc ( len , GFP_KERNEL );
static void __devinit pci_fixed_bar_fixup ( struct pci_dev * dev ) <nl> u32 size ; <nl> int i ; <nl>  <nl> + /* Must have extended configuration space */ <nl> + if ( dev -> cfg_size < PCIE_CAP_OFFSET + 4 ) <nl> + return ; <nl> + <nl> /* Fixup the BAR sizes for fixed BAR devices and make them unmoveable */ <nl> offset = fixed_bar_cap ( dev -> bus , dev -> devfn ); <nl> if (! offset || PCI_DEVFN ( 2 , 0 ) == dev -> devfn ||
static int ath10k_hw_scan ( struct ieee80211_hw * hw , <nl> arg . ssids [ i ]. len = req -> ssids [ i ]. ssid_len ; <nl> arg . ssids [ i ]. ssid = req -> ssids [ i ]. ssid ; <nl> } <nl> + } else { <nl> + arg . scan_ctrl_flags |= WMI_SCAN_FLAG_PASSIVE ; <nl> } <nl>  <nl> if ( req -> n_channels ) {
static int changed_cb ( struct btrfs_root * left_root , <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> + /* Ignore non - FS objects */ <nl> + if ( key -> objectid == BTRFS_FREE_INO_OBJECTID || <nl> + key -> objectid == BTRFS_FREE_SPACE_OBJECTID ) <nl> + goto out ; <nl> + <nl> if ( key -> type == BTRFS_INODE_ITEM_KEY ) <nl> ret = changed_inode ( sctx , result ); <nl> else if ( key -> type == BTRFS_INODE_REF_KEY )
static int bnx2x_get_hwinfo ( struct bnx2x * bp ) <nl> } else <nl> BNX2X_DEV_INFO (" illegal OV for SD \ n "); <nl> break ; <nl> + case SHARED_FEAT_CFG_FORCE_SF_MODE_FORCED_SF : <nl> + bp -> mf_config [ vn ] = 0 ; <nl> + break ; <nl> default : <nl> /* Unknown configuration : reset mf_config */ <nl> bp -> mf_config [ vn ] = 0 ;
static int drm_crtc_convert_umode ( struct drm_display_mode * out , <nl> if ( in -> clock > INT_MAX || in -> vrefresh > INT_MAX ) <nl> return - ERANGE ; <nl>  <nl> - /* At most , 1 set bit describing the 3D layout of the mode */ <nl> - if ( hweight32 ( in -> flags & DRM_MODE_FLAG_3D_MASK ) > 1 ) <nl> - return - EINVAL ; <nl> - <nl> out -> clock = in -> clock ; <nl> out -> hdisplay = in -> hdisplay ; <nl> out -> hsync_start = in -> hsync_start ;
static long pmcraid_ioctl_passthrough ( <nl> rc = - EFAULT ; <nl> goto out_free_buffer ; <nl> } <nl> + } else if ( request_size < 0 ) { <nl> + rc = - EINVAL ; <nl> + goto out_free_buffer ; <nl> } <nl>  <nl> /* check if we have any additional command parameters */
static int sdhci_bcm_kona_probe ( struct platform_device * pdev ) <nl> kona_dev = sdhci_pltfm_priv ( pltfm_priv ); <nl> mutex_init (& kona_dev -> write_lock ); <nl>  <nl> - mmc_of_parse ( host -> mmc ); <nl> + ret = mmc_of_parse ( host -> mmc ); <nl> + if ( ret ) <nl> + goto err_pltfm_free ; <nl>  <nl> if (! host -> mmc -> f_max ) { <nl> dev_err (& pdev -> dev , " Missing max - freq for SDHCI cfg \ n ");
static int clk_div16_get_divider ( unsigned long parent_rate , unsigned long rate ) <nl> if ( divider_u16 - 1 < 0 ) <nl> return 0 ; <nl>  <nl> - if ( divider_u16 - 1 > 255 ) <nl> + if ( divider_u16 - 1 > 0xFFFF ) <nl> return - EINVAL ; <nl>  <nl> return divider_u16 - 1 ;
static int bond_xmit_roundrobin ( struct sk_buff * skb , struct net_device * bond_dev <nl> * send the join / membership reports . The curr_active_slave found <nl> * will send all of this type of traffic . <nl> */ <nl> - if (( iph -> protocol == htons ( IPPROTO_IGMP )) && <nl> + if (( iph -> protocol == IPPROTO_IGMP ) && <nl> ( skb -> protocol == htons ( ETH_P_IP ))) { <nl>  <nl> read_lock (& bond -> curr_slave_lock );
static int blkvsc_do_operation ( struct block_device_context * blkdev , <nl>  <nl> page_buf = alloc_page ( GFP_KERNEL ); <nl> if (! page_buf ) { <nl> - kmem_cache_free ( blkvsc_req -> dev -> request_pool , blkvsc_req ); <nl> + kmem_cache_free ( blkdev -> request_pool , blkvsc_req ); <nl> return - ENOMEM ; <nl> } <nl>  <nl> cleanup : <nl>  <nl> __free_page ( page_buf ); <nl>  <nl> - kmem_cache_free ( blkvsc_req -> dev -> request_pool , blkvsc_req ); <nl> + kmem_cache_free ( blkdev -> request_pool , blkvsc_req ); <nl>  <nl> return ret ; <nl> }
static void * alloc_qos_entry ( void ) <nl> } <nl> spin_unlock_irqrestore (& qos_free_list . lock , flags ); <nl>  <nl> - entry = kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> - return entry ; <nl> + return kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> } <nl>  <nl> static void free_qos_entry ( void * entry )
static int be_get_config ( struct be_adapter * adapter ) <nl> if ( status ) <nl> return status ; <nl> memcpy ( adapter -> netdev -> dev_addr , mac , ETH_ALEN ); <nl> + memcpy ( adapter -> netdev -> perm_addr , mac , ETH_ALEN ); <nl>  <nl> return 0 ; <nl> }
static int __devinit td_probe ( struct platform_device * pdev ) <nl> pdata -> channels + i ; <nl>  <nl> /* even channels are RX , odd are TX */ <nl> - if ((( i % 2 ) && pchan -> rx ) || (!( i % 2 ) && ! pchan -> rx )) { <nl> + if (( i % 2 ) == pchan -> rx ) { <nl> dev_err (& pdev -> dev , " Wrong channel configuration \ n "); <nl> err = - EINVAL ; <nl> goto err_tasklet_kill ;
static inline void calculate_imbalance ( struct lb_env * env , struct sd_lb_stats * s <nl> * max load less than avg load ( as we skip the groups at or below <nl> * its cpu_power , while calculating max_load ..) <nl> */ <nl> - if ( busiest -> avg_load < sds -> avg_load ) { <nl> + if ( busiest -> avg_load <= sds -> avg_load || <nl> + local -> avg_load >= sds -> avg_load ) { <nl> env -> imbalance = 0 ; <nl> return fix_small_imbalance ( env , sds ); <nl> }
static int mlx4_en_complete_rx_desc ( struct mlx4_en_priv * priv , <nl> PCI_DMA_FROMDEVICE ); <nl> } <nl> /* Adjust size of last fragment to match actual length */ <nl> - skb_frags_rx [ nr - 1 ]. size = length - <nl> - priv -> frag_info [ nr - 1 ]. frag_prefix_size ; <nl> + if ( nr > 0 ) <nl> + skb_frags_rx [ nr - 1 ]. size = length - <nl> + priv -> frag_info [ nr - 1 ]. frag_prefix_size ; <nl> return nr ; <nl>  <nl> fail :
int intel_framebuffer_init ( struct drm_device * dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> + /* FIXME need to adjust LINOFF / TILEOFF accordingly . */ <nl> + if ( mode_cmd -> offsets [ 0 ] != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = drm_framebuffer_init ( dev , & intel_fb -> base , & intel_fb_funcs ); <nl> if ( ret ) { <nl> DRM_ERROR (" framebuffer init failed % d \ n ", ret );
static u8 * __init alloc_event_buffer ( struct amd_iommu * iommu ) <nl> if ( iommu -> evt_buf == NULL ) <nl> return NULL ; <nl>  <nl> + iommu -> evt_buf_size = EVT_BUFFER_SIZE ; <nl> + <nl> return iommu -> evt_buf ; <nl> } <nl> 
static int rt2x00mac_tx_rts_cts ( struct rt2x00_dev * rt2x00dev , <nl> ( struct ieee80211_rts *)( skb -> data )); <nl>  <nl> if ( rt2x00queue_write_tx_frame ( queue , skb )) { <nl> + dev_kfree_skb_any ( skb ); <nl> WARNING ( rt2x00dev , " Failed to send RTS / CTS frame .\ n "); <nl> return NETDEV_TX_BUSY ; <nl> }
void rt2x00link_update_stats ( struct rt2x00_dev * rt2x00dev , <nl> struct link_ant * ant = & rt2x00dev -> link . ant ; <nl> struct ieee80211_hdr * hdr = ( struct ieee80211_hdr *) skb -> data ; <nl>  <nl> + /* <nl> + * No need to update the stats for != STA interfaces <nl> + */ <nl> + if (! rt2x00dev -> intf_sta_count ) <nl> + return ; <nl> + <nl> /* <nl> * Frame was received successfully since non - succesfull <nl> * frames would have been dropped by the hardware .
static void oz_hcd_complete_set_interface ( struct oz_port * port , struct urb * urb , <nl> struct usb_hcd * hcd = port -> ozhcd -> hcd ; <nl> int rc = 0 ; <nl>  <nl> - if ( rcode == 0 ) { <nl> + if (( rcode == 0 ) && ( port -> config_num > 0 )) { <nl> struct usb_host_config * config ; <nl> struct usb_host_interface * intf ; <nl> oz_dbg ( ON , " Set interface % d alt % d \ n ", if_num , alt );
static int ip_vs_wrr_init_svc ( struct ip_vs_service * svc ) <nl> /* <nl> * Allocate the mark variable for WRR scheduling <nl> */ <nl> - mark = kmalloc ( sizeof ( struct ip_vs_wrr_mark ), GFP_ATOMIC ); <nl> + mark = kmalloc ( sizeof ( struct ip_vs_wrr_mark ), GFP_KERNEL ); <nl> if ( mark == NULL ) <nl> return - ENOMEM ; <nl> 
int cx25821_openfile_audio ( struct cx25821_dev * dev , <nl> vfs_read_retval = <nl> vfs_read ( myfile , mybuf , line_size , & pos ); <nl>  <nl> - if ( vfs_read_retval > 0 <nl> - && vfs_read_retval == line_size <nl> - && dev -> _audiodata_buf_virt_addr != NULL ) { <nl> + if ( vfs_read_retval > 0 && <nl> + vfs_read_retval == line_size && <nl> + dev -> _audiodata_buf_virt_addr != NULL ) { <nl> memcpy (( void *)( dev -> <nl> _audiodata_buf_virt_addr <nl> + offset / 4 ), mybuf ,
static int efx_init_lm87 ( struct efx_nic * efx , struct i2c_board_info * info , <nl> if (! client ) <nl> return - EIO ; <nl>  <nl> + /* Read - to - clear alarm / interrupt status */ <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS1 ); <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS2 ); <nl> + <nl> rc = efx_poke_lm87 ( client , reg_values ); <nl> if ( rc ) <nl> goto err ;
static ssize_t __iscsi_ ## prefix ## _store_ ## name ( \ <nl> \ <nl> if (! capable ( CAP_SYS_ADMIN )) \ <nl> return - EPERM ; \ <nl> - \ <nl> + if ( count >= sizeof ( auth -> name )) \ <nl> + return - EINVAL ; \ <nl> snprintf ( auth -> name , sizeof ( auth -> name ), "% s ", page ); \ <nl> if (! strncmp (" NULL ", auth -> name , 4 )) \ <nl> auth -> naf_flags &= ~ flags ; \
static int vmw_gmr2_bind ( struct vmw_private * dev_priv , <nl> cmd += sizeof ( remap_cmd ) / sizeof ( uint32 ); <nl>  <nl> for ( i = 0 ; i < num_pages ; ++ i ) { <nl> - if ( VMW_PPN_SIZE > 4 ) <nl> + if ( VMW_PPN_SIZE <= 4 ) <nl> * cmd = page_to_pfn (* pages ++); <nl> else <nl> *(( uint64_t *) cmd ) = page_to_pfn (* pages ++);
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> int run_pending = 0 ; <nl>  <nl> switch ( event ) { <nl> + case NETDEV_REGISTER : <nl> + if (! idev ) { <nl> + idev = ipv6_add_dev ( dev ); <nl> + if (! idev ) <nl> + printk ( KERN_WARNING " IPv6 : add_dev failed for % s \ n ", <nl> + dev -> name ); <nl> + } <nl> + break ; <nl> case NETDEV_UP : <nl> case NETDEV_CHANGE : <nl> if ( event == NETDEV_UP ) {
int mccic_irq ( struct mcam_camera * cam , unsigned int irqs ) <nl> if ( irqs & ( IRQ_EOF0 << frame )) { <nl> mcam_frame_complete ( cam , frame ); <nl> handled = 1 ; <nl> + if ( cam -> buffer_mode == B_DMA_sg ) <nl> + break ; <nl> } <nl> /* <nl> * If a frame starts , note that we have DMA active . This
static int do_swap_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> } else if ( PageHWPoison ( page )) { <nl> ret = VM_FAULT_HWPOISON ; <nl> delayacct_clear_flag ( DELAYACCT_PF_SWAPIN ); <nl> - goto out ; <nl> + goto out_release ; <nl> } <nl>  <nl> lock_page ( page ); <nl> out_nomap : <nl> pte_unmap_unlock ( page_table , ptl ); <nl> out_page : <nl> unlock_page ( page ); <nl> + out_release : <nl> page_cache_release ( page ); <nl> return ret ; <nl> }
error3 : atomic_dec (& cm_id_priv -> refcount ); <nl> cm_deref_id ( listen_cm_id_priv ); <nl> cm_cleanup_timewait ( cm_id_priv -> timewait_info ); <nl> error2 : kfree ( cm_id_priv -> timewait_info ); <nl> + cm_id_priv -> timewait_info = NULL ; <nl> error1 : ib_destroy_cm_id (& cm_id_priv -> id ); <nl> return ret ; <nl> }
static int new_term ( struct parse_events_term ** _term , int type_val , <nl> term -> val . str = str ; <nl> break ; <nl> default : <nl> + free ( term ); <nl> return - EINVAL ; <nl> } <nl> 
static void md_update_sb ( mddev_t * mddev , int force_change ) <nl> int sync_req ; <nl> int nospares = 0 ; <nl>  <nl> + if ( mddev -> external ) <nl> + return ; <nl> repeat : <nl> spin_lock_irq (& mddev -> write_lock ); <nl> 
static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPW , <nl> . default_value = 720 , <nl> DEFREF ( cropw ), <nl> + DEFINT ( 0 , 864 ), <nl> . get_max_value = ctrl_cropw_max_get , <nl> . get_def_value = ctrl_get_cropcapdw , <nl> }, { <nl> static const struct pvr2_ctl_info control_defs [] = { <nl> . internal_id = PVR2_CID_CROPH , <nl> . default_value = 480 , <nl> DEFREF ( croph ), <nl> + DEFINT ( 0 , 576 ), <nl> . get_max_value = ctrl_croph_max_get , <nl> . get_def_value = ctrl_get_cropcapdh , <nl> }, {
static void batadv_tt_local_event ( struct batadv_priv * bat_priv , <nl> del : <nl> list_del (& entry -> list ); <nl> kfree ( entry ); <nl> + kfree ( tt_change_node ); <nl> event_removed = true ; <nl> goto unlock ; <nl> }
static void ath9k_hw_read_revisions ( struct ath_hw * ah ) <nl> val = REG_READ ( ah , AR_SREV ); <nl> ah -> hw_version . macRev = MS ( val , AR_SREV_REVISION2 ); <nl> return ; <nl> + case AR9300_DEVID_QCA955X : <nl> + ah -> hw_version . macVersion = AR_SREV_VERSION_9550 ; <nl> + return ; <nl> } <nl>  <nl> val = REG_READ ( ah , AR_SREV ) & AR_SREV_ID ;
ssize_t usb_store_new_id ( struct usb_dynids * dynids , <nl> if ( fields > 4 ) { <nl> const struct usb_device_id * id = id_table ; <nl>  <nl> + if (! id ) <nl> + return - ENODEV ; <nl> + <nl> for (; id -> match_flags ; id ++) <nl> if ( id -> idVendor == refVendor && id -> idProduct == refProduct ) <nl> break ;
static int setup_p6_watchdog ( unsigned nmi_hz ) <nl> perfctr_msr = MSR_P6_PERFCTR0 ; <nl> evntsel_msr = MSR_P6_EVNTSEL0 ; <nl>  <nl> - wrmsrl ( perfctr_msr , 0UL ); <nl> + /* KVM doesn ' t implement this MSR */ <nl> + if ( wrmsr_safe ( perfctr_msr , 0 , 0 ) < 0 ) <nl> + return 0 ; <nl>  <nl> evntsel = P6_EVNTSEL_INT <nl> | P6_EVNTSEL_OS
static void setup_frame_info ( struct ieee80211_hw * hw , <nl> fi -> keyix = ATH9K_TXKEYIX_INVALID ; <nl> fi -> keytype = keytype ; <nl> fi -> framelen = framelen ; <nl> + <nl> + if (! rate ) <nl> + return ; <nl> fi -> rtscts_rate = rate -> hw_value ; <nl> if ( short_preamble ) <nl> fi -> rtscts_rate |= rate -> hw_value_short ;
netdev_tx_t ixgbe_xmit_frame_ring ( struct sk_buff * skb , <nl> tx_flags |= IXGBE_TX_FLAGS_SW_VLAN ; <nl> } <nl>  <nl> + skb_tx_timestamp ( skb ); <nl> + <nl> # ifdef CONFIG_IXGBE_PTP <nl> if ( unlikely ( skb_shinfo ( skb )-> tx_flags & SKBTX_HW_TSTAMP )) { <nl> skb_shinfo ( skb )-> tx_flags |= SKBTX_IN_PROGRESS ;
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
static int ioapic_service ( struct kvm_ioapic * ioapic , int irq , bool line_status ) <nl> BUG_ON ( ioapic -> rtc_status . pending_eoi != 0 ); <nl> ret = kvm_irq_delivery_to_apic ( ioapic -> kvm , NULL , & irqe , <nl> ioapic -> rtc_status . dest_map ); <nl> - ioapic -> rtc_status . pending_eoi = ret ; <nl> + ioapic -> rtc_status . pending_eoi = ( ret < 0 ? 0 : ret ); <nl> } else <nl> ret = kvm_irq_delivery_to_apic ( ioapic -> kvm , NULL , & irqe , NULL ); <nl> 
static int i915_pipe_crc_open ( struct inode * inode , struct file * filep ) <nl> struct drm_i915_private * dev_priv = info -> dev -> dev_private ; <nl> struct intel_pipe_crc * pipe_crc = & dev_priv -> pipe_crc [ info -> pipe ]; <nl>  <nl> + if ( info -> pipe >= INTEL_INFO ( info -> dev )-> num_pipes ) <nl> + return - ENODEV ; <nl> + <nl> spin_lock_irq (& pipe_crc -> lock ); <nl>  <nl> if ( pipe_crc -> opened ) {
static const struct file_operations vfio_device_fops = { <nl> */ <nl> static char * vfio_devnode ( struct device * dev , umode_t * mode ) <nl> { <nl> - if ( MINOR ( dev -> devt ) == 0 ) <nl> + if ( mode && ( MINOR ( dev -> devt ) == 0 )) <nl> * mode = S_IRUGO | S_IWUGO ; <nl>  <nl> return kasprintf ( GFP_KERNEL , " vfio /% s ", dev_name ( dev ));
EXPORT_SYMBOL_GPL ( rcu_sched_lock_map ); <nl> # endif <nl>  <nl> int rcu_scheduler_active __read_mostly ; <nl> + EXPORT_SYMBOL_GPL ( rcu_scheduler_active ); <nl>  <nl> /* <nl> * This function is invoked towards the end of the scheduler ' s initialization
void intel_panel_enable_backlight ( struct intel_connector * connector ) <nl>  <nl> WARN_ON ( panel -> backlight . max == 0 ); <nl>  <nl> - if ( panel -> backlight . level == 0 ) { <nl> + if ( panel -> backlight . level <= panel -> backlight . min ) { <nl> panel -> backlight . level = panel -> backlight . max ; <nl> if ( panel -> backlight . device ) <nl> panel -> backlight . device -> props . brightness =
vxge_probe ( struct pci_dev * pdev , const struct pci_device_id * pre ) <nl> driver_config -> config_dev_cnt = 0 ; <nl> driver_config -> total_dev_cnt = 0 ; <nl> driver_config -> g_no_cpus = 0 ; <nl> - driver_config -> vpath_per_dev = max_config_vpath ; <nl> } <nl>  <nl> + driver_config -> vpath_per_dev = max_config_vpath ; <nl> + <nl> driver_config -> total_dev_cnt ++; <nl> if (++ driver_config -> config_dev_cnt > max_config_dev ) { <nl> ret = 0 ;
static int btrfs_remount ( struct super_block * sb , int * flags , char * data ) <nl> struct btrfs_root * root = btrfs_sb ( sb ); <nl> int ret ; <nl>  <nl> + ret = btrfs_parse_options ( root , data ); <nl> + if ( ret ) <nl> + return - EINVAL ; <nl> + <nl> if ((* flags & MS_RDONLY ) == ( sb -> s_flags & MS_RDONLY )) <nl> return 0 ; <nl> 
static int dwc3_gadget_init_hw_endpoints ( struct dwc3 * dwc , <nl>  <nl> dep -> endpoint . name = dep -> name ; <nl>  <nl> + dev_vdbg ( dwc -> dev , " initializing % s \ n ", dep -> name ); <nl> + <nl> if ( epnum == 0 || epnum == 1 ) { <nl> dep -> endpoint . maxpacket = 512 ; <nl> dep -> endpoint . maxburst = 1 ;
static int init_volumes ( struct ubi_device * ubi , <nl>  <nl> /* Static volumes only */ <nl> av = ubi_find_av ( ai , i ); <nl> - if (! av ) { <nl> + if (! av || ! av -> leb_count ) { <nl> /* <nl> * No eraseblocks belonging to this volume found . We <nl> * don ' t actually know whether this static volume is
static void __unregister_request ( struct ceph_osd_client * osdc , <nl> */ <nl> static void __cancel_request ( struct ceph_osd_request * req ) <nl> { <nl> - if ( req -> r_sent ) { <nl> + if ( req -> r_sent && req -> r_osd ) { <nl> ceph_con_revoke (& req -> r_osd -> o_con , req -> r_request ); <nl> req -> r_sent = 0 ; <nl> }
ARIZONA_MIXER_CONTROLS (" DSP2R ", ARIZONA_DSP2RMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP3L ", ARIZONA_DSP3LMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP3R ", ARIZONA_DSP3RMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP4L ", ARIZONA_DSP4LMIX_INPUT_1_SOURCE ), <nl> - ARIZONA_MIXER_CONTROLS (" DSP5R ", ARIZONA_DSP4RMIX_INPUT_1_SOURCE ), <nl> + ARIZONA_MIXER_CONTROLS (" DSP4R ", ARIZONA_DSP4RMIX_INPUT_1_SOURCE ), <nl>  <nl> ARIZONA_MIXER_CONTROLS (" Mic ", ARIZONA_MICMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" Noise ", ARIZONA_NOISEMIX_INPUT_1_SOURCE ),
void i915_debugfs_cleanup ( struct drm_minor * minor ); <nl> int i915_debugfs_connector_add ( struct drm_connector * connector ); <nl> void intel_display_crc_init ( struct drm_device * dev ); <nl> # else <nl> - static inline int i915_debugfs_connector_add ( struct drm_connector * connector ) {} <nl> + static inline int i915_debugfs_connector_add ( struct drm_connector * connector ) <nl> +{ return 0 ; } <nl> static inline void intel_display_crc_init ( struct drm_device * dev ) {} <nl> # endif <nl> 
static int sh_eth_drv_probe ( struct platform_device * pdev ) <nl> } <nl> mdp -> tsu_addr = ioremap ( rtsu -> start , <nl> resource_size ( rtsu )); <nl> + if ( mdp -> tsu_addr == NULL ) { <nl> + ret = - ENOMEM ; <nl> + dev_err (& pdev -> dev , " TSU ioremap failed .\ n "); <nl> + goto out_release ; <nl> + } <nl> mdp -> port = devno % 2 ; <nl> ndev -> features = NETIF_F_HW_VLAN_FILTER ; <nl> }
static int aic3x_read ( struct snd_soc_codec * codec , unsigned int reg , <nl> if ( reg >= AIC3X_CACHEREGNUM ) <nl> return - 1 ; <nl>  <nl> - * value = codec -> hw_read ( codec , reg ); <nl> + codec -> cache_bypass = 1 ; <nl> + * value = snd_soc_read ( codec , reg ); <nl> + codec -> cache_bypass = 0 ; <nl> + <nl> cache [ reg ] = * value ; <nl>  <nl> return 0 ;
static int __devinit twl4030_madc_probe ( struct platform_device * pdev ) <nl> if (! madc ) <nl> return - ENOMEM ; <nl>  <nl> + madc -> dev = & pdev -> dev ; <nl> + <nl> /* <nl> * Phoenix provides 2 interrupt lines . The first one is connected to <nl> * the OMAP . The other one can be connected to the other processor such
struct iwl_cfg iwl6000i_2bg_cfg = { <nl> . fw_name_pre = IWL6050_FW_PRE , \ <nl> . ucode_api_max = IWL6050_UCODE_API_MAX , \ <nl> . ucode_api_min = IWL6050_UCODE_API_MIN , \ <nl> + . valid_tx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> + . valid_rx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> . ops = & iwl6050_ops , \ <nl> . eeprom_ver = EEPROM_6050_EEPROM_VERSION , \ <nl> . eeprom_calib_ver = EEPROM_6050_TX_POWER_VERSION , \
int mls_context_to_sid ( char oldc , <nl>  <nl> if (! selinux_mls_enabled ) { <nl> if ( def_sid != SECSID_NULL && oldc ) <nl> - * scontext += strlen (* scontext ); <nl> + * scontext += strlen (* scontext )+ 1 ; <nl> return 0 ; <nl> } <nl> 
static int virtscsi_queuecommand ( struct Scsi_Host * sh , struct scsi_cmnd * sc ) <nl> sizeof cmd -> req . cmd , sizeof cmd -> resp . cmd , <nl> GFP_ATOMIC ) >= 0 ) <nl> ret = 0 ; <nl> + else <nl> + mempool_free ( cmd , virtscsi_cmd_pool ); <nl>  <nl> out : <nl> return ret ;
static struct tgfx __init * tgfx_probe ( int parport , int * n_buttons , int n_devs ) <nl> if ( n_buttons [ i ] < 1 ) <nl> continue ; <nl>  <nl> - if ( n_buttons [ i ] > 6 ) { <nl> + if ( n_buttons [ i ] > ARRAY_SIZE ( tgfx_buttons )) { <nl> printk ( KERN_ERR " turbografx . c : Invalid number of buttons % d \ n ", n_buttons [ i ]); <nl> err = - EINVAL ; <nl> goto err_unreg_devs ;
static void cx24120_check_cmd ( struct cx24120_state * state , u8 id ) <nl> case CMD_DISEQC_MSG2 : <nl> case CMD_SETVOLTAGE : <nl> case CMD_SETTONE : <nl> + case CMD_DISEQC_BURST : <nl> cx24120_msg_mpeg_output_global_config ( state , 0 ); <nl> /* Old driver would do a msleep ( 100 ) here */ <nl> default :
int eeh_dn_check_failure ( struct device_node * dn , struct pci_dev * dev ) <nl> no_dn ++; <nl> return 0 ; <nl> } <nl> + dn = find_device_pe ( dn ); <nl> pdn = PCI_DN ( dn ); <nl>  <nl> /* Access to IO BARs might get this far and still not want checking . */
static int alps_enter_command_mode ( struct psmouse * psmouse , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( param [ 0 ] != 0x88 && param [ 1 ] != 0x07 ) { <nl> + if ( param [ 0 ] != 0x88 || ( param [ 1 ] != 0x07 && param [ 1 ] != 0x08 )) { <nl> psmouse_dbg ( psmouse , <nl> " unknown response while entering command mode \ n "); <nl> return - 1 ;
static int time_cpufreq_notifier ( struct notifier_block * nb , unsigned long val , <nl> tsc_khz = cpufreq_scale ( tsc_khz_ref , ref_freq , freq -> new ); <nl> if (!( freq -> flags & CPUFREQ_CONST_LOOPS )) <nl> mark_tsc_unstable (" cpufreq changes "); <nl> - } <nl>  <nl> - set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void __init mark_kpte_bitmap ( unsigned long start , unsigned long end ) <nl> while ( start < end ) { <nl> long remains ; <nl>  <nl> + remains = end - start ; <nl> + if ( remains < size_256MB ) <nl> + break ; <nl> + <nl> if ( start & mask_256MB ) { <nl> start = ( start + size_256MB ) & ~ mask_256MB ; <nl> continue ; <nl> } <nl>  <nl> - remains = end - start ; <nl> while ( remains >= size_256MB ) { <nl> unsigned long index = start >> shift_256MB ; <nl> 
static void __init create_mapping ( phys_addr_t phys , unsigned long virt , <nl> void __iomem * __init early_io_map ( phys_addr_t phys , unsigned long virt ) <nl> { <nl> unsigned long size , mask ; <nl> - bool page64k = IS_ENABLED ( ARM64_64K_PAGES ); <nl> + bool page64k = IS_ENABLED ( CONFIG_ARM64_64K_PAGES ); <nl> pgd_t * pgd ; <nl> pud_t * pud ; <nl> pmd_t * pmd ;
static void * vb2_dc_alloc ( void * alloc_ctx , unsigned long size ) <nl> if (! buf ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> + /* align image size to PAGE_SIZE */ <nl> + size = PAGE_ALIGN ( size ); <nl> + <nl> buf -> vaddr = dma_alloc_coherent ( dev , size , & buf -> dma_addr , GFP_KERNEL ); <nl> if (! buf -> vaddr ) { <nl> dev_err ( dev , " dma_alloc_coherent of size % ld failed \ n ", size );
int pinconf_generic_parse_dt_config ( struct device_node * np , <nl> ncfg ++; <nl> } <nl>  <nl> + /* no configs found at all */ <nl> + if ( ncfg == 0 ) { <nl> + * configs = NULL ; <nl> + * nconfigs = 0 ; <nl> + return 0 ; <nl> + } <nl> + <nl> /* <nl> * Now limit the number of configs to the real number of <nl> * found properties .
static void do_ubd_request ( struct request_queue * q ) <nl> " errno = % d \ n ", - n ); <nl> else if ( list_empty (& dev -> restart )) <nl> list_add (& dev -> restart , & restart ); <nl> + kfree ( io_req ); <nl> return ; <nl> } <nl> 
static u32 tcp_yeah_ssthresh ( struct sock * sk ) <nl> yeah -> fast_count = 0 ; <nl> yeah -> reno_count = max ( yeah -> reno_count >> 1 , 2U ); <nl>  <nl> - return tp -> snd_cwnd - reduction ; <nl> + return max_t ( int , tp -> snd_cwnd - reduction , 2 ); <nl> } <nl>  <nl> static struct tcp_congestion_ops tcp_yeah __read_mostly = {
void ieee80211_beacon_connection_loss_work ( struct work_struct * work ) <nl> struct sta_info * sta ; <nl>  <nl> if ( ifmgd -> associated ) { <nl> + rcu_read_lock (); <nl> sta = sta_info_get ( sdata , ifmgd -> bssid ); <nl> if ( sta ) <nl> sta -> beacon_loss_count ++; <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> if ( sdata -> local -> hw . flags & IEEE80211_HW_CONNECTION_MONITOR )
static ssize_t read_vmcore ( struct file * file , char __user * buffer , <nl>  <nl> static const struct file_operations proc_vmcore_operations = { <nl> . read = read_vmcore , <nl> - . llseek = generic_file_llseek , <nl> + . llseek = default_llseek , <nl> }; <nl>  <nl> static struct vmcore * __init get_new_element ( void )
static void kswapd_try_to_sleep ( pg_data_t * pgdat , int order , int classzone_idx ) <nl> * them before going back to sleep . <nl> */ <nl> set_pgdat_percpu_threshold ( pgdat , calculate_normal_threshold ); <nl> - schedule (); <nl> + <nl> + if (! kthread_should_stop ()) <nl> + schedule (); <nl> + <nl> set_pgdat_percpu_threshold ( pgdat , calculate_pressure_threshold ); <nl> } else { <nl> if ( remaining )
static void handle_critical_trips ( struct thermal_zone_device * tz , <nl> tz -> ops -> get_trip_temp ( tz , trip , & trip_temp ); <nl>  <nl> /* If we have not crossed the trip_temp , we do not care . */ <nl> - if ( tz -> temperature < trip_temp ) <nl> + if ( trip_temp <= 0 || tz -> temperature < trip_temp ) <nl> return ; <nl>  <nl> trace_thermal_zone_trip ( tz , trip , trip_type );
bool rtl92cu_rx_query_desc ( struct ieee80211_hw * hw , <nl> ( bool ) GET_RX_DESC_PAGGR ( pdesc )); <nl> rx_status -> mactime = GET_RX_DESC_TSFL ( pdesc ); <nl> if ( phystatus ) { <nl> - p_drvinfo = ( struct rx_fwinfo_92c *)( pdesc + RTL_RX_DESC_SIZE ); <nl> + p_drvinfo = ( struct rx_fwinfo_92c *)( skb -> data + <nl> + stats -> rx_bufshift ); <nl> rtl92c_translate_rx_signal_stuff ( hw , skb , stats , pdesc , <nl> p_drvinfo ); <nl> }
static struct usb_device_id usb_ids [] = { <nl> { USB_DEVICE ( 0x157e , 0x300d ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x079b , 0x0062 ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x1582 , 0x6003 ), . driver_info = DEVICE_ZD1211B }, <nl> + { USB_DEVICE ( 0x050d , 0x705c ), . driver_info = DEVICE_ZD1211B }, <nl> /* " Driverless " devices that need ejecting */ <nl> { USB_DEVICE ( 0x0ace , 0x2011 ), . driver_info = DEVICE_INSTALLER }, <nl> {}
static struct mount * clone_mnt ( struct mount * old , struct dentry * root , <nl> } <nl>  <nl> /* Don ' t allow unprivileged users to reveal what is under a mount */ <nl> - if (( flag & CL_UNPRIVILEGED ) && list_empty (& old -> mnt_expire )) <nl> + if (( flag & CL_UNPRIVILEGED ) && <nl> + (!( flag & CL_EXPIRE ) || list_empty (& old -> mnt_expire ))) <nl> mnt -> mnt . mnt_flags |= MNT_LOCKED ; <nl>  <nl> atomic_inc (& sb -> s_active );
void kvm_check_async_pf_completion ( struct kvm_vcpu * vcpu ) <nl> spin_unlock (& vcpu -> async_pf . lock ); <nl>  <nl> kvm_arch_async_page_ready ( vcpu , work ); <nl> - kvm_arch_async_page_present ( vcpu , work ); <nl> + kvm_async_page_present_async ( vcpu , work ); <nl>  <nl> list_del (& work -> queue ); <nl> vcpu -> async_pf . queued --;
static unsigned int xhci_microframes_to_exponent ( struct usb_device * udev , <nl> static unsigned int xhci_parse_microframe_interval ( struct usb_device * udev , <nl> struct usb_host_endpoint * ep ) <nl> { <nl> + if ( ep -> desc . bInterval == 0 ) <nl> + return 0 ; <nl> return xhci_microframes_to_exponent ( udev , ep , <nl> ep -> desc . bInterval , 0 , 15 ); <nl> }
static int gpio_setup_irq ( struct gpio_desc * desc , struct device * dev , <nl> return 0 ; <nl>  <nl> free_sd : <nl> - sysfs_put ( pdesc -> value_sd ); <nl> + if ( pdesc ) <nl> + sysfs_put ( pdesc -> value_sd ); <nl> free_id : <nl> idr_remove (& pdesc_idr , id ); <nl> desc -> flags &= GPIO_FLAGS_MASK ;
static void __init atmci_get_cap ( struct atmel_mci * host ) <nl> /* keep only major version number */ <nl> switch ( version & 0xf00 ) { <nl> case 0x100 : <nl> + host -> caps . has_pdc = 1 ; <nl> + break ; <nl> case 0x200 : <nl> host -> caps . has_pdc = 1 ; <nl> host -> caps . has_rwproof = 1 ;
int of_gpio_simple_xlate ( struct gpio_chip * gc , <nl> if ( WARN_ON ( gpiospec -> args_count < gc -> of_gpio_n_cells )) <nl> return - EINVAL ; <nl>  <nl> - if ( gpiospec -> args [ 0 ] > gc -> ngpio ) <nl> + if ( gpiospec -> args [ 0 ] >= gc -> ngpio ) <nl> return - EINVAL ; <nl>  <nl> if ( flags )
void kvm_hv_process_stimers ( struct kvm_vcpu * vcpu ) <nl> for ( i = 0 ; i < ARRAY_SIZE ( hv_vcpu -> stimer ); i ++) <nl> if ( test_and_clear_bit ( i , hv_vcpu -> stimer_pending_bitmap )) { <nl> stimer = & hv_vcpu -> stimer [ i ]; <nl> - stimer_stop ( stimer ); <nl> if ( stimer -> config & HV_STIMER_ENABLE ) { <nl> time_now = get_time_ref_counter ( vcpu -> kvm ); <nl> if ( time_now >= stimer -> exp_time )
static int chaoskey_rng_read ( struct hwrng * rng , void * data , <nl> if ( this_time > max ) <nl> this_time = max ; <nl>  <nl> - memcpy ( data , dev -> buf , this_time ); <nl> + memcpy ( data , dev -> buf + dev -> used , this_time ); <nl>  <nl> dev -> used += this_time ; <nl> 
static int mxs_dcp_start_dma ( struct dcp_async_ctx * actx ) <nl> struct dcp * sdcp = global_sdcp ; <nl> const int chan = actx -> chan ; <nl> uint32_t stat ; <nl> - int ret ; <nl> + unsigned long ret ; <nl> struct dcp_dma_desc * desc = & sdcp -> coh -> desc [ actx -> chan ]; <nl>  <nl> dma_addr_t desc_phys = dma_map_single ( sdcp -> dev , desc , sizeof (* desc ),
static int add_munmap ( unsigned long addr , unsigned long len , <nl> struct host_vm_op * last ; <nl> int ret = 0 ; <nl>  <nl> + if (( addr >= STUB_START ) && ( addr < STUB_END )) <nl> + return - EINVAL ; <nl> + <nl> if ( hvc -> index != 0 ) { <nl> last = & hvc -> ops [ hvc -> index - 1 ]; <nl> if (( last -> type == MUNMAP ) &&
static int amdgpu_vm_clear_bo ( struct amdgpu_device * adev , <nl> if ( r ) <nl> return r ; <nl>  <nl> + r = reservation_object_reserve_shared ( bo -> tbo . resv ); <nl> + if ( r ) <nl> + return r ; <nl> + <nl> r = ttm_bo_validate (& bo -> tbo , & bo -> placement , true , false ); <nl> if ( r ) <nl> goto error_unreserve ;
SYSCALL_DEFINE6 ( sparc_ipc , unsigned int , call , int , first , unsigned long , second <nl> long err ; <nl>  <nl> /* No need for backward compatibility . We can start fresh ... */ <nl> - if ( call <= SEMCTL ) { <nl> + if ( call <= SEMTIMEDOP ) { <nl> switch ( call ) { <nl> case SEMOP : <nl> err = sys_semtimedop ( first , ptr ,
static void __init exynos_reserve ( void ) <nl> " samsung , mfc - v5 ", <nl> " samsung , mfc - v6 ", <nl> " samsung , mfc - v7 ", <nl> + " samsung , mfc - v8 ", <nl> }; <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( mfc_mem ); i ++)
at86rf230_tx_complete ( void * context ) <nl> { <nl> struct at86rf230_state_change * ctx = context ; <nl> struct at86rf230_local * lp = ctx -> lp ; <nl> - struct sk_buff * skb = lp -> tx_skb ; <nl>  <nl> enable_irq ( lp -> spi -> irq ); <nl>  <nl> - ieee802154_xmit_complete ( lp -> hw , skb , ! lp -> tx_aret ); <nl> + ieee802154_xmit_complete ( lp -> hw , lp -> tx_skb , ! lp -> tx_aret ); <nl> } <nl>  <nl> static void
recheck : <nl> */ <nl> if (! capable ( CAP_SYS_NICE )) { <nl> /* can ' t change policy */ <nl> - if ( policy != p -> policy ) <nl> + if ( policy != p -> policy && <nl> + ! p -> signal -> rlim [ RLIMIT_RTPRIO ]. rlim_cur ) <nl> return - EPERM ; <nl> /* can ' t increase priority */ <nl> if ( policy != SCHED_NORMAL &&
parse_init_table ( struct nvbios * bios , uint16_t offset , struct init_exec * iexec ) <nl> int count = 0 , i , ret ; <nl> uint8_t id ; <nl>  <nl> + /* catch NULL script pointers */ <nl> + if ( offset == 0 ) <nl> + return 0 ; <nl> + <nl> /* <nl> * Loop until INIT_DONE causes us to break out of the loop <nl> * ( or until offset > bios length just in case ... )
static int __devinit adp8870_probe ( struct i2c_client * client , <nl> mutex_init (& data -> lock ); <nl>  <nl> memset (& props , 0 , sizeof ( props )); <nl> + props . type = BACKLIGHT_RAW ; <nl> props . max_brightness = props . brightness = ADP8870_MAX_BRIGHTNESS ; <nl> bl = backlight_device_register ( dev_driver_string (& client -> dev ), <nl> & client -> dev , data , & adp8870_bl_ops , & props );
ldebugfs_fid_write_common ( const char __user * buffer , size_t count , <nl> rc = sscanf ( kernbuf , "[% llx - % llx ]\ n ", <nl> ( unsigned long long *)& tmp . lsr_start , <nl> ( unsigned long long *)& tmp . lsr_end ); <nl> + if ( rc != 2 ) <nl> + return - EINVAL ; <nl> if (! range_is_sane (& tmp ) || range_is_zero (& tmp ) || <nl> tmp . lsr_start < range -> lsr_start || tmp . lsr_end > range -> lsr_end ) <nl> return - EINVAL ;
static enum print_line_t trace_stack_print ( struct trace_iterator * iter , <nl> trace_assign_type ( field , iter -> ent ); <nl>  <nl> for ( i = 0 ; i < FTRACE_STACK_ENTRIES ; i ++) { <nl> + if (! field -> caller [ i ]) <nl> + break ; <nl> if ( i ) { <nl> if (! trace_seq_puts ( s , " <= ")) <nl> goto partial ;
static void moxa_start ( struct tty_struct * tty ) <nl> if ( ch == NULL ) <nl> return ; <nl>  <nl> - if (!( ch -> statusflags & TXSTOPPED )) <nl> + if (! test_bit ( TXSTOPPED , & ch -> statusflags )) <nl> return ; <nl>  <nl> MoxaPortTxEnable ( ch );
static int vmw_cotable_unbind ( struct vmw_resource * res , <nl> struct vmw_private * dev_priv = res -> dev_priv ; <nl> struct ttm_buffer_object * bo = val_buf -> bo ; <nl> struct vmw_fence_obj * fence ; <nl> - int ret ; <nl>  <nl> if ( list_empty (& res -> mob_head )) <nl> return 0 ; <nl> static int vmw_cotable_unbind ( struct vmw_resource * res , <nl> if ( likely ( fence != NULL )) <nl> vmw_fence_obj_unreference (& fence ); <nl>  <nl> - return ret ; <nl> + return 0 ; <nl> } <nl>  <nl> /**
static int hostap_set_generic_element ( PSDevice pDevice , <nl> { <nl> PSMgmtObject pMgmt = pDevice -> pMgmt ; <nl>  <nl> + if ( param -> u . generic_elem . len > sizeof ( pMgmt -> abyWPAIE )) <nl> + return - EINVAL ; <nl> + <nl> memcpy ( pMgmt -> abyWPAIE , <nl> param -> u . generic_elem . data , <nl> param -> u . generic_elem . len
static int sh_mdio_release ( struct net_device * ndev ) <nl> /* remove mdio bus info from net_device */ <nl> dev_set_drvdata (& ndev -> dev , NULL ); <nl>  <nl> + /* free interrupts memory */ <nl> + kfree ( bus -> irq ); <nl> + <nl> /* free bitbang info */ <nl> free_mdio_bitbang ( bus ); <nl> 
retry : <nl> handle = ext3_journal_start ( inode , 2 ); <nl> if ( IS_ERR ( handle )) { <nl> /* This is really bad luck . We ' ve written the data <nl> - * but cannot extend i_size . Bail out and pretend <nl> - * the write failed ... */ <nl> + * but cannot extend i_size . Truncate allocated blocks <nl> + * and pretend the write failed ... */ <nl> + ext3_truncate ( inode ); <nl> ret = PTR_ERR ( handle ); <nl> goto out ; <nl> }
void __init tsc_calibrate ( void ) <nl> if ( hpet ) { <nl> printk ( KERN_INFO " TSC calibrated against HPET \ n "); <nl> if ( hpet2 < hpet1 ) <nl> - hpet2 += 0x100000000 ; <nl> + hpet2 += 0x100000000UL ; <nl> hpet2 -= hpet1 ; <nl> tsc1 = ( hpet2 * hpet_readl ( HPET_PERIOD )) / 1000000 ; <nl> } else {
_base_sas_log_info ( struct MPT2SAS_ADAPTER * ioc , u32 log_info ) <nl> return ; <nl>  <nl> /* eat the loginfos associated with task aborts */ <nl> - if ( ioc -> ignore_loginfos && ( log_info == 30050000 || log_info == <nl> + if ( ioc -> ignore_loginfos && ( log_info == 0x30050000 || log_info == <nl> 0x31140000 || log_info == 0x31130000 )) <nl> return ; <nl> 
static SENSOR_DEVICE_ATTR ( temp4_input , S_IRUGO , show_temp , NULL , 3 ); <nl> REG : count of 90kHz pulses / revolution */ <nl> static int fan_from_reg ( u16 reg ) <nl> { <nl> + if ( reg == 0 || reg == 0xffff ) <nl> + return 0 ; <nl> return 90000 * 60 / reg ; <nl> } <nl> 
static int nl80211_start_radar_detection ( struct sk_buff * skb , <nl> struct net_device * dev = info -> user_ptr [ 1 ]; <nl> struct wireless_dev * wdev = dev -> ieee80211_ptr ; <nl> struct cfg80211_chan_def chandef ; <nl> + enum nl80211_dfs_regions dfs_region ; <nl> int err ; <nl>  <nl> + dfs_region = reg_get_dfs_region ( wdev -> wiphy ); <nl> + if ( dfs_region == NL80211_DFS_UNSET ) <nl> + return - EINVAL ; <nl> + <nl> err = nl80211_parse_chandef ( rdev , info , & chandef ); <nl> if ( err ) <nl> return err ;
static int mxs_mmc_probe ( struct platform_device * pdev ) <nl> if (! ssp -> dmach ) { <nl> dev_err ( mmc_dev ( host -> mmc ), <nl> "% s : failed to request dma \ n ", __func__ ); <nl> + ret = - ENODEV ; <nl> goto out_clk_put ; <nl> } <nl> 
static void blade_image_blit ( struct tridentfb_par * par , const char * data , <nl> writemmr ( par , DST1 , point ( x , y )); <nl> writemmr ( par , DST2 , point ( x + w - 1 , y + h - 1 )); <nl>  <nl> - memcpy ( par -> io_virt + 0x10000 , data , 4 * size ); <nl> + iowrite32_rep ( par -> io_virt + 0x10000 , data , size ); <nl> } <nl>  <nl> static void blade_copy_rect ( struct tridentfb_par * par ,
static int wm5100_set_sysclk ( struct snd_soc_codec * codec , int clk_id , <nl> fval = 1 ; <nl> break ; <nl> case 22579200 : <nl> - case 2457600 : <nl> + case 24576000 : <nl> fval = 2 ; <nl> break ; <nl> default : <nl> static int wm5100_set_sysclk ( struct snd_soc_codec * codec , int clk_id , <nl>  <nl> case 6144000 : <nl> case 12288000 : <nl> - case 2457600 : <nl> + case 24576000 : <nl> audio_rate = 48000 ; <nl> break ; <nl> 
static ssize_t iio_write_channel_info ( struct device * dev , <nl> if ( buf [ 0 ] == '-') { <nl> negative = true ; <nl> buf ++; <nl> + } else if ( buf [ 0 ] == '+') { <nl> + buf ++; <nl> } <nl>  <nl> while (* buf ) {
int irq_domain_simple_dt_translate ( struct irq_domain * d , <nl> return - EINVAL ; <nl> if ( intsize < 1 ) <nl> return - EINVAL ; <nl> + if ( d -> nr_irq && (( intspec [ 0 ] < d -> hwirq_base ) || <nl> + ( intspec [ 0 ] >= d -> hwirq_base + d -> nr_irq ))) <nl> + return - EINVAL ; <nl>  <nl> * out_hwirq = intspec [ 0 ]; <nl> * out_type = IRQ_TYPE_NONE ;
static int tcp_tso_should_defer ( struct sock * sk , struct tcp_sock * tp , struct sk_ <nl> if ( TCP_SKB_CB ( skb )-> flags & TCPCB_FLAG_FIN ) <nl> return 0 ; <nl>  <nl> + if ( tp -> ca_state != TCP_CA_Open ) <nl> + return 0 ; <nl> + <nl> in_flight = tcp_packets_in_flight ( tp ); <nl>  <nl> BUG_ON ( tcp_skb_pcount ( skb ) <= 1 ||
static int snd_compr_allocate_buffer ( struct snd_compr_stream * stream , <nl> unsigned int buffer_size ; <nl> void * buffer ; <nl>  <nl> + if ( params -> buffer . fragment_size == 0 || <nl> + params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + return - EINVAL ; <nl> + <nl> buffer_size = params -> buffer . fragment_size * params -> buffer . fragments ; <nl> if ( stream -> ops -> copy ) { <nl> buffer = NULL ;
retry : <nl> goto found ; <nl> } <nl>  <nl> - while ( addr + size >= first -> va_start && addr + size <= vend ) { <nl> + while ( addr + size > first -> va_start && addr + size <= vend ) { <nl> addr = ALIGN ( first -> va_end + PAGE_SIZE , align ); <nl>  <nl> n = rb_next (& first -> rb_node );
i915_gem_set_tiling ( struct drm_device * dev , void * data , <nl> } <nl>  <nl> mutex_lock (& dev -> struct_mutex ); <nl> - if ( i915_gem_obj_is_pinned ( obj ) || obj -> framebuffer_references ) { <nl> + if ( obj -> pin_display || obj -> framebuffer_references ) { <nl> ret = - EBUSY ; <nl> goto err ; <nl> }
ecryptfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> ecryptfs_copy_attr_all ( old_dir , lower_old_dir_dentry -> d_inode ); <nl> out_lock : <nl> unlock_rename ( lower_old_dir_dentry , lower_new_dir_dentry ); <nl> + dput ( lower_new_dentry -> d_parent ); <nl> + dput ( lower_old_dentry -> d_parent ); <nl> dput ( lower_new_dentry ); <nl> dput ( lower_old_dentry ); <nl> return rc ;
struct fbcon_ops { <nl> # define attr_fgcol ( fgshift , s ) \ <nl> ((( s ) >> ( fgshift )) & 0x0f ) <nl> # define attr_bgcol ( bgshift , s ) \ <nl> - ((( s ) >> ( bgshift )) & 0x0f ) <nl> + ((( s ) >> ( bgshift )) & 0x07 ) <nl>  <nl> /* Monochrome */ <nl> # define attr_bold ( s ) \
struct ipic * __init ipic_init ( struct device_node * node , unsigned int flags ) <nl> ipic -> irqhost = irq_alloc_host ( node , IRQ_HOST_MAP_LINEAR , <nl> NR_IPIC_INTS , <nl> & ipic_host_ops , 0 ); <nl> - if ( ipic -> irqhost == NULL ) <nl> + if ( ipic -> irqhost == NULL ) { <nl> + kfree ( ipic ); <nl> return NULL ; <nl> + } <nl>  <nl> ipic -> regs = ioremap ( res . start , res . end - res . start + 1 ); <nl> 
static int ccmp_encrypt_skb ( struct ieee80211_tx_data * tx , struct sk_buff * skb ) <nl> memmove ( pos , pos + CCMP_HDR_LEN , hdrlen ); <nl>  <nl> /* the HW only needs room for the IV , but not the actual IV */ <nl> - if ( info -> control . hw_key -> flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE ) <nl> + if ( info -> control . hw_key && <nl> + ( info -> control . hw_key -> flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE )) <nl> return 0 ; <nl>  <nl> hdr = ( struct ieee80211_hdr *) pos ;
void __init setup_per_cpu_areas ( void ) <nl> fc = __alloc_bootmem ( unit_size , PAGE_SIZE , __pa ( MAX_DMA_ADDRESS )); <nl> if (! ai || ! fc ) <nl> panic (" Failed to allocate memory for percpu areas ."); <nl> + /* kmemleak tracks the percpu allocations separately */ <nl> + kmemleak_free ( fc ); <nl>  <nl> ai -> dyn_size = unit_size ; <nl> ai -> unit_size = unit_size ;
int l2tp_xmit_skb ( struct l2tp_session * session , struct sk_buff * skb , int hdr_len <nl> headroom = NET_SKB_PAD + sizeof ( struct iphdr ) + <nl> uhlen + hdr_len ; <nl> old_headroom = skb_headroom ( skb ); <nl> - if ( skb_cow_head ( skb , headroom )) <nl> + if ( skb_cow_head ( skb , headroom )) { <nl> + dev_kfree_skb ( skb ); <nl> goto abort ; <nl> + } <nl>  <nl> new_headroom = skb_headroom ( skb ); <nl> skb_orphan ( skb );
static int hba_setup_cid_tbls ( struct beiscsi_hba * phba ) <nl> kfree ( phba -> ep_array ); <nl> phba -> ep_array = NULL ; <nl> ret = - ENOMEM ; <nl> + <nl> + goto free_memory ; <nl> } <nl>  <nl> for ( i = 0 ; i < phba -> params . cxns_per_ctrl ; i ++) {
handle_locking_key ( struct input_dev * visorinput_dev , <nl> int keycode , int desired_state ) <nl> { <nl> int led ; <nl> - char * sled ; <nl>  <nl> switch ( keycode ) { <nl> case KEY_CAPSLOCK : <nl> led = LED_CAPSL ; <nl> - sled = " CAP "; <nl> break ; <nl> case KEY_SCROLLLOCK : <nl> led = LED_SCROLLL ; <nl> - sled = " SCR "; <nl> break ; <nl> case KEY_NUMLOCK : <nl> led = LED_NUML ; <nl> - sled = " NUM "; <nl> break ; <nl> default : <nl> led = - 1 ;
static struct inotify_watch * create_watch ( struct inotify_device * dev , <nl> return ERR_PTR ( ret ); <nl> } <nl>  <nl> - dev -> last_wd = ret ; <nl> + dev -> last_wd = watch -> wd ; <nl> watch -> mask = mask ; <nl> atomic_set (& watch -> count , 0 ); <nl> INIT_LIST_HEAD (& watch -> d_list );
static int __devinit savagefb_probe ( struct pci_dev * dev , <nl> # if defined ( CONFIG_FB_SAVAGE_I2C ) <nl> savagefb_create_i2c_busses ( info ); <nl> savagefb_probe_i2c_connector ( info , & par -> edid ); <nl> - kfree ( par -> edid ); <nl> fb_edid_to_monspecs ( par -> edid , & info -> monspecs ); <nl> + kfree ( par -> edid ); <nl> fb_videomode_to_modelist ( info -> monspecs . modedb , <nl> info -> monspecs . modedb_len , <nl> & info -> modelist );
static int __init parse_crashkernel_mem ( char * cmdline , <nl> } while (* cur ++ == ','); <nl>  <nl> if (* crash_size > 0 ) { <nl> - while (* cur != ' ' && * cur != '@') <nl> + while (* cur && * cur != ' ' && * cur != '@') <nl> cur ++; <nl> if (* cur == '@') { <nl> cur ++;
int btrfs_sync_file ( struct file * file , struct dentry * dentry , int datasync ) <nl> } <nl> mutex_lock (& dentry -> d_inode -> i_mutex ); <nl> out : <nl> - return ret > 0 ? EIO : ret ; <nl> + return ret > 0 ? - EIO : ret ; <nl> } <nl>  <nl> static const struct vm_operations_struct btrfs_file_vm_ops = {
int rsnd_dai_connect ( struct rsnd_mod * mod , <nl> if (! mod ) <nl> return - EIO ; <nl>  <nl> + if ( io -> mod [ type ]) <nl> + return - EINVAL ; <nl> + <nl> priv = rsnd_mod_to_priv ( mod ); <nl> dev = rsnd_priv_to_dev ( priv ); <nl> 
static int btrfs_get_sb ( struct file_system_type * fs_type , int flags , <nl> mutex_unlock (& root -> d_inode -> i_mutex ); <nl>  <nl> if ( IS_ERR ( new_root )) { <nl> + dput ( root ); <nl> deactivate_locked_super ( s ); <nl> error = PTR_ERR ( new_root ); <nl> - dput ( root ); <nl> goto error_free_subvol_name ; <nl> } <nl> if (! new_root -> d_inode ) {
skip_create_disk : <nl> blk_queue_max_hw_sectors ( dd -> queue , 0xffff ); <nl> blk_queue_max_segment_size ( dd -> queue , 0x400000 ); <nl> blk_queue_io_min ( dd -> queue , 4096 ); <nl> + blk_queue_bounce_limit ( dd -> queue , dd -> pdev -> dma_mask ); <nl>  <nl> /* <nl> * write back cache is not supported in the device . FUA depends on
static int mmu_topup_memory_cache_page ( struct kvm_mmu_memory_cache * cache , <nl> static void mmu_free_memory_cache_page ( struct kvm_mmu_memory_cache * mc ) <nl> { <nl> while ( mc -> nobjs ) <nl> - __free_page ( mc -> objects [-- mc -> nobjs ]); <nl> + free_page (( unsigned long ) mc -> objects [-- mc -> nobjs ]); <nl> } <nl>  <nl> static int __mmu_topup_memory_caches ( struct kvm_vcpu * vcpu , gfp_t gfp_flags )
void blkcg_drain_queue ( struct request_queue * q ) <nl> { <nl> lockdep_assert_held ( q -> queue_lock ); <nl>  <nl> + /* <nl> + * @ q could be exiting and already have destroyed all blkgs as <nl> + * indicated by NULL root_blkg . If so , don ' t confuse policies . <nl> + */ <nl> + if (! q -> root_blkg ) <nl> + return ; <nl> + <nl> blk_throtl_drain ( q ); <nl> } <nl> 
static int snapshot_status ( struct dm_target * ti , status_type_t type , <nl> unsigned sz = 0 ; <nl> struct dm_snapshot * snap = ti -> private ; <nl>  <nl> + down_write (& snap -> lock ); <nl> + <nl> switch ( type ) { <nl> case STATUSTYPE_INFO : <nl> if (! snap -> valid ) <nl> static int snapshot_status ( struct dm_target * ti , status_type_t type , <nl> break ; <nl> } <nl>  <nl> + up_write (& snap -> lock ); <nl> + <nl> return 0 ; <nl> } <nl> 
static void __init h2_init_smc91x ( void ) <nl>  <nl> static struct i2c_board_info __initdata h2_i2c_board_info [] = { <nl> { <nl> + I2C_BOARD_INFO (" tps65010 ", 0x48 ), <nl> + . type = " tps65010 ", <nl> + . irq = OMAP_GPIO_IRQ ( 58 ), <nl> + }, { <nl> I2C_BOARD_INFO (" isp1301_omap ", 0x2d ), <nl> . type = " isp1301_omap ", <nl> . irq = OMAP_GPIO_IRQ ( 2 ),
int pci_mmap_page_range ( struct pci_dev * dev , struct vm_area_struct * vma , <nl> */ <nl> prot |= _PAGE_CACHE_UC_MINUS ; <nl>  <nl> + prot |= _PAGE_IOMAP ; /* creating a mapping for IO */ <nl> + <nl> vma -> vm_page_prot = __pgprot ( prot ); <nl>  <nl> if ( io_remap_pfn_range ( vma , vma -> vm_start , vma -> vm_pgoff ,
static int __devinit fealnx_init_one ( struct pci_dev * pdev , <nl> if ( np -> flags == HAS_MII_XCVR ) { <nl> int phy , phy_idx = 0 ; <nl>  <nl> - for ( phy = 1 ; phy < 32 && phy_idx < 4 ; phy ++) { <nl> + for ( phy = 1 ; phy < 32 && phy_idx < ARRAY_SIZE ( np -> phys ); <nl> + phy ++) { <nl> int mii_status = mdio_read ( dev , phy , 1 ); <nl>  <nl> if ( mii_status != 0xffff && mii_status != 0x0000 ) {
static int send_reply ( struct svcxprt_rdma * rdma , <nl> " svcrdma : could not post a receive buffer , err =% d ." <nl> " Closing transport % p .\ n ", ret , rdma ); <nl> set_bit ( XPT_CLOSE , & rdma -> sc_xprt . xpt_flags ); <nl> - return 0 ; <nl> + svc_rdma_put_context ( ctxt , 0 ); <nl> + return - ENOTCONN ; <nl> } <nl>  <nl> /* Prepare the context */
struct inode * ovl_d_select_inode ( struct dentry * dentry , unsigned file_flags ) <nl> ovl_path_upper ( dentry , & realpath ); <nl> } <nl>  <nl> + if ( realpath . dentry -> d_flags & DCACHE_OP_SELECT_INODE ) <nl> + return realpath . dentry -> d_op -> d_select_inode ( realpath . dentry , file_flags ); <nl> + <nl> return d_backing_inode ( realpath . dentry ); <nl> } <nl> 
static int cgroup_release_agent_show ( struct seq_file * seq , void * v ) <nl> { <nl> struct cgroup * cgrp = seq_css ( seq )-> cgroup ; <nl>  <nl> - if (! cgroup_lock_live_group ( cgrp )) <nl> - return - ENODEV ; <nl> + spin_lock (& release_agent_path_lock ); <nl> seq_puts ( seq , cgrp -> root -> release_agent_path ); <nl> + spin_unlock (& release_agent_path_lock ); <nl> seq_putc ( seq , '\ n '); <nl> - mutex_unlock (& cgroup_mutex ); <nl> return 0 ; <nl> } <nl> 
int irlan_extract_param ( __u8 * buf , char * name , char * value , __u16 * len ) <nl> memcpy (& val_len , buf + n , 2 ); /* To avoid alignment problems */ <nl> le16_to_cpus (& val_len ); n += 2 ; <nl>  <nl> - if ( val_len > 1016 ) { <nl> + if ( val_len >= 1016 ) { <nl> IRDA_DEBUG ( 2 , "% s (), parameter length to long \ n ", __func__ ); <nl> return - RSP_INVALID_COMMAND_FORMAT ; <nl> }
int ixgbe_ndo_set_vf_spoofchk ( struct net_device * netdev , int vf , bool setting ) <nl> struct ixgbe_hw * hw = & adapter -> hw ; <nl> u32 regval ; <nl>  <nl> + if ( vf >= adapter -> num_vfs ) <nl> + return - EINVAL ; <nl> + <nl> adapter -> vfinfo [ vf ]. spoofchk_enabled = setting ; <nl>  <nl> regval = IXGBE_READ_REG ( hw , IXGBE_PFVFSPOOF ( vf_target_reg ));
static int __devinit snd_cs423x_pnp_init_mpu ( int dev , struct pnp_dev * pdev ) <nl> static int __devinit snd_card_cs4232_pnp ( int dev , struct snd_card_cs4236 * acard , <nl> struct pnp_dev * pdev ) <nl> { <nl> + acard -> wss = pdev ; <nl> if ( snd_cs423x_pnp_init_wss ( dev , acard -> wss ) < 0 ) <nl> return - EBUSY ; <nl> cport [ dev ] = - 1 ;
static int mmc_sdio_resume ( struct mmc_host * host ) <nl> mmc_claim_host ( host ); <nl> err = mmc_sdio_init_card ( host , host -> ocr , host -> card , <nl> ( host -> pm_flags & MMC_PM_KEEP_POWER )); <nl> + if (! err && host -> sdio_irqs ) <nl> + mmc_signal_sdio_irq ( host ); <nl> mmc_release_host ( host ); <nl>  <nl> /*
static void binder_transaction ( struct binder_proc * proc , <nl> proc -> pid , thread -> pid , <nl> ( u64 ) fp -> binder , node -> debug_id , <nl> ( u64 ) fp -> cookie , ( u64 ) node -> cookie ); <nl> + return_error = BR_FAILED_REPLY ; <nl> goto err_binder_get_ref_for_node_failed ; <nl> } <nl> ref = binder_get_ref_for_node ( target_proc , node );
int ip6_mr_input ( struct sk_buff * skb ) <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> cache = ip6mr_cache_find ( mrt ,
void dump_stack ( void ) <nl>  <nl> show_stack ( current , & stack ); <nl> } <nl> + EXPORT_SYMBOL ( dump_stack ); <nl>  <nl> void show_registers ( struct pt_regs * regs ) <nl> {
out_unlock : <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
static void ftdi_process_read ( struct work_struct * work ) <nl> spin_unlock_irqrestore (& priv -> rx_lock , flags ); <nl> dbg ("% s - deferring remainder until unthrottled ", <nl> __func__ ); <nl> - return ; <nl> + goto out ; <nl> } <nl> spin_unlock_irqrestore (& priv -> rx_lock , flags ); <nl> /* if the port is closed stop trying to read */
void wl1271_tx_work_locked ( struct wl1271 * wl ) <nl>  <nl> /* if rates have changed , re - configure the rate policy */ <nl> if ( unlikely ( sta_rates )) { <nl> + ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + woken_up = true ; <nl> + <nl> wl -> rate_set = wl1271_tx_enabled_rates_get ( wl , sta_rates ); <nl> wl1271_acx_rate_policies ( wl ); <nl> }
void card_send_command ( struct ft1000_device * ft1000dev , void * ptempbuffer , <nl>  <nl> DEBUG (" card_send_command : enter card_send_command ... size =% d \ n ", size ); <nl>  <nl> - commandbuf = ( unsigned char *) kmalloc ( size + 2 , GFP_KERNEL ); <nl> + commandbuf = kmalloc ( size + 2 , GFP_KERNEL ); <nl> memcpy (( void *) commandbuf + 2 , ( void *) ptempbuffer , size ); <nl>  <nl> ft1000_read_register ( ft1000dev , & temp , FT1000_REG_DOORBELL );
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> if (! dev -> suspend_in_progress ) <nl> b43_rng_init ( wl ); <nl>  <nl> + ieee80211_wake_queues ( dev -> wl -> hw ); <nl> + <nl> b43_set_status ( dev , B43_STAT_INITIALIZED ); <nl>  <nl> if (! dev -> suspend_in_progress )
static int hyp_init_cpu_notify ( struct notifier_block * self , <nl> switch ( action ) { <nl> case CPU_STARTING : <nl> case CPU_STARTING_FROZEN : <nl> - cpu_init_hyp_mode ( NULL ); <nl> + if ( __hyp_get_vectors () == hyp_default_vectors ) <nl> + cpu_init_hyp_mode ( NULL ); <nl> break ; <nl> } <nl> 
static int pmic_irq_type ( unsigned irq , unsigned type ) <nl> u32 gpio = irq - pg -> irq_base ; <nl> unsigned long flags ; <nl>  <nl> - if ( gpio > pg -> chip . ngpio ) <nl> + if ( gpio >= pg -> chip . ngpio ) <nl> return - EINVAL ; <nl>  <nl> spin_lock_irqsave (& pg -> irqtypes . lock , flags );
int rt2x00mac_set_key ( struct ieee80211_hw * hw , enum set_key_cmd cmd , <nl> crypto . cipher = rt2x00crypto_key_to_cipher ( key ); <nl> if ( crypto . cipher == CIPHER_NONE ) <nl> return - EOPNOTSUPP ; <nl> + if ( crypto . cipher == CIPHER_TKIP && rt2x00_is_usb ( rt2x00dev )) <nl> + return - EOPNOTSUPP ; <nl>  <nl> crypto . cmd = cmd ; <nl> 
int ide_noacpitfs = 1 ; <nl> int ide_noacpionboot = 1 ; <nl> # endif <nl>  <nl> -/* <nl> - * This is declared extern in ide . h , for access by other IDE modules : <nl> - */ <nl> ide_hwif_t ide_hwifs [ MAX_HWIFS ]; /* master data repository */ <nl>  <nl> - EXPORT_SYMBOL ( ide_hwifs ); <nl> - <nl> static void ide_port_init_devices_data ( ide_hwif_t *); <nl>  <nl> /*
static int sep_get_time_handler ( unsigned long arg ) <nl> struct sep_driver_get_time_t command_args ; <nl>  <nl> error = sep_set_time (& command_args . time_physical_address , & command_args . time_value ); <nl> - error = copy_to_user (( void *) arg , ( void *) & command_args , sizeof ( struct sep_driver_get_time_t )); <nl> + if ( error == 0 ) <nl> + error = copy_to_user (( void __user *) arg , <nl> + & command_args , sizeof ( struct sep_driver_get_time_t )); <nl> return error ; <nl>  <nl> }
void rtw_alloc_hwxmits ( struct adapter * padapter ) <nl>  <nl> pxmitpriv -> hwxmit_entry = HWXMIT_ENTRY ; <nl>  <nl> - pxmitpriv -> hwxmits = kzalloc ( sizeof ( struct hw_xmit ) * pxmitpriv -> hwxmit_entry , GFP_KERNEL ); <nl> + pxmitpriv -> hwxmits = kcalloc ( pxmitpriv -> hwxmit_entry , <nl> + sizeof ( struct hw_xmit ), GFP_KERNEL ); <nl>  <nl> hwxmits = pxmitpriv -> hwxmits ; <nl> 
static int sg_io ( struct file * file , request_queue_t * q , <nl>  <nl> rq -> cmd_type = REQ_TYPE_BLOCK_PC ; <nl>  <nl> - /* <nl> - * bounce this after holding a reference to the original bio , it ' s <nl> - * needed for proper unmapping <nl> - */ <nl> - if ( rq -> bio ) <nl> - blk_queue_bounce ( q , & rq -> bio ); <nl> - <nl> rq -> timeout = jiffies_to_msecs ( hdr -> timeout ); <nl> if (! rq -> timeout ) <nl> rq -> timeout = q -> sg_timeout ;
void hists__output_recalc_col_len ( struct hists * hists , int max_rows ) <nl>  <nl> while ( next && row ++ < max_rows ) { <nl> n = rb_entry ( next , struct hist_entry , rb_node ); <nl> - hists__calc_col_len ( hists , n ); <nl> + if (! n -> filtered ) <nl> + hists__calc_col_len ( hists , n ); <nl> next = rb_next (& n -> rb_node ); <nl> } <nl> }
dma_addr_t xhci_trb_virt_to_dma ( struct xhci_segment * seg , <nl> return 0 ; <nl> /* offset in TRBs */ <nl> segment_offset = trb - seg -> trbs ; <nl> - if ( segment_offset > TRBS_PER_SEGMENT ) <nl> + if ( segment_offset >= TRBS_PER_SEGMENT ) <nl> return 0 ; <nl> return seg -> dma + ( segment_offset * sizeof (* trb )); <nl> }
void sctp_transport_lower_cwnd ( struct sctp_transport * transport , <nl> transport -> ssthresh = max ( transport -> cwnd / 2 , <nl> 4 * transport -> asoc -> pathmtu ); <nl> transport -> cwnd = transport -> asoc -> pathmtu ; <nl> + <nl> + /* T3 - rtx also clears fast recovery on the transport */ <nl> + transport -> fast_recovery = 0 ; <nl> break ; <nl>  <nl> case SCTP_LOWER_CWND_FAST_RTX :
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> + memset ( sax , 0 , sizeof ( sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
static void cirrus_connector_destroy ( struct drm_connector * connector ) <nl> kfree ( connector ); <nl> } <nl>  <nl> - struct drm_connector_helper_funcs cirrus_vga_connector_helper_funcs = { <nl> + static const struct drm_connector_helper_funcs cirrus_vga_connector_helper_funcs = { <nl> . get_modes = cirrus_vga_get_modes , <nl> . best_encoder = cirrus_connector_best_encoder , <nl> }; <nl>  <nl> - struct drm_connector_funcs cirrus_vga_connector_funcs = { <nl> + static const struct drm_connector_funcs cirrus_vga_connector_funcs = { <nl> . dpms = drm_helper_connector_dpms , <nl> . detect = cirrus_vga_detect , <nl> . fill_modes = drm_helper_probe_single_connector_modes ,
static unsigned long ramfs_nommu_get_unmapped_area ( struct file * file , <nl>  <nl> /* gang - find the pages */ <nl> ret = - ENOMEM ; <nl> - pages = kzalloc ( lpages * sizeof ( struct page *), GFP_KERNEL ); <nl> + pages = kcalloc ( lpages , sizeof ( struct page *), GFP_KERNEL ); <nl> if (! pages ) <nl> goto out_free ; <nl> 
vt6656_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl> hw = ieee80211_alloc_hw ( sizeof ( struct vnt_private ), & vnt_mac_ops ); <nl> if (! hw ) { <nl> dev_err (& udev -> dev , " could not register ieee80211_hw \ n "); <nl> + rc = - ENOMEM ; <nl> goto err_nomem ; <nl> } <nl> 
static long media_device_enum_entities ( struct media_device * mdev , <nl> struct media_entity * ent ; <nl> struct media_entity_desc u_ent ; <nl>  <nl> + memset (& u_ent , 0 , sizeof ( u_ent )); <nl> if ( copy_from_user (& u_ent . id , & uent -> id , sizeof ( u_ent . id ))) <nl> return - EFAULT ; <nl> 
static int ethtool_phys_id ( struct net_device * dev , void __user * useraddr ) <nl> if ( rc == 0 ) { <nl> /* Driver will handle this itself */ <nl> schedule_timeout_interruptible ( <nl> - id . data ? id . data : MAX_SCHEDULE_TIMEOUT ); <nl> + id . data ? ( id . data * HZ ) : MAX_SCHEDULE_TIMEOUT ); <nl> } else { <nl> /* Driver expects to be called periodically */ <nl> do {
static int scsi_eh_completed_normally ( struct scsi_cmnd * scmd ) <nl> scsi_handle_queue_full ( scmd -> device ); <nl> /* fall through */ <nl> case BUSY : <nl> + return NEEDS_RETRY ; <nl> default : <nl> return FAILED ; <nl> }
static int easycap_usb_probe ( struct usb_interface * intf , <nl> /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */ <nl> pdata_urb = kzalloc ( sizeof ( struct data_urb ), GFP_KERNEL ); <nl> if (! pdata_urb ) { <nl> + usb_free_urb ( purb ); <nl> SAM (" ERROR : Could not allocate struct data_urb .\ n "); <nl> return - ENOMEM ; <nl> }
static int econet_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> if ( peer ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset ( sec , 0 , sizeof (* sec )); <nl> mutex_lock (& econet_mutex ); <nl>  <nl> sk = sock -> sk ;
void hugetlb_unreserve_pages ( struct inode * inode , long offset , long freed ) <nl> long chg = region_truncate (& inode -> i_mapping -> private_list , offset ); <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> - inode -> i_blocks -= blocks_per_huge_page ( h ); <nl> + inode -> i_blocks -= ( blocks_per_huge_page ( h ) * freed ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> hugetlb_put_quota ( inode -> i_mapping , ( chg - freed ));
static enum i40e_media_type i40e_get_media_type ( struct i40e_hw * hw ) <nl> } <nl>  <nl> # define I40E_PF_RESET_WAIT_COUNT_A0 200 <nl> -# define I40E_PF_RESET_WAIT_COUNT 110 <nl> +# define I40E_PF_RESET_WAIT_COUNT 200 <nl> /** <nl> * i40e_pf_reset - Reset the PF <nl> * @ hw : pointer to the hardware structure
int __init main ( int argc , char ** argv , char ** envp ) <nl> # endif <nl>  <nl> do_uml_initcalls (); <nl> + change_sig ( SIGPIPE , 0 ); <nl> ret = linux_main ( argc , argv ); <nl>  <nl> /*
ath_reg_apply_active_scan_flags ( struct wiphy * wiphy , <nl> int r ; <nl>  <nl> sband = wiphy -> bands [ IEEE80211_BAND_2GHZ ]; <nl> + if (! sband ) <nl> + return ; <nl>  <nl> /* <nl> * If no country IE has been received always enable active scan
int do_sigtimedwait ( const sigset_t * which , siginfo_t * info , <nl> recalc_sigpending (); <nl> spin_unlock_irq (& tsk -> sighand -> siglock ); <nl>  <nl> - timeout = schedule_timeout_interruptible ( timeout ); <nl> + timeout = freezable_schedule_timeout_interruptible ( timeout ); <nl>  <nl> spin_lock_irq (& tsk -> sighand -> siglock ); <nl> __set_task_blocked ( tsk , & tsk -> real_blocked );
static int ir_do_setkeycode ( struct input_dev * dev , <nl> break ; <nl> } <nl>  <nl> - if ( old_keycode == KEY_RESERVED ) { <nl> + if ( old_keycode == KEY_RESERVED && keycode != KEY_RESERVED ) { <nl> /* No previous mapping found , we might need to grow the table */ <nl> if ( ir_resize_table ( rc_tab )) <nl> return - ENOMEM ;
static int ipw_up ( struct ipw_priv * priv ) <nl> if (!( priv -> config & CFG_CUSTOM_MAC )) <nl> eeprom_parse_mac ( priv , priv -> mac_addr ); <nl> memcpy ( priv -> net_dev -> dev_addr , priv -> mac_addr , ETH_ALEN ); <nl> + memcpy ( priv -> net_dev -> perm_addr , priv -> mac_addr , ETH_ALEN ); <nl>  <nl> for ( j = 0 ; j < ARRAY_SIZE ( ipw_geos ); j ++) { <nl> if (! memcmp (& priv -> eeprom [ EEPROM_COUNTRY_CODE ],
int brcmf_fws_hdrpull ( struct brcmf_pub * drvr , int ifidx , s16 signal_len , <nl> if (! signal_len ) <nl> return 0 ; <nl> /* if flow control disabled , skip to packet data and leave */ <nl> - if (! fws -> fw_signals ) { <nl> + if ((! fws ) || (! fws -> fw_signals )) { <nl> skb_pull ( skb , signal_len ); <nl> return 0 ; <nl> }
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 84 " <nl> -# define DRV_MODULE_RELDATE " October 12 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 85 " <nl> +# define DRV_MODULE_RELDATE " October 18 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static int omap3_onenand_read_bufferram ( struct mtd_info * mtd , int area , <nl> if ( bram_offset & 3 || ( size_t ) buf & 3 || count < 384 ) <nl> goto out_copy ; <nl>  <nl> + /* panic_write () may be in an interrupt context */ <nl> + if ( in_interrupt ()) <nl> + goto out_copy ; <nl> + <nl> if ( buf >= high_memory ) { <nl> struct page * p1 ; <nl> 
static void __init xen_rebuild_p2m_list ( unsigned long * p2m ) <nl> p2m_missing_pte : p2m_identity_pte ; <nl> for ( i = 0 ; i < PMDS_PER_MID_PAGE ; i ++) { <nl> pmdp = populate_extra_pmd ( <nl> - ( unsigned long )( p2m + pfn + i * PTRS_PER_PTE )); <nl> + ( unsigned long )( p2m + pfn ) + i * PMD_SIZE ); <nl> set_pmd ( pmdp , __pmd ( __pa ( ptep ) | _KERNPG_TABLE )); <nl> } <nl> }
static int vfio_set_trigger ( struct vfio_platform_device * vdev , int index , <nl> int ret ; <nl>  <nl> if ( irq -> trigger ) { <nl> + irq_clear_status_flags ( irq -> hwirq , IRQ_NOAUTOEN ); <nl> free_irq ( irq -> hwirq , irq ); <nl> kfree ( irq -> name ); <nl> eventfd_ctx_put ( irq -> trigger );
int max1363_single_channel_from_ring ( long mask , struct max1363_state * st ) <nl> ret = - EBUSY ; <nl> goto error_ret ; <nl> } <nl> - numvals = hweight_long ( st -> current_mode -> modemask ); <nl>  <nl> - ring_data = kmalloc ( numvals * 2 , GFP_KERNEL ); <nl> + ring_data = kmalloc ( ring -> access . get_bytes_per_datum ( ring ), GFP_KERNEL ); <nl> if ( ring_data == NULL ) { <nl> ret = - ENOMEM ; <nl> goto error_ret ;
void kvm_ioapic_calculate_eoi_exitmap ( struct kvm_vcpu * vcpu , <nl> if (! e -> fields . mask && <nl> ( e -> fields . trig_mode == IOAPIC_LEVEL_TRIG || <nl> kvm_irq_has_notifier ( ioapic -> kvm , KVM_IRQCHIP_IOAPIC , <nl> - index ))) { <nl> + index ) || index == RTC_GSI )) { <nl> if ( kvm_apic_match_dest ( vcpu , NULL , 0 , <nl> e -> fields . dest_id , e -> fields . dest_mode )) <nl> __set_bit ( e -> fields . vector , ( unsigned long *) eoi_exit_bitmap );
DECLARE_EVENT_CLASS ( xhci_log_event , <nl> __field ( u64 , dma ) <nl> __field ( u32 , status ) <nl> __field ( u32 , flags ) <nl> - __dynamic_array ( __le32 , trb , 4 ) <nl> + __dynamic_array ( u8 , trb , sizeof ( struct xhci_generic_trb )) <nl> ), <nl> TP_fast_assign ( <nl> __entry -> va = trb_va ;
static int validate_region_size ( struct raid_set * rs , unsigned long region_size ) <nl> static int validate_raid_redundancy ( struct raid_set * rs ) <nl> { <nl> unsigned i , rebuild_cnt = 0 ; <nl> - unsigned rebuilds_per_group , copies , d ; <nl> + unsigned rebuilds_per_group = 0 , copies , d ; <nl> unsigned group_size , last_group_start ; <nl>  <nl> for ( i = 0 ; i < rs -> md . raid_disks ; i ++)
static int exynos_drm_fbdev_update ( struct drm_fb_helper * helper , <nl>  <nl> fbi -> screen_base = buffer -> kvaddr + offset ; <nl> fbi -> screen_size = size ; <nl> + fbi -> fix . smem_len = size ; <nl>  <nl> return 0 ; <nl> }
static void sonic_rx ( struct net_device * dev ) <nl> status = sonic_rda_get ( dev , entry , SONIC_RD_STATUS ); <nl> if ( status & SONIC_RCR_PRX ) { <nl> /* Malloc up new buffer . */ <nl> - new_skb = netdev_alloc_skb ( SONIC_RBSIZE + 2 ); <nl> + new_skb = netdev_alloc_skb ( dev , SONIC_RBSIZE + 2 ); <nl> if ( new_skb == NULL ) { <nl> printk ( KERN_ERR "% s : Memory squeeze , dropping packet .\ n ", dev -> name ); <nl> lp -> stats . rx_dropped ++;
static void tpacket_destruct_skb ( struct sk_buff * skb ) <nl> struct packet_sock * po = pkt_sk ( skb -> sk ); <nl> void * ph ; <nl>  <nl> - BUG_ON ( skb == NULL ); <nl> - <nl> if ( likely ( po -> tx_ring . pg_vec )) { <nl> ph = skb_shinfo ( skb )-> destructor_arg ; <nl> BUG_ON ( __packet_get_status ( po , ph ) != TP_STATUS_SENDING );
unsigned long long sched_clock ( void ) <nl>  <nl> /* jiffies based sched_clock if no clocksource is installed */ <nl> if (! clocksource_sh . rating ) <nl> - return ( unsigned long long ) jiffies * ( NSEC_PER_SEC / HZ ); <nl> + return ( jiffies_64 - INITIAL_JIFFIES ) * ( NSEC_PER_SEC / HZ ); <nl>  <nl> cycles = clocksource_sh . read (& clocksource_sh ); <nl> return cyc2ns (& clocksource_sh , cycles );
static int crypto_ccm_auth ( struct aead_request * req , struct scatterlist * plain , <nl> if ( assoclen ) { <nl> pctx -> ilen = format_adata ( idata , assoclen ); <nl> get_data_to_compute ( cipher , pctx , req -> assoc , req -> assoclen ); <nl> + } else { <nl> + pctx -> ilen = 0 ; <nl> } <nl>  <nl> /* compute plaintext into mac */
probe_fail_cdrom_register : <nl> del_gendisk ( gd . disk ); <nl> probe_fail_no_disk : <nl> kfree ( gd . cd_info ); <nl> + probe_fail_no_mem : <nl> unregister_blkdev ( gdrom_major , GDROM_DEV_NAME ); <nl> gdrom_major = 0 ; <nl> - probe_fail_no_mem : <nl> pr_warning (" Probe failed - error is 0x % X \ n ", err ); <nl> return err ; <nl> }
skip_type : <nl> if ( pmu -> pmu_cpu_context ) <nl> goto got_cpu_context ; <nl>  <nl> + ret = - ENOMEM ; <nl> pmu -> pmu_cpu_context = alloc_percpu ( struct perf_cpu_context ); <nl> if (! pmu -> pmu_cpu_context ) <nl> goto free_dev ;
int vnt_init ( struct vnt_private * priv ) <nl>  <nl> priv -> mac_hw = true ; <nl>  <nl> + vnt_radio_power_off ( priv ); <nl> + <nl> return 0 ; <nl> } <nl> 
int eprintf ( int level , const char * fmt , ...) <nl>  <nl> if ( verbose >= level ) { <nl> va_start ( args , fmt ); <nl> - if ( use_browser > 1 ) <nl> + if ( use_browser >= 1 ) <nl> ui_helpline__vshow ( fmt , args ); <nl> else <nl> ret = vfprintf ( stderr , fmt , args );
int scsi_error_handler ( void * data ) <nl> set_current_state ( TASK_INTERRUPTIBLE ); <nl> } <nl>  <nl> + __set_current_state ( TASK_RUNNING ); <nl> + <nl> SCSI_LOG_ERROR_RECOVERY ( 1 , printk (" Error handler scsi_eh_ % d " <nl> " exiting \ n ", shost -> host_no )); <nl> 
static struct scsi_host_template aac_driver_template = { <nl>  <nl> static void __aac_shutdown ( struct aac_dev * aac ) <nl> { <nl> - kthread_stop ( aac -> thread ); <nl> + if ( aac -> aif_thread ) <nl> + kthread_stop ( aac -> thread ); <nl> aac_send_shutdown ( aac ); <nl> aac_adapter_disable_int ( aac ); <nl> free_irq ( aac -> pdev -> irq , aac );
static void ieee80211_enable_ps ( struct ieee80211_local * local , <nl> { <nl> struct ieee80211_conf * conf = & local -> hw . conf ; <nl>  <nl> + /* <nl> + * If we are scanning right now then the parameters will <nl> + * take effect when scan finishes . <nl> + */ <nl> + if ( local -> hw_scanning || local -> sw_scanning ) <nl> + return ; <nl> + <nl> if ( conf -> dynamic_ps_timeout > 0 && <nl> !( local -> hw . flags & IEEE80211_HW_SUPPORTS_DYNAMIC_PS )) { <nl> mod_timer (& local -> dynamic_ps_timer , jiffies +
static ssize_t ci_port_test_write ( struct file * file , const char __user * ubuf , <nl> if ( sscanf ( buf , "% u ", & mode ) != 1 ) <nl> return - EINVAL ; <nl>  <nl> + if ( mode > 255 ) <nl> + return - EBADRQC ; <nl> + <nl> pm_runtime_get_sync ( ci -> dev ); <nl> spin_lock_irqsave (& ci -> lock , flags ); <nl> ret = hw_port_test_set ( ci , mode );
static int change_memory_common ( unsigned long addr , int numpages , <nl> WARN_ON_ONCE ( 1 ); <nl> } <nl>  <nl> + if (! numpages ) <nl> + return 0 ; <nl> + <nl> if ( start < MODULES_VADDR || start >= MODULES_END ) <nl> return - EINVAL ; <nl> 
scsi_internal_device_unblock ( struct scsi_device * sdev , <nl> * Try to transition the scsi device to SDEV_RUNNING or one of the <nl> * offlined states and goose the device queue if successful . <nl> */ <nl> - if ( sdev -> sdev_state == SDEV_BLOCK ) <nl> + if (( sdev -> sdev_state == SDEV_BLOCK ) || <nl> + ( sdev -> sdev_state == SDEV_TRANSPORT_OFFLINE )) <nl> sdev -> sdev_state = new_state ; <nl> else if ( sdev -> sdev_state == SDEV_CREATED_BLOCK ) { <nl> if ( new_state == SDEV_TRANSPORT_OFFLINE ||
static int gen6_drpc_info ( struct seq_file * m ) <nl>  <nl> rpmodectl1 = I915_READ ( GEN6_RP_CONTROL ); <nl> rcctl1 = I915_READ ( GEN6_RC_CONTROL ); <nl> - sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> mutex_unlock (& dev -> struct_mutex ); <nl> + mutex_lock (& dev_priv -> rps . hw_lock ); <nl> + sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> + mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> seq_printf ( m , " Video Turbo Mode : % s \ n ", <nl> yesno ( rpmodectl1 & GEN6_RP_MEDIA_TURBO ));
static int __devinit piix_init_one ( struct pci_dev * pdev , <nl> u8 tmp ; <nl> pci_read_config_byte ( pdev , PIIX_SCC , & tmp ); <nl> if ( tmp == PIIX_AHCI_DEVICE ) { <nl> - int rc = piix_disable_ahci ( pdev ); <nl> + rc = piix_disable_ahci ( pdev ); <nl> if ( rc ) <nl> return rc ; <nl> }
static bool compliance_mode_recovery_timer_quirk_check ( void ) <nl>  <nl> dmi_product_name = dmi_get_system_info ( DMI_PRODUCT_NAME ); <nl> dmi_sys_vendor = dmi_get_system_info ( DMI_SYS_VENDOR ); <nl> + if (! dmi_product_name || ! dmi_sys_vendor ) <nl> + return false ; <nl>  <nl> if (!( strstr ( dmi_sys_vendor , " Hewlett - Packard "))) <nl> return false ;
void ixgbe_update_stats ( struct ixgbe_adapter * adapter ) <nl> u32 i , missed_rx = 0 , mpc , bprc , lxon , lxoff , xon_off_tot ; <nl> u64 non_eop_descs = 0 , restart_queue = 0 ; <nl>  <nl> + if ( test_bit ( __IXGBE_DOWN , & adapter -> state ) || <nl> + test_bit ( __IXGBE_RESETTING , & adapter -> state )) <nl> + return ; <nl> + <nl> if ( adapter -> flags2 & IXGBE_FLAG2_RSC_ENABLED ) { <nl> u64 rsc_count = 0 ; <nl> u64 rsc_flush = 0 ;
DEFINE_PER_CPU_READ_MOSTLY ( cpumask_var_t , cpu_llc_shared_map ); <nl> DEFINE_PER_CPU_SHARED_ALIGNED ( struct cpuinfo_x86 , cpu_info ); <nl> EXPORT_PER_CPU_SYMBOL ( cpu_info ); <nl>  <nl> - static DEFINE_PER_CPU ( struct completion , die_complete ); <nl> - <nl> atomic_t init_deasserted ; <nl>  <nl> /* <nl> void cpu_disable_common ( void ) <nl> fixup_irqs (); <nl> } <nl>  <nl> + static DEFINE_PER_CPU ( struct completion , die_complete ); <nl> + <nl> int native_cpu_disable ( void ) <nl> { <nl> int ret ;
static int __init wb_module_init ( void ) <nl>  <nl> err = map_bios (); <nl> if ( err ) <nl> - return err ; <nl> + goto err_free_keymap ; <nl>  <nl> err = platform_driver_register (& wistron_driver ); <nl> if ( err ) <nl> static int __init wb_module_init ( void ) <nl> platform_driver_unregister (& wistron_driver ); <nl> err_unmap_bios : <nl> unmap_bios (); <nl> + err_free_keymap : <nl> + kfree ( keymap ); <nl>  <nl> return err ; <nl> }
static unsigned long cfq_slice_offset ( struct cfq_data * cfqd , <nl> /* <nl> * just an approximation , should be ok . <nl> */ <nl> - return (( cfqd -> busy_queues - 1 ) * cfq_prio_slice ( cfqd , 1 , 0 )); <nl> + return ( cfqd -> busy_queues - 1 ) * ( cfq_prio_slice ( cfqd , 1 , 0 ) - <nl> + cfq_prio_slice ( cfqd , cfq_cfqq_sync ( cfqq ), cfqq -> ioprio )); <nl> } <nl>  <nl> /*
static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
static void tg3_timer ( unsigned long __opaque ) <nl> * resets . <nl> */ <nl> if (!-- tp -> asf_counter ) { <nl> - if ( tp -> tg3_flags & TG3_FLAG_ENABLE_ASF ) { <nl> + if (( tp -> tg3_flags & TG3_FLAG_ENABLE_ASF ) && <nl> + !( tp -> tg3_flags3 & TG3_FLG3_ENABLE_APE )) { <nl> u32 val ; <nl>  <nl> tg3_wait_for_event_ack ( tp );
static int zoran_dqbuf ( struct file * file , void * __fh , struct v4l2_buffer * buf ) <nl> res = - EAGAIN ; <nl> goto dqbuf_unlock_and_return ; <nl> } <nl> + bs . frame = 0 ; /* suppress compiler warning */ <nl> res = jpg_sync ( fh , & bs ); <nl> if ( res ) <nl> goto dqbuf_unlock_and_return ;
static irqreturn_t intel_sst_interrupt ( int irq , void * context ) <nl> unsigned int size = 0 , str_id ; <nl> struct stream_info * stream ; <nl>  <nl> + /* Do not handle interrupt in suspended state */ <nl> + if ( drv -> sst_state == SST_SUSPENDED ) <nl> + return IRQ_NONE ; <nl> /* Interrupt arrived , check src */ <nl> isr . full = sst_shim_read ( drv -> shim , SST_ISRX ); <nl> 
static unsigned long iommu_range_alloc ( struct iommu_table * tbl , <nl> /* This allocator was derived from x86_64 ' s bit string search */ <nl>  <nl> /* Sanity check */ <nl> - if ( unlikely ( npages ) == 0 ) { <nl> + if ( unlikely ( npages == 0 )) { <nl> if ( printk_ratelimit ()) <nl> WARN_ON ( 1 ); <nl> return DMA_ERROR_CODE ;
static struct platform_driver omap_hsmmc_driver = { <nl> static int __init omap_hsmmc_init ( void ) <nl> { <nl> /* Register the MMC driver */ <nl> - return platform_driver_register (& omap_hsmmc_driver ); <nl> + return platform_driver_probe (& omap_hsmmc_driver , omap_hsmmc_probe ); <nl> } <nl>  <nl> static void __exit omap_hsmmc_cleanup ( void )
store_priv_session_ ## field ( struct device * dev , \ <nl> # define iscsi_priv_session_rw_attr ( field , format ) \ <nl> iscsi_priv_session_attr_show ( field , format ) \ <nl> iscsi_priv_session_attr_store ( field ) \ <nl> - static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUGO , \ <nl> + static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUSR , \ <nl> show_priv_session_ ## field , \ <nl> store_priv_session_ ## field ) <nl> iscsi_priv_session_rw_attr ( recovery_tmo , "% d ");
static int dr_interception ( struct vcpu_svm * svm ) <nl> kvm_register_write (& svm -> vcpu , reg , val ); <nl> } <nl>  <nl> + skip_emulated_instruction (& svm -> vcpu ); <nl> + <nl> return 1 ; <nl> } <nl> 
EXPORT_SYMBOL ( of_find_node_with_property ); <nl> const struct of_device_id * of_match_node ( const struct of_device_id * matches , <nl> const struct device_node * node ) <nl> { <nl> + if (! matches ) <nl> + return NULL ; <nl> + <nl> while ( matches -> name [ 0 ] || matches -> type [ 0 ] || matches -> compatible [ 0 ]) { <nl> int match = 1 ; <nl> if ( matches -> name [ 0 ])
static struct fileIdentDesc * udf_find_entry ( struct inode * dir , <nl> } <nl>  <nl> if (( cfi -> fileCharacteristics & FID_FILE_CHAR_PARENT ) && <nl> - isdotdot ) { <nl> - brelse ( epos . bh ); <nl> - return fi ; <nl> - } <nl> + isdotdot ) <nl> + goto out_ok ; <nl>  <nl> if (! lfi ) <nl> continue ;
struct led_classdev { <nl>  <nl> extern int led_classdev_register ( struct device * parent , <nl> struct led_classdev * led_cdev ); <nl> - extern void led_classdev_unregister ( struct led_classdev * lcd ); <nl> + extern void led_classdev_unregister ( struct led_classdev * led_cdev ); <nl> extern void led_classdev_suspend ( struct led_classdev * led_cdev ); <nl> extern void led_classdev_resume ( struct led_classdev * led_cdev ); <nl> 
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> + if ( ret != sizeof ( event )) { <nl> + printf (" Reading event failed !\ n "); <nl> + ret = - EIO ; <nl> + break ; <nl> + } <nl> + <nl> print_event (& event ); <nl> } <nl> 
static int mc13783_probe ( struct snd_soc_codec * codec ) <nl> { <nl> struct mc13783_priv * priv = snd_soc_codec_get_drvdata ( codec ); <nl>  <nl> - codec -> control_data = priv -> mc13xxx ; <nl> - <nl> mc13xxx_lock ( priv -> mc13xxx ); <nl>  <nl> /* these are the reset values */
static const struct attribute_group isl29108_group = { <nl> static int isl29018_chip_init ( struct isl29018_chip * chip ) <nl> { <nl> int status ; <nl> - int new_adc_bit ; <nl> + unsigned int new_adc_bit ; <nl> unsigned int new_range ; <nl>  <nl> /* Code added per Intersil Application Note 1534 :
static int ov7670_read ( struct i2c_client * c , unsigned char reg , <nl> int ret ; <nl>  <nl> ret = i2c_smbus_read_byte_data ( c , reg ); <nl> - if ( ret >= 0 ) <nl> + if ( ret >= 0 ) { <nl> * value = ( unsigned char ) ret ; <nl> + ret = 0 ; <nl> + } <nl> return ret ; <nl> } <nl> 
static ssize_t mtd_write ( struct file * file , const char __user * buf , size_t count <nl> ops . mode = MTD_OOB_RAW ; <nl> ops . datbuf = kbuf ; <nl> ops . oobbuf = NULL ; <nl> + ops . ooboffs = 0 ; <nl> ops . len = len ; <nl>  <nl> ret = mtd -> write_oob ( mtd , * ppos , & ops );
struct ion_device * ion_device_create ( long (* custom_ioctl ) <nl> ret = misc_register (& idev -> dev ); <nl> if ( ret ) { <nl> pr_err (" ion : failed to register misc device .\ n "); <nl> + kfree ( idev ); <nl> return ERR_PTR ( ret ); <nl> } <nl> 
static enum odd_mech_type zpodd_get_mech_type ( struct ata_device * dev ) <nl> static bool odd_can_poweroff ( struct ata_device * ata_dev ) <nl> { <nl> acpi_handle handle ; <nl> - acpi_status status ; <nl> struct acpi_device * acpi_dev ; <nl>  <nl> handle = ata_dev_acpi_handle ( ata_dev ); <nl> if (! handle ) <nl> return false ; <nl>  <nl> - status = acpi_bus_get_device ( handle , & acpi_dev ); <nl> - if ( ACPI_FAILURE ( status )) <nl> + if ( acpi_bus_get_device ( handle , & acpi_dev )) <nl> return false ; <nl>  <nl> return acpi_device_can_poweroff ( acpi_dev );
static int corsair_input_mapping ( struct hid_device * dev , <nl> { <nl> int gkey ; <nl>  <nl> + if (( usage -> hid & HID_USAGE_PAGE ) != HID_UP_KEYBOARD ) <nl> + return 0 ; <nl> + <nl> gkey = corsair_usage_to_gkey ( usage -> hid & HID_USAGE ); <nl> if ( gkey != 0 ) { <nl> hid_map_usage_clear ( input , usage , bit , max , EV_KEY ,
remove_write ( struct device_driver * drv , const char * buf , size_t count ) <nl> count = IFNAMSIZ - 1 ; <nl>  <nl> for ( i = 0 , p =( char *) buf ; i < count && * p ; i ++, p ++) { <nl> - if ((* p == '\ n ') | (* p == ' ')) { <nl> + if ((* p == '\ n ') || (* p == ' ')) { <nl> /* trailing lf , grr */ <nl> break ; <nl> } else {
void radeon_compute_pll ( struct radeon_pll * pll , <nl> * frac_fb_div_p = best_frac_feedback_div ; <nl> * ref_div_p = best_ref_div ; <nl> * post_div_p = best_post_div ; <nl> + DRM_DEBUG_KMS ("% d % d , pll dividers - fb : % d .% d ref : % d , post % d \ n ", <nl> + freq , best_freq / 1000 , best_feedback_div , best_frac_feedback_div , <nl> + best_ref_div , best_post_div ); <nl> + <nl> } <nl>  <nl> static void radeon_user_framebuffer_destroy ( struct drm_framebuffer * fb )
static __devinit int vpbe_probe ( struct platform_device * pdev ) <nl>  <nl> if ( cfg -> outputs -> num_modes > 0 ) <nl> vpbe_dev -> current_timings = vpbe_dev -> cfg -> outputs [ 0 ]. modes [ 0 ]; <nl> - else <nl> + else { <nl> + kfree ( vpbe_dev ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> /* set the driver data in platform device */ <nl> platform_set_drvdata ( pdev , vpbe_dev );
static int bnx2x_init_dev ( struct bnx2x * bp , struct pci_dev * pdev , <nl> pci_write_config_dword ( bp -> pdev , PCICFG_GRC_ADDRESS , <nl> PCICFG_VENDOR_ID_OFFSET ); <nl>  <nl> + /* Set PCIe reset type to fundamental for EEH recovery */ <nl> + pdev -> needs_freset = 1 ; <nl> + <nl> /* AER ( Advanced Error reporting ) configuration */ <nl> rc = pci_enable_pcie_error_reporting ( pdev ); <nl> if (! rc )
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
static int sn_hwperf_op_cpu ( struct sn_hwperf_op_info * op_info ) <nl> else { <nl> /* migrate the task before calling SAL */ <nl> save_allowed = current -> cpus_allowed ; <nl> - set_cpus_allowed ( current , cpumask_of_cpu ( cpu )); <nl> + set_cpus_allowed_ptr ( current , cpumask_of ( cpu )); <nl> sn_hwperf_call_sal ( op_info ); <nl> - set_cpus_allowed ( current , save_allowed ); <nl> + set_cpus_allowed_ptr ( current , & save_allowed ); <nl> } <nl> } <nl> r = op_info -> ret ;
static int intel_crtc_page_flip ( struct drm_crtc * crtc , <nl> to_intel_plane ( primary )-> frontbuffer_bit ); <nl>  <nl> intel_fbc_disable ( dev ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> intel_frontbuffer_flip_prepare ( dev , <nl> to_intel_plane ( primary )-> frontbuffer_bit ); <nl> - mutex_unlock (& dev -> struct_mutex ); <nl>  <nl> trace_i915_flip_request ( intel_crtc -> plane , obj ); <nl> 
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static int hdmi_pcm_open ( struct hda_pcm_stream * hinfo , <nl> * codec_pars = * hinfo ; <nl>  <nl> eld = & spec -> sink_eld [ idx ]; <nl> - if ( eld -> sad_count > 0 ) { <nl> + if ( eld -> eld_valid && eld -> sad_count > 0 ) { <nl> hdmi_eld_update_pcm_info ( eld , hinfo , codec_pars ); <nl> if ( hinfo -> channels_min > hinfo -> channels_max || <nl> ! hinfo -> rates || ! hinfo -> formats )
# define SEEK_MAX SEEK_END <nl>  <nl> struct fstrim_range { <nl> - uint64_t start ; <nl> - uint64_t len ; <nl> - uint64_t minlen ; <nl> + __u64 start ; <nl> + __u64 len ; <nl> + __u64 minlen ; <nl> }; <nl>  <nl> /* And dynamically - tunable limits and defaults : */
static int do_end_io ( struct multipath * m , struct request * clone , <nl> if (! error && ! clone -> errors ) <nl> return 0 ; /* I / O complete */ <nl>  <nl> - if ( error == - EOPNOTSUPP || error == - EREMOTEIO ) <nl> + if ( error == - EOPNOTSUPP || error == - EREMOTEIO || error == - EILSEQ ) <nl> return error ; <nl>  <nl> if ( mpio -> pgpath )
static int m5mols_s_stream ( struct v4l2_subdev * sd , int enable ) <nl> if ( enable ) { <nl> if ( is_code ( code , M5MOLS_RESTYPE_MONITOR )) <nl> ret = m5mols_start_monitor ( info ); <nl> - if ( is_code ( code , M5MOLS_RESTYPE_CAPTURE )) <nl> + else if ( is_code ( code , M5MOLS_RESTYPE_CAPTURE )) <nl> ret = m5mols_start_capture ( info ); <nl> else <nl> ret = - EINVAL ;
static int __devinit e100_probe ( struct pci_dev * pdev , <nl>  <nl> e100_get_defaults ( nic ); <nl>  <nl> + /* D100 MAC doesn ' t allow rx of vlan packets with normal MTU */ <nl> + if ( nic -> mac < mac_82558_D101_A4 ) <nl> + netdev -> features |= NETIF_F_VLAN_CHALLENGED ; <nl> + <nl> /* locks must be initialized before calling hw_reset */ <nl> spin_lock_init (& nic -> cb_lock ); <nl> spin_lock_init (& nic -> cmd_lock );
again : <nl> key . offset = split ; <nl>  <nl> ret = btrfs_search_slot ( trans , root , & key , path , - 1 , 1 ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> if ( ret > 0 && path -> slots [ 0 ] > 0 ) <nl> path -> slots [ 0 ]--; <nl> 
static int snd_hdsp_get_adat_sync_check ( struct snd_kcontrol * kcontrol , struct sn <nl> struct hdsp * hdsp = snd_kcontrol_chip ( kcontrol ); <nl>  <nl> offset = ucontrol -> id . index - 1 ; <nl> - snd_BUG_ON ( offset < 0 ); <nl> + if ( snd_BUG_ON ( offset < 0 )) <nl> + return - EINVAL ; <nl>  <nl> switch ( hdsp -> io_type ) { <nl> case Digiface :
int snd_hda_parse_pin_def_config ( struct hda_codec * codec , <nl> cfg -> input_pins [ AUTO_PIN_AUX ] = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_OUT : <nl> + case AC_JACK_DIG_OTHER_OUT : <nl> cfg -> dig_out_pin = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_IN : <nl> + case AC_JACK_DIG_OTHER_IN : <nl> cfg -> dig_in_pin = nid ; <nl> break ; <nl> }
out_unlock : <nl> * because the core might be gone away while we unlocked the mutex . */ <nl> static struct b43_wldev * b43_wireless_core_stop ( struct b43_wldev * dev ) <nl> { <nl> - struct b43_wl * wl = dev -> wl ; <nl> + struct b43_wl * wl ; <nl> struct b43_wldev * orig_dev ; <nl> u32 mask ; <nl>  <nl> + if (! dev ) <nl> + return NULL ; <nl> + wl = dev -> wl ; <nl> redo : <nl> if (! dev || b43_status ( dev ) < B43_STAT_STARTED ) <nl> return dev ;
static void balloon_up ( struct work_struct * dummy ) <nl> floor = compute_balloon_floor (); <nl>  <nl> /* Refuse to balloon below the floor , keep the 2M granularity . */ <nl> - if ( val . freeram - num_pages < floor ) { <nl> + if ( val . freeram < num_pages || val . freeram - num_pages < floor ) { <nl> num_pages = val . freeram > floor ? ( val . freeram - floor ) : 0 ; <nl> num_pages -= num_pages % PAGES_IN_2M ; <nl> }
int regmap_register_patch ( struct regmap * map , const struct reg_default * regs , <nl> int i , ret ; <nl> bool bypass ; <nl>  <nl> + if ( WARN_ONCE ( num_regs <= 0 , " invalid registers number (% d )\ n ", <nl> + num_regs )) <nl> + return 0 ; <nl> + <nl> map -> lock ( map -> lock_arg ); <nl>  <nl> bypass = map -> cache_bypass ;
int __init ip_rt_init ( void ) <nl> 0 , <nl> & rt_hash_log , <nl> & rt_hash_mask , <nl> - 0 ); <nl> + rhash_entries ? 0 : 512 * 1024 ); <nl> memset ( rt_hash_table , 0 , ( rt_hash_mask + 1 ) * sizeof ( struct rt_hash_bucket )); <nl> rt_hash_lock_init (); <nl> 
static int init_fb_chan ( struct mx3fb_data * mx3fb , struct idmac_channel * ichan ) <nl> init_completion (& mx3fbi -> flip_cmpl ); <nl> disable_irq ( ichan -> eof_irq ); <nl> dev_dbg ( mx3fb -> dev , " disabling irq % d \ n ", ichan -> eof_irq ); <nl> - ret = mx3fb_set_par ( fbi ); <nl> - if ( ret < 0 ) <nl> - goto esetpar ; <nl> - <nl> - mx3fb_blank ( FB_BLANK_UNBLANK , fbi ); <nl>  <nl> dev_info ( dev , " registered , using mode % s \ n ", fb_mode ); <nl> 
int snd_hda_input_jack_add ( struct hda_codec * codec , hda_nid_t nid , int type , <nl> err = snd_jack_new ( codec -> bus -> card , name , type , & jack -> jack ); <nl> if ( err < 0 ) <nl> return err ; <nl> + jack -> type = type ; <nl> jack -> jack -> private_data = jack ; <nl> jack -> jack -> private_free = hda_free_jack_priv ; <nl> return 0 ;
static int xfrm6_fill_dst ( struct xfrm_dst * xdst , struct net_device * dev , <nl> dev_hold ( dev ); <nl>  <nl> xdst -> u . rt6 . rt6i_idev = in6_dev_get ( dev ); <nl> - if (! xdst -> u . rt6 . rt6i_idev ) <nl> + if (! xdst -> u . rt6 . rt6i_idev ) { <nl> + dev_put ( dev ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> rt6_transfer_peer (& xdst -> u . rt6 , rt ); <nl> 
static int run_perf_stat ( int argc __used , const char ** argv ) <nl> "\ t Consider tweaking " <nl> " / proc / sys / kernel / perf_event_paranoid or running as root .", <nl> system_wide ? " system - wide " : ""); <nl> + } else if ( errno == ENOENT ) { <nl> + error ("% s event is not supported . ", event_name ( counter )); <nl> } else { <nl> error (" open_counter returned with % d (% s ). " <nl> "/ bin / dmesg may provide additional information .\ n ",
out : <nl>  <nl> static int jfs_ci_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> { <nl> - if ( nd -> flags & LOOKUP_RCU ) <nl> + if ( nd && nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl> /* <nl> * This is not negative dentry . Always valid .
static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
static int igb_set_eee ( struct net_device * netdev , <nl> ( hw -> phy . media_type != e1000_media_type_copper )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset (& eee_curr , 0 , sizeof ( struct ethtool_eee )); <nl> + <nl> ret_val = igb_get_eee ( netdev , & eee_curr ); <nl> if ( ret_val ) <nl> return ret_val ;
cputime_to_timeval ( const cputime_t cputime , struct timeval * value ) <nl> value -> tv_usec = rp . subreg . even / 4096 ; <nl> value -> tv_sec = rp . subreg . odd ; <nl> # else <nl> - value -> tv_usec = cputime % 4096000000ULL ; <nl> + value -> tv_usec = ( cputime % 4096000000ULL ) / 4096 ; <nl> value -> tv_sec = cputime / 4096000000ULL ; <nl> # endif <nl> }
void iio_disable_all_buffers ( struct iio_dev * indio_dev ) <nl> indio_dev -> currentmode = INDIO_DIRECT_MODE ; <nl> if ( indio_dev -> setup_ops -> postdisable ) <nl> indio_dev -> setup_ops -> postdisable ( indio_dev ); <nl> + <nl> + if ( indio_dev -> available_scan_masks == NULL ) <nl> + kfree ( indio_dev -> active_scan_mask ); <nl> } <nl>  <nl> int iio_update_buffers ( struct iio_dev * indio_dev ,
int __nvme_submit_sync_cmd ( struct request_queue * q , struct nvme_command * cmd , <nl> return PTR_ERR ( req ); <nl>  <nl> req -> cmd_type = REQ_TYPE_DRV_PRIV ; <nl> + req -> cmd_flags = REQ_FAILFAST_DRIVER ; <nl> req -> __data_len = 0 ; <nl> req -> __sector = ( sector_t ) - 1 ; <nl> req -> bio = req -> biotail = NULL ;
static long rtc_dev_ioctl ( struct file * file , <nl> err = ops -> ioctl ( rtc -> dev . parent , cmd , arg ); <nl> if ( err == - ENOIOCTLCMD ) <nl> err = - ENOTTY ; <nl> - } <nl> + } else <nl> + err = - ENOTTY ; <nl> break ; <nl> } <nl> 
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> /* Device config is special , because it must work on <nl> * an unconfigured device . */ <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> + if ( minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> + /* Device config not appropriate on non - board minors . */ <nl> + rc = - ENOTTY ; <nl> + goto done ; <nl> + } <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> if ( rc == 0 )
static int emc1403_probe ( struct i2c_client * client , <nl> res = sysfs_create_group (& client -> dev . kobj , & m_thermal_gr ); <nl> if ( res ) { <nl> dev_warn (& client -> dev , " create group failed \ n "); <nl> - hwmon_device_unregister ( data -> hwmon_dev ); <nl> goto thermal_error1 ; <nl> } <nl> data -> hwmon_dev = hwmon_device_register (& client -> dev );
static ssize_t wm8962_beep_set ( struct device * dev , <nl> { <nl> struct wm8962_priv * wm8962 = dev_get_drvdata ( dev ); <nl> long int time ; <nl> + int ret ; <nl>  <nl> - strict_strtol ( buf , 10 , & time ); <nl> + ret = strict_strtol ( buf , 10 , & time ); <nl> + if ( ret != 0 ) <nl> + return ret ; <nl>  <nl> input_event ( wm8962 -> beep , EV_SND , SND_TONE , time ); <nl> 
static int pcc_cpufreq_target ( struct cpufreq_policy * policy , <nl> return 0 ; <nl>  <nl> cmd_incomplete : <nl> + freqs . new = freqs . old ; <nl> + cpufreq_notify_transition ( policy , & freqs , CPUFREQ_POSTCHANGE ); <nl> iowrite16 ( 0 , & pcch_hdr -> status ); <nl> spin_unlock (& pcc_lock ); <nl> return - EINVAL ;
void afs_cache_permit ( struct afs_vnode * vnode , struct key * key , long acl_order ) <nl> if (! permits ) <nl> goto out_unlock ; <nl>  <nl> - memcpy ( permits -> permits , xpermits -> permits , <nl> - count * sizeof ( struct afs_permit )); <nl> + if ( xpermits ) <nl> + memcpy ( permits -> permits , xpermits -> permits , <nl> + count * sizeof ( struct afs_permit )); <nl>  <nl> _debug (" key % x access % x ", <nl> key_serial ( key ), vnode -> status . caller_access );
spider_net_stop ( struct net_device * netdev ) <nl> /* release chains */ <nl> spider_net_release_tx_chain ( card , 1 ); <nl>  <nl> + spider_net_free_rx_chain_contents ( card ); <nl> + <nl> spider_net_free_chain ( card , & card -> tx_chain ); <nl> spider_net_free_chain ( card , & card -> rx_chain ); <nl> 
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> req -> pages [ req -> num_pages ] = page ; <nl> req -> num_pages ++; <nl>  <nl> + offset = 0 ; <nl> num -= this_num ; <nl> total_len += this_num ; <nl> index ++;
static int genwqe_pin_mem ( struct genwqe_file * cfile , struct genwqe_mem * m ) <nl> if ( rc != 0 ) { <nl> dev_err (& pci_dev -> dev , <nl> "[% s ] genwqe_user_vmap rc =% d \ n ", __func__ , rc ); <nl> + kfree ( dma_map ); <nl> return rc ; <nl> } <nl> 
static bool hist_browser__toggle_fold ( struct hist_browser * browser ) <nl> struct callchain_list * cl = container_of ( ms , struct callchain_list , ms ); <nl> bool has_children ; <nl>  <nl> + if (! he || ! ms ) <nl> + return false ; <nl> + <nl> if ( ms == & he -> ms ) <nl> has_children = hist_entry__toggle_fold ( he ); <nl> else
int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> which_atu = 0 ; <nl> } <nl>  <nl> - if (! which_atu ) <nl> + if (! which_atu ) { <nl> + kfree ( res ); <nl> return 0 ; <nl> + } <nl>  <nl> switch ( which_atu ) { <nl> case IOP13XX_INIT_ATU_ATUX : <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> sys -> map_irq = iop13xx_pcie_map_irq ; <nl> break ; <nl> default : <nl> + kfree ( res ); <nl> return 0 ; <nl> } <nl> 
void start_tty ( struct tty_struct * tty ) <nl>  <nl> /* If we have a running line discipline it may need kicking */ <nl> tty_wakeup ( tty ); <nl> - wake_up_interruptible (& tty -> write_wait ); <nl> } <nl>  <nl> EXPORT_SYMBOL ( start_tty );
SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> if ( which > PRIO_USER || which < PRIO_PROCESS ) <nl> return - EINVAL ; <nl>  <nl> + rcu_read_lock (); <nl> read_lock (& tasklist_lock ); <nl> switch ( which ) { <nl> case PRIO_PROCESS : <nl> SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> } <nl> out_unlock : <nl> read_unlock (& tasklist_lock ); <nl> + rcu_read_unlock (); <nl>  <nl> return retval ; <nl> }
xfs_qm_freelist_destroy ( xfs_frlist_t * ql ) <nl> xfs_qm_dqdestroy ( dqp ); <nl> dqp = nextdqp ; <nl> } <nl> - /* <nl> - * Don ' t bother about unlocking . <nl> - */ <nl> + mutex_unlock (& ql -> qh_lock ); <nl> mutex_destroy (& ql -> qh_lock ); <nl>  <nl> ASSERT ( ql -> qh_nelems == 0 );
static void ttm_tt_clear_mapping ( struct ttm_tt * ttm ) <nl> pgoff_t i ; <nl> struct page ** page = ttm -> pages ; <nl>  <nl> + if ( ttm -> page_flags & TTM_PAGE_FLAG_SG ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < ttm -> num_pages ; ++ i ) { <nl> (* page )-> mapping = NULL ; <nl> (* page ++)-> index = 0 ;
xfs_ioc_trim ( <nl>  <nl> if (! capable ( CAP_SYS_ADMIN )) <nl> return - XFS_ERROR ( EPERM ); <nl> + if (! blk_queue_discard ( q )) <nl> + return - XFS_ERROR ( EOPNOTSUPP ); <nl> if ( copy_from_user (& range , urange , sizeof ( range ))) <nl> return - XFS_ERROR ( EFAULT ); <nl> 
int get_dnode_of_data ( struct dnode_of_data * dn , pgoff_t index , int mode ) <nl>  <nl> /* if inline_data is set , should not report any block indices */ <nl> if ( f2fs_has_inline_data ( dn -> inode ) && index ) { <nl> - err = - EINVAL ; <nl> + err = - ENOENT ; <nl> f2fs_put_page ( npage [ 0 ], 1 ); <nl> goto release_out ; <nl> }
int vmw_kms_update_layout_ioctl ( struct drm_device * dev , void * data , <nl> ret = copy_from_user ( rects , user_rects , rects_size ); <nl> if ( unlikely ( ret != 0 )) { <nl> DRM_ERROR (" Failed to get rects .\ n "); <nl> + ret = - EFAULT ; <nl> goto out_free ; <nl> } <nl> 
static int amd_xgbe_phy_set_mode ( struct phy_device * phydev , <nl> static enum amd_xgbe_phy_an amd_xgbe_an_tx_training ( struct phy_device * phydev , <nl> enum amd_xgbe_phy_rx * state ) <nl> { <nl> + struct amd_xgbe_phy_priv * priv = phydev -> priv ; <nl> int ad_reg , lp_reg , ret ; <nl>  <nl> * state = AMD_XGBE_RX_COMPLETE ;
brcmf_notify_sched_scan_results ( struct brcmf_if * ifp , <nl> } <nl>  <nl> set_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status ); <nl> + cfg -> escan_info . run = brcmf_run_escan ; <nl> err = brcmf_do_escan ( cfg , wiphy , ifp , request ); <nl> if ( err ) { <nl> clear_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status );
int f2fs_getxattr ( struct inode * inode , int name_index , const char * name , <nl> if ( name == NULL ) <nl> return - EINVAL ; <nl> name_len = strlen ( name ); <nl> + if ( name_len > F2FS_NAME_LEN ) <nl> + return - ERANGE ; <nl>  <nl> base_addr = read_all_xattrs ( inode , NULL ); <nl> if (! base_addr )
static int ocfs2_initialize_super ( struct super_block * sb , <nl> cbits = le32_to_cpu ( di -> id2 . i_super . s_clustersize_bits ); <nl> bbits = le32_to_cpu ( di -> id2 . i_super . s_blocksize_bits ); <nl> sb -> s_maxbytes = ocfs2_max_file_offset ( bbits , cbits ); <nl> + memcpy ( sb -> s_uuid , di -> id2 . i_super . s_uuid , <nl> + sizeof ( di -> id2 . i_super . s_uuid )); <nl>  <nl> osb -> osb_dx_mask = ( 1 << ( cbits - bbits )) - 1 ; <nl> 
static void mv_chan_set_mode ( struct mv_xor_chan * chan , <nl> config &= ~ 0x7 ; <nl> config |= op_mode ; <nl>  <nl> - if ( IS_ENABLED ( __BIG_ENDIAN )) <nl> - config |= XOR_DESCRIPTOR_SWAP ; <nl> - else <nl> - config &= ~ XOR_DESCRIPTOR_SWAP ; <nl> +# if defined ( __BIG_ENDIAN ) <nl> + config |= XOR_DESCRIPTOR_SWAP ; <nl> +# else <nl> + config &= ~ XOR_DESCRIPTOR_SWAP ; <nl> +# endif <nl>  <nl> writel_relaxed ( config , XOR_CONFIG ( chan )); <nl> chan -> current_type = type ;
static void hsmmc_command_incomplete ( struct omap_hsmmc_host * host , <nl> if ( host -> data ) { <nl> omap_hsmmc_reset_controller_fsm ( host , SRD ); <nl> omap_hsmmc_dma_cleanup ( host , err ); <nl> - } <nl> - <nl> + } else if ( host -> mrq && host -> mrq -> cmd ) <nl> + host -> mrq -> cmd -> error = err ; <nl> } <nl>  <nl> static void omap_hsmmc_do_irq ( struct omap_hsmmc_host * host , int status )
# ifndef _LINUX_BFS_FS_H <nl> # define _LINUX_BFS_FS_H <nl>  <nl> +# include < linux / types . h > <nl> + <nl> # define BFS_BSIZE_BITS 9 <nl> # define BFS_BSIZE ( 1 << BFS_BSIZE_BITS ) <nl>  <nl> # define BFS_VDIR 2L <nl> # define BFS_VREG 1L <nl>  <nl> - <nl> /* BFS inode layout on disk */ <nl> struct bfs_inode { <nl> __le16 i_ino ;
int iwl_enqueue_hcmd ( struct iwl_priv * priv , struct iwl_host_cmd * cmd ) <nl> return - EIO ; <nl> } <nl>  <nl> + if (( priv -> ucode_owner == IWL_OWNERSHIP_TM ) && <nl> + !( cmd -> flags & CMD_ON_DEMAND )) { <nl> + IWL_DEBUG_HC ( priv , " tm own the uCode , no regular hcmd send \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> copy_size = sizeof ( out_cmd -> hdr ); <nl> cmd_size = sizeof ( out_cmd -> hdr ); <nl> 
static void ext4_orphan_cleanup ( struct super_block * sb , <nl> jbd_debug ( 2 , " truncating inode % lu to % lld bytes \ n ", <nl> inode -> i_ino , inode -> i_size ); <nl> mutex_lock (& inode -> i_mutex ); <nl> + truncate_inode_pages ( inode -> i_mapping , inode -> i_size ); <nl> ext4_truncate ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> nr_truncates ++;
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static void btrfs_qgroup_rescan_worker ( struct btrfs_work * work ) <nl> out : <nl> kfree ( scratch_leaf ); <nl> ulist_free ( qgroups ); <nl> + ulist_free ( tmp ); <nl> btrfs_free_path ( path ); <nl>  <nl> mutex_lock (& fs_info -> qgroup_rescan_lock );
static int ehci_mxc_drv_remove ( struct platform_device * pdev ) <nl> if ( pdata && pdata -> exit ) <nl> pdata -> exit ( pdev ); <nl>  <nl> - if ( pdata -> otg ) <nl> + if ( pdata && pdata -> otg ) <nl> usb_phy_shutdown ( pdata -> otg ); <nl>  <nl> clk_disable_unprepare ( priv -> usbclk );
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> if ( IS_VALLEYVIEW ( dev )) <nl> bios_reserved = 1024 * 1024 ; /* top 1M on VLV / BYT */ <nl>  <nl> + if ( WARN_ON ( bios_reserved > dev_priv -> gtt . stolen_size )) <nl> + return 0 ; <nl> + <nl> /* Basic memrange allocator for stolen space */ <nl> drm_mm_init (& dev_priv -> mm . stolen , 0 , dev_priv -> gtt . stolen_size - <nl> bios_reserved );
static int dwc3_ep0_set_config ( struct dwc3 * dwc , struct usb_ctrlrequest * ctrl ) <nl> case DWC3_ADDRESS_STATE : <nl> ret = dwc3_ep0_delegate_req ( dwc , ctrl ); <nl> /* if the cfg matches and the cfg is non zero */ <nl> - if (! ret && cfg ) <nl> + if ( cfg && (! ret || ( ret == USB_GADGET_DELAYED_STATUS ))) <nl> dwc -> dev_state = DWC3_CONFIGURED_STATE ; <nl> break ; <nl> 
int mthca_init_db_tab ( struct mthca_dev * dev ) <nl>  <nl> init_MUTEX (& dev -> db_tab -> mutex ); <nl>  <nl> - dev -> db_tab -> npages = dev -> uar_table . uarc_size / PAGE_SIZE ; <nl> + dev -> db_tab -> npages = dev -> uar_table . uarc_size / 4096 ; <nl> dev -> db_tab -> max_group1 = 0 ; <nl> dev -> db_tab -> min_group2 = dev -> db_tab -> npages - 1 ; <nl> 
cfq_should_preempt ( struct cfq_data * cfqd , struct cfq_queue * new_cfqq , <nl> if ( cfq_class_idle ( cfqq )) <nl> return true ; <nl>  <nl> + /* <nl> + * Don ' t allow a non - RT request to preempt an ongoing RT cfqq timeslice . <nl> + */ <nl> + if ( cfq_class_rt ( cfqq ) && ! cfq_class_rt ( new_cfqq )) <nl> + return false ; <nl> + <nl> /* <nl> * if the new request is sync , but the currently running queue is <nl> * not , let the sync request have priority .
static int __access_remote_vm ( struct task_struct * tsk , struct mm_struct * mm , <nl> */ <nl> # ifdef CONFIG_HAVE_IOREMAP_PROT <nl> vma = find_vma ( mm , addr ); <nl> - if (! vma ) <nl> + if (! vma || vma -> vm_start > addr ) <nl> break ; <nl> if ( vma -> vm_ops && vma -> vm_ops -> access ) <nl> ret = vma -> vm_ops -> access ( vma , addr , buf ,
static int ide_diag_taskfile ( ide_drive_t * drive , ide_task_t * args , unsigned long <nl> struct request rq ; <nl>  <nl> memset (& rq , 0 , sizeof ( rq )); <nl> + rq . ref_count = 1 ; <nl> rq . cmd_type = REQ_TYPE_ATA_TASKFILE ; <nl> rq . buffer = buf ; <nl> 
static int mcp230xx_probe ( struct i2c_client * client , <nl> pdata = devm_kzalloc (& client -> dev , <nl> sizeof ( struct mcp23s08_platform_data ), <nl> GFP_KERNEL ); <nl> + if (! pdata ) <nl> + return - ENOMEM ; <nl> pdata -> base = - 1 ; <nl> } <nl> }
int __init mem_reserve ( unsigned long start , unsigned long end , int must_exist ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( it && start - it -> start < bank_sz ) { <nl> + if ( it && start - it -> start <= bank_sz ) { <nl> if ( start == it -> start ) { <nl> if ( end - it -> start < bank_sz ) { <nl> it -> start = end ;
void laptop_mode_timer_fn ( unsigned long data ) <nl> if (! bdi_has_dirty_io (& q -> backing_dev_info )) <nl> return ; <nl>  <nl> + rcu_read_lock (); <nl> bdi_for_each_wb ( wb , & q -> backing_dev_info , & iter , 0 ) <nl> if ( wb_has_dirty_io ( wb )) <nl> wb_start_writeback ( wb , nr_pages , true , <nl> WB_REASON_LAPTOP_TIMER ); <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> /*
static acpi_status intel_menlow_register_sensor ( acpi_handle handle , u32 lvl , <nl> return AE_ERROR ; <nl> } <nl>  <nl> + return AE_OK ; <nl> + <nl> aux1_not_found : <nl> if ( status == AE_NOT_FOUND ) <nl> return AE_OK ;
static struct usb_device_id usb_ids [] = { <nl> { USB_DEVICE ( 0x0411 , 0x00da ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x2019 , 0x5303 ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x129b , 0x1667 ), . driver_info = DEVICE_ZD1211B }, <nl> + { USB_DEVICE ( 0x0cde , 0x001a ), . driver_info = DEVICE_ZD1211B }, <nl> /* " Driverless " devices that need ejecting */ <nl> { USB_DEVICE ( 0x0ace , 0x2011 ), . driver_info = DEVICE_INSTALLER }, <nl> { USB_DEVICE ( 0x0ace , 0x20ff ), . driver_info = DEVICE_INSTALLER },
static void ppp_cp_parse_cr ( struct net_device * dev , u16 pid , u8 id , <nl> for ( opt = data ; len ; len -= opt [ 1 ], opt += opt [ 1 ]) { <nl> if ( len < 2 || len < opt [ 1 ]) { <nl> dev -> stats . rx_errors ++; <nl> + kfree ( out ); <nl> return ; /* bad packet , drop silently */ <nl> } <nl> 
 <nl> # define BUILD_IRQ ( nr ) \ <nl> asmlinkage void IRQ_NAME ( nr ); \ <nl> - asm ("\ n . p2align \ n " \ <nl> + asm ("\ n . text \ n . p2align \ n " \ <nl> " IRQ " # nr " _interrupt :\ n \ t " \ <nl> " push $~(" # nr ") ; " \ <nl> " jmp common_interrupt ");
asmlinkage long compat_sys_nfsservctl ( int cmd , struct compat_nfsctl_arg __user * <nl>  <nl> default : <nl> err = - EINVAL ; <nl> - goto done ; <nl> + break ; <nl> } <nl>  <nl> + if ( err ) <nl> + goto done ; <nl> + <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> /* The __user pointer casts are valid because of the set_fs () */
static int __devinit toshiba_acpi_setup_backlight ( struct toshiba_acpi_dev * dev ) <nl> ret = get_tr_backlight_status ( dev , & enabled ); <nl> dev -> tr_backlight_supported = ! ret ; <nl>  <nl> + memset (& props , 0 , sizeof ( props )); <nl> props . type = BACKLIGHT_PLATFORM ; <nl> props . max_brightness = HCI_LCD_BRIGHTNESS_LEVELS - 1 ; <nl> - memset (& props , 0 , sizeof ( props )); <nl>  <nl> /* adding an extra level and having 0 change to transflective mode */ <nl> if ( dev -> tr_backlight_supported )
static int __init byt_gpio_init ( void ) <nl> { <nl> return platform_driver_register (& byt_gpio_driver ); <nl> } <nl> - <nl> subsys_initcall ( byt_gpio_init ); <nl> + <nl> + static void __exit byt_gpio_exit ( void ) <nl> +{ <nl> + platform_driver_unregister (& byt_gpio_driver ); <nl> +} <nl> + module_exit ( byt_gpio_exit );
struct map * machine__new_module ( struct machine * machine , u64 start , <nl> if ( kmod_path__parse_name (& m , filename )) <nl> return NULL ; <nl>  <nl> + map = map_groups__find_by_name (& machine -> kmaps , MAP__FUNCTION , <nl> + m . name ); <nl> + if ( map ) <nl> + goto out ; <nl> + <nl> dso = machine__module_dso ( machine , & m , filename ); <nl> if ( dso == NULL ) <nl> goto out ;
static int visornic_probe ( struct visor_device * dev ) <nl> goto cleanup_netdev ; <nl> } <nl>  <nl> - devdata -> rcvbuf = kzalloc ( sizeof ( struct sk_buff *) * <nl> - devdata -> num_rcv_bufs , GFP_KERNEL ); <nl> + devdata -> rcvbuf = kcalloc ( devdata -> num_rcv_bufs , <nl> + sizeof ( struct sk_buff *), GFP_KERNEL ); <nl> if (! devdata -> rcvbuf ) { <nl> err = - ENOMEM ; <nl> goto cleanup_rcvbuf ;
static int ieee80211_prep_connection ( struct ieee80211_sub_if_data * sdata , <nl> chanctx_conf = rcu_dereference ( sdata -> vif . chanctx_conf ); <nl> if ( WARN_ON (! chanctx_conf )) { <nl> rcu_read_unlock (); <nl> + sta_info_free ( local , new_sta ); <nl> return - EINVAL ; <nl> } <nl> rate_flags = ieee80211_chandef_rate_flags (& chanctx_conf -> def );
nouveau_bo_move_flips ( struct ttm_buffer_object * bo , bool evict , bool intr , <nl> if ( ret ) <nl> goto out ; <nl>  <nl> - ret = nouveau_bo_move_m2mf ( bo , true , intr , no_wait , new_mem ); <nl> + ret = nouveau_bo_move_m2mf ( bo , evict , intr , no_wait , new_mem ); <nl> if ( ret ) <nl> goto out ; <nl> 
bool f2fs_may_inline ( struct inode * inode ) <nl> if ( f2fs_is_atomic_file ( inode )) <nl> return false ; <nl>  <nl> - if (! S_ISREG ( inode -> i_mode )) <nl> + if (! S_ISREG ( inode -> i_mode ) && ! S_ISLNK ( inode -> i_mode )) <nl> return false ; <nl>  <nl> if ( i_size_read ( inode ) > MAX_INLINE_DATA )
static int ab8500_fg_get_ext_psy_data ( struct device * dev , void * data ) <nl> case POWER_SUPPLY_PROP_TECHNOLOGY : <nl> switch ( ext -> type ) { <nl> case POWER_SUPPLY_TYPE_BATTERY : <nl> - if (! di -> flags . batt_id_received ) { <nl> + if (! di -> flags . batt_id_received && <nl> + di -> bm -> batt_id != BATTERY_UNKNOWN ) { <nl> const struct abx500_battery_type * b ; <nl>  <nl> b = &( di -> bm -> bat_type [ di -> bm -> batt_id ]);
static int at91_rtc_resume ( struct device * dev ) <nl>  <nl> static SIMPLE_DEV_PM_OPS ( at91_rtc_pm_ops , at91_rtc_suspend , at91_rtc_resume ); <nl>  <nl> +# ifdef CONFIG_OF <nl> static const struct of_device_id at91_rtc_dt_ids [] = { <nl> { . compatible = " atmel , at91rm9200 - rtc " }, <nl> { /* sentinel */ } <nl> }; <nl> MODULE_DEVICE_TABLE ( of , at91_rtc_dt_ids ); <nl> +# endif <nl>  <nl> static struct platform_driver at91_rtc_driver = { <nl> . remove = __exit_p ( at91_rtc_remove ),
int sdma_init ( struct hfi1_devdata * dd , u8 port ) <nl>  <nl> sde -> progress_check_head = 0 ; <nl>  <nl> - init_timer (& sde -> err_progress_check_timer ); <nl> - sde -> err_progress_check_timer . function = <nl> - sdma_err_progress_check ; <nl> - sde -> err_progress_check_timer . data = ( unsigned long ) sde ; <nl> + setup_timer (& sde -> err_progress_check_timer , <nl> + sdma_err_progress_check , ( unsigned long ) sde ); <nl>  <nl> sde -> descq = dma_zalloc_coherent ( <nl> & dd -> pcidev -> dev ,
static int ibmvfc_get_err_result ( struct ibmvfc_cmd * vfc_cmd ) <nl> int fc_rsp_len = rsp -> fcp_rsp_len ; <nl>  <nl> if (( rsp -> flags & FCP_RSP_LEN_VALID ) && <nl> - ((! fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> + (( fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> rsp -> data . info . rsp_code )) <nl> return DID_ERROR << 16 ; <nl> 
static int mwl8k_tx_wait_empty ( struct ieee80211_hw * hw ) <nl>  <nl> rc = - ETIMEDOUT ; <nl> } <nl> + priv -> tx_wait = NULL ; <nl> spin_unlock_bh (& priv -> tx_lock ); <nl>  <nl> return rc ;
static int snd_card_asihpi_trigger ( struct snd_pcm_substream * substream , <nl> VPRINTK1 ( KERN_INFO " start \ n "); <nl> /* start the master stream */ <nl> snd_card_asihpi_pcm_timer_start ( substream ); <nl> - if ( substream -> stream == SNDRV_PCM_STREAM_CAPTURE ) <nl> + if (( substream -> stream == SNDRV_PCM_STREAM_CAPTURE ) || <nl> + ! card -> support_mmap ) <nl> hpi_handle_error ( hpi_stream_start ( dpcm -> h_stream )); <nl> break ; <nl> 
static int patch_stac922x ( struct hda_codec * codec ) <nl> */ <nl> printk ( KERN_INFO " hda_codec : STAC922x , Apple subsys_id =% x \ n ", codec -> subsystem_id ); <nl> switch ( codec -> subsystem_id ) { <nl> + case 0x106b0a00 : /* MacBook First generatoin */ <nl> + spec -> board_config = STAC_MACBOOK ; <nl> + break ; <nl> case 0x106b0200 : /* MacBook Pro first generation */ <nl> spec -> board_config = STAC_MACBOOK_PRO_V1 ; <nl> break ;
intel_crt_load_detect ( struct drm_crtc * crtc , struct intel_encoder * intel_encoder <nl> if ( IS_I9XX ( dev )) { <nl> uint32_t pipeconf = I915_READ ( pipeconf_reg ); <nl> I915_WRITE ( pipeconf_reg , pipeconf | PIPECONF_FORCE_BORDER ); <nl> + POSTING_READ ( pipeconf_reg ); <nl> /* Wait for next Vblank to substitue <nl> * border color for Color info */ <nl> intel_wait_for_vblank ( dev , pipe );
static int of_mpc8xxx_spi_get_chipselects ( struct device * dev ) <nl> gpio = of_get_gpio_flags ( np , i , & flags ); <nl> if (! gpio_is_valid ( gpio )) { <nl> dev_err ( dev , " invalid gpio #% d : % d \ n ", i , gpio ); <nl> + ret = gpio ; <nl> goto err_loop ; <nl> } <nl> 
static int pcrypt_aead_init_tfm ( struct crypto_tfm * tfm ) <nl> return PTR_ERR ( cipher ); <nl>  <nl> ctx -> child = cipher ; <nl> - tfm -> crt_aead . reqsize = sizeof ( struct pcrypt_request ) <nl> - + sizeof ( struct aead_givcrypt_request ) <nl> - + crypto_aead_reqsize ( cipher ); <nl> + crypto_aead_set_reqsize ( __crypto_aead_cast ( tfm ), <nl> + sizeof ( struct pcrypt_request ) + <nl> + sizeof ( struct aead_givcrypt_request ) + <nl> + crypto_aead_reqsize ( cipher )); <nl>  <nl> return 0 ; <nl> }
static unsigned int cpg_div6_clock_calc_div ( unsigned long rate , <nl> { <nl> unsigned int div ; <nl>  <nl> + if (! rate ) <nl> + rate = 1 ; <nl> + <nl> div = DIV_ROUND_CLOSEST ( parent_rate , rate ); <nl> return clamp_t ( unsigned int , div , 1 , 64 ); <nl> }
static void reset_connection ( struct ceph_connection * con ) <nl> con -> out_msg = NULL ; <nl> } <nl> con -> in_seq = 0 ; <nl> + con -> in_seq_acked = 0 ; <nl> } <nl>  <nl> /*
static int __devinit fs_enet_mdio_probe ( struct of_device * ofdev , <nl>  <nl> ret = of_address_to_resource ( ofdev -> node , 0 , & res ); <nl> if ( ret ) <nl> - return ret ; <nl> + goto out_res ; <nl>  <nl> snprintf ( new_bus -> id , MII_BUS_ID_SIZE , "% x ", res . start ); <nl>  <nl> out_free_irqs : <nl> kfree ( new_bus -> irq ); <nl> out_unmap_regs : <nl> iounmap ( fec -> fecp ); <nl> + out_res : <nl> out_fec : <nl> kfree ( fec ); <nl> out_mii :
hub_port_init ( struct usb_hub * hub , struct usb_device * udev , int port1 , <nl>  <nl> did_new_scheme = true ; <nl> retval = hub_enable_device ( udev ); <nl> - if ( retval < 0 ) <nl> + if ( retval < 0 ) { <nl> + dev_err (& udev -> dev , <nl> + " hub failed to enable device , error % d \ n ", <nl> + retval ); <nl> goto fail ; <nl> + } <nl>  <nl> # define GET_DESCRIPTOR_BUFSIZE 64 <nl> buf = kmalloc ( GET_DESCRIPTOR_BUFSIZE , GFP_NOIO );
static void __exit powernv_cpufreq_exit ( void ) <nl> unregister_reboot_notifier (& powernv_cpufreq_reboot_nb ); <nl> opal_message_notifier_unregister ( OPAL_MSG_OCC , <nl> & powernv_cpufreq_opal_nb ); <nl> + kfree ( chips ); <nl> cpufreq_unregister_driver (& powernv_cpufreq_driver ); <nl> } <nl> module_exit ( powernv_cpufreq_exit );
static void bmc150_accel_unregister_triggers ( struct bmc150_accel_data * data , <nl> { <nl> int i ; <nl>  <nl> - for ( i = from ; i >= 0 ; i ++) { <nl> + for ( i = from ; i >= 0 ; i --) { <nl> if ( data -> triggers [ i ]. indio_trig ) { <nl> iio_trigger_unregister ( data -> triggers [ i ]. indio_trig ); <nl> data -> triggers [ i ]. indio_trig = NULL ;
xlog_recover_do_reg_buffer ( <nl> stale_buf = 1 ; <nl> break ; <nl> } <nl> - if ( be16_to_cpu ( dip -> di_core . di_mode )) <nl> + if ( dip -> di_core . di_mode ) <nl> mode_count ++; <nl> - if ( be16_to_cpu ( dip -> di_core . di_gen )) <nl> + if ( dip -> di_core . di_gen ) <nl> gen_count ++; <nl> } <nl> 
int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
static void unlock_two_stripes ( struct stripe_head * sh1 , struct stripe_head * sh2 ) <nl> /* Only freshly new full stripe normal write stripe can be added to a batch list */ <nl> static bool stripe_can_batch ( struct stripe_head * sh ) <nl> { <nl> + struct r5conf * conf = sh -> raid_conf ; <nl> + <nl> + if ( conf -> log ) <nl> + return false ; <nl> return test_bit ( STRIPE_BATCH_READY , & sh -> state ) && <nl> ! test_bit ( STRIPE_BITMAP_PENDING , & sh -> state ) && <nl> is_full_stripe_write ( sh );
void fuse_put_request ( struct fuse_conn * fc , struct fuse_req * req ) <nl> spin_unlock (& fc -> lock ); <nl> } <nl>  <nl> - if ( req -> waiting ) <nl> + if ( req -> waiting ) { <nl> atomic_dec (& fc -> num_waiting ); <nl> + req -> waiting = 0 ; <nl> + } <nl>  <nl> if ( req -> stolen_file ) <nl> put_reserved_req ( fc , req );
static void __init of_omap2_apll_setup ( struct device_node * node ) <nl> const char * parent_name ; <nl> u32 val ; <nl>  <nl> - ad = kzalloc ( sizeof (* clk_hw ), GFP_KERNEL ); <nl> + ad = kzalloc ( sizeof (* ad ), GFP_KERNEL ); <nl> clk_hw = kzalloc ( sizeof (* clk_hw ), GFP_KERNEL ); <nl> init = kzalloc ( sizeof (* init ), GFP_KERNEL ); <nl> 
static int __devinit ntc_thermistor_probe ( struct platform_device * pdev ) <nl>  <nl> data -> dev = & pdev -> dev ; <nl> data -> pdata = pdata ; <nl> - strncpy ( data -> name , pdev -> id_entry -> name , PLATFORM_NAME_SIZE ); <nl> + strlcpy ( data -> name , pdev -> id_entry -> name , sizeof ( data -> name )); <nl>  <nl> switch ( pdev -> id_entry -> driver_data ) { <nl> case TYPE_NCPXXWB473 :
u32 ath_calcrxfilter ( struct ath_softc * sc ) <nl> rfilt |= ATH9K_RX_FILTER_COMP_BAR ; <nl>  <nl> if ( sc -> nvifs > 1 || ( sc -> rx . rxfilter & FIF_OTHER_BSS )) { <nl> - /* The following may also be needed for other older chips */ <nl> - if ( sc -> sc_ah -> hw_version . macVersion == AR_SREV_VERSION_9160 ) <nl> + /* This is needed for older chips */ <nl> + if ( sc -> sc_ah -> hw_version . macVersion <= AR_SREV_VERSION_9160 ) <nl> rfilt |= ATH9K_RX_FILTER_PROM ; <nl> rfilt |= ATH9K_RX_FILTER_MCAST_BCAST_ALL ; <nl> }
static int ieee80211_stop ( struct net_device * dev ) <nl> case IEEE80211_IF_TYPE_STA : <nl> case IEEE80211_IF_TYPE_IBSS : <nl> sdata -> u . sta . state = IEEE80211_DISABLED ; <nl> + memset ( sdata -> u . sta . bssid , 0 , ETH_ALEN ); <nl> del_timer_sync (& sdata -> u . sta . timer ); <nl> /* <nl> * When we get here , the interface is marked down .
event_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_preds ( call ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl>  <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_subsystem_preds ( system ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl> 
static struct dmi_system_id acer_quirks [] = { <nl> }, <nl> . driver_data = & quirk_lenovo_ideapad_s205 , <nl> }, <nl> + { <nl> + . callback = dmi_matched , <nl> + . ident = " Lenovo Ideapad S205 - 1038DPG ", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " LENOVO "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME , " 1038DPG "), <nl> + }, <nl> + . driver_data = & quirk_lenovo_ideapad_s205 , <nl> + }, <nl> {} <nl> }; <nl> 
static irqreturn_t s6000_pcm_irq ( int irq , void * data ) <nl> substream -> runtime && <nl> snd_pcm_running ( substream )) { <nl> dev_dbg ( pcm -> dev , " xrun \ n "); <nl> + snd_pcm_stream_lock ( substream ); <nl> snd_pcm_stop ( substream , SNDRV_PCM_STATE_XRUN ); <nl> + snd_pcm_stream_unlock ( substream ); <nl> ret = IRQ_HANDLED ; <nl> } <nl> 
acpi_ds_build_internal_package_obj ( struct acpi_walk_state * walk_state , <nl> arg = arg -> common . next ; <nl> } <nl>  <nl> - ACPI_ERROR (( AE_INFO , <nl> + ACPI_WARNING (( AE_INFO , <nl> " Package List length (% X ) larger than NumElements count (% X ), truncated \ n ", <nl> i , element_count )); <nl> } else if ( i < element_count ) {
static int process_pool_aeb ( struct ubi_device * ubi , struct ubi_attach_info * ai , <nl> av = tmp_av ; <nl> else { <nl> ubi_err (" orphaned volume in fastmap pool !"); <nl> + kmem_cache_free ( ai -> aeb_slab_cache , new_aeb ); <nl> return UBI_BAD_FASTMAP ; <nl> } <nl> 
static struct xfrm_policy * xfrm_compile_policy ( u16 family , int opt , <nl> if ( nr > XFRM_MAX_DEPTH ) <nl> return NULL ; <nl>  <nl> + if ( p -> dir > XFRM_POLICY_OUT ) <nl> + return NULL ; <nl> + <nl> xp = xfrm_policy_alloc ( GFP_KERNEL ); <nl> if ( xp == NULL ) { <nl> * dir = - ENOBUFS ;
static int special_clk_ctl_put ( struct snd_kcontrol * kctl , <nl> struct special_params * params = bebob -> maudio_special_quirk ; <nl> int err , id ; <nl>  <nl> - mutex_lock (& bebob -> mutex ); <nl> - <nl> id = uval -> value . enumerated . item [ 0 ]; <nl> if ( id >= ARRAY_SIZE ( special_clk_labels )) <nl> return 0 ; <nl>  <nl> + mutex_lock (& bebob -> mutex ); <nl> + <nl> err = avc_maudio_set_special_clk ( bebob , id , <nl> params -> dig_in_fmt , <nl> params -> dig_out_fmt ,
static int dn_shutdown ( struct socket * sock , int how ) <nl> if ( scp -> state == DN_O ) <nl> goto out ; <nl>  <nl> - if ( how != SHUTDOWN_MASK ) <nl> + if ( how != SHUT_RDWR ) <nl> goto out ; <nl>  <nl> - sk -> sk_shutdown = how ; <nl> + sk -> sk_shutdown = SHUTDOWN_MASK ; <nl> dn_destroy_sock ( sk ); <nl> err = 0 ; <nl> 
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> int bios_reserved = 0 ; <nl>  <nl> + if ( dev_priv -> gtt . stolen_size == 0 ) <nl> + return 0 ; <nl> + <nl> dev_priv -> mm . stolen_base = i915_stolen_to_physical ( dev ); <nl> if ( dev_priv -> mm . stolen_base == 0 ) <nl> return 0 ;
int comedi_device_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> " driver '% s ' does not support attach using comedi_config \ n ", <nl> driv -> driver_name ); <nl> module_put ( driv -> module ); <nl> - ret = - ENOSYS ; <nl> + ret = - EIO ; <nl> goto out ; <nl> } <nl> dev -> driver = driv ;
static int sti_drm_platform_probe ( struct platform_device * pdev ) <nl> master = platform_device_register_resndata ( dev , <nl> DRIVER_NAME " __master ", - 1 , <nl> NULL , 0 , NULL , 0 ); <nl> - if (! master ) <nl> - return - EINVAL ; <nl> + if ( IS_ERR ( master )) <nl> + return PTR_ERR ( master ); <nl>  <nl> platform_set_drvdata ( pdev , master ); <nl> return 0 ;
static void tty_audit_buf_push ( struct task_struct * tsk , uid_t loginuid , <nl> get_task_comm ( name , tsk ); <nl> audit_log_untrustedstring ( ab , name ); <nl> audit_log_format ( ab , " data ="); <nl> - audit_log_n_untrustedstring ( ab , buf -> data , buf -> valid ); <nl> + audit_log_n_hex ( ab , buf -> data , buf -> valid ); <nl> audit_log_end ( ab ); <nl> } <nl> buf -> valid = 0 ;
static int amd8111e_rx_poll ( struct napi_struct * napi , int budget ) <nl> int rx_pkt_limit = budget ; <nl> unsigned long flags ; <nl>  <nl> + if ( rx_pkt_limit <= 0 ) <nl> + goto rx_not_empty ; <nl> + <nl> do { <nl> /* process receive packets until we use the quota */ <nl> /* If we own the next entry , it ' s a new packet . Send it up . */
void flush_thread ( void ) <nl>  <nl> void start_thread ( struct pt_regs * regs , unsigned long eip , unsigned long esp ) <nl> { <nl> + get_safe_registers ( regs -> regs . gp , regs -> regs . fp ); <nl> PT_REGS_IP ( regs ) = eip ; <nl> PT_REGS_SP ( regs ) = esp ; <nl> current -> ptrace &= ~ PT_DTRACE ;
batadv_purge_outstanding_packets ( struct batadv_priv * bat_priv , <nl> * we delete only packets belonging to the given interface <nl> */ <nl> if (( hard_iface ) && <nl> - ( forw_packet -> if_incoming != hard_iface )) <nl> + ( forw_packet -> if_incoming != hard_iface ) && <nl> + ( forw_packet -> if_outgoing != hard_iface )) <nl> continue ; <nl>  <nl> spin_unlock_bh (& bat_priv -> forw_bcast_list_lock );
static ssize_t hid_debug_events_read ( struct file * file , char __user * buffer , <nl>  <nl> if (! list -> hdev || ! list -> hdev -> debug ) { <nl> ret = - EIO ; <nl> - break ; <nl> + set_current_state ( TASK_RUNNING ); <nl> + goto out ; <nl> } <nl>  <nl> /* allow O_NONBLOCK from other threads */
static int load_segment_descriptor ( struct x86_emulate_ctxt * ctxt , <nl> seg_desc . type = 3 ; <nl> seg_desc . p = 1 ; <nl> seg_desc . s = 1 ; <nl> + if ( ctxt -> mode == X86EMUL_MODE_VM86 ) <nl> + seg_desc . dpl = 3 ; <nl> goto load ; <nl> } <nl> 
static int pcf2123_probe ( struct spi_device * spi ) <nl>  <nl> if (!( rxbuf [ 0 ] & 0x20 )) { <nl> dev_err (& spi -> dev , " chip not found \ n "); <nl> + ret = - ENODEV ; <nl> goto kfree_exit ; <nl> } <nl> 
static int musb_gadget_stop ( struct usb_gadget * g , <nl> dev_dbg ( musb -> controller , " unregistering driver % s \ n ", driver -> function ); <nl>  <nl> musb -> is_active = 0 ; <nl> + musb -> gadget_driver = NULL ; <nl> musb_platform_try_idle ( musb , 0 ); <nl> spin_unlock_irqrestore (& musb -> lock , flags ); <nl> 
struct clk * icst_clk_register ( struct device * dev , <nl>  <nl> pclone = kmemdup ( desc -> params , sizeof (* pclone ), GFP_KERNEL ); <nl> if (! pclone ) { <nl> + kfree ( icst ); <nl> pr_err (" could not clone ICST params \ n "); <nl> return ERR_PTR (- ENOMEM ); <nl> }
static long clk_divider_round_rate ( struct clk_hw * hw , unsigned long rate , <nl> bestdiv = readl ( divider -> reg ) >> divider -> shift ; <nl> bestdiv &= div_mask ( divider -> width ); <nl> bestdiv = _get_div ( divider -> table , bestdiv , divider -> flags ); <nl> - return bestdiv ; <nl> + return DIV_ROUND_UP (* prate , bestdiv ); <nl> } <nl>  <nl> return divider_round_rate ( hw , rate , prate , divider -> table ,
EXPORT_SYMBOL_GPL ( irq_of_parse_and_map ); <nl>  <nl> void irq_dispose_mapping ( unsigned int virq ) <nl> { <nl> - struct irq_host * host = irq_map [ virq ]. host ; <nl> + struct irq_host * host ; <nl> irq_hw_number_t hwirq ; <nl> unsigned long flags ; <nl>  <nl> + if ( virq == NO_IRQ ) <nl> + return ; <nl> + <nl> + host = irq_map [ virq ]. host ; <nl> WARN_ON ( host == NULL ); <nl> if ( host == NULL ) <nl> return ;
int r100_cs_parse ( struct radeon_cs_parser * p ) <nl> int r ; <nl>  <nl> track = kzalloc ( sizeof (* track ), GFP_KERNEL ); <nl> + if (! track ) <nl> + return - ENOMEM ; <nl> r100_cs_track_clear ( p -> rdev , track ); <nl> p -> track = track ; <nl> do {
static int __init ubi_mtd_param_parse ( const char * val , struct kernel_param * kp ) <nl> char * pbuf = & buf [ 0 ]; <nl> char * tokens [ 3 ] = { NULL , NULL , NULL }; <nl>  <nl> + if (! val ) <nl> + return - EINVAL ; <nl> + <nl> if ( mtd_devs == UBI_MAX_DEVICES ) { <nl> printk (" UBI error : too many parameters , max . is % d \ n ", <nl> UBI_MAX_DEVICES );
static int mxcnd_probe ( struct platform_device * pdev ) <nl> if ( err ) <nl> return err ; <nl>  <nl> - clk_prepare_enable ( host -> clk ); <nl> + err = clk_prepare_enable ( host -> clk ); <nl> + if ( err ) <nl> + return err ; <nl> host -> clk_act = 1 ; <nl>  <nl> /*
static int mtk_spi_remove ( struct platform_device * pdev ) <nl> pm_runtime_disable (& pdev -> dev ); <nl>  <nl> mtk_spi_reset ( mdata ); <nl> - clk_disable_unprepare ( mdata -> spi_clk ); <nl> spi_master_put ( master ); <nl>  <nl> return 0 ;
int svc_create_xprt ( struct svc_serv * serv , const char * xprt_name , <nl> list_add (& newxprt -> xpt_list , & serv -> sv_permsocks ); <nl> spin_unlock_bh (& serv -> sv_lock ); <nl> newport = svc_xprt_local_port ( newxprt ); <nl> - clear_bit ( XPT_BUSY , & newxprt -> xpt_flags ); <nl> + svc_xprt_received ( newxprt ); <nl> return newport ; <nl> } <nl> err :
static int ci_udc_pullup ( struct usb_gadget * _gadget , int is_on ) <nl> { <nl> struct ci_hdrc * ci = container_of ( _gadget , struct ci_hdrc , gadget ); <nl>  <nl> + if (! ci -> vbus_active ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> if ( is_on ) <nl> hw_write ( ci , OP_USBCMD , USBCMD_RS , USBCMD_RS ); <nl> else
void reset_vma_resv_huge_pages ( struct vm_area_struct * vma ) <nl> /* Returns true if the VMA has associated reserve pages */ <nl> static int vma_has_reserves ( struct vm_area_struct * vma ) <nl> { <nl> + if ( vma -> vm_flags & VM_NORESERVE ) <nl> + return 0 ; <nl> if ( vma -> vm_flags & VM_MAYSHARE ) <nl> return 1 ; <nl> if ( is_vma_resv_set ( vma , HPAGE_RESV_OWNER ))
static int readable ( struct pcmcia_socket * s , struct resource * res , <nl> destroy_cis_cache ( s ); <nl> } <nl> s -> cis_mem . res = NULL ; <nl> - if (( ret != 0 ) || ( count == 0 )) <nl> + if (( ret != 0 ) || (* count == 0 )) <nl> return 0 ; <nl> return 1 ; <nl> }
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
void __init init_bsp_APIC ( void ) <nl> /** <nl> * setup_local_APIC - setup the local APIC <nl> */ <nl> - void __devinit setup_local_APIC ( void ) <nl> + void __cpuinit setup_local_APIC ( void ) <nl> { <nl> unsigned long oldvalue , value , maxlvt , integrated ; <nl> int i , j ;
static struct hw_breakpoint { <nl> unsigned long addr ; <nl> int len ; <nl> int type ; <nl> - struct perf_event ** pev ; <nl> + struct perf_event * __percpu * pev ; <nl> } breakinfo [ HBP_NUM ]; <nl>  <nl> static unsigned long early_dr7 ;
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
cache_type_store ( struct device * dev , struct device_attribute * attr , <nl> buffer_data [ 2 ] &= ~ 0x05 ; <nl> buffer_data [ 2 ] |= wce << 2 | rcd ; <nl> sp = buffer_data [ 0 ] & 0x80 ? 1 : 0 ; <nl> + buffer_data [ 0 ] &= ~ 0x80 ; <nl>  <nl> if ( scsi_mode_select ( sdp , 1 , sp , 8 , buffer_data , len , SD_TIMEOUT , <nl> SD_MAX_RETRIES , & data , & sshdr )) {
static int gfar_start_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> kfree_skb ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> + <nl> + /* Steal sock reference for processing TX time stamps */ <nl> + swap ( skb_new -> sk , skb -> sk ); <nl> + swap ( skb_new -> destructor , skb -> destructor ); <nl> kfree_skb ( skb ); <nl> skb = skb_new ; <nl> }
static int ad1836_register ( struct ad1836_priv * ad1836 ) <nl>  <nl> if ( ad1836_codec ) { <nl> dev_err ( codec -> dev , " Another ad1836 is registered \ n "); <nl> + kfree ( ad1836 ); <nl> return - EINVAL ; <nl> } <nl> 
static int vq_memory_access_ok ( void __user * log_base , struct vhost_memory * mem , <nl> int log_all ) <nl> { <nl> int i ; <nl> + <nl> + if (! mem ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < mem -> nregions ; ++ i ) { <nl> struct vhost_memory_region * m = mem -> regions + i ; <nl> unsigned long a = m -> userspace_addr ;
static int twl4030_kp_probe ( struct platform_device * pdev ) <nl> err3 : <nl> /* mask all events - we don ' t care about the result */ <nl> ( void ) twl4030_kpwrite_u8 ( kp , 0xff , KEYP_IMR1 ); <nl> - free_irq ( kp -> irq , NULL ); <nl> + free_irq ( kp -> irq , kp ); <nl> err2 : <nl> input_unregister_device ( input ); <nl> input = NULL ;
int __max730x_remove ( struct device * dev ) <nl> ts -> write ( dev , 0x04 , 0x00 ); <nl> gpiochip_remove (& ts -> chip ); <nl> mutex_destroy (& ts -> lock ); <nl> - kfree ( ts ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( __max730x_remove );
void led_blink_set ( struct led_classdev * led_cdev , <nl> unsigned long * delay_on , <nl> unsigned long * delay_off ) <nl> { <nl> + del_timer_sync (& led_cdev -> blink_timer ); <nl> + <nl> if ( led_cdev -> blink_set && <nl> ! led_cdev -> blink_set ( led_cdev , delay_on , delay_off )) { <nl> led_cdev -> blink_delay_on = * delay_on ;
static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
static void xlr_make_tx_desc ( struct nlm_fmn_msg * msg , unsigned long addr , <nl> (( u64 ) fr_stn_id << 54 ) | /* Free back id */ <nl> ( u64 ) 0 << 40 | /* Set len to 0 */ <nl> (( u64 ) physkb & 0xffffffff )); /* 32bit address */ <nl> - msg -> msg2 = msg -> msg3 = 0 ; <nl> + msg -> msg2 = 0 ; <nl> + msg -> msg3 = 0 ; <nl> } <nl>  <nl> static void __maybe_unused xlr_wakeup_queue ( unsigned long dev )
static irqreturn_t edt_ft5x06_ts_isr ( int irq , void * dev_id ) <nl> if ( type == TOUCH_EVENT_RESERVED ) <nl> continue ; <nl>  <nl> + /* ignore TOUCH_DOWN events , might have bogus coordinates */ <nl> + if ( type == TOUCH_EVENT_DOWN ) <nl> + continue ; <nl> + <nl> x = (( buf [ 0 ] << 8 ) | buf [ 1 ]) & 0x0fff ; <nl> y = (( buf [ 2 ] << 8 ) | buf [ 3 ]) & 0x0fff ; <nl> id = ( buf [ 2 ] >> 4 ) & 0x0f ;
static void ni_write_caldac ( struct comedi_device * dev , int addr , int val ) <nl> addr -= caldacs [ type ]. n_chans ; <nl> } <nl>  <nl> + /* bits will be 0 if there is no caldac for the given addr */ <nl> + if ( bits == 0 ) <nl> + return ; <nl> + <nl> for ( bit = 1 << ( bits - 1 ); bit ; bit >>= 1 ) { <nl> ni_writeb ( dev , (( bit & bitstring ) ? 0x02 : 0 ), Serial_Command ); <nl> udelay ( 1 );
void tune_serdes ( struct hfi1_pportdata * ppd ) <nl> ppd -> driver_link_ready = 0 ; <nl> ppd -> offline_disabled_reason = HFI1_ODR_MASK ( OPA_LINKDOWN_REASON_NONE ); <nl>  <nl> - if ( loopback == LOOPBACK_SERDES || loopback == LOOPBACK_LCB || <nl> + /* Skip the tuning for testing ( loopback != none ) and simulations */ <nl> + if ( loopback != LOOPBACK_NONE || <nl> ppd -> dd -> icode == ICODE_FUNCTIONAL_SIMULATOR || <nl> ! dd -> pcfg_cache . cache_valid ) { <nl> ppd -> driver_link_ready = 1 ;
static int rtas_excl_open ( struct inode * inode , struct file * file ) <nl>  <nl> /* Enforce exclusive open with use count of PDE */ <nl> spin_lock (& flash_file_open_lock ); <nl> - if ( atomic_read (& dp -> count ) > 1 ) { <nl> + if ( atomic_read (& dp -> count ) > 2 ) { <nl> spin_unlock (& flash_file_open_lock ); <nl> return - EBUSY ; <nl> }
static inline void of_pci_check_probe_only ( void ) { } <nl> int of_pci_get_host_bridge_resources ( struct device_node * dev , <nl> unsigned char busno , unsigned char bus_max , <nl> struct list_head * resources , resource_size_t * io_base ); <nl> +# else <nl> + static inline int of_pci_get_host_bridge_resources ( struct device_node * dev , <nl> + unsigned char busno , unsigned char bus_max , <nl> + struct list_head * resources , resource_size_t * io_base ) <nl> +{ <nl> + return - EINVAL ; <nl> +} <nl> # endif <nl>  <nl> # if defined ( CONFIG_OF ) && defined ( CONFIG_PCI_MSI )
static int initializing = 1 ; <nl>  <nl> static int pmac_late_init ( void ) <nl> { <nl> + if (! machine_is ( powermac )) <nl> + return - ENODEV ; <nl> + <nl> initializing = 0 ; <nl> /* this is udbg ( which is __init ) and we can later use it during <nl> * cpu hotplug ( in smp_core99_kick_cpu ) */
static void ath6kl_recovery_work ( struct work_struct * work ) <nl>  <nl> ar -> fw_recovery . err_reason = 0 ; <nl>  <nl> - if ( ar -> fw_recovery . enable ) <nl> + if ( ar -> fw_recovery . hb_poll ) <nl> mod_timer (& ar -> fw_recovery . hb_timer , jiffies + <nl> msecs_to_jiffies ( ar -> fw_recovery . hb_poll )); <nl> }
MODULE_LICENSE (" GPL v2 "); <nl>  <nl> static int debug ; <nl> module_param ( debug , int , 0644 ); <nl> - MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on \ n "); <nl> + MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on "); <nl>  <nl> /* # define MPX_DEBUG */ <nl> 
wl12xx_scan_sched_scan_ssid_list ( struct wl1271 * wl , <nl> * so they ' re used in probe requests . <nl> */ <nl> for ( i = 0 ; i < req -> n_ssids ; i ++) { <nl> + if (! req -> ssids [ i ]. ssid_len ) <nl> + continue ; <nl> + <nl> for ( j = 0 ; j < cmd -> n_ssids ; j ++) <nl> if (! memcmp ( req -> ssids [ i ]. ssid , <nl> cmd -> ssids [ j ]. ssid ,
PyMODINIT_FUNC initperf ( void ) <nl> pyrf_cpu_map__setup_types () < 0 ) <nl> return ; <nl>  <nl> + page_size = sysconf ( _SC_PAGE_SIZE ); <nl> + <nl> Py_INCREF (& pyrf_evlist__type ); <nl> PyModule_AddObject ( module , " evlist ", ( PyObject *)& pyrf_evlist__type ); <nl> 
asmlinkage long sys_uselib ( const char __user * library ) <nl> if ( error ) <nl> goto out ; <nl>  <nl> + error = - EACCES ; <nl> + if ( nd . mnt -> mnt_flags & MNT_NOEXEC ) <nl> + goto exit ; <nl> error = - EINVAL ; <nl> if (! S_ISREG ( nd . dentry -> d_inode -> i_mode )) <nl> goto exit ;
static void wlcore_op_stop_locked ( struct wl1271 * wl ) <nl>  <nl> /* <nl> * FW channels must be re - calibrated after recovery , <nl> - * clear the last Reg - Domain channel configuration . <nl> + * save current Reg - Domain channel configuration and clear it . <nl> */ <nl> + memcpy ( wl -> reg_ch_conf_pending , wl -> reg_ch_conf_last , <nl> + sizeof ( wl -> reg_ch_conf_pending )); <nl> memset ( wl -> reg_ch_conf_last , 0 , sizeof ( wl -> reg_ch_conf_last )); <nl> } <nl> 
nouveau_pm_profile_set ( struct drm_device * dev , const char * profile ) <nl> return - EPERM ; <nl>  <nl> strncpy ( string , profile , sizeof ( string )); <nl> + string [ sizeof ( string ) - 1 ] = 0 ; <nl> if (( ptr = strchr ( string , '\ n '))) <nl> * ptr = '\ 0 '; <nl> 
match ( const struct sk_buff * skb , <nl> return 0 ; <nl> } <nl>  <nl> + if ( mh -> ip6mh_proto != IPPROTO_NONE ) { <nl> + duprintf (" Dropping invalid MH Payload Proto : % u \ n ", <nl> + mh -> ip6mh_proto ); <nl> + * hotdrop = 1 ; <nl> + return 0 ; <nl> + } <nl> + <nl> return type_match ( mhinfo -> types [ 0 ], mhinfo -> types [ 1 ], mh -> ip6mh_type , <nl> !!( mhinfo -> invflags & IP6T_MH_INV_TYPE )); <nl> }
struct hda_bus { <nl>  <nl> /* codec linked list */ <nl> struct list_head codec_list ; <nl> - struct hda_codec * caddr_tbl [ HDA_MAX_CODEC_ADDRESS ]; /* caddr -> codec */ <nl> + struct hda_codec * caddr_tbl [ HDA_MAX_CODEC_ADDRESS + 1 ]; /* caddr -> codec */ <nl>  <nl> struct semaphore cmd_mutex ; <nl> 
static inline void ata_tf_init ( struct ata_device * dev , struct ata_taskfile * tf ) <nl>  <nl> static inline void ata_qc_reinit ( struct ata_queued_cmd * qc ) <nl> { <nl> + qc -> dma_dir = DMA_NONE ; <nl> qc -> __sg = NULL ; <nl> qc -> flags = 0 ; <nl> qc -> cursect = qc -> cursg = qc -> cursg_ofs = 0 ;
static void intel_pin_eld_notify ( void * audio_ptr , int port ) <nl> */ <nl> if ( snd_power_get_state ( codec -> card ) != SNDRV_CTL_POWER_D0 ) <nl> return ; <nl> + /* ditto during suspend / resume process itself */ <nl> + if ( atomic_read (&( codec )-> core . in_pm )) <nl> + return ; <nl>  <nl> check_presence_and_report ( codec , pin_nid ); <nl> }
# define OP_31_XOP_SLBIA 498 <nl> # define OP_31_XOP_MFSR 595 <nl> # define OP_31_XOP_MFSRIN 659 <nl> +# define OP_31_XOP_DCBA 758 <nl> # define OP_31_XOP_SLBMFEV 851 <nl> # define OP_31_XOP_EIOIO 854 <nl> # define OP_31_XOP_SLBMFEE 915 <nl> int kvmppc_core_emulate_op ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> kvmppc_set_gpr ( vcpu , get_rt ( inst ), t ); <nl> } <nl> break ; <nl> + case OP_31_XOP_DCBA : <nl> + /* Gets treated as NOP */ <nl> + break ; <nl> case OP_31_XOP_DCBZ : <nl> { <nl> ulong rb = kvmppc_get_gpr ( vcpu , get_rb ( inst ));
static int perl_start_script ( const char * script , int argc , const char ** argv ) <nl> goto error ; <nl> } <nl>  <nl> - perl_run ( my_perl ); <nl> + if ( perl_run ( my_perl )) { <nl> + err = - 1 ; <nl> + goto error ; <nl> + } <nl> + <nl> if ( SvTRUE ( ERRSV )) { <nl> err = - 1 ; <nl> goto error ;
int pci_get_new_domain_nr ( void ) <nl> void pci_bus_assign_domain_nr ( struct pci_bus * bus , struct device * parent ) <nl> { <nl> static int use_dt_domains = - 1 ; <nl> - int domain = of_get_pci_domain_nr ( parent -> of_node ); <nl> + int domain = - 1 ; <nl>  <nl> + if ( parent ) <nl> + domain = of_get_pci_domain_nr ( parent -> of_node ); <nl> /* <nl> * Check DT domain and use_dt_domains values . <nl> *
do_open_permission ( struct svc_rqst * rqstp , struct svc_fh * current_fh , struct nfs <nl>  <nl> if ( open -> op_share_access & NFS4_SHARE_ACCESS_READ ) <nl> accmode |= MAY_READ ; <nl> - if ( open -> op_share_deny & NFS4_SHARE_ACCESS_WRITE ) <nl> + if ( open -> op_share_access & NFS4_SHARE_ACCESS_WRITE ) <nl> accmode |= ( MAY_WRITE | MAY_TRUNC ); <nl> + if ( open -> op_share_deny & NFS4_SHARE_DENY_WRITE ) <nl> + accmode |= MAY_WRITE ; <nl>  <nl> status = fh_verify ( rqstp , current_fh , S_IFREG , accmode ); <nl> 
static void __save_error_info ( struct super_block * sb , const char * func , <nl> struct ext4_super_block * es = EXT4_SB ( sb )-> s_es ; <nl>  <nl> EXT4_SB ( sb )-> s_mount_state |= EXT4_ERROR_FS ; <nl> + if ( bdev_read_only ( sb -> s_bdev )) <nl> + return ; <nl> es -> s_state |= cpu_to_le16 ( EXT4_ERROR_FS ); <nl> es -> s_last_error_time = cpu_to_le32 ( get_seconds ()); <nl> strncpy ( es -> s_last_error_func , func , sizeof ( es -> s_last_error_func ));
static int sprom_extract ( struct ssb_bus * bus , struct ssb_sprom * out , <nl> ssb_printk ( KERN_WARNING PFX " Unsupported SPROM " <nl> " revision % d detected . Will extract " <nl> " v1 \ n ", out -> revision ); <nl> + out -> revision = 1 ; <nl> sprom_extract_r123 ( out , in ); <nl> } <nl> }
static void cxl_pci_disable_device ( struct pci_dev * dev ) <nl> dev_err (& dev -> dev , " Default context started \ n "); <nl> return ; <nl> } <nl> + dev -> dev . archdata . cxl_ctx = NULL ; <nl> cxl_release_context ( ctx ); <nl> } <nl> }
int __pci_read_base ( struct pci_dev * dev , enum pci_bar_type type , <nl> /* Address above 32 - bit boundary ; disable the BAR */ <nl> pci_write_config_dword ( dev , pos , 0 ); <nl> pci_write_config_dword ( dev , pos + 4 , 0 ); <nl> + res -> flags |= IORESOURCE_UNSET ; <nl> region . start = 0 ; <nl> region . end = sz64 ; <nl> bar_disabled = true ;
void gfs2_inplace_release ( struct gfs2_inode * ip ) <nl> { <nl> struct gfs2_blkreserv * rs = ip -> i_res ; <nl>  <nl> - gfs2_blkrsv_put ( ip ); <nl> if ( rs -> rs_rgd_gh . gh_gl ) <nl> gfs2_glock_dq_uninit (& rs -> rs_rgd_gh ); <nl> + gfs2_blkrsv_put ( ip ); <nl> } <nl>  <nl> /**
done : <nl> atomic_dec (& urb -> use_count ); <nl> if ( urb -> reject ) <nl> wake_up (& usb_kill_urb_queue ); <nl> - usb_put_urb ( urb ); <nl> usbmon_urb_submit_error (& hcd -> self , urb , status ); <nl> + usb_put_urb ( urb ); <nl> } <nl> return status ; <nl> }
static int atomic_open ( struct nameidata * nd , struct dentry * dentry , <nl> dput ( dentry ); <nl> dentry = file -> f_path . dentry ; <nl> } <nl> + if ( create_error && dentry -> d_inode == NULL ) { <nl> + error = create_error ; <nl> + goto out ; <nl> + } <nl> goto looked_up ; <nl> } <nl> 
struct vm_struct * alloc_vm_area ( size_t size ) <nl> return NULL ; <nl> } <nl>  <nl> - /* Make sure the pagetables are constructed in process kernel <nl> - mappings */ <nl> - vmalloc_sync_all (); <nl> - <nl> return area ; <nl> } <nl> EXPORT_SYMBOL_GPL ( alloc_vm_area );
int xfrm_init_replay ( struct xfrm_state * x ) <nl> replay_esn -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> return - EINVAL ; <nl>  <nl> + if (( x -> props . flags & XFRM_STATE_ESN ) && replay_esn -> replay_window == 0 ) <nl> + return - EINVAL ; <nl> + <nl> if (( x -> props . flags & XFRM_STATE_ESN ) && x -> replay_esn ) <nl> x -> repl = & xfrm_replay_esn ; <nl> else
static bool ixgbe_clean_tx_irq ( struct ixgbe_q_vector * q_vector , <nl> total_packets += tx_buffer -> gso_segs ; <nl>  <nl> /* free the skb */ <nl> - dev_kfree_skb_any ( tx_buffer -> skb ); <nl> + dev_consume_skb_any ( tx_buffer -> skb ); <nl>  <nl> /* unmap skb header data */ <nl> dma_unmap_single ( tx_ring -> dev ,
store_dh_state ( struct device * dev , struct device_attribute * attr , <nl> struct scsi_device_handler * scsi_dh ; <nl> int err = - EINVAL ; <nl>  <nl> + if ( sdev -> sdev_state == SDEV_CANCEL || <nl> + sdev -> sdev_state == SDEV_DEL ) <nl> + return - ENODEV ; <nl> + <nl> if (! sdev -> scsi_dh_data ) { <nl> /* <nl> * Attach to a device handler
int hmc5843_common_probe ( struct device * dev , struct regmap * regmap , <nl> mutex_init (& data -> lock ); <nl>  <nl> indio_dev -> dev . parent = dev ; <nl> + indio_dev -> name = dev -> driver -> name ; <nl> indio_dev -> info = & hmc5843_info ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl> indio_dev -> channels = data -> variant -> channels ;
static enum dma_status d40_tx_status ( struct dma_chan * chan , <nl> } <nl>  <nl> ret = dma_cookie_status ( chan , cookie , txstate ); <nl> - if ( ret != DMA_SUCCESS ) <nl> + if ( ret != DMA_COMPLETE ) <nl> dma_set_residue ( txstate , stedma40_residue ( chan )); <nl>  <nl> if ( d40_is_paused ( d40c ))
int tf_tonga_thermal_setup_fan_table ( struct pp_hwmgr * hwmgr , void * input , void * <nl> int res ; <nl> uint64_t tmp64 ; <nl>  <nl> + if (! phm_cap_enabled ( hwmgr -> platform_descriptor . platformCaps , PHM_PlatformCaps_MicrocodeFanControl )) <nl> + return 0 ; <nl> + <nl> if ( 0 == data -> fan_table_start ) { <nl> phm_cap_unset ( hwmgr -> platform_descriptor . platformCaps , PHM_PlatformCaps_MicrocodeFanControl ); <nl> return 0 ;
static int ath10k_pci_probe ( struct pci_dev * pdev , <nl> if (! ath10k_pci_chip_is_supported ( pdev -> device , chip_id )) { <nl> ath10k_err ( ar , " device % 04x with chip_id % 08x isn ' t supported \ n ", <nl> pdev -> device , chip_id ); <nl> - goto err_sleep ; <nl> + goto err_free_irq ; <nl> } <nl>  <nl> ret = ath10k_core_register ( ar , chip_id );
qla2x00_mailbox_command ( scsi_qla_host_t * vha , mbx_cmd_t * mcp ) <nl> " mb [ 0 ] = 0x % x .\ n ", mb0 ); <nl> ql_dump_regs ( ql_dbg_mbx + ql_dbg_buffer , vha , 0x1019 ); <nl>  <nl> + /* <nl> + * Attempt to capture a firmware dump for further analysis <nl> + * of the current firmware state <nl> + */ <nl> + ha -> isp_ops -> fw_dump ( vha , 0 ); <nl> + <nl> rval = QLA_FUNCTION_TIMEOUT ; <nl> } <nl> 
int i915_vma_unbind ( struct i915_vma * vma ) <nl>  <nl> i915_gem_gtt_finish_object ( obj ); <nl>  <nl> - list_del (& vma -> mm_list ); <nl> + list_del_init (& vma -> mm_list ); <nl> /* Avoid an unnecessary call to unbind on rebind . */ <nl> if ( i915_is_ggtt ( vma -> vm )) <nl> obj -> map_and_fenceable = true ;
int symbol__init ( void ) <nl> if ( symbol_conf . initialized ) <nl> return 0 ; <nl>  <nl> + symbol_conf . priv_size = ALIGN ( symbol_conf . priv_size , sizeof ( u64 )); <nl> + <nl> elf_version ( EV_CURRENT ); <nl> if ( symbol_conf . sort_by_name ) <nl> symbol_conf . priv_size += ( sizeof ( struct symbol_name_rb_node ) -
static int xfrm_get_policy ( struct sk_buff * skb , struct nlmsghdr * nlh , void ** xfr <nl> MSG_DONTWAIT ); <nl> } <nl> } else { <nl> + c . data . byid = p -> index ; <nl> c . event = XFRM_SAP_DELETED ; <nl> c . seq = nlh -> nlmsg_seq ; <nl> c . pid = nlh -> nlmsg_pid ;
struct vpif_disp_buffer { <nl> }; <nl>  <nl> struct common_obj { <nl> - /* Buffer specific parameters */ <nl> - u8 * fbuffers [ VIDEO_MAX_FRAME ]; /* List of buffer pointers for <nl> - * storing frames */ <nl> u32 numbuffers ; /* number of buffers */ <nl> struct vpif_disp_buffer * cur_frm ; /* Pointer pointing to current <nl> * vb2_buffer */
lnet_parse_reply ( lnet_ni_t * ni , lnet_msg_t * msg ) <nl> LASSERT ( md -> md_offset == 0 ); <nl>  <nl> rlength = hdr -> payload_length ; <nl> - mlength = min_t ( int , rlength , md -> md_length ); <nl> + mlength = min_t ( uint , rlength , md -> md_length ); <nl>  <nl> if ( mlength < rlength && <nl> ( md -> md_options & LNET_MD_TRUNCATE ) == 0 ) {
static noinline_for_stack int ethtool_get_rxnfc ( struct net_device * dev , <nl>  <nl> if ( info . cmd == ETHTOOL_GRXCLSRLALL ) { <nl> if ( info . rule_cnt > 0 ) { <nl> - rule_buf = kmalloc ( info . rule_cnt * sizeof ( u32 ), <nl> - GFP_USER ); <nl> + if ( info . rule_cnt <= KMALLOC_MAX_SIZE / sizeof ( u32 )) <nl> + rule_buf = kmalloc ( info . rule_cnt * sizeof ( u32 ), <nl> + GFP_USER ); <nl> if (! rule_buf ) <nl> return - ENOMEM ; <nl> }
int cfhsi_probe ( struct platform_device * pdev ) <nl> dev_err (& ndev -> dev , "% s : Registration error : % d .\ n ", <nl> __func__ , res ); <nl> free_netdev ( ndev ); <nl> + return - ENODEV ; <nl> } <nl> /* Add CAIF HSI device to list . */ <nl> spin_lock (& cfhsi_list_lock );
# include < linux / module . h > /* Modules */ <nl> # include < linux / init . h > /* Initdata */ <nl> # include < linux / ioport . h > /* request_region */ <nl> +# include < linux / delay . h > /* msleep */ <nl> # include < linux / videodev2 . h > /* kernel radio structs */ <nl> # include < linux / version . h > /* for KERNEL_VERSION MACRO */ <nl> # include < linux / io . h > /* outb , outb_p */
static void qeth_l3_set_ip_addr_list ( struct qeth_card * card ) <nl> spin_unlock_irqrestore (& card -> ip_lock , flags ); <nl> rc = qeth_l3_register_addr_entry ( card , todo ); <nl> spin_lock_irqsave (& card -> ip_lock , flags ); <nl> - if (! rc ) <nl> + if (! rc || ( rc == IPA_RC_LAN_OFFLINE )) <nl> list_add_tail (& todo -> entry , & card -> ip_list ); <nl> else <nl> kfree ( todo );
static int video_open ( struct file * file ) <nl>  <nl> if ( NULL == dev ) { <nl> mutex_unlock (& cx25821_devlist_mutex ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> 
static inline u32 ext4_chksum ( struct ext4_sb_info * sbi , u32 crc , <nl> { <nl> struct { <nl> struct shash_desc shash ; <nl> - char ctx [ crypto_shash_descsize ( sbi -> s_chksum_driver )]; <nl> + char ctx [ 4 ]; <nl> } desc ; <nl> int err ; <nl>  <nl> + BUG_ON ( crypto_shash_descsize ( sbi -> s_chksum_driver )!= sizeof ( desc . ctx )); <nl> + <nl> desc . shash . tfm = sbi -> s_chksum_driver ; <nl> desc . shash . flags = 0 ; <nl> *( u32 *) desc . ctx = crc ;
static hda_nid_t path_power_update ( struct hda_codec * codec , <nl>  <nl> for ( i = 0 ; i < path -> depth ; i ++) { <nl> nid = path -> path [ i ]; <nl> + if (!( get_wcaps ( codec , nid ) & AC_WCAP_POWER )) <nl> + continue ; <nl> if ( nid == codec -> core . afg ) <nl> continue ; <nl> if (! allow_powerdown || is_active_nid_for_any ( codec , nid ))
static enum dma_status omap_dma_tx_status ( struct dma_chan * chan , <nl>  <nl> if ( d -> dir == DMA_MEM_TO_DEV ) <nl> pos = omap_dma_get_src_pos ( c ); <nl> - else if ( d -> dir == DMA_DEV_TO_MEM ) <nl> + else if ( d -> dir == DMA_DEV_TO_MEM || d -> dir == DMA_MEM_TO_MEM ) <nl> pos = omap_dma_get_dst_pos ( c ); <nl> else <nl> pos = 0 ;
static void default_idle ( void ) <nl> if ( test_thread_flag ( TIF_MCCK_PENDING )) { <nl> local_mcck_enable (); <nl> local_irq_enable (); <nl> - s390_handle_mcck (); <nl> return ; <nl> } <nl> trace_hardirqs_on (); <nl> void cpu_idle ( void ) <nl> for (;;) { <nl> tick_nohz_idle_enter (); <nl> rcu_idle_enter (); <nl> - while (! need_resched ()) <nl> + while (! need_resched () && ! test_thread_flag ( TIF_MCCK_PENDING )) <nl> default_idle (); <nl> rcu_idle_exit (); <nl> tick_nohz_idle_exit (); <nl> + if ( test_thread_flag ( TIF_MCCK_PENDING )) <nl> + s390_handle_mcck (); <nl> preempt_enable_no_resched (); <nl> schedule (); <nl> preempt_disable ();
static void storvsc_handle_error ( struct vmscsi_request * vm_srb , <nl> do_work = true ; <nl> process_err_fn = storvsc_remove_lun ; <nl> break ; <nl> - case ( SRB_STATUS_ABORTED | SRB_STATUS_AUTOSENSE_VALID ): <nl> - if (( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> + case SRB_STATUS_ABORTED : <nl> + if ( vm_srb -> srb_status & SRB_STATUS_AUTOSENSE_VALID && <nl> + ( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> do_work = true ; <nl> process_err_fn = storvsc_device_scan ; <nl> /*
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) <nl> vnt_mac_set_bssid_addr ( priv , ( u8 *) conf -> bssid ); <nl>  <nl> 
static int nl80211_trigger_scan ( struct sk_buff * skb , struct genl_info * info ) <nl> tmp ) { <nl> enum ieee80211_band band = nla_type ( attr ); <nl>  <nl> - if ( band < 0 || band > IEEE80211_NUM_BANDS ) { <nl> + if ( band < 0 || band >= IEEE80211_NUM_BANDS ) { <nl> err = - EINVAL ; <nl> goto out_free ; <nl> }
int ext3_group_add ( struct super_block * sb , struct ext3_new_group_data * input ) <nl> if ( input -> group != EXT3_SB ( sb )-> s_groups_count ) { <nl> ext3_warning ( sb , __FUNCTION__ , <nl> " multiple resizers run on filesystem !\ n "); <nl> + err = - EBUSY ; <nl> goto exit_journal ; <nl> } <nl> 
static int pit_ioport_read ( struct kvm_io_device * this , <nl> return - EOPNOTSUPP ; <nl>  <nl> addr &= KVM_PIT_CHANNEL_MASK ; <nl> + if ( addr == 3 ) <nl> + return 0 ; <nl> + <nl> s = & pit_state -> channels [ addr ]; <nl>  <nl> mutex_lock (& pit_state -> lock );
int usb_set_interface ( struct usb_device * dev , int interface , int alternate ) <nl> interface ); <nl> return - EINVAL ; <nl> } <nl> + if ( iface -> unregistering ) <nl> + return - ENODEV ; <nl>  <nl> alt = usb_altnum_to_altsetting ( iface , alternate ); <nl> if (! alt ) {
int devres_release_all ( struct device * dev ) <nl> { <nl> unsigned long flags ; <nl>  <nl> + /* Looks like an uninitialized device structure */ <nl> + if ( WARN_ON ( dev -> devres_head . next == NULL )) <nl> + return - ENODEV ; <nl> spin_lock_irqsave (& dev -> devres_lock , flags ); <nl> return release_nodes ( dev , dev -> devres_head . next , & dev -> devres_head , <nl> flags );
static const char * get_input_type ( struct hda_gnode * node , unsigned int * pinctl ) <nl> return " Front Aux "; <nl> return " Aux "; <nl> case AC_JACK_MIC_IN : <nl> - if ( node -> pin_caps & <nl> - ( AC_PINCAP_VREF_80 << AC_PINCAP_VREF_SHIFT )) <nl> + if ( pinctl && <nl> + ( node -> pin_caps & <nl> + ( AC_PINCAP_VREF_80 << AC_PINCAP_VREF_SHIFT ))) <nl> * pinctl |= AC_PINCTL_VREF_80 ; <nl> if (( location & 0x0f ) == AC_JACK_LOC_FRONT ) <nl> return " Front Mic ";
mptctl_getiocinfo ( unsigned long arg , unsigned int data_size ) <nl> else <nl> karg -> adapterType = MPT_IOCTL_INTERFACE_SCSI ; <nl>  <nl> - if ( karg -> hdr . port > 1 ) <nl> + if ( karg -> hdr . port > 1 ) { <nl> + kfree ( karg ); <nl> return - EINVAL ; <nl> + } <nl> port = karg -> hdr . port ; <nl>  <nl> karg -> port = port ;
static int __devinit tc35815_read_plat_dev_addr ( struct net_device * dev ) <nl> return - ENODEV ; <nl> } <nl> # else <nl> - static int __devinit tc35815_read_plat_dev_addr ( struct device * dev ) <nl> + static int __devinit tc35815_read_plat_dev_addr ( struct net_device * dev ) <nl> { <nl> return - ENODEV ; <nl> }
hw_perf_counter_init ( struct perf_counter * counter ) <nl> return NULL ; <nl> } <nl> events [ n ] = ev ; <nl> + ctrs [ n ] = counter ; <nl> if ( check_excludes ( ctrs , n , 1 )) <nl> return NULL ; <nl> if ( power_check_constraints ( events , n + 1 ))
cfq_update_io_seektime ( struct cfq_data * cfqd , struct cfq_io_context * cic , <nl> sector_t sdist ; <nl> u64 total ; <nl>  <nl> - if ( cic -> last_request_pos < rq -> sector ) <nl> + if (! cic -> last_request_pos ) <nl> + sdist = 0 ; <nl> + else if ( cic -> last_request_pos < rq -> sector ) <nl> sdist = rq -> sector - cic -> last_request_pos ; <nl> else <nl> sdist = cic -> last_request_pos - rq -> sector ;
flush_thread ( void ) <nl> ia32_drop_ia64_partial_page_list ( current ); <nl> current -> thread . task_size = IA32_PAGE_OFFSET ; <nl> set_fs ( USER_DS ); <nl> + memset ( current -> thread . tls_array , 0 , sizeof ( current -> thread . tls_array )); <nl> } <nl> # endif <nl> }
DT_MACHINE_START ( R8A7794_DT , " Generic R8A7794 ( Flattened Device Tree )") <nl> . init_early = shmobile_init_delay , <nl> . init_late = shmobile_init_late , <nl> . init_time = rcar_gen2_timer_init , <nl> + . reserve = rcar_gen2_reserve , <nl> . dt_compat = r8a7794_boards_compat_dt , <nl> MACHINE_END
int n_tty_ioctl ( struct tty_struct * tty , struct file * file , <nl> ld = tty_ldisc_ref ( tty ); <nl> switch ( arg ) { <nl> case TCIFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> break ; <nl> case TCIOFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> /* fall through */ <nl> case TCOFLUSH :
acpi_ex_load_op ( union acpi_operand_object * obj_desc , <nl>  <nl> /* table_ptr was deallocated above */ <nl>  <nl> + acpi_ut_remove_reference ( ddb_handle ); <nl> return_ACPI_STATUS ( status ); <nl> } <nl> 
static int sixpack_ioctl ( struct tty_struct * tty , struct file * file , <nl> unsigned int cmd , unsigned long arg ) <nl> { <nl> struct sixpack * sp = sp_get ( tty ); <nl> - struct net_device * dev = sp -> dev ; <nl> + struct net_device * dev ; <nl> unsigned int tmp , err ; <nl>  <nl> if (! sp ) <nl> return - ENXIO ; <nl> + dev = sp -> dev ; <nl>  <nl> switch ( cmd ) { <nl> case SIOCGIFNAME :
static void dbri_debug_read ( struct snd_info_entry * entry , <nl> } <nl> # endif <nl>  <nl> - void __devinit snd_dbri_proc ( struct snd_card * card ) <nl> + static void __devinit snd_dbri_proc ( struct snd_card * card ) <nl> { <nl> struct snd_dbri * dbri = card -> private_data ; <nl> struct snd_info_entry * entry ;
int __devinit of_mtd_parse_partitions ( struct device * dev , <nl> return nr_parts ; <nl> } <nl> EXPORT_SYMBOL ( of_mtd_parse_partitions ); <nl> + <nl> + MODULE_LICENSE (" GPL ");
static int __init sh_pm_runtime_init ( void ) <nl> ! of_machine_is_compatible (" renesas , r8a7779 ") && <nl> ! of_machine_is_compatible (" renesas , r8a7790 ") && <nl> ! of_machine_is_compatible (" renesas , r8a7791 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7792 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7793 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7794 ") && <nl> ! of_machine_is_compatible (" renesas , sh7372 ") && <nl> ! of_machine_is_compatible (" renesas , sh73a0 ")) <nl> return 0 ;
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> - err = do_migrate_pages ( mm , & old , & new , MPOL_MF_MOVE ); <nl> + err = do_migrate_pages ( mm , & old , & new , <nl> + capable ( CAP_SYS_ADMIN ) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE ); <nl> out : <nl> mmput ( mm ); <nl> return err ;
static void parse_system_parameter_string ( struct seq_file * m ) <nl> if (! workbuffer ) { <nl> printk ( KERN_ERR "% s % s kmalloc failure at line % d \ n ", <nl> __FILE__ , __FUNCTION__ , __LINE__ ); <nl> + kfree ( local_buffer ); <nl> return ; <nl> } <nl> # ifdef LPARCFG_DEBUG
qla2x00_do_dpc_vp ( scsi_qla_host_t * vha ) <nl> struct qla_hw_data * ha = vha -> hw ; <nl> scsi_qla_host_t * base_vha = pci_get_drvdata ( ha -> pdev ); <nl>  <nl> + if (!( ha -> current_topology & ISP_CFG_F )) <nl> + return 0 ; <nl> + <nl> if ( test_and_clear_bit ( VP_IDX_ACQUIRED , & vha -> vp_flags )) { <nl> /* VP acquired . complete port configuration */ <nl> if ( atomic_read (& base_vha -> loop_state ) == LOOP_READY ) {
static int scan ( struct wiphy * wiphy , struct cfg80211_scan_request * request ) <nl> kmalloc_array ( request -> n_ssids , <nl> sizeof ( struct hidden_network ), <nl> GFP_KERNEL ); <nl> + if (! strHiddenNetwork . net_info ) <nl> + return - ENOMEM ; <nl> strHiddenNetwork . n_ssids = request -> n_ssids ; <nl>  <nl> 
int wl12xx_allocate_link ( struct wl1271 * wl , struct wl12xx_vif * wlvif , u8 * hlid ) <nl> __set_bit ( link , wl -> links_map ); <nl> __set_bit ( link , wlvif -> links_map ); <nl> spin_unlock_irqrestore (& wl -> wl_lock , flags ); <nl> + <nl> + /* take the last " freed packets " value from the current FW status */ <nl> + wl -> links [ link ]. prev_freed_pkts = <nl> + wl -> fw_status_2 -> counters . tx_lnk_free_pkts [ link ]; <nl> * hlid = link ; <nl> return 0 ; <nl> }
Configuration options : <nl> # define BCD 0x01 <nl>  <nl> # define ATAO_2_RTSISHFT 0x06 /* W 8 */ <nl> -# define RSI 0x01 <nl> +# define ATAO_RTSISHFT_RSI ( 1 << 0 ) <nl>  <nl> # define ATAO_2_RTSISTRB 0x07 /* W 8 */ <nl> 
static inline void dev_set_cma_area ( struct device * dev , struct cma * cma ) <nl> { <nl> if ( dev ) <nl> dev -> cma_area = cma ; <nl> - if (! dev || ! dma_contiguous_default_area ) <nl> + if (! dev && ! dma_contiguous_default_area ) <nl> dma_contiguous_default_area = cma ; <nl> } <nl> 
static ssize_t w1_seq_show ( struct device * device , <nl> w1_write_8 ( sl -> master , W1_42_COND_READ ); <nl> rv = w1_read_block ( sl -> master , ( u8 *)& rn , 8 ); <nl> reg_num = ( struct w1_reg_num *) & rn ; <nl> - if (( char ) reg_num -> family == W1_42_FINISHED_BYTE ) <nl> + if ( reg_num -> family == W1_42_FINISHED_BYTE ) <nl> break ; <nl> if ( sl -> reg_num . id == reg_num -> id ) <nl> seq = i ;
static void iso_callback ( struct fw_iso_context * context , u32 cycle , <nl> struct client * client = data ; <nl> struct iso_interrupt_event * e ; <nl>  <nl> - e = kzalloc ( sizeof (* e ) + header_length , GFP_ATOMIC ); <nl> + e = kmalloc ( sizeof (* e ) + header_length , GFP_ATOMIC ); <nl> if ( e == NULL ) <nl> return ; <nl> 
static const struct usb_device_id hso_ids [] = { <nl> { USB_DEVICE ( 0x0af0 , 0x8600 )}, <nl> { USB_DEVICE ( 0x0af0 , 0x8800 )}, <nl> { USB_DEVICE ( 0x0af0 , 0x8900 )}, <nl> + { USB_DEVICE ( 0x0af0 , 0x9000 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd035 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd055 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd155 )},
static void snd_als4000_configure ( struct snd_sb * chip ) <nl> /* SPECS_PAGE : 39 */ <nl> for ( i = ALS4K_GCR91_DMA0_ADDR ; i <= ALS4K_GCR96_DMA3_MODE_COUNT ; ++ i ) <nl> snd_als4k_gcr_write ( chip , i , 0 ); <nl> - <nl> + /* enable burst mode to prevent dropouts during high PCI bus usage */ <nl> snd_als4k_gcr_write ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL , <nl> - snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL )); <nl> + snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL ) | 0x04 ); <nl> spin_unlock_irq (& chip -> reg_lock ); <nl> } <nl> 
static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> int err ; <nl> unsigned int mss ; <nl>  <nl> + if ( packets == 0 ) <nl> + return ; <nl> + <nl> WARN_ON ( packets > tp -> packets_out ); <nl> if ( tp -> lost_skb_hint ) { <nl> skb = tp -> lost_skb_hint ;
static int do_dev_config ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> if (! devs ) { <nl> dev_err ( dev -> class_dev , <nl> " Could not allocate memory . Out of memory ?\ n "); <nl> + kfree ( bdev ); <nl> return - ENOMEM ; <nl> } <nl> devpriv -> devs = devs ;
int prism2_scan ( struct wiphy * wiphy , struct net_device * dev , <nl> msg1 . msgcode = DIDmsg_dot11req_scan ; <nl> msg1 . bsstype . data = P80211ENUM_bsstype_any ; <nl>  <nl> - memset (&( msg1 . bssid . data ), 0xFF , sizeof ( p80211item_pstr6_t )); <nl> + memset (& msg1 . bssid . data , 0xFF , sizeof ( msg1 . bssid . data )); <nl> msg1 . bssid . data . len = 6 ; <nl>  <nl> if ( request -> n_ssids > 0 ) {
static struct dmar_domain * dmar_insert_one_dev_info ( struct intel_iommu * iommu , <nl>  <nl> if ( ret ) { <nl> spin_unlock_irqrestore (& device_domain_lock , flags ); <nl> + free_devinfo_mem ( info ); <nl> return NULL ; <nl> } <nl> 
static void * i915_gem_dmabuf_vmap ( struct dma_buf * dma_buf ) <nl> goto error ; <nl>  <nl> i = 0 ; <nl> - for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ); <nl> + for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ) <nl> pages [ i ++] = sg_page_iter_page (& sg_iter ); <nl>  <nl> obj -> dma_buf_vmapping = vmap ( pages , i , 0 , PAGE_KERNEL );
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
xfs_log_force_umount ( <nl> * before we mark the filesystem SHUTDOWN and wake <nl> * everybody up to tell the bad news . <nl> */ <nl> - spin_lock (& log -> l_grant_lock ); <nl> spin_lock (& log -> l_icloglock ); <nl> + spin_lock (& log -> l_grant_lock ); <nl> mp -> m_flags |= XFS_MOUNT_FS_SHUTDOWN ; <nl> XFS_BUF_DONE ( mp -> m_sb_bp ); <nl> /*
static int ocfs2_truncate_file ( struct inode * inode , <nl> ( unsigned long long ) OCFS2_I ( inode )-> ip_blkno , <nl> ( unsigned long long ) new_i_size ); <nl>  <nl> + unmap_mapping_range ( inode -> i_mapping , new_i_size + PAGE_SIZE - 1 , 0 , 1 ); <nl> truncate_inode_pages ( inode -> i_mapping , new_i_size ); <nl>  <nl> fe = ( struct ocfs2_dinode *) di_bh -> b_data ;
void __init pxa_init_irq_gpio ( int gpio_nr ) <nl> set_irq_flags ( irq , IRQF_VALID | IRQF_PROBE ); <nl> } <nl>  <nl> - for ( irq = IRQ_GPIO ( 2 ); irq <= IRQ_GPIO ( gpio_nr ); irq ++) { <nl> + for ( irq = IRQ_GPIO ( 2 ); irq < IRQ_GPIO ( gpio_nr ); irq ++) { <nl> set_irq_chip ( irq , & pxa_muxed_gpio_chip ); <nl> set_irq_handler ( irq , handle_edge_irq ); <nl> set_irq_flags ( irq , IRQF_VALID | IRQF_PROBE );
static int rtl8169_rx_interrupt ( struct net_device * dev , <nl> pkt_size , PCI_DMA_FROMDEVICE ); <nl> rtl8169_mark_to_asic ( desc , tp -> rx_buf_sz ); <nl> } else { <nl> - pci_unmap_single ( pdev , addr , pkt_size , <nl> + pci_unmap_single ( pdev , addr , tp -> rx_buf_sz , <nl> PCI_DMA_FROMDEVICE ); <nl> tp -> Rx_skbuff [ entry ] = NULL ; <nl> }
void __update_cache ( struct vm_area_struct * vma , <nl> return ; <nl>  <nl> page = pfn_to_page ( pfn ); <nl> - if ( pfn_valid ( pfn ) && page_mapping ( page )) { <nl> + if ( pfn_valid ( pfn )) { <nl> int dirty = test_and_clear_bit ( PG_dcache_dirty , & page -> flags ); <nl> if ( dirty ) { <nl> unsigned long addr = ( unsigned long ) page_address ( page );
static ssize_t cpufv_store ( struct device * dev , <nl> return rv ; <nl> if ( value < 0 || value >= c . num ) <nl> return - EINVAL ; <nl> - set_acpi ( eeepc , CM_ASL_CPUFV , value ); <nl> + rv = set_acpi ( eeepc , CM_ASL_CPUFV , value ); <nl> + if ( rv ) <nl> + return rv ; <nl> return count ; <nl> } <nl> 
static int isp_register_entities ( struct isp_device * isp ) <nl> goto done ; <nl>  <nl> /* Register external entities */ <nl> - for ( subdevs = pdata -> subdevs ; subdevs -> subdevs ; ++ subdevs ) { <nl> + for ( subdevs = pdata -> subdevs ; subdevs && subdevs -> subdevs ; ++ subdevs ) { <nl> struct v4l2_subdev * sensor ; <nl> struct media_entity * input ; <nl> unsigned int flags ;
static __init void nested_vmx_setup_ctls_msrs ( void ) <nl> /* nested EPT : emulate EPT also to L1 */ <nl> nested_vmx_secondary_ctls_high |= SECONDARY_EXEC_ENABLE_EPT ; <nl> nested_vmx_ept_caps = VMX_EPT_PAGE_WALK_4_BIT | <nl> - VMX_EPTP_WB_BIT | VMX_EPT_INVEPT_BIT ; <nl> + VMX_EPTP_WB_BIT | VMX_EPT_2MB_PAGE_BIT | <nl> + VMX_EPT_INVEPT_BIT ; <nl> nested_vmx_ept_caps &= vmx_capability . ept ; <nl> /* <nl> * Since invept is completely emulated we support both global
static int option_inquiry ( struct us_data * us ) <nl>  <nl> result = memcmp ( buffer + 8 , " Option ", 6 ); <nl>  <nl> + if ( result != 0 ) <nl> + result = memcmp ( buffer + 8 , " ZCOPTION ", 8 ); <nl> + <nl> /* Read the CSW */ <nl> usb_stor_bulk_transfer_buf ( us , <nl> us -> recv_bulk_pipe ,
static irqreturn_t snd_hdsp_interrupt ( int irq , void * dev_id ) <nl> midi0status = hdsp_read ( hdsp , HDSP_midiStatusIn0 ) & 0xff ; <nl> midi1status = hdsp_read ( hdsp , HDSP_midiStatusIn1 ) & 0xff ; <nl>  <nl> + if (!( hdsp -> state & HDSP_InitializationComplete )) <nl> + return IRQ_HANDLED ; <nl> + <nl> if ( audio ) { <nl> if ( hdsp -> capture_substream ) <nl> snd_pcm_period_elapsed ( hdsp -> pcm -> streams [ SNDRV_PCM_STREAM_CAPTURE ]. substream );
retry : <nl> list_add_tail (& cap -> session_caps , & session -> s_caps ); <nl> session -> s_nr_caps ++; <nl> spin_unlock (& session -> s_cap_lock ); <nl> - } <nl> + } else if ( new_cap ) <nl> + ceph_put_cap ( mdsc , new_cap ); <nl>  <nl> if (! ci -> i_snap_realm ) { <nl> /*
fail_put : <nl> ip -> i_gl -> gl_object = NULL ; <nl> gfs2_glock_put ( ip -> i_gl ); <nl> fail : <nl> - iput ( inode ); <nl> + iget_failed ( inode ); <nl> return ERR_PTR ( error ); <nl> } <nl> 
struct e1000_nvm_operations { <nl>  <nl> struct e1000_mac_info { <nl> struct e1000_mac_operations ops ; <nl> - <nl> - u8 addr [ 6 ]; <nl> - u8 perm_addr [ 6 ]; <nl> + u8 addr [ ETH_ALEN ]; <nl> + u8 perm_addr [ ETH_ALEN ]; <nl>  <nl> enum e1000_mac_type type ; <nl> 
xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
static unsigned int br_nf_pre_routing ( unsigned int hook , struct sk_buff * skb , <nl>  <nl> pskb_trim_rcsum ( skb , len ); <nl>  <nl> + /* BUG : Should really parse the IP options here . */ <nl> + memset ( IPCB ( skb ), 0 , sizeof ( struct inet_skb_parm )); <nl> + <nl> nf_bridge_put ( skb -> nf_bridge ); <nl> if (! nf_bridge_alloc ( skb )) <nl> return NF_DROP ;
static void das16m1_handler ( struct comedi_device * dev , unsigned int status ) <nl> num_samples = FIFO_SIZE ; <nl> insw ( dev -> iobase , devpriv -> ai_buffer , num_samples ); <nl> munge_sample_array ( devpriv -> ai_buffer , num_samples ); <nl> - cfc_write_array_to_buffer ( s , devpriv -> ai_buffer , <nl> - num_samples * sizeof ( short )); <nl> + comedi_buf_write_samples ( s , devpriv -> ai_buffer , num_samples ); <nl> devpriv -> adc_count += num_samples ; <nl>  <nl> if ( cmd -> stop_src == TRIG_COUNT ) {
void thread__delete ( struct thread * thread ) <nl> { <nl> struct comm * comm , * tmp ; <nl>  <nl> - map_groups__put ( thread -> mg ); <nl> - thread -> mg = NULL ; <nl> + if ( thread -> mg ) { <nl> + map_groups__put ( thread -> mg ); <nl> + thread -> mg = NULL ; <nl> + } <nl> list_for_each_entry_safe ( comm , tmp , & thread -> comm_list , list ) { <nl> list_del (& comm -> list ); <nl> comm__free ( comm );
static inline bool i40e_vc_isvalid_vsi_id ( struct i40e_vf * vf , u8 vsi_id ) <nl> { <nl> struct i40e_pf * pf = vf -> pf ; <nl>  <nl> + if ( vsi_id > pf -> num_alloc_vsi ) <nl> + return false ; <nl> return pf -> vsi [ vsi_id ]-> vf_id == vf -> vf_id ; <nl> } <nl> 
static int tda10071_get_frontend ( struct dvb_frontend * fe ) <nl> if ( ret ) <nl> goto error ; <nl>  <nl> - c -> symbol_rate = ( buf [ 0 ] << 16 ) | ( buf [ 1 ] << 8 ) | ( buf [ 2 ] << 0 ); <nl> + c -> symbol_rate = (( buf [ 0 ] << 16 ) | ( buf [ 1 ] << 8 ) | ( buf [ 2 ] << 0 )) * 1000 ; <nl>  <nl> return ret ; <nl> error :
int __init acpi_ec_ecdt_probe ( void ) <nl> saved_ec = kmalloc ( sizeof ( struct acpi_ec ), GFP_KERNEL ); <nl> if (! saved_ec ) <nl> return - ENOMEM ; <nl> - memcpy (& saved_ec , boot_ec , sizeof ( saved_ec )); <nl> + memcpy ( saved_ec , boot_ec , sizeof (* saved_ec )); <nl> /* fall through */ <nl> } <nl> /* This workaround is needed only on some broken machines ,
int setup_arg_pages ( struct linux_binprm * bprm , <nl> # else <nl> stack_top = arch_align_stack ( stack_top ); <nl> stack_top = PAGE_ALIGN ( stack_top ); <nl> + <nl> + if ( unlikely ( stack_top < mmap_min_addr ) || <nl> + unlikely ( vma -> vm_end - vma -> vm_start >= stack_top - mmap_min_addr )) <nl> + return - ENOMEM ; <nl> + <nl> stack_shift = vma -> vm_end - stack_top ; <nl>  <nl> bprm -> p -= stack_shift ;
static void vmlinux_path__exit ( void ) <nl> { <nl> while (-- vmlinux_path__nr_entries >= 0 ) <nl> zfree (& vmlinux_path [ vmlinux_path__nr_entries ]); <nl> + vmlinux_path__nr_entries = 0 ; <nl>  <nl> zfree (& vmlinux_path ); <nl> }
static void netlink_rcv_cb ( struct sk_buff * skb ) <nl> if ( skb -> len >= NLMSG_HDRLEN ) { <nl> nlh = ( struct nlmsghdr *) skb -> data ; <nl>  <nl> - if ( skb -> len < nlh -> nlmsg_len || <nl> + if ( nlh -> nlmsg_len < ND_IFINDEX_LEN || <nl> + nlh -> nlmsg_len > skb -> len || <nl> nlh -> nlmsg_len > ND_MAX_MSG_LEN ) { <nl> netdev_err ( skb -> dev , " Invalid length (% d ,% d )\ n ", <nl> skb -> len , nlh -> nlmsg_len );
static int parse_opts ( char * params , struct p9_fd_opts * opts ) <nl> opts -> port = P9_PORT ; <nl> opts -> rfd = ~ 0 ; <nl> opts -> wfd = ~ 0 ; <nl> + opts -> privport = 0 ; <nl>  <nl> if (! params ) <nl> return 0 ;
static int efi_status_to_err ( efi_status_t status ) <nl> err = - EACCES ; <nl> break ; <nl> case EFI_NOT_FOUND : <nl> - err = - ENOENT ; <nl> + err = - EIO ; <nl> break ; <nl> default : <nl> err = - EINVAL ;
xfs_bmapi ( <nl> xfs_fsblock_t abno ; /* allocated block number */ <nl> xfs_extlen_t alen ; /* allocated extent length */ <nl> xfs_fileoff_t aoff ; /* allocated file offset */ <nl> - xfs_bmalloca_t bma ; /* args for xfs_bmap_alloc */ <nl> + xfs_bmalloca_t bma = { 0 }; /* args for xfs_bmap_alloc */ <nl> xfs_btree_cur_t * cur ; /* bmap btree cursor */ <nl> xfs_fileoff_t end ; /* end of mapped file region */ <nl> int eof ; /* we ' ve hit the end of extents */
int ieee80211_request_sched_scan_start ( struct ieee80211_sub_if_data * sdata , <nl> for ( i = 0 ; i < IEEE80211_NUM_BANDS ; i ++) { <nl> local -> sched_scan_ies . ie [ i ] = kzalloc ( 2 + <nl> IEEE80211_MAX_SSID_LEN + <nl> - local -> scan_ies_len , <nl> + local -> scan_ies_len + <nl> + req -> ie_len , <nl> GFP_KERNEL ); <nl> if (! local -> sched_scan_ies . ie [ i ]) { <nl> ret = - ENOMEM ;
static int pctv452e_read_mac_address ( struct dvb_usb_device * d , u8 mac [ 6 ]) <nl> return 0 ; <nl>  <nl> failed : <nl> - memset ( mac , 0 , 6 ); <nl> + eth_zero_addr ( mac ); <nl>  <nl> return ret ; <nl> }
INTEL_VGA_DEVICE ( 0x191D , info ) /* WKS GT2 */ <nl>  <nl> # define INTEL_SKL_GT3_IDS ( info ) \ <nl> + INTEL_VGA_DEVICE ( 0x1923 , info ), /* ULT GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x1926 , info ), /* ULT GT3 */ \ <nl> + INTEL_VGA_DEVICE ( 0x1927 , info ), /* ULT GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x192B , info ), /* Halo GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x192A , info ) /* SRV GT3 */ <nl> 
void omap_gem_init ( struct drm_device * dev ) <nl> } <nl>  <nl> usergart = kzalloc ( 3 * sizeof (* usergart ), GFP_KERNEL ); <nl> + if (! usergart ) { <nl> + dev_warn ( dev -> dev , " could not allocate usergart \ n "); <nl> + return ; <nl> + } <nl>  <nl> /* reserve 4k aligned / wide regions for userspace mappings : */ <nl> for ( i = 0 ; i < ARRAY_SIZE ( fmts ); i ++) {
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> + if ( count > XFS_ACL_MAX_ENTRIES ) <nl> + return ERR_PTR (- EFSCORRUPTED ); <nl>  <nl> acl = posix_acl_alloc ( count , GFP_KERNEL ); <nl> if (! acl )
static void __call_console_drivers ( unsigned long start , unsigned long end ) <nl>  <nl> static int __read_mostly ignore_loglevel ; <nl>  <nl> - int __init ignore_loglevel_setup ( char * str ) <nl> + static int __init ignore_loglevel_setup ( char * str ) <nl> { <nl> ignore_loglevel = 1 ; <nl> printk ( KERN_INFO " debug : ignoring loglevel setting .\ n ");
static void __cpuinit early_init_intel ( struct cpuinfo_x86 * c ) <nl> { <nl> /* Unmask CPUID levels if masked : */ <nl> - if ( c -> x86 == 6 && c -> x86_model >= 15 ) { <nl> + if ( c -> x86 > 6 || ( c -> x86 == 6 && c -> x86_model >= 0xd )) { <nl> u64 misc_enable ; <nl>  <nl> rdmsrl ( MSR_IA32_MISC_ENABLE , misc_enable );
static int snd_rme32_capture_close ( struct snd_pcm_substream * substream ) <nl> spin_lock_irq (& rme32 -> lock ); <nl> rme32 -> capture_substream = NULL ; <nl> rme32 -> capture_periodsize = 0 ; <nl> - spin_unlock (& rme32 -> lock ); <nl> + spin_unlock_irq (& rme32 -> lock ); <nl> return 0 ; <nl> } <nl> 
static int xhci_setup_device ( struct usb_hcd * hcd , struct usb_device * udev , <nl>  <nl> mutex_lock (& xhci -> mutex ); <nl>  <nl> + if ( xhci -> xhc_state ) /* dying or halted */ <nl> + goto out ; <nl> + <nl> if (! udev -> slot_id ) { <nl> xhci_dbg_trace ( xhci , trace_xhci_dbg_address , <nl> " Bad Slot ID % d ", udev -> slot_id );
static void r600_cs_parser_fini ( struct radeon_cs_parser * parser , int error ) <nl> kfree ( parser -> relocs ); <nl> for ( i = 0 ; i < parser -> nchunks ; i ++) { <nl> kfree ( parser -> chunks [ i ]. kdata ); <nl> + kfree ( parser -> chunks [ i ]. kpage [ 0 ]); <nl> + kfree ( parser -> chunks [ i ]. kpage [ 1 ]); <nl> } <nl> kfree ( parser -> chunks ); <nl> kfree ( parser -> chunks_array );
static int show_numa_map ( struct seq_file * m , void * v , int is_pid ) <nl> walk . mm = mm ; <nl>  <nl> pol = get_vma_policy ( task , vma , vma -> vm_start ); <nl> - mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> + n = mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> mpol_cond_put ( pol ); <nl> + if ( n < 0 ) <nl> + return n ; <nl>  <nl> seq_printf ( m , "% 08lx % s ", vma -> vm_start , buffer ); <nl> 
static int __init omapdss_hdmihw_probe ( struct platform_device * pdev ) <nl> hdmi . ip_data . pll_offset = HDMI_PLLCTRL ; <nl> hdmi . ip_data . phy_offset = HDMI_PHY ; <nl>  <nl> + hdmi_init_output ( pdev ); <nl> + <nl> r = hdmi_panel_init (); <nl> if ( r ) { <nl> DSSERR (" can ' t init panel \ n "); <nl> static int __init omapdss_hdmihw_probe ( struct platform_device * pdev ) <nl>  <nl> dss_debugfs_create_file (" hdmi ", hdmi_dump_regs ); <nl>  <nl> - hdmi_init_output ( pdev ); <nl> - <nl> hdmi_probe_pdata ( pdev ); <nl>  <nl> return 0 ;
static int ath10k_station_assoc ( struct ath10k * ar , struct ath10k_vif * arvif , <nl> return ret ; <nl> } <nl>  <nl> - if (! sta -> wme ) { <nl> + if (! sta -> wme && ! reassoc ) { <nl> arvif -> num_legacy_stations ++; <nl> ret = ath10k_recalc_rtscts_prot ( arvif ); <nl> if ( ret ) {
recurse : <nl> if ( err ) <nl> goto out_free ; <nl>  <nl> - kobject_uevent (& b -> kobj , KOBJ_ADD ); <nl> + if ( b ) <nl> + kobject_uevent (& b -> kobj , KOBJ_ADD ); <nl>  <nl> return err ; <nl> 
static int get_info ( struct net * net , void __user * user , <nl> private = & tmp ; <nl> } <nl> # endif <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . valid_hooks = t -> valid_hooks ; <nl> memcpy ( info . hook_entry , private -> hook_entry , <nl> sizeof ( info . hook_entry ));
int ehci_suspend ( struct usb_hcd * hcd , bool do_wakeup ) <nl> clear_bit ( HCD_FLAG_HW_ACCESSIBLE , & hcd -> flags ); <nl> spin_unlock_irq (& ehci -> lock ); <nl>  <nl> + synchronize_irq ( hcd -> irq ); <nl> + <nl> + /* Check for race with a wakeup request */ <nl> + if ( do_wakeup && HCD_WAKEUP_PENDING ( hcd )) { <nl> + ehci_resume ( hcd , false ); <nl> + return - EBUSY ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( ehci_suspend );
static int ath9k_htc_aggr_oper ( struct ath9k_htc_priv * priv , <nl> int ret = 0 ; <nl> u8 cmd_rsp ; <nl>  <nl> - if ( tid > ATH9K_HTC_MAX_TID ) <nl> + if ( tid >= ATH9K_HTC_MAX_TID ) <nl> return - EINVAL ; <nl>  <nl> memset (& aggr , 0 , sizeof ( struct ath9k_htc_target_aggr ));
int lapic_watchdog_init ( unsigned nmi_hz ) <nl> probe_nmi_watchdog (); <nl> if (! wd_ops ) <nl> return - 1 ; <nl> + <nl> + if (! wd_ops -> reserve ()) { <nl> + printk ( KERN_ERR <nl> + " NMI watchdog : cannot reserve perfctrs \ n "); <nl> + return - 1 ; <nl> + } <nl> } <nl>  <nl> if (!( wd_ops -> setup ( nmi_hz ))) {
struct btrfs_root * btrfs_create_tree ( struct btrfs_trans_handle * trans , <nl> fail : <nl> if ( leaf ) { <nl> btrfs_tree_unlock ( leaf ); <nl> + free_extent_buffer ( root -> commit_root ); <nl> free_extent_buffer ( leaf ); <nl> } <nl> kfree ( root );
static int mei_irq_thread_write_handler ( struct mei_io_list * cmpl_list , <nl> return 0 ; <nl> } <nl> * slots = mei_count_empty_write_slots ( dev ); <nl> + if (* slots <= 0 ) <nl> + return - EMSGSIZE ; <nl> + <nl> /* complete all waiting for write CB */ <nl> dev_dbg (& dev -> pdev -> dev , " complete all waiting for write cb .\ n "); <nl> 
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> loff_t file_size ; <nl> unsigned int num ; <nl> unsigned int offset ; <nl> - size_t total_len ; <nl> + size_t total_len = 0 ; <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req ))
proc_dodebug ( ctl_table * table , int write , struct file * file , <nl> left --, s ++; <nl> *( unsigned int *) table -> data = value ; <nl> /* Display the RPC tasks on writing to rpc_debug */ <nl> - if ( table -> ctl_name == CTL_RPCDEBUG ) { <nl> + if ( strcmp ( table -> procname , " rpc_debug ") == 0 ) <nl> rpc_show_tasks (); <nl> - } <nl> } else { <nl> if (! access_ok ( VERIFY_WRITE , buffer , left )) <nl> return - EFAULT ;
struct ieee80211_hw * ieee80211_alloc_hw ( size_t priv_data_len , <nl> local -> uapsd_max_sp_len = IEEE80211_DEFAULT_MAX_SP_LEN ; <nl>  <nl> INIT_LIST_HEAD (& local -> interfaces ); <nl> + <nl> + __hw_addr_init (& local -> mc_list ); <nl> + <nl> mutex_init (& local -> iflist_mtx ); <nl> mutex_init (& local -> scan_mtx ); <nl> 
static int do_ipv6_getsockopt ( struct sock * sk , int level , int optname , <nl> return - EINVAL ; <nl> if ( copy_from_user (& gsf , optval , GROUP_FILTER_SIZE ( 0 ))) <nl> return - EFAULT ; <nl> + if ( gsf . gf_group . ss_family != AF_INET6 ) <nl> + return - EADDRNOTAVAIL ; <nl> lock_sock ( sk ); <nl> err = ip6_mc_msfget ( sk , & gsf , <nl> ( struct group_filter __user *) optval , optlen );
static int snd_ctl_elem_add ( struct snd_ctl_file * file , <nl>  <nl> if ( card -> user_ctl_count >= MAX_USER_CONTROLS ) <nl> return - ENOMEM ; <nl> - if ( info -> count > 1024 ) <nl> + if ( info -> count < 1 ) <nl> return - EINVAL ; <nl> access = info -> access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE : <nl> ( info -> access & ( SNDRV_CTL_ELEM_ACCESS_READWRITE |
static void handle_callback ( struct gfs2_glock * gl , unsigned int state , int remot <nl> } <nl> return ; <nl> } <nl> - } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED ) { <nl> - gl -> gl_demote_state = state ; <nl> + } else if ( gl -> gl_demote_state != LM_ST_UNLOCKED && <nl> + gl -> gl_demote_state != state ) { <nl> + gl -> gl_demote_state = LM_ST_UNLOCKED ; <nl> } <nl> spin_unlock (& gl -> gl_spin ); <nl> }
static struct w1_master * w1_alloc_dev ( u32 id , int slave_count , int slave_ttl , <nl> memcpy (& dev -> dev , device , sizeof ( struct device )); <nl> dev_set_name (& dev -> dev , " w1_bus_master % u ", dev -> id ); <nl> snprintf ( dev -> name , sizeof ( dev -> name ), " w1_bus_master % u ", dev -> id ); <nl> + dev -> dev . init_name = dev -> name ; <nl>  <nl> dev -> driver = driver ; <nl> 
int intel_framebuffer_init ( struct drm_device * dev , <nl> switch ( mode_cmd -> bpp ) { <nl> case 8 : <nl> case 16 : <nl> + /* Only pre - ILK can handle 5 : 5 : 5 */ <nl> + if ( mode_cmd -> depth == 15 && ! HAS_PCH_SPLIT ( dev )) <nl> + return - EINVAL ; <nl> + break ; <nl> + <nl> case 24 : <nl> case 32 : <nl> break ;
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
__append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
int radeon_ring_alloc ( struct radeon_device * rdev , struct radeon_ring * ring , unsi <nl> if ( ndw < ring -> ring_free_dw ) { <nl> break ; <nl> } <nl> + mutex_unlock (& ring -> mutex ); <nl> r = radeon_fence_wait_next ( rdev , radeon_ring_index ( rdev , ring )); <nl> + mutex_lock (& ring -> mutex ); <nl> if ( r ) <nl> return r ; <nl> }
static int ixgbe_rcv_msg_from_vf ( struct ixgbe_adapter * adapter , u32 vf ) <nl> } <nl> break ; <nl> case IXGBE_VF_SET_MACVLAN : <nl> + if ( adapter -> vfinfo [ vf ]. pf_set_mac ) { <nl> + e_warn ( drv , " VF % d requested MACVLAN filter but is " <nl> + " administratively denied \ n ", vf ); <nl> + retval = - 1 ; <nl> + break ; <nl> + } <nl> index = ( msgbuf [ 0 ] & IXGBE_VT_MSGINFO_MASK ) >> <nl> IXGBE_VT_MSGINFO_SHIFT ; <nl> /*
static void mlx4_enable_msi_x ( struct mlx4_dev * dev ) <nl> nreq = err ; <nl> goto retry ; <nl> } <nl> - <nl> + kfree ( entries ); <nl> goto no_msi ; <nl> } <nl> 
__visible void prepare_exit_to_usermode ( struct pt_regs * regs ) <nl> READ_ONCE ( pt_regs_to_thread_info ( regs )-> flags ); <nl>  <nl> if (!( cached_flags & ( _TIF_SIGPENDING | _TIF_NOTIFY_RESUME | <nl> - _TIF_UPROBE | _TIF_NEED_RESCHED ))) <nl> + _TIF_UPROBE | _TIF_NEED_RESCHED | <nl> + _TIF_USER_RETURN_NOTIFY ))) <nl> break ; <nl>  <nl> /* We have work to do . */
again : <nl> case STAC_DELL_M4_3 : <nl> spec -> num_dmics = 1 ; <nl> spec -> num_smuxes = 0 ; <nl> - spec -> num_dmuxes = 0 ; <nl> + spec -> num_dmuxes = 1 ; <nl> break ; <nl> default : <nl> spec -> num_dmics = STAC92HD71BXX_NUM_DMICS ;
vpfe_get_pdata ( struct platform_device * pdev ) <nl> pdata -> asd [ i ] = devm_kzalloc (& pdev -> dev , <nl> sizeof ( struct v4l2_async_subdev ), <nl> GFP_KERNEL ); <nl> + if (! pdata -> asd [ i ]) { <nl> + of_node_put ( rem ); <nl> + pdata = NULL ; <nl> + goto done ; <nl> + } <nl> + <nl> pdata -> asd [ i ]-> match_type = V4L2_ASYNC_MATCH_OF ; <nl> pdata -> asd [ i ]-> match . of . node = rem ; <nl> of_node_put ( endpoint );
again : <nl> } <nl> spin_unlock (& fs_info -> reada_lock ); <nl>  <nl> + kfree ( multi ); <nl> return re ; <nl>  <nl> error : <nl> error : <nl> kref_put (& zone -> refcnt , reada_zone_release ); <nl> spin_unlock (& fs_info -> reada_lock ); <nl> } <nl> + kfree ( multi ); <nl> kfree ( re ); <nl> if ( looped ) <nl> goto again ;
static bool snd_soc_set_cache_val ( void * base , unsigned int idx , <nl> static unsigned int snd_soc_get_cache_val ( const void * base , unsigned int idx , <nl> unsigned int word_size ) <nl> { <nl> + if (! base ) <nl> + return - 1 ; <nl> + <nl> switch ( word_size ) { <nl> case 1 : { <nl> const u8 * cache = base ;
static int btrfs_show_devname ( struct seq_file * m , struct dentry * root ) <nl> list_for_each_entry ( dev , head , dev_list ) { <nl> if ( dev -> missing ) <nl> continue ; <nl> + if (! dev -> name ) <nl> + continue ; <nl> if (! first_dev || dev -> devid < first_dev -> devid ) <nl> first_dev = dev ; <nl> }
EXPORT_SYMBOL_GPL ( regmap_irq_chip_get_base ); <nl> */ <nl> int regmap_irq_get_virq ( struct regmap_irq_chip_data * data , int irq ) <nl> { <nl> + /* Handle holes in the IRQ list */ <nl> + if (! data -> chip -> irqs [ irq ]. mask ) <nl> + return - EINVAL ; <nl> + <nl> return irq_create_mapping ( data -> domain , irq ); <nl> } <nl> EXPORT_SYMBOL_GPL ( regmap_irq_get_virq );
static int drbd_accept ( const char ** what , struct socket * sock , struct socket ** n <nl> goto out ; <nl> } <nl> (* newsock )-> ops = sock -> ops ; <nl> + __module_get ((* newsock )-> ops -> owner ); <nl>  <nl> out : <nl> return err ;
static __inline__ int rt6_check_expired ( const struct rt6_info * rt ) <nl> static inline int rt6_need_strict ( struct in6_addr * daddr ) <nl> { <nl> return ( ipv6_addr_type ( daddr ) & <nl> - ( IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL )); <nl> + ( IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL | IPV6_ADDR_LOOPBACK )); <nl> } <nl>  <nl> /*
static int __init amijoy_init ( void ) <nl> int i , j ; <nl> int err ; <nl>  <nl> + if (! MACH_IS_AMIGA ) <nl> + return - ENODEV ; <nl> + <nl> for ( i = 0 ; i < 2 ; i ++) { <nl> if (! amijoy [ i ]) <nl> continue ;
static int hist_browser__run ( struct hist_browser * browser , const char * help ) <nl>  <nl> hists__browser_title ( browser -> hists , hbt , title , sizeof ( title )); <nl>  <nl> - if ( ui_browser__show (& browser -> b , title , help ) < 0 ) <nl> + if ( ui_browser__show (& browser -> b , title , "% s ", help ) < 0 ) <nl> return - 1 ; <nl>  <nl> while ( 1 ) {
e1000_configure_tx ( struct e1000_adapter * adapter ) <nl> } <nl>  <nl> /* Set the default values for the Tx Inter Packet Gap timer */ <nl> - <nl> - if ( hw -> media_type == e1000_media_type_fiber || <nl> - hw -> media_type == e1000_media_type_internal_serdes ) <nl> + if ( adapter -> hw . mac_type <= e1000_82547_rev_2 && <nl> + ( hw -> media_type == e1000_media_type_fiber || <nl> + hw -> media_type == e1000_media_type_internal_serdes )) <nl> tipg = DEFAULT_82543_TIPG_IPGT_FIBER ; <nl> else <nl> tipg = DEFAULT_82543_TIPG_IPGT_COPPER ;
pvpanic_panic_notify ( struct notifier_block * nb , unsigned long code , <nl>  <nl> static struct notifier_block pvpanic_panic_nb = { <nl> . notifier_call = pvpanic_panic_notify , <nl> + . priority = 1 , /* let this called before broken drm_fb_helper */ <nl> }; <nl>  <nl> 
int lustre_start_mgc ( struct super_block * sb ) <nl>  <nl> /* Random uuid for MGC allows easier reconnects */ <nl> OBD_ALLOC_PTR ( uuid ); <nl> + if (! uuid ) { <nl> + rc = - ENOMEM ; <nl> + goto out_free ; <nl> + } <nl> + <nl> ll_generate_random_uuid ( uuidc ); <nl> class_uuid_unparse ( uuidc , uuid ); <nl> 
cfq_should_preempt ( struct cfq_data * cfqd , struct cfq_queue * new_cfqq , <nl> if ( cfq_class_idle ( cfqq )) <nl> return true ; <nl>  <nl> - if ( cfqd -> serving_type == SYNC_NOIDLE_WORKLOAD <nl> - && new_cfqq -> service_tree == cfqq -> service_tree ) <nl> + if ( cfqd -> serving_type == SYNC_NOIDLE_WORKLOAD && <nl> + cfqq_type ( new_cfqq ) == SYNC_NOIDLE_WORKLOAD && <nl> + new_cfqq -> service_tree -> count == 1 ) <nl> return true ; <nl>  <nl> /*
static int i915_dma_cleanup ( struct drm_device * dev ) <nl> if ( dev -> irq_enabled ) <nl> drm_irq_uninstall ( dev ); <nl>  <nl> + mutex_lock (& dev -> struct_mutex ); <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> render_ring ); <nl> if ( HAS_BSD ( dev )) <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> bsd_ring ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl>  <nl> /* Clear the HWS virtual address at teardown */ <nl> if ( I915_NEED_GFX_HWS ( dev ))
static ssize_t dlpar_cpu_probe ( const char * buf , size_t count ) <nl> return - ENODEV ; <nl>  <nl> dn = dlpar_configure_connector ( cpu_to_be32 ( drc_index ), parent ); <nl> + of_node_put ( parent ); <nl> if (! dn ) <nl> return - EINVAL ; <nl>  <nl> - of_node_put ( parent ); <nl> - <nl> rc = dlpar_attach_node ( dn ); <nl> if ( rc ) { <nl> dlpar_release_drc ( drc_index );
void __init lpc32xx_serial_init ( void ) <nl>  <nl> /* This needs to be done after all UART clocks are setup */ <nl> __raw_writel ( clkmodes , LPC32XX_UARTCTL_CLKMODE ); <nl> - for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ) - 1 ; i ++) { <nl> + for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ); i ++) { <nl> /* Force a flush of the RX FIFOs to work around a HW bug */ <nl> puart = serial_std_platform_data [ i ]. mapbase ; <nl> __raw_writel ( 0xC1 , LPC32XX_UART_IIR_FCR ( puart ));
static void _rtl_usb_tx_preprocess ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> u8 tid = 0 ; <nl> u16 seq_number = 0 ; <nl>  <nl> + memset (& tcb_desc , 0 , sizeof ( struct rtl_tcb_desc )); <nl> if ( ieee80211_is_auth ( fc )) { <nl> RT_TRACE ( rtlpriv , COMP_SEND , DBG_DMESG , (" MAC80211_LINKING \ n ")); <nl> rtl_ips_nic_on ( hw );
static void posix_acl_fix_xattr_userns ( <nl> break ; <nl> case ACL_GROUP : <nl> gid = make_kgid ( from , le32_to_cpu ( entry -> e_id )); <nl> - entry -> e_id = cpu_to_le32 ( from_kuid ( to , uid )); <nl> + entry -> e_id = cpu_to_le32 ( from_kgid ( to , gid )); <nl> break ; <nl> default : <nl> break ;
static int ch7006_encoder_create_resources ( struct drm_encoder * encoder , <nl> drm_mode_create_tv_properties ( dev , NUM_TV_NORMS , ch7006_tv_norm_names ); <nl>  <nl> priv -> scale_property = drm_property_create_range ( dev , 0 , " scale ", 0 , 2 ); <nl> + if (! priv -> scale_property ) <nl> + return - ENOMEM ; <nl>  <nl> drm_object_attach_property (& connector -> base , conf -> tv_select_subconnector_property , <nl> priv -> select_subconnector );
void odm_DIGInit ( struct odm_dm_struct * pDM_Odm ) <nl> struct adapter * adapter = pDM_Odm -> Adapter ; <nl> struct rtw_dig * pDM_DigTable = & pDM_Odm -> DM_DigTable ; <nl>  <nl> - pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG ( IGI_A , pDM_Odm ), ODM_BIT ( IGI , pDM_Odm )); <nl> + pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG_IGI_A_11N , ODM_BIT_IGI_11N ); <nl> pDM_DigTable -> RssiLowThresh = DM_DIG_THRESH_LOW ; <nl> pDM_DigTable -> RssiHighThresh = DM_DIG_THRESH_HIGH ; <nl> pDM_DigTable -> FALowThresh = DM_false_ALARM_THRESH_LOW ;
service_in_request ( struct musb * musb , const struct usb_ctrlrequest * ctrlrequest ) <nl> static void musb_g_ep0_giveback ( struct musb * musb , struct usb_request * req ) <nl> { <nl> musb_g_giveback (& musb -> endpoints [ 0 ]. ep_in , req , 0 ); <nl> - musb -> ep0_state = MUSB_EP0_STAGE_SETUP ; <nl> } <nl>  <nl> /*
static const struct usb_device_id usbtouch_devices [] = { <nl>  <nl> # ifdef CONFIG_TOUCHSCREEN_USB_ITM <nl> { USB_DEVICE ( 0x0403 , 0xf9e9 ), . driver_info = DEVTYPE_ITM }, <nl> + { USB_DEVICE ( 0x16e3 , 0xf9e9 ), . driver_info = DEVTYPE_ITM }, <nl> # endif <nl>  <nl> # ifdef CONFIG_TOUCHSCREEN_USB_ETURBO
static int parse_addr ( const struct nf_conn * ct , const char * cp , <nl> int family = ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . l3num ; <nl> int ret = 0 ; <nl>  <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> switch ( family ) { <nl> case AF_INET : <nl> ret = in4_pton ( cp , limit - cp , ( u8 *)& addr -> ip , - 1 , & end );
static void __devinit uvesafb_init_info ( struct fb_info * info , <nl> } <nl>  <nl> info -> flags = FBINFO_FLAG_DEFAULT | <nl> - ( par -> ypan ) ? FBINFO_HWACCEL_YPAN : 0 ; <nl> + ( par -> ypan ? FBINFO_HWACCEL_YPAN : 0 ); <nl>  <nl> if (! par -> ypan ) <nl> info -> fbops -> fb_pan_display = NULL ;
static void * vexpress_sysreg_config_func_get ( struct device * dev , <nl> struct device_node * node ) <nl> { <nl> struct vexpress_sysreg_config_func * config_func ; <nl> - u32 site ; <nl> + u32 site = 0 ; <nl> u32 position = 0 ; <nl> u32 dcc = 0 ; <nl> u32 func_device [ 2 ];
static void batadv_hash_init ( struct batadv_hashtable * hash ) <nl> { <nl> uint32_t i ; <nl>  <nl> - for ( i = 0 ; i < hash -> size ; i ++) { <nl> + for ( i = 0 ; i < hash -> size ; i ++) { <nl> INIT_HLIST_HEAD (& hash -> table [ i ]); <nl> spin_lock_init (& hash -> list_locks [ i ]); <nl> }
out_dio : <nl> * async dio is going to do it in the future or an end_io after an <nl> * error has already done it . <nl> */ <nl> - if ( ret == - EIOCBQUEUED || ! ocfs2_iocb_is_rw_locked ( iocb )) { <nl> + if (( ret == - EIOCBQUEUED ) || (! ocfs2_iocb_is_rw_locked ( iocb ))) { <nl> rw_level = - 1 ; <nl> have_alloc_sem = 0 ; <nl> }
read_again : <nl> skb = xgbe_create_skb ( pdata , rdata , & put_len ); <nl> if (! skb ) { <nl> error = 1 ; <nl> - goto read_again ; <nl> + goto skip_data ; <nl> } <nl> } <nl>  <nl> read_again : <nl> } <nl> } <nl>  <nl> + skip_data : <nl> if ( incomplete || context_next ) <nl> goto read_again ; <nl>  <nl> - /* Stray Context Descriptor ? */ <nl> if (! skb ) <nl> goto next_packet ; <nl> 
void qlcnic_set_multi ( struct net_device * netdev ) <nl> netdev_for_each_mc_addr ( ha , netdev ) { <nl> cur = kzalloc ( sizeof ( struct qlcnic_mac_list_s ), <nl> GFP_ATOMIC ); <nl> + if ( cur == NULL ) <nl> + break ; <nl> memcpy ( cur -> mac_addr , <nl> ha -> addr , ETH_ALEN ); <nl> list_add_tail (& cur -> list , & adapter -> vf_mc_list );
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
int __inet_inherit_port ( const struct sock * sk , struct sock * child ) <nl>  <nl> spin_lock (& head -> lock ); <nl> tb = inet_csk ( sk )-> icsk_bind_hash ; <nl> + if ( unlikely (! tb )) { <nl> + spin_unlock (& head -> lock ); <nl> + return - ENOENT ; <nl> + } <nl> if ( tb -> port != port ) { <nl> /* NOTE : using tproxy and redirecting skbs to a proxy <nl> * on a different listener port breaks the assumption
int pci_vpd_truncate ( struct pci_dev * dev , size_t size ) <nl> return - EINVAL ; <nl>  <nl> dev -> vpd -> len = size ; <nl> - dev -> vpd -> attr -> size = size ; <nl> + if ( dev -> vpd -> attr ) <nl> + dev -> vpd -> attr -> size = size ; <nl>  <nl> return 0 ; <nl> }
static inline unsigned int elapsed_jiffies_msecs ( unsigned long start ) <nl> if ( end >= start ) <nl> return jiffies_to_msecs ( end - start ); <nl>  <nl> - return jiffies_to_msecs ( end + ( MAX_JIFFY_OFFSET - start ) + 1 ); <nl> + return jiffies_to_msecs ( end + ( ULONG_MAX - start ) + 1 ); <nl> } <nl>  <nl> void
struct clk * imx_clk_fixup_mux ( const char * name , void __iomem * reg , <nl> init . ops = & clk_fixup_mux_ops ; <nl> init . parent_names = parents ; <nl> init . num_parents = num_parents ; <nl> + init . flags = 0 ; <nl>  <nl> fixup_mux -> mux . reg = reg ; <nl> fixup_mux -> mux . shift = shift ;
void nfs_inode_return_delegation_noreclaim ( struct inode * inode ) <nl>  <nl> delegation = nfs_inode_detach_delegation ( inode ); <nl> if ( delegation != NULL ) <nl> - nfs_do_return_delegation ( inode , delegation , 0 ); <nl> + nfs_do_return_delegation ( inode , delegation , 1 ); <nl> } <nl>  <nl> /**
static int uas_queuecommand_lck ( struct scsi_cmnd * cmnd , <nl> return SCSI_MLQUEUE_DEVICE_BUSY ; <nl> } <nl>  <nl> + memset ( cmdinfo , 0 , sizeof (* cmdinfo )); <nl> + <nl> if ( blk_rq_tagged ( cmnd -> request )) { <nl> cmdinfo -> stream = cmnd -> request -> tag + 2 ; <nl> } else {
luan_setup_hoses ( void ) <nl>  <nl> /* Allocate hoses for PCIX1 and PCIX2 */ <nl> hose1 = pcibios_alloc_controller (); <nl> + if (! hose1 ) <nl> + return ; <nl> + <nl> hose2 = pcibios_alloc_controller (); <nl> - if (! hose1 || ! hose2 ) <nl> + if (! hose2 ) { <nl> + pcibios_free_controller ( hose1 ); <nl> return ; <nl> + } <nl>  <nl> /* Setup PCIX1 */ <nl> hose1 -> first_busno = 0 ;
static struct urb * usbtv_setup_iso_transfer ( struct usbtv * usbtv ) <nl> ip -> transfer_flags = URB_ISO_ASAP ; <nl> ip -> transfer_buffer = kzalloc ( size * USBTV_ISOC_PACKETS , <nl> GFP_KERNEL ); <nl> + if (! ip -> transfer_buffer ) { <nl> + usb_free_urb ( ip ); <nl> + return NULL ; <nl> + } <nl> ip -> complete = usbtv_iso_cb ; <nl> ip -> number_of_packets = USBTV_ISOC_PACKETS ; <nl> ip -> transfer_buffer_length = size * USBTV_ISOC_PACKETS ;
nouveau_gem_object_open ( struct drm_gem_object * gem , struct drm_file * file_priv ) <nl> } <nl>  <nl> ret = pm_runtime_get_sync ( dev ); <nl> - if ( ret < 0 && ret != - EACCES ) <nl> + if ( ret < 0 && ret != - EACCES ) { <nl> + kfree ( vma ); <nl> goto out ; <nl> + } <nl>  <nl> ret = nouveau_bo_vma_add ( nvbo , cli -> vm , vma ); <nl> if ( ret )
static int tpm_binary_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 5th : delimiter */ <nl> seq_putc ( m , '\ 0 '); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl>  <nl> static int tpm_ascii_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 4th : eventname <= max + \' 0 ' delimiter */ <nl> seq_printf ( m , " % s \ n ", eventname ); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl> 
static struct drm_driver driver = { <nl> . minor = DRIVER_MINOR , <nl> . patchlevel = DRIVER_PATCHLEVEL , <nl>  <nl> - . gem_free_object = ast_gem_free_object , <nl> + . gem_free_object_unlocked = ast_gem_free_object , <nl> . dumb_create = ast_dumb_create , <nl> . dumb_map_offset = ast_dumb_mmap_offset , <nl> . dumb_destroy = drm_gem_dumb_destroy ,
int drm_fb_helper_init ( struct drm_device * dev , <nl> struct drm_crtc * crtc ; <nl> int i ; <nl>  <nl> + if (! max_conn_count ) <nl> + return - EINVAL ; <nl> + <nl> fb_helper -> dev = dev ; <nl>  <nl> INIT_LIST_HEAD (& fb_helper -> kernel_fb_list );
int dlpar_detach_node ( struct device_node * dn ) <nl> if ( rc ) <nl> return rc ; <nl>  <nl> - of_node_put ( dn ); /* Must decrement the refcount */ <nl> return 0 ; <nl> } <nl> 
static int atmel_serial_remove ( struct platform_device * pdev ) <nl> struct atmel_uart_port * atmel_port = to_atmel_uart_port ( port ); <nl> int ret = 0 ; <nl>  <nl> + tasklet_kill (& atmel_port -> tasklet ); <nl> + <nl> device_init_wakeup (& pdev -> dev , 0 ); <nl>  <nl> ret = uart_remove_one_port (& atmel_uart , port ); <nl>  <nl> - tasklet_kill (& atmel_port -> tasklet ); <nl> kfree ( atmel_port -> rx_ring . buf ); <nl>  <nl> /* " port " is allocated statically , so we shouldn ' t free it */
static inline int open_arg ( int flags , int mask ) <nl>  <nl> static int audit_match_perm ( struct audit_context * ctx , int mask ) <nl> { <nl> + unsigned n ; <nl> if ( unlikely (! ctx )) <nl> return 0 ; <nl>  <nl> - unsigned n = ctx -> major ; <nl> + n = ctx -> major ; <nl> switch ( audit_classify_syscall ( ctx -> arch , n )) { <nl> case 0 : /* native */ <nl> if (( mask & AUDIT_PERM_WRITE ) &&
static void _gb_power_supplies_release ( struct gb_power_supplies * supplies ) <nl> { <nl> int i ; <nl>  <nl> + if (! supplies -> supply ) <nl> + return ; <nl> + <nl> mutex_lock (& supplies -> supplies_lock ); <nl> for ( i = 0 ; i < supplies -> supplies_count ; i ++) <nl> _gb_power_supply_release (& supplies -> supply [ i ]);
static int bnx2x_cnic_handle_cfc_del ( struct bnx2x * bp , u32 cid , <nl> union event_ring_elem * elem ) <nl> { <nl> if (! bp -> cnic_eth_dev . starting_cid || <nl> - cid < bp -> cnic_eth_dev . starting_cid ) <nl> + ( cid < bp -> cnic_eth_dev . starting_cid && <nl> + cid != bp -> cnic_eth_dev . iscsi_l2_cid )) <nl> return 1 ; <nl>  <nl> DP ( BNX2X_MSG_SP , " got delete ramrod for CNIC CID % d \ n ", cid );
static enum dlm_status dlmunlock_common ( struct dlm_ctxt * dlm , <nl> else <nl> status = dlm_get_unlock_actions ( dlm , res , lock , lksb , & actions ); <nl>  <nl> - if ( status != DLM_NORMAL && status != DLM_CANCELGRANT ) <nl> + if ( status != DLM_NORMAL && ( status != DLM_CANCELGRANT || ! master_node )) <nl> goto leave ; <nl>  <nl> /* By now this has been masked out of cancel requests . */
static void ati_remote_input_report ( struct urb * urb , struct pt_regs * regs ) <nl> input_regs ( dev , regs ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 1 ); <nl> + input_sync ( dev ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 0 ); <nl> input_sync ( dev );
static int vb2_internal_streamon ( struct vb2_queue * q , enum v4l2_buf_type type ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if (! q -> num_buffers ) { <nl> + dprintk ( 1 , " streamon : no buffers have been allocated \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* <nl> * If any buffers were queued before streamon , <nl> * we can now pass them to driver for processing .
static int __devinit plat_nand_probe ( struct platform_device * pdev ) <nl> struct resource * res ; <nl> int err = 0 ; <nl>  <nl> + if ( pdata -> chip . nr_chips < 1 ) { <nl> + dev_err (& pdev -> dev , " invalid number of chips specified \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) <nl> return - ENXIO ;
static void isp1760_hub_descriptor ( struct isp1760_hcd * priv , <nl> int ports = HCS_N_PORTS ( priv -> hcs_params ); <nl> u16 temp ; <nl>  <nl> - desc -> bDescriptorType = 0x29 ; <nl> + desc -> bDescriptorType = USB_DT_HUB ; <nl> /* priv 1 . 0 , 2 . 3 . 9 says 20ms max */ <nl> desc -> bPwrOn2PwrGood = 10 ; <nl> desc -> bHubContrCurrent = 0 ;
static struct task_struct * dup_task_struct ( struct task_struct * orig ) <nl>  <nl> /* One for us , one for whoever does the " release_task ()" ( usually parent ) */ <nl> atomic_set (& tsk -> usage , 2 ); <nl> + atomic_set (& tsk -> fs_excl , 0 ); <nl> return tsk ; <nl> } <nl> 
struct page * kmap_to_page ( void * vaddr ) <nl>  <nl> return virt_to_page ( addr ); <nl> } <nl> + EXPORT_SYMBOL ( kmap_to_page ); <nl>  <nl> static void flush_all_zero_pkmaps ( void ) <nl> {
void rtl92e_set_key ( struct net_device * dev , u8 EntryNo , u8 KeyIndex , <nl> } <nl> } <nl> priv -> rtllib -> is_set_key = true ; <nl> - if ( EntryNo >= TOTAL_CAM_ENTRY ) <nl> + if ( EntryNo >= TOTAL_CAM_ENTRY ) { <nl> netdev_info ( dev , "% s (): Invalid CAM entry \ n ", __func__ ); <nl> + return ; <nl> + } <nl>  <nl> RT_TRACE ( COMP_SEC , <nl> "====> to rtl92e_set_key (), dev :% p , EntryNo :% d , KeyIndex :% d , KeyType :% d , MacAddr % pM \ n ",
static int siu_dai_prepare ( struct snd_pcm_substream * substream , <nl> ret = siu_dai_spbstart ( port_info ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl> + } else { <nl> + ret = 0 ; <nl> } <nl>  <nl> port_info -> play_cap |= self ;
enum { <nl> # define DM_DEV_SET_GEOMETRY _IOWR ( DM_IOCTL , DM_DEV_SET_GEOMETRY_CMD , struct dm_ioctl ) <nl>  <nl> # define DM_VERSION_MAJOR 4 <nl> -# define DM_VERSION_MINOR 31 <nl> +# define DM_VERSION_MINOR 32 <nl> # define DM_VERSION_PATCHLEVEL 0 <nl> -# define DM_VERSION_EXTRA "- ioctl ( 2015 - 3 - 12 )" <nl> +# define DM_VERSION_EXTRA "- ioctl ( 2015 - 6 - 26 )" <nl>  <nl> /* Status bits */ <nl> # define DM_READONLY_FLAG ( 1 << 0 ) /* In / Out */
static void bit_putcs ( struct vc_data * vc , struct fb_info * info , <nl> image . depth = 1 ; <nl>  <nl> if ( attribute ) { <nl> - buf = kmalloc ( cellsize , GFP_KERNEL ); <nl> + buf = kmalloc ( cellsize , GFP_ATOMIC ); <nl> if (! buf ) <nl> return ; <nl> }
static int qat_alg_sgl_to_bufl ( struct qat_crypto_instance * inst , <nl> goto err ; <nl>  <nl> for_each_sg ( assoc , sg , assoc_n , i ) { <nl> + if (! sg -> length ) <nl> + continue ; <nl> bufl -> bufers [ bufs ]. addr = dma_map_single ( dev , <nl> sg_virt ( sg ), <nl> sg -> length ,
u32 bond_xmit_hash ( struct bonding * bond , struct sk_buff * skb ) <nl> struct flow_keys flow ; <nl> u32 hash ; <nl>  <nl> + if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_ENCAP34 && <nl> + skb -> l4_hash ) <nl> + return skb -> hash ; <nl> + <nl> if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_LAYER2 || <nl> ! bond_flow_dissect ( bond , skb , & flow )) <nl> return bond_eth_hash ( skb );
struct drm_gem_object * msm_gem_new ( struct drm_device * dev , <nl>  <nl> fail : <nl> if ( obj ) <nl> - drm_gem_object_unreference_unlocked ( obj ); <nl> + drm_gem_object_unreference ( obj ); <nl>  <nl> return ERR_PTR ( ret ); <nl> }
nfqnl_build_packet_message ( struct nfqnl_instance * queue , <nl> struct net_device * indev ; <nl> struct net_device * outdev ; <nl>  <nl> - size = NLMSG_ALIGN ( sizeof ( struct nfgenmsg )) <nl> + size = NLMSG_SPACE ( sizeof ( struct nfgenmsg )) <nl> + nla_total_size ( sizeof ( struct nfqnl_msg_packet_hdr )) <nl> + nla_total_size ( sizeof ( u_int32_t )) /* ifindex */ <nl> + nla_total_size ( sizeof ( u_int32_t )) /* ifindex */
static const struct x86_cpu_id rapl_ids [] = { <nl> RAPL_CPU ( 0x45 , rapl_defaults_core ),/* Haswell ULT */ <nl> RAPL_CPU ( 0x4C , rapl_defaults_atom ),/* Braswell */ <nl> RAPL_CPU ( 0x4A , rapl_defaults_atom ),/* Tangier */ <nl> + RAPL_CPU ( 0x56 , rapl_defaults_core ),/* Future Xeon */ <nl> RAPL_CPU ( 0x5A , rapl_defaults_atom ),/* Annidale */ <nl> {} <nl> };
omap_i2c_probe ( struct platform_device * pdev ) <nl> struct omap_i2c_dev * dev ; <nl> struct i2c_adapter * adap ; <nl> struct resource * mem , * irq , * ioarea ; <nl> - struct omap_i2c_bus_platform_data * pdata = pdev -> dev . platform_data ; <nl> + const struct omap_i2c_bus_platform_data * pdata = <nl> + pdev -> dev . platform_data ; <nl> struct device_node * node = pdev -> dev . of_node ; <nl> const struct of_device_id * match ; <nl> irq_handler_t isr ;
static int __init musb_probe ( struct platform_device * pdev ) <nl> void __iomem * base ; <nl>  <nl> iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - if (! iomem || irq == 0 ) <nl> + if (! iomem || irq <= 0 ) <nl> return - ENODEV ; <nl>  <nl> base = ioremap ( iomem -> start , resource_size ( iomem ));
static s16 * read_rds_samples ( struct cx88_core * core , u32 * N ) <nl> current_address , <nl> current_address - srch -> fifo_start , sample_count , <nl> cx_read ( MO_AUD_INTSTAT )); <nl> - <nl> - samples = kmalloc ( sizeof ( s16 )* sample_count , GFP_KERNEL ); <nl> + samples = kmalloc_array ( sample_count , sizeof (* samples ), GFP_KERNEL ); <nl> if (! samples ) <nl> return NULL ; <nl> 
static int synaptics_board_id ( struct psmouse * psmouse ) <nl> struct synaptics_data * priv = psmouse -> private ; <nl> unsigned char bid [ 3 ]; <nl>  <nl> + /* firmwares prior 7 . 5 have no board_id encoded */ <nl> + if ( SYN_ID_FULL ( priv -> identity ) < 0x705 ) <nl> + return 0 ; <nl> + <nl> if ( synaptics_send_cmd ( psmouse , SYN_QUE_MODES , bid )) <nl> return - 1 ; <nl> priv -> board_id = (( bid [ 0 ] & 0xfc ) << 6 ) | bid [ 1 ];
static const char * intel_pt_err_msgs [] = { <nl>  <nl> int intel_pt__strerror ( int code , char * buf , size_t buflen ) <nl> { <nl> - if ( code < 1 || code > INTEL_PT_ERR_MAX ) <nl> + if ( code < 1 || code >= INTEL_PT_ERR_MAX ) <nl> code = INTEL_PT_ERR_UNK ; <nl> strlcpy ( buf , intel_pt_err_msgs [ code ], buflen ); <nl> return 0 ;
retry : <nl> return 0 ; <nl>  <nl> n_group = ext4_get_group_number ( sb , n_blocks_count - 1 ); <nl> + if ( n_group > ( 0xFFFFFFFFUL / EXT4_INODES_PER_GROUP ( sb ))) { <nl> + ext4_warning ( sb , " resize would cause inodes_count overflow "); <nl> + return - EINVAL ; <nl> + } <nl> ext4_get_group_no_and_offset ( sb , o_blocks_count - 1 , & o_group , & offset ); <nl>  <nl> n_desc_blocks = num_desc_blocks ( sb , n_group + 1 );
int kvmppc_handle_exit ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> } <nl> break ; <nl>  <nl> + case BOOKE_INTERRUPT_FP_UNAVAIL : <nl> + kvmppc_queue_exception ( vcpu , exit_nr ); <nl> + r = RESUME_GUEST ; <nl> + break ; <nl> + <nl> case BOOKE_INTERRUPT_DATA_STORAGE : <nl> vcpu -> arch . dear = vcpu -> arch . fault_dear ; <nl> vcpu -> arch . esr = vcpu -> arch . fault_esr ;
static int snd_pcm_oss_open_file ( struct file * file , <nl> for ( idx = 0 ; idx < 2 ; idx ++) { <nl> if ( setup [ idx ]. disable ) <nl> continue ; <nl> + if (! pcm -> streams [ idx ]. substream_count ) <nl> + continue ; /* no matching substream */ <nl> if ( idx == SNDRV_PCM_STREAM_PLAYBACK ) { <nl> if (! ( f_mode & FMODE_WRITE )) <nl> continue ;
static int ff_layout_async_handle_error_v3 ( struct rpc_task * task , <nl> return - NFS4ERR_RESET_TO_PNFS ; <nl> out_retry : <nl> task -> tk_status = 0 ; <nl> - rpc_restart_call ( task ); <nl> + rpc_restart_call_prepare ( task ); <nl> rpc_delay ( task , NFS_JUKEBOX_RETRY_TIME ); <nl> return - EAGAIN ; <nl> }
void btrfs_apply_pending_changes ( struct btrfs_fs_info * fs_info ) <nl> unsigned long prev ; <nl> unsigned long bit ; <nl>  <nl> - prev = cmpxchg (& fs_info -> pending_changes , 0 , 0 ); <nl> + prev = xchg (& fs_info -> pending_changes , 0 ); <nl> if (! prev ) <nl> return ; <nl> 
static ssize_t stm_char_write ( struct file * file , const char __user * buf , <nl> char * kbuf ; <nl> int err ; <nl>  <nl> + if ( count + 1 > PAGE_SIZE ) <nl> + count = PAGE_SIZE - 1 ; <nl> + <nl> /* <nl> * if no m / c have been assigned to this writer up to this <nl> * point , use " default " policy entry
void ath9k_btcoex_stop_gen_timer ( struct ath_softc * sc ) <nl> { <nl> struct ath_btcoex * btcoex = & sc -> btcoex ; <nl>  <nl> - ath9k_gen_timer_stop ( sc -> sc_ah , btcoex -> no_stomp_timer ); <nl> + if ( btcoex -> hw_timer_enabled ) <nl> + ath9k_gen_timer_stop ( sc -> sc_ah , btcoex -> no_stomp_timer ); <nl> } <nl>  <nl> u16 ath9k_btcoex_aggr_limit ( struct ath_softc * sc , u32 max_4ms_framelen )
int hfsplus_find_cat ( struct super_block * sb , u32 cnid , <nl> return - EIO ; <nl> } <nl>  <nl> + if ( be16_to_cpu ( tmp . thread . nodeName . length ) > 255 ) { <nl> + printk ( KERN_ERR " hfs : catalog name length corrupted \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> hfsplus_cat_build_key_uni ( fd -> search_key , be32_to_cpu ( tmp . thread . parentID ), <nl> & tmp . thread . nodeName ); <nl> return hfs_brec_find ( fd );
int dm_suspend ( struct mapped_device * md , unsigned suspend_flags ) <nl> * requests are being added to md -> deferred list . <nl> */ <nl>  <nl> - dm_table_postsuspend_targets ( map ); <nl> - <nl> set_bit ( DMF_SUSPENDED , & md -> flags ); <nl>  <nl> + dm_table_postsuspend_targets ( map ); <nl> + <nl> out : <nl> dm_table_put ( map ); <nl> 
static int __init scx200_create_isa ( const char * text , unsigned long base , <nl> if ( iface == NULL ) <nl> return - ENOMEM ; <nl>  <nl> - if ( request_region ( base , 8 , iface -> adapter . name ) == 0 ) { <nl> + if (! request_region ( base , 8 , iface -> adapter . name )) { <nl> printk ( KERN_ERR NAME ": can ' t allocate io 0x % lx - 0x % lx \ n ", <nl> base , base + 8 - 1 ); <nl> rc = - EBUSY ;
static void line6_destruct ( struct snd_card * card ) <nl> /* Free buffer memory first . We cannot depend on the existence of private <nl> * data from the ( podhd ) module , it may be gone already during this call <nl> */ <nl> - if ( line6 -> buffer_message ) <nl> - kfree ( line6 -> buffer_message ); <nl> + kfree ( line6 -> buffer_message ); <nl>  <nl> kfree ( line6 -> buffer_listen ); <nl> 
static int wl12xx_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> struct wl12xx * wl = hw -> priv ; <nl> int ret ; <nl>  <nl> + mutex_lock (& wl -> mutex ); <nl> + <nl> ret = wl12xx_acx_rts_threshold ( wl , ( u16 ) value ); <nl>  <nl> if ( ret < 0 ) <nl> wl12xx_warning (" wl12xx_op_set_rts_threshold failed : % d ", ret ); <nl>  <nl> + mutex_unlock (& wl -> mutex ); <nl> + <nl> return ret ; <nl> } <nl> 
drm_do_get_edid ( struct drm_connector * connector , struct i2c_adapter * adapter ) <nl> break ; <nl> } <nl> } <nl> - if ( i == 4 ) <nl> + <nl> + if ( i == 4 && print_bad_edid ) { <nl> dev_warn ( connector -> dev -> dev , <nl> "% s : Ignoring invalid EDID block % d .\ n ", <nl> drm_get_connector_name ( connector ), j ); <nl> + <nl> + connector -> bad_edid_counter ++; <nl> + } <nl> } <nl>  <nl> if ( valid_extensions != block [ 0x7e ]) {
struct vp_config_entry_24xx { <nl> uint16_t id ; <nl> uint16_t reserved_4 ; <nl> uint16_t hopct ; <nl> - uint8_t reserved_5 ; <nl> + uint8_t reserved_5 [ 2 ]; <nl> }; <nl>  <nl> # define VP_RPT_ID_IOCB_TYPE 0x32 /* Report ID Acquisition entry . */
static ktime_t tick_nohz_stop_sched_tick ( struct tick_sched * ts , <nl> */ <nl> if ( delta == 0 ) { <nl> tick_nohz_restart ( ts , now ); <nl> + /* <nl> + * Make sure next tick stop doesn ' t get fooled by past <nl> + * clock deadline <nl> + */ <nl> + ts -> next_tick = 0 ; <nl> goto out ; <nl> } <nl> }
struct fimc_fmt * find_format ( struct v4l2_format * f , unsigned int mask ) <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( fimc_formats ); ++ i ) { <nl> fmt = & fimc_formats [ i ]; <nl> - if ( fmt -> fourcc == f -> fmt . pix . pixelformat && <nl> + if ( fmt -> fourcc == f -> fmt . pix_mp . pixelformat && <nl> ( fmt -> flags & mask )) <nl> break ; <nl> }
i915_gem_wait_ioctl ( struct drm_device * dev , void * data , struct drm_file * file ) <nl> u32 seqno = 0 ; <nl> int ret = 0 ; <nl>  <nl> + if ( args -> flags != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = i915_mutex_lock_interruptible ( dev ); <nl> if ( ret ) <nl> return ret ;
static int br_afspec ( struct net_bridge * br , <nl> if ( nla_len ( attr ) != sizeof ( struct bridge_vlan_info )) <nl> return - EINVAL ; <nl> vinfo = nla_data ( attr ); <nl> + if (! vinfo -> vid || vinfo -> vid >= VLAN_VID_MASK ) <nl> + return - EINVAL ; <nl> if ( vinfo -> flags & BRIDGE_VLAN_INFO_RANGE_BEGIN ) { <nl> if ( vinfo_start ) <nl> return - EINVAL ;
static int enum_fmt ( void * priv , struct v4l2_fmtdesc * f , <nl> fmt = & formats [ i ]; <nl> strlcpy ( f -> description , fmt -> name , sizeof ( f -> description )); <nl> f -> pixelformat = fmt -> fourcc ; <nl> + if (! coda_format_is_yuv ( fmt -> fourcc )) <nl> + f -> flags |= V4L2_FMT_FLAG_COMPRESSED ; <nl> return 0 ; <nl> } <nl> 
static int kvm_vcpu_ioctl_x86_set_vcpu_events ( struct kvm_vcpu * vcpu , <nl> | KVM_VCPUEVENT_VALID_SMM )) <nl> return - EINVAL ; <nl>  <nl> + if ( events -> exception . injected && <nl> + ( events -> exception . nr > 31 || events -> exception . nr == NMI_VECTOR )) <nl> + return - EINVAL ; <nl> + <nl> process_nmi ( vcpu ); <nl> vcpu -> arch . exception . pending = events -> exception . injected ; <nl> vcpu -> arch . exception . nr = events -> exception . nr ;
static struct sock * tcp_fastopen_create_child ( struct sock * sk , <nl> tcp_fastopen_add_skb ( child , skb ); <nl>  <nl> tcp_rsk ( req )-> rcv_nxt = tp -> rcv_nxt ; <nl> + tp -> rcv_wup = tp -> rcv_nxt ; <nl> /* tcp_conn_request () is sending the SYNACK , <nl> * and queues the child into listener accept queue . <nl> */
int tulip_refill_rx ( struct net_device * dev ) <nl>  <nl> mapping = pci_map_single ( tp -> pdev , skb -> data , PKT_BUF_SZ , <nl> PCI_DMA_FROMDEVICE ); <nl> + if ( dma_mapping_error (& tp -> pdev -> dev , mapping )) { <nl> + dev_kfree_skb ( skb ); <nl> + tp -> rx_buffers [ entry ]. skb = NULL ; <nl> + break ; <nl> + } <nl> + <nl> tp -> rx_buffers [ entry ]. mapping = mapping ; <nl>  <nl> tp -> rx_ring [ entry ]. buffer1 = cpu_to_le32 ( mapping );
static int lzo_compress_pages ( struct list_head * ws , <nl> } <nl>  <nl> /* we ' re making it bigger , give up */ <nl> - if ( tot_in > 8192 && tot_in < tot_out ) <nl> + if ( tot_in > 8192 && tot_in < tot_out ) { <nl> + ret = - 1 ; <nl> goto out ; <nl> + } <nl>  <nl> /* we ' re all done */ <nl> if ( tot_in >= len )
void eth_header_cache_update ( struct hh_cache * hh , struct net_device * dev , <nl> static int eth_mac_addr ( struct net_device * dev , void * p ) <nl> { <nl> struct sockaddr * addr = p ; <nl> + <nl> if ( netif_running ( dev )) <nl> return - EBUSY ; <nl> + if (! is_valid_ether_addr ( addr -> sa_data )) <nl> + return - EADDRNOTAVAIL ; <nl> memcpy ( dev -> dev_addr , addr -> sa_data , dev -> addr_len ); <nl> return 0 ; <nl> }
enum acer_wmi_event_ids { <nl> static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x01 , { KEY_WLAN } }, /* WiFi */ <nl> { KE_KEY , 0x03 , { KEY_WLAN } }, /* WiFi */ <nl> + { KE_KEY , 0x04 , { KEY_WLAN } }, /* WiFi */ <nl> { KE_KEY , 0x12 , { KEY_BLUETOOTH } }, /* BT */ <nl> { KE_KEY , 0x21 , { KEY_PROG1 } }, /* Backup */ <nl> { KE_KEY , 0x22 , { KEY_PROG2 } }, /* Arcade */
static int knav_setup_queue_range ( struct knav_device * kdev , <nl>  <nl> range -> num_irqs ++; <nl>  <nl> - if ( oirq . args_count == 3 ) <nl> + if ( IS_ENABLED ( CONFIG_SMP ) && oirq . args_count == 3 ) <nl> range -> irqs [ i ]. cpu_map = <nl> ( oirq . args [ 2 ] & 0x0000ff00 ) >> 8 ; <nl> }
static int find_first_block_group ( struct btrfs_root * root , <nl> } else { <nl> ret = 0 ; <nl> } <nl> + free_extent_map ( em ); <nl> goto out ; <nl> } <nl> path -> slots [ 0 ]++;
static int skl_tplg_mixer_dapm_post_pmd_event ( struct snd_soc_dapm_widget * w , <nl> int ret = 0 ; <nl>  <nl> skl_tplg_free_pipe_mcps ( skl , mconfig ); <nl> + skl_tplg_free_pipe_mem ( skl , mconfig ); <nl>  <nl> list_for_each_entry ( w_module , & s_pipe -> w_list , node ) { <nl> dst_module = w_module -> w -> priv ; <nl> static int skl_tplg_mixer_dapm_post_pmd_event ( struct snd_soc_dapm_widget * w , <nl> } <nl>  <nl> ret = skl_delete_pipe ( ctx , mconfig -> pipe ); <nl> - skl_tplg_free_pipe_mem ( skl , mconfig ); <nl>  <nl> return ret ; <nl> }
int __init mxc_register_gpios ( void ) <nl> # ifdef CONFIG_MACH_MX21 <nl> static struct resource mx21_usbhc_resources [] = { <nl> { <nl> - . start = MX21_BASE_ADDR , <nl> - . end = MX21_BASE_ADDR + 0x1FFF , <nl> + . start = MX21_USBOTG_BASE_ADDR , <nl> + . end = MX21_USBOTG_BASE_ADDR + SZ_8K - 1 , <nl> . flags = IORESOURCE_MEM , <nl> }, <nl> {
static struct page * alloc_misplaced_dst_page ( struct page * page , <nl> __GFP_NOMEMALLOC | __GFP_NORETRY | <nl> __GFP_NOWARN ) & <nl> ~ GFP_IOFS , 0 ); <nl> + if ( newpage ) <nl> + page_xchg_last_nid ( newpage , page_last_nid ( page )); <nl> + <nl> return newpage ; <nl> } <nl> 
# include < linux / list . h > <nl> # include < linux / slab . h > <nl> # include < linux / export . h > <nl> +# include < linux / vmalloc . h > <nl> # include < net / net_namespace . h > <nl> # include < net / ip . h > <nl> # include < net / protocol . h >
static struct packet_type ip_packet_type __read_mostly = { <nl>  <nl> static int __init inet_init ( void ) <nl> { <nl> - struct sk_buff * dummy_skb ; <nl> struct inet_protosw * q ; <nl> struct list_head * r ; <nl> int rc = - EINVAL ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct inet_skb_parm ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct inet_skb_parm ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> sysctl_local_reserved_ports = kzalloc ( 65536 / 8 , GFP_KERNEL ); <nl> if (! sysctl_local_reserved_ports )
static u8 parse_subframe ( struct sk_buff * skb , <nl> # else <nl> /* Allocate new skb for releasing to upper layer */ <nl> sub_skb = dev_alloc_skb ( nSubframe_Length + 12 ); <nl> + if (! sub_skb ) <nl> + return 0 ; <nl> skb_reserve ( sub_skb , 12 ); <nl> data_ptr = ( u8 *) skb_put ( sub_skb , nSubframe_Length ); <nl> memcpy ( data_ptr , skb -> data , nSubframe_Length );
void __attribute__ (( weak )) bust_spinlocks ( int yes ) <nl> { <nl> if ( yes ) { <nl> - oops_in_progress = 1 ; <nl> + ++ oops_in_progress ; <nl> } else { <nl> # ifdef CONFIG_VT <nl> unblank_screen (); <nl> # endif <nl> - oops_in_progress = 0 ; <nl> - wake_up_klogd (); <nl> + if (-- oops_in_progress == 0 ) <nl> + wake_up_klogd (); <nl> } <nl> } <nl> 
static int ads7846_remove ( struct spi_device * spi ) <nl>  <nl> ads784x_hwmon_unregister ( spi , ts ); <nl>  <nl> - regulator_disable ( ts -> reg ); <nl> regulator_put ( ts -> reg ); <nl>  <nl> if (! ts -> get_pendown_state ) {
dev_config ( struct file * fd , const char __user * buf , size_t len , loff_t * ptr ) <nl>  <nl> spin_lock_irq (& dev -> lock ); <nl> value = - EINVAL ; <nl> - if ( dev -> buf ) <nl> + if ( dev -> buf ) { <nl> + kfree ( kbuf ); <nl> goto fail ; <nl> + } <nl> dev -> buf = kbuf ; <nl>  <nl> /* full or low speed config */
print_graph_entry ( struct ftrace_graph_ent_entry * field , struct trace_seq * s , <nl>  <nl> /* Proc */ <nl> if ( tracer_flags . val & TRACE_GRAPH_PRINT_PROC ) { <nl> - ret = print_graph_proc ( s , pid ); <nl> + ret = print_graph_proc ( s , ent -> pid ); <nl> if ( ret == TRACE_TYPE_PARTIAL_LINE ) <nl> return TRACE_TYPE_PARTIAL_LINE ; <nl> 
static void __exit cleanup_nsc ( void ) <nl> if ( pdev ) { <nl> tpm_nsc_remove (& pdev -> dev ); <nl> platform_device_unregister ( pdev ); <nl> - kfree ( pdev ); <nl> - pdev = NULL ; <nl> } <nl>  <nl> platform_driver_unregister (& nsc_drv );
sid_to_id ( struct cifs_sb_info * cifs_sb , struct cifs_sid * psid , <nl> * probably a safe assumption but might be better to check based on <nl> * sidtype . <nl> */ <nl> + BUILD_BUG_ON ( sizeof ( uid_t ) != sizeof ( gid_t )); <nl> if ( sidkey -> datalen != sizeof ( uid_t )) { <nl> rc = - EIO ; <nl> cFYI ( 1 , "% s : Downcall contained malformed key "
static int __devinit corgipm_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if (! machine_is_corgi () && ! machine_is_shepherd () <nl> + && ! machine_is_husky ()) <nl> + return - ENODEV ; <nl> + <nl> corgipm_device = platform_device_alloc (" sharpsl - pm ", - 1 ); <nl> if (! corgipm_device ) <nl> return - ENOMEM ;
static void dlm_run_purge_list ( struct dlm_ctxt * dlm , <nl> /* This may drop and reacquire the dlm spinlock if it <nl> * has to do migration . */ <nl> mlog ( 0 , " calling dlm_purge_lockres !\ n "); <nl> + dlm_lockres_get ( lockres ); <nl> if ( dlm_purge_lockres ( dlm , lockres )) <nl> BUG (); <nl> + dlm_lockres_put ( lockres ); <nl> mlog ( 0 , " DONE calling dlm_purge_lockres !\ n "); <nl>  <nl> /* Avoid adding any scheduling latencies */
static int lbs_spi_thread ( void * data ) <nl> up (& card -> spi_thread_terminated ); <nl> do_exit ( 0 ); <nl> } <nl> - } while ( err == EINTR ); <nl> + } while ( err == - EINTR ); <nl>  <nl> /* Read the host interrupt status register to see what we <nl> * can do . */
void mdfld_dbi_dsr_exit ( struct drm_device * dev ) <nl> struct drm_psb_private * dev_priv = dev -> dev_private ; <nl> struct mdfld_dbi_dsr_info * dsr_info = dev_priv -> dbi_dsr_info ; <nl>  <nl> - if (! dsr_info ) { <nl> + if ( dsr_info ) { <nl> del_timer_sync (& dsr_info -> dsr_timer ); <nl> kfree ( dsr_info ); <nl> dev_priv -> dbi_dsr_info = NULL ;
# include < linux / version . h > <nl>  <nl> /* Simplified build - specific string for starting entropy . */ <nl> - static const char * build_str = UTS_RELEASE " (" LINUX_COMPILE_BY "@" <nl> + static const char build_str [] = UTS_RELEASE " (" LINUX_COMPILE_BY "@" <nl> LINUX_COMPILE_HOST ") (" LINUX_COMPILER ") " UTS_VERSION ; <nl>  <nl> # define I8254_PORT_CONTROL 0x43
static int ecryptfs_setattr ( struct dentry * dentry , struct iattr * ia ) <nl> } <nl> } <nl> mutex_unlock (& crypt_stat -> cs_mutex ); <nl> + if ( S_ISREG ( inode -> i_mode )) { <nl> + rc = filemap_write_and_wait ( inode -> i_mapping ); <nl> + if ( rc ) <nl> + goto out ; <nl> + fsstack_copy_attr_all ( inode , lower_inode ); <nl> + } <nl> memcpy (& lower_ia , ia , sizeof ( lower_ia )); <nl> if ( ia -> ia_valid & ATTR_FILE ) <nl> lower_ia . ia_file = ecryptfs_file_to_lower ( ia -> ia_file );
static u32 asle_set_backlight ( struct drm_device * dev , u32 bclp ) <nl> return ASLE_BACKLIGHT_FAILED ; <nl>  <nl> intel_panel_set_backlight ( dev , bclp , 255 ); <nl> - iowrite32 (( bclp * 0x64 )/ 0xff | ASLE_CBLV_VALID , & asle -> cblv ); <nl> + iowrite32 ( DIV_ROUND_UP ( bclp * 100 , 255 ) | ASLE_CBLV_VALID , & asle -> cblv ); <nl>  <nl> return 0 ; <nl> }
static void rt2800_config_channel_rf55xx ( struct rt2x00_dev * rt2x00dev , <nl> rt2800_rfcsr_write ( rt2x00dev , 49 , rfcsr ); <nl>  <nl> rt2800_rfcsr_read ( rt2x00dev , 50 , & rfcsr ); <nl> - if ( info -> default_power1 > power_bound ) <nl> + if ( info -> default_power2 > power_bound ) <nl> rt2x00_set_field8 (& rfcsr , RFCSR50_TX , power_bound ); <nl> else <nl> rt2x00_set_field8 (& rfcsr , RFCSR50_TX , info -> default_power2 );
static inline int rsi_create_kthread ( struct rsi_common * common , <nl> u8 * name ) <nl> { <nl> init_completion (& thread -> completion ); <nl> - thread -> task = kthread_run ( func_ptr , common , name ); <nl> + thread -> task = kthread_run ( func_ptr , common , "% s ", name ); <nl> if ( IS_ERR ( thread -> task )) <nl> return ( int ) PTR_ERR ( thread -> task ); <nl> 
static int fd_ioctl ( struct block_device * bdev , fmode_t mode , unsigned int cmd , <nl> (( cmd & 0x80 ) && ! capable ( CAP_SYS_ADMIN ))) <nl> return - EPERM ; <nl>  <nl> + if ( WARN_ON ( size < 0 || size > sizeof ( inparam ))) <nl> + return - EINVAL ; <nl> + <nl> /* copyin */ <nl> CLEARSTRUCT (& inparam ); <nl> if ( _IOC_DIR ( cmd ) & _IOC_WRITE )
static int __split_vma ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> return 0 ; <nl>  <nl> /* Clean everything up if vma_adjust failed . */ <nl> - new -> vm_ops -> close ( new ); <nl> + if ( new -> vm_ops && new -> vm_ops -> close ) <nl> + new -> vm_ops -> close ( new ); <nl> if ( new -> vm_file ) { <nl> if ( vma -> vm_flags & VM_EXECUTABLE ) <nl> removed_exe_file_vma ( mm );
static int handshake_on_error_set_halt ( struct ehci_hcd * ehci , void __iomem * ptr , <nl> if ( error ) { <nl> ehci_halt ( ehci ); <nl> ehci_to_hcd ( ehci )-> state = HC_STATE_HALT ; <nl> - ehci_err ( ehci , " force halt ; handhake % p % 08x % 08x -> % d \ n ", <nl> + ehci_err ( ehci , " force halt ; handshake % p % 08x % 08x -> % d \ n ", <nl> ptr , mask , done , error ); <nl> } <nl> 
static bool vgic_its_check_device_id ( struct kvm * kvm , struct vgic_its * its , <nl> & indirect_ptr , sizeof ( indirect_ptr ))) <nl> return false ; <nl>  <nl> + indirect_ptr = le64_to_cpu ( indirect_ptr ); <nl> + <nl> /* check the valid bit of the first level entry */ <nl> if (!( indirect_ptr & BIT_ULL ( 63 ))) <nl> return false ;
static int pn533_target_found ( struct pn533 * dev , <nl> if ( resp -> tg != 1 ) <nl> return - EPROTO ; <nl>  <nl> + memset (& nfc_tgt , 0 , sizeof ( struct nfc_target )); <nl> + <nl> target_data_len = resp_len - sizeof ( struct pn533_poll_response ); <nl>  <nl> switch ( dev -> poll_mod_curr ) {
static int cp2112_gpio_direction_input ( struct gpio_chip * chip , unsigned offset ) <nl>  <nl> exit : <nl> mutex_unlock (& dev -> lock ); <nl> - return ret <= 0 ? ret : - EIO ; <nl> + return ret < 0 ? ret : - EIO ; <nl> } <nl>  <nl> static void cp2112_gpio_set ( struct gpio_chip * chip , unsigned offset , int value )
static void uniphier_pctl_pin_dbg_show ( struct pinctrl_dev * pctldev , <nl> case UNIPHIER_PIN_PULL_DOWN : <nl> pull_dir = " DOWN "; <nl> break ; <nl> + case UNIPHIER_PIN_PULL_UP_FIXED : <nl> + pull_dir = " UP ( FIXED )"; <nl> + break ; <nl> + case UNIPHIER_PIN_PULL_DOWN_FIXED : <nl> + pull_dir = " DOWN ( FIXED )"; <nl> + break ; <nl> case UNIPHIER_PIN_PULL_NONE : <nl> pull_dir = " NONE "; <nl> break ;
static int mmci_probe ( struct amba_device * dev , <nl> dev_dbg ( mmc_dev ( mmc ), " clocking block at % u Hz \ n ", mmc -> f_max ); <nl>  <nl> /* Get regulators and the supported OCR mask */ <nl> - mmc_regulator_get_supply ( mmc ); <nl> + ret = mmc_regulator_get_supply ( mmc ); <nl> + if ( ret == - EPROBE_DEFER ) <nl> + goto clk_disable ; <nl> + <nl> if (! mmc -> ocr_avail ) <nl> mmc -> ocr_avail = plat -> ocr_mask ; <nl> else if ( plat -> ocr_mask )
static struct print_arg * make_bprint_args ( char * fmt , void * data , int size , struc <nl> goto process_again ; <nl> case '.': <nl> goto process_again ; <nl> + case ' z ': <nl> + case ' Z ': <nl> + ls = 1 ; <nl> + goto process_again ; <nl> case ' p ': <nl> ls = 1 ; <nl> /* fall through */
static int selinux_setprocattr ( struct task_struct * p , <nl> return error ; <nl>  <nl> /* Obtain a SID for the context , if one was specified . */ <nl> - if ( size && str [ 1 ] && str [ 1 ] != '\ n ') { <nl> + if ( size && str [ 0 ] && str [ 0 ] != '\ n ') { <nl> if ( str [ size - 1 ] == '\ n ') { <nl> str [ size - 1 ] = 0 ; <nl> size --;
static int ath10k_start ( struct ieee80211_hw * hw ) <nl> goto err_core_stop ; <nl> } <nl>  <nl> + ret = ath10k_wmi_pdev_set_param ( ar , <nl> + ar -> wmi . pdev_param -> ani_enable , 1 ); <nl> + if ( ret ) { <nl> + ath10k_warn ( ar , " failed to enable ani by default : % d \ n ", <nl> + ret ); <nl> + goto err_core_stop ; <nl> + } <nl> + <nl> ar -> num_started_vdevs = 0 ; <nl> ath10k_regd_update ( ar ); <nl> 
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
static int pb0100_start ( struct sd * sd ) <nl>  <nl> intf = usb_ifnum_to_if ( sd -> gspca_dev . dev , sd -> gspca_dev . iface ); <nl> alt = usb_altnum_to_altsetting ( intf , sd -> gspca_dev . alt ); <nl> + if (! alt ) <nl> + return - ENODEV ; <nl> packet_size = le16_to_cpu ( alt -> endpoint [ 0 ]. desc . wMaxPacketSize ); <nl>  <nl> /* If we don ' t have enough bandwidth use a lower framerate */
acpi_db_walk_for_execute ( acpi_handle obj_handle , <nl>  <nl> status = acpi_get_object_info ( obj_handle , & obj_info ); <nl> if ( ACPI_FAILURE ( status )) { <nl> + ACPI_FREE ( pathname ); <nl> return ( status ); <nl> } <nl> 
int cfg80211_mgd_wext_connect ( struct cfg80211_registered_device * rdev , <nl> if ( wdev -> wext . keys ) { <nl> wdev -> wext . keys -> def = wdev -> wext . default_key ; <nl> wdev -> wext . keys -> defmgmt = wdev -> wext . default_mgmt_key ; <nl> - wdev -> wext . connect . privacy = true ; <nl> + if ( wdev -> wext . default_key != - 1 ) <nl> + wdev -> wext . connect . privacy = true ; <nl> } <nl>  <nl> if (! wdev -> wext . connect . ssid_len )
unsigned long acpi_realmode_flags ; <nl> static unsigned long acpi_realmode ; <nl>  <nl> # if defined ( CONFIG_SMP ) && defined ( CONFIG_64BIT ) <nl> - static char temp_stack [ 10240 ]; <nl> + static char temp_stack [ 4096 ]; <nl> # endif <nl>  <nl> /**
static void sti_crtc_atomic_flush ( struct drm_crtc * crtc , <nl>  <nl> switch ( plane -> status ) { <nl> case STI_PLANE_UPDATED : <nl> + /* ignore update for other CRTC */ <nl> + if ( p -> state -> crtc != crtc ) <nl> + continue ; <nl> + <nl> /* update planes tag as updated */ <nl> DRM_DEBUG_DRIVER (" update plane % s \ n ", <nl> sti_plane_to_str ( plane ));
static void radeon_compute_pll_legacy ( struct radeon_pll * pll , <nl> max_fractional_feed_div = pll -> max_frac_feedback_div ; <nl> } <nl>  <nl> - for ( post_div = min_post_div ; post_div <= max_post_div ; ++ post_div ) { <nl> + for ( post_div = max_post_div ; post_div >= min_post_div ; -- post_div ) { <nl> uint32_t ref_div ; <nl>  <nl> if (( pll -> flags & RADEON_PLL_NO_ODD_POST_DIV ) && ( post_div & 1 ))
static void mce_async_callback ( struct urb * urb , struct pt_regs * regs ) <nl> mceusb_dev_printdata ( ir , urb -> transfer_buffer , 0 , len , true ); <nl> } <nl>  <nl> + /* the transfer buffer and urb were allocated in mce_request_packet */ <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> } <nl>  <nl> /* request incoming or send outgoing usb packet - used to initialize remote */
i915_gem_execbuffer2 ( struct drm_device * dev , void * data , <nl> struct drm_i915_gem_exec_object2 * exec2_list = NULL ; <nl> int ret ; <nl>  <nl> - if ( args -> buffer_count < 1 ) { <nl> + if ( args -> buffer_count < 1 || <nl> + args -> buffer_count > UINT_MAX / sizeof (* exec2_list )) { <nl> DRM_DEBUG (" execbuf2 with % d buffers \ n ", args -> buffer_count ); <nl> return - EINVAL ; <nl> }
static int s3c64xx_setparent_clksrc ( struct clk * clk , struct clk * parent ) <nl> clksrc |= src_nr << sclk -> shift ; <nl>  <nl> __raw_writel ( clksrc , S3C_CLK_SRC ); <nl> + <nl> + clk -> parent = parent ; <nl> return 0 ; <nl> } <nl> 
void intel_setup_bios ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl>  <nl> /* Set the Panel Power On / Off timings if uninitialized . */ <nl> - if (( I915_READ ( PP_ON_DELAYS ) == 0 ) && ( I915_READ ( PP_OFF_DELAYS ) == 0 )) { <nl> + if (! HAS_PCH_SPLIT ( dev ) && <nl> + I915_READ ( PP_ON_DELAYS ) == 0 && I915_READ ( PP_OFF_DELAYS ) == 0 ) { <nl> /* Set T2 to 40ms and T5 to 200ms */ <nl> I915_WRITE ( PP_ON_DELAYS , 0x019007d0 ); <nl> 
static void b43_request_firmware ( struct work_struct * work ) <nl> for ( i = 0 ; i < B43_NR_FWTYPES ; i ++) { <nl> errmsg = ctx -> errors [ i ]; <nl> if ( strlen ( errmsg )) <nl> - b43err ( dev -> wl , errmsg ); <nl> + b43err ( dev -> wl , "% s ", errmsg ); <nl> } <nl> b43_print_fw_helptext ( dev -> wl , 1 ); <nl> goto out ;
static int atc_control ( struct dma_chan * chan , enum dma_ctrl_cmd cmd , <nl> list_splice_init (& atchan -> queue , & list ); <nl> list_splice_init (& atchan -> active_list , & list ); <nl>  <nl> - spin_unlock_bh (& atchan -> lock ); <nl> - <nl> /* Flush all pending and queued descriptors */ <nl> list_for_each_entry_safe ( desc , _desc , & list , desc_node ) <nl> atc_chain_complete ( atchan , desc ); <nl>  <nl> + spin_unlock_bh (& atchan -> lock ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int dw_i2c_probe ( struct platform_device * pdev ) <nl> adap = & dev -> adapter ; <nl> i2c_set_adapdata ( adap , dev ); <nl> adap -> owner = THIS_MODULE ; <nl> - adap -> class = I2C_CLASS_HWMON | I2C_CLASS_DEPRECATED ; <nl> + adap -> class = I2C_CLASS_DEPRECATED ; <nl> strlcpy ( adap -> name , " Synopsys DesignWare I2C adapter ", <nl> sizeof ( adap -> name )); <nl> adap -> algo = & i2c_dw_algo ;
static int sa1111_resume ( struct platform_device * dev ) <nl> # define sa1111_resume NULL <nl> # endif <nl>  <nl> - static int sa1111_probe ( struct platform_device * pdev ) <nl> + static int __devinit sa1111_probe ( struct platform_device * pdev ) <nl> { <nl> struct resource * mem ; <nl> int irq ;
static struct intel_iommu * device_to_iommu ( struct device * dev , u8 * bus , u8 * devf <nl> * which we used for the IOMMU lookup . Strictly speaking <nl> * we could do this for all PCI devices ; we only need to <nl> * get the BDF # from the scope table for ACPI matches . */ <nl> - if ( pdev -> is_virtfn ) <nl> + if ( pdev && pdev -> is_virtfn ) <nl> goto got_pdev ; <nl>  <nl> * bus = drhd -> devices [ i ]. bus ;
done : <nl> init_timer (& dpriv -> timer ); <nl> dpriv -> timer . expires = jiffies + 10 * HZ ; <nl> dpriv -> timer . data = ( unsigned long ) dev ; <nl> - dpriv -> timer . function = & dscc4_timer ; <nl> + dpriv -> timer . function = dscc4_timer ; <nl> add_timer (& dpriv -> timer ); <nl> netif_carrier_on ( dev ); <nl> 
static long do_ixj_ioctl ( struct file * file_p , unsigned int cmd , unsigned long ar <nl> case IXJCTL_SET_FILTER : <nl> if ( copy_from_user (& jf , argp , sizeof ( jf ))) <nl> retval = - EFAULT ; <nl> - retval = ixj_init_filter ( j , & jf ); <nl> + else <nl> + retval = ixj_init_filter ( j , & jf ); <nl> break ; <nl> case IXJCTL_SET_FILTER_RAW : <nl> if ( copy_from_user (& jfr , argp , sizeof ( jfr )))
static unsigned int bsg_poll ( struct file * file , poll_table * wait ) <nl> spin_lock_irq (& bd -> lock ); <nl> if (! list_empty (& bd -> done_list )) <nl> mask |= POLLIN | POLLRDNORM ; <nl> - if ( bd -> queued_cmds >= bd -> max_queue ) <nl> + if ( bd -> queued_cmds < bd -> max_queue ) <nl> mask |= POLLOUT ; <nl> spin_unlock_irq (& bd -> lock ); <nl> 
static int gfx_v9_0_set_clockgating_state ( void * handle , <nl> { <nl> struct amdgpu_device * adev = ( struct amdgpu_device *) handle ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> switch ( adev -> asic_type ) { <nl> case CHIP_VEGA10 : <nl> gfx_v9_0_update_gfx_clock_gating ( adev ,
void __init s3c6400_init_irq ( void ) <nl> s3c64xx_init_irq (~ 0 & ~( 0xf << 5 ), ~ 0 ); <nl> } <nl>  <nl> - struct sysdev_class s3c6400_sysclass = { <nl> + static struct sysdev_class s3c6400_sysclass = { <nl> . name = " s3c6400 - core ", <nl> }; <nl> 
static void sky2_restart ( struct work_struct * work ) <nl>  <nl> rtnl_lock (); <nl>  <nl> - napi_disable (& hw -> napi ); <nl> - synchronize_irq ( hw -> pdev -> irq ); <nl> imask = sky2_read32 ( hw , B0_IMSK ); <nl> sky2_write32 ( hw , B0_IMSK , 0 ); <nl> + synchronize_irq ( hw -> pdev -> irq ); <nl> + napi_disable (& hw -> napi ); <nl>  <nl> for ( i = 0 ; i < hw -> ports ; i ++) { <nl> struct net_device * dev = hw -> dev [ i ];
static void usbhsh_pipe_detach ( struct usbhsh_hpriv * hpriv , <nl> struct device * dev = usbhs_priv_to_dev ( priv ); <nl> unsigned long flags ; <nl>  <nl> + if ( unlikely (! uep )) { <nl> + dev_err ( dev , " no uep \ n "); <nl> + return ; <nl> + } <nl> + <nl> /******************** spin lock ********************/ <nl> usbhs_lock ( priv , flags ); <nl> 
static void efx_pci_remove ( struct pci_dev * pci_dev ) <nl> efx_dissociate ( efx ); <nl> dev_close ( efx -> net_dev ); <nl> efx_disable_interrupts ( efx ); <nl> + efx -> state = STATE_UNINIT ; <nl> rtnl_unlock (); <nl>  <nl> if ( efx -> type -> sriov_fini )
static struct afs_server * afs_alloc_server ( struct afs_cell * cell , <nl>  <nl> memcpy (& server -> addr , addr , sizeof ( struct in_addr )); <nl> server -> addr . s_addr = addr -> s_addr ; <nl> + _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> + } else { <nl> + _leave (" = NULL [ nomem ]"); <nl> } <nl> - <nl> - _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> return server ; <nl> } <nl> 
radeon_add_legacy_encoder ( struct drm_device * dev , uint32_t encoder_id , uint32_t <nl>  <nl> switch ( radeon_encoder -> encoder_id ) { <nl> case ENCODER_OBJECT_ID_INTERNAL_LVDS : <nl> + encoder -> possible_crtcs = 0x1 ; <nl> drm_encoder_init ( dev , encoder , & radeon_legacy_lvds_enc_funcs , DRM_MODE_ENCODER_LVDS ); <nl> drm_encoder_helper_add ( encoder , & radeon_legacy_lvds_helper_funcs ); <nl> if ( rdev -> is_atom_bios )
__ieee80211_get_channel_mode ( struct ieee80211_local * local , <nl> if (! sdata -> u . ap . beacon ) <nl> continue ; <nl> break ; <nl> + case NL80211_IFTYPE_MESH_POINT : <nl> + if (! sdata -> wdev . mesh_id_len ) <nl> + continue ; <nl> + break ; <nl> default : <nl> break ; <nl> }
struct greybus_device * greybus_new_module ( struct device * parent , <nl>  <nl> return gdev ; <nl> error : <nl> + put_device (& gdev -> dev ); <nl> greybus_module_release (& gdev -> dev ); <nl> return NULL ; <nl> }
int i915_gem_stolen_setup_compression ( struct drm_device * dev , int size , int fb_c <nl> if (! drm_mm_initialized (& dev_priv -> mm . stolen )) <nl> return - ENODEV ; <nl>  <nl> - if ( size < dev_priv -> fbc . uncompressed_size ) <nl> + if ( size <= dev_priv -> fbc . uncompressed_size ) <nl> return 0 ; <nl>  <nl> /* Release any current block */
restore_state : <nl>  <nl> return err ; <nl> } <nl> + EXPORT_SYMBOL ( xfrm_migrate ); <nl> # endif <nl> 
MPI mpi_read_raw_data ( const void * xbuffer , size_t nbytes ) <nl> mpi_limb_t a ; <nl> MPI val = NULL ; <nl>  <nl> - while ( nbytes >= 0 && buffer [ 0 ] == 0 ) { <nl> + while ( nbytes > 0 && buffer [ 0 ] == 0 ) { <nl> buffer ++; <nl> nbytes --; <nl> }
void ieee80211_tx_status ( struct ieee80211_hw * hw , struct sk_buff * skb ) <nl> if (! netif_running ( sdata -> dev )) <nl> continue ; <nl>  <nl> + if (( sdata -> u . mntr_flags & MONITOR_FLAG_COOK_FRAMES ) && <nl> + !( info -> flags & IEEE80211_TX_CTL_INJECTED ) && <nl> + ( type == IEEE80211_FTYPE_DATA )) <nl> + continue ; <nl> + <nl> if ( prev_dev ) { <nl> skb2 = skb_clone ( skb , GFP_ATOMIC ); <nl> if ( skb2 ) {
journal_t * jbd2_journal_init_dev ( struct block_device * bdev , <nl> __FUNCTION__ ); <nl> kfree ( journal ); <nl> journal = NULL ; <nl> + goto out ; <nl> } <nl> journal -> j_dev = bdev ; <nl> journal -> j_fs_dev = fs_dev ; <nl> journal_t * jbd2_journal_init_dev ( struct block_device * bdev , <nl> J_ASSERT ( bh != NULL ); <nl> journal -> j_sb_buffer = bh ; <nl> journal -> j_superblock = ( journal_superblock_t *) bh -> b_data ; <nl> - <nl> + out : <nl> return journal ; <nl> } <nl> 
cifs_put_tcon ( struct cifsTconInfo * tcon ) <nl> CIFSSMBTDis ( xid , tcon ); <nl> _FreeXid ( xid ); <nl>  <nl> - tconInfoFree ( tcon ); <nl> cifs_fscache_release_super_cookie ( tcon ); <nl> + tconInfoFree ( tcon ); <nl> cifs_put_smb_ses ( ses ); <nl> } <nl> 
int snd_es1688_pcm ( struct snd_card * card , struct snd_es1688 * chip , int device ) <nl>  <nl> pcm -> private_data = chip ; <nl> pcm -> info_flags = SNDRV_PCM_INFO_HALF_DUPLEX ; <nl> - sprintf ( pcm -> name , snd_es1688_chip_id ( chip )); <nl> + strcpy ( pcm -> name , snd_es1688_chip_id ( chip )); <nl> chip -> pcm = pcm ; <nl>  <nl> snd_pcm_lib_preallocate_pages_for_all ( pcm , SNDRV_DMA_TYPE_DEV ,
rerun_vcpu : <nl> if ( rc == SIE_INTERCEPT_RERUNVCPU ) <nl> goto rerun_vcpu ; <nl>  <nl> - if ( signal_pending ( current ) && ! rc ) <nl> + if ( signal_pending ( current ) && ! rc ) { <nl> + kvm_run -> exit_reason = KVM_EXIT_INTR ; <nl> rc = - EINTR ; <nl> + } <nl>  <nl> if ( rc == - ENOTSUPP ) { <nl> /* intercept cannot be handled in - kernel , prepare kvm - run */
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 81 " <nl> -# define DRV_MODULE_RELDATE " September 5 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 82 " <nl> +# define DRV_MODULE_RELDATE " October 5 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static int rose_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> skb_copy_datagram_iovec ( skb , 0 , msg -> msg_iov , copied ); <nl>  <nl> if ( srose != NULL ) { <nl> + memset ( srose , 0 , msg -> msg_namelen ); <nl> srose -> srose_family = AF_ROSE ; <nl> srose -> srose_addr = rose -> dest_addr ; <nl> srose -> srose_call = rose -> dest_call ;
static int __open_dso ( struct dso * dso , struct machine * machine ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if (! is_regular_file ( name )) <nl> + return - EINVAL ; <nl> + <nl> fd = do_open ( name ); <nl> free ( name ); <nl> return fd ;
 <nl> bool rtw_IOL_applied ( struct adapter * adapter ) <nl> { <nl> - if ( 1 == adapter -> registrypriv . fw_iol ) <nl> + if ( adapter -> registrypriv . fw_iol == 1 ) <nl> return true ; <nl>  <nl> - if (( 2 == adapter -> registrypriv . fw_iol ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> + if (( adapter -> registrypriv . fw_iol == 2 ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> return true ; <nl> return false ; <nl> }
ssize_t ttm_bo_io ( struct ttm_bo_device * bdev , struct file * filp , <nl> return - EFAULT ; <nl>  <nl> driver = bo -> bdev -> driver ; <nl> - if ( unlikely ( driver -> verify_access )) { <nl> + if ( unlikely (! driver -> verify_access )) { <nl> ret = - EPERM ; <nl> goto out_unref ; <nl> }
read_rtc : <nl> } <nl> ds1307 -> nvram -> attr . name = " nvram "; <nl> ds1307 -> nvram -> attr . mode = S_IRUGO | S_IWUSR ; <nl> + sysfs_bin_attr_init ( ds1307 -> nvram ); <nl> ds1307 -> nvram -> read = ds1307_nvram_read , <nl> ds1307 -> nvram -> write = ds1307_nvram_write , <nl> ds1307 -> nvram -> size = chip -> nvram_size ;
static struct pxamci_platform_data magician_mci_info = { <nl>  <nl> static struct pxaohci_platform_data magician_ohci_info = { <nl> . port_mode = PMM_PERPORT_MODE , <nl> - . flags = ENABLE_PORT1 | ENABLE_PORT3 | POWER_CONTROL_LOW , <nl> + /* port1 : CSR Bluetooth , port2 : OTG with UDC */ <nl> + . flags = ENABLE_PORT1 | ENABLE_PORT2 | POWER_CONTROL_LOW , <nl> . power_budget = 0 , <nl> + . power_on_delay = 100 , <nl> }; <nl>  <nl> /*
struct mapped_device * dm_get_from_kobject ( struct kobject * kobj ) <nl> if (& md -> kobj != kobj ) <nl> return NULL ; <nl>  <nl> + if ( test_bit ( DMF_FREEING , & md -> flags ) || <nl> + test_bit ( DMF_DELETING , & md -> flags )) <nl> + return NULL ; <nl> + <nl> dm_get ( md ); <nl> return md ; <nl> }
static __u32 get_ftdi_divisor ( struct tty_struct * tty , <nl> case FT2232H : /* FT2232H chip */ <nl> case FT4232H : /* FT4232H chip */ <nl> case FT232H : /* FT232H chip */ <nl> - if (( baud <= 12000000 ) & ( baud >= 1200 )) { <nl> + if (( baud <= 12000000 ) && ( baud >= 1200 )) { <nl> div_value = ftdi_2232h_baud_to_divisor ( baud ); <nl> } else if ( baud < 1200 ) { <nl> div_value = ftdi_232bm_baud_to_divisor ( baud );
int extcon_register_interest ( struct extcon_specific_cable_nb * obj , <nl>  <nl> obj -> cable_index = extcon_find_cable_index ( obj -> edev , cable_name ); <nl> if ( obj -> cable_index < 0 ) <nl> - return - ENODEV ; <nl> + return obj -> cable_index ; <nl>  <nl> obj -> user_nb = nb ; <nl> 
static ssize_t numa_node_store ( struct device * dev , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - if (! node_online ( node )) <nl> + if ( node >= MAX_NUMNODES || ! node_online ( node )) <nl> return - EINVAL ; <nl>  <nl> add_taint ( TAINT_FIRMWARE_WORKAROUND , LOCKDEP_STILL_OK );
static int __init parse_memmap_opt ( char * p ) <nl> char * oldp ; <nl> u64 start_at , mem_size ; <nl>  <nl> + if (! p ) <nl> + return - EINVAL ; <nl> + <nl> if (! strcmp ( p , " exactmap ")) { <nl> # ifdef CONFIG_CRASH_DUMP <nl> /*
static void __init s3c24xx_gpiolib_add_chips ( struct samsung_gpio_chip * chip , <nl> struct gpio_chip * gc = & chip -> chip ; <nl>  <nl> for ( i = 0 ; i < nr_chips ; i ++, chip ++) { <nl> + /* skip banks not present on SoC */ <nl> + if ( chip -> chip . base >= S3C_GPIO_END ) <nl> + continue ; <nl> + <nl> if (! chip -> config ) <nl> chip -> config = & s3c24xx_gpiocfg_default ; <nl> if (! chip -> pm )
static void maxiradio_remove ( struct pci_dev * pdev ) <nl> outb ( 0 , dev -> io ); <nl> v4l2_device_unregister ( v4l2_dev ); <nl> release_region ( pci_resource_start ( pdev , 0 ), pci_resource_len ( pdev , 0 )); <nl> + kfree ( dev ); <nl> } <nl>  <nl> static struct pci_device_id maxiradio_pci_tbl [] = {
int snd_timer_new ( struct snd_card * card , char * id , struct snd_timer_id * tid , <nl> timer -> tmr_subdevice = tid -> subdevice ; <nl> if ( id ) <nl> strlcpy ( timer -> id , id , sizeof ( timer -> id )); <nl> + timer -> sticks = 1 ; <nl> INIT_LIST_HEAD (& timer -> device_list ); <nl> INIT_LIST_HEAD (& timer -> open_list_head ); <nl> INIT_LIST_HEAD (& timer -> active_list_head );
void jffs2_rtime_exit ( void ); <nl> int jffs2_zlib_init ( void ); <nl> void jffs2_zlib_exit ( void ); <nl> # endif <nl> +# ifdef CONFIG_JFFS2_LZO <nl> + int jffs2_lzo_init ( void ); <nl> + void jffs2_lzo_exit ( void ); <nl> +# endif <nl>  <nl> # endif /* __JFFS2_COMPR_H__ */
static const char * const cw1200_debug_link_id [] = { <nl> " REQ ", <nl> " SOFT ", <nl> " HARD ", <nl> + " RESET ", <nl> + " RESET_REMAP ", <nl> }; <nl>  <nl> static const char * cw1200_debug_mode ( int mode )
int __kvm_set_memory_region ( struct kvm * kvm , <nl> /* Allocate if a slot is being created */ <nl> # ifndef CONFIG_S390 <nl> if ( npages && ! new . rmap ) { <nl> - new . rmap = vmalloc ( npages * sizeof ( struct page *)); <nl> + new . rmap = vmalloc ( npages * sizeof (* new . rmap )); <nl>  <nl> if (! new . rmap ) <nl> goto out_free ;
# define SCLK_MACREF_OUT 106 <nl> # define SCLK_VOP0_PWM 107 <nl> # define SCLK_VOP1_PWM 108 <nl> -# define SCLK_RGA 109 <nl> +# define SCLK_RGA_CORE 109 <nl> # define SCLK_ISP0 110 <nl> # define SCLK_ISP1 111 <nl> # define SCLK_HDMI_CEC 112
void __init pxa_set_mci_info ( struct pxamci_platform_data * info ) <nl> } <nl>  <nl>  <nl> - static struct pxa2xx_udc_mach_info pxa_udc_info ; <nl> + static struct pxa2xx_udc_mach_info pxa_udc_info = { <nl> + . gpio_pullup = - 1 , <nl> + . gpio_vbus = - 1 , <nl> +}; <nl>  <nl> void __init pxa_set_udc_info ( struct pxa2xx_udc_mach_info * info ) <nl> {
static int __init mod_init ( void ) <nl> if ( err ) { <nl> printk ( KERN_ERR PFX " RNG registering failed (% d )\ n ", <nl> err ); <nl> - goto out ; <nl> + goto err_unmap ; <nl> } <nl> out : <nl> return err ;
static int fn_trie_insert ( struct fib_table * tb , struct fib_config * cfg ) <nl> struct fib_info * fi_drop ; <nl> u8 state ; <nl>  <nl> + if ( fi -> fib_treeref > 1 ) <nl> + goto out ; <nl> + <nl> err = - ENOBUFS ; <nl> new_fa = kmem_cache_alloc ( fn_alias_kmem , GFP_KERNEL ); <nl> if ( new_fa == NULL )
int drbd_adm_resize ( struct sk_buff * skb , struct genl_info * info ) <nl> mutex_unlock (& device -> resource -> conf_update ); <nl> synchronize_rcu (); <nl> kfree ( old_disk_conf ); <nl> + new_disk_conf = NULL ; <nl> } <nl>  <nl> ddsf = ( rs . resize_force ? DDSF_FORCED : 0 ) | ( rs . no_resync ? DDSF_NO_RESYNC : 0 ); <nl> int drbd_adm_resize ( struct sk_buff * skb , struct genl_info * info ) <nl>  <nl> fail_ldev : <nl> put_ldev ( device ); <nl> + kfree ( new_disk_conf ); <nl> goto fail ; <nl> } <nl> 
void rtl_8821ae_c2h_command_handle ( struct ieee80211_hw * hw ) <nl> rtl_write_byte ( rtlpriv , 0x1AF , 0x00 ); <nl> return ; <nl> } <nl> - ptmp_buf = ( u8 *) kmalloc ( c2h_event . cmd_len , GFP_KERNEL ); <nl> + ptmp_buf = kmalloc ( c2h_event . cmd_len , GFP_KERNEL ); <nl> if ( ptmp_buf == NULL ) { <nl> RT_TRACE ( COMP_FW , DBG_TRACE , (" malloc cmd buf failed \ n ")); <nl> return ;
void tipc_link_delete ( struct link * l_ptr ) <nl>  <nl> static void link_start ( struct link * l_ptr ) <nl> { <nl> + tipc_node_lock ( l_ptr -> owner ); <nl> link_state_event ( l_ptr , STARTING_EVT ); <nl> + tipc_node_unlock ( l_ptr -> owner ); <nl> } <nl>  <nl> /**
static void valleyview_irq_preinstall ( struct drm_device * dev ) <nl> I915_WRITE ( RING_IMR ( GEN6_BSD_RING_BASE ), 0 ); <nl> I915_WRITE ( RING_IMR ( BLT_RING_BASE ), 0 ); <nl>  <nl> - /* and GT */ <nl> - I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> - I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> - <nl> gen5_gt_irq_reset ( dev ); <nl>  <nl> I915_WRITE ( DPINVGTT , DPINVGTT_STATUS_MASK );
# include " dvb_frontend . h " <nl> # include " au8522_priv . h " <nl>  <nl> + MODULE_LICENSE (" GPL "); <nl> + <nl> static int debug ; <nl>  <nl> # define dprintk ( arg ...)\
static void tenxpress_phy_fini ( struct efx_nic * efx ) <nl> { <nl> int reg ; <nl>  <nl> - if ( efx -> phy_type == PHY_TYPE_SFT9001B ) { <nl> + if ( efx -> phy_type == PHY_TYPE_SFT9001B ) <nl> device_remove_file (& efx -> pci_dev -> dev , <nl> & dev_attr_phy_short_reach ); <nl> - } else { <nl> + <nl> + if ( efx -> phy_type == PHY_TYPE_SFX7101 ) { <nl> /* Power down the LNPGA */ <nl> reg = ( 1 << PMA_PMD_LNPGA_POWERDOWN_LBN ); <nl> mdio_clause45_write ( efx , efx -> mii . phy_id , MDIO_MMD_PMAPMD ,
static bool ath9k_hw_chip_reset ( struct ath_hw * ah , <nl> if ( AR_SREV_9330 ( ah )) <nl> ar9003_hw_internal_regulator_apply ( ah ); <nl> ath9k_hw_init_pll ( ah , chan ); <nl> - ath9k_hw_set_rfmode ( ah , chan ); <nl>  <nl> return true ; <nl> } <nl> int ath9k_hw_reset ( struct ath_hw * ah , struct ath9k_channel * chan , <nl> if ( r ) <nl> return r ; <nl>  <nl> + ath9k_hw_set_rfmode ( ah , chan ); <nl> + <nl> if ( ath9k_hw_mci_is_enabled ( ah )) <nl> ar9003_mci_reset ( ah , false , IS_CHAN_2GHZ ( chan ), save_fullsleep ); <nl> 
static void __devexit pm2fb_remove ( struct pci_dev * pdev ) <nl> release_mem_region ( fix -> mmio_start , fix -> mmio_len ); <nl>  <nl> pci_set_drvdata ( pdev , NULL ); <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> kfree ( info -> pixmap . addr ); <nl> kfree ( info ); <nl> }
static void add_pin_to_irq_node ( struct irq_cfg * cfg , int node , int apic , int pin <nl> } <nl>  <nl> entry = get_one_free_irq_2_pin ( node ); <nl> + if (! entry ) { <nl> + printk ( KERN_ERR " can not alloc irq_pin_list \ n "); <nl> + BUG_ON ( 1 ); <nl> + } <nl> entry -> apic = apic ; <nl> entry -> pin = pin ; <nl> 
static struct irqaction psurge_irqaction = { <nl>  <nl> static void __init smp_psurge_setup_cpu ( int cpu_nr ) <nl> { <nl> - if ( cpu_nr != 0 ) <nl> + if ( cpu_nr != 0 || ! psurge_start ) <nl> return ; <nl>  <nl> /* reset the entry point so if we get another intr we won ' t
xfs_mountfs ( <nl> * Allocate and initialize the per - ag data . <nl> */ <nl> spin_lock_init (& mp -> m_perag_lock ); <nl> - INIT_RADIX_TREE (& mp -> m_perag_tree , GFP_NOFS ); <nl> + INIT_RADIX_TREE (& mp -> m_perag_tree , GFP_ATOMIC ); <nl> error = xfs_initialize_perag ( mp , sbp -> sb_agcount , & mp -> m_maxagi ); <nl> if ( error ) { <nl> cmn_err ( CE_WARN , " XFS : Failed per - ag init : % d ", error );
static int saa7134_try_get_set_fmt_vbi_cap ( struct file * file , void * priv , <nl> struct saa7134_dev * dev = fh -> dev ; <nl> struct saa7134_tvnorm * norm = dev -> tvnorm ; <nl>  <nl> + memset (& f -> fmt . vbi . reserved , 0 , sizeof ( f -> fmt . vbi . reserved )); <nl> f -> fmt . vbi . sampling_rate = 6750000 * 4 ; <nl> f -> fmt . vbi . samples_per_line = 2048 /* VBI_LINE_LENGTH */; <nl> f -> fmt . vbi . sample_format = V4L2_PIX_FMT_GREY ;
__cmd_probe ( int argc , const char ** argv , const char * prefix __maybe_unused ) <nl> OPT_CALLBACK (' x ', " exec ", NULL , " executable | path ", <nl> " target executable name or path ", opt_set_target ), <nl> OPT_BOOLEAN ( 0 , " demangle ", & symbol_conf . demangle , <nl> - " Disable symbol demangling "), <nl> + " Enable symbol demangling "), <nl> OPT_BOOLEAN ( 0 , " demangle - kernel ", & symbol_conf . demangle_kernel , <nl> " Enable kernel symbol demangling "), <nl> OPT_END ()
struct net_device * alloc_rtllib ( int sizeof_priv ) <nl> rtllib_softmac_init ( ieee ); <nl>  <nl> ieee -> pHTInfo = kzalloc ( sizeof ( struct rt_hi_throughput ), GFP_KERNEL ); <nl> - if ( ieee -> pHTInfo == NULL ) <nl> + if (! ieee -> pHTInfo ) <nl> return NULL ; <nl>  <nl> HTUpdateDefaultSetting ( ieee );
static hda_nid_t set_path_power ( struct hda_codec * codec , hda_nid_t nid , <nl>  <nl> for ( n = 0 ; n < spec -> paths . used ; n ++) { <nl> path = snd_array_elem (& spec -> paths , n ); <nl> + if (! path -> depth ) <nl> + continue ; <nl> if ( path -> path [ 0 ] == nid || <nl> path -> path [ path -> depth - 1 ] == nid ) { <nl> bool pin_old = path -> pin_enabled ;
static int __init atmel_nand_probe ( struct platform_device * pdev ) <nl> } <nl> } <nl> if ( use_dma ) <nl> - dev_info ( host -> dev , " Using DMA for NAND access .\ n "); <nl> + dev_info ( host -> dev , " Using % s for DMA transfers .\ n ", <nl> + dma_chan_name ( host -> dma_chan )); <nl> else <nl> dev_info ( host -> dev , " No DMA support for NAND access .\ n "); <nl> 
# define PAGE_SIZE ( _AC ( 1 , UL ) << PAGE_SHIFT ) <nl> # define PAGE_MASK (~( PAGE_SIZE - 1 )) <nl>  <nl> -# define __PHYSICAL_MASK (( phys_addr_t )( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> +# define __PHYSICAL_MASK (( phys_addr_t )(( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 )) <nl> # define __VIRTUAL_MASK (( 1UL << __VIRTUAL_MASK_SHIFT ) - 1 ) <nl>  <nl> /* Cast PAGE_MASK to a signed type so that it is sign - extended if
static int map_sg_data ( struct scsi_cmnd * cmd , <nl> sdev_printk ( KERN_ERR , cmd -> device , <nl> " Can ' t allocate memory " <nl> " for indirect table \ n "); <nl> + scsi_dma_unmap ( cmd ); <nl> return 0 ; <nl> } <nl> }
static int fb_deferred_io_mkwrite ( struct vm_area_struct * vma , <nl> deferred framebuffer IO . then if userspace touches a page <nl> again , we repeat the same scheme */ <nl>  <nl> + file_update_time ( vma -> vm_file ); <nl> + <nl> /* protect against the workqueue changing the page list */ <nl> mutex_lock (& fbdefio -> lock ); <nl> 
static void dashtty_timer ( unsigned long ignored ) <nl> if ( channel >= 0 ) <nl> fetch_data ( channel ); <nl>  <nl> - mod_timer_pinned (& poll_timer , jiffies + DA_TTY_POLL ); <nl> + mod_timer (& poll_timer , jiffies + DA_TTY_POLL ); <nl> } <nl>  <nl> static void add_poll_timer ( struct timer_list * poll_timer ) <nl> { <nl> - setup_timer ( poll_timer , dashtty_timer , 0 ); <nl> + setup_pinned_timer ( poll_timer , dashtty_timer , 0 ); <nl> poll_timer -> expires = jiffies + DA_TTY_POLL ; <nl>  <nl> /*
try_offline_again : <nl> */ <nl> ata_msleep ( ap , 1 ); <nl>  <nl> + sata_set_spd ( link ); <nl> + <nl> /* <nl> * Now , bring the host controller online again , this can take time <nl> * as PHY reset and communication establishment , 1st D2H FIS and
static void svm_get_segment ( struct kvm_vcpu * vcpu , <nl> if ( seg == VCPU_SREG_CS ) <nl> var -> g = s -> limit > 0xfffff ; <nl>  <nl> + /* <nl> + * Work around a bug where the busy flag in the tr selector <nl> + * isn ' t exposed <nl> + */ <nl> + if ( seg == VCPU_SREG_TR ) <nl> + var -> type |= 0x2 ; <nl> + <nl> var -> unusable = ! var -> present ; <nl> } <nl> 
static int mt9t112_probe ( struct i2c_client * client , <nl> v4l2_i2c_subdev_init (& priv -> subdev , client , & mt9t112_subdev_ops ); <nl>  <nl> ret = mt9t112_camera_probe ( client ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> kfree ( priv ); <nl> + return ret ; <nl> + } <nl>  <nl> /* Cannot fail : using the default supported pixel code */ <nl> mt9t112_set_params ( priv , & rect , V4L2_MBUS_FMT_UYVY8_2X8 );
int ath5k_hw_attach ( struct ath5k_softc * sc ) <nl> ah -> ah_limit_tx_retries = AR5K_INIT_TX_RETRY ; <nl> ah -> ah_software_retry = false ; <nl> ah -> ah_ant_mode = AR5K_ANTMODE_DEFAULT ; <nl> + ah -> ah_noise_floor = - 95 ; /* until first NF calibration is run */ <nl>  <nl> /* <nl> * Find the mac version
int kernfs_rename_ns ( struct kernfs_node * kn , struct kernfs_node * new_parent , <nl> if (! new_name ) <nl> goto out ; <nl>  <nl> - kfree ( kn -> name ); <nl> + if ( kn -> flags & KERNFS_STATIC_NAME ) <nl> + kn -> flags &= ~ KERNFS_STATIC_NAME ; <nl> + else <nl> + kfree ( kn -> name ); <nl> + <nl> kn -> name = new_name ; <nl> } <nl> 
static int mt8173_nor_write_single_byte ( struct mt8173_nor * mt8173_nor , <nl> mt8173_nor_set_addr ( mt8173_nor , addr ); <nl>  <nl> for ( i = 0 ; i < length ; i ++) { <nl> + writeb (* data ++, mt8173_nor -> base + MTK_NOR_WDATA_REG ); <nl> ret = mt8173_nor_execute_cmd ( mt8173_nor , MTK_NOR_PIO_WR_CMD ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> - writeb (* data ++, mt8173_nor -> base + MTK_NOR_WDATA_REG ); <nl> } <nl> return 0 ; <nl> }
static int __devinit xencons_probe ( struct xenbus_device * dev , <nl> if ( devid == 0 ) <nl> return - ENODEV ; <nl>  <nl> - info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL | __GFP_ZERO ); <nl> + info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL ); <nl> if (! info ) <nl> - goto error_nomem ; <nl> + return - ENOMEM ; <nl> dev_set_drvdata (& dev -> dev , info ); <nl> info -> xbdev = dev ; <nl> info -> vtermno = xenbus_devid_to_vtermno ( devid );
static long privcmd_ioctl ( struct file * file , <nl> # ifndef HAVE_ARCH_PRIVCMD_MMAP <nl> static int privcmd_fault ( struct vm_area_struct * vma , struct vm_fault * vmf ) <nl> { <nl> + printk ( KERN_DEBUG " privcmd_fault : vma =% p % lx -% lx , pgoff =% lx , uv =% p \ n ", <nl> + vma , vma -> vm_start , vma -> vm_end , <nl> + vmf -> pgoff , vmf -> virtual_address ); <nl> + <nl> return VM_FAULT_SIGBUS ; <nl> } <nl> 
static int serio_raw_connect ( struct serio * serio , struct serio_driver * drv ) <nl>  <nl> serio_raw -> dev . minor = PSMOUSE_MINOR ; <nl> serio_raw -> dev . name = serio_raw -> name ; <nl> + serio_raw -> dev . dev = & serio -> dev ; <nl> serio_raw -> dev . fops = & serio_raw_fops ; <nl>  <nl> err = misc_register (& serio_raw -> dev );
static int ixgbe_resume ( struct pci_dev * pdev ) <nl>  <nl> pci_wake_from_d3 ( pdev , false ); <nl>  <nl> + rtnl_lock (); <nl> err = ixgbe_init_interrupt_scheme ( adapter ); <nl> + rtnl_unlock (); <nl> if ( err ) { <nl> e_dev_err (" Cannot initialize interrupts for device \ n "); <nl> return err ;
int snd_usb_caiaq_audio_init ( struct snd_usb_caiaqdev * cdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( cdev -> n_streams < 2 ) { <nl> + dev_err ( dev , " bogus number of streams : % d \ n ", cdev -> n_streams ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> ret = snd_pcm_new ( cdev -> chip . card , cdev -> product_name , 0 , <nl> cdev -> n_audio_out , cdev -> n_audio_in , & cdev -> pcm ); <nl> 
static int ioctl_standard_iw_point ( struct iw_point * iwp , unsigned int cmd , <nl> err = - EFAULT ; <nl> goto out ; <nl> } <nl> + <nl> + if ( cmd == SIOCSIWENCODEEXT ) { <nl> + struct iw_encode_ext * ee = ( void *) extra ; <nl> + <nl> + if ( iwp -> length < sizeof (* ee ) + ee -> key_len ) <nl> + return - EFAULT ; <nl> + } <nl> } <nl>  <nl> err = handler ( dev , info , ( union iwreq_data *) iwp , extra );
struct gb_sdio_get_caps_response { <nl>  <nl> /* see possible values below at vdd */ <nl> __le32 ocr ; <nl> - __le16 max_blk_count ; <nl> - __le16 max_blk_size ; <nl> __le32 f_min ; <nl> __le32 f_max ; <nl> + __le16 max_blk_count ; <nl> + __le16 max_blk_size ; <nl> } __packed ; <nl>  <nl> /* set ios request : response has no payload */
void xhci_mem_cleanup ( struct xhci_hcd * xhci ) <nl>  <nl> if ( xhci -> lpm_command ) <nl> xhci_free_command ( xhci , xhci -> lpm_command ); <nl> + xhci -> lpm_command = NULL ; <nl> if ( xhci -> cmd_ring ) <nl> xhci_ring_free ( xhci , xhci -> cmd_ring ); <nl> xhci -> cmd_ring = NULL ;
struct spi_message { <nl> void * state ; <nl> }; <nl>  <nl> + static inline void spi_message_init_no_memset ( struct spi_message * m ) <nl> +{ <nl> + INIT_LIST_HEAD (& m -> transfers ); <nl> +} <nl> + <nl> static inline void spi_message_init ( struct spi_message * m ) <nl> { <nl> memset ( m , 0 , sizeof * m ); <nl> - INIT_LIST_HEAD (& m -> transfers ); <nl> + spi_message_init_no_memset ( m ); <nl> } <nl>  <nl> static inline void
static int xfrm_add_pol_expire ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl> if ( err ) <nl> return err ; <nl>  <nl> + err = verify_policy_dir ( p -> dir ); <nl> + if ( err ) <nl> + return err ; <nl> + <nl> if ( p -> index ) <nl> xp = xfrm_policy_byid ( net , mark , type , p -> dir , p -> index , 0 , & err ); <nl> else {
static int usb_audio_probe ( struct usb_interface * intf , <nl> goto __error ; <nl> } <nl> chip = usb_chip [ i ]; <nl> - dev_set_drvdata (& dev -> dev , chip ); <nl> atomic_inc (& chip -> active ); /* avoid autopm */ <nl> break ; <nl> } <nl> static int usb_audio_probe ( struct usb_interface * intf , <nl> goto __error ; <nl> } <nl> } <nl> + dev_set_drvdata (& dev -> dev , chip ); <nl>  <nl> /* <nl> * For devices with more than one control interface , we assume the
static long gpio_ioctl ( struct file * filp , unsigned int cmd , unsigned long arg ) <nl> if ( cmd == GPIO_GET_CHIPINFO_IOCTL ) { <nl> struct gpiochip_info chipinfo ; <nl>  <nl> + memset (& chipinfo , 0 , sizeof ( chipinfo )); <nl> + <nl> strncpy ( chipinfo . name , dev_name (& gdev -> dev ), <nl> sizeof ( chipinfo . name )); <nl> chipinfo . name [ sizeof ( chipinfo . name )- 1 ] = '\ 0 ';
static void fsg_unbind ( struct usb_configuration * c , struct usb_function * f ) <nl> fsg_common_put ( common ); <nl> usb_free_descriptors ( fsg -> function . descriptors ); <nl> usb_free_descriptors ( fsg -> function . hs_descriptors ); <nl> + usb_free_descriptors ( fsg -> function . ss_descriptors ); <nl> kfree ( fsg ); <nl> } <nl> 
static int __init amd64_edac_init ( void ) <nl> mcis = kzalloc ( amd_nb_num () * sizeof ( mcis [ 0 ]), GFP_KERNEL ); <nl> ecc_stngs = kzalloc ( amd_nb_num () * sizeof ( ecc_stngs [ 0 ]), GFP_KERNEL ); <nl> if (!( mcis && ecc_stngs )) <nl> - goto err_ret ; <nl> + goto err_free ; <nl>  <nl> msrs = msrs_alloc (); <nl> if (! msrs )
struct lgdt3306a_config { <nl> u16 vsb_if_khz ; <nl>  <nl> /* disable i2c repeater - 0 : repeater enabled 1 : repeater disabled */ <nl> - int deny_i2c_rptr : 1 ; <nl> + unsigned int deny_i2c_rptr : 1 ; <nl>  <nl> /* spectral inversion - 0 : disabled 1 : enabled */ <nl> - int spectral_inversion : 1 ; <nl> + unsigned int spectral_inversion : 1 ; <nl>  <nl> enum lgdt3306a_mpeg_mode mpeg_mode ; <nl> enum lgdt3306a_tp_clock_edge tpclk_edge ;
static int pcs_parse_one_pinctrl_entry ( struct pcs_device * pcs , <nl> (* map )-> data . mux . function = np -> name ; <nl>  <nl> if ( pcs -> is_pinconf ) { <nl> - if ( pcs_parse_pinconf ( pcs , np , function , map )) <nl> + res = pcs_parse_pinconf ( pcs , np , function , map ); <nl> + if ( res ) <nl> goto free_pingroups ; <nl> * num_maps = 2 ; <nl> } else {
struct vm_area_struct * vb2_get_vma ( struct vm_area_struct * vma ) <nl>  <nl> return vma_copy ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( vb2_get_vma ); <nl>  <nl> /** <nl> * vb2_put_userptr () - release a userspace virtual memory area
static struct sst_acpi_mach sst_acpi_chv [] = { <nl> & chv_platform_data }, <nl> {" 10EC3276 ", " bytcr_rt5640 ", " intel / fw_sst_22a8 . bin ", " bytcr_rt5640 ", NULL , <nl> & chv_platform_data }, <nl> - <nl> + /* some CHT - T platforms rely on RT5651 , use Baytrail machine driver */ <nl> + {" 10EC5651 ", " bytcr_rt5651 ", " intel / fw_sst_22a8 . bin ", " bytcr_rt5651 ", NULL , <nl> + & chv_platform_data }, <nl> {}, <nl> }; <nl> 
static int egalax_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> struct egalax_data * td ; <nl> struct hid_report * report ; <nl>  <nl> - td = kmalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> + td = kzalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> if (! td ) { <nl> dev_err (& hdev -> dev , " cannot allocate eGalax data \ n "); <nl> return - ENOMEM ;
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> */ <nl> if ( skb_shared ( skb )) { <nl> tmp_skb = skb ; <nl> - skb = skb_copy ( skb , GFP_ATOMIC ); <nl> + skb = skb_clone ( skb , GFP_ATOMIC ); <nl> kfree_skb ( tmp_skb ); <nl>  <nl> if (! skb ) {
static int bcma_get_next_core ( struct bcma_bus * bus , u32 __iomem ** eromptr , <nl> switch ( core -> id . id ) { <nl> case BCMA_CORE_4706_MAC_GBIT_COMMON : <nl> case BCMA_CORE_NS_CHIPCOMMON_B : <nl> + case BCMA_CORE_PMU : <nl> + case BCMA_CORE_GCI : <nl> /* Not used yet : case BCMA_CORE_OOB_ROUTER : */ <nl> break ; <nl> default :
static long kvm_vm_ioctl ( struct file * filp , <nl> routing . nr * sizeof (* entries ))) <nl> goto out_free_irq_routing ; <nl> } <nl> + /* avoid races with KVM_CREATE_IRQCHIP on x86 */ <nl> + mutex_lock (& kvm -> lock ); <nl> r = kvm_set_irq_routing ( kvm , entries , routing . nr , <nl> routing . flags ); <nl> + mutex_unlock (& kvm -> lock ); <nl> out_free_irq_routing : <nl> vfree ( entries ); <nl> break ;
static int newseg ( struct ipc_namespace * ns , struct ipc_params * params ) <nl> if ( size < SHMMIN || size > ns -> shm_ctlmax ) <nl> return - EINVAL ; <nl>  <nl> + if ( numpages << PAGE_SHIFT < size ) <nl> + return - ENOSPC ; <nl> + <nl> if ( ns -> shm_tot + numpages < ns -> shm_tot || <nl> ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> return - ENOSPC ;
static ssize_t rng_dev_read ( struct file * filp , char __user * buf , size_t size , <nl> if ( filp -> f_flags & O_NONBLOCK ) <nl> return ret ? : - EAGAIN ; <nl>  <nl> - if ( need_resched ()){ <nl> - current -> state = TASK_INTERRUPTIBLE ; <nl> - schedule_timeout ( 1 ); <nl> - } <nl> + if ( need_resched ()) <nl> + schedule_timeout_interruptible ( 1 ); <nl> } <nl> else return n ; <nl> if ( signal_pending ( current ))
static int scsi_check_sense ( struct scsi_cmnd * scmd ) <nl> return SUCCESS ; <nl>  <nl> case MEDIUM_ERROR : <nl> + if ( sshdr . asc == 0x11 || /* UNRECOVERED READ ERR */ <nl> + sshdr . asc == 0x13 || /* AMNF DATA FIELD */ <nl> + sshdr . asc == 0x14 ) { /* RECORD NOT FOUND */ <nl> + return SUCCESS ; <nl> + } <nl> return NEEDS_RETRY ; <nl>  <nl> case HARDWARE_ERROR :
static int lov_add_target ( struct obd_device * obd , struct obd_uuid * uuidp , <nl> struct lov_tgt_desc ** newtgts , ** old = NULL ; <nl> __u32 newsize , oldsize = 0 ; <nl>  <nl> - newsize = max ( lov -> lov_tgt_size , ( __u32 ) 2 ); <nl> + newsize = max_t ( __u32 , lov -> lov_tgt_size , 2 ); <nl> while ( newsize < index + 1 ) <nl> newsize = newsize << 1 ; <nl> OBD_ALLOC ( newtgts , sizeof (* newtgts ) * newsize );
static irqreturn_t s3c64xx_dma_irq ( int irq , void * pw ) <nl>  <nl> s3c64xx_dma_bufffdone ( chan , buff , res ); <nl>  <nl> + /* Free the node and update curr , if non - circular queue */ <nl> + if (!( chan -> flags & S3C2410_DMAF_CIRCULAR )) { <nl> + chan -> curr = buff -> next ; <nl> + s3c64xx_dma_freebuff ( buff ); <nl> + } <nl> + <nl> /* Update ' next ' */ <nl> buff = chan -> next ; <nl> if ( chan -> next == chan -> end ) {
int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl> + IEEE80211_ENCRYPT_HEADROOM ; <nl> ndev -> needed_tailroom = IEEE80211_ENCRYPT_TAILROOM ; <nl>  <nl> + ret = dev_alloc_name ( ndev , ndev -> name ); <nl> + if ( ret < 0 ) <nl> + goto fail ; <nl> + <nl> ieee80211_assign_perm_addr ( local , ndev , type ); <nl> memcpy ( ndev -> dev_addr , ndev -> perm_addr , ETH_ALEN ); <nl> SET_NETDEV_DEV ( ndev , wiphy_dev ( local -> hw . wiphy ));
void ieee80211_scan_work ( struct work_struct * work ) <nl>  <nl> rc = __ieee80211_start_scan ( sdata , req ); <nl> if ( rc ) { <nl> + /* need to complete scan in cfg80211 */ <nl> + local -> scan_req = req ; <nl> aborted = true ; <nl> goto out_complete ; <nl> } else
EXPORT_SYMBOL ( get_trigger_mask ); <nl> EXPORT_SYMBOL ( global_trigger_mask ); <nl> # endif <nl>  <nl> + EXPORT_SYMBOL ( clear_page ); <nl> + EXPORT_SYMBOL ( copy_page ); <nl> EXPORT_SYMBOL ( empty_zero_page ); <nl>  <nl> EXPORT_SYMBOL ( pfn_base );
static inline void iounmap ( volatile void __iomem * addr ) <nl> { <nl> } <nl>  <nl> + static inline void __iomem * ioport_map ( unsigned long port , unsigned int nr ) <nl> +{ <nl> + return NULL ; <nl> +} <nl> + <nl> + static inline void ioport_unmap ( void __iomem * p ) <nl> +{ <nl> +} <nl> + <nl> /* <nl> * s390 needs a private implementation of pci_iomap since ioremap with its <nl> * offset parameter isn ' t sufficient . That ' s because BAR spaces are not
static struct usb_request * isp1760_ep_alloc_request ( struct usb_ep * ep , <nl> struct isp1760_request * req ; <nl>  <nl> req = kzalloc ( sizeof (* req ), gfp_flags ); <nl> + if (! req ) <nl> + return NULL ; <nl>  <nl> return & req -> req ; <nl> }
static int ceph_con_in_msg_alloc ( struct ceph_connection * con , int * skip ) <nl> msg = con -> ops -> alloc_msg ( con , hdr , skip ); <nl> mutex_lock (& con -> mutex ); <nl> if ( con -> state != CON_STATE_OPEN ) { <nl> - ceph_msg_put ( msg ); <nl> + if ( msg ) <nl> + ceph_msg_put ( msg ); <nl> return - EAGAIN ; <nl> } <nl> con -> in_msg = msg ;
struct ti_ohci { <nl>  <nl> static inline int cross_bound ( unsigned long addr , unsigned int size ) <nl> { <nl> + if ( size == 0 ) <nl> + return 0 ; <nl> + <nl> if ( size > PAGE_SIZE ) <nl> return 1 ; <nl> 
static void hw_init ( void ) <nl> break ; <nl> } <nl>  <nl> + /* magic required on VX900 for correct modesetting on IGA1 */ <nl> + via_write_reg_mask ( VIACR , 0x45 , 0x00 , 0x01 ); <nl> + <nl> /* probably this should go to the scaling code one day */ <nl> via_write_reg_mask ( VIACR , 0xFD , 0 , 0x80 ); /* VX900 hw scale on IGA2 */ <nl> viafb_write_regx ( scaling_parameters , ARRAY_SIZE ( scaling_parameters ));
static struct resource * res_pci_find_mem ( u_long base , u_long num , <nl>  <nl> static int res_pci_init ( struct pcmcia_socket * s ) <nl> { <nl> - if (! s -> cb_dev || (! s -> features & SS_CAP_PAGE_REGS )) { <nl> + if (! s -> cb_dev || !( s -> features & SS_CAP_PAGE_REGS )) { <nl> dev_err (& s -> dev , " not supported by res_pci \ n "); <nl> return - EOPNOTSUPP ; <nl> }
static struct shash_alg alg = { <nl> . export = md5_export , <nl> . import = md5_import , <nl> . descsize = sizeof ( struct md5_state ), <nl> + . statesize = sizeof ( struct md5_state ), <nl> . base = { <nl> . cra_name = " md5 ", <nl> . cra_flags = CRYPTO_ALG_TYPE_SHASH ,
static int __init ion_dummy_init ( void ) <nl> int i , err ; <nl>  <nl> idev = ion_device_create ( NULL ); <nl> - heaps = kzalloc ( sizeof ( struct ion_heap *) * dummy_ion_pdata . nr , <nl> + heaps = kcalloc ( dummy_ion_pdata . nr , sizeof ( struct ion_heap *), <nl> GFP_KERNEL ); <nl> if (! heaps ) <nl> return - ENOMEM ;
static inline pte_t pfn_pte ( unsigned long pfn , pgprot_t prot ) <nl> sz_bits = 0UL ; <nl> if ( _PAGE_SZBITS_4U != 0UL || _PAGE_SZBITS_4V != 0UL ) { <nl> __asm__ __volatile__ ( <nl> - "\ n661 : sethi % uhi (% 1 ), % 0 \ n " <nl> + "\ n661 : sethi %% uhi (% 1 ), % 0 \ n " <nl> " sllx % 0 , 32 , % 0 \ n " <nl> " . section . sun4v_2insn_patch , \" ax \"\ n " <nl> " . word 661b \ n "
static ssize_t xenbus_file_read ( struct file * filp , <nl> int ret ; <nl>  <nl> mutex_lock (& u -> reply_mutex ); <nl> + again : <nl> while ( list_empty (& u -> read_buffers )) { <nl> mutex_unlock (& u -> reply_mutex ); <nl> if ( filp -> f_flags & O_NONBLOCK ) <nl> static ssize_t xenbus_file_read ( struct file * filp , <nl> struct read_buffer , list ); <nl> } <nl> } <nl> + if ( i == 0 ) <nl> + goto again ; <nl>  <nl> out : <nl> mutex_unlock (& u -> reply_mutex );
static int pppoe_rcv_core ( struct sock * sk , struct sk_buff * skb ) <nl> * can ' t change . <nl> */ <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto abort_kfree ; <nl> + <nl> if ( sk -> sk_state & PPPOX_BOUND ) { <nl> ppp_input (& po -> chan , skb ); <nl> } else if ( sk -> sk_state & PPPOX_RELAY ) {
xfer_exit : <nl>  <nl> static void dma_pl330_rqcb ( struct dma_pl330_desc * desc , enum pl330_op_err err ) <nl> { <nl> - struct dma_pl330_chan * pch = desc -> pchan ; <nl> + struct dma_pl330_chan * pch ; <nl> unsigned long flags ; <nl>  <nl> + if (! desc ) <nl> + return ; <nl> + <nl> + pch = desc -> pchan ; <nl> + <nl> /* If desc aborted */ <nl> if (! pch ) <nl> return ;
void flush_thread ( void ) <nl>  <nl> void start_thread ( struct pt_regs * regs , unsigned long eip , unsigned long esp ) <nl> { <nl> + get_safe_registers ( regs -> regs . gp , regs -> regs . fp ); <nl> PT_REGS_IP ( regs ) = eip ; <nl> PT_REGS_SP ( regs ) = esp ; <nl> current -> ptrace &= ~ PT_DTRACE ;
static int rs_get_tbl_info_from_mcs ( const u32 rate_n_flags , <nl> u8 num_of_ant = get_num_of_ant_from_rate ( rate_n_flags ); <nl> u8 mcs ; <nl>  <nl> - memset ( tbl , 0 , sizeof ( struct iwl_scale_tbl_info )); <nl> + memset ( tbl , 0 , offsetof ( struct iwl_scale_tbl_info , win )); <nl> * rate_idx = iwl_hwrate_to_plcp_idx ( rate_n_flags ); <nl>  <nl> if (* rate_idx == IWL_RATE_INVALID ) {
void falcon_remove_nic ( struct efx_nic * efx ) <nl> struct falcon_nic_data * nic_data = efx -> nic_data ; <nl> int rc ; <nl>  <nl> + /* Remove I2C adapter and clear it in preparation for a retry */ <nl> rc = i2c_del_adapter (& efx -> i2c_adap ); <nl> BUG_ON ( rc ); <nl> + memset (& efx -> i2c_adap , 0 , sizeof ( efx -> i2c_adap )); <nl>  <nl> falcon_remove_spi_devices ( efx ); <nl> falcon_free_buffer ( efx , & efx -> irq_status );
vpif_enum_dv_timings ( struct file * file , void * priv , <nl> int ret ; <nl>  <nl> ret = v4l2_subdev_call ( ch -> sd , video , enum_dv_timings , timings ); <nl> - if ( ret == - ENOIOCTLCMD && ret == - ENODEV ) <nl> + if ( ret == - ENOIOCTLCMD || ret == - ENODEV ) <nl> return - EINVAL ; <nl> return ret ; <nl> }
static int irda_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl>  <nl> err = irda_open_tsap ( self , addr -> sir_lsap_sel , addr -> sir_name ); <nl> if ( err < 0 ) { <nl> - kfree ( self -> ias_obj -> name ); <nl> - kfree ( self -> ias_obj ); <nl> + irias_delete_object ( self -> ias_obj ); <nl> + self -> ias_obj = NULL ; <nl> goto out ; <nl> } <nl> 
static inline struct page * pte_alloc_one ( struct mm_struct * mm , <nl> struct page * pte ; <nl>  <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT , PTE_ORDER ); <nl> - if ( pte ) { <nl> - clear_highpage ( pte ); <nl> - pgtable_page_ctor ( pte ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + clear_highpage ( pte ); <nl> + if (! pgtable_page_ctor ( pte )) { <nl> + __free_page ( pte ); <nl> + return NULL ; <nl> } <nl> return pte ; <nl> }
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> } <nl> btrfs_release_path ( root , path ); <nl>  <nl> - if ( key . offset + datal < off || <nl> + if ( key . offset + datal <= off || <nl> key . offset >= off + len ) <nl> goto next ; <nl> 
long do_shmat ( int shmid , char __user * shmaddr , int shmflg , ulong * raddr , <nl> down_write (& current -> mm -> mmap_sem ); <nl> if ( addr && !( shmflg & SHM_REMAP )) { <nl> err = - EINVAL ; <nl> + if ( addr + size < addr ) <nl> + goto invalid ; <nl> + <nl> if ( find_vma_intersection ( current -> mm , addr , addr + size )) <nl> goto invalid ; <nl> /*
void kvm_lapic_reset ( struct kvm_vcpu * vcpu , bool init_event ) <nl> apic_set_reg ( apic , APIC_DFR , 0xffffffffU ); <nl> apic_set_spiv ( apic , 0xff ); <nl> apic_set_reg ( apic , APIC_TASKPRI , 0 ); <nl> - kvm_apic_set_ldr ( apic , 0 ); <nl> + if (! apic_x2apic_mode ( apic )) <nl> + kvm_apic_set_ldr ( apic , 0 ); <nl> apic_set_reg ( apic , APIC_ESR , 0 ); <nl> apic_set_reg ( apic , APIC_ICR , 0 ); <nl> apic_set_reg ( apic , APIC_ICR2 , 0 );
static int da9055_rtc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> alm_irq = platform_get_irq_byname ( pdev , " ALM "); <nl> - alm_irq = regmap_irq_get_virq ( rtc -> da9055 -> irq_data , alm_irq ); <nl> + if ( alm_irq < 0 ) <nl> + return alm_irq ; <nl> + <nl> ret = devm_request_threaded_irq (& pdev -> dev , alm_irq , NULL , <nl> da9055_rtc_alm_irq , <nl> IRQF_TRIGGER_HIGH | IRQF_ONESHOT ,
static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
static void cpufreq_offline_finish ( unsigned int cpu ) <nl> * since this is a core component , and is essential for the <nl> * subsequent light - weight -> init () to succeed . <nl> */ <nl> - if ( cpufreq_driver -> exit ) <nl> + if ( cpufreq_driver -> exit ) { <nl> cpufreq_driver -> exit ( policy ); <nl> + policy -> freq_table = NULL ; <nl> + } <nl> } <nl>  <nl> /**
static int dgrp_net_release ( struct inode * inode , struct file * file ) <nl>  <nl> spin_unlock_irqrestore (& dgrp_poll_data . poll_lock , lock_flags ); <nl>  <nl> - done : <nl> down (& nd -> nd_net_semaphore ); <nl>  <nl> dgrp_monitor_message ( nd , " Net Close "); <nl>  <nl> up (& nd -> nd_net_semaphore ); <nl>  <nl> + done : <nl> module_put ( THIS_MODULE ); <nl> file -> private_data = NULL ; <nl> return 0 ;
static int cpuhp_invoke_ap_callback ( int cpu , enum cpuhp_state state , <nl> if (! cpu_online ( cpu )) <nl> return 0 ; <nl>  <nl> + /* <nl> + * If we are up and running , use the hotplug thread . For early calls <nl> + * we invoke the thread function directly . <nl> + */ <nl> + if (! st -> thread ) <nl> + return cpuhp_invoke_callback ( cpu , state , cb ); <nl> + <nl> st -> cb_state = state ; <nl> st -> cb = cb ; <nl> /*
static void reparent_leader ( struct task_struct * father , struct task_struct * p , <nl> { <nl> list_move_tail (& p -> sibling , & p -> real_parent -> children ); <nl>  <nl> - if ( task_detached ( p )) <nl> + if ( p -> exit_state == EXIT_DEAD ) <nl> return ; <nl> /* <nl> * If this is a threaded reparent there is no need to
lookup_pi_state ( u32 uval , struct futex_hash_bucket * hb , <nl> if (! p ) <nl> return - ESRCH ; <nl>  <nl> + if (! p -> mm ) { <nl> + put_task_struct ( p ); <nl> + return - EPERM ; <nl> + } <nl> + <nl> /* <nl> * We need to look at the task state flags to figure out , <nl> * whether the task is exiting . To protect against the do_exit
int usb_disable_lpm ( struct usb_device * udev ) <nl> return 0 ; <nl>  <nl> udev -> lpm_disable_count ++; <nl> - if (( udev -> u1_params . timeout == 0 && udev -> u1_params . timeout == 0 )) <nl> + if (( udev -> u1_params . timeout == 0 && udev -> u2_params . timeout == 0 )) <nl> return 0 ; <nl>  <nl> /* If LPM is enabled , attempt to disable it . */
static int tilcdc_irq_postinstall ( struct drm_device * dev ) <nl> struct tilcdc_drm_private * priv = dev -> dev_private ; <nl>  <nl> /* enable FIFO underflow irq : */ <nl> - if ( priv -> rev == 1 ) { <nl> + if ( priv -> rev == 1 ) <nl> tilcdc_set ( dev , LCDC_RASTER_CTRL_REG , LCDC_V1_UNDERFLOW_INT_ENA ); <nl> - } else { <nl> + else <nl> tilcdc_set ( dev , LCDC_INT_ENABLE_SET_REG , LCDC_V2_UNDERFLOW_INT_ENA ); <nl> - } <nl>  <nl> return 0 ; <nl> }
static void __cpuinit init_amd ( struct cpuinfo_x86 * c ) <nl> level = cpuid_eax ( 1 ); <nl> if ( c -> x86 == 15 && (( level >= 0x0f48 && level < 0x0f50 ) || level >= 0x0f58 )) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl> - if ( c -> x86 == 0x10 ) <nl> + if ( c -> x86 == 0x10 || c -> x86 == 0x11 ) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl>  <nl> /* Enable workaround for FXSAVE leak */
have_busfreq : <nl> static int powernow_k6_cpu_exit ( struct cpufreq_policy * policy ) <nl> { <nl> unsigned int i ; <nl> - for ( i = 0 ; i < 8 ; i ++) { <nl> - if ( i == max_multiplier ) <nl> + <nl> + for ( i = 0 ; ( clock_ratio [ i ]. frequency != CPUFREQ_TABLE_END ); i ++) { <nl> + if ( clock_ratio [ i ]. driver_data == max_multiplier ) <nl> powernow_k6_target ( policy , i ); <nl> } <nl> return 0 ;
struct radeon_device { <nl> struct radeon_object * fbdev_robj ; <nl> struct radeon_framebuffer * fbdev_rfb ; <nl> /* Register mmio */ <nl> - unsigned long rmmio_base ; <nl> - unsigned long rmmio_size ; <nl> + resource_size_t rmmio_base ; <nl> + resource_size_t rmmio_size ; <nl> void * rmmio ; <nl> radeon_rreg_t mm_rreg ; <nl> radeon_wreg_t mm_wreg ;
int rt28xx_sta_ioctl ( IN struct net_device * net_dev , <nl> Status = <nl> copy_to_user ( erq -> pointer , pAd -> nickname , <nl> erq -> length ); <nl> + if ( Status ) <nl> + Status = - EFAULT ; <nl> break ; <nl> } <nl> case SIOCGIWRATE : /* get default bit rate ( bps ) */
static struct priority_group * parse_priority_group ( struct arg_set * as , <nl>  <nl> if ( as -> argc < nr_params ) { <nl> ti -> error = " not enough path parameters "; <nl> + r = - EINVAL ; <nl> goto bad ; <nl> } <nl> 
static int snd_line6_pcm_free ( struct snd_device * device ) <nl> */ <nl> static void pcm_disconnect_substream ( struct snd_pcm_substream * substream ) <nl> { <nl> - if ( substream -> runtime && snd_pcm_running ( substream )) <nl> + if ( substream -> runtime && snd_pcm_running ( substream )) { <nl> + snd_pcm_stream_lock_irq ( substream ); <nl> snd_pcm_stop ( substream , SNDRV_PCM_STATE_DISCONNECTED ); <nl> + snd_pcm_stream_unlock_irq ( substream ); <nl> + } <nl> } <nl>  <nl> /*
static void fsl_ssi_config ( struct fsl_ssi_private * ssi_private , bool enable , <nl> * ( online configuration ) <nl> */ <nl> if ( enable ) { <nl> - regmap_update_bits ( regs , CCSR_SSI_SIER , vals -> sier , vals -> sier ); <nl> regmap_update_bits ( regs , CCSR_SSI_SRCR , vals -> srcr , vals -> srcr ); <nl> regmap_update_bits ( regs , CCSR_SSI_STCR , vals -> stcr , vals -> stcr ); <nl> + regmap_update_bits ( regs , CCSR_SSI_SIER , vals -> sier , vals -> sier ); <nl> } else { <nl> u32 sier ; <nl> u32 srcr ;
static inline int free_area ( unsigned long pfn , unsigned long end , char * s ) <nl> static inline void poison_init_mem ( void * s , size_t count ) <nl> { <nl> u32 * p = ( u32 *) s ; <nl> - while (( count = count - 4 )) <nl> + for (; count != 0 ; count -= 4 ) <nl> * p ++ = 0xe7fddef0 ; <nl> } <nl> 
static int set_agc_rf ( struct drxk_state * state , <nl> } <nl>  <nl> /* Set TOP , only if IF - AGC is in AUTO mode */ <nl> - if ( p_if_agc_settings -> ctrl_mode == DRXK_AGC_CTRL_AUTO ) <nl> + if ( p_if_agc_settings -> ctrl_mode == DRXK_AGC_CTRL_AUTO ) { <nl> status = write16 ( state , <nl> SCU_RAM_AGC_IF_IACCU_HI_TGT_MAX__A , <nl> p_agc_cfg -> top ); <nl> if ( status < 0 ) <nl> goto error ; <nl> + } <nl>  <nl> /* Cut - Off current */ <nl> status = write16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A ,
int drbd_merge_bvec ( struct request_queue * q , struct bvec_merge_data * bvm , struct <nl> struct request_queue * const b = <nl> device -> ldev -> backing_bdev -> bd_disk -> queue ; <nl> if ( b -> merge_bvec_fn ) { <nl> + bvm -> bi_bdev = device -> ldev -> backing_bdev ; <nl> backing_limit = b -> merge_bvec_fn ( b , bvm , bvec ); <nl> limit = min ( limit , backing_limit ); <nl> }
static int clcdfb_of_get_mode ( struct device * dev , struct device_node * endpoint , <nl>  <nl> len = clcdfb_snprintf_mode ( NULL , 0 , mode ); <nl> name = devm_kzalloc ( dev , len + 1 , GFP_KERNEL ); <nl> + if (! name ) <nl> + return - ENOMEM ; <nl> + <nl> clcdfb_snprintf_mode ( name , len + 1 , mode ); <nl> mode -> name = name ; <nl> 
static int qat_hal_init_esram ( struct icp_qat_fw_loader_handle * handle ) <nl> unsigned int csr_val ; <nl> int times = 30 ; <nl>  <nl> + if ( handle -> pci_dev -> device == ADF_C3XXX_PCI_DEVICE_ID ) <nl> + return 0 ; <nl> + <nl> csr_val = ADF_CSR_RD ( csr_addr , 0 ); <nl> if (( csr_val & ESRAM_AUTO_TINIT ) && ( csr_val & ESRAM_AUTO_TINIT_DONE )) <nl> return 0 ;
static irqreturn_t skge_intr ( int irq , void * dev_id ) <nl>  <nl> if ( status & IS_HW_ERR ) <nl> skge_error_irq ( hw ); <nl> - <nl> + out : <nl> skge_write32 ( hw , B0_IMSK , hw -> intr_mask ); <nl> skge_read32 ( hw , B0_IMSK ); <nl> - out : <nl> spin_unlock (& hw -> hw_lock ); <nl>  <nl> return IRQ_RETVAL ( handled );
static struct spi_board_info mc13783_dev __initdata = { <nl> . bus_num = 1 , <nl> . chip_select = 0 , <nl> . platform_data = & mc13783_pdata , <nl> + . irq = IOMUX_TO_IRQ ( MX31_PIN_GPIO1_3 ), <nl> }; <nl>  <nl> static int mx31lilly_baseboard ;
static int __init twl4030_pwrbutton_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> free_irq : <nl> - free_irq ( irq , NULL ); <nl> + free_irq ( irq , pwr ); <nl> free_input_dev : <nl> input_free_device ( pwr ); <nl> return err ;
static struct phy * tegra_xusb_padctl_xlate ( struct device * dev , <nl> if ( args -> args_count <= 0 ) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> - if ( index > ARRAY_SIZE ( padctl -> phys )) <nl> + if ( index >= ARRAY_SIZE ( padctl -> phys )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> return padctl -> phys [ index ];
static int __devinit w83627ehf_probe ( struct platform_device * pdev ) <nl> mutex_init (& data -> lock ); <nl> mutex_init (& data -> update_lock ); <nl> data -> name = w83627ehf_device_names [ sio_data -> kind ]; <nl> + data -> bank = 0xff ; /* Force initial bank selection */ <nl> platform_set_drvdata ( pdev , data ); <nl>  <nl> /* 627EHG and 627EHF have 10 voltage inputs ; 627DHG and 667HG have 9 */
int btrfs_commit_transaction ( struct btrfs_trans_handle * trans , <nl> spin_unlock (& root -> fs_info -> trans_lock ); <nl> } <nl>  <nl> - if ( now < cur_trans -> start_time || now - cur_trans -> start_time < 1 ) <nl> + if (! btrfs_test_opt ( root , SSD ) && <nl> + ( now < cur_trans -> start_time || now - cur_trans -> start_time < 1 )) <nl> should_grow = 1 ; <nl>  <nl> do {
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
int cmd_top ( int argc , const char ** argv , const char * prefix ) <nl> event_id [ 0 ] = 0 ; <nl> } <nl>  <nl> + if ( delay_secs < 1 ) <nl> + delay_secs = 1 ; <nl> + <nl> for ( counter = 0 ; counter < nr_counters ; counter ++) { <nl> if ( event_count [ counter ]) <nl> continue ;
static dma_addr_t intel_map_page ( struct device * dev , struct page * page , <nl> struct dma_attrs * attrs ) <nl> { <nl> return __intel_map_single ( dev , page_to_phys ( page ) + offset , size , <nl> - dir , to_pci_dev ( dev )-> dma_mask ); <nl> + dir , * dev -> dma_mask ); <nl> } <nl>  <nl> static void flush_unmaps ( void )
static void ironlake_irq_uninstall ( struct drm_device * dev ) <nl> I915_WRITE ( GTIMR , 0xffffffff ); <nl> I915_WRITE ( GTIER , 0x0 ); <nl> I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> + <nl> + I915_WRITE ( SDEIMR , 0xffffffff ); <nl> + I915_WRITE ( SDEIER , 0x0 ); <nl> + I915_WRITE ( SDEIIR , I915_READ ( SDEIIR )); <nl> } <nl>  <nl> static void i915_driver_irq_uninstall ( struct drm_device * dev )
static struct pci_driver adv_pci1710_pci_driver = { <nl> module_comedi_pci_driver ( adv_pci1710_driver , adv_pci1710_pci_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi : Advantech PCI - 1710 Series Multifunction DAS Cards "); <nl> MODULE_LICENSE (" GPL ");
EXPORT_SYMBOL ( genphy_read_status ); <nl>  <nl> static int genphy_config_init ( struct phy_device * phydev ) <nl> { <nl> - u32 val ; <nl> + int val ; <nl> u32 features ; <nl>  <nl> /* For now , I ' ll claim that the generic driver supports
data_sock_getname ( struct socket * sock , struct sockaddr * addr , <nl> lock_sock ( sk ); <nl>  <nl> * addr_len = sizeof (* maddr ); <nl> + maddr -> family = AF_ISDN ; <nl> maddr -> dev = _pms ( sk )-> dev -> id ; <nl> maddr -> channel = _pms ( sk )-> ch . nr ; <nl> maddr -> sapi = _pms ( sk )-> ch . addr & 0xff ;
static int rv3029c2_rtc_i2c_set_alarm ( struct i2c_client * client , <nl> dev_dbg (& client -> dev , " alarm IRQ armed \ n "); <nl> } else { <nl> /* disable AIE irq */ <nl> - ret = rv3029c2_rtc_i2c_alarm_set_irq ( client , 1 ); <nl> + ret = rv3029c2_rtc_i2c_alarm_set_irq ( client , 0 ); <nl> if ( ret ) <nl> return ret ; <nl> 
static int __init init_balloon_drv ( void ) <nl> return vmbus_driver_register (& balloon_drv ); <nl> } <nl>  <nl> - static void exit_balloon_drv ( void ) <nl> -{ <nl> - <nl> - vmbus_driver_unregister (& balloon_drv ); <nl> -} <nl> - <nl> module_init ( init_balloon_drv ); <nl> - module_exit ( exit_balloon_drv ); <nl>  <nl> MODULE_DESCRIPTION (" Hyper - V Balloon "); <nl> MODULE_VERSION ( HV_DRV_VERSION );
void ieee80211_check_fast_xmit ( struct sta_info * sta ) <nl> goto out ; <nl>  <nl> /* fast - xmit doesn ' t handle fragmentation at all */ <nl> - if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 ) <nl> + if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 && <nl> + ! local -> ops -> set_frag_threshold ) <nl> goto out ; <nl>  <nl> rcu_read_lock ();
nouveau_sgdma_populate ( struct ttm_backend * be , unsigned long num_pages , <nl> return - ENOMEM ; <nl>  <nl> nvbe -> ttm_alloced = kmalloc ( sizeof ( bool ) * num_pages , GFP_KERNEL ); <nl> - if (! nvbe -> ttm_alloced ) <nl> + if (! nvbe -> ttm_alloced ) { <nl> + kfree ( nvbe -> pages ); <nl> + nvbe -> pages = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> nvbe -> nr_pages = 0 ; <nl> while ( num_pages --) {
cfg80211_bss_update ( struct cfg80211_registered_device * dev , <nl> memcpy ( ies , res -> pub . information_elements , ielen ); <nl> found -> ies_allocated = true ; <nl> found -> pub . information_elements = ies ; <nl> + found -> pub . len_information_elements = ielen ; <nl> } <nl> } <nl> }
static int <nl> lpfc_parse_vpd ( struct lpfc_hba * phba , uint8_t * vpd , int len ) <nl> { <nl> uint8_t lenlo , lenhi ; <nl> - uint32_t Length ; <nl> + int Length ; <nl> int i , j ; <nl> int finished = 0 ; <nl> int index = 0 ;
static void ipu_plane_dpms ( struct ipu_plane * ipu_plane , int mode ) <nl>  <nl> ipu_idmac_put ( ipu_plane -> ipu_ch ); <nl> ipu_dmfc_put ( ipu_plane -> dmfc ); <nl> - ipu_dp_put ( ipu_plane -> dp ); <nl> + if ( ipu_plane -> dp ) <nl> + ipu_dp_put ( ipu_plane -> dp ); <nl> } <nl> } <nl> 
static void copy_from_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy <nl>  <nl> static void copy_to_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy_info * p , int dir ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> sel , & xp -> selector , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & xp -> lft , sizeof ( p -> lft )); <nl> memcpy (& p -> curlft , & xp -> curlft , sizeof ( p -> curlft ));
int mei_cl_notify_get ( struct mei_cl * cl , bool block , bool * notify_ev ) <nl>  <nl> dev = cl -> dev ; <nl>  <nl> + if (! dev -> hbm_f_ev_supported ) { <nl> + cl_dbg ( dev , cl , " notifications not supported \ n "); <nl> + return - EOPNOTSUPP ; <nl> + } <nl> + <nl> if (! mei_cl_is_connected ( cl )) <nl> return - ENODEV ; <nl> 
void clear_local_APIC ( void ) <nl> } <nl>  <nl> /* lets not touch this if we didn ' t frob it */ <nl> -# if defined ( CONFIG_X86_MCE_P4THERMAL ) || defined ( X86_MCE_INTEL ) <nl> +# if defined ( CONFIG_X86_MCE_P4THERMAL ) || defined ( CONFIG_X86_MCE_INTEL ) <nl> if ( maxlvt >= 5 ) { <nl> v = apic_read ( APIC_LVTTHMR ); <nl> apic_write ( APIC_LVTTHMR , v | APIC_LVT_MASKED );
static int r820t_signal ( struct dvb_frontend * fe , u16 * strength ) <nl>  <nl> /* A higher gain at LNA means a lower signal strength */ <nl> * strength = ( 45 - rc ) << 4 | 0xff ; <nl> + if (* strength == 0xff ) <nl> + * strength = 0 ; <nl> } else { <nl> * strength = 0 ; <nl> }
static void __wa_populate_buf_in_urb_isoc ( struct wahc * wa , struct wa_xfer * xfer , <nl> wa -> buf_in_urb -> transfer_dma = xfer -> urb -> transfer_dma + <nl> xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. offset ; <nl> wa -> buf_in_urb -> transfer_buffer_length = <nl> - xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. length ; <nl> + xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. actual_length ; <nl> wa -> buf_in_urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> wa -> buf_in_urb -> transfer_buffer = NULL ; <nl> wa -> buf_in_urb -> sg = NULL ;
static void gfs2_write_super ( struct super_block * sb ) <nl> static int gfs2_sync_fs ( struct super_block * sb , int wait ) <nl> { <nl> sb -> s_dirt = 0 ; <nl> - if ( wait ) <nl> + if ( wait && sb -> s_fs_info ) <nl> gfs2_log_flush ( sb -> s_fs_info , NULL ); <nl> return 0 ; <nl> }
befs_find_brun_dblindirect ( struct super_block * sb , <nl> struct buffer_head * indir_block ; <nl> befs_block_run indir_run ; <nl> befs_disk_inode_addr * iaddr_array ; <nl> - struct befs_sb_info * befs_sb = BEFS_SB ( sb ); <nl>  <nl> befs_blocknr_t indir_start_blk = <nl> - data -> max_indirect_range >> befs_sb -> block_shift ; <nl> + data -> max_indirect_range >> BEFS_SB ( sb )-> block_shift ; <nl>  <nl> off_t dbl_indir_off = blockno - indir_start_blk ; <nl> 
static void _rtl92s_phy_get_txpower_index ( struct ieee80211_hw * hw , u8 channel , <nl> /* Read HT 40 OFDM TX power */ <nl> ofdmpowerLevel [ 0 ] = rtlefuse -> txpwrlevel_ht40_2s [ 0 ][ index ]; <nl> ofdmpowerLevel [ 1 ] = rtlefuse -> txpwrlevel_ht40_2s [ 1 ][ index ]; <nl> + } else { <nl> + ofdmpowerLevel [ 0 ] = 0 ; <nl> + ofdmpowerLevel [ 1 ] = 0 ; <nl> } <nl> } <nl> 
static void fc_exch_rrq_resp ( struct fc_seq * sp , struct fc_frame * fp , void * arg ) <nl> if ( IS_ERR ( fp )) { <nl> int err = PTR_ERR ( fp ); <nl>  <nl> - if ( err == - FC_EX_CLOSED ) <nl> + if ( err == - FC_EX_CLOSED || err == - FC_EX_TIMEOUT ) <nl> goto cleanup ; <nl> FC_DBG (" Cannot process RRQ , because of frame error % d \ n ", err ); <nl> return ;
static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
static int uio_pdrv_remove ( struct platform_device * pdev ) <nl>  <nl> uio_unregister_device ( pdata -> uioinfo ); <nl>  <nl> + kfree ( pdata ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int ibmvscsis_drop_nexus ( struct ibmvscsis_tport * tport ) <nl> /* <nl> * Release the SCSI I_T Nexus to the emulated ibmvscsis Target Port <nl> */ <nl> + target_wait_for_sess_cmds ( se_sess ); <nl> + transport_deregister_session_configfs ( se_sess ); <nl> transport_deregister_session ( se_sess ); <nl> tport -> ibmv_nexus = NULL ; <nl> kfree ( nexus );
int kvm_vcpu_ioctl_config_tlb ( struct kvm_vcpu * vcpu , <nl>  <nl> num_pages = DIV_ROUND_UP ( cfg -> array + array_len - 1 , PAGE_SIZE ) - <nl> cfg -> array / PAGE_SIZE ; <nl> - pages = kmalloc ( sizeof ( struct page *) * num_pages , GFP_KERNEL ); <nl> + pages = kmalloc_array ( num_pages , sizeof (* pages ), GFP_KERNEL ); <nl> if (! pages ) <nl> return - ENOMEM ; <nl> 
int ocfs2_fiemap ( struct inode * inode , struct fiemap_extent_info * fieinfo , <nl> & hole_size , & rec , & is_last ); <nl> if ( ret ) { <nl> mlog_errno ( ret ); <nl> - goto out ; <nl> + goto out_unlock ; <nl> } <nl>  <nl> if ( rec . e_blkno == 0ULL ) {
struct io_context { <nl> /* <nl> * For request batching <nl> */ <nl> - unsigned long last_waited ; /* Time last woken after wait for request */ <nl> int nr_batch_requests ; /* Number of requests left in the batch */ <nl> + unsigned long last_waited ; /* Time last woken after wait for request */ <nl>  <nl> struct radix_tree_root radix_root ; <nl> struct hlist_head cic_list ;
static int s3c_fb_check_var ( struct fb_var_screeninfo * var , <nl>  <nl> default : <nl> dev_err ( sfb -> dev , " invalid bpp \ n "); <nl> + return - EINVAL ; <nl> } <nl>  <nl> dev_dbg ( sfb -> dev , "% s : verified parameters \ n ", __func__ );
static void lm8333_key_handler ( struct lm8333 * lm8333 ) <nl> return ; <nl> } <nl>  <nl> - for ( i = 0 ; keys [ i ] && i < LM8333_FIFO_TRANSFER_SIZE ; i ++) { <nl> + for ( i = 0 ; i < LM8333_FIFO_TRANSFER_SIZE && keys [ i ]; i ++) { <nl> pressed = keys [ i ] & 0x80 ; <nl> code = keys [ i ] & 0x7f ; <nl> 
static struct of_device * __init scan_one_device ( struct device_node * dp , <nl> if (! parent ) <nl> strcpy ( op -> dev . bus_id , " root "); <nl> else <nl> - strcpy ( op -> dev . bus_id , dp -> path_component_name ); <nl> + sprintf ( op -> dev . bus_id , "% s @% 08x ", dp -> name , dp -> node ); <nl>  <nl> if ( of_device_register ( op )) { <nl> printk ("% s : Could not register of device .\ n ",
static int __init r8a66597_probe ( struct platform_device * pdev ) <nl>  <nl> r8a66597 -> ep0_req = r8a66597_alloc_request (& r8a66597 -> ep [ 0 ]. ep , <nl> GFP_KERNEL ); <nl> - if ( r8a66597 -> ep0_req == NULL ) <nl> + if ( r8a66597 -> ep0_req == NULL ) { <nl> + ret = - ENOMEM ; <nl> goto clean_up3 ; <nl> + } <nl> r8a66597 -> ep0_req -> complete = nop_completion ; <nl>  <nl> ret = usb_add_gadget_udc (& pdev -> dev , & r8a66597 -> gadget );
static int spi_qup_remove ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> ret = pm_runtime_get_sync (& pdev -> dev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> return ret ; <nl>  <nl> ret = spi_qup_set_state ( controller , QUP_STATE_RESET );
static struct pci_device_id azx_ids [] = { <nl> { PCI_DEVICE ( 0x10de , 0x044b ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x055c ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x055d ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> + { PCI_DEVICE ( 0x10de , 0x0590 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0774 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0775 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0776 ), . driver_data = AZX_DRIVER_NVIDIA },
static int f2fs_write_data_pages ( struct address_space * mapping , <nl> available_free_memory ( sbi , DIRTY_DENTS )) <nl> goto skip_write ; <nl>  <nl> + /* during POR , we don ' t need to trigger writepage at all . */ <nl> + if ( unlikely ( is_sbi_flag_set ( sbi , SBI_POR_DOING ))) <nl> + goto skip_write ; <nl> + <nl> diff = nr_pages_to_write ( sbi , DATA , wbc ); <nl>  <nl> if (! S_ISDIR ( inode -> i_mode )) {
static int __init dm_bufio_init ( void ) <nl> { <nl> __u64 mem ; <nl>  <nl> + dm_bufio_allocated_kmem_cache = 0 ; <nl> + dm_bufio_allocated_get_free_pages = 0 ; <nl> + dm_bufio_allocated_vmalloc = 0 ; <nl> + dm_bufio_current_allocated = 0 ; <nl> + <nl> memset (& dm_bufio_caches , 0 , sizeof dm_bufio_caches ); <nl> memset (& dm_bufio_cache_names , 0 , sizeof dm_bufio_cache_names ); <nl> 
unsigned fuse_file_poll ( struct file * file , poll_table * wait ) <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req )) <nl> - return PTR_ERR ( req ); <nl> + return POLLERR ; <nl>  <nl> req -> in . h . opcode = FUSE_POLL ; <nl> req -> in . h . nodeid = ff -> nodeid ;
static int cx22700_set_tps ( struct cx22700_state * state , <nl>  <nl> cx22700_writereg ( state , 0x04 , val ); <nl>  <nl> + if ( p -> code_rate_HP - FEC_1_2 >= sizeof ( fec_tab ) || <nl> + p -> code_rate_LP - FEC_1_2 >= sizeof ( fec_tab )) <nl> + return - EINVAL ; <nl> val = fec_tab [ p -> code_rate_HP - FEC_1_2 ] << 3 ; <nl> val |= fec_tab [ p -> code_rate_LP - FEC_1_2 ]; <nl> 
void ceph_handle_caps ( struct ceph_mds_session * session , <nl> cap -> cap_id = le64_to_cpu ( h -> cap_id ); <nl> cap -> mseq = mseq ; <nl> cap -> seq = seq ; <nl> + cap -> issue_seq = seq ; <nl> spin_lock (& session -> s_cap_lock ); <nl> list_add_tail (& cap -> session_caps , <nl> & session -> s_cap_releases );
struct kvm_pit * kvm_create_pit ( struct kvm * kvm , u32 flags ) <nl> pit -> wq = create_singlethread_workqueue (" kvm - pit - wq "); <nl> if (! pit -> wq ) { <nl> mutex_unlock (& pit -> pit_state . lock ); <nl> + kvm_free_irq_source_id ( kvm , pit -> irq_source_id ); <nl> kfree ( pit ); <nl> return NULL ; <nl> }
static int v9fs_vfs_readlink ( struct dentry * dentry , char __user * buffer , <nl> int ret ; <nl> char * link = __getname (); <nl>  <nl> - if ( strlen ( link ) < buflen ) <nl> - buflen = strlen ( link ); <nl> + if ( buflen > PATH_MAX ) <nl> + buflen = PATH_MAX ; <nl>  <nl> dprintk ( DEBUG_VFS , " dentry : % s (% p )\ n ", dentry -> d_iname , dentry ); <nl> 
static int mount_ubifs ( struct ubifs_info * c ) <nl> if ( err ) <nl> goto out_orphans ; <nl> err = ubifs_rcvry_gc_commit ( c ); <nl> + if ( err ) <nl> + goto out_orphans ; <nl> } else { <nl> err = take_gc_lnum ( c ); <nl> if ( err )
static unsigned int pxa2xx_ssp_get_clk_div ( struct driver_data * drv_data , <nl> switch ( drv_data -> ssp_type ) { <nl> case QUARK_X1000_SSP : <nl> clk_div = quark_x1000_get_clk_div ( rate , & chip -> dds_rate ); <nl> + break ; <nl> default : <nl> clk_div = ssp_get_clk_div ( drv_data , rate ); <nl> + break ; <nl> } <nl> return clk_div << 8 ; <nl> }
int datagram_send_ctl ( struct net * net , struct sock * sk , <nl>  <nl> if ( addr_type != IPV6_ADDR_ANY ) { <nl> int strict = __ipv6_addr_src_scope ( addr_type ) <= IPV6_ADDR_SCOPE_LINKLOCAL ; <nl> - if (! inet_sk ( sk )-> transparent && <nl> + if (!( inet_sk ( sk )-> freebind || inet_sk ( sk )-> transparent ) && <nl> ! ipv6_chk_addr ( net , & src_info -> ipi6_addr , <nl> strict ? dev : NULL , 0 )) <nl> err = - EINVAL ;
listxattr ( struct dentry * d , char __user * list , size_t size ) <nl> error = d -> d_inode -> i_op -> listxattr ( d , klist , size ); <nl> } else { <nl> error = security_inode_listsecurity ( d -> d_inode , klist , size ); <nl> - if ( size && error >= size ) <nl> + if ( size && error > size ) <nl> error = - ERANGE ; <nl> } <nl> if ( error > 0 ) {
svc_tcp_accept ( struct svc_sock * svsk ) <nl> serv -> sv_name ); <nl> printk ( KERN_NOTICE <nl> "% s : last TCP connect from % s \ n ", <nl> - serv -> sv_name , buf ); <nl> + serv -> sv_name , __svc_print_addr ( sin , <nl> + buf , sizeof ( buf ))); <nl> } <nl> /* <nl> * Always select the oldest socket . It ' s not fair ,
nfulnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl> ret = - EINVAL ; <nl> break ; <nl> } <nl> + <nl> + if (! inst ) <nl> + goto out ; <nl> } else { <nl> if (! inst ) { <nl> UDEBUG (" no config command , and no instance for " <nl> nfulnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl>  <nl> out_put : <nl> instance_put ( inst ); <nl> + out : <nl> return ret ; <nl> } <nl> 
static void dma_pte_free_level ( struct dmar_domain * domain , int level , <nl>  <nl> /* If range covers entire pagetable , free it */ <nl> if (!( start_pfn > level_pfn || <nl> - last_pfn < level_pfn + level_size ( level ))) { <nl> + last_pfn < level_pfn + level_size ( level ) - 1 )) { <nl> dma_clear_pte ( pte ); <nl> domain_flush_cache ( domain , pte , sizeof (* pte )); <nl> free_pgtable_page ( level_pte );
static int max31790_read_pwm ( struct device * dev , u32 attr , int channel , <nl> long * val ) <nl> { <nl> struct max31790_data * data = max31790_update_device ( dev ); <nl> - u8 fan_config = data -> fan_config [ channel ]; <nl> + u8 fan_config ; <nl>  <nl> if ( IS_ERR ( data )) <nl> return PTR_ERR ( data ); <nl>  <nl> + fan_config = data -> fan_config [ channel ]; <nl> + <nl> switch ( attr ) { <nl> case hwmon_pwm_input : <nl> * val = data -> pwm [ channel ] >> 8 ;
static int logger_release ( struct inode * ignored , struct file * file ) <nl> { <nl> if ( file -> f_mode & FMODE_READ ) { <nl> struct logger_reader * reader = file -> private_data ; <nl> + struct logger_log * log = reader -> log ; <nl> + <nl> + mutex_lock (& log -> mutex ); <nl> list_del (& reader -> list ); <nl> + mutex_unlock (& log -> mutex ); <nl> + <nl> kfree ( reader ); <nl> } <nl> 
static void <nl> activate_substream ( struct snd_usb_caiaqdev * dev , <nl> struct snd_pcm_substream * sub ) <nl> { <nl> + spin_lock (& dev -> spinlock ); <nl> + <nl> if ( sub -> stream == SNDRV_PCM_STREAM_PLAYBACK ) <nl> dev -> sub_playback [ sub -> number ] = sub ; <nl> else <nl> dev -> sub_capture [ sub -> number ] = sub ; <nl> + <nl> + spin_unlock (& dev -> spinlock ); <nl> } <nl>  <nl> static void
extern struct mlog_bits mlog_and_bits , mlog_not_bits ; <nl> # define mlog_errno ( st ) do { \ <nl> int _st = ( st ); \ <nl> if ( _st != - ERESTARTSYS && _st != - EINTR && \ <nl> - _st != AOP_TRUNCATED_PAGE ) \ <nl> + _st != AOP_TRUNCATED_PAGE && _st != - ENOSPC ) \ <nl> mlog ( ML_ERROR , " status = % lld \ n ", ( long long ) _st ); \ <nl> } while ( 0 ) <nl> 
static int p54_tx ( struct ieee80211_hw * dev , struct sk_buff * skb ) <nl> struct p54_common * priv = dev -> priv ; <nl> struct p54_hdr * hdr ; <nl> struct p54_tx_data * txhdr ; <nl> - size_t padding , len , tim_len ; <nl> + size_t padding , len , tim_len = 0 ; <nl> int i , j , ridx ; <nl> u16 hdr_flags = 0 , aid = 0 ; <nl> u8 rate , queue ;
static int corgi_bl_update_status ( struct backlight_device * bd ) <nl>  <nl> if ( corgibl_flags & CORGIBL_SUSPENDED ) <nl> intensity = 0 ; <nl> - if ( corgibl_flags & CORGIBL_BATTLOW ) <nl> - intensity &= lcd -> limit_mask ; <nl> + <nl> + if (( corgibl_flags & CORGIBL_BATTLOW ) && intensity > lcd -> limit_mask ) <nl> + intensity = lcd -> limit_mask ; <nl>  <nl> return corgi_bl_set_intensity ( lcd , intensity ); <nl> }
TRACE_EVENT ( iwlwifi_dev_tx , <nl> __entry -> framelen = buf0_len + buf1_len ; <nl> memcpy ( __get_dynamic_array ( tfd ), tfd , tfdlen ); <nl> memcpy ( __get_dynamic_array ( buf0 ), buf0 , buf0_len ); <nl> - memcpy ( __get_dynamic_array ( buf1 ), buf1 , buf0_len ); <nl> + memcpy ( __get_dynamic_array ( buf1 ), buf1 , buf1_len ); <nl> ), <nl> TP_printk ("[% p ] TX %. 2x (% zu bytes )", <nl> __entry -> priv ,
static bool check_device_tree ( struct ath6kl * ar ) <nl> board_filename , ret ); <nl> continue ; <nl> } <nl> + of_node_put ( node ); <nl> return true ; <nl> } <nl> return false ;
static void skl_power_well_post_enable ( struct drm_i915_private * dev_priv , <nl> 1 << PIPE_C | 1 << PIPE_B ); <nl> } <nl>  <nl> - if ( power_well -> data == SKL_DISP_PW_1 ) <nl> + if ( power_well -> data == SKL_DISP_PW_1 ) { <nl> + intel_prepare_ddi ( dev ); <nl> gen8_irq_power_well_post_enable ( dev_priv , 1 << PIPE_A ); <nl> + } <nl> } <nl>  <nl> static void hsw_set_power_well ( struct drm_i915_private * dev_priv ,
static void eeh_handle_special_event ( void ) <nl>  <nl> /* Notify all devices to be down */ <nl> eeh_pe_state_clear ( pe , EEH_PE_PRI_BUS ); <nl> + eeh_pe_dev_traverse ( pe , <nl> + eeh_report_failure , NULL ); <nl> bus = eeh_pe_bus_get ( phb_pe ); <nl> if (! bus ) { <nl> pr_err ("% s : Cannot find PCI bus for " <nl> static void eeh_handle_special_event ( void ) <nl> pe -> addr ); <nl> break ; <nl> } <nl> - eeh_pe_dev_traverse ( pe , <nl> - eeh_report_failure , NULL ); <nl> pci_hp_remove_devices ( bus ); <nl> } <nl> pci_unlock_rescan_remove ();
const struct imx_imx_ssi_data imx35_imx_ssi_data [] __initconst = { <nl> # ifdef CONFIG_SOC_IMX51 <nl> const struct imx_imx_ssi_data imx51_imx_ssi_data [] __initconst = { <nl> # define imx51_imx_ssi_data_entry ( _id , _hwid ) \ <nl> - imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_4K ) <nl> + imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_16K ) <nl> imx51_imx_ssi_data_entry ( 0 , 1 ), <nl> imx51_imx_ssi_data_entry ( 1 , 2 ), <nl> imx51_imx_ssi_data_entry ( 2 , 3 ),
static void b43_sdio_remove ( struct sdio_func * func ) <nl> struct b43_sdio * sdio = sdio_get_drvdata ( func ); <nl>  <nl> ssb_bus_unregister (& sdio -> ssb ); <nl> + sdio_claim_host ( func ); <nl> sdio_disable_func ( func ); <nl> + sdio_release_host ( func ); <nl> kfree ( sdio ); <nl> sdio_set_drvdata ( func , NULL ); <nl> }
static void amdgpu_gem_va_update_vm ( struct amdgpu_device * adev , <nl> if ( domain == AMDGPU_GEM_DOMAIN_CPU ) <nl> goto error_unreserve ; <nl> } <nl> + r = amdgpu_vm_update_page_directory ( adev , bo_va -> vm ); <nl> + if ( r ) <nl> + goto error_unreserve ; <nl>  <nl> r = amdgpu_vm_clear_freed ( adev , bo_va -> vm ); <nl> if ( r )
int dmar_set_interrupt ( struct intel_iommu * iommu ) <nl> return 0 ; <nl>  <nl> irq = create_irq (); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> pr_err (" IOMMU : no free vectors \ n "); <nl> return - EINVAL ; <nl> }
int vmbus_open ( struct vmbus_channel * newchannel , u32 send_ringbuffer_size , <nl> ret = vmbus_post_msg ( open_msg , <nl> sizeof ( struct vmbus_channel_open_channel )); <nl>  <nl> - if ( ret != 0 ) <nl> + if ( ret != 0 ) { <nl> + err = ret ; <nl> goto error1 ; <nl> + } <nl>  <nl> t = wait_for_completion_timeout (& open_info -> waitevent , 5 * HZ ); <nl> if ( t == 0 ) {
static int p9_mux_poll_start ( struct p9_conn * m ) <nl> } <nl>  <nl> if ( i >= ARRAY_SIZE ( p9_mux_poll_tasks )) { <nl> - if ( vptlast == NULL ) <nl> + if ( vptlast == NULL ) { <nl> + mutex_unlock (& p9_mux_task_lock ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> P9_DPRINTK ( P9_DEBUG_MUX , " put in proc % d \ n ", i ); <nl> list_add (& m -> mux_list , & vptlast -> mux_list );
static int perf_top_config ( const char * var , const char * value , void * cb ) <nl>  <nl> if (! strcmp ( var , " top . call - graph ")) <nl> return record_parse_callchain ( value , & top -> record_opts ); <nl> + if (! strcmp ( var , " top . children ")) { <nl> + symbol_conf . cumulate_callchain = perf_config_bool ( var , value ); <nl> + return 0 ; <nl> + } <nl>  <nl> return perf_default_config ( var , value , cb ); <nl> }
static int net2280_stop ( struct usb_gadget * _gadget , <nl> device_remove_file (& dev -> pdev -> dev , & dev_attr_function ); <nl> device_remove_file (& dev -> pdev -> dev , & dev_attr_queues ); <nl>  <nl> - DEBUG ( dev , " unregistered driver '% s '\ n ", driver -> driver . name ); <nl> + DEBUG ( dev , " unregistered driver '% s '\ n ", <nl> + driver ? driver -> driver . name : ""); <nl> + <nl> return 0 ; <nl> } <nl> 
static void ixgbe_free_irq ( struct ixgbe_adapter * adapter ) <nl>  <nl> i --; <nl> for (; i >= 0 ; i --) { <nl> + /* free only the irqs that were actually requested */ <nl> + if (! adapter -> q_vector [ i ]-> rxr_count && <nl> + ! adapter -> q_vector [ i ]-> txr_count ) <nl> + continue ; <nl> + <nl> free_irq ( adapter -> msix_entries [ i ]. vector , <nl> adapter -> q_vector [ i ]); <nl> }
static int als_wait_for_data_ready ( struct device * dev ) <nl> ret = i2c_smbus_read_byte_data ( client , 0x86 ); <nl> } while (!( ret & 0x80 ) && retry --); <nl>  <nl> - if (! retry ) { <nl> + if ( retry < 0 ) { <nl> dev_warn ( dev , " timeout waiting for data ready \ n "); <nl> return - ETIMEDOUT ; <nl> }
void blk_mq_delay_queue ( struct blk_mq_hw_ctx * hctx , unsigned long msecs ) <nl> if ( unlikely (! blk_mq_hw_queue_mapped ( hctx ))) <nl> return ; <nl>  <nl> + blk_mq_stop_hw_queue ( hctx ); <nl> kblockd_schedule_delayed_work_on ( blk_mq_hctx_next_cpu ( hctx ), <nl> & hctx -> delay_work , msecs_to_jiffies ( msecs )); <nl> }
static enum dvbfe_search cxd2820r_search ( struct dvb_frontend * fe ) <nl> /* frontend lock wait loop count */ <nl> switch ( priv -> delivery_system ) { <nl> case SYS_DVBT : <nl> + case SYS_DVBC_ANNEX_A : <nl> i = 20 ; <nl> break ; <nl> case SYS_DVBT2 :
int i40e_ndo_get_vf_config ( struct net_device * netdev , <nl> else <nl> ivi -> linkstate = IFLA_VF_LINK_STATE_DISABLE ; <nl> ivi -> spoofchk = vf -> spoofchk ; <nl> + ivi -> trusted = vf -> trusted ; <nl> ret = 0 ; <nl>  <nl> error_param :
static int ahash_prepare_alg ( struct ahash_alg * alg ) <nl> struct crypto_alg * base = & alg -> halg . base ; <nl>  <nl> if ( alg -> halg . digestsize > PAGE_SIZE / 8 || <nl> - alg -> halg . statesize > PAGE_SIZE / 8 ) <nl> + alg -> halg . statesize > PAGE_SIZE / 8 || <nl> + alg -> halg . statesize == 0 ) <nl> return - EINVAL ; <nl>  <nl> base -> cra_type = & crypto_ahash_type ;
static ssize_t port_fops_read ( struct file * filp , char __user * ubuf , <nl> if ( ret < 0 ) <nl> return ret ; <nl> } <nl> + /* Port got hot - unplugged . */ <nl> + if (! port -> guest_connected ) <nl> + return - ENODEV ; <nl> /* <nl> * We could ' ve received a disconnection message while we were <nl> * waiting for more data .
static void ieee80211_do_stop ( struct ieee80211_sub_if_data * sdata , <nl> rcu_barrier (); <nl> sta_info_flush_cleanup ( sdata ); <nl>  <nl> - skb_queue_purge (& sdata -> skb_queue ); <nl> - <nl> /* <nl> * Free all remaining keys , there shouldn ' t be any , <nl> * except maybe in WDS mode ? <nl> */ <nl> ieee80211_free_keys ( sdata ); <nl>  <nl> + /* fall through */ <nl> + case NL80211_IFTYPE_AP : <nl> + skb_queue_purge (& sdata -> skb_queue ); <nl> + <nl> drv_remove_interface_debugfs ( local , sdata ); <nl>  <nl> if ( going_down )
static __be32 nfsd_set_fh_dentry ( struct svc_rqst * rqstp , struct svc_fh * fhp ) <nl> * fix that case easily . <nl> */ <nl> struct cred * new = prepare_creds (); <nl> - if (! new ) <nl> - return nfserrno (- ENOMEM ); <nl> + if (! new ) { <nl> + error = nfserrno (- ENOMEM ); <nl> + goto out ; <nl> + } <nl> new -> cap_effective = <nl> cap_raise_nfsd_set ( new -> cap_effective , <nl> new -> cap_permitted );
static int __devinit i2o_pci_probe ( struct pci_dev * pdev , <nl> i2o_pci_free ( c ); <nl>  <nl> free_controller : <nl> - i2o_iop_free ( c ); <nl> put_device ( c -> device . parent ); <nl> + i2o_iop_free ( c ); <nl>  <nl> disable : <nl> pci_disable_device ( pdev );
static int open ( struct tty_struct * tty , struct file * filp ) <nl> if ( info -> port . count == 1 ) { <nl> /* 1st open on this device , init hardware */ <nl> retval = startup ( info ); <nl> - if ( retval < 0 ) <nl> + if ( retval < 0 ) { <nl> + mutex_unlock (& info -> port . mutex ); <nl> goto cleanup ; <nl> + } <nl> } <nl> mutex_unlock (& info -> port . mutex ); <nl> retval = block_til_ready ( tty , filp , info );
static int i2c_register_adapter ( struct i2c_adapter * adap ) <nl>  <nl> dev_dbg (& adap -> dev , " adapter [% s ] registered \ n ", adap -> name ); <nl>  <nl> + pm_runtime_no_callbacks (& adap -> dev ); <nl> + <nl> # ifdef CONFIG_I2C_COMPAT <nl> res = class_compat_create_link ( i2c_adapter_compat_class , & adap -> dev , <nl> adap -> dev . parent );
static struct sg_table * omap_gem_map_dma_buf ( <nl> /* this should be after _get_paddr () to ensure we have pages attached */ <nl> omap_gem_dma_sync ( obj , dir ); <nl>  <nl> - out : <nl> - if ( ret ) <nl> - return ERR_PTR ( ret ); <nl> return sg ; <nl> + out : <nl> + kfree ( sg ); <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> static void omap_gem_unmap_dma_buf ( struct dma_buf_attachment * attachment ,
static int bsd_compress ( void * state , struct sk_buff * skb_in , struct sk_buff * skb <nl> db -> n_bits ++; <nl>  <nl> /* If output length is too large then this is an incompressible frame . */ <nl> - if (! skb_out || ( skb_out && skb_out -> len >= skb_in -> len )) { <nl> + if (! skb_out || skb_out -> len >= skb_in -> len ) { <nl> ++ db -> incomp_count ; <nl> db -> incomp_bytes += isize ; <nl> return 0 ;
static int ioc4_serial_remove_one ( struct ioc4_driver_data * idd ) <nl> if ( soft ) { <nl> free_irq ( control -> ic_irq , soft ); <nl> if ( soft -> is_ioc4_serial_addr ) { <nl> + iounmap ( soft -> is_ioc4_serial_addr ); <nl> release_region (( unsigned long ) <nl> soft -> is_ioc4_serial_addr , <nl> sizeof ( struct ioc4_serial )); <nl> out4 : <nl> out3 : <nl> kfree ( control ); <nl> out2 : <nl> + if ( serial ) <nl> + iounmap ( serial ); <nl> release_region ( tmp_addr1 , sizeof ( struct ioc4_serial )); <nl> out1 : <nl> 
static int get_strength ( struct drxk_state * state , u64 * strength ) <nl> return status ; <nl>  <nl> /* SCU c . o . c . */ <nl> - read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> + status = read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> if ( status < 0 ) <nl> return status ; <nl> 
static int musb_probe ( struct platform_device * pdev ) <nl> struct resource * iomem ; <nl> void __iomem * base ; <nl>  <nl> - iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - if (! iomem || irq <= 0 ) <nl> + if ( irq <= 0 ) <nl> return - ENODEV ; <nl>  <nl> + iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> base = devm_ioremap_resource ( dev , iomem ); <nl> if ( IS_ERR ( base )) <nl> return PTR_ERR ( base );
static int ir_rc5_decode ( struct rc_dev * dev , struct ir_raw_event ev ) <nl> u32 scancode ; <nl> enum rc_type protocol ; <nl>  <nl> - if (!( dev -> enabled_protocols & ( RC_BIT_RC5 | RC_BIT_RC5X ))) <nl> + if (!( dev -> enabled_protocols & ( RC_BIT_RC5 | RC_BIT_RC5X | RC_BIT_RC5_SZ ))) <nl> return 0 ; <nl>  <nl> if (! is_timing_event ( ev )) {
static irqreturn_t interrupt_pcl816 ( int irq , void * d ) <nl> } <nl>  <nl> outb ( 0 , dev -> iobase + PCL816_CLRINT ); /* clear INT request */ <nl> - if (! dev -> irq || ! devpriv -> ai_cmd_running || ! devpriv -> int816_mode ) { <nl> + if (! devpriv -> ai_cmd_running || ! devpriv -> int816_mode ) { <nl> if ( devpriv -> irq_was_now_closed ) { <nl> devpriv -> irq_was_now_closed = 0 ; <nl> /* comedi_error ( dev ," last IRQ .."); */
static int cramfs_readpage ( struct file * file , struct page * page ) <nl> pgdata = kmap ( page ); <nl> if ( compr_len == 0 ) <nl> ; /* hole */ <nl> + else if ( compr_len > ( PAGE_CACHE_SIZE << 1 )) <nl> + printk ( KERN_ERR " cramfs : bad compressed blocksize % u \ n ", compr_len ); <nl> else { <nl> mutex_lock (& read_mutex ); <nl> bytes_filled = cramfs_uncompress_block ( pgdata ,
err_out_unregister : <nl>  <nl> err_unlock_policy : <nl> unlock_policy_rwsem_write ( cpu ); <nl> + free_cpumask_var ( policy -> related_cpus ); <nl> err_free_cpumask : <nl> free_cpumask_var ( policy -> cpus ); <nl> err_free_policy :
nvkm_ltc_tags_clear ( struct nvkm_ltc * ltc , u32 first , u32 count ) <nl>  <nl> BUG_ON (( first > limit ) || ( limit >= ltc -> num_tags )); <nl>  <nl> + mutex_lock (& ltc -> subdev . mutex ); <nl> ltc -> func -> cbc_clear ( ltc , first , limit ); <nl> ltc -> func -> cbc_wait ( ltc ); <nl> + mutex_unlock (& ltc -> subdev . mutex ); <nl> } <nl>  <nl> int
static int pl330_dma_device_slave_caps ( struct dma_chan * dchan , <nl> caps -> directions = BIT ( DMA_DEV_TO_MEM ) | BIT ( DMA_MEM_TO_DEV ); <nl> caps -> cmd_pause = false ; <nl> caps -> cmd_terminate = true ; <nl> + caps -> residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR ; <nl>  <nl> return 0 ; <nl> }
void inet_initpeers ( void ) __init ; <nl> static inline void inetpeer_set_addr_v4 ( struct inetpeer_addr * iaddr , __be32 ip ) <nl> { <nl> iaddr -> a4 . addr = ip ; <nl> + iaddr -> a4 . vif = 0 ; <nl> iaddr -> family = AF_INET ; <nl> } <nl> 
rockchip_drm_framebuffer_init ( struct drm_device * dev , <nl>  <nl> rockchip_fb = rockchip_fb_alloc ( dev , mode_cmd , & obj , 1 ); <nl> if ( IS_ERR ( rockchip_fb )) <nl> - return NULL ; <nl> + return ERR_CAST ( rockchip_fb ); <nl>  <nl> return & rockchip_fb -> fb ; <nl> }
EXPORT_SYMBOL_GPL ( usbnet_resume ); <nl>  <nl> static int __init usbnet_init ( void ) <nl> { <nl> - /* compiler should optimize this out */ <nl> - BUILD_BUG_ON ( sizeof ((( struct sk_buff *) 0 )-> cb ) <nl> - < sizeof ( struct skb_data )); <nl> + /* Compiler should optimize this out . */ <nl> + BUILD_BUG_ON ( <nl> + FIELD_SIZEOF ( struct sk_buff , cb ) < sizeof ( struct skb_data )); <nl>  <nl> random_ether_addr ( node_id ); <nl> return 0 ;
static int i915_drm_freeze ( struct drm_device * dev ) <nl> * Disable CRTCs directly since we want to preserve sw state <nl> * for _thaw . <nl> */ <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> list_for_each_entry ( crtc , & dev -> mode_config . crtc_list , head ) <nl> dev_priv -> display . crtc_disable ( crtc ); <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl>  <nl> intel_modeset_suspend_hw ( dev ); <nl> }
static int __devexit i2c_hid_remove ( struct i2c_client * client ) <nl>  <nl> free_irq ( client -> irq , ihid ); <nl>  <nl> + if ( ihid -> bufsize ) <nl> + i2c_hid_free_buffers ( ihid ); <nl> + <nl> kfree ( ihid ); <nl>  <nl> return 0 ;
static ssize_t mdc_kuc_write ( struct file * file , <nl> /* for mockup below */ 2 * cfs_size_round ( sizeof (* hai )); <nl>  <nl> OBD_ALLOC ( lh , len ); <nl> + if (! lh ) <nl> + return - ENOMEM ; <nl>  <nl> lh -> kuc_magic = KUC_MAGIC ; <nl> lh -> kuc_transport = KUC_TRANSPORT_HSM ;
static const struct ich_laptop ich_laptop [] = { <nl> { 0x27DF , 0x0005 , 0x0280 }, /* ICH7 on Acer 5602WLMi */ <nl> { 0x27DF , 0x1025 , 0x0110 }, /* ICH7 on Acer 3682WLMi */ <nl> { 0x27DF , 0x1043 , 0x1267 }, /* ICH7 on Asus W5F */ <nl> + { 0x27DF , 0x103C , 0x30A1 }, /* ICH7 on HP Compaq nc2400 */ <nl> { 0x24CA , 0x1025 , 0x0061 }, /* ICH4 on ACER Aspire 2023WLMi */ <nl> /* end marker */ <nl> { 0 , }
s32 fm10k_disable_queues_generic ( struct fm10k_hw * hw , u16 q_cnt ) <nl> /* clear tx_ready to prevent any false hits for reset */ <nl> hw -> mac . tx_ready = false ; <nl>  <nl> + if ( FM10K_REMOVED ( hw -> hw_addr )) <nl> + return 0 ; <nl> + <nl> /* clear the enable bit for all rings */ <nl> for ( i = 0 ; i < q_cnt ; i ++) { <nl> reg = fm10k_read_reg ( hw , FM10K_TXDCTL ( i ));
static struct rfkill * amilo_rfkill_dev ; <nl>  <nl> static int __devinit amilo_rfkill_probe ( struct platform_device * device ) <nl> { <nl> + int rc ; <nl> const struct dmi_system_id * system_id = <nl> dmi_first_match ( amilo_rfkill_id_table ); <nl> - int rc ; <nl> + <nl> + if (! system_id ) <nl> + return - ENXIO ; <nl>  <nl> amilo_rfkill_dev = rfkill_alloc ( KBUILD_MODNAME , & device -> dev , <nl> RFKILL_TYPE_WLAN ,
struct mmc_host * mmc_alloc_host ( int extra , struct device * dev ) <nl>  <nl> if ( mmc_gpio_alloc ( host )) { <nl> put_device (& host -> class_dev ); <nl> + ida_simple_remove (& mmc_host_ida , host -> index ); <nl> + kfree ( host ); <nl> return NULL ; <nl> } <nl> 
static struct pci_driver cciss_pci_driver = { <nl> */ <nl> static int __init cciss_init ( void ) <nl> { <nl> + /* <nl> + * The hardware requires that commands are aligned on a 64 - bit <nl> + * boundary . Given that we use pci_alloc_consistent () to allocate an <nl> + * array of them , the size must be a multiple of 8 bytes . <nl> + */ <nl> + BUILD_BUG_ON ( sizeof ( CommandList_struct ) % 8 ); <nl> + <nl> printk ( KERN_INFO DRIVER_NAME "\ n "); <nl>  <nl> /* Register for our PCI devices */
xfs_uuid_mount ( <nl> uuid_t * uuid = & mp -> m_sb . sb_uuid ; <nl> int hole , i ; <nl>  <nl> + /* Publish UUID in struct super_block */ <nl> + BUILD_BUG_ON ( sizeof ( mp -> m_super -> s_uuid ) != sizeof ( uuid_t )); <nl> + memcpy (& mp -> m_super -> s_uuid , uuid , sizeof ( uuid_t )); <nl> + <nl> if ( mp -> m_flags & XFS_MOUNT_NOUUID ) <nl> return 0 ; <nl> 
void kvm_arch_vcpu_load ( struct kvm_vcpu * vcpu , int cpu ) <nl> if ( check_tsc_unstable ()) { <nl> kvm_x86_ops -> adjust_tsc_offset ( vcpu , - tsc_delta ); <nl> vcpu -> arch . tsc_catchup = 1 ; <nl> - kvm_make_request ( KVM_REQ_CLOCK_UPDATE , vcpu ); <nl> } <nl> + kvm_make_request ( KVM_REQ_CLOCK_UPDATE , vcpu ); <nl> if ( vcpu -> cpu != cpu ) <nl> kvm_migrate_timers ( vcpu ); <nl> vcpu -> cpu = cpu ;
static int find_variable ( Dwarf_Die * sc_die , struct probe_finder * pf ) <nl> if (! die_find_variable_at (& pf -> cu_die , pf -> pvar -> var , 0 , & vr_die )) <nl> ret = - ENOENT ; <nl> } <nl> - if ( ret == 0 ) <nl> + if ( ret >= 0 ) <nl> ret = convert_variable (& vr_die , pf ); <nl>  <nl> if ( ret < 0 )
EXPORT_SYMBOL ( clk_add_alias ); <nl> */ <nl> void clkdev_drop ( struct clk_lookup * cl ) <nl> { <nl> + struct clk_lookup_alloc * cla = container_of ( cl , struct clk_lookup_alloc , cl ); <nl> + <nl> mutex_lock (& clocks_mutex ); <nl> list_del (& cl -> node ); <nl> mutex_unlock (& clocks_mutex ); <nl> - kfree ( cl ); <nl> + kfree ( cla ); <nl> } <nl> EXPORT_SYMBOL ( clkdev_drop );
static int sha384_neon_final ( struct shash_desc * desc , u8 * hash ) <nl> sha512_neon_final ( desc , D ); <nl>  <nl> memcpy ( hash , D , SHA384_DIGEST_SIZE ); <nl> - memset ( D , 0 , SHA512_DIGEST_SIZE ); <nl> + memzero_explicit ( D , SHA512_DIGEST_SIZE ); <nl>  <nl> return 0 ; <nl> }
struct device_node * v4l2_of_get_next_endpoint ( const struct device_node * parent , <nl> if (! endpoint ) <nl> pr_err ("% s (): no endpoint nodes specified for % s \ n ", <nl> __func__ , parent -> full_name ); <nl> + of_node_put ( node ); <nl> } else { <nl> port = of_get_parent ( prev ); <nl> if (! port )
static int loop_clr_fd ( struct loop_device * lo , struct block_device * bdev ) <nl> lo -> lo_state = Lo_unbound ; <nl> /* This is safe : open () is still holding a reference . */ <nl> module_put ( THIS_MODULE ); <nl> - if ( max_part > 0 ) <nl> + if ( max_part > 0 && bdev ) <nl> ioctl_by_bdev ( bdev , BLKRRPART , 0 ); <nl> mutex_unlock (& lo -> lo_ctl_mutex ); <nl> /*
static int __devinit tmiofb_probe ( struct platform_device * dev ) <nl> dev_err (& dev -> dev , " NULL platform data !\ n "); <nl> return - EINVAL ; <nl> } <nl> + if ( ccr == NULL || lcr == NULL || vram == NULL || irq < 0 ) { <nl> + dev_err (& dev -> dev , " missing resources \ n "); <nl> + return - EINVAL ; <nl> + } <nl>  <nl> info = framebuffer_alloc ( sizeof ( struct tmiofb_par ), & dev -> dev ); <nl> 
dma_alloc_coherent ( struct device * dev , size_t size , dma_addr_t * dma_handle , <nl> goto again ; <nl> } <nl>  <nl> + /* Let low level make its own zone decisions */ <nl> + gfp &= ~( GFP_DMA32 | GFP_DMA ); <nl> + <nl> if ( dma_ops -> alloc_coherent ) <nl> return dma_ops -> alloc_coherent ( dev , size , <nl> dma_handle , gfp );
new_slab : <nl> stat ( s , ALLOC_SLAB ); <nl> c -> node = page_to_nid ( page ); <nl> c -> page = page ; <nl> + <nl> + if ( kmem_cache_debug ( s )) <nl> + goto debug ; <nl> goto load_freelist ; <nl> } <nl> if (!( gfpflags & __GFP_NOWARN ) && printk_ratelimit ())
static int ecryptfs_open ( struct inode * inode , struct file * file ) <nl> file , ecryptfs_inode_to_private ( inode )-> lower_file ); <nl> if ( S_ISDIR ( ecryptfs_dentry -> d_inode -> i_mode )) { <nl> ecryptfs_printk ( KERN_DEBUG , " This is a directory \ n "); <nl> + mutex_lock (& crypt_stat -> cs_mutex ); <nl> crypt_stat -> flags &= ~( ECRYPTFS_ENCRYPTED ); <nl> + mutex_unlock (& crypt_stat -> cs_mutex ); <nl> rc = 0 ; <nl> goto out ; <nl> }
static int __cmd_record ( struct perf_record * rec , int argc , const char ** argv ) <nl> return err ; <nl> } <nl>  <nl> - if (!! rec -> no_buildid <nl> + if (! rec -> no_buildid <nl> && ! perf_header__has_feat (& session -> header , HEADER_BUILD_ID )) { <nl> - pr_err (" Couldn ' t generating buildids . " <nl> + pr_err (" Couldn ' t generate buildids . " <nl> " Use -- no - buildid to profile anyway .\ n "); <nl> return - 1 ; <nl> }
static int at76_iw_handler_get_scan ( struct net_device * netdev , <nl> if (! iwe ) <nl> return - ENOMEM ; <nl>  <nl> - if ( priv -> scan_state != SCAN_COMPLETED ) <nl> + if ( priv -> scan_state != SCAN_COMPLETED ) { <nl> /* scan not yet finished */ <nl> + kfree ( iwe ); <nl> return - EAGAIN ; <nl> + } <nl>  <nl> spin_lock_irqsave (& priv -> bss_list_spinlock , flags ); <nl> 
static int hdmi_display_check_timing ( struct omap_dss_device * dssdev , <nl> { <nl> struct omap_dss_device * out = & hdmi . output ; <nl>  <nl> + /* TODO : proper interlace support */ <nl> + if ( timings -> interlace ) <nl> + return - EINVAL ; <nl> + <nl> if (! dispc_mgr_timings_ok ( out -> dispc_channel , timings )) <nl> return - EINVAL ; <nl> 
static int radeon_atom_pick_pll ( struct drm_crtc * crtc ) <nl> if ( pll != ATOM_PPLL_INVALID ) <nl> return pll ; <nl> } <nl> - } else { <nl> + } else if (! ASIC_IS_DCE41 ( rdev )) { /* Don ' t share PLLs on DCE4 . 1 chips */ <nl> /* use the same PPLL for all monitors with the same clock */ <nl> pll = radeon_get_shared_nondp_ppll ( crtc ); <nl> if ( pll != ATOM_PPLL_INVALID )
static int sanitize_enable_ppgtt ( struct drm_device * dev , int enable_ppgtt ) <nl> } <nl> # endif <nl>  <nl> + /* Early VLV doesn ' t have this */ <nl> + if ( IS_VALLEYVIEW ( dev ) && dev -> pdev -> revision < 0xb ) { <nl> + DRM_DEBUG_DRIVER (" disabling PPGTT on pre - B3 step VLV \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> return HAS_ALIASING_PPGTT ( dev ) ? 1 : 0 ; <nl> } <nl> 
static inline pte_t * pte_alloc_one_kernel ( struct mm_struct * mm , <nl> static inline pgtable_t pte_alloc_one ( struct mm_struct * mm , <nl> unsigned long addr ) <nl> { <nl> + pte_t * pte ; <nl> struct page * page ; <nl>  <nl> - page = virt_to_page ( pte_alloc_one_kernel ( mm , addr )); <nl> + pte = pte_alloc_one_kernel ( mm , addr ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + page = virt_to_page ( pte ); <nl> pgtable_page_ctor ( page ); <nl> return page ; <nl> }
static int pcmcia_device_query ( struct pcmcia_device * p_dev ) <nl> tmp = vers1 -> str + vers1 -> ofs [ i ]; <nl>  <nl> length = strlen ( tmp ) + 1 ; <nl> - if (( length < 3 ) || ( length > 255 )) <nl> + if (( length < 2 ) || ( length > 255 )) <nl> continue ; <nl>  <nl> p_dev -> prod_id [ i ] = kmalloc ( sizeof ( char ) * length ,
static noinline size_t if_nlmsg_size ( const struct net_device * dev , <nl> + rtnl_link_get_af_size ( dev , ext_filter_mask ) /* IFLA_AF_SPEC */ <nl> + nla_total_size ( MAX_PHYS_ITEM_ID_LEN ) /* IFLA_PHYS_PORT_ID */ <nl> + nla_total_size ( MAX_PHYS_ITEM_ID_LEN ) /* IFLA_PHYS_SWITCH_ID */ <nl> + + nla_total_size ( IFNAMSIZ ) /* IFLA_PHYS_PORT_NAME */ <nl> + nla_total_size ( 1 ); /* IFLA_PROTO_DOWN */ <nl>  <nl> }
static unsigned long si5351_clkout_recalc_rate ( struct clk_hw * hw , <nl> unsigned char reg ; <nl> unsigned char rdiv ; <nl>  <nl> - if ( hwdata -> num > 5 ) <nl> + if ( hwdata -> num <= 5 ) <nl> reg = si5351_msynth_params_address ( hwdata -> num ) + 2 ; <nl> else <nl> reg = SI5351_CLK6_7_OUTPUT_DIVIDER ;
long sys_sigreturn ( struct pt_regs regs ) <nl> unsigned long __user * extramask = frame -> extramask ; <nl> int sig_size = ( _NSIG_WORDS - 1 ) * sizeof ( unsigned long ); <nl>  <nl> - if ( copy_from_user (& set . sig [ 0 ], oldmask , sizeof (& set . sig [ 0 ])) || <nl> + if ( copy_from_user (& set . sig [ 0 ], oldmask , sizeof ( set . sig [ 0 ])) || <nl> copy_from_user (& set . sig [ 1 ], extramask , sig_size )) <nl> goto segfault ; <nl> 
static void sdma_add_scripts ( struct sdma_engine * sdma , <nl> s32 * saddr_arr = ( u32 *) sdma -> script_addrs ; <nl> int i ; <nl>  <nl> + /* use the default firmware in ROM if missing external firmware */ <nl> + if (! sdma -> script_number ) <nl> + sdma -> script_number = SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1 ; <nl> + <nl> for ( i = 0 ; i < sdma -> script_number ; i ++) <nl> if ( addr_arr [ i ] > 0 ) <nl> saddr_arr [ i ] = addr_arr [ i ];
retry : <nl>  <nl> blkcg = bio_blkcg ( bio ); <nl> cfqg = cfq_lookup_create_cfqg ( cfqd , blkcg ); <nl> + if (! cfqg ) { <nl> + cfqq = & cfqd -> oom_cfqq ; <nl> + goto out ; <nl> + } <nl> + <nl> cfqq = cic_to_cfqq ( cic , is_sync ); <nl>  <nl> /* <nl> retry : <nl> } else <nl> cfqq = & cfqd -> oom_cfqq ; <nl> } <nl> - <nl> + out : <nl> if ( new_cfqq ) <nl> kmem_cache_free ( cfq_pool , new_cfqq ); <nl> 
EXPORT_SYMBOL ( smp_num_siblings ); <nl>  <nl> /* Last level cache ID of each logical CPU */ <nl> u8 cpu_llc_id [ NR_CPUS ] __cpuinitdata = {[ 0 ... NR_CPUS - 1 ] = BAD_APICID }; <nl> - EXPORT_SYMBOL ( cpu_llc_id ); <nl>  <nl> /* Bitmask of currently online CPUs */ <nl> cpumask_t cpu_online_map __read_mostly ;
static bool have_cpu_die ( void ) <nl> # ifdef CONFIG_HOTPLUG_CPU <nl> int any_cpu = raw_smp_processor_id (); <nl>  <nl> - if ( cpu_ops [ any_cpu ]-> cpu_die ) <nl> + if ( cpu_ops [ any_cpu ] && cpu_ops [ any_cpu ]-> cpu_die ) <nl> return true ; <nl> # endif <nl> return false ;
static int hdmi_remove ( struct platform_device * pdev ) <nl>  <nl> pm_runtime_disable ( dev ); <nl>  <nl> - free_irq ( hdata -> irq , hdata ); <nl> + free_irq ( hdata -> irq , ctx ); <nl>  <nl>  <nl> /* hdmiphy i2c driver */
static int slic_ioctl ( struct net_device * dev , struct ifreq * rq , int cmd ) <nl> else <nl> duplex = 0 ; <nl> slic_link_config ( adapter , speed , duplex ); <nl> - slic_link_event_handler ( adapter ); <nl> + if ( slic_link_event_handler ( adapter )) <nl> + return - EFAULT ; <nl> } <nl> } <nl> return 0 ;
static int eseqiv_givencrypt ( struct skcipher_givcrypt_request * req ) <nl> if ( err ) <nl> goto out ; <nl>  <nl> - eseqiv_complete2 ( req ); <nl> + if ( giv != req -> giv ) <nl> + eseqiv_complete2 ( req ); <nl>  <nl> out : <nl> return err ;
int intel_setup_gmbus ( struct drm_device * dev ) <nl> bus -> reg0 = i | GMBUS_RATE_100KHZ ; <nl>  <nl> /* XXX force bit banging until GMBUS is fully debugged */ <nl> - bus -> force_bit = intel_gpio_create ( dev_priv , i ); <nl> + if ( IS_GEN2 ( dev )) <nl> + bus -> force_bit = intel_gpio_create ( dev_priv , i ); <nl> } <nl>  <nl> intel_i2c_reset ( dev_priv -> dev );
intel_user_framebuffer_create ( struct drm_device * dev , <nl>  <nl> ret = intel_framebuffer_create ( dev , mode_cmd , & fb , obj ); <nl> if ( ret ) { <nl> + mutex_lock (& dev -> struct_mutex ); <nl> drm_gem_object_unreference ( obj ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return NULL ; <nl> } <nl> 
int ceph_atomic_open ( struct inode * dir , struct dentry * dentry , <nl> } <nl> err = finish_open ( file , dentry , ceph_open , opened ); <nl> } <nl> - <nl> out_err : <nl> + if (! req -> r_err && req -> r_target_inode ) <nl> + ceph_put_fmode ( ceph_inode ( req -> r_target_inode ), req -> r_fmode ); <nl> ceph_mdsc_put_request ( req ); <nl> dout (" atomic_open result =% d \ n ", err ); <nl> return err ;
static void radeon_cs_parser_fini ( struct radeon_cs_parser * parser , int error ) <nl> { <nl> unsigned i ; <nl>  <nl> - if ( error ) { <nl> + if ( error && parser -> ib ) { <nl> radeon_bo_list_unvalidate (& parser -> validated , <nl> parser -> ib -> fence ); <nl> } else {
static void * ixgbe_fwd_add ( struct net_device * pdev , struct net_device * vdev ) <nl> ( adapter -> num_rx_pools > IXGBE_MAX_MACVLANS )) <nl> return ERR_PTR (- EBUSY ); <nl>  <nl> - fwd_adapter = kcalloc ( 1 , sizeof ( struct ixgbe_fwd_adapter ), GFP_KERNEL ); <nl> + fwd_adapter = kzalloc ( sizeof (* fwd_adapter ), GFP_KERNEL ); <nl> if (! fwd_adapter ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
static struct i2c_algorithm osif_algorithm = { <nl> # define USB_OSIF_VENDOR_ID 0x1964 <nl> # define USB_OSIF_PRODUCT_ID 0x0001 <nl>  <nl> - static struct usb_device_id osif_table [] = { <nl> + static const struct usb_device_id osif_table [] = { <nl> { USB_DEVICE ( USB_OSIF_VENDOR_ID , USB_OSIF_PRODUCT_ID ) }, <nl> { } <nl> };
static int lstats_open ( struct inode * inode , struct file * file ) <nl> struct seq_file * m ; <nl> struct task_struct * task = get_proc_task ( inode ); <nl>  <nl> + if (! task ) <nl> + return - ENOENT ; <nl> ret = single_open ( file , lstats_show_proc , NULL ); <nl> if (! ret ) { <nl> m = file -> private_data ;
static int skge_down ( struct net_device * dev ) <nl> struct skge_hw * hw = skge -> hw ; <nl> int port = skge -> port ; <nl>  <nl> - if ( skge -> mem == NULL ) <nl> + if (! skge -> mem ) <nl> return 0 ; <nl>  <nl> netif_info ( skge , ifdown , skge -> netdev , " disabling interface \ n ");
void BSP_gettod ( int * yearp , int * monp , int * dayp , <nl> { <nl> } <nl>  <nl> - int BSP_hwclk ( int op , struct hwclk_time * t ) <nl> + int BSP_hwclk ( int op , struct rtc_time * t ) <nl> { <nl> if (! op ) { <nl> /* read */
static long tce_iommu_register_pages ( struct tce_container * container , <nl> return ret ; <nl>  <nl> tcemem = kzalloc ( sizeof (* tcemem ), GFP_KERNEL ); <nl> + if (! tcemem ) { <nl> + mm_iommu_put ( container -> mm , mem ); <nl> + return - ENOMEM ; <nl> + } <nl> + <nl> tcemem -> mem = mem ; <nl> list_add (& tcemem -> next , & container -> prereg_list ); <nl> 
int __devinit snd_emu10k1_mixer ( struct snd_emu10k1 * emu , <nl> if ( emu -> ac97 -> id == AC97_ID_STAC9758 ) { <nl> emu -> rear_ac97 = 1 ; <nl> snd_emu10k1_ptr_write ( emu , AC97SLOT , 0 , AC97SLOT_CNTR | AC97SLOT_LFE | AC97SLOT_REAR_LEFT | AC97SLOT_REAR_RIGHT ); <nl> + snd_ac97_write_cache ( emu -> ac97 , AC97_HEADPHONE , 0x0202 ); <nl> } <nl> /* remove unused AC97 controls */ <nl> snd_ac97_write_cache ( emu -> ac97 , AC97_SURROUND_MASTER , 0x0202 );
void tcp_fastopen_add_skb ( struct sock * sk , struct sk_buff * skb ) <nl> tp -> segs_in = 0 ; <nl> tcp_segs_in ( tp , skb ); <nl> __skb_pull ( skb , tcp_hdrlen ( skb )); <nl> + sk_forced_mem_schedule ( sk , skb -> truesize ); <nl> skb_set_owner_r ( skb , sk ); <nl>  <nl> TCP_SKB_CB ( skb )-> seq ++;
static int noinline dirty_and_release_pages ( struct btrfs_trans_handle * trans , <nl> */ <nl> inline_size = end_pos ; <nl> if ( isize >= BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> - inline_size > 32768 || <nl> + inline_size > 8192 || <nl> inline_size >= BTRFS_MAX_INLINE_DATA_SIZE ( root )) { <nl> u64 last_end ; <nl> u64 existing_delalloc = 0 ;
static int fbcon_blank ( struct vc_data * vc , int blank , int mode_switch ) <nl> update_screen ( vc ); <nl> } <nl>  <nl> - if ( fbcon_is_inactive ( vc , info ) || <nl> + if ( mode_switch || fbcon_is_inactive ( vc , info ) || <nl> ops -> blank_state != FB_BLANK_UNBLANK ) <nl> fbcon_del_cursor_timer ( info ); <nl> else
static inline uint8_t elf_sym__type ( const GElf_Sym * sym ) <nl>  <nl> static inline int elf_sym__is_function ( const GElf_Sym * sym ) <nl> { <nl> - return elf_sym__type ( sym ) == STT_FUNC && <nl> + return ( elf_sym__type ( sym ) == STT_FUNC || <nl> + elf_sym__type ( sym ) == STT_GNU_IFUNC ) && <nl> sym -> st_name != 0 && <nl> sym -> st_shndx != SHN_UNDEF ; <nl> }
static int rr_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> return 0 ; <nl>  <nl> out : <nl> + if ( rrpriv -> evt_ring ) <nl> + pci_free_consistent ( pdev , EVT_RING_SIZE , rrpriv -> evt_ring , <nl> + rrpriv -> evt_ring_dma ); <nl> if ( rrpriv -> rx_ring ) <nl> pci_free_consistent ( pdev , RX_TOTAL_SIZE , rrpriv -> rx_ring , <nl> rrpriv -> rx_ring_dma );
restart : <nl> * with multiple processes reclaiming pages , the total <nl> * freeing target can get unreasonably large . <nl> */ <nl> - if ( nr_reclaimed >= nr_to_reclaim && priority < DEF_PRIORITY ) <nl> + if ( nr_reclaimed >= nr_to_reclaim ) <nl> + nr_to_reclaim = 0 ; <nl> + else <nl> + nr_to_reclaim -= nr_reclaimed ; <nl> + <nl> + if (! nr_to_reclaim && priority < DEF_PRIORITY ) <nl> break ; <nl> } <nl> blk_finish_plug (& plug );
static int aspeed_gpio_set_config ( struct gpio_chip * chip , unsigned int offset , <nl> param == PIN_CONFIG_BIAS_PULL_DOWN || <nl> param == PIN_CONFIG_DRIVE_STRENGTH ) <nl> return pinctrl_gpio_set_config ( offset , config ); <nl> + else if ( param == PIN_CONFIG_DRIVE_OPEN_DRAIN || <nl> + param == PIN_CONFIG_DRIVE_OPEN_SOURCE ) <nl> + /* Return - ENOTSUPP to trigger emulation , as per datasheet */ <nl> + return - ENOTSUPP ; <nl>  <nl> return - ENOTSUPP ; <nl> }
static int davinci_gpio_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& chips [ i ]. lock ); <nl>  <nl> regs = gpio2regs ( base ); <nl> + if (! regs ) <nl> + return - ENXIO ; <nl> chips [ i ]. regs = regs ; <nl> chips [ i ]. set_data = & regs -> set_data ; <nl> chips [ i ]. clr_data = & regs -> clr_data ;
static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh , <nl>  <nl> if (! buffer_dirty ( bh )) { <nl> /* bdflush has written it . We can drop it now */ <nl> + __jbd2_journal_remove_checkpoint ( jh ); <nl> goto zap_buffer ; <nl> } <nl>  <nl> static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh , <nl> /* The orphan record ' s transaction has <nl> * committed . We can cleanse this buffer */ <nl> clear_buffer_jbddirty ( bh ); <nl> + __jbd2_journal_remove_checkpoint ( jh ); <nl> goto zap_buffer ; <nl> } <nl> }
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
static int btree_gc_coalesce ( struct btree * b , struct btree_op * op , <nl> BUG_ON ( btree_bset_first ( new_nodes [ 0 ])-> keys ); <nl> btree_node_free ( new_nodes [ 0 ]); <nl> rw_unlock ( true , new_nodes [ 0 ]); <nl> + new_nodes [ 0 ] = NULL ; <nl>  <nl> for ( i = 0 ; i < nodes ; i ++) { <nl> if ( __bch_keylist_realloc (& keylist , bkey_u64s (& r [ i ]. b -> key )))
static int __devinit snd_card_ad1816a_pnp ( int dev , struct snd_card_ad1816a * acar <nl> struct pnp_resource_table * cfg = kmalloc ( sizeof (* cfg ), GFP_KERNEL ); <nl> int err ; <nl>  <nl> + if (! cfg ) <nl> + return - ENOMEM ; <nl> acard -> dev = pnp_request_card_device ( card , id -> devs [ 0 ]. id , NULL ); <nl> if ( acard -> dev == NULL ) { <nl> kfree ( cfg );
static int __init aha152x_init ( void ) <nl>  <nl> static void __exit aha152x_exit ( void ) <nl> { <nl> - struct aha152x_hostdata * hd ; <nl> + struct aha152x_hostdata * hd , * tmp ; <nl>  <nl> - list_for_each_entry ( hd , & aha152x_host_list , host_list ) { <nl> + list_for_each_entry_safe ( hd , tmp , & aha152x_host_list , host_list ) { <nl> struct Scsi_Host * shost = container_of (( void *) hd , struct Scsi_Host , hostdata ); <nl>  <nl> aha152x_release ( shost );
static struct ip_conntrack_expect * find_expect ( struct ip_conntrack * ct , <nl> tuple . dst . protonum = IPPROTO_TCP ; <nl>  <nl> exp = __ip_conntrack_expect_find (& tuple ); <nl> - if ( exp -> master == ct ) <nl> + if ( exp && exp -> master == ct ) <nl> return exp ; <nl> return NULL ; <nl> }
static int __init sdma_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + ret = dma_coerce_mask_and_coherent (& pdev -> dev , DMA_BIT_MASK ( 32 )); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> sdma = kzalloc ( sizeof (* sdma ), GFP_KERNEL ); <nl> if (! sdma ) <nl> return - ENOMEM ;
static int amd64_check_ecc_enabled ( struct amd64_pvt * pvt ) <nl> " ECC is enabled by BIOS , Proceeding " <nl> " with EDAC module initialization \ n "); <nl>  <nl> + /* Signal good ECC status */ <nl> + ret = 0 ; <nl> + <nl> /* CLEAR the override , since BIOS controlled it */ <nl> ecc_enable_override = 0 ; <nl> }
static int ipmmu_domain_init_context ( struct ipmmu_vmsa_domain * domain ) <nl> domain -> cfg . ias = 32 ; <nl> domain -> cfg . oas = 40 ; <nl> domain -> cfg . tlb = & ipmmu_gather_ops ; <nl> + domain -> io_domain . geometry . aperture_end = DMA_BIT_MASK ( 32 ); <nl> + domain -> io_domain . geometry . force_aperture = true ; <nl> /* <nl> * TODO : Add support for coherent walk through CCI with DVM and remove <nl> * cache handling . For now , delegate it to the io - pgtable code .
static struct pernet_operations __net_initdata proc_net_ns_ops = { <nl>  <nl> int __init proc_net_init ( void ) <nl> { <nl> - proc_symlink (" net ", NULL , " self / net "); <nl> + proc_symlink (" net ", NULL , " thread - self / net "); <nl>  <nl> return register_pernet_subsys (& proc_net_ns_ops ); <nl> }
bool intel_enable_ppgtt ( struct drm_device * dev , bool full ) <nl>  <nl> /* Full ppgtt disabled by default for now due to issues . */ <nl> if ( full ) <nl> - return false ; /* HAS_PPGTT ( dev ) */ <nl> + return HAS_PPGTT ( dev ) && ( i915 . enable_ppgtt == 2 ); <nl> else <nl> return HAS_ALIASING_PPGTT ( dev ); <nl> }
static struct usb_driver go7007_usb_driver = { <nl> }; <nl>  <nl> module_usb_driver ( go7007_usb_driver ); <nl> + MODULE_LICENSE (" GPL v2 ");
static int ccw_uevent ( struct device * dev , char ** envp , int num_envp , <nl> snprint_alias ( modalias_buf , sizeof ( modalias_buf ), id , ""); <nl> ret = add_uevent_var ( envp , num_envp , & i , buffer , buffer_size , & len , <nl> " MODALIAS =% s ", modalias_buf ); <nl> - return ret ; <nl> + if ( ret ) <nl> + return ret ; <nl> + envp [ i ] = NULL ; <nl> + return 0 ; <nl> } <nl>  <nl> struct bus_type ccw_bus_type ;
static void k8_map_sysaddr_to_csrow ( struct mem_ctl_info * mci , <nl> * different from the node that detected the error . <nl> */ <nl> src_mci = find_mc_by_sys_addr ( mci , SystemAddress ); <nl> - if ( src_mci ) { <nl> + if (! src_mci ) { <nl> amd64_mc_printk ( mci , KERN_ERR , <nl> " failed to map error address 0x % lx to a node \ n ", <nl> ( unsigned long ) SystemAddress );
static ssize_t dgap_driver_pollrate_show ( struct device_driver * ddp , char * buf ) <nl>  <nl> static ssize_t dgap_driver_pollrate_store ( struct device_driver * ddp , const char * buf , size_t count ) <nl> { <nl> - sscanf ( buf , "% d \ n ", & dgap_poll_tick ); <nl> + if ( sscanf ( buf , "% d \ n ", & dgap_poll_tick ) != 1 ) <nl> + return - EINVAL ; <nl> return count ; <nl> } <nl> static DRIVER_ATTR ( pollrate , ( S_IRUSR | S_IWUSR ), dgap_driver_pollrate_show , dgap_driver_pollrate_store );
struct sctp_cookie { <nl> __u32 adaptation_ind ; <nl>  <nl> __u8 auth_random [ sizeof ( sctp_paramhdr_t ) + SCTP_AUTH_RANDOM_LENGTH ]; <nl> - __u8 auth_hmacs [ SCTP_AUTH_NUM_HMACS + 2 ]; <nl> + __u8 auth_hmacs [ SCTP_AUTH_NUM_HMACS * sizeof ( __u16 ) + 2 ]; <nl> __u8 auth_chunks [ sizeof ( sctp_paramhdr_t ) + SCTP_AUTH_MAX_CHUNKS ]; <nl>  <nl> /* This is a shim for my peer ' s INIT packet , followed by
static int macronix_quad_enable ( struct spi_nor * nor ) <nl> val = read_sr ( nor ); <nl> if ( val < 0 ) <nl> return val ; <nl> + if ( val & SR_QUAD_EN_MX ) <nl> + return 0 ; <nl> + <nl> write_enable ( nor ); <nl>  <nl> write_sr ( nor , val | SR_QUAD_EN_MX );
scsi_reset_provider ( struct scsi_device * dev , int flag ) <nl> } <nl>  <nl> scmd = scsi_get_command ( dev , GFP_KERNEL ); <nl> + if (! scmd ) { <nl> + rtn = FAILED ; <nl> + put_device (& dev -> sdev_gendev ); <nl> + goto out_put_autopm_host ; <nl> + } <nl> + <nl> blk_rq_init ( NULL , & req ); <nl> scmd -> request = & req ; <nl> 
static int __devexit max77693_muic_remove ( struct platform_device * pdev ) <nl> free_irq ( muic_irqs [ i ]. virq , info ); <nl> cancel_work_sync (& info -> irq_work ); <nl> extcon_dev_unregister ( info -> edev ); <nl> + kfree ( info -> edev ); <nl> kfree ( info ); <nl>  <nl> return 0 ;
static void hdmi5_core_audio_config ( struct hdmi_core_data * core , <nl>  <nl> /* Source number */ <nl> val = cfg -> iec60958_cfg -> status [ 2 ] & IEC958_AES2_CON_SOURCE ; <nl> - REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 2 ), val , 3 , 4 ); <nl> + REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 2 ), val , 3 , 0 ); <nl>  <nl> /* Channel number right 0 */ <nl> REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 3 ), 2 , 3 , 0 );
static void sahara_decode_status ( struct sahara_dev * dev , unsigned int status ) <nl> if ( status & SAHARA_STATUS_MODE_BATCH ) <nl> dev_dbg ( dev -> device , " - Batch Mode .\ n "); <nl> else if ( status & SAHARA_STATUS_MODE_DEDICATED ) <nl> - dev_dbg ( dev -> device , " - Decidated Mode .\ n "); <nl> + dev_dbg ( dev -> device , " - Dedicated Mode .\ n "); <nl> else if ( status & SAHARA_STATUS_MODE_DEBUG ) <nl> dev_dbg ( dev -> device , " - Debug Mode .\ n "); <nl> 
static int sierra_resume ( struct usb_serial * serial ) <nl> if ( err < 0 ) { <nl> intfdata -> in_flight --; <nl> usb_unanchor_urb ( urb ); <nl> - usb_scuttle_anchored_urbs (& portdata -> delayed ); <nl> - break ; <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> + spin_lock (& portdata -> lock ); <nl> + portdata -> outstanding_urbs --; <nl> + spin_unlock (& portdata -> lock ); <nl> + continue ; <nl> } <nl> } <nl> 
static int input_open_polled_device ( struct input_dev * input ) <nl> dev -> open ( dev ); <nl>  <nl> /* Only start polling if polling is enabled */ <nl> - if ( dev -> poll_interval > 0 ) <nl> - queue_delayed_work ( system_freezable_wq , & dev -> work , 0 ); <nl> + if ( dev -> poll_interval > 0 ) { <nl> + dev -> poll ( dev ); <nl> + input_polldev_queue_work ( dev ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void rtl8152_get_drvinfo ( struct net_device * netdev , <nl> { <nl> struct r8152 * tp = netdev_priv ( netdev ); <nl>  <nl> - strncpy ( info -> driver , MODULENAME , ETHTOOL_BUSINFO_LEN ); <nl> - strncpy ( info -> version , DRIVER_VERSION , ETHTOOL_BUSINFO_LEN ); <nl> + strlcpy ( info -> driver , MODULENAME , sizeof ( info -> driver )); <nl> + strlcpy ( info -> version , DRIVER_VERSION , sizeof ( info -> version )); <nl> usb_make_path ( tp -> udev , info -> bus_info , sizeof ( info -> bus_info )); <nl> } <nl> 
int perf_evlist__parse_mmap_pages ( const struct option * opt , const char * str , <nl> unsigned long max = UINT_MAX ; <nl> long pages ; <nl>  <nl> - if ( max < SIZE_MAX / page_size ) <nl> + if ( max > SIZE_MAX / page_size ) <nl> max = SIZE_MAX / page_size ; <nl>  <nl> pages = parse_pages_arg ( str , 1 , max );
static struct sh_eth_cpu_data sh7734_data = { <nl> . tsu = 1 , <nl> . hw_checksum = 1 , <nl> . select_mii = 1 , <nl> + . magic = 1 , <nl> }; <nl>  <nl> /* SH7763 */
static struct regmap_config hmc5843_i2c_regmap_config = { <nl> . cache_type = REGCACHE_RBTREE , <nl> }; <nl>  <nl> - static int hmc5843_i2c_probe ( struct i2c_client * client , <nl> + static int hmc5843_i2c_probe ( struct i2c_client * cli , <nl> const struct i2c_device_id * id ) <nl> { <nl> - return hmc5843_common_probe (& client -> dev , <nl> - devm_regmap_init_i2c ( client , & hmc5843_i2c_regmap_config ), <nl> + return hmc5843_common_probe (& cli -> dev , <nl> + devm_regmap_init_i2c ( cli , & hmc5843_i2c_regmap_config ), <nl> id -> driver_data ); <nl> } <nl> 
void disable_lapic_nmi_watchdog ( void ) <nl> return ; <nl>  <nl> on_each_cpu ( stop_apic_nmi_watchdog , NULL , 0 , 1 ); <nl> - wd_ops -> unreserve (); <nl> + <nl> + if ( wd_ops ) <nl> + wd_ops -> unreserve (); <nl>  <nl> BUG_ON ( atomic_read (& nmi_active ) != 0 ); <nl> }
static int __mlxsw_sp_port_fid_join ( struct mlxsw_sp_port * mlxsw_sp_port , <nl> { <nl> struct mlxsw_sp_fid * f ; <nl>  <nl> + if ( test_bit ( fid , mlxsw_sp_port -> active_vlans )) <nl> + return 0 ; <nl> + <nl> f = mlxsw_sp_fid_find ( mlxsw_sp_port -> mlxsw_sp , fid ); <nl> if (! f ) { <nl> f = mlxsw_sp_fid_create ( mlxsw_sp_port -> mlxsw_sp , fid );
static int proc_thermal_add ( struct device * dev , <nl> int ret ; <nl>  <nl> adev = ACPI_COMPANION ( dev ); <nl> + if (! adev ) <nl> + return - ENODEV ; <nl>  <nl> status = acpi_evaluate_object ( adev -> handle , " PPCC ", NULL , & buf ); <nl> if ( ACPI_FAILURE ( status ))
int cpufreq_update_policy ( unsigned int cpu ) <nl> */ <nl> if ( cpufreq_driver -> get ) { <nl> new_policy . cur = cpufreq_driver -> get ( cpu ); <nl> + if ( WARN_ON (! new_policy . cur )) { <nl> + ret = - EIO ; <nl> + goto no_policy ; <nl> + } <nl> + <nl> if (! policy -> cur ) { <nl> pr_debug (" Driver did not initialize current freq "); <nl> policy -> cur = new_policy . cur ;
static int sendconfirmsleep ( struct lbs_private * priv , u8 * cmdptr , u16 size ) <nl> lbs_deb_hex ( LBS_DEB_HOST , " sleep confirm command ", cmdptr , size ); <nl>  <nl> ret = priv -> hw_host_to_card ( priv , MVMS_CMD , cmdptr , size ); <nl> - priv -> dnld_sent = DNLD_RES_RECEIVED ; <nl>  <nl> spin_lock_irqsave (& priv -> driver_lock , flags ); <nl> if ( priv -> intcounter || priv -> currenttxskb )
static int encode_caps_cb ( struct inode * inode , struct ceph_cap * cap , <nl> spin_lock (& ci -> i_ceph_lock ); <nl> cap -> seq = 0 ; /* reset cap seq */ <nl> cap -> issue_seq = 0 ; /* and issue_seq */ <nl> + cap -> mseq = 0 ; /* and migrate_seq */ <nl>  <nl> if ( recon_state -> flock ) { <nl> rec . v2 . cap_id = cpu_to_le64 ( cap -> cap_id );
static int xenbus_write_transaction ( unsigned msg_type , <nl> return xenbus_command_reply ( u , XS_ERROR , " ENOENT "); <nl>  <nl> rc = xenbus_dev_request_and_reply (& u -> u . msg , u ); <nl> - if ( rc ) <nl> + if ( rc && trans ) { <nl> + list_del (& trans -> list ); <nl> kfree ( trans ); <nl> + } <nl>  <nl> out : <nl> return rc ;
static int pm8001_dev_found_notify ( struct domain_device * dev ) <nl> wait_for_completion (& completion ); <nl> if ( dev -> dev_type == SAS_END_DEV ) <nl> msleep ( 50 ); <nl> - pm8001_ha -> flags = PM8001F_RUN_TIME ; <nl> + pm8001_ha -> flags |= PM8001F_RUN_TIME ; <nl> return 0 ; <nl> found_out : <nl> spin_unlock_irqrestore (& pm8001_ha -> lock , flags );
static int mcp23s08_irq_setup ( struct mcp23s08 * mcp ) <nl> return - ENODEV ; <nl>  <nl> err = devm_request_threaded_irq ( chip -> dev , mcp -> irq , NULL , mcp23s08_irq , <nl> - IRQF_TRIGGER_LOW | IRQF_ONESHOT , <nl> + IRQF_TRIGGER_LOW | IRQF_ONESHOT | <nl> + IRQF_SHARED , <nl> dev_name ( chip -> dev ), mcp ); <nl> if ( err != 0 ) { <nl> dev_err ( chip -> dev , " unable to request IRQ #% d : % d \ n ",
static void ad1884_fixup_thinkpad ( struct hda_codec * codec , <nl> { <nl> struct ad198x_spec * spec = codec -> spec ; <nl>  <nl> - if ( action == HDA_FIXUP_ACT_PRE_PROBE ) <nl> + if ( action == HDA_FIXUP_ACT_PRE_PROBE ) { <nl> spec -> gen . keep_eapd_on = 1 ; <nl> + spec -> gen . vmaster_mute . hook = ad_vmaster_eapd_hook ; <nl> + spec -> eapd_nid = 0x12 ; <nl> + } <nl> } <nl>  <nl> /* set magic COEFs for dmic */
EXPORT_SYMBOL_GPL ( tcp_slow_start ); <nl> */ <nl> void tcp_cong_avoid_ai ( struct tcp_sock * tp , u32 w , u32 acked ) <nl> { <nl> + /* If credits accumulated at a higher w , apply them gently now . */ <nl> + if ( tp -> snd_cwnd_cnt >= w ) { <nl> + tp -> snd_cwnd_cnt = 0 ; <nl> + tp -> snd_cwnd ++; <nl> + } <nl> + <nl> tp -> snd_cwnd_cnt += acked ; <nl> if ( tp -> snd_cwnd_cnt >= w ) { <nl> u32 delta = tp -> snd_cwnd_cnt / w ;
static void fan_watchdog_reset ( void ) <nl> { <nl> static int fan_watchdog_active = 0 ; <nl>  <nl> + if ( fan_control_access_mode == TPACPI_FAN_WR_NONE ) <nl> + return ; <nl> + <nl> if ( fan_watchdog_active ) <nl> cancel_delayed_work (& fan_watchdog_task ); <nl> 
struct hisi_clock_data * hisi_clk_alloc ( struct platform_device * pdev , <nl> if (! clk_data -> base ) <nl> return NULL ; <nl>  <nl> - clk_table = devm_kmalloc (& pdev -> dev , sizeof ( struct clk *) * nr_clks , <nl> - GFP_KERNEL ); <nl> + clk_table = devm_kmalloc_array (& pdev -> dev , nr_clks , <nl> + sizeof (* clk_table ), <nl> + GFP_KERNEL ); <nl> if (! clk_table ) <nl> return NULL ; <nl> 
int afu_register_irqs ( struct cxl_context * ctx , u32 count ) <nl> */ <nl> INIT_LIST_HEAD (& ctx -> irq_names ); <nl> for ( r = 1 ; r < CXL_IRQ_RANGES ; r ++) { <nl> - for ( i = 0 ; i < ctx -> irqs . range [ r ]; hwirq ++, i ++) { <nl> + for ( i = 0 ; i < ctx -> irqs . range [ r ]; i ++) { <nl> irq_name = kmalloc ( sizeof ( struct cxl_irq_name ), <nl> GFP_KERNEL ); <nl> if (! irq_name )
STATIC int INIT gunzip ( unsigned char * buf , int len , <nl> strm -> next_in ++; <nl> strm -> next_in ++; <nl> } <nl> - strm -> avail_in = len - 10 ; <nl> + strm -> avail_in = len - ( strm -> next_in - zbuf ); <nl>  <nl> strm -> next_out = out_buf ; <nl> strm -> avail_out = out_len ;
static int ext4_mb_init_per_dev_proc ( struct super_block * sb ) <nl> struct proc_dir_entry * proc ; <nl> char devname [ 64 ]; <nl>  <nl> + if ( proc_root_ext4 == NULL ) { <nl> + sbi -> s_mb_proc = NULL ; <nl> + return - EINVAL ; <nl> + } <nl> bdevname ( sb -> s_bdev , devname ); <nl> sbi -> s_mb_proc = proc_mkdir ( devname , proc_root_ext4 ); <nl> 
struct flex_array * flex_array_alloc ( int element_size , unsigned int total , <nl> ret -> element_size = element_size ; <nl> ret -> total_nr_elements = total ; <nl> if ( elements_fit_in_base ( ret ) && !( flags & __GFP_ZERO )) <nl> - memset ( ret -> parts [ 0 ], FLEX_ARRAY_FREE , <nl> + memset (& ret -> parts [ 0 ], FLEX_ARRAY_FREE , <nl> FLEX_ARRAY_BASE_BYTES_LEFT ); <nl> return ret ; <nl> }
static int ti_qspi_dma_xfer ( struct ti_qspi * qspi , dma_addr_t dma_dst , <nl> tx -> callback = ti_qspi_dma_callback ; <nl> tx -> callback_param = qspi ; <nl> cookie = tx -> tx_submit ( tx ); <nl> + reinit_completion (& qspi -> transfer_complete ); <nl>  <nl> ret = dma_submit_error ( cookie ); <nl> if ( ret ) {
# include < mach / mmc . h > <nl> # include < mach / ohci . h > <nl> # include < mach / pxa2xx - regs . h > <nl> +# include < mach / audio . h > <nl>  <nl> # include " generic . h " <nl> # include " devices . h " <nl> static void __init csb726_init ( void ) <nl> pxa27x_set_i2c_power_info ( NULL ); <nl> pxa_set_mci_info (& csb726_mci ); <nl> pxa_set_ohci_info (& csb726_ohci_platform_data ); <nl> + pxa_set_ac97_info ( NULL ); <nl>  <nl> platform_add_devices ( devices , ARRAY_SIZE ( devices )); <nl> }
void radeon_audio_detect ( struct drm_connector * connector , <nl> return ; <nl>  <nl> rdev = connector -> encoder -> dev -> dev_private ; <nl> + <nl> + if (! radeon_audio_chipset_supported ( rdev )) <nl> + return ; <nl> + <nl> radeon_encoder = to_radeon_encoder ( connector -> encoder ); <nl> dig = radeon_encoder -> enc_priv ; <nl> 
int radeon_sa_bo_new ( struct radeon_device * rdev , <nl> offset = 0 ; <nl> list_for_each_entry ( tmp , & sa_manager -> sa_bo , list ) { <nl> /* room before this object ? */ <nl> - if (( tmp -> offset - offset ) >= size ) { <nl> + if ( offset < tmp -> offset && ( tmp -> offset - offset ) >= size ) { <nl> head = tmp -> list . prev ; <nl> goto out ; <nl> }
static int ll_dir_setdirstripe ( struct inode * parent , struct lmv_user_md * lump , <nl> PFID ( ll_inode2fid ( parent )), parent , dirname , <nl> ( int ) lump -> lum_stripe_offset , lump -> lum_stripe_count ); <nl>  <nl> + if ( lump -> lum_stripe_count > 1 && <nl> + !( exp_connect_flags ( sbi -> ll_md_exp ) & OBD_CONNECT_DIR_STRIPE )) <nl> + return - EINVAL ; <nl> + <nl> if ( lump -> lum_magic != cpu_to_le32 ( LMV_USER_MAGIC )) <nl> lustre_swab_lmv_user_md ( lump ); <nl> 
int hcd_buffer_create ( struct usb_hcd * hcd ) <nl> char name [ 16 ]; <nl> int i , size ; <nl>  <nl> + if (! hcd -> self . controller -> dma_mask ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < HCD_BUFFER_POOLS ; i ++) { <nl> if (!( size = pool_max [ i ])) <nl> continue ;
xfs_setattr_nonsize ( <nl> } <nl> if (! gid_eq ( igid , gid )) { <nl> if ( XFS_IS_QUOTA_RUNNING ( mp ) && XFS_IS_GQUOTA_ON ( mp )) { <nl> - ASSERT (! XFS_IS_PQUOTA_ON ( mp )); <nl> + ASSERT ( xfs_sb_version_has_pquotino (& mp -> m_sb ) || <nl> + ! XFS_IS_PQUOTA_ON ( mp )); <nl> ASSERT ( mask & ATTR_GID ); <nl> ASSERT ( gdqp ); <nl> olddquot2 = xfs_qm_vop_chown ( tp , ip ,
void oz_apps_term ( void ) <nl> void oz_handle_app_elt ( struct oz_pd * pd , u8 app_id , struct oz_elt * elt ) <nl> { <nl> struct oz_app_if * ai ; <nl> - if ( app_id > OZ_APPID_MAX ) <nl> + if ( app_id == 0 || app_id > OZ_APPID_MAX ) <nl> return ; <nl> ai = & g_app_if [ app_id - 1 ]; <nl> ai -> rx ( pd , elt );
send : <nl> ret = batadv_send_skb_via_tt ( bat_priv , skb , dst_hint , <nl> vid ); <nl> } <nl> - if ( ret == NET_XMIT_DROP ) <nl> + if ( ret != NET_XMIT_SUCCESS ) <nl> goto dropped_freed ; <nl> } <nl> 
static int __devinit rtsx_probe ( struct pci_dev * pci , <nl> th = kthread_create ( rtsx_scan_thread , dev , " rtsx - scan "); <nl> if ( IS_ERR ( th )) { <nl> printk ( KERN_ERR " Unable to start the device - scanning thread \ n "); <nl> + complete (& dev -> scanning_done ); <nl> quiesce_and_remove_host ( dev ); <nl> err = PTR_ERR ( th ); <nl> goto errout ;
struct s3c2410_spigpio { <nl>  <nl> static inline struct s3c2410_spigpio * spidev_to_sg ( struct spi_device * spi ) <nl> { <nl> - return spi -> controller_data ; <nl> + return spi_master_get_devdata ( spi -> master ); <nl> } <nl>  <nl> static inline void setsck ( struct spi_device * dev , int on )
cfq_rq_enqueued ( struct cfq_data * cfqd , struct cfq_queue * cfqq , <nl> if ( blk_rq_bytes ( rq ) > PAGE_CACHE_SIZE || <nl> cfqd -> busy_queues > 1 ) { <nl> del_timer (& cfqd -> idle_slice_timer ); <nl> - __blk_run_queue ( cfqd -> queue ); <nl> - } <nl> - cfq_mark_cfqq_must_dispatch ( cfqq ); <nl> + __blk_run_queue ( cfqd -> queue ); <nl> + } else <nl> + cfq_mark_cfqq_must_dispatch ( cfqq ); <nl> } <nl> } else if ( cfq_should_preempt ( cfqd , cfqq , rq )) { <nl> /*
void nf_log_packet ( int pf , <nl> const struct net_device * in , <nl> const struct net_device * out , <nl> const struct nf_loginfo * li , <nl> - const char * fmt , ...); <nl> + const char * fmt , ...) __attribute__ (( format ( printf , 7 , 8 ))); <nl>  <nl> # endif /* _NF_LOG_H */
__xfs_printk ( <nl> const struct xfs_mount * mp , <nl> struct va_format * vaf ) <nl> { <nl> - if ( mp && mp -> m_fsname ) <nl> + if ( mp && mp -> m_fsname ) { <nl> printk ("% sXFS (% s ): % pV \ n ", level , mp -> m_fsname , vaf ); <nl> + return ; <nl> + } <nl> printk ("% sXFS : % pV \ n ", level , vaf ); <nl> } <nl> 
static void b43_supported_bands ( struct b43_wldev * dev , bool * have_2ghz_phy , <nl> * have_5ghz_phy = true ; <nl> return ; <nl> case 0x4321 : /* BCM4306 */ <nl> + /* There are 14e4 : 4321 PCI devs with 2 . 4 GHz BCM4321 ( N - PHY ) */ <nl> + if ( dev -> phy . type != B43_PHYTYPE_G ) <nl> + break ; <nl> + /* fall through */ <nl> case 0x4313 : /* BCM4311 */ <nl> case 0x431a : /* BCM4318 */ <nl> case 0x432a : /* BCM4321 */
static int do_calculate_time ( int status , enum apm_source source ) <nl> return - 1 ; <nl> } <nl>  <nl> + if (! I . intval ) <nl> + return 0 ; <nl> + <nl> switch ( source ) { <nl> case SOURCE_CHARGE : <nl> full_prop = POWER_SUPPLY_PROP_CHARGE_FULL ;
static const struct comedi_lrange * dac_range_table [] = { <nl>  <nl> static const struct comedi_lrange * dac_range_lkup ( int opt ) <nl> { <nl> - if ( opt < 0 || opt > 5 ) <nl> + if ( opt < 0 || opt >= 5 ) <nl> return & range_unknown ; <nl> return dac_range_table [ opt ]; <nl> }
done : <nl>  <nl> out : <nl> if ( unlikely ( frozen_buffer )) /* It ' s usually NULL */ <nl> - kfree ( frozen_buffer ); <nl> + jbd_slab_free ( frozen_buffer , bh -> b_size ); <nl>  <nl> JBUFFER_TRACE ( jh , " exit "); <nl> return error ;
set_v4l_control ( struct inode * inode , <nl>  <nl> /* ----------------------------------------------------------------- */ <nl>  <nl> - const static unsigned int palette2pixelformat [] = { <nl> + static const unsigned int palette2pixelformat [] = { <nl> [ VIDEO_PALETTE_GREY ] = V4L2_PIX_FMT_GREY , <nl> [ VIDEO_PALETTE_RGB555 ] = V4L2_PIX_FMT_RGB555 , <nl> [ VIDEO_PALETTE_RGB565 ] = V4L2_PIX_FMT_RGB565 ,
int scsi_dh_activate ( struct request_queue * q , activate_complete fn , void * data ) <nl>  <nl> if (! sdev -> handler ) <nl> goto out_fn ; <nl> + err = SCSI_DH_NOTCONN ; <nl> if ( sdev -> sdev_state == SDEV_CANCEL || <nl> sdev -> sdev_state == SDEV_DEL ) <nl> goto out_fn ;
restart : <nl> } else { <nl> spin_unlock (& gl -> gl_spin ); <nl>  <nl> - new_gh = gfs2_holder_get ( gl , state , LM_FLAG_TRY , GFP_KERNEL ); <nl> + new_gh = gfs2_holder_get ( gl , state , LM_FLAG_TRY , GFP_NOFS ); <nl> if (! new_gh ) <nl> return ; <nl> set_bit ( HIF_DEMOTE , & new_gh -> gh_iflags );
static inline void update_sd_power_savings_stats ( struct sched_group * group , <nl> * capacity but still has some space to pick up some load <nl> * from other group and save more power <nl> */ <nl> - if ( sgs -> sum_nr_running > sgs -> group_capacity - 1 ) <nl> + if ( sgs -> sum_nr_running + 1 > sgs -> group_capacity ) <nl> return ; <nl>  <nl> if ( sgs -> sum_nr_running > sds -> leader_nr_running ||
static int __init gc_setup_pad ( struct gc * gc , int idx , int pad_type ) <nl> int i ; <nl> int err ; <nl>  <nl> - if ( pad_type < 1 || pad_type > GC_MAX ) { <nl> + if ( pad_type < 1 || pad_type >= GC_MAX ) { <nl> pr_err (" Pad type % d unknown \ n ", pad_type ); <nl> return - EINVAL ; <nl> }
static int sd_sdr_tuning ( struct rtsx_chip * chip ) <nl> int retval ; <nl>  <nl> retval = sd_tuning_tx ( chip ); <nl> - if ( retval != STATUS_SUCCESS ) { <nl> + if ( retval != STATUS_SUCCESS ) <nl> TRACE_RET ( chip , STATUS_FAIL ); <nl> - } <nl>  <nl> retval = sd_tuning_rx ( chip ); <nl> - if ( retval != STATUS_SUCCESS ) { <nl> + if ( retval != STATUS_SUCCESS ) <nl> TRACE_RET ( chip , STATUS_FAIL ); <nl> - } <nl>  <nl> return STATUS_SUCCESS ; <nl> }
static int __init gt641xx_timer0_clockevent_init ( void ) <nl>  <nl> cd = & gt641xx_timer0_clockevent ; <nl> cd -> rating = 200 + gt641xx_base_clock / 10000000 ; <nl> + clockevent_set_clock ( cd , gt641xx_base_clock ); <nl> cd -> max_delta_ns = clockevent_delta2ns ( 0x7fffffff , cd ); <nl> cd -> min_delta_ns = clockevent_delta2ns ( 0x300 , cd ); <nl> - clockevent_set_clock ( cd , gt641xx_base_clock ); <nl>  <nl> clockevents_register_device (& gt641xx_timer0_clockevent ); <nl> 
int skb_gro_receive ( struct sk_buff ** head , struct sk_buff * skb ) <nl> * NAPI_GRO_CB ( nskb ) = * NAPI_GRO_CB ( p ); <nl> skb_shinfo ( nskb )-> frag_list = p ; <nl> skb_shinfo ( nskb )-> gso_size = pinfo -> gso_size ; <nl> + pinfo -> gso_size = 0 ; <nl> skb_header_release ( p ); <nl> nskb -> prev = p ; <nl> 
int drm_mode_create_dumb_ioctl ( struct drm_device * dev , <nl> return - EINVAL ; <nl>  <nl> /* overflow checks for 32bit size calculations */ <nl> + /* NOTE : DIV_ROUND_UP () can overflow */ <nl> cpp = DIV_ROUND_UP ( args -> bpp , 8 ); <nl> - if ( cpp > 0xffffffffU / args -> width ) <nl> + if (! cpp || cpp > 0xffffffffU / args -> width ) <nl> return - EINVAL ; <nl> stride = cpp * args -> width ; <nl> if ( args -> height > 0xffffffffU / stride )
static void igb_update_ring_itr ( struct igb_q_vector * q_vector ) <nl> else <nl> new_val = avg_wire_size / 2 ; <nl>  <nl> + /* when in itr mode 3 do not exceed 20K ints / sec */ <nl> + if ( adapter -> rx_itr_setting == 3 && new_val < 196 ) <nl> + new_val = 196 ; <nl> + <nl> set_itr_val : <nl> if ( new_val != q_vector -> itr_val ) { <nl> q_vector -> itr_val = new_val ;
static int be_get_config ( struct be_adapter * adapter ) <nl> if ( status ) <nl> return status ; <nl>  <nl> - /* primary mac needs 1 pmac entry */ <nl> - adapter -> pmac_id = kcalloc ( be_max_uc ( adapter ) + 1 , sizeof ( u32 ), <nl> - GFP_KERNEL ); <nl> + adapter -> pmac_id = kcalloc ( be_max_uc ( adapter ), <nl> + sizeof (* adapter -> pmac_id ), GFP_KERNEL ); <nl> if (! adapter -> pmac_id ) <nl> return - ENOMEM ; <nl> 
void kvm_exit ( void ) <nl> kvm_arch_hardware_unsetup (); <nl> kvm_arch_exit (); <nl> free_cpumask_var ( cpus_hardware_enabled ); <nl> + __free_page ( fault_page ); <nl> __free_page ( hwpoison_page ); <nl> __free_page ( bad_page ); <nl> }
static void qlcnic_get_ethtool_stats ( struct net_device * dev , <nl> tx_ring = & adapter -> tx_ring [ ring ]; <nl> data = qlcnic_fill_tx_queue_stats ( data , tx_ring ); <nl> qlcnic_update_stats ( adapter ); <nl> + } else { <nl> + data += QLCNIC_TX_STATS_LEN ; <nl> } <nl> } <nl> 
static int _sp2d_alloc ( unsigned pages_in_unit , unsigned group_width , <nl> num_a1pa = min_t ( unsigned , PAGE_SIZE / sizeof__a1pa , <nl> pages_in_unit - i ); <nl>  <nl> - __a1pa = kzalloc ( num_a1pa * sizeof__a1pa , GFP_KERNEL ); <nl> + __a1pa = kcalloc ( num_a1pa , sizeof__a1pa , GFP_KERNEL ); <nl> if ( unlikely (! __a1pa )) { <nl> ORE_DBGMSG ("!! Failed to _alloc_1p_arrays =% d \ n ", <nl> num_a1pa );
static struct comedi_driver pcl726_driver = { <nl> module_comedi_driver ( pcl726_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for Advantech PCL - 726 & compatibles "); <nl> MODULE_LICENSE (" GPL ");
static void esdhc_writew_le ( struct sdhci_host * host , u16 val , int reg ) <nl> new_val |= ESDHC_VENDOR_SPEC_FRC_SDCLK_ON ; <nl> else <nl> new_val &= ~ ESDHC_VENDOR_SPEC_FRC_SDCLK_ON ; <nl> - writel ( new_val , host -> ioaddr + ESDHC_VENDOR_SPEC ); <nl> + writel ( new_val , host -> ioaddr + ESDHC_VENDOR_SPEC ); <nl> return ; <nl> case SDHCI_HOST_CONTROL2 : <nl> new_val = readl ( host -> ioaddr + ESDHC_VENDOR_SPEC );
int be_cmd_loopback_test ( struct be_adapter * adapter , u32 port_num , <nl>  <nl> be_cmd_hdr_prepare (& req -> hdr , CMD_SUBSYSTEM_LOWLEVEL , <nl> OPCODE_LOWLEVEL_LOOPBACK_TEST , sizeof (* req )); <nl> + req -> hdr . timeout = 4 ; <nl>  <nl> req -> pattern = cpu_to_le64 ( pattern ); <nl> req -> src_port = cpu_to_le32 ( port_num );
static void ext3_put_super ( struct super_block * sb ) <nl> } <nl> sb -> s_fs_info = NULL ; <nl> kfree ( sbi -> s_blockgroup_lock ); <nl> + mutex_destroy (& sbi -> s_orphan_lock ); <nl> + mutex_destroy (& sbi -> s_resize_lock ); <nl> kfree ( sbi ); <nl> } <nl> 
static void iwl_mvm_unshare_queue ( struct iwl_mvm * mvm , int queue ) <nl>  <nl> /* If aggs should be turned back on - do it */ <nl> if ( mvmsta -> tid_data [ tid ]. state == IWL_AGG_ON ) { <nl> - struct iwl_mvm_add_sta_cmd cmd ; <nl> + struct iwl_mvm_add_sta_cmd cmd = { 0 }; <nl>  <nl> mvmsta -> tid_disable_agg &= ~ BIT ( tid ); <nl> 
static inline void bio_list_add ( struct bio_list * bl , struct bio * bio ) <nl>  <nl> static inline void bio_list_merge ( struct bio_list * bl , struct bio_list * bl2 ) <nl> { <nl> + if (! bl2 -> head ) <nl> + return ; <nl> + <nl> if ( bl -> tail ) <nl> bl -> tail -> bi_next = bl2 -> head ; <nl> else
static int omap_mcbsp_dai_startup ( struct snd_pcm_substream * substream , <nl> * smaller buffer than the FIFO size to avoid underruns <nl> */ <nl> snd_pcm_hw_rule_add ( substream -> runtime , 0 , <nl> - SNDRV_PCM_HW_PARAM_CHANNELS , <nl> + SNDRV_PCM_HW_PARAM_BUFFER_SIZE , <nl> omap_mcbsp_hwrule_min_buffersize , <nl> mcbsp , <nl> - SNDRV_PCM_HW_PARAM_BUFFER_SIZE , - 1 ); <nl> + SNDRV_PCM_HW_PARAM_CHANNELS , - 1 ); <nl>  <nl> /* Make sure , that the period size is always even */ <nl> snd_pcm_hw_constraint_step ( substream -> runtime , 0 ,
int snd_hda_create_dig_out_ctls ( struct hda_codec * codec , <nl> return - EBUSY ; <nl> } <nl> spdif = snd_array_new (& codec -> spdif_out ); <nl> + if (! spdif ) <nl> + return - ENOMEM ; <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> if (! kctl )
parse_dcb15_entry ( struct drm_device * dev , struct dcb_table * dcb , <nl> entry -> tvconf . has_component_output = false ; <nl> break ; <nl> case OUTPUT_LVDS : <nl> - if (( conn & 0x00003f00 ) != 0x10 ) <nl> + if (( conn & 0x00003f00 ) >> 8 != 0x10 ) <nl> entry -> lvdsconf . use_straps_for_mode = true ; <nl> entry -> lvdsconf . use_power_scripts = true ; <nl> break ;
struct pci_dev * of_create_pci_dev ( struct pci_pbm_info * pbm , <nl> const char * type ; <nl> u32 class ; <nl>  <nl> - dev = kzalloc ( sizeof ( struct pci_dev ), GFP_KERNEL ); <nl> + dev = alloc_pci_dev (); <nl> if (! dev ) <nl> return NULL ; <nl> 
struct snd_soc_card { <nl> /* <nl> * Card - specific routes and widgets . <nl> */ <nl> - struct snd_soc_dapm_widget * dapm_widgets ; <nl> + const struct snd_soc_dapm_widget * dapm_widgets ; <nl> int num_dapm_widgets ; <nl> - struct snd_soc_dapm_route * dapm_routes ; <nl> + const struct snd_soc_dapm_route * dapm_routes ; <nl> int num_dapm_routes ; <nl>  <nl> struct work_struct deferred_resume_work ;
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> * queued ( again ) by ceph_update_snap_trace () <nl> * below . Queue it _now_ , under the old context . <nl> */ <nl> + spin_lock (& realm -> inodes_with_caps_lock ); <nl> list_del_init (& ci -> i_snap_realm_item ); <nl> + spin_unlock (& realm -> inodes_with_caps_lock ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> ceph_queue_cap_snap ( ci ,
static int e1000_change_mtu ( struct net_device * netdev , int new_mtu ) <nl> struct e1000_adapter * adapter = netdev_priv ( netdev ); <nl> int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN ; <nl>  <nl> - if (( max_frame < ETH_ZLEN + ETH_FCS_LEN ) || <nl> + if (( new_mtu < ETH_ZLEN + ETH_FCS_LEN + VLAN_HLEN ) || <nl> ( max_frame > MAX_JUMBO_FRAME_SIZE )) { <nl> e_err (" Invalid MTU setting \ n "); <nl> return - EINVAL ;
static int _hardware_enqueue ( struct ci_hw_ep * hwep , struct ci_hw_req * hwreq ) <nl> rest -= count ; <nl> } <nl>  <nl> - if ( hwreq -> req . zero && hwreq -> req . length <nl> + if ( hwreq -> req . zero && hwreq -> req . length && hwep -> dir == TX <nl> && ( hwreq -> req . length % hwep -> ep . maxpacket == 0 )) <nl> add_td_to_list ( hwep , hwreq , 0 ); <nl> 
static int mxt_lookup_bootloader_address ( struct mxt_data * data ) <nl> switch ( appmode ) { <nl> case 0x4a : <nl> case 0x4b : <nl> + /* Chips after 1664S use different scheme */ <nl> + if ( data -> info . family_id >= 0xa2 ) { <nl> + bootloader = appmode - 0x24 ; <nl> + break ; <nl> + } <nl> + /* Fall through for normal case */ <nl> case 0x4c : <nl> case 0x4d : <nl> case 0x5a :
xfs_allocbt_free_block ( <nl> xfs_extent_busy_insert ( cur -> bc_tp , be32_to_cpu ( agf -> agf_seqno ), bno , 1 , <nl> XFS_EXTENT_BUSY_SKIP_DISCARD ); <nl> xfs_trans_agbtree_delta ( cur -> bc_tp , - 1 ); <nl> + <nl> + xfs_trans_binval ( cur -> bc_tp , bp ); <nl> return 0 ; <nl> } <nl> 
static int ravb_close ( struct net_device * ndev ) <nl> priv -> phydev = NULL ; <nl> } <nl>  <nl> + if ( priv -> chip_id == RCAR_GEN3 ) <nl> + free_irq ( priv -> emac_irq , ndev ); <nl> free_irq ( ndev -> irq , ndev ); <nl>  <nl> napi_disable (& priv -> napi [ RAVB_NC ]);
card_probe_error : <nl> if ( card -> remove ) <nl> card -> remove ( card ); <nl>  <nl> + snd_soc_dapm_free (& card -> dapm ); <nl> soc_cleanup_card_debugfs ( card ); <nl> snd_card_free ( card -> snd_card ); <nl> 
static ssize_t gt_boost_freq_mhz_store ( struct device * kdev , <nl> { <nl> struct drm_minor * minor = dev_to_drm_minor ( kdev ); <nl> struct drm_device * dev = minor -> dev ; <nl> - struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> + struct drm_i915_private * dev_priv = to_i915 ( dev ); <nl> u32 val ; <nl> ssize_t ret ; <nl> 
int w1_process ( void * data ) <nl> jremain = 1 ; <nl> } <nl>  <nl> - try_to_freeze (); <nl> __set_current_state ( TASK_INTERRUPTIBLE ); <nl>  <nl> /* hold list_mutex until after interruptible to prevent loosing
out : <nl>  <nl> static void __init zynq_timer_init ( void ) <nl> { <nl> - zynq_early_slcr_init (); <nl> - <nl> zynq_clock_init (); <nl> of_clk_init ( NULL ); <nl> clocksource_probe (); <nl> static void __init zynq_map_io ( void ) <nl>  <nl> static void __init zynq_irq_init ( void ) <nl> { <nl> + zynq_early_slcr_init (); <nl> irqchip_init (); <nl> } <nl> 
pkttype_mt ( const struct sk_buff * skb , const struct net_device * in , <nl> const struct xt_pkttype_info * info = matchinfo ; <nl>  <nl> if ( skb -> pkt_type == PACKET_LOOPBACK ) <nl> - type = ipv4_is_multicast ( ip_hdr ( skb )-> daddr ) <nl> + type = match -> family == AF_INET && <nl> + ipv4_is_multicast ( ip_hdr ( skb )-> daddr ) <nl> ? PACKET_MULTICAST <nl> : PACKET_BROADCAST ; <nl> else
static ssize_t btrfs_direct_IO ( int rw , struct kiocb * iocb , <nl> btrfs_submit_direct , 0 ); <nl> } <nl>  <nl> +# define BTRFS_FIEMAP_FLAGS ( FIEMAP_FLAG_SYNC ) <nl> + <nl> static int btrfs_fiemap ( struct inode * inode , struct fiemap_extent_info * fieinfo , <nl> __u64 start , __u64 len ) <nl> { <nl> + int ret ; <nl> + <nl> + ret = fiemap_check_flags ( fieinfo , BTRFS_FIEMAP_FLAGS ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> return extent_fiemap ( inode , fieinfo , start , len , btrfs_get_extent_fiemap ); <nl> } <nl> 
static int __init hdaps_init ( void ) <nl>  <nl> if (! dmi_check_system ( hdaps_whitelist )) { <nl> printk ( KERN_WARNING " hdaps : supported laptop not found !\ n "); <nl> - ret = - ENXIO ; <nl> + ret = - ENODEV ; <nl> goto out ; <nl> } <nl> 
int mei_cl_connect ( struct mei_cl * cl , struct file * file ) <nl> mutex_lock (& dev -> device_lock ); <nl>  <nl> if ( cl -> state != MEI_FILE_CONNECTED ) { <nl> + cl -> state = MEI_FILE_DISCONNECTED ; <nl> /* something went really wrong */ <nl> if (! cl -> status ) <nl> cl -> status = - EFAULT ;
static void __init ati_bugs_contd ( int num , int slot , int func ) <nl> if ( rev >= 0x40 ) <nl> acpi_fix_pin2_polarity = 1 ; <nl>  <nl> - if ( rev > 0x13 ) <nl> + /* <nl> + * SB600 : revisions 0x11 , 0x12 , 0x13 , 0x14 , ... <nl> + * SB700 : revisions 0x39 , 0x3a , ... <nl> + * SB800 : revisions 0x40 , 0x41 , ... <nl> + */ <nl> + if ( rev >= 0x39 ) <nl> return ; <nl>  <nl> if ( acpi_use_timer_override )
static void intel_sanitize_crtc ( struct intel_crtc * crtc ) <nl> * ... */ <nl> plane = crtc -> plane ; <nl> crtc -> plane = ! plane ; <nl> + crtc -> primary_enabled = true ; <nl> dev_priv -> display . crtc_disable (& crtc -> base ); <nl> crtc -> plane = plane ; <nl> 
static inline void calculate_imbalance ( struct lb_env * env , struct sd_lb_stats * s <nl> * max load less than avg load ( as we skip the groups at or below <nl> * its cpu_power , while calculating max_load ..) <nl> */ <nl> - if ( busiest -> avg_load < sds -> avg_load ) { <nl> + if ( busiest -> avg_load <= sds -> avg_load || <nl> + local -> avg_load >= sds -> avg_load ) { <nl> env -> imbalance = 0 ; <nl> return fix_small_imbalance ( env , sds ); <nl> }
static int stripe_ctr ( struct dm_target * ti , unsigned int argc , char ** argv ) <nl> sc -> stripes_shift = __ffs ( stripes ); <nl>  <nl> r = dm_set_target_max_io_len ( ti , chunk_size ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( sc ); <nl> return r ; <nl> + } <nl>  <nl> ti -> num_flush_bios = stripes ; <nl> ti -> num_discard_bios = stripes ;
void rv770_set_uvd_clock_before_set_eng_clock ( struct radeon_device * rdev , <nl> if ( new_state -> high . sclk >= current_state -> high . sclk ) <nl> return ; <nl>  <nl> - radeon_set_uvd_clocks ( rdev , new_ps -> vclk , old_ps -> dclk ); <nl> + radeon_set_uvd_clocks ( rdev , new_ps -> vclk , new_ps -> dclk ); <nl> } <nl>  <nl> void rv770_set_uvd_clock_after_set_eng_clock ( struct radeon_device * rdev ,
retry : <nl> /* <nl> * Recalculate credits when extent tree depth changes . <nl> */ <nl> - if ( depth >= 0 && depth != ext_depth ( inode )) { <nl> + if ( depth != ext_depth ( inode )) { <nl> credits = ext4_chunk_trans_blocks ( inode , len ); <nl> depth = ext_depth ( inode ); <nl> }
static int mv643xx_eth_receive_queue ( struct net_device * dev ) <nl> netif_rx ( skb ); <nl> # endif <nl> } <nl> + dev -> last_rx = jiffies ; <nl> } <nl>  <nl> return received_packets ;
int inet_recv_error ( struct sock * sk , struct msghdr * msg , int len , <nl>  <nl> static inline void inet_ctl_sock_destroy ( struct sock * sk ) <nl> { <nl> - sock_release ( sk -> sk_socket ); <nl> + if ( sk ) <nl> + sock_release ( sk -> sk_socket ); <nl> } <nl>  <nl> # endif
static int shmem_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> i_size_write ( inode , newsize ); <nl> inode -> i_ctime = inode -> i_mtime = CURRENT_TIME ; <nl> } <nl> - if ( newsize < oldsize ) { <nl> + if ( newsize <= oldsize ) { <nl> loff_t holebegin = round_up ( newsize , PAGE_SIZE ); <nl> unmap_mapping_range ( inode -> i_mapping , holebegin , 0 , 1 ); <nl> shmem_truncate_range ( inode , newsize , ( loff_t )- 1 );
# endif <nl> # include " igb . h " <nl>  <nl> -# define DRV_VERSION " 2 . 4 . 13 - k2 " <nl> +# define MAJ 3 <nl> +# define MIN 0 <nl> +# define BUILD 6 <nl> +# define KFIX 2 <nl> +# define DRV_VERSION __stringify ( MAJ ) "." __stringify ( MIN ) "." \ <nl> + __stringify ( BUILD ) "- k " __stringify ( KFIX ) <nl> char igb_driver_name [] = " igb "; <nl> char igb_driver_version [] = DRV_VERSION ; <nl> static const char igb_driver_string [] =
static int __diag_ipl_functions ( struct kvm_vcpu * vcpu ) <nl>  <nl> VCPU_EVENT ( vcpu , 5 , " diag ipl functions , subcode % lx ", subcode ); <nl> switch ( subcode ) { <nl> + case 0 : <nl> + case 1 : <nl> + page_table_reset_pgste ( current -> mm , 0 , TASK_SIZE ); <nl> + return - EOPNOTSUPP ; <nl> case 3 : <nl> vcpu -> run -> s390_reset_flags = KVM_S390_RESET_CLEAR ; <nl> page_table_reset_pgste ( current -> mm , 0 , TASK_SIZE );
static int __devinit isp1761_pci_probe ( struct pci_dev * dev , <nl> hcd = isp1760_register ( pci_mem_phy0 , length , dev -> irq , <nl> IRQF_SHARED | IRQF_DISABLED , & dev -> dev , dev_name (& dev -> dev ), <nl> devflags ); <nl> - pci_set_drvdata ( dev , hcd ); <nl> - if (! hcd ) <nl> + if (! IS_ERR ( hcd )) { <nl> + pci_set_drvdata ( dev , hcd ); <nl> return 0 ; <nl> + } <nl> clean : <nl> status = - ENODEV ; <nl> iounmap ( iobase );
void perf_hpp__column_disable ( unsigned col ) <nl>  <nl> void perf_hpp__cancel_cumulate ( void ) <nl> { <nl> + if ( field_order ) <nl> + return ; <nl> + <nl> perf_hpp__column_disable ( PERF_HPP__OVERHEAD_ACC ); <nl> perf_hpp__format [ PERF_HPP__OVERHEAD ]. header = hpp__header_overhead ; <nl> }
struct nv40_mpeg_priv { <nl> }; <nl>  <nl> struct nv40_mpeg_chan { <nl> - struct nouveau_mpeg base ; <nl> + struct nouveau_mpeg_chan base ; <nl> }; <nl>  <nl> /*******************************************************************************
int mccic_irq ( struct mcam_camera * cam , unsigned int irqs ) <nl> if ( irqs & ( IRQ_EOF0 << frame )) { <nl> mcam_frame_complete ( cam , frame ); <nl> handled = 1 ; <nl> + if ( cam -> buffer_mode == B_DMA_sg ) <nl> + break ; <nl> } <nl> /* <nl> * If a frame starts , note that we have DMA active . This
static ssize_t set_pwm_mode ( struct device * dev , struct device_attribute * attr , <nl> if (!( val == 0 || val == 1 )) <nl> return - EINVAL ; <nl>  <nl> + /* F75373 does not support DC ( linear voltage ) fan control mode */ <nl> + if ( data -> kind == f75373 && val == 0 ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& data -> update_lock ); <nl> conf = f75375_read8 ( client , F75375_REG_CONFIG1 ); <nl> conf &= ~( 1 << FAN_CTRL_LINEAR ( nr ));
static u32 mop500_sdi0_vdd_handler ( struct device * dev , unsigned int vdd , <nl> unsigned char power_mode ) <nl> { <nl> if ( power_mode == MMC_POWER_UP ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 1 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 1 ); <nl> else if ( power_mode == MMC_POWER_OFF ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 0 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 0 ); <nl>  <nl> return MCI_FBCLKEN | MCI_CMDDIREN | MCI_DATA0DIREN | <nl> MCI_DATA2DIREN | MCI_DATA31DIREN ;
static void bfin_musb_try_idle ( struct musb * musb , unsigned long timeout ) <nl> mod_timer (& musb_conn_timer , jiffies + TIMER_DELAY ); <nl> } <nl>  <nl> - static int bfin_musb_get_vbus_status ( struct musb * musb ) <nl> + static int bfin_musb_vbus_status ( struct musb * musb ) <nl> { <nl> return 0 ; <nl> }
static int ocfs2_rename ( struct inode * old_dir , <nl> * <nl> * And that ' s why , just like the VFS , we need a file system <nl> * rename lock . */ <nl> - if ( old_dentry != new_dentry ) { <nl> + if ( old_dir != new_dir && S_ISDIR ( old_inode -> i_mode )) { <nl> status = ocfs2_rename_lock ( osb ); <nl> if ( status < 0 ) { <nl> mlog_errno ( status );
static int hws_cpu_callback ( struct notifier_block * nfb , <nl> { <nl> /* We do not have sampler space available for all possible CPUs . <nl> All CPUs should be online when hw sampling is activated . */ <nl> - return NOTIFY_BAD ; <nl> + return ( hws_state <= HWS_DEALLOCATED ) ? NOTIFY_OK : NOTIFY_BAD ; <nl> } <nl>  <nl> static struct notifier_block hws_cpu_notifier = {
static int __rfcomm_dlc_close ( struct rfcomm_dlc * d , int err ) <nl> rfcomm_dlc_unlock ( d ); <nl>  <nl> skb_queue_purge (& d -> tx_queue ); <nl> - rfcomm_session_put ( s ); <nl> - <nl> rfcomm_dlc_unlink ( d ); <nl> } <nl>  <nl> static struct rfcomm_session * rfcomm_session_create ( bdaddr_t * src , bdaddr_t * dst <nl> goto failed ; <nl> } <nl>  <nl> - rfcomm_session_hold ( s ); <nl> - <nl> s -> initiator = 1 ; <nl>  <nl> bacpy (& addr . l2_bdaddr , dst );
struct kvm_vcpu * kvm_arch_vcpu_create ( struct kvm * kvm , unsigned int id ) <nl> int err ; <nl> struct kvm_vcpu * vcpu ; <nl>  <nl> + if ( irqchip_in_kernel ( kvm ) && vgic_initialized ( kvm )) { <nl> + err = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> vcpu = kmem_cache_zalloc ( kvm_vcpu_cache , GFP_KERNEL ); <nl> if (! vcpu ) { <nl> err = - ENOMEM ;
static ssize_t dfs_file_write ( struct file * file , const char __user * user_buf , <nl> } else if ( dent == d -> dfs_emulate_power_cut ) { <nl> if ( kstrtoint ( buf , 0 , & val ) != 0 ) <nl> count = - EINVAL ; <nl> - d -> emulate_power_cut = val ; <nl> + else <nl> + d -> emulate_power_cut = val ; <nl> goto out ; <nl> } <nl> 
cy_ioctl ( struct tty_struct * tty , <nl> break ; <nl> # ifndef CONFIG_CYZ_INTR <nl> case CYZSETPOLLCYCLE : <nl> + if ( arg > LONG_MAX / HZ ) <nl> + return - ENODEV ; <nl> cyz_polling_cycle = ( arg * HZ ) / 1000 ; <nl> break ; <nl> case CYZGETPOLLCYCLE :
static int em28xx_i2c_xfer ( struct i2c_adapter * i2c_adap , <nl> if ( dev -> disconnected ) <nl> return - ENODEV ; <nl>  <nl> - rc = rt_mutex_trylock (& dev -> i2c_bus_lock ); <nl> - if ( rc < 0 ) <nl> - return rc ; <nl> + if (! rt_mutex_trylock (& dev -> i2c_bus_lock )) <nl> + return - EAGAIN ; <nl>  <nl> /* Switch I2C bus if needed */ <nl> if ( bus != dev -> cur_i2c_bus &&
EXPORT_SYMBOL_GPL ( omap_dm_timer_enable ); <nl>  <nl> void omap_dm_timer_disable ( struct omap_dm_timer * timer ) <nl> { <nl> - pm_runtime_put (& timer -> pdev -> dev ); <nl> + pm_runtime_put_sync (& timer -> pdev -> dev ); <nl> } <nl> EXPORT_SYMBOL_GPL ( omap_dm_timer_disable ); <nl> 
static int cxgb_extension_ioctl ( struct net_device * dev , void __user * useraddr ) <nl> case CHELSIO_GET_QSET_NUM :{ <nl> struct ch_reg edata ; <nl>  <nl> + memset (& edata , 0 , sizeof ( struct ch_reg )); <nl> + <nl> edata . cmd = CHELSIO_GET_QSET_NUM ; <nl> edata . val = pi -> nqsets ; <nl> if ( copy_to_user ( useraddr , & edata , sizeof ( edata )))
static ssize_t macvtap_put_user ( struct macvtap_queue * q , <nl> if ( copy_to_iter (& vnet_hdr , sizeof ( vnet_hdr ), iter ) != <nl> sizeof ( vnet_hdr )) <nl> return - EFAULT ; <nl> + <nl> + iov_iter_advance ( iter , vnet_hdr_len - sizeof ( vnet_hdr )); <nl> } <nl> total = vnet_hdr_len ; <nl> total += skb -> len ;
static pci_ers_result_t atl1c_io_error_detected ( struct pci_dev * pdev , <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + if ( state == pci_channel_io_perm_failure ) <nl> + return PCI_ERS_RESULT_DISCONNECT ; <nl> + <nl> if ( netif_running ( netdev )) <nl> atl1c_down ( adapter ); <nl> 
static int rx_intr_handler ( struct ring_info * ring_data , int budget ) <nl> struct RxD1 * rxdp1 ; <nl> struct RxD3 * rxdp3 ; <nl>  <nl> + if ( budget <= 0 ) <nl> + return napi_pkts ; <nl> + <nl> get_info = ring_data -> rx_curr_get_info ; <nl> get_block = get_info . block_index ; <nl> memcpy (& put_info , & ring_data -> rx_curr_put_info , sizeof ( put_info ));
static void tilcdc_crtc_destroy ( struct drm_crtc * crtc ) <nl> struct tilcdc_crtc * tilcdc_crtc = to_tilcdc_crtc ( crtc ); <nl> struct tilcdc_drm_private * priv = crtc -> dev -> dev_private ; <nl>  <nl> + drm_modeset_lock_crtc ( crtc , NULL ); <nl> tilcdc_crtc_disable ( crtc ); <nl> + drm_modeset_unlock_crtc ( crtc ); <nl>  <nl> flush_workqueue ( priv -> wq ); <nl> 
static void xmit_common ( struct sk_buff * skb , struct ehea_swqe * swqe ) <nl> { <nl> swqe -> tx_control |= EHEA_SWQE_IMM_DATA_PRESENT | EHEA_SWQE_CRC ; <nl>  <nl> - if ( skb -> protocol != htons ( ETH_P_IP )) <nl> + if ( vlan_get_protocol ( skb ) != htons ( ETH_P_IP )) <nl> return ; <nl>  <nl> if ( skb -> ip_summed == CHECKSUM_PARTIAL )
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
int __init acpi_parse_mcfg ( unsigned long phys_addr , unsigned long size ) <nl> if ( mcfg -> config [ i ]. base_reserved ) { <nl> printk ( KERN_ERR PREFIX <nl> " MMCONFIG not in low 4GB of memory \ n "); <nl> + kfree ( pci_mmcfg_config ); <nl> + pci_mmcfg_config_num = 0 ; <nl> return - ENODEV ; <nl> } <nl> }
static struct resource wdt_sch_resource = { <nl>  <nl> static struct mfd_cell tunnelcreek_cells [] = { <nl> { <nl> - . name = " tunnelcreek_wdt ", <nl> + . name = " ie6xx_wdt ", <nl> . num_resources = 1 , <nl> . resources = & wdt_sch_resource , <nl> },
static int jr3_pci_auto_attach ( struct comedi_device * dev , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if ( pci_resource_len ( pcidev , 0 ) < board -> n_subdevs * sizeof (* block )) <nl> + return - ENXIO ; <nl> + <nl> dev -> mmio = pci_ioremap_bar ( pcidev , 0 ); <nl> if (! dev -> mmio ) <nl> return - ENOMEM ;
int p54_parse_eeprom ( struct ieee80211_hw * dev , void * eeprom , int len ) <nl> case PDR_END : <nl> i = len ; <nl> break ; <nl> + default : <nl> + printk ( KERN_INFO " p54 : unknown eeprom code : 0x % x \ n ", <nl> + le16_to_cpu ( entry -> code )); <nl> + break ; <nl> } <nl>  <nl> entry = ( void *) entry + ( entry_len + 1 )* 2 ;
ssize_t uwb_est_get_size ( struct uwb_rc * uwb_rc , struct uwb_est * est , <nl> case UWB_EST_8 : type_size = sizeof ( u8 ); break ; <nl> default : BUG (); <nl> } <nl> - if ( offset + type_size >= rceb_size ) { <nl> + if ( offset + type_size > rceb_size ) { <nl> if ( printk_ratelimit ()) <nl> dev_err ( dev , " EST % p 0x % 04x /% 04x /% 04x [% u ]: " <nl> " not enough data to read extra size \ n ",
static int create_device ( struct ramzswap * rzs , int device_id ) <nl> * or set equal to backing swap device ( if provided ) <nl> */ <nl> set_capacity ( rzs -> disk , 0 ); <nl> + <nl> + blk_queue_physical_block_size ( rzs -> disk -> queue , PAGE_SIZE ); <nl> + blk_queue_logical_block_size ( rzs -> disk -> queue , PAGE_SIZE ); <nl> + <nl> add_disk ( rzs -> disk ); <nl>  <nl> rzs -> init_done = 0 ;
static struct platform_driver snvs_rtc_driver = { <nl> . name = " snvs_rtc ", <nl> . owner = THIS_MODULE , <nl> . pm = & snvs_rtc_pm_ops , <nl> - . of_match_table = of_match_ptr ( snvs_dt_ids ), <nl> + . of_match_table = snvs_dt_ids , <nl> }, <nl> . probe = snvs_rtc_probe , <nl> };
static int slic_mcast_add_list ( struct adapter * adapter , char * address ) <nl> } <nl>  <nl> /* Doesn ' t already exist . Allocate a structure to hold it */ <nl> - mcaddr = kmalloc ( sizeof ( struct mcast_address ), GFP_ATOMIC ); <nl> + mcaddr = kmalloc ( sizeof (* mcaddr ), GFP_ATOMIC ); <nl> if ( mcaddr == NULL ) <nl> return 1 ; <nl> 
int dns_query ( const char * type , const char * name , size_t namelen , <nl> if (!* _result ) <nl> goto put ; <nl>  <nl> - memcpy (* _result , upayload -> data , len + 1 ); <nl> + memcpy (* _result , upayload -> data , len ); <nl> + * _result [ len ] = '\ 0 '; <nl> + <nl> if ( _expiry ) <nl> * _expiry = rkey -> expiry ; <nl> 
static void gen3_init_clock_gating ( struct drm_device * dev ) <nl> dstate |= DSTATE_PLL_D3_OFF | DSTATE_GFX_CLOCK_GATING | <nl> DSTATE_DOT_CLOCK_GATING ; <nl> I915_WRITE ( D_STATE , dstate ); <nl> + <nl> + if ( IS_PINEVIEW ( dev )) <nl> + I915_WRITE ( ECOSKPD , _MASKED_BIT_ENABLE ( ECO_GATING_CX_ONLY )); <nl> } <nl>  <nl> static void i85x_init_clock_gating ( struct drm_device * dev )
static long vop_ioctl ( struct file * f , unsigned int cmd , unsigned long arg ) <nl> ret = - EFAULT ; <nl> goto free_ret ; <nl> } <nl> + /* Ensure desc has not changed between the two reads */ <nl> + if ( memcmp (& dd , dd_config , sizeof ( dd ))) { <nl> + ret = - EINVAL ; <nl> + goto free_ret ; <nl> + } <nl> mutex_lock (& vdev -> vdev_mutex ); <nl> mutex_lock (& vi -> vop_mutex ); <nl> ret = vop_virtio_add_device ( vdev , dd_config );
static int evm_protect_xattr ( struct dentry * dentry , const char * xattr_name , <nl> goto out ; <nl> } <nl> evm_status = evm_verify_current_integrity ( dentry ); <nl> + if ( evm_status == INTEGRITY_NOXATTRS ) { <nl> + struct integrity_iint_cache * iint ; <nl> + <nl> + iint = integrity_iint_find ( dentry -> d_inode ); <nl> + if ( iint && ( iint -> flags & IMA_NEW_FILE )) <nl> + return 0 ; <nl> + } <nl> out : <nl> if ( evm_status != INTEGRITY_PASS ) <nl> integrity_audit_msg ( AUDIT_INTEGRITY_METADATA , dentry -> d_inode ,
int snd_hda_codec_reset ( struct hda_codec * codec ) <nl> codec -> num_pcms = 0 ; <nl> codec -> pcm_info = NULL ; <nl> codec -> preset = NULL ; <nl> + memset (& codec -> patch_ops , 0 , sizeof ( codec -> patch_ops )); <nl> + codec -> slave_dig_outs = NULL ; <nl> + codec -> spdif_status_reset = 0 ; <nl> module_put ( codec -> owner ); <nl> codec -> owner = NULL ; <nl> 
struct iwl_cfg iwl6000i_2bg_cfg = { <nl> . fw_name_pre = IWL6050_FW_PRE , \ <nl> . ucode_api_max = IWL6050_UCODE_API_MAX , \ <nl> . ucode_api_min = IWL6050_UCODE_API_MIN , \ <nl> + . valid_tx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> + . valid_rx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> . ops = & iwl6050_ops , \ <nl> . eeprom_ver = EEPROM_6050_EEPROM_VERSION , \ <nl> . eeprom_calib_ver = EEPROM_6050_TX_POWER_VERSION , \
static struct hv_pci_dev * new_pcichild_device ( struct hv_pcibus_device * hbus , <nl> struct hv_pci_dev * hpdev ; <nl> struct pci_child_message * res_req ; <nl> struct q_res_req_compl comp_pkt ; <nl> - union { <nl> - struct pci_packet init_packet ; <nl> - u8 buffer [ 0x100 ]; <nl> + struct { <nl> + struct pci_packet init_packet ; <nl> + u8 buffer [ sizeof ( struct pci_child_message )]; <nl> } pkt ; <nl> unsigned long flags ; <nl> int ret ;
static int i2s_pll_clk_probe ( struct platform_device * pdev ) <nl> if ( IS_ERR ( pll_clk -> base )) <nl> return PTR_ERR ( pll_clk -> base ); <nl>  <nl> + memset (& init , 0 , sizeof ( init )); <nl> clk_name = node -> name ; <nl> init . name = clk_name ; <nl> init . ops = & i2s_pll_ops ;
static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x23 , { KEY_PROG3 } }, /* P_Key */ <nl> { KE_KEY , 0x24 , { KEY_PROG4 } }, /* Social networking_Key */ <nl> { KE_KEY , 0x64 , { KEY_SWITCHVIDEOMODE } }, /* Display Switch */ <nl> - { KE_KEY , 0x82 , { KEY_F22 } }, /* Touch Pad On / Off */ <nl> + { KE_KEY , 0x82 , { KEY_TOUCHPAD_TOGGLE } }, /* Touch Pad On / Off */ <nl> { KE_END , 0 } <nl> }; <nl> 
int symsrc__init ( struct symsrc * ss , struct dso * dso __maybe_unused , <nl> if (! ss -> name ) <nl> goto out_close ; <nl>  <nl> + ss -> fd = fd ; <nl> ss -> type = type ; <nl>  <nl> return 0 ;
cifs_set_file_info ( struct inode * inode , struct iattr * attrs , int xid , <nl> struct cifsTconInfo * pTcon = cifs_sb -> tcon ; <nl> FILE_BASIC_INFO info_buf ; <nl>  <nl> + if ( attrs == NULL ) <nl> + return - EINVAL ; <nl> + <nl> if ( attrs -> ia_valid & ATTR_ATIME ) { <nl> set_time = true ; <nl> info_buf . LastAccessTime =
static void EChannel_proc_rcv ( struct hisax_d_if * d_if ) <nl> # ifdef CONFIG_PCI <nl> # include < linux / pci . h > <nl>  <nl> - static struct pci_device_id hisax_pci_tbl [] __devinitdata = { <nl> + static struct pci_device_id hisax_pci_tbl [] __devinitdata __used = { <nl> # ifdef CONFIG_HISAX_FRITZPCI <nl> { PCI_VDEVICE ( AVM , PCI_DEVICE_ID_AVM_A1 ) }, <nl> # endif
static int rt2800_get_gain_calibration_delta ( struct rt2x00_dev * rt2x00dev ) <nl> u8 step ; <nl> int i ; <nl>  <nl> + /* <nl> + * First check if temperature compensation is supported . <nl> + */ <nl> + rt2800_eeprom_read ( rt2x00dev , EEPROM_NIC_CONF1 , & eeprom ); <nl> + if (! rt2x00_get_field16 ( eeprom , EEPROM_NIC_CONF1_EXTERNAL_TX_ALC )) <nl> + return 0 ; <nl> + <nl> /* <nl> * Read TSSI boundaries for temperature compensation from <nl> * the EEPROM .
static void frontend_init ( struct dvb_bt8xx_card * card , u32 type ) <nl> /* DST is not a frontend , attaching the ASIC */ <nl> if ( dvb_attach ( dst_attach , state , & card -> dvb_adapter ) == NULL ) { <nl> pr_err ("% s : Could not find a Twinhan DST \ n ", __func__ ); <nl> + kfree ( state ); <nl> break ; <nl> } <nl> /* Attach other DST peripherals if any */
static void logfs_put_super ( struct super_block * sb ) <nl> { <nl> struct logfs_super * super = logfs_super ( sb ); <nl> /* kill the meta - inodes */ <nl> - iput ( super -> s_master_inode ); <nl> iput ( super -> s_segfile_inode ); <nl> + iput ( super -> s_master_inode ); <nl> iput ( super -> s_mapping_inode ); <nl> } <nl> 
static int si_common_early_init ( void * handle ) <nl> AMD_CG_SUPPORT_HDP_LS | <nl> AMD_CG_SUPPORT_HDP_MGCG ; <nl> adev -> pg_flags = 0 ; <nl> + adev -> external_rev_id = 60 ; <nl> break ; <nl> case CHIP_HAINAN : <nl> adev -> cg_flags =
int mwifiex_bss_start ( struct mwifiex_private * priv , struct cfg80211_bss * bss , <nl> mwifiex_dbg ( adapter , ERROR , <nl> " Attempt to reconnect on csa closed chan (% d )\ n ", <nl> bss_desc -> channel ); <nl> + ret = - 1 ; <nl> goto done ; <nl> } <nl> 
int eeh_dn_check_failure ( struct device_node * dn , struct pci_dev * dev ) <nl> no_dn ++; <nl> return 0 ; <nl> } <nl> + dn = find_device_pe ( dn ); <nl> pdn = PCI_DN ( dn ); <nl>  <nl> /* Access to IO BARs might get this far and still not want checking . */
typedef struct { <nl> uint8_t max_lun ; <nl>  <nl> uint32_t unique_id ; <nl> - uint8_t irq ; <nl> + int irq ; <nl> uint8_t ito ; <nl> caddr_t ibuf ; <nl> dma_addr_t ibuf_dma_h ;
struct dvb_frontend * dib9000_attach ( struct i2c_adapter * i2c_adap , u8 i2c_addr , c <nl> if ( st == NULL ) <nl> return NULL ; <nl> fe = kzalloc ( sizeof ( struct dvb_frontend ), GFP_KERNEL ); <nl> - if ( fe == NULL ) <nl> + if ( fe == NULL ) { <nl> + kfree ( st ); <nl> return NULL ; <nl> + } <nl>  <nl> memcpy (& st -> chip . d9 . cfg , cfg , sizeof ( struct dib9000_config )); <nl> st -> i2c . i2c_adap = i2c_adap ;
static int get_sig_strength ( struct drx_demod_instance * demod , u16 * sig_strength ) <nl> * sig_strength = ( 20 * if_gain / if_agc_sns ); <nl> } <nl>  <nl> + if (* sig_strength <= 7 ) <nl> + * sig_strength = 0 ; <nl> + <nl> return 0 ; <nl> rw_error : <nl> return - EIO ;
unsigned int ata_sff_qc_issue ( struct ata_queued_cmd * qc ) <nl> break ; <nl>  <nl> default : <nl> - WARN_ON_ONCE ( 1 ); <nl> return AC_ERR_SYSTEM ; <nl> } <nl> 
static void cyberjack_read_int_callback ( struct urb * urb ) <nl>  <nl> old_rdtodo = priv -> rdtodo ; <nl>  <nl> - if ( old_rdtodo + size < old_rdtodo ) { <nl> + if ( old_rdtodo > SHRT_MAX - size ) { <nl> dev_dbg ( dev , " To many bulk_in urbs to do .\ n "); <nl> spin_unlock (& priv -> lock ); <nl> goto resubmit ;
struct wm8994_ldo_pdata { <nl> int enable ; <nl>  <nl> const char * supply ; <nl> - struct regulator_init_data * init_data ; <nl> + const struct regulator_init_data * init_data ; <nl> }; <nl>  <nl> # define WM8994_CONFIGURE_GPIO 0x10000
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> if (!( src_file -> f_mode & FMODE_READ )) <nl> goto out_fput ; <nl>  <nl> + /* don ' t make the dst file partly checksummed */ <nl> + if (( BTRFS_I ( src )-> flags & BTRFS_INODE_NODATASUM ) != <nl> + ( BTRFS_I ( inode )-> flags & BTRFS_INODE_NODATASUM )) <nl> + goto out_fput ; <nl> + <nl> ret = - EISDIR ; <nl> if ( S_ISDIR ( src -> i_mode ) || S_ISDIR ( inode -> i_mode )) <nl> goto out_fput ;
struct device_type usb_device_type = { <nl>  <nl> static int ksuspend_usb_init ( void ) <nl> { <nl> - ksuspend_usb_wq = create_singlethread_workqueue (" ksuspend_usbd "); <nl> + /* This workqueue is supposed to be both freezable and <nl> + * singlethreaded . Its job doesn ' t justify running on more <nl> + * than one CPU . <nl> + */ <nl> + ksuspend_usb_wq = create_freezeable_workqueue (" ksuspend_usbd "); <nl> if (! ksuspend_usb_wq ) <nl> return - ENOMEM ; <nl> return 0 ;
static void get_new_segment ( struct f2fs_sb_info * sbi , <nl> if (! new_sec && ((* newseg + 1 ) % sbi -> segs_per_sec )) { <nl> segno = find_next_zero_bit ( free_i -> free_segmap , <nl> TOTAL_SEGS ( sbi ), * newseg + 1 ); <nl> - if ( segno < TOTAL_SEGS ( sbi )) <nl> + if ( segno - * newseg < sbi -> segs_per_sec - <nl> + (* newseg % sbi -> segs_per_sec )) <nl> goto got_it ; <nl> } <nl> find_other_zone :
void r8712_joinbss_event_callback ( struct _adapter * adapter , u8 * pbuf ) <nl>  <nl> if ( sizeof ( struct list_head ) == 4 * sizeof ( u32 )) { <nl> pnetwork = kmalloc ( sizeof ( struct wlan_network ), GFP_ATOMIC ); <nl> + if (! pnetwork ) <nl> + return ; <nl> memcpy (( u8 *) pnetwork + 16 , ( u8 *) pbuf + 8 , <nl> sizeof ( struct wlan_network ) - 16 ); <nl> } else
int sirfsoc_uart_probe ( struct platform_device * pdev ) <nl>  <nl> if ( sirfport -> hw_flow_ctrl ) { <nl> sirfport -> p = pinctrl_get_select_default (& pdev -> dev ); <nl> - ret = IS_ERR ( sirfport -> p ); <nl> - if ( ret ) <nl> + if ( IS_ERR ( sirfport -> p )) { <nl> + ret = PTR_ERR ( sirfport -> p ); <nl> goto err ; <nl> + } <nl> } <nl>  <nl> sirfport -> clk = clk_get (& pdev -> dev , NULL );
static void do_ubd_request ( struct request_queue * q ) <nl> " errno = % d \ n ", - n ); <nl> else if ( list_empty (& dev -> restart )) <nl> list_add (& dev -> restart , & restart ); <nl> + kfree ( io_req ); <nl> return ; <nl> } <nl> 
static int imx_thermal_probe ( struct platform_device * pdev ) <nl> data -> tempmon = map ; <nl>  <nl> data -> socdata = of_device_get_match_data (& pdev -> dev ); <nl> + if (! data -> socdata ) { <nl> + dev_err (& pdev -> dev , " no device match found \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> /* make sure the IRQ flag is clear before enabling irq on i . MX6SX */ <nl> if ( data -> socdata -> version == TEMPMON_IMX6SX ) {
static void kvmppc_fast_vcpu_kick_hv ( struct kvm_vcpu * vcpu ) <nl> ++ vcpu -> stat . halt_wakeup ; <nl> } <nl>  <nl> - if ( kvmppc_ipi_thread ( vcpu -> arch . thread_cpu )) <nl> + cpu = READ_ONCE ( vcpu -> arch . thread_cpu ); <nl> + if ( cpu >= 0 && kvmppc_ipi_thread ( cpu )) <nl> return ; <nl>  <nl> /* CPU points to the first thread of the core */
int fsl_rio_setup_rmu ( struct rio_mport * mport , struct device_node * node ) <nl> if (! msg_addr ) { <nl> pr_err ("% s : unable to find ' reg ' property of message - unit \ n ", <nl> node -> full_name ); <nl> + kfree ( rmu ); <nl> return - ENOMEM ; <nl> } <nl> msg_start = of_read_number ( msg_addr , aw );
static int pcm3168a_set_dai_sysclk ( struct snd_soc_dai * dai , <nl> int clk_id , unsigned int freq , int dir ) <nl> { <nl> struct pcm3168a_priv * pcm3168a = snd_soc_codec_get_drvdata ( dai -> codec ); <nl> + int ret ; <nl>  <nl> if ( freq > PCM1368A_MAX_SYSCLK ) <nl> return - EINVAL ; <nl>  <nl> + ret = clk_set_rate ( pcm3168a -> scki , freq ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> pcm3168a -> sysclk = freq ; <nl>  <nl> return 0 ;
static int st33zp24_spi_evaluate_latency ( void * phy_id ) <nl> & data , 1 ); <nl> latency ++; <nl> } <nl> + if ( status < 0 ) <nl> + return status ; <nl> + if ( latency == MAX_SPI_LATENCY ) <nl> + return - ENODEV ; <nl> + <nl> return latency - 1 ; <nl> } /* evaluate_latency () */ <nl> 
static void snd_card_asihpi_timer_function ( unsigned long data ) <nl> s -> number ); <nl> ds -> drained_count ++; <nl> if ( ds -> drained_count > 20 ) { <nl> + unsigned long flags ; <nl> + snd_pcm_stream_lock_irqsave ( s , flags ); <nl> snd_pcm_stop ( s , SNDRV_PCM_STATE_XRUN ); <nl> + snd_pcm_stream_unlock_irqrestore ( s , flags ); <nl> continue ; <nl> } <nl> } else {
static ide_startstop_t cdrom_do_block_pc ( ide_drive_t * drive , struct request * rq ) <nl> /* <nl> * check if dma is safe <nl> */ <nl> - if (( rq -> data_len & mask ) || ( addr & mask )) <nl> + if (( rq -> data_len & 3 ) || ( addr & mask )) <nl> info -> dma = 0 ; <nl> } <nl> 
static void proc_flush_task_mnt ( struct vfsmount * mnt , pid_t pid , pid_t tgid ) <nl> dput ( dentry ); <nl> } <nl>  <nl> + if ( pid == tgid ) <nl> + return ; <nl> + <nl> name . name = buf ; <nl> name . len = snprintf ( buf , sizeof ( buf ), "% d ", tgid ); <nl> leader = d_hash_and_lookup ( mnt -> mnt_root , & name );
int gpiochip_add_data ( struct gpio_chip * chip , void * data ) <nl> * First : allocate and populate the internal stat container , and <nl> * set up the struct device . <nl> */ <nl> - gdev = kmalloc ( sizeof (* gdev ), GFP_KERNEL ); <nl> + gdev = kzalloc ( sizeof (* gdev ), GFP_KERNEL ); <nl> if (! gdev ) <nl> return - ENOMEM ; <nl> gdev -> dev . bus = & gpio_bus_type ;
static int lx_pipe_wait_for_state ( struct lx6464es * chip , u32 pipe , <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> - if ( current_state == state ) <nl> + if (! err && current_state == state ) <nl> return 0 ; <nl>  <nl> mdelay ( 1 );
static void lkdtm_do_action ( enum ctype which ) <nl> break ; <nl>  <nl> val = kmalloc ( 1024 , GFP_KERNEL ); <nl> - if (! val ) <nl> + if (! val ) { <nl> + free_page ( p ); <nl> break ; <nl> + } <nl>  <nl> base = ( int *) p ; <nl> 
union acpi_parse_object ; <nl>  <nl> static char * acpi_gbl_mutex_names [ ACPI_NUM_MUTEX ] = { <nl> " ACPI_MTX_Interpreter ", <nl> - " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Namespace ", <nl> + " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Events ", <nl> " ACPI_MTX_Caches ", <nl> " ACPI_MTX_Memory ",
static inline void task_state ( struct seq_file * m , struct pid_namespace * ns , <nl> " FDSize :\ t % d \ n " <nl> " Groups :\ t ", <nl> fdt ? fdt -> max_fds : 0 ); <nl> + task_unlock ( p ); <nl> rcu_read_unlock (); <nl>  <nl> group_info = cred -> group_info ; <nl> - task_unlock ( p ); <nl> - <nl> for ( g = 0 ; g < group_info -> ngroups ; g ++) <nl> seq_printf ( m , "% d ", <nl> from_kgid_munged ( user_ns , GROUP_AT ( group_info , g )));
u32 crypto4xx_build_pd ( struct crypto_async_request * req , <nl>  <nl> /* figure how many gd is needed */ <nl> num_gd = sg_nents_for_len ( src , datalen ); <nl> + if (( int ) num_gd < 0 ) { <nl> + dev_err ( dev -> core_dev -> device , " Invalid number of src SG .\ n "); <nl> + return - EINVAL ; <nl> + } <nl> if ( num_gd == 1 ) <nl> num_gd = 0 ; <nl> 
cont : <nl> PAGE_SET_WRITEBACK | <nl> page_error_op | <nl> PAGE_END_WRITEBACK ); <nl> - btrfs_free_reserved_data_space_noquota ( inode , start , <nl> - end - start + 1 ); <nl> + if ( ret == 0 ) <nl> + btrfs_free_reserved_data_space_noquota ( inode , <nl> + start , <nl> + end - start + 1 ); <nl> goto free_pages_out ; <nl> } <nl> }
static void virtcons_remove ( struct virtio_device * vdev ) <nl> /* Disable interrupts for vqs */ <nl> vdev -> config -> reset ( vdev ); <nl> /* Finish up work that ' s lined up */ <nl> - cancel_work_sync (& portdev -> control_work ); <nl> + if ( use_multiport ( portdev )) <nl> + cancel_work_sync (& portdev -> control_work ); <nl>  <nl> list_for_each_entry_safe ( port , port2 , & portdev -> ports , list ) <nl> unplug_port ( port );
static int tsi721_rio_map_inb_mem ( struct rio_mport * mport , dma_addr_t lstart , <nl> } else if ( ibw_start < ( ib_win -> rstart + ib_win -> size ) && <nl> ( ibw_start + ibw_size ) > ib_win -> rstart ) { <nl> /* Return error if address translation involved */ <nl> - if ( direct && ib_win -> xlat ) { <nl> + if (! direct || ib_win -> xlat ) { <nl> ret = - EFAULT ; <nl> break ; <nl> }
acpi_parse_lapic ( struct acpi_subtable_header * header , const unsigned long end ) <nl>  <nl> acpi_table_print_madt_entry ( header ); <nl>  <nl> + /* Ignore invalid ID */ <nl> + if ( processor -> id == 0xff ) <nl> + return 0 ; <nl> + <nl> /* <nl> * We need to register disabled CPU as well to permit <nl> * counting disabled CPUs . This allows us to size
static int et131x_phy_mii_read ( struct et131x_adapter * adapter , u8 addr , <nl> mii_indicator ); <nl>  <nl> status = - EIO ; <nl> + goto out ; <nl> } <nl>  <nl> /* If we hit here we were able to read the register and we need to <nl> static int et131x_phy_mii_read ( struct et131x_adapter * adapter , u8 addr , <nl> */ <nl> * value = readl (& mac -> mii_mgmt_stat ) & ET_MAC_MIIMGMT_STAT_PHYCRTL_MASK ; <nl>  <nl> + out : <nl> /* Stop the read operation */ <nl> writel ( 0 , & mac -> mii_mgmt_cmd ); <nl> 
static int fusbh200_hcd_fusbh200_probe ( struct platform_device * pdev ) <nl>  <nl> retval = fusbh200_setup ( hcd ); <nl> if ( retval ) <nl> - return retval ; <nl> + goto fail_add_hcd ; <nl>  <nl> fusbh200_init ( fusbh200 ); <nl> 
qla2x00_probe_one ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> sht = & qla2x00_driver_template ; <nl> if ( pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2422 || <nl> - pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 ) <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP2432 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5422 || <nl> + pdev -> device == PCI_DEVICE_ID_QLOGIC_ISP5432 ) <nl> sht = & qla24xx_driver_template ; <nl> host = scsi_host_alloc ( sht , sizeof ( scsi_qla_host_t )); <nl> if ( host == NULL ) {
static void intel_hdmi_mode_set ( struct intel_encoder * encoder ) <nl> else <nl> hdmi_val |= SDVO_COLOR_FORMAT_8bpc ; <nl>  <nl> - /* Required on CPT */ <nl> - if ( intel_hdmi -> has_hdmi_sink && HAS_PCH_CPT ( dev )) <nl> + if ( intel_hdmi -> has_hdmi_sink && <nl> + ( HAS_PCH_CPT ( dev ) || IS_VALLEYVIEW ( dev ))) <nl> hdmi_val |= HDMI_MODE_SELECT_HDMI ; <nl>  <nl> if ( intel_hdmi -> has_audio ) {
static int t4_sched_queue_bind ( struct port_info * pi , struct ch_sched_queue * p ) <nl>  <nl> /* Unbind queue from any existing class */ <nl> err = t4_sched_queue_unbind ( pi , p ); <nl> - if ( err ) <nl> + if ( err ) { <nl> + t4_free_mem ( qe ); <nl> goto out ; <nl> + } <nl>  <nl> /* Bind queue to specified class */ <nl> memset ( qe , 0 , sizeof (* qe ));
static struct talitos_crypto_alg * talitos_alg_alloc ( struct device * dev , <nl> break ; <nl> default : <nl> dev_err ( dev , " unknown algorithm type % d \ n ", t_alg -> algt . type ); <nl> + kfree ( t_alg ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> 
static int __devinit snd_gusextreme_probe ( struct device * dev , unsigned int n ) <nl> " detected at 0x % lx \ n ", dev -> bus_id , gus -> gf1 . port ); <nl> goto out ; <nl> } <nl> + gus -> codec_flag = 1 ; <nl>  <nl> error = snd_es1688_pcm ( es1688 , 0 , NULL ); <nl> if ( error < 0 )
static int x2apic_acpi_madt_oem_check ( char * oem_id , char * oem_table_id ) <nl> { <nl> if ( x2apic_phys ) <nl> return x2apic_enabled (); <nl> + else if (( acpi_gbl_FADT . header . revision >= FADT2_REVISION_ID ) && <nl> + ( acpi_gbl_FADT . flags & ACPI_FADT_APIC_PHYSICAL ) && <nl> + x2apic_enabled ()) { <nl> + printk ( KERN_DEBUG " System requires x2apic physical mode \ n "); <nl> + return 1 ; <nl> + } <nl> else <nl> return 0 ; <nl> }
ldebugfs_fid_write_common ( const char __user * buffer , size_t count , <nl> rc = sscanf ( kernbuf , "[% llx - % llx ]\ n ", <nl> ( unsigned long long *)& tmp . lsr_start , <nl> ( unsigned long long *)& tmp . lsr_end ); <nl> + if ( rc != 2 ) <nl> + return - EINVAL ; <nl> if (! range_is_sane (& tmp ) || range_is_zero (& tmp ) || <nl> tmp . lsr_start < range -> lsr_start || tmp . lsr_end > range -> lsr_end ) <nl> return - EINVAL ;
ieee80211_sta_process_chanswitch ( struct ieee80211_sub_if_data * sdata , <nl> case - 1 : <nl> cfg80211_chandef_create (& new_chandef , new_chan , <nl> NL80211_CHAN_NO_HT ); <nl> + /* keep width for 5 / 10 MHz channels */ <nl> + switch ( sdata -> vif . bss_conf . chandef . width ) { <nl> + case NL80211_CHAN_WIDTH_5 : <nl> + case NL80211_CHAN_WIDTH_10 : <nl> + new_chandef . width = sdata -> vif . bss_conf . chandef . width ; <nl> + break ; <nl> + default : <nl> + break ; <nl> + } <nl> break ; <nl> } <nl> 
void pcmcia_disable_device ( struct pcmcia_device * p_dev ) { <nl> pcmcia_release_configuration ( p_dev ); <nl> pcmcia_release_io ( p_dev , & p_dev -> io ); <nl> pcmcia_release_irq ( p_dev , & p_dev -> irq ); <nl> - if (& p_dev -> win ) <nl> + if ( p_dev -> win ) <nl> pcmcia_release_window ( p_dev -> win ); <nl> } <nl> EXPORT_SYMBOL ( pcmcia_disable_device );
static int sep_register_driver_with_fs ( struct sep_device * sep ) <nl> if ( ret_val ) { <nl> dev_warn (& sep -> pdev -> dev , " sysfs attribute1 fails for SEP % x \ n ", <nl> ret_val ); <nl> + misc_deregister (& sep -> miscdev_sep ); <nl> return ret_val ; <nl> } <nl> 
static int palmas_i2c_probe ( struct i2c_client * i2c , <nl> ret = - ENOMEM ; <nl> goto err ; <nl> } <nl> + palmas -> i2c_clients [ i ]-> dev . of_node = of_node_get ( node ); <nl> } <nl> palmas -> regmap [ i ] = devm_regmap_init_i2c ( palmas -> i2c_clients [ i ], <nl> & palmas_regmap_config [ i ]);
# include < syslog . h > <nl> # endif <nl>  <nl> -# define S8 int8_t <nl> # define S16 int16_t <nl> # define S32 int32_t <nl> # define S64 int64_t
static bool vgic_update_irq_pending ( struct kvm * kvm , int cpuid , <nl> } else { <nl> vgic_dist_irq_clear_pending ( vcpu , irq_num ); <nl> } <nl> + <nl> + ret = false ; <nl> + goto out ; <nl> } <nl>  <nl> enabled = vgic_irq_is_enabled ( vcpu , irq_num );
static int hdlcdrv_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> case HDLCDRVCTL_CALIBRATE : <nl> if (! capable ( CAP_SYS_RAWIO )) <nl> return - EPERM ; <nl> + if ( bi . data . calibrate > INT_MAX / s -> par . bitrate ) <nl> + return - EINVAL ; <nl> s -> hdlctx . calibrate = bi . data . calibrate * s -> par . bitrate / 16 ; <nl> return 0 ; <nl> 
static int fs_open ( struct atm_vcc * atm_vcc ) <nl> /* Docs are vague about this atm_hdr field . By the way , the FS <nl> * chip makes odd errors if lower bits are set .... -- REW */ <nl> tc -> atm_hdr = ( vpi << 20 ) | ( vci << 4 ); <nl> + tmc0 = 0 ; <nl> { <nl> int pcr = atm_pcr_goal ( txtp ); <nl> 
relookup : <nl> secure_ipv6_id ( daddr -> addr . a6 )); <nl> p -> metrics [ RTAX_LOCK - 1 ] = INETPEER_METRICS_NEW ; <nl> p -> rate_tokens = 0 ; <nl> - p -> rate_last = 0 ; <nl> + /* 60 * HZ is arbitrary , but chosen enough high so that the first <nl> + * calculation of tokens is at its maximum . <nl> + */ <nl> + p -> rate_last = jiffies - 60 * HZ ; <nl> INIT_LIST_HEAD (& p -> gc_list ); <nl>  <nl> /* Link the node . */
static void pci_acpi_cleanup ( struct device * dev ) <nl>  <nl> static bool pci_acpi_bus_match ( struct device * dev ) <nl> { <nl> - return dev -> bus == & pci_bus_type ; <nl> + return dev_is_pci ( dev ); <nl> } <nl>  <nl> static struct acpi_bus_type acpi_pci_bus = {
static int usb_remote_probe ( struct usb_interface * intf , <nl> devnum = dev -> devnum ; <nl> maxp = usb_maxpacket ( dev , pipe , usb_pipeout ( pipe )); <nl>  <nl> - dprintk ( DRIVER_NAME "[% d ]: bytes_in_key =% lu maxp =% d \ n ", <nl> + dprintk ( DRIVER_NAME "[% d ]: bytes_in_key =% zu maxp =% d \ n ", <nl> devnum , CODE_LENGTH , maxp ); <nl>  <nl> 
static void blade_image_blit ( struct tridentfb_par * par , const char * data , <nl> writemmr ( par , DST1 , point ( x , y )); <nl> writemmr ( par , DST2 , point ( x + w - 1 , y + h - 1 )); <nl>  <nl> - memcpy ( par -> io_virt + 0x10000 , data , 4 * size ); <nl> + iowrite32_rep ( par -> io_virt + 0x10000 , data , size ); <nl> } <nl>  <nl> static void blade_copy_rect ( struct tridentfb_par * par ,
* XXX We need to find a better place for these things ... <nl> */ <nl> bool perf_host = true ; <nl> - bool perf_guest = true ; <nl> + bool perf_guest = false ; <nl>  <nl> void event_attr_init ( struct perf_event_attr * attr ) <nl> {
static bool shadow_walk_okay ( struct kvm_shadow_walk_iterator * iterator ) <nl> { <nl> if ( iterator -> level < PT_PAGE_TABLE_LEVEL ) <nl> return false ; <nl> + <nl> + if ( iterator -> level == PT_PAGE_TABLE_LEVEL ) <nl> + if ( is_large_pte (* iterator -> sptep )) <nl> + return false ; <nl> + <nl> iterator -> index = SHADOW_PT_INDEX ( iterator -> addr , iterator -> level ); <nl> iterator -> sptep = (( u64 *) __va ( iterator -> shadow_addr )) + iterator -> index ; <nl> return true ;
static int t7l66xb_probe ( struct platform_device * dev ) <nl> struct resource * iomem , * rscr ; <nl> int ret ; <nl>  <nl> + if ( pdata == NULL ) <nl> + return - EINVAL ; <nl> + <nl> iomem = platform_get_resource ( dev , IORESOURCE_MEM , 0 ); <nl> if (! iomem ) <nl> return - EINVAL ;
static void omap_dma_free_chan_resources ( struct dma_chan * chan ) <nl> vchan_free_chan_resources (& c -> vc ); <nl> omap_free_dma ( c -> dma_ch ); <nl>  <nl> - dev_dbg ( od -> ddev . dev , " freeing channel for % u \ n ", c -> dma_sig ); <nl> + dev_dbg ( od -> ddev . dev , " freeing channel % u used for % u \ n ", c -> dma_ch , <nl> + c -> dma_sig ); <nl> c -> dma_sig = 0 ; <nl> } <nl> 
struct dvb_frontend * tda829x_attach ( struct dvb_frontend * fe , <nl> } <nl>  <nl> if ((!( cfg ) || ( TDA829X_PROBE_TUNER == cfg -> probe_tuner )) && <nl> - ( tda829x_find_tuner ( fe ) < 0 )) <nl> + ( tda829x_find_tuner ( fe ) < 0 )) { <nl> + memset (& fe -> ops . analog_ops , 0 , sizeof ( struct analog_demod_ops )); <nl> + <nl> goto fail ; <nl> + } <nl>  <nl> switch ( priv -> ver ) { <nl> case TDA8290 :
i915_gem_set_tiling ( struct drm_device * dev , void * data , <nl> } <nl>  <nl> mutex_lock (& dev -> struct_mutex ); <nl> - if ( i915_gem_obj_is_pinned ( obj ) || obj -> framebuffer_references ) { <nl> + if ( obj -> pin_display || obj -> framebuffer_references ) { <nl> ret = - EBUSY ; <nl> goto err ; <nl> }
static void v9fs_fd_close ( struct v9fs_transport * trans ) <nl> if (! trans ) <nl> return ; <nl>  <nl> - trans -> status = Disconnected ; <nl> - ts = trans -> priv ; <nl> + ts = xchg (& trans -> priv , NULL ); <nl>  <nl> if (! ts ) <nl> return ; <nl>  <nl> + trans -> status = Disconnected ; <nl> if ( ts -> in_file ) <nl> fput ( ts -> in_file ); <nl> 
_scsih_error_recovery_delete_devices ( struct MPT2SAS_ADAPTER * ioc ) <nl>  <nl> if ( ioc -> is_driver_loading ) <nl> return ; <nl> + <nl> + fw_event = kzalloc ( sizeof ( struct fw_event_work ), GFP_ATOMIC ); <nl> + if (! fw_event ) <nl> + return ; <nl> + <nl> fw_event -> event = MPT2SAS_REMOVE_UNRESPONDING_DEVICES ; <nl> fw_event -> ioc = ioc ; <nl> _scsih_fw_event_add ( ioc , fw_event );
static void vmw_fb_dirty_flush ( struct vmw_fb_par * par ) <nl> SVGAFifoCmdUpdate body ; <nl> } * cmd ; <nl>  <nl> + if ( vmw_priv -> suspended ) <nl> + return ; <nl> + <nl> spin_lock_irqsave (& par -> dirty . lock , flags ); <nl> if (! par -> dirty . active ) { <nl> spin_unlock_irqrestore (& par -> dirty . lock , flags );
# define IRQ_PXA168_DDR_INT 26 <nl> # define IRQ_PXA168_UART1 27 <nl> # define IRQ_PXA168_UART2 28 <nl> +# define IRQ_PXA168_UART3 29 <nl> # define IRQ_PXA168_WDT 35 <nl> +# define IRQ_PXA168_MAIN_PMU 36 <nl> # define IRQ_PXA168_FRQ_CHANGE 38 <nl> # define IRQ_PXA168_SDH1 39 <nl> # define IRQ_PXA168_SDH2 40 <nl> # define IRQ_PXA168_USB2 51 <nl> # define IRQ_PXA168_AC97 57 <nl> # define IRQ_PXA168_TWSI1 58 <nl> -# define IRQ_PXA168_PMU 60 <nl> +# define IRQ_PXA168_AP_PMU 60 <nl> # define IRQ_PXA168_SM_INT 63 <nl>  <nl> /*
int mmap_min_addr_handler ( struct ctl_table * table , int write , <nl> { <nl> int ret ; <nl>  <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> + <nl> ret = proc_doulongvec_minmax ( table , write , buffer , lenp , ppos ); <nl>  <nl> update_mmap_min_addr ();
static ssize_t bonding_show_mii_status ( struct device * d , <nl> char * buf ) <nl> { <nl> struct bonding * bond = to_bond ( d ); <nl> + bool active = !! rcu_access_pointer ( bond -> curr_active_slave ); <nl>  <nl> - return sprintf ( buf , "% s \ n ", bond -> curr_active_slave ? " up " : " down "); <nl> + return sprintf ( buf , "% s \ n ", active ? " up " : " down "); <nl> } <nl> static DEVICE_ATTR ( mii_status , S_IRUGO , bonding_show_mii_status , NULL ); <nl> 
static int exynos_drm_crtc_mode_set_base ( struct drm_crtc * crtc , int x , int y , <nl>  <nl> DRM_DEBUG_KMS ("% s \ n ", __FILE__ ); <nl>  <nl> + /* when framebuffer changing is requested , crtc ' s dpms should be on */ <nl> + if ( exynos_crtc -> dpms > DRM_MODE_DPMS_ON ) { <nl> + DRM_ERROR (" failed framebuffer changing request .\ n "); <nl> + return - EPERM ; <nl> + } <nl> + <nl> crtc_w = crtc -> fb -> width - x ; <nl> crtc_h = crtc -> fb -> height - y ; <nl> 
static int dmx_section_feed_release_filter ( struct dmx_section_feed * feed , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( feed -> is_filtering ) <nl> + if ( feed -> is_filtering ) { <nl> + /* release dvbdmx -> mutex as far as <nl> + it is acquired by stop_filtering () itself */ <nl> + mutex_unlock (& dvbdmx -> mutex ); <nl> feed -> stop_filtering ( feed ); <nl> + mutex_lock (& dvbdmx -> mutex ); <nl> + } <nl>  <nl> spin_lock_irq (& dvbdmx -> lock ); <nl> f = dvbdmxfeed -> filter ;
static void ext4_free_data ( handle_t * handle , struct inode * inode , <nl> * block pointed to itself , it would have been detached when <nl> * the block was cleared . Check for this instead of OOPSing . <nl> */ <nl> - if ( bh2jh ( this_bh )) <nl> + if (( EXT4_JOURNAL ( inode ) == NULL ) || bh2jh ( this_bh )) <nl> ext4_handle_dirty_metadata ( handle , inode , this_bh ); <nl> else <nl> ext4_error ( inode -> i_sb , __func__ ,
static void ironlake_enable_pch_transcoder ( struct drm_i915_private * dev_priv , <nl> val |= TRANS_PROGRESSIVE ; <nl>  <nl> I915_WRITE ( reg , val | TRANS_ENABLE ); <nl> - if ( wait_for ( I915_READ ( reg ) & TRANS_STATE_ENABLE , 100 )) <nl> + if ( intel_wait_for_register ( dev_priv , <nl> + reg , TRANS_STATE_ENABLE , TRANS_STATE_ENABLE , <nl> + 100 )) <nl> DRM_ERROR (" failed to enable transcoder % c \ n ", pipe_name ( pipe )); <nl> } <nl> 
static struct inotify_watch * create_watch ( struct inotify_device * dev , <nl> return ERR_PTR ( ret ); <nl> } <nl>  <nl> - dev -> last_wd = ret ; <nl> + dev -> last_wd = watch -> wd ; <nl> watch -> mask = mask ; <nl> atomic_set (& watch -> count , 0 ); <nl> INIT_LIST_HEAD (& watch -> d_list );
intel_dp_check_link_status ( struct intel_dp * intel_dp ) <nl> if (! to_intel_crtc ( intel_encoder -> base . crtc )-> active ) <nl> return ; <nl>  <nl> + /* FIXME : we need to synchronize this sort of stuff with hardware <nl> + * readout */ <nl> + if ( WARN_ON_ONCE (! intel_dp -> lane_count )) <nl> + return ; <nl> + <nl> /* if link training is requested we should perform it always */ <nl> if (( intel_dp -> compliance_test_type == DP_TEST_LINK_TRAINING ) || <nl> (! drm_dp_channel_eq_ok ( link_status , intel_dp -> lane_count ))) {
static void clk_pllv2_unprepare ( struct clk_hw * hw ) <nl> __raw_writel ( reg , pllbase + MXC_PLL_DP_CTL ); <nl> } <nl>  <nl> - struct clk_ops clk_pllv2_ops = { <nl> + static struct clk_ops clk_pllv2_ops = { <nl> . prepare = clk_pllv2_prepare , <nl> . unprepare = clk_pllv2_unprepare , <nl> . recalc_rate = clk_pllv2_recalc_rate ,
void <nl> trace_printk_seq ( struct trace_seq * s ) <nl> { <nl> /* Probably should print a warning here . */ <nl> - if ( s -> len >= 1000 ) <nl> - s -> len = 1000 ; <nl> + if ( s -> len >= TRACE_MAX_PRINT ) <nl> + s -> len = TRACE_MAX_PRINT ; <nl>  <nl> /* should be zero ended , but we are paranoid . */ <nl> s -> buffer [ s -> len ] = 0 ;
static struct iwl_power_vec_entry range_2 [ IWL_POWER_MAX ] = { <nl> /* set card power command */ <nl> static int iwl_set_power ( struct iwl_priv * priv , void * cmd ) <nl> { <nl> - return iwl_send_cmd_pdu_async ( priv , POWER_TABLE_CMD , <nl> - sizeof ( struct iwl_powertable_cmd ), <nl> - cmd , NULL ); <nl> + return iwl_send_cmd_pdu ( priv , POWER_TABLE_CMD , <nl> + sizeof ( struct iwl_powertable_cmd ), cmd ); <nl> } <nl> /* decide the right power level according to association status <nl> * and battery status
int rsnd_dai_connect ( struct rsnd_mod * mod , <nl> if (! mod ) <nl> return - EIO ; <nl>  <nl> + if ( io -> mod [ type ]) <nl> + return - EINVAL ; <nl> + <nl> priv = rsnd_mod_to_priv ( mod ); <nl> dev = rsnd_priv_to_dev ( priv ); <nl> 
static inline int phy_set_mode ( struct phy * phy , enum phy_mode mode ) <nl> return - ENOSYS ; <nl> } <nl>  <nl> + static inline int phy_reset ( struct phy * phy ) <nl> +{ <nl> + if (! phy ) <nl> + return 0 ; <nl> + return - ENOSYS ; <nl> +} <nl> + <nl> static inline int phy_get_bus_width ( struct phy * phy ) <nl> { <nl> return - ENOSYS ;
int btrfs_reserve_extent ( struct btrfs_root * root , <nl> u64 empty_size , u64 hint_byte , <nl> struct btrfs_key * ins , int is_data , int delalloc ) <nl> { <nl> - bool final_tried = false ; <nl> + bool final_tried = num_bytes == min_alloc_size ; <nl> u64 flags ; <nl> int ret ; <nl> 
unmap_intr_base : <nl> iounmap ( priv -> avs_intr_base ); <nl> unmap_base : <nl> iounmap ( priv -> base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return ret ; <nl> } <nl> static int brcm_avs_cpufreq_remove ( struct platform_device * pdev ) <nl> priv = platform_get_drvdata ( pdev ); <nl> iounmap ( priv -> base ); <nl> iounmap ( priv -> avs_intr_base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return 0 ; <nl> }
static void rt6_free_pcpu ( struct rt6_info * non_pcpu_rt ) <nl> } <nl> } <nl>  <nl> + free_percpu ( non_pcpu_rt -> rt6i_pcpu ); <nl> non_pcpu_rt -> rt6i_pcpu = NULL ; <nl> } <nl> 
static int __devinit fealnx_init_one ( struct pci_dev * pdev , <nl> if ( np -> flags == HAS_MII_XCVR ) { <nl> int phy , phy_idx = 0 ; <nl>  <nl> - for ( phy = 1 ; phy < 32 && phy_idx < 4 ; phy ++) { <nl> + for ( phy = 1 ; phy < 32 && phy_idx < ARRAY_SIZE ( np -> phys ); <nl> + phy ++) { <nl> int mii_status = mdio_read ( dev , phy , 1 ); <nl>  <nl> if ( mii_status != 0xffff && mii_status != 0x0000 ) {
int mac802154_llsec_key_del ( struct mac802154_llsec * sec , <nl> mkey = container_of ( pos -> key , struct mac802154_llsec_key , key ); <nl>  <nl> if ( llsec_key_id_equal (& pos -> id , key )) { <nl> + list_del_rcu (& pos -> list ); <nl> llsec_key_put ( mkey ); <nl> return 0 ; <nl> }
static unsigned int get_max_cost ( struct f2fs_sb_info * sbi , <nl> if ( p -> alloc_mode == SSR ) <nl> return sbi -> blocks_per_seg ; <nl> if ( p -> gc_mode == GC_GREEDY ) <nl> - return sbi -> blocks_per_seg * p -> ofs_unit ; <nl> + return 2 * sbi -> blocks_per_seg * p -> ofs_unit ; <nl> else if ( p -> gc_mode == GC_CB ) <nl> return UINT_MAX ; <nl> else /* No other gc_mode */
static int emi26_load_firmware ( struct usb_device * dev ) <nl>  <nl> /* De - assert reset ( let the CPU run ) */ <nl> err = emi26_set_reset ( dev , 0 ); <nl> + if ( err < 0 ) { <nl> + err ("% s - error loading firmware : error = % d ", __FUNCTION__ , err ); <nl> + goto wraperr ; <nl> + } <nl> msleep ( 250 ); /* let device settle */ <nl>  <nl> /* 2 . We upload the FPGA firmware into the EMI
static int send_reply ( struct svcxprt_rdma * rdma , <nl> " svcrdma : could not post a receive buffer , err =% d ." <nl> " Closing transport % p .\ n ", ret , rdma ); <nl> set_bit ( XPT_CLOSE , & rdma -> sc_xprt . xpt_flags ); <nl> - return 0 ; <nl> + svc_rdma_put_context ( ctxt , 0 ); <nl> + return - ENOTCONN ; <nl> } <nl>  <nl> /* Prepare the context */
static int TSS_authhmac ( unsigned char * digest , const unsigned char * key , <nl> if ( dlen == 0 ) <nl> break ; <nl> data = va_arg ( argp , unsigned char *); <nl> + if (! data ) { <nl> + ret = - EINVAL ; <nl> + va_end ( argp ); <nl> + goto out ; <nl> + } <nl> ret = crypto_shash_update (& sdesc -> shash , data , dlen ); <nl> if ( ret < 0 ) { <nl> va_end ( argp );
int free_stream_context ( struct intel_sst_drv * ctx , unsigned int str_id ) <nl> if ( ret ) <nl> sst_clean_stream (& ctx -> streams [ str_id ]); <nl> return ret ; <nl> + } else { <nl> + dev_err ( ctx -> dev , " we tried to free stream context % d which was freed !!!\ n ", str_id ); <nl> } <nl> return ret ; <nl> }
static int __inode_security_revalidate ( struct inode * inode , <nl>  <nl> might_sleep_if ( may_sleep ); <nl>  <nl> - if ( isec -> initialized != LABEL_INITIALIZED ) { <nl> + if ( ss_initialized && isec -> initialized != LABEL_INITIALIZED ) { <nl> if (! may_sleep ) <nl> return - ECHILD ; <nl> 
pte_t * huge_pte_offset ( struct mm_struct * mm , unsigned long addr ) <nl> pmd_t * pmd = NULL ; <nl>  <nl> pgd = pgd_offset ( mm , addr ); <nl> - pud = pud_offset ( pgd , addr ); <nl> - pmd = pmd_offset ( pud , addr ); <nl> + if ( pgd_present (* pgd )) { <nl> + pud = pud_offset ( pgd , addr ); <nl> + if ( pud_present (* pud )) <nl> + pmd = pmd_offset ( pud , addr ); <nl> + } <nl> return ( pte_t *) pmd ; <nl> } <nl> 
irq_handler ( int irq , void * device ) <nl>  <nl> pci_int_status = reg_read ( lynx , PCI_INT_STATUS ); <nl>  <nl> + if ( pci_int_status == ~ 0 ) <nl> + /* Card was ejected . */ <nl> + return IRQ_NONE ; <nl> + <nl> if (( pci_int_status & PCI_INT_INT_PEND ) == 0 ) <nl> /* Not our interrupt , bail out quickly . */ <nl> return IRQ_NONE ;
struct bcm2835_audio_instance { <nl> short peer_version ; <nl> }; <nl>  <nl> - bool force_bulk = false ; <nl> + static bool force_bulk ; <nl>  <nl> /* ---- Private Variables ---------------------------------------------------- */ <nl> 
struct sk_buff * __skb_recv_datagram ( struct sock * sk , unsigned int flags , <nl> skb_queue_walk ( queue , skb ) { <nl> * peeked = skb -> peeked ; <nl> if ( flags & MSG_PEEK ) { <nl> - if (* off >= skb -> len && skb -> len ) { <nl> + if (* off >= skb -> len && ( skb -> len || * off || <nl> + skb -> peeked )) { <nl> * off -= skb -> len ; <nl> continue ; <nl> }
static void gb_tty_set_termios ( struct tty_struct * tty , <nl>  <nl> if ( C_BAUD ( tty ) == B0 ) { <nl> newline . rate = gb_tty -> line_coding . rate ; <nl> - newctrl &= GB_UART_CTRL_DTR ; <nl> + newctrl &= ~ GB_UART_CTRL_DTR ; <nl> } else if ( termios_old && ( termios_old -> c_cflag & CBAUD ) == B0 ) { <nl> newctrl |= GB_UART_CTRL_DTR ; <nl> }
int mlx5_core_access_reg ( struct mlx5_core_dev * dev , void * data_in , <nl> in -> arg = cpu_to_be32 ( arg ); <nl> in -> register_id = cpu_to_be16 ( reg_num ); <nl> err = mlx5_cmd_exec ( dev , in , sizeof (* in ) + size_in , out , <nl> - sizeof ( out ) + size_out ); <nl> + sizeof (* out ) + size_out ); <nl> if ( err ) <nl> goto ex2 ; <nl> 
do_kern_mount ( const char * fstype , int flags , const char * name , void * data ) <nl> mnt -> mnt_parent = mnt ; <nl> mnt -> mnt_namespace = current -> namespace ; <nl> up_write (& sb -> s_umount ); <nl> + free_secdata ( secdata ); <nl> put_filesystem ( type ); <nl> return mnt ; <nl> out_sb :
radeon_user_framebuffer_create ( struct drm_device * dev , <nl> return ERR_PTR (- ENOENT ); <nl> } <nl>  <nl> + /* Handle is imported dma - buf , so cannot be migrated to VRAM for scanout */ <nl> + if ( obj -> import_attach ) { <nl> + DRM_DEBUG_KMS (" Cannot create framebuffer from imported dma_buf \ n "); <nl> + return ERR_PTR (- EINVAL ); <nl> + } <nl> + <nl> radeon_fb = kzalloc ( sizeof (* radeon_fb ), GFP_KERNEL ); <nl> if ( radeon_fb == NULL ) { <nl> drm_gem_object_unreference_unlocked ( obj );
int ip6_mr_input ( struct sk_buff * skb ) <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> cache = ip6mr_cache_find ( mrt ,
imsttfb_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> default : <nl> printk ( KERN_INFO " imsttfb : Device 0x % x unknown , " <nl> " contact maintainer .\ n ", pdev -> device ); <nl> + release_mem_region ( addr , size ); <nl> + framebuffer_release ( info ); <nl> return - ENODEV ; <nl> } <nl> 
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . fuse = gm107_fuse_new , <nl> . gpio = gk104_gpio_new , <nl> + . i2c = gm200_i2c_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> . mmu = gf100_mmu_new ,
static void ui_browser__hists_seek ( struct ui_browser * browser , <nl> * and stop when we printed enough lines to fill the screen . <nl> */ <nl> do_offset : <nl> + if (! nd ) <nl> + return ; <nl> + <nl> if ( offset > 0 ) { <nl> do { <nl> h = rb_entry ( nd , struct hist_entry , rb_node );
__cmpxchg_u32 ( volatile unsigned int * p , unsigned int old , unsigned int new ) <nl> " bra 2f ; \ n " <nl> " . fillinsn \ n " <nl> " 1 :" <nl> - M32R_UNLOCK " % 2 , @% 1 ; \ n " <nl> + M32R_UNLOCK " % 0 , @% 1 ; \ n " <nl> " . fillinsn \ n " <nl> " 2 :" <nl> : "=& r " ( retval )
void __init smp_prepare_boot_cpu ( void ) <nl>  <nl> static void send_ipi_message ( const struct cpumask * mask , enum ipi_msg_type msg ) <nl> { <nl> - unsigned long flags ; <nl> - <nl> - local_irq_save ( flags ); <nl> - <nl> /* <nl> * Call the platform specific cross - CPU call function . <nl> */ <nl> smp_cross_call ( mask , msg ); <nl> - <nl> - local_irq_restore ( flags ); <nl> } <nl>  <nl> void arch_send_call_function_ipi_mask ( const struct cpumask * mask )
static struct collection collections [] = { <nl>  <nl> /* Iterate over all benchmarks within a collection : */ <nl> # define for_each_bench ( coll , bench ) \ <nl> - for ( bench = coll -> benchmarks ; bench -> name ; bench ++) <nl> + for ( bench = coll -> benchmarks ; bench && bench -> name ; bench ++) <nl>  <nl> static void dump_benchmarks ( struct collection * coll ) <nl> {
static int si_dpm_init_microcode ( struct amdgpu_device * adev ) <nl> ( adev -> pdev -> revision == 0x80 ) || <nl> ( adev -> pdev -> revision == 0x81 ) || <nl> ( adev -> pdev -> revision == 0x83 ) || <nl> + ( adev -> pdev -> revision == 0x87 ) || <nl> ( adev -> pdev -> device == 0x6604 ) || <nl> ( adev -> pdev -> device == 0x6605 )) <nl> chip_name = " oland_k ";
s32 igb_get_phy_id ( struct e1000_hw * hw ) <nl> u16 phy_id ; <nl>  <nl> /* ensure PHY page selection to fix misconfigured i210 */ <nl> - if ( hw -> mac . type == e1000_i210 ) <nl> + if (( hw -> mac . type == e1000_i210 ) || ( hw -> mac . type == e1000_i211 )) <nl> phy -> ops . write_reg ( hw , I347AT4_PAGE_SELECT , 0 ); <nl>  <nl> ret_val = phy -> ops . read_reg ( hw , PHY_ID1 , & phy_id );
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> if (! dev -> suspend_in_progress ) <nl> b43_rng_init ( wl ); <nl>  <nl> + ieee80211_wake_queues ( dev -> wl -> hw ); <nl> + <nl> b43_set_status ( dev , B43_STAT_INITIALIZED ); <nl>  <nl> if (! dev -> suspend_in_progress )
int fcoe_ctlr_els_send ( struct fcoe_ctlr * fip , struct fc_lport * lport , <nl>  <nl> if ( fip -> state == FIP_ST_NON_FIP ) <nl> return 0 ; <nl> + if (! fip -> sel_fcf ) <nl> + goto drop ; <nl>  <nl> switch ( op ) { <nl> case ELS_FLOGI :
static void sppp_lcp_input ( struct sppp * sp , struct sk_buff * skb ) <nl> struct net_device * dev = sp -> pp_if ; <nl> int len = skb -> len ; <nl> u8 * p , opt [ 6 ]; <nl> - u32 rmagic ; <nl> + u32 rmagic = 0 ; <nl>  <nl> if (! pskb_may_pull ( skb , sizeof ( struct lcp_header ))) { <nl> if ( sp -> pp_flags & PP_DEBUG )
static int qxl_palette_create_1bit ( struct qxl_bo * palette_bo , <nl> * correctly globaly , since that would require <nl> * tracking all of our palettes . */ <nl> ret = qxl_bo_kmap ( palette_bo , ( void **)& pal ); <nl> + if ( ret ) <nl> + return ret ; <nl> pal -> num_ents = 2 ; <nl> pal -> unique = unique ++; <nl> if ( visual == FB_VISUAL_TRUECOLOR || visual == FB_VISUAL_DIRECTCOLOR ) {
static void tcp_init_metrics ( struct sock * sk ) <nl> } <nl> if ( dst_metric ( dst , RTAX_RTTVAR ) > tp -> mdev ) { <nl> tp -> mdev = dst_metric ( dst , RTAX_RTTVAR ); <nl> - tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , TCP_RTO_MIN ); <nl> + tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , tcp_rto_min ( sk )); <nl> } <nl> tcp_set_rto ( sk ); <nl> tcp_bound_rto ( sk );
void usb_buffer_unmap_sg ( struct usb_device * dev , unsigned pipe , <nl>  <nl> static int verify_suspended ( struct device * dev , void * unused ) <nl> { <nl> + if ( dev -> driver == NULL ) <nl> + return 0 ; <nl> return ( dev -> power . power_state . event == PM_EVENT_ON ) ? - EBUSY : 0 ; <nl> } <nl> 
deinit : <nl> wl1271_free_ap_keys ( wl , wlvif ); <nl> } <nl>  <nl> + dev_kfree_skb ( wlvif -> probereq ); <nl> + wlvif -> probereq = NULL ; <nl> wl12xx_tx_reset_wlvif ( wl , wlvif ); <nl> if ( wl -> last_wlvif == wlvif ) <nl> wl -> last_wlvif = NULL ;
static int pwm_setup_backlight ( struct intel_connector * connector , <nl> return - ENODEV ; <nl> } <nl>  <nl> + /* <nl> + * FIXME : pwm_apply_args () should be removed when switching to <nl> + * the atomic PWM API . <nl> + */ <nl> + pwm_apply_args ( panel -> backlight . pwm ); <nl> + <nl> retval = pwm_config ( panel -> backlight . pwm , CRC_PMIC_PWM_PERIOD_NS , <nl> CRC_PMIC_PWM_PERIOD_NS ); <nl> if ( retval < 0 ) {
static void sh_msiof_spi_chipselect ( struct spi_device * spi , int is_on ) <nl> } <nl>  <nl> /* use spi -> controller data for CS ( same strategy as spi_gpio ) */ <nl> - gpio_set_value (( unsigned ) spi -> controller_data , value ); <nl> + gpio_set_value (( uintptr_t ) spi -> controller_data , value ); <nl>  <nl> if ( is_on == BITBANG_CS_INACTIVE ) { <nl> if ( test_and_clear_bit ( 0 , & p -> flags )) {
static int shmem_unuse_inode ( struct shmem_inode_info * info , swp_entry_t entry , s <nl> if ( size > ENTRIES_PER_PAGE ) <nl> size = ENTRIES_PER_PAGE ; <nl> offset = shmem_find_swp ( entry , ptr , ptr + size ); <nl> + shmem_swp_unmap ( ptr ); <nl> if ( offset >= 0 ) { <nl> shmem_dir_unmap ( dir ); <nl> + ptr = shmem_swp_map ( subdir ); <nl> goto found ; <nl> } <nl> - shmem_swp_unmap ( ptr ); <nl> } <nl> } <nl> lost1 :
static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
static int ath9k_add_interface ( struct ieee80211_hw * hw , <nl> } <nl> } <nl>  <nl> - if (( vif -> type == NL80211_IFTYPE_ADHOC ) && <nl> - sc -> nvifs > 0 ) { <nl> + if (( ah -> opmode == NL80211_IFTYPE_ADHOC ) || <nl> + (( vif -> type == NL80211_IFTYPE_ADHOC ) && <nl> + sc -> nvifs > 0 )) { <nl> ath_err ( common , " Cannot create ADHOC interface when other " <nl> " interfaces already exist .\ n "); <nl> ret = - EINVAL ;
int perf_session__cpu_bitmap ( struct perf_session * session , <nl> } <nl>  <nl> map = cpu_map__new ( cpu_list ); <nl> + if ( map == NULL ) { <nl> + pr_err (" Invalid cpu_list \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> for ( i = 0 ; i < map -> nr ; i ++) { <nl> int cpu = map -> map [ i ];
unsigned int create_irq_nr ( unsigned int irq_want , int node ) <nl> continue ; <nl>  <nl> desc_new = move_irq_desc ( desc_new , node ); <nl> + cfg_new = desc_new -> chip_data ; <nl>  <nl> if ( __assign_irq_vector ( new , cfg_new , apic -> target_cpus ()) == 0 ) <nl> irq = new ;
static int selinux_setprocattr ( struct task_struct * p , <nl> return error ; <nl>  <nl> /* Obtain a SID for the context , if one was specified . */ <nl> - if ( size && str [ 1 ] && str [ 1 ] != '\ n ') { <nl> + if ( size && str [ 0 ] && str [ 0 ] != '\ n ') { <nl> if ( str [ size - 1 ] == '\ n ') { <nl> str [ size - 1 ] = 0 ; <nl> size --;
static __be32 encode_cb_sequence_res ( struct svc_rqst * rqstp , <nl> if ( unlikely ( status != 0 )) <nl> goto out ; <nl>  <nl> - encode_sessionid ( xdr , & res -> csr_sessionid ); <nl> + status = encode_sessionid ( xdr , & res -> csr_sessionid ); <nl> + if ( status ) <nl> + goto out ; <nl>  <nl> p = xdr_reserve_space ( xdr , 4 * sizeof ( uint32_t )); <nl> if ( unlikely ( p == NULL ))
static unsigned long ramfs_nommu_get_unmapped_area ( struct file * file , <nl>  <nl> /* gang - find the pages */ <nl> ret = - ENOMEM ; <nl> - pages = kzalloc ( lpages * sizeof ( struct page *), GFP_KERNEL ); <nl> + pages = kcalloc ( lpages , sizeof ( struct page *), GFP_KERNEL ); <nl> if (! pages ) <nl> goto out_free ; <nl> 
int call_usermodehelper_exec ( struct subprocess_info * sub_info , int wait ) <nl> DECLARE_COMPLETION_ONSTACK ( done ); <nl> int retval = 0 ; <nl>  <nl> + if (! sub_info -> path ) { <nl> + call_usermodehelper_freeinfo ( sub_info ); <nl> + return - EINVAL ; <nl> + } <nl> helper_lock (); <nl> if (! khelper_wq || usermodehelper_disabled ) { <nl> retval = - EBUSY ;
static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> } <nl> logical = re -> logical ; <nl>  <nl> - reada_extent_put ( fs_info , re ); <nl> - <nl> atomic_inc (& dev -> reada_in_flight ); <nl> ret = reada_tree_block_flagged ( fs_info -> extent_root , logical , <nl> mirror_num , & eb ); <nl> static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> if ( eb ) <nl> free_extent_buffer ( eb ); <nl>  <nl> + reada_extent_put ( fs_info , re ); <nl> + <nl> return 1 ; <nl>  <nl> }
static int start_ap ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> priv = wiphy_priv ( wiphy ); <nl> vif = netdev_priv ( dev ); <nl> - wl = vif -> wilc ; <nl> + wl = vif -> wilc ; <nl> PRINT_D ( HOSTAPD_DBG , " Starting ap \ n "); <nl>  <nl> PRINT_D ( HOSTAPD_DBG , " Interval = % d \ n DTIM period = % d \ n Head length = % zu Tail length = % zu \ n ",
static int spi_imx_setupxfer ( struct spi_device * spi , <nl> config . bpw = t ? t -> bits_per_word : spi -> bits_per_word ; <nl> config . speed_hz = t ? t -> speed_hz : spi -> max_speed_hz ; <nl> config . mode = spi -> mode ; <nl> + config . cs = spi_imx -> chipselect [ spi -> chip_select ]; <nl>  <nl> if (! config . speed_hz ) <nl> config . speed_hz = spi -> max_speed_hz ;
static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
static irqreturn_t sdhci_irq ( int irq , void * dev_id ) <nl>  <nl> intmask = readl ( host -> ioaddr + SDHCI_INT_STATUS ); <nl>  <nl> - if (! intmask ) { <nl> + if (! intmask || intmask == 0xffffffff ) { <nl> result = IRQ_NONE ; <nl> goto out ; <nl> }
int del_mtd_blktrans_dev ( struct mtd_blktrans_dev * old ) <nl> BUG (); <nl> } <nl>  <nl> - /* Stop new requests to arrive */ <nl> - del_gendisk ( old -> disk ); <nl> - <nl> if ( old -> disk_attributes ) <nl> sysfs_remove_group (& disk_to_dev ( old -> disk )-> kobj , <nl> old -> disk_attributes ); <nl>  <nl> + /* Stop new requests to arrive */ <nl> + del_gendisk ( old -> disk ); <nl> + <nl> + <nl> /* Stop the thread */ <nl> kthread_stop ( old -> thread ); <nl> 
void dlm_lowcomms_stop ( void ) <nl> con = __nodeid2con ( i , 0 ); <nl> if ( con ) { <nl> close_connection ( con , true ); <nl> + if ( con -> othercon ) <nl> + kmem_cache_free ( con_cache , con -> othercon ); <nl> kmem_cache_free ( con_cache , con ); <nl> } <nl> }
tracing_buffers_splice_read ( struct file * file , loff_t * ppos , <nl> if (! ref ) <nl> break ; <nl>  <nl> + ref -> ref = 1 ; <nl> ref -> buffer = info -> tr -> buffer ; <nl> ref -> page = ring_buffer_alloc_read_page ( ref -> buffer ); <nl> if (! ref -> page ) {
static int __devinit mei_probe ( struct pci_dev * pdev , <nl> err = request_threaded_irq ( pdev -> irq , <nl> NULL , <nl> mei_interrupt_thread_handler , <nl> - 0 , mei_driver_name , dev ); <nl> + IRQF_ONESHOT , mei_driver_name , dev ); <nl> else <nl> err = request_threaded_irq ( pdev -> irq , <nl> mei_interrupt_quick_handler ,
static inline struct kmem_cache * cache_from_obj ( struct kmem_cache * s , void * x ) <nl> return cachep ; <nl>  <nl> pr_err ("% s : Wrong slab cache . % s but object is from % s \ n ", <nl> - __FUNCTION__ , cachep -> name , s -> name ); <nl> + __func__ , cachep -> name , s -> name ); <nl> WARN_ON_ONCE ( 1 ); <nl> return s ; <nl> }
static void get_total_mem ( struct mv64x60_mc_pdata * pdata ) <nl> if (! np ) <nl> return ; <nl>  <nl> - reg = get_property ( np , " reg ", NULL ); <nl> + reg = of_get_property ( np , " reg ", NULL ); <nl>  <nl> pdata -> total_mem = reg [ 1 ]; <nl> }
static int threshold_create_bank ( unsigned int cpu , unsigned int bank ) <nl> const char * name = get_name ( bank , NULL ); <nl> int err = 0 ; <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( is_shared_bank ( bank )) { <nl> nb = node_to_amd_nb ( amd_get_nb_id ( cpu )); <nl> 
int dwc3_send_gadget_generic_command ( struct dwc3 * dwc , unsigned cmd , u32 param ) <nl> dwc3_trace ( trace_dwc3_gadget , <nl> " Command Complete --> % d ", <nl> DWC3_DGCMD_STATUS ( reg )); <nl> + if ( DWC3_DGCMD_STATUS ( reg )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static int rohm_ts_load_firmware ( struct i2c_client * client , <nl> break ; <nl>  <nl> error = - EIO ; <nl> - } while (++ retry >= FIRMWARE_RETRY_MAX ); <nl> + } while (++ retry <= FIRMWARE_RETRY_MAX ); <nl>  <nl> out : <nl> error2 = i2c_smbus_write_byte_data ( client , INT_MASK , INT_ALL );
void bpf_jit_compile ( struct bpf_prog * fp ) <nl>  <nl> memset (& ctx , 0 , sizeof ( ctx )); <nl>  <nl> - ctx . offsets = kcalloc ( fp -> len , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> + ctx . offsets = kcalloc ( fp -> len + 1 , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> if ( ctx . offsets == NULL ) <nl> return ; <nl> 
int macvlan_common_newlink ( struct net * src_net , struct net_device * dev , <nl>  <nl> list_add_tail_rcu (& vlan -> list , & port -> vlans ); <nl> netif_stacked_transfer_operstate ( lowerdev , dev ); <nl> + linkwatch_fire_event ( dev ); <nl>  <nl> return 0 ; <nl>  <nl> static int macvlan_device_event ( struct notifier_block * unused , <nl> port = macvlan_port_get_rtnl ( dev ); <nl>  <nl> switch ( event ) { <nl> + case NETDEV_UP : <nl> case NETDEV_CHANGE : <nl> list_for_each_entry ( vlan , & port -> vlans , list ) <nl> netif_stacked_transfer_operstate ( vlan -> lowerdev ,
static int __devinit snd_ca0106_probe ( struct pci_dev * pci , <nl> snd_ca0106_proc_init ( chip ); <nl> # endif <nl>  <nl> + snd_card_set_dev ( card , & pci -> dev ); <nl> + <nl> if (( err = snd_card_register ( card )) < 0 ) { <nl> snd_card_free ( card ); <nl> return err ;
static ssize_t rbd_add ( struct bus_type * bus , <nl> if (! try_module_get ( THIS_MODULE )) <nl> return - ENODEV ; <nl>  <nl> - mon_dev_name = kmalloc ( RBD_MAX_OPT_LEN , GFP_KERNEL ); <nl> + mon_dev_name = kmalloc ( count , GFP_KERNEL ); <nl> if (! mon_dev_name ) <nl> goto err_out_mod ; <nl>  <nl> - options = kmalloc ( RBD_MAX_OPT_LEN , GFP_KERNEL ); <nl> + options = kmalloc ( count , GFP_KERNEL ); <nl> if (! options ) <nl> goto err_mon_dev ; <nl> 
static int __init b44_init ( void ) <nl>  <nl> /* Setup paramaters for syncing RX / TX DMA descriptors */ <nl> dma_desc_align_mask = ~( dma_desc_align_size - 1 ); <nl> - dma_desc_sync_size = max ( dma_desc_align_size , sizeof ( struct dma_desc )); <nl> + dma_desc_sync_size = max_t ( unsigned int , dma_desc_align_size , sizeof ( struct dma_desc )); <nl>  <nl> return pci_module_init (& b44_driver ); <nl> }
static int p54u_probe ( struct usb_interface * intf , <nl> priv -> upload_fw = p54u_upload_firmware_net2280 ; <nl> } <nl> err = p54u_load_firmware ( dev , intf ); <nl> + if ( err ) { <nl> + usb_put_dev ( udev ); <nl> + p54_free_common ( dev ); <nl> + } <nl> return err ; <nl> } <nl> 
static int omap_hdq_remove ( struct platform_device * pdev ) <nl>  <nl> if ( hdq_data -> hdq_usecount ) { <nl> dev_dbg (& pdev -> dev , " removed when use count is not zero \ n "); <nl> + mutex_unlock (& hdq_data -> hdq_mutex ); <nl> return - EBUSY ; <nl> } <nl> 
struct dnode_of_data { <nl> static inline void set_new_dnode ( struct dnode_of_data * dn , struct inode * inode , <nl> struct page * ipage , struct page * npage , nid_t nid ) <nl> { <nl> + memset ( dn , 0 , sizeof (* dn )); <nl> dn -> inode = inode ; <nl> dn -> inode_page = ipage ; <nl> dn -> node_page = npage ; <nl> dn -> nid = nid ; <nl> - dn -> inode_page_locked = 0 ; <nl> } <nl>  <nl> /*
static void do_checkpoint ( struct f2fs_sb_info * sbi , bool is_umount ) <nl> /* Here , we only have one bio having CP pack */ <nl> sync_meta_pages ( sbi , META_FLUSH , LONG_MAX ); <nl>  <nl> - if ( unlikely (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG ))) { <nl> + if (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG )) { <nl> clear_prefree_segments ( sbi ); <nl> release_dirty_inode ( sbi ); <nl> F2FS_RESET_SB_DIRT ( sbi );
snd_pmac_burgundy_busy_wait ( struct snd_pmac * chip ) <nl> int timeout = 50 ; <nl> while (( in_le32 (& chip -> awacs -> codec_ctrl ) & MASK_NEWECMD ) && timeout --) <nl> udelay ( 1 ); <nl> - if (! timeout ) <nl> + if ( timeout < 0 ) <nl> printk ( KERN_DEBUG " burgundy_busy_wait : timeout \ n "); <nl> } <nl> 
static void tpacket_destruct_skb ( struct sk_buff * skb ) <nl> struct packet_sock * po = pkt_sk ( skb -> sk ); <nl> void * ph ; <nl>  <nl> - BUG_ON ( skb == NULL ); <nl> - <nl> if ( likely ( po -> tx_ring . pg_vec )) { <nl> ph = skb_shinfo ( skb )-> destructor_arg ; <nl> BUG_ON ( __packet_get_status ( po , ph ) != TP_STATUS_SENDING );
void __init exynos5_init_irq ( void ) <nl> * Theses parameters should be NULL and 0 because EXYNOS4 <nl> * uses GIC instead of VIC . <nl> */ <nl> - s5p_init_irq ( NULL , 0 ); <nl> + if (! of_machine_is_compatible (" samsung , exynos5440 ")) <nl> + s5p_init_irq ( NULL , 0 ); <nl>  <nl> gic_arch_extn . irq_set_wake = s3c_irq_wake ; <nl> }
void destroy_preds ( struct ftrace_event_call * call ) <nl> filter_free_pred ( filter -> preds [ i ]); <nl> } <nl> kfree ( filter -> preds ); <nl> + kfree ( filter -> filter_string ); <nl> kfree ( filter ); <nl> call -> filter = NULL ; <nl> }
static const struct of_device_id bcm_kona_i2c_of_match [] = { <nl> {. compatible = " brcm , kona - i2c ",}, <nl> {}, <nl> }; <nl> - MODULE_DEVICE_TABLE ( of , kona_i2c_of_match ); <nl> + MODULE_DEVICE_TABLE ( of , bcm_kona_i2c_of_match ); <nl>  <nl> static struct platform_driver bcm_kona_i2c_driver = { <nl> . driver = {
static int amdgpu_ttm_io_mem_reserve ( struct ttm_bo_device * bdev , struct ttm_mem_ <nl> mem -> bus . addr = <nl> ioremap_nocache ( mem -> bus . base + mem -> bus . offset , <nl> mem -> bus . size ); <nl> + if (! mem -> bus . addr ) <nl> + return - ENOMEM ; <nl>  <nl> /* <nl> * Alpha : Use just the bus offset plus
static int ceph_con_in_msg_alloc ( struct ceph_connection * con , int * skip ) <nl> msg = con -> ops -> alloc_msg ( con , hdr , skip ); <nl> mutex_lock (& con -> mutex ); <nl> if ( con -> state != CON_STATE_OPEN ) { <nl> - ceph_msg_put ( msg ); <nl> + if ( msg ) <nl> + ceph_msg_put ( msg ); <nl> return - EAGAIN ; <nl> } <nl> con -> in_msg = msg ;
coh901318_tx_status ( struct dma_chan * chan , dma_cookie_t cookie , <nl> enum dma_status ret ; <nl>  <nl> ret = dma_cookie_status ( chan , cookie , txstate ); <nl> - if ( ret == DMA_COMPLETE ) <nl> + if ( ret == DMA_COMPLETE || ! txstate ) <nl> return ret ; <nl>  <nl> dma_set_residue ( txstate , coh901318_get_bytes_left ( chan ));
static int read_exceptions ( struct pstore * ps , <nl> r = insert_exceptions ( ps , area , callback , callback_context , <nl> & full ); <nl>  <nl> + if (! full ) <nl> + memcpy ( ps -> area , area , ps -> store -> chunk_size << SECTOR_SHIFT ); <nl> + <nl> dm_bufio_release ( bp ); <nl>  <nl> dm_bufio_forget ( client , chunk );
int ssb_bus_scan ( struct ssb_bus * bus , <nl> /* Ignore PCI cores on PCI - E cards . <nl> * Ignore PCI - E cores on PCI cards . */ <nl> if ( dev -> id . coreid == SSB_DEV_PCI ) { <nl> - if ( bus -> host_pci -> is_pcie ) <nl> + if ( pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } else { <nl> - if (! bus -> host_pci -> is_pcie ) <nl> + if (! pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } <nl> }
vmw_execbuf_copy_fence_user ( struct vmw_private * dev_priv , <nl> if ( user_fence_rep == NULL ) <nl> return ; <nl>  <nl> + memset (& fence_rep , 0 , sizeof ( fence_rep )); <nl> + <nl> fence_rep . error = ret ; <nl> if ( ret == 0 ) { <nl> BUG_ON ( fence == NULL );
static void xfrm_hash_rebuild ( struct work_struct * work ) <nl>  <nl> /* re - insert all policies by order of creation */ <nl> list_for_each_entry_reverse ( policy , & net -> xfrm . policy_all , walk . all ) { <nl> + if ( xfrm_policy_id2dir ( policy -> index ) >= XFRM_POLICY_MAX ) { <nl> + /* skip socket policies */ <nl> + continue ; <nl> + } <nl> newpos = NULL ; <nl> chain = policy_hash_bysel ( net , & policy -> selector , <nl> policy -> family ,
static void atombios_crtc_program_ss ( struct drm_crtc * crtc , <nl> return ; <nl> } <nl> args . v2 . ucEnable = enable ; <nl> - if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK )) <nl> + if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK ) || ASIC_IS_DCE41 ( rdev )) <nl> args . v2 . ucEnable = ATOM_DISABLE ; <nl> } else if ( ASIC_IS_DCE3 ( rdev )) { <nl> args . v1 . usSpreadSpectrumPercentage = cpu_to_le16 ( ss -> percentage );
static int __pppoe_xmit ( struct sock * sk , struct sk_buff * skb ) <nl> * give dev_queue_xmit something it can free . <nl> */ <nl> skb2 = skb_clone ( skb , GFP_ATOMIC ); <nl> + <nl> + if ( skb2 == NULL ) <nl> + goto abort ; <nl> } <nl>  <nl> ph = ( struct pppoe_hdr *) skb_push ( skb2 , sizeof ( struct pppoe_hdr ));
static void pic_clear_isr ( struct kvm_kpic_state * s , int irq ) <nl> void kvm_pic_clear_isr_ack ( struct kvm * kvm ) <nl> { <nl> struct kvm_pic * s = pic_irqchip ( kvm ); <nl> + pic_lock ( s ); <nl> s -> pics [ 0 ]. isr_ack = 0xff ; <nl> s -> pics [ 1 ]. isr_ack = 0xff ; <nl> + pic_unlock ( s ); <nl> } <nl>  <nl> /*
radeon_atom_encoder_dpms_dig ( struct drm_encoder * encoder , int mode ) <nl> * does the same thing and more . <nl> */ <nl> if (( rdev -> family != CHIP_RV710 ) && ( rdev -> family != CHIP_RV730 ) && <nl> - ( rdev -> family != CHIP_RS880 )) <nl> + ( rdev -> family != CHIP_RS780 ) && ( rdev -> family != CHIP_RS880 )) <nl> atombios_dig_transmitter_setup ( encoder , ATOM_TRANSMITTER_ACTION_ENABLE_OUTPUT , 0 , 0 ); <nl> } <nl> if ( ENCODER_MODE_IS_DP ( atombios_get_encoder_mode ( encoder )) && connector ) {
static struct davinci_nand_pdata davinci_nand_data = { <nl> . nr_parts = ARRAY_SIZE ( davinci_nand_partitions ), <nl> . ecc_mode = NAND_ECC_HW , <nl> . options = NAND_USE_FLASH_BBT , <nl> + . ecc_bits = 4 , <nl> }; <nl>  <nl> static struct resource davinci_nand_resources [] = {
static int __init create_setup_data_nodes ( struct dentry * parent ) <nl> if ( PageHighMem ( pg )) { <nl> data = ioremap_cache ( pa_data , sizeof (* data )); <nl> if (! data ) { <nl> + kfree ( node ); <nl> error = - ENXIO ; <nl> goto err_dir ; <nl> }
static int __init list_sort_test ( void ) <nl> } <nl> count ++; <nl> } <nl> + if ( head . prev != cur ) { <nl> + printk ( KERN_ERR " list_sort_test : error : list is corrupted \ n "); <nl> + goto exit ; <nl> + } <nl> + <nl>  <nl> if ( count != TEST_LIST_LEN ) { <nl> printk ( KERN_ERR " list_sort_test : error : bad list length % d ",
static int cipso_v4_map_cat_rbm_hton ( const struct cipso_v4_doi * doi_def , <nl>  <nl> switch ( doi_def -> type ) { <nl> case CIPSO_V4_MAP_PASS : <nl> - net_spot_max = host_cat_len - 1 ; <nl> - while ( net_spot_max > 0 && host_cat [ net_spot_max ] == 0 ) <nl> + net_spot_max = host_cat_len ; <nl> + while ( net_spot_max > 0 && host_cat [ net_spot_max - 1 ] == 0 ) <nl> net_spot_max --; <nl> if ( net_spot_max > net_cat_len ) <nl> return - EINVAL ;
int main ( int argc , char ** argv ) <nl> read_relocs ( fp ); <nl> if ( show_absolute_syms ) { <nl> print_absolute_symbols (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> if ( show_absolute_relocs ) { <nl> print_absolute_relocs (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> emit_relocs ( as_text , use_real_mode ); <nl> + out : <nl> + fclose ( fp ); <nl> return 0 ; <nl> }
set_ether_config ( struct eth_dev * dev , gfp_t gfp_flags ) <nl> result = usb_ep_enable ( dev -> out_ep , dev -> out ); <nl> if ( result != 0 ) { <nl> DEBUG ( dev , " enable % s --> % d \ n ", <nl> - dev -> in_ep -> name , result ); <nl> + dev -> out_ep -> name , result ); <nl> goto done ; <nl> } <nl> }
int oxygen_pci_probe ( struct pci_dev * pci , int index , char * id , <nl> goto err_pci_regions ; <nl>  <nl> if ( chip -> model . model_data_size ) { <nl> - chip -> model_data = kmalloc ( chip -> model . model_data_size , <nl> + chip -> model_data = kzalloc ( chip -> model . model_data_size , <nl> GFP_KERNEL ); <nl> if (! chip -> model_data ) { <nl> err = - ENOMEM ;
static int uvc_v4l2_open ( struct file * file ) <nl> if ( atomic_inc_return (& stream -> dev -> users ) == 1 ) { <nl> ret = uvc_status_start ( stream -> dev ); <nl> if ( ret < 0 ) { <nl> - usb_autopm_put_interface ( stream -> dev -> intf ); <nl> atomic_dec (& stream -> dev -> users ); <nl> + usb_autopm_put_interface ( stream -> dev -> intf ); <nl> kfree ( handle ); <nl> return ret ; <nl> }
static void bnx2x_set_one_mac_e1x ( struct bnx2x * bp , <nl> /* Reset the ramrod data buffer */ <nl> memset ( config , 0 , sizeof (* config )); <nl>  <nl> - bnx2x_vlan_mac_set_rdata_e1x ( bp , o , BNX2X_FILTER_MAC_PENDING , <nl> + bnx2x_vlan_mac_set_rdata_e1x ( bp , o , raw -> state , <nl> cam_offset , add , <nl> elem -> cmd_data . vlan_mac . u . mac . mac , 0 , <nl> ETH_VLAN_FILTER_ANY_VLAN , config );
struct rxrpc_call * rxrpc_new_incoming_call ( struct rxrpc_local * local , <nl>  <nl> /* Get the socket providing the service */ <nl> rx = rcu_dereference ( local -> service ); <nl> - if ( service_id == rx -> srx . srx_service ) <nl> + if ( rx && service_id == rx -> srx . srx_service ) <nl> goto found_service ; <nl>  <nl> trace_rxrpc_abort (" INV ", sp -> hdr . cid , sp -> hdr . callNumber , sp -> hdr . seq ,
static int atalk_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> return - ENOBUFS ; <nl>  <nl> * uaddr_len = sizeof ( struct sockaddr_at ); <nl> + memset (& sat . sat_zero , 0 , sizeof ( sat . sat_zero )); <nl>  <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED )
found : <nl>  <nl> if ( codec -> reg_cache ) <nl> kfree ( codec -> reg_cache ); <nl> + kfree ( codec -> name ); <nl> kfree ( codec ); <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_unregister_codec );
struct rtnl_link_stats64 * e1000e_get_stats64 ( struct net_device * netdev , <nl> static int e1000_change_mtu ( struct net_device * netdev , int new_mtu ) <nl> { <nl> struct e1000_adapter * adapter = netdev_priv ( netdev ); <nl> - int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN ; <nl> + int max_frame = new_mtu + VLAN_HLEN + ETH_HLEN + ETH_FCS_LEN ; <nl>  <nl> /* Jumbo frame support */ <nl> if (( max_frame > ETH_FRAME_LEN + ETH_FCS_LEN ) &&
void psched_ratecfg_precompute ( struct psched_ratecfg * r , u32 rate ) <nl> u64 mult ; <nl> int shift ; <nl>  <nl> - r -> rate_bps = rate << 3 ; <nl> + r -> rate_bps = ( u64 ) rate << 3 ; <nl> r -> shift = 0 ; <nl> r -> mult = 1 ; <nl> /*
 <nl> void btrfs_tree_lock ( struct extent_buffer * eb ); <nl> void btrfs_tree_unlock ( struct extent_buffer * eb ); <nl> - int btrfs_try_spin_lock ( struct extent_buffer * eb ); <nl>  <nl> void btrfs_tree_read_lock ( struct extent_buffer * eb ); <nl> void btrfs_tree_read_unlock ( struct extent_buffer * eb );
static struct array_cache ** alloc_alien_cache ( int node , int limit ) <nl> } <nl> ac_ptr [ i ] = alloc_arraycache ( node , limit , 0xbaadf00d ); <nl> if (! ac_ptr [ i ]) { <nl> - for ( i --; i <= 0 ; i --) <nl> + for ( i --; i >= 0 ; i --) <nl> kfree ( ac_ptr [ i ]); <nl> kfree ( ac_ptr ); <nl> return NULL ;
static int sunxi_pctrl_dt_node_to_map ( struct pinctrl_dev * pctldev , <nl> configlen ++; <nl>  <nl> pinconfig = kzalloc ( configlen * sizeof (* pinconfig ), GFP_KERNEL ); <nl> + if (! pinconfig ) { <nl> + kfree (* map ); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> if (! of_property_read_u32 ( node , " allwinner , drive ", & val )) { <nl> u16 strength = ( val + 1 ) * 10 ;
static int dbgp_control_msg ( unsigned devnum , int requesttype , <nl> int ret ; <nl>  <nl> read = ( requesttype & USB_DIR_IN ) != 0 ; <nl> - if ( size > ( read ? DBGP_MAX_PACKET : 0 )) <nl> + if ( size > ( read ? DBGP_MAX_PACKET : 0 )) <nl> return - 1 ; <nl>  <nl> /* Compute the control message */
static ssize_t do_generic_file_read ( struct file * filp , loff_t * ppos , <nl>  <nl> cond_resched (); <nl> find_page : <nl> + if ( fatal_signal_pending ( current )) { <nl> + error = - EINTR ; <nl> + goto out ; <nl> + } <nl> + <nl> page = find_get_page ( mapping , index ); <nl> if (! page ) { <nl> page_cache_sync_readahead ( mapping ,
static int ad799x_read_event_value ( struct iio_dev * indio_dev , <nl> if ( ret < 0 ) <nl> return ret ; <nl> * val = ( ret >> chan -> scan_type . shift ) & <nl> - GENMASK ( chan -> scan_type . realbits - 1 , 0 ); <nl> + GENMASK ( chan -> scan_type . realbits - 1 , 0 ); <nl>  <nl> return IIO_VAL_INT ; <nl> }
struct tcp_md5sig_pool * __tcp_get_md5sig_pool ( int cpu ) <nl>  <nl> EXPORT_SYMBOL ( __tcp_get_md5sig_pool ); <nl>  <nl> - void __tcp_put_md5sig_pool ( void ) { <nl> - __tcp_free_md5sig_pool ( tcp_md5sig_pool ); <nl> + void __tcp_put_md5sig_pool ( void ) <nl> +{ <nl> + tcp_free_md5sig_pool (); <nl> } <nl>  <nl> EXPORT_SYMBOL ( __tcp_put_md5sig_pool );
int rtllib_rx ( struct rtllib_device * ieee , struct sk_buff * skb , <nl> return ret ; <nl>  <nl> rx_dropped : <nl> - ieee -> stats . rx_dropped ++; <nl> + if ( ieee ) <nl> + ieee -> stats . rx_dropped ++; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( rtllib_rx );
static int bdc_udc_set_selfpowered ( struct usb_gadget * gadget , <nl> unsigned long flags ; <nl>  <nl> dev_dbg ( bdc -> dev , "% s ()\ n ", __func__ ); <nl> + gadget -> is_selfpowered = ( is_self != 0 ); <nl> spin_lock_irqsave (& bdc -> lock , flags ); <nl> if (! is_self ) <nl> bdc -> devstatus |= 1 << USB_DEVICE_SELF_POWERED ;
static bool _is_valid_div ( struct clk_divider * divider , unsigned int div ) <nl> static int _round_up_table ( const struct clk_div_table * table , int div ) <nl> { <nl> const struct clk_div_table * clkt ; <nl> - int up = _get_table_maxdiv ( table ); <nl> + int up = INT_MAX ; <nl>  <nl> for ( clkt = table ; clkt -> div ; clkt ++) { <nl> if ( clkt -> div == div )
static int __ocfs2_change_file_space ( struct file * file , struct inode * inode , <nl> if ( ret < 0 ) <nl> mlog_errno ( ret ); <nl>  <nl> + if ( file -> f_flags & O_SYNC ) <nl> + handle -> h_sync = 1 ; <nl> + <nl> ocfs2_commit_trans ( osb , handle ); <nl>  <nl> out_inode_unlock :
static int smc91c92_config ( struct pcmcia_device * link ) <nl> struct net_device * dev = link -> priv ; <nl> struct smc_private * smc = netdev_priv ( dev ); <nl> char * name ; <nl> - int i , j , rev ; <nl> + int i , rev , j = 0 ; <nl> unsigned int ioaddr ; <nl> u_long mir ; <nl> 
void __init iop13xx_platform_init ( void ) <nl>  <nl> # ifdef CONFIG_MTD_PHYSMAP <nl> iq8134x_flash_resource . end = iq8134x_flash_resource . start + <nl> - iq8134x_probe_flash_size (); <nl> + iq8134x_probe_flash_size () - 1 ; <nl> if ( iq8134x_flash_resource . end > iq8134x_flash_resource . start ) <nl> iop13xx_devices [ plat_idx ++] = & iq8134x_flash ; <nl> else
void register_lapic_address ( unsigned long address ); <nl> extern void setup_boot_APIC_clock ( void ); <nl> extern void setup_secondary_APIC_clock ( void ); <nl> extern int APIC_init_uniprocessor ( void ); <nl> + <nl> +# ifdef CONFIG_X86_64 <nl> + static inline int apic_force_enable ( unsigned long addr ) <nl> +{ <nl> + return - 1 ; <nl> +} <nl> +# else <nl> extern int apic_force_enable ( unsigned long addr ); <nl> +# endif <nl>  <nl> extern int apic_bsp_setup ( bool upmode ); <nl> extern void apic_ap_setup ( void );
void usb_del_gadget_udc ( struct usb_gadget * gadget ) <nl> flush_work (& gadget -> work ); <nl> device_unregister (& udc -> dev ); <nl> device_unregister (& gadget -> dev ); <nl> + memset (& gadget -> dev , 0x00 , sizeof ( gadget -> dev )); <nl> } <nl> EXPORT_SYMBOL_GPL ( usb_del_gadget_udc ); <nl> 
struct tpm_chip * tpm_register_hardware ( struct device * dev , const struct tpm_vend <nl> put_device ( dev ); <nl> clear_bit ( chip -> dev_num , dev_mask ); <nl> kfree ( chip ); <nl> + kfree ( devname ); <nl> return NULL ; <nl> } <nl> 
void drm_mm_remove_node ( struct drm_mm_node * node ) <nl> struct drm_mm * mm = node -> mm ; <nl> struct drm_mm_node * prev_node ; <nl>  <nl> + if ( WARN_ON (! node -> allocated )) <nl> + return ; <nl> + <nl> BUG_ON ( node -> scanned_block || node -> scanned_prev_free <nl> || node -> scanned_next_free ); <nl> 
void ceph_mdsc_sync ( struct ceph_mds_client * mdsc ) <nl> { <nl> u64 want_tid , want_flush ; <nl>  <nl> + if ( mdsc -> client -> mount_state == CEPH_MOUNT_SHUTDOWN ) <nl> + return ; <nl> + <nl> dout (" sync \ n "); <nl> mutex_lock (& mdsc -> mutex ); <nl> want_tid = mdsc -> last_tid ;
static inline int dev_hard_header ( struct sk_buff * skb , struct net_device * dev , <nl> const void * daddr , const void * saddr , <nl> unsigned len ) <nl> { <nl> - if (! dev -> header_ops ) <nl> + if (! dev -> header_ops || ! dev -> header_ops -> create ) <nl> return 0 ; <nl>  <nl> return dev -> header_ops -> create ( skb , dev , type , daddr , saddr , len );
static struct rt6_info * ip6_route_redirect ( struct in6_addr * dest , <nl> }, <nl> }, <nl> }, <nl> - . gateway = * gateway , <nl> }; <nl>  <nl> + ipv6_addr_copy (& rdfl . gateway , gateway ); <nl> + <nl> if ( rt6_need_strict ( dest )) <nl> flags |= RT6_LOOKUP_F_IFACE ; <nl> 
__ieee80211_tx_prepare ( struct ieee80211_tx_data * tx , <nl> tx -> local = local ; <nl> tx -> sdata = IEEE80211_DEV_TO_SUB_IF ( dev ); <nl> tx -> channel = local -> hw . conf . channel ; <nl> + tx -> rate_idx = - 1 ; <nl> + tx -> last_frag_rate_idx = - 1 ; <nl> /* <nl> * Set this flag ( used below to indicate " automatic fragmentation "), <nl> * it will be cleared / left by radiotap as desired .
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> /* Device config is special , because it must work on <nl> * an unconfigured device . */ <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> + if ( minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> + /* Device config not appropriate on non - board minors . */ <nl> + rc = - ENOTTY ; <nl> + goto done ; <nl> + } <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> if ( rc == 0 )
static void queue_event ( struct client * client , struct event * event , <nl> event -> v [ 1 ]. size = size1 ; <nl>  <nl> spin_lock_irqsave (& client -> lock , flags ); <nl> - <nl> list_add_tail (& event -> link , & client -> event_list ); <nl> - wake_up_interruptible (& client -> wait ); <nl> - <nl> spin_unlock_irqrestore (& client -> lock , flags ); <nl> + <nl> + wake_up_interruptible (& client -> wait ); <nl> } <nl>  <nl> static int
static int ina2xx_buffer_enable ( struct iio_dev * indio_dev ) <nl> chip -> prev_ns = iio_get_time_ns (); <nl>  <nl> chip -> task = kthread_run ( ina2xx_capture_thread , ( void *) indio_dev , <nl> - " ina2xx -% uus ", sampling_us ); <nl> + "% s :% d -% uus ", indio_dev -> name , indio_dev -> id , <nl> + sampling_us ); <nl>  <nl> return PTR_ERR_OR_ZERO ( chip -> task ); <nl> }
static int emc1403_probe ( struct i2c_client * client , <nl> res = sysfs_create_group (& client -> dev . kobj , & m_thermal_gr ); <nl> if ( res ) { <nl> dev_warn (& client -> dev , " create group failed \ n "); <nl> - hwmon_device_unregister ( data -> hwmon_dev ); <nl> goto thermal_error1 ; <nl> } <nl> data -> hwmon_dev = hwmon_device_register (& client -> dev );
static unsigned long output_ptr = 0 ; <nl> static void * malloc ( int size ); <nl> static void free ( void * where ); <nl>  <nl> + void * memset ( void * s , int c , unsigned n ); <nl> + void * memcpy ( void * dest , const void * src , unsigned n ); <nl> + <nl> static void putstr ( const char *); <nl>  <nl> extern int end ;
static int virtnet_probe ( struct virtio_device * vdev ) <nl> dev -> mtu = mtu ; <nl> dev -> max_mtu = mtu ; <nl> } <nl> + <nl> + /* TODO : size buffers correctly in this case . */ <nl> + if ( dev -> mtu > ETH_DATA_LEN ) <nl> + vi -> big_packets = true ; <nl> } <nl>  <nl> if ( vi -> any_header_sg )
int blkcg_init_queue ( struct request_queue * q ) <nl> if ( preloaded ) <nl> radix_tree_preload_end (); <nl>  <nl> - if ( IS_ERR ( blkg )) { <nl> - blkg_free ( new_blkg ); <nl> + if ( IS_ERR ( blkg )) <nl> return PTR_ERR ( blkg ); <nl> - } <nl>  <nl> q -> root_blkg = blkg ; <nl> q -> root_rl . blkg = blkg ;
static int s3c24xx_eint_init ( struct samsung_pinctrl_drv_data * d ) <nl> irq = bank -> eint_offset ; <nl> mask = bank -> eint_mask ; <nl> for ( pin = 0 ; mask ; ++ pin , mask >>= 1 ) { <nl> - if ( irq > NUM_EINT ) <nl> + if ( irq >= NUM_EINT ) <nl> break ; <nl> if (!( mask & 1 )) <nl> continue ;
int load_bpf_file ( char * path ) <nl> Elf_Data * data , * data_prog , * symbols = NULL ; <nl> char * shname , * shname_prog ; <nl>  <nl> + /* reset global variables */ <nl> + kern_version = 0 ; <nl> + memset ( license , 0 , sizeof ( license )); <nl> + memset ( processed_sec , 0 , sizeof ( processed_sec )); <nl> + <nl> if ( elf_version ( EV_CURRENT ) == EV_NONE ) <nl> return 1 ; <nl> 
static int raid10_add_disk ( mddev_t * mddev , mdk_rdev_t * rdev ) <nl> if (! enough ( conf )) <nl> return - EINVAL ; <nl>  <nl> - if ( rdev -> raid_disk ) <nl> + if ( rdev -> raid_disk >= 0 ) <nl> first = last = rdev -> raid_disk ; <nl>  <nl> if ( rdev -> saved_raid_disk >= 0 &&
static int tiadc_read_raw ( struct iio_dev * indio_dev , <nl> return - EAGAIN ; <nl> } <nl> } <nl> - map_val = chan -> channel + TOTAL_CHANNELS ; <nl> + map_val = adc_dev -> channel_step [ chan -> scan_index ]; <nl>  <nl> /* <nl> * We check the complete FIFO . We programmed just one entry but in case
void cpuset_print_task_mems_allowed ( struct task_struct * tsk ) <nl>  <nl> struct cgroup * cgrp = task_cs ( tsk )-> css . cgroup ; <nl>  <nl> + rcu_read_lock (); <nl> spin_lock (& cpuset_buffer_lock ); <nl>  <nl> nodelist_scnprintf ( cpuset_nodelist , CPUSET_NODELIST_LEN , <nl> void cpuset_print_task_mems_allowed ( struct task_struct * tsk ) <nl> tsk -> comm , cgroup_name ( cgrp ), cpuset_nodelist ); <nl>  <nl> spin_unlock (& cpuset_buffer_lock ); <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> /*
static inline void ftrace_dump ( enum ftrace_dump_mode oops_dump_mode ) { } <nl> # define COMPACTION_BUILD 0 <nl> # endif <nl>  <nl> +/* This helps us to avoid # ifdef CONFIG_SYMBOL_PREFIX */ <nl> +# ifdef CONFIG_SYMBOL_PREFIX <nl> +# define SYMBOL_PREFIX CONFIG_SYMBOL_PREFIX <nl> +# else <nl> +# define SYMBOL_PREFIX "" <nl> +# endif <nl> + <nl> /* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */ <nl> # ifdef CONFIG_FTRACE_MCOUNT_RECORD <nl> # define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD
static void __init sanity_check_meminfo ( void ) <nl> * Check whether this memory bank would entirely overlap <nl> * the vmalloc area . <nl> */ <nl> - if ( __va ( bank -> start ) >= VMALLOC_MIN ) { <nl> + if ( __va ( bank -> start ) >= VMALLOC_MIN || <nl> + __va ( bank -> start ) < PAGE_OFFSET ) { <nl> printk ( KERN_NOTICE " Ignoring RAM at %. 8lx -%. 8lx " <nl> "( vmalloc region overlap ).\ n ", <nl> bank -> start , bank -> start + bank -> size - 1 );
static umode_t lm3533_attr_is_visible ( struct kobject * kobj , <nl> struct device_attribute * dattr = to_dev_attr ( attr ); <nl> struct lm3533_device_attribute * lattr = to_lm3533_dev_attr ( dattr ); <nl> enum lm3533_attribute_type type = lattr -> type ; <nl> - mode_t mode = attr -> mode ; <nl> + umode_t mode = attr -> mode ; <nl>  <nl> if (! lm3533 -> have_backlights && type == LM3533_ATTR_TYPE_BACKLIGHT ) <nl> mode = 0 ;
static void unfreeze_partials ( struct kmem_cache * s ) <nl>  <nl> new . frozen = 0 ; <nl>  <nl> - if (! new . inuse && (! n || n -> nr_partial < s -> min_partial )) <nl> + if (! new . inuse && (! n || n -> nr_partial > s -> min_partial )) <nl> m = M_FREE ; <nl> else { <nl> struct kmem_cache_node * n2 = get_node ( s ,
EXPORT_SYMBOL ( writeback_in_progress ); <nl>  <nl> struct backing_dev_info * inode_to_bdi ( struct inode * inode ) <nl> { <nl> - struct super_block * sb = inode -> i_sb ; <nl> + struct super_block * sb ; <nl> + <nl> + if (! inode ) <nl> + return & noop_backing_dev_info ; <nl> + <nl> + sb = inode -> i_sb ; <nl> # ifdef CONFIG_BLOCK <nl> if ( sb_is_blkdev_sb ( sb )) <nl> return blk_get_backing_dev_info ( I_BDEV ( inode ));
static inline void __cache_free ( struct kmem_cache * cachep , void * objp ) <nl> check_irq_off (); <nl> objp = cache_free_debugcheck ( cachep , objp , __builtin_return_address ( 0 )); <nl>  <nl> - if ( use_alien_caches && cache_free_alien ( cachep , objp )) <nl> + if ( cache_free_alien ( cachep , objp )) <nl> return ; <nl>  <nl> if ( likely ( ac -> avail < ac -> limit )) {
int aix_partition ( struct parsed_partitions * state ) <nl> numlvs = be16_to_cpu ( p -> numlvs ); <nl> put_dev_sector ( sect ); <nl> } <nl> - lvip = kzalloc ( sizeof ( struct lv_info ) * state -> limit , GFP_KERNEL ); <nl> + lvip = kcalloc ( state -> limit , sizeof ( struct lv_info ), GFP_KERNEL ); <nl> if (! lvip ) <nl> return 0 ; <nl> if ( numlvs && ( d = read_part_sector ( state , vgda_sector + 1 , & sect ))) {
static int exynos_iommu_add_device ( struct device * dev ) <nl> struct iommu_group * group ; <nl> int ret ; <nl>  <nl> + if (! has_sysmmu ( dev )) <nl> + return - ENODEV ; <nl> + <nl> group = iommu_group_get ( dev ); <nl>  <nl> if (! group ) { <nl> static int exynos_iommu_add_device ( struct device * dev ) <nl>  <nl> static void exynos_iommu_remove_device ( struct device * dev ) <nl> { <nl> + if (! has_sysmmu ( dev )) <nl> + return ; <nl> + <nl> iommu_group_remove_device ( dev ); <nl> } <nl> 
static int __init cy_detect_isa ( void ) <nl> continue ; <nl> } <nl> # ifdef MODULE <nl> - if ( isparam && irq [ i ]) <nl> + if ( isparam && i < NR_CARDS && irq [ i ]) <nl> cy_isa_irq = irq [ i ]; <nl> else <nl> # endif
skbfree ( struct sk_buff * skb ) <nl> return ; <nl> while ( atomic_read (& skb_shinfo ( skb )-> dataref ) != 1 && i -- > 0 ) <nl> msleep ( Sms ); <nl> - if ( i <= 0 ) { <nl> + if ( i < 0 ) { <nl> printk ( KERN_ERR <nl> " aoe : % s holds ref : % s \ n ", <nl> skb -> dev ? skb -> dev -> name : " netif ",
::" a " ( rw ) : " memory ") <nl>  <nl> # define __build_write_lock_const ( rw , helper ) \ <nl> - asm volatile ( LOCK " subl $" RW_LOCK_BIAS_STR ",(% 0 )\ n \ t " \ <nl> + asm volatile ( LOCK " subl $" RW_LOCK_BIAS_STR ",% 0 \ n \ t " \ <nl> " jnz 2f \ n " \ <nl> " 1 :\ n " \ <nl> LOCK_SECTION_START ("") \
static void dm_stat_free ( struct rcu_head * head ) <nl> int cpu ; <nl> struct dm_stat * s = container_of ( head , struct dm_stat , rcu_head ); <nl>  <nl> + kfree ( s -> histogram_boundaries ); <nl> kfree ( s -> program_id ); <nl> kfree ( s -> aux_data ); <nl> for_each_possible_cpu ( cpu ) {
static void ieee80211_do_stop ( struct ieee80211_sub_if_data * sdata , <nl> if ( sdata -> vif . txq ) { <nl> struct txq_info * txqi = to_txq_info ( sdata -> vif . txq ); <nl>  <nl> + spin_lock_bh (& txqi -> queue . lock ); <nl> ieee80211_purge_tx_queue (& local -> hw , & txqi -> queue ); <nl> + spin_unlock_bh (& txqi -> queue . lock ); <nl> + <nl> atomic_set (& sdata -> txqs_len [ txqi -> txq . ac ], 0 ); <nl> } <nl> 
static int __init init_spkm3_module ( void ) <nl> status = gss_mech_register (& gss_spkm3_mech ); <nl> if ( status ) <nl> printk (" Failed to register spkm3 gss mechanism !\ n "); <nl> - return 0 ; <nl> + return status ; <nl> } <nl>  <nl> static void __exit cleanup_spkm3_module ( void )
static int clip_constructor ( struct neighbour * neigh ) <nl>  <nl> static int clip_encap ( struct atm_vcc * vcc , int mode ) <nl> { <nl> + if (! CLIP_VCC ( vcc )) <nl> + return - EBADFD ; <nl> + <nl> CLIP_VCC ( vcc )-> encap = mode ; <nl> return 0 ; <nl> }
static int newseg ( struct ipc_namespace * ns , struct ipc_params * params ) <nl> if ( size < SHMMIN || size > ns -> shm_ctlmax ) <nl> return - EINVAL ; <nl>  <nl> - if ( ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> + if ( ns -> shm_tot + numpages < ns -> shm_tot || <nl> + ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> return - ENOSPC ; <nl>  <nl> shp = ipc_rcu_alloc ( sizeof (* shp ));
int vmw_kms_update_layout_ioctl ( struct drm_device * dev , void * data , <nl> ret = copy_from_user ( rects , user_rects , rects_size ); <nl> if ( unlikely ( ret != 0 )) { <nl> DRM_ERROR (" Failed to get rects .\ n "); <nl> + ret = - EFAULT ; <nl> goto out_free ; <nl> } <nl> 
int __vma_adjust ( struct vm_area_struct * vma , unsigned long start , <nl> * If next doesn ' t have anon_vma , import from vma after <nl> * next , if the vma overlaps with it . <nl> */ <nl> - if ( remove_next == 2 && next && ! next -> anon_vma ) <nl> + if ( remove_next == 2 && ! next -> anon_vma ) <nl> exporter = next -> vm_next ; <nl>  <nl> } else if ( end > next -> vm_start ) {
static int pty_set_pktmode ( struct tty_struct * tty , int __user * arg ) <nl> spin_lock_irq (& tty -> ctrl_lock ); <nl> if ( pktmode ) { <nl> if (! tty -> packet ) { <nl> - tty -> packet = 1 ; <nl> tty -> link -> ctrl_status = 0 ; <nl> + smp_mb (); <nl> + tty -> packet = 1 ; <nl> } <nl> } else <nl> tty -> packet = 0 ;
int qed_resc_alloc ( struct qed_dev * cdev ) <nl> DP_ERR ( p_hwfn , <nl> " Cannot allocate 0x % x EQ elements . The maximum of a u16 chain is 0x % x \ n ", <nl> n_eqes , 0xFFFF ); <nl> + rc = - EINVAL ; <nl> goto alloc_err ; <nl> } <nl> 
static ssize_t store_scaling_governor ( struct cpufreq_policy * policy , <nl> return - EINVAL ; <nl>  <nl> ret = cpufreq_set_policy ( policy , & new_policy ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> policy -> user_policy . policy = policy -> policy ; <nl> policy -> user_policy . governor = policy -> governor ; <nl> - <nl> - if ( ret ) <nl> - return ret ; <nl> - else <nl> - return count ; <nl> + return count ; <nl> } <nl>  <nl> /**
long vhost_dev_ioctl ( struct vhost_dev * d , unsigned int ioctl , void __user * argp ) <nl> } <nl> if ( eventfp != d -> log_file ) { <nl> filep = d -> log_file ; <nl> + d -> log_file = eventfp ; <nl> ctx = d -> log_ctx ; <nl> d -> log_ctx = eventfp ? <nl> eventfd_ctx_fileget ( eventfp ) : NULL ;
static void taal_esd_work ( struct work_struct * work ) <nl> } <nl> /* Self - diagnostics result is also shown on TE GPIO line . We need <nl> * to re - enable TE after self diagnostics */ <nl> - if ( td -> use_ext_te && td -> te_enabled ) <nl> - taal_enable_te ( dssdev , true ); <nl> + if ( td -> use_ext_te && td -> te_enabled ) { <nl> + r = taal_dcs_write_1 ( DCS_TEAR_ON , 0 ); <nl> + if ( r ) <nl> + goto err ; <nl> + } <nl>  <nl> dsi_bus_unlock (); <nl> 
static int tcp_ack_update_window ( struct sock * sk , struct tcp_sock * tp , <nl> static void tcp_conservative_spur_to_response ( struct tcp_sock * tp ) <nl> { <nl> tp -> snd_cwnd = min ( tp -> snd_cwnd , tp -> snd_ssthresh ); <nl> + tp -> snd_cwnd_cnt = 0 ; <nl> tcp_moderate_cwnd ( tp ); <nl> } <nl> 
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> + if ( rc == 0 ) <nl> + /* Evade comedi_auto_unconfig (). */ <nl> + dev_file_info -> hardware_device = NULL ; <nl> goto done ; <nl> } <nl> 
static void map_cpu_to_logical_apicid ( void ) <nl> { <nl> int cpu = smp_processor_id (); <nl> int apicid = logical_smp_processor_id (); <nl> + int node = apicid_to_node ( apicid ); <nl> + <nl> + if (! node_online ( node )) <nl> + node = first_online_node ; <nl>  <nl> cpu_2_logical_apicid [ cpu ] = apicid ; <nl> - map_cpu_to_node ( cpu , apicid_to_node ( apicid )); <nl> + map_cpu_to_node ( cpu , node ); <nl> } <nl>  <nl> static void unmap_cpu_to_logical_apicid ( int cpu )
void dlm_user_add_ast ( struct dlm_lkb * lkb , int type ) <nl> spin_unlock (& proc -> asts_spin ); <nl>  <nl> if ( eol ) { <nl> - spin_lock (& ua -> proc -> locks_spin ); <nl> + spin_lock (& proc -> locks_spin ); <nl> if (! list_empty (& lkb -> lkb_ownqueue )) { <nl> list_del_init (& lkb -> lkb_ownqueue ); <nl> dlm_put_lkb ( lkb ); <nl> } <nl> - spin_unlock (& ua -> proc -> locks_spin ); <nl> + spin_unlock (& proc -> locks_spin ); <nl> } <nl> out : <nl> mutex_unlock (& ls -> ls_clear_proc_locks );
static int kvm_dev_ioctl_get_supported_cpuid ( struct kvm_cpuid2 * cpuid , <nl> for ( func = 0x80000001 ; func <= limit && nent < cpuid -> nent ; ++ func ) <nl> do_cpuid_ent (& cpuid_entries [ nent ], func , 0 , <nl> & nent , cpuid -> nent ); <nl> + r = - E2BIG ; <nl> + if ( nent >= cpuid -> nent ) <nl> + goto out_free ; <nl> + <nl> r = - EFAULT ; <nl> if ( copy_to_user ( entries , cpuid_entries , <nl> nent * sizeof ( struct kvm_cpuid_entry2 )))
static int __devinit ci13xxx_pci_probe ( struct pci_dev * pdev , <nl> struct resource res [ 3 ]; <nl> int retval = 0 , nres = 2 ; <nl>  <nl> + if (! driver ) { <nl> + dev_err (& pdev -> dev , " device doesn ' t provide driver data \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> retval = pci_enable_device ( pdev ); <nl> if ( retval ) <nl> goto done ;
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static inline struct sk_buff * sk_stream_alloc_pskb ( struct sock * sk , <nl> { <nl> struct sk_buff * skb ; <nl>  <nl> + /* The TCP header must be at least 32 - bit aligned . */ <nl> + size = ALIGN ( size , 4 ); <nl> + <nl> skb = alloc_skb_fclone ( size + sk -> sk_prot -> max_header , gfp ); <nl> if ( skb ) { <nl> skb -> truesize += mem ;
static void ieee80211_handle_filtered_frame ( struct ieee80211_local * local , <nl> struct ieee80211_hdr * hdr = ( void *) skb -> data ; <nl> int ac ; <nl>  <nl> + if ( info -> flags & IEEE80211_TX_CTL_NO_PS_BUFFER ) { <nl> + ieee80211_free_txskb (& local -> hw , skb ); <nl> + return ; <nl> + } <nl> + <nl> /* <nl> * This skb ' survived ' a round - trip through the driver , and <nl> * hopefully the driver didn ' t mangle it too badly . However ,
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
static int dwc3_ep0_set_config ( struct dwc3 * dwc , struct usb_ctrlrequest * ctrl ) <nl> case DWC3_ADDRESS_STATE : <nl> ret = dwc3_ep0_delegate_req ( dwc , ctrl ); <nl> /* if the cfg matches and the cfg is non zero */ <nl> - if (! ret && cfg ) <nl> + if ( cfg && (! ret || ( ret == USB_GADGET_DELAYED_STATUS ))) <nl> dwc -> dev_state = DWC3_CONFIGURED_STATE ; <nl> break ; <nl> 
int dlm_posix_lock ( dlm_lockspace_t * lockspace , u64 number , struct file * file , <nl> send_op ( op ); <nl>  <nl> if ( xop -> callback == NULL ) { <nl> - rv = wait_event_killable ( recv_wq , ( op -> done != 0 )); <nl> + rv = wait_event_interruptible ( recv_wq , ( op -> done != 0 )); <nl> if ( rv == - ERESTARTSYS ) { <nl> log_debug ( ls , " dlm_posix_lock : wait killed % llx ", <nl> ( unsigned long long ) number );
populate_shared_memory : <nl> */ <nl> if ( ret == - EAGAIN && op_state_purged ( new_op )) { <nl> orangefs_bufmap_put ( bufmap , buffer_index ); <nl> + buffer_index = - 1 ; <nl> gossip_debug ( GOSSIP_FILE_DEBUG , <nl> "% s : going to repopulate_shared_memory .\ n ", <nl> __func__ );
static int auerchain_start_wait_urb ( pauerchain_t acp , struct urb * urb , int time <nl> } else <nl> status = urb -> status ; <nl>  <nl> - if ( actual_length ) <nl> + if ( status >= 0 ) <nl> * actual_length = urb -> actual_length ; <nl>  <nl> return status ;
static void visual_init ( struct vc_data * vc , int num , int init ) <nl> __module_get ( vc -> vc_sw -> owner ); <nl> vc -> vc_num = num ; <nl> vc -> vc_display_fg = & master_display_fg ; <nl> + if ( vc -> vc_uni_pagedir_loc ) <nl> + con_free_unimap ( vc ); <nl> vc -> vc_uni_pagedir_loc = & vc -> vc_uni_pagedir ; <nl> vc -> vc_uni_pagedir = NULL ; <nl> vc -> vc_hi_font_mask = 0 ;
asmlinkage long sys_socketcall ( int call , unsigned long __user * args ) <nl> if ( copy_from_user ( a , args , nargs [ call ])) <nl> return - EFAULT ; <nl>  <nl> - err = audit_socketcall ( nargs [ call ]/ sizeof ( unsigned long ), args ); <nl> + err = audit_socketcall ( nargs [ call ]/ sizeof ( unsigned long ), a ); <nl> if ( err ) <nl> return err ; <nl> 
static int ide_diag_taskfile ( ide_drive_t * drive , ide_task_t * args , unsigned long <nl> struct request rq ; <nl>  <nl> memset (& rq , 0 , sizeof ( rq )); <nl> + rq . ref_count = 1 ; <nl> rq . cmd_type = REQ_TYPE_ATA_TASKFILE ; <nl> rq . buffer = buf ; <nl> 
int rtl8188eu_init_recv_priv ( struct adapter * padapter ) <nl> _rtw_init_queue (& precvpriv -> free_recv_buf_queue ); <nl>  <nl> precvpriv -> pallocated_recv_buf = <nl> - kzalloc ( NR_RECVBUFF * sizeof ( struct recv_buf ), GFP_KERNEL ); <nl> + kcalloc ( NR_RECVBUFF , sizeof ( struct recv_buf ), GFP_KERNEL ); <nl> if ( precvpriv -> pallocated_recv_buf == NULL ) { <nl> res = _FAIL ; <nl> RT_TRACE ( _module_rtl871x_recv_c_ , _drv_err_ ,
int ath9k_hw_reset ( struct ath_hw * ah , struct ath9k_channel * chan , <nl>  <nl> if ( ah -> hw -> conf . radar_enabled ) { <nl> /* set HW specific DFS configuration */ <nl> + ah -> radar_conf . ext_channel = IS_CHAN_HT40 ( chan ); <nl> ath9k_hw_set_radar_params ( ah ); <nl> } <nl> 
bool i40e_is_vsi_in_vlan ( struct i40e_vsi * vsi ) <nl> * so we have to go through all the list in order to make sure <nl> */ <nl> list_for_each_entry ( f , & vsi -> mac_filter_list , list ) { <nl> - if ( f -> vlan >= 0 ) <nl> + if ( f -> vlan >= 0 || vsi -> info . pvid ) <nl> return true ; <nl> } <nl> 
static struct platform_device * crag6410_devices [] __initdata = { <nl> & s3c_device_fb , <nl> & s3c_device_ohci , <nl> & s3c_device_usb_hsotg , <nl> - & s3c_device_adc , <nl> - & s3c_device_rtc , <nl> - & s3c_device_ts , <nl> & s3c_device_timer [ 0 ], <nl> & s3c64xx_device_iis0 , <nl> & s3c64xx_device_iis1 ,
static int __devinit snd_hdspm_create ( struct snd_card * card , <nl> hdspm -> card_name = " RME AES32 "; <nl> hdspm -> midiPorts = 2 ; <nl> break ; <nl> + default : <nl> + snd_printk ( KERN_ERR " HDSPM : unknown firmware revision % x \ n ", <nl> + hdspm -> firmware_rev ); <nl> + return - ENODEV ; <nl> } <nl>  <nl> err = pci_enable_device ( pci );
static int ks7010_upload_firmware ( struct ks_sdio_card * card ) <nl> unsigned char * rom_buf ; <nl> unsigned char rw_data = 0 ; <nl> int ret ; <nl> - int length ; <nl> + unsigned int length ; <nl> const struct firmware * fw_entry = NULL ; <nl>  <nl> /* buffer allocate */
static void handle_stop_signal ( int sig , struct task_struct * p ) <nl> { <nl> struct task_struct * t ; <nl>  <nl> - if ( p -> flags & SIGNAL_GROUP_EXIT ) <nl> + if ( p -> signal -> flags & SIGNAL_GROUP_EXIT ) <nl> /* <nl> * The process is in the middle of dying already . <nl> */
struct nfs_server * nfs4_create_referral_server ( struct nfs_clone_mount * data , <nl> parent_server -> client -> cl_xprt -> prot , <nl> parent_client -> retrans_timeo , <nl> parent_client -> retrans_count ); <nl> + if ( error < 0 ) <nl> + goto error ; <nl>  <nl> /* Initialise the client representation from the parent server */ <nl> nfs_server_copy_userdata ( server , parent_server );
static int __init mpf_checksum ( unsigned char * mp , int len ) <nl> return sum & 0xFF ; <nl> } <nl>  <nl> - static void __cpuinit MP_processor_info ( struct mpc_config_processor * m ) <nl> + static void __init MP_processor_info ( struct mpc_config_processor * m ) <nl> { <nl> int apicid ; <nl> char * bootup_cpu = "";
struct ipoib_neigh * ipoib_neigh_alloc ( struct neighbour * neighbour , <nl>  <nl> neigh -> neighbour = neighbour ; <nl> neigh -> dev = dev ; <nl> + memset (& neigh -> dgid . raw , 0 , sizeof ( union ib_gid )); <nl> * to_ipoib_neigh ( neighbour ) = neigh ; <nl> skb_queue_head_init (& neigh -> queue ); <nl> ipoib_cm_set ( neigh , NULL );
again : <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> + page = compound_head ( page ); <nl> lock_page ( page ); <nl> if (! page -> mapping ) { <nl> unlock_page ( page );
static int atmel_pdmic_cpu_dai_startup ( struct snd_pcm_substream * substream , <nl> return ret ; <nl>  <nl> ret = clk_prepare_enable ( dd -> pclk ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + clk_disable_unprepare ( dd -> gclk ); <nl> return ret ; <nl> + } <nl>  <nl> /* Clear all bits in the Control Register ( PDMIC_CR ) */ <nl> regmap_write ( dd -> regmap , PDMIC_CR , 0 );
static int gb_loopback_transfer ( struct gb_loopback * gb , u32 len ) <nl> return - ENOMEM ; <nl> } <nl>  <nl> + memset ( request -> data , 0x5A , len ); <nl> + <nl> request -> len = cpu_to_le32 ( len ); <nl>  <nl> do_gettimeofday (& ts );
static int max77686_clk_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + drv_data -> num_clks = num_clks ; <nl> drv_data -> max_clk_data = devm_kcalloc ( dev , num_clks , <nl> sizeof (* drv_data -> max_clk_data ), <nl> GFP_KERNEL );
static inline void rtsx_exclusive_enter_ss ( struct rtsx_chip * chip ) <nl> { <nl> struct rtsx_dev * dev = chip -> rtsx ; <nl>  <nl> - spin_lock (&( dev -> reg_lock )); <nl> + spin_lock (& dev -> reg_lock ); <nl> rtsx_enter_ss ( chip ); <nl> - spin_unlock (&( dev -> reg_lock )); <nl> + spin_unlock (& dev -> reg_lock ); <nl> } <nl>  <nl> static inline void rtsx_reset_detected_cards ( struct rtsx_chip * chip , int flag )
skl_ddi_pll_select ( struct intel_crtc * intel_crtc , <nl> DPLL_CFGCR2_KDIV ( wrpll_params . kdiv ) | <nl> DPLL_CFGCR2_PDIV ( wrpll_params . pdiv ) | <nl> wrpll_params . central_freq ; <nl> - } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT ) { <nl> + } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT || <nl> + intel_encoder -> type == INTEL_OUTPUT_DP_MST ) { <nl> switch ( crtc_state -> port_clock / 2 ) { <nl> case 81000 : <nl> ctrl1 |= DPLL_CTRL1_LINK_RATE ( DPLL_CTRL1_LINK_RATE_810 , 0 );
static int ath9k_tx ( struct ieee80211_hw * hw , <nl> struct ath_tx_control txctl ; <nl> int hdrlen , padsize ; <nl>  <nl> + if ( aphy -> state != ATH_WIPHY_ACTIVE ) { <nl> + printk ( KERN_DEBUG " ath9k : % s : TX in unexpected wiphy state " <nl> + "% d \ n ", wiphy_name ( hw -> wiphy ), aphy -> state ); <nl> + goto exit ; <nl> + } <nl> + <nl> memset (& txctl , 0 , sizeof ( struct ath_tx_control )); <nl>  <nl> /*
noinline int btrfs_truncate_inode_items ( struct btrfs_trans_handle * trans , <nl> if ( root -> ref_cows ) <nl> btrfs_drop_extent_cache ( inode , new_size & (~ mask ), ( u64 )- 1 , 0 ); <nl> path = btrfs_alloc_path (); <nl> - path -> reada = - 1 ; <nl> BUG_ON (! path ); <nl> + path -> reada = - 1 ; <nl>  <nl> /* FIXME , add redo link to tree so we don ' t leak on crash */ <nl> key . objectid = inode -> i_ino ;
static int chap_server_compute_md5 ( <nl> pr_err (" Unable to convert incoming challenge \ n "); <nl> goto out ; <nl> } <nl> + if ( challenge_len > 1024 ) { <nl> + pr_err (" CHAP_C exceeds maximum binary size of 1024 bytes \ n "); <nl> + goto out ; <nl> + } <nl> /* <nl> * During mutual authentication , the CHAP_C generated by the <nl> * initiator must not match the original CHAP_C generated by
long vt_compat_ioctl ( struct tty_struct * tty , struct file * file , <nl>  <nl> case PIO_UNIMAP : <nl> case GIO_UNIMAP : <nl> - ret = do_unimap_ioctl ( cmd , up , perm , vc ); <nl> + ret = compat_unimap_ioctl ( cmd , up , perm , vc ); <nl> break ; <nl>  <nl> /*
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - counter_config = kmalloc ( sizeof ( struct op_counter_config ) * spec -> num_counters , <nl> + counter_config = kcalloc ( spec -> num_counters , sizeof ( struct op_counter_config ), <nl> GFP_KERNEL ); <nl> if (! counter_config ) <nl> return - ENOMEM ;
static void belkin_sa_set_termios ( struct usb_serial_port * port , struct ktermios <nl> } <nl>  <nl> baud = tty_get_baud_rate ( port -> tty ); <nl> + if ( baud == 0 ) { <nl> + dbg ("% s - tty_get_baud_rate says 0 baud ", __FUNCTION__ ); <nl> + return ; <nl> + } <nl> urb_value = BELKIN_SA_BAUD ( baud ); <nl> /* Clip to maximum speed */ <nl> if ( urb_value == 0 )
static int acpi_cpufreq_cpu_init ( struct cpufreq_policy * policy ) <nl>  <nl> switch ( perf -> control_register . space_id ) { <nl> case ACPI_ADR_SPACE_SYSTEM_IO : <nl> + if ( boot_cpu_data . x86_vendor == X86_VENDOR_AMD && <nl> + boot_cpu_data . x86 == 0xf ) { <nl> + pr_debug (" AMD K8 systems must use native drivers .\ n "); <nl> + result = - ENODEV ; <nl> + goto err_unreg ; <nl> + } <nl> pr_debug (" SYSTEM IO addr space \ n "); <nl> data -> cpu_feature = SYSTEM_IO_CAPABLE ; <nl> break ;
void brcmf_fweh_process_event ( struct brcmf_pub * drvr , <nl> alloc_flag = GFP_ATOMIC ; <nl>  <nl> event = kzalloc ( sizeof (* event ) + datalen , alloc_flag ); <nl> + if (! event ) <nl> + return ; <nl> + <nl> event -> code = code ; <nl> event -> ifidx = * ifidx ; <nl> 
struct map * machine__new_module ( struct machine * machine , u64 start , <nl> if ( kmod_path__parse_name (& m , filename )) <nl> return NULL ; <nl>  <nl> + map = map_groups__find_by_name (& machine -> kmaps , MAP__FUNCTION , <nl> + m . name ); <nl> + if ( map ) <nl> + goto out ; <nl> + <nl> dso = machine__module_dso ( machine , & m , filename ); <nl> if ( dso == NULL ) <nl> goto out ;
static void falcon_handle_rx_event ( struct efx_channel * channel , <nl> * UDP / IPv4 , then we can rely on the hardware checksum . <nl> */ <nl> checksummed = <nl> - rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_TCP || <nl> - rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_UDP ; <nl> + efx -> rx_checksum_enabled && <nl> + ( rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_TCP || <nl> + rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_UDP ); <nl> } else { <nl> falcon_handle_rx_not_ok ( rx_queue , event , & rx_ev_pkt_ok , <nl> & discard );
static int rtl8152_close ( struct net_device * netdev ) <nl> netif_stop_queue ( netdev ); <nl>  <nl> res = usb_autopm_get_interface ( tp -> intf ); <nl> - if ( res < 0 ) { <nl> + if ( res < 0 || test_bit ( RTL8152_UNPLUG , & tp -> flags )) { <nl> rtl_drop_queued_tx ( tp ); <nl> rtl_stop_rx ( tp ); <nl> } else {
static ssize_t spufs_mbox_read ( struct file * file , char __user * buf , <nl> udata = ( void __user *) buf ; <nl>  <nl> spu_acquire ( ctx ); <nl> - for ( count = 0 ; count <= len ; count += 4 , udata ++) { <nl> + for ( count = 0 ; ( count + 4 ) <= len ; count += 4 , udata ++) { <nl> int ret ; <nl> ret = ctx -> ops -> mbox_read ( ctx , & mbox_data ); <nl> if ( ret == 0 )
static int dax_pmem_probe ( struct device * dev ) <nl> nsio = to_nd_namespace_io (& ndns -> dev ); <nl>  <nl> /* parse the ' pfn ' info block via -> rw_bytes */ <nl> - devm_nsio_enable ( dev , nsio ); <nl> + rc = devm_nsio_enable ( dev , nsio ); <nl> + if ( rc ) <nl> + return rc ; <nl> altmap = nvdimm_setup_pfn ( nd_pfn , & res , & __altmap ); <nl> if ( IS_ERR ( altmap )) <nl> return PTR_ERR ( altmap );
static int be_resume ( struct pci_dev * pdev ) <nl> pci_set_power_state ( pdev , PCI_D0 ); <nl> pci_restore_state ( pdev ); <nl>  <nl> + status = be_fw_wait_ready ( adapter ); <nl> + if ( status ) <nl> + return status ; <nl> + <nl> /* tell fw we ' re ready to fire cmds */ <nl> status = be_cmd_fw_init ( adapter ); <nl> if ( status )
relookup : <nl> p -> rate_last = 0 ; <nl> p -> pmtu_expires = 0 ; <nl> p -> pmtu_orig = 0 ; <nl> + p -> redirect_genid = 0 ; <nl> memset (& p -> redirect_learned , 0 , sizeof ( p -> redirect_learned )); <nl>  <nl> 
ieee80211softmac_assoc_req ( struct ieee80211_assoc_request ** pkt , <nl> return 0 ; <nl> ieee80211softmac_hdr_3addr ( mac , &((* pkt )-> header ), IEEE80211_STYPE_ASSOC_REQ , net -> bssid , net -> bssid ); <nl>  <nl> + /* Fill in the capabilities */ <nl> + (* pkt )-> capability = ieee80211softmac_capabilities ( mac , net ); <nl> + <nl> /* Fill in Listen Interval (?) */ <nl> (* pkt )-> listen_interval = cpu_to_le16 ( 10 ); <nl> 
static int i2s_startup ( struct snd_pcm_substream * substream , <nl> /* Enforce set_sysclk in Master mode */ <nl> i2s -> rclk_srcrate = 0 ; <nl>  <nl> + if (! any_active ( i2s ) && ( i2s -> quirks & QUIRK_NEED_RSTCLR )) <nl> + writel ( CON_RSTCLR , i2s -> addr + I2SCON ); <nl> + <nl> spin_unlock_irqrestore (& lock , flags ); <nl>  <nl> return 0 ;
int mxc_initialize_usb_hw ( int port , unsigned int flags ) <nl> # ifdef CONFIG_ARCH_MX51 <nl> if ( cpu_is_mx51 ()) { <nl> void __iomem * usb_base ; <nl> - u32 usbotg_base ; <nl> - u32 usbother_base ; <nl> + void __iomem * usbotg_base ; <nl> + void __iomem * usbother_base ; <nl> int ret = 0 ; <nl>  <nl> usb_base = ioremap ( MX51_OTG_BASE_ADDR , SZ_4K );
int iwl_mvm_scan_request ( struct iwl_mvm * mvm , <nl> basic_ssid ? 1 : 0 ); <nl>  <nl> cmd -> tx_cmd . tx_flags = cpu_to_le32 ( TX_CMD_FLG_SEQ_CTL | <nl> - TX_CMD_FLG_BT_DIS ); <nl> + 3 << TX_CMD_FLG_BT_PRIO_POS ); <nl> + <nl> cmd -> tx_cmd . sta_id = mvm -> aux_sta . sta_id ; <nl> cmd -> tx_cmd . life_time = cpu_to_le32 ( TX_CMD_LIFE_TIME_INFINITE ); <nl> cmd -> tx_cmd . rate_n_flags =
skl_compute_ddb ( struct drm_atomic_state * state ) <nl> ret = skl_allocate_pipe_ddb ( cstate , ddb ); <nl> if ( ret ) <nl> return ret ; <nl> + <nl> + ret = drm_atomic_add_affected_planes ( state , & intel_crtc -> base ); <nl> + if ( ret ) <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static int fsg_setup ( struct usb_function * f , <nl> *( u8 *) req -> buf = fsg -> common -> nluns - 1 ; <nl>  <nl> /* Respond with data / status */ <nl> - req -> length = min ( 1 , w_length ); <nl> + req -> length = min (( u16 ) 1 , w_length ); <nl> fsg -> common -> ep0req_name = <nl> ctrl -> bRequestType & USB_DIR_IN ? " ep0 - in " : " ep0 - out "; <nl> return ep0_queue ( fsg -> common );
static int ibmvfc_get_err_result ( struct ibmvfc_cmd * vfc_cmd ) <nl> int fc_rsp_len = rsp -> fcp_rsp_len ; <nl>  <nl> if (( rsp -> flags & FCP_RSP_LEN_VALID ) && <nl> - ((! fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> + (( fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> rsp -> data . info . rsp_code )) <nl> return DID_ERROR << 16 ; <nl> 
static int sep_prepare_input_output_dma_table_in_dcb ( struct sep_device * sep , <nl> } <nl> } <nl> if ( tail_size ) { <nl> + if ( tail_size > sizeof ( dcb_table_ptr -> tail_data )) <nl> + return - EINVAL ; <nl> if ( is_kva == true ) { <nl> memcpy ( dcb_table_ptr -> tail_data , <nl> ( void *)( app_in_address + data_in_size -
static const char * sata_pmp_spec_rev_str ( const u32 * gscr ) <nl> { <nl> u32 rev = gscr [ SATA_PMP_GSCR_REV ]; <nl>  <nl> + if ( rev & ( 1 << 3 )) <nl> + return " 1 . 2 "; <nl> if ( rev & ( 1 << 2 )) <nl> return " 1 . 1 "; <nl> if ( rev & ( 1 << 1 ))
static int of_mpc8xxx_spi_get_chipselects ( struct device * dev ) <nl> gpio = of_get_gpio_flags ( np , i , & flags ); <nl> if (! gpio_is_valid ( gpio )) { <nl> dev_err ( dev , " invalid gpio #% d : % d \ n ", i , gpio ); <nl> + ret = gpio ; <nl> goto err_loop ; <nl> } <nl> 
static struct kvmppc_linear_info * kvm_alloc_linear ( int type ) <nl> break ; <nl> } <nl> spin_unlock (& linear_lock ); <nl> + memset ( ri -> base_virt , 0 , ri -> npages << PAGE_SHIFT ); <nl> return ri ; <nl> } <nl> 
static int receive_DataRequest ( struct drbd_conf * mdev , enum drbd_packets cmd , un <nl> " no local data .\ n "); <nl> drbd_send_ack_rp ( mdev , cmd == P_DATA_REQUEST ? P_NEG_DREPLY : <nl> P_NEG_RS_DREPLY , p ); <nl> - return TRUE ; <nl> + /* drain possibly payload */ <nl> + return drbd_drain_block ( mdev , digest_size ); <nl> } <nl>  <nl> /* GFP_NOIO , because we must not cause arbitrary write - out : in a DRBD
mwifiex_wmm_get_highest_priolist_ptr ( struct mwifiex_adapter * adapter , <nl> list_for_each_entry ( ptr , & tid_ptr -> ra_list , <nl> list ) { <nl>  <nl> - if (! skb_queue_empty (& ptr -> skb_head )) <nl> + if (! ptr -> tx_paused && <nl> + ! skb_queue_empty (& ptr -> skb_head )) <nl> /* holds both locks */ <nl> goto found ; <nl> }
void radeon_atom_backlight_init ( struct radeon_encoder * radeon_encoder , <nl> u8 backlight_level ; <nl> char bl_name [ 16 ]; <nl>  <nl> + /* Mac laptops with multiple GPUs use the gmux driver for backlight <nl> + * so don ' t register a backlight device <nl> + */ <nl> + if (( rdev -> pdev -> subsystem_vendor == PCI_VENDOR_ID_APPLE ) && <nl> + ( rdev -> pdev -> device == 0x6741 )) <nl> + return ; <nl> + <nl> if (! radeon_encoder -> enc_priv ) <nl> return ; <nl> 
static int dvb_register ( struct cx23885_tsport * port ) <nl>  <nl> fe = dvb_attach ( xc4000_attach , fe0 -> dvb . frontend , <nl> & dev -> i2c_bus [ 1 ]. i2c_adap , & cfg ); <nl> + if (! fe ) { <nl> + printk ( KERN_ERR "% s / 2 : xc4000 attach failed \ n ", <nl> + dev -> name ); <nl> + goto frontend_detach ; <nl> + } <nl> } <nl> break ; <nl> case CX23885_BOARD_TBS_6920 :
static void reset_connection ( struct ceph_connection * con ) <nl> con -> out_msg = NULL ; <nl> } <nl> con -> in_seq = 0 ; <nl> + con -> in_seq_acked = 0 ; <nl> } <nl>  <nl> /*
int snd_hda_parse_pin_def_config ( struct hda_codec * codec , <nl> memmove ( sequences_hp + i , sequences_hp + i + 1 , <nl> sizeof ( sequences_hp [ 0 ]) * ( cfg -> hp_outs - i )); <nl> } <nl> + memset ( cfg -> hp_pins + cfg -> hp_outs , 0 , <nl> + sizeof ( hda_nid_t ) * ( AUTO_CFG_MAX_OUTS - cfg -> hp_outs )); <nl> } <nl>  <nl> /* sort by sequence */
static unsigned int xuartps_set_baud_rate ( struct uart_port * port , <nl> unsigned int baud ) <nl> { <nl> unsigned int calc_baud ; <nl> - u32 cd , bdiv ; <nl> + u32 cd = 0 , bdiv = 0 ; <nl> u32 mreg ; <nl> int div8 ; <nl> struct xuartps * xuartps = port -> private_data ;
static int ac97_write ( struct snd_soc_codec * codec , unsigned int reg , <nl> { <nl> u16 * cache = codec -> reg_cache ; <nl>  <nl> - soc_ac97_ops . write ( codec -> ac97 , reg , val ); <nl> + if ( reg < 0x7c ) <nl> + soc_ac97_ops . write ( codec -> ac97 , reg , val ); <nl> reg = reg >> 1 ; <nl> if ( reg < ( ARRAY_SIZE ( wm9712_reg ))) <nl> cache [ reg ] = val ;
struct fib6_node * fib6_lookup ( struct fib6_node * root , struct in6_addr * daddr , <nl> } <nl> }; <nl>  <nl> - fn = fib6_lookup_1 ( root , args ); <nl> + fn = fib6_lookup_1 ( root , daddr ? args : args + 1 ); <nl>  <nl> if ( fn == NULL || fn -> fn_flags & RTN_TL_ROOT ) <nl> fn = root ;
extern void __qdisc_run ( struct Qdisc * q ); <nl>  <nl> static inline void qdisc_run ( struct Qdisc * q ) <nl> { <nl> - if (! test_and_set_bit ( __QDISC_STATE_RUNNING , & q -> state )) <nl> + struct netdev_queue * txq = q -> dev_queue ; <nl> + <nl> + if (! netif_tx_queue_stopped ( txq ) && <nl> + ! test_and_set_bit ( __QDISC_STATE_RUNNING , & q -> state )) <nl> __qdisc_run ( q ); <nl> } <nl> 
static int w1_f29_add_slave ( struct w1_slave * sl ) <nl> static void w1_f29_remove_slave ( struct w1_slave * sl ) <nl> { <nl> int i ; <nl> - for ( i = NB_SYSFS_BIN_FILES ; i <= 0 ; -- i ) <nl> + for ( i = NB_SYSFS_BIN_FILES - 1 ; i >= 0 ; -- i ) <nl> sysfs_remove_bin_file (& sl -> dev . kobj , <nl> &( w1_f29_sysfs_bin_files [ i ])); <nl> }
static int btrfs_finish_ordered_io ( struct btrfs_ordered_extent * ordered_extent ) <nl> EXTENT_DEFRAG , 1 , cached_state ); <nl> if ( ret ) { <nl> u64 last_snapshot = btrfs_root_last_snapshot (& root -> root_item ); <nl> - if ( last_snapshot >= BTRFS_I ( inode )-> generation ) <nl> + if ( 0 && last_snapshot >= BTRFS_I ( inode )-> generation ) <nl> /* the inode is shared */ <nl> new = record_old_file_extents ( inode , ordered_extent ); <nl> 
static int alloc_iommu ( struct dmar_drhd_unit * drhd ) <nl> intel_iommu_groups , <nl> "% s ", iommu -> name ); <nl>  <nl> + if ( IS_ERR ( iommu -> iommu_dev )) { <nl> + drhd -> iommu = NULL ; <nl> + err = PTR_ERR ( iommu -> iommu_dev ); <nl> + goto err_unmap ; <nl> + } <nl> + <nl> return 0 ; <nl>  <nl> err_unmap :
static int device_change_notifier ( struct notifier_block * nb , <nl> case BUS_NOTIFY_UNBOUND_DRIVER : <nl> if (! domain ) <nl> goto out ; <nl> + if ( iommu_pass_through ) <nl> + break ; <nl> detach_device ( domain , devid ); <nl> break ; <nl> case BUS_NOTIFY_ADD_DEVICE :
static int __of_iio_channel_get ( struct iio_channel * channel , <nl> channel -> indio_dev = indio_dev ; <nl> index = iiospec . args_count ? iiospec . args [ 0 ] : 0 ; <nl> if ( index >= indio_dev -> num_channels ) { <nl> - return - EINVAL ; <nl> + err = - EINVAL ; <nl> goto err_put ; <nl> } <nl> channel -> channel = & indio_dev -> channels [ index ];
static void i40evf_watchdog_task ( struct work_struct * work ) <nl> watchdog_done : <nl> clear_bit ( __I40EVF_IN_CRITICAL_TASK , & adapter -> crit_section ); <nl> restart_watchdog : <nl> + if ( adapter -> state == __I40EVF_REMOVE ) <nl> + return ; <nl> if ( adapter -> aq_required ) <nl> mod_timer (& adapter -> watchdog_timer , <nl> jiffies + msecs_to_jiffies ( 20 ));
static int uas_find_endpoints ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < n_endpoints ; i ++) { <nl> unsigned char * extra = endpoint [ i ]. extra ; <nl> int len = endpoint [ i ]. extralen ; <nl> - while ( len > 1 ) { <nl> + while ( len >= 3 ) { <nl> if ( extra [ 1 ] == USB_DT_PIPE_USAGE ) { <nl> unsigned pipe_id = extra [ 2 ]; <nl> if ( pipe_id > 0 && pipe_id < 5 )
static int skl_probe ( struct pci_dev * pci , <nl> if ( err < 0 ) <nl> goto out_free ; <nl>  <nl> + skl -> nhlt = skl_nhlt_init ( bus -> dev ); <nl> + <nl> + if ( skl -> nhlt == NULL ) <nl> + goto out_free ; <nl> + <nl> pci_set_drvdata ( skl -> pci , ebus ); <nl>  <nl> /* check if dsp is there */
static void intel_find_plane_obj ( struct intel_crtc * intel_crtc , <nl> return ; <nl>  <nl> kfree ( intel_crtc -> base . fb ); <nl> + intel_crtc -> base . fb = NULL ; <nl>  <nl> /* <nl> * Failed to alloc the obj , check to see if we should share
static int AdvLoadMicrocode ( AdvPortAddr iop_base , unsigned char * buf , int size , <nl> i += 2 ; <nl> len += 2 ; <nl> } else { <nl> - unsigned char off = buf [ i ] * 2 ; <nl> + unsigned int off = buf [ i ] * 2 ; <nl> unsigned short word = ( buf [ off + 1 ] << 8 ) | buf [ off ]; <nl> AdvWriteWordAutoIncLram ( iop_base , word ); <nl> len += 2 ;
static void alloc_mem ( void ** dst , void ** src , size_t length ) <nl> * src = zalloc ( length ); <nl> if (!* src ) <nl> die (" memory allocation failed - maybe length is too large ?\ n "); <nl> + /* Make sure to always replace the zero pages even if MMAP_THRESH is crossed */ <nl> + memset (* src , 0 , length ); <nl> } <nl>  <nl> static u64 do_memcpy_cycle ( memcpy_t fn , size_t len , bool prefault )
static void __init setup_hwcaps ( void ) <nl> void __init <nl> setup_arch ( char ** cmdline_p ) <nl> { <nl> + /* set up preferred console */ <nl> + add_preferred_console (" ttyS ", 0 , NULL ); <nl> + <nl> /* <nl> * print what head . S has found out about the machine <nl> */
static int vhost_scsi_set_endpoint ( <nl> /* Flushing the vhost_work acts as synchronize_rcu */ <nl> mutex_lock (& vq -> mutex ); <nl> rcu_assign_pointer ( vq -> private_data , vs_tpg ); <nl> + vhost_init_used ( vq ); <nl> mutex_unlock (& vq -> mutex ); <nl> } <nl> ret = 0 ;
static int dw_msi_setup_irq ( struct msi_controller * chip , struct pci_dev * pdev , <nl> struct msi_msg msg ; <nl> struct pcie_port * pp = sys_to_pcie ( pdev -> bus -> sysdata ); <nl>  <nl> + if ( desc -> msi_attrib . is_msix ) <nl> + return - EINVAL ; <nl> + <nl> irq = assign_irq ( 1 , desc , & pos ); <nl> if ( irq < 0 ) <nl> return irq ;
static void __init vpac270_onenand_init ( void ) {} <nl> # if defined ( CONFIG_MMC_PXA ) || defined ( CONFIG_MMC_PXA_MODULE ) <nl> static struct pxamci_platform_data vpac270_mci_platform_data = { <nl> . ocr_mask = MMC_VDD_32_33 | MMC_VDD_33_34 , <nl> + . gpio_power = - 1 , <nl> . gpio_card_detect = GPIO53_VPAC270_SD_DETECT_N , <nl> . gpio_card_ro = GPIO52_VPAC270_SD_READONLY , <nl> . detect_delay_ms = 200 ,
BYTE byPwr = pDevice -> byCCKPwr ; <nl> return TRUE ; <nl> } <nl>  <nl> + if ( uCH == 0 ) <nl> + return - EINVAL ; <nl> + <nl> switch ( uRATE ) { <nl> case RATE_1M : <nl> case RATE_2M :
static int alc_cap_getput_caller ( struct snd_kcontrol * kcontrol , <nl> { <nl> struct hda_codec * codec = snd_kcontrol_chip ( kcontrol ); <nl> struct alc_spec * spec = codec -> spec ; <nl> - int i , err ; <nl> + int i , err = 0 ; <nl>  <nl> mutex_lock (& codec -> control_mutex ); <nl> if ( check_adc_switch && spec -> dual_adc_switch ) {
static void acm_tty_flush_chars ( struct tty_struct * tty ) <nl> int err ; <nl> unsigned long flags ; <nl>  <nl> + if (! cur ) /* nothing to do */ <nl> + return ; <nl> + <nl> acm -> putbuffer = NULL ; <nl> err = usb_autopm_get_interface_async ( acm -> control ); <nl> spin_lock_irqsave (& acm -> write_lock , flags ); <nl> if ( err < 0 ) { <nl> cur -> use = 0 ; <nl> + acm -> putbuffer = cur ; <nl> goto out ; <nl> } <nl> 
static int digi_read_oob_callback ( struct urb * urb ) <nl> return - 1 ; <nl>  <nl> /* handle each oob command */ <nl> - for ( i = 0 ; i < urb -> actual_length - 4 ; i += 4 ) { <nl> + for ( i = 0 ; i < urb -> actual_length - 3 ; i += 4 ) { <nl> opcode = buf [ i ]; <nl> line = buf [ i + 1 ]; <nl> status = buf [ i + 2 ];
parse_dcb20_entry ( struct drm_device * dev , struct dcb_table * dcb , <nl> case 0 : <nl> entry -> dpconf . link_bw = 162000 ; <nl> break ; <nl> - default : <nl> + case 1 : <nl> entry -> dpconf . link_bw = 270000 ; <nl> break ; <nl> + default : <nl> + entry -> dpconf . link_bw = 540000 ; <nl> + break ; <nl> } <nl> switch (( conf & 0x0f000000 ) >> 24 ) { <nl> case 0xf :
static int econet_sendmsg ( struct kiocb * iocb , struct socket * sock , <nl> udpdest . sin_addr . s_addr = htonl ( network | addr . station ); <nl> } <nl>  <nl> + memset (& ah , 0 , sizeof ( ah )); <nl> ah . port = port ; <nl> ah . cb = cb & 0x7f ; <nl> ah . code = 2 ; /* magic */ <nl> - ah . pad = 0 ; <nl>  <nl> /* tack our header on the front of the iovec */ <nl> size = sizeof ( struct aunhdr );
static void scrub_print_warning ( const char * errstr , struct scrub_block * sblock ) <nl> u64 flags = 0 ; <nl> u64 ref_root ; <nl> u32 item_size ; <nl> - u8 ref_level ; <nl> + u8 ref_level = 0 ; <nl> int ret ; <nl>  <nl> WARN_ON ( sblock -> page_count < 1 );
static int ucc_hdlc_probe ( struct platform_device * pdev ) <nl>  <nl> err_hdlc_init : <nl> err_miss_tsa_property : <nl> - kfree ( uhdlc_priv ); <nl> if ( uhdlc_priv -> tsa ) <nl> kfree ( utdm ); <nl> err_alloc_utdm :
static int sctp_send_asconf_del_ip ( struct sock * sk , <nl> continue ; <nl> asoc -> asconf_addr_del_pending = <nl> kzalloc ( sizeof ( union sctp_addr ), GFP_ATOMIC ); <nl> + if ( asoc -> asconf_addr_del_pending == NULL ) { <nl> + retval = - ENOMEM ; <nl> + goto out ; <nl> + } <nl> asoc -> asconf_addr_del_pending -> sa . sa_family = <nl> addrs -> sa_family ; <nl> asoc -> asconf_addr_del_pending -> v4 . sin_port =
static void line6_stream_stop ( struct snd_line6_pcm * line6pcm , int direction , <nl> spin_lock_irqsave (& pstr -> lock , flags ); <nl> clear_bit ( type , & pstr -> running ); <nl> if (! pstr -> running ) { <nl> + spin_unlock_irqrestore (& pstr -> lock , flags ); <nl> line6_unlink_audio_urbs ( line6pcm , pstr ); <nl> + spin_lock_irqsave (& pstr -> lock , flags ); <nl> if ( direction == SNDRV_PCM_STREAM_CAPTURE ) { <nl> line6pcm -> prev_fbuf = NULL ; <nl> line6pcm -> prev_fsize = 0 ;
static void rockchip_drm_unbind ( struct device * dev ) <nl> rockchip_drm_fbdev_fini ( drm_dev ); <nl> drm_kms_helper_poll_fini ( drm_dev ); <nl>  <nl> + drm_atomic_helper_shutdown ( drm_dev ); <nl> drm_vblank_cleanup ( drm_dev ); <nl> component_unbind_all ( dev , drm_dev ); <nl> drm_mode_config_cleanup ( drm_dev );
enum mtd_file_modes { <nl> MTD_FILE_MODE_RAW , <nl> }; <nl>  <nl> + static inline int mtd_type_is_nand_user ( const struct mtd_info_user * mtd ) <nl> +{ <nl> + return mtd -> type == MTD_NANDFLASH || mtd -> type == MTD_MLCNANDFLASH ; <nl> +} <nl> + <nl> # endif /* __MTD_ABI_H__ */
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> fail_workqueue : <nl> wiphy_unregister ( local -> hw . wiphy ); <nl> fail_wiphy_register : <nl> - kfree ( local -> int_scan_req -> channels ); <nl> + kfree ( local -> int_scan_req ); <nl> return result ; <nl> } <nl> EXPORT_SYMBOL ( ieee80211_register_hw );
qlcnic_setup_netdev ( struct qlcnic_adapter * adapter , struct net_device * netdev , <nl> if ( err ) <nl> return err ; <nl>  <nl> + qlcnic_dcb_init_dcbnl_ops ( adapter -> dcb ); <nl> + <nl> err = register_netdev ( netdev ); <nl> if ( err ) { <nl> dev_err (& pdev -> dev , " failed to register net device \ n "); <nl> return err ; <nl> } <nl>  <nl> - qlcnic_dcb_init_dcbnl_ops ( adapter -> dcb ); <nl> - <nl> return 0 ; <nl> } <nl> 
u16 hpi_entity_alloc_and_pack ( const enum e_entity_type type , <nl> if ( hE ) <nl> return hE ; <nl>  <nl> - HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_ROLE ); <nl> + HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_TYPE ); <nl>  <nl> bytes_to_copy = entity_type_to_size [ type ] * item_count ; <nl> total_size = hpi_entity_header_size (* entity ) + bytes_to_copy ;
void cfg80211_conn_work ( struct work_struct * work ) <nl> wdev_unlock ( wdev ); <nl> continue ; <nl> } <nl> - if ( wdev -> sme_state != CFG80211_SME_CONNECTING ) { <nl> + if ( wdev -> sme_state != CFG80211_SME_CONNECTING || ! wdev -> conn ) { <nl> wdev_unlock ( wdev ); <nl> continue ; <nl> }
static struct platform_driver td_driver = { <nl> . owner = THIS_MODULE , <nl> }, <nl> . probe = td_probe , <nl> - . remove = __exit_p ( td_remove ), <nl> + . remove = td_remove , <nl> }; <nl>  <nl> module_platform_driver ( td_driver );
int mmc_send_app_op_cond ( struct mmc_host * host , u32 ocr , u32 * rocr ) <nl> mmc_delay ( 10 ); <nl> } <nl>  <nl> + if (! i ) <nl> + pr_err ("% s : card never left busy state \ n ", mmc_hostname ( host )); <nl> + <nl> if ( rocr && ! mmc_host_is_spi ( host )) <nl> * rocr = cmd . resp [ 0 ]; <nl> 
static void carl9170_ps_beacon ( struct ar9170 * ar , void * data , unsigned int len ) <nl> cam = ieee80211_check_tim ( tim_ie , tim_len , ar -> common . curaid ); <nl>  <nl> /* 2 . Maybe the AP wants to send multicast / broadcast data ? */ <nl> - cam = !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl> + cam |= !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl>  <nl> if (! cam ) { <nl> /* back to low - power land . */
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> int bios_reserved = 0 ; <nl>  <nl> + if ( dev_priv -> gtt . stolen_size == 0 ) <nl> + return 0 ; <nl> + <nl> dev_priv -> mm . stolen_base = i915_stolen_to_physical ( dev ); <nl> if ( dev_priv -> mm . stolen_base == 0 ) <nl> return 0 ;
nv130_chipset = { <nl> static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> + . bios = nvkm_bios_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
static struct wiphy * wlan_create_wiphy ( struct device * dev , wlandevice_t * wlandev <nl> wiphy -> n_cipher_suites = PRISM2_NUM_CIPHER_SUITES ; <nl> wiphy -> cipher_suites = prism2_cipher_suites ; <nl>  <nl> - if ( wiphy_register ( wiphy ) < 0 ) <nl> + if ( wiphy_register ( wiphy ) < 0 ) { <nl> + wiphy_free ( wiphy ); <nl> return NULL ; <nl> + } <nl>  <nl> return wiphy ; <nl> }
long amdgpu_fence_wait_seq_timeout ( struct amdgpu_device * adev , u64 * target_seq , <nl> bool signaled ; <nl> int i , r ; <nl>  <nl> + if ( timeout == 0 ) { <nl> + return amdgpu_fence_any_seq_signaled ( adev , target_seq ); <nl> + } <nl> + <nl> while (! amdgpu_fence_any_seq_signaled ( adev , target_seq )) { <nl>  <nl> /* Save current sequence values , used to check for GPU lockups */
ecryptfs_add_global_auth_tok ( struct ecryptfs_mount_crypt_stat * mount_crypt_stat , <nl> struct ecryptfs_global_auth_tok * new_auth_tok ; <nl> int rc = 0 ; <nl>  <nl> - new_auth_tok = kmem_cache_alloc ( ecryptfs_global_auth_tok_cache , <nl> + new_auth_tok = kmem_cache_zalloc ( ecryptfs_global_auth_tok_cache , <nl> GFP_KERNEL ); <nl> if (! new_auth_tok ) { <nl> rc = - ENOMEM ;
asmlinkage long sys_oabi_semtimedop ( int semid , <nl> long err ; <nl> int i ; <nl>  <nl> - if ( nsops < 1 ) <nl> + if ( nsops < 1 || nsops > SEMOPM ) <nl> return - EINVAL ; <nl> sops = kmalloc ( sizeof (* sops ) * nsops , GFP_KERNEL ); <nl> if (! sops )
static int gfx_v9_0_rlc_resume ( struct amdgpu_device * adev ) <nl> { <nl> int r ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> gfx_v9_0_rlc_stop ( adev ); <nl>  <nl> /* disable CG */
static void pti_tty_cleanup ( struct tty_struct * tty ) <nl> if ( pti_tty_data == NULL ) <nl> return ; <nl> pti_release_masterchannel ( pti_tty_data -> mc ); <nl> - kfree ( tty -> driver_data ); <nl> + kfree ( pti_tty_data ); <nl> tty -> driver_data = NULL ; <nl> } <nl> 
void __init tegra_add_of_provider ( struct device_node * np ) <nl> of_clk_add_provider ( np , of_clk_src_onecell_get , & clk_data ); <nl>  <nl> rst_ctlr . of_node = np ; <nl> - rst_ctlr . nr_resets = clk_num * 32 ; <nl> + rst_ctlr . nr_resets = periph_banks * 32 ; <nl> reset_controller_register (& rst_ctlr ); <nl> } <nl> 
static int rbd_get_client ( struct rbd_device * rbd_dev , const char * mon_addr , <nl> rbdc = __rbd_client_find ( opt ); <nl> if ( rbdc ) { <nl> ceph_destroy_options ( opt ); <nl> + kfree ( rbd_opts ); <nl>  <nl> /* using an existing client */ <nl> kref_get (& rbdc -> kref );
static inline void print_ipv6_addr ( struct audit_buffer * ab , <nl> char * name1 , char * name2 ) <nl> { <nl> if (! ipv6_addr_any ( addr )) <nl> - audit_log_format ( ab , " % s =% pI6 ", name1 , addr ); <nl> + audit_log_format ( ab , " % s =% pI6c ", name1 , addr ); <nl> if ( port ) <nl> audit_log_format ( ab , " % s =% d ", name2 , ntohs ( port )); <nl> }
static ssize_t sn2_ptc_proc_write ( struct file * file , const char __user * user , si <nl> int cpu ; <nl> char optstr [ 64 ]; <nl>  <nl> + if ( count > sizeof ( optstr )) <nl> + return - EINVAL ; <nl> if ( copy_from_user ( optstr , user , count )) <nl> return - EFAULT ; <nl> optstr [ count - 1 ] = '\ 0 ';
static int moxart_gpio_probe ( struct platform_device * pdev ) <nl> gc -> parent = dev ; <nl> gc -> owner = THIS_MODULE ; <nl>  <nl> - ret = gpiochip_add_data ( gc , NULL ); <nl> + ret = devm_gpiochip_add_data ( dev , gc , NULL ); <nl> if ( ret ) { <nl> dev_err ( dev , "% s : gpiochip_add failed \ n ", <nl> dev -> of_node -> full_name );
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
LNetCtl ( unsigned int cmd , void * arg ) <nl> int rc ; <nl> unsigned long secs_passed ; <nl>  <nl> + BUILD_BUG_ON ( LIBCFS_IOC_DATA_MAX < <nl> + sizeof ( struct lnet_ioctl_net_config ) + <nl> + sizeof ( struct lnet_ioctl_config_data )); <nl> + <nl> switch ( cmd ) { <nl> case IOC_LIBCFS_GET_NI : <nl> rc = LNetGetId ( data -> ioc_count , & id );
static void WILC_WFI_mon_setup ( struct net_device * dev ) <nl> ether_setup ( dev ); <nl> dev -> tx_queue_len = 0 ; <nl> dev -> type = ARPHRD_IEEE80211_RADIOTAP ; <nl> - memset ( dev -> dev_addr , 0 , ETH_ALEN ); <nl> + eth_zero_addr ( dev -> dev_addr ); <nl>  <nl> # ifdef USE_WIRELESS <nl> {
static struct sysfs_dirent * sysfs_new_dirent ( struct sysfs_dirent * parent_sd , <nl>  <nl> memset ( sd , 0 , sizeof (* sd )); <nl> atomic_set (& sd -> s_count , 1 ); <nl> - atomic_set (& sd -> s_event , 0 ); <nl> + atomic_set (& sd -> s_event , 1 ); <nl> INIT_LIST_HEAD (& sd -> s_children ); <nl> list_add (& sd -> s_sibling , & parent_sd -> s_children ); <nl> sd -> s_element = element ;
static int vidioc_s_register ( struct file * file , void * priv , <nl> 0x02 , <nl> ( u16 ) reg -> reg , 1 , <nl> value , 1 , 2 ); <nl> + break ; <nl> case 0x322 : <nl> ret = <nl> cx231xx_write_i2c_master ( dev ,
static int move_tasks ( struct rq * this_rq , int this_cpu , struct rq * busiest , <nl> max_load_move - total_load_moved , <nl> sd , idle , all_pinned , & this_best_prio ); <nl> class = class -> next ; <nl> + <nl> + if ( idle == CPU_NEWLY_IDLE && this_rq -> nr_running ) <nl> + break ; <nl> + <nl> } while ( class && max_load_move > total_load_moved ); <nl>  <nl> return total_load_moved > 0 ;
err_regulator_enable : <nl> err_regulator_get : <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8731 -> supplies ), wm8731 -> supplies ); <nl>  <nl> - kfree ( wm8731 ); <nl> return ret ; <nl> } <nl> 
static bool davinci_spi_can_dma ( struct spi_master * master , <nl>  <nl> if ( spicfg ) <nl> can_dma = ( spicfg -> io_type == SPI_IO_TYPE_DMA ) && <nl> - ( xfer -> len >= DMA_MIN_BYTES ); <nl> + ( xfer -> len >= DMA_MIN_BYTES ) && <nl> + ! is_vmalloc_addr ( xfer -> rx_buf ) && <nl> + ! is_vmalloc_addr ( xfer -> tx_buf ); <nl>  <nl> return can_dma ; <nl> }
enum stb0899_status stb0899_dvbs2_algo ( struct stb0899_state * state ) <nl> else <nl> internal -> inversion = IQ_SWAP_OFF ; <nl>  <nl> - offsetfreq *= internal -> inversion ; <nl> - <nl> - internal -> freq = internal -> freq - offsetfreq ; <nl> + internal -> freq = internal -> freq + offsetfreq ; <nl> internal -> srate = stb0899_dvbs2_get_srate ( state ); <nl>  <nl> reg = STB0899_READ_S2REG ( STB0899_S2DEMOD , UWP_STAT2 );
static int ad1836_register ( struct ad1836_priv * ad1836 ) <nl>  <nl> if ( ad1836_codec ) { <nl> dev_err ( codec -> dev , " Another ad1836 is registered \ n "); <nl> + kfree ( ad1836 ); <nl> return - EINVAL ; <nl> } <nl> 
static int get_sb_mtd_aux ( struct file_system_type * fs_type , int flags , <nl> DEBUG ( 1 , " MTDSB : New superblock for device % d (\"% s \")\ n ", <nl> mtd -> index , mtd -> name ); <nl>  <nl> + sb -> s_flags = flags ; <nl> + <nl> ret = fill_super ( sb , data , flags & MS_SILENT ? 1 : 0 ); <nl> if ( ret < 0 ) { <nl> up_write (& sb -> s_umount );
static int do_fault_around ( struct fault_env * fe , pgoff_t start_pgoff ) <nl>  <nl> if ( pmd_none (* fe -> pmd )) { <nl> fe -> prealloc_pte = pte_alloc_one ( fe -> vma -> vm_mm , fe -> address ); <nl> + if (! fe -> prealloc_pte ) <nl> + goto out ; <nl> smp_wmb (); /* See comment in __pte_alloc () */ <nl> } <nl> 
static void iwlagn_bt_traffic_change_work ( struct work_struct * work ) <nl> priv -> cfg -> ops -> lib -> update_chain_flags ( priv ); <nl>  <nl> if ( smps_request != - 1 ) { <nl> + priv -> current_ht_config . smps = smps_request ; <nl> for_each_context ( priv , ctx ) { <nl> if ( ctx -> vif && ctx -> vif -> type == NL80211_IFTYPE_STATION ) <nl> ieee80211_request_smps ( ctx -> vif , smps_request );
static int __init caam_rng_init ( void ) <nl> rng_ctx = kmalloc ( sizeof ( struct caam_rng_ctx ), GFP_DMA ); <nl> if (! rng_ctx ) <nl> return - ENOMEM ; <nl> - caam_init_rng (& rng_ctx , dev ); <nl> + caam_init_rng ( rng_ctx , dev ); <nl>  <nl> dev_info ( dev , " registering rng - caam \ n "); <nl> return hwrng_register (& caam_rng );
static int snd_timer_user_params ( struct file * file , <nl> if ( tu -> timeri -> flags & SNDRV_TIMER_IFLG_EARLY_EVENT ) { <nl> if ( tu -> tread ) { <nl> struct snd_timer_tread tread ; <nl> + memset (& tread , 0 , sizeof ( tread )); <nl> tread . event = SNDRV_TIMER_EVENT_EARLY ; <nl> tread . tstamp . tv_sec = 0 ; <nl> tread . tstamp . tv_nsec = 0 ;
static int create_trace_kprobe ( int argc , char ** argv ) <nl> pr_info (" Failed to parse symbol .\ n "); <nl> return ret ; <nl> } <nl> + if ( offset && is_return && <nl> + ! arch_function_offset_within_entry ( offset )) { <nl> + pr_info (" Given offset is not valid for return probe .\ n "); <nl> + return - EINVAL ; <nl> + } <nl> } <nl> argc -= 2 ; argv += 2 ; <nl> 
static int hostap_enable_hostapd ( PSDevice pDevice , int rtnl_locked ) <nl>  <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO "% s : Enabling hostapd mode \ n ", dev -> name ); <nl>  <nl> - pDevice -> apdev = kzalloc ( sizeof ( struct net_device ), GFP_KERNEL ); <nl> + pDevice -> apdev = alloc_etherdev ( sizeof (* apdev_priv )); <nl> if ( pDevice -> apdev == NULL ) <nl> return - ENOMEM ; <nl> 
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> + if ( device_name == NULL ) <nl> + return - 1 ; <nl> + <nl> /* Find the device requested */ <nl> dev_num = find_type_by_name ( device_name , " device "); <nl> if ( dev_num < 0 ) {
*/ <nl> # define AT91_BASE_SYS 0xffffc000 <nl>  <nl> +# endif <nl> + <nl> /* <nl> * On sama5d4 there is no system controller , we map some needed peripherals <nl> */ <nl> # define AT91_ALT_BASE_SYS 0xfc069000 <nl> -# endif <nl>  <nl> /* <nl> * On all at91 have the Advanced Interrupt Controller starts at address <nl> */ <nl> # define AT91_IO_PHYS_BASE AT91_BASE_SYS <nl> # define AT91_IO_VIRT_BASE IOMEM ( AT91_IO_PHYS_BASE ) <nl> + <nl> +# define AT91_ALT_IO_PHYS_BASE AT91_ALT_BASE_SYS <nl> +# define AT91_ALT_IO_VIRT_BASE IOMEM ( AT91_ALT_BASE_SYS ) <nl> # endif <nl>  <nl> # define AT91_IO_SIZE ( 0xFFFFFFFF - AT91_IO_PHYS_BASE + 1 )
qla2xxx_queuecommand ( struct Scsi_Host * host , struct scsi_cmnd * cmd ) <nl> * Return target busy if we ' ve received a non - zero retry_delay_timer <nl> * in a FCP_RSP . <nl> */ <nl> - if ( time_after ( jiffies , fcport -> retry_delay_timestamp )) <nl> + if ( fcport -> retry_delay_timestamp == 0 ) { <nl> + /* retry delay not set */ <nl> + } else if ( time_after ( jiffies , fcport -> retry_delay_timestamp )) <nl> fcport -> retry_delay_timestamp = 0 ; <nl> else <nl> goto qc24_target_busy ;
static int edma_config_pset ( struct dma_chan * chan , struct edmacc_param * pset , <nl> int absync ; <nl>  <nl> acnt = dev_width ; <nl> + <nl> + /* src / dst_maxburst == 0 is the same case as src / dst_maxburst == 1 */ <nl> + if (! burst ) <nl> + burst = 1 ; <nl> /* <nl> * If the maxburst is equal to the fifo width , use <nl> * A - synced transfers . This allows for large contiguous
static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
int ocfs2_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> } <nl>  <nl> if ( size_change && attr -> ia_size != i_size_read ( inode )) { <nl> - if ( attr -> ia_size > sb -> s_maxbytes ) { <nl> - status = - EFBIG ; <nl> + status = inode_newsize_ok ( inode , attr -> ia_size ); <nl> + if ( status ) <nl> goto bail_unlock ; <nl> - } <nl>  <nl> if ( i_size_read ( inode ) > attr -> ia_size ) { <nl> if ( ocfs2_should_order_data ( inode )) {
int tm6000_reset ( struct tm6000_core * dev ) <nl>  <nl> msleep ( 5 ); <nl>  <nl> + /* <nl> + * Not all devices have int_in defined <nl> + */ <nl> + if (! dev -> int_in . endp ) <nl> + return 0 ; <nl> + <nl> err = usb_set_interface ( dev -> udev , dev -> isoc_in . bInterfaceNumber , 2 ); <nl> if ( err < 0 ) { <nl> tm6000_err (" failed to select interface % d , alt . setting 2 \ n ",
static void nvmet_execute_rw ( struct nvmet_req * req ) <nl>  <nl> if ( req -> cmd -> rw . opcode == nvme_cmd_write ) { <nl> op = REQ_OP_WRITE ; <nl> + op_flags = WRITE_ODIRECT ; <nl> if ( req -> cmd -> rw . control & cpu_to_le16 ( NVME_RW_FUA )) <nl> op_flags |= REQ_FUA ; <nl> } else {
err : <nl> */ <nl> void fcoe_ctlr_recv ( struct fcoe_ctlr * fip , struct sk_buff * skb ) <nl> { <nl> + skb = skb_share_check ( skb , GFP_ATOMIC ); <nl> + if (! skb ) <nl> + return ; <nl> skb_queue_tail (& fip -> fip_recv_list , skb ); <nl> schedule_work (& fip -> recv_work ); <nl> }
static ssize_t ab8500_subscribe_write ( struct file * file , <nl> */ <nl> dev_attr [ irq_index ] = kmalloc ( sizeof ( struct device_attribute ), <nl> GFP_KERNEL ); <nl> + if (! dev_attr [ irq_index ]) <nl> + return - ENOMEM ; <nl> + <nl> event_name [ irq_index ] = kmalloc ( count , GFP_KERNEL ); <nl> sprintf ( event_name [ irq_index ], "% lu ", user_val ); <nl> dev_attr [ irq_index ]-> show = show_irq ;
int blkdev_ioctl ( struct block_device * bdev , fmode_t mode , unsigned cmd , <nl> * We need to set the startsect first , the driver may <nl> * want to override it . <nl> */ <nl> + memset (& geo , 0 , sizeof ( geo )); <nl> geo . start = get_start_sect ( bdev ); <nl> ret = disk -> fops -> getgeo ( bdev , & geo ); <nl> if ( ret )
module_exit ( visornic_cleanup ); <nl>  <nl> MODULE_AUTHOR (" Unisys "); <nl> MODULE_LICENSE (" GPL "); <nl> - MODULE_DESCRIPTION (" sPAR nic driver for sparlinux : ver 1 . 0 . 0 . 0 "); <nl> - MODULE_VERSION (" 1 . 0 . 0 . 0 "); <nl> + MODULE_DESCRIPTION (" sPAR nic driver for sparlinux ");
cntrlEnd : <nl>  <nl> BCM_DEBUG_PRINT ( Adapter , DBG_TYPE_OTHERS , OSAL_DBG , DBG_LVL_ALL , " Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO \ n "); <nl>  <nl> + memset (& DevInfo , 0 , sizeof ( DevInfo )); <nl> DevInfo . MaxRDMBufferSize = BUFFER_4K ; <nl> DevInfo . u32DSDStartOffset = EEPROM_CALPARAM_START ; <nl> DevInfo . u32RxAlignmentCorrection = 0 ;
struct hisi_clock_data * hisi_clk_init ( struct device_node * np , <nl> goto err ; <nl> } <nl> clk_data -> base = base ; <nl> - <nl> - clk_table = kzalloc ( sizeof ( struct clk *) * nr_clks , GFP_KERNEL ); <nl> + clk_table = kcalloc ( nr_clks , sizeof (* clk_table ), GFP_KERNEL ); <nl> if (! clk_table ) { <nl> pr_err ("% s : could not allocate clock lookup table \ n ", __func__ ); <nl> goto err_data ;
static int o2cb_cluster_check ( void ) <nl> set_bit ( node_num , netmap ); <nl> if (! memcmp ( hbmap , netmap , sizeof ( hbmap ))) <nl> return 0 ; <nl> - if ( i < O2CB_MAP_STABILIZE_COUNT ) <nl> + if ( i < O2CB_MAP_STABILIZE_COUNT - 1 ) <nl> msleep ( 1000 ); <nl> } <nl> 
int t4vf_wr_mbox_core ( struct adapter * adapter , const void * cmd , int size , <nl> delay_idx = 0 ; <nl> ms = delay [ 0 ]; <nl>  <nl> - for ( i = 0 ; i < 500 ; i += ms ) { <nl> + for ( i = 0 ; i < FW_CMD_MAX_TIMEOUT ; i += ms ) { <nl> if ( sleep_ok ) { <nl> ms = delay [ delay_idx ]; <nl> if ( delay_idx < ARRAY_SIZE ( delay ) - 1 )
int cdc_ncm_bind_common ( struct usbnet * dev , struct usb_interface * intf , u8 data_ <nl> u8 iface_no ; <nl>  <nl> ctx = kzalloc ( sizeof (* ctx ), GFP_KERNEL ); <nl> - if ( ctx == NULL ) <nl> - return - ENODEV ; <nl> + if (! ctx ) <nl> + return - ENOMEM ; <nl>  <nl> hrtimer_init (& ctx -> tx_timer , CLOCK_MONOTONIC , HRTIMER_MODE_REL ); <nl> ctx -> tx_timer . function = & cdc_ncm_tx_timer_cb ;
EXPORT_SYMBOL ( hwsw_unmap_sg ); <nl> EXPORT_SYMBOL ( hwsw_dma_supported ); <nl> EXPORT_SYMBOL ( hwsw_alloc_coherent ); <nl> EXPORT_SYMBOL ( hwsw_free_coherent ); <nl> + EXPORT_SYMBOL ( hwsw_sync_single_for_cpu ); <nl> + EXPORT_SYMBOL ( hwsw_sync_single_for_device ); <nl> + EXPORT_SYMBOL ( hwsw_sync_sg_for_cpu ); <nl> + EXPORT_SYMBOL ( hwsw_sync_sg_for_device );
struct img_hash_request_ctx { <nl> unsigned long op ; <nl>  <nl> size_t bufcnt ; <nl> - u8 buffer [ 0 ] __aligned ( sizeof ( u32 )); <nl> struct ahash_request fallback_req ; <nl> + <nl> + /* Zero length buffer must remain last member of struct */ <nl> + u8 buffer [ 0 ] __aligned ( sizeof ( u32 )); <nl> }; <nl>  <nl> struct img_hash_ctx {
d40_get_dev_addr ( struct d40_chan * chan , enum dma_data_direction direction ) <nl> { <nl> struct stedma40_platform_data * plat = chan -> base -> plat_data ; <nl> struct stedma40_chan_cfg * cfg = & chan -> dma_cfg ; <nl> - dma_addr_t addr ; <nl> + dma_addr_t addr = 0 ; <nl>  <nl> if ( chan -> runtime_addr ) <nl> return chan -> runtime_addr ;
static int ucode_init ( loader_block * lb , amb_dev * dev ) <nl> const struct firmware * fw ; <nl> unsigned long start_address ; <nl> const struct ihex_binrec * rec ; <nl> - const char * errmsg = 0 ; <nl> + const char * errmsg = NULL ; <nl> int res ; <nl>  <nl> res = request_ihex_firmware (& fw , " atmsar11 . fw ", & dev -> pci_dev -> dev );
nv134_chipset = { <nl> . name = " GP104 ", <nl> . bar = gf100_bar_new , <nl> . bios = nvkm_bios_new , <nl> + . bus = gf100_bus_new , <nl> . devinit = gm200_devinit_new , <nl> . fb = gp104_fb_new , <nl> . imem = nv50_instmem_new ,
static void mpc_push ( struct atm_vcc * vcc , struct sk_buff * skb ) <nl> eg -> packets_rcvd ++; <nl> mpc -> eg_ops -> put ( eg ); <nl>  <nl> - memset ( ATM_SKB ( skb ), 0 , sizeof ( struct atm_skb_data )); <nl> + memset ( ATM_SKB ( new_skb ), 0 , sizeof ( struct atm_skb_data )); <nl> netif_rx ( new_skb ); <nl> } <nl> 
static int pty_write ( struct tty_struct * tty , const unsigned char * buf , <nl>  <nl> static int pty_write_room ( struct tty_struct * tty ) <nl> { <nl> + if ( tty -> stopped ) <nl> + return 0 ; <nl> return pty_space ( tty -> link ); <nl> } <nl> 
int xfrm_init_replay ( struct xfrm_state * x ) <nl> replay_esn -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> return - EINVAL ; <nl>  <nl> + if (( x -> props . flags & XFRM_STATE_ESN ) && replay_esn -> replay_window == 0 ) <nl> + return - EINVAL ; <nl> + <nl> if (( x -> props . flags & XFRM_STATE_ESN ) && x -> replay_esn ) <nl> x -> repl = & xfrm_replay_esn ; <nl> else
int recover_inode_page ( struct f2fs_sb_info * sbi , struct page * page ) <nl> new_ni = old_ni ; <nl> new_ni . ino = ino ; <nl>  <nl> + if (! inc_valid_node_count ( sbi , NULL , 1 )) <nl> + WARN_ON ( 1 ); <nl> set_node_addr ( sbi , & new_ni , NEW_ADDR ); <nl> inc_valid_inode_count ( sbi ); <nl> 
int xhci_halt ( struct xhci_hcd * xhci ) <nl> STS_HALT , STS_HALT , XHCI_MAX_HALT_USEC ); <nl> if (! ret ) <nl> xhci -> xhc_state |= XHCI_STATE_HALTED ; <nl> + else <nl> + xhci_warn ( xhci , " Host not halted after % u microseconds .\ n ", <nl> + XHCI_MAX_HALT_USEC ); <nl> return ret ; <nl> } <nl> 
unapply_new_state : <nl> pinmux_enable_setting ( setting ); <nl> } <nl> } <nl> + <nl> + p -> state = old_state ; <nl> return ret ; <nl> } <nl> 
static int dtl1_hci_send_frame ( struct sk_buff * skb ) <nl> nsh . len = skb -> len ; <nl>  <nl> s = bt_skb_alloc ( NSHL + skb -> len + 1 , GFP_ATOMIC ); <nl> + if (! s ) <nl> + return - ENOMEM ; <nl> + <nl> skb_reserve ( s , NSHL ); <nl> memcpy ( skb_put ( s , skb -> len ), skb -> data , skb -> len ); <nl> if ( skb -> len & 0x0001 )
static void remove_event_file_dir ( struct ftrace_event_file * file ) <nl>  <nl> list_del (& file -> list ); <nl> remove_subsystem ( file -> system ); <nl> + free_event_filter ( file -> filter ); <nl> kmem_cache_free ( file_cachep , file ); <nl> } <nl> 
struct gpio_chip * gpiochip_find ( void * data , <nl>  <nl> spin_lock_irqsave (& gpio_lock , flags ); <nl> list_for_each_entry ( gdev , & gpio_devices , list ) <nl> - if ( match ( gdev -> chip , data )) <nl> + if ( gdev -> chip && match ( gdev -> chip , data )) <nl> break ; <nl>  <nl> /* No match ? */
static int revoke_lo_scan_elements ( struct gfs2_jdesc * jd , unsigned int start , <nl> blkno = be64_to_cpu (*( __be64 *)( bh -> b_data + offset )); <nl>  <nl> error = gfs2_revoke_add ( sdp , blkno , start ); <nl> - if ( error < 0 ) <nl> + if ( error < 0 ) { <nl> + brelse ( bh ); <nl> return error ; <nl> + } <nl> else if ( error ) <nl> sdp -> sd_found_revokes ++; <nl> 
static void rtl8150_disconnect ( struct usb_interface * intf ) <nl> usb_set_intfdata ( intf , NULL ); <nl> if ( dev ) { <nl> set_bit ( RTL8150_UNPLUG , & dev -> flags ); <nl> - tasklet_disable (& dev -> tl ); <nl> tasklet_kill (& dev -> tl ); <nl> unregister_netdev ( dev -> netdev ); <nl> unlink_all_urbs ( dev );
static int cgroup_get_sb ( struct file_system_type * fs_type , <nl> BUG_ON ( root -> number_of_cgroups != 1 ); <nl>  <nl> cgroup_populate_dir ( root_cgrp ); <nl> - mutex_unlock (& inode -> i_mutex ); <nl> mutex_unlock (& cgroup_mutex ); <nl> + mutex_unlock (& inode -> i_mutex ); <nl> } <nl>  <nl> simple_set_mnt ( mnt , sb );
v9fs_mux_rpc ( struct v9fs_mux_data * m , struct v9fs_fcall * tc , <nl> r . rcall || r . err ); <nl> } while (! r . rcall && ! r . err && err ==- ERESTARTSYS && <nl> m -> trans -> status == Connected && ! m -> err ); <nl> + <nl> + err = - ERESTARTSYS ; <nl> } <nl> sigpending = 1 ; <nl> }
struct fpgaimage { <nl> char part [ MAX_STR ]; <nl> char date [ MAX_STR ]; <nl> char time [ MAX_STR ]; <nl> - int32_t lendata ; <nl> + int lendata ; <nl> char * fpgadata ; <nl> };
gen6_read ## x ( struct drm_i915_private * dev_priv , off_t reg , bool trace ) { \ <nl> static u ## x \ <nl> vlv_read ## x ( struct drm_i915_private * dev_priv , off_t reg , bool trace ) { \ <nl> unsigned fwengine = 0 ; \ <nl> - unsigned * fwcount = 0 ; \ <nl> + unsigned * fwcount ; \ <nl> REG_READ_HEADER ( x ); \ <nl> if ( FORCEWAKE_VLV_RENDER_RANGE_OFFSET ( reg )) { \ <nl> fwengine = FORCEWAKE_RENDER ; \
void free_reloc_roots ( struct list_head * list ) <nl> while (! list_empty ( list )) { <nl> reloc_root = list_entry ( list -> next , struct btrfs_root , <nl> root_list ); <nl> + free_extent_buffer ( reloc_root -> node ); <nl> + free_extent_buffer ( reloc_root -> commit_root ); <nl> + reloc_root -> node = NULL ; <nl> + reloc_root -> commit_root = NULL ; <nl> __del_reloc_root ( reloc_root ); <nl> } <nl> }
int cfhsi_probe ( struct platform_device * pdev ) <nl> dev_err (& ndev -> dev , "% s : Registration error : % d .\ n ", <nl> __func__ , res ); <nl> free_netdev ( ndev ); <nl> + return - ENODEV ; <nl> } <nl> /* Add CAIF HSI device to list . */ <nl> spin_lock (& cfhsi_list_lock );
xfs_check_page_type ( <nl> if ( type == XFS_IO_UNWRITTEN ) <nl> return true ; <nl> } else if ( buffer_delay ( bh )) { <nl> - if ( type == XFS_IO_DELALLOC ); <nl> + if ( type == XFS_IO_DELALLOC ) <nl> return true ; <nl> } else if ( buffer_dirty ( bh ) && buffer_mapped ( bh )) { <nl> - if ( type == XFS_IO_OVERWRITE ); <nl> + if ( type == XFS_IO_OVERWRITE ) <nl> return true ; <nl> } <nl> 
static int tmio_probe ( struct platform_device * dev ) <nl> nand_chip = & tmio -> chip ; <nl> mtd -> priv = nand_chip ; <nl> mtd -> name = " tmio - nand "; <nl> + mtd -> dev . parent = & dev -> dev ; <nl>  <nl> tmio -> ccr = devm_ioremap (& dev -> dev , ccr -> start , resource_size ( ccr )); <nl> if (! tmio -> ccr )
static void sky2_power_aux ( struct sky2_hw * hw ) <nl> Y2_CLK_GAT_LNK1_DIS | Y2_PCI_CLK_LNK2_DIS | <nl> Y2_COR_CLK_LNK2_DIS | Y2_CLK_GAT_LNK2_DIS ); <nl>  <nl> - /* switch power to VAUX */ <nl> - if ( sky2_read32 ( hw , B0_CTST ) & Y2_VAUX_AVAIL ) <nl> + /* switch power to VAUX if supported and PME from D3cold */ <nl> + if ( ( sky2_read32 ( hw , B0_CTST ) & Y2_VAUX_AVAIL ) && <nl> + pci_pme_capable ( hw -> pdev , PCI_D3cold )) <nl> sky2_write8 ( hw , B0_POWER_CTRL , <nl> ( PC_VAUX_ENA | PC_VCC_ENA | <nl> PC_VAUX_ON | PC_VCC_OFF ));
static char * hidpp_get_unifying_name ( struct hidpp_device * hidpp_dev ) <nl>  <nl> len = response . rap . params [ 1 ]; <nl>  <nl> + if ( 2 + len > sizeof ( response . rap . params )) <nl> + return NULL ; <nl> + <nl> name = kzalloc ( len + 1 , GFP_KERNEL ); <nl> if (! name ) <nl> return NULL ;
retry : <nl> } <nl>  <nl> /* Check info buffer */ <nl> - info = ( void *)& msg [ 1 ]; <nl> + info = ( void *)& bcdc -> buf [ 0 ]; <nl>  <nl> /* Copy info buffer */ <nl> if ( buf ) {
static const struct of_device_id regulator_haptic_dt_match [] = { <nl> { . compatible = " regulator - haptic " }, <nl> { /* sentinel */ }, <nl> }; <nl> + MODULE_DEVICE_TABLE ( of , regulator_haptic_dt_match ); <nl>  <nl> static struct platform_driver regulator_haptic_driver = { <nl> . probe = regulator_haptic_probe ,
/* <nl> * Literals <nl> */ <nl> -# define IPR_DRIVER_VERSION " 2 . 6 . 1 " <nl> -# define IPR_DRIVER_DATE "( March 12 , 2015 )" <nl> +# define IPR_DRIVER_VERSION " 2 . 6 . 2 " <nl> +# define IPR_DRIVER_DATE "( June 11 , 2015 )" <nl>  <nl> /* <nl> * IPR_MAX_CMD_PER_LUN : This defines the maximum number of outstanding
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
static int pfkey_spdadd ( struct sock * sk , struct sk_buff * skb , struct sadb_msg * h <nl> return 0 ; <nl>  <nl> out : <nl> + xp -> dead = 1 ; <nl> xfrm_policy_destroy ( xp ); <nl> return err ; <nl> }
int of_irq_map_one ( struct device_node * device , int index , struct of_irq * out_irq <nl> intsize = * tmp ; <nl>  <nl> /* Check index */ <nl> - if ( index * intsize >= intlen ) <nl> + if (( index + 1 ) * intsize > intlen ) <nl> return - EINVAL ; <nl>  <nl> /* Get new specifier and map it */
i915_gem_object_pin ( struct drm_i915_gem_object * obj , <nl> uint32_t alignment , <nl> unsigned flags ) <nl> { <nl> + struct drm_i915_private * dev_priv = obj -> base . dev -> dev_private ; <nl> struct i915_vma * vma ; <nl> int ret ; <nl>  <nl> + if ( WARN_ON ( vm == & dev_priv -> mm . aliasing_ppgtt -> base )) <nl> + return - ENODEV ; <nl> + <nl> if ( WARN_ON ( flags & ( PIN_GLOBAL | PIN_MAPPABLE ) && ! i915_is_ggtt ( vm ))) <nl> return - EINVAL ; <nl> 
struct ulpi_info { <nl> /* ULPI hardcoded IDs , used for probing */ <nl> static struct ulpi_info ulpi_ids [] = { <nl> ULPI_INFO ( ULPI_ID ( 0x04cc , 0x1504 ), " NXP ISP1504 "), <nl> - ULPI_INFO ( ULPI_ID ( 0x0424 , 0x0006 ), " SMSC USB3319 "), <nl> + ULPI_INFO ( ULPI_ID ( 0x0424 , 0x0006 ), " SMSC USB331x "), <nl> }; <nl>  <nl> static int ulpi_set_otg_flags ( struct otg_transceiver * otg )
static void * raid0_takeover_raid1 ( mddev_t * mddev ) <nl> mddev -> new_layout = 0 ; <nl> mddev -> new_chunk_sectors = 128 ; /* by default set chunk size to 64k */ <nl> mddev -> delta_disks = 1 - mddev -> raid_disks ; <nl> + mddev -> raid_disks = 1 ; <nl> /* make sure it will be not marked as dirty */ <nl> mddev -> recovery_cp = MaxSector ; <nl> 
static int snd_fm801_free ( struct fm801 * chip ) <nl> cmdw |= 0x00c3 ; <nl> fm801_writew ( chip , IRQ_MASK , cmdw ); <nl>  <nl> + devm_free_irq (& chip -> pci -> dev , chip -> irq , chip ); <nl> + <nl> __end_hw : <nl> # ifdef CONFIG_SND_FM801_TEA575X_BOOL <nl> if (!( chip -> tea575x_tuner & TUNER_DISABLED )) {
TRACE_EVENT ( task_rename , <nl> TP_fast_assign ( <nl> __entry -> pid = task -> pid ; <nl> memcpy ( entry -> oldcomm , task -> comm , TASK_COMM_LEN ); <nl> - memcpy ( entry -> newcomm , comm , TASK_COMM_LEN ); <nl> + strlcpy ( entry -> newcomm , comm , TASK_COMM_LEN ); <nl> __entry -> oom_score_adj = task -> signal -> oom_score_adj ; <nl> ), <nl> 
vector_setup_out : <nl> **/ <nl> static struct i40e_vsi * i40e_vsi_reinit_setup ( struct i40e_vsi * vsi ) <nl> { <nl> - struct i40e_pf * pf = vsi -> back ; <nl> + struct i40e_pf * pf ; <nl> u8 enabled_tc ; <nl> int ret ; <nl>  <nl> + if (! vsi ) <nl> + return NULL ; <nl> + <nl> + pf = vsi -> back ; <nl> + <nl> i40e_put_lump ( pf -> qp_pile , vsi -> base_queue , vsi -> idx ); <nl> i40e_vsi_clear_rings ( vsi ); <nl> 
minstrel_aggr_check ( struct minstrel_priv * mp , struct ieee80211_sta * pubsta , stru <nl> if ( likely ( sta -> ampdu_mlme . tid_tx [ tid ])) <nl> return ; <nl>  <nl> + if ( skb_get_queue_mapping ( skb ) == IEEE80211_AC_VO ) <nl> + return ; <nl> + <nl> ieee80211_start_tx_ba_session ( pubsta , tid ); <nl> } <nl> 
static int __devinit bfin_lq035_probe ( struct platform_device * pdev ) <nl> i2c_add_driver (& ad5280_driver ); <nl>  <nl> memset (& props , 0 , sizeof ( props )); <nl> + props . type = BACKLIGHT_RAW ; <nl> props . max_brightness = MAX_BRIGHENESS ; <nl> bl_dev = backlight_device_register (" bf537 - bl ", NULL , NULL , <nl> & bfin_lq035fb_bl_ops , & props );
process_filter ( struct event_format * event , struct filter_arg ** parg , <nl> * parg = current_op ; <nl> else <nl> * parg = current_exp ; <nl> + free ( token ); <nl> return PEVENT_ERRNO__UNBALANCED_PAREN ; <nl> } <nl> break ; <nl> process_filter ( struct event_format * event , struct filter_arg ** parg , <nl>  <nl> * parg = current_op ; <nl>  <nl> + free ( token ); <nl> return 0 ; <nl>  <nl> fail_alloc :
void blk_mq_free_request ( struct request * rq ) <nl> hctx = q -> mq_ops -> map_queue ( q , ctx -> cpu ); <nl> __blk_mq_free_request ( hctx , ctx , rq ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( blk_mq_free_request ); <nl>  <nl> inline void __blk_mq_end_request ( struct request * rq , int error ) <nl> {
static int ibmvfc_map_sg_data ( struct scsi_cmnd * scmd , <nl> & evt -> ext_list_token ); <nl>  <nl> if (! evt -> ext_list ) { <nl> - scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> + scsi_dma_unmap ( scmd ); <nl> + if ( vhost -> log_level > IBMVFC_DEFAULT_LOG_LEVEL ) <nl> + scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> return - ENOMEM ; <nl> } <nl> }
static int omap_i2c_remove ( struct platform_device * pdev ) <nl> return ret ; <nl>  <nl> omap_i2c_write_reg ( omap , OMAP_I2C_CON_REG , 0 ); <nl> - pm_runtime_put (& pdev -> dev ); <nl> + pm_runtime_put_sync (& pdev -> dev ); <nl> pm_runtime_disable (& pdev -> dev ); <nl> return 0 ; <nl> }
static int osc_extent_wait ( const struct lu_env * env , struct osc_extent * ext , <nl> "% s : wait ext to % d timedout , recovery in progress ?\ n ", <nl> osc_export ( obj )-> exp_obd -> obd_name , state ); <nl>  <nl> - lwi = LWI_INTR ( LWI_ON_SIGNAL_NOOP , NULL ); <nl> + lwi = LWI_INTR ( NULL , NULL ); <nl> rc = l_wait_event ( ext -> oe_waitq , extent_wait_cb ( ext , state ), <nl> & lwi ); <nl> }
static void dce3_2_afmt_write_sad_regs ( struct drm_encoder * encoder ) <nl> } <nl>  <nl> sad_count = drm_edid_to_sad ( radeon_connector -> edid , & sads ); <nl> - if ( sad_count < 0 ) { <nl> + if ( sad_count <= 0 ) { <nl> DRM_ERROR (" Couldn ' t read SADs : % d \ n ", sad_count ); <nl> return ; <nl> }
static int btrfs_parse_early_options ( const char * options , fmode_t flags , <nl> token = match_token ( p , tokens , args ); <nl> switch ( token ) { <nl> case Opt_subvol : <nl> + kfree (* subvol_name ); <nl> * subvol_name = match_strdup (& args [ 0 ]); <nl> break ; <nl> case Opt_subvolid :
static int rename_volumes ( struct ubi_device * ubi , <nl> req -> ents [ i ]. name [ req -> ents [ i ]. name_len ] = '\ 0 '; <nl> n = strlen ( req -> ents [ i ]. name ); <nl> if ( n != req -> ents [ i ]. name_len ) <nl> - err = - EINVAL ; <nl> + return - EINVAL ; <nl> } <nl>  <nl> /* Make sure volume IDs and names are unique */
int dm_split_args ( int * argc , char *** argvp , char * input ) <nl> unsigned array_size = 0 ; <nl>  <nl> * argc = 0 ; <nl> + <nl> + if (! input ) { <nl> + * argvp = NULL ; <nl> + return 0 ; <nl> + } <nl> + <nl> argv = realloc_argv (& array_size , argv ); <nl> if (! argv ) <nl> return - ENOMEM ;
int reiserfs_acl_chmod ( struct inode * inode ) <nl> return 0 ; <nl> } <nl>  <nl> + reiserfs_write_unlock ( inode -> i_sb ); <nl> acl = reiserfs_get_acl ( inode , ACL_TYPE_ACCESS ); <nl> + reiserfs_write_lock ( inode -> i_sb ); <nl> if (! acl ) <nl> return 0 ; <nl> if ( IS_ERR ( acl ))
safe_sig_queue_validate ( struct signal_queue_header * psafe_sqh , <nl> struct signal_queue_header * punsafe_sqh , <nl> u32 * phead , u32 * ptail ) <nl> { <nl> - if ((* phead >= psafe_sqh -> max_slots ) <nl> - || (* ptail >= psafe_sqh -> max_slots )) { <nl> + if ((* phead >= psafe_sqh -> max_slots ) || <nl> + (* ptail >= psafe_sqh -> max_slots )) { <nl> /* Choose 0 or max , maybe based on current tail value */ <nl> * phead = 0 ; <nl> * ptail = 0 ;
static int doc_write_oob ( struct mtd_info * mtd , loff_t ofs , <nl> oobdelta = mtd -> ecclayout -> oobavail ; <nl> break ; <nl> default : <nl> - oobdelta = 0 ; <nl> + return - EINVAL ; <nl> } <nl> if (( len % DOC_LAYOUT_PAGE_SIZE ) || ( ooblen % oobdelta ) || <nl> ( ofs % DOC_LAYOUT_PAGE_SIZE ))
static ssize_t tun_chr_aio_read ( struct kiocb * iocb , const struct iovec * iv , <nl> ret = tun_do_read ( tun , tfile , iocb , iv , len , <nl> file -> f_flags & O_NONBLOCK ); <nl> ret = min_t ( ssize_t , ret , len ); <nl> + if ( ret > 0 ) <nl> + iocb -> ki_pos = ret ; <nl> out : <nl> tun_put ( tun ); <nl> return ret ;
static inline void dec_valid_block_count ( struct f2fs_sb_info * sbi , <nl> static inline void inc_page_count ( struct f2fs_sb_info * sbi , int count_type ) <nl> { <nl> percpu_counter_inc (& sbi -> nr_pages [ count_type ]); <nl> + <nl> + if ( count_type == F2FS_DIRTY_DATA || count_type == F2FS_INMEM_PAGES ) <nl> + return ; <nl> + <nl> set_sbi_flag ( sbi , SBI_IS_DIRTY ); <nl> } <nl> 
static int brcmf_sdbrcm_write_vars ( struct brcmf_sdio * bus ) <nl> /* Verify NVRAM bytes */ <nl> brcmf_dbg ( INFO , " Compare NVRAM dl & ul ; varsize =% d \ n ", varsize ); <nl> nvram_ularray = kmalloc ( varsize , GFP_ATOMIC ); <nl> - if (! nvram_ularray ) <nl> + if (! nvram_ularray ) { <nl> + kfree ( vbuffer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> /* Upload image to verify downloaded contents . */ <nl> memset ( nvram_ularray , 0xaa , varsize );
spider_net_prepare_rx_descr ( struct spider_net_card * card , <nl> /* and we need to have it 128 byte aligned , therefore we allocate a <nl> * bit more */ <nl> /* allocate an skb */ <nl> - descr -> skb = dev_alloc_skb ( bufsize + SPIDER_NET_RXBUF_ALIGN - 1 ); <nl> + descr -> skb = netdev_alloc_skb ( card -> netdev , <nl> + bufsize + SPIDER_NET_RXBUF_ALIGN - 1 ); <nl> if (! descr -> skb ) { <nl> if ( netif_msg_rx_err ( card ) && net_ratelimit ()) <nl> pr_err (" Not enough memory to allocate rx buffer \ n ");
static long lineevent_ioctl ( struct file * filep , unsigned int cmd , <nl> if ( cmd == GPIOHANDLE_GET_LINE_VALUES_IOCTL ) { <nl> int val ; <nl>  <nl> + memset (& ghd , 0 , sizeof ( ghd )); <nl> + <nl> val = gpiod_get_value_cansleep ( le -> desc ); <nl> if ( val < 0 ) <nl> return val ;
static int __vhost_add_used_n ( struct vhost_virtqueue * vq , <nl>  <nl> start = vq -> last_used_idx % vq -> num ; <nl> used = vq -> used -> ring + start ; <nl> - if ( copy_to_user ( used , heads , count * sizeof * used )) { <nl> + if ( __copy_to_user ( used , heads , count * sizeof * used )) { <nl> vq_err ( vq , " Failed to write used "); <nl> return - EFAULT ; <nl> }
static void __init request_standard_resources ( void ) <nl> res = alloc_bootmem_low ( sizeof (* res )); <nl> if ( memblock_is_nomap ( region )) { <nl> res -> name = " reserved "; <nl> - res -> flags = IORESOURCE_MEM | IORESOURCE_BUSY ; <nl> + res -> flags = IORESOURCE_MEM ; <nl> } else { <nl> res -> name = " System RAM "; <nl> res -> flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY ;
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> - err = do_migrate_pages ( mm , & old , & new , MPOL_MF_MOVE ); <nl> + err = do_migrate_pages ( mm , & old , & new , <nl> + capable ( CAP_SYS_ADMIN ) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE ); <nl> out : <nl> mmput ( mm ); <nl> return err ;
jme_alloc_and_feed_skb ( struct jme_adapter * jme , int idx ) <nl> jme -> jme_vlan_rx ( skb , jme -> vlgrp , <nl> le16_to_cpu ( rxdesc -> descwb . vlan )); <nl> NET_STAT ( jme ). rx_bytes += 4 ; <nl> + } else { <nl> + dev_kfree_skb ( skb ); <nl> } <nl> } else { <nl> jme -> jme_rx ( skb );
static struct tx_agg * r8152_get_tx_agg ( struct r8152 * tp ) <nl> struct tx_agg * agg = NULL ; <nl> unsigned long flags ; <nl>  <nl> + if ( list_empty (& tp -> tx_free )) <nl> + return NULL ; <nl> + <nl> spin_lock_irqsave (& tp -> tx_lock , flags ); <nl> if (! list_empty (& tp -> tx_free )) { <nl> struct list_head * cursor ;
int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
struct device ; <nl>  <nl> enum led_brightness { <nl> LED_OFF = 0 , <nl> + LED_ON = 1 , <nl> LED_HALF = 127 , <nl> LED_FULL = 255 , <nl> };
__acquires (& pool -> lock ) <nl> * kernels , where a requeueing work item waiting for something to <nl> * happen could deadlock with stop_machine as such work item could <nl> * indefinitely requeue itself while all other CPUs are trapped in <nl> - * stop_machine . <nl> + * stop_machine . At the same time , report a quiescent RCU state so <nl> + * the same condition doesn ' t freeze RCU . <nl> */ <nl> + rcu_note_voluntary_context_switch ( current ); <nl> cond_resched (); <nl>  <nl> spin_lock_irq (& pool -> lock );
static void raid10d ( mddev_t * mddev ) <nl> sl --; <nl> d = r10_bio -> devs [ sl ]. devnum ; <nl> rdev = conf -> mirrors [ d ]. rdev ; <nl> - atomic_add ( s , & rdev -> corrected_errors ); <nl> if ( rdev && <nl> test_bit ( In_sync , & rdev -> flags )) { <nl> + atomic_add ( s , & rdev -> corrected_errors ); <nl> if ( sync_page_io ( rdev -> bdev , <nl> r10_bio -> devs [ sl ]. addr + <nl> sect + rdev -> data_offset ,
static ssize_t btrfs_label_store ( struct kobject * kobj , <nl> int ret ; <nl> size_t p_len ; <nl>  <nl> + if ( fs_info -> sb -> s_flags & MS_RDONLY ) <nl> + return - EROFS ; <nl> + <nl> /* <nl> * p_len is the len until the first occurrence of either <nl> * '\ n ' or '\ 0 '
static int dspi_request_dma ( struct fsl_dspi * dspi , phys_addr_t phy_addr ) <nl> return 0 ; <nl>  <nl> err_slave_config : <nl> - devm_kfree ( dev , dma -> rx_dma_buf ); <nl> + dma_free_coherent ( dev , DSPI_DMA_BUFSIZE , <nl> + dma -> rx_dma_buf , dma -> rx_dma_phys ); <nl> err_rx_dma_buf : <nl> - devm_kfree ( dev , dma -> tx_dma_buf ); <nl> + dma_free_coherent ( dev , DSPI_DMA_BUFSIZE , <nl> + dma -> tx_dma_buf , dma -> tx_dma_phys ); <nl> err_tx_dma_buf : <nl> dma_release_channel ( dma -> chan_tx ); <nl> err_tx_channel :
static void napi_reuse_skb ( struct napi_struct * napi , struct sk_buff * skb ) <nl> __skb_pull ( skb , skb_headlen ( skb )); <nl> skb_reserve ( skb , NET_IP_ALIGN - skb_headroom ( skb )); <nl> skb -> vlan_tci = 0 ; <nl> + skb -> dev = napi -> dev ; <nl>  <nl> napi -> skb = skb ; <nl> }
int test_range_bit ( struct extent_io_tree * tree , u64 start , u64 end , <nl> bitset = 0 ; <nl> break ; <nl> } <nl> + <nl> + if ( state -> end == ( u64 )- 1 ) <nl> + break ; <nl> + <nl> start = state -> end + 1 ; <nl> if ( start > end ) <nl> break ;
static int fsl_pq_mdio_probe ( struct of_device * ofdev , <nl> dev_set_drvdata (& ofdev -> dev , new_bus ); <nl>  <nl> if ( of_device_is_compatible ( np , " fsl , gianfar - mdio ") || <nl> + of_device_is_compatible ( np , " fsl , gianfar - tbi ") || <nl> of_device_is_compatible ( np , " gianfar ")) { <nl> # ifdef CONFIG_GIANFAR <nl> tbipa = get_gfar_tbipa ( regs );
int serial8250_em485_init ( struct uart_8250_port * p ) <nl> if ( p -> em485 != NULL ) <nl> return 0 ; <nl>  <nl> - p -> em485 = kmalloc ( sizeof ( struct uart_8250_em485 ), GFP_KERNEL ); <nl> + p -> em485 = kmalloc ( sizeof ( struct uart_8250_em485 ), GFP_ATOMIC ); <nl> if ( p -> em485 == NULL ) <nl> return - ENOMEM ; <nl> 
extern void numa_initmem_init ( unsigned long start_pfn , unsigned long end_pfn ); <nl> extern unsigned long numa_free_all_bootmem ( void ); <nl>  <nl> extern void reserve_bootmem_generic ( unsigned long phys , unsigned len ); <nl> - extern void free_bootmem_generic ( unsigned long phys , unsigned len ); <nl>  <nl> extern void load_gs_index ( unsigned gs ); <nl> 
void usbip_stop_eh ( struct usbip_device * ud ) <nl> { <nl> struct usbip_task * eh = & ud -> eh ; <nl>  <nl> + if ( eh -> thread == current ) <nl> + return ; /* do not wait for myself */ <nl> + <nl> wait_for_completion (& eh -> thread_done ); <nl> usbip_dbg_eh (" usbip_eh has finished \ n "); <nl> }
static s32 brcmf_p2p_run_escan ( struct brcmf_cfg80211_info * cfg , <nl> } <nl> err = brcmf_p2p_escan ( p2p , num_nodfs , chanspecs , search_state , <nl> action , P2PAPI_BSSCFG_DEVICE ); <nl> + kfree ( chanspecs ); <nl> } <nl> exit : <nl> if ( err )
void host1x_set_drm_data ( struct device * dev , void * data ) <nl> void * host1x_get_drm_data ( struct device * dev ) <nl> { <nl> struct host1x * host1x = dev_get_drvdata ( dev ); <nl> - return host1x -> drm_data ; <nl> + return host1x ? host1x -> drm_data : NULL ; <nl> } <nl>  <nl> void host1x_sync_writel ( struct host1x * host1x , u32 v , u32 r )
ibx_get_dpll ( struct intel_crtc * crtc , struct intel_crtc_state * crtc_state , <nl> DPLL_ID_PCH_PLL_B ); <nl> } <nl>  <nl> + if (! pll ) <nl> + return NULL ; <nl> + <nl> /* reference the pll */ <nl> intel_reference_shared_dpll ( pll , crtc_state ); <nl> 
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
static int idmouse_probe ( struct usb_interface * interface , <nl> if ( iface_desc -> desc . bInterfaceClass != 0x0A ) <nl> return - ENODEV ; <nl>  <nl> + if ( iface_desc -> desc . bNumEndpoints < 1 ) <nl> + return - ENODEV ; <nl> + <nl> /* allocate memory for our device state and initialize it */ <nl> dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> if ( dev == NULL )
static int ocfs2_truncate_file ( struct inode * inode , <nl> ( unsigned long long ) OCFS2_I ( inode )-> ip_blkno , <nl> ( unsigned long long ) new_i_size ); <nl>  <nl> + unmap_mapping_range ( inode -> i_mapping , new_i_size + PAGE_SIZE - 1 , 0 , 1 ); <nl> truncate_inode_pages ( inode -> i_mapping , new_i_size ); <nl>  <nl> fe = ( struct ocfs2_dinode *) di_bh -> b_data ;
static u32 crc32c_vpmsum ( u32 crc , unsigned char const * p , size_t len ) <nl> } <nl>  <nl> if ( len & ~ VMX_ALIGN_MASK ) { <nl> + preempt_disable (); <nl> pagefault_disable (); <nl> enable_kernel_altivec (); <nl> crc = __crc32c_vpmsum ( crc , p , len & ~ VMX_ALIGN_MASK ); <nl> + disable_kernel_altivec (); <nl> pagefault_enable (); <nl> + preempt_enable (); <nl> } <nl>  <nl> tail = len & VMX_ALIGN_MASK ;
int switch_ssc_clock ( struct rtsx_chip * chip , int clk ) <nl> return STATUS_FAIL ; <nl> } <nl>  <nl> - mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> + mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> if ( mcu_cnt > 7 ) <nl> mcu_cnt = 7 ; <nl> 
static int run ( mddev_t * mddev ) <nl> int i ; <nl>  <nl> conf_t * conf = kmalloc ( sizeof (* conf ), GFP_KERNEL ); <nl> + if (! conf ) <nl> + return - ENOMEM ; <nl>  <nl> for ( i = 0 ; i < Modes ; i ++) { <nl> atomic_set (& conf -> counters [ i ], 0 );
static void iwl_mvm_stat_iterator ( void * _data , u8 * mac , <nl> if ( vif -> type != NL80211_IFTYPE_STATION ) <nl> return ; <nl>  <nl> + if ( sig == 0 ) { <nl> + IWL_DEBUG_RX ( mvm , " RSSI is 0 - skip signal based decision \ n "); <nl> + return ; <nl> + } <nl> + <nl> mvmvif -> bf_data . ave_beacon_signal = sig ; <nl>  <nl> /* BT Coex */
static int qla4xxx_fw_ready ( struct scsi_qla_host * ha ) <nl> DEBUG2 ( printk (" scsi % ld : % s : FW initialized , but " <nl> " auto - discovery still in process \ n ", <nl> ha -> host_no , __func__ )); <nl> + ready = 1 ; <nl> } <nl>  <nl> return ready ;
static int sysfs_get_sb ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb ) || sb -> s_fs_info != info ) <nl> kfree ( info ); <nl> if ( IS_ERR ( sb )) { <nl> - kfree ( info ); <nl> error = PTR_ERR ( sb ); <nl> goto out ; <nl> }
static struct usbip_imported_device * imported_device_init ( struct usbip_imported_ <nl> goto err ; <nl>  <nl> memcpy ( new_cdev , cdev , sizeof (* new_cdev )); <nl> - dlist_unshift ( idev -> cdev_list , ( void *) new_cdev ); <nl> + dlist_unshift ( idev -> cdev_list , ( void *) new_cdev ); <nl> } <nl> } <nl> 
void __init kmem_cache_init ( void ) <nl> /* Allocate two kmem_caches from the page allocator */ <nl> kmalloc_size = ALIGN ( kmem_size , cache_line_size ()); <nl> order = get_order ( 2 * kmalloc_size ); <nl> - kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT , order ); <nl> + kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT | __GFP_ZERO , order ); <nl>  <nl> /* <nl> * Must first have the slab cache available for the allocations of the
int vfs_quota_on_mount ( struct super_block * sb , char * qf_name , <nl> struct dentry * dentry ; <nl> int error ; <nl>  <nl> + mutex_lock (& sb -> s_root -> d_inode -> i_mutex ); <nl> dentry = lookup_one_len ( qf_name , sb -> s_root , strlen ( qf_name )); <nl> + mutex_unlock (& sb -> s_root -> d_inode -> i_mutex ); <nl> if ( IS_ERR ( dentry )) <nl> return PTR_ERR ( dentry ); <nl> 
static bool attempt_plug_merge ( struct request_queue * q , struct bio * bio , <nl> struct request * rq ; <nl> bool ret = false ; <nl>  <nl> + if ( blk_queue_nomerges ( q )) <nl> + goto out ; <nl> + <nl> plug = current -> plug ; <nl> if (! plug ) <nl> goto out ;
static void intel_agp_insert_sg_entries ( struct agp_memory * mem , <nl> off_t pg_start , int mask_type ) <nl> { <nl> int i , j ; <nl> + u32 cache_bits = 0 ; <nl> + <nl> + if ( agp_bridge -> dev -> device == PCI_DEVICE_ID_INTEL_SANDYBRIDGE_HB ) { <nl> + cache_bits = I830_PTE_SYSTEM_CACHED ; <nl> + } <nl>  <nl> for ( i = 0 , j = pg_start ; i < mem -> page_count ; i ++, j ++) { <nl> writel ( agp_bridge -> driver -> mask_memory ( agp_bridge ,
static u32 * vgic_bytemap_get_reg ( struct vgic_bytemap * x , int cpuid , u32 offset ) <nl> { <nl> offset >>= 2 ; <nl> BUG_ON ( offset > ( VGIC_NR_IRQS / 4 )); <nl> - if ( offset < 4 ) <nl> + if ( offset < 8 ) <nl> return x -> percpu [ cpuid ] + offset ; <nl> else <nl> return x -> shared + offset - 8 ;
static int emac_link_differs ( struct emac_instance * dev ) <nl> static void emac_link_timer ( struct work_struct * work ) <nl> { <nl> struct emac_instance * dev = <nl> - container_of (( struct delayed_work *) work , <nl> + container_of ( to_delayed_work ( work ), <nl> struct emac_instance , link_work ); <nl> int link_poll_interval ; <nl> 
static struct tty_audit_buf * tty_audit_buf_alloc ( int major , int minor , <nl> { <nl> struct tty_audit_buf * buf ; <nl>  <nl> - buf = kmalloc ( sizeof (* buf ), GFP_KERNEL ); <nl> + buf = kmalloc ( sizeof (* buf ), GFP_KERNEL ); <nl> if (! buf ) <nl> goto err ; <nl> if ( PAGE_SIZE != N_TTY_BUF_SIZE )
static void fimd_dp_clock_enable ( struct exynos_drm_crtc * crtc , bool enable ) <nl> * clock . On these SoCs the bootloader may enable it but any <nl> * power domain off / on will reset it to disable state . <nl> */ <nl> - if ( ctx -> driver_data != & exynos5_fimd_driver_data || <nl> + if ( ctx -> driver_data != & exynos5_fimd_driver_data && <nl> ctx -> driver_data != & exynos5420_fimd_driver_data ) <nl> return ; <nl> 
static int generic_set_freq ( struct dvb_frontend * fe , u32 freq /* in HZ */, <nl> offset += 200000 ; <nl> } <nl> # endif <nl> + break ; <nl> default : <nl> tuner_err (" Unsupported tuner type % d .\ n ", new_type ); <nl> break ;
MODULE_DEVICE_TABLE ( spi , mcp320x_id ); <nl> static struct spi_driver mcp320x_driver = { <nl> . driver = { <nl> . name = " mcp320x ", <nl> + . of_match_table = of_match_ptr ( mcp320x_dt_ids ), <nl> . owner = THIS_MODULE , <nl> }, <nl> . probe = mcp320x_probe ,
int sst_hsw_stream_get_volume ( struct sst_hsw * hsw , struct sst_hsw_stream * stream <nl> return - EINVAL ; <nl>  <nl> sst_dsp_read ( hsw -> dsp , volume , <nl> - stream -> reply . volume_register_address [ channel ], sizeof ( volume )); <nl> + stream -> reply . volume_register_address [ channel ], <nl> + sizeof (* volume )); <nl>  <nl> return 0 ; <nl> }
static int setup_blkring ( struct xenbus_device * dev , <nl> if ( err ) <nl> goto fail ; <nl>  <nl> - err = bind_evtchn_to_irqhandler ( info -> evtchn , <nl> - blkif_interrupt , <nl> - IRQF_SAMPLE_RANDOM , " blkif ", info ); <nl> + err = bind_evtchn_to_irqhandler ( info -> evtchn , blkif_interrupt , 0 , <nl> + " blkif ", info ); <nl> if ( err <= 0 ) { <nl> xenbus_dev_fatal ( dev , err , <nl> " bind_evtchn_to_irqhandler failed ");
static int mxs_gpio_set_wake_irq ( u32 irq , u32 enable ) <nl> } <nl>  <nl> static struct irq_chip gpio_irq_chip = { <nl> + . name = " mxs gpio ", <nl> . ack = mxs_gpio_ack_irq , <nl> . mask = mxs_gpio_mask_irq , <nl> . unmask = mxs_gpio_unmask_irq ,
struct sched_domain * build_sched_domain ( struct sched_domain_topology_level * tl , <nl> sd -> level = child -> level + 1 ; <nl> sched_domain_level_max = max ( sched_domain_level_max , sd -> level ); <nl> child -> parent = sd ; <nl> + sd -> child = child ; <nl> } <nl> - sd -> child = child ; <nl> set_domain_attribute ( sd , attr ); <nl>  <nl> return sd ;
static int s5m87xx_i2c_probe ( struct i2c_client * i2c , <nl> s5m87xx -> rtc = i2c_new_dummy ( i2c -> adapter , RTC_I2C_ADDR ); <nl> i2c_set_clientdata ( s5m87xx -> rtc , s5m87xx ); <nl>  <nl> - if ( pdata -> cfg_pmic_irq ) <nl> + if ( pdata && pdata -> cfg_pmic_irq ) <nl> pdata -> cfg_pmic_irq (); <nl>  <nl> s5m_irq_init ( s5m87xx );
static int zcache_comp_init ( void ) <nl> # else <nl> if (* zcache_comp_name != '\ 0 ') { <nl> ret = crypto_has_comp ( zcache_comp_name , 0 , 0 ); <nl> - if (! ret ) <nl> + if (! ret ) { <nl> pr_info (" zcache : % s not supported \ n ", <nl> zcache_comp_name ); <nl> - goto out ; <nl> + ret = 1 ; <nl> + goto out ; <nl> + } <nl> } <nl> if (! ret ) <nl> strcpy ( zcache_comp_name , " lzo ");
static int elm_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> pm_runtime_enable (& pdev -> dev ); <nl> - if ( pm_runtime_get_sync (& pdev -> dev )) { <nl> + if ( pm_runtime_get_sync (& pdev -> dev ) < 0 ) { <nl> ret = - EINVAL ; <nl> pm_runtime_disable (& pdev -> dev ); <nl> dev_err (& pdev -> dev , " can ' t enable clock \ n ");
static int __devinit mwl8k_probe ( struct pci_dev * pdev , <nl> if ( rc ) { <nl> printk ( KERN_ERR "% s : Cannot register device \ n ", <nl> wiphy_name ( hw -> wiphy )); <nl> - goto err_free_irq ; <nl> + goto err_free_queues ; <nl> } <nl>  <nl> printk ( KERN_INFO "% s : % s v % d , % pM , % s firmware % u .% u .% u .% u \ n ",
put_clk : <nl>  <nl> static int g2d_remove ( struct platform_device * pdev ) <nl> { <nl> - struct g2d_dev * dev = ( struct g2d_dev *) platform_get_drvdata ( pdev ); <nl> + struct g2d_dev * dev = platform_get_drvdata ( pdev ); <nl>  <nl> v4l2_info (& dev -> v4l2_dev , " Removing " G2D_NAME ); <nl> v4l2_m2m_release ( dev -> m2m_dev );
static int wm8962_readable_register ( unsigned int reg ) <nl>  <nl> static int wm8962_reset ( struct snd_soc_codec * codec ) <nl> { <nl> - return snd_soc_write ( codec , WM8962_SOFTWARE_RESET , 0 ); <nl> + return snd_soc_write ( codec , WM8962_SOFTWARE_RESET , 0x6243 ); <nl> } <nl>  <nl> static const DECLARE_TLV_DB_SCALE ( inpga_tlv , - 2325 , 75 , 0 );
static int xc5000_release ( struct dvb_frontend * fe ) <nl>  <nl> if ( priv ) { <nl> cancel_delayed_work (& priv -> timer_sleep ); <nl> - hybrid_tuner_release_state ( priv ); <nl> if ( priv -> firmware ) <nl> release_firmware ( priv -> firmware ); <nl> + hybrid_tuner_release_state ( priv ); <nl> } <nl>  <nl> mutex_unlock (& xc5000_list_mutex );
void i40evf_virtchnl_completion ( struct i40evf_adapter * adapter , <nl> sizeof ( struct i40e_virtchnl_vsi_resource ); <nl> memcpy ( adapter -> vf_res , msg , min ( msglen , len )); <nl> i40e_vf_parse_hw_config (& adapter -> hw , adapter -> vf_res ); <nl> + /* restore current mac address */ <nl> + ether_addr_copy ( adapter -> hw . mac . addr , netdev -> dev_addr ); <nl> i40evf_process_config ( adapter ); <nl> } <nl> break ;
static void gb_tty_set_termios ( struct tty_struct * tty , <nl> send_control ( gb_tty , newctrl ); <nl> } <nl>  <nl> - if ( memcpy (& gb_tty -> line_coding , & newline , sizeof ( newline ))) { <nl> + if ( memcmp (& gb_tty -> line_coding , & newline , sizeof ( newline ))) { <nl> memcpy (& gb_tty -> line_coding , & newline , sizeof ( newline )); <nl> send_line_coding ( gb_tty ); <nl> }
static int igb_setup_loopback_test ( struct igb_adapter * adapter ) <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_SERDES ) || <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_BACKPLANE ) || <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_SFP ) || <nl> - ( hw -> device_id == E1000_DEV_ID_I354_SGMII )) { <nl> - <nl> + ( hw -> device_id == E1000_DEV_ID_I354_SGMII ) || <nl> + ( hw -> device_id == E1000_DEV_ID_I354_BACKPLANE_2_5GBPS )) { <nl> /* Enable DH89xxCC MPHY for near end loopback */ <nl> reg = rd32 ( E1000_MPHY_ADDR_CTL ); <nl> reg = ( reg & E1000_MPHY_ADDR_CTL_OFFSET_MASK ) |
static ssize_t gt_max_freq_mhz_show ( struct device * kdev , struct device_attribute <nl> int ret ; <nl>  <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl> - ret = dev_priv -> rps . hw_max * GT_FREQUENCY_MULTIPLIER ; <nl> + ret = dev_priv -> rps . max_delay * GT_FREQUENCY_MULTIPLIER ; <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> return snprintf ( buf , PAGE_SIZE , "% d \ n ", ret );
static void dwc2_hc_set_even_odd_frame ( struct dwc2_hsotg * hsotg , <nl> if ( chan -> ep_type == USB_ENDPOINT_XFER_INT || <nl> chan -> ep_type == USB_ENDPOINT_XFER_ISOC ) { <nl> /* 1 if _next_ frame is odd , 0 if it ' s even */ <nl> - if ( dwc2_hcd_get_frame_number ( hsotg ) & 0x1 ) <nl> + if (!( dwc2_hcd_get_frame_number ( hsotg ) & 0x1 )) <nl> * hcchar |= HCCHAR_ODDFRM ; <nl> } <nl> }
static int ade7854_set_irq ( struct device * dev , bool enable ) <nl> else <nl> irqen &= ~ BIT ( 17 ); <nl>  <nl> - ret = st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> - <nl> - return ret ; <nl> + return st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> } <nl>  <nl> static int ade7854_initial_setup ( struct iio_dev * indio_dev )
static void * slob_alloc ( size_t size , gfp_t gfp , int align , int node ) <nl> /* Improve fragment distribution and reduce our average <nl> * search time by starting our next search here . ( see <nl> * Knuth vol 1 , sec 2 . 5 , pg 449 ) */ <nl> - if ( free_slob_pages . next != prev -> next ) <nl> + if ( prev != free_slob_pages . prev && <nl> + free_slob_pages . next != prev -> next ) <nl> list_move_tail (& free_slob_pages , prev -> next ); <nl> break ; <nl> }
static void add_new_bitmap ( struct btrfs_block_group_cache * block_group , <nl> BUG_ON ( block_group -> total_bitmaps >= max_bitmaps ); <nl>  <nl> info -> offset = offset_to_bitmap ( block_group , offset ); <nl> + info -> bytes = 0 ; <nl> link_free_space ( block_group , info ); <nl> block_group -> total_bitmaps ++; <nl> 
extern unsigned long wall_jiffies ; <nl> */ <nl> unsigned long long sched_clock ( void ) <nl> { <nl> - return (( get_clock () - jiffies_timer_cc ) * 1000 ) >> 12 ; <nl> + return (( get_clock () - jiffies_timer_cc ) * 125 ) >> 9 ; <nl> } <nl>  <nl> void tod_to_timeval ( __u64 todval , struct timespec * xtime )
static void p54_pspoll_workaround ( struct p54_common * priv , struct sk_buff * skb ) <nl> return ; <nl>  <nl> /* only consider beacons from the associated BSSID */ <nl> - if (! ether_addr_equal ( hdr -> addr3 , priv -> bssid )) <nl> + if (! ether_addr_equal_64bits ( hdr -> addr3 , priv -> bssid )) <nl> return ; <nl>  <nl> tim = p54_find_ie ( skb , WLAN_EID_TIM );
static int p54_tx_qos_accounting_alloc ( struct p54_common * priv , <nl> struct p54_tx_queue_stats * queue ; <nl> unsigned long flags ; <nl>  <nl> - if ( WARN_ON ( p54_queue > P54_QUEUE_NUM )) <nl> + if ( WARN_ON ( p54_queue >= P54_QUEUE_NUM )) <nl> return - EINVAL ; <nl>  <nl> queue = & priv -> tx_stats [ p54_queue ];
static int mxsfb_attach_endpoint ( struct drm_device * drm , <nl>  <nl> int mxsfb_create_output ( struct drm_device * drm ) <nl> { <nl> + struct mxsfb_drm_private * mxsfb = drm -> dev_private ; <nl> struct device_node * ep_np = NULL ; <nl> struct of_endpoint ep ; <nl> int ret ; <nl> int mxsfb_create_output ( struct drm_device * drm ) <nl> } <nl> } <nl>  <nl> + if (! mxsfb -> panel ) <nl> + return - EPROBE_DEFER ; <nl> + <nl> return 0 ; <nl> }
int invalidate_inode_pages2_range ( struct address_space * mapping , <nl> pagevec_release (& pvec ); <nl> cond_resched (); <nl> } <nl> + WARN_ON_ONCE ( ret ); <nl> return ret ; <nl> } <nl> EXPORT_SYMBOL_GPL ( invalidate_inode_pages2_range );
static void omap_pcm_limit_supported_formats ( void ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < SNDRV_PCM_FORMAT_LAST ; i ++) { <nl> + for ( i = 0 ; i <= SNDRV_PCM_FORMAT_LAST ; i ++) { <nl> switch ( snd_pcm_format_physical_width ( i )) { <nl> case 8 : <nl> case 16 :
get_matching_model_microcode ( int cpu , unsigned long start , <nl> unsigned int mc_saved_count = mc_saved_data -> mc_saved_count ; <nl> int i ; <nl>  <nl> - while ( leftover ) { <nl> + while ( leftover && mc_saved_count < ARRAY_SIZE ( mc_saved_tmp )) { <nl> mc_header = ( struct microcode_header_intel *) ucode_ptr ; <nl>  <nl> mc_size = get_totalsize ( mc_header );
acpi_table_parse_entries ( char * id , <nl> unsigned long table_end ; <nl> acpi_size tbl_size ; <nl>  <nl> + if ( acpi_disabled ) <nl> + return - ENODEV ; <nl> + <nl> if (! handler ) <nl> return - EINVAL ; <nl>  <nl> int __init acpi_table_parse ( char * id , acpi_table_handler handler ) <nl> struct acpi_table_header * table = NULL ; <nl> acpi_size tbl_size ; <nl>  <nl> + if ( acpi_disabled ) <nl> + return - ENODEV ; <nl> + <nl> if (! handler ) <nl> return - EINVAL ; <nl> 
void igb_update_stats ( struct igb_adapter * adapter , <nl>  <nl> rcu_read_lock (); <nl> for ( i = 0 ; i < adapter -> num_rx_queues ; i ++) { <nl> - u32 rqdpc = rd32 ( E1000_RQDPC ( i )); <nl> struct igb_ring * ring = adapter -> rx_ring [ i ]; <nl> + u32 rqdpc = rd32 ( E1000_RQDPC ( i )); <nl> + if ( hw -> mac . type >= e1000_i210 ) <nl> + wr32 ( E1000_RQDPC ( i ), 0 ); <nl>  <nl> if ( rqdpc ) { <nl> ring -> rx_stats . drops += rqdpc ;
static int hid_scan_report ( struct hid_device * hid ) <nl> item . type == HID_ITEM_TYPE_MAIN && <nl> item . tag == HID_MAIN_ITEM_TAG_BEGIN_COLLECTION && <nl> ( item_udata (& item ) & 0xff ) == HID_COLLECTION_PHYSICAL && <nl> - hid -> bus == BUS_USB ) <nl> + ( hid -> bus == BUS_USB || hid -> bus == BUS_I2C )) <nl> hid -> group = HID_GROUP_SENSOR_HUB ; <nl> } <nl> 
static void rtl8169_hw_phy_config ( struct net_device * dev ) <nl> return ; <nl> } <nl>  <nl> - /* phy config for RTL8169s mac_version C chip */ <nl> + if (( tp -> mac_version != RTL_GIGA_MAC_VER_02 ) && <nl> + ( tp -> mac_version != RTL_GIGA_MAC_VER_03 )) <nl> + return ; <nl> + <nl> mdio_write ( ioaddr , 31 , 0x0001 ); // w 31 2 0 1 <nl> mdio_write ( ioaddr , 21 , 0x1000 ); // w 21 15 0 1000 <nl> mdio_write ( ioaddr , 24 , 0x65c7 ); // w 24 15 0 65c7
static int vxlan_newlink ( struct net * net , struct net_device * dev , <nl>  <nl> if (! tb [ IFLA_MTU ]) <nl> dev -> mtu = lowerdev -> mtu - VXLAN_HEADROOM ; <nl> + <nl> + /* update header length based on lower device */ <nl> + dev -> hard_header_len = lowerdev -> hard_header_len + <nl> + VXLAN_HEADROOM ; <nl> } <nl>  <nl> if ( data [ IFLA_VXLAN_TOS ])
static void magicmouse_setup_input ( struct input_dev * input , struct hid_device * h <nl> __set_bit ( BTN_TOOL_TRIPLETAP , input -> keybit ); <nl> __set_bit ( BTN_TOOL_QUADTAP , input -> keybit ); <nl> __set_bit ( BTN_TOUCH , input -> keybit ); <nl> + __set_bit ( INPUT_PROP_POINTER , input -> propbit ); <nl> + __set_bit ( INPUT_PROP_BUTTONPAD , input -> propbit ); <nl> } <nl>  <nl> if ( report_touches ) {
out0 : <nl>  <nl> static int __devexit nuc900_ac97_drvremove ( struct platform_device * pdev ) <nl> { <nl> - <nl> snd_soc_unregister_dai (& pdev -> dev ); <nl>  <nl> clk_put ( nuc900_ac97_data -> clk ); <nl> static int __devexit nuc900_ac97_drvremove ( struct platform_device * pdev ) <nl> release_mem_region ( nuc900_ac97_data -> res -> start , <nl> resource_size ( nuc900_ac97_data -> res )); <nl>  <nl> + kfree ( nuc900_ac97_data ); <nl> nuc900_ac97_data = NULL ; <nl>  <nl> return 0 ;
void __init m68k_setup_node ( int node ) <nl> */ <nl>  <nl> void * empty_zero_page ; <nl> + EXPORT_SYMBOL ( empty_zero_page ); <nl>  <nl> void show_mem ( void ) <nl> {
static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
static int davinci_spi_bufs_dma ( struct spi_device * spi , struct spi_transfer * t ) <nl>  <nl> data1_reg_val = ioread32 ( davinci_spi -> base + SPIDAT1 ); <nl>  <nl> - INIT_COMPLETION ( davinci_spi -> done ); <nl> - <nl> init_completion (& davinci_spi_dma -> dma_rx_completion ); <nl> init_completion (& davinci_spi_dma -> dma_tx_completion ); <nl> 
static int find_probes ( int fd , struct probe_finder * pf ) <nl> . file = pp -> file , <nl> . cu_die = & pf -> cu_die , <nl> . sp_die = & pf -> sp_die , <nl> + . found = 0 , <nl> }; <nl> struct dwarf_callback_param probe_param = { <nl> . data = pf ,
static int child_wait_callback ( wait_queue_t * wait , unsigned mode , <nl> if (! eligible_child ( wo , p )) <nl> return 0 ; <nl>  <nl> + if (( wo -> wo_flags & __WNOTHREAD ) && wait -> private != p -> parent ) <nl> + return 0 ; <nl> + <nl> return default_wake_function ( wait , mode , sync , key ); <nl> } <nl> 
struct omap_dss_output * omap_dss_get_output ( enum omap_dss_output_id id ) <nl>  <nl> return NULL ; <nl> } <nl> + EXPORT_SYMBOL ( omap_dss_get_output ); <nl>  <nl> static const struct dss_mgr_ops * dss_mgr_ops ; <nl> 
bnad_get_strings ( struct net_device * netdev , u32 stringset , u8 * string ) <nl> for ( i = 0 ; i < BNAD_ETHTOOL_STATS_NUM ; i ++) { <nl> BUG_ON (!( strlen ( bnad_net_stats_strings [ i ]) < <nl> ETH_GSTRING_LEN )); <nl> - memcpy ( string , bnad_net_stats_strings [ i ], <nl> - ETH_GSTRING_LEN ); <nl> + strncpy ( string , bnad_net_stats_strings [ i ], <nl> + ETH_GSTRING_LEN ); <nl> string += ETH_GSTRING_LEN ; <nl> } <nl> bmap = bna_tx_rid_mask (& bnad -> bna );
static int selinux_is_sblabel_mnt ( struct super_block * sb ) <nl> return sbsec -> behavior == SECURITY_FS_USE_XATTR || <nl> sbsec -> behavior == SECURITY_FS_USE_TRANS || <nl> sbsec -> behavior == SECURITY_FS_USE_TASK || <nl> + sbsec -> behavior == SECURITY_FS_USE_NATIVE || <nl> /* Special handling . Genfs but also in - core setxattr handler */ <nl> ! strcmp ( sb -> s_type -> name , " sysfs ") || <nl> ! strcmp ( sb -> s_type -> name , " pstore ") ||
static void vp_del_vq ( struct virtqueue * vq ) <nl> { <nl> struct virtio_pci_device * vp_dev = to_vp_device ( vq -> vdev ); <nl> struct virtio_pci_vq_info * info = vq -> priv ; <nl> - unsigned long size ; <nl> + unsigned long flags , size ; <nl> + <nl> + spin_lock_irqsave (& vp_dev -> lock , flags ); <nl> + list_del (& info -> node ); <nl> + spin_unlock_irqrestore (& vp_dev -> lock , flags ); <nl>  <nl> iowrite16 ( info -> queue_index , vp_dev -> ioaddr + VIRTIO_PCI_QUEUE_SEL ); <nl> 
static struct irq_chip puv3_low_gpio_chip = { <nl> * irq_controller_lock held , and IRQs disabled . Decode the IRQ <nl> * and call the handler . <nl> */ <nl> - static void <nl> - puv3_gpio_handler ( unsigned int irq , struct irq_desc * desc ) <nl> + static void puv3_gpio_handler ( unsigned int __irq , struct irq_desc * desc ) <nl> { <nl> - unsigned int mask ; <nl> + unsigned int mask , irq ; <nl>  <nl> mask = readl ( GPIO_GEDR ); <nl> do {
snd_m3_enable_ints ( struct snd_m3 * chip ) <nl> val = ASSP_INT_ENABLE /*| MPU401_INT_ENABLE */; <nl> if ( chip -> hv_config & HV_CTRL_ENABLE ) <nl> val |= HV_INT_ENABLE ; <nl> + outb ( val , chip -> iobase + HOST_INT_STATUS ); <nl> outw ( val , io + HOST_INT_CTRL ); <nl> outb ( inb ( io + ASSP_CONTROL_C ) | ASSP_HOST_INT_ENABLE , <nl> io + ASSP_CONTROL_C );
static int trusted_update ( struct key * key , const void * data , size_t datalen ) <nl> ret = datablob_parse ( datablob , new_p , new_o ); <nl> if ( ret != Opt_update ) { <nl> ret = - EINVAL ; <nl> + kfree ( new_p ); <nl> goto out ; <nl> } <nl> /* copy old key values , and reseal with new pcrs */
static ssize_t hugetlb_cgroup_write ( struct kernfs_open_file * of , <nl> ret = res_counter_memparse_write_strategy ( buf , & val ); <nl> if ( ret ) <nl> break ; <nl> + val = ALIGN ( val , 1ULL << huge_page_shift (& hstates [ idx ])); <nl> ret = res_counter_set_limit (& h_cg -> hugepage [ idx ], val ); <nl> break ; <nl> default :
int __init irttp_init ( void ) <nl> if (! irttp -> tsaps ) { <nl> IRDA_ERROR ("% s : can ' t allocate IrTTP hashbin !\ n ", <nl> __FUNCTION__ ); <nl> + kfree ( irttp ); <nl> return - ENOMEM ; <nl> } <nl> 
static int get_info ( struct net * net , void __user * user , <nl> private = & tmp ; <nl> } <nl> # endif <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . valid_hooks = t -> valid_hooks ; <nl> memcpy ( info . hook_entry , private -> hook_entry , <nl> sizeof ( info . hook_entry ));
visorchannel_create_guts ( u64 physaddr , unsigned long channel_bytes , <nl> int err ; <nl> size_t size = sizeof ( struct channel_header ); <nl>  <nl> + if ( physaddr == 0 ) <nl> + return NULL ; <nl> + <nl> channel = kzalloc ( sizeof (* channel ), gfp ); <nl> if (! channel ) <nl> goto cleanup ;
out : <nl> static int wl1271_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> { <nl> struct wl1271 * wl = hw -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> mutex_lock (& wl -> mutex ); <nl>  <nl> + if ( unlikely ( wl -> state == WL1271_STATE_OFF )) <nl> + goto out ; <nl> + <nl> ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> if ( ret < 0 ) <nl> goto out ;
static void i40evf_remove ( struct pci_dev * pdev ) <nl> i40evf_reset_interrupt_capability ( adapter ); <nl> } <nl>  <nl> - del_timer_sync (& adapter -> watchdog_timer ); <nl> + if ( adapter -> watchdog_timer . function ) <nl> + del_timer_sync (& adapter -> watchdog_timer ); <nl> + <nl> flush_scheduled_work (); <nl>  <nl> if ( hw -> aq . asq . count )
static int cache_create ( struct cache_args * ca , struct cache ** result ) <nl> atomic_set (& cache -> nr_migrations , 0 ); <nl> init_waitqueue_head (& cache -> migration_wait ); <nl>  <nl> + r = - ENOMEM ; <nl> cache -> nr_dirty = 0 ; <nl> cache -> dirty_bitset = alloc_bitset ( from_cblock ( cache -> cache_size )); <nl> if (! cache -> dirty_bitset ) {
static int ath9k_htc_aggr_oper ( struct ath9k_htc_priv * priv , <nl> int ret = 0 ; <nl> u8 cmd_rsp ; <nl>  <nl> - if ( tid > ATH9K_HTC_MAX_TID ) <nl> + if ( tid >= ATH9K_HTC_MAX_TID ) <nl> return - EINVAL ; <nl>  <nl> memset (& aggr , 0 , sizeof ( struct ath9k_htc_target_aggr ));
struct btrfs_root * btrfs_create_tree ( struct btrfs_trans_handle * trans , <nl> fail : <nl> if ( leaf ) { <nl> btrfs_tree_unlock ( leaf ); <nl> + free_extent_buffer ( root -> commit_root ); <nl> free_extent_buffer ( leaf ); <nl> } <nl> kfree ( root );
static int sony_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> } <nl>  <nl> ret = sony_set_operational ( hdev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_stop ; <nl>  <nl> return 0 ;
static int vpfe_open ( struct file * file ) <nl> if (! vpfe_dev -> initialized ) { <nl> if ( vpfe_initialize_device ( vpfe_dev )) { <nl> mutex_unlock (& vpfe_dev -> lock ); <nl> + v4l2_fh_exit (& fh -> fh ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> }
static int machine__process_kernel_mmap_event ( struct machine * machine , <nl> if ( __machine__create_kernel_maps ( machine , kernel ) < 0 ) <nl> goto out_problem ; <nl>  <nl> + if ( strstr ( dso -> long_name , " vmlinux ")) <nl> + dso__set_short_name ( dso , "[ kernel . vmlinux ]", false ); <nl> + <nl> machine__set_kernel_mmap_len ( machine , event ); <nl>  <nl> /*
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> loff_t file_size ; <nl> unsigned int num ; <nl> unsigned int offset ; <nl> - size_t total_len ; <nl> + size_t total_len = 0 ; <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req ))
void pcibios_free_controller ( struct pci_controller * phb ) <nl> if ( phb -> is_dynamic ) <nl> kfree ( phb ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( pcibios_free_controller ); <nl>  <nl> /* <nl> * The function is used to return the minimal alignment
void * devm_memremap ( struct device * dev , resource_size_t offset , <nl> if ( addr ) { <nl> * ptr = addr ; <nl> devres_add ( dev , ptr ); <nl> - } else <nl> + } else { <nl> devres_free ( ptr ); <nl> + return ERR_PTR (- ENXIO ); <nl> + } <nl>  <nl> return addr ; <nl> }
static void ath9k_hw_get_def_gain_boundaries_pdadcs ( struct ath_hw * ah , <nl> vpdTableI [ i ][ sizeCurrVpdTable - 2 ]); <nl> vpdStep = ( int16_t )(( vpdStep < 1 ) ? 1 : vpdStep ); <nl>  <nl> - if ( tgtIndex > maxIndex ) { <nl> + if ( tgtIndex >= maxIndex ) { <nl> while (( ss <= tgtIndex ) && <nl> ( k < ( AR5416_NUM_PDADC_VALUES - 1 ))) { <nl> tmpVal = ( int16_t )(( vpdTableI [ i ][ sizeCurrVpdTable - 1 ] +
static int rtd520_probe_fifo_depth ( comedi_device * dev ) <nl> return - EIO ; <nl> } <nl> RtdAdcClearFifo ( dev ); <nl> - if ( fifo_size != 0x400 || fifo_size != 0x2000 ) <nl> + if ( fifo_size != 0x400 && fifo_size != 0x2000 ) <nl> { <nl> rt_printk ("\ ncomedi : % s : unexpected fifo size of % i , expected 1024 or 8192 .\ n ", <nl> DRV_NAME , fifo_size );
static void alc269_fill_coef ( struct hda_codec * codec ) <nl>  <nl> if ( spec -> codec_variant != ALC269_TYPE_ALC269VB ) <nl> return ; <nl> + /* ALC271X doesn ' t seem to support these COEFs ( bko # 52181 ) */ <nl> + if (! strcmp ( codec -> chip_name , " ALC271X ")) <nl> + return ; <nl>  <nl> if (( alc_get_coef0 ( codec ) & 0x00ff ) < 0x015 ) { <nl> alc_write_coef_idx ( codec , 0xf , 0x960b );
svga3dsurface_get_mip_size ( surf_size_struct base_level , u32 mip_level ) <nl> size . width = max_t ( u32 , base_level . width >> mip_level , 1 ); <nl> size . height = max_t ( u32 , base_level . height >> mip_level , 1 ); <nl> size . depth = max_t ( u32 , base_level . depth >> mip_level , 1 ); <nl> + size . pad64 = 0 ; <nl> + <nl> return size ; <nl> } <nl> 
static int soc_camera_close ( struct file * file ) <nl> pm_runtime_suspend (& icd -> vdev -> dev ); <nl> pm_runtime_disable (& icd -> vdev -> dev ); <nl>  <nl> - ici -> ops -> remove ( icd ); <nl> if ( ici -> ops -> init_videobuf2 ) <nl> vb2_queue_release (& icd -> vb2_vidq ); <nl> + ici -> ops -> remove ( icd ); <nl>  <nl> soc_camera_power_off ( icd , icl ); <nl> }
static int __init s3c2410fb_probe ( struct platform_device * pdev ) <nl>  <nl> info = fbinfo -> par ; <nl> info -> fb = fbinfo ; <nl> + info -> dev = & pdev -> dev ; <nl> + <nl> platform_set_drvdata ( pdev , fbinfo ); <nl>  <nl> dprintk (" devinit \ n ");
mwifiex_11n_aggregate_pkt ( struct mwifiex_private * priv , <nl> tx_info_aggr -> flags |= MWIFIEX_BUF_FLAG_TDLS_PKT ; <nl> tx_info_aggr -> flags |= MWIFIEX_BUF_FLAG_AGGR_PKT ; <nl> skb_aggr -> priority = skb_src -> priority ; <nl> + skb_aggr -> tstamp = skb_src -> tstamp ; <nl>  <nl> do_gettimeofday (& tv ); <nl> skb_aggr -> tstamp = timeval_to_ktime ( tv );
static int __init longhaul_cpu_init ( struct cpufreq_policy * policy ) <nl> if ( pr == NULL ) goto err_acpi ; <nl>  <nl> cx = & pr -> power . states [ ACPI_STATE_C3 ]; <nl> - if ( cx == NULL || cx -> latency > 1000 ) goto err_acpi ; <nl> + if ( cx -> address == 0 || cx -> latency > 1000 ) goto err_acpi ; <nl>  <nl> /* Now check what we have on this motherboard */ <nl> switch ( c -> x86_model ) {
ssize_t lirc_dev_fop_read ( struct file * file , <nl> return - ENODEV ; <nl> } <nl>  <nl> + if (! LIRC_CAN_REC ( ir -> d . features )) <nl> + return - EINVAL ; <nl> + <nl> dev_dbg ( ir -> d . dev , LOGHEAD " read called \ n ", ir -> d . name , ir -> d . minor ); <nl>  <nl> buf = kzalloc ( ir -> chunk_size , GFP_KERNEL );
again : <nl> } <nl> spin_unlock (& fs_info -> reada_lock ); <nl>  <nl> + kfree ( multi ); <nl> return re ; <nl>  <nl> error : <nl> error : <nl> kref_put (& zone -> refcnt , reada_zone_release ); <nl> spin_unlock (& fs_info -> reada_lock ); <nl> } <nl> + kfree ( multi ); <nl> kfree ( re ); <nl> if ( looped ) <nl> goto again ;
static void usb_alphatrack_disconnect ( struct usb_interface * intf ) <nl> mutex_unlock (& dev -> mtx ); <nl> usb_alphatrack_delete ( dev ); <nl> } else { <nl> + atomic_set (& dev -> writes_pending , 0 ); <nl> dev -> intf = NULL ; <nl> mutex_unlock (& dev -> mtx ); <nl> } <nl>  <nl> - atomic_set (& dev -> writes_pending , 0 ); <nl> mutex_unlock (& disconnect_mutex ); <nl>  <nl> dev_info (& intf -> dev , " Alphatrack Surface #% d now disconnected \ n ",
static int xhci_plat_probe ( struct platform_device * pdev ) <nl> ret = clk_prepare_enable ( clk ); <nl> if ( ret ) <nl> goto put_hcd ; <nl> + } else if ( PTR_ERR ( clk ) == - EPROBE_DEFER ) { <nl> + ret = - EPROBE_DEFER ; <nl> + goto put_hcd ; <nl> } <nl>  <nl> xhci = hcd_to_xhci ( hcd );
static int super_written ( struct bio * bio , unsigned int bytes_done , int error ) <nl>  <nl> if ( atomic_dec_and_test (& rdev -> mddev -> pending_writes )) <nl> wake_up (& rdev -> mddev -> sb_wait ); <nl> + bio_put ( bio ); <nl> return 0 ; <nl> } <nl> 
static struct pci_device_id agp_sis_pci_table [] = { <nl> . subvendor = PCI_ANY_ID , <nl> . subdevice = PCI_ANY_ID , <nl> }, <nl> - { <nl> - . class = ( PCI_CLASS_BRIDGE_HOST << 8 ), <nl> - . class_mask = ~ 0 , <nl> - . vendor = PCI_VENDOR_ID_SI , <nl> - . device = PCI_DEVICE_ID_SI_760 , <nl> - . subvendor = PCI_ANY_ID , <nl> - . subdevice = PCI_ANY_ID , <nl> - }, <nl> { } <nl> }; <nl> 
EXPORT_SYMBOL ( set_memory_array_uc ); <nl>  <nl> int _set_memory_wc ( unsigned long addr , int numpages ) <nl> { <nl> - return change_page_attr_set (& addr , numpages , <nl> + int ret ; <nl> + ret = change_page_attr_set (& addr , numpages , <nl> + __pgprot ( _PAGE_CACHE_UC_MINUS ), 0 ); <nl> + <nl> + if (! ret ) { <nl> + ret = change_page_attr_set (& addr , numpages , <nl> __pgprot ( _PAGE_CACHE_WC ), 0 ); <nl> + } <nl> + return ret ; <nl> } <nl>  <nl> int set_memory_wc ( unsigned long addr , int numpages )
void iwl_irq_tasklet ( struct iwl_trans * trans ) <nl> } <nl> # endif <nl>  <nl> - spin_unlock_irqrestore (& trans -> shrd -> lock , flags ); <nl> - <nl> /* saved interrupt in inta variable now we can reset trans_pcie -> inta */ <nl> trans_pcie -> inta = 0 ; <nl>  <nl> + spin_unlock_irqrestore (& trans -> shrd -> lock , flags ); <nl> + <nl> /* Now service all interrupt bits discovered above . */ <nl> if ( inta & CSR_INT_BIT_HW_ERR ) { <nl> IWL_ERR ( trans , " Hardware error detected . Restarting .\ n ");
void sun4c_update_mmu_cache ( struct vm_area_struct * vma , unsigned long address , p <nl> unsigned long flags ; <nl> int pseg ; <nl>  <nl> + if ( vma -> vm_mm -> context == NO_CONTEXT ) <nl> + return ; <nl> + <nl> local_irq_save ( flags ); <nl> address &= PAGE_MASK ; <nl> if (( pseg = sun4c_get_segmap ( address )) == invalid_segment ) {
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> } <nl> ++ p ; <nl> } <nl> + /* limit mclk on all R7 370 parts for stability */ <nl> + if ( rdev -> pdev -> device == 0x6811 && <nl> + rdev -> pdev -> revision == 0x81 ) <nl> + max_mclk = 120000 ; <nl>  <nl> if ( rps -> vce_active ) { <nl> rps -> evclk = rdev -> pm . dpm . vce_states [ rdev -> pm . dpm . vce_level ]. evclk ;
static int dac33_set_bias_level ( struct snd_soc_codec * codec , <nl> } <nl> break ; <nl> case SND_SOC_BIAS_OFF : <nl> + /* Do not power off , when the codec is already off */ <nl> + if ( codec -> bias_level == SND_SOC_BIAS_OFF ) <nl> + return 0 ; <nl> ret = dac33_hard_power ( codec , 0 ); <nl> if ( ret != 0 ) <nl> return ret ;
static netdev_tx_t eth_start_xmit ( struct sk_buff * skb , <nl> /* Multi frame CDC protocols may store the frame for <nl> * later which is not a dropped frame . <nl> */ <nl> - if ( dev -> port_usb -> supports_multi_frame ) <nl> + if ( dev -> port_usb && <nl> + dev -> port_usb -> supports_multi_frame ) <nl> goto multiframe ; <nl> goto drop ; <nl> }
static struct machine * machines__find_for_cpumode ( struct machines * machines , <nl>  <nl> machine = machines__find ( machines , pid ); <nl> if (! machine ) <nl> - machine = machines__find ( machines , DEFAULT_GUEST_KERNEL_ID ); <nl> + machine = machines__findnew ( machines , DEFAULT_GUEST_KERNEL_ID ); <nl> return machine ; <nl> } <nl> 
int simple_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl>  <nl> if ( new_dentry -> d_inode ) { <nl> simple_unlink ( new_dir , new_dentry ); <nl> - if ( they_are_dirs ) <nl> + if ( they_are_dirs ) { <nl> + drop_nlink ( new_dentry -> d_inode ); <nl> drop_nlink ( old_dir ); <nl> + } <nl> } else if ( they_are_dirs ) { <nl> drop_nlink ( old_dir ); <nl> inc_nlink ( new_dir );
static int update_nodemask ( struct cpuset * cs , struct cpuset * trialcs , <nl> spin_unlock_irq (& callback_lock ); <nl>  <nl> /* use trialcs -> mems_allowed as a temp variable */ <nl> - update_nodemasks_hier ( cs , & cs -> mems_allowed ); <nl> + update_nodemasks_hier ( cs , & trialcs -> mems_allowed ); <nl> done : <nl> return retval ; <nl> }
static unsigned long nid_range ( unsigned long start , unsigned long end , <nl> start += PAGE_SIZE ; <nl> } <nl>  <nl> + if ( start > end ) <nl> + start = end ; <nl> + <nl> return start ; <nl> } <nl> # else
out : <nl> } <nl>  <nl> cl_env_put ( env , & refcheck ); <nl> - return tot_bytes ? : result ; <nl> + return tot_bytes ? tot_bytes : result ; <nl> } <nl>  <nl> /**
static int ixgbe_open ( struct net_device * netdev ) <nl> int err ; <nl> u32 num_rx_queues = adapter -> num_rx_queues ; <nl>  <nl> + /* disallow open during test */ <nl> + if ( test_bit ( __IXGBE_TESTING , & adapter -> state )) <nl> + return - EBUSY ; <nl> + <nl> try_intr_reinit : <nl> /* allocate transmit descriptors */ <nl> err = ixgbe_setup_all_tx_resources ( adapter );
cifs_setlk ( struct file * file , struct file_lock * flock , __u32 type , <nl> rc = server -> ops -> mand_unlock_range ( cfile , flock , xid ); <nl>  <nl> out : <nl> - if ( flock -> fl_flags & FL_POSIX ) <nl> - posix_lock_file_wait ( file , flock ); <nl> + if ( flock -> fl_flags & FL_POSIX && ! rc ) <nl> + rc = posix_lock_file_wait ( file , flock ); <nl> return rc ; <nl> } <nl> 
int p9dirent_read ( char * buf , int len , struct p9_dirent * dirent , <nl> } <nl>  <nl> strcpy ( dirent -> d_name , nameptr ); <nl> + kfree ( nameptr ); <nl>  <nl> out : <nl> return fake_pdu . offset ;
static int socrates_nand_probe ( struct platform_device * ofdev ) <nl> nand_release ( mtd ); <nl>  <nl> out : <nl> - dev_set_drvdata (& ofdev -> dev , NULL ); <nl> iounmap ( host -> io_base ); <nl> kfree ( host ); <nl> return res ; <nl> static int socrates_nand_remove ( struct platform_device * ofdev ) <nl>  <nl> nand_release ( mtd ); <nl>  <nl> - dev_set_drvdata (& ofdev -> dev , NULL ); <nl> iounmap ( host -> io_base ); <nl> kfree ( host ); <nl> 
static int ch7006_encoder_create_resources ( struct drm_encoder * encoder , <nl> drm_mode_create_tv_properties ( dev , NUM_TV_NORMS , ch7006_tv_norm_names ); <nl>  <nl> priv -> scale_property = drm_property_create_range ( dev , 0 , " scale ", 0 , 2 ); <nl> + if (! priv -> scale_property ) <nl> + return - ENOMEM ; <nl>  <nl> drm_object_attach_property (& connector -> base , conf -> tv_select_subconnector_property , <nl> priv -> select_subconnector );
static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
static int get_exec_file ( struct cfg_devnode * dev_node_obj , <nl> if (! drv_datap || ! drv_datap -> base_img ) <nl> return - EFAULT ; <nl>  <nl> - if ( strlen ( drv_datap -> base_img ) > size ) <nl> + if ( strlen ( drv_datap -> base_img ) >= size ) <nl> return - EINVAL ; <nl>  <nl> strcpy ( exec_file , drv_datap -> base_img );
static void dump_dev_cap_flags ( struct mlx4_dev * dev , u32 flags ) <nl> int i ; <nl>  <nl> mlx4_dbg ( dev , " DEV_CAP flags :\ n "); <nl> - for ( i = 0 ; i < 32 ; ++ i ) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( fname ); ++ i ) <nl> if ( fname [ i ] && ( flags & ( 1 << i ))) <nl> mlx4_dbg ( dev , " % s \ n ", fname [ i ]); <nl> }
static int iwlagn_mac_sta_add ( struct ieee80211_hw * hw , <nl> { <nl> struct iwl_priv * priv = hw -> priv ; <nl> struct iwl_station_priv * sta_priv = ( void *) sta -> drv_priv ; <nl> - bool is_ap = priv -> iw_mode == NL80211_IFTYPE_STATION ; <nl> + bool is_ap = vif -> type == NL80211_IFTYPE_STATION ; <nl> int ret ; <nl> u8 sta_id ; <nl> 
struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
int hardif_add_interface ( char * dev , int if_num ) <nl> return 1 ; <nl>  <nl> out : <nl> - if ( batman_if -> packet_buff ) <nl> - kfree ( batman_if -> packet_buff ); <nl> + kfree ( batman_if -> packet_buff ); <nl> kfree ( batman_if ); <nl> kfree ( dev ); <nl> return - 1 ;
static int em28xx_dvb_init ( struct em28xx * dev ) <nl> dvb -> i2c_client_demod = client ; <nl>  <nl> /* attach tuner */ <nl> + memset (& si2157_config , 0 , sizeof ( si2157_config )); <nl> si2157_config . fe = dvb -> fe [ 0 ]; <nl> memset (& info , 0 , sizeof ( struct i2c_board_info )); <nl> strlcpy ( info . type , " si2157 ", I2C_NAME_SIZE );
static int __init pwrdms_setup ( struct powerdomain * pwrdm ) <nl> if (! pwrdm -> pwrsts ) <nl> return 0 ; <nl>  <nl> - pwrst = kmalloc ( sizeof ( struct power_state ), GFP_KERNEL ); <nl> + pwrst = kmalloc ( sizeof ( struct power_state ), GFP_ATOMIC ); <nl> if (! pwrst ) <nl> return - ENOMEM ; <nl> pwrst -> pwrdm = pwrdm ;
static int parse_addr ( const struct nf_conn * ct , const char * cp , <nl> int family = ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . l3num ; <nl> int ret = 0 ; <nl>  <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> switch ( family ) { <nl> case AF_INET : <nl> ret = in4_pton ( cp , limit - cp , ( u8 *)& addr -> ip , - 1 , & end );
xfs_fs_geometry ( <nl> xfs_fsop_geom_t * geo , <nl> int new_version ) <nl> { <nl> + <nl> + memset ( geo , 0 , sizeof (* geo )); <nl> + <nl> geo -> blocksize = mp -> m_sb . sb_blocksize ; <nl> geo -> rtextsize = mp -> m_sb . sb_rextsize ; <nl> geo -> agblocks = mp -> m_sb . sb_agblocks ;
static loff_t fuse_file_llseek ( struct file * file , loff_t offset , int origin ) <nl> mutex_lock (& inode -> i_mutex ); <nl> switch ( origin ) { <nl> case SEEK_END : <nl> + retval = fuse_update_attributes ( inode , NULL , file , NULL ); <nl> + if ( retval ) <nl> + return retval ; <nl> offset += i_size_read ( inode ); <nl> break ; <nl> case SEEK_CUR :
int ceph_init_dentry ( struct dentry * dentry ) <nl> return - ENOMEM ; /* oh well */ <nl>  <nl> spin_lock (& dentry -> d_lock ); <nl> - if ( dentry -> d_fsdata ) /* lost a race */ <nl> + if ( dentry -> d_fsdata ) { <nl> + /* lost a race */ <nl> + kmem_cache_free ( ceph_dentry_cachep , di ); <nl> goto out_unlock ; <nl> + } <nl> di -> dentry = dentry ; <nl> di -> lease_session = NULL ; <nl> dentry -> d_fsdata = di ;
static int wm8904_remove ( struct snd_soc_codec * codec ) <nl>  <nl> wm8904_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8904 -> supplies ), wm8904 -> supplies ); <nl> + kfree ( wm8904 -> retune_mobile_texts ); <nl> + kfree ( wm8904 -> drc_texts ); <nl>  <nl> return 0 ; <nl> }
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
static int i915_drm_thaw ( struct drm_device * dev ) <nl> drm_irq_install ( dev ); <nl>  <nl> /* Resume the modeset for every activated CRTC */ <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> drm_helper_resume_force_mode ( dev ); <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl>  <nl> if ( IS_IRONLAKE_M ( dev )) <nl> ironlake_enable_rc6 ( dev );
static int hgcm_call_preprocess_linaddr ( <nl> if (! bounce_buf ) <nl> return - ENOMEM ; <nl>  <nl> + * bounce_buf_ret = bounce_buf ; <nl> + <nl> if ( copy_in ) { <nl> ret = copy_from_user ( bounce_buf , ( void __user *) buf , len ); <nl> if ( ret ) <nl> static int hgcm_call_preprocess_linaddr ( <nl> memset ( bounce_buf , 0 , len ); <nl> } <nl>  <nl> - * bounce_buf_ret = bounce_buf ; <nl> hgcm_call_add_pagelist_size ( bounce_buf , len , extra ); <nl> return 0 ; <nl> }
static int drm_queue_vblank_event ( struct drm_device * dev , int pipe , <nl> if (( vblwait -> request . type & _DRM_VBLANK_NEXTONMISS ) && <nl> ( seq - vblwait -> request . sequence ) <= ( 1 << 23 )) { <nl> vblwait -> request . sequence = seq + 1 ; <nl> + vblwait -> reply . sequence = vblwait -> request . sequence ; <nl> } <nl>  <nl> DRM_DEBUG (" event on vblank count % d , current % d , crtc % d \ n ",
int mwifiex_ret_wmm_get_status ( struct mwifiex_private * priv , <nl> " WMM Parameter Set Count : % d \ n ", <nl> wmm_param_ie -> qos_info_bitmap & mask ); <nl>  <nl> + if ( wmm_param_ie -> vend_hdr . len + 2 > <nl> + sizeof ( struct ieee_types_wmm_parameter )) <nl> + break ; <nl> + <nl> memcpy (( u8 *) & priv -> curr_bss_params . bss_descriptor . <nl> wmm_ie , wmm_param_ie , <nl> wmm_param_ie -> vend_hdr . len + 2 );
static char * res_strings [] = { <nl> " reserved 37 ", <nl> " reserved 38 ", <nl> " reserved 39 ", <nl> - " reseverd 40 ", <nl> + " reserved 40 ", <nl> " reserved 41 ", <nl> " reserved 42 ", <nl> " reserved 43 ",
static enum dma_status rcar_dmac_tx_status ( struct dma_chan * chan , <nl> residue = rcar_dmac_chan_get_residue ( rchan , cookie ); <nl> spin_unlock_irqrestore (& rchan -> lock , flags ); <nl>  <nl> + /* if there ' s no residue , the cookie is complete */ <nl> + if (! residue ) <nl> + return DMA_COMPLETE ; <nl> + <nl> dma_set_residue ( txstate , residue ); <nl>  <nl> return status ;
static int ssm4567_set_power ( struct ssm4567 * ssm4567 , bool enable ) <nl> regcache_cache_only ( ssm4567 -> regmap , ! enable ); <nl>  <nl> if ( enable ) { <nl> + ret = regmap_write ( ssm4567 -> regmap , SSM4567_REG_SOFT_RESET , <nl> + 0x00 ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> ret = regmap_update_bits ( ssm4567 -> regmap , <nl> SSM4567_REG_POWER_CTRL , <nl> SSM4567_POWER_SPWDN , 0x00 );
int megasas_alloc_cmds ( struct megasas_instance * instance ) <nl> if ( megasas_create_frame_pool ( instance )) { <nl> dev_printk ( KERN_DEBUG , & instance -> pdev -> dev , " Error creating frame DMA pool \ n "); <nl> megasas_free_cmds ( instance ); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> return 0 ;
static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
void free_user_ns ( struct kref * kref ) <nl> struct user_namespace * ns ; <nl>  <nl> ns = container_of ( kref , struct user_namespace , kref ); <nl> + free_uid ( ns -> root_user ); <nl> kfree ( ns ); <nl> } <nl> 
void __init paging_init ( void ) <nl>  <nl> mem_map = NODE_DATA ( 0 )-> node_mem_map ; <nl>  <nl> - memset ( zero_page , 0 , PAGE_SIZE ); <nl> empty_zero_page = virt_to_page ( zero_page ); <nl> flush_dcache_page ( empty_zero_page ); <nl> }
static void free_pcppages_bulk ( struct zone * zone , int count , <nl> list = & pcp -> lists [ migratetype ]; <nl> } while ( list_empty ( list )); <nl>  <nl> + /* This is the only non - empty list . Free them all . */ <nl> + if ( batch_free == MIGRATE_PCPTYPES ) <nl> + batch_free = to_free ; <nl> + <nl> do { <nl> page = list_entry ( list -> prev , struct page , lru ); <nl> /* must delete as __free_one_page list manipulates */
static int do_ipv6_setsockopt ( struct sock * sk , int level , int optname , <nl> break ; <nl>  <nl> case IPV6_TRANSPARENT : <nl> + if (! capable ( CAP_NET_ADMIN )) { <nl> + retv = - EPERM ; <nl> + break ; <nl> + } <nl> if ( optlen < sizeof ( int )) <nl> goto e_inval ; <nl> /* we don ' t have a separate transparent bit for IPV6 we use the one in the IPv4 socket */
static noinline int cow_file_range_inline ( struct btrfs_root * root , <nl> data_len = compressed_size ; <nl>  <nl> if ( start > 0 || <nl> - actual_end >= PAGE_CACHE_SIZE || <nl> - data_len >= BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> + actual_end > PAGE_CACHE_SIZE || <nl> + data_len > BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> (! compressed_size && <nl> ( actual_end & ( root -> sectorsize - 1 )) == 0 ) || <nl> end + 1 < isize ||
static int xl_open ( struct net_device * dev ) <nl> if ( i == 0 ) { <nl> printk ( KERN_WARNING "% s : Not enough memory to allocate rx buffers . Adapter disabled \ n ", dev -> name ) ; <nl> free_irq ( dev -> irq , dev ) ; <nl> + kfree ( xl_priv -> xl_tx_ring ); <nl> + kfree ( xl_priv -> xl_rx_ring ); <nl> return - EIO ; <nl> } <nl> 
static ssize_t cifsFYI_proc_write ( struct file * file , const char __user * buffer , <nl> cifsFYI = bv ; <nl> else if (( c [ 0 ] > ' 1 ') && ( c [ 0 ] <= ' 9 ')) <nl> cifsFYI = ( int ) ( c [ 0 ] - ' 0 '); /* see cifs_debug . h for meanings */ <nl> + else <nl> + return - EINVAL ; <nl>  <nl> return count ; <nl> }
static struct kioctx * ioctx_alloc ( unsigned nr_events ) <nl> err_cleanup : <nl> aio_nr_sub ( ctx -> max_reqs ); <nl> err : <nl> - aio_free_ring ( ctx ); <nl> free_percpu ( ctx -> cpu ); <nl> free_percpu ( ctx -> reqs . pcpu_count ); <nl> free_percpu ( ctx -> users . pcpu_count );
void __init init_IRQ ( void ) <nl> struct irq_desc * desc ; <nl> int irq ; <nl>  <nl> - for ( irq = 0 ; irq < nr_irqs ; irq ++) <nl> + for ( irq = 0 ; irq < nr_irqs ; irq ++) { <nl> + desc = irq_to_desc_alloc_node ( irq , 0 ); <nl> desc -> status |= IRQ_NOREQUEST | IRQ_NOPROBE ; <nl> + } <nl>  <nl> init_arch_irq (); <nl> }
static int yam_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> break ; <nl>  <nl> case SIOCYAMGCFG : <nl> + memset (& yi , 0 , sizeof ( yi )); <nl> yi . cfg . mask = 0xffffffff ; <nl> yi . cfg . iobase = yp -> iobase ; <nl> yi . cfg . irq = yp -> irq ;
static struct cpuidle_state s3c64xx_cpuidle_set [] = { <nl> [ 0 ] = { <nl> . enter = s3c64xx_enter_idle , <nl> . exit_latency = 1 , <nl> - . target_residency = 100000 , <nl> + . target_residency = 1 , <nl> . flags = CPUIDLE_FLAG_TIME_VALID , <nl> . name = " IDLE ", <nl> . desc = " System active , ARM gated ",
static int mv643xx_eth_shared_probe ( struct platform_device * pdev ) <nl> * Detect hardware parameters . <nl> */ <nl> msp -> t_clk = ( pd != NULL && pd -> t_clk != 0 ) ? pd -> t_clk : 133000000 ; <nl> - msp -> tx_csum_limit = pd -> tx_csum_limit ? pd -> tx_csum_limit : 9 * 1024 ; <nl> + msp -> tx_csum_limit = ( pd != NULL && pd -> tx_csum_limit ) ? <nl> + pd -> tx_csum_limit : 9 * 1024 ; <nl> infer_hw_params ( msp ); <nl>  <nl> platform_set_drvdata ( pdev , msp );
int ring_buffer_resize ( struct ring_buffer * buffer , unsigned long size ) <nl> list_del_init (& page -> list ); <nl> free_buffer_page ( page ); <nl> } <nl> + mutex_unlock (& buffer -> mutex ); <nl> return - ENOMEM ; <nl> } <nl> 
static void igmp_heard_query ( struct in_device * in_dev , struct sk_buff * skb , <nl> * to be intended in a v3 query . <nl> */ <nl> max_delay = IGMPV3_MRC ( ih3 -> code )*( HZ / IGMP_TIMER_SCALE ); <nl> + if (! max_delay ) <nl> + max_delay = 1 ; /* can ' t mod w / 0 */ <nl> } else { /* v3 */ <nl> if (! pskb_may_pull ( skb , sizeof ( struct igmpv3_query ))) <nl> return ;
static int tegra_kbc_start ( struct tegra_kbc * kbc ) <nl> /* Reset the KBC controller to clear all previous status .*/ <nl> reset_control_assert ( kbc -> rst ); <nl> udelay ( 100 ); <nl> - reset_control_assert ( kbc -> rst ); <nl> + reset_control_deassert ( kbc -> rst ); <nl> udelay ( 100 ); <nl>  <nl> tegra_kbc_config_pins ( kbc );
static int __devexit ssm2602_i2c_remove ( struct i2c_client * client ) <nl>  <nl> static const struct i2c_device_id ssm2602_i2c_id [] = { <nl> { " ssm2602 ", SSM2602 }, <nl> + { " ssm2603 ", SSM2602 }, <nl> { " ssm2604 ", SSM2604 }, <nl> { } <nl> }; <nl> static void __exit ssm2602_exit ( void ) <nl> } <nl> module_exit ( ssm2602_exit ); <nl>  <nl> - MODULE_DESCRIPTION (" ASoC SSM2602 / SSM2604 driver "); <nl> + MODULE_DESCRIPTION (" ASoC SSM2602 / SSM2603 / SSM2604 driver "); <nl> MODULE_AUTHOR (" Cliff Cai "); <nl> MODULE_LICENSE (" GPL ");
int BcmGetSectionValEndOffset ( struct bcm_mini_adapter * Adapter , enum bcm_flash2x <nl> case CONTROL_SECTION : <nl> /* Not Clear So Putting failure . confirm and fix it . */ <nl> SectEndOffset = STATUS_FAILURE ; <nl> + break ; <nl> case ISO_IMAGE1_PART2 : <nl> if ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End != UNINIT_PTR_IN_CS ) <nl> SectEndOffset = ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End );
static int __init net_ns_init ( void ) <nl>  <nl> register_pernet_subsys (& net_ns_ops ); <nl>  <nl> - rtnl_register ( PF_UNSPEC , RTM_NEWNSID , rtnl_net_newid , NULL , 0 ); <nl> + rtnl_register ( PF_UNSPEC , RTM_NEWNSID , rtnl_net_newid , NULL , <nl> + RTNL_FLAG_DOIT_UNLOCKED ); <nl> rtnl_register ( PF_UNSPEC , RTM_GETNSID , rtnl_net_getid , rtnl_net_dumpid , <nl> - 0 ); <nl> + RTNL_FLAG_DOIT_UNLOCKED ); <nl>  <nl> return 0 ; <nl> }
qla24xx_report_id_acquisition ( scsi_qla_host_t * vha , <nl> if ( vp_idx == 0 && ( MSB ( stat ) != 1 )) <nl> goto reg_needed ; <nl>  <nl> - if ( MSB ( stat ) == 1 ) { <nl> + if ( MSB ( stat ) != 0 ) { <nl> ql_dbg ( ql_dbg_mbx , vha , 0x10ba , <nl> " Could not acquire ID for VP [% d ].\ n ", vp_idx ); <nl> return ;
static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
struct fb_info * fbtft_framebuffer_alloc ( struct fbtft_display * display , <nl> /* Transmit buffer */ <nl> if ( txbuflen == - 1 ) <nl> txbuflen = vmem_size + 2 ; /* add in case startbyte is used */ <nl> + if ( txbuflen >= vmem_size + 2 ) <nl> + txbuflen = 0 ; <nl>  <nl> # ifdef __LITTLE_ENDIAN <nl> if ((! txbuflen ) && ( bpp > 8 ))
static int sof_set_get_large_ctrl_data ( struct snd_sof_dev * sdev , <nl> else <nl> err = sof_get_ctrl_copy_params ( cdata -> type , partdata , cdata , <nl> sparams ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree ( partdata ); <nl> return err ; <nl> + } <nl>  <nl> msg_bytes = sparams -> msg_bytes ; <nl> pl_size = sparams -> pl_size ;
static struct aead_edesc * aead_giv_edesc_alloc ( struct aead_givcrypt_request <nl> assoc_nents = assoc_nents ? : 1 ; <nl> src_nents = src_nents ? : 1 ; <nl> sec4_sg_len += assoc_nents + 1 + src_nents ; <nl> - if ( likely ( req -> src == req -> dst )) <nl> + if ( req -> src == req -> dst && <nl> + ( src_nents || iv_dma + ivsize != sg_dma_address ( req -> src ))) <nl> contig &= ~ GIV_DST_CONTIG ; <nl> } <nl> 
static int snd_seq_ioctl_remove_events ( struct snd_seq_client * client , <nl> * No restrictions so for a user client we can clear <nl> * the whole fifo <nl> */ <nl> - if ( client -> type == USER_CLIENT ) <nl> + if ( client -> type == USER_CLIENT && client -> data . user . fifo ) <nl> snd_seq_fifo_clear ( client -> data . user . fifo ); <nl> } <nl> 
static int vc4_plane_mode_set ( struct drm_plane * plane , <nl> /* Control word */ <nl> vc4_dlist_write ( vc4_state , <nl> SCALER_CTL0_VALID | <nl> + VC4_SET_FIELD ( SCALER_CTL0_RGBA_EXPAND_ROUND , SCALER_CTL0_RGBA_EXPAND ) | <nl> ( format -> pixel_order << SCALER_CTL0_ORDER_SHIFT ) | <nl> ( format -> hvs << SCALER_CTL0_PIXEL_FORMAT_SHIFT ) | <nl> VC4_SET_FIELD ( tiling , SCALER_CTL0_TILING ) |
static int fd_ioctl ( struct block_device * bdev , fmode_t mode , unsigned int cmd , <nl> (( cmd & 0x80 ) && ! capable ( CAP_SYS_ADMIN ))) <nl> return - EPERM ; <nl>  <nl> + if ( WARN_ON ( size < 0 || size > sizeof ( inparam ))) <nl> + return - EINVAL ; <nl> + <nl> /* copyin */ <nl> CLEARSTRUCT (& inparam ); <nl> if ( _IOC_DIR ( cmd ) & _IOC_WRITE )
static void e1000_get_ethtool_stats ( struct net_device * netdev , <nl> p = ( char *) adapter + <nl> e1000_gstrings_stats [ i ]. stat_offset ; <nl> break ; <nl> + default : <nl> + data [ i ] = 0 ; <nl> + continue ; <nl> } <nl>  <nl> data [ i ] = ( e1000_gstrings_stats [ i ]. sizeof_stat ==
static int pn533_target_found ( struct pn533 * dev , <nl> if ( resp -> tg != 1 ) <nl> return - EPROTO ; <nl>  <nl> + memset (& nfc_tgt , 0 , sizeof ( struct nfc_target )); <nl> + <nl> target_data_len = resp_len - sizeof ( struct pn533_poll_response ); <nl>  <nl> switch ( dev -> poll_mod_curr ) {
static int cp2112_gpio_direction_input ( struct gpio_chip * chip , unsigned offset ) <nl>  <nl> exit : <nl> mutex_unlock (& dev -> lock ); <nl> - return ret <= 0 ? ret : - EIO ; <nl> + return ret < 0 ? ret : - EIO ; <nl> } <nl>  <nl> static void cp2112_gpio_set ( struct gpio_chip * chip , unsigned offset , int value )
struct ehci_hcd { /* one per controller */ <nl> # ifdef DEBUG <nl> struct dentry * debug_dir ; <nl> # endif <nl> + <nl> + /* platform - specific data -- must come last */ <nl> + unsigned long priv [ 0 ] __aligned ( sizeof ( s64 )); <nl> }; <nl>  <nl> /* convert between an HCD pointer and the corresponding EHCI_HCD */
static int emac_poll_rx ( void * param , int budget ) <nl> goto next ; <nl> } <nl>  <nl> + if ( len < ETH_HLEN ) { <nl> + ++ dev -> estats . rx_dropped_stack ; <nl> + emac_recycle_rx_skb ( dev , slot , len ); <nl> + goto next ; <nl> + } <nl> + <nl> if ( len && len < EMAC_RX_COPY_THRESH ) { <nl> struct sk_buff * copy_skb = <nl> alloc_skb ( len + EMAC_RX_SKB_HEADROOM + 2 , GFP_ATOMIC );
static int scsi_report_lun_scan ( struct scsi_target * starget , int bflags , <nl> out_err : <nl> kfree ( lun_data ); <nl> out : <nl> - scsi_device_put ( sdev ); <nl> if ( scsi_device_created ( sdev )) <nl> /* <nl> * the sdev we used didn ' t appear in the report luns scan <nl> */ <nl> __scsi_remove_device ( sdev ); <nl> + scsi_device_put ( sdev ); <nl> return ret ; <nl> } <nl> 
static int do_pages_stat ( struct mm_struct * mm , unsigned long nr_pages , <nl> int err ; <nl>  <nl> for ( i = 0 ; i < nr_pages ; i += chunk_nr ) { <nl> - if ( chunk_nr + i > nr_pages ) <nl> + if ( chunk_nr > nr_pages - i ) <nl> chunk_nr = nr_pages - i ; <nl>  <nl> err = copy_from_user ( chunk_pages , & pages [ i ],
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
ath5k_intr ( int irq , void * dev_id ) <nl> tasklet_schedule (& sc -> restq ); <nl> } else { <nl> if ( status & AR5K_INT_SWBA ) { <nl> - tasklet_schedule (& sc -> beacontq ); <nl> + tasklet_hi_schedule (& sc -> beacontq ); <nl> } <nl> if ( status & AR5K_INT_RXEOL ) { <nl> /*
static void recalibrate ( struct dp83640_clock * clock ) <nl> u16 cal_gpio , cfg0 , evnt , ptp_trig , trigger , val ; <nl>  <nl> trigger = CAL_TRIGGER ; <nl> - cal_gpio = gpio_tab [ CALIBRATE_GPIO ]; <nl> + cal_gpio = 1 + ptp_find_pin ( clock -> ptp_clock , PTP_PF_PHYSYNC , 0 ); <nl> + if ( cal_gpio < 1 ) { <nl> + pr_err (" PHY calibration pin not avaible - PHY is not calibrated ."); <nl> + return ; <nl> + } <nl>  <nl> mutex_lock (& clock -> extreg_lock ); <nl> 
static int cdrom_ioctl_select_disc ( struct cdrom_device_info * cdi , <nl> return - ENOSYS ; <nl>  <nl> if ( arg != CDSL_CURRENT && arg != CDSL_NONE ) { <nl> - if (( int ) arg >= cdi -> capacity ) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> } <nl> 
static int wm8731_hw_params ( struct snd_pcm_substream * substream , <nl> case 24 : <nl> iface |= 0x0008 ; <nl> break ; <nl> + case 32 : <nl> + iface |= 0x000c ; <nl> + break ; <nl> } <nl>  <nl> wm8731_set_deemph ( codec ); <nl> static int wm8731_startup ( struct snd_pcm_substream * substream , <nl> # define WM8731_RATES SNDRV_PCM_RATE_8000_96000 <nl>  <nl> # define WM8731_FORMATS ( SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S20_3LE |\ <nl> - SNDRV_PCM_FMTBIT_S24_LE ) <nl> + SNDRV_PCM_FMTBIT_S24_LE | SNDRV_PCM_FMTBIT_S32_LE ) <nl>  <nl> static const struct snd_soc_dai_ops wm8731_dai_ops = { <nl> . startup = wm8731_startup ,
static int __init fm10k_init_module ( void ) <nl> /* create driver workqueue */ <nl> fm10k_workqueue = alloc_workqueue ("% s ", WQ_MEM_RECLAIM , 0 , <nl> fm10k_driver_name ); <nl> + if (! fm10k_workqueue ) <nl> + return - ENOMEM ; <nl>  <nl> fm10k_dbg_init (); <nl> 
static inline void __fpu_invalidate_fpregs_state ( struct fpu * fpu ) <nl>  <nl> static inline int fpregs_state_valid ( struct fpu * fpu , unsigned int cpu ) <nl> { <nl> - return fpu == this_cpu_read_stable ( fpu_fpregs_owner_ctx ) && cpu == fpu -> last_cpu ; <nl> + return fpu == this_cpu_read ( fpu_fpregs_owner_ctx ) && cpu == fpu -> last_cpu ; <nl> } <nl>  <nl> /*
static ssize_t bundle_class_show ( struct device * dev , <nl> { <nl> struct gb_bundle * bundle = to_gb_bundle ( dev ); <nl>  <nl> - return sprintf ( buf , "% d \ n ", bundle -> class ); <nl> + return sprintf ( buf , " 0x % 02x \ n ", bundle -> class ); <nl> } <nl> static DEVICE_ATTR_RO ( bundle_class ); <nl> 
int rtl92ce_hw_init ( struct ieee80211_hw * hw ) <nl> local_save_flags ( flags ); <nl> local_irq_enable (); <nl>  <nl> + rtlhal -> fw_ready = false ; <nl> rtlpriv -> intf_ops -> disable_aspm ( hw ); <nl> rtstatus = _rtl92ce_init_mac ( hw ); <nl> if (! rtstatus ) { <nl> int rtl92ce_hw_init ( struct ieee80211_hw * hw ) <nl> goto exit ; <nl> } <nl>  <nl> + rtlhal -> fw_ready = true ; <nl> rtlhal -> last_hmeboxnum = 0 ; <nl> rtl92c_phy_mac_config ( hw ); <nl> /* because last function modify RCR , so we update
static int set_link_state ( struct ibmvnic_adapter * adapter , u8 link_state ) <nl> /* Partuial success , delay and re - send */ <nl> mdelay ( 1000 ); <nl> resend = true ; <nl> + } else if ( adapter -> init_done_rc ) { <nl> + netdev_warn ( netdev , " Unable to set link state , rc =% d \ n ", <nl> + adapter -> init_done_rc ); <nl> + return adapter -> init_done_rc ; <nl> } <nl> } while ( resend ); <nl> 
batadv_iv_ogm_orig_get ( struct batadv_priv * bat_priv , const uint8_t * addr ) <nl> free_bcast_own : <nl> kfree ( orig_node -> bat_iv . bcast_own ); <nl> free_orig_node : <nl> + /* free twice , as batadv_orig_node_new sets refcount to 2 */ <nl> + batadv_orig_node_free_ref ( orig_node ); <nl> batadv_orig_node_free_ref ( orig_node ); <nl>  <nl> return NULL ;
static bool hist_browser__toggle_fold ( struct hist_browser * browser ) <nl> struct callchain_list * cl = container_of ( ms , struct callchain_list , ms ); <nl> bool has_children ; <nl>  <nl> + if (! he || ! ms ) <nl> + return false ; <nl> + <nl> if ( ms == & he -> ms ) <nl> has_children = hist_entry__toggle_fold ( he ); <nl> else
static int svm_check_intercept ( struct kvm_vcpu * vcpu , <nl> if ( info -> intercept == x86_intercept_in || <nl> info -> intercept == x86_intercept_ins ) { <nl> exit_info |= SVM_IOIO_TYPE_MASK ; <nl> - bytes = info -> src_bytes ; <nl> - } else { <nl> bytes = info -> dst_bytes ; <nl> + } else { <nl> + bytes = info -> src_bytes ; <nl> } <nl>  <nl> if ( info -> intercept == x86_intercept_outs ||
predicate_parse ( const char * str , int nr_parens , int nr_preds , <nl>  <nl> switch (* next ) { <nl> case '(': /* # 2 */ <nl> - if ( top - op_stack > nr_parens ) <nl> - return ERR_PTR (- EINVAL ); <nl> + if ( top - op_stack > nr_parens ) { <nl> + ret = - EINVAL ; <nl> + goto out_free ; <nl> + } <nl> *(++ top ) = invert ; <nl> continue ; <nl> case '!': /* # 3 */
static struct radio_si4713_platform_data rx51_si4713_data __initdata_or_module = <nl> . subdev_board_info = & rx51_si4713_board_info , <nl> }; <nl>  <nl> - static struct platform_device rx51_si4713_dev = { <nl> + static struct platform_device rx51_si4713_dev __initdata_or_module = { <nl> . name = " radio - si4713 ", <nl> . id = - 1 , <nl> . dev = {
static int rsi_send_beacon ( struct rsi_common * common ) <nl> skb_pull ( skb , ( 64 - dword_align_bytes )); <nl> if ( rsi_prepare_beacon ( common , skb )) { <nl> rsi_dbg ( ERR_ZONE , " Failed to prepare beacon \ n "); <nl> + dev_kfree_skb ( skb ); <nl> return - EINVAL ; <nl> } <nl> skb_queue_tail (& common -> tx_queue [ MGMT_BEACON_Q ], skb );
struct cfg80211_bss * cfg80211_get_bss ( struct wiphy * wiphy , <nl> continue ; <nl> if ( channel && bss -> pub . channel != channel ) <nl> continue ; <nl> + if (! is_valid_ether_addr ( bss -> pub . bssid )) <nl> + continue ; <nl> /* Don ' t get expired BSS structs */ <nl> if ( time_after ( now , bss -> ts + IEEE80211_SCAN_RESULT_EXPIRE ) && <nl> ! atomic_read (& bss -> hold ))
int fb_find_mode ( struct fb_var_screeninfo * var , <nl> "", ( margins ) ? " with margins " : "", ( interlace ) ? <nl> " interlaced " : ""); <nl>  <nl> + memset (& cvt_mode , 0 , sizeof ( cvt_mode )); <nl> cvt_mode . xres = xres ; <nl> cvt_mode . yres = yres ; <nl> cvt_mode . refresh = ( refresh ) ? refresh : 60 ;
static int ci_hdrc_usb2_probe ( struct platform_device * pdev ) <nl>  <nl> if (! ci_pdata ) { <nl> ci_pdata = devm_kmalloc ( dev , sizeof (* ci_pdata ), GFP_KERNEL ); <nl> + if (! ci_pdata ) <nl> + return - ENOMEM ; <nl> * ci_pdata = ci_default_pdata ; /* struct copy */ <nl> } <nl> 
static int storage_probe ( struct usb_interface * intf , <nl> return - ENOMEM ; <nl> } <nl>  <nl> + /* <nl> + * Allow 16 - byte CDBs and thus > 2TB <nl> + */ <nl> + host -> max_cmd_len = 16 ; <nl> us = host_to_us ( host ); <nl> memset ( us , 0 , sizeof ( struct us_data )); <nl> mutex_init (&( us -> dev_mutex ));
SYSCALL_DEFINE4 ( epoll_ctl , int , epfd , int , op , int , fd , <nl> if ( op == EPOLL_CTL_ADD ) { <nl> if ( is_file_epoll ( tfile )) { <nl> error = - ELOOP ; <nl> - if ( ep_loop_check ( ep , tfile ) != 0 ) <nl> + if ( ep_loop_check ( ep , tfile ) != 0 ) { <nl> + clear_tfile_check_list (); <nl> goto error_tgt_fput ; <nl> + } <nl> } else <nl> list_add (& tfile -> f_tfile_llink , & tfile_check_list ); <nl> }
static int process_all_refs ( struct send_ctx * sctx , <nl> root = sctx -> parent_root ; <nl> cb = __record_deleted_ref ; <nl> } else { <nl> - BUG (); <nl> + btrfs_err ( sctx -> send_root -> fs_info , <nl> + " Wrong command % d in process_all_refs ", cmd ); <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> } <nl>  <nl> key . objectid = sctx -> cmp_key -> objectid ;
static int soc_cleanup_card_resources ( struct snd_soc_card * card ) <nl> if ( card -> remove ) <nl> card -> remove ( card ); <nl>  <nl> + snd_soc_dapm_free (& card -> dapm ); <nl> + <nl> kfree ( card -> rtd ); <nl> snd_card_free ( card -> snd_card ); <nl> return 0 ;
static int sh_veu_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> * vdev = sh_veu_videodev ; <nl> + vdev -> v4l2_dev = & veu -> v4l2_dev ; <nl> spin_lock_init (& veu -> lock ); <nl> mutex_init (& veu -> fop_lock ); <nl> vdev -> lock = & veu -> fop_lock ;
static int moxart_probe ( struct platform_device * pdev ) <nl> goto out ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out ; <nl>  <nl> dma_cap_zero ( mask ); <nl> dma_cap_set ( DMA_SLAVE , mask );
static bool device_init_rings ( struct vnt_private * priv ) <nl> CB_MAX_BUF_SIZE , <nl> & priv -> tx_bufs_dma0 , <nl> GFP_ATOMIC ); <nl> - if ( priv -> tx0_bufs == NULL ) { <nl> + if (! priv -> tx0_bufs ) { <nl> dev_err (& priv -> pcid -> dev , " allocate buf dma memory failed \ n "); <nl>  <nl> dma_free_coherent (& priv -> pcid -> dev ,
static int pcnet32_phys_id ( struct net_device * dev , u32 data ) <nl> if ((! data ) || ( data > ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ))) <nl> data = ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ); <nl>  <nl> - schedule_timeout ( data * HZ ); <nl> + msleep_interruptible ( data * 1000 ); <nl> del_timer_sync (& lp -> blink_timer ); <nl>  <nl> /* Restore the original value of the bcrs */
struct ahash_request { <nl> void * __ctx [] CRYPTO_MINALIGN_ATTR ; <nl> }; <nl>  <nl> +# define AHASH_REQUEST_ON_STACK ( name , ahash ) \ <nl> + char __ ## name ## _desc [ sizeof ( struct ahash_request ) + \ <nl> + crypto_ahash_reqsize ( ahash )] CRYPTO_MINALIGN_ATTR ; \ <nl> + struct ahash_request * name = ( void *) __ ## name ## _desc <nl> + <nl> /** <nl> * struct ahash_alg - asynchronous message digest definition <nl> * @ init : Initialize the transformation context . Intended only to initialize the
static int ttm_bo_cleanup_refs_and_unlock ( struct ttm_buffer_object * bo , <nl> } <nl>  <nl> ttm_bo_del_from_lru ( bo ); <nl> + if (! list_empty (& bo -> ddestroy ) && ( bo -> resv != & bo -> ttm_resv )) <nl> + reservation_object_fini (& bo -> ttm_resv ); <nl> list_del_init (& bo -> ddestroy ); <nl> kref_put (& bo -> list_kref , ttm_bo_ref_bug ); <nl> 
static ssize_t sta_ht_capa_read ( struct file * file , char __user * userbuf , <nl> if ( _cond ) \ <nl> p += scnprintf ( p , sizeof ( buf )+ buf - p , "\ t " _str "\ n "); \ <nl> } while ( 0 ) <nl> - char buf [ 1024 ], * p = buf ; <nl> + char buf [ 512 ], * p = buf ; <nl> int i ; <nl> struct sta_info * sta = file -> private_data ; <nl> struct ieee80211_sta_ht_cap * htc = & sta -> sta . ht_cap ;
int omap3isp_csiphy_acquire ( struct isp_csiphy * phy ) <nl> if ( rval < 0 ) <nl> goto done ; <nl>  <nl> - omap3isp_csi2_reset ( phy -> csi2 ); <nl> + rval = omap3isp_csi2_reset ( phy -> csi2 ); <nl> + if ( rval < 0 ) <nl> + goto done ; <nl>  <nl> csiphy_dphy_config ( phy ); <nl> csiphy_lanes_config ( phy );
struct qmp * qmp_get ( struct device * dev ) <nl>  <nl> qmp = platform_get_drvdata ( pdev ); <nl>  <nl> - return qmp ? qmp : ERR_PTR (- EPROBE_DEFER ); <nl> + if (! qmp ) { <nl> + put_device (& pdev -> dev ); <nl> + return ERR_PTR (- EPROBE_DEFER ); <nl> + } <nl> + return qmp ; <nl> } <nl> EXPORT_SYMBOL ( qmp_get ); <nl> 
qlafx00_soc_cpu_reset ( scsi_qla_host_t * vha ) <nl> /* Kick in Core0 to start boot process */ <nl> QLAFX00_SET_HBA_SOC_REG ( ha , SOC_SW_RST_CONTROL_REG_CORE0 , ( 0xF00 )); <nl>  <nl> + spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> + <nl> /* Wait 10secs for soft - reset to complete . */ <nl> for ( cnt = 10 ; cnt ; cnt --) { <nl> msleep ( 1000 ); <nl> barrier (); <nl> } <nl> - spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> } <nl>  <nl> /**
static int ad5686_write_raw ( struct iio_dev * indio_dev , <nl>  <nl> switch ( mask ) { <nl> case 0 : <nl> - if ( val > ( 1 << chan -> scan_type . realbits )) <nl> + if ( val > ( 1 << chan -> scan_type . realbits ) || val < 0 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& indio_dev -> mlock );
static const struct hda_fixup alc269_fixups [] = { <nl> [ ALC269_FIXUP_HEADSET_MODE ] = { <nl> . type = HDA_FIXUP_FUNC , <nl> . v . func = alc_fixup_headset_mode , <nl> + . chained = true , <nl> + . chain_id = ALC255_FIXUP_DELL_WMI_MIC_MUTE_LED <nl> }, <nl> [ ALC269_FIXUP_HEADSET_MODE_NO_HP_MIC ] = { <nl> . type = HDA_FIXUP_FUNC ,
nv140_chipset = { <nl> . i2c = gm200_i2c_new , <nl> . ibus = gm200_ibus_new , <nl> . imem = nv50_instmem_new , <nl> + . ltc = gp102_ltc_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
int snd_usbmidi_create ( struct snd_card * card , <nl> else <nl> err = snd_usbmidi_create_endpoints ( umidi , endpoints ); <nl> if ( err < 0 ) { <nl> - snd_usbmidi_free ( umidi ); <nl> return err ; <nl> } <nl> 
iblock_execute_rw ( struct se_cmd * cmd , struct scatterlist * sgl , u32 sgl_nents , <nl> sg_num --; <nl> } <nl>  <nl> - if ( cmd -> prot_type ) { <nl> + if ( cmd -> prot_type && dev -> dev_attrib . pi_prot_type ) { <nl> int rc = iblock_alloc_bip ( cmd , bio_start ); <nl> if ( rc ) <nl> goto fail_put_bios ;
static void cpufreq_policy_free ( struct cpufreq_policy * policy ) <nl>  <nl> static void update_policy_cpu ( struct cpufreq_policy * policy , unsigned int cpu ) <nl> { <nl> + if ( cpu == policy -> cpu ) <nl> + return ; <nl> + <nl> policy -> last_cpu = policy -> cpu ; <nl> policy -> cpu = cpu ; <nl> 
static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
static const char * intercon [][ 3 ] = { <nl> {" HPRCOM ", NULL , " Right HP Com "}, <nl>  <nl> /* Mono Output */ <nl> - {" MONOLOUT ", NULL , " Mono Out "}, <nl> - {" MONOLOUT ", NULL , " Mono Out "}, <nl> + {" MONO_LOUT ", NULL , " Mono Out "}, <nl> + {" MONO_LOUT ", NULL , " Mono Out "}, <nl>  <nl> /* Left Input */ <nl> {" Left Line1L Mux ", " single - ended ", " LINE1L "},
static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
static int goldfish_tty_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_tty_register_device_failed : <nl> - free_irq ( irq , pdev ); <nl> + free_irq ( irq , qtty ); <nl> err_request_irq_failed : <nl> goldfish_tty_current_line_count --; <nl> if ( goldfish_tty_current_line_count == 0 )
static int _regmap_read ( struct regmap * map , unsigned int reg , <nl> if ( map -> cache_only ) <nl> return - EBUSY ; <nl>  <nl> + if (! regmap_readable ( map , reg )) <nl> + return - EIO ; <nl> + <nl> ret = map -> reg_read ( context , reg , val ); <nl> if ( ret == 0 ) { <nl> # ifdef LOG_DEVICE
static void snd_timer_user_tinterrupt ( struct snd_timer_instance * timeri , <nl> } <nl> if (( tu -> filter & ( 1 << SNDRV_TIMER_EVENT_RESOLUTION )) && <nl> tu -> last_resolution != resolution ) { <nl> + memset (& r1 , 0 , sizeof ( r1 )); <nl> r1 . event = SNDRV_TIMER_EVENT_RESOLUTION ; <nl> r1 . tstamp = tstamp ; <nl> r1 . val = resolution ;
int CIFSFindNext ( const int xid , struct cifs_tcon * tcon , <nl> T2_FNEXT_RSP_PARMS * parms ; <nl> char * response_data ; <nl> int rc = 0 ; <nl> - int bytes_returned , name_len ; <nl> + int bytes_returned ; <nl> + unsigned int name_len ; <nl> __u16 params , byte_count ; <nl>  <nl> cFYI ( 1 , " In FindNext ");
int tpm_open ( struct inode * inode , struct file * file ) <nl> return - EBUSY ; <nl> } <nl>  <nl> - chip -> data_buffer = kmalloc ( TPM_BUFSIZE * sizeof ( u8 ), GFP_KERNEL ); <nl> + chip -> data_buffer = kzalloc ( TPM_BUFSIZE , GFP_KERNEL ); <nl> if ( chip -> data_buffer == NULL ) { <nl> clear_bit ( 0 , & chip -> is_open ); <nl> put_device ( chip -> dev );
void vmw_kms_helper_resource_finish ( struct vmw_validation_ctx * ctx , <nl> vmw_kms_helper_buffer_finish ( res -> dev_priv , NULL , ctx -> buf , <nl> out_fence , NULL ); <nl>  <nl> + vmw_dmabuf_unreference (& ctx -> buf ); <nl> vmw_resource_unreserve ( res , false , NULL , 0 ); <nl> mutex_unlock (& res -> dev_priv -> cmdbuf_mutex ); <nl> }
static int cb_pcidas_attach ( struct comedi_device * dev , <nl>  <nl> /* 8255 */ <nl> s = dev -> subdevices + 2 ; <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + ret = subdev_8255_init ( dev , s , NULL , <nl> + devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* serial EEPROM , */ <nl> s = dev -> subdevices + 3 ;
static int hwsim_new_radio_nl ( struct sk_buff * msg , struct genl_info * info ) <nl> if ( info -> attrs [ HWSIM_ATTR_CHANNELS ]) <nl> param . channels = nla_get_u32 ( info -> attrs [ HWSIM_ATTR_CHANNELS ]); <nl>  <nl> + if ( param . channels < 1 ) { <nl> + GENL_SET_ERR_MSG ( info , " must have at least one channel "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( param . channels > CFG80211_MAX_NUM_DIFFERENT_CHANNELS ) { <nl> GENL_SET_ERR_MSG ( info , " too many channels specified "); <nl> return - EINVAL ;
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
snd_ali_playback_pointer ( struct snd_pcm_substream * substream ) <nl> spin_unlock (& codec -> reg_lock ); <nl> dev_dbg ( codec -> card -> dev , " playback pointer returned cso =% xh .\ n ", cso ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl>  <nl> static snd_pcm_uframes_t snd_ali_pointer ( struct snd_pcm_substream * substream ) <nl> cso = inw ( ALI_REG ( codec , ALI_CSO_ALPHA_FMS + 2 )); <nl> spin_unlock (& codec -> reg_lock ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl> 
static int iwl_nvm_read_section ( struct iwl_mvm * mvm , u16 section , <nl> offset += ret ; <nl> } <nl>  <nl> - IWL_INFO ( mvm , " NVM section % d read completed \ n ", section ); <nl> + IWL_DEBUG_EEPROM ( mvm -> trans -> dev , <nl> + " NVM section % d read completed \ n ", section ); <nl> return offset ; <nl> } <nl> 
static void sba_process_received_request ( struct sba_device * sba , <nl>  <nl> WARN_ON ( tx -> cookie < 0 ); <nl> if ( tx -> cookie > 0 ) { <nl> + spin_lock_irqsave (& sba -> reqs_lock , flags ); <nl> dma_cookie_complete ( tx ); <nl> + spin_unlock_irqrestore (& sba -> reqs_lock , flags ); <nl> dmaengine_desc_get_callback_invoke ( tx , NULL ); <nl> dma_descriptor_unmap ( tx ); <nl> tx -> callback = NULL ;
static int goldfish_audio_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_misc_register_failed : <nl> + free_irq ( data -> irq , data ); <nl> err_request_irq_failed : <nl> dma_free_coherent (& pdev -> dev , COMBINED_BUFFER_SIZE , <nl> data -> buffer_virt , data -> buffer_phys );
static int atmel_ssc_set_dai_clkdiv ( struct snd_soc_dai * cpu_dai , <nl> * transmit and receive , so if a value has already <nl> * been set , it must match this value . <nl> */ <nl> - if ( ssc_p -> cmr_div == 0 ) <nl> + if ( ssc_p -> dir_mask != <nl> + ( SSC_DIR_MASK_PLAYBACK | SSC_DIR_MASK_CAPTURE )) <nl> + ssc_p -> cmr_div = div ; <nl> + else if ( ssc_p -> cmr_div == 0 ) <nl> ssc_p -> cmr_div = div ; <nl> else <nl> if ( div != ssc_p -> cmr_div )
int __init acpi_table_init ( void ) <nl>  <nl> static int __init acpi_parse_apic_instance ( char * str ) <nl> { <nl> + if (! str ) <nl> + return - EINVAL ; <nl>  <nl> acpi_apic_instance = simple_strtoul ( str , NULL , 0 ); <nl> 
static void arm_ccn_pmu_xp_dt_config ( struct perf_event * event , int enable ) <nl> struct arm_ccn_component * xp ; <nl> u32 val , dt_cfg ; <nl>  <nl> + /* Nothing to do for cycle counter */ <nl> + if ( hw -> idx == CCN_IDX_PMU_CYCLE_COUNTER ) <nl> + return ; <nl> + <nl> if ( CCN_CONFIG_TYPE ( event -> attr . config ) == CCN_TYPE_XP ) <nl> xp = & ccn -> xp [ CCN_CONFIG_XP ( event -> attr . config )]; <nl> else
static void ip6gre_tnl_link_config ( struct ip6_tnl * t , int set_mtu ) <nl> dev -> mtu = rt -> dst . dev -> mtu - addend ; <nl> if (!( t -> parms . flags & IP6_TNL_F_IGN_ENCAP_LIMIT )) <nl> dev -> mtu -= 8 ; <nl> + if ( dev -> type == ARPHRD_ETHER ) <nl> + dev -> mtu -= ETH_HLEN ; <nl>  <nl> if ( dev -> mtu < IPV6_MIN_MTU ) <nl> dev -> mtu = IPV6_MIN_MTU ;
static void copy_from_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy <nl>  <nl> static void copy_to_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy_info * p , int dir ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> sel , & xp -> selector , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & xp -> lft , sizeof ( p -> lft )); <nl> memcpy (& p -> curlft , & xp -> curlft , sizeof ( p -> curlft ));
static ssize_t rpmsg_eptdev_write_iter ( struct kiocb * iocb , <nl> if (! kbuf ) <nl> return - ENOMEM ; <nl>  <nl> - if (! copy_from_iter_full ( kbuf , len , from )) <nl> - return - EFAULT ; <nl> + if (! copy_from_iter_full ( kbuf , len , from )) { <nl> + ret = - EFAULT ; <nl> + goto free_kbuf ; <nl> + } <nl>  <nl> if ( mutex_lock_interruptible (& eptdev -> ept_lock )) { <nl> ret = - ERESTARTSYS ;
int brcmf_fweh_activate_events ( struct brcmf_if * ifp ) <nl> int i , err ; <nl> s8 eventmask [ BRCMF_EVENTING_MASK_LEN ]; <nl>  <nl> + memset ( eventmask , 0 , sizeof ( eventmask )); <nl> for ( i = 0 ; i < BRCMF_E_LAST ; i ++) { <nl> if ( ifp -> drvr -> fweh . evt_handler [ i ]) { <nl> brcmf_dbg ( EVENT , " enable event % s \ n ",
static int netvsc_start_xmit ( struct sk_buff * skb , struct net_device * net ) <nl> } else { <nl> /* we are shutting down or bus overloaded , just drop packet */ <nl> net -> stats . tx_dropped ++; <nl> - netvsc_xmit_completion ( packet ); <nl> + kfree ( packet ); <nl> + dev_kfree_skb_any ( skb ); <nl> } <nl>  <nl> return NETDEV_TX_OK ;
static int musb_urb_enqueue ( <nl> * we only have work to do in the former case . <nl> */ <nl> spin_lock_irqsave (& musb -> lock , flags ); <nl> - if ( hep -> hcpriv ) { <nl> + if ( hep -> hcpriv || ! next_urb ( qh )) { <nl> /* some concurrent activity submitted another urb to hep ... <nl> * odd , rare , error prone , but legal . <nl> */
static const struct soc_device_attribute gen3_soc_whitelist [] = { <nl> /* generic ones */ <nl> { . soc_id = " r8a7795 " }, <nl> { . soc_id = " r8a7796 " }, <nl> + { . soc_id = " r8a77980 " }, <nl> { . soc_id = " r8a77995 " }, <nl> { /* sentinel */ } <nl> };
static int usbdev_open ( struct inode * inode , struct file * file ) <nl> if (! ps ) <nl> goto out ; <nl>  <nl> - ret = - ENOENT ; <nl> + ret = - ENODEV ; <nl>  <nl> /* usbdev device - node */ <nl> if ( imajor ( inode ) == USB_DEVICE_MAJOR )
static int ov5642_set_fmt ( struct v4l2_subdev * sd , <nl> mf -> field = V4L2_FIELD_NONE ; <nl>  <nl> if ( format -> which == V4L2_SUBDEV_FORMAT_ACTIVE ) <nl> - priv -> fmt = ov5642_find_datafmt ( mf -> code ); <nl> + priv -> fmt = fmt ; <nl> else <nl> cfg -> try_fmt = * mf ; <nl> return 0 ;
void blk_execute_rq_nowait ( struct request_queue * q , struct gendisk * bd_disk , <nl> spin_lock_irq ( q -> queue_lock ); <nl> __elv_add_request ( q , rq , where , 1 ); <nl> __generic_unplug_device ( q ); <nl> + /* the queue is stopped so it won ' t be plugged + unplugged */ <nl> + if ( blk_pm_resume_request ( rq )) <nl> + q -> request_fn ( q ); <nl> spin_unlock_irq ( q -> queue_lock ); <nl> } <nl> EXPORT_SYMBOL_GPL ( blk_execute_rq_nowait );
int iwl_trans_pcie_dyn_txq_alloc ( struct iwl_trans * trans , <nl> rsp = ( void *) hcmd . resp_pkt -> data ; <nl> qid = le16_to_cpu ( rsp -> queue_number ); <nl>  <nl> - if ( qid > ARRAY_SIZE ( trans_pcie -> txq )) { <nl> + if ( qid >= ARRAY_SIZE ( trans_pcie -> txq )) { <nl> WARN_ONCE ( 1 , " queue index % d unsupported ", qid ); <nl> ret = - EIO ; <nl> goto error_free_resp ;
int netvsc_send ( struct hv_device * device , <nl> if (! net_device ) <nl> return - ENODEV ; <nl>  <nl> + /* We may race with netvsc_connect_vsp ()/ netvsc_init_buf () and get <nl> + * here before the negotiation with the host is finished and <nl> + * send_section_map may not be allocated yet . <nl> + */ <nl> + if (! net_device -> send_section_map ) <nl> + return - EAGAIN ; <nl> + <nl> out_channel = net_device -> chn_table [ q_idx ]; <nl>  <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ;
mwifiex_cmd_802_11_ad_hoc_start ( struct mwifiex_private * priv , <nl>  <nl> memset ( adhoc_start -> ssid , 0 , IEEE80211_MAX_SSID_LEN ); <nl>  <nl> + if ( req_ssid -> ssid_len > IEEE80211_MAX_SSID_LEN ) <nl> + req_ssid -> ssid_len = IEEE80211_MAX_SSID_LEN ; <nl> memcpy ( adhoc_start -> ssid , req_ssid -> ssid , req_ssid -> ssid_len ); <nl>  <nl> mwifiex_dbg ( adapter , INFO , " info : ADHOC_S_CMD : SSID = % s \ n ",
static void __devinit dfx_bus_init ( struct net_device * dev ) <nl> * Interrupts are disabled at the adapter bus - specific logic . <nl> */ <nl>  <nl> - static void __devinit dfx_bus_uninit ( struct net_device * dev ) <nl> + static void __devexit dfx_bus_uninit ( struct net_device * dev ) <nl> { <nl> DFX_board_t * bp = netdev_priv ( dev ); <nl> struct device * bdev = bp -> bus_dev ;
int yama_task_prctl ( int option , unsigned long arg2 , unsigned long arg3 , <nl> if ( arg2 == 0 ) { <nl> yama_ptracer_del ( NULL , myself ); <nl> rc = 0 ; <nl> - } else if ( arg2 == PR_SET_PTRACER_ANY ) { <nl> + } else if ( arg2 == PR_SET_PTRACER_ANY || ( int ) arg2 == - 1 ) { <nl> rc = yama_ptracer_add ( NULL , myself ); <nl> } else { <nl> struct task_struct * tracer ;
EXPORT_SYMBOL_GPL ( crypto_init_spawn2 ); <nl>  <nl> void crypto_drop_spawn ( struct crypto_spawn * spawn ) <nl> { <nl> + if (! spawn -> alg ) <nl> + return ; <nl> + <nl> down_write (& crypto_alg_sem ); <nl> list_del (& spawn -> list ); <nl> up_write (& crypto_alg_sem );
static void __devinit pnv_pci_ioda_setup_seg ( void ) <nl> } <nl> } <nl>  <nl> + static void __devinit pnv_pci_ioda_setup_DMA ( void ) <nl> +{ <nl> + struct pci_controller * hose , * tmp ; <nl> + <nl> + list_for_each_entry_safe ( hose , tmp , & hose_list , list_node ) { <nl> + pnv_ioda_setup_dma ( hose -> private_data ); <nl> + } <nl> +} <nl> + <nl> static void __devinit pnv_pci_ioda_fixup ( void ) <nl> { <nl> pnv_pci_ioda_setup_PEs (); <nl> pnv_pci_ioda_setup_seg (); <nl> + pnv_pci_ioda_setup_DMA (); <nl> } <nl>  <nl> /*
int xhci_queue_bulk_tx ( struct xhci_hcd * xhci , gfp_t mem_flags , <nl> send_addr = addr ; <nl>  <nl> /* Queue the TRBs , even if they are zero - length */ <nl> - for ( enqd_len = 0 ; enqd_len < full_len ; enqd_len += trb_buff_len ) { <nl> + for ( enqd_len = 0 ; first_trb || enqd_len < full_len ; <nl> + enqd_len += trb_buff_len ) { <nl> field = TRB_TYPE ( TRB_NORMAL ); <nl>  <nl> /* TRB buffer should not cross 64KB boundaries */
static int __open_dso ( struct dso * dso , struct machine * machine ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (! is_regular_file ( name )) <nl> + if (! is_regular_file ( name )) { <nl> + free ( name ); <nl> return - EINVAL ; <nl> + } <nl>  <nl> fd = do_open ( name ); <nl> free ( name );
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( ieee80211_vif_is_mesh (& sdata -> vif ) && <nl> ! sdata -> u . mesh . user_mpm ) <nl> init_timer (& sta -> plink_timer ); <nl> + sta -> nonpeer_pm = NL80211_MESH_POWER_ACTIVE ; <nl> # endif <nl>  <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN );
static int cdrom_ioctl_drive_status ( struct cdrom_device_info * cdi , <nl> if (! CDROM_CAN ( CDC_SELECT_DISC ) || <nl> ( arg == CDSL_CURRENT || arg == CDSL_NONE )) <nl> return cdi -> ops -> drive_status ( cdi , CDSL_CURRENT ); <nl> - if ((( int ) arg >= cdi -> capacity )) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> return cdrom_slot_status ( cdi , arg ); <nl> }
static struct wlan_bssid_ex * collect_bss_info ( struct rtw_adapter * padapter , <nl> memcpy ( bssid -> Ssid . ssid , p + 2 , p [ 1 ]); <nl> bssid -> Ssid . ssid_len = p [ 1 ]; <nl>  <nl> - memset ( bssid -> SupportedRates , 0 , NDIS_802_11_LENGTH_RATES_EX ); <nl> - <nl> /* checking rate info ... */ <nl> i = 0 ; <nl> p = cfg80211_find_ie ( WLAN_EID_SUPP_RATES , bssid -> IEs + ie_offset ,
int assoc_array_gc ( struct assoc_array * array , <nl> shortcut = assoc_array_ptr_to_shortcut ( ptr ); <nl> slot = shortcut -> parent_slot ; <nl> cursor = shortcut -> back_pointer ; <nl> + if (! cursor ) <nl> + goto gc_complete ; <nl> } else { <nl> slot = node -> parent_slot ; <nl> cursor = ptr ; <nl> } <nl> - BUG_ON (! ptr ); <nl> + BUG_ON (! cursor ); <nl> node = assoc_array_ptr_to_node ( cursor ); <nl> slot ++; <nl> goto continue_node ;
enum dc_status dce110_apply_ctx_to_hw ( <nl> if ( pipe_ctx -> stream == pipe_ctx_old -> stream ) <nl> continue ; <nl>  <nl> + if ( pipe_ctx -> stream && pipe_ctx_old -> stream <nl> + && ! pipe_need_reprogram ( pipe_ctx_old , pipe_ctx )) <nl> + continue ; <nl> + <nl> if ( pipe_ctx -> top_pipe ) <nl> continue ; <nl> 
void ieee80211_csa_finalize_work ( struct work_struct * work ) <nl> container_of ( work , struct ieee80211_sub_if_data , <nl> csa_finalize_work ); <nl> struct ieee80211_local * local = sdata -> local ; <nl> - int err , changed ; <nl> + int err , changed = 0 ; <nl>  <nl> if (! ieee80211_sdata_running ( sdata )) <nl> return ;
static int __devinit rdc321x_wdt_probe ( struct platform_device * pdev ) <nl> struct resource * r ; <nl> struct rdc321x_wdt_pdata * pdata ; <nl>  <nl> - pdata = pdev -> dev . platform_data ; <nl> + pdata = platform_get_drvdata ( pdev ); <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " no platform data supplied \ n "); <nl> return - ENODEV ;
static int __init sa1110_clk_init ( void ) <nl> struct sdram_params * sdram ; <nl> const char * name = sdram_name ; <nl>  <nl> + if (! cpu_is_sa1110 ()) <nl> + return - ENODEV ; <nl> + <nl> if (! name [ 0 ]) { <nl> if ( machine_is_assabet ()) <nl> name = " TC59SM716 - CL3 ";
static int validate_user_key ( struct fscrypt_info * crypt_info , <nl> goto out ; <nl> } <nl> ukp = user_key_payload_locked ( keyring_key ); <nl> + if (! ukp ) { <nl> + /* key was revoked before we acquired its semaphore */ <nl> + res = - EKEYREVOKED ; <nl> + goto out ; <nl> + } <nl> if ( ukp -> datalen != sizeof ( struct fscrypt_key )) { <nl> res = - EINVAL ; <nl> goto out ;
static int annotate_browser__run ( struct annotate_browser * self , int evidx , <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' H ': <nl> + case ' h ': <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' S ': <nl> + case ' s ': <nl> if ( annotate_browser__toggle_source ( self )) <nl> ui_helpline__puts ( help ); <nl> continue ;
int ath10k_wmi_start_scan ( struct ath10k * ar , <nl> return ret ; <nl>  <nl> if ( test_bit ( ATH10K_FW_FEATURE_WMI_10X , ar -> fw_features )) <nl> - len = sizeof ( struct wmi_start_scan_cmd ) + <nl> + len = sizeof ( struct wmi_10x_start_scan_cmd ) + <nl> ath10k_wmi_start_scan_tlvs_len ( arg ); <nl> else <nl> - len = sizeof ( struct wmi_10x_start_scan_cmd ) + <nl> + len = sizeof ( struct wmi_start_scan_cmd ) + <nl> ath10k_wmi_start_scan_tlvs_len ( arg ); <nl>  <nl> skb = ath10k_wmi_alloc_skb ( ar , len );
static int saa7134_s_fmt_overlay ( struct file * file , void * priv , <nl> struct saa7134_fh * fh = priv ; <nl> struct saa7134_dev * dev = fh -> dev ; <nl> int err ; <nl> - unsigned int flags ; <nl> + unsigned long flags ; <nl>  <nl> if ( saa7134_no_overlay > 0 ) { <nl> printk ( KERN_ERR " V4L2_BUF_TYPE_VIDEO_OVERLAY : no_overlay \ n ");
static long pmcraid_ioctl_passthrough ( <nl> pmcraid_err (" couldn ' t build passthrough ioadls \ n "); <nl> goto out_free_buffer ; <nl> } <nl> + } else if ( request_size < 0 ) { <nl> + rc = - EINVAL ; <nl> + goto out_free_buffer ; <nl> } <nl>  <nl> /* If data is being written into the device , copy the data from user
static irqreturn_t rtl8169_interrupt ( int irq , void * dev_instance ) <nl> handled = 1 ; <nl>  <nl> rtl_irq_disable ( tp ); <nl> - napi_schedule (& tp -> napi ); <nl> + napi_schedule_irqoff (& tp -> napi ); <nl> } <nl> } <nl> return IRQ_RETVAL ( handled );
static inline int uinput_request_reserve_slot ( struct uinput_device * udev , struct <nl>  <nl> static void uinput_request_done ( struct uinput_device * udev , struct uinput_request * request ) <nl> { <nl> - complete (& request -> done ); <nl> - <nl> /* Mark slot as available */ <nl> udev -> requests [ request -> id ] = NULL ; <nl> wake_up_interruptible (& udev -> requests_waitq ); <nl> + <nl> + complete (& request -> done ); <nl> } <nl>  <nl> static int uinput_request_submit ( struct input_dev * dev , struct uinput_request * request )
void arch_update_cpu_topology ( void ); <nl> . busy_idx = 3 , \ <nl> . idle_idx = 3 , \ <nl> . flags = SD_LOAD_BALANCE \ <nl> - | SD_SERIALIZE , \ <nl> + | SD_BALANCE_NEWIDLE \ <nl> + | SD_WAKE_AFFINE \ <nl> + | SD_SERIALIZE , \ <nl> . last_balance = jiffies , \ <nl> . balance_interval = 64 , \ <nl> }
static int kill_something_info ( int sig , struct siginfo * info , pid_t pid ) <nl> return ret ; <nl> } <nl>  <nl> + /* - INT_MIN is undefined . Exclude this case to avoid a UBSAN warning */ <nl> + if ( pid == INT_MIN ) <nl> + return - ESRCH ; <nl> + <nl> read_lock (& tasklist_lock ); <nl> if ( pid != - 1 ) { <nl> ret = __kill_pgrp_info ( sig , info ,
ufs_extend_tail ( struct inode * inode , u64 writes_to , <nl>  <nl> p = ufs_get_direct_data_ptr ( uspi , ufsi , block ); <nl> tmp = ufs_new_fragments ( inode , p , lastfrag , ufs_data_ptr_to_cpu ( sb , p ), <nl> - new_size , err , locked_page ); <nl> + new_size - ( lastfrag & uspi -> s_fpbmask ), err , <nl> + locked_page ); <nl> return tmp != 0 ; <nl> } <nl> 
static struct dentry * aio_mount ( struct file_system_type * fs_type , <nl> static const struct dentry_operations ops = { <nl> . d_dname = simple_dname , <nl> }; <nl> - return mount_pseudo ( fs_type , " aio :", NULL , & ops , AIO_RING_MAGIC ); <nl> + struct dentry * root = mount_pseudo ( fs_type , " aio :", NULL , & ops , <nl> + AIO_RING_MAGIC ); <nl> + <nl> + if (! IS_ERR ( root )) <nl> + root -> d_sb -> s_iflags |= SB_I_NOEXEC ; <nl> + return root ; <nl> } <nl>  <nl> /* aio_setup
static ssize_t unix_stream_sendpage ( struct socket * socket , struct page * page , <nl> * this - does no harm <nl> */ <nl> consume_skb ( newskb ); <nl> + newskb = NULL ; <nl> } <nl>  <nl> if ( skb_append_pagefrags ( skb , page , offset , size )) {
+// SPDX - License - Identifier : GPL - 2 . 0 <nl> /* <nl> - * linux / drivers / pinctrl / pinctrl - rt2880 . c <nl> - * <nl> - * This program is free software ; you can redistribute it and / or modify <nl> - * it under the terms of the GNU General Public License version 2 as <nl> - * publishhed by the Free Software Foundation . <nl> - * <nl> * Copyright ( C ) 2013 John Crispin < blogic @ openwrt . org > <nl> */ <nl> 
int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> cur_devices -> num_devices --; <nl> cur_devices -> total_devices --; <nl> + /* Update total_devices of the parent fs_devices if it ' s seed */ <nl> + if ( cur_devices != fs_devices ) <nl> + fs_devices -> total_devices --; <nl>  <nl> if ( test_bit ( BTRFS_DEV_STATE_MISSING , & device -> dev_state )) <nl> cur_devices -> missing_devices --;
static int eseqiv_givencrypt ( struct skcipher_givcrypt_request * req ) <nl> if ( err ) <nl> goto out ; <nl>  <nl> - eseqiv_complete2 ( req ); <nl> + if ( giv != req -> giv ) <nl> + eseqiv_complete2 ( req ); <nl>  <nl> out : <nl> return err ;
static struct pci_driver jr3_pci_pci_driver = { <nl> module_comedi_pci_driver ( jr3_pci_driver , jr3_pci_pci_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for JR3 / PCI force sensor board "); <nl> MODULE_LICENSE (" GPL "); <nl> MODULE_FIRMWARE (" comedi / jr3pci . idm ");
static int remove_and_add_spares ( struct mddev * mddev , <nl> ! test_bit ( Bitmap_sync , & rdev -> flags ))) <nl> continue ; <nl>  <nl> - rdev -> recovery_offset = 0 ; <nl> + if ( rdev -> saved_raid_disk < 0 ) <nl> + rdev -> recovery_offset = 0 ; <nl> if ( mddev -> pers -> <nl> hot_add_disk ( mddev , rdev ) == 0 ) { <nl> if ( sysfs_link_rdev ( mddev , rdev ))
static void iomd_get_next_sg ( struct scatterlist * sg , struct iomd_dma * idma ) <nl>  <nl> if ( idma -> dma . sg -> length == 0 ) { <nl> if ( idma -> dma . sgcount > 1 ) { <nl> - idma -> dma . sg ++; <nl> + idma -> dma . sg = sg_next ( idma -> dma . sg ); <nl> idma -> dma . sgcount --; <nl> } else { <nl> idma -> dma . sg = NULL ;
static void rt2800pci_txstatus_interrupt ( struct rt2x00_dev * rt2x00dev ) <nl> * Since we have only one producer and one consumer we don ' t <nl> * need to lock the kfifo . <nl> */ <nl> - for ( i = 0 ; i < rt2x00dev -> ops -> tx -> entry_num ; i ++) { <nl> + for ( i = 0 ; i < rt2x00dev -> tx -> limit ; i ++) { <nl> rt2x00mmio_register_read ( rt2x00dev , TX_STA_FIFO , & status ); <nl>  <nl> if (! rt2x00_get_field32 ( status , TX_STA_FIFO_VALID ))
static void ath9k_hw_ar9300_set_txpower ( struct ath_hw * ah , <nl> regulatory -> max_power_level = targetPowerValT2 [ i ]; <nl> } <nl>  <nl> + ath9k_hw_update_regulatory_maxpower ( ah ); <nl> + <nl> if ( test ) <nl> return ; <nl> 
static int davinci_gpio_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& chips [ i ]. lock ); <nl>  <nl> regs = gpio2regs ( base ); <nl> + if (! regs ) <nl> + return - ENXIO ; <nl> chips [ i ]. regs = regs ; <nl> chips [ i ]. set_data = & regs -> set_data ; <nl> chips [ i ]. clr_data = & regs -> clr_data ;
static int rtl8192eu_parse_efuse ( struct rtl8xxxu_priv * priv ) <nl> raw [ i + 6 ], raw [ i + 7 ]); <nl> } <nl> } <nl> + /* <nl> + * Temporarily disable 8192eu support <nl> + */ <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static int sym_prepare_setting ( struct Scsi_Host * shost , struct sym_hcb * np , stru <nl>  <nl> tp -> usrflags |= ( SYM_DISC_ENABLED | SYM_TAGS_ENABLED ); <nl> tp -> usrtags = SYM_SETUP_MAX_TAG ; <nl> + tp -> usr_width = np -> maxwide ; <nl> + tp -> usr_period = 9 ; <nl>  <nl> sym_nvram_setup_target ( tp , i , nvram ); <nl> 
static int eb_relocate_vma ( struct i915_execbuffer * eb , struct i915_vma * vma ) <nl> * to read . However , if the array is not writable the user loses <nl> * the updated relocation values . <nl> */ <nl> - if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof ( urelocs )))) <nl> + if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof (* urelocs )))) <nl> return - EFAULT ; <nl>  <nl> do {
static int iwl_store_ucode_sec ( struct iwl_firmware_pieces * pieces , <nl>  <nl> sec -> offset = le32_to_cpu ( sec_parse -> offset ); <nl> sec -> data = sec_parse -> data ; <nl> + sec -> size = size - sizeof ( sec_parse -> offset ); <nl>  <nl> ++ img -> sec_counter ; <nl> 
qla2x00_do_dpc ( void * data ) <nl> } else { <nl> fcport -> login_retry = 0 ; <nl> } <nl> - if ( fcport -> login_retry == 0 ) <nl> + if ( fcport -> login_retry == 0 && status != QLA_SUCCESS ) <nl> fcport -> loop_id = FC_NO_LOOP_ID ; <nl> } <nl> if ( test_bit ( LOOP_RESYNC_NEEDED , & ha -> dpc_flags ))
int acpi_bus_start ( struct acpi_device * device ) <nl> { <nl> struct acpi_bus_ops ops ; <nl>  <nl> + if (! device ) <nl> + return - EINVAL ; <nl> + <nl> memset (& ops , 0 , sizeof ( ops )); <nl> ops . acpi_op_start = 1 ; <nl> 
extern struct debug_obj_descr rcuhead_debug_descr ; <nl>  <nl> static inline void debug_rcu_head_queue ( struct rcu_head * head ) <nl> { <nl> + WARN_ON_ONCE (( unsigned long ) head & 0x3 ); <nl> debug_object_activate ( head , & rcuhead_debug_descr ); <nl> debug_object_active_state ( head , & rcuhead_debug_descr , <nl> STATE_RCU_HEAD_READY ,
static int gb_uart_flush ( struct gb_tty * gb_tty , u8 flags ) <nl> & request , sizeof ( request ), NULL , 0 ); <nl> } <nl>  <nl> - static struct gb_tty * get_gb_by_minor ( unsigned minor ) <nl> + static struct gb_tty * get_gb_by_minor ( unsigned int minor ) <nl> { <nl> struct gb_tty * gb_tty ; <nl> 
static u32 Handle_ListenStateExpired ( struct host_if_drv * hif_drv , <nl> wid . size = 2 ; <nl> wid . val = kmalloc ( wid . size , GFP_KERNEL ); <nl>  <nl> - if (! wid . val ) <nl> + if (! wid . val ) { <nl> PRINT_ER (" Failed to allocate memory \ n "); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> wid . val [ 0 ] = u8remain_on_chan_flag ; <nl> wid . val [ 1 ] = FALSE_FRMWR_CHANNEL ;
qla2x00_configure_fabric ( scsi_qla_host_t * vha ) <nl> fcport -> d_id . b . domain , <nl> fcport -> d_id . b . area , <nl> fcport -> d_id . b . al_pa ); <nl> - fcport -> loop_id = FC_NO_LOOP_ID ; <nl> + qla2x00_clear_loop_id ( fcport ); <nl> } <nl> } <nl> }
static void ax25_kill_by_device ( struct net_device * dev ) <nl> ax25_for_each ( s , & ax25_list ) { <nl> if ( s -> ax25_dev == ax25_dev ) { <nl> sk = s -> sk ; <nl> + if (! sk ) { <nl> + spin_unlock_bh (& ax25_list_lock ); <nl> + s -> ax25_dev = NULL ; <nl> + ax25_disconnect ( s , ENETUNREACH ); <nl> + spin_lock_bh (& ax25_list_lock ); <nl> + goto again ; <nl> + } <nl> sock_hold ( sk ); <nl> spin_unlock_bh (& ax25_list_lock ); <nl> lock_sock ( sk );
static void option_instat_callback ( struct urb * urb ) <nl> dev_dbg ( dev , "% s : type % x req % x \ n ", __func__ , <nl> req_pkt -> bRequestType , req_pkt -> bRequest ); <nl> } <nl> + } else if ( status == - ENOENT || status == - ESHUTDOWN ) { <nl> + dev_dbg ( dev , "% s : urb stopped : % d \ n ", __func__ , status ); <nl> } else <nl> dev_err ( dev , "% s : error % d \ n ", __func__ , status ); <nl> 
void snd_pcm_period_elapsed ( struct snd_pcm_substream * substream ) <nl> snd_timer_interrupt ( substream -> timer , 1 ); <nl> # endif <nl> _end : <nl> - snd_pcm_stream_unlock_irqrestore ( substream , flags ); <nl> kill_fasync (& runtime -> fasync , SIGIO , POLL_IN ); <nl> + snd_pcm_stream_unlock_irqrestore ( substream , flags ); <nl> } <nl>  <nl> EXPORT_SYMBOL ( snd_pcm_period_elapsed );
static int gic_shared_irq_domain_map ( struct irq_domain * d , unsigned int virq , <nl>  <nl> spin_lock_irqsave (& gic_lock , flags ); <nl> gic_map_to_pin ( intr , gic_cpu_pin ); <nl> - gic_map_to_vpe ( intr , vpe ); <nl> + gic_map_to_vpe ( intr , mips_cm_vp_id ( vpe )); <nl> for ( i = 0 ; i < min ( gic_vpes , NR_CPUS ); i ++) <nl> clear_bit ( intr , pcpu_masks [ i ]. pcpu_mask ); <nl> set_bit ( intr , pcpu_masks [ vpe ]. pcpu_mask );
static struct btrfs_trans_handle * start_transaction ( struct btrfs_root * root , <nl> */ <nl> if ( type != TRANS_JOIN_NOLOCK && <nl> ! __sb_start_write ( root -> fs_info -> sb , SB_FREEZE_FS , false )) { <nl> - if ( type == TRANS_JOIN_FREEZE ) <nl> + if ( type == TRANS_JOIN_FREEZE ) { <nl> + kmem_cache_free ( btrfs_trans_handle_cachep , h ); <nl> return ERR_PTR (- EPERM ); <nl> + } <nl> sb_start_intwrite ( root -> fs_info -> sb ); <nl> } <nl> 
static int usbdev_open ( struct inode * inode , struct file * file ) <nl> struct dev_state * ps ; <nl> int ret ; <nl>  <nl> + lock_kernel (); <nl> /* Protect against simultaneous removal or release */ <nl> mutex_lock (& usbfs_mutex ); <nl>  <nl> static int usbdev_open ( struct inode * inode , struct file * file ) <nl> if ( ret ) <nl> kfree ( ps ); <nl> mutex_unlock (& usbfs_mutex ); <nl> + unlock_kernel (); <nl> return ret ; <nl> } <nl> 
static int nvm_get_dir_info ( struct net_device * dev , u32 * entries , u32 * length ) <nl>  <nl> static int bnxt_get_eeprom_len ( struct net_device * dev ) <nl> { <nl> + struct bnxt * bp = netdev_priv ( dev ); <nl> + <nl> + if ( BNXT_VF ( bp )) <nl> + return 0 ; <nl> + <nl> /* The - 1 return value allows the entire 32 - bit range of offsets to be <nl> * passed via the ethtool command - line utility . <nl> */
static struct parser_context * parser_init_byte_stream ( u64 addr , u32 bytes , <nl> return ctx ; <nl>  <nl> err_finish_ctx : <nl> - parser_done ( ctx ); <nl> + kfree ( ctx ); <nl> return NULL ; <nl> } <nl> 
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> + if (! nodes_subset ( new , node_online_map )) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> err = security_task_movememory ( task ); <nl> if ( err ) <nl> goto out ;
static void gfx_v8_0_ring_emit_vm_flush ( struct amdgpu_ring * ring , <nl>  <nl> amdgpu_ring_write ( ring , PACKET3 ( PACKET3_WAIT_REG_MEM , 5 )); <nl> amdgpu_ring_write ( ring , ( WAIT_REG_MEM_MEM_SPACE ( 1 ) | /* memory */ <nl> - WAIT_REG_MEM_FUNCTION ( 3 ))); /* equal */ <nl> + WAIT_REG_MEM_FUNCTION ( 3 ) | /* equal */ <nl> + WAIT_REG_MEM_ENGINE ( usepfp ))); /* pfp or me */ <nl> amdgpu_ring_write ( ring , addr & 0xfffffffc ); <nl> amdgpu_ring_write ( ring , upper_32_bits ( addr ) & 0xffffffff ); <nl> amdgpu_ring_write ( ring , seq );
static int apparmor_setprocattr ( struct task_struct * task , char * name , <nl> sa . aad . op = OP_SETPROCATTR ; <nl> sa . aad . info = name ; <nl> sa . aad . error = - EINVAL ; <nl> - return aa_audit ( AUDIT_APPARMOR_DENIED , NULL , GFP_KERNEL , <nl> + return aa_audit ( AUDIT_APPARMOR_DENIED , <nl> + __aa_current_profile (), GFP_KERNEL , <nl> & sa , NULL ); <nl> } <nl> } else if ( strcmp ( name , " exec ") == 0 ) {
static void init_once ( void * foo ) <nl>  <nl> static struct inode * befs_iget ( struct super_block * sb , unsigned long ino ) <nl> { <nl> - struct buffer_head * bh = NULL ; <nl> + struct buffer_head * bh ; <nl> befs_inode * raw_inode = NULL ; <nl> struct befs_sb_info * befs_sb = BEFS_SB ( sb ); <nl> struct befs_inode_info * befs_ino = NULL ;
static struct proc_dir_entry * proc_sn2_ptc ; <nl>  <nl> static int __init sn2_ptc_init ( void ) <nl> { <nl> + if (! ia64_platform_is (" sn2 ")) <nl> + return - ENOSYS ; <nl> + <nl> if (!( proc_sn2_ptc = create_proc_entry ( PTC_BASENAME , 0444 , NULL ))) { <nl> printk ( KERN_ERR " unable to create % s proc entry ", PTC_BASENAME ); <nl> return - EINVAL ;
static void cciss_geometry_inquiry ( int ctlr , int logvol , <nl> " does not support reading geometry \ n "); <nl> drv -> heads = 255 ; <nl> drv -> sectors = 32 ; // Sectors per track <nl> + drv -> raid_level = RAID_UNKNOWN ; <nl> } else { <nl> drv -> heads = inq_buff -> data_byte [ 6 ]; <nl> drv -> sectors = inq_buff -> data_byte [ 7 ];
static void t3_io_resume ( struct pci_dev * pdev ) <nl> CH_ALERT ( adapter , " adapter recovering , PEX ERR 0x % x \ n ", <nl> t3_read_reg ( adapter , A_PCIE_PEX_ERR )); <nl>  <nl> + rtnl_lock (); <nl> t3_resume_ports ( adapter ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> static const struct pci_error_handlers t3_err_handler = {
dev_config ( struct file * fd , const char __user * buf , size_t len , loff_t * ptr ) <nl> spin_lock_irq (& dev -> lock ); <nl> value = - EINVAL ; <nl> if ( dev -> buf ) { <nl> + spin_unlock_irq (& dev -> lock ); <nl> kfree ( kbuf ); <nl> - goto fail ; <nl> + return value ; <nl> } <nl> dev -> buf = kbuf ; <nl> 
validate_event ( struct pmu_hw_events * hw_events , <nl> struct arm_pmu * armpmu = to_arm_pmu ( event -> pmu ); <nl> struct pmu * leader_pmu = event -> group_leader -> pmu ; <nl>  <nl> + if ( is_software_event ( event )) <nl> + return 1 ; <nl> + <nl> if ( event -> pmu != leader_pmu || event -> state < PERF_EVENT_STATE_OFF ) <nl> return 1 ; <nl> 
static int eb_copy_relocations ( const struct i915_execbuffer * eb ) <nl> min_t ( u64 , BIT_ULL ( 31 ), size - copied ); <nl>  <nl> if ( __copy_from_user (( char *) relocs + copied , <nl> - ( char *) urelocs + copied , <nl> + ( char __user *) urelocs + copied , <nl> len )) { <nl> kvfree ( relocs ); <nl> err = - EFAULT ;
static unsigned long axi_clkgen_recalc_rate ( struct clk_hw * clk_hw , <nl> tmp = ( unsigned long long )( parent_rate / d ) * m ; <nl> do_div ( tmp , dout ); <nl>  <nl> - if ( tmp > ULONG_MAX ) <nl> - return ULONG_MAX ; <nl> - <nl> - return tmp ; <nl> + return min_t ( unsigned long long , tmp , ULONG_MAX ); <nl> } <nl>  <nl> static int axi_clkgen_enable ( struct clk_hw * clk_hw )
static void __split_and_process_bio ( struct mapped_device * md , <nl> } <nl>  <nl> /* drop the extra reference count */ <nl> - dec_pending ( ci . io , error ); <nl> + dec_pending ( ci . io , errno_to_blk_status ( error )); <nl> } <nl> /*----------------------------------------------------------------- <nl> * CRUD END
static int ascot2e_write_regs ( struct ascot2e_priv * priv , <nl> } <nl> }; <nl>  <nl> - if ( len + 1 >= sizeof ( buf )) { <nl> + if ( len + 1 > sizeof ( buf )) { <nl> dev_warn (& priv -> i2c -> dev ," wr reg =% 04x : len =% d is too big !\ n ", <nl> reg , len + 1 ); <nl> return - E2BIG ;
static void process_checks ( struct r1bio * r1_bio ) <nl> struct page ** ppages = get_resync_pages ( pbio )-> pages ; <nl> struct page ** spages = get_resync_pages ( sbio )-> pages ; <nl> struct bio_vec * bi ; <nl> - int page_len [ RESYNC_PAGES ]; <nl> + int page_len [ RESYNC_PAGES ] = { 0 }; <nl>  <nl> if ( sbio -> bi_end_io != end_sync_read ) <nl> continue ;
static int fill_inode ( struct inode * inode , struct page * locked_page , <nl> } <nl>  <nl> /* finally update i_version */ <nl> - ci -> i_version = le64_to_cpu ( info -> version ); <nl> + if ( le64_to_cpu ( info -> version ) > ci -> i_version ) <nl> + ci -> i_version = le64_to_cpu ( info -> version ); <nl>  <nl> inode -> i_mapping -> a_ops = & ceph_aops ; <nl> 
tracing_iter_ctrl_write ( struct file * filp , const char __user * ubuf , <nl> break ; <nl> } <nl> } <nl> + /* <nl> + * If no option could be set , return an error : <nl> + */ <nl> + if (! trace_options [ i ]) <nl> + return - EINVAL ; <nl>  <nl> filp -> f_pos += cnt ; <nl> 
drm_atomic_plane_get_property ( struct drm_plane * plane , <nl> * val = state -> src_w ; <nl> } else if ( property == config -> prop_src_h ) { <nl> * val = state -> src_h ; <nl> + } else if ( property == config -> rotation_property ) { <nl> + * val = state -> rotation ; <nl> } else if ( plane -> funcs -> atomic_get_property ) { <nl> return plane -> funcs -> atomic_get_property ( plane , state , property , val ); <nl> } else {
static int hi3660_stub_clk_probe ( struct platform_device * pdev ) <nl> return PTR_ERR ( stub_clk_chan . mbox ); <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> + if (! res ) <nl> + return - EINVAL ; <nl> freq_reg = devm_ioremap ( dev , res -> start , resource_size ( res )); <nl> if (! freq_reg ) <nl> return - ENOMEM ;
static int snd_ctl_elem_add ( struct snd_ctl_file * file , <nl> if ( ue == NULL ) <nl> return - ENOMEM ; <nl> ue -> info = * info ; <nl> + ue -> info . access = 0 ; <nl> ue -> elem_data = ( char *) ue + sizeof (* ue ); <nl> ue -> elem_data_size = private_size ; <nl> kctl . private_free = snd_ctl_elem_user_free ;
static int __devexit wl1271_remove ( struct spi_device * spi ) <nl>  <nl> static struct spi_driver wl1271_spi_driver = { <nl> . driver = { <nl> - . name = " wl1271 ", <nl> + . name = " wl1271_spi ", <nl> . bus = & spi_bus_type , <nl> . owner = THIS_MODULE , <nl> },
static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh ) <nl> clear_buffer_mapped ( bh ); <nl> clear_buffer_req ( bh ); <nl> clear_buffer_new ( bh ); <nl> + clear_buffer_delay ( bh ); <nl> + clear_buffer_unwritten ( bh ); <nl> bh -> b_bdev = NULL ; <nl> return may_free ; <nl> }
static inline void __fsnotify_update_dcache_flags ( struct dentry * dentry ) <nl> assert_spin_locked (& dentry -> d_lock ); <nl>  <nl> parent = dentry -> d_parent ; <nl> - if ( fsnotify_inode_watches_children ( parent -> d_inode )) <nl> + if ( parent -> d_inode && fsnotify_inode_watches_children ( parent -> d_inode )) <nl> dentry -> d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED ; <nl> else <nl> dentry -> d_flags &= ~ DCACHE_FSNOTIFY_PARENT_WATCHED ;
static int __init hp_wmi_input_setup ( void ) <nl> int err ; <nl>  <nl> hp_wmi_input_dev = input_allocate_device (); <nl> + if (! hp_wmi_input_dev ) <nl> + return - ENOMEM ; <nl>  <nl> hp_wmi_input_dev -> name = " HP WMI hotkeys "; <nl> hp_wmi_input_dev -> phys = " wmi / input0 ";
int io_msg_ring ( struct io_kiocb * req , unsigned int issue_flags ) <nl> req_set_fail ( req ); <nl> io_req_set_res ( req , ret , 0 ); <nl> /* put file to avoid an attempt to IOPOLL the req */ <nl> - io_put_file ( req -> file ); <nl> + if (!( req -> flags & REQ_F_FIXED_FILE )) <nl> + io_put_file ( req -> file ); <nl> req -> file = NULL ; <nl> return IOU_OK ; <nl> }
void __init mem_init ( void ) <nl>  <nl> pci_iommu_alloc (); <nl>  <nl> - /* clear the zero - page */ <nl> - memset ( empty_zero_page , 0 , PAGE_SIZE ); <nl> + /* clear_bss () already clear the empty_zero_page */ <nl>  <nl> reservedpages = 0 ; <nl> 
static int lmp91000_probe ( struct i2c_client * client , <nl> indio_dev -> channels = lmp91000_channels ; <nl> indio_dev -> num_channels = ARRAY_SIZE ( lmp91000_channels ); <nl> indio_dev -> name = LMP91000_DRV_NAME ; <nl> + indio_dev -> dev . parent = & client -> dev ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl> i2c_set_clientdata ( client , indio_dev ); <nl> 
static int rtl2832u_get_rc_config ( struct dvb_usb_device * d , <nl> rc -> allowed_protos = RC_BIT_ALL ; <nl> rc -> driver_type = RC_DRIVER_IR_RAW ; <nl> rc -> query = rtl2832u_rc_query ; <nl> - rc -> interval = 400 ; <nl> + rc -> interval = 200 ; <nl>  <nl> return 0 ; <nl> }
static void __ip_rt_update_pmtu ( struct rtable * rt , struct flowi4 * fl4 , u32 mtu ) <nl> if ( mtu < ip_rt_min_pmtu ) <nl> mtu = ip_rt_min_pmtu ; <nl>  <nl> + if ( rt -> rt_pmtu == mtu && <nl> + time_before ( jiffies , dst -> expires - ip_rt_mtu_expires / 2 )) <nl> + return ; <nl> + <nl> rcu_read_lock (); <nl> if ( fib_lookup ( dev_net ( dst -> dev ), fl4 , & res ) == 0 ) { <nl> struct fib_nh * nh = & FIB_RES_NH ( res );
nouveau_fbcon_destroy ( struct drm_device * dev , struct nouveau_fbdev * fbcon ) <nl> drm_fb_helper_unregister_fbi (& fbcon -> helper ); <nl> drm_fb_helper_fini (& fbcon -> helper ); <nl>  <nl> - if ( nouveau_fb -> nvbo ) { <nl> + if ( nouveau_fb && nouveau_fb -> nvbo ) { <nl> nouveau_vma_del (& nouveau_fb -> vma ); <nl> nouveau_bo_unmap ( nouveau_fb -> nvbo ); <nl> nouveau_bo_unpin ( nouveau_fb -> nvbo );
static int ucb1400_ts_remove ( struct platform_device * dev ) <nl> # ifdef CONFIG_PM <nl> static int ucb1400_ts_resume ( struct platform_device * dev ) <nl> { <nl> - struct ucb1400_ts * ucb = platform_get_drvdata ( dev ); <nl> + struct ucb1400_ts * ucb = dev -> dev . platform_data ; <nl>  <nl> if ( ucb -> ts_task ) { <nl> /*
void kvm_check_async_pf_completion ( struct kvm_vcpu * vcpu ) <nl> spin_unlock (& vcpu -> async_pf . lock ); <nl>  <nl> kvm_arch_async_page_ready ( vcpu , work ); <nl> - kvm_arch_async_page_present ( vcpu , work ); <nl> + kvm_async_page_present_async ( vcpu , work ); <nl>  <nl> list_del (& work -> queue ); <nl> vcpu -> async_pf . queued --;
static ssize_t fuse_dev_splice_read ( struct file * in , loff_t * ppos , <nl> * code can Oops if the buffer persists after module unload . <nl> */ <nl> bufs [ page_nr ]. ops = & nosteal_pipe_buf_ops ; <nl> + bufs [ page_nr ]. flags = 0 ; <nl> ret = add_to_pipe ( pipe , & bufs [ page_nr ++]); <nl> if ( unlikely ( ret < 0 )) <nl> break ;
static int complete_emulated_mmio ( struct kvm_vcpu * vcpu ) <nl> frag -> len -= len ; <nl> } <nl>  <nl> - if ( vcpu -> mmio_cur_fragment == vcpu -> mmio_nr_fragments ) { <nl> + if ( vcpu -> mmio_cur_fragment >= vcpu -> mmio_nr_fragments ) { <nl> vcpu -> mmio_needed = 0 ; <nl>  <nl> /* FIXME : return into emulator if single - stepping . */
static int octeon_usb_hub_status_data ( struct usb_hcd * hcd , char * buf ) <nl> buf [ 0 ] = 0 ; <nl> buf [ 0 ] = port_status . connect_change << 1 ; <nl>  <nl> - return ( buf [ 0 ] != 0 ); <nl> + return buf [ 0 ] != 0 ; <nl> } <nl>  <nl> static int octeon_usb_hub_control ( struct usb_hcd * hcd , u16 typeReq , u16 wValue , u16 wIndex , char * buf , u16 wLength )
void mctp_dev_hold ( struct mctp_dev * mdev ) <nl> void mctp_dev_put ( struct mctp_dev * mdev ) <nl> { <nl> if ( mdev && refcount_dec_and_test (& mdev -> refs )) { <nl> + kfree ( mdev -> addrs ); <nl> dev_put ( mdev -> dev ); <nl> kfree_rcu ( mdev , rcu ); <nl> } <nl> static void mctp_unregister ( struct net_device * dev ) <nl>  <nl> mctp_route_remove_dev ( mdev ); <nl> mctp_neigh_remove_dev ( mdev ); <nl> - kfree ( mdev -> addrs ); <nl>  <nl> mctp_dev_put ( mdev ); <nl> }
static int pcc_get_offset ( int cpu ) <nl> pr = per_cpu ( processors , cpu ); <nl> pcc_cpu_data = per_cpu_ptr ( pcc_cpu_info , cpu ); <nl>  <nl> + if (! pr ) <nl> + return - ENODEV ; <nl> + <nl> status = acpi_evaluate_object ( pr -> handle , " PCCP ", NULL , & buffer ); <nl> if ( ACPI_FAILURE ( status )) <nl> return - ENODEV ;
static void kone_keep_values_up_to_date ( struct kone_device * kone , <nl> { <nl> switch ( event -> event ) { <nl> case kone_mouse_event_switch_profile : <nl> + kone -> actual_dpi = kone -> profiles [ event -> value - 1 ]. <nl> + startup_dpi ; <nl> case kone_mouse_event_osd_profile : <nl> kone -> actual_profile = event -> value ; <nl> - kone -> actual_dpi = kone -> profiles [ kone -> actual_profile - 1 ]. <nl> - startup_dpi ; <nl> break ; <nl> case kone_mouse_event_switch_dpi : <nl> case kone_mouse_event_osd_dpi :
static int ti_pipe3_exit ( struct phy * x ) <nl> u32 val ; <nl> unsigned long timeout ; <nl>  <nl> + /* SATA DPLL can ' t be powered down due to Errata i783 */ <nl> + if ( of_device_is_compatible ( phy -> dev -> of_node , " ti , phy - pipe3 - sata ")) <nl> + return 0 ; <nl> + <nl> /* Put DPLL in IDLE mode */ <nl> val = ti_pipe3_readl ( phy -> pll_ctrl_base , PLL_CONFIGURATION2 ); <nl> val |= PLL_IDLE ;
int mdfld_dsi_send_gen_short ( struct mdfld_dsi_pkg_sender * sender , u8 param0 , <nl> unsigned long flags ; <nl> u8 data_type ; <nl>  <nl> - if (! sender || param_num < 0 || param_num > 2 ) { <nl> + if (! sender || param_num > 2 ) { <nl> DRM_ERROR (" Invalid parameter \ n "); <nl> return - EINVAL ; <nl> }
struct vmxnet3_adapter { <nl> struct net_device * netdev ; <nl> struct pci_dev * pdev ; <nl>  <nl> - u8 * hw_addr0 ; /* for BAR 0 */ <nl> - u8 * hw_addr1 ; /* for BAR 1 */ <nl> + u8 __iomem * hw_addr0 ; /* for BAR 0 */ <nl> + u8 __iomem * hw_addr1 ; /* for BAR 1 */ <nl>  <nl> /* feature control */ <nl> bool rxcsum ;
static inline pte_t huge_ptep_get_and_clear ( struct mm_struct * mm , <nl> static inline void huge_ptep_clear_flush ( struct vm_area_struct * vma , <nl> unsigned long addr , pte_t * ptep ) <nl> { <nl> + ptep_clear_flush ( vma , addr , ptep ); <nl> } <nl>  <nl> static inline int huge_pte_none ( pte_t pte )
static int fuse_notify_inval_entry ( struct fuse_conn * fc , unsigned int size , <nl> if ( outarg . namelen > FUSE_NAME_MAX ) <nl> goto err ; <nl>  <nl> + err = - EINVAL ; <nl> + if ( size != sizeof ( outarg ) + outarg . namelen + 1 ) <nl> + goto err ; <nl> + <nl> name . name = buf ; <nl> name . len = outarg . namelen ; <nl> err = fuse_copy_one ( cs , buf , outarg . namelen + 1 );
static enum io_status ccwreq_status ( struct ccw_device * cdev , struct irb * lcirb ) <nl> /* Ask the driver what to do */ <nl> if ( cdev -> drv && cdev -> drv -> uc_handler ) { <nl> todo = cdev -> drv -> uc_handler ( cdev , lcirb ); <nl> + CIO_TRACE_EVENT ( 2 , " uc_response "); <nl> + CIO_HEX_EVENT ( 2 , & todo , sizeof ( todo )); <nl> switch ( todo ) { <nl> case UC_TODO_RETRY : <nl> return IO_STATUS_ERROR ;
static int tegra_io_pad_prepare ( enum tegra_io_pad id , unsigned long * request , <nl> } <nl>  <nl> rate = clk_get_rate ( pmc -> clk ); <nl> + if (! rate ) <nl> + return - ENODEV ; <nl>  <nl> tegra_pmc_writel ( DPD_SAMPLE_ENABLE , DPD_SAMPLE ); <nl> 
int __init acpi_debugfs_init ( void ) <nl> if (! acpi_dir ) <nl> goto err ; <nl>  <nl> - cm_dentry = debugfs_create_file (" custom_method ", S_IWUGO , <nl> + cm_dentry = debugfs_create_file (" custom_method ", S_IWUSR , <nl> acpi_dir , NULL , & cm_fops ); <nl> if (! cm_dentry ) <nl> goto err ;
static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> ctl_dir = container_of ( head , struct ctl_dir , header ); <nl>  <nl> if (! dir_emit_dots ( file , ctx )) <nl> - return 0 ; <nl> + goto out ; <nl>  <nl> pos = 2 ; <nl>  <nl> static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> break ; <nl> } <nl> } <nl> + out : <nl> sysctl_head_finish ( head ); <nl> return 0 ; <nl> }
static void save_microcode_patch ( void * data , unsigned int size ) <nl> p = memdup_patch ( data , size ); <nl> if (! p ) <nl> pr_err (" Error allocating buffer % p \ n ", data ); <nl> - else <nl> + else { <nl> list_replace (& iter -> plist , & p -> plist ); <nl> + kfree ( iter -> data ); <nl> + kfree ( iter ); <nl> + } <nl> } <nl> } <nl> 
int btrfs_read_sys_array ( struct btrfs_root * root ) <nl> if ( ret ) <nl> break ; <nl> } else { <nl> + printk ( KERN_ERR <nl> + " BTRFS : unexpected item type % u in sys_array at offset % u \ n ", <nl> + ( u32 ) key . type , cur_offset ); <nl> ret = - EIO ; <nl> break ; <nl> }
static void devm_pci_release_host_bridge_dev ( struct device * dev ) <nl>  <nl> if ( bridge -> release_fn ) <nl> bridge -> release_fn ( bridge ); <nl> + <nl> + pci_free_resource_list (& bridge -> windows ); <nl> } <nl>  <nl> static void pci_release_host_bridge_dev ( struct device * dev ) <nl> { <nl> devm_pci_release_host_bridge_dev ( dev ); <nl> - pci_free_host_bridge ( to_pci_host_bridge ( dev )); <nl> + kfree ( to_pci_host_bridge ( dev )); <nl> } <nl>  <nl> struct pci_host_bridge * pci_alloc_host_bridge ( size_t priv )
static int create_encryption_context_from_policy ( struct inode * inode , <nl> int fscrypt_process_policy ( struct inode * inode , <nl> const struct fscrypt_policy * policy ) <nl> { <nl> + if (! inode_owner_or_capable ( inode )) <nl> + return - EACCES ; <nl> + <nl> if ( policy -> version != 0 ) <nl> return - EINVAL ; <nl> 
s32 igb_get_phy_id ( struct e1000_hw * hw ) <nl> u16 phy_id ; <nl>  <nl> /* ensure PHY page selection to fix misconfigured i210 */ <nl> - if ( hw -> mac . type == e1000_i210 ) <nl> + if (( hw -> mac . type == e1000_i210 ) || ( hw -> mac . type == e1000_i211 )) <nl> phy -> ops . write_reg ( hw , I347AT4_PAGE_SELECT , 0 ); <nl>  <nl> ret_val = phy -> ops . read_reg ( hw , PHY_ID1 , & phy_id );
static void alc_auto_init_extra_out ( struct hda_codec * codec ) <nl> int i ; <nl> hda_nid_t pin , dac ; <nl>  <nl> - for ( i = 0 ; i < spec -> autocfg . speaker_outs ; i ++) { <nl> + for ( i = 0 ; i < spec -> autocfg . hp_outs ; i ++) { <nl> pin = spec -> autocfg . hp_pins [ i ]; <nl> if (! pin ) <nl> break ;
static void spear_smi_hw_init ( struct spear_smi * dev ) <nl> val = HOLD1 | BANK_EN | DSEL_TIME | ( prescale << 8 ); <nl>  <nl> mutex_lock (& dev -> lock ); <nl> + /* clear all interrupt conditions */ <nl> + writel ( 0 , dev -> io_base + SMI_SR ); <nl> + <nl> writel ( val , dev -> io_base + SMI_CR1 ); <nl> mutex_unlock (& dev -> lock ); <nl> }
int ath_tx_aggr_start ( struct ath_softc * sc , struct ieee80211_sta * sta , <nl> txtid -> paused = true ; <nl> * ssn = txtid -> seq_start = txtid -> seq_next ; <nl>  <nl> + memset ( txtid -> tx_buf , 0 , sizeof ( txtid -> tx_buf )); <nl> + txtid -> baw_head = txtid -> baw_tail = 0 ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int omap_system_dma_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl>  <nl> if ( dma_omap2plus ()) { <nl> - dma_linked_lch = kzalloc ( sizeof ( struct dma_link_info ) * <nl> - dma_lch_count , GFP_KERNEL ); <nl> + dma_linked_lch = kcalloc ( dma_lch_count , <nl> + sizeof (* dma_linked_lch ), <nl> + GFP_KERNEL ); <nl> if (! dma_linked_lch ) { <nl> ret = - ENOMEM ; <nl> goto exit_dma_lch_fail ;
static void gdm_usb_disconnect ( struct usb_interface * intf ) <nl> { <nl> struct phy_dev * phy_dev ; <nl> struct lte_udev * udev ; <nl> - u16 idVendor , idProduct ; <nl> struct usb_device * usbdev ; <nl>  <nl> usbdev = interface_to_usbdev ( intf ); <nl> - <nl> - idVendor = __le16_to_cpu ( usbdev -> descriptor . idVendor ); <nl> - idProduct = __le16_to_cpu ( usbdev -> descriptor . idProduct ); <nl> - <nl> phy_dev = usb_get_intfdata ( intf ); <nl>  <nl> udev = phy_dev -> priv_dev ;
static int check_vmentry_postreqs ( struct kvm_vcpu * vcpu , struct vmcs12 * vmcs12 , <nl> return 1 ; <nl> } <nl>  <nl> + if (( vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS ) && <nl> + ( is_noncanonical_address ( vmcs12 -> guest_bndcfgs & PAGE_MASK , vcpu ) || <nl> + ( vmcs12 -> guest_bndcfgs & MSR_IA32_BNDCFGS_RSVD ))) <nl> + return 1 ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int iterate_dir_item ( struct btrfs_root * root , struct btrfs_path * path , <nl> buf = tmp ; <nl> } <nl> if (! buf ) { <nl> - buf = vmalloc ( buf_len ); <nl> + buf = kvmalloc ( buf_len , GFP_KERNEL ); <nl> if (! buf ) { <nl> ret = - ENOMEM ; <nl> goto out ;
static int threshold_create_bank ( unsigned int cpu , unsigned int bank ) <nl> const char * name = get_name ( bank , NULL ); <nl> int err = 0 ; <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( is_shared_bank ( bank )) { <nl> nb = node_to_amd_nb ( amd_get_nb_id ( cpu )); <nl> 
int vfio_pci_set_irqs_ioctl ( struct vfio_pci_device * vdev , uint32_t flags , <nl> func = vfio_pci_set_err_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> case VFIO_PCI_REQ_IRQ_INDEX : <nl> switch ( flags & VFIO_IRQ_SET_ACTION_TYPE_MASK ) { <nl> case VFIO_IRQ_SET_ACTION_TRIGGER : <nl> func = vfio_pci_set_req_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> } <nl>  <nl> if (! func )
tegra_xusb_find_port_node ( struct tegra_xusb_padctl * padctl , const char * type , <nl> name = kasprintf ( GFP_KERNEL , "% s -% u ", type , index ); <nl> if (! name ) { <nl> of_node_put ( ports ); <nl> - return ERR_PTR (- ENOMEM ); <nl> + return NULL ; <nl> } <nl> np = of_get_child_by_name ( ports , name ); <nl> kfree ( name );
static int max1363_probe ( struct i2c_client * client , <nl> goto error_out ; <nl> } <nl>  <nl> + indio_dev -> dev . of_node = client -> dev . of_node ; <nl> ret = iio_map_array_register ( indio_dev , client -> dev . platform_data ); <nl> if ( ret < 0 ) <nl> goto error_free_device ;
static int cnic_start_hw ( struct cnic_dev * dev ) <nl> return 0 ; <nl>  <nl> err1 : <nl> - cp -> free_resc ( dev ); <nl> + if ( ethdev -> drv_state & CNIC_DRV_STATE_HANDLES_IRQ ) <nl> + cp -> stop_hw ( dev ); <nl> + else <nl> + cp -> free_resc ( dev ); <nl> pci_dev_put ( dev -> pcidev ); <nl> return err ; <nl> }
dcb_outp_parse ( struct nouveau_bios * bios , u8 idx , u8 * ver , u8 * len , <nl> struct dcb_output * outp ) <nl> { <nl> u16 dcb = dcb_outp ( bios , idx , ver , len ); <nl> + memset ( outp , 0x00 , sizeof (* outp )); <nl> if ( dcb ) { <nl> if (* ver >= 0x20 ) { <nl> u32 conn = nv_ro32 ( bios , dcb + 0x00 );
static int snd_sst_fill_kernel_list ( struct stream_info * stream , <nl> return - ENOMEM ; <nl> if ( copy_from_user (( void *) & rar_handle , <nl> iovec [ index ]. iov_base , <nl> - sizeof ( __u32 ))) <nl> + sizeof ( __u32 ))) { <nl> + kfree ( stream_bufs ); <nl> return - EFAULT ; <nl> + } <nl> stream_bufs -> addr = ( char *) rar_handle ; <nl> stream_bufs -> in_use = false ; <nl> stream_bufs -> size = iovec [ 0 ]. iov_len ;
static int intel_ring_context_pin ( struct intel_engine_cs * engine , <nl> ret = context_pin ( ctx ); <nl> if ( ret ) <nl> goto error ; <nl> + <nl> + ce -> state -> obj -> mm . dirty = true ; <nl> } <nl>  <nl> /* The kernel context is only used as a placeholder for flushing the
struct task_struct { <nl> int exit_state ; <nl> int exit_code , exit_signal ; <nl> int pdeath_signal ; /* The signal sent when the parent dies */ <nl> - unsigned int jobctl ; /* JOBCTL_ *, siglock protected */ <nl> + unsigned long jobctl ; /* JOBCTL_ *, siglock protected */ <nl>  <nl> /* Used for emulating ABI behavior of previous Linux versions */ <nl> unsigned int personality ;
static struct task_struct * producer ; <nl> static struct task_struct * consumer ; <nl> static unsigned long read ; <nl>  <nl> - static int disable_reader ; <nl> + static unsigned int disable_reader ; <nl> module_param ( disable_reader , uint , 0644 ); <nl> MODULE_PARM_DESC ( disable_reader , " only run producer "); <nl>  <nl> - static int write_iteration = 50 ; <nl> + static unsigned int write_iteration = 50 ; <nl> module_param ( write_iteration , uint , 0644 ); <nl> MODULE_PARM_DESC ( write_iteration , "# of writes between timestamp readings "); <nl> 
int hugetlb_mcopy_atomic_pte ( struct mm_struct * dst_mm , <nl> return ret ; <nl> out_release_unlock : <nl> spin_unlock ( ptl ); <nl> - out_release_nounlock : <nl> if ( vm_shared ) <nl> unlock_page ( page ); <nl> + out_release_nounlock : <nl> put_page ( page ); <nl> goto out ; <nl> }
int iwl_mvm_sched_scan_start ( struct iwl_mvm * mvm , <nl> return - EBUSY ; <nl> } <nl>  <nl> + /* we don ' t support " match all " in the firmware */ <nl> + if (! req -> n_match_sets ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> ret = iwl_mvm_check_running_scans ( mvm , type ); <nl> if ( ret ) <nl> return ret ;
static const struct file_operations evtchn_fops = { <nl>  <nl> static struct miscdevice evtchn_miscdev = { <nl> . minor = MISC_DYNAMIC_MINOR , <nl> - . name = " evtchn ", <nl> + . name = " xen / evtchn ", <nl> . fops = & evtchn_fops , <nl> }; <nl> static int __init evtchn_init ( void )
static irqreturn_t dcon_interrupt ( int irq , void * id ) <nl> return IRQ_HANDLED ; <nl> } <nl>  <nl> - static struct i2c_device_id dcon_idtable [] = { <nl> + static const struct i2c_device_id dcon_idtable [] = { <nl> { " olpc_dcon ", 0 }, <nl> { } <nl> }; <nl> static int __init olpc_dcon_init ( void ) <nl> { <nl> pdata = & dcon_pdata_xo_1 ; <nl>  <nl> - i2c_add_driver (& dcon_driver ); <nl> - return 0 ; <nl> + return i2c_add_driver (& dcon_driver ); <nl> } <nl>  <nl> static void __exit olpc_dcon_exit ( void )
static int __devinit pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> unsigned int delay ; <nl> u8 pon_cntl ; <nl> struct pmic8xxx_pwrkey * pwrkey ; <nl> - const struct pm8xxx_pwrkey_platform_data * pdata = mfd_get_data ( pdev ); <nl> + const struct pm8xxx_pwrkey_platform_data * pdata = <nl> + dev_get_platdata (& pdev -> dev ); <nl>  <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " power key platform data not supplied \ n ");
static int omninet_port_remove ( struct usb_serial_port * port ) <nl>  <nl> static int omninet_open ( struct tty_struct * tty , struct usb_serial_port * port ) <nl> { <nl> - struct usb_serial * serial = port -> serial ; <nl> - struct usb_serial_port * wport ; <nl> - <nl> - wport = serial -> port [ 1 ]; <nl> - tty_port_tty_set (& wport -> port , tty ); <nl> - <nl> return usb_serial_generic_open ( tty , port ); <nl> } <nl> 
EXPORT_SYMBOL ( clk_enable ); <nl>  <nl> void clk_disable ( struct clk * clk ) <nl> { <nl> + if (! clk ) <nl> + return ; <nl> + <nl> if ( clk -> ops && clk -> ops -> disable ) <nl> clk -> ops -> disable ( clk ); <nl> }
static int list_devices ( struct file * filp , struct dm_ioctl * param , size_t param_ <nl> * Grab our output buffer . <nl> */ <nl> nl = orig_nl = get_result_buffer ( param , param_size , & len ); <nl> - if ( len < needed ) { <nl> + if ( len < needed || len < sizeof ( nl -> dev )) { <nl> param -> flags |= DM_BUFFER_FULL_FLAG ; <nl> goto out ; <nl> }
static inline void callchain_init ( struct callchain_node * node ) <nl> INIT_LIST_HEAD (& node -> children ); <nl> INIT_LIST_HEAD (& node -> val ); <nl>  <nl> + node -> children_hit = 0 ; <nl> node -> parent = NULL ; <nl> node -> hit = 0 ; <nl> }
static int init_fb_chan ( struct mx3fb_data * mx3fb , struct idmac_channel * ichan ) <nl> init_completion (& mx3fbi -> flip_cmpl ); <nl> disable_irq ( ichan -> eof_irq ); <nl> dev_dbg ( mx3fb -> dev , " disabling irq % d \ n ", ichan -> eof_irq ); <nl> - ret = mx3fb_set_par ( fbi ); <nl> - if ( ret < 0 ) <nl> - goto esetpar ; <nl> - <nl> - mx3fb_blank ( FB_BLANK_UNBLANK , fbi ); <nl>  <nl> dev_info ( dev , " registered , using mode % s \ n ", fb_mode ); <nl> 
struct dst_entry * icmp6_dst_alloc ( struct net_device * dev , <nl> struct net * net = dev_net ( dev ); <nl>  <nl> if ( unlikely (! idev )) <nl> - return NULL ; <nl> + return ERR_PTR (- ENODEV ); <nl>  <nl> rt = ip6_dst_alloc (& net -> ipv6 . ip6_dst_ops , dev , 0 ); <nl> if ( unlikely (! rt )) {
struct sock * inet_csk_clone_lock ( const struct sock * sk , <nl> /* listeners have SOCK_RCU_FREE , not the children */ <nl> sock_reset_flag ( newsk , SOCK_RCU_FREE ); <nl>  <nl> + inet_sk ( newsk )-> mc_list = NULL ; <nl> + <nl> newsk -> sk_mark = inet_rsk ( req )-> ir_mark ; <nl> atomic64_set (& newsk -> sk_cookie , <nl> atomic64_read (& inet_rsk ( req )-> ir_cookie ));
static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
static void __init early_vmalloc ( char ** arg ) <nl> " vmalloc area too small , limiting to % luMB \ n ", <nl> vmalloc_reserve >> 20 ); <nl> } <nl> + <nl> + if ( vmalloc_reserve > VMALLOC_END - ( PAGE_OFFSET + SZ_32M )) { <nl> + vmalloc_reserve = VMALLOC_END - ( PAGE_OFFSET + SZ_32M ); <nl> + printk ( KERN_WARNING <nl> + " vmalloc area is too big , limiting to % luMB \ n ", <nl> + vmalloc_reserve >> 20 ); <nl> + } <nl> } <nl> __early_param (" vmalloc =", early_vmalloc ); <nl> 
static void solo_enc_free ( struct solo_enc_dev * solo_enc ) <nl> if ( solo_enc == NULL ) <nl> return ; <nl>  <nl> + pci_free_consistent ( solo_enc -> solo_dev -> pdev , <nl> + sizeof ( struct solo_p2m_desc ) * solo_enc -> desc_nelts , <nl> + solo_enc -> desc_items , solo_enc -> desc_dma ); <nl> video_unregister_device ( solo_enc -> vfd ); <nl> v4l2_ctrl_handler_free (& solo_enc -> hdl ); <nl> kfree ( solo_enc );
static struct clk * of_clk_gpio_delayed_register_get ( <nl> num_parents = of_clk_get_parent_count ( data -> node ); <nl>  <nl> parent_names = kcalloc ( num_parents , sizeof ( char *), GFP_KERNEL ); <nl> - if (! parent_names ) <nl> - return ERR_PTR (- ENOMEM ); <nl> + if (! parent_names ) { <nl> + clk = ERR_PTR (- ENOMEM ); <nl> + goto out ; <nl> + } <nl>  <nl> for ( i = 0 ; i < num_parents ; i ++) <nl> parent_names [ i ] = of_clk_get_parent_name ( data -> node , i );
static inline void omap3xxx_restart ( enum reboot_mode mode , const char * cmd ) <nl> } <nl> # endif <nl>  <nl> -# if defined ( CONFIG_ARCH_OMAP4 ) || defined ( CONFIG_SOC_OMAP5 ) <nl> +# if defined ( CONFIG_ARCH_OMAP4 ) || defined ( CONFIG_SOC_OMAP5 ) || \ <nl> + defined ( CONFIG_SOC_DRA7XX ) || defined ( CONFIG_SOC_AM43XX ) <nl> void omap44xx_restart ( enum reboot_mode mode , const char * cmd ); <nl> # else <nl> static inline void omap44xx_restart ( enum reboot_mode mode , const char * cmd )
static int sunxi_pinctrl_probe ( struct platform_device * pdev ) <nl> goto gpiochip_error ; <nl> } <nl>  <nl> - clk_prepare_enable ( clk ); <nl> + ret = clk_prepare_enable ( clk ); <nl> + if ( ret ) <nl> + goto gpiochip_error ; <nl>  <nl> pctl -> irq = irq_of_parse_and_map ( node , 0 ); <nl> if (! pctl -> irq ) {
static irqreturn_t sil24_interrupt ( int irq , void * dev_instance , struct pt_regs * <nl>  <nl> status = readl ( hpriv -> host_base + HOST_IRQ_STAT ); <nl>  <nl> + if ( status == 0xffffffff ) { <nl> + printk ( KERN_ERR DRV_NAME ": IRQ status == 0xffffffff , " <nl> + " PCI fault or device removal ?\ n "); <nl> + goto out ; <nl> + } <nl> + <nl> if (!( status & IRQ_STAT_4PORTS )) <nl> goto out ; <nl> 
static int fsl_elbc_chip_remove ( struct fsl_elbc_mtd * priv ) <nl>  <nl> elbc_fcm_ctrl -> chips [ priv -> bank ] = NULL ; <nl> kfree ( priv ); <nl> - kfree ( elbc_fcm_ctrl ); <nl> return 0 ; <nl> } <nl> 
xfs_qm_freelist_destroy ( xfs_frlist_t * ql ) <nl> xfs_qm_dqdestroy ( dqp ); <nl> dqp = nextdqp ; <nl> } <nl> - /* <nl> - * Don ' t bother about unlocking . <nl> - */ <nl> + mutex_unlock (& ql -> qh_lock ); <nl> mutex_destroy (& ql -> qh_lock ); <nl>  <nl> ASSERT ( ql -> qh_nelems == 0 );
static int __floppy_read_block_0 ( struct block_device * bdev ) <nl> bio . bi_size = size ; <nl> bio . bi_bdev = bdev ; <nl> bio . bi_sector = 0 ; <nl> + bio . bi_flags = BIO_QUIET ; <nl> init_completion (& complete ); <nl> bio . bi_private = & complete ; <nl> bio . bi_end_io = floppy_rb0_complete ;
brcmf_notify_sched_scan_results ( struct brcmf_if * ifp , <nl> } <nl>  <nl> set_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status ); <nl> + cfg -> escan_info . run = brcmf_run_escan ; <nl> err = brcmf_do_escan ( cfg , wiphy , ifp , request ); <nl> if ( err ) { <nl> clear_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status );
static int e1000_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> goto err_hw_init ; <nl>  <nl> if (( adapter -> flags & FLAG_IS_ICH ) && <nl> - ( adapter -> flags & FLAG_READ_ONLY_NVM )) <nl> + ( adapter -> flags & FLAG_READ_ONLY_NVM ) && <nl> + ( hw -> mac . type < e1000_pch_spt )) <nl> e1000e_write_protect_nvm_ich8lan (& adapter -> hw ); <nl>  <nl> hw -> mac . ops . get_bus_info (& adapter -> hw );
int cap_bprm_set_creds ( struct linux_binprm * bprm ) <nl> } <nl> skip : <nl>  <nl> + /* if we have fs caps , clear dangerous personality flags */ <nl> + if (! cap_issubset ( new -> cap_permitted , old -> cap_permitted )) <nl> + bprm -> per_clear |= PER_CLEAR_ON_SETID ; <nl> + <nl> + <nl> /* Don ' t let someone trace a set [ ug ] id / setpcap binary with the revised <nl> * credentials unless they have the appropriate permit <nl> */
void __init setup_arch ( char ** cmdline_p ) <nl> init_mm . brk = ( unsigned long ) & _end ; <nl>  <nl> * cmdline_p = m68k_command_line ; <nl> - memcpy ( saved_command_line , * cmdline_p , CL_SIZE ); <nl> + memcpy ( boot_command_line , * cmdline_p , CL_SIZE ); <nl>  <nl> /* Parse the command line for arch - specific options . <nl> * For the m68k , this is currently only " debug = xxx " to enable printing
static int __exit twl4030_usb_remove ( struct platform_device * pdev ) <nl> /* disable complete OTG block */ <nl> twl4030_usb_clear_bits ( twl , POWER_CTRL , POWER_CTRL_OTG_ENAB ); <nl>  <nl> - twl4030_phy_power ( twl , 0 ); <nl> + if (! twl -> asleep ) <nl> + twl4030_phy_power ( twl , 0 ); <nl> regulator_put ( twl -> usb1v5 ); <nl> regulator_put ( twl -> usb1v8 ); <nl> regulator_put ( twl -> usb3v1 );
static ssize_t wait_for_direct_io ( enum ORANGEFS_io_type type , struct inode * inod <nl> */ <nl> if ( ret == - EAGAIN && op_state_purged ( new_op )) { <nl> orangefs_bufmap_put ( bufmap , buffer_index ); <nl> + buffer_index = - 1 ; <nl> gossip_debug ( GOSSIP_FILE_DEBUG , <nl> "% s : going to repopulate_shared_memory .\ n ", <nl> __func__ );
void * ion_heap_map_kernel ( struct ion_heap * heap , <nl> struct page ** tmp = pages ; <nl>  <nl> if (! pages ) <nl> - return NULL ; <nl> + return ERR_PTR (- ENOMEM ); <nl>  <nl> if ( buffer -> flags & ION_FLAG_CACHED ) <nl> pgprot = PAGE_KERNEL ;
static int ext4_file_open ( struct inode * inode , struct file * filp ) <nl> path . dentry = mnt -> mnt_root ; <nl> cp = d_path (& path , buf , sizeof ( buf )); <nl> if (! IS_ERR ( cp )) { <nl> - memcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> - sizeof ( sbi -> s_es -> s_last_mounted )); <nl> + strlcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> + sizeof ( sbi -> s_es -> s_last_mounted )); <nl> ext4_mark_super_dirty ( sb ); <nl> } <nl> }
void __kvm_migrate_pit_timer ( struct kvm_vcpu * vcpu ) <nl> return ; <nl>  <nl> timer = & pit -> pit_state . timer ; <nl> + mutex_lock (& pit -> pit_state . lock ); <nl> if ( hrtimer_cancel ( timer )) <nl> hrtimer_start_expires ( timer , HRTIMER_MODE_ABS ); <nl> + mutex_unlock (& pit -> pit_state . lock ); <nl> } <nl>  <nl> static void destroy_pit_timer ( struct kvm_pit * pit )
static int create_queue_cpsch ( struct device_queue_manager * dqm , struct queue * q , <nl> return retval ; <nl> } <nl>  <nl> - int fence_wait_timeout ( unsigned int * fence_addr , unsigned int fence_value , <nl> - unsigned long timeout ) <nl> + static int fence_wait_timeout ( unsigned int * fence_addr , <nl> + unsigned int fence_value , <nl> + unsigned long timeout ) <nl> { <nl> BUG_ON (! fence_addr ); <nl> timeout += jiffies ;
static int of_iommu_xlate ( struct device * dev , <nl> * a proper probe - ordering dependency mechanism in future . <nl> */ <nl> if (! ops ) <nl> - return - EPROBE_DEFER ; <nl> + return driver_deferred_probe_check_state ( dev ); <nl>  <nl> return ops -> of_xlate ( dev , iommu_spec ); <nl> }
void compact_pgdat ( pg_data_t * pgdat , int order ) <nl> . sync = false , <nl> }; <nl>  <nl> + if (! order ) <nl> + return ; <nl> + <nl> __compact_pgdat ( pgdat , & cc ); <nl> } <nl> 
get_futex_key ( u32 __user * uaddr , int fshared , union futex_key * key , int rw ) <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> + page = compound_head ( page ); <nl> lock_page ( page ); <nl> if (! page -> mapping ) { <nl> unlock_page ( page );
int cx23885_tuner_callback ( void * priv , int component , int command , int arg ) <nl> struct cx23885_dev * dev = port -> dev ; <nl> u32 bitmask = 0 ; <nl>  <nl> - if ( command == XC2028_RESET_CLK ) <nl> + if (( command == XC2028_RESET_CLK ) || ( command == XC2028_I2C_FLUSH )) <nl> return 0 ; <nl>  <nl> if ( command != 0 ) {
static void mmu_set_spte ( struct kvm_vcpu * vcpu , u64 * sptep , <nl>  <nl> child = page_header ( pte & PT64_BASE_ADDR_MASK ); <nl> mmu_page_remove_parent_pte ( child , sptep ); <nl> + __set_spte ( sptep , shadow_trap_nonpresent_pte ); <nl> + kvm_flush_remote_tlbs ( vcpu -> kvm ); <nl> } else if ( pfn != spte_to_pfn (* sptep )) { <nl> pgprintk (" hfn old % lx new % lx \ n ", <nl> spte_to_pfn (* sptep ), pfn );
static void imon_incoming_packet ( struct imon_context * ictx , <nl> if ( press_type == 0 ) <nl> rc_keyup ( ictx -> rdev ); <nl> else { <nl> - if ( ictx -> rc_type == RC_BIT_RC6_MCE ) <nl> + if ( ictx -> rc_type == RC_BIT_RC6_MCE || <nl> + ictx -> rc_type == RC_BIT_OTHER ) <nl> rc_keydown ( ictx -> rdev , <nl> ictx -> rc_type == RC_BIT_RC6_MCE ? RC_TYPE_RC6_MCE : RC_TYPE_OTHER , <nl> ictx -> rc_scancode , ictx -> rc_toggle );
static const struct regmap_config rt5677_regmap = { <nl> static const struct i2c_device_id rt5677_i2c_id [] = { <nl> { " rt5677 ", RT5677 }, <nl> { " rt5676 ", RT5676 }, <nl> + { " RT5677CE : 00 ", RT5677 }, <nl> { } <nl> }; <nl> MODULE_DEVICE_TABLE ( i2c , rt5677_i2c_id );
static void wl3501_free_tx_buffer ( struct wl3501_card * this , u16 ptr ) <nl>  <nl> static int wl3501_esbq_req_test ( struct wl3501_card * this ) <nl> { <nl> - u8 tmp ; <nl> + u8 tmp = 0 ; <nl>  <nl> wl3501_get_from_wla ( this , this -> esbq_req_head + 3 , & tmp , sizeof ( tmp )); <nl> return tmp & 0x80 ;
static void slic_card_cleanup ( struct sliccard * card ) <nl> { <nl> if ( card -> loadtimerset ) { <nl> card -> loadtimerset = 0 ; <nl> - del_timer (& card -> loadtimer ); <nl> + del_timer_sync (& card -> loadtimer ); <nl> } <nl>  <nl> slic_debug_card_destroy ( card );
static void sw_perf_event_destroy ( struct perf_event * event ) <nl>  <nl> static int perf_swevent_init ( struct perf_event * event ) <nl> { <nl> - int event_id = event -> attr . config ; <nl> + u64 event_id = event -> attr . config ; <nl>  <nl> if ( event -> attr . type != PERF_TYPE_SOFTWARE ) <nl> return - ENOENT ;
void bnx2x_disable_sriov ( struct bnx2x * bp ) <nl> static int bnx2x_vf_ndo_sanity ( struct bnx2x * bp , int vfidx , <nl> struct bnx2x_virtf * vf ) <nl> { <nl> + if ( bp -> state != BNX2X_STATE_OPEN ) { <nl> + BNX2X_ERR (" vf ndo called though PF is down \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if (! IS_SRIOV ( bp )) { <nl> BNX2X_ERR (" vf ndo called though sriov is disabled \ n "); <nl> return - EINVAL ;
static int __init ptp_kvm_init ( void ) <nl> { <nl> long ret ; <nl>  <nl> + if (! kvm_para_available ()) <nl> + return - ENODEV ; <nl> + <nl> clock_pair_gpa = slow_virt_to_phys (& clock_pair ); <nl> hv_clock = pvclock_pvti_cpu0_va (); <nl> 
void gfs2_set_iop ( struct inode * inode ) <nl> inode -> i_op = & gfs2_symlink_iops ; <nl> } else { <nl> inode -> i_op = & gfs2_file_iops ; <nl> + init_special_inode ( inode , inode -> i_mode , inode -> i_rdev ); <nl> } <nl>  <nl> unlock_new_inode ( inode );
static int rds_ib_laddr_check ( __be32 addr ) <nl> ret = rdma_bind_addr ( cm_id , ( struct sockaddr *)& sin ); <nl> /* due to this , we will claim to support iWARP devices unless we <nl> check node_type . */ <nl> - if ( ret || cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> + if ( ret || ! cm_id -> device || <nl> + cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> ret = - EADDRNOTAVAIL ; <nl>  <nl> rdsdebug (" addr % pI4 ret % d node type % d \ n ",
unsigned long __init lmb_alloc_base ( unsigned long size , unsigned long align , <nl>  <nl> alloc = __lmb_alloc_base ( size , align , max_addr ); <nl>  <nl> - if ( alloc < 0 ) <nl> + if ( alloc == 0 ) <nl> panic (" ERROR : Failed to allocate 0x % lx bytes below 0x % lx .\ n ", <nl> size , max_addr ); <nl> 
int __cfs_fail_timeout_set ( __u32 id , __u32 value , int ms , int set ) <nl> int ret ; <nl>  <nl> ret = __cfs_fail_check_set ( id , value , set ); <nl> - if ( ret ) { <nl> + if ( ret && likely ( ms > 0 )) { <nl> CERROR (" cfs_fail_timeout id % x sleeping for % dms \ n ", <nl> id , ms ); <nl> set_current_state ( TASK_UNINTERRUPTIBLE );
static int i965_do_reset ( struct drm_device * dev ) <nl> { <nl> int ret ; <nl>  <nl> + /* FIXME : i965g / gm need a display save / restore for gpu reset . */ <nl> + return - ENODEV ; <nl> + <nl> /* <nl> * Set the domains we want to reset ( GRDOM / bits 2 and 3 ) as <nl> * well as the reset bit ( GR / bit 0 ). Setting the GR bit
static int uas_find_endpoints ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < n_endpoints ; i ++) { <nl> unsigned char * extra = endpoint [ i ]. extra ; <nl> int len = endpoint [ i ]. extralen ; <nl> - while ( len > 1 ) { <nl> + while ( len >= 3 ) { <nl> if ( extra [ 1 ] == USB_DT_PIPE_USAGE ) { <nl> unsigned pipe_id = extra [ 2 ]; <nl> if ( pipe_id > 0 && pipe_id < 5 )
vc4_get_bcl ( struct drm_device * dev , struct vc4_exec_info * exec ) <nl> args -> shader_rec_count ); <nl> struct vc4_bo * bo ; <nl>  <nl> - if ( uniforms_offset < shader_rec_offset || <nl> + if ( shader_rec_offset < args -> bin_cl_size || <nl> + uniforms_offset < shader_rec_offset || <nl> exec_size < uniforms_offset || <nl> args -> shader_rec_count >= ( UINT_MAX / <nl> sizeof ( struct vc4_shader_state )) ||
static unsigned int ata_eh_speed_down ( struct ata_device * dev , <nl> */ <nl> static inline int ata_eh_worth_retry ( struct ata_queued_cmd * qc ) <nl> { <nl> - if ( qc -> flags & AC_ERR_MEDIA ) <nl> + if ( qc -> err_mask & AC_ERR_MEDIA ) <nl> return 0 ; /* don ' t retry media errors */ <nl> if ( qc -> flags & ATA_QCFLAG_IO ) <nl> return 1 ; /* otherwise retry anything from fs stack */
struct gb_interface * gb_interface_create ( struct greybus_host_device * hd , <nl>  <nl> free_intf : <nl> put_device (& intf -> dev ); <nl> - kfree ( intf ); <nl> put_module : <nl> put_device (& module -> dev ); <nl> return NULL ;
static int bio_readpage_error ( struct bio * failed_bio , u64 phy_offset , <nl> ret = tree -> ops -> submit_bio_hook ( inode , read_mode , bio , <nl> failrec -> this_mirror , <nl> failrec -> bio_flags , 0 ); <nl> + if ( ret ) { <nl> + free_io_failure ( inode , failrec , 0 ); <nl> + bio_put ( bio ); <nl> + } <nl> + <nl> return ret ; <nl> } <nl> 
int fix_alignment ( struct pt_regs * regs ) <nl>  <nl> type = op . type & INSTR_TYPE_MASK ; <nl> if (! OP_IS_LOAD_STORE ( type )) { <nl> - if ( type != CACHEOP + DCBZ ) <nl> + if ( op . type != CACHEOP + DCBZ ) <nl> return - EINVAL ; <nl> PPC_WARN_ALIGNMENT ( dcbz , regs ); <nl> r = emulate_dcbz ( op . ea , regs );
static int ieee80211_fragment ( struct ieee80211_tx_data * tx , <nl> } <nl>  <nl> /* adjust first fragment ' s length */ <nl> - skb -> len = hdrlen + per_fragm ; <nl> + skb_trim ( skb , hdrlen + per_fragm ); <nl> return 0 ; <nl> } <nl> 
i915_gem_detect_bit_6_swizzle ( struct drm_device * dev ) <nl> } <nl> } <nl>  <nl> + /* FIXME : check with memory config on IGDNG */ <nl> + if ( IS_IGDNG ( dev )) { <nl> + DRM_ERROR (" disable tiling on IGDNG ...\ n "); <nl> + swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + } <nl> + <nl> dev_priv -> mm . bit_6_swizzle_x = swizzle_x ; <nl> dev_priv -> mm . bit_6_swizzle_y = swizzle_y ; <nl> }
static int ipoib_mcast_join_complete ( int status , <nl> } <nl>  <nl> if ( mcast -> logcount ++ < 20 ) { <nl> - if ( status == - ETIMEDOUT ) { <nl> + if ( status == - ETIMEDOUT || status == - EAGAIN ) { <nl> ipoib_dbg_mcast ( priv , " multicast join failed for % pI6 , status % d \ n ", <nl> mcast -> mcmember . mgid . raw , status ); <nl> } else {
static void qlcnic_get_ethtool_stats ( struct net_device * dev , <nl> memset ( data , 0 , stats -> n_stats * sizeof ( u64 )); <nl>  <nl> for ( ring = 0 , index = 0 ; ring < adapter -> drv_tx_rings ; ring ++) { <nl> - if ( test_bit ( __QLCNIC_DEV_UP , & adapter -> state )) { <nl> + if ( adapter -> is_up == QLCNIC_ADAPTER_UP_MAGIC ) { <nl> tx_ring = & adapter -> tx_ring [ ring ]; <nl> data = qlcnic_fill_tx_queue_stats ( data , tx_ring ); <nl> qlcnic_update_stats ( adapter );
static int handle_emulation_failure ( struct kvm_vcpu * vcpu ) <nl>  <nl> ++ vcpu -> stat . insn_emulation_fail ; <nl> trace_kvm_emulate_insn_failed ( vcpu ); <nl> - if (! is_guest_mode ( vcpu )) { <nl> + if (! is_guest_mode ( vcpu ) && kvm_x86_ops -> get_cpl ( vcpu ) == 0 ) { <nl> vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> vcpu -> run -> internal . ndata = 0 ;
struct clk * icst_clk_register ( struct device * dev , <nl>  <nl> pclone = kmemdup ( desc -> params , sizeof (* pclone ), GFP_KERNEL ); <nl> if (! pclone ) { <nl> + kfree ( icst ); <nl> pr_err (" could not clone ICST params \ n "); <nl> return ERR_PTR (- ENOMEM ); <nl> }
static int meson_mmc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> dev_err (& pdev -> dev , " failed to get interrupt resource .\ n "); <nl> ret = - EINVAL ; <nl> goto free_host ;
static int moxart_gpio_probe ( struct platform_device * pdev ) <nl> gc -> parent = dev ; <nl> gc -> owner = THIS_MODULE ; <nl>  <nl> - ret = gpiochip_add_data ( gc , NULL ); <nl> + ret = devm_gpiochip_add_data ( dev , gc , NULL ); <nl> if ( ret ) { <nl> dev_err ( dev , "% s : gpiochip_add failed \ n ", <nl> dev -> of_node -> full_name );
struct io_reg CX700_ModeXregs [] = { { VIASR , SR10 , 0xFF , 0x01 }, <nl> { VIACR , CR96 , 0xFF , 0x00 }, <nl> { VIACR , CR97 , 0xFF , 0x00 }, <nl> { VIACR , CR99 , 0xFF , 0x00 }, <nl> -{ VIACR , CR9B , 0xFF , 0x00 }, <nl> -{ VIACR , CRD2 , 0xFF , 0xFF } /* TMDS / LVDS control register . */ <nl> +{ VIACR , CR9B , 0xFF , 0x00 } <nl> }; <nl>  <nl> /* Video Mode Table */
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
static void atmel_spi_next_xfer ( struct spi_master * master , <nl> xfer , xfer -> len , xfer -> tx_buf , xfer -> tx_dma , <nl> xfer -> rx_buf , xfer -> rx_dma , spi_readl ( as , IMR )); <nl>  <nl> - spi_writel ( as , TCR , len ); <nl> spi_writel ( as , RCR , len ); <nl> + spi_writel ( as , TCR , len ); <nl> spi_writel ( as , PTCR , SPI_BIT ( TXTEN ) | SPI_BIT ( RXTEN )); <nl> } <nl> 
static void atiixp_set_dmamode ( struct ata_port * ap , struct ata_device * adev ) <nl> * We must now look at the PIO mode situation . We may need to <nl> * adjust the PIO mode to keep the timings acceptable <nl> */ <nl> - if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> - wanted_pio = 4 ; <nl> + if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> + wanted_pio = 4 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_1 ) <nl> wanted_pio = 3 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_0 )
void hostif_data_indication ( struct ks_wlan_private * priv ) <nl> { <nl> unsigned int rx_ind_size ; /* indicate data size */ <nl> struct sk_buff * skb ; <nl> - unsigned short auth_type ; <nl> + u16 auth_type ; <nl> unsigned char temp [ 256 ]; <nl> struct ether_hdr * eth_hdr ; <nl> unsigned short eth_proto ;
unsigned long sctp_transport_timeout ( struct sctp_transport * trans ) <nl> trans -> state != SCTP_PF ) <nl> timeout += trans -> hbinterval ; <nl>  <nl> - return timeout ; <nl> + return max_t ( unsigned long , timeout , HZ / 5 ); <nl> } <nl>  <nl> /* Reset transport variables to their initial values */
int vbd_create ( blkif_t * blkif , blkif_vdev_t handle , unsigned major , <nl>  <nl> vbd -> pdevice = MKDEV ( major , minor ); <nl>  <nl> - bdev = open_by_devnum ( vbd -> pdevice , <nl> - vbd -> readonly ? FMODE_READ : FMODE_WRITE ); <nl> + bdev = blkdev_get_by_dev ( vbd -> pdevice , vbd -> readonly ? <nl> + FMODE_READ : FMODE_WRITE , NULL ); <nl>  <nl> if ( IS_ERR ( bdev )) { <nl> DPRINTK (" vbd_creat : device % 08x could not be opened .\ n ",
int add_mtd_blktrans_dev ( struct mtd_blktrans_dev * new ) <nl> new -> rq -> queuedata = new ; <nl> blk_queue_logical_block_size ( new -> rq , tr -> blksize ); <nl>  <nl> - if ( tr -> discard ) <nl> - queue_flag_set_unlocked ( QUEUE_FLAG_DISCARD , <nl> - new -> rq ); <nl> + if ( tr -> discard ) { <nl> + queue_flag_set_unlocked ( QUEUE_FLAG_DISCARD , new -> rq ); <nl> + new -> rq -> limits . max_discard_sectors = UINT_MAX ; <nl> + } <nl>  <nl> gd -> queue = new -> rq ; <nl> 
static inline void ata_tf_init ( struct ata_device * dev , struct ata_taskfile * tf ) <nl>  <nl> static inline void ata_qc_reinit ( struct ata_queued_cmd * qc ) <nl> { <nl> + qc -> dma_dir = DMA_NONE ; <nl> qc -> __sg = NULL ; <nl> qc -> flags = 0 ; <nl> qc -> cursect = qc -> cursg = qc -> cursg_ofs = 0 ;
bool i40e_dcb_need_reconfig ( struct i40e_pf * pf , <nl> /* Check if APP Table has changed */ <nl> if ( memcmp (& new_cfg -> app , <nl> & old_cfg -> app , <nl> - sizeof ( new_cfg -> app ))) <nl> + sizeof ( new_cfg -> app ))) { <nl> need_reconfig = true ; <nl> dev_info (& pf -> pdev -> dev , " APP Table change detected .\ n "); <nl> + } <nl>  <nl> return need_reconfig ; <nl> }
int ptrace_setxregs ( struct task_struct * child , void __user * uregs ) <nl> elf_xtregs_t * xtregs = uregs ; <nl> int ret = 0 ; <nl>  <nl> + if (! access_ok ( VERIFY_READ , uregs , sizeof ( elf_xtregs_t ))) <nl> + return - EFAULT ; <nl> + <nl> # if XTENSA_HAVE_COPROCESSORS <nl> /* Flush all coprocessors before we overwrite them . */ <nl> coprocessor_flush_all ( ti );
static int ca8210_probe ( struct spi_device * spi_device ) <nl> goto error ; <nl> } <nl>  <nl> + priv -> spi -> dev . platform_data = pdata ; <nl> ret = ca8210_get_platform_data ( priv -> spi , pdata ); <nl> if ( ret ) { <nl> dev_crit (& spi_device -> dev , " ca8210_get_platform_data failed \ n "); <nl> goto error ; <nl> } <nl> - priv -> spi -> dev . platform_data = pdata ; <nl>  <nl> ret = ca8210_dev_com_init ( priv ); <nl> if ( ret ) {
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> * and we need some headroom for passing the frame to monitor <nl> * interfaces , but never both at the same time . <nl> */ <nl> - local -> tx_headroom = max ( local -> hw . extra_tx_headroom , <nl> - sizeof ( struct ieee80211_tx_status_rtap_hdr )); <nl> + local -> tx_headroom = max_t ( unsigned int , local -> hw . extra_tx_headroom , <nl> + sizeof ( struct ieee80211_tx_status_rtap_hdr )); <nl>  <nl> debugfs_hw_add ( local ); <nl> 
static inline int gro_cells_init ( struct gro_cells * gcells , struct net_device * de <nl> int i ; <nl>  <nl> gcells -> gro_cells_mask = roundup_pow_of_two ( netif_get_num_default_rss_queues ()) - 1 ; <nl> - gcells -> cells = kcalloc ( sizeof ( struct gro_cell ), <nl> - gcells -> gro_cells_mask + 1 , <nl> + gcells -> cells = kcalloc ( gcells -> gro_cells_mask + 1 , <nl> + sizeof ( struct gro_cell ), <nl> GFP_KERNEL ); <nl> if (! gcells -> cells ) <nl> return - ENOMEM ;
static inline u8 rc5_data ( struct rc_map_table * key ) <nl> return key -> scancode & 0xff ; <nl> } <nl>  <nl> - static inline u8 rc5_scan ( struct rc_map_table * key ) <nl> + static inline u16 rc5_scan ( struct rc_map_table * key ) <nl> { <nl> return key -> scancode & 0xffff ; <nl> }
static int audit_log_single_execve_arg ( struct audit_context * context , <nl> * so we can be sure nothing was lost . <nl> */ <nl> if (( i == 0 ) && ( too_long )) <nl> - audit_log_format (* ab , " a % d_len =% ld ", arg_num , <nl> + audit_log_format (* ab , " a % d_len =% zu ", arg_num , <nl> has_cntl ? 2 * len : len ); <nl>  <nl> /*
device_create_groups_vargs ( struct class * class , struct device * parent , <nl> goto error ; <nl> } <nl>  <nl> + device_initialize ( dev ); <nl> dev -> devt = devt ; <nl> dev -> class = class ; <nl> dev -> parent = parent ; <nl> device_create_groups_vargs ( struct class * class , struct device * parent , <nl> if ( retval ) <nl> goto error ; <nl>  <nl> - retval = device_register ( dev ); <nl> + retval = device_add ( dev ); <nl> if ( retval ) <nl> goto error ; <nl> 
static int do_video_set_spu_palette ( unsigned int fd , unsigned int cmd , <nl>  <nl> err = get_user ( palp , & up -> palette ); <nl> err |= get_user ( length , & up -> length ); <nl> + if ( err ) <nl> + return - EFAULT ; <nl>  <nl> up_native = compat_alloc_user_space ( sizeof ( struct video_spu_palette )); <nl> err = put_user ( compat_ptr ( palp ), & up_native -> palette );
copy_from_user ( void * to , const void __user * from , unsigned long n ) <nl> __kernel_size_t __copy_size = ( __kernel_size_t ) n ; <nl>  <nl> if ( __copy_size && __access_ok ( __copy_from , __copy_size )) <nl> - return __copy_user ( to , from , __copy_size ); <nl> + __copy_size = __copy_user ( to , from , __copy_size ); <nl> + <nl> + if ( unlikely ( __copy_size )) <nl> + memset ( to + ( n - __copy_size ), 0 , __copy_size ); <nl>  <nl> return __copy_size ; <nl> }
static int perf_sched__read_events ( struct perf_sched * sched ) <nl> struct perf_data_file file = { <nl> . path = input_name , <nl> . mode = PERF_DATA_MODE_READ , <nl> + . force = sched -> force , <nl> }; <nl> int rc = - 1 ; <nl> 
static struct page * get_partial ( struct kmem_cache * s , gfp_t flags , int node ) <nl> int searchnode = ( node == NUMA_NO_NODE ) ? numa_node_id () : node ; <nl>  <nl> page = get_partial_node ( get_node ( s , searchnode )); <nl> - if ( page || node != - 1 ) <nl> + if ( page || node != NUMA_NO_NODE ) <nl> return page ; <nl>  <nl> return get_any_partial ( s , flags );
static int io_files_update_with_index_alloc ( struct io_kiocb * req , <nl> struct file * file ; <nl> int ret , fd ; <nl>  <nl> + if (! req -> ctx -> file_data ) <nl> + return - ENXIO ; <nl> + <nl> for ( done = 0 ; done < req -> rsrc_update . nr_args ; done ++) { <nl> if ( copy_from_user (& fd , & fds [ done ], sizeof ( fd ))) { <nl> ret = - EFAULT ;
static int stmmac_pci_probe ( struct pci_dev * pdev , <nl> priv = stmmac_dvr_probe (&( pdev -> dev ), & plat_dat , addr ); <nl> if (! priv ) { <nl> pr_err ("% s : main driver probe failed ", __func__ ); <nl> + ret = - ENODEV ; <nl> goto err_out ; <nl> } <nl> priv -> dev -> irq = pdev -> irq ;
int exynos_eint_wkup_init ( struct samsung_pinctrl_drv_data * d ) <nl> if ( match ) { <nl> irq_chip = kmemdup ( match -> data , <nl> sizeof (* irq_chip ), GFP_KERNEL ); <nl> + if (! irq_chip ) <nl> + return - ENOMEM ; <nl> wkup_np = np ; <nl> break ; <nl> }
static const struct snd_pci_quirk alc662_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x1025 , 0x038b , " Acer Aspire 8943G ", ALC662_FIXUP_ASPIRE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05d8 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05db , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> + SND_PCI_QUIRK ( 0x1028 , 0x0626 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x103c , 0x1632 , " HP RP5800 ", ALC662_FIXUP_HP_RP5800 ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1477 , " ASUS N56VZ ", ALC662_FIXUP_BASS_CHMAP ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1bf3 , " ASUS N76VZ ", ALC662_FIXUP_BASS_CHMAP ),
static int vfio_pci_mmap ( void * device_data , struct vm_area_struct * vma ) <nl> return ret ; <nl>  <nl> vdev -> barmap [ index ] = pci_iomap ( pdev , index , 0 ); <nl> + if (! vdev -> barmap [ index ]) { <nl> + pci_release_selected_regions ( pdev , 1 << index ); <nl> + return - ENOMEM ; <nl> + } <nl> } <nl>  <nl> vma -> vm_private_data = vdev ;
static inline int sctp_frag_point ( const struct sctp_association * asoc , int pmtu ) <nl> if ( asoc -> user_frag ) <nl> frag = min_t ( int , frag , asoc -> user_frag ); <nl>  <nl> - frag = min_t ( int , frag , SCTP_MAX_CHUNK_LEN ); <nl> + frag = WORD_TRUNC ( min_t ( int , frag , SCTP_MAX_CHUNK_LEN )); <nl>  <nl> return frag ; <nl> }
nouveau_framebuffer_init ( struct drm_device * dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( nvbo -> tile_flags & NOUVEAU_GEM_TILE_NONCONTIG ) { <nl> + NV_ERROR ( drm , " framebuffer requires contiguous bo \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( nv_device ( drm -> device )-> chipset == 0x50 ) <nl> nv_fb -> r_format |= ( tile_flags << 8 ); <nl> 
struct btrfs_root * open_ctree ( struct super_block * sb , <nl> kfree ( tree_root ); <nl> bdi_destroy (& fs_info -> bdi ); <nl> kfree ( fs_info ); <nl> + kfree ( chunk_root ); <nl> + kfree ( dev_root ); <nl> return ERR_PTR ( err ); <nl> } <nl> 
static int sctp_wait_for_sndbuf ( struct sctp_association * asoc , long * timeo_p , <nl> */ <nl> release_sock ( sk ); <nl> current_timeo = schedule_timeout ( current_timeo ); <nl> - BUG_ON ( sk != asoc -> base . sk ); <nl> + if ( sk != asoc -> base . sk ) <nl> + goto do_error ; <nl> lock_sock ( sk ); <nl>  <nl> * timeo_p = current_timeo ;
static int atl2_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl>  <nl> err = - EIO ; <nl>  <nl> - netdev -> hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX ; <nl> + netdev -> hw_features = NETIF_F_HW_VLAN_CTAG_RX ; <nl> netdev -> features |= ( NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX ); <nl>  <nl> /* Init PHY as early as possible due to power saving issue */
int ext4_insert_range ( struct inode * inode , loff_t offset , loff_t len ) <nl> up_write (& EXT4_I ( inode )-> i_data_sem ); <nl> goto out_stop ; <nl> } <nl> + } else { <nl> + ext4_ext_drop_refs ( path ); <nl> + kfree ( path ); <nl> } <nl>  <nl> ret = ext4_es_remove_extent ( inode , offset_lblk ,
do { \ <nl> # define this_cpu_generic_read ( pcp ) \ <nl> ({ \ <nl> typeof ( pcp ) __ret ; \ <nl> - preempt_disable (); \ <nl> + preempt_disable_notrace (); \ <nl> __ret = raw_cpu_generic_read ( pcp ); \ <nl> - preempt_enable (); \ <nl> + preempt_enable_notrace (); \ <nl> __ret ; \ <nl> }) <nl> 
smb_proc_setattr_unix ( struct dentry * d , struct iattr * attr , <nl> LSET ( data , 32 , SMB_TIME_NO_CHANGE ); <nl> LSET ( data , 40 , SMB_UID_NO_CHANGE ); <nl> LSET ( data , 48 , SMB_GID_NO_CHANGE ); <nl> - LSET ( data , 56 , smb_filetype_from_mode ( attr -> ia_mode )); <nl> + DSET ( data , 56 , smb_filetype_from_mode ( attr -> ia_mode )); <nl> LSET ( data , 60 , major ); <nl> LSET ( data , 68 , minor ); <nl> LSET ( data , 76 , 0 );
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int io_rw_init_file ( struct io_kiocb * req , fmode_t mode ) <nl> if (!( kiocb -> ki_flags & IOCB_DIRECT ) || ! file -> f_op -> iopoll ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + kiocb -> private = NULL ; <nl> kiocb -> ki_flags |= IOCB_HIPRI | IOCB_ALLOC_CACHE ; <nl> kiocb -> ki_complete = io_complete_rw_iopoll ; <nl> req -> iopoll_completed = 0 ;
int v4l2_async_notifier_register ( struct v4l2_device * v4l2_dev , <nl> struct v4l2_async_subdev * asd ; <nl> int i ; <nl>  <nl> - if (! notifier -> num_subdevs || notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> + if (! v4l2_dev || ! notifier -> num_subdevs || <nl> + notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> return - EINVAL ; <nl>  <nl> notifier -> v4l2_dev = v4l2_dev ;
static int rave_sp_probe ( struct serdev_device * serdev ) <nl> return ret ; <nl>  <nl> serdev_device_set_baudrate ( serdev , baud ); <nl> + serdev_device_set_flow_control ( serdev , false ); <nl> + <nl> + ret = serdev_device_set_parity ( serdev , SERDEV_PARITY_NONE ); <nl> + if ( ret ) { <nl> + dev_err ( dev , " Failed to set parity \ n "); <nl> + return ret ; <nl> + } <nl>  <nl> ret = rave_sp_get_status ( sp ); <nl> if ( ret ) {
int tcp_rcv_state_process ( struct sock * sk , struct sk_buff * skb , <nl> goto discard ; <nl>  <nl> if ( th -> syn ) { <nl> + if ( th -> fin ) <nl> + goto discard ; <nl> if ( icsk -> icsk_af_ops -> conn_request ( sk , skb ) < 0 ) <nl> return 1 ; <nl> 
static int eb_lookup_vmas ( struct i915_execbuffer * eb ) <nl>  <nl> err = radix_tree_insert ( handles_vma , handle , vma ); <nl> if ( unlikely ( err )) { <nl> - kfree ( lut ); <nl> + kmem_cache_free ( eb -> i915 -> luts , lut ); <nl> goto err_obj ; <nl> } <nl> 
static int __devinit ad7879_probe ( struct spi_device * spi ) <nl> kfree ( ts ); <nl> } <nl>  <nl> - return 0 ; <nl> + return error ; <nl> } <nl>  <nl> static int __devexit ad7879_remove ( struct spi_device * spi ) <nl> static int __devinit ad7879_probe ( struct i2c_client * client , <nl> kfree ( ts ); <nl> } <nl>  <nl> - return 0 ; <nl> + return error ; <nl> } <nl>  <nl> static int __devexit ad7879_remove ( struct i2c_client * client )
static void handle_keypress ( int c ) <nl> switch ( c ) { <nl> case ' d ': <nl> prompt_integer (& delay_secs , " Enter display delay "); <nl> + if ( delay_secs < 1 ) <nl> + delay_secs = 1 ; <nl> break ; <nl> case ' e ': <nl> prompt_integer (& print_entries , " Enter display entries ( lines )");
int vcc_recvmsg ( struct kiocb * iocb , struct socket * sock , struct msghdr * msg , <nl> struct sk_buff * skb ; <nl> int copied , error = - EINVAL ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> if ( sock -> state != SS_CONNECTED ) <nl> return - ENOTCONN ; <nl> 
static int sco_sock_bind ( struct socket * sock , struct sockaddr * addr , <nl> if (! addr || addr -> sa_family != AF_BLUETOOTH ) <nl> return - EINVAL ; <nl>  <nl> + if ( addr_len < sizeof ( struct sockaddr_sco )) <nl> + return - EINVAL ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state != BT_OPEN ) {
__hwmon_device_register ( struct device * dev , const char * name , void * drvdata , <nl> if ( err ) <nl> goto free_hwmon ; <nl>  <nl> - if ( chip && chip -> ops -> read && <nl> + if ( dev && chip && chip -> ops -> read && <nl> chip -> info [ 0 ]-> type == hwmon_chip && <nl> ( chip -> info [ 0 ]-> config [ 0 ] & HWMON_C_REGISTER_TZ )) { <nl> const struct hwmon_channel_info ** info = chip -> info ;
struct reset_control * of_reset_control_get ( struct device_node * node , <nl> { <nl> int index = 0 ; <nl>  <nl> - if ( id ) <nl> + if ( id ) { <nl> index = of_property_match_string ( node , <nl> " reset - names ", id ); <nl> + if ( index < 0 ) <nl> + return ERR_PTR (- ENOENT ); <nl> + } <nl> return of_reset_control_get_by_index ( node , index ); <nl> } <nl> EXPORT_SYMBOL_GPL ( of_reset_control_get );
getname_kernel ( const char * filename ) <nl> if ( len <= EMBEDDED_NAME_MAX ) { <nl> result -> name = ( char *) result -> iname ; <nl> } else if ( len <= PATH_MAX ) { <nl> + const size_t size = offsetof ( struct filename , iname [ 1 ]); <nl> struct filename * tmp ; <nl>  <nl> - tmp = kmalloc ( sizeof (* tmp ), GFP_KERNEL ); <nl> + tmp = kmalloc ( size , GFP_KERNEL ); <nl> if ( unlikely (! tmp )) { <nl> __putname ( result ); <nl> return ERR_PTR (- ENOMEM );
i2c_dw_xfer ( struct i2c_adapter * adap , struct i2c_msg msgs [], int num ) <nl> i2c_dw_xfer_init ( dev ); <nl>  <nl> /* wait for tx to complete */ <nl> - if (! wait_for_completion_timeout (& dev -> cmd_complete , HZ )) { <nl> + if (! wait_for_completion_timeout (& dev -> cmd_complete , adap -> timeout )) { <nl> dev_err ( dev -> dev , " controller timed out \ n "); <nl> /* i2c_dw_init implicitly disables the adapter */ <nl> i2c_dw_init ( dev );
static int i40e_setup_macvlans ( struct i40e_vsi * vsi , u16 macvlan_cnt , u16 qcnt , <nl> ch -> num_queue_pairs = qcnt ; <nl> if (! i40e_setup_channel ( pf , vsi , ch )) { <nl> ret = - EINVAL ; <nl> + kfree ( ch ); <nl> goto err_free ; <nl> } <nl> ch -> parent_vsi = vsi ;
static void kick_requests ( struct ceph_osd_client * osdc , int force_resend ) <nl> __register_request ( osdc , req ); <nl> __unregister_linger_request ( osdc , req ); <nl> } <nl> + reset_changed_osds ( osdc ); <nl> mutex_unlock (& osdc -> request_mutex ); <nl>  <nl> if ( needmap ) { <nl> dout ("% d requests for down osds , need new map \ n ", needmap ); <nl> ceph_monc_request_next_osdmap (& osdc -> client -> monc ); <nl> } <nl> - reset_changed_osds ( osdc ); <nl> } <nl>  <nl> 
static int tipc_l2_device_event ( struct notifier_block * nb , unsigned long evt , <nl> break ; <nl> case NETDEV_UNREGISTER : <nl> case NETDEV_CHANGENAME : <nl> - bearer_disable ( dev_net ( dev ), b ); <nl> + bearer_disable ( net , b ); <nl> break ; <nl> } <nl> return NOTIFY_OK ;
get_matching_model_microcode ( int cpu , unsigned long start , <nl> unsigned int mc_saved_count = mc_saved_data -> mc_saved_count ; <nl> int i ; <nl>  <nl> - while ( leftover ) { <nl> + while ( leftover && mc_saved_count < ARRAY_SIZE ( mc_saved_tmp )) { <nl> mc_header = ( struct microcode_header_intel *) ucode_ptr ; <nl>  <nl> mc_size = get_totalsize ( mc_header );
static int snd_imx_open ( struct snd_pcm_substream * substream ) <nl> dma_params = snd_soc_dai_get_dma_data ( rtd -> cpu_dai , substream ); <nl>  <nl> dma_data = kzalloc ( sizeof (* dma_data ), GFP_KERNEL ); <nl> + if (! dma_data ) <nl> + return - ENOMEM ; <nl> + <nl> dma_data -> peripheral_type = dma_params -> shared_peripheral ? <nl> IMX_DMATYPE_SSI_SP : IMX_DMATYPE_SSI ; <nl> dma_data -> priority = DMA_PRIO_HIGH ;
static int mcp3422_probe ( struct i2c_client * client , <nl> | MCP3422_CHANNEL_VALUE ( 0 ) <nl> | MCP3422_PGA_VALUE ( MCP3422_PGA_1 ) <nl> | MCP3422_SAMPLE_RATE_VALUE ( MCP3422_SRATE_240 )); <nl> - mcp3422_update_config ( adc , config ); <nl> + err = mcp3422_update_config ( adc , config ); <nl> + if ( err < 0 ) <nl> + return err ; <nl>  <nl> err = devm_iio_device_register (& client -> dev , indio_dev ); <nl> if ( err < 0 )
static void imc_common_cpuhp_mem_free ( struct imc_pmu * pmu_ptr ) <nl> } <nl>  <nl> /* Only free the attr_groups which are dynamically allocated */ <nl> - kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]-> attrs ); <nl> + if ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]) <nl> + kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]-> attrs ); <nl> kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]); <nl> kfree ( pmu_ptr ); <nl> return ;
struct drm_i915_file_private { <nl>  <nl> # define HAS_FORCE_WAKE ( dev ) ( INTEL_INFO ( dev )-> has_force_wake ) <nl>  <nl> -# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev )) <nl> +# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev ) || IS_HASWELL ( dev )) <nl>  <nl> # include " i915_trace . h " <nl> 
static struct dm_region * __rh_alloc ( struct dm_region_hash * rh , region_t region ) <nl>  <nl> nreg = mempool_alloc ( rh -> region_pool , GFP_ATOMIC ); <nl> if ( unlikely (! nreg )) <nl> - nreg = kmalloc ( sizeof (* nreg ), GFP_NOIO ); <nl> + nreg = kmalloc ( sizeof (* nreg ), GFP_NOIO | __GFP_NOFAIL ); <nl>  <nl> nreg -> state = rh -> log -> type -> in_sync ( rh -> log , region , 1 ) ? <nl> DM_RH_CLEAN : DM_RH_NOSYNC ;
static int blan_mdlm_bind ( struct usbnet * dev , struct usb_interface * intf ) <nl> } <nl> /* expect bcdVersion 1 . 0 , ignore */ <nl> if ( memcmp (& desc -> bGUID , blan_guid , 16 ) <nl> - && memcmp (& desc -> bGUID , blan_guid , 16 ) ) { <nl> + && memcmp (& desc -> bGUID , safe_guid , 16 ) ) { <nl> /* hey , this one might _really_ be MDLM ! */ <nl> dev_dbg (& intf -> dev , " MDLM guid \ n "); <nl> goto bad_desc ;
static int mwifiex_pcie_alloc_cmdrsp_buf ( struct mwifiex_adapter * adapter ) <nl> } <nl> skb_put ( skb , MWIFIEX_UPLD_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MWIFIEX_UPLD_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> return - 1 ; <nl> + } <nl>  <nl> card -> cmdrsp_buf = skb ; <nl> 
static int pl2303_tiocmset ( struct tty_struct * tty , <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl>  <nl> mutex_lock (& serial -> disc_mutex ); <nl> - if (! serial -> disconnected ) <nl> + if (! serial -> disconnected ) { <nl> ret = pl2303_set_control_lines ( port , control ); <nl> - else <nl> + if ( ret ) <nl> + ret = usb_translate_errors ( ret ); <nl> + } else { <nl> ret = - ENODEV ; <nl> + } <nl> mutex_unlock (& serial -> disc_mutex ); <nl>  <nl> return ret ;
static int sdio_read_cis ( struct mmc_card * card , struct sdio_func * func ) <nl> if ( tpl_code == 0xff ) <nl> break ; <nl>  <nl> + /* null entries have no link field or data */ <nl> + if ( tpl_code == 0x00 ) <nl> + continue ; <nl> + <nl> ret = mmc_io_rw_direct ( card , 0 , 0 , ptr ++, 0 , & tpl_link ); <nl> if ( ret ) <nl> break ;
# define MPT2SAS_DRIVER_NAME " mpt2sas " <nl> # define MPT2SAS_AUTHOR " LSI Corporation < DL - MPTFusionLinux @ lsi . com >" <nl> # define MPT2SAS_DESCRIPTION " LSI MPT Fusion SAS 2 . 0 Device Driver " <nl> -# define MPT2SAS_DRIVER_VERSION " 07 . 100 . 00 . 00 " <nl> -# define MPT2SAS_MAJOR_VERSION 07 <nl> +# define MPT2SAS_DRIVER_VERSION " 08 . 100 . 00 . 00 " <nl> +# define MPT2SAS_MAJOR_VERSION 08 <nl> # define MPT2SAS_MINOR_VERSION 100 <nl> # define MPT2SAS_BUILD_VERSION 00 <nl> # define MPT2SAS_RELEASE_VERSION 00
static int ac100_rtc_probe ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> chip = devm_kzalloc (& pdev -> dev , sizeof (* chip ), GFP_KERNEL ); <nl> + if (! chip ) <nl> + return - ENOMEM ; <nl> + <nl> platform_set_drvdata ( pdev , chip ); <nl> chip -> dev = & pdev -> dev ; <nl> chip -> regmap = ac100 -> regmap ;
int __ceph_caps_used ( struct ceph_inode_info * ci ) <nl> used |= CEPH_CAP_PIN ; <nl> if ( ci -> i_rd_ref ) <nl> used |= CEPH_CAP_FILE_RD ; <nl> - if ( ci -> i_rdcache_ref || ci -> i_rdcache_gen ) <nl> + if ( ci -> i_rdcache_ref || ci -> vfs_inode . i_data . nrpages ) <nl> used |= CEPH_CAP_FILE_CACHE ; <nl> if ( ci -> i_wr_ref ) <nl> used |= CEPH_CAP_FILE_WR ;
int ip6_xmit ( struct sock * sk , struct sk_buff * skb , struct flowi * fl , <nl> skb_reset_network_header ( skb ); <nl> hdr = ipv6_hdr ( skb ); <nl>  <nl> + /* Allow local fragmentation . */ <nl> + if ( ipfragok ) <nl> + skb -> local_df = 1 ; <nl> + <nl> /* <nl> * Fill in the IPv6 header <nl> */
static long __video_do_ioctl ( struct file * file , <nl> break ; <nl>  <nl> ret = 0 ; <nl> + p -> parm . capture . readbuffers = 2 ; <nl> if ( ops -> vidioc_g_std ) <nl> ret = ops -> vidioc_g_std ( file , fh , & std ); <nl> if ( ret == 0 )
static int do_loopback ( struct path * path , const char * old_name , <nl>  <nl> if ( IS_ERR ( mnt )) { <nl> err = PTR_ERR ( mnt ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> err = graft_tree ( mnt , path );
static int adis16400_read_raw ( struct iio_dev * indio_dev , <nl> * val = st -> variant -> temp_scale_nano / 1000000 ; <nl> * val2 = ( st -> variant -> temp_scale_nano % 1000000 ); <nl> return IIO_VAL_INT_PLUS_MICRO ; <nl> + case IIO_PRESSURE : <nl> + /* 20 uBar = 0 . 002kPascal */ <nl> + * val = 0 ; <nl> + * val2 = 2000 ; <nl> + return IIO_VAL_INT_PLUS_MICRO ; <nl> default : <nl> return - EINVAL ; <nl> }
__append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
static void ixgbe_configure_dcb ( struct ixgbe_adapter * adapter ) <nl> if ( hw -> mac . type == ixgbe_mac_82598EB ) <nl> netif_set_gso_max_size ( adapter -> netdev , 32768 ); <nl>  <nl> - ixgbe_dcb_check_config (& adapter -> dcb_cfg ); <nl> ixgbe_dcb_calculate_tc_credits (& adapter -> dcb_cfg , DCB_TX_CONFIG ); <nl> ixgbe_dcb_calculate_tc_credits (& adapter -> dcb_cfg , DCB_RX_CONFIG ); <nl> 
xfsbufd ( <nl>  <nl> current -> flags |= PF_MEMALLOC ; <nl>  <nl> + set_freezable (); <nl> + <nl> do { <nl> if ( unlikely ( freezing ( current ))) { <nl> set_bit ( XBT_FORCE_SLEEP , & target -> bt_flags );
static bool snd_soc_set_cache_val ( void * base , unsigned int idx , <nl> static unsigned int snd_soc_get_cache_val ( const void * base , unsigned int idx , <nl> unsigned int word_size ) <nl> { <nl> + if (! base ) <nl> + return - 1 ; <nl> + <nl> switch ( word_size ) { <nl> case 1 : { <nl> const u8 * cache = base ;
static void usb_alphatrack_disconnect ( struct usb_interface * intf ) <nl> mutex_unlock (& dev -> mtx ); <nl> usb_alphatrack_delete ( dev ); <nl> } else { <nl> + atomic_set (& dev -> writes_pending , 0 ); <nl> dev -> intf = NULL ; <nl> mutex_unlock (& dev -> mtx ); <nl> } <nl>  <nl> - atomic_set (& dev -> writes_pending , 0 ); <nl> mutex_unlock (& disconnect_mutex ); <nl>  <nl> dev_info (& intf -> dev , " Alphatrack Surface #% d now disconnected \ n ",
static struct irq_chip amd_gpio_irqchip = { <nl> . irq_set_type = amd_gpio_irq_set_type , <nl> }; <nl>  <nl> - static void amd_gpio_irq_handler ( unsigned int irq , struct irq_desc * desc ) <nl> + static void amd_gpio_irq_handler ( unsigned int __irq , struct irq_desc * desc ) <nl> { <nl> + unsigned int irq = irq_desc_get_irq ( desc ); <nl> u32 i ; <nl> u32 off ; <nl> u32 reg ;
unsigned long slice_get_unmapped_area ( unsigned long addr , unsigned long len , <nl> unsigned long high_limit ; <nl>  <nl> high_limit = DEFAULT_MAP_WINDOW ; <nl> - if ( addr >= high_limit ) <nl> + if ( addr >= high_limit || ( fixed && ( addr + len > high_limit ))) <nl> high_limit = TASK_SIZE ; <nl>  <nl> if ( len > high_limit )
static int ubifs_get_sb ( struct file_system_type * fs_type , int flags , <nl> */ <nl> ubi = open_ubi ( name , UBI_READONLY ); <nl> if ( IS_ERR ( ubi )) { <nl> - ubifs_err (" cannot open \"% s \", error % d ", <nl> - name , ( int ) PTR_ERR ( ubi )); <nl> + dbg_err (" cannot open \"% s \", error % d ", <nl> + name , ( int ) PTR_ERR ( ubi )); <nl> return PTR_ERR ( ubi ); <nl> } <nl> ubi_get_volume_info ( ubi , & vi );
static int pfkey_register ( struct sock * sk , struct sk_buff * skb , const struct sad <nl>  <nl> xfrm_probe_algs (); <nl>  <nl> - supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL ); <nl> + supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL | __GFP_ZERO ); <nl> if (! supp_skb ) { <nl> if ( hdr -> sadb_msg_satype != SADB_SATYPE_UNSPEC ) <nl> pfk -> registered &= ~( 1 << hdr -> sadb_msg_satype );
void scsi_io_completion ( struct scsi_cmnd * cmd , unsigned int good_bytes ) <nl> */ <nl> req -> next_rq -> resid_len = scsi_in ( cmd )-> resid ; <nl>  <nl> + scsi_release_buffers ( cmd ); <nl> blk_end_request_all ( req , 0 ); <nl>  <nl> - scsi_release_buffers ( cmd ); <nl> scsi_next_command ( cmd ); <nl> return ; <nl> }
static void flush_tmregs_to_thread ( struct task_struct * tsk ) <nl> * in the appropriate thread structures from live . <nl> */ <nl>  <nl> - if ( tsk != current ) <nl> + if ((! cpu_has_feature ( CPU_FTR_TM )) || ( tsk != current )) <nl> return ; <nl>  <nl> if ( MSR_TM_SUSPENDED ( mfmsr ())) {
int i2400m_msg_check_status ( const struct i2400m_l3l4_hdr * l3l4_hdr , <nl>  <nl> if ( status == 0 ) <nl> return 0 ; <nl> - if ( status > ARRAY_SIZE ( ms_to_errno )) { <nl> + if ( status >= ARRAY_SIZE ( ms_to_errno )) { <nl> str = " unknown status code "; <nl> result = - EBADR ; <nl> } else {
static int r8a66597_hub_status_data ( struct usb_hcd * hcd , char * buf ) <nl> static void r8a66597_hub_descriptor ( struct r8a66597 * r8a66597 , <nl> struct usb_hub_descriptor * desc ) <nl> { <nl> - desc -> bDescriptorType = 0x29 ; <nl> + desc -> bDescriptorType = USB_DT_HUB ; <nl> desc -> bHubContrCurrent = 0 ; <nl> desc -> bNbrPorts = r8a66597 -> max_root_hub ; <nl> desc -> bDescLength = 9 ;
static void rs_free_sta ( void * priv_r , struct ieee80211_sta * sta , <nl> void * priv_sta ) <nl> { <nl> struct iwl_lq_sta * lq_sta = priv_sta ; <nl> - struct iwl_priv * priv = priv_r ; <nl> + struct iwl_priv * priv __maybe_unused = priv_r ; <nl>  <nl> IWL_DEBUG_RATE (" enter \ n "); <nl> kfree ( lq_sta );
static struct urb * usbtv_setup_iso_transfer ( struct usbtv * usbtv ) <nl> ip -> transfer_flags = URB_ISO_ASAP ; <nl> ip -> transfer_buffer = kzalloc ( size * USBTV_ISOC_PACKETS , <nl> GFP_KERNEL ); <nl> + if (! ip -> transfer_buffer ) { <nl> + usb_free_urb ( ip ); <nl> + return NULL ; <nl> + } <nl> ip -> complete = usbtv_iso_cb ; <nl> ip -> number_of_packets = USBTV_ISOC_PACKETS ; <nl> ip -> transfer_buffer_length = size * USBTV_ISOC_PACKETS ;
static int atmel_serial_remove ( struct platform_device * pdev ) <nl> struct atmel_uart_port * atmel_port = to_atmel_uart_port ( port ); <nl> int ret = 0 ; <nl>  <nl> + tasklet_kill (& atmel_port -> tasklet ); <nl> + <nl> device_init_wakeup (& pdev -> dev , 0 ); <nl>  <nl> ret = uart_remove_one_port (& atmel_uart , port ); <nl>  <nl> - tasklet_kill (& atmel_port -> tasklet ); <nl> kfree ( atmel_port -> rx_ring . buf ); <nl>  <nl> /* " port " is allocated statically , so we shouldn ' t free it */
static void load_render_mocs ( struct drm_i915_private * dev_priv ) <nl> }; <nl> int ring_id , i ; <nl>  <nl> - for ( ring_id = 0 ; ring_id < I915_NUM_ENGINES ; ring_id ++) { <nl> + for ( ring_id = 0 ; ring_id < ARRAY_SIZE ( regs ); ring_id ++) { <nl> offset . reg = regs [ ring_id ]; <nl> for ( i = 0 ; i < 64 ; i ++) { <nl> gen9_render_mocs . control_table [ ring_id ][ i ] =
static int cpufreq_governor_dbs ( struct cpufreq_policy * policy , <nl> if ( latency == 0 ) <nl> latency = 1 ; <nl>  <nl> - def_sampling_rate = latency * <nl> + def_sampling_rate = 10 * latency * <nl> DEF_SAMPLING_RATE_LATENCY_MULTIPLIER ; <nl>  <nl> if ( def_sampling_rate < MIN_STAT_SAMPLING_RATE )
qdio_check_ccq ( struct qdio_q * q , unsigned int ccq ) <nl> { <nl> char dbf_text [ 15 ]; <nl>  <nl> - if ( ccq == 0 || ccq == 32 || ccq == 96 ) <nl> + if ( ccq == 0 || ccq == 32 ) <nl> return 0 ; <nl> - if ( ccq == 97 ) <nl> + if ( ccq == 96 || ccq == 97 ) <nl> return 1 ; <nl> /* notify devices immediately */ <nl> sprintf ( dbf_text ,"% d ", ccq );
static void ssb_pmu_resources_init ( struct ssb_chipcommon * cc ) <nl>  <nl> switch ( bus -> chip_id ) { <nl> case 0x4312 : <nl> + min_msk = 0xCBB ; <nl> + break ; <nl> case 0x4322 : <nl> /* We keep the default settings : <nl> * min_msk = 0xCBB
int btrfs_read_sys_array ( struct btrfs_root * root ) <nl> sb_array_offset += len ; <nl> cur_offset += len ; <nl> } <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return ret ; <nl>  <nl> out_short_read : <nl> printk ( KERN_ERR " BTRFS : sys_array too short to read % u bytes at offset % u \ n ", <nl> len , cur_offset ); <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return - EIO ; <nl> }
static void t3e3_remove_card ( struct pci_dev * pdev ) <nl> struct channel * channel0 = pci_get_drvdata ( pdev ); <nl> struct card * card = channel0 -> card ; <nl>  <nl> - del_timer (& card -> timer ); <nl> + del_timer_sync (& card -> timer ); <nl> if ( has_two_ports ( channel0 -> pdev )) { <nl> t3e3_remove_channel (& card -> channels [ 1 ]); <nl> pci_dev_put ( card -> channels [ 1 ]. pdev );
aoedev_freedev ( struct aoedev * d ) <nl> put_disk ( d -> gd ); <nl> } <nl> kfree ( d -> frames ); <nl> - mempool_destroy ( d -> bufpool ); <nl> + if ( d -> bufpool ) <nl> + mempool_destroy ( d -> bufpool ); <nl> kfree ( d ); <nl> } <nl> 
struct fman_mac * memac_config ( struct fman_mac_params * params ) <nl> /* Save FMan revision */ <nl> fman_get_revision ( memac -> fm , & memac -> fm_rev_info ); <nl>  <nl> - if ( memac -> phy_if == PHY_INTERFACE_MODE_SGMII ) { <nl> + if ( memac -> phy_if == PHY_INTERFACE_MODE_SGMII || <nl> + memac -> phy_if == PHY_INTERFACE_MODE_QSGMII ) { <nl> if (! params -> internal_phy_node ) { <nl> pr_err (" PCS PHY node is not available \ n "); <nl> memac_free ( memac );
static int start_ap ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> priv = wiphy_priv ( wiphy ); <nl> vif = netdev_priv ( dev ); <nl> - wl = vif -> wilc ; <nl> + wl = vif -> wilc ; <nl>  <nl> s32Error = set_channel ( wiphy , & settings -> chandef ); <nl> 
static void pptp_expectfn ( struct nf_conn * ct , <nl>  <nl> rcu_read_lock (); <nl> nf_nat_pptp_expectfn = rcu_dereference ( nf_nat_pptp_hook_expectfn ); <nl> - if ( nf_nat_pptp_expectfn && ct -> status & IPS_NAT_MASK ) <nl> + if ( nf_nat_pptp_expectfn && ct -> master -> status & IPS_NAT_MASK ) <nl> nf_nat_pptp_expectfn ( ct , exp ); <nl> else { <nl> struct nf_conntrack_tuple inv_t ;
static int __arm_v7s_map ( struct arm_v7s_io_pgtable * data , unsigned long iova , <nl> pte |= ARM_V7S_ATTR_NS_TABLE ; <nl>  <nl> __arm_v7s_set_pte ( ptep , pte , 1 , cfg ); <nl> - } else { <nl> + } else if ( ARM_V7S_PTE_IS_TABLE ( pte , lvl )) { <nl> cptep = iopte_deref ( pte , lvl ); <nl> + } else { <nl> + /* We require an unmap first */ <nl> + WARN_ON (! selftest_running ); <nl> + return - EEXIST ; <nl> } <nl>  <nl> /* Rinse , repeat */
static int pn_send ( struct sk_buff * skb , struct net_device * dev , <nl> struct phonethdr * ph ; <nl> int err ; <nl>  <nl> - if ( skb -> len + 2 > 0xffff ) { <nl> - /* Phonet length field would overflow */ <nl> + if ( skb -> len + 2 > 0xffff /* Phonet length field limit */ || <nl> + skb -> len + sizeof ( struct phonethdr ) > dev -> mtu ) { <nl> err = - EMSGSIZE ; <nl> goto drop ; <nl> }
static int soc_tplg_dapm_widget_create ( struct soc_tplg * tplg , <nl> widget -> dobj . type = SND_SOC_DOBJ_WIDGET ; <nl> widget -> dobj . ops = tplg -> ops ; <nl> widget -> dobj . index = tplg -> index ; <nl> + kfree ( template . sname ); <nl> + kfree ( template . name ); <nl> list_add (& widget -> dobj . list , & tplg -> comp -> dobj_list ); <nl> return 0 ; <nl> 
void rtsx_add_cmd ( struct rtsx_chip * chip , <nl> void rtsx_send_cmd_no_wait ( struct rtsx_chip * chip ); <nl> int rtsx_send_cmd ( struct rtsx_chip * chip , u8 card , int timeout ); <nl>  <nl> - extern inline u8 * rtsx_get_cmd_data ( struct rtsx_chip * chip ) <nl> + static inline u8 * rtsx_get_cmd_data ( struct rtsx_chip * chip ) <nl> { <nl> # ifdef CMD_USING_SG <nl> return ( u8 *)( chip -> host_sg_tbl_ptr );
static int device_rx_srv ( struct vnt_private * pDevice , unsigned int uIdx ) <nl> pRD = pRD -> next ) { <nl> if ( works ++ > 15 ) <nl> break ; <nl> + <nl> + if (! pRD -> pRDInfo -> skb ) <nl> + break ; <nl> + <nl> if ( vnt_receive_frame ( pDevice , pRD )) { <nl> if (! device_alloc_rx_buf ( pDevice , pRD )) { <nl> dev_err (& pDevice -> pcid -> dev ,
# include < linux / interrupt . h > <nl> # include < linux / pci . h > <nl> # include < linux / firmware . h > <nl> +# include < linux / vmalloc . h > <nl> # include < asm / io . h > <nl> # include < sound / core . h > <nl> # include " mixart . h "
i915_gem_wait_ioctl ( struct drm_device * dev , void * data , struct drm_file * file ) <nl> u32 seqno = 0 ; <nl> int ret = 0 ; <nl>  <nl> + if ( args -> flags != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = i915_mutex_lock_interruptible ( dev ); <nl> if ( ret ) <nl> return ret ;
composite_setup ( struct usb_gadget * gadget , const struct usb_ctrlrequest * ctrl ) <nl> if ( w_index != 0x5 || ( w_value >> 8 )) <nl> break ; <nl> interface = w_value & 0xFF ; <nl> + if ( interface >= MAX_CONFIG_INTERFACES || <nl> + ! os_desc_cfg -> interface [ interface ]) <nl> + break ; <nl> buf [ 6 ] = w_index ; <nl> count = count_ext_prop ( os_desc_cfg , <nl> interface );
int dw_mci_probe ( struct dw_mci * host ) <nl> } <nl> } <nl>  <nl> - if ( host -> pdata -> num_slots > 1 ) { <nl> + if ( host -> pdata -> num_slots < 1 ) { <nl> dev_err ( host -> dev , <nl> " Platform data must supply num_slots .\ n "); <nl> return - ENODEV ;
static int iucv_sock_sendmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> txmsg . class = 0 ; <nl> + memcpy (& txmsg . class , skb -> data , skb -> len >= 4 ? 4 : skb -> len ); <nl> txmsg . tag = iucv -> send_tag ++; <nl> memcpy ( skb -> cb , & txmsg . tag , 4 ); <nl> skb_queue_tail (& iucv -> send_skb_q , skb );
static int ilk_compute_pipe_wm ( struct intel_crtc * intel_crtc , <nl> return PTR_ERR ( cstate ); <nl>  <nl> pipe_wm = & cstate -> wm . optimal . ilk ; <nl> + memset ( pipe_wm , 0 , sizeof (* pipe_wm )); <nl>  <nl> for_each_intel_plane_on_crtc ( dev , intel_crtc , intel_plane ) { <nl> ps = drm_atomic_get_plane_state ( state ,
static const struct snd_pci_quirk alc269_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x103c , 0x226e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x2271 , " HP ", ALC286_FIXUP_HP_GPIO_LED ), <nl> SND_PCI_QUIRK ( 0x103c , 0x2272 , " HP ", ALC280_FIXUP_HP_DOCK_PINS ), <nl> + SND_PCI_QUIRK ( 0x103c , 0x2273 , " HP ", ALC280_FIXUP_HP_DOCK_PINS ), <nl> SND_PCI_QUIRK ( 0x103c , 0x229e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22b2 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22b7 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ),
int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> + mutex_lock (& gpu -> lock ); <nl> + <nl> fence = etnaviv_gpu_fence_alloc ( gpu ); <nl> if (! fence ) { <nl> event_free ( gpu , event ); <nl> int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> - mutex_lock (& gpu -> lock ); <nl> - <nl> gpu -> event [ event ]. fence = fence ; <nl> submit -> fence = fence -> seqno ; <nl> gpu -> active_fence = submit -> fence ;
struct snd_kcontrol * snd_soc_cnew ( const struct snd_kcontrol_new * _template , <nl>  <nl> if ( prefix ) { <nl> name_len = strlen ( long_name ) + strlen ( prefix ) + 2 ; <nl> - name = kmalloc ( name_len , GFP_ATOMIC ); <nl> + name = kmalloc ( name_len , GFP_KERNEL ); <nl> if (! name ) <nl> return NULL ; <nl> 
xfs_fs_remount ( <nl>  <nl> /* ro -> rw */ <nl> if (( mp -> m_flags & XFS_MOUNT_RDONLY ) && !(* flags & MS_RDONLY )) { <nl> + if ( mp -> m_flags & XFS_MOUNT_NORECOVERY ) { <nl> + xfs_warn ( mp , <nl> + " ro -> rw transition prohibited on norecovery mount "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> mp -> m_flags &= ~ XFS_MOUNT_RDONLY ; <nl>  <nl> /*
static int drbg_generate_long ( struct drbg_state * drbg , <nl> if ( 0 >= tmplen ) <nl> return tmplen ; <nl> len += tmplen ; <nl> - } while ( slice > 0 ); <nl> + } while ( slice > 0 && ( len < buflen )); <nl> return len ; <nl> } <nl> 
struct kvm_s390_float_interrupt { <nl> struct list_head list ; <nl> atomic_t active ; <nl> int next_rr_cpu ; <nl> - unsigned long idle_mask [( 64 + sizeof ( long ) - 1 ) / sizeof ( long )]; <nl> - struct kvm_s390_local_interrupt * local_int [ 64 ]; <nl> + unsigned long idle_mask [( KVM_MAX_VCPUS + sizeof ( long ) - 1 ) <nl> + / sizeof ( long )]; <nl> + struct kvm_s390_local_interrupt * local_int [ KVM_MAX_VCPUS ]; <nl> }; <nl>  <nl> 
static void _qed_iscsi_get_tstats ( struct qed_hwfn * p_hwfn , <nl> HILO_64_REGPAIR ( tstats . iscsi_rx_bytes_cnt ); <nl> p_stats -> iscsi_rx_packet_cnt = <nl> HILO_64_REGPAIR ( tstats . iscsi_rx_packet_cnt ); <nl> + p_stats -> iscsi_rx_new_ooo_isle_events_cnt = <nl> + HILO_64_REGPAIR ( tstats . iscsi_rx_new_ooo_isle_events_cnt ); <nl> p_stats -> iscsi_cmdq_threshold_cnt = <nl> le32_to_cpu ( tstats . iscsi_cmdq_threshold_cnt ); <nl> p_stats -> iscsi_rq_threshold_cnt =
void Dot11d_Init ( struct ieee80211_device * ieee ) <nl>  <nl> pDot11dInfo -> State = DOT11D_STATE_NONE ; <nl> pDot11dInfo -> CountryIeLen = 0 ; <nl> - memset ( pDot11dInfo -> channel_map , 0 , MAX_CHANNEL_NUMBER + 1 ); <nl> + memset ( pDot11dInfo -> channel_map , 0 , MAX_CHANNEL_NUMBER + 1 ); <nl> memset ( pDot11dInfo -> MaxTxPwrDbmList , 0xFF , MAX_CHANNEL_NUMBER + 1 ); <nl> RESET_CIE_WATCHDOG ( ieee ); <nl> 
static const struct labpc_board_struct labpc_cs_boards [] = { <nl> }, <nl> }; <nl>  <nl> -/* <nl> - * Useful for shorthand access to the particular board structure <nl> - */ <nl> -# define thisboard (( const struct labpc_board_struct *) dev -> board_ptr ) <nl> - <nl> static int labpc_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> { <nl> + const struct labpc_board_struct * thisboard = comedi_board ( dev ); <nl> struct labpc_private * devpriv ; <nl> unsigned long iobase = 0 ; <nl> unsigned int irq = 0 ;
static void scsi_finish_async_scan ( struct async_scan_data * data ) <nl> printk ("% s called twice for host % d ", __FUNCTION__ , <nl> shost -> host_no ); <nl> dump_stack (); <nl> + mutex_unlock (& shost -> scan_mutex ); <nl> return ; <nl> } <nl> 
static void ide_complete_power_step ( ide_drive_t * drive , struct request * rq , u8 s <nl>  <nl> switch ( rq -> pm -> pm_step ) { <nl> case ide_pm_flush_cache : /* Suspend step 1 ( flush cache ) complete */ <nl> - if ( rq -> pm -> pm_state == 4 ) <nl> + if ( rq -> pm -> pm_state == PM_EVENT_FREEZE ) <nl> rq -> pm -> pm_step = ide_pm_state_completed ; <nl> else <nl> rq -> pm -> pm_step = idedisk_pm_standby ;
static int create_filter ( struct trace_event_call * call , <nl> if ( err && set_str ) <nl> append_filter_err ( ps , filter ); <nl> } <nl> + if ( err && ! set_str ) { <nl> + free_event_filter ( filter ); <nl> + filter = NULL ; <nl> + } <nl> create_filter_finish ( ps ); <nl>  <nl> * filterp = filter ;
static int _mei_irq_thread_read ( struct mei_device * dev , s32 * slots , <nl> struct mei_cl * cl , <nl> struct mei_io_list * cmpl_list ) <nl> { <nl> - if ((* slots * sizeof ( u32 )) >= ( sizeof ( struct mei_msg_hdr ) + <nl> + if ((* slots * sizeof ( u32 )) < ( sizeof ( struct mei_msg_hdr ) + <nl> sizeof ( struct hbm_flow_control ))) { <nl> /* return the cancel routine */ <nl> list_del (& cb_pos -> cb_list );
static int snapshot_open ( struct inode * inode , struct file * filp ) <nl> if ( error ) <nl> pm_notifier_call_chain ( PM_POST_RESTORE ); <nl> } <nl> - if ( error ) <nl> + if ( error ) { <nl> + free_basic_memory_bitmaps (); <nl> atomic_inc (& snapshot_device_available ); <nl> + } <nl> data -> frozen = 0 ; <nl> data -> ready = 0 ; <nl> data -> platform_support = 0 ;
fill_write_buffer ( struct sysfs_buffer * buffer , const char __user * buf , size_t <nl> return - ENOMEM ; <nl>  <nl> if ( count >= PAGE_SIZE ) <nl> - count = PAGE_SIZE ; <nl> + count = PAGE_SIZE - 1 ; <nl> error = copy_from_user ( buffer -> page , buf , count ); <nl> buffer -> needs_read_fill = 1 ; <nl> return error ? - EFAULT : count ;
static int ipw2100_get_firmware ( struct ipw2100_priv * priv , <nl> return 0 ; <nl> } <nl>  <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("- i ")); <nl> +# ifdef CONFIG_IPW2100_MONITOR <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("- p ")); <nl> +# endif <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("")); <nl> + <nl> static void ipw2100_release_firmware ( struct ipw2100_priv * priv , <nl> struct ipw2100_fw * fw ) <nl> {
static int i915_display_info ( struct seq_file * m , void * unused ) <nl> x , y , crtc -> cursor_addr , <nl> yesno ( active )); <nl> } <nl> + <nl> + seq_printf ( m , "\ tunderrun reporting : cpu =% s pch =% s \ n ", <nl> + yesno (! crtc -> cpu_fifo_underrun_disabled ), <nl> + yesno (! crtc -> pch_fifo_underrun_disabled )); <nl> } <nl>  <nl> seq_printf ( m , "\ n ");
static long __write_once initfree = 1 ; <nl> static int __init set_initfree ( char * str ) <nl> { <nl> long val ; <nl> - if ( strict_strtol ( str , 0 , & val )) { <nl> + if ( strict_strtol ( str , 0 , & val ) == 0 ) { <nl> initfree = val ; <nl> pr_info (" initfree : % s free init pages \ n ", <nl> initfree ? " will " : " won ' t ");
static int rtnl_fill_ifinfo ( struct sk_buff * skb , struct net_device * dev , <nl> * report anything . <nl> */ <nl> ivi . spoofchk = - 1 ; <nl> + memset ( ivi . mac , 0 , sizeof ( ivi . mac )); <nl> if ( dev -> netdev_ops -> ndo_get_vf_config ( dev , i , & ivi )) <nl> break ; <nl> vf_mac . vf =
void gen8_fbc_sw_flush ( struct drm_device * dev , u32 value ) <nl> if (! IS_GEN8 ( dev )) <nl> return ; <nl>  <nl> + if (! intel_fbc_enabled ( dev )) <nl> + return ; <nl> + <nl> I915_WRITE ( MSG_FBC_REND_STATE , value ); <nl> } <nl> 
i915_gem_execbuffer2 ( struct drm_device * dev , void * data , <nl> struct drm_i915_gem_exec_object2 * exec2_list = NULL ; <nl> int ret ; <nl>  <nl> - if ( args -> buffer_count < 1 ) { <nl> + if ( args -> buffer_count < 1 || <nl> + args -> buffer_count > UINT_MAX / sizeof (* exec2_list )) { <nl> DRM_DEBUG (" execbuf2 with % d buffers \ n ", args -> buffer_count ); <nl> return - EINVAL ; <nl> }
static void cnl_display_core_init ( struct drm_i915_private * dev_priv , bool resume <nl>  <nl> /* 6 . Enable DBUF */ <nl> gen9_dbuf_enable ( dev_priv ); <nl> + <nl> + if ( resume && dev_priv -> csr . dmc_payload ) <nl> + intel_csr_load_program ( dev_priv ); <nl> } <nl>  <nl> # undef CNL_PROCMON_IDX
static void __ipr_remove ( struct pci_dev * pdev ) <nl> spin_unlock_irqrestore ( ioa_cfg -> host -> host_lock , host_lock_flags ); <nl> wait_event ( ioa_cfg -> reset_wait_q , ! ioa_cfg -> in_reset_reload ); <nl> flush_work (& ioa_cfg -> work_q ); <nl> + INIT_LIST_HEAD (& ioa_cfg -> used_res_q ); <nl> spin_lock_irqsave ( ioa_cfg -> host -> host_lock , host_lock_flags ); <nl>  <nl> spin_lock (& ipr_driver_lock );
struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
static int __devexit gpio_keys_remove ( struct platform_device * pdev ) <nl> } <nl>  <nl> input_unregister_device ( input ); <nl> + kfree ( ddata ); <nl>  <nl> return 0 ; <nl> }
int mdiobus_register ( struct mii_bus * bus ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> + put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
static int llc_ui_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> int target ; /* Read at least this many bytes */ <nl> long timeo ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl> copied = - ENOTCONN ; <nl> if ( unlikely ( sk -> sk_type == SOCK_STREAM && sk -> sk_state == TCP_LISTEN ))
static void do_boot_cpu ( __u8 cpuid ); <nl> static void do_quad_bootstrap ( void ); <nl>  <nl> int hard_smp_processor_id ( void ); <nl> + int safe_smp_processor_id ( void ); <nl>  <nl> /* Inline functions */ <nl> static inline void <nl> hard_smp_processor_id ( void ) <nl> return 0 ; <nl> } <nl>  <nl> + int <nl> + safe_smp_processor_id ( void ) <nl> +{ <nl> + return hard_smp_processor_id (); <nl> +} <nl> + <nl> /* broadcast a halt to all other CPUs */ <nl> void <nl> smp_send_stop ( void )
static ssize_t environ_read ( struct file * file , char __user * buf , <nl> struct mm_struct * mm = file -> private_data ; <nl> unsigned long env_start , env_end ; <nl>  <nl> - if (! mm ) <nl> + /* Ensure the process spawned far enough to have an environment . */ <nl> + if (! mm || ! mm -> env_end ) <nl> return 0 ; <nl>  <nl> page = ( char *) __get_free_page ( GFP_TEMPORARY );
static int rose_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> skb_copy_datagram_iovec ( skb , 0 , msg -> msg_iov , copied ); <nl>  <nl> if ( srose != NULL ) { <nl> + memset ( srose , 0 , msg -> msg_namelen ); <nl> srose -> srose_family = AF_ROSE ; <nl> srose -> srose_addr = rose -> dest_addr ; <nl> srose -> srose_call = rose -> dest_call ;
static ssize_t pubek_show ( struct device * dev , struct device_attribute * attr , <nl> ssize_t err ; <nl> int i , rc ; <nl> char * str = buf ; <nl> - <nl> struct tpm_chip * chip = to_tpm_chip ( dev ); <nl>  <nl> + memset (& tpm_cmd , 0 , sizeof ( tpm_cmd )); <nl> + <nl> tpm_cmd . header . in = tpm_readpubek_header ; <nl> err = tpm_transmit_cmd ( chip , NULL , & tpm_cmd , READ_PUBEK_RESULT_SIZE , <nl> READ_PUBEK_RESULT_MIN_BODY_SIZE , 0 ,
static void set_pool_mode ( struct pool * pool , enum pool_mode new_mode ) <nl> case PM_WRITE : <nl> if ( old_mode != new_mode ) <nl> notify_of_pool_mode_change ( pool , " write "); <nl> + pool -> pf . error_if_no_space = pt -> requested_pf . error_if_no_space ; <nl> dm_pool_metadata_read_write ( pool -> pmd ); <nl> pool -> process_bio = process_bio ; <nl> pool -> process_discard = process_discard_bio ;
static int msm_otg_read_dt ( struct platform_device * pdev , struct msm_otg * motg ) <nl> motg -> pdata = pdata ; <nl>  <nl> id = of_match_device ( msm_otg_dt_match , & pdev -> dev ); <nl> - pdata -> phy_type = ( int ) id -> data ; <nl> + pdata -> phy_type = ( enum msm_usb_phy_type ) id -> data ; <nl>  <nl> motg -> link_rst = devm_reset_control_get (& pdev -> dev , " link "); <nl> if ( IS_ERR ( motg -> link_rst ))
xfs_rmap_convert_shared ( <nl> */ <nl> error = xfs_rmap_lookup_le_range ( cur , bno , owner , offset , flags , <nl> & PREV , & i ); <nl> + if ( error ) <nl> + goto done ; <nl> XFS_WANT_CORRUPTED_GOTO ( mp , i == 1 , done ); <nl>  <nl> ASSERT ( PREV . rm_offset <= offset );
int dccp_disconnect ( struct sock * sk , int flags ) <nl> sk -> sk_err = ECONNRESET ; <nl>  <nl> dccp_clear_xmit_timers ( sk ); <nl> + <nl> __skb_queue_purge (& sk -> sk_receive_queue ); <nl> + __skb_queue_purge (& sk -> sk_write_queue ); <nl> if ( sk -> sk_send_head != NULL ) { <nl> __kfree_skb ( sk -> sk_send_head ); <nl> sk -> sk_send_head = NULL ;
static int crypto_report ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
static int sgtl5000_set_clock ( struct snd_soc_codec * codec , int frame_rate ) <nl> } else { <nl> dev_err ( codec -> dev , <nl> " PLL not supported in slave mode \ n "); <nl> + dev_err ( codec -> dev , "% d ratio is not supported . " <nl> + " SYS_MCLK needs to be 256 , 384 or 512 * fs \ n ", <nl> + sgtl5000 -> sysclk / sys_fs ); <nl> return - EINVAL ; <nl> } <nl> }
static void * skd_alloc_dma ( struct skd_device * skdev , struct kmem_cache * s , <nl> return NULL ; <nl> * dma_handle = dma_map_single ( dev , buf , s -> size , dir ); <nl> if ( dma_mapping_error ( dev , * dma_handle )) { <nl> - kfree ( buf ); <nl> + kmem_cache_free ( s , buf ); <nl> buf = NULL ; <nl> } <nl> return buf ;
static int __xen_pcibk_add_pci_dev ( struct xen_pcibk_device * pdev , <nl> /* Publish this device . */ <nl> if (! err ) <nl> err = publish_cb ( pdev , 0 , 0 , PCI_DEVFN ( slot , func ), devid ); <nl> + else <nl> + kfree ( dev_entry ); <nl>  <nl> out : <nl> return err ;
static int ath9k_htc_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath9k_htc_sta * ista ; <nl> int ret = 0 ; <nl>  <nl> + mutex_lock (& priv -> mutex ); <nl> + <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> break ; <nl> static int ath9k_htc_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( priv -> ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> + mutex_unlock (& priv -> mutex ); <nl> + <nl> return ret ; <nl> } <nl> 
static struct protection_domain * get_domain ( struct device * dev ) <nl> domain = to_pdomain ( io_domain ); <nl> attach_device ( dev , domain ); <nl> } <nl> + if ( domain == NULL ) <nl> + return ERR_PTR (- EBUSY ); <nl> + <nl> if (! dma_ops_domain ( domain )) <nl> return ERR_PTR (- EBUSY ); <nl> 
etrax_ethernet_init ( void ) <nl> * does not share cacheline with any other data ( to avoid cache bug ) <nl> */ <nl> RxDescList [ i ]. skb = dev_alloc_skb ( MAX_MEDIA_DATA_SIZE + 2 * L1_CACHE_BYTES ); <nl> + if (! RxDescList [ i ]. skb ) <nl> + return - ENOMEM ; <nl> RxDescList [ i ]. descr . ctrl = 0 ; <nl> RxDescList [ i ]. descr . sw_len = MAX_MEDIA_DATA_SIZE ; <nl> RxDescList [ i ]. descr . next = virt_to_phys (& RxDescList [ i + 1 ]);
void xhci_mem_cleanup ( struct xhci_hcd * xhci ) <nl>  <nl> if ( xhci -> lpm_command ) <nl> xhci_free_command ( xhci , xhci -> lpm_command ); <nl> + xhci -> lpm_command = NULL ; <nl> if ( xhci -> cmd_ring ) <nl> xhci_ring_free ( xhci , xhci -> cmd_ring ); <nl> xhci -> cmd_ring = NULL ;
int vcc_getsockopt ( struct socket * sock , int level , int optname , <nl>  <nl> if (! vcc -> dev || ! test_bit ( ATM_VF_ADDR , & vcc -> flags )) <nl> return - ENOTCONN ; <nl> + memset (& pvc , 0 , sizeof ( pvc )); <nl> pvc . sap_family = AF_ATMPVC ; <nl> pvc . sap_addr . itf = vcc -> dev -> number ; <nl> pvc . sap_addr . vpi = vcc -> vpi ;
static void kvm_write_wall_clock ( struct kvm * kvm , gpa_t wall_clock ) <nl> */ <nl> getboottime (& boot ); <nl>  <nl> + if ( kvm -> arch . kvmclock_offset ) { <nl> + struct timespec ts = ns_to_timespec ( kvm -> arch . kvmclock_offset ); <nl> + boot = timespec_sub ( boot , ts ); <nl> + } <nl> wc . sec = boot . tv_sec ; <nl> wc . nsec = boot . tv_nsec ; <nl> wc . version = version ;
static netdev_tx_t ipip6_tunnel_xmit ( struct sk_buff * skb , <nl> if ( tunnel -> parms . iph . daddr && skb_dst ( skb )) <nl> skb_dst ( skb )-> ops -> update_pmtu ( skb_dst ( skb ), NULL , skb , mtu ); <nl>  <nl> - if ( skb -> len > mtu ) { <nl> + if ( skb -> len > mtu && ! skb_is_gso ( skb )) { <nl> icmpv6_send ( skb , ICMPV6_PKT_TOOBIG , 0 , mtu ); <nl> ip_rt_put ( rt ); <nl> goto tx_error ;
static void pty_close ( struct tty_struct * tty , struct file * filp ) <nl> mutex_unlock (& devpts_mutex ); <nl> } <nl> # endif <nl> + tty_unlock ( tty ); <nl> tty_vhangup ( tty -> link ); <nl> + tty_lock ( tty ); <nl> } <nl> } <nl> 
static int eql_g_master_cfg ( struct net_device * dev , master_config_t __user * mcp ) <nl> equalizer_t * eql ; <nl> master_config_t mc ; <nl>  <nl> + memset (& mc , 0 , sizeof ( master_config_t )); <nl> + <nl> if ( eql_is_master ( dev )) { <nl> eql = netdev_priv ( dev ); <nl> mc . max_slaves = eql -> max_slaves ;
static int translate_desc ( struct vhost_dev * dev , u64 addr , u32 len , <nl> } <nl> _iov = iov + ret ; <nl> size = reg -> memory_size - addr + reg -> guest_phys_addr ; <nl> - _iov -> iov_len = min (( u64 ) len , size ); <nl> + _iov -> iov_len = min (( u64 ) len - s , size ); <nl> _iov -> iov_base = ( void __user *)( unsigned long ) <nl> ( reg -> userspace_addr + addr - reg -> guest_phys_addr ); <nl> s += size ;
int x86_emulate_instruction ( struct kvm_vcpu * vcpu , <nl> if ( reexecute_instruction ( vcpu , cr2 , write_fault_to_spt , <nl> emulation_type )) <nl> return EMULATE_DONE ; <nl> + if ( ctxt -> have_exception && inject_emulated_exception ( vcpu )) <nl> + return EMULATE_DONE ; <nl> if ( emulation_type & EMULTYPE_SKIP ) <nl> return EMULATE_FAIL ; <nl> return handle_emulation_failure ( vcpu );
brcmf_cfg80211_start_ap ( struct wiphy * wiphy , struct net_device * ndev , <nl> ( u8 *)& settings -> beacon . head [ ie_offset ], <nl> settings -> beacon . head_len - ie_offset , <nl> WLAN_EID_SSID ); <nl> - if (! ssid_ie ) <nl> + if (! ssid_ie || ssid_ie -> len > IEEE80211_MAX_SSID_LEN ) <nl> return - EINVAL ; <nl>  <nl> memcpy ( ssid_le . SSID , ssid_ie -> data , ssid_ie -> len );
static int tipc_connect ( struct socket * sock , struct sockaddr * dest , <nl> if ( dst -> family == AF_UNSPEC ) { <nl> memset (& tsk -> remote , 0 , sizeof ( struct sockaddr_tipc )); <nl> tsk -> connected = 0 ; <nl> + } else if ( destlen != sizeof ( struct sockaddr_tipc )) { <nl> + res = - EINVAL ; <nl> } else { <nl> memcpy (& tsk -> remote , dest , destlen ); <nl> tsk -> connected = 1 ;
mountpoint_last ( struct nameidata * nd , struct path * path ) <nl> goto out ; <nl> } <nl> path -> dentry = dentry ; <nl> - path -> mnt = mntget ( nd -> path . mnt ); <nl> + path -> mnt = nd -> path . mnt ; <nl> if ( should_follow_link ( dentry , nd -> flags & LOOKUP_FOLLOW )) <nl> return 1 ; <nl> + mntget ( path -> mnt ); <nl> follow_mount ( path ); <nl> error = 0 ; <nl> out :
static int exofs_read_lookup_dev_table ( struct exofs_sb_info ** psbi , <nl> } <nl>  <nl> od = osduld_info_lookup (& odi ); <nl> - if ( unlikely ( IS_ERR ( od ))) { <nl> + if ( IS_ERR ( od )) { <nl> ret = PTR_ERR ( od ); <nl> EXOFS_ERR (" ERROR : device requested is not found " <nl> " osd_name -% s =>% d \ n ", odi . osdname , ret );
static int fs_enet_rx_napi ( struct napi_struct * napi , int budget ) <nl> u16 pkt_len , sc ; <nl> int curidx ; <nl>  <nl> + if ( budget <= 0 ) <nl> + return received ; <nl> + <nl> /* <nl> * First , grab all of the stats for the incoming packet . <nl> * These get messed up if we get called due to a busy condition .
void msm_gem_free_object ( struct drm_gem_object * obj ) <nl> if ( msm_obj -> pages ) <nl> drm_free_large ( msm_obj -> pages ); <nl>  <nl> + drm_prime_gem_destroy ( obj , msm_obj -> sgt ); <nl> } else { <nl> vunmap ( msm_obj -> vaddr ); <nl> put_pages ( obj );
static int uas_slave_configure ( struct scsi_device * sdev ) <nl> if ( devinfo -> flags & US_FL_BROKEN_FUA ) <nl> sdev -> broken_fua = 1 ; <nl>  <nl> + scsi_change_queue_depth ( sdev , devinfo -> qdepth - 2 ); <nl> return 0 ; <nl> } <nl> 
static long kvm_dev_ioctl ( struct file * filp , <nl> num_msrs_to_save * sizeof ( u32 ))) <nl> goto out ; <nl> r = 0 ; <nl> + break ; <nl> } <nl> default : <nl> ;
asmlinkage void math_emulate ( long arg ) <nl> entry_sel_off . offset = FPU_ORIG_EIP ; <nl> entry_sel_off . selector = FPU_CS ; <nl> entry_sel_off . opcode = ( byte1 << 8 ) | FPU_modrm ; <nl> + entry_sel_off . empty = 0 ; <nl>  <nl> FPU_rm = FPU_modrm & 7 ; <nl> 
void scatterwalk_map_and_copy ( void * buf , struct scatterlist * sg , <nl> struct scatter_walk walk ; <nl> unsigned int offset = 0 ; <nl>  <nl> + if (! nbytes ) <nl> + return ; <nl> + <nl> for (;;) { <nl> scatterwalk_start (& walk , sg ); <nl> 
static void __init tegra210_pll_init ( void __iomem * clk_base , <nl>  <nl> /* PLLU_VCO */ <nl> val = readl ( clk_base + pll_u_vco_params . base_reg ); <nl> - val &= ~ BIT ( 24 ); /* disable PLLU_OVERRIDE */ <nl> + val &= ~ PLLU_BASE_OVERRIDE ; /* disable PLLU_OVERRIDE */ <nl> writel ( val , clk_base + pll_u_vco_params . base_reg ); <nl>  <nl> clk = tegra_clk_register_pllre (" pll_u_vco ", " pll_ref ", clk_base , pmc ,
nfs4_preprocess_seqid_op ( struct svc_fh * current_fh , u32 seqid , stateid_t * statei <nl> printk (" NFSD : preprocess_seqid_op : old stateid !\ n "); <nl> goto out ; <nl> } <nl> - /* XXX renew the client lease here */ <nl> + renew_client ( sop -> so_client ); <nl> status = nfs_ok ; <nl>  <nl> out :
tda998x_encoder_init ( struct i2c_client * client , <nl>  <nl> priv -> current_page = 0xff ; <nl> priv -> cec = i2c_new_dummy ( client -> adapter , 0x34 ); <nl> - if (! priv -> cec ) <nl> + if (! priv -> cec ) { <nl> + kfree ( priv ); <nl> return - ENODEV ; <nl> + } <nl> priv -> dpms = DRM_MODE_DPMS_OFF ; <nl>  <nl> encoder_slave -> slave_priv = priv ;
static void apc_agent_timeout ( unsigned long data ) <nl> configure_phy_mask = ~ port_agent -> phy_configured_mask & port_agent -> phy_ready_mask ; <nl>  <nl> if (! configure_phy_mask ) <nl> - return ; <nl> + goto done ; <nl>  <nl> for ( index = 0 ; index < SCI_MAX_PHYS ; index ++) { <nl> if (( configure_phy_mask & ( 1 << index )) == 0 )
struct pci_bus * pci_acpi_scan_root ( struct acpi_pci_root * root ) <nl> return NULL ; <nl>  <nl> root_ops = kzalloc_node ( sizeof (* root_ops ), GFP_KERNEL , node ); <nl> - if (! root_ops ) <nl> + if (! root_ops ) { <nl> + kfree ( ri ); <nl> return NULL ; <nl> + } <nl>  <nl> ri -> cfg = pci_acpi_setup_ecam_mapping ( root ); <nl> if (! ri -> cfg ) {
xfs_iext_remove_node ( <nl> node -> ptrs [ nr_entries ] = NULL ; <nl>  <nl> if ( pos == 0 && nr_entries > 0 ) { <nl> - xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , <nl> - node ); <nl> + xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , node ); <nl> offset = node -> keys [ 0 ]; <nl> } <nl> 
static void tg3_skb_error_unmap ( struct tg3_napi * tnapi , <nl> dma_unmap_addr ( txb , mapping ), <nl> skb_headlen ( skb ), <nl> PCI_DMA_TODEVICE ); <nl> - for ( i = 0 ; i <= last ; i ++) { <nl> + for ( i = 0 ; i < last ; i ++) { <nl> skb_frag_t * frag = & skb_shinfo ( skb )-> frags [ i ]; <nl>  <nl> entry = NEXT_TX ( entry );
static int safexcel_probe ( struct platform_device * pdev ) <nl> snprintf ( irq_name , 6 , " ring % d ", i ); <nl> irq = safexcel_request_ring_irq ( pdev , irq_name , safexcel_irq_ring , <nl> ring_irq ); <nl> - <nl> - if ( irq < 0 ) <nl> + if ( irq < 0 ) { <nl> + ret = irq ; <nl> goto err_clk ; <nl> + } <nl>  <nl> priv -> ring [ i ]. work_data . priv = priv ; <nl> priv -> ring [ i ]. work_data . ring = i ;
static u32 apic_get_tmcct ( struct kvm_lapic * apic ) <nl> ASSERT ( apic != NULL ); <nl>  <nl> /* if initial count is 0 , current count should also be 0 */ <nl> - if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 ) <nl> + if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 || <nl> + apic -> lapic_timer . period == 0 ) <nl> return 0 ; <nl>  <nl> remaining = hrtimer_get_remaining (& apic -> lapic_timer . timer );
bool wil_fw_verify_file_exists ( struct wil6210_priv * wil , const char * name ) <nl> rc = request_firmware (& fw , name , wil_to_dev ( wil )); <nl> if (! rc ) <nl> release_firmware ( fw ); <nl> - return rc != - ENOENT ; <nl> + else <nl> + wil_dbg_fw ( wil , "<% s > not available : % d \ n ", name , rc ); <nl> + return ! rc ; <nl> }
static inline void tcp_check_send_head ( struct sock * sk , struct sk_buff * skb_unli <nl> { <nl> if ( sk -> sk_send_head == skb_unlinked ) <nl> sk -> sk_send_head = NULL ; <nl> + if ( tcp_sk ( sk )-> highest_sack == skb_unlinked ) <nl> + tcp_sk ( sk )-> highest_sack = NULL ; <nl> } <nl>  <nl> static inline void tcp_init_send_head ( struct sock * sk )
static void bdw_load_gamma_lut ( struct drm_crtc_state * state , u32 offset ) <nl> } <nl>  <nl> /* Program the max register to clamp values > 1 . 0 . */ <nl> + i = lut_size - 1 ; <nl> I915_WRITE ( PREC_PAL_GC_MAX ( pipe , 0 ), <nl> drm_color_lut_extract ( lut [ i ]. red , 16 )); <nl> I915_WRITE ( PREC_PAL_GC_MAX ( pipe , 1 ),
static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
static long ppc_set_hwdebug ( struct task_struct * child , <nl>  <nl> brk . address = bp_info -> addr & ~ 7UL ; <nl> brk . type = HW_BRK_TYPE_TRANSLATE ; <nl> + brk . len = 8 ; <nl> if ( bp_info -> trigger_type & PPC_BREAKPOINT_TRIGGER_READ ) <nl> brk . type |= HW_BRK_TYPE_READ ; <nl> if ( bp_info -> trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE )
static int coda_try_fmt ( struct coda_ctx * ctx , struct coda_codec * codec , <nl> BUG (); <nl> } <nl>  <nl> + f -> fmt . pix . priv = 0 ; <nl> + <nl> return 0 ; <nl> } <nl> 
static void etb_update_buffer ( struct coresight_device * csdev , <nl>  <nl> capacity = drvdata -> buffer_depth * ETB_FRAME_SIZE_WORDS ; <nl>  <nl> - CS_UNLOCK ( drvdata -> base ); <nl> etb_disable_hw ( drvdata ); <nl> + CS_UNLOCK ( drvdata -> base ); <nl>  <nl> /* unit is in words , not bytes */ <nl> read_ptr = readl_relaxed ( drvdata -> base + ETB_RAM_READ_POINTER );
static int check_hw_params_convention ( struct snd_usb_substream * subs ) <nl>  <nl> channels = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> rates = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> + if (! channels || ! rates ) <nl> + goto __out ; <nl>  <nl> list_for_each ( p , & subs -> fmt_list ) { <nl> struct audioformat * f ;
static int create_in_format_blob ( struct drm_device * dev , struct drm_plane * plane <nl> plane -> format_types [ j ], <nl> plane -> modifiers [ i ])) { <nl>  <nl> - mod -> formats |= 1 << j ; <nl> + mod -> formats |= 1ULL << j ; <nl> } <nl> } <nl> 
static int gbcodec_trigger ( struct snd_pcm_substream * substream , int cmd , <nl> dev_err ( dai -> dev , "% d : Error during % s stream \ n ", ret , <nl> start ? " Start " : " Stop "); <nl>  <nl> + /* in case device removed , return 0 for stop trigger */ <nl> + if ( stop && ( ret == - ENODEV )) <nl> + ret = 0 ; <nl> + <nl> func_exit : <nl> mutex_unlock (& gb -> lock ); <nl> return ret ;
static u32 __init allocate_aperture ( void ) <nl> printk (" Cannot allocate aperture memory hole (% p ,% uK )\ n ", <nl> p , aper_size >> 10 ); <nl> if ( p ) <nl> - free_bootmem_node ( nd0 , ( unsigned long ) p , aper_size ); <nl> + free_bootmem_node ( nd0 , __pa ( p ), aper_size ); <nl> return 0 ; <nl> } <nl> printk (" Mapping aperture over % d KB of RAM @ % lx \ n ",
+// SPDX - License - Identifier : GPL - 2 . 0 <nl> /* <nl> * Copyright ( C ) Maxime Coquelin 2015 <nl> + * Copyright ( C ) STMicroelectronics 2017 <nl> * Author : Maxime Coquelin < mcoquelin . stm32 @ gmail . com > <nl> - * License terms : GNU General Public License ( GPL ), version 2 <nl> */ <nl>  <nl> # include < linux / kernel . h >
int batadv_recv_unicast_packet ( struct sk_buff * skb , <nl> batadv_dbg ( BATADV_DBG_BLA , bat_priv , <nl> " recv_unicast_packet (): Dropped unicast pkt received from another backbone gw % pM .\ n ", <nl> orig_addr_gw ); <nl> - return NET_RX_DROP ; <nl> + goto free_skb ; <nl> } <nl> } <nl> 
EXPORT_SYMBOL_GPL ( crypto_givcipher_type ); <nl>  <nl> const char * crypto_default_geniv ( const struct crypto_alg * alg ) <nl> { <nl> + if ((( alg -> cra_flags & CRYPTO_ALG_TYPE_MASK ) == <nl> + CRYPTO_ALG_TYPE_BLKCIPHER ? alg -> cra_blkcipher . ivsize : <nl> + alg -> cra_ablkcipher . ivsize ) != <nl> + alg -> cra_blocksize ) <nl> + return " chainiv "; <nl> + <nl> return alg -> cra_flags & CRYPTO_ALG_ASYNC ? <nl> " eseqiv " : skcipher_default_geniv ; <nl> }
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> tnl_hlen = skb_tnl_header_len ( skb ); <nl> - if ( skb_headroom ( skb ) < ( tnl_hlen + frag_hdr_sz )) { <nl> + if ( skb -> mac_header < ( tnl_hlen + frag_hdr_sz )) { <nl> if ( gso_pskb_expand_head ( skb , tnl_hlen + frag_hdr_sz )) <nl> goto out ; <nl> }
static int bnxt_get_nvram_item ( struct net_device * dev , u32 index , u32 offset , <nl> dma_addr_t dma_handle ; <nl> struct hwrm_nvm_read_input req = { 0 }; <nl>  <nl> + if (! length ) <nl> + return - EINVAL ; <nl> + <nl> buf = dma_alloc_coherent (& bp -> pdev -> dev , length , & dma_handle , <nl> GFP_KERNEL ); <nl> if (! buf ) {
static __devexit int wm831x_power_remove ( struct platform_device * pdev ) <nl> power_supply_unregister (& wm831x_power -> battery ); <nl> power_supply_unregister (& wm831x_power -> wall ); <nl> power_supply_unregister (& wm831x_power -> usb ); <nl> + kfree ( wm831x_power ); <nl> return 0 ; <nl> } <nl> 
int virtio_gpu_object_create ( struct virtio_gpu_device * vgdev , <nl> return - ENOMEM ; <nl> size = roundup ( size , PAGE_SIZE ); <nl> ret = drm_gem_object_init ( vgdev -> ddev , & bo -> gem_base , size ); <nl> - if ( ret != 0 ) <nl> + if ( ret != 0 ) { <nl> + kfree ( bo ); <nl> return ret ; <nl> + } <nl> bo -> dumb = false ; <nl> virtio_gpu_init_ttm_placement ( bo , pinned ); <nl> 
brcmf_cfg80211_add_key ( struct wiphy * wiphy , struct net_device * ndev , <nl> if (! check_vif_up ( ifp -> vif )) <nl> return - EIO ; <nl>  <nl> - if ( mac_addr ) { <nl> + if ( mac_addr && <nl> + ( params -> cipher != WLAN_CIPHER_SUITE_WEP40 ) && <nl> + ( params -> cipher != WLAN_CIPHER_SUITE_WEP104 )) { <nl> brcmf_dbg ( TRACE , " Exit "); <nl> return brcmf_add_keyext ( wiphy , ndev , key_idx , mac_addr , params ); <nl> }
int i915_gem_freeze_late ( struct drm_i915_private * dev_priv ) <nl> */ <nl>  <nl> i915_gem_shrink ( dev_priv , - 1UL , I915_SHRINK_UNBOUND ); <nl> + i915_gem_drain_freed_objects ( dev_priv ); <nl>  <nl> mutex_lock (& dev_priv -> drm . struct_mutex ); <nl> for ( p = phases ; * p ; p ++) {
snd_nm256_mixer ( struct nm256 * chip ) <nl> . read = snd_nm256_ac97_read , <nl> }; <nl>  <nl> - chip -> ac97_regs = kcalloc ( sizeof ( short ), <nl> - ARRAY_SIZE ( nm256_ac97_init_val ), GFP_KERNEL ); <nl> + chip -> ac97_regs = kcalloc ( ARRAY_SIZE ( nm256_ac97_init_val ), <nl> + sizeof ( short ), GFP_KERNEL ); <nl> if (! chip -> ac97_regs ) <nl> return - ENOMEM ; <nl> 
int drm_atomic_helper_setup_commit ( struct drm_atomic_state * state , <nl> ! try_wait_for_completion (& old_plane_state -> commit -> flip_done )) <nl> return - EBUSY ; <nl>  <nl> - commit = crtc_or_fake_commit ( state , old_plane_state -> crtc ); <nl> + commit = crtc_or_fake_commit ( state , new_plane_state -> crtc ?: old_plane_state -> crtc ); <nl> if (! commit ) <nl> return - ENOMEM ; <nl> 
ath10k_wmi_10_4_gen_update_fw_tdls_state ( struct ath10k * ar , u32 vdev_id , <nl> if (! skb ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> - if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map )) <nl> + if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map ) && <nl> + state == WMI_TDLS_ENABLE_ACTIVE ) <nl> state = WMI_TDLS_ENABLE_PASSIVE ; <nl>  <nl> if ( test_bit ( WMI_SERVICE_TDLS_UAPSD_BUFFER_STA , ar -> wmi . svc_map ))
struct sched_entity { <nl>  <nl> struct sched_rt_entity { <nl> struct list_head run_list ; <nl> - unsigned int time_slice ; <nl> unsigned long timeout ; <nl> + unsigned int time_slice ; <nl> int nr_cpus_allowed ; <nl>  <nl> struct sched_rt_entity * back ;
static int max30102_probe ( struct i2c_client * client , <nl> dev_err (& client -> dev , " regmap initialization failed \ n "); <nl> return PTR_ERR ( data -> regmap ); <nl> } <nl> - max30102_set_powermode ( data , false ); <nl> + <nl> + ret = max30102_set_powermode ( data , false ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = max30102_chip_init ( data ); <nl> if ( ret )
cifs_put_tcp_session ( struct TCP_Server_Info * server , int from_reconnect ) <nl> server -> session_key . response = NULL ; <nl> server -> session_key . len = 0 ; <nl> kfree ( server -> hostname ); <nl> + server -> hostname = NULL ; <nl>  <nl> task = xchg (& server -> tsk , NULL ); <nl> if ( task )
static int parse_features ( struct dm_arg_set * as , struct flakey_c * fc , <nl> arg_name = dm_shift_arg ( as ); <nl> argc --; <nl>  <nl> + if (! arg_name ) { <nl> + ti -> error = " Insufficient feature arguments "; <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* <nl> * drop_writes <nl> */
int blk_init_allocated_queue ( struct request_queue * q ) <nl> q -> exit_rq_fn ( q , q -> fq -> flush_rq ); <nl> out_free_flush_queue : <nl> blk_free_flush_queue ( q -> fq ); <nl> + q -> fq = NULL ; <nl> return - ENOMEM ; <nl> } <nl> EXPORT_SYMBOL ( blk_init_allocated_queue );
void sctp_association_free ( struct sctp_association * asoc ) <nl> /* Only real associations count against the endpoint , so <nl> * don ' t bother for if this is a temporary association . <nl> */ <nl> - if (! asoc -> temp ) { <nl> + if (! list_empty (& asoc -> asocs )) { <nl> list_del (& asoc -> asocs ); <nl>  <nl> /* Decrement the backlog value for a TCP - style listening
static int visor_thread_start ( struct visor_thread_info * thrinfo , <nl> void * thrcontext , char * name ) <nl> { <nl> /* used to stop the thread */ <nl> - thrinfo -> task = kthread_run ( threadfn , thrcontext , name ); <nl> + thrinfo -> task = kthread_run ( threadfn , thrcontext , "% s ", name ); <nl> if ( IS_ERR ( thrinfo -> task )) { <nl> pr_debug ("% s failed (% ld )\ n ", <nl> __func__ , PTR_ERR ( thrinfo -> task ));
s32 fm10k_disable_queues_generic ( struct fm10k_hw * hw , u16 q_cnt ) <nl> /* clear tx_ready to prevent any false hits for reset */ <nl> hw -> mac . tx_ready = false ; <nl>  <nl> + if ( FM10K_REMOVED ( hw -> hw_addr )) <nl> + return 0 ; <nl> + <nl> /* clear the enable bit for all rings */ <nl> for ( i = 0 ; i < q_cnt ; i ++) { <nl> reg = fm10k_read_reg ( hw , FM10K_TXDCTL ( i ));
xfs_error_get_cfg ( <nl> { <nl> struct xfs_error_cfg * cfg ; <nl>  <nl> + if ( error < 0 ) <nl> + error = - error ; <nl> + <nl> switch ( error ) { <nl> case EIO : <nl> cfg = & mp -> m_error_cfg [ error_class ][ XFS_ERR_EIO ];
struct oz_pd * oz_pd_alloc ( const u8 * mac_addr ) <nl> /* <nl> * Context : softirq or process <nl> */ <nl> - void oz_pd_free ( struct work_struct * work ) <nl> + static void oz_pd_free ( struct work_struct * work ) <nl> { <nl> struct list_head * e ; <nl> struct oz_tx_frame * f ;
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> return - EBUSY ; <nl> } <nl>  <nl> + if ( bond_dev == slave_dev ) { <nl> + pr_err ("% s : cannot enslave bond to itself .\ n ", bond_dev -> name ); <nl> + return - EPERM ; <nl> + } <nl> + <nl> /* vlan challenged mutual exclusion */ <nl> /* no need to lock since we ' re protected by rtnl_lock */ <nl> if ( slave_dev -> features & NETIF_F_VLAN_CHALLENGED ) {
static struct xhci_container_ctx * xhci_alloc_container_ctx ( struct xhci_hcd * xhci <nl> ctx -> size += CTX_SIZE ( xhci -> hcc_params ); <nl>  <nl> ctx -> bytes = dma_pool_alloc ( xhci -> device_pool , flags , & ctx -> dma ); <nl> + if (! ctx -> bytes ) { <nl> + kfree ( ctx ); <nl> + return NULL ; <nl> + } <nl> memset ( ctx -> bytes , 0 , ctx -> size ); <nl> return ctx ; <nl> }
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> btrfs_wait_ordered_range ( src , off , len ); <nl> } <nl>  <nl> + /* truncate page cache pages from target inode range */ <nl> + truncate_inode_pages_range (& inode -> i_data , off , <nl> + ALIGN ( off + len , PAGE_CACHE_SIZE ) - 1 ); <nl> + <nl> /* clone data */ <nl> key . objectid = btrfs_ino ( src ); <nl> key . type = BTRFS_EXTENT_DATA_KEY ;
int cpuidle_enter_state ( struct cpuidle_device * dev , struct cpuidle_driver * drv , <nl>  <nl> time_end = ktime_get (); <nl>  <nl> - local_irq_enable (); <nl> + if (! cpuidle_state_is_coupled ( dev , drv , entered_state )) <nl> + local_irq_enable (); <nl>  <nl> diff = ktime_to_us ( ktime_sub ( time_end , time_start )); <nl> if ( diff > INT_MAX )
static int rtc_dev_ioctl ( struct inode * inode , struct file * file , <nl> break ; <nl>  <nl> case RTC_PIE_ON : <nl> - if (! capable ( CAP_SYS_RESOURCE )) <nl> + if ( rtc -> irq_freq > rtc -> max_user_freq && <nl> + ! capable ( CAP_SYS_RESOURCE )) <nl> return - EACCES ; <nl> break ; <nl> }
static void test_acipher_speed ( const char * algo , int enc , unsigned int secs , <nl> goto out_free_req ; <nl> } <nl>  <nl> - sg_init_table ( sg , TVMEMSIZE ); <nl> - <nl> k = * keysize + * b_size ; <nl> + sg_init_table ( sg , DIV_ROUND_UP ( k , PAGE_SIZE )); <nl> + <nl> if ( k > PAGE_SIZE ) { <nl> sg_set_buf ( sg , tvmem [ 0 ] + * keysize , <nl> PAGE_SIZE - * keysize );
static bool fib6_rule_suppress ( struct fib_rule * rule , struct fib_lookup_arg * arg <nl> return false ; <nl>  <nl> suppress_route : <nl> - ip6_rt_put ( rt ); <nl> + if (!( arg -> flags & FIB_LOOKUP_NOREF )) <nl> + ip6_rt_put ( rt ); <nl> return true ; <nl> } <nl> 
static int deprecated_sysctl_warning ( struct __sysctl_args * args ) <nl> int name [ CTL_MAXNAME ]; <nl> int i ; <nl>  <nl> + /* Check args -> nlen . */ <nl> + if ( args -> nlen < 0 || args -> nlen > CTL_MAXNAME ) <nl> + return - ENOTDIR ; <nl> + <nl> /* Read in the sysctl name for better debug message logging */ <nl> for ( i = 0 ; i < args -> nlen ; i ++) <nl> if ( get_user ( name [ i ], args -> name + i ))
static struct domain_device * sas_ex_discover_expander ( <nl>  <nl> res = sas_discover_expander ( child ); <nl> if ( res ) { <nl> + spin_lock_irq (& parent -> port -> dev_list_lock ); <nl> + list_del (& child -> dev_list_node ); <nl> + spin_unlock_irq (& parent -> port -> dev_list_lock ); <nl> kfree ( child ); <nl> return NULL ; <nl> }
extern long __put_user_asm_l ( void *, long ); <nl> extern long __put_user_asm_q ( void *, long ); <nl> extern void __put_user_unknown ( void ); <nl>  <nl> + extern long __strnlen_user ( const char * __s , long __n ); <nl> + <nl> # endif /* __ASM_SH_UACCESS_64_H */
static void __init sun6i_rtc_clk_init ( struct device_node * node ) <nl>  <nl> clk_data = kzalloc ( sizeof (* clk_data ) + ( sizeof (* clk_data -> hws ) * 2 ), <nl> GFP_KERNEL ); <nl> - if (! clk_data ) <nl> + if (! clk_data ) { <nl> + kfree ( rtc ); <nl> return ; <nl> + } <nl>  <nl> spin_lock_init (& rtc -> lock ); <nl> 
static void zfcp_fc_adisc_handler ( unsigned long data ) <nl> struct zfcp_port * port = adisc -> els . port ; <nl> struct zfcp_ls_adisc_acc * ls_adisc = & adisc -> ls_adisc_acc ; <nl>  <nl> - if (! adisc -> els . status ) { <nl> + if ( adisc -> els . status ) { <nl> /* request rejected or timed out */ <nl> zfcp_erp_port_forced_reopen ( port , 0 , 63 , NULL ); <nl> goto out ;
int journal_create ( journal_t * journal ) <nl> if ( err ) <nl> return err ; <nl> bh = __getblk ( journal -> j_dev , blocknr , journal -> j_blocksize ); <nl> + if ( unlikely (! bh )) <nl> + return - ENOMEM ; <nl> lock_buffer ( bh ); <nl> memset ( bh -> b_data , 0 , journal -> j_blocksize ); <nl> BUFFER_TRACE ( bh , " marking dirty ");
static int sti_cpufreq_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if ((! of_machine_is_compatible (" st , stih407 ")) && <nl> + (! of_machine_is_compatible (" st , stih410 "))) <nl> + return - ENODEV ; <nl> + <nl> ddata . cpu = get_cpu_device ( 0 ); <nl> if (! ddata . cpu ) { <nl> dev_err ( ddata . cpu , " Failed to get device for CPU0 \ n ");
static void pch_gpio_setup ( struct pch_gpio * chip ) <nl> static int pch_irq_type ( struct irq_data * d , unsigned int type ) <nl> { <nl> u32 im ; <nl> - u32 * im_reg ; <nl> + u32 __iomem * im_reg ; <nl> u32 ien ; <nl> u32 im_pos ; <nl> int ch ;
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> __le16 fc ; <nl> struct ieee80211_hdr hdr ; <nl> struct ieee80211s_hdr mesh_hdr __maybe_unused ; <nl> - struct mesh_path * mppath = NULL ; <nl> + struct mesh_path __maybe_unused * mppath = NULL ; <nl> const u8 * encaps_data ; <nl> int encaps_len , skip_header_bytes ; <nl> int nh_pos , h_pos ;
static struct glink_channel * qcom_glink_alloc_channel ( struct qcom_glink * glink , <nl>  <nl> init_completion (& channel -> open_req ); <nl> init_completion (& channel -> open_ack ); <nl> + init_completion (& channel -> intent_req_comp ); <nl>  <nl> INIT_LIST_HEAD (& channel -> done_intents ); <nl> INIT_WORK (& channel -> intent_work , qcom_glink_rx_done_work );
static int intel_pt_walk_to_ip ( struct intel_pt_decoder * decoder ) <nl> break ; <nl>  <nl> case INTEL_PT_PSB : <nl> + intel_pt_clear_stack (& decoder -> stack ); <nl> err = intel_pt_walk_psb ( decoder ); <nl> if ( err ) <nl> return err ;
subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> err = filter_add_subsystem_pred ( system , pred ); <nl> if ( err < 0 ) { <nl> - filter_free_subsystem_preds ( system ); <nl> filter_free_pred ( pred ); <nl> return err ; <nl> }
static inline int __is_running ( const struct exynos_mipi_phy_desc * data , <nl> struct exynos_mipi_video_phy * state ) <nl> { <nl> u32 val ; <nl> + int ret ; <nl> + <nl> + ret = regmap_read ( state -> regmaps [ data -> resetn_map ], data -> resetn_reg , & val ); <nl> + if ( ret ) <nl> + return 0 ; <nl>  <nl> - regmap_read ( state -> regmaps [ data -> resetn_map ], data -> resetn_reg , & val ); <nl> return val & data -> resetn_val ; <nl> } <nl> 
static int macronix_quad_enable ( struct spi_nor * nor ) <nl> val = read_sr ( nor ); <nl> if ( val < 0 ) <nl> return val ; <nl> + if ( val & SR_QUAD_EN_MX ) <nl> + return 0 ; <nl> + <nl> write_enable ( nor ); <nl>  <nl> write_sr ( nor , val | SR_QUAD_EN_MX );
static int sierra_resume ( struct usb_serial * serial ) <nl> if ( err < 0 ) { <nl> intfdata -> in_flight --; <nl> usb_unanchor_urb ( urb ); <nl> - usb_scuttle_anchored_urbs (& portdata -> delayed ); <nl> - break ; <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> + spin_lock (& portdata -> lock ); <nl> + portdata -> outstanding_urbs --; <nl> + spin_unlock (& portdata -> lock ); <nl> + continue ; <nl> } <nl> } <nl> 
static int dm_update_crtcs_state ( struct dc * dc , <nl> } <nl> } <nl>  <nl> - if ( dc_is_stream_unchanged ( new_stream , dm_old_crtc_state -> stream ) && <nl> + if ( enable && dc_is_stream_unchanged ( new_stream , dm_old_crtc_state -> stream ) && <nl> dc_is_stream_scaling_unchanged ( new_stream , dm_old_crtc_state -> stream )) { <nl>  <nl> new_crtc_state -> mode_changed = false ;
int i915_gem_init_stolen ( struct drm_i915_private * dev_priv ) <nl>  <nl> mutex_init (& dev_priv -> mm . stolen_lock ); <nl>  <nl> + if ( intel_vgpu_active ( dev_priv )) { <nl> + DRM_INFO (" iGVT - g active , disabling use of stolen memory \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> # ifdef CONFIG_INTEL_IOMMU <nl> if ( intel_iommu_gfx_mapped && INTEL_GEN ( dev_priv ) < 8 ) { <nl> DRM_INFO (" DMAR active , disabling use of stolen memory \ n ");
void handle_ra_miss ( struct address_space * mapping , <nl> { <nl> ra -> flags |= RA_FLAG_MISS ; <nl> ra -> flags &= ~ RA_FLAG_INCACHE ; <nl> + ra -> cache_hit = 0 ; <nl> } <nl>  <nl> /*
# define DRIVER_AUTHOR " Maksim Salau < maksim . salau @ gmail . com >" <nl>  <nl> static const struct usb_device_id id_table [] = { <nl> - { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x0409 , 0x0063 ) }, /* V850ESJX3 - STICK */ <nl> + { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x064B , 0x7825 ) }, /* Analog Devices EVAL - ADXL362Z - DB */ <nl> {} <nl> };
static int mfd_add_device ( struct platform_device * parent , <nl> if ( ret ) <nl> goto fail_device ; <nl>  <nl> - memzero ( res , sizeof ( res )); <nl> + memset ( res , 0 , sizeof ( res )); <nl> for ( r = 0 ; r < cell -> num_resources ; r ++) { <nl> res [ r ]. name = cell -> resources [ r ]. name ; <nl> res [ r ]. flags = cell -> resources [ r ]. flags ;
static int cuse_channel_release ( struct inode * inode , struct file * file ) <nl> unregister_chrdev_region ( cc -> cdev -> dev , 1 ); <nl> cdev_del ( cc -> cdev ); <nl> } <nl> + /* Base reference is now owned by " fud " */ <nl> + fuse_conn_put (& cc -> fc ); <nl>  <nl> rc = fuse_dev_release ( inode , file ); /* puts the base reference */ <nl> 
struct sta_info * ieee80211_ibss_add_sta ( struct ieee80211_sub_if_data * sdata , <nl> if (! sta ) <nl> return NULL ; <nl>  <nl> + sta -> last_rx = jiffies ; <nl> set_sta_flags ( sta , WLAN_STA_AUTHORIZED ); <nl>  <nl> /* make sure mandatory rates are always added */
static int fsi_resume ( struct device * dev ) <nl> return 0 ; <nl> } <nl>  <nl> - static struct dev_pm_ops fsi_pm_ops = { <nl> + static const struct dev_pm_ops fsi_pm_ops = { <nl> . suspend = fsi_suspend , <nl> . resume = fsi_resume , <nl> };
static int spi_map_buf ( struct spi_master * master , struct device * dev , <nl> } <nl>  <nl> ret = dma_map_sg ( dev , sgt -> sgl , sgt -> nents , dir ); <nl> + if (! ret ) <nl> + ret = - ENOMEM ; <nl> if ( ret < 0 ) { <nl> sg_free_table ( sgt ); <nl> return ret ;
static int ext4_ext_journal_restart ( handle_t * handle , int needed ) <nl> if ( handle -> h_buffer_credits > needed ) <nl> return 0 ; <nl> err = ext4_journal_extend ( handle , needed ); <nl> - if ( err ) <nl> + if ( err <= 0 ) <nl> return err ; <nl> return ext4_journal_restart ( handle , needed ); <nl> }
static int btrfs_add_system_chunk ( struct btrfs_root * root , <nl> u8 * ptr ; <nl>  <nl> array_size = btrfs_super_sys_array_size ( super_copy ); <nl> - if ( array_size + item_size > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> + if ( array_size + item_size + sizeof ( disk_key ) <nl> + > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> return - EFBIG ; <nl>  <nl> ptr = super_copy -> sys_chunk_array + array_size ;
struct extent_map * btrfs_get_extent ( struct inode * inode , struct page * page , <nl> goto not_found ; <nl> if ( start + len <= found_key . offset ) <nl> goto not_found ; <nl> + if ( start > found_key . offset ) <nl> + goto next ; <nl> em -> start = start ; <nl> em -> orig_start = start ; <nl> em -> len = found_key . offset - start ;
static ssize_t iwl_dbgfs_fw_rx_stats_read ( struct file * file , <nl>  <nl> mutex_lock (& mvm -> mutex ); <nl>  <nl> + if ( iwl_mvm_firmware_running ( mvm )) <nl> + iwl_mvm_request_statistics ( mvm , false ); <nl> + <nl> pos += scnprintf ( buf + pos , bufsz - pos , fmt_header , <nl> " Statistics_Rx - OFDM "); <nl> if (! iwl_mvm_has_new_rx_stats_api ( mvm )) {
static struct drm_driver tegra_drm_driver = { <nl> . debugfs_cleanup = tegra_debugfs_cleanup , <nl> # endif <nl>  <nl> - . gem_free_object = tegra_bo_free_object , <nl> + . gem_free_object_unlocked = tegra_bo_free_object , <nl> . gem_vm_ops = & tegra_bo_vm_ops , <nl>  <nl> . prime_handle_to_fd = drm_gem_prime_handle_to_fd ,
enum { none , prepare , done , } __init_state ; <nl> static void init_preload ( void ); <nl> static void try_init_preload ( void ) <nl> { <nl> - if (! __init_state != done ) <nl> + if ( __init_state != done ) <nl> init_preload (); <nl> } <nl> 
static int xfrm6_tunnel_rcv ( struct sk_buff * skb ) <nl> __be32 spi ; <nl>  <nl> spi = xfrm6_tunnel_spi_lookup (( xfrm_address_t *)& iph -> saddr ); <nl> - return xfrm6_rcv_spi ( skb , spi ); <nl> + return xfrm6_rcv_spi ( skb , spi ) > 0 ? : 0 ; <nl> } <nl>  <nl> static int xfrm6_tunnel_err ( struct sk_buff * skb , struct inet6_skb_parm * opt ,
int snd_hda_multi_out_analog_open ( struct hda_codec * codec , <nl> if ( mout -> spdif_maxbps < hinfo -> maxbps ) <nl> hinfo -> maxbps = mout -> spdif_maxbps ; <nl> } <nl> + mutex_unlock (& codec -> spdif_mutex ); <nl> } <nl> - mutex_unlock (& codec -> spdif_mutex ); <nl> return snd_pcm_hw_constraint_step ( substream -> runtime , 0 , <nl> SNDRV_PCM_HW_PARAM_CHANNELS , 2 ); <nl> }
static int futex_requeue ( u32 __user * uaddr1 , unsigned int flags , <nl> struct futex_q * this , * next ; <nl> DEFINE_WAKE_Q ( wake_q ); <nl>  <nl> + if ( nr_wake < 0 || nr_requeue < 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * When PI not supported : return - ENOSYS if requeue_pi is true , <nl> * consequently the compiler knows requeue_pi is always false past
static int snd_soc_dapm_set_bias_level ( struct snd_soc_dapm_context * dapm , <nl> if ( dapm -> codec -> driver -> set_bias_level ) <nl> ret = dapm -> codec -> driver -> set_bias_level ( dapm -> codec , <nl> level ); <nl> - else <nl> - dapm -> bias_level = level ; <nl> - } <nl> + } else <nl> + dapm -> bias_level = level ; <nl> + <nl> if ( ret != 0 ) <nl> goto out ; <nl> 
static int ipw_load ( struct ipw_priv * priv ) <nl> ipw_rx_queue_reset ( priv , priv -> rxq ); <nl> if (! priv -> rxq ) { <nl> IPW_ERROR (" Unable to initialize Rx queue \ n "); <nl> + rc = - ENOMEM ; <nl> goto error ; <nl> } <nl> 
static void __init kvm_guest_init ( void ) <nl> } <nl>  <nl> if ( kvm_para_has_feature ( KVM_FEATURE_PV_TLB_FLUSH ) && <nl> + ! kvm_para_has_hint ( KVM_HINTS_DEDICATED ) && <nl> ! kvm_para_has_feature ( KVM_FEATURE_STEAL_TIME )) <nl> pv_mmu_ops . flush_tlb_others = kvm_flush_tlb_others ; <nl>  <nl> static __init int kvm_setup_pv_tlb_flush ( void ) <nl> int cpu ; <nl>  <nl> if ( kvm_para_has_feature ( KVM_FEATURE_PV_TLB_FLUSH ) && <nl> + ! kvm_para_has_hint ( KVM_HINTS_DEDICATED ) && <nl> ! kvm_para_has_feature ( KVM_FEATURE_STEAL_TIME )) { <nl> for_each_possible_cpu ( cpu ) { <nl> zalloc_cpumask_var_node ( per_cpu_ptr (& __pv_tlb_mask , cpu ),
static int adv7511_get_modes ( struct adv7511 * adv7511 , <nl> adv7511_set_config_csc ( adv7511 , connector , adv7511 -> rgb , <nl> drm_detect_hdmi_monitor ( edid )); <nl>  <nl> - kfree ( edid ); <nl> - <nl> cec_s_phys_addr_from_edid ( adv7511 -> cec_adap , edid ); <nl>  <nl> + kfree ( edid ); <nl> + <nl> return count ; <nl> } <nl> 
static int rockchip_i2s_remove ( struct platform_device * pdev ) <nl> if (! pm_runtime_status_suspended (& pdev -> dev )) <nl> i2s_runtime_suspend (& pdev -> dev ); <nl>  <nl> - clk_disable_unprepare ( i2s -> mclk ); <nl> clk_disable_unprepare ( i2s -> hclk ); <nl>  <nl> return 0 ;
static void * alloc_qos_entry ( void ) <nl> } <nl> spin_unlock_irqrestore (& qos_free_list . lock , flags ); <nl>  <nl> - entry = kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> - return entry ; <nl> + return kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> } <nl>  <nl> static void free_qos_entry ( void * entry )
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> + . mmu = gf100_mmu_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new , <nl> . top = gk104_top_new ,
static int s3c2410_udc_ep_enable ( struct usb_ep * _ep , <nl>  <nl> ep = to_s3c2410_ep ( _ep ); <nl>  <nl> - if (! _ep || ! desc || ep -> ep . desc <nl> + if (! _ep || ! desc <nl> || _ep -> name == ep0name <nl> || desc -> bDescriptorType != USB_DT_ENDPOINT ) <nl> return - EINVAL ;
int ll_fid2path ( struct inode * inode , void __user * arg ) <nl> if ( get_user ( pathlen , & gfin -> gf_pathlen )) <nl> return - EFAULT ; <nl>  <nl> + if ( pathlen > PATH_MAX ) <nl> + return - EINVAL ; <nl> + <nl> outsize = sizeof (* gfout ) + pathlen ; <nl>  <nl> OBD_ALLOC ( gfout , outsize );
static int cxgb4vf_open ( struct net_device * dev ) <nl> if ( err ) <nl> return err ; <nl> set_bit ( pi -> port_id , & adapter -> open_device_map ); <nl> - link_start ( dev ); <nl> + err = link_start ( dev ); <nl> + if ( err ) <nl> + return err ; <nl> netif_tx_start_all_queues ( dev ); <nl> return 0 ; <nl> }
static struct platform_driver mxc_w1_driver = { <nl> . name = " mxc_w1 ", <nl> }, <nl> . probe = mxc_w1_probe , <nl> - . remove = __devexit_p ( mxc_w1_remove ), <nl> + . remove = mxc_w1_remove , <nl> }; <nl> module_platform_driver ( mxc_w1_driver ); <nl> 
static void clear_huge_page ( struct page * page , <nl> { <nl> int i ; <nl>  <nl> - if ( unlikely ( sz > MAX_ORDER_NR_PAGES )) { <nl> + if ( unlikely ( sz / PAGE_SIZE > MAX_ORDER_NR_PAGES )) { <nl> clear_gigantic_page ( page , addr , sz ); <nl> return ; <nl> }
static int ina2xx_probe ( struct i2c_client * client , <nl> data -> config = & ina2xx_config [ data -> kind ]; <nl> data -> client = client ; <nl>  <nl> - if ( data -> rshunt <= 0 ) <nl> + if ( data -> rshunt <= 0 || <nl> + data -> rshunt > data -> config -> calibration_factor ) <nl> return - ENODEV ; <nl>  <nl> ret = ina2xx_init ( data );
static int acp_dma_hw_params ( struct snd_pcm_substream * substream , <nl> if ( WARN_ON (! rtd )) <nl> return - EINVAL ; <nl>  <nl> - rtd -> i2s_instance = pinfo -> i2s_instance ; <nl> + if ( pinfo ) <nl> + rtd -> i2s_instance = pinfo -> i2s_instance ; <nl> if ( adata -> asic_type == CHIP_STONEY ) { <nl> val = acp_reg_read ( adata -> acp_mmio , <nl> mmACP_I2S_16BIT_RESOLUTION_EN );
static int new_term ( struct parse_events_term ** _term , int type_val , <nl> term -> val . str = str ; <nl> break ; <nl> default : <nl> + free ( term ); <nl> return - EINVAL ; <nl> } <nl> 
nv134_chipset = { <nl> . top = gk104_top_new , <nl> . disp = gp104_disp_new , <nl> . dma = gf119_dma_new , <nl> + . fifo = gp100_fifo_new , <nl> }; <nl>  <nl> static int
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
void rxrpc_discard_prealloc ( struct rxrpc_sock * rx ) <nl> tail = b -> call_backlog_tail ; <nl> while ( CIRC_CNT ( head , tail , size ) > 0 ) { <nl> struct rxrpc_call * call = b -> call_backlog [ tail ]; <nl> + call -> socket = rx ; <nl> if ( rx -> discard_new_call ) { <nl> _debug (" discard % lx ", call -> user_call_ID ); <nl> rx -> discard_new_call ( call , call -> user_call_ID );
static void tmu_set_mode ( enum clock_event_mode mode , <nl> { <nl> switch ( mode ) { <nl> case CLOCK_EVT_MODE_PERIODIC : <nl> - ctrl_outl ( ctrl_inl ( TMU0_TCNT ), TMU0_TCOR ); <nl> + ctrl_outl ( tmu_latest_interval [ TMU0 ], TMU0_TCOR ); <nl> break ; <nl> case CLOCK_EVT_MODE_ONESHOT : <nl> ctrl_outl ( 0 , TMU0_TCOR );
static int dwc3_gadget_init_hw_endpoints ( struct dwc3 * dwc , <nl>  <nl> dep -> endpoint . name = dep -> name ; <nl>  <nl> + dev_vdbg ( dwc -> dev , " initializing % s \ n ", dep -> name ); <nl> + <nl> if ( epnum == 0 || epnum == 1 ) { <nl> dep -> endpoint . maxpacket = 512 ; <nl> dep -> endpoint . maxburst = 1 ;
static inline void * kvmalloc_array ( size_t n , size_t size , gfp_t flags ) <nl> return kvmalloc ( bytes , flags ); <nl> } <nl>  <nl> + static inline void * kvcalloc ( size_t n , size_t size , gfp_t flags ) <nl> +{ <nl> + return kvmalloc_array ( n , size , flags | __GFP_ZERO ); <nl> +} <nl> + <nl> extern void kvfree ( const void * addr ); <nl>  <nl> static inline atomic_t * compound_mapcount_ptr ( struct page * page )
static void __exit ams_delta_serio_exit ( void ) <nl> free_irq ( OMAP_GPIO_IRQ ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ), 0 ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_DATA ); <nl> - kfree ( ams_delta_serio ); <nl> } <nl> module_exit ( ams_delta_serio_exit );
static int btrfs_ioctl_setflags ( struct file * file , void __user * arg ) <nl> goto out_drop ; <nl>  <nl> } else { <nl> + ret = btrfs_set_prop ( inode , " btrfs . compression ", NULL , 0 , 0 ); <nl> + if ( ret && ret != - ENODATA ) <nl> + goto out_drop ; <nl> ip -> flags &= ~( BTRFS_INODE_COMPRESS | BTRFS_INODE_NOCOMPRESS ); <nl> } <nl> 
void snd_timer_interrupt ( struct snd_timer * timer , unsigned long ticks_left ) <nl> } else { <nl> ti -> flags &= ~ SNDRV_TIMER_IFLG_RUNNING ; <nl> if (-- timer -> running ) <nl> - list_del (& ti -> active_list ); <nl> + list_del_init (& ti -> active_list ); <nl> } <nl> if (( timer -> hw . flags & SNDRV_TIMER_HW_TASKLET ) || <nl> ( ti -> flags & SNDRV_TIMER_IFLG_FAST ))
static int ovl_copy_up_locked ( struct dentry * workdir , struct dentry * upperdir , <nl>  <nl> out_cleanup : <nl> ovl_cleanup ( wdir , newdentry ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> /*
int dvb_create_media_graph ( struct dvb_adapter * adap , <nl> return - ENOMEM ; <nl> adap -> conn = conn ; <nl>  <nl> - adap -> conn_pads = kcalloc ( 1 , sizeof (* adap -> conn_pads ), <nl> - GFP_KERNEL ); <nl> + adap -> conn_pads = kzalloc ( sizeof (* adap -> conn_pads ), GFP_KERNEL ); <nl> if (! adap -> conn_pads ) <nl> return - ENOMEM ; <nl> 
int vty_init ( const struct file_operations * console_fops ); <nl>  <nl> static inline bool vt_force_oops_output ( struct vc_data * vc ) <nl> { <nl> - if ( oops_in_progress && vc -> vc_panic_force_write ) <nl> + if ( oops_in_progress && vc -> vc_panic_force_write && panic_timeout >= 0 ) <nl> return true ; <nl> return false ; <nl> }
vnet_select_queue ( struct net_device * dev , struct sk_buff * skb , <nl> struct vnet * vp = netdev_priv ( dev ); <nl> struct vnet_port * port = __tx_port_find ( vp , skb ); <nl>  <nl> + if ( port == NULL ) <nl> + return 0 ; <nl> return port -> q_index ; <nl> } <nl> 
static const char * const mic_bias_level_text [] = { <nl> }; <nl>  <nl> static const struct soc_enum mic_bias_level_enum = <nl> - SOC_ENUM_SINGLE ( CS42L52_IFACE_CTL1 , 0 , <nl> + SOC_ENUM_SINGLE ( CS42L52_IFACE_CTL2 , 0 , <nl> ARRAY_SIZE ( mic_bias_level_text ), mic_bias_level_text ); <nl>  <nl> static const char * const cs42l52_mic_text [] = { " Single ", " Differential " };
struct thread_struct { <nl> . pgdir = swapper_pg_dir , \ <nl> } <nl>  <nl> -/* Do necessary setup to start up a newly executed thread . */ <nl> - void start_thread ( struct pt_regs * regs , <nl> - unsigned long pc , unsigned long usp ); <nl>  <nl> /* Free all resources held by a thread . */ <nl> extern inline void release_thread ( struct task_struct * dead_task )
static int omap_i2c_init ( struct omap_i2c_dev * dev ) <nl> * to get longer filter period for better noise suppression . <nl> * The filter is iclk ( fclk for HS ) period . <nl> */ <nl> - if ( dev -> speed > 400 || cpu_is_omap_2430 ()) <nl> + if ( dev -> speed > 400 || cpu_is_omap2430 ()) <nl> internal_clk = 19200 ; <nl> else if ( dev -> speed > 100 ) <nl> internal_clk = 9600 ;
static int ieee80211_update_mesh_config ( struct wiphy * wiphy , <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_TTL , mask )) <nl> conf -> dot11MeshTTL = nconf -> dot11MeshTTL ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_ELEMENT_TTL , mask )) <nl> - conf -> dot11MeshTTL = nconf -> element_ttl ; <nl> + conf -> element_ttl = nconf -> element_ttl ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_AUTO_OPEN_PLINKS , mask )) <nl> conf -> auto_open_plinks = nconf -> auto_open_plinks ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_SYNC_OFFSET_MAX_NEIGHBOR , mask ))
typedef struct xfs_attr_list_context { <nl> struct attrlist_cursor_kern * cursor ; /* position in list */ <nl> char * alist ; /* output buffer */ <nl> int seen_enough ; /* T / F : seen enough of list ? */ <nl> - int count ; /* num used entries */ <nl> + ssize_t count ; /* num used entries */ <nl> int dupcnt ; /* count dup hashvals seen */ <nl> int bufsize ; /* total buffer size */ <nl> int firstu ; /* first used byte in buffer */
void ieee80211_beacon_connection_loss_work ( struct work_struct * work ) <nl> struct sta_info * sta ; <nl>  <nl> if ( ifmgd -> associated ) { <nl> + rcu_read_lock (); <nl> sta = sta_info_get ( sdata , ifmgd -> bssid ); <nl> if ( sta ) <nl> sta -> beacon_loss_count ++; <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> if ( sdata -> local -> hw . flags & IEEE80211_HW_CONNECTION_MONITOR )
int mwifiex_ret_802_11_scan ( struct mwifiex_private * priv , <nl>  <nl> pmatch = adapter -> nd_info -> matches [ idx ]; <nl>  <nl> - if (! pmatch ) { <nl> + if ( pmatch ) { <nl> memset ( pmatch , 0 , sizeof (* pmatch )); <nl> if ( chan_band_tlv ) { <nl> pmatch -> n_channels = 1 ;
static int st33zp24_spi_evaluate_latency ( void * phy_id ) <nl> & data , 1 ); <nl> latency ++; <nl> } <nl> + if ( status < 0 ) <nl> + return status ; <nl> + if ( latency == MAX_SPI_LATENCY ) <nl> + return - ENODEV ; <nl> + <nl> return latency - 1 ; <nl> } /* evaluate_latency () */ <nl> 
static void proc_flush_task_mnt ( struct vfsmount * mnt , pid_t pid , pid_t tgid ) <nl> dput ( dentry ); <nl> } <nl>  <nl> + if ( pid == tgid ) <nl> + return ; <nl> + <nl> name . name = buf ; <nl> name . len = snprintf ( buf , sizeof ( buf ), "% d ", tgid ); <nl> leader = d_hash_and_lookup ( mnt -> mnt_root , & name );
static int kvm_vm_ioctl_create_vcpu ( struct kvm * kvm , u32 id ) <nl> int r ; <nl> struct kvm_vcpu * vcpu , * v ; <nl>  <nl> + if ( id >= KVM_MAX_VCPUS ) <nl> + return - EINVAL ; <nl> + <nl> vcpu = kvm_arch_vcpu_create ( kvm , id ); <nl> if ( IS_ERR ( vcpu )) <nl> return PTR_ERR ( vcpu );
static int mxs_phy_probe ( struct platform_device * pdev ) <nl> mxs_phy -> phy . set_suspend = mxs_phy_suspend ; <nl> mxs_phy -> phy . notify_connect = mxs_phy_on_connect ; <nl> mxs_phy -> phy . notify_disconnect = mxs_phy_on_disconnect ; <nl> + mxs_phy -> phy . type = USB_PHY_TYPE_USB2 ; <nl>  <nl> ATOMIC_INIT_NOTIFIER_HEAD (& mxs_phy -> phy . notifier ); <nl> 
static int __exit fsl_udc_remove ( struct platform_device * pdev ) <nl> if (! udc_controller ) <nl> return - ENODEV ; <nl>  <nl> - usb_del_gadget_udc (& udc_controller -> gadget ); <nl> udc_controller -> done = & done ; <nl> + usb_del_gadget_udc (& udc_controller -> gadget ); <nl>  <nl> fsl_udc_clk_release (); <nl> 
struct md_op_data * ll_prep_md_op_data ( struct md_op_data * op_data , <nl> op_data -> op_default_stripe_offset = - 1 ; <nl> if ( S_ISDIR ( i1 -> i_mode )) { <nl> op_data -> op_mea1 = ll_i2info ( i1 )-> lli_lsm_md ; <nl> - op_data -> op_default_stripe_offset = <nl> - ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> + if ( opc == LUSTRE_OPC_MKDIR ) <nl> + op_data -> op_default_stripe_offset = <nl> + ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> } <nl>  <nl> if ( i2 ) {
static struct snd_soc_dai_driver ipq806x_lpass_cpu_dai_driver = { <nl>  <nl> static int ipq806x_lpass_alloc_dma_channel ( struct lpass_data * drvdata , int dir ) <nl> { <nl> - return IPQ806X_LPAIF_RDMA_CHAN_MI2S ; <nl> + if ( dir == SNDRV_PCM_STREAM_PLAYBACK ) <nl> + return IPQ806X_LPAIF_RDMA_CHAN_MI2S ; <nl> + else /* Capture currently not implemented */ <nl> + return - EINVAL ; <nl> } <nl>  <nl> static int ipq806x_lpass_free_dma_channel ( struct lpass_data * drvdata , int chan )
static int prepare_for_handlers ( struct ieee80211_rx_data * rx , <nl> * and location updates . Note that mac80211 <nl> * itself never looks at these frames . <nl> */ <nl> + if (! multicast && <nl> + ! ether_addr_equal ( sdata -> vif . addr , hdr -> addr1 )) <nl> + return 0 ; <nl> if ( ieee80211_is_public_action ( hdr , skb -> len )) <nl> return 1 ; <nl> if (! ieee80211_is_beacon ( hdr -> frame_control ))
static void sm501_free_init_fb ( struct sm501fb_info * info , <nl> { <nl> struct fb_info * fbi = info -> fb [ head ]; <nl>  <nl> + if (! fbi ) <nl> + return ; <nl> + <nl> fb_dealloc_cmap (& fbi -> cmap ); <nl> } <nl> 
static int sep_register_driver_with_fs ( struct sep_device * sep ) <nl> if ( ret_val ) { <nl> dev_warn (& sep -> pdev -> dev , " sysfs attribute1 fails for SEP % x \ n ", <nl> ret_val ); <nl> + misc_deregister (& sep -> miscdev_sep ); <nl> return ret_val ; <nl> } <nl> 
static long vbg_misc_device_ioctl ( struct file * filp , unsigned int req , <nl> if (! buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( copy_from_user ( buf , ( void *) arg , hdr . size_in )) { <nl> + *(( struct vbg_ioctl_hdr *) buf ) = hdr ; <nl> + if ( copy_from_user ( buf + sizeof ( hdr ), ( void *) arg + sizeof ( hdr ), <nl> + hdr . size_in - sizeof ( hdr ))) { <nl> ret = - EFAULT ; <nl> goto out ; <nl> }
static int ath10k_add_interface ( struct ieee80211_hw * hw , <nl> goto err_vdev_delete ; <nl> } <nl>  <nl> - if ( ar -> cfg_tx_chainmask ) { <nl> + /* Configuring number of spatial stream for monitor interface is causing <nl> + * target assert in qca9888 and qca6174 . <nl> + */ <nl> + if ( ar -> cfg_tx_chainmask && ( vif -> type != NL80211_IFTYPE_MONITOR )) { <nl> u16 nss = get_nss_from_chainmask ( ar -> cfg_tx_chainmask ); <nl>  <nl> vdev_param = ar -> wmi . vdev_param -> nss ;
cy_put_char ( struct tty_struct * tty , unsigned char ch ) <nl> if ( serial_paranoia_check ( info , tty -> name , " cy_put_char ")) <nl> return ; <nl>  <nl> - if (! tty || ! info -> xmit_buf ) <nl> + if (! info -> xmit_buf ) <nl> return ; <nl>  <nl> local_irq_save ( flags ); <nl> cy_write ( struct tty_struct * tty , <nl> return 0 ; <nl> } <nl>  <nl> - if (! tty || ! info -> xmit_buf ){ <nl> + if (! info -> xmit_buf ){ <nl> return 0 ; <nl> } <nl> 
static int sx150x_regmap_reg_width ( struct sx150x_pinctrl * pctl , <nl> reg == data -> pri . x123 . reg_advanced ) <nl> || <nl> ( data -> model == SX150X_456 && <nl> + data -> pri . x456 . reg_advanced && <nl> reg == data -> pri . x456 . reg_advanced )) { <nl> return 8 ; <nl> } else {
static int gart_map_sg ( struct device * dev , struct scatterlist * sg , int nents , <nl>  <nl> error : <nl> flush_gart (); <nl> - gart_unmap_sg ( dev , sg , nents , dir ); <nl> + gart_unmap_sg ( dev , sg , out , dir ); <nl> /* When it was forced or merged try again in a dumb way */ <nl> if ( force_iommu || iommu_merge ) { <nl> out = dma_map_sg_nonforce ( dev , sg , nents , dir );
static struct pnp_driver tpm_inf_pnp_driver = { <nl> . probe = tpm_inf_pnp_probe , <nl> . suspend = tpm_inf_pnp_suspend , <nl> . resume = tpm_inf_pnp_resume , <nl> - . remove = __devexit_p ( tpm_inf_pnp_remove ) <nl> + . remove = tpm_inf_pnp_remove <nl> }; <nl>  <nl> static int __init init_inf ( void )
static int ima_lsm_rule_init ( struct ima_measure_rule_entry * entry , <nl> result = security_filter_rule_init ( entry -> lsm [ lsm_rule ]. type , <nl> Audit_equal , args , <nl> & entry -> lsm [ lsm_rule ]. rule ); <nl> + if (! entry -> lsm [ lsm_rule ]. rule ) <nl> + return - EINVAL ; <nl> return result ; <nl> } <nl> 
CIFSSMBSetAttrLegacy ( int xid , struct cifsTconInfo * tcon , char * fileName , <nl>  <nl> int <nl> CIFSSMBUnixSetInfo ( const int xid , struct cifsTconInfo * tcon , char * fileName , <nl> - const struct cifs_unix_set_info_args * args , <nl> + const struct cifs_unix_set_info_args * args , <nl> const struct nls_table * nls_codepage , int remap ) <nl> { <nl> TRANSACTION2_SPI_REQ * pSMB = NULL ;
int cfg80211_get_station ( struct net_device * dev , const u8 * mac_addr , <nl> if (! rdev -> ops -> get_station ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset ( sinfo , 0 , sizeof (* sinfo )); <nl> + <nl> return rdev_get_station ( rdev , dev , mac_addr , sinfo ); <nl> } <nl> EXPORT_SYMBOL ( cfg80211_get_station );
int kvm_arch_vcpu_ioctl_set_sregs ( struct kvm_vcpu * vcpu , <nl> kvm_set_segment ( vcpu , & sregs -> tr , VCPU_SREG_TR ); <nl> kvm_set_segment ( vcpu , & sregs -> ldt , VCPU_SREG_LDTR ); <nl>  <nl> + update_cr8_intercept ( vcpu ); <nl> + <nl> /* Older userspace won ' t unhalt the vcpu on reset . */ <nl> if ( kvm_vcpu_is_bsp ( vcpu ) && kvm_rip_read ( vcpu ) == 0xfff0 && <nl> sregs -> cs . selector == 0xf000 && sregs -> cs . base == 0xffff0000 &&
static int igb_ptp_feature_enable_i210 ( struct ptp_clock_info * ptp , <nl> ts . tv_nsec = rq -> perout . period . nsec ; <nl> ns = timespec64_to_ns (& ts ); <nl> ns = ns >> 1 ; <nl> - if ( on && ns <= 70000000LL ) { <nl> + if ( on && (( ns <= 70000000LL ) || ( ns == 125000000LL ) || <nl> + ( ns == 250000000LL ) || ( ns == 500000000LL ))) { <nl> if ( ns < 8LL ) <nl> return - EINVAL ; <nl> use_freq = 1 ;
static int hba_setup_cid_tbls ( struct beiscsi_hba * phba ) <nl> kfree ( phba -> ep_array ); <nl> phba -> ep_array = NULL ; <nl> ret = - ENOMEM ; <nl> + <nl> + goto free_memory ; <nl> } <nl>  <nl> for ( i = 0 ; i < phba -> params . cxns_per_ctrl ; i ++) {
static int __init tegra_pmc_early_init ( void ) <nl> bool invert ; <nl> u32 value ; <nl>  <nl> + mutex_init (& pmc -> powergates_lock ); <nl> + <nl> np = of_find_matching_node_and_match ( NULL , tegra_pmc_match , & match ); <nl> if (! np ) { <nl> /* <nl> static int __init tegra_pmc_early_init ( void ) <nl> return - ENXIO ; <nl> } <nl>  <nl> - mutex_init (& pmc -> powergates_lock ); <nl> - <nl> if ( np ) { <nl> pmc -> soc = match -> data ; <nl> 
static struct pci_resource * get_max_resource ( struct pci_resource ** head , u32 siz <nl> temp = temp -> next ; <nl> } <nl>  <nl> - temp -> next = max -> next ; <nl> + if ( temp ) <nl> + temp -> next = max -> next ; <nl> } <nl>  <nl> max -> next = NULL ;
static int hist_entry__srcline_snprintf ( struct hist_entry * self , char * bf , <nl> if ( path != NULL ) <nl> goto out_path ; <nl>  <nl> + if (! self -> ms . map ) <nl> + goto out_ip ; <nl> + <nl> snprintf ( cmd , sizeof ( cmd ), " addr2line - e % s % 016 " PRIx64 , <nl> self -> ms . map -> dso -> long_name , self -> ip ); <nl> fp = popen ( cmd , " r ");
void btrfs_update_iflags ( struct inode * inode ) <nl> */ <nl> void btrfs_inherit_iflags ( struct inode * inode , struct inode * dir ) <nl> { <nl> - unsigned int flags = BTRFS_I ( dir )-> flags ; <nl> + unsigned int flags ; <nl> + <nl> + if (! dir ) <nl> + return ; <nl> + <nl> + flags = BTRFS_I ( dir )-> flags ; <nl>  <nl> if ( S_ISREG ( inode -> i_mode )) <nl> flags &= ~ BTRFS_INODE_DIRSYNC ;
static inline void clear_operand_string ( struct filter_parse_state * ps ) <nl>  <nl> static inline int append_operand_char ( struct filter_parse_state * ps , char c ) <nl> { <nl> - if ( ps -> operand . tail == MAX_FILTER_STR_VAL ) <nl> + if ( ps -> operand . tail == MAX_FILTER_STR_VAL - 1 ) <nl> return - EINVAL ; <nl>  <nl> ps -> operand . string [ ps -> operand . tail ++] = c ;
int cx23885_dvb_unregister ( struct cx23885_tsport * port ) <nl> * implement MFE support . <nl> */ <nl> fe0 = videobuf_dvb_get_frontend (& port -> frontends , 1 ); <nl> - if ( fe0 -> dvb . frontend ) <nl> + if ( fe0 && fe0 -> dvb . frontend ) <nl> videobuf_dvb_unregister_bus (& port -> frontends ); <nl>  <nl> switch ( port -> dev -> board ) {
int cvm_oct_common_init ( struct net_device * dev ) <nl> mac = of_get_mac_address ( priv -> of_node ); <nl>  <nl> if ( mac ) <nl> - memcpy ( dev -> dev_addr , mac , ETH_ALEN ); <nl> + ether_addr_copy ( dev -> dev_addr , mac ); <nl> else <nl> eth_hw_addr_random ( dev ); <nl> 
static void efx_filter_rfs_work ( struct work_struct * data ) <nl> struct efx_channel * channel = efx_get_channel ( efx , req -> rxq_index ); <nl> int rc ; <nl>  <nl> - rc = efx -> type -> filter_insert ( efx , & req -> spec , false ); <nl> + rc = efx -> type -> filter_insert ( efx , & req -> spec , true ); <nl> if ( rc >= 0 ) { <nl> /* Remember this so we can check whether to expire the filter <nl> * later .
static int igb_find_enabled_vfs ( struct igb_adapter * adapter ) <nl> vf_devfn = pdev -> devfn + 0x80 ; <nl> pvfdev = pci_get_device ( hw -> vendor_id , device_id , NULL ); <nl> while ( pvfdev ) { <nl> - if ( pvfdev -> devfn == vf_devfn ) <nl> + if ( pvfdev -> devfn == vf_devfn && <nl> + ( pvfdev -> bus -> number >= pdev -> bus -> number )) <nl> vfs_found ++; <nl> vf_devfn += vf_stride ; <nl> pvfdev = pci_get_device ( hw -> vendor_id ,
static int vidioc_querycap ( struct file * file , void * priv , <nl> strcpy ( cap -> driver , " vivi "); <nl> strcpy ( cap -> card , " vivi "); <nl> strlcpy ( cap -> bus_info , dev -> v4l2_dev . name , sizeof ( cap -> bus_info )); <nl> - cap -> capabilities = V4L2_CAP_VIDEO_CAPTURE | V4L2_CAP_STREAMING | \ <nl> - V4L2_CAP_READWRITE ; <nl> + cap -> capabilities = V4L2_CAP_VIDEO_CAPTURE | V4L2_CAP_STREAMING | <nl> + V4L2_CAP_READWRITE | V4L2_CAP_DEVICE_CAPS ; <nl> + cap -> device_caps = cap -> capabilities ; <nl> return 0 ; <nl> } <nl> 
static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
extern int blk_iopoll_enabled ; <nl> static int sixty = 60 ; <nl> # endif <nl>  <nl> + static int neg_one = - 1 ; <nl> static int zero ; <nl> static int __maybe_unused one = 1 ; <nl> static int __maybe_unused two = 2 ;
int afs_prepare_write ( struct file * file , struct page * page , <nl> _leave (" = % d [ prep ]", ret ); <nl> return ret ; <nl> } <nl> - SetPageUptodate ( page ); <nl> } <nl>  <nl> try_again : <nl> int afs_commit_write ( struct file * file , struct page * page , <nl> spin_unlock (& vnode -> writeback_lock ); <nl> } <nl>  <nl> + SetPageUptodate ( page ); <nl> set_page_dirty ( page ); <nl> - <nl> if ( PageDirty ( page )) <nl> _debug (" dirtied "); <nl> 
int ubifs_run_commit ( struct ubifs_info * c ) <nl>  <nl> spin_lock (& c -> cs_lock ); <nl> if ( c -> cmt_state == COMMIT_BROKEN ) { <nl> - err = - EINVAL ; <nl> + err = - EROFS ; <nl> goto out ; <nl> } <nl>  <nl> int ubifs_run_commit ( struct ubifs_info * c ) <nl> * re - check it . <nl> */ <nl> if ( c -> cmt_state == COMMIT_BROKEN ) { <nl> - err = - EINVAL ; <nl> + err = - EROFS ; <nl> goto out_cmt_unlock ; <nl> } <nl> 
int saa7164_cmd_send ( struct saa7164_dev * dev , u8 id , enum tmComResCmd command , <nl> dprintk ( DBGLVL_CMD , <nl> "% s () UNKNOWN OR INVALID CONTROL \ n ", <nl> __func__ ); <nl> + ret = SAA_ERR_NOT_SUPPORTED ; <nl> + break ; <nl> default : <nl> dprintk ( DBGLVL_CMD , "% s () UNKNOWN \ n ", __func__ ); <nl> ret = SAA_ERR_NOT_SUPPORTED ;
int inet6_csk_bind_conflict ( const struct sock * sk , <nl> if ( ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> break ; <nl> } <nl> + if (! relax && reuse && sk2 -> sk_reuse && <nl> + sk2 -> sk_state != TCP_LISTEN && <nl> + ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> + break ; <nl> } <nl> } <nl> 
static void i40iw_cm_disconn_true ( struct i40iw_qp * iwqp ) <nl> /* Flush the queues */ <nl> i40iw_flush_wqes ( iwdev , iwqp ); <nl>  <nl> - if ( qp -> term_flags ) { <nl> + if ( qp -> term_flags && iwqp -> ibqp . event_handler ) { <nl> ibevent . device = iwqp -> ibqp . device ; <nl> ibevent . event = ( qp -> eventtype == TERM_EVENT_QP_FATAL ) ? <nl> IB_EVENT_QP_FATAL : IB_EVENT_QP_ACCESS_ERR ;
static int xen_allocate_irq_gsi ( unsigned gsi ) <nl>  <nl> static void xen_free_irq ( unsigned irq ) <nl> { <nl> + /* Legacy IRQ descriptors are managed by the arch . */ <nl> + if ( irq < NR_IRQS_LEGACY ) <nl> + return ; <nl> + <nl> irq_free_desc ( irq ); <nl> } <nl> 
void gspca_frame_add ( struct gspca_dev * gspca_dev , <nl> } else { <nl> switch ( gspca_dev -> last_packet_type ) { <nl> case DISCARD_PACKET : <nl> - if ( packet_type == LAST_PACKET ) <nl> + if ( packet_type == LAST_PACKET ) { <nl> gspca_dev -> last_packet_type = packet_type ; <nl> + gspca_dev -> image = NULL ; <nl> + gspca_dev -> image_len = 0 ; <nl> + } <nl> return ; <nl> case LAST_PACKET : <nl> return ;
static int aead_setkey ( struct crypto_aead * tfm , const u8 * key , <nl> ctx -> authkey_len = keys . authkeylen ; <nl> ctx -> enckey_len = keys . enckeylen ; <nl>  <nl> + memzero_explicit (& keys , sizeof ( keys )); <nl> return aead_setup ( tfm , crypto_aead_authsize ( tfm )); <nl> badkey : <nl> crypto_aead_set_flags ( tfm , CRYPTO_TFM_RES_BAD_KEY_LEN ); <nl> + memzero_explicit (& keys , sizeof ( keys )); <nl> return - EINVAL ; <nl> } <nl> 
void __init exynos5_init_irq ( void ) <nl> * Theses parameters should be NULL and 0 because EXYNOS4 <nl> * uses GIC instead of VIC . <nl> */ <nl> - s5p_init_irq ( NULL , 0 ); <nl> + if (! of_machine_is_compatible (" samsung , exynos5440 ")) <nl> + s5p_init_irq ( NULL , 0 ); <nl>  <nl> gic_arch_extn . irq_set_wake = s3c_irq_wake ; <nl> }
static struct attribute * module_attrs [] = { <nl> }; <nl> ATTRIBUTE_GROUPS ( module ); <nl>  <nl> - static void greybus_module_release ( struct device * dev ) <nl> + static void gb_module_release ( struct device * dev ) <nl> { <nl> struct gb_module * module = to_gb_module ( dev ); <nl>  <nl> static void greybus_module_release ( struct device * dev ) <nl>  <nl> struct device_type greybus_module_type = { <nl> . name = " greybus_module ", <nl> - . release = greybus_module_release , <nl> + . release = gb_module_release , <nl> }; <nl>  <nl> struct module_find {
int sysfs_move_dir ( struct kobject * kobj , struct kobject * new_parent_kobj ) <nl> /* Remove from old parent ' s list and insert into new parent ' s list . */ <nl> sysfs_unlink_sibling ( sd ); <nl> sysfs_get ( new_parent_sd ); <nl> + drop_nlink ( old_parent -> d_inode ); <nl> sysfs_put ( sd -> s_parent ); <nl> sd -> s_parent = new_parent_sd ; <nl> + inc_nlink ( new_parent -> d_inode ); <nl> sysfs_link_sibling ( sd ); <nl>  <nl> out_unlock :
static void aead_release ( void * private ) <nl> struct aead_tfm * tfm = private ; <nl>  <nl> crypto_free_aead ( tfm -> aead ); <nl> + crypto_put_default_null_skcipher2 (); <nl> kfree ( tfm ); <nl> } <nl>  <nl> static void aead_sock_destruct ( struct sock * sk ) <nl> unsigned int ivlen = crypto_aead_ivsize ( tfm ); <nl>  <nl> af_alg_pull_tsgl ( sk , ctx -> used , NULL , 0 ); <nl> - crypto_put_default_null_skcipher2 (); <nl> sock_kzfree_s ( sk , ctx -> iv , ivlen ); <nl> sock_kfree_s ( sk , ctx , ctx -> len ); <nl> af_alg_release_parent ( sk );
extern pgprot_t phys_mem_access_prot ( struct file * file , unsigned long pfn , <nl> # define pmd_sect ( pmd ) (( pmd_val ( pmd ) & PMD_TYPE_MASK ) == \ <nl> PMD_TYPE_SECT ) <nl>  <nl> -# ifdef CONFIG_ARM64_64K_PAGES <nl> +# if defined ( CONFIG_ARM64_64K_PAGES ) || CONFIG_PGTABLE_LEVELS < 3 <nl> # define pud_sect ( pud ) ( 0 ) <nl> # define pud_table ( pud ) ( 1 ) <nl> # else
static void acpi_ac_notify ( struct acpi_device * device , u32 event ) <nl> acpi_bus_generate_netlink_event ( device -> pnp . device_class , <nl> dev_name (& device -> dev ), event , <nl> ( u32 ) ac -> state ); <nl> + acpi_notifier_call_chain ( device , event , ( u32 ) ac -> state ); <nl> # ifdef CONFIG_ACPI_SYSFS_POWER <nl> kobject_uevent (& ac -> charger . dev -> kobj , KOBJ_CHANGE ); <nl> # endif
static int wm8753_register ( struct wm8753_priv * wm8753 ) <nl> codec -> reg_cache = & wm8753 -> reg_cache ; <nl> codec -> private_data = wm8753 ; <nl>  <nl> - memcpy ( codec -> reg_cache , wm8753_reg , sizeof ( codec -> reg_cache )); <nl> + memcpy ( codec -> reg_cache , wm8753_reg , sizeof ( wm8753 -> reg_cache )); <nl> INIT_DELAYED_WORK (& codec -> delayed_work , wm8753_work ); <nl>  <nl> ret = wm8753_reset ( codec );
int p9_client_remove ( struct p9_fid * fid ) <nl> P9_DPRINTK ( P9_DEBUG_9P , "<<< RREMOVE fid % d \ n ", fid -> fid ); <nl>  <nl> p9_free_req ( clnt , req ); <nl> - p9_fid_destroy ( fid ); <nl> - <nl> error : <nl> + p9_fid_destroy ( fid ); <nl> return err ; <nl> } <nl> EXPORT_SYMBOL ( p9_client_remove );
static void ieee80211_xmit ( struct ieee80211_sub_if_data * sdata , <nl> list ) { <nl> if (! ieee80211_sdata_running ( tmp_sdata )) <nl> continue ; <nl> - if ( tmp_sdata -> vif . type != NL80211_IFTYPE_AP ) <nl> + if ( tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_MONITOR || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_AP_VLAN || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_WDS ) <nl> continue ; <nl> if ( compare_ether_addr ( tmp_sdata -> vif . addr , <nl> hdr -> addr2 ) == 0 ) {
static int mem_cgroup_hierarchical_reclaim ( struct mem_cgroup * root_mem , <nl> excess = res_counter_soft_limit_excess (& root_mem -> res ) >> PAGE_SHIFT ; <nl>  <nl> /* If memsw_is_minimum == 1 , swap - out is of - no - use . */ <nl> - if ( root_mem -> memsw_is_minimum ) <nl> + if (! check_soft && root_mem -> memsw_is_minimum ) <nl> noswap = true ; <nl>  <nl> while ( 1 ) {
static const struct amdgpu_irq_src_funcs dm_hpd_irq_funcs = { <nl>  <nl> void amdgpu_dm_set_irq_funcs ( struct amdgpu_device * adev ) <nl> { <nl> - if ( adev -> mode_info . num_crtc > 0 ) <nl> - adev -> crtc_irq . num_types = AMDGPU_CRTC_IRQ_VLINE1 + adev -> mode_info . num_crtc ; <nl> - else <nl> - adev -> crtc_irq . num_types = 0 ; <nl> + <nl> + adev -> crtc_irq . num_types = adev -> mode_info . num_crtc ; <nl> adev -> crtc_irq . funcs = & dm_crtc_irq_funcs ; <nl>  <nl> adev -> pageflip_irq . num_types = adev -> mode_info . num_crtc ;
static int intel_crtc_mode_set ( struct drm_crtc * crtc , <nl> } <nl> dpll |= DPLL_DVO_HIGH_SPEED ; <nl> } <nl> - if ( is_dp ) <nl> + if ( is_dp || intel_encoder_is_pch_edp (& has_edp_encoder -> base )) <nl> dpll |= DPLL_DVO_HIGH_SPEED ; <nl>  <nl> /* compute bitmask from p1 value */
int dma_async_device_register ( struct dma_device * device ) <nl> } <nl> chan -> client_count = 0 ; <nl> } <nl> + <nl> + if (! chancnt ) { <nl> + dev_err ( device -> dev , "% s : device has no channels !\ n ", __func__ ); <nl> + rc = - ENODEV ; <nl> + goto err_out ; <nl> + } <nl> + <nl> device -> chancnt = chancnt ; <nl>  <nl> mutex_lock (& dma_list_mutex );
int __nvme_submit_sync_cmd ( struct request_queue * q , struct nvme_command * cmd , <nl> return PTR_ERR ( req ); <nl>  <nl> req -> cmd_type = REQ_TYPE_DRV_PRIV ; <nl> + req -> cmd_flags = REQ_FAILFAST_DRIVER ; <nl> req -> __data_len = 0 ; <nl> req -> __sector = ( sector_t ) - 1 ; <nl> req -> bio = req -> biotail = NULL ;
int main ( int argc , char ** argv ) <nl> "% s - dev % d ", device_name , dev_num ); <nl> if ( ret < 0 ) { <nl> ret = - ENOMEM ; <nl> - goto error_ret ; <nl> + goto error_free_dev_dir_name ; <nl> } <nl> } <nl>  <nl> int main ( int argc , char ** argv ) <nl> error_free_triggername : <nl> if ( datardytrigger ) <nl> free ( trigger_name ); <nl> + error_free_dev_dir_name : <nl> + free ( dev_dir_name ); <nl> error_ret : <nl> return ret ; <nl> }
static int f2fs_ioc_defragment ( struct file * filp , unsigned long arg ) <nl> goto out ; <nl> } <nl>  <nl> + if ( unlikely (( range . start + range . len ) >> PAGE_SHIFT > <nl> + sbi -> max_file_blocks )) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> err = f2fs_defragment_range ( sbi , filp , & range ); <nl> f2fs_update_time ( sbi , REQ_TIME ); <nl> if ( err < 0 )
__ieee80211_tx_prepare ( struct ieee80211_tx_data * tx , <nl> tx -> local = local ; <nl> tx -> sdata = IEEE80211_DEV_TO_SUB_IF ( dev ); <nl> tx -> channel = local -> hw . conf . channel ; <nl> + tx -> rate_idx = - 1 ; <nl> + tx -> last_frag_rate_idx = - 1 ; <nl> /* <nl> * Set this flag ( used below to indicate " automatic fragmentation "), <nl> * it will be cleared / left by radiotap as desired .
static void bnx2x_tpa_stop ( struct bnx2x * bp , struct bnx2x_fastpath * fp , <nl>  <nl> return ; <nl> } <nl> - bnx2x_frag_free ( fp , new_data ); <nl> + if ( new_data ) <nl> + bnx2x_frag_free ( fp , new_data ); <nl> drop : <nl> /* drop the packet and keep the buffer in the bin */ <nl> DP ( NETIF_MSG_RX_STATUS ,
static int asus_laptop_get_info ( struct asus_laptop * asus ) <nl> } <nl> } <nl> asus -> name = kstrdup ( string , GFP_KERNEL ); <nl> - if (! asus -> name ) <nl> + if (! asus -> name ) { <nl> + kfree ( buffer . pointer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if (* string ) <nl> pr_notice (" % s model detected \ n ", string );
static const struct dmi_system_id min_max_dmi_table [] __initconst = { <nl> }, <nl> . driver_data = ( int []){ 1024 , 5052 , 2258 , 4832 }, <nl> }, <nl> + { <nl> + /* Lenovo ThinkPad X240 */ <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " LENOVO "), <nl> + DMI_MATCH ( DMI_PRODUCT_VERSION , " ThinkPad X240 "), <nl> + }, <nl> + . driver_data = ( int []){ 1232 , 5710 , 1156 , 4696 }, <nl> + }, <nl> { <nl> /* Lenovo ThinkPad T440s */ <nl> . matches = {
static int __devinit eni_start ( struct atm_dev * dev ) <nl> kfree ( eni_dev -> free_list ); <nl>  <nl> free_irq : <nl> - free_irq ( eni_dev -> irq , eni_dev ); <nl> + free_irq ( eni_dev -> irq , dev ); <nl>  <nl> out : <nl> return error ;
static int intel_atomic_check ( struct drm_device * dev , <nl> struct intel_crtc_state * pipe_config = <nl> to_intel_crtc_state ( crtc_state ); <nl>  <nl> + memset (& to_intel_crtc ( crtc )-> atomic , 0 , <nl> + sizeof ( struct intel_crtc_atomic_commit )); <nl> + <nl> /* Catch I915_MODE_FLAG_INHERITED */ <nl> if ( crtc_state -> mode . private_flags != crtc -> state -> mode . private_flags ) <nl> crtc_state -> mode_changed = true ;
int acpi_bus_generate_proc_event4 ( const char * device_class , const char * bus_id , <nl> if (! event_is_open ) <nl> return 0 ; <nl>  <nl> - event = kmalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> + event = kzalloc ( sizeof ( struct acpi_bus_event ), GFP_ATOMIC ); <nl> if (! event ) <nl> return - ENOMEM ; <nl> 
static int wm_adsp_fw_put ( struct snd_kcontrol * kcontrol , <nl> if ( adsp [ e -> shift_l ]. running ) <nl> return - EBUSY ; <nl>  <nl> - adsp -> fw = ucontrol -> value . integer . value [ 0 ]; <nl> + adsp [ e -> shift_l ]. fw = ucontrol -> value . integer . value [ 0 ]; <nl>  <nl> return 0 ; <nl> }
static int check_leaf ( struct fib_table * tb , struct trie * t , struct leaf * l , <nl>  <nl> if ( fa -> fa_tos && fa -> fa_tos != flp -> flowi4_tos ) <nl> continue ; <nl> + if ( fi -> fib_dead ) <nl> + continue ; <nl> if ( fa -> fa_info -> fib_scope < flp -> flowi4_scope ) <nl> continue ; <nl> fib_alias_accessed ( fa );
static void atomic_switch_perf_msrs ( struct vcpu_vmx * vmx ) <nl> msrs [ i ]. host ); <nl> } <nl>  <nl> - void vmx_arm_hv_timer ( struct kvm_vcpu * vcpu ) <nl> + static void vmx_arm_hv_timer ( struct kvm_vcpu * vcpu ) <nl> { <nl> struct vcpu_vmx * vmx = to_vmx ( vcpu ); <nl> u64 tscl ;
int __vma_adjust ( struct vm_area_struct * vma , unsigned long start , <nl> * If next doesn ' t have anon_vma , import from vma after <nl> * next , if the vma overlaps with it . <nl> */ <nl> - if ( remove_next == 2 && next && ! next -> anon_vma ) <nl> + if ( remove_next == 2 && ! next -> anon_vma ) <nl> exporter = next -> vm_next ; <nl>  <nl> } else if ( end > next -> vm_start ) {
static int snd_echo_resume ( struct pci_dev * pci ) <nl> DE_INIT ((" resume start \ n ")); <nl> pci_restore_state ( pci ); <nl> commpage_bak = kmalloc ( sizeof ( struct echoaudio ), GFP_KERNEL ); <nl> + if ( commpage_bak == NULL ) <nl> + return - ENOMEM ; <nl> commpage = chip -> comm_page ; <nl> memcpy ( commpage_bak , commpage , sizeof ( struct comm_page )); <nl> 
static int imx_ldb_connector_get_modes ( struct drm_connector * connector ) <nl> struct drm_display_mode * mode ; <nl>  <nl> mode = drm_mode_create ( connector -> dev ); <nl> + if (! mode ) <nl> + return - EINVAL ; <nl> drm_mode_copy ( mode , & imx_ldb_ch -> mode ); <nl> mode -> type |= DRM_MODE_TYPE_DRIVER | DRM_MODE_TYPE_PREFERRED ; <nl> drm_mode_probed_add ( connector , mode );
static __init void parse_cmdline_early ( char ** cmdline_p ) <nl> if (! memcmp ( from , " noapic ", 6 )) <nl> skip_ioapic_setup = 1 ; <nl>  <nl> - if (! memcmp ( from , " apic ", 4 )) { <nl> + /* Make sure to not confuse with apic = */ <nl> + if (! memcmp ( from , " apic ", 4 ) && <nl> + ( from [ 4 ] == ' ' || from [ 4 ] == 0 )) { <nl> skip_ioapic_setup = 0 ; <nl> ioapic_force = 1 ; <nl> }
int kprobe_fault_handler ( struct pt_regs * regs , unsigned long cause ); <nl> void kretprobe_trampoline ( void ); <nl> void trap_is_kprobe ( unsigned long address , struct pt_regs * regs ); <nl> # else <nl> - static void trap_is_kprobe ( unsigned long address , struct pt_regs * regs ) <nl> -{ <nl> -} <nl> +# define trap_is_kprobe ( address , regs ) <nl> # endif /* CONFIG_KPROBES */ <nl>  <nl> # endif /* _ARC_KPROBES_H */
static void iwl_mvm_scan_fill_ssids ( struct iwl_ssid_ie * cmd_ssid , <nl> static u16 iwl_mvm_get_active_dwell ( enum ieee80211_band band , int n_ssids ) <nl> { <nl> if ( band == IEEE80211_BAND_2GHZ ) <nl> - return 30 + 3 * ( n_ssids + 1 ); <nl> - return 20 + 2 * ( n_ssids + 1 ); <nl> + return 20 + 3 * ( n_ssids + 1 ); <nl> + return 10 + 2 * ( n_ssids + 1 ); <nl> } <nl>  <nl> static u16 iwl_mvm_get_passive_dwell ( enum ieee80211_band band )
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static int __devinit ipr_probe_ioa ( struct pci_dev * pdev , <nl> uproc = readl ( ioa_cfg -> regs . sense_uproc_interrupt_reg32 ); <nl> if (( mask & IPR_PCII_HRRQ_UPDATED ) == 0 || ( uproc & IPR_UPROCI_RESET_ALERT )) <nl> ioa_cfg -> needs_hard_reset = 1 ; <nl> - if ( interrupts & IPR_PCII_ERROR_INTERRUPTS ) <nl> + if (( interrupts & IPR_PCII_ERROR_INTERRUPTS ) || reset_devices ) <nl> ioa_cfg -> needs_hard_reset = 1 ; <nl> if ( interrupts & IPR_PCII_IOA_UNIT_CHECKED ) <nl> ioa_cfg -> ioa_unit_checked = 1 ;
void uncore_perf_event_update ( struct intel_uncore_box * box , struct perf_event * e <nl> u64 prev_count , new_count , delta ; <nl> int shift ; <nl>  <nl> - if ( event -> hw . idx >= UNCORE_PMC_IDX_FIXED ) <nl> + if ( event -> hw . idx == UNCORE_PMC_IDX_FIXED ) <nl> shift = 64 - uncore_fixed_ctr_bits ( box ); <nl> else <nl> shift = 64 - uncore_perf_ctr_bits ( box );
static int __access_remote_vm ( struct task_struct * tsk , struct mm_struct * mm , <nl> */ <nl> # ifdef CONFIG_HAVE_IOREMAP_PROT <nl> vma = find_vma ( mm , addr ); <nl> - if (! vma ) <nl> + if (! vma || vma -> vm_start > addr ) <nl> break ; <nl> if ( vma -> vm_ops && vma -> vm_ops -> access ) <nl> ret = vma -> vm_ops -> access ( vma , addr , buf ,
static struct davinci_aemif_timing da830_evm_nandflash_timing = { <nl> }; <nl>  <nl> static struct davinci_nand_pdata da830_evm_nand_pdata = { <nl> + . core_chipsel = 1 , <nl> . parts = da830_evm_nand_partitions , <nl> . nr_parts = ARRAY_SIZE ( da830_evm_nand_partitions ), <nl> . ecc_mode = NAND_ECC_HW ,
static enum dma_status sun6i_dma_tx_status ( struct dma_chan * chan , <nl> size_t bytes = 0 ; <nl>  <nl> ret = dma_cookie_status ( chan , cookie , state ); <nl> - if ( ret == DMA_COMPLETE ) <nl> + if ( ret == DMA_COMPLETE || ! state ) <nl> return ret ; <nl>  <nl> spin_lock_irqsave (& vchan -> vc . lock , flags );
static int bnx2x_populate_ext_phy ( struct bnx2x * bp , <nl> return - EINVAL ; <nl> default : <nl> * phy = phy_null ; <nl> + /* In case external PHY wasn ' t found */ <nl> + if (( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_DIRECT ) && <nl> + ( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_NOT_CONN )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
__tree_mod_log_free_eb ( struct btrfs_fs_info * fs_info , struct extent_buffer * eb ) <nl> u32 nritems ; <nl> int ret ; <nl>  <nl> + if ( btrfs_header_level ( eb ) == 0 ) <nl> + return ; <nl> + <nl> nritems = btrfs_header_nritems ( eb ); <nl> for ( i = nritems - 1 ; i >= 0 ; i --) { <nl> ret = tree_mod_log_insert_key_locked ( fs_info , eb , i ,
static void seq_set_overflow ( struct seq_file * m ) <nl>  <nl> static void * seq_buf_alloc ( unsigned long size ) <nl> { <nl> + if ( unlikely ( size > MAX_RW_COUNT )) <nl> + return NULL ; <nl> + <nl> return kvmalloc ( size , GFP_KERNEL_ACCOUNT ); <nl> } <nl> 
static int btrfs_get_name ( struct dentry * parent , char * name , <nl> name_len = btrfs_inode_ref_name_len ( leaf , iref ); <nl> } <nl>  <nl> + ret = btrfs_is_name_len_valid ( leaf , path -> slots [ 0 ], name_ptr , name_len ); <nl> + if (! ret ) { <nl> + btrfs_free_path ( path ); <nl> + return - EIO ; <nl> + } <nl> read_extent_buffer ( leaf , name , name_ptr , name_len ); <nl> btrfs_free_path ( path ); <nl> 
static struct dma_chan * k3_of_dma_simple_xlate ( struct of_phandle_args * dma_spec , <nl> struct k3_dma_dev * d = ofdma -> of_dma_data ; <nl> unsigned int request = dma_spec -> args [ 0 ]; <nl>  <nl> - if ( request > d -> dma_requests ) <nl> + if ( request >= d -> dma_requests ) <nl> return NULL ; <nl>  <nl> return dma_get_slave_channel (&( d -> chans [ request ]. vc . chan ));
int ath9k_wmi_cmd ( struct wmi * wmi , enum wmi_cmd_id cmd_id , <nl> ath_dbg ( common , WMI , " Timeout waiting for WMI command : % s \ n ", <nl> wmi_cmd_to_name ( cmd_id )); <nl> mutex_unlock (& wmi -> op_mutex ); <nl> + kfree_skb ( skb ); <nl> return - ETIMEDOUT ; <nl> } <nl> 
acornfb_pan_display ( struct fb_var_screeninfo * var , struct fb_info * info ) <nl> if (!( var -> vmode & FB_VMODE_YWRAP )) <nl> y_bottom += var -> yres ; <nl>  <nl> - BUG_ON ( y_bottom > var -> yres_virtual ); <nl> + if ( y_bottom > var -> yres_virtual ) <nl> + return - EINVAL ; <nl>  <nl> acornfb_update_dma ( info , var ); <nl> 
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> dev -> name ); <nl> break ; <nl> } <nl> + <nl> + if ( idev ) <nl> + idev -> if_flags |= IF_READY ; <nl> } else { <nl> if (! netif_carrier_ok ( dev )) { <nl> /* device is still not ready . */
static struct mfd_cell max77686_devs [] = { <nl> { . name = " max77686 - pmic ", }, <nl> { . name = " max77686 - rtc ", }, <nl> + { . name = " max77686 - clk ", }, <nl> }; <nl>  <nl> static struct regmap_config max77686_regmap_config = {
static void __exit cmdq_drv_exit ( void ) <nl>  <nl> subsys_initcall ( cmdq_drv_init ); <nl> module_exit ( cmdq_drv_exit ); <nl> + <nl> + MODULE_LICENSE (" GPL v2 ");
static void fbcon_deinit ( struct vc_data * vc ) <nl> finished : <nl>  <nl> fbcon_free_font ( p , free_font ); <nl> + if ( free_font ) <nl> + vc -> vc_font . data = NULL ; <nl>  <nl> if (! con_is_bound (& fb_con )) <nl> fbcon_exit ();
static int sensor_hub_probe ( struct hid_device * hdev , <nl> if ( name == NULL ) { <nl> hid_err ( hdev , " Failed MFD device name \ n "); <nl> ret = - ENOMEM ; <nl> + kfree ( hsdev ); <nl> goto err_no_mem ; <nl> } <nl> sd -> hid_sensor_hub_client_devs [
static struct platform_driver wm8505fb_driver = { <nl> . driver = { <nl> . owner = THIS_MODULE , <nl> . name = DRIVER_NAME , <nl> - . of_match_table = of_match_ptr ( wmt_dt_ids ), <nl> + . of_match_table = wmt_dt_ids , <nl> }, <nl> }; <nl> 
static int do_umount ( struct mount * mnt , int flags ) <nl> * Special case for " unmounting " root ... <nl> * we just try to remount it readonly . <nl> */ <nl> + if (! capable ( CAP_SYS_ADMIN )) <nl> + return - EPERM ; <nl> down_write (& sb -> s_umount ); <nl> if (!( sb -> s_flags & MS_RDONLY )) <nl> retval = do_remount_sb ( sb , MS_RDONLY , NULL , 0 );
static inline void mtd_erase_callback ( struct erase_info * instr ) <nl> printk ( KERN_INFO args ); \ <nl> } while ( 0 ) <nl> # else /* CONFIG_MTD_DEBUG */ <nl> -# define DEBUG ( n , args ...) do { } while ( 0 ) <nl> +# define DEBUG ( n , args ...) \ <nl> + do { \ <nl> + if ( 0 ) \ <nl> + printk ( KERN_INFO args ); \ <nl> + } while ( 0 ) <nl>  <nl> # endif /* CONFIG_MTD_DEBUG */ <nl> 
megasas_sysfs_set_dbg_lvl ( struct device_driver * dd , const char * buf , size_t coun <nl> return retval ; <nl> } <nl>  <nl> - static DRIVER_ATTR ( dbg_lvl , S_IRUGO | S_IWUGO , megasas_sysfs_show_dbg_lvl , <nl> + static DRIVER_ATTR ( dbg_lvl , S_IRUGO | S_IWUSR , megasas_sysfs_show_dbg_lvl , <nl> megasas_sysfs_set_dbg_lvl ); <nl>  <nl> static ssize_t
ccp_run_sha_cmd ( struct ccp_cmd_queue * cmd_q , struct ccp_cmd * cmd ) <nl> LSB_ITEM_SIZE ); <nl> break ; <nl> default : <nl> + kfree ( hmac_buf ); <nl> ret = - EINVAL ; <nl> - goto e_ctx ; <nl> + goto e_data ; <nl> } <nl>  <nl> memset (& hmac_cmd , 0 , sizeof ( hmac_cmd ));
int i915_gem_init ( struct drm_device * dev ) <nl> i915_gem_init_global_gtt ( dev ); <nl>  <nl> ret = i915_gem_context_init ( dev ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return ret ; <nl> + } <nl>  <nl> ret = i915_gem_init_hw ( dev ); <nl> mutex_unlock (& dev -> struct_mutex );
static int __init con3215_init ( void ) <nl> spin_lock_init (& raw3215_freelist_lock ); <nl> for ( i = 0 ; i < NR_3215_REQ ; i ++) { <nl> req = kzalloc ( sizeof ( struct raw3215_req ), GFP_KERNEL | GFP_DMA ); <nl> + if (! req ) <nl> + return - ENOMEM ; <nl> req -> next = raw3215_freelist ; <nl> raw3215_freelist = req ; <nl> }
int snd_hda_queue_unsol_event ( struct hda_bus * bus , u32 res , u32 res_ex ) <nl> struct hda_bus_unsolicited * unsol ; <nl> unsigned int wp ; <nl>  <nl> + if (! bus || ! bus -> workq ) <nl> + return 0 ; <nl> + <nl> trace_hda_unsol_event ( bus , res , res_ex ); <nl> unsol = bus -> unsol ; <nl> if (! unsol )
int imx1_pinctrl_core_probe ( struct platform_device * pdev , <nl>  <nl> ipctl -> base = devm_ioremap_nocache (& pdev -> dev , res -> start , <nl> resource_size ( res )); <nl> - if ( IS_ERR ( ipctl -> base )) <nl> - return PTR_ERR ( ipctl -> base ); <nl> + if (! ipctl -> base ) <nl> + return - ENOMEM ; <nl>  <nl> pctl_desc = & imx1_pinctrl_desc ; <nl> pctl_desc -> name = dev_name (& pdev -> dev );
asmlinkage long sys_oabi_semtimedop ( int semid , <nl> long err ; <nl> int i ; <nl>  <nl> - if ( nsops < 1 ) <nl> + if ( nsops < 1 || nsops > SEMOPM ) <nl> return - EINVAL ; <nl> sops = kmalloc ( sizeof (* sops ) * nsops , GFP_KERNEL ); <nl> if (! sops )
static void assert_can_enable_dc9 ( struct drm_i915_private * dev_priv ) <nl> " DC9 already programmed to be enabled .\ n "); <nl> WARN_ONCE ( I915_READ ( DC_STATE_EN ) & DC_STATE_EN_UPTO_DC5 , <nl> " DC5 still not disabled to enable DC9 .\ n "); <nl> - WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ), " Power well on .\ n "); <nl> + WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ) & <nl> + SKL_POWER_WELL_REQ ( SKL_DISP_PW_2 ), <nl> + " Power well 2 on .\ n "); <nl> WARN_ONCE ( intel_irqs_enabled ( dev_priv ), <nl> " Interrupts not disabled yet .\ n "); <nl> 
static int __init setup_maxnodemem ( char * str ) <nl> { <nl> char * endp ; <nl> unsigned long long maxnodemem ; <nl> - long node ; <nl> + unsigned long node ; <nl>  <nl> node = str ? simple_strtoul ( str , & endp , 0 ) : INT_MAX ; <nl> if ( node >= MAX_NUMNODES || * endp != ':')
static int ftrace_function_set_regexp ( struct ftrace_ops * ops , int filter , <nl> static int __ftrace_function_set_filter ( int filter , char * buf , int len , <nl> struct function_filter_data * data ) <nl> { <nl> - int i , re_cnt , ret ; <nl> + int i , re_cnt , ret = - EINVAL ; <nl> int * reset ; <nl> char ** re ; <nl> 
static ssize_t max_threshold_occ_write ( struct kernfs_open_file * of , <nl>  <nl> intel_cqm_threshold = bytes / r -> mon_scale ; <nl>  <nl> - return ret ?: nbytes ; <nl> + return nbytes ; <nl> } <nl>  <nl> /* rdtgroup information files for one cache resource . */
static struct sysfs_dirent * sysfs_new_dirent ( struct sysfs_dirent * parent_sd , <nl>  <nl> memset ( sd , 0 , sizeof (* sd )); <nl> atomic_set (& sd -> s_count , 1 ); <nl> - atomic_set (& sd -> s_event , 0 ); <nl> + atomic_set (& sd -> s_event , 1 ); <nl> INIT_LIST_HEAD (& sd -> s_children ); <nl> list_add (& sd -> s_sibling , & parent_sd -> s_children ); <nl> sd -> s_element = element ;
static int dgap_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> { <nl> int rc ; <nl>  <nl> + if ( dgap_NumBoards >= MAXBOARDS ) <nl> + return - EPERM ; <nl> + <nl> /* wake up and enable device */ <nl> rc = pci_enable_device ( pdev ); <nl> 
int arm_pmu_acpi_probe ( armpmu_init_fn init_fn ) <nl> ret = armpmu_register ( pmu ); <nl> if ( ret ) { <nl> pr_warn (" Failed to register PMU for CPU % d \ n ", cpu ); <nl> + kfree ( pmu -> name ); <nl> return ret ; <nl> } <nl> }
static int _write_mirror ( struct ore_io_state * ios , int cur_comp ) <nl> struct bio * bio ; <nl>  <nl> if ( per_dev != master_dev ) { <nl> - bio = bio_clone_kmalloc ( master_dev -> bio , <nl> - GFP_KERNEL ); <nl> + bio = bio_clone_fast ( master_dev -> bio , <nl> + GFP_KERNEL , NULL ); <nl> if ( unlikely (! bio )) { <nl> ORE_DBGMSG ( <nl> " Failed to allocate BIO size =% u \ n ",
xfs_setattr_nonsize ( <nl>  <nl> out_cancel : <nl> xfs_trans_cancel ( tp ); <nl> + xfs_iunlock ( ip , XFS_ILOCK_EXCL ); <nl> out_dqrele : <nl> xfs_qm_dqrele ( udqp ); <nl> xfs_qm_dqrele ( gdqp );
static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> struct gfs2_holder * t_gh ) <nl> { <nl> struct gfs2_inode * ip ; <nl> - struct gfs2_holder ji_gh ; <nl> struct gfs2_jdesc * jd ; <nl> struct lfcc * lfcc ; <nl> LIST_HEAD ( list ); <nl> static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> gfs2_glock_dq_uninit (& lfcc -> gh ); <nl> kfree ( lfcc ); <nl> } <nl> - gfs2_glock_dq_uninit (& ji_gh ); <nl> return error ; <nl> } <nl> 
static int sfb_dump ( struct Qdisc * sch , struct sk_buff * skb ) <nl>  <nl> sch -> qstats . backlog = q -> qdisc -> qstats . backlog ; <nl> opts = nla_nest_start ( skb , TCA_OPTIONS ); <nl> + if ( opts == NULL ) <nl> + goto nla_put_failure ; <nl> if ( nla_put ( skb , TCA_SFB_PARMS , sizeof ( opt ), & opt )) <nl> goto nla_put_failure ; <nl> return nla_nest_end ( skb , opts );
static struct pernet_operations unix_net_ops = { <nl> static int __init af_unix_init ( void ) <nl> { <nl> int rc = - 1 ; <nl> - struct sk_buff * dummy_skb ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> rc = proto_register (& unix_proto , 1 ); <nl> if ( rc != 0 ) {
static int skl_manifest_load ( struct snd_soc_component * cmpnt , <nl> struct skl * skl = ebus_to_skl ( ebus ); <nl> int ret = 0 ; <nl>  <nl> + /* proceed only if we have private data defined */ <nl> + if ( manifest -> priv . size == 0 ) <nl> + return 0 ; <nl> + <nl> minfo = & skl -> skl_sst -> manifest ; <nl>  <nl> skl_tplg_get_manifest_data ( manifest , bus -> dev , minfo );
static void stv0900_set_search_standard ( struct stv0900_internal * intp , <nl> break ; <nl> case STV0900_SEARCH_DSS : <nl> dprintk (" Search Standard = DSS \ n "); <nl> - case STV0900_SEARCH_DVBS2 : <nl> break ; <nl> + case STV0900_SEARCH_DVBS2 : <nl> dprintk (" Search Standard = DVBS2 \ n "); <nl> + break ; <nl> case STV0900_AUTO_SEARCH : <nl> default : <nl> dprintk (" Search Standard = AUTO \ n ");
void rt2x00lib_txdone ( struct queue_entry * entry , <nl> * Update TX statistics . <nl> */ <nl> rt2x00dev -> link . qual . tx_success += success ; <nl> - rt2x00dev -> link . qual . tx_failed += txdesc -> retry + fail ; <nl> + rt2x00dev -> link . qual . tx_failed += fail ; <nl>  <nl> /* <nl> * Initialize TX status
void __aa_fs_profile_migrate_dents ( struct aa_profile * old , <nl>  <nl> for ( i = 0 ; i < AAFS_PROF_SIZEOF ; i ++) { <nl> new -> dents [ i ] = old -> dents [ i ]; <nl> + if ( new -> dents [ i ]) <nl> + new -> dents [ i ]-> d_inode -> i_mtime = CURRENT_TIME ; <nl> old -> dents [ i ] = NULL ; <nl> } <nl> }
i915_gem_execbuffer_relocate_entry ( struct drm_i915_gem_object * obj , <nl> else <nl> ret = relocate_entry_gtt ( obj , reloc ); <nl>  <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* and update the user ' s relocation entry */ <nl> reloc -> presumed_offset = target_offset ; <nl> 
static int perl_start_script ( const char * script , int argc , const char ** argv ) <nl> goto error ; <nl> } <nl>  <nl> - perl_run ( my_perl ); <nl> + if ( perl_run ( my_perl )) { <nl> + err = - 1 ; <nl> + goto error ; <nl> + } <nl> + <nl> if ( SvTRUE ( ERRSV )) { <nl> err = - 1 ; <nl> goto error ;
static int qlcnic_82xx_setup_intr ( struct qlcnic_adapter * adapter ) <nl> qlcnic_disable_multi_tx ( adapter ); <nl>  <nl> err = qlcnic_enable_msi_legacy ( adapter ); <nl> - if (! err ) <nl> + if ( err ) <nl> return err ; <nl> } <nl> }
static int pvc_getname ( struct socket * sock , struct sockaddr * sockaddr , <nl> return - ENOTCONN ; <nl> * sockaddr_len = sizeof ( struct sockaddr_atmpvc ); <nl> addr = ( struct sockaddr_atmpvc *) sockaddr ; <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> addr -> sap_family = AF_ATMPVC ; <nl> addr -> sap_addr . itf = vcc -> dev -> number ; <nl> addr -> sap_addr . vpi = vcc -> vpi ;
static int dtl1_hci_send_frame ( struct sk_buff * skb ) <nl> nsh . len = skb -> len ; <nl>  <nl> s = bt_skb_alloc ( NSHL + skb -> len + 1 , GFP_ATOMIC ); <nl> + if (! s ) <nl> + return - ENOMEM ; <nl> + <nl> skb_reserve ( s , NSHL ); <nl> memcpy ( skb_put ( s , skb -> len ), skb -> data , skb -> len ); <nl> if ( skb -> len & 0x0001 )
static void perf_addr_filters_adjust ( struct vm_area_struct * vma ) <nl> struct perf_event_context * ctx ; <nl> int ctxn ; <nl>  <nl> + /* <nl> + * Data tracing isn ' t supported yet and as such there is no need <nl> + * to keep track of anything that isn ' t related to executable code : <nl> + */ <nl> + if (!( vma -> vm_flags & VM_EXEC )) <nl> + return ; <nl> + <nl> rcu_read_lock (); <nl> for_each_task_context_nr ( ctxn ) { <nl> ctx = rcu_dereference ( current -> perf_event_ctxp [ ctxn ]);
static inline void ttwu_post_activation ( struct task_struct * p , struct rq * rq , <nl> if ( p -> sched_class -> task_woken ) <nl> p -> sched_class -> task_woken ( rq , p ); <nl>  <nl> - if ( unlikely ( rq -> idle_stamp )) { <nl> + if ( rq -> idle_stamp ) { <nl> u64 delta = rq -> clock - rq -> idle_stamp ; <nl> u64 max = 2 * sysctl_sched_migration_cost ; <nl> 
int trace_get_user ( struct trace_parser * parser , const char __user * ubuf , <nl>  <nl> /* read the non - space input */ <nl> while ( cnt && ! isspace ( ch )) { <nl> - if ( parser -> idx < parser -> size ) <nl> + if ( parser -> idx < parser -> size - 1 ) <nl> parser -> buffer [ parser -> idx ++] = ch ; <nl> else { <nl> ret = - EINVAL ;
static int proc_pid_permission ( struct inode * inode , int mask ) <nl> bool has_perms ; <nl>  <nl> task = get_proc_task ( inode ); <nl> + if (! task ) <nl> + return - ESRCH ; <nl> has_perms = has_pid_permissions ( pid , task , 1 ); <nl> put_task_struct ( task ); <nl> 
static int video_open ( struct file * file ) <nl>  <nl> if ( NULL == dev ) { <nl> mutex_unlock (& cx25821_devlist_mutex ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> 
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) <nl> vnt_mac_set_bssid_addr ( priv , ( u8 *) conf -> bssid ); <nl>  <nl> 
static inline void TCP_ECN_send ( struct sock * sk , struct sk_buff * skb , <nl> */ <nl> static void tcp_init_nondata_skb ( struct sk_buff * skb , u32 seq , u8 flags ) <nl> { <nl> + skb -> ip_summed = CHECKSUM_PARTIAL ; <nl> skb -> csum = 0 ; <nl>  <nl> TCP_SKB_CB ( skb )-> flags = flags ;
SYSCALL_DEFINE1 ( inotify_init1 , int , flags ) <nl> if ( ret >= 0 ) <nl> return ret ; <nl>  <nl> + fsnotify_put_group ( group ); <nl> atomic_dec (& user -> inotify_devs ); <nl> out_free_uid : <nl> free_uid ( user );
static int ethtool_get_eeprom ( struct net_device * dev , void __user * useraddr ) <nl> bytes_remaining -= eeprom . len ; <nl> } <nl>  <nl> + eeprom . len = userbuf - ( useraddr + sizeof ( eeprom )); <nl> + eeprom . offset -= eeprom . len ; <nl> + if ( copy_to_user ( useraddr , & eeprom , sizeof ( eeprom ))) <nl> + ret = - EFAULT ; <nl> + <nl> kfree ( data ); <nl> return ret ; <nl> }
static int pci_call_probe ( struct pci_driver * drv , struct pci_dev * dev , <nl> if ( node >= 0 && node != numa_node_id ()) { <nl> int cpu ; <nl>  <nl> - get_online_cpus (); <nl> + cpu_hotplug_disable (); <nl> cpu = cpumask_any_and ( cpumask_of_node ( node ), cpu_online_mask ); <nl> if ( cpu < nr_cpu_ids ) <nl> error = work_on_cpu ( cpu , local_pci_probe , & ddi ); <nl> else <nl> error = local_pci_probe (& ddi ); <nl> - put_online_cpus (); <nl> + cpu_hotplug_enable (); <nl> } else <nl> error = local_pci_probe (& ddi ); <nl> 
int vb2_mmap ( struct vb2_queue * q , struct vm_area_struct * vma ) <nl> { <nl> unsigned long off = vma -> vm_pgoff << PAGE_SHIFT ; <nl> struct vb2_buffer * vb ; <nl> - unsigned int buffer , plane ; <nl> + unsigned int buffer = 0 , plane = 0 ; <nl> int ret ; <nl> unsigned long length ; <nl> 
static const struct file_operations __fops = { \ <nl> . release = simple_attr_release , \ <nl> . read = debugfs_attr_read , \ <nl> . write = debugfs_attr_write , \ <nl> - . llseek = generic_file_llseek , \ <nl> + . llseek = no_llseek , \ <nl> } <nl>  <nl> # if defined ( CONFIG_DEBUG_FS )
static void snd_als4000_configure ( struct snd_sb * chip ) <nl> /* SPECS_PAGE : 39 */ <nl> for ( i = ALS4K_GCR91_DMA0_ADDR ; i <= ALS4K_GCR96_DMA3_MODE_COUNT ; ++ i ) <nl> snd_als4k_gcr_write ( chip , i , 0 ); <nl> - <nl> + /* enable burst mode to prevent dropouts during high PCI bus usage */ <nl> snd_als4k_gcr_write ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL , <nl> - snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL )); <nl> + snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL ) | 0x04 ); <nl> spin_unlock_irq (& chip -> reg_lock ); <nl> } <nl> 
static int sr_init_command ( struct scsi_cmnd * SCpnt ) <nl> static int sr_block_open ( struct inode * inode , struct file * file ) <nl> { <nl> struct gendisk * disk = inode -> i_bdev -> bd_disk ; <nl> - struct scsi_cd * cd = scsi_cd ( inode -> i_bdev -> bd_disk ); <nl> + struct scsi_cd * cd ; <nl> int ret = 0 ; <nl>  <nl> if (!( cd = scsi_cd_get ( disk )))
int test_range_bit ( struct extent_io_tree * tree , u64 start , u64 end , <nl> bitset = 0 ; <nl> break ; <nl> } <nl> + <nl> + if ( state -> end == ( u64 )- 1 ) <nl> + break ; <nl> + <nl> start = state -> end + 1 ; <nl> if ( start > end ) <nl> break ;
__build_packet_message ( struct nfulnl_instance * inst , <nl> } <nl>  <nl> nlh -> nlmsg_len = inst -> skb -> tail - old_tail ; <nl> + inst -> lastnlh = nlh ; <nl> return 0 ; <nl>  <nl> nlmsg_failure :
int evm_update_evmxattr ( struct dentry * dentry , const char * xattr_name , <nl> rc = __vfs_setxattr_noperm ( dentry , XATTR_NAME_EVM , <nl> & xattr_data , <nl> sizeof ( xattr_data ), 0 ); <nl> - } <nl> - else if ( rc == - ENODATA ) <nl> + } else if ( rc == - ENODATA && inode -> i_op -> removexattr ) { <nl> rc = inode -> i_op -> removexattr ( dentry , XATTR_NAME_EVM ); <nl> + } <nl> return rc ; <nl> } <nl> 
static int __devexit wm8753_spi_remove ( struct spi_device * spi ) <nl>  <nl> snd_soc_unregister_codec (& spi -> dev ); <nl> regmap_exit ( wm8753 -> regmap ); <nl> - kfree ( wm8753 ); <nl> return 0 ; <nl> } <nl> 
int vmw_du_crtc_cursor_set ( struct drm_crtc * crtc , struct drm_file * file_priv , <nl> if (! ret ) { <nl> if (! surface -> snooper . image ) { <nl> DRM_ERROR (" surface not suitable for cursor \ n "); <nl> + vmw_surface_unreference (& surface ); <nl> return - EINVAL ; <nl> } <nl> } else {
void ceph_check_caps ( struct ceph_inode_info * ci , int flags , <nl>  <nl> if ( cap == ci -> i_auth_cap && ci -> i_dirty_caps ) <nl> flushing = __mark_caps_flushing ( inode , session ); <nl> + else <nl> + flushing = 0 ; <nl>  <nl> mds = cap -> mds ; /* remember mds , so we don ' t repeat */ <nl> sent ++;
static int get_free_pipe_id_locked ( struct goldfish_pipe_dev * dev ) <nl> /* Reallocate the array */ <nl> u32 new_capacity = 2 * dev -> pipes_capacity ; <nl> struct goldfish_pipe ** pipes = <nl> - kcalloc ( new_capacity , sizeof (* pipes ), GFP_KERNEL ); <nl> + kcalloc ( new_capacity , sizeof (* pipes ), GFP_ATOMIC ); <nl> if (! pipes ) <nl> return - ENOMEM ; <nl> memcpy ( pipes , dev -> pipes , sizeof (* pipes ) * dev -> pipes_capacity );
static int __devinit mwl8k_probe ( struct pci_dev * pdev , <nl> if ( rc ) { <nl> printk ( KERN_ERR "% s : Cannot register device \ n ", <nl> wiphy_name ( hw -> wiphy )); <nl> - goto err_free_irq ; <nl> + goto err_free_queues ; <nl> } <nl>  <nl> printk ( KERN_INFO "% s : % s v % d , % pM , % s firmware % u .% u .% u .% u \ n ",
int bxt_sst_dsp_init ( struct device * dev , void __iomem * mmio_base , int irq , <nl> sst_dsp_mailbox_init ( sst , ( BXT_ADSP_SRAM0_BASE + SKL_ADSP_W0_STAT_SZ ), <nl> SKL_ADSP_W0_UP_SZ , BXT_ADSP_SRAM1_BASE , SKL_ADSP_W1_SZ ); <nl>  <nl> + INIT_LIST_HEAD (& sst -> module_list ); <nl> ret = skl_ipc_init ( dev , skl ); <nl> if ( ret ) <nl> return ret ;
int uwb_rsv_find_best_allocation ( struct uwb_rsv * rsv , struct uwb_mas_bm * availab <nl> int bit_index ; <nl>  <nl> ai = kzalloc ( sizeof ( struct uwb_rsv_alloc_info ), GFP_KERNEL ); <nl> - <nl> + if (! ai ) <nl> + return UWB_RSV_ALLOC_NOT_FOUND ; <nl> ai -> min_mas = rsv -> min_mas ; <nl> ai -> max_mas = rsv -> max_mas ; <nl> ai -> max_interval = rsv -> max_interval ;
int xfrm_policy_flush ( u8 type , struct xfrm_audit * audit_info ) <nl> continue ; <nl> hlist_del (& pol -> bydst ); <nl> hlist_del (& pol -> byidx ); <nl> + list_del (& pol -> walk . all ); <nl> write_unlock_bh (& xfrm_policy_lock ); <nl>  <nl> xfrm_audit_policy_delete ( pol , 1 , audit_info -> loginuid ,
static acpi_status setup_res ( struct acpi_resource * acpi_res , void * data ) <nl> struct resource * res = data ; <nl> struct resource_win win ; <nl>  <nl> + /* <nl> + * We might assign this to ' res ' later , make sure all pointers are <nl> + * cleared before the resource is added to the global list <nl> + */ <nl> + memset (& win , 0 , sizeof ( win )); <nl> + <nl> res -> flags = 0 ; <nl> if ( acpi_dev_filter_resource_type ( acpi_res , IORESOURCE_MEM )) <nl> return AE_OK ;
static void atmel_aes_get_cap ( struct atmel_aes_dev * dd ) <nl>  <nl> /* keep only major version number */ <nl> switch ( dd -> hw_version & 0xff0 ) { <nl> + case 0x200 : <nl> + dd -> caps . has_dualbuff = 1 ; <nl> + dd -> caps . has_cfb64 = 1 ; <nl> + dd -> caps . max_burst_size = 4 ; <nl> + break ; <nl> case 0x130 : <nl> dd -> caps . has_dualbuff = 1 ; <nl> dd -> caps . has_cfb64 = 1 ;
static ssize_t hiddev_read ( struct file * file , char __user * buffer , size_t coun <nl> } <nl>  <nl> schedule (); <nl> + set_current_state ( TASK_INTERRUPTIBLE ); <nl> } <nl>  <nl> set_current_state ( TASK_RUNNING );
void brcmf_fweh_process_event ( struct brcmf_pub * drvr , <nl> if ( code != BRCMF_E_IF && ! fweh -> evt_handler [ code ]) <nl> return ; <nl>  <nl> - if ( datalen > BRCMF_DCMD_MAXLEN ) <nl> + if ( datalen > BRCMF_DCMD_MAXLEN || <nl> + datalen + sizeof (* event_packet ) > packet_len ) <nl> return ; <nl>  <nl> if ( in_interrupt ())
static int i2c_mux_reg_probe_dt ( struct regmux * mux , <nl> mux -> data . idle_in_use = true ; <nl>  <nl> /* map address from " reg " if exists */ <nl> - if ( of_address_to_resource ( np , 0 , & res )) { <nl> + if ( of_address_to_resource ( np , 0 , & res ) == 0 ) { <nl> mux -> data . reg_size = resource_size (& res ); <nl> mux -> data . reg = devm_ioremap_resource (& pdev -> dev , & res ); <nl> if ( IS_ERR ( mux -> data . reg ))
int rhashtable_walk_start_check ( struct rhashtable_iter * iter ) <nl> skip ++; <nl> if ( list == iter -> list ) { <nl> iter -> p = p ; <nl> - skip = skip ; <nl> + iter -> skip = skip ; <nl> goto found ; <nl> } <nl> }
struct mmc_host_ops { <nl>  <nl> int (* start_signal_voltage_switch )( struct mmc_host * host , struct mmc_ios * ios ); <nl>  <nl> + /* Check if the card is pulling dat [ 0 : 3 ] low */ <nl> + int (* card_busy )( struct mmc_host * host ); <nl> + <nl> /* The tuning command opcode value is different for SD and eMMC cards */ <nl> int (* execute_tuning )( struct mmc_host * host , u32 opcode ); <nl> void (* enable_preset_value )( struct mmc_host * host , bool enable );
static void __reada_start_machine ( struct btrfs_fs_info * fs_info ) <nl>  <nl> do { <nl> enqueued = 0 ; <nl> + mutex_lock (& fs_devices -> device_list_mutex ); <nl> list_for_each_entry ( device , & fs_devices -> devices , dev_list ) { <nl> if ( atomic_read (& device -> reada_in_flight ) < <nl> MAX_IN_FLIGHT ) <nl> enqueued += reada_start_machine_dev ( fs_info , <nl> device ); <nl> } <nl> + mutex_unlock (& fs_devices -> device_list_mutex ); <nl> total += enqueued ; <nl> } while ( enqueued && total < 10000 ); <nl> 
static void orinoco_process_scan_results ( struct work_struct * work ) <nl>  <nl> spin_lock_irqsave (& priv -> scan_lock , flags ); <nl> list_for_each_entry_safe ( sd , temp , & priv -> scan_list , list ) { <nl> - spin_unlock_irqrestore (& priv -> scan_lock , flags ); <nl>  <nl> buf = sd -> buf ; <nl> len = sd -> len ; <nl> type = sd -> type ; <nl>  <nl> list_del (& sd -> list ); <nl> + spin_unlock_irqrestore (& priv -> scan_lock , flags ); <nl> kfree ( sd ); <nl>  <nl> if ( len > 0 ) {
static int adxrs450_read_raw ( struct iio_dev * indio_dev , <nl> * val = t ; <nl> ret = IIO_VAL_INT ; <nl> break ; <nl> + case IIO_CHAN_INFO_CALIBBIAS : <nl> + ret = adxrs450_spi_read_reg_16 ( indio_dev , ADXRS450_DNC1 , & t ); <nl> + if ( ret ) <nl> + break ; <nl> + * val = t ; <nl> + ret = IIO_VAL_INT ; <nl> + break ; <nl> default : <nl> ret = - EINVAL ; <nl> break ;
const u32_t zcFwImage [] = { <nl> 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 , <nl> }; <nl>  <nl> - const u32_t zcFwImageSize = 15936 ; <nl> + const u32_t zcFwImageSize = 15936 ;
static s32 Handle_Get_InActiveTime ( struct wilc_vif * vif , <nl> wid . type = WID_STR ; <nl> wid . size = ETH_ALEN ; <nl> wid . val = kmalloc ( wid . size , GFP_KERNEL ); <nl> + if (! wid . val ) <nl> + return - ENOMEM ; <nl>  <nl> stamac = wid . val ; <nl> ether_addr_copy ( stamac , strHostIfStaInactiveT -> mac );
static struct fb_ops macfb_ops = { <nl> . fb_imageblit = cfb_imageblit , <nl> }; <nl>  <nl> - void __init macfb_setup ( char * options ) <nl> + static void __init macfb_setup ( char * options ) <nl> { <nl> char * this_opt ; <nl> 
static void sas_discover_domain ( struct work_struct * work ) <nl> case FANOUT_DEV : <nl> error = sas_discover_root_expander ( dev ); <nl> break ; <nl> -# ifdef CONFIG_SCSI_SAS_ATA <nl> case SATA_DEV : <nl> case SATA_PM : <nl> +# ifdef CONFIG_SCSI_SAS_ATA <nl> error = sas_discover_sata ( dev ); <nl> break ; <nl> +# else <nl> + SAS_DPRINTK (" ATA device seen but CONFIG_SCSI_SAS_ATA = N so cannot attach \ n "); <nl> + /* Fall through */ <nl> # endif <nl> default : <nl> error = - ENXIO ;
static int tipc_nl_compat_link_dump ( struct tipc_nl_compat_msg * msg , <nl>  <nl> link_info . dest = nla_get_flag ( link [ TIPC_NLA_LINK_DEST ]); <nl> link_info . up = htonl ( nla_get_flag ( link [ TIPC_NLA_LINK_UP ])); <nl> - strcpy ( link_info . str , nla_data ( link [ TIPC_NLA_LINK_NAME ])); <nl> + nla_strlcpy ( link_info . str , nla_data ( link [ TIPC_NLA_LINK_NAME ]), <nl> + TIPC_MAX_LINK_NAME ); <nl>  <nl> return tipc_add_tlv ( msg -> rep , TIPC_TLV_LINK_INFO , <nl> & link_info , sizeof ( link_info ));
void __init setup_ioapic_dest ( void ) <nl> mask = apic -> target_cpus (); <nl>  <nl> chip = irq_data_get_irq_chip ( idata ); <nl> - chip -> irq_set_affinity ( idata , mask , false ); <nl> + /* Might be lapic_chip for irq 0 */ <nl> + if ( chip -> irq_set_affinity ) <nl> + chip -> irq_set_affinity ( idata , mask , false ); <nl> } <nl> } <nl> # endif
int asn1_ber_decoder ( const struct asn1_decoder * decoder , <nl> if ( unlikely ( len > datalen - dp )) <nl> goto data_overrun_error ; <nl> } <nl> + } else { <nl> + if ( unlikely ( len > datalen - dp )) <nl> + goto data_overrun_error ; <nl> } <nl>  <nl> if ( flags & FLAG_CONS ) {
static int xgifb_probe ( struct pci_dev * pdev , <nl>  <nl> if ( xgifb_info -> mode_idx < 0 ) { <nl> dev_err (& pdev -> dev , " No supported video mode found \ n "); <nl> + ret = - EINVAL ; <nl> goto error_1 ; <nl> } <nl> 
static int mxs_lradc_ts_register ( struct mxs_lradc * lradc ) <nl> __set_bit ( EV_ABS , input -> evbit ); <nl> __set_bit ( EV_KEY , input -> evbit ); <nl> __set_bit ( BTN_TOUCH , input -> keybit ); <nl> + __set_bit ( INPUT_PROP_DIRECT , input -> propbit ); <nl> input_set_abs_params ( input , ABS_X , 0 , LRADC_SINGLE_SAMPLE_MASK , 0 , 0 ); <nl> input_set_abs_params ( input , ABS_Y , 0 , LRADC_SINGLE_SAMPLE_MASK , 0 , 0 ); <nl> input_set_abs_params ( input , ABS_PRESSURE , 0 , LRADC_SINGLE_SAMPLE_MASK ,
cifs_setlk ( struct file * file , struct file_lock * flock , __u32 type , <nl> rc = server -> ops -> mand_unlock_range ( cfile , flock , xid ); <nl>  <nl> out : <nl> - if ( flock -> fl_flags & FL_POSIX ) <nl> - posix_lock_file_wait ( file , flock ); <nl> + if ( flock -> fl_flags & FL_POSIX && ! rc ) <nl> + rc = posix_lock_file_wait ( file , flock ); <nl> return rc ; <nl> } <nl> 
static struct rpmsg_device * rpmsg_virtio_add_ctrl_dev ( struct virtio_device * vdev <nl>  <nl> err = rpmsg_ctrldev_register_device ( rpdev_ctrl ); <nl> if ( err ) { <nl> - kfree ( vch ); <nl> + /* vch will be free in virtio_rpmsg_release_device () */ <nl> return ERR_PTR ( err ); <nl> } <nl> 
struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
static int crypt_alloc_tfms ( struct crypt_config * cc , char * ciphermode ) <nl> unsigned i ; <nl> int err ; <nl>  <nl> - cc -> tfms = kmalloc ( cc -> tfms_count * sizeof ( struct crypto_skcipher *), <nl> + cc -> tfms = kzalloc ( cc -> tfms_count * sizeof ( struct crypto_skcipher *), <nl> GFP_KERNEL ); <nl> if (! cc -> tfms ) <nl> return - ENOMEM ;
static void ixgbe_clean_rx_irq ( struct ixgbe_q_vector * q_vector , <nl> if ( ixgbe_rx_is_fcoe ( adapter , rx_desc )) { <nl> ddp_bytes = ixgbe_fcoe_ddp ( adapter , rx_desc , skb , <nl> staterr ); <nl> - if (! ddp_bytes ) <nl> + if (! ddp_bytes ) { <nl> + dev_kfree_skb_any ( skb ); <nl> goto next_desc ; <nl> + } <nl> } <nl> # endif /* IXGBE_FCOE */ <nl> ixgbe_receive_skb ( q_vector , skb , staterr , rx_ring , rx_desc );
static void intel_i9xx_setup_flush ( void ) <nl> intel_private . ifp_resource . flags = IORESOURCE_MEM ; <nl>  <nl> /* Setup chipset flush for 915 */ <nl> - if ( IS_I965 || IS_G33 ) { <nl> + if ( IS_I965 || IS_G33 || IS_G4X ) { <nl> intel_i965_g33_setup_chipset_flush (); <nl> } else { <nl> intel_i915_setup_chipset_flush ();
static int crypto_authenc_verify ( struct aead_request * req , <nl> unsigned int authsize ; <nl>  <nl> areq_ctx -> complete = authenc_verify_ahash_done ; <nl> - areq_ctx -> complete = authenc_verify_ahash_update_done ; <nl> + areq_ctx -> update_complete = authenc_verify_ahash_update_done ; <nl>  <nl> ohash = authenc_ahash_fn ( req , CRYPTO_TFM_REQ_MAY_SLEEP ); <nl> if ( IS_ERR ( ohash ))
int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
static inline void debugfs_remove_domain_dir ( struct irq_domain * d ) { } <nl> # endif <nl>  <nl> const struct fwnode_operations irqchip_fwnode_ops ; <nl> + EXPORT_SYMBOL_GPL ( irqchip_fwnode_ops ); <nl>  <nl> /** <nl> * irq_domain_alloc_fwnode - Allocate a fwnode_handle suitable for
iscsi_if_rx ( struct sk_buff * skb ) <nl> uint32_t group ; <nl>  <nl> nlh = nlmsg_hdr ( skb ); <nl> - if ( nlh -> nlmsg_len < sizeof (* nlh ) || <nl> + if ( nlh -> nlmsg_len < sizeof (* nlh ) + sizeof (* ev ) || <nl> skb -> len < nlh -> nlmsg_len ) { <nl> break ; <nl> }
static void imx_pinconf_group_dbg_show ( struct pinctrl_dev * pctldev , <nl> const char * name ; <nl> int i , ret ; <nl>  <nl> - if ( group > pctldev -> num_groups ) <nl> + if ( group >= pctldev -> num_groups ) <nl> return ; <nl>  <nl> seq_puts ( s , "\ n ");
static int f2fs_write_data_pages ( struct address_space * mapping , <nl> if (! mapping -> a_ops -> writepage ) <nl> return 0 ; <nl>  <nl> + /* skip writing if there is no dirty page in this inode */ <nl> + if (! get_dirty_pages ( inode ) && wbc -> sync_mode == WB_SYNC_NONE ) <nl> + return 0 ; <nl> + <nl> if ( S_ISDIR ( inode -> i_mode ) && wbc -> sync_mode == WB_SYNC_NONE && <nl> get_dirty_pages ( inode ) < nr_pages_to_skip ( sbi , DATA ) && <nl> available_free_memory ( sbi , DIRTY_DENTS ))
static int __iwl_mvm_suspend ( struct ieee80211_hw * hw , <nl> out : <nl> if ( ret < 0 ) { <nl> iwl_mvm_ref ( mvm , IWL_MVM_REF_UCODE_DOWN ); <nl> - ieee80211_restart_hw ( mvm -> hw ); <nl> + if ( mvm -> restart_fw > 0 ) { <nl> + mvm -> restart_fw --; <nl> + ieee80211_restart_hw ( mvm -> hw ); <nl> + } <nl> iwl_mvm_free_nd ( mvm ); <nl> } <nl> out_noreset :
static int radeon_uvd_cs_reloc ( struct radeon_cs_parser * p , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (( start >> 28 ) != ( end >> 28 )) { <nl> + if (( start >> 28 ) != (( end - 1 ) >> 28 )) { <nl> DRM_ERROR (" reloc % LX -% LX crossing 256MB boundary !\ n ", <nl> start , end ); <nl> return - EINVAL ;
void __init omap_detect_sram ( void ) <nl> if ( cpu_is_omap34xx ()) { <nl> omap_sram_base = OMAP3_SRAM_PUB_VA ; <nl> omap_sram_start = OMAP3_SRAM_PUB_PA ; <nl> - omap_sram_size = 0x8000 ; /* 32K */ <nl> + if (( omap_type () == OMAP2_DEVICE_TYPE_EMU ) || <nl> + ( omap_type () == OMAP2_DEVICE_TYPE_SEC )) { <nl> + omap_sram_size = 0x7000 ; /* 28K */ <nl> + } else { <nl> + omap_sram_size = 0x8000 ; /* 32K */ <nl> + } <nl> } else { <nl> omap_sram_base = OMAP2_SRAM_PUB_VA ; <nl> omap_sram_start = OMAP2_SRAM_PUB_PA ;
static int btrfs_real_readdir ( struct file * filp , void * dirent , <nl>  <nl> /* Reached end of directory / root . Bump pos past the last item . */ <nl> if ( key_type == BTRFS_DIR_INDEX_KEY ) <nl> - filp -> f_pos = INT_LIMIT ( off_t ); <nl> + /* <nl> + * 32 - bit glibc will use getdents64 , but then strtol - <nl> + * so the last number we can serve is this . <nl> + */ <nl> + filp -> f_pos = 0x7fffffff ; <nl> else <nl> filp -> f_pos ++; <nl> nopos :
static int sdhci_st_probe ( struct platform_device * pdev ) <nl> if ( IS_ERR ( icnclk )) <nl> icnclk = NULL ; <nl>  <nl> - rstc = devm_reset_control_get (& pdev -> dev , NULL ); <nl> + rstc = devm_reset_control_get_exclusive (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( rstc )) <nl> rstc = NULL ; <nl> else
int snd_soc_register_dais ( struct device * dev , <nl> pr_debug (" Registered DAI '% s '\ n ", dai -> name ); <nl> } <nl>  <nl> + mutex_lock (& client_mutex ); <nl> snd_soc_instantiate_cards (); <nl> + mutex_unlock (& client_mutex ); <nl> return 0 ; <nl>  <nl> err :
static void conf_message_callback ( const char * fmt , va_list ap ) <nl>  <nl> static void show_help ( struct menu * menu ) <nl> { <nl> - struct gstr help = str_new (); <nl> + struct gstr help ; <nl> + <nl> + if (! menu ) <nl> + return ; <nl> + <nl> + help = str_new (); <nl> menu_get_ext_help ( menu , & help ); <nl> show_scroll_win ( main_window , _ ( menu_get_prompt ( menu )), str_get (& help )); <nl> str_free (& help );
static int logi_dj_ll_raw_request ( struct hid_device * hid , <nl> if (! out_buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( count < DJREPORT_SHORT_LENGTH - 2 ) <nl> + if ( count > DJREPORT_SHORT_LENGTH - 2 ) <nl> count = DJREPORT_SHORT_LENGTH - 2 ; <nl>  <nl> out_buf [ 0 ] = REPORT_ID_DJ_SHORT ;
struct el3_private { <nl> spinlock_t lock ; <nl> }; <nl>  <nl> - static const char * if_names [] = { " auto ", " 10baseT ", " 10base2 ", " AUI " }; <nl> + static const char * if_names [] = { " auto ", " 10base2 ", " 10baseT ", " AUI " }; <nl>  <nl> /*====================================================================*/ <nl> 
static int sc16is7xx_probe ( struct device * dev , <nl> else <nl> return PTR_ERR ( s -> clk ); <nl> } else { <nl> + clk_prepare_enable ( s -> clk ); <nl> freq = clk_get_rate ( s -> clk ); <nl> } <nl> 
static void udf_sb_free_partitions ( struct super_block * sb ) <nl> { <nl> struct udf_sb_info * sbi = UDF_SB ( sb ); <nl> int i ; <nl> - <nl> + if ( sbi -> s_partmaps == NULL ) <nl> + return ; <nl> for ( i = 0 ; i < sbi -> s_partitions ; i ++) <nl> udf_free_partition (& sbi -> s_partmaps [ i ]); <nl> kfree ( sbi -> s_partmaps );
int dw_hdmi_probe ( struct platform_device * pdev , <nl> const struct dw_hdmi_plat_data * plat_data ) <nl> { <nl> struct dw_hdmi * hdmi ; <nl> - int ret ; <nl>  <nl> hdmi = __dw_hdmi_probe ( pdev , plat_data ); <nl> if ( IS_ERR ( hdmi )) <nl> return PTR_ERR ( hdmi ); <nl>  <nl> - ret = drm_bridge_add (& hdmi -> bridge ); <nl> - if ( ret < 0 ) { <nl> - __dw_hdmi_remove ( hdmi ); <nl> - return ret ; <nl> - } <nl> + drm_bridge_add (& hdmi -> bridge ); <nl>  <nl> return 0 ; <nl> }
static int __init orion_nand_probe ( struct platform_device * pdev ) <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) { <nl> - err = - ENODEV ; <nl> + ret = - ENODEV ; <nl> goto no_res ; <nl> } <nl> 
static int iucv_sock_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> struct sk_buff * skb , * rskb , * cskb ; <nl> int err = 0 ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> if (( sk -> sk_state == IUCV_DISCONN ) && <nl> skb_queue_empty (& iucv -> backlog_skb_q ) && <nl> skb_queue_empty (& sk -> sk_receive_queue ) &&
static void efifb_fixup_resources ( struct pci_dev * dev ) <nl> if (! base ) <nl> return ; <nl>  <nl> - for ( i = 0 ; i < PCI_STD_RESOURCE_END ; i ++) { <nl> + for ( i = 0 ; i <= PCI_STD_RESOURCE_END ; i ++) { <nl> struct resource * res = & dev -> resource [ i ]; <nl>  <nl> if (!( res -> flags & IORESOURCE_MEM ))
extern int vdso_enabled ; <nl>  <nl> # endif /* ! CONFIG_X86_32 */ <nl>  <nl> +# define CORE_DUMP_USE_REGSET <nl> # define USE_ELF_CORE_DUMP <nl> # define ELF_EXEC_PAGESIZE 4096 <nl> 
static inline int logfs_get_sb_bdev ( struct logfs_super * s , <nl>  <nl> /* dev_mtd . c */ <nl> # ifdef CONFIG_MTD <nl> - int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> + int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ); <nl> # else <nl> static inline int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> {
int hfsplus_find_cat ( struct super_block * sb , u32 cnid , <nl> return - EIO ; <nl> } <nl>  <nl> + if ( be16_to_cpu ( tmp . thread . nodeName . length ) > 255 ) { <nl> + printk ( KERN_ERR " hfs : catalog name length corrupted \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> hfsplus_cat_build_key_uni ( fd -> search_key , be32_to_cpu ( tmp . thread . parentID ), <nl> & tmp . thread . nodeName ); <nl> return hfs_brec_find ( fd );
static void zynqmp_gqspi_selectslave ( struct zynqmp_qspi * instanceptr , <nl> case GQSPI_SELECT_FLASH_CS_BOTH : <nl> instanceptr -> genfifocs = GQSPI_GENFIFO_CS_LOWER | <nl> GQSPI_GENFIFO_CS_UPPER ; <nl> + break ; <nl> case GQSPI_SELECT_FLASH_CS_UPPER : <nl> instanceptr -> genfifocs = GQSPI_GENFIFO_CS_UPPER ; <nl> break ;
static void raise_dtr ( struct isi_port * port ) <nl> } <nl>  <nl> /* card -> lock HAS to be held */ <nl> - static inline void drop_dtr ( struct isi_port * port ) <nl> + static void drop_dtr ( struct isi_port * port ) <nl> { <nl> struct isi_board * card = port -> card ; <nl> unsigned long base = card -> base ;
int btrfs_remove_free_space ( struct btrfs_block_group_cache * block_group , <nl> ctl -> total_bitmaps --; <nl> } <nl> kmem_cache_free ( btrfs_free_space_cachep , info ); <nl> + ret = 0 ; <nl> goto out_lock ; <nl> } <nl>  <nl> int btrfs_remove_free_space ( struct btrfs_block_group_cache * block_group , <nl> unlink_free_space ( ctl , info ); <nl> info -> offset += bytes ; <nl> info -> bytes -= bytes ; <nl> - link_free_space ( ctl , info ); <nl> + ret = link_free_space ( ctl , info ); <nl> + WARN_ON ( ret ); <nl> goto out_lock ; <nl> } <nl> 
static void pvr2_hdw_state_log_state ( struct pvr2_hdw * hdw ) <nl> printk ( KERN_INFO "% s %.* s \ n ", hdw -> name , ccnt , buf ); <nl> } <nl> ccnt = pvr2_hdw_report_clients ( hdw , buf , sizeof ( buf )); <nl> + if ( ccnt >= sizeof ( buf )) <nl> + ccnt = sizeof ( buf ); <nl> + <nl> ucnt = 0 ; <nl> while ( ucnt < ccnt ) { <nl> lcnt = 0 ;
static int autofs4_tree_busy ( struct vfsmount * mnt , <nl> struct autofs_info * ino = autofs4_dentry_ino ( p ); <nl> unsigned int ino_count = atomic_read (& ino -> count ); <nl>  <nl> + /* <nl> + * Clean stale dentries below that have not been <nl> + * invalidated after a mount fail during lookup <nl> + */ <nl> + d_invalidate ( p ); <nl> + <nl> /* allow for dget above and top is already dgot */ <nl> if ( p == top ) <nl> ino_count += 2 ;
static struct comedi_driver das16_driver = { <nl> module_comedi_driver ( das16_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for DAS16 compatible boards "); <nl> MODULE_LICENSE (" GPL ");
static noinline long btrfs_ioctl_start_sync ( struct file * file , void __user * argp <nl> return PTR_ERR ( trans ); <nl> transid = trans -> transid ; <nl> ret = btrfs_commit_transaction_async ( trans , root , 0 ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + btrfs_end_transaction ( trans , root ); <nl> return ret ; <nl> + } <nl>  <nl> if ( argp ) <nl> if ( copy_to_user ( argp , & transid , sizeof ( transid )))
static noinline int btrfs_ioctl_resize ( struct btrfs_root * root , <nl> } <nl> ret = btrfs_grow_device ( trans , device , new_size ); <nl> btrfs_commit_transaction ( trans , root ); <nl> - } else { <nl> + } else if ( new_size < old_size ) { <nl> ret = btrfs_shrink_device ( device , new_size ); <nl> } <nl> 
static int tegra_pcie_probe ( struct platform_device * pdev ) <nl> struct pci_bus * child ; <nl> int err ; <nl>  <nl> - host = pci_alloc_host_bridge ( sizeof (* pcie )); <nl> + host = devm_pci_alloc_host_bridge ( dev , sizeof (* pcie )); <nl> if (! host ) <nl> return - ENOMEM ; <nl> 
long drm_ioctl ( struct file * filp , <nl> retcode = - EFAULT ; <nl> goto err_i1 ; <nl> } <nl> - } <nl> + } else <nl> + memset ( kdata , 0 , _IOC_SIZE ( cmd )); <nl> + <nl> if ( ioctl -> flags & DRM_UNLOCKED ) <nl> retcode = func ( dev , kdata , file_priv ); <nl> else {
vc4_get_bcl ( struct drm_device * dev , struct vc4_exec_info * exec ) <nl> sizeof ( struct vc4_shader_state )) || <nl> temp_size < exec_size ) { <nl> DRM_ERROR (" overflow in exec arguments \ n "); <nl> + ret = - EINVAL ; <nl> goto fail ; <nl> } <nl> 
MODULE_DEVICE_TABLE ( pci , epca_pci_tbl ); <nl> int __init init_PCI ( void ) <nl> { /* Begin init_PCI */ <nl> memset (& epca_driver , 0 , sizeof ( epca_driver )); <nl> + epca_driver . owner = THIS_MODULE ; <nl> epca_driver . name = " epca "; <nl> epca_driver . id_table = epca_pci_tbl ; <nl> epca_driver . probe = epca_init_one ;
static int atmel_hlcdc_plane_init_properties ( struct atmel_hlcdc_plane * plane , <nl> drm_object_attach_property (& plane -> base . base , <nl> props -> alpha , 255 ); <nl>  <nl> - if ( desc -> layout . xstride && desc -> layout . pstride ) { <nl> + if ( desc -> layout . xstride [ 0 ] && desc -> layout . pstride [ 0 ]) { <nl> int ret ; <nl>  <nl> ret = drm_plane_create_rotation_property (& plane -> base ,
static int vpbe_display_g_register ( struct file * file , void * priv , <nl> struct v4l2_dbg_register * reg ) <nl> { <nl> struct v4l2_dbg_match * match = & reg -> match ; <nl> + struct vpbe_fh * fh = file -> private_data ; <nl> + struct vpbe_device * vpbe_dev = fh -> disp_dev -> vpbe_dev ; <nl>  <nl> if ( match -> type >= 2 ) { <nl> v4l2_subdev_call ( vpbe_dev -> venc ,
static int m41t80_probe ( struct i2c_client * client , <nl> m41t80_rtc_ops . read_alarm = m41t80_read_alarm ; <nl> m41t80_rtc_ops . set_alarm = m41t80_set_alarm ; <nl> m41t80_rtc_ops . alarm_irq_enable = m41t80_alarm_irq_enable ; <nl> + /* Enable the wakealarm */ <nl> + device_init_wakeup (& client -> dev , true ); <nl> } <nl> } <nl> 
static int wcn36xx_start ( struct ieee80211_hw * hw ) <nl> wcn36xx_smd_stop ( wcn ); <nl> out_free_smd_buf : <nl> kfree ( wcn -> hal_buf ); <nl> - out_free_dxe_pool : <nl> - wcn36xx_dxe_free_mem_pools ( wcn ); <nl> out_free_dxe_ctl : <nl> wcn36xx_dxe_free_ctl_blks ( wcn ); <nl> + out_free_dxe_pool : <nl> + wcn36xx_dxe_free_mem_pools ( wcn ); <nl> out_smd_close : <nl> wcn36xx_smd_close ( wcn ); <nl> out_err :
static int stmmac_open ( struct net_device * dev ) <nl> if ( ret ) { <nl> pr_err ("% s : Cannot attach to PHY ( error : % d )\ n ", <nl> __func__ , ret ); <nl> - goto phy_error ; <nl> + return ret ; <nl> } <nl> } <nl>  <nl> static int stmmac_open ( struct net_device * dev ) <nl> dma_desc_error : <nl> if ( priv -> phydev ) <nl> phy_disconnect ( priv -> phydev ); <nl> - phy_error : <nl> - clk_disable_unprepare ( priv -> stmmac_clk ); <nl>  <nl> return ret ; <nl> }
static int mmci_probe ( struct amba_device * dev , <nl> dev_dbg ( mmc_dev ( mmc ), " clocking block at % u Hz \ n ", mmc -> f_max ); <nl>  <nl> /* Get regulators and the supported OCR mask */ <nl> - mmc_regulator_get_supply ( mmc ); <nl> + ret = mmc_regulator_get_supply ( mmc ); <nl> + if ( ret == - EPROBE_DEFER ) <nl> + goto clk_disable ; <nl> + <nl> if (! mmc -> ocr_avail ) <nl> mmc -> ocr_avail = plat -> ocr_mask ; <nl> else if ( plat -> ocr_mask )
int host_int_set_wep_default_key ( struct host_if_drv * hif_drv , u8 index ); <nl> * @ date 8 March 2012 <nl> * @ version 1 . 0 <nl> */ <nl> - int host_int_add_wep_key_bss_sta ( struct host_if_drv * hWFIDrv , const u8 * pu8WepKey , u8 u8WepKeylen , u8 u8Keyidx ); <nl> + int host_int_add_wep_key_bss_sta ( struct host_if_drv * hif_drv , const u8 * pu8WepKey , u8 u8WepKeylen , u8 u8Keyidx ); <nl> /** <nl> * @ brief host_int_add_wep_key_bss_ap <nl> * @ details valid only in AP mode if External Supplicant support is enabled .
nvkm_disp_oneinit ( struct nvkm_engine * engine ) <nl> /* Create output path objects for each VBIOS display path . */ <nl> i = - 1 ; <nl> while (( data = dcb_outp_parse ( bios , ++ i , & ver , & hdr , & dcbE ))) { <nl> + if ( ver < 0x40 ) /* No support for chipsets prior to NV50 . */ <nl> + break ; <nl> if ( dcbE . type == DCB_OUTPUT_UNUSED ) <nl> continue ; <nl> if ( dcbE . type == DCB_OUTPUT_EOL )
static ssize_t hidraw_get_report ( struct file * file , char __user * buffer , size_t <nl> int ret = 0 , len ; <nl> unsigned char report_number ; <nl>  <nl> + if (! hidraw_table [ minor ] || ! hidraw_table [ minor ]-> exist ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl> + <nl> dev = hidraw_table [ minor ]-> hid ; <nl>  <nl> if (! dev -> ll_driver -> raw_request ) {
static int arizona_runtime_resume ( struct device * dev ) <nl> return ret ; <nl> } <nl>  <nl> - regcache_sync ( arizona -> regmap ); <nl> + ret = regcache_sync ( arizona -> regmap ); <nl> + if ( ret != 0 ) { <nl> + dev_err ( arizona -> dev , " Failed to restore register cache \ n "); <nl> + regulator_disable ( arizona -> dcvdd ); <nl> + return ret ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static int find_data_references ( struct reloc_control * rc , <nl> } <nl>  <nl> key . objectid = ref_objectid ; <nl> - key . offset = ref_offset ; <nl> key . type = BTRFS_EXTENT_DATA_KEY ; <nl> + if ( ref_offset > (( u64 )- 1 << 32 )) <nl> + key . offset = 0 ; <nl> + else <nl> + key . offset = ref_offset ; <nl>  <nl> path -> search_commit_root = 1 ; <nl> path -> skip_locking = 1 ;
int iptunnel_xmit ( struct sock * sk , struct rtable * rt , struct sk_buff * skb , <nl> __be32 src , __be32 dst , __u8 proto , <nl> __u8 tos , __u8 ttl , __be16 df , bool xnet ) <nl> { <nl> - int pkt_len = skb -> len ; <nl> + int pkt_len = skb -> len - skb_inner_network_offset ( skb ); <nl> struct iphdr * iph ; <nl> int err ; <nl> 
static inline struct dma_chan <nl> if ( chan ) <nl> return chan ; <nl>  <nl> + if (! fn || ! fn_param ) <nl> + return NULL ; <nl> + <nl> return __dma_request_channel ( mask , fn , fn_param ); <nl> } <nl> # endif /* DMAENGINE_H */
long btrfs_ioctl_send ( struct file * mnt_file , void __user * arg_ ) <nl> goto out ; <nl> } <nl>  <nl> + if ( arg -> clone_sources_count > <nl> + ULLONG_MAX / sizeof (* arg -> clone_sources )) { <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> if (! access_ok ( VERIFY_READ , arg -> clone_sources , <nl> sizeof (* arg -> clone_sources ) * <nl> arg -> clone_sources_count )) {
static int sur40_probe ( struct usb_interface * interface , <nl> sur40 -> alloc_ctx = vb2_dma_sg_init_ctx ( sur40 -> dev ); <nl> if ( IS_ERR ( sur40 -> alloc_ctx )) { <nl> dev_err ( sur40 -> dev , " Can ' t allocate buffer context "); <nl> + error = PTR_ERR ( sur40 -> alloc_ctx ); <nl> goto err_unreg_v4l2 ; <nl> } <nl> 
int mmc_send_if_cond ( struct mmc_host * host , u32 ocr ) <nl> static const u8 test_pattern = 0xAA ; <nl> u8 result_pattern ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( struct mmc_command )); <nl> + <nl> /* <nl> * To support SD 2 . 0 cards , we must always invoke SD_SEND_IF_COND <nl> * before SD_APP_OP_COND . This command will harmlessly fail for
static void picolcd_debug_out_report ( struct picolcd_data * data , <nl> report -> id , raw_size ); <nl> hid_debug_event ( hdev , buff ); <nl> if ( raw_size + 5 > sizeof ( raw_data )) { <nl> + kfree ( buff ); <nl> hid_debug_event ( hdev , " TOO BIG \ n "); <nl> return ; <nl> } else {
void ath6kl_rx ( struct htc_target * target , struct htc_packet * packet ) <nl> /* aggregation code will handle the skb */ <nl> return ; <nl> } <nl> - } <nl> + } else if (! is_broadcast_ether_addr ( datap -> h_dest )) <nl> + vif -> net_stats . multicast ++; <nl>  <nl> ath6kl_deliver_frames_to_nw_stack ( vif -> ndev , skb ); <nl> }
static int ieee80211_change_station ( struct wiphy * wiphy , <nl> } <nl>  <nl> if ( params -> vlan -> ieee80211_ptr -> use_4addr ) { <nl> - if ( vlansdata -> u . vlan . sta ) <nl> + if ( vlansdata -> u . vlan . sta ) { <nl> + rcu_read_unlock (); <nl> return - EBUSY ; <nl> + } <nl>  <nl> rcu_assign_pointer ( vlansdata -> u . vlan . sta , sta ); <nl> }
static void unicast_arp_send ( struct sk_buff * skb , struct net_device * dev , <nl> skb_push ( skb , sizeof * phdr ); <nl> __skb_queue_tail (& path -> queue , skb ); <nl>  <nl> - if ( path_rec_start ( dev , path )) { <nl> + if (! path -> query && path_rec_start ( dev , path )) { <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl> path_free ( dev , path ); <nl> return ;
typedef __s64 int64_t ; <nl> # endif <nl>  <nl> /* this is a special 64bit data type that is 8 - byte aligned */ <nl> -# define aligned_u64 unsigned long long __attribute__ (( aligned ( 8 ))) <nl> +# define aligned_u64 __u64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_be64 __be64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_le64 __le64 __attribute__ (( aligned ( 8 ))) <nl> 
int beiscsi_cmd_mccq_create ( struct beiscsi_hba * phba , <nl> spin_lock (& phba -> ctrl . mbox_lock ); <nl> ctrl = & phba -> ctrl ; <nl> wrb = wrb_from_mbox (& ctrl -> mbox_mem ); <nl> + memset ( wrb , 0 , sizeof (* wrb )); <nl> req = embedded_payload ( wrb ); <nl> ctxt = & req -> context ; <nl> 
static int twl6040_vibra_probe ( struct platform_device * pdev ) <nl>  <nl> info -> input_dev -> name = " twl6040 : vibrator "; <nl> info -> input_dev -> id . version = 1 ; <nl> - info -> input_dev -> dev . parent = pdev -> dev . parent ; <nl> info -> input_dev -> close = twl6040_vibra_close ; <nl> __set_bit ( FF_RUMBLE , info -> input_dev -> ffbit ); <nl> 
static void valleyview_disable_rps ( struct drm_device * dev ) <nl>  <nl> int intel_enable_rc6 ( const struct drm_device * dev ) <nl> { <nl> + /* No RC6 before Ironlake */ <nl> + if ( INTEL_INFO ( dev )-> gen < 5 ) <nl> + return 0 ; <nl> + <nl> /* Respect the kernel parameter if it is set */ <nl> if ( i915_enable_rc6 >= 0 ) <nl> return i915_enable_rc6 ;
int btrfs_read_chunk_tree ( struct btrfs_root * root ) <nl> key . type = 0 ; <nl> again : <nl> ret = btrfs_search_slot ( NULL , root , & key , path , 0 , 0 ); <nl> + if ( ret < 0 ) <nl> + goto error ; <nl> while ( 1 ) { <nl> leaf = path -> nodes [ 0 ]; <nl> slot = path -> slots [ 0 ];
int fib_nh_match ( struct fib_config * cfg , struct fib_info * fi , <nl> fi -> fib_nh , cfg , extack )) <nl> return 1 ; <nl> } <nl> +# ifdef CONFIG_IP_ROUTE_CLASSID <nl> + if ( cfg -> fc_flow && <nl> + cfg -> fc_flow != fi -> fib_nh -> nh_tclassid ) <nl> + return 1 ; <nl> +# endif <nl> if ((! cfg -> fc_oif || cfg -> fc_oif == fi -> fib_nh -> nh_oif ) && <nl> (! cfg -> fc_gw || cfg -> fc_gw == fi -> fib_nh -> nh_gw )) <nl> return 0 ;
static unsigned int xdr_set_page_base ( struct xdr_stream * xdr , <nl> void * kaddr ; <nl>  <nl> maxlen = xdr -> buf -> page_len ; <nl> - if ( base >= maxlen ) { <nl> - base = maxlen ; <nl> - maxlen = 0 ; <nl> - } else <nl> + if ( base >= maxlen ) <nl> + return 0 ; <nl> + else <nl> maxlen -= base ; <nl> if ( len > maxlen ) <nl> len = maxlen ;
lba_legacy_resources ( struct parisc_device * pa_dev , struct lba_device * lba_dev ) <nl> r -> name = " LBA PCI Busses "; <nl> r -> start = lba_num & 0xff ; <nl> r -> end = ( lba_num >> 8 ) & 0xff ; <nl> + r -> flags = IORESOURCE_BUS ; <nl>  <nl> /* Set up local PCI Bus resources - we don ' t need them for <nl> ** Legacy boxes but it ' s nice to see in / proc / iomem .
drm_est3_modes ( struct drm_connector * connector , struct detailed_timing * timing ) <nl> u8 * est = (( u8 *) timing ) + 5 ; <nl>  <nl> for ( i = 0 ; i < 6 ; i ++) { <nl> - for ( j = 7 ; j > 0 ; j --) { <nl> + for ( j = 7 ; j >= 0 ; j --) { <nl> m = ( i * 8 ) + ( 7 - j ); <nl> if ( m >= ARRAY_SIZE ( est3_modes )) <nl> break ;
static int trunc_start ( struct gfs2_inode * ip , u64 size ) <nl>  <nl> if ( gfs2_is_stuffed ( ip )) { <nl> u64 dsize = size + sizeof ( struct gfs2_inode ); <nl> + ip -> i_disksize = size ; <nl> ip -> i_inode . i_mtime = ip -> i_inode . i_ctime = CURRENT_TIME ; <nl> gfs2_trans_add_bh ( ip -> i_gl , dibh , 1 ); <nl> gfs2_dinode_out ( ip , dibh -> b_data );
ips_link_to_i915_driver ( void ) <nl> EXPORT_SYMBOL_GPL ( ips_link_to_i915_driver ); <nl>  <nl> static const struct pci_device_id ips_id_table [] = { <nl> - { PCI_DEVICE ( PCI_VENDOR_ID_INTEL , <nl> - PCI_DEVICE_ID_INTEL_THERMAL_SENSOR ), }, <nl> + { PCI_VDEVICE ( INTEL , PCI_DEVICE_ID_INTEL_THERMAL_SENSOR ), }, <nl> { 0 , } <nl> }; <nl> 
static int __init cell_iommu_fixed_mapping_init ( void ) <nl> fbase = _ALIGN_UP ( fbase , 1 << IO_SEGMENT_SHIFT ); <nl> fsize = lmb_phys_mem_size (); <nl>  <nl> - if (( fbase + fsize ) <= 0x800000000 ) <nl> + if (( fbase + fsize ) <= 0x800000000ul ) <nl> hbase = 0 ; /* use the device tree window */ <nl> else { <nl> /* If we ' re over 32 GB we need to cheat . We can ' t map all of
static __u8 * cp_report_fixup ( struct hid_device * hdev , __u8 * rdesc , <nl> if (!( quirks & CP_RDESC_SWAPPED_MIN_MAX )) <nl> return rdesc ; <nl>  <nl> + if (* rsize < 4 ) <nl> + return rdesc ; <nl> + <nl> for ( i = 0 ; i < * rsize - 4 ; i ++) <nl> if ( rdesc [ i ] == 0x29 && rdesc [ i + 2 ] == 0x19 ) { <nl> rdesc [ i ] = 0x19 ;
void dlm_add_cb ( struct dlm_lkb * lkb , uint32_t flags , int mode , int status , <nl>  <nl> spin_lock (& dlm_cb_seq_spin ); <nl> new_seq = ++ dlm_cb_seq ; <nl> + if (! dlm_cb_seq ) <nl> + new_seq = ++ dlm_cb_seq ; <nl> spin_unlock (& dlm_cb_seq_spin ); <nl>  <nl> if ( lkb -> lkb_flags & DLM_IFL_USER ) {
lpfc_els_flush_cmd ( struct lpfc_vport * vport ) <nl> */ <nl> spin_lock_irq (& phba -> hbalock ); <nl> pring = lpfc_phba_elsring ( phba ); <nl> + <nl> + /* Bail out if we ' ve no ELS wq , like in PCI error recovery case . */ <nl> + if ( unlikely (! pring )) { <nl> + spin_unlock_irq (& phba -> hbalock ); <nl> + return ; <nl> + } <nl> + <nl> if ( phba -> sli_rev == LPFC_SLI_REV4 ) <nl> spin_lock (& pring -> ring_lock ); <nl> 
static void ax25_kill_by_device ( struct net_device * dev ) <nl> lock_sock ( sk ); <nl> s -> ax25_dev = NULL ; <nl> ax25_dev_put ( ax25_dev ); <nl> - release_sock ( sk ); <nl> ax25_disconnect ( s , ENETUNREACH ); <nl> + release_sock ( sk ); <nl> spin_lock_bh (& ax25_list_lock ); <nl> sock_put ( sk ); <nl> /* The entry could have been deleted from the
int perf_uprobe_init ( struct perf_event * p_event , bool is_retprobe ) <nl> return - ENOMEM ; <nl> ret = strncpy_from_user ( <nl> path , u64_to_user_ptr ( p_event -> attr . uprobe_path ), PATH_MAX ); <nl> + if ( ret == PATH_MAX ) <nl> + return - E2BIG ; <nl> if ( ret < 0 ) <nl> goto out ; <nl> if ( path [ 0 ] == '\ 0 ') {
struct ti_ohci { <nl>  <nl> static inline int cross_bound ( unsigned long addr , unsigned int size ) <nl> { <nl> + if ( size == 0 ) <nl> + return 0 ; <nl> + <nl> if ( size > PAGE_SIZE ) <nl> return 1 ; <nl> 
static struct shash_alg alg = { <nl> . export = md5_export , <nl> . import = md5_import , <nl> . descsize = sizeof ( struct md5_state ), <nl> + . statesize = sizeof ( struct md5_state ), <nl> . base = { <nl> . cra_name = " md5 ", <nl> . cra_flags = CRYPTO_ALG_TYPE_SHASH ,
static int handle_conflicting_encoders ( struct drm_atomic_state * state , <nl>  <nl> if ( funcs -> atomic_best_encoder ) <nl> new_encoder = funcs -> atomic_best_encoder ( connector , conn_state ); <nl> - else <nl> + else if ( funcs -> best_encoder ) <nl> new_encoder = funcs -> best_encoder ( connector ); <nl> + else <nl> + new_encoder = drm_atomic_helper_best_encoder ( connector ); <nl>  <nl> if ( new_encoder ) { <nl> if ( encoder_mask & ( 1 << drm_encoder_index ( new_encoder ))) {
* under normal circumstances , used to verify that nobody uses <nl> * non - initialized list entries . <nl> */ <nl> -# define LIST_POISON1 (( void *) 0x00100100 + POISON_POINTER_DELTA ) <nl> -# define LIST_POISON2 (( void *) 0x00200200 + POISON_POINTER_DELTA ) <nl> +# define LIST_POISON1 (( void *) 0x100 + POISON_POINTER_DELTA ) <nl> +# define LIST_POISON2 (( void *) 0x200 + POISON_POINTER_DELTA ) <nl>  <nl> /********** include / linux / timer . h **********/ <nl> /*
static int blkif_release ( struct inode * inode , struct file * filep ) <nl> struct xenbus_device * dev = info -> xbdev ; <nl> enum xenbus_state state = xenbus_read_driver_state ( dev -> otherend ); <nl>  <nl> - if ( state == XenbusStateClosing ) <nl> + if ( state == XenbusStateClosing && info -> is_ready ) <nl> blkfront_closing ( dev ); <nl> } <nl> return 0 ;
static int ibmvscsi_probe ( struct vio_dev * vdev , const struct vio_device_id * id ) <nl> host -> max_lun = 8 ; <nl> host -> max_id = max_id ; <nl> host -> max_channel = max_channel ; <nl> + host -> max_cmd_len = 16 ; <nl>  <nl> if ( scsi_add_host ( hostdata -> host , hostdata -> dev )) <nl> goto add_host_failed ;
static int ieee80211_get_key ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> if ( pairwise && key_idx < NUM_DEFAULT_KEYS ) <nl> key = rcu_dereference ( sta -> ptk [ key_idx ]); <nl> - else if (! pairwise && key_idx < NUM_DEFAULT_KEYS ) <nl> + else if (! pairwise && <nl> + key_idx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ) <nl> key = rcu_dereference ( sta -> gtk [ key_idx ]); <nl> } else <nl> key = rcu_dereference ( sdata -> keys [ key_idx ]);
static void i40e_service_task ( struct work_struct * work ) <nl> service_task ); <nl> unsigned long start_time = jiffies ; <nl>  <nl> + /* don ' t bother with service tasks if a reset is in progress */ <nl> + if ( test_bit ( __I40E_RESET_RECOVERY_PENDING , & pf -> state )) { <nl> + i40e_service_event_complete ( pf ); <nl> + return ; <nl> + } <nl> + <nl> i40e_reset_subtask ( pf ); <nl> i40e_handle_mdd_event ( pf ); <nl> i40e_vc_process_vflr_event ( pf );
static int poll_select_copy_remaining ( struct timespec * end_time , void __user * p , <nl> rts . tv_sec = rts . tv_nsec = 0 ; <nl>  <nl> if ( timeval ) { <nl> + if ( sizeof ( rtv ) > sizeof ( rtv . tv_sec ) + sizeof ( rtv . tv_usec )) <nl> + memset (& rtv , 0 , sizeof ( rtv )); <nl> rtv . tv_sec = rts . tv_sec ; <nl> rtv . tv_usec = rts . tv_nsec / NSEC_PER_USEC ; <nl> 
struct dentry * ovl_lookup ( struct inode * dir , struct dentry * dentry , <nl> } <nl>  <nl> if ( d . redirect ) { <nl> + err = - ENOMEM ; <nl> upperredirect = kstrdup ( d . redirect , GFP_KERNEL ); <nl> if (! upperredirect ) <nl> goto out_put_upper ;
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl>  <nl> void oprofile_arch_exit ( void ) <nl> { <nl> - model -> exit (); <nl> + if ( model ) <nl> + model -> exit (); <nl> }
void qla4xxx_free_ddb_index ( struct scsi_qla_host * ha ) <nl> ret = qla4xxx_get_fwddb_entry ( ha , idx , NULL , 0 , NULL , <nl> & next_idx , & state , & conn_err , <nl> NULL , NULL ); <nl> - if ( ret == QLA_ERROR ) <nl> + if ( ret == QLA_ERROR ) { <nl> + next_idx ++; <nl> continue ; <nl> + } <nl> if ( state == DDB_DS_NO_CONNECTION_ACTIVE || <nl> state == DDB_DS_SESSION_FAILED ) { <nl> DEBUG2 ( ql4_printk ( KERN_INFO , ha ,
int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> if ( IS_ERR ( device )) { <nl> if ( PTR_ERR ( device ) == - ENOENT && <nl> - strcmp ( device_path , " missing ") == 0 ) <nl> + device_path && strcmp ( device_path , " missing ") == 0 ) <nl> ret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND ; <nl> else <nl> ret = PTR_ERR ( device );
static int ocfs2_remove_inode_range ( struct inode * inode , <nl> ocfs2_truncate_cluster_pages ( inode , byte_start , byte_len ); <nl>  <nl> out : <nl> + ocfs2_free_path ( path ); <nl> ocfs2_schedule_truncate_log_flush ( osb , 1 ); <nl> ocfs2_run_deallocs ( osb , & dealloc ); <nl> 
uint16_t fixed_point_to_int_frac ( <nl> arg )); <nl>  <nl> if ( d <= ( uint16_t )( 1 << integer_bits ) - ( 1 / ( uint16_t ) divisor )) <nl> - numerator = ( uint16_t ) dal_fixed31_32_floor ( <nl> + numerator = ( uint16_t ) dal_fixed31_32_round ( <nl> dal_fixed31_32_mul_int ( <nl> arg , <nl> divisor ));
static noinline struct module * load_module ( void __user * umod , <nl> free_unload : <nl> module_unload_free ( mod ); <nl> # if defined ( CONFIG_MODULE_UNLOAD ) && defined ( CONFIG_SMP ) <nl> - free_init : <nl> percpu_modfree ( mod -> refptr ); <nl> + free_init : <nl> # endif <nl> module_free ( mod , mod -> module_init ); <nl> free_core :
static void xilinx_msi_teardown_irq ( struct msi_controller * chip , <nl> unsigned int irq ) <nl> { <nl> xilinx_pcie_destroy_msi ( irq ); <nl> + irq_dispose_mapping ( irq ); <nl> } <nl>  <nl> /**
sg_unlink_reserve ( Sg_fd * sfp , Sg_request * srp ) <nl> req_schp -> page_order = 0 ; <nl> req_schp -> sglist_len = 0 ; <nl> srp -> res_used = 0 ; <nl> + /* Called without mutex lock to avoid deadlock */ <nl> + sfp -> res_in_use = 0 ; <nl> } <nl>  <nl> static Sg_request *
static int imx_ssi_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> ssi -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( ssi -> irq < 0 ) { <nl> + dev_err (& pdev -> dev , " Failed to get IRQ : % d \ n ", ssi -> irq ); <nl> + return ssi -> irq ; <nl> + } <nl>  <nl> ssi -> clk = devm_clk_get (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( ssi -> clk )) {
struct dma_buf * exynos_dmabuf_prime_export ( struct drm_device * drm_dev , <nl> struct exynos_drm_gem_obj * exynos_gem_obj = to_exynos_gem_obj ( obj ); <nl>  <nl> return dma_buf_export ( exynos_gem_obj , & exynos_dmabuf_ops , <nl> - exynos_gem_obj -> base . size , 0600 ); <nl> + exynos_gem_obj -> base . size , flags ); <nl> } <nl>  <nl> struct drm_gem_object * exynos_dmabuf_prime_import ( struct drm_device * drm_dev ,
struct perf_evsel * perf_evlist__id2evsel ( struct perf_evlist * evlist , u64 id ) <nl> hlist_for_each_entry ( sid , pos , head , node ) <nl> if ( sid -> id == id ) <nl> return sid -> evsel ; <nl> + <nl> + if (! perf_evlist__sample_id_all ( evlist )) <nl> + return list_entry ( evlist -> entries . next , struct perf_evsel , node ); <nl> + <nl> return NULL ; <nl> } <nl> 
static int soc_tplg_dai_create ( struct soc_tplg * tplg , <nl> set_stream_info ( stream , caps ); <nl> } <nl>  <nl> + if ( pcm -> compress ) <nl> + dai_drv -> compress_new = snd_soc_new_compress ; <nl> + <nl> /* pass control to component driver for optional further init */ <nl> ret = soc_tplg_dai_load ( tplg , dai_drv , pcm , NULL ); <nl> if ( ret < 0 ) {
i915_gem_execbuffer_reserve ( struct intel_ring_buffer * ring , <nl>  <nl> obj -> base . pending_read_domains = 0 ; <nl> obj -> base . pending_write_domain = 0 ; <nl> + obj -> pending_fenced_gpu_access = false ; <nl> } <nl> list_splice (& ordered_objects , objects ); <nl> 
xfs_btree_simple_query_range ( <nl> if ( error ) <nl> goto out ; <nl>  <nl> + /* Nothing ? See if there ' s anything to the right . */ <nl> + if (! stat ) { <nl> + error = xfs_btree_increment ( cur , 0 , & stat ); <nl> + if ( error ) <nl> + goto out ; <nl> + } <nl> + <nl> while ( stat ) { <nl> /* Find the record . */ <nl> error = xfs_btree_get_rec ( cur , & recp , & stat );
static void * etm_setup_aux ( int event_cpu , void ** pages , <nl> if (! sink_ops ( sink )-> alloc_buffer ) <nl> goto err ; <nl>  <nl> + cpu = cpumask_first ( mask ); <nl> /* Get the AUX specific data from the sink buffer */ <nl> event_data -> snk_config = <nl> sink_ops ( sink )-> alloc_buffer ( sink , cpu , pages ,
void speakup_remove_virtual_keyboard ( void ) <nl> { <nl> if ( virt_keyboard != NULL ) { <nl> input_unregister_device ( virt_keyboard ); <nl> - input_free_device ( virt_keyboard ); <nl> virt_keyboard = NULL ; <nl> } <nl> }
static void do_pata_set_dmamode ( struct ata_port * ap , struct ata_device * adev , i <nl> u16 master_data ; <nl> u8 speed = adev -> dma_mode ; <nl> int devid = adev -> devno + 2 * ap -> port_no ; <nl> - u8 udma_enable ; <nl> + u8 udma_enable = 0 ; <nl>  <nl> static const /* ISP RTC */ <nl> u8 timings [][ 2 ] = { { 0 , 0 },
static int dpaa2_eth_poll ( struct napi_struct * napi , int budget ) <nl> err = dpaa2_io_service_rearm ( NULL , & ch -> nctx ); <nl> cpu_relax (); <nl> } while ( err == - EBUSY ); <nl> + WARN_ONCE ( err , " CDAN notifications rearm failed on core % d ", <nl> + ch -> nctx . desired_cpu ); <nl> } <nl>  <nl> ch -> stats . frames += cleaned ;
static int clone_backref_node ( struct btrfs_trans_handle * trans , <nl> new_node -> bytenr = dest -> node -> start ; <nl> new_node -> level = node -> level ; <nl> new_node -> lowest = node -> lowest ; <nl> + new_node -> checked = 1 ; <nl> new_node -> root = dest ; <nl>  <nl> if (! node -> lowest ) {
static int das16m1_attach ( struct comedi_device * dev , <nl>  <nl> s = & dev -> subdevices [ 3 ]; <nl> /* 8255 */ <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> extra_iobase ); <nl> + ret = subdev_8255_init ( dev , s , NULL , devpriv -> extra_iobase ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* disable upper half of hardware conversion counter so it doesn ' t mess with us */ <nl> outb ( TOTAL_CLEAR , dev -> iobase + DAS16M1_8254_FIRST_CNTRL );
struct usb_function * ecm_alloc ( struct usb_function_instance * fi ) <nl> sizeof ( ecm -> ethaddr )); <nl> if ( status < 12 ) { <nl> kfree ( ecm ); <nl> + mutex_unlock (& opts -> lock ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> ecm_string_defs [ 1 ]. s = ecm -> ethaddr ;
static int __devinit wl1271_probe ( struct sdio_func * func , <nl> goto out_free ; <nl> } <nl>  <nl> + enable_irq_wake ( wl -> irq ); <nl> + <nl> disable_irq ( wl -> irq ); <nl>  <nl> ret = wl1271_init_ieee80211 ( wl ); <nl> static void __devexit wl1271_remove ( struct sdio_func * func ) <nl> pm_runtime_get_noresume (& func -> dev ); <nl>  <nl> wl1271_unregister_hw ( wl ); <nl> + disable_irq_wake ( wl -> irq ); <nl> free_irq ( wl -> irq , wl ); <nl> wl1271_free_hw ( wl ); <nl> }
void drbd_start_resync ( struct drbd_device * device , enum drbd_conns side ) <nl> return ; <nl> } <nl>  <nl> + if (! connection ) { <nl> + drbd_err ( device , " No connection to peer , aborting !\ n "); <nl> + return ; <nl> + } <nl> + <nl> if (! test_bit ( B_RS_H_DONE , & device -> flags )) { <nl> if ( side == C_SYNC_TARGET ) { <nl> /* Since application IO was locked out during C_WF_BITMAP_T and
static int do_md_run ( mddev_t * mddev ) <nl> if ( spares && mddev -> pers -> sync_request ) { <nl> mddev -> recovery = 0 ; <nl> set_bit ( MD_RECOVERY_RUNNING , & mddev -> recovery ); <nl> + set_bit ( MD_RECOVERY_RECOVER , & mddev -> recovery ); <nl> mddev -> sync_thread = md_register_thread ( md_do_sync , <nl> mddev , <nl> " resync ");
static int mv_udc_get_frame ( struct usb_gadget * gadget ) <nl>  <nl> udc = container_of ( gadget , struct mv_udc , gadget ); <nl>  <nl> - retval = readl ( udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl> + retval = readl (& udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl>  <nl> return retval ; <nl> }
static void __init smp_init_package_map ( void ) <nl> * primary cores . <nl> */ <nl> ncpus = boot_cpu_data . x86_max_cores ; <nl> + if (! ncpus ) { <nl> + pr_warn (" x86_max_cores == zero !?!?"); <nl> + ncpus = 1 ; <nl> + } <nl> + <nl> __max_logical_packages = DIV_ROUND_UP ( total_cpus , ncpus ); <nl>  <nl> /*
static int check_stack_boundary ( struct bpf_verifier_env * env , int regno , <nl> tnum_strn ( tn_buf , sizeof ( tn_buf ), regs [ regno ]. var_off ); <nl> verbose ( env , " invalid variable stack read R % d var_off =% s \ n ", <nl> regno , tn_buf ); <nl> + return - EACCES ; <nl> } <nl> off = regs [ regno ]. off + regs [ regno ]. var_off . value ; <nl> if ( off >= 0 || off < - MAX_BPF_STACK || off + access_size > 0 ||
static struct mfd_cell db8500_prcmu_devs [] = { <nl> . pdata_size = sizeof ( db8500_regulators ), <nl> }, <nl> { <nl> - . name = " cpufreq - u8500 ", <nl> - . of_compatible = " stericsson , cpufreq - u8500 ", <nl> + . name = " cpufreq - ux500 ", <nl> + . of_compatible = " stericsson , cpufreq - ux500 ", <nl> . platform_data = & db8500_cpufreq_table , <nl> . pdata_size = sizeof ( db8500_cpufreq_table ), <nl> },
static int __devinit i2o_pci_probe ( struct pci_dev * pdev , <nl> i2o_pci_free ( c ); <nl>  <nl> free_controller : <nl> - i2o_iop_free ( c ); <nl> put_device ( c -> device . parent ); <nl> + i2o_iop_free ( c ); <nl>  <nl> disable : <nl> pci_disable_device ( pdev );
static bool mem_cgroup_out_of_memory ( struct mem_cgroup * memcg , gfp_t gfp_mask , <nl> mem_cgroup_iter_break ( memcg , iter ); <nl> if ( chosen ) <nl> put_task_struct ( chosen ); <nl> + /* Set a dummy value to return " true ". */ <nl> + chosen = ( void *) 1 ; <nl> goto unlock ; <nl> case OOM_SCAN_OK : <nl> break ;
static int dcp_probe ( struct platform_device * pdev ) <nl>  <nl> r = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> dev -> dcp_regs_base = devm_ioremap_resource (& pdev -> dev , r ); <nl> + if ( IS_ERR ( dev -> dcp_regs_base )) <nl> + return PTR_ERR ( dev -> dcp_regs_base ); <nl>  <nl> dcp_set ( dev , DCP_CTRL_SFRST , DCP_REG_CTRL ); <nl> udelay ( 10 );
int mei_cl_disconnect ( struct mei_cl * cl ) <nl> cl_err ( dev , cl , " failed to disconnect .\ n "); <nl> goto free ; <nl> } <nl> + cl -> timer_count = MEI_CONNECT_TIMEOUT ; <nl> mdelay ( 10 ); /* Wait for hardware disconnection ready */ <nl> list_add_tail (& cb -> list , & dev -> ctrl_rd_list . list ); <nl> } else {
static void bus_reset_work ( struct work_struct * work ) <nl> { <nl> struct fw_ohci * ohci = <nl> container_of ( work , struct fw_ohci , bus_reset_work ); <nl> - int self_id_count , i , j , reg ; <nl> - int generation , new_generation ; <nl> + int self_id_count , generation , new_generation , i , j ; <nl> + u32 reg ; <nl> unsigned long flags ; <nl> void * free_rom = NULL ; <nl> dma_addr_t free_rom_bus = 0 ;
vsock_stream_recvmsg ( struct kiocb * kiocb , <nl> vsk = vsock_sk ( sk ); <nl> err = 0 ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state != SS_CONNECTED ) {
static int amdgpu_dm_atomic_check ( struct drm_device * dev , <nl> } <nl> } else { <nl> for_each_oldnew_crtc_in_state ( state , crtc , old_crtc_state , new_crtc_state , i ) { <nl> - if (! drm_atomic_crtc_needs_modeset ( new_crtc_state )) <nl> + if (! drm_atomic_crtc_needs_modeset ( new_crtc_state ) && <nl> + ! new_crtc_state -> color_mgmt_changed ) <nl> continue ; <nl>  <nl> if (! new_crtc_state -> enable )
int bnxt_re_create_srq ( struct ib_srq * ib_srq , <nl> dev_err ( rdev_to_dev ( rdev ), " SRQ copy to udata failed !"); <nl> bnxt_qplib_destroy_srq (& rdev -> qplib_res , <nl> & srq -> qplib_srq ); <nl> - goto exit ; <nl> + goto fail ; <nl> } <nl> } <nl> if ( nq )
cfg80211_inform_bss_frame ( struct wiphy * wiphy , <nl> return NULL ; <nl>  <nl> if ( WARN_ON ( wiphy -> signal_type == CFG80211_SIGNAL_TYPE_UNSPEC && <nl> - ( signal < 0 || signal > 100 ))) <nl> + ( signal < 0 || signal > 100 ))) <nl> return NULL ; <nl>  <nl> if ( WARN_ON ( len < offsetof ( struct ieee80211_mgmt , u . probe_resp . variable )))
void exit_shm ( struct task_struct * task ) <nl> { <nl> struct ipc_namespace * ns = task -> nsproxy -> ipc_ns ; <nl>  <nl> + if ( shm_ids ( ns ). in_use == 0 ) <nl> + return ; <nl> + <nl> /* Destroy all already created segments , but not mapped yet */ <nl> down_write (& shm_ids ( ns ). rw_mutex ); <nl> if ( shm_ids ( ns ). in_use )
static struct file * do_last ( struct nameidata * nd , struct path * path , <nl> if ( path -> dentry -> d_inode -> i_op -> follow_link ) <nl> return NULL ; <nl> error = - ENOTDIR ; <nl> - if (* want_dir & ! path -> dentry -> d_inode -> i_op -> lookup ) <nl> + if (* want_dir && ! path -> dentry -> d_inode -> i_op -> lookup ) <nl> goto exit_dput ; <nl> path_to_nameidata ( path , nd ); <nl> audit_inode ( pathname , nd -> path . dentry );
static __inline__ struct page * drm_do_vm_shm_nopage ( struct vm_area_struct * vma , <nl>  <nl> offset = address - vma -> vm_start ; <nl> i = ( unsigned long ) map -> handle + offset ; <nl> - page = vmalloc_to_page (( void *) i ); <nl> + page = ( map -> type == _DRM_CONSISTENT ) ? <nl> + virt_to_page (( void *) i ) : vmalloc_to_page (( void *) i ); <nl> if (! page ) <nl> return NOPAGE_OOM ; <nl> get_page ( page );
static void sdhci_prepare_data ( struct sdhci_host * host , struct mmc_command * cmd ) <nl> int sg_cnt ; <nl>  <nl> sg_cnt = sdhci_pre_dma_transfer ( host , data , NULL ); <nl> - if ( sg_cnt == 0 ) { <nl> + if ( sg_cnt <= 0 ) { <nl> /* <nl> * This only happens when someone fed <nl> * us an invalid request .
static int __init d40_of_probe ( struct platform_device * pdev , <nl> list = of_get_property ( np , " disabled - channels ", & num_disabled ); <nl> num_disabled /= sizeof (* list ); <nl>  <nl> - if ( num_disabled > STEDMA40_MAX_PHYS || num_disabled < 0 ) { <nl> + if ( num_disabled >= STEDMA40_MAX_PHYS || num_disabled < 0 ) { <nl> d40_err (& pdev -> dev , <nl> " Invalid number of disabled channels specified (% d )\ n ", <nl> num_disabled );
int usb_sg_init ( struct usb_sg_request * io , struct usb_device * dev , <nl> } <nl>  <nl> /* initialize all the urbs we ' ll use */ <nl> - io -> urbs = kmalloc ( io -> entries * sizeof * io -> urbs , mem_flags ); <nl> + io -> urbs = kmalloc ( io -> entries * sizeof (* io -> urbs ), mem_flags ); <nl> if (! io -> urbs ) <nl> goto nomem ; <nl> 
int __cpuinit local_timer_setup ( struct clock_event_device * evt ) <nl>  <nl> /* Use existing clock_event for cpu 0 */ <nl> if (! smp_processor_id ()) <nl> - return ; <nl> + return 0 ; <nl>  <nl> writel ( DGT_CLK_CTL_DIV_4 , MSM_TMR_BASE + DGT_CLK_CTL ); <nl> 
static const struct drm_i915_gem_object_ops i915_gem_phys_ops = { <nl> . release = i915_gem_object_release_phys , <nl> }; <nl>  <nl> - int <nl> - i915_gem_object_unbind ( struct drm_i915_gem_object * obj ) <nl> + int i915_gem_object_unbind ( struct drm_i915_gem_object * obj ) <nl> { <nl> struct i915_vma * vma ; <nl> LIST_HEAD ( still_in_list ); <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> /* The vma will only be freed if it is marked as closed , and if we wait <nl> * upon rendering to the vma , we may unbind anything in the list .
int batadv_bla_tx ( struct batadv_priv * bat_priv , struct sk_buff * skb , <nl> if (! atomic_read (& bat_priv -> bridge_loop_avoidance )) <nl> goto allow ; <nl>  <nl> - /* in VLAN case , the mac header might not be set . */ <nl> - skb_reset_mac_header ( skb ); <nl> - <nl> if ( batadv_bla_process_claim ( bat_priv , primary_if , skb )) <nl> goto handled ; <nl> 
static void hidinput_configure_usage ( struct hid_input * hidinput , struct hid_fiel <nl> case 0x0b6 : map_key_clear ( KEY_PREVIOUSSONG ); break ; <nl> case 0x0b7 : map_key_clear ( KEY_STOPCD ); break ; <nl> case 0x0b8 : map_key_clear ( KEY_EJECTCD ); break ; <nl> + case 0x0bc : map_key_clear ( KEY_MEDIA_REPEAT ); break ; <nl>  <nl> case 0x0cd : map_key_clear ( KEY_PLAYPAUSE ); break ; <nl> case 0x0e0 : map_abs_clear ( ABS_VOLUME ); break ;
static int skge_down ( struct net_device * dev ) <nl> struct skge_hw * hw = skge -> hw ; <nl> int port = skge -> port ; <nl>  <nl> - if ( skge -> mem == NULL ) <nl> + if (! skge -> mem ) <nl> return 0 ; <nl>  <nl> netif_info ( skge , ifdown , skge -> netdev , " disabling interface \ n ");
void rds_inc_info_copy ( struct rds_incoming * inc , <nl> minfo . fport = inc -> i_hdr . h_dport ; <nl> } <nl>  <nl> + minfo . flags = 0 ; <nl> + <nl> rds_info_copy ( iter , & minfo , sizeof ( minfo )); <nl> }
static int si21_writeregs ( struct si21xx_state * state , u8 reg1 , <nl> . len = len + 1 <nl> }; <nl>  <nl> + if ( len > sizeof ( buf ) - 1 ) <nl> + return - EINVAL ; <nl> + <nl> msg . buf [ 0 ] = reg1 ; <nl> memcpy ( msg . buf + 1 , data , len ); <nl> 
static void isd200_ata_command ( struct scsi_cmnd * srb , struct us_data * us ) <nl>  <nl> /* Make sure driver was initialized */ <nl>  <nl> - if ( us -> extra == NULL ) <nl> + if ( us -> extra == NULL ) { <nl> usb_stor_dbg ( us , " ERROR Driver not initialized \ n "); <nl> + srb -> result = DID_ERROR << 16 ; <nl> + return ; <nl> + } <nl>  <nl> scsi_set_resid ( srb , 0 ); <nl> /* scsi_bufflen might change in protocol translation to ata */
static enum page_references page_check_references ( struct page * page , <nl> */ <nl> SetPageReferenced ( page ); <nl>  <nl> - if ( referenced_page ) <nl> + if ( referenced_page || referenced_ptes > 1 ) <nl> return PAGEREF_ACTIVATE ; <nl>  <nl> return PAGEREF_KEEP ;
static int wanxl_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> ifr -> ifr_settings . size = size ; /* data size wanted */ <nl> return - ENOBUFS ; <nl> } <nl> + memset (& line , 0 , sizeof ( line )); <nl> line . clock_type = get_status ( port )-> clocking ; <nl> line . clock_rate = 0 ; <nl> line . loopback = 0 ;
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
static int yurex_probe ( struct usb_interface * interface , const struct usb_device_ <nl> usb_rcvintpipe ( dev -> udev , dev -> int_in_endpointAddr ), <nl> dev -> int_buffer , YUREX_BUF_SIZE , yurex_interrupt , <nl> dev , 1 ); <nl> - dev -> cntl_urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> + dev -> urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> if ( usb_submit_urb ( dev -> urb , GFP_KERNEL )) { <nl> retval = - EIO ; <nl> err (" Could not submitting URB ");
__init int intel_pmu_init ( void ) <nl> if ( version > 1 ) <nl> x86_pmu . num_counters_fixed = max (( int ) edx . split . num_counters_fixed , 3 ); <nl>  <nl> - /* <nl> - * v2 and above have a perf capabilities MSR <nl> - */ <nl> - if ( version > 1 ) { <nl> + if ( boot_cpu_has ( X86_FEATURE_PDCM )) { <nl> u64 capabilities ; <nl>  <nl> rdmsrl ( MSR_IA32_PERF_CAPABILITIES , capabilities );
typedef struct kl_config_hdr { <nl> /* --- New Macros for the changed kl_config_hdr_t structure --- */ <nl>  <nl> # define PTR_CH_MALLOC_HDR ( _k ) (( klc_malloc_hdr_t *)\ <nl> - ( unsigned long ) _k + ( _k -> ch_malloc_hdr_off ))) <nl> + (( unsigned long ) _k + ( _k -> ch_malloc_hdr_off ))) <nl>  <nl> # define KL_CONFIG_CH_MALLOC_HDR ( _n ) PTR_CH_MALLOC_HDR ( KL_CONFIG_HDR ( _n )) <nl> 
static int ismt_access ( struct i2c_adapter * adap , u16 addr , <nl>  <nl> case I2C_SMBUS_BLOCK_PROC_CALL : <nl> dev_dbg ( dev , " I2C_SMBUS_BLOCK_PROC_CALL \ n "); <nl> + if ( data -> block [ 0 ] > I2C_SMBUS_BLOCK_MAX ) <nl> + return - EINVAL ; <nl> + <nl> dma_size = I2C_SMBUS_BLOCK_MAX ; <nl> desc -> tgtaddr_rw = ISMT_DESC_ADDR_RW ( addr , 1 ); <nl> desc -> wr_len_cmd = data -> block [ 0 ] + 1 ;
void btrfs_invalidate_inodes ( struct btrfs_root * root ) <nl> struct inode * inode ; <nl> u64 objectid = 0 ; <nl>  <nl> - WARN_ON ( btrfs_root_refs (& root -> root_item ) != 0 ); <nl> + if (! test_bit ( BTRFS_FS_STATE_ERROR , & root -> fs_info -> fs_state )) <nl> + WARN_ON ( btrfs_root_refs (& root -> root_item ) != 0 ); <nl>  <nl> spin_lock (& root -> inode_lock ); <nl> again :
static int sfi_cpufreq_cpu_init ( struct cpufreq_policy * policy ) <nl> { <nl> policy -> shared_type = CPUFREQ_SHARED_TYPE_HW ; <nl> policy -> cpuinfo . transition_latency = 100000 ; /* 100us */ <nl> + policy -> freq_table = freq_table ; <nl>  <nl> - return cpufreq_table_validate_and_show ( policy , freq_table ); <nl> + return 0 ; <nl> } <nl>  <nl> static struct cpufreq_driver sfi_cpufreq_driver = {
void bochs_fbdev_fini ( struct bochs_device * bochs ) <nl> if ( bochs -> fb . initialized ) <nl> bochs_fbdev_destroy ( bochs ); <nl>  <nl> - drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + if ( bochs -> fb . helper . fbdev ) <nl> + drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + <nl> bochs -> fb . initialized = false ; <nl> }
unsigned int kstat_irqs ( unsigned int irq ) <nl> */ <nl> unsigned int kstat_irqs_usr ( unsigned int irq ) <nl> { <nl> - int sum ; <nl> + unsigned int sum ; <nl>  <nl> irq_lock_sparse (); <nl> sum = kstat_irqs ( irq );
static void process_init_reply ( struct fuse_conn * fc , struct fuse_req * req ) <nl> int i ; <nl> struct fuse_init_out * arg = & req -> misc . init_out ; <nl>  <nl> - if ( arg -> major != FUSE_KERNEL_VERSION ) <nl> + if ( req -> out . h . error || arg -> major != FUSE_KERNEL_VERSION ) <nl> fc -> conn_error = 1 ; <nl> else { <nl> fc -> minor = arg -> minor ;
int __init pci_legacy_init ( void ) <nl>  <nl> return 0 ; <nl> } <nl> - EXPORT_SYMBOL_GPL ( pci_legacy_init ); <nl>  <nl> void pcibios_scan_specific_bus ( int busn ) <nl> {
static int dim2_probe ( struct platform_device * pdev ) <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - dev -> disable_platform = pdata ? pdata -> disable : 0 ; <nl> + dev -> disable_platform = pdata ? pdata -> disable : NULL ; <nl>  <nl> dev_info (& pdev -> dev , " sync : num of frames per sub - buffer : % u \ n ", fcnt ); <nl> hal_ret = dim_startup ( dev -> io_base , dev -> clk_speed , fcnt );
static int zd1201_resume ( struct usb_interface * interface ) <nl> { <nl> struct zd1201 * zd = usb_get_intfdata ( interface ); <nl>  <nl> + if (! zd || ! zd -> dev ) <nl> + return - ENODEV ; <nl> + <nl> netif_device_attach ( zd -> dev ); <nl>  <nl> if ( zd -> was_enabled )
static int cqspi_setup_flash ( struct cqspi_st * cqspi , struct device_node * np ) <nl> goto err ; <nl> } <nl>  <nl> - if ( cs > CQSPI_MAX_CHIPSELECT ) { <nl> + if ( cs >= CQSPI_MAX_CHIPSELECT ) { <nl> dev_err ( dev , " Chip select % d out of range .\ n ", cs ); <nl> goto err ; <nl> }
int azx_codec_configure ( struct azx * chip ) <nl> list_for_each_codec_safe ( codec , next , & chip -> bus ) { <nl> snd_hda_codec_configure ( codec ); <nl> } <nl> + <nl> + if (! azx_bus ( chip )-> num_codecs ) <nl> + return - ENODEV ; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( azx_codec_configure );
static struct net_device * fs_init_instance ( struct device * dev , <nl> if ( registered ) <nl> unregister_netdev ( ndev ); <nl>  <nl> - if ( fep != NULL ) { <nl> + if ( fep && fep -> ops ) { <nl> (* fep -> ops -> free_bd )( ndev ); <nl> (* fep -> ops -> cleanup_data )( ndev ); <nl> }
static struct sk_buff * isdn_ppp_decompress ( struct sk_buff * skb , struct ippp_struc <nl> rsparm . maxdlen = IPPP_RESET_MAXDATABYTES ; <nl>  <nl> skb_out = dev_alloc_skb ( is -> mru + PPP_HDRLEN ); <nl> + if (! skb_out ) { <nl> + kfree_skb ( skb ); <nl> + printk ( KERN_ERR " ippp : decomp memory allocation failure \ n "); <nl> + return NULL ; <nl> + } <nl> len = ipc -> decompress ( stat , skb , skb_out , & rsparm ); <nl> kfree_skb ( skb ); <nl> if ( len <= 0 ) {
static void pcie_disable_notification ( struct controller * ctrl ) <nl> u16 mask ; <nl> mask = ( PCI_EXP_SLTCTL_PDCE | PCI_EXP_SLTCTL_ABPE | <nl> PCI_EXP_SLTCTL_MRLSCE | PCI_EXP_SLTCTL_PFDE | <nl> - PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE ); <nl> + PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE | <nl> + PCI_EXP_SLTCTL_DLLSCE ); <nl> if ( pcie_write_cmd ( ctrl , 0 , mask )) <nl> ctrl_warn ( ctrl , " Cannot disable software notification \ n "); <nl> }
static ssize_t write_ports ( struct file * file , char * buf , size_t size ) <nl> /* Decrease the count , but don ' t shutdown the <nl> * the service <nl> */ <nl> + lock_kernel (); <nl> nfsd_serv -> sv_nrthreads --; <nl> + unlock_kernel (); <nl> } <nl> return err ; <nl> }
static struct tnode * tnode_new ( t_key key , int pos , int bits ) <nl> } <nl>  <nl> pr_debug (" AT % p s =% zu % zu \ n ", tn , sizeof ( struct tnode ), <nl> - sizeof ( struct rt_trie_node ) << bits ); <nl> + sizeof ( struct rt_trie_node *) << bits ); <nl> return tn ; <nl> } <nl> 
int i2400m_op_rfkill_sw_toggle ( struct wimax_dev * wimax_dev , <nl> "% d \ n ", result ); <nl> result = 0 ; <nl> error_cmd : <nl> - kfree ( cmd ); <nl> kfree_skb ( ack_skb ); <nl> error_msg_to_dev : <nl> error_alloc : <nl> d_fnend ( 4 , dev , "( wimax_dev % p state % d ) = % d \ n ", <nl> wimax_dev , state , result ); <nl> + kfree ( cmd ); <nl> return result ; <nl> } <nl> 
static int iscsit_build_sendtargets_response ( struct iscsi_cmd * cmd ) <nl> if (! text_ptr ) { <nl> pr_err (" Unable to locate '=' string in text_in :" <nl> " % s \ n ", text_in ); <nl> + kfree ( payload ); <nl> return - EINVAL ; <nl> } <nl> /*
static void ds1374_set_tlet ( ulong arg ) <nl> " can ' t confirm time set from rtc chip \ n "); <nl> } <nl>  <nl> - ulong new_time ; <nl> + static ulong new_time ; <nl>  <nl> DECLARE_TASKLET_DISABLED ( ds1374_tasklet , ds1374_set_tlet , ( ulong ) & new_time ); <nl> 
struct inode * reiserfs_iget ( struct super_block * s , const struct cpu_key * key ) <nl>  <nl> args . objectid = key -> on_disk_key . k_objectid ; <nl> args . dirid = key -> on_disk_key . k_dir_id ; <nl> + reiserfs_write_unlock ( s ); <nl> inode = iget5_locked ( s , key -> on_disk_key . k_objectid , <nl> reiserfs_find_actor , reiserfs_init_locked_inode , <nl> ( void *)(& args )); <nl> + reiserfs_write_lock ( s ); <nl> if (! inode ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
int __ieee80211_suspend ( struct ieee80211_hw * hw , struct cfg80211_wowlan * wowlan ) <nl> int err = drv_suspend ( local , wowlan ); <nl> if ( err < 0 ) { <nl> local -> quiescing = false ; <nl> + local -> wowlan = false ; <nl> return err ; <nl> } else if ( err > 0 ) { <nl> WARN_ON ( err != 1 );
bfin_debug_mmrs_dma ( struct dentry * parent , unsigned long base , int num , char mdm <nl> __DMA ( CURR_DESC_PTR , curr_desc_ptr ); <nl> __DMA ( CURR_ADDR , curr_addr ); <nl> __DMA ( IRQ_STATUS , irq_status ); <nl> - __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> + if ( strcmp ( pfx , " IMDMA ") != 0 ) <nl> + __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> __DMA ( CURR_X_COUNT , curr_x_count ); <nl> __DMA ( CURR_Y_COUNT , curr_y_count ); <nl> }
unsigned int solo_eeprom_ewen ( struct solo_dev * solo_dev , int w_en ) <nl> __be16 solo_eeprom_read ( struct solo_dev * solo_dev , int loc ) <nl> { <nl> int read_cmd = loc | ( EE_READ_CMD << ADDR_LEN ); <nl> - unsigned short retval = 0 ; <nl> + u16 retval = 0 ; <nl> int i ; <nl>  <nl> solo_eeprom_cmd ( solo_dev , read_cmd );
static int sgtl5000_i2c_probe ( struct i2c_client * client , <nl> if ( IS_ERR ( sgtl5000 -> mclk )) { <nl> ret = PTR_ERR ( sgtl5000 -> mclk ); <nl> dev_err (& client -> dev , " Failed to get mclock : % d \ n ", ret ); <nl> + /* Defer the probe to see if the clk will be provided later */ <nl> + if ( ret == - ENOENT ) <nl> + return - EPROBE_DEFER ; <nl> return ret ; <nl> } <nl> 
static ssize_t btrfs_label_store ( struct kobject * kobj , <nl> struct btrfs_fs_info * fs_info = to_fs_info ( kobj ); <nl> size_t p_len ; <nl>  <nl> + if (! fs_info ) <nl> + return - EPERM ; <nl> + <nl> if ( fs_info -> sb -> s_flags & MS_RDONLY ) <nl> return - EROFS ; <nl> 
static int kvm_ioctl_create_device ( struct kvm * kvm , <nl>  <nl> ret = anon_inode_getfd ( ops -> name , & kvm_device_fops , dev , O_RDWR | O_CLOEXEC ); <nl> if ( ret < 0 ) { <nl> - ops -> destroy ( dev ); <nl> mutex_lock (& kvm -> lock ); <nl> list_del (& dev -> vm_node ); <nl> mutex_unlock (& kvm -> lock ); <nl> + ops -> destroy ( dev ); <nl> return ret ; <nl> } <nl> 
static int wm97xx_init_pen_irq ( struct wm97xx * wm ) <nl> * provided . */ <nl> BUG_ON (! wm -> mach_ops -> irq_enable ); <nl>  <nl> - if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , IRQF_SHARED , <nl> + if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , <nl> + IRQF_SHARED | IRQF_SAMPLE_RANDOM , <nl> " wm97xx - pen ", wm )) { <nl> dev_err ( wm -> dev , <nl> " Failed to register pen down interrupt , polling ");
static bool g4x_compute_wm0 ( struct drm_device * dev , <nl> int entries , tlb_miss ; <nl>  <nl> crtc = intel_get_crtc_for_plane ( dev , plane ); <nl> - if ( crtc -> fb == NULL || ! crtc -> enabled ) <nl> + if ( crtc -> fb == NULL || ! crtc -> enabled ) { <nl> + * cursor_wm = cursor -> guard_size ; <nl> + * plane_wm = display -> guard_size ; <nl> return false ; <nl> + } <nl>  <nl> htotal = crtc -> mode . htotal ; <nl> hdisplay = crtc -> mode . hdisplay ;
static void task_fork_fair ( struct task_struct * p ) <nl>  <nl> update_rq_clock ( rq ); <nl>  <nl> - if ( unlikely ( task_cpu ( p ) != this_cpu )) <nl> + if ( unlikely ( task_cpu ( p ) != this_cpu )) { <nl> + rcu_read_lock (); <nl> __set_task_cpu ( p , this_cpu ); <nl> + rcu_read_unlock (); <nl> + } <nl>  <nl> update_curr ( cfs_rq ); <nl> 
static int i915_context_status ( struct seq_file * m , void * unused ) <nl> } <nl>  <nl> list_for_each_entry ( ctx , & dev_priv -> context_list , link ) { <nl> + if ( ctx -> obj == NULL ) <nl> + continue ; <nl> + <nl> seq_puts ( m , " HW context "); <nl> describe_ctx ( m , ctx ); <nl> for_each_ring ( ring , dev_priv , i )
static int alloc_wbufs ( struct ubifs_info * c ) <nl> { <nl> int i , err ; <nl>  <nl> - c -> jheads = kzalloc ( c -> jhead_cnt * sizeof ( struct ubifs_jhead ), <nl> - GFP_KERNEL ); <nl> + c -> jheads = kcalloc ( c -> jhead_cnt , sizeof ( struct ubifs_jhead ), <nl> + GFP_KERNEL ); <nl> if (! c -> jheads ) <nl> return - ENOMEM ; <nl> 
static ssize_t cxlflash_show_port_status ( struct device * dev , <nl> u64 * fc_regs ; <nl>  <nl> rc = kstrtouint (( attr -> attr . name + 4 ), 10 , & port ); <nl> - if ( rc || ( port > NUM_FC_PORTS )) <nl> + if ( rc || ( port >= NUM_FC_PORTS )) <nl> return 0 ; <nl>  <nl> fc_regs = & afu -> afu_map -> global . fc_regs [ port ][ 0 ];
qla27xx_fwdt_entry_t270 ( struct scsi_qla_host * vha , <nl> qla27xx_write_reg ( reg , 0xc0 , addr | 0x80000000 , buf ); <nl> qla27xx_insert32 ( addr , buf , len ); <nl> qla27xx_read_off ( reg , 0xc4 , buf , len ); <nl> - addr ++; <nl> + addr += sizeof ( uint32_t ); <nl> } <nl>  <nl> return false ;
static int shash_update_unaligned ( struct shash_desc * desc , const u8 * data , <nl> u8 buf [ shash_align_buffer_size ( unaligned_len , alignmask )] <nl> __attribute__ (( aligned )); <nl>  <nl> + if ( unaligned_len > len ) <nl> + unaligned_len = len ; <nl> + <nl> memcpy ( buf , data , unaligned_len ); <nl>  <nl> return shash -> update ( desc , buf , unaligned_len ) ?:
static ssize_t dfs_file_write ( struct file * file , const char __user * user_buf , <nl> } else if ( dent == d -> dfs_emulate_power_cut ) { <nl> if ( kstrtoint ( buf , 0 , & val ) != 0 ) <nl> count = - EINVAL ; <nl> - d -> emulate_power_cut = val ; <nl> + else <nl> + d -> emulate_power_cut = val ; <nl> goto out ; <nl> } <nl> 
struct kvm_vcpu * kvm_arch_vcpu_create ( struct kvm * kvm , unsigned int id ) <nl> int err ; <nl> struct kvm_vcpu * vcpu ; <nl>  <nl> + if ( irqchip_in_kernel ( kvm ) && vgic_initialized ( kvm )) { <nl> + err = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> vcpu = kmem_cache_zalloc ( kvm_vcpu_cache , GFP_KERNEL ); <nl> if (! vcpu ) { <nl> err = - ENOMEM ;
static int uvesafb_setcmap ( struct fb_cmap * cmap , struct fb_info * info ) <nl> info -> cmap . len || cmap -> start < info -> cmap . start ) <nl> return - EINVAL ; <nl>  <nl> - entries = kmalloc ( sizeof (* entries ) * cmap -> len , GFP_KERNEL ); <nl> + entries = kmalloc_array ( cmap -> len , sizeof (* entries ), <nl> + GFP_KERNEL ); <nl> if (! entries ) <nl> return - ENOMEM ; <nl> 
i915_gem_userptr_ioctl ( struct drm_device * dev , <nl> I915_USERPTR_UNSYNCHRONIZED )) <nl> return - EINVAL ; <nl>  <nl> + if (! args -> user_size ) <nl> + return - EINVAL ; <nl> + <nl> if ( offset_in_page ( args -> user_ptr | args -> user_size )) <nl> return - EINVAL ; <nl> 
struct lock_chain { <nl> }; <nl>  <nl> # define MAX_LOCKDEP_KEYS_BITS 11 <nl> -# define MAX_LOCKDEP_KEYS ( 1UL << MAX_LOCKDEP_KEYS_BITS ) <nl> +/* <nl> + * Subtract one because we offset hlock -> class_idx by 1 in order <nl> + * to make 0 mean no class . This avoids overflowing the class_idx <nl> + * bitfield and hitting the BUG in hlock_class (). <nl> + */ <nl> +# define MAX_LOCKDEP_KEYS (( 1UL << MAX_LOCKDEP_KEYS_BITS ) - 1 ) <nl>  <nl> struct held_lock { <nl> /*
static inline int verify_replay ( struct xfrm_usersa_info * p , <nl> if (! rt ) <nl> return 0 ; <nl>  <nl> + if ( p -> id . proto != IPPROTO_ESP ) <nl> + return - EINVAL ; <nl> + <nl> if ( p -> replay_window != 0 ) <nl> return - EINVAL ; <nl> 
static int init_phy ( struct net_device * dev ) <nl> if ( priv -> phy_interface == PHY_INTERFACE_MODE_SGMII ) <nl> uec_configure_serdes ( dev ); <nl>  <nl> - phydev -> supported &= ( ADVERTISED_10baseT_Half | <nl> - ADVERTISED_10baseT_Full | <nl> - ADVERTISED_100baseT_Half | <nl> - ADVERTISED_100baseT_Full ); <nl> + phydev -> supported &= ( SUPPORTED_MII | <nl> + SUPPORTED_Autoneg | <nl> + ADVERTISED_10baseT_Half | <nl> + ADVERTISED_10baseT_Full | <nl> + ADVERTISED_100baseT_Half | <nl> + ADVERTISED_100baseT_Full ); <nl>  <nl> if ( priv -> max_speed == SPEED_1000 ) <nl> phydev -> supported |= ADVERTISED_1000baseT_Full ;
void dccp_close ( struct sock * sk , long timeout ) <nl> __kfree_skb ( skb ); <nl> } <nl>  <nl> + /* If socket has been already reset kill it . */ <nl> + if ( sk -> sk_state == DCCP_CLOSED ) <nl> + goto adjudge_to_death ; <nl> + <nl> if ( data_was_unread ) { <nl> /* Unread data was tossed , send an appropriate Reset Code */ <nl> DCCP_WARN (" ABORT with % u bytes unread \ n ", data_was_unread );
static int amd_gpio_remove ( struct platform_device * pdev ) <nl> gpio_dev = platform_get_drvdata ( pdev ); <nl>  <nl> gpiochip_remove (& gpio_dev -> gc ); <nl> - pinctrl_unregister ( gpio_dev -> pctrl ); <nl>  <nl> return 0 ; <nl> }
static struct rds_connection * __rds_conn_create ( struct net * net , <nl> * should end up here , but if it <nl> * does , reset / destroy the connection . <nl> */ <nl> + kfree ( conn -> c_path ); <nl> kmem_cache_free ( rds_conn_slab , conn ); <nl> conn = ERR_PTR (- EOPNOTSUPP ); <nl> goto out ;
struct timekeeper { <nl> u32 mult ; <nl> }; <nl>  <nl> - struct timekeeper timekeeper ; <nl> + static struct timekeeper timekeeper ; <nl>  <nl> /** <nl> * timekeeper_setup_internals - Set up internals to use clocksource clock . <nl> static struct timespec total_sleep_time ; <nl> /* <nl> * The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock . <nl> */ <nl> - struct timespec raw_time ; <nl> + static struct timespec raw_time ; <nl>  <nl> /* flag for if timekeeping is suspended */ <nl> int __read_mostly timekeeping_suspended ;
static int omap2_onenand_probe ( struct platform_device * pdev ) <nl> } <nl> } <nl>  <nl> - if ( pdata -> skip_initial_unlocking ) <nl> - this -> options |= ONENAND_SKIP_INITIAL_UNLOCKING ; <nl> - <nl> if (( r = onenand_scan (& c -> mtd , 1 )) < 0 ) <nl> goto err_release_dma ; <nl> 
int ubi_io_read ( const struct ubi_device * ubi , void * buf , int pnum , int offset , <nl> return UBI_IO_BITFLIPS ; <nl> } <nl>  <nl> - if ( read != len && retries ++ < UBI_IO_RETRIES ) { <nl> + if ( retries ++ < UBI_IO_RETRIES ) { <nl> dbg_io (" error % d % s while reading % d bytes from PEB % d :% d ," <nl> " read only % zd bytes , retry ", <nl> err , errstr , len , pnum , offset , read );
static struct clk * __init clkgen_odf_register ( const char * parent_name , <nl> gate -> lock = odf_lock ; <nl>  <nl> div = kzalloc ( sizeof (* div ), GFP_KERNEL ); <nl> - if (! div ) <nl> + if (! div ) { <nl> + kfree ( gate ); <nl> return ERR_PTR (- ENOMEM ); <nl> + } <nl>  <nl> div -> flags = CLK_DIVIDER_ONE_BASED | CLK_DIVIDER_ALLOW_ZERO ; <nl> div -> reg = reg + pll_data -> odf [ odf ]. offset ;
static void release_resources ( struct ibmvnic_adapter * adapter ) <nl> } <nl> } <nl> } <nl> + kfree ( adapter -> napi ); <nl> + adapter -> napi = NULL ; <nl>  <nl> release_login_rsp_buffer ( adapter ); <nl> }
typedef struct { <nl> uint8_t max_lun ; <nl>  <nl> uint32_t unique_id ; <nl> - uint8_t irq ; <nl> + int irq ; <nl> uint8_t ito ; <nl> caddr_t ibuf ; <nl> dma_addr_t ibuf_dma_h ;
static int rfcomm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> break ; <nl> } <nl>  <nl> + tty_unlock (); <nl> schedule (); <nl> + tty_lock (); <nl> } <nl> set_current_state ( TASK_RUNNING ); <nl> remove_wait_queue (& dev -> wait , & wait );
static int __unioxx5_subdev_init ( struct comedi_device * dev , <nl> return - ENOMEM ; <nl>  <nl> ret = __comedi_request_region ( dev , iobase , UNIOXX5_SIZE ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + kfree ( usp ); <nl> return ret ; <nl> + } <nl> usp -> usp_iobase = iobase ; <nl>  <nl> /* defining modules types */
static int skcipher_sendmsg ( struct socket * sock , struct msghdr * msg , <nl>  <nl> sgl = list_entry ( ctx -> tsgl . prev , struct skcipher_sg_list , list ); <nl> sg = sgl -> sg ; <nl> - sg_unmark_end ( sg + sgl -> cur ); <nl> + if ( sgl -> cur ) <nl> + sg_unmark_end ( sg + sgl -> cur - 1 ); <nl> do { <nl> i = sgl -> cur ; <nl> plen = min_t ( size_t , len , PAGE_SIZE );
static int das16cs_attach ( struct comedi_device * dev , <nl> dev -> driver -> driver_name , dev -> board_name , <nl> dev -> iobase , dev -> irq ); <nl>  <nl> - return 1 ; <nl> + return 0 ; <nl> } <nl>  <nl> static void das16cs_detach ( struct comedi_device * dev )
fst_get_iface ( struct fst_card_info * card , struct fst_port_info * port , <nl> } <nl>  <nl> i = port -> index ; <nl> + memset (& sync , 0 , sizeof ( sync )); <nl> sync . clock_rate = FST_RDL ( card , portConfig [ i ]. lineSpeed ); <nl> /* Lucky card and linux use same encoding here */ <nl> sync . clock_type = FST_RDB ( card , portConfig [ i ]. internalClock ) ==
# include " tg3 . h " <nl>  <nl> # define DRV_MODULE_NAME " tg3 " <nl> -# define DRV_MODULE_VERSION " 3 . 108 " <nl> -# define DRV_MODULE_RELDATE " February 17 , 2010 " <nl> +# define DRV_MODULE_VERSION " 3 . 109 " <nl> +# define DRV_MODULE_RELDATE " April 2 , 2010 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static void pmf_gpio_set_ ## name ( struct gpio_runtime * rt , int on )\ <nl> \ <nl> if ( unlikely (! rt )) return ; \ <nl> rc = pmf_call_function ( rt -> node , # name "- mute ", & args ); \ <nl> - if ( rc ) \ <nl> + if ( rc && rc != - ENODEV ) \ <nl> printk ( KERN_WARNING " pmf_gpio_set_ " # name \ <nl> " failed , rc : % d \ n ", rc ); \ <nl> rt -> implementation_private &= ~( 1 << bit ); \
int vfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> bool new_is_dir = false ; <nl> unsigned max_links = new_dir -> i_sb -> s_max_links ; <nl>  <nl> - if ( source == target ) <nl> + /* <nl> + * Check source == target . <nl> + * On overlayfs need to look at underlying inodes . <nl> + */ <nl> + if ( vfs_select_inode ( old_dentry , 0 ) == vfs_select_inode ( new_dentry , 0 )) <nl> return 0 ; <nl>  <nl> error = may_delete ( old_dir , old_dentry , is_dir );
int ocfs2_xattr_get_nolock ( struct inode * inode , <nl> return - EOPNOTSUPP ; <nl>  <nl> if (!( oi -> ip_dyn_features & OCFS2_HAS_XATTR_FL )) <nl> - ret = - ENODATA ; <nl> + return - ENODATA ; <nl>  <nl> xis . inode_bh = xbs . inode_bh = di_bh ; <nl> di = ( struct ocfs2_dinode *) di_bh -> b_data ;
static void release_one_tty ( struct work_struct * work ) <nl> list_del_init (& tty -> tty_files ); <nl> file_list_unlock (); <nl>  <nl> + put_pid ( tty -> pgrp ); <nl> + put_pid ( tty -> session ); <nl> free_tty_struct ( tty ); <nl> } <nl> 
static const struct snd_pci_quirk stac92hd73xx_cfg_tbl [] = { <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02bd , <nl> " Dell Studio 1557 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02fe , <nl> - " Dell Studio XPS 1645 ", STAC_DELL_M6_BOTH ), <nl> + " Dell Studio XPS 1645 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x0413 , <nl> " Dell Studio 1558 ", STAC_DELL_M6_DMIC ), <nl> {} /* terminator */
static int iommu_map_page ( struct protection_domain * dom , <nl> count = PAGE_SIZE_PTE_COUNT ( page_size ); <nl> pte = alloc_pte ( dom , bus_addr , page_size , NULL , GFP_KERNEL ); <nl>  <nl> + if (! pte ) <nl> + return - ENOMEM ; <nl> + <nl> for ( i = 0 ; i < count ; ++ i ) <nl> if ( IOMMU_PTE_PRESENT ( pte [ i ])) <nl> return - EBUSY ;
vfs_removexattr ( struct dentry * dentry , const char * name ) <nl> if ( error ) <nl> return error ; <nl>  <nl> + mutex_lock (& inode -> i_mutex ); <nl> error = security_inode_removexattr ( dentry , name ); <nl> - if ( error ) <nl> + if ( error ) { <nl> + mutex_unlock (& inode -> i_mutex ); <nl> return error ; <nl> + } <nl>  <nl> - mutex_lock (& inode -> i_mutex ); <nl> error = inode -> i_op -> removexattr ( dentry , name ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> 
static long vop_ioctl ( struct file * f , unsigned int cmd , unsigned long arg ) <nl> ret = - EFAULT ; <nl> goto free_ret ; <nl> } <nl> + /* Ensure desc has not changed between the two reads */ <nl> + if ( memcmp (& dd , dd_config , sizeof ( dd ))) { <nl> + ret = - EINVAL ; <nl> + goto free_ret ; <nl> + } <nl> mutex_lock (& vdev -> vdev_mutex ); <nl> mutex_lock (& vi -> vop_mutex ); <nl> ret = vop_virtio_add_device ( vdev , dd_config );
static int goldfish_fb_remove ( struct platform_device * pdev ) <nl> dma_free_coherent (& pdev -> dev , framesize , ( void *) fb -> fb . screen_base , <nl> fb -> fb . fix . smem_start ); <nl> iounmap ( fb -> reg_base ); <nl> + kfree ( fb ); <nl> return 0 ; <nl> } <nl> 
static int tty_open ( struct inode * inode , struct file * filp ) <nl> if ( IS_ERR ( tty )) { <nl> tty_unlock (); <nl> mutex_unlock (& tty_mutex ); <nl> + tty_driver_kref_put ( driver ); <nl> return PTR_ERR ( tty ); <nl> } <nl> }
struct iio_channel * iio_channel_get ( const char * name , const char * channel_name ) <nl> if ( c == NULL ) <nl> return ERR_PTR (- ENODEV ); <nl>  <nl> - channel = kmalloc ( sizeof (* channel ), GFP_KERNEL ); <nl> + channel = kzalloc ( sizeof (* channel ), GFP_KERNEL ); <nl> if ( channel == NULL ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
static int is_valid_state_transition ( struct drbd_conf * mdev , <nl> os . conn < C_CONNECTED ) <nl> rv = SS_NEED_CONNECTION ; <nl>  <nl> + if (( ns . conn == C_SYNC_TARGET || ns . conn == C_SYNC_SOURCE ) <nl> + && os . conn < C_WF_REPORT_PARAMS ) <nl> + rv = SS_NEED_CONNECTION ; /* No NetworkFailure -> SyncTarget etc ... */ <nl> + <nl> return rv ; <nl> } <nl> 
void global_dirty_limits ( unsigned long * pbackground , unsigned long * pdirty ) <nl> { <nl> unsigned long background ; <nl> unsigned long dirty ; <nl> - unsigned long available_memory = determine_dirtyable_memory (); <nl> + unsigned long uninitialized_var ( available_memory ); <nl> struct task_struct * tsk ; <nl>  <nl> + if (! vm_dirty_bytes || ! dirty_background_bytes ) <nl> + available_memory = determine_dirtyable_memory (); <nl> + <nl> if ( vm_dirty_bytes ) <nl> dirty = DIV_ROUND_UP ( vm_dirty_bytes , PAGE_SIZE ); <nl> else
static void intel_hdmi_mode_set ( struct intel_encoder * encoder ) <nl> else <nl> hdmi_val |= SDVO_COLOR_FORMAT_8bpc ; <nl>  <nl> - /* Required on CPT */ <nl> - if ( intel_hdmi -> has_hdmi_sink && HAS_PCH_CPT ( dev )) <nl> + if ( intel_hdmi -> has_hdmi_sink && <nl> + ( HAS_PCH_CPT ( dev ) || IS_VALLEYVIEW ( dev ))) <nl> hdmi_val |= HDMI_MODE_SELECT_HDMI ; <nl>  <nl> if ( intel_hdmi -> has_audio ) {
static int sm501fb_start ( struct sm501fb_info * info , <nl> info -> fbmem_len = resource_size ( res ); <nl>  <nl> /* clear framebuffer memory - avoids garbage data on unused fb */ <nl> - memset ( info -> fbmem , 0 , info -> fbmem_len ); <nl> + memset_io ( info -> fbmem , 0 , info -> fbmem_len ); <nl>  <nl> /* clear palette ram - undefined at power on */ <nl> for ( k = 0 ; k < ( 256 * 3 ); k ++)
static int davinci_wdt_probe ( struct platform_device * pdev ) <nl> return PTR_ERR ( davinci_wdt -> clk ); <nl> } <nl>  <nl> - clk_prepare_enable ( davinci_wdt -> clk ); <nl> + ret = clk_prepare_enable ( davinci_wdt -> clk ); <nl> + if ( ret ) { <nl> + dev_err (& pdev -> dev , " failed to prepare clock \ n "); <nl> + return ret ; <nl> + } <nl>  <nl> platform_set_drvdata ( pdev , davinci_wdt ); <nl> 
static int wil_cfg80211_stop_ap ( struct wiphy * wiphy , <nl> wil6210_bus_request ( wil , WIL_DEFAULT_BUS_REQUEST_KBPS ); <nl> wil_set_recovery_state ( wil , fw_recovery_idle ); <nl>  <nl> + set_bit ( wil_status_resetting , wil -> status ); <nl> + <nl> mutex_lock (& wil -> mutex ); <nl>  <nl> wmi_pcp_stop ( wil );
static void mwifiex_tdls_add_aid ( struct mwifiex_private * priv , <nl> pos = ( void *) skb_put ( skb , 4 ); <nl> * pos ++ = WLAN_EID_AID ; <nl> * pos ++ = 2 ; <nl> - * pos ++ = le16_to_cpu ( assoc_rsp -> a_id ); <nl> + memcpy ( pos , & assoc_rsp -> a_id , sizeof ( assoc_rsp -> a_id )); <nl>  <nl> return ; <nl> }
ieee80211_sta_process_chanswitch ( struct ieee80211_sub_if_data * sdata , <nl> case - 1 : <nl> cfg80211_chandef_create (& new_chandef , new_chan , <nl> NL80211_CHAN_NO_HT ); <nl> + /* keep width for 5 / 10 MHz channels */ <nl> + switch ( sdata -> vif . bss_conf . chandef . width ) { <nl> + case NL80211_CHAN_WIDTH_5 : <nl> + case NL80211_CHAN_WIDTH_10 : <nl> + new_chandef . width = sdata -> vif . bss_conf . chandef . width ; <nl> + break ; <nl> + default : <nl> + break ; <nl> + } <nl> break ; <nl> } <nl> 
static int drbd_asb_recover_0p ( struct drbd_conf * mdev ) __must_hold ( local ) <nl> break ; <nl> } <nl> /* Else fall through to one of the other strategies ... */ <nl> - dev_warn ( DEV , " Discard younger / older primary did not found a decision \ n " <nl> + dev_warn ( DEV , " Discard younger / older primary did not find a decision \ n " <nl> " Using discard - least - changes instead \ n "); <nl> case ASB_DISCARD_ZERO_CHG : <nl> if ( ch_peer == 0 && ch_self == 0 ) {
static int __build_sched_domains ( const cpumask_t * cpu_map , <nl> error : <nl> free_sched_groups ( cpu_map , tmpmask ); <nl> SCHED_CPUMASK_FREE (( void *) allmasks ); <nl> + kfree ( rd ); <nl> return - ENOMEM ; <nl> # endif <nl> }
static int snd_compr_allocate_buffer ( struct snd_compr_stream * stream , <nl> unsigned int buffer_size ; <nl> void * buffer ; <nl>  <nl> + if ( params -> buffer . fragment_size == 0 || <nl> + params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + return - EINVAL ; <nl> + <nl> buffer_size = params -> buffer . fragment_size * params -> buffer . fragments ; <nl> if ( stream -> ops -> copy ) { <nl> buffer = NULL ;
static void dwc2_hcd_cleanup_channels ( struct dwc2_hsotg * hsotg ) <nl> */ <nl> channel -> qh = NULL ; <nl> } <nl> + /* All channels have been freed , mark them available */ <nl> + if ( hsotg -> core_params -> uframe_sched > 0 ) { <nl> + hsotg -> available_host_channels = <nl> + hsotg -> core_params -> host_channels ; <nl> + } else { <nl> + hsotg -> non_periodic_channels = 0 ; <nl> + hsotg -> periodic_channels = 0 ; <nl> + } <nl> } <nl>  <nl> /**
mlxreg_hotplug_health_work_helper ( struct mlxreg_hotplug_priv_data * priv , <nl> { <nl> struct mlxreg_core_data * data = item -> data ; <nl> u32 regval ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> for ( i = 0 ; i < item -> count ; i ++, data ++) { <nl> /* Mask event . */
static const struct pinmux_ops sunxi_pmx_ops = { <nl> . get_function_groups = sunxi_pmx_get_func_groups , <nl> . set_mux = sunxi_pmx_set_mux , <nl> . gpio_set_direction = sunxi_pmx_gpio_set_direction , <nl> + . strict = true , <nl> }; <nl>  <nl> static int sunxi_pinctrl_gpio_direction_input ( struct gpio_chip * chip ,
int adis_update_scan_mode ( struct iio_dev * indio_dev , <nl> return - ENOMEM ; <nl>  <nl> adis -> buffer = kcalloc ( indio_dev -> scan_bytes , 2 , GFP_KERNEL ); <nl> - if (! adis -> buffer ) <nl> + if (! adis -> buffer ) { <nl> + kfree ( adis -> xfer ); <nl> + adis -> xfer = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> rx = adis -> buffer ; <nl> tx = rx + scan_count ;
int MoxaDriverIoctl ( unsigned int cmd , unsigned long arg , int port ) <nl> case MOXA_FIND_BOARD : <nl> case MOXA_LOAD_C320B : <nl> case MOXA_LOAD_CODE : <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> break ; <nl> } <nl> 
static int emc1403_detect ( struct i2c_client * client , <nl> } <nl>  <nl> id = i2c_smbus_read_byte_data ( client , THERMAL_REVISION_REG ); <nl> - if ( id != 0x01 ) <nl> + if ( id < 0x01 || id > 0x04 ) <nl> return - ENODEV ; <nl>  <nl> return 0 ;
static int spear_cpufreq_target ( struct cpufreq_policy * policy , <nl> } <nl>  <nl> newfreq = clk_round_rate ( srcclk , newfreq * mult ); <nl> - if ( newfreq < 0 ) { <nl> + if ( newfreq <= 0 ) { <nl> pr_err (" clk_round_rate failed for cpu src clock \ n "); <nl> return newfreq ; <nl> }
static void compat_input ( struct dlm_write_request * kb , <nl> static void compat_output ( struct dlm_lock_result * res , <nl> struct dlm_lock_result32 * res32 ) <nl> { <nl> + memset ( res32 , 0 , sizeof (* res32 )); <nl> + <nl> res32 -> version [ 0 ] = res -> version [ 0 ]; <nl> res32 -> version [ 1 ] = res -> version [ 1 ]; <nl> res32 -> version [ 2 ] = res -> version [ 2 ];
static inline void intel_ring_emit_wa ( struct intel_engine_cs * ring , <nl> struct drm_device * dev = ring -> dev ; <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl>  <nl> - if ( dev_priv -> num_wa_regs > I915_MAX_WA_REGS ) <nl> + if ( dev_priv -> num_wa_regs >= I915_MAX_WA_REGS ) <nl> return ; <nl>  <nl> intel_ring_emit ( ring , MI_LOAD_REGISTER_IMM ( 1 ));
static int scmi_hwmon_probe ( struct scmi_device * sdev ) <nl> scmi_chip_info . info = ptr_scmi_ci ; <nl> chip_info = & scmi_chip_info ; <nl>  <nl> - for ( type = 0 ; type < hwmon_max && nr_count [ type ]; type ++) { <nl> + for ( type = 0 ; type < hwmon_max ; type ++) { <nl> + if (! nr_count [ type ]) <nl> + continue ; <nl> + <nl> scmi_hwmon_add_chan_info ( scmi_hwmon_chan , dev , nr_count [ type ], <nl> type , hwmon_attributes [ type ]); <nl> * ptr_scmi_ci ++ = scmi_hwmon_chan ++;
static void ar9003_hw_prog_ini ( struct ath_hw * ah , <nl> u32 val = INI_RA ( iniArr , i , column ); <nl>  <nl> REG_WRITE ( ah , reg , val ); <nl> - <nl> - /* <nl> - * Determine if this is a shift register value , and insert the <nl> - * configured delay if so . <nl> - */ <nl> - if ( reg >= 0x16000 && reg < 0x17000 <nl> - && ah -> config . analog_shiftreg ) <nl> - udelay ( 100 ); <nl> - <nl> DO_DELAY ( regWrites ); <nl> } <nl> }
EXPORT_SYMBOL_GPL ( mnt_clone_write ); <nl> */ <nl> int mnt_want_write_file ( struct file * file ) <nl> { <nl> - if (!( file -> f_mode & FMODE_WRITE )) <nl> + struct inode * inode = file -> f_dentry -> d_inode ; <nl> + if (!( file -> f_mode & FMODE_WRITE ) || special_file ( inode -> i_mode )) <nl> return mnt_want_write ( file -> f_path . mnt ); <nl> else <nl> return mnt_clone_write ( file -> f_path . mnt );
static noinline int __btrfs_cow_block ( struct btrfs_trans_handle * trans , <nl> btrfs_set_node_ptr_generation ( parent , parent_slot , <nl> trans -> transid ); <nl> btrfs_mark_buffer_dirty ( parent ); <nl> - tree_mod_log_free_eb ( root -> fs_info , buf ); <nl> + if ( last_ref ) <nl> + tree_mod_log_free_eb ( root -> fs_info , buf ); <nl> btrfs_free_tree_block ( trans , root , buf , parent_start , <nl> last_ref ); <nl> }
static int do_ip_setsockopt ( struct sock * sk , int level , <nl> * Check the arguments are allowable <nl> */ <nl>  <nl> + if ( optlen < sizeof ( struct in_addr )) <nl> + goto e_inval ; <nl> + <nl> err = - EFAULT ; <nl> if ( optlen >= sizeof ( struct ip_mreqn )) { <nl> if ( copy_from_user (& mreq , optval , sizeof ( mreq )))
struct hid_device * hid_allocate_device ( void ) <nl> device_initialize (& hdev -> dev ); <nl> hdev -> dev . release = hid_device_release ; <nl> hdev -> dev . bus = & hid_bus_type ; <nl> + device_enable_async_suspend (& hdev -> dev ); <nl>  <nl> hid_close_report ( hdev ); <nl> 
do_send_specific ( pid_t tgid , pid_t pid , int sig , struct siginfo * info ) <nl>  <nl> static int do_tkill ( pid_t tgid , pid_t pid , int sig ) <nl> { <nl> - struct siginfo info ; <nl> + struct siginfo info = {}; <nl>  <nl> info . si_signo = sig ; <nl> info . si_errno = 0 ;
int nvdimm_has_flush ( struct nd_region * nd_region ) <nl> { <nl> int i ; <nl>  <nl> - /* no nvdimm == flushing capability unknown */ <nl> - if ( nd_region -> ndr_mappings == 0 ) <nl> + /* no nvdimm or pmem api == flushing capability unknown */ <nl> + if ( nd_region -> ndr_mappings == 0 <nl> + || ! IS_ENABLED ( CONFIG_ARCH_HAS_PMEM_API )) <nl> return - ENXIO ; <nl>  <nl> for ( i = 0 ; i < nd_region -> ndr_mappings ; i ++) {
static struct irq_desc * __real_move_irq_desc ( struct irq_desc * old_desc , <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
static int upgrade_fw ( struct adapter * adap ) <nl> if (! ret ) <nl> dev_info ( dev , " firmware upgraded to version % pI4 from " <nl> FW_FNAME "\ n ", & hdr -> fw_ver ); <nl> + } else { <nl> + /* <nl> + * Tell our caller that we didn ' t upgrade the firmware . <nl> + */ <nl> + ret = - EINVAL ; <nl> } <nl> + <nl> out : release_firmware ( fw ); <nl> return ret ; <nl> }
static void ks_sdio_interrupt ( struct sdio_func * func ) <nl> int ret ; <nl> struct ks_sdio_card * card ; <nl> struct ks_wlan_private * priv ; <nl> - unsigned char status , rsize , byte ; <nl> + u8 status , rsize , byte ; <nl>  <nl> card = sdio_get_drvdata ( func ); <nl> priv = card -> priv ;
rpcrdma_register_internal ( struct rpcrdma_ia * ia , void * va , int len , <nl> */ <nl> iov -> addr = ib_dma_map_single ( ia -> ri_id -> device , <nl> va , len , DMA_BIDIRECTIONAL ); <nl> + if ( ib_dma_mapping_error ( ia -> ri_id -> device , iov -> addr )) <nl> + return - ENOMEM ; <nl> + <nl> iov -> length = len ; <nl>  <nl> if ( ia -> ri_have_dma_lkey ) {
static void __ccw_device_pm_restore ( struct ccw_device * cdev ) <nl> * available again . Kick re - detection . <nl> */ <nl> cdev -> private -> flags . resuming = 1 ; <nl> + cdev -> private -> path_new_mask = LPM_ANYPATH ; <nl> css_schedule_eval ( sch -> schid ); <nl> spin_unlock_irq ( sch -> lock ); <nl> css_complete_work ();
static int pcnet32_open ( struct net_device * dev ) <nl> lp -> rx_dma_addr [ i ] = 0 ; <nl> } <nl>  <nl> - pcnet32_free_ring ( dev ); <nl> - <nl> /* <nl> * Switch back to 16bit mode to avoid problems with dumb <nl> * DOS packet driver after a warm reboot
static int __get_data_block ( struct inode * inode , sector_t iblock , <nl> if (! err ) { <nl> map_bh ( bh , inode -> i_sb , map . m_pblk ); <nl> bh -> b_state = ( bh -> b_state & ~ F2FS_MAP_FLAGS ) | map . m_flags ; <nl> - bh -> b_size = map . m_len << inode -> i_blkbits ; <nl> + bh -> b_size = ( u64 ) map . m_len << inode -> i_blkbits ; <nl> } <nl> return err ; <nl> }
e1000_up ( struct e1000_adapter * adapter ) <nl> return err ; <nl>  <nl> mod_timer (& adapter -> watchdog_timer , jiffies ); <nl> - e1000_irq_enable ( adapter ); <nl>  <nl> # ifdef CONFIG_E1000_NAPI <nl> netif_poll_enable ( netdev ); <nl> # endif <nl> + e1000_irq_enable ( adapter ); <nl> + <nl> return 0 ; <nl> } <nl> 
void rtw_alloc_hwxmits ( struct adapter * padapter ) <nl>  <nl> pxmitpriv -> hwxmit_entry = HWXMIT_ENTRY ; <nl>  <nl> - pxmitpriv -> hwxmits = kzalloc ( sizeof ( struct hw_xmit ) * pxmitpriv -> hwxmit_entry , GFP_KERNEL ); <nl> + pxmitpriv -> hwxmits = kcalloc ( pxmitpriv -> hwxmit_entry , <nl> + sizeof ( struct hw_xmit ), GFP_KERNEL ); <nl>  <nl> hwxmits = pxmitpriv -> hwxmits ; <nl> 
static inline void activate_mm ( struct mm_struct * old , struct mm_struct * new ) <nl> * possible . <nl> */ <nl> if ( old != new && ( current -> flags & PF_BORROWED_MM )) <nl> - force_flush_all (); <nl> + CHOOSE_MODE ( force_flush_all (), <nl> + switch_mm_skas (& new -> context . skas . id )); <nl> } <nl>  <nl> static inline void switch_mm ( struct mm_struct * prev , struct mm_struct * next ,
static void amdgpu_gem_va_update_vm ( struct amdgpu_device * adev , <nl> goto error ; <nl>  <nl> if ( operation == AMDGPU_VA_OP_MAP || <nl> - operation == AMDGPU_VA_OP_REPLACE ) <nl> + operation == AMDGPU_VA_OP_REPLACE ) { <nl> r = amdgpu_vm_bo_update ( adev , bo_va , false ); <nl> + if ( r ) <nl> + goto error ; <nl> + } <nl>  <nl> r = amdgpu_vm_update_directories ( adev , vm ); <nl> - if ( r ) <nl> - goto error ; <nl>  <nl> error : <nl> if ( r && r != - ERESTARTSYS )
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> + memset ( sax , 0 , sizeof ( sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
u64 snd_usb_interface_dsd_format_quirks ( struct snd_usb_audio * chip , <nl> } <nl> } <nl> break ; <nl> + case USB_ID ( 0x16d0 , 0x0a23 ): <nl> + if ( fp -> altsetting == 2 ) <nl> + return SNDRV_PCM_FMTBIT_DSD_U32_BE ; <nl> + break ; <nl>  <nl> default : <nl> break ;
static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
static long media_device_enum_entities ( struct media_device * mdev , <nl> struct media_entity * ent ; <nl> struct media_entity_desc u_ent ; <nl>  <nl> + memset (& u_ent , 0 , sizeof ( u_ent )); <nl> if ( copy_from_user (& u_ent . id , & uent -> id , sizeof ( u_ent . id ))) <nl> return - EFAULT ; <nl> 
static int easycap_usb_probe ( struct usb_interface * intf , <nl> /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */ <nl> pdata_urb = kzalloc ( sizeof ( struct data_urb ), GFP_KERNEL ); <nl> if (! pdata_urb ) { <nl> + usb_free_urb ( purb ); <nl> SAM (" ERROR : Could not allocate struct data_urb .\ n "); <nl> return - ENOMEM ; <nl> }
int brcmf_fws_hdrpull ( struct brcmf_pub * drvr , int ifidx , s16 signal_len , <nl> if (! signal_len ) <nl> return 0 ; <nl> /* if flow control disabled , skip to packet data and leave */ <nl> - if (! fws -> fw_signals ) { <nl> + if ((! fws ) || (! fws -> fw_signals )) { <nl> skb_pull ( skb , signal_len ); <nl> return 0 ; <nl> }
static int tvp5150_fill_fmt ( struct v4l2_subdev * sd , <nl> struct v4l2_mbus_framefmt * f ; <nl> struct tvp5150 * decoder = to_tvp5150 ( sd ); <nl>  <nl> - if (! format || format -> pad ) <nl> + if (! format || ( format -> pad != DEMOD_PAD_VID_OUT )) <nl> return - EINVAL ; <nl>  <nl> f = & format -> format ;
static int count_fastmap_pebs ( struct ubi_attach_info * ai ) <nl> list_for_each_entry ( aeb , & ai -> free , u . list ) <nl> n ++; <nl>  <nl> - ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> + ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> ubi_rb_for_each_entry ( rb2 , aeb , & av -> root , u . rb ) <nl> n ++; <nl> 
int i915_driver_load ( struct drm_device * dev , unsigned long flags ) <nl>  <nl> intel_irq_init ( dev ); <nl> intel_pm_init ( dev ); <nl> - intel_uncore_sanitize ( dev ); <nl> intel_uncore_init ( dev ); <nl> + intel_uncore_sanitize ( dev ); <nl>  <nl> /* Try to make sure MCHBAR is enabled before poking at it */ <nl> intel_setup_mchbar ( dev );
static int may_commit_transaction ( struct btrfs_fs_info * fs_info , <nl>  <nl> spin_lock (& delayed_rsv -> lock ); <nl> if ( percpu_counter_compare (& space_info -> total_bytes_pinned , <nl> - bytes - delayed_rsv -> size ) >= 0 ) { <nl> + bytes - delayed_rsv -> size ) < 0 ) { <nl> spin_unlock (& delayed_rsv -> lock ); <nl> return - ENOSPC ; <nl> }
int snd_soc_dapm_device_event ( struct snd_soc_device * socdev , int event ) <nl> struct snd_soc_machine * machine = socdev -> machine ; <nl>  <nl> if ( machine -> dapm_event ) <nl> - machine -> dapm_event ( machine , event ); <nl> + machine -> dapm_event ( machine , event ); <nl> if ( codec -> dapm_event ) <nl> - codec -> dapm_event ( codec , event ); <nl> + codec -> dapm_event ( codec , event ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_device_event );
static int gen6_drpc_info ( struct seq_file * m ) <nl>  <nl> rpmodectl1 = I915_READ ( GEN6_RP_CONTROL ); <nl> rcctl1 = I915_READ ( GEN6_RC_CONTROL ); <nl> - sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> mutex_unlock (& dev -> struct_mutex ); <nl> + mutex_lock (& dev_priv -> rps . hw_lock ); <nl> + sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> + mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> seq_printf ( m , " Video Turbo Mode : % s \ n ", <nl> yesno ( rpmodectl1 & GEN6_RP_MEDIA_TURBO ));
static int adu_release ( struct inode * inode , struct file * file ) <nl> retval = adu_release_internal ( dev ); <nl>  <nl> exit : <nl> - up (& dev -> sem ); <nl> + if ( dev ) <nl> + up (& dev -> sem ); <nl> dbg ( 2 ," % s : leave , return value % d ", __FUNCTION__ , retval ); <nl> return retval ; <nl> }
static struct dentry * proc_mount ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb )) <nl> return ERR_CAST ( sb ); <nl>  <nl> + /* <nl> + * procfs isn ' t actually a stacking filesystem ; however , there is <nl> + * too much magic going on inside it to permit stacking things on <nl> + * top of it <nl> + */ <nl> + sb -> s_stack_depth = FILESYSTEM_MAX_STACK_DEPTH ; <nl> + <nl> if (! proc_parse_options ( options , ns )) { <nl> deactivate_locked_super ( sb ); <nl> return ERR_PTR (- EINVAL );
int cdc_parse_cdc_header ( struct usb_cdc_parsed_header * hdr , <nl> elength = 1 ; <nl> goto next_desc ; <nl> } <nl> + if (( buflen < elength ) || ( elength < 3 )) { <nl> + dev_err (& intf -> dev , " invalid descriptor buffer length \ n "); <nl> + break ; <nl> + } <nl> if ( buffer [ 1 ] != USB_DT_CS_INTERFACE ) { <nl> dev_err (& intf -> dev , " skipping garbage \ n "); <nl> goto next_desc ;
void * knav_pool_create ( const char * name , <nl> bool slot_found ; <nl> int ret ; <nl>  <nl> + if (! kdev ) <nl> + return ERR_PTR (- EPROBE_DEFER ); <nl> + <nl> if (! kdev -> dev ) <nl> return ERR_PTR (- ENODEV ); <nl> 
static const struct attribute_group isl29108_group = { <nl> static int isl29018_chip_init ( struct isl29018_chip * chip ) <nl> { <nl> int status ; <nl> - int new_adc_bit ; <nl> + unsigned int new_adc_bit ; <nl> unsigned int new_range ; <nl>  <nl> /* Code added per Intersil Application Note 1534 :
static const struct iio_chan_spec st_press_1_channels [] = { <nl> }, <nl> . info_mask_separate = <nl> BIT ( IIO_CHAN_INFO_RAW ) | BIT ( IIO_CHAN_INFO_SCALE ), <nl> + . info_mask_shared_by_all = BIT ( IIO_CHAN_INFO_SAMP_FREQ ), <nl> }, <nl> { <nl> . type = IIO_TEMP , <nl> static const struct iio_chan_spec st_press_1_channels [] = { <nl> BIT ( IIO_CHAN_INFO_RAW ) | <nl> BIT ( IIO_CHAN_INFO_SCALE ) | <nl> BIT ( IIO_CHAN_INFO_OFFSET ), <nl> + . info_mask_shared_by_all = BIT ( IIO_CHAN_INFO_SAMP_FREQ ), <nl> }, <nl> IIO_CHAN_SOFT_TIMESTAMP ( 2 ) <nl> };
static void release_sub_crqs ( struct ibmvnic_adapter * adapter ) <nl> release_sub_crq_queue ( adapter , <nl> adapter -> tx_scrq [ i ]); <nl> } <nl> + kfree ( adapter -> tx_scrq ); <nl> adapter -> tx_scrq = NULL ; <nl> } <nl>  <nl> static void release_sub_crqs ( struct ibmvnic_adapter * adapter ) <nl> release_sub_crq_queue ( adapter , <nl> adapter -> rx_scrq [ i ]); <nl> } <nl> + kfree ( adapter -> rx_scrq ); <nl> adapter -> rx_scrq = NULL ; <nl> } <nl> }
static int __net_init __ip_vs_ftp_init ( struct net * net ) <nl> struct ip_vs_app * app ; <nl> struct netns_ipvs * ipvs = net_ipvs ( net ); <nl>  <nl> + if (! ipvs ) <nl> + return - ENOENT ; <nl> app = kmemdup (& ip_vs_ftp , sizeof ( struct ip_vs_app ), GFP_KERNEL ); <nl> if (! app ) <nl> return - ENOMEM ;
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> write_unlock_bh (& bond -> curr_slave_lock ); <nl> read_unlock (& bond -> lock ); <nl> } <nl> + slave_disable_netpoll ( new_slave ); <nl>  <nl> err_close : <nl> slave_dev -> priv_flags &= ~ IFF_BONDING ;
static struct s3c_camif_drvdata s3c6410_camif_drvdata = { <nl> . bus_clk_freq = 133000000UL , <nl> }; <nl>  <nl> - static struct platform_device_id s3c_camif_driver_ids [] = { <nl> + static const struct platform_device_id s3c_camif_driver_ids [] = { <nl> { <nl> . name = " s3c2440 - camif ", <nl> . driver_data = ( unsigned long )& s3c244x_camif_drvdata ,
static ssize_t i40e_dbg_command_write ( struct file * filp , <nl> if (! cmd_buf ) <nl> return count ; <nl> bytes_not_copied = copy_from_user ( cmd_buf , buffer , count ); <nl> - if ( bytes_not_copied < 0 ) <nl> + if ( bytes_not_copied < 0 ) { <nl> + kfree ( cmd_buf ); <nl> return bytes_not_copied ; <nl> + } <nl> if ( bytes_not_copied > 0 ) <nl> count -= bytes_not_copied ; <nl> cmd_buf [ count ] = '\ 0 ';
static void __init __e820_add_region ( struct e820map * e820x , u64 start , u64 size , <nl> { <nl> int x = e820x -> nr_map ; <nl>  <nl> - if ( x == ARRAY_SIZE ( e820x -> map )) { <nl> + if ( x >= ARRAY_SIZE ( e820x -> map )) { <nl> printk ( KERN_ERR " Ooops ! Too many entries in the memory map !\ n "); <nl> return ; <nl> }
static int64_t _sort__sym_cmp ( struct symbol * sym_l , struct symbol * sym_r ) <nl> if ( sym_l == sym_r ) <nl> return 0 ; <nl>  <nl> + if ( sym_l -> inlined || sym_r -> inlined ) <nl> + return strcmp ( sym_l -> name , sym_r -> name ); <nl> + <nl> if ( sym_l -> start != sym_r -> start ) <nl> return ( int64_t )( sym_r -> start - sym_l -> start ); <nl> 
static int rdma_cma_handler ( struct rdma_cm_id * cma_id , <nl> if ( xprt ) { <nl> set_bit ( XPT_CLOSE , & xprt -> xpt_flags ); <nl> svc_xprt_enqueue ( xprt ); <nl> + svc_xprt_put ( xprt ); <nl> } <nl> break ; <nl> case RDMA_CM_EVENT_DEVICE_REMOVAL :
static void handle_swbp ( struct pt_regs * regs ) <nl> if ( unlikely (! test_bit ( UPROBE_COPY_INSN , & uprobe -> flags ))) <nl> goto out ; <nl>  <nl> + /* Tracing handlers use -> utask to communicate with fetch methods */ <nl> + if (! get_utask ()) <nl> + goto out ; <nl> + <nl> handler_chain ( uprobe , regs ); <nl> if ( can_skip_sstep ( uprobe , regs )) <nl> goto out ;
static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath_softc * sc = hw -> priv ; <nl> int ret = 0 ; <nl>  <nl> - local_bh_disable (); <nl> + mutex_lock (& sc -> mutex ); <nl>  <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( sc -> sc_ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> - local_bh_enable (); <nl> + mutex_unlock (& sc -> mutex ); <nl>  <nl> return ret ; <nl> }
int blkcg_init_queue ( struct request_queue * q ) <nl> if ( preloaded ) <nl> radix_tree_preload_end (); <nl>  <nl> - if ( IS_ERR ( blkg )) { <nl> - blkg_free ( new_blkg ); <nl> + if ( IS_ERR ( blkg )) <nl> return PTR_ERR ( blkg ); <nl> - } <nl>  <nl> q -> root_blkg = blkg ; <nl> q -> root_rl . blkg = blkg ;
static int hpsa_eh_device_reset_handler ( struct scsi_cmnd * scsicmd ) <nl> return FAILED ; <nl> } <nl>  <nl> + if ( dev -> devtype == TYPE_ENCLOSURE ) <nl> + return SUCCESS ; <nl> + <nl> /* if controller locked up , we can guarantee command won ' t complete */ <nl> if ( lockup_detected ( h )) { <nl> snprintf ( msg , sizeof ( msg ),
static int ext4_quota_enable ( struct super_block * sb , int type , int format_id , <nl> return PTR_ERR ( qf_inode ); <nl> } <nl>  <nl> + /* Don ' t account quota for quota files to avoid recursion */ <nl> + qf_inode -> i_flags |= S_NOQUOTA ; <nl> err = dquot_enable ( qf_inode , type , format_id , flags ); <nl> iput ( qf_inode ); <nl> 
# include < linux / hdmi . h > <nl> # include " hdmi . h " <nl>  <nl> - <nl> -/* Supported HDMI Audio channels */ <nl> -# define MSM_HDMI_AUDIO_CHANNEL_2 0 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_4 1 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_6 2 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_8 3 <nl> - <nl> /* maps MSM_HDMI_AUDIO_CHANNEL_n consts used by audio driver to # of channels : */ <nl> static int nchannels [] = { 2 , 4 , 6 , 8 }; <nl> 
static int copy_to_user_tmpl ( struct xfrm_policy * xp , struct sk_buff * skb ) <nl> struct xfrm_user_tmpl * up = & vec [ i ]; <nl> struct xfrm_tmpl * kp = & xp -> xfrm_vec [ i ]; <nl>  <nl> + memset ( up , 0 , sizeof (* up )); <nl> memcpy (& up -> id , & kp -> id , sizeof ( up -> id )); <nl> up -> family = kp -> encap_family ; <nl> memcpy (& up -> saddr , & kp -> saddr , sizeof ( up -> saddr ));
int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> which_atu = 0 ; <nl> } <nl>  <nl> - if (! which_atu ) <nl> + if (! which_atu ) { <nl> + kfree ( res ); <nl> return 0 ; <nl> + } <nl>  <nl> switch ( which_atu ) { <nl> case IOP13XX_INIT_ATU_ATUX : <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> sys -> map_irq = iop13xx_pcie_map_irq ; <nl> break ; <nl> default : <nl> + kfree ( res ); <nl> return 0 ; <nl> } <nl> 
static int __init cy_detect_isa ( void ) <nl> continue ; <nl> } <nl> # ifdef MODULE <nl> - if ( isparam && irq [ i ]) <nl> + if ( isparam && i < NR_CARDS && irq [ i ]) <nl> cy_isa_irq = irq [ i ]; <nl> else <nl> # endif
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
static int dlm_add_member ( struct dlm_ls * ls , int nodeid ) <nl> return - ENOMEM ; <nl>  <nl> w = dlm_node_weight ( ls -> ls_name , nodeid ); <nl> - if ( w < 0 ) <nl> + if ( w < 0 ) { <nl> + kfree ( memb ); <nl> return w ; <nl> + } <nl>  <nl> memb -> nodeid = nodeid ; <nl> memb -> weight = w ;
static void sl_tx_timeout ( struct net_device * dev , unsigned int txqueue ) <nl> spin_lock (& sl -> lock ); <nl>  <nl> if ( netif_queue_stopped ( dev )) { <nl> - if (! netif_running ( dev )) <nl> + if (! netif_running ( dev ) || ! sl -> tty ) <nl> goto out ; <nl>  <nl> /* May be we must check transmitter timeout here ?
static int vesafb_setcolreg ( unsigned regno , unsigned red , unsigned green , <nl>  <nl> static void vesafb_destroy ( struct fb_info * info ) <nl> { <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> if ( info -> screen_base ) <nl> iounmap ( info -> screen_base ); <nl> release_mem_region ( info -> apertures -> ranges [ 0 ]. base , info -> apertures -> ranges [ 0 ]. size );
static int acm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> else <nl> rv = 0 ; <nl>  <nl> + set_bit ( TTY_NO_WRITE_SPLIT , & tty -> flags ); <nl> tty -> driver_data = acm ; <nl> acm -> tty = tty ; <nl> 
static int __cpu_find_by ( int (* compare )( int , int , void *), void * compare_arg , <nl> int err = check_cpu_node ( dp -> node , & cur_inst , <nl> compare , compare_arg , <nl> prom_node , mid ); <nl> - if (! err ) <nl> + if (! err ) { <nl> + of_node_put ( dp ); <nl> return 0 ; <nl> + } <nl> } <nl>  <nl> return - ENODEV ;
bool kvm_vgic_map_is_active ( struct kvm_vcpu * vcpu , struct irq_phys_map * map ) <nl> return true ; <nl> } <nl>  <nl> - return dist_active_irq ( vcpu ); <nl> + return vgic_irq_is_active ( vcpu , map -> virt_irq ); <nl> } <nl>  <nl> /*
int x86_decode_insn ( struct x86_emulate_ctxt * ctxt , void * insn , int insn_len ) <nl> /* Decode and fetch the destination operand : register or memory . */ <nl> rc = decode_operand ( ctxt , & ctxt -> dst , ( ctxt -> d >> DstShift ) & OpMask ); <nl>  <nl> - done : <nl> if ( ctxt -> rip_relative ) <nl> ctxt -> memopp -> addr . mem . ea += ctxt -> _eip ; <nl>  <nl> + done : <nl> return ( rc != X86EMUL_CONTINUE ) ? EMULATION_FAILED : EMULATION_OK ; <nl> } <nl> 
void rtl8188eu_set_hal_ops ( struct adapter * adapt ) <nl>  <nl>  <nl> adapt -> HalData = kzalloc ( sizeof ( struct hal_data_8188e ), GFP_KERNEL ); <nl> - if ( adapt -> HalData == NULL ) <nl> + if (! adapt -> HalData ) <nl> DBG_88E (" cant not alloc memory for HAL DATA \ n "); <nl>  <nl> halfunc -> hal_power_on = rtl8188eu_InitPowerOn ;
static int ext4_valid_extent ( struct inode * inode , struct ext4_extent * ext ) <nl> ext4_fsblk_t block = ext4_ext_pblock ( ext ); <nl> int len = ext4_ext_get_actual_len ( ext ); <nl>  <nl> + if ( len == 0 ) <nl> + return 0 ; <nl> return ext4_data_block_valid ( EXT4_SB ( inode -> i_sb ), block , len ); <nl> } <nl> 
static inline struct sk_buff * sk_stream_alloc_pskb ( struct sock * sk , <nl> { <nl> struct sk_buff * skb ; <nl>  <nl> + /* The TCP header must be at least 32 - bit aligned . */ <nl> + size = ALIGN ( size , 4 ); <nl> + <nl> skb = alloc_skb_fclone ( size + sk -> sk_prot -> max_header , gfp ); <nl> if ( skb ) { <nl> skb -> truesize += mem ;
static int twl6040_probe ( struct i2c_client * client , <nl> init_completion (& twl6040 -> ready ); <nl>  <nl> twl6040 -> rev = twl6040_reg_read ( twl6040 , TWL6040_REG_ASICREV ); <nl> + if ( twl6040 -> rev < 0 ) { <nl> + dev_err (& client -> dev , " Failed to read revision register : % d \ n ", <nl> + twl6040 -> rev ); <nl> + goto gpio_err ; <nl> + } <nl>  <nl> /* ERRATA : Automatic power - up is not possible in ES1 . 0 */ <nl> if ( twl6040_get_revid ( twl6040 ) > TWL6040_REV_ES1_0 )
static int __devinit lpc32xx_nand_probe ( struct platform_device * pdev ) <nl> dev_err (& pdev -> dev , " Missing platform data \ n "); <nl> return - ENOENT ; <nl> } <nl> + if ( host -> ncfg -> wp_gpio == - EPROBE_DEFER ) <nl> + return - EPROBE_DEFER ; <nl> if ( gpio_is_valid ( host -> ncfg -> wp_gpio ) && <nl> gpio_request ( host -> ncfg -> wp_gpio , " NAND WP ")) { <nl> dev_err (& pdev -> dev , " GPIO not available \ n ");
decompress_fn __init decompress_method ( const unsigned char * inbuf , long len , <nl> { <nl> const struct compress_format * cf ; <nl>  <nl> - if ( len < 2 ) <nl> + if ( len < 2 ) { <nl> + if ( name ) <nl> + * name = NULL ; <nl> return NULL ; /* Need at least this much ... */ <nl> + } <nl>  <nl> pr_debug (" Compressed data magic : %#. 2x %#. 2x \ n ", inbuf [ 0 ], inbuf [ 1 ]); <nl> 
static struct console usbcons = { <nl>  <nl> void usb_serial_console_disconnect ( struct usb_serial * serial ) <nl> { <nl> - if ( serial -> port [ 0 ] == usbcons_info . port ) { <nl> + if ( serial -> port [ 0 ] && serial -> port [ 0 ] == usbcons_info . port ) { <nl> usb_serial_console_exit (); <nl> usb_serial_put ( serial ); <nl> }
int fscrypt_has_permitted_context ( struct inode * parent , struct inode * child ) <nl> BUG_ON ( 1 ); <nl> } <nl>  <nl> + /* No restrictions on file types which are never encrypted */ <nl> + if (! S_ISREG ( child -> i_mode ) && ! S_ISDIR ( child -> i_mode ) && <nl> + ! S_ISLNK ( child -> i_mode )) <nl> + return 1 ; <nl> + <nl> /* no restrictions if the parent directory is not encrypted */ <nl> if (! parent -> i_sb -> s_cop -> is_encrypted ( parent )) <nl> return 1 ;
static int write_vmem ( struct fbtft_par * par , size_t offset , size_t len ) <nl> signed short * convert_buf = kmalloc ( par -> info -> var . xres * <nl> par -> info -> var . yres * sizeof ( signed short ), GFP_NOIO ); <nl>  <nl> + if (! convert_buf ) <nl> + return - ENOMEM ; <nl> + <nl> fbtft_par_dbg ( DEBUG_WRITE_VMEM , par , "% s ()\ n ", __func__ ); <nl>  <nl> /* converting to grayscale16 */
void con_protect_unimap ( struct vc_data * vc , int rdonly ); <nl> int con_copy_unimap ( struct vc_data * dst_vc , struct vc_data * src_vc ); <nl>  <nl> # define vc_translate ( vc , c ) (( vc )-> vc_translate [( c ) | \ <nl> - ( vc )-> vc_toggle_meta ? 0x80 : 0 ]) <nl> + (( vc )-> vc_toggle_meta ? 0x80 : 0 )]) <nl> # else <nl> # define con_set_trans_old ( arg ) ( 0 ) <nl> # define con_get_trans_old ( arg ) (- EINVAL )
int be_cmd_if_create ( struct be_adapter * adapter , u32 cap_flags , u32 en_flags , <nl> if (! status ) { <nl> struct be_cmd_resp_if_create * resp = embedded_payload ( wrb ); <nl> * if_handle = le32_to_cpu ( resp -> interface_id ); <nl> + <nl> + /* Hack to retrieve VF ' s pmac - id on BE3 */ <nl> + if ( BE3_chip ( adapter ) && ! be_physfn ( adapter )) <nl> + adapter -> pmac_id [ 0 ] = le32_to_cpu ( resp -> pmac_id ); <nl> } <nl>  <nl> err :
static void execlists_submission_tasklet ( unsigned long data ) <nl> trace_i915_request_out ( rq ); <nl> i915_request_put ( rq ); <nl>  <nl> + GEM_TRACE ("% s completed ctx =% d \ n ", <nl> + engine -> name , port -> context_id ); <nl> + <nl> execlists_port_complete ( execlists , port ); <nl> } else { <nl> port_set ( port , port_pack ( rq , count ));
struct gb_host_device * gb_hd_create ( struct gb_hd_driver * driver , <nl> return ERR_PTR (- EINVAL ); <nl> } <nl>  <nl> - if ( num_cports == 0 || num_cports > CPORT_ID_MAX ) { <nl> + if ( num_cports == 0 || num_cports > CPORT_ID_MAX + 1 ) { <nl> dev_err ( parent , " Invalid number of CPorts : % zu \ n ", num_cports ); <nl> return ERR_PTR (- EINVAL ); <nl> }
static struct sock * pep_sock_accept ( struct sock * sk , int flags , int * errp , <nl>  <nl> err = pep_accept_conn ( newsk , skb ); <nl> if ( err ) { <nl> + __sock_put ( sk ); <nl> sock_put ( newsk ); <nl> newsk = NULL ; <nl> goto drop ;
static int rsc_parse ( struct cache_detail * cd , <nl> /* number of additional gid ' s */ <nl> if ( get_int (& mesg , & N )) <nl> goto out ; <nl> + if ( N < 0 || N > NGROUPS_MAX ) <nl> + goto out ; <nl> status = - ENOMEM ; <nl> rsci . cred . cr_group_info = groups_alloc ( N ); <nl> if ( rsci . cred . cr_group_info == NULL )
static inline int pmd_none_or_trans_huge_or_clear_bad ( pmd_t * pmd ) <nl> # ifdef CONFIG_TRANSPARENT_HUGEPAGE <nl> barrier (); <nl> # endif <nl> - if ( pmd_none ( pmdval )) <nl> + if ( pmd_none ( pmdval ) || pmd_trans_huge ( pmdval )) <nl> return 1 ; <nl> if ( unlikely ( pmd_bad ( pmdval ))) { <nl> - if (! pmd_trans_huge ( pmdval )) <nl> - pmd_clear_bad ( pmd ); <nl> + pmd_clear_bad ( pmd ); <nl> return 1 ; <nl> } <nl> return 0 ;
int selinux_task_prlimit ( const struct cred * cred , const struct cred * tcred , <nl> { <nl> u32 av = 0 ; <nl>  <nl> + if (! flags ) <nl> + return 0 ; <nl> if ( flags & LSM_PRLIMIT_WRITE ) <nl> av |= PROCESS__SETRLIMIT ; <nl> if ( flags & LSM_PRLIMIT_READ )
intel_sdvo_tv_init ( struct intel_sdvo * intel_sdvo , int type ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl>  <nl> intel_sdvo_lvds_init ( struct intel_sdvo * intel_sdvo , int device ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl> 
static void scan_requests ( struct ceph_osd * osd , <nl> list_add_tail (& lreq -> scan_item , need_resend_linger ); <nl> break ; <nl> case CALC_TARGET_POOL_DNE : <nl> + list_del_init (& lreq -> scan_item ); <nl> check_linger_pool_dne ( lreq ); <nl> break ; <nl> }
static int pinconf_dbg_config_write ( struct file * file , <nl> int i ; <nl>  <nl> /* Get userspace string and assure termination */ <nl> - buf_size = min ( count , ( sizeof ( buf )- 1 )); <nl> + buf_size = min ( count , ( size_t )( sizeof ( buf )- 1 )); <nl> if ( copy_from_user ( buf , user_buf , buf_size )) <nl> return - EFAULT ; <nl> buf [ buf_size ] = 0 ;
xlog_recover_do_reg_buffer ( <nl> stale_buf = 1 ; <nl> break ; <nl> } <nl> - if ( be16_to_cpu ( dip -> di_core . di_mode )) <nl> + if ( dip -> di_core . di_mode ) <nl> mode_count ++; <nl> - if ( be16_to_cpu ( dip -> di_core . di_gen )) <nl> + if ( dip -> di_core . di_gen ) <nl> gen_count ++; <nl> } <nl> 
struct nvkm_pll_vals ; <nl>  <nl> struct nv04_devinit_priv { <nl> struct nvkm_devinit base ; <nl> - u8 owner ; <nl> + int owner ; <nl> }; <nl>  <nl> int nv04_devinit_ctor ( struct nvkm_object *, struct nvkm_object *,
static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> err = rpmsg_ns_register_device ( rpdev_ns ); <nl> if ( err ) <nl> - goto free_vch ; <nl> + /* vch will be free in virtio_rpmsg_release_device () */ <nl> + goto free_ctrldev ; <nl> } <nl>  <nl> /* <nl> static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> return 0 ; <nl>  <nl> - free_vch : <nl> - kfree ( vch ); <nl> free_ctrldev : <nl> rpmsg_virtio_del_ctrl_dev ( rpdev_ctrl ); <nl> free_coherent :
intel_ioapic_set_affinity ( struct irq_data * data , const struct cpumask * mask , <nl>  <nl> err = apic -> cpu_mask_to_apicid_and ( cfg -> domain , mask , & dest ); <nl> if ( err ) { <nl> - if ( assign_irq_vector ( irq , cfg , data -> affinity )); <nl> + if ( assign_irq_vector ( irq , cfg , data -> affinity )) <nl> pr_err (" Failed to recover vector for irq % d \ n ", irq ); <nl> return err ; <nl> }
static int Handle1401Esc ( DEVICE_EXTENSION * pdx , char * pCh , <nl> /* This can never happen , really */ <nl> dev_err (& pdx -> interface -> dev , <nl> " ERROR : DMA setup while transfer still waiting "); <nl> - spin_unlock (& pdx -> stagedLock ); <nl> + spin_unlock (& pdx -> stagedLock ); <nl> } else { <nl> if (( wTransType == TM_EXTTOHOST ) <nl> || ( wTransType == TM_EXTTO1401 )) {
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> fail_workqueue : <nl> wiphy_unregister ( local -> hw . wiphy ); <nl> fail_wiphy_register : <nl> - kfree ( local -> int_scan_req -> channels ); <nl> + kfree ( local -> int_scan_req ); <nl> return result ; <nl> } <nl> EXPORT_SYMBOL ( ieee80211_register_hw );
void intel_lrc_irq_handler ( struct intel_engine_cs * ring ) <nl>  <nl> spin_unlock (& ring -> execlist_lock ); <nl>  <nl> - WARN ( submit_contexts > 2 , " More than two context complete events ?\ n "); <nl> + if ( unlikely ( submit_contexts > 2 )) <nl> + DRM_ERROR (" More than two context complete events ?\ n "); <nl> + <nl> ring -> next_context_status_buffer = write_pointer % GEN8_CSB_ENTRIES ; <nl>  <nl> /* Update the read pointer to the old write pointer . Manual ringbuffer
static int kcm_sendmsg ( struct socket * sock , struct msghdr * msg , size_t len ) <nl> } else { <nl> /* Message not complete , save state */ <nl> partial_message : <nl> - kcm -> seq_skb = head ; <nl> - kcm_tx_msg ( head )-> last_skb = skb ; <nl> + if ( head ) { <nl> + kcm -> seq_skb = head ; <nl> + kcm_tx_msg ( head )-> last_skb = skb ; <nl> + } <nl> } <nl>  <nl> KCM_STATS_ADD ( kcm -> stats . tx_bytes , copied );
compat_ptr ( compat_uptr_t uptr ) <nl> return ( void __user *) ( unsigned long ) uptr ; <nl> } <nl>  <nl> + static inline compat_uptr_t <nl> + ptr_to_compat ( void __user * uptr ) <nl> +{ <nl> + return ( u32 )( unsigned long ) uptr ; <nl> +} <nl> + <nl> static __inline__ void __user * <nl> compat_alloc_user_space ( long len ) <nl> {
static struct dma_page * pool_find_page ( struct dma_pool * pool , dma_addr_t dma ) <nl> list_for_each_entry ( page , & pool -> page_list , page_list ) { <nl> if ( dma < page -> dma ) <nl> continue ; <nl> - if ( dma < ( page -> dma + pool -> allocation )) <nl> + if (( dma - page -> dma ) < pool -> allocation ) <nl> return page ; <nl> } <nl> return NULL ;
nv130_chipset = { <nl> static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> + . bios = nvkm_bios_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
static struct input_dev * gb_svc_input_create ( struct gb_svc * svc ) <nl> return input_dev ; <nl>  <nl> err_free_input : <nl> - input_free_device ( svc -> input ); <nl> + input_free_device ( input_dev ); <nl> return ERR_PTR (- ENOMEM ); <nl> } <nl> 
static int ttusbdecfe_dvbs_diseqc_send_master_cmd ( struct dvb_frontend * fe , struc <nl> 0x00 , 0x00 , 0x00 , 0x00 , <nl> 0x00 , 0x00 }; <nl>  <nl> + if ( cmd -> msg_len > sizeof ( b ) - 4 ) <nl> + return - EINVAL ; <nl> + <nl> memcpy (& b [ 4 ], cmd -> msg , cmd -> msg_len ); <nl>  <nl> state -> config -> send_command ( fe , 0x72 ,
xfs_qm_dquot_walk ( <nl> skipped = 0 ; <nl> break ; <nl> } <nl> + /* we ' re done if id overflows back to zero */ <nl> + if (! next_index ) <nl> + break ; <nl> } <nl>  <nl> if ( skipped ) {
EXPORT_SYMBOL ( ipmi_get_smi_info ); <nl> static void free_user ( struct kref * ref ) <nl> { <nl> struct ipmi_user * user = container_of ( ref , struct ipmi_user , refcount ); <nl> + cleanup_srcu_struct (& user -> release_barrier ); <nl> kfree ( user ); <nl> } <nl>  <nl> int ipmi_destroy_user ( struct ipmi_user * user ) <nl> { <nl> _ipmi_destroy_user ( user ); <nl>  <nl> - cleanup_srcu_struct (& user -> release_barrier ); <nl> kref_put (& user -> refcount , free_user ); <nl>  <nl> return 0 ;
static bool is_fullscreen ( struct drm_crtc_state * cstate , <nl> (( pstate -> crtc_y + pstate -> crtc_h ) >= cstate -> mode . vdisplay ); <nl> } <nl>  <nl> - enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> + static enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> struct drm_crtc_state * new_crtc_state , <nl> struct drm_plane_state * bpstate ) <nl> {
static int rk_hw_params ( struct snd_pcm_substream * substream , <nl> case 96000 : <nl> mclk = 12288000 ; <nl> break ; <nl> + case 192000 : <nl> + mclk = 24576000 ; <nl> + break ; <nl> case 11025 : <nl> case 22050 : <nl> case 44100 :
static int acm_probe ( struct usb_interface * intf , <nl> if ( quirks == NO_UNION_NORMAL ) { <nl> data_interface = usb_ifnum_to_if ( usb_dev , 1 ); <nl> control_interface = usb_ifnum_to_if ( usb_dev , 0 ); <nl> + /* we would crash */ <nl> + if (! data_interface || ! control_interface ) <nl> + return - ENODEV ; <nl> goto skip_normal_probe ; <nl> } <nl> 
static int wm8731_probe ( struct snd_soc_codec * codec ) <nl> err_regulator_get : <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8731 -> supplies ), wm8731 -> supplies ); <nl>  <nl> - kfree ( wm8731 ); <nl> return ret ; <nl> } <nl> 
static int snd_usb_copy_string_desc ( struct mixer_build * state , <nl> int index , char * buf , int maxlen ) <nl> { <nl> int len = usb_string ( state -> chip -> dev , index , buf , maxlen - 1 ); <nl> + <nl> + if ( len < 0 ) <nl> + return 0 ; <nl> + <nl> buf [ len ] = 0 ; <nl> return len ; <nl> }
static struct snd_soc_codec_driver soc_codec_device_ak4104 = { <nl> . probe = ak4104_probe , <nl> . remove = ak4104_remove , <nl> . reg_cache_size = AK4104_NUM_REGS , <nl> - . reg_word_size = sizeof ( u16 ), <nl> + . reg_word_size = sizeof ( u8 ), <nl> }; <nl>  <nl> static int ak4104_spi_probe ( struct spi_device * spi )
static unsigned long ext4_get_stripe_size ( struct ext4_sb_info * sbi ) <nl>  <nl> if ( sbi -> s_stripe && sbi -> s_stripe <= sbi -> s_blocks_per_group ) <nl> ret = sbi -> s_stripe ; <nl> - else if ( stripe_width <= sbi -> s_blocks_per_group ) <nl> + else if ( stripe_width && stripe_width <= sbi -> s_blocks_per_group ) <nl> ret = stripe_width ; <nl> - else if ( stride <= sbi -> s_blocks_per_group ) <nl> + else if ( stride && stride <= sbi -> s_blocks_per_group ) <nl> ret = stride ; <nl> else <nl> ret = 0 ;
static int iio_device_add_event ( struct iio_dev * indio_dev , <nl> & indio_dev -> event_interface -> dev_attr_list ); <nl> kfree ( postfix ); <nl>  <nl> + if (( ret == - EBUSY ) && ( shared_by != IIO_SEPARATE )) <nl> + continue ; <nl> + <nl> if ( ret ) <nl> return ret ; <nl> 
static void finish_csr_load ( const struct firmware * fw , void * context ) <nl> } <nl> csr -> mmio_count = dmc_header -> mmio_count ; <nl> for ( i = 0 ; i < dmc_header -> mmio_count ; i ++) { <nl> - if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE && <nl> + if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE || <nl> dmc_header -> mmioaddr [ i ] > CSR_MMIO_END_RANGE ) { <nl> DRM_ERROR (" Firmware has wrong mmio address 0x % x \ n ", <nl> dmc_header -> mmioaddr [ i ]);
static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> val ); <nl>  <nl> case LIRC_SET_REC_CARRIER_RANGE : <nl> + if (! dev -> s_rx_carrier_range ) <nl> + return - ENOTTY ; <nl> + <nl> if ( val <= 0 ) <nl> return - EINVAL ; <nl>  <nl> static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> break ; <nl>  <nl> case LIRC_SET_REC_TIMEOUT_REPORTS : <nl> + if (! dev -> timeout ) <nl> + return - ENOTTY ; <nl> + <nl> lirc -> send_timeout_reports = !! val ; <nl> break ; <nl> 
static struct ib_qp * i40iw_create_qp ( struct ib_pd * ibpd , <nl> return & iwqp -> ibqp ; <nl> error : <nl> i40iw_free_qp_resources ( iwdev , iwqp , qp_num ); <nl> - kfree ( mem ); <nl> return ERR_PTR ( err_code ); <nl> } <nl> 
struct hisi_clock_data * hisi_clk_init ( struct device_node * np , <nl> goto err ; <nl> } <nl> clk_data -> base = base ; <nl> - <nl> - clk_table = kzalloc ( sizeof ( struct clk *) * nr_clks , GFP_KERNEL ); <nl> + clk_table = kcalloc ( nr_clks , sizeof (* clk_table ), GFP_KERNEL ); <nl> if (! clk_table ) { <nl> pr_err ("% s : could not allocate clock lookup table \ n ", __func__ ); <nl> goto err_data ;
static inline struct crypto_kpp * crypto_kpp_reqtfm ( struct kpp_request * req ) <nl> return __crypto_kpp_tfm ( req -> base . tfm ); <nl> } <nl>  <nl> + static inline u32 crypto_kpp_get_flags ( struct crypto_kpp * tfm ) <nl> +{ <nl> + return crypto_tfm_get_flags ( crypto_kpp_tfm ( tfm )); <nl> +} <nl> + <nl> + static inline void crypto_kpp_set_flags ( struct crypto_kpp * tfm , u32 flags ) <nl> +{ <nl> + crypto_tfm_set_flags ( crypto_kpp_tfm ( tfm ), flags ); <nl> +} <nl> + <nl> /** <nl> * crypto_free_kpp () - free KPP tfm handle <nl> *
xfs_growfs_rt ( <nl> /* <nl> * Initial error checking . <nl> */ <nl> - if ( mp -> m_rtdev_targp || mp -> m_rbmip == NULL || <nl> + if ( mp -> m_rtdev_targp == NULL || mp -> m_rbmip == NULL || <nl> ( nrblocks = in -> newblocks ) <= sbp -> sb_rblocks || <nl> ( sbp -> sb_rblocks && ( in -> extsize != sbp -> sb_rextsize ))) <nl> return XFS_ERROR ( EINVAL );
static int ucode_init ( loader_block * lb , amb_dev * dev ) <nl> const struct firmware * fw ; <nl> unsigned long start_address ; <nl> const struct ihex_binrec * rec ; <nl> - const char * errmsg = 0 ; <nl> + const char * errmsg = NULL ; <nl> int res ; <nl>  <nl> res = request_ihex_firmware (& fw , " atmsar11 . fw ", & dev -> pci_dev -> dev );
static int pty_write ( struct tty_struct * tty , const unsigned char * buf , <nl>  <nl> static int pty_write_room ( struct tty_struct * tty ) <nl> { <nl> + if ( tty -> stopped ) <nl> + return 0 ; <nl> return pty_space ( tty -> link ); <nl> } <nl> 
static inline bool is_new_conn_expected ( const struct ip_vs_conn * cp , <nl> switch ( cp -> protocol ) { <nl> case IPPROTO_TCP : <nl> return ( cp -> state == IP_VS_TCP_S_TIME_WAIT ) || <nl> + ( cp -> state == IP_VS_TCP_S_CLOSE ) || <nl> (( conn_reuse_mode & 2 ) && <nl> ( cp -> state == IP_VS_TCP_S_FIN_WAIT ) && <nl> ( cp -> flags & IP_VS_CONN_F_NOOUTPUT ));
static void ks_wlan_hw_rx ( struct ks_wlan_private * priv , uint16_t size ) <nl> int ret ; <nl> struct rx_device_buffer * rx_buffer ; <nl> struct hostif_hdr * hdr ; <nl> - unsigned short event = 0 ; <nl> + u16 event = 0 ; <nl>  <nl> /* receive data */ <nl> if ( rxq_count ( priv ) >= ( RX_DEVICE_BUFF_SIZE - 1 )) {
static inline int virtqueue_add ( struct virtqueue * _vq , <nl> * host should service the ring ASAP . */ <nl> if ( out_sgs ) <nl> vq -> notify (& vq -> vq ); <nl> + if ( indirect ) <nl> + kfree ( desc ); <nl> END_USE ( vq ); <nl> return - ENOSPC ; <nl> }
static void after_state_ch ( struct drbd_conf * mdev , union drbd_state os , <nl> drbd_free_bc ( mdev -> ldev ); <nl> mdev -> ldev = NULL ;); <nl>  <nl> - if ( mdev -> md_io_tmpp ) <nl> + if ( mdev -> md_io_tmpp ) { <nl> __free_page ( mdev -> md_io_tmpp ); <nl> + mdev -> md_io_tmpp = NULL ; <nl> + } <nl> } <nl>  <nl> /* Disks got bigger while they were detached */
vbuschannel_itoa ( char * p , int remain , int num ) <nl> } <nl> /* form a backwards decimal ascii string in < s > */ <nl> while ( num > 0 ) { <nl> - if ( digits >= ( int ) sizeof ( s )) <nl> + if ( digits >= ( int ) sizeof ( s )) <nl> return 0 ; <nl> s [ digits ++] = ( num % 10 ) + ' 0 '; <nl> num = num / 10 ;
static struct ext4_new_flex_group_data * alloc_flex_gd ( unsigned long flexbg_size ) <nl> if ( flex_gd == NULL ) <nl> goto out3 ; <nl>  <nl> - if ( flexbg_size >= UINT_MAX / sizeof ( struct ext4_new_flex_group_data )) <nl> + if ( flexbg_size >= UINT_MAX / sizeof ( struct ext4_new_group_data )) <nl> goto out2 ; <nl> flex_gd -> count = flexbg_size ; <nl> 
static void snd_soc_instantiate_card ( struct snd_soc_card * card ) <nl> snd_soc_dapm_add_routes (& card -> dapm , card -> dapm_routes , <nl> card -> num_dapm_routes ); <nl>  <nl> + snd_soc_dapm_new_widgets (& card -> dapm ); <nl> + <nl> for ( i = 0 ; i < card -> num_links ; i ++) { <nl> dai_link = & card -> dai_link [ i ]; <nl> 
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
int inode_init_always ( struct super_block * sb , struct inode * inode ) <nl> mapping -> a_ops = & empty_aops ; <nl> mapping -> host = inode ; <nl> mapping -> flags = 0 ; <nl> + mapping -> wb_err = 0 ; <nl> atomic_set (& mapping -> i_mmap_writable , 0 ); <nl> mapping_set_gfp_mask ( mapping , GFP_HIGHUSER_MOVABLE ); <nl> mapping -> private_data = NULL ;
static int pit_ioport_read ( struct kvm_io_device * this , <nl> return - EOPNOTSUPP ; <nl>  <nl> addr &= KVM_PIT_CHANNEL_MASK ; <nl> + if ( addr == 3 ) <nl> + return 0 ; <nl> + <nl> s = & pit_state -> channels [ addr ]; <nl>  <nl> mutex_lock (& pit_state -> lock );
static int ieee80211_set_cqm_rssi_config ( struct wiphy * wiphy , <nl>  <nl> bss_conf -> cqm_rssi_thold = rssi_thold ; <nl> bss_conf -> cqm_rssi_hyst = rssi_hyst ; <nl> + sdata -> u . mgd . last_cqm_event_signal = 0 ; <nl>  <nl> /* tell the driver upon association , unless already associated */ <nl> if ( sdata -> u . mgd . associated &&
static int nfs4_stat_to_errno ( int ); <nl> 2 + encode_verifier_maxsz + 5 + \ <nl> nfs4_label_maxsz ) <nl> # define decode_readdir_maxsz ( op_decode_hdr_maxsz + \ <nl> - decode_verifier_maxsz + \ <nl> - nfs4_label_maxsz + nfs4_fattr_maxsz ) <nl> + decode_verifier_maxsz ) <nl> # define encode_readlink_maxsz ( op_encode_hdr_maxsz ) <nl> # define decode_readlink_maxsz ( op_decode_hdr_maxsz + 1 ) <nl> # define encode_write_maxsz ( op_encode_hdr_maxsz + \
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> b43_set_phytxctl_defaults ( dev ); <nl>  <nl> /* Minimum Contention Window */ <nl> - if ( phy -> type == B43_PHYTYPE_B ) { <nl> + if ( phy -> type == B43_PHYTYPE_B ) <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0x1F ); <nl> - } else { <nl> + else <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0xF ); <nl> - } <nl> /* Maximum Contention Window */ <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MAXCONT , 0x3FF ); <nl> 
int reiserfs_acl_chmod ( struct inode * inode ) <nl> return 0 ; <nl> } <nl>  <nl> + reiserfs_write_unlock ( inode -> i_sb ); <nl> acl = reiserfs_get_acl ( inode , ACL_TYPE_ACCESS ); <nl> + reiserfs_write_lock ( inode -> i_sb ); <nl> if (! acl ) <nl> return 0 ; <nl> if ( IS_ERR ( acl ))
static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> mutex_lock (& minors_lock ); <nl> dev = hidraw_table [ minor ]; <nl> + if (! dev ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl>  <nl> switch ( cmd ) { <nl> case HIDIOCGRDESCSIZE : <nl> static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> ret = - ENOTTY ; <nl> } <nl> + out : <nl> mutex_unlock (& minors_lock ); <nl> return ret ; <nl> }
static int __devinit hvcs_initialize ( void ) <nl> num_ttys_to_alloc = hvcs_parm_num_devs ; <nl>  <nl> hvcs_tty_driver = alloc_tty_driver ( num_ttys_to_alloc ); <nl> - if (! hvcs_tty_driver ) <nl> + if (! hvcs_tty_driver ) { <nl> + mutex_unlock (& hvcs_init_mutex ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if ( hvcs_alloc_index_list ( num_ttys_to_alloc )) { <nl> rc = - ENOMEM ;
int max8998_irq_init ( struct max8998_dev * max8998 ) <nl>  <nl> void max8998_irq_exit ( struct max8998_dev * max8998 ) <nl> { <nl> + if ( max8998 -> ono ) <nl> + free_irq ( max8998 -> ono , max8998 ); <nl> + <nl> if ( max8998 -> irq ) <nl> free_irq ( max8998 -> irq , max8998 ); <nl> }
long kvm_vm_ioctl_create_spapr_tce ( struct kvm * kvm , <nl> int ret = - ENOMEM ; <nl> int i ; <nl>  <nl> - if (! args -> size ) <nl> + if (! args -> size || args -> page_shift < 12 || args -> page_shift > 34 || <nl> + ( args -> offset + args -> size > ( ULLONG_MAX >> args -> page_shift ))) <nl> return - EINVAL ; <nl>  <nl> size = _ALIGN_UP ( args -> size , PAGE_SIZE >> 3 );
void ieee80211_sta_rx_notify ( struct ieee80211_sub_if_data * sdata , <nl> if ( is_multicast_ether_addr ( hdr -> addr1 )) <nl> return ; <nl>  <nl> + /* <nl> + * In case we receive frames after disassociation . <nl> + */ <nl> + if (! sdata -> u . mgd . associated ) <nl> + return ; <nl> + <nl> ieee80211_sta_reset_conn_monitor ( sdata ); <nl> } <nl> 
struct rchan * relay_open ( const char * base_filename , <nl>  <nl> kref_put (& chan -> kref , relay_destroy_channel ); <nl> mutex_unlock (& relay_channels_mutex ); <nl> + kfree ( chan ); <nl> return NULL ; <nl> } <nl> EXPORT_SYMBOL_GPL ( relay_open );
int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
static int aic3x_set_power ( struct snd_soc_codec * codec , int power ) <nl>  <nl> /* Sync reg_cache with the hardware */ <nl> codec -> cache_only = 0 ; <nl> - for ( i = 0 ; i < ARRAY_SIZE ( aic3x_reg ); i ++) <nl> + for ( i = AIC3X_SAMPLE_RATE_SEL_REG ; i < ARRAY_SIZE ( aic3x_reg ); i ++) <nl> snd_soc_write ( codec , i , cache [ i ]); <nl> if ( aic3x -> model == AIC3X_MODEL_3007 ) <nl> aic3x_init_3007 ( codec );
qeth_l3_add_mc_to_hash ( struct qeth_card * card , struct in_device * in4_dev ) <nl>  <nl> tmp -> u . a4 . addr = be32_to_cpu ( im4 -> multiaddr ); <nl> memcpy ( tmp -> mac , buf , sizeof ( tmp -> mac )); <nl> + tmp -> is_multicast = 1 ; <nl>  <nl> ipm = qeth_l3_ip_from_hash ( card , tmp ); <nl> if ( ipm ) {
int snd_ctl_add ( struct snd_card * card , struct snd_kcontrol * kcontrol ) <nl> if ( snd_BUG_ON (! card || ! kcontrol -> info )) <nl> goto error ; <nl> id = kcontrol -> id ; <nl> + if ( id . index > UINT_MAX - kcontrol -> count ) <nl> + goto error ; <nl> + <nl> down_write (& card -> controls_rwsem ); <nl> if ( snd_ctl_find_id ( card , & id )) { <nl> up_write (& card -> controls_rwsem );
static void __ibmvnic_reset ( struct work_struct * work ) <nl>  <nl> if ( rc ) { <nl> free_all_rwi ( adapter ); <nl> + mutex_unlock (& adapter -> reset_lock ); <nl> return ; <nl> } <nl> 
static int parse_output ( struct hda_codec * codec ) <nl> memcpy ( cfg -> speaker_pins , cfg -> line_out_pins , <nl> sizeof ( cfg -> speaker_pins )); <nl> cfg -> line_outs = 0 ; <nl> + memset ( cfg -> line_out_pins , 0 , sizeof ( cfg -> line_out_pins )); <nl> } <nl>  <nl> return 0 ;
int bdi_register ( struct backing_dev_info * bdi , struct device * parent , <nl> int ret = 0 ; <nl> struct device * dev ; <nl>  <nl> + if ( WARN_ON ( bdi -> dev )) <nl> + goto exit ; <nl> + <nl> va_start ( args , fmt ); <nl> dev = device_create_vargs ( bdi_class , parent , MKDEV ( 0 , 0 ), bdi , fmt , args ); <nl> va_end ( args );
static inline int _loop ( unsigned dry_run , u8 buf [], <nl> unsigned lcnt0 , lcnt1 , ljmp0 , ljmp1 ; <nl> struct _arg_LPEND lpend ; <nl>  <nl> + if (* bursts == 1 ) <nl> + return _bursts ( dry_run , buf , pxs , 1 ); <nl> + <nl> /* Max iterations possible in DMALP is 256 */ <nl> if (* bursts >= 256 * 256 ) { <nl> lcnt1 = 256 ;
copy_user_handle_tail ( char * to , char * from , unsigned len , unsigned zerorest ) <nl> char c ; <nl> unsigned zero_len ; <nl>  <nl> - for (; len ; -- len ) { <nl> + for (; len ; -- len , to ++) { <nl> if ( __get_user_nocheck ( c , from ++, sizeof ( char ))) <nl> break ; <nl> - if ( __put_user_nocheck ( c , to ++, sizeof ( char ))) <nl> + if ( __put_user_nocheck ( c , to , sizeof ( char ))) <nl> break ; <nl> } <nl> 
static const struct ich_laptop ich_laptop [] = { <nl> { 0x27DF , 0x0005 , 0x0280 }, /* ICH7 on Acer 5602WLMi */ <nl> { 0x27DF , 0x1025 , 0x0110 }, /* ICH7 on Acer 3682WLMi */ <nl> { 0x27DF , 0x1043 , 0x1267 }, /* ICH7 on Asus W5F */ <nl> + { 0x24CA , 0x1025 , 0x0061 }, /* ICH4 on ACER Aspire 2023WLMi */ <nl> /* end marker */ <nl> { 0 , } <nl> };
static void lut_close ( struct i915_gem_context * ctx ) <nl> kmem_cache_free ( ctx -> i915 -> luts , lut ); <nl> } <nl>  <nl> + rcu_read_lock (); <nl> radix_tree_for_each_slot ( slot , & ctx -> handles_vma , & iter , 0 ) { <nl> struct i915_vma * vma = rcu_dereference_raw (* slot ); <nl> struct drm_i915_gem_object * obj = vma -> obj ; <nl> static void lut_close ( struct i915_gem_context * ctx ) <nl>  <nl> __i915_gem_object_release_unless_active ( obj ); <nl> } <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> static void i915_gem_context_free ( struct i915_gem_context * ctx )
static void tg3_phy_toggle_apd ( struct tg3 * tp , bool enable ) <nl> { <nl> u32 reg ; <nl>  <nl> - if (!( tp -> tg3_flags2 & TG3_FLG2_5705_PLUS )) <nl> + if (!( tp -> tg3_flags2 & TG3_FLG2_5705_PLUS ) || <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5906 ) <nl> return ; <nl>  <nl> reg = MII_TG3_MISC_SHDW_WREN |
static bool malidp_check_pages_threshold ( struct malidp_plane_state * ms , <nl> else <nl> sgt = obj -> funcs -> get_sg_table ( obj ); <nl>  <nl> - if (! sgt ) <nl> + if ( IS_ERR ( sgt )) <nl> return false ; <nl>  <nl> sgl = sgt -> sgl ;
static int mon_bin_mmap ( struct file * filp , struct vm_area_struct * vma ) <nl> { <nl> /* don ' t do anything here : " fault " will set up page table entries */ <nl> vma -> vm_ops = & mon_bin_vm_ops ; <nl> + <nl> + if ( vma -> vm_flags & VM_WRITE ) <nl> + return - EPERM ; <nl> + <nl> + vma -> vm_flags &= ~ VM_MAYWRITE ; <nl> vma -> vm_flags |= VM_DONTEXPAND | VM_DONTDUMP ; <nl> vma -> vm_private_data = filp -> private_data ; <nl> mon_bin_vma_open ( vma );
static void cx_auto_check_auto_mic ( struct hda_codec * codec ) <nl> int pset [ INPUT_PIN_ATTR_NORMAL + 1 ]; <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < INPUT_PIN_ATTR_NORMAL ; i ++) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( pset ); i ++) <nl> pset [ i ] = - 1 ; <nl> for ( i = 0 ; i < spec -> private_imux . num_items ; i ++) { <nl> hda_nid_t pin = spec -> imux_info [ i ]. pin ;
static int __ext4_ext_check ( const char * function , unsigned int line , <nl> error_msg = " invalid extent entries "; <nl> goto corrupted ; <nl> } <nl> + if ( unlikely ( depth > 32 )) { <nl> + error_msg = " too large eh_depth "; <nl> + goto corrupted ; <nl> + } <nl> /* Verify checksum on non - root extent tree nodes */ <nl> if ( ext_depth ( inode ) != depth && <nl> ! ext4_extent_block_csum_verify ( inode , eh )) {
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( sta -> sta . txq [ 0 ]) <nl> kfree ( to_txq_info ( sta -> sta . txq [ 0 ])); <nl> free : <nl> + free_percpu ( sta -> pcpu_rx_stats ); <nl> # ifdef CONFIG_MAC80211_MESH <nl> kfree ( sta -> mesh ); <nl> # endif
int imx_drm_add_crtc ( struct drm_crtc * crtc , <nl>  <nl> mutex_lock (& imxdrm -> mutex ); <nl>  <nl> + /* <nl> + * The vblank arrays are dimensioned by MAX_CRTC - we can ' t <nl> + * pass IDs greater than this to those functions . <nl> + */ <nl> + if ( imxdrm -> pipes >= MAX_CRTC ) { <nl> + ret = - EINVAL ; <nl> + goto err_busy ; <nl> + } <nl> + <nl> if ( imxdrm -> drm -> open_count ) { <nl> ret = - EBUSY ; <nl> goto err_busy ;
static int parse_mount_options ( struct ceph_mount_options ** pfsopt , <nl>  <nl> fsopt -> rsize = CEPH_MOUNT_RSIZE_DEFAULT ; <nl> fsopt -> snapdir_name = kstrdup ( CEPH_SNAPDIRNAME_DEFAULT , GFP_KERNEL ); <nl> + fsopt -> caps_wanted_delay_min = CEPH_CAPS_WANTED_DELAY_MIN_DEFAULT ; <nl> + fsopt -> caps_wanted_delay_max = CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT ; <nl> fsopt -> cap_release_safety = CEPH_CAP_RELEASE_SAFETY_DEFAULT ; <nl> fsopt -> max_readdir = CEPH_MAX_READDIR_DEFAULT ; <nl> fsopt -> max_readdir_bytes = CEPH_MAX_READDIR_BYTES_DEFAULT ;
asmlinkage long sys_rt_sigreturn_wrapper ( void ); <nl> * The sys_call_table array must be 4K aligned to be accessible from <nl> * kernel / entry . S . <nl> */ <nl> - void * sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> + void * const sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> [ 0 ... __NR_syscalls - 1 ] = sys_ni_syscall , <nl> # include < asm / unistd . h > <nl> };
static int sony_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> } <nl>  <nl> ret = sony_set_operational ( hdev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_stop ; <nl>  <nl> return 0 ;
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , u32 features ) <nl> skb -> ip_summed = CHECKSUM_NONE ; <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> - if (( skb_headroom ( skb ) < frag_hdr_sz ) && <nl> + if (( skb_mac_header ( skb ) < skb -> head + frag_hdr_sz ) && <nl> pskb_expand_head ( skb , frag_hdr_sz , 0 , GFP_ATOMIC )) <nl> goto out ; <nl> 
long dgnc_mgmt_ioctl ( struct file * file , unsigned int cmd , unsigned long arg ) <nl>  <nl> spin_lock_irqsave (& dgnc_global_lock , flags ); <nl>  <nl> + memset (& ddi , 0 , sizeof ( ddi )); <nl> ddi . dinfo_nboards = dgnc_NumBoards ; <nl> sprintf ( ddi . dinfo_version , "% s ", DG_PART ); <nl> 
static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> cnt += tcp_skb_pcount ( skb ); <nl>  <nl> if ( cnt > packets ) { <nl> - if ( tcp_is_sack ( tp ) || ( oldcnt >= packets )) <nl> + if (( tcp_is_sack ( tp ) && ! tcp_is_fack ( tp )) || <nl> + ( oldcnt >= packets )) <nl> break ; <nl>  <nl> mss = skb_shinfo ( skb )-> gso_size ;
static int __devinit snd_interwave_pnp ( int dev , struct snd_interwave * iwcard , <nl> struct pnp_resource_table * cfg = kmalloc ( sizeof ( struct pnp_resource_table ), GFP_KERNEL ); <nl> int err ; <nl>  <nl> + if (! cfg ) <nl> + return - ENOMEM ; <nl> iwcard -> dev = pnp_request_card_device ( card , id -> devs [ 0 ]. id , NULL ); <nl> if ( iwcard -> dev == NULL ) { <nl> kfree ( cfg );
static int init_volumes ( struct ubi_device * ubi , const struct ubi_scan_info * si , <nl> if ( ubi -> autoresize_vol_id != - 1 ) { <nl> ubi_err (" more then one auto - resize volume (% d " <nl> " and % d )", ubi -> autoresize_vol_id , i ); <nl> + kfree ( vol ); <nl> return - EINVAL ; <nl> } <nl> 
static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
int st_sensors_check_device_support ( struct iio_dev * indio_dev , <nl> break ; <nl> } <nl> if ( n == ARRAY_SIZE ( sensor_settings [ i ]. sensors_supported )) { <nl> - dev_err (& indio_dev -> dev , " device name and WhoAmI mismatch .\ n "); <nl> + dev_err (& indio_dev -> dev , " device name \"% s \" and WhoAmI ( 0x % 02x ) mismatch ", <nl> + indio_dev -> name , wai ); <nl> goto sensor_name_mismatch ; <nl> } <nl> 
static int fastrpc_dma_buf_attach ( struct dma_buf * dmabuf , <nl> FASTRPC_PHYS ( buffer -> phys ), buffer -> size ); <nl> if ( ret < 0 ) { <nl> dev_err ( buffer -> dev , " failed to get scatterlist from DMA API \ n "); <nl> + kfree ( a ); <nl> return - EINVAL ; <nl> } <nl> 
static int rt6_score_route ( struct rt6_info * rt , int oif , <nl> m |= IPV6_DECODE_PREF ( IPV6_EXTRACT_PREF ( rt -> rt6i_flags )) << 2 ; <nl> # endif <nl> n = rt6_check_neigh ( rt ); <nl> - if ( n > 1 ) <nl> - m |= 16 ; <nl> - else if (! n && strict & RT6_LOOKUP_F_REACHABLE ) <nl> + if (! n && ( strict & RT6_LOOKUP_F_REACHABLE )) <nl> return - 1 ; <nl> return m ; <nl> }
static void zynqmp_dma_chan_remove ( struct zynqmp_dma_chan * chan ) <nl> if (! chan ) <nl> return ; <nl>  <nl> - devm_free_irq ( chan -> zdev -> dev , chan -> irq , chan ); <nl> + if ( chan -> irq ) <nl> + devm_free_irq ( chan -> zdev -> dev , chan -> irq , chan ); <nl> tasklet_kill (& chan -> tasklet ); <nl> list_del (& chan -> common . device_node ); <nl> }
static int btrfs_extent_same ( struct inode * src , u64 loff , u64 olen , <nl> inode_lock ( src ); <nl>  <nl> ret = extent_same_check_offsets ( src , loff , & len , olen ); <nl> + if ( ret ) <nl> + goto out_unlock ; <nl> + ret = extent_same_check_offsets ( src , dst_loff , & len , olen ); <nl> if ( ret ) <nl> goto out_unlock ; <nl> 
char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> compose_mount_options_err : <nl> kfree ( mountdata ); <nl> mountdata = ERR_PTR ( rc ); <nl> + kfree (* devname ); <nl> + * devname = NULL ; <nl> goto compose_mount_options_out ; <nl> } <nl> 
static int iscsi_handle_reject ( struct iscsi_conn * conn , struct iscsi_hdr * hdr , <nl> if ( opcode != ISCSI_OP_NOOP_OUT ) <nl> return 0 ; <nl>  <nl> - if ( rejected_pdu . itt == cpu_to_be32 ( ISCSI_RESERVED_TAG )) { <nl> + if ( rejected_pdu . itt == cpu_to_be32 ( ISCSI_RESERVED_TAG )) { <nl> /* <nl> * nop - out in response to target ' s nop - out rejected . <nl> * Just resend .
static int rpmsg_dev_probe ( struct device * dev ) <nl> goto out ; <nl> } <nl>  <nl> - if ( rpdev -> ops -> announce_create ) <nl> + if ( ept && rpdev -> ops -> announce_create ) <nl> err = rpdev -> ops -> announce_create ( rpdev ); <nl> out : <nl> return err ;
static int __ipmi_bmc_register ( struct ipmi_smi * intf , <nl> bmc -> pdev . name = " ipmi_bmc "; <nl>  <nl> rv = ida_simple_get (& ipmi_bmc_ida , 0 , 0 , GFP_KERNEL ); <nl> - if ( rv < 0 ) <nl> + if ( rv < 0 ) { <nl> + kfree ( bmc ); <nl> goto out ; <nl> + } <nl> + <nl> bmc -> pdev . dev . driver = & ipmidriver . driver ; <nl> bmc -> pdev . id = rv ; <nl> bmc -> pdev . dev . release = release_bmc_device ;
static int lpc32xx_adc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if (( irq < 0 ) || ( irq >= NR_IRQS )) { <nl> + if ( irq <= 0 ) { <nl> dev_err (& pdev -> dev , " failed getting interrupt resource \ n "); <nl> return - EINVAL ; <nl> }
static long aac_compat_do_ioctl ( struct aac_dev * dev , unsigned cmd , unsigned long <nl> static int aac_compat_ioctl ( struct scsi_device * sdev , int cmd , void __user * arg ) <nl> { <nl> struct aac_dev * dev = ( struct aac_dev *) sdev -> host -> hostdata ; <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> return aac_compat_do_ioctl ( dev , cmd , ( unsigned long ) arg ); <nl> } <nl> 
int tcp_sendmsg ( struct sock * sk , struct msghdr * msg , size_t size ) <nl>  <nl> if (! skb_can_coalesce ( skb , i , pfrag -> page , <nl> pfrag -> offset )) { <nl> - if ( i == sysctl_max_skb_frags || ! sg ) { <nl> + if ( i >= sysctl_max_skb_frags || ! sg ) { <nl> tcp_mark_push ( tp , skb ); <nl> goto new_segment ; <nl> }
static int i40e_set_channels ( struct net_device * dev , <nl> * class queue mapping <nl> */ <nl> new_count = i40e_reconfig_rss_queues ( pf , count ); <nl> - if ( new_count > 1 ) <nl> + if ( new_count > 0 ) <nl> return 0 ; <nl> else <nl> return - EINVAL ;
migration_call ( struct notifier_block * nfb , unsigned long action , void * hcpu ) <nl> migrate_tasks ( rq ); <nl> BUG_ON ( rq -> nr_running != 1 ); /* the migration thread */ <nl> raw_spin_unlock_irqrestore (& rq -> lock , flags ); <nl> - break ; <nl> - <nl> - case CPU_DEAD : <nl> calc_load_migrate ( rq ); <nl> break ; <nl> # endif
void parse_events__free_terms ( struct list_head * terms ) <nl>  <nl> list_for_each_entry_safe ( term , h , terms , list ) <nl> free ( term ); <nl> - <nl> - free ( terms ); <nl> }
static void exynos5_powerdown_conf ( enum sys_powerdown mode ) <nl> void exynos_sys_powerdown_conf ( enum sys_powerdown mode ) <nl> { <nl> unsigned int i ; <nl> + const struct exynos_pmu_data * pmu_data ; <nl> + <nl> + if (! pmu_context ) <nl> + return ; <nl>  <nl> - const struct exynos_pmu_data * pmu_data = pmu_context -> pmu_data ; <nl> + pmu_data = pmu_context -> pmu_data ; <nl>  <nl> if ( pmu_data -> powerdown_conf ) <nl> pmu_data -> powerdown_conf ( mode );
static isolate_migrate_t isolate_migratepages ( struct zone * zone , <nl> low_pfn = isolate_migratepages_block ( cc , low_pfn , end_pfn , <nl> isolate_mode ); <nl>  <nl> - if (! low_pfn || cc -> contended ) <nl> + if (! low_pfn || cc -> contended ) { <nl> + acct_isolated ( zone , cc ); <nl> return ISOLATE_ABORT ; <nl> + } <nl>  <nl> /* <nl> * Either we isolated something and proceed with migration . Or
void DisableVGA ( volatile STG4000REG __iomem * pSTGReg ) <nl> { <nl> u32 tmp ; <nl> - volatile u32 count , i ; <nl> + volatile u32 count = 0 , i ; <nl>  <nl> /* Reset the VGA registers */ <nl> tmp = STG_READ_REG ( SoftwareReset );
long ext4_fallocate ( struct file * file , int mode , loff_t offset , loff_t len ) <nl> blkbits ) >> blkbits )) <nl> new_size = offset + len ; <nl> else <nl> - new_size = ( map . m_lblk + ret ) << blkbits ; <nl> + new_size = (( loff_t ) map . m_lblk + ret ) << blkbits ; <nl>  <nl> ext4_falloc_update_inode ( inode , mode , new_size , <nl> ( map . m_flags & EXT4_MAP_NEW ));
static struct sg_table * omap_gem_map_dma_buf ( <nl> /* this should be after _get_paddr () to ensure we have pages attached */ <nl> omap_gem_dma_sync ( obj , dir ); <nl>  <nl> - out : <nl> - if ( ret ) <nl> - return ERR_PTR ( ret ); <nl> return sg ; <nl> + out : <nl> + kfree ( sg ); <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> static void omap_gem_unmap_dma_buf ( struct dma_buf_attachment * attachment ,
static void setup_mailboxes ( int base_io , struct Scsi_Host * shpnt ); <nl> static int aha1542_restart ( struct Scsi_Host * shost ); <nl> static void aha1542_intr_handle ( struct Scsi_Host * shost ); <nl>  <nl> -# define aha1542_intr_reset ( base ) outb ( IRST , CONTROL ( base )) <nl> + static inline void aha1542_intr_reset ( u16 base ) <nl> +{ <nl> + outb ( IRST , CONTROL ( base )); <nl> +} <nl>  <nl> # define WAIT ( port , mask , allof , noneof ) \ <nl> { register int WAITbits ; \
char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> * string to the length of the original string to allow for worst case . <nl> */ <nl> md_len = strlen ( sb_mountdata ) + INET6_ADDRSTRLEN ; <nl> - mountdata = kzalloc ( md_len + 1 , GFP_KERNEL ); <nl> + mountdata = kzalloc ( md_len + sizeof (" ip =") + 1 , GFP_KERNEL ); <nl> if ( mountdata == NULL ) { <nl> rc = - ENOMEM ; <nl> goto compose_mount_options_err ;
void ath_init_leds ( struct ath_softc * sc ) <nl> { <nl> int ret ; <nl>  <nl> + if ( AR_SREV_9100 ( sc -> sc_ah )) <nl> + return ; <nl> + <nl> if ( sc -> sc_ah -> led_pin < 0 ) { <nl> if ( AR_SREV_9287 ( sc -> sc_ah )) <nl> sc -> sc_ah -> led_pin = ATH_LED_PIN_9287 ;
static int ocores_i2c_of_probe ( struct platform_device * pdev , <nl> & i2c -> reg_io_width ); <nl>  <nl> match = of_match_node ( ocores_i2c_match , pdev -> dev . of_node ); <nl> - if ( match && ( int ) match -> data == TYPE_GRLIB ) { <nl> + if ( match && ( long ) match -> data == TYPE_GRLIB ) { <nl> dev_dbg (& pdev -> dev , " GRLIB variant of i2c - ocores \ n "); <nl> i2c -> setreg = oc_setreg_grlib ; <nl> i2c -> getreg = oc_getreg_grlib ;
static void ohci_stop ( struct usb_hcd * hcd ) <nl>  <nl> ohci_usb_reset ( ohci ); <nl> ohci_writel ( ohci , OHCI_INTR_MIE , & ohci -> regs -> intrdisable ); <nl> - <nl> + free_irq ( hcd -> irq , hcd ); <nl> + hcd -> irq = - 1 ; <nl> + <nl> remove_debug_files ( ohci ); <nl> ohci_mem_cleanup ( ohci ); <nl> if ( ohci -> hcca ) {
static const char * intel_pt_err_msgs [] = { <nl>  <nl> int intel_pt__strerror ( int code , char * buf , size_t buflen ) <nl> { <nl> - if ( code < 1 || code > INTEL_PT_ERR_MAX ) <nl> + if ( code < 1 || code >= INTEL_PT_ERR_MAX ) <nl> code = INTEL_PT_ERR_UNK ; <nl> strlcpy ( buf , intel_pt_err_msgs [ code ], buflen ); <nl> return 0 ;
static void hub_activate ( struct usb_hub * hub , enum hub_activation_type type ) <nl> PREPARE_DELAYED_WORK (& hub -> init_work , hub_init_func2 ); <nl> schedule_delayed_work (& hub -> init_work , <nl> msecs_to_jiffies ( delay )); <nl> + <nl> + /* Suppress autosuspend until init is done */ <nl> + to_usb_interface ( hub -> intfdev )-> pm_usage_cnt = 1 ; <nl> return ; /* Continues at init2 : below */ <nl> } else { <nl> hub_power_on ( hub , true );
static int nand_default_block_markbad ( struct mtd_info * mtd , loff_t ofs ) <nl> int block , ret ; <nl>  <nl> /* Get block number */ <nl> - block = (( int ) ofs ) >> chip -> bbt_erase_shift ; <nl> + block = ( int )( ofs >> chip -> bbt_erase_shift ); <nl> if ( chip -> bbt ) <nl> chip -> bbt [ block >> 2 ] |= 0x01 << (( block & 0x03 ) << 1 ); <nl> 
int __kprobes arch_prepare_kprobe ( struct kprobe * p ) <nl> if (! ret ) { <nl> memcpy ( p -> ainsn . insn , p -> addr , MAX_INSN_SIZE * sizeof ( kprobe_opcode_t )); <nl> p -> opcode = * p -> addr ; <nl> + flush_icache_range (( unsigned long ) p -> ainsn . insn , <nl> + ( unsigned long ) p -> ainsn . insn + sizeof ( kprobe_opcode_t )); <nl> } <nl>  <nl> return ret ;
static int vivid_fb_ioctl ( struct fb_info * info , unsigned cmd , unsigned long arg ) <nl> case FBIOGET_VBLANK : { <nl> struct fb_vblank vblank ; <nl>  <nl> + memset (& vblank , 0 , sizeof ( vblank )); <nl> vblank . flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT | <nl> FB_VBLANK_HAVE_VSYNC ; <nl> vblank . count = 0 ;
static int meson_sar_adc_lock ( struct iio_dev * indio_dev ) <nl> regmap_read ( priv -> regmap , MESON_SAR_ADC_DELAY , & val ); <nl> } while ( val & MESON_SAR_ADC_DELAY_BL30_BUSY && timeout --); <nl>  <nl> - if ( timeout < 0 ) <nl> + if ( timeout < 0 ) { <nl> + mutex_unlock (& indio_dev -> mlock ); <nl> return - ETIMEDOUT ; <nl> + } <nl> } <nl>  <nl> return 0 ;
static int ems_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> goto err ; <nl> } <nl>  <nl> - emsff_init ( hdev ); <nl> + ret = emsff_init ( hdev ); <nl> + if ( ret ) { <nl> + dev_err (& hdev -> dev , " force feedback init failed \ n "); <nl> + hid_hw_stop ( hdev ); <nl> + goto err ; <nl> + } <nl>  <nl> return 0 ; <nl> err :
static void parse_bsd ( struct parsed_partitions * state , <nl> continue ; <nl> bsd_start = le32_to_cpu ( p -> p_offset ); <nl> bsd_size = le32_to_cpu ( p -> p_size ); <nl> + if ( memcmp ( flavour , " bsd \ 0 ", 4 ) == 0 ) <nl> + bsd_start += offset ; <nl> if ( offset == bsd_start && size == bsd_size ) <nl> /* full parent partition , we have it already */ <nl> continue ;
isofs_export_encode_fh ( struct inode * inode , <nl> len = 3 ; <nl> fh32 [ 0 ] = ei -> i_iget5_block ; <nl> fh16 [ 2 ] = ( __u16 ) ei -> i_iget5_offset ; /* fh16 [ sic ] */ <nl> + fh16 [ 3 ] = 0 ; /* avoid leaking uninitialized data */ <nl> fh32 [ 2 ] = inode -> i_generation ; <nl> if ( parent ) { <nl> struct iso_inode_info * eparent ;
static void uwb_drp_handle_alien_drp ( struct uwb_rc * rc , struct uwb_ie_drp * drp_i <nl>  <nl> /* alloc and initialize new uwb_cnflt_alien */ <nl> cnflt = kzalloc ( sizeof ( struct uwb_cnflt_alien ), GFP_KERNEL ); <nl> - if (! cnflt ) <nl> + if (! cnflt ) { <nl> dev_err ( dev , " failed to alloc uwb_cnflt_alien struct \ n "); <nl> + return ; <nl> + } <nl> + <nl> INIT_LIST_HEAD (& cnflt -> rc_node ); <nl> init_timer (& cnflt -> timer ); <nl> cnflt -> timer . function = uwb_cnflt_timer ;
int ext4_expand_extra_isize_ea ( struct inode * inode , int new_extra_isize , <nl> s_min_extra_isize ) { <nl> tried_min_extra_isize ++; <nl> new_extra_isize = s_min_extra_isize ; <nl> + kfree ( is ); is = NULL ; <nl> + kfree ( bs ); bs = NULL ; <nl> goto retry ; <nl> } <nl> error = - 1 ;
static bool lm3533_readable_register ( struct device * dev , unsigned int reg ) <nl> static bool lm3533_volatile_register ( struct device * dev , unsigned int reg ) <nl> { <nl> switch ( reg ) { <nl> - case 0x34 : /* zone */ <nl> + case 0x34 ... 0x36 : /* zone */ <nl> case 0x37 ... 0x38 : /* adc */ <nl> case 0xb0 ... 0xb1 : /* fault */ <nl> return true ;
static int mb86s70_gpio_request ( struct gpio_chip * gc , unsigned gpio ) <nl> spin_lock_irqsave (& gchip -> lock , flags ); <nl>  <nl> val = readl ( gchip -> base + PFR ( gpio )); <nl> + if (!( val & OFFSET ( gpio ))) { <nl> + spin_unlock_irqrestore (& gchip -> lock , flags ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> val &= ~ OFFSET ( gpio ); <nl> writel ( val , gchip -> base + PFR ( gpio )); <nl> 
int extent_range_uptodate ( struct extent_io_tree * tree , <nl> while ( start <= end ) { <nl> index = start >> PAGE_CACHE_SHIFT ; <nl> page = find_get_page ( tree -> mapping , index ); <nl> + if (! page ) <nl> + return 1 ; <nl> uptodate = PageUptodate ( page ); <nl> page_cache_release ( page ); <nl> if (! uptodate ) {
void * __raw_xsave_addr ( struct xregs_state * xsave , int xstate_feature_mask ) <nl> { <nl> int feature_nr = fls64 ( xstate_feature_mask ) - 1 ; <nl>  <nl> + if (! xfeature_enabled ( feature_nr )) { <nl> + WARN_ON_FPU ( 1 ); <nl> + return NULL ; <nl> + } <nl> + <nl> return ( void *) xsave + xstate_comp_offsets [ feature_nr ]; <nl> } <nl> /*
static long __validate_layout ( struct ceph_mds_client * mdsc , <nl> /* validate striping parameters */ <nl> if (( l -> object_size & ~ PAGE_MASK ) || <nl> ( l -> stripe_unit & ~ PAGE_MASK ) || <nl> - (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit )) <nl> + ( l -> stripe_unit != 0 && <nl> + (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit ))) <nl> return - EINVAL ; <nl>  <nl> /* make sure it ' s a valid data pool */
int ubi_wl_get_peb ( struct ubi_device * ubi ) <nl> peb = __wl_get_peb ( ubi ); <nl> spin_unlock (& ubi -> wl_lock ); <nl>  <nl> + if ( peb < 0 ) <nl> + return peb ; <nl> + <nl> err = ubi_self_check_all_ff ( ubi , peb , ubi -> vid_hdr_aloffset , <nl> ubi -> peb_size - ubi -> vid_hdr_aloffset ); <nl> if ( err ) {
int tusb6010_platform_retime ( unsigned is_refclk ) <nl> unsigned sysclk_ps ; <nl> int status ; <nl>  <nl> - if (! refclk_psec || sysclk_ps == 0 ) <nl> + if (! refclk_psec || fclk_ps == 0 ) <nl> return - ENODEV ; <nl>  <nl> sysclk_ps = is_refclk ? refclk_psec : TUSB6010_OSCCLK_60 ;
static int __devinit gen_74x164_probe ( struct spi_device * spi ) <nl> } <nl>  <nl> chip -> gpio_chip . ngpio = GEN_74X164_NUMBER_GPIOS * chip -> registers ; <nl> - chip -> buffer = devm_kzalloc (& spi -> dev , chip -> gpio_chip . ngpio , GFP_KERNEL ); <nl> + chip -> buffer = devm_kzalloc (& spi -> dev , chip -> registers , GFP_KERNEL ); <nl> if (! chip -> buffer ) { <nl> ret = - ENOMEM ; <nl> goto exit_destroy ;
int iwl_mvm_rx_card_state_notif ( struct iwl_mvm * mvm , <nl> ( flags & CT_KILL_CARD_DISABLED ) ? <nl> " Reached " : " Not reached "); <nl>  <nl> - if ( flags & CARD_DISABLED_MSK ) <nl> - iwl_write32 ( mvm -> trans , CSR_UCODE_DRV_GP1_SET , <nl> - CSR_UCODE_DRV_GP1_BIT_CMD_BLOCKED ); <nl> - <nl> return 0 ; <nl> } <nl> 
static struct zx_dma_desc_sw * zx_alloc_desc_resource ( int num , <nl> kfree ( ds ); <nl> return NULL ; <nl> } <nl> - memset ( ds -> desc_hw , sizeof ( struct zx_desc_hw ) * num , 0 ); <nl> + memset ( ds -> desc_hw , 0 , sizeof ( struct zx_desc_hw ) * num ); <nl> ds -> desc_num = num ; <nl> return ds ; <nl> }
static int ath10k_usb_hif_tx_sg ( struct ath10k * ar , u8 pipe_id , <nl> ath10k_dbg ( ar , ATH10K_DBG_USB_BULK , <nl> " usb bulk transmit failed : % d \ n ", ret ); <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> ret = - EINVAL ; <nl> goto err_free_urb_to_pipe ; <nl> }
static int annotate_browser__run ( struct annotate_browser * self , int evidx , <nl> timer , arg , delay_secs ); <nl> } <nl> break ; <nl> + case NEWT_KEY_LEFT : <nl> + case NEWT_KEY_ESCAPE : <nl> case ' q ': <nl> case CTRL (' c '): <nl> goto out ;
struct ring_buffer * __ring_buffer_alloc ( unsigned long size , unsigned flags , <nl> buffer -> reader_lock_key = key ; <nl>  <nl> /* need at least two pages */ <nl> - if ( buffer -> pages == 1 ) <nl> - buffer -> pages ++; <nl> + if ( buffer -> pages < 2 ) <nl> + buffer -> pages = 2 ; <nl>  <nl> /* <nl> * In case of non - hotplug cpu , if the ring - buffer is allocated
static int hdmi_get_edid ( void * ctx , struct drm_connector * connector , <nl> DRM_DEBUG_KMS ("% s : width [% d ] x height [% d ]\ n ", <nl> ( hdata -> dvi_mode ? " dvi monitor " : " hdmi monitor "), <nl> raw_edid -> width_cm , raw_edid -> height_cm ); <nl> + kfree ( raw_edid ); <nl> } else { <nl> return - ENODEV ; <nl> }
static void qlcnic_83xx_mailbox_worker ( struct work_struct * work ) <nl> __func__ , cmd -> cmd_op , cmd -> type , ahw -> pci_func , <nl> ahw -> op_mode ); <nl> clear_bit ( QLC_83XX_MBX_READY , & mbx -> status ); <nl> + qlcnic_dump_mbx ( adapter , cmd ); <nl> qlcnic_83xx_idc_request_reset ( adapter , <nl> QLCNIC_FORCE_FW_DUMP_KEY ); <nl> cmd -> rsp_opcode = QLCNIC_RCODE_TIMEOUT ;
static void radeon_compute_pll_legacy ( struct radeon_pll * pll , <nl> max_fractional_feed_div = pll -> max_frac_feedback_div ; <nl> } <nl>  <nl> - for ( post_div = min_post_div ; post_div <= max_post_div ; ++ post_div ) { <nl> + for ( post_div = max_post_div ; post_div >= min_post_div ; -- post_div ) { <nl> uint32_t ref_div ; <nl>  <nl> if (( pll -> flags & RADEON_PLL_NO_ODD_POST_DIV ) && ( post_div & 1 ))
static void tcp_cwnd_reduction ( struct sock * sk , const int prior_unsacked , <nl> int newly_acked_sacked = prior_unsacked - <nl> ( tp -> packets_out - tp -> sacked_out ); <nl>  <nl> + if ( newly_acked_sacked <= 0 || WARN_ON_ONCE (! tp -> prior_cwnd )) <nl> + return ; <nl> + <nl> tp -> prr_delivered += newly_acked_sacked ; <nl> if ( delta < 0 ) { <nl> u64 dividend = ( u64 ) tp -> snd_ssthresh * tp -> prr_delivered +
static bool assoc_array_insert_into_terminal_node ( struct assoc_array_edit * edit , <nl> free_slot = i ; <nl> continue ; <nl> } <nl> - if ( ops -> compare_object ( assoc_array_ptr_to_leaf ( ptr ), index_key )) { <nl> + if ( assoc_array_ptr_is_leaf ( ptr ) && <nl> + ops -> compare_object ( assoc_array_ptr_to_leaf ( ptr ), <nl> + index_key )) { <nl> pr_devel (" replace in slot % d \ n ", i ); <nl> edit -> leaf_p = & node -> slots [ i ]; <nl> edit -> dead_leaf = node -> slots [ i ];
void rtl88e_dm_watchdog ( struct ieee80211_hw * hw ) <nl> if ( ppsc -> p2p_ps_info . p2p_ps_mode ) <nl> fw_ps_awake = false ; <nl>  <nl> + spin_lock (& rtlpriv -> locks . rf_ps_lock ); <nl> if (( ppsc -> rfpwr_state == ERFON ) && <nl> ((! fw_current_inpsmode ) && fw_ps_awake ) && <nl> (! ppsc -> rfchange_inprogress )) { <nl> void rtl88e_dm_watchdog ( struct ieee80211_hw * hw ) <nl> rtl88e_dm_check_edca_turbo ( hw ); <nl> rtl88e_dm_antenna_diversity ( hw ); <nl> } <nl> + spin_unlock (& rtlpriv -> locks . rf_ps_lock ); <nl> }
static int l2tp_ip6_recvmsg ( struct kiocb * iocb , struct sock * sk , <nl> lsa -> l2tp_addr = ipv6_hdr ( skb )-> saddr ; <nl> lsa -> l2tp_flowinfo = 0 ; <nl> lsa -> l2tp_scope_id = 0 ; <nl> + lsa -> l2tp_conn_id = 0 ; <nl> if ( ipv6_addr_type (& lsa -> l2tp_addr ) & IPV6_ADDR_LINKLOCAL ) <nl> lsa -> l2tp_scope_id = IP6CB ( skb )-> iif ; <nl> }
int tcp_recvmsg ( struct kiocb * iocb , struct sock * sk , struct msghdr * msg , <nl>  <nl> cleanup_rbuf ( sk , copied ); <nl>  <nl> - if ( tp -> ucopy . task == user_recv ) { <nl> + if (! sysctl_tcp_low_latency && tp -> ucopy . task == user_recv ) { <nl> /* Install new reader */ <nl> if (! user_recv && !( flags & ( MSG_TRUNC | MSG_PEEK ))) { <nl> user_recv = current ;
static ssize_t state_store ( struct subsystem * subsys , const char * buf , size_t n <nl> if (* s && ! strncmp ( buf , * s , len )) <nl> break ; <nl> } <nl> - if (* s ) <nl> + if ( state < PM_SUSPEND_MAX && * s ) <nl> error = enter_state ( state ); <nl> else <nl> error = - EINVAL ;
static void scan_add_host ( struct soc_camera_host * ici ) <nl>  <nl> list_for_each_entry ( icd , & devices , list ) { <nl> if ( icd -> iface == ici -> nr ) { <nl> - int ret ; <nl> - <nl> icd -> parent = ici -> v4l2_dev . dev ; <nl> - ret = soc_camera_probe ( icd ); <nl> + soc_camera_probe ( icd ); <nl> } <nl> } <nl> 
struct bio * bch_bio_split ( struct bio * bio , int sectors , <nl>  <nl> if ( bio -> bi_rw & REQ_DISCARD ) { <nl> ret = bio_alloc_bioset ( gfp , 1 , bs ); <nl> + if (! ret ) <nl> + return NULL ; <nl> idx = 0 ; <nl> goto out ; <nl> }
int svc_register ( const struct svc_serv * serv , struct net * net , <nl> if ( vers -> vs_hidden ) <nl> continue ; <nl>  <nl> + /* <nl> + * Don ' t register a UDP port if we need congestion <nl> + * control . <nl> + */ <nl> + if ( vers -> vs_need_cong_ctrl && proto == IPPROTO_UDP ) <nl> + continue ; <nl> + <nl> error = __svc_register ( net , progp -> pg_name , progp -> pg_prog , <nl> i , family , proto , port ); <nl> 
verbose_printk (" btrfs : send_create_inode % llu \ n ", ino ); <nl> TLV_PUT_PATH ( sctx , BTRFS_SEND_A_PATH_LINK , p ); <nl> } else if ( S_ISCHR ( mode ) || S_ISBLK ( mode ) || <nl> S_ISFIFO ( mode ) || S_ISSOCK ( mode )) { <nl> - TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , rdev ); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , new_encode_dev ( rdev )); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_MODE , mode ); <nl> } <nl>  <nl> ret = send_cmd ( sctx );
static void <nl> iscsi_tcp_conn_stop ( struct iscsi_cls_conn * cls_conn , int flag ) <nl> { <nl> struct iscsi_conn * conn = cls_conn -> dd_data ; <nl> + struct iscsi_tcp_conn * tcp_conn = conn -> dd_data ; <nl>  <nl> iscsi_conn_stop ( cls_conn , flag ); <nl> iscsi_tcp_release_conn ( conn ); <nl> + tcp_conn -> hdr_size = sizeof ( struct iscsi_hdr ); <nl> } <nl>  <nl> static int
static int __devinit max17042_probe ( struct i2c_client * client , <nl> reg |= CONFIG_ALRT_BIT_ENBL ; <nl> max17042_write_reg ( client , MAX17042_CONFIG , reg ); <nl> max17042_set_soc_threshold ( chip , 1 ); <nl> - } else <nl> + } else { <nl> + client -> irq = 0 ; <nl> dev_err (& client -> dev , "% s (): cannot get IRQ \ n ", <nl> __func__ ); <nl> + } <nl> } <nl>  <nl> reg = max17042_read_reg ( chip -> client , MAX17042_STATUS );
static int ccs811_start_sensor_application ( struct i2c_client * client ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (( ret & CCS811_STATUS_FW_MODE_APPLICATION )) <nl> + return 0 ; <nl> + <nl> if (( ret & CCS811_STATUS_APP_VALID_MASK ) != <nl> CCS811_STATUS_APP_VALID_LOADED ) <nl> return - EIO ;
store_vrm_reg ( struct device * dev , struct device_attribute * attr , const char * buf <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> data -> vrm = val ; <nl>  <nl> return count ;
static int ni_660x_request_mite_channel ( struct comedi_device * dev , <nl> struct mite_channel * mite_chan ; <nl>  <nl> spin_lock_irqsave (& devpriv -> mite_channel_lock , flags ); <nl> - BUG_ON ( counter -> mite_chan ); <nl> mite_chan = mite_request_channel ( devpriv -> mite , <nl> mite_ring ( devpriv , counter )); <nl> if (! mite_chan ) {
static int start_afu ( struct cxlflash_cfg * cfg ) <nl>  <nl> init_pcr ( cfg ); <nl>  <nl> + /* After an AFU reset , RRQ entries are stale , clear them */ <nl> + memset (& afu -> rrq_entry , 0 , sizeof ( afu -> rrq_entry )); <nl> + <nl> /* Initialize RRQ pointers */ <nl> afu -> hrrq_start = & afu -> rrq_entry [ 0 ]; <nl> afu -> hrrq_end = & afu -> rrq_entry [ NUM_RRQ_ENTRY - 1 ];
static int at_dma_remove ( struct platform_device * pdev ) <nl>  <nl> /* Disable interrupts */ <nl> atc_disable_chan_irq ( atdma , chan -> chan_id ); <nl> - tasklet_disable (& atchan -> tasklet ); <nl>  <nl> tasklet_kill (& atchan -> tasklet ); <nl> list_del (& chan -> device_node );
static void isp_video_buffer_query ( struct isp_video_buffer * buf , <nl> switch ( buf -> state ) { <nl> case ISP_BUF_STATE_ERROR : <nl> vbuf -> flags |= V4L2_BUF_FLAG_ERROR ; <nl> + /* Fallthrough */ <nl> case ISP_BUF_STATE_DONE : <nl> vbuf -> flags |= V4L2_BUF_FLAG_DONE ; <nl> + break ; <nl> case ISP_BUF_STATE_QUEUED : <nl> case ISP_BUF_STATE_ACTIVE : <nl> vbuf -> flags |= V4L2_BUF_FLAG_QUEUED ;
static int __init parse_options ( struct early_uart_device * device , char * options ) <nl>  <nl> if (( options = strchr ( options , ','))) { <nl> options ++; <nl> - device -> baud = simple_strtoul ( options , 0 , 0 ); <nl> + device -> baud = simple_strtoul ( options , NULL , 0 ); <nl> length = min ( strcspn ( options , " "), sizeof ( device -> options )); <nl> strncpy ( device -> options , options , length ); <nl> } else {
static int __devinit tg3_get_invariants ( struct tg3 * tp ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5717 && <nl> + ( tp -> pci_chip_rev_id != CHIPREV_ID_5717_A0 || <nl> + ( tp -> tg3_flags2 & TG3_FLG2_MII_SERDES ))) <nl> + return - ENOTSUPP ; <nl> + <nl> /* Initialize data / descriptor byte / word swapping . */ <nl> val = tr32 ( GRC_MODE ); <nl> val &= GRC_MODE_HOST_STACKUP ;
enum batadv_dbg_level { <nl> BATADV_DBG_NC = BIT ( 5 ), <nl> BATADV_DBG_MCAST = BIT ( 6 ), <nl> BATADV_DBG_TP_METER = BIT ( 7 ), <nl> - BATADV_DBG_ALL = 127 , <nl> + BATADV_DBG_ALL = 255 , <nl> }; <nl>  <nl> # ifdef CONFIG_BATMAN_ADV_DEBUG
struct rockchip_thermal_data { <nl> # define TSADCV2_HIGHT_TSHUT_DEBOUNCE_COUNT 4 <nl> # define TSADCV2_AUTO_PERIOD_TIME 250 /* 250ms */ <nl> # define TSADCV2_AUTO_PERIOD_HT_TIME 50 /* 50ms */ <nl> -# define TSADCV3_AUTO_PERIOD_TIME 187500 /* 250ms */ <nl> -# define TSADCV3_AUTO_PERIOD_HT_TIME 37500 /* 50ms */ <nl> +# define TSADCV3_AUTO_PERIOD_TIME 1875 /* 2 . 5ms */ <nl> +# define TSADCV3_AUTO_PERIOD_HT_TIME 1875 /* 2 . 5ms */ <nl>  <nl> # define TSADCV2_USER_INTER_PD_SOC 0x340 /* 13 clocks */ <nl> 
qla25xx_process_bidir_status_iocb ( scsi_qla_host_t * vha , void * pkt , <nl> bsg_job -> reply_len = sizeof ( struct fc_bsg_reply ); <nl> /* Always return DID_OK , bsg will send the vendor specific response <nl> * in this case only */ <nl> - sp -> done ( sp , DID_OK << 6 ); <nl> + sp -> done ( sp , DID_OK << 16 ); <nl>  <nl> } <nl> 
static int inet6_rtm_getroute ( struct sk_buff * in_skb , struct nlmsghdr * nlh ) <nl> rt = ( struct rt6_info *) ip6_route_output ( net , NULL , & fl6 ); <nl> } <nl>  <nl> + if ( rt == net -> ipv6 . ip6_null_entry ) { <nl> + err = rt -> dst . error ; <nl> + ip6_rt_put ( rt ); <nl> + goto errout ; <nl> + } <nl> + <nl> skb = alloc_skb ( NLMSG_GOODSIZE , GFP_KERNEL ); <nl> if (! skb ) { <nl> ip6_rt_put ( rt );
int cx231xx_set_mode ( struct cx231xx * dev , enum cx231xx_mode set_mode ) <nl> } <nl> } <nl>  <nl> - return errCode ? - EINVAL : 0 ; <nl> + if ( errCode < 0 ) { <nl> + dev_err ( dev -> dev , " Failed to set devmode to % s : error : % i ", <nl> + dev -> mode == CX231XX_DIGITAL_MODE ? " digital " : " analog ", <nl> + errCode ); <nl> + return errCode ; <nl> + } <nl> + <nl> + return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( cx231xx_set_mode ); <nl> 
void rtl92se_disable_interrupt ( struct ieee80211_hw * hw ) <nl> rtl_write_dword ( rtlpriv , INTA_MASK + 4 , 0 ); <nl>  <nl> rtlpci -> irq_enabled = false ; <nl> + synchronize_irq ( rtlpci -> pdev -> irq ); <nl> } <nl>  <nl> 
static inline void wilc_wfi_cfg_parse_ch_attr ( u8 * buf , u32 len , u8 sta_ch ) <nl> if ( index + sizeof (* e ) + attr_size > len ) <nl> return ; <nl>  <nl> - if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST ) <nl> + if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST && <nl> + attr_size >= ( sizeof ( struct wilc_attr_ch_list ) - sizeof (* e ))) <nl> ch_list_idx = index ; <nl> else if ( e -> attr_type == IEEE80211_P2P_ATTR_OPER_CHANNEL && <nl> attr_size == ( sizeof ( struct wilc_attr_oper_ch ) - sizeof (* e )))
# include < linux / vmalloc . h > <nl>  <nl> # include < media / videobuf2 - core . h > <nl> +# include < media / videobuf2 - vmalloc . h > <nl> # include < media / videobuf2 - memops . h > <nl>  <nl> struct vb2_vmalloc_buf {
static void __init offb_init_fb ( const char * name , const char * full_name , <nl> return ; <nl> } <nl>  <nl> - size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 17 ; <nl> + size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 16 ; <nl>  <nl> info = kmalloc ( size , GFP_ATOMIC ); <nl> 
static void ccwgroup_ungroup_callback ( struct device * dev ) <nl> struct ccwgroup_device * gdev = to_ccwgroupdev ( dev ); <nl>  <nl> mutex_lock (& gdev -> reg_mutex ); <nl> - __ccwgroup_remove_symlinks ( gdev ); <nl> - device_unregister ( dev ); <nl> + if ( device_is_registered (& gdev -> dev )) { <nl> + __ccwgroup_remove_symlinks ( gdev ); <nl> + device_unregister ( dev ); <nl> + } <nl> mutex_unlock (& gdev -> reg_mutex ); <nl> } <nl> 
int ext3_sync_file ( struct file * file , struct dentry * dentry , int datasync ) <nl> goto out ; <nl> } <nl>  <nl> + if ( datasync && !( inode -> i_state & I_DIRTY_DATASYNC )) <nl> + goto out ; <nl> + <nl> /* <nl> * The VFS has written the file data . If the inode is unaltered <nl> * then we need not start a commit .
static int si_common_early_init ( void * handle ) <nl> AMD_CG_SUPPORT_HDP_LS | <nl> AMD_CG_SUPPORT_HDP_MGCG ; <nl> adev -> pg_flags = 0 ; <nl> + adev -> external_rev_id = 70 ; <nl> break ; <nl>  <nl> default :
xfs_errortag_add ( int error_tag , xfs_mount_t * mp ) <nl> int len ; <nl> int64_t fsid ; <nl>  <nl> + if ( error_tag >= XFS_ERRTAG_MAX ) <nl> + return - EINVAL ; <nl> + <nl> memcpy (& fsid , mp -> m_fixedfsid , sizeof ( xfs_fsid_t )); <nl>  <nl> for ( i = 0 ; i < XFS_NUM_INJECT_ERROR ; i ++) {
static void snd_timer_user_ccallback ( struct snd_timer_instance * timeri , <nl> tu -> tstamp = * tstamp ; <nl> if (( tu -> filter & ( 1 << event )) == 0 || ! tu -> tread ) <nl> return ; <nl> + memset (& r1 , 0 , sizeof ( r1 )); <nl> r1 . event = event ; <nl> r1 . tstamp = * tstamp ; <nl> r1 . val = resolution ;
static int xive_spapr_get_ipi ( unsigned int cpu , struct xive_cpu * xc ) <nl>  <nl> static void xive_spapr_put_ipi ( unsigned int cpu , struct xive_cpu * xc ) <nl> { <nl> + if (! xc -> hw_ipi ) <nl> + return ; <nl> + <nl> xive_irq_bitmap_free ( xc -> hw_ipi ); <nl> + xc -> hw_ipi = 0 ; <nl> } <nl> # endif /* CONFIG_SMP */ <nl> 
static int bcma_get_next_core ( struct bcma_bus * bus , u32 __iomem ** eromptr , <nl> switch ( core -> id . id ) { <nl> case BCMA_CORE_4706_MAC_GBIT_COMMON : <nl> case BCMA_CORE_NS_CHIPCOMMON_B : <nl> + case BCMA_CORE_PMU : <nl> + case BCMA_CORE_GCI : <nl> /* Not used yet : case BCMA_CORE_OOB_ROUTER : */ <nl> break ; <nl> default :
static void start_apic_timer ( struct kvm_lapic * apic ) <nl> { <nl> ktime_t now = apic -> lapic_timer . timer . base -> get_time (); <nl>  <nl> - apic -> lapic_timer . period = apic_get_reg ( apic , APIC_TMICT ) * <nl> + apic -> lapic_timer . period = ( u64 ) apic_get_reg ( apic , APIC_TMICT ) * <nl> APIC_BUS_CYCLE_NS * apic -> divide_count ; <nl> atomic_set (& apic -> lapic_timer . pending , 0 ); <nl> 
static int omap_gpio_irq_type ( struct irq_data * d , unsigned type ) <nl>  <nl> spin_lock_irqsave (& bank -> lock , flags ); <nl> retval = omap_set_gpio_triggering ( bank , offset , type ); <nl> - if ( retval ) <nl> + if ( retval ) { <nl> + spin_unlock_irqrestore (& bank -> lock , flags ); <nl> goto error ; <nl> + } <nl> omap_gpio_init_irq ( bank , offset ); <nl> if (! omap_gpio_is_input ( bank , offset )) { <nl> spin_unlock_irqrestore (& bank -> lock , flags );
void jfs_evict_inode ( struct inode * inode ) <nl> dquot_initialize ( inode ); <nl>  <nl> if ( JFS_IP ( inode )-> fileset == FILESYSTEM_I ) { <nl> + struct inode * ipimap = JFS_SBI ( inode -> i_sb )-> ipimap ; <nl> truncate_inode_pages_final (& inode -> i_data ); <nl>  <nl> if ( test_cflag ( COMMIT_Freewmap , inode )) <nl> jfs_free_zero_link ( inode ); <nl>  <nl> - if ( JFS_SBI ( inode -> i_sb )-> ipimap ) <nl> + if ( ipimap && JFS_IP ( ipimap )-> i_imap ) <nl> diFree ( inode ); <nl>  <nl> /*
int extract_param ( <nl> if ( len < 0 ) <nl> return - 1 ; <nl>  <nl> - if ( len > max_length ) { <nl> + if ( len >= max_length ) { <nl> pr_err (" Length of input : % d exceeds max_length :" <nl> " % d \ n ", len , max_length ); <nl> return - 1 ;
static int ll_ioctl_fiemap ( struct inode * inode , unsigned long arg ) <nl> if ( get_user ( extent_count , <nl> &(( struct ll_user_fiemap __user *) arg )-> fm_extent_count )) <nl> return - EFAULT ; <nl> + <nl> + if ( extent_count >= <nl> + ( SIZE_MAX - sizeof (* fiemap_s )) / sizeof ( struct ll_fiemap_extent )) <nl> + return - EINVAL ; <nl> num_bytes = sizeof (* fiemap_s ) + ( extent_count * <nl> sizeof ( struct ll_fiemap_extent )); <nl> 
void tsb_grow ( struct mm_struct * mm , unsigned long tsb_index , unsigned long rss ) <nl> if ( new_size > ( PAGE_SIZE * 2 )) <nl> gfp_flags = __GFP_NOWARN | __GFP_NORETRY ; <nl>  <nl> - new_tsb = kmem_cache_alloc ( tsb_caches [ new_cache_index ], gfp_flags ); <nl> + new_tsb = kmem_cache_alloc_node ( tsb_caches [ new_cache_index ], <nl> + gfp_flags , numa_node_id ()); <nl> if ( unlikely (! new_tsb )) { <nl> /* Not being able to fork due to a high - order TSB <nl> * allocation failure is very bad behavior . Just back
static int ata_eh_recover ( struct ata_port * ap , ata_prereset_fn_t prereset , <nl> down_xfermask = 0 ; <nl> rc = 0 ; <nl>  <nl> + /* if UNLOADING , finish immediately */ <nl> + if ( ap -> flags & ATA_FLAG_UNLOADING ) <nl> + goto out ; <nl> + <nl> /* skip EH if possible . */ <nl> if ( ata_eh_skip_recovery ( ap )) <nl> ehc -> i . action = 0 ;
acpi_status asmlinkage acpi_enter_sleep_state ( u8 sleep_state ) <nl> /* <nl> * 2 ) Enable all wakeup GPEs <nl> */ <nl> + status = acpi_hw_disable_all_gpes (); <nl> + if ( ACPI_FAILURE ( status )) { <nl> + return_ACPI_STATUS ( status ); <nl> + } <nl> + <nl> acpi_gbl_system_awake_and_running = FALSE ; <nl>  <nl> status = acpi_hw_enable_all_wakeup_gpes ();
struct inode * ext2_new_inode ( struct inode * dir , umode_t mode , <nl>  <nl> for ( i = 0 ; i < sbi -> s_groups_count ; i ++) { <nl> gdp = ext2_get_group_desc ( sb , group , & bh2 ); <nl> + if (! gdp ) { <nl> + if (++ group == sbi -> s_groups_count ) <nl> + group = 0 ; <nl> + continue ; <nl> + } <nl> brelse ( bitmap_bh ); <nl> bitmap_bh = read_inode_bitmap ( sb , group ); <nl> if (! bitmap_bh ) {
static ssize_t port_fops_write ( struct file * filp , const char __user * ubuf , <nl> if ( ret < 0 ) <nl> return ret ; <nl> } <nl> + /* Port got hot - unplugged . */ <nl> + if (! port -> guest_connected ) <nl> + return - ENODEV ; <nl>  <nl> count = min (( size_t )( 32 * 1024 ), count ); <nl> 
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> struct posix_acl_entry * acl_e ; <nl> struct posix_acl * acl ; <nl> struct xfs_acl_entry * ace ; <nl> - int count , i ; <nl> + unsigned int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> if ( count > XFS_ACL_MAX_ENTRIES )
static int uas_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl>  <nl> shost -> max_cmd_len = 16 + 252 ; <nl> shost -> max_id = 1 ; <nl> + shost -> max_lun = 256 ; <nl> + shost -> max_channel = 0 ; <nl> shost -> sg_tablesize = udev -> bus -> sg_tablesize ; <nl>  <nl> devinfo -> intf = intf ;
static int sdhci_acpi_probe ( struct platform_device * pdev ) <nl> host -> hw_name = " ACPI "; <nl> host -> ops = & sdhci_acpi_ops_dflt ; <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq <= 0 ) { <nl> + err = - EINVAL ; <nl> + goto err_free ; <nl> + } <nl>  <nl> host -> ioaddr = devm_ioremap_nocache ( dev , iomem -> start , <nl> resource_size ( iomem ));
static inline struct page * pte_alloc_one ( struct mm_struct * mm , <nl> struct page * pte ; <nl>  <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT , PTE_ORDER ); <nl> - if ( pte ) { <nl> - clear_highpage ( pte ); <nl> - pgtable_page_ctor ( pte ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + clear_highpage ( pte ); <nl> + if (! pgtable_page_ctor ( pte )) { <nl> + __free_page ( pte ); <nl> + return NULL ; <nl> } <nl> return pte ; <nl> }
static int _search_rsb ( struct dlm_ls * ls , char * name , int len , int b , <nl> kref_get (& r -> res_ref ); <nl> goto out ; <nl> } <nl> + if ( error == - ENOTBLK ) <nl> + goto out ; <nl> + <nl> error = dlm_search_rsb_tree (& ls -> ls_rsbtbl [ b ]. toss , name , len , flags , & r ); <nl> if ( error ) <nl> goto out ;
static int bf5xx_i2s_hw_params ( struct snd_pcm_substream * substream , <nl> bf5xx_i2s -> tcr2 |= 7 ; <nl> bf5xx_i2s -> rcr2 |= 7 ; <nl> sport_handle -> wdsize = 1 ; <nl> + break ; <nl> case SNDRV_PCM_FORMAT_S16_LE : <nl> bf5xx_i2s -> tcr2 |= 15 ; <nl> bf5xx_i2s -> rcr2 |= 15 ;
int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = dev_alloc_name ( ndev , ndev -> name ); <nl> if ( ret < 0 ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl>  <nl> int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = register_netdevice ( ndev ); <nl> if ( ret ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl> }
usba_ep_enable ( struct usb_ep * _ep , const struct usb_endpoint_descriptor * desc ) <nl>  <nl> spin_lock_irqsave (& ep -> udc -> lock , flags ); <nl>  <nl> - if ( ep -> ep . desc ) { <nl> - spin_unlock_irqrestore (& ep -> udc -> lock , flags ); <nl> - DBG ( DBG_ERR , " ep % d already enabled \ n ", ep -> index ); <nl> - return - EBUSY ; <nl> - } <nl> - <nl> ep -> ep . desc = desc ; <nl> ep -> ep . maxpacket = maxpacket ; <nl> 
static s32 ixgbe_setup_kr_x550em ( struct ixgbe_hw * hw ) <nl> if ( hw -> phy . autoneg_advertised & IXGBE_LINK_SPEED_2_5GB_FULL ) <nl> return 0 ; <nl>  <nl> + if ( ixgbe_check_reset_blocked ( hw )) <nl> + return 0 ; <nl> + <nl> return ixgbe_setup_kr_speed_x550em ( hw , hw -> phy . autoneg_advertised ); <nl> } <nl> 
static int xfrm_del_sa ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl>  <nl> static void copy_to_user_state ( struct xfrm_state * x , struct xfrm_usersa_info * p ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> id , & x -> id , sizeof ( p -> id )); <nl> memcpy (& p -> sel , & x -> sel , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & x -> lft , sizeof ( p -> lft ));
static int vga_video_font_height ; <nl> static int vga_scan_lines __read_mostly ; <nl> static unsigned int vga_rolled_over ; <nl>  <nl> - int vgacon_text_mode_force = 0 ; <nl> + static int vgacon_text_mode_force ; <nl>  <nl> bool vgacon_text_force ( void ) <nl> {
static int bcap_probe ( struct platform_device * pdev ) <nl> q -> mem_ops = & vb2_dma_contig_memops ; <nl> q -> timestamp_type = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC ; <nl>  <nl> - vb2_queue_init ( q ); <nl> + ret = vb2_queue_init ( q ); <nl> + if ( ret ) <nl> + goto err_free_handler ; <nl>  <nl> mutex_init (& bcap_dev -> mutex ); <nl> init_completion (& bcap_dev -> comp );
static int blkcg_print_stat ( struct seq_file * sf , void * v ) <nl> struct cftype blkcg_files [] = { <nl> { <nl> . name = " stat ", <nl> + . flags = CFTYPE_NOT_ON_ROOT , <nl> . seq_show = blkcg_print_stat , <nl> }, <nl> { } /* terminate */
static int i7core_register_mci ( struct i7core_dev * i7core_dev , <nl> } <nl>  <nl> fail : <nl> - edac_mc_free ( mci ); <nl> + if ( rc < 0 ) <nl> + edac_mc_free ( mci ); <nl> return rc ; <nl> } <nl> 
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
struct request_list * __blk_queue_next_rl ( struct request_list * rl , <nl> */ <nl> if ( rl == & q -> root_rl ) { <nl> ent = & q -> blkg_list ; <nl> + /* There are no more block groups , hence no request lists */ <nl> + if ( list_empty ( ent )) <nl> + return NULL ; <nl> } else { <nl> blkg = container_of ( rl , struct blkcg_gq , rl ); <nl> ent = & blkg -> q_node ;
static void __devinit tg3_get_eeprom_hw_cfg ( struct tg3 * tp ) <nl> ( cfg2 & NIC_SRAM_DATA_CFG_2_APD_EN )) <nl> tp -> tg3_flags3 |= TG3_FLG3_PHY_ENABLE_APD ; <nl>  <nl> - if ( tp -> tg3_flags2 & TG3_FLG2_PCI_EXPRESS ) { <nl> + if (( tp -> tg3_flags2 & TG3_FLG2_PCI_EXPRESS ) && <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) != ASIC_REV_5785 && <nl> + !( tp -> tg3_flags3 & TG3_FLG3_5717_PLUS )) { <nl> u32 cfg3 ; <nl>  <nl> tg3_read_mem ( tp , NIC_SRAM_DATA_CFG_3 , & cfg3 );
static int doDevConfig ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> int sdev = - 1 , nchans , tmp ; <nl> struct BondedDevice * bdev = NULL ; <nl>  <nl> - if ( minor < 0 || minor > COMEDI_NUM_BOARD_MINORS ) { <nl> + if ( minor < 0 || minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> ERROR (" Minor % d is invalid !\ n ", minor ); <nl> return 0 ; <nl> }
static inline void x86_assign_hw_event ( struct perf_event * event , <nl> hwc -> event_base = 0 ; <nl> } else if ( hwc -> idx >= X86_PMC_IDX_FIXED ) { <nl> hwc -> config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL ; <nl> - hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 ; <nl> + hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + ( hwc -> idx - X86_PMC_IDX_FIXED ); <nl> } else { <nl> hwc -> config_base = x86_pmu_config_addr ( hwc -> idx ); <nl> hwc -> event_base = x86_pmu_event_addr ( hwc -> idx );
static int tun_set_iff ( struct net * net , struct file * file , struct ifreq * ifr ) <nl> if (! dev ) <nl> return - ENOMEM ; <nl> err = dev_get_valid_name ( net , dev , name ); <nl> - if ( err ) <nl> + if ( err < 0 ) <nl> goto err_free_dev ; <nl>  <nl> dev_net_set ( dev , net );
static int load_module ( struct load_info * info , const char __user * uargs , <nl> mod_sysfs_teardown ( mod ); <nl> coming_cleanup : <nl> mod -> state = MODULE_STATE_GOING ; <nl> + destroy_params ( mod -> kp , mod -> num_kp ); <nl> blocking_notifier_call_chain (& module_notify_list , <nl> MODULE_STATE_GOING , mod ); <nl> klp_module_going ( mod );
static int gdma_dma_remove ( struct platform_device * pdev ) <nl> struct gdma_dma_dev * dma_dev = platform_get_drvdata ( pdev ); <nl>  <nl> tasklet_kill (& dma_dev -> task ); <nl> - of_dma_controller_free ( pdev -> dev . of_node ); <nl> + of_dma_controller_free ( pdev -> dev . of_node ); <nl> dma_async_device_unregister (& dma_dev -> ddev ); <nl>  <nl> return 0 ;
static int gic_set_affinity ( struct irq_data * d , const struct cpumask * mask_val , <nl> int enabled ; <nl> u64 val ; <nl>  <nl> + if ( cpu >= nr_cpu_ids ) <nl> + return - EINVAL ; <nl> + <nl> if ( gic_irq_in_rdist ( d )) <nl> return - EINVAL ; <nl> 
BPF_PROG_TYPE_FNS ( tracepoint , BPF_PROG_TYPE_TRACEPOINT ); <nl> BPF_PROG_TYPE_FNS ( xdp , BPF_PROG_TYPE_XDP ); <nl> BPF_PROG_TYPE_FNS ( perf_event , BPF_PROG_TYPE_PERF_EVENT ); <nl>  <nl> -# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ), type } <nl> +# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ) - 1 , type } <nl> static const struct { <nl> const char * sec ; <nl> size_t len ;
int policydb_read ( struct policydb * p , void * fp ) <nl> } else <nl> tr -> tclass = p -> process_class ; <nl>  <nl> + rc = - EINVAL ; <nl> if (! policydb_role_isvalid ( p , tr -> role ) || <nl> ! policydb_type_isvalid ( p , tr -> type ) || <nl> ! policydb_class_isvalid ( p , tr -> tclass ) ||
static void cgroup_enable_task_cg_lists ( void ) <nl> } <nl>  <nl> void cgroup_iter_start ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __acquires ( css_set_lock ) <nl> { <nl> /* <nl> * The first time anyone tries to iterate across a cgroup , <nl> struct task_struct * cgroup_iter_next ( struct cgroup * cgrp , <nl> } <nl>  <nl> void cgroup_iter_end ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __releases ( css_set_lock ) <nl> { <nl> read_unlock (& css_set_lock ); <nl> }
static int __init mvebu_soc_id_init ( void ) <nl>  <nl> res_ioremap : <nl> clk_disable_unprepare ( clk ); <nl> + clk_put ( clk ); <nl>  <nl> clk_err : <nl> of_node_put ( child );
int uprobe_write_opcode ( struct mm_struct * mm , unsigned long vaddr , <nl>  <nl> retry : <nl> /* Read the page with vaddr into memory */ <nl> - ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , FOLL_FORCE , & old_page , <nl> - & vma , NULL ); <nl> + ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , <nl> + FOLL_FORCE | FOLL_SPLIT , & old_page , & vma , NULL ); <nl> if ( ret <= 0 ) <nl> return ret ; <nl> 
void __iomem * __ioremap ( unsigned long addr , unsigned long size , <nl> pa = addr & PAGE_MASK ; <nl> size = PAGE_ALIGN ( addr + size ) - pa ; <nl>  <nl> - if ( size == 0 ) <nl> + if (( size == 0 ) || ( pa == 0 )) <nl> return NULL ; <nl>  <nl> if ( mem_init_done ) {
i915_gem_pwrite_ioctl ( struct drm_device * dev , void * data , <nl>  <nl> if ( obj -> gtt_space && <nl> obj -> cache_level == I915_CACHE_NONE && <nl> + obj -> tiling_mode == I915_TILING_NONE && <nl> obj -> map_and_fenceable && <nl> obj -> base . write_domain != I915_GEM_DOMAIN_CPU ) { <nl> ret = i915_gem_gtt_pwrite_fast ( dev , obj , args , file );
static int wm5102_sysclk_ev ( struct snd_soc_dapm_widget * w , <nl> struct snd_kcontrol * kcontrol , int event ) <nl> { <nl> struct snd_soc_codec * codec = w -> codec ; <nl> - struct arizona * arizona = dev_get_drvdata ( codec -> dev ); <nl> + struct arizona * arizona = dev_get_drvdata ( codec -> dev -> parent ); <nl> struct regmap * regmap = codec -> control_data ; <nl> const struct reg_default * patch = NULL ; <nl> int i , patch_size ;
xfrm_init_tempstate ( struct xfrm_state * x , const struct flowi * fl , <nl> { <nl> struct xfrm_state_afinfo * afinfo = xfrm_state_afinfo_get_rcu ( family ); <nl>  <nl> - if ( afinfo ) <nl> - afinfo -> init_tempsel (& x -> sel , fl ); <nl> + if (! afinfo ) <nl> + return ; <nl> + <nl> + afinfo -> init_tempsel (& x -> sel , fl ); <nl>  <nl> if ( family != tmpl -> encap_family ) { <nl> afinfo = xfrm_state_afinfo_get_rcu ( tmpl -> encap_family );
static bool ironlake_get_pipe_config ( struct intel_crtc * crtc , <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> uint32_t tmp ; <nl>  <nl> + if (! intel_display_power_enabled ( dev_priv , <nl> + POWER_DOMAIN_PIPE ( crtc -> pipe ))) <nl> + return false ; <nl> + <nl> pipe_config -> cpu_transcoder = ( enum transcoder ) crtc -> pipe ; <nl> pipe_config -> shared_dpll = DPLL_ID_PRIVATE ; <nl> 
static netdev_tx_t w83977af_hard_xmit ( struct sk_buff * skb , <nl>  <nl> mtt = irda_get_mtt ( skb ); <nl> pr_debug ("% s (% ld ), mtt =% d \ n ", __func__ , jiffies , mtt ); <nl> - if ( mtt ) <nl> + if ( mtt > 1000 ) <nl> + mdelay ( mtt / 1000 ); <nl> + else if ( mtt ) <nl> udelay ( mtt ); <nl>  <nl> /* Enable DMA interrupt */
void cs46xx_dsp_destroy_pcm_channel ( struct snd_cs46xx * chip , <nl> if (! pcm_channel -> src_scb -> ref_count ) { <nl> cs46xx_dsp_remove_scb ( chip , pcm_channel -> src_scb ); <nl>  <nl> - snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot <= DSP_MAX_SRC_NR , <nl> + snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot < DSP_MAX_SRC_NR , <nl> return ); <nl>  <nl> ins -> src_scb_slots [ pcm_channel -> src_slot ] = 0 ;
int adf_create_ring ( struct adf_accel_dev * accel_dev , const char * section , <nl> dev_err (& GET_DEV ( accel_dev ), " Can ' t get ring number \ n "); <nl> return - EFAULT ; <nl> } <nl> + if ( ring_num >= ADF_ETR_MAX_RINGS_PER_BANK ) { <nl> + dev_err (& GET_DEV ( accel_dev ), " Invalid ring number \ n "); <nl> + return - EFAULT ; <nl> + } <nl>  <nl> bank = & transport_data -> banks [ bank_num ]; <nl> if ( adf_reserve_ring ( bank , ring_num )) {
int cpqhp_configure_device ( struct controller * ctrl , struct pci_func * func ) <nl> } <nl>  <nl> if ( func -> pci_dev -> hdr_type == PCI_HEADER_TYPE_BRIDGE ) { <nl> + int max ; <nl> pci_read_config_byte ( func -> pci_dev , PCI_SECONDARY_BUS , & bus ); <nl> child = ( struct pci_bus *) pci_add_new_bus ( func -> pci_dev -> bus , ( func -> pci_dev ), bus ); <nl> - pci_do_scan_bus ( child ); <nl> + max = pci_do_scan_bus ( child ); <nl> + pci_bus_update_busn_res_end ( child , max ); <nl> } <nl>  <nl> pci_dev_put ( func -> pci_dev );
static void pl011_dma_probe ( struct uart_amba_port * uap ) <nl> /* Optionally make use of an RX channel as well */ <nl> chan = dma_request_slave_channel ( dev , " rx "); <nl>  <nl> - if (! chan && plat -> dma_rx_param ) { <nl> + if (! chan && plat && plat -> dma_rx_param ) { <nl> chan = dma_request_channel ( mask , plat -> dma_filter , plat -> dma_rx_param ); <nl>  <nl> if (! chan ) {
static struct platform_driver rk_iommu_driver = { <nl>  <nl> static int __init rk_iommu_init ( void ) <nl> { <nl> + struct device_node * np ; <nl> int ret ; <nl>  <nl> + np = of_find_matching_node ( NULL , rk_iommu_dt_ids ); <nl> + if (! np ) <nl> + return 0 ; <nl> + <nl> + of_node_put ( np ); <nl> + <nl> ret = bus_set_iommu (& platform_bus_type , & rk_iommu_ops ); <nl> if ( ret ) <nl> return ret ;
static void cpufreq_stats_free_table ( unsigned int cpu ) <nl> static void cpufreq_stats_free_sysfs ( unsigned int cpu ) <nl> { <nl> struct cpufreq_policy * policy = cpufreq_cpu_get ( cpu ); <nl> + <nl> + if (! cpufreq_frequency_get_table ( cpu )) <nl> + return ; <nl> + <nl> if ( policy && ! policy_is_shared ( policy )) { <nl> pr_debug ("% s : Free sysfs stat \ n ", __func__ ); <nl> sysfs_remove_group (& policy -> kobj , & stats_attr_group );
int i40e_ndo_get_vf_config ( struct net_device * netdev , <nl> else <nl> ivi -> linkstate = IFLA_VF_LINK_STATE_DISABLE ; <nl> ivi -> spoofchk = vf -> spoofchk ; <nl> + ivi -> trusted = vf -> trusted ; <nl> ret = 0 ; <nl>  <nl> error_param :
static noinline int btrfs_search_path_in_tree ( struct btrfs_fs_info * info , <nl> key . objectid = key . offset ; <nl> key . offset = ( u64 )- 1 ; <nl> dirid = key . objectid ; <nl> - <nl> } <nl> if ( ptr < name ) <nl> goto out ; <nl> - memcpy ( name , ptr , total_len ); <nl> + memmove ( name , ptr , total_len ); <nl> name [ total_len ]='\ 0 '; <nl> ret = 0 ; <nl> out :
SMB2_tcon ( const unsigned int xid , struct cifs_ses * ses , const char * tree , <nl> tcon_error_exit : <nl> if ( rsp -> hdr . Status == STATUS_BAD_NETWORK_NAME ) { <nl> cifs_dbg ( VFS , " BAD_NETWORK_NAME : % s \ n ", tree ); <nl> - tcon -> bad_network_name = true ; <nl> + if ( tcon ) <nl> + tcon -> bad_network_name = true ; <nl> } <nl> goto tcon_exit ; <nl> }
static int xadc_parse_dt ( struct iio_dev * indio_dev , struct device_node * np , <nl> chan -> address = XADC_REG_VPVN ; <nl> } else { <nl> chan -> scan_index = 15 + reg ; <nl> - chan -> scan_index = XADC_REG_VAUX ( reg - 1 ); <nl> + chan -> address = XADC_REG_VAUX ( reg - 1 ); <nl> } <nl> num_channels ++; <nl> chan ++;
void __init reserve_early_overlap_ok ( u64 start , u64 end , char * name ) <nl> */ <nl> void __init reserve_early ( u64 start , u64 end , char * name ) <nl> { <nl> + if ( start >= end ) <nl> + return ; <nl> + <nl> drop_overlaps_that_are_ok ( start , end ); <nl> __reserve_early ( start , end , name , 0 ); <nl> }
static int __init dw_probe ( struct platform_device * pdev ) <nl> dma_writel ( dw , CFG , DW_CFG_DMA_EN ); <nl>  <nl> printk ( KERN_INFO "% s : DesignWare DMA Controller , % d channels \ n ", <nl> - pdev -> dev . bus_id , dw -> dma . chancnt ); <nl> + dev_name (& pdev -> dev ), dw -> dma . chancnt ); <nl>  <nl> dma_async_device_register (& dw -> dma ); <nl> 
struct dst_entry * fib6_rule_lookup ( struct net * net , struct flowi6 * fl6 , <nl> if ( rt -> dst . error == - EAGAIN ) { <nl> ip6_rt_put_flags ( rt , flags ); <nl> rt = net -> ipv6 . ip6_null_entry ; <nl> - if (!( flags | RT6_LOOKUP_F_DST_NOREF )) <nl> + if (!( flags & RT6_LOOKUP_F_DST_NOREF )) <nl> dst_hold (& rt -> dst ); <nl> } <nl> 
static int key_notify_policy_flush ( const struct km_event * c ) <nl> hdr -> sadb_msg_pid = c -> portid ; <nl> hdr -> sadb_msg_version = PF_KEY_V2 ; <nl> hdr -> sadb_msg_errno = ( uint8_t ) 0 ; <nl> + hdr -> sadb_msg_satype = SADB_SATYPE_UNSPEC ; <nl> hdr -> sadb_msg_len = ( sizeof ( struct sadb_msg ) / sizeof ( uint64_t )); <nl> pfkey_broadcast ( skb_out , GFP_ATOMIC , BROADCAST_ALL , NULL , c -> net ); <nl> return 0 ;
static void mcam_ctlr_image ( struct mcam_camera * cam ) <nl> mcam_reg_write_mask ( cam , REG_CTRL0 , <nl> C0_DF_RGB | C0_RGBF_565 | C0_RGB5_BGGR , C0_DF_MASK ); <nl> break ; <nl> + case V4L2_PIX_FMT_SBGGR8 : <nl> + mcam_reg_write_mask ( cam , REG_CTRL0 , <nl> + C0_DF_RGB | C0_RGB5_GRBG , C0_DF_MASK ); <nl> + break ; <nl> default : <nl> cam_err ( cam , " camera : unknown format : %# x \ n ", fmt -> pixelformat ); <nl> break ;
void do_exit ( long code ) <nl>  <nl> module_put ( task_thread_info ( tsk )-> exec_domain -> module ); <nl>  <nl> - proc_exit_connector ( tsk ); <nl> /* <nl> * FIXME : do that only when needed , using sched_exit tracepoint <nl> */ <nl> flush_ptrace_hw_breakpoint ( tsk ); <nl>  <nl> exit_notify ( tsk , group_dead ); <nl> + proc_exit_connector ( tsk ); <nl> # ifdef CONFIG_NUMA <nl> task_lock ( tsk ); <nl> mpol_put ( tsk -> mempolicy );
static int sched_read_attr ( struct sched_attr __user * uattr , <nl> attr -> size = usize ; <nl> } <nl>  <nl> - ret = copy_to_user ( uattr , attr , usize ); <nl> + ret = copy_to_user ( uattr , attr , attr -> size ); <nl> if ( ret ) <nl> return - EFAULT ; <nl> 
struct lcd_device * lcd_device_register ( const char * name , struct device * parent , <nl>  <nl> rc = device_register (& new_ld -> dev ); <nl> if ( rc ) { <nl> - kfree ( new_ld ); <nl> + put_device (& new_ld -> dev ); <nl> return ERR_PTR ( rc ); <nl> } <nl> 
static void disk_seqf_stop ( struct seq_file * seqf , void * v ) <nl> if ( iter ) { <nl> class_dev_iter_exit ( iter ); <nl> kfree ( iter ); <nl> + seqf -> private = NULL ; <nl> } <nl> } <nl> 
u32 __tcp_select_window ( struct sock * sk ) <nl> */ <nl> if ( window <= free_space - mss || window > free_space ) <nl> window = ( free_space / mss )* mss ; <nl> + else if ( mss == full_space && <nl> + free_space > window + full_space / 2 ) <nl> + window = free_space ; <nl> } <nl>  <nl> return window ;
struct g2d_runqueue_node { <nl> struct list_head list ; <nl> struct list_head run_cmdlist ; <nl> struct list_head event_list ; <nl> + pid_t pid ; <nl> struct completion complete ; <nl> int async ; <nl> }; <nl> int exynos_g2d_exec_ioctl ( struct drm_device * drm_dev , void * data , <nl> } <nl>  <nl> mutex_lock (& g2d -> runqueue_mutex ); <nl> + runqueue_node -> pid = current -> pid ; <nl> list_add_tail (& runqueue_node -> list , & g2d -> runqueue ); <nl> if (! g2d -> runqueue_node ) <nl> g2d_exec_runqueue ( g2d );
void intel_irq_init ( struct drm_device * dev ) <nl> dev -> driver -> get_vblank_counter = gm45_get_vblank_counter ; <nl> } <nl>  <nl> - <nl> - dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + if ( drm_core_check_feature ( dev , DRIVER_MODESET )) <nl> + dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + else <nl> + dev -> driver -> get_vblank_timestamp = NULL ; <nl> dev -> driver -> get_scanout_position = i915_get_crtc_scanoutpos ; <nl>  <nl> if ( IS_IVYBRIDGE ( dev )) {
static int configure_tda827x_fe ( struct saa7134_dev * dev , <nl> /* Get the first frontend */ <nl> fe0 = videobuf_dvb_get_frontend (& dev -> frontends , 1 ); <nl>  <nl> + if (! fe0 ) <nl> + return - EINVAL ; <nl> + <nl> fe0 -> dvb . frontend = dvb_attach ( tda10046_attach , cdec_conf , & dev -> i2c_adap ); <nl> if ( fe0 -> dvb . frontend ) { <nl> if ( cdec_conf -> i2c_gate )
static struct perf_pmu * pmu_lookup ( const char * name ) <nl> LIST_HEAD ( aliases ); <nl> __u32 type ; <nl>  <nl> + /* No support for intel_bts or intel_pt so disallow them */ <nl> + if (! strcmp ( name , " intel_bts ") || ! strcmp ( name , " intel_pt ")) <nl> + return NULL ; <nl> + <nl> /* <nl> * The pmu data we store & need consists of the pmu <nl> * type value and format definitions . Load both right
int hists__browse ( struct hists * self , const char * helpline , const char * ev_name ) <nl>  <nl> switch ( key ) { <nl> case ' a ': <nl> - if ( browser -> selection -> map == NULL && <nl> + if ( browser -> selection -> map == NULL || <nl> browser -> selection -> map -> dso -> annotate_warned ) <nl> continue ; <nl> goto do_annotate ;
static int load_firmware ( struct octeon_device * oct ) <nl> char fw_name [ LIO_MAX_FW_FILENAME_LEN ]; <nl> char * tmp_fw_type ; <nl>  <nl> - if ( fw_type_is_auto ()) <nl> + if ( fw_type_is_auto ()) { <nl> tmp_fw_type = LIO_FW_NAME_TYPE_NIC ; <nl> - else <nl> + strncpy ( fw_type , tmp_fw_type , sizeof ( fw_type )); <nl> + } else { <nl> tmp_fw_type = fw_type ; <nl> + } <nl>  <nl> sprintf ( fw_name , "% s % s % s_ % s % s ", LIO_FW_DIR , LIO_FW_BASE_NAME , <nl> octeon_get_conf ( oct )-> card_name , tmp_fw_type ,
static int btree_gc_coalesce ( struct btree * b , struct btree_op * op , <nl> BUG_ON ( btree_bset_first ( new_nodes [ 0 ])-> keys ); <nl> btree_node_free ( new_nodes [ 0 ]); <nl> rw_unlock ( true , new_nodes [ 0 ]); <nl> + new_nodes [ 0 ] = NULL ; <nl>  <nl> for ( i = 0 ; i < nodes ; i ++) { <nl> if ( __bch_keylist_realloc (& keylist , bkey_u64s (& r [ i ]. b -> key )))
void __detach_mounts ( struct dentry * dentry ) <nl>  <nl> namespace_lock (); <nl> mp = lookup_mountpoint ( dentry ); <nl> - if (! mp ) <nl> + if ( IS_ERR_OR_NULL ( mp )) <nl> goto out_unlock ; <nl>  <nl> lock_mount_hash ();
static struct ip_conntrack_expect * find_expect ( struct ip_conntrack * ct , <nl> tuple . dst . protonum = IPPROTO_TCP ; <nl>  <nl> exp = __ip_conntrack_expect_find (& tuple ); <nl> - if ( exp -> master == ct ) <nl> + if ( exp && exp -> master == ct ) <nl> return exp ; <nl> return NULL ; <nl> }
static cycle_t e1000e_cyclecounter_read ( const struct cyclecounter * cc ) <nl> systimeh = er32 ( SYSTIMH ); <nl> systimel_2 = er32 ( SYSTIML ); <nl> /* Check for overflow . If there was no overflow , use the values */ <nl> - if ( systimel_1 < systimel_2 ) { <nl> + if ( systimel_1 <= systimel_2 ) { <nl> systim = ( cycle_t ) systimel_1 ; <nl> systim |= ( cycle_t ) systimeh << 32 ; <nl> } else {
int __init dmar_parse_dev_scope ( void * start , void * end , int * cnt , <nl> if ( scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT || <nl> scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE ) <nl> (* cnt )++; <nl> - else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC ) { <nl> + else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC && <nl> + scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_HPET ) { <nl> pr_warn (" Unsupported device scope \ n "); <nl> } <nl> start += scope -> length ;
int vmw_get_cap_3d_ioctl ( struct drm_device * dev , void * data , <nl> memcpy_fromio ( bounce , & fifo_mem [ SVGA_FIFO_3D_CAPS ], size ); <nl>  <nl> ret = copy_to_user ( buffer , bounce , size ); <nl> + if ( ret ) <nl> + ret = - EFAULT ; <nl> vfree ( bounce ); <nl>  <nl> if ( unlikely ( ret != 0 ))
static int netvsc_init_buf ( struct hv_device * device ) <nl> net_device -> map_words = DIV_ROUND_UP ( net_device -> send_section_cnt , <nl> BITS_PER_LONG ); <nl>  <nl> - net_device -> send_section_map = <nl> - kzalloc ( net_device -> map_words * sizeof ( ulong ), GFP_KERNEL ); <nl> + net_device -> send_section_map = kcalloc ( net_device -> map_words , <nl> + sizeof ( ulong ), GFP_KERNEL ); <nl> if ( net_device -> send_section_map == NULL ) { <nl> ret = - ENOMEM ; <nl> goto cleanup ;
static int parse_audio_selector_unit ( struct mixer_build * state , int unitid , unsi <nl> kctl = snd_ctl_new1 (& mixer_selectunit_ctl , cval ); <nl> if (! kctl ) { <nl> snd_printk ( KERN_ERR " cannot malloc kcontrol \ n "); <nl> + kfree ( namelist ); <nl> kfree ( cval ); <nl> return - ENOMEM ; <nl> }
static int handle_emulation_failure ( struct kvm_vcpu * vcpu ) <nl> vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> vcpu -> run -> internal . ndata = 0 ; <nl> - r = EMULATE_FAIL ; <nl> + r = EMULATE_USER_EXIT ; <nl> } <nl> kvm_queue_exception ( vcpu , UD_VECTOR ); <nl> 
static int lz4_uncompress ( const char * source , char * dest , int osize ) <nl> len = * ip ++; <nl> for (; len == 255 ; length += 255 ) <nl> len = * ip ++; <nl> + if ( unlikely ( length > ( size_t )( length + len ))) <nl> + goto _output_error ; <nl> length += len ; <nl> } <nl> 
ixgb_restore_vlan ( struct ixgb_adapter * adapter ) <nl>  <nl> static void ixgb_netpoll ( struct net_device * dev ) <nl> { <nl> - struct ixgb_adapter * adapter = dev -> priv ; <nl> + struct ixgb_adapter * adapter = netdev_priv ( dev ); <nl>  <nl> disable_irq ( adapter -> pdev -> irq ); <nl> ixgb_intr ( adapter -> pdev -> irq , dev , NULL );
static int wm8903_probe ( struct snd_soc_codec * codec ) <nl> /* power down chip */ <nl> static int wm8903_remove ( struct snd_soc_codec * codec ) <nl> { <nl> + struct wm8903_priv * wm8903 = snd_soc_codec_get_drvdata ( codec ); <nl> + <nl> wm8903_free_gpio ( codec ); <nl> wm8903_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> + if ( wm8903 -> irq ) <nl> + free_irq ( wm8903 -> irq , codec ); <nl> + <nl> return 0 ; <nl> } <nl> 
static void i40evf_adminq_task ( struct work_struct * work ) <nl>  <nl> /* check for error indications */ <nl> val = rd32 ( hw , hw -> aq . arq . len ); <nl> + if ( val == 0xdeadbeef ) /* indicates device in reset */ <nl> + goto freedom ; <nl> oldval = val ; <nl> if ( val & I40E_VF_ARQLEN1_ARQVFE_MASK ) { <nl> dev_info (& adapter -> pdev -> dev , " ARQ VF Error detected \ n ");
static int ccid3_hc_tx_getsockopt ( struct sock * sk , const int optname , int len , <nl> case DCCP_SOCKOPT_CCID_TX_INFO : <nl> if ( len < sizeof ( tfrc )) <nl> return - EINVAL ; <nl> + memset (& tfrc , 0 , sizeof ( tfrc )); <nl> tfrc . tfrctx_x = hc -> tx_x ; <nl> tfrc . tfrctx_x_recv = hc -> tx_x_recv ; <nl> tfrc . tfrctx_x_calc = hc -> tx_x_calc ;
static int fsl_lpspi_probe ( struct platform_device * pdev ) <nl> ret = pm_runtime_get_sync ( fsl_lpspi -> dev ); <nl> if ( ret < 0 ) { <nl> dev_err ( fsl_lpspi -> dev , " failed to enable clock \ n "); <nl> - return ret ; <nl> + goto out_controller_put ; <nl> } <nl>  <nl> temp = readl ( fsl_lpspi -> base + IMX7ULP_PARAM );
int afu_register_irqs ( struct cxl_context * ctx , u32 count ) <nl> */ <nl> INIT_LIST_HEAD (& ctx -> irq_names ); <nl> for ( r = 1 ; r < CXL_IRQ_RANGES ; r ++) { <nl> - for ( i = 0 ; i < ctx -> irqs . range [ r ]; hwirq ++, i ++) { <nl> + for ( i = 0 ; i < ctx -> irqs . range [ r ]; i ++) { <nl> irq_name = kmalloc ( sizeof ( struct cxl_irq_name ), <nl> GFP_KERNEL ); <nl> if (! irq_name )
static void __init cpg_mssr_register_mod_clk ( const struct mssr_mod_clk * mod , <nl> # else <nl> dev_dbg ( dev , " Ignoring MSTP % s to prevent disabling \ n ", <nl> mod -> name ); <nl> + kfree ( clock ); <nl> return ; <nl> # endif <nl> }
static ssize_t ci_port_test_write ( struct file * file , const char __user * ubuf , <nl> if ( sscanf ( buf , "% u ", & mode ) != 1 ) <nl> return - EINVAL ; <nl>  <nl> + if ( mode > 255 ) <nl> + return - EBADRQC ; <nl> + <nl> pm_runtime_get_sync ( ci -> dev ); <nl> spin_lock_irqsave (& ci -> lock , flags ); <nl> ret = hw_port_test_set ( ci , mode );
static void __init reset_all_timers ( void ) <nl> * In other cases ( such as with VSAless OpenFirmware ), the system firmware <nl> * leaves timers available for us to use . <nl> */ <nl> - static int __init scan_timers ( struct cs5535_mfgpt_chip * mfgpt ) <nl> + static int __devinit scan_timers ( struct cs5535_mfgpt_chip * mfgpt ) <nl> { <nl> struct cs5535_mfgpt_timer timer = { . chip = mfgpt }; <nl> unsigned long flags ;
static int btrfs_extent_same ( struct inode * src , u64 loff , u64 len , <nl> if ( src == dst ) <nl> return - EINVAL ; <nl>  <nl> + if ( len == 0 ) <nl> + return 0 ; <nl> + <nl> btrfs_double_lock ( src , loff , dst , dst_loff , len ); <nl>  <nl> ret = extent_same_check_offsets ( src , loff , len );
int i40e_ndo_set_vf_port_vlan ( struct net_device * netdev , int vf_id , <nl> VLAN_VID_MASK )); <nl> } <nl>  <nl> + spin_unlock_bh (& vsi -> mac_filter_hash_lock ); <nl> if ( vlan_id || qos ) <nl> ret = i40e_vsi_add_pvid ( vsi , vlanprio ); <nl> else <nl> i40e_vsi_remove_pvid ( vsi ); <nl> + spin_lock_bh (& vsi -> mac_filter_hash_lock ); <nl>  <nl> if ( vlan_id ) { <nl> dev_info (& pf -> pdev -> dev , " Setting VLAN % d , QOS 0x % x on VF % d \ n ",
static irqreturn_t ab8500_irq ( int irq , void * dev ) <nl> do { <nl> int bit = __ffs ( value ); <nl> int line = i * 8 + bit ; <nl> + int virq = ab8500_irq_get_virq ( ab8500 , line ); <nl>  <nl> - handle_nested_irq ( ab8500 -> irq_base + line ); <nl> + handle_nested_irq ( virq ); <nl> value &= ~( 1 << bit ); <nl>  <nl> } while ( value );
static inline int stack_map_data_size ( struct bpf_map * map ) <nl>  <nl> static int prealloc_elems_and_freelist ( struct bpf_stack_map * smap ) <nl> { <nl> - u32 elem_size = sizeof ( struct stack_map_bucket ) + smap -> map . value_size ; <nl> + u64 elem_size = sizeof ( struct stack_map_bucket ) + <nl> + ( u64 ) smap -> map . value_size ; <nl> int err ; <nl>  <nl> smap -> elems = bpf_map_area_alloc ( elem_size * smap -> map . max_entries ,
static int f2fs_move_file_range ( struct file * file_in , loff_t pos_in , <nl> if ( f2fs_encrypted_inode ( src ) || f2fs_encrypted_inode ( dst )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( src == dst ) { <nl> + if ( pos_in == pos_out ) <nl> + return 0 ; <nl> + if ( pos_out > pos_in && pos_out < pos_in + len ) <nl> + return - EINVAL ; <nl> + } <nl> + <nl> inode_lock ( src ); <nl> if ( src != dst ) { <nl> if (! inode_trylock ( dst )) {
static inline void tpg_s_bytesperline ( struct tpg_data * tpg , unsigned plane , unsi <nl>  <nl> tpg -> bytesperline [ p ] = plane_w / tpg -> hdownsampling [ p ]; <nl> } <nl> + if ( tpg_g_interleaved ( tpg )) <nl> + tpg -> bytesperline [ 1 ] = tpg -> bytesperline [ 0 ]; <nl> } <nl>  <nl> 
enum nvkm_devidx { <nl> NVKM_SUBDEV_MC , <nl> NVKM_SUBDEV_BUS , <nl> NVKM_SUBDEV_TIMER , <nl> + NVKM_SUBDEV_INSTMEM , <nl> NVKM_SUBDEV_FB , <nl> NVKM_SUBDEV_LTC , <nl> - NVKM_SUBDEV_INSTMEM , <nl> NVKM_SUBDEV_MMU , <nl> NVKM_SUBDEV_BAR , <nl> NVKM_SUBDEV_PMU ,
static int cpufreq_add_dev ( struct sys_device * sys_dev ) <nl>  <nl> spin_lock_irqsave (& cpufreq_driver_lock , flags ); <nl> for_each_cpu ( j , policy -> cpus ) { <nl> + if (! cpu_online ( j )) <nl> + continue ; <nl> per_cpu ( cpufreq_cpu_data , j ) = policy ; <nl> per_cpu ( policy_cpu , j ) = policy -> cpu ; <nl> }
static int find_parent_nodes ( struct btrfs_trans_handle * trans , <nl> } <nl> ret = find_extent_in_eb ( eb , bytenr , <nl> * extent_item_pos , & eie ); <nl> - ref -> inode_list = eie ; <nl> free_extent_buffer ( eb ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + ref -> inode_list = eie ; <nl> } <nl> ret = ulist_add_merge ( refs , ref -> parent , <nl> ( uintptr_t ) ref -> inode_list ,
static acpi_status WMID_set_capabilities ( void ) <nl> devices = *(( u32 *) obj -> buffer . pointer ); <nl> } else if ( obj -> type == ACPI_TYPE_INTEGER ) { <nl> devices = ( u32 ) obj -> integer . value ; <nl> + } else { <nl> + kfree ( out . pointer ); <nl> + return AE_ERROR ; <nl> } <nl> } else { <nl> kfree ( out . pointer );
static inline int __mkroute_input ( struct sk_buff * skb , <nl> # endif <nl> if ( in_dev -> cnf . no_policy ) <nl> rth -> u . dst . flags |= DST_NOPOLICY ; <nl> - if ( in_dev -> cnf . no_xfrm ) <nl> + if ( out_dev -> cnf . no_xfrm ) <nl> rth -> u . dst . flags |= DST_NOXFRM ; <nl> rth -> fl . fl4_dst = daddr ; <nl> rth -> rt_dst = daddr ;
# endif <nl> # include " igb . h " <nl>  <nl> -# define DRV_VERSION " 2 . 4 . 13 - k2 " <nl> +# define MAJ 3 <nl> +# define MIN 0 <nl> +# define BUILD 6 <nl> +# define KFIX 2 <nl> +# define DRV_VERSION __stringify ( MAJ ) "." __stringify ( MIN ) "." \ <nl> + __stringify ( BUILD ) "- k " __stringify ( KFIX ) <nl> char igb_driver_name [] = " igb "; <nl> char igb_driver_version [] = DRV_VERSION ; <nl> static const char igb_driver_string [] =
static bool radeon_atom_apply_quirks ( struct drm_device * dev , <nl> if (( supported_device == ATOM_DEVICE_CRT1_SUPPORT ) || <nl> ( supported_device == ATOM_DEVICE_DFP2_SUPPORT )) <nl> return false ; <nl> + if ( supported_device == ATOM_DEVICE_CRT2_SUPPORT ) <nl> + * line_mux = 0x90 ; <nl> } <nl>  <nl> /* ASUS HD 3600 XT board lists the DVI port as HDMI */
static int futex_unlock_pi ( u32 __user * uaddr , struct rw_semaphore * fshared ) <nl> attempt ); <nl> if ( ret ) <nl> goto out ; <nl> + uval = 0 ; <nl> goto retry_unlocked ; <nl> } <nl> 
static void local_exit ( void ) <nl> DMINFO (" cleaned up "); <nl> } <nl>  <nl> - int (* _inits [])( void ) __initdata = { <nl> + static int (* _inits [])( void ) __initdata = { <nl> local_init , <nl> dm_target_init , <nl> dm_linear_init , <nl> int (* _inits [])( void ) __initdata = { <nl> dm_interface_init , <nl> }; <nl>  <nl> - void (* _exits [])( void ) = { <nl> + static void (* _exits [])( void ) = { <nl> local_exit , <nl> dm_target_exit , <nl> dm_linear_exit ,
static int __ocfs2_move_extent ( handle_t * handle , <nl> } <nl>  <nl> out : <nl> + ocfs2_free_path ( path ); <nl> return ret ; <nl> } <nl> 
static void change_port_settings ( struct tty_struct * tty , <nl> if (! baud ) { <nl> /* pick a default , any default ... */ <nl> baud = 9600 ; <nl> - } else <nl> + } else { <nl> + /* Avoid a zero divisor . */ <nl> + baud = min ( baud , 461550 ); <nl> tty_encode_baud_rate ( tty , baud , baud ); <nl> + } <nl>  <nl> edge_port -> baud_rate = baud ; <nl> config -> wBaudRate = ( __u16 )(( 461550L + baud / 2 ) / baud );
int skl_init_module ( struct skl_sst * ctx , <nl> return ret ; <nl> } <nl> mconfig -> m_state = SKL_MODULE_INIT_DONE ; <nl> - <nl> + kfree ( param_data ); <nl> return ret ; <nl> } <nl> 
static struct phy * exynos_mipi_video_phy_xlate ( struct device * dev , <nl> { <nl> struct exynos_mipi_video_phy * state = dev_get_drvdata ( dev ); <nl>  <nl> - if ( WARN_ON ( args -> args [ 0 ] > EXYNOS_MIPI_PHYS_NUM )) <nl> + if ( WARN_ON ( args -> args [ 0 ] >= EXYNOS_MIPI_PHYS_NUM )) <nl> return ERR_PTR (- ENODEV ); <nl>  <nl> return state -> phys [ args -> args [ 0 ]]. phy ;
static void __unregister_request ( struct ceph_osd_client * osdc , <nl> */ <nl> static void __cancel_request ( struct ceph_osd_request * req ) <nl> { <nl> - if ( req -> r_sent ) { <nl> + if ( req -> r_sent && req -> r_osd ) { <nl> ceph_con_revoke (& req -> r_osd -> o_con , req -> r_request ); <nl> req -> r_sent = 0 ; <nl> }
static bool intel_sdvo_detect_hdmi_audio ( struct drm_connector * connector ) <nl> edid = intel_sdvo_get_edid ( connector ); <nl> if ( edid != NULL && edid -> input & DRM_EDID_INPUT_DIGITAL ) <nl> has_audio = drm_detect_monitor_audio ( edid ); <nl> + kfree ( edid ); <nl>  <nl> return has_audio ; <nl> }
static void iwl4965_rx_reply_tx ( struct iwl_priv * priv , <nl> struct ieee80211_tx_info * info ; <nl> struct iwl4965_tx_resp * tx_resp = ( void *)& pkt -> u . raw [ 0 ]; <nl> u32 status = le32_to_cpu ( tx_resp -> u . status ); <nl> - int tid = MAX_TID_COUNT ; <nl> + int tid = MAX_TID_COUNT - 1 ; <nl> int sta_id ; <nl> int freed ; <nl> u8 * qc = NULL ;
int v4l2_m2m_streamoff ( struct file * file , struct v4l2_m2m_ctx * m2m_ctx , <nl> /* Drop queue , since streamoff returns device to the same state as after <nl> * calling reqbufs . */ <nl> INIT_LIST_HEAD (& q_ctx -> rdy_queue ); <nl> + q_ctx -> num_rdy = 0 ; <nl> spin_unlock_irqrestore (& q_ctx -> rdy_spinlock , flags ); <nl>  <nl> if ( m2m_dev -> curr_ctx == m2m_ctx ) {
static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x23 , { KEY_PROG3 } }, /* P_Key */ <nl> { KE_KEY , 0x24 , { KEY_PROG4 } }, /* Social networking_Key */ <nl> { KE_KEY , 0x64 , { KEY_SWITCHVIDEOMODE } }, /* Display Switch */ <nl> - { KE_KEY , 0x82 , { KEY_F22 } }, /* Touch Pad On / Off */ <nl> + { KE_KEY , 0x82 , { KEY_TOUCHPAD_TOGGLE } }, /* Touch Pad On / Off */ <nl> { KE_END , 0 } <nl> }; <nl> 
DOT11D_GetMaxTxPwrInDbm ( <nl> netdev_info ( dev -> dev , " DOT11D_GetMaxTxPwrInDbm (): Invalid Channel \ n "); <nl> return MaxTxPwrInDbm ; <nl> } <nl> - if ( pDot11dInfo -> channel_map [ Channel ]) { <nl> + if ( pDot11dInfo -> channel_map [ Channel ]) <nl> MaxTxPwrInDbm = pDot11dInfo -> MaxTxPwrDbmList [ Channel ]; <nl> - } <nl>  <nl> return MaxTxPwrInDbm ; <nl> }
static int alps_enter_command_mode ( struct psmouse * psmouse , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( param [ 0 ] != 0x88 && param [ 1 ] != 0x07 ) { <nl> + if ( param [ 0 ] != 0x88 || ( param [ 1 ] != 0x07 && param [ 1 ] != 0x08 )) { <nl> psmouse_dbg ( psmouse , <nl> " unknown response while entering command mode \ n "); <nl> return - 1 ;
static int irda_recvmsg_dgram ( struct kiocb * iocb , struct socket * sock , <nl>  <nl> IRDA_DEBUG ( 4 , "% s ()\ n ", __func__ ); <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> skb = skb_recv_datagram ( sk , flags & ~ MSG_DONTWAIT , <nl> flags & MSG_DONTWAIT , & err ); <nl> if (! skb )
static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl>  <nl> mutex_lock (& dev -> lock ); <nl>  <nl> - if ( dev -> open ++) <nl> + if ( dev -> open ) <nl> goto unlock ; <nl>  <nl> kref_get (& dev -> ref ); <nl> static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl> goto error_release ; <nl>  <nl> unlock : <nl> + dev -> open ++; <nl> mutex_unlock (& dev -> lock ); <nl> blktrans_dev_put ( dev ); <nl> return ret ;
static int xen_add_device ( struct device * dev ) <nl>  <nl> # ifdef CONFIG_ACPI <nl> handle = DEVICE_ACPI_HANDLE (& pci_dev -> dev ); <nl> - if (! handle ) <nl> + if (! handle && pci_dev -> bus -> bridge ) <nl> handle = DEVICE_ACPI_HANDLE ( pci_dev -> bus -> bridge ); <nl> # ifdef CONFIG_PCI_IOV <nl> if (! handle && pci_dev -> is_virtfn )
int compat_get_timex ( struct timex * txc , const struct compat_timex __user * utp ) <nl> { <nl> struct compat_timex tx32 ; <nl>  <nl> + memset ( txc , 0 , sizeof ( struct timex )); <nl> if ( copy_from_user (& tx32 , utp , sizeof ( struct compat_timex ))) <nl> return - EFAULT ; <nl> 
static int ar9003_hw_set_channel ( struct ath_hw * ah , struct ath9k_channel * chan ) <nl> u32 chan_frac ; <nl>  <nl> channelSel = ( freq * 2 ) / 75 ; <nl> - chan_frac = (( freq % 75 ) * 0x20000 ) / 75 ; <nl> + chan_frac = ((( freq * 2 ) % 75 ) * 0x20000 ) / 75 ; <nl> channelSel = ( channelSel << 17 ) | chan_frac ; <nl> } else { <nl> channelSel = CHANSEL_5G ( freq );
int iomap_fiemap ( struct inode * inode , struct fiemap_extent_info * fi , <nl> while ( len > 0 ) { <nl> ret = iomap_apply ( inode , start , len , 0 , ops , & ctx , <nl> iomap_fiemap_actor ); <nl> + /* inode with no ( attribute ) mapping will give ENOENT */ <nl> + if ( ret == - ENOENT ) <nl> + break ; <nl> if ( ret < 0 ) <nl> return ret ; <nl> if ( ret == 0 )
struct platform_device * __init imx_add_platform_device_dmamask ( <nl> ret = platform_device_add ( pdev ); <nl> if ( ret ) { <nl> err : <nl> + if ( dmamask ) <nl> + kfree ( pdev -> dev . dma_mask ); <nl> platform_device_put ( pdev ); <nl> return ERR_PTR ( ret ); <nl> }
static inline int ptr_ring_consume_batched_bh ( struct ptr_ring * r , <nl>  <nl> static inline void ** __ptr_ring_init_queue_alloc ( unsigned int size , gfp_t gfp ) <nl> { <nl> + if ( size * sizeof ( void *) > KMALLOC_MAX_SIZE ) <nl> + return NULL ; <nl> return kcalloc ( size , sizeof ( void *), gfp ); <nl> } <nl> 
static int intel_pstate_set_policy ( struct cpufreq_policy * policy ) <nl> limits -> max_sysfs_pct ); <nl> limits -> max_perf_pct = max ( limits -> min_policy_pct , <nl> limits -> max_perf_pct ); <nl> + limits -> max_perf = round_up ( limits -> max_perf , 8 ); <nl>  <nl> /* Make sure min_perf_pct <= max_perf_pct */ <nl> limits -> min_perf_pct = min ( limits -> max_perf_pct , limits -> min_perf_pct );
static ssize_t input_dev_show_modalias ( struct class_device * dev , char * buf ) <nl>  <nl> len = input_print_modalias ( buf , PAGE_SIZE , id , 1 ); <nl>  <nl> - return max_t ( int , len , PAGE_SIZE ); <nl> + return min_t ( int , len , PAGE_SIZE ); <nl> } <nl> static CLASS_DEVICE_ATTR ( modalias , S_IRUGO , input_dev_show_modalias , NULL ); <nl> 
ecryptfs_setxattr ( struct dentry * dentry , const char * name , const void * value , <nl> } <nl>  <nl> rc = vfs_setxattr ( lower_dentry , name , value , size , flags ); <nl> + if (! rc ) <nl> + fsstack_copy_attr_all ( dentry -> d_inode , lower_dentry -> d_inode ); <nl> out : <nl> return rc ; <nl> }
static void __init atmci_get_cap ( struct atmel_mci * host ) <nl> /* keep only major version number */ <nl> switch ( version & 0xf00 ) { <nl> case 0x100 : <nl> + host -> caps . has_pdc = 1 ; <nl> + break ; <nl> case 0x200 : <nl> host -> caps . has_pdc = 1 ; <nl> host -> caps . has_rwproof = 1 ;
static s32 i2c_smbus_xfer_emulated ( struct i2c_adapter * adapter , u16 addr , <nl> status = i2c_transfer ( adapter , msg , num ); <nl> if ( status < 0 ) <nl> return status ; <nl> + if ( status != num ) <nl> + return - EIO ; <nl>  <nl> /* Check PEC if last message is a read */ <nl> if ( i && ( msg [ num - 1 ]. flags & I2C_M_RD )) {
static void do_config_file ( const char * filename ) <nl> perror ( filename ); <nl> exit ( 2 ); <nl> } <nl> - fstat ( fd , & st ); <nl> + if ( fstat ( fd , & st ) < 0 ) { <nl> + fprintf ( stderr , " fixdep : error fstat ' ing config file : "); <nl> + perror ( filename ); <nl> + exit ( 2 ); <nl> + } <nl> if ( st . st_size == 0 ) { <nl> close ( fd ); <nl> return ;
static int stmmac_hw_init ( struct stmmac_priv * priv ) <nl> struct mac_device_info * mac ; <nl>  <nl> /* Identify the MAC HW device */ <nl> - if ( priv -> plat -> has_gmac ) <nl> + if ( priv -> plat -> has_gmac ) { <nl> + priv -> dev -> priv_flags |= IFF_UNICAST_FLT ; <nl> mac = dwmac1000_setup ( priv -> ioaddr ); <nl> - else <nl> + } else { <nl> mac = dwmac100_setup ( priv -> ioaddr ); <nl> + } <nl> if (! mac ) <nl> return - ENOMEM ; <nl> 
static int msr_open ( struct inode * inode , struct file * file ) <nl> unsigned int cpu ; <nl> struct cpuinfo_x86 * c ; <nl>  <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> + <nl> cpu = iminor ( file -> f_path . dentry -> d_inode ); <nl> if ( cpu >= nr_cpu_ids || ! cpu_online ( cpu )) <nl> return - ENXIO ; /* No such CPU */
int qla24xx_async_notify_ack ( scsi_qla_host_t * vha , fc_port_t * fcport , <nl> qla2x00_init_timer ( sp , qla2x00_get_async_timeout ( vha )+ 2 ); <nl>  <nl> sp -> u . iocb_cmd . u . nack . ntfy = ntfy ; <nl> - <nl> + sp -> u . iocb_cmd . timeout = qla2x00_async_iocb_timeout ; <nl> sp -> done = qla2x00_async_nack_sp_done ; <nl>  <nl> rval = qla2x00_start_sp ( sp );
static int synic_set_irq ( struct kvm_vcpu_hv_synic * synic , u32 sint ) <nl> struct kvm_lapic_irq irq ; <nl> int ret , vector ; <nl>  <nl> + if ( KVM_BUG_ON (! lapic_in_kernel ( vcpu ), vcpu -> kvm )) <nl> + return - EINVAL ; <nl> + <nl> if ( sint >= ARRAY_SIZE ( synic -> sint )) <nl> return - EINVAL ; <nl> 
void check_and_switch_context ( struct mm_struct * mm , unsigned int cpu ) <nl> raw_spin_unlock_irqrestore (& cpu_asid_lock , flags ); <nl>  <nl> switch_mm_fastpath : <nl> + <nl> + arm64_apply_bp_hardening (); <nl> + <nl> /* <nl> * Defer TTBR0_EL1 setting for user threads to uaccess_enable () when <nl> * emulating PAN . <nl> asmlinkage void post_ttbr_update_workaround ( void ) <nl> " ic iallu ; dsb nsh ; isb ", <nl> ARM64_WORKAROUND_CAVIUM_27456 , <nl> CONFIG_CAVIUM_ERRATUM_27456 )); <nl> - <nl> - arm64_apply_bp_hardening (); <nl> } <nl>  <nl> static int asids_init ( void )
void dma_run_dependencies ( struct dma_async_tx_descriptor * tx ) <nl> if (! dep ) <nl> return ; <nl>  <nl> + /* we ' ll submit tx -> next now , so clear the link */ <nl> + tx -> next = NULL ; <nl> chan = dep -> chan ; <nl>  <nl> /* keep submitting up until a channel switch is detected
static void vmw_fb_dirty_flush ( struct vmw_fb_par * par ) <nl> SVGAFifoCmdUpdate body ; <nl> } * cmd ; <nl>  <nl> + if ( vmw_priv -> suspended ) <nl> + return ; <nl> + <nl> spin_lock_irqsave (& par -> dirty . lock , flags ); <nl> if (! par -> dirty . active ) { <nl> spin_unlock_irqrestore (& par -> dirty . lock , flags );
int rtw_disassoc_cmd23a ( struct rtw_adapter * padapter , u32 deauth_timeout_ms , <nl> } else { <nl> /* no need to enqueue , do the cmd hdl directly and <nl> free cmd parameter */ <nl> - if ( H2C_SUCCESS != disconnect_hdl23a ( padapter , ( u8 *) param )) <nl> + if ( disconnect_hdl23a ( padapter , ( u8 *) param ) != H2C_SUCCESS ) <nl> res = _FAIL ; <nl> kfree ( param ); <nl> }
static void edge_bulk_in_callback ( struct urb * urb ) <nl>  <nl> port_number = edge_port -> port -> port_number ; <nl>  <nl> - if ( edge_port -> lsr_event ) { <nl> + if ( urb -> actual_length > 0 && edge_port -> lsr_event ) { <nl> edge_port -> lsr_event = 0 ; <nl> dev_dbg ( dev , "% s ===== Port % u LSR Status = % 02x , Data = % 02x ======\ n ", <nl> __func__ , port_number , edge_port -> lsr_mask , * data );
int iio_sw_buffer_preenable ( struct iio_dev * indio_dev ) <nl> buffer -> scan_mask ); <nl> else <nl> indio_dev -> active_scan_mask = buffer -> scan_mask ; <nl> + <nl> + if ( indio_dev -> active_scan_mask == NULL ) <nl> + return - EINVAL ; <nl> + <nl> iio_update_demux ( indio_dev ); <nl>  <nl> if ( indio_dev -> info -> update_scan_mode )
int ipu_dp_init ( struct ipu_soc * ipu , struct device * dev , unsigned long base ) <nl> int i ; <nl>  <nl> priv = devm_kzalloc ( dev , sizeof (* priv ), GFP_KERNEL ); <nl> + if (! priv ) <nl> + return - ENOMEM ; <nl> priv -> dev = dev ; <nl> priv -> ipu = ipu ; <nl> 
__update_curr ( struct cfs_rq * cfs_rq , struct sched_entity * curr , <nl> schedstat_set ( curr -> exec_max , max (( u64 ) delta_exec , curr -> exec_max )); <nl>  <nl> curr -> sum_exec_runtime += delta_exec ; <nl> - cfs_rq -> exec_clock += delta_exec ; <nl> + schedstat_add ( cfs_rq , exec_clock , delta_exec ); <nl> delta_exec_weighted = delta_exec ; <nl> if ( unlikely ( curr -> load . weight != NICE_0_LOAD )) { <nl> delta_exec_weighted = calc_delta_fair ( delta_exec_weighted ,
static struct snd_emu_chip_details emu_chip_details [] = { <nl> . ca0151_chip = 1 , <nl> . spk71 = 1 , <nl> . spdif_bug = 1 , <nl> + . invert_shared_spdif = 1 , /* digital / analog switch swapped */ <nl> . ac97_chip = 1 } , <nl> {. vendor = 0x1102 , . device = 0x0004 , . subsystem = 0x10021102 , <nl> . driver = " Audigy2 ", . name = " SB Audigy 2 Platinum [ SB0240P ]",
int rds_rdma_extra_size ( struct rds_rdma_args * args ) <nl>  <nl> local_vec = ( struct rds_iovec __user *)( unsigned long ) args -> local_vec_addr ; <nl>  <nl> + if ( args -> nr_local == 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* figure out the number of pages in the vector */ <nl> for ( i = 0 ; i < args -> nr_local ; i ++) { <nl> if ( copy_from_user (& vec , & local_vec [ i ],
static const char * ext4_decode_error ( struct super_block * sb , int errno , <nl> errstr = " Out of memory "; <nl> break ; <nl> case - EROFS : <nl> - if (! sb || EXT4_SB ( sb )-> s_journal -> j_flags & JBD2_ABORT ) <nl> + if (! sb || ( EXT4_SB ( sb )-> s_journal && <nl> + EXT4_SB ( sb )-> s_journal -> j_flags & JBD2_ABORT )) <nl> errstr = " Journal has aborted "; <nl> else <nl> errstr = " Readonly filesystem ";
int free_stream_context ( struct intel_sst_drv * ctx , unsigned int str_id ) <nl> if ( ret ) <nl> sst_clean_stream (& ctx -> streams [ str_id ]); <nl> return ret ; <nl> + } else { <nl> + dev_err ( ctx -> dev , " we tried to free stream context % d which was freed !!!\ n ", str_id ); <nl> } <nl> return ret ; <nl> }
int genl_register_family ( struct genl_family * family ) <nl> start , end + 1 , GFP_KERNEL ); <nl> if ( family -> id < 0 ) { <nl> err = family -> id ; <nl> - goto errout_locked ; <nl> + goto errout_free ; <nl> } <nl>  <nl> err = genl_validate_assign_mc_groups ( family ); <nl> int genl_register_family ( struct genl_family * family ) <nl>  <nl> errout_remove : <nl> idr_remove (& genl_fam_idr , family -> id ); <nl> + errout_free : <nl> kfree ( family -> attrbuf ); <nl> errout_locked : <nl> genl_unlock_all ();
static int elo_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl>  <nl> return 0 ; <nl> err_free : <nl> + usb_put_dev ( udev ); <nl> kfree ( priv ); <nl> return ret ; <nl> }
static int db8500_prcmu_probe ( struct platform_device * pdev ) <nl> } <nl> tcdm_base = devm_ioremap (& pdev -> dev , res -> start , <nl> resource_size ( res )); <nl> + if (! tcdm_base ) { <nl> + dev_err (& pdev -> dev , <nl> + " failed to ioremap prcmu - tcdm register memory \ n "); <nl> + return - ENOENT ; <nl> + } <nl>  <nl> /* Clean up the mailbox interrupts after pre - kernel code . */ <nl> writel ( ALL_MBOX_BITS , PRCM_ARM_IT1_CLR );
sg_start_req ( Sg_request * srp , unsigned char * cmd ) <nl> md -> from_user = 0 ; <nl> } <nl>  <nl> + if ( unlikely ( iov_count > MAX_UIOVEC )) <nl> + return - EINVAL ; <nl> + <nl> if ( iov_count ) { <nl> int size = sizeof ( struct iovec ) * iov_count ; <nl> struct iovec * iov ;
static int vidioc_try_fmt_vid_cap ( struct file * file , void * priv , <nl> else <nl> f -> fmt . pix . field = dev -> interlaced ? <nl> V4L2_FIELD_INTERLACED : V4L2_FIELD_TOP ; <nl> + f -> fmt . pix . priv = 0 ; <nl>  <nl> return 0 ; <nl> }
static int destroy_queue_nocpsch ( struct device_queue_manager * dqm , <nl> } <nl> dqm -> sdma_queue_count --; <nl> deallocate_sdma_queue ( dqm , q -> sdma_id ); <nl> + } else { <nl> + pr_debug (" q -> properties . type is invalid (% d )\ n ", <nl> + q -> properties . type ); <nl> + retval = - EINVAL ; <nl> + goto out ; <nl> } <nl>  <nl> retval = mqd -> destroy_mqd ( mqd , q -> mqd ,
int ath6kl_debug_init_fs ( struct ath6kl * ar ) <nl> void ath6kl_debug_cleanup ( struct ath6kl * ar ) <nl> { <nl> skb_queue_purge (& ar -> debug . fwlog_queue ); <nl> + complete (& ar -> debug . fwlog_completion ); <nl> kfree ( ar -> debug . roam_tbl ); <nl> } <nl> 
struct stedma40_platform_data dma40_plat_data = { <nl> struct platform_device u8500_dma40_device = { <nl> . dev = { <nl> . platform_data = & dma40_plat_data , <nl> + . coherent_dma_mask = DMA_BIT_MASK ( 32 ), <nl> }, <nl> . name = " dma40 ", <nl> . id = 0 ,
int sock_diag_register ( struct sock_diag_handler * hndl ) <nl> { <nl> int err = 0 ; <nl>  <nl> - if ( hndl -> family > AF_MAX ) <nl> + if ( hndl -> family >= AF_MAX ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& sock_diag_table_mutex ); <nl> void sock_diag_unregister ( struct sock_diag_handler * hnld ) <nl> { <nl> int family = hnld -> family ; <nl>  <nl> - if ( family > AF_MAX ) <nl> + if ( family >= AF_MAX ) <nl> return ; <nl>  <nl> mutex_lock (& sock_diag_table_mutex );
xfs_da3_fixhashpath ( <nl> node = blk -> bp -> b_addr ; <nl> dp -> d_ops -> node_hdr_from_disk (& nodehdr , node ); <nl> btree = dp -> d_ops -> node_tree_p ( node ); <nl> - if ( be32_to_cpu ( btree -> hashval ) == lasthash ) <nl> + if ( be32_to_cpu ( btree [ blk -> index ]. hashval ) == lasthash ) <nl> break ; <nl> blk -> hashval = lasthash ; <nl> btree [ blk -> index ]. hashval = cpu_to_be32 ( lasthash );
int add_mtd_partitions ( struct mtd_info * master , <nl>  <nl> for ( i = 0 ; i < nbparts ; i ++) { <nl> slave = allocate_partition ( master , parts + i , i , cur_offset ); <nl> - if ( IS_ERR ( slave )) <nl> + if ( IS_ERR ( slave )) { <nl> + del_mtd_partitions ( master ); <nl> return PTR_ERR ( slave ); <nl> + } <nl>  <nl> mutex_lock (& mtd_partitions_mutex ); <nl> list_add (& slave -> list , & mtd_partitions );
static void audit_log_feature_change ( int which , u32 old_feature , u32 new_feature <nl> { <nl> struct audit_buffer * ab ; <nl>  <nl> + if ( audit_enabled == AUDIT_OFF ) <nl> + return ; <nl> + <nl> ab = audit_log_start ( NULL , GFP_KERNEL , AUDIT_FEATURE_CHANGE ); <nl> audit_log_format ( ab , " feature =% s old =% d new =% d old_lock =% d new_lock =% d res =% d ", <nl> audit_feature_names [ which ], !! old_feature , !! new_feature ,
static int ethtool_phys_id ( struct net_device * dev , void __user * useraddr ) <nl> if ( rc == 0 ) { <nl> /* Driver will handle this itself */ <nl> schedule_timeout_interruptible ( <nl> - id . data ? id . data : MAX_SCHEDULE_TIMEOUT ); <nl> + id . data ? ( id . data * HZ ) : MAX_SCHEDULE_TIMEOUT ); <nl> } else { <nl> /* Driver expects to be called periodically */ <nl> do {
static int llog_process_thread ( void * arg ) <nl> else <nl> last_index = LLOG_BITMAP_BYTES * 8 - 1 ; <nl>  <nl> + /* Record is not in this buffer . */ <nl> + if ( index > last_index ) <nl> + goto out ; <nl> + <nl> while ( rc == 0 ) { <nl> struct llog_rec_hdr * rec ; <nl> 
static int do_recover_data ( struct f2fs_sb_info * sbi , struct inode * inode , <nl> # endif <nl> /* We should not get - ENOSPC */ <nl> f2fs_bug_on ( sbi , err ); <nl> + if ( err ) <nl> + goto err ; <nl> } <nl>  <nl> /* Check the previous node page having this index */
static int omap_pcm_new ( struct snd_soc_pcm_runtime * rtd ) <nl> } <nl>  <nl> out : <nl> + /* free preallocated buffers in case of error */ <nl> + if ( ret ) <nl> + omap_pcm_free_dma_buffers ( pcm ); <nl> + <nl> return ret ; <nl> } <nl> 
static struct xfrm_state * pfkey_msg2xfrm_state ( struct net * net , <nl> ext_hdrs [ SADB_X_EXT_NAT_T_DPORT - 1 ]; <nl> natt -> encap_dport = n_port -> sadb_x_nat_t_port_port ; <nl> } <nl> + memset (& natt -> encap_oa , 0 , sizeof ( natt -> encap_oa )); <nl> } <nl>  <nl> err = xfrm_init_state ( x );
static void btc8821a2ant_run_coexist_mechanism ( struct btc_coexist * btcoexist ) <nl> return ; <nl> } <nl>  <nl> + if ( coex_sta -> under_ips ) { <nl> + RT_TRACE ( rtlpriv , COMP_BT_COEXIST , DBG_LOUD , <nl> + "[ BTCoex ], wifi is under IPS !!!\ n "); <nl> + return ; <nl> + } <nl> + <nl> algorithm = btc8821a2ant_action_algorithm ( btcoexist ); <nl> if ( coex_sta -> c2h_bt_inquiry_page && <nl> ( BT_8821A_2ANT_COEX_ALGO_PANHS != algorithm )) {
static void __ieee80211_scan_completed ( struct ieee80211_hw * hw , bool aborted , <nl> if ( local -> scan_req != local -> int_scan_req ) <nl> cfg80211_scan_done ( local -> scan_req , aborted ); <nl> local -> scan_req = NULL ; <nl> - local -> scan_sdata = NULL ; <nl> + rcu_assign_pointer ( local -> scan_sdata , NULL ); <nl>  <nl> local -> scanning = 0 ; <nl> local -> scan_channel = NULL ;
void psb_intel_crtc_init ( struct drm_device * dev , int pipe , <nl> ( struct drm_connector **) ( psb_intel_crtc + 1 ); <nl> psb_intel_crtc -> mode_set . num_connectors = 0 ; <nl> psb_intel_cursor_init ( dev , psb_intel_crtc ); <nl> + <nl> + /* Set to true so that the pipe is forced off on initial config . */ <nl> + psb_intel_crtc -> active = true ; <nl> } <nl>  <nl> int psb_intel_get_pipe_from_crtc_id ( struct drm_device * dev , void * data ,
static void oz_add_farewell ( struct oz_pd * pd , u8 ep_num , u8 index , <nl> return ; <nl> f -> ep_num = ep_num ; <nl> f -> index = index ; <nl> + f -> len = len ; <nl> memcpy ( f -> report , report , len ); <nl> oz_dbg ( ON , " RX : Adding farewell report \ n "); <nl> spin_lock (& g_polling_lock );
asmlinkage int sys_rt_sigreturn ( struct pt_regs * regs ) <nl> if ( restore_sigcontext ( regs , & frame -> uc . uc_mcontext )) <nl> goto badframe ; <nl>  <nl> + if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> sp ) == - EFAULT ) <nl> + goto badframe ; <nl> + <nl> pr_debug (" Context restored : pc = % 08lx , lr = % 08lx , sp = % 08lx \ n ", <nl> regs -> pc , regs -> lr , regs -> sp ); <nl> 
long join_session_keyring ( const char * name ) <nl> ret = PTR_ERR ( keyring ); <nl> goto error2 ; <nl> } else if ( keyring == new -> session_keyring ) { <nl> + key_put ( keyring ); <nl> ret = 0 ; <nl> goto error2 ; <nl> }
static int intel_backlight_device_update_status ( struct backlight_device * bd ) <nl> */ <nl> if ( panel -> backlight . enabled ) { <nl> if ( panel -> backlight_power ) { <nl> - bool enable = bd -> props . power == FB_BLANK_UNBLANK ; <nl> + bool enable = bd -> props . power == FB_BLANK_UNBLANK && <nl> + bd -> props . brightness != 0 ; <nl> panel -> backlight_power ( connector , enable ); <nl> } <nl> } else {
store_priv_session_ ## field ( struct device * dev , \ <nl> # define iscsi_priv_session_rw_attr ( field , format ) \ <nl> iscsi_priv_session_attr_show ( field , format ) \ <nl> iscsi_priv_session_attr_store ( field ) \ <nl> - static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUGO , \ <nl> + static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUSR , \ <nl> show_priv_session_ ## field , \ <nl> store_priv_session_ ## field ) <nl> iscsi_priv_session_rw_attr ( recovery_tmo , "% d ");
void drm_helper_resume_force_mode ( struct drm_device * dev ) <nl> int encoder_dpms ; <nl> bool ret ; <nl>  <nl> + drm_modeset_lock_all ( dev ); <nl> list_for_each_entry ( crtc , & dev -> mode_config . crtc_list , head ) { <nl>  <nl> if (! crtc -> enabled ) <nl> void drm_helper_resume_force_mode ( struct drm_device * dev ) <nl>  <nl> /* disable the unused connectors while restoring the modesetting */ <nl> __drm_helper_disable_unused_functions ( dev ); <nl> + drm_modeset_unlock_all ( dev ); <nl> } <nl> EXPORT_SYMBOL ( drm_helper_resume_force_mode ); <nl> 
static int uvc_v4l2_open ( struct file * file ) <nl> if ( atomic_inc_return (& stream -> dev -> users ) == 1 ) { <nl> ret = uvc_status_start ( stream -> dev ); <nl> if ( ret < 0 ) { <nl> - usb_autopm_put_interface ( stream -> dev -> intf ); <nl> atomic_dec (& stream -> dev -> users ); <nl> + usb_autopm_put_interface ( stream -> dev -> intf ); <nl> kfree ( handle ); <nl> return ret ; <nl> }
static int hpsa_get_pdisk_of_ioaccel2 ( struct ctlr_info * h , <nl>  <nl> /* Get the list of physical devices */ <nl> physicals = kzalloc ( reportsize , GFP_KERNEL ); <nl> + if ( physicals == NULL ) <nl> + return 0 ; <nl> if ( hpsa_scsi_do_report_phys_luns ( h , ( struct ReportLUNdata *) physicals , <nl> reportsize , extended )) { <nl> dev_err (& h -> pdev -> dev ,
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static int pxad_probe ( struct platform_device * op ) <nl> pdev -> slave . dst_addr_widths = widths ; <nl> pdev -> slave . directions = BIT ( DMA_MEM_TO_DEV ) | BIT ( DMA_DEV_TO_MEM ); <nl> pdev -> slave . residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR ; <nl> + pdev -> slave . descriptor_reuse = true ; <nl>  <nl> pdev -> slave . dev = & op -> dev ; <nl> ret = pxad_init_dmadev ( op , pdev , dma_channels );
static int mmc_ext_csd_open ( struct inode * inode , struct file * filp ) <nl> if ( err ) <nl> goto out_free ; <nl>  <nl> - for ( i = 511 ; i >= 0 ; i --) <nl> + for ( i = 0 ; i < 512 ; i ++) <nl> n += sprintf ( buf + n , "% 02x ", ext_csd [ i ]); <nl> n += sprintf ( buf + n , "\ n "); <nl> BUG_ON ( n != EXT_CSD_STR_LEN );
static inline int copy_linear_skb ( struct sk_buff * skb , int len , int off , <nl> { <nl> int n , copy = len - off ; <nl>  <nl> + if ( copy < 0 ) <nl> + return - EINVAL ; <nl> n = copy_to_iter ( skb -> data + off , copy , to ); <nl> if ( n == copy ) <nl> return 0 ;
static s32 ixgbe_reset_hw_X550em ( struct ixgbe_hw * hw ) <nl> hw -> phy . sfp_setup_needed = false ; <nl> } <nl>  <nl> + if ( status == IXGBE_ERR_SFP_NOT_SUPPORTED ) <nl> + return status ; <nl> + <nl> /* Reset PHY */ <nl> if (! hw -> phy . reset_disable && hw -> phy . ops . reset ) <nl> hw -> phy . ops . reset ( hw );
static int ci_get_platdata ( struct device * dev , <nl> return ret ; <nl> } <nl>  <nl> + if ( of_find_property ( dev -> of_node , " non - zero - ttctrl - ttha ", NULL )) <nl> + platdata -> flags |= CI_HDRC_SET_NON_ZERO_TTHA ; <nl> + <nl> ext_id = ERR_PTR (- ENODEV ); <nl> ext_vbus = ERR_PTR (- ENODEV ); <nl> if ( of_property_read_bool ( dev -> of_node , " extcon ")) {
static int cdrom_ioctl_media_changed ( struct cdrom_device_info * cdi , <nl> if (! CDROM_CAN ( CDC_SELECT_DISC ) || arg == CDSL_CURRENT ) <nl> return media_changed ( cdi , 1 ); <nl>  <nl> - if (( unsigned int ) arg >= cdi -> capacity ) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl>  <nl> info = kmalloc ( sizeof (* info ), GFP_KERNEL );
void drm_mode_config_reset ( struct drm_device * dev ) <nl> if ( encoder -> funcs -> reset ) <nl> encoder -> funcs -> reset ( encoder ); <nl>  <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> drm_for_each_connector ( connector , dev ) { <nl> connector -> status = connector_status_unknown ; <nl>  <nl> if ( connector -> funcs -> reset ) <nl> connector -> funcs -> reset ( connector ); <nl> } <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl> } <nl> EXPORT_SYMBOL ( drm_mode_config_reset ); <nl> 
int amdgpu_device_ip_suspend ( struct amdgpu_device * adev ) <nl> if ( amdgpu_sriov_vf ( adev )) <nl> amdgpu_virt_request_full_gpu ( adev , false ); <nl>  <nl> + /* ungate SMC block powergating */ <nl> + if ( adev -> powerplay . pp_feature & PP_GFXOFF_MASK ) <nl> + amdgpu_device_ip_set_powergating_state ( adev , <nl> + AMD_IP_BLOCK_TYPE_SMC , <nl> + AMD_CG_STATE_UNGATE ); <nl> + <nl> /* ungate SMC block first */ <nl> r = amdgpu_device_ip_set_clockgating_state ( adev , AMD_IP_BLOCK_TYPE_SMC , <nl> AMD_CG_STATE_UNGATE );
static void mtk_plane_atomic_update ( struct drm_plane * plane , <nl> pitch = fb -> pitches [ 0 ]; <nl> format = fb -> pixel_format ; <nl>  <nl> - addr += ( plane -> state -> src . x1 >> 16 ) * 4 ; <nl> + addr += ( plane -> state -> src . x1 >> 16 ) * drm_format_plane_cpp ( format , 0 ); <nl> addr += ( plane -> state -> src . y1 >> 16 ) * pitch ; <nl>  <nl> state -> pending . enable = true ;
batadv_frag_merge_packets ( struct hlist_head * chain , struct sk_buff * skb ) <nl> kfree ( entry ); <nl>  <nl> /* Make room for the rest of the fragments . */ <nl> - if ( pskb_expand_head ( skb_out , 0 , size - skb -> len , GFP_ATOMIC ) < 0 ) { <nl> + if ( pskb_expand_head ( skb_out , 0 , size - skb_out -> len , GFP_ATOMIC ) < 0 ) { <nl> kfree_skb ( skb_out ); <nl> skb_out = NULL ; <nl> goto free ;
static void ni6527_reset ( struct comedi_device * dev ) <nl> /* disable deglitch filters on all channels */ <nl> ni6527_set_filter_enable ( dev , 0 ); <nl>  <nl> + /* disable edge detection */ <nl> + ni6527_set_edge_detection ( dev , 0xffffffff , 0 , 0 ); <nl> + <nl> writeb ( NI6527_CLR_IRQS | NI6527_CLR_RESET_FILT , <nl> mmio + NI6527_CLR_REG ); <nl> writeb ( NI6527_CTRL_DISABLE_IRQS , mmio + NI6527_CTRL_REG );
static int __devinit bq20z75_probe ( struct i2c_client * client , <nl>  <nl> INIT_DELAYED_WORK (& bq20z75_device -> work , bq20z75_delayed_work ); <nl>  <nl> + bq20z75_device -> enable_detection = true ; <nl> + <nl> return 0 ; <nl>  <nl> exit_psupply :
nvkm_pmu_reset ( struct nvkm_pmu * pmu ) <nl> ); <nl>  <nl> /* Reset . */ <nl> - pmu -> func -> reset ( pmu ); <nl> + if ( pmu -> func -> reset ) <nl> + pmu -> func -> reset ( pmu ); <nl>  <nl> /* Wait for IMEM / DMEM scrubbing to be complete . */ <nl> nvkm_msec ( device , 2000 ,
static int of_platform_serial_setup ( struct platform_device * ofdev , <nl> port -> line = ret ; <nl>  <nl> port -> irq = irq_of_parse_and_map ( np , 0 ); <nl> + if (! port -> irq ) { <nl> + ret = - EPROBE_DEFER ; <nl> + goto err_unprepare ; <nl> + } <nl>  <nl> info -> rst = devm_reset_control_get_optional_shared (& ofdev -> dev , NULL ); <nl> if ( IS_ERR ( info -> rst )) {
int ip6_append_data ( struct sock * sk , int getfrag ( void * from , char * to , <nl> if ( WARN_ON ( np -> cork . opt )) <nl> return - EINVAL ; <nl>  <nl> - np -> cork . opt = kmalloc ( opt -> tot_len , sk -> sk_allocation ); <nl> + np -> cork . opt = kzalloc ( opt -> tot_len , sk -> sk_allocation ); <nl> if ( unlikely ( np -> cork . opt == NULL )) <nl> return - ENOBUFS ; <nl> 
static int change_memory_common ( unsigned long addr , int numpages , <nl> if (! size ) <nl> return 0 ; <nl>  <nl> - if (! in_range ( start , size , MODULES_VADDR , MODULES_END )) <nl> + if (! in_range ( start , size , MODULES_VADDR , MODULES_END ) && <nl> + ! in_range ( start , size , VMALLOC_START , VMALLOC_END )) <nl> return - EINVAL ; <nl>  <nl> data . set_mask = set_mask ;
static ssize_t iwl_dbgfs_sram_read ( struct file * file , <nl> const struct fw_img * img ; <nl> size_t bufsz ; <nl>  <nl> + if (! iwl_is_ready_rf ( priv )) <nl> + return - EAGAIN ; <nl> + <nl> /* default is to dump the entire data segment */ <nl> if (! priv -> dbgfs_sram_offset && ! priv -> dbgfs_sram_len ) { <nl> priv -> dbgfs_sram_offset = 0x800000 ;
EXPORT_SYMBOL ( vprintk_emit ); <nl>  <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> { <nl> - return vprintk_emit ( 0 , LOGLEVEL_DEFAULT , NULL , 0 , fmt , args ); <nl> + return vprintk_func ( fmt , args ); <nl> } <nl> EXPORT_SYMBOL ( vprintk ); <nl> 
static int dgnc_found_board ( struct pci_dev * pdev , int id ) <nl> return - ENOMEM ; <nl>  <nl> /* make a temporary message buffer for the boot messages */ <nl> - brd -> msgbuf_head = kzalloc ( sizeof ( u8 ) * 8192 , GFP_KERNEL ); <nl> + brd -> msgbuf_head = kcalloc ( 8192 , sizeof ( u8 ), GFP_KERNEL ); <nl> brd -> msgbuf = brd -> msgbuf_head ; <nl>  <nl> if (! brd -> msgbuf ) {
int iwl_enqueue_hcmd ( struct iwl_priv * priv , struct iwl_host_cmd * cmd ) <nl> return - EIO ; <nl> } <nl>  <nl> + if (( priv -> ucode_owner == IWL_OWNERSHIP_TM ) && <nl> + !( cmd -> flags & CMD_ON_DEMAND )) { <nl> + IWL_DEBUG_HC ( priv , " tm own the uCode , no regular hcmd send \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> copy_size = sizeof ( out_cmd -> hdr ); <nl> cmd_size = sizeof ( out_cmd -> hdr ); <nl> 
int smb2_handle_negotiate ( struct ksmbd_work * work ) <nl> status ); <nl> rsp -> hdr . Status = status ; <nl> rc = - EINVAL ; <nl> + kfree ( conn -> preauth_info ); <nl> + conn -> preauth_info = NULL ; <nl> goto err_out ; <nl> } <nl>  <nl> rc = init_smb3_11_server ( conn ); <nl> if ( rc < 0 ) { <nl> rsp -> hdr . Status = STATUS_INVALID_PARAMETER ; <nl> + kfree ( conn -> preauth_info ); <nl> + conn -> preauth_info = NULL ; <nl> goto err_out ; <nl> } <nl> 
static loff_t mtd_lseek ( struct file * file , loff_t offset , int orig ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( offset >= 0 && offset < mtd -> size ) <nl> + if ( offset >= 0 && offset <= mtd -> size ) <nl> return file -> f_pos = offset ; <nl>  <nl> return - EINVAL ;
static inline void rt2x00lib_set_if_combinations ( struct rt2x00_dev * rt2x00dev ) <nl> */ <nl> if_limit = & rt2x00dev -> if_limits_ap ; <nl> if_limit -> max = rt2x00dev -> ops -> max_ap_intf ; <nl> - if_limit -> types = BIT ( NL80211_IFTYPE_AP ); <nl> + if_limit -> types = BIT ( NL80211_IFTYPE_AP ) | <nl> + BIT ( NL80211_IFTYPE_MESH_POINT ); <nl>  <nl> /* <nl> * Build up AP interface combinations structure .
EXPORT_SYMBOL ( param_set_copystring ); <nl> int param_get_string ( char * buffer , const struct kernel_param * kp ) <nl> { <nl> const struct kparam_string * kps = kp -> str ; <nl> - return strlcpy ( buffer , kps -> string , kps -> maxlen ); <nl> + return strlcpy ( buffer , kps -> string , PAGE_SIZE ); <nl> } <nl> EXPORT_SYMBOL ( param_get_string ); <nl> 
static int snd_compress_check_input ( struct snd_compr_params * params ) <nl> { <nl> /* first let ' s check the buffer parameter ' s */ <nl> if ( params -> buffer . fragment_size == 0 || <nl> - params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + params -> buffer . fragments > INT_MAX / params -> buffer . fragment_size ) <nl> return - EINVAL ; <nl>  <nl> /* now codec parameters */
static int __devinit nmk_gpio_probe ( struct platform_device * dev ) <nl> struct clk * clk ; <nl> int secondary_irq ; <nl> void __iomem * base ; <nl> - int irq_start = - 1 ; <nl> + int irq_start = 0 ; <nl> int irq ; <nl> int ret ; <nl> 
static const struct omap_video_timings tpo_td043_timings = { <nl> static int tpo_td043_power_on ( struct tpo_td043_device * tpo_td043 ) <nl> { <nl> int nreset_gpio = tpo_td043 -> nreset_gpio ; <nl> + int r ; <nl>  <nl> if ( tpo_td043 -> powered_on ) <nl> return 0 ; <nl>  <nl> - regulator_enable ( tpo_td043 -> vcc_reg ); <nl> + r = regulator_enable ( tpo_td043 -> vcc_reg ); <nl> + if ( r != 0 ) <nl> + return r ; <nl>  <nl> /* wait for regulator to stabilize */ <nl> msleep ( 160 );
ath5k_set_key ( struct ieee80211_hw * hw , enum set_key_cmd cmd , <nl> if ( modparam_nohwcrypt ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( sc -> opmode == NL80211_IFTYPE_AP ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> switch ( key -> alg ) { <nl> case ALG_WEP : <nl> case ALG_TKIP :
struct powerdomain * omap_hwmod_get_pwrdm ( struct omap_hwmod * oh ) <nl> c = oh -> slaves [ oh -> _mpu_port_index ]-> _clk ; <nl> } <nl>  <nl> + if (! c -> clkdm ) <nl> + return NULL ; <nl> + <nl> return c -> clkdm -> pwrdm . ptr ; <nl>  <nl> }
static int regcache_default_sync ( struct regmap * map , unsigned int min , <nl> unsigned int val ; <nl> int ret ; <nl>  <nl> - if ( regmap_volatile ( map , reg )) <nl> + if ( regmap_volatile ( map , reg ) || <nl> + ! regmap_writeable ( map , reg )) <nl> continue ; <nl>  <nl> ret = regcache_read ( map , reg , & val );
struct inet_peer * inet_getpeer ( const struct inetpeer_addr * daddr , int create ) <nl> p -> rate_last = 0 ; <nl> p -> pmtu_expires = 0 ; <nl> p -> pmtu_orig = 0 ; <nl> + p -> redirect_genid = 0 ; <nl> memset (& p -> redirect_learned , 0 , sizeof ( p -> redirect_learned )); <nl>  <nl> 
static struct snd_pci_quirk alc883_cfg_tbl [] = { <nl> SND_PCI_QUIRK ( 0x17c0 , 0x4071 , " MEDION MD2 ", ALC883_MEDION_MD2 ), <nl> SND_PCI_QUIRK ( 0x1991 , 0x5625 , " Haier W66 ", ALC883_HAIER_W66 ), <nl> SND_PCI_QUIRK ( 0x17aa , 0x3bfc , " Lenovo NB0763 ", ALC883_LENOVO_NB0763 ), <nl> + SND_PCI_QUIRK ( 0x1043 , 0x8249 , " Asus M2A - VM HDMI ", ALC883_3ST_6ch_DIG ), <nl> + SND_PCI_QUIRK ( 0x147b , 0x1083 , " Abit IP35 - PRO ", ALC883_6ST_DIG ), <nl> {} <nl> }; <nl> 
static bool event_compare ( struct fsnotify_event * old , struct fsnotify_event * new <nl> /* remember , after old was put on the wait_q we aren ' t <nl> * allowed to look at the inode any more , only thing <nl> * left to check was if the file_name is the same */ <nl> - if ( old -> name_len && <nl> + if (! old -> name_len || <nl> ! strcmp ( old -> file_name , new -> file_name )) <nl> return true ; <nl> break ;
void dasd_int_handler ( struct ccw_device * cdev , unsigned long intparm , <nl> if ( cqr -> status == DASD_CQR_CLEAR_PENDING && <nl> scsw_fctl (& irb -> scsw ) & SCSW_FCTL_CLEAR_FUNC ) { <nl> cqr -> status = DASD_CQR_CLEARED ; <nl> - if ( cqr -> callback_data == DASD_SLEEPON_START_TAG ) <nl> - cqr -> callback_data = DASD_SLEEPON_END_TAG ; <nl> dasd_device_clear_timer ( device ); <nl> wake_up (& dasd_flush_wq ); <nl> - wake_up (& generic_waitq ); <nl> dasd_schedule_device_bh ( device ); <nl> return ; <nl> }
static void bmc150_accel_unregister_triggers ( struct bmc150_accel_data * data , <nl> { <nl> int i ; <nl>  <nl> - for ( i = from ; i >= 0 ; i ++) { <nl> + for ( i = from ; i >= 0 ; i --) { <nl> if ( data -> triggers [ i ]. indio_trig ) { <nl> iio_trigger_unregister ( data -> triggers [ i ]. indio_trig ); <nl> data -> triggers [ i ]. indio_trig = NULL ;
int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
cifs_mount ( struct super_block * sb , struct cifs_sb_info * cifs_sb , <nl>  <nl> remote_path_check : <nl> /* check if a whole path ( including prepath ) is not remote */ <nl> - if (! rc && cifs_sb -> prepathlen && tcon ) { <nl> + if (! rc && tcon ) { <nl> /* build_path_to_root works only when we have a valid tcon */ <nl> full_path = cifs_build_path_to_root ( cifs_sb , tcon ); <nl> if ( full_path == NULL ) {
static struct snd_seq_queue * queue_new ( int owner , int locked ) <nl> static void queue_delete ( struct snd_seq_queue * q ) <nl> { <nl> /* stop and release the timer */ <nl> + mutex_lock (& q -> timer_mutex ); <nl> snd_seq_timer_stop ( q -> timer ); <nl> snd_seq_timer_close ( q ); <nl> + mutex_unlock (& q -> timer_mutex ); <nl> /* wait until access free */ <nl> snd_use_lock_sync (& q -> use_lock ); <nl> /* release resources ... */
int iwl_run_init_mvm_ucode ( struct iwl_mvm * mvm , bool read_nvm ) <nl> ret = iwl_nvm_check_version ( mvm -> nvm_data , mvm -> trans ); <nl> WARN_ON ( ret ); <nl>  <nl> + /* Send TX valid antennas before triggering calibrations */ <nl> + ret = iwl_send_tx_ant_cfg ( mvm , mvm -> nvm_data -> valid_tx_ant ); <nl> + if ( ret ) <nl> + goto error ; <nl> + <nl> /* Override the calibrations from TLV and the const of fw */ <nl> iwl_set_default_calib_trigger ( mvm ); <nl> 
event_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_preds ( call ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl>  <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_subsystem_preds ( system ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl> 
int drm_open ( struct inode * inode , struct file * filp ) <nl> retcode = drm_open_helper ( inode , filp , dev ); <nl> if (! retcode ) { <nl> atomic_inc (& dev -> counts [ _DRM_STAT_OPENS ]); <nl> - if (! dev -> open_count ++) <nl> + if (! dev -> open_count ++) { <nl> retcode = drm_setup ( dev ); <nl> + if ( retcode ) <nl> + dev -> open_count --; <nl> + } <nl> } <nl> if (! retcode ) { <nl> mutex_lock (& dev -> struct_mutex );
do_ip_vs_get_ctl ( struct sock * sk , int cmd , void __user * user , int * len ) <nl> { <nl> struct ip_vs_timeout_user t ; <nl>  <nl> + memset (& t , 0 , sizeof ( t )); <nl> __ip_vs_get_timeouts ( net , & t ); <nl> if ( copy_to_user ( user , & t , sizeof ( t )) != 0 ) <nl> ret = - EFAULT ;
static const struct mssr_mod_clk r8a7795_mod_clks [] __initconst = { <nl> DEF_MOD (" scif2 ", 310 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" pcie1 ", 318 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" pcie0 ", 319 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if1 ", 327 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if0 ", 328 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" intc - ap ", 408 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" audmac0 ", 502 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" audmac1 ", 501 , R8A7795_CLK_S3D4 ),
__alloc_pages_slowpath ( gfp_t gfp_mask , unsigned int order , <nl> if ( p -> flags & PF_MEMALLOC ) <nl> goto nopage ; <nl>  <nl> + /* Avoid allocations with no watermarks from looping endlessly */ <nl> + if ( test_thread_flag ( TIF_MEMDIE ) && !( gfp_mask & __GFP_NOFAIL )) <nl> + goto nopage ; <nl> + <nl> /* Try direct reclaim and then allocating */ <nl> page = __alloc_pages_direct_reclaim ( gfp_mask , order , <nl> zonelist , high_zoneidx ,
static int hwsim_new_radio_nl ( struct sk_buff * msg , struct genl_info * info ) <nl> if ( info -> attrs [ HWSIM_ATTR_REG_CUSTOM_REG ]) { <nl> u32 idx = nla_get_u32 ( info -> attrs [ HWSIM_ATTR_REG_CUSTOM_REG ]); <nl>  <nl> - if ( idx >= ARRAY_SIZE ( hwsim_world_regdom_custom )) <nl> + if ( idx >= ARRAY_SIZE ( hwsim_world_regdom_custom )) { <nl> + kfree ( hwname ); <nl> return - EINVAL ; <nl> + } <nl> param . regd = hwsim_world_regdom_custom [ idx ]; <nl> } <nl> 
EXPORT_SYMBOL_GPL ( memory_add_physaddr_to_nid ); <nl> void __init acpi_numa_slit_init ( struct acpi_table_slit * slit ) <nl> { <nl> } <nl> + <nl> + void __init <nl> + acpi_numa_processor_affinity_init ( struct acpi_srat_cpu_affinity * pa ) <nl> +{ <nl> +} <nl> # endif
static void ilk_pipe_wm_get_hw_state ( struct drm_crtc * crtc ) <nl> if ( IS_HASWELL ( dev ) || IS_BROADWELL ( dev )) <nl> hw -> wm_linetime [ pipe ] = I915_READ ( PIPE_WM_LINETIME ( pipe )); <nl>  <nl> + memset ( active , 0 , sizeof (* active )); <nl> + <nl> active -> pipe_enabled = intel_crtc -> active ; <nl>  <nl> if ( active -> pipe_enabled ) {
static int setup_rt_frame ( int sig , struct k_sigaction * ka , siginfo_t * info , <nl> } else { <nl> regs -> gprs [ 14 ] = ( unsigned long ) <nl> frame -> retcode | PSW_ADDR_AMODE ; <nl> - err |= __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> - ( u16 __user *)( frame -> retcode )); <nl> + if ( __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> + ( u16 __user *)( frame -> retcode ))) <nl> + goto give_sigsegv ; <nl> } <nl>  <nl> /* Set up backchain . */
static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> ch . band = BRCMU_CHAN_BAND_2G ; <nl> ch . bw = BRCMU_CHAN_BW_40 ; <nl> + ch . sb = BRCMU_CHAN_SB_NONE ; <nl> ch . chnum = 0 ; <nl> cfg -> d11inf . encchspec (& ch ); <nl>  <nl> static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> brcmf_update_bw40_channel_flag (& band -> channels [ j ], & ch ); <nl> } <nl> + kfree ( pbuf ); <nl> } <nl> return err ; <nl> }
asmlinkage void __sched schedule ( void ) <nl> } <nl> EXPORT_SYMBOL ( schedule ); <nl>  <nl> -# ifdef CONFIG_SMP <nl> +# ifdef CONFIG_MUTEX_SPIN_ON_OWNER <nl> /* <nl> * Look out ! " owner " is an entirely speculative pointer <nl> * access and not reliable .
nlmsvc_lock ( struct svc_rqst * rqstp , struct nlm_file * file , <nl> goto out ; <nl> case - EAGAIN : <nl> ret = nlm_lck_denied ; <nl> - goto out ; <nl> + break ; <nl> case FILE_LOCK_DEFERRED : <nl> if ( wait ) <nl> break ; <nl> nlmsvc_lock ( struct svc_rqst * rqstp , struct nlm_file * file , <nl> goto out ; <nl> } <nl>  <nl> + ret = nlm_lck_denied ; <nl> + if (! wait ) <nl> + goto out ; <nl> + <nl> ret = nlm_lck_blocked ; <nl>  <nl> /* Append to list of blocked */
static int ip_frag_reasm ( struct ipq * qp , struct sk_buff * prev , <nl> skb_morph ( head , qp -> q . fragments ); <nl> head -> next = qp -> q . fragments -> next ; <nl>  <nl> - kfree_skb ( qp -> q . fragments ); <nl> + consume_skb ( qp -> q . fragments ); <nl> qp -> q . fragments = head ; <nl> } <nl> 
static int fsl_ifc_chip_init ( struct fsl_ifc_mtd * priv ) <nl> chip -> ecc . algo = NAND_ECC_HAMMING ; <nl> } <nl>  <nl> - if ( ctrl -> version == FSL_IFC_VERSION_1_1_0 ) <nl> + if ( ctrl -> version >= FSL_IFC_VERSION_1_1_0 ) <nl> fsl_ifc_sram_init ( priv ); <nl>  <nl> return 0 ;
static int dell_wmi_events_set_enabled ( bool enable ) <nl> int ret ; <nl>  <nl> buffer = kzalloc ( sizeof ( struct calling_interface_buffer ), GFP_KERNEL ); <nl> + if (! buffer ) <nl> + return - ENOMEM ; <nl> buffer -> cmd_class = CLASS_INFO ; <nl> buffer -> cmd_select = SELECT_APP_REGISTRATION ; <nl> buffer -> input [ 0 ] = 0x10000 ;
static int __devinit cas_init_one ( struct pci_dev * pdev , <nl> INIT_WORK (& cp -> reset_task , cas_reset_task ); <nl>  <nl> /* Default link parameters */ <nl> - if ( link_mode >= 0 && link_mode <= 6 ) <nl> + if ( link_mode >= 0 && link_mode < 6 ) <nl> cp -> link_cntl = link_modes [ link_mode ]; <nl> else <nl> cp -> link_cntl = BMCR_ANENABLE ;
void radeon_compute_pll_avivo ( struct radeon_pll * pll , <nl>  <nl> /* avoid high jitter with small fractional dividers */ <nl> if ( pll -> flags & RADEON_PLL_USE_FRAC_FB_DIV && ( fb_div % 10 )) { <nl> - fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 60 ); <nl> + fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 50 ); <nl> if ( fb_div < fb_div_min ) { <nl> unsigned tmp = DIV_ROUND_UP ( fb_div_min , fb_div ); <nl> fb_div *= tmp ;
static noinline void key_gc_unused_keys ( struct list_head * keys ) <nl> if ( test_bit ( KEY_FLAG_INSTANTIATED , & key -> flags )) <nl> atomic_dec (& key -> user -> nikeys ); <nl>  <nl> - key_user_put ( key -> user ); <nl> - <nl> /* now throw away the key memory */ <nl> if ( key -> type -> destroy ) <nl> key -> type -> destroy ( key ); <nl>  <nl> + key_user_put ( key -> user ); <nl> + <nl> kfree ( key -> description ); <nl>  <nl> # ifdef KEY_DEBUGGING
mlxsw_sp_lpm_tree_get ( struct mlxsw_sp * mlxsw_sp , <nl>  <nl> for ( i = 0 ; i < MLXSW_SP_LPM_TREE_COUNT ; i ++) { <nl> lpm_tree = & mlxsw_sp -> router . lpm_trees [ i ]; <nl> - if ( lpm_tree -> proto == proto && <nl> + if ( lpm_tree -> ref_count != 0 && <nl> + lpm_tree -> proto == proto && <nl> mlxsw_sp_prefix_usage_eq (& lpm_tree -> prefix_usage , <nl> prefix_usage )) <nl> goto inc_ref_count ;
static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
static void kvm_vcpu_init ( struct kvm_vcpu * vcpu , struct kvm * kvm , unsigned id ) <nl>  <nl> static void kvm_vcpu_destroy ( struct kvm_vcpu * vcpu ) <nl> { <nl> - kvm_dirty_ring_free (& vcpu -> dirty_ring ); <nl> kvm_arch_vcpu_destroy ( vcpu ); <nl> + kvm_dirty_ring_free (& vcpu -> dirty_ring ); <nl>  <nl> /* <nl> * No need for rcu_read_lock as VCPU_RUN is the only place that changes
ieee80211_deliver_skb ( struct ieee80211_rx_data * rx ) <nl> } <nl>  <nl> if ( xmit_skb ) { <nl> - /* send to wireless media */ <nl> + /* <nl> + * Send to wireless media and increase priority by 256 to <nl> + * keep the received priority instead of reclassifying <nl> + * the frame ( see cfg80211_classify8021d ). <nl> + */ <nl> + xmit_skb -> priority += 256 ; <nl> xmit_skb -> protocol = htons ( ETH_P_802_3 ); <nl> skb_reset_network_header ( xmit_skb ); <nl> skb_reset_mac_header ( xmit_skb );
MODULE_LICENSE (" GPL v2 "); <nl>  <nl> static int debug ; <nl> module_param ( debug , int , 0644 ); <nl> - MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on \ n "); <nl> + MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on "); <nl>  <nl> /* # define MPX_DEBUG */ <nl> 
static long bcm_char_ioctl ( struct file * filp , UINT cmd , ULONG arg ) <nl>  <nl> BCM_DEBUG_PRINT ( Adapter , DBG_TYPE_OTHERS , OSAL_DBG , DBG_LVL_ALL , " Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO \ n "); <nl>  <nl> + memset (& DevInfo , 0 , sizeof ( DevInfo )); <nl> DevInfo . MaxRDMBufferSize = BUFFER_4K ; <nl> DevInfo . u32DSDStartOffset = EEPROM_CALPARAM_START ; <nl> DevInfo . u32RxAlignmentCorrection = 0 ;
static void mmc_deselect_cards ( struct mmc_host * host ) <nl>  <nl> static inline void mmc_delay ( unsigned int ms ) <nl> { <nl> - if ( ms < HZ / 1000 ) { <nl> - yield (); <nl> + if ( ms < 1000 / HZ ) { <nl> + cond_resched (); <nl> mdelay ( ms ); <nl> } else { <nl> - msleep_interruptible ( ms ); <nl> + msleep ( ms ); <nl> } <nl> } <nl> 
static int f2fs_write_begin ( struct file * file , struct address_space * mapping , <nl>  <nl> /* check inline_data */ <nl> ipage = get_node_page ( sbi , inode -> i_ino ); <nl> - if ( IS_ERR ( ipage )) <nl> + if ( IS_ERR ( ipage )) { <nl> + err = PTR_ERR ( ipage ); <nl> goto unlock_fail ; <nl> + } <nl>  <nl> set_new_dnode (& dn , inode , ipage , ipage , 0 ); <nl> 
static int atmci_regs_show ( struct seq_file * s , void * v ) <nl> atmci_show_status_reg ( s , " SR ", buf [ MCI_SR / 4 ]); <nl> atmci_show_status_reg ( s , " IMR ", buf [ MCI_IMR / 4 ]); <nl>  <nl> + kfree ( buf ); <nl> + <nl> return 0 ; <nl> } <nl> 
static void cpu_ready_for_interrupts ( void ) <nl> * If we are not in hypervisor mode the job is done once for <nl> * the whole partition in configure_exceptions (). <nl> */ <nl> - if ( early_cpu_has_feature ( CPU_FTR_HVMODE ) && <nl> - early_cpu_has_feature ( CPU_FTR_ARCH_207S )) { <nl> + if ( cpu_has_feature ( CPU_FTR_HVMODE ) && <nl> + cpu_has_feature ( CPU_FTR_ARCH_207S )) { <nl> unsigned long lpcr = mfspr ( SPRN_LPCR ); <nl> mtspr ( SPRN_LPCR , lpcr | LPCR_AIL_3 ); <nl> }
static int __proc_dobitmasks ( void * data , int write , <nl> } else { <nl> rc = cfs_trace_copyin_string ( tmpstr , tmpstrlen , buffer , nob ); <nl> if ( rc < 0 ) { <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl>  <nl> static int __proc_dobitmasks ( void * data , int write , <nl> * mask |= D_EMERG ; <nl> } <nl>  <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl> 
i915_gem_do_execbuffer ( struct drm_device * dev , void * data , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( args -> num_cliprects > UINT_MAX / sizeof (* cliprects )) { <nl> + DRM_DEBUG (" execbuf with % u cliprects \ n ", <nl> + args -> num_cliprects ); <nl> + return - EINVAL ; <nl> + } <nl> cliprects = kmalloc ( args -> num_cliprects * sizeof (* cliprects ), <nl> GFP_KERNEL ); <nl> if ( cliprects == NULL ) {
void cec_received_msg ( struct cec_adapter * adap , struct cec_msg * msg ) <nl> if (! valid_la || msg -> len <= 1 ) <nl> return ; <nl>  <nl> + if ( adap -> log_addrs . log_addr_mask == 0 ) <nl> + return ; <nl> + <nl> /* <nl> * Process the message on the protocol level . If is_reply is true , <nl> * then cec_receive_notify () won ' t pass on the reply to the listener ( s )
int iioutils_get_type ( unsigned * is_signed , <nl> ret = - errno ; <nl> printf (" failed to pass scan type description \ n "); <nl> goto error_close_sysfsfp ; <nl> + } else if ( ret != 5 ) { <nl> + ret = - EIO ; <nl> + printf (" scan type description didn ' t match \ n "); <nl> + goto error_close_sysfsfp ; <nl> } <nl> * be = ( endianchar == ' b '); <nl> * bytes = padint / 8 ;
efi_initialize_iomem_resources ( struct resource * code_resource , <nl> if ( md -> attribute & EFI_MEMORY_WP ) { <nl> name = " System ROM "; <nl> flags |= IORESOURCE_READONLY ; <nl> - } else { <nl> + } else if ( md -> attribute == EFI_MEMORY_UC ) <nl> + name = " Uncached RAM "; <nl> + else <nl> name = " System RAM "; <nl> - } <nl> break ; <nl>  <nl> case EFI_ACPI_MEMORY_NVS :
bool ieee80211_set_channel_type ( struct ieee80211_local * local , <nl> switch ( tmp -> vif . bss_conf . channel_type ) { <nl> case NL80211_CHAN_NO_HT : <nl> case NL80211_CHAN_HT20 : <nl> + if ( superchan > tmp -> vif . bss_conf . channel_type ) <nl> + break ; <nl> + <nl> superchan = tmp -> vif . bss_conf . channel_type ; <nl> break ; <nl> case NL80211_CHAN_HT40PLUS :
static void tce_buildmulti_pSeriesLP ( struct iommu_table * tbl , long tcenum , <nl> union tce_entry tce , * tcep ; <nl> long l , limit ; <nl>  <nl> - if ( npages == 1 ) <nl> + if ( TCE_PAGE_FACTOR == 0 && npages == 1 ) <nl> return tce_build_pSeriesLP ( tbl , tcenum , npages , uaddr , <nl> direction ); <nl> 
static int ipw_wx_set_retry ( struct net_device * dev , <nl> if (!( wrqu -> retry . flags & IW_RETRY_LIMIT )) <nl> return 0 ; <nl>  <nl> - if ( wrqu -> retry . value < 0 || wrqu -> retry . value > 255 ) <nl> + if ( wrqu -> retry . value < 0 || wrqu -> retry . value >= 255 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& priv -> mutex );
static int get_task_ioprio ( struct task_struct * p ) <nl> if ( ret ) <nl> goto out ; <nl> ret = IOPRIO_PRIO_VALUE ( IOPRIO_CLASS_NONE , IOPRIO_NORM ); <nl> + task_lock ( p ); <nl> if ( p -> io_context ) <nl> ret = p -> io_context -> ioprio ; <nl> + task_unlock ( p ); <nl> out : <nl> return ret ; <nl> }
static int pcmciamtd_config ( struct pcmcia_device * link ) <nl> } <nl> dev_info (& dev -> p_dev -> dev , " mtd % d : % s \ n ", mtd -> index , mtd -> name ); <nl> return 0 ; <nl> - <nl> - dev_err (& dev -> p_dev -> dev , " CS Error , exiting \ n "); <nl> - pcmciamtd_release ( link ); <nl> - return - ENODEV ; <nl> } <nl>  <nl> 
static void mousevsc_on_channel_callback ( void * context ) <nl> static int mousevsc_connect_to_vsp ( struct hv_device * device ) <nl> { <nl> int ret = 0 ; <nl> - int t ; <nl> + unsigned long t ; <nl> struct mousevsc_dev * input_dev = hv_get_drvdata ( device ); <nl> struct mousevsc_prt_msg * request ; <nl> struct mousevsc_prt_msg * response ;
static int kjournald2 ( void * arg ) <nl> goto loop ; <nl>  <nl> end_loop : <nl> - write_unlock (& journal -> j_state_lock ); <nl> del_timer_sync (& journal -> j_commit_timer ); <nl> journal -> j_task = NULL ; <nl> wake_up (& journal -> j_wait_done_commit ); <nl> jbd_debug ( 1 , " Journal thread exiting .\ n "); <nl> + write_unlock (& journal -> j_state_lock ); <nl> return 0 ; <nl> } <nl> 
static int img_prl_out_set_fmt ( struct snd_soc_dai * dai , unsigned int fmt ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + pm_runtime_get_sync ( prl -> dev ); <nl> reg = img_prl_out_readl ( prl , IMG_PRL_OUT_CTL ); <nl> reg = ( reg & ~ IMG_PRL_OUT_CTL_EDGE_MASK ) | control_set ; <nl> img_prl_out_writel ( prl , reg , IMG_PRL_OUT_CTL ); <nl> + pm_runtime_put ( prl -> dev ); <nl>  <nl> return 0 ; <nl> }
static int patch_alc269 ( struct hda_codec * codec ) <nl>  <nl> spec = codec -> spec ; <nl> spec -> gen . shared_mic_vref_pin = 0x18 ; <nl> + codec -> power_save_node = 1 ; <nl>  <nl> snd_hda_pick_fixup ( codec , alc269_fixup_models , <nl> alc269_fixup_tbl , alc269_fixups );
extern int debug_locks_off ( void ); <nl> ({ \ <nl> int __ret = 0 ; \ <nl> \ <nl> - if ( unlikely ( c )) { \ <nl> + if (! oops_in_progress && unlikely ( c )) { \ <nl> if ( debug_locks_off () && ! debug_locks_silent ) \ <nl> WARN_ON ( 1 ); \ <nl> __ret = 1 ; \
static int do_dentry_open ( struct file * f , <nl> return 0 ; <nl>  <nl> cleanup_all : <nl> + if ( WARN_ON_ONCE ( error > 0 )) <nl> + error = - EINVAL ; <nl> fops_put ( f -> f_op ); <nl> if ( f -> f_mode & FMODE_WRITER ) { <nl> put_write_access ( inode );
static int ar9002_hw_calibrate ( struct ath_hw * ah , struct ath9k_channel * chan , <nl> return 0 ; <nl>  <nl> ah -> cal_list_curr = currCal = currCal -> calNext ; <nl> - if ( currCal -> calState == CAL_WAITING ) { <nl> + if ( currCal -> calState == CAL_WAITING ) <nl> ath9k_hw_reset_calibration ( ah , currCal ); <nl> - return 0 ; <nl> - } <nl> + <nl> + return 0 ; <nl> } <nl>  <nl> /* Do NF cal only at longer intervals */
static int check_ptr_alignment ( struct bpf_verifier_env * env , <nl> break ; <nl> case PTR_TO_STACK : <nl> pointer_desc = " stack "; <nl> + /* The stack spill tracking logic in check_stack_write () <nl> + * and check_stack_read () relies on stack accesses being <nl> + * aligned . <nl> + */ <nl> + strict = true ; <nl> break ; <nl> default : <nl> break ;
int crypto_reportstat ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
static void storvsc_handle_error ( struct vmscsi_request * vm_srb , <nl> do_work = true ; <nl> process_err_fn = storvsc_remove_lun ; <nl> break ; <nl> - case ( SRB_STATUS_ABORTED | SRB_STATUS_AUTOSENSE_VALID ): <nl> - if (( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> + case SRB_STATUS_ABORTED : <nl> + if ( vm_srb -> srb_status & SRB_STATUS_AUTOSENSE_VALID && <nl> + ( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> do_work = true ; <nl> process_err_fn = storvsc_device_scan ; <nl> /*
static int l2tp_ip6_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> lsa -> l2tp_family = AF_INET6 ; <nl> lsa -> l2tp_flowinfo = 0 ; <nl> lsa -> l2tp_scope_id = 0 ; <nl> + lsa -> l2tp_unused = 0 ; <nl> if ( peer ) { <nl> if (! lsk -> peer_conn_id ) <nl> return - ENOTCONN ;
static int zfcp_ccw_set_online ( struct ccw_device * ccw_device ) <nl> zfcp_erp_adapter_reopen ( adapter , ZFCP_STATUS_COMMON_ERP_FAILED , 85 , <nl> NULL ); <nl> zfcp_erp_wait ( adapter ); <nl> - goto out ; <nl> + up (& zfcp_data . config_sema ); <nl> + flush_work (& adapter -> scan_work ); <nl> + return 0 ; <nl>  <nl> out_scsi_register : <nl> zfcp_erp_thread_kill ( adapter );
extern void to_tm ( int tim , struct rtc_time * tm ); <nl> extern void tick_broadcast_ipi_handler ( void ); <nl>  <nl> extern void generic_calibrate_decr ( void ); <nl> + extern void hdec_interrupt ( struct pt_regs * regs ); <nl>  <nl> /* Some sane defaults : 125 MHz timebase , 1GHz processor */ <nl> extern unsigned long ppc_proc_freq ;
void exit_io_context ( void ) <nl> ioc -> aic -> exit ( ioc -> aic ); <nl> cfq_exit ( ioc ); <nl>  <nl> - put_io_context ( ioc ); <nl> } <nl> + put_io_context ( ioc ); <nl> } <nl>  <nl> struct io_context * alloc_io_context ( gfp_t gfp_flags , int node )
static int crypt_iv_essiv_ctr ( struct crypt_config * cc , struct dm_target * ti , <nl>  <nl> if ( err ) { <nl> ti -> error = " Error calculating hash in ESSIV "; <nl> + kfree ( salt ); <nl> return err ; <nl> } <nl> 
static int ext2_fill_super ( struct super_block * sb , void * data , int silent ) <nl> if ( EXT2_INODE_SIZE ( sb ) == 0 ) <nl> goto cantfind_ext2 ; <nl> sbi -> s_inodes_per_block = sb -> s_blocksize / EXT2_INODE_SIZE ( sb ); <nl> - if ( sbi -> s_inodes_per_block == 0 ) <nl> + if ( sbi -> s_inodes_per_block == 0 || sbi -> s_inodes_per_group == 0 ) <nl> goto cantfind_ext2 ; <nl> sbi -> s_itb_per_group = sbi -> s_inodes_per_group / <nl> sbi -> s_inodes_per_block ;
int mwifiex_del_mgmt_ies ( struct mwifiex_private * priv ) <nl> ar_ie , & priv -> assocresp_idx ); <nl>  <nl> done : <nl> + kfree ( gen_ie ); <nl> kfree ( beacon_ie ); <nl> kfree ( pr_ie ); <nl> kfree ( ar_ie );
void task_tick_numa ( struct rq * rq , struct task_struct * curr ) <nl> now = curr -> se . sum_exec_runtime ; <nl> period = ( u64 ) curr -> numa_scan_period * NSEC_PER_MSEC ; <nl>  <nl> - if ( now - curr -> node_stamp > period ) { <nl> + if ( now > curr -> node_stamp + period ) { <nl> if (! curr -> node_stamp ) <nl> curr -> numa_scan_period = task_scan_min ( curr ); <nl> curr -> node_stamp += period ;
void snd_seq_device_load_drivers ( void ) <nl> flush_work (& autoload_work ); <nl> } <nl> EXPORT_SYMBOL ( snd_seq_device_load_drivers ); <nl> +# define cancel_autoload_drivers () cancel_work_sync (& autoload_work ) <nl> # else <nl> # define queue_autoload_drivers () /* NOP */ <nl> +# define cancel_autoload_drivers () /* NOP */ <nl> # endif <nl>  <nl> /* <nl> static int snd_seq_device_dev_free ( struct snd_device * device ) <nl> { <nl> struct snd_seq_device * dev = device -> device_data ; <nl>  <nl> + cancel_autoload_drivers (); <nl> put_device (& dev -> dev ); <nl> return 0 ; <nl> }
xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
static inline struct dentry * ovl_lookup_real ( struct dentry * dir , <nl> { <nl> struct dentry * dentry ; <nl>  <nl> - inode_lock ( dir -> d_inode ); <nl> - dentry = lookup_one_len ( name -> name , dir , name -> len ); <nl> - inode_unlock ( dir -> d_inode ); <nl> + dentry = lookup_hash ( name , dir ); <nl>  <nl> if ( IS_ERR ( dentry )) { <nl> if ( PTR_ERR ( dentry ) == - ENOENT )
cpufreq_stat_notifier_trans ( struct notifier_block * nb , unsigned long val , <nl> return 0 ; <nl> } <nl>  <nl> - static int __cpuinit cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> + static int cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> unsigned long action , void * hcpu ) <nl> { <nl> unsigned int cpu = ( unsigned long ) hcpu ;
static int create_fixed_stream_quirk ( struct snd_usb_audio * chip , <nl> } <nl> alts = & iface -> altsetting [ fp -> altset_idx ]; <nl> altsd = get_iface_desc ( alts ); <nl> + if ( altsd -> bNumEndpoints < 1 ) { <nl> + kfree ( fp ); <nl> + kfree ( rate_table ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> fp -> protocol = altsd -> bInterfaceProtocol ; <nl>  <nl> if ( fp -> datainterval == 0 )
static int ax25_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> ax25_address src ; <nl> const unsigned char * mac = skb_mac_header ( skb ); <nl>  <nl> + memset ( sax , 0 , sizeof ( struct full_sockaddr_ax25 )); <nl> ax25_addr_parse ( mac + 1 , skb -> data - mac - 1 , & src , NULL , <nl> & digi , NULL , NULL ); <nl> sax -> sax25_family = AF_AX25 ;
static int log_one_block ( struct log_writes_c * lc , <nl> goto out ; <nl> sector ++; <nl>  <nl> + atomic_inc (& lc -> io_blocks ); <nl> bio = bio_alloc ( GFP_KERNEL , block -> vec_cnt ); <nl> if (! bio ) { <nl> DMERR (" Couldn ' t alloc log bio "); <nl> goto error ; <nl> } <nl> - atomic_inc (& lc -> io_blocks ); <nl> bio -> bi_iter . bi_size = 0 ; <nl> bio -> bi_iter . bi_sector = sector ; <nl> bio -> bi_bdev = lc -> logdev -> bdev ;
static void iwl_req_fw_callback ( const struct firmware * ucode_raw , void * context ) <nl> op -> name , err ); <nl> # endif <nl> } <nl> + kfree ( pieces ); <nl> return ; <nl>  <nl> try_again :
static ssize_t gt_max_freq_mhz_show ( struct device * kdev , struct device_attribute <nl> int ret ; <nl>  <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl> - ret = dev_priv -> rps . hw_max * GT_FREQUENCY_MULTIPLIER ; <nl> + ret = dev_priv -> rps . max_delay * GT_FREQUENCY_MULTIPLIER ; <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> return snprintf ( buf , PAGE_SIZE , "% d \ n ", ret );
static int do_prlimit ( struct task_struct * tsk , unsigned int resource , <nl>  <nl> if ( resource >= RLIM_NLIMITS ) <nl> return - EINVAL ; <nl> + resource = array_index_nospec ( resource , RLIM_NLIMITS ); <nl> + <nl> if ( new_rlim ) { <nl> if ( new_rlim -> rlim_cur > new_rlim -> rlim_max ) <nl> return - EINVAL ;
static int ade7854_set_irq ( struct device * dev , bool enable ) <nl> else <nl> irqen &= ~ BIT ( 17 ); <nl>  <nl> - ret = st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> - <nl> - return ret ; <nl> + return st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> } <nl>  <nl> static int ade7854_initial_setup ( struct iio_dev * indio_dev )
static int __devinit snd_hdspm_create ( struct snd_card * card , <nl> hdspm -> io_type = AES32 ; <nl> hdspm -> card_name = " RME AES32 "; <nl> hdspm -> midiPorts = 2 ; <nl> - } else if (( hdspm -> firmware_rev == 0xd5 ) || <nl> + } else if (( hdspm -> firmware_rev == 0xd2 ) || <nl> (( hdspm -> firmware_rev >= 0xc8 ) && <nl> ( hdspm -> firmware_rev <= 0xcf ))) { <nl> hdspm -> io_type = MADI ;
int snd_midi_event_encode_byte ( struct snd_midi_event * dev , int c , <nl> ev -> type = status_event [ ST_SPECIAL + c - 0xf0 ]. event ; <nl> ev -> flags &= ~ SNDRV_SEQ_EVENT_LENGTH_MASK ; <nl> ev -> flags |= SNDRV_SEQ_EVENT_LENGTH_FIXED ; <nl> - return 1 ; <nl> + return ev -> type != SNDRV_SEQ_EVENT_NONE ; <nl> } <nl>  <nl> spin_lock_irqsave (& dev -> lock , flags );
# include " viosrp . h " <nl>  <nl> # define IBMVFC_NAME " ibmvfc " <nl> -# define IBMVFC_DRIVER_VERSION " 1 . 0 . 3 " <nl> -# define IBMVFC_DRIVER_DATE "( October 28 , 2008 )" <nl> +# define IBMVFC_DRIVER_VERSION " 1 . 0 . 4 " <nl> +# define IBMVFC_DRIVER_DATE "( November 14 , 2008 )" <nl>  <nl> # define IBMVFC_DEFAULT_TIMEOUT 15 <nl> # define IBMVFC_INIT_TIMEOUT 30
struct inode { <nl> struct timespec i_atime ; <nl> struct timespec i_mtime ; <nl> struct timespec i_ctime ; <nl> - unsigned int i_blkbits ; <nl> blkcnt_t i_blocks ; <nl> + unsigned int i_blkbits ; <nl> unsigned short i_bytes ; <nl> umode_t i_mode ; <nl> spinlock_t i_lock ; /* i_blocks , i_bytes , maybe i_size */
void btrfs_destroy_workqueue ( struct btrfs_workqueue * wq ) <nl> if ( wq -> high ) <nl> __btrfs_destroy_workqueue ( wq -> high ); <nl> __btrfs_destroy_workqueue ( wq -> normal ); <nl> + kfree ( wq ); <nl> } <nl>  <nl> void btrfs_workqueue_set_max ( struct btrfs_workqueue * wq , int max )
int seq_buf_putmem_hex ( struct seq_buf * s , const void * mem , <nl>  <nl> WARN_ON ( s -> size == 0 ); <nl>  <nl> + BUILD_BUG_ON ( MAX_MEMHEX_BYTES * 2 >= HEX_CHARS ); <nl> + <nl> while ( len ) { <nl> - start_len = min ( len , HEX_CHARS - 1 ); <nl> + start_len = min ( len , MAX_MEMHEX_BYTES ); <nl> # ifdef __BIG_ENDIAN <nl> for ( i = 0 , j = 0 ; i < start_len ; i ++) { <nl> # else
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
static int qe_ep_enable ( struct usb_ep * _ep , <nl> ep = container_of ( _ep , struct qe_ep , ep ); <nl>  <nl> /* catch various bogus parameters */ <nl> - if (! _ep || ! desc || ep -> ep . desc || _ep -> name == ep_name [ 0 ] || <nl> + if (! _ep || ! desc || _ep -> name == ep_name [ 0 ] || <nl> ( desc -> bDescriptorType != USB_DT_ENDPOINT )) <nl> return - EINVAL ; <nl> 
static void set_truncation ( <nl> REG_UPDATE_3 ( FMT_BIT_DEPTH_CONTROL , <nl> FMT_TRUNCATE_EN , 1 , <nl> FMT_TRUNCATE_DEPTH , <nl> - params -> flags . TRUNCATE_MODE , <nl> + params -> flags . TRUNCATE_DEPTH , <nl> FMT_TRUNCATE_MODE , <nl> - params -> flags . TRUNCATE_DEPTH ); <nl> + params -> flags . TRUNCATE_MODE ); <nl> } <nl>  <nl> 
static int ptlrpc_main ( void * arg ) <nl> /* Process all incoming reqs before handling any */ <nl> if ( ptlrpc_server_request_incoming ( svcpt )) { <nl> lu_context_enter (& env -> le_ctx ); <nl> + env -> le_ses = NULL ; <nl> ptlrpc_server_handle_req_in ( svcpt , thread ); <nl> lu_context_exit (& env -> le_ctx ); <nl> 
int inet6_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl> */ <nl> v4addr = LOOPBACK4_IPV6 ; <nl> if (!( addr_type & IPV6_ADDR_MULTICAST )) { <nl> - if (! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> + if (! inet -> transparent && <nl> + ! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> dev , 0 )) { <nl> err = - EADDRNOTAVAIL ; <nl> goto out_unlock ;
struct tracepoint { <nl> do { \ <nl> void ** it_func ; \ <nl> \ <nl> - rcu_read_lock_sched (); \ <nl> + rcu_read_lock_sched_notrace (); \ <nl> it_func = rcu_dereference (( tp )-> funcs ); \ <nl> if ( it_func ) { \ <nl> do { \ <nl> (( void (*)( proto ))(* it_func ))( args ); \ <nl> } while (*(++ it_func )); \ <nl> } \ <nl> - rcu_read_unlock_sched (); \ <nl> + rcu_read_unlock_sched_notrace (); \ <nl> } while ( 0 ) <nl>  <nl> /*
int dm_kcopyd_copy ( struct dm_kcopyd_client * kc , struct dm_io_region * from , <nl> job -> fn = fn ; <nl> job -> context = context ; <nl>  <nl> - if ( job -> source . count < SUB_JOB_SIZE ) <nl> + if ( job -> source . count <= SUB_JOB_SIZE ) <nl> dispatch_job ( job ); <nl>  <nl> else {
void ath10k_htt_tx_free ( struct ath10k_htt * htt ) <nl> { <nl> int size ; <nl>  <nl> + tasklet_kill (& htt -> txrx_compl_task ); <nl> + <nl> idr_for_each (& htt -> pending_tx , ath10k_htt_tx_clean_up_pending , htt -> ar ); <nl> idr_destroy (& htt -> pending_tx ); <nl> 
int __init igafb_init ( void ) <nl> iounmap ( info -> screen_base ); <nl> kfree ( par -> mmap_map ); <nl> kfree ( info ); <nl> + return - ENODEV ; <nl> } <nl>  <nl> # ifdef CONFIG_SPARC
static int onenand_write_ops_nolock ( struct mtd_info * mtd , loff_t to , <nl> } <nl>  <nl> /* Only check verify write turn on */ <nl> - ret = onenand_verify ( mtd , ( u_char *) wbuf , to , thislen ); <nl> + ret = onenand_verify ( mtd , buf , to , thislen ); <nl> if ( ret ) { <nl> printk ( KERN_ERR " onenand_write_ops_nolock : verify failed % d \ n ", ret ); <nl> break ;
static int beagle_twl_gpio_setup ( struct device * dev , <nl>  <nl> /* TWL4030_GPIO_MAX + 0 == ledA , EHCI nEN_USB_PWR ( out , active low ) */ <nl> gpio_request ( gpio + TWL4030_GPIO_MAX , " nEN_USB_PWR "); <nl> - gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 1 ); <nl> + gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 0 ); <nl>  <nl> /* TWL4030_GPIO_MAX + 1 == ledB , PMU_STAT ( out , active low LED ) */ <nl> gpio_leds [ 2 ]. gpio = gpio + TWL4030_GPIO_MAX + 1 ;
struct vm_area_struct * vb2_get_vma ( struct vm_area_struct * vma ) <nl> { <nl> struct vm_area_struct * vma_copy ; <nl>  <nl> - vma_copy = kmalloc ( sizeof (* vma_copy ), GFP_KERNEL ); <nl> + vma_copy = kmem_cache_alloc ( vm_area_cachep , GFP_KERNEL ); <nl> if ( vma_copy == NULL ) <nl> return NULL ; <nl>  <nl> void vb2_put_vma ( struct vm_area_struct * vma ) <nl> if ( vma -> vm_file ) <nl> fput ( vma -> vm_file ); <nl>  <nl> - kfree ( vma ); <nl> + kmem_cache_free ( vm_area_cachep , vma ); <nl> } <nl> EXPORT_SYMBOL_GPL ( vb2_put_vma ); <nl> 
static int yam_siocdevprivate ( struct net_device * dev , struct ifreq * ifr , void __ <nl> ym = memdup_user ( data , sizeof ( struct yamdrv_ioctl_mcs )); <nl> if ( IS_ERR ( ym )) <nl> return PTR_ERR ( ym ); <nl> - if ( ym -> cmd != SIOCYAMSMCS ) <nl> - return - EINVAL ; <nl> - if ( ym -> bitrate > YAM_MAXBITRATE ) { <nl> + if ( ym -> cmd != SIOCYAMSMCS || ym -> bitrate > YAM_MAXBITRATE ) { <nl> kfree ( ym ); <nl> return - EINVAL ; <nl> }
static int snd_timer_user_tselect ( struct file * file , <nl> if ( err < 0 ) <nl> goto __err ; <nl>  <nl> + tu -> qhead = tu -> qtail = tu -> qused = 0 ; <nl> kfree ( tu -> queue ); <nl> tu -> queue = NULL ; <nl> kfree ( tu -> tqueue );
struct platform_device * __init imx_add_mxc_mmc ( <nl> struct resource res [] = { <nl> { <nl> . start = data -> iobase , <nl> - . end = data -> iobase + SZ_4K - 1 , <nl> + . end = data -> iobase + data -> iosize - 1 , <nl> . flags = IORESOURCE_MEM , <nl> }, { <nl> . start = data -> irq ,
static int tile_net_poll ( struct napi_struct * napi , int budget ) <nl> struct info_mpipe * info_mpipe = <nl> container_of ( napi , struct info_mpipe , napi ); <nl>  <nl> + if ( budget <= 0 ) <nl> + goto done ; <nl> + <nl> instance = info_mpipe -> instance ; <nl> while (( n = gxio_mpipe_iqueue_try_peek ( <nl> & info_mpipe -> iqueue ,
static int acl_permission_check ( struct inode * inode , int mask ) <nl> if ( current_user_ns () != inode_userns ( inode )) <nl> goto other_perms ; <nl>  <nl> - if ( current_fsuid () == inode -> i_uid ) <nl> + if ( likely ( current_fsuid () == inode -> i_uid )) <nl> mode >>= 6 ; <nl> else { <nl> if ( IS_POSIXACL ( inode ) && ( mode & S_IRWXG )) {
static netdev_tx_t xgene_enet_start_xmit ( struct sk_buff * skb , <nl> return NETDEV_TX_OK ; <nl> } <nl>  <nl> - pdata -> ring_ops -> wr_cmd ( tx_ring , count ); <nl> skb_tx_timestamp ( skb ); <nl>  <nl> pdata -> stats . tx_packets ++; <nl> pdata -> stats . tx_bytes += skb -> len ; <nl>  <nl> + pdata -> ring_ops -> wr_cmd ( tx_ring , count ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
static void __init_discard_policy ( struct f2fs_sb_info * sbi , <nl> dpolicy -> min_interval = DEF_MIN_DISCARD_ISSUE_TIME ; <nl> dpolicy -> max_interval = DEF_MAX_DISCARD_ISSUE_TIME ; <nl> dpolicy -> io_aware = true ; <nl> + dpolicy -> sync = false ; <nl> if ( utilization ( sbi ) > DEF_DISCARD_URGENT_UTIL ) { <nl> dpolicy -> granularity = 1 ; <nl> dpolicy -> max_interval = DEF_MIN_DISCARD_ISSUE_TIME ;
static void nvmet_execute_identify_ctrl ( struct nvmet_req * req ) <nl> id -> vid = 0 ; <nl> id -> ssvid = 0 ; <nl>  <nl> + memset ( id -> sn , ' ', sizeof ( id -> sn )); <nl> bin2hex ( id -> sn , & ctrl -> subsys -> serial , <nl> min ( sizeof ( ctrl -> subsys -> serial ), sizeof ( id -> sn ) / 2 )); <nl> memcpy_and_pad ( id -> mn , sizeof ( id -> mn ), model , sizeof ( model ) - 1 , ' ');
static struct obstack obstack_for_string ; <nl> # define STRING_1GROW ( Char ) \ <nl> obstack_1grow (& obstack_for_string , Char ) <nl>  <nl> -# define STRING_FREE () \ <nl> +# ifdef NDEBUG <nl> +# define STRING_FREE () \ <nl> obstack_free (& obstack_for_string , last_string ) <nl> +# else <nl> +# define STRING_FREE () \ <nl> + do { \ <nl> + obstack_free (& obstack_for_string , last_string ); \ <nl> + last_string = NULL ; \ <nl> + } while ( 0 ) <nl> +# endif <nl>  <nl> # endif
MultiPartInputFile :: initialize () <nl> // Perform usual check on headers . <nl> // <nl>  <nl> + if ( _data -> _headers . size () == 0 ) <nl> + { <nl> + throw IEX_NAMESPACE :: ArgExc (" Files must contain at least one header "); <nl> + } <nl> + <nl> for ( size_t i = 0 ; i < _data -> _headers . size (); i ++) <nl> { <nl> //
B44Compressor :: B44Compressor <nl> // <nl>  <nl> _tmpBuffer = new unsigned short <nl> - [ checkArraySize ( uiMult ( maxScanLineSize , numScanLines ), <nl> + [ checkArraySize ( uiMult ( maxScanLineSize / sizeof ( unsigned short ), numScanLines ), <nl> sizeof ( unsigned short ))]; <nl>  <nl> const ChannelList & channels = header (). channels ();
DU_getStringDOElement ( DcmItem * obj , DcmTagKey t , char * s , size_t bufsize ) <nl> s [ 0 ] = '\ 0 '; <nl> } else { <nl> ec = elem -> getString ( aString ); <nl> - OFStandard :: strlcpy ( s , aString , bufsize ); <nl> + if ( ec == EC_Normal ) <nl> + OFStandard :: strlcpy ( s , aString , bufsize ); <nl> } <nl> } <nl> return ( ec == EC_Normal );
BaseType_t xQueueGenericReset ( QueueHandle_t xQueue , <nl> /* Check for multiplication overflow . */ <nl> configASSERT ( ( uxItemSize == 0 ) || ( uxQueueLength == ( xQueueSizeInBytes / uxItemSize ) ) ); <nl>  <nl> + /* Check for addition overflow . */ <nl> + configASSERT ( ( sizeof ( Queue_t ) + xQueueSizeInBytes ) > xQueueSizeInBytes ); <nl> + <nl> /* Allocate the queue and storage area . Justification for MISRA <nl> * deviation as follows : pvPortMalloc () always ensures returned memory <nl> * blocks are aligned per the requirements of the MCU stack . In this case
fribidi_cap_rtl_to_unicode ( <nl> } <nl> } <nl> else <nl> - us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + { <nl> + if (( int ) s [ i ] < 0 ) <nl> + us [ j ++] = '?'; <nl> + else <nl> + us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + } <nl> } <nl>  <nl> return j ;
_gnutls_x509_dn_to_string ( const char * oid , void * value , <nl> if ( ret < 0 ) { <nl> gnutls_assert (); <nl> gnutls_free ( str -> data ); <nl> + str -> data = NULL ; <nl> return ret ; <nl> } <nl> str -> size = size ;
int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl>  <nl> if ( ret < 0 && ret != GNUTLS_E_REQUESTED_DATA_NOT_AVAILABLE ) { <nl> gnutls_assert (); <nl> - gnutls_free ( san . data ); <nl> goto cleanup ; <nl> } <nl> 
lzw_result lzw_decode ( struct lzw_ctx * ctx , <nl> /* Code is invalid */ <nl> return LZW_BAD_CODE ; <nl>  <nl> + } else if ( code_new >= 1 << LZW_CODE_MAX ) { <nl> + /* Don ' t access out of bound */ <nl> + return LZW_BAD_CODE ; <nl> + <nl> } else if ( code_new < current_entry ) { <nl> /* Code is in table */ <nl> code_out = code_new ;
static int stszin ( int size ) <nl> u32in (); <nl> // Number of entries <nl> mp4config . frame . ents = u32in (); <nl> - // fixme : check atom size <nl> + <nl> + if (!( mp4config . frame . ents + 1 )) <nl> + return ERR_FAIL ; <nl> + <nl> mp4config . frame . data = malloc ( sizeof (* mp4config . frame . data ) <nl> * ( mp4config . frame . ents + 1 )); <nl> 
 <nl> # include " idn2 . h " <nl>  <nl> +# include < sys / types . h > <nl> # include < stdbool . h > <nl>  <nl> # include " bidi . h " <nl> static bool <nl> _isBidi ( const uint32_t * label , size_t llen ) <nl> { <nl> - while ( llen -- > 0 ) { <nl> + for (; ( ssize_t ) llen > 0 ; llen --) { <nl> int bc = uc_bidi_category (* label ++); <nl>  <nl> if ( bc == UC_BIDI_R || bc == UC_BIDI_AL || bc == UC_BIDI_AN )
int secure_decrypt ( void * data , unsigned int data_length , int is_signed ) <nl> /* Check the CMAC */ <nl> fixed_length = at91_aes_roundup ( data_length ); <nl> cmac = ( const unsigned int *)(( char *) data + fixed_length ); <nl> - if ( memcmp ( cmac , computed_cmac , AT91_AES_BLOCK_SIZE_BYTE )) <nl> + if (! consttime_memequal ( cmac , computed_cmac , AT91_AES_BLOCK_SIZE_BYTE )) <nl> goto exit ; <nl> } <nl> 
static void ProcessRadioRxDone ( void ) <nl> } <nl> } <nl>  <nl> + // Abort on empty radio frames <nl> + if ( size == 0 ) <nl> + { <nl> + MacCtx . McpsIndication . Status = LORAMAC_EVENT_INFO_STATUS_ERROR ; <nl> + PrepareRxDoneAbort ( ); <nl> + return ; <nl> + } <nl> + <nl> macHdr . Value = payload [ pktHeaderLen ++]; <nl>  <nl> // Accept frames of LoRaWAN Major Version 1 only
parserule ( struct scanner * s , struct environment * env ) <nl> var = scanname ( s ); <nl> parselet ( s , & val ); <nl> ruleaddvar ( r , var , val ); <nl> + if (! val ) <nl> + continue ; <nl> if ( strcmp ( var , " command ") == 0 ) <nl> hascommand = true ; <nl> else if ( strcmp ( var , " rspfile ") == 0 )
pspdf_prepare_outpages () <nl> chapter_outstarts [ c ] = num_outpages ; <nl>  <nl> for ( i = chapter_starts [ c ], j = 0 , nup = - 1 , page = pages + i ; <nl> - i <= chapter_ends [ c ]; <nl> + i <= chapter_ends [ c ] && num_outpages < num_pages ; <nl> i ++, page ++) <nl> { <nl> if ( nup != page -> nup )
static int cmd_handle_untagged ( IMAP_DATA * idata ) <nl> dprint ( 2 , ( debugfile , " Handling untagged NO \ n ")); <nl>  <nl> /* Display the warning message from the server */ <nl> - mutt_error ("% s ", s + 3 ); <nl> + mutt_error ("% s ", s + 2 ); <nl> mutt_sleep ( 2 ); <nl> } <nl> 
static int cbor2json ( OSCTXT * pCborCtxt , OSCTXT * pJsonCtxt ) <nl> case OSRTCBOR_UTF8STR : { <nl> OSUTF8CHAR * utf8str ; <nl> ret = rtCborDecDynUTF8Str ( pCborCtxt , ub , ( char **)& utf8str ); <nl> + if ( 0 != ret ) return LOG_RTERR ( pCborCtxt , ret ); <nl>  <nl> ret = rtJsonEncStringValue ( pJsonCtxt , utf8str ); <nl> rtxMemFreePtr ( pCborCtxt , utf8str );
njs_module_path ( njs_vm_t * vm , const njs_str_t * dir , njs_module_info_t * info ) <nl> length = info -> name . length ; <nl>  <nl> if ( dir != NULL ) { <nl> - length = dir -> length ; <nl> + length += dir -> length ; <nl>  <nl> if ( length == 0 ) { <nl> return NJS_DECLINED ;
njs_function_frame_save ( njs_vm_t * vm , njs_frame_t * frame , u_char * pc ) <nl> njs_native_frame_t * active , * native ; <nl>  <nl> * frame = * vm -> active_frame ; <nl> + <nl> frame -> previous_active_frame = NULL ; <nl>  <nl> native = & frame -> native ; <nl> + native -> size = 0 ; <nl> + native -> free = NULL ; <nl> + native -> free_size = 0 ; <nl>  <nl> active = & vm -> active_frame -> native ; <nl> value_count = njs_function_frame_value_count ( active );
static char * clean_path ( char * path ) <nl> char * ch ; <nl> char * ch2 ; <nl> char * str ; <nl> - str = xmalloc ( strlen ( path )); <nl> + str = xmalloc ( strlen ( path ) + 1 ); <nl> ch = path ; <nl> ch2 = str ; <nl> while ( true ) {
int delete_sdp_line ( struct sip_msg * msg , char * s , struct sdp_stream_cell * str <nl>  <nl> while (* end != '\ n ' && end < ( stream -> body . s + stream -> body . len ) ) <nl> end ++; <nl> - end ++; <nl> + if ( * end == '\ n ') <nl> + end ++; <nl>  <nl> /* delete the entry */ <nl> if ( del_lump ( msg , start - msg -> buf , end - start , 0 ) == NULL )
static int stream_process ( struct sip_msg * msg , struct sdp_stream_cell * cell , <nl> /* when trimming the very last payload , avoid trailing ws */ <nl> if ( cur == lmp -> u . value + lmp -> len ) { <nl> tmp = found . s ; <nl> - while (*(-- tmp ) == ' ') { <nl> + while ( tmp > lmp -> u . value && *(-- tmp ) == ' ') { <nl> found . s --; <nl> found . len ++; <nl> }
OGRKMLLayer :: OGRKMLLayer ( const char * pszName , <nl> if ( poSRSIn != nullptr ) <nl> { <nl> poSRS_ -> SetWellKnownGeogCS ( " WGS84 " ); <nl> + poSRS_ -> SetAxisMappingStrategy ( OAMS_TRADITIONAL_GIS_ORDER ); <nl> if ( ! poSRS_ -> IsSame ( poSRSIn ) ) <nl> { <nl> poCT_ = OGRCreateCoordinateTransformation ( poSRSIn , poSRS_ );
IMPEG2D_ERROR_CODES_T impeg2d_dec_p_b_slice ( dec_state_t * ps_dec ) <nl>  <nl> if ( ret ) <nl> return IMPEG2D_MB_TEX_DECODE_ERR ; <nl> + <nl> + if ( 0 >= ps_dec -> u2_num_mbs_left ) <nl> + { <nl> + break ; <nl> + } <nl> + <nl> IMPEG2D_TRACE_MB_START ( ps_dec -> u2_mb_x , ps_dec -> u2_mb_y ); <nl>  <nl> u4_x_dst_offset = u4_frm_offset + ( ps_dec -> u2_mb_x << 4 );
WORD32 ih264d_decode_gaps_in_frame_num ( dec_struct_t * ps_dec , <nl>  <nl> ps_cur_slice = ps_dec -> ps_cur_slice ; <nl> ps_pic_params = ps_dec -> ps_cur_pps ; <nl> - ps_cur_slice -> u1_field_pic_flag = 0 ; <nl>  <nl> i4_frame_gaps = 0 ; <nl> ps_dpb_mgr = ps_dec -> ps_dpb_mgr ;
WORD32 ih264d_video_decode ( iv_obj_t * dec_hdl , void * pv_api_ip , void * pv_api_op ) <nl> else <nl> prev_slice_err = 2 ; <nl>  <nl> + if ( ps_dec -> u4_first_slice_in_pic && ( ps_dec -> u2_total_mbs_coded == 0 )) <nl> + prev_slice_err = 1 ; <nl> + <nl> ret1 = ih264d_mark_err_slice_skip ( ps_dec , num_mb_skipped , ps_dec -> u1_nal_unit_type == IDR_SLICE_NAL , ps_dec -> ps_cur_slice -> u2_frame_num , <nl> & temp_poc , prev_slice_err ); <nl> 
WORD32 ih264d_process_intra_mb ( dec_struct_t * ps_dec , <nl> + ( pu1_rem_intra4x4_pred_mode [ u1_cur_sub_block ] <nl> >= i1_intra_pred ); <nl> } <nl> + i1_intra_pred = CLIP3 ( 0 , 8 , i1_intra_pred ); <nl> { <nl> UWORD8 u1_err_code = pu1_intra_err_codes [ i1_intra_pred ]; <nl>  <nl> WORD32 ih264d_process_intra_mb ( dec_struct_t * ps_dec , <nl> + ( pu1_rem_intra4x4_pred_mode [ u1_sub_mb_num ] <nl> >= i1_intra_pred ); <nl> } <nl> + i1_intra_pred = CLIP3 ( 0 , 8 , i1_intra_pred ); <nl> { <nl> UWORD8 u1_err_code = pu1_intra_err_codes [ i1_intra_pred ]; <nl> 
WORD32 ih264d_start_of_pic ( dec_struct_t * ps_dec , <nl> ps_cur_pic -> pu1_col_zero_flag = ( UWORD8 *) ps_col_mv -> pv_col_zero_flag ; <nl> ps_cur_pic -> ps_mv = ( mv_pred_t *) ps_col_mv -> pv_mv ; <nl> ps_dec -> au1_pic_buf_ref_flag [ cur_pic_buf_id ] = 0 ; <nl> - if ( ps_dec -> u1_first_slice_in_stream ) <nl> + <nl> { <nl> /* make first entry of list0 point to cur pic , so that if first Islice is in error , ref pic struct will have valid entries */ <nl> ps_dec -> ps_ref_pic_buf_lx [ 0 ] = ps_dec -> ps_dpb_mgr -> ps_init_dpb [ 0 ];
status_t BnGraphicBufferConsumer :: onTransact ( <nl> CHECK_INTERFACE ( IGraphicBufferConsumer , data , reply ); <nl> sp < GraphicBuffer > buffer = new GraphicBuffer (); <nl> data . read (* buffer . get ()); <nl> - int slot ; <nl> + int slot = - 1 ; <nl> int result = attachBuffer (& slot , buffer ); <nl> reply -> writeInt32 ( slot ); <nl> reply -> writeInt32 ( result );
vq_endchains ( struct virtio_vq_info * vq , int used_all_avail ) <nl> uint16_t event_idx , new_idx , old_idx ; <nl> int intr ; <nl>  <nl> + if (! vq || ! vq -> used ) <nl> + return ; <nl> + <nl> /* <nl> * Interrupt generation : if we ' re using EVENT_IDX , <nl> * interrupt if we ' ve crossed the event threshold .
uint8_t rfc_parse_data ( tRFC_MCB * p_mcb , MX_FRAME * p_frame , BT_HDR * p_buf ) { <nl>  <nl> eal = *( p_data )& RFCOMM_EA ; <nl> len = *( p_data )++ >> RFCOMM_SHIFT_LENGTH1 ; <nl> - if ( eal == 0 && p_buf -> len < RFCOMM_CTRL_FRAME_LEN ) { <nl> + if ( eal == 0 && p_buf -> len > RFCOMM_CTRL_FRAME_LEN ) { <nl> len += (*( p_data )++ << RFCOMM_SHIFT_LENGTH2 ); <nl> } else if ( eal == 0 ) { <nl> RFCOMM_TRACE_ERROR (" Bad Length when EAL = 0 : % d ", p_buf -> len );
int CGIFFF DGifSlurp ( CGIFFF GifFileType * GifFile ) <nl> memcpy ( ep -> Bytes , ExtData , ep -> ByteCount * sizeof ( char )); <nl> # else <nl> if ( ExtData == NULL ) break ; <nl> - AddExtensionBlock ( sp , ExtData [ 0 ], ExtData + 1 ); <nl> + AddExtensionBlock (& ext , ExtData [ 0 ], ExtData + 1 ); <nl> + ext . ExtensionBlocks [ ext . ExtensionBlockCount - 1 ]. code = ext_code ; <nl> # endif <nl> } <nl> }
void NuPlayer :: GenericSource :: notifyPreparedAndCleanup ( status_t err ) { <nl> { <nl> Mutex :: Autolock _l ( mDisconnectLock ); <nl> mDataSource . clear (); <nl> + mDrmManagerClient = NULL ; <nl> mCachedSource . clear (); <nl> mHttpSource . clear (); <nl> }
int64_t NuPlayer :: GenericSource :: getLastReadPosition () { <nl>  <nl> status_t NuPlayer :: GenericSource :: setBuffers ( <nl> bool audio , Vector < MediaBuffer *> & buffers ) { <nl> - if ( mIsWidevine && ! audio && mVideoTrack . mSource != NULL ) { <nl> + if ( mIsSecure && ! audio && mVideoTrack . mSource != NULL ) { <nl> return mVideoTrack . mSource -> setBuffers ( buffers ); <nl> } <nl> return INVALID_OPERATION ;
status_t OMXNodeInstance :: allocateBufferWithBackup ( <nl> } <nl>  <nl> CHECK_EQ ( header -> pAppPrivate , buffer_meta ); <nl> - memset ( header -> pBuffer , 0 , header -> nAllocLen ); <nl>  <nl> * buffer = makeBufferID ( header ); <nl> 
int modbus_reply ( modbus_t * ctx , const uint8_t * req , <nl> nb_write , nb , MODBUS_MAX_WR_WRITE_REGISTERS , MODBUS_MAX_WR_READ_REGISTERS ); <nl> } else if ( mapping_address < 0 || <nl> ( mapping_address + nb ) > mb_mapping -> nb_registers || <nl> - mapping_address < 0 || <nl> + mapping_address_write < 0 || <nl> ( mapping_address_write + nb_write ) > mb_mapping -> nb_registers ) { <nl> rsp_length = response_exception ( <nl> ctx , & sft , MODBUS_EXCEPTION_ILLEGAL_DATA_ADDRESS , rsp , FALSE ,
static int MP4_ReadBox_String ( stream_t * p_stream , MP4_Box_t * p_box ) <nl> { <nl> MP4_READBOX_ENTER ( MP4_Box_data_string_t ); <nl>  <nl> + if ( p_box -> i_size < 8 || p_box -> i_size > SIZE_MAX ) <nl> + MP4_READBOX_EXIT ( 0 ); <nl> + <nl> p_box -> data . p_string -> psz_text = malloc ( p_box -> i_size + 1 - 8 ); /* +\ 0 , - name , - size */ <nl> if ( p_box -> data . p_string -> psz_text == NULL ) <nl> MP4_READBOX_EXIT ( 0 );
static bool GetUpdateFile ( update_t * p_update ) <nl> } <nl>  <nl> const int64_t i_read = stream_Size ( p_stream ); <nl> + <nl> + if ( i_read < 0 || i_read >= UINT16_MAX ) <nl> + { <nl> + msg_Err ( p_update -> p_libvlc , " Status file too large "); <nl> + goto error ; <nl> + } <nl> + <nl> psz_update_data = malloc ( i_read + 1 ); /* terminating '\ 0 ' */ <nl> if ( ! psz_update_data ) <nl> goto error ;
static block_t * Encode ( encoder_t * p_enc , picture_t * p_pic ) <nl> * is appended to the sequence header to allow guard <nl> * against poor streaming servers */ <nl> /* XXX , should this be done using the packetizer ? */ <nl> + <nl> + if ( len > UINT32_MAX - sizeof ( eos ) ) <nl> + return NULL ; <nl> + <nl> p_enc -> fmt_out . p_extra = malloc ( len + sizeof ( eos ) ); <nl> if ( ! p_enc -> fmt_out . p_extra ) <nl> return NULL ;
class c_single_allocator <nl>  <nl> std :: size_t size () const { return _buf_size ; } <nl>  <nl> - void resize ( std :: size_t new_size_ ) { _buf_size = new_size_ ; } <nl> + // This buffer is fixed , size must not be changed <nl> + void resize ( std :: size_t new_size_ ) { LIBZMQ_UNUSED ( new_size_ ); } <nl>  <nl> private : <nl> std :: size_t _buf_size ;
static int oidc_request_post_preserved_restore ( request_rec * r , <nl> " input . type = \" hidden \";\ n " <nl> " document . forms [ 0 ]. appendChild ( input );\ n " <nl> " }\ n " <nl> - " document . forms [ 0 ]. action = '% s ';\ n " <nl> + " document . forms [ 0 ]. action = \"% s \";\ n " <nl> " document . forms [ 0 ]. submit ();\ n " <nl> " }\ n " <nl> " </ script >\ n ", method , original_url );
int init_result ( RESULT & result , void *& data ) { <nl> log_messages . printf ( MSG_DEBUG , " Check result \ n "); <nl>  <nl> char buff [ 256 ]; <nl> - n = fscanf ( f , "% s ", buff ); <nl> + // n = fscanf ( f , "% s ", buff ); <nl> + fgets ( buff , 256 , f ); <nl> char * pch ; <nl> pch = strtok ( buff , " ,"); <nl> if ( pch != NULL ) {
void csync_daemon_session () <nl> goto conn_without_ssl_ok ; <nl> } <nl> cmd_error = conn_response ( CR_ERR_SSL_EXPECTED ); <nl> + peer = NULL ; <nl> } <nl> conn_without_ssl_ok :; <nl> # endif
resolve_iffeature ( struct lys_iffeature * expr ) <nl> { <nl> int index_e = 0 , index_f = 0 ; <nl>  <nl> - if ( expr -> expr ) { <nl> + if ( expr -> expr && expr -> features [ 0 ]) { <nl> return resolve_iffeature_recursive ( expr , & index_e , & index_f ); <nl> } <nl> return 0 ;
TfLiteStatus ExpandTensorDim ( TfLiteContext * context , const TfLiteTensor & input , <nl> axis = input_dims . size + 1 + axis ; <nl> } <nl> TF_LITE_ENSURE ( context , axis <= input_dims . size ); <nl> + TF_LITE_ENSURE ( context , axis >= 0 ); <nl>  <nl> TfLiteIntArray * output_dims = TfLiteIntArrayCreate ( input_dims . size + 1 ); <nl> for ( int i = 0 ; i < output_dims -> size ; ++ i ) {
int MatchingArraySize ( const ArrayType1 & array1 , int index1 , <nl> inline int MatchingDim ( const RuntimeShape & shape1 , int index1 , <nl> const RuntimeShape & shape2 , int index2 ) { <nl> TFLITE_DCHECK_EQ ( shape1 . Dims ( index1 ), shape2 . Dims ( index2 )); <nl> - return shape1 . Dims ( index1 ); <nl> + return std :: min ( shape1 . Dims ( index1 ), shape2 . Dims ( index2 )); <nl> } <nl>  <nl> template < typename ... Args >
TfLiteStatus ResizeOutput ( TfLiteContext * context , const TfLiteTensor * input , <nl> axis_value += NumDimensions ( input ); <nl> } <nl>  <nl> + TF_LITE_ENSURE ( context , axis_value >= 0 ); <nl> + TF_LITE_ENSURE ( context , axis_value < NumDimensions ( input )); <nl> + <nl> // Copy the input dimensions to output except the axis dimension . <nl> TfLiteIntArray * output_dims = TfLiteIntArrayCreate ( NumDimensions ( input ) - 1 ); <nl> int j = 0 ;
TfLiteStatus Prepare ( TfLiteContext * context , TfLiteNode * node ) { <nl>  <nl> TF_LITE_ENSURE_EQ ( context , NumDimensions ( input ), 4 ); <nl> TF_LITE_ENSURE_EQ ( context , NumDimensions ( filter ), 4 ); <nl> + TF_LITE_ENSURE ( context , params -> dilation_height_factor > 0 ); <nl> + TF_LITE_ENSURE ( context , params -> dilation_width_factor > 0 ); <nl>  <nl> const TfLiteType data_type = input -> type ; <nl> 
class RaggedTensorToVariantOp : public OpKernel { <nl> batched_ragged_input . mutable_nested_splits ()-> reserve ( <nl> ragged_nested_splits_len ); <nl> for ( int i = 0 ; i < ragged_nested_splits_len ; i ++) { <nl> + OP_REQUIRES ( context , ragged_nested_splits_in [ i ]. dims () == 1 , <nl> + errors :: InvalidArgument (" Requires nested_row_splits [", i , "]", <nl> + " to be rank 1 but is rank ", <nl> + ragged_nested_splits_in [ i ]. dims ())); <nl> batched_ragged_input . append_splits ( ragged_nested_splits_in [ i ]); <nl> } <nl> 
int fmt_mtm_load_song ( song_t * song , slurp_t * fp , unsigned int lflags ) <nl>  <nl> song -> patterns [ pat ] = csf_allocate_pattern ( MAX ( rows , 32 )); <nl> song -> pattern_size [ pat ] = song -> pattern_alloc_size [ pat ] = 64 ; <nl> - tracknote = trackdata [ n ]; <nl> for ( chan = 0 ; chan < 32 ; chan ++) { <nl> slurp_read ( fp , & tmp , 2 ); <nl> tmp = bswapLE16 ( tmp );
static void _jbn_add_item ( JBL_NODE parent , JBL_NODE node ); <nl>  <nl> void iwjson_ftoa ( long double val , char buf [ static IWNUMBUF_SIZE ], size_t * out_len ) { <nl> // TODO : review <nl> - int len = snprintf ( buf , 64 , "%. 8Lf ", val ); <nl> + int len = snprintf ( buf , IWNUMBUF_SIZE , "%. 8Lf ", val ); <nl> if ( len <= 0 ) { <nl> buf [ 0 ] = '\ 0 '; <nl> * out_len = 0 ;
static int tls_new_ciphertext ( struct tls_connection * tls , <nl> if ( is_block_cipher ( cipher ) ) { <nl> pad_len = tls_verify_padding ( tls , last ); <nl> if ( pad_len < 0 ) { <nl> - rc = pad_len ; <nl> - return rc ; <nl> + /* Assume zero padding length to avoid timing attacks */ <nl> + pad_len = 0 ; <nl> } <nl> iob_unput ( last , pad_len ); <nl> len -= pad_len ;
future < fragmented_temporary_buffer > cql_server :: connection :: read_and_decompress_ <nl> if ( ret < 0 ) { <nl> throw std :: runtime_error (" CQL frame LZ4 uncompression failure "); <nl> } <nl> - return out . size (); <nl> + if ( ret != out . size ()) { <nl> + throw std :: runtime_error (" Malformed CQL frame - provided uncompressed size different than real uncompressed size "); <nl> + } <nl> + return static_cast < size_t >( ret ); <nl> }); <nl> on_compression_buffer_use (); <nl> return uncomp ;
tport_t * tport_tsend ( tport_t * self , <nl> tp_name_t tpn [ 1 ]; <nl> struct sigcomp_compartment * cc ; <nl>  <nl> - assert ( self ); <nl> - <nl> if (! self || ! msg || ! _tpn ) { <nl> msg_set_errno ( msg , EINVAL ); <nl> return NULL ;
static int parse_packet ( const char * payload , struct ncrx_msg * msg ) <nl> goto einval ; <nl> if (! msg -> text_len || <nl> nf_len >= NCRX_LINE_MAX || <nl> + nf_off >= nf_len || <nl> nf_off + msg -> text_len > nf_len ) <nl> goto einval ; <nl> 
PerformanceNavigationTiming :: PerformanceNavigationTiming ( <nl> ResourceTimingInfo * info , <nl> TimeTicks time_origin , <nl> const WebVector < WebServerTimingInfo >& server_timing ) <nl> - : PerformanceResourceTiming ( info ? info -> InitialURL (). GetString () : "", <nl> - " navigation ", <nl> - time_origin , <nl> - server_timing ), <nl> + : PerformanceResourceTiming ( <nl> + info ? info -> FinalResponse (). Url (). GetString () : "", <nl> + " navigation ", <nl> + time_origin , <nl> + server_timing ), <nl> ContextClient ( frame ), <nl> resource_timing_info_ ( info ) { <nl> DCHECK ( frame );
static HB_Bool myanmar_shape_syllable ( HB_Bool openType , HB_ShaperItem * item , HB_ <nl> if ( kinzi >= 0 && i > base && ( cc & Mymr_CF_AFTER_KINZI )) { <nl> reordered [ len ] = Mymr_C_NGA ; <nl> reordered [ len + 1 ] = Mymr_C_VIRAMA ; <nl> - properties [ len - 1 ] = AboveForm ; <nl> + if ( len > 0 ) <nl> + properties [ len - 1 ] = AboveForm ; <nl> properties [ len ] = AboveForm ; <nl> len += 2 ; <nl> kinzi = - 1 ;
std :: string ResourceBundle :: LoadLocaleResources ( <nl>  <nl> std :: unique_ptr < DataPack > data_pack ( new DataPack ( SCALE_FACTOR_100P )); <nl> if (! data_pack -> LoadFromPath ( locale_file_path )) { <nl> - LOG ( ERROR ) << " failed to load locale . pak "; <nl> + LOG ( ERROR ) << " failed to load locale file : " << locale_file_path ; <nl> NOTREACHED (); <nl> return std :: string (); <nl> }
void RendererSchedulerImpl :: OnShutdownTaskQueue ( <nl> case MainThreadTaskQueue :: QueueClass :: kTimer : <nl> task_queue -> RemoveTaskObserver ( <nl> & main_thread_only (). timer_task_cost_estimator ); <nl> + break ; <nl> case MainThreadTaskQueue :: QueueClass :: kLoading : <nl> task_queue -> RemoveTaskObserver ( <nl> & main_thread_only (). loading_task_cost_estimator ); <nl> + break ; <nl> default : <nl> break ; <nl> }
void FeatureInfo :: EnableOESTextureHalfFloatLinear () { <nl> return ; <nl> AddExtensionString (" GL_OES_texture_half_float_linear "); <nl> feature_flags_ . enable_texture_half_float_linear = true ; <nl> + <nl> + // TODO ( capn ) : Re - enable this once we have ANGLE + SwiftShader supporting <nl> + // IOSurfaces . <nl> + if ( workarounds_ . disable_half_float_for_gmb ) <nl> + return ; <nl> feature_flags_ . gpu_memory_buffer_formats . Add ( gfx :: BufferFormat :: RGBA_F16 ); <nl> } <nl> 
class TouchActionBrowserTest : public ContentBrowserTest { <nl> // Expect that the compositor scrolled at least one pixel while the <nl> // main thread was in a busy loop . <nl> while ( wait_until_scrolled && <nl> - frame_watcher -> LastMetadata (). root_scroll_offset . y () <= 0 ) { <nl> + frame_watcher -> LastMetadata (). root_scroll_offset . y () < <nl> + distance . y ()) { <nl> frame_watcher -> WaitFrames ( 1 ); <nl> } <nl> 
void GpuDataManager :: UpdateGpuInfo ( const GPUInfo & gpu_info ) { <nl> base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> if (! gpu_info_ . Merge ( gpu_info )) <nl> return ; <nl> + } <nl> + <nl> + RunGpuInfoUpdateCallbacks (); <nl>  <nl> - RunGpuInfoUpdateCallbacks (); <nl> + { <nl> + base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> content :: GetContentClient ()-> SetGpuInfo ( gpu_info_ ); <nl> } <nl> 
void InterstitialPage :: Observe ( NotificationType type , <nl> // request won ' t be blocked if the same RenderViewHost was used for the <nl> // new navigation . <nl> Disable (); <nl> - DCHECK (! resource_dispatcher_host_notified_ ); <nl> TakeActionOnResourceDispatcher ( CANCEL ); <nl> break ; <nl> case NotificationType :: RENDER_WIDGET_HOST_DESTROYED :
void OpenPDFInReaderView :: Update ( content :: WebContents * web_contents ) { <nl> } <nl>  <nl> SetVisible (!! model_ ); <nl> + <nl> + // Hide the bubble if it is currently shown and the icon is hidden . <nl> + if (! model_ && bubble_ ) <nl> + bubble_ -> GetWidget ()-> Hide (); <nl> } <nl>  <nl> void OpenPDFInReaderView :: ShowBubble () {
void MediaStreamDispatcherHost :: DoOpenDevice ( <nl> } <nl>  <nl> media_stream_manager_ -> OpenDevice ( <nl> - render_process_id_ , render_frame_id_ , page_request_id , requester_id_ , <nl> + render_process_id_ , render_frame_id_ , requester_id_ , page_request_id , <nl> device_id , type , std :: move ( salt_and_origin ), std :: move ( callback ), <nl> base :: BindRepeating (& MediaStreamDispatcherHost :: OnDeviceStopped , <nl> weak_factory_ . GetWeakPtr ()));
IN_PROC_BROWSER_TEST_F ( PanelBrowserTest , MAYBE_FocusLostOnMinimize ) { <nl> panel -> Close (); <nl> } <nl>  <nl> -// TODO ( dimich ): try / enable on other platforms . <nl> -# if defined ( OS_MACOSX ) || defined ( OS_WIN ) <nl> +// TODO ( dimich ): try / enable on other platforms . See bug 103253 for details on <nl> +// why this is disabled on windows . <nl> +# if defined ( OS_MACOSX ) <nl> # define MAYBE_MinimizeTwoPanelsWithoutTabbedWindow \ <nl> MinimizeTwoPanelsWithoutTabbedWindow <nl> # else
SessionStartupPref StartupBrowserCreator :: GetSessionStartupPref ( <nl> pref . type = SessionStartupPref :: LAST ; <nl> } <nl>  <nl> - if ( pref . type == SessionStartupPref :: LAST && <nl> - IncognitoModePrefs :: ShouldLaunchIncognito ( command_line , prefs )) { <nl> + if ( pref . type == SessionStartupPref :: LAST && profile -> IsOffTheRecord ()) { <nl> // We don ' t store session information when incognito . If the user has <nl> // chosen to restore last session and launched incognito , fallback to <nl> // default launch behavior .
class InotifyReaderTask : public Task { <nl> : reader_ ( reader ), <nl> inotify_fd_ ( inotify_fd ), <nl> shutdown_fd_ ( shutdown_fd ) { <nl> + // Make sure the file descriptors are good for use with select (). <nl> + CHECK_LE ( 0 , inotify_fd_ ); <nl> + CHECK_GT ( FD_SETSIZE , inotify_fd_ ); <nl> + CHECK_LE ( 0 , shutdown_fd_ ); <nl> + CHECK_GT ( FD_SETSIZE , shutdown_fd_ ); <nl> } <nl>  <nl> virtual void Run () {
xsltCompileLocationPathPattern ( xsltParserContextPtr ctxt , int novar ) { <nl> SKIP_BLANKS ; <nl> if (( CUR == '(') && ! xmlXPathIsNodeType ( name )) { <nl> xsltCompileIdKeyPattern ( ctxt , name , 1 , novar , 0 ); <nl> + if ( ctxt -> error ) <nl> + return ; <nl> if (( CUR == '/') && ( NXT ( 1 ) == '/')) { <nl> PUSH ( XSLT_OP_ANCESTOR , NULL , NULL , novar ); <nl> NEXT ;
class LocalHttpTestServer : public TestServer { <nl> FilePath ()) {} <nl> }; <nl>  <nl> - TEST_F ( URLRequestTest , DelayedCookieCallback ) { <nl> + TEST_F ( URLRequestTest , FLAKY_DelayedCookieCallback ) { <nl> LocalHttpTestServer test_server ; <nl> ASSERT_TRUE ( test_server . Start ()); <nl>  <nl> class URLRequestTestFTP : public URLRequestTest { <nl> }; <nl>  <nl> // Make sure an FTP request using an unsafe ports fails . <nl> - TEST_F ( URLRequestTestFTP , UnsafePort ) { <nl> + TEST_F ( URLRequestTestFTP , FLAKY_UnsafePort ) { <nl> ASSERT_TRUE ( test_server_ . Start ()); <nl>  <nl> URLRequestJobFactoryImpl job_factory ;
TEST ( VideoFrameMac , CheckWrapperFrame ) { <nl> { PIXEL_FORMAT_NV12 , kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange }, <nl> }; <nl>  <nl> - const gfx :: Size size ( kWidth , kHeight ); <nl> for ( const auto & format_pair : format_pairs ) { <nl> base :: ScopedCFTypeRef < CVPixelBufferRef > pb ; <nl> CVPixelBufferCreate ( nullptr , kWidth , kHeight , format_pair . corevideo ,
void AudioOutputController :: DoFlush () { <nl> if (! sync_reader_ ) { <nl> if ( state_ != kPaused ) <nl> return ; <nl> + AutoLock auto_lock ( lock_ ); <nl> buffer_ . Clear (); <nl> } <nl> }
void WebGLRenderingContextBase :: TexImageHelperImageData ( <nl> adjusted_source_image_rect . Height (), format , type , bytes ); <nl> } else { <nl> GLint upload_height = adjusted_source_image_rect . Height (); <nl> - if ( unpack_image_height ) { <nl> - // GL_UNPACK_IMAGE_HEIGHT overrides the passed - in height . <nl> - upload_height = unpack_image_height ; <nl> - } <nl> if ( function_id == kTexImage3D ) { <nl> ContextGL ()-> TexImage3D ( target , level , internalformat , <nl> adjusted_source_image_rect . Width (), upload_height ,
bool SimplifiedBackwardsTextIterator :: handleTextNode () <nl> m_textLength = m_positionEndOffset - m_positionStartOffset ; <nl> m_textCharacters = text . characters () + ( m_positionStartOffset - offsetInNode ); <nl> ASSERT ( m_textCharacters >= text . characters ()); <nl> - ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl> + RELEASE_ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl>  <nl> m_lastCharacter = text [ m_positionEndOffset - 1 ]; <nl> 
bool HTMLFormElement :: validateInteractively ( Event * event ) <nl>  <nl> bool HTMLFormElement :: prepareForSubmission ( Event * event ) <nl> { <nl> + RefPtr < HTMLFormElement > protector ( this ); <nl> Frame * frame = document (). frame (); <nl> if ( m_isSubmittingOrPreparingForSubmission || ! frame ) <nl> return m_isSubmittingOrPreparingForSubmission ;
SK_API void SkDebugf_FileLine ( const char * file , int line , bool fatal , <nl> # define SK_USE_LEGACY_DISTANCE_FIELDS <nl> # endif <nl>  <nl> -// To stage layout result changes ( minor ) related to convexity calculations <nl> -# ifndef SK_SUPPORT_LEGACY_CACHE_CONVEXITY <nl> -# define SK_SUPPORT_LEGACY_CACHE_CONVEXITY <nl> -# endif <nl> - <nl> // Skia is enabling this feature soon . Chrome probably does <nl> // not want it for M64 <nl> # ifndef SK_DISABLE_EXPLICIT_GPU_RESOURCE_ALLOCATION
bool PlatformFontSkia :: InitDefaultFont () { <nl>  <nl> bool success = false ; <nl> std :: string family = kFallbackFontFamilyName ; <nl> - int size_pixels = 12 ; <nl> + int size_pixels = PlatformFont :: kDefaultBaseFontSize ; <nl> int style = Font :: NORMAL ; <nl> Font :: Weight weight = Font :: Weight :: NORMAL ; <nl> FontRenderParams params ;

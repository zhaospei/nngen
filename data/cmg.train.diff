start_play_tune ( GstSidDec * siddec ) <nl>  <nl> gst_segment_init (& segment , GST_FORMAT_TIME ); <nl> gst_pad_push_event ( siddec -> srcpad , gst_event_new_segment (& segment )); <nl> + siddec -> total_bytes = 0 ; <nl>  <nl> res = gst_pad_start_task ( siddec -> srcpad , <nl> ( GstTaskFunction ) play_loop , siddec -> srcpad , NULL );
gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl> gst_buffer_ref ( buf ); <nl> if (! gst_buffer_map ( buf , & minfo , GST_MAP_READ )) { <nl> GST_ERROR_OBJECT ( mpeg2dec , " Failed to map input buffer "); <nl> + gst_buffer_unref ( buf ); <nl> return GST_FLOW_ERROR ; <nl> } <nl> 
gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl> frame -> system_frame_number , <nl> GST_TIME_ARGS ( frame -> pts ), GST_TIME_ARGS ( frame -> duration )); <nl>  <nl> + gst_buffer_ref ( buf ); <nl> if (! gst_buffer_map ( buf , & minfo , GST_MAP_READ )) { <nl> GST_ERROR_OBJECT ( mpeg2dec , " Failed to map input buffer "); <nl> return GST_FLOW_ERROR ; <nl> gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl>  <nl> done : <nl> gst_buffer_unmap ( buf , & minfo ); <nl> + gst_buffer_unref ( buf ); <nl> return ret ; <nl> } <nl> 
create_request_failed : <nl> { <nl> GST_ELEMENT_ERROR ( ctx , LIBRARY , INIT , <nl> (" Could not create request ."), ( NULL )); <nl> + g_free ( req_url ); <nl> goto reset ; <nl> } <nl> send_error :
gst_rdt_depay_push ( GstRDTDepay * rdtdepay , GstBuffer * buffer ) <nl> rdtdepay -> need_newsegment = FALSE ; <nl> } <nl>  <nl> + buffer = gst_buffer_make_metadata_writable ( buffer ); <nl> gst_buffer_set_caps ( buffer , GST_PAD_CAPS ( rdtdepay -> srcpad )); <nl>  <nl> if ( rdtdepay -> discont ) {
free_tree ( struct tree * t ) <nl> { <nl> size_t i ; <nl>  <nl> + if ( t == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < t -> nr_files ; ++ i ) { <nl> free ( t -> files [ i ]. path ); <nl> guestfs_free_statns ( t -> files [ i ]. stat );
main ( int argc , char * argv []) <nl> name = strrchr ( drvs -> a . filename , '/'); <nl> if ( name == NULL ) <nl> name = drvs -> a . filename ; <nl> + else <nl> + name ++; /* skip '/' character */ <nl> break ; <nl> case drv_d : <nl> name = drvs -> d . guest ;
inspect_mount_handle ( guestfs_h * g ) <nl> exit ( EXIT_FAILURE ); <nl> } <nl>  <nl> + /* Free old global if there is one . */ <nl> + free ( root ); <nl> + <nl> root = roots [ 0 ]; <nl> free ( roots ); <nl> 
static int mb_config_add_host ( oconfig_item_t * ci ) /* {{{ */ <nl>  <nl> status = cf_util_get_string_buffer ( ci , host -> host , sizeof ( host -> host )); <nl> if ( status != 0 ) <nl> + { <nl> + sfree ( host ); <nl> return ( status ); <nl> + } <nl> if ( host -> host [ 0 ] == 0 ) <nl> + { <nl> + sfree ( host ); <nl> return ( EINVAL ); <nl> + } <nl>  <nl> for ( i = 0 ; i < ci -> children_num ; i ++) <nl> {
static char * replace_str ( const char * str , const char * old , /* {{{ */ <nl> } else <nl> retlen = strlen ( str ); <nl>  <nl> - ret = malloc ( retlen + 1 ); <nl> + ret = calloc ( 1 , retlen + 1 ); <nl> if ( ret == NULL ) <nl> return NULL ; <nl> // added to original : not optimized , but keeps valgrind happy . <nl> - memset ( ret , 0 , retlen + 1 ); <nl>  <nl> r = ret ; <nl> p = str ;
static int bind_config ( oconfig_item_t * ci ) /* {{{ */ <nl> return (- 1 ); <nl> } <nl>  <nl> + sfree ( url ); <nl> url = strdup ( child -> values [ 0 ]. value . string ); <nl> } else if ( strcasecmp (" OpCodes ", child -> key ) == 0 ) <nl> bind_config_set_bool (" OpCodes ", & global_opcodes , child );
static int perl_config_includedir ( oconfig_item_t * ci ) <nl> || ( OCONFIG_TYPE_STRING != ci -> values [ 0 ]. type )) <nl> return 1 ; <nl>  <nl> + if ( NULL == aTHX ) { <nl> + log_warn (" EnableDebugger has no effects if used after LoadPlugin ."); <nl> + return 1 ; <nl> + } <nl> + <nl> value = ci -> values [ 0 ]. value . string ; <nl>  <nl> if ( NULL == perl ) {
c_avl_tree_t * c_avl_create ( int (* compare ) ( const void *, const void *)) <nl>  <nl> void c_avl_destroy ( c_avl_tree_t * t ) <nl> { <nl> + if ( t == NULL ) <nl> + return ; <nl> free_node ( t -> root ); <nl> free ( t ); <nl> }
static int cpu_read ( void ) <nl> submit ( cpu , " wait ", wait ); <nl> submit ( cpu , " interrupt ", intr ); <nl> submit ( cpu , " softirq ", sitr ); <nl> + <nl> + if ( numfields >= 9 ) <nl> + submit ( cpu , " steal ", atoll ( fields [ 8 ])); <nl> } <nl> } <nl> 
static int tss2_add_vserver ( int vserver_port ) <nl> } <nl>  <nl> /* Allocate memory */ <nl> - entry = malloc ( sizeof (* entry )); <nl> + entry = calloc ( 1 , sizeof (* entry )); <nl> if ( entry == NULL ) <nl> { <nl> - ERROR (" teamspeak2 plugin : malloc failed ."); <nl> + ERROR (" teamspeak2 plugin : calloc failed ."); <nl> return (- 1 ); <nl> } <nl> - memset ( entry , 0 , sizeof ( vserver_list_t )); <nl>  <nl> /* Save data */ <nl> entry -> port = vserver_port ;
submit_counters ( struct thread_data * t , struct core_data * c , struct pkg_data * p ) <nl>  <nl> /* SMI */ <nl> if ( do_smi ) <nl> - turbostat_submit ( name , " current ", NULL , t -> smi_count ); <nl> + turbostat_submit ( name , " count ", NULL , t -> smi_count ); <nl>  <nl> /* submit per - core data only for 1st thread in core */ <nl> if (!( t -> flags & CPU_IS_FIRST_THREAD_IN_CORE ))
static int wh_write_command ( const data_set_t * ds , const value_list_t * vl , /* {{ <nl> } <nl> assert ( command_len < cb -> send_buffer_free ); <nl>  <nl> + /* Make scan - build happy . */ <nl> + assert ( cb -> send_buffer != NULL ); <nl> + <nl> /* ` command_len + 1 ' because ` command_len ' does not include the <nl> * trailing null byte . Neither does ` send_buffer_fill '. */ <nl> memcpy ( cb -> send_buffer + cb -> send_buffer_fill ,
static int c_psql_config_query ( oconfig_item_t * ci ) <nl> else <nl> log_warn (" Ignoring unknown config key \"% s \".", c -> key ); <nl> } <nl> + <nl> + if ( NULL == query -> query ) { <nl> + log_err (" Query \"% s \" does not include an SQL query string - " <nl> + " please check your configuration .", query -> name ); <nl> + c_psql_query_delete ( query ); <nl> + -- queries_num ; <nl> + return 1 ; <nl> + } <nl> return 0 ; <nl> } /* c_psql_config_query */ <nl> 
int uc_get_names ( char *** ret_names , cdtime_t ** ret_times , size_t * ret_number ) <nl> if ( status != 0 ) <nl> { <nl> size_t i ; <nl> - <nl> + <nl> for ( i = 0 ; i < number ; i ++) <nl> { <nl> sfree ( names [ i ]); <nl> } <nl> sfree ( names ); <nl> + sfree ( times ); <nl>  <nl> return (- 1 ); <nl> }
krb5_dbe_def_cpw ( krb5_context context , <nl> krb5_boolean keepold , <nl> krb5_db_entry * db_entry ); <nl>  <nl> + krb5_error_code <nl> + krb5_def_promote_db ( krb5_context , char *, char **); <nl> + <nl> krb5_error_code <nl> krb5_db_create_policy ( krb5_context kcontext , <nl> osa_policy_ent_t policy );
errcode_t profile_open_file ( const_profile_filespec_t filespec , <nl> } <nl> if ( data ) { <nl> data -> refcount ++; <nl> + data -> last_stat = 0 ; /* Make sure to stat when updating . */ <nl> k5_mutex_unlock (& g_shared_trees_mutex ); <nl> retval = profile_update_file_data ( data , NULL ); <nl> free ( expanded_filename );
int main ( int argc , char ** argv ) <nl> return 1 ; <nl> } <nl> krb5_klog_syslog ( LOG_INFO , " commencing operation "); <nl> + if ( nofork ) <nl> + fprintf ( stderr , "% s : starting ...\ n ", kdc_progname ); <nl> if (( retval = listen_and_process ())) { <nl> kdc_err ( kcontext , retval , " while processing network requests "); <nl> errout ++;
krb5_kdc_req * val ; <nl> krb5_free_principal ( val -> client ); <nl> if ( val -> server ) <nl> krb5_free_principal ( val -> server ); <nl> + if ( val -> etype ) <nl> + xfree ( val -> etype ); <nl> if ( val -> addresses ) <nl> krb5_free_address ( val -> addresses ); <nl> if ( val -> authorization_data . ciphertext . data )
krb5_get_realm_domain ( realm , domain ) <nl> krb5_xfree ( realmlist [ 0 ]); <nl> krb5_xfree ( realmlist ); <nl> } <nl> - * domain = NULL ; <nl> + if (( retdomain = malloc ( strlen ( realm ) + 2 )) == NULL ) <nl> + return ENOMEM ; <nl> + strcpy ( retdomain , "."); <nl> + strcat ( retdomain , realm ); /* return the realm as the domain <nl> + if lookup fails */ <nl> + * domain = retdomain ; <nl> return 0 ; <nl> } <nl> continue ;
kadm5_setkey_principal_3 ( void * server_handle , <nl> goto done ; <nl> } <nl> tptr = & kdb . key_data [ i ]; <nl> + tptr -> key_data_ver = tmp_key_data . key_data_ver ; <nl> + tptr -> key_data_kvno = tmp_key_data . key_data_kvno ; <nl> for ( k = 0 ; k < tmp_key_data . key_data_ver ; k ++) { <nl> tptr -> key_data_type [ k ] = tmp_key_data . key_data_type [ k ]; <nl> tptr -> key_data_length [ k ] = tmp_key_data . key_data_length [ k ];
try_one_princ ( krb5_context context , const krb5_ap_req * req , <nl> if ( ret ) <nl> return ret ; <nl> ret = try_one_entry ( context , req , & ent , keyblock_out ); <nl> + if ( ret == 0 ) <nl> + TRACE_RD_REQ_DECRYPT_SPECIFIC ( context , ent . principal , & ent . key ); <nl> ( void ) krb5_free_keytab_entry_contents ( context , & ent ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - TRACE_RD_REQ_DECRYPT_SPECIFIC ( context , ent . principal , & ent . key ); <nl> return 0 ; <nl> } <nl> 
template int SSLWrap < TLSCallbacks >:: TLSExtStatusCallback ( SSL * s , void * arg ); <nl>  <nl>  <nl> static void crypto_threadid_cb ( CRYPTO_THREADID * tid ) { <nl> - static_assert ( sizeof ( uv_thread_t ) <= sizeof ( void *), <nl> + static_assert ( sizeof ( uv_thread_t ) <= sizeof ( void *), // NOLINT ( runtime / sizeof ) <nl> " uv_thread_t does not fit in a pointer "); <nl> CRYPTO_THREADID_set_pointer ( tid , reinterpret_cast < void *>( uv_thread_self ())); <nl> }
Handle < Value > ReadFloatGeneric ( const Arguments & args ) { <nl> return ThrowTypeError (" offset is not uint "); <nl> size_t len = static_cast < size_t >( <nl> args . This ()-> GetIndexedPropertiesExternalArrayDataLength ()); <nl> - if ( offset + sizeof ( T ) > len ) <nl> + if ( offset + sizeof ( T ) > len || offset + sizeof ( T ) < offset ) <nl> return ThrowRangeError (" Trying to read beyond buffer length "); <nl> } <nl> 
static void GetStringWidth ( const FunctionCallbackInfo < Value >& args ) { <nl> TwoByteValue value ( env -> isolate (), args [ 0 ]); <nl> // reinterpret_cast is required by windows to compile <nl> UChar * str = reinterpret_cast < UChar *>(* value ); <nl> - UChar32 c ; <nl> + static_assert ( sizeof (* str ) == sizeof (** value ), <nl> + " sizeof (* str ) == sizeof (** value )"); <nl> + UChar32 c = 0 ; <nl> UChar32 p ; <nl> size_t n = 0 ; <nl> uint32_t width = 0 ;
void ECDH :: SetPrivateKey ( const FunctionCallbackInfo < Value >& args ) { <nl> if ( priv == nullptr ) <nl> return env -> ThrowError (" Failed to convert Buffer to BN "); <nl>  <nl> - if (! EC_KEY_set_private_key ( ecdh -> key_ , priv )) <nl> + int result = EC_KEY_set_private_key ( ecdh -> key_ , priv ); <nl> + BN_free ( priv ); <nl> + <nl> + if (! result ) { <nl> return env -> ThrowError (" Failed to convert BN to a private key "); <nl> + } <nl> } <nl>  <nl> 
void RandomBytesCheck ( RandomBytesRequest * req , Local < Value > argv [ 2 ]) { <nl> Buffer * buffer = Buffer :: New ( req -> data_ , req -> size_ , RandomBytesFree , NULL ); <nl> argv [ 0 ] = Local < Value >:: New ( Null ()); <nl> argv [ 1 ] = Local < Object >:: New ( buffer -> handle_ ); <nl> + req -> data_ = NULL ; <nl> } <nl> + free ( req -> data_ ); <nl> } <nl>  <nl> 
void SetupProcessObject ( Environment * env , <nl> READONLY_PROPERTY ( process , <nl> " _preload_modules ", <nl> array ); <nl> + <nl> + delete [] preload_modules ; <nl> + preload_modules = nullptr ; <nl> + preload_module_count = 0 ; <nl> } <nl>  <nl> // -- no - deprecation
void CipherBase :: Final ( const FunctionCallbackInfo < Value >& args ) { <nl>  <nl> args . GetReturnValue (). Set ( <nl> Buffer :: New ( env , reinterpret_cast < char *>( out_value ), out_len )); <nl> + delete [] out_value ; <nl> } <nl>  <nl> 
int unrar_open ( int fd , const char * dirname , unrar_state_t * state ) <nl> unrar_dbgmsg (" UNRAR : Offset : % x \ n ", offset ); <nl> if ( offset < 0 ){ <nl> unrar_dbgmsg (" UNRAR : Error Offset : % d \ n ", offset ); <nl> - offset = 0 ; <nl> + free ( main_hdr ); <nl> + free ( state -> comment_dir ); <nl> + free ( unpack_data ); <nl> + return UNRAR_ERR ; <nl> } <nl> comment_header = read_header ( fd , COMM_HEAD ); <nl> if ( comment_header ) {
abort : <nl> } <nl> if ( file_tmp_o1 ) { <nl> html_output_flush ( file_tmp_o1 ); <nl> - close ( file_tmp_o1 -> fd ); <nl> + if ( file_buff_text -> fd != - 1 ) <nl> + close ( file_tmp_o1 -> fd ); <nl> free ( file_tmp_o1 ); <nl> } <nl> return retval ;
static char * sha256file ( const char * file , unsigned int * size ) <nl> sha256_final (& ctx , digest ); <nl> sha = ( char *) malloc ( 65 ); <nl> if (! sha ) <nl> + { <nl> + fclose ( fh ); <nl> return NULL ; <nl> + } <nl> for ( i = 0 ; i < 32 ; i ++) <nl> sprintf ( sha + i * 2 , "% 02x ", digest [ i ]); <nl> + <nl> + fclose ( fh ); <nl> return sha ; <nl> } <nl> 
inline static int ac_special_altstr ( const char * hexpr , uint8_t sigopts , struct c <nl> /* allocate reusable subexpr */ <nl> if (!( subexpr = cli_calloc ( slen + 1 , sizeof ( char )))) { <nl> cli_errmsg (" ac_special_altstr : Can ' t allocate subexpr container \ n "); <nl> + free ( hexprcpy ); <nl> return CL_EMEM ; <nl> } <nl> 
static int updatedb ( const char * dbname , const char * hostname , char * ip , int * sig <nl> } <nl>  <nl> if ( rename ( newfile , newdb ) == - 1 ) { <nl> - logg ("! Can ' t rename % s to % s \ n ", newfile , newdb ); <nl> + logg ("! Can ' t rename % s to % s : % s \ n ", newfile , newdb , strerror ( errno )); <nl> unlink ( newfile ); <nl> free ( newfile ); <nl> return 57 ;
static int url_hash_match ( const struct regex_matcher * rlist , const char * inurl , <nl> size_t j , k , ji , ki ; <nl> int rc ; <nl>  <nl> - if (! rlist -> md5_hashes . bm_patterns ) { <nl> + if (! rlist || ! rlist -> md5_hashes . bm_patterns ) { <nl> return CL_SUCCESS ; <nl> } <nl> if (! inurl )
static int handle_stream ( client_conn_t * conn , struct fd_buf * buf , const struct o <nl> pthread_mutex_unlock (& exit_mutex ); <nl> } <nl> * error = 1 ; <nl> + return - 1 ; <nl> } else { <nl> pos = 4 ; <nl> memmove ( buf -> buffer , & buf -> buffer [ pos ], buf -> off - pos );
char * pdf_finalize_string ( struct pdf_struct * pdf , struct pdf_obj * obj , const cha <nl> /* TODO : replace the escape sequences directly in the wrkstr */ <nl> if ( strchr ( wrkstr , '\\')) { <nl> output = cli_calloc ( wrklen + 1 , sizeof ( char )); <nl> - if (! output ) <nl> + if (! output ) { <nl> + free ( wrkstr ); <nl> return NULL ; <nl> + } <nl>  <nl> outlen = 0 ; <nl> for ( i = 0 ; i < wrklen ; ++ i ) {
cib_perform_op ( const char * op , int call_options , cib_op_t * fn , gboolean is_query <nl>  <nl> if ( rc == cib_ok && scratch ) { <nl> const char * new_version = crm_element_value ( scratch , XML_ATTR_CRM_VERSION ); <nl> - if ( new_version && compare_version ( new_version , CRM_FEATURE_SET ) < 0 ) { <nl> + if ( new_version && compare_version ( new_version , CRM_FEATURE_SET ) > 0 ) { <nl> crm_err (" Discarding update with feature set '% s ' greater than our own '% s '", <nl> new_version , CRM_FEATURE_SET ); <nl> rc = cib_NOTSUPPORTED ;
do_cl_join_finalize_respond ( long long action , <nl> erase_status_tag ( fsa_our_uname , XML_CIB_TAG_LRM ); <nl>  <nl> /* Just in case attrd was still around too */ <nl> - if ( is_not_set ( input_register , R_SHUTDOWN )) { <nl> + if ( is_not_set ( fsa_input_register , R_SHUTDOWN )) { <nl> update_attrd ( fsa_our_uname , " terminate ", NULL ); <nl> update_attrd ( fsa_our_uname , XML_CIB_ATTR_SHUTDOWN , NULL ); <nl> }
do_cib_control ( long long action , <nl> clear_bit_inplace ( fsa_input_register , R_CIB_CONNECTED ); <nl> if ( fsa_cib_conn != NULL <nl> && fsa_cib_conn -> state != cib_disconnected ) { <nl> + fsa_cib_conn -> cmds -> set_slave ( <nl> + fsa_cib_conn , cib_scope_local ); <nl> fsa_cib_conn -> cmds -> signoff ( fsa_cib_conn ); <nl> } <nl> }
lrmd_init_remote_tls_server () <nl> if ( rc != 0 ) { <nl> crm_warn (" A cluster connection will not be possible until the key is available "); <nl> } <nl> + gnutls_free ( psk_key . data ); <nl>  <nl> memset (& hints , 0 , sizeof ( struct addrinfo )); <nl> /* Bind to the wildcard address ( INADDR_ANY or IN6ADDR_ANY_INIT ).
need_abort ( crm_data_t * update ) <nl> return NULL ; <nl> } <nl>  <nl> + xml_prop_iter ( update , name , value , <nl> + if ( safe_str_eq ( name , XML_ATTR_ID ) == FALSE ) { <nl> + return update ; <nl> + } <nl> + ); <nl> + <nl> section = XML_CIB_TAG_NODES ; <nl> section_xml = get_object_root ( section , update ); <nl> xml_child_iter ( section_xml , child ,
send_smtp_trap ( const char * node , const char * rsc , const char * task , int target_r <nl> char crm_mail_body [ BODY_MAX ]; <nl> char * crm_mail_subject = NULL ; <nl>  <nl> + memset (& sa , 0 , sizeof ( struct sigaction )); <nl> + <nl> if ( node == NULL ) { <nl> node = "-"; <nl> }
void master_rsc_colocation_rh ( <nl> clone_variant_data_t * clone_data = NULL ; <nl> get_clone_variant_data ( clone_data , rsc_rh ); <nl>  <nl> + CRM_CHECK ( rsc_rh != NULL , return ); <nl> if ( rsc_rh -> provisional ) { <nl> return ; <nl> 
cib_ha_peer_callback ( HA_Message * msg , void * private_data ) <nl> { <nl> xmlNode * xml = convert_ha_message ( NULL , msg , __FUNCTION__ ); <nl> cib_peer_callback ( xml , private_data ); <nl> + free_xml ( xml ); <nl> } <nl>  <nl> void
synthesize_lrmd_failure ( lrm_state_t * lrm_state , xmlNode * action , int rc ) <nl> lrmd_free_rsc_info ( rsc_info ); <nl> process_lrm_event ( lrm_state , op , NULL ); <nl>  <nl> - } else { <nl> + } else if ( controld_action_is_recordable ( op -> op_type )) { <nl> /* If we can ' t process the result normally , at least write it to the CIB <nl> * if possible , so the scheduler can act on it . <nl> */
static xmlNode * inject_node_state ( cib_t * cib_conn , char * node ) <nl> rc = cib_conn -> cmds -> query ( cib_conn , xpath , & cib_object , cib_xpath | cib_sync_call | cib_scope_local ); <nl> } <nl>  <nl> + crm_free ( xpath ); <nl> CRM_ASSERT ( rc == cib_ok ); <nl> return cib_object ; <nl> }
crm_graph_functions_t te_graph_fns = { <nl> te_fence_node <nl> }; <nl>  <nl> + extern GMainLoop * mainloop ; <nl> + <nl> void <nl> notify_crmd ( crm_graph_t * graph ) <nl> { <nl> notify_crmd ( crm_graph_t * graph ) <nl>  <nl> case tg_shutdown : <nl> crm_info (" Exiting after transition "); <nl> + if ( mainloop != NULL && g_main_is_running ( mainloop )) { <nl> + g_main_quit ( mainloop ); <nl> + return ; <nl> + } <nl> exit ( LSB_EXIT_OK ); <nl> } <nl> 
write_cib_contents ( gpointer p ) <nl> crm_free ( tmp2 ); <nl> crm_free ( tmp1 ); <nl>  <nl> + free_xml ( local_cib ); <nl> + <nl> if ( p == NULL ) { <nl> /* exit () could potentially affect the parent by closing things it shouldn ' t <nl> * Use _exit instead
get_rsc_restart_list ( lrm_rsc_t * rsc , lrm_op_t * op ) <nl> } <nl>  <nl> metadata = string2xml ( metadata_str ); <nl> + if ( metadata == NULL ) { <nl> + crm_err (" Metadata for % s ::% s :% s is not valid XML ", <nl> + rsc -> provider , rsc -> class , rsc -> type ); <nl> + return NULL ; <nl> + } <nl> + <nl> actions = find_xml_node ( metadata , " actions ", TRUE ); <nl>  <nl> xml_child_iter_filter (
process_pe_message ( xmlNode * msg , xmlNode * xml_data , qb_ipcs_connection_t * send <nl> crm_xml_add_int ( data_set . graph , " transition_id ", 0 ); <nl> crm_xml_add_int ( data_set . graph , " cluster - delay ", 0 ); <nl> process = FALSE ; <nl> + free ( digest ); <nl>  <nl> } else if ( safe_str_eq ( digest , last_digest )) { <nl> crm_info (" Input has not changed since last time , not saving to disk "); <nl> is_repoke = TRUE ; <nl> + free ( digest ); <nl>  <nl> } else { <nl> free ( last_digest );
update_attr ( cib_t * the_cib , int call_options , <nl> CRM_CHECK ( set_name != NULL , return cib_missing ); <nl>  <nl> if ( attr_value == NULL ) { <nl> + free_xml ( xml_obj ); <nl> return cib_missing_data ; <nl> } <nl>  <nl> update_attr ( cib_t * the_cib , int call_options , <nl> xml_obj = create_xml_node ( xml_obj , XML_TAG_ATTRS ); <nl> crm_free ( local_set_name ); <nl> } else { <nl> + free_xml ( xml_obj ); <nl> xml_obj = NULL ; <nl> } <nl> 
attrd_local_callback ( xmlNode * msg ) <nl> goto set_unexpanded ; <nl> } <nl>  <nl> - int_value = char2score ( value ); <nl> + int_value = char2score ( hash_entry -> value ); <nl> if ( value [ plus_plus_len + 1 ] != '+') { <nl> const char * offset_s = value +( plus_plus_len + 2 ); <nl> offset = char2score ( offset_s );
common_apply_stickiness ( resource_t * rsc , node_t * node , pe_working_set_t * data_se <nl> value = g_hash_table_lookup ( node -> details -> attrs , fail_attr ); <nl> if ( value != NULL ) { <nl> crm_debug ("% s : % s ", fail_attr , value ); <nl> - fail_count = crm_parse_int ( value , " 0 "); <nl> + fail_count = char2score ( value ); <nl> } <nl> crm_free ( fail_attr ); <nl> 
void BitcoinGUI :: message ( const QString & title , const QString & message , unsigned <nl>  <nl> showNormalIfMinimized (); <nl> QMessageBox mBox ( static_cast < QMessageBox :: Icon >( nMBoxIcon ), strTitle , message , buttons , this ); <nl> + mBox . setTextFormat ( Qt :: PlainText ); <nl> int r = mBox . exec (); <nl> if ( ret != nullptr ) <nl> * ret = r == QMessageBox :: Ok ;
void ServiceConnection ( AcceptedConnection * conn ) <nl> // Read HTTP message headers and body <nl> ReadHTTPMessage ( conn -> stream (), mapHeaders , strRequest , nProto ); <nl>  <nl> + if ( strURI != "/") { <nl> + conn -> stream () << HTTPReply ( HTTP_NOT_FOUND , "", false ) << std :: flush ; <nl> + break ; <nl> + } <nl> + <nl> // Check authorization <nl> if ( mapHeaders . count (" authorization ") == 0 ) <nl> {
int git_libgit2_opts ( int key , ...) <nl> void git_strarray_free ( git_strarray * array ) <nl> { <nl> size_t i ; <nl> + <nl> + if ( array == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < array -> count ; ++ i ) <nl> git__free ( array -> strings [ i ]); <nl> 
static void impl__free ( git_odb_backend * _backend ) <nl> { <nl> struct memory_packer_db * db = ( struct memory_packer_db *) _backend ; <nl>  <nl> + git_mempack_reset ( _backend ); <nl> git_oidmap_free ( db -> objects ); <nl> git__free ( db ); <nl> }
int git_futils_mmap_ro_file ( git_map * out , const char * path ) <nl> if ( fd < 0 ) <nl> return fd ; <nl>  <nl> - len = git_futils_filesize ( fd ); <nl> + if (( len = git_futils_filesize ( fd )) < 0 ) <nl> + return - 1 ; <nl> + <nl> if (! git__is_sizet ( len )) { <nl> giterr_set ( GITERR_OS , " file `% s ` too large to mmap ", path ); <nl> return - 1 ;
static int config_delete ( git_config_file * cfg , const char * name ) <nl> pos = git_strmap_lookup_index ( b -> values , key ); <nl> git__free ( key ); <nl>  <nl> - if (! git_strmap_valid_index ( b -> values , pos )) <nl> + if (! git_strmap_valid_index ( b -> values , pos )) { <nl> + giterr_set ( GITERR_CONFIG , " Could not find key '% s ' to delete ", name ); <nl> return GIT_ENOTFOUND ; <nl> + } <nl>  <nl> var = git_strmap_value_at ( b -> values , pos ); <nl> 
int git_remote_update_tips ( git_remote * remote , int (* cb )( const char * refname , co <nl> for (; i < refs -> length ; ++ i ) { <nl> head = refs -> contents [ i ]; <nl>  <nl> + /* Skip tag annotations */ <nl> + if (! git__suffixcmp ( head -> name , "^{}")) <nl> + continue ; <nl> + <nl> if ( git_refspec_transform_r (& refname , spec , head -> name ) < 0 ) <nl> goto on_error ; <nl> 
static int valid_entry_name ( const char * filename ) <nl> (* filename != '.' || <nl> ( strcmp ( filename , ".") != 0 && <nl> strcmp ( filename , "..") != 0 && <nl> - strcmp ( filename , DOT_GIT ) != 0 )); <nl> + strcasecmp ( filename , DOT_GIT ) != 0 )); <nl> } <nl>  <nl> static int entry_sort_cmp ( const void * a , const void * b )
git_oid_shorten * git_oid_shorten_new ( size_t min_length ) <nl>  <nl> void git_oid_shorten_free ( git_oid_shorten * os ) <nl> { <nl> + if ( os == NULL ) <nl> + return ; <nl> + <nl> git__free ( os -> nodes ); <nl> git__free ( os ); <nl> }
int git_futils_mkdir ( <nl> min_root_len = git_path_root ( make_path . ptr ); <nl> if ( root < min_root_len ) <nl> root = min_root_len ; <nl> - while ( make_path . ptr [ root ] == '/') <nl> + while ( root >= 0 && make_path . ptr [ root ] == '/') <nl> ++ root ; <nl>  <nl> /* clip root to make_path length */
int git_path_diriter_init ( <nl> unsigned int flags ) <nl> { <nl> git_win32_path path_filter ; <nl> - git_buf hack = { 0 }; <nl>  <nl> static int is_win7_or_later = - 1 ; <nl> if ( is_win7_or_later < 0 )
int git_repository_config__weakptr ( git_config ** out , git_repository * repo ) <nl> res = load_config (& repo -> _config , repo , global_config_path , xdg_config_path , system_config_path ); <nl>  <nl> git_buf_free (& global_buf ); <nl> + git_buf_free (& xdg_buf ); <nl> git_buf_free (& system_buf ); <nl>  <nl> if ( res < 0 )
static int limit_list ( git_commit_list ** out , git_revwalk * walk , git_commit_list <nl> break ; <nl> } <nl>  <nl> - if (! commit -> uninteresting && walk -> hide_cb && walk -> hide_cb (& commit -> oid , walk -> hide_cb_payload )) <nl> - continue ; <nl> + if ( walk -> hide_cb && walk -> hide_cb (& commit -> oid , walk -> hide_cb_payload )) <nl> + continue ; <nl>  <nl> time = commit -> time ; <nl> p = & git_commit_list_insert ( commit , p )-> next ;
void git_revwalk_reset ( git_revwalk * walk ) <nl> git_commit_list_free (& walk -> iterator_rand ); <nl> git_commit_list_free (& walk -> iterator_reverse ); <nl> git_commit_list_free (& walk -> user_input ); <nl> + walk -> first_parent = 0 ; <nl> walk -> walking = 0 ; <nl> walk -> did_push = walk -> did_hide = 0 ; <nl> }
int git_openssl_stream_global_init ( void ) <nl> * to speak TLSv1 to perform the encryption itself . <nl> */ <nl> git__ssl_ctx = SSL_CTX_new ( SSLv23_method ()); <nl> + if (! git__ssl_ctx ) { <nl> + return - 1 ; <nl> + } <nl> + <nl> SSL_CTX_set_options ( git__ssl_ctx , ssl_opts ); <nl> SSL_CTX_set_mode ( git__ssl_ctx , SSL_MODE_AUTO_RETRY ); <nl> SSL_CTX_set_verify ( git__ssl_ctx , SSL_VERIFY_NONE , NULL );
uint32_t git_pool__system_page_size ( void ) <nl> size_t page_size ; <nl> if ( git__page_size (& page_size ) < 0 ) <nl> page_size = 4096 ; <nl> - size = page_size - 2 * sizeof ( void *); /* allow space for malloc overhead */ <nl> + /* allow space for malloc overhead */ <nl> + size = page_size - ( 2 * sizeof ( void *)) - sizeof ( git_pool_page ); <nl> } <nl>  <nl> return size ;
int git_reference_rename ( git_reference * ref , const char * new_name , int force ) <nl> head_target = git_reference_target ( head ); <nl>  <nl> if ( head_target && ! strcmp ( head_target , ref -> name )) { <nl> + git_reference_free ( head ); <nl> + head = NULL ; <nl> + <nl> if ( git_reference_create_symbolic (& head , ref -> owner , " HEAD ", new_name , 1 ) < 0 ) { <nl> giterr_set ( GITERR_REFERENCE , <nl> " Failed to update HEAD after renaming reference ");
int git_repository_head_unborn ( git_repository * repo ) <nl> error = git_repository_head (& ref , repo ); <nl> git_reference_free ( ref ); <nl>  <nl> - if ( error == GIT_EUNBORNBRANCH ) <nl> + if ( error == GIT_EUNBORNBRANCH ) { <nl> + giterr_clear (); <nl> return 1 ; <nl> + } <nl>  <nl> if ( error < 0 ) <nl> return - 1 ;
static int wait_while_ack ( gitno_buffer * buf ) <nl> ( pkt -> status != GIT_ACK_CONTINUE || <nl> pkt -> status != GIT_ACK_COMMON )) { <nl> git__free ( pkt ); <nl> - break ; <nl> + return 0 ; <nl> } <nl> } <nl> 
cmsHANDLE CMSEXPORT cmsIT8LoadFromMem ( cmsContext ContextID , const void * Ptr , cm <nl>  <nl> it8 = ( cmsIT8 *) hIT8 ; <nl> it8 -> MemoryBlock = ( char *) _cmsMalloc ( ContextID , len + 1 ); <nl> + if ( it8 -> MemoryBlock == NULL ) <nl> + { <nl> + cmsIT8Free ( hIT8 ); <nl> + return FALSE ; <nl> + } <nl>  <nl> strncpy ( it8 -> MemoryBlock , ( const char *) Ptr , len ); <nl> it8 -> MemoryBlock [ len ] = 0 ;
cmsPipeline * DefaultICCintents ( cmsContext ContextID , <nl> // Concatenate to the output LUT <nl> if (! cmsPipelineCat ( Result , Lut )) <nl> goto Error ; <nl> + <nl> cmsPipelineFree ( Lut ); <nl> + Lut = NULL ; <nl>  <nl> // Update current space <nl> CurrentColorSpace = ColorSpaceOut ; <nl> cmsPipeline * DefaultICCintents ( cmsContext ContextID , <nl>  <nl> Error : <nl>  <nl> - cmsPipelineFree ( Lut ); <nl> + if ( Lut != NULL ) cmsPipelineFree ( Lut ); <nl> if ( Result != NULL ) cmsPipelineFree ( Result ); <nl> return NULL ; <nl> 
void * Type_MLU_Read ( struct _cms_typehandler_struct * self , cmsIOHANDLER * io , cmsU <nl>  <nl> // Check for overflow <nl> if ( Offset < ( SizeOfHeader + 8 )) goto Error ; <nl> + if (( Offset + Len ) > SizeOfTag + 8 ) goto Error ; <nl>  <nl> // True begin of the string <nl> BeginOfThisString = Offset - SizeOfHeader - 8 ;
const char * make_absolute_path ( const char * path ) <nl> char * last_elem = NULL ; <nl> struct stat st ; <nl>  <nl> + /* We ' ve already done it */ <nl> + if ( path == buf || path == next_buf ) <nl> + return path ; <nl> + <nl> if ( strlcpy ( buf , path , PATH_MAX ) >= PATH_MAX ) <nl> die (" Too long path : %.* s ", 60 , path ); <nl> 
# define BLOCKSIZE ( RECORDSIZE * 20 ) <nl>  <nl> static const char tar_tree_usage [] = <nl> -" git - tar - tree [-- remote =< repo >] < ent > [ basedir ]"; <nl> +" git - tar - tree [-- remote =< repo >] < tree - ish > [ basedir ]"; <nl>  <nl> static char block [ BLOCKSIZE ]; <nl> static unsigned long offset ;
static const char * format_time ( unsigned long time , const char * tz_str , <nl> int tz ; <nl>  <nl> if ( show_raw_time ) { <nl> - sprintf ( time_buf , "% lu % s ", time , tz_str ); <nl> + snprintf ( time_buf , sizeof ( time_buf ), "% lu % s ", time , tz_str ); <nl> } <nl> else { <nl> tz = atoi ( tz_str );
void diff_setup ( struct diff_options * options ) <nl>  <nl> int diff_setup_done ( struct diff_options * options ) <nl> { <nl> - if (( options -> find_copies_harder && <nl> - options -> detect_rename != DIFF_DETECT_COPY ) || <nl> - ( 0 <= options -> rename_limit && ! options -> detect_rename )) <nl> + if ( options -> find_copies_harder ) <nl> + options -> detect_rename = DIFF_DETECT_COPY ; <nl> + <nl> + if (( 0 <= options -> rename_limit && ! options -> detect_rename ) <nl> return - 1 ; <nl>  <nl> if ( options -> output_format & ( DIFF_FORMAT_NAME |
static void find_deltas ( struct object_entry ** list , unsigned * list_size , <nl> * depth , leaving it in the window is pointless . we <nl> * should evict it first . <nl> */ <nl> - if ( entry -> delta && depth <= n -> depth ) <nl> + if ( entry -> delta && max_depth <= n -> depth ) <nl> continue ; <nl>  <nl> /*
static void setup_progress_signal ( void ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> SHA_CTX ctx ; <nl> - char line [ PATH_MAX + 20 ]; <nl> + char line [ 40 + 1 + PATH_MAX + 2 ]; <nl> int window = 10 , depth = 10 , pack_to_stdout = 0 ; <nl> struct object_entry ** list ; <nl> int num_preferred_base = 0 ;
static int read_patches ( const char * range , struct string_list * list ) <nl> strbuf_addbuf (& buf , & line ); <nl> strbuf_addstr (& buf , "\ n \ n "); <nl> } else if ( starts_with ( line . buf , " ")) { <nl> + strbuf_rtrim (& line ); <nl> strbuf_addbuf (& buf , & line ); <nl> strbuf_addch (& buf , '\ n '); <nl> }
static void show_line ( struct grep_opt * opt , char * bol , char * eol , <nl>  <nl> * eol = '\ 0 '; <nl> while ( next_match ( opt , bol , eol , ctx , & match , eflags )) { <nl> + if ( match . rm_so == match . rm_eo ) <nl> + break ; <nl> printf ("%.* s % s %.* s % s ", <nl> ( int ) match . rm_so , bol , <nl> opt -> color_match ,
int send_pack ( struct send_pack_args * args , <nl> ref -> status = REF_STATUS_NONE ; <nl> if ( args -> stateless_rpc ) <nl> close ( out ); <nl> + if ( git_connection_is_socket ( conn )) <nl> + shutdown ( fd [ 0 ], SHUT_WR ); <nl> if ( use_sideband ) <nl> finish_async (& demux ); <nl> return - 1 ;
static void prune_directory ( struct dir_struct * dir , const char ** pathspec , int p <nl> free ( entry ); <nl> continue ; <nl> } <nl> + if ( entry -> ignored_entry ) <nl> + fprintf ( stderr , " warning : '% s ' is an ignored path .\ n ", <nl> + entry -> name ); <nl> * dst ++ = entry ; <nl> } <nl> dir -> nr = dst - dir -> entries ;
static int write_pseudoref ( const char * pseudoref , const struct object_id * oid , <nl> struct strbuf buf = STRBUF_INIT ; <nl> int ret = - 1 ; <nl>  <nl> + if (! oid ) <nl> + return 0 ; <nl> + <nl> strbuf_addf (& buf , "% s \ n ", oid_to_hex ( oid )); <nl>  <nl> filename = git_path ("% s ", pseudoref );
static int cmd_log ( int argc , const char ** argv , char ** envp ) <nl> prepare_revision_walk (& rev ); <nl> setup_pager (); <nl> while (( commit = get_revision (& rev )) != NULL ) { <nl> - if ( commit_format != CMIT_FMT_ONELINE && shown ) <nl> + if ( shown && do_diff && commit_format != CMIT_FMT_ONELINE ) <nl> putchar ('\ n '); <nl> fputs ( commit_prefix , stdout ); <nl> if ( abbrev_commit && abbrev )
int main ( int argc , const char ** argv ) <nl> else <nl> revs . header_prefix = " commit "; <nl> } <nl> + else if ( revs . verbose_header ) <nl> + /* Only -- header was specified */ <nl> + revs . commit_format = CMIT_FMT_RAW ; <nl>  <nl> list = revs . commits ; <nl> 
static int get_sha1_oneline ( const char * prefix , unsigned char * sha1 ) <nl> unsigned long size ; <nl>  <nl> commit = pop_most_recent_commit (& list , ONELINE_SEEN ); <nl> - parse_object ( commit -> object . sha1 ); <nl> + if (! parse_object ( commit -> object . sha1 )) <nl> + continue ; <nl> if ( temp_commit_buffer ) <nl> free ( temp_commit_buffer ); <nl> if ( commit -> buffer )
void verify_non_filename ( const char * prefix , const char * arg ) <nl> if (! lstat ( name , & st )) <nl> die (" ambiguous argument '% s ': both revision and filename \ n " <nl> " Use '--' to separate filenames from revisions ", arg ); <nl> - if ( errno != ENOENT ) <nl> + if ( errno != ENOENT && errno != ENOTDIR ) <nl> die ("'% s ': % s ", arg , strerror ( errno )); <nl> } <nl> 
int xmkstemp ( char * template ) <nl> int saved_errno = errno ; <nl> const char * nonrelative_template ; <nl>  <nl> - if (! template [ 0 ]) <nl> + if ( strlen ( template ) != strlen ( origtemplate )) <nl> template = origtemplate ; <nl>  <nl> nonrelative_template = absolute_path ( template );
const char * const local_repo_env [ LOCAL_REPO_ENV_SIZE + 1 ] = { <nl> static void setup_git_env ( void ) <nl> { <nl> git_dir = getenv ( GIT_DIR_ENVIRONMENT ); <nl> - if (! git_dir ) <nl> + if (! git_dir ) { <nl> git_dir = read_gitfile_gently ( DEFAULT_GIT_DIR_ENVIRONMENT ); <nl> + git_dir = git_dir ? xstrdup ( git_dir ) : NULL ; <nl> + } <nl> if (! git_dir ) <nl> git_dir = DEFAULT_GIT_DIR_ENVIRONMENT ; <nl> git_object_dir = getenv ( DB_ENVIRONMENT );
static void wt_shortstatus_print_tracking ( struct wt_status * s ) <nl> base = shorten_unambiguous_ref ( base , 0 ); <nl> color_fprintf ( s -> fp , header_color , "..."); <nl> color_fprintf ( s -> fp , branch_color_remote , "% s ", base ); <nl> + free (( char *) base ); <nl>  <nl> if (! upstream_is_gone && ! num_ours && ! num_theirs ) { <nl> fputc ( s -> null_termination ? '\ 0 ' : '\ n ', s -> fp );
static int setup_index ( unsigned char * sha1 ) <nl> return - 1 ; <nl>  <nl> new_pack = parse_pack_index ( sha1 ); <nl> + if (! new_pack ) <nl> + return - 1 ; /* parse_pack_index () already issued error message */ <nl> new_pack -> next = repo -> packs ; <nl> repo -> packs = new_pack ; <nl> return 0 ;
static void print_summary ( const char * prefix , const unsigned char * sha1 ) <nl> rev . show_root_diff = 1 ; <nl> rev . commit_format = get_commit_format (" format :% h : % s "); <nl> rev . always_show_header = 0 ; <nl> + diff_setup_done (& rev . diffopt ); <nl>  <nl> printf (" Created % scommit ", initial_commit ? " initial " : ""); <nl> 
static int fetch_object ( struct alt_base * repo , unsigned char * sha1 ) <nl> curl_result = curl_easy_perform ( curl ); <nl> curl_easy_setopt ( curl , CURLOPT_HTTPHEADER , no_range_header ); <nl> if ( curl_result != 0 ) { <nl> - unlink ( tmpfile ); <nl> return error ("% s ", curl_errorstr ); <nl> } <nl> 
static void cmd_user_ver2 ( struct doveadm_cmd_context * cctx ) <nl> ( void ) doveadm_cmd_param_str ( cctx , " field ", & show_field ); <nl> ( void ) doveadm_cmd_param_bool ( cctx , " userdb - only ", & userdb_only ); <nl>  <nl> + memset (& input , 0 , sizeof ( input )); <nl> if ( doveadm_cmd_param_array ( cctx , " auth - info ", & optval )) <nl> for (;* optval != NULL ; optval ++) <nl> auth_user_info_parse (& input . info , * optval );
int fts_expunge_log_uid_count ( struct fts_expunge_log * log , <nl> { <nl> int ret ; <nl>  <nl> - if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) <nl> + if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) { <nl> + * expunges_r = 0 ; <nl> return ret ; <nl> + } <nl>  <nl> return fts_expunge_log_read_expunge_count ( log , expunges_r ); <nl> }
static const struct command imap4rev1_commands [] = { <nl> { " APPEND ", cmd_append , COMMAND_FLAG_BREAKS_SEQS }, <nl> { " EXAMINE ", cmd_examine , COMMAND_FLAG_BREAKS_MAILBOX }, <nl> { " CREATE ", cmd_create , 0 }, <nl> - { " DELETE ", cmd_delete , COMMAND_FLAG_USE_NONEXISTENT }, <nl> + { " DELETE ", cmd_delete , COMMAND_FLAG_BREAKS_MAILBOX | <nl> + COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " RENAME ", cmd_rename , COMMAND_FLAG_USE_NONEXISTENT }, <nl> { " LIST ", cmd_list , 0 }, <nl> { " LSUB ", cmd_lsub , 0 },
bool passdb_get_credentials ( struct auth_request * auth_request , <nl> /* anything goes . change the credentials_scheme to what we <nl> actually got , so blocking passdbs work . */ <nl> auth_request -> credentials_scheme = <nl> - p_strdup ( auth_request -> pool , input_scheme ); <nl> + p_strdup ( auth_request -> pool , t_strcut ( input_scheme , '.')); <nl> return TRUE ; <nl> } <nl> 
void master_service_deinit ( struct master_service ** _service ) <nl> lib_signals_deinit (); <nl> io_loop_destroy (& service -> ioloop ); <nl>  <nl> + if ( service -> listener_names != NULL ) <nl> + p_strsplit_free ( default_pool , service -> listener_names ); <nl> i_free ( service -> listeners ); <nl> i_free ( service -> getopt_str ); <nl> i_free ( service -> name );
static bool server_connection_input_one ( struct server_connection * conn ) <nl> return FALSE ; <nl>  <nl> /* check logs */ <nl> - ( void ) server_connection_print_log ( conn ); <nl> + if ( conn -> log_input != NULL ) <nl> + ( void ) server_connection_print_log ( conn ); <nl>  <nl> switch ( conn -> state ) { <nl> case SERVER_REPLY_STATE_DONE :
int mail_modifylog_mark_synced ( MailModifyLog * log ) <nl> { <nl> i_assert ( log -> index -> lock_type != MAIL_LOCK_UNLOCK ); <nl>  <nl> + if (! mmap_update ( log )) <nl> + return FALSE ; <nl> + <nl> if ( log -> header -> sync_id == SYNC_ID_FULL ) { <nl> /* log file is full , switch to next one */ <nl> return mail_modifylog_switch_file ( log );
void smtp_server_connection_data_chunk_init ( struct smtp_server_cmd_ctx * cmd ) <nl> command -> hook_replied = cmd_data_chunk_replied ; <nl> command -> hook_destroy = cmd_data_destroy ; <nl>  <nl> - if ( conn -> state . data_chain == NULL ) { <nl> + if (! conn -> state . data_failed && conn -> state . data_chain == NULL ) { <nl> i_assert ( data_cmd -> chunk_first ); <nl> i_assert ( conn -> state . data_chain_input == NULL ); <nl> conn -> state . data_chain_input =
imapc_sync_send_commands ( struct imapc_sync_context * ctx , uint32_t first_uid ) <nl> { <nl> string_t * cmd = t_str_new ( 64 ); <nl>  <nl> + if ( ctx -> mbox -> exists_count == 0 ) { <nl> + /* empty mailbox - no point in fetching anything */ <nl> + return ; <nl> + } <nl> + <nl> str_printfa ( cmd , " UID FETCH % u :* ( FLAGS ", first_uid ); <nl> if ( imapc_mailbox_has_modseqs ( ctx -> mbox )) { <nl> str_append ( cmd , " MODSEQ ");
static void o_stream_metawrap_call_callback ( struct metawrap_ostream * mstream ) <nl> if ( write_callback != NULL ) { <nl> mstream -> write_callback = NULL ; <nl> write_callback ( mstream -> context ); <nl> + /* metadata headers aren ' t counted as part of the offset */ <nl> + mstream -> ostream . ostream . offset = 0 ; <nl> } <nl> } <nl> 
squat_trie_lookup_real ( struct squat_trie * trie , const char * str , <nl> unsigned int i , start , bytes , str_bytelen , str_charlen ; <nl> int ret = 0 ; <nl>  <nl> + array_clear ( definite_uids ); <nl> + array_clear ( maybe_uids ); <nl> + <nl> memset (& ctx , 0 , sizeof ( ctx )); <nl> ctx . trie = trie ; <nl> ctx . type = type ;
static int dbox_sync_index_rebuild_dir ( struct dbox_sync_rebuild_context * ctx , <nl> dir = opendir ( path ); <nl> if ( dir == NULL ) { <nl> if ( errno == ENOENT ) { <nl> + if (! primary ) { <nl> + /* alt directory doesn ' t exist , ignore */ <nl> + return 0 ; <nl> + } <nl> mailbox_set_deleted (& ctx -> mbox -> ibox . box ); <nl> return - 1 ; <nl> }
static int raw_sync ( struct raw_mailbox * mbox ) <nl> MAIL_INDEX_SYNC_FLAG_FLUSH_DIRTY | <nl> MAIL_INDEX_SYNC_FLAG_REQUIRE_CHANGES ; <nl>  <nl> + if ( mail_index_view_get_messages_count ( mbox -> box . view ) > 0 ) { <nl> + /* already - synced index was opened via <nl> + mail - index - alloc - cache . */ <nl> + return 0 ; <nl> + } <nl> + <nl> ret = mail_index_sync_begin ( mbox -> box . index , & index_sync_ctx , <nl> & sync_view , & trans , sync_flags ); <nl> if ( ret <= 0 ) {
static void smtp_server_connection_input ( struct connection * _conn ) <nl> bool smtp_server_connection_pending_command_data ( <nl> struct smtp_server_connection * conn ) <nl> { <nl> + if ( conn -> smtp_parser == NULL ) <nl> + return FALSE ; <nl> return smtp_command_parser_pending_data ( conn -> smtp_parser ); <nl> } <nl> 
fs_list_get_path ( struct mailbox_list * _list , const char * name , <nl> i_assert ( mailbox_list_is_valid_name ( _list , name , & error )); <nl>  <nl> if ( mailbox_list_try_get_absolute_path ( _list , & name )) { <nl> + if ( type == MAILBOX_LIST_PATH_TYPE_INDEX && <nl> + * set -> index_dir == '\ 0 ') <nl> + return 0 ; <nl> * path_r = name ; <nl> return 1 ; <nl> }
static bool pop3_uidl_assign_by_size ( struct mailbox * box ) <nl> struct imap_msg_map * imap_map ; <nl> unsigned int i , pop3_count , imap_count , count ; <nl>  <nl> + if ( mstorage -> skip_size_check ) <nl> + return FALSE ; <nl> + <nl> pop3_map = array_get_modifiable (& mstorage -> pop3_uidl_map , & pop3_count ); <nl> imap_map = array_get_modifiable (& mbox -> imap_msg_map , & imap_count ); <nl> count = I_MIN ( pop3_count , imap_count );
int rfc822_parse_phrase ( struct rfc822_parser_context * ctx , string_t * str ) <nl> obs - phrase = word *( word / "." / CFWS ) <nl> */ <nl>  <nl> + if ( ctx -> data == ctx -> end ) <nl> + return 0 ; <nl> if (* ctx -> data == '.') <nl> return - 1 ; <nl> 
static void driver_pgsql_close ( struct pgsql_db * db ) <nl> db -> io_dir = 0 ; <nl> db -> fatal_error = FALSE ; <nl>  <nl> + driver_pgsql_stop_io ( db ); <nl> + <nl> PQfinish ( db -> pg ); <nl> db -> pg = NULL ; <nl>  <nl> - driver_pgsql_stop_io ( db ); <nl> if ( db -> to_connect != NULL ) <nl> timeout_remove (& db -> to_connect ); <nl> 
dsync_mailbox_find_common_uid ( struct dsync_mailbox_importer * importer , <nl> } <nl> return ; <nl> } <nl> - if ( importer -> revert_local_changes ) { <nl> + if ( importer -> revert_local_changes && <nl> + change -> type != DSYNC_MAIL_CHANGE_TYPE_EXPUNGE ) { <nl> dsync_mailbox_revert_missing ( importer , change ); <nl> * result_r = " Reverting local change by deleting mailbox "; <nl> } else if ( dsync_mailbox_find_common_expunged_uid ( importer , change )) {
static int snarf ( struct mailbox * srcbox , struct mailbox * destbox ) <nl> enum mail_error error ; <nl> int ret ; <nl>  <nl> + /* make sure the destination mailbox has been opened */ <nl> + if ( mailbox_open ( destbox ) < 0 ) <nl> + return - 1 ; <nl> + <nl> if ( mailbox_sync ( srcbox , MAILBOX_SYNC_FLAG_FULL_READ ) < 0 ) <nl> return - 1 ; <nl> 
static void db_ldap_get_fd ( struct ldap_connection * conn ) <nl> i_fatal (" LDAP : Can ' t get connection fd : % s ", <nl> ldap_err2string ( ret )); <nl> } <nl> + if ( conn -> fd <= CLIENT_LISTEN_FD ) { <nl> + /* Solaris LDAP library seems to be broken */ <nl> + i_fatal (" LDAP : Buggy LDAP library returned wrong fd : % d ", <nl> + conn -> fd ); <nl> + } <nl> i_assert ( conn -> fd != - 1 ); <nl> net_set_nonblock ( conn -> fd , TRUE ); <nl> }
static int <nl> imapc_mailbox_exists ( struct mailbox * box , bool auto_boxes ATTR_UNUSED , <nl> enum mailbox_existence * existence_r ) <nl> { <nl> + if ( strcmp ( box -> list -> name , MAILBOX_LIST_NAME_IMAPC ) != 0 ) { <nl> + if ( box -> inbox_any ) <nl> + * existence_r = MAILBOX_EXISTENCE_SELECT ; <nl> + else <nl> + * existence_r = MAILBOX_EXISTENCE_NONE ; <nl> + return 0 ; <nl> + } <nl> + <nl> enum mailbox_info_flags flags ; <nl>  <nl> struct imapc_mailbox_list * list = ( struct imapc_mailbox_list *) box -> list ;
struct file_dict_transaction_context { <nl> }; <nl>  <nl> static struct dotlock_settings file_dict_dotlock_settings = { <nl> - . timeout = 30 , <nl> - . stale_timeout = 5 <nl> + . timeout = 60 * 2 , <nl> + . stale_timeout = 60 , <nl> + . use_io_notify = TRUE <nl> }; <nl>  <nl> static struct dict * file_dict_init ( struct dict * driver , const char * uri ,
int openssl_cert_match_name ( SSL * ssl , const char * verify_name ) <nl> } <nl> } <nl> sk_GENERAL_NAME_pop_free ( gnames , GENERAL_NAME_free ); <nl> + X509_free ( cert ); <nl> + <nl> /* verify against CommonName only when there wasn ' t any DNS <nl> SubjectAltNames */ <nl> if ( dns_names )
cmd_append_handle_args ( struct client_command_context * cmd , <nl> /* invalid keywords - delay failure */ <nl> client_send_box_error ( cmd , ctx -> box ); <nl> ctx -> failed = TRUE ; <nl> + keywords = NULL ; <nl> } <nl> } <nl> 
const char * t_abspath ( const char * path ) <nl> { <nl> const char * dir ; <nl> + i_assert ( path != NULL ); <nl>  <nl> if (* path == '/') <nl> return path ; <nl> const char * t_abspath ( const char * path ) <nl>  <nl> const char * t_abspath_to ( const char * path , const char * root ) <nl> { <nl> + i_assert ( path != NULL ); <nl> + i_assert ( root != NULL ); <nl> + <nl> if (* path == '/') <nl> return path ; <nl> 
int index_storage_search_deinit ( struct mail_search_context * _ctx ) <nl> array_free (& ctx -> mail_ctx . results ); <nl> array_free (& ctx -> mail_ctx . module_contexts ); <nl>  <nl> - array_foreach_modifiable (& ctx -> mails , mailp ) <nl> + array_foreach_modifiable (& ctx -> mails , mailp ) { <nl> + struct index_mail * imail = ( struct index_mail *)* mailp ; <nl> + <nl> + imail -> search_mail = FALSE ; <nl> mail_free ( mailp ); <nl> + } <nl> array_free (& ctx -> mails ); <nl> i_free ( ctx ); <nl> return ret ;
void sdbox_update_header ( struct sdbox_mailbox * mbox , <nl> mail_index_update_header_ext ( trans , mbox -> hdr_ext_id , 0 , <nl> & new_hdr , sizeof ( new_hdr )); <nl> } <nl> + memcpy ( mbox -> mailbox_guid , new_hdr . mailbox_guid , <nl> + sizeof ( mbox -> mailbox_guid )); <nl> } <nl>  <nl> static int sdbox_mailbox_create_indexes ( struct mailbox * box ,
static void mail_transaction_log_2_unlink_old ( struct mail_transaction_log * log ) <nl> return ; <nl> } <nl>  <nl> - if ( st . st_mtime + log -> index -> log_rotate_log2_stale_secs <= ioloop_time && <nl> + if ( ioloop_time - st . st_mtime >= ( time_t ) log -> index -> log_rotate_log2_stale_secs && <nl> ! log -> index -> readonly ) <nl> i_unlink_if_exists ( log -> filepath2 ); <nl> }
static int auth_master_run_cmd ( struct auth_master_connection * conn , <nl> io_loop_run ( conn -> ioloop ); <nl> } <nl>  <nl> - auth_master_unset_io ( conn , prev_ioloop ); <nl> + if ( prev_ioloop != NULL ) <nl> + auth_master_unset_io ( conn , prev_ioloop ); <nl> if ( conn -> aborted ) { <nl> conn -> aborted = FALSE ; <nl> auth_connection_close ( conn );
int i_getpwnam ( const char * name , struct passwd * pwd_r ) <nl> errno = getpwnam_r ( name , pwd_r , pwbuf , pwbuf_size , & result ); <nl> if ( result != NULL ) <nl> return 1 ; <nl> + if ( errno == EINVAL ) { <nl> + /* FreeBSD fails here when name =" user @ domain " */ <nl> + return 0 ; <nl> + } <nl> return errno == 0 ? 0 : - 1 ; <nl> } <nl> 
void program_client_program_input ( struct program_client * pclient ) <nl> } <nl> if ( program_client_input_pending ( pclient )) <nl> return ; <nl> - if (! input -> eof ) { <nl> + if ( pclient -> program_input != NULL && ! input -> eof ) { <nl> program_client_fail ( pclient , <nl> PROGRAM_CLIENT_ERROR_IO ); <nl> return ;
ssize_t i_stream_read ( struct istream * stream ) <nl> errno = stream -> stream_errno ; <nl> } else { <nl> i_assert ( stream -> eof ); <nl> + i_assert ( old_size == _stream -> pos - _stream -> skip ); <nl> } <nl> break ; <nl> case 0 :
fts_search_arg_create_or ( const struct mail_search_arg * orig_arg , pool_t pool , <nl> array_foreach ( tokens , tokenp ) { <nl> arg = p_new ( pool , struct mail_search_arg , 1 ); <nl> * arg = * orig_arg ; <nl> + arg -> match_not = FALSE ; /* we copied this to the parent SUB */ <nl> arg -> next = NULL ; <nl> arg -> value . str = p_strdup ( pool , * tokenp ); <nl> 
static void hook_build_update ( struct hook_build_context * ctx , void * _vlast ) <nl> void (** vlast )() = _vlast ; <nl> struct hook_stack * stack ; <nl>  <nl> + if ( ctx -> tail -> vfuncs == vlast ) { <nl> + /* no vfuncs overridden */ <nl> + return ; <nl> + } <nl> + <nl> /* ctx -> vfuncs_stack -> vfuncs points to the root vfuncs , <nl> ctx -> vfuncs_stack -> next -> vfuncs points to the first super function <nl> that is being called , and so on .
imap_msgpart_crlf_seek ( struct mail * mail , struct istream * input , <nl> if ( message_skip_virtual ( input , virtual_skip , & cr_skipped ) < 0 ) { <nl> errinput = i_stream_create_error ( errno ); <nl> i_stream_set_name ( errinput , i_stream_get_name ( input )); <nl> + i_stream_unref (& input ); <nl> return errinput ; <nl> } <nl> 
uid_range_to_seqs ( struct fts_search_context * fctx , <nl> if (! array_is_created ( seq_range )) <nl> p_array_init ( seq_range , fctx -> result_pool , count ); <nl> for ( i = 0 ; i < count ; i ++) { <nl> + if ( range [ i ]. seq1 > range [ i ]. seq2 ) <nl> + continue ; <nl> mailbox_get_seq_range ( fctx -> box , range [ i ]. seq1 , range [ i ]. seq2 , <nl> & seq1 , & seq2 ); <nl> if ( seq1 != 0 )
mail_index_sync_ext_atomic_inc ( struct mail_index_sync_map_ctx * ctx , <nl> ext -> record_size ); <nl> return - 1 ; <nl> } <nl> - if ( u -> diff < 0 && ( uint32_t )(- u -> diff ) > orig_num ) { <nl> + if ( u -> diff < 0 && ( uint64_t )(- u -> diff ) > orig_num ) { <nl> mail_index_sync_set_corrupted ( ctx , <nl> " Extension record inc drops number below zero " <nl> "( uid =% u , diff =% d , orig =% llu )",
uint64_t mail_index_transaction_get_highest_modseq ( struct mail_index_transaction <nl> new_highest_modseq ++; <nl> } <nl> if ( array_is_created (& t -> updates ) && <nl> - transaction_flag_updates_have_non_internal ( t ) > 0 ) <nl> + transaction_flag_updates_have_non_internal ( t )) <nl> new_highest_modseq ++; <nl> if ( array_is_created (& t -> keyword_updates )) { <nl> new_highest_modseq +=
void io_loop_set_current ( struct ioloop * ioloop ) <nl> io_switch_callback_t * const * callbackp ; <nl> struct ioloop * prev_ioloop = current_ioloop ; <nl>  <nl> + if ( ioloop == current_ioloop ) <nl> + return ; <nl> + <nl> current_ioloop = ioloop ; <nl> if ( array_is_created (& io_switch_callbacks )) { <nl> array_foreach (& io_switch_callbacks , callbackp )
void ssl_iostream_destroy ( struct ssl_iostream ** _ssl_io ) <nl> { <nl> struct ssl_iostream * ssl_io = * _ssl_io ; <nl>  <nl> + if ( _ssl_io == NULL || * _ssl_io == NULL ) <nl> + return ; <nl> + <nl> + ssl_io = * _ssl_io ; <nl> * _ssl_io = NULL ; <nl> ssl_vfuncs -> destroy ( ssl_io ); <nl> }
void buffer_verify_pool ( buffer_t * _buf ) <nl> const struct real_buffer * buf = ( const struct real_buffer *) _buf ; <nl> void * ret ; <nl>  <nl> - if ( buf -> pool != NULL && buf -> pool -> datastack_pool ) { <nl> + if ( buf -> pool != NULL && buf -> pool -> datastack_pool && buf -> alloc > 0 ) { <nl> /* this doesn ' t really do anything except verify the <nl> stack frame */ <nl> ret = p_realloc ( buf -> pool , buf -> w_buffer ,
mail_storage_service_init_post ( struct mail_storage_service_ctx * ctx , <nl> if ( errno == EACCES ) { <nl> i_error ("% s ", eacces_error_get (" chdir ", <nl> t_strconcat ( home , "/", NULL ))); <nl> - } if ( errno != ENOENT ) <nl> + } else if ( errno != ENOENT ) <nl> i_error (" chdir (% s ) failed : % m ", home ); <nl> else if ( mail_set -> mail_debug ) <nl> i_debug (" Home dir not found : % s ", home );
static void push_notification_event_mailboxunsubscribe_event ( <nl>  <nl> data = p_new ( ptxn -> pool , <nl> struct push_notification_event_mailboxunsubscribe_data , 1 ); <nl> - data -> subscribe = TRUE ; <nl> + data -> subscribe = FALSE ; <nl>  <nl> push_notification_txn_mbox_set_eventdata ( ptxn , mbox , ec , data ); <nl> }
int mountpoint_get ( const char * path , pool_t pool , struct mountpoint * point_r ) <nl> if ( device_path == NULL ) <nl> return 0 ; <nl>  <nl> + memset ( point_r , 0 , sizeof (* point_r )); <nl> point_r -> device_path = p_strdup ( pool , device_path ); <nl> point_r -> mount_path = p_strdup ( pool , mount_path ); <nl> point_r -> type = p_strdup ( pool , type );
static void list_send ( struct list_send_context * ctx , struct list_node * node , <nl> name = node -> name ; <nl> send_name = name ; <nl>  <nl> - if ( node -> flags != MAILBOX_PLACEHOLDER ) <nl> + if ( node -> flags != MAILBOX_PLACEHOLDER && <nl> + node -> flags != MAILBOX_NOSELECT ) <nl> match = IMAP_MATCH_YES ; <nl> else { <nl> /* make sure the placeholder matches . */
password_scheme_detect ( const char * plain_password , const char * crypted_password , <nl> break ; <nl> key = NULL ; <nl> } <nl> + hash_table_iterate_deinit (& ctx ); <nl> return key ; <nl> } <nl> 
int http_header_parse_next_field ( struct http_header_parser * parser , <nl> const uoff_t max_size = parser -> limits . max_size ; <nl> const uoff_t max_field_size = parser -> limits . max_field_size ; <nl> const unsigned char * data ; <nl> - uoff_t size ; <nl> + size_t size ; <nl> int ret ; <nl>  <nl> * error_r = NULL ;
acl_backend_vfile_get_local_dir ( struct acl_backend * backend , const char * name ) <nl> dir = mailbox_list_get_path ( ns -> list , name , <nl> MAILBOX_LIST_PATH_TYPE_MAILBOX ); <nl> } <nl> - if ( name == NULL ) { <nl> + if ( name == NULL && dir != NULL ) { <nl> /* verify that the directory isn ' t same as INBOX ' s directory . <nl> this is mainly for Maildir . */ <nl> inbox = mailbox_list_get_path ( ns -> list , " INBOX ",
imapc_mail_get_stream ( struct mail * _mail , bool get_body , <nl> mail_set_aborted ( _mail ); <nl> return - 1 ; <nl> } <nl> + if ( _mail -> expunged ) { <nl> + /* We already detected that the mail is expunged . <nl> + Don ' t spend time trying to FETCH it again . */ <nl> + mail_set_expunged ( _mail ); <nl> + return - 1 ; <nl> + } <nl> fetch_field = get_body || <nl> ( data -> access_part & READ_BODY ) != 0 ? <nl> MAIL_FETCH_STREAM_BODY : MAIL_FETCH_STREAM_HEADER ;
void i_set_failure_prefix ( const char * prefix ) <nl> { <nl> i_free ( log_prefix ); <nl> log_prefix = i_strdup ( prefix ); <nl> - i_warning (" new prefix =% s ", prefix ); <nl> } <nl>  <nl> static int ATTR_FORMAT ( 2 , 0 )
static int mail_index_open_init ( MailIndex * index , int update_recent , <nl> index -> set_flags |= MAIL_INDEX_FLAG_REBUILD ; <nl> } <nl>  <nl> - return TRUE ; <nl> + /* finally reset the modify log marks , fsck or syncing might have <nl> + deleted some messages , and since we ' re only just opening the <nl> + index , there ' s no need to remember them */ <nl> + return mail_modifylog_mark_synced ( index -> modifylog ); <nl> } <nl>  <nl> static int mail_index_open_file ( MailIndex * index , const char * filename ,
cmd_save_to_mailbox ( struct save_cmd_context * ctx , struct mailbox * box , <nl> i_error (" open (% s ) failed : % s ", <nl> i_stream_get_name ( input ), <nl> i_stream_get_error ( input )); <nl> + ctx -> ctx . exit_code = EX_TEMPFAIL ; <nl> return - 1 ; <nl> } <nl> 
int mailbox_list_delete_trash ( const char * path , const char ** error_r ) <nl> errno = ELOOP ; <nl> return - 1 ; <nl> } <nl> + return - 1 ; <nl> } <nl> return 0 ; <nl> }
authd_abort_client ( struct Client * client_p ) <nl>  <nl> /* XXX should we blindly allow like this ? */ <nl> authd_decide_client ( client_p , "*", "*", true , '\ 0 ', NULL , NULL ); <nl> - <nl> client_p -> preClient -> authd_cid = 0 ; <nl> } <nl> 
add_conf_item ( const char * topconf , const char * name , int type , void (* func ) ( voi <nl> if (( tc = find_top_conf ( topconf )) == NULL ) <nl> return - 1 ; <nl>  <nl> - if (( cf = find_conf_item ( tc , name )) != NULL ) <nl> + if ( find_conf_item ( tc , name ) != NULL ) <nl> return - 1 ; <nl>  <nl> cf = rb_malloc ( sizeof ( struct ConfEntry ));
rb_get_ssl_certfp ( rb_fde_t * F , uint8_t certfp [ RB_SSL_CERTFP_LEN ]) <nl> res == X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT ) <nl> { <nl> memcpy ( certfp , cert -> sha1_hash , RB_SSL_CERTFP_LEN ); <nl> + X509_free ( cert ); <nl> return 1 ; <nl> } <nl> X509_free ( cert );
static int ldap_win_bind_auth ( LDAP * server , const char * user , <nl> const char * passwd , unsigned long authflags ) <nl> { <nl> ULONG method = 0 ; <nl> - SEC_WINNT_AUTH_IDENTITY cred = { 0 , }; <nl> + SEC_WINNT_AUTH_IDENTITY cred ; <nl> int rc = LDAP_AUTH_METHOD_NOT_SUPPORTED ; <nl>  <nl> + memset (& cred , 0 , sizeof ( cred )); <nl> + <nl> # if defined ( USE_SPNEGO ) <nl> if ( authflags & CURLAUTH_NEGOTIATE ) { <nl> method = LDAP_AUTH_NEGOTIATE ;
static CURLcode file_range ( struct connectdata * conn ) <nl> else { <nl> /* X - Y */ <nl> totalsize = to - from ; <nl> + if ( totalsize == CURL_OFF_T_MAX ) <nl> + /* this is too big to increase , so bail out */ <nl> + return CURLE_RANGE_ERROR ; <nl> data -> req . maxdownload = totalsize + 1 ; /* include last byte */ <nl> data -> state . resume_from = from ; <nl> DEBUGF ( infof ( data , " RANGE from %" CURL_FORMAT_CURL_OFF_T
static CURLcode tftp_rx ( tftp_state_data_t * state , tftp_event_t event ) <nl> } <nl>  <nl> /* Check if completed ( That is , a less than full packet is received ) */ <nl> - if ( state -> rbytes < sizeof ( state -> spacket )){ <nl> + if ( state -> rbytes < ( ssize_t ) sizeof ( state -> spacket )){ <nl> state -> state = TFTP_STATE_FIN ; <nl> } <nl> else {
ConnectionExists ( struct Curl_easy * data , <nl> if ( chosen ) { <nl> /* mark it as used before releasing the lock */ <nl> chosen -> inuse = TRUE ; <nl> + chosen -> data = data ; /* own it ! */ <nl> Curl_conncache_unlock ( needle ); <nl> * usethis = chosen ; <nl> return TRUE ; /* yes , we found one to use ! */
int cert_stuff ( struct connectdata * conn , <nl> EVP_PKEY_free ( pktmp ); <nl> } <nl>  <nl> -# if ! defined ( OPENSSL_NO_RSA ) <nl> +# if ! defined ( OPENSSL_NO_RSA ) && ! defined ( OPENSSL_IS_BORINGSSL ) <nl> { <nl> /* If RSA is used , don ' t check the private key if its flags indicate <nl> * it doesn ' t support it . */
Curl_cookie_add ( struct Curl_easy * data , <nl> /* too long individual name or contents , or too long combination of <nl> name + contents . Chrome and Firefox support 4095 or 4096 bytes <nl> combo . */ <nl> - free ( co ); <nl> + freecookie ( co ); <nl> infof ( data , " oversized cookie dropped , name / val % d + % d bytes \ n ", <nl> nlen , len ); <nl> return NULL ;
int Curl_select ( int nfds , <nl> SET_SOCKERRNO ( EINVAL ); <nl> return - 1 ; <nl> } <nl> - timeout_ms = ( timeout -> tv_sec * 1000 ) + ( timeout -> tv_usec / 1000 ); <nl> + timeout_ms = ( int )( timeout -> tv_sec * 1000 ) + ( int )( timeout -> tv_usec / 1000 ); <nl> } <nl> else { <nl> timeout_ms = - 1 ;
_CURL_WARNING ( _curl_easy_getinfo_err_curl_socket , <nl> # endif <nl>  <nl> /* evaluates to true if expr is of type FILE * */ <nl> -# define _curl_is_FILE ( expr ) \ <nl> - ( __builtin_types_compatible_p ( __typeof__ ( expr ), FILE *)) <nl> +# define _curl_is_FILE ( expr ) \ <nl> + ( _curl_is_NULL ( expr ) || \ <nl> + ( __builtin_types_compatible_p ( __typeof__ ( expr ), FILE *))) <nl>  <nl> /* evaluates to true if expr can be passed as POST data ( void * or char *) */ <nl> # define _curl_is_postfields ( expr ) \
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> if ( res == CURLE_OK ) { <nl> bool retry = Curl_retry_request ( conn , & newurl ); <nl>  <nl> - if ( retry ) <nl> + if ( retry ) { <nl> follow = FOLLOW_RETRY ; <nl> + if (! newurl ) <nl> + res = CURLE_OUT_OF_MEMORY ; <nl> + } <nl> else { <nl> /* <nl> * We must duplicate the new URL here as the connection data may
void curl_mime_free ( curl_mime * mime ) <nl> curl_mimepart * part ; <nl>  <nl> if ( mime ) { <nl> + mime_subparts_unbind ( mime ); /* Be sure it ' s not referenced anymore . */ <nl> while ( mime -> firstpart ) { <nl> part = mime -> firstpart ; <nl> mime -> firstpart = part -> nextpart ;
static void read_tcp_data ( ares_channel channel , fd_set * read_fds , time_t now ) <nl> * what ' s left to read of it ). <nl> */ <nl> count = recv ( server -> tcp_socket , <nl> - ( void *)( server -> tcp_lenbuf + server -> tcp_buffer_pos ), <nl> - 2 - server -> tcp_buffer_pos , 0 ); <nl> + ( void *)( server -> tcp_lenbuf + server -> tcp_lenbuf_pos ), <nl> + 2 - server -> tcp_lenbuf_pos , 0 ); <nl> if ( count <= 0 ) <nl> { <nl> handle_error ( channel , i , now );
CURLcode Curl_ntlm_create_type1_message ( const char * userp , <nl> *( dup_domain . tchar_ptr + domlen ) = TEXT ('\ 0 '); <nl> ntlm -> identity . Domain = dup_domain . tbyte_ptr ; <nl> ntlm -> identity . DomainLength = curlx_uztoul ( domlen ); <nl> - free ( dup_domain . tchar_ptr ); <nl> dup_domain . tchar_ptr = NULL ; <nl>  <nl> Curl_unicodefree ( useranddomain . tchar_ptr );
void Curl_sasl_digest_cleanup ( struct digestdata * digest ) <nl> * This is used to generate an already encoded NTLM type - 1 message ready for <nl> * sending to the recipient . <nl> * <nl> -* Note : This is a simple wrapper of the NTLM function which means that any <nl> -* SASL based protocols don ' t have to include the NTLM functions directly . <nl> -* <nl> * Parameters : <nl> * <nl> * userp [ in ] - The user name in the format User or Domain \ User .
static CURLcode AddFormData ( struct FormData ** formp , <nl> file */ <nl> if (! strequal ("-", newform -> line )) { <nl> struct_stat file ; <nl> - if (! stat ( newform -> line , & file ) && S_ISREG ( file . st_mode )) <nl> + if (! stat ( newform -> line , & file ) && ! S_ISDIR ( file . st_mode )) <nl> * size += file . st_size ; <nl> else <nl> return CURLE_BAD_FUNCTION_ARGUMENT ;
CURLcode Curl_sasl_create_digest_md5_message ( struct SessionHandle * data , <nl> " qop =% s ", <nl> userp , realm , nonce , <nl> cnonce , nonceCount , spn , resp_hash_hex , qop ); <nl> + Curl_safefree ( spn ); <nl> if (! response ) <nl> return CURLE_OUT_OF_MEMORY ; <nl>  <nl> CURLcode Curl_sasl_create_digest_md5_message ( struct SessionHandle * data , <nl> result = Curl_base64_encode ( data , response , 0 , outptr , outlen ); <nl>  <nl> Curl_safefree ( response ); <nl> - Curl_safefree ( spn ); <nl>  <nl> return result ; <nl> }
# define OS " AmigaOS " <nl>  <nl> # define PACKAGE " curl " <nl> -# define PACKAGE_BUGREPORT " a suitable curl mailing list : https :// curl . haxx . se / mail /" <nl> +# define PACKAGE_BUGREPORT " a suitable mailing list : https :// curl . haxx . se / mail /" <nl> # define PACKAGE_NAME " curl " <nl> # define PACKAGE_STRING " curl -" <nl> # define PACKAGE_TARNAME " curl "
static CURLcode ssh_statemach_act ( struct connectdata * conn , bool * block ) <nl> figure out a " real " bitmask */ <nl> sshc -> orig_waitfor = data -> req . keepon ; <nl>  <nl> + /* since we don ' t really wait for anything at this point , we want the <nl> + state machine to move on as soon as possible so we set a very short <nl> + timeout here */ <nl> + Curl_expire ( data , 1 ); <nl> + <nl> state ( conn , SSH_STOP ); <nl> } <nl> break ;
# undef ssize_t <nl>  <nl> /* Define this to ' int ' if socklen_t is not an available typedefed type */ <nl> -# undef socklen_t size_t <nl> +# define socklen_t size_t <nl>  <nl> /* Define this as a suitable file to read random data from */ <nl> # undef RANDOM_FILE
static CURLcode imap_disconnect ( struct connectdata * conn ) <nl>  <nl> Curl_pp_disconnect (& imapc -> pp ); <nl>  <nl> + free ( imapc -> mailbox ); <nl> + <nl> return CURLE_OK ; <nl> } <nl> 
CURLcode Curl_conncache_add_conn ( struct conncache * connc , <nl> return result ; <nl>  <nl> key = hashkey ( conn ); <nl> - if (! key ) <nl> + if (! key ) { <nl> + bundle_destroy ( new_bundle ); <nl> return CURLE_OUT_OF_MEMORY ; <nl> + } <nl>  <nl> rc = conncache_add_bundle ( data -> state . conn_cache , key , new_bundle ); <nl> free ( key );
CURLMcode Curl_pipeline_set_site_blacklist ( char ** sites , <nl> bool Curl_pipeline_server_blacklisted ( struct SessionHandle * handle , <nl> char * server_name ) <nl> { <nl> - if ( handle -> multi ) { <nl> + if ( handle -> multi && server_name ) { <nl> struct curl_llist * blacklist = <nl> Curl_multi_pipelining_server_bl ( handle -> multi ); <nl> 
CURLcode Curl_setopt ( struct Curl_easy * data , CURLoption option , <nl> arg = va_arg ( param , long ); <nl> if ( arg < CURLSSH_AUTH_NONE ) <nl> return CURLE_BAD_FUNCTION_ARGUMENT ; <nl> - data -> set . ssh_auth_types = va_arg ( param , long ); <nl> + data -> set . ssh_auth_types = arg ; <nl> break ; <nl>  <nl> case CURLOPT_SSH_PUBLIC_KEYFILE :
CURLcode Curl_is_resolved ( struct connectdata * conn , <nl> if ( conn -> async . done ) { <nl> /* we ' re done , kill the ares handle */ <nl> if (! conn -> async . dns ) { <nl> - failf ( data , " Could not resolve host : % s (% s )", conn -> name , <nl> + failf ( data , " Could not resolve host : % s (% s )", conn -> host . dispname , <nl> ares_strerror ( conn -> async . status )); <nl> return CURLE_COULDNT_RESOLVE_HOST ; <nl> }
void Curl_updateconninfo ( struct connectdata * conn , curl_socket_t sockfd ) <nl> struct SessionHandle * data = conn -> data ; <nl> struct PureInfo * info = & conn -> data -> info ; <nl>  <nl> + if ( conn -> bits . reuse ) <nl> + /* reusing same connection */ <nl> + return ; <nl> + <nl> len = sizeof ( struct Curl_sockaddr_storage ); <nl> if ( getpeername ( sockfd , ( struct sockaddr *) & ssrem , & len )) { <nl> error = SOCKERRNO ;
mbed_connect_step3 ( struct connectdata * conn , <nl>  <nl> ret = mbedtls_ssl_get_session (& connssl -> ssl , our_ssl_sessionid ); <nl> if ( ret ) { <nl> + free ( our_ssl_sessionid ); <nl> failf ( data , " mbedtls_ssl_get_session returned - 0x % x ", - ret ); <nl> return CURLE_SSL_CONNECT_ERROR ; <nl> }
CURLcode http_auth_headers ( struct connectdata * conn , <nl> if (! data -> state . authstage ) { <nl> if ( conn -> bits . httpproxy && conn -> bits . proxy_user_passwd ) <nl> Curl_http_auth_stage ( data , 407 ); <nl> - else <nl> + else if ( conn -> bits . user_passwd ) <nl> Curl_http_auth_stage ( data , 401 ); <nl> + else <nl> + return CURLE_OK ; /* no authentication with no user or password */ <nl> } <nl>  <nl> /* To prevent the user + password to get sent to other than the original
CURLcode Curl_output_ntlm_wb ( struct connectdata * conn , <nl> conn -> response_header = NULL ; <nl> break ; <nl> case NTLMSTATE_TYPE2 : <nl> - input = aprintf (" TT % s ", conn -> challenge_header ); <nl> + input = aprintf (" TT % s \ n ", conn -> challenge_header ); <nl> if (! input ) <nl> return CURLE_OUT_OF_MEMORY ; <nl> res = ntlm_wb_response ( conn , input , ntlm -> state );
static GlobCode glob_range ( URLGlob * glob , char ** patternp , <nl> } <nl> else <nl> step_n = 1 ; <nl> - if (* endp == ']') { <nl> + if ( endp && (* endp == ']')) { <nl> pattern = endp + 1 ; <nl> } <nl> else
_hb_ot_layout_set_glyph_class ( hb_face_t * face , <nl> unsigned char * new_klasses ; <nl>  <nl> new_len = len == 0 ? 120 : 2 * len ; <nl> - if ( new_len > 65535 ) <nl> - new_len = 65535 ; <nl> + if ( new_len > 65536 ) <nl> + new_len = 65536 ; <nl> new_klasses = ( unsigned char *) realloc ( layout -> new_gdef . klasses , new_len * sizeof ( unsigned char )); <nl>  <nl> if ( HB_UNLIKELY (! new_klasses ))
_pango_emoji_iter_next ( PangoEmojiIter * iter ) <nl> if ( iter -> is_emoji == PANGO_EMOJI_TYPE_IS_EMOJI ( current_emoji_type )) <nl> { <nl> iter -> is_emoji = ! PANGO_EMOJI_TYPE_IS_EMOJI ( current_emoji_type ); <nl> + <nl> + /* Make sure we make progress . Weird sequences , like a VC15 followed <nl> + * by VC16 , can trick us into stalling otherwise . */ <nl> + if ( iter -> start == iter -> end ) <nl> + iter -> end = g_utf8_next_char ( iter -> end ); <nl> + <nl> return TRUE ; <nl> } <nl> }
char * pbse_to_txt ( int err ) <nl> exit ( 1 ); <nl> } <nl>  <nl> + int trq_cg_remove_process_from_accts ( job * pjob ) <nl> + { <nl> + return ( PBSE_NONE ); <nl> + }
void * queue_route ( void * vp ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - void acct_close ( void ) <nl> + void acct_close ( bool ) <nl> { <nl> fprintf ( stderr , " The call to acct_close needs to be mocked !!\ n "); <nl> exit ( 1 );
# include < stdlib . h > <nl> # include < stdio . h > <nl> # include " pbs_error . h " <nl> +# include " momctl . h " <nl>  <nl> extern int flush_rc ; <nl> extern char * string_read ;
void check_busy ( double mla ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - void mom_is_request ( struct tcp_chan * chan , int version , int * cmdp ) <nl> + void mom_is_request ( struct tcp_chan * chan , int version , int * cmdp , struct sockaddr_in * pSockAddr ) <nl> { <nl> fprintf ( stderr , " The call to mom_is_request needs to be mocked !!\ n "); <nl> exit ( 1 );
int trq_main ( <nl> { <nl> printf (" Daemon exit requested \ n "); <nl> } <nl> - if ( trq_server_ip != NULL ) <nl> - free ( trq_server_ip ); <nl> if ( the_key != NULL ) <nl> free ( the_key ); <nl> return rc ;
bool Chip :: spread_place_cores ( <nl>  <nl> if ( fits == true ) <nl> { <nl> - int step_count = 1 ; <nl> + int step_count = step ; <nl> + <nl> + if ( lprocs_per_task_remaining == 1 ) <nl> + step_count = 1 ; <nl> + <nl>  <nl> /* cores_placed and cores_to_fill are used because we only want to make sure we <nl> fill the number of cores for this task */
int delete_cpuset ( <nl> */ <nl> else if (! strcmp ( pdirent -> d_name , " tasks ")) <nl> { <nl> + slept = 0 ; <nl> + <nl> do <nl> { <nl> npids = 0 ; <nl> - slept = 0 ; <nl> if (( fd = fopen ( path , " r ")) != NULL ) <nl> { <nl> while (( fgets ( tid , sizeof ( tid ), fd )) != NULL )
job * job_clone ( <nl> pnewjob -> ji_qs . ji_jobid [ PBS_MAXSVRJOBID ] = '\ 0 '; <nl> snprintf ( pnewjob -> ji_qs . ji_jobid , PBS_MAXSVRJOBID ,"% s -% d .% s ", <nl> oldid , taskid , hostname ); <nl> - <nl> + free ( oldid ); <nl> /* update the job filename <nl> * We could optimize the sub - jobs to all use the same file . We would need a <nl> * way to track the number of tasks still using the job file so we know when
void Chip :: calculateStepCounts ( <nl> int & place_count_remaining ) <nl>  <nl> { <nl> + if ( lprocs_per_task == 0 ) <nl> + { <nl> + step = 0 ; <nl> + step_remainder = processing_units_per_task ; <nl> + place_count = 0 ; <nl> + return ; <nl> + } <nl> + <nl> if ( lprocs_per_task == 1 ) <nl> { <nl> step = ( processing_units_per_task / 2 ) + 1 ;
int procs_requested ( <nl> if ( proplist (& str , & prop , & num_procs , & num_gpus , & num_mics )) <nl> { <nl> free ( tmp_spec ); <nl> + if ( prop != NULL ) <nl> + free_prop ( prop ); <nl> return (- 1 ); <nl> } <nl> } <nl> int procs_requested ( <nl> { <nl> /* must be a prop list with no number in front */ <nl> free ( tmp_spec ); <nl> + if ( prop != NULL ) <nl> + free_prop ( prop ); <nl>  <nl> return (- 1 ); <nl> }
 <nl> START_TEST ( test_one ) <nl> { <nl> + /* As this is site specific , there is no implementation in this function */ <nl> + char * user = NULL ; <nl> + char * host = NULL ; <nl> + int rc = - 1 ; <nl> + rc = site_allow_u ( user , host ); <nl> + fail_unless ( rc == 0 , " The return value has changed !!!"); <nl> } <nl> END_TEST <nl> 
work_task * next_task ( <nl> pthread_mutex_lock ( at -> alltasks_mutex ); <nl>  <nl> wt = next_thing ( at -> ra , iter ); <nl> + if ( wt != NULL ) <nl> + pthread_mutex_lock ( wt -> wt_mutex ); <nl>  <nl> pthread_mutex_unlock ( at -> alltasks_mutex ); <nl>  <nl> if ( wt != NULL ) <nl> { <nl> - pthread_mutex_lock ( wt -> wt_mutex ); <nl> - <nl> if ( wt -> wt_being_recycled == TRUE ) <nl> { <nl> pthread_mutex_unlock ( wt -> wt_mutex );
 <nl> # define MAX_UPDATES_BEFORE_SENDING 20 <nl> # define PMOMTCPTIMEOUT 60 /* duration in seconds mom TCP requests will block */ <nl> +# define TCP_READ_PROTO_TIMEOUT 2 <nl>  <nl> /* Global Data Items */ <nl>  <nl> int tcp_read_proto_version ( <nl>  <nl> tmpT = pbs_tcp_timeout ; <nl>  <nl> - pbs_tcp_timeout = 0 ; <nl> + pbs_tcp_timeout = TCP_READ_PROTO_TIMEOUT ; <nl>  <nl> * proto = disrsi ( chan , & rc ); <nl> 
struct pbsnode * find_fitting_node ( <nl> if (( pnode = check_node ( ln , needed )) != NULL ) <nl> { <nl> ln -> times_used ++; <nl> + free_resizable_array ( ordered ); <nl> return ( pnode ); <nl> } <nl> }
int read_config ( <nl> return ( 0 ); <nl> } <nl>  <nl> + void free_pwnam ( struct passwd * pwdp , char * buf ) <nl> + {}
void main_func ( <nl> script_tmp , /* O */ <nl> & ji )) != 0 ) <nl> { <nl> + fclose ( script_fp ); <nl> unlink ( script_tmp ); <nl>  <nl> exit ( 1 );
static char * active_pbs_server ; <nl> pbs_net_t trq_server_addr ; <nl> char trq_hostname [ PBS_MAXSERVERNAME + 1 ]; <nl>  <nl> +/* Get the name of the active pbs_server */ <nl> int load_trqauthd_config ( <nl>  <nl> char ** default_server_name ,
int pres_process_body ( publ_info_t * publ , str ** fin_body , int ver , str ** tuple_pa <nl> { <nl> if ( tuple == NULL ) <nl> { <nl> + if ( strlen ( tuple_id )>= 50 ) { <nl> + LM_ERR (" tuple id is too long : % s \ n ", tuple_id ); <nl> + goto error ; <nl> + } <nl> strcpy ( buf , tuple_id ); <nl> xmlFree ( tuple_id ); <nl> tuple_id = buf ;
# include "../../ mem / mem . h " <nl> # include "../../ md5utils . h " <nl> # include "../../ ip_addr . h " <nl> +# include "../../ parser / parse_uri . h " <nl>  <nl> # include " config . h " <nl> # include " lock . h "
__dialog_created ( struct dlg_cell * dlg , int type , struct dlg_cb_params * _params ) <nl> ( include_req_uri )?&( dlg -> req_uri ):&( dlg -> to_uri ), <nl> &( dlg -> callid ), 1 , dlginfo -> lifetime , <nl> 0 , 0 , 0 , 0 , ( send_publish_flag ==- 1 )? 1 : 0 ); <nl> - free_dlginfo_cell ( dlginfo ); <nl>  <nl> } <nl> 
int rx_send_str ( str * rx_session_id ) { <nl> // so just wait for STA or for Grace Timout to happen <nl> LM_DBG (" Hmmm , auth session already in disconnected state \ n "); <nl> cdpb . AAASessionsUnlock ( auth -> hash ); <nl> - CSCF_RETURN_FALSE ; <nl> + return CSCF_RETURN_FALSE ; <nl> } <nl>  <nl> LM_DBG (" Creating STR \ n ");
void clean_hdr_field ( struct hdr_field * hf ) <nl> break ; <nl>  <nl> case HDR_SESSIONEXPIRES_T : <nl> + if (* h_parsed ) { <nl> + (( hf_parsed_t *)(* h_parsed ))-> hfree (* h_parsed ); <nl> + * h_parsed = 0 ; <nl> + } <nl> + break ; <nl> + <nl> case HDR_MIN_SE_T : <nl> case HDR_ACCEPTCONTACT_T : <nl> case HDR_ALLOWEVENTS_T :
static int rtpproxy_set_store ( modparam_t type , void * val ){ <nl> return - 1 ; <nl> } <nl> } else {/* realloc to make room for the current set */ <nl> - rtpp_strings = ( char **) pkg_realloc ( rtpp_strings , <nl> + rtpp_strings = ( char **) pkg_reallocxf ( rtpp_strings , <nl> ( rtpp_sets + 1 )* sizeof ( char *)); <nl> if (! rtpp_strings ){ <nl> LM_ERR (" no pkg memory left \ n ");
inline static str * binrpc_val_conv_str ( struct binrpc_ctx * ctx , <nl> s = int2str ( v -> u . intval , & len ); <nl> ret = ctl_malloc ( sizeof (* ret )+ len + 1 ); <nl> if ( ret == 0 || binrpc_gc_track ( ctx , ret )!= 0 ){ <nl> + if ( ret != 0 ) ctl_free ( ret ); <nl> * err = E_BINRPC_OVERFLOW ; <nl> return 0 ; <nl> }
void tm_ctx_set_branch_index ( int v ); <nl>  <nl> # else <nl>  <nl> -# define tm_ctx_get () <nl> +# define tm_ctx_get () NULL <nl> # define tm_ctx_init () <nl> # define tm_ctx_set_branch_index ( v ) <nl> 
if ( rtplen != prtpstat -> len ) <nl> LM_ERR (" Unable to find RTPSTAT pv !\ n "); <nl> goto initerr ; <nl> } <nl> - prtp_pv = pv_cache_get (& prtpstat [ 0 ]); <nl> + prtp_pv = pv_cache_get ( prtpstat ); <nl> if (! prtp_pv ) <nl> { <nl> LM_ERR (" Unable to find pv spec for RTPSTAT !\ n ");
kz_amqp_zone_ptr kz_primary_zone = NULL ; <nl> amqp_exchange_declare_ok_t * AMQP_CALL kz_amqp_exchange_declare ( amqp_connection_state_t state , amqp_channel_t channel , <nl> amqp_bytes_t exchange , amqp_bytes_t type , <nl> amqp_boolean_t passive , amqp_boolean_t durable , amqp_table_t arguments ) { <nl> -# if AMQP_VERSION_MINOR == 5 <nl> +# if AMQP_VERSION_MAJOR == 0 && AMQP_VERSION_MINOR < 6 <nl> return amqp_exchange_declare ( state , channel , exchange , type , passive , durable , arguments ); <nl> # else <nl> return amqp_exchange_declare ( state , channel , exchange , type , passive , durable , 0 , 0 , arguments );
static void mod_destroy ( void ) <nl> free_atom_fmt_buff (); <nl> free_list_fmt_buff (); <nl> free_xbuff_fmt_buff (); <nl> + free_pid_fmt_buff (); <nl> } <nl>  <nl> static int postprocess_request ( struct sip_msg * msg , unsigned int flags , void * _param ) <nl> static int postprocess_request ( struct sip_msg * msg , unsigned int flags , void * _p <nl> free_atom_fmt_buff (); <nl> free_list_fmt_buff (); <nl> free_xbuff_fmt_buff (); <nl> + free_pid_fmt_buff (); <nl> return 0 ; <nl> } <nl> 
static int init_mi_uptime ( void ) <nl> { <nl> char * p ; <nl>  <nl> + if ( kmi_up_since_ctime . s != 0 ) <nl> + return 0 ; <nl> time (& kmi_up_since ); <nl> p = ctime (& kmi_up_since ); <nl> kmi_up_since_ctime . len = strlen ( p )- 1 ;
int t_load_contacts ( struct sip_msg * msg , char * key , char * value ) <nl> return - 1 ; <nl> } <nl>  <nl> + memset ( next , 0 , sizeof ( struct contact )); <nl> next -> uri . s = branch -> uri ; <nl> next -> uri . len = branch -> len ; <nl> next -> dst_uri . s = branch -> dst_uri ;
static int mod_init ( void ) <nl> */ <nl> static int child_init ( int rank ) <nl> { <nl> + if ( rank == PROC_INIT ) { <nl> + return 0 ; <nl> + } <nl> _apy_process_rank = rank ; <nl> PyOS_AfterFork (); <nl> return apy_init_script ( rank );
int db_postgres_store_result ( const db1_con_t * _con , db1_res_t ** _r ) <nl> } <nl>  <nl> done : <nl> - db_postgres_free_query ( _con ); <nl> return ( rc ); <nl> } <nl> 
static int mi_child_init ( void ) <nl> return - 1 ; <nl> } <nl> } <nl> + <nl> + if ( sruid_init (& _ul_sruid , '-', " ulcx ", SRUID_INC )< 0 ) <nl> + return - 1 ; <nl> done = 1 ; <nl>  <nl> return 0 ;
void rpc_shv_set ( rpc_t * rpc , void * c ) <nl> rpc -> fault ( c , 500 , " Cannot set shared variable value "); <nl> LM_ERR (" cannot set shv value \ n "); <nl> } else { <nl> - rpc -> printf ( c , " Ok . Variable set to new value ."); <nl> + rpc -> rpl_printf ( c , " Ok . Variable set to new value ."); <nl> } <nl>  <nl> unlock_shvar ( shv );
static struct timeval time_from_string ( str * time_value ) <nl> return time_error ; <nl> } <nl>  <nl> - return ( struct timeval ) { atoi ( zero_terminated_value ), <nl> - atoi ( dot_address + 1 )}; <nl> + time_res -> tv_sec = strtol ( zero_terminated_value , ( char **) NULL , 10 ); <nl> + time_res -> tv_usec = strtol ( dot_address + 1 , ( char **) NULL , 10 ); <nl> + return 0 ; <nl> } <nl>  <nl> /* set the duration in the dialog struct */
struct module_exports exports = { <nl> static int mod_init ( void ) { <nl> struct stat fs ; <nl>  <nl> + if ( register_mi_mod ( exports . name , mi_cmds )!= 0 ) <nl> + { <nl> + LM_ERR (" failed to register MI commands \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> subscriber_table . len = strlen ( subscriber_table . s ); <nl> subscriber_username_col . len = strlen ( subscriber_username_col . s ); <nl> subscriber_domain_col . len = strlen ( subscriber_domain_col . s );
static int do_load_gws ( struct sip_msg * _m , int grp_id ) <nl> } <nl> from_uri = get_from ( _m )-> uri ; <nl> } <nl> - if ( from_uri . len < MAX_FROM_URI_LEN ) { <nl> + if ( from_uri . len <= MAX_FROM_URI_LEN ) { <nl> strncpy ( from_uri_str , from_uri . s , from_uri . len ); <nl> from_uri_str [ from_uri . len ] = '\ 0 '; <nl> } else {
int tmx_check_pretran ( sip_msg_t * msg ) <nl> if ( likely ( vbr != NULL )) { <nl> svbranch = vbr -> value ; <nl> trim (& svbranch ); <nl> - dsize += svbranch . len ; <nl> + dsize += svbranch . len + 1 ; <nl> } <nl> if ( dsize < 256 ) dsize = 256 ; <nl> 
rs_set_warmth_auto ( RS_BLOB * rs ) <nl> gdouble dsum [ 8 ], dmax ; <nl> gfloat tint , warmth ; <nl>  <nl> + if ( unlikely (! rs -> in_use )) return ; <nl> + <nl> for ( row = 0 ; row < rs -> input -> h - 7 ; row += 8 ) <nl> for ( col = 0 ; col < rs -> input -> w - 7 ; col += 8 ) <nl> {
makernote_nikon ( RAWFILE * rawfile , guint offset , RSMetadata * meta ) <nl> || g_str_equal ( meta -> model_ascii , " NIKON 1 V1 ") <nl> || g_str_equal ( meta -> model_ascii , " NIKON 1 V2 ") <nl> || g_str_equal ( meta -> model_ascii , " NIKON D7000 ") <nl> + || g_str_equal ( meta -> model_ascii , " NIKON D7100 ") <nl> || g_str_equal ( meta -> model_ascii , " COOLPIX P7700 ")) <nl> { <nl> meta -> cam_mul [ 0 ] = get_rational ( rawfile , offset );
gui_drawingarea_motion_callback ( GtkWidget * widget , GdkEventMotion * event , RS_BLO <nl> gint y = ( gint ) event -> y ; <nl> gushort * pixel ; <nl>  <nl> + if (! rs -> photo ) return FALSE ; <nl> + <nl> /* Draw RGB - values at bottom of screen */ <nl> gui_set_values ( rs , x , y ); <nl> 
ACTION ( copy_settings ) <nl>  <nl> ACTION ( paste_settings ) <nl> { <nl> - gint mask ; <nl> + gint mask = 0xffffff ; <nl>  <nl> GtkWidget * dialog , * cb_box ; <nl> GtkWidget * cb_exposure , * cb_saturation , * cb_hue , * cb_contrast , * cb_whitebalance , * cb_curve , * cb_sharpen ;
GtkWidget * <nl> gui_make_scale_from_adj ( RS_BLOB * rs , GCallback cb , GtkObject * adj , gint mask ) <nl> { <nl> GtkWidget * hscale , * box , * rimage , * revent ; <nl> - struct reset_carrier * rc = malloc ( sizeof ( struct reset_carrier )); <nl> + struct reset_carrier * rc = g_malloc ( sizeof ( struct reset_carrier )); <nl> rc -> rs = rs ; <nl> rc -> mask = mask ; <nl> 
static void dump_index ( demuxer_t * demuxer , int stream_id ) <nl> if ( verbose <= 1 ) <nl> return ; <nl>  <nl> - if ( stream_id > MAX_STREAMS ) <nl> + if ( stream_id >= MAX_STREAMS ) <nl> return ; <nl>  <nl> index = priv -> index_table [ stream_id ];
bool fbotex_change ( struct fbotex * fbo , GL * gl , struct mp_log * log , int w , int h , <nl>  <nl> GLenum filter = fbo -> tex_filter ; <nl>  <nl> + fbotex_uninit ( fbo ); <nl> + <nl> * fbo = ( struct fbotex ) { <nl> . gl = gl , <nl> . rw = w ,
# include " config . h " <nl> # include " mp_msg . h " <nl>  <nl> -# include " fastmemcpy . h " <nl> +# include "../../ libvo / fastmemcpy . h " <nl>  <nl> # include " libmpdemux / nuppelvideo . h " <nl> # include " RTjpegN . h "
int mp_header_process_sequence_header ( mp_mpeg_header_t * picture , const unsigne <nl> picture -> mpeg1 = 1 ; <nl> picture -> picture_structure = 3 ; // FRAME_PICTURE ; <nl> picture -> display_time = 100 ; <nl> + picture -> frame_rate_extension_n = 1 ; <nl> + picture -> frame_rate_extension_d = 1 ; <nl> return 0 ; <nl> } <nl> 
static int control ( struct af_instance_s * af , int cmd , void * arg ) <nl> *( float *) arg = s -> scale ; <nl> return AF_OK ; <nl> case AF_CONTROL_COMMAND_LINE :{ <nl> - strarg_t speed ; <nl> + strarg_t speed = {}; <nl> opt_t subopts [] = { <nl> {" scale ", OPT_ARG_FLOAT , & s -> scale_nominal , NULL }, <nl> {" stride ", OPT_ARG_FLOAT , & s -> ms_stride , NULL },
static void drm_egl_uninit ( MPGLContext * ctx ) <nl>  <nl> static int drm_egl_init ( struct MPGLContext * ctx , int flags ) <nl> { <nl> + if ( ctx -> vo -> probing ) { <nl> + MP_VERBOSE ( ctx -> vo , " DRM EGL backend can be activated only manually .\ n "); <nl> + return - 1 ; <nl> + } <nl> struct priv * p = ctx -> priv ; <nl> p -> kms = NULL ; <nl> p -> old_crtc = NULL ;
static void pass_prepare_src_tex ( struct gl_video * p ) <nl> static void render_pass_quad ( struct gl_video * p , int vp_w , int vp_h , <nl> const struct mp_rect * dst , int flags ) <nl> { <nl> - struct vertex va [ 4 ]; <nl> + struct vertex va [ 4 ] = { 0 }; <nl>  <nl> struct gl_transform t ; <nl> gl_transform_ortho (& t , 0 , vp_w , 0 , vp_h );
static mp_cmd_t * interpret_key ( struct input_ctx * ictx , int code ) <nl> code &= ~ KEY_MODIFIER_SHIFT ; <nl>  <nl> if ( code & MP_KEY_DOWN ) { <nl> - if ( ictx -> num_key_down > MP_MAX_KEY_DOWN ) { <nl> + if ( ictx -> num_key_down >= MP_MAX_KEY_DOWN ) { <nl> mp_tmsg ( MSGT_INPUT , MSGL_ERR , " Too many key down events " <nl> " at the same time \ n "); <nl> return NULL ;
static int demux_mkv_read_tags ( demuxer_t * demuxer ) <nl> demux_info_add_bstr ( demuxer , tag . simple_tag [ j ]. tag_name , tag . simple_tag [ j ]. tag_string ); <nl> } <nl>  <nl> + talloc_free ( parse_ctx . talloc_ctx ); <nl> return 0 ; <nl> } <nl> 
static void ao_chain_uninit ( struct ao_chain * ao_c ) <nl> talloc_free ( ao_c -> conv ); <nl> talloc_free ( ao_c -> input_frame ); <nl> talloc_free ( ao_c -> input_format ); <nl> + talloc_free ( ao_c -> output_frame ); <nl> talloc_free ( ao_c -> filter_input_format ); <nl> talloc_free ( ao_c -> ao_buffer ); <nl> talloc_free ( ao_c );
static int find_entrypoint ( int format , VAEntrypoint * ep , int num_ep ) <nl>  <nl> static int is_direct_mapping ( VADisplay display ) <nl> { <nl> - VADisplayAttribute attr ; <nl> + VADisplayAttribute attr = { 0 }; <nl> VAStatus status ; <nl>  <nl> # if VA_CHECK_VERSION ( 0 , 34 , 0 )
void demux_seek_mpg ( demuxer_t * demuxer , float rel_seek_secs , float audio_delay , in <nl> continue ; <nl> } <nl> } <nl> + if (! sh_video ) break ; <nl> i = sync_video_packet ( d_video ); <nl> if ( sh_video -> format == 0x10000004 ) { // mpeg4 <nl> if ( i == 0x1B6 ) { // vop ( frame ) startcode
struct mp_csp_equalizer * gl_video_eq_ptr ( struct gl_video * p ) <nl> // Call when the mp_csp_equalizer returned by gl_video_eq_ptr () was changed . <nl> void gl_video_eq_update ( struct gl_video * p ) <nl> { <nl> + gl_video_reset_surfaces ( p ); <nl> } <nl>  <nl> static int validate_scaler_opt ( struct mp_log * log , const m_option_t * opt ,
static const struct gl_functions gl_functions [] = { <nl> // uniform buffer object extensions , requires OpenGL 3 . 1 . <nl> { <nl> . ver_core = 310 , <nl> - . extension = " ARB_uniform_buffer_object ", <nl> + . extension = " GL_ARB_uniform_buffer_object ", <nl> . functions = ( const struct gl_function []) { <nl> DEF_FN ( GetUniformBlockIndex ), <nl> DEF_FN ( UniformBlockBinding ),
static void print_status ( float a_pos , float a_v , float corr ) <nl> width = screen_width ; <nl> else <nl> width = 80 ; <nl> +# ifdef WIN32 <nl> + // windows command line is broken ( MinGW ' s rxvt works though , but we <nl> + // should not depend on that ). <nl> + width --; <nl> +# endif <nl> line = malloc ( width + 1 ); // one additional for terminating null <nl>  <nl> // Audio time
struct exports exp_msvcr80 []={ <nl> FF ( _initterm_e , - 1 ) <nl> FF ( _initterm , - 1 ) <nl> FF ( _decode_pointer , - 1 ) <nl> +/* needed by KGV1 - VFW . dll */ <nl> + {"?? 2 @ YAPAXI @ Z ", - 1 , expnew }, <nl> + {"?? 3 @ YAXPAX @ Z ", - 1 , expdelete } <nl> }; <nl>  <nl> struct exports exp_msvcp60 []={
d_dvdsub = demuxer -> sub ; <nl> sh_audio = d_audio -> sh ; <nl> sh_video = d_video -> sh ; <nl>  <nl> + if (! sh_video ) <nl> + { <nl> + mp_msg ( MSGT_CPLAYER , MSGL_FATAL ," Video stream is mandatory !\ n "); <nl> + mencoder_exit ( 1 , NULL ); <nl> + } <nl> + <nl> if (! video_read_properties ( sh_video )){ <nl> printf ( MSGTR_CannotReadVideoProperties ); <nl> mencoder_exit ( 1 , NULL );
void gl_video_render_frame ( struct gl_video * p , struct vo_frame * frame , int fbo ) <nl> GL * gl = p -> gl ; <nl> struct video_image * vimg = & p -> image ; <nl>  <nl> + if ( fbo && !( gl -> mpgl_caps & MPGL_CAP_FB )) { <nl> + MP_FATAL ( p , " Rendering to FBO requested , but no FBO extension found !\ n "); <nl> + return ; <nl> + } <nl> + <nl> p -> broken_frame = false ; <nl>  <nl> gl -> BindFramebuffer ( GL_FRAMEBUFFER , fbo );
static void uninit ( struct dec_audio * da ) <nl> av_freep (& lavf_ctx -> pb -> buffer ); <nl> av_freep (& lavf_ctx -> pb ); <nl> avformat_free_context ( lavf_ctx ); <nl> + spdif_ctx -> lavf_ctx = NULL ; <nl> } <nl> } <nl> 
void gl_video_upload_image ( struct gl_video * p , struct mp_image * mpi ) <nl> p -> osd_pts = mpi -> pts ; <nl>  <nl> if ( p -> hwdec_active ) { <nl> + talloc_free ( vimg -> hwimage ); <nl> vimg -> hwimage = mpi ; <nl> p -> have_image = true ; <nl> return ;
const struct ao_driver audio_out_null = { <nl> . priv_size = sizeof ( struct priv ), <nl> . priv_defaults = &( const struct priv ) { <nl> . bufferlen = 0 . 2 , <nl> - . latency_sec = 0 . 5 , <nl> . outburst = 256 , <nl> . speed = 1 , <nl> },
int mpcodecs_config_vo ( sh_video_t * sh , int w , int h , unsigned int preferred_outf <nl> } <nl> } <nl>  <nl> + if ( video_out -> get_info ) <nl> { const vo_info_t * info = video_out -> get_info (); <nl> mp_msg ( MSGT_CPLAYER , MSGL_INFO ," VO : [% s ] % dx % d => % dx % d % s % s % s % s % s \ n ", info -> short_name , <nl> sh -> disp_w , sh -> disp_h ,
static HRESULT init_session_display ( struct wasapi_state * state ) { <nl> exit_label : <nl> MP_ERR ( state , " Error setting audio session display name : % s ( 0x %" PRIx32 ")\ n ", <nl> wasapi_explain_err ( hr ), ( uint32_t ) hr ); <nl> - return hr ; <nl> + // No reason to abort initialization . <nl> + return S_OK ; <nl> } <nl>  <nl> static HRESULT fix_format ( struct ao * ao )
int streaming_bufferize ( streaming_ctrl_t * streaming_ctrl , char * buffer , int siz <nl> int nop_streaming_read ( int fd , char * buffer , int size , streaming_ctrl_t * stream_ctrl ); <nl> int nop_streaming_seek ( int fd , off_t pos , streaming_ctrl_t * stream_ctrl ); <nl>  <nl> + int connect2Server ( char * host , int port ); <nl> + <nl> # endif
 <nl> static int av_log_level_to_mp_level ( int av_level ) <nl> { <nl> + if ( av_level > AV_LOG_VERBOSE ) <nl> + return MSGL_DBG2 ; <nl> if ( av_level > AV_LOG_INFO ) <nl> return MSGL_V ; <nl> if ( av_level > AV_LOG_WARNING )
void add_subtitles ( char * filename , float fps , int silent ) <nl> # ifdef USE_ASS <nl> if ( ass_enabled ) <nl> asst = ass_read_file ( filename ); <nl> - if ( ass_enabled && ! asst ) <nl> + if ( ass_enabled && subd && ! asst ) <nl> asst = ass_read_subdata ( subd , fps ); <nl>  <nl> if (! asst && ! subd && ! silent )
static bool resize_d3d ( d3d_priv * priv ) <nl> return 0 ; <nl> } <nl>  <nl> - if (! priv -> d3d_device ) <nl> + if (! priv -> d3d_device || ! priv -> image_format ) <nl> return 1 ; <nl>  <nl> if (! create_d3d_surfaces ( priv ))
static int update_display_size ( struct vo * vo ) <nl> } <nl> p -> sc = gl_sc_create ( p -> egl . gl , vo -> log , vo -> global ), <nl> p -> osd = mpgl_osd_init ( p -> egl . gl , vo -> log , vo -> osd ); <nl> + p -> osd_change_counter = - 1 ; // force initial overlay rendering <nl>  <nl> p -> display_fps = 0 ; <nl> TV_GET_STATE_RESP_T tvstate ;
static bool get_screenshot_from_texture ( d3d_priv * priv , mp_image_t * image ) <nl> struct texplane * plane = & priv -> planes [ n ]; <nl>  <nl> int width = priv -> src_width >> plane -> shift_x ; <nl> - int height = priv -> src_height >> plane -> shift_x ; <nl> + int height = priv -> src_height >> plane -> shift_y ; <nl>  <nl> memcpy_pic ( image -> planes [ n ], plane -> locked_rect . pBits , <nl> width * plane -> bytes_per_pixel , height ,
static int af_find_output_conversion ( struct af_stream * s , struct mp_audio * cfg ) <nl> ! mp_chmap_equals_reordered (& af -> fmt_in . channels , & af -> fmt_out . channels )) <nl> return AF_ERROR ; <nl> } <nl> + // And not if it ' s the only filter . <nl> + if ( conv -> prev == s -> first && conv -> next == s -> last ) <nl> + return AF_ERROR ; <nl>  <nl> * cfg = s -> output ; <nl> return AF_OK ;
static int d_check_file ( struct demuxer * demuxer , enum demux_check check ) <nl> * p = ( struct priv ) { <nl> . track = track , <nl> }; <nl> + demuxer -> priv = p ; <nl>  <nl> struct sh_stream * sh = new_sh_stream ( demuxer , STREAM_SUB ); <nl> sh -> sub -> track = track ;
int init_audio_codec ( sh_audio_t * sh_audio ) <nl> sh_audio -> sample_format = AFMT_S16_LE ; <nl> # endif <nl> sh_audio -> samplerate = 0 ; <nl> + sh_audio -> channels = 0 ; <nl> sh_audio -> i_bps = 0 ; // input rate ( bytes / sec ) <nl> sh_audio -> o_bps = 0 ; // output rate ( bytes / sec ) <nl> 
struct vo * init_best_video_out ( struct MPOpts * opts , <nl> // continue ... <nl> free ( name ); <nl> ++ vo_list ; <nl> - if (!( vo_list [ 0 ])) <nl> + if (!( vo_list [ 0 ])) { <nl> + talloc_free ( vo ); <nl> return NULL ; // do NOT fallback to others <nl> + } <nl> } <nl> // now try the rest ... <nl> vo_subdevice = NULL ;
double get_current_time ( struct MPContext * mpctx ) <nl> return 0 ; <nl> if ( demuxer -> stream_pts != MP_NOPTS_VALUE ) <nl> return demuxer -> stream_pts ; <nl> + if ( mpctx -> hrseek_active ) <nl> + return mpctx -> hrseek_pts ; <nl> double apts = playing_audio_pts ( mpctx ); <nl> if ( apts != MP_NOPTS_VALUE ) <nl> return apts ;
static int seek_to_chapter ( stream_t * stream , ifo_handle_t * vts_file , tt_srpt_t * <nl> if (! vts_file || ! tt_srpt ) <nl> return 0 ; <nl>  <nl> - if ( chapter < 0 || chapter > vts_file -> vts_ptt_srpt -> title [ title_no ]. nr_of_ptts ) // no such chapter <nl> + if ( chapter < 0 || chapter > vts_file -> vts_ptt_srpt -> title [ title_no ]. nr_of_ptts - 1 ) // no such chapter <nl> return 0 ; <nl>  <nl> ptt = vts_file -> vts_ptt_srpt -> title [ title_no ]. ptt [ chapter ];
static void draw_image ( struct vo * vo , struct mp_image * mpi ) <nl> return ; <nl> } <nl> mp_image_copy_attributes ( dst , mpi ); <nl> - mpi = dst ; <nl> + talloc_free ( mpi ); <nl> + mpi = mp_image_new_ref ( dst ); <nl> } <nl>  <nl> talloc_free ( p -> output_surfaces [ p -> output_surface ]);
bool ca_asbd_equals ( const AudioStreamBasicDescription * a , <nl> a -> mBitsPerChannel == b -> mBitsPerChannel && <nl> ca_normalize_formatid ( a -> mFormatID ) == <nl> ca_normalize_formatid ( b -> mFormatID ) && <nl> - a -> mBytesPerPacket == b -> mBytesPerPacket ; <nl> + a -> mBytesPerPacket == b -> mBytesPerPacket && <nl> + a -> mChannelsPerFrame == b -> mChannelsPerFrame && <nl> + a -> mSampleRate == b -> mSampleRate ; <nl> } <nl>  <nl> // Return the AF_FORMAT_ * ( AF_FORMAT_S16 etc .) corresponding to the asbd .
static enum check_result libztex_checkDevice ( struct libusb_device * dev ) <nl> ret = CHECK_RESCAN ; <nl>  <nl> done : <nl> + free ( fw_buf ); <nl> if ( fp ) <nl> fclose ( fp ); <nl> if ( hndl )
void ft232r_close ( struct ft232r_device_handle * dev ) <nl> libusb_release_interface ( dev -> h , 0 ); <nl> libusb_reset_device ( dev -> h ); <nl> libusb_close ( dev -> h ); <nl> + free ( dev ); <nl> } <nl>  <nl> bool ft232r_purge_buffers ( struct ft232r_device_handle * dev , enum ft232r_reset_purge purge )
static bool avalon_detect_one ( libusb_device * dev , struct usb_find_devices * found <nl> info -> temp_sum = 0 ; <nl> info -> temp_old = 0 ; <nl>  <nl> - ret = avalon_reset ( avalon , true ); <nl> - if ( ret && ! configured ) <nl> + if (! add_cgpu ( avalon )) <nl> goto unshin ; <nl>  <nl> - if (! add_cgpu ( avalon )) <nl> + ret = avalon_reset ( avalon , true ); <nl> + if ( ret && ! configured ) <nl> goto unshin ; <nl>  <nl> update_usb_stats ( avalon );
void hashmeter2 ( struct thr_info * thr ) <nl>  <nl> gettimeofday (& tv_now , NULL ); <nl> timersub (& tv_now , & thr -> tv_lastupdate , & tv_elapsed ); <nl> - if ( tv_elapsed . tv_sec >= opt_log_interval ) { <nl> + /* Update the hashmeter at most 5 times per second */ <nl> + if ( tv_elapsed . tv_sec > 0 || tv_elapsed . tv_usec > 200 ) { <nl> hashmeter ( thr -> id , & tv_elapsed , thr -> hashes_done ); <nl> thr -> hashes_done = 0 ; <nl> thr -> tv_lastupdate = tv_now ;
static bool setup_stratum_curl ( struct pool * pool ) <nl> quit ( 1 , " Failed to curl_easy_init in initiate_stratum "); <nl> if ( pool -> sockbuf ) <nl> pool -> sockbuf [ 0 ] = '\ 0 '; <nl> - mutex_unlock (& pool -> stratum_lock ); <nl>  <nl> curl = pool -> stratum_curl ; <nl>  <nl> static bool setup_stratum_curl ( struct pool * pool ) <nl> pool -> cgminer_pool_stats . times_sent ++; <nl> pool -> cgminer_pool_stats . times_received ++; <nl>  <nl> + mutex_unlock (& pool -> stratum_lock ); <nl> + <nl> return true ; <nl> } <nl> 
void decay_time ( double * f , double fadd ) <nl> ratio = 1 / ratio ; <nl> } <nl>  <nl> - if ( ratio > 0 . 9 ) <nl> + if ( ratio > 0 . 95 ) <nl> * f = ( fadd * 0 . 1 + * f ) / 1 . 1 ; <nl> else <nl> * f = ( fadd + * f * 0 . 1 ) / 1 . 1 ;
static inline void string_elist_del ( struct string_elist * item ) <nl> if ( item -> free_me ) <nl> free ( item -> string ); <nl> list_del (& item -> list ); <nl> + free ( item ); <nl> } <nl>  <nl> 
static void avalon_parse_results ( struct cgpu_info * avalon , struct avalon_info * i <nl> } <nl>  <nl> if (! found ) <nl> - spare = * offset - AVALON_READ_SIZE - 1 ; <nl> + spare = * offset - AVALON_READ_SIZE ; <nl> else <nl> spare = AVALON_READ_SIZE + i ; <nl> applog ( LOG_WARNING , " Avalon : Discarding % d bytes from buffer ", spare );
bool hashbusterusb_vrm_unlock ( struct cgpu_info * const proc , const char * const <nl> hex2bin (& buf [ 1 ], code , size ); <nl>  <nl> hashbusterusb_io ( h , buf , buf ); <nl> - return memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> + return ! memcmp ( buf , "\ x12 \ 0 ", 2 ); <nl> } <nl>  <nl> static
void bfg_waddstr ( WINDOW * win , const char * s ) <nl>  <nl> while ( true ) <nl> { <nl> - while ( likely ( p [ 0 ] >= 0x20 && p [ 0 ] < stop_ascii )) <nl> + while ( likely ( p [ 0 ] == '\ n ' || ( p [ 0 ] >= 0x20 && p [ 0 ] < stop_ascii ))) <nl> { <nl> // Printable ASCII <nl> ++ p ;
bool initiate_stratum ( struct pool * pool ) <nl> goto out ; <nl> } <nl>  <nl> + free ( pool -> nonce1 ); <nl> pool -> nonce1 = strdup ( json_string_value ( json_array_get ( res_val , 1 ))); <nl> if (! pool -> nonce1 ) { <nl> applog ( LOG_WARNING , " Failed to get nonce1 in initiate_stratum ");
static void * submit_work_thread ( __maybe_unused void * userdata ) <nl> } <nl> ++ wip ; <nl> } <nl> - else <nl> + else { <nl> -- total_submitting ; <nl> + free_work ( work ); <nl> + } <nl> } <nl> if ( unlikely ( shutting_down && ! wip )) <nl> break ;
void knc_poll ( struct thr_info * const thr ) <nl> if ( KNC_REPLY_NONCE_FOUND == rtype ) <nl> { <nl> nonce = get_u32be (& rxbuf [ 4 ]); <nl> + nonce = le32toh ( nonce ); <nl> inc_hw_errors2 ( mythr , NULL , & nonce ); <nl> } <nl> else
AcpiTbInstallTableWithOverride ( <nl> * DESCRIPTION : This function is called to verify and install an ACPI table . <nl> * When this function is called by " Load " or " LoadTable " opcodes , <nl> * or by AcpiLoadTable () API , the " Reload " parameter is set . <nl> - * After sucessfully returning from this function , table is <nl> + * After successfully returning from this function , table is <nl> * " INSTALLED " but not " VALIDATED ". <nl> * <nl> ******************************************************************************/
AtHardwTest0010 ( void ) <nl> return ( Status ); <nl> } <nl>  <nl> - Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress ); <nl> + Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress , 0 ); <nl> if ( ACPI_FAILURE ( Status )) <nl> { <nl> AapiErrors ++; <nl> AtHardwTest0011 ( void ) <nl> return ( Status ); <nl> } <nl>  <nl> - Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress ); <nl> + Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress , 0 ); <nl> if ( Status != AE_NO_ACPI_TABLES ) <nl> { <nl> AapiErrors ++;
/****************************************************************************** <nl> * <nl> * Name : acwin . h - OS specific defines , etc . <nl> - * $ Revision : 1 . 27 $ <nl> + * $ Revision : 1 . 28 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> typedef COMPILER_DEPENDENT_UINT64 u64 ; <nl> # define ACPI_SIMPLE_RETURN_MACROS <nl> # endif <nl>  <nl> +/*! [ End ] no source code translation !*/ <nl> + <nl> /* <nl> * Global Lock acquire / release code <nl> *
ApIsValidHeader ( <nl>  <nl> /* Check for minimum table length */ <nl>  <nl> - if ( Table -> Length <= sizeof ( ACPI_TABLE_HEADER )) <nl> + if ( Table -> Length < sizeof ( ACPI_TABLE_HEADER )) <nl> { <nl> fprintf ( stderr , " Table length ( 0x % 8 . 8X ) is invalid \ n ", <nl> Table -> Length );
/****************************************************************************** <nl> * <nl> * Name : acglobal . h - Declarations for global variables <nl> - * $ Revision : 1 . 153 $ <nl> + * $ Revision : 1 . 154 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> # define ACPI_INIT_GLOBAL ( a , b ) a <nl> # endif <nl>  <nl> -/* <nl> +/* <nl> * Keep local copies of these FADT - based registers . NOTE : These globals <nl> * are first in this file for alignment reasons on 64 - bit systems . <nl> */
/****************************************************************************** <nl> * <nl> * Module Name : evregion - ACPI AddressSpace / OpRegion handler dispatch <nl> - * $ Revision : 1 . 79 $ <nl> + * $ Revision : 1 . 80 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AcpiEvInstallDefaultAddressSpaceHandlers ( <nl>  <nl> FUNCTION_TRACE (" EvInstallDefaultAddressSpaceHandlers "); <nl>  <nl> - /* <nl> + /* <nl> * All address spaces ( PCI Config , EC , SMBus ) are scope dependent <nl> * and registration must occur for a specific device . In the case <nl> * system memory and IO address spaces there is currently no device
OpcDoPld ( <nl> { <nl> UINT8 * Buffer ; <nl> ACPI_PARSE_OBJECT * Node ; <nl> - ACPI_PLD_INFO PldInfo = { 0 }; <nl> + ACPI_PLD_INFO PldInfo ; <nl> ACPI_PARSE_OBJECT * NewOp ; <nl>  <nl>  <nl> OpcDoPld ( <nl> return ; <nl> } <nl>  <nl> + ACPI_MEMSET (& PldInfo , 0 , sizeof ( ACPI_PLD_INFO )); <nl> + <nl> Node = Op -> Asl . Child ; <nl> while ( Node ) <nl> {
static int scsi_disk_initfn ( SCSIDevice * dev ) <nl> } <nl> s -> bs = s -> qdev . conf . dinfo -> bdrv ; <nl>  <nl> + if ( bdrv_is_sg ( s -> bs )) { <nl> + qemu_error (" scsi - disk : unwanted / dev / sg *\ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( bdrv_get_type_hint ( s -> bs ) == BDRV_TYPE_CDROM ) { <nl> s -> cluster_size = 4 ; <nl> } else {
void tlb_fill ( CPUCRISState * env , target_ulong addr , int is_write , int mmu_idx , <nl> int ret ; <nl>  <nl> D_LOG ("% s pc =% x tpc =% x ra =% p \ n ", __func__ , <nl> - env -> pc , env -> debug1 , ( void *) retaddr ); <nl> + env -> pc , env -> pregs [ PR_EDA ], ( void *) retaddr ); <nl> ret = cpu_cris_handle_mmu_fault ( env , addr , is_write , mmu_idx ); <nl> if ( unlikely ( ret )) { <nl> if ( retaddr ) {
void usb_packet_complete ( USBDevice * dev , USBPacket * p ) <nl>  <nl> while (! QTAILQ_EMPTY (& ep -> queue )) { <nl> p = QTAILQ_FIRST (& ep -> queue ); <nl> + if ( p -> state == USB_PACKET_ASYNC ) { <nl> + break ; <nl> + } <nl> assert ( p -> state == USB_PACKET_QUEUED ); <nl> ret = usb_process_one ( p ); <nl> if ( ret == USB_RET_ASYNC ) {
uint32_t HELPER ( neon_rshl_u32 )( uint32_t val , uint32_t shiftop ) <nl> uint64_t HELPER ( neon_rshl_u64 )( uint64_t val , uint64_t shiftop ) <nl> { <nl> int8_t shift = ( uint8_t ) shiftop ; <nl> - if ( shift >= 64 || shift < 64 ) { <nl> + if ( shift >= 64 || shift < - 64 ) { <nl> val = 0 ; <nl> } else if ( shift == - 64 ) { <nl> /* Rounding a 1 - bit result just preserves that bit . */
typedef struct { <nl>  <nl> static inline int num_effective_busses ( XilinxSPIPS * s ) <nl> { <nl> - return ( s -> regs [ R_LQSPI_STS ] & LQSPI_CFG_SEP_BUS && <nl> - s -> regs [ R_LQSPI_STS ] & LQSPI_CFG_TWO_MEM ) ? s -> num_busses : 1 ; <nl> + return ( s -> regs [ R_LQSPI_CFG ] & LQSPI_CFG_SEP_BUS && <nl> + s -> regs [ R_LQSPI_CFG ] & LQSPI_CFG_TWO_MEM ) ? s -> num_busses : 1 ; <nl> } <nl>  <nl> static void xilinx_spips_update_cs_lines ( XilinxSPIPS * s )
static void qemu_tcg_init_cpu_signals ( void ) <nl> } <nl> # endif /* _WIN32 */ <nl>  <nl> - QemuMutex qemu_global_mutex ; <nl> + static QemuMutex qemu_global_mutex ; <nl> static QemuCond qemu_io_proceeded_cond ; <nl> static bool iothread_requesting_mutex ; <nl> 
static int qemu_event_init ( void ) <nl>  <nl> static void qemu_event_increment ( void ) <nl> { <nl> - SetEvent ( qemu_event_handle ); <nl> + if (! SetEvent ( qemu_event_handle )) { <nl> + fprintf ( stderr , " qemu_event_increment : SetEvent failed : % d \ n ", <nl> + GetLastError ()); <nl> + exit ( 1 ); <nl> + } <nl> } <nl> # endif <nl> 
static int raw_open ( BlockDriverState * bs , const char * filename , int flags ) <nl> } <nl> # endif <nl> # ifdef CONFIG_COCOA <nl> - u_int32_t blockSize = 512 ; <nl> + uint32_t blockSize = 512 ; <nl> if ( ! ioctl ( fd , DKIOCGETBLOCKSIZE , & blockSize ) && blockSize > bufsize ) { <nl> bufsize = blockSize ; <nl> }
bool aio_wait ( AioContext * ctx ) <nl> * Otherwise , if there are no AIO requests , qemu_aio_wait () would <nl> * wait indefinitely . <nl> */ <nl> - if ( node -> io_flush ) { <nl> + if (! node -> deleted && node -> io_flush ) { <nl> if ( node -> io_flush ( node -> opaque ) == 0 ) { <nl> continue ; <nl> }
static void smc91c111_writeb ( void * opaque , hwaddr offset , <nl> return ; <nl> case 12 : /* Early receive . */ <nl> s -> ercv = value & 0x1f ; <nl> + return ; <nl> case 13 : <nl> /* Ignore . */ <nl> return ;
DMAContext * spapr_tce_new_dma_context ( uint32_t liobn , size_t window_size ) <nl> { <nl> sPAPRTCETable * tcet ; <nl>  <nl> + if ( spapr_tce_find_by_liobn ( liobn )) { <nl> + fprintf ( stderr , " Attempted to create TCE table with duplicate " <nl> + " LIOBN 0x % x \ n ", liobn ); <nl> + return NULL ; <nl> + } <nl> + <nl> if (! window_size ) { <nl> return NULL ; <nl> }
static GSList * gd_vc_gfx_init ( GtkDisplayState * s , VirtualConsole * vc , <nl>  <nl> if ( dpy_ui_info_supported ( vc -> gfx . dcl . con )) { <nl> gtk_menu_item_activate ( GTK_MENU_ITEM ( s -> zoom_fit_item )); <nl> + s -> free_scale = true ; <nl> } <nl>  <nl> return group ;
static int alloc_refcount_block ( BlockDriverState * bs , <nl> uint64_t last_table_size ; <nl> uint64_t blocks_clusters ; <nl> do { <nl> - uint64_t table_clusters = size_to_clusters ( s , table_size ); <nl> + uint64_t table_clusters = <nl> + size_to_clusters ( s , table_size * sizeof ( uint64_t )); <nl> blocks_clusters = 1 + <nl> (( table_clusters + refcount_block_clusters - 1 ) <nl> / refcount_block_clusters );
int paio_init ( void ) <nl> s -> first_aio = NULL ; <nl> if ( qemu_pipe ( fds ) == - 1 ) { <nl> fprintf ( stderr , " failed to create pipe \ n "); <nl> + g_free ( s ); <nl> return - 1 ; <nl> } <nl> 
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> + qemu_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
const VMStateDescription vmstate_ahci = { <nl> VMSTATE_UINT32 ( control_regs . impl , AHCIState ), <nl> VMSTATE_UINT32 ( control_regs . version , AHCIState ), <nl> VMSTATE_UINT32 ( idp_index , AHCIState ), <nl> - VMSTATE_INT32 ( ports , AHCIState ), <nl> + VMSTATE_INT32_EQUAL ( ports , AHCIState ), <nl> VMSTATE_END_OF_LIST () <nl> }, <nl> };
petalogix_ml605_init ( QEMUMachineInitArgs * args ) <nl> dma = qdev_create ( NULL , " xlnx . axi - dma "); <nl>  <nl> /* FIXME : attach to the sysbus instead */ <nl> + object_property_add_child ( qdev_get_machine (), " xilinx - eth ", OBJECT ( eth0 ), <nl> + NULL ); <nl> object_property_add_child ( qdev_get_machine (), " xilinx - dma ", OBJECT ( dma ), <nl> NULL ); <nl> 
static void hda_audio_set_amp ( HDAAudioStream * st ) <nl> left = left * 255 / QEMU_HDA_AMP_STEPS ; <nl> right = right * 255 / QEMU_HDA_AMP_STEPS ; <nl>  <nl> + if (! st -> state -> mixer ) { <nl> + return ; <nl> + } <nl> if ( st -> output ) { <nl> AUD_set_volume_out ( st -> voice . out , muted , left , right ); <nl> } else {
static void vring_init ( struct vring * vr , unsigned int num , void * p , <nl> vr -> used -> flags = VRING_USED_F_NO_NOTIFY ; <nl> vr -> used -> idx = 0 ; <nl> vr -> used_idx = 0 ; <nl> + vr -> next_idx = 0 ; <nl>  <nl> debug_print_addr (" init vr ", vr ); <nl> }
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> { <nl> DPRINTF ("+++ C + mode offloaded task checksum \ n "); <nl>  <nl> + /* Large enough for Ethernet and IP headers ? */ <nl> + if ( saved_size < ETH_HLEN + sizeof ( ip_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ip packet header */ <nl> ip_header * ip = NULL ; <nl> int hlen = 0 ;
const VMStateDescription vmstate_ide_atapi_gesn_state = { <nl> . fields = ( VMStateField []) { <nl> VMSTATE_BOOL ( events . new_media , IDEState ), <nl> VMSTATE_BOOL ( events . eject_request , IDEState ), <nl> + VMSTATE_END_OF_LIST () <nl> } <nl> }; <nl> 
uint32_t lm4549_write_samples ( lm4549_state * s , uint32_t left , uint32_t right ) <nl> This model supports 16 - bit playback . <nl> */ <nl>  <nl> - if ( s -> buffer_level >= LM4549_BUFFER_SIZE ) { <nl> + if ( s -> buffer_level > LM4549_BUFFER_SIZE - 2 ) { <nl> DPRINTF (" write_sample Buffer full \ n "); <nl> return 0 ; <nl> }
void virtio_cleanup ( VirtIODevice * vdev ) <nl> if ( vdev -> config ) <nl> qemu_free ( vdev -> config ); <nl> qemu_free ( vdev -> vq ); <nl> + qemu_free ( vdev ); <nl> } <nl>  <nl> static void virtio_vmstate_change ( void * opaque , int running , int reason )
void cpu_loop ( CPUMIPSState * env ) <nl> syscall_num = env -> active_tc . gpr [ 2 ] - 4000 ; <nl> env -> active_tc . PC += 4 ; <nl> if ( syscall_num >= sizeof ( mips_syscall_args )) { <nl> - ret = - ENOSYS ; <nl> + ret = - TARGET_ENOSYS ; <nl> } else { <nl> int nb_args ; <nl> abi_ulong sp_reg ;
static void gen_compute_branch ( DisasContext * ctx , uint32_t opc , int r1 , <nl> break ; <nl> case OPCM_32_BRR_LOOP : <nl> if ( MASK_OP_BRR_OP2 ( ctx -> opcode ) == OPC2_32_BRR_LOOP ) { <nl> - gen_loop ( ctx , r1 , offset * 2 ); <nl> + gen_loop ( ctx , r2 , offset * 2 ); <nl> } else { <nl> /* OPC2_32_BRR_LOOPU */ <nl> gen_goto_tb ( ctx , 0 , ctx -> pc + offset * 2 );
static int sd_create ( const char * filename , QemuOpts * opts , <nl> bdrv_unref ( bs ); <nl> } <nl>  <nl> + s -> aio_context = qemu_get_aio_context (); <nl> ret = do_sd_create ( s , & vid , 0 , errp ); <nl> if ( ret ) { <nl> goto out ;
static void cirrus_init_common ( CirrusVGAState * s , int device_id , int is_pci ) <nl> s -> vga . cursor_draw_line = cirrus_cursor_draw_line ; <nl>  <nl> qemu_register_reset ( cirrus_reset , s ); <nl> - cirrus_reset ( s ); <nl> } <nl>  <nl> /***************************************
static int sd_snapshot_goto ( BlockDriverState * bs , const char * snapshot_id ) <nl> if ( snapid ) { <nl> tag [ 0 ] = 0 ; <nl> } else { <nl> - pstrcpy ( tag , sizeof ( tag ), s -> name ); <nl> + pstrcpy ( tag , sizeof ( tag ), snapshot_id ); <nl> } <nl>  <nl> ret = reload_inode ( s , snapid , tag );
static TCGArg * tcg_constant_folding ( TCGContext * s , uint16_t * tcg_opc_ptr , <nl> /* Simplify LT / GE comparisons vs zero to a single compare <nl> vs the high word of the input . */ <nl> s -> gen_opc_buf [ op_index ] = INDEX_op_setcond_i32 ; <nl> + reset_temp ( args [ 0 ]); <nl> gen_args [ 0 ] = args [ 0 ]; <nl> gen_args [ 1 ] = args [ 2 ]; <nl> gen_args [ 2 ] = args [ 4 ];
static int refresh_total_sectors ( BlockDriverState * bs , int64_t hint ) <nl> { <nl> BlockDriver * drv = bs -> drv ; <nl>  <nl> + /* Do not attempt drv -> bdrv_getlength () on scsi - generic devices */ <nl> + if ( bs -> sg ) <nl> + return 0 ; <nl> + <nl> /* query actual device if possible , otherwise just trust the hint */ <nl> if ( drv -> bdrv_getlength ) { <nl> int64_t length = drv -> bdrv_getlength ( bs );
static uint32_t parse_enumeration ( char * str , <nl> { <nl> uint32_t ret = not_found_value ; <nl>  <nl> + if ( str == NULL ) <nl> + return 0 ; <nl> + <nl> while ( table -> name != NULL ) { <nl> if ( strcmp ( table -> name , str ) == 0 ) { <nl> ret = table -> value ;
fail : <nl> qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) table ); <nl> } <nl> s -> l1_table [ l1_index ] = old_l2_offset ; <nl> + if ( l2_offset > 0 ) { <nl> + qcow2_free_clusters ( bs , l2_offset , s -> l2_size * sizeof ( uint64_t ), <nl> + QCOW2_DISCARD_ALWAYS ); <nl> + } <nl> return ret ; <nl> } <nl> 
static int kvm_sclp_service_call ( S390CPU * cpu , struct kvm_run * run , <nl> int r = 0 ; <nl>  <nl> cpu_synchronize_state ( CPU ( cpu )); <nl> + if ( env -> psw . mask & PSW_MASK_PSTATE ) { <nl> + enter_pgmcheck ( cpu , PGM_PRIVILEGED ); <nl> + return 0 ; <nl> + } <nl> sccb = env -> regs [ ipbh0 & 0xf ]; <nl> code = env -> regs [( ipbh0 & 0xf0 ) >> 4 ]; <nl> 
void vnc_display_open ( const char * id , Error ** errp ) <nl> /* listen for connects */ <nl> if ( strncmp ( vnc , " unix :", 5 ) == 0 ) { <nl> vs -> lsock = unix_listen ( vnc + 5 , NULL , 0 , errp ); <nl> + if ( vs -> lsock < 0 ) { <nl> + goto fail ; <nl> + } <nl> vs -> is_unix = true ; <nl> } else { <nl> vs -> lsock = inet_listen_opts ( sopts , 5900 , errp );
static const char * const tcg_target_reg_names [ TCG_TARGET_NB_REGS ] = { <nl> # endif <nl>  <nl> static const int tcg_target_reg_alloc_order [] = { <nl> - TCG_REG_EAX , <nl> - TCG_REG_EDX , <nl> - TCG_REG_ECX , <nl> TCG_REG_EBX , <nl> TCG_REG_ESI , <nl> TCG_REG_EDI , <nl> TCG_REG_EBP , <nl> + TCG_REG_ECX , <nl> + TCG_REG_EDX , <nl> + TCG_REG_EAX , <nl> }; <nl>  <nl> static const int tcg_target_call_iarg_regs [ 3 ] = { TCG_REG_EAX , TCG_REG_EDX , TCG_REG_ECX };
static void coroutine_fn backup_run ( void * opaque ) <nl>  <nl> if ( job -> sync_bitmap ) { <nl> BdrvDirtyBitmap * bm ; <nl> - if ( ret < 0 ) { <nl> + if ( ret < 0 || block_job_is_cancelled (& job -> common )) { <nl> /* Merge the successor back into the parent , delete nothing . */ <nl> bm = bdrv_reclaim_dirty_bitmap ( bs , job -> sync_bitmap , NULL ); <nl> assert ( bm );
static CharDriverState * qemu_chr_open_udp ( QemuOpts * opts ) <nl>  <nl> fd = inet_dgram_opts ( opts , & local_err ); <nl> if ( fd < 0 ) { <nl> + qerror_report_err ( local_err ); <nl> + error_free ( local_err ); <nl> return NULL ; <nl> } <nl> return qemu_chr_open_udp_fd ( fd );
 <nl> typedef struct SuperIOConfig <nl> { <nl> - uint8_t config [ 0xff ]; <nl> + uint8_t config [ 0x100 ]; <nl> uint8_t index ; <nl> uint8_t data ; <nl> } SuperIOConfig ;
bool bdrv_dev_is_medium_locked ( BlockDriverState * bs ) <nl> */ <nl> int bdrv_check ( BlockDriverState * bs , BdrvCheckResult * res , BdrvCheckMode fix ) <nl> { <nl> + if ( bs -> drv == NULL ) { <nl> + return - ENOMEDIUM ; <nl> + } <nl> if ( bs -> drv -> bdrv_check == NULL ) { <nl> return - ENOTSUP ; <nl> }
int net_init_vhost_user ( const NetClientOptions * opts , const char * name , <nl> } <nl>  <nl> queues = vhost_user_opts -> has_queues ? vhost_user_opts -> queues : 1 ; <nl> + if ( queues < 1 ) { <nl> + error_setg ( errp , <nl> + " vhost - user number of queues must be bigger than zero "); <nl> + return - 1 ; <nl> + } <nl>  <nl> return net_vhost_user_init ( peer , " vhost_user ", name , chr , queues ); <nl> }
static int qcow2_read_extensions ( BlockDriverState * bs , uint64_t start_offset , <nl> # ifdef DEBUG_EXT <nl> printf (" ext . magic = 0x % x \ n ", ext . magic ); <nl> # endif <nl> + if ( ext . len > end_offset - offset ) { <nl> + error_report (" Header extension too large "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> switch ( ext . magic ) { <nl> case QCOW2_EXT_MAGIC_END : <nl> return 0 ;
static void disas_s390_insn ( DisasContext * s ) <nl> store_reg32 ( r1 , tmp32_1 ); <nl> tcg_gen_trunc_i64_i32 ( tmp32_2 , tmp2 ); <nl> store_reg32 ( r1 + 1 , tmp32_2 ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> + tcg_temp_free_i64 ( tmp2 ); <nl> break ; <nl> case 0x98 : /* LM R1 , R3 , D2 ( B2 ) [ RS ] */ <nl> case 0x90 : /* STM R1 , R3 , D2 ( B2 ) [ RS ] */
ssize_t v9fs_list_xattr ( FsContext * ctx , const char * path , <nl>  <nl> /* Get the actual len */ <nl> xattr_len = llistxattr ( rpath ( ctx , path ), value , 0 ); <nl> + if ( xattr_len <= 0 ) { <nl> + return xattr_len ; <nl> + } <nl>  <nl> /* Now fetch the xattr and find the actual size */ <nl> orig_value = qemu_malloc ( xattr_len );
void qcow2_free_any_clusters ( BlockDriverState * bs , uint64_t l2_entry , <nl> } <nl> break ; <nl> case QCOW2_CLUSTER_NORMAL : <nl> - qcow2_free_clusters ( bs , l2_entry & L2E_OFFSET_MASK , <nl> - nb_clusters << s -> cluster_bits , type ); <nl> + case QCOW2_CLUSTER_ZERO : <nl> + if ( l2_entry & L2E_OFFSET_MASK ) { <nl> + qcow2_free_clusters ( bs , l2_entry & L2E_OFFSET_MASK , <nl> + nb_clusters << s -> cluster_bits , type ); <nl> + } <nl> break ; <nl> case QCOW2_CLUSTER_UNALLOCATED : <nl> - case QCOW2_CLUSTER_ZERO : <nl> break ; <nl> default : <nl> abort ();
ram_addr_t qemu_ram_alloc_from_ptr ( ram_addr_t size , void * host , <nl>  <nl> qemu_ram_setup_dump ( new_block -> host , size ); <nl> qemu_madvise ( new_block -> host , size , QEMU_MADV_HUGEPAGE ); <nl> + qemu_madvise ( new_block -> host , size , QEMU_MADV_DONTFORK ); <nl>  <nl> if ( kvm_enabled ()) <nl> kvm_setup_guest_memory ( new_block -> host , size );
static void xhci_doorbell_write ( void * ptr , hwaddr reg , <nl> } <nl> } <nl>  <nl> + static void xhci_cap_write ( void * opaque , hwaddr addr , uint64_t val , <nl> + unsigned width ) <nl> +{ <nl> + /* nothing */ <nl> +} <nl> + <nl> static const MemoryRegionOps xhci_cap_ops = { <nl> . read = xhci_cap_read , <nl> + . write = xhci_cap_write , <nl> . valid . min_access_size = 1 , <nl> . valid . max_access_size = 4 , <nl> . impl . min_access_size = 4 ,
# ifndef FW_CFG_H <nl> # define FW_CFG_H <nl>  <nl> +# ifndef NO_QEMU_PROTOS <nl> +# include < stdint . h > <nl> +# include < stddef . h > <nl> + <nl> +# include " exec / hwaddr . h " <nl> +# endif <nl> + <nl> # define FW_CFG_SIGNATURE 0x00 <nl> # define FW_CFG_ID 0x01 <nl> # define FW_CFG_UUID 0x02
static int iscsi_open ( BlockDriverState * bs , QDict * options , int flags , <nl> QemuOpts * opts ; <nl> Error * local_err = NULL ; <nl> const char * filename ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> if (( BDRV_SECTOR_SIZE % 512 ) != 0 ) { <nl> error_setg ( errp , " iSCSI : Invalid BDRV_SECTOR_SIZE . "
static int ioreq_map ( struct ioreq * ioreq ) <nl> xen_be_printf (& ioreq -> blkdev -> xendev , 0 , <nl> " can ' t map grant ref % d (% s , % d maps )\ n ", <nl> refs [ i ], strerror ( errno ), ioreq -> blkdev -> cnt_map ); <nl> + ioreq -> mapped = 1 ; <nl> ioreq_unmap ( ioreq ); <nl> return - 1 ; <nl> }
static int decode_micromips_opc ( CPUMIPSState * env , DisasContext * ctx , int * is_b <nl> case LB32 : <nl> case LH32 : <nl> case DADDIU32 : <nl> - case POOL48A : /* ??? */ <nl> case LWC132 : <nl> case LDC132 : <nl> case LD32 :
static int aio_flush_request ( void * opaque ) <nl> return ! QLIST_EMPTY (& s -> outstanding_aio_head ); <nl> } <nl>  <nl> -# ifdef _WIN32 <nl> +# if ! defined ( SOL_TCP ) || ! defined ( TCP_CORK ) <nl>  <nl> static int set_cork ( int fd , int v ) <nl> {
struct m_hdr { <nl> struct mbuf { <nl> struct m_hdr m_hdr ; <nl> Slirp * slirp ; <nl> + bool arp_requested ; <nl> + uint64_t expiration_date ; <nl> + /* start of dynamic buffer area , must be last element */ <nl> union M_dat { <nl> char m_dat_ [ 1 ]; /* ANSI don ' t like 0 sized arrays */ <nl> char * m_ext_ ; <nl> } M_dat ; <nl> - bool arp_requested ; <nl> - uint64_t expiration_date ; <nl> }; <nl>  <nl> # define m_next m_hdr . mh_next
static sd_rsp_type_t sd_normal_command ( SDState * sd , <nl> } <nl> break ; <nl>  <nl> + case 5 : /* CMD5 : reserved for SDIO cards */ <nl> + sd -> card_status |= ILLEGAL_COMMAND ; <nl> + return sd_r0 ; <nl> + <nl> case 6 : /* CMD6 : SWITCH_FUNCTION */ <nl> if ( sd -> spi ) <nl> goto bad_cmd ;
static void ehci_async_complete_packet ( USBPort * port , USBPacket * packet ) <nl> assert ( p -> async == EHCI_ASYNC_INFLIGHT ); <nl> p -> async = EHCI_ASYNC_FINISHED ; <nl> p -> usb_status = packet -> result ; <nl> + <nl> + if ( p -> queue -> async ) { <nl> + qemu_bh_schedule ( p -> queue -> ehci -> async_bh ); <nl> + } <nl> } <nl>  <nl> static void ehci_execute_complete ( EHCIQueue * q )
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> if (( txdw0 & CP_TX_LGSEN ) && ip_protocol == IP_PROTO_TCP ) <nl> { <nl> + /* Large enough for the TCP header ? */ <nl> + if ( ip_data_len < sizeof ( tcp_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> int large_send_mss = ( txdw0 >> 16 ) & CP_TC_LGSEN_MSS_MASK ; <nl>  <nl> DPRINTF ("+++ C + mode offloaded task TSO MTU =% d IP data % d "
uint32_t gic_acknowledge_irq ( GICState * s , int cpu ) <nl> } <nl> s -> last_active [ irq ][ cpu ] = s -> running_irq [ cpu ]; <nl>  <nl> - if ( s -> revision == REV_11MPCORE ) { <nl> + if ( s -> revision == REV_11MPCORE || s -> revision == REV_NVIC ) { <nl> /* Clear pending flags for both level and edge triggered interrupts . <nl> * Level triggered IRQs will be reasserted once they become inactive . <nl> */
static void decode_opc ( CPUState * env , DisasContext * ctx ) <nl> gen_goto_tb ( ctx , 1 , ctx -> pc + 4 ); <nl> gen_set_label ( l1 ); <nl> } <nl> + <nl> + if ( unlikely ( qemu_loglevel_mask ( CPU_LOG_TB_OP ))) <nl> + tcg_gen_debug_insn_start ( ctx -> pc ); <nl> + <nl> op = MASK_OP_MAJOR ( ctx -> opcode ); <nl> rs = ( ctx -> opcode >> 21 ) & 0x1f ; <nl> rt = ( ctx -> opcode >> 16 ) & 0x1f ;
static int cpu_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> - return cpu_post_load ( env , version_id ); <nl> + tlb_flush ( env , 1 ); <nl> + return 0 ; <nl> } <nl>  <nl> const VMStateDescription vmstate_cpu = {
static int sysbus_device_init ( DeviceState * dev ) <nl> SysBusDevice * sd = SYS_BUS_DEVICE ( dev ); <nl> SysBusDeviceClass * sbc = SYS_BUS_DEVICE_GET_CLASS ( sd ); <nl>  <nl> + if (! sbc -> init ) { <nl> + return 0 ; <nl> + } <nl> return sbc -> init ( sd ); <nl> } <nl> 
static int htab_load ( QEMUFile * f , void * opaque , int version_id ) <nl> if ( section_hdr ) { <nl> /* First section , just the hash shift */ <nl> if ( spapr -> htab_shift != section_hdr ) { <nl> + error_report (" htab_shift mismatch : source % d target % d ", <nl> + section_hdr , spapr -> htab_shift ); <nl> return - EINVAL ; <nl> } <nl> return 0 ;
void * __lzma_wrap_alloc ( void * unused , size_t size ) { <nl> return NULL ; <nl> } <nl>  <nl> - return cli_malloc ( size ); <nl> + return cli_calloc ( 1 , size ); <nl> } <nl> void __lzma_wrap_free ( void * unused , void * freeme ) { <nl> UNUSEDPARAM ( unused );
static int unpack ( const struct optstruct * opts ) <nl> name [ sizeof ( name )- 1 ]='\ 0 '; <nl> } <nl>  <nl> + if ( cl_cvdverify ( name ) != CL_SUCCESS ) { <nl> + mprintf ("! unpack : % s is not a valid CVD \ n ", name ); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( cli_cvdunpack ( name , ".") == - 1 ) { <nl> mprintf ("! unpack : Can ' t unpack file % s \ n ", name ); <nl> return - 1 ;
int cli_url_canon ( const char * inurl , size_t len , char * urlbuff , size_t dest_len , <nl> ++ host_begin ; <nl>  <nl> /* ignore username in URL */ <nl> - p = strchr ( host_begin , '@'); <nl> + while (( host_begin < urlend ) && * host_begin == '/') ++ host_begin ; <nl> + host_len = strcspn ( host_begin , ":/?"); <nl> + p = memchr ( host_begin , '@', host_len ); <nl> if ( p ) <nl> host_begin = p + 1 ; <nl> url = host_begin ;
const char * cl_strerror ( int clerror ) <nl> return " Can ' t map file into memory "; <nl> case CL_EMEM : <nl> return " Can ' t allocate memory "; <nl> + case CL_ETIMEOUT : <nl> + return " Time limit reached "; <nl> /* internal ( needed for debug messages ) */ <nl> case CL_EMAXREC : <nl> return " CL_EMAXREC ";
void cache_add ( unsigned char * md5 , size_t size , cli_ctx * ctx ) { <nl> uint32_t level ; <nl> struct CACHE * c ; <nl>  <nl> - if (! ctx || ! ctx -> engine || ! ctx -> engine -> cache ) <nl> + if (! ctx || ! ctx -> engine || ! ctx -> engine -> cache || ctx -> found_possibly_unwanted ) <nl> return ; <nl>  <nl> level = (* ctx -> fmap && (* ctx -> fmap )-> dont_cache_flag ) ? ctx -> recursion : 0 ;
int cli_scanhwp3 ( cli_ctx * ctx ) <nl>  <nl> offset += HWP3_DOCSUMMARY_SIZE ; <nl>  <nl> + /* password - protected document - cannot parse */ <nl> + if ( docinfo . di_passwd ) { <nl> + cli_dbgmsg (" HWP3 . x : password - protected file , skip parsing \ n "); <nl> + return CL_SUCCESS ; <nl> + } <nl> + <nl> if ( docinfo . di_infoblksize ) { <nl> /* OPTIONAL TODO : HANDLE OPTIONAL INFORMATION BLOCK # 0 ' s FOR PRECLASS */ <nl> offset += docinfo . di_infoblksize ;
void * cli_jsonarray_nojson ( const char * key ) <nl> int cli_jsonint_array_nojson ( int32_t val ) <nl> { <nl> nojson_func (" nojson : % d \ n ", val ); <nl> + return CL_SUCCESS ; <nl> } <nl>  <nl> # endif
int cli_parse_add ( struct cli_matcher * root , const char * virname , const char * hex <nl> if (( n = cli_strtok ( pt , 2 , "-"))) { /* strict check */ <nl> error = 1 ; <nl> free ( n ); <nl> + break ; <nl> } <nl> } <nl> }
bool read_sequence ( u8 const * & src , u8 const * const end , u8 const * & literal , u <nl> literal = src ; <nl> src += literal_len ; <nl>  <nl> - if ( src > end - 2 ) <nl> + if ( src > end - 2 || src < literal ) <nl> return false ; <nl>  <nl> match_dist = * src ++;
GlyphCache :: GlyphCache ( const Face & face , const uint32 face_options ) <nl> } <nl> delete _glyph_loader ; <nl> _glyph_loader = 0 ; <nl> + // coverity [ leaked_storage : FALSE ] - calling read_glyph on index 0 saved <nl> + // glyphs as _glyphs [ 0 ]. Setting _glyph_loader to nullptr here flags that <nl> + // the dtor needs to call delete [] on _glyphs [ 0 ] to release what was allocated <nl> + // as glyphs <nl> } <nl>  <nl> if ( _glyphs && glyph ( 0 ) == 0 )
static int count_column_width ( struct libscols_table * tb , <nl>  <nl> cl -> width = 0 ; <nl>  <nl> - <nl> - if ( cl -> width_min ) { <nl> + if (! cl -> width_min ) { <nl> if ( cl -> width_hint < 1 && scols_table_is_maxout ( tb )) <nl> cl -> width_min = ( size_t ) ( cl -> width_hint * tb -> termwidth ) - ( is_last_column ( cl ) ? 0 : 1 ); <nl> if ( scols_cell_get_data (& cl -> header )) {
read_extended_partition ( int fd , struct partition * ep , <nl> if (++ loopct > 100 ) <nl> return n ; <nl>  <nl> - bp = getblock ( fd , here ); <nl> + bp = getblock ( fd , here * ssf ); /* in 512 blocks */ <nl> if ( bp == NULL ) <nl> return n ; <nl> 
static FILE * dump ( FILE * in , const char * filename , int follow , FILE * out ) <nl> struct utmp ut ; <nl>  <nl> if ( follow ) <nl> - fseek ( in , - 10 * sizeof ( ut ), SEEK_END ); <nl> + ignore_result ( fseek ( in , - 10 * sizeof ( ut ), SEEK_END ) ); <nl>  <nl> while ( fread (& ut , sizeof ( ut ), 1 , in ) == 1 ) <nl> print_utline ( ut , out );
static void <nl> xbsd_write_bootstrap ( void ) <nl> { <nl> char * bootdir = BSD_LINUX_BOOTDIR ; <nl> - char path [ MAXPATHLEN ]; <nl> + char path [ sizeof ( BSD_LINUX_BOOTDIR ) + 1 + 2 + 4 ]; /* BSD_LINUX_BOOTDIR + / + { sd , wd } + boot */ <nl> char * dkbasename ; <nl> struct xbsd_disklabel dl ; <nl> char * d , * p , * e ;
int read_hypervisor_dmi ( void ) <nl> if ( rc ) <nl> goto done ; <nl> free ( buf ); <nl> - <nl> + buf = NULL ; <nl> memory_scan : <nl> # if defined ( __x86_64__ ) || defined ( __i386__ ) <nl> /* Fallback to memory scan ( x86 , x86_64 ) */
 <nl> # if defined ( __i386__ ) || defined ( __sparc__ ) || defined ( __arm__ ) || \ <nl> defined ( __mips__ ) || defined ( __s390__ ) || defined ( __sh__ ) || \ <nl> - defined ( __aarch64__ ) || \ <nl> + defined ( __aarch64__ ) || defined ( __xtensa__ ) || \ <nl> defined ( __x86_64__ ) || defined ( __avr32__ ) || defined ( __cris__ ) <nl> # define BSD_LABELSECTOR 1 <nl> # define BSD_LABELOFFSET 0
int setpwnam ( struct passwd * pwd ) <nl> } <nl>  <nl> /* Set up the limits so that we ' re not foiled */ <nl> - static void pw_init () <nl> + static void pw_init ( void ) <nl> { <nl> struct rlimit rlim ; <nl> 
int proc_next_tid ( struct proc_tasks * tasks , pid_t * tid ) <nl> struct dirent * d ; <nl> char * end ; <nl>  <nl> + if (! tasks || ! tid ) <nl> + return - 1 ; <nl> + <nl> * tid = 0 ; <nl> errno = 0 ; <nl> 
details_only : <nl> /* <nl> * Gather PART_ENTRY_ * values if the current device is a partition . <nl> */ <nl> - if (( blkid_partitions_get_flags ( pr ) & BLKID_PARTS_ENTRY_DETAILS )) { <nl> + if (! chn -> binary && <nl> + ( blkid_partitions_get_flags ( pr ) & BLKID_PARTS_ENTRY_DETAILS )) { <nl> + <nl> if (! blkid_partitions_probe_partition ( pr )) <nl> rc = 0 ; <nl> }
wchar_t * buf ; <nl>  <nl> static void sig_handler ( int signo __attribute__ (( __unused__ ))) <nl> { <nl> - free ( buf ); <nl> _exit ( EXIT_SUCCESS ); <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> if (( pwd = getrootpwent ( opt_e )) == NULL ) { <nl> warnx ( _ (" cannot open password database .")); <nl> sleep ( 2 ); <nl> + return EXIT_FAILURE ; <nl> } <nl>  <nl> /*
makemsg ( fname ) <nl> putc ('\ n ', fp ); <nl> cnt = 0 ; <nl> } <nl> - carefulputc ( ch , fp ); <nl> + if ( ch != '\ n ') <nl> + carefulputc ( ch , fp ); <nl> } <nl> } <nl> fprintf ( fp , "% 79s \ r \ n ", " ");
void GeoIPManager :: configure () <nl> const bool enabled = Preferences :: instance ()-> resolvePeerCountries (); <nl> if ( m_enabled != enabled ) { <nl> m_enabled = enabled ; <nl> - if ( m_enabled && ! m_geoIPDatabase ) <nl> + if ( m_enabled && ! m_geoIPDatabase ) { <nl> loadDatabase (); <nl> + } <nl> + else if (! m_enabled && m_geoIPDatabase ) { <nl> + delete m_geoIPDatabase ; <nl> + m_geoIPDatabase = 0 ; <nl> + } <nl> } <nl> } <nl> 
void MainWindow :: serverDisconnected ( QAbstractSocket :: SocketError err , QString re <nl> if (! Database :: getDigest ( host , port ). isNull ()) { <nl> basereason = tr ("< b > WARNING :</ b > The server presented a certificate that was different from the stored one ."); <nl> } else { <nl> - basereason = tr (" Sever presented a certificate which failed verification ."); <nl> + basereason = tr (" Server presented a certificate which failed verification ."); <nl> } <nl> QStringList qsl ; <nl> foreach ( QSslError e , g . sh -> qlErrors )
void Settings :: save () { <nl> SAVELOAD ( bSuppressMacEventTapWarning , " shortcut / mac / suppresswarning "); <nl> SAVELOAD ( bEnableEvdev , " shortcut / linux / evdev / enable "); <nl> SAVELOAD ( bEnableXInput2 , " shortcut / x11 / xinput2 / enable "); <nl> + SAVELOAD ( bEnableGKey , " shortcut / gkey "); <nl> SAVELOAD ( bEnableXboxInput , " shortcut / windows / xbox / enable "); <nl> SAVELOAD ( bEnableWinHooks , " winhooks "); <nl> SAVELOAD ( bDirectInputVerboseLogging , " shortcut / windows / directinput / verboselogging ");
int openDatabase ( char * prefix , char * dbpath , rpmdb * rpmdbp , int mode , <nl> int i ; <nl> struct flock lockinfo ; <nl>  <nl> + /* we should accept NULL as a valid prefix */ <nl> + if (! prefix ) prefix =""; <nl> + <nl> i = strlen ( dbpath ); <nl> if ( dbpath [ i - 1 ] != '/') { <nl> filename = alloca ( i + 2 );
static int doSign ( poptContext optCon ) <nl> } <nl>  <nl> exit : <nl> + free ( passPhrase ); <nl> free ( name ); <nl> return rc ; <nl> }
static int rdToken ( ParseState state ) <nl> size_t ts ; <nl>  <nl> p ++; <nl> - for ( ts = 1 ; p [ ts ] && p [ ts ] != '\"'; ts ++); <nl> + for ( ts = 0 ; p [ ts ] && p [ ts ] != '\"'; ts ++); <nl> temp = xmalloc ( ts + 1 ); <nl> memcpy ( temp , p , ts ); <nl> p += ts - 1 ;
char * headerSprintf ( Header h , const char * origFmt , <nl>  <nl> strcat ( answer , piece ); <nl> answerLength += pieceLength ; <nl> + free ( piece ); <nl> } <nl> } <nl> 
static int makeHDRDigest ( Header sigh , const char * file , rpmTagVal sigTag ) <nl> ( void ) rpmDigestUpdate ( ctx , utd . data , utd . count ); <nl> ( void ) rpmDigestFinal ( ctx , ( void **)& SHA1 , NULL , 1 ); <nl> rpmtdFreeData (& utd ); <nl> + } else { <nl> + rpmlog ( RPMLOG_ERR , _ (" Cannot sign RPM v3 packages \ n ")); <nl> + goto exit ; <nl> } <nl>  <nl> if ( SHA1 == NULL )
static int installArchive ( char * prefix , int fd , struct fileToInstall * files , <nl> kill ( SIGTERM , child ); <nl> } <nl>  <nl> - if ( write ( p [ 1 ], buf , bytesRead ) != bytesRead ) { <nl> + if ( bytesRead && write ( p [ 1 ], buf , bytesRead ) != bytesRead ) { <nl> cpioFailed = 1 ; <nl> childDead = 1 ; <nl> kill ( SIGTERM , child );
static void removeIndexEntry ( dbIndex * dbi , char * key , dbIndexRecord rec , <nl> case 2 : <nl> break ; /* error message already generated from dbindex . c */ <nl> } <nl> + <nl> + freeDBIndexRecord ( matches ); <nl> } <nl>  <nl> int rpmdbRemove ( rpmdb db , unsigned int offset , int tolerant ) { <nl> int rpmdbRemove ( rpmdb db , unsigned int offset , int tolerant ) { <nl>  <nl> unblockSignals (); <nl>  <nl> + freeHeader ( h ); <nl> + <nl> return 0 ; <nl> } <nl> 
envelope_err : <nl> void unwrap_flaglist ( strarray_t * strlist , strarray_t ** flaglist , <nl> variable_list_t * variables ) <nl> { <nl> + if (! strlist ) return ; <nl> + <nl> int len = strarray_size ( strlist ); <nl>  <nl> if ( len ) {
int sync_crc_calc ( struct mailbox * mailbox , char * buf , int maxlen ) <nl> if ( mailbox_read_index_record ( mailbox , recno , & record )) <nl> continue ; <nl>  <nl> + /* always skip EXPUNGED flags , so we don ' t count the annots */ <nl> + if ( record . system_flags & FLAG_EXPUNGED ) <nl> + continue ; <nl> + <nl> sync_crc_algorithm -> addrecord ( mailbox , & record , sync_crc_covers ); <nl> if (( sync_crc_covers & SYNC_CRC_ANNOTATIONS ) && user_annot_db ) { <nl> r = read_annotations ( mailbox , & record , & annots );
void config_read_file ( const char * filename ) <nl> infile = fopen ( filename , " r "); <nl>  <nl> if (! infile ) { <nl> - snprintf ( buf , bufsize , " can ' t open configuration file % s : % m ", <nl> - filename ); <nl> + snprintf ( buf , bufsize , " can ' t open configuration file % s : % s ", <nl> + filename , strerror ( errno )); <nl> fatal ( buf , EC_CONFIG ); <nl> } <nl> 
static int jmap_write_calendarevent ( json_t * event , <nl> } <nl> jmap_calendarevent_to_ical ( comp , event , & rock ); <nl> jmap_timezones_to_ical ( ical , & rock ); <nl> + if ( rock . oldcomp ) { <nl> + icalcomponent_free ( rock . oldcomp ); <nl> + } <nl> calevent_rock_free (& rock ); <nl>  <nl> /* Handle any property errors and bail out . */
static int caldav_delete_cal ( struct transaction_t * txn , <nl> sched_reply ( userid , strarray_nth (& schedule_addresses , 0 ), ical , NULL ); <nl> } <nl>  <nl> + free ( userid ); <nl> strarray_fini (& schedule_addresses ); <nl> } <nl> 
static int setCalendarEvents ( struct jmap_req * req ) <nl>  <nl> json_incref ( set ); <nl> json_t * item = json_pack ("[]"); <nl> - json_array_append_new ( item , json_string (" calendarsEventsSet ")); <nl> + json_array_append_new ( item , json_string (" calendarEventsSet ")); <nl> json_array_append_new ( item , set ); <nl> json_array_append_new ( item , json_string ( req -> tag )); <nl> json_array_append_new ( req -> response , item );
static int mboxlist_do_find ( struct find_rock * rock , const strarray_t * patterns ) <nl> if ( len ) len --; // trailing separator <nl>  <nl> if (! strncmp ( rock -> namespace -> prefix [ NAMESPACE_USER ], commonpat , MIN ( len , prefixlen ))) { <nl> - if ( prefixlen < len ) { <nl> + if ( prefixlen <= len ) { <nl> /* we match all users */ <nl> strlcpy ( domainpat + domainlen , " user .", sizeof ( domainpat )- domainlen ); <nl> }
static json_t * jmap_mailbox_from_mbox ( struct mailbox * mbox , <nl> } <nl> } <nl> json_object_set_new ( obj , " sortOrder ", json_integer ( sortOrder )); <nl> + buf_free (& attrib ); <nl> } <nl> if ( _wantprop ( props , " parentId ")) { <nl> json_object_set_new ( obj , " parentId ", parent && parent != inbox ?
int mailbox_rename_cleanup ( struct mailbox ** mailboxptr , int isinbox ) <nl> */ <nl> int mailbox_copyfile ( const char * from , const char * to , int nolink ) <nl> { <nl> - int flags = 0 ; <nl> + int flags = COPYFILE_MKDIR ; <nl> if ( nolink ) flags |= COPYFILE_NOLINK ; <nl> - return cyrus_copyfile ( from , to , flags ); <nl> + <nl> + if ( cyrus_copyfile ( from , to , flags )) <nl> + return IMAP_IOERROR ; <nl> + <nl> + return 0 ; <nl> } <nl>  <nl> /* ---------------------------------------------------------------------- */
static int mboxlist_do_find ( struct find_rock * rock , const strarray_t * patterns ) <nl> int i ; <nl> const char * p ; <nl>  <nl> + if ( patterns -> count < 1 ) return 0 ; /* nothing to do */ <nl> + <nl> for ( i = 0 ; i < patterns -> count ; i ++) { <nl> glob * g = glob_init ( strarray_nth ( patterns , i ), rock -> namespace -> hier_sep ); <nl> ptrarray_append (& rock -> globs , g );
int sync_update_mailbox ( struct sync_folder * local , <nl> flags |= SYNC_FLAG_ISREPEAT ; <nl>  <nl> if ( r == IMAP_AGAIN ) { <nl> + local -> batchsize = 0 ; /* don ' t batch the re - update , means sync to 2 . 4 will still work after fullsync */ <nl> r = mailbox_full_update ( local , reserve_list , sync_be , flags ); <nl> if (! r ) r = update_mailbox_once ( local , remote , topart , <nl> reserve_list , sync_be , flags );
EXPORTED int index_snippets ( struct index_state * state , <nl> int nmatches = 0 ; <nl> struct snippet_rock srock ; <nl>  <nl> + /* reload index */ <nl> + r = index_refresh ( state ); <nl> + if ( r ) return r ; <nl> + <nl> bx = search_begin_search ( state -> mailbox , SEARCH_MULTIPLE ); <nl> if (! bx ) { <nl> r = IMAP_INTERNAL ;
int meth_delete ( struct transaction_t * txn , void * params ) <nl> else if ( r == IMAP_MAILBOX_NONEXISTENT ) ret = HTTP_NOT_FOUND ; <nl> else if ( r ) ret = HTTP_SERVER_ERROR ; <nl>  <nl> - dparams -> davdb . close_db ( davdb ); <nl> - <nl> goto done ; <nl> } <nl> 
EXPORTED void mbname_free ( mbname_t ** mbnamep ) <nl> free ( mbname -> intname ); <nl> free ( mbname -> extname ); <nl> free ( mbname -> extuserid ); <nl> + free ( mbname -> recipient ); <nl>  <nl> /* thing itself */ <nl> free ( mbname );
int service_main ( int argc , char ** argv , <nl> struct io_count * io_count_stop ; <nl>  <nl> if ( config_iolog ) { <nl> - io_count_start = malloc ( sizeof ( struct io_count )); <nl> - io_count_stop = malloc ( sizeof ( struct io_count )); <nl> + io_count_start = xmalloc ( sizeof ( struct io_count )); <nl> + io_count_stop = xmalloc ( sizeof ( struct io_count )); <nl> read_io_count ( io_count_start ); <nl> } <nl> 
static int chkchildren ( char * name , <nl> int r ; <nl>  <nl> r = mboxlist_lookup ( name , & mbentry , 0 ); <nl> + /* deleted mailboxes don ' t count as children */ <nl> + if ( r == IMAP_MAILBOX_NONEXISTENT ) return 0 ; <nl> if ( r ) return r ; <nl>  <nl> if (! strcmp ( part , mbentry -> partition ))
static void add_attendees ( icalcomponent * ical , <nl> prop = icalcomponent_get_next_invitee ( comp )) { <nl>  <nl> const char * attendee = icalproperty_get_invitee ( prop ); <nl> + if (! attendee ) continue ; <nl> + <nl> if (! strncasecmp ( attendee , " mailto :", 7 )) attendee += 7 ; <nl>  <nl> /* Skip where attendee == organizer */
int mailbox_ensure_cache ( struct mailbox * mailbox , unsigned offset ) <nl> mailbox -> cache_fd = open ( fname , openflags , 0 ); <nl> if ( mailbox -> cache_fd == - 1 ) <nl> goto fail ; <nl> + <nl> + if ( mailbox -> cache_buf . s ) <nl> + map_free (( const char **)& mailbox -> cache_buf . s , & mailbox -> cache_len ); <nl> + mailbox -> cache_buf . len = 0 ; <nl> } <nl>  <nl> if ( offset >= mailbox -> cache_buf . len ) {
EXPORTED int mboxlist_renamemailbox ( const char * oldname , const char * newname , <nl>  <nl> /* log the rename */ <nl> sync_log_mailbox_double ( oldname , newname ); <nl> + /* and log an append so that squatter indexes it */ <nl> + sync_log_append ( newname ); <nl> } <nl>  <nl> /* free memory */
int meth_acl ( struct transaction_t * txn , void * params ) <nl> else if (! xmlStrcmp ( priv -> name , <nl> BAD_CAST " read ")) <nl> rights |= DACL_READ ; <nl> + else if (! xmlStrcmp ( priv -> name , <nl> + BAD_CAST " read - free - busy ")) <nl> + rights |= DACL_READFB ; <nl> else if (! xmlStrcmp ( priv -> name , <nl> BAD_CAST " write ")) <nl> rights |= DACL_WRITE ;
static int is_incompressible ( const char * p , size_t n ) <nl> */ <nl> EXPORTED int prot_data_boundary ( struct protstream * s ) <nl> { <nl> - s -> boundary = 1 ; <nl> + // XXX - appears to be broken , so just don ' t set the boundary . We ' ll <nl> + // spend trivially more CPU when transferring binary parts . Boo hoo <nl> + // re - enable this once the bug is fixed <nl> + // s -> boundary = 1 ; <nl> return 0 ; <nl> } <nl> 
static int caldav_put ( struct transaction_t * txn , <nl> } <nl> else { <nl> /* Attendee scheduling object resource */ <nl> + if (! oldical ) { <nl> + /* Can ' t reply to a non - existent invitation */ <nl> + ret = HTTP_FORBIDDEN ; <nl> + goto done ; <nl> + } <nl> sched_reply ( userid , oldical , ical ); <nl> } <nl> 
EXPORTED int carddav_writecard ( struct carddav_db * carddavdb , struct carddav_data <nl> else if (! strcmp ( name , " x - fm - otheraccount - member ")) { <nl> if ( strncmp ( propval , " urn : uuid :", 9 )) continue ; <nl> struct vparse_param * param = vparse_get_param ( ventry , " userid "); <nl> + if (! param ) continue ; <nl> strarray_append (& member_uids , propval + 9 ); <nl> strarray_append (& member_uids , param -> value ); <nl> }
void do_zonedir ( const char * dir , struct hash_table * tzentries , <nl> ical = icalparser_parse_string ( base ); <nl> map_free (& base , & len ); <nl>  <nl> + if (! ical ) continue ; /* skip non - iCalendar files */ <nl> + <nl> comp = icalcomponent_get_first_component ( ical , <nl> ICAL_VTIMEZONE_COMPONENT ); <nl> prop = icalcomponent_get_first_property ( comp , ICAL_TZID_PROPERTY );
static int usersubs_cb ( void * rock , const char * key , size_t keylen , <nl> mboxname_userownsmailbox ( mbrock -> userid , mboxname )) return 0 ; <nl>  <nl> r = mboxlist_lookup ( mboxname , & mbrock -> mbentry , NULL ); <nl> + if ( r == IMAP_MAILBOX_NONEXISTENT ) return 0 ; <nl> + <nl> if ( r ) { <nl> syslog ( LOG_INFO , " mboxlist_lookup (% s ) failed : % s ", <nl> mboxname , error_message ( r ));
int meth_mkcol ( struct transaction_t * txn , void * params ) <nl> /* Start construction of our mkcol / mkcalendar response */ <nl> buf_appendcstr (& txn -> buf , "- response "); <nl> root = init_xml_response ( buf_cstring (& txn -> buf ), NS_REQ_ROOT , root , ns ); <nl> + buf_reset (& txn -> buf ); <nl> if (! root ) { <nl> ret = HTTP_SERVER_ERROR ; <nl> txn -> error . desc = " Unable to create XML response \ r \ n ";
int report_sync_col ( struct transaction_t * txn , <nl>  <nl> /* XXX Handle Depth ( cal - home - set at toplevel ) */ <nl>  <nl> + memset (& istate , 0 , sizeof ( struct index_state )); <nl> istate . map = NULL ; <nl>  <nl> /* Open mailbox for reading */
int cacheScheduleIOPushJobs ( int flags ) { <nl>  <nl> if ( op -> type != REDIS_IO_LOAD && flags & REDIS_IO_ONLYLOADS ) break ; <nl>  <nl> - if (!( flags & REDIS_IO_ASAP ) && <nl> + /* Don ' t execute SAVE before the scheduled time for completion */ <nl> + if ( op -> type == REDIS_IO_SAVE && !( flags & REDIS_IO_ASAP ) && <nl> ( now - op -> ctime ) < server . cache_flush_delay ) break ; <nl>  <nl> /* Don ' t add a SAVE job in the IO thread queue if there is already
void moduleHandleBlockedClients ( void ) { <nl> if ( bc -> privdata && bc -> free_privdata ) <nl> bc -> free_privdata ( bc -> privdata ); <nl> zfree ( bc ); <nl> - if ( c != NULL ) unblockClient ( bc -> client ); <nl> + if ( c != NULL ) unblockClient ( c ); <nl>  <nl> /* Lock again before to iterate the loop . */ <nl> pthread_mutex_lock (& moduleUnblockedClientsMutex );
void sendReplyToClient ( aeEventLoop * el , int fd , void * privdata , int mask ) { <nl>  <nl> /* If the buffer was sent , set bufpos to zero to continue with <nl> * the remainder of the reply . */ <nl> - if ( c -> sentlen == c -> bufpos ) { <nl> + if (( int ) c -> sentlen == c -> bufpos ) { <nl> c -> bufpos = 0 ; <nl> c -> sentlen = 0 ; <nl> }
static void repl ( void ) { <nl> } <nl>  <nl> elapsed = mstime ()- start_time ; <nl> - if ( elapsed >= 500 ) { <nl> + if ( elapsed >= 500 && <nl> + config . output == OUTPUT_STANDARD ) <nl> + { <nl> printf ("(%. 2fs )\ n ",( double ) elapsed / 1000 ); <nl> } <nl> }
void xreadCommand ( client * c ) { <nl> } <nl>  <nl> if ( strcmp ( c -> argv [ i ]-> ptr ,"$") == 0 ) { <nl> + if ( xreadgroup ) { <nl> + addReplyError ( c ," The $ ID can be specified only when calling " <nl> + " XREAD without GROUP option ."); <nl> + goto cleanup ; <nl> + } <nl> if ( o ) { <nl> stream * s = o -> ptr ; <nl> ids [ id_idx ] = s -> last_id ;
int clusterLoadConfig ( char * filename ) { <nl> while ( fgets ( line , maxline , fp ) != NULL ) { <nl> int argc ; <nl> sds * argv = sdssplitargs ( line ,& argc ); <nl> + if ( argv == NULL ) goto fmterr ; <nl> + <nl> clusterNode * n , * master ; <nl> char * p , * s ; <nl> 
int luaRedisGenericCommand ( lua_State * lua , int raise_error ) { <nl> luaSortArray ( lua ); <nl> } <nl> sdsfree ( reply ); <nl> + c -> reply_bytes = 0 ; <nl>  <nl> cleanup : <nl> /* Clean up . Command code may have changed argv / argc so we use the
int rdbSaveDoubleValue ( rio * rdb , double val ) { <nl>  <nl> /* For information about double serialization check rdbSaveDoubleValue () */ <nl> int rdbLoadDoubleValue ( rio * rdb , double * val ) { <nl> - char buf [ 128 ]; <nl> + char buf [ 256 ]; <nl> unsigned char len ; <nl>  <nl> if ( rioRead ( rdb ,& len , 1 ) == 0 ) return - 1 ;
static int handle_metadata ( RTMPContext * rt , RTMPPacket * pkt ) <nl> pts = cts ; <nl> ts += cts - pts ; <nl> pts = cts ; <nl> + if ( size + 3 + 4 > pkt -> data + pkt -> size - next ) <nl> + break ; <nl> bytestream_put_byte (& p , type ); <nl> bytestream_put_be24 (& p , size ); <nl> bytestream_put_be24 (& p , ts );
static int read_header ( ShortenContext * s ) <nl> s -> channels = get_uint ( s , CHANSIZE ); <nl> if ( s -> channels <= 0 || s -> channels > MAX_CHANNELS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + s -> channels = 0 ; <nl> return - 1 ; <nl> } <nl> s -> avctx -> channels = s -> channels ;
static int audio_decode_frame ( VideoState * is , double * pts_ptr ) <nl> /* free the current packet */ <nl> if ( pkt -> data ) <nl> av_free_packet ( pkt ); <nl> + memset ( pkt_temp , 0 , sizeof (* pkt_temp )); <nl>  <nl> if ( is -> paused || is -> audioq . abort_request ) { <nl> return - 1 ;
static int fic_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " Could not allocate slice data .\ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( ctx -> slice_data , 0 , nslices * sizeof ( ctx -> slice_data [ 0 ])); <nl>  <nl> for ( slice = 0 ; slice < nslices ; slice ++) { <nl> unsigned slice_off = AV_RB32 ( src + tsize + FIC_HEADER_SIZE + slice * 4 );
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> break ; <nl> case MKTAG (' s ', ' t ', ' r ', ' f '): <nl> /* stream header */ <nl> - if ( stream_index >= s -> nb_streams || avi -> dv_demux ) { <nl> + if ( stream_index >= ( unsigned ) s -> nb_streams || avi -> dv_demux ) { <nl> url_fskip ( pb , size ); <nl> } else { <nl> st = s -> streams [ stream_index ];
int avfilter_graph_add_filter ( AVFilterGraph * graph , AVFilterContext * filter ) <nl> graph -> filters = filters ; <nl> graph -> filters [ graph -> nb_filters ++] = filter ; <nl>  <nl> +# if FF_API_FOO_COUNT <nl> + graph -> filter_count = graph -> nb_filters ; <nl> +# endif <nl> + <nl> return 0 ; <nl> } <nl> 
static int decode_frame_adu ( AVCodecContext * avctx , void * data , <nl> /* update codec info */ <nl> avctx -> sample_rate = s -> sample_rate ; <nl> avctx -> channels = s -> nb_channels ; <nl> + avctx -> channel_layout = s -> nb_channels == 1 ? AV_CH_LAYOUT_MONO : AV_CH_LAYOUT_STEREO ; <nl> if (! avctx -> bit_rate ) <nl> avctx -> bit_rate = s -> bit_rate ; <nl> 
static int udp_open ( URLContext * h , const char * uri , int flags ) <nl> goto fail ; <nl> } <nl>  <nl> - if (( s -> is_multicast || ! s -> local_port ) && ( h -> flags & AVIO_FLAG_READ )) <nl> + if (( s -> is_multicast || s -> local_port < 0 ) && ( h -> flags & AVIO_FLAG_READ )) <nl> s -> local_port = port ; <nl>  <nl> if ( localaddr [ 0 ])
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> if (! get_bits1 (& ctx -> gb ) || ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> transform_id = get_bits (& ctx -> gb , 5 ); <nl> - if (! transforms [ transform_id ]. inv_trans ) { <nl> + if ( transform_id >= FF_ARRAY_ELEMS ( transforms ) || <nl> + ! transforms [ transform_id ]. inv_trans ) { <nl> av_log_ask_for_sample ( avctx , " Unimplemented transform : % d !\ n ", transform_id ); <nl> return AVERROR_PATCHWELCOME ; <nl> }
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> if ( st -> codec -> bit_rate > 0 ) { <nl> - if ( INT_MAX - st -> codec -> bit_rate > bit_rate ) { <nl> + if ( INT_MAX - st -> codec -> bit_rate < bit_rate ) { <nl> bit_rate = 0 ; <nl> break ; <nl> }
static int mjpeg_decode_frame ( AVCodecContext * avctx , <nl> *( dst ++) = x ; <nl> if ( x == 0xff ) <nl> { <nl> - while (* src == 0xff ) src ++; <nl> + while ( src < buf_end && x == 0xff ) <nl> + x = *( src ++); <nl>  <nl> - x = *( src ++); <nl> if ( x >= 0xd0 && x <= 0xd7 ) <nl> *( dst ++) = x ; <nl> else if ( x )
static int video_thread ( void * arg ) <nl> filt_out = is -> out_video_filter ; <nl> # endif <nl>  <nl> + if (! frame ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> for (;;) { <nl> # if CONFIG_AVFILTER <nl> AVRational tb ;
static inline int put_bits_count ( PutBitContext * s ) <nl> static inline void flush_put_bits ( PutBitContext * s ) <nl> { <nl> # ifndef BITSTREAM_WRITER_LE <nl> - s -> bit_buf <<= s -> bit_left ; <nl> + if ( s -> bit_left < 32 ) <nl> + s -> bit_buf <<= s -> bit_left ; <nl> # endif <nl> while ( s -> bit_left < 32 ) { <nl> /* XXX : should test end of buffer */
static int mov_write_udta_tag ( ByteIOContext * pb , MOVMuxContext * mov , <nl> put_be32 ( pb , size + 8 ); <nl> put_tag ( pb , " udta "); <nl> put_buffer ( pb , buf , size ); <nl> - av_free ( buf ); <nl> } <nl> + av_free ( buf ); <nl>  <nl> return 0 ; <nl> }
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> * q ++ = 0xe0 ; <nl> } else if ( st -> codec -> codec_type == AVMEDIA_TYPE_AUDIO && <nl> ( st -> codec -> codec_id == CODEC_ID_MP2 || <nl> - st -> codec -> codec_id == CODEC_ID_MP3 )) { <nl> + st -> codec -> codec_id == CODEC_ID_MP3 || <nl> + st -> codec -> codec_id == CODEC_ID_AAC )) { <nl> * q ++ = 0xc0 ; <nl> } else { <nl> * q ++ = 0xbd ;
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> for ( i = 0 ; i < sc -> chunk_count ; i ++) { <nl> current_offset = sc -> chunk_offsets [ i ]; <nl> - if ( stsc_index + 1 < sc -> stsc_count && <nl> + while ( stsc_index + 1 < sc -> stsc_count && <nl> i + 1 == sc -> stsc_data [ stsc_index + 1 ]. first ) <nl> stsc_index ++; <nl> for ( j = 0 ; j < sc -> stsc_data [ stsc_index ]. count ; j ++) {
static int set_channel_layout ( AVCodecContext * avctx , int channels , int num_core_ <nl> s -> channel_order_tab = ff_dca_channel_reorder_nolfe [ s -> amode ]; <nl> } <nl>  <nl> + if ( channels < ff_dca_channels [ s -> amode ]) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( channels > !! s -> lfe && <nl> s -> channel_order_tab [ channels - 1 - !! s -> lfe ] < 0 ) <nl> return AVERROR_INVALIDDATA ;
static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> char * p ; <nl> int file_idx = strtol ( in -> name , & p , 0 ); <nl>  <nl> - if ( file_idx < 0 || file_idx > nb_input_files ) { <nl> + if ( file_idx < 0 || file_idx >= nb_input_files ) { <nl> av_log ( NULL , AV_LOG_FATAL , " Invalid file index % d in filtegraph description % s .\ n ", <nl> file_idx , fg -> graph_desc ); <nl> exit_program ( 1 );
static void decode_frame ( SiprContext * ctx , SiprParameters * params , <nl> memcpy ( ctx -> postfilter_syn5k0 , ctx -> postfilter_syn5k0 + frame_size , <nl> LP_FILTER_ORDER * sizeof ( float )); <nl> } <nl> - memcpy ( ctx -> excitation , excitation - PITCH_DELAY_MAX - L_INTERPOL , <nl> + memmove ( ctx -> excitation , excitation - PITCH_DELAY_MAX - L_INTERPOL , <nl> ( PITCH_DELAY_MAX + L_INTERPOL ) * sizeof ( float )); <nl>  <nl> ff_acelp_apply_order_2_transfer_function ( out_data , synth ,
int ff_rtmp_packet_create ( RTMPPacket * pkt , int channel_id , RTMPPacketType type , <nl> int timestamp , int size ) <nl> { <nl> if ( size ) { <nl> - pkt -> data = av_malloc ( size ); <nl> + pkt -> data = av_realloc ( NULL , size ); <nl> if (! pkt -> data ) <nl> return AVERROR ( ENOMEM ); <nl> }
static int xa_read_packet ( AVFormatContext * s , <nl> unsigned int packet_size ; <nl> int ret ; <nl>  <nl> - if ( xa -> sent_bytes > xa -> out_size ) <nl> - return AVERROR ( EIO ); <nl> + if ( xa -> sent_bytes >= xa -> out_size ) <nl> + return AVERROR_EOF ; <nl> /* 1 byte header and 14 bytes worth of samples * number channels per block */ <nl> packet_size = 15 * st -> codec -> channels ; <nl> 
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> break ; <nl>  <nl> arraylen = avio_rb32 ( ioc ); <nl> + if ( arraylen >> 28 ) <nl> + break ; <nl> + <nl> /* <nl> * Expect only ' times ' or ' filepositions ' sub - arrays in other case refuse to use such metadata <nl> * for indexing
static av_cold int qsv_enc_init ( AVCodecContext * avctx ) <nl>  <nl> if ( q -> load_plugin != LOAD_PLUGIN_NONE ) { <nl> static const char * uid_hevcenc_sw = " 2fca99749fdb49aeb121a5b63ef568f7 "; <nl> - static const char * uid_hevcenc_hw = " e5400a06c74d41f5b12d430bbaa23d0b "; <nl> + static const char * uid_hevcenc_hw = " 6fadc791a0c2eb479ab6dcd5ea9da347 "; <nl>  <nl> if ( q -> qsv . load_plugins [ 0 ]) { <nl> av_log ( avctx , AV_LOG_WARNING ,
static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> FLACContext * s = avctx -> priv_data ; <nl> s -> avctx = avctx ; <nl>  <nl> + avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> + <nl> if ( avctx -> extradata_size > 4 ) { <nl> /* initialize based on the demuxer - supplied streamdata header */ <nl> if ( avctx -> extradata_size == FLAC_STREAMINFO_SIZE ) { <nl> static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> } <nl> } <nl>  <nl> - avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> return 0 ; <nl> } <nl> 
static int tls_open ( URLContext * h , const char * uri , int flags , AVDictionary ** op <nl> struct addrinfo hints = { 0 }, * ai = NULL ; <nl> const char * proxy_path ; <nl> int use_proxy ; <nl> -# if CONFIG_OPENSSL <nl> +# if CONFIG_OPENSSL && ! CONFIG_GNUTLS <nl> BIO * bio ; <nl> # endif <nl> 
void mpeg4_encode_mb ( MpegEncContext * s , <nl> cbpy ^= 0xf ; <nl> put_bits ( pb2 , cbpy_tab [ cbpy ][ 1 ], cbpy_tab [ cbpy ][ 0 ]); <nl>  <nl> + if (! s -> progressive_sequence ){ <nl> + if ( cbp ) <nl> + put_bits ( pb2 , 1 , s -> interlaced_dct ); <nl> + } <nl> + <nl> if ( interleaved_stats ){ <nl> bits = get_bit_count (& s -> pb ); <nl> s -> misc_bits += bits - s -> last_bits ;
static int ffm_write_header ( AVFormatContext * s ) <nl> static int ffm_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> int64_t dts ; <nl> - uint8_t header [ FRAME_HEADER_SIZE ]; <nl> + uint8_t header [ FRAME_HEADER_SIZE + 4 ]; <nl> int header_size = FRAME_HEADER_SIZE ; <nl>  <nl> dts = s -> timestamp + pkt -> dts ;
static int pva_read_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> flags = get_byte ( pb ); <nl> length = get_be16 ( pb ); <nl>  <nl> - pts_flag = ( flags & 0x10 ) >> 4 ; <nl> + pts_flag = flags & 0x10 ; <nl>  <nl> if ( syncword != PVA_MAGIC ) { <nl> av_log ( s , AV_LOG_ERROR , " invalid syncword \ n ");
static int vp9_handle_packet ( AVFormatContext * ctx , PayloadContext * rtp_vp9_ctx , <nl> return 0 ; <nl> } <nl>  <nl> + static void vp9_close_context ( PayloadContext * vp9 ) <nl> +{ <nl> + ffio_free_dyn_buf (& vp9 -> buf ); <nl> +} <nl> + <nl> RTPDynamicProtocolHandler ff_vp9_dynamic_handler = { <nl> . enc_name = " VP9 ", <nl> . codec_type = AVMEDIA_TYPE_VIDEO , <nl> . codec_id = AV_CODEC_ID_VP9 , <nl> . priv_data_size = sizeof ( PayloadContext ), <nl> . init = vp9_init , <nl> + . close = vp9_close_context , <nl> . parse_packet = vp9_handle_packet <nl> };
static int decode_wdlt ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> segments = bytestream2_get_le16 ( gb ); <nl> } <nl> line_ptr = frame ; <nl> + if ( frame_end - frame < width ) <nl> + return AVERROR_INVALIDDATA ; <nl> frame += width ; <nl> y ++; <nl> while ( segments --) {
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
static void vc1_mc_4mv_chroma4 ( VC1Context * v ) <nl> mquant = v -> altpq ; \ <nl> if (( edges & 8 ) && s -> mb_y == ( s -> mb_height - 1 )) \ <nl> mquant = v -> altpq ; \ <nl> + if (! mquant || mquant > 31 ) { \ <nl> + av_log ( v -> s . avctx , AV_LOG_ERROR , \ <nl> + " Overriding invalid mquant % d \ n ", mquant ); \ <nl> + mquant = 1 ; \ <nl> + } \ <nl> } <nl>  <nl> /**
static int mpegts_read_packet ( AVFormatContext * s , <nl> MpegTSContext * ts = s -> priv_data ; <nl> int ret , i ; <nl>  <nl> + pkt -> size = - 1 ; <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> static int mpegts_read_packet ( AVFormatContext * s , <nl> } <nl> } <nl>  <nl> + if (! ret && pkt -> size < 0 ) <nl> + ret = AVERROR ( EINTR ); <nl> return ret ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> } else if ( s -> bit_depth == 16 && <nl> s -> color_type == PNG_COLOR_TYPE_GRAY_ALPHA ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_YA16BE ; <nl> + } else if ( s -> bit_depth == 16 && <nl> + s -> color_type == PNG_COLOR_TYPE_RGB_ALPHA ) { <nl> + avctx -> pix_fmt = AV_PIX_FMT_RGBA64BE ; <nl> } else { <nl> avpriv_report_missing_feature ( avctx , <nl> " Bit depth % d color type % d ",
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
# define XAVS_PART_B8X8 0x100 /* Analyze b16x8 , b */ <nl>  <nl> typedef struct XavsContext { <nl> + AVClass * class ; <nl> xavs_param_t params ; <nl> xavs_t * enc ; <nl> xavs_picture_t pic ;
static void set_codec_str ( AVFormatContext * s , AVCodecParameters * par , <nl> tags [ 0 ] = ff_mp4_obj_type ; <nl> oti = av_codec_get_tag ( tags , par -> codec_id ); <nl> if ( oti ) <nl> - av_strlcatf ( str , size , ".% 02x ", oti ); <nl> + av_strlcatf ( str , size , ".% 02 " SCNx32 , oti ); <nl> else <nl> return ; <nl> 
static inline int op ( uint8_t ** dst , const uint8_t * dst_end , <nl> int striplen = FFMIN ( count , remaining ); <nl> if ( buf ) { <nl> striplen = FFMIN ( striplen , buf_end - * buf ); <nl> + if (* buf >= buf_end ) <nl> + goto exhausted ; <nl> memcpy (* dst , * buf , striplen ); <nl> * buf += striplen ; <nl> } else if ( pixel >= 0 )
FF_ENABLE_DEPRECATION_WARNINGS <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Invalid combination - slices % d and - max_nal_size % d .\ n ", <nl> avctx -> slices , s -> max_nal_size ); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl>  <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } else { <nl> av_log ( avctx , AV_LOG_ERROR , " Invalid - max_nal_size , " <nl> " specify a valid max_nal_size to use - slice_mode dyn \ n "); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl> }
int ff_adts_decode_extradata ( AVFormatContext * s , ADTSContext * adts , uint8_t * buf <nl> av_log ( s , AV_LOG_ERROR , " Scalable configurations are not allowed in ADTS \ n "); <nl> return - 1 ; <nl> } <nl> + if ( get_bits (& gb , 1 )) { <nl> + av_log ( s , AV_LOG_ERROR , " Extension flag is not allowed in ADTS \ n "); <nl> + return - 1 ; <nl> + } <nl> if (! adts -> channel_conf ) { <nl> init_put_bits (& pb , adts -> pce_data , MAX_PCE_SIZE ); <nl> 
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl>  <nl> log = av_log2 ( buf ); <nl>  <nl> - if ( log > 31 - 11 ){ <nl> + if ( log - k >= 32 - MIN_CACHE_BITS ){ <nl> buf >>= log - k ; <nl> buf += ( 30 - log )<< k ; <nl> LAST_SKIP_BITS ( re , gb , 32 + k - log );
static int xan_unpack ( uint8_t * dest , const int dest_len , <nl> if ( size + size2 > dest_end - dest ) <nl> break ; <nl> } <nl> - if ( src + size > src_end || dest + size + size2 > dest_end ) <nl> + if ( src + size > src_end || dest + size + size2 > dest_end || <nl> + dest - orig_dest + size < back ) <nl> return - 1 ; <nl> bytestream_get_buffer (& src , dest , size ); <nl> dest += size ;
static int a64_write_header ( AVFormatContext * s ) <nl> 0x00 , // charset_lifetime ( multi only ) <nl> 0x00 // fps in 50 / fps ; <nl> }; <nl> + <nl> + if ( avctx -> extradata_size < 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Missing extradata \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> switch ( avctx -> codec -> id ) { <nl> case AV_CODEC_ID_A64_MULTI : <nl> header [ 2 ] = 0x00 ;
static void unpack_alpha ( GetBitContext * gb , uint16_t * dst , int num_coeffs , <nl> dst [ idx ++] = alpha_val >> 6 ; <nl> else <nl> dst [ idx ++] = ( alpha_val << 2 ) | ( alpha_val >> 6 ); <nl> - if ( idx == num_coeffs - 1 ) <nl> + if ( idx >= num_coeffs - 1 ) <nl> break ; <nl> } while ( get_bits1 ( gb )); <nl> val = get_bits ( gb , 4 );
static int flush_packet ( AVFormatContext * ctx , int stream_index , <nl>  <nl> if ( stuffing_size < 0 ) <nl> stuffing_size = 0 ; <nl> + <nl> + if ( startcode == PRIVATE_STREAM_1 && id >= 0xa0 ) { <nl> + if ( payload_size < av_fifo_size ( stream -> fifo )) <nl> + stuffing_size += payload_size % stream -> lpcm_align ; <nl> + } <nl> + <nl> if ( stuffing_size > 16 ) { /*<= 16 for MPEG - 1 , <= 32 for MPEG - 2 */ <nl> pad_packet_bytes += stuffing_size ; <nl> packet_size -= stuffing_size ;
int ff_rtp_chain_mux_open ( AVFormatContext ** out , AVFormatContext * s , <nl> /* Get the payload type from the codec */ <nl> if ( st -> id < RTP_PT_PRIVATE ) <nl> rtpctx -> streams [ 0 ]-> id = <nl> - ff_rtp_get_payload_type ( rtpctx , st -> codec , idx ); <nl> + ff_rtp_get_payload_type ( s , st -> codec , idx ); <nl> else <nl> rtpctx -> streams [ 0 ]-> id = st -> id ; <nl> 
int ffurl_open ( URLContext ** puc , const char * filename , int flags , <nl> int ret = ffurl_alloc ( puc , filename , flags , int_cb , protocols ); <nl> if ( ret ) <nl> return ret ; <nl> + if ( options && <nl> + ( ret = av_opt_set_dict (* puc , options )) < 0 ) <nl> + goto fail ; <nl> if ( options && (* puc )-> prot -> priv_data_class && <nl> ( ret = av_opt_set_dict ((* puc )-> priv_data , options )) < 0 ) <nl> goto fail ;
static int probe_buf_write ( void * opaque , uint8_t * buf , int buf_size ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> int ret ; <nl> - uint8_t * buffer = av_malloc ( AVP_BUFFSIZE ); <nl> + uint8_t * buffer = av_mallocz ( AVP_BUFFSIZE ); <nl>  <nl> if (! buffer ) <nl> exit ( 1 );
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + case NAL_IDR_SLICE : <nl> + case NAL_SLICE : <nl> nals_needed = nal_index ; <nl> } <nl> continue ;
static int cyuv_decode_init ( AVCodecContext * avctx ) <nl>  <nl> s -> avctx = avctx ; <nl> s -> width = avctx -> width ; <nl> + /* width needs to be divisible by 4 for this codec to work */ <nl> + if ( s -> width & 0x3 ) <nl> + return - 1 ; <nl> s -> height = avctx -> height ; <nl> avctx -> pix_fmt = PIX_FMT_YUV411P ; <nl> avctx -> has_b_frames = 0 ;
static void truemotion1_decode_24bit ( TrueMotion1Context * s ) <nl> int index ; <nl>  <nl> /* clean out the line buffer */ <nl> - memset ( s -> vert_pred , 0 , s -> avctx -> width * sizeof ( unsigned short )); <nl> + memset ( s -> vert_pred , 0 , s -> avctx -> width * sizeof ( unsigned int )); <nl>  <nl> GET_NEXT_INDEX (); <nl> 
static int mov_read_stsd ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> sc -> stsd_count = entries ; <nl> - sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof ( sc -> extradata_size )); <nl> + sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof (* sc -> extradata_size )); <nl> if (! sc -> extradata_size ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int process_line ( URLContext * h , char * line , int line_count , <nl> # ifdef DEBUG <nl> printf (" http_code =% d \ n ", s -> http_code ); <nl> # endif <nl> + /* error codes are 4xx and 5xx */ <nl> + if ( s -> http_code >= 400 && s -> http_code < 600 ) <nl> + return - 1 ; <nl> } else { <nl> while (* p != '\ 0 ' && * p != ':') <nl> p ++;
int attribute_align_arg avcodec_encode_audio2 ( AVCodecContext * avctx , <nl> avctx -> time_base ); <nl> } <nl> avpkt -> dts = avpkt -> pts ; <nl> + } else { <nl> + avpkt -> size = 0 ; <nl> } <nl> } else { <nl> /* for compatibility with encoders not supporting encode2 (), we need to
static int sol_read_packet ( AVFormatContext * s , <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR ( EIO ); <nl> ret = av_get_packet ( s -> pb , pkt , MAX_SIZE ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> pkt -> stream_index = 0 ; <nl>  <nl> /* note : we need to modify the packet size here to handle the last
static int decode_vol_header ( MpegEncContext * s , GetBitContext * gb ){ <nl>  <nl> s -> progressive_sequence = <nl> s -> progressive_frame = get_bits1 ( gb )^ 1 ; <nl> + s -> interlaced_dct = 0 ; <nl> if (! get_bits1 ( gb ) && ( s -> avctx -> debug & FF_DEBUG_PICT_INFO )) <nl> av_log ( s -> avctx , AV_LOG_INFO , " MPEG4 OBMC not supported ( very likely buggy encoder )\ n "); /* OBMC Disable */ <nl> if ( vo_ver_id == 1 ) {
static int mp3_read_probe ( AVProbeData * p ) <nl>  <nl> max_frames = 0 ; <nl> buf = buf0 ; <nl> - end = buf + p -> buf_size - sizeof ( uint32_t ); <nl> + end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl>  <nl> for (; buf < end ; buf = buf2 + 1 ) { <nl> buf2 = buf ;
static int mpegts_read_header ( AVFormatContext * s , <nl> /* normal demux */ <nl>  <nl> /* first do a scaning to get all the services */ <nl> - if ( avio_seek ( pb , pos , SEEK_SET ) < 0 ) <nl> + if ( pb -> seekable && avio_seek ( pb , pos , SEEK_SET ) < 0 ) <nl> av_log ( s , AV_LOG_ERROR , " Unable to seek back to the start \ n "); <nl>  <nl> mpegts_open_section_filter ( ts , SDT_PID , sdt_cb , ts , 1 );
static int vaapi_encode_h264_write_extra_header ( AVCodecContext * avctx , <nl>  <nl> if ( priv -> sei_needed ) { <nl> if ( priv -> aud_needed ) { <nl> - vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + err = vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + if ( err < 0 ) <nl> + goto fail ; <nl> priv -> aud_needed = 0 ; <nl> } <nl> 
static int ac3_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> } <nl>  <nl> /* check for crc mismatch */ <nl> - if ( avctx -> error_resilience > 0 ) { <nl> + if ( avctx -> error_resilience >= FF_ER_CAREFUL ) { <nl> if ( av_crc ( av_crc_get_table ( AV_CRC_16_ANSI ), 0 , & buf [ 2 ], s -> frame_size - 2 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame CRC mismatch \ n "); <nl> return - 1 ;
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> /* skip B - frames if we don ' t have reference frames */ <nl> if ( s -> last_picture_ptr == NULL && ( s -> pict_type == AV_PICTURE_TYPE_B || s -> droppable )) { <nl> - goto err ; <nl> + goto end ; <nl> } <nl> if (( avctx -> skip_frame >= AVDISCARD_NONREF && s -> pict_type == AV_PICTURE_TYPE_B ) || <nl> ( avctx -> skip_frame >= AVDISCARD_NONKEY && s -> pict_type != AV_PICTURE_TYPE_I ) ||
static int oma_read_header ( AVFormatContext * s , <nl>  <nl> ff_id3v2_read ( s , ID3v2_EA3_MAGIC ); <nl> ret = avio_read ( s -> pb , buf , EA3_HEADER_SIZE ); <nl> + if ( ret < EA3_HEADER_SIZE ) <nl> + return - 1 ; <nl>  <nl> if ( memcmp ( buf , (( const uint8_t []){' E ', ' A ', ' 3 '}), 3 ) || buf [ 4 ] != 0 || buf [ 5 ] != EA3_HEADER_SIZE ) { <nl> av_log ( s , AV_LOG_ERROR , " Couldn ' t find the EA3 header !\ n ");
static int codebook_sanity_check_for_rate_quarter ( const uint8_t * cbgain ) <nl> * @ param gain array holding the 4 pitch subframe gain values <nl> * @ param cdn_vector array for the generated scaled codebook vector <nl> */ <nl> - static void compute_svector ( const QCELPContext * q , const float * gain , <nl> + static void compute_svector ( QCELPContext * q , const float * gain , <nl> float * cdn_vector ) <nl> { <nl> int i , j , k ;
static int cbs_h2645_write_nal_unit ( CodedBitstreamContext * ctx , <nl> // Overflow but we didn ' t notice . <nl> av_assert0 ( put_bits_count (& pbc ) <= 8 * priv -> write_buffer_size ); <nl>  <nl> + if ( err < 0 ) { <nl> + // Write failed for some other reason . <nl> + return err ; <nl> + } <nl> + <nl> if ( put_bits_count (& pbc ) % 8 ) <nl> unit -> data_bit_padding = 8 - put_bits_count (& pbc ) % 8 ; <nl> else
void avcodec_pix_fmt_string ( char * buf , int buf_size , enum PixelFormat pix_fmt ) <nl> char is_alpha_char = info . is_alpha ? ' y ' : ' n '; <nl>  <nl> snprintf ( buf , buf_size , <nl> - "%- 11s " " % 1d " " % 2d " " % c ", <nl> + "%- 11s % 5d % 9d % 6c ", <nl> info . name , <nl> info . nb_channels , <nl> info . depth ,
static int avi_read_header ( AVFormatContext * s ) <nl> ast = s -> streams [ 0 ]-> priv_data ; <nl> av_freep (& s -> streams [ 0 ]-> codec -> extradata ); <nl> av_freep (& s -> streams [ 0 ]-> codec ); <nl> + av_freep (& s -> streams [ 0 ]-> info ); <nl> av_freep (& s -> streams [ 0 ]); <nl> s -> nb_streams = 0 ; <nl> if ( CONFIG_DV_DEMUXER ) {
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels == 0 || channels > 64 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
static int append_entry ( HLSContext * hls , uint64_t duration ) <nl> if (! en ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - av_get_frame_filename ( en -> name , sizeof ( en -> name ), hls -> basename , <nl> + av_get_frame_filename ( en -> name , sizeof ( en -> name ), <nl> + av_basename ( hls -> basename ), <nl> hls -> number - 1 ); <nl>  <nl> en -> duration = duration ;
static int handle_connection ( HTTPContext * c ) <nl> } <nl> if ( http_send_data ( c ) < 0 ) <nl> return - 1 ; <nl> + /* close connection if trailer sent */ <nl> + if ( c -> state == HTTPSTATE_SEND_DATA_TRAILER ) <nl> + return - 1 ; <nl> break ; <nl> case HTTPSTATE_RECEIVE_DATA : <nl> /* no need to read if no events */
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & bc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
int read_mainconfig ( int reload ) <nl> if ( mainconfig . reject_delay > mainconfig . cleanup_delay ) { <nl> mainconfig . reject_delay = mainconfig . cleanup_delay ; <nl> } <nl> + if ( mainconfig . reject_delay < 0 ) mainconfig . reject_delay = 0 ; <nl>  <nl> /* <nl> * Initialize the old " bind_address " and " port ", first .
static int dhcp_process ( REQUEST * request ) <nl> break ; <nl> } <nl>  <nl> + /* <nl> + * Releases don ' t get replies . <nl> + */ <nl> + if ( request -> packet -> code == PW_DHCP_RELEASE ) { <nl> + request -> reply -> code = 0 ; <nl> + } <nl> + <nl> return 1 ; <nl> } <nl> 
static void msn_close ( struct gaim_connection * gc ) <nl> g_slist_free ( md -> msgq ); <nl> } <nl>  <nl> + g_free ( md -> grouplist ); <nl> + <nl> g_free ( md ); <nl> } <nl> 
file_transfer_t * imcb_file_send_start ( struct im_connection * ic , char * handle , ch <nl> bee_t * bee = ic -> bee ; <nl> bee_user_t * bu = bee_user_by_handle ( bee , ic , handle ); <nl>  <nl> - if ( bee -> ui -> ft_in_start ) { <nl> + if ( bee -> ui -> ft_in_start && bu ) { <nl> return bee -> ui -> ft_in_start ( bee , bu , file_name , file_size ); <nl> } else { <nl> return NULL ;
static gboolean prplcb_xfer_new_send_cb ( gpointer data , gint fd , b_input_conditio <nl> /* TODO ( wilmer ): After spreading some more const goodness in BitlBee , <nl> remove the evil cast below . */ <nl> px -> ft = imcb_file_send_start ( ic , ( char *) who , xfer -> filename , xfer -> size ); <nl> + <nl> + if (! px -> ft ) { <nl> + return FALSE ; <nl> + } <nl> px -> ft -> data = px ; <nl>  <nl> px -> ft -> accept = prpl_xfer_accept ;
dissect_smpp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint32 offset = 0 ; <nl> while ( tvb_reported_length_remaining ( tvb , offset ) > 0 ) { <nl> guint16 pdu_len = tvb_get_ntohl ( tvb , offset ); <nl> + if ( pdu_len < 1 ) <nl> + THROW ( ReportedBoundsError ); <nl> gint pdu_real_len = tvb_length_remaining ( tvb , offset ); <nl> tvbuff_t * pdu_tvb ; <nl> 
proto_reg_handoff_e100 ( void ) <nl> /* Check all UDP traffic , as the specific UDP port is configurable */ <nl> heur_dissector_add (" udp ", dissect_e100 , " E100 over UDP ", " e100_udp ", proto_e100 , HEURISTIC_ENABLE ); <nl> /* e100 traffic encapsulates traffic from the ethernet frame on */ <nl> - eth_handle = find_dissector (" eth "); <nl> + eth_handle = find_dissector (" eth_withoutfcs "); <nl> } <nl>  <nl> /*
capture_all_filter_check_syntax_cb ( GtkWidget * w _U_ , gpointer user_data _U_ ) <nl> } <nl> # ifdef HAVE_EXTCAP <nl> /* Can ' t verify extcap capture filters */ <nl> - if ( device . if_info . extcap != NULL ) <nl> + if ( device . if_info . extcap != NULL && strlen ( device . if_info . extcap ) > 0 ) <nl> continue ; <nl> # endif <nl> filter_text = gtk_combo_box_text_get_active_text ( GTK_COMBO_BOX_TEXT ( filter_cm ));
static void dissect_HEARTBEAT_VIRTUAL ( tvbuff_t * tvb , packet_info * pinfo _U_ , gin <nl> if (!( flags & FLAG_VIRTUAL_HEARTBEAT_N )) { <nl> proto_tree_add_item ( sil_tree_writer , hf_rtps_virtual_heartbeat_num_virtual_guids , tvb , <nl> offset , 4 , little_endian ? ENC_LITTLE_ENDIAN : ENC_BIG_ENDIAN ); <nl> + num_virtual_guids = NEXT_guint32 ( tvb , offset , little_endian ); <nl> offset += 4 ; <nl> } else { <nl> num_virtual_guids = 0 ;
csv_handle ( GtkTreeModel * model , GtkTreePath * path _U_ , GtkTreeIter * iter , <nl> i == PERCENT_COLUMN || i == PROTECTION_COLUMN ) { <nl> gtk_tree_model_get ( model , iter , i , & table_text , - 1 ); <nl> g_string_append ( CSV_str , table_text ); <nl> + g_free ( table_text ); <nl> } else { <nl> gtk_tree_model_get ( model , iter , i , & table_value , - 1 ); <nl> g_string_append_printf ( CSV_str , "% u ", table_value );
dissect_data_chunk ( tvbuff_t * chunk_tvb , <nl> */ <nl> if ( b_bit ) <nl> { <nl> - gboolean retval ; <nl> + gboolean retval = FALSE ; <nl>  <nl> /* <nl> * If this particular fragment happens to get a ReportedBoundsError
UAT_CSTRING_CB_DEF ( uat_ue_keys_records , rrcIntegrityKeyString , uat_ue_keys_reco <nl>  <nl> static gboolean global_pdcp_decipher_signalling = FALSE ; <nl> static gboolean global_pdcp_decipher_userplane = FALSE ; <nl> - static gboolean global_pdcp_check_integrity = FALSE ; <nl> # endif <nl> + static gboolean global_pdcp_check_integrity = FALSE ; <nl>  <nl> static const value_string direction_vals [] = <nl> {
PacketList :: PacketList ( QWidget * parent ) : <nl> decode_as_ ( NULL ), <nl> ctx_column_ (- 1 ), <nl> capture_in_progress_ ( false ), <nl> - tail_timer_id_ ( 0 ) <nl> + tail_timer_id_ ( 0 ), <nl> + rows_inserted_ ( false ) <nl> { <nl> QMenu * submenu , * subsubmenu ; <nl> QAction * action ;
void <nl> frame_data_reset ( frame_data * fdata ) <nl> { <nl> fdata -> flags . visited = 0 ; <nl> + fdata -> subnum = 0 ; <nl>  <nl> if ( fdata -> pfd ) { <nl> g_slist_free ( fdata -> pfd );
add_content_type ( proto_tree * tree , tvbuff_t * tvb , guint32 val_start , <nl> So we have to disable that one and become " slow " by pretending that <nl> the tree is " visible ". <nl> */ <nl> - PTREE_DATA ( tree )-> visible = 1 ; <nl> + if ( tree ) <nl> + PTREE_DATA ( tree )-> visible = 1 ; <nl>  <nl> * textual_content = NULL ; <nl> * well_known_content = 0 ;
tvb_captured_length ( const tvbuff_t * tvb ) <nl> static inline gint <nl> _tvb_captured_length_remaining ( const tvbuff_t * tvb , const gint offset ) <nl> { <nl> - guint abs_offset , rem_length ; <nl> + guint abs_offset = 0 , rem_length ; <nl> int exception ; <nl>  <nl> exception = compute_offset_and_remaining ( tvb , offset , & abs_offset , & rem_length );
save_decode_as_entries ( gchar ** err ) <nl>  <nl> dissector_all_tables_foreach_changed ( decode_as_write_entry , da_file ); <nl> fclose ( da_file ); <nl> + g_free ( daf_path ); <nl> return 0 ; <nl> } <nl> 
static int FieldInfo_get_range ( lua_State * L ) { <nl> r -> tvb = ep_new ( struct _wslua_tvb ); <nl>  <nl> r -> tvb -> ws_tvb = fi -> ds_tvb ; <nl> + r -> tvb -> expired = FALSE ; <nl> + r -> tvb -> need_free = FALSE ; <nl> r -> offset = fi -> start ; <nl> r -> len = fi -> length ; <nl> 
gboolean <nl> profile_exists ( const gchar * profilename , gboolean global ) <nl> { <nl> gchar * path = NULL , * global_path ; <nl> + if (! profilename ) <nl> + return FALSE ; <nl> if ( global ) { <nl> global_path = get_global_profiles_dir (); <nl> path = g_strdup_printf ("% s % s % s ", global_path ,
static void parse_outhdr_string ( const guchar * outhdr_string , gint outhdr_string_ <nl> guint d ; <nl>  <nl> /* Find digits */ <nl> - for ( ; n < outhdr_string_len ; n ++) { <nl> + for ( ; ( n < outhdr_string_len ) && ( number_digits < MAX_OUTHDR_VALUES ); n ++) { <nl> if (! g_ascii_isdigit ( outhdr_string [ n ])) { <nl> break ; <nl> }
dissect_sip_contact_item ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , gi <nl> gint current_offset ; <nl> gint queried_offset ; <nl> gint contact_params_start_offset = - 1 ; <nl> - gint contact_param_end_offset = - 1 ; <nl> + /* gint contact_param_end_offset = - 1 ;*/ <nl> uri_offset_info uri_offsets ; <nl>  <nl> /* skip Spaces and Tabs */
dissect_dcm_pdv_fragmented ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> pdv_body_len , <nl> !( pdv -> is_last_fragment )); <nl>  <nl> - if (( head && ( head -> next == NULL )) || pdv -> is_last_fragment ) { <nl> + if ( head && ( head -> next == NULL )) { <nl> /* Was not really fragmented , therefore use ' conventional ' decoding <nl> fragment_add_seq_next () won ' t add any items to the list , when last fragment only <nl> */
epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_da <nl> register_cb cb , <nl> gpointer client_data ) <nl> { <nl> - gboolean status = TRUE ; <nl> + volatile gboolean status = TRUE ; <nl>  <nl> /* initialize memory allocation subsystem */ <nl> wmem_init ();
fill_list ( GtkWidget * main_w ) <nl> * and use it later without any crashes . This may not be a <nl> * valid assumption . <nl> */ <nl> + g_free ( l_select ); <nl> l_select = ( GtkTreeIter *) g_memdup (& iter , sizeof ( iter )); <nl> } <nl> fl_entry = g_list_next ( fl_entry );
Dot11DecryptDecryptWPABroadcastKey ( const EAPOL_RSN_KEY * pEAPKey , guint8 * decrypt <nl> DEBUG_DUMP (" FullDecrKey :", new_key , 32 ); <nl>  <nl> if ( gcry_cipher_open (& rc4_handle , GCRY_CIPHER_ARCFOUR , GCRY_CIPHER_MODE_STREAM , 0 )) { <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> if ( gcry_cipher_setkey ( rc4_handle , new_key , sizeof ( new_key ))) { <nl> gcry_cipher_close ( rc4_handle ); <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> 
proto_register_sua ( void ) <nl> " This may affect TCAP ' s ability to recognize which messages belong to which TCAP session .", & set_addresses ); <nl>  <nl> heur_subdissector_list = register_heur_dissector_list (" sua "); <nl> - sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_NOT_ALLOW_DUPLICATE ); <nl> + sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_ALLOW_DUPLICATE ); <nl> sua_tap = register_tap (" sua "); <nl>  <nl> assocs = wmem_tree_new_autoreset ( wmem_epan_scope (), wmem_file_scope ());
void OverlayScrollBar :: paintEvent ( QPaintEvent * event ) <nl> pm_painter . setPen ( border_color ); <nl> pm_painter . drawLine ( near_dest . topLeft (), near_dest . bottomLeft ()); <nl> pm_painter . drawLine ( near_dest . topRight (), near_dest . bottomRight ()); <nl> + pm_painter . drawLine ( near_dest . bottomLeft (), near_dest . bottomRight ()); <nl> pm_painter . restore (); <nl>  <nl> // Draw the map .
typedef struct _ext_toolbar_update_list_t <nl> GList * entries ; <nl> } ext_toolbar_update_list_t ; <nl>  <nl> - extern gint <nl> + static gint <nl> ext_toolbar_find_item ( gconstpointer a , gconstpointer b ) <nl> { <nl> if ( a == 0 || b == 0 )
static int decode_bgp_link_nlri_prefix_descriptors ( tvbuff_t * tvb , <nl> break ; <nl>  <nl> case BGP_NLRI_TLV_IP_REACHABILITY_INFORMATION : <nl> - decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> - tvb , offset + 4 , 0 , " Reachability "); <nl> + if ( decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> + tvb , offset + 4 , 0 , " Reachability ") == - 1 ) <nl> + return diss_length ; <nl> break ; <nl> } <nl> 
dissect_esp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> /* Copy back the Authentication which was not encrypted */ <nl> if ( decrypted_len >= esp_auth_len ) <nl> { <nl> - tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , sizeof ( struct newesp )+ decrypted_len - esp_auth_len , esp_auth_len ); <nl> + tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , ( gint )( sizeof ( struct newesp )+ decrypted_len - esp_auth_len ), esp_auth_len ); <nl> } <nl>  <nl> /* Decryption has finished */
rtp_streams_stat_draw ( void * arg _U_ ) <nl>  <nl> list = g_list_next ( list ); <nl>  <nl> - g_free ( payload_type ); <nl> wmem_free ( NULL , src_addr ); <nl> wmem_free ( NULL , dst_addr ); <nl> wmem_free ( NULL , payload_type );
WirelessFrame :: WirelessFrame ( QWidget * parent ) : <nl>  <nl> WirelessFrame ::~ WirelessFrame () <nl> { <nl> + ws80211_free_interfaces ( interfaces_ ); <nl> delete ui ; <nl> } <nl> 
do_simple_message_box ( ESD_TYPE_E type , gboolean * notagain , <nl> if ( notagain != NULL ) { <nl> checkbox = gtk_check_button_new_with_label (" Don ' t show this message again ."); <nl> gtk_container_set_border_width ( GTK_CONTAINER ( checkbox ), 12 ); <nl> - gtk_box_pack_start ( GTK_BOX ( gtk_message_dialog_get_message_area ( GTK_MESSAGE_DIALOG ( msg_dialog ))), checkbox , <nl> - TRUE , TRUE , 0 ); <nl> + gtk_box_pack_start ( GTK_BOX ( gtk_dialog_get_content_area ( GTK_DIALOG ( msg_dialog ))), <nl> + checkbox , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( checkbox ); <nl> } <nl> 
extcap_register_preferences_callback ( gpointer key , gpointer value _U_ , gpointer <nl>  <nl> void extcap_register_preferences ( void ) <nl> { <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> module_t * dev_module = prefs_find_module (" extcap "); <nl>  <nl> if (! dev_module ) <nl> extcap_load_interface_list ( void ) <nl> gchar * argv ; <nl> gchar * error ; <nl>  <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> if ( _toolbars ) <nl> { <nl> // Remove existing interface toolbars here instead of in extcap_clear_interfaces ()
ws_pipe_wait_for_pipe ( HANDLE * pipe_handles , int num_pipe_handles , HANDLE pid ) <nl> int num_waiting_to_connect = 0 ; <nl> int num_handles = num_pipe_handles + 1 ; // PID handle is also added to list of handles . <nl>  <nl> + SecureZeroMemory ( pipeinsts , sizeof ( pipeinsts )); <nl> + <nl> if ( num_pipe_handles == 0 || num_pipe_handles > 3 ) <nl> { <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_DEBUG , " Invalid number of pipes given as argument .");
dissect_qnet6 ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * dat <nl> /* <nl> * data after header <nl> */ <nl> - if ( cklen != 0 ) <nl> + if ( cklen > 0 ) <nl> { <nl> crc = crc32_mpeg2_seed ( tvb_get_ptr ( tvb , 36 + 2 , cklen ), cklen , ~ crc ); <nl> crc = ~ crc ;
dissect_vmlab ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint8 attributes ; <nl> guint8 portgroup ; <nl>  <nl> - guint16 encap_proto ; <nl> + volatile guint16 encap_proto ; <nl>  <nl> col_set_str ( pinfo -> cinfo , COL_PROTOCOL , " VMLAB "); <nl> col_clear ( pinfo -> cinfo , COL_INFO );
nextcontext : <nl> tvb_previous_offset = tvb_find_guint8 ( tvb , tvb_current_offset , <nl> tvb_len , '=')+ 1 ; <nl> tvb_previous_offset = tvb_skip_wsp ( tvb , tvb_previous_offset ); <nl> - tvb_current_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> + tvb_next_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> tvb_len , '{'); <nl> + if ( tvb_current_offset >= tvb_next_offset ) { <nl> + proto_tree_add_text ( megaco_tree , tvb , 0 , 0 , "[ Parse error : Invalid offset ]"); <nl> + return ; <nl> + } <nl> + tvb_current_offset = tvb_next_offset ; <nl>  <nl>  <nl> tokenlen = tvb_current_offset - tvb_previous_offset ;
private Q_SLOTS : <nl> }; <nl>  <nl> Q_DECLARE_METATYPE ( ExtcapArgument ) <nl> + Q_DECLARE_METATYPE ( ExtcapArgument *) <nl>  <nl> class ExtArgText : public ExtcapArgument <nl> {
write_recent ( void ) <nl> g_free ( rf_path ); <nl> return FALSE ; <nl> } <nl> + g_free ( rf_path ); <nl>  <nl> fputs ("# Recent settings file for Wireshark " VERSION ".\ n " <nl> "#\ n "
dissect_rtp_hext_rfc5215_onebyte ( tvbuff_t * tvb , packet_info * pinfo , <nl> return ; <nl>  <nl> ext_length = ( ext_hdr_hdr & 0x0F ) + 1 ; <nl> + <nl> + /* Exit on malformed extension headers */ <nl> + if ( ext_offset + ext_length + 1 > tvb_captured_length ( tvb )) { <nl> + return ; <nl> + } <nl> + <nl> if ( rtp_hext_tree ) { <nl> rtp_hext_rfc5285_tree = proto_tree_add_subtree ( rtp_hext_tree , tvb , ext_offset , ext_length + 1 , <nl> ett_hdr_ext_rfc5285 , NULL , " RFC 5285 Header Extension ( One - Byte Header )");
void MulticastStatisticsDialog :: updateMulticastParameters () <nl>  <nl> param = buffer_alarm_threshold_le_ -> text (). toInt (& ok ); <nl> if ( ok && param > 0 ) { <nl> - mcast_stream_trigger = param ; <nl> + mcast_stream_bufferalarm = param ; <nl> } <nl>  <nl> param = stream_empty_speed_le_ -> text (). toInt (& ok );
dissect_attribute_value_pairs ( proto_tree * tree , packet_info * pinfo , tvbuff_t * tv <nl>  <nl> avp_vsa_len -= avp_vsa_header_len ; <nl>  <nl> + memset (& vendor_type , 0 , sizeof ( vendor_type )); <nl> if ( avp_is_extended ) { <nl> vendor_type . u8_code [ 0 ] = avp_type . u8_code [ 0 ]; <nl> vendor_type . u8_code [ 1 ] = avp_vsa_type ;
bool InterfaceTreeWidgetItem :: operator < ( const QTreeWidgetItem & other ) const { <nl> # include < QComboBox > <nl>  <nl> InterfaceTreeDelegate :: InterfaceTreeDelegate ( QObject * parent ) <nl> - : QStyledItemDelegate ( parent ) <nl> + : QStyledItemDelegate ( parent ), tree_ ( NULL ) <nl> { <nl> } <nl> 
int ieee80211_radiotap_iterator_init ( <nl>  <nl> /* find payload start allowing for extended bitmap ( s ) */ <nl> if ( iterator -> _bitmap_shifter & ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> + if (! ITERATOR_VALID ( iterator , sizeof ( guint32 ))) <nl> + return - EINVAL ; <nl> while ( get_unaligned_le32 ( iterator -> _arg ) & <nl> ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> iterator -> _arg += sizeof ( guint32 );
ZEND_API int zend_register_constant ( zend_constant * c ELS_DC ) <nl>  <nl> zend_str_tolower ( lowercase_name , c -> name_len ); <nl> if ( zend_hash_add ( EG ( zend_constants ), lowercase_name , c -> name_len , ( void *) c , sizeof ( zend_constant ), NULL )== FAILURE ) { <nl> + free ( c -> name ); <nl> zval_dtor (& c -> value ); <nl> zend_error ( E_NOTICE ," Constant % s already defined ", lowercase_name ); <nl> ret = FAILURE ;
static php_iconv_err_t _php_iconv_strpos ( unsigned int * pretval , <nl> ndl_buf_left -= GENERIC_SUPERSET_NBYTES ; <nl> if ( ndl_buf_left == 0 ) { <nl> * pretval = match_ofs ; <nl> + ndl_buf_p = ndl_buf ; <nl> + ndl_buf_left = ndl_buf_len ; <nl> + match_ofs = - 1 ; <nl> } <nl> } else { <nl> unsigned int i , j , lim ;
static php_stream * php_stream_url_wrap_rfc2397 ( php_stream_wrapper * wrapper , cha <nl> ts -> mode = mode && mode [ 0 ] == ' r ' ? TEMP_STREAM_READONLY : 0 ; <nl> ts -> meta = meta ; <nl> } <nl> + efree ( comma ); <nl>  <nl> return stream ; <nl> }
static int create_segments ( size_t requested_size , zend_shared_segment *** shared_ <nl> /* creating segment here */ <nl> * shared_segments_count = 1 ; <nl> * shared_segments_p = ( zend_shared_segment **) calloc ( 1 , sizeof ( zend_shared_segment )+ sizeof ( void *)); <nl> + if (!* shared_segments_p ) { <nl> + zend_win_error_message ( ACCEL_LOG_FATAL , " calloc () failed "); <nl> + * error_in = " calloc "; <nl> + return ALLOC_FAILURE ; <nl> + } <nl> shared_segment = ( zend_shared_segment *)(( char *)(* shared_segments_p ) + sizeof ( void *)); <nl> (* shared_segments_p )[ 0 ] = shared_segment ; <nl> 
PS_READ_FUNC ( files ) <nl> return FAILURE ; <nl>  <nl> data -> st_size = * vallen = sbuf . st_size ; <nl> + <nl> + if ( sbuf . st_size == 0 ) { <nl> + * val = STR_EMPTY_ALLOC (); <nl> + return SUCCESS ; <nl> + } <nl> + <nl> * val = emalloc ( sbuf . st_size ); <nl>  <nl> # if defined ( HAVE_PREAD )
ZEND_METHOD ( reflection_class , isSubclassOf ) <nl> case IS_UNICODE : <nl> if ( zend_u_lookup_class ( Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name ), Z_UNILEN_P ( class_name ), & pce TSRMLS_CC ) == FAILURE ) { <nl> zend_throw_exception_ex ( reflection_exception_ptr , 0 TSRMLS_CC , <nl> - " Interface % R does not exist ", Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name )); <nl> + " Class % R does not exist ", Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name )); <nl> return ; <nl> } <nl> class_ce = * pce ;
PHP_FUNCTION ( simplexml_import_dom ) <nl>  <nl> if ( object -> node && object -> node -> node ) { <nl> nodep = object -> node -> node ; <nl> + if ( nodep -> doc == NULL ) { <nl> + php_error ( E_WARNING , " Imported Node must have associated Document "); <nl> + RETURN_NULL (); <nl> + } <nl> if ( nodep -> type == XML_DOCUMENT_NODE || nodep -> type == XML_HTML_DOCUMENT_NODE ) { <nl> nodep = xmlDocGetRootElement (( xmlDocPtr ) nodep ); <nl> }
PHPAPI int _php_stream_copy_to_stream_ex ( php_stream * src , php_stream * dest , size <nl>  <nl> * len = didwrite ; <nl>  <nl> - /* read bytes match written */ <nl> - if ( mapped == didwrite ) { <nl> + /* we ' ve got at least 1 byte to read <nl> + * less than 1 is an error <nl> + * AND read bytes match written */ <nl> + if ( mapped > 0 && mapped == didwrite ) { <nl> return SUCCESS ; <nl> } <nl> return FAILURE ;
PHP_METHOD ( Phar , copy ) <nl> } <nl> } <nl>  <nl> - if ( phar_path_check (& newfile , & newfile_len , & pcr_error ) > pcr_is_ok ) { <nl> + if ( phar_path_check (& newfile , ( int *) & newfile_len , & pcr_error ) > pcr_is_ok ) { <nl> zend_throw_exception_ex ( spl_ce_UnexpectedValueException , 0 TSRMLS_CC , <nl> " file \"% s \" contains invalid characters % s , cannot be copied from \"% s \" in phar % s ", newfile , pcr_error , oldfile , phar_obj -> archive -> fname ); <nl> RETURN_FALSE ;
static void _free_mysql_result ( zend_rsrc_list_entry * rsrc TSRMLS_DC ) <nl> MYSQL_RES * mysql_result = ( MYSQL_RES *) rsrc -> ptr ; <nl>  <nl> mysql_free_result ( mysql_result ); <nl> + MySG ( result_allocated )--; <nl> } <nl> /* }}} */ <nl> 
static void _php_mb_regex_ereg_replace_exec ( INTERNAL_FUNCTION_PARAMETERS , OnigOp <nl> ! ZVAL_IS_UNDEF (& retval )) { <nl> convert_to_string_ex (& retval ); <nl> smart_str_appendl (& out_buf , Z_STRVAL ( retval ), Z_STRLEN ( retval )); <nl> - eval_buf . s -> len = 0 ; <nl> + if ( eval_buf . s ) { <nl> + eval_buf . s -> len = 0 ; <nl> + } <nl> zval_ptr_dtor (& retval ); <nl> } else { <nl> efree ( description );
int call_user_function_ex ( HashTable * function_table , zval ** object_pp , zval * fun <nl>  <nl> zend_ptr_stack_n_push (& EG ( argument_stack ), 2 , ( void *) ( long ) param_count , NULL ); <nl>  <nl> + EG ( function_state_ptr ) = & function_state ; <nl> + <nl> if ( function_state . function -> type == ZEND_USER_FUNCTION ) { <nl> calling_symbol_table = EG ( active_symbol_table ); <nl> if ( symbol_table ) {
PHP_FUNCTION ( oci_password_change ) <nl> WRONG_PARAM_COUNT ; <nl> } <nl>  <nl> + convert_to_string_ex ( user_param ); <nl> + convert_to_string_ex ( pass_old_param ); <nl> + convert_to_string_ex ( pass_new_param ); <nl> + <nl> user = Z_STRVAL_PP ( user_param ); <nl> pass_old = Z_STRVAL_PP ( pass_old_param ); <nl> pass_new = Z_STRVAL_PP ( pass_new_param );
int zend_register_functions ( zend_class_entry * scope , zend_function_entry * functi <nl> char * lowercase_name ; <nl> int fname_len ; <nl>  <nl> + memset ( internal_function , 0 , sizeof ( zend_function )); <nl> if ( type == MODULE_PERSISTENT ) { <nl> error_type = E_CORE_WARNING ; <nl> } else {
static sdlParamPtr get_param ( sdlFunctionPtr function , char * param_name , int inde <nl> } else { <nl> ht = function -> responseParameters ; <nl> } <nl> + <nl> + if ( ht == NULL ) { <nl> + return NULL ; <nl> + } <nl>  <nl> if ( param_name != NULL ) { <nl> if ( zend_hash_find ( ht , param_name , strlen ( param_name ), ( void **)& tmp ) != FAILURE ) {
static void ps_files_open ( ps_files * data , const char * key ) <nl> data -> basedir = NULL ; <nl> data -> basedir_len = 0 ; <nl> } <nl> + efree ( data ); <nl> php_error_docref ( NULL , E_WARNING , " The session id is too long or contains illegal characters , valid characters are a - z , A - Z , 0 - 9 and '-,'"); <nl> return ; <nl> }
ZEND_API int zend_restore_ini_entry ( char * name , uint name_length , int stage ) /* <nl> } <nl>  <nl> if ( EG ( modified_ini_directives )) { <nl> - zend_restore_ini_entry_cb ( ini_entry , stage TSRMLS_CC ); <nl> - zend_hash_del ( EG ( modified_ini_directives ), name , name_length ); <nl> + if ( zend_restore_ini_entry_cb ( ini_entry , stage TSRMLS_CC ) == 0 ) { <nl> + zend_hash_del ( EG ( modified_ini_directives ), name , name_length ); <nl> + } else { <nl> + return FAILURE ; <nl> + } <nl> } <nl>  <nl> return SUCCESS ;
int phar_get_entry_data ( phar_entry_data ** ret , char * fname , int fname_len , char <nl> if ( entry -> link ) { <nl> phar_entry_info * link = phar_get_link_source ( entry TSRMLS_CC ); <nl> if (! link ) { <nl> + efree (* ret ); <nl> return FAILURE ; <nl> } <nl> (* ret )-> zero = phar_get_fp_offset ( link TSRMLS_CC );
typedef int32_t zend_off_t ; <nl> # define ZEND_STRTOUL ( s0 , s1 , base ) strtoull (( s0 ), ( s1 ), ( base )) <nl> # define ZEND_STRTOL_PTR strtoll <nl> # define ZEND_STRTOUL_PTR strtoull <nl> -# define ZEND_ABS llabs <nl> +# define ZEND_ABS imaxabs <nl> # endif <nl> # else <nl> # define ZEND_STRTOL ( s0 , s1 , base ) strtol (( s0 ), ( s1 ), ( base ))
apprentice_map ( struct magic_set * ms , const char * fn ) <nl> if ( dbname == NULL ) <nl> goto error ; <nl>  <nl> - stream = php_stream_open_wrapper (( char *) fn , " rb ", REPORT_ERRORS , NULL ); <nl> + stream = php_stream_open_wrapper (( char *) fn , " rb ", REPORT_ERRORS , NULL ); <nl>  <nl> if (! stream ) { <nl> goto error ;
PHP_FUNCTION ( fd_set ) <nl> FD_SET ( fd , & readfd ); <nl> if ( fd > max_fd ) max_fd = fd ; <nl> } <nl> + efree ( args ); <nl> } <nl> RETURN_LONG ( 1 ); <nl> }
PHPAPI size_t php_strip_tags ( char * rbuf , int len , int * stateptr , char * allow , in <nl>  <nl> while ( i < len ) { <nl> switch ( c ) { <nl> + case '\ 0 ': <nl> + break ; <nl> case '<': <nl> if ( isspace (*( p + 1 ))) { <nl> goto reg_char ;
static int php_cli_server_poller_iter_on_active ( php_cli_server_poller * poller , v <nl> SOCKET fd ; <nl> int events ; <nl> } entries [ FD_SETSIZE * 2 ]; <nl> - php_socket_t fd = 0 ; <nl> size_t i ; <nl> struct socket_entry * n = entries , * m ; <nl> 
SAPI_API SAPI_POST_HANDLER_FUNC ( rfc1867_post_handler ) <nl> int llen = 0 ; <nl> int upload_cnt = INI_INT (" max_file_uploads "); <nl>  <nl> - if ( SG ( request_info ). content_length > SG ( post_max_size )) { <nl> + if ( SG ( post_max_size ) > 0 && SG ( request_info ). content_length > SG ( post_max_size )) { <nl> sapi_module . sapi_error ( E_WARNING , " POST Content - Length of % ld bytes exceeds the limit of % ld bytes ", SG ( request_info ). content_length , SG ( post_max_size )); <nl> return ; <nl> }
PHPAPI extern const char php_sig_gif [ 3 ]; <nl> PHPAPI extern const char php_sig_jpg [ 3 ]; <nl> PHPAPI extern const char php_sig_png [ 3 ]; <nl> - PHPAPI const char php_sig_gd2 [ 3 ] = {' g ', ' d ', ' 2 '}; <nl> + PHPAPI extern const char php_sig_gd2 [ 3 ] = {' g ', ' d ', ' 2 '}; <nl>  <nl> extern zend_module_entry gd_module_entry ; <nl> # define phpext_gd_ptr & gd_module_entry
void php_mysqli_fetch_into_hash ( INTERNAL_FUNCTION_PARAMETERS , int override_flags <nl> { <nl> MYSQL_RES * result ; <nl> zval * mysql_result ; <nl> - int fetchtype ; <nl> + long fetchtype ; <nl> unsigned int i ; <nl> MYSQL_FIELD * fields ; <nl> MYSQL_ROW row ;
SAPI_API size_t sapi_apply_default_charset ( char ** mimetype , size_t len TSRMLS_DC <nl> newtype = emalloc ( newlen + 1 ); <nl> PHP_STRLCPY ( newtype , * mimetype , newlen + 1 , len ); <nl> strlcat ( newtype , "; charset =", newlen + 1 ); <nl> + strlcat ( newtype , charset , newlen + 1 ); <nl> efree (* mimetype ); <nl> * mimetype = newtype ; <nl> return newlen ;
static inline zval * zend_assign_to_variable ( zval * variable_ptr , zval * value TSRM <nl> value = Z_REFVAL_P ( value ); <nl> } <nl> if ( Z_REFCOUNTED_P ( value )) { <nl> + if ( UNEXPECTED ( variable_ptr == value )) { <nl> + return variable_ptr ; <nl> + } <nl> Z_ADDREF_P ( value ); <nl> } <nl> }
static PHP_INI_MH ( OnTypeLibFileUpdate ) <nl> char * strtok_buf = NULL ; <nl> int cached ; <nl>  <nl> - if (! new_value || ( typelib_file = VCWD_FOPEN ( new_value , " r "))== NULL ) { <nl> + if (! new_value || ! new_value [ 0 ] || ( typelib_file = VCWD_FOPEN ( new_value , " r "))== NULL ) { <nl> return FAILURE ; <nl> } <nl> 
SPL_METHOD ( SplFileInfo , getExtension ) <nl>  <nl> p = zend_memrchr ( ret -> val , '.', ret -> len ); <nl> if ( p ) { <nl> - idx = p - fname ; <nl> + idx = p - ret -> val ; <nl> RETVAL_STRINGL ( ret -> val + idx + 1 , ret -> len - idx - 1 ); <nl> STR_RELEASE ( ret ); <nl> return ;
php_sprintf_appenddouble ( char ** buffer , int * pos , <nl> if (( adjust & ADJ_PRECISION ) == 0 ) { <nl> precision = FLOAT_PRECISION ; <nl> } else if ( precision > MAX_FLOAT_PRECISION ) { <nl> + php_error_docref ( NULL TSRMLS_CC , E_NOTICE , " Requested precision of % d digits was truncated to PHP maximum of % d digits ", precision , MAX_FLOAT_PRECISION ); <nl> precision = MAX_FLOAT_PRECISION ; <nl> } <nl> 
static int php_zip_extract_file ( struct zip * za , char * dest , char * file , int fil <nl> * safemode status as its parent folder ? <nl> */ <nl> if ( OPENBASEDIR_CHECKPATH ( fullpath )) { <nl> + efree ( fullpath ); <nl> efree ( file_dirname_fullpath ); <nl> efree ( file_basename ); <nl> return 0 ;
static inline int _php_stream_path_param_encode ( zval ** ppzval , char ** ppath , int <nl> if ( FAILURE == php_stream_path_encode ( NULL , & path , & path_len , Z_USTRVAL_PP ( ppzval ), Z_USTRLEN_PP ( ppzval ), options , context )) { <nl> return FAILURE ; <nl> } <nl> + Z_ADDREF_PP ( ppzval ); /* the conversion removes a refcount */ <nl> MAKE_STD_ZVAL ( zpath ); <nl> ZVAL_STRINGL ( zpath , path , path_len , 0 ); <nl> Z_UNSET_ISREF_P ( zpath );
PHPAPI void php_pcre_match_impl ( pcre_cache_entry * pce , char * subject , int subjec <nl> if ( pcre_get_substring_list ( subject , offsets , count , & stringlist ) < 0 ) { <nl> efree ( subpat_names ); <nl> efree ( offsets ); <nl> + if ( match_sets ) efree ( match_sets ); <nl> php_error_docref ( NULL TSRMLS_CC , E_WARNING , " Get subpatterns list failed "); <nl> RETURN_FALSE ; <nl> }
static HashTable * zend_closure_get_debug_info ( zval * object , int * is_temp TSRMLS_ <nl> } <nl> info_len = zend_spprintf (& info , 0 , "% s ", <nl> i >= required ? "< optional >" : "< required >"); <nl> - add_assoc_stringl_ex (& val , name , name_len , info , info_len , 0 ); <nl> +//??? TODO : avoid reallocation <nl> + add_assoc_stringl_ex (& val , name , name_len , info , info_len , 1 ); <nl> + efree ( info ); <nl> efree ( name ); <nl> arg_info ++; <nl> }
struct _php_stream { <nl> char * orig_path ; <nl>  <nl> zend_resource * ctx ; <nl> - int flags ; /* PHP_STREAM_FLAG_XXX */ <nl> + uint32_t flags ; /* PHP_STREAM_FLAG_XXX */ <nl>  <nl> int eof ; <nl> 
PHPAPI void php_pcre_match_impl ( pcre_cache_entry * pce , char * subject , int subjec <nl> if ( pcre_get_substring_list ( subject , offsets , count , & stringlist ) < 0 ) { <nl> efree ( subpat_names ); <nl> efree ( offsets ); <nl> + if ( match_sets ) efree ( match_sets ); <nl> php_error_docref ( NULL TSRMLS_CC , E_WARNING , " Get subpatterns list failed "); <nl> RETURN_FALSE ; <nl> }
# define _FTP_H <nl>  <nl> # include < stdio . h > <nl> -# if HAVE_UINSTD_H <nl> +# if ! PHP_WIN32 <nl> # include < netinet / in . h > <nl> # endif <nl> 
PHP_FUNCTION ( file_put_contents ) <nl> RETURN_FALSE ; <nl> } <nl> switch ( Z_TYPE_P ( data )) { <nl> + case IS_RESOURCE : <nl> + { <nl> + php_stream * srcstream ; <nl> + php_stream_from_zval ( srcstream , & data ); <nl> + <nl> + numbytes = php_stream_copy_to_stream ( srcstream , stream , PHP_STREAM_COPY_ALL ); <nl> + <nl> + break ; <nl> + } <nl> case IS_NULL : <nl> case IS_LONG : <nl> case IS_DOUBLE :
export_desktop_file ( const char * app , <nl> g_string_append_printf ( new_exec , " ", arg ); <nl> else if ( strcasecmp ( arg , "% u ") == 0 ) <nl> g_string_append_printf ( new_exec , " ", arg ); <nl> - else if ( strcmp ( arg , " u ") == 0 ) <nl> + else if ( g_str_has_prefix ( arg , "@@")) <nl> g_print ( _ (" Skipping invalid Exec argument % s \ n "), arg ); <nl> else <nl> g_string_append_printf ( new_exec , " % s ", arg );
flatpak_installation_drop_caches ( FlatpakInstallation * self , <nl> { <nl> priv -> dir_unlocked = clone ; <nl> g_object_unref ( old ); <nl> + res = TRUE ; <nl> } <nl>  <nl> G_UNLOCK ( dir );
flatpak_run_add_environment_args ( GPtrArray * argv_array , <nl> "/ dev / dri ", <nl> /* mali */ <nl> "/ dev / mali ", <nl> + "/ dev / mali0 ", <nl> "/ dev / umplock ", <nl> /* nvidia */ <nl> "/ dev / nvidiactl ",
main ( int argc , <nl> die_with_error (" Failed to make / slave "); <nl>  <nl> /* Create a tmpfs which we will use as / in the namespace */ <nl> - if ( mount ("", newroot , " tmpfs ", MS_NODEV | MS_NOEXEC | MS_NOSUID , NULL ) != 0 ) <nl> + if ( mount ("", newroot , " tmpfs ", MS_NODEV | MS_NOSUID , NULL ) != 0 ) <nl> die_with_error (" Failed to mount tmpfs "); <nl>  <nl> old_cwd = get_current_dir_name ();
flatpak_builtin_ls_remote ( int argc , char ** argv , GCancellable * cancellable , GEr <nl> if ( deploy_data == NULL ) <nl> continue ; <nl>  <nl> + if ( g_strcmp0 ( flatpak_deploy_data_get_origin ( deploy_data ), remote ) != 0 ) <nl> + continue ; <nl> + <nl> if ( g_strcmp0 ( flatpak_deploy_data_get_commit ( deploy_data ), checksum ) == 0 ) <nl> continue ; <nl> }
setup_seccomp ( FlatpakBwrap * bwrap , <nl> { SCMP_SYS ( umount ), EPERM }, <nl> { SCMP_SYS ( umount2 ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> + { SCMP_SYS ( chroot ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ ) <nl> /* Architectures with CONFIG_CLONE_BACKWARDS2 : the child stack <nl> * and flags arguments are reversed so the flags come second */
flatpak_number_prompt ( int min , int max , const char * prompt , ...) <nl>  <nl> va_start ( var_args , prompt ); <nl> s = g_strdup_vprintf ( prompt , var_args ); <nl> + va_end ( var_args ); <nl>  <nl> while ( TRUE ) <nl> {
setup_seccomp ( FlatpakBwrap * bwrap , <nl> { SCMP_SYS ( unshare ), EPERM }, <nl> { SCMP_SYS ( setns ), EPERM }, <nl> { SCMP_SYS ( mount ), EPERM }, <nl> + { SCMP_SYS ( umount ), EPERM }, <nl> + { SCMP_SYS ( umount2 ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ ) <nl> /* Architectures with CONFIG_CLONE_BACKWARDS2 : the child stack
add_related ( GHashTable * all_refs , <nl> g_hash_table_insert ( all_refs , g_steal_pointer (& ext_collection_ref ), c_s ); <nl> } <nl>  <nl> + g_list_free_full ( extensions , ( GDestroyNotify ) flatpak_extension_free ); <nl> + <nl> return TRUE ; <nl> } <nl> 
xdp_fuse_init ( GError ** error ) <nl>  <nl> path = xdp_fuse_get_mountpoint (); <nl> if (( stat ( path , & st ) == - 1 && errno == ENOTCONN ) || <nl> - (( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN || <nl> + ((( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN ) || <nl> ( statfs_res == 0 && stfs . f_type == 0x65735546 /* fuse */))) <nl> { <nl> int count ;
static void f_synIDattr ( typval_T * argvars , typval_T * rettv ) <nl> modec = TOLOWER_ASC ( mode [ 0 ]); <nl> if ( modec != ' c ' && modec != ' g ') <nl> modec = 0 ; /* replace invalid with current */ <nl> + } else if ( ui_rgb_attached ()) { <nl> + modec = ' g '; <nl> } else { <nl> modec = ' c '; <nl> }
static void qf_free_stack ( win_T * wp , qf_info_T * qi ) <nl> // If the location list window is open , then create a new empty location <nl> // list <nl> qf_info_T * new_ll = ll_new_list (); <nl> + <nl> + // first free the list reference in the location list window <nl> + ll_free_all (& orig_wp -> w_llist_ref ); <nl> + <nl> orig_wp -> w_llist_ref = new_ll ; <nl> if ( llwin != NULL ) { <nl> llwin -> w_llist = new_ll ;
 <nl> # include " nvim / api / private / defs . h " <nl> # include " nvim / func_attr . h " <nl> +# include " nvim / eval / typval . h " <nl> +# include " nvim / ex_cmds_defs . h " <nl>  <nl> // Generated by msgpack - gen . lua <nl> void nlua_add_api_functions ( lua_State * lstate ) REAL_FATTR_NONNULL_ALL ;
char ** crypto_cert_subject_alt_name ( X509 * xcert , int * count , int ** lengths ) <nl> * lengths = NULL ; <nl> return NULL ; <nl> } <nl> + GENERAL_NAMES_free ( subject_alt_names ); <nl>  <nl> return strings ; <nl> }
BOOL CloseHandle ( HANDLE hObject ) <nl> if ( pipe -> serverfd != - 1 ) <nl> close ( pipe -> serverfd ); <nl>  <nl> - free ( Object ); <nl> + free ( pipe -> lpFileName ); <nl> + free ( pipe -> lpFilePath ); <nl> + free ( pipe -> name ); <nl> + free ( pipe ); <nl>  <nl> return TRUE ; <nl> }
char * crypto_print_name ( X509_NAME * name ) <nl> if ( X509_NAME_print_ex ( outBIO , name , 0 , XN_FLAG_ONELINE ) > 0 ) <nl> { <nl> unsigned long size = BIO_number_written ( outBIO ); <nl> - buffer = xzalloc ( size ); <nl> - memset ( buffer , 0 , size ); <nl> + buffer = xzalloc ( size + 1 ); <nl> + memset ( buffer , 0 , size + 1 ); <nl> BIO_read ( outBIO , buffer , size ); <nl> } <nl> 
void update_free ( rdpUpdate * update ) <nl>  <nl> xfree ( update -> bitmap_update . rectangles ); <nl> xfree ( update -> pointer ); <nl> + xfree ( update -> primary -> polyline . points ); <nl> + xfree ( update -> primary -> polygon_sc . points ); <nl> xfree ( update -> primary ); <nl> xfree ( update -> secondary ); <nl> xfree ( update -> altsec );
int send_arp ( u_char type , struct ip_addr * sip , u_int8 * smac , struct ip_addr * tip <nl>  <nl> SEND_LOCK ; <nl>  <nl> + // FIXME <nl> + // why without clearing again the packet I get issue # 245 ? <nl> + libnet_clear_packet ( GBL_IFACE -> lnet ); <nl> + <nl> /* ARP uses 00 : 00 : 00 : 00 : 00 : 00 broadcast */ <nl> if ( type == ARPOP_REQUEST && tmac == MEDIA_BROADCAST ) <nl> tmac = ARP_BROADCAST ;
IODeviceSocket ::~ IODeviceSocket () <nl> proc -> kill (); <nl> } <nl>  <nl> - delete d ; <nl> + d -> deleteLater (); <nl> } <nl>  <nl> bool IODeviceSocket :: canReadLine ()
List :: List ( const Kind _kind , const QByteArray & line , int & start ): <nl>  <nl> ++ start ; <nl>  <nl> - if ( start >= line . size ()) <nl> + if ( start >= line . size () - 2 ) <nl> throw NoData ( line , start ); // no mailbox <nl>  <nl> mailbox = LowLevelParser :: getMailbox ( line , start );
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl>  <nl> ll = l + strspn ( l , WHITESPACE ); <nl>  <nl> - if ( HAS_FEATURE_MEMORY_SANITIZER && startswith ( ll , " ListenNetlink =")) { <nl> + if ( HAS_FEATURE_MEMORY_SANITIZER && startswith ( ll , " ListenNetlink ")) { <nl> /* ListenNetlink causes a false positive in msan , <nl> * let ' s skip this for now . */ <nl> log_notice (" Skipping test because ListenNetlink = is present ");
end : <nl>  <nl> /* Removing empty dropin dirs */ <nl> if (! arg_full ) { <nl> - _cleanup_free_ char * dir = dirname_malloc (* original ); <nl> + _cleanup_free_ char * dir ; <nl> + <nl> + dir = dirname_malloc (* original ); <nl> + if (! dir ) <nl> + return log_oom (); <nl> + <nl> /* no need to check if the dir is empty , rmdir <nl> * does nothing if it is not the case . <nl> */
int main ( int argc , char * argv []) { <nl> log_error_errno ( r , " Failed to iterate through journal : % m "); <nl> goto finish ; <nl> } <nl> + if ( r == 0 ) { <nl> + printf ("-- No entries --\ n "); <nl> + goto finish ; <nl> + } <nl>  <nl> if (! arg_follow ) <nl> pager_open_if_enabled ();
int main ( int argc , char * argv []) { <nl> const void * data ; <nl> size_t size ; <nl>  <nl> + r = sd_journal_set_data_threshold ( j , 0 ); <nl> + if ( r < 0 ) { <nl> + log_error (" Failed to unset data size threshold "); <nl> + return EXIT_FAILURE ; <nl> + } <nl> + <nl> r = sd_journal_query_unique ( j , arg_field ); <nl> if ( r < 0 ) { <nl> log_error (" Failed to query unique data objects : % s ", strerror (- r ));
static void item_free ( Item * i ) { <nl> free ( i -> uid_path ); <nl> free ( i -> gid_path ); <nl> free ( i -> description ); <nl> + free ( i -> home ); <nl> free ( i ); <nl> } <nl> 
static int check_unit ( DBusConnection * bus , char ** args , unsigned n ) { <nl> if (! arg_quiet ) <nl> puts ( state ); <nl>  <nl> - if ( streq ( state , " active ") || startswith ( state , " active -")) <nl> + if ( streq ( state , " active ") || startswith ( state , " reloading ")) <nl> r = 0 ; <nl>  <nl> dbus_message_unref ( m );
static int get_key ( struct udev * udev , char ** line , char ** key , enum operation_ty <nl> char * temp ; <nl>  <nl> linepos = * line ; <nl> - if ( linepos == NULL && linepos [ 0 ] == '\ 0 ') <nl> + if ( linepos == NULL || linepos [ 0 ] == '\ 0 ') <nl> return - 1 ; <nl>  <nl> /* skip whitespace */
static int bus_message_setup_kmsg ( sd_bus_message * m ) { <nl> if (! m -> kdbus ) <nl> return - ENOMEM ; <nl>  <nl> + memset ( m -> kdbus , 0 , sz ); <nl> + <nl> m -> kdbus -> flags = <nl> (( m -> header -> flags & SD_BUS_MESSAGE_NO_REPLY_EXPECTED ) ? 0 : KDBUS_MSG_FLAGS_EXPECT_REPLY ) | <nl> (( m -> header -> flags & SD_BUS_MESSAGE_NO_AUTO_START ) ? KDBUS_MSG_FLAGS_NO_AUTO_START : 0 );
int main ( int argc , char * argv [], char * envp []) <nl> dbg (" error fcntl on write pipe : % s ", strerror ( errno )); <nl>  <nl> /* set signal handlers */ <nl> + memset (& act , 0x00 , sizeof ( struct sigaction )); <nl> act . sa_handler = ( void (*) ( int )) sig_handler ; <nl> sigemptyset (& act . sa_mask ); <nl> act . sa_flags = SA_RESTART ;
static int create_item ( Item * i ) { <nl>  <nl> case CREATE_FILE : <nl> case TRUNCATE_FILE : <nl> + r = write_one_file ( i , i -> path ); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + break ; <nl> case WRITE_FILE : <nl> r = glob_item ( i , write_one_file ); <nl> if ( r < 0 )
struct DnsTransaction { <nl>  <nl> uint16_t id ; <nl>  <nl> - bool initial_jitter_scheduled ; <nl> - bool initial_jitter_elapsed ; <nl> + bool initial_jitter_scheduled : 1 ; <nl> + bool initial_jitter_elapsed : 1 ; <nl>  <nl> DnsPacket * sent , * received ; <nl> 
finish : <nl>  <nl> if ( n_arguments > 3 ) { <nl> arguments [ n_arguments ] = NULL ; <nl> + strv_uniq ( arguments ); <nl> execv ("/ sbin / modprobe ", arguments ); <nl>  <nl> log_error (" Failed to execute / sbin / modprobe : % m ");
static void output_units_list ( const struct unit_info * unit_infos , unsigned c ) { <nl>  <nl> n_shown ++; <nl>  <nl> - if ( streq ( u -> load_state , " error ")) { <nl> + if ( streq ( u -> load_state , " error ") || <nl> + streq ( u -> load_state , " not - found ")) { <nl> on_loaded = on = ansi_highlight_red ( true ); <nl> off_loaded = off = ansi_highlight_red ( false ); <nl> } else
static int show_one ( <nl> */ <nl> if ( info . pid_file && access ( info . pid_file , F_OK ) == 0 ) <nl> r = 1 ; <nl> + else if ( streq_ptr ( info . load_state , " not - found ") && streq_ptr ( info . active_state , " inactive ")) <nl> + r = 4 ; <nl> else <nl> r = 3 ; <nl> }
static int method_set_vc_keyboard ( sd_bus_message * m , void * userdata , sd_bus_erro <nl> } <nl>  <nl> # ifdef HAVE_XKBCOMMON <nl> + _printf_ ( 3 , 0 ) <nl> static void log_xkb ( struct xkb_context * ctx , enum xkb_log_level lvl , const char * format , va_list args ) { <nl> const char * fmt ; <nl> 
static int systemctl_parse_argv ( int argc , char * argv []) { <nl> size_t size ; <nl>  <nl> FOREACH_WORD_SEPARATOR ( word , size , optarg , ",", state ) { <nl> - char * s ; <nl> + _cleanup_free_ char * s = NULL ; <nl>  <nl> s = strndup ( word , size ); <nl> if (! s )
static int unit_find_paths ( <nl> _cleanup_free_ char * template = NULL ; <nl>  <nl> r = unit_name_template ( unit_name , & template ); <nl> - if ( r != - EINVAL ) <nl> + if ( r < 0 && r != - EINVAL ) <nl> return log_error_errno ( r , " Failed to determine template name : % m "); <nl> if ( r >= 0 ) { <nl> r = unit_file_find_path ( lp , template , & path );
static void font_copy_to_all_vcs ( int fd ) { <nl> return ; <nl> } <nl>  <nl> - for ( i = 1 ; i <= 15 ; i ++) { <nl> + for ( i = 1 ; i <= 63 ; i ++) { <nl> char vcname [ strlen ("/ dev / vcs ") + DECIMAL_STR_MAX ( int )]; <nl> _cleanup_close_ int vcfd = - 1 ; <nl> struct console_font_op cfo = {};
int main ( int argc , char ** argv ) { <nl> log_parse_environment (); <nl>  <nl> r = parse_config (); <nl> - if ( r <= 0 ) <nl> + if ( r < 0 ) <nl> goto finish ; <nl>  <nl> r = parse_argv ( argc , argv );
static int event_make_signal_data ( <nl> d -> priority = priority ; <nl>  <nl> r = hashmap_put ( e -> signal_data , & d -> priority , d ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( d ); <nl> return r ; <nl> + } <nl>  <nl> added = true ; <nl> }
_public_ int sd_bus_path_decode_many ( const char * path , const char * path_template <nl> } <nl> va_end ( list ); <nl>  <nl> - free ( labels ); <nl> - labels = NULL ; <nl> + labels = mfree ( labels ); <nl> return 1 ; <nl> } <nl> 
static void output_unit_file_list ( const UnitFileList * units , unsigned c ) { <nl> } else <nl> id_cols = max_id_len ; <nl>  <nl> - if (! arg_no_legend ) <nl> + if (! arg_no_legend && c > 0 ) <nl> printf ("%-* s %-* s \ n ", <nl> id_cols , " UNIT FILE ", <nl> state_cols , " STATE ");
This file is part of systemd . <nl>  <nl> Copyright 2010 Kay Sievers <nl> + Copyright 2016 Michal Soltys < soltys @ ziu . info > <nl>  <nl> systemd is free software ; you can redistribute it and / or modify it <nl> under the terms of the GNU Lesser General Public License as published by
static int list_machines ( int argc , char * argv [], void * userdata ) { <nl> while (( r = sd_bus_message_read ( reply , "( ssso )", & name , & class , & service , & object )) > 0 ) { <nl> size_t l ; <nl>  <nl> + if ( name [ 0 ] == '.' && ! arg_all ) <nl> + continue ; <nl> + <nl> if (! GREEDY_REALLOC ( machines , n_allocated , n_machines + 1 )) <nl> return log_oom (); <nl> 
static void print_status_info ( <nl>  <nl> printf (" CGroup : % s \ n ", i -> control_group ); <nl>  <nl> - if ( arg_transport == BUS_TRANSPORT_LOCAL ) { <nl> + if ( arg_transport == BUS_TRANSPORT_LOCAL || arg_transport == BUS_TRANSPORT_CONTAINER ) { <nl> unsigned k = 0 ; <nl> pid_t extra [ 2 ]; <nl> char prefix [] = " ";
static int manager_sigusr2 ( sd_event_source * s , const struct signalfd_siginfo * si <nl> assert ( m ); <nl>  <nl> manager_flush_caches ( m ); <nl> - log_info (" Flushed all caches ."); <nl>  <nl> return 0 ; <nl> } <nl> void manager_flush_caches ( Manager * m ) { <nl>  <nl> LIST_FOREACH ( scopes , scope , m -> dns_scopes ) <nl> dns_cache_flush (& scope -> cache ); <nl> + <nl> + log_info (" Flushed all caches ."); <nl> }
int dns_zone_put ( DnsZone * z , DnsScope * s , DnsResourceRecord * rr , bool probe ) { <nl> if ( established ) <nl> i -> state = DNS_ZONE_ITEM_ESTABLISHED ; <nl> else { <nl> + i -> state = DNS_ZONE_ITEM_PROBING ; <nl> + <nl> r = dns_zone_item_probe_start ( i ); <nl> if ( r < 0 ) { <nl> dns_zone_item_remove_and_free ( z , i ); <nl> i = NULL ; <nl> return r ; <nl> } <nl> - <nl> - i -> state = DNS_ZONE_ITEM_PROBING ; <nl> } <nl> } else <nl> i -> state = DNS_ZONE_ITEM_ESTABLISHED ;
bool ignore_file ( const char * filename ) { <nl> assert ( filename ); <nl>  <nl> if ( endswith ( filename , "~")) <nl> - return false ; <nl> + return true ; <nl>  <nl> return ignore_file_allow_backup ( filename ); <nl> }
struct udev * udev_new ( void ) <nl> } <nl>  <nl> if ( strcasecmp ( key , " udev_log ") == 0 ) { <nl> - udev -> log_priority = util_log_priority ( val ); <nl> + udev_set_log_priority ( udev , util_log_priority ( val )); <nl> continue ; <nl> } <nl> if ( strcasecmp ( key , " udev_root ") == 0 ) {
int main ( int argc , char * argv []) { <nl> finish : <nl> pager_close (); <nl>  <nl> + strv_free ( arg_file ); <nl> + <nl> return r < 0 ? EXIT_FAILURE : EXIT_SUCCESS ; <nl> }
bool socket_address_equal ( const SocketAddress * a , const SocketAddress * b ) { <nl> break ; <nl>  <nl> case AF_UNIX : <nl> + if ( a -> size <= offsetof ( struct sockaddr_un , sun_path ) || <nl> + b -> size <= offsetof ( struct sockaddr_un , sun_path )) <nl> + return false ; <nl> + <nl> if (( a -> sockaddr . un . sun_path [ 0 ] == 0 ) != ( b -> sockaddr . un . sun_path [ 0 ] == 0 )) <nl> return false ; <nl> 
static void bus_free ( sd_bus * b ) { <nl>  <nl> sd_bus_detach_event ( b ); <nl>  <nl> + if ( b -> default_bus_ptr ) <nl> + * b -> default_bus_ptr = NULL ; <nl> + <nl> bus_close_fds ( b ); <nl>  <nl> if ( b -> kdbus_buffer )
static DBusHandlerResult locale_message_handler ( <nl> " Locale \ 0 "); <nl> if (! changed ) <nl> goto oom ; <nl> - } <nl> + } else <nl> + strv_free ( l ); <nl> + <nl> } else if ( dbus_message_is_method_call ( message , " org . freedesktop . locale1 ", " SetVConsoleKeyboard ")) { <nl>  <nl> const char * keymap , * keymap_toggle ;
int link_config_apply ( link_config_ctx * ctx , link_config * config , <nl> if ( ctx -> enable_name_policy && config -> name_policy ) { <nl> NamePolicy * policy ; <nl>  <nl> - for ( policy = config -> name_policy ; ! respect_predictable && ! new_name && <nl> - * policy != _NAMEPOLICY_INVALID ; policy ++) { <nl> + for ( policy = config -> name_policy ; <nl> + ! new_name && * policy != _NAMEPOLICY_INVALID ; policy ++) { <nl> switch (* policy ) { <nl> case NAMEPOLICY_KERNEL : <nl> respect_predictable = true ;
static int cache_space_refresh ( Server * s , JournalStorage * storage ) { <nl>  <nl> ts = now ( CLOCK_MONOTONIC ); <nl>  <nl> - if ( space -> timestamp + RECHECK_SPACE_USEC > ts ) <nl> + if ( space -> timestamp != 0 && space -> timestamp + RECHECK_SPACE_USEC > ts ) <nl> return 0 ; <nl>  <nl> r = determine_path_usage ( s , storage -> path , & vfs_used , & vfs_avail );
int base_filesystem_create ( const char * root ) { <nl> const char * target = NULL ; <nl> const char * s ; <nl>  <nl> + if ( faccessat ( fd , table [ i ]. dir , F_OK , AT_SYMLINK_NOFOLLOW ) >= 0 ) <nl> + continue ; <nl> + <nl> /* check if one of the targets exists */ <nl> NULSTR_FOREACH ( s , table [ i ]. target ) { <nl> if ( faccessat ( fd , s , F_OK , AT_SYMLINK_NOFOLLOW ) < 0 )
int message_append_basic ( sd_bus_message * m , char type , const void * p , const void <nl> void * a ; <nl> char * e = NULL ; <nl> int fd = - 1 ; <nl> - uint32_t fdi ; <nl> + uint32_t fdi = 0 ; <nl> int r ; <nl>  <nl> if (! m )
int sd_rtnl_message_append_ether_addr ( sd_rtnl_message * m , unsigned short type , c <nl> return - ENOTSUP ; <nl> } <nl>  <nl> - r = add_rtattr ( m , type , data , sizeof ( data )); <nl> + r = add_rtattr ( m , type , data , ETH_ALEN ); <nl> if ( r < 0 ) <nl> return r ; <nl> 
subst : <nl> cpos ++; <nl> while ( isspace ( cpos [ 0 ])) <nl> cpos ++; <nl> + if ( cpos [ 0 ] == '\ 0 ') <nl> + break ; <nl> } <nl> if ( i > 0 ) { <nl> log_error (" requested part of result string not found ");
static int import_program_into_properties ( struct udev_device * dev , const char * p <nl> { <nl> struct udev * udev = udev_device_get_udev ( dev ); <nl> char ** envp ; <nl> - char result [ 2048 ]; <nl> + char result [ 4096 ]; <nl> size_t reslen ; <nl> char * line ; <nl> 
static int builtin_kmod ( struct udev_device * dev , int argc , char * argv [], bool te <nl> struct udev * udev = udev_device_get_udev ( dev ); <nl> int i ; <nl>  <nl> - if ( ctx ) <nl> + if (! ctx ) <nl> return 0 ; <nl>  <nl> if ( argc < 3 || strcmp ( argv [ 1 ], " load ")) {
void seat_evict_position ( Seat * s , Session * session ) { <nl> * position ( eg ., during gdm -> session transition ), so let ' s look <nl> * for it and set it on the free slot . */ <nl> LIST_FOREACH ( sessions_by_seat , iter , s -> sessions ) { <nl> - if ( iter -> position == pos ) { <nl> + if ( iter -> position == pos && session_get_state ( iter ) != SESSION_CLOSING ) { <nl> s -> positions [ pos ] = iter ; <nl> break ; <nl> }
int parse_timestamp ( const char * t , usec_t * usec ) { <nl>  <nl> x = time ( NULL ); <nl> assert_se ( localtime_r (& x , & tm )); <nl> + tm . tm_isdst = - 1 ; <nl>  <nl> if ( streq ( t , " now ")) <nl> goto finish ;
static int parse_password ( const char * filename , char ** wall ) { <nl> } <nl> } <nl>  <nl> + if ( pid > 0 && <nl> + kill ( pid , 0 ) < 0 && <nl> + errno == ESRCH ) { <nl> + r = 0 ; <nl> + goto finish ; <nl> + } <nl> + <nl> if ( arg_action == ACTION_LIST ) <nl> printf ("'% s ' ( PID % u )\ n ", message , pid ); <nl> else if ( arg_action == ACTION_WALL ) {
static int output_cat ( sd_journal * j , OutputMode mode , unsigned line , <nl>  <nl> r = sd_journal_get_data ( j , " MESSAGE ", & data , & l ); <nl> if ( r < 0 ) { <nl> + /* An entry without MESSAGE =? */ <nl> + if ( r == - ENOENT ) <nl> + return 0 ; <nl> + <nl> log_error (" Failed to get data : % s ", strerror (- r )); <nl> return r ; <nl> }
static int json_parse_tokens ( JsonVariant ** tokens , size_t ntokens , JsonVariant * <nl> size_t it = 0 ; <nl> int r ; <nl> JsonVariant * e ; <nl> - _cleanup_jsonunref_ JsonVariant * p ; <nl> + _cleanup_jsonunref_ JsonVariant * p = NULL ; <nl>  <nl> assert ( tokens ); <nl> assert ( ntokens );
static int detect_vm_xen ( void ) { <nl> r = read_one_line_file ("/ proc / xen / capabilities ", & domcap ); <nl> if ( r == - ENOENT ) <nl> return VIRTUALIZATION_NONE ; <nl> + if ( r < 0 ) <nl> + return r ; <nl>  <nl> i = domcap ; <nl> while (( cap = strsep (& i , ",")))
int sd_rtnl_call ( sd_rtnl * rtnl , <nl> r = rtnl_poll ( rtnl , true , left ); <nl> if ( r < 0 ) <nl> return r ; <nl> + else if ( r == 0 ) <nl> + return - ETIMEDOUT ; <nl>  <nl> r = dispatch_wqueue ( rtnl ); <nl> if ( r < 0 )
void seat_claim_position ( Seat * s , Session * session , unsigned int pos ) { <nl> seat_evict_position ( s , session ); <nl>  <nl> session -> position = pos ; <nl> - if ( pos > 0 && ! s -> positions [ pos ]) <nl> + if ( pos > 0 ) <nl> s -> positions [ pos ] = session ; <nl> } <nl> 
static int add_string ( struct udev_rules * rules , const char * str ) <nl> unsigned short node_off ; <nl> unsigned char key ; <nl> size_t len ; <nl> - int depth ; <nl> + unsigned int depth ; <nl> unsigned int off ; <nl>  <nl> len = strlen ( str );
int selinux_setup ( bool * loaded_policy ) { <nl> * loaded_policy = true ; <nl>  <nl> } else { <nl> + log_open (); <nl> + <nl> if ( enforce > 0 ) { <nl> - log_error (" Failed to load SELinux policy ."); <nl> + log_error (" Failed to load SELinux policy . Freezing ."); <nl> return - EIO ; <nl> } else <nl> - log_debug (" Unable to load SELinux policy ."); <nl> + log_debug (" Unable to load SELinux policy . Ignoring ."); <nl> } <nl> # endif <nl> 
static int parse_request ( uint8_t code , uint8_t len , const void * option , void * us <nl>  <nl> break ; <nl> case SD_DHCP_OPTION_MAXIMUM_MESSAGE_SIZE : <nl> - if ( len == 2 ) <nl> + <nl> + if ( len == 2 && unaligned_read_be16 ( option ) >= sizeof ( DHCPPacket )) <nl> req -> max_optlen = unaligned_read_be16 ( option ) - sizeof ( DHCPPacket ); <nl>  <nl> break ;
static int transaction_verify_order_one ( Transaction * tr , Job * j , Job * from , unsi <nl> " Found dependency on % s /% s ", <nl> k -> unit -> id , job_type_to_string ( k -> type )); <nl>  <nl> - if (! delete && <nl> + if (! delete && hashmap_get ( tr -> jobs , k -> unit ) && <nl> ! unit_matters_to_anchor ( k -> unit , k )) { <nl> /* Ok , we can drop this one , so let ' s <nl> * do so . */
Client_Key_Exchange :: Client_Key_Exchange ( Handshake_IO & io , <nl>  <nl> DL_Group group ( p , g ); <nl>  <nl> - if (! group . verify_group ( rng , true )) <nl> - throw Internal_Error (" DH group failed validation , possible attack "); <nl> + if (! group . verify_group ( rng , false )) <nl> + throw TLS_Exception ( Alert :: INSUFFICIENT_SECURITY , <nl> + " DH group validation failed "); <nl>  <nl> DH_PublicKey counterparty_key ( group , Y ); <nl> 
roken_detach_prep ( int argc , char ** argv , char * special_arg ) <nl> do { <nl> bytes = read ( pipefds [ 0 ], buf , sizeof ( buf )); <nl> } while ( bytes == - 1 && errno == EINTR ); <nl> + ( void ) close ( pipefds [ 0 ]); <nl> + pipefds [ 0 ] = - 1 ; <nl> if ( bytes == - 1 ) { <nl> /* <nl> * No need to wait for the process . We ' ve killed it . If it
create_and_write_cookie ( char * xauthfile , <nl> struct in_addr loopback ; <nl> struct hostent * h ; <nl>  <nl> - k_gethostname ( hostname , sizeof ( hostname )); <nl> + gethostname ( hostname , sizeof ( hostname )); <nl> loopback . s_addr = htonl ( INADDR_LOOPBACK ); <nl>  <nl> auth . family = FamilyLocal ;
EVP_CipherInit_ex ( EVP_CIPHER_CTX * ctx , const EVP_CIPHER * c , ENGINE * engine , <nl> ctx -> cipher = c ; <nl> ctx -> key_len = c -> key_len ; <nl>  <nl> - ctx -> cipher_data = malloc ( c -> ctx_size ); <nl> + ctx -> cipher_data = calloc ( 1 , c -> ctx_size ); <nl> if ( ctx -> cipher_data == NULL && c -> ctx_size != 0 ) <nl> return 0 ; <nl> 
gss_pseudo_random ( OM_uint32 * minor_status , <nl> gss_buffer_t prf_out ) <nl> { <nl> struct _gss_context * ctx = ( struct _gss_context *) context ; <nl> - gssapi_mech_interface m = ctx -> gc_mech ; <nl> + gssapi_mech_interface m ; <nl> OM_uint32 major_status ; <nl>  <nl> _mg_buffer_zero ( prf_out ); <nl> gss_pseudo_random ( OM_uint32 * minor_status , <nl> return GSS_S_NO_CONTEXT ; <nl> } <nl>  <nl> + m = ctx -> gc_mech ; <nl> + <nl> if ( m -> gm_pseudo_random == NULL ) <nl> return GSS_S_UNAVAILABLE ; <nl> 
 <nl> extern require_preauth ; <nl> extern sig_atomic_t exit_flag ; <nl> + extern char * keyfile ; <nl>  <nl> extern struct timeval now ; <nl> # define kdc_time ( now . tv_sec ) <nl> void loop ( krb5_context ); <nl>  <nl> void kdc_log ( int , const char * fmt , ...); <nl>  <nl> + Key * unseal_key ( Key * key ); <nl> + <nl> # define ALLOC ( X ) (( X ) = malloc ( sizeof (*( X )))) <nl>  <nl> # endif /* __KDC_LOCL_H__ */
cms_create_sd ( struct cms_create_sd_options * opt , int argc , char ** argv ) <nl> if ( ret ) <nl> hx509_err ( context , 1 , ret , " hx509_certs_find "); <nl> } <nl> + if (! opt -> embedded_certs_flag ) <nl> + flags |= HX509_CMS_SIGNATURE_NO_CERTS ; <nl> + if ( opt -> embed_leaf_only_flag ) <nl> + flags |= HX509_CMS_SIGNATURE_LEAF_ONLY ; <nl>  <nl> ret = rk_undumpdata ( infile , & p , & sz ); <nl> if ( ret )
verify_checksum ( krb5_context context , <nl> if ( keyed_checksum && crypto == NULL ) { <nl> krb5_set_error_message ( context , KRB5_PROG_SUMTYPE_NOSUPP , <nl> N_ (" Checksum type % s is keyed but no " <nl> - " crypto context ( key ) was passed in ", "") <nl> + " crypto context ( key ) was passed in ", ""), <nl> ct -> name ); <nl> return KRB5_PROG_SUMTYPE_NOSUPP ; /* XXX */ <nl> }
# ifndef KRB5_DEPRECATED <nl> # if defined ( __GNUC__ ) && (( __GNUC__ > 3 ) || (( __GNUC__ == 3 ) && ( __GNUC_MINOR__ >= 1 ))) <nl> # define KRB5_DEPRECATED __attribute__ (( deprecated )) <nl> -# elif defined ( _MSC_VER ) <nl> +# elif defined ( _MSC_VER ) && ( _MSC_VER > 1200 ) <nl> # define KRB5_DEPRECATED __declspec ( deprecated ) <nl> # else <nl> # define KRB5_DEPRECATED
main ( int argc , char ** argv ) <nl>  <nl> setprogname ( argv [ 0 ]); <nl>  <nl> + setlocale ( LC_ALL , ""); <nl> + bindtextdomain (" heimdal_kuser ", HEIMDAL_LOCALEDIR ); <nl> + textdomain (" heimdal_kuser "); <nl> + <nl> ret = krb5_init_context (& context ); <nl> if ( ret == KRB5_CONFIG_BADFORMAT ) <nl> errx ( 1 , " krb5_init_context failed to parse configuration file ");
EVP_BytesToKey ( const EVP_CIPHER * type , <nl> void * keydata , <nl> void * ivdata ) <nl> { <nl> + return - 1 ; <nl> } <nl> 
check ( void * opt , int argc , char ** argv ) <nl> p2 = strdup ( realm ); <nl> if ( p2 == NULL ) { <nl> krb5_warn ( context , errno , " malloc "); <nl> - free ( p ); <nl> goto fail ; <nl> } <nl> strlwr ( p2 );
tgs_build_reply ( astgs_request_t priv , <nl>  <nl> s = & adtkt . cname ; <nl> r = adtkt . crealm ; <nl> + } else if ( s == NULL ) { <nl> + ret = KRB5KDC_ERR_S_PRINCIPAL_UNKNOWN ; <nl> + _kdc_set_e_text ( r , " No server in request "); <nl> + goto out ; <nl> } <nl>  <nl> _krb5_principalname2krb5_principal ( context , & sp , * s , r );
del_enctype ( void * opt , int argc , char ** argv ) <nl> goto out2 ; <nl> } <nl>  <nl> + if ( kadm5_all_keys_are_bogus ( princ . n_key_data , princ . key_data )) { <nl> + krb5_warnx ( context , " user lacks get - keys privilege "); <nl> + goto out ; <nl> + } <nl> + <nl> new_key_data = malloc ( princ . n_key_data * sizeof (* new_key_data )); <nl> if ( new_key_data == NULL && princ . n_key_data != 0 ) { <nl> krb5_warnx ( context , " out of memory ");
stats_config ( <nl> rawstats . fp = NULL ; <nl> filegen_setup (& rawstats , now . l_ui ); <nl> } <nl> +# ifdef OPENSSL <nl> + if ( cryptostats . prefix == & statsdir [ 0 ] && <nl> + cryptostats . fp != NULL ) { <nl> + fclose ( cryptostats . fp ); <nl> + cryptostats . fp = NULL ; <nl> + filegen_setup (& cryptostats , now . l_ui ); <nl> + } <nl> +# endif /* OPENSSL */ <nl> } <nl> break ; <nl> 
int rdp_redirection_apply_settings ( rdpRdp * rdp ) <nl> settings -> TargetNetAddresses [ i ] = _strdup ( redirection -> TargetNetAddresses [ i ]); <nl> if (! settings -> TargetNetAddresses [ i ]) <nl> { <nl> - for (; i > 0 ; -- i ) <nl> - free ( settings -> TargetNetAddresses [ i ]); <nl> + UINT32 j ; <nl> + <nl> + for ( j = 0 ; j < i ; j ++) <nl> + free ( settings -> TargetNetAddresses [ j ]); <nl> return - 1 ; <nl> } <nl> }
rdpSettings * freerdp_settings_new ( DWORD flags ) <nl> settings -> SaltedChecksum = TRUE ; <nl> settings -> ServerPort = 3389 ; <nl> settings -> GatewayPort = 443 ; <nl> + settings -> GatewayBypassLocal = TRUE ; <nl> settings -> DesktopResize = TRUE ; <nl> settings -> ToggleFullscreen = TRUE ; <nl> settings -> DesktopPosX = 0 ;
void winpr_HexDump ( const char * tag , int level , const BYTE * data , int length ) <nl> const BYTE * p = data ; <nl> int i , line , offset = 0 ; <nl> const size_t llen = ( length > WINPR_HEXDUMP_LINE_LENGTH ) ? WINPR_HEXDUMP_LINE_LENGTH : length ; <nl> - size_t blen = 5 + llen * 5 ; <nl> + size_t blen = 7 + WINPR_HEXDUMP_LINE_LENGTH * 5 ; <nl> size_t pos = 0 ; <nl> char * buffer = malloc ( blen ); <nl> 
static int cliprdr_server_receive_format_list ( CliprdrServerContext * context , wSt <nl>  <nl> for ( index = 0 ; index < formatList . numFormats ; index ++) <nl> { <nl> - if ( formats [ index ]. formatName ) <nl> - free ( formats [ index ]. formatName ); <nl> + if ( formatList . formats [ index ]. formatName ) <nl> + free ( formatList . formats [ index ]. formatName ); <nl> } <nl>  <nl> - free ( formats ); <nl> + free ( formatList . formats ); <nl>  <nl> return 1 ; <nl> }
int makecert_context_process ( MAKECERT_CONTEXT * context , int argc , char ** argv ) <nl> if (! rsa ) <nl> return - 1 ; <nl>  <nl> + context -> rsa = RSA_new (); <nl> + if (! context -> rsa ) <nl> + { <nl> + BN_clear_free ( rsa ); <nl> + return - 1 ; <nl> + } <nl> BN_set_word ( rsa , RSA_F4 ); <nl> rc = RSA_generate_key_ex ( context -> rsa , key_length , rsa , NULL ); <nl> BN_clear_free ( rsa );
int WLog_ParseFilters () <nl> g_Filters = calloc ( g_FilterCount , sizeof ( wLogFilter )); <nl>  <nl> if (! g_Filters ) <nl> + { <nl> + free ( strs ); <nl> return - 1 ; <nl> + } <nl>  <nl> for ( count = 0 ; count < g_FilterCount ; count ++) <nl> { <nl> status = WLog_ParseFilter (& g_Filters [ count ], strs [ count ]); <nl>  <nl> if ( status < 0 ) <nl> + { <nl> + free ( strs ); <nl> return - 1 ; <nl> + } <nl> } <nl>  <nl> free ( strs );
static void * rdpei_schedule_thread ( void * arg ) <nl>  <nl> out : <nl>  <nl> - if ( error && rdpei -> rdpcontext ) <nl> + if ( error && rdpei && rdpei -> rdpcontext ) <nl> setChannelError ( rdpei -> rdpcontext , error , <nl> " rdpei_schedule_thread reported an error "); <nl> 
BOOL xf_event_process ( freerdp * instance , XEvent * event ) <nl> if ( event -> xcookie . type == GenericEvent && <nl> event -> xcookie . extension == xfi -> XInputOpcode ) <nl> { <nl> - switch ( cookie . evtype ) <nl> + switch ( cookie -> evtype ) <nl> { <nl> case XI_ButtonPress : <nl> case XI_Motion :
int rdtk_font_parse_descriptor_buffer ( rdtkFont * font , BYTE * buffer , int size ) <nl> } <nl>  <nl> font -> glyphCount = count ; <nl> - font -> glyphs = ( rdtkGlyph *) calloc ( font -> glyphCount , sizeof ( rdtkGlyph )); <nl> + font -> glyphs = NULL ; <nl> + if ( count > 0 ) <nl> + font -> glyphs = ( rdtkGlyph *) calloc ( font -> glyphCount , sizeof ( rdtkGlyph )); <nl>  <nl> if (! font -> glyphs ) <nl> return - 1 ;
void update_free ( rdpUpdate * update ) <nl>  <nl> xfree ( update -> bitmap_update . rectangles ); <nl> xfree ( update -> pointer ); <nl> + xfree ( update -> primary -> polyline . points ); <nl> + xfree ( update -> primary -> polygon_sc . points ); <nl> xfree ( update -> primary ); <nl> xfree ( update -> secondary ); <nl> xfree ( update -> altsec );
void tsmf_playback_ack ( IWTSVirtualChannelCallback * pChannelCallback , <nl> if (! callback || ! callback -> channel || ! callback -> channel -> Write ) <nl> { <nl> WLog_ERR ( TAG , " callback =% p , channel =% p , write =% p ", callback , <nl> - callback -> channel , callback -> channel -> Write ); <nl> + callback ? callback -> channel : NULL , <nl> + ( callback && callback -> channel ) ? callback -> channel -> Write : NULL ); <nl> } <nl> else <nl> {
boolean security_establish_keys ( uint8 * client_random , rdpRdp * rdp ) <nl>  <nl> memcpy ( rdp -> decrypt_update_key , rdp -> decrypt_key , 16 ); <nl> memcpy ( rdp -> encrypt_update_key , rdp -> encrypt_key , 16 ); <nl> + rdp -> decrypt_use_count = 0 ; <nl> + rdp -> decrypt_checksum_use_count = 0 ; <nl> + rdp -> encrypt_use_count = 0 ; <nl> + rdp -> encrypt_checksum_use_count = 0 ; <nl>  <nl> return true ; <nl> }
boolean nego_send_negotiation_response ( rdpNego * nego ) <nl> settings -> encryption_method = ENCRYPTION_METHOD_40BIT | ENCRYPTION_METHOD_128BIT | ENCRYPTION_METHOD_FIPS ; <nl> settings -> encryption_level = ENCRYPTION_LEVEL_CLIENT_COMPATIBLE ; <nl> } <nl> + if ( settings -> encryption && settings -> server_key == NULL && settings -> rdp_key_file == NULL ) <nl> + return false ; <nl> } <nl> else if ( settings -> selected_protocol == PROTOCOL_TLS ) <nl> {
int freerdp_parse_args ( rdpSettings * settings , int argc , char ** argv , <nl> } <nl> else if ( strcmp ("-- plugin ", argv [ index ]) == 0 ) <nl> { <nl> - t = index ; <nl> index ++; <nl> + t = index ; <nl> if ( index == argc ) <nl> { <nl> printf (" missing plugin name \ n ");
static BOOL update_read_bitmap_data ( rdpUpdate * update , wStream * s , BITMAP_DATA * <nl> { <nl> if (!( bitmapData -> flags & NO_BITMAP_COMPRESSION_HDR )) <nl> { <nl> + if ( Stream_GetRemainingLength ( s ) < 8 ) <nl> + return FALSE ; <nl> + <nl> Stream_Read_UINT16 ( s , <nl> bitmapData -> cbCompFirstRowSize ); /* cbCompFirstRowSize ( 2 bytes ) */ <nl> Stream_Read_UINT16 ( s ,
boolean fastpath_send_update_pdu ( rdpFastPath * fastpath , uint8 updateCode , STREAM <nl> try_comp = rdp -> settings -> compression ; <nl> comp_update = stream_new ( 0 ); <nl>  <nl> - for ( fragment = 0 ; totalLength > 0 ; fragment ++) <nl> + for ( fragment = 0 ; totalLength > 0 || fragment == 0 ; fragment ++) <nl> { <nl> stream_get_mark ( s , holdp ); <nl> ls = s ;
SECURITY_STATUS ntlm_read_NegotiateMessage ( NTLM_CONTEXT * context , PSecBuffer buf <nl> return SEC_E_INVALID_TOKEN ; <nl> } <nl>  <nl> + if ( Stream_GetRemainingLength ( s ) < 4 ) <nl> + { <nl> + Stream_Free ( s , FALSE ); <nl> + return SEC_E_INVALID_TOKEN ; <nl> + } <nl> Stream_Read_UINT32 ( s , message -> NegotiateFlags ); /* NegotiateFlags ( 4 bytes ) */ <nl>  <nl> if (!(( message -> NegotiateFlags & NTLMSSP_REQUEST_TARGET ) &&
BOOL security_fips_decrypt ( BYTE * data , size_t length , rdpRdp * rdp ) <nl> { <nl> size_t olen ; <nl>  <nl> + if (! rdp || ! rdp -> fips_decrypt ) <nl> + return FALSE ; <nl> + <nl> if (! winpr_Cipher_Update ( rdp -> fips_decrypt , data , length , data , & olen )) <nl> return FALSE ; <nl> 
static UINT drive_process_irp_query_directory ( DRIVE_DEVICE * drive , IRP * irp ) <nl> Stream_Read_UINT32 ( irp -> input , PathLength ); <nl> Stream_Seek ( irp -> input , 23 ); /* Padding */ <nl> path = ( WCHAR *) Stream_Pointer ( irp -> input ); <nl> + if (! Stream_CheckAndLogRequiredLength ( TAG , irp -> input , PathLength )) <nl> + return ERROR_INVALID_DATA ; <nl> + <nl> file = drive_get_file_by_id ( drive , irp -> FileId ); <nl>  <nl> if ( file == NULL )
static void rdpsnd_process_receive ( rdpSvcPlugin * plugin , STREAM * data_in ) <nl> if ( rdpsnd -> expectingWave ) <nl> { <nl> rdpsnd_process_message_wave ( rdpsnd , data_in ); <nl> + stream_free ( data_in ); <nl> return ; <nl> } <nl>  <nl> static void rdpsnd_process_receive ( rdpSvcPlugin * plugin , STREAM * data_in ) <nl> DEBUG_WARN (" unknown msgType % d ", msgType ); <nl> break ; <nl> } <nl> + <nl> + stream_free ( data_in ); <nl> } <nl>  <nl> static void rdpsnd_register_device_plugin ( rdpsndPlugin * rdpsnd , rdpsndDevicePlugin * device )
wReference * ReferenceTable_GetFreeEntry ( wReferenceTable * referenceTable ) <nl> new_size = referenceTable -> size * 2 ; <nl> new_ref = ( wReference *) realloc ( referenceTable -> array , <nl> sizeof ( wReference ) * new_size ); <nl> + if (! new_ref ) <nl> + return NULL ; <nl>  <nl> referenceTable -> size = new_size ; <nl> referenceTable -> array = new_ref ;
BOOL ValidFileNameComponent ( LPCWSTR lpFileName ) <nl> { <nl> LPCWSTR c = NULL ; <nl>  <nl> + if (! lpFileName ) <nl> + return FALSE ; <nl> + <nl> /* CON */ <nl> if (( lpFileName [ 0 ] != L '\ 0 ' && ( lpFileName [ 0 ] == L ' C ' || lpFileName [ 0 ] == L ' c ')) && <nl> ( lpFileName [ 1 ] != L '\ 0 ' && ( lpFileName [ 1 ] == L ' O ' || lpFileName [ 1 ] == L ' o ')) &&
struct rdp_freerdp <nl> Must be set to NULL if not needed . */ <nl> UINT64 paddingC [ 47 - 35 ]; /* 35 */ <nl>  <nl> - ALIGN64 ConnectionCallbackState ; /* 48 */ <nl> + ALIGN64 UINT ConnectionCallbackState ; /* 47 */ <nl>  <nl> ALIGN64 pPreConnect PreConnect ; /**< ( offset 48 ) <nl> Callback for pre - connect operations .
rdpSettings * freerdp_settings_new ( DWORD flags ) <nl> settings -> NoBitmapCompressionHeader = TRUE ; <nl> settings -> RefreshRect = TRUE ; <nl> settings -> SuppressOutput = TRUE ; <nl> - settings -> GlyphSupportLevel = GLYPH_SUPPORT_FULL ; <nl> + settings -> GlyphSupportLevel = GLYPH_SUPPORT_NONE ; <nl> settings -> GlyphCache = malloc ( sizeof ( GLYPH_CACHE_DEFINITION ) * 10 ); <nl>  <nl> if (! settings -> GlyphCache )
BOOL glyph_cache_put ( rdpGlyphCache * glyphCache , UINT32 id , UINT32 index , rdpGlyp <nl> return FALSE ; <nl> } <nl>  <nl> - if ( index > glyphCache -> glyphCache [ id ]. number ) <nl> + if ( index >= glyphCache -> glyphCache [ id ]. number ) <nl> { <nl> WLog_ERR ( TAG , " invalid glyph cache index : %" PRIu32 " in cache id : %" PRIu32 "", index , id ); <nl> return FALSE ;
int transport_write ( rdpTransport * transport , wStream * s ) <nl> int status = - 1 ; <nl> int writtenlength = 0 ; <nl>  <nl> + if (! transport ) <nl> + return - 1 ; <nl> + <nl> + if (! transport -> frontBio ) <nl> + { <nl> + transport -> layer = TRANSPORT_LAYER_CLOSED ; <nl> + return - 1 ; <nl> + } <nl> + <nl> EnterCriticalSection (&( transport -> WriteLock )); <nl>  <nl> length = Stream_GetPosition ( s );
BOOL rdp_server_establish_keys ( rdpRdp * rdp , wStream * s ) <nl> if ( rand_len != key_len + 8 ) <nl> { <nl> WLog_ERR ( TAG , " invalid encrypted client random length "); <nl> + free ( client_random ); <nl> goto end ; <nl> } <nl>  <nl> crypt_client_random = calloc ( 1 , rand_len ); <nl> if (! crypt_client_random ) <nl> + { <nl> + free ( client_random ); <nl> goto end ; <nl> + } <nl> + <nl> Stream_Read ( s , crypt_client_random , rand_len ); <nl>  <nl> mod = rdp -> settings -> RdpServerRsaKey -> Modulus ;
public : <nl> return CONTINUE ; <nl> } <nl>  <nl> + void OnNick ( const CNick & Nick , const CString & sNewNick , const std :: vector < CChan *>& vChans ) override { <nl> + for ( CChan * pChan : vChans ) { <nl> + Message (* pChan ); <nl> + } <nl> + } <nl> + <nl> void ShowCommand ( const CString & sLine ) { <nl> PutModule (" Current limit is " + CString ( m_iThresholdMsgs ) + " lines " <nl> " in " + CString ( m_iThresholdSecs ) + " secs .");
BGD_DECLARE ( gdImagePtr ) gdImageCropThreshold ( gdImagePtr im , const unsigned int c <nl> return NULL ; <nl> } <nl>  <nl> + if ( color < 0 || (! gdImageTrueColor ( im ) && color >= gdImageColorsTotal ( im ))) { <nl> + return NULL ; <nl> + } <nl> + <nl> /* TODO : Add gdImageGetRowPtr and works with ptr at the row level <nl> * for the true color and palette images <nl> * new formats will simply work with ptr
namespace mongo { <nl> class BalancingWindowUnitTest : public UnitTest { <nl> public : <nl> void run () { <nl> + <nl> + if ( ! cmdLine . isMongos () ) <nl> + return ; <nl> + <nl> // T0 < T1 < now < T2 < T3 and Error <nl> const string T0 = " 9 : 00 "; <nl> const string T1 = " 11 : 00 ";
namespace mongo { <nl>  <nl> virtual void startRequest () {} <nl>  <nl> + virtual void onAddAuthorizedPrincipal ( Principal *) {} <nl> + <nl> + virtual void onLogoutDatabase ( const std :: string & dbname ) {} <nl> + <nl> private : <nl> bool _returnValue ; <nl> };
namespace mongo { <nl> void DBConfig :: enableSharding () { <nl> if ( _shardingEnabled ) <nl> return ; <nl> + <nl> + assert ( _name != " config " ); <nl> + <nl> scoped_lock lk ( _lock ); <nl> _shardingEnabled = true ; <nl> _save ();
namespace cling { <nl> Interpreter :: Interpreter ( int argc , const char * const * argv , <nl> const char * llvmdir /*= 0 */) : <nl> m_UniqueCounter ( 0 ), m_PrintDebug ( false ), <nl> - m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ) { <nl> + m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ), <nl> + m_LastCustomPragmaDiagPopPoint (){ <nl>  <nl> m_LLVMContext . reset ( new llvm :: LLVMContext ); <nl> std :: vector < unsigned > LeftoverArgsIdx ;
namespace cling { <nl> ///\ param [ in ] file1 - A file to diff <nl> ///\ param [ in ] file2 - A file to diff <nl> ///\ param [ in ] differences - The differences if any between file1 and file2 <nl> + ///\ param [ in ] ignores - A list of differences to ignore . <nl> ///\ returns true if there is difference in the contents . <nl> /// <nl> bool differentContent ( const std :: string & file1 , const std :: string & file2 ,
namespace utils { <nl> newBody . insert ( newBody . begin () + indexOfLastExpr , DRE ); <nl>  <nl> // Attach the new body ( note : it does dealloc / alloc of all nodes ) <nl> - CS -> setStmts ( S -> getASTContext (), newBody . data (), newBody . size ()); <nl> + CS -> setStmts ( S -> getASTContext (), & newBody . front (), newBody . size ()); <nl> if ( FoundAt ) <nl> * FoundAt = indexOfLastExpr ; <nl> return DRE ;
static int asf_parse_packet ( AVFormatContext * s , AVIOContext * pb , AVPacket * pkt ) <nl> continue ; <nl> } <nl> asf -> asf_st = & asf -> streams [ s -> streams [ asf -> stream_index ]-> id ]; <nl> - asf -> asf_st -> skip_to_key = 0 ; <nl> + if (! asf -> packet_frag_offset ) <nl> + asf -> asf_st -> skip_to_key = 0 ; <nl> } <nl> asf_st = asf -> asf_st ; <nl> av_assert0 ( asf_st );
void ff_MPV_frame_end ( MpegEncContext * s ) <nl> s -> avctx -> coded_frame = & s -> current_picture_ptr -> f ; <nl>  <nl> if ( s -> codec_id != CODEC_ID_H264 && s -> current_picture . f . reference ) { <nl> - ff_thread_report_progress (& s -> current_picture_ptr -> f , <nl> - s -> mb_height - 1 , 0 ); <nl> + ff_thread_report_progress (& s -> current_picture_ptr -> f , INT_MAX , 0 ); <nl> } <nl> } <nl> 
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> } <nl>  <nl> /* Generic DSP setup */ <nl> + memset (& dsp , 0 , sizeof ( dsp )); <nl> ff_dsputil_init (& dsp , avctx ); <nl> ff_set_cmp (& dsp , dsp . ildct_cmp , avctx -> ildct_cmp ); <nl> s -> get_pixels = dsp . get_pixels ;
int ff_mov_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> else <nl> samples_in_chunk = 1 ; <nl>  <nl> + if ( samples_in_chunk < 1 ) { <nl> + av_log ( s , AV_LOG_ERROR , " fatal error , input packet contains no samples \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> /* copy extradata if it exists */ <nl> if ( trk -> vos_len == 0 && par -> extradata_size > 0 && <nl> ! TAG_IS_AVCI ( trk -> tag ) &&
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> uint32_t u ; <nl> } value ; <nl>  <nl> - int sign ; <nl> + unsigned int sign ; <nl> int exp = s -> float_max_exp ; <nl>  <nl> if ( s -> got_extra_bits ) {
int av_get_audio_frame_duration ( AVCodecContext * avctx , int frame_bytes ) <nl> return frame_bytes * 8 / bps ; <nl> } <nl>  <nl> - if ( ch > 0 ) { <nl> + if ( ch > 0 && ch < INT_MAX / 16 ) { <nl> /* calc from frame_bytes and channels */ <nl> switch ( id ) { <nl> case AV_CODEC_ID_ADPCM_AFC :
static int decode_pic_hdr ( IVI5DecContext * ctx , AVCodecContext * avctx ) <nl> ctx -> gop_invalid = 0 ; <nl> } <nl>  <nl> + if ( ctx -> frame_type == FRAMETYPE_INTER_SCAL && ! ctx -> is_scalable ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Scalable inter frame in non scaleable stream \ n "); <nl> + ctx -> frame_type = FRAMETYPE_INTER ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( ctx -> frame_type != FRAMETYPE_NULL ) { <nl> ctx -> frame_flags = get_bits (& ctx -> gb , 8 ); <nl> 
static const int prof_h264_high [] = { FF_PROFILE_H264_CONSTRAINED_BASELINE , <nl> FF_PROFILE_UNKNOWN }; <nl> static const int prof_hevc_main [] = { FF_PROFILE_HEVC_MAIN , <nl> FF_PROFILE_UNKNOWN }; <nl> - static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN , <nl> - FF_PROFILE_HEVC_MAIN_10 , <nl> + static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN_10 , <nl> FF_PROFILE_UNKNOWN }; <nl>  <nl> static const dxva_mode dxva_modes [] = {
static int mpeg_decode_slice ( Mpeg1Context * s1 , int mb_y , <nl> break ; <nl> } <nl> } <nl> + if ( s -> mb_x >= ( unsigned ) s -> mb_width ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " initial skip overflow \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> s -> resync_mb_x = s -> mb_x ; <nl> s -> resync_mb_y = s -> mb_y = mb_y ;
decode_intra_mb : <nl>  <nl> // We assume these blocks are very rare so we do not optimize it . <nl> h -> intra_pcm_ptr = align_get_bits (& h -> gb ); <nl> + if ( get_bits_left (& h -> gb ) < mb_size ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , " Not enough data for an intra PCM block .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> skip_bits_long (& h -> gb , mb_size ); <nl>  <nl> // In deblocking , the quantizer is 0
int avcodec_copy_context ( AVCodecContext * dest , const AVCodecContext * src ) <nl> memcpy ( dest , src , sizeof (* dest )); <nl>  <nl> dest -> priv_data = orig_priv_data ; <nl> + <nl> + if ( orig_priv_data ) <nl> + av_opt_copy ( orig_priv_data , src -> priv_data ); <nl> + <nl> dest -> codec = orig_codec ; <nl>  <nl> /* set values specific to opened codecs back to their default state */
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
static int cdg_decode_frame ( AVCodecContext * avctx , <nl> int buf_size = avpkt -> size ; <nl> int ret ; <nl> uint8_t command , inst ; <nl> - uint8_t cdg_data [ CDG_DATA_SIZE ]; <nl> + uint8_t cdg_data [ CDG_DATA_SIZE ] = { 0 }; <nl> AVFrame * frame = data ; <nl> CDGraphicsContext * cc = avctx -> priv_data ; <nl> 
static int ea_read_header ( AVFormatContext * s ) <nl> ea -> audio_codec = 0 ; <nl> return 1 ; <nl> } <nl> - if ( ea -> bytes <= 0 ) { <nl> + if ( ea -> bytes <= 0 || ea -> bytes > 2 ) { <nl> av_log ( s , AV_LOG_ERROR , <nl> " Invalid number of bytes per sample : % d \ n ", ea -> bytes ); <nl> ea -> audio_codec = AV_CODEC_ID_NONE ;
static int process_audio_header_elements ( AVFormatContext * s ) <nl> } <nl>  <nl> switch ( compression_type ) { <nl> + case 0 : ea -> audio_codec = CODEC_ID_PCM_S16LE ; break ; <nl> case 7 : ea -> audio_codec = CODEC_ID_ADPCM_EA ; break ; <nl> default : <nl> av_log ( s , AV_LOG_ERROR , " unsupported stream type ; compression_type =% i \ n ", compression_type );
static void compute_pkt_fields ( AVFormatContext * s , AVStream * st , <nl> if ( pkt -> dts != AV_NOPTS_VALUE && <nl> pkt -> pts == AV_NOPTS_VALUE && <nl> st -> last_IP_duration > 0 && <nl> - ( st -> cur_dts - next_dts ) <= 1 && <nl> + (( uint64_t ) st -> cur_dts - ( uint64_t ) next_dts + 1 ) <= 2 && <nl> next_dts != next_pts && <nl> next_pts != AV_NOPTS_VALUE ) <nl> pkt -> pts = next_dts ;
static const int prof_h264_high [] = { FF_PROFILE_H264_CONSTRAINED_BASELINE , <nl> FF_PROFILE_UNKNOWN }; <nl> static const int prof_hevc_main [] = { FF_PROFILE_HEVC_MAIN , <nl> FF_PROFILE_UNKNOWN }; <nl> - static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN , <nl> - FF_PROFILE_HEVC_MAIN_10 , <nl> + static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN_10 , <nl> FF_PROFILE_UNKNOWN }; <nl>  <nl> static const dxva_mode dxva_modes [] = {
int ff_vorbis_len2vlc ( uint8_t * bits , uint32_t * codes , unsigned num ) <nl> exit_at_level [ i ] = 0 ; <nl> // construct code ( append 0s to end ) and introduce new exits <nl> for ( j = i + 1 ; j <= bits [ p ]; ++ j ) <nl> - exit_at_level [ j ] = code + ( 1 << ( j - 1 )); <nl> + exit_at_level [ j ] = code + ( 1u << ( j - 1 )); <nl> codes [ p ] = code ; <nl> } <nl> 
static void dca_exss_parse_header ( DCAContext * s ) <nl> } <nl> } <nl>  <nl> + av_assert0 ( num_assets > 0 ); // silence a warning <nl> + <nl> for ( i = 0 ; i < num_assets ; i ++) <nl> asset_size [ i ] = get_bits_long (& s -> gb , 16 + 4 * blownup ); <nl> 
int ff_frame_thread_init ( AVCodecContext * avctx ) <nl> p -> frame = av_frame_alloc (); <nl> if (! p -> frame ) { <nl> err = AVERROR ( ENOMEM ); <nl> + av_freep (& copy ); <nl> goto error ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , const uint8_t * databuf , <nl>  <nl>  <nl> /* set the bitstream reader at the start of the second Sound Unit */ <nl> - init_get_bits8 (& q -> gb , <nl> + ret = init_get_bits8 (& q -> gb , <nl> ptr1 , q -> decoded_bytes_buffer + js_block_align - ptr1 ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> /* Fill the Weighting coeffs delay buffer */ <nl> memmove ( q -> weighting_delay [ js_pair ], & q -> weighting_delay [ js_pair ][ 2 ],
static int rm_write_audio ( AVFormatContext * s , const uint8_t * buf , int size , int <nl>  <nl> /* XXX : suppress this malloc */ <nl> buf1 = av_malloc ( size * sizeof ( uint8_t )); <nl> + if (! buf1 ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> write_packet_header ( s , stream , size , !!( flags & AV_PKT_FLAG_KEY )); <nl> 
int ff_rate_control_init ( MpegEncContext * s ) <nl> rcc -> pass1_rc_eq_output_sum = 0 . 001 ; <nl> rcc -> pass1_wanted_bits = 0 . 001 ; <nl>  <nl> + if ( s -> avctx -> qblur > 1 . 0 ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " qblur too large \ n "); <nl> + return - 1 ; <nl> + } <nl> /* init stuff with the user specified complexity */ <nl> if ( s -> avctx -> rc_initial_cplx ){ <nl> for ( i = 0 ; i < 60 * 30 ; i ++){
static inline int get_duration ( AVIStream * ast , int len ) <nl> static int get_riff ( AVFormatContext * s , AVIOContext * pb ) <nl> { <nl> AVIContext * avi = s -> priv_data ; <nl> - char header [ 8 ]; <nl> + char header [ 8 ] = { 0 }; <nl> int i ; <nl>  <nl> /* check RIFF header */
static int parse_video_var ( AVFormatContext * avctx , AVStream * st , const char * nam <nl> st -> nb_frames = st -> duration = var_read_int ( pb , size ); <nl> } else if (! strcmp ( name , " COMPRESSION ")) { <nl> char * str = var_read_string ( pb , size ); <nl> + if (! str ) <nl> + return AVERROR_INVALIDDATA ; <nl> if (! strcmp ( str , " 1 ")) { <nl> st -> codec -> codec_id = AV_CODEC_ID_MVC1 ; <nl> } else if (! strcmp ( str , " 2 ")) {
extern void idct_add_altivec ( uint8_t * dest , int line_size , int16_t * block ); <nl>  <nl> void MPV_common_init_altivec ( MpegEncContext * s ) <nl> { <nl> + if ( mm_flags & MM_ALTIVEC == 0 ) return ; <nl> + <nl> if ( s -> avctx -> lowres == 0 ) <nl> { <nl> if (( s -> avctx -> idct_algo == FF_IDCT_AUTO ) ||
void av_opt_freep_ranges ( AVOptionRanges ** rangesp ) <nl> int i ; <nl> AVOptionRanges * ranges = * rangesp ; <nl>  <nl> + if (! ranges ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < ranges -> nb_ranges * ranges -> nb_components ; i ++) { <nl> AVOptionRange * range = ranges -> range [ i ]; <nl> av_freep (& range -> str );
static int read_header ( AVFormatContext * s ) <nl>  <nl> b -> data_start = avio_tell ( s -> pb ); <nl>  <nl> - if (( major != 1 || minor ) && ! bfstm ) <nl> + if (! bfstm && ( major != 1 || minor )) <nl> avpriv_request_sample ( s , " Version % d .% d ", major , minor ); <nl>  <nl> return 0 ;
static void smc_decode_stream ( SmcContext * s ) <nl> } else <nl> color_table_index = CQUAD * s -> buf [ stream_ptr ++]; <nl>  <nl> - while ( n_blocks --) { <nl> + while ( n_blocks -- && stream_ptr + 3 < s -> size ) { <nl> color_flags = AV_RB32 (& s -> buf [ stream_ptr ]); <nl> stream_ptr += 4 ; <nl> /* flag mask actually acts as a bit shift count here */
# include " cabac_functions . h " <nl> # include " hevc . h " <nl>  <nl> -# define CABAC_MAX_BIN 100 <nl> +# define CABAC_MAX_BIN 31 <nl>  <nl> /** <nl> * number of bin by SyntaxElement .
av_cold void ff_fft_fixed_init_arm ( FFTContext * s ) <nl> s -> fft_calc = ff_fft_fixed_calc_neon ; <nl>  <nl> # if CONFIG_MDCT <nl> - if (! s -> inverse && s -> mdct_bits >= 5 ) { <nl> + if (! s -> inverse && s -> nbits >= 3 ) { <nl> s -> mdct_permutation = FF_MDCT_PERM_INTERLEAVE ; <nl> s -> mdct_calc = ff_mdct_fixed_calc_neon ; <nl> s -> mdct_calcw = ff_mdct_fixed_calcw_neon ;
int main ( int argc , char ** argv ){ <nl> FILE * f [ 2 ]; <nl> int i , pos ; <nl> int siglen , datlen ; <nl> - int bestpos ; <nl> + int bestpos = 0 ; <nl> double bestc = 0 ; <nl> double sigamp = 0 ; <nl> int16_t * signal , * data ;
void ff_frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> av_freep (& p -> avctx -> slice_offset ); <nl> } <nl>  <nl> + av_buffer_unref (& p -> avctx -> hw_frames_ctx ); <nl> + <nl> av_freep (& p -> avctx -> internal ); <nl> av_freep (& p -> avctx ); <nl> }
static int decode_frame ( NUTContext * nut , AVPacket * pkt , int frame_code ) <nl> if ( ret != size ) { <nl> if ( ret < 0 ) <nl> return ret ; <nl> - av_shrink_packet ( pkt , nut -> header_len [ header_idx ] + size ); <nl> } <nl> + av_shrink_packet ( pkt , nut -> header_len [ header_idx ] + ret ); <nl>  <nl> pkt -> stream_index = stream_id ; <nl> if ( stc -> last_flags & FLAG_KEY )
static int tcp_write_packet ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl> interleave_header [ 0 ] = '$'; <nl> interleave_header [ 1 ] = id ; <nl> AV_WB16 ( interleave_header + 2 , packet_len ); <nl> - url_write ( rt -> rtsp_hd , interleaved_packet , 4 + packet_len ); <nl> + url_write ( rt -> rtsp_hd_out , interleaved_packet , 4 + packet_len ); <nl> ptr += packet_len ; <nl> size -= packet_len ; <nl> }
static int vorbis_packet ( AVFormatContext * s , int idx ) <nl> s -> streams [ idx ]-> start_time = os -> lastpts + first_duration ; <nl> if ( s -> streams [ idx ]-> duration ) <nl> s -> streams [ idx ]-> duration -= s -> streams [ idx ]-> start_time ; <nl> - s -> streams [ idx ]-> cur_dts = AV_NOPTS_VALUE ; <nl> priv -> final_pts = AV_NOPTS_VALUE ; <nl> avpriv_vorbis_parse_reset (& priv -> vp ); <nl> }
static int tscc2_decode_mb ( TSCC2Context * c , int * q , int vlc_set , <nl> if ( ac == 0x1000 ) <nl> ac = get_bits ( gb , 12 ); <nl> bpos += ac & 0xF ; <nl> - if ( bpos >= 64 ) <nl> + if ( bpos >= 16 ) <nl> return AVERROR_INVALIDDATA ; <nl> val = sign_extend ( ac >> 4 , 8 ); <nl> c -> block [ tscc2_zigzag [ bpos ++]] = val ;
void av_opt_freep_ranges ( AVOptionRanges ** rangesp ) <nl>  <nl> for ( i = 0 ; i < ranges -> nb_ranges * ranges -> nb_components ; i ++) { <nl> AVOptionRange * range = ranges -> range [ i ]; <nl> - av_freep (& range -> str ); <nl> - av_freep (& ranges -> range [ i ]); <nl> + if ( range ) { <nl> + av_freep (& range -> str ); <nl> + av_freep (& ranges -> range [ i ]); <nl> + } <nl> } <nl> av_freep (& ranges -> range ); <nl> av_freep ( rangesp );
static int parse_read_interval ( const char * interval_spec , <nl> } <nl> interval -> end = lli ; <nl> } else { <nl> + interval -> duration_frames = 0 ; <nl> ret = av_parse_time (& us , p , 1 ); <nl> if ( ret < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid interval end / duration specification '% s '\ n ", p );
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> LclDecContext * const c = avctx -> priv_data ; <nl> unsigned int pixel_ptr ; <nl> int row , col ; <nl> - unsigned char * encoded , * outptr ; <nl> + unsigned char * encoded = avpkt -> data , * outptr ; <nl> uint8_t * y_out , * u_out , * v_out ; <nl> unsigned int width = avctx -> width ; // Real image width <nl> unsigned int height = avctx -> height ; // Real image height
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterLink * outlink = inlink -> dst -> outputs [ 0 ]; <nl>  <nl> AVFilterBufferRef * out ; <nl> - int direct , c ; <nl> + int direct = 0 , c ; <nl>  <nl> if ( in -> perms & AV_PERM_WRITE ) { <nl> direct = 1 ;
static void copy_bits ( PutBitContext * pb , <nl> rmn_bits = rmn_bytes = get_bits_left ( gb ); <nl> if ( rmn_bits < nbits ) <nl> return ; <nl> + if ( nbits > pb -> size_in_bits - put_bits_count ( pb )) <nl> + return ; <nl> rmn_bits &= 7 ; rmn_bytes >>= 3 ; <nl> if (( rmn_bits = FFMIN ( rmn_bits , nbits )) > 0 ) <nl> put_bits ( pb , rmn_bits , get_bits ( gb , rmn_bits ));
static void hls_prediction_unit ( HEVCContext * s , int x0 , int y0 , <nl>  <nl> MvField * tab_mvf = s -> ref -> tab_mvf ; <nl> RefPicList * refPicList = s -> ref -> refPicList ; <nl> - HEVCFrame * ref0 , * ref1 ; <nl> + HEVCFrame * ref0 = NULL , * ref1 = NULL ; <nl> uint8_t * dst0 = POS ( 0 , x0 , y0 ); <nl> uint8_t * dst1 = POS ( 1 , x0 , y0 ); <nl> uint8_t * dst2 = POS ( 2 , x0 , y0 );
static int decode_iccp_chunk ( PNGDecContext * s , int length , AVFrame * f ) <nl> return ret ; <nl>  <nl> av_bprint_finalize (& bp , ( char **)& data ); <nl> + if (! data ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> sd = av_frame_new_side_data ( f , AV_FRAME_DATA_ICC_PROFILE , bp . len ); <nl> if (! sd ) {
static av_cold int mpc8_decode_init ( AVCodecContext * avctx ) <nl> c -> frames = 1 << ( get_bits (& gb , 3 ) * 2 ); <nl>  <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ; <nl> - avctx -> channel_layout = ( avctx -> channels == 2 ) ? AV_CH_LAYOUT_STEREO : AV_CH_LAYOUT_MONO ; <nl> + avctx -> channel_layout = ( channels == 2 ) ? AV_CH_LAYOUT_STEREO : AV_CH_LAYOUT_MONO ; <nl> + avctx -> channels = channels ; <nl>  <nl> if ( vlc_initialized ) return 0 ; <nl> av_log ( avctx , AV_LOG_DEBUG , " Initing VLC \ n ");
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ; <nl>  <nl> - s -> filter_slice_edges = av_malloc ( ctb_count ); <nl> + s -> filter_slice_edges = av_mallocz ( ctb_count ); <nl> s -> tab_slice_address = av_malloc_array ( pic_size_in_ctb , <nl> sizeof (* s -> tab_slice_address )); <nl> s -> qp_y_tab = av_malloc_array ( pic_size_in_ctb ,
static int decode_frame ( AVCodecContext * avctx , void * data , <nl> buf = av_realloc ( s -> packet_buffer , avpkt -> size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! buf ) <nl> return AVERROR ( ENOMEM ); <nl> + memset ( buf + avpkt -> size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> s -> packet_buffer = buf ; <nl> memcpy ( s -> packet_buffer , avpkt -> data , avpkt -> size ); <nl> if (( ret = init_get_bits8 ( gb , s -> packet_buffer , avpkt -> size )) < 0 )
void checkasm_stack_clobber ( uint64_t clobber , ...); <nl> }\ <nl> } while ( 0 ) <nl> # else <nl> -# define bench_new (...) <nl> +# define bench_new (...) while ( 0 ) <nl> # endif <nl>  <nl> # endif
static int vqf_read_header ( AVFormatContext * s ) <nl>  <nl> header_size -= len ; <nl>  <nl> - } while ( header_size >= 0 ); <nl> + } while ( header_size >= 0 && ! url_feof ( s -> pb )); <nl>  <nl> switch ( rate_flag ) { <nl> case - 1 :
static void mxf_free_metadataset ( MXFMetadataSet ** ctx , int freectx ) <nl> case MaterialPackage : <nl> av_freep (&(( MXFPackage *)* ctx )-> tracks_refs ); <nl> av_freep (&(( MXFPackage *)* ctx )-> name ); <nl> + av_freep (&(( MXFPackage *)* ctx )-> comment_refs ); <nl> break ; <nl> case TaggedValue : <nl> av_freep (&(( MXFTaggedValue *)* ctx )-> name );
static void free_tables ( H264Context * h ){ <nl> av_freep (& h -> mb2b_xy ); <nl> av_freep (& h -> mb2b8_xy ); <nl>  <nl> - for ( i = 0 ; i < h -> s . avctx -> thread_count ; i ++) { <nl> + for ( i = 0 ; i < MAX_THREADS ; i ++) { <nl> hx = h -> thread_context [ i ]; <nl> if (! hx ) continue ; <nl> av_freep (& hx -> top_borders [ 1 ]);
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl>  <nl> h -> b_stride = 4 * s -> mb_width ; <nl>  <nl> - ff_h264_alloc_tables ( h ); <nl> + if ( ff_h264_alloc_tables ( h ) < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " svq3 memory allocation failed \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> return 0 ;
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> end = ptr + FFMIN ( 2 + text_length , avpkt -> size ); <nl> ptr += 2 ; <nl>  <nl> + mov_text_cleanup ( m ); <nl> + <nl> tsmb_size = 0 ; <nl> m -> tracksize = 2 + text_length ; <nl> m -> style_entries = 0 ;
void ff_h264_direct_ref_list_init ( H264Context * const h ){ <nl> h -> col_fieldoff = 2 * h -> ref_list [ 1 ][ 0 ]. f . reference - 3 ; <nl> } <nl>  <nl> - if ( cur -> f . pict_type != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> + if ( h -> slice_type_nos != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> return ; <nl>  <nl> for ( list = 0 ; list < 2 ; list ++){
static uint32_t softfloat_mul ( uint32_t x , uint64_t mantissa ) <nl>  <nl> static uint8_t lag_calc_zero_run ( int8_t x ) <nl> { <nl> - return ( x << 1 ) ^ ( x >> 7 ); <nl> + return ( x * 2 ) ^ ( x >> 7 ); <nl> } <nl>  <nl> static int lag_decode_prob ( GetBitContext * gb , uint32_t * value )
static int url_alloc_for_protocol ( URLContext ** puc , struct URLProtocol * up , <nl> av_log ( uc , AV_LOG_ERROR , " Error parsing options string % s \ n ", start ); <nl> av_freep (& uc -> priv_data ); <nl> av_freep (& uc ); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl> memmove ( start , key + 1 , strlen ( key ));
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> avio_read ( pb , buf , 4 ); <nl> buf [ 4 ] = 0 ; <nl> } else { <nl> + AV_WL32 ( buf , 0 ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */ <nl> ast -> deint_id = AV_RL32 ( buf ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */
void avsubtitle_free ( AVSubtitle * sub ) <nl>  <nl> static int do_decode ( AVCodecContext * avctx , AVPacket * pkt ) <nl> { <nl> - int got_frame ; <nl> + int got_frame = 0 ; <nl> int ret ; <nl>  <nl> av_assert0 (! avctx -> internal -> buffer_frame -> buf [ 0 ]);
static av_cold int encode_init ( AVCodecContext * avctx ) <nl> // case PIX_FMT_YUV444P : <nl> // case PIX_FMT_YUV422P : <nl> case PIX_FMT_YUV420P : <nl> - case PIX_FMT_GRAY8 : <nl> +// case PIX_FMT_GRAY8 : <nl> // case PIX_FMT_YUV411P : <nl> // case PIX_FMT_YUV410P : <nl> s -> colorspace_type = 0 ;
static int mpeg_mux_write_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> stream -> next_packet = & stream -> premux_packet ; <nl> * stream -> next_packet = <nl> pkt_desc = av_mallocz ( sizeof ( PacketDesc )); <nl> + if (! pkt_desc ) <nl> + return AVERROR ( ENOMEM ); <nl> pkt_desc -> pts = pts ; <nl> pkt_desc -> dts = dts ; <nl> pkt_desc -> unwritten_size =
reload : <nl>  <nl> return ret ; <nl> } <nl> - if ( c -> http_persistent ) { <nl> + if ( c -> http_persistent && av_strstart ( seg -> url , " http ", NULL )) { <nl> v -> input_read_done = 1 ; <nl> } else { <nl> ff_format_io_close ( v -> parent , & v -> input );
int main ( int argc , char ** argv ) <nl> goto end ; <nl> } <nl> w_name = av_strtok ( print_format , "=", & buf ); <nl> + if (! w_name ) { <nl> + av_log ( NULL , AV_LOG_ERROR , <nl> + " No name specified for the output format \ n "); <nl> + ret = AVERROR ( EINVAL ); <nl> + goto end ; <nl> + } <nl> w_args = buf ; <nl>  <nl> if ( show_data_hash ) {
int Configure ( void ** ctxp , int argc , char * argv []) <nl> if ( argc > 1 ) <nl> { <nl> * ctxp = av_mallocz ( sizeof ( ContextInfo )); <nl> - if ( ctxp != NULL && argc > 1 ) <nl> + if ( * ctxp != NULL && argc > 1 ) <nl> { <nl> ContextInfo * info = ( ContextInfo *)* ctxp ; <nl> info -> rw = rwpipe_open ( argc - 1 , & argv [ 1 ] );
static av_cold int truemotion1_decode_init ( AVCodecContext * avctx ) <nl> /* there is a vertical predictor for each pixel in a line ; each vertical <nl> * predictor is 0 to start with */ <nl> av_fast_malloc (& s -> vert_pred , & s -> vert_pred_size , s -> avctx -> width * sizeof ( unsigned int )); <nl> - if (! s -> vert_pred ) <nl> + if (! s -> vert_pred ) { <nl> + av_frame_free (& s -> frame ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int handle_packet ( MpegTSContext * ts , const uint8_t * packet ) <nl> return 0 ; <nl>  <nl> pos = avio_tell ( ts -> stream -> pb ); <nl> - av_assert0 ( pos >= TS_PACKET_SIZE ); <nl> - ts -> pos47_full = pos - TS_PACKET_SIZE ; <nl> + if ( pos >= 0 ) { <nl> + av_assert0 ( pos >= TS_PACKET_SIZE ); <nl> + ts -> pos47_full = pos - TS_PACKET_SIZE ; <nl> + } <nl>  <nl> if ( tss -> type == MPEGTS_SECTION ) { <nl> if ( is_start ) {
static void apply_independent_coupling_fixed ( AACContext * ac , <nl> else { <nl> for ( i = 0 ; i < len ; i ++) { <nl> tmp = ( int )((( int64_t ) src [ i ] * c + ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ i ] += tmp << shift ; <nl> + dest [ i ] += tmp * ( 1 << shift ); <nl> } <nl> } <nl> }
static int v410_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 * avctx -> height * avctx -> width ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> pic -> reference = 0 ; <nl>  <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) {
static int config_props ( AVFilterLink * inlink ) <nl> double res ; <nl>  <nl> /* create the parsed expression */ <nl> + av_expr_free ( s -> comp_expr [ comp ]); <nl> + s -> comp_expr [ comp ] = NULL ; <nl> ret = av_expr_parse (& s -> comp_expr [ comp ], s -> comp_expr_str [ comp ], <nl> var_names , funcs1_names , funcs1 , NULL , NULL , 0 , ctx ); <nl> if ( ret < 0 ) {
static int matroska_parse_rm_audio ( MatroskaDemuxContext * matroska , <nl> } <nl> memcpy ( track -> audio . buf + y * w , data , w ); <nl> } else { <nl> - if ( size < sps * w / sps || h <= 0 ) { <nl> + if ( size < sps * w / sps || h <= 0 || w % sps ) { <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Corrupt generic RM - style audio packet size \ n "); <nl> return AVERROR_INVALIDDATA ;
static int matroska_decode_buffer ( uint8_t ** buf , int * buf_size , <nl> int result = 0 ; <nl> int olen ; <nl>  <nl> + if ( pkt_size >= 10000000 ) <nl> + return - 1 ; <nl> + <nl> switch ( encodings [ 0 ]. compression . algo ) { <nl> case MATROSKA_TRACK_ENCODING_COMP_HEADERSTRIP : <nl> return encodings [ 0 ]. compression . settings . size ;
av_cold int ff_ivi_decode_close ( AVCodecContext * avctx ) <nl> if ( ctx -> mb_vlc . cust_tab . table ) <nl> ff_free_vlc (& ctx -> mb_vlc . cust_tab ); <nl>  <nl> + if ( ctx -> blk_vlc . cust_tab . table ) <nl> + ff_free_vlc (& ctx -> blk_vlc . cust_tab ); <nl> + <nl> av_frame_free (& ctx -> p_frame ); <nl>  <nl> return 0 ;
static int read_interval_packets ( WriterContext * w , AVFormatContext * fmt_ctx , <nl> } <nl>  <nl> frame = av_frame_alloc (); <nl> + if (! frame ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> while (! av_read_frame ( fmt_ctx , & pkt )) { <nl> if ( selected_streams [ pkt . stream_index ]) { <nl> AVRational tb = fmt_ctx -> streams [ pkt . stream_index ]-> time_base ;
AVCodec ff_jpegls_encoder = { <nl> AV_PIX_FMT_GRAY8 , AV_PIX_FMT_GRAY16 , <nl> AV_PIX_FMT_NONE <nl> }, <nl> + . caps_internal = FF_CODEC_CAP_INIT_THREADSAFE | <nl> + FF_CODEC_CAP_INIT_CLEANUP , <nl> };
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> av_log ( avctx , AV_LOG_ERROR , " Custom quant matrix encountered !\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( band -> quant_mat > 21 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid quant matrix encountered !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> /* decode block huffman codebook */
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> sub_type = avio_r8 ( s -> pb ); <nl> type = avio_r8 ( s -> pb ); <nl> size = avio_rl16 ( s -> pb ); <nl> + if ( size < 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> avs -> remaining_frame_size -= size ; <nl>  <nl> switch ( type ) {
static void gif_copy_img_rect ( const uint32_t * src , uint32_t * dst , <nl> const uint32_t * src_px , * src_pr , <nl> * src_py = src + y_start , <nl> * dst_py = dst + y_start ; <nl> - const uint32_t * src_pb = src_py + t * linesize ; <nl> + const uint32_t * src_pb = src_py + h * linesize ; <nl> uint32_t * dst_px ; <nl>  <nl> for (; src_py < src_pb ; src_py += linesize , dst_py += linesize ) {
static void update_stream_timings ( AVFormatContext * ic ) <nl> end_time1 = av_rescale_q_rnd ( st -> duration , st -> time_base , <nl> AV_TIME_BASE_Q , <nl> AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX ); <nl> - if ( end_time1 != AV_NOPTS_VALUE ) { <nl> + if ( end_time1 != AV_NOPTS_VALUE && start_time1 <= INT64_MAX - end_time1 ) { <nl> end_time1 += start_time1 ; <nl> end_time = FFMAX ( end_time , end_time1 ); <nl> }
static void destroy_buffers ( SANMVideoContext * ctx ) <nl> ctx -> frm0_size = <nl> ctx -> frm1_size = <nl> ctx -> frm2_size = 0 ; <nl> + init_sizes ( ctx , 0 , 0 ); <nl> } <nl>  <nl> static av_cold int init_buffers ( SANMVideoContext * ctx )
static int parse_video_info ( AVIOContext * pb , AVStream * st ) <nl> st -> codecpar -> codec_id = ff_codec_get_id ( ff_codec_bmp_tags , tag ); <nl> size_bmp = FFMAX ( size_asf , size_bmp ); <nl>  <nl> - if ( size_bmp > BMP_HEADER_SIZE ) { <nl> + if ( size_bmp > BMP_HEADER_SIZE && <nl> + size_bmp < INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) { <nl> int ret ; <nl> st -> codecpar -> extradata_size = size_bmp - BMP_HEADER_SIZE ; <nl> if (!( st -> codecpar -> extradata = av_malloc ( st -> codecpar -> extradata_size +
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> } <nl>  <nl> if ( S ) { <nl> - S <<= s -> float_shift ; <nl> + S *= 1 << s -> float_shift ; <nl> sign = S < 0 ; <nl> if ( sign ) <nl> S = - S ;
int ff_h264_frame_start ( H264Context * h ) <nl>  <nl> if (( ret = alloc_picture ( h , pic )) < 0 ) <nl> return ret ; <nl> - if (! h -> sync && ! h -> avctx -> hwaccel ) <nl> + if (! h -> sync && ! h -> avctx -> hwaccel && <nl> + !( h -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU )) <nl> avpriv_color_frame (& pic -> f , c ); <nl>  <nl> h -> cur_pic_ptr = pic ;
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> switch ( type ) { <nl> case AVS_PALETTE : <nl> + if ( size - 4 > sizeof ( palette )) <nl> + return AVERROR_INVALIDDATA ; <nl> ret = avio_read ( s -> pb , palette , size - 4 ); <nl> if ( ret < size - 4 ) <nl> return AVERROR ( EIO );
static int filter_packet ( void * log_ctx , AVPacket * pkt , <nl> AVFormatContext * fmt_ctx , AVBitStreamFilterContext * bsf_ctx ) <nl> { <nl> AVCodecContext * enc_ctx = fmt_ctx -> streams [ pkt -> stream_index ]-> codec ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> while ( bsf_ctx ) { <nl> AVPacket new_pkt = * pkt ;
static int decode_subframe_fixed ( FLACContext * s , int32_t * decoded , <nl> int pred_order , int bps ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> - int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl> + unsigned av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ); <nl> + int i ; <nl> int ret ; <nl>  <nl> /* warm up samples */
static int gxf_write_header ( AVFormatContext * s ) <nl> if ( ff_audio_interleave_init ( s , GXF_samples_per_frame , ( AVRational ){ 1 , 48000 }) < 0 ) <nl> return - 1 ; <nl>  <nl> - if ( tcr ) <nl> + if ( tcr && vsc ) <nl> gxf_init_timecode ( s , & gxf -> tc , tcr -> value , vsc -> fields ); <nl>  <nl> gxf_init_timecode_track (& gxf -> timecode_track , vsc );
static void opt_output_file ( const char * filename ) <nl>  <nl> audio_enc -> bit_rate = audio_bit_rate ; <nl> audio_enc -> sample_rate = audio_sample_rate ; <nl> + audio_enc -> strict_std_compliance = strict ; <nl> /* For audio codecs other than AC3 we limit */ <nl> /* the number of coded channels to stereo */ <nl> if ( audio_channels > 2 && codec_id != CODEC_ID_AC3 ) {
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl>  <nl> av_freep (& h -> rbsp_buffer [ 0 ]); <nl> av_freep (& h -> rbsp_buffer [ 1 ]); <nl> + ff_h264_unref_picture ( h , & h -> last_pic_for_ec ); <nl> memcpy ( h , h1 , offsetof ( H264Context , intra_pcm_ptr )); <nl> memcpy (& h -> cabac , & h1 -> cabac , <nl> sizeof ( H264Context ) - offsetof ( H264Context , cabac ));
static int16_t g726_decode ( G726Context * c , int I ) <nl> c -> se += mult ( i2f ( c -> a [ i ] >> 2 , & f ), & c -> sr [ i ]); <nl> c -> se >>= 1 ; <nl>  <nl> - return av_clip ( re_signal << 2 , - 0xffff , 0xffff ); <nl> + return av_clip ( re_signal * 4 , - 0xffff , 0xffff ); <nl> } <nl>  <nl> static av_cold int g726_reset ( G726Context * c )
static int fraps2_decode_plane ( FrapsContext * s , uint8_t * dst , int stride , int w , <nl> else if ( Uoff ) dst [ i ] += 0x80 ; <nl> } <nl> dst += stride ; <nl> + if ( get_bits_left (& gb ) < 0 ) <nl> + return - 1 ; <nl> } <nl> free_vlc (& vlc ); <nl> return 0 ;
fixup_vorbis_headers ( AVFormatContext * as , struct oggvorbis_private * priv , <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> memcpy (& ptr [ offset ], priv -> packet [ i ], priv -> len [ i ]); <nl> offset += priv -> len [ i ]; <nl> + av_freep (& priv -> packet [ i ]); <nl> } <nl> * buf = av_realloc (* buf , offset + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> return offset ;
static int write_adaptation_set ( AVFormatContext * s , int as_index ) <nl> ret = write_representation ( s , s -> streams [ as -> streams [ i ]], <nl> representation_id , ! width_in_as , <nl> ! height_in_as , ! sample_rate_in_as ); <nl> - if ( ret ) return ret ; <nl> av_free ( representation_id ); <nl> + if ( ret ) return ret ; <nl> } <nl> avio_printf ( s -> pb , "</ AdaptationSet >\ n "); <nl> return 0 ;
int ff_dca_xll_decode_audio ( DCAContext * s , AVFrame * frame ) <nl> } <nl> for ( i = 0 ; i < chset -> channels ; i ++) { <nl> int param_index = params -> seg_type ? 0 : i ; <nl> - int bits = params -> pancABIT0 [ param_index ]; <nl> int part0 = params -> nSamplPart0 [ param_index ]; <nl> + int bits = part0 ? params -> pancABIT0 [ param_index ] : 0 ; <nl> int * sample_buf = s -> xll_sample_buf + <nl> ( in_channel + i ) * s -> xll_smpl_in_seg ; <nl> 
static int encode_block ( SVQ1Context * s , uint8_t * src , uint8_t * ref , uint8_t * dec <nl> } <nl>  <nl> best_count = 0 ; <nl> - best_score -= (( block_sum [ 0 ]* block_sum [ 0 ])>>( level + 3 )); <nl> + best_score -= ( int )((( unsigned ) block_sum [ 0 ]* block_sum [ 0 ])>>( level + 3 )); <nl> best_mean = ( block_sum [ 0 ] + ( size >> 1 )) >> ( level + 3 ); <nl>  <nl> if ( level < 4 ){
static int vorbis_parse ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl>  <nl> if (! s -> vp && avctx -> extradata && avctx -> extradata_size ) { <nl> s -> vp = av_vorbis_parse_init ( avctx -> extradata , avctx -> extradata_size ); <nl> - if (! s -> vp ) <nl> - goto end ; <nl> } <nl> + if (! s -> vp ) <nl> + goto end ; <nl>  <nl> if (( duration = av_vorbis_parse_frame ( s -> vp , buf , buf_size )) >= 0 ) <nl> s1 -> duration = duration ;
static double bessel ( double x ){ <nl> lastv = v ; <nl> t *= x * inv [ i ]; <nl> v += t ; <nl> + av_assert2 ( i < 99 ); <nl> } <nl> return v ; <nl> }
av_cold struct FFPsyPreprocessContext * ff_psy_preprocess_init ( AVCodecContext * av <nl> if (! cutoff_coeff && avctx -> codec_id == AV_CODEC_ID_AAC ) <nl> cutoff_coeff = 2 . 0 * AAC_CUTOFF ( avctx ) / avctx -> sample_rate ; <nl>  <nl> - if ( cutoff_coeff ) <nl> + if ( cutoff_coeff && cutoff_coeff < 0 . 98 ) <nl> ctx -> fcoeffs = ff_iir_filter_init_coeffs ( avctx , FF_FILTER_TYPE_BUTTERWORTH , <nl> FF_FILTER_MODE_LOWPASS , FILT_ORDER , <nl> cutoff_coeff , 0 . 0 , 0 . 0 );
redo_frame : <nl> || !( height >>( s -> chroma_v_shift + s -> spatial_decomposition_count ))) <nl> s -> spatial_decomposition_count --; <nl>  <nl> + if ( s -> spatial_decomposition_count <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Resolution too low \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> s -> m . pict_type = pic -> pict_type ; <nl> s -> qbias = pic -> pict_type == AV_PICTURE_TYPE_P ? 2 : 0 ; <nl> 
static int get_cookies ( HTTPContext * s , char ** cookies , const char * path , <nl> } <nl>  <nl> done_cookie : <nl> - av_free ( cdomain ); <nl> - av_free ( cpath ); <nl> - av_free ( cvalue ); <nl> + av_freep (& cdomain ); <nl> + av_freep (& cpath ); <nl> + av_freep (& cvalue ); <nl> if ( ret < 0 ) { <nl> if (* cookies ) av_freep ( cookies ); <nl> av_free ( cset_cookies );
typedef struct Jpeg2000Component { <nl> /* misc tools */ <nl> static inline int ff_jpeg2000_ceildivpow2 ( int a , int b ) <nl> { <nl> - return -((( int64_t )(- a )) >> b ); <nl> + return -((-( int64_t ) a ) >> b ); <nl> } <nl>  <nl> static inline int ff_jpeg2000_ceildiv ( int a , int b )
static int set_params ( AVFilterContext * ctx , const char * params ) <nl> Frei0rContext * frei0r = ctx -> priv ; <nl> int i ; <nl>  <nl> + if (! params ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < frei0r -> plugin_info . num_params ; i ++) { <nl> f0r_param_info_t info ; <nl> char * param ;
int ff_mpeg_update_thread_context ( AVCodecContext * dst , <nl> s -> input_picture_number = s1 -> input_picture_number ; <nl>  <nl> av_assert0 (! s -> picture || s -> picture != s1 -> picture ); <nl> + if ( s -> picture ) <nl> for ( i = 0 ; i < MAX_PICTURE_COUNT ; i ++) { <nl> ff_mpeg_unref_picture ( s , & s -> picture [ i ]); <nl> if ( s1 -> picture [ i ]. f . data [ 0 ] &&
static int mov_text_decode_close ( AVCodecContext * avctx ) <nl> { <nl> MovTextContext * m = avctx -> priv_data ; <nl> mov_text_cleanup_ftab ( m ); <nl> + mov_text_cleanup ( m ); <nl> return 0 ; <nl> } <nl> 
static void filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * samplesref ) <nl> for ( i = 0 ; i < ctx -> nb_outputs ; i ++) <nl> ff_filter_samples ( inlink -> dst -> outputs [ i ], <nl> avfilter_ref_buffer ( samplesref , ~ AV_PERM_WRITE )); <nl> + avfilter_unref_buffer ( samplesref ); <nl> } <nl>  <nl> AVFilter avfilter_af_asplit = {
static int decode_header ( EXRContext * s ) <nl> channel -> xsub = xsub ; <nl> channel -> ysub = ysub ; <nl>  <nl> - s -> current_channel_offset += 1 << current_pixel_type ; <nl> + if ( current_pixel_type == EXR_HALF ) { <nl> + s -> current_channel_offset += 2 ; <nl> + } else {/* Float or UINT32 */ <nl> + s -> current_channel_offset += 4 ; <nl> + } <nl> } <nl>  <nl> /* Check if all channels are set with an offset or if the channels
static inline int mdec_decode_block_intra ( MDECContext * a , int16_t * block , int n ) <nl> if ( diff >= 0xffff ) <nl> return AVERROR_INVALIDDATA ; <nl> a -> last_dc [ component ] += diff ; <nl> - block [ 0 ] = a -> last_dc [ component ] << 3 ; <nl> + block [ 0 ] = a -> last_dc [ component ] * ( 1 << 3 ); <nl> } <nl>  <nl> i = 0 ;
void av_frame_unref ( AVFrame * frame ) <nl> { <nl> int i ; <nl>  <nl> + if (! frame ) <nl> + return ; <nl> + <nl> wipe_side_data ( frame ); <nl>  <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( frame -> buf ); i ++)
static int matroska_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> ret = matroska_parse_cluster ( matroska ); <nl> } <nl>  <nl> - if ( ret == AVERROR_INVALIDDATA ) { <nl> + if ( ret == AVERROR_INVALIDDATA && pkt -> data ) { <nl> pkt -> flags |= AV_PKT_FLAG_CORRUPT ; <nl> return 0 ; <nl> }
static av_cold int bktr_init ( const char * video_device , int width , int height , <nl> long ioctl_frequency ; <nl> char * arg ; <nl> int c ; <nl> - struct sigaction act = { 0 }, old ; <nl> + struct sigaction act = { { 0 } }, old ; <nl>  <nl> if ( idev < 0 || idev > 4 ) <nl> {
static int encode_audio_frame ( AVFormatContext * s , OutputStream * ost , <nl> pkt . data = NULL ; <nl> pkt . size = 0 ; <nl>  <nl> - if ( buf ) { <nl> + if ( buf && buf_size ) { <nl> if (! ost -> output_frame ) { <nl> ost -> output_frame = avcodec_alloc_frame (); <nl> if (! ost -> output_frame ) {
static void start_children ( FFServerStream * feed ) <nl> av_free ( pathname ); <nl> _exit ( 1 ); <nl> } <nl> + av_free ( pathname ); <nl> } <nl>  <nl> /* open a listening socket */
static av_cold int adpcm_encode_init ( AVCodecContext * avctx ) <nl> goto error ; <nl> } <nl>  <nl> - avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (!( avctx -> coded_frame = avcodec_alloc_frame ())) <nl> + goto error ; <nl>  <nl> return 0 ; <nl> error :
static inline int l3_unscale ( int value , int exponent ) <nl> if ( e < 1 ) <nl> av_log ( NULL , AV_LOG_WARNING , " l3_unscale : e is % d \ n ", e ); <nl> # endif <nl> - if ( e > 31 ) <nl> + if ( e > ( SUINT ) 31 ) <nl> return 0 ; <nl> m = ( m + ( 1 << ( e - 1 ))) >> e ; <nl> 
static const AVOption options [] = { <nl> { " safe ", " enable safe mode ", <nl> OFFSET ( safe ), AV_OPT_TYPE_INT , {. i64 = - 1 }, - 1 , 1 , DEC }, <nl> { " auto_convert ", " automatically convert bitstream format ", <nl> - OFFSET ( auto_convert ), AV_OPT_TYPE_INT , {. i64 = 0 }, 0 , 1 , DEC }, <nl> + OFFSET ( auto_convert ), AV_OPT_TYPE_INT , {. i64 = 1 }, 0 , 1 , DEC }, <nl> { NULL } <nl> }; <nl> 
 <nl> static av_cold int encode_init ( AVCodecContext * avctx ) <nl> { <nl> + if ( avctx -> width > 65535 || avctx -> height > 65535 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " Unsupported resolution % dx % d .\ n ", avctx -> width , avctx -> height ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> avctx -> coded_frame = av_frame_alloc (); <nl> if (! avctx -> coded_frame ) <nl> return AVERROR ( ENOMEM );
static int guess_ni_flag ( AVFormatContext * s ){ <nl> if ( last_start > first_end ) <nl> return 1 ; <nl> idx = av_mallocz ( sizeof (* idx ) * s -> nb_streams ); <nl> - for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1 ) { <nl> + for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1LU ) { <nl> int64_t max_dts = INT64_MIN / 2 , min_dts = INT64_MAX / 2 ; <nl> min_pos = INT64_MAX ; <nl> 
static av_always_inline void RENAME ( decode_line )( FFV1Context * s , int w , <nl> } <nl>  <nl> if ( sign ) <nl> - diff = - diff ; <nl> + diff = -( unsigned ) diff ; <nl>  <nl> sample [ 1 ][ x ] = av_mod_uintp2 ( RENAME ( predict )( sample [ 1 ] + x , sample [ 0 ] + x ) + ( SUINT ) diff , bits ); <nl> }
int main ( int argc , char * argv []) <nl> /* keep ftyp atom */ <nl> if ( atom_type == FTYP_ATOM ) { <nl> ftyp_atom_size = atom_size ; <nl> + free ( ftyp_atom ); <nl> ftyp_atom = malloc ( ftyp_atom_size ); <nl> if (! ftyp_atom ) { <nl> printf (" could not allocate %" PRIu64 " byte for ftyp atom \ n ",
int ff_h264_decode_sei ( H264Context * h ){ <nl> size += show_bits (& s -> gb , 8 ); <nl> } while ( get_bits (& s -> gb , 8 ) == 255 ); <nl>  <nl> + if ( s -> avctx -> debug & FF_DEBUG_STARTCODE ) <nl> + av_log ( h -> s . avctx , AV_LOG_DEBUG , " SEI % d len :% d \ n ", type , size ); <nl> + <nl> switch ( type ){ <nl> case SEI_TYPE_PIC_TIMING : // Picture timing SEI <nl> if ( decode_picture_timing ( h ) < 0 )
int av_stream_add_side_data ( AVStream * st , enum AVPacketSideDataType type , <nl> } <nl> } <nl>  <nl> - tmp = av_realloc_array ( st -> side_data , st -> nb_side_data + 1 , sizeof (* tmp )); <nl> + if (( unsigned ) st -> nb_side_data + 1 >= INT_MAX / sizeof (* st -> side_data )) <nl> + return AVERROR ( ERANGE ); <nl> + <nl> + tmp = av_realloc ( st -> side_data , st -> nb_side_data + 1 * sizeof (* tmp )); <nl> if (! tmp ) { <nl> return AVERROR ( ENOMEM ); <nl> }
static void clear_context ( MpegEncContext * s ) <nl>  <nl> s -> parse_context . buffer = NULL ; <nl> s -> parse_context . buffer_size = 0 ; <nl> + s -> parse_context . overread = 0 ; <nl> s -> bitstream_buffer = NULL ; <nl> s -> allocated_bitstream_buffer_size = 0 ; <nl> s -> picture = NULL ;
static int imc_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> IMCContext * q = avctx -> priv_data ; <nl>  <nl> - LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 ]); <nl> + LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 + FF_INPUT_BUFFER_PADDING_SIZE / 2 ]); <nl>  <nl> if ( buf_size < IMC_BLOCK_SIZE * avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame too small !\ n ");
int ff_read_riff_info ( AVFormatContext * s , int64_t size ) <nl> AV_WL32 ( key , chunk_code ); <nl>  <nl> if ( avio_read ( pb , value , chunk_size ) != chunk_size ) { <nl> - av_freep ( key ); <nl> - av_freep ( value ); <nl> + av_free ( value ); <nl> av_log ( s , AV_LOG_ERROR , " premature end of file while reading INFO tag \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
int av_samples_get_buffer_size ( int * linesize , int nb_channels , int nb_samples , <nl>  <nl> /* auto - select alignment if not specified */ <nl> if (! align ) { <nl> + if ( nb_samples > INT_MAX - 31 ) <nl> + return AVERROR ( EINVAL ); <nl> align = 1 ; <nl> nb_samples = FFALIGN ( nb_samples , 32 ); <nl> }
static ResampleContext * resample_init ( ResampleContext * c , int out_rate , int in_r <nl> av_assert0 ( 0 ); <nl> } <nl>  <nl> + if ( filter_size / factor > INT32_MAX / 256 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " Filter length too large \ n "); <nl> + goto error ; <nl> + } <nl> + <nl> c -> phase_shift = phase_shift ; <nl> c -> phase_mask = phase_count - 1 ; <nl> c -> linear = linear ;
static int submit_packet ( PerThreadContext * p , AVPacket * avpkt ) <nl> } <nl>  <nl> fctx -> prev_thread = p ; <nl> + fctx -> next_decoding ++; <nl>  <nl> return 0 ; <nl> } <nl> int ff_thread_decode_frame ( AVCodecContext * avctx , <nl> err = submit_packet ( p , avpkt ); <nl> if ( err ) return err ; <nl>  <nl> - fctx -> next_decoding ++; <nl> - <nl> /* <nl> * If we ' re still receiving the initial packets , don ' t return a frame . <nl> */
int ff_mpeg1_find_frame_end ( ParseContext * pc , const uint8_t * buf , int buf_size , <nl> pc -> frame_start_found = 4 ; <nl> } <nl> if ( state == SEQ_END_CODE ) { <nl> + pc -> frame_start_found = 0 ; <nl> pc -> state =- 1 ; <nl> return i + 1 ; <nl> }
int avpriv_adx_decode_header ( AVCodecContext * avctx , const uint8_t * buf , <nl>  <nl> /* channels */ <nl> avctx -> channels = buf [ 7 ]; <nl> - if ( avctx -> channels > 2 ) <nl> + if ( avctx -> channels <= 0 || avctx -> channels > 2 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> /* sample rate */
reload : <nl> /* If we need to reload the playlist again below ( if <nl> * there ' s still no more segments ), switch to a reload <nl> * interval of half the target duration . */ <nl> - reload_interval = v -> target_duration * 500000 ; <nl> + reload_interval = v -> target_duration * 500000LL ; <nl> } <nl> if ( v -> cur_seq_no < v -> start_seq_no ) { <nl> av_log ( NULL , AV_LOG_WARNING ,
static int read_header ( ShortenContext * s ) <nl> s -> blocksize = blocksize ; <nl>  <nl> maxnlpc = get_uint ( s , LPCQSIZE ); <nl> + if ( maxnlpc > 1024U ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " maxnlpc is : % d \ n ", maxnlpc ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> nmean = get_uint ( s , 0 ); <nl>  <nl> skip_bytes = get_uint ( s , NSKIPSIZE );
static void decode_nal_sei_decoded_picture_hash ( HEVCContext * s ) <nl> static void decode_nal_sei_frame_packing_arrangement ( HEVCContext * s ) <nl> { <nl> GetBitContext * gb = & s -> HEVClc -> gb ; <nl> - int cancel , type , quincunx , content ; <nl> + int cancel ; <nl> + int quincunx = 0 ; <nl> + int content = - 1 ; <nl> + int type = - 1 ; <nl>  <nl> get_ue_golomb ( gb ); // frame_packing_arrangement_id <nl> cancel = get_bits1 ( gb ); // frame_packing_cancel_flag
static void mp_decode_frame_helper ( MotionPixelsContext * mp , GetBitContext * gb ) <nl> YuvPixel p ; <nl> int y , y0 ; <nl>  <nl> + av_assert1 ( mp -> changes_map [ 0 ]); <nl> + <nl> for ( y = 0 ; y < mp -> avctx -> height ; ++ y ) { <nl> if ( mp -> changes_map [ y * mp -> avctx -> width ] != 0 ) { <nl> memset ( mp -> gradient_scale , 1 , sizeof ( mp -> gradient_scale ));
static int mov_read_close ( AVFormatContext * s ) <nl> av_freep (& sc -> rap_group ); <nl> av_freep (& sc -> display_matrix ); <nl>  <nl> - for ( j = 0 ; j < sc -> stsd_count ; j ++) <nl> - av_free ( sc -> extradata [ j ]); <nl> + if ( sc -> extradata ) <nl> + for ( j = 0 ; j < sc -> stsd_count ; j ++) <nl> + av_free ( sc -> extradata [ j ]); <nl> av_freep (& sc -> extradata ); <nl> av_freep (& sc -> extradata_size ); <nl> 
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> ThreadFrame tframe = { . f = frame }; <nl> WavpackFrameContext * s ; <nl> GetByteContext gb ; <nl> - void * samples_l , * samples_r ; <nl> + void * samples_l = NULL , * samples_r = NULL ; <nl> int ret ; <nl> int got_terms = 0 , got_weights = 0 , got_samples = 0 , <nl> got_entropy = 0 , got_bs = 0 , got_float = 0 , got_hybrid = 0 ;
static int get_cookies ( HTTPContext * s , char ** cookies , const char * path , <nl> snprintf (* cookies , str_size , "% s ; % s =% s ", tmp , cookie_entry -> key , cookie_entry -> value ); <nl> av_free ( tmp ); <nl> } <nl> + av_dict_free (& cookie_params ); <nl> } <nl>  <nl> av_free ( set_cookies );
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> ADD_METADATA ( count , " ModelTiepointTag ", NULL ); <nl> break ; <nl> case TIFF_GEO_KEY_DIRECTORY : <nl> + if ( s -> geotag_count ) { <nl> + avpriv_request_sample ( s -> avctx , " Multiple geo key directories \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> ADD_METADATA ( 1 , " GeoTIFF_Version ", NULL ); <nl> ADD_METADATA ( 2 , " GeoTIFF_Key_Revision ", "."); <nl> s -> geotag_count = ff_tget_short (& s -> gb , s -> le );
static AVIOContext * wtvfile_open_sector ( int first_sector , uint64_t length , int <nl> return NULL ; <nl> } <nl>  <nl> - if ( wf -> sectors [ wf -> nb_sectors - 1 ] << WTV_SECTOR_BITS > avio_tell ( s -> pb )) <nl> + if (( int64_t ) wf -> sectors [ wf -> nb_sectors - 1 ] << WTV_SECTOR_BITS > avio_tell ( s -> pb )) <nl> av_log ( s , AV_LOG_WARNING , " truncated file \ n "); <nl>  <nl> /* check length */
static void decode ( AVCodecContext * dec_ctx , AVFrame * frame , AVPacket * pkt , <nl>  <nl> /* the picture is allocated by the decoder . no need to <nl> free it */ <nl> - snprintf ( buf , sizeof ( buf ), filename , dec_ctx -> frame_number ); <nl> + snprintf ( buf , sizeof ( buf ), "% s -% d ", filename , dec_ctx -> frame_number ); <nl> pgm_save ( frame -> data [ 0 ], frame -> linesize [ 0 ], <nl> frame -> width , frame -> height , buf ); <nl> }
int av_packet_unpack_dictionary ( const uint8_t * data , int size , AVDictionary ** di <nl> const uint8_t * key = data ; <nl> const uint8_t * val = data + strlen ( key ) + 1 ; <nl>  <nl> - if ( val >= end ) <nl> + if ( val >= end || !* key ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> ret = av_dict_set ( dict , key , val , 0 );
static void process_client ( AVIOContext * client , const char * in_uri ) <nl> // may return empty string . <nl> if ( resource && strlen ( resource )) <nl> break ; <nl> + av_freep (& resource ); <nl> } <nl> if ( ret < 0 ) <nl> goto end ; <nl> end : <nl> avio_close ( client ); <nl> fprintf ( stderr , " Closing input \ n "); <nl> avio_close ( input ); <nl> + av_freep (& resource ); <nl> } <nl>  <nl> int main ( int argc , char ** argv )
static int parse_bsfs ( void * log_ctx , const char * bsfs_spec , <nl> AVBitStreamFilterContext ** bsfs ) <nl> { <nl> char * bsf_name , * buf , * saveptr ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> if (!( buf = av_strdup ( bsfs_spec ))) <nl> return AVERROR ( ENOMEM );
static int ipvideo_decode_block_opcode_0xA ( IpvideoContext * s , AVFrame * frame ) <nl> unsigned char P [ 8 ]; <nl> int flags = 0 ; <nl>  <nl> + if ( bytestream2_get_bytes_left (& s -> stream_ptr ) < 16 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too little data for opcode 0xA \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> bytestream2_get_buffer (& s -> stream_ptr , P , 4 ); <nl>  <nl> /* 4 - color encoding for each 4x4 quadrant , or 4 - color encoding on
int ff_flac_parse_picture ( AVFormatContext * s , uint8_t * buf , int buf_size ) <nl> if (!( data = av_buffer_alloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ))) { <nl> RETURN_ERROR ( AVERROR ( ENOMEM )); <nl> } <nl> + memset ( data -> data + len , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if ( avio_read ( pb , data -> data , len ) != len ) { <nl> av_log ( s , AV_LOG_ERROR , " Error reading attached picture data .\ n "); <nl> if ( s -> error_recognition & AV_EF_EXPLODE )
static int ea_read_header ( AVFormatContext * s , <nl> ea -> audio_codec = 0 ; <nl> return 1 ; <nl> } <nl> + if ( ea -> bytes <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of bytes per sample : % d \ n ", ea -> bytes ); <nl> + ea -> audio_codec = CODEC_ID_NONE ; <nl> + return 1 ; <nl> + } <nl>  <nl> /* initialize the audio decoder stream */ <nl> st = avformat_new_stream ( s , NULL );
static av_cold int dirac_decode_end ( AVCodecContext * avctx ) <nl> static inline int coeff_unpack_golomb ( GetBitContext * gb , int qfactor , int qoffset ) <nl> { <nl> int coeff = dirac_get_se_golomb ( gb ); <nl> - const int sign = FFSIGN ( coeff ); <nl> + const unsigned sign = FFSIGN ( coeff ); <nl> if ( coeff ) <nl> coeff = sign *(( sign * coeff * qfactor + qoffset ) >> 2 ); <nl> return coeff ;
static av_cold int wmv2_encode_init ( AVCodecContext * avctx ){ <nl> ff_wmv2_common_init ( w ); <nl>  <nl> avctx -> extradata_size = 4 ; <nl> - avctx -> extradata = av_mallocz ( avctx -> extradata_size + 10 ); <nl> + avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! avctx -> extradata ) <nl> + return AVERROR ( ENOMEM ); <nl> encode_ext_header ( w ); <nl>  <nl> return 0 ;
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , " Stream discovered after head already parsed \ n "); <nl> st = create_stream ( s , <nl> ( int []){ AVMEDIA_TYPE_VIDEO , AVMEDIA_TYPE_AUDIO , AVMEDIA_TYPE_DATA }[ stream_type ]); <nl> + if (! st ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> } <nl> av_dlog ( s , "% d % X % d \ n ", stream_type , flags , st -> discard );
static int convert_sub_to_old_ass_form ( AVSubtitle * sub , const AVPacket * pkt , AVR <nl> int ts_start , ts_duration = - 1 ; <nl> long int layer ; <nl>  <nl> - if ( rect -> type != SUBTITLE_ASS || ! strncmp ( rect -> ass , " Dialogue ", 10 )) <nl> + if ( rect -> type != SUBTITLE_ASS || ! strncmp ( rect -> ass , " Dialogue : ", 10 )) <nl> continue ; <nl>  <nl> av_bprint_clear (& buf );
static int bmp_decode_frame ( AVCodecContext * avctx , <nl> BiCompression comp ; <nl> unsigned int ihsize ; <nl> int i , j , n , linesize , ret ; <nl> - uint32_t rgb [ 3 ]; <nl> + uint32_t rgb [ 3 ] = { 0 }; <nl> uint32_t alpha = 0 ; <nl> uint8_t * ptr ; <nl> int dsize ;
static av_cold int twin_decode_init ( AVCodecContext * avctx ) <nl> default : avctx -> sample_rate = isampf * 1000 ; break ; <nl> } <nl>  <nl> - if ( avctx -> channels > CHANNELS_MAX ) { <nl> + if ( avctx -> channels <= 0 || avctx -> channels > CHANNELS_MAX ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unsupported number of channels : % i \ n ", <nl> avctx -> channels ); <nl> return - 1 ;
static int tqi_decode_frame ( AVCodecContext * avctx , <nl> for ( s -> mb_x = 0 ; s -> mb_x <( avctx -> width + 15 )/ 16 ; s -> mb_x ++) <nl> { <nl> if ( tqi_decode_mb ( s , t -> block ) < 0 ) <nl> - break ; <nl> + goto end ; <nl> tqi_idct_put ( t , t -> block ); <nl> } <nl> + end : <nl>  <nl> * data_size = sizeof ( AVFrame ); <nl> *( AVFrame *) data = t -> frame ;
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl>  <nl> /* first slice */ <nl> if ( si . start == 0 ) { <nl> - if ( s -> mb_num_left > 0 ) { <nl> + if ( s -> mb_num_left > 0 && s -> current_picture_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " New frame but still % d MB left .\ n ", <nl> s -> mb_num_left ); <nl> ff_er_frame_end (& s -> er );
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl>  <nl> if ( frames_hwctx -> texture ) <nl> ID3D11Texture2D_Release ( frames_hwctx -> texture ); <nl> + frames_hwctx -> texture = NULL ; <nl>  <nl> if ( s -> staging_texture ) <nl> ID3D11Texture2D_Release ( s -> staging_texture ); <nl> + s -> staging_texture = NULL ; <nl> } <nl>  <nl> static void free_texture ( void * opaque , uint8_t * data )
static inline int mpeg4_decode_block ( MpegEncContext * s , DCTELEM * block , <nl> int n , int coded , int intra , int rvlc ) <nl> { <nl> int level , i , last , run ; <nl> - int dc_pred_dir ; <nl> + int av_uninit ( dc_pred_dir ); <nl> RLTable * rl ; <nl> RL_VLC_ELEM * rl_vlc ; <nl> const uint8_t * scan_table ;
static attribute_align_arg void * frame_worker_thread ( void * arg ) <nl>  <nl> pthread_mutex_lock (& p -> progress_mutex ); <nl> for ( i = 0 ; i < MAX_BUFFERS ; i ++) <nl> - if ( p -> progress_used [ i ]) { <nl> + if ( p -> progress_used [ i ] && ( p -> got_frame || p -> result < 0 || avctx -> codec_id != CODEC_ID_H264 )) { <nl> p -> progress [ i ][ 0 ] = INT_MAX ; <nl> p -> progress [ i ][ 1 ] = INT_MAX ; <nl> }
static int decode_vol_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> else <nl> s -> quarter_sample = 0 ; <nl>  <nl> + if ( get_bits_left ( gb ) < 4 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " VOL Header truncated \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (! get_bits1 ( gb )) { <nl> int pos = get_bits_count ( gb ); <nl> int estimation_method = get_bits ( gb , 2 );
static int pcx_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> nplanes = buf [ 65 ]; <nl> bytes_per_scanline = nplanes * bytes_per_line ; <nl>  <nl> - if ( bytes_per_scanline < w * bits_per_pixel * nplanes / 8 || <nl> + if ( bytes_per_scanline < ( w * bits_per_pixel * nplanes + 7 ) / 8 || <nl> (! compressed && bytes_per_scanline > buf_size / h )) { <nl> av_log ( avctx , AV_LOG_ERROR , " PCX data is corrupted \ n "); <nl> return AVERROR_INVALIDDATA ;
static int sap_write_close ( AVFormatContext * s ) <nl> url_fclose ( rtpctx -> pb ); <nl> av_metadata_free (& rtpctx -> streams [ 0 ]-> metadata ); <nl> av_metadata_free (& rtpctx -> metadata ); <nl> + av_free ( rtpctx -> streams [ 0 ]-> info ); <nl> av_free ( rtpctx -> streams [ 0 ]); <nl> av_free ( rtpctx ); <nl> s -> streams [ i ]-> priv_data = NULL ;
static int vp8_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> VP8Context * s = avctx -> priv_data ; <nl> int ret , mb_x , mb_y , i , y , referenced ; <nl> enum AVDiscard skip_thresh ; <nl> - AVFrame * curframe ; <nl> + AVFrame * curframe = NULL ; <nl>  <nl> if (( ret = decode_frame_header ( s , avpkt -> data , avpkt -> size )) < 0 ) <nl> return ret ;
static int dvbsub_parse_page_segment ( AVCodecContext * avctx , <nl> region_id = * buf ++; <nl> buf += 1 ; <nl>  <nl> + display = ctx -> display_list ; <nl> + while ( display && display -> region_id != region_id ) { <nl> + display = display -> next ; <nl> + } <nl> + if ( display ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " duplicate region \ n "); <nl> + break ; <nl> + } <nl> + <nl> display = tmp_display_list ; <nl> tmp_ptr = & tmp_display_list ; <nl> 
int ff_wma_init ( AVCodecContext * avctx , int flags2 ) <nl> int coef_vlc_table ; <nl>  <nl> if ( avctx -> sample_rate <= 0 || avctx -> sample_rate > 50000 <nl> - || avctx -> channels <= 0 || avctx -> channels > 8 <nl> + || avctx -> channels <= 0 || avctx -> channels > 2 <nl> || avctx -> bit_rate <= 0 ) <nl> return - 1 ; <nl> 
enum OutputFormat { <nl> # define MPEG_BUF_SIZE ( 16 * 1024 ) <nl>  <nl> # define QMAT_SHIFT_MMX 16 <nl> -# define QMAT_SHIFT 22 <nl> +# define QMAT_SHIFT 21 <nl>  <nl> # define MAX_FCODE 7 <nl> # define MAX_MV 2048
static void pkt_dump_internal ( void * avcl , FILE * f , int level , const AVPacket * pk <nl> HEXDUMP_PRINT ("\ n "); <nl> HEXDUMP_PRINT (" size =% d \ n ", pkt -> size ); <nl> if ( dump_payload ) <nl> - av_hex_dump ( f , pkt -> data , pkt -> size ); <nl> + hex_dump_internal ( avcl , f , level , pkt -> data , pkt -> size ); <nl> } <nl>  <nl> void av_pkt_dump2 ( FILE * f , const AVPacket * pkt , int dump_payload , const AVStream * st )
int av_packet_ref ( AVPacket * dst , const AVPacket * src ) <nl> if ( ret < 0 ) <nl> goto fail ; <nl> memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl> - } else <nl> + } else { <nl> dst -> buf = av_buffer_ref ( src -> buf ); <nl> + if (! dst -> buf ) <nl> + goto fail ; <nl> + } <nl>  <nl> dst -> size = src -> size ; <nl> dst -> data = dst -> buf -> data ;
static int init ( AVFilterContext * ctx , const char * args ) <nl> eval -> class = & aevalsrc_class ; <nl> av_opt_set_defaults ( eval ); <nl>  <nl> + if (! args1 ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Argument is empty \ n "); <nl> + ret = args ? AVERROR ( ENOMEM ) : AVERROR ( EINVAL ); <nl> + goto end ; <nl> + } <nl> + <nl> /* parse expressions */ <nl> buf = args1 ; <nl> i = 0 ;
static int cinepak_decode_strip ( CinepakContext * s , <nl> while (( data + 4 ) <= eod ) { <nl> chunk_id = BE_16 (& data [ 0 ]); <nl> chunk_size = BE_16 (& data [ 2 ]) - 4 ; <nl> + if ( chunk_size < 0 ) <nl> + return - 1 ; <nl> + <nl> data += 4 ; <nl> chunk_size = (( data + chunk_size ) > eod ) ? ( eod - data ) : chunk_size ; <nl> 
static int rtsp_read_header ( AVFormatContext * s ) <nl> return ret ; <nl>  <nl> rt -> real_setup_cache = ! s -> nb_streams ? NULL : <nl> - av_mallocz ( 2 * s -> nb_streams * sizeof (* rt -> real_setup_cache )); <nl> + av_mallocz_array ( s -> nb_streams , 2 * sizeof (* rt -> real_setup_cache )); <nl> if (! rt -> real_setup_cache && s -> nb_streams ) <nl> return AVERROR ( ENOMEM ); <nl> rt -> real_setup = rt -> real_setup_cache + s -> nb_streams ;
static void pre_process_video_frame ( InputStream * ist , AVPicture * picture , void * <nl>  <nl> /* create temporary picture */ <nl> size = avpicture_get_size ( dec -> pix_fmt , dec -> width , dec -> height ); <nl> + if ( size < 0 ) <nl> + return ; <nl> buf = av_malloc ( size ); <nl> if (! buf ) <nl> return ;
static av_cold int decode_close_mp3on4 ( AVCodecContext * avctx ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < s -> frames ; i ++) <nl> - av_free ( s -> mp3decctx [ i ]); <nl> + av_freep (& s -> mp3decctx [ i ]); <nl>  <nl> return 0 ; <nl> }
static int ac3_parse_audio_block ( AC3DecodeContext * s , int blk ) <nl> /* coupling in use */ <nl> int cpl_begin_freq , cpl_end_freq ; <nl>  <nl> + if ( channel_mode < AC3_CHMODE_STEREO ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " coupling not allowed in mono or dual - mono \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> /* determine which channels are coupled */ <nl> for ( ch = 1 ; ch <= fbw_channels ; ch ++) <nl> s -> channel_in_cpl [ ch ] = get_bits1 ( gbc );
static int vqf_probe ( AVProbeData * probe_packet ) <nl> if (! memcmp ( probe_packet -> buf + 4 , " 00052200 ", 8 )) <nl> return AVPROBE_SCORE_MAX ; <nl>  <nl> + if ( AV_RL32 ( probe_packet -> buf + 12 ) > ( 1 << 27 )) <nl> + return AVPROBE_SCORE_EXTENSION / 2 ; <nl> + <nl> return AVPROBE_SCORE_EXTENSION ; <nl> } <nl> 
AVFormatContext * avformat_alloc_context ( void ) <nl> return NULL ; <nl> } <nl> ic -> internal -> offset = AV_NOPTS_VALUE ; <nl> + ic -> internal -> raw_packet_buffer_remaining_size = RAW_PACKET_BUFFER_SIZE ; <nl>  <nl> return ic ; <nl> }
int ff_wma_init ( AVCodecContext * avctx , int flags2 ) <nl>  <nl> /* compute MDCT block size */ <nl> s -> frame_len_bits = ff_wma_get_frame_len_bits ( s -> sample_rate , s -> version , 0 ); <nl> + s -> next_block_len_bits = s -> frame_len_bits ; <nl> + s -> prev_block_len_bits = s -> frame_len_bits ; <nl> + s -> block_len_bits = s -> frame_len_bits ; <nl>  <nl> s -> frame_len = 1 << s -> frame_len_bits ; <nl> if ( s -> use_variable_block_len ) {
static int asf_read_close ( AVFormatContext * s ) <nl> av_dict_free (& asf -> asf_sd [ i ]. asf_met ); <nl> } <nl>  <nl> + asf -> nb_streams = 0 ; <nl> return 0 ; <nl> } <nl> 
int ff_http_do_new_request ( URLContext * h , const char * uri ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if ( s -> willclose ) <nl> + return AVERROR_EOF ; <nl> + <nl> s -> end_chunked_post = 0 ; <nl> s -> chunkend = 0 ; <nl> s -> off = 0 ;
static int draw_text ( AVFilterContext * ctx , AVFilterBufferRef * picref , <nl> if ( dtext -> tc_opt_string ) { <nl> char tcbuf [ AV_TIMECODE_STR_SIZE ]; <nl> av_timecode_make_string (& dtext -> tc , tcbuf , dtext -> frame_id ++); <nl> + av_free ( buf ); <nl> buf = av_asprintf ("% s % s ", dtext -> text , tcbuf ); <nl> } <nl> 
static int g2m_init_buffers ( G2MContext * c ) <nl> if (! c -> synth_tile || ! c -> jpeg_tile || <nl> c -> old_tile_w < c -> tile_width || <nl> c -> old_tile_h < c -> tile_height ) { <nl> - c -> tile_stride = FFALIGN ( c -> tile_width * 3 , 16 ); <nl> + c -> tile_stride = FFALIGN ( c -> tile_width , 16 ) * 3 ; <nl> aligned_height = FFALIGN ( c -> tile_height , 16 ); <nl> av_free ( c -> synth_tile ); <nl> av_free ( c -> jpeg_tile );
static inline void codeblock ( DiracContext * s , SubBand * b , <nl> } \ <nl>  <nl> INTRA_DC_PRED ( 8 , int16_t ) <nl> - INTRA_DC_PRED ( 10 , int32_t ) <nl> + INTRA_DC_PRED ( 10 , uint32_t ) <nl>  <nl> /** <nl> * Dirac Specification ->
static int oma_read_seek ( struct AVFormatContext * s , <nl> int stream_index , int64_t timestamp , int flags ) <nl> { <nl> OMAContext * oc = s -> priv_data ; <nl> - int err = ff_pcm_read_seek ( s , stream_index , timestamp , flags ); <nl> + int64_t err = ff_pcm_read_seek ( s , stream_index , timestamp , flags ); <nl>  <nl> if (! oc -> encrypted ) <nl> return err ;
static int mov_write_audio_tag ( AVFormatContext * s , AVIOContext * pb , MOVMuxContex <nl> uint32_t tag = track -> tag ; <nl>  <nl> if ( track -> mode == MODE_MOV ) { <nl> - if ( track -> timescale > UINT16_MAX ) { <nl> + if ( track -> timescale > UINT16_MAX || ! track -> par -> channels ) { <nl> if ( mov_get_lpcm_flags ( track -> par -> codec_id )) <nl> tag = AV_RL32 (" lpcm "); <nl> version = 2 ;
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
static int sp5x_decode_frame ( AVCodecContext * avctx , <nl> recoded [ j ++] = 0xFF ; <nl> recoded [ j ++] = 0xD9 ; <nl>  <nl> - avctx -> flags &= ~ CODEC_FLAG_EMU_EDGE ; <nl> av_init_packet (& avpkt_recoded ); <nl> avpkt_recoded . data = recoded ; <nl> avpkt_recoded . size = j ; <nl> AVCodec ff_amv_decoder = { <nl> NULL , <nl> ff_mjpeg_decode_end , <nl> sp5x_decode_frame , <nl> - CODEC_CAP_DR1 , <nl> + 0 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" AMV Video "), <nl> };
static int asf_read_metadata_obj ( AVFormatContext * s , const GUIDParseTable * g ) <nl> if (( ret = process_metadata ( s , name , name_len , val_len , type , <nl> & asf -> asf_sd [ st_num ]. asf_met )) < 0 ) <nl> break ; <nl> - } <nl> + } else <nl> + av_freep (& name ); <nl> } <nl> } <nl> 
static int mpeg_decode_slice ( MpegEncContext * s , int mb_y , <nl> } <nl> } <nl> eos : // end of slice <nl> + if ( get_bits_left (& s -> gb ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> * buf += ( get_bits_count (& s -> gb )- 1 )/ 8 ; <nl> av_dlog ( s , " y % d % d % d % d \ n ", s -> resync_mb_x , s -> resync_mb_y , s -> mb_x , s -> mb_y ); <nl> return 0 ;
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> break ; <nl>  <nl> arraylen = avio_rb32 ( ioc ); <nl> + if ( arraylen >> 28 ) <nl> + break ; <nl> + <nl> /* <nl> * Expect only ' times ' or ' filepositions ' sub - arrays in other case refuse to use such metadata <nl> * for indexing
static int config_props ( AVFilterLink * outlink ) <nl>  <nl> if (( ret = sws_init_context (* s , NULL , NULL )) < 0 ) <nl> return ret ; <nl> + if (! scale -> interlaced ) <nl> + break ; <nl> } <nl> } <nl> 
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl>  <nl> if ( x >= w ) { <nl> av_log ( NULL , AV_LOG_ERROR , " run overflow \ n "); <nl> + av_assert0 ( x <= w ); <nl> return ; <nl> } <nl> 
int ff_h264_ref_picture ( H264Context * h , H264Picture * dst , H264Picture * src ) <nl> dst -> poc = src -> poc ; <nl> dst -> frame_num = src -> frame_num ; <nl> dst -> mmco_reset = src -> mmco_reset ; <nl> - dst -> pic_id = src -> pic_id ; <nl> dst -> long_ref = src -> long_ref ; <nl> dst -> mbaff = src -> mbaff ; <nl> dst -> field_picture = src -> field_picture ;
static int matroska_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> return - 1 ; <nl> matroska_execute_seekhead ( matroska ); <nl>  <nl> + if (! matroska -> time_scale ) <nl> + matroska -> time_scale = 1000000 ; <nl> if ( matroska -> duration ) <nl> matroska -> ctx -> duration = matroska -> duration * matroska -> time_scale <nl> * 1000 / AV_TIME_BASE ;
static int kalman_smoothen ( WMAVoiceContext * s , int pitch , <nl> float optimal_gain = 0 , dot ; <nl> const float * ptr = & in [- FFMAX ( s -> min_pitch_val , pitch - 3 )], <nl> * end = & in [- FFMIN ( s -> max_pitch_val , pitch + 3 )], <nl> - * best_hist_ptr ; <nl> + * best_hist_ptr = NULL ; <nl>  <nl> /* find best fitting point in history */ <nl> do {
static int decode_frame_mp3on4 ( AVCodecContext * avctx , void * data , <nl>  <nl> avpriv_mpegaudio_decode_header (( MPADecodeHeader *) m , header ); <nl>  <nl> - if ( ch + m -> nb_channels > avctx -> channels ) { <nl> + if ( ch + m -> nb_channels > avctx -> channels || <nl> + s -> coff [ fr ] + m -> nb_channels > avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame channel count exceeds codec " <nl> " channel count \ n "); <nl> return AVERROR_INVALIDDATA ;
static int read_gab2_sub ( AVFormatContext * s , AVStream * st , AVPacket * pkt ) <nl> if (! sub_demuxer ) <nl> goto error ; <nl>  <nl> + if ( strcmp ( sub_demuxer -> name , " srt ") && strcmp ( sub_demuxer -> name , " ass ")) <nl> + goto error ; <nl> + <nl> if (!( ast -> sub_ctx = avformat_alloc_context ())) <nl> goto error ; <nl> 
enum AVCodecID av_guess_codec ( AVOutputFormat * fmt , const char * short_name , <nl> enum AVMediaType type ) <nl> { <nl> if ( av_match_name (" segment ", fmt -> name ) || av_match_name (" ssegment ", fmt -> name )) { <nl> - fmt = av_guess_format ( NULL , filename , NULL ); <nl> + AVOutputFormat * fmt2 = av_guess_format ( NULL , filename , NULL ); <nl> + if ( fmt2 ) <nl> + fmt = fmt2 ; <nl> } <nl>  <nl> if ( type == AVMEDIA_TYPE_VIDEO ) {
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> s -> picture_structure = last_pic_structure ; <nl> s -> dropable = last_pic_dropable ; <nl> return AVERROR_INVALIDDATA ; <nl> + } else if (! s -> current_picture_ptr ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , <nl> + " unset current_picture_ptr on % d . slice \ n ", <nl> + h0 -> current_slice + 1 ); <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl> } else { <nl> /* Shorten frame num gaps so we don ' t have to allocate reference
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl>  <nl> if ( hdr -> substreamid == info -> num_ind_sub + 1 ) { <nl> // info -> num_ind_sub ++; <nl> - avpriv_request_sample ( track -> par , " Multiple independent substreams "); <nl> + avpriv_request_sample ( mov -> fc , " Multiple independent substreams "); <nl> ret = AVERROR_PATCHWELCOME ; <nl> goto end ; <nl> } else if ( hdr -> substreamid < info -> num_ind_sub ||
static int decode_cblk ( Jpeg2000DecoderContext * s , Jpeg2000CodingStyle * codsty , <nl> ff_mqc_initdec (& t1 -> mqc , cblk -> data ); <nl>  <nl> while ( passno --) { <nl> + if ( bpno < 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " bpno invalid \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> switch ( pass_t ) { <nl> case 0 : <nl> decode_sigpass ( t1 , width , height , bpno + 1 , bandpos ,
static int vdpau_vc1_start_frame ( AVCodecContext * avctx , <nl> else <nl> info -> picture_type = s -> pict_type - 1 + s -> pict_type / 3 ; <nl>  <nl> - info -> frame_coding_mode = v -> fcm ; <nl> + info -> frame_coding_mode = v -> fcm ? v -> fcm + 1 : 0 ; <nl> info -> postprocflag = v -> postprocflag ; <nl> info -> pulldown = v -> broadcast ; <nl> info -> interlace = v -> interlace ;
int ff_alsa_get_device_list ( AVDeviceInfoList * device_list , snd_pcm_stream_t stre <nl> & device_list -> nb_devices , new_device )) < 0 ) { <nl> goto fail ; <nl> } <nl> + if (! strcmp ( new_device -> device_name , " default ")) <nl> + device_list -> default_device = device_list -> nb_devices - 1 ; <nl> new_device = NULL ; <nl> } <nl> fail :
static int qdm2_parse_packet ( AVFormatContext * s , PayloadContext * qdm , <nl> * to the decoder that it is OK to initialize . */ <nl> st -> codec -> codec_id = CODEC_ID_QDM2 ; <nl> } <nl> + if ( st -> codec -> codec_id == CODEC_ID_NONE ) <nl> + return AVERROR ( EAGAIN ); <nl>  <nl> /* subpackets */ <nl> while ( end - p >= 4 ) {
static inline void skip_bits1 ( GetBitContext * s ) <nl> */ <nl> static inline unsigned int get_bits_long ( GetBitContext * s , int n ) <nl> { <nl> + av_assert2 ( n >= 0 && n <= 32 ); <nl> if (! n ) { <nl> return 0 ; <nl> } else if ( n <= MIN_CACHE_BITS ) {
static inline int mpeg2_fast_decode_block_non_intra ( MpegEncContext * s , <nl> } <nl>  <nl> block [ j ] = level ; <nl> - if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF ) <nl> + if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF || i >= 64 ) <nl> break ; <nl> + <nl> UPDATE_CACHE ( re , & s -> gb ); <nl> } <nl> end :
int ff_pcm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int ret , size ; <nl>  <nl> size = RAW_SAMPLES * s -> streams [ 0 ]-> codec -> block_align ; <nl> + if ( size <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> ret = av_get_packet ( s -> pb , pkt , size ); <nl> 
int ff_hevc_output_frame ( HEVCContext * s , AVFrame * out , int flush ) <nl> if (( frame -> flags & HEVC_FRAME_FLAG_OUTPUT ) && <nl> frame -> sequence == s -> seq_output ) { <nl> nb_output ++; <nl> - if ( frame -> poc < min_poc ) { <nl> + if ( frame -> poc < min_poc || nb_output == 1 ) { <nl> min_poc = frame -> poc ; <nl> min_idx = i ; <nl> }
static int truemotion2rt_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if ( avctx -> width / s -> hscale * avctx -> height * s -> delta_size > avpkt -> size * 8LL * 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> ret = init_get_bits8 ( gb , avpkt -> data + ret , avpkt -> size - ret ); <nl> if ( ret < 0 ) <nl> return ret ;
static int amovie_request_frame ( AVFilterLink * outlink ) <nl>  <nl> if ( movie -> is_done ) <nl> return AVERROR_EOF ; <nl> - if (( ret = amovie_get_samples ( outlink )) < 0 ) <nl> - return ret ; <nl> + do { <nl> + if (( ret = amovie_get_samples ( outlink )) < 0 ) <nl> + return ret ; <nl> + } while (! movie -> samplesref ); <nl>  <nl> avfilter_filter_samples ( outlink , avfilter_ref_buffer ( movie -> samplesref , ~ 0 )); <nl> avfilter_unref_buffer ( movie -> samplesref );
int ff_thread_decode_frame ( AVCodecContext * avctx , <nl> FrameThreadContext * fctx = avctx -> internal -> thread_ctx ; <nl> int finished = fctx -> next_finished ; <nl> PerThreadContext * p ; <nl> - int err , ret ; <nl> + int err , ret = 0 ; <nl>  <nl> /* release the async lock , permitting blocked hwaccel threads to <nl> * go forward while we are in this function */
static int decode_nal_units ( H264Context * h , uint8_t * buf , int buf_size ){ <nl> if ( ptr == NULL || dst_length < 0 ){ <nl> return - 1 ; <nl> } <nl> - while ( ptr [ dst_length - 1 ] == 0 && dst_length > 1 ) <nl> + while ( ptr [ dst_length - 1 ] == 0 && dst_length > 0 ) <nl> dst_length --; <nl> bit_length = 8 * dst_length - decode_rbsp_trailing ( h , ptr + dst_length - 1 ); <nl> 
static int ffm_write_packet ( AVFormatContext * s , int stream_index , <nl> /* packet size & key_frame */ <nl> header [ 0 ] = stream_index ; <nl> header [ 1 ] = 0 ; <nl> - if ( st -> codec . coded_picture -> key_frame ) <nl> + if ( st -> codec . coded_picture && st -> codec . coded_picture -> key_frame ) <nl> header [ 1 ] |= FLAG_KEY_FRAME ; <nl> header [ 2 ] = ( size >> 16 ) & 0xff ; <nl> header [ 3 ] = ( size >> 8 ) & 0xff ;
static int encode_superframe ( AVCodecContext * avctx , <nl> } <nl> } <nl>  <nl> + if ( buf_size < 2 * MAX_CODED_SUPERFRAME_SIZE ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " output buffer size is too small \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> # if 1 <nl> total_gain = 128 ; <nl> for ( i = 64 ; i ; i >>= 1 ){
static inline void skip_put_bits ( PutBitContext * s , int n ) <nl> */ <nl> static inline void set_put_bits_buffer_size ( PutBitContext * s , int size ) <nl> { <nl> + av_assert0 ( size <= INT_MAX / 8 - 32 ); <nl> s -> buf_end = s -> buf + size ; <nl> s -> size_in_bits = 8 * size ; <nl> }
static int mpeg_mux_write_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> s -> preload *= 2 ; <nl> } <nl> preload = av_rescale ( s -> preload , 90000 , AV_TIME_BASE ); <nl> + av_log ( ctx , AV_LOG_DEBUG , " First SCR : %" PRId64 " First DTS : %" PRId64 "\ n ", s -> last_scr , dts + preload ); <nl> } <nl>  <nl> if ( dts != AV_NOPTS_VALUE ) dts += preload ;
static int ape_read_header ( AVFormatContext * s ) <nl> return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < ape -> seektablelength / sizeof ( uint32_t ); i ++) <nl> ape -> seektable [ i ] = avio_rl32 ( pb ); <nl> + } else { <nl> + av_log ( s , AV_LOG_ERROR , " Missing seektable \ n "); <nl> + return - 1 ; <nl> } <nl>  <nl> ape -> frames [ 0 ]. pos = ape -> firstframe ;
static int Stagefright_decode_frame ( AVCodecContext * avctx , void * data , <nl> frame -> size = orig_size ; <nl> } <nl> memcpy ( frame -> buffer , ptr , orig_size ); <nl> + if ( avpkt == & pkt ) <nl> + av_free ( avpkt -> data ); <nl>  <nl> frame -> time = ++ s -> frame_index ; <nl> (* s -> ts_map )[ s -> frame_index ]. pts = avpkt -> pts ;
static inline uint64_t get_bits64 ( GetBitContext * s , int n ) <nl> */ <nl> static inline int get_sbits_long ( GetBitContext * s , int n ) <nl> { <nl> + // sign_extend ( x , 0 ) is undefined <nl> + if (! n ) <nl> + return 0 ; <nl> + <nl> return sign_extend ( get_bits_long ( s , n ), n ); <nl> } <nl> 
static int mpc8_probe ( AVProbeData * p ) <nl> size = bs_get_v (& bs ); <nl> if ( size < 2 ) <nl> return 0 ; <nl> - if ( bs + size - 2 >= bs_end ) <nl> + if ( size >= bs_end - bs + 2 ) <nl> return AVPROBE_SCORE_EXTENSION - 1 ; // seems to be valid MPC but no header yet <nl> if ( header_found ) { <nl> if ( size < 11 || size > 28 )
void ff_slice_thread_free ( AVCodecContext * avctx ) <nl> pthread_mutex_destroy (& c -> current_job_lock ); <nl> pthread_cond_destroy (& c -> current_job_cond ); <nl> pthread_cond_destroy (& c -> last_job_cond ); <nl> - av_free ( c -> workers ); <nl> + av_freep (& c -> workers ); <nl> av_freep (& avctx -> internal -> thread_ctx ); <nl> } <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl> } <nl>  <nl> + if ( avctx -> slices > 1 && <nl> + ( avctx -> codec_id == AV_CODEC_ID_FLV1 || avctx -> codec_id == AV_CODEC_ID_H261 )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Multiple slices are not supported by this codec \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> if ( s -> avctx -> thread_count > 1 && <nl> s -> codec_id != AV_CODEC_ID_MPEG4 && <nl> s -> codec_id != AV_CODEC_ID_MPEG1VIDEO &&
static int read_extra_header ( FFV1Context * f ) <nl> } <nl>  <nl> f -> quant_table_count = get_symbol ( c , state , 0 ); <nl> - if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES ) <nl> + if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES || ! f -> quant_table_count ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> for ( i = 0 ; i < f -> quant_table_count ; i ++) {
int ff_h264_execute_ref_pic_marking ( H264Context * h , MMCO * mmco , int mmco_count ) <nl> */ <nl> if ( h -> short_ref_count && h -> short_ref [ 0 ] == h -> cur_pic_ptr ) { <nl> /* Just mark the second field valid */ <nl> - h -> cur_pic_ptr -> reference = PICT_FRAME ; <nl> + h -> cur_pic_ptr -> reference |= h -> picture_structure ; <nl> } else if ( h -> cur_pic_ptr -> long_ref ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " illegal short term reference " <nl> " assignment for second field "
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl> } <nl> if ( ctx -> swfurl ) { <nl> av_strlcat ( filename , " swfUrl =", len ); <nl> - av_strlcat ( filename , ctx -> pageurl , len ); <nl> + av_strlcat ( filename , ctx -> swfurl , len ); <nl> } <nl> if ( ctx -> flashver ) { <nl> av_strlcat ( filename , " flashVer =", len );
static int flashsv2_prime ( FlashSVContext * s , uint8_t * src , int size ) <nl> z_stream zs ; <nl> int zret ; // Zlib return code <nl>  <nl> + if (! src ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> zs . zalloc = NULL ; <nl> zs . zfree = NULL ; <nl> zs . opaque = NULL ;
static inline int get_len ( LZOContext * c , int x , int mask ) <nl> { <nl> int cnt = x & mask ; <nl> if (! cnt ) { <nl> - while (!( x = get_byte ( c ))) <nl> + while (!( x = get_byte ( c ))) { <nl> + if ( cnt >= INT_MAX - 1000 ) { <nl> + c -> error |= AV_LZO_ERROR ; <nl> + break ; <nl> + } <nl> cnt += 255 ; <nl> + } <nl> cnt += mask + x ; <nl> } <nl> return cnt ;
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> } <nl> } <nl>  <nl> -# define DURATION_MAX_READ_SIZE 250000 <nl> +# define DURATION_MAX_READ_SIZE 250000LL <nl> # define DURATION_MAX_RETRY 4 <nl>  <nl> /* only usable for MPEG - PS streams */
static int read_matrix_params ( MLPDecodeContext * m , unsigned int substr , GetBitCo <nl> if ( get_bits1 ( gbp )) <nl> coeff_val = get_sbits ( gbp , frac_bits + 2 ); <nl>  <nl> - s -> matrix_coeff [ mat ][ ch ] = coeff_val << ( 14 - frac_bits ); <nl> + s -> matrix_coeff [ mat ][ ch ] = coeff_val * ( 1 << ( 14 - frac_bits )); <nl> } <nl>  <nl> if ( s -> noise_type )
static void adpcm_compress_trellis ( AVCodecContext * avctx , <nl> uint8_t * h ;\ <nl> dec_sample = av_clip_int16 ( dec_sample );\ <nl> d = sample - dec_sample ;\ <nl> - ssd = nodes [ j ]-> ssd + d * d ;\ <nl> + ssd = nodes [ j ]-> ssd + d *( unsigned ) d ;\ <nl> /* Check for wraparound , skip such samples completely . \ <nl> * Note , changing ssd to a 64 bit variable would be \ <nl> * simpler , avoiding this check , but it ' s slower on \
int avformat_open_input ( AVFormatContext ** ps , const char * filename , AVInputForma <nl> { <nl> AVFormatContext * s = * ps ; <nl> int ret = 0 ; <nl> - AVFormatParameters ap = { 0 }; <nl> + AVFormatParameters ap = { { 0 } }; <nl> AVDictionary * tmp = NULL ; <nl>  <nl> if (! s && !( s = avformat_alloc_context ()))
int ff_lzw_decode ( LZWState * p , uint8_t * buf , int len ){ <nl> if ((-- l ) == 0 ) <nl> goto the_end ; <nl> } <nl> + if ( s -> ebuf < s -> pbuf ) { <nl> + av_log ( 0 , AV_LOG_ERROR , " lzw overread \ n "); <nl> + goto the_end ; <nl> + } <nl> c = lzw_get_code ( s ); <nl> if ( c == s -> end_code ) { <nl> break ;
static int initFilter ( int16_t ** outFilter , int16_t ** filterPos , int * outFilterSi <nl> int j ; <nl> (* filterPos )[ i ]= xx ; <nl> for ( j = 0 ; j < filterSize ; j ++) { <nl> - int64_t d = (( int64_t ) FFABS (( xx << 17 ) - xDstInSrc ))<< 13 ; <nl> + int64_t d = ( FFABS ((( int64_t ) xx << 17 ) - xDstInSrc ))<< 13 ; <nl> double floatd ; <nl> int64_t coeff ; <nl> 
static av_cold void uninit ( AVFilterContext * ctx ) <nl>  <nl> if ( select -> do_scene_detect ) { <nl> avfilter_unref_bufferp (& select -> prev_picref ); <nl> - avcodec_close ( select -> avctx ); <nl> - av_freep (& select -> avctx ); <nl> + if ( select -> avctx ) { <nl> + avcodec_close ( select -> avctx ); <nl> + av_freep (& select -> avctx ); <nl> + } <nl> } <nl> } <nl> 
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> uint8_t mask2 = -( esc_count < 3 ); <nl> uint8_t * end = dst + ( width - 2 ); <nl>  <nl> + avpriv_request_sample ( l -> avctx , " zero_run_line "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i );
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> goto fail ; <nl>  <nl> s -> cbf_luma = av_malloc ( sps -> min_tb_width * sps -> min_tb_height ); <nl> - s -> tab_ipm = av_malloc ( min_pu_size ); <nl> + s -> tab_ipm = av_mallocz ( min_pu_size ); <nl> s -> is_pcm = av_malloc ( min_pu_size ); <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ;
static int stream_component_open ( VideoState * is , int stream_index ) <nl> goto fail ; <nl> link = is -> out_audio_filter -> inputs [ 0 ]; <nl> sample_rate = link -> sample_rate ; <nl> - nb_channels = link -> channels ; <nl> + nb_channels = avfilter_link_get_channels ( link ); <nl> channel_layout = link -> channel_layout ; <nl> } <nl> # else
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_INIT_VIDEO_BUFFERS : <nl> av_dlog ( NULL , " initialize video buffers \ n "); <nl> - if (( opcode_version > 2 ) || ( opcode_size > 8 )) { <nl> + if (( opcode_version > 2 ) || ( opcode_size > 8 ) || opcode_size < 4 ) { <nl> av_dlog ( NULL , " bad init_video_buffers opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static void copy_bits ( PutBitContext * pb , <nl> rmn_bits = rmn_bytes = get_bits_left ( gb ); <nl> if ( rmn_bits < nbits ) <nl> return ; <nl> + if ( nbits > pb -> size_in_bits - put_bits_count ( pb )) <nl> + return ; <nl> rmn_bits &= 7 ; rmn_bytes >>= 3 ; <nl> if (( rmn_bits = FFMIN ( rmn_bits , nbits )) > 0 ) <nl> put_bits ( pb , rmn_bits , get_bits ( gb , rmn_bits ));
av_cold int ff_h264_decode_init ( AVCodecContext * avctx ) <nl>  <nl> if ( avctx -> extradata_size > 0 && avctx -> extradata ) { <nl> ret = ff_h264_decode_extradata ( h ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + ff_h264_free_context ( h ); <nl> return ret ; <nl> + } <nl> } <nl>  <nl> if ( h -> sps . bitstream_restriction_flag &&
static void frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> pthread_cond_signal (& p -> input_cond ); <nl> pthread_mutex_unlock (& p -> mutex ); <nl>  <nl> - pthread_join ( p -> thread , NULL ); <nl> + if ( p -> thread ) <nl> + pthread_join ( p -> thread , NULL ); <nl>  <nl> if ( codec -> close ) <nl> codec -> close ( p -> avctx );
typedef struct { <nl> static int probe ( AVProbeData * p ) <nl> { <nl> if ( AV_RL16 ( p -> buf ) == 0 && AV_RL16 ( p -> buf + 2 ) == 1 && AV_RL16 ( p -> buf + 4 )) <nl> - return AVPROBE_SCORE_MAX / 3 ; <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> return 0 ; <nl> } <nl> 
static void alac_linear_predictor ( AlacEncodeContext * s , int ch ) <nl>  <nl> sum >>= lpc . lpc_quant ; <nl> sum += samples [ 0 ]; <nl> - residual [ i ] = samples [ lpc . lpc_order + 1 ] - sum ; <nl> + residual [ i ] = ( samples [ lpc . lpc_order + 1 ] - sum ) << ( 32 - s -> write_sample_size ) >> <nl> + ( 32 - s -> write_sample_size ); <nl> res_val = residual [ i ]; <nl>  <nl> if ( res_val ) {
static void rv34_pred_4x4_block ( RV34DecContext * r , uint8_t * dst , int stride , int <nl> if ( itype == VERT_LEFT_PRED ) itype = VERT_LEFT_PRED_RV40_NODOWN ; <nl> } <nl> if (! right && up ){ <nl> - topleft = dst [- stride + 3 ] * 0x01010101 ; <nl> + topleft = dst [- stride + 3 ] * 0x01010101u ; <nl> prev = ( uint8_t *)& topleft ; <nl> } <nl> r -> h . pred4x4 [ itype ]( dst , prev , stride );
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl> } else { <nl> int i ; <nl> for ( i = 0 ; SHOW_UBITS ( re , gb , 1 ) == 0 ; i ++){ <nl> - if ( get_bits_left ( gb )<= 0 ) <nl> + if ( gb -> size_in_bits <= re_index ) <nl> return - 1 ; <nl> LAST_SKIP_BITS ( re , gb , 1 ); <nl> UPDATE_CACHE ( re , gb );
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> if (! avctx -> rc_initial_buffer_occupancy ) <nl> - avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3 / 4 ; <nl> + avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3LL / 4 ; <nl>  <nl> if ( avctx -> ticks_per_frame && avctx -> time_base . num && <nl> avctx -> ticks_per_frame > INT_MAX / avctx -> time_base . num ) {
static int mxf_read_close ( AVFormatContext * s ) <nl> case MaterialPackage : <nl> av_freep (&(( MXFPackage *) mxf -> metadata_sets [ i ])-> tracks_refs ); <nl> break ; <nl> + case Track : <nl> + mxf -> metadata_sets [ i ] = NULL ; /* will be freed later */ <nl> + break ; <nl> default : <nl> break ; <nl> }
static int rtp_write ( URLContext * h , const uint8_t * buf , int size ) <nl> int ret ; <nl> URLContext * hd ; <nl>  <nl> + if ( size < 2 ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> if ( RTP_PT_IS_RTCP ( buf [ 1 ])) { <nl> /* RTCP payload type */ <nl> hd = s -> rtcp_hd ;
static int mjpeg_decode_scan ( MJpegDecodeContext * s , int nb_components , int Ah , <nl> uint8_t * data [ MAX_COMPONENTS ]; <nl> const uint8_t * reference_data [ MAX_COMPONENTS ]; <nl> int linesize [ MAX_COMPONENTS ]; <nl> - GetBitContext mb_bitmask_gb ; <nl> + GetBitContext mb_bitmask_gb = { 0 }; // initialize to silence gcc warning <nl> int bytes_per_pixel = 1 + ( s -> bits > 8 ); <nl>  <nl> if ( mb_bitmask ) {
# include < stdlib . h > <nl> # include < stdio . h > <nl>  <nl> +# include " libavutil / common . h " <nl> # include " libavformat / avformat . h " <nl>  <nl> # undef exit <nl> int main ( int argc , char ** argv ) <nl>  <nl> for ( i = 0 ; ; i ++){ <nl> AVPacket pkt ; <nl> - AVStream * st ; <nl> + AVStream * av_uninit ( st ); <nl>  <nl> memset (& pkt , 0 , sizeof ( pkt )); <nl> if ( ret >= 0 ){
static int alac_set_info ( ALACContext * alac ) <nl>  <nl> alac -> max_samples_per_frame = bytestream2_get_be32u (& gb ); <nl> if (! alac -> max_samples_per_frame || <nl> - alac -> max_samples_per_frame > INT_MAX / sizeof ( int32_t )) { <nl> + alac -> max_samples_per_frame > 4096 * 4096 ) { <nl> av_log ( alac -> avctx , AV_LOG_ERROR , <nl> " max samples per frame invalid : %" PRIu32 "\ n ", <nl> alac -> max_samples_per_frame );
int avpriv_mpeg4audio_get_config ( MPEG4AudioConfig * c , const uint8_t * buf , <nl> GetBitContext gb ; <nl> int specific_config_bitindex ; <nl>  <nl> + if ( bit_size <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> init_get_bits (& gb , buf , bit_size ); <nl> c -> object_type = get_object_type (& gb ); <nl> c -> sample_rate = get_sample_rate (& gb , & c -> sampling_index );
static inline void encode_vlc_codeword ( PutBitContext * pb , unsigned codebook , int <nl> exponent = av_log2 ( val ); <nl>  <nl> put_bits ( pb , exponent - exp_order + switch_bits , 0 ); <nl> - put_bits ( pb , 1 , 1 ); <nl> - put_bits ( pb , exponent , val ); <nl> + put_bits ( pb , exponent + 1 , val ); <nl> } else { <nl> exponent = val >> rice_order ; <nl> 
static av_cold int libwebp_anim_encode_init ( AVCodecContext * avctx ) <nl> int ret = ff_libwebp_encode_init_common ( avctx ); <nl> if (! ret ) { <nl> LibWebPAnimContext * s = avctx -> priv_data ; <nl> - WebPAnimEncoderOptions enc_options ; <nl> + WebPAnimEncoderOptions enc_options = { 0 }; <nl> WebPAnimEncoderOptionsInit (& enc_options ); <nl> // TODO ( urvang ): Expose some options on command - line perhaps . <nl> s -> enc = WebPAnimEncoderNew ( avctx -> width , avctx -> height , & enc_options );
static int aiff_read_packet ( AVFormatContext * s , <nl> if ( max_size <= 0 ) <nl> return AVERROR_EOF ; <nl>  <nl> + if (! st -> codecpar -> block_align ) { <nl> + av_log ( s , AV_LOG_ERROR , " block_align not set \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* Now for that packet */ <nl> switch ( st -> codecpar -> codec_id ) { <nl> case AV_CODEC_ID_ADPCM_IMA_QT :
static int compute_mask ( int step , uint32_t * mask ) <nl> ret = AVERROR ( ENOMEM ); <nl> goto end ; <nl> } <nl> - counter = av_mallocz ( counter_size ); <nl> + counter = av_mallocz ( sizeof ( uint32_t *) * ( 2 * step + 1 )); <nl> if (! counter ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto end ;
int ff_thread_init ( AVCodecContext * s ){ <nl> return 0 ; <nl> } <nl>  <nl> - s -> active_thread_type = FF_THREAD_SLICE ; <nl> - <nl> if ( s -> thread_count <= 1 ) <nl> return 0 ; <nl>  <nl> + s -> active_thread_type = FF_THREAD_SLICE ; <nl> + <nl> assert (! s -> thread_opaque ); <nl> c = av_mallocz ( sizeof ( ThreadContext )* s -> thread_count ); <nl> s -> thread_opaque = c ;
static int fileTest ( uint8_t * ref [ 4 ], int refStride [ 4 ], int w , int h , FILE * fp , <nl> struct Results r ; <nl> enum AVPixelFormat srcFormat ; <nl> char srcStr [ 12 ]; <nl> - int srcW , srcH ; <nl> + int srcW = 0 , srcH = 0 ; <nl> enum AVPixelFormat dstFormat ; <nl> char dstStr [ 12 ]; <nl> - int dstW , dstH ; <nl> + int dstW = 0 , dstH = 0 ; <nl> int flags ; <nl> int ret ; <nl> 
static void mov_fix_index ( MOVContext * mov , AVStream * st ) <nl> int first_non_zero_audio_edit = - 1 ; <nl> int packet_skip_samples = 0 ; <nl>  <nl> - if (! msc -> elst_data || msc -> elst_count <= 0 ) { <nl> + if (! msc -> elst_data || msc -> elst_count <= 0 || nb_old <= 0 ) { <nl> return ; <nl> } <nl> // Clean AVStream from traces of old index
static int msrle_decode_pal4 ( AVCodecContext * avctx , AVPicture * pic , <nl> unsigned int pixel_ptr = 0 ; <nl> int row_dec = pic -> linesize [ 0 ]; <nl> int row_ptr = ( avctx -> height - 1 ) * row_dec ; <nl> - int frame_size = row_dec * avctx -> height ; <nl> + int frame_size = FFABS ( row_dec ) * avctx -> height ; <nl> int i ; <nl>  <nl> while ( row_ptr >= 0 ) {
AVFilterFormats * avfilter_merge_formats ( AVFilterFormats * a , AVFilterFormats * b ) <nl> AVFilterFormats * ret ; <nl> unsigned i , j , k = 0 , m_count ; <nl>  <nl> + if ( a == b ) <nl> + return a ; <nl> + <nl> ret = av_mallocz ( sizeof (* ret )); <nl>  <nl> /* merge list of formats */
static void av_update_stream_timings ( AVFormatContext * ic ) <nl> duration = INT64_MIN ; <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> - if ( st -> start_time != AV_NOPTS_VALUE ) { <nl> + if ( st -> start_time != AV_NOPTS_VALUE && st -> time_base . den ) { <nl> start_time1 = av_rescale_q ( st -> start_time , st -> time_base , AV_TIME_BASE_Q ); <nl> if ( start_time1 < start_time ) <nl> start_time = start_time1 ;
static int h263p_decode_umotion ( MpegEncContext * s , int pred ) <nl> code += get_bits1 (& s -> gb ); <nl> if ( code >= 32768 ) { <nl> avpriv_request_sample ( s -> avctx , " Huge DMV "); <nl> - return AVERROR_INVALIDDATA ; <nl> + return 0xffff ; <nl> } <nl> } <nl> sign = code & 1 ;
static int parse_tonal ( DCALbrDecoder * s , int group ) <nl> break ; // End of subframe <nl>  <nl> freq += diff - 2 ; <nl> - if ( freq >> ( 5 - group ) > s -> nsubbands * 4 - 5 ) { <nl> + if ( freq >> ( 5 - group ) > s -> nsubbands * 4 - 6 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid spectral line offset \ n "); <nl> return - 1 ; <nl> }
static int decode_slice ( struct AVCodecContext * avctx , void * arg ) <nl> return 0 ; <nl> } else { <nl> ff_er_add_slice ( s , s -> resync_mb_x , s -> resync_mb_y , <nl> - s -> mb_x , s -> mb_y , <nl> + s -> mb_x - 1 , s -> mb_y , <nl> ER_MB_END & part_mask ); <nl>  <nl> return - 1 ;
static int ism_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> SmoothStreamingContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ pkt -> stream_index ]; <nl> - int64_t end_dts = ( c -> nb_fragments + 1 ) * c -> min_frag_duration ; <nl> + int64_t end_dts = ( c -> nb_fragments + 1 ) * ( int64_t ) c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int read_access_unit ( AVCodecContext * avctx , void * data , <nl> int ret ; <nl>  <nl> if ( buf_size < 4 ) <nl> - return 0 ; <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> length = ( AV_RB16 ( buf ) & 0xfff ) * 2 ; <nl> 
static int mpegps_read_packet ( AVFormatContext * s , <nl> MpegDemuxContext * m = s -> priv_data ; <nl> AVStream * st ; <nl> int len , startcode , i , es_type , ret ; <nl> - int lpcm_header_len ; <nl> + int lpcm_header_len = - 1 ; // Init to supress warning <nl> int request_probe = 0 ; <nl> enum AVCodecID codec_id = AV_CODEC_ID_NONE ; <nl> enum AVMediaType type ;
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> } else { <nl> pkt = add_to_pktbuf (& ic -> packet_buffer , & pkt1 , <nl> & ic -> packet_buffer_end ); <nl> + if (! pkt ) <nl> + goto find_stream_info_err ; <nl> if (( ret = av_dup_packet ( pkt )) < 0 ) <nl> goto find_stream_info_err ; <nl> }
static int read_var_block_data ( ALSDecContext * ctx , ALSBlockData * bd ) <nl>  <nl>  <nl> if (! sconf -> rlslms ) { <nl> - if ( sconf -> adapt_order ) { <nl> + if ( sconf -> adapt_order && sconf -> max_order ) { <nl> int opt_order_length = av_ceil_log2 ( av_clip (( bd -> block_length >> 3 ) - 1 , <nl> 2 , sconf -> max_order + 1 )); <nl> * bd -> opt_order = get_bits ( gb , opt_order_length );
static int get_channel ( char ** map , uint64_t * ch , char delim ) <nl> static av_cold int channelmap_init ( AVFilterContext * ctx ) <nl> { <nl> ChannelMapContext * s = ctx -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> char * mapping , separator = '|'; <nl> int map_entries = 0 ; <nl> char buf [ 256 ];
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> AVFrame * const p = data ; <nl> int mb_x , mb_y , ret ; <nl>  <nl> + if ( buf_size * 8LL < a -> mb_height2 * a -> mb_width2 * 13LL ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 ) <nl> return ret ; <nl> p -> pict_type = AV_PICTURE_TYPE_I ;
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl>  <nl> if ( frames_hwctx -> texture ) <nl> ID3D11Texture2D_Release ( frames_hwctx -> texture ); <nl> + frames_hwctx -> texture = NULL ; <nl>  <nl> if ( s -> staging_texture ) <nl> ID3D11Texture2D_Release ( s -> staging_texture ); <nl> + s -> staging_texture = NULL ; <nl> } <nl>  <nl> static void free_texture ( void * opaque , uint8_t * data )
FF_ENABLE_DEPRECATION_WARNINGS <nl>  <nl> if ( mxg -> soi_ptr - mxg -> buffer > mxg -> cache_size ) { <nl> if ( mxg -> cache_size > 0 ) { <nl> - memcpy ( mxg -> buffer , mxg -> buffer_ptr , mxg -> cache_size ); <nl> + memmove ( mxg -> buffer , mxg -> buffer_ptr , mxg -> cache_size ); <nl> } <nl>  <nl> mxg -> buffer_ptr = mxg -> buffer ;
static int vaapi_encode_h264_write_extra_header ( AVCodecContext * avctx , <nl>  <nl> if ( priv -> sei_needed ) { <nl> if ( priv -> aud_needed ) { <nl> - vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + err = vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + if ( err < 0 ) <nl> + goto fail ; <nl> priv -> aud_needed = 0 ; <nl> } <nl> 
static enum AVPixelFormat get_format ( HEVCContext * s , const HEVCSPS * sps ) <nl> * fmt ++ = sps -> pix_fmt ; <nl> * fmt = AV_PIX_FMT_NONE ; <nl>  <nl> - return ff_get_format ( s -> avctx , pix_fmts ); <nl> + return ff_thread_get_format ( s -> avctx , pix_fmts ); <nl> } <nl>  <nl> static int set_sps ( HEVCContext * s , const HEVCSPS * sps ,
static int query_formats ( AVFilterContext * ctx ) <nl> main_formats = ff_make_format_list ( main_pix_fmts_rgb ); <nl> overlay_formats = ff_make_format_list ( overlay_pix_fmts_rgb ); <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl>  <nl> ff_formats_ref ( main_formats , & ctx -> inputs [ MAIN ]-> out_formats );
int av_frame_copy ( AVFrame * dst , const AVFrame * src ) <nl>  <nl> if ( dst -> width > 0 && dst -> height > 0 ) <nl> return frame_copy_video ( dst , src ); <nl> - else if ( dst -> nb_samples > 0 && dst -> channel_layout ) <nl> + else if ( dst -> nb_samples > 0 && dst -> channels > 0 ) <nl> return frame_copy_audio ( dst , src ); <nl>  <nl> return AVERROR ( EINVAL );
static int read_thread ( void * arg ) <nl> stream_component_close ( is , is -> video_stream ); <nl> if ( is -> subtitle_stream >= 0 ) <nl> stream_component_close ( is , is -> subtitle_stream ); <nl> - if ( is -> ic ) { <nl> - avformat_close_input (& is -> ic ); <nl> + if ( ic ) { <nl> + avformat_close_input (& ic ); <nl> + is -> ic = NULL ; <nl> } <nl>  <nl> if ( ret != 0 ) {
static int mpeg4_decode_sprite_trajectory ( Mpeg4DecContext * ctx , GetBitContext * g <nl> int a = 2 << s -> sprite_warping_accuracy ; <nl> int rho = 3 - s -> sprite_warping_accuracy ; <nl> int r = 16 / a ; <nl> - int alpha = 0 ; <nl> + int alpha = 1 ; <nl> int beta = 0 ; <nl> int w = s -> width ; <nl> int h = s -> height ;
static int decode_bmv_frame ( const uint8_t * source , int src_len , uint8_t * frame , <nl> mode += 1 + advance_mode ; <nl> if ( mode >= 4 ) <nl> mode -= 3 ; <nl> - if ( FFABS ( dst_end - dst ) < len ) <nl> + if ( len <= 0 || FFABS ( dst_end - dst ) < len ) <nl> return AVERROR_INVALIDDATA ; <nl> switch ( mode ) { <nl> case 1 :
int av_cold ff_ivi_init_tiles ( IVIPlaneDesc * planes , int tile_width , int tile_hei <nl> t_width >>= 1 ; <nl> t_height >>= 1 ; <nl> } <nl> + if ( t_width <= 0 || t_height <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> for ( b = 0 ; b < planes [ p ]. num_bands ; b ++) { <nl> band = & planes [ p ]. bands [ b ];
PCA * ff_pca_init ( int n ){ <nl> if ( n <= 0 ) <nl> return NULL ; <nl>  <nl> - pca = av_mallocz ( sizeof ( PCA )); <nl> + pca = av_mallocz ( sizeof (* pca )); <nl> pca -> n = n ; <nl> pca -> z = av_malloc ( sizeof (* pca -> z ) * n ); <nl> pca -> count = 0 ;
static int v4l2_set_parameters ( AVFormatContext * s1 ) <nl> standard . index = i ; <nl> if ( v4l2_ioctl ( s -> fd , VIDIOC_ENUMSTD , & standard ) < 0 ) { <nl> ret = AVERROR ( errno ); <nl> + if ( ret == AVERROR ( EINVAL )) { <nl> + tpf = & streamparm . parm . capture . timeperframe ; <nl> + break ; <nl> + } <nl> av_log ( s1 , AV_LOG_ERROR , " ioctl ( VIDIOC_ENUMSTD ): % s \ n ", av_err2str ( ret )); <nl> return ret ; <nl> }
static int img_read_header ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl>  <nl> st = av_new_stream ( s1 , 0 ); <nl> if (! st ) { <nl> - av_free ( s ); <nl> return - ENOMEM ; <nl> } <nl>  <nl> static int img_read_header ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl> if (! s -> is_pipe ) <nl> url_fclose ( f ); <nl> fail : <nl> - av_free ( s ); <nl> return AVERROR_IO ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> avio_flush ( probe_out ); <nl> av_freep (& probe_out ); <nl> av_freep (& buffer ); <nl> - <nl> + uninit_opts (); <nl> avformat_network_deinit (); <nl>  <nl> return ret ;
static int rsd_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> par -> channels = avio_rl32 ( pb ); <nl> - if (! par -> channels ) <nl> + if ( par -> channels <= 0 || par -> channels > INT_MAX / 36 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels : % d \ n ", par -> channels ); <nl> return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> avio_skip ( pb , 4 ); // Bit depth <nl> par -> sample_rate = avio_rl32 ( pb );
static void ffmpeg_cleanup ( int ret ) <nl> av_freep (& ost -> audio_channels_map ); <nl> ost -> audio_channels_mapped = 0 ; <nl>  <nl> + av_dict_free (& ost -> sws_dict ); <nl> + <nl> avcodec_free_context (& ost -> enc_ctx ); <nl>  <nl> av_freep (& output_streams [ i ]);
int ff_h264_queue_decode_slice ( H264Context * h , const H2645NAL * nal ) <nl> return ret ; <nl>  <nl> // discard redundant pictures <nl> - if ( sl -> redundant_pic_count > 0 ) <nl> + if ( sl -> redundant_pic_count > 0 ) { <nl> + sl -> ref_count [ 0 ] = sl -> ref_count [ 1 ] = 0 ; <nl> return 0 ; <nl> + } <nl>  <nl> if ( sl -> first_mb_addr == 0 || ! h -> current_slice ) { <nl> if ( h -> setup_finished ) {
static int filter_packet ( void * log_ctx , AVPacket * pkt , <nl> } <nl>  <nl> if ( ret > 0 ) { <nl> + pkt -> side_data = NULL ; <nl> + pkt -> side_data_elems = 0 ; <nl> av_packet_unref ( pkt ); <nl> new_pkt . buf = av_buffer_create ( new_pkt . data , new_pkt . size , <nl> av_buffer_default_free , NULL , 0 );
static av_cold int common_init ( AVCodecContext * avctx ){ <nl> s -> avctx = avctx ; <nl> s -> flags = avctx -> flags ; <nl>  <nl> + avcodec_get_frame_defaults (& s -> picture ); <nl> + <nl> dsputil_init (& s -> dsp , avctx ); <nl>  <nl> s -> width = avctx -> width ;
static int dca_parse ( AVCodecParserContext * s , AVCodecContext * avctx , <nl> /* read the duration and sample rate from the frame header */ <nl> if (! dca_parse_params ( buf , buf_size , & duration , & sample_rate , & pc1 -> framesize )) { <nl> s -> duration = duration ; <nl> - avctx -> sample_rate = sample_rate ; <nl> } else <nl> s -> duration = 0 ; <nl> 
static int encode_init ( AVCodecContext * avctx ){ <nl> if ( avctx -> channels > MAX_CHANNELS ) <nl> return - 1 ; <nl>  <nl> + if ( avctx -> bit_rate < 24 * 1000 ) <nl> + return - 1 ; <nl> + <nl> /* extract flag infos */ <nl> flags1 = 0 ; <nl> flags2 = 1 ;
# define FELEM_MIN INT16_MIN <nl> # define WINDOW_TYPE 9 <nl> # elif ! defined ( CONFIG_RESAMPLE_AUDIOPHILE_KIDDY_MODE ) <nl> -# define FILTER_SHIFT 30 <nl> +# define FILTER_SHIFT 22 <nl>  <nl> # define FELEM int32_t <nl> # define FELEM2 int64_t
end : <nl> free_and_end : <nl> av_dict_free (& tmp ); <nl> av_freep (& avctx -> priv_data ); <nl> - if ( avctx -> internal ) <nl> + if ( avctx -> internal ) { <nl> + av_frame_free (& avctx -> internal -> to_free ); <nl> av_freep (& avctx -> internal -> pool ); <nl> + } <nl> av_freep (& avctx -> internal ); <nl> avctx -> codec = NULL ; <nl> goto end ;
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> INC_MED ( 2 ); <nl> } <nl> if (! c -> error_limit ) { <nl> + if ( add >= 0x2000000U ) { <nl> + av_log ( ctx -> avctx , AV_LOG_ERROR , " k % d is too large \ n ", add ); <nl> + goto error ; <nl> + } <nl> ret = base + get_tail ( gb , add ); <nl> if ( get_bits_left ( gb ) <= 0 ) <nl> goto error ;
unsigned int avpriv_toupper4 ( unsigned int x ) <nl> return av_toupper ( x & 0xFF ) + <nl> ( av_toupper (( x >> 8 ) & 0xFF ) << 8 ) + <nl> ( av_toupper (( x >> 16 ) & 0xFF ) << 16 ) + <nl> - ( av_toupper (( x >> 24 ) & 0xFF ) << 24 ); <nl> +(( unsigned ) av_toupper (( x >> 24 ) & 0xFF ) << 24 ); <nl> } <nl>  <nl> int ff_thread_ref_frame ( ThreadFrame * dst , ThreadFrame * src )
static int twolame_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> if ( ret < 0 ) // twolame error <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame ) { <nl> + avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame -> pts != AV_NOPTS_VALUE ) <nl> avpkt -> pts = frame -> pts - ff_samples_to_time_base ( avctx , avctx -> initial_padding ); <nl> } else {
static int pmp_header ( AVFormatContext * s ) <nl> avio_skip ( pb , 10 ); <nl> srate = avio_rl32 ( pb ); <nl> channels = avio_rl32 ( pb ) + 1 ; <nl> - pos = avio_tell ( pb ) + 4 * index_cnt ; <nl> + pos = avio_tell ( pb ) + 4LL * index_cnt ; <nl> for ( i = 0 ; i < index_cnt ; i ++) { <nl> uint32_t size = avio_rl32 ( pb ); <nl> int flags = size & 1 ? AVINDEX_KEYFRAME : 0 ;
static int init_input ( AVFormatContext * s , const char * filename ) <nl> int ret ; <nl> AVProbeData pd = { filename , NULL , 0 }; <nl>  <nl> + if ( s -> iformat && ! strlen ( filename )) <nl> + return 0 ; <nl> + <nl> if ( s -> pb ) { <nl> s -> flags |= AVFMT_FLAG_CUSTOM_IO ; <nl> if (! s -> iformat )
static int read_packet ( AVFormatContext * s , uint8_t * buf , int raw_packet_size ) <nl> static int handle_packets ( MpegTSContext * ts , int nb_packets ) <nl> { <nl> AVFormatContext * s = ts -> stream ; <nl> - uint8_t packet [ TS_PACKET_SIZE ]; <nl> + uint8_t packet [ TS_PACKET_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> int packet_num , ret = 0 ; <nl>  <nl> if ( avio_tell ( s -> pb ) != ts -> last_pos ) {
static int ape_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl>  <nl> if ( ape -> seektablelength > 0 ) { <nl> ape -> seektable = av_malloc ( ape -> seektablelength ); <nl> + if (! ape -> seektable ) <nl> + return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < ape -> seektablelength / sizeof ( uint32_t ); i ++) <nl> ape -> seektable [ i ] = avio_rl32 ( pb ); <nl> }
cl_program av_opencl_compile ( const char * program_name , const char * build_opts ) <nl> int i ; <nl> cl_int status , build_status ; <nl> int kernel_code_idx = 0 ; <nl> - const char * kernel_source ; <nl> + const char * kernel_source = NULL ; <nl> size_t kernel_code_len ; <nl> char * ptr = NULL ; <nl> cl_program program = NULL ;
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + case NAL_IDR_SLICE : <nl> + case NAL_SLICE : <nl> nals_needed = nal_index ; <nl> } <nl> continue ;
static int concat_read_packet ( AVFormatContext * avf , AVPacket * pkt ) <nl> ( ret = open_next_file ( avf )) < 0 ) <nl> break ; <nl> } <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> delta = av_rescale_q ( cat -> cur_file -> start_time - cat -> avf -> start_time , <nl> AV_TIME_BASE_Q , <nl> cat -> avf -> streams [ pkt -> stream_index ]-> time_base );
static int ff_vp56_decode_mbs ( AVCodecContext * avctx , void * data , <nl> int ret = vp56_decode_mb ( s , mb_row , mb_col , is_alpha ); <nl> if ( ret < 0 ) { <nl> damaged = 1 ; <nl> - if (! s -> have_undamaged_frame ) { <nl> + if (! s -> have_undamaged_frame || ! avctx -> error_concealment ) { <nl> s -> discard_frame = 1 ; <nl> return AVERROR_INVALIDDATA ; <nl> }
static int find_video_stream_info ( AVFormatContext * fmt_ctx , int decode ) <nl> end : <nl> av_packet_unref (& pkt ); <nl>  <nl> + /* close all codecs opened in try_decode_video_frame */ <nl> + for ( i = 0 ; i < fmt_ctx -> nb_streams ; i ++) { <nl> + AVStream * st = fmt_ctx -> streams [ i ]; <nl> + avcodec_close ( st -> codec ); <nl> + } <nl> + <nl> return ret < 0 ; <nl> } <nl> 
static int xvid_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> xvid_enc_frame_t xvid_enc_frame = { 0 }; <nl> xvid_enc_stats_t xvid_enc_stats = { 0 }; <nl>  <nl> - if (( ret = ff_alloc_packet2 ( avctx , pkt , mb_width * mb_height * MAX_MB_BYTES + FF_MIN_BUFFER_SIZE )) < 0 ) <nl> + if (( ret = ff_alloc_packet2 ( avctx , pkt , mb_width *( int64_t ) mb_height * MAX_MB_BYTES + FF_MIN_BUFFER_SIZE )) < 0 ) <nl> return ret ; <nl>  <nl> /* Start setting up the frame */
again : <nl>  <nl> if ( avctx -> bits_per_raw_sample != h -> sps . bit_depth_luma || <nl> h -> cur_chroma_format_idc != h -> sps . chroma_format_idc ) { <nl> - if ( s -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU <nl> + if ( s -> avctx -> codec && <nl> + s -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU <nl> && ( h -> sps . bit_depth_luma != 8 || <nl> h -> sps . chroma_format_idc > 1 )) { <nl> av_log ( avctx , AV_LOG_ERROR ,
static av_cold int truemotion1_decode_end ( AVCodecContext * avctx ) <nl> TrueMotion1Context * s = avctx -> priv_data ; <nl>  <nl> av_frame_unref (& s -> frame ); <nl> - av_free ( s -> vert_pred ); <nl> + av_freep (& s -> vert_pred ); <nl>  <nl> return 0 ; <nl> }
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl> } else { <nl> int i ; <nl> for ( i = 0 ; SHOW_UBITS ( re , gb , 1 ) == 0 ; i ++){ <nl> + if ( get_bits_left ( gb )<= 0 ) <nl> + return - 1 ; <nl> LAST_SKIP_BITS ( re , gb , 1 ); <nl> UPDATE_CACHE ( re , gb ); <nl> }
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> break ; <nl> default : <nl> av_log ( c , AV_LOG_ERROR , " Unsupported pixel format .\ n "); <nl> + av_free ( config ); <nl> return NULL ; <nl> } <nl> 
static int xpm_decode_frame ( AVCodecContext * avctx , void * data , <nl> avctx -> pix_fmt = AV_PIX_FMT_BGRA ; <nl>  <nl> end = avpkt -> data + avpkt -> size ; <nl> - while ( memcmp ( ptr , "/* XPM */\ n ", 10 ) && ptr < end - 10 ) <nl> + while ( memcmp ( ptr , "/* XPM */", 9 ) && ptr < end - 9 ) <nl> ptr ++; <nl>  <nl> if ( ptr >= end ) {
recover : <nl> pes_flags = avio_rb16 ( pb ); <nl> pes_header_data_length = avio_r8 ( pb ); <nl>  <nl> + if ( avio_feof ( pb )) { <nl> + return AVERROR_EOF ; <nl> + } <nl> + <nl> if ( pes_signal != 1 || pes_header_data_length == 0 ) { <nl> pva_log ( s , AV_LOG_WARNING , " expected non empty signaled PES packet , " <nl> " trying to recover \ n ");
int ff_j2k_dwt_init ( DWTContext * s , uint16_t border [ 2 ][ 2 ], int decomp_levels , int <nl> int i , j , lev = decomp_levels , maxlen , <nl> b [ 2 ][ 2 ]; <nl>  <nl> - if ( decomp_levels >= FF_DWT_MAX_DECLVLS ) <nl> + if (( unsigned ) decomp_levels >= FF_DWT_MAX_DECLVLS ) <nl> return AVERROR_INVALIDDATA ; <nl> s -> ndeclevels = decomp_levels ; <nl> s -> type = type ;
static int adpcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> /* Each EA ADPCM frame has a 12 - byte header followed by 30 - byte pieces , <nl> each coding 28 stereo samples . */ <nl>  <nl> + if ( avctx -> channels != 2 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> src += 4 ; // skip sample count ( already read ) <nl>  <nl> current_left_sample = ( int16_t ) bytestream_get_le16 (& src );
static int amr_read_packet ( AVFormatContext * s , <nl> AVPacket * pkt ) <nl> { <nl> AVCodecContext * enc = s -> streams [ 0 ]-> codec ; <nl> - int read , size , toc , mode ; <nl> + int read , size = 0 , toc , mode ; <nl>  <nl> if ( url_feof (& s -> pb )) <nl> {
static int vobsub_read_header ( AVFormatContext * s ) <nl>  <nl> while (* p == ' ') <nl> p ++; <nl> - av_log ( s , AV_LOG_DEBUG , " IDX stream [% d ] name =% s \ n ", st -> id , p ); <nl> + av_log ( s , AV_LOG_DEBUG , " IDX stream [% d ] name =% s \ n ", stream_id , p ); <nl> av_strlcpy ( alt , p , sizeof ( alt )); <nl> header_parsed = 1 ; <nl> 
static const AVClass writer_class = { <nl>  <nl> static void writer_close ( WriterContext ** wctx ) <nl> { <nl> - if (* wctx && (* wctx )-> writer -> uninit ) <nl> - (* wctx )-> writer -> uninit (* wctx ); <nl> + if (!* wctx ) <nl> + return ; <nl>  <nl> + if ((* wctx )-> writer -> uninit ) <nl> + (* wctx )-> writer -> uninit (* wctx ); <nl> av_freep (&((* wctx )-> priv )); <nl> av_freep ( wctx ); <nl> }
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> + av_shrink_packet ( pkt , CDXL_HEADER_SIZE + ret ); <nl> pkt -> stream_index = cdxl -> video_stream_index ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> pkt -> pos = pos ;
static void get_private_data ( OutputStream * os ) <nl> return ; <nl> os -> private_str = av_mallocz ( 2 * size + 1 ); <nl> if (! os -> private_str ) <nl> - return ; <nl> + goto fail ; <nl> for ( i = 0 ; i < size ; i ++) <nl> snprintf (& os -> private_str [ 2 * i ], 3 , "% 02x ", ptr [ i ]); <nl> + fail : <nl> if ( ptr != codec -> extradata ) <nl> av_free ( ptr ); <nl> }
static av_always_inline av_const int avpriv_isnan ( double x ) <nl> uint64_t v = av_double2int ( x ); <nl> if (( v & 0x7ff0000000000000 ) != 0x7ff0000000000000 ) <nl> return 0 ; <nl> - return v & 0x000fffffffffffff ; <nl> + return ( v & 0x000fffffffffffff ) && 1 ; <nl> } <nl>  <nl> # define isnan ( x ) \
static int rtp_mpegts_write_header ( AVFormatContext * s ) <nl> st -> time_base . num = 1 ; <nl> st -> time_base . den = 90000 ; <nl> st -> codec -> codec_id = AV_CODEC_ID_MPEG2TS ; <nl> - chain -> rtp_ctx = rtp_ctx ; <nl> rtp_ctx -> pb = s -> pb ; <nl> if (( ret = avformat_write_header ( rtp_ctx , NULL )) < 0 ) <nl> goto fail ; <nl> - rtp_ctx = NULL ; <nl> + chain -> rtp_ctx = rtp_ctx ; <nl>  <nl> return 0 ; <nl> 
void ff_rtp_send_h263 ( AVFormatContext * s1 , const uint8_t * buf1 , int size ) <nl>  <nl> while ( size > 0 ) { <nl> q = s -> buf ; <nl> - if (( buf1 [ 0 ] == 0 ) && ( buf1 [ 1 ] == 0 )) { <nl> + if ( size >= 2 && ( buf1 [ 0 ] == 0 ) && ( buf1 [ 1 ] == 0 )) { <nl> * q ++ = 0x04 ; <nl> buf1 += 2 ; <nl> size -= 2 ;
static void search_for_ms_mips ( AACEncContext * s , ChannelElement * cpe ) <nl> # endif /* HAVE_INLINE_ASM */ <nl>  <nl> void ff_aac_coder_init_mips ( AACEncContext * c ) { <nl> -# if HAVE_INLINE_ASM <nl> +# if 0 // HAVE_INLINE_ASM <nl> AACCoefficientsEncoder * e = c -> coder ; <nl> int option = c -> options . aac_coder ; <nl> 
static av_cold int truespeech_decode_init ( AVCodecContext * avctx ) <nl> { <nl> // TSContext * c = avctx -> priv_data ; <nl>  <nl> + if ( avctx -> channels != 1 ) { <nl> + av_log_ask_for_sample ( avctx , " Unsupported channel count : % d \ n ", avctx -> channels ); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ; <nl> return 0 ; <nl> }
static inline int wnv1_get_code ( WNV1Context * w , int base_value ) <nl> if ( v == 15 ) <nl> return ff_reverse [ get_bits (& w -> gb , 8 - w -> shift )]; <nl> else <nl> - return base_value + (( v - 7 ) << w -> shift ); <nl> + return base_value + (( v - 7U ) << w -> shift ); <nl> } <nl>  <nl> static int decode_frame ( AVCodecContext * avctx ,
static int bfi_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> return ret ; <nl>  <nl> pkt -> pts = bfi -> video_frame ; <nl> - bfi -> video_frame += ret / bfi -> video_size ; <nl> + bfi -> video_frame += bfi -> video_size ? ret / bfi -> video_size : 1 ; <nl>  <nl> /* One less frame to read . A cursory decrement . */ <nl> bfi -> nframes --;
static int init_image ( TiffContext * s , AVFrame * frame ) <nl> case 161 : <nl> s -> avctx -> pix_fmt = s -> le ? AV_PIX_FMT_GRAY16LE : AV_PIX_FMT_GRAY16BE ; <nl> break ; <nl> + case 162 : <nl> + s -> avctx -> pix_fmt = AV_PIX_FMT_YA8 ; <nl> + break ; <nl> case 322 : <nl> s -> avctx -> pix_fmt = s -> le ? AV_PIX_FMT_YA16LE : AV_PIX_FMT_YA16BE ; <nl> break ;
static void update_stream_timings ( AVFormatContext * ic ) <nl> end_time1 = av_rescale_q_rnd ( st -> duration , st -> time_base , <nl> AV_TIME_BASE_Q , <nl> AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX ); <nl> - if ( end_time1 != AV_NOPTS_VALUE && start_time1 <= INT64_MAX - end_time1 ) { <nl> + if ( end_time1 != AV_NOPTS_VALUE && ( end_time1 > 0 ? start_time1 <= INT64_MAX - end_time1 : start_time1 >= INT64_MIN - end_time1 )) { <nl> end_time1 += start_time1 ; <nl> end_time = FFMAX ( end_time , end_time1 ); <nl> }
static int opus_header ( AVFormatContext * avf , int idx ) <nl> /* gain = AV_RL16 ( packet + 16 );*/ <nl> /* channel_map = AV_RL8 ( packet + 18 );*/ <nl>  <nl> + av_freep (& st -> codecpar -> extradata ); <nl> if ( ff_alloc_extradata ( st -> codecpar , os -> psize )) <nl> return AVERROR ( ENOMEM ); <nl> 
static int get_video_frame ( VideoState * is , AVFrame * frame , int64_t * pts , AVPacke <nl> return 0 ; <nl> } <nl>  <nl> - avcodec_decode_video2 ( is -> video_st -> codec , frame , & got_picture , pkt ); <nl> + if ( avcodec_decode_video2 ( is -> video_st -> codec , frame , & got_picture , pkt ) < 0 ) <nl> + return - 1 ; <nl>  <nl> if ( got_picture ) { <nl> int ret = 1 ;
void mpeg1_init_vlc ( MpegEncContext * s ) <nl> static int done = 0 ; <nl>  <nl> if (! done ) { <nl> + done = 1 ; <nl>  <nl> init_vlc (& dc_lum_vlc , 9 , 12 , <nl> vlc_dc_lum_bits , 1 , 1 ,
static int load_input_picture ( MpegEncContext * s , const AVFrame * pic_arg ) <nl> EDGE_BOTTOM ); <nl> } <nl> } <nl> + emms_c (); <nl> } <nl> } <nl> ret = av_frame_copy_props ( pic -> f , pic_arg );
static int dnxhd_encode_rdo ( AVCodecContext * avctx , DNXHDEncContext * ctx ) <nl> last_higher = FFMAX ( lambda , last_higher ); <nl> if ( last_lower != INT_MAX ) <nl> lambda = ( lambda + last_lower )>> 1 ; <nl> + else if (( int64_t ) lambda + up_step > INT_MAX ) <nl> + return - 1 ; <nl> else <nl> lambda += up_step ; <nl> - up_step *= 5 ; <nl> + up_step = FFMIN (( int64_t ) up_step * 5 , INT_MAX ); <nl> down_step = 1 << LAMBDA_FRAC_BITS ; <nl> } <nl> }
static int64_t mmsh_seek ( URLContext * h , int64_t pos , int whence ) <nl> MMSContext * mms = & mmsh -> mms ; <nl>  <nl> if ( pos == 0 && whence == SEEK_CUR ) <nl> - return mms -> asf_header_read_size + mms -> remaining_in_len + mmsh -> chunk_seq * mms -> asf_packet_len ; <nl> + return mms -> asf_header_read_size + mms -> remaining_in_len + mmsh -> chunk_seq * ( int64_t ) mms -> asf_packet_len ; <nl> return AVERROR ( ENOSYS ); <nl> } <nl> 
static void vc1_decode_p_blocks ( VC1Context * v ) <nl> if ( s -> mb_y != s -> start_mb_y ) ff_draw_horiz_band ( s , ( s -> mb_y - 1 ) * 16 , 16 ); <nl> s -> first_slice_line = 0 ; <nl> } <nl> - if ( apply_loop_filter ) { <nl> + if ( apply_loop_filter && v -> fcm == PROGRESSIVE ) { <nl> s -> mb_x = 0 ; <nl> ff_init_block_index ( s ); <nl> for (; s -> mb_x < s -> mb_width ; s -> mb_x ++) {
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl> int offset = ( get_bits_count (& gb )+ 7 )>> 3 ; <nl> uint8_t * buf ; <nl>  <nl> - if (( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> + if ( watermark_height <= 0 || ( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> return - 1 ; <nl>  <nl> buf = av_malloc ( buf_len );
static int mxf_read_generic_descriptor ( void * arg , AVIOContext * pb , int tag , int <nl> default : <nl> /* Private uid used by SONY C0023S01 . mxf */ <nl> if ( IS_KLV_KEY ( uid , mxf_sony_mpeg4_extradata )) { <nl> + av_free ( descriptor -> extradata ); <nl> + descriptor -> extradata_size = 0 ; <nl> descriptor -> extradata = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! descriptor -> extradata ) <nl> return AVERROR ( ENOMEM );
int ff_hevc_decode_nal_vps ( HEVCContext * s ) <nl> if ( get_bits_left ( gb ) < 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , <nl> " Overread VPS by % d bits \ n ", - get_bits_left ( gb )); <nl> - goto err ; <nl> + if ( s -> vps_list [ vps_id ]) <nl> + goto err ; <nl> } <nl>  <nl> if ( s -> vps_list [ vps_id ] &&
static int gxf_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> gxf -> flt_entries_nb + 500 , <nl> sizeof (* gxf -> flt_entries ))) < 0 ) { <nl> gxf -> flt_entries_nb = 0 ; <nl> + gxf -> nb_fields = 0 ; <nl> av_log ( s , AV_LOG_ERROR , " could not reallocate flt entries \ n "); <nl> return err ; <nl> }
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl> memset (& h -> mb , 0 , sizeof ( h -> mb )); <nl> memset (& h -> mb_luma_dc , 0 , sizeof ( h -> mb_luma_dc )); <nl> memset (& h -> mb_padding , 0 , sizeof ( h -> mb_padding )); <nl> + memset (& h -> cur_pic , 0 , sizeof ( h -> cur_pic )); <nl>  <nl> h -> avctx = dst ; <nl> h -> DPB = NULL ;
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl>  <nl> x265pic . pts = pic -> pts ; <nl> x265pic . bitDepth = av_pix_fmt_desc_get ( avctx -> pix_fmt )-> comp [ 0 ]. depth_minus1 + 1 ; <nl> + <nl> + x265pic . sliceType = pic -> pict_type == AV_PICTURE_TYPE_I ? X265_TYPE_I : <nl> + pic -> pict_type == AV_PICTURE_TYPE_P ? X265_TYPE_P : <nl> + pic -> pict_type == AV_PICTURE_TYPE_B ? X265_TYPE_B : <nl> + X265_TYPE_AUTO ; <nl> } <nl>  <nl> ret = x265_encoder_encode ( ctx -> encoder , & nal , & nnal ,
# include < stddef . h > <nl> # include < stdint . h > <nl>  <nl> - typedef int16_t dwtcoef ; <nl> + typedef int32_t dwtcoef ; <nl>  <nl> enum VC2TransformType { <nl> VC2_TRANSFORM_9_7 = 0 , /* Deslauriers - Dubuc ( 9 , 7 ) */
av_cold void ff_cavsdsp_init_x86 ( CAVSDSPContext * c , AVCodecContext * avctx ) <nl> { <nl> av_unused int cpu_flags = av_get_cpu_flags (); <nl>  <nl> - cavsdsp_init_mmx ( c , avctx ); <nl> + if ( X86_MMX ( cpu_flags )) <nl> + cavsdsp_init_mmx ( c , avctx ); <nl> + <nl> # if HAVE_AMD3DNOW_INLINE <nl> if ( INLINE_AMD3DNOW ( cpu_flags )) <nl> cavsdsp_init_3dnow ( c , avctx );
static int read_tfra ( MOVContext * mov , AVIOContext * f ) <nl> } <nl> for ( i = 0 ; i < index -> item_count ; i ++) { <nl> int64_t time , offset ; <nl> + <nl> + if ( avio_feof ( f )) { <nl> + index -> item_count = 0 ; <nl> + av_freep (& index -> items ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( version == 1 ) { <nl> time = avio_rb64 ( f ); <nl> offset = avio_rb64 ( f );
static int decode_init_thread_copy ( AVCodecContext * avctx ) <nl> memset ( h -> sps_buffers , 0 , sizeof ( h -> sps_buffers )); <nl> memset ( h -> pps_buffers , 0 , sizeof ( h -> pps_buffers )); <nl>  <nl> + h -> avctx = avctx ; <nl> h -> rbsp_buffer [ 0 ] = NULL ; <nl> h -> rbsp_buffer [ 1 ] = NULL ; <nl> h -> rbsp_buffer_size [ 0 ] = 0 ;
int ff_spatial_idwt_init2 ( DWTContext * d , IDWTELEM * buffer , int width , int height <nl> d -> vertical_compose_l0 = ( void *) vertical_compose_fidelityiL0 ; <nl> d -> vertical_compose_h0 = ( void *) vertical_compose_fidelityiH0 ; <nl> d -> horizontal_compose = horizontal_compose_fidelityi ; <nl> + d -> support = 0 ; // not really used <nl> break ; <nl> case DWT_DIRAC_DAUB9_7 : <nl> d -> spatial_compose = spatial_compose_daub97i_dy ;
static int unpack_vlcs ( Vp3DecodeContext * s , GetBitContext * gb , <nl> if ( blocks_ended ) <nl> dct_tokens [ j ++] = blocks_ended << 2 ; <nl>  <nl> - while ( coeff_i < num_coeffs ) { <nl> + while ( coeff_i < num_coeffs && get_bits_left ( gb ) > 0 ) { <nl> /* decode a VLC into a token */ <nl> token = get_vlc2 ( gb , vlc_table , 5 , 3 ); <nl> /* use the token to get a zero run , a coefficient , and an eob run */
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> av_log ( s , AV_LOG_ERROR , " No seek table at given position \ n "); <nl> return ; <nl> } <nl> + if ( size > INT_MAX / 10 || size <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Seek table size is invalid \ n "); <nl> + return ; <nl> + } <nl> if (!( buf = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ))) <nl> return ; <nl> avio_read ( s -> pb , buf , size );
static int query_formats ( AVFilterContext * ctx ) <nl> EvalContext * eval = ctx -> priv ; <nl> enum AVSampleFormat sample_fmts [] = { AV_SAMPLE_FMT_DBL , AV_SAMPLE_FMT_NONE }; <nl> int64_t chlayouts [] = { eval -> chlayout , - 1 }; <nl> + int sample_rates [] = { eval -> sample_rate , - 1 }; <nl>  <nl> avfilter_set_common_sample_formats ( ctx , avfilter_make_format_list ( sample_fmts )); <nl> ff_set_common_channel_layouts ( ctx , avfilter_make_format64_list ( chlayouts )); <nl> + ff_set_common_samplerates ( ctx , avfilter_make_format_list ( sample_rates )); <nl>  <nl> return 0 ; <nl> }
static int theora_decode_header ( AVCodecContext * avctx , GetBitContext * gb ) <nl>  <nl> fps . num = get_bits_long ( gb , 32 ); <nl> fps . den = get_bits_long ( gb , 32 ); <nl> - if ( fps . num && fps . den ) { <nl> + if ( fps . num > 0 && fps . den > 0 ) { <nl> av_reduce (& avctx -> time_base . num , & avctx -> time_base . den , <nl> fps . den , fps . num , 1 << 30 ); <nl> }
void ff_ivi_process_empty_tile ( AVCodecContext * avctx , IVIBandDesc * band , <nl> if ( band -> inherit_qdelta && ref_mb ) <nl> mb -> q_delta = ref_mb -> q_delta ; <nl>  <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> /* motion vector inheritance */ <nl> if ( mv_scale ) { <nl> mb -> mv_x = ivi_scale_mv ( ref_mb -> mv_x , mv_scale );
void ffserver_parse_acl_row ( FFServerStream * stream , FFServerStream * feed , <nl> } <nl>  <nl> nacl = av_mallocz ( sizeof (* nacl )); <nl> + if (! nacl ) { <nl> + fprintf ( stderr , " Failed to allocate FFServerIPAddressACL \ n "); <nl> + goto bail ; <nl> + } <nl> + <nl> naclp = 0 ; <nl>  <nl> acl . next = 0 ;
static void check_luma_dc_wht ( void ) <nl> } <nl>  <nl> # define SRC_BUF_STRIDE 32 <nl> -# define SRC_BUF_SIZE (( size + 5 ) * SRC_BUF_STRIDE ) <nl> +# define SRC_BUF_SIZE ((( size << ( size < 16 )) + 5 ) * SRC_BUF_STRIDE ) <nl> // The mc subpixel interpolation filter needs the 2 previous pixels in either <nl> // direction , the + 1 is to make sure the actual load addresses always are <nl> // unaligned .
int ff_mpeg_update_thread_context ( AVCodecContext * dst , <nl> # define UPDATE_PICTURE ( pic )\ <nl> do {\ <nl> ff_mpeg_unref_picture ( s , & s -> pic );\ <nl> - if ( s1 -> pic . f -> buf [ 0 ])\ <nl> + if ( s1 -> pic . f && s1 -> pic . f -> buf [ 0 ])\ <nl> ret = ff_mpeg_ref_picture ( s , & s -> pic , & s1 -> pic );\ <nl> else \ <nl> ret = update_picture_tables (& s -> pic , & s1 -> pic );\
static int vp3_decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> last_frame = s -> golden_frame ; <nl> s -> last_frame . type = FF_BUFFER_TYPE_COPY ; <nl> + ff_thread_report_progress (& s -> last_frame , INT_MAX , 0 ); <nl> } <nl> } <nl> 
int av_write_trailer ( AVFormatContext * s ) <nl> if ( s -> oformat -> write_trailer ) <nl> ret = s -> oformat -> write_trailer ( s ); <nl>  <nl> - if (!( s -> oformat -> flags & AVFMT_NOFILE )) <nl> - avio_flush ( s -> pb ); <nl> - <nl> fail : <nl> if ( s -> pb ) <nl> avio_flush ( s -> pb );
static int speex_header ( AVFormatContext * s , int idx ) { <nl> if ( frames_per_packet ) <nl> spxp -> packet_size *= frames_per_packet ; <nl>  <nl> - ff_alloc_extradata ( st -> codec , os -> psize ); <nl> + if ( ff_alloc_extradata ( st -> codec , os -> psize ) < 0 ) <nl> + return AVERROR ( ENOMEM ); <nl> memcpy ( st -> codec -> extradata , p , st -> codec -> extradata_size ); <nl>  <nl> avpriv_set_pts_info ( st , 64 , 1 , st -> codec -> sample_rate );
static int av_encode ( AVFormatContext ** output_files , <nl> break ; <nl> case CODEC_TYPE_VIDEO : <nl> data_size = ( ist -> st -> codec . width * ist -> st -> codec . height * 3 ) / 2 ; <nl> + /* XXX : allocate picture correctly */ <nl> + memset (& picture , 0 , sizeof ( picture )); <nl> ret = avcodec_decode_video (& ist -> st -> codec , <nl> & picture , & got_picture , ptr , len ); <nl> ist -> st -> quality = picture . quality ;
static inline int decode_subframe ( FLACContext * s , int channel ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( wasted ) { <nl> + if ( wasted && wasted < 32 ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> decoded [ i ] = ( unsigned ) decoded [ i ] << wasted ;
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl>  <nl> flag = 0 ; <nl>  <nl> - if ( state * 4ULL > 0xFF || i >= size ) <nl> + if (( uint64_t ) state > 0xFF / 4 || i >= size ) <nl> continue ; <nl>  <nl> pfx = (( state + 8 ) >> 5 ) + ( state ? ff_clz ( state ): 32 ) - 24 ;
static void term_init ( void ) <nl> # if HAVE_TERMIOS_H <nl> if (! run_as_daemon ){ <nl> struct termios tty ; <nl> - <nl> +# if HAVE_ISATTY <nl> + if ( isatty ( 0 ) && isatty ( 2 )) <nl> +# endif <nl> if ( tcgetattr ( 0 , & tty ) == 0 ) { <nl> oldtty = tty ; <nl> restore_tty = 1 ;
int ff_msmpeg4_decode_block ( MpegEncContext * s , int16_t * block , <nl> if ( level < 0 ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " dc overflow - block : % d qscale : % d //\ n ", n , s -> qscale ); <nl> if ( s -> inter_intra_pred ) level = 0 ; <nl> - else return - 1 ; <nl> } <nl> if ( n < 4 ) { <nl> rl = & ff_rl_table [ s -> rl_table_index ];
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> } <nl> st -> info -> duration_count ++; <nl> // ignore the first 4 values , they might have some random jitter <nl> - if ( st -> info -> duration_count > 3 ) <nl> + if ( st -> info -> duration_count > 3 && is_relative ( pkt -> dts ) == is_relative ( last )) <nl> st -> info -> duration_gcd = av_gcd ( st -> info -> duration_gcd , duration ); <nl> } <nl> if ( pkt -> dts != AV_NOPTS_VALUE )
av_cold int vaapi_device_init ( const char * device ) <nl> { <nl> int err ; <nl>  <nl> + av_buffer_unref (& hw_device_ctx ); <nl> + <nl> err = av_hwdevice_ctx_create (& hw_device_ctx , AV_HWDEVICE_TYPE_VAAPI , <nl> device , NULL , 0 ); <nl> if ( err < 0 ) {
int av_image_check_sar ( unsigned int w , unsigned int h , AVRational sar ) <nl> { <nl> int64_t scaled_dim ; <nl>  <nl> - if (! sar . den ) <nl> + if ( sar . den <= 0 || sar . num < 0 ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> if (! sar . num || sar . num == sar . den )
static int svq1_decode_frame ( AVCodecContext * avctx , <nl> av_dlog ( s -> avctx , " Error in svq1_decode_frame_header % i \ n ", result ); <nl> return result ; <nl> } <nl> + avcodec_set_dimensions ( avctx , s -> width , s -> height ); <nl>  <nl> // FIXME this avoids some confusion for " B frames " without 2 references <nl> // this should be removed after libavcodec can handle more flexible picture types & ordering
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> if (!( buf = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ))) <nl> return ; <nl> avio_read ( s -> pb , buf , size ); <nl> + memset ( buf + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + <nl> init_get_bits (& gb , buf , size * 8 ); <nl> size = gb_get_v (& gb ); <nl> if ( size > UINT_MAX / 4 || size > c -> samples / 1152 ){
decode_intra_mb : <nl> } <nl>  <nl> // The pixels are stored in the same order as levels in h -> mb array . <nl> + if (( int ) ( h -> cabac . bytestream_end - ptr ) < mb_size ) <nl> + return - 1 ; <nl> memcpy ( h -> mb , ptr , mb_size ); ptr += mb_size ; <nl>  <nl> ff_init_cabac_decoder (& h -> cabac , ptr , h -> cabac . bytestream_end - ptr );
static void free_geotags ( TiffContext * const s ) <nl> av_freep (& s -> geotags [ i ]. val ); <nl> } <nl> av_freep (& s -> geotags ); <nl> + s -> geotag_count = 0 ; <nl> } <nl>  <nl> # define RET_GEOKEY ( TYPE , array , element )\
static int smacker_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> smk -> cur_frame ++; <nl> smk -> nextpos = avio_tell ( s -> pb ); <nl> } else { <nl> - if ( smk -> stream_id [ smk -> curstream ] < 0 ) <nl> + if ( smk -> stream_id [ smk -> curstream ] < 0 || ! smk -> bufs [ smk -> curstream ]) <nl> return AVERROR_INVALIDDATA ; <nl> if ( av_new_packet ( pkt , smk -> buf_sizes [ smk -> curstream ])) <nl> return AVERROR ( ENOMEM );
const uint8_t ff_png_pass_mask [ NB_PASSES ] = { <nl>  <nl> void * ff_png_zalloc ( void * opaque , unsigned int items , unsigned int size ) <nl> { <nl> - if ( items >= UINT_MAX / size ) <nl> - return NULL ; <nl> - return av_malloc ( items * size ); <nl> + return av_mallocz_array ( items , size ); <nl> } <nl>  <nl> void ff_png_zfree ( void * opaque , void * ptr )
static void set_frag_stream ( MOVFragmentIndex * frag_index , int id ) <nl> static MOVFragmentStreamInfo * get_current_frag_stream_info ( <nl> MOVFragmentIndex * frag_index ) <nl> { <nl> + if ( frag_index -> current < 0 || <nl> + frag_index -> current >= frag_index -> nb_items ) <nl> + return NULL ; <nl> + <nl> MOVFragmentIndexItem * item = & frag_index -> item [ frag_index -> current ]; <nl> if ( item -> current >= 0 && item -> current < item -> nb_stream_info ) <nl> return & item -> stream_info [ item -> current ];
static int wma_decode_block ( WMACodecContext * s ) <nl> coef escape coding */ <nl> total_gain = 1 ; <nl> for (;;) { <nl> + if ( get_bits_left (& s -> gb ) < 7 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " total_gain overread \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> a = get_bits (& s -> gb , 7 ); <nl> total_gain += a ; <nl> if ( a != 127 )
static inline int l3_unscale ( int value , int exponent ) <nl> # endif <nl> if ( e > ( SUINT ) 31 ) <nl> return 0 ; <nl> - m = ( m + ( 1 << ( e - 1 ))) >> e ; <nl> + m = ( m + (( 1U << e )>> 1 )) >> e ; <nl>  <nl> return m ; <nl> }
static int pcm_read_header ( AVFormatContext * s ) <nl> av_log ( s , AV_LOG_ERROR , <nl> " Invalid sample_rate found in mime_type \"% s \"\ n ", <nl> mime_type ); <nl> + av_freep (& mime_type ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> st -> codecpar -> sample_rate = rate ; <nl> static int pcm_read_header ( AVFormatContext * s ) <nl> st -> codecpar -> channels = channels ; <nl> } <nl> } <nl> + av_freep (& mime_type ); <nl>  <nl> st -> codecpar -> bits_per_coded_sample = <nl> av_get_bits_per_sample ( st -> codecpar -> codec_id );
static int read_random ( uint32_t * dst , const char * file ) <nl>  <nl> if ( fd == - 1 ) <nl> return - 1 ; <nl> -# if HAVE_FCNTL && defined ( O_NONBLOCK ) <nl> - if ( fcntl ( fd , F_SETFL , fcntl ( fd , F_GETFL ) | O_NONBLOCK ) != - 1 ) <nl> -# endif <nl> err = read ( fd , dst , sizeof (* dst )); <nl> close ( fd ); <nl> 
static OutputStream * new_video_stream ( OptionsContext * o , AVFormatContext * oc , in <nl> } <nl> /* FIXME realloc failure */ <nl> video_enc -> rc_override = <nl> - av_realloc ( video_enc -> rc_override , <nl> - sizeof ( RcOverride ) * ( i + 1 )); <nl> + av_realloc_array ( video_enc -> rc_override , <nl> + i + 1 , sizeof ( RcOverride )); <nl> video_enc -> rc_override [ i ]. start_frame = start ; <nl> video_enc -> rc_override [ i ]. end_frame = end ; <nl> if ( q > 0 ) {
static void unpack_alpha ( GetBitContext * gb , uint16_t * dst , int num_coeffs , <nl> dst [ idx ++] = alpha_val >> 6 ; <nl> else <nl> dst [ idx ++] = ( alpha_val << 2 ) | ( alpha_val >> 6 ); <nl> - if ( idx == num_coeffs - 1 ) <nl> + if ( idx >= num_coeffs - 1 ) <nl> break ; <nl> } while ( get_bits1 ( gb )); <nl> val = get_bits ( gb , 4 );
static int mov_read_keys ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> avio_skip ( pb , 4 ); <nl> count = avio_rb32 ( pb ); <nl> - if ( count > UINT_MAX / sizeof (* c -> meta_keys )) { <nl> + if ( count > UINT_MAX / sizeof (* c -> meta_keys ) - 1 ) { <nl> av_log ( c -> fc , AV_LOG_ERROR , <nl> " The ' keys ' atom with the invalid key count : % d \ n ", count ); <nl> return AVERROR_INVALIDDATA ;
static int smacker_decode_header_tree ( SmackVContext * smk , GetBitContext * gb , int <nl> huff . maxlength = 0 ; <nl> huff . current = 0 ; <nl> huff . values = av_mallocz ( huff . length * sizeof ( int )); <nl> + if (! huff . values ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( smacker_decode_bigtree ( gb , & huff , & ctx ) < 0 ) <nl> err = - 1 ;
static int decode_entropy_coded_image ( WebPContext * s , enum ImageRole role , <nl> length = offset + get_bits (& s -> gb , extra_bits ) + 1 ; <nl> } <nl> prefix_code = huff_reader_get_symbol (& hg [ HUFF_IDX_DIST ], & s -> gb ); <nl> - if ( prefix_code > 39 ) { <nl> + if ( prefix_code > 39U ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , <nl> " distance prefix code too large : % d \ n ", prefix_code ); <nl> return AVERROR_INVALIDDATA ;
int av_dict_set ( AVDictionary ** pm , const char * key , const char * value , <nl> m = * pm = av_mallocz ( sizeof (* m )); <nl>  <nl> if ( tag ) { <nl> - if ( flags & AV_DICT_DONT_OVERWRITE ) <nl> + if ( flags & AV_DICT_DONT_OVERWRITE ) { <nl> + if ( flags & AV_DICT_DONT_STRDUP_KEY ) av_free ( key ); <nl> + if ( flags & AV_DICT_DONT_STRDUP_VAL ) av_free ( value ); <nl> return 0 ; <nl> + } <nl> if ( flags & AV_DICT_APPEND ) <nl> oldval = tag -> value ; <nl> else
typedef struct { <nl>  <nl> typedef struct { <nl> int64_t frames_hdr_strm ; <nl> - int audio_strm_length ; <nl> + int64_t audio_strm_length ; <nl> int packet_count ; <nl> int entry ; <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> for ( i = 0 ; i < SUBFRAMES ; i ++) <nl> put_subframe ( c , i ); <nl>  <nl> + <nl> + for ( i = put_bits_count (& c -> pb ); i < 8 * c -> frame_size ; i ++) <nl> + put_bits (& c -> pb , 1 , 0 ); <nl> + <nl> flush_put_bits (& c -> pb ); <nl>  <nl> avpkt -> pts = frame -> pts ;
int main ( int argc , char ** argv ) <nl> return 1 ; <nl> } <nl>  <nl> - frame -> pts = 0 ; <nl> + if ( frame ) <nl> + frame -> pts = 0 ; <nl> for (;;) { <nl> /* Compute current audio and video time . */ <nl> if ( audio_st )
static void mov_text_cleanup_ftab ( MovTextContext * m ) <nl>  <nl> static int mov_text_tx3g ( AVCodecContext * avctx , MovTextContext * m ) <nl> { <nl> - char * tx3g_ptr = avctx -> extradata ; <nl> + uint8_t * tx3g_ptr = avctx -> extradata ; <nl> int i , box_size , font_length ; <nl> int8_t v_align , h_align ; <nl> int style_fontID ;
int32_t ff_mlp_pack_output ( int32_t lossless_check_data , <nl> ( 1U << output_shift [ mat_ch ]); <nl> lossless_check_data ^= ( sample & 0xffffff ) << mat_ch ; <nl> if ( is32 ) <nl> - * data_32 ++ = sample << 8 ; <nl> + * data_32 ++ = sample * 256 ; <nl> else <nl> * data_16 ++ = sample >> 8 ; <nl> }
# include " cabac_functions . h " <nl> # include " hevc . h " <nl>  <nl> -# define CABAC_MAX_BIN 100 <nl> +# define CABAC_MAX_BIN 31 <nl>  <nl> /** <nl> * number of bin by SyntaxElement .
static int get_video_frame ( VideoState * is , AVFrame * frame , int64_t * pts , AVPacke <nl> is -> frame_last_dropped_pos = pkt -> pos ; <nl> is -> frame_last_dropped_pts = dpts ; <nl> is -> frame_drops_early ++; <nl> + av_frame_unref ( frame ); <nl> ret = 0 ; <nl> } <nl> }
static int decode_studio_vop_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> return 0 ; <nl>  <nl> s -> partitioned_frame = 0 ; <nl> + s -> interlaced_dct = 0 ; <nl> s -> decode_mb = mpeg4_decode_studio_mb ; <nl>  <nl> decode_smpte_tc ( ctx , gb );
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> h = ( buf [ 1 ] + 1 ) * 8 ; <nl> buf += 2 ; <nl>  <nl> + if ( avpkt -> size < 2 + w * h / 513 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( w != avctx -> width || h != avctx -> height ) { <nl> av_freep (& c -> frame_buffer ); <nl> av_freep (& c -> last_frame_buffer );
int ff_raw_audio_read_header ( AVFormatContext * s , <nl> st -> codec -> codec_type = AVMEDIA_TYPE_AUDIO ; <nl> st -> codec -> codec_id = s -> iformat -> value ; <nl> st -> need_parsing = AVSTREAM_PARSE_FULL ; <nl> + st -> start_time = 0 ; <nl> /* the parameters will be extracted from the compressed bitstream */ <nl>  <nl> return 0 ;
static int sap_write_close ( AVFormatContext * s ) <nl> url_fclose ( rtpctx -> pb ); <nl> av_metadata_free (& rtpctx -> streams [ 0 ]-> metadata ); <nl> av_metadata_free (& rtpctx -> metadata ); <nl> + av_free ( rtpctx -> streams [ 0 ]-> info ); <nl> av_free ( rtpctx -> streams [ 0 ]); <nl> av_free ( rtpctx ); <nl> s -> streams [ i ]-> priv_data = NULL ;
static av_cold int rl2_decode_end ( AVCodecContext * avctx ) <nl> { <nl> Rl2Context * s = avctx -> priv_data ; <nl>  <nl> - av_free ( s -> back_frame ); <nl> + av_freep (& s -> back_frame ); <nl>  <nl> return 0 ; <nl> }
static int mp3_read_probe ( AVProbeData * p ) <nl>  <nl> max_frames = 0 ; <nl> buf = buf0 ; <nl> - end = buf + p -> buf_size - sizeof ( uint32_t ); <nl> + end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl>  <nl> for (; buf < end ; buf = buf2 + 1 ) { <nl> buf2 = buf ;
static int decompress_p ( AVCodecContext * avctx , <nl> return ret ; <nl>  <nl> max += temp << 8 ; <nl> + if ( min > max ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> memset ( s -> blocks , 0 , sizeof (* s -> blocks ) * s -> nbcount ); <nl>  <nl> while ( min <= max ) {
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> memcpy ( c , h -> s . thread_context [ i ], sizeof ( MpegEncContext )); <nl> memset (& c -> s + 1 , 0 , sizeof ( H264Context ) - sizeof ( MpegEncContext )); <nl> c -> h264dsp = h -> h264dsp ; <nl> + c -> h264qpel = h -> h264qpel ; <nl> c -> sps = h -> sps ; <nl> c -> pps = h -> pps ; <nl> c -> pixel_shift = h -> pixel_shift ;
static void smc_decode_stream ( SmcContext * s ) <nl> row_ptr , image_size ); <nl> return ; <nl> } <nl> + if ( bytestream2_get_bytes_left (& s -> gb ) < 1 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " input too small \ n "); <nl> + return ; <nl> + } <nl>  <nl> opcode = bytestream2_get_byte (& s -> gb ); <nl> switch ( opcode & 0xF0 ) {
static int video_thread ( void * arg ) <nl> frame -> opaque = picref ; <nl> } <nl>  <nl> - if ( av_cmp_q ( tb , is -> video_st -> time_base )) { <nl> + if ( ret >= 0 && av_cmp_q ( tb , is -> video_st -> time_base )) { <nl> av_unused int64_t pts1 = pts_int ; <nl> pts_int = av_rescale_q ( pts_int , tb , is -> video_st -> time_base ); <nl> av_dlog ( NULL , " video_thread (): "
static int img_read_header ( AVFormatContext * s1 ) <nl> s -> img_last = last_index ; <nl> s -> img_number = first_index ; <nl> /* compute duration */ <nl> - st -> start_time = 0 ; <nl> - st -> duration = last_index - first_index + 1 ; <nl> + if (! s -> ts_from_file ) { <nl> + st -> start_time = 0 ; <nl> + st -> duration = last_index - first_index + 1 ; <nl> + } <nl> } <nl>  <nl> if ( s1 -> video_codec_id ) {
skip : <nl> // sign extension <nl> int32_t cts = ( avio_rb24 ( s -> pb ) + 0xff800000 ) ^ 0xff800000 ; <nl> pts = dts + cts ; <nl> - if ( cts < 0 ) { // dts are wrong <nl> + if ( cts < 0 && ! flv -> wrong_dts ) { // dts might be wrong <nl> flv -> wrong_dts = 1 ; <nl> av_log ( s , AV_LOG_WARNING , <nl> " Negative cts , previous timestamps might be wrong .\ n ");
int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , SwsFilter * dstFilter ) <nl> /* precalculate vertical scaler filter coefficients */ <nl> { <nl> const int filterAlign = <nl> - ( HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX ) && ( flags & SWS_ACCURATE_RND ) ? 2 : <nl> + ( HAVE_MMX && cpu_flags & AV_CPU_FLAG_MMX ) ? 2 : <nl> ( HAVE_ALTIVEC && cpu_flags & AV_CPU_FLAG_ALTIVEC ) ? 8 : <nl> 1 ; <nl> 
end : <nl> frame -> height = avctx -> height ; <nl> } <nl>  <nl> + if ( ret < 0 ) <nl> + av_frame_unref ( frame ); <nl> + <nl> return ret ; <nl> } <nl> 
static int mxf_read_seek ( AVFormatContext * s , int stream_index , int64_t sample_ti <nl> sample_time = FFMIN ( sample_time , source_track -> original_duration - 1 ); <nl> } <nl>  <nl> - if (( ret = mxf_edit_unit_absolute_offset ( mxf , t , sample_time , & sample_time , & seekpos , 1 )) << 0 ) <nl> + if (( ret = mxf_edit_unit_absolute_offset ( mxf , t , sample_time , & sample_time , & seekpos , 1 )) < 0 ) <nl> return ret ; <nl>  <nl> ff_update_cur_dts ( s , st , sample_time );
static int twolame_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> if ( ret < 0 ) // twolame error <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame ) { <nl> + avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame -> pts != AV_NOPTS_VALUE ) <nl> avpkt -> pts = frame -> pts - ff_samples_to_time_base ( avctx , avctx -> delay ); <nl> } else {
static void vda_decoder_callback ( void * vda_hw_ctx , <nl> vda_frame * new_frame ; <nl> vda_frame * queue_walker ; <nl>  <nl> - if (!( new_frame = av_mallocz ( sizeof ( vda_frame )))) <nl> + if (!( new_frame = av_mallocz ( sizeof (* new_frame )))) <nl> return ; <nl> new_frame -> next_frame = NULL ; <nl> new_frame -> cv_buffer = CVPixelBufferRetain ( image_buffer );
int ff_thread_get_buffer ( AVCodecContext * avctx , AVFrame * f ) <nl> p -> requested_frame = f ; <nl> p -> state = STATE_GET_BUFFER ; <nl> pthread_mutex_lock (& p -> progress_mutex ); <nl> - pthread_cond_signal (& p -> progress_cond ); <nl> + pthread_cond_broadcast (& p -> progress_cond ); <nl>  <nl> while ( p -> state != STATE_SETTING_UP ) <nl> pthread_cond_wait (& p -> progress_cond , & p -> progress_mutex );
# ifndef AVUTIL_MEM_INTERNAL_H <nl> # define AVUTIL_MEM_INTERNAL_H <nl>  <nl> +# include " avassert . h " <nl> +# include " mem . h " <nl> + <nl> static inline int ff_fast_malloc ( void * ptr , unsigned int * size , size_t min_size , int zero_realloc ) <nl> { <nl> void * val ;
static int decode_picture_header ( AVCodecContext * avctx , const uint8_t * buf , cons <nl> \ <nl> if ( q > switch_bits ) { /* exp golomb */ \ <nl> bits = exp_order - switch_bits + ( q << 1 ); \ <nl> - if ( bits > MIN_CACHE_BITS ) \ <nl> + if ( bits > FFMIN ( MIN_CACHE_BITS , 31 )) \ <nl> return AVERROR_INVALIDDATA ; \ <nl> val = SHOW_UBITS ( re , gb , bits ) - ( 1 << exp_order ) + \ <nl> (( switch_bits + 1 ) << rice_order ); \
static int http_read_stream ( URLContext * h , uint8_t * buf , int size ) <nl>  <nl> av_log ( NULL , AV_LOG_TRACE , " Chunked encoding data size : %" PRId64 "'\ n ", <nl> s -> chunksize ); <nl> - <nl> - if (! s -> chunksize ) <nl> + if ( s -> chunksize < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + else if (! s -> chunksize ) <nl> return 0 ; <nl> break ; <nl> }
static int matroska_ebmlnum_uint ( MatroskaDemuxContext * matroska , <nl> { <nl> ByteIOContext pb ; <nl> init_put_byte (& pb , data , size , 0 , NULL , NULL , NULL , NULL ); <nl> - return ebml_read_num ( matroska , & pb , 8 , num ); <nl> + return ebml_read_num ( matroska , & pb , FFMIN ( size , 8 ), num ); <nl> } <nl>  <nl> /*
static inline int wv_unpack_stereo ( WavpackFrameContext * s , GetBitContext * gb , <nl> } <nl>  <nl> if ( type == AV_SAMPLE_FMT_S16P ) { <nl> - if ( FFABS ( L ) + FFABS ( R ) > ( 1 << 19 )) { <nl> + if ( FFABS ( L ) + ( unsigned ) FFABS ( R ) > ( 1 << 19 )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " sample % d % d too large \ n ", L , R ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int channelmap_filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * bu <nl> if ( buf -> extended_data == buf -> data ) { <nl> buf -> extended_data = new_extended_data ; <nl> } else { <nl> - buf -> extended_data = new_extended_data ; <nl> av_free ( buf -> extended_data ); <nl> + buf -> extended_data = new_extended_data ; <nl> } <nl> } else if ( buf -> extended_data != buf -> data ) { <nl> av_free ( buf -> extended_data );
static int find_headers_search_validate ( FLACParseContext * fpc , int offset ) <nl> (* end_handle )-> offset = offset ; <nl> (* end_handle )-> link_penalty = av_malloc ( sizeof ( int ) * <nl> FLAC_MAX_SEQUENTIAL_HEADERS ); <nl> + if (!(* end_handle )-> link_penalty ) { <nl> + av_freep ( end_handle ); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> + <nl> for ( i = 0 ; i < FLAC_MAX_SEQUENTIAL_HEADERS ; i ++) <nl> (* end_handle )-> link_penalty [ i ] = FLAC_HEADER_NOT_PENALIZED_YET ; <nl> 
static int mxf_write_footer ( AVFormatContext * s ) <nl> AVIOContext * pb = s -> pb ; <nl> int err = 0 ; <nl>  <nl> + if (! mxf -> header_written || <nl> + ( s -> oformat == & ff_mxf_opatom_muxer && ! mxf -> body_partition_offset )) { <nl> + /* reason could be invalid options / not supported codec / out of memory */ <nl> + err = AVERROR_UNKNOWN ; <nl> + goto end ; <nl> + } <nl> + <nl> mxf -> duration = mxf -> last_indexed_edit_unit + mxf -> edit_units_count ; <nl>  <nl> mxf_write_klv_fill ( s );
int av_asrc_buffer_add_buffer ( AVFilterContext * ctx , <nl> int sample_fmt , int64_t channel_layout , int planar , <nl> int64_t pts , int av_unused flags ) <nl> { <nl> - uint8_t * data [ 8 ]; <nl> + uint8_t * data [ 8 ] = { 0 }; <nl> int linesize [ 8 ]; <nl> int nb_channels = av_get_channel_layout_nb_channels ( channel_layout ), <nl> nb_samples = buf_size / nb_channels / av_get_bytes_per_sample ( sample_fmt );
av_cold int ff_dcaadpcm_init ( DCAADPCMEncContext * s ) <nl> return - 1 ; <nl>  <nl> s -> private_data = av_malloc ( sizeof ( premultiplied_coeffs ) * DCA_ADPCM_VQCODEBOOK_SZ ); <nl> + if (! s -> private_data ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> precalc ( s -> private_data ); <nl> return 0 ; <nl> }
static int read_header ( AVFormatContext * s ) <nl> return ret ; <nl> } <nl>  <nl> - avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + if ( vst -> index_entries ) <nl> + avio_seek ( pb , vst -> index_entries [ 0 ]. pos , SEEK_SET ); <nl> + else <nl> + avio_skip ( pb , 4 ); <nl>  <nl> bink -> current_track = - 1 ; <nl> return 0 ;
static void asfrtp_close_context ( PayloadContext * asf ) <nl> { <nl> ffio_free_dyn_buf (& asf -> pktbuf ); <nl> av_freep (& asf -> buf ); <nl> - av_free ( asf ); <nl> } <nl>  <nl> # define RTP_ASF_HANDLER ( n , s , t ) \
int vc1_parse_frame_header_adv ( VC1Context * v , GetBitContext * gb ) <nl> int scale , shift , i ; /* for initializing LUT for intensity compensation */ <nl>  <nl> v -> numref = 0 ; <nl> + v -> fcm = 0 ; <nl> + v -> field_mode = 0 ; <nl> v -> p_frame_skipped = 0 ; <nl> if ( v -> second_field ) { <nl> v -> s . pict_type = ( v -> fptype & 1 ) ? AV_PICTURE_TYPE_P : AV_PICTURE_TYPE_I ;
static int mov_write_udta_tag ( ByteIOContext * pb , MOVMuxContext * mov , <nl> put_be32 ( pb , size + 8 ); <nl> put_tag ( pb , " udta "); <nl> put_buffer ( pb , buf , size ); <nl> - av_free ( buf ); <nl> } <nl> + av_free ( buf ); <nl>  <nl> return 0 ; <nl> }
static void apply_unsharp ( uint8_t * dst , int dst_stride , <nl>  <nl> int32_t res ; <nl> int x , y , z ; <nl> - const uint8_t * src2 ; <nl> + const uint8_t * src2 = NULL ; // silence a warning <nl>  <nl> if (! fp -> amount ) { <nl> if ( dst_stride == src_stride )
static int check_n_master ( AVCodecContext * avctx , int n_master , int bs_xover_band <nl> static int sbr_make_f_master ( AACContext * ac , SpectralBandReplication * sbr , <nl> SpectrumParameters * spectrum ) <nl> { <nl> - unsigned int temp , max_qmf_subbands ; <nl> + unsigned int temp , max_qmf_subbands = 0 ; <nl> unsigned int start_min , stop_min ; <nl> int k ; <nl> const int8_t * sbr_offset_ptr ;
static int tak_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> if ( s -> sample_shift [ chan ] > 0 ) <nl> for ( i = 0 ; i < s -> nb_samples ; i ++) <nl> - decoded [ i ] <<= s -> sample_shift [ chan ]; <nl> + decoded [ i ] *= 1 << s -> sample_shift [ chan ]; <nl> } <nl> } <nl> 
int ff_mjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> /* EOF */ <nl> if ( start_code < 0 ) { <nl> goto the_end ; <nl> - } else if ( unescaped_buf_size > ( 1U << 29 )) { <nl> + } else if ( unescaped_buf_size > ( 1U << 28 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " MJPEG packet 0x % x too big ( 0x % x / 0x % x ), corrupt data ?\ n ", <nl> start_code , unescaped_buf_size , buf_size ); <nl> return AVERROR_INVALIDDATA ;
static int v4l2_set_parameters ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl> struct v4l2_streamparm streamparm = { 0 }; <nl> struct v4l2_fract * tpf = & streamparm . parm . capture . timeperframe ; <nl> int i , ret ; <nl> - AVRational framerate_q ; <nl> + AVRational framerate_q ={ 0 }; <nl>  <nl> streamparm . type = V4L2_BUF_TYPE_VIDEO_CAPTURE ; <nl> 
int attribute_align_arg av_buffersink_get_frame_flags ( AVFilterContext * ctx , AVFr <nl>  <nl> if ( flags & AV_BUFFERSINK_FLAG_PEEK ) { <nl> cur_frame = *(( AVFrame **) av_fifo_peek2 ( buf -> fifo , 0 )); <nl> - av_frame_ref ( frame , cur_frame ); /* TODO check failure */ <nl> + if (( ret = av_frame_ref ( frame , cur_frame )) < 0 ) <nl> + return ret ; <nl> } else { <nl> av_fifo_generic_read ( buf -> fifo , & cur_frame , sizeof ( cur_frame ), NULL ); <nl> av_frame_move_ref ( frame , cur_frame );
void ff_lzw_decode_tail ( LZWState * p ) <nl>  <nl> if ( s -> mode == FF_LZW_GIF ) { <nl> while ( s -> bs > 0 ) { <nl> - if ( s -> pbuf + s -> bs >= s -> ebuf ) { <nl> + if ( s -> bs >= s -> ebuf - s -> pbuf ) { <nl> s -> pbuf = s -> ebuf ; <nl> break ; <nl> } else {
static inline int mxf_read_utf16_string ( AVIOContext * pb , int size , char ** str , i <nl> int ret ; <nl> size_t buf_size ; <nl>  <nl> - if ( size < 0 ) <nl> + if ( size < 0 || size > INT_MAX / 2 ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> buf_size = size + size / 2 + 1 ;
static opj_image_t * mj2_create_image ( AVCodecContext * avctx , opj_cparameters_t * p <nl>  <nl> img = opj_image_create ( numcomps , cmptparm , color_space ); <nl>  <nl> + if (! img ) <nl> + return NULL ; <nl> + <nl> // x0 , y0 is the top left corner of the image <nl> // x1 , y1 is the width , height of the reference grid <nl> img -> x0 = 0 ;
static int mxf_read_primer_pack ( void * arg , AVIOContext * pb , int tag , int size , U <nl> avpriv_request_sample ( pb , " Primer pack item length % d ", item_len ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> - if ( item_num > 65536 ) { <nl> + if ( item_num > 65536 || item_num < 0 ) { <nl> av_log ( mxf -> fc , AV_LOG_ERROR , " item_num % d is too large \ n ", item_num ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int ebml_read_binary ( AVIOContext * pb , int length , EbmlBin * bin ) <nl> bin -> pos = avio_tell ( pb ); <nl> if ( avio_read ( pb , bin -> data , length ) != length ) { <nl> av_freep (& bin -> data ); <nl> + bin -> size = 0 ; <nl> return AVERROR ( EIO ); <nl> } <nl> 
static void flush_dpb ( AVCodecContext * avctx ) <nl> h -> parse_context . overread_index = 0 ; <nl> h -> parse_context . index = 0 ; <nl> h -> parse_context . last_index = 0 ; <nl> + <nl> + free_tables ( h , 1 ); <nl> + h -> context_initialized = 0 ; <nl> } <nl>  <nl> int ff_init_poc ( H264Context * h , int pic_field_poc [ 2 ], int * pic_poc )
static int decode_block ( MJpegDecodeContext * s , int16_t * block , int component , <nl> av_log ( s -> avctx , AV_LOG_ERROR , " error dc \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> - val = val * quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> + val = val * ( unsigned ) quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> val = av_clip_int16 ( val ); <nl> s -> last_dc [ component ] = val ; <nl> block [ 0 ] = val ;
static int mxf_parse_physical_source_package ( MXFContext * mxf , MXFTrack * source_t <nl> break ; <nl>  <nl> /* the name of physical source package is name of the reel or tape */ <nl> - if ( physical_package -> name [ 0 ]) <nl> + if ( physical_package -> name && physical_package -> name [ 0 ]) <nl> av_dict_set (& st -> metadata , " reel_name ", physical_package -> name , 0 ); <nl>  <nl> /* the source timecode is calculated by adding the start_position of the sourceclip from the file source package track
static int read_header ( AVFormatContext * s , <nl> jvf -> audio_size = avio_rl32 ( pb ); <nl> jvf -> video_size = avio_rl32 ( pb ); <nl> jvf -> palette_size = avio_r8 ( pb ) ? 768 : 0 ; <nl> + jvf -> video_size = FFMIN ( FFMAX ( jvf -> video_size , 0 ), <nl> + INT_MAX - JV_PREAMBLE_SIZE - jvf -> palette_size ); <nl> if ( avio_r8 ( pb )) <nl> av_log ( s , AV_LOG_WARNING , " unsupported audio codec \ n "); <nl> jvf -> video_type = avio_r8 ( pb );
static int mpeg_decode_slice ( MpegEncContext * s , int mb_y , <nl> if ( mb_y == 0 && s -> codec_tag == AV_RL32 (" SLIF ")) { <nl> skip_bits1 (& s -> gb ); <nl> } else { <nl> - for (;;) { <nl> + while ( get_bits_left (& s -> gb ) > 0 ) { <nl> int code = get_vlc2 (& s -> gb , mbincr_vlc . table , MBINCR_VLC_BITS , 2 ); <nl> if ( code < 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " first mb_incr damaged \ n ");
static int vorbis_parse ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl>  <nl> if (! s -> vp && avctx -> extradata && avctx -> extradata_size ) { <nl> s -> vp = av_vorbis_parse_init ( avctx -> extradata , avctx -> extradata_size ); <nl> - if (! s -> vp ) <nl> - goto end ; <nl> } <nl> + if (! s -> vp ) <nl> + goto end ; <nl>  <nl> if (( duration = av_vorbis_parse_frame ( s -> vp , buf , buf_size )) >= 0 ) <nl> s1 -> duration = duration ;
static float voice_factor ( float * p_vector , float p_gain , <nl> AMRWB_SFR_SIZE ) * <nl> f_gain * f_gain ; <nl>  <nl> - return ( p_ener - f_ener ) / ( p_ener + f_ener ); <nl> + return ( p_ener - f_ener ) / ( p_ener + f_ener + 0 . 01 ); <nl> } <nl>  <nl> /**
static int dca_decode_frame ( AVCodecContext * avctx , void * data , <nl> } else { <nl> s -> channel_order_tab = dca_channel_reorder_nolfe_xch [ s -> amode ]; <nl> } <nl> + if ( s -> channel_order_tab [ s -> xch_base_channel ] < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> } else { <nl> channels = num_core_channels + !! s -> lfe ; <nl> s -> xch_present = 0 ; /* disable further xch processing */
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size , <nl> h -> workaround_bugs |= FF_BUG_TRUNCATED ; <nl>  <nl> if (!( h -> workaround_bugs & FF_BUG_TRUNCATED )) <nl> - while ( ptr [ dst_length - 1 ] == 0 && dst_length > 0 ) <nl> + while ( dst_length > 0 && ptr [ dst_length - 1 ] == 0 ) <nl> dst_length --; <nl> bit_length = ! dst_length ? 0 <nl> : ( 8 * dst_length -
int opt_default ( void * optctx , const char * opt , const char * arg ) <nl> # endif <nl> # if CONFIG_AVRESAMPLE <nl> rc_class = avresample_get_class (); <nl> - if ( av_opt_find (& rc_class , opt , NULL , 0 , <nl> - AV_OPT_SEARCH_CHILDREN | AV_OPT_SEARCH_FAKE_OBJ )) { <nl> + if (( o = av_opt_find (& rc_class , opt , NULL , 0 , <nl> + AV_OPT_SEARCH_CHILDREN | AV_OPT_SEARCH_FAKE_OBJ ))) { <nl> av_dict_set (& resample_opts , opt , arg , FLAGS ); <nl> consumed = 1 ; <nl> }
static void pop_output_configuration ( AACContext * ac ) { <nl> ac -> oc [ 1 ] = ac -> oc [ 0 ]; <nl> ac -> avctx -> channels = ac -> oc [ 1 ]. channels ; <nl> ac -> avctx -> channel_layout = ac -> oc [ 1 ]. channel_layout ; <nl> - } else { <nl> - ac -> avctx -> channels = 0 ; <nl> - ac -> avctx -> channel_layout = 0 ; <nl> } <nl> } <nl> }
static int daala_header ( AVFormatContext * s , int idx ) <nl> if ( hdr -> gpshift >= 32 ) { <nl> av_log ( s , AV_LOG_ERROR , " Too large gpshift % d (>= 32 ).\ n ", <nl> hdr -> gpshift ); <nl> + hdr -> gpshift = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> hdr -> gpmask = ( 1U << hdr -> gpshift ) - 1 ;
static int decode_frame ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( rbuf + buf_size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 ) { <nl> av_free ( rbuf );
static int decode_exp_vlc ( WMACodecContext * s , int ch ) <nl> } <nl> /* NOTE : this offset is the same as MPEG4 AAC ! */ <nl> last_exp += code - 60 ; <nl> - if (( unsigned ) last_exp + 60 > FF_ARRAY_ELEMS ( pow_tab )) { <nl> + if (( unsigned ) last_exp + 60 >= FF_ARRAY_ELEMS ( pow_tab )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Exponent out of range : % d \ n ", <nl> last_exp ); <nl> return - 1 ;
static int ogg_build_opus_headers ( AVCodecContext * avctx , <nl> static int ogg_write_header ( AVFormatContext * s ) <nl> { <nl> OGGContext * ogg = s -> priv_data ; <nl> - OGGStreamContext * oggstream ; <nl> + OGGStreamContext * oggstream = NULL ; <nl> int i , j ; <nl>  <nl> if ( ogg -> pref_size )
static int hls_decode_entry_wpp ( AVCodecContext * avctxt , void * input_ctb_row , int <nl>  <nl> if ( more_data < 0 ) { <nl> s -> tab_slice_address [ ctb_addr_rs ] = - 1 ; <nl> + avpriv_atomic_int_set (& s1 -> wpp_err , 1 ); <nl> + ff_thread_report_progress2 ( s -> avctx , ctb_row , thread , SHIFT_CTB_WPP ); <nl> return more_data ; <nl> } <nl> 
# include " mlz . h " <nl>  <nl> av_cold void ff_mlz_init_dict ( void * context , MLZ * mlz ) { <nl> - mlz -> dict = av_malloc_array ( TABLE_SIZE , sizeof (* mlz -> dict )); <nl> + mlz -> dict = av_mallocz_array ( TABLE_SIZE , sizeof (* mlz -> dict )); <nl>  <nl> mlz -> flush_code = FLUSH_CODE ; <nl> mlz -> current_dic_index_max = DIC_INDEX_INIT ;
static int dirac_decode_picture_header ( DiracContext * s ) <nl> get_buffer_with_edge ( s -> avctx , s -> ref_pics [ i ]-> avframe , AV_GET_BUFFER_FLAG_REF ); <nl> break ; <nl> } <nl> + <nl> + if (! s -> ref_pics [ i ]) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Reference could not be allocated \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> } <nl>  <nl> /* retire the reference frames that are not used anymore */
static int escape124_decode_frame ( AVCodecContext * avctx , <nl> cb_size = s -> num_superblocks << cb_depth ; <nl> } <nl> } <nl> + if ( s -> num_superblocks >= INT_MAX >> cb_depth ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Depth or num_superblocks are too large \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> av_freep (& s -> codebooks [ i ]. blocks ); <nl> s -> codebooks [ i ] = unpack_codebook (& gb , cb_depth , cb_size ); <nl> if (! s -> codebooks [ i ]. blocks )
 <nl> FFTContext * av_fft_init ( int nbits , int inverse ) <nl> { <nl> - FFTContext * s = av_malloc ( sizeof (* s )); <nl> + FFTContext * s = av_mallocz ( sizeof (* s )); <nl>  <nl> if ( s && ff_fft_init ( s , nbits , inverse )) <nl> av_freep (& s );
static int lag_decode_prob ( GetBitContext * gb , uint32_t * value ) <nl> } <nl>  <nl> val = get_bits_long ( gb , bits ); <nl> - val |= 1 << bits ; <nl> + val |= 1U << bits ; <nl>  <nl> * value = val - 1 ; <nl> 
static av_cold int vtenc_close ( AVCodecContext * avctx ) <nl>  <nl> if (! vtctx -> session ) return 0 ; <nl>  <nl> - VTCompressionSessionInvalidate ( vtctx -> session ); <nl> pthread_cond_destroy (& vtctx -> cv_sample_sent ); <nl> pthread_mutex_destroy (& vtctx -> lock ); <nl> CFRelease ( vtctx -> session );
static void sample_queue_push ( HintSampleQueue * queue , uint8_t * data , int size , <nl> return ; <nl> if (! queue -> samples || queue -> len >= queue -> size ) { <nl> HintSample * samples ; <nl> - samples = av_realloc ( queue -> samples , sizeof ( HintSample ) * ( queue -> size + 10 )); <nl> + samples = av_realloc_array ( queue -> samples , queue -> size + 10 , sizeof ( HintSample )); <nl> if (! samples ) <nl> return ; <nl> queue -> size += 10 ;
static int decode_codestream ( J2kDecoderContext * s ) <nl> } <nl>  <nl> marker = bytestream_get_be16 (& s -> buf ); <nl> + if ( s -> avctx -> debug & FF_DEBUG_STARTCODE ) <nl> + av_log ( s -> avctx , AV_LOG_DEBUG , " marker 0x %. 4X at pos 0x % x \ n ", marker , s -> buf - s -> buf_start - 4 ); <nl> oldbuf = s -> buf ; <nl>  <nl> if ( marker == J2K_SOD ){
static int adts_aac_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - return av_append_packet ( s -> pb , pkt , fsize - ADTS_HEADER_SIZE ); <nl> + ret = av_append_packet ( s -> pb , pkt , fsize - ADTS_HEADER_SIZE ); <nl> + if ( ret < 0 ) <nl> + av_packet_unref ( pkt ); <nl> + <nl> + return ret ; <nl> } <nl>  <nl> AVInputFormat ff_aac_demuxer = {
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> int err ; <nl>  <nl> if ( buf_index >= next_avc ) { <nl> - if ( buf_index >= buf_size ) break ; <nl> + if ( buf_index >= buf_size - h -> nal_length_size ) break ; <nl> nalsize = 0 ; <nl> for ( i = 0 ; i < h -> nal_length_size ; i ++) <nl> nalsize = ( nalsize << 8 ) | buf [ buf_index ++];
int av_tempfile ( const char * prefix , char ** filename , int log_offset , void * log_c <nl> if ( fd < 0 ) { <nl> int err = AVERROR ( errno ); <nl> av_log (& file_log_ctx , AV_LOG_ERROR , " ff_tempfile : Cannot open temporary file % s \ n ", * filename ); <nl> + av_freep ( filename ); <nl> return err ; <nl> } <nl> return fd ; /* success */
static int tgv_decode_frame ( AVCodecContext * avctx , <nl> frame -> pict_type = AV_PICTURE_TYPE_I ; <nl>  <nl> if (! s -> frame_buffer && <nl> - !( s -> frame_buffer = av_malloc ( s -> width * s -> height ))) <nl> + !( s -> frame_buffer = av_mallocz ( s -> width * s -> height ))) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> if ( unpack ( buf , buf_end , s -> frame_buffer , s -> avctx -> width , s -> avctx -> height ) < 0 ) {
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> bytestream2_init (& gbc , avpkt -> data , avpkt -> size ); <nl> if ( bytestream2_get_bytes_left (& gbc ) >= 552 <nl> - && ! check_header ( gbc . buffer , bytestream2_get_bytes_left (& gbc )) <nl> && check_header ( gbc . buffer + 512 , bytestream2_get_bytes_left (& gbc ) - 512 ) <nl> ) <nl> bytestream2_skip (& gbc , 512 );
static int decode_dds1 ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> return AVERROR_INVALIDDATA ; <nl> frame += v ; <nl> } else { <nl> - if ( frame_end - frame < width + 3 ) <nl> + if ( frame_end - frame < width + 4 ) <nl> return AVERROR_INVALIDDATA ; <nl> frame [ 0 ] = frame [ 1 ] = <nl> frame [ width ] = frame [ width + 1 ] = bytestream2_get_byte ( gb );
retry : <nl> } else { <nl> level = SHOW_UBITS ( re , & s -> gb , 5 ); <nl> SKIP_CACHE ( re , & s -> gb , 5 ); <nl> - level |= SHOW_SBITS ( re , & s -> gb , 6 )<< 5 ; <nl> + level |= SHOW_SBITS ( re , & s -> gb , 6 ) * ( 1 << 5 ); <nl> SKIP_COUNTER ( re , & s -> gb , 5 + 6 ); <nl> } <nl> }
static int decode_sei ( H264Context * h ){ <nl>  <nl> switch ( type ){ <nl> case 5 : <nl> - if ( decode_unregistered_user_data ( h , size ) < 0 ); <nl> + if ( decode_unregistered_user_data ( h , size ) < 0 ) <nl> return - 1 ; <nl> break ; <nl> default :
static int read_interval_packets ( WriterContext * w , AVFormatContext * fmt_ctx , <nl> AVPacket pkt , pkt1 ; <nl> AVFrame frame ; <nl> int ret = 0 , i = 0 , frame_count = 0 ; <nl> - int64_t start , end = interval -> end ; <nl> + int64_t start = - INT64_MAX , end = interval -> end ; <nl> int has_start = 0 , has_end = interval -> has_end && ! interval -> end_is_offset ; <nl>  <nl> av_init_packet (& pkt );
typedef struct SmcContext { <nl> row_ptr += stride * 4 ; \ <nl> } \ <nl> total_blocks --; \ <nl> - if ( total_blocks < 0 ) \ <nl> + if ( total_blocks < 0 + !! n_blocks ) \ <nl> { \ <nl> av_log ( s -> avctx , AV_LOG_INFO , " warning : block counter just went negative ( this should not happen )\ n "); \ <nl> return ; \
int ff_hevc_cu_qp_delta_abs ( HEVCContext * s ) <nl> suffix_val += 1 << k ; <nl> k ++; <nl> } <nl> - if ( k == CABAC_MAX_BIN ) <nl> + if ( k == CABAC_MAX_BIN ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " CABAC_MAX_BIN : % d \ n ", k ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> while ( k --) <nl> suffix_val += get_cabac_bypass (& s -> HEVClc -> cc ) << k ;
static void get_attachment ( AVFormatContext * s , AVIOContext * pb , int length ) <nl> st -> codec -> codec_id = AV_CODEC_ID_MJPEG ; <nl> st -> codec -> codec_type = AVMEDIA_TYPE_ATTACHMENT ; <nl> st -> codec -> extradata = av_mallocz ( filesize ); <nl> + st -> id = - 1 ; <nl> if (! st -> codec -> extradata ) <nl> goto done ; <nl> st -> codec -> extradata_size = filesize ;
static void qdm2_fft_decode_tones ( QDM2Context * q , int duration , GetBitContext * <nl> return ; <nl>  <nl> local_int_14 = ( offset >> local_int_8 ); <nl> + if ( local_int_14 >= FF_ARRAY_ELEMS ( fft_level_index_table )) <nl> + return ; <nl>  <nl> if ( q -> nb_channels > 1 ) { <nl> channel = get_bits1 ( gb );
static int ffm_seek ( AVFormatContext * s , int stream_index , int64_t wanted_pts , in <nl> while ( pos_min <= pos_max ) { <nl> pts_min = get_dts ( s , pos_min ); <nl> pts_max = get_dts ( s , pos_max ); <nl> - if ( pts_min > wanted_pts || pts_max < wanted_pts ) { <nl> + if ( pts_min > wanted_pts || pts_max <= wanted_pts ) { <nl> pos = pts_min > wanted_pts ? pos_min : pos_max ; <nl> goto found ; <nl> }
static int cache_read ( URLContext * h , unsigned char * buf , int size ) <nl> { <nl> Context * c = h -> priv_data ; <nl> CacheEntry * entry , * next [ 2 ] = { NULL , NULL }; <nl> - int r ; <nl> + int64_t r ; <nl>  <nl> entry = av_tree_find ( c -> root , & c -> logical_pos , cmp , ( void **) next ); <nl> 
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> return ret ; <nl> } <nl> } else { <nl> - if (( ret = ff_MPV_common_init ( s ) < 0 )) { <nl> + if (( ret = ff_MPV_common_init ( s )) < 0 ) { <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " ff_MPV_common_init () failed .\ n "); <nl> return ret ; <nl> }
typedef struct A64Context { <nl> AVLFG randctx ; <nl> int mc_lifetime ; <nl> int mc_use_5col ; <nl> - int mc_frame_counter ; <nl> + unsigned mc_frame_counter ; <nl> int * mc_meta_charset ; <nl> int * mc_charmap ; <nl> int * mc_best_cb ;
static int decode_pic_hdr ( IVI45DecContext * ctx , AVCodecContext * avctx ) <nl>  <nl> /* decode subdivision of the planes */ <nl> pic_conf . luma_bands = decode_plane_subdivision (& ctx -> gb ); <nl> + pic_conf . chroma_bands = 0 ; <nl> if ( pic_conf . luma_bands ) <nl> pic_conf . chroma_bands = decode_plane_subdivision (& ctx -> gb ); <nl> ctx -> is_scalable = pic_conf . luma_bands != 1 || pic_conf . chroma_bands != 1 ;
static int lag_read_prob_header ( lag_rac * rac , GetBitContext * gb ) <nl> } <nl>  <nl> scale_factor ++; <nl> - cumulative_target = 1 << scale_factor ; <nl> + if ( scale_factor >= 32U ) <nl> + return AVERROR_INVALIDDATA ; <nl> + cumulative_target = 1U << scale_factor ; <nl>  <nl> if ( scaled_cumul_prob > cumulative_target ) { <nl> av_log ( rac -> avctx , AV_LOG_ERROR ,
static int xan_decode_frame_type0 ( AVCodecContext * avctx ) <nl> int dec_size ; <nl>  <nl> bytestream2_seek (& s -> gb , 8 + corr_off , SEEK_SET ); <nl> - dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size ); <nl> + dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size / 2 ); <nl> if ( dec_size < 0 ) <nl> dec_size = 0 ; <nl> for ( i = 0 ; i < dec_size ; i ++)
static int initFilter ( int16_t ** outFilter , int16_t ** filterPos , int * outFilterSi <nl> if ( xInc <= 1 << 16 ) filterSize = 1 + sizeFactor ; // upscale <nl> else filterSize = 1 + ( sizeFactor * srcW + dstW - 1 )/ dstW ; <nl>  <nl> - if ( filterSize > srcW - 2 ) filterSize = srcW - 2 ; <nl> + filterSize = av_clip ( filterSize , 1 , srcW - 2 ); <nl>  <nl> FF_ALLOC_OR_GOTO ( NULL , filter , dstW * sizeof (* filter )* filterSize , fail ); <nl> 
static int read_thread ( void * arg ) <nl> } <nl> if ( is -> queue_attachments_req ) { <nl> if ( is -> video_st && is -> video_st -> disposition & AV_DISPOSITION_ATTACHED_PIC ) { <nl> - AVPacket copy ; <nl> + AVPacket copy = { 0 }; <nl> if (( ret = av_copy_packet (& copy , & is -> video_st -> attached_pic )) < 0 ) <nl> goto fail ; <nl> packet_queue_put (& is -> videoq , & copy );
FF_ENABLE_DEPRECATION_WARNINGS <nl> /* Encode a dummy frame to get the extradata immediately */ <nl> if ( x -> quicktime_format ) { <nl> AVFrame * picture ; <nl> - AVPacket packet ; <nl> + AVPacket packet = { 0 }; <nl> int size , got_packet , ret ; <nl>  <nl> av_init_packet (& packet );
static int tm2_read_stream ( TM2Context * ctx , const uint8_t * buf , int stream_id , i <nl> if ( len == 0 ) <nl> return 4 ; <nl>  <nl> - if ( len >= INT_MAX / 4 - 1 || len < 0 || len > buf_size ) { <nl> + if ( len >= INT_MAX / 4 - 1 || len < 0 || skip > buf_size ) { <nl> av_log ( ctx -> avctx , AV_LOG_ERROR , " Error , invalid stream size .\ n "); <nl> return - 1 ; <nl> }
static int decode_lowdelay ( DiracContext * s ) <nl> slice_num ++; <nl>  <nl> buf += bytes ; <nl> - bufsize -= bytes * 8 ; <nl> + if ( bufsize / 8 >= bytes ) <nl> + bufsize -= bytes * 8 ; <nl> + else <nl> + bufsize = 0 ; <nl> } <nl>  <nl> avctx -> execute ( avctx , decode_lowdelay_slice , slices , NULL , slice_num ,
static av_cold int adx_encode_init ( AVCodecContext * avctx ) <nl> avctx -> frame_size = BLOCK_SAMPLES ; <nl>  <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (! avctx -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> /* the cutoff can be adjusted , but this seems to work pretty well */ <nl> c -> cutoff = 500 ;
static int xan_huffman_decode ( uint8_t * dest , int dest_len , <nl> return ret ; <nl>  <nl> while ( val != 0x16 ) { <nl> - unsigned idx = val - 0x17 + get_bits1 (& gb ) * byte ; <nl> + unsigned idx ; <nl> + if ( get_bits_left (& gb ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + idx = val - 0x17 + get_bits1 (& gb ) * byte ; <nl> if ( idx >= 2 * byte ) <nl> return AVERROR_INVALIDDATA ; <nl> val = src [ idx ];
static int vmd_read_header ( AVFormatContext * s ) <nl> vst -> codec -> width >>= 1 ; <nl> vst -> codec -> height >>= 1 ; <nl> } <nl> - vst -> codec -> extradata_size = VMD_HEADER_SIZE ; <nl> vst -> codec -> extradata = av_mallocz ( VMD_HEADER_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! vst -> codec -> extradata ) <nl> + return AVERROR ( ENOMEM ); <nl> + vst -> codec -> extradata_size = VMD_HEADER_SIZE ; <nl> memcpy ( vst -> codec -> extradata , vmd -> vmd_header , VMD_HEADER_SIZE ); <nl> } <nl> 
static int compat_decode ( AVCodecContext * avctx , AVFrame * frame , <nl> int * got_frame , const AVPacket * pkt ) <nl> { <nl> AVCodecInternal * avci = avctx -> internal ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> av_assert0 ( avci -> compat_decode_consumed == 0 ); <nl> 
static int msf_probe ( AVProbeData * p ) <nl> if ( AV_RB32 ( p -> buf + 16 ) <= 0 ) <nl> return 0 ; <nl>  <nl> + if ( AV_RB32 ( p -> buf + 4 ) > 16 ) <nl> + return AVPROBE_SCORE_MAX / 5 ; // unsupported / unknown codec <nl> + <nl> return AVPROBE_SCORE_MAX / 3 * 2 ; <nl> } <nl> 
static int mv_read_header ( AVFormatContext * avctx ) <nl> { <nl> MvContext * mv = avctx -> priv_data ; <nl> AVIOContext * pb = avctx -> pb ; <nl> - AVStream * ast , * vst ; <nl> + AVStream * ast = NULL , * vst = NULL ; // initialization to suppress warning <nl> int version , i ; <nl>  <nl> avio_skip ( pb , 4 );
static void ra144_encode_subblock ( RA144Context * ractx , <nl> float zero [ BLOCKSIZE ], cba [ BLOCKSIZE ], cb1 [ BLOCKSIZE ], cb2 [ BLOCKSIZE ]; <nl> int16_t cba_vect [ BLOCKSIZE ]; <nl> int cba_idx , cb1_idx , cb2_idx , gain ; <nl> - int i , n , m [ 3 ]; <nl> + int i , n ; <nl> + unsigned m [ 3 ]; <nl> float g [ 3 ]; <nl> float error , best_error ; <nl> 
static void free_stream ( AVStream ** pst ) <nl> av_freep (& st -> index_entries ); <nl> # if FF_API_LAVF_AVCTX <nl> FF_DISABLE_DEPRECATION_WARNINGS <nl> - av_freep (& st -> codec -> extradata ); <nl> - av_freep (& st -> codec -> subtitle_header ); <nl> - av_freep (& st -> codec ); <nl> + avcodec_free_context (& st -> codec ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> # endif <nl> av_freep (& st -> priv_data );
static int decode_frame ( AVCodecContext * avctx , <nl> int prev_y = 0 , prev_u = 0 , prev_v = 0 ; <nl> uint8_t * rbuf ; <nl>  <nl> + if ( buf_size <= 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " buf_size % d is too small \ n ", buf_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> rbuf = av_malloc ( buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! rbuf ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n ");
static int ftp_send_command ( FTPContext * s , const char * command , <nl> if ( response ) <nl> * response = NULL ; <nl>  <nl> + if (! s -> conn_control ) <nl> + return AVERROR ( EIO ); <nl> + <nl> if (( err = ffurl_write ( s -> conn_control , command , strlen ( command ))) < 0 ) <nl> return err ; <nl> if (! err )
void av_dump_format ( AVFormatContext * ic , <nl> int is_output ) <nl> { <nl> int i ; <nl> - uint8_t * printed = av_mallocz ( ic -> nb_streams ); <nl> + uint8_t * printed = ic -> nb_streams ? av_mallocz ( ic -> nb_streams ) : NULL ; <nl> if ( ic -> nb_streams && ! printed ) <nl> return ; <nl> 
error : <nl> static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , <nl> int S ) <nl> { <nl> - int bit ; <nl> + unsigned bit ; <nl>  <nl> if ( s -> extra_bits ) { <nl> S <<= s -> extra_bits ;
int avpriv_ac3_parse_header ( AC3HeaderInfo ** phdr , const uint8_t * buf , <nl> return AVERROR ( ENOMEM ); <nl> hdr = * phdr ; <nl>  <nl> - init_get_bits8 (& gb , buf , size ); <nl> + err = init_get_bits8 (& gb , buf , size ); <nl> + if ( err < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> err = ff_ac3_parse_header (& gb , hdr ); <nl> if ( err < 0 ) <nl> return AVERROR_INVALIDDATA ;
static int open_file ( AVFormatContext * avf , unsigned fileno ) <nl> if (! cat -> avf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - cat -> avf -> flags |= avf -> flags ; <nl> + cat -> avf -> flags |= avf -> flags & ~ AVFMT_FLAG_CUSTOM_IO ; <nl> cat -> avf -> interrupt_callback = avf -> interrupt_callback ; <nl>  <nl> if (( ret = ff_copy_whiteblacklists ( cat -> avf , avf )) < 0 )
static int xan_decode_frame ( AVCodecContext * avctx , <nl> } <nl> buf_size = buf_end - buf ; <nl> } <nl> + if ( s -> palettes_count <= 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " No palette found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = avctx -> get_buffer ( avctx , & s -> current_frame ))) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return ret ;
static void compute_default_clut ( AVSubtitleRect * rect , int w , int h ) <nl> list_inv [ i ] = bestv ; <nl> } <nl>  <nl> - count = i - 1 ; <nl> + count = FFMAX ( i - 1 , 1 ); <nl> for ( i --; i >= 0 ; i --) { <nl> int v = i * 255 / count ; <nl> AV_WN32 ( rect -> data [ 1 ] + 4 * list_inv [ i ], RGBA ( v / 2 , v , v / 2 , v ));
static void show_packets ( WriterContext * w , AVFormatContext * fmt_ctx ) <nl> if ( do_show_frames && <nl> get_video_frame ( fmt_ctx , & frame , & pkt )) { <nl> show_frame ( w , & frame , fmt_ctx -> streams [ pkt . stream_index ]); <nl> - av_destruct_packet (& pkt ); <nl> + av_free_packet (& pkt ); <nl> } <nl> } <nl> av_init_packet (& pkt );
retry : <nl> if ( c -> itunes_metadata && atom . size > 8 ) { <nl> int data_size = avio_rb32 ( pb ); <nl> int tag = avio_rl32 ( pb ); <nl> - if ( tag == MKTAG (' d ',' a ',' t ',' a ')) { <nl> + if ( tag == MKTAG (' d ',' a ',' t ',' a ') && data_size <= atom . size ) { <nl> data_type = avio_rb32 ( pb ); // type <nl> avio_rb32 ( pb ); // unknown <nl> str_size = data_size - 16 ;
static int process_output_surface ( AVCodecContext * avctx , AVPacket * pkt , NvencSur <nl> } <nl> slice_offsets = av_mallocz ( slice_mode_data * sizeof (* slice_offsets )); <nl>  <nl> - if (! slice_offsets ) <nl> + if (! slice_offsets ) { <nl> + res = AVERROR ( ENOMEM ); <nl> goto error ; <nl> + } <nl>  <nl> lock_params . version = NV_ENC_LOCK_BITSTREAM_VER ; <nl> 
static int applehttp_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> /* If this isn ' t a live stream , calculate the total duration of the <nl> * stream . */ <nl> if ( c -> finished ) { <nl> - int duration = 0 ; <nl> + int64_t duration = 0 ; <nl> for ( i = 0 ; i < c -> variants [ 0 ]-> n_segments ; i ++) <nl> duration += c -> variants [ 0 ]-> segments [ i ]-> duration ; <nl> s -> duration = duration * AV_TIME_BASE ;
static int ivi_init_tiles ( IVIBandDesc * band , IVITile * ref_tile , <nl> band -> mb_size ); <nl>  <nl> av_freep (& tile -> mbs ); <nl> - tile -> mbs = av_malloc ( tile -> num_MBs * sizeof ( IVIMbInfo )); <nl> + tile -> mbs = av_mallocz ( tile -> num_MBs * sizeof ( IVIMbInfo )); <nl> if (! tile -> mbs ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int libopenjpeg_decode_frame ( AVCodecContext * avctx , <nl> // Decode the codestream <nl> image = opj_decode_with_info ( dec , stream , NULL ); <nl> opj_cio_close ( stream ); <nl> + if (! image ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Error decoding codestream .\ n "); <nl> + opj_destroy_decompress ( dec ); <nl> + return - 1 ; <nl> + } <nl>  <nl> pixel_size = av_pix_fmt_descriptors [ avctx -> pix_fmt ]. comp [ 0 ]. step_minus1 + 1 ; <nl> ispacked = libopenjpeg_ispacked ( avctx -> pix_fmt );
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl> } <nl> } else { <nl> - avpriv_request_sample ( s , " Uncompressed image "); <nl> + avpriv_request_sample ( avctx , " Uncompressed image "); <nl> return avpkt -> size ; <nl> } <nl> finish :
av_cold int ff_msmpeg4_decode_init ( AVCodecContext * avctx ) <nl> int i ; <nl> MVTable * mv ; <nl>  <nl> + if ( avctx -> width <= 0 || avctx -> height <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " invalid dimensions \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( ff_h263_decode_init ( avctx ) < 0 ) <nl> return - 1 ; <nl> 
static int segment_mux_init ( AVFormatContext * s ) <nl> oc -> opaque = s -> opaque ; <nl> oc -> io_close = s -> io_close ; <nl> oc -> io_open = s -> io_open ; <nl> + oc -> flags = s -> flags ; <nl>  <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> AVStream * st ;
static int g2m_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( got_header ) <nl> c -> got_header = 1 ; <nl>  <nl> - if ( c -> width && c -> height ) { <nl> + if ( c -> width && c -> height && c -> framebuf ) { <nl> if (( ret = ff_get_buffer ( avctx , pic , 0 )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return ret ;
static int ogg_write_trailer ( AVFormatContext * s ) <nl> av_free ( oggstream -> header [ 0 ]); <nl> av_free ( oggstream -> header [ 1 ]); <nl> } <nl> + else <nl> + av_free ( oggstream -> header [ 1 ]); <nl> av_freep (& st -> priv_data ); <nl> } <nl> return 0 ;
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> flv_same_video_codec ( st -> codec , flags )) { <nl> break ; <nl> } <nl> - } else if ( st -> id == stream_type ) { <nl> - break ; <nl> + } else if ( stream_type == FLV_STREAM_TYPE_DATA ) { <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_DATA ) <nl> + break ; <nl> } <nl> } <nl> if ( i == s -> nb_streams ){
static int h261_decode_mb ( H261Context * h ){ <nl>  <nl> // Read mtype <nl> h -> mtype = get_vlc2 (& s -> gb , h261_mtype_vlc . table , H261_MTYPE_VLC_BITS , 2 ); <nl> + if ( h -> mtype < 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " illegal mtype % d \ n ", h -> mtype ); <nl> + return SLICE_ERROR ; <nl> + } <nl> h -> mtype = h261_mtype_map [ h -> mtype ]; <nl>  <nl> // Read mquant
static int encode_picture_ls ( AVCodecContext * avctx , AVPacket * pkt , <nl>  <nl> ls_store_lse ( state , & pb ); <nl>  <nl> - zero = av_mallocz ( p -> linesize [ 0 ]); <nl> + zero = av_mallocz ( FFABS ( p -> linesize [ 0 ])); <nl> + if (! zero ) <nl> + return AVERROR ( ENOMEM ); <nl> last = zero ; <nl> cur = p -> data [ 0 ]; <nl> if ( avctx -> pix_fmt == PIX_FMT_GRAY8 ){
static void ff_compute_band_indexes ( MPADecodeContext * s , GranuleDef * g ) <nl> else <nl> g -> long_end = 6 ; <nl>  <nl> - g -> short_start = 2 + ( s -> sample_rate_index != 8 ); <nl> + g -> short_start = 3 ; <nl> } else { <nl> g -> long_end = 0 ; <nl> g -> short_start = 0 ;
static int create_filter ( AVFilterContext ** filt_ctx , AVFilterGraph * ctx , int ind <nl> return ret ; <nl> } <nl>  <nl> - if (! strcmp ( filt_name , " scale ") && ! strstr ( args , " flags ")) { <nl> + if (! strcmp ( filt_name , " scale ") && args && ! strstr ( args , " flags ")) { <nl> snprintf ( tmp_args , sizeof ( tmp_args ), "% s :% s ", <nl> args , ctx -> scale_sws_opts ); <nl> args = tmp_args ;
static void fix_bitshift ( ShortenContext * s , int32_t * buffer ) <nl>  <nl> if ( s -> bitshift != 0 ) <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - buffer [ s -> nwrap + i ] <<= s -> bitshift ; <nl> + buffer [ i ] <<= s -> bitshift ; <nl> } <nl>  <nl> 
static int idcin_read_packet ( AVFormatContext * s , <nl> } <nl>  <nl> chunk_size = avio_rl32 ( pb ); <nl> + if ( chunk_size < 4 || chunk_size > INT_MAX - 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " invalid chunk size : % u \ n ", chunk_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> /* skip the number of decoded bytes ( always equal to width * height ) */ <nl> avio_skip ( pb , 4 ); <nl> chunk_size -= 4 ;
static inline int read_line ( AVFormatContext * s , char * rbuf , const int rbufsize , <nl>  <nl> do { <nl> ret = ffurl_read_complete ( rt -> rtsp_hd , rbuf + idx , 1 ); <nl> - if ( ret < 0 ) <nl> - return ret ; <nl> + if ( ret <= 0 ) <nl> + return ret ? ret : AVERROR_EOF ; <nl> if ( rbuf [ idx ] == '\ r ') { <nl> /* Ignore */ <nl> } else if ( rbuf [ idx ] == '\ n ') {
static int mov_read_stsz ( MOVContext * c , ByteIOContext * pb , MOVAtom atom ) <nl> return - 1 ; <nl> } <nl>  <nl> - if ( entries >= UINT_MAX / sizeof ( int )) <nl> + if ( entries >= UINT_MAX / sizeof ( int ) || entries >= ( UINT_MAX - 4 ) / field_size ) <nl> return - 1 ; <nl> sc -> sample_sizes = av_malloc ( entries * sizeof ( int )); <nl> if (! sc -> sample_sizes )
static int wsvqa_read_packet ( AVFormatContext * s , <nl> switch ( chunk_type ) { <nl> case SND1_TAG : <nl> /* unpacked size is stored in header */ <nl> - pkt -> duration = AV_RL16 ( pkt -> data ) / wsvqa -> channels ; <nl> + if ( pkt -> data ) <nl> + pkt -> duration = AV_RL16 ( pkt -> data ) / wsvqa -> channels ; <nl> break ; <nl> case SND2_TAG : <nl> /* 2 samples / byte , 1 or 2 samples per frame depending on stereo */
FF_ENABLE_DEPRECATION_WARNINGS <nl> if (! avctx -> rc_initial_buffer_occupancy ) <nl> avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3 / 4 ; <nl>  <nl> - if ( avctx -> ticks_per_frame && <nl> + if ( avctx -> ticks_per_frame && avctx -> time_base . num && <nl> avctx -> ticks_per_frame > INT_MAX / avctx -> time_base . num ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " ticks_per_frame % d too large for the timebase % d /% d .",
static int X264_frame ( AVCodecContext * ctx , AVPacket * pkt , const AVFrame * frame , <nl> X264Context * x4 = ctx -> priv_data ; <nl> x264_nal_t * nal ; <nl> int nnal , i , ret ; <nl> - x264_picture_t pic_out ; <nl> + x264_picture_t pic_out = { 0 }; <nl>  <nl> x264_picture_init ( & x4 -> pic ); <nl> x4 -> pic . img . i_csp = x4 -> params . i_csp ;
static int ra144_decode_frame ( AVCodecContext * avctx , void * data , <nl> do_output_subblock ( ractx , block_coefs [ i ], refl_rms [ i ], & gb ); <nl>  <nl> for ( j = 0 ; j < BLOCKSIZE ; j ++) <nl> - * samples ++ = av_clip_int16 ( ractx -> curr_sblock [ j + 10 ] << 2 ); <nl> + * samples ++ = av_clip_int16 ( ractx -> curr_sblock [ j + 10 ] * ( 1 << 2 )); <nl> } <nl>  <nl> ractx -> old_energy = energy ;
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> tag = avio_rl32 ( pb ); <nl> size = avio_rl32 ( pb ); <nl>  <nl> + if ( size > avi -> fsize ){ <nl> + av_log ( s , AV_LOG_ERROR , " chunk size is too big during header parsing \ n "); <nl> + goto fail ; <nl> + } <nl> + <nl> print_tag (" tag ", tag , size ); <nl>  <nl> switch ( tag ) {
static int gxf_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> } <nl>  <nl> static int gxf_seek ( AVFormatContext * s , int stream_index , int64_t timestamp , int flags ) { <nl> - int res = 0 ; <nl> + int64_t res = 0 ; <nl> uint64_t pos ; <nl> uint64_t maxlen = 100 * 1024 * 1024 ; <nl> AVStream * st = s -> streams [ 0 ];
 <nl> static av_always_inline int even ( uint64_t layout ) <nl> { <nl> - return (! layout || ( layout & ( layout - 1 ))); <nl> + return (! layout || !!( layout & ( layout - 1 ))); <nl> } <nl>  <nl> static int sane_layout ( uint64_t layout )
int av_reallocp_array ( void * ptr , size_t nmemb , size_t size ) <nl> { <nl> void ** ptrptr = ptr ; <nl> * ptrptr = av_realloc_f (* ptrptr , nmemb , size ); <nl> - if (!* ptrptr && !( nmemb && size )) <nl> + if (!* ptrptr && nmemb && size ) <nl> return AVERROR ( ENOMEM ); <nl> return 0 ; <nl> }
static int video_thread ( void * arg ) <nl> ret = queue_picture ( is , frame , pts , duration , frame -> pkt_pos , is -> viddec . pkt_serial ); <nl> av_frame_unref ( frame ); <nl> # if CONFIG_AVFILTER <nl> + if ( is -> videoq . serial != is -> viddec . pkt_serial ) <nl> + break ; <nl> } <nl> # endif <nl> 
static int hnm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int ret ; <nl> uint16_t chunk_id ; <nl>  <nl> + if ( avpkt -> size < 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " packet too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (( ret = ff_get_buffer ( avctx , frame , 0 )) < 0 ) <nl> return ret ; <nl> 
int ff_init_vlc_sparse ( VLC * vlc , int nb_bits , int nb_codes , <nl> av_dlog ( NULL , " build table nb_codes =% d \ n ", nb_codes ); <nl>  <nl> buf = av_malloc (( nb_codes + 1 ) * sizeof ( VLCcode )); <nl> + if (! buf ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> av_assert0 ( symbols_size <= 2 || ! symbols ); <nl> j = 0 ;
static int dxv_decompress_raw ( AVCodecContext * avctx ) <nl> DXVContext * ctx = avctx -> priv_data ; <nl> GetByteContext * gbc = & ctx -> gbc ; <nl>  <nl> + if ( bytestream2_get_bytes_left ( gbc ) < ctx -> tex_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> bytestream2_get_buffer ( gbc , ctx -> tex_data , ctx -> tex_size ); <nl> return 0 ; <nl> }
static int tm2_build_huff_table ( TM2Context * ctx , TM2Codes * code ) <nl> huff . val_bits , huff . max_bits ); <nl> return - 1 ; <nl> } <nl> - if (( huff . nodes < 0 ) || ( huff . nodes > 0x10000 )) { <nl> + if (( huff . nodes <= 0 ) || ( huff . nodes > 0x10000 )) { <nl> av_log ( ctx -> avctx , AV_LOG_ERROR , " Incorrect number of Huffman tree nodes : % i \ n ", huff . nodes ); <nl> return - 1 ; <nl> }
static int gdv_decode_frame ( AVCodecContext * avctx , void * data , <nl> unsigned r = bytestream2_get_byte ( gb ); <nl> unsigned g = bytestream2_get_byte ( gb ); <nl> unsigned b = bytestream2_get_byte ( gb ); <nl> - gdv -> pal [ i ] = 0xFF << 24 | r << 18 | g << 10 | b << 2 ; <nl> + gdv -> pal [ i ] = 0xFFU << 24 | r << 18 | g << 10 | b << 2 ; <nl> } <nl> break ; <nl> case 3 :
static int filter_frame ( AVFilterLink * inlink , AVFrame * insamples ) <nl> break ; <nl> av_assert1 ( input_number < am -> nb_inputs ); <nl> if ( ff_bufqueue_is_full (& am -> in [ input_number ]. queue )) { <nl> - av_log ( ctx , AV_LOG_ERROR , " Buffer queue overflow \ n "); <nl> av_frame_free (& insamples ); <nl> return AVERROR ( ENOMEM ); <nl> }
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
static av_cold void init_cplscales_table ( COOKContext * q ) <nl> static inline int decode_bytes ( const uint8_t * inbuffer , uint8_t * out , int bytes ) <nl> { <nl> static const uint32_t tab [ 4 ] = { <nl> - AV_BE2NE32C ( 0x37c511f2 ), AV_BE2NE32C ( 0xf237c511 ), <nl> - AV_BE2NE32C ( 0x11f237c5 ), AV_BE2NE32C ( 0xc511f237 ), <nl> + AV_BE2NE32C ( 0x37c511f2U ), AV_BE2NE32C ( 0xf237c511U ), <nl> + AV_BE2NE32C ( 0x11f237c5U ), AV_BE2NE32C ( 0xc511f237U ), <nl> }; <nl> int i , off ; <nl> uint32_t c ;
static int idcin_read_packet ( AVFormatContext * s , <nl> chunk_size = avio_rl32 ( pb ); <nl> /* skip the number of decoded bytes ( always equal to width * height ) */ <nl> avio_skip ( pb , 4 ); <nl> + if ( chunk_size < 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> chunk_size -= 4 ; <nl> ret = av_get_packet ( pb , pkt , chunk_size ); <nl> if ( ret < 0 )
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> INC_MED ( 1 ); <nl> DEC_MED ( 2 ); <nl> } else { <nl> - base = GET_MED ( 0 ) + GET_MED ( 1 ) + GET_MED ( 2 ) * ( t - 2 ); <nl> + base = GET_MED ( 0 ) + GET_MED ( 1 ) + GET_MED ( 2 ) * ( t - 2U ); <nl> add = GET_MED ( 2 ) - 1 ; <nl> INC_MED ( 0 ); <nl> INC_MED ( 1 );
static void dwt_decode97_int ( DWTContext * s , int32_t * t ) <nl> line += 5 ; <nl>  <nl> for ( i = 0 ; i < w * h ; i ++) <nl> - data [ i ] <<= I_PRESHIFT ; <nl> + data [ i ] *= 1 << I_PRESHIFT ; <nl>  <nl> for ( lev = 0 ; lev < s -> ndeclevels ; lev ++) { <nl> int lh = s -> linelen [ lev ][ 0 ],
static av_cold int init ( AVCodecContext * avctx ) <nl> int dummy_int ; <nl>  <nl> /* Back up the extradata so it can be restored at close time . */ <nl> - priv -> orig_extradata = av_malloc ( avctx -> extradata_size ); <nl> + priv -> orig_extradata = av_malloc ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! priv -> orig_extradata ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " Failed to allocate copy of extradata \ n ");
int av_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , <nl> " Dropped corrupted packet ( stream = % d )\ n ", <nl> pkt -> stream_index ); <nl> + av_free_packet ( pkt ); <nl> continue ; <nl> } <nl> 
matroska_read_seek ( AVFormatContext * s , int stream_index , int64_t timestamp , <nl> matroska -> skip_to_keyframe = !( flags & AVSEEK_FLAG_ANY ); <nl> matroska -> skip_to_stream = st ; <nl> matroska -> peek_id = 0 ; <nl> + av_update_cur_dts ( s , st , st -> index_entries [ index ]. timestamp ); <nl> return 0 ; <nl> } <nl> 
# define CODE_UNSET - 1 <nl> # define CODE_BIT_INIT 9 <nl> # define DIC_INDEX_INIT 512 // 2 ^ 9 <nl> -# define DIC_INDEX_MAX 32768l // 2 ^ 15 <nl> +# define DIC_INDEX_MAX 32768 // 2 ^ 15 <nl> # define FLUSH_CODE 256 <nl> # define FREEZE_CODE 257 <nl> # define FIRST_CODE 258 <nl> -# define MAX_CODE 32767l <nl> -# define TABLE_SIZE 35023l // TABLE_SIZE must be a prime number <nl> +# define MAX_CODE 32767 <nl> +# define TABLE_SIZE 35023 // TABLE_SIZE must be a prime number <nl>  <nl> /** Dictionary structure for mlz decompression <nl> */
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> continue ; <nl> } <nl> v -> second_field = 1 ; <nl> - v -> blocks_off = s -> mb_width * s -> mb_height << 1 ; <nl> + v -> blocks_off = s -> b8_stride * ( s -> mb_height &~ 1 ); <nl> v -> mb_off = s -> mb_stride * s -> mb_height >> 1 ; <nl> } else { <nl> v -> second_field = 0 ;
static int vqf_read_header ( AVFormatContext * s ) <nl> break ; <nl> default : <nl> st -> codec -> sample_rate = rate_flag * 1000 ; <nl> + if ( st -> codec -> sample_rate <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " sample rate % d is invalid \ n ", st -> codec -> sample_rate ); <nl> + return - 1 ; <nl> + } <nl> break ; <nl> } <nl> 
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> frame_len = 1 << frame_len_bits ; <nl>  <nl> - if ( s -> channels > MAX_CHANNELS ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + if ( avctx -> channels > MAX_CHANNELS ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", avctx -> channels ); <nl> return - 1 ; <nl> } <nl> 
static av_always_inline av_const int32_t av_clipl_int32_c ( int64_t a ) <nl> */ <nl> static av_always_inline av_const int av_clip_intp2_c ( int a , int p ) <nl> { <nl> - if (( a + ( 1 << p )) & ~(( 2 << p ) - 1 )) <nl> + if ((( unsigned ) a + ( 1 << p )) & ~(( 2 << p ) - 1 )) <nl> return ( a >> 31 ) ^ (( 1 << p ) - 1 ); <nl> else <nl> return a ;
static int apng_read_header ( AVFormatContext * s ) <nl> AVIOContext * pb = s -> pb ; <nl> uint32_t len , tag ; <nl> AVStream * st ; <nl> - int ret = AVERROR_INVALIDDATA , acTL_found = 0 ; <nl> + int acTL_found = 0 ; <nl> + int64_t ret = AVERROR_INVALIDDATA ; <nl>  <nl> /* verify PNGSIG */ <nl> if ( avio_rb64 ( pb ) != PNGSIG )
static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , <nl> unsigned bit ; <nl>  <nl> if ( s -> extra_bits ) { <nl> - S <<= s -> extra_bits ; <nl> + S *= 1 << s -> extra_bits ; <nl>  <nl> if ( s -> got_extra_bits && <nl> get_bits_left (& s -> gb_extra_bits ) >= s -> extra_bits ) {
FFPsyChannelGroup * ff_psy_find_group ( FFPsyContext * ctx , int channel ) <nl>  <nl> av_cold void ff_psy_end ( FFPsyContext * ctx ) <nl> { <nl> - if ( ctx -> model -> end ) <nl> + if ( ctx -> model && ctx -> model -> end ) <nl> ctx -> model -> end ( ctx ); <nl> av_freep (& ctx -> bands ); <nl> av_freep (& ctx -> num_bands );
static inline uint8_t lag_get_rac ( lag_rac * l ) <nl> l -> range -= range_scaled * l -> prob [ 255 ]; <nl> } <nl>  <nl> + if (! l -> range ) <nl> + l -> range = 0x80 ; <nl> + <nl> l -> low -= range_scaled * l -> prob [ val ]; <nl>  <nl> return val ;
static void writer_close ( WriterContext ** wctx ) <nl> if ((* wctx )-> writer -> priv_class ) <nl> av_opt_free ((* wctx )-> priv ); <nl> av_freep (&((* wctx )-> priv )); <nl> + av_opt_free (* wctx ); <nl> av_freep ( wctx ); <nl> } <nl> 
static int thp_read_packet ( AVFormatContext * s , <nl> pkt -> stream_index = thp -> video_stream_index ; <nl> } else { <nl> ret = av_get_packet ( pb , pkt , thp -> audiosize ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( ret != thp -> audiosize ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO );
static int vp3_decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> last_frame = s -> golden_frame ; <nl> s -> last_frame . type = FF_BUFFER_TYPE_COPY ; <nl> + ff_thread_report_progress (& s -> last_frame , INT_MAX , 0 ); <nl> } <nl> } <nl> 
void ff_set_mpeg4_time ( MpegEncContext * s ) <nl>  <nl> static void mpeg4_encode_gop_header ( MpegEncContext * s ) <nl> { <nl> - int hours , minutes , seconds ; <nl> + int64_t hours , minutes , seconds ; <nl> int64_t time ; <nl>  <nl> put_bits (& s -> pb , 16 , 0 );
static int ac3_decode_frame ( AVCodecContext * avctx , void * data , <nl> avctx -> audio_service_type = AV_AUDIO_SERVICE_TYPE_KARAOKE ; <nl>  <nl> /* get output buffer */ <nl> + avctx -> channels = s -> out_channels ; <nl> s -> frame . nb_samples = s -> num_blocks * 256 ; <nl> if (( ret = avctx -> get_buffer ( avctx , & s -> frame )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n ");
static int mov_write_single_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int64_t frag_duration = 0 ; <nl> int size = pkt -> size ; <nl>  <nl> + int ret = check_pkt ( s , pkt ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> if ( mov -> flags & FF_MOV_FLAG_FRAG_DISCONT ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> nb_streams ; i ++)
static int parse_cookie ( HTTPContext * s , const char * p , AVDictionary ** cookies ) <nl> } <nl> } <nl> } <nl> + av_dict_free (& new_params ); <nl>  <nl> // duplicate the cookie name ( dict will dupe the value ) <nl> if (!( eql = strchr ( p , '='))) return AVERROR ( EINVAL );
static int decode_frame ( AVCodecContext * avctx , <nl> buf_size - offset ); <nl> if ( t < 0 ) { <nl> int j = tm2_stream_order [ i ]; <nl> - memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> + if ( l -> tok_lens [ j ]) <nl> + memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> return t ; <nl> } <nl> offset += t ;
static int decode_audio_specific_config ( AACContext * ac , <nl> */ <nl> static av_always_inline int lcg_random ( int previous_val ) <nl> { <nl> - return previous_val * 1664525 + 1013904223 ; <nl> + union { unsigned u ; int s ; } v = { previous_val * 1664525u + 1013904223 }; <nl> + return v . s ; <nl> } <nl>  <nl> static av_always_inline void reset_predict_state ( PredictorState * ps )
static int filter_frame ( AVFilterLink * inlink , AVFrame * buf ) <nl> av_frame_free (& buf ); <nl>  <nl> end : <nl> - vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += buf -> nb_samples ; <nl> + vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += out_buf -> nb_samples ; <nl> return ff_filter_frame ( outlink , out_buf ); <nl> } <nl> 
static inline uint64_t v4l2_get_pts ( V4L2Buffer * avbuf ) <nl> int64_t v4l2_pts ; <nl>  <nl> /* convert pts back to encoder timebase */ <nl> - v4l2_pts = avbuf -> buf . timestamp . tv_sec * USEC_PER_SEC + avbuf -> buf . timestamp . tv_usec ; <nl> + v4l2_pts = ( int64_t ) avbuf -> buf . timestamp . tv_sec * USEC_PER_SEC + <nl> + avbuf -> buf . timestamp . tv_usec ; <nl>  <nl> return av_rescale_q ( v4l2_pts , v4l2_timebase , s -> avctx -> time_base ); <nl> }
static int check_fps ( int fps ) <nl>  <nl> static int check_timecode ( void * log_ctx , AVTimecode * tc ) <nl> { <nl> - if ( tc -> fps <= 0 ) { <nl> + if (( int ) tc -> fps <= 0 ) { <nl> av_log ( log_ctx , AV_LOG_ERROR , " Timecode frame rate must be specified \ n "); <nl> return AVERROR ( EINVAL ); <nl> }
vbv_retry : <nl> if ( s -> mb_info ) <nl> av_packet_shrink_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , s -> mb_info_size ); <nl> } else { <nl> - assert (( put_bits_ptr (& s -> pb ) == s -> pb . buf )); <nl> s -> frame_bits = 0 ; <nl> } <nl> assert (( s -> frame_bits & 7 ) == 0 );
static inline void xan_wc3_copy_pixel_run ( XanContext * s , AVFrame * frame , <nl> prevframe_index = ( y + motion_y ) * stride + x + motion_x ; <nl> prevframe_x = x + motion_x ; <nl>  <nl> - if ( prev_palette_plane == palette_plane && FFABS ( curframe_index - prevframe_index ) < pixel_count ) { <nl> + if ( prev_palette_plane == palette_plane && FFABS ( motion_x + width * motion_y ) < pixel_count ) { <nl> avpriv_request_sample ( s -> avctx , " Overlapping copy "); <nl> return ; <nl> }
int64_t av_gcd ( int64_t a , int64_t b ) { <nl> v -= u ; <nl> v >>= ff_ctzll ( v ); <nl> } <nl> - return u << k ; <nl> + return ( uint64_t ) u << k ; <nl> } <nl>  <nl> int64_t av_rescale_rnd ( int64_t a , int64_t b , int64_t c , enum AVRounding rnd )
static int ljpeg_decode_yuv_scan ( MJpegDecodeContext * s , int predictor , <nl> if ( s -> interlaced && s -> bottom_field ) <nl> ptr16 += linesize >> 1 ; <nl> pred &= mask ; <nl> - * ptr16 = pred + ( dc << point_transform ); <nl> + * ptr16 = pred + (( unsigned ) dc << point_transform ); <nl> } <nl> if (++ x == h ) { <nl> x = 0 ;
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> if (! get_bits1 (& ctx -> gb ) || ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> transform_id = get_bits (& ctx -> gb , 5 ); <nl> - if (! transforms [ transform_id ]. inv_trans ) { <nl> + if ( transform_id >= FF_ARRAY_ELEMS ( transforms ) || <nl> + ! transforms [ transform_id ]. inv_trans ) { <nl> av_log_ask_for_sample ( avctx , " Unimplemented transform : % d !\ n ", transform_id ); <nl> return AVERROR_PATCHWELCOME ; <nl> }
static int get_delayed_pic ( DiracContext * s , AVFrame * picture , int * got_frame ) <nl>  <nl> if ( out ) { <nl> out -> reference ^= DELAYED_PIC_REF ; <nl> - * got_frame = 1 ; <nl> if (( ret = av_frame_ref ( picture , out -> avframe )) < 0 ) <nl> return ret ; <nl> + * got_frame = 1 ; <nl> } <nl>  <nl> return 0 ;
static int recode_subtitle ( AVCodecContext * avctx , <nl> goto end ; <nl> } <nl> outpkt -> size -= outl ; <nl> - outpkt -> data [ outpkt -> size - 1 ] = '\ 0 '; <nl> + memset ( outpkt -> data + outpkt -> size , 0 , outl ); <nl>  <nl> end : <nl> if ( cd != ( iconv_t )- 1 )
static int pmp_header ( AVFormatContext * s ) <nl> uint32_t index_cnt ; <nl> int audio_codec_id = AV_CODEC_ID_NONE ; <nl> int srate , channels ; <nl> - int i ; <nl> + unsigned i ; <nl> uint64_t pos ; <nl> int64_t fsize = avio_size ( pb ); <nl> 
int ff_h264_decode_ref_pic_list_reordering ( H264Context * h , H264SliceContext * sl ) <nl>  <nl> long_idx = pic_num_extract ( h , pic_id , & pic_structure ); <nl>  <nl> - if ( long_idx > 31 ) { <nl> + if ( long_idx > 31U ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> " long_term_pic_idx overflow \ n "); <nl> return AVERROR_INVALIDDATA ;
static void apply_tns ( float coef [ 1024 ], TemporalNoiseShaping * tns , <nl> int w , filt , m , i ; <nl> int bottom , top , order , start , end , size , inc ; <nl> float lpc [ TNS_MAX_ORDER ]; <nl> - float tmp [ TNS_MAX_ORDER ]; <nl> + float tmp [ TNS_MAX_ORDER + 1 ]; <nl>  <nl> for ( w = 0 ; w < ics -> num_windows ; w ++) { <nl> bottom = ics -> num_swb ;
ogm_dshow_header ( AVFormatContext * s , int idx ) <nl> if (* p != 1 ) <nl> return 1 ; <nl>  <nl> + if ( os -> psize < 100 ) <nl> + return AVERROR_INVALIDDATA ; <nl> t = AV_RL32 ( p + 96 ); <nl>  <nl> if ( t == 0x05589f80 ){
int64_t av_rescale_rnd ( int64_t a , int64_t b , int64_t c , enum AVRounding rnd ) <nl> else { <nl> int64_t ad = a / c ; <nl> int64_t a2 = ( a % c * b + r ) / c ; <nl> - if ( ad >= INT32_MAX && ad > ( INT64_MAX - a2 ) / b ) <nl> + if ( ad >= INT32_MAX && b && ad > ( INT64_MAX - a2 ) / b ) <nl> return INT64_MIN ; <nl> return ad * b + a2 ; <nl> }
static int read_seek ( AVFormatContext * s , int stream_index , <nl> next_node [ 1 ]-> pos , next_node [ 1 ]-> pos , <nl> next_node [ 0 ]-> ts , next_node [ 1 ]-> ts , <nl> AVSEEK_FLAG_BACKWARD , & ts , nut_read_timestamp ); <nl> + if ( pos < 0 ) <nl> + return pos ; <nl>  <nl> if (!( flags & AVSEEK_FLAG_BACKWARD )) { <nl> dummy . pos = pos + 16 ;
static int decode_fctl_chunk ( AVFormatContext * s , APNGDemuxContext * ctx , AVPacket <nl> static int apng_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> APNGDemuxContext * ctx = s -> priv_data ; <nl> - int ret ; <nl> + int64_t ret ; <nl> int64_t size ; <nl> AVIOContext * pb = s -> pb ; <nl> uint32_t len , tag ;
static int update_context_from_thread ( AVCodecContext * dst , AVCodecContext * src , <nl> { <nl> int err = 0 ; <nl>  <nl> - if ( dst != src ) { <nl> + if ( dst != src && ( for_user || !( av_codec_get_codec_descriptor ( src )-> props & AV_CODEC_PROP_INTRA_ONLY ))) { <nl> dst -> time_base = src -> time_base ; <nl> dst -> framerate = src -> framerate ; <nl> dst -> width = src -> width ;
end : <nl> free_and_end : <nl> av_dict_free (& tmp ); <nl> av_freep (& avctx -> priv_data ); <nl> - if ( avctx -> internal ) <nl> + if ( avctx -> internal ) { <nl> av_freep (& avctx -> internal -> pool ); <nl> + av_frame_free (& avctx -> internal -> to_free ); <nl> + } <nl> av_freep (& avctx -> internal ); <nl> avctx -> codec = NULL ; <nl> goto end ;
static av_always_inline int wp_exp2 ( int16_t val ) <nl> return neg ? - res : res ; <nl> } <nl>  <nl> - static av_always_inline int wp_log2 ( int32_t val ) <nl> + static av_always_inline int wp_log2 ( uint32_t val ) <nl> { <nl> int bits ; <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> return ret ; <nl> fail : <nl> av_dict_free (& metadata ); <nl> + ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> ret = AVERROR_INVALIDDATA ; <nl> - ff_thread_release_buffer ( avctx , & s -> picture ); <nl> goto the_end ; <nl> } <nl> 
av_cold static int auto_matrix ( SwrContext * s ) <nl> } else <nl> maxval = INT_MAX ; <nl>  <nl> - if ( maxcoef > maxval ){ <nl> + if ( maxcoef > maxval || s -> rematrix_volume < 0 ){ <nl> maxcoef /= maxval ; <nl> for ( i = 0 ; i < SWR_CH_MAX ; i ++) <nl> for ( j = 0 ; j < SWR_CH_MAX ; j ++){
int av_packet_ref ( AVPacket * dst , const AVPacket * src ) <nl> ret = packet_alloc (& dst -> buf , src -> size ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl> - memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl> + if ( src -> size ) <nl> + memcpy ( dst -> buf -> data , src -> data , src -> size ); <nl>  <nl> dst -> data = dst -> buf -> data ; <nl> } else {
int main ( int argc , char * argv []) <nl> } <nl> } <nl>  <nl> - max = ( 1 << ( 8 * len )) - 1 ; <nl> + max = ( 1LL << ( 8 * len )) - 1 ; <nl>  <nl> f [ 0 ] = fopen ( argv [ 1 ], " rb "); <nl> f [ 1 ] = fopen ( argv [ 2 ], " rb ");
int attribute_align_arg avcodec_decode_audio4 ( AVCodecContext * avctx , <nl> * extended_data are doing it correctly */ <nl> if (* got_frame_ptr ) { <nl> planar = av_sample_fmt_is_planar ( frame -> format ); <nl> - channels = av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> + channels = frame -> channels ; <nl> if (!( planar && channels > AV_NUM_DATA_POINTERS )) <nl> frame -> extended_data = frame -> data ; <nl> } else {
static int mxf_read_generic_descriptor ( void * arg , AVIOContext * pb , int tag , int <nl> default : <nl> /* Private uid used by SONY C0023S01 . mxf */ <nl> if ( IS_KLV_KEY ( uid , mxf_sony_mpeg4_extradata )) { <nl> - descriptor -> extradata = av_malloc ( size ); <nl> + descriptor -> extradata = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! descriptor -> extradata ) <nl> return - 1 ; <nl> descriptor -> extradata_size = size ;
static int Faac_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl>  <nl> /* add current frame to the queue */ <nl> if ( frame ) { <nl> - if (( ret = ff_af_queue_add (& s -> afq , frame ) < 0 )) <nl> + if (( ret = ff_af_queue_add (& s -> afq , frame )) < 0 ) <nl> return ret ; <nl> } <nl> 
av_cold int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , <nl> } <nl> } <nl>  <nl> -# define USE_MMAP ( HAVE_MMAP && HAVE_MPROTECT && defined MAP_ANONYMOUS ) <nl> +# if HAVE_MMAP && HAVE_MPROTECT && defined ( MAP_ANONYMOUS ) <nl> +# define USE_MMAP 1 <nl> +# else <nl> +# define USE_MMAP 0 <nl> +# endif <nl>  <nl> /* precalculate horizontal scaler filter coefficients */ <nl> {
int ff_wma_run_level_decode ( AVCodecContext * avctx , GetBitContext * gb , <nl> } <nl> /** NOTE : EOB can be omitted */ <nl> if ( offset > num_coefs ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " overflow in spectral RLE , ignoring \ n "); <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " overflow (% d > % d ) in spectral RLE , ignoring \ n ", <nl> + offset , <nl> + num_coefs <nl> + ); <nl> return - 1 ; <nl> } <nl> 
static void read_ttag ( AVFormatContext * s , AVIOContext * pb , int taglen , const cha <nl> } <nl> else if (* dst ) <nl> dict_flags |= AV_DICT_DONT_STRDUP_VAL ; <nl> + else <nl> + av_freep (& dst ); <nl>  <nl> if ( dst ) <nl> av_dict_set (& s -> metadata , key , dst , dict_flags );
leave : <nl> av_log ( s , AV_LOG_ERROR , " Packet mismatch % d % d \ n ", last , orig_size + 11 ); <nl> avio_seek ( s -> pb , pos + 1 , SEEK_SET ); <nl> ret = resync ( s ); <nl> + av_free_packet ( pkt ); <nl> if ( ret >= 0 ) { <nl> - av_free_packet ( pkt ); <nl> goto retry ; <nl> } <nl> }
static void dvbsub_parse_page_segment ( AVCodecContext * avctx , <nl>  <nl> av_dlog ( avctx , " Page time out % ds , state % d \ n ", ctx -> time_out , page_state ); <nl>  <nl> - if ( page_state == 2 ) { <nl> + if ( page_state == 1 || page_state == 2 ) { <nl> delete_regions ( ctx ); <nl> delete_objects ( ctx ); <nl> delete_cluts ( ctx );
for examples see get_bits , show_bits , skip_bits , get_vlc <nl>  <nl> # define OPEN_READER ( name , gb ) \ <nl> unsigned int name ## _index = ( gb )-> index ; \ <nl> - int name ## _cache = 0 <nl> + unsigned int name ## _cache = 0 <nl>  <nl> # define CLOSE_READER ( name , gb ) ( gb )-> index = name ## _index <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> s -> current_picture -> linesize [ 2 ], w >> s -> chroma_h_shift , h >> s -> chroma_v_shift , <nl> EDGE_WIDTH >> s -> chroma_h_shift , EDGE_WIDTH >> s -> chroma_v_shift , EDGE_TOP | EDGE_BOTTOM ); <nl> } <nl> + emms_c (); <nl> } <nl>  <nl> ff_snow_frame_start ( s ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> } <nl> + emms_c (); <nl>  <nl> update_last_header_values ( s ); <nl> 
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int ret , i , type , size , flags , is_audio ; <nl> int64_t next , pos ; <nl> int64_t dts , pts = AV_NOPTS_VALUE ; <nl> - int sample_rate , channels ; <nl> + int sample_rate = 0 , channels = 0 ; <nl> AVStream * st = NULL ; <nl>  <nl> for (;; avio_skip ( s -> pb , 4 )){ /* pkt size is repeated at end . skip it */
static char * get_content_url ( xmlNodePtr * baseurl_nodes , <nl> return NULL ; <nl> } <nl> av_strlcpy ( tmp_str , url , sizeof ( tmp_str )); <nl> - av_free ( url ); <nl> } <nl> if ( rep_bandwidth_val && tmp_str [ 0 ] != '\ 0 ') { <nl> + // free any previously assigned url before reassigning <nl> + av_free ( url ); <nl> url = av_strireplace ( tmp_str , "$ Bandwidth $", ( const char *) rep_bandwidth_val ); <nl> if (! url ) { <nl> return NULL ;
int ff_ass_add_rect ( AVSubtitle * sub , const char * dialog , <nl> sub -> rects = rects ; <nl> sub -> end_display_time = FFMAX ( sub -> end_display_time , 10 * duration ); <nl> rects [ sub -> num_rects ] = av_mallocz ( sizeof (* rects [ 0 ])); <nl> + if (! rects [ sub -> num_rects ]) <nl> + goto errnomem ; <nl> rects [ sub -> num_rects ]-> type = SUBTITLE_ASS ; <nl> ret = av_bprint_finalize (& buf , & rects [ sub -> num_rects ]-> ass ); <nl> if ( ret < 0 )
static int transcode ( OutputFile * output_files , <nl> int64_t ipts_min ; <nl> double opts_min ; <nl>  <nl> - redo : <nl> ipts_min = INT64_MAX ; <nl> opts_min = 1e100 ; <nl> /* if ' q ' pressed , exits */ <nl> static int transcode ( OutputFile * output_files , <nl> if ( exit_on_error ) <nl> exit_program ( 1 ); <nl> av_free_packet (& pkt ); <nl> - goto redo ; <nl> + continue ; <nl> } <nl>  <nl> discard_packet :
static int select_reference_stream ( AVFormatContext * s ) <nl> ret = avformat_match_stream_specifier ( s , s -> streams [ i ], <nl> seg -> reference_stream_specifier ); <nl> if ( ret < 0 ) <nl> - break ; <nl> + return ret ; <nl> if ( ret > 0 ) { <nl> seg -> reference_stream_index = i ; <nl> break ;
static int rtmp_packet_read_one_chunk ( URLContext * h , RTMPPacket * p , <nl> prev -> data = p -> data ; <nl> prev -> read = p -> read ; <nl> prev -> offset = p -> offset ; <nl> + p -> data = NULL ; <nl> return AVERROR ( EAGAIN ); <nl> } <nl> 
int img_convert ( AVPicture * dst , int dst_pix_fmt , <nl> else <nl> int_pix_fmt = PIX_FMT_RGB24 ; <nl> } <nl> + if ( src_pix_fmt == int_pix_fmt ) <nl> + return - 1 ; <nl> if ( avpicture_alloc ( tmp , int_pix_fmt , dst_width , dst_height ) < 0 ) <nl> return - 1 ; <nl> ret = - 1 ;
static void compute_stereo ( MPADecodeContext * s , GranuleDef * g0 , GranuleDef * g1 ) <nl> { <nl> int i , j , k , l ; <nl> int sf_max , sf , len , non_zero_found ; <nl> - INTFLOAT (* is_tab )[ 16 ], * tab0 , * tab1 , tmp0 , tmp1 , v1 , v2 ; <nl> + INTFLOAT (* is_tab )[ 16 ], * tab0 , * tab1 , v1 , v2 ; <nl> + SUINTFLOAT tmp0 , tmp1 ; <nl> int non_zero_found_short [ 3 ]; <nl>  <nl> /* intensity stereo */
static av_always_inline av_const float roundf ( float x ) <nl> } <nl> # endif /* HAVE_ROUNDF */ <nl>  <nl> +# if ! HAVE_TRUNC <nl> + static av_always_inline av_const double trunc ( double x ) <nl> +{ <nl> + return ( x > 0 ) ? floor ( x ) : ceil ( x ); <nl> +} <nl> +# endif /* HAVE_TRUNC */ <nl> + <nl> # if ! HAVE_TRUNCF <nl> static av_always_inline av_const float truncf ( float x ) <nl> {
void ff_estimate_b_frame_motion ( MpegEncContext * s , <nl> score = fbmin ; <nl> type = MB_TYPE_BIDIR ; <nl> } <nl> - score = ( score * score + 128 * 256 )>> 16 ; <nl> + score = (( unsigned )( score * score + 128 * 256 ))>> 16 ; <nl> s -> mc_mb_var_sum += score ; <nl> s -> mc_mb_var [ mb_y * s -> mb_width + mb_x ] = score ; // FIXME use SSD <nl> }
static int configure_output_video_filter ( FilterGraph * fg , OutputFilter * ofilter , <nl> snprintf ( name , sizeof ( name ), " output stream % d :% d ", ost -> file_index , ost -> index ); <nl> ret = avfilter_graph_create_filter (& ofilter -> filter , <nl> avfilter_get_by_name (" buffersink "), <nl> - name , NULL , pix_fmts , fg -> graph ); <nl> + name , NULL , NULL , fg -> graph ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
static FilterGraph * init_simple_filtergraph ( InputStream * ist , OutputStream * ost ) <nl>  <nl> static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> { <nl> - InputStream * ist ; <nl> + InputStream * ist = NULL ; <nl> enum AVMediaType type = in -> filter_ctx -> input_pads [ in -> pad_idx ]. type ; <nl> int i ; <nl> 
static int decode_frame_headers ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl>  <nl> if ( ctx -> data_size == 16 ) <nl> return 4 ; <nl> - if ( ctx -> data_size > buf_size ) <nl> - ctx -> data_size = buf_size ; <nl> + ctx -> data_size = FFMIN ( ctx -> data_size , buf_size - 16 ); <nl>  <nl> bytestream2_skip (& gb , 3 ); // skip reserved byte and checksum <nl> 
static int tta_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> return - 1 ; <nl> } <nl> st -> codec -> extradata = av_mallocz ( st -> codec -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! st -> codec -> extradata ) { <nl> + st -> codec -> extradata_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> avio_seek ( s -> pb , start_offset , SEEK_SET ); <nl> avio_read ( s -> pb , st -> codec -> extradata , st -> codec -> extradata_size ); <nl> 
int av_grow_packet ( AVPacket * pkt , int grow_by ) <nl> pkt -> buf = av_buffer_alloc ( new_size ); <nl> if (! pkt -> buf ) <nl> return AVERROR ( ENOMEM ); <nl> - memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> + if ( pkt -> size > 0 ) <nl> + memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> pkt -> data = pkt -> buf -> data ; <nl> } <nl> pkt -> size += grow_by ;
static int remove_decoded_packets ( AVFormatContext * ctx , int64_t scr ){ <nl> if ( stream -> buffer_index < pkt_desc -> size || <nl> stream -> predecode_packet == stream -> premux_packet ){ <nl> av_log ( ctx , AV_LOG_ERROR , <nl> - " buffer underflow i =% d bufi =% d size =% d \ n ", <nl> + " buffer underflow st =% d bufi =% d size =% d \ n ", <nl> i , stream -> buffer_index , pkt_desc -> size ); <nl> break ; <nl> }
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterBufferRef * out ; <nl> int hsub0 = desc -> log2_chroma_w ; <nl> int vsub0 = desc -> log2_chroma_h ; <nl> - int direct ; <nl> + int direct = 0 ; <nl> int plane ; <nl>  <nl> if (( in -> perms & AV_PERM_WRITE ) && !( in -> perms & AV_PERM_PRESERVE )) {
static int mxf_read_primer_pack ( void * arg , AVIOContext * pb , int tag , int size , U <nl> avpriv_request_sample ( pb , " Primer pack item length % d ", item_len ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> - if ( item_num > 65536 ) { <nl> + if ( item_num > 65536 || item_num < 0 ) { <nl> av_log ( mxf -> fc , AV_LOG_ERROR , " item_num % d is too large \ n ", item_num ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl> while ( x < w ) { <nl> int err , pred ; <nl>  <nl> + if ( get_bits_left (& s -> gb ) <= 0 ) <nl> + return ; <nl> + <nl> /* compute gradients */ <nl> Ra = x ? R ( dst , x - stride ) : R ( last , x ); <nl> Rb = R ( last , x );
static void amr_decode_fix_avctx ( AVCodecContext * avctx ) <nl> { <nl> const int is_amr_wb = 1 + ( avctx -> codec_id == AV_CODEC_ID_AMR_WB ); <nl>  <nl> - avctx -> sample_rate = 8000 * is_amr_wb ; <nl> + if (! avctx -> sample_rate ) <nl> + avctx -> sample_rate = 8000 * is_amr_wb ; <nl>  <nl> if ( avctx -> channels > 1 ) { <nl> av_log_missing_feature ( avctx , " multi - channel AMR ", 0 );
static int decode_frame ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " The answer to life , universe and everything is not correct !\ n "); <nl> return - 1 ; <nl> } <nl> + // Reset these pointers so we can tell if they were set this frame <nl> + s -> stripsizes = s -> stripdata = NULL ; <nl> /* parse image file directory */ <nl> off = tget_long (& buf , le ); <nl> if ( off >= UINT_MAX - 14 || end_buf - orig_buf < off + 14 ) {
static int svq3_decode_frame ( AVCodecContext * avctx , void * data , <nl> h -> mb_x = h -> mb_y = h -> mb_xy = 0 ; <nl>  <nl> if ( s -> watermark_key ) { <nl> - av_fast_malloc (& s -> buf , & s -> buf_size , <nl> - buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + av_fast_padded_malloc (& s -> buf , & s -> buf_size , buf_size ); <nl> if (! s -> buf ) <nl> return AVERROR ( ENOMEM ); <nl> memcpy ( s -> buf , avpkt -> data , buf_size );
int avresample_convert ( AVAudioResampleContext * avr , void ** output , <nl> } <nl> } <nl>  <nl> - return handle_buffered_output ( avr , & output_buffer , current_buffer ); <nl> + return handle_buffered_output ( avr , output ? & output_buffer : NULL , <nl> + current_buffer ); <nl> } <nl>  <nl> int avresample_available ( AVAudioResampleContext * avr )
static void mov_metadata_creation_time ( AVDictionary ** metadata , int64_t time ) <nl> if ( time ) { <nl> if ( time >= 2082844800 ) <nl> time -= 2082844800 ; /* seconds between 1904 - 01 - 01 and Epoch */ <nl> + <nl> + if (( int64_t )( time * 1000000ULL ) / 1000000 != time ) { <nl> + av_log ( NULL , AV_LOG_DEBUG , " creation_time is not representable \ n "); <nl> + return ; <nl> + } <nl> + <nl> avpriv_dict_set_timestamp ( metadata , " creation_time ", time * 1000000 ); <nl> } <nl> }
int ff_rm_read_mdpr_codecdata ( AVFormatContext * s , AVIOContext * pb , <nl> skip : <nl> /* skip codec info */ <nl> size = avio_tell ( pb ) - codec_pos ; <nl> - avio_skip ( pb , codec_data_size - size ); <nl> + if ( codec_data_size >= size ) { <nl> + avio_skip ( pb , codec_data_size - size ); <nl> + } else { <nl> + av_log ( s , AV_LOG_WARNING , " codec_data_size % u < size % d \ n ", codec_data_size , size ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void update_stream_timings ( AVFormatContext * ic ) <nl> if ( end_time != INT64_MIN ) <nl> duration = FFMAX ( duration , end_time - start_time ); <nl> } <nl> - if ( duration != INT64_MIN && ic -> duration == AV_NOPTS_VALUE ) { <nl> + if ( duration != INT64_MIN && duration > 0 && ic -> duration == AV_NOPTS_VALUE ) { <nl> ic -> duration = duration ; <nl> } <nl> if ( ic -> pb && ( filesize = avio_size ( ic -> pb )) > 0 && ic -> duration != AV_NOPTS_VALUE ) {
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> return res ; <nl> av_assert1 ( block_duration != AV_NOPTS_VALUE ); <nl>  <nl> - block_time = AV_RB16 ( data ); <nl> + block_time = sign_extend ( AV_RB16 ( data ), 16 ); <nl> data += 2 ; <nl> flags = * data ++; <nl> size -= 3 ;
static av_cold int adpcm_decode_init ( AVCodecContext * avctx ) <nl> max_channels = 6 ; <nl> break ; <nl> } <nl> - if ( avctx -> channels > max_channels ){ <nl> - return - 1 ; <nl> + if ( avctx -> channels <= 0 || avctx -> channels > max_channels ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR ( EINVAL ); <nl> } <nl>  <nl> switch ( avctx -> codec -> id ) {
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> entries = avio_rb32 ( pb ); <nl> if ( entries >= UINT_MAX / sizeof (* sc -> drefs )) <nl> return AVERROR_INVALIDDATA ; <nl> + av_free ( sc -> drefs ); <nl> sc -> drefs = av_mallocz ( entries * sizeof (* sc -> drefs )); <nl> if (! sc -> drefs ) <nl> return AVERROR ( ENOMEM );
int ff_img_read_header ( AVFormatContext * s1 ) <nl> break ; <nl> } <nl> } <nl> - ffio_rewind_with_probe_data ( s1 -> pb , & probe_buffer , probe_buffer_size ); <nl> + if ( s1 -> flags & AVFMT_FLAG_CUSTOM_IO ) { <nl> + avio_seek ( s1 -> pb , 0 , SEEK_SET ); <nl> + } else <nl> + ffio_rewind_with_probe_data ( s1 -> pb , & probe_buffer , probe_buffer_size ); <nl> } <nl> if ( st -> codec -> codec_id == AV_CODEC_ID_NONE ) <nl> st -> codec -> codec_id = ff_guess_image2_codec ( s -> path );
static int get_packet ( URLContext * s , int for_header ) <nl> } <nl> } <nl> rt -> bytes_read += ret ; <nl> - if ( rt -> bytes_read > rt -> last_bytes_read + rt -> client_report_size ) { <nl> + if ( rt -> bytes_read - rt -> last_bytes_read > rt -> client_report_size ) { <nl> av_log ( s , AV_LOG_DEBUG , " Sending bytes read report \ n "); <nl> gen_bytes_read ( s , rt , rpkt . timestamp + 1 ); <nl> rt -> last_bytes_read = rt -> bytes_read ;
void ff_parse_specific_params ( AVStream * st , int * au_rate , <nl>  <nl> void ff_riff_write_info_tag ( AVIOContext * pb , const char * tag , const char * str ) <nl> { <nl> - int len = strlen ( str ); <nl> - if ( len > 0 ) { <nl> + size_t len = strlen ( str ); <nl> + if ( len > 0 && len < UINT32_MAX ) { <nl> len ++; <nl> ffio_wfourcc ( pb , tag ); <nl> avio_wl32 ( pb , len );
static int decode_blocks ( SnowContext * s ){ <nl>  <nl> for ( y = 0 ; y < h ; y ++){ <nl> for ( x = 0 ; x < w ; x ++){ <nl> + if ( s -> c . bytestream >= s -> c . bytestream_end ) <nl> + return AVERROR_INVALIDDATA ; <nl> if (( res = decode_q_branch ( s , 0 , x , y )) < 0 ) <nl> return res ; <nl> }
int64_t ff_ape_parse_tag ( AVFormatContext * s ) <nl> return 0 ; <nl> } <nl>  <nl> - tag_start = file_size - tag_bytes - APE_TAG_FOOTER_BYTES ; <nl> - if ( tag_start < 0 ) { <nl> + if ( tag_bytes > file_size - APE_TAG_FOOTER_BYTES ) { <nl> av_log ( s , AV_LOG_ERROR , " Invalid tag size % u .\ n ", tag_bytes ); <nl> return 0 ; <nl> } <nl> + tag_start = file_size - tag_bytes - APE_TAG_FOOTER_BYTES ; <nl>  <nl> fields = avio_rl32 ( pb ); /* number of fields */ <nl> if ( fields > 65536 ) {
static int dca_decode_frame ( AVCodecContext * avctx , <nl> } else <nl> s -> channel_order_tab = dca_channel_reorder_nolfe [ s -> amode ]; <nl>  <nl> + if ( s -> prim_channels > 0 && <nl> + s -> channel_order_tab [ s -> prim_channels - 1 ] < 0 ) <nl> + return - 1 ; <nl> + <nl> if ( avctx -> request_channels == 2 && s -> prim_channels > 2 ) { <nl> channels = 2 ; <nl> s -> output = DCA_STEREO ;
static int theora_header ( AVFormatContext * s , int idx ) <nl> st -> codec -> extradata_size = 0 ; <nl> return err ; <nl> } <nl> + memset ( st -> codec -> extradata + cds , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + <nl> cdp = st -> codec -> extradata + st -> codec -> extradata_size ; <nl> * cdp ++ = os -> psize >> 8 ; <nl> * cdp ++ = os -> psize & 0xff ;
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> uint8_t * end = dst + ( width - 2 ); <nl>  <nl> avpriv_request_sample ( l -> avctx , " zero_run_line "); <nl> - return AVERROR_PATCHWELCOME ; <nl> + <nl> + memset ( dst , 0 , width ); <nl>  <nl> output_zeros : <nl> if ( l -> zeros_rem ) {
static int mpeg_mux_init ( AVFormatContext * ctx ) <nl> if (! s -> mux_rate ) { <nl> /* we increase slightly the bitrate to take into account the <nl> headers . XXX : compute it exactly */ <nl> - bitrate += bitrate * 5 / 100 ; <nl> + bitrate += bitrate * 5LL / 100 ; <nl> bitrate += 10000 ; <nl> s -> mux_rate = ( bitrate + ( 8 * 50 ) - 1 ) / ( 8 * 50 ); <nl> }
int main ( void ){ <nl> put_cabac (& c , state , r [ i ]& 1 ); <nl> } <nl>  <nl> - put_cabac_terminate (& c , 1 ); <nl> + i = put_cabac_terminate (& c , 1 ); <nl> + b [ i ++] = av_lfg_get (& prng ); <nl> + b [ i ] = av_lfg_get (& prng ); <nl>  <nl> ff_init_cabac_decoder (& c , b , SIZE ); <nl> 
static int mxf_write_header ( AVFormatContext * s ) <nl> mxf -> edit_unit_byte_count += klv_fill_size ( mxf -> edit_unit_byte_count ); <nl>  <nl> sc -> signal_standard = 1 ; <nl> + sc -> color_siting = 0 ; <nl> } <nl> if ( mxf -> signal_standard >= 0 ) <nl> sc -> signal_standard = mxf -> signal_standard ;
AVCodec ff_ffv1_decoder = { <nl> . update_thread_context = ONLY_IF_THREADS_ENABLED ( update_thread_context ), <nl> . capabilities = AV_CODEC_CAP_DR1 /*| AV_CODEC_CAP_DRAW_HORIZ_BAND */ | <nl> AV_CODEC_CAP_FRAME_THREADS | AV_CODEC_CAP_SLICE_THREADS , <nl> + . caps_internal = FF_CODEC_CAP_INIT_CLEANUP <nl> };
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> cases . */ <nl> static void imdct12 ( INTFLOAT * out , INTFLOAT * in ) <nl> { <nl> - INTFLOAT in0 , in1 , in2 , in3 , in4 , in5 , t1 , t2 ; <nl> + SUINTFLOAT in0 , in1 , in2 , in3 , in4 , in5 , t1 , t2 ; <nl>  <nl> in0 = in [ 0 * 3 ]; <nl> in1 = in [ 1 * 3 ] + in [ 0 * 3 ];
static int get_stats ( AVCodecContext * avctx , int eos ) <nl> // libtheora generates a summary header at the end <nl> memcpy ( h -> stats , buf , bytes ); <nl> avctx -> stats_out = av_malloc ( b64_size ); <nl> + if (! avctx -> stats_out ) <nl> + return AVERROR ( ENOMEM ); <nl> av_base64_encode ( avctx -> stats_out , b64_size , h -> stats , h -> stats_offset ); <nl> } <nl> return 0 ;
static const struct URLProtocol * url_find_protocol ( const char * filename ) <nl> * ptr = '\ 0 '; <nl>  <nl> protocols = ffurl_get_protocols ( NULL , NULL ); <nl> + if (! protocols ) <nl> + return NULL ; <nl> for ( i = 0 ; protocols [ i ]; i ++) { <nl> const URLProtocol * up = protocols [ i ]; <nl> if (! strcmp ( proto_str , up -> name )) {
static int decode_block ( MJpegDecodeContext * s , int16_t * block , int component , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> val = val * quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> - val = FFMIN ( val , 32767 ); <nl> + val = av_clip_int16 ( val ); <nl> s -> last_dc [ component ] = val ; <nl> block [ 0 ] = val ; <nl> /* AC coefs */
static int a64multi_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> } else { <nl> /* fill up mc_meta_charset with data until lifetime exceeds */ <nl> if ( c -> mc_frame_counter < c -> mc_lifetime ) { <nl> - * p = * pict ; <nl> + ret = av_frame_ref ( p , pict ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> p -> key_frame = 1 ; <nl> to_meta_with_crop ( avctx , p , meta + 32000 * c -> mc_frame_counter );
static av_cold void init_cplscales_table ( COOKContext * q ) <nl> static inline int decode_bytes ( const uint8_t * inbuffer , uint8_t * out , int bytes ) <nl> { <nl> static const uint32_t tab [ 4 ] = { <nl> - AV_BE2NE32C ( 0x37c511f2 ), AV_BE2NE32C ( 0xf237c511 ), <nl> - AV_BE2NE32C ( 0x11f237c5 ), AV_BE2NE32C ( 0xc511f237 ), <nl> + AV_BE2NE32C ( 0x37c511f2u ), AV_BE2NE32C ( 0xf237c511u ), <nl> + AV_BE2NE32C ( 0x11f237c5u ), AV_BE2NE32C ( 0xc511f237u ), <nl> }; <nl> int i , off ; <nl> uint32_t c ;
static void show_packets ( WriterContext * w , AVFormatContext * fmt_ctx ) <nl> if ( do_show_frames && <nl> get_video_frame ( fmt_ctx , & frame , & pkt )) { <nl> show_frame ( w , & frame , fmt_ctx -> streams [ pkt . stream_index ]); <nl> - av_free_packet (& pkt ); <nl> } <nl> + av_free_packet (& pkt ); <nl> } <nl> av_init_packet (& pkt ); <nl> pkt . data = NULL ;
static int flv_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> unsigned ts ; <nl> int size = pkt -> size ; <nl> uint8_t * data = NULL ; <nl> - int flags , flags_size ; <nl> + int flags = 0 , flags_size ; <nl>  <nl> // av_log ( s , AV_LOG_DEBUG , " type :% d pts : %" PRId64 " size :% d \ n ", <nl> // enc -> codec_type , timestamp , size );
static int writer_open ( WriterContext ** wctx , const Writer * writer , const char * a <nl> { <nl> int i , ret = 0 ; <nl>  <nl> - if (!(* wctx = av_malloc ( sizeof ( WriterContext )))) { <nl> + if (!(* wctx = av_mallocz ( sizeof ( WriterContext )))) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ; <nl> }
static int read_quant_tables ( RangeCoder * c , <nl> int context_count = 1 ; <nl>  <nl> for ( i = 0 ; i < 5 ; i ++) { <nl> - context_count *= read_quant_table ( c , quant_table [ i ], context_count ); <nl> + int ret = read_quant_table ( c , quant_table [ i ], context_count ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + context_count *= ret ; <nl> if ( context_count > 32768U ) { <nl> return AVERROR_INVALIDDATA ; <nl> }
static int execute_decode_slices ( H264Context * h , int context_count ) <nl> H264Context * hx ; <nl> int i ; <nl>  <nl> + if ( h -> mb_y >= h -> mb_height ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , <nl> + " Input contains more MB rows than the frame height .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( h -> avctx -> hwaccel ) <nl> return 0 ; <nl> if ( context_count == 1 ) {
static int sonic_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( buf_size == 0 ) return 0 ; <nl>  <nl> - s -> frame . nb_samples = s -> frame_size ; <nl> + s -> frame . nb_samples = s -> frame_size / avctx -> channels ; <nl> if (( ret = ff_get_buffer ( avctx , & s -> frame , 0 )) < 0 ) <nl> return ret ; <nl> samples = ( int16_t *) s -> frame . data [ 0 ];
static int adx_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf += header_size ; <nl> buf_size -= header_size ; <nl> } <nl> + if ( c -> channels <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> /* calculate number of blocks in the packet */ <nl> num_blocks = buf_size / ( BLOCK_SIZE * c -> channels );
static inline int asym_quant ( int c , int e , int qbits ) <nl> { <nl> int m ; <nl>  <nl> - c = ((( c << e ) >> ( 24 - qbits )) + 1 ) >> 1 ; <nl> + c = ((( c * ( 1 << e )) >> ( 24 - qbits )) + 1 ) >> 1 ; <nl> m = ( 1 << ( qbits - 1 )); <nl> if ( c >= m ) <nl> c = m - 1 ;
av_cold int ff_snow_common_init ( AVCodecContext * avctx ){ <nl> ff_scale_mv_ref [ i ][ j ] = 256 *( i + 1 )/( j + 1 ); <nl>  <nl> s -> avctx -> get_buffer ( s -> avctx , & s -> mconly_picture ); <nl> - s -> scratchbuf = av_malloc ( s -> mconly_picture . linesize [ 0 ]* 7 * MB_SIZE ); <nl> + s -> scratchbuf = av_mallocz ( s -> mconly_picture . linesize [ 0 ]* 7 * MB_SIZE ); <nl>  <nl> return 0 ; <nl> }
static void show_packets ( AVFormatContext * fmt_ctx ) <nl>  <nl> av_init_packet (& pkt ); <nl> probe_array_header (" packets ", 0 ); <nl> - while (! av_read_frame ( fmt_ctx , & pkt )) <nl> + while (! av_read_frame ( fmt_ctx , & pkt )) { <nl> show_packet ( fmt_ctx , & pkt ); <nl> + av_packet_unref (& pkt ); <nl> + } <nl> probe_array_footer (" packets ", 0 ); <nl> } <nl> 
static av_cold int tiff_end ( AVCodecContext * avctx ) <nl>  <nl> ff_lzw_decode_close (& s -> lzw ); <nl> av_freep (& s -> deinvert_buf ); <nl> + s -> deinvert_buf_size = 0 ; <nl> av_freep (& s -> fax_buffer ); <nl> s -> fax_buffer_size = 0 ; <nl> return 0 ;
typedef struct ListEntry { <nl>  <nl> typedef struct HLSContext { <nl> const AVClass * class ; // Class for private options . <nl> - int number ; <nl> + unsigned number ; <nl> int64_t sequence ; <nl> AVOutputFormat * oformat ; <nl> AVFormatContext * avf ;
static int read_len_table ( uint8_t * dst , GetBitContext * gb ){ <nl> if ( repeat == 0 ) <nl> repeat = get_bits ( gb , 8 ); <nl> // printf ("% d % d \ n ", val , repeat ); <nl> - if ( i + repeat > 256 ) { <nl> + if ( i + repeat > 256 || get_bits_left ( gb ) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error reading huffman table \ n "); <nl> return - 1 ; <nl> }
static void decode_channel_map ( uint8_t layout_map [][ 3 ], <nl> case AAC_CHANNEL_LFE : <nl> syn_ele = TYPE_LFE ; <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl> layout_map [ 0 ][ 0 ] = syn_ele ; <nl> layout_map [ 0 ][ 1 ] = get_bits ( gb , 4 );
static int decode_frame ( AVCodecContext * avctx , <nl> buf += 5 ; <nl>  <nl> if ( video_size ) { <nl> + if ( video_size < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " video size % d invalid \ n ", video_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( avctx -> reget_buffer ( avctx , & s -> frame ) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return - 1 ;
static int prompeg_write ( URLContext * h , const uint8_t * buf , int size ) { <nl>  <nl> // FEC ( column ) send block - aligned <nl> if (! s -> first && s -> packet_idx % s -> d == 0 ) { <nl> - col_out_idx = s -> packet_idx / s -> l ; <nl> + col_out_idx = s -> packet_idx / s -> d ; <nl> if (( ret = prompeg_write_fec ( h , s -> fec_col [ col_out_idx ], PROMPEG_FEC_COL )) < 0 ) <nl> goto end ; <nl> written += ret ;
static void swap_guid ( ff_asf_guid guid ) <nl>  <nl> static void align_position ( AVIOContext * pb , int64_t offset , uint64_t size ) <nl> { <nl> - if ( avio_tell ( pb ) != offset + size ) <nl> + if ( size < INT64_MAX - offset && avio_tell ( pb ) != offset + size ) <nl> avio_seek ( pb , offset + size , SEEK_SET ); <nl> } <nl> 
static int prepare_packet ( AVPacket * pkt , const FailingMuxerPacketData * pkt_data , <nl> { <nl> int ret ; <nl> FailingMuxerPacketData * data = av_malloc ( sizeof (* data )); <nl> + if (! data ) { <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> memcpy ( data , pkt_data , sizeof ( FailingMuxerPacketData )); <nl> ret = av_packet_from_data ( pkt , ( uint8_t *) data , sizeof (* data )); <nl> 
static int sami_paragraph_to_ass ( AVCodecContext * avctx , const char * src ) <nl> AVBPrint * dst_content = & sami -> encoded_content ; <nl> AVBPrint * dst_source = & sami -> encoded_source ; <nl>  <nl> + if (! dupsrc ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> av_bprint_clear (& sami -> encoded_content ); <nl> av_bprint_clear (& sami -> content ); <nl> av_bprint_clear (& sami -> encoded_source );
static int dnxhd_probe ( AVProbeData * p ) <nl> if (! w || ! h ) <nl> return 0 ; <nl> compression_id = AV_RB32 ( p -> buf + 0x28 ); <nl> - if ( compression_id < 1235 || compression_id > 1258 ) <nl> + if ( compression_id < 1235 || compression_id > 1260 ) <nl> return 0 ; <nl> return AVPROBE_SCORE_MAX ; <nl> }
static void sub_hfyu_median_prediction_int16_c ( uint16_t * dst , const uint16_t * sr <nl> * left_top = lt ; <nl> } <nl>  <nl> - static int add_hfyu_left_prediction_int16_c ( uint16_t * dst , const uint16_t * src , unsigned mask , int w , int acc ){ <nl> + static int add_hfyu_left_prediction_int16_c ( uint16_t * dst , const uint16_t * src , unsigned mask , int w , unsigned acc ){ <nl> int i ; <nl>  <nl> for ( i = 0 ; i < w - 1 ; i ++){
static int dsf_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> st -> codecpar -> block_align *= st -> codecpar -> channels ; <nl> + st -> codecpar -> bit_rate = st -> codecpar -> channels * st -> codecpar -> sample_rate * 8LL ; <nl> avio_skip ( pb , 4 ); <nl>  <nl> /* data chunk */
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> if ( st -> codec -> bit_rate > 0 ) { <nl> - if ( INT_MAX - st -> codec -> bit_rate > bit_rate ) { <nl> + if ( INT_MAX - st -> codec -> bit_rate < bit_rate ) { <nl> bit_rate = 0 ; <nl> break ; <nl> }
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 4 ] = jvf -> video_type ; <nl> if (( size = avio_read ( pb , pkt -> data + JV_PREAMBLE_SIZE , size )) < 0 ) <nl> return AVERROR ( EIO ); <nl> + memset ( pkt -> data + JV_PREAMBLE_SIZE + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> pkt -> size = size + JV_PREAMBLE_SIZE ; <nl> pkt -> stream_index = 1 ;
static av_cold int tdsc_init ( AVCodecContext * avctx ) <nl> ctx -> jpeg_avctx -> flags2 = avctx -> flags2 ; <nl> ctx -> jpeg_avctx -> dct_algo = avctx -> dct_algo ; <nl> ctx -> jpeg_avctx -> idct_algo = avctx -> idct_algo ;; <nl> - ret = avcodec_open2 ( ctx -> jpeg_avctx , codec , NULL ); <nl> + ret = ff_codec_open2_recursive ( ctx -> jpeg_avctx , codec , NULL ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
int av_reallocp ( void * ptr , size_t size ) <nl> void ** ptrptr = ptr ; <nl> void * ret ; <nl>  <nl> + if (! size ) { <nl> + av_freep ( ptr ); <nl> + return 0 ; <nl> + } <nl> ret = av_realloc (* ptrptr , size ); <nl>  <nl> if (! ret ) {
static int decode_slice ( AVCodecContext * avctx , void * tdata ) <nl>  <nl> /* if V or alpha component size is negative that means that previous <nl> component sizes are too large */ <nl> - if ( v_data_size < 0 || a_data_size < 0 || hdr_size < 6 ) { <nl> + if ( v_data_size < 0 || a_data_size < 0 || hdr_size < 6 || coff [ 3 ] > slice_data_size ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid data size \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame_header ( AVCodecContext * ctx , <nl> s -> lf_delta . ref [ 3 ] = - 1 ; <nl> s -> lf_delta . mode [ 0 ] = 0 ; <nl> s -> lf_delta . mode [ 1 ] = 0 ; <nl> + memset ( s -> segmentation . feat , 0 , sizeof ( s -> segmentation . feat )); <nl> } <nl> s -> filter . level = get_bits (& s -> gb , 6 ); <nl> sharp = get_bits (& s -> gb , 3 );
static void mxf_read_pixel_layout ( AVIOContext * pb , MXFDescriptor * descriptor ) <nl> if ( ofs <= 14 ) { <nl> layout [ ofs ++] = code ; <nl> layout [ ofs ++] = value ; <nl> - } <nl> + } else <nl> + break ; /* don ' t read byte by byte on sneaky files filled with lots of non - zeroes */ <nl> } while ( code != 0 ); /* SMPTE 377M E . 2 . 46 */ <nl>  <nl> ff_mxf_decode_pixel_layout ( layout , & descriptor -> pix_fmt );
static int on2avc_decode_band_scales ( On2AVCContext * c , GetBitContext * gb ) <nl> } else { <nl> scale += get_vlc2 ( gb , c -> scale_diff . table , 9 , 3 ) - 60 ; <nl> } <nl> - if ( scale < 0 || scale > 128 ) { <nl> + if ( scale < 0 || scale > 127 ) { <nl> av_log ( c -> avctx , AV_LOG_ERROR , " Invalid scale value % d \ n ", <nl> scale ); <nl> return AVERROR_INVALIDDATA ;
static int check ( AVIOContext * pb , int64_t pos , uint32_t * ret_header ) <nl> return CHECK_SEEK_FAILED ; <nl>  <nl> ret = avio_read ( pb , & header_buf [ 0 ], 4 ); <nl> - if ( ret < 0 ) <nl> + /* We should always find four bytes for a valid mpa header . */ <nl> + if ( ret < 4 ) <nl> return CHECK_SEEK_FAILED ; <nl>  <nl> header = AV_RB32 (& header_buf [ 0 ]);
static int vqf_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 1 ] = c -> last_frame_bits ; <nl> ret = avio_read ( s -> pb , pkt -> data + 2 , size ); <nl>  <nl> - if ( ret <= 0 ) { <nl> + if ( ret != size ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO ); <nl> }
static int parse_playlist ( AppleHTTPContext * c , const char * url , <nl> enum KeyType key_type = KEY_NONE ; <nl> uint8_t iv [ 16 ] = ""; <nl> int has_iv = 0 ; <nl> - char key [ MAX_URL_SIZE ]; <nl> + char key [ MAX_URL_SIZE ] = ""; <nl> char line [ 1024 ]; <nl> const char * ptr ; <nl> int close_in = 0 ;
static int read_highpass ( AVCodecContext * avctx , uint8_t * ptr , int plane , AVFrame <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( a == INT32_MIN ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> ret = read_high_coeffs ( avctx , ptr + bytestream2_tell (& ctx -> gb ), dest , size , <nl> c , ( b >= FFABS ( a )) ? b : a , d , <nl> ctx -> band [ plane ][ i + 1 ]. width , stride );
static int svq3_decode_frame ( AVCodecContext * avctx , <nl> s -> next_p_frame_damaged = 0 ; <nl> } <nl>  <nl> - frame_start ( h ); <nl> + if ( frame_start ( h ) < 0 ) <nl> + return - 1 ; <nl>  <nl> if ( s -> pict_type == B_TYPE ) { <nl> h -> frame_num_offset = ( h -> slice_num - h -> prev_frame_num );
int ff_hevc_annexb2mp4 ( AVIOContext * pb , const uint8_t * buf_in , <nl> } <nl>  <nl> end : <nl> - free ( start ); <nl> + av_free ( start ); <nl> if ( ps_count ) <nl> * ps_count = num_ps ; <nl> return ret ; <nl> int ff_hevc_annexb2mp4_buf ( const uint8_t * buf_in , uint8_t ** buf_out , <nl> * size = avio_close_dyn_buf ( pb , buf_out ); <nl>  <nl> end : <nl> - free ( start ); <nl> + av_free ( start ); <nl> if ( ps_count ) <nl> * ps_count = num_ps ; <nl> return ret ;
int ff_mov_read_esds ( AVFormatContext * fc , ByteIOContext * pb , MOVAtom atom ) <nl> dprintf ( fc , " Specific MPEG4 header len =% d \ n ", len ); <nl> if (( uint64_t ) len > ( 1 << 30 )) <nl> return - 1 ; <nl> + av_free ( st -> codec -> extradata ); <nl> st -> codec -> extradata = av_mallocz ( len + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! st -> codec -> extradata ) <nl> return AVERROR ( ENOMEM );
static int ape_read_header ( AVFormatContext * s ) <nl> ape -> seektablelength / sizeof (* ape -> seektable ), ape -> totalframes ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> - ape -> frames = av_malloc ( ape -> totalframes * sizeof ( APEFrame )); <nl> + ape -> frames = av_malloc_array ( ape -> totalframes , sizeof ( APEFrame )); <nl> if (! ape -> frames ) <nl> return AVERROR ( ENOMEM ); <nl> ape -> firstframe = ape -> junklength + ape -> descriptorlength + ape -> headerlength + ape -> seektablelength + ape -> wavheaderlength ;
again : <nl> break ; <nl>  <nl> if ( h -> sei_recovery_frame_cnt >= 0 && ( h -> frame_num != h -> sei_recovery_frame_cnt || hx -> slice_type_nos != AV_PICTURE_TYPE_I )) <nl> - h -> valid_recovery_point ++; <nl> + h -> valid_recovery_point = 1 ; <nl>  <nl> if ( h -> sei_recovery_frame_cnt >= 0 <nl> && ( h -> recovery_frame < 0
unsigned avutil_version ( void ) <nl> av_assert0 ( LIBAVUTIL_VERSION_MICRO >= 100 ); <nl> av_assert0 ( HAVE_MMX2 == HAVE_MMXEXT ); <nl>  <nl> + if ( av_sat_dadd32 ( 1 , 2 ) != 5 ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Libavutil has been build with a broken binutils , please upgrade binutils and rebuild \ n "); <nl> + abort (); <nl> + } <nl> + <nl> return LIBAVUTIL_VERSION_INT ; <nl> } <nl> 
static float dca_dmix_code ( unsigned code ) <nl> static int scan_for_extensions ( AVCodecContext * avctx ) <nl> { <nl> DCAContext * s = avctx -> priv_data ; <nl> - int core_ss_end , ret ; <nl> + int core_ss_end , ret = 0 ; <nl>  <nl> core_ss_end = FFMIN ( s -> frame_size , s -> dca_buffer_size ) * 8 ; <nl> 
av_cold static int auto_matrix ( SwrContext * s ) <nl>  <nl> memset ( s -> matrix , 0 , sizeof ( s -> matrix )); <nl> for ( i = 0 ; i < 64 ; i ++){ <nl> - if ( in_ch_layout & out_ch_layout & ( 1LL << i )) <nl> + if ( in_ch_layout & out_ch_layout & ( 1ULL << i )) <nl> matrix [ i ][ i ]= 1 . 0 ; <nl> } <nl> 
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl> AVCodecContext * ctx = avcodec_alloc_context3 ( NULL ); <nl> if (! ctx ) <nl> error (" Failed memory allocation "); <nl> + <nl> + ctx -> max_pixels = 4096 * 4096 ; // To reduce false positive OOM and hangs <nl> + <nl> int res = avcodec_open2 ( ctx , c , NULL ); <nl> if ( res < 0 ) <nl> return res ;
reload : <nl>  <nl> static int hls_read_header ( AVFormatContext * s ) <nl> { <nl> - URLContext * u = s -> pb -> opaque ; <nl> + URLContext * u = ( s -> flags & AVFMT_FLAG_CUSTOM_IO ) ? NULL : s -> pb -> opaque ; <nl> HLSContext * c = s -> priv_data ; <nl> int ret = 0 , i , j , stream_offset = 0 ; <nl> 
int ff_h2645_packet_split ( H2645Packet * pkt , const uint8_t * buf , int length , <nl>  <nl> buf += 3 ; <nl> length -= 3 ; <nl> - extract_length = length ; <nl> + extract_length = FFMIN ( length , next_avc - buf ); <nl>  <nl> if ( buf >= next_avc ) { <nl> /* skip to the start of the next NAL */
static int roq_dpcm_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> context -> input_frames ++; <nl> return 0 ; <nl> } <nl> + } <nl> + if ( context -> input_frames < 8 ) { <nl> in = context -> frame_buffer ; <nl> } <nl> 
int ff_qsv_enc_init ( AVCodecContext * avctx , QSVEncContext * q ) <nl> } <nl>  <nl> ret = MFXVideoENCODE_Init ( q -> session , & q -> param ); <nl> - if ( ret < 0 ) { <nl> + if ( MFX_WRN_PARTIAL_ACCELERATION == ret ) { <nl> + av_log ( avctx , AV_LOG_WARNING , " Encoder will work with partial HW acceleration \ n "); <nl> + } else if ( ret < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Error initializing the encoder \ n "); <nl> return ff_qsv_error ( ret ); <nl> }
static void dvbsub_parse_region_segment ( AVCodecContext * avctx , <nl> } <nl>  <nl> region -> depth = 1 << (((* buf ++) >> 2 ) & 7 ); <nl> + if ( region -> depth < 2 || region -> depth > 8 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " region depth % d is invalid \ n ", region -> depth ); <nl> + region -> depth = 4 ; <nl> + } <nl> region -> clut = * buf ++; <nl>  <nl> if ( region -> depth == 8 )
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl>  <nl> if ( h -> ref_count [ 0 ] > max_refs || h -> ref_count [ 1 ] > max_refs ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " reference overflow \ n "); <nl> - h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 1 ; <nl> + h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int latm_decode_audio_specific_config ( struct LATMContext * latmctx , <nl> " config not byte aligned .\ n ", 1 ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( asclen <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> bits_consumed = decode_audio_specific_config ( NULL , avctx , & m4ac , <nl> gb -> buffer + ( config_start_bit / 8 ), <nl> asclen , sync_extension );
static int output_frame ( AVFilterLink * outlink , int nb_samples ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> in_buf = ff_get_audio_buffer ( outlink , AV_PERM_WRITE , nb_samples ); <nl> - if (! in_buf ) <nl> + if (! in_buf ) { <nl> + avfilter_unref_buffer ( out_buf ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> for ( i = 0 ; i < s -> nb_inputs ; i ++) { <nl> if ( s -> input_state [ i ] == INPUT_ON ) {
static void opt_list ( void * obj , void * av_log_obj , const char * unit , <nl> av_log ( av_log_obj , AV_LOG_INFO , " ( default "); <nl> switch ( opt -> type ) { <nl> case AV_OPT_TYPE_FLAGS : <nl> - av_log ( av_log_obj , AV_LOG_INFO , "% 0llX ", opt -> default_val . i64 ); <nl> + av_log ( av_log_obj , AV_LOG_INFO , "%" PRIX64 , opt -> default_val . i64 ); <nl> break ; <nl> case AV_OPT_TYPE_DURATION : <nl> case AV_OPT_TYPE_INT :
int vp56_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> int is_alpha , av_uninit ( alpha_offset ); <nl>  <nl> if ( s -> has_alpha ) { <nl> + if ( remaining_buf_size < 3 ) <nl> + return - 1 ; <nl> alpha_offset = bytestream_get_be24 (& buf ); <nl> remaining_buf_size -= 3 ; <nl> + if ( remaining_buf_size < alpha_offset ) <nl> + return - 1 ; <nl> } <nl>  <nl> for ( is_alpha = 0 ; is_alpha < 1 + s -> has_alpha ; is_alpha ++) {
int av_strerror ( int errnum , char * errbuf , size_t errbuf_size ) <nl> av_strlcpy ( errbuf , entry -> str , errbuf_size ); <nl> } else { <nl> # if HAVE_STRERROR_R <nl> - ret = strerror_r ( AVUNERROR ( errnum ), errbuf , errbuf_size ); <nl> + ret = AVERROR ( strerror_r ( AVUNERROR ( errnum ), errbuf , errbuf_size )); <nl> # else <nl> ret = - 1 ; <nl> # endif
static int mov_read_sidx ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> MOVFragmentStreamInfo * si ; <nl> si = & item -> stream_info [ j ]; <nl> if ( si -> sidx_pts != AV_NOPTS_VALUE ) { <nl> - ref_st = c -> fc -> streams [ i ]; <nl> + ref_st = c -> fc -> streams [ j ]; <nl> ref_sc = ref_st -> priv_data ; <nl> break ; <nl> }
int av_bsf_list_parse_str ( const char * str , AVBSFContext ** bsf_lst ) <nl> if (! lst ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - if (!( dup = buf = av_strdup ( str ))) <nl> - return AVERROR ( ENOMEM ); <nl> + if (!( dup = buf = av_strdup ( str ))) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl>  <nl> while ( 1 ) { <nl> bsf_str = av_strtok ( buf , ",", & saveptr );
static av_cold int wavpack_encode_init ( AVCodecContext * avctx ) <nl> s -> avctx = avctx ; <nl>  <nl> if ( avctx -> channels > 255 ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " Too many channels \ n ", avctx -> channels ); <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid channel count : % d \ n ", avctx -> channels ); <nl> return AVERROR ( EINVAL ); <nl> } <nl> 
static int encode_apng ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int enc_row_size ; <nl> size_t max_packet_size ; <nl> - APNGFctlChunk fctl_chunk ; <nl> + APNGFctlChunk fctl_chunk = { 0 }; <nl>  <nl> if ( pict && avctx -> codec_id == AV_CODEC_ID_APNG && s -> color_type == PNG_COLOR_TYPE_PALETTE ) { <nl> uint32_t checksum = ~ av_crc ( av_crc_get_table ( AV_CRC_32_IEEE_LE ), ~ 0U , pict -> data [ 1 ], 256 * sizeof ( uint32_t ));
static int iff_read_packet ( AVFormatContext * s , <nl> buf = pkt -> data ; <nl> bytestream_put_be16 (& buf , 2 ); <nl> ret = avio_read ( pb , buf , iff -> body_size ); <nl> + if ( ret >= 0 && ret < iff -> body_size ) <nl> + av_shrink_packet ( pkt , ret + 2 ); <nl> } else { <nl> av_assert0 ( 0 ); <nl> }
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl> escape = av_mod_uintp2 ( 16383 , pfx ); <nl> cnt1 = get_unary ( b , 0 , 8 ); <nl> if ( cnt1 < 8 ) { <nl> + if ( pfx < 1 || pfx > 25 ) <nl> + return AVERROR_INVALIDDATA ; <nl> value = show_bits ( b , pfx ); <nl> if ( value > 1 ) { <nl> skip_bits ( b , pfx );
void ff_ivi_output_plane ( IVIPlaneDesc * plane , uint8_t * dst , int dst_pitch ) <nl> const int16_t * src = plane -> bands [ 0 ]. buf ; <nl> uint32_t pitch = plane -> bands [ 0 ]. pitch ; <nl>  <nl> + if (! src ) <nl> + return ; <nl> + <nl> for ( y = 0 ; y < plane -> height ; y ++) { <nl> for ( x = 0 ; x < plane -> width ; x ++) <nl> dst [ x ] = av_clip_uint8 ( src [ x ] + 128 );
av_cold void ff_ac3dsp_init_x86 ( AC3DSPContext * c , int bit_exact ) <nl> c -> ac3_rshift_int32 = ff_ac3_rshift_int32_mmx ; <nl> } <nl> if ( EXTERNAL_AMD3DNOW ( mm_flags )) { <nl> - c -> extract_exponents = ff_ac3_extract_exponents_3dnow ; <nl> if (! bit_exact ) { <nl> c -> float_to_fixed24 = ff_float_to_fixed24_3dnow ; <nl> }
mp_image_t * vf_get_image ( vf_instance_t * vf , unsigned int outfmt , int mp_imgtype , <nl> } <nl>  <nl> mpi -> qscale = NULL ; <nl> - } <nl> mpi -> usage_count ++; <nl> + } <nl> // printf ("\ rVF_MPI : % p % p % p % d % d % d \ n ", <nl> // mpi -> planes [ 0 ], mpi -> planes [ 1 ], mpi -> planes [ 2 ], <nl> // mpi -> stride [ 0 ], mpi -> stride [ 1 ], mpi -> stride [ 2 ]);
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> tracks [ i ]. stream , st -> index_entries [ index ]. timestamp , <nl> AVSEEK_FLAG_BACKWARD ); <nl> while ( index_sub >= 0 && <nl> - index_min >= 0 && <nl> + index_min > 0 && <nl> tracks [ i ]. stream -> index_entries [ index_sub ]. pos < st -> index_entries [ index_min ]. pos && <nl> st -> index_entries [ index ]. timestamp - tracks [ i ]. stream -> index_entries [ index_sub ]. timestamp < 30000000000 / matroska -> time_scale ) <nl> index_min --;
int attribute_align_arg avcodec_encode_audio2 ( AVCodecContext * avctx , <nl> implement encode2 () */ <nl> buf_size = 2 * avctx -> frame_size * avctx -> channels * <nl> av_get_bytes_per_sample ( avctx -> sample_fmt ); <nl> - buf_size += FF_MIN_BUFFER_SIZE ; <nl> + buf_size += 2 * FF_MIN_BUFFER_SIZE ; <nl> } <nl> } <nl> if (( ret = ff_alloc_packet ( avpkt , buf_size )))
fail : <nl> if ( pkt -> stream_index == seg -> reference_stream_index ) <nl> seg -> frame_count ++; <nl>  <nl> - if ( ret < 0 ) { <nl> - if ( seg -> list ) <nl> - avio_close ( seg -> list_pb ); <nl> - avformat_free_context ( oc ); <nl> - } <nl> - <nl> return ret ; <nl> } <nl> 
static int hevc_frame_start ( HEVCContext * s ) <nl>  <nl> fail : <nl> if ( s -> ref ) <nl> - ff_thread_report_progress (& s -> ref -> tf , INT_MAX , 0 ); <nl> + ff_hevc_unref_frame ( s , s -> ref , ~ 0 ); <nl> s -> ref = NULL ; <nl> return ret ; <nl> }
static void put_ebml_num ( ByteIOContext * pb , uint64_t num , int bytes ) <nl> static void put_ebml_uint ( ByteIOContext * pb , unsigned int elementid , uint64_t val ) <nl> { <nl> int i , bytes = 1 ; <nl> - while ( val >> bytes * 8 && bytes < 8 ) bytes ++; <nl> + while ( bytes < 8 && val >> bytes * 8 ) bytes ++; <nl>  <nl> put_ebml_id ( pb , elementid ); <nl> put_ebml_num ( pb , bytes , 0 );
AV_WN32A ( v0 + i , r ); \ <nl> AV_WN32A ( v1 + i , r ); \ <nl> } \ <nl> - for ( i = 0 ; i < BUF_SIZE * 8 / 3 ; i += 4 ) { \ <nl> + for ( i = 0 ; i < width * 8 / 3 ; i += 4 ) { \ <nl> uint32_t r = rnd (); \ <nl> AV_WN32A ( dst0 + i , r ); \ <nl> AV_WN32A ( dst1 + i , r ); \
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> case XWD_GRAY_SCALE : <nl> if ( bpp != 1 && bpp != 8 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( pixdepth == 1 ) { <nl> + if ( bpp == 1 && pixdepth == 1 ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_MONOWHITE ; <nl> - } else if ( pixdepth == 8 ) { <nl> + } else if ( bpp == 8 && pixdepth == 8 ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_GRAY8 ; <nl> } <nl> break ;
typedef struct WmallDecodeCtx { <nl>  <nl> int8_t mclms_order ; <nl> int8_t mclms_scaling ; <nl> - int16_t mclms_coeffs [ 128 ]; <nl> - int16_t mclms_coeffs_cur [ 4 ]; <nl> + int16_t mclms_coeffs [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS * 32 ]; <nl> + int16_t mclms_coeffs_cur [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS ]; <nl> int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ;
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl>  <nl> if ( hdr -> substreamid == info -> num_ind_sub + 1 ) { <nl> // info -> num_ind_sub ++; <nl> - avpriv_request_sample ( track -> par , " Multiple independent substreams "); <nl> + avpriv_request_sample ( mov -> fc , " Multiple independent substreams "); <nl> return AVERROR_PATCHWELCOME ; <nl> } else if ( hdr -> substreamid < info -> num_ind_sub || <nl> hdr -> substreamid == 0 && info -> substream [ 0 ]. bsid ) {
static inline int convert_frame ( AVAudioResampleContext * avr , <nl>  <nl> static inline int available_samples ( AVFrame * out ) <nl> { <nl> + int samples ; <nl> int bytes_per_sample = av_get_bytes_per_sample ( out -> format ); <nl> - int samples = out -> linesize [ 0 ] / bytes_per_sample ; <nl> + if (! bytes_per_sample ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> + samples = out -> linesize [ 0 ] / bytes_per_sample ; <nl> if ( av_sample_fmt_is_planar ( out -> format )) { <nl> return samples ; <nl> } else {
static int decode_motion ( GetBitContext * gb ) <nl> static int decode_mb ( MadContext * s , AVFrame * frame , int inter ) <nl> { <nl> int mv_map = 0 ; <nl> - int mv_x , mv_y ; <nl> + int av_uninit ( mv_x ), av_uninit ( mv_y ); <nl> int j ; <nl>  <nl> if ( inter ) {
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> S *= 1U << s -> float_shift ; <nl> sign = S < 0 ; <nl> if ( sign ) <nl> - S = - S ; <nl> - if ( S >= 0x1000000 ) { <nl> + S = -( unsigned ) S ; <nl> + if ( S >= 0x1000000U ) { <nl> if ( s -> got_extra_bits && get_bits1 (& s -> gb_extra_bits )) <nl> S = get_bits (& s -> gb_extra_bits , 23 ); <nl> else
static int wma_decode_superframe ( AVCodecContext * avctx , <nl> return 0 ; <nl> } <nl> if ( buf_size < s -> block_align ) <nl> - return 0 ; <nl> + return AVERROR ( EINVAL ); <nl> buf_size = s -> block_align ; <nl>  <nl> samples = data ;
int ff_ivi_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> avctx -> release_buffer ( avctx , & ctx -> frame ); <nl>  <nl> ctx -> frame . reference = 0 ; <nl> + avcodec_set_dimensions ( avctx , ctx -> planes [ 0 ]. width , ctx -> planes [ 0 ]. height ); <nl> if (( result = avctx -> get_buffer ( avctx , & ctx -> frame )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return result ;
 <nl> typedef struct Hnm4VideoContext { <nl> uint8_t version ; <nl> - uint16_t width ; <nl> - uint16_t height ; <nl> + int width ; <nl> + int height ; <nl> uint8_t * current ; <nl> uint8_t * previous ; <nl> uint8_t * buffer1 ;
static int aiff_read_packet ( AVFormatContext * s , <nl> size = st -> codecpar -> block_align ; <nl> break ; <nl> default : <nl> - size = ( MAX_SIZE / st -> codecpar -> block_align ) * st -> codecpar -> block_align ; <nl> + size = st -> codecpar -> block_align ? ( MAX_SIZE / st -> codecpar -> block_align ) * st -> codecpar -> block_align : MAX_SIZE ; <nl> } <nl> size = FFMIN ( max_size , size ); <nl> res = av_get_packet ( s -> pb , pkt , size );
static int ebml_parse_elem ( MatroskaDemuxContext * matroska , <nl> data = ( char *) data + syntax -> data_offset ; <nl> if ( syntax -> list_elem_size ) { <nl> EbmlList * list = data ; <nl> - newelem = av_realloc ( list -> elem , ( list -> nb_elem + 1 )* syntax -> list_elem_size ); <nl> + newelem = av_realloc_array ( list -> elem , list -> nb_elem + 1 , syntax -> list_elem_size ); <nl> if (! newelem ) <nl> return AVERROR ( ENOMEM ); <nl> list -> elem = newelem ;
static av_cold int init ( AVFilterContext * ctx , const char * args , void * opaque ) <nl> static av_cold void uninit ( AVFilterContext * ctx ) <nl> { <nl> BufferSourceContext * s = ctx -> priv ; <nl> - while ( av_fifo_size ( s -> fifo )) { <nl> + while ( s -> fifo && av_fifo_size ( s -> fifo )) { <nl> AVFilterBufferRef * buf ; <nl> av_fifo_generic_read ( s -> fifo , & buf , sizeof ( buf ), NULL ); <nl> avfilter_unref_buffer ( buf );
static void compute_scale_factors ( unsigned char scale_code [ SBLIMIT ], <nl> vmax = v ; <nl> } <nl> /* compute the scale factor index using log 2 computations */ <nl> - if ( vmax > 0 ) { <nl> + if ( vmax > 1 ) { <nl> n = av_log2 ( vmax ); <nl> /* n is the position of the MSB of vmax . now <nl> use at most 2 compares to find the index */
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> pkt . size = data_size ; <nl>  <nl> ret = ff_vp8_decode_frame ( avctx , p , got_frame , & pkt ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( s -> has_alpha ) { <nl> ret = vp8_lossy_decode_alpha ( avctx , p , s -> alpha_data , <nl> s -> alpha_data_size );
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> avio_skip ( pb , 16 ); <nl>  <nl> for ( type = 0 ; type != - 1 && avio_tell ( pb ) < next ; ) { <nl> + if ( url_feof ( pb )) <nl> + return AVERROR ( EOF ); <nl> type = avio_rb16 ( pb ); <nl> len = avio_rb16 ( pb ); <nl> av_log ( c -> fc , AV_LOG_DEBUG , " type % d , len % d \ n ", type , len );
static int svq1_encode_frame ( AVCodecContext * avctx , unsigned char * buf , <nl> init_put_bits (& s -> pb , buf , buf_size ); <nl>  <nl> * p = * pict ; <nl> - p -> pict_type = avctx -> frame_number % avctx -> gop_size ? P_TYPE : I_TYPE ; <nl> + p -> pict_type = avctx -> gop_size && avctx -> frame_number % avctx -> gop_size ? P_TYPE : I_TYPE ; <nl> p -> key_frame = p -> pict_type == I_TYPE ; <nl>  <nl> svq1_write_header ( s , p -> pict_type );
static int mxf_get_sorted_table_segments ( MXFContext * mxf , int * nb_sorted_segment <nl> if ( mxf -> metadata_sets [ i ]-> type == IndexTableSegment ) <nl> nb_segments ++; <nl>  <nl> + if (! nb_segments ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> * sorted_segments = av_mallocz ( nb_segments * sizeof (** sorted_segments )); <nl> unsorted_segments = av_mallocz ( nb_segments * sizeof (* unsorted_segments )); <nl> if (! sorted_segments || ! unsorted_segments ) {
avs_decode_frame ( AVCodecContext * avctx , <nl> int i , j , x , y , stride , vect_w = 3 , vect_h = 3 ; <nl> AvsVideoSubType sub_type ; <nl> AvsBlockType type ; <nl> - GetBitContext change_map ; <nl> + GetBitContext change_map = { 0 }; // init to silence warning <nl>  <nl> if ( avctx -> reget_buffer ( avctx , p )) { <nl> av_log ( avctx , AV_LOG_ERROR , " reget_buffer () failed \ n ");
static void ebml_free ( EbmlSyntax * syntax , void * data ) <nl> j ++, ptr += syntax [ i ]. list_elem_size ) <nl> ebml_free ( syntax [ i ]. def . n , ptr ); <nl> av_freep (& list -> elem ); <nl> + list -> nb_elem = 0 ; <nl> } else <nl> ebml_free ( syntax [ i ]. def . n , data_off ); <nl> default :
static void park_frame_worker_threads ( FrameThreadContext * fctx , int thread_count <nl> pthread_cond_wait (& p -> output_cond , & p -> progress_mutex ); <nl> pthread_mutex_unlock (& p -> progress_mutex ); <nl> } <nl> + p -> got_frame = 0 ; <nl> } <nl> } <nl> 
void av_dump_format ( AVFormatContext * ic , int index , <nl> av_log ( NULL , AV_LOG_INFO , " Duration : "); <nl> if ( ic -> duration != AV_NOPTS_VALUE ) { <nl> int hours , mins , secs , us ; <nl> - int64_t duration = ic -> duration + 5000 ; <nl> + int64_t duration = ic -> duration + ( ic -> duration <= INT64_MAX - 5000 ? 5000 : 0 ); <nl> secs = duration / AV_TIME_BASE ; <nl> us = duration % AV_TIME_BASE ; <nl> mins = secs / 60 ;
static int allocate_buffers ( ShortenContext * s ) <nl>  <nl> static inline unsigned int get_uint ( ShortenContext * s , int k ) <nl> { <nl> - if ( s -> version != 0 ) <nl> + if ( s -> version != 0 ) { <nl> k = get_ur_golomb_shorten (& s -> gb , ULONGSIZE ); <nl> + if ( k > 31U ) <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> return get_ur_golomb_shorten (& s -> gb , k ); <nl> } <nl> 
static inline int get_ue_golomb ( GetBitContext * gb ) <nl> int log = 2 * av_log2 ( buf ) - 31 ; <nl> LAST_SKIP_BITS ( re , gb , 32 - log ); <nl> CLOSE_READER ( re , gb ); <nl> - if ( CONFIG_FTRAPV && log < 0 ) { <nl> + if ( log < 7 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid UE golomb code \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int idcin_read_seek ( AVFormatContext * s , int stream_index , <nl> IdcinDemuxContext * idcin = s -> priv_data ; <nl>  <nl> if ( idcin -> first_pkt_pos > 0 ) { <nl> - int ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> + int64_t ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> ff_update_cur_dts ( s , s -> streams [ idcin -> video_stream_index ], 0 );
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> int lower_transport , const char * real_challenge ) <nl> { <nl> RTSPState * rt = s -> priv_data ; <nl> - int rtx , j , i , err , interleave = 0 ; <nl> + int rtx = 0 , j , i , err , interleave = 0 ; <nl> RTSPStream * rtsp_st ; <nl> RTSPMessageHeader reply1 , * reply = & reply1 ; <nl> char cmd [ 2048 ];
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> + modeex . Size = sizeof ( D3DDISPLAYMODEEX ); <nl> hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> if ( FAILED ( hr )) { <nl> IDirect3D9Ex_Release ( d3d9ex );
int ff_hevc_decode_nal_sps ( HEVCContext * s ) <nl> if ( s -> sps_list [ sps_id ] && s -> sps == ( HEVCSPS *) s -> sps_list [ sps_id ]-> data ) { <nl> av_buffer_unref (& s -> current_sps ); <nl> s -> current_sps = av_buffer_ref ( s -> sps_list [ sps_id ]); <nl> + if (! s -> current_sps ) <nl> + s -> sps = NULL ; <nl> } <nl> av_buffer_unref (& s -> sps_list [ sps_id ]); <nl> s -> sps_list [ sps_id ] = sps_buf ;
static int mov_write_vmhd_tag ( ByteIOContext * pb ) <nl>  <nl> static int mov_write_hdlr_tag ( ByteIOContext * pb , MOVTrack * track ) <nl> { <nl> - const char * descr , * hdlr , * hdlr_type ; <nl> + const char * hdlr , * descr = NULL , * hdlr_type = NULL ; <nl> int64_t pos = url_ftell ( pb ); <nl>  <nl> if (! track ) { /* no media --> data handler */
static inline void render_line_unrolled ( intptr_t x , unsigned char y , int x1 , <nl> } <nl> } <nl>  <nl> - static void render_line ( int x0 , int y0 , int x1 , int y1 , float * buf ) <nl> + static void render_line ( int x0 , unsigned char y0 , int x1 , int y1 , float * buf ) <nl> { <nl> int dy = y1 - y0 ; <nl> int adx = x1 - x0 ;
static int read_code_table ( CLLCContext * ctx , GetBitContext * gb , VLC * vlc ) <nl>  <nl> count ++; <nl> } <nl> + if ( prefix > ( 65535 - 256 )/ 2 ) { <nl> + vlc -> table = NULL ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> prefix <<= 1 ; <nl> }
static int scale_vector ( int16_t * dst , const int16_t * vector , int length ) <nl> for ( i = 0 ; i < length ; i ++) <nl> max |= FFABS ( vector [ i ]); <nl>  <nl> - max = FFMIN ( max , 0x7FFF ); <nl> bits = normalize_bits ( max , 15 ); <nl>  <nl> if ( bits == 15 )
int ff_audio_interleave_init ( AVFormatContext * s , <nl> aic -> time_base = time_base ; <nl>  <nl> aic -> fifo_size = 100 * * aic -> samples ; <nl> - aic -> fifo = av_fifo_alloc_array ( 100 , * aic -> samples ); <nl> + if (!( aic -> fifo = av_fifo_alloc_array ( 100 , * aic -> samples ))) <nl> + return AVERROR ( ENOMEM ); <nl> } <nl> } <nl> 
static int msrle_decode_frame ( AVCodecContext * avctx , <nl> uint8_t * buf = avpkt -> data + ( avctx -> height - 1 )* istride ; <nl> int i , j ; <nl>  <nl> + if ( linesize < 0 ) <nl> + return linesize ; <nl> + <nl> for ( i = 0 ; i < avctx -> height ; i ++) { <nl> if ( avctx -> bits_per_coded_sample == 4 ) { <nl> for ( j = 0 ; j < avctx -> width - 1 ; j += 2 ) {
static int mov_read_close ( AVFormatContext * s ) <nl> av_freep (& mov -> fragment_index_data ); <nl>  <nl> av_freep (& mov -> aes_decrypt ); <nl> + av_freep (& mov -> chapter_tracks ); <nl>  <nl> return 0 ; <nl> }
static int read_header ( AVFormatContext * s ) <nl> uint8_t root [ WTV_SECTOR_SIZE ]; <nl> AVIOContext * pb ; <nl> int64_t timeline_pos ; <nl> - int ret ; <nl> + int64_t ret ; <nl>  <nl> wtv -> epoch = <nl> wtv -> pts =
static int fourxm_read_packet ( AVFormatContext * s , <nl>  <nl> if ( ret < 0 ) { <nl> av_free_packet ( pkt ); <nl> - } else <nl> + } else { <nl> packet_read = 1 ; <nl> + av_shrink_packet ( pkt , ret + 8 ); <nl> + } <nl> break ; <nl>  <nl> case snd__TAG :
AVStream * avformat_new_stream ( AVFormatContext * s , const AVCodec * c ) <nl> st -> info -> last_dts = AV_NOPTS_VALUE ; <nl>  <nl> st -> codec = avcodec_alloc_context3 ( c ); <nl> + if (! st -> codec ) { <nl> + av_free ( st -> info ); <nl> + av_free ( st ); <nl> + return NULL ; <nl> + } <nl> if ( s -> iformat ) { <nl> /* no default bitrate if decoding */ <nl> st -> codec -> bit_rate = 0 ;
int avformat_seek_file ( AVFormatContext * s , int stream_index , int64_t min_ts , int <nl> { <nl> if ( min_ts > ts || max_ts < ts ) <nl> return - 1 ; <nl> + if ( stream_index < - 1 || stream_index >= ( int ) s -> nb_streams ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> if ( s -> seek2any > 0 ) <nl> flags |= AVSEEK_FLAG_ANY ;
sigterm_handler ( int sig ) <nl> received_nb_signals ++; <nl> term_exit_sigsafe (); <nl> if ( received_nb_signals > 3 ) <nl> - exit_program ( 123 ); <nl> + exit ( 123 ); <nl> } <nl>  <nl> void term_init ( void )
int avpriv_mpeg4audio_get_config ( MPEG4AudioConfig * c , const uint8_t * buf , <nl> GetBitContext gb ; <nl> int specific_config_bitindex ; <nl>  <nl> - if ( bit_size <= 0 ) <nl> + if ( bit_size <= 0 || init_get_bits (& gb , buf , bit_size ) < 0 ) <nl> return AVERROR_INVALIDDATA ; <nl> - <nl> - init_get_bits (& gb , buf , bit_size ); <nl> c -> object_type = get_object_type (& gb ); <nl> c -> sample_rate = get_sample_rate (& gb , & c -> sampling_index ); <nl> c -> chan_config = get_bits (& gb , 4 );
static int mp_decode_frame ( AVCodecContext * avctx , <nl> if ( sz == 0 ) <nl> goto end ; <nl>  <nl> + if ( mp -> max_codes_bits <= 0 ) <nl> + goto end ; <nl> if ( init_vlc (& mp -> vlc , mp -> max_codes_bits , mp -> codes_count , & mp -> codes [ 0 ]. size , sizeof ( HuffCode ), 1 , & mp -> codes [ 0 ]. code , sizeof ( HuffCode ), 4 , 0 )) <nl> goto end ; <nl> mp_decode_frame_helper ( mp , & gb );
av_cold int ff_vp8_decode_free ( AVCodecContext * avctx ) <nl> VP8Context * s = avctx -> priv_data ; <nl> int i ; <nl>  <nl> + if (! s ) <nl> + return 0 ; <nl> + <nl> vp8_decode_flush_impl ( avctx , 1 ); <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( s -> frames ); i ++) <nl> av_frame_free (& s -> frames [ i ]. tf . f );
static int decode_frame ( AVCodecContext * avctx , <nl> int size , offset , start = 0 ; <nl>  <nl> offset = bytestream2_get_le16 ( gb ); <nl> - if ( offset > s -> nb_blocks ) <nl> + if ( offset >= s -> nb_blocks ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> size = bytestream2_get_le16 ( gb );
int main ( void ){ <nl> AVPixFmtDescriptor * desc = av_pix_fmt_desc_get ( i ); <nl> if (! desc ) <nl> continue ; <nl> - av_log ( 0 , AV_LOG_INFO , " pix fmt % s % d \ n ", desc -> name , is_yuv_planar ( i )); <nl> + av_log ( 0 , AV_LOG_INFO , " pix fmt % s yuv_plan :% d avg_bpp :% d \ n ", desc -> name , is_yuv_planar ( i ), avg_bits_per_pixel ( i )); <nl> } <nl> return 0 ; <nl> }
static int vorbis_decode_frame ( AVCodecContext * avctx , void * data , <nl> if (! vc -> first_frame ) { <nl> vc -> first_frame = 1 ; <nl> * got_frame_ptr = 0 ; <nl> + av_frame_unref ( frame ); <nl> return buf_size ; <nl> } <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> wipe_side_data ( dst ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + if ( sd_src -> buf ) { <nl> sd_dst -> buf = av_buffer_ref ( sd_src -> buf ); <nl> if (! sd_dst -> buf ) { <nl> wipe_side_data ( dst ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl> sd_dst -> data = sd_dst -> buf -> data ; <nl> sd_dst -> size = sd_dst -> buf -> size ; <nl> + } <nl> } <nl> av_dict_copy (& sd_dst -> metadata , sd_src -> metadata , 0 ); <nl> }
void ff_aac_update_ltp ( AACEncContext * s , SingleChannelElement * sce ) <nl> lag = i ; <nl> } <nl> } <nl> - lag = av_clip ( lag , 0 , 2048 ); /* 11 bits => 2 ^ 11 = 2048 */ <nl> + lag = av_clip ( lag , 0 , 2047 ); /* 11 bits => 2 ^ 11 = 0 -> 2047 */ <nl>  <nl> if (! lag ) { <nl> sce -> ics . ltp . lag = lag ;
static int pmp_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( pmp -> cur_stream == 0 ) { <nl> int num_packets ; <nl> pmp -> audio_packets = avio_r8 ( pb ); <nl> + if (! pmp -> audio_packets ) { <nl> + av_log_ask_for_sample ( s , " 0 audio packets \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> num_packets = ( pmp -> num_streams - 1 ) * pmp -> audio_packets + 1 ; <nl> avio_skip ( pb , 8 ); <nl> pmp -> current_packet = 0 ;
static int vc1_decode_frame ( AVCodecContext * avctx , <nl> divider = find_next_marker ( buf , buf + buf_size ); <nl> if (( divider == ( buf + buf_size )) || AV_RB32 ( divider ) != VC1_CODE_FIELD ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Error in WVC1 interlaced frame \ n "); <nl> + av_free ( buf2 ); <nl> return - 1 ; <nl> } <nl> 
int attribute_align_arg avcodec_encode_audio ( AVCodecContext * avctx , <nl> const short * samples ) <nl> { <nl> AVPacket pkt ; <nl> - AVFrame frame0 ; <nl> + AVFrame frame0 = { 0 }; <nl> AVFrame * frame ; <nl> int ret , samples_size , got_packet ; <nl> 
static int configure_output_audio_filter ( FilterGraph * fg , OutputFilter * ofilter , <nl> pad_idx = 0 ; \ <nl> } while ( 0 ) <nl>  <nl> - if ( audio_sync_method > 0 ) { <nl> + if ( audio_sync_method > 0 && 0 ) { <nl> char args [ 256 ] = { 0 }; <nl>  <nl> av_strlcatf ( args , sizeof ( args ), " min_comp = 0 . 001 : min_hard_comp =% f ", audio_drift_threshold );
static int get_qcd ( Jpeg2000DecoderContext * s , int n , Jpeg2000QuantStyle * q , <nl> Jpeg2000QuantStyle tmp ; <nl> int compno , ret ; <nl>  <nl> + memset (& tmp , 0 , sizeof ( tmp )); <nl> + <nl> if (( ret = get_qcx ( s , n , & tmp )) < 0 ) <nl> return ret ; <nl> for ( compno = 0 ; compno < s -> ncomponents ; compno ++)
uint64_t time = rdtsc (); <nl> s -> workaround_bugs = avctx -> workaround_bugs ; <nl> s -> flags = avctx -> flags ; <nl>  <nl> - /* no supplementary picture */ <nl> + * data_size = 0 ; <nl> + <nl> + /* no supplementary picture */ <nl> if ( buf_size == 0 ) { <nl> - * data_size = 0 ; <nl> return 0 ; <nl> } <nl> 
static int config_props ( AVFilterLink * inlink ) <nl> s -> hsub = pixdesc -> log2_chroma_w ; <nl> s -> vsub = pixdesc -> log2_chroma_h ; <nl>  <nl> - s -> bpp = av_get_bits_per_pixel ( pixdesc ) >> 3 ; <nl> + s -> bpp = pixdesc -> flags & AV_PIX_FMT_FLAG_PLANAR ? <nl> + 1 : <nl> + av_get_bits_per_pixel ( pixdesc ) >> 3 ; <nl> s -> alpha &= !!( pixdesc -> flags & AV_PIX_FMT_FLAG_ALPHA ); <nl> s -> is_packed_rgb = ff_fill_rgba_map ( s -> rgba_map , inlink -> format ) >= 0 ; <nl> 
static int mxf_set_audio_pts ( MXFContext * mxf , AVCodecContext * codec , AVPacket * p <nl> { <nl> MXFTrack * track = mxf -> fc -> streams [ pkt -> stream_index ]-> priv_data ; <nl> pkt -> pts = track -> sample_count ; <nl> + if ( codec -> channels <= 0 || av_get_bits_per_sample ( codec -> codec_id ) <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl> track -> sample_count += pkt -> size / ( codec -> channels * av_get_bits_per_sample ( codec -> codec_id ) / 8 ); <nl> return 0 ; <nl> }
static int parse_header ( OutputStream * os , const uint8_t * buf , int buf_size ) <nl> if ( size > buf_size ) <nl> return AVERROR_INVALIDDATA ; <nl> if ( type == 8 || type == 9 ) { <nl> - if ( os -> nb_extra_packets > FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> + if ( os -> nb_extra_packets >= FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> return AVERROR_INVALIDDATA ; <nl> os -> extra_packet_sizes [ os -> nb_extra_packets ] = size ; <nl> os -> extra_packets [ os -> nb_extra_packets ] = av_malloc ( size );
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * insamples ) <nl> av_rescale_q (( p - ( int16_t *) insamples -> data [ 0 ]) / nb_channels , <nl> ( AVRational ){ 1 , inlink -> sample_rate }, <nl> outlink -> time_base ); <nl> - outlink -> out_buf = outpicref ; <nl> linesize = outpicref -> linesize [ 0 ]; <nl> memset ( outpicref -> data [ 0 ], 0 , showwaves -> h * linesize ); <nl> }
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> int best_fps = 0 ; <nl> double best_error = 0 . 01 ; <nl>  <nl> + if ( delta_dts >= INT64_MAX / st -> time_base . num || <nl> + delta_packets >= INT64_MAX / st -> time_base . den ) <nl> + continue ; <nl> av_reduce (& st -> avg_frame_rate . num , & st -> avg_frame_rate . den , <nl> delta_packets *( int64_t ) st -> time_base . den , <nl> delta_dts *( int64_t ) st -> time_base . num , 60000 );
static int dca_convert_bitstream ( uint8_t * src , int src_size , uint8_t * dst , <nl> uint16_t * ssrc = ( uint16_t *) src , * sdst = ( uint16_t *) dst ; <nl> PutBitContext pb ; <nl>  <nl> + if (( unsigned ) src_size > ( unsigned ) max_size ) <nl> + return - 1 ; <nl> + <nl> mrk = AV_RB32 ( src ); <nl> switch ( mrk ) { <nl> case DCA_MARKER_RAW_BE :
static inline int mpeg1_fast_decode_block_inter ( MpegEncContext * s , int16_t * bloc <nl> } <nl>  <nl> block [ j ] = level ; <nl> - if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF ) <nl> + if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF || i >= 64 ) <nl> break ; <nl> UPDATE_CACHE ( re , & s -> gb ); <nl> }
static AVFilterContext * create_filter ( AVFilterGraph * ctx , int index , <nl> AVFilter * filt ; <nl> char inst_name [ 30 ]; <nl>  <nl> - snprintf ( inst_name , sizeof ( inst_name ), " Parsed filter % d ", index ); <nl> + snprintf ( inst_name , sizeof ( inst_name ), " Filter % d % s ", index , filt_name ); <nl>  <nl> filt = avfilter_get_by_name ( filt_name ); <nl> 
static int msmpeg4v34_decode_mb ( MpegEncContext * s , int16_t block [ 6 ][ 64 ]) <nl> uint8_t * coded_val ; <nl> uint32_t * const mb_type_ptr = & s -> current_picture . mb_type [ s -> mb_x + s -> mb_y * s -> mb_stride ]; <nl>  <nl> + if ( get_bits_left (& s -> gb ) <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( s -> pict_type == AV_PICTURE_TYPE_P ) { <nl> if ( s -> use_skip_mb_code ) { <nl> if ( get_bits1 (& s -> gb )) {
static void fix_bitshift ( ShortenContext * s , int32_t * buffer ) <nl>  <nl> if ( s -> bitshift != 0 ) <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - buffer [ s -> nwrap + i ] <<= s -> bitshift ; <nl> + buffer [ i ] <<= s -> bitshift ; <nl> } <nl>  <nl> 
pp_mode * pp_get_mode_by_name_and_quality ( const char * name , int quality ) <nl> } <nl>  <nl> ppMode = av_malloc ( sizeof ( PPMode )); <nl> + if (! ppMode ) <nl> + return NULL ; <nl>  <nl> ppMode -> lumMode = 0 ; <nl> ppMode -> chromMode = 0 ;
static const struct { <nl>  <nl> static int webvtt_event_to_ass ( AVBPrint * buf , const char * p ) <nl> { <nl> - int i , again , skip = 0 ; <nl> + int i , again = 0 , skip = 0 ; <nl>  <nl> while (* p ) { <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> } else { <nl> frame -> key_frame = 0 ; <nl> frame -> pict_type = AV_PICTURE_TYPE_P ; <nl> + if ( c -> decomp_len < 2LL * (( c -> width + c -> bw - 1 ) / c -> bw ) * (( c -> height + c -> bh - 1 ) / c -> bh )) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( c -> decomp_len ) <nl> c -> decode_xor ( c ); <nl> }
static int parse_timestamp ( struct sbg_parser * p , <nl>  <nl> static int parse_fade ( struct sbg_parser * p , struct sbg_fade * fr ) <nl> { <nl> - struct sbg_fade f ; <nl> + struct sbg_fade f = { 0 }; <nl>  <nl> if ( lex_char ( p , '<')) <nl> f . in = SBG_FADE_SILENCE ;
static int ftp_status ( FTPContext * s , char ** line , const int response_codes []) <nl>  <nl> while (! code_found || dash ) { <nl> if (( err = ftp_get_line ( s , buf , sizeof ( buf ))) < 0 ) { <nl> - av_bprint_finalize (& line_buffer , NULL ); <nl> + if ( line ) <nl> + av_bprint_finalize (& line_buffer , NULL ); <nl> return err ; <nl> } <nl> 
int64_t ff_gen_search ( AVFormatContext * s , int stream_index , int64_t target_ts , <nl> } <nl>  <nl> if ( ts_max == AV_NOPTS_VALUE ){ <nl> - int step = 1024 ; <nl> + int64_t step = 1024 ; <nl> filesize = avio_size ( s -> pb ); <nl> pos_max = filesize - 1 ; <nl> do {
int swri_realloc_audio ( AudioData * a , int count ){ <nl> av_assert0 ( a -> bps ); <nl> av_assert0 ( a -> ch_count ); <nl>  <nl> - a -> data = av_mallocz ( countb * a -> ch_count ); <nl> + a -> data = av_mallocz_array ( countb , a -> ch_count ); <nl> if (! a -> data ) <nl> return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < a -> ch_count ; i ++){
static int tm2_read_stream ( TM2Context * ctx , const uint8_t * buf , int stream_id , i <nl> buf += 4 ; cur += 4 ; <nl> buf += 4 ; cur += 4 ; /* unused by decoder */ <nl>  <nl> + if ( skip < cur ) <nl> + return - 1 ; <nl> init_get_bits (& ctx -> gb , buf , ( skip - cur ) * 8 ); <nl> if ( tm2_build_huff_table ( ctx , & codes ) == - 1 ) <nl> return - 1 ;
static void do_subtitle_out ( AVFormatContext * s , <nl>  <nl> if (! subtitle_out ) { <nl> subtitle_out = av_malloc ( subtitle_out_max_size ); <nl> + if (! subtitle_out ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Failed to allocate subtitle_out \ n "); <nl> + exit_program ( 1 ); <nl> + } <nl> } <nl>  <nl> /* Note : DVB subtitle need one packet to draw them and one other
static int decode_frame ( WmallDecodeCtx * s ) <nl> /* decode tile information */ <nl> if (( ret = decode_tilehdr ( s ))) { <nl> s -> packet_loss = 1 ; <nl> + av_frame_unref ( s -> frame ); <nl> return ret ; <nl> } <nl> 
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + avctx -> height * lsize ) { <nl> + if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + ( uint64_t ) avctx -> height * lsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " input buffer too small \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl>  <nl> + if ( avpkt -> size < 20 + avctx -> width * avctx -> height / 16 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Input packet too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( s -> format != format ) { <nl> if ( ret < 0 ) <nl> return ret ;
static int decodeTonalComponents ( GetBitContext * gb , tonal_component * pComponent <nl>  <nl> for ( k = 0 ; k < coded_components ; k ++) { <nl> sfIndx = get_bits ( gb , 6 ); <nl> + if ( component_count >= 64 ) <nl> + return AVERROR_INVALIDDATA ; <nl> pComponent [ component_count ]. pos = j * 64 + ( get_bits ( gb , 6 )); <nl> max_coded_values = SAMPLES_PER_FRAME - pComponent [ component_count ]. pos ; <nl> coded_values = coded_values_per_component + 1 ;
static void read_apic ( AVFormatContext * s , AVIOContext * pb , int taglen , char * tag <nl> goto fail ; <nl> } <nl>  <nl> - apic -> buf = av_buffer_alloc ( taglen ); <nl> + apic -> buf = av_buffer_alloc ( taglen + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + apic -> buf -> size -= FF_INPUT_BUFFER_PADDING_SIZE ; <nl> if (! apic -> buf || ! taglen || avio_read ( pb , apic -> buf -> data , taglen ) != taglen ) <nl> goto fail ; <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> for (; yq < slice_h && yq < h ; yq ++){ <nl> IDWTELEM * line = slice_buffer_get_line (& s -> sb , yq ); <nl> for ( x = 0 ; x < w ; x ++){ <nl> - line [ x ] <<= FRAC_BITS ; <nl> + line [ x ] *= 1 << FRAC_BITS ; <nl> } <nl> } <nl> }
int ff_rtsp_open_transport_ctx ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl>  <nl> if (! rtsp_st -> transport_priv ) { <nl> return AVERROR ( ENOMEM ); <nl> - } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP ) { <nl> + } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP && <nl> + s -> iformat ) { <nl> RTPDemuxContext * rtpctx = rtsp_st -> transport_priv ; <nl> rtpctx -> ssrc = rtsp_st -> ssrc ; <nl> if ( rtsp_st -> dynamic_handler ) {
const uint8_t * avpriv_mpv_find_start_code ( const uint8_t * restrict p , <nl> av_cold int ff_dct_common_init ( MpegEncContext * s ) <nl> { <nl> ff_dsputil_init (& s -> dsp , s -> avctx ); <nl> - ff_videodsp_init (& s -> vdsp , 8 ); <nl> + ff_videodsp_init (& s -> vdsp , s -> avctx -> bits_per_raw_sample ); <nl>  <nl> s -> dct_unquantize_h263_intra = dct_unquantize_h263_intra_c ; <nl> s -> dct_unquantize_h263_inter = dct_unquantize_h263_inter_c ;
struct ff_timecode { <nl> char * str ; ///< string following the hh : mm : ss [:;.] ff format <nl> int start ; ///< timecode frame start <nl> int drop ; ///< drop flag ( 1 if drop , else 0 ) <nl> - AVRational rate ; ///< Frame rate in rationnal form <nl> + AVRational rate ; ///< Frame rate in rational form <nl> }; <nl>  <nl> /**
static inline struct rgbvec lerp ( const struct rgbvec * v0 , const struct rgbvec * v <nl>  <nl> # define NEAR ( x ) (( int )(( x ) + . 5 )) <nl> # define PREV ( x ) (( int )( x )) <nl> -# define NEXT ( x ) (( int )( x ) + 1 ) <nl> +# define NEXT ( x ) ( FFMIN (( int )( x ) + 1 , lut3d -> lutsize - 1 )) <nl>  <nl> /** <nl> * Get the nearest defined point
static int sdp_probe ( AVProbeData * p1 ) <nl>  <nl> /* we look for a line beginning " c = IN IP " */ <nl> while ( p < p_end && * p != '\ 0 ') { <nl> - if ( p + sizeof (" c = IN IP ") - 1 < p_end && <nl> + if ( sizeof (" c = IN IP ") - 1 < p_end - p && <nl> av_strstart ( p , " c = IN IP ", NULL )) <nl> return AVPROBE_SCORE_EXTENSION ; <nl> 
void ff_set_mpeg4_time ( MpegEncContext * s ) <nl>  <nl> static void mpeg4_encode_gop_header ( MpegEncContext * s ) <nl> { <nl> - int hours , minutes , seconds ; <nl> + int64_t hours , minutes , seconds ; <nl> int64_t time ; <nl>  <nl> put_bits (& s -> pb , 16 , 0 );
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i ); <nl> + if ( end - dst < count ) { <nl> + av_log ( l -> avctx , AV_LOG_ERROR , " Too many zeros remaining .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> memset ( dst , 0 , count ); <nl> l -> zeros_rem -= count ; <nl> dst += count ;
static av_cold int init ( AVFilterContext * ctx , const char * args ) <nl>  <nl> aresample -> next_pts = AV_NOPTS_VALUE ; <nl> aresample -> swr = swr_alloc (); <nl> - if (! aresample -> swr ) <nl> - return AVERROR ( ENOMEM ); <nl> + if (! aresample -> swr ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl>  <nl> if ( args ) { <nl> char * ptr = argd , * token ;
static void revert_cdlms ( WmallDecodeCtx * s , int ch , <nl> s -> channel_residues [ ch ][ icoef ] = input ; <nl> } <nl> } <nl> + emms_c (); <nl> } <nl>  <nl> static void revert_inter_ch_decorr ( WmallDecodeCtx * s , int tile_size )
static int compat_decode ( AVCodecContext * avctx , AVFrame * frame , <nl> int * got_frame , AVPacket * pkt ) <nl> { <nl> AVCodecInternal * avci = avctx -> internal ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> av_assert0 ( avci -> compat_decode_consumed == 0 ); <nl> 
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> if (! s -> context_initialized ) { <nl> if ( ff_msmpeg4_decode_init ( avctx ) < 0 || ff_vc1_decode_init_alloc_tables ( v ) < 0 ) <nl> - return - 1 ; <nl> + goto err ; <nl>  <nl> s -> low_delay = ! avctx -> has_b_frames || v -> res_sprite ; <nl> 
static char * choose_pix_fmts ( OutputStream * ost ) <nl> } <nl> if ( ost -> st -> codec -> pix_fmt != PIX_FMT_NONE ) { <nl> return av_strdup ( av_get_pix_fmt_name ( choose_pixel_fmt ( ost -> st , ost -> enc , ost -> st -> codec -> pix_fmt ))); <nl> - } else if ( ost -> enc -> pix_fmts ) { <nl> + } else if ( ost -> enc && ost -> enc -> pix_fmts ) { <nl> const enum PixelFormat * p ; <nl> AVIOContext * s = NULL ; <nl> uint8_t * ret ;
typedef struct { <nl>  <nl> static const AVOption options [] = { <nl> { " oggpagesize ", " Set preferred Ogg page size .", <nl> - offsetof ( OGGContext , pref_size ), FF_OPT_TYPE_INT , 0 , 0 , MAX_PAGE_SIZE , AV_OPT_FLAG_ENCODING_PARAM }, <nl> + offsetof ( OGGContext , pref_size ), FF_OPT_TYPE_INT , {. dbl = 0 }, 0 , MAX_PAGE_SIZE , AV_OPT_FLAG_ENCODING_PARAM }, <nl> { NULL }, <nl> }; <nl> 
static int h264_mp4toannexb_filter ( AVBitStreamFilterContext * bsfc , <nl> buf += ctx -> length_size ; <nl> unit_type = * buf & 0x1f ; <nl>  <nl> - if ( buf + nal_size > buf_end || nal_size < 0 ) <nl> + if ( nal_size > buf_end - buf || nal_size < 0 ) <nl> goto fail ; <nl>  <nl> if ( unit_type == 7 )
static int find_and_decode_index ( NUTContext * nut ) <nl> has_keyframe [ n ++] = flag ; <nl> has_keyframe [ n ++] = ! flag ; <nl> } else { <nl> + if ( x <= 1 ) { <nl> + av_log ( s , AV_LOG_ERROR , " index : x %" PRIu64 " is invalid \ n ", x ); <nl> + goto fail ; <nl> + } <nl> while ( x != 1 ) { <nl> if ( n >= syncpoint_count + 1 ) { <nl> av_log ( s , AV_LOG_ERROR , " index overflow B \ n ");
void avcodec_align_dimensions2 ( AVCodecContext * s , int * width , int * height , <nl> case AV_PIX_FMT_YUV422P9BE : <nl> case AV_PIX_FMT_YUV422P10LE : <nl> case AV_PIX_FMT_YUV422P10BE : <nl> + case AV_PIX_FMT_YUVA422P10LE : <nl> + case AV_PIX_FMT_YUVA422P10BE : <nl> case AV_PIX_FMT_YUV444P9LE : <nl> case AV_PIX_FMT_YUV444P9BE : <nl> case AV_PIX_FMT_YUV444P10LE : <nl> case AV_PIX_FMT_YUV444P10BE : <nl> + case AV_PIX_FMT_YUVA444P10LE : <nl> + case AV_PIX_FMT_YUVA444P10BE : <nl> case AV_PIX_FMT_GBRP9LE : <nl> case AV_PIX_FMT_GBRP9BE : <nl> case AV_PIX_FMT_GBRP10LE :
static int mpc7_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> bits = av_malloc ((( buf_size - 1 ) & ~ 3 ) + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! bits ) <nl> + return AVERROR ( ENOMEM ); <nl> c -> dsp . bswap_buf (( uint32_t *) bits , ( const uint32_t *)( buf + 4 ), ( buf_size - 4 ) >> 2 ); <nl> init_get_bits (& gb , bits , ( buf_size - 4 )* 8 ); <nl> skip_bits_long (& gb , buf [ 0 ]);
av_cold void ff_msmpeg4_encode_init ( MpegEncContext * s ) <nl>  <nl> for ( i = 0 ; i < NB_RL_TABLES ; i ++){ <nl> int level ; <nl> - for ( level = 0 ; level <= MAX_LEVEL ; level ++){ <nl> + for ( level = 1 ; level <= MAX_LEVEL ; level ++) { <nl> int run ; <nl> for ( run = 0 ; run <= MAX_RUN ; run ++){ <nl> int last ;
matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , int size , <nl> pkt = av_mallocz ( sizeof ( AVPacket )); <nl> /* XXX : prevent data copy ... */ <nl> if ( av_new_packet ( pkt , pkt_size + offset ) < 0 ) { <nl> + av_free ( pkt ); <nl> res = AVERROR ( ENOMEM ); <nl> n = laces - 1 ; <nl> break ;
static int Stagefright_decode_frame ( AVCodecContext * avctx , void * data , <nl> AVFrame * ret_frame ; <nl>  <nl> if (! s -> thread_started ) { <nl> - pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx ); <nl> + if ( pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx )) <nl> + return AVERROR ( ENOMEM ); <nl> s -> thread_started = true ; <nl> } <nl> 
static av_cold int vc1_decode_init ( AVCodecContext * avctx ) <nl> avctx -> idct_algo = FF_IDCT_WMV2 ; <nl> } <nl>  <nl> - if ( ff_h263_decode_init ( avctx ) < 0 ) <nl> + if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) <nl> return - 1 ; <nl> if ( vc1_init_common ( v ) < 0 ) return - 1 ; <nl> - // only for ff_msmp4_mb_i_table <nl> - if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) return - 1 ; <nl>  <nl> avctx -> coded_width = avctx -> width ; <nl> avctx -> coded_height = avctx -> height ;
static int msf_read_header ( AVFormatContext * s ) <nl> st -> codec -> codec_type = AVMEDIA_TYPE_AUDIO ; <nl> codec = avio_rb32 ( s -> pb ); <nl> st -> codec -> channels = avio_rb32 ( s -> pb ); <nl> - if ( st -> codec -> channels <= 0 ) <nl> + if ( st -> codec -> channels <= 0 || st -> codec -> channels >= INT_MAX / 1024 ) <nl> return AVERROR_INVALIDDATA ; <nl> size = avio_rb32 ( s -> pb ); <nl> st -> codec -> sample_rate = avio_rb32 ( s -> pb );
static int http_prepare_data ( HTTPContext * c ) <nl>  <nl> av_freep (& c -> pb_buffer ); <nl> len = avio_close_dyn_buf ( ctx -> pb , & c -> pb_buffer ); <nl> + ctx -> pb = NULL ; <nl> c -> cur_frame_bytes = len ; <nl> c -> buffer_ptr = c -> pb_buffer ; <nl> c -> buffer_end = c -> pb_buffer + len ;
static int poll_filters ( void ) <nl> } <nl> break ; <nl> } <nl> + frame_pts = AV_NOPTS_VALUE ; <nl> if ( ost -> enc -> type == AVMEDIA_TYPE_VIDEO ) <nl> filtered_frame -> pts = frame_pts = av_rescale_q ( picref -> pts , ist_pts_tb , AV_TIME_BASE_Q ); <nl> else if ( picref -> pts != AV_NOPTS_VALUE )
int ff_dca_lbr_parse ( DCALbrDecoder * s , uint8_t * data , DCAExssAsset * asset ) <nl> LBRChunk hr_grid [ DCA_LBR_CHANNELS / 2 ]; <nl> LBRChunk ts1 [ DCA_LBR_CHANNELS / 2 ]; <nl> LBRChunk ts2 [ DCA_LBR_CHANNELS / 2 ]; <nl> - } chunk = { 0 }; <nl> + } chunk = { { 0 } }; <nl>  <nl> GetByteContext gb ; <nl> 
static int config_props ( AVFilterLink * outlink ) <nl>  <nl> scale -> input_is_pal = av_pix_fmt_descriptors [ inlink -> format ]. flags & PIX_FMT_PAL ; <nl>  <nl> + if ( scale -> sws ) <nl> + sws_freeContext ( scale -> sws ); <nl> scale -> sws = sws_getContext ( inlink -> w , inlink -> h , inlink -> format , <nl> outlink -> w , outlink -> h , outlink -> format , <nl> scale -> flags , NULL , NULL , NULL );
typedef struct ScalingList { <nl> } ScalingList ; <nl>  <nl> typedef struct HEVCSPS { <nl> - int vps_id ; <nl> + unsigned vps_id ; <nl> int chroma_format_idc ; <nl> uint8_t separate_colour_plane_flag ; <nl>  <nl> typedef struct HEVCSPS { <nl> } HEVCSPS ; <nl>  <nl> typedef struct HEVCPPS { <nl> - int sps_id ; ///< seq_parameter_set_id <nl> + unsigned sps_id ; ///< seq_parameter_set_id <nl>  <nl> uint8_t sign_data_hiding_flag ; <nl> 
static int mpeg_mux_init ( AVFormatContext * ctx ) <nl> if (! s -> mux_rate ) { <nl> /* we increase slightly the bitrate to take into account the <nl> headers . XXX : compute it exactly */ <nl> - bitrate += bitrate * 5 / 100 ; <nl> + bitrate += bitrate / 20 ; <nl> bitrate += 10000 ; <nl> s -> mux_rate = ( bitrate + ( 8 * 50 ) - 1 ) / ( 8 * 50 ); <nl> }
static int rtsp_read_packet ( AVFormatContext * s , <nl> case RTSP_PROTOCOL_RTP_UDP : <nl> case RTSP_PROTOCOL_RTP_UDP_MULTICAST : <nl> len = udp_read_packet ( s , & rtsp_st , buf , sizeof ( buf )); <nl> - if ( rtsp_st -> rtp_ctx ) <nl> + if ( len >= 0 && rtsp_st -> rtp_ctx ) <nl> rtp_check_and_send_back_rr ( rtsp_st -> rtp_ctx , len ); <nl> break ; <nl> }
int32_t ff_mlp_pack_output ( int32_t lossless_check_data , <nl> ( 1U << output_shift [ mat_ch ]); <nl> lossless_check_data ^= ( sample & 0xffffff ) << mat_ch ; <nl> if ( is32 ) <nl> - * data_32 ++ = sample * 256 ; <nl> + * data_32 ++ = sample * 256U ; <nl> else <nl> * data_16 ++ = sample >> 8 ; <nl> }
resync : <nl> } <nl> ast -> frame_offset += get_duration ( ast , pkt -> size ); <nl> } <nl> - ast -> remaining -= size ; <nl> + ast -> remaining -= err ; <nl> if (! ast -> remaining ){ <nl> avi -> stream_index = - 1 ; <nl> ast -> packet_size = 0 ;
AVCodec mjpeg_decoder = { <nl> ff_mjpeg_decode_frame , <nl> CODEC_CAP_DR1 , <nl> NULL , <nl> - . max_lowres = 8 , <nl> + . max_lowres = 4 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" MJPEG ( Motion JPEG )"), <nl> }; <nl> 
SwsFunc ff_yuv2rgb_get_func_ptr ( SwsContext * c ) <nl> } <nl>  <nl> static void fill_table ( uint8_t * table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , <nl> - const int inc , void * y_tab ) <nl> + const int64_t inc , void * y_tab ) <nl> { <nl> int i ; <nl> uint8_t * y_table = y_tab ;
void av_image_copy ( uint8_t * dst_data [ 4 ], int dst_linesizes [ 4 ], <nl> for ( i = 0 ; i < planes_nb ; i ++) { <nl> int h = height ; <nl> int bwidth = av_image_get_linesize ( pix_fmt , width , i ); <nl> + if ( bwidth < 0 ) { <nl> + av_log ( 0 , AV_LOG_ERROR , " av_image_get_linesize failed \ n "); <nl> + return ; <nl> + } <nl> if ( i == 1 || i == 2 ) { <nl> h = -((- height )>> desc -> log2_chroma_h ); <nl> }
int ff_dirac_golomb_read_32bit ( DiracGolombLUT * lut_ctx , const uint8_t * buf , <nl> if (( c_idx + 1 ) > coeffs ) <nl> return c_idx ; <nl>  <nl> + if ( res_bits >= RSIZE_BITS ) <nl> + res_bits = res = 0 ; <nl> + <nl> /* res_bits is a hint for better branch prediction */ <nl> if ( res_bits && l -> sign ) { <nl> int32_t coeff = 1 ;
int ff_hevc_decode_nal_pps ( HEVCContext * s ) <nl> int pps_range_extensions_flag = get_bits1 ( gb ); <nl> /* int pps_extension_7bits = */ get_bits ( gb , 7 ); <nl> if ( sps -> ptl . general_ptl . profile_idc == FF_PROFILE_HEVC_REXT && pps_range_extensions_flag ) { <nl> - pps_range_extensions ( s , pps , sps ); <nl> + if (( ret = pps_range_extensions ( s , pps , sps )) < 0 ) <nl> + goto err ; <nl> } <nl> } <nl> 
static inline int put_bits_count ( PutBitContext * s ) <nl> static inline void flush_put_bits ( PutBitContext * s ) <nl> { <nl> # ifndef BITSTREAM_WRITER_LE <nl> - s -> bit_buf <<= s -> bit_left ; <nl> + if ( s -> bit_left < 32 ) <nl> + s -> bit_buf <<= s -> bit_left ; <nl> # endif <nl> while ( s -> bit_left < 32 ) { <nl> /* XXX : should test end of buffer */
void av_packet_free ( AVPacket ** pkt ) <nl> static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> { <nl> int ret ; <nl> - if (( unsigned ) size >= ( unsigned ) size + AV_INPUT_BUFFER_PADDING_SIZE ) <nl> + if ( size < 0 || size >= INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> ret = av_buffer_realloc ( buf , size + AV_INPUT_BUFFER_PADDING_SIZE );
static int decode_channel_residues ( WmallDecodeCtx * s , int ch , int tile_size ) <nl> residue = quo ; <nl> else { <nl> rem_bits = av_ceil_log2 ( ave_mean ); <nl> - rem = rem_bits ? get_bits (& s -> gb , rem_bits ) : 0 ; <nl> + rem = rem_bits ? get_bits_long (& s -> gb , rem_bits ) : 0 ; <nl> residue = ( quo << rem_bits ) + rem ; <nl> } <nl> 
static int encode_thread ( AVCodecContext * c , void * arg ){ <nl> int mb_x , mb_y , pdif = 0 ; <nl> int chr_h = 16 >> s -> chroma_y_shift ; <nl> int i , j ; <nl> - MpegEncContext best_s , backup_s ; <nl> + MpegEncContext best_s = { 0 }, backup_s ; <nl> uint8_t bit_buf [ 2 ][ MAX_MB_BYTES ]; <nl> uint8_t bit_buf2 [ 2 ][ MAX_MB_BYTES ]; <nl> uint8_t bit_buf_tex [ 2 ][ MAX_MB_BYTES ];
static av_cold int X264_close ( AVCodecContext * avctx ) <nl> if ( x4 -> enc ) <nl> x264_encoder_close ( x4 -> enc ); <nl>  <nl> - av_free ( x4 -> preset ); <nl> - av_free ( x4 -> tune ); <nl> - av_free ( x4 -> profile ); <nl> - av_free ( x4 -> level ); <nl> - av_free ( x4 -> stats ); <nl> - av_free ( x4 -> weightp ); <nl> - av_free ( x4 -> x264opts ); <nl> - <nl> return 0 ; <nl> } <nl> 
int av_write_frame ( AVFormatContext * s , AVPacket * pkt ) <nl> return 1 ; <nl> } <nl>  <nl> + ret = do_packet_auto_bsf ( s , pkt ); <nl> + if ( ret <= 0 ) <nl> + return ret ; <nl> + <nl> # if FF_API_COMPUTE_PKT_FIELDS2 && FF_API_LAVF_AVCTX <nl> ret = compute_muxer_pkt_fields ( s , s -> streams [ pkt -> stream_index ], pkt ); <nl> 
static int xwma_read_header ( AVFormatContext * s ) <nl>  <nl> /* Estimate the duration from the total number of output bytes . */ <nl> const uint64_t total_decoded_bytes = dpds_table [ dpds_table_size - 1 ]; <nl> + <nl> + if (! bytes_per_sample ) { <nl> + av_log ( s , AV_LOG_ERROR , " bytes_per_sample is 0 \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> st -> duration = total_decoded_bytes / bytes_per_sample ; <nl>  <nl> /* Use the dpds data to build a seek table . We can only do this after
static int decode_dvd_subtitles ( DVDSubContext * ctx , AVSubtitle * sub_header , <nl> w = x2 - x1 + 1 ; <nl> if ( w < 0 ) <nl> w = 0 ; <nl> - h = y2 - y1 ; <nl> + h = y2 - y1 + 1 ; <nl> if ( h < 0 ) <nl> h = 0 ; <nl> if ( w > 0 && h > 0 ) {
static inline int decode_subframe ( FLACContext * s , int channel ) <nl> if ( wasted ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - decoded [ i ] <<= wasted ; <nl> + decoded [ i ] = ( unsigned ) decoded [ i ] << wasted ; <nl> } <nl>  <nl> return 0 ;
static int huf_uncompress ( GetByteContext * gb , <nl>  <nl> fail : <nl> for ( i = 0 ; i < HUF_DECSIZE ; i ++) { <nl> - if ( hdec [ i ]. p ) <nl> + if ( hdec ) <nl> av_freep (& hdec [ i ]. p ); <nl> } <nl> 
static int parse_bintree ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " SkipCell procedure not implemented yet !\ n "); <nl>  <nl> CHECK_CELL <nl> + if (! curr_cell . mv_ptr ) <nl> + return AVERROR_INVALIDDATA ; <nl> copy_cell ( ctx , plane , & curr_cell ); <nl> return 0 ; <nl> }
static inline int set_options ( AVFilterContext * ctx , const char * args ) <nl>  <nl> hue -> hue_expr = NULL ; <nl> hue -> hue_deg_expr = NULL ; <nl> + hue -> saturation_expr = NULL ; <nl>  <nl> if (( ret = av_set_options_string ( hue , args , "=", ":")) < 0 ) <nl> return ret ;
static void stream_component_close ( VideoState * is , int stream_index ) <nl> if ( is -> rdft ) { <nl> av_rdft_end ( is -> rdft ); <nl> av_freep (& is -> rdft_data ); <nl> + is -> rdft = NULL ; <nl> + is -> rdft_bits = 0 ; <nl> } <nl> break ; <nl> case AVMEDIA_TYPE_VIDEO :
static void fix_coding_method_array ( int sb , int channels , sb_int8_array coding_ <nl> run = 1 ; <nl> case_val = 8 ; <nl> } else { <nl> - switch ( switchtable [ coding_method [ ch ][ sb ][ j ]]) { <nl> + switch ( switchtable [ coding_method [ ch ][ sb ][ j ]- 8 ]) { <nl> case 0 : run = 10 ; case_val = 10 ; break ; <nl> case 1 : run = 1 ; case_val = 16 ; break ; <nl> case 2 : run = 5 ; case_val = 24 ; break ;
static int cin_read_frame_header ( CinDemuxContext * cin , AVIOContext * pb ) { <nl>  <nl> if ( avio_rl32 ( pb ) != 0xAA55AA55 ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( hdr -> video_frame_size < 0 || hdr -> audio_frame_size < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> return 0 ; <nl> }
static inline void mv_pred_direct ( AVSContext * h , cavs_vector * pmv_fw , <nl> cavs_vector * col_mv ) <nl> { <nl> cavs_vector * pmv_bw = pmv_fw + MV_BWD_OFFS ; <nl> - int den = h -> direct_den [ col_mv -> ref ]; <nl> + unsigned den = h -> direct_den [ col_mv -> ref ]; <nl> int m = FF_SIGNBIT ( col_mv -> x ); <nl>  <nl> pmv_fw -> dist = h -> dist [ 1 ];
static int vp9_raw_reorder_make_output ( AVBSFContext * bsf , <nl> "(%" PRId64 ") from slot % d .\ n ", <nl> frame -> sequence , frame -> pts , s ); <nl>  <nl> - frame -> packet = av_packet_alloc (); <nl> - if (! frame -> packet ) <nl> - return AVERROR ( ENOMEM ); <nl> - <nl> err = av_new_packet ( out , 2 ); <nl> if ( err < 0 ) <nl> return err ;
static int vqf_read_seek ( AVFormatContext * s , <nl> { <nl> VqfContext * c = s -> priv_data ; <nl> AVStream * st ; <nl> - int ret ; <nl> + int64_t ret ; <nl> int64_t pos ; <nl>  <nl> st = s -> streams [ stream_index ];
static void jpeg2000_dec_cleanup ( Jpeg2000DecoderContext * s ) <nl> } <nl> } <nl> av_freep (& s -> tile ); <nl> + memset ( s -> codsty , 0 , sizeof ( s -> codsty )); <nl> + memset ( s -> qntsty , 0 , sizeof ( s -> qntsty )); <nl> s -> numXtiles = s -> numYtiles = 0 ; <nl> } <nl> 
static int fdk_aac_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size , 0 ); <nl> + err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size / sizeof ( INT_PCM ), 0 ); <nl> if ( err == AAC_DEC_NOT_ENOUGH_BITS ) { <nl> ret = avpkt -> size - valid ; <nl> goto end ;
static void png_handle_row ( PNGDecContext * s ) <nl> } <nl> s -> y ++; <nl> if ( s -> y == s -> height ) { <nl> + memset ( s -> last_row , 0 , s -> row_size ); <nl> for (;;) { <nl> if ( s -> pass == NB_PASSES - 1 ) { <nl> s -> state |= PNG_ALLIMAGE ;
static int skeleton_header ( AVFormatContext * s , int idx ) <nl> start_num = AV_RL64 ( buf + 12 ); <nl> start_den = AV_RL64 ( buf + 20 ); <nl>  <nl> - if ( start_den ) { <nl> + if ( start_den > 0 && start_num > 0 ) { <nl> int base_den ; <nl> av_reduce (& start_time , & base_den , start_num , start_den , INT_MAX ); <nl> avpriv_set_pts_info ( st , 64 , 1 , base_den );
static void mpeg_er_decode_mb ( void * opaque , int ref , int mv_dir , int mv_type , <nl> s -> mb_skipped = mb_skipped ; <nl> s -> mb_x = mb_x ; <nl> s -> mb_y = mb_y ; <nl> + s -> mcsel = 0 ; <nl> memcpy ( s -> mv , mv , sizeof (* mv )); <nl>  <nl> ff_init_block_index ( s );
static int read_header ( ShortenContext * s ) <nl> s -> channels = get_uint ( s , CHANSIZE ); <nl> if ( s -> channels <= 0 || s -> channels > MAX_CHANNELS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + s -> channels = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> s -> avctx -> channels = s -> channels ;
static int cmv_decode_frame ( AVCodecContext * avctx , <nl> CmvContext * s = avctx -> priv_data ; <nl> const uint8_t * buf_end = buf + buf_size ; <nl>  <nl> + if ( buf_end - buf < EA_PREAMBLE_SIZE ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( AV_RL32 ( buf )== MVIh_TAG || AV_RB32 ( buf )== MVIh_TAG ) { <nl> cmv_process_header ( s , buf + EA_PREAMBLE_SIZE , buf_end ); <nl> return buf_size ;
typedef struct TargaContext { <nl> } TargaContext ; <nl>  <nl> # define CHECK_BUFFER_SIZE ( buf , buf_end , needed , where ) \ <nl> - if ( buf + needed > buf_end ){ \ <nl> + if ( needed > buf_end - buf ){ \ <nl> av_log ( avctx , AV_LOG_ERROR , " Problem : unexpected end of data while reading " where "\ n "); \ <nl> return - 1 ; \ <nl> } \
static int parse_playlist ( URLContext * h , const char * url ) <nl> return ret ; <nl>  <nl> read_chomp_line ( in , line , sizeof ( line )); <nl> - if ( strcmp ( line , "# EXTM3U ")) <nl> - return AVERROR_INVALIDDATA ; <nl> + if ( strcmp ( line , "# EXTM3U ")) { <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto fail ; <nl> + } <nl>  <nl> free_segment_list ( s ); <nl> s -> finished = 0 ;
static int find_slice_quant ( AVCodecContext * avctx , const AVFrame * pic , <nl> if ( ctx -> alpha_bits ) <nl> bits += estimate_alpha_plane ( ctx , & error , src , linesize [ 3 ], <nl> mbs_per_slice , q , td -> blocks [ 3 ]); <nl> - if ( bits > 65000 * 8 ) { <nl> + if ( bits > 65000 * 8 ) <nl> error = SCORE_LIMIT ; <nl> - break ; <nl> - } <nl> + <nl> slice_bits [ q ] = bits ; <nl> slice_score [ q ] = error ; <nl> }
static int ape_tag_read_field ( AVFormatContext * s ) <nl> if (! value ) <nl> return AVERROR ( ENOMEM ); <nl> c = avio_read ( pb , value , size ); <nl> + if ( c < 0 ) <nl> + return c ; <nl> value [ c ] = 0 ; <nl> av_dict_set (& s -> metadata , key , value , AV_DICT_DONT_STRDUP_VAL ); <nl> }
static int dca_exss_parse_asset_header ( DCAContext * s ) <nl> { <nl> int header_pos = get_bits_count (& s -> gb ); <nl> int header_size ; <nl> - int channels ; <nl> + int channels = 0 ; <nl> int embedded_stereo = 0 ; <nl> int embedded_6ch = 0 ; <nl> int drc_code_present ; <nl> - int extensions_mask ; <nl> + int av_uninit ( extensions_mask ); <nl> int i , j ; <nl>  <nl> if ( get_bits_left (& s -> gb ) < 16 )
static int mkv_parse_video_projection ( AVStream * st , const MatroskaTrack * track ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> break ; <nl> + case MATROSKA_VIDEO_PROJECTION_TYPE_RECTANGULAR : <nl> + /* No Spherical metadata */ <nl> + return 0 ; <nl> default : <nl> av_log ( NULL , AV_LOG_WARNING , <nl> " Unknown spherical metadata type %" PRIu64 "\ n ",
static int decode_frame ( AVCodecContext * avctx , <nl> decoded = loco_decode_plane ( l , p -> data [ 0 ] + p -> linesize [ 0 ]*( avctx -> height - 1 ) + 3 , avctx -> width , avctx -> height , <nl> - p -> linesize [ 0 ], buf , buf_size , 4 ); <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl>  <nl> if ( decoded < 0 || decoded > buf_size )
static int film_probe ( AVProbeData * p ) <nl> if ( AV_RB32 (& p -> buf [ 0 ]) != FILM_TAG ) <nl> return 0 ; <nl>  <nl> + if ( AV_RB32 (& p -> buf [ 16 ]) != FDSC_TAG ) <nl> + return 0 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
static int rv20_decode_picture_header ( MpegEncContext * s ) <nl> if ( s -> avctx -> debug & FF_DEBUG_PICT_INFO ){ <nl> av_log ( s -> avctx , AV_LOG_DEBUG , " F % d /% d \ n ", f , rpr_bits ); <nl> } <nl> - } <nl> + } else if ( av_image_check_size ( s -> width , s -> height , 0 , s -> avctx ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> mb_pos = ff_h263_decode_mba ( s ); <nl> 
static int read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( size > 0 ) { <nl> - if ( pos + size < pos ) <nl> + if ( pos > INT64_MAX - size ) <nl> return AVERROR_INVALIDDATA ; <nl> avio_skip ( pb , FFMAX ( 0 , pos + size - avio_tell ( pb ))); <nl> }
end : <nl>  <nl> return ret ; <nl> free_and_end : <nl> - if ( avctx -> codec && <nl> + if ( avctx -> codec && avctx -> codec -> close && <nl> ( codec_init_ok || <nl> ( avctx -> codec -> caps_internal & FF_CODEC_CAP_INIT_CLEANUP ))) <nl> avctx -> codec -> close ( avctx );
static int expand_rle_row ( SgiState * s , uint8_t * out_buf , <nl> } <nl>  <nl> /* Check for buffer overflow . */ <nl> - if ( out_buf + pixelstride * ( count - 1 ) >= out_end ) <nl> + if ( out_end - out_buf <= pixelstride * ( count - 1 )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if ( pixel & 0x80 ) {
static int ff_asf_parse_packet ( AVFormatContext * s , AVIOContext * pb , AVPacket * pk <nl> // printf (" packet % d % d \ n ", asf_st -> pkt . size , asf -> packet_frag_size ); <nl> asf_st -> pkt . size = 0 ; <nl> asf_st -> pkt . data = 0 ; <nl> + asf_st -> pkt . side_data_elems = 0 ; <nl> + asf_st -> pkt . side_data = NULL ; <nl> break ; // packet completed <nl> } <nl> }
static int pcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR ( EINVAL ); <nl> } <nl>  <nl> + if ( avctx -> channels == 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> n = avctx -> channels * sample_size ; <nl>  <nl> if ( n && buf_size % n ) {
static int sol_read_packet ( AVFormatContext * s , <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR ( EIO ); <nl> ret = av_get_packet ( s -> pb , pkt , MAX_SIZE ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> pkt -> stream_index = 0 ; <nl>  <nl> /* note : we need to modify the packet size here to handle the last
static int iff_read_header ( AVFormatContext * s ) <nl> break ; <nl>  <nl> case ID_CMAP : <nl> + if ( data_size < 3 || data_size > 768 || data_size % 3 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid CMAP chunk size % d \ n ", <nl> + data_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> st -> codec -> extradata_size = data_size ; <nl> st -> codec -> extradata = av_malloc ( data_size ); <nl> if (! st -> codec -> extradata )
int main ( int argc , char * argv []) <nl> end : <nl> avformat_close_input (& fmt_ctx ); <nl> /* note : the internal buffer could have changed , and be != avio_ctx_buffer */ <nl> - av_freep (& avio_ctx -> buffer ); <nl> - av_freep (& avio_ctx ); <nl> + if ( avio_ctx ) { <nl> + av_freep (& avio_ctx -> buffer ); <nl> + av_freep (& avio_ctx ); <nl> + } <nl> av_file_unmap ( buffer , buffer_size ); <nl>  <nl> if ( ret < 0 ) {
rdt_new_context ( void ) <nl> { <nl> PayloadContext * rdt = av_mallocz ( sizeof ( PayloadContext )); <nl>  <nl> - avformat_open_input (& rdt -> rmctx , "", & ff_rdt_demuxer , NULL ); <nl> + int ret = avformat_open_input (& rdt -> rmctx , "", & ff_rdt_demuxer , NULL ); <nl> + if ( ret < 0 ) { <nl> + av_free ( rdt ); <nl> + return NULL ; <nl> + } <nl>  <nl> return rdt ; <nl> }
static int ape_tag_read_field ( AVFormatContext * s ) <nl> { <nl> AVIOContext * pb = s -> pb ; <nl> uint8_t key [ 1024 ], * value ; <nl> - uint32_t size , flags ; <nl> + int64_t size , flags ; <nl> int i , c ; <nl>  <nl> size = avio_rl32 ( pb ); /* field size */
static int thp_read_packet ( AVFormatContext * s , <nl> thp -> frame ++; <nl>  <nl> ret = av_get_packet ( pb , pkt , size ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( ret != size ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO );
static void hScale16_c ( SwsContext * c , int16_t * _dst , int dstW , const uint8_t * _s <nl> for ( i = 0 ; i < dstW ; i ++) { <nl> int j ; <nl> int srcPos = filterPos [ i ]; <nl> - unsigned int val = 0 ; <nl> + int val = 0 ; <nl>  <nl> for ( j = 0 ; j < filterSize ; j ++) { <nl> val += src [ srcPos + j ] * filter [ filterSize * i + j ];
int ff_img_read_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> } <nl>  <nl> if ( codec -> codec_id == AV_CODEC_ID_NONE ) { <nl> - AVProbeData pd ; <nl> + AVProbeData pd = { 0 }; <nl> AVInputFormat * ifmt ; <nl> uint8_t header [ PROBE_BUF_MIN + AVPROBE_PADDING_SIZE ]; <nl> int ret ;
static int lrc_read_header ( AVFormatContext * s ) <nl> } <nl> ff_subtitles_queue_finalize ( s , & lrc -> q ); <nl> ff_metadata_conv_ctx ( s , NULL , ff_lrc_metadata_conv ); <nl> + av_bprint_finalize (& line , NULL ); <nl> return 0 ; <nl> } <nl> 
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels == 0 || channels > 64 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> ff_dv_rl_vlc [ i ]. run = run ; <nl> } <nl> ff_free_vlc (& dv_vlc ); <nl> - <nl> - dv_vlc_map_tableinit (); <nl> } <nl>  <nl> /* Generic DSP setup */ <nl> static av_cold int dvvideo_init_encoder ( AVCodecContext * avctx ) <nl> return AVERROR ( EINVAL ); <nl> } <nl>  <nl> + dv_vlc_map_tableinit (); <nl> + <nl> return ff_dvvideo_init ( avctx ); <nl> } <nl> 
static void write_frame ( AVFormatContext * s , AVPacket * pkt , OutputStream * ost ) <nl> * reordering , see do_video_out () <nl> */ <nl> if (!( avctx -> codec_type == AVMEDIA_TYPE_VIDEO && avctx -> codec )) { <nl> - if ( ost -> frame_number >= ost -> max_frames ) <nl> + if ( ost -> frame_number >= ost -> max_frames ) { <nl> + av_free_packet ( pkt ); <nl> return ; <nl> + } <nl> ost -> frame_number ++; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> + p -> key_frame = 1 ; <nl> * got_frame = 1 ; <nl>  <nl> return buf_size ;
static int dvbsub_decode ( AVCodecContext * avctx , <nl> break ; <nl> case DVBSUB_DISPLAYDEFINITION_SEGMENT : <nl> dvbsub_parse_display_definition_segment ( avctx , p , segment_length ); <nl> + break ; <nl> case DVBSUB_DISPLAY_SEGMENT : <nl> * data_size = dvbsub_display_end_segment ( avctx , p , segment_length , sub ); <nl> break ;
static int encode_mode ( CinepakEncContext * s , int h , AVPicture * scratch_pict , AVP <nl> int needs_extra_bit , should_write_temp ; <nl> unsigned char temp [ 64 ]; // 32 / 2 = 16 V4 blocks at 4 B each -> 64 B <nl> mb_info * mb ; <nl> - AVPicture sub_scratch , sub_last ; <nl> + AVPicture sub_scratch = {{ 0 }}, sub_last = {{ 0 }}; <nl>  <nl> // encode codebooks <nl> ////// MacOS vintage decoder compatibility dictates the presence of
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( flv -> wrong_dts ) <nl> dts = AV_NOPTS_VALUE ; <nl> } <nl> - if ( type == 0 ) { <nl> + <nl> + if ( type == 0 && ! st -> codec -> extradata ) { <nl> if (( ret = flv_get_extradata ( s , st , size )) < 0 ) <nl> return ret ; <nl> if ( st -> codec -> codec_id == CODEC_ID_AAC ) {
static void write_section_data ( MpegTSContext * ts , MpegTSFilter * tss1 , <nl> } else <nl> crc_valid = 2 ; <nl> } <nl> - if ( crc_valid ) <nl> + if ( crc_valid ) { <nl> tss -> section_cb ( tss1 , tss -> section_buf , tss -> section_h_size ); <nl> + if ( crc_valid != 1 ) <nl> + tss -> last_ver = - 1 ; <nl> + } <nl> } <nl> } <nl> 
static inline int read_huff_channels ( MLPDecodeContext * m , GetBitContext * gbp , <nl> result = ( result << lsb_bits ) + get_bits ( gbp , lsb_bits ); <nl>  <nl> result += cp -> sign_huff_offset ; <nl> - result <<= quant_step_size ; <nl> + result *= 1 << quant_step_size ; <nl>  <nl> m -> sample_buffer [ pos + s -> blockpos ][ channel ] = result ; <nl> }
static int sap_write_header ( AVFormatContext * s ) <nl> freeaddrinfo ( ai ); <nl> } <nl>  <nl> - contexts = av_mallocz ( sizeof ( AVFormatContext *) * s -> nb_streams ); <nl> + contexts = av_mallocz_array ( s -> nb_streams , sizeof ( AVFormatContext *)); <nl> if (! contexts ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ;
static void search_for_quantizers_anmr ( AVCodecContext * avctx , AACEncContext * s , <nl> } <nl> while ( idx ) { <nl> sce -> sf_idx [ bandaddr [ idx ]] = minq + q0 ; <nl> - minq = paths [ idx ][ minq ]. prev ; <nl> + minq = FFMAX ( paths [ idx ][ minq ]. prev , 0 ); <nl> idx --; <nl> } <nl> // set the same quantizers inside window groups
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl>  <nl> if ( wc -> ch_offset + s -> stereo >= avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_WARNING , " Too many channels coded in a packet .\ n "); <nl> - return ( avctx -> err_recognition & AV_EF_EXPLODE ) ? AVERROR_INVALIDDATA : 0 ; <nl> + return (( avctx -> err_recognition & AV_EF_EXPLODE ) || ! wc -> ch_offset ) ? AVERROR_INVALIDDATA : 0 ; <nl> } <nl>  <nl> samples_l = frame -> extended_data [ wc -> ch_offset ];
static int normalize_bits ( int num , int width ) <nl> if ( num < 0 ) <nl> num = ~ num ; <nl>  <nl> - return width - av_log2 ( num ); <nl> + return width - av_log2 ( num ) - 1 ; <nl> } <nl>  <nl> /**
static int au_read_header ( AVFormatContext * s ) <nl> st -> codec -> channels = channels ; <nl> st -> codec -> sample_rate = rate ; <nl> if ( data_size != AU_UNKNOWN_SIZE ) <nl> - st -> duration = ((( int64_t ) data_size )<< 3 ) / ( st -> codec -> channels * bps ); <nl> + st -> duration = ((( int64_t ) data_size )<< 3 ) / ( st -> codec -> channels * ( int64_t ) bps ); <nl> avpriv_set_pts_info ( st , 64 , 1 , rate ); <nl> return 0 ; <nl> }
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> uint32_t u ; <nl> } value ; <nl>  <nl> - int sign ; <nl> + unsigned int sign ; <nl> int exp = s -> float_max_exp ; <nl>  <nl> if ( s -> got_extra_bits ) {
fail : <nl> fail_at_end : <nl> av_freep (& pic -> codec_picture_params ); <nl> av_frame_free (& pic -> recon_image ); <nl> + av_buffer_unref (& pic -> output_buffer_ref ); <nl> + pic -> output_buffer = VA_INVALID_ID ; <nl> return err ; <nl> } <nl> 
static int opt_show_format_entry ( void * optctx , const char * opt , const char * arg ) <nl> char * buf = av_asprintf (" format =% s ", arg ); <nl> int ret ; <nl>  <nl> + if (! buf ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> av_log ( NULL , AV_LOG_WARNING , <nl> " Option '% s ' is deprecated , use '- show_entries format =% s ' instead \ n ", <nl> opt , arg );
static int send_invoke_response ( URLContext * s , RTMPPacket * pkt ) <nl> { <nl> RTMPContext * rt = s -> priv_data ; <nl> double seqnum ; <nl> - char filename [ 64 ]; <nl> + char filename [ 128 ]; <nl> char command [ 64 ]; <nl> int stringlen ; <nl> char * pchar ;
static int mov_write_audio_tag ( AVFormatContext * s , AVIOContext * pb , MOVMuxContex <nl> uint32_t tag = track -> tag ; <nl>  <nl> if ( track -> mode == MODE_MOV ) { <nl> - if ( track -> timescale > UINT16_MAX ) { <nl> + if ( track -> timescale > UINT16_MAX || ! track -> par -> channels ) { <nl> if ( mov_get_lpcm_flags ( track -> par -> codec_id )) <nl> tag = AV_RL32 (" lpcm "); <nl> version = 2 ;
retry : <nl> uint8_t * side_data = av_packet_new_side_data ( pkt , <nl> AV_PKT_DATA_METADATA_UPDATE , <nl> os -> new_metadata_size ); <nl> + if ( side_data == NULL ) { <nl> + av_free_packet ( pkt ); <nl> + av_free ( pkt ); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> memcpy ( side_data , os -> new_metadata , os -> new_metadata_size ); <nl> av_freep (& os -> new_metadata ); <nl> os -> new_metadata_size = 0 ;
static int libshine_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> SHINEContext * s = avctx -> priv_data ; <nl> MPADecodeHeader hdr ; <nl> unsigned char * data ; <nl> - long written ; <nl> + int written ; <nl> int ret , len ; <nl>  <nl> if ( frame )
static int svq1_decode_frame ( AVCodecContext * avctx , <nl> if ( s -> f_code != 0x20 ) { <nl> uint32_t * src = ( uint32_t *) ( buf + 4 ); <nl>  <nl> + if ( buf_size < 36 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> src [ i ] = (( src [ i ] << 16 ) | ( src [ i ] >> 16 )) ^ src [ 7 - i ]; <nl> }
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> } <nl> } <nl> total_size += 8 ; <nl> - if ( a . size == 1 ) { /* 64 bit extended size */ <nl> + if ( a . size == 1 && total_size + 8 <= atom . size ) { /* 64 bit extended size */ <nl> a . size = avio_rb64 ( pb ) - 8 ; <nl> total_size += 8 ; <nl> }
static AVStream * init_stream ( AVFormatContext * s ) <nl> avpriv_set_pts_info ( st , 60 , bin -> framerate . den , bin -> framerate . num ); <nl>  <nl> /* simulate tty display speed */ <nl> - bin -> chars_per_frame = FFMAX ( av_q2d ( st -> time_base ) * bin -> chars_per_frame , 1 ); <nl> + bin -> chars_per_frame = av_clip ( av_q2d ( st -> time_base ) * bin -> chars_per_frame , 1 , INT_MAX ); <nl>  <nl> return st ; <nl> }
av_cold int ff_snow_common_init ( AVCodecContext * avctx ){ <nl>  <nl> s -> avctx = avctx ; <nl> s -> max_ref_frames = 1 ; // just make sure it ' s not an invalid value in case of no initial keyframe <nl> + s -> spatial_decomposition_count = 1 ; <nl>  <nl> ff_me_cmp_init (& s -> mecc , avctx ); <nl> ff_hpeldsp_init (& s -> hdsp , avctx -> flags );
static int load_ipmovie_packet ( IPMVEContext * s , AVIOContext * pb , <nl> int chunk_type ; <nl>  <nl> if ( s -> audio_chunk_offset ) { <nl> + if ( s -> audio_type == CODEC_ID_NONE ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " Can not read audio packet before " <nl> + " audio codec is known \ n "); <nl> + return CHUNK_BAD ; <nl> + } <nl>  <nl> /* adjust for PCM audio by skipping chunk header */ <nl> if ( s -> audio_type != CODEC_ID_INTERPLAY_DPCM ) {
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame_ptr , <nl> header = AV_RB32 ( buf ); <nl> if ( header >> 8 == AV_RB32 (" TAG ")>> 8 ) { <nl> av_log ( avctx , AV_LOG_DEBUG , " discarding ID3 tag \ n "); <nl> - return buf_size ; <nl> + return buf_size + skipped ; <nl> } <nl> ret = avpriv_mpegaudio_decode_header (( MPADecodeHeader *) s , header ); <nl> if ( ret < 0 ) {
 <nl> static int null_filter_samples ( AVFilterLink * link , AVFilterBufferRef * samplesref ) <nl> { <nl> + avfilter_unref_bufferp (& samplesref ); <nl> return 0 ; <nl> } <nl> 
int attribute_align_arg avresample_convert ( AVAudioResampleContext * avr , <nl> resample_out = & output_buffer ; <nl> else <nl> resample_out = avr -> resample_out_buffer ; <nl> - av_dlog ( avr , "[ resample ] % s to % s \ n ", current_buffer -> name , <nl> + av_dlog ( avr , "[ resample ] % s to % s \ n ", <nl> + current_buffer ? current_buffer -> name : " null ", <nl> resample_out -> name ); <nl> ret = ff_audio_resample ( avr -> resample , resample_out , <nl> current_buffer );
int ff_get_cpu_flags_x86 ( void ) <nl>  <nl> if ( max_ext_level >= 0x80000001 ){ <nl> cpuid ( 0x80000001 , eax , ebx , ecx , ext_caps ); <nl> - if ( ext_caps & ( 1 << 31 )) <nl> + if ( ext_caps & ( 1U << 31 )) <nl> rval |= AV_CPU_FLAG_3DNOW ; <nl> if ( ext_caps & ( 1 << 30 )) <nl> rval |= AV_CPU_FLAG_3DNOWEXT ;
static int filter_frame ( AVFilterLink * inlink , AVFrame * inpicref ) <nl>  <nl> inpicref -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> ret = ff_filter_frame ( outlink , inpicref ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_frame_free (& second ); <nl> return ret ; <nl> + } <nl>  <nl> second -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> return ff_filter_frame ( outlink , second );
static int escape124_decode_frame ( AVCodecContext * avctx , <nl> // This codebook can be cut off at places other than <nl> // powers of 2 , leaving some of the entries undefined . <nl> cb_size = get_bits_long (& gb , 20 ); <nl> + if (! cb_size ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid codebook size 0 .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> cb_depth = av_log2 ( cb_size - 1 ) + 1 ; <nl> } else { <nl> cb_depth = get_bits (& gb , 4 );
ebml_read_ascii ( MatroskaDemuxContext * matroska , <nl> offset_t pos = url_ftell ( pb ); <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Read error at pos . %" PRIu64 " ( 0x %" PRIx64 ")\ n ", pos , pos ); <nl> + av_free (* str ); <nl> return AVERROR ( EIO ); <nl> } <nl> (* str )[ size ] = '\ 0 ';
static int decode_pic_hdr ( IVI45DecContext * ctx , AVCodecContext * avctx ) <nl> /* skip picture header extension if any */ <nl> while ( get_bits1 (& ctx -> gb )) { <nl> ff_dlog ( avctx , " Pic hdr extension encountered !\ n "); <nl> + if ( get_bits_left (& ctx -> gb ) < 10 ) <nl> + return AVERROR_INVALIDDATA ; <nl> skip_bits (& ctx -> gb , 8 ); <nl> } <nl> 
static int ac3_sync ( uint64_t state , AACAC3ParseContext * hdr_info , <nl> int err ; <nl> union { <nl> uint64_t u64 ; <nl> - uint8_t u8 [ 8 ]; <nl> + uint8_t u8 [ 8 + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> } tmp = { av_be2ne64 ( state ) }; <nl> AC3HeaderInfo hdr ; <nl> GetBitContext gbc ;
static int hls_write_header ( AVFormatContext * s ) <nl> int ret , i ; <nl> char * p ; <nl> const char * pattern = "% d . ts "; <nl> - int basename_size = strlen ( s -> filename ) + strlen ( pattern ); <nl> + int basename_size = strlen ( s -> filename ) + strlen ( pattern ) + 1 ; <nl>  <nl> hls -> number = 0 ; <nl> 
static int read_quant_table ( RangeCoder * c , int16_t * quant_table , int scale ) <nl> memset ( state , 128 , sizeof ( state )); <nl>  <nl> for ( v = 0 ; i < 128 ; v ++) { <nl> - unsigned len = get_symbol ( c , state , 0 ) + 1 ; <nl> + unsigned len = get_symbol ( c , state , 0 ) + 1U ; <nl>  <nl> if ( len > 128 - i || ! len ) <nl> return AVERROR_INVALIDDATA ;
static int mov_read_udta_string ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> if (! key ) <nl> return 0 ; <nl> - if ( atom . size < 0 ) <nl> + if ( atom . size < 0 || str_size >= INT_MAX / 2 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> // worst - case requirement for output string in case of utf8 coded input
static int decodeTonalComponents ( GetBitContext * gb , tonal_component * pComponent <nl>  <nl> for ( k = 0 ; k < coded_components ; k ++) { <nl> sfIndx = get_bits ( gb , 6 ); <nl> + if ( component_count >= 64 ) <nl> + return AVERROR_INVALIDDATA ; <nl> pComponent [ component_count ]. pos = j * 64 + ( get_bits ( gb , 6 )); <nl> max_coded_values = SAMPLES_PER_FRAME - pComponent [ component_count ]. pos ; <nl> coded_values = coded_values_per_component + 1 ;
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> if (! reslevel -> band ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> + if ( reslevel -> num_precincts_x * ( uint64_t ) reslevel -> num_precincts_y * reslevel -> nbands > avctx -> max_pixels / sizeof (* reslevel -> band -> prec )) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> for ( bandno = 0 ; bandno < reslevel -> nbands ; bandno ++, gbandno ++) { <nl> ret = init_band ( avctx , reslevel , <nl> comp , codsty , qntsty ,
int ff_ivi_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> } <nl> } <nl> } <nl> + } else { <nl> + if ( ctx -> is_scalable ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> + for ( p = 0 ; p < 3 ; p ++) { <nl> + if (! ctx -> planes [ p ]. bands [ 0 ]. buf ) <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> // STOP_TIMER (" decode_planes "); }
static int lag_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if (! l -> rgb_planes ) { <nl> l -> rgb_stride = FFALIGN ( avctx -> width , 16 ); <nl> - l -> rgb_planes = av_malloc ( l -> rgb_stride * avctx -> height * planes ); <nl> + l -> rgb_planes = av_malloc ( l -> rgb_stride * avctx -> height * planes + 1 ); <nl> if (! l -> rgb_planes ) { <nl> av_log ( avctx , AV_LOG_ERROR , " cannot allocate temporary buffer \ n "); <nl> return AVERROR ( ENOMEM );
static int read_frame ( BVID_DemuxContext * vid , AVIOContext * pb , AVPacket * pkt , <nl> if ( vid -> palette ) { <nl> uint8_t * pdata = av_packet_new_side_data ( pkt , AV_PKT_DATA_PALETTE , <nl> BVID_PALETTE_SIZE ); <nl> - memcpy ( pdata , vid -> palette , BVID_PALETTE_SIZE ); <nl> + if ( pdata ) <nl> + memcpy ( pdata , vid -> palette , BVID_PALETTE_SIZE ); <nl> av_freep (& vid -> palette ); <nl> } <nl> 
static int mov_write_header ( AVFormatContext * s ) <nl> else if (! TAG_IS_AVCI ( track -> tag )){ <nl> track -> vos_len = st -> codec -> extradata_size ; <nl> track -> vos_data = av_malloc ( track -> vos_len ); <nl> - if (! track -> vos_data ) <nl> + if (! track -> vos_data ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> goto error ; <nl> + } <nl> memcpy ( track -> vos_data , st -> codec -> extradata , track -> vos_len ); <nl> } <nl> }
static int ape_tag_read_field ( AVFormatContext * s ) <nl> if (! value ) <nl> return AVERROR ( ENOMEM ); <nl> c = avio_read ( pb , value , size ); <nl> - if ( c < 0 ) <nl> + if ( c < 0 ) { <nl> + av_free ( value ); <nl> return c ; <nl> + } <nl> value [ c ] = 0 ; <nl> av_dict_set (& s -> metadata , key , value , AV_DICT_DONT_STRDUP_VAL ); <nl> }
static inline int ff_fast_malloc ( void * ptr , unsigned int * size , size_t min_size , <nl> void ** p = ptr ; <nl> if ( min_size <= * size && * p ) <nl> return 0 ; <nl> - min_size = FFMAX ( 17 * min_size / 16 + 32 , min_size ); <nl> + min_size = FFMAX ( min_size + min_size / 16 + 32 , min_size ); <nl> av_free (* p ); <nl> * p = zero_realloc ? av_mallocz ( min_size ) : av_malloc ( min_size ); <nl> if (!* p )
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl>  <nl> rt -> state = STATE_START ; <nl> if ( rtmp_handshake ( s , rt )) <nl> - return - 1 ; <nl> + goto fail ; <nl>  <nl> rt -> chunk_size = 128 ; <nl> rt -> state = STATE_HANDSHAKED ;
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl>  <nl> display_matrix = ( uint32_t *) av_stream_get_side_data ( st , AV_PKT_DATA_DISPLAYMATRIX , <nl> & display_matrix_size ); <nl> - if ( display_matrix_size < 9 * sizeof (* display_matrix )) <nl> + if ( display_matrix && display_matrix_size < 9 * sizeof (* display_matrix )) <nl> display_matrix = NULL ; <nl> } <nl> 
static int opus_packet ( AVFormatContext * avf , int idx ) <nl>  <nl> if (! os -> psize ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( os -> granule > INT64_MAX - UINT32_MAX ) { <nl> + if ( os -> granule > ( 1LL << 62 )) { <nl> av_log ( avf , AV_LOG_ERROR , " Unsupported huge granule pos %" PRId64 "\ n ", os -> granule ); <nl> return AVERROR_INVALIDDATA ; <nl> }
int vp56_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> VP56Context * s = avctx -> priv_data ; <nl> AVFrame * const p = s -> framep [ VP56_FRAME_CURRENT ]; <nl> int remaining_buf_size = buf_size ; <nl> - int is_alpha , alpha_offset ; <nl> + int is_alpha , av_uninit ( alpha_offset ); <nl>  <nl> if ( s -> has_alpha ) { <nl> alpha_offset = bytestream_get_be24 (& buf );
static int swf_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> tag = get_swf_tag ( pb , & len ); <nl> if ( tag < 0 ) <nl> return tag ; <nl> + if ( len < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " len % d is invalid \ n ", len ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( tag == TAG_VIDEOSTREAM ) { <nl> int ch_id = avio_rl16 ( pb ); <nl> len -= 2 ;
ogg_gptopts ( AVFormatContext * s , int i , uint64_t gp , int64_t * dts ) <nl> if ( dts ) <nl> * dts = pts ; <nl> } <nl> + if ( pts > INT64_MAX && pts != AV_NOPTS_VALUE ) { <nl> + // The return type is unsigned , we thus cannot return negative pts <nl> + av_log ( s , AV_LOG_ERROR , " invalid pts %" PRId64 "\ n ", pts ); <nl> + pts = AV_NOPTS_VALUE ; <nl> + } <nl>  <nl> return pts ; <nl> }
static int join_request_frame ( AVFilterLink * outlink ) <nl>  <nl> ret = ff_filter_frame ( outlink , frame ); <nl>  <nl> - memset ( s -> input_frames , 0 , sizeof (* s -> input_frames ) * ctx -> nb_inputs ); <nl> + for ( i = 0 ; i < ctx -> nb_inputs ; i ++) <nl> + av_frame_free (& s -> input_frames [ i ]); <nl>  <nl> return ret ; <nl> 
static uint64_t calc_rice_params ( RiceContext * rc , <nl> bits [ pmin ] = UINT32_MAX ; <nl> for ( i = pmax ; ; ) { <nl> bits [ i ] = calc_optimal_rice_params (& tmp_rc , i , sums , n , pred_order , kmax , exact ); <nl> - if ( bits [ i ] < bits [ opt_porder ]) { <nl> + if ( bits [ i ] < bits [ opt_porder ] || pmax == pmin ) { <nl> opt_porder = i ; <nl> * rc = tmp_rc ; <nl> }
retry : <nl> StreamInfo * stream = st -> priv_data ; <nl> const int avail_data = av_fifo_size ( stream -> fifo ); <nl> const int space = stream -> max_buffer_size - stream -> buffer_index ; <nl> - int rel_space = 1024 * space / stream -> max_buffer_size ; <nl> + int rel_space = 1024LL * space / stream -> max_buffer_size ; <nl> PacketDesc * next_pkt = stream -> premux_packet ; <nl>  <nl> /* for subtitle , a single PES packet must be generated ,
static int decode_frame ( AVCodecContext * avctx , <nl> buf += 5 ; <nl>  <nl> if ( video_size ) { <nl> - if ( video_size < 0 ) { <nl> + if ( video_size < 0 || video_size > buf_size ) { <nl> av_log ( avctx , AV_LOG_ERROR , " video size % d invalid \ n ", video_size ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static void vc1_mc_4mv_chroma4 ( VC1Context * v ) <nl> uvmy_field [ i ] = ( uvmy_field [ i ] & 3 ) << 1 ; <nl>  <nl> if ( fieldmv && !( uvsrc_y & 1 )) <nl> - v_edge_pos --; <nl> + v_edge_pos = ( s -> v_edge_pos >> 1 ) - 1 ; <nl> + <nl> if ( fieldmv && ( uvsrc_y & 1 ) && uvsrc_y < 2 ) <nl> uvsrc_y --; <nl> if (( v -> mv_mode == MV_PMODE_INTENSITY_COMP )
int main ( int argc , char ** argv ) <nl> goto end ; <nl>  <nl> /* read all packets */ <nl> + packet0 . data = NULL ; <nl> packet . data = NULL ; <nl> while ( 1 ) { <nl> if (! packet0 . data ) {
static int rtp_write_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> const uint8_t * mb_info = <nl> av_packet_get_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , <nl> & mb_info_size ); <nl> + if (! mb_info ) { <nl> + av_log ( s1 , AV_LOG_ERROR , " failed to allocate side data \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> ff_rtp_send_h263_rfc2190 ( s1 , pkt -> data , size , mb_info , mb_info_size ); <nl> break ; <nl> }
static void scale_coefs ( <nl> int dynrng , <nl> int len ) <nl> { <nl> - int i , shift , round ; <nl> - unsigned mul ; <nl> + int i , shift ; <nl> + unsigned mul , round ; <nl> int temp , temp1 , temp2 , temp3 , temp4 , temp5 , temp6 , temp7 ; <nl>  <nl> mul = ( dynrng & 0x1f ) + 0x20 ;
static av_cold void uninit ( AVFilterContext * ctx ) <nl> FrameRateContext * s = ctx -> priv ; <nl> int i ; <nl>  <nl> - for ( i = s -> frst + 1 ; i < s -> last ; i ++) { <nl> + for ( i = s -> frst ; i < s -> last ; i ++) { <nl> if ( s -> srce [ i ] && ( s -> srce [ i ] != s -> srce [ i + 1 ])) <nl> av_frame_free (& s -> srce [ i ]); <nl> }
static inline int dirac_get_arith_uint ( DiracArith * c , int follow_ctx , int data_c <nl> { <nl> int ret = 1 ; <nl> while (! dirac_get_arith_bit ( c , follow_ctx )) { <nl> + if ( ret >= 0x40000000 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " dirac_get_arith_uint overflow \ n "); <nl> + return - 1 ; <nl> + } <nl> ret <<= 1 ; <nl> ret += dirac_get_arith_bit ( c , data_ctx ); <nl> follow_ctx = ff_dirac_next_ctx [ follow_ctx ];
static int crypto_open ( URLContext * h , const char * uri , int flags ) <nl>  <nl> return 0 ; <nl> err : <nl> - av_free ( c -> key ); <nl> - av_free ( c -> iv ); <nl> + av_freep (& c -> key ); <nl> + av_freep (& c -> iv ); <nl> return ret ; <nl> } <nl> 
# include " libavutil / pixdesc . h " <nl> # include " avcodec . h " <nl> # include " internal . h " <nl> + <nl> +# if defined ( _MSC_VER ) <nl> +# define X264_API_IMPORTS 1 <nl> +# endif <nl> + <nl> # include < x264 . h > <nl> # include < float . h > <nl> # include < math . h >
static int roq_read_packet ( AVFormatContext * s , <nl> pkt -> pos = avio_tell ( pb ); <nl> ret = avio_read ( pb , pkt -> data + RoQ_CHUNK_PREAMBLE_SIZE , <nl> chunk_size ); <nl> - if ( ret != chunk_size ) <nl> + if ( ret != chunk_size ) { <nl> + av_packet_unref ( pkt ); <nl> ret = AVERROR ( EIO ); <nl> + } <nl>  <nl> packet_read = 1 ; <nl> break ;
AVCodec ff_mjpeg_encoder = { <nl> . type = AVMEDIA_TYPE_VIDEO , <nl> . id = AV_CODEC_ID_MJPEG , <nl> . priv_data_size = sizeof ( MpegEncContext ), <nl> - . priv_class = & mjpeg_class , <nl> . init = ff_mpv_encode_init , <nl> . encode2 = ff_mpv_encode_picture , <nl> . close = ff_mpv_encode_end ,
static int udp_close ( URLContext * h ) <nl> ret = pthread_join ( s -> circular_buffer_thread , NULL ); <nl> if ( ret != 0 ) <nl> av_log ( h , AV_LOG_ERROR , " pthread_join (): % s \ n ", strerror ( ret )); <nl> + pthread_mutex_destroy (& s -> mutex ); <nl> + pthread_cond_destroy (& s -> cond ); <nl> } <nl> - <nl> - pthread_mutex_destroy (& s -> mutex ); <nl> - pthread_cond_destroy (& s -> cond ); <nl> # endif <nl> av_fifo_free ( s -> fifo ); <nl> return 0 ;
void ff_h264_free_tables ( H264Context * h , int free_rbsp ) <nl> if ( free_rbsp && h -> DPB ) { <nl> for ( i = 0 ; i < H264_MAX_PICTURE_COUNT ; i ++) <nl> ff_h264_unref_picture ( h , & h -> DPB [ i ]); <nl> + memset ( h -> delayed_pic , 0 , sizeof ( h -> delayed_pic )); <nl> av_freep (& h -> DPB ); <nl> } else if ( h -> DPB ) { <nl> for ( i = 0 ; i < H264_MAX_PICTURE_COUNT ; i ++)
static int bmp_decode_frame ( AVCodecContext * avctx , <nl>  <nl> hsize = bytestream_get_le32 (& buf ); /* header size */ <nl> ihsize = bytestream_get_le32 (& buf ); /* more header size */ <nl> - if ( ihsize + 14 > hsize ) { <nl> + if ( ihsize + 14LL > hsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid header size % u \ n ", hsize ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int config_input ( AVFilterLink * inlink ) <nl> const AVPixFmtDescriptor * desc = av_pix_fmt_desc_get ( inlink -> format ); <nl> int i ; <nl>  <nl> + uninit ( inlink -> dst ); <nl> + <nl> s -> hsub = desc -> log2_chroma_w ; <nl> s -> vsub = desc -> log2_chroma_h ; <nl> s -> depth = desc -> comp [ 0 ]. depth_minus1 + 1 ;
int avcodec_copy_context ( AVCodecContext * dest , const AVCodecContext * src ) <nl> av_opt_free ( dest ); <nl>  <nl> memcpy ( dest , src , sizeof (* dest )); <nl> + av_opt_copy ( dest , src ); <nl>  <nl> dest -> priv_data = orig_priv_data ; <nl> 
static int handle_metadata ( RTMPContext * rt , RTMPPacket * pkt ) <nl> pts = cts ; <nl> ts += cts - pts ; <nl> pts = cts ; <nl> + if ( size + 3 + 4 > pkt -> data + pkt -> size - next ) <nl> + break ; <nl> bytestream_put_byte (& p , type ); <nl> bytestream_put_be24 (& p , size ); <nl> bytestream_put_be24 (& p , ts );
static int dirac_unpack_prediction_parameters ( DiracContext * s ) <nl> s -> globalmc [ ref ]. perspective [ 0 ] = dirac_get_se_golomb ( gb ); <nl> s -> globalmc [ ref ]. perspective [ 1 ] = dirac_get_se_golomb ( gb ); <nl> } <nl> + if ( s -> globalmc [ ref ]. perspective_exp + ( uint64_t ) s -> globalmc [ ref ]. zrs_exp > 30 ) { <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> } <nl> } <nl> 
int main ( int argc , char ** argv ) <nl>  <nl> if (! print_format ) <nl> print_format = av_strdup (" default "); <nl> + if (! print_format ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> w_name = av_strtok ( print_format , "=", & buf ); <nl> w_args = buf ; <nl> 
static int alloc_sequence_buffers ( DiracContext * s ) <nl> s -> mctmp = av_malloc (( w + 64 + MAX_BLOCKSIZE ) * ( h * MAX_BLOCKSIZE ) * sizeof (* s -> mctmp )); <nl> s -> mcscratch = av_malloc (( w + 64 )* MAX_BLOCKSIZE ); <nl>  <nl> - if (! s -> sbsplit || ! s -> blmotion ) <nl> + if (! s -> sbsplit || ! s -> blmotion || ! s -> mctmp || ! s -> mcscratch ) <nl> return AVERROR ( ENOMEM ); <nl> return 0 ; <nl> }
av_cold int ff_MPV_encode_init ( AVCodecContext * avctx ) <nl> av_log ( avctx , AV_LOG_ERROR , " b frames not supported by codec \ n "); <nl> return - 1 ; <nl> } <nl> + if ( s -> max_b_frames < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " max b frames must be 0 or postive for mpegvideo based encoders \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> if (( s -> codec_id == AV_CODEC_ID_MPEG4 || <nl> s -> codec_id == AV_CODEC_ID_H263 ||
static int vp3_update_thread_context ( AVCodecContext * dst , const AVCodecContext * <nl> } <nl>  <nl> if ( s != s1 ) { <nl> + if (! s -> current_frame . f ) <nl> + return AVERROR ( ENOMEM ); <nl> // init tables if the first frame hasn ' t been decoded <nl> if (! s -> current_frame . f -> data [ 0 ]) { <nl> int y_fragment_count , c_fragment_count ;
static int mpegts_read_packet ( AVFormatContext * s , <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> + av_free_packet ( ts -> pkt ); <nl> /* flush pes data left */ <nl> for ( i = 0 ; i < NB_PID_MAX ; i ++) { <nl> if ( ts -> pids [ i ] && ts -> pids [ i ]-> type == MPEGTS_PES ) {
static int vp3_decode_init ( AVCodecContext * avctx ) <nl> & superblock_run_length_vlc_table [ 0 ][ 1 ], 4 , 2 , <nl> & superblock_run_length_vlc_table [ 0 ][ 0 ], 4 , 2 , 0 ); <nl>  <nl> - init_vlc (& s -> fragment_run_length_vlc , 5 , 31 , <nl> + init_vlc (& s -> fragment_run_length_vlc , 5 , 30 , <nl> & fragment_run_length_vlc_table [ 0 ][ 1 ], 4 , 2 , <nl> & fragment_run_length_vlc_table [ 0 ][ 0 ], 4 , 2 , 0 ); <nl> 
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int i ; <nl>  <nl> + x265_picture_init ( ctx -> params , & x265pic ); <nl> + <nl> if ( pic ) { <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> x265pic . planes [ i ] = pic -> data [ i ];
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> sfb_offsets [ i ][ band - 1 ] = subframe_len ; <nl> s -> num_sfb [ i ] = band - 1 ; <nl> + if ( s -> num_sfb [ i ] <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " num_sfb invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> 
static int vqf_read_header ( AVFormatContext * s ) <nl> rate_flag = AV_RB32 ( comm_chunk + 8 ); <nl> avio_skip ( s -> pb , len - 12 ); <nl>  <nl> + if ( st -> codec -> channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> st -> codec -> bit_rate = read_bitrate * 1000 ; <nl> break ; <nl> case MKTAG (' D ',' S ',' I ',' Z '): // size of compressed data
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> void * tmp_ptr ; <nl> s -> max_framesize = 8192 ; // should hopefully be enough for the first header <nl> tmp_ptr = av_fast_realloc ( s -> bitstream , & s -> allocated_bitstream_size , <nl> - s -> max_framesize ); <nl> + s -> max_framesize + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! tmp_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM );
int ff_rtsp_open_transport_ctx ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl>  <nl> if (! rtsp_st -> transport_priv ) { <nl> return AVERROR ( ENOMEM ); <nl> - } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP ) { <nl> + } else if ( CONFIG_RTPDEC && rt -> transport == RTSP_TRANSPORT_RTP && <nl> + s -> iformat ) { <nl> RTPDemuxContext * rtpctx = rtsp_st -> transport_priv ; <nl> rtpctx -> ssrc = rtsp_st -> ssrc ; <nl> if ( rtsp_st -> dynamic_handler ) {
static void compute_chapters_end ( AVFormatContext * s ) <nl> if ( j != i && next_start > ch -> start && next_start < end ) <nl> end = next_start ; <nl> } <nl> - ch -> end = ( end == INT64_MAX ) ? ch -> start : end ; <nl> + ch -> end = ( end == INT64_MAX || end < ch -> start ) ? ch -> start : end ; <nl> } <nl> } <nl> 
static void apply_dependent_coupling_fixed ( AACContext * ac , <nl> for ( k = offsets [ i ]; k < offsets [ i + 1 ]; k ++) { <nl> tmp = ( int )((( int64_t ) src [ group * 128 + k ] * c + \ <nl> ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ group * 128 + k ] += tmp * ( 1 << shift ); <nl> + dest [ group * 128 + k ] += tmp * ( 1U << shift ); <nl> } <nl> } <nl> }
static void update_stream_timings ( AVFormatContext * ic ) <nl> st = ic -> streams [ i ]; <nl> if ( st -> start_time != AV_NOPTS_VALUE && st -> time_base . den ) { <nl> start_time1 = av_rescale_q ( st -> start_time , st -> time_base , AV_TIME_BASE_Q ); <nl> - if ( st -> codec -> codec_id == CODEC_ID_DVB_TELETEXT ) { <nl> + if ( st -> codec -> codec_id == CODEC_ID_DVB_TELETEXT || st -> codec -> codec_type == AVMEDIA_TYPE_SUBTITLE ) { <nl> if ( start_time1 < start_time_text ) <nl> start_time_text = start_time1 ; <nl> } else
void ff_frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> av_freep (& p -> avctx -> slice_offset ); <nl> } <nl>  <nl> - if ( p -> avctx ) <nl> + if ( p -> avctx ) { <nl> av_freep (& p -> avctx -> internal ); <nl> + av_buffer_unref (& p -> avctx -> hw_frames_ctx ); <nl> + } <nl> + <nl> av_freep (& p -> avctx ); <nl> } <nl> 
ogg_get_length ( AVFormatContext * s ) <nl> url_fseek (& s -> pb , end , SEEK_SET ); <nl>  <nl> while (! ogg_read_page ( s , & i )){ <nl> - if ( ogg -> streams [ i ]. granule != - 1 && ogg -> streams [ i ]. granule != 0 ) <nl> + if ( ogg -> streams [ i ]. granule != - 1 && ogg -> streams [ i ]. granule != 0 && <nl> + ogg -> streams [ i ]. codec ) <nl> idx = i ; <nl> } <nl> 
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * buf ) <nl>  <nl> if ( labs ( delta ) > s -> min_delta ) { <nl> av_log ( ctx , AV_LOG_VERBOSE , " Discontinuity - %" PRId64 " samples .\ n ", delta ); <nl> - out_size += delta ; <nl> + out_size = av_clipl_int32 (( int64_t ) out_size + delta ); <nl> } else { <nl> if ( s -> resample ) { <nl> int comp = av_clip ( delta , - s -> max_comp , s -> max_comp );
static int mp2_write_trailer ( struct AVFormatContext * s ) <nl> return 0 ; <nl> } <nl>  <nl> - static int query_codec ( enum CodecID id , int std_compliance ) <nl> + static int query_codec ( enum AVCodecID id , int std_compliance ) <nl> { <nl> CodecMime * cm = ff_id3v2_mime_tags ; <nl> - while ( cm -> id != CODEC_ID_NONE ) { <nl> + while ( cm -> id != AV_CODEC_ID_NONE ) { <nl> if ( id == cm -> id ) <nl> return MKTAG (' A ', ' P ', ' I ', ' C '); <nl> cm ++;
static void matroska_add_index_entries ( MatroskaDemuxContext * matroska ) <nl> { <nl> EbmlList * index_list ; <nl> MatroskaIndex * index ; <nl> - int index_scale = 1 ; <nl> + uint64_t index_scale = 1 ; <nl> int i , j ; <nl>  <nl> if ( matroska -> ctx -> flags & AVFMT_FLAG_IGNIDX )
static int compute_bit_allocation ( AC3EncodeContext * s ) <nl> */ <nl> static inline int sym_quant ( int c , int e , int levels ) <nl> { <nl> - int v = (((( levels * c ) >> ( 24 - e )) + 1 ) >> 1 ) + ( levels >> 1 ); <nl> + int v = ((( levels * c ) >> ( 24 - e )) + levels ) >> 1 ; <nl> av_assert2 ( v >= 0 && v < levels ); <nl> return v ; <nl> }
int ff_get_wav_header ( AVIOContext * pb , AVCodecContext * codec , int size ) <nl> codec -> sample_rate = 0 ; <nl> } <nl> /* override bits_per_coded_sample for G . 726 */ <nl> - if ( codec -> codec_id == AV_CODEC_ID_ADPCM_G726 ) <nl> + if ( codec -> codec_id == AV_CODEC_ID_ADPCM_G726 && codec -> sample_rate ) <nl> codec -> bits_per_coded_sample = codec -> bit_rate / codec -> sample_rate ; <nl>  <nl> return 0 ;
void ff_rtsp_close_streams ( AVFormatContext * s ) <nl> if ( rtsp_st -> dynamic_handler && rtsp_st -> dynamic_protocol_context ) <nl> rtsp_st -> dynamic_handler -> close ( <nl> rtsp_st -> dynamic_protocol_context ); <nl> + av_free ( rtsp_st ); <nl> } <nl> } <nl> av_free ( rt -> rtsp_streams );
static void ipvideo_decode_opcodes ( IpvideoContext * s , AVFrame * frame ) <nl> init_get_bits (& gb , s -> decoding_map , s -> decoding_map_size * 8 ); <nl> for ( y = 0 ; y < s -> avctx -> height ; y += 8 ) { <nl> for ( x = 0 ; x < s -> avctx -> width ; x += 8 ) { <nl> + if ( get_bits_left (& gb ) < 4 ) <nl> + return ; <nl> opcode = get_bits (& gb , 4 ); <nl>  <nl> ff_tlog ( s -> avctx ,
static int yop_read_header ( AVFormatContext * s ) <nl>  <nl> audio_stream = avformat_new_stream ( s , NULL ); <nl> video_stream = avformat_new_stream ( s , NULL ); <nl> + if (! audio_stream || ! video_stream ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> // Extra data that will be passed to the decoder <nl> video_stream -> codec -> extradata_size = 8 ;
static int parse_pixel_format ( AVCodecContext * avctx ) <nl> normal_map = flags & DDPF_NORMALMAP ; <nl> fourcc = bytestream2_get_le32 ( gbc ); <nl>  <nl> + if ( ctx -> compressed && ctx -> paletted ) { <nl> + av_log ( avctx , AV_LOG_WARNING , <nl> + " Disabling invalid palette flag for compressed dds .\ n "); <nl> + ctx -> paletted = 0 ; <nl> + } <nl> + <nl> bpp = bytestream2_get_le32 ( gbc ); // rgbbitcount <nl> r = bytestream2_get_le32 ( gbc ); // rbitmask <nl> g = bytestream2_get_le32 ( gbc ); // gbitmask
static OSStatus ffat_decode_callback ( AudioConverterRef converter , UInt32 * nb_pac <nl> return 0 ; <nl> } <nl>  <nl> + av_packet_unref (& at -> in_pkt ); <nl> av_packet_move_ref (& at -> in_pkt , & at -> new_in_pkt ); <nl> at -> new_in_pkt . data = 0 ; <nl> at -> new_in_pkt . size = 0 ;
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + if ( FAILED ( hr )) { <nl> + IDirect3D9Ex_Release ( d3d9ex ); <nl> + return AVERROR_UNKNOWN ; <nl> + } <nl>  <nl> d3dpp . BackBufferFormat = modeex . Format ; <nl> 
static int rm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> flags = ( seq ++ == 1 ) ? 2 : 0 ; <nl> } else { <nl> len = sync ( s , & timestamp , & flags , & i , & pos ); <nl> - st = s -> streams [ i ]; <nl> + if ( len > 0 ) <nl> + st = s -> streams [ i ]; <nl> } <nl>  <nl> if ( len < 0 || url_feof ( s -> pb ))
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + nals_needed = nal_index ; <nl> + break ; <nl> case NAL_IDR_SLICE : <nl> case NAL_SLICE : <nl> - nals_needed = nal_index ; <nl> + init_get_bits (& hx -> s . gb , ptr , bit_length ); <nl> + if (! get_ue_golomb (& hx -> s . gb )) <nl> + nals_needed = nal_index ; <nl> } <nl> continue ; <nl> }
static int mov_codec_id ( AVStream * st , uint32_t format ) <nl> static void mov_parse_stsd_video ( MOVContext * c , AVIOContext * pb , <nl> AVStream * st , MOVStreamContext * sc ) <nl> { <nl> - uint8_t codec_name [ 32 ]; <nl> + uint8_t codec_name [ 32 ] = { 0 }; <nl> int64_t stsd_start ; <nl> unsigned int len ; <nl> 
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_CREATE_TIMER : <nl> av_dlog ( NULL , " create timer \ n "); <nl> - if (( opcode_version > 0 ) || ( opcode_size > 6 )) { <nl> + if (( opcode_version > 0 ) || ( opcode_size != 6 )) { <nl> av_dlog ( NULL , " bad create_timer opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static av_cold int hevc_decode_free ( AVCodecContext * avctx ) <nl>  <nl> pic_arrays_free ( s ); <nl>  <nl> - av_freep (& lc -> edge_emu_buffer ); <nl> + if ( lc ) <nl> + av_freep (& lc -> edge_emu_buffer ); <nl> av_freep (& s -> md5_ctx ); <nl>  <nl> for ( i = 0 ; i < s -> nals_allocated ; i ++) {
static av_cold void dsputil_init_sse2 ( DSPContext * c , AVCodecContext * avctx , <nl> # if HAVE_SSE2_INLINE <nl> const int high_bit_depth = avctx -> bits_per_raw_sample > 8 ; <nl>  <nl> - if (! high_bit_depth && avctx -> idct_algo == FF_IDCT_XVIDMMX ) { <nl> + if (! high_bit_depth && avctx -> idct_algo == FF_IDCT_XVIDMMX && avctx -> lowres == 0 ) { <nl> c -> idct_put = ff_idct_xvid_sse2_put ; <nl> c -> idct_add = ff_idct_xvid_sse2_add ; <nl> c -> idct = ff_idct_xvid_sse2 ;
static const AVCodecDescriptor codec_descriptors [] = { <nl> . type = AVMEDIA_TYPE_VIDEO , <nl> . name = " fraps ", <nl> . long_name = NULL_IF_CONFIG_SMALL (" Fraps "), <nl> - . props = AV_CODEC_PROP_LOSSLESS , <nl> + . props = AV_CODEC_PROP_INTRA_ONLY | AV_CODEC_PROP_LOSSLESS , <nl> }, <nl> { <nl> . id = AV_CODEC_ID_TRUEMOTION2 ,
retry_duration : <nl> flv -> last_channels = <nl> channels = st -> codec -> channels ; <nl> } else { <nl> - AVCodecContext ctx ; <nl> + AVCodecContext ctx = { 0 }; <nl> ctx . sample_rate = sample_rate ; <nl> flv_set_audio_codec ( s , st , & ctx , flags & FLV_AUDIO_CODECID_MASK ); <nl> sample_rate = ctx . sample_rate ;
int av_set_string3 ( void * obj , const char * name , const char * val , int alloc , cons <nl>  <nl> int av_opt_set ( void * obj , const char * name , const char * val , int search_flags ) <nl> { <nl> - int ret ; <nl> + int ret = 0 ; <nl> void * dst , * target_obj ; <nl> const AVOption * o = av_opt_find2 ( obj , name , NULL , 0 , search_flags , & target_obj ); <nl> if (! o || ! target_obj )
static int rv30_decode_mb_info ( RV34DecContext * r ) <nl> GetBitContext * gb = & s -> gb ; <nl> int code = svq3_get_ue_golomb ( gb ); <nl>  <nl> - if ( code > 11 ){ <nl> + if ( code < 0 || code > 11 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Incorrect MB type code \ n "); <nl> return - 1 ; <nl> }
static int decode_residual_block ( AVSContext * h , GetBitContext * gb , <nl> const dec_2dvlc_t * r , int esc_golomb_order , <nl> int qp , uint8_t * dst , int stride ) { <nl> int i , level_code , esc_code , level , run , mask ; <nl> - DCTELEM level_buf [ 64 ]; <nl> - uint8_t run_buf [ 64 ]; <nl> + DCTELEM level_buf [ 65 ]; <nl> + uint8_t run_buf [ 65 ]; <nl> DCTELEM * block = h -> block ; <nl>  <nl> for ( i = 0 ; i < 65 ; i ++) {
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> } <nl> } <nl>  <nl> + if ( s -> pb -> eof_reached ) <nl> + return AVERROR_EOF ; <nl> + <nl> return AVERROR ( EIO ); <nl> } <nl> 
int main ( int argc , char ** argv ){ <nl>  <nl> selfTest ( src , stride , W , H ); <nl>  <nl> - return 123 ; <nl> + return 0 ; <nl> }
static int mpeg_decode_mb ( MpegEncContext * s , int16_t block [ 12 ][ 64 ]) <nl>  <nl> cbp = get_vlc2 (& s -> gb , ff_mb_pat_vlc . table , MB_PAT_VLC_BITS , 1 ); <nl> if ( mb_block_count > 6 ) { <nl> - cbp <<= mb_block_count - 6 ; <nl> + cbp *= 1 << mb_block_count - 6 ; <nl> cbp |= get_bits (& s -> gb , mb_block_count - 6 ); <nl> s -> bdsp . clear_blocks ( s -> block [ 6 ]); <nl> }
static int vobsub_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> FFDemuxSubtitlesQueue * q ; <nl> AVIOContext * pb = vobsub -> sub_ctx -> pb ; <nl> int ret , psize , total_read = 0 , i ; <nl> - AVPacket idx_pkt ; <nl> + AVPacket idx_pkt = { 0 }; <nl>  <nl> int64_t min_ts = INT64_MAX ; <nl> int sid = 0 ;
static const AVProfile mpeg2_video_profiles [] = { <nl> { FF_PROFILE_MPEG2_SIMPLE , " Simple " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> + { FF_PROFILE_UNKNOWN }, <nl> }; <nl>  <nl> 
typedef struct PanContext { <nl> static int parse_channel_name ( char ** arg , int * rchannel , int * rnamed ) <nl> { <nl> char buf [ 8 ]; <nl> - int len , i , channel_id ; <nl> + int len , i , channel_id = 0 ; <nl> int64_t layout , layout0 ; <nl>  <nl> if ( sscanf (* arg , " % 7 [ A - Z ] % n ", buf , & len )) {
static av_cold void uninit ( AVFilterContext * ctx ) <nl>  <nl> av_freep (& s -> frame_data . input ); <nl> av_freep (& s -> frame_data . temp ); <nl> + av_freep (& s -> fdsp ); <nl> av_frame_free (& s -> second ); <nl> } <nl> 
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> break ; <nl> } <nl>  <nl> - if ( last ){ <nl> + if ( last && s -> current_picture_ptr ){ <nl> if ( r -> loop_filter ) <nl> r -> loop_filter ( r , s -> mb_height - 1 ); <nl> ff_er_frame_end ( s );
static int rtp_parse_packet_internal ( RTPDemuxContext * s , AVPacket * pkt , <nl> if ( ret < 0 ) <nl> return AVERROR ( EAGAIN ); <nl> if ( ret < len ) { <nl> - s -> read_buf_size = len - ret ; <nl> + s -> read_buf_size = FFMIN ( len - ret , sizeof ( s -> buf )); <nl> memcpy ( s -> buf , buf + ret , s -> read_buf_size ); <nl> s -> read_buf_index = 0 ; <nl> return 1 ;
enum dirac_subband { <nl> /* magic number division by 3 from schroedinger */ <nl> static inline int divide3 ( int x ) <nl> { <nl> - return (( x + 1 )* 21845 + 10922 ) >> 16 ; <nl> + return ( int )(( x + 1U )* 21845 + 10922 ) >> 16 ; <nl> } <nl>  <nl> static DiracFrame * remove_frame ( DiracFrame * framelist [], int picnum )
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> else { <nl> switch ( type ) { <nl> case TIFF_BYTE : <nl> - s -> bpp = ( off & 0xFF ) + (( off >> 8 ) & 0xFF ) + <nl> - (( off >> 16 ) & 0xFF ) + (( off >> 24 ) & 0xFF ); <nl> - break ; <nl> case TIFF_SHORT : <nl> case TIFF_LONG : <nl> s -> bpp = 0 ;
static int r3d_read_reda ( AVFormatContext * s , AVPacket * pkt , Atom * atom ) <nl> dts = avio_rb32 ( s -> pb ); <nl>  <nl> st -> codec -> sample_rate = avio_rb32 ( s -> pb ); <nl> + if ( st -> codec -> sample_rate < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " negative sample rate \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> samples = avio_rb32 ( s -> pb ); <nl> 
static av_cold int pcm_encode_init ( AVCodecContext * avctx ) <nl>  <nl> avctx -> bits_per_coded_sample = av_get_bits_per_sample ( avctx -> codec -> id ); <nl> avctx -> block_align = avctx -> channels * avctx -> bits_per_coded_sample / 8 ; <nl> - avctx -> bit_rate = avctx -> block_align * avctx -> sample_rate * 8 ; <nl> + avctx -> bit_rate = avctx -> block_align * 8LL * avctx -> sample_rate ; <nl>  <nl> return 0 ; <nl> }
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> return 0 ; <nl> } <nl>  <nl> + if ( vstream -> nb_index_entries > 0 ){ <nl> + av_log ( s , AV_LOG_WARNING , " Skiping duplicate index \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> while ( avio_tell ( ioc ) < max_pos - 2 && amf_get_string ( ioc , str_val , sizeof ( str_val )) > 0 ) { <nl> int64_t ** current_array ; <nl> unsigned int arraylen ;
static int read_access_unit ( AVCodecContext * avctx , void * data , <nl> substr_header_size += 2 ; <nl> } <nl>  <nl> + if ( length < header_size + substr_header_size ) { <nl> + av_log ( m -> avctx , AV_LOG_ERROR , " Insuffient data for headers \ n "); <nl> + goto error ; <nl> + } <nl> + <nl> if (!( nonrestart_substr ^ m -> is_major_sync_unit )) { <nl> av_log ( m -> avctx , AV_LOG_ERROR , " Invalid nonrestart_substr .\ n "); <nl> goto error ;
void avcodec_align_dimensions2 ( AVCodecContext * s , int * width , int * height , <nl> case AV_PIX_FMT_YUVJ411P : <nl> case AV_PIX_FMT_UYYVYY411 : <nl> w_align = 32 ; <nl> - h_align = 8 ; <nl> + h_align = 16 * 2 ; <nl> break ; <nl> case AV_PIX_FMT_YUV410P : <nl> if ( s -> codec_id == AV_CODEC_ID_SVQ1 ) {
int ff_xvid_rate_control_init ( MpegEncContext * s ){ <nl>  <nl> if ( write ( fd , tmp , strlen ( tmp )) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error % s writing 2pass logfile \ n ", strerror ( errno )); <nl> + av_free ( tmp_name ); <nl> + close ( fd ); <nl> return AVERROR ( errno ); <nl> } <nl> }
enum AVPixelFormat avcodec_find_best_pix_fmt_of_list ( const enum AVPixelFormat * p <nl> int loss ; <nl>  <nl> for ( i = 0 ; pix_fmt_list [ i ] != AV_PIX_FMT_NONE ; i ++) { <nl> - loss = * loss_ptr ; <nl> + loss = loss_ptr ? * loss_ptr : 0 ; <nl> best = avcodec_find_best_pix_fmt_of_2 ( best , pix_fmt_list [ i ], src_pix_fmt , has_alpha , & loss ); <nl> } <nl>  <nl> - * loss_ptr = loss ; <nl> + if ( loss_ptr ) <nl> + * loss_ptr = loss ; <nl> return best ; <nl> } <nl> 
static int64_t mxf_essence_container_end ( MXFContext * mxf , int body_sid ) <nl> static int mxf_edit_unit_absolute_offset ( MXFContext * mxf , MXFIndexTable * index_table , int64_t edit_unit , int64_t * edit_unit_out , int64_t * offset_out , int nag ) <nl> { <nl> int i ; <nl> - int offset_temp = 0 ; <nl> + int64_t offset_temp = 0 ; <nl>  <nl> for ( i = 0 ; i < index_table -> nb_segments ; i ++) { <nl> MXFIndexTableSegment * s = index_table -> segments [ i ];
int ff_lpc_calc_coefs ( LPCContext * s , <nl> LLSModel m [ 2 ]; <nl> double var [ MAX_LPC_ORDER + 1 ], av_uninit ( weight ); <nl>  <nl> + if ( lpc_passes <= 0 ) <nl> + lpc_passes = 2 ; <nl> + <nl> for ( pass = 0 ; pass < lpc_passes ; pass ++){ <nl> av_init_lls (& m [ pass & 1 ], max_order ); <nl> 
static inline int read_line ( AVFormatContext * s , char * rbuf , const int rbufsize , <nl>  <nl> do { <nl> ret = ffurl_read_complete ( rt -> rtsp_hd , rbuf + idx , 1 ); <nl> - if ( ret < 0 ) <nl> - return ret ; <nl> + if ( ret <= 0 ) <nl> + return ret ? ret : AVERROR_EOF ; <nl> if ( rbuf [ idx ] == '\ r ') { <nl> /* Ignore */ <nl> } else if ( rbuf [ idx ] == '\ n ') {
static av_cold int vqa_decode_init ( AVCodecContext * avctx ) <nl> /* allocate decode buffer */ <nl> s -> decode_buffer_size = ( s -> width / s -> vector_width ) * <nl> ( s -> height / s -> vector_height ) * 2 ; <nl> - s -> decode_buffer = av_malloc ( s -> decode_buffer_size ); <nl> + s -> decode_buffer = av_mallocz ( s -> decode_buffer_size ); <nl> if (! s -> decode_buffer ) <nl> goto fail ; <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> AVFrame * const p = & s -> picture ; <nl> uint8_t * ptr ; <nl>  <nl> - int magic_num , offset , endian ; <nl> + unsigned int offset ; <nl> + int magic_num , endian ; <nl> int x , y ; <nl> int w , h , stride , bits_per_color , descriptor , elements , target_packet_size , source_packet_size ; <nl> 
void avcodec_register_all ( void ) <nl> REGISTER_ENCDEC ( XSUB , xsub ); <nl>  <nl> /* external libraries */ <nl> - REGISTER_ENCODER ( LIBAACPLUS , libaacplus ); <nl> REGISTER_DECODER ( LIBCELT , libcelt ); <nl> REGISTER_DECODER ( LIBDIRAC , libdirac ); <nl> REGISTER_ENCODER ( LIBFAAC , libfaac ); <nl> void avcodec_register_all ( void ) <nl> REGISTER_ENCODER ( LIBX264RGB , libx264rgb ); <nl> REGISTER_ENCODER ( LIBXAVS , libxavs ); <nl> REGISTER_ENCODER ( LIBXVID , libxvid ); <nl> + REGISTER_ENCODER ( LIBAACPLUS , libaacplus ); <nl>  <nl> /* text */ <nl> REGISTER_DECODER ( BINTEXT , bintext );
static int tiff_decode_tag ( TiffContext * s ) <nl> " Samples per pixel requires a single value , many provided \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( value > 4U ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , <nl> + " Samples per pixel % d is too large \ n ", value ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( s -> bppcount == 1 ) <nl> s -> bpp *= value ; <nl> s -> bppcount = value ;
static int process_output_surface ( AVCodecContext * avctx , AVPacket * pkt , NvencSur <nl> NV_ENCODE_API_FUNCTION_LIST * p_nvenc = & dl_fn -> nvenc_funcs ; <nl>  <nl> uint32_t slice_mode_data ; <nl> - uint32_t * slice_offsets ; <nl> + uint32_t * slice_offsets = NULL ; <nl> NV_ENC_LOCK_BITSTREAM lock_params = { 0 }; <nl> NVENCSTATUS nv_status ; <nl> int res = 0 ;
static int tak_read_header ( AVFormatContext * s ) <nl> buffer = av_malloc ( size - 3 + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! buffer ) <nl> return AVERROR ( ENOMEM ); <nl> + memset ( buffer + size - 3 , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> ffio_init_checksum ( pb , tak_check_crc , 0xCE04B7U ); <nl> if ( avio_read ( pb , buffer , size - 3 ) != size - 3 ) {
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> if ( entries >= UINT_MAX / sizeof (* sc -> drefs )) <nl> return AVERROR_INVALIDDATA ; <nl> av_free ( sc -> drefs ); <nl> + sc -> drefs_count = 0 ; <nl> sc -> drefs = av_mallocz ( entries * sizeof (* sc -> drefs )); <nl> if (! sc -> drefs ) <nl> return AVERROR ( ENOMEM );
static int ea_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( ea -> audio_codec ) { <nl> - if ( ea -> num_channels <= 0 ) { <nl> + if ( ea -> num_channels <= 0 || ea -> num_channels > 2 ) { <nl> av_log ( s , AV_LOG_WARNING , <nl> " Unsupported number of channels : % d \ n ", ea -> num_channels ); <nl> ea -> audio_codec = 0 ;
static int decode_frame ( AVCodecContext * avctx , <nl> int prev_y = 0 , prev_u = 0 , prev_v = 0 ; <nl> uint8_t * rbuf ; <nl>  <nl> - if ( buf_size <= 8 ) { <nl> + if ( buf_size < 8 + avctx -> height * ( avctx -> width / 2 )/ 8 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Packet size % d is too small \ n ", buf_size ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int txd_read_header ( AVFormatContext * s , AVFormatParameters * ap ) { <nl> st -> codec -> time_base . den = 5 ; <nl> st -> codec -> time_base . num = 1 ; <nl> /* the parameters will be extracted from the compressed bitstream */ <nl> + <nl> + s -> pb -> maxsize = avio_size ( s -> pb ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int http_open_cnx ( URLContext * h ) <nl> { <nl> const char * path , * proxy_path , * lower_proto = " tcp ", * local_path ; <nl> char hostname [ 1024 ], hoststr [ 1024 ], proto [ 10 ]; <nl> - char auth [ 1024 ], proxyauth [ 1024 ]; <nl> + char auth [ 1024 ], proxyauth [ 1024 ] = ""; <nl> char path1 [ 1024 ]; <nl> char buf [ 1024 ], urlbuf [ 1024 ]; <nl> int port , use_proxy , err , location_changed = 0 , redirects = 0 ;
static int vorbis_parse_audio_packet ( vorbis_context * vc ) <nl> ch_left -= ch ; <nl> } <nl>  <nl> + if ( ch_left > 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> // Inverse coupling <nl>  <nl> for ( i = mapping -> coupling_steps - 1 ; i >= 0 ; -- i ) { // warning : i has to be signed
static int bit_allocation ( IMCContext * q , IMCChannel * chctx , <nl> iacc += chctx -> bandWidthT [ i ]; <nl> summa += chctx -> bandWidthT [ i ] * chctx -> flcoeffs4 [ i ]; <nl> } <nl> + <nl> + if (! iacc ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> chctx -> bandWidthT [ BANDS - 1 ] = 0 ; <nl> summa = ( summa * 0 . 5 - freebits ) / iacc ; <nl> 
static av_cold int opus_decode_init ( AVCodecContext * avctx ) <nl>  <nl> /* find out the channel configuration */ <nl> ret = ff_opus_parse_extradata ( avctx , c ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& c -> channel_maps ); <nl> + av_freep (& c -> fdsp ); <nl> return ret ; <nl> + } <nl>  <nl> /* allocate and init each independent decoder */ <nl> c -> streams = av_mallocz_array ( c -> nb_streams , sizeof (* c -> streams ));
int ff_jpegls_decode_picture ( MJpegDecodeContext * s , int near , <nl> avpriv_report_missing_feature ( s -> avctx , " Sample interleaved images "); <nl> ret = AVERROR_PATCHWELCOME ; <nl> goto end ; <nl> + } else { /* unknown interleaving */ <nl> + avpriv_report_missing_feature ( s -> avctx , " Unknown interleaved images "); <nl> + ret = AVERROR_PATCHWELCOME ; <nl> + goto end ; <nl> } <nl>  <nl> if ( s -> xfrm && s -> nb_components == 3 ) {
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> static int mov_probe ( AVProbeData * p ) <nl> { <nl> - unsigned int offset ; <nl> + int64_t offset ; <nl> uint32_t tag ; <nl> int score = 0 ; <nl> 
static int resolve_content_path ( AVFormatContext * s , const char * url , int * max_ur <nl> if (!( node = baseurl_nodes [ rootId ])) { <nl> continue ; <nl> } <nl> - if ( ishttp ( xmlNodeGetContent ( node ))) { <nl> + text = xmlNodeGetContent ( node ); <nl> + if ( ishttp ( text )) { <nl> + xmlFree ( text ); <nl> break ; <nl> } <nl> + xmlFree ( text ); <nl> } <nl>  <nl> node = baseurl_nodes [ rootId ];
resync : <nl> pkt -> data , pkt -> size ); <nl> pkt -> destruct = dstr ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> + if ( size < 0 ) <nl> + av_free_packet ( pkt ); <nl> } else { <nl> /* XXX : How to handle B - frames in AVI ? */ <nl> pkt -> dts = ast -> frame_offset ;
static int cinvideo_decode_frame ( AVCodecContext * avctx , <nl> if ( bitmap_frame_size < palette_colors_count * ( 3 + ( palette_type != 0 ))) <nl> return AVERROR_INVALIDDATA ; <nl> if ( palette_type == 0 ) { <nl> + if ( palette_colors_count > 256 ) <nl> + return AVERROR_INVALIDDATA ; <nl> for ( i = 0 ; i < palette_colors_count ; ++ i ) { <nl> cin -> palette [ i ] = bytestream_get_le24 (& buf ); <nl> bitmap_frame_size -= 3 ;
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + avctx -> height * lsize ) { <nl> + if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + ( uint64_t ) avctx -> height * lsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " input buffer too small \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
int ff_vda_destroy_decoder ( struct vda_context * vda_ctx ) <nl> static int vda_h264_uninit ( AVCodecContext * avctx ) <nl> { <nl> VDAContext * vda = avctx -> internal -> hwaccel_priv_data ; <nl> - av_freep (& vda -> bitstream ); <nl> - if ( vda -> frame ) <nl> - CVPixelBufferRelease ( vda -> frame ); <nl> + if ( vda ) { <nl> + av_freep (& vda -> bitstream ); <nl> + if ( vda -> frame ) <nl> + CVPixelBufferRelease ( vda -> frame ); <nl> + } <nl> return 0 ; <nl> } <nl> 
static int ea_read_header ( AVFormatContext * s , <nl> st -> codec -> height = ea -> height ; <nl> } <nl>  <nl> + if ( ea -> num_channels <= 0 ) { <nl> + av_log ( s , AV_LOG_WARNING , " Unsupported number of channels : % d \ n ", ea -> num_channels ); <nl> + ea -> audio_codec = 0 ; <nl> + } <nl> + <nl> if ( ea -> audio_codec ) { <nl> /* initialize the audio decoder stream */ <nl> st = av_new_stream ( s , 0 );
uint8_t * av_packet_new_side_data ( AVPacket * pkt , enum AVPacketSideDataType type , <nl> if (! pkt -> side_data ) <nl> return NULL ; <nl>  <nl> - pkt -> side_data [ elems ]. data = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + pkt -> side_data [ elems ]. data = av_mallocz ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! pkt -> side_data [ elems ]. data ) <nl> return NULL ; <nl> pkt -> side_data [ elems ]. size = size ;
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> GradFunContext * gf = inlink -> dst -> priv ; <nl> AVFilterLink * outlink = inlink -> dst -> outputs [ 0 ]; <nl> AVFilterBufferRef * out ; <nl> - int p , direct ; <nl> + int p , direct = 0 ; <nl>  <nl> if (( in -> perms & AV_PERM_WRITE ) && !( in -> perms & AV_PERM_PRESERVE )) { <nl> direct = 1 ;
av_cold int ff_ac3_encode_close ( AVCodecContext * avctx ) <nl> AC3EncodeContext * s = avctx -> priv_data ; <nl>  <nl> av_freep (& s -> windowed_samples ); <nl> + if ( s -> planar_samples ) <nl> for ( ch = 0 ; ch < s -> channels ; ch ++) <nl> av_freep (& s -> planar_samples [ ch ]); <nl> av_freep (& s -> planar_samples );
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & bc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
typedef struct HEVCPPS { <nl> uint8_t chroma_qp_offset_list_enabled_flag ; <nl> uint8_t diff_cu_chroma_qp_offset_depth ; <nl> uint8_t chroma_qp_offset_list_len_minus1 ; <nl> - int8_t cb_qp_offset_list [ 5 ]; <nl> - int8_t cr_qp_offset_list [ 5 ]; <nl> + int8_t cb_qp_offset_list [ 6 ]; <nl> + int8_t cr_qp_offset_list [ 6 ]; <nl> uint8_t log2_sao_offset_scale_luma ; <nl> uint8_t log2_sao_offset_scale_chroma ; <nl> 
static int gdv_decode_frame ( AVCodecContext * avctx , void * data , <nl> case 1 : <nl> memset ( gdv -> frame + PREAMBLE_SIZE , 0 , gdv -> frame_size - PREAMBLE_SIZE ); <nl> case 0 : <nl> + if ( bytestream2_get_bytes_left ( gb ) < 256 * 3 ) <nl> + return AVERROR_INVALIDDATA ; <nl> for ( i = 0 ; i < 256 ; i ++) { <nl> unsigned r = bytestream2_get_byte ( gb ); <nl> unsigned g = bytestream2_get_byte ( gb );
char * av_strdup ( const char * s ) <nl> char * ptr = NULL ; <nl> if ( s ) { <nl> int len = strlen ( s ) + 1 ; <nl> - ptr = av_malloc ( len ); <nl> + ptr = av_realloc ( NULL , len ); <nl> if ( ptr ) <nl> memcpy ( ptr , s , len ); <nl> }
static int adx_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf_size -= BLOCK_SIZE ; <nl> buf += BLOCK_SIZE ; <nl> } <nl> - samples_offset += BLOCK_SAMPLES ; <nl> + if (! c -> eof ) <nl> + samples_offset += BLOCK_SAMPLES ; <nl> } <nl>  <nl> + frame -> nb_samples = samples_offset ; <nl> * got_frame_ptr = 1 ; <nl>  <nl> return buf - avpkt -> data ;
int ff_audio_mix ( AudioMix * am , AudioData * src ) <nl>  <nl> if ( am -> in_matrix_channels && am -> out_matrix_channels ) { <nl> uint8_t ** data ; <nl> - uint8_t * data0 [ AVRESAMPLE_MAX_CHANNELS ]; <nl> + uint8_t * data0 [ AVRESAMPLE_MAX_CHANNELS ] = { NULL }; <nl>  <nl> if ( am -> out_matrix_channels < am -> out_channels || <nl> am -> in_matrix_channels < am -> in_channels ) {
static void fill_table ( uint8_t * table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int <nl> } <nl> } <nl>  <nl> - static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int inc ) <nl> + static void fill_gv_table ( int table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , const int64_t inc ) <nl> { <nl> int i ; <nl> int off = -( inc >> 9 );
static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> av_buffer_realloc ( buf , size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> - if (! buf ) <nl> + if (!* buf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> memset ((* buf )-> data + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE );
static int mkv_write_attachments ( AVFormatContext * s ) <nl>  <nl> mkv -> attachments = av_mallocz ( sizeof (* mkv -> attachments )); <nl> if (! mkv -> attachments ) <nl> - return ret ; <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> av_lfg_init (& c , av_get_random_seed ()); <nl> 
static int decode_lowdelay ( DiracContext * s ) <nl> s -> slice_params_buf = av_realloc_f ( s -> slice_params_buf , s -> num_x * s -> num_y , sizeof ( DiracSlice )); <nl> if (! s -> slice_params_buf ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " slice params buffer allocation failure \ n "); <nl> + s -> slice_params_num_buf = 0 ; <nl> return AVERROR ( ENOMEM ); <nl> } <nl> s -> slice_params_num_buf = s -> num_x * s -> num_y ;
static int smush_read_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> case MKBETAG (' W ', ' a ', ' v ', ' e '): <nl> if ( size < 13 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( av_get_packet ( pb , pkt , size ) < 0 ) <nl> + if ( av_get_packet ( pb , pkt , size ) < 13 ) <nl> return AVERROR ( EIO ); <nl>  <nl> pkt -> stream_index = smush -> audio_stream_index ;
static int encode_thread ( AVCodecContext * c , void * arg ){ <nl> av_fast_padded_malloc (& new_buffer , & new_buffer_size , new_size ); <nl> if ( new_buffer ) { <nl> memcpy ( new_buffer , s -> avctx -> internal -> byte_buffer , s -> avctx -> internal -> byte_buffer_size ); <nl> + av_free ( s -> avctx -> internal -> byte_buffer ); <nl> s -> avctx -> internal -> byte_buffer = new_buffer ; <nl> s -> avctx -> internal -> byte_buffer_size = new_buffer_size ; <nl> rebase_put_bits (& s -> pb , new_buffer , new_buffer_size );
void * av_malloc ( size_t size ) <nl> long diff ; <nl> # endif <nl>  <nl> + assert ( size ); <nl> + <nl> /* let ' s disallow possible ambiguous cases */ <nl> - if ( size > ( INT_MAX - 32 ) ) <nl> + if ( size > ( INT_MAX - 32 ) || ! size ) <nl> return NULL ; <nl>  <nl> # if CONFIG_MEMALIGN_HACK
typedef struct { <nl>  <nl>  <nl> static const AVOption options [] = { <nl> - { " use_odml ", " use odml index ", offsetof ( AVIContext , use_odml ), FF_OPT_TYPE_INT , 1 , - 1 , 1 , AV_OPT_FLAG_DECODING_PARAM }, <nl> + { " use_odml ", " use odml index ", offsetof ( AVIContext , use_odml ), FF_OPT_TYPE_INT , {. dbl = 1 }, - 1 , 1 , AV_OPT_FLAG_DECODING_PARAM }, <nl> { NULL }, <nl> }; <nl> 
static int vdpau_hevc_start_frame ( AVCodecContext * avctx , <nl> const HEVCFrame * frame = & h -> DPB [ i ]; <nl> if ( frame != h -> ref && ( frame -> flags & ( HEVC_FRAME_FLAG_LONG_REF | <nl> HEVC_FRAME_FLAG_SHORT_REF ))) { <nl> - if ( j > 16 ) { <nl> + if ( j > 15 ) { <nl> av_log ( avctx , AV_LOG_WARNING , <nl> " VDPAU only supports up to 16 references in the DPB . " <nl> " This frame may not be decoded correctly .\ n ");
static int swf_write_trailer ( AVFormatContext * s ) <nl> put_le32 ( pb , file_size ); <nl> url_fseek ( pb , swf -> duration_pos , SEEK_SET ); <nl> put_le16 ( pb , video_enc -> frame_number ); <nl> + url_fseek ( pb , file_size , SEEK_SET ); <nl> } <nl>  <nl> av_free ( swf -> audio_fifo );
static int read_matrix_params ( MLPDecodeContext * m , unsigned int substr , GetBitCo <nl> av_log ( m -> avctx , AV_LOG_ERROR , <nl> " Number of primitive matrices cannot be greater than % d .\ n ", <nl> max_primitive_matrices ); <nl> + s -> num_primitive_matrices = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
int ff_pnm_decode_header ( AVCodecContext * avctx , PNMContext * const s ) <nl>  <nl> avctx -> width = w ; <nl> avctx -> height = h ; <nl> + s -> maxval = maxval ; <nl> if ( depth == 1 ) { <nl> if ( maxval == 1 ) <nl> avctx -> pix_fmt = PIX_FMT_MONOWHITE ;
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> s -> slices [ i ][ j ]. start = offset + header_size ; <nl> - s -> slices [ i ][ j ]. size = avpkt -> size - offset ; <nl> + s -> slices [ i ][ j ]. size = avpkt -> size - s -> slices [ i ][ j ]. start ; <nl> } <nl>  <nl> if ( bytestream2_get_byte (& gb ) != s -> planes )
static int mov_preroll_write_stbl_atoms ( AVIOContext * pb , MOVTrack * track ) <nl> } <nl> entries ++; <nl>  <nl> - if (! group ) <nl> + if (! group ) { <nl> + av_free ( sgpd_entries ); <nl> return 0 ; <nl> + } <nl>  <nl> /* Write sgpd tag */ <nl> avio_wb32 ( pb , 24 + ( group * 2 )); /* size */
static av_cold int dnxhd_decode_init ( AVCodecContext * avctx ) <nl>  <nl> ctx -> avctx = avctx ; <nl> ctx -> cid = - 1 ; <nl> - avctx -> colorspace = AVCOL_SPC_BT709 ; <nl> + if ( avctx -> colorspace == AVCOL_SPC_UNSPECIFIED ) { <nl> + avctx -> colorspace = AVCOL_SPC_BT709 ; <nl> + } <nl>  <nl> avctx -> coded_width = FFALIGN ( avctx -> width , 16 ); <nl> avctx -> coded_height = FFALIGN ( avctx -> height , 16 );
static av_cold int vtenc_init ( AVCodecContext * avctx ) <nl> kCFAllocatorDefault , <nl> & has_b_frames_cfbool ); <nl>  <nl> - if (! status ) { <nl> + if (! status && has_b_frames_cfbool ) { <nl> // Some devices don ' t output B - frames for main profile , even if requested . <nl> vtctx -> has_b_frames = CFBooleanGetValue ( has_b_frames_cfbool ); <nl> CFRelease ( has_b_frames_cfbool );
static int mov_write_sidx_tag ( AVIOContext * pb , <nl> } <nl> } else { <nl> entries = track -> nb_frag_info ; <nl> + if ( entries <= 0 ) <nl> + return 0 ; <nl> presentation_time = track -> frag_info [ 0 ]. time ; <nl> } <nl> 
static int tcp_open ( URLContext * h , const char * uri , int flags ) <nl> } <nl> /* test error */ <nl> optlen = sizeof ( ret ); <nl> - getsockopt ( fd , SOL_SOCKET , SO_ERROR , & ret , & optlen ); <nl> + if ( getsockopt ( fd , SOL_SOCKET , SO_ERROR , & ret , & optlen )) <nl> + ret = AVUNERROR ( ff_neterrno ()); <nl> if ( ret != 0 ) { <nl> char errbuf [ 100 ]; <nl> ret = AVERROR ( ret );
void ff_lzw_decode_tail ( LZWState * p ) <nl> struct LZWState * s = ( struct LZWState *) p ; <nl>  <nl> if ( s -> mode == FF_LZW_GIF ) { <nl> - while ( s -> pbuf < s -> ebuf && s -> bs > 0 ){ <nl> + while ( s -> pbuf + s -> bs < s -> ebuf && s -> bs > 0 ){ <nl> s -> pbuf += s -> bs ; <nl> s -> bs = * s -> pbuf ++; <nl> }
end : <nl> frame -> height = avctx -> height ; <nl> } <nl>  <nl> + if ( ret < 0 ) <nl> + av_frame_unref ( frame ); <nl> + <nl> return ret ; <nl> } <nl> 
typedef struct TargaContext { <nl> } TargaContext ; <nl>  <nl> # define CHECK_BUFFER_SIZE ( buf , buf_end , needed , where ) \ <nl> - if ( buf + needed > buf_end ){ \ <nl> + if ( needed > buf_end - buf ){ \ <nl> av_log ( avctx , AV_LOG_ERROR , " Problem : unexpected end of data while reading " where "\ n "); \ <nl> return - 1 ; \ <nl> } \
typedef struct FlacSubframe { <nl> int shift ; <nl> RiceContext rc ; <nl> int32_t samples [ FLAC_MAX_BLOCKSIZE ]; <nl> - int32_t residual [ FLAC_MAX_BLOCKSIZE ]; <nl> + int32_t residual [ FLAC_MAX_BLOCKSIZE + 1 ]; <nl> } FlacSubframe ; <nl>  <nl> typedef struct FlacFrame {
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> avio_seek ( s -> pb , st -> index_entries [ st -> nb_index_entries - 1 ]. pos , SEEK_SET ); <nl> matroska -> current_id = 0 ; <nl> while (( index = av_index_search_timestamp ( st , timestamp , flags )) < 0 ) { <nl> + matroska -> prev_pkt = NULL ; <nl> matroska_clear_queue ( matroska ); <nl> if ( matroska_parse_cluster ( matroska ) < 0 ) <nl> break ;
static int read_frame_data ( ALSDecContext * ctx , unsigned int ra_frame ) <nl>  <nl> // TODO : read_diff_float_data <nl>  <nl> + if ( get_bits_left ( gb ) < 0 ) { <nl> + av_log ( ctx -> avctx , AV_LOG_ERROR , " Overread % d \ n ", - get_bits_left ( gb )); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static int h261_probe ( AVProbeData * p ) <nl>  <nl> static int ac3_probe ( AVProbeData * p ) <nl> { <nl> - int max_frames , first_frames , frames ; <nl> + int max_frames , first_frames = 0 , frames ; <nl> uint8_t * buf , * buf2 , * end ; <nl> AC3HeaderInfo hdr ; <nl> 
static int cbs_jpeg_split_fragment ( CodedBitstreamContext * ctx , <nl> if ( marker == JPEG_MARKER_SOS ) { <nl> length = AV_RB16 ( frag -> data + start ); <nl>  <nl> + if ( length > end - start ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> data_ref = NULL ; <nl> data = av_malloc ( end - start + <nl> AV_INPUT_BUFFER_PADDING_SIZE );
static int mjpeg_decode_app ( MJpegDecodeContext * s ) <nl>  <nl> if ( id == AV_RB32 (" JFIF ")) { <nl> int t_w , t_h , v1 , v2 ; <nl> + if ( len < 8 ) <nl> + goto out ; <nl> skip_bits (& s -> gb , 8 ); /* the trailing zero - byte */ <nl> v1 = get_bits (& s -> gb , 8 ); <nl> v2 = get_bits (& s -> gb , 8 );
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> return ret ; <nl> } <nl> } else { <nl> - if (( ret = ff_MPV_common_init ( s ) < 0 )) { <nl> + if (( ret = ff_MPV_common_init ( s )) < 0 ) { <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " ff_MPV_common_init () failed .\ n "); <nl> return ret ; <nl> }
static void search_for_quantizers_twoloop ( AVCodecContext * avctx , <nl> sce -> sf_idx [ i ] -= qstep ; <nl> } <nl> qstep >>= 1 ; <nl> - if (! qstep && tbits > destbits * 1 . 02 ) <nl> + if (! qstep && tbits > destbits * 1 . 02 && sce -> sf_idx [ 0 ] < 217 ) <nl> qstep = 1 ; <nl> - if ( sce -> sf_idx [ 0 ] >= 217 ) <nl> - break ; <nl> } while ( qstep ); <nl>  <nl> fflag = 0 ;
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl> info -> ec3_done = 1 ; <nl> goto concatenate ; <nl> } <nl> + } else { <nl> + if ( hdr -> substreamid != 0 ) { <nl> + avpriv_request_sample ( mov -> fc , " Multiple non EAC3 independent substreams "); <nl> + ret = AVERROR_PATCHWELCOME ; <nl> + goto end ; <nl> + } <nl> } <nl>  <nl> /* fill the info needed for the " dec3 " atom */
static const struct URLProtocol * url_find_protocol ( const char * filename ) <nl> return up ; <nl> } <nl> } <nl> + av_freep (& protocols ); <nl>  <nl> return NULL ; <nl> }
static int flashsv_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log_missing_feature ( avctx , " zlibprime_curr ", 1 ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> + if (! s -> blocks && ( s -> zlibprime_curr || s -> zlibprime_prev )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " no data available for zlib " <nl> + " priming \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> size --; // account for flags byte <nl> } <nl> 
static av_cold int mp_decode_init ( AVCodecContext * avctx ) <nl> int w4 = ( avctx -> width + 3 ) & ~ 3 ; <nl> int h4 = ( avctx -> height + 3 ) & ~ 3 ; <nl>  <nl> + if ( avctx -> extradata_size < 2 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " extradata too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> motionpixels_tableinit (); <nl> mp -> avctx = avctx ; <nl> ff_dsputil_init (& mp -> dsp , avctx );
ogm_header ( AVFormatContext * s , int idx ) <nl> if ( size > 52 ) { <nl> av_assert0 ( AV_INPUT_BUFFER_PADDING_SIZE <= 52 ); <nl> size -= 52 ; <nl> + if ( bytestream2_get_bytes_left (& p ) < size ) <nl> + return AVERROR_INVALIDDATA ; <nl> ff_alloc_extradata ( st -> codecpar , size ); <nl> bytestream2_get_buffer (& p , st -> codecpar -> extradata , st -> codecpar -> extradata_size ); <nl> }
static int decode_residual_block ( AVSContext * h , GetBitContext * gb , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> esc_code = get_ue_code ( gb , esc_golomb_order ); <nl> + if ( esc_code < 0 || esc_code > 32767 ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , " esc_code invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> level = esc_code + ( run > r -> max_run ? 1 : r -> level_add [ run ]); <nl> while ( level > r -> inc_limit ) <nl> r ++;
static int find_image_range ( int * pfirst_index , int * plast_index , <nl> if ( avio_check ( buf , AVIO_FLAG_READ ) > 0 ) <nl> break ; <nl> } <nl> - if ( first_index == 5 ) <nl> + if ( first_index == start_index + 5 ) <nl> goto fail ; <nl>  <nl> /* find the last image */
int ff_h264_decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> } <nl> } <nl>  <nl> - if ( h == h0 && h -> dequant_coeff_pps != pps_id ) { <nl> + if ( first_slice && h -> dequant_coeff_pps != pps_id ) { <nl> h -> dequant_coeff_pps = pps_id ; <nl> h264_init_dequant_tables ( h ); <nl> }
static int xbm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int number , len ; <nl>  <nl> ptr += strcspn ( ptr , "#"); <nl> - if ( sscanf ( ptr , "# define % 256s % u ", name , & number ) != 2 ) { <nl> + if ( sscanf ( ptr , "# define % 255s % u ", name , & number ) != 2 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unexpected preprocessor directive \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_wdlt ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> y += skip_lines ; <nl> segments = bytestream2_get_le16 ( gb ); <nl> } <nl> + <nl> + if ( frame_end <= frame ) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( segments & 0x8000 ) { <nl> frame [ width - 1 ] = segments & 0xFF ; <nl> segments = bytestream2_get_le16 ( gb );
int ff_raw_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> st -> codec -> width = width ; <nl> st -> codec -> height = height ; <nl> st -> codec -> pix_fmt = pix_fmt ; <nl> - break ; <nl> fail : <nl> av_freep (& s1 -> video_size ); <nl> av_freep (& s1 -> pixel_format );
static int hls_write_trailer ( struct AVFormatContext * s ) <nl> av_write_trailer ( oc ); <nl> hls -> size = avio_tell ( hls -> avf -> pb ) - hls -> start_pos ; <nl> avio_closep (& oc -> pb ); <nl> - avformat_free_context ( oc ); <nl> av_free ( hls -> basename ); <nl> hls_append_segment ( hls , hls -> duration , hls -> start_pos , hls -> size ); <nl> + avformat_free_context ( oc ); <nl> + hls -> avf = NULL ; <nl> hls_window ( s , 1 ); <nl>  <nl> hls_free_segments ( hls );
static void read_tree ( GetBitContext * gb , Tree * tree ) <nl> tree -> syms [ i ] = get_bits ( gb , 4 ); <nl> tmp1 [ tree -> syms [ i ]] = 1 ; <nl> } <nl> - for ( i = 0 ; i < 16 ; i ++) <nl> + for ( i = 0 ; i < 16 && len < 16 - 1 ; i ++) <nl> if (! tmp1 [ i ]) <nl> tree -> syms [++ len ] = i ; <nl> } else {
static void do_video_out ( AVFormatContext * s , <nl> if (! ost -> last_frame ) <nl> ost -> last_frame = av_frame_alloc (); <nl> av_frame_unref ( ost -> last_frame ); <nl> - if ( next_picture ) <nl> + if ( next_picture && ost -> last_frame ) <nl> av_frame_ref ( ost -> last_frame , next_picture ); <nl> else <nl> av_frame_free (& ost -> last_frame );
static int mkv_write_header ( AVFormatContext * s ) <nl> // initialize stream_duration fields <nl> mkv -> stream_durations = av_mallocz ( s -> nb_streams * sizeof ( int64_t )); <nl> mkv -> stream_duration_offsets = av_mallocz ( s -> nb_streams * sizeof ( int64_t )); <nl> + if (! mkv -> stream_durations || ! mkv -> stream_duration_offsets ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl>  <nl> ret = mkv_write_tracks ( s ); <nl> if ( ret < 0 )
static int yop_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> s -> low_nibble = NULL ; <nl>  <nl> is_odd_frame = avpkt -> data [ 0 ]; <nl> + if ( is_odd_frame > 1 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " frame is too odd % d \ n ", is_odd_frame ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> firstcolor = s -> first_color [ is_odd_frame ]; <nl> palette = ( uint32_t *) s -> frame . data [ 1 ]; <nl> 
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> s -> channel_in_cpl [ ch ] = 0 ; <nl> s -> first_cpl_coords [ ch ] = 1 ; <nl> } <nl> - s -> first_cpl_leak = 1 ; <nl> + s -> first_cpl_leak = s -> eac3 ; <nl> s -> phase_flags_in_use = 0 ; <nl> } <nl> } else if (! s -> eac3 ) {
static void imc_get_coeffs ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_WARNING , <nl> " Potential problem on band % i , coefficient % i " <nl> ": cw_len =% i \ n ", i , j , cw_len ); <nl> - } <nl> - <nl> - cw = get_bits (& q -> gb , cw_len ); <nl> + } else <nl> + cw = get_bits (& q -> gb , cw_len ); <nl> } <nl>  <nl> chctx -> codewords [ j ] = cw ;
ImgReSampleContext * img_resample_full_init ( int owidth , int oheight , <nl> { <nl> ImgReSampleContext * s ; <nl>  <nl> + if (! owidth || ! oheight || ! iwidth || ! iheight ) <nl> + return NULL ; <nl> + <nl> s = av_mallocz ( sizeof ( ImgReSampleContext )); <nl> if (! s ) <nl> return NULL ;
typedef struct AVStream { <nl> int codec_info_nb_frames ; <nl> # endif <nl> /** encoding : PTS generation when outputing stream */ <nl> - AVFrac pts ; <nl> + struct AVFrac pts ; <nl>  <nl> /** <nl> * this is the fundamental unit of time ( in seconds ) in terms
static int qsv_decode_init ( AVCodecContext * avctx , QSVContext * q ) <nl> const AVPixFmtDescriptor * desc ; <nl> mfxSession session = NULL ; <nl> int iopattern = 0 ; <nl> - mfxVideoParam param = { { 0 } }; <nl> + mfxVideoParam param = { 0 }; <nl> int frame_width = avctx -> coded_width ; <nl> int frame_height = avctx -> coded_height ; <nl> int ret ;
static void cdxl_decode_rgb ( CDXLVideoContext * c ) <nl> { <nl> uint32_t * new_palette = ( uint32_t *) c -> frame . data [ 1 ]; <nl>  <nl> + memset ( c -> frame . data [ 1 ], 0 , AVPALETTE_SIZE ); <nl> import_palette ( c , new_palette ); <nl> import_format ( c , c -> frame . linesize [ 0 ], c -> frame . data [ 0 ]); <nl> }
static int apng_write_packet ( AVFormatContext * format_context , AVPacket * packet ) <nl> int ret ; <nl>  <nl> if (! apng -> prev_packet ) { <nl> - apng -> prev_packet = av_malloc ( sizeof (* apng -> prev_packet )); <nl> + apng -> prev_packet = av_packet_alloc (); <nl> if (! apng -> prev_packet ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int decode_byterun ( uint8_t * dst , int dst_size , <nl> } <nl> x += length ; <nl> } <nl> + if ( x < dst_size ) { <nl> + av_log ( NULL , AV_LOG_WARNING , " decode_byterun ended before plane size \ n "); <nl> + memset ( dst + x , 0 , dst_size - x ); <nl> + } <nl> return buf - buf_start ; <nl> } <nl> 
int ff_mpv_reallocate_putbitbuffer ( MpegEncContext * s , size_t threshold , size_t s <nl> uint8_t * new_buffer = NULL ; <nl> int new_buffer_size = 0 ; <nl>  <nl> + if (( s -> avctx -> internal -> byte_buffer_size + size_increase ) >= INT_MAX / 8 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Cannot reallocate putbit buffer \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> + <nl> av_fast_padded_malloc (& new_buffer , & new_buffer_size , <nl> s -> avctx -> internal -> byte_buffer_size + size_increase ); <nl> if (! new_buffer )
static int cavs_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> if (! s -> low_delay && h -> DPB [ 0 ]. f . data [ 0 ]) { <nl> * data_size = sizeof ( AVPicture ); <nl> * picture = h -> DPB [ 0 ]. f ; <nl> + memset (& h -> DPB [ 0 ], 0 , sizeof ( h -> DPB [ 0 ])); <nl> } <nl> return 0 ; <nl> }
static int jp2_find_codestream ( Jpeg2000DecoderContext * s ) <nl> atom2_end = bytestream2_tell (& s -> g ) + atom2_size - 8 ; <nl> if ( atom2_size < 8 || atom2_end > atom_end || atom2_end < atom2_size ) <nl> break ; <nl> + atom2_size -= 8 ; <nl> if ( atom2 == JP2_CODESTREAM ) { <nl> return 1 ; <nl> } else if ( atom2 == MKBETAG (' c ',' o ',' l ',' r ') && atom2_size >= 7 ) {
static int tls_client_handshake_loop ( URLContext * h , int initial ) <nl> TLSContext * c = h -> priv_data ; <nl> TLSShared * s = & c -> tls_shared ; <nl> SECURITY_STATUS sspi_ret ; <nl> - SecBuffer outbuf [ 3 ]; <nl> + SecBuffer outbuf [ 3 ] = { 0 }; <nl> SecBufferDesc outbuf_desc ; <nl> SecBuffer inbuf [ 2 ]; <nl> SecBufferDesc inbuf_desc ;
static int get_dimension ( GetBitContext * gb , const int * dim ) <nl> val = dim [ get_bits1 ( gb ) - val ]; <nl> if (! val ){ <nl> do { <nl> + if ( get_bits_left ( gb ) < 8 ) <nl> + return AVERROR_INVALIDDATA ; <nl> t = get_bits ( gb , 8 ); <nl> val += t << 2 ; <nl> } while ( t == 0xFF );
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> least one frame of codec data , this makes sure the codec initializes <nl> the channel configuration and does not only trust the values from the container . <nl> */ <nl> - try_decode_frame ( st , pkt , ( options && i <= orig_nb_streams )? & options [ i ] : NULL ); <nl> + try_decode_frame ( st , pkt , ( options && i < orig_nb_streams )? & options [ i ] : NULL ); <nl>  <nl> st -> codec_info_nb_frames ++; <nl> count ++;
void ff_mpeg_unref_picture ( MpegEncContext * s , Picture * pic ) <nl>  <nl> av_buffer_unref (& pic -> hwaccel_priv_buf ); <nl>  <nl> + if ( pic -> needs_realloc ) <nl> + free_picture_tables ( pic ); <nl> + <nl> memset (( uint8_t *) pic + off , 0 , sizeof (* pic ) - off ); <nl> } <nl> 
static void do_video_out ( AVFormatContext * s , <nl> } else <nl> in_picture = next_picture ; <nl>  <nl> + if (! in_picture ) <nl> + return ; <nl> + <nl> in_picture -> pts = ost -> sync_opts ; <nl>  <nl> # if 1
static int cmv_decode_frame ( AVCodecContext * avctx , <nl> CmvContext * s = avctx -> priv_data ; <nl> const uint8_t * buf_end = buf + buf_size ; <nl>  <nl> + if ( buf_end - buf < EA_PREAMBLE_SIZE ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( AV_RL32 ( buf )== MVIh_TAG || AV_RB32 ( buf )== MVIh_TAG ) { <nl> cmv_process_header ( s , buf + EA_PREAMBLE_SIZE , buf_end ); <nl> return buf_size ;
static void init_uni_ac_vlc ( RLTable * rl , uint8_t * uni_ac_vlc_len ){ <nl> for ( i = 0 ; i < 128 ; i ++){ <nl> int level = i - 64 ; <nl> int run ; <nl> + if (! level ) <nl> + continue ; <nl> for ( run = 0 ; run < 64 ; run ++){ <nl> int len , bits , code ; <nl> 
decode_intra_mb : <nl>  <nl> dquant = get_se_golomb (& sl -> gb ); <nl>  <nl> - sl -> qscale += dquant ; <nl> + sl -> qscale += ( unsigned ) dquant ; <nl>  <nl> if ((( unsigned ) sl -> qscale ) > max_qp ){ <nl> if ( sl -> qscale < 0 ) sl -> qscale += max_qp + 1 ;
enum AVCodecID av_codec_get_id ( const AVCodecTag * const * tags , unsigned int tag ) <nl> static void compute_chapters_end ( AVFormatContext * s ) <nl> { <nl> unsigned int i , j ; <nl> - int64_t max_time = s -> duration + <nl> + int64_t max_time = 0 ; <nl> + <nl> + if ( s -> duration > 0 ) <nl> + max_time = s -> duration + <nl> (( s -> start_time == AV_NOPTS_VALUE ) ? 0 : s -> start_time ); <nl>  <nl> for ( i = 0 ; i < s -> nb_chapters ; i ++)
static void apply_channel_coupling ( AC3EncodeContext * s ) <nl> # else <nl> int32_t (* fixed_cpl_coords )[ AC3_MAX_CHANNELS ][ 16 ] = cpl_coords ; <nl> # endif <nl> - int blk , ch , bnd , i , j ; <nl> + int av_uninit ( blk ), ch , bnd , i , j ; <nl> CoefSumType energy [ AC3_MAX_BLOCKS ][ AC3_MAX_CHANNELS ][ 16 ] = {{{ 0 }}}; <nl> int cpl_start , num_cpl_coefs ; <nl> 
static int submit_stats ( AVCodecContext * avctx ) <nl> } <nl> h -> stats_size = strlen ( avctx -> stats_in ) * 3 / 4 ; <nl> h -> stats = av_malloc ( h -> stats_size ); <nl> + if (! h -> stats ) { <nl> + h -> stats_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> h -> stats_size = av_base64_decode ( h -> stats , avctx -> stats_in , h -> stats_size ); <nl> } <nl> while ( h -> stats_size - h -> stats_offset > 0 ) {
static int open_slave ( AVFormatContext * avf , char * slave , TeeSlave * tee_slave ) <nl>  <nl> end : <nl> av_free ( format ); <nl> + av_free ( select ); <nl> av_dict_free (& options ); <nl> return ret ; <nl> }
static inline av_const SoftFloat av_sub_sf ( SoftFloat a , SoftFloat b ){ <nl> */ <nl> static inline av_const SoftFloat av_int2sf ( int v , int frac_bits ){ <nl> int exp_offset = 0 ; <nl> - if ( v == INT_MIN ){ <nl> + if ( v <= INT_MIN + 1 ){ <nl> exp_offset = 1 ; <nl> v >>= 1 ; <nl> }
static int load_apply_palette ( FFFrameSync * fs ) <nl>  <nl> error : <nl> av_frame_free (& master ); <nl> - av_frame_free (& second ); <nl> return ret ; <nl> } <nl> 
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( avctx -> codec_id == AV_CODEC_ID_VC1 || avctx -> codec_id == AV_CODEC_ID_VC1IMAGE ) { <nl> int buf_size2 = 0 ; <nl> buf2 = av_mallocz ( buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! buf2 ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( IS_MARKER ( AV_RB32 ( buf ))) { /* frame starts with marker and needs to be parsed */ <nl> const uint8_t * start , * end , * next ;
av_cold int ff_yuv2rgb_c_init_tables ( SwsContext * c , const int inv_table [ 4 ], int <nl> uint8_t * y_table ; <nl> uint16_t * y_table16 ; <nl> uint32_t * y_table32 ; <nl> - int i , base , rbase , gbase , bbase , abase , needAlpha ; <nl> + int i , base , rbase , gbase , bbase , av_uninit ( abase ), needAlpha ; <nl> const int yoffs = fullRange ? 384 : 326 ; <nl>  <nl> int64_t crv = inv_table [ 0 ];
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & gbc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
static av_cold int cook_decode_init ( AVCodecContext * avctx ) <nl> int extradata_size = avctx -> extradata_size ; <nl> int s = 0 ; <nl> unsigned int channel_mask = 0 ; <nl> - int samples_per_frame ; <nl> + int samples_per_frame = 0 ; <nl> int ret ; <nl> q -> avctx = avctx ; <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> flush_dpb ( s -> avctx ); <nl> ff_MPV_common_end ( s ); <nl> h -> list_count = 0 ; <nl> + h -> current_slice = 0 ; <nl> } <nl> if (! s -> context_initialized ) { <nl> if ( h != h0 ) {
AVInputFormat ff_ivr_demuxer = { <nl> . read_probe = ivr_probe , <nl> . read_header = ivr_read_header , <nl> . read_packet = ivr_read_packet , <nl> + . read_close = rm_read_close , <nl> . extensions = " ivr ", <nl> };
int ff_thread_get_buffer ( AVCodecContext * avctx , AVFrame * f ) <nl> avctx -> get_buffer == avcodec_default_get_buffer ) { <nl> err = avctx -> get_buffer ( avctx , f ); <nl> } else { <nl> + pthread_mutex_lock (& p -> progress_mutex ); <nl> p -> requested_frame = f ; <nl> p -> state = STATE_GET_BUFFER ; <nl> - pthread_mutex_lock (& p -> progress_mutex ); <nl> pthread_cond_broadcast (& p -> progress_cond ); <nl>  <nl> while ( p -> state != STATE_SETTING_UP )
void avcodec_flush_buffers ( AVCodecContext * avctx ) <nl> ff_thread_flush ( avctx ); <nl> else if ( avctx -> codec -> flush ) <nl> avctx -> codec -> flush ( avctx ); <nl> + <nl> + if (! avctx -> refcounted_frames ) <nl> + av_frame_unref (& avctx -> internal -> to_free ); <nl> } <nl>  <nl> int av_get_exact_bits_per_sample ( enum AVCodecID codec_id )
end : <nl> int ff_get_buffer ( AVCodecContext * avctx , AVFrame * frame , int flags ) <nl> { <nl> int ret = get_buffer_internal ( avctx , frame , flags ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> + frame -> width = frame -> height = 0 ; <nl> + } <nl> return ret ; <nl> } <nl> 
static int query_formats ( AVFilterContext * ctx ) <nl>  <nl> static int glyph_enu_free ( void * opaque , void * elem ) <nl> { <nl> + Glyph * glyph = elem ; <nl> + <nl> + FT_Done_Glyph (* glyph -> glyph ); <nl> + av_freep (& glyph -> glyph ); <nl> av_free ( elem ); <nl> return 0 ; <nl> }
static int decode_lt_rps ( HEVCContext * s , LongTermRPS * rps , GetBitContext * gb ) <nl> nb_sps = get_ue_golomb_long ( gb ); <nl> nb_sh = get_ue_golomb_long ( gb ); <nl>  <nl> - if ( nb_sh + nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> + if ( nb_sh + ( uint64_t ) nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> rps -> nb_refs = nb_sh + nb_sps ;
void avformat_free_context ( AVFormatContext * s ) <nl> av_dict_free (& s -> metadata ); <nl> av_dict_free (& s -> internal -> id3v2_meta ); <nl> av_freep (& s -> streams ); <nl> - av_freep (& s -> internal ); <nl> flush_packet_queue ( s ); <nl> + av_freep (& s -> internal ); <nl> av_free ( s ); <nl> } <nl> 
static int decode_5 ( SANMVideoContext * ctx ) <nl> # if HAVE_BIGENDIAN <nl> npixels = ctx -> npixels ; <nl> frm = ctx -> frm0 ; <nl> - while ( npixels --) <nl> - * frm ++ = av_bswap16 (* frm ); <nl> + while ( npixels --) { <nl> + * frm = av_bswap16 (* frm ); <nl> + frm ++; <nl> + } <nl> # endif <nl>  <nl> return 0 ;
static int decode_mime_header ( AMRWBContext * ctx , const uint8_t * buf ) <nl> { <nl> /* Decode frame header ( 1st octet ) */ <nl> ctx -> fr_cur_mode = buf [ 0 ] >> 3 & 0x0F ; <nl> - ctx -> fr_quality = ( buf [ 0 ] & 0x4 ) != 0x4 ; <nl> + ctx -> fr_quality = ( buf [ 0 ] & 0x4 ) == 0x4 ; <nl>  <nl> return 1 ; <nl> }
static int ljpeg_decode_rgb_scan ( MJpegDecodeContext * s , int nb_components , int p <nl> int resync_mb_y = 0 ; <nl> int resync_mb_x = 0 ; <nl>  <nl> + if ( s -> nb_components != 3 && s -> nb_components != 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + if ( s -> v_max != 1 || s -> h_max != 1 || ! s -> lossless ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> + <nl> s -> restart_count = s -> restart_interval ; <nl>  <nl> av_fast_malloc (& s -> ljpeg_buffer , & s -> ljpeg_buffer_size ,
intra : <nl> } <nl> end : <nl>  <nl> + if ( get_bits_left (& s -> gb ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /* per - MB end of slice check */ <nl> { <nl> int v = show_bits (& s -> gb , 16 );
int ff_mp4_read_dec_config_descr ( AVFormatContext * fc , AVStream * st , AVIOContext <nl> return AVERROR ( ENOMEM ); <nl> avio_read ( pb , st -> codec -> extradata , len ); <nl> if ( st -> codec -> codec_id == AV_CODEC_ID_AAC ) { <nl> - MPEG4AudioConfig cfg ; <nl> + MPEG4AudioConfig cfg = { 0 }; <nl> avpriv_mpeg4audio_get_config (& cfg , st -> codec -> extradata , <nl> st -> codec -> extradata_size * 8 , 1 ); <nl> st -> codec -> channels = cfg . channels ;
static int mm_decode_intra ( MmContext * s , int half_horiz , int half_vert ) <nl> */ <nl> static int mm_decode_inter ( MmContext * s , int half_horiz , int half_vert ) <nl> { <nl> - int data_off = bytestream2_get_le16 (& s -> gb ), y ; <nl> + int data_off = bytestream2_get_le16 (& s -> gb ); <nl> + int y = 0 ; <nl> GetByteContext data_ptr ; <nl>  <nl> if ( bytestream2_get_bytes_left (& s -> gb ) < data_off )
static int rtsp_read_header ( AVFormatContext * s , <nl> rt -> real_setup_cache = av_mallocz ( 2 * s -> nb_streams * sizeof (* rt -> real_setup_cache )); <nl> if (! rt -> real_setup_cache ) <nl> return AVERROR ( ENOMEM ); <nl> - rt -> real_setup = rt -> real_setup_cache + s -> nb_streams * sizeof (* rt -> real_setup ); <nl> + rt -> real_setup = rt -> real_setup_cache + s -> nb_streams ; <nl>  <nl> if ( ap -> initial_pause ) { <nl> /* do not start immediately */
static int decode_frame_mp3on4 ( AVCodecContext * avctx , void * data , <nl>  <nl> avpriv_mpegaudio_decode_header (( MPADecodeHeader *) m , header ); <nl>  <nl> - if ( ch + m -> nb_channels > avctx -> channels ) { <nl> + if ( ch + m -> nb_channels > avctx -> channels || s -> coff [ fr ] + m -> nb_channels > avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame channel count exceeds codec " <nl> " channel count \ n "); <nl> return AVERROR_INVALIDDATA ;
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> for ( j = 0 ; j < c -> slices ; j ++) { <nl> slice_end = bytestream2_get_le32u (& gb ); <nl> if ( slice_end < 0 || slice_end < slice_start || <nl> - bytestream2_get_bytes_left (& gb ) < slice_end ) { <nl> + bytestream2_get_bytes_left (& gb ) < slice_end + 1024LL ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Incorrect slice size \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int mpeg4_decode_header ( AVCodecParserContext * s1 , AVCodecContext * avctx , <nl> if ( avctx -> extradata_size && pc -> first_picture ) { <nl> init_get_bits ( gb , avctx -> extradata , avctx -> extradata_size * 8 ); <nl> ret = ff_mpeg4_decode_picture_header ( dec_ctx , gb ); <nl> + if ( ret < 0 ) <nl> + av_log ( avctx , AV_LOG_WARNING , " Failed to parse extradata \ n "); <nl> } <nl>  <nl> init_get_bits ( gb , buf , 8 * buf_size );
int vc1_parse_frame_header_adv ( VC1Context * v , GetBitContext * gb ) <nl> v -> rnd = get_bits1 ( gb ); <nl> if ( v -> interlace ) <nl> v -> uvsamp = get_bits1 ( gb ); <nl> + if (! ff_vc1_bfraction_vlc . table ) <nl> + return 0 ; // parsing only , vlc tables havnt been allocated <nl> if ( v -> field_mode ) { <nl> if (! v -> refdist_flag ) <nl> v -> refdist = 0 ;
void ff_ivi_recompose53 ( const IVIPlaneDesc * plane , uint8_t * dst , <nl> b3_ptr = plane -> bands [ 3 ]. buf ; <nl>  <nl> for ( y = 0 ; y < plane -> height ; y += 2 ) { <nl> + <nl> + if ( y + 2 >= plane -> height ) <nl> + pitch = 0 ; <nl> /* load storage variables with values */ <nl> if ( num_bands > 0 ) { <nl> b0_1 = b0_ptr [ 0 ];
int av_find_best_stream ( AVFormatContext * ic , enum AVMediaType type , <nl> st -> disposition & ( AV_DISPOSITION_HEARING_IMPAIRED | <nl> AV_DISPOSITION_VISUAL_IMPAIRED )) <nl> continue ; <nl> - if ( type == AVMEDIA_TYPE_AUDIO && ! avctx -> channels ) <nl> + if ( type == AVMEDIA_TYPE_AUDIO && !( avctx -> channels && avctx -> sample_rate )) <nl> continue ; <nl> if ( decoder_ret ) { <nl> decoder = find_decoder ( ic , st , st -> codec -> codec_id );
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> if ( ast -> sub_packet_size <= 0 || <nl> ast -> sub_packet_size > ast -> audio_framesize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( ast -> audio_framesize % ast -> sub_packet_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> break ; <nl> case DEINT_ID_SIPR : <nl> case DEINT_ID_INT0 :
av_cold void ff_psy_preprocess_end ( struct FFPsyPreprocessContext * ctx ) <nl> for ( i = 0 ; i < ctx -> avctx -> channels ; i ++) <nl> ff_iir_filter_free_state ( ctx -> fstate [ i ]); <nl> av_freep (& ctx -> fstate ); <nl> + av_free ( ctx ); <nl> } <nl> 
int ff_mpeg4_frame_end ( AVCodecContext * avctx , const uint8_t * buf , int buf_size ) <nl> } <nl>  <nl> if ( startcode_found ) { <nl> - av_fast_malloc (& s -> bitstream_buffer , <nl> + av_fast_padded_malloc (& s -> bitstream_buffer , <nl> & s -> allocated_bitstream_buffer_size , <nl> - buf_size - current_pos + <nl> - FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + buf_size - current_pos ); <nl> if (! s -> bitstream_buffer ) <nl> return AVERROR ( ENOMEM ); <nl> memcpy ( s -> bitstream_buffer , buf + current_pos ,
static av_always_inline void decode_bgr_1 ( HYuvContext * s , int count , <nl> index = SHOW_UBITS ( re , & s -> gb , VLC_BITS ); <nl> VLC_INTERN ( s -> temp [ 0 ][ 4 * i + A ], s -> vlc [ 2 ]. table , <nl> & s -> gb , re , VLC_BITS , 3 ); <nl> - } <nl> + } else <nl> + s -> temp [ 0 ][ 4 * i + A ] = 0 ; <nl> } <nl> } <nl> CLOSE_READER ( re , & s -> gb );
static int fraps2_decode_plane ( FrapsContext * s , uint8_t * dst , int stride , int w , <nl> */ <nl> if ( j ) dst [ i ] += dst [ i - stride ]; <nl> else if ( Uoff ) dst [ i ] += 0x80 ; <nl> + if ( get_bits_left (& gb ) < 0 ) { <nl> + free_vlc (& vlc ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl> dst += stride ; <nl> }
static int ape_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR_EOF ; <nl> - if ( ape -> currentframe > ape -> totalframes ) <nl> + if ( ape -> currentframe >= ape -> totalframes ) <nl> return AVERROR_EOF ; <nl>  <nl> if ( avio_seek ( s -> pb , ape -> frames [ ape -> currentframe ]. pos , SEEK_SET ) < 0 )
 <nl> # include < stdarg . h > <nl> # include " avcodec . h " <nl> -# include " movtext . h " <nl> # include " libavutil / avstring . h " <nl> +# include " libavutil / intreadwrite . h " <nl> # include " ass_split . h " <nl> # include " ass . h " <nl> 
static void force_codec_ids ( AVFormatContext * s , AVStream * st ) <nl> break ; <nl> case AVMEDIA_TYPE_DATA : <nl> if ( s -> data_codec_id ) <nl> - st -> codec -> codec_id = s -> data_codec_id ; <nl> + st -> codecpar -> codec_id = s -> data_codec_id ; <nl> break ; <nl> } <nl> }
static int mov_read_header ( AVFormatContext * s ) <nl> if ( s -> streams [ j ]-> id == sc -> timecode_track ) <nl> tmcd_st_id = j ; <nl>  <nl> - if ( tmcd_st_id < 0 ) <nl> + if ( tmcd_st_id < 0 || tmcd_st_id == i ) <nl> continue ; <nl> tcr = av_dict_get ( s -> streams [ tmcd_st_id ]-> metadata , " timecode ", NULL , 0 ); <nl> if ( tcr )
static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> char * p ; <nl> int file_idx = strtol ( in -> name , & p , 0 ); <nl>  <nl> - if ( file_idx < 0 || file_idx > nb_input_files ) { <nl> + if ( file_idx < 0 || file_idx >= nb_input_files ) { <nl> av_log ( NULL , AV_LOG_FATAL , " Invalid file index % d in filtegraph description % s .\ n ", <nl> file_idx , fg -> graph_desc ); <nl> exit_program ( 1 );
static int parse_playlist ( HLSContext * c , const char * url , <nl> uint8_t iv [ 16 ] = ""; <nl> int has_iv = 0 ; <nl> char key [ MAX_URL_SIZE ] = ""; <nl> - char line [ 1024 ]; <nl> + char line [ MAX_URL_SIZE ]; <nl> const char * ptr ; <nl> int close_in = 0 ; <nl> 
int ff_ass_split_override_codes ( const ASSCodesCallbacks * callbacks , void * priv , <nl> char new_line [ 2 ]; <nl> int text_len = 0 ; <nl>  <nl> - while (* buf ) { <nl> + while ( buf && * buf ) { <nl> if ( text && callbacks -> text && <nl> ( sscanf ( buf , "\\% 1 [ nN ]", new_line ) == 1 || <nl> ! strncmp ( buf , "{\\", 2 ))) {
static int bit_allocation ( IMCContext * q , IMCChannel * chctx , <nl> iacc += chctx -> bandWidthT [ i ]; <nl> summa += chctx -> bandWidthT [ i ] * chctx -> flcoeffs4 [ i ]; <nl> } <nl> + <nl> + if (! iacc ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> chctx -> bandWidthT [ BANDS - 1 ] = 0 ; <nl> summa = ( summa * 0 . 5 - freebits ) / iacc ; <nl> 
static int decode_hq_slice ( DiracContext * s , DiracSlice * slice , uint8_t * tmp_buf ) <nl> skip_bits_long ( gb , 8 * s -> highquality . prefix_bytes ); <nl> quant_idx = get_bits ( gb , 8 ); <nl>  <nl> - if ( quant_idx > DIRAC_MAX_QUANT_INDEX ) { <nl> + if ( quant_idx > DIRAC_MAX_QUANT_INDEX - 1 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid quantization index - % i \ n ", quant_idx ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static inline int msmpeg4_decode_block ( MpegEncContext * s , DCTELEM * block , <nl> if ( i > 62 ){ <nl> i -= 192 ; <nl> if ( i &(~ 63 )){ <nl> - if ( s -> error_resilience < 0 ){ <nl> + if (( i + 192 == 64 && level / qmul ==- 1 ) || s -> error_resilience < 0 ){ <nl> fprintf ( stderr , " ignoring overflow at % d % d \ n ", s -> mb_x , s -> mb_y ); <nl> break ; <nl> } else {
av_cold int ffv1_init_slice_contexts ( FFV1Context * f ) <nl> int i ; <nl>  <nl> f -> slice_count = f -> num_h_slices * f -> num_v_slices ; <nl> + if ( f -> slice_count <= 0 ) { <nl> + av_log ( f -> avctx , AV_LOG_ERROR , " Invalid number of slices \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl>  <nl> for ( i = 0 ; i < f -> slice_count ; i ++) { <nl> FFV1Context * fs = av_mallocz ( sizeof (* fs ));
static int dirac_decode_picture_header ( DiracContext * s ) <nl> if (! s -> all_frames [ j ]. avframe . data [ 0 ]) { <nl> s -> ref_pics [ i ] = & s -> all_frames [ j ]; <nl> s -> avctx -> get_buffer ( s -> avctx , & s -> ref_pics [ i ]-> avframe ); <nl> + break ; <nl> } <nl> } <nl> 
static int expand_rle_row16 ( SgiState * s , uint16_t * out_buf , <nl> break ; <nl>  <nl> /* Check for buffer overflow . */ <nl> - if ( pixelstride * ( count - 1 ) >= len ) { <nl> + if ( out_end - out_buf <= pixelstride * ( count - 1 )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid pixel count .\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
int av_open_input_file ( AVFormatContext ** ic_ptr , const char * filename , <nl> int err ; <nl> AVDictionary * opts = convert_format_parameters ( ap ); <nl>  <nl> - if (! ap -> prealloced_context ) <nl> + if (! ap || ! ap -> prealloced_context ) <nl> * ic_ptr = NULL ; <nl>  <nl> err = avformat_open_input ( ic_ptr , filename , fmt , & opts );
static int film_read_packet ( AVFormatContext * s , <nl> av_free ( film -> stereo_buffer ); <nl> film -> stereo_buffer_size = sample -> sample_size ; <nl> film -> stereo_buffer = av_malloc ( film -> stereo_buffer_size ); <nl> + if (! film -> stereo_buffer ) { <nl> + film -> stereo_buffer_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> pkt -> pos = avio_tell ( pb );
static av_cold int decode_end ( AVCodecContext * avctx ) <nl> { <nl> MadContext * t = avctx -> priv_data ; <nl> av_frame_free (& t -> last_frame ); <nl> - av_free ( t -> bitstream_buf ); <nl> + av_freep (& t -> bitstream_buf ); <nl> return 0 ; <nl> } <nl> 
int ff_mjpeg_find_marker ( MJpegDecodeContext * s , <nl> while ( b < t ) { <nl> uint8_t x = src [ b ++]; <nl> put_bits (& pb , 8 , x ); <nl> - if ( x == 0xFF ) { <nl> + if ( x == 0xFF && b < t ) { <nl> x = src [ b ++]; <nl> if ( x & 0x80 ) { <nl> av_log ( s -> avctx , AV_LOG_WARNING , " Invalid escape sequence \ n ");
int ff_h264_decode_slice_header ( H264Context * h , H264SliceContext * sl ) <nl> } <nl>  <nl> if ( h -> context_initialized && needs_reinit ) { <nl> + h -> context_initialized = 0 ; <nl> if ( sl != h -> slice_ctx ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> " changing width % d -> % d / height % d -> % d on "
static int ivr_probe ( AVProbeData * p ) <nl> static int ivr_read_header ( AVFormatContext * s ) <nl> { <nl> unsigned tag , type , len , tlen , value ; <nl> - int i , j , n , count , nb_streams , ret ; <nl> + int i , j , n , count , nb_streams = 0 , ret ; <nl> uint8_t key [ 256 ], val [ 256 ]; <nl> AVIOContext * pb = s -> pb ; <nl> AVStream * st ;
int ff_h264_build_ref_list ( H264Context * h , H264SliceContext * sl ) <nl> break ; <nl> } <nl> default : <nl> - av_assert1 ( 0 ); <nl> + av_assert0 ( 0 ); <nl> } <nl>  <nl> if ( i < 0 ) {
static int get_stream_info ( AVCodecContext * avctx ) <nl>  <nl> for ( i = 0 ; i < info -> numChannels ; i ++) { <nl> AUDIO_CHANNEL_TYPE ctype = info -> pChannelType [ i ]; <nl> - if ( ctype <= ACT_NONE || ctype > FF_ARRAY_ELEMS ( channel_counts )) { <nl> + if ( ctype <= ACT_NONE || ctype >= FF_ARRAY_ELEMS ( channel_counts )) { <nl> av_log ( avctx , AV_LOG_WARNING , " unknown channel type \ n "); <nl> break ; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
void decode_mvs ( VP8Context * s , VP8Macroblock * mb , int mb_x , int mb_y ) <nl>  <nl> AV_ZERO32 (& near_mv [ 0 ]); <nl> AV_ZERO32 (& near_mv [ 1 ]); <nl> + AV_ZERO32 (& near_mv [ 2 ]); <nl>  <nl> /* Process MB on top , left and top - left */ <nl> # define MV_EDGE_CHECK ( n )\
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( cdxl -> framerate ) <nl> st -> duration = frames ; <nl> else <nl> - st -> duration = frames * audio_size ; <nl> + st -> duration = frames * ( int64_t ) audio_size ; <nl> } <nl> st -> start_time = 0 ; <nl> cdxl -> video_stream_index = st -> index ;
static int mp3_read_probe ( AVProbeData * p ) <nl> const uint8_t * buf , * buf0 , * buf2 , * end ; <nl> AVCodecContext * avctx = avcodec_alloc_context3 ( NULL ); <nl>  <nl> + if (! avctx ) <nl> + return 0 ; <nl> + <nl> buf0 = p -> buf ; <nl> end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl> while ( buf0 < end && !* buf0 )
static int matroska_parse_cluster_incremental ( MatroskaDemuxContext * matroska ) <nl> } <nl> } <nl>  <nl> - if ( res < 0 ) matroska -> done = 1 ; <nl> return res ; <nl> } <nl> 
static int mpegts_read_packet ( AVFormatContext * s , <nl> MpegTSContext * ts = s -> priv_data ; <nl> int ret , i ; <nl>  <nl> + pkt -> size = - 1 ; <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> static int mpegts_read_packet ( AVFormatContext * s , <nl> } <nl> } <nl>  <nl> + if (! ret && pkt -> size < 0 ) <nl> + ret = AVERROR ( EINTR ); <nl> return ret ; <nl> } <nl> 
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> output = s -> plane [ plane ]. subband [ 0 ]; <nl> for ( i = 0 ; i < lowpass_height * 2 ; i ++) { <nl> for ( j = 0 ; j < lowpass_width * 2 ; j ++) <nl> - output [ j ] <<= 2 ; <nl> + output [ j ] *= 4 ; <nl>  <nl> output += lowpass_width * 2 ; <nl> }
int av_add_index_entry ( AVStream * st , <nl> memmove ( entries + index + 1 , entries + index , sizeof ( AVIndexEntry )*( st -> nb_index_entries - index )); <nl> } <nl> st -> nb_index_entries ++; <nl> + } else { <nl> + if ( ie -> pos == pos && distance < ie -> min_distance ) // dont reduce the distance <nl> + distance = ie -> min_distance ; <nl> } <nl> } else { <nl> index = st -> nb_index_entries ++;
static int decode_frame ( AVCodecContext * avctx , <nl> t = tm2_read_stream ( l , l -> buffer + offset , tm2_stream_order [ i ], <nl> buf_size - offset ); <nl> if ( t < 0 ) { <nl> + int j = tm2_stream_order [ i ]; <nl> + memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> return t ; <nl> } <nl> offset += t ;
static void gif_copy_img_rect ( const uint32_t * src , uint32_t * dst , <nl> const uint32_t * src_px , * src_pr , <nl> * src_py = src + y_start , <nl> * dst_py = dst + y_start ; <nl> - const uint32_t * src_pb = src_py + ( t + h ) * linesize ; <nl> + const uint32_t * src_pb = src_py + t * linesize ; <nl> uint32_t * dst_px ; <nl>  <nl> for (; src_py < src_pb ; src_py += linesize , dst_py += linesize ) {
static av_cold int MPA_encode_init ( AVCodecContext * avctx ) <nl> s -> freq_index = i ; <nl>  <nl> /* encoding bitrate & frequency */ <nl> - for ( i = 0 ; i < 15 ; i ++) { <nl> + for ( i = 1 ; i < 15 ; i ++) { <nl> if ( avpriv_mpa_bitrate_tab [ s -> lsf ][ 1 ][ i ] == bitrate ) <nl> break ; <nl> }
static inline uint32_t celt_icwrsi ( uint32_t N , uint32_t K , const int * y ) <nl> idx += CELT_PVQ_U ( N - i , sum ) + ( y [ i ] < 0 )* i_s ; <nl> sum += FFABS ( y [ i ]); <nl> } <nl> - av_assert0 ( sum == K ); <nl> return idx ; <nl> } <nl> 
static int unpack_bitstream ( G723_1_Context * p , const uint8_t * buf , <nl> /** <nl> * Bitexact implementation of sqrt ( val / 2 ). <nl> */ <nl> - static int16_t square_root ( int val ) <nl> + static int16_t square_root ( unsigned val ) <nl> { <nl> + av_assert2 (!( val & 0x80000000 )); <nl> + <nl> return ( ff_sqrt ( val << 1 ) >> 1 ) & (~ 1 ); <nl> } <nl> 
static void ffmpeg_cleanup ( int ret ) <nl> avcodec_free_context (& ost -> enc_ctx ); <nl> avcodec_parameters_free (& ost -> ref_par ); <nl>  <nl> - while ( av_fifo_size ( ost -> muxing_queue )) { <nl> + while ( ost -> muxing_queue && av_fifo_size ( ost -> muxing_queue )) { <nl> AVPacket pkt ; <nl> av_fifo_generic_read ( ost -> muxing_queue , & pkt , sizeof ( pkt ), NULL ); <nl> av_packet_unref (& pkt );
unsigned ff_els_decode_unsigned ( ElsDecCtx * ctx , ElsUnsignedRung * ur ) <nl>  <nl> /* handle the error / overflow case */ <nl> if ( ctx -> err || n >= ELS_EXPGOLOMB_LEN ) { <nl> - ctx -> err = AVERROR ( EOVERFLOW ); <nl> + ctx -> err = AVERROR_INVALIDDATA ; <nl> return 0 ; <nl> } <nl> 
struct vfw_ctx { <nl> static enum PixelFormat vfw_pixfmt ( DWORD biCompression , WORD biBitCount ) <nl> { <nl> switch ( biCompression ) { <nl> + case MKTAG (' U ', ' Y ', ' V ', ' Y '): <nl> + return PIX_FMT_UYVY422 ; <nl> case MKTAG (' Y ', ' U ', ' Y ', ' 2 '): <nl> return PIX_FMT_YUYV422 ; <nl> case MKTAG (' I ', ' 4 ', ' 2 ', ' 0 '):
retry : <nl> } <nl> buf = c -> decomp_buf ; <nl> buf_size = c -> decomp_size - FFMAX ( FF_INPUT_BUFFER_PADDING_SIZE , AV_LZO_OUTPUT_PADDING ) - outlen ; <nl> + memset ( c -> decomp_buf + buf_size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> } <nl> if ( c -> codec_frameheader ) { <nl> int w , h , q ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> tmp_ptr = av_fast_realloc ( s -> bitstream , & s -> allocated_bitstream_size , <nl> s -> max_framesize + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! tmp_ptr ) { <nl> + s -> max_framesize = 0 ; <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> }
typedef struct SmcContext { <nl> row_ptr += stride * 4 ; \ <nl> } \ <nl> total_blocks --; \ <nl> - if ( total_blocks < 0 ) \ <nl> + if ( total_blocks < !! n_blocks ) \ <nl> { \ <nl> av_log ( s -> avctx , AV_LOG_INFO , " warning : block counter just went negative ( this should not happen )\ n "); \ <nl> return ; \
static int start_frame_overlay ( AVFilterLink * inlink , AVFilterBufferRef * inpicref <nl> OverlayContext * over = ctx -> priv ; <nl>  <nl> inlink -> cur_buf = NULL ; <nl> + avfilter_unref_bufferp (& over -> overpicref ); <nl> over -> overpicref = inpicref ; <nl> over -> overpicref -> pts = av_rescale_q ( inpicref -> pts , ctx -> inputs [ OVERLAY ]-> time_base , <nl> ctx -> outputs [ 0 ]-> time_base );
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (!* got_frame ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> update_canvas_size ( avctx , avctx -> width , avctx -> height ); <nl>  <nl> if ( s -> has_alpha ) {
static int vc1_decode_sprites ( VC1Context * v , GetBitContext * gb ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - if (! s -> current_picture . f -> data [ 0 ]) { <nl> + if (! s -> current_picture . f || ! s -> current_picture . f -> data [ 0 ]) { <nl> av_log ( avctx , AV_LOG_ERROR , " Got no sprites \ n "); <nl> return - 1 ; <nl> }
static int mxf_read_close ( AVFormatContext * s ) <nl>  <nl> for ( i = 0 ; i < mxf -> metadata_sets_count ; i ++) { <nl> switch ( mxf -> metadata_sets [ i ]-> type ) { <nl> + case Descriptor : <nl> + av_freep (&(( MXFDescriptor *) mxf -> metadata_sets [ i ])-> extradata ); <nl> + break ; <nl> case MultipleDescriptor : <nl> av_freep (&(( MXFDescriptor *) mxf -> metadata_sets [ i ])-> sub_descriptors_refs ); <nl> break ;
ogm_header ( AVFormatContext * s , int idx ) <nl>  <nl> time_unit = bytestream2_get_le64 (& p ); <nl> spu = bytestream2_get_le64 (& p ); <nl> + if (! time_unit || ! spu ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid timing values .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> bytestream2_skip (& p , 4 ); /* default_len */ <nl> bytestream2_skip (& p , 8 ); /* buffersize + bits_per_sample */ <nl> 
AVFilter avfilter_af_asyncts = { <nl> . config_props = config_props , <nl> . request_frame = request_frame }, <nl> { NULL }}, <nl> - . priv_size = & asyncts_class , <nl> + . priv_class = & asyncts_class , <nl> };
static int start_frame ( AVFilterLink * inlink , AVFilterBufferRef * picref ) <nl> avfilter_unref_buffer ( tinterlace -> cur ); <nl> tinterlace -> cur = tinterlace -> next ; <nl> tinterlace -> next = picref ; <nl> + inlink -> cur_buf = NULL ; <nl> return 0 ; <nl> } <nl> 
static void print_report ( int is_last_report , int64_t timer_start , int64_t cur_ti <nl> AVCodecContext * enc ; <nl> int frame_number , vid , i ; <nl> double bitrate ; <nl> - int64_t pts = INT64_MIN ; <nl> + int64_t pts = INT64_MIN + 1 ; <nl> static int64_t last_time = - 1 ; <nl> static int qp_histogram [ 52 ]; <nl> int hours , mins , secs , us ;
static int transcode_init ( void ) <nl> FilterGraph * fg = filtergraphs [ i ]; <nl> for ( j = 0 ; j < fg -> nb_outputs ; j ++) { <nl> OutputFilter * ofilter = fg -> outputs [ j ]; <nl> - if ( ofilter -> ost -> source_index >= 0 ) <nl> + if (! ofilter -> ost || ofilter -> ost -> source_index >= 0 ) <nl> continue ; <nl> if ( fg -> nb_inputs != 1 ) <nl> continue ;
static av_cold int tta_decode_init ( AVCodecContext * avctx ) <nl> s -> data_length = get_bits_long (& s -> gb , 32 ); <nl> skip_bits (& s -> gb , 32 ); // CRC32 of header <nl>  <nl> + if ( s -> channels == 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> switch ( s -> bps ) { <nl> case 2 : <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ;
int ff_vaapi_render_picture ( struct vaapi_context * vactx , VASurfaceID surface ) <nl> VABufferID va_buffers [ 3 ]; <nl> unsigned int n_va_buffers = 0 ; <nl>  <nl> + if (! vactx -> pic_param_buf_id ) <nl> + return 0 ; <nl> + <nl> vaUnmapBuffer ( vactx -> display , vactx -> pic_param_buf_id ); <nl> va_buffers [ n_va_buffers ++] = vactx -> pic_param_buf_id ; <nl> 
static av_cold void uninit ( AVFilterContext * ctx ) <nl> { <nl> MovieContext * movie = ctx -> priv ; <nl>  <nl> - av_free ( movie -> file_name ); <nl> - av_free ( movie -> format_name ); <nl> if ( movie -> codec_ctx ) <nl> avcodec_close ( movie -> codec_ctx ); <nl> if ( movie -> format_ctx )
static av_cold int init ( AVFilterContext * ctx ) <nl> int nb_formats = 1 ; <nl> int i ; <nl>  <nl> + if (! s -> pix_fmts ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Empty output format string .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> /* count the formats */ <nl> cur = s -> pix_fmts ; <nl> while (( cur = strchr ( cur , '|'))) {
static int matroska_read_header ( AVFormatContext * s ) <nl> break ; <nl> if ( i >= FF_ARRAY_ELEMS ( matroska_doctypes )) { <nl> av_log ( s , AV_LOG_WARNING , " Unknown EBML doctype '% s '\ n ", ebml . doctype ); <nl> + if ( matroska -> ctx -> error_recognition & AV_EF_EXPLODE ) { <nl> + ebml_free ( ebml_syntax , & ebml ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl> ebml_free ( ebml_syntax , & ebml ); <nl> 
static void dwt_decode97_int ( DWTContext * s , int32_t * t ) <nl> line += 5 ; <nl>  <nl> for ( i = 0 ; i < w * h ; i ++) <nl> - data [ i ] *= 1 << I_PRESHIFT ; <nl> + data [ i ] *= 1LL << I_PRESHIFT ; <nl>  <nl> for ( lev = 0 ; lev < s -> ndeclevels ; lev ++) { <nl> int lh = s -> linelen [ lev ][ 0 ],
static int read_packet ( AVFormatContext * s , <nl> return 0 ; <nl>  <nl> case MM_TYPE_AUDIO : <nl> + if ( s -> nb_streams != 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , <nl> + " Unexpected audio packet , skipping \ n "); <nl> + avio_skip ( pb , length ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( av_get_packet ( s -> pb , pkt , length )< 0 ) <nl> return AVERROR ( ENOMEM ); <nl> pkt -> size = length ;
static int hls_read_header ( AVFormatContext * s ) <nl>  <nl> return 0 ; <nl> fail : <nl> - free_playlist_list ( c ); <nl> - free_variant_list ( c ); <nl> - free_rendition_list ( c ); <nl> + hls_close ( s ); <nl> return ret ; <nl> } <nl> 
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl>  <nl> if ( s -> v_max == 1 && s -> h_max == 1 && s -> lossless == 1 && nb_components == 3 ) <nl> s -> rgb = 1 ; <nl> + else if (! s -> lossless ) <nl> + s -> rgb = 0 ; <nl>  <nl> /* if different size , realloc / alloc picture */ <nl> if ( width != s -> width || height != s -> height
void ff_convert_matrix ( MpegEncContext * s , int (* qmat )[ 64 ], <nl> qmat16 [ qscale ][ 0 ][ i ] == 128 * 256 ) <nl> qmat16 [ qscale ][ 0 ][ i ] = 128 * 256 - 1 ; <nl> qmat16 [ qscale ][ 1 ][ i ] = <nl> - ROUNDED_DIV ( bias << ( 16 - QUANT_BIAS_SHIFT ), <nl> + ROUNDED_DIV ( bias * ( 1 <<( 16 - QUANT_BIAS_SHIFT )), <nl> qmat16 [ qscale ][ 0 ][ i ]); <nl> } <nl> }
static AVCRC av_crc_table [ AV_CRC_MAX ][ 257 ]; <nl> * @ return < 0 on failure <nl> */ <nl> int av_crc_init ( AVCRC * ctx , int le , int bits , uint32_t poly , int ctx_size ){ <nl> - int i , j ; <nl> + unsigned i , j ; <nl> uint32_t c ; <nl>  <nl> if ( bits < 8 || bits > 32 || poly >= ( 1LL << bits ))
static int vp56_size_changed ( VP56Context * s ) <nl> s -> plane_height [ 0 ] = s -> plane_height [ 3 ] = avctx -> coded_height ; <nl> s -> plane_height [ 1 ] = s -> plane_height [ 2 ] = avctx -> coded_height / 2 ; <nl>  <nl> + s -> have_undamaged_frame = 0 ; <nl> + <nl> for ( i = 0 ; i < 4 ; i ++) <nl> s -> stride [ i ] = s -> flip * s -> frames [ VP56_FRAME_CURRENT ]-> linesize [ i ]; <nl> 
av_cold int ff_h264_decode_init ( AVCodecContext * avctx ) <nl> s -> low_delay = 0 ; <nl> } <nl>  <nl> + ff_init_cabac_states (); <nl> + <nl> return 0 ; <nl> } <nl>  <nl> static int decode_slice ( struct AVCodecContext * avctx , void * arg ) <nl> align_get_bits (& s -> gb ); <nl>  <nl> /* init cabac */ <nl> - ff_init_cabac_states (); <nl> ff_init_cabac_decoder (& h -> cabac , <nl> s -> gb . buffer + get_bits_count (& s -> gb ) / 8 , <nl> ( get_bits_left (& s -> gb ) + 7 ) / 8 );
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl> while ( avpkt . size > 0 && it ++ < maxiteration ) { <nl> av_frame_unref ( frame ); <nl> int ret = decode_handler ( ctx , frame , & got_frame , & avpkt ); <nl> + <nl> + if ( it > 20 ) <nl> + ctx -> error_concealment = 0 ; <nl> + <nl> if ( ret <= 0 || ret > avpkt . size ) <nl> break ; <nl> avpkt . data += ret ;
static int mov_read_mac_string ( MOVContext * c , AVIOContext * pb , int len , <nl> uint8_t t , c = avio_r8 ( pb ); <nl> if ( c < 0x80 && p < end ) <nl> * p ++ = c ; <nl> - else <nl> + else if ( p < end ) <nl> PUT_UTF8 ( mac_to_unicode [ c - 0x80 ], t , if ( p < end ) * p ++ = t ;); <nl> } <nl> * p = 0 ;
static int imc_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> IMCContext * q = avctx -> priv_data ; <nl>  <nl> - LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 ]); <nl> + LOCAL_ALIGNED_16 ( uint16_t , buf16 , [( IMC_BLOCK_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ) / 2 ]); <nl>  <nl> if ( buf_size < IMC_BLOCK_SIZE * avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame too small !\ n ");
static int flashsv_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> if ( has_diff ) { <nl> + if (! s -> keyframe ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " inter frame without keyframe \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> diff_start = get_bits (& gb , 8 ); <nl> s -> diff_height = get_bits (& gb , 8 ); <nl> av_log ( avctx , AV_LOG_DEBUG ,
retry : <nl> /* If we need to reload the playlist again below ( if <nl> * there ' s still no more segments ), switch to a reload <nl> * interval of half the target duration . */ <nl> - reload_interval = s -> target_duration * 500000 ; <nl> + reload_interval = s -> target_duration * 500000LL ; <nl> } <nl> } <nl> if ( s -> cur_seq_no < s -> start_seq_no ) {
static int paf_video_decode ( AVCodecContext * avctx , void * data , <nl> bytestream2_init (& c -> gb , pkt -> data , pkt -> size ); <nl>  <nl> code = bytestream2_get_byte (& c -> gb ); <nl> - if (( code & 0xF ) > 4 ) { <nl> + if (( code & 0xF ) > 4 || ( code & 0xF ) == 3 ) { <nl> avpriv_request_sample ( avctx , " unknown / invalid code "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int mxf_compute_sample_count ( MXFContext * mxf , int stream_index , uint64_t <nl>  <nl> av_assert2 ( size ); <nl>  <nl> - * sample_count = ( mxf -> current_edit_unit / size ) * total ; <nl> + * sample_count = ( mxf -> current_edit_unit / size ) * ( uint64_t ) total ; <nl> for ( i = 0 ; i < mxf -> current_edit_unit % size ; i ++) { <nl> * sample_count += spf -> samples_per_frame [ i ]; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> inbuf_ptr = s -> inbuf ; <nl> s -> frame_size = 0 ; <nl> - * data_size = out_size ; <nl> + if ( out_size >= 0 ) <nl> + * data_size = out_size ; <nl> + else <nl> + av_log ( avctx , AV_LOG_DEBUG , " Error while decoding mpeg audio frame \ n "); // FIXME return - 1 / but also return the number of bytes consumed <nl> break ; <nl> } <nl> }
static int a64multi_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int b_width ; <nl>  <nl> int req_size , ret ; <nl> - uint8_t * buf ; <nl> + uint8_t * buf = NULL ; <nl>  <nl> int * charmap = c -> mc_charmap ; <nl> uint8_t * colram = c -> mc_colram ;
static int dnxhd_decode_header ( DNXHDContext * ctx , AVFrame * frame , <nl> frame -> top_field_first = first_field ^ ctx -> cur_field ; <nl> av_log ( ctx -> avctx , AV_LOG_DEBUG , <nl> " interlaced % d , cur field % d \ n ", buf [ 5 ] & 3 , ctx -> cur_field ); <nl> + } else { <nl> + ctx -> cur_field = 0 ; <nl> } <nl>  <nl> ctx -> height = AV_RB16 ( buf + 0x18 );
static av_cold int init ( AVFilterContext * ctx ) <nl> OCVContext * ocv = ctx -> priv ; <nl> int i ; <nl>  <nl> + if (! ocv -> name ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " No libopencv filter name specified \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( ocv_filter_entries ); i ++) { <nl> OCVFilterEntry * entry = & ocv_filter_entries [ i ]; <nl> if (! strcmp ( ocv -> name , entry -> name )) {
void ff_h264_direct_ref_list_init ( const H264Context * const h , H264SliceContext * <nl> memcpy ( cur -> ref_poc [ 1 ], cur -> ref_poc [ 0 ], sizeof ( cur -> ref_poc [ 0 ])); <nl> } <nl>  <nl> - cur -> mbaff = FRAME_MBAFF ( h ); <nl> + if ( h -> current_slice == 0 ) { <nl> + cur -> mbaff = FRAME_MBAFF ( h ); <nl> + } else { <nl> + av_assert0 ( cur -> mbaff == FRAME_MBAFF ( h )); <nl> + } <nl>  <nl> sl -> col_fieldoff = 0 ; <nl> 
static void ogg_free ( AVFormatContext * s ) <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> AVStream * st = s -> streams [ i ]; <nl> OGGStreamContext * oggstream = st -> priv_data ; <nl> + if (! oggstream ) <nl> + continue ; <nl> if ( st -> codecpar -> codec_id == AV_CODEC_ID_FLAC || <nl> st -> codecpar -> codec_id == AV_CODEC_ID_SPEEX || <nl> st -> codecpar -> codec_id == AV_CODEC_ID_OPUS ||
static av_cold int init_subtitles ( AVFilterContext * ctx , const char * args ) <nl> } <nl>  <nl> end : <nl> - if ( fmt ) <nl> - avformat_close_input (& fmt ); <nl> if ( dec_ctx ) <nl> avcodec_close ( dec_ctx ); <nl> + if ( fmt ) <nl> + avformat_close_input (& fmt ); <nl> return ret ; <nl> } <nl> 
static void FUNC ( dequant )( int16_t * coeffs , int16_t log2_size ) <nl> } else { <nl> for ( y = 0 ; y < size ; y ++) { <nl> for ( x = 0 ; x < size ; x ++) { <nl> - * coeffs = * coeffs << - shift ; <nl> + * coeffs = *( uint16_t *) coeffs << - shift ; <nl> coeffs ++; <nl> } <nl> }
static void apply_dependent_coupling_fixed ( AACContext * ac , <nl> for ( k = offsets [ i ]; k < offsets [ i + 1 ]; k ++) { <nl> tmp = ( int )((( int64_t ) src [ group * 128 + k ] * c + \ <nl> ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ group * 128 + k ] += tmp << shift ; <nl> + dest [ group * 128 + k ] += tmp * ( 1 << shift ); <nl> } <nl> } <nl> }
static int64_t run_opencl_bench ( AVOpenCLExternalEnv * ext_opencl_env ) <nl> cl_int status ; <nl> size_t kernel_len ; <nl> char * inbuf ; <nl> - int * mask ; <nl> + int * mask = NULL ; <nl> int buf_size = width * height * sizeof ( char ); <nl> int mask_size = sizeof ( uint32_t ) * 128 ; <nl> 
static int init_input ( AVFormatContext * s , const char * filename , AVDictionary ** o <nl> int ret ; <nl> AVProbeData pd = { filename , NULL , 0 }; <nl>  <nl> - if ( s -> iformat && ! strlen ( filename )) <nl> - return 0 ; <nl> - <nl> if ( s -> pb ) { <nl> s -> flags |= AVFMT_FLAG_CUSTOM_IO ; <nl> if (! s -> iformat )
return - 1 ; <nl> } <nl> # endif <nl> s -> qscale = get_bits (& s -> gb , 5 ); <nl> + if ( s -> qscale == 0 ){ <nl> + fprintf ( stderr , " invalid qscale \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> if ( s -> pict_type == I_TYPE ) { <nl> code = get_bits (& s -> gb , 5 );
static int hevc_init ( AVCodecParserContext * s ) <nl> { <nl> HEVCContext * h = &(( HEVCParseContext *) s -> priv_data )-> h ; <nl> h -> HEVClc = av_mallocz ( sizeof ( HEVCLocalContext )); <nl> + if (! h -> HEVClc ) <nl> + return AVERROR ( ENOMEM ); <nl> h -> skipped_bytes_pos_size = INT_MAX ; <nl>  <nl> return 0 ;
av_cold int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , <nl> } <nl> } <nl>  <nl> -# define USE_MMAP ( HAVE_MMAP && HAVE_MPROTECT && defined MAP_ANONYMOUS ) <nl> +# if HAVE_MMAP && HAVE_MPROTECT && defined ( MAP_ANONYMOUS ) <nl> +# define USE_MMAP 1 <nl> +# else <nl> +# define USE_MMAP 0 <nl> +# endif <nl>  <nl> /* precalculate horizontal scaler filter coefficients */ <nl> {
static int tak_decode_frame ( AVCodecContext * avctx , void * data , <nl> for ( chan = 0 ; chan < avctx -> channels ; chan ++) { <nl> int32_t * samples = ( int32_t *) frame -> extended_data [ chan ]; <nl> for ( i = 0 ; i < s -> nb_samples ; i ++) <nl> - samples [ i ] <<= 8 ; <nl> + samples [ i ] *= 1 << 8 ; <nl> } <nl> break ; <nl> }
static int smvjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_siz <nl>  <nl> cur_frame = avpkt -> pts % s -> frames_per_jpeg ; <nl>  <nl> + /* cur_frame is later used to calculate the buffer offset , so it mustn ' t be negative */ <nl> + if ( cur_frame < 0 ) <nl> + cur_frame += s -> frames_per_jpeg ; <nl> + <nl> /* Are we at the start of a block ? */ <nl> if (! cur_frame ) { <nl> av_frame_unref ( mjpeg_data );
static int mov_write_tkhd_tag ( AVIOContext * pb , MOVMuxContext * mov , <nl>  <nl> display_matrix = ( uint32_t *) av_stream_get_side_data ( st , AV_PKT_DATA_DISPLAYMATRIX , <nl> & display_matrix_size ); <nl> - if ( display_matrix_size < 9 * sizeof (* display_matrix )) <nl> + if ( display_matrix && display_matrix_size < 9 * sizeof (* display_matrix )) <nl> display_matrix = NULL ; <nl> } <nl> 
static int commit_bitstream_and_slice_buffer ( AVCodecContext * avctx , <nl> const H264Picture * current_picture = h -> cur_pic_ptr ; <nl> struct dxva2_picture_context * ctx_pic = current_picture -> hwaccel_picture_private ; <nl> DXVA_Slice_H264_Short * slice = NULL ; <nl> - void * dxva_data_ptr ; <nl> + void * dxva_data_ptr = NULL ; <nl> uint8_t * dxva_data , * current , * end ; <nl> - unsigned dxva_size ; <nl> + unsigned dxva_size = 0 ; <nl> void * slice_data ; <nl> unsigned slice_size ; <nl> unsigned padding ;
ERROR <nl> # endif <nl>  <nl> void RENAME ( swri_noise_shaping )( SwrContext * s , AudioData * dsts , const AudioData * srcs , const AudioData * noises , int count ){ <nl> - int i , j , pos , ch ; <nl> + int pos = s -> dither . ns_pos ; <nl> + int i , j , ch ; <nl> int taps = s -> dither . ns_taps ; <nl> float S = s -> dither . ns_scale ; <nl> float S_1 = s -> dither . ns_scale_1 ;
static const AVProfile mpeg2_video_profiles [] = { <nl> { FF_PROFILE_MPEG2_SIMPLE , " Simple " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> + { FF_PROFILE_UNKNOWN }, <nl> }; <nl>  <nl> 
static void celt_pvq_search ( float * X , int * y , int K , int N ) <nl> for ( i = 0 ; i < N ; i ++) <nl> res += FFABS ( X [ i ]); <nl>  <nl> - res = K / res ; <nl> + res = K /( res + FLT_EPSILON ); <nl>  <nl> for ( i = 0 ; i < N ; i ++) { <nl> y [ i ] = lrintf ( res * X [ i ]);
return - 1 ; <nl> # endif <nl>  <nl> if ( s -> msmpeg4_version == 1 ){ <nl> - int start_code ; <nl> - start_code = ( get_bits (& s -> gb , 16 )<< 16 ) | get_bits (& s -> gb , 16 ); <nl> + int start_code = get_bits_long (& s -> gb , 32 ); <nl> if ( start_code != 0x00000100 ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " invalid startcode \ n "); <nl> return - 1 ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( tmp_ptr , 0 , s -> allocated_bitstream_size ); <nl> s -> bitstream = tmp_ptr ; <nl> } <nl> 
static int apply_color_indexing_transform ( WebPContext * s ) <nl> uint8_t * line ; <nl> int pixel_bits = 8 >> pal -> size_reduction ; <nl>  <nl> - line = av_malloc ( img -> frame -> linesize [ 0 ]); <nl> + line = av_malloc ( img -> frame -> linesize [ 0 ] + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! line ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> TestSourceContext * test = outlink -> src -> priv ; <nl> AVFilterBufferRef * picref ; <nl>  <nl> - if ( test -> max_pts >= 0 && test -> pts > test -> max_pts ) <nl> + if ( test -> max_pts >= 0 && test -> pts >= test -> max_pts ) <nl> return AVERROR_EOF ; <nl> picref = avfilter_get_video_buffer ( outlink , AV_PERM_WRITE , <nl> test -> w , test -> h );
static int webm_dash_manifest_cues ( AVFormatContext * s , int64_t init_range ) <nl> "%" PRId64 , s -> streams [ 0 ]-> index_entries [ i ]. timestamp ); <nl> if ( ret <= 0 || ( ret == 20 && i == s -> streams [ 0 ]-> nb_index_entries - 1 )) { <nl> av_log ( s , AV_LOG_ERROR , " timestamp too long .\ n "); <nl> + av_free ( buf ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> end += ret ;
static int aac_decode_frame_int ( AVCodecContext * avctx , void * data , <nl>  <nl> if ( samples ) <nl> ac -> frame -> nb_samples = samples ; <nl> + else <nl> + av_frame_unref ( ac -> frame ); <nl> * got_frame_ptr = !! samples ; <nl>  <nl> if ( is_dmono ) {
static int expand_tseq ( void * log , struct sbg_script * s , int * nb_ev_max , <nl> } else { <nl> ev = alloc_array_elem (( void **)& s -> events , sizeof (* ev ), <nl> & s -> nb_events , nb_ev_max ); <nl> + if (! ev ) <nl> + return AVERROR ( ENOMEM ); <nl> ev -> ts = tseq -> ts . t ; <nl> ev -> elements = def -> elements ; <nl> ev -> nb_elements = def -> nb_elements ;
static int h264_slice_init ( H264Context * h , H264SliceContext * sl , <nl>  <nl> if ( sl -> slice_type_nos == AV_PICTURE_TYPE_B && ! sl -> direct_spatial_mv_pred ) <nl> ff_h264_direct_dist_scale_factor ( h , sl ); <nl> - ff_h264_direct_ref_list_init ( h , sl ); <nl> + if (! h -> setup_finished ) <nl> + ff_h264_direct_ref_list_init ( h , sl ); <nl>  <nl> if ( h -> avctx -> skip_loop_filter >= AVDISCARD_ALL || <nl> ( h -> avctx -> skip_loop_filter >= AVDISCARD_NONKEY &&
void checkasm_stack_clobber ( uint64_t clobber , ...); <nl> }\ <nl> } while ( 0 ) <nl> # else <nl> -# define bench_new (...) <nl> +# define bench_new (...) while ( 0 ) <nl> # endif <nl>  <nl> # endif
AVBitStreamFilterContext * av_bitstream_filter_init ( const char * name ){ <nl> if (! strcmp ( name , bsf -> name )){ <nl> AVBitStreamFilterContext * bsfc = av_mallocz ( sizeof ( AVBitStreamFilterContext )); <nl> bsfc -> filter = bsf ; <nl> - bsfc -> priv_data = av_mallocz ( bsf -> priv_data_size ); <nl> + bsfc -> priv_data = bsf -> priv_data_size ? av_mallocz ( bsf -> priv_data_size ) : NULL ; <nl> return bsfc ; <nl> } <nl> bsf = bsf -> next ;
resync : <nl> err = av_get_packet ( pb , pkt , size ); <nl> if ( err < 0 ) <nl> return err ; <nl> + size = err ; <nl>  <nl> if ( ast -> has_pal && pkt -> data && pkt -> size <( unsigned ) INT_MAX / 2 ){ <nl> uint8_t * pal ;
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> ret = 0 ; <nl>  <nl> the_end : <nl> - av_free ( crow_base ); <nl> - av_free ( progressive_buf ); <nl> - av_free ( top_buf ); <nl> + av_freep (& crow_base ); <nl> + av_freep (& progressive_buf ); <nl> + av_freep (& top_buf ); <nl> deflateEnd (& s -> zstream ); <nl> return ret ; <nl> fail :
static void vc1_decode_i_blocks_adv ( VC1Context * v ) <nl> s -> mb_x = 0 ; <nl> ff_init_block_index ( s ); <nl> memset (& s -> coded_block [ s -> block_index [ 0 ]- s -> b8_stride ], 0 , <nl> - s -> b8_stride * sizeof (* s -> coded_block )); <nl> + ( 1 + s -> b8_stride ) * sizeof (* s -> coded_block )); <nl> } <nl> for (; s -> mb_y < s -> end_mb_y ; s -> mb_y ++) { <nl> s -> mb_x = 0 ;
static int wavpack_encode_block ( WavPackEncodeContext * s , <nl> uint8_t * out , int out_size ) <nl> { <nl> int block_size , start , end , data_size , tcount , temp , m = 0 ; <nl> - int i , j , ret , got_extra = 0 , nb_samples = s -> block_samples ; <nl> + int i , j , ret = 0 , got_extra = 0 , nb_samples = s -> block_samples ; <nl> uint32_t crc = 0xffffffffu ; <nl> struct Decorr * dpp ; <nl> PutByteContext pb ;
static int mov_read_stts ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> if ( entries >= UINT_MAX / sizeof (* sc -> stts_data )) <nl> return AVERROR ( EINVAL ); <nl>  <nl> + av_free ( sc -> stts_data ); <nl> sc -> stts_data = av_malloc ( entries * sizeof (* sc -> stts_data )); <nl> if (! sc -> stts_data ) <nl> return AVERROR ( ENOMEM );
static const QCELPBitmap qcelp_rate_octave_bitmap [] = { <nl> QCELP_OF ( lspv [ 8 ], 0 , 1 ), // 8 <nl> QCELP_OF ( cbsign [ 15 ], 0 , 1 ), // 7 <nl> QCELP_OF ( lspv [ 9 ], 0 , 1 ), // 6 <nl> - QCELP_OF ( cbgain [ 0 ], 0 , 2 ), // 7 <nl> + QCELP_OF ( cbgain [ 0 ], 0 , 2 ), // 5 <nl> QCELP_OF ( reserved , 0 , 4 ) // 3 <nl> }; <nl> 
fail_kernel_arg : <nl> kernel_arg , cle ); <nl> err = AVERROR ( EIO ); <nl> fail : <nl> + av_frame_free (& output ); <nl> return err ; <nl> } <nl> 
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> s -> subsampling [ i ] = ff_tget (& s -> gb , type , s -> le ); <nl> if ( s -> subsampling [ i ] <= 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " subsampling % d is invalid \ n ", s -> subsampling [ i ]); <nl> + s -> subsampling [ i ] = 1 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> }
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> } else { <nl> t = get_unary_0_33 ( gb ); <nl> if ( t >= 2 ) { <nl> - if ( get_bits_left ( gb ) < t - 1 ) <nl> + if ( t >= 32 || get_bits_left ( gb ) < t - 1 ) <nl> goto error ; <nl> t = get_bits_long ( gb , t - 1 ) | ( 1 << ( t - 1 )); <nl> } else {
static void vp8_decode_flush ( AVCodecContext * avctx ) <nl> memset ( s -> framep , 0 , sizeof ( s -> framep )); <nl>  <nl> av_freep (& s -> macroblocks_base ); <nl> + av_freep (& s -> filter_strength ); <nl> av_freep (& s -> intra4x4_pred_mode_base ); <nl> av_freep (& s -> top_nnz ); <nl> av_freep (& s -> edge_emu_buffer );
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> s -> level ++; <nl> av_log ( avctx , AV_LOG_DEBUG , " Subband number %" PRIu16 "\ n ", data ); <nl> s -> subband_num = data ; <nl> - if ( s -> level > DWT_LEVELS ) { <nl> + if ( s -> level >= DWT_LEVELS ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Invalid level \ n "); <nl> ret = AVERROR ( EINVAL ); <nl> break ;
static int libschroedinger_decode_frame ( AVCodecContext * avctx , <nl> /* Grab next frame to be returned from the top of the queue . */ <nl> framewithpts = ff_schro_queue_pop (& p_schro_params -> dec_frame_queue ); <nl>  <nl> - if ( framewithpts && framewithpts -> frame ) { <nl> + if ( framewithpts && framewithpts -> frame && framewithpts -> frame -> components [ 0 ]. stride ) { <nl> int ret ; <nl>  <nl> if (( ret = ff_get_buffer ( avctx , avframe , 0 )) < 0 )
static void picmemset ( PicContext * s , AVFrame * frame , int value , int run , <nl> if (* y < 0 ) { <nl> * y = s -> height - 1 ; <nl> * plane += 1 ; <nl> - value <<= bits_per_plane ; <nl> - mask <<= bits_per_plane ; <nl> if (* plane >= s -> nb_planes ) <nl> return ; <nl> + value <<= bits_per_plane ; <nl> + mask <<= bits_per_plane ; <nl> } <nl> } <nl> }
static void formant_postfilter ( G723_1_Context * p , int16_t * lpc , int16_t * buf ) <nl>  <nl> /* Compensation filter */ <nl> for ( j = 0 ; j < SUBFRAME_LEN ; j ++) { <nl> - buf_ptr [ j ] = av_clipl_int32 ( signal_ptr [ j ] + <nl> + buf_ptr [ j ] = av_clipl_int32 (( int64_t ) signal_ptr [ j ] + <nl> (( signal_ptr [ j - 1 ] >> 16 ) * <nl> temp << 1 )) >> 16 ; <nl> }
static void asf_build_simple_index ( AVFormatContext * s , int stream_index ) <nl> last_pos = pos ; <nl> } <nl> } <nl> - asf -> index_read = 1 ; <nl> + asf -> index_read = ict > 0 ; <nl> } <nl> avio_seek ( s -> pb , current_pos , SEEK_SET ); <nl> }
static int flush_packet ( AVFormatContext * ctx , int stream_index , <nl>  <nl> if ( stuffing_size < 0 ) <nl> stuffing_size = 0 ; <nl> + <nl> + if ( startcode == PRIVATE_STREAM_1 && id >= 0xa0 ) { <nl> + if ( payload_size < av_fifo_size ( stream -> fifo )) <nl> + stuffing_size += payload_size % stream -> lpcm_align ; <nl> + } <nl> + <nl> if ( stuffing_size > 16 ) { /*<= 16 for MPEG - 1 , <= 32 for MPEG - 2 */ <nl> pad_packet_bytes += stuffing_size ; <nl> packet_size -= stuffing_size ;
static int aac_adtstoasc_filter ( AVBitStreamFilterContext * bsfc , <nl> buf_size -= get_bits_count (& gb )/ 8 ; <nl> buf += get_bits_count (& gb )/ 8 ; <nl> } <nl> + av_free ( avctx -> extradata ); <nl> avctx -> extradata_size = 2 + pce_size ; <nl> avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> # endif <nl> { <nl> ret = decode_tiles ( avctx , data , size ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + ff_thread_report_progress (& s -> s . frames [ CUR_FRAME ]. tf , INT_MAX , 0 ); <nl> return ret ; <nl> + } <nl> } <nl>  <nl> // Sum all counts fields into td [ 0 ]. counts for tile threading
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size , <nl> continue ; <nl>  <nl> again : <nl> - if ( !( avctx -> active_thread_type & FF_THREAD_FRAME ) <nl> - || nals_needed >= nal_index ) <nl> + if ( (!( avctx -> active_thread_type & FF_THREAD_FRAME ) || nals_needed >= nal_index ) <nl> + && ! h -> current_slice ) <nl> h -> au_pps_id = - 1 ; <nl> /* Ignore per frame NAL unit type during extradata <nl> * parsing . Decoding slices is not possible in codec init
static int mm_decode_intra ( MmContext * s , int half_horiz , int half_vert ) <nl> */ <nl> static int mm_decode_inter ( MmContext * s , int half_horiz , int half_vert ) <nl> { <nl> - int data_off = bytestream2_get_le16 (& s -> gb ), y ; <nl> + int data_off = bytestream2_get_le16 (& s -> gb ), y = 0 ; <nl> GetByteContext data_ptr ; <nl>  <nl> if ( bytestream2_get_bytes_left (& s -> gb ) < data_off )
static int dxv_decompress_dxt5 ( AVCodecContext * avctx ) <nl> AV_WL32 ( ctx -> tex_data + 4 * pos , prev ); <nl> pos ++; <nl> } else { <nl> + if ( bytestream2_get_bytes_left ( gbc ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( state == 0 ) { <nl> value = bytestream2_get_le32 ( gbc ); <nl> state = 16 ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> if ( s -> oformat -> init && ( ret = s -> oformat -> init ( s )) < 0 ) { <nl> - s -> oformat -> deinit ( s ); <nl> + if ( s -> oformat -> deinit ) <nl> + s -> oformat -> deinit ( s ); <nl> goto fail ; <nl> } <nl> 
enum AVCodecID ff_codec_guid_get_id ( const AVCodecGuid * guids , ff_asf_guid guid ) <nl> static void parse_waveformatex ( AVIOContext * pb , AVCodecParameters * par ) <nl> { <nl> ff_asf_guid subformat ; <nl> - par -> bits_per_coded_sample = avio_rl16 ( pb ); <nl> + int bps ; <nl> + <nl> + bps = avio_rl16 ( pb ); <nl> + if ( bps ) <nl> + par -> bits_per_coded_sample = bps ; <nl> par -> channel_layout = avio_rl32 ( pb ); /* dwChannelMask */ <nl>  <nl> ff_get_guid ( pb , & subformat );
static int codec_get_buffer ( AVCodecContext * s , AVFrame * frame ) <nl> FrameBuffer * buf ; <nl> int ret , i ; <nl>  <nl> + if ( av_image_check_size ( s -> width , s -> height , 0 , s )) <nl> + return - 1 ; <nl> + <nl> if (! ist -> buffer_pool && ( ret = alloc_buffer ( s , ist , & ist -> buffer_pool )) < 0 ) <nl> return ret ; <nl> 
static int qsv_decode ( AVCodecContext * avctx , QSVContext * q , <nl>  <nl> do { <nl> ret = get_surface ( avctx , q , & insurf ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& sync ); <nl> return ret ; <nl> + } <nl>  <nl> ret = MFXVideoDECODE_DecodeFrameAsync ( q -> session , avpkt -> size ? & bs : NULL , <nl> insurf , & outsurf , sync );
static void create_adapt_vect ( float * vect , const int16_t * cb , int lag ) <nl> static int adaptive_cb_search ( const int16_t * adapt_cb , float * work , <nl> const float * coefs , float * data ) <nl> { <nl> - int i , best_vect ; <nl> - float score , gain , best_score , best_gain ; <nl> + int i , av_uninit ( best_vect ); <nl> + float score , gain , best_score , av_uninit ( best_gain ); <nl> float exc [ BLOCKSIZE ]; <nl>  <nl> gain = best_score = 0 ;
static int asf_read_metadata ( AVFormatContext * s , int64_t size ) <nl> } else { <nl> get_tag ( s , name , value_type , value_len , 16 ); <nl> } <nl> + av_freep (& name ); <nl> } <nl>  <nl> return 0 ;
static int decode_subframe_length ( WMAProDecodeCtx * s , int offset ) <nl> if ( offset == s -> samples_per_frame - s -> min_samples_per_subframe ) <nl> return s -> min_samples_per_subframe ; <nl>  <nl> + if ( get_bits_left (& s -> gb ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /** 1 bit indicates if the subframe is of maximum length */ <nl> if ( s -> max_subframe_len_bit ) { <nl> if ( get_bits1 (& s -> gb ))
static int init_pass2 ( MpegEncContext * s ) <nl> double rate_factor = 0 ; <nl> double step ; <nl> const int filter_size = ( int )( a -> qblur * 4 ) | 1 ; <nl> - double expected_bits ; <nl> + double expected_bits = 0 ; // init to silence gcc warning <nl> double * qscale , * blurred_qscale , qscale_sum ; <nl>  <nl> /* find complexity & const_bits & decide the pict_types */
static void update_initial_timestamps ( AVFormatContext * s , int stream_index , <nl> if ( st -> first_dts != AV_NOPTS_VALUE || <nl> dts == AV_NOPTS_VALUE || <nl> st -> cur_dts == AV_NOPTS_VALUE || <nl> + st -> cur_dts < INT_MIN + RELATIVE_TS_BASE || <nl> is_relative ( dts )) <nl> return ; <nl> 
static int applehttp_read_seek ( AVFormatContext * s , int stream_index , <nl> int64_t timestamp , int flags ) <nl> { <nl> AppleHTTPContext * c = s -> priv_data ; <nl> - int pos = 0 , i ; <nl> + int64_t pos = 0 ; <nl> + int i ; <nl> struct variant * var = c -> variants [ 0 ]; <nl>  <nl> if (( flags & AVSEEK_FLAG_BYTE ) || ! c -> finished )
static int mp_get_vlc ( MotionPixelsContext * mp , GetBitContext * gb ) <nl> int i ; <nl>  <nl> i = ( mp -> codes_count == 1 ) ? 0 : get_vlc2 ( gb , mp -> vlc . table , mp -> max_codes_bits , 1 ); <nl> + i = FFMIN ( i , FF_ARRAY_ELEMS ( mp -> codes ) - 1 ); <nl> return mp -> codes [ i ]. delta ; <nl> } <nl> 
static int sp5x_decode_frame ( AVCodecContext * avctx , <nl> recoded [ j ++] = 0xFF ; <nl> recoded [ j ++] = 0xD9 ; <nl>  <nl> - avctx -> flags &= ~ CODEC_FLAG_EMU_EDGE ; <nl> av_init_packet (& avpkt_recoded ); <nl> avpkt_recoded . data = recoded ; <nl> avpkt_recoded . size = j ; <nl> AVCodec ff_amv_decoder = { <nl> NULL , <nl> ff_mjpeg_decode_end , <nl> sp5x_decode_frame , <nl> - CODEC_CAP_DR1 , <nl> + 0 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" AMV Video "), <nl> };
static void decode_postinit ( H264Context * h , int setup_finished ){ <nl>  <nl> if ( s -> avctx -> strict_std_compliance >= FF_COMPLIANCE_STRICT <nl> && ! h -> sps . bitstream_restriction_flag ){ <nl> - s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT ; <nl> + s -> avctx -> has_b_frames = MAX_DELAYED_PIC_COUNT - 1 ; <nl> s -> low_delay = 0 ; <nl> } <nl> 
int ff_ac3_bit_alloc_calc_mask ( AC3BitAllocParameters * s , int16_t * band_psd , <nl> int band_start , band_end , begin , end1 ; <nl> int lowcomp , fastleak , slowleak ; <nl>  <nl> + if ( end <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /* excitation function */ <nl> band_start = ff_ac3_bin_to_band_tab [ start ]; <nl> band_end = ff_ac3_bin_to_band_tab [ end - 1 ] + 1 ;
static int smacker_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> } <nl> flags >>= 1 ; <nl> } <nl> - if ( frame_size < 0 ) <nl> + if ( frame_size < 0 || frame_size >= INT_MAX / 2 ) <nl> return AVERROR_INVALIDDATA ; <nl> if ( av_new_packet ( pkt , frame_size + 769 )) <nl> return AVERROR ( ENOMEM );
static const char * type_string ( int type ) <nl> return "< LINK >"; <nl> case AVIO_ENTRY_SOCKET : <nl> return "< SOCKET >"; <nl> + case AVIO_ENTRY_SERVER : <nl> + return "< SERVER >"; <nl> + case AVIO_ENTRY_SHARE : <nl> + return "< SHARE >"; <nl> + case AVIO_ENTRY_WORKGROUP : <nl> + return "< WORKGROUP >"; <nl> case AVIO_ENTRY_UNKNOWN : <nl> default : <nl> break ;
static int matroska_parse_rm_audio ( MatroskaDemuxContext * matroska , <nl> } <nl> memcpy ( track -> audio . buf + y * w , data , w ); <nl> } else { <nl> - if ( size < sps * w / sps ) { <nl> + if ( size < sps * w / sps || h <= 0 ) { <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Corrupt generic RM - style audio packet size \ n "); <nl> return AVERROR_INVALIDDATA ;
static int avi_read_header ( AVFormatContext * s ) <nl> codec_type = AVMEDIA_TYPE_VIDEO ; <nl>  <nl> ast -> sample_size = 0 ; <nl> + st -> avg_frame_rate = av_inv_q ( st -> time_base ); <nl> break ; <nl> case MKTAG (' a ', ' u ', ' d ', ' s '): <nl> codec_type = AVMEDIA_TYPE_AUDIO ;
static int decode_seq_header ( AVSContext * h ) { <nl> av_log_missing_feature ( s , " Width / height changing in CAVS is ", 0 ); <nl> return - 1 ; <nl> } <nl> + if ( width <= 0 || height <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Dimensions invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> width = width ; <nl> s -> height = height ; <nl> skip_bits (& s -> gb , 2 ); // chroma format
* <nl> */ <nl>  <nl> -# define WTV_SECTOR_BITS 12 <nl> +# define WTV_SECTOR_BITS INT64_C ( 12 ) <nl> # define WTV_SECTOR_SIZE ( 1 << WTV_SECTOR_BITS ) <nl> # define WTV_BIGSECTOR_BITS 18 <nl> 
static int mov_read_close ( AVFormatContext * s ) <nl> MOVContext * mov = ( MOVContext *) s -> priv_data ; <nl> for ( i = 0 ; i < mov -> total_streams ; i ++) <nl> mov_free_stream_context ( mov -> streams [ i ]); <nl> - for ( i = 0 ; i < s -> nb_streams ; i ++) <nl> - av_freep (& s -> streams [ i ]); <nl> /* free color tabs */ <nl> for ( i = 0 ; i < mov -> ctab_size ; i ++) <nl> av_freep (& mov -> ctab [ i ]);
static int open_output_file ( const char * filename ) <nl> || dec_ctx -> codec_type == AVMEDIA_TYPE_AUDIO ) { <nl> /* in this example , we choose transcoding to same codec */ <nl> encoder = avcodec_find_encoder ( dec_ctx -> codec_id ); <nl> + if (! encoder ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Neccessary encoder not found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> /* In this example , we transcode to same properties ( picture size , <nl> * sample rate etc .). These properties can be changed for output
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - s -> version_b = avctx -> extradata && avctx -> extradata [ 3 ] == ' b '; <nl> + s -> version_b = avctx -> extradata_size >= 4 && avctx -> extradata [ 3 ] == ' b '; <nl>  <nl> if ( avctx -> codec -> id == CODEC_ID_BINKAUDIO_RDFT ) { <nl> // audio is already interleaved for the RDFT format variant
static int has_codec_parameters ( AVStream * st , const char ** errmsg_ptr ) <nl> FAIL (" unspecified sample rate "); <nl> if (! avctx -> channels ) <nl> FAIL (" unspecified number of channels "); <nl> + if ( st -> info -> found_decoder >= 0 && ! st -> nb_decoded_frames && avctx -> codec_id == AV_CODEC_ID_DTS ) <nl> + FAIL (" no decodable DTS frames "); <nl> break ; <nl> case AVMEDIA_TYPE_VIDEO : <nl> if (! avctx -> width )
static void encode_frame ( MpegAudioContext * s , <nl> q1 += 1 << P ; <nl> if ( q1 < 0 ) <nl> q1 = 0 ; <nl> - q [ m ] = ( unsigned )( q1 * steps ) >> ( P + 1 ); <nl> + q [ m ] = ( q1 * ( unsigned ) steps ) >> ( P + 1 ); <nl> } <nl> # endif <nl> if ( q [ m ] >= steps )
av_cold void ff_init_range_decoder ( RangeCoder * c , const uint8_t * buf , <nl> c -> bytestream += 2 ; <nl> if ( c -> low >= 0xFF00 ) { <nl> c -> low = 0xFF00 ; <nl> - c -> bytestream_end = c -> bytestream + 2 ; <nl> + c -> bytestream_end = c -> bytestream ; <nl> } <nl> } <nl> 
RMStream * ff_rm_alloc_rmstream ( void ) <nl>  <nl> void ff_rm_free_rmstream ( RMStream * rms ) <nl> { <nl> - av_free ( rms -> videobuf ); <nl> - av_free ( rms -> audiobuf ); <nl> + av_freep (& rms -> videobuf ); <nl> + av_freep (& rms -> audiobuf ); <nl> } <nl>  <nl> static int rm_read_audio_stream_info ( AVFormatContext * s , ByteIOContext * pb ,
static int xv_write_trailer ( AVFormatContext * s ) <nl> XShmDetach ( xv -> display , & xv -> yuv_shminfo ); <nl> shmdt ( xv -> yuv_image -> data ); <nl> XFree ( xv -> yuv_image ); <nl> + XFreeGC ( xv -> display , xv -> gc ); <nl> XCloseDisplay ( xv -> display ); <nl> return 0 ; <nl> }
static int asf_write_header ( AVFormatContext * s ) <nl> asf -> nb_packets = 0 ; <nl>  <nl> asf -> index_ptr = av_malloc ( sizeof ( ASFIndex ) * ASF_INDEX_BLOCK ); <nl> + if (! asf -> index_ptr ) <nl> + return AVERROR ( ENOMEM ); <nl> asf -> nb_index_memory_alloc = ASF_INDEX_BLOCK ; <nl> asf -> maximum_packet = 0 ; <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> ret = calc_active_inputs ( s ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + } <nl>  <nl> + if ( s -> active_inputs > 1 ) { <nl> available_samples = get_available_samples ( s ); <nl> if (! available_samples ) <nl> return AVERROR ( EAGAIN );
static void compute_rematrixing_strategy ( AC3EncodeContext * s ) <nl> { <nl> int nb_coefs ; <nl> int blk , bnd ; <nl> - AC3Block * block , * block0 ; <nl> + AC3Block * block , * block0 = NULL ; <nl>  <nl> if ( s -> channel_mode != AC3_CHMODE_STEREO ) <nl> return ;
static int hls_slice_header ( HEVCContext * s ) <nl>  <nl> if ( s -> pps -> slice_header_extension_present_flag ) { <nl> unsigned int length = get_ue_golomb_long ( gb ); <nl> + if ( length * 8LL > get_bits_left ( gb )) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many slice_header_extension_data_bytes \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> for ( i = 0 ; i < length ; i ++) <nl> skip_bits ( gb , 8 ); // slice_header_extension_data_byte <nl> }
static int webm_dash_manifest_read_header ( AVFormatContext * s ) <nl> av_log ( s , AV_LOG_ERROR , " Failed to read file headers \ n "); <nl> return - 1 ; <nl> } <nl> + if (! s -> nb_streams ) { <nl> + matroska_read_close ( s ); <nl> + av_log ( s , AV_LOG_ERROR , " No streams found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> if (! matroska -> is_live ) { <nl> buf = av_asprintf ("% g ", matroska -> duration );
not_extra : <nl> if (!( s -> flags2 & CODEC_FLAG2_CHUNKS ) && ! s -> current_picture_ptr ){ <nl> if ( avctx -> skip_frame >= AVDISCARD_NONREF || <nl> buf_size >= 4 && ! memcmp (" Q264 ", buf , 4 )) <nl> - return 0 ; <nl> + return buf_size ; <nl> av_log ( avctx , AV_LOG_ERROR , " no frame !\ n "); <nl> return - 1 ; <nl> }
static int pxr24_uncompress ( EXRContext * s , const uint8_t * src , <nl> in = ptr [ 2 ] + td -> xsize ; <nl>  <nl> for ( j = 0 ; j < td -> xsize ; ++ j ) { <nl> - uint32_t diff = (*( ptr [ 0 ]++) << 24 ) | <nl> + uint32_t diff = (( unsigned )*( ptr [ 0 ]++) << 24 ) | <nl> (*( ptr [ 1 ]++) << 16 ) | <nl> (*( ptr [ 2 ]++) << 8 ); <nl> pixel += diff ;
static int vp3_update_thread_context ( AVCodecContext * dst , const AVCodecContext * <nl> || s -> width != s1 -> width <nl> || s -> height != s1 -> height ) { <nl> if ( s != s1 ) <nl> - copy_fields ( s , s1 , golden_frame , current_frame ); <nl> + copy_fields ( s , s1 , golden_frame , keyframe ); <nl> return - 1 ; <nl> } <nl> 
static av_cold int xvid_encode_close ( AVCodecContext * avctx ) { <nl> xvid_encore ( x -> encoder_handle , XVID_ENC_DESTROY , NULL , NULL ); <nl>  <nl> if ( avctx -> extradata != NULL ) <nl> - av_free ( avctx -> extradata ); <nl> + av_freep (& avctx -> extradata ); <nl> if ( x -> twopassbuffer != NULL ) { <nl> av_free ( x -> twopassbuffer ); <nl> av_free ( x -> old_twopassbuffer );
static int get_metadata_size ( const uint8_t * buf , int buf_size ) <nl>  <nl> buf += 4 ; <nl> do { <nl> + if ( buf_end - buf < 4 ) <nl> + return 0 ; <nl> ff_flac_parse_block_header ( buf , & metadata_last , NULL , & metadata_size ); <nl> buf += 4 ; <nl> - if ( buf + metadata_size > buf_end ) { <nl> + if ( buf_end - buf < metadata_size ) { <nl> /* need more data in order to read the complete header */ <nl> return 0 ; <nl> }
av_cold void ff_init_range_decoder ( RangeCoder * c , const uint8_t * buf , <nl>  <nl> c -> low = AV_RB16 ( c -> bytestream ); <nl> c -> bytestream += 2 ; <nl> + if ( c -> low >= 0xFF00 ) { <nl> + c -> low = 0xFF00 ; <nl> + c -> bytestream_end = c -> bytestream + 2 ; <nl> + } <nl> } <nl>  <nl> void ff_build_rac_states ( RangeCoder * c , int factor , int max_p )
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl> s -> avctx -> bits_per_raw_sample = <nl> bits = get_bits (& s -> gb , 8 ); <nl>  <nl> + if ( bits > 16 || bits < 1 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " bits % d is invalid \ n ", bits ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( s -> pegasus_rct ) <nl> bits = 9 ; <nl> if ( bits == 9 && ! s -> pegasus_rct )
typedef struct XMVAudioPacket { <nl> uint16_t bits_per_sample ; ///< Bits per compressed sample . <nl> uint32_t bit_rate ; ///< Bits of compressed data per second . <nl> uint16_t flags ; ///< Flags <nl> - uint16_t block_align ; ///< Bytes per compressed block . <nl> + unsigned block_align ; ///< Bytes per compressed block . <nl> uint16_t block_samples ; ///< Decompressed samples per compressed block . <nl>  <nl> enum AVCodecID codec_id ; ///< The codec ID of the compression scheme .
static int mpeg4_update_thread_context ( AVCodecContext * dst , <nl> s -> time_increment_bits = s1 -> time_increment_bits ; <nl> s -> vol_sprite_usage = s1 -> vol_sprite_usage ; <nl> s -> rvlc = s1 -> rvlc ; <nl> + s -> divx_version = s1 -> divx_version ; <nl> + s -> divx_build = s1 -> divx_build ; <nl> + s -> xvid_build = s1 -> xvid_build ; <nl> + s -> lavc_build = s1 -> lavc_build ; <nl>  <nl> return 0 ; <nl> }
copy_oid ( const oid * from , oid * to ) <nl> to -> components = malloc ( to -> length * sizeof (* to -> components )); <nl> if ( to -> length != 0 && to -> components == NULL ) <nl> return ENOMEM ; <nl> - memcpy ( to -> components , from -> components , to -> length ); <nl> + memcpy ( to -> components , from -> components , to -> length * sizeof (* to -> components )); <nl> return 0 ; <nl> }
wav_new_out ( struct fileops * ops , struct dev * dev , <nl> } <nl> f -> mode = mode ; <nl> f -> pstate = WAV_CFG ; <nl> - f -> endpos = f -> startpos = 0 ; <nl> + f -> mmcpos = f -> endpos = f -> startpos = 0 ; <nl> f -> next = wav_list ; <nl> wav_list = f ; <nl> if ( hdr == HDR_WAV ) {
struct cpu_disklabel { <nl> int cd_dummy ; /* must have one element . */ <nl> }; <nl>  <nl> -# ifdef _KERNEL <nl> - struct disklabel ; <nl> - int bounds_check_with_label __P (( struct buf *, struct disklabel *, int )); <nl> -# endif <nl> - <nl> # endif /* _MACHINE_DISKLABEL_H_ */
API_EXPORT ( int ) ap_call_exec ( request_rec * r , child_info * pinfo , char * argv0 , <nl> else if (( conf -> cgi_command_args == AP_FLAG_OFF ) <nl> || (! r -> args ) || (! r -> args [ 0 ]) <nl> || strchr ( r -> args , '=')) { <nl> - execle ( r -> filename , argv0 , NULL , env ); <nl> + execle ( r -> filename , argv0 , ( void *) NULL , env ); <nl> } <nl>  <nl> else {
hvn_nvs_cmd ( struct hvn_softc * sc , void * cmd , size_t cmdsize , uint64_t tid , <nl> return ( rv ); <nl> } <nl>  <nl> + if ( timo == 0 ) <nl> + return ( 0 ); <nl> + <nl> do { <nl> if ( cold ) <nl> delay ( 1000 );
# define _PATH_MAN "/ usr / share / man " <nl> # define _PATH_MEM "/ dev / mem " <nl> # define _PATH_NOLOGIN "/ etc / nologin " <nl> +# define _PATH_RSH "/ usr / bin / rsh " <nl> # define _PATH_SENDMAIL "/ usr / sbin / sendmail " <nl> # define _PATH_SHELLS "/ etc / shells " <nl> # define _PATH_TTY "/ dev / tty "
-/* $ OpenBSD : file_media . c , v 1 . 13 2016 / 01 / 11 07 : 57 : 54 jasper Exp $ */ <nl> +/* $ OpenBSD : file_media . c , v 1 . 14 2016 / 01 / 11 14 : 27 : 29 jasper Exp $ */ <nl>  <nl> /* <nl> * file_media . c - <nl> compute_block_size ( int fd ) <nl> } <nl> } <nl> } <nl> + free ( buffer ); <nl> return 0 ; <nl> } <nl> 
elf32_arm_size_dynamic_sections ( bfd * output_bfd ATTRIBUTE_UNUSED , <nl> if ( elf_hash_table ( info )-> dynamic_sections_created ) <nl> { <nl> /* Set the contents of the . interp section to the interpreter . */ <nl> - if ( info -> executable ) <nl> + if ( info -> executable && ! info -> static_link ) <nl> { <nl> s = bfd_get_section_by_name ( dynobj , ". interp "); <nl> BFD_ASSERT ( s != NULL );
ksymsmmap ( dev , off , prot ) <nl> int off , prot ; <nl> { <nl> # define ksyms_btop ( x ) (( vm_offset_t )( x ) >> PGSHIFT <nl> + if ( off < 0 ) <nl> + return (- 1 ); <nl> if (( unsigned ) off >= ( unsigned )( esym - symtab ) + k1 -> a_text ) <nl> return (- 1 ); <nl> 
_dl_lookup_symbol ( const char * undef_name , const Elf32_Sym ** ref , <nl> { <nl> Elf32_Addr a ; <nl> const Elf32_Sym * s ; <nl> - } weak_value = { 0 , NULL }; <nl> + } weak_value ; /* = { 0 , NULL }; breaks GCC 2 . 8 due to implicit memset */ <nl> + <nl> + _dl_memset (& weak_value , 0 , sizeof ( weak_value )); <nl>  <nl> /* Search the relevant loaded objects for a definition . */ <nl> for ( map = symbol_scope ; map ; map = map -> next )
static int uhci_handle_td ( UHCIState * s , uint32_t addr , UHCI_TD * td , uint32_t * in <nl>  <nl> /* Allocate new packet */ <nl> async = uhci_async_alloc ( uhci_queue_get ( s , td ), addr ); <nl> - if (! async ) <nl> - return TD_RESULT_NEXT_QH ; <nl>  <nl> /* valid needs to be large enough to handle 10 frame delay <nl> * for initial isochronous requests
# ifndef TRACE__EVENT_INTERNAL_H <nl> # define TRACE__EVENT_INTERNAL_H <nl>  <nl> -# include " trace / generated - events . h " <nl> - <nl> - <nl> /** <nl> * TraceEvent : <nl> * @ id : Unique event identifier . <nl> * Opaque generic description of a tracing event . <nl> */ <nl> typedef struct TraceEvent { <nl> - TraceEventID id ; <nl> - TraceEventVCPUID vcpu_id ; <nl> + uint32_t id ; <nl> + uint32_t vcpu_id ; <nl> const char * name ; <nl> const bool sstate ; <nl> uint16_t * dstate ;
static int parallels_open ( BlockDriverState * bs , QDict * options , int flags , <nl> if ( local_err != NULL ) { <nl> goto fail_options ; <nl> } <nl> - if (! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> + <nl> + if (!( flags & BDRV_O_RESIZE ) || ! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> bdrv_truncate ( bs -> file , bdrv_getlength ( bs -> file -> bs )) != 0 ) { <nl> s -> prealloc_mode = PRL_PREALLOC_MODE_FALLOCATE ; <nl> }
int qcow2_alloc_cluster_link_l2 ( BlockDriverState * bs , QCowL2Meta * m ) <nl> } <nl> qcow2_cache_entry_mark_dirty ( s -> l2_table_cache , l2_table ); <nl>  <nl> + assert ( l2_index + m -> nb_clusters <= s -> l2_size ); <nl> for ( i = 0 ; i < m -> nb_clusters ; i ++) { <nl> /* if two concurrent writes happen to the same unallocated cluster <nl> * each write allocates separate cluster and writes data concurrently .
 <nl> # include " virtio - net . h " <nl> # include " vhost_net . h " <nl> +# include " qemu - error . h " <nl>  <nl> # include " config . h " <nl>  <nl> void vhost_net_cleanup ( struct vhost_net * net ) <nl> struct vhost_net * vhost_net_init ( VLANClientState * backend , int devfd , <nl> bool force ) <nl> { <nl> + error_report (" vhost - net support is not compiled in "); <nl> return NULL ; <nl> } <nl> 
int vhost_set_vring_enable ( NetClientState * nc , int enable ) <nl>  <nl> nc -> vring_enable = enable ; <nl>  <nl> - if ( vhost_ops -> vhost_set_vring_enable ) { <nl> + if ( vhost_ops && vhost_ops -> vhost_set_vring_enable ) { <nl> return vhost_ops -> vhost_set_vring_enable (& net -> dev , enable ); <nl> } <nl> 
static int qed_create ( const char * filename , uint32_t cluster_size , <nl> return ret ; <nl> } <nl>  <nl> + /* File must start empty and grow , check truncate is supported */ <nl> + ret = bdrv_truncate ( bs , 0 ); <nl> + if ( ret < 0 ) { <nl> + goto out ; <nl> + } <nl> + <nl> if ( backing_file ) { <nl> header . features |= QED_F_BACKING_FILE ; <nl> header . backing_filename_offset = sizeof ( le_header );
opts_visitor_cleanup ( OptsVisitor * ov ) <nl> g_hash_table_destroy ( ov -> unprocessed_opts ); <nl> } <nl> g_free ( ov -> fake_id_opt ); <nl> - memset ( ov , '\ 0 ', sizeof * ov ); <nl> + g_free ( ov ); <nl> } <nl>  <nl> 
int unix_listen_opts ( QemuOpts * opts , Error ** errp ) <nl> qemu_opt_set ( opts , " path ", un . sun_path , & error_abort ); <nl> } <nl>  <nl> - if (( access ( un . sun_path , F_OK ) == 0 ) && <nl> - unlink ( un . sun_path ) < 0 ) { <nl> + if ( unlink ( un . sun_path ) < 0 && errno != ENOENT ) { <nl> error_setg_errno ( errp , errno , <nl> " Failed to unlink socket % s ", un . sun_path ); <nl> goto err ;
# include < sys / resource . h > <nl> # endif <nl>  <nl> -# if ( defined ( _WIN32 ) || defined RUSAGE_THREAD ) <nl> +# if ( defined ( _WIN32 ) || defined RUSAGE_THREAD ) && \ <nl> + ( defined ( CONFIG_NETTLE_KDF ) || defined ( CONFIG_GCRYPT_KDF )) <nl> # define TEST_LUKS <nl> # else <nl> # undef TEST_LUKS
static int usb_serial_initfn ( USBDevice * dev ) <nl> USBSerialState * s = DO_UPCAST ( USBSerialState , dev , dev ); <nl> s -> dev . speed = USB_SPEED_FULL ; <nl>  <nl> + if (! s -> cs ) { <nl> + error_report (" Property chardev is required "); <nl> + return - 1 ; <nl> + } <nl> + <nl> qemu_chr_add_handlers ( s -> cs , usb_serial_can_read , usb_serial_read , <nl> usb_serial_event , s ); <nl> usb_serial_handle_reset ( dev );
DMAContext * spapr_tce_new_dma_context ( uint32_t liobn , size_t window_size ) <nl> { <nl> sPAPRTCETable * tcet ; <nl>  <nl> + if ( spapr_tce_find_by_liobn ( liobn )) { <nl> + fprintf ( stderr , " Attempted to create TCE table with duplicate " <nl> + " LIOBN 0x % x \ n ", liobn ); <nl> + return NULL ; <nl> + } <nl> + <nl> if (! window_size ) { <nl> return NULL ; <nl> }
void qemu_input_event_send ( QemuConsole * src , InputEvent * evt ) <nl>  <nl> /* send event */ <nl> s = qemu_input_find_handler ( 1 << evt -> kind ); <nl> + if (! s ) { <nl> + return ; <nl> + } <nl> s -> handler -> event ( s -> dev , src , evt ); <nl> s -> events ++; <nl> }
int bdrv_open_image ( BlockDriverState ** pbs , const char * filename , <nl> bdref_key ); <nl> ret = - EINVAL ; <nl> } <nl> + QDECREF ( image_options ); <nl> goto done ; <nl> } <nl> 
static void test_dispatch_cmd_io ( void ) <nl>  <nl> ret3 = qobject_to_qint ( test_qmp_dispatch ( req )); <nl> assert ( qint_get_int ( ret3 ) == 66 ); <nl> - QDECREF ( ret ); <nl> + QDECREF ( ret3 ); <nl>  <nl> QDECREF ( req ); <nl> }
int64_t bdrv_getlength ( BlockDriverState * bs ) <nl> { <nl> int64_t ret = bdrv_nb_sectors ( bs ); <nl>  <nl> + ret = ret > INT64_MAX / BDRV_SECTOR_SIZE ? - EFBIG : ret ; <nl> return ret < 0 ? ret : ret * BDRV_SECTOR_SIZE ; <nl> } <nl> 
static void usb_msd_realize_bot ( USBDevice * dev , Error ** errp ) <nl> usb_desc_init ( dev ); <nl> scsi_bus_new (& s -> bus , sizeof ( s -> bus ), DEVICE ( dev ), <nl> & usb_msd_scsi_info_bot , NULL ); <nl> - s -> bus . qbus . allow_hotplug = 0 ; <nl> usb_msd_handle_reset ( dev ); <nl> } <nl> 
static void ide_dma_cb ( void * opaque , int ret ) <nl> } <nl> if ( ret < 0 ) { <nl> if ( ide_handle_rw_error ( s , - ret , ide_dma_cmd_to_retry ( s -> dma_cmd ))) { <nl> + s -> bus -> dma -> aiocb = NULL ; <nl> return ; <nl> } <nl> }
static void qvirtio_scsi_pci_free ( QVirtIOSCSI * vs ) <nl> qvirtqueue_cleanup ( vs -> dev -> bus , vs -> vq [ i ], vs -> qs -> alloc ); <nl> } <nl> qvirtio_pci_device_disable ( container_of ( vs -> dev , QVirtioPCIDevice , vdev )); <nl> - g_free ( vs -> dev ); <nl> + qvirtio_pci_device_free (( QVirtioPCIDevice *) vs -> dev ); <nl> qvirtio_scsi_stop ( vs -> qs ); <nl> g_free ( vs ); <nl> }
static inline int array_ensure_allocated ( array_t * array , int index ) <nl> array -> pointer = g_realloc ( array -> pointer , new_size ); <nl> if (! array -> pointer ) <nl> return - 1 ; <nl> + memset ( array -> pointer + array -> size , 0 , new_size - array -> size ); <nl> array -> size = new_size ; <nl> array -> next = index + 1 ; <nl> }
static int img_amend ( int argc , char ** argv ) <nl> } <nl>  <nl> if ( optind != argc - 1 ) { <nl> - error_exit (" Expecting one image file name "); <nl> + error_report (" Expecting one image file name "); <nl> + ret = - 1 ; <nl> + goto out ; <nl> } <nl>  <nl> flags = BDRV_O_FLAGS | BDRV_O_RDWR ;
int qemu_acl_remove ( qemu_acl * acl , <nl> i ++; <nl> if ( strcmp ( entry -> match , match ) == 0 ) { <nl> QTAILQ_REMOVE (& acl -> entries , entry , next ); <nl> + acl -> nentries --; <nl> + g_free ( entry -> match ); <nl> + g_free ( entry ); <nl> return i ; <nl> } <nl> }
void tb_invalidate_phys_addr ( target_phys_addr_t addr ) <nl>  <nl> static void breakpoint_invalidate ( CPUArchState * env , target_ulong pc ) <nl> { <nl> - tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( env , pc )); <nl> + tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( env , pc ) | <nl> + ( pc & ~ TARGET_PAGE_MASK )); <nl> } <nl> # endif <nl> # endif /* TARGET_HAS_ICE */
PropertyInfo qdev_prop_bit = { <nl> static uint64_t qdev_get_prop_mask64 ( Property * prop ) <nl> { <nl> assert ( prop -> info == & qdev_prop_bit ); <nl> - return 0x1 << prop -> bitnr ; <nl> + return 0x1ull << prop -> bitnr ; <nl> } <nl>  <nl> static void bit64_prop_set ( DeviceState * dev , Property * props , bool val )
static target_ulong disas_insn ( DisasContext * s , target_ulong pc_start ) <nl> break ; <nl> case 5 : /* lfence */ <nl> case 6 : /* mfence */ <nl> - if (( modrm & 0xc7 ) != 0xc0 || !( s -> cpuid_features & CPUID_SSE )) <nl> + if (( modrm & 0xc7 ) != 0xc0 || !( s -> cpuid_features & CPUID_SSE2 )) <nl> goto illegal_op ; <nl> break ; <nl> case 7 : /* sfence / clflush */
static void leon3_generic_hw_init ( MachineState * machine ) <nl> fprintf ( stderr , " Can ' t read bios image % s \ n ", filename ); <nl> exit ( 1 ); <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Can directly load an application . */ <nl> if ( kernel_filename != NULL ) {
static const TypeInfo qemu_s390_skeys_info = { <nl> . instance_init = qemu_s390_skeys_init , <nl> . instance_size = sizeof ( QEMUS390SKeysState ), <nl> . class_init = qemu_s390_skeys_class_init , <nl> - . instance_size = sizeof ( S390SKeysClass ), <nl> + . class_size = sizeof ( S390SKeysClass ), <nl> }; <nl>  <nl> static void s390_storage_keys_save ( QEMUFile * f , void * opaque )
void qemu_chr_be_write ( CharDriverState * s , uint8_t * buf , int len ) <nl> int qemu_chr_fe_get_msgfd ( CharDriverState * s ) <nl> { <nl> int fd ; <nl> - return ( qemu_chr_fe_get_msgfds ( s , & fd , 1 ) >= 0 ) ? fd : - 1 ; <nl> + return ( qemu_chr_fe_get_msgfds ( s , & fd , 1 ) == 1 ) ? fd : - 1 ; <nl> } <nl>  <nl> int qemu_chr_fe_get_msgfds ( CharDriverState * s , int * fds , int len )
void ppce500_init ( MachineState * machine , PPCE500Params * params ) <nl> exit ( 1 ); <nl> } <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Reserve space for dtb */ <nl> dt_base = ( loadaddr + bios_size + DTC_LOAD_PAD ) & ~ DTC_PAD_MASK ;
static uint64_t <nl> e1000e_io_read ( void * opaque , hwaddr addr , unsigned size ) <nl> { <nl> E1000EState * s = opaque ; <nl> - uint32_t idx ; <nl> + uint32_t idx = 0 ; <nl> uint64_t val ; <nl>  <nl> switch ( addr ) { <nl> e1000e_io_write ( void * opaque , hwaddr addr , <nl> uint64_t val , unsigned size ) <nl> { <nl> E1000EState * s = opaque ; <nl> - uint32_t idx ; <nl> + uint32_t idx = 0 ; <nl>  <nl> switch ( addr ) { <nl> case E1000_IOADDR :
static void * qpa_thread_out ( void * arg ) <nl> return NULL ; <nl> } <nl>  <nl> + pa -> live = 0 ; <nl> pa -> rpos = rpos ; <nl> - pa -> live -= decr ; <nl> pa -> decr += decr ; <nl> } <nl> 
static void ehci_detach ( USBPort * port ) <nl> ehci_queues_rip_device ( s , port -> dev , 0 ); <nl> ehci_queues_rip_device ( s , port -> dev , 1 ); <nl>  <nl> - * portsc &= ~( PORTSC_CONNECT | PORTSC_PED ); <nl> + * portsc &= ~( PORTSC_CONNECT | PORTSC_PED | PORTSC_SUSPEND ); <nl> * portsc |= PORTSC_CSC ; <nl>  <nl> ehci_raise_irq ( s , USBSTS_PCD );
glue ( cirrus_bitblt_rop_fwd_ , ROP_NAME )( CirrusVGAState * s , <nl> dstpitch -= bltwidth ; <nl> srcpitch -= bltwidth ; <nl>  <nl> - if ( dstpitch < 0 || srcpitch < 0 ) { <nl> - /* is 0 valid ? srcpitch == 0 could be useful */ <nl> + if ( bltheight > 1 && ( dstpitch < 0 || srcpitch < 0 )) { <nl> return ; <nl> } <nl> 
file_backend_instance_init ( Object * o ) <nl> set_mem_path , NULL ); <nl> } <nl>  <nl> + static void file_backend_instance_finalize ( Object * o ) <nl> +{ <nl> + HostMemoryBackendFile * fb = MEMORY_BACKEND_FILE ( o ); <nl> + <nl> + g_free ( fb -> mem_path ); <nl> +} <nl> + <nl> static const TypeInfo file_backend_info = { <nl> . name = TYPE_MEMORY_BACKEND_FILE , <nl> . parent = TYPE_MEMORY_BACKEND , <nl> . class_init = file_backend_class_init , <nl> . instance_init = file_backend_instance_init , <nl> + . instance_finalize = file_backend_instance_finalize , <nl> . instance_size = sizeof ( HostMemoryBackendFile ), <nl> }; <nl> 
void qemu_opts_del ( QemuOpts * opts ) <nl> { <nl> QemuOpt * opt ; <nl>  <nl> + if ( opts == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> for (;;) { <nl> opt = QTAILQ_FIRST (& opts -> head ); <nl> if ( opt == NULL )
int vhost_dev_init ( struct vhost_dev * hdev , void * opaque , <nl> if (!( hdev -> features & ( 0x1ULL << VHOST_F_LOG_ALL ))) { <nl> error_setg (& hdev -> migration_blocker , <nl> " Migration disabled : vhost lacks VHOST_F_LOG_ALL feature ."); <nl> - } else if (! qemu_memfd_check ()) { <nl> + } else if ( vhost_dev_log_is_shared ( hdev ) && ! qemu_memfd_check ()) { <nl> error_setg (& hdev -> migration_blocker , <nl> " Migration disabled : failed to allocate shared memory "); <nl> }
static int iscsi_open ( BlockDriverState * bs , QDict * options , int flags , <nl> QemuOpts * opts ; <nl> Error * local_err = NULL ; <nl> const char * filename ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> if (( BDRV_SECTOR_SIZE % 512 ) != 0 ) { <nl> error_setg ( errp , " iSCSI : Invalid BDRV_SECTOR_SIZE . "
void restore_boot_order ( void * opaque ) <nl> return ; <nl> } <nl>  <nl> - qemu_boot_set ( normal_boot_order , NULL ); <nl> + if ( boot_set_handler ) { <nl> + qemu_boot_set ( normal_boot_order , & error_abort ); <nl> + } <nl>  <nl> qemu_unregister_reset ( restore_boot_order , normal_boot_order ); <nl> g_free ( normal_boot_order );
static int get_cluster_offset ( BlockDriverState * bs , <nl> uint32_t min_count , * l2_table ; <nl> bool zeroed = false ; <nl> int64_t ret ; <nl> - int32_t cluster_sector ; <nl> + int64_t cluster_sector ; <nl>  <nl> if ( m_data ) { <nl> m_data -> valid = 0 ;
static void qemu_cleanup_net_client ( NetClientState * nc ) <nl> { <nl> QTAILQ_REMOVE (& net_clients , nc , next ); <nl>  <nl> - nc -> info -> cleanup ( nc ); <nl> + if ( nc -> info -> cleanup ) { <nl> + nc -> info -> cleanup ( nc ); <nl> + } <nl> } <nl>  <nl> static void qemu_free_net_client ( NetClientState * nc )
int net_init_l2tpv3 ( const NetClientOptions * opts , <nl> if ( fd == - 1 ) { <nl> fd = - errno ; <nl> error_report (" l2tpv3_open : socket creation failed , errno = % d ", - fd ); <nl> - freeaddrinfo ( result ); <nl> goto outerr ; <nl> } <nl> if ( bind ( fd , ( struct sockaddr *) result -> ai_addr , result -> ai_addrlen )) {
int mmu_translate ( CPUS390XState * env , target_ulong vaddr , int rw , uint64_t asc , <nl> /* Convert real address -> absolute address */ <nl> * raddr = mmu_real2abs ( env , * raddr ); <nl>  <nl> - if (* raddr <= ram_size ) { <nl> + if (* raddr < ram_size ) { <nl> sk = & env -> storage_keys [* raddr / TARGET_PAGE_SIZE ]; <nl> if (* flags & PAGE_READ ) { <nl> * sk |= SK_R ;
static int ioreq_runio_qemu_aio ( struct ioreq * ioreq ) <nl> break ; <nl> case BLKIF_OP_WRITE : <nl> case BLKIF_OP_WRITE_BARRIER : <nl> - ioreq -> aio_inflight ++; <nl> if (! ioreq -> req . nr_segments ) <nl> break ; <nl> + ioreq -> aio_inflight ++; <nl> bdrv_aio_writev ( blkdev -> bs , ioreq -> start / BLOCK_SIZE , <nl> & ioreq -> v , ioreq -> v . size / BLOCK_SIZE , <nl> qemu_aio_complete , ioreq );
static int handle_instruction ( CPUState * env , struct kvm_run * run ) <nl> if ( r < 0 ) { <nl> enter_pgmcheck ( env , 0x0001 ); <nl> } <nl> - return r ; <nl> + return 0 ; <nl> } <nl>  <nl> static int handle_intercept ( CPUState * env )
int rom_add_file ( const char * file , const char * fw_dir , <nl> err : <nl> if ( fd != - 1 ) <nl> close ( fd ); <nl> + <nl> g_free ( rom -> data ); <nl> g_free ( rom -> path ); <nl> g_free ( rom -> name ); <nl> + if ( fw_dir ) { <nl> + g_free ( rom -> fw_dir ); <nl> + g_free ( rom -> fw_file ); <nl> + } <nl> g_free ( rom ); <nl> + <nl> return - 1 ; <nl> } <nl> 
static uint32_t get_cmd ( ESPState * s , uint8_t * buf ) <nl> s -> ti_rptr = 0 ; <nl> s -> ti_wptr = 0 ; <nl>  <nl> - if ( s -> current_dev ) { <nl> + if ( s -> current_req ) { <nl> /* Started a new command before the old one finished . Cancel it . */ <nl> scsi_req_cancel ( s -> current_req ); <nl> s -> async_len = 0 ;
static int print_block_option_help ( const char * filename , const char * fmt ) <nl> proto_drv = bdrv_find_protocol ( filename , true ); <nl> if (! proto_drv ) { <nl> error_report (" Unknown protocol '% s '", filename ); <nl> + free_option_parameters ( create_options ); <nl> return 1 ; <nl> } <nl> create_options = append_option_parameters ( create_options ,
e1000e_set_pbaclr ( E1000ECore * core , int index , uint32_t val ) <nl>  <nl> core -> mac [ PBACLR ] = val & E1000_PBACLR_VALID_MASK ; <nl>  <nl> - if ( msix_enabled ( core -> owner )) { <nl> + if (! msix_enabled ( core -> owner )) { <nl> return ; <nl> } <nl> 
static void make_dirty ( uint8_t device ) <nl>  <nl> guest_buf = guest_alloc ( guest_malloc , len ); <nl> buf = g_malloc ( len ); <nl> + memset ( buf , rand () % 255 + 1 , len ); <nl> g_assert ( guest_buf ); <nl> g_assert ( buf ); <nl> 
static int floppy_probe_device ( const char * filename ) <nl> struct stat st ; <nl>  <nl> if ( strstart ( filename , "/ dev / fd ", NULL ) && <nl> - ! strstart ( filename , "/ dev / fdset /", NULL )) { <nl> + ! strstart ( filename , "/ dev / fdset /", NULL ) && <nl> + ! strstart ( filename , "/ dev / fd /", NULL )) { <nl> prio = 50 ; <nl> } <nl> 
static void calxeda_init ( MachineState * machine , enum cxmachines machine_id ) <nl> if ( bios_name != NULL ) { <nl> sysboot_filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> if ( sysboot_filename != NULL ) { <nl> - uint32_t filesize = get_image_size ( sysboot_filename ); <nl> - if ( load_image_targphys (" sysram . bin ", 0xfff88000 , filesize ) < 0 ) { <nl> + if ( load_image_targphys ( sysboot_filename , 0xfff88000 , 0x8000 ) < 0 ) { <nl> hw_error (" Unable to load % s \ n ", bios_name ); <nl> } <nl> g_free ( sysboot_filename );
static inline void hwsetup_add_tag ( HWSetup * hw , enum hwsetup_tag t ) <nl>  <nl> static inline void hwsetup_add_str ( HWSetup * hw , const char * str ) <nl> { <nl> - strncpy ( hw -> ptr , str , 31 ); /* make sure last byte is zero */ <nl> + pstrcpy ( hw -> ptr , 32 , str ); <nl> hw -> ptr += 32 ; <nl> } <nl> 
static inline void elf_core_copy_regs ( target_elf_gregset_t * regs , <nl> int i ; <nl>  <nl> for ( i = 0 ; i < 16 ; i ++) { <nl> - (* regs [ i ]) = tswapreg ( env -> gregs [ i ]); <nl> + (* regs )[ i ] = tswapreg ( env -> gregs [ i ]); <nl> } <nl>  <nl> (* regs )[ TARGET_REG_PC ] = tswapreg ( env -> pc );
static void pmac_ide_atapi_transfer_cb ( void * opaque , int ret ) <nl> s -> io_buffer_size = MIN ( s -> io_buffer_size , io -> len ); <nl> dma_memory_write (& address_space_memory , io -> addr , s -> io_buffer , <nl> s -> io_buffer_size ); <nl> + io -> len = 0 ; <nl> ide_atapi_cmd_ok ( s ); <nl> m -> dma_active = false ; <nl> goto done ;
static int img_check ( int argc , char ** argv ) <nl> static const struct option long_options [] = { <nl> {" help ", no_argument , 0 , ' h '}, <nl> {" format ", required_argument , 0 , ' f '}, <nl> - {" repair ", no_argument , 0 , ' r '}, <nl> + {" repair ", required_argument , 0 , ' r '}, <nl> {" output ", required_argument , 0 , OPTION_OUTPUT }, <nl> { 0 , 0 , 0 , 0 } <nl> };
static void sdhci_send_command ( SDHCIState * s ) <nl> ( s -> cmdreg & SDHC_CMD_RESPONSE ) == SDHC_CMD_RSP_WITH_BUSY ) { <nl> s -> norintsts |= SDHC_NIS_TRSCMP ; <nl> } <nl> - } else if ( rlen != 0 && ( s -> errintstsen & SDHC_EISEN_CMDIDX )) { <nl> - s -> errintsts |= SDHC_EIS_CMDIDX ; <nl> - s -> norintsts |= SDHC_NIS_ERR ; <nl> } <nl>  <nl> if ( s -> norintstsen & SDHC_NISEN_CMDCMP ) {
void qmp_block_resize ( bool has_device , const char * device , <nl> goto out ; <nl> } <nl>  <nl> - /* complete all in - flight operations before resizing the device */ <nl> - bdrv_drain_all (); <nl> - <nl> + bdrv_drained_begin ( bs ); <nl> ret = blk_truncate ( blk , size , errp ); <nl> + bdrv_drained_end ( bs ); <nl>  <nl> out : <nl> blk_unref ( blk );
static int load_refcount_block ( BlockDriverState * bs , <nl> static int get_refcount ( BlockDriverState * bs , int64_t cluster_index ) <nl> { <nl> BDRVQcowState * s = bs -> opaque ; <nl> - int refcount_table_index , block_index ; <nl> + uint64_t refcount_table_index , block_index ; <nl> int64_t refcount_block_offset ; <nl> int ret ; <nl> uint16_t * refcount_block ;
static void vnc_async_encoding_end ( VncState * orig , VncState * local ) <nl> orig -> hextile = local -> hextile ; <nl> orig -> zrle = local -> zrle ; <nl> orig -> lossy_rect = local -> lossy_rect ; <nl> + <nl> + queue -> buffer = local -> output ; <nl> } <nl>  <nl> static int vnc_worker_thread_loop ( VncJobQueue * queue )
void ptimer_set_limit ( ptimer_state * s , uint64_t limit , int reload ) <nl> * on the current generation of host machines . <nl> */ <nl>  <nl> - if ( limit * s -> period < 10000 && s -> period ) { <nl> + if (! use_icount && limit * s -> period < 10000 && s -> period ) { <nl> limit = 10000 / s -> period ; <nl> } <nl> 
int pci_piix3_xen_ide_unplug ( DeviceState * dev ) <nl> { <nl> PCIIDEState * pci_ide ; <nl> DriveInfo * di ; <nl> - int i = 0 ; <nl> + int i ; <nl>  <nl> pci_ide = PCI_IDE ( dev ); <nl>  <nl> - for (; i < 3 ; i ++) { <nl> + for ( i = 0 ; i < 4 ; i ++) { <nl> di = drive_get_by_index ( IF_IDE , i ); <nl> if ( di != NULL && ! di -> media_cd ) { <nl> BlockBackend * blk = blk_by_legacy_dinfo ( di );
static int vhost_user_cleanup ( struct vhost_dev * dev ) <nl>  <nl> u = dev -> opaque ; <nl> if ( u -> slave_fd >= 0 ) { <nl> + qemu_set_fd_handler ( u -> slave_fd , NULL , NULL , NULL ); <nl> close ( u -> slave_fd ); <nl> u -> slave_fd = - 1 ; <nl> }
static void qio_channel_websock_handshake_send_res_ok ( QIOChannelWebsock * ioc , <nl> char combined_key [ QIO_CHANNEL_WEBSOCK_CLIENT_KEY_LEN + <nl> QIO_CHANNEL_WEBSOCK_GUID_LEN + 1 ]; <nl> char * accept = NULL ; <nl> - char * date = qio_channel_websock_date_str (); <nl> + char * date = NULL ; <nl>  <nl> g_strlcpy ( combined_key , key , QIO_CHANNEL_WEBSOCK_CLIENT_KEY_LEN + 1 ); <nl> g_strlcat ( combined_key , QIO_CHANNEL_WEBSOCK_GUID , <nl> static void qio_channel_websock_handshake_send_res_ok ( QIOChannelWebsock * ioc , <nl> return ; <nl> } <nl>  <nl> + date = qio_channel_websock_date_str (); <nl> qio_channel_websock_handshake_send_res ( <nl> ioc , QIO_CHANNEL_WEBSOCK_HANDSHAKE_RES_OK , date , accept ); <nl> 
Object * object_resolve_path_component ( Object * parent , const gchar * part ) <nl> } <nl>  <nl> if ( object_property_is_link ( prop )) { <nl> - return *( Object **) prop -> opaque ; <nl> + LinkProperty * lprop = prop -> opaque ; <nl> + return * lprop -> child ; <nl> } else if ( object_property_is_child ( prop )) { <nl> return prop -> opaque ; <nl> } else {
write_refblocks : <nl> * this will leak that range , but we can easily fix that by running <nl> * a leak - fixing check after this rebuild operation */ <nl> reftable_offset = - 1 ; <nl> + } else { <nl> + assert ( on_disk_reftable ); <nl> } <nl> on_disk_reftable [ refblock_index ] = refblock_offset ; <nl>  <nl> write_refblocks : <nl> goto write_refblocks ; <nl> } <nl>  <nl> - assert ( on_disk_reftable ); <nl> - <nl> for ( refblock_index = 0 ; refblock_index < reftable_size ; refblock_index ++) { <nl> cpu_to_be64s (& on_disk_reftable [ refblock_index ]); <nl> }
int v9fs_co_st_gen ( V9fsPDU * pdu , V9fsPath * path , mode_t st_mode , <nl> }); <nl> v9fs_path_unlock ( s ); <nl> } <nl> + /* The ioctl may not be supported depending on the path */ <nl> + if ( err == - ENOTTY ) { <nl> + err = 0 ; <nl> + } <nl> return err ; <nl> } <nl> 
static struct iovec * lock_iovec ( int type , abi_ulong target_addr , <nl> errno = 0 ; <nl> return NULL ; <nl> } <nl> - if ( count > IOV_MAX ) { <nl> + if ( count < 0 || count > IOV_MAX ) { <nl> errno = EINVAL ; <nl> return NULL ; <nl> }
void init_paths ( const char * prefix ) <nl> base = new_entry ("", NULL , pref_buf ); <nl> base = add_dir_maybe ( base ); <nl> if ( base -> num_entries == 0 ) { <nl> - free ( base ); <nl> + g_free ( base -> pathname ); <nl> + free ( base -> name ); <nl> + free ( base ); <nl> base = NULL ; <nl> } else { <nl> set_parents ( base , base );
void do_unassigned_access ( target_phys_addr_t addr , int is_write , int is_exec , <nl> else <nl> raise_exception ( TT_DATA_ACCESS ); <nl> } <nl> - env = saved_env ; <nl>  <nl> /* flush neverland mappings created during no - fault mode , <nl> so the sequential MMU faults report proper fault types */ <nl> if ( env -> mmuregs [ 0 ] & MMU_NF ) { <nl> tlb_flush ( env , 1 ); <nl> } <nl> + <nl> + env = saved_env ; <nl> } <nl> # else <nl> void do_unassigned_access ( target_phys_addr_t addr , int is_write , int is_exec ,
static void vhost_user_cleanup ( NetClientState * nc ) <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> s -> vhost_net = NULL ; <nl> } <nl> + if ( s -> chr ) { <nl> + qemu_chr_add_handlers ( s -> chr , NULL , NULL , NULL , NULL ); <nl> + qemu_chr_fe_release ( s -> chr ); <nl> + s -> chr = NULL ; <nl> + } <nl>  <nl> qemu_purge_queued_packets ( nc ); <nl> }
int pci_bridge_initfn ( PCIDevice * dev ) <nl> br -> bus_name ); <nl> sec_bus -> parent_dev = dev ; <nl> sec_bus -> map_irq = br -> map_irq ; <nl> + /* TODO : use memory API to perform memory filtering . */ <nl> + sec_bus -> address_space_mem = parent -> address_space_mem ; <nl> + sec_bus -> address_space_io = parent -> address_space_io ; <nl>  <nl> QLIST_INIT (& sec_bus -> child ); <nl> QLIST_INSERT_HEAD (& parent -> child , sec_bus , sibling );
static int usb_msd_initfn ( USBDevice * dev ) <nl> usb_msd_handle_reset ( dev ); <nl>  <nl> if ( bdrv_key_required ( s -> conf . dinfo -> bdrv )) { <nl> - if ( s -> dev . qdev . hotplugged ) { <nl> + if ( cur_mon ) { <nl> monitor_read_bdrv_key_start ( cur_mon , s -> conf . dinfo -> bdrv , <nl> usb_msd_password_cb , s ); <nl> s -> dev . auto_attach = 0 ;
static int vmdk_write_extent ( VmdkExtent * extent , int64_t cluster_offset , <nl> goto out ; <nl> } <nl>  <nl> - data -> lba = offset >> BDRV_SECTOR_BITS ; <nl> - data -> size = buf_len ; <nl> + data -> lba = cpu_to_le64 ( offset >> BDRV_SECTOR_BITS ); <nl> + data -> size = cpu_to_le32 ( buf_len ); <nl>  <nl> n_bytes = buf_len + sizeof ( VmdkGrainMarker ); <nl> iov = ( struct iovec ) {
int socket_dgram ( SocketAddress * remote , SocketAddress * local , Error ** errp ) <nl>  <nl> default : <nl> error_setg ( errp , " socket type unsupported for datagram "); <nl> - return - 1 ; <nl> + fd = - 1 ; <nl> } <nl> qemu_opts_del ( opts ); <nl> return fd ;
PCIDevice * virtio_net_init ( PCIBus * bus , NICInfo * nd , int devfn ) <nl> n -> promisc = 1 ; /* for compatibility */ <nl>  <nl> n -> mac_table . macs = qemu_mallocz ( MAC_TABLE_ENTRIES * ETH_ALEN ); <nl> - if (! n -> mac_table . macs ) <nl> - return NULL ; <nl>  <nl> n -> vlans = qemu_mallocz ( MAX_VLAN >> 3 ); <nl> - if (! n -> vlans ) <nl> - return NULL ; <nl>  <nl> register_savevm (" virtio - net ", virtio_net_id ++, VIRTIO_NET_VM_VERSION , <nl> virtio_net_save , virtio_net_load , n );
static void vscsi_process_login ( VSCSIState * s , vscsi_req * req ) <nl> struct srp_login_rsp * rsp = & iu -> srp . login_rsp ; <nl> uint64_t tag = iu -> srp . rsp . tag ; <nl>  <nl> - trace_spapr_vscsi__process_login (); <nl> + trace_spapr_vscsi_process_login (); <nl>  <nl> /* TODO handle case that requested size is wrong and <nl> * buffer format is wrong
static void timer_update_irq ( struct fs_timer_t * t ) <nl> qemu_irq_lower ( t -> irq [ 0 ]); <nl> } <nl>  <nl> - static void timer_hit ( struct fs_timer_t * t ) <nl> + static void timer_hit ( void * opaque ) <nl> { <nl> + struct fs_timer_t * t = opaque ; <nl> t -> r_intr |= 1 ; <nl> timer_update_irq ( t ); <nl> }
static int inc_refcounts ( BlockDriverState * bs , <nl> if ( refcount == s -> refcount_max ) { <nl> fprintf ( stderr , " ERROR : overflow cluster offset = 0x %" PRIx64 <nl> "\ n ", cluster_offset ); <nl> + fprintf ( stderr , " Use qemu - img amend to increase the refcount entry " <nl> + " width or qemu - img convert to create a clean copy if the " <nl> + " image cannot be opened for writing \ n "); <nl> res -> corruptions ++; <nl> continue ; <nl> }
static void dump_qobject ( fprintf_function func_fprintf , void * f , <nl> case QTYPE_QERROR : { <nl> QString * value = qerror_human (( QError *) obj ); <nl> func_fprintf ( f , "% s ", qstring_get_str ( value )); <nl> + QDECREF ( value ); <nl> break ; <nl> } <nl> case QTYPE_NONE :
static int qcow2_co_flush ( BlockDriverState * bs ) <nl> qemu_co_mutex_lock (& s -> lock ); <nl> ret = qcow2_cache_flush ( bs , s -> l2_table_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl>  <nl> ret = qcow2_cache_flush ( bs , s -> refcount_block_cache ); <nl> if ( ret < 0 ) { <nl> + qemu_co_mutex_unlock (& s -> lock ); <nl> return ret ; <nl> } <nl> qemu_co_mutex_unlock (& s -> lock );
int qemu_spice_set_pw_expire ( time_t expires ); <nl> int qemu_spice_migrate_info ( const char * hostname , int port , int tls_port , <nl> const char * subject ); <nl>  <nl> -# define SPICE_NEEDS_SET_MM_TIME \ <nl> - (! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 )) <nl> +# if ! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 ) <nl> +# define SPICE_NEEDS_SET_MM_TIME 1 <nl> +# else <nl> +# define SPICE_NEEDS_SET_MM_TIME 0 <nl> +# endif <nl>  <nl> # if SPICE_SERVER_VERSION >= 0x000c02 <nl> void qemu_spice_register_ports ( void );
static void spr_write_excp_prefix ( void * opaque , int sprn , int gprn ) <nl> tcg_gen_and_tl ( t0 , t0 , cpu_gpr [ gprn ]); <nl> tcg_gen_st_tl ( t0 , cpu_env , offsetof ( CPUState , excp_prefix )); <nl> gen_store_spr ( sprn , t0 ); <nl> + tcg_temp_free ( t0 ); <nl> } <nl>  <nl> static void spr_write_excp_vector ( void * opaque , int sprn , int gprn )
static int write_refcount_block_entries ( BlockDriverState * bs , <nl> return 0 ; <nl> } <nl>  <nl> + if ( first_index < 0 ) { <nl> + return 0 ; <nl> + } <nl> + <nl> first_index &= ~( REFCOUNTS_PER_SECTOR - 1 ); <nl> last_index = ( last_index + REFCOUNTS_PER_SECTOR ) <nl> & ~( REFCOUNTS_PER_SECTOR - 1 );
static uint64_t do_cvttq ( CPUAlphaState * env , uint64_t a , int roundmode ) <nl> frac = a & 0xfffffffffffffull ; <nl>  <nl> if ( exp == 0 ) { <nl> - if ( unlikely ( frac != 0 )) { <nl> + if ( unlikely ( frac != 0 ) && ! env -> fp_status . flush_inputs_to_zero ) { <nl> goto do_underflow ; <nl> } <nl> } else if ( exp == 0x7ff ) {
static void isa_irq_handler ( void * opaque , int n , int level ) <nl> if ( n < 16 ) { <nl> qemu_set_irq ( isa -> i8259 [ n ], level ); <nl> } <nl> - qemu_set_irq ( isa -> ioapic [ n ], level ); <nl> + if ( isa -> ioapic ) <nl> + qemu_set_irq ( isa -> ioapic [ n ], level ); <nl> }; <nl>  <nl> static void ioport80_write ( void * opaque , uint32_t addr , uint32_t data )
static void openrisc_pic_cpu_handler ( void * opaque , int irq , int level ) <nl> { <nl> OpenRISCCPU * cpu = ( OpenRISCCPU *) opaque ; <nl> CPUState * cs = CPU ( cpu ); <nl> - uint32_t irq_bit = 1 << irq ; <nl> + uint32_t irq_bit ; <nl>  <nl> if ( irq > 31 || irq < 0 ) { <nl> return ; <nl> } <nl>  <nl> + irq_bit = 1U << irq ; <nl> + <nl> if ( level ) { <nl> cpu -> env . picsr |= irq_bit ; <nl> } else {
void qcow2_free_clusters ( BlockDriverState * bs , <nl> ret = update_refcount ( bs , offset , size , - 1 ); <nl> if ( ret < 0 ) { <nl> fprintf ( stderr , " qcow2_free_clusters failed : % s \ n ", strerror (- ret )); <nl> - abort (); <nl> + /* TODO Remember the clusters to free them later and avoid leaking */ <nl> } <nl> } <nl> 
static bool cmd_smart ( IDEState * s , uint8_t cmd ) <nl> case 2 : /* extended self test */ <nl> s -> smart_selftest_count ++; <nl> if ( s -> smart_selftest_count > 21 ) { <nl> - s -> smart_selftest_count = 0 ; <nl> + s -> smart_selftest_count = 1 ; <nl> } <nl> n = 2 + ( s -> smart_selftest_count - 1 ) * 24 ; <nl> s -> smart_selftest_data [ n ] = s -> sector ;
print_insn_sparc ( bfd_vma memaddr , disassemble_info * info ) <nl> } <nl>  <nl> info -> insn_type = dis_noninsn ; /* Mark as non - valid instruction . */ <nl> - (* info -> fprintf_func ) ( stream , _ (" unknown ")); <nl> + (* info -> fprintf_func ) ( stream , ". long %# 08lx ", insn ); <nl> return sizeof ( buffer ); <nl> }
void hmp_info_local_apic ( Monitor * mon , const QDict * qdict ) <nl>  <nl> void hmp_info_io_apic ( Monitor * mon , const QDict * qdict ) <nl> { <nl> - if ( kvm_irqchip_in_kernel ()) { <nl> + if ( kvm_irqchip_in_kernel () && <nl> + ! kvm_irqchip_is_split ()) { <nl> kvm_ioapic_dump_state ( mon , qdict ); <nl> } else { <nl> ioapic_dump_state ( mon , qdict );
struct TranslationBlock ; <nl> typedef struct TranslationBlock TranslationBlock ; <nl>  <nl> /* XXX : make safe guess about sizes */ <nl> -# if ( HOST_LONG_BITS == 32 ) && ( TARGET_LONG_BITS == 64 ) <nl> -# define MAX_OP_PER_INSTR 128 <nl> -# else <nl> -# define MAX_OP_PER_INSTR 96 <nl> -# endif <nl> +# define MAX_OP_PER_INSTR 208 <nl>  <nl> # if HOST_LONG_BITS == 32 <nl> # define MAX_OPC_PARAM_PER_ARG 2
int main ( int argc , char ** argv ) <nl>  <nl> process_requests ( sock ); <nl> error : <nl> + g_free ( rpath ); <nl> + g_free ( sock_name ); <nl> do_log ( LOG_INFO , " Done \ n "); <nl> closelog (); <nl> return 0 ;
static CharDriverState * create_eventfd_chr_device ( IVShmemState * s , <nl> int vector ) <nl> { <nl> /* create a event character device based on the passed eventfd */ <nl> - PCIDevice * pdev = PCI_DEVICE ( s ); <nl> int eventfd = event_notifier_get_fd ( n ); <nl> CharDriverState * chr ; <nl>  <nl> - s -> msi_vectors [ vector ]. pdev = pdev ; <nl> - <nl> chr = qemu_chr_open_eventfd ( eventfd ); <nl>  <nl> if ( chr == NULL ) {
static int coroutine_fn bdrv_aligned_preadv ( BlockDriverState * bs , <nl> } <nl>  <nl> max_bytes = ROUND_UP ( MAX ( 0 , total_bytes - offset ), align ); <nl> - if ( bytes < max_bytes ) { <nl> + if ( bytes <= max_bytes ) { <nl> ret = bdrv_driver_preadv ( bs , offset , bytes , qiov , 0 ); <nl> } else if ( max_bytes > 0 ) { <nl> QEMUIOVector local_qiov ;
void replay_configure ( QemuOpts * opts ) <nl> rr = qemu_opt_get ( opts , " rr "); <nl> if (! rr ) { <nl> /* Just enabling icount */ <nl> - return ; <nl> + goto out ; <nl> } else if (! strcmp ( rr , " record ")) { <nl> mode = REPLAY_MODE_RECORD ; <nl> } else if (! strcmp ( rr , " replay ")) { <nl> void replay_configure ( QemuOpts * opts ) <nl>  <nl> replay_enable ( fname , mode ); <nl>  <nl> + out : <nl> loc_pop (& loc ); <nl> } <nl> 
void watchdog_perform_action ( void ) <nl> exit ( 0 ); <nl>  <nl> case WDT_PAUSE : /* same as ' stop ' command in monitor */ <nl> + /* In a timer callback , when vm_stop calls qemu_clock_enable <nl> + * you would get a deadlock . Bypass the problem . <nl> + */ <nl> + qemu_system_vmstop_request_prepare (); <nl> qapi_event_send_watchdog ( WATCHDOG_EXPIRATION_ACTION_PAUSE , & error_abort ); <nl> - vm_stop ( RUN_STATE_WATCHDOG ); <nl> + qemu_system_vmstop_request ( RUN_STATE_WATCHDOG ); <nl> break ; <nl>  <nl> case WDT_DEBUG :
static void ncq_cb ( void * opaque , int ret ) <nl> NCQTransferState * ncq_tfs = ( NCQTransferState *) opaque ; <nl> IDEState * ide_state = & ncq_tfs -> drive -> port . ifs [ 0 ]; <nl>  <nl> + ncq_tfs -> aiocb = NULL ; <nl> if ( ret == - ECANCELED ) { <nl> return ; <nl> }
static int scsi_disk_emulate_inquiry ( SCSIRequest * req , uint8_t * outbuf ) <nl> } <nl>  <nl> l = strlen ( s -> serial ); <nl> - if ( l > 20 ) { <nl> - l = 20 ; <nl> + if ( l > 36 ) { <nl> + l = 36 ; <nl> } <nl>  <nl> DPRINTF (" Inquiry EVPD [ Serial number ] "
static void tcp_chr_tls_handshake ( QIOTask * task , <nl> if ( qio_task_propagate_error ( task , NULL )) { <nl> tcp_chr_disconnect ( chr ); <nl> } else { <nl> - /* tn3270 does not support TLS yet */ <nl> - if ( s -> do_telnetopt && ! s -> is_tn3270 ) { <nl> + if ( s -> do_telnetopt ) { <nl> tcp_chr_telnet_init ( chr ); <nl> } else { <nl> tcp_chr_connect ( chr );
static int qcow2_amend_options ( BlockDriverState * bs , QemuOpts * opts , <nl> } <nl>  <nl> if ( new_size ) { <nl> - ret = bdrv_truncate ( bs , new_size ); <nl> + BlockBackend * blk = blk_new (); <nl> + blk_insert_bs ( blk , bs ); <nl> + ret = blk_truncate ( blk , new_size ); <nl> + blk_unref ( blk ); <nl> + <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
static void qemu_rdma_cleanup ( RDMAContext * rdma ) <nl> rdma_destroy_event_channel ( rdma -> channel ); <nl> rdma -> channel = NULL ; <nl> } <nl> + g_free ( rdma -> host ); <nl> + rdma -> host = NULL ; <nl> } <nl>  <nl> 
static PixelFormat sdl_to_qemu_pixelformat ( SDL_PixelFormat * sdl_pf ) <nl> static DisplaySurface * sdl_create_displaysurface ( int width , int height ) <nl> { <nl> DisplaySurface * surface = ( DisplaySurface *) g_malloc0 ( sizeof ( DisplaySurface )); <nl> - if ( surface == NULL ) { <nl> - fprintf ( stderr , " sdl_create_displaysurface : malloc failed \ n "); <nl> - exit ( 1 ); <nl> - } <nl>  <nl> surface -> width = width ; <nl> surface -> height = height ;
static int cpu_post_load ( void * opaque , int version_id ) <nl>  <nl> # if defined ( TARGET_PPC64 ) <nl> if ( cpu -> compat_pvr ) { <nl> + uint32_t compat_pvr = cpu -> compat_pvr ; <nl> Error * local_err = NULL ; <nl>  <nl> - ppc_set_compat ( cpu , cpu -> compat_pvr , & local_err ); <nl> + cpu -> compat_pvr = 0 ; <nl> + ppc_set_compat ( cpu , compat_pvr , & local_err ); <nl> if ( local_err ) { <nl> error_report_err ( local_err ); <nl> return - 1 ;
static void digic_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = digic_realize ; <nl> + /* Reason : Uses serial_hds in the realize function --> not usable twice */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo digic_type_info = {
static target_ulong disas_insn ( CPUX86State * env , DisasContext * s , <nl> } <nl> /* fallthru */ <nl> case 0xf9 ... 0xff : /* sfence */ <nl> + if (!( s -> cpuid_features & CPUID_SSE ) <nl> + || ( prefixes & PREFIX_LOCK )) { <nl> + goto illegal_op ; <nl> + } <nl> + break ; <nl> case 0xe8 ... 0xef : /* lfence */ <nl> case 0xf0 ... 0xf7 : /* mfence */ <nl> if (!( s -> cpuid_features & CPUID_SSE2 )
static void build_guest_fsinfo_for_virtual_device ( char const * syspath , <nl> dirpath = g_strdup_printf ("% s / slaves ", syspath ); <nl> dir = opendir ( dirpath ); <nl> if (! dir ) { <nl> - error_setg_errno ( errp , errno , " opendir (\"% s \")", dirpath ); <nl> + if ( errno != ENOENT ) { <nl> + error_setg_errno ( errp , errno , " opendir (\"% s \")", dirpath ); <nl> + } <nl> g_free ( dirpath ); <nl> return ; <nl> }
static int spapr_vty_init ( VIOsPAPRDevice * sdev ) <nl> { <nl> VIOsPAPRVTYDevice * dev = ( VIOsPAPRVTYDevice *) sdev ; <nl>  <nl> + if (! dev -> chardev ) { <nl> + fprintf ( stderr , " spapr - vty : Can ' t create vty without a chardev !\ n "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> qemu_chr_add_handlers ( dev -> chardev , vty_can_receive , <nl> vty_receive , NULL , dev ); <nl> 
static void hmp_migrate_status_cb ( void * opaque ) <nl> MigrationInfo * info ; <nl>  <nl> info = qmp_query_migrate ( NULL ); <nl> - if (! info -> has_status || strcmp ( info -> status , " active ") == 0 ) { <nl> + if (! info -> has_status || strcmp ( info -> status , " active ") == 0 || <nl> + strcmp ( info -> status , " setup ") == 0 ) { <nl> if ( info -> has_disk ) { <nl> int progress ; <nl> 
long do_rt_sigreturn ( CPUM68KState * env ) <nl> { <nl> struct target_rt_sigframe * frame ; <nl> abi_ulong frame_addr = env -> aregs [ 7 ] - 4 ; <nl> - target_sigset_t target_set ; <nl> sigset_t set ; <nl>  <nl> trace_user_do_rt_sigreturn ( env , frame_addr ); <nl> if (! lock_user_struct ( VERIFY_READ , frame , frame_addr , 1 )) <nl> goto badframe ; <nl>  <nl> - target_to_host_sigset_internal (& set , & target_set ); <nl> + target_to_host_sigset (& set , & frame -> uc . tuc_sigmask ); <nl> set_sigmask (& set ); <nl>  <nl> /* restore registers */
void target_set_brk ( abi_ulong new_brk ) <nl> abi_long do_brk ( abi_ulong new_brk ) <nl> { <nl> abi_long mapped_addr ; <nl> - int new_alloc_size ; <nl> + abi_ulong new_alloc_size ; <nl>  <nl> DEBUGF_BRK (" do_brk (" TARGET_ABI_FMT_lx ") -> ", new_brk ); <nl> 
static int vvfat_write ( BlockDriverState * bs , int64_t sector_num , <nl>  <nl> DLOG ( checkpoint ()); <nl>  <nl> + /* Check if we ' re operating in read - only mode */ <nl> + if ( s -> qcow == NULL ) { <nl> + return - EACCES ; <nl> + } <nl> + <nl> vvfat_close_current_file ( s ); <nl>  <nl> /*
static void usb_ohci_init ( OHCIState * ohci , DeviceState * dev , <nl>  <nl> ohci -> as = as ; <nl>  <nl> + if ( num_ports > OHCI_MAX_PORTS ) { <nl> + error_setg ( errp , " OHCI num - ports =% d is too big ( limit is % d ports )", <nl> + num_ports , OHCI_MAX_PORTS ); <nl> + return ; <nl> + } <nl> + <nl> if ( usb_frame_time == 0 ) { <nl> # ifdef OHCI_TIME_WARP <nl> usb_frame_time = NANOSECONDS_PER_SECOND ;
static int read_cpuinfo ( const char * field , char * value , int len ) <nl> break ; <nl> } <nl> if (! strncmp ( line , field , field_len )) { <nl> - strncpy ( value , line , len ); <nl> + pstrcpy ( value , len , line ); <nl> ret = 0 ; <nl> break ; <nl> }
static void bdrv_dirty_bitmap_truncate ( BlockDriverState * bs ) <nl> continue ; <nl> } <nl> hbitmap_truncate ( bitmap -> bitmap , size ); <nl> + bitmap -> size = size ; <nl> } <nl> } <nl> 
int qemu_create_pidfile ( const char * filename ) <nl> return - 1 ; <nl> } <nl>  <nl> - close ( fd ); <nl> + /* keep pidfile open & locked forever */ <nl> return 0 ; <nl> }
ssize_t ne2000_receive ( NetClientState * nc , const uint8_t * buf , size_t size_ ) <nl> if ( index <= s -> stop ) <nl> avail = s -> stop - index ; <nl> else <nl> - avail = 0 ; <nl> + break ; <nl> len = size ; <nl> if ( len > avail ) <nl> len = avail ;
static void spapr_rng_class_init ( ObjectClass * oc , void * data ) <nl> dc -> realize = spapr_rng_realize ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> props = spapr_rng_properties ; <nl> + dc -> hotpluggable = false ; <nl> } <nl>  <nl> static const TypeInfo spapr_rng_info = {
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> update_displaychangelistener (& d -> ssd . dcl , GUI_REFRESH_INTERVAL_DEFAULT ); <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> + qemu_spice_display_switch (& d -> ssd , d -> ssd . ds ); <nl> vga_dirty_log_start (& d -> vga ); <nl> graphic_hw_update ( d -> vga . con ); <nl> }
void vfio_region_finalize ( VFIORegion * region ) <nl> g_free ( region -> mmaps ); <nl>  <nl> trace_vfio_region_finalize ( region -> vbasedev -> name , region -> nr ); <nl> + <nl> + region -> mem = NULL ; <nl> + region -> mmaps = NULL ; <nl> + region -> nr_mmaps = 0 ; <nl> + region -> size = 0 ; <nl> + region -> flags = 0 ; <nl> + region -> nr = 0 ; <nl> } <nl>  <nl> void vfio_region_mmaps_set_enabled ( VFIORegion * region , bool enabled )
static int aio_write_f ( BlockBackend * blk , int argc , char ** argv ) <nl> int64_t count = cvtnum ( argv [ optind ]); <nl> if ( count < 0 ) { <nl> print_cvtnum_err ( count , argv [ optind ]); <nl> + g_free ( ctx ); <nl> return 0 ; <nl> } <nl> 
static void build_pci_bus_end ( PCIBus * bus , void * bus_state ) <nl>  <nl> build_append_byte ( notify , 0x7B ); /* AndOp */ <nl> build_append_byte ( notify , 0x68 ); /* Arg0Op */ <nl> - build_append_int ( notify , 0x1 << i ); <nl> + build_append_int ( notify , 0x1U << i ); <nl> build_append_byte ( notify , 0x00 ); /* NullName */ <nl> build_append_byte ( notify , 0x86 ); /* NotifyOp */ <nl> build_append_nameseg ( notify , " S %. 02X_ ", PCI_DEVFN ( i , 0 ));
static void cuda_receive_packet ( CUDAState * s , <nl> } <nl> break ; <nl> default : <nl> + obuf [ 0 ] = ERROR_PACKET ; <nl> + obuf [ 1 ] = 0x2 ; <nl> + obuf [ 2 ] = CUDA_PACKET ; <nl> + obuf [ 3 ] = data [ 0 ]; <nl> + cuda_send_packet_to_host ( s , obuf , 4 ); <nl> break ; <nl> } <nl> }
static void win_stdio_close ( CharDriverState * chr ) <nl> } <nl>  <nl> g_free ( chr -> opaque ); <nl> - g_free ( chr ); <nl> } <nl>  <nl> static CharDriverState * qemu_chr_open_stdio ( const char * id ,
void microblaze_load_kernel ( MicroBlazeCPU * cpu , hwaddr ddr_base , <nl> big_endian , ELF_MACHINE , 0 ); <nl> } <nl> /* Always boot into physical ram . */ <nl> - boot_info . bootstrap_pc = ddr_base + ( entry & 0x0fffffff ); <nl> + boot_info . bootstrap_pc = ( uint32_t ) entry ; <nl>  <nl> /* If it wasn ' t an ELF image , try an u - boot image . */ <nl> if ( kernel_size < 0 ) {
static int ohci_bus_start ( OHCIState * ohci ) <nl> /* Stop sending SOF tokens on the bus */ <nl> static void ohci_bus_stop ( OHCIState * ohci ) <nl> { <nl> - if ( ohci -> eof_timer ) <nl> + if ( ohci -> eof_timer ) { <nl> timer_del ( ohci -> eof_timer ); <nl> + timer_free ( ohci -> eof_timer ); <nl> + } <nl> ohci -> eof_timer = NULL ; <nl> } <nl> 
static void setup_frame ( int sig , struct target_sigaction * ka , <nl>  <nl> long do_rt_sigreturn ( CPUARMState * env ) <nl> { <nl> - struct target_rt_sigframe * frame ; <nl> + struct target_rt_sigframe * frame = NULL ; <nl> abi_ulong frame_addr = env -> xregs [ 31 ]; <nl>  <nl> if ( frame_addr & 15 ) {
static bool ga_open_pidfile ( const char * pidfile ) <nl> int pidfd ; <nl> char pidstr [ 32 ]; <nl>  <nl> - pidfd = open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> + pidfd = qemu_open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> if ( pidfd == - 1 || lockf ( pidfd , F_TLOCK , 0 )) { <nl> g_critical (" Cannot lock pid file , % s ", strerror ( errno )); <nl> if ( pidfd != - 1 ) {
static void error_exit ( int err , const char * msg ) <nl> void qemu_mutex_init ( QemuMutex * mutex ) <nl> { <nl> int err ; <nl> - pthread_mutexattr_t mutexattr ; <nl>  <nl> - pthread_mutexattr_init (& mutexattr ); <nl> - pthread_mutexattr_settype (& mutexattr , PTHREAD_MUTEX_ERRORCHECK ); <nl> - err = pthread_mutex_init (& mutex -> lock , & mutexattr ); <nl> - pthread_mutexattr_destroy (& mutexattr ); <nl> + err = pthread_mutex_init (& mutex -> lock , NULL ); <nl> if ( err ) <nl> error_exit ( err , __func__ ); <nl> }
static target_long monitor_get_ccr ( const struct MonitorDef * md , int val ) <nl>  <nl> u = 0 ; <nl> for ( i = 0 ; i < 8 ; i ++) <nl> - u |= env -> crf [ i ] << ( 32 - ( 4 * i )); <nl> + u |= env -> crf [ i ] << ( 32 - ( 4 * ( i + 1 ))); <nl>  <nl> return u ; <nl> }
static uint64_t pit_ioport_read ( void * opaque , hwaddr addr , <nl> PITChannelState * s ; <nl>  <nl> addr &= 3 ; <nl> + <nl> + if ( addr == 3 ) { <nl> + /* Mode / Command register is write only , read is ignored */ <nl> + return 0 ; <nl> + } <nl> + <nl> s = & pit -> channels [ addr ]; <nl> if ( s -> status_latched ) { <nl> s -> status_latched = 0 ;
TranslationBlock * tb_gen_code ( CPUState * cpu , <nl> /* flush must be done */ <nl> tb_flush ( cpu ); <nl> mmap_unlock (); <nl> + /* Make the execution loop process the flush as soon as possible . */ <nl> + cpu -> exception_index = EXCP_INTERRUPT ; <nl> cpu_loop_exit ( cpu ); <nl> } <nl> 
static void loongarch_cpu_reset ( DeviceState * dev ) <nl>  <nl> # ifndef CONFIG_USER_ONLY <nl> env -> pc = 0x1c000000 ; <nl> + memset ( env -> tlb , 0 , sizeof ( env -> tlb )); <nl> # endif <nl>  <nl> restore_fp_status ( env );
static void handle_ti ( ESPState * s ) <nl> { <nl> uint32_t dmalen , minlen ; <nl>  <nl> + if ( s -> dma && ! s -> dma_enabled ) { <nl> + s -> dma_cb = handle_ti ; <nl> + return ; <nl> + } <nl> + <nl> dmalen = s -> rregs [ ESP_TCLO ] | ( s -> rregs [ ESP_TCMID ] << 8 ); <nl> if ( dmalen == 0 ) { <nl> dmalen = 0x10000 ;
static void cg3_realizefn ( DeviceState * dev , Error ** errp ) <nl> if ( fcode_filename ) { <nl> ret = load_image_targphys ( fcode_filename , s -> prom_addr , <nl> FCODE_MAX_ROM_SIZE ); <nl> + g_free ( fcode_filename ); <nl> if ( ret < 0 || ret > FCODE_MAX_ROM_SIZE ) { <nl> error_report (" cg3 : could not load prom '% s '", CG3_ROM_FILE ); <nl> }
static void vfio_put_device ( VFIOPCIDevice * vdev ) <nl> { <nl> g_free ( vdev -> vbasedev . name ); <nl> if ( vdev -> msix ) { <nl> + object_unparent ( OBJECT (& vdev -> msix -> mmap_mem )); <nl> g_free ( vdev -> msix ); <nl> vdev -> msix = NULL ; <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> break ; <nl> } <nl> # endif <nl> + case PR_GET_SECCOMP : <nl> + case PR_SET_SECCOMP : <nl> + /* Disable seccomp to prevent the target disabling syscalls we <nl> + * need . */ <nl> + ret = - TARGET_EINVAL ; <nl> + break ; <nl> default : <nl> /* Most prctl options have no pointer arguments */ <nl> ret = get_errno ( prctl ( arg1 , arg2 , arg3 , arg4 , arg5 ));
static void net_slirp_cleanup ( NetClientState * nc ) <nl> SlirpState * s = DO_UPCAST ( SlirpState , nc , nc ); <nl>  <nl> slirp_cleanup ( s -> slirp ); <nl> - qemu_remove_exit_notifier (& s -> exit_notifier ); <nl> + if ( s -> exit_notifier . notify ) { <nl> + qemu_remove_exit_notifier (& s -> exit_notifier ); <nl> + } <nl> slirp_smb_cleanup ( s ); <nl> QTAILQ_REMOVE (& slirp_stacks , s , entry ); <nl> }
static int count_contiguous_clusters ( uint64_t nb_clusters , int cluster_size , <nl> uint64_t * l2_table , uint64_t stop_flags ) <nl> { <nl> int i ; <nl> - uint64_t mask = stop_flags | L2E_OFFSET_MASK | QCOW2_CLUSTER_COMPRESSED ; <nl> + uint64_t mask = stop_flags | L2E_OFFSET_MASK | QCOW_OFLAG_COMPRESSED ; <nl> uint64_t first_entry = be64_to_cpu ( l2_table [ 0 ]); <nl> uint64_t offset = first_entry & mask ; <nl> 
TPMVersion tpm_tis_get_tpm_version ( Object * obj ) <nl> { <nl> TPMState * s = TPM ( obj ); <nl>  <nl> + if ( tpm_backend_had_startup_error ( s -> be_driver )) { <nl> + return TPM_VERSION_UNSPEC ; <nl> + } <nl> + <nl> return tpm_backend_get_tpm_version ( s -> be_driver ); <nl> } <nl> 
static int bad_mode_switch ( CPUARMState * env , int mode ) <nl> return ! arm_feature ( env , ARM_FEATURE_EL2 ) <nl> || arm_current_el ( env ) < 2 || arm_is_secure ( env ); <nl> case ARM_CPU_MODE_MON : <nl> - return ! arm_is_secure ( env ); <nl> + return arm_current_el ( env ) < 3 ; <nl> default : <nl> return 1 ; <nl> }
static void gen_sync ( DisasContext * ctx ) <nl> /* wait */ <nl> static void gen_wait ( DisasContext * ctx ) <nl> { <nl> - TCGv_i32 t0 = tcg_temp_new_i32 (); <nl> + TCGv_i32 t0 = tcg_const_i32 ( 1 ); <nl> tcg_gen_st_i32 ( t0 , cpu_env , <nl> - offsetof ( PowerPCCPU , env ) + offsetof ( CPUState , halted )); <nl> tcg_temp_free_i32 ( t0 );
DeviceState * qdev_device_add ( QemuOpts * opts ) <nl> } <nl> } <nl>  <nl> - if (! oc ) { <nl> + if (! object_class_dynamic_cast ( oc , TYPE_DEVICE )) { <nl> qerror_report ( QERR_INVALID_PARAMETER_VALUE , " driver ", " device type "); <nl> return NULL ; <nl> }
static void do_pci_unregister_device ( PCIDevice * pci_dev ) <nl> pci_dev -> bus -> devices [ pci_dev -> devfn ] = NULL ; <nl> pci_config_free ( pci_dev ); <nl>  <nl> - memory_region_del_subregion (& pci_dev -> bus_master_container_region , <nl> - & pci_dev -> bus_master_enable_region ); <nl> + if ( memory_region_is_mapped (& pci_dev -> bus_master_enable_region )) { <nl> + memory_region_del_subregion (& pci_dev -> bus_master_container_region , <nl> + & pci_dev -> bus_master_enable_region ); <nl> + } <nl> address_space_destroy (& pci_dev -> bus_master_as ); <nl> } <nl> 
static void s390_init ( ram_addr_t ram_size , <nl>  <nl> bios_filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> bios_size = load_image ( bios_filename , qemu_get_ram_ptr ( ZIPL_LOAD_ADDR )); <nl> + qemu_free ( bios_filename ); <nl>  <nl> if (( long ) bios_size < 0 ) { <nl> hw_error (" could not load bootloader '% s '\ n ", bios_name );
static void qemu_chr_free_common ( CharDriverState * chr ) <nl> if ( chr -> logfd != - 1 ) { <nl> close ( chr -> logfd ); <nl> } <nl> + qemu_mutex_destroy (& chr -> chr_write_lock ); <nl> g_free ( chr ); <nl> } <nl> 
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl> /* The snapshot list position has not yet been updated , so these clusters <nl> * must indeed be completely free */ <nl> ret = qcow2_pre_write_overlap_check ( bs , QCOW2_OL_DEFAULT , offset , <nl> - s -> snapshots_size ); <nl> + snapshots_size ); <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
static int css_interpret_ccw ( SubchDev * sch , hwaddr ccw_addr , <nl> if (! ccw_addr ) { <nl> return - EIO ; <nl> } <nl> + /* Check doubleword aligned and 31 or 24 ( fmt 0 ) bit addressable . */ <nl> + if ( ccw_addr & ( sch -> ccw_fmt_1 ? 0x80000007 : 0xff000007 )) { <nl> + return - EINVAL ; <nl> + } <nl>  <nl> /* Translate everything to format - 1 ccws - the information is the same . */ <nl> ccw = copy_ccw_from_guest ( ccw_addr , sch -> ccw_fmt_1 );
int load_elf ( const char * filename , uint64_t (* translate_fn )( void *, uint64_t ), <nl> target_data_order = ELFDATA2LSB ; <nl> } <nl>  <nl> - if ( target_data_order != e_ident [ EI_DATA ]) <nl> - return - 1 ; <nl> + if ( target_data_order != e_ident [ EI_DATA ]) { <nl> + goto fail ; <nl> + } <nl>  <nl> lseek ( fd , 0 , SEEK_SET ); <nl> if ( e_ident [ EI_CLASS ] == ELFCLASS64 ) {
char * vnc_display_local_addr ( const char * id ) <nl> { <nl> VncDisplay * vs = vnc_display_find ( id ); <nl>  <nl> + assert ( vs ); <nl> return vnc_socket_local_addr ("% s :% s ", vs -> lsock ); <nl> } <nl> 
typedef struct RAMBlock { <nl> static inline void * ramblock_ptr ( RAMBlock * block , ram_addr_t offset ) <nl> { <nl> assert ( offset < block -> length ); <nl> + assert ( block -> host ); <nl> return ( char *) block -> host + offset ; <nl> } <nl> 
int qemu_file_rate_limit ( QEMUFile * f ) <nl>  <nl> size_t qemu_file_set_rate_limit ( QEMUFile * f , size_t new_rate ) <nl> { <nl> - if ( f -> set_rate_limit ) <nl> + /* any failed or completed migration keeps its state to allow probing of <nl> + * migration data , but has no associated file anymore */ <nl> + if ( f && f -> set_rate_limit ) <nl> return f -> set_rate_limit ( f -> opaque , new_rate ); <nl>  <nl> return 0 ;
void nand_setio ( DeviceState * dev , uint32_t value ) <nl>  <nl> if ( s -> ale ) { <nl> unsigned int shift = s -> addrlen * 8 ; <nl> - unsigned int mask = ~( 0xff << shift ); <nl> - unsigned int v = value << shift ; <nl> + uint64_t mask = ~( 0xffull << shift ); <nl> + uint64_t v = ( uint64_t ) value << shift ; <nl>  <nl> s -> addr = ( s -> addr & mask ) | v ; <nl> s -> addrlen ++;
static void tap_cleanup ( VLANClientState * nc ) <nl>  <nl> if ( s -> vhost_net ) { <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> + s -> vhost_net = NULL ; <nl> } <nl>  <nl> qemu_purge_queued_packets ( nc );
static void disas_s390_insn ( DisasContext * s ) <nl> store_reg32 ( r1 , tmp32_1 ); <nl> tcg_gen_trunc_i64_i32 ( tmp32_2 , tmp2 ); <nl> store_reg32 ( r1 + 1 , tmp32_2 ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> + tcg_temp_free_i64 ( tmp2 ); <nl> break ; <nl> case 0x98 : /* LM R1 , R3 , D2 ( B2 ) [ RS ] */ <nl> case 0x90 : /* STM R1 , R3 , D2 ( B2 ) [ RS ] */
void fork_end ( int child ) <nl> Discard information about the parent threads . */ <nl> CPU_FOREACH_SAFE ( cpu , next_cpu ) { <nl> if ( cpu != thread_cpu ) { <nl> - QTAILQ_REMOVE (& cpus , thread_cpu , node ); <nl> + QTAILQ_REMOVE (& cpus , cpu , node ); <nl> } <nl> } <nl> pending_cpus = 0 ;
static void entropy_available ( void * opaque ) <nl> ssize_t len ; <nl>  <nl> len = read ( s -> fd , buffer , s -> size ); <nl> + if ( len < 0 && errno == EAGAIN ) { <nl> + return ; <nl> + } <nl> g_assert ( len != - 1 ); <nl>  <nl> s -> receive_func ( s -> opaque , buffer , len );
int main ( int argc , char ** argv ) <nl>  <nl> path = g_strdup_printf (" eepro100 /% s ", models [ i ]); <nl> qtest_add_data_func ( path , models [ i ], test_device ); <nl> + g_free ( path ); <nl> } <nl>  <nl> return g_test_run ();
# include " exec / cpu - defs . h " <nl> # define TARGET_PAGE_BITS 12 <nl>  <nl> -# define TARGET_PHYS_ADDR_SPACE_BITS 64 <nl> +/* Actually 64 - bits , limited by the memory API to 62 bits . We <nl> + * never use that much . <nl> + */ <nl> +# define TARGET_PHYS_ADDR_SPACE_BITS 62 <nl> # define TARGET_VIRT_ADDR_SPACE_BITS 64 <nl>  <nl> # include " exec / cpu - all . h "
static always_inline void gen_store_gpr64 ( int reg , TCGv t ) { <nl> tcg_gen_mov_i64 ( cpu_gpr [ reg ], t ); <nl> # else <nl> tcg_gen_trunc_i64_i32 ( cpu_gpr [ reg ], t ); <nl> - TCGv tmp = tcg_temp_local_new ( TCG_TYPE_I64 ); <nl> + TCGv tmp = tcg_temp_new ( TCG_TYPE_I64 ); <nl> tcg_gen_shri_i64 ( tmp , t , 32 ); <nl> tcg_gen_trunc_i64_i32 ( cpu_gprh [ reg ], tmp ); <nl> tcg_temp_free ( tmp );
void coroutine_fn qemu_coroutine_yield ( void ) <nl> } <nl>  <nl> self -> caller = NULL ; <nl> - coroutine_swap ( self , to ); <nl> + qemu_coroutine_switch ( self , to , COROUTINE_YIELD ); <nl> }
static void tcg_liveness_analysis ( TCGContext * s ) <nl>  <nl> nb_ops = gen_opc_ptr - gen_opc_buf ; <nl>  <nl> - /* XXX : make it really dynamic */ <nl> - s -> op_dead_iargs = tcg_malloc ( OPC_BUF_SIZE * sizeof ( uint16_t )); <nl> + s -> op_dead_iargs = tcg_malloc ( nb_ops * sizeof ( uint16_t )); <nl>  <nl> dead_temps = tcg_malloc ( s -> nb_temps ); <nl> memset ( dead_temps , 1 , s -> nb_temps );
static int enable_write_target ( BDRVVVFATState * s , Error ** errp ) <nl> set_option_parameter ( options , BLOCK_OPT_BACKING_FILE , " fat :"); <nl>  <nl> ret = bdrv_create ( bdrv_qcow , s -> qcow_filename , options , errp ); <nl> + free_option_parameters ( options ); <nl> if ( ret < 0 ) { <nl> goto err ; <nl> }
size_t ram_control_save_page ( QEMUFile * f , ram_addr_t block_offset , <nl> offset , size , bytes_sent ); <nl>  <nl> if ( ret != RAM_SAVE_CONTROL_DELAYED ) { <nl> - if (* bytes_sent > 0 ) { <nl> + if ( bytes_sent && * bytes_sent > 0 ) { <nl> qemu_update_position ( f , * bytes_sent ); <nl> } else if ( ret < 0 ) { <nl> qemu_file_set_error ( f , ret );
static void vhost_scsi_unrealize ( DeviceState * dev , Error ** errp ) <nl> /* This will stop vhost backend . */ <nl> vhost_scsi_set_status ( vdev , 0 ); <nl>  <nl> + vhost_dev_cleanup (& s -> dev ); <nl> g_free ( s -> dev . vqs ); <nl>  <nl> virtio_scsi_common_unrealize ( dev , errp );
static inline void bitmap_directory_to_be ( uint8_t * dir , size_t size ) <nl>  <nl> static void bitmap_free ( Qcow2Bitmap * bm ) <nl> { <nl> + if ( bm == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> g_free ( bm -> name ); <nl> g_free ( bm ); <nl> }
static int xen_platform_initfn ( PCIDevice * dev ) <nl> PCIXenPlatformState * d = XEN_PLATFORM ( dev ); <nl> uint8_t * pci_conf ; <nl>  <nl> + /* Device will crash on reset if xen is not initialized */ <nl> + assert ( xen_enabled ()); <nl> + <nl> pci_conf = dev -> config ; <nl>  <nl> pci_set_word ( pci_conf + PCI_COMMAND , PCI_COMMAND_IO | PCI_COMMAND_MEMORY );
static void dump_map_entry ( OutputFormat output_format , MapEntry * e , <nl> ( e -> flags & BDRV_BLOCK_ZERO ) ? " true " : " false ", <nl> ( e -> flags & BDRV_BLOCK_DATA ) ? " true " : " false "); <nl> if ( e -> flags & BDRV_BLOCK_OFFSET_VALID ) { <nl> - printf (", ' offset ': %" PRId64 "", e -> offset ); <nl> + printf (", \" offset \": %" PRId64 "", e -> offset ); <nl> } <nl> putchar ('}'); <nl> 
static int parse_drive ( DeviceState * dev , Property * prop , const char * str ) <nl> static int print_drive ( DeviceState * dev , Property * prop , char * dest , size_t len ) <nl> { <nl> DriveInfo ** ptr = qdev_get_prop_ptr ( dev , prop ); <nl> - return snprintf ( dest , len , "% s ", (* ptr )-> id ); <nl> + return snprintf ( dest , len , "% s ", (* ptr ) ? (* ptr )-> id : "< null >"); <nl> } <nl>  <nl> PropertyInfo qdev_prop_drive = {
static void spapr_tce_reset ( DeviceState * dev ) <nl> sPAPRTCETable * tcet = SPAPR_TCE_TABLE ( dev ); <nl> size_t table_size = tcet -> nb_table * sizeof ( uint64_t ); <nl>  <nl> - memset ( tcet -> table , 0 , table_size ); <nl> + if ( tcet -> nb_table ) { <nl> + memset ( tcet -> table , 0 , table_size ); <nl> + } <nl> } <nl>  <nl> static target_ulong put_tce_emu ( sPAPRTCETable * tcet , target_ulong ioba ,
static sd_rsp_type_t sd_app_command ( SDState * sd , <nl> } <nl>  <nl> fprintf ( stderr , " SD : ACMD % i in a wrong state \ n ", req . cmd ); <nl> - return sd_r0 ; <nl> + return sd_illegal ; <nl> } <nl>  <nl> static int cmd_valid_while_locked ( SDState * sd , SDRequest * req )
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> + if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> + return - EINVAL ; <nl> + } <nl> return 0 ; <nl> } <nl> 
void qemu_mutex_lock_iothread ( void ) <nl> * TCG code execution . <nl> */ <nl> if (! tcg_enabled () || qemu_in_vcpu_thread () || <nl> - ! first_cpu || ! first_cpu -> thread ) { <nl> + ! first_cpu || ! first_cpu -> created ) { <nl> qemu_mutex_lock (& qemu_global_mutex ); <nl> atomic_dec (& iothread_requesting_mutex ); <nl> } else {
CharDriverState * qemu_chr_open ( const char * label , const char * filename , void (* i <nl> if ( chr && qemu_opt_get_bool ( opts , " mux ", 0 )) { <nl> monitor_init ( chr , MONITOR_USE_READLINE ); <nl> } <nl> + qemu_opts_del ( opts ); <nl> return chr ; <nl> } <nl> 
void cuda_init ( int * cuda_mem_index , qemu_irq irq ) <nl>  <nl> s -> timers [ 1 ]. index = 1 ; <nl>  <nl> - qemu_get_timedate (& tm , RTC_OFFSET ); <nl> - s -> tick_offset = mktimegm (& tm ); <nl> + qemu_get_timedate (& tm , 0 ); <nl> + s -> tick_offset = ( uint32_t ) mktimegm (& tm ) + RTC_OFFSET ; <nl>  <nl> s -> adb_poll_timer = qemu_new_timer ( vm_clock , cuda_adb_poll , s ); <nl> * cuda_mem_index = cpu_register_io_memory ( 0 , cuda_read , cuda_write , s );
glue ( glue ( cirrus_bitblt_rop_bkwd_transp_ , ROP_NAME ), _16 )( CirrusVGAState * s , <nl> srcpitch += bltwidth ; <nl> for ( y = 0 ; y < bltheight ; y ++) { <nl> for ( x = 0 ; x < bltwidth ; x += 2 ) { <nl> - ROP_OP_TR_16 ( s , dstaddr , cirrus_src16 ( s , srcaddr ), transp ); <nl> + ROP_OP_TR_16 ( s , dstaddr - 1 , cirrus_src16 ( s , srcaddr - 1 ), transp ); <nl> dstaddr -= 2 ; <nl> srcaddr -= 2 ; <nl> }
static int32_t scsi_send_command ( SCSIDevice * d , uint32_t tag , <nl> uint8_t * cmd , int lun ) <nl> { <nl> SCSIDeviceState * s = d -> state ; <nl> - uint32_t len ; <nl> - int cmdlen ; <nl> + uint32_t len = 0 ; <nl> + int cmdlen = 0 ; <nl> SCSIRequest * r ; <nl> int ret ; <nl> 
int qdev_device_help ( QemuOpts * opts ) <nl> return 1 ; <nl> } <nl>  <nl> - if (! qemu_opt_get ( opts , "?")) { <nl> + if (! driver || ! qemu_opt_get ( opts , "?")) { <nl> return 0 ; <nl> } <nl> 
static void test_smram_lock ( void ) <nl> g_assert ( smram_test_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN ) == false ); <nl> smram_set_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN , true ); <nl> g_assert ( smram_test_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN ) == true ); <nl> + <nl> + g_free ( pcidev ); <nl> + qpci_free_pc ( pcibus ); <nl> } <nl>  <nl> int main ( int argc , char ** argv )
static int megasas_dcmd_ld_get_info ( MegasasState * s , MegasasCmd * cmd ) <nl>  <nl> static int megasas_dcmd_cfg_read ( MegasasState * s , MegasasCmd * cmd ) <nl> { <nl> - uint8_t data [ 4096 ]; <nl> + uint8_t data [ 4096 ] = { 0 }; <nl> struct mfi_config_data * info ; <nl> int num_pd_disks = 0 , array_offset , ld_offset ; <nl> BusChild * kid ;
void object_property_set_qobject ( Object * obj , QObject * value , <nl> const char * name , Error ** errp ) <nl> { <nl> Visitor * v ; <nl> - /* TODO : Should we reject , rather than ignore , excess input ? */ <nl> - v = qobject_input_visitor_new ( value , false ); <nl> + <nl> + v = qobject_input_visitor_new ( value , true ); <nl> object_property_set ( obj , v , name , errp ); <nl> visit_free ( v ); <nl> }
static void tci_out_label ( TCGContext * s , TCGArg arg ) <nl> assert ( label -> u . value ); <nl> } else { <nl> tcg_out_reloc ( s , s -> code_ptr , sizeof ( tcg_target_ulong ), arg , 0 ); <nl> - tcg_out_i ( s , 0 ); <nl> + s -> code_ptr += sizeof ( tcg_target_ulong ); <nl> } <nl> } <nl> 
static int slirp_smb ( SlirpState * s , const char * exported_dir , <nl> fprintf ( f , <nl> "[ global ]\ n " <nl> " private dir =% s \ n " <nl> - " smb ports = 0 \ n " <nl> " socket address = 127 . 0 . 0 . 1 \ n " <nl> " pid directory =% s \ n " <nl> " lock directory =% s \ n "
retry : <nl> goto retry ; <nl> } <nl> } <nl> + <nl> + /* Make sure that all offsets in the " allocated " range are representable <nl> + * in an int64_t */ <nl> + if ( s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) { <nl> + return - EFBIG ; <nl> + } <nl> + <nl> # ifdef DEBUG_ALLOC2 <nl> fprintf ( stderr , " alloc_clusters : size =%" PRId64 " -> %" PRId64 "\ n ", <nl> size ,
void acpi_setup ( PcGuestInfo * guest_info ) <nl> return ; <nl> } <nl>  <nl> + if (! acpi_enabled ) { <nl> + ACPI_BUILD_DPRINTF ( 3 , " ACPI disabled . Bailing out .\ n "); <nl> + return ; <nl> + } <nl> + <nl> build_state = g_malloc0 ( sizeof * build_state ); <nl>  <nl> build_state -> guest_info = guest_info ;
void qmp_drive_mirror ( const char * device , const char * target , <nl> if (! source && sync == MIRROR_SYNC_MODE_TOP ) { <nl> sync = MIRROR_SYNC_MODE_FULL ; <nl> } <nl> + if ( sync == MIRROR_SYNC_MODE_NONE ) { <nl> + source = bs ; <nl> + } <nl>  <nl> size = bdrv_getlength ( bs ); <nl> if ( size < 0 ) {
SCSIRequest * scsi_req_new ( SCSIDevice * d , uint32_t tag , uint32_t lun , <nl> } else { <nl> trace_scsi_req_parsed ( d -> id , lun , tag , buf [ 0 ], <nl> cmd . mode , cmd . xfer ); <nl> - if ( req -> cmd . lba != - 1 ) { <nl> + if ( cmd . lba != - 1 ) { <nl> trace_scsi_req_parsed_lba ( d -> id , lun , tag , buf [ 0 ], <nl> cmd . lba ); <nl> }
void qemu_iovec_destroy ( QEMUIOVector * qiov ) <nl> { <nl> assert ( qiov -> nalloc != - 1 ); <nl>  <nl> + qemu_iovec_reset ( qiov ); <nl> g_free ( qiov -> iov ); <nl> + qiov -> nalloc = 0 ; <nl> + qiov -> iov = NULL ; <nl> } <nl>  <nl> void qemu_iovec_reset ( QEMUIOVector * qiov )
static void omap2_gpio_module_write ( void * opaque , target_phys_addr_t addr , <nl>  <nl> static uint32_t omap2_gpio_module_readp ( void * opaque , target_phys_addr_t addr ) <nl> { <nl> - return omap2_gpio_module_readp ( opaque , addr ) >> (( addr & 3 ) << 3 ); <nl> + return omap2_gpio_module_read ( opaque , addr & ~ 3 ) >> (( addr & 3 ) << 3 ); <nl> } <nl>  <nl> static void omap2_gpio_module_writep ( void * opaque , target_phys_addr_t addr ,
void qbus_free ( BusState * bus ) <nl> QLIST_REMOVE ( bus , sibling ); <nl> bus -> parent -> num_child_bus --; <nl> } <nl> + qemu_free (( void *) bus -> name ); <nl> if ( bus -> qdev_allocated ) { <nl> qemu_free ( bus ); <nl> }
static gboolean ga_channel_listen_accept ( GIOChannel * channel , <nl> ret = ga_channel_client_add ( c , client_fd ); <nl> if ( ret ) { <nl> g_warning (" error setting up connection "); <nl> + close ( client_fd ); <nl> goto out ; <nl> } <nl> accepted = true ;
int64_t throttle_compute_wait ( LeakyBucket * bkt ) <nl> /* If the main bucket is not full yet we still have to check the <nl> * burst bucket in order to enforce the burst limit */ <nl> if ( bkt -> burst_length > 1 ) { <nl> + assert ( bkt -> max > 0 ); /* see throttle_is_valid () */ <nl> extra = bkt -> burst_level - burst_bucket_size ; <nl> if ( extra > 0 ) { <nl> return throttle_do_compute_wait ( bkt -> max , extra );
static void do_test_equality ( bool expected , int _ , ...) <nl> g_assert ( qobject_is_equal ( args [ i ], args [ j ]) == expected ); <nl> } <nl> } <nl> + <nl> + g_free ( args ); <nl> } <nl>  <nl> # define check_equal (...) \
static int rtl8139_can_receive ( VLANClientState * nc ) <nl> } else { <nl> avail = MOD2 ( s -> RxBufferSize + s -> RxBufPtr - s -> RxBufAddr , <nl> s -> RxBufferSize ); <nl> - return ( avail == 0 || avail >= 1514 ); <nl> + return ( avail == 0 || avail >= 1514 || ( s -> IntrMask & RxOverflow )); <nl> } <nl> } <nl> 
static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail ; <nl> } <nl>  <nl> - s -> pagetable = qemu_blockalign ( bs , s -> max_table_entries * 4 ); <nl> + s -> pagetable = qemu_try_blockalign ( bs -> file , s -> max_table_entries * 4 ); <nl> + if ( s -> pagetable == NULL ) { <nl> + ret = - ENOMEM ; <nl> + goto fail ; <nl> + } <nl>  <nl> s -> bat_offset = be64_to_cpu ( dyndisk_header -> table_offset ); <nl> 
static target_ulong put_tce_emu ( sPAPRTCETable * tcet , target_ulong ioba , <nl> sPAPRTCE * tcep ; <nl>  <nl> if ( ioba >= tcet -> window_size ) { <nl> - hcall_dprintf (" spapr_vio_put_tce on out - of - boards IOBA 0x " <nl> + hcall_dprintf (" spapr_vio_put_tce on out - of - bounds IOBA 0x " <nl> TARGET_FMT_lx "\ n ", ioba ); <nl> return H_PARAMETER ; <nl> }
static void pci_qdev_unrealize ( DeviceState * dev , Error ** errp ) <nl> pc -> exit ( pci_dev ); <nl> } <nl>  <nl> + pci_device_deassert_intx ( pci_dev ); <nl> do_pci_unregister_device ( pci_dev ); <nl> } <nl> 
static void serial_update_parameters ( SerialState * s ) <nl> int speed , parity , data_bits , stop_bits , frame_size ; <nl> QEMUSerialSetParams ssp ; <nl>  <nl> - if ( s -> divider == 0 ) <nl> + if ( s -> divider == 0 || s -> divider > s -> baudbase ) { <nl> return ; <nl> + } <nl>  <nl> /* Start bit . */ <nl> frame_size = 1 ;
static void v9fs_post_lcreate ( V9fsState * s , V9fsLcreateState * vs , int err ) <nl> err = vs -> offset ; <nl> } else { <nl> vs -> fidp -> fid_type = P9_FID_NONE ; <nl> - close ( vs -> fidp -> fs . fd ); <nl> err = - errno ; <nl> + if ( vs -> fidp -> fs . fd > 0 ) { <nl> + close ( vs -> fidp -> fs . fd ); <nl> + } <nl> } <nl>  <nl> complete_pdu ( s , vs -> pdu , err );
QGuestAllocator * pc_alloc_init ( void ) <nl> /* Respect PCI hole */ <nl> s -> end = MIN ( ram_size , 0xE0000000 ); <nl>  <nl> + /* clean - up */ <nl> + g_free ( fw_cfg ); <nl> + <nl> return & s -> alloc ; <nl> }
static QObject * qmp_output_pop ( QmpOutputVisitor * qov ) <nl> static QObject * qmp_output_first ( QmpOutputVisitor * qov ) <nl> { <nl> QStackEntry * e = QTAILQ_LAST (& qov -> stack , QStack ); <nl> + <nl> + /* FIXME - find a better way to deal with NULL values */ <nl> + if (! e ) { <nl> + return NULL ; <nl> + } <nl> + <nl> return e -> value ; <nl> } <nl> 
static void pc_fw_add_pflash_drv ( void ) <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl>  <nl> opts = drive_add ( IF_PFLASH , - 1 , filename , " readonly = on "); <nl> + <nl> + g_free ( filename ); <nl> + <nl> if ( opts == NULL ) { <nl> return ; <nl> }
int unix_connect_opts ( QemuOpts * opts ) <nl> snprintf ( un . sun_path , sizeof ( un . sun_path ), "% s ", path ); <nl> if ( connect ( sock , ( struct sockaddr *) & un , sizeof ( un )) < 0 ) { <nl> fprintf ( stderr , " connect ( unix :% s ): % s \ n ", path , strerror ( errno )); <nl> + close ( sock ); <nl> return - 1 ; <nl> } <nl> 
static int img_bench ( int argc , char ** argv ) <nl> BlockBackend * blk = NULL ; <nl> BenchData data = {}; <nl> int flags = 0 ; <nl> - bool writethrough ; <nl> + bool writethrough = false ; <nl> struct timeval t1 , t2 ; <nl> int i ; <nl> 
static QemuOptsList nbd_runtime_opts = { <nl> . type = QEMU_OPT_STRING , <nl> . help = " ID of the TLS credentials to use ", <nl> }, <nl> + { /* end of list */ } <nl> }, <nl> }; <nl> 
static int ide_drive_pio_post_load ( void * opaque , int version_id ) <nl> { <nl> IDEState * s = opaque ; <nl>  <nl> - if ( s -> end_transfer_fn_idx > ARRAY_SIZE ( transfer_end_table )) { <nl> + if ( s -> end_transfer_fn_idx >= ARRAY_SIZE ( transfer_end_table )) { <nl> return - EINVAL ; <nl> } <nl> s -> end_transfer_func = transfer_end_table [ s -> end_transfer_fn_idx ];
static ssize_t rtl8139_do_receive ( NetClientState * nc , const uint8_t * buf , size_t <nl> s -> IntrStatus |= RxOverflow ; <nl> ++ s -> RxMissed ; <nl> rtl8139_update_irq ( s ); <nl> - return size_ ; <nl> + return 0 ; <nl> } <nl>  <nl> packet_header |= RxStatusOK ;
static void gen_rot_rm_im ( DisasContext * s , int ot , int op1 , int op2 , <nl> if ( is_right ) { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask - 1 ); <nl> tcg_gen_shri_tl ( cpu_cc_dst , cpu_T [ 0 ], mask ); <nl> + tcg_gen_andi_tl ( cpu_cc_dst , cpu_cc_dst , 1 ); <nl> } else { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask ); <nl> tcg_gen_andi_tl ( cpu_cc_dst , cpu_T [ 0 ], 1 );
fork_exec ( struct socket * so , const char * ex , int do_pty ) <nl> bind ( s , ( struct sockaddr *)& addr , addrlen ) < 0 || <nl> listen ( s , 1 ) < 0 ) { <nl> error_report (" Error : inet socket : % s ", strerror ( errno )); <nl> - closesocket ( s ); <nl> + if ( s >= 0 ) { <nl> + closesocket ( s ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static char * SocketAddress_to_str ( const char * prefix , SocketAddress * addr , <nl> return g_strdup_printf ("% sfd :% s % s ", prefix , addr -> u . fd . data -> str , <nl> is_listen ? ", server " : ""); <nl> break ; <nl> + case SOCKET_ADDRESS_KIND_VSOCK : <nl> + return g_strdup_printf ("% svsock :% s :% s ", prefix , <nl> + addr -> u . vsock . data -> cid , <nl> + addr -> u . vsock . data -> port ); <nl> default : <nl> abort (); <nl> }
# define TT_DPROT 0x6c <nl> # define TT_SPILL 0x80 <nl> # define TT_FILL 0xc0 <nl> -# define TT_WOTHER 0x10 <nl> +# define TT_WOTHER ( 1 << 5 ) <nl> # define TT_TRAP 0x100 <nl> # endif <nl> 
int qemu_acl_insert ( qemu_acl * acl , <nl>  <nl> if ( index <= 0 ) <nl> return - 1 ; <nl> - if ( index >= acl -> nentries ) <nl> + if ( index > acl -> nentries ) { <nl> return qemu_acl_append ( acl , deny , match ); <nl> - <nl> + } <nl>  <nl> entry = g_malloc ( sizeof (* entry )); <nl> entry -> match = g_strdup ( match );
fail : <nl> QDECREF ( bs -> options ); <nl> QDECREF ( options ); <nl> bs -> options = NULL ; <nl> + bs -> explicit_options = NULL ; <nl> bdrv_unref ( bs ); <nl> error_propagate ( errp , local_err ); <nl> return NULL ; <nl> static void bdrv_close ( BlockDriverState * bs ) <nl> QDECREF ( bs -> options ); <nl> QDECREF ( bs -> explicit_options ); <nl> bs -> options = NULL ; <nl> + bs -> explicit_options = NULL ; <nl> QDECREF ( bs -> full_open_options ); <nl> bs -> full_open_options = NULL ; <nl> }
PCIDevice * pci_nic_init_nofail ( NICInfo * nd , PCIBus * rootbus , <nl>  <nl> res = pci_nic_init ( nd , rootbus , default_model , default_devaddr , & err ); <nl> if (! res ) { <nl> - error_report_err ( err ); <nl> + if ( err ) { <nl> + error_report_err ( err ); <nl> + } <nl> exit ( 1 ); <nl> } <nl> return res ;
static void pprint_data ( V9fsPDU * pdu , int rx , size_t * offsetp , const char * name ) <nl>  <nl> if ( rx ) { <nl> count = pdu -> elem . in_num ; <nl> - } else <nl> + } else { <nl> count = pdu -> elem . out_num ; <nl> } <nl> 
void ppc_tb_set_jmp_target ( unsigned long jmp_addr , unsigned long addr ); <nl> static inline void tb_set_jmp_target1 ( uintptr_t jmp_addr , uintptr_t addr ) <nl> { <nl> /* patch the branch destination */ <nl> - *( uint32_t *) jmp_addr = addr - ( jmp_addr + 4 ); <nl> + stl_p (( void *) jmp_addr , addr - ( jmp_addr + 4 )); <nl> /* no need to flush icache explicitly */ <nl> } <nl> # elif defined ( __aarch64__ )
static void pc87312_class_init ( ObjectClass * klass , void * data ) <nl> dc -> reset = pc87312_reset ; <nl> dc -> vmsd = & vmstate_pc87312 ; <nl> dc -> props = pc87312_properties ; <nl> + /* Reason : Uses parallel_hds [ 0 ] in realize (), so it can ' t be used twice */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo pc87312_type_info = {
void bdrv_detach_dev ( BlockDriverState * bs , void * dev ) <nl> bs -> dev = NULL ; <nl> bs -> dev_ops = NULL ; <nl> bs -> dev_opaque = NULL ; <nl> + bs -> buffer_alignment = 512 ; <nl> } <nl>  <nl> /* TODO change to return DeviceState * when all users are qdevified */
static int get_S2prot ( CPUARMState * env , int s2ap , int xn ) <nl> prot |= PAGE_WRITE ; <nl> } <nl> if (! xn ) { <nl> - prot |= PAGE_EXEC ; <nl> + if ( arm_el_is_aa64 ( env , 2 ) || prot & PAGE_READ ) { <nl> + prot |= PAGE_EXEC ; <nl> + } <nl> } <nl> return prot ; <nl> }
static int usbnet_can_receive ( NetClientState * nc ) <nl> { <nl> USBNetState * s = qemu_get_nic_opaque ( nc ); <nl>  <nl> + if (! s -> dev . config ) { <nl> + return 0 ; <nl> + } <nl> + <nl> if ( is_rndis ( s ) && s -> rndis_state != RNDIS_DATA_INITIALIZED ) { <nl> return 1 ; <nl> }
void qmp_blockdev_change_medium ( const char * device , const char * filename , <nl> } <nl>  <nl> bdrv_flags = blk_get_open_flags_from_root_state ( blk ); <nl> + bdrv_flags &= ~( BDRV_O_TEMPORARY | BDRV_O_SNAPSHOT | BDRV_O_NO_BACKING | <nl> + BDRV_O_PROTOCOL ); <nl>  <nl> if (! has_read_only ) { <nl> read_only = BLOCKDEV_CHANGE_READ_ONLY_MODE_RETAIN ;
static int proxy_init ( FsContext * ctx ) <nl> sock_id = atoi ( ctx -> fs_root ); <nl> if ( sock_id < 0 ) { <nl> fprintf ( stderr , " socket descriptor not initialized \ n "); <nl> + g_free ( proxy ); <nl> return - 1 ; <nl> } <nl> } <nl> g_free ( ctx -> fs_root ); <nl> + ctx -> fs_root = NULL ; <nl>  <nl> proxy -> in_iovec . iov_base = g_malloc ( PROXY_MAX_IO_SZ + PROXY_HDR_SZ ); <nl> proxy -> in_iovec . iov_len = PROXY_MAX_IO_SZ + PROXY_HDR_SZ ;
X86RegisterInfo32 x86_reg_info_32 [ CPU_NB_REGS32 ] = { <nl>  <nl> const char * get_register_name_32 ( unsigned int reg ) <nl> { <nl> - if ( reg > CPU_NB_REGS32 ) { <nl> + if ( reg >= CPU_NB_REGS32 ) { <nl> return NULL ; <nl> } <nl> return x86_reg_info_32 [ reg ]. name ;
static void lan9118_eeprom_cmd ( lan9118_state * s , int cmd , int addr ) <nl> } else { <nl> DPRINTF (" EEPROM Write All ( ignored )\ n "); <nl> } <nl> + break ; <nl> case 5 : /* ERASE */ <nl> if ( s -> eeprom_writable ) { <nl> s -> eeprom [ addr ] = 0xff ;
static int grow_refcount_table ( BlockDriverState * bs , int min_size ) <nl> qemu_free ( s -> refcount_table ); <nl> s -> refcount_table = new_table ; <nl> s -> refcount_table_size = new_table_size ; <nl> + s -> refcount_table_offset = table_offset ; <nl>  <nl> update_refcount ( bs , table_offset , new_table_size2 , 1 ); <nl> return 0 ;
int do_snapshot_blkdev ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> int ret = 0 ; <nl> int flags ; <nl>  <nl> + if (! filename ) { <nl> + qerror_report ( QERR_MISSING_PARAMETER , " snapshot_file "); <nl> + ret = - 1 ; <nl> + goto out ; <nl> + } <nl> + <nl> bs = bdrv_find ( device ); <nl> if (! bs ) { <nl> qerror_report ( QERR_DEVICE_NOT_FOUND , device );
static int ide_dev_initfn ( IDEDevice * dev , IDEDriveKind kind ) <nl> return - 1 ; <nl> } <nl>  <nl> + if ( dev -> conf . logical_block_size != 512 ) { <nl> + error_report (" logical_block_size must be 512 for IDE "); <nl> + return - 1 ; <nl> + } <nl> + <nl> blkconf_serial (& dev -> conf , & dev -> serial ); <nl> if ( kind != IDE_CD ) { <nl> blkconf_geometry (& dev -> conf , & dev -> chs_trans , 65536 , 16 , 255 , & err );
static always_inline int translate_one ( DisasContext * ctx , uint32_t insn ) <nl> break ; <nl> case 0x2C : <nl> /* XXX : incorrect */ <nl> - if ( fn11 == 0x2AC ) { <nl> + if ( fn11 == 0x2AC || fn11 == 0x6AC ) { <nl> /* CVTST */ <nl> gen_farith2 (& helper_cvtst , rb , rc ); <nl> } else {
static int slirp_smb ( SlirpState * s , const char * exported_dir , <nl> " state directory =% s \ n " <nl> " log file =% s / log . smbd \ n " <nl> " smb passwd file =% s / smbpasswd \ n " <nl> - " security = share \ n " <nl> + " security = user \ n " <nl> + " map to guest = Bad User \ n " <nl> "[ qemu ]\ n " <nl> " path =% s \ n " <nl> " read only = no \ n "
static void tcg_target_qemu_prologue ( TCGContext * s ) <nl> } <nl>  <nl> /* Call generated code */ <nl> - tcg_out_opc_reg ( s , OPC_JR , 0 , tcg_target_call_iarg_regs [ 1 ]), 0 ); <nl> + tcg_out_opc_reg ( s , OPC_JR , 0 , tcg_target_call_iarg_regs [ 1 ], 0 ); <nl> tcg_out_mov ( s , TCG_TYPE_PTR , TCG_AREG0 , tcg_target_call_iarg_regs [ 0 ]); <nl> tb_ret_addr = s -> code_ptr ; <nl> 
build_ssdt ( GArray * table_data , GArray * linker , <nl>  <nl> patch_pci_windows ( pci , ssdt_ptr , sizeof ( ssdp_misc_aml )); <nl>  <nl> - *( uint16_t *)( ssdt_ptr + * ssdt_isa_pest ) = <nl> - cpu_to_le16 ( misc -> pvpanic_port ); <nl> + ACPI_BUILD_SET_LE ( ssdt_ptr , sizeof ( ssdp_misc_aml ), <nl> + ssdt_isa_pest [ 0 ], 16 , misc -> pvpanic_port ); <nl>  <nl> { <nl> GArray * sb_scope = build_alloc_array ();
static int xen_pt_bar_reg_read ( XenPCIPassthroughState * s , XenPTReg * cfg_entry , <nl>  <nl> /* get BAR index */ <nl> index = xen_pt_bar_offset_to_index ( reg -> offset ); <nl> - if ( index < 0 || index >= PCI_NUM_REGIONS ) { <nl> + if ( index < 0 || index >= PCI_NUM_REGIONS - 1 ) { <nl> XEN_PT_ERR (& s -> dev , " Internal error : Invalid BAR index [% d ].\ n ", index ); <nl> return - 1 ; <nl> }
void usb_ehci_realize ( EHCIState * s , DeviceState * dev , Error ** errp ) <nl> NB_PORTS ); <nl> return ; <nl> } <nl> + if ( s -> maxframes < 8 || s -> maxframes > 512 ) { <nl> + error_setg ( errp , " maxframes % d out if range ( 8 .. 512 )", <nl> + s -> maxframes ); <nl> + return ; <nl> + } <nl>  <nl> usb_bus_new (& s -> bus , sizeof ( s -> bus ), s -> companion_enable ? <nl> & ehci_bus_ops_companion : & ehci_bus_ops_standalone , dev );
static void breakpoint_invalidate ( CPUState * cpu , target_ulong pc ) <nl> # else <nl> static void breakpoint_invalidate ( CPUState * cpu , target_ulong pc ) <nl> { <nl> - tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( cpu , pc ) | <nl> - ( pc & ~ TARGET_PAGE_MASK )); <nl> + hwaddr phys = cpu_get_phys_page_debug ( cpu , pc ); <nl> + if ( phys != - 1 ) { <nl> + tb_invalidate_phys_addr ( phys | ( pc & ~ TARGET_PAGE_MASK )); <nl> + } <nl> } <nl> # endif <nl> # endif /* TARGET_HAS_ICE */
static ExitStatus op_ex ( DisasContext * s , DisasOps * o ) <nl> TCGv_i64 tmp ; <nl>  <nl> update_psw_addr ( s ); <nl> - update_cc_op ( s ); <nl> + gen_op_calc_cc ( s ); <nl>  <nl> tmp = tcg_const_i64 ( s -> next_pc ); <nl> gen_helper_ex ( cc_op , cpu_env , cc_op , o -> in1 , o -> in2 , tmp ); <nl> tcg_temp_free_i64 ( tmp ); <nl>  <nl> - set_cc_static ( s ); <nl> return NO_EXIT ; <nl> } <nl> 
abi_long target_mmap ( abi_ulong start , abi_ulong len , int prot , <nl> goto fail ; <nl> if (!( prot & PROT_WRITE )) { <nl> ret = target_mprotect ( start , len , prot ); <nl> - if ( ret != 0 ) { <nl> - start = ret ; <nl> - goto the_end ; <nl> - } <nl> + assert ( ret == 0 ); <nl> } <nl> goto the_end ; <nl> }
static int blk_send_response_one ( struct ioreq * ioreq ) <nl> break ; <nl> default : <nl> dst = NULL ; <nl> + return 0 ; <nl> } <nl> memcpy ( dst , & resp , sizeof ( resp )); <nl> blkdev -> rings . common . rsp_prod_pvt ++;
void ahci_realize ( AHCIState * s , DeviceState * qdev , AddressSpace * as , int ports ) <nl> ad -> port . dma -> ops = & ahci_dma_ops ; <nl> ide_register_restart_cb (& ad -> port ); <nl> } <nl> + g_free ( irqs ); <nl> } <nl>  <nl> void ahci_uninit ( AHCIState * s )
void pcie_aer_root_init ( PCIDevice * dev ) <nl> PCI_ERR_ROOT_CMD_EN_MASK ); <nl> pci_set_long ( dev -> w1cmask + pos + PCI_ERR_ROOT_STATUS , <nl> PCI_ERR_ROOT_STATUS_REPORT_MASK ); <nl> + /* PCI_ERR_ROOT_IRQ is RO but devices change it using a <nl> + * device - specific method . <nl> + */ <nl> + pci_set_long ( dev -> cmask + pos + PCI_ERR_ROOT_STATUS , <nl> + ~ PCI_ERR_ROOT_IRQ ); <nl> } <nl>  <nl> void pcie_aer_root_reset ( PCIDevice * dev )
static void gen_load_fp ( DisasContext * s , int opsize , TCGv addr , TCGv_ptr fp ) <nl> case OS_DOUBLE : <nl> tcg_gen_qemu_ld64 ( t64 , addr , index ); <nl> gen_helper_extf64 ( cpu_env , fp , t64 ); <nl> - tcg_temp_free_i64 ( t64 ); <nl> break ; <nl> case OS_EXTENDED : <nl> if ( m68k_feature ( s -> env , M68K_FEATURE_CF_FPU )) {
static void ioapic_class_init ( ObjectClass * klass , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( klass ); <nl>  <nl> k -> realize = ioapic_realize ; <nl> + /* <nl> + * If APIC is in kernel , we need to update the kernel cache after <nl> + * migration , otherwise first 24 gsi routes will be invalid . <nl> + */ <nl> + k -> post_load = ioapic_update_kvm_routes ; <nl> dc -> reset = ioapic_reset_common ; <nl> dc -> props = ioapic_properties ; <nl> }
void test_clone ( void ) <nl> CLONE_VM | CLONE_FS | CLONE_FILES | SIGCHLD , " hello2 ")); <nl>  <nl> while ( waitpid ( pid1 , & status1 , 0 ) != pid1 ); <nl> + free ( stack1 ); <nl> while ( waitpid ( pid2 , & status2 , 0 ) != pid2 ); <nl> + free ( stack2 ); <nl> if ( thread1_res != 5 || <nl> thread2_res != 6 ) <nl> error (" clone ");
int64_t qcow2_alloc_bytes ( BlockDriverState * bs , int size ) <nl> return new_cluster ; <nl> } <nl>  <nl> + if ( new_cluster == 0 ) { <nl> + qcow2_signal_corruption ( bs , true , - 1 , - 1 , " Preventing invalid " <nl> + " allocation of compressed cluster " <nl> + " at offset 0 "); <nl> + return - EIO ; <nl> + } <nl> + <nl> if (! offset || ROUND_UP ( offset , s -> cluster_size ) != new_cluster ) { <nl> offset = new_cluster ; <nl> free_in_cluster = s -> cluster_size ;
int main ( int argc , char ** argv ) <nl> " - device ipmi - bmc - extern , chardev = ipmi0 , id = bmc0 " <nl> " - device isa - ipmi - bt , bmc = bmc0 ", emu_port ); <nl> qtest_start ( cmdline ); <nl> + g_free ( cmdline ); <nl> qtest_irq_intercept_in ( global_qtest , " ioapic "); <nl> qtest_add_func ("/ ipmi / extern / connect ", test_connect ); <nl> qtest_add_func ("/ ipmi / extern / bt_base ", test_bt_base );
static void zynq_xadc_write ( void * opaque , hwaddr offset , uint64_t val , <nl> break ; <nl> } <nl>  <nl> - if ( xadc_reg > ZYNQ_XADC_NUM_ADC_REGS && xadc_cmd != CMD_NOP ) { <nl> + if ( xadc_reg >= ZYNQ_XADC_NUM_ADC_REGS && xadc_cmd != CMD_NOP ) { <nl> qemu_log_mask ( LOG_GUEST_ERROR , " read / write op to invalid xadc " <nl> " reg 0x % x \ n ", xadc_reg ); <nl> break ;
static inline int kvmppc_remove_spapr_tce ( void * table , int pfd , <nl>  <nl> static inline int kvmppc_reset_htab ( int shift_hint ) <nl> { <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl>  <nl> static inline uint64_t kvmppc_rma_size ( uint64_t current_size ,
static inline void gen_bcond ( DisasContext * ctx , int type ) <nl> gen_update_nip ( ctx , ctx -> nip ); <nl> tcg_gen_exit_tb ( 0 ); <nl> } <nl> - if ( type == BCOND_LR || type == BCOND_CTR ) { <nl> + if ( type == BCOND_LR || type == BCOND_CTR || type == BCOND_TAR ) { <nl> tcg_temp_free ( target ); <nl> } <nl> }
DeviceState * qdev_device_add ( QemuOpts * opts ) <nl> return NULL ; <nl> } <nl>  <nl> + if ( object_class_is_abstract ( oc )) { <nl> + qerror_report ( QERR_INVALID_PARAMETER_VALUE , " driver ", <nl> + " non - abstract device type "); <nl> + return NULL ; <nl> + } <nl> + <nl> dc = DEVICE_CLASS ( oc ); <nl>  <nl> /* find bus */
int qdev_device_help ( QemuOpts * opts ) <nl> } <nl> } <nl>  <nl> - if (! klass ) { <nl> + if (! object_class_dynamic_cast ( klass , TYPE_DEVICE )) { <nl> return 0 ; <nl> } <nl> do {
static int aio_write_f ( int argc , char ** argv ) <nl> case ' P ': <nl> pattern = parse_pattern ( optarg ); <nl> if ( pattern < 0 ) { <nl> + free ( ctx ); <nl> return 0 ; <nl> } <nl> break ;
static inline int vfp_exceptbits_from_host ( int host_bits ) <nl> target_bits |= 2 ; <nl> if ( host_bits & float_flag_overflow ) <nl> target_bits |= 4 ; <nl> - if ( host_bits & float_flag_underflow ) <nl> + if ( host_bits & ( float_flag_underflow | float_flag_output_denormal )) <nl> target_bits |= 8 ; <nl> if ( host_bits & float_flag_inexact ) <nl> target_bits |= 0x10 ;
int load_image_targphys ( const char * filename , <nl> int size ; <nl>  <nl> size = get_image_size ( filename ); <nl> - if ( size > 0 ) <nl> + if ( size > max_sz ) { <nl> + return - 1 ; <nl> + } <nl> + if ( size > 0 ) { <nl> rom_add_file_fixed ( filename , addr , - 1 ); <nl> + } <nl> return size ; <nl> } <nl> 
int qcow2_pre_write_overlap_check ( BlockDriverState * bs , int ign , int64_t offset , <nl> offset , <nl> true , <nl> size , <nl> + true , <nl> & error_abort ); <nl> g_free ( message ); <nl> 
static int usb_msd_initfn_storage ( USBDevice * dev ) <nl> s -> conf . bootindex , dev -> serial , <nl> & err ); <nl> if (! scsi_dev ) { <nl> + error_report ("% s ", error_get_pretty ( err )); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> s -> bus . qbus . allow_hotplug = 0 ;
int qcow2_get_cluster_offset ( BlockDriverState * bs , uint64_t offset , <nl> break ; <nl> case QCOW2_CLUSTER_ZERO : <nl> if ( s -> qcow_version < 3 ) { <nl> + qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) & l2_table ); <nl> return - EIO ; <nl> } <nl> c = count_contiguous_clusters ( nb_clusters , s -> cluster_size ,
static ImageInfoSpecific * qcow2_get_specific_info ( BlockDriverState * bs ) <nl> . lazy_refcounts = s -> compatible_features & <nl> QCOW2_COMPAT_LAZY_REFCOUNTS , <nl> . has_lazy_refcounts = true , <nl> + . corrupt = s -> incompatible_features & <nl> + QCOW2_INCOMPAT_CORRUPT , <nl> + . has_corrupt = true , <nl> }; <nl> } <nl> 
void virtio_reset ( void * opaque ) <nl> vdev -> vq [ i ]. signalled_used_valid = false ; <nl> vdev -> vq [ i ]. notification = true ; <nl> vdev -> vq [ i ]. vring . num = vdev -> vq [ i ]. vring . num_default ; <nl> + vdev -> vq [ i ]. inuse = 0 ; <nl> } <nl> } <nl> 
void s390_machine_reset ( void ) <nl> { <nl> S390CPU * ipl_cpu = S390_CPU ( qemu_get_cpu ( 0 )); <nl>  <nl> - qemu_devices_reset (); <nl> s390_cmma_reset (); <nl> + qemu_devices_reset (); <nl> s390_crypto_reset (); <nl>  <nl> /* all cpus are stopped - configure and start the ipl cpu only */
static uint32_t cc_calc_abs_64 ( int64_t dst ) <nl> if (( uint64_t ) dst == 0x8000000000000000ULL ) { <nl> return 3 ; <nl> } else if ( dst ) { <nl> - return 1 ; <nl> + return 2 ; <nl> } else { <nl> return 0 ; <nl> } <nl> static uint32_t cc_calc_abs_32 ( int32_t dst ) <nl> if (( uint32_t ) dst == 0x80000000UL ) { <nl> return 3 ; <nl> } else if ( dst ) { <nl> - return 1 ; <nl> + return 2 ; <nl> } else { <nl> return 0 ; <nl> }
static int tftp_session_allocate ( Slirp * slirp , struct sockaddr_storage * srcsas , <nl>  <nl> found : <nl> memset ( spt , 0 , sizeof (* spt )); <nl> - spt -> client_addr = * srcsas ; <nl> + memcpy (& spt -> client_addr , srcsas , sockaddr_size ( srcsas )); <nl> spt -> fd = - 1 ; <nl> spt -> block_size = 512 ; <nl> spt -> client_port = tp -> udp . uh_sport ;
static void aw_a10_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = aw_a10_realize ; <nl> + /* Reason : Uses serial_hds in realize and nd_table in instance_init */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo aw_a10_type_info = {
void s390_init_cpus ( MachineState * machine ) <nl> machine -> cpu_model = " host "; <nl> } <nl>  <nl> - cpu_states = g_malloc0 ( sizeof ( S390CPU *) * max_cpus ); <nl> + cpu_states = g_new0 ( S390CPU *, max_cpus ); <nl>  <nl> for ( i = 0 ; i < max_cpus ; i ++) { <nl> name = g_strdup_printf (" cpu [% i ]", i );
static inline bool cptype_valid ( int cptype ) <nl> */ <nl> static inline int arm_current_el ( CPUARMState * env ) <nl> { <nl> + if ( arm_feature ( env , ARM_FEATURE_M )) { <nl> + return !(( env -> v7m . exception == 0 ) && ( env -> v7m . control & 1 )); <nl> + } <nl> + <nl> if ( is_a64 ( env )) { <nl> return extract32 ( env -> pstate , 2 , 2 ); <nl> }
void tcg_add_target_add_op_defs ( const TCGTargetOpDef * tdefs ) <nl> if ( tdefs -> op == ( TCGOpcode )- 1 ) <nl> break ; <nl> op = tdefs -> op ; <nl> - assert ( op >= 0 && op < NB_OPS ); <nl> + assert (( unsigned ) op < NB_OPS ); <nl> def = & tcg_op_defs [ op ]; <nl> # if defined ( CONFIG_DEBUG_TCG ) <nl> /* Duplicate entry in op definitions ? */
static inline int test_and_change_bit ( int nr , volatile unsigned long * addr ) <nl> { <nl> unsigned long mask = BIT_MASK ( nr ); <nl> unsigned long * p = (( unsigned long *) addr ) + BIT_WORD ( nr ); <nl> - unsigned long old ; <nl> + unsigned long old = * p ; <nl>  <nl> * p = old ^ mask ; <nl> return ( old & mask ) != 0 ;
static void aspeed_soc_class_init ( ObjectClass * oc , void * data ) <nl>  <nl> sc -> info = ( AspeedSoCInfo *) data ; <nl> dc -> realize = aspeed_soc_realize ; <nl> + /* Reason : Uses serial_hds and nd_table in realize () directly */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo aspeed_soc_type_info = {
static int qcow2_read_extensions ( BlockDriverState * bs , uint64_t start_offset , <nl> # ifdef DEBUG_EXT <nl> printf (" ext . magic = 0x % x \ n ", ext . magic ); <nl> # endif <nl> - if ( ext . len > end_offset - offset ) { <nl> + if ( offset > end_offset || ext . len > end_offset - offset ) { <nl> error_setg ( errp , " Header extension too large "); <nl> return - EINVAL ; <nl> }
int spapr_populate_pci_devices ( sPAPRPHBState * phb , <nl> uint64_t child ; <nl> uint64_t parent ; <nl> uint64_t size ; <nl> - } __attribute__ (( packed )) ranges [] = { <nl> + } QEMU_PACKED ranges [] = { <nl> { <nl> cpu_to_be32 ( b_ss ( 1 )), cpu_to_be64 ( 0 ), <nl> cpu_to_be64 ( phb -> io_win_addr ),
static int64_t ivshmem_recv_msg ( IVShmemState * s , int * pfd , Error ** errp ) <nl> } while ( n < sizeof ( msg )); <nl>  <nl> * pfd = qemu_chr_fe_get_msgfd (& s -> server_chr ); <nl> - return msg ; <nl> + return le64_to_cpu ( msg ); <nl> } <nl>  <nl> static void ivshmem_recv_setup ( IVShmemState * s , Error ** errp )
uint32_t lm4549_write_samples ( lm4549_state * s , uint32_t left , uint32_t right ) <nl> This model supports 16 - bit playback . <nl> */ <nl>  <nl> - if ( s -> buffer_level >= LM4549_BUFFER_SIZE ) { <nl> + if ( s -> buffer_level > LM4549_BUFFER_SIZE - 2 ) { <nl> DPRINTF (" write_sample Buffer full \ n "); <nl> return 0 ; <nl> }
static uint32_t get_cmd ( ESPState * s , uint8_t * buf , uint8_t buflen ) <nl> s -> dma_memory_read ( s -> dma_opaque , buf , dmalen ); <nl> } else { <nl> dmalen = s -> ti_size ; <nl> + if ( dmalen > TI_BUFSZ ) { <nl> + return 0 ; <nl> + } <nl> memcpy ( buf , s -> ti_buf , dmalen ); <nl> buf [ 0 ] = buf [ 2 ] >> 5 ; <nl> }
static uint64_t serial_ioport_read ( void * opaque , hwaddr addr , unsigned size ) <nl> ret = s -> divider & 0xff ; <nl> } else { <nl> if ( s -> fcr & UART_FCR_FE ) { <nl> - ret = fifo8_is_full (& s -> recv_fifo ) ? <nl> + ret = fifo8_is_empty (& s -> recv_fifo ) ? <nl> 0 : fifo8_pop (& s -> recv_fifo ); <nl> if ( s -> recv_fifo . num == 0 ) { <nl> s -> lsr &= ~( UART_LSR_DR | UART_LSR_BI );
static void cpu_class_init ( ObjectClass * oc , void * data ) <nl> k -> get_receive_mask = receive_mask ; <nl> k -> read_event_data = read_event_data ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> + /* <nl> + * Reason : raise_irq_cpu_hotplug () depends on an unique <nl> + * TYPE_SCLP_CPU_HOTPLUG device , which is already created <nl> + * by the sclp event facility <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo sclp_cpu_info = {
void vhost_dev_stop ( struct vhost_dev * hdev , VirtIODevice * vdev ) <nl>  <nl> hdev -> started = false ; <nl> qemu_free ( hdev -> log ); <nl> + hdev -> log = NULL ; <nl> hdev -> log_size = 0 ; <nl> }
static int cryptodev_builtin_create_cipher_session ( <nl> return - 1 ; <nl> } <nl> break ; <nl> + case VIRTIO_CRYPTO_CIPHER_AES_XTS : <nl> + mode = QCRYPTO_CIPHER_MODE_XTS ; <nl> + algo = cryptodev_builtin_get_aes_algo ( sess_info -> key_len , <nl> + mode , errp ); <nl> + if ( algo < 0 ) { <nl> + return - 1 ; <nl> + } <nl> + break ; <nl> case VIRTIO_CRYPTO_CIPHER_DES_ECB : <nl> mode = QCRYPTO_CIPHER_MODE_ECB ; <nl> algo = QCRYPTO_CIPHER_ALG_DES_RFB ;
static void build_guest_fsinfo_for_real_device ( char const * syspath , <nl> break ; <nl> } <nl>  <nl> + g_free ( driver ); <nl> if ( sscanf ( p , "/% x :% x :% x .% x % n ", <nl> pci , pci + 1 , pci + 2 , pci + 3 , & pcilen ) == 4 ) { <nl> p += pcilen ;
static TCGArg * tcg_constant_folding ( TCGContext * s , uint16_t * tcg_opc_ptr , <nl> switch ( op ) { <nl> CASE_OP_32_64 ( or ): <nl> CASE_OP_32_64 ( and ): <nl> - if ( args [ 1 ] == args [ 2 ]) { <nl> + if ( temps_are_copies ( args [ 1 ], args [ 2 ])) { <nl> if ( temps_are_copies ( args [ 0 ], args [ 1 ])) { <nl> gen_opc_buf [ op_index ] = INDEX_op_nop ; <nl> } else {
static int vmdk_parent_open ( BlockDriverState * bs , const char * filename ) <nl> p_name += sizeof (" parentFileNameHint ") + 1 ; <nl> if (( end_name = strchr ( p_name ,'\"')) == 0 ) <nl> return - 1 ; <nl> + if (( end_name - p_name ) > sizeof ( s -> hd -> backing_file ) - 1 ) <nl> + return - 1 ; <nl>  <nl> strncpy ( s -> hd -> backing_file , p_name , end_name - p_name ); <nl> if ( stat ( s -> hd -> backing_file , & file_buf ) != 0 ) {
static inline void futex_wait ( QemuEvent * ev , unsigned val ) <nl> # else <nl> static inline void futex_wake ( QemuEvent * ev , int n ) <nl> { <nl> + pthread_mutex_lock (& ev -> lock ); <nl> if ( n == 1 ) { <nl> pthread_cond_signal (& ev -> cond ); <nl> } else { <nl> pthread_cond_broadcast (& ev -> cond ); <nl> } <nl> + pthread_mutex_unlock (& ev -> lock ); <nl> } <nl>  <nl> static inline void futex_wait ( QemuEvent * ev , unsigned val )
void * virtqueue_pop ( VirtQueue * vq , size_t sz ) <nl>  <nl> max = vq -> vring . num ; <nl>  <nl> + if ( vq -> inuse >= vq -> vring . num ) { <nl> + error_report (" Virtqueue size exceeded "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> i = head = virtqueue_get_head ( vq , vq -> last_avail_idx ++); <nl> if ( virtio_vdev_has_feature ( vdev , VIRTIO_RING_F_EVENT_IDX )) { <nl> vring_set_avail_event ( vq , vq -> last_avail_idx );
qemu_irq * i8259_init ( ISABus * bus , qemu_irq parent_irq ) <nl> ISADevice * isadev ; <nl> int i ; <nl>  <nl> - irq_set = g_malloc ( ISA_NUM_IRQS * sizeof ( qemu_irq )); <nl> + irq_set = g_new0 ( qemu_irq , ISA_NUM_IRQS ); <nl>  <nl> isadev = i8259_init_chip ( TYPE_I8259 , bus , true ); <nl> dev = DEVICE ( isadev );
static int vhost_user_write ( struct vhost_dev * dev , VhostUserMsg * msg , <nl> return 0 ; <nl> } <nl>  <nl> - qemu_chr_fe_set_msgfds ( chr , fds , fd_num ); <nl> + if ( qemu_chr_fe_set_msgfds ( chr , fds , fd_num ) < 0 ) { <nl> + return - 1 ; <nl> + } <nl>  <nl> return qemu_chr_fe_write_all ( chr , ( const uint8_t *) msg , size ) == size ? <nl> 0 : - 1 ;
void stream_start ( const char * job_id , BlockDriverState * bs , <nl>  <nl> fail : <nl> if ( orig_bs_flags != bdrv_get_flags ( bs )) { <nl> - bdrv_reopen ( bs , s -> bs_flags , NULL ); <nl> + bdrv_reopen ( bs , orig_bs_flags , NULL ); <nl> } <nl> }
void virtqueue_discard ( VirtQueue * vq , const VirtQueueElement * elem , <nl> unsigned int len ) <nl> { <nl> vq -> last_avail_idx --; <nl> + vq -> inuse --; <nl> virtqueue_unmap_sg ( vq , elem , len ); <nl> } <nl> 
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static void scsi_cmd_xfer_mode ( SCSICommand * cmd ) <nl> case WRITE_LONG_10 : <nl> case WRITE_SAME_10 : <nl> case WRITE_SAME_16 : <nl> + case UNMAP : <nl> case SEARCH_HIGH_12 : <nl> case SEARCH_EQUAL_12 : <nl> case SEARCH_LOW_12 : <nl> static void scsi_cmd_xfer_mode ( SCSICommand * cmd ) <nl> case SEND_DVD_STRUCTURE : <nl> case PERSISTENT_RESERVE_OUT : <nl> case MAINTENANCE_OUT : <nl> + case ATA_PASSTHROUGH : <nl> cmd -> mode = SCSI_XFER_TO_DEV ; <nl> break ; <nl> default :
struct ICSState { <nl>  <nl> static inline bool ics_valid_irq ( ICSState * ics , uint32_t nr ) <nl> { <nl> - return ( nr >= ics -> offset ) <nl> + return ( ics -> offset != 0 ) && ( nr >= ics -> offset ) <nl> && ( nr < ( ics -> offset + ics -> nr_irqs )); <nl> } <nl> 
static void machvirt_init ( MachineState * machine ) <nl> } <nl>  <nl> object_property_set_bool ( cpuobj , true , " realized ", NULL ); <nl> + object_unref ( cpuobj ); <nl> } <nl> fdt_add_timer_nodes ( vms ); <nl> fdt_add_cpu_nodes ( vms );
static void external_snapshot_prepare ( BlkTransactionState * common , <nl> return ; <nl> } <nl>  <nl> - if ( has_snapshot_node_name && bdrv_find_node ( snapshot_node_name )) { <nl> - error_setg ( errp , " New snapshot node name already existing "); <nl> + if ( has_snapshot_node_name && <nl> + bdrv_lookup_bs ( snapshot_node_name , snapshot_node_name , NULL )) { <nl> + error_setg ( errp , " New snapshot node name already in use "); <nl> return ; <nl> } <nl> 
int spapr_ovec_populate_dt ( void * fdt , int fdt_offset , <nl> } <nl> } <nl>  <nl> - return fdt_setprop ( fdt , fdt_offset , name , vec , vec_len ); <nl> + return fdt_setprop ( fdt , fdt_offset , name , vec , vec_len + 1 ); <nl> }
fail : <nl>  <nl> if ( use_local_qiov ) { <nl> qemu_iovec_destroy (& local_qiov ); <nl> - qemu_vfree ( head_buf ); <nl> - qemu_vfree ( tail_buf ); <nl> } <nl> + qemu_vfree ( head_buf ); <nl> + qemu_vfree ( tail_buf ); <nl>  <nl> return ret ; <nl> }
static DriveInfo * blockdev_init ( QemuOpts * all_opts , <nl>  <nl> drv = bdrv_find_whitelisted_format ( buf , ro ); <nl> if (! drv ) { <nl> - error_report ("'% s ' invalid format ", buf ); <nl> + if (! ro && bdrv_find_whitelisted_format ( buf , ! ro )) { <nl> + error_report ("'% s ' can be only used as read - only device .", buf ); <nl> + } else { <nl> + error_report ("'% s ' invalid format ", buf ); <nl> + } <nl> return NULL ; <nl> } <nl> }
coroutine_fn iscsi_co_write_zeroes ( BlockDriverState * bs , int64_t sector_num , <nl> nb_blocks = sector_qemu2lun ( nb_sectors , iscsilun ); <nl>  <nl> if ( iscsilun -> zeroblock == NULL ) { <nl> - iscsilun -> zeroblock = g_malloc0 ( iscsilun -> block_size ); <nl> + iscsilun -> zeroblock = g_try_malloc0 ( iscsilun -> block_size ); <nl> + if ( iscsilun -> zeroblock == NULL ) { <nl> + return - ENOMEM ; <nl> + } <nl> } <nl>  <nl> iscsi_co_init_iscsitask ( iscsilun , & iTask );
static int bdrv_check_byte_request ( BlockDriverState * bs , int64_t offset , <nl> { <nl> int64_t len ; <nl>  <nl> + if ( size > INT_MAX ) { <nl> + return - EIO ; <nl> + } <nl> + <nl> if (! bdrv_is_inserted ( bs )) <nl> return - ENOMEDIUM ; <nl> 
int qdev_device_help ( QemuOpts * opts ) <nl> return 0 ; <nl> } <nl>  <nl> - if (! object_class_by_name ( driver )) { <nl> - const char * typename = find_typename_by_alias ( driver ); <nl> - <nl> - if ( typename ) { <nl> - driver = typename ; <nl> - } <nl> + qdev_get_device_class (& driver , & local_err ); <nl> + if ( local_err ) { <nl> + goto error ; <nl> } <nl>  <nl> prop_list = qmp_device_list_properties ( driver , & local_err );
SCSIDevice * scsi_bus_legacy_add_drive ( SCSIBus * bus , BlockDriverState * bdrv , <nl> if ( object_property_find ( OBJECT ( dev ), " removable ", NULL )) { <nl> qdev_prop_set_bit ( dev , " removable ", removable ); <nl> } <nl> - if ( serial ) { <nl> + if ( serial && object_property_find ( OBJECT ( dev ), " serial ", NULL )) { <nl> qdev_prop_set_string ( dev , " serial ", serial ); <nl> } <nl> if ( qdev_prop_set_drive ( dev , " drive ", bdrv ) < 0 ) {
DisplayState * init_displaystate ( void ) <nl> gchar * name ; <nl> int i ; <nl>  <nl> - if (! display_state ) { <nl> - display_state = g_new0 ( DisplayState , 1 ); <nl> - } <nl> - <nl> + get_alloc_displaystate (); <nl> for ( i = 0 ; i < nb_consoles ; i ++) { <nl> if ( consoles [ i ]-> console_type != GRAPHIC_CONSOLE && <nl> consoles [ i ]-> ds == NULL ) {
static FeatureWordInfo feature_word_info [ FEATURE_WORDS ] = { <nl> " ibpb ", NULL , NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> - NULL , " virt - ssbd ", NULL , NULL , <nl> + " amd - ssbd ", " virt - ssbd ", NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> }, <nl> . cpuid_eax = 0x80000008 ,
static void ahci_reset_port ( AHCIState * s , int port ) <nl> ncq_tfs -> aiocb = NULL ; <nl> } <nl>  <nl> + /* Maybe we just finished the request thanks to bdrv_aio_cancel () */ <nl> + if (! ncq_tfs -> used ) { <nl> + continue ; <nl> + } <nl> + <nl> qemu_sglist_destroy (& ncq_tfs -> sglist ); <nl> ncq_tfs -> used = 0 ; <nl> }
static void scsi_write_same_complete ( void * opaque , int ret ) <nl> data -> sector << BDRV_SECTOR_BITS , <nl> & data -> qiov , 0 , <nl> scsi_write_same_complete , data ); <nl> + aio_context_release ( blk_get_aio_context ( s -> qdev . conf . blk )); <nl> return ; <nl> } <nl> 
static QTAILQ_HEAD ( CharDriverStateHead , CharDriverState ) chardevs = <nl> CharDriverState * qemu_chr_alloc ( void ) <nl> { <nl> CharDriverState * chr = g_malloc0 ( sizeof ( CharDriverState )); <nl> + qemu_mutex_init (& chr -> chr_write_lock ); <nl> return chr ; <nl> } <nl> 
static void usb_msd_realize_storage ( USBDevice * dev , Error ** errp ) <nl> error_propagate ( errp , err ); <nl> return ; <nl> } <nl> - s -> bus . qbus . allow_hotplug = 0 ; <nl> usb_msd_handle_reset ( dev ); <nl>  <nl> if ( bdrv_key_required ( bs )) {
static void assign_failed_examine ( AssignedDevice * dev ) <nl> goto fail ; <nl> } <nl>  <nl> + driver [ r ] = 0 ; <nl> ns = strrchr ( driver , '/'); <nl> if (! ns ) { <nl> goto fail ;
static void blkverify_err ( BlkverifyAIOCB * acb , const char * fmt , ...) <nl> va_list ap ; <nl>  <nl> va_start ( ap , fmt ); <nl> - fprintf ( stderr , " blkverify : % s sector_num =% ld nb_sectors =% d ", <nl> + fprintf ( stderr , " blkverify : % s sector_num =%" PRId64 " nb_sectors =% d ", <nl> acb -> is_write ? " write " : " read ", acb -> sector_num , <nl> acb -> nb_sectors ); <nl> vfprintf ( stderr , fmt , ap );
static void i6300esb_restart_timer ( I6300State * d , int stage ) <nl> * multiply here can exceed 64 - bits , before we divide by 33MHz , so <nl> * we use a higher - precision intermediate result . <nl> */ <nl> - timeout = muldiv64 ( get_ticks_per_sec (), timeout , 33000000 ); <nl> + timeout = muldiv64 ( timeout , get_ticks_per_sec (), 33000000 ); <nl>  <nl> i6300esb_debug (" stage % d , timeout %" PRIi64 "\ n ", d -> stage , timeout ); <nl> 
static int disas_neon_data_insn ( CPUState * env , DisasContext * s , uint32_t insn ) <nl> } <nl> tmp3 = neon_load_reg ( rm , 1 ); <nl> gen_helper_neon_tbl ( tmp3 , tmp3 , tmp , tmp4 , tmp5 ); <nl> - dead_tmp ( tmp5 ); <nl> - dead_tmp ( tmp4 ); <nl> + tcg_temp_free_i32 ( tmp5 ); <nl> + tcg_temp_free_i32 ( tmp4 ); <nl> neon_store_reg ( rd , 0 , tmp2 ); <nl> neon_store_reg ( rd , 1 , tmp3 ); <nl> dead_tmp ( tmp );
static int kvmppc_read_host_property ( const char * node_path , const char * prop , <nl> { <nl> char * path ; <nl> FILE * f ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> int pathlen ; <nl>  <nl> pathlen = snprintf ( NULL , 0 , "% s /% s /% s ", PROC_DEVTREE_PATH , node_path , prop )
void unregister_displaychangelistener ( DisplayChangeListener * dcl ) <nl> dcl -> con -> dcls --; <nl> } <nl> QLIST_REMOVE ( dcl , next ); <nl> + dcl -> ds = NULL ; <nl> gui_setup_refresh ( ds ); <nl> } <nl> 
static void qvirtio_9p_pci_stop ( QVirtIO9P * v9p ) <nl> { <nl> qvirtqueue_cleanup ( v9p -> dev -> bus , v9p -> vq , v9p -> qs -> alloc ); <nl> qvirtio_pci_device_disable ( container_of ( v9p -> dev , QVirtioPCIDevice , vdev )); <nl> - g_free ( v9p -> dev ); <nl> + qvirtio_pci_device_free (( QVirtioPCIDevice *) v9p -> dev ); <nl> qvirtio_9p_stop ( v9p ); <nl> } <nl> 
static ssize_t nc_sendv_compat ( NetClientState * nc , const struct iovec * iov , <nl> offset = iov [ 0 ]. iov_len ; <nl> } else { <nl> buffer = buf ; <nl> - offset = iov_to_buf ( iov , iovcnt , 0 , buffer , sizeof ( buffer )); <nl> + offset = iov_to_buf ( iov , iovcnt , 0 , buf , sizeof ( buf )); <nl> } <nl>  <nl> if ( flags & QEMU_NET_PACKET_FLAG_RAW && nc -> info -> receive_raw ) {
static size_t qcrypto_hash_alg_size [ QCRYPTO_HASH_ALG__MAX ] = { <nl>  <nl> size_t qcrypto_hash_digest_len ( QCryptoHashAlgorithm alg ) <nl> { <nl> - if ( alg >= G_N_ELEMENTS ( qcrypto_hash_alg_size )) { <nl> - return 0 ; <nl> - } <nl> + assert ( alg < G_N_ELEMENTS ( qcrypto_hash_alg_size )); <nl> return qcrypto_hash_alg_size [ alg ]; <nl> } <nl> 
static inline TCGv iwmmxt_load_creg ( int reg ) <nl> static inline void iwmmxt_store_creg ( int reg , TCGv var ) <nl> { <nl> tcg_gen_st_i32 ( var , cpu_env , offsetof ( CPUState , iwmmxt . cregs [ reg ])); <nl> + dead_tmp ( var ); <nl> } <nl>  <nl> static inline void gen_op_iwmmxt_movq_wRn_M0 ( int rn ) <nl> static int disas_iwmmxt_insn ( CPUState * env , DisasContext * s , uint32_t insn ) <nl> } <nl> } <nl> } <nl> + dead_tmp ( addr ); <nl> return 0 ; <nl> } <nl> 
static void mirror_iteration_done ( MirrorOp * op , int ret ) <nl> bitmap_set ( s -> cow_bitmap , chunk_num , nb_chunks ); <nl> } <nl>  <nl> + qemu_iovec_destroy (& op -> qiov ); <nl> g_slice_free ( MirrorOp , op ); <nl> qemu_coroutine_enter ( s -> common . co , NULL ); <nl> }
void AcpiCpuHotplug_add ( ACPIGPE * gpe , AcpiCpuHotplug * g , CPUState * cpu ) <nl>  <nl> * gpe -> sts = * gpe -> sts | ACPI_CPU_HOTPLUG_STATUS ; <nl> cpu_id = k -> get_arch_id ( CPU ( cpu )); <nl> + g_assert (( cpu_id / 8 ) < ACPI_GPE_PROC_LEN ); <nl> g -> sts [ cpu_id / 8 ] |= ( 1 << ( cpu_id % 8 )); <nl> } <nl> 
static void bmdma_irq ( void * opaque , int n , int level ) <nl> return ; <nl> } <nl>  <nl> - if ( bm ) { <nl> - bm -> status |= BM_STATUS_INT ; <nl> - } <nl> + bm -> status |= BM_STATUS_INT ; <nl>  <nl> /* trigger the real irq */ <nl> qemu_set_irq ( bm -> irq , level );
static always_inline void gen_bcond ( DisasContext * ctx , int type ) <nl> # endif <nl> gen_op_btest_T1 ( ctx -> nip ); <nl> no_test : <nl> - if ( ctx -> singlestep_enabled & GDBSTUB_SINGLE_STEP ) { <nl> - gen_update_nip ( ctx , ctx -> nip ); <nl> - gen_op_debug (); <nl> - } <nl> tcg_gen_exit_tb ( 0 ); <nl> } <nl> }
static void json_message_process_token ( JSONLexer * lexer , QString * token , JSONTok <nl> parser -> bracket_count == 0 )) { <nl> goto out_emit ; <nl> } else if ( parser -> token_size > MAX_TOKEN_SIZE || <nl> - parser -> bracket_count > MAX_NESTING || <nl> - parser -> brace_count > MAX_NESTING ) { <nl> + parser -> bracket_count + parser -> brace_count > MAX_NESTING ) { <nl> /* Security consideration , we limit total memory allocated per object <nl> * and the maximum recursion depth that a message can force . <nl> */
int bdrv_pwrite_sync ( BlockDriverState * bs , int64_t offset , <nl> return ret ; <nl> } <nl>  <nl> - /* No flush needed for cache modes that already do it */ <nl> - if ( bs -> enable_write_cache ) { <nl> - bdrv_flush ( bs ); <nl> + ret = bdrv_flush ( bs ); <nl> + if ( ret < 0 ) { <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
void kvm_set_phys_mem ( target_phys_addr_t start_addr , <nl>  <nl> mem = kvm_lookup_slot ( s , start_addr ); <nl> if ( mem ) { <nl> - if ( flags == IO_MEM_UNASSIGNED ) { <nl> + if (( flags == IO_MEM_UNASSIGNED ) || ( flags >= TLB_MMIO )) { <nl> mem -> memory_size = 0 ; <nl> mem -> guest_phys_addr = start_addr ; <nl> mem -> userspace_addr = 0 ;
static int vhost_user_read ( struct vhost_dev * dev , VhostUserMsg * msg ) <nl>  <nl> r = qemu_chr_fe_read_all ( chr , p , size ); <nl> if ( r != size ) { <nl> - error_report (" Failed to read msg header . Read % d instead of % d .", r , <nl> - size ); <nl> + error_report (" Failed to read msg header . Read % d instead of % d ." <nl> + " Original request % d .", r , size , msg -> request ); <nl> goto fail ; <nl> } <nl> 
static void pci_info_device ( PCIBus * bus , PCIDevice * d ) <nl> base , limit ); <nl>  <nl> base = pci_bridge_get_base ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> - limit = pci_config_get_memory_base ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> + limit = pci_bridge_get_limit ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> monitor_printf ( mon , <nl> " memory range [ 0x % 08 " PRIx64 ", 0x % 08 " PRIx64 "]\ n ", <nl> base , limit );
static void x86_cpu_realizefn ( DeviceState * dev , Error ** errp ) <nl> env -> cpuid_ext3_features &= TCG_EXT3_FEATURES ; <nl> env -> cpuid_svm_features &= TCG_SVM_FEATURES ; <nl> } else { <nl> -# ifdef CONFIG_KVM <nl> - filter_features_for_kvm ( cpu ); <nl> -# endif <nl> if ( check_cpuid && kvm_check_features_against_host ( cpu ) <nl> && enforce_cpuid ) { <nl> error_setg ( errp , " Host ' s CPU doesn ' t support requested features "); <nl> return ; <nl> } <nl> +# ifdef CONFIG_KVM <nl> + filter_features_for_kvm ( cpu ); <nl> +# endif <nl> } <nl>  <nl> # ifndef CONFIG_USER_ONLY
int kvm_init ( int smp_cpus ) <nl> int ret ; <nl> int i ; <nl>  <nl> - if ( smp_cpus > 1 ) <nl> + if ( smp_cpus > 1 ) { <nl> + fprintf ( stderr , " No SMP KVM support , use '- smp 1 '\ n "); <nl> return - EINVAL ; <nl> + } <nl>  <nl> s = qemu_mallocz ( sizeof ( KVMState )); <nl> 
void memory_mapping_filter ( MemoryMappingList * list , int64_t begin , <nl> if ( cur -> phys_addr >= begin + length || <nl> cur -> phys_addr + cur -> length <= begin ) { <nl> QTAILQ_REMOVE (& list -> head , cur , next ); <nl> + g_free ( cur ); <nl> list -> num --; <nl> continue ; <nl> }
void hmp_info_block_jobs ( Monitor * mon , const QDict * qdict ) <nl> } <nl> list = list -> next ; <nl> } <nl> + <nl> + qapi_free_BlockJobInfoList ( list ); <nl> } <nl>  <nl> void hmp_info_tpm ( Monitor * mon , const QDict * qdict )
static void hid_keyboard_process_keycode ( HIDState * hs ) <nl> slot = hs -> head & QUEUE_MASK ; QUEUE_INCR ( hs -> head ); hs -> n --; <nl> keycode = hs -> kbd . keycodes [ slot ]; <nl>  <nl> + if (! hs -> n ) { <nl> + trace_hid_kbd_queue_empty (); <nl> + } <nl> + <nl> key = keycode & 0x7f ; <nl> index = key | (( hs -> kbd . modifiers & ( 1 << 8 )) >> 1 ); <nl> hid_code = hid_usage_keys [ index ];
mips_mipssim_init ( MachineState * machine ) <nl> ! kernel_filename && ! qtest_enabled ()) { <nl> /* Bail out if we have neither a kernel image nor boot vector code . */ <nl> error_report (" Could not load MIPS bios '% s ', and no " <nl> - "- kernel argument was specified ", filename ); <nl> + "- kernel argument was specified ", bios_name ); <nl> exit ( 1 ); <nl> } else { <nl> /* We have a boot vector start address . */
static int vfio_populate_device ( VFIODevice * vbasedev ) <nl> return ret ; <nl> } <nl>  <nl> - vdev -> regions = g_malloc0_n ( vbasedev -> num_regions , <nl> - sizeof ( VFIORegion *)); <nl> + vdev -> regions = g_new0 ( VFIORegion *, vbasedev -> num_regions ); <nl>  <nl> for ( i = 0 ; i < vbasedev -> num_regions ; i ++) { <nl> struct vfio_region_info reg_info = { . argsz = sizeof ( reg_info ) };
static void vhost_region_del ( MemoryListener * listener , <nl> == section -> offset_within_address_space ) { <nl> -- dev -> n_mem_sections ; <nl> memmove (& dev -> mem_sections [ i ], & dev -> mem_sections [ i + 1 ], <nl> - dev -> n_mem_sections - i ); <nl> + ( dev -> n_mem_sections - i ) * sizeof (* dev -> mem_sections )); <nl> break ; <nl> } <nl> }
static uint64_t uart_read ( void * opaque , hwaddr addr , <nl> r = s -> regs [ R_RXTX ]; <nl> s -> regs [ R_LSR ] &= ~ LSR_DR ; <nl> uart_update_irq ( s ); <nl> + qemu_chr_accept_input ( s -> chr ); <nl> break ; <nl> case R_IIR : <nl> case R_LSR :
static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> } <nl> } <nl>  <nl> + if ( header . backing_file_offset > s -> cluster_size ) { <nl> + error_setg ( errp , " Invalid backing file offset "); <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> + } <nl> + <nl> if ( header . backing_file_offset ) { <nl> ext_end = header . backing_file_offset ; <nl> } else {
static int object_create ( QemuOpts * opts , void * opaque ) <nl>  <nl> obj = object_new ( type ); <nl> if ( qemu_opt_foreach ( opts , object_set_property , obj , 1 ) < 0 ) { <nl> + object_unref ( obj ); <nl> return - 1 ; <nl> } <nl>  <nl> object_property_add_child ( container_get ( object_get_root (), "/ objects "), <nl> id , obj , NULL ); <nl> - <nl> + object_unref ( obj ); <nl> return 0 ; <nl> } <nl> 
static void build_pci_bus_end ( PCIBus * bus , void * bus_state ) <nl> * to make acpi tables compatible with legacy machine types . <nl> */ <nl> if (! child -> pcihp_bridge_en && bus -> parent_dev ) { <nl> + build_free_array ( bus_table ); <nl> + build_pci_bus_state_cleanup ( child ); <nl> + g_free ( child ); <nl> return ; <nl> } <nl> 
static void spapr_dr_connector_class_init ( ObjectClass * k , void * data ) <nl> drck -> attach = attach ; <nl> drck -> detach = detach ; <nl> drck -> release_pending = release_pending ; <nl> + /* <nl> + * Reason : it crashes FIXME find and document the real reason <nl> + */ <nl> + dk -> cannot_instantiate_with_device_add_yet = true ; <nl> } <nl>  <nl> static const TypeInfo spapr_dr_connector_info = {
static int iothread_stop ( Object * object , void * opaque ) <nl> IOThread * iothread ; <nl>  <nl> iothread = ( IOThread *) object_dynamic_cast ( object , TYPE_IOTHREAD ); <nl> - if (! iothread || ! iothread -> ctx ) { <nl> + if (! iothread || ! iothread -> ctx || iothread -> stopping ) { <nl> return 0 ; <nl> } <nl> iothread -> stopping = true ;
static Property vhost_scsi_properties [] = { <nl> DEFINE_PROP_STRING (" wwpn ", VirtIOSCSICommon , conf . wwpn ), <nl> DEFINE_PROP_UINT32 (" boot_tpgt ", VirtIOSCSICommon , conf . boot_tpgt , 0 ), <nl> DEFINE_PROP_UINT32 (" num_queues ", VirtIOSCSICommon , conf . num_queues , 1 ), <nl> + DEFINE_PROP_UINT32 (" virtqueue_size ", VirtIOSCSICommon , conf . virtqueue_size , <nl> + 128 ), <nl> DEFINE_PROP_UINT32 (" max_sectors ", VirtIOSCSICommon , conf . max_sectors , <nl> 0xFFFF ), <nl> DEFINE_PROP_UINT32 (" cmd_per_lun ", VirtIOSCSICommon , conf . cmd_per_lun , 128 ),
static void set_cfg_value ( bool is_max , int index , int value ) <nl> { <nl> if ( is_max ) { <nl> cfg . buckets [ index ]. max = value ; <nl> + /* If max is set , avg should never be 0 */ <nl> + cfg . buckets [ index ]. avg = MAX ( cfg . buckets [ index ]. avg , 1 ); <nl> } else { <nl> cfg . buckets [ index ]. avg = value ; <nl> }
udp_listen ( Slirp * slirp , uint32_t haddr , u_int hport , uint32_t laddr , <nl> return NULL ; <nl> } <nl> so -> s = qemu_socket ( AF_INET , SOCK_DGRAM , 0 ); <nl> + if ( so -> s < 0 ) { <nl> + sofree ( so ); <nl> + return NULL ; <nl> + } <nl> so -> so_expire = curtime + SO_EXPIRE ; <nl> insque ( so , & slirp -> udb ); <nl> 
static int get_real_id ( const char * devpath , const char * idname , uint16_t * val ) <nl> if ( fscanf ( f , "% li \ n ", & id ) == 1 ) { <nl> * val = id ; <nl> } else { <nl> + fclose ( f ); <nl> return - 1 ; <nl> } <nl> fclose ( f );
static inline int thunk_type_size ( const argtype * type_ptr , int is_host ) <nl> defined ( HOST_PARISC ) || defined ( HOST_SPARC64 ) <nl> return 4 ; <nl> # elif defined ( HOST_PPC ) <nl> - return TARGET_ABI_BITS / 8 ; <nl> + return sizeof ( void *); <nl> # else <nl> return 2 ; <nl> # endif
static void test_interface_impl ( const char * type ) <nl>  <nl> g_assert ( iobj ); <nl> g_assert ( ioc -> test == PATTERN ); <nl> + object_unref ( obj ); <nl> } <nl>  <nl> static void interface_direct_test ( void )
static void acpi_get_pm_info ( AcpiPmInfo * pm ) <nl> Object * obj = NULL ; <nl> QObject * o ; <nl>  <nl> + pm -> cpu_hp_io_base = 0 ; <nl> pm -> pcihp_io_base = 0 ; <nl> pm -> pcihp_io_len = 0 ; <nl> if ( piix ) {
int net_init_vhost_user ( const NetClientOptions * opts , const char * name , <nl> } <nl>  <nl> queues = vhost_user_opts -> has_queues ? vhost_user_opts -> queues : 1 ; <nl> + if ( queues < 1 ) { <nl> + error_setg ( errp , <nl> + " vhost - user number of queues must be bigger than zero "); <nl> + return - 1 ; <nl> + } <nl>  <nl> return net_vhost_user_init ( peer , " vhost_user ", name , chr , queues ); <nl> }
static uint64_t arm_ldq_ptw ( CPUState * cs , hwaddr addr , bool is_secure , <nl> MemTxAttrs attrs = {}; <nl> MemTxResult result = MEMTX_OK ; <nl> AddressSpace * as ; <nl> - uint32_t data ; <nl> + uint64_t data ; <nl>  <nl> attrs . secure = is_secure ; <nl> as = arm_addressspace ( cs , attrs );
static GSList * gd_vc_gfx_init ( GtkDisplayState * s , VirtualConsole * vc , <nl> QemuConsole * con , int idx , <nl> GSList * group , GtkWidget * view_menu ) <nl> { <nl> - Error * local_err = NULL ; <nl> Object * obj ; <nl>  <nl> - obj = object_property_get_link ( OBJECT ( con ), " device ", & local_err ); <nl> + obj = object_property_get_link ( OBJECT ( con ), " device ", NULL ); <nl> if ( obj ) { <nl> vc -> label = g_strdup_printf ("% s ", object_get_typename ( obj )); <nl> } else {
int main ( int argc , char ** argv ) <nl> qtest_add_func ("/ fdc / media_change ", test_media_change ); <nl> qtest_add_func ("/ fdc / sense_interrupt ", test_sense_interrupt ); <nl> qtest_add_func ("/ fdc / relative_seek ", test_relative_seek ); <nl> + qtest_add_func ("/ fdc / media_insert ", test_media_insert ); <nl> qtest_add_func ("/ fdc / fuzz - registers ", fuzz_registers ); <nl>  <nl> ret = g_test_run ();
static void vhost_iommu_region_add ( MemoryListener * listener , <nl> struct vhost_iommu * iommu ; <nl> Int128 end ; <nl> int iommu_idx ; <nl> - IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + IOMMUMemoryRegion * iommu_mr ; <nl>  <nl> if (! memory_region_is_iommu ( section -> mr )) { <nl> return ; <nl> } <nl>  <nl> + iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + <nl> iommu = g_malloc0 ( sizeof (* iommu )); <nl> end = int128_add ( int128_make64 ( section -> offset_within_region ), <nl> section -> size );
void qemu_ram_remap ( ram_addr_t addr , ram_addr_t length ) <nl> abort (); <nl> } else { <nl> flags = MAP_FIXED ; <nl> - munmap ( vaddr , length ); <nl> if ( block -> fd >= 0 ) { <nl> flags |= ( block -> flags & RAM_SHARED ? <nl> MAP_SHARED : MAP_PRIVATE );
Error * error_copy ( const Error * err ) <nl> err_new = g_malloc0 ( sizeof (* err )); <nl> err_new -> msg = g_strdup ( err -> msg ); <nl> err_new -> err_class = err -> err_class ; <nl> + err_new -> src = err -> src ; <nl> + err_new -> line = err -> line ; <nl> + err_new -> func = err -> func ; <nl> if ( err -> hint ) { <nl> err_new -> hint = g_string_new ( err -> hint -> str ); <nl> }
static abi_ulong mmap_find_vma_reserved ( abi_ulong start , abi_ulong size ) <nl> if ( prot ) { <nl> end_addr = addr ; <nl> } <nl> - if ( addr + size == end_addr ) { <nl> + if ( addr && addr + size == end_addr ) { <nl> break ; <nl> } <nl> addr -= qemu_host_page_size ;
static gboolean ga_channel_open ( GAChannel * c , const gchar * path , GAChannelMethod <nl> ret = ga_channel_client_add ( c , fd ); <nl> if ( ret ) { <nl> g_critical (" error adding channel to main loop "); <nl> + close ( fd ); <nl> return false ; <nl> } <nl> break ;
guint qemu_chr_fe_add_watch ( CharBackend * be , GIOCondition cond , <nl> } <nl>  <nl> g_source_set_callback ( src , ( GSourceFunc ) func , user_data , NULL ); <nl> - tag = g_source_attach ( src , NULL ); <nl> + tag = g_source_attach ( src , s -> gcontext ); <nl> g_source_unref ( src ); <nl>  <nl> return tag ;
static int xen_pt_bar_reg_write ( XenPCIPassthroughState * s , XenPTReg * cfg_entry , <nl> bar_ro_mask = XEN_PT_BAR_IO_RO_MASK | ( r_size - 1 ); <nl> break ; <nl> case XEN_PT_BAR_FLAG_UPPER : <nl> + assert ( index > 0 ); <nl> + r_size = d -> io_regions [ index - 1 ]. size >> 32 ; <nl> bar_emu_mask = XEN_PT_BAR_ALLF ; <nl> bar_ro_mask = r_size ? r_size - 1 : 0 ; <nl> break ;
int qdev_unplug ( DeviceState * dev ) <nl> dev -> parent_bus -> name ); <nl> return - 1 ; <nl> } <nl> + assert ( dev -> info -> unplug != NULL ); <nl> + <nl> return dev -> info -> unplug ( dev ); <nl> } <nl> 
static const QEMUFileOps rdma_write_ops = { <nl>  <nl> static void * qemu_fopen_rdma ( RDMAContext * rdma , const char * mode ) <nl> { <nl> - QEMUFileRDMA * r = g_malloc0 ( sizeof ( QEMUFileRDMA )); <nl> + QEMUFileRDMA * r ; <nl>  <nl> if ( qemu_file_mode_is_not_valid ( mode )) { <nl> return NULL ; <nl> } <nl>  <nl> + r = g_malloc0 ( sizeof ( QEMUFileRDMA )); <nl> r -> rdma = rdma ; <nl>  <nl> if ( mode [ 0 ] == ' w ') {
USBPacket * usb_ep_find_packet_by_id ( USBDevice * dev , int pid , int ep , <nl> struct USBEndpoint * uep = usb_ep_get ( dev , pid , ep ); <nl> USBPacket * p ; <nl>  <nl> - while (( p = QTAILQ_FIRST (& uep -> queue )) != NULL ) { <nl> + QTAILQ_FOREACH ( p , & uep -> queue , queue ) { <nl> if ( p -> id == id ) { <nl> return p ; <nl> }
static int img_convert ( int argc , char ** argv ) <nl>  <nl> if ( options && ! strcmp ( options , "?")) { <nl> print_option_help ( drv -> create_options ); <nl> + free ( bs ); <nl> return 0 ; <nl> } <nl> 
DeviceState * qdev_try_create ( BusState * bus , const char * name ) <nl> { <nl> DeviceState * dev ; <nl>  <nl> + if ( object_class_by_name ( name ) == NULL ) { <nl> + return NULL ; <nl> + } <nl> dev = DEVICE ( object_new ( name )); <nl> if (! dev ) { <nl> return NULL ;
static void multiwrite_cb ( void * opaque , int ret ) <nl> { <nl> MultiwriteCB * mcb = opaque ; <nl>  <nl> - if ( ret < 0 ) { <nl> + if ( ret < 0 && ! mcb -> error ) { <nl> mcb -> error = ret ; <nl> multiwrite_user_cb ( mcb ); <nl> }
static QTAILQ_HEAD (, Rom ) roms = QTAILQ_HEAD_INITIALIZER ( roms ); <nl>  <nl> static inline bool rom_order_compare ( Rom * rom , Rom * item ) <nl> { <nl> - return ( rom -> as > item -> as ) || <nl> + return (( uintptr_t )( void *) rom -> as > ( uintptr_t )( void *) item -> as ) || <nl> ( rom -> as == item -> as && rom -> addr >= item -> addr ); <nl> } <nl> 
static inline void gdb_continue ( GDBState * s ) <nl> # ifdef CONFIG_USER_ONLY <nl> s -> running_state = 1 ; <nl> # else <nl> - vm_start (); <nl> + if ( runstate_check ( RUN_STATE_DEBUG )) { <nl> + vm_start (); <nl> + } <nl> # endif <nl> } <nl> 
static void arm_cpu_reset ( CPUState * s ) <nl> */ <nl> env -> v7m . ccr = R_V7M_CCR_STKALIGN_MASK ; <nl>  <nl> + /* Unlike A / R profile , M profile defines the reset LR value */ <nl> + env -> regs [ 14 ] = 0xffffffff ; <nl> + <nl> /* Load the initial SP and PC from the vector table at address 0 */ <nl> rom = rom_ptr ( 0 ); <nl> if ( rom ) {
int main ( int argc , char ** argv , char ** envp ) <nl> /* init remote displays */ <nl> qemu_opts_foreach ( qemu_find_opts (" vnc "), vnc_init_func , NULL , 0 ); <nl> if ( show_vnc_port ) { <nl> - printf (" VNC server running on `% s '\ n ", <nl> - vnc_display_local_addr (" default ")); <nl> + char * ret = vnc_display_local_addr (" default "); <nl> + printf (" VNC server running on `% s '\ n ", ret ); <nl> + g_free ( ret ); <nl> } <nl> # endif <nl> # ifdef CONFIG_SPICE
static inline abi_long do_semctl ( int semid , int semnum , int cmd , <nl> { <nl> union semun arg ; <nl> struct semid_ds dsarg ; <nl> - unsigned short * array ; <nl> + unsigned short * array = NULL ; <nl> struct seminfo seminfo ; <nl> abi_long ret = - TARGET_EINVAL ; <nl> abi_long err ;
int msix_init ( struct PCIDevice * dev , unsigned short nentries , <nl> return 0 ; <nl>  <nl> err_config : <nl> + dev -> msix_entries_nr = 0 ; <nl> cpu_unregister_io_memory ( dev -> msix_mmio_index ); <nl> err_index : <nl> qemu_free ( dev -> msix_table_page );
qcrypto_tls_session_check_certificate ( QCryptoTLSSession * session , <nl>  <nl> allow = qemu_acl_party_is_allowed ( acl , session -> peername ); <nl>  <nl> - error_setg ( errp , " TLS x509 ACL check for % s is % s ", <nl> - session -> peername , allow ? " allowed " : " denied "); <nl> if (! allow ) { <nl> + error_setg ( errp , " TLS x509 ACL check for % s is denied ", <nl> + session -> peername ); <nl> goto error ; <nl> } <nl> }
size_t qlist_size ( const QList * qlist ) <nl> */ <nl> QList * qobject_to_qlist ( const QObject * obj ) <nl> { <nl> - if ( qobject_type ( obj ) != QTYPE_QLIST ) { <nl> + if (! obj || qobject_type ( obj ) != QTYPE_QLIST ) { <nl> return NULL ; <nl> } <nl> - <nl> return container_of ( obj , QList , base ); <nl> } <nl> 
static BlockBackend * img_open_opts ( const char * optstr , <nl> if ( qdict_haskey ( options , BDRV_OPT_FORCE_SHARE ) <nl> && ! qdict_get_bool ( options , BDRV_OPT_FORCE_SHARE )) { <nl> error_report ("-- force - share /- U conflicts with image options "); <nl> + QDECREF ( options ); <nl> return NULL ; <nl> } <nl> qdict_put ( options , BDRV_OPT_FORCE_SHARE , qbool_from_bool ( true ));
static uint64_t pci_host_data_read ( void * opaque , <nl> { <nl> PCIHostState * s = opaque ; <nl> uint32_t val ; <nl> - if (!( s -> config_reg & ( 1 << 31 ))) <nl> + if (!( s -> config_reg & ( 1U << 31 ))) { <nl> return 0xffffffff ; <nl> + } <nl> val = pci_data_read ( s -> bus , s -> config_reg | ( addr & 3 ), len ); <nl> PCI_DPRINTF (" read addr " TARGET_FMT_plx " len % d val % x \ n ", <nl> addr , len , val );
static int bdrv_open_common ( BlockDriverState * bs , BlockDriverState * file , <nl> ret = - EINVAL ; <nl> goto free_and_fail ; <nl> } <nl> - assert ( file != NULL ); <nl> bs -> file = file ; <nl> ret = drv -> bdrv_open ( bs , options , open_flags ); <nl> }
static void kvm_apic_realize ( DeviceState * dev , Error ** errp ) <nl> { <nl> APICCommonState * s = APIC_COMMON ( dev ); <nl>  <nl> - memory_region_init_io (& s -> io_memory , NULL , & kvm_apic_io_ops , s , " kvm - apic - msi ", <nl> - APIC_SPACE_SIZE ); <nl> + memory_region_init_io (& s -> io_memory , OBJECT ( s ), & kvm_apic_io_ops , s , <nl> + " kvm - apic - msi ", APIC_SPACE_SIZE ); <nl>  <nl> if ( kvm_has_gsi_routing ()) { <nl> msi_nonbroken = true ;
int kvm_arch_init_vcpu ( CPUState * cs ) <nl> cpuid_data . cpuid . nent = cpuid_i ; <nl>  <nl> if ((( env -> cpuid_version >> 8 )& 0xF ) >= 6 <nl> - && ( env -> cpuid_features &( CPUID_MCE | CPUID_MCA )) == ( CPUID_MCE | CPUID_MCA ) <nl> + && ( env -> cpuid_features & ( CPUID_MCE | CPUID_MCA )) == <nl> + ( CPUID_MCE | CPUID_MCA ) <nl> && kvm_check_extension ( cs -> kvm_state , KVM_CAP_MCE ) > 0 ) { <nl> uint64_t mcg_cap ; <nl> int banks ;
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl> return 0 ; <nl>  <nl> fail : <nl> + if ( snapshots_offset > 0 ) { <nl> + qcow2_free_clusters ( bs , snapshots_offset , snapshots_size , <nl> + QCOW2_DISCARD_ALWAYS ); <nl> + } <nl> return ret ; <nl> } <nl> 
static int qemu_event_init ( void ) <nl>  <nl> static void qemu_event_increment ( void ) <nl> { <nl> - SetEvent ( qemu_event_handle ); <nl> + if (! SetEvent ( qemu_event_handle )) { <nl> + fprintf ( stderr , " qemu_event_increment : SetEvent failed : % d \ n ", <nl> + GetLastError ()); <nl> + exit ( 1 ); <nl> + } <nl> } <nl> # endif <nl> 
static int oss_open ( int in , struct oss_params * req , <nl> goto err ; <nl> } <nl>  <nl> + if (! abinfo . fragstotal || ! abinfo . fragsize ) { <nl> + AUD_log ( AUDIO_CAP , " Returned bogus buffer information (% d , % d ) for % s \ n ", <nl> + abinfo . fragstotal , abinfo . fragsize , typ ); <nl> + goto err ; <nl> + } <nl> + <nl> obt -> fmt = fmt ; <nl> obt -> nchannels = nchannels ; <nl> obt -> freq = freq ;
static void spapr_finalize_fdt ( sPAPREnvironment * spapr , <nl>  <nl> cpu_physical_memory_write ( fdt_addr , fdt , fdt_totalsize ( fdt )); <nl>  <nl> + g_free ( bootlist ); <nl> g_free ( fdt ); <nl> } <nl> 
static int ehci_process_itd ( EHCIState * ehci , <nl> if ( off + len > 4096 ) { <nl> /* transfer crosses page border */ <nl> if ( pg == 6 ) { <nl> + qemu_sglist_destroy (& ehci -> isgl ); <nl> return - 1 ; /* avoid page pg + 1 */ <nl> } <nl> ptr2 = ( itd -> bufptr [ pg + 1 ] & ITD_BUFPTR_MASK );
static void xenfb_handle_events ( struct XenFB * xenfb ) <nl>  <nl> prod = page -> out_prod ; <nl> out_cons = page -> out_cons ; <nl> - if ( prod - out_cons >= XENFB_OUT_RING_LEN ) { <nl> + if ( prod - out_cons > XENFB_OUT_RING_LEN ) { <nl> return ; <nl> } <nl> xen_rmb (); /* ensure we see ring contents up to prod */
DriveInfo * drive_init ( QemuOpts * opts , int default_to_scsi , int * fatal_error ) <nl> dinfo -> on_write_error = on_write_error ; <nl> dinfo -> opts = opts ; <nl> if ( serial ) <nl> - strncpy ( dinfo -> serial , serial , sizeof ( serial )); <nl> + strncpy ( dinfo -> serial , serial , sizeof ( dinfo -> serial ) - 1 ); <nl> QTAILQ_INSERT_TAIL (& drives , dinfo , next ); <nl>  <nl> switch ( type ) {
GEN_HANDLER ( dcbtst , 0x1F , 0x16 , 0x07 , 0x03E00001 , PPC_CACHE ) <nl> # define op_dcbz () (* gen_op_dcbz [ ctx -> mem_idx ])() <nl> static GenOpFunc * gen_op_dcbz [] = { <nl> & gen_op_dcbz_user , <nl> + & gen_op_dcbz_user , <nl> + & gen_op_dcbz_kernel , <nl> & gen_op_dcbz_kernel , <nl> }; <nl> # endif
static int send_sub_rect_nojpeg ( VncState * vs , int x , int y , int w , int h , <nl> ret = send_mono_rect ( vs , x , y , w , h , bg , fg ); <nl> } else if ( colors <= 256 ) { <nl> ret = send_palette_rect ( vs , x , y , w , h , palette ); <nl> + } else { <nl> + ret = 0 ; <nl> } <nl> return ret ; <nl> }
static int htab_save_iterate ( QEMUFile * f , void * opaque ) <nl> /* Iteration header */ <nl> if (! spapr -> htab_shift ) { <nl> qemu_put_be32 ( f , - 1 ); <nl> - return 0 ; <nl> + return 1 ; <nl> } else { <nl> qemu_put_be32 ( f , 0 ); <nl> }
static void tcx_realizefn ( DeviceState * dev , Error ** errp ) <nl> if ( fcode_filename ) { <nl> ret = load_image_targphys ( fcode_filename , s -> prom_addr , <nl> FCODE_MAX_ROM_SIZE ); <nl> + g_free ( fcode_filename ); <nl> if ( ret < 0 || ret > FCODE_MAX_ROM_SIZE ) { <nl> error_report (" tcx : could not load prom '% s '", TCX_ROM_FILE ); <nl> }
static void ppc_spapr_init ( QEMUMachineInitArgs * args ) <nl> NULL , & lowaddr , NULL , 0 , ELF_MACHINE , 0 ); <nl> kernel_le = kernel_size > 0 ; <nl> } <nl> - if ( kernel_size < 0 ) { <nl> - kernel_size = load_image_targphys ( kernel_filename , <nl> - KERNEL_LOAD_ADDR , <nl> - load_limit - KERNEL_LOAD_ADDR ); <nl> - } <nl> if ( kernel_size < 0 ) { <nl> fprintf ( stderr , " qemu : could not load kernel '% s '\ n ", <nl> kernel_filename );
uint64_t pc_dimm_get_free_addr ( uint64_t address_space_start , <nl> uint64_t address_space_end = address_space_start + address_space_size ; <nl>  <nl> g_assert ( QEMU_ALIGN_UP ( address_space_start , align ) == address_space_start ); <nl> - g_assert ( QEMU_ALIGN_UP ( address_space_size , align ) == address_space_size ); <nl>  <nl> if (! address_space_size ) { <nl> error_setg ( errp , " memory hotplug is not enabled , "
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> s -> current_mapping -> path = buffer ; <nl> s -> current_mapping -> read_only = <nl> ( st . st_mode & ( S_IWUSR | S_IWGRP | S_IWOTH )) == 0 ; <nl> - } <nl> + } else { <nl> + g_free ( buffer ); <nl> + } <nl> } <nl> closedir ( dir ); <nl> 
DisplaySurface * qemu_create_displaysurface_guestmem ( int width , int height , <nl> linesize = width * PIXMAN_FORMAT_BPP ( format ) / 8 ; <nl> } <nl>  <nl> - size = linesize * height ; <nl> + size = ( hwaddr ) linesize * height ; <nl> data = cpu_physical_memory_map ( addr , & size , 0 ); <nl> - if ( size != linesize * height ) { <nl> + if ( size != ( hwaddr ) linesize * height ) { <nl> cpu_physical_memory_unmap ( data , size , 0 , 0 ); <nl> return NULL ; <nl> }
void console_select ( unsigned int index ) <nl> if ( s ) { <nl> DisplayState * ds = s -> ds ; <nl>  <nl> - if ( active_console -> cursor_timer ) { <nl> + if ( active_console && active_console -> cursor_timer ) { <nl> qemu_del_timer ( active_console -> cursor_timer ); <nl> } <nl> active_console = s ;
static void vhost_dev_sync_region ( struct vhost_dev * dev , <nl> log = __sync_fetch_and_and ( from , 0 ); <nl> while (( bit = sizeof ( log ) > sizeof ( int ) ? <nl> ffsll ( log ) : ffs ( log ))) { <nl> + ram_addr_t ram_addr ; <nl> bit -= 1 ; <nl> - cpu_physical_memory_set_dirty ( addr + bit * VHOST_LOG_PAGE ); <nl> + ram_addr = cpu_get_physical_page_desc ( addr + bit * VHOST_LOG_PAGE ); <nl> + cpu_physical_memory_set_dirty ( ram_addr ); <nl> log &= ~( 0x1ull << bit ); <nl> } <nl> addr += VHOST_LOG_CHUNK ;
aio_read_f ( int argc , char ** argv ) <nl> case ' P ': <nl> ctx -> Pflag = 1 ; <nl> ctx -> pattern = parse_pattern ( optarg ); <nl> - if ( ctx -> pattern < 0 ) <nl> + if ( ctx -> pattern < 0 ) { <nl> + free ( ctx ); <nl> return 0 ; <nl> + } <nl> break ; <nl> case ' q ': <nl> ctx -> qflag = 1 ;
char * socket_address_to_string ( struct SocketAddress * addr , Error ** errp ) <nl>  <nl> SocketAddress * socket_address_flatten ( SocketAddressLegacy * addr_legacy ) <nl> { <nl> - SocketAddress * addr = g_new ( SocketAddress , 1 ); <nl> + SocketAddress * addr ; <nl>  <nl> if (! addr_legacy ) { <nl> return NULL ; <nl> } <nl>  <nl> + addr = g_new ( SocketAddress , 1 ); <nl> + <nl> switch ( addr_legacy -> type ) { <nl> case SOCKET_ADDRESS_LEGACY_KIND_INET : <nl> addr -> type = SOCKET_ADDRESS_TYPE_INET ;
static void tcg_reg_alloc_mov ( TCGContext * s , const TCGOpDef * def , <nl> } <nl> ots -> val_type = TEMP_VAL_CONST ; <nl> ots -> val = ts -> val ; <nl> + if ( IS_DEAD_ARG ( 1 )) { <nl> + temp_dead ( s , args [ 1 ]); <nl> + } <nl> } else { <nl> /* The code in the first if block should have moved the <nl> temp to a register . */
* THE SOFTWARE . <nl> */ <nl>  <nl> -# include " sysemu / sysemu . h " <nl> -# include " monitor / monitor . h " <nl> -# include " ui / console . h " <nl> - <nl> -# include " hw / hw . h " <nl> - <nl> +# include " qemu / main - loop . h " <nl> # include " qemu / timer . h " <nl> + <nl> # ifdef CONFIG_POSIX <nl> # include < pthread . h > <nl> # endif
static void x86_cpu_apic_create ( X86CPU * cpu , Error ** errp ) <nl>  <nl> object_property_add_child ( OBJECT ( cpu ), " lapic ", <nl> OBJECT ( cpu -> apic_state ), & error_abort ); <nl> + object_unref ( OBJECT ( cpu -> apic_state )); <nl>  <nl> qdev_prop_set_uint8 ( cpu -> apic_state , " id ", cpu -> apic_id ); <nl> /* TODO : convert to link <> */
static int ioreq_map ( struct ioreq * ioreq ) <nl> xen_be_printf (& ioreq -> blkdev -> xendev , 0 , <nl> " can ' t map grant ref % d (% s , % d maps )\ n ", <nl> refs [ i ], strerror ( errno ), ioreq -> blkdev -> cnt_map ); <nl> + ioreq -> mapped = 1 ; <nl> ioreq_unmap ( ioreq ); <nl> return - 1 ; <nl> }
int cpu_get_dump_info ( ArchDumpInfo * info , <nl> } else { <nl> info -> d_endian = ELFDATA2LSB ; <nl> } <nl> + /* 64KB is the max page size for pseries kernel */ <nl> + if ( strncmp ( object_get_typename ( qdev_get_machine ()), <nl> + " pseries -", 8 ) == 0 ) { <nl> + info -> page_size = ( 1U << 16 ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int find_and_clear_dirty_height ( VncState * vs , <nl> static int vnc_update_client ( VncState * vs , int has_dirty , bool sync ) <nl> { <nl> vs -> has_dirty += has_dirty ; <nl> - if ( vs -> need_update && vs -> ioc != NULL ) { <nl> + if ( vs -> need_update && ! vs -> disconnecting ) { <nl> VncDisplay * vd = vs -> vd ; <nl> VncJob * job ; <nl> int y ;
void cpu_physical_memory_write_rom ( target_phys_addr_t addr , <nl> /* ROM / RAM case */ <nl> ptr = qemu_get_ram_ptr ( addr1 ); <nl> memcpy ( ptr , buf , l ); <nl> + if (! cpu_physical_memory_is_dirty ( addr1 )) { <nl> + /* invalidate code */ <nl> + tb_invalidate_phys_page_range ( addr1 , addr1 + l , 0 ); <nl> + /* set dirty bit */ <nl> + cpu_physical_memory_set_dirty_flags ( <nl> + addr1 , ( 0xff & ~ CODE_DIRTY_FLAG )); <nl> + } <nl> qemu_put_ram_ptr ( ptr ); <nl> } <nl> len -= l ;
static GIOStatus ga_channel_write ( GAChannel * c , const char * buf , size_t size , <nl> GIOStatus ga_channel_write_all ( GAChannel * c , const char * buf , size_t size ) <nl> { <nl> GIOStatus status = G_IO_STATUS_NORMAL ; <nl> - size_t count ; <nl> + size_t count = 0 ; <nl>  <nl> while ( size ) { <nl> status = ga_channel_write ( c , buf , size , & count );
static void tap_cleanup ( VLANClientState * nc ) <nl>  <nl> if ( s -> vhost_net ) { <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> + s -> vhost_net = NULL ; <nl> } <nl>  <nl> qemu_purge_queued_packets ( nc );
static bool run_poll_handlers_once ( AioContext * ctx ) <nl>  <nl> QLIST_FOREACH_RCU ( node , & ctx -> aio_handlers , node ) { <nl> if (! node -> deleted && node -> io_poll && <nl> - node -> io_poll ( node -> opaque )) { <nl> + aio_node_check ( ctx , node -> is_external ) && <nl> + node -> io_poll ( node -> opaque )) { <nl> progress = true ; <nl> } <nl> 
int cpu_exec ( CPUArchState * env ) <nl> * local variables as longjmp is marked ' noreturn '. */ <nl> cpu = current_cpu ; <nl> env = cpu -> env_ptr ; <nl> +# if !( defined ( CONFIG_USER_ONLY ) && \ <nl> + ( defined ( TARGET_M68K ) || defined ( TARGET_PPC ) || defined ( TARGET_S390X ))) <nl> + cc = CPU_GET_CLASS ( cpu ); <nl> +# endif <nl> } <nl> } /* for (;;) */ <nl> 
tcp_listen ( Slirp * slirp , u_int32_t haddr , u_int hport , u_int32_t laddr , <nl> struct socket * so ; <nl> int s , opt = 1 ; <nl> socklen_t addrlen = sizeof ( addr ); <nl> + memset (& addr , 0 , addrlen ); <nl>  <nl> DEBUG_CALL (" tcp_listen "); <nl> DEBUG_ARG (" haddr = % x ", haddr );
reply_maybe_async : <nl> reply_async -> IOCLogInfo = count ; <nl> return ; <nl> } <nl> + g_free ( reply_async ); <nl> reply . TerminationCount = count ; <nl> break ; <nl> 
int qemu_opts_foreach ( QemuOptsList * list , qemu_opts_loopfunc func , void * opaque , <nl> int rc = 0 ; <nl>  <nl> QTAILQ_FOREACH ( opts , & list -> head , next ) { <nl> - rc = func ( opts , opaque ); <nl> + rc |= func ( opts , opaque ); <nl> if ( abort_on_failure && rc != 0 ) <nl> break ; <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> /* libc does special remapping of the return value of <nl> * sys_getpriority () so it ' s just easiest to call <nl> * sys_getpriority () directly rather than through libc . */ <nl> - ret = sys_getpriority ( arg1 , arg2 ); <nl> + ret = get_errno ( sys_getpriority ( arg1 , arg2 )); <nl> break ; <nl> case TARGET_NR_setpriority : <nl> ret = get_errno ( setpriority ( arg1 , arg2 , arg3 ));
static void nbd_refresh_filename ( BlockDriverState * bs , QDict * options ) <nl> ov = qobject_output_visitor_new (& saddr_qdict ); <nl> visit_type_SocketAddress ( ov , NULL , & s -> saddr , & error_abort ); <nl> visit_complete ( ov , & saddr_qdict ); <nl> + visit_free ( ov ); <nl> assert ( qobject_type ( saddr_qdict ) == QTYPE_QDICT ); <nl>  <nl> qdict_put_obj ( opts , " server ", saddr_qdict );
static CharDriverState * gd_vc_handler ( ChardevVC * vc , Error ** errp ) <nl> chr -> chr_set_echo = gd_vc_chr_set_echo ; <nl>  <nl> /* Temporary , until gd_vc_vte_init runs . */ <nl> - chr -> opaque = g_new ( VirtualConsole , 1 ); <nl> + chr -> opaque = g_new0 ( VirtualConsole , 1 ); <nl>  <nl> /* defer OPENED events until our vc is fully initialized */ <nl> chr -> explicit_be_open = true ;
static void win32_aio_process_completion ( QEMUWin32AIOState * s , <nl> memcpy ( qiov -> iov [ i ]. iov_base , p , qiov -> iov [ i ]. iov_len ); <nl> p += qiov -> iov [ i ]. iov_len ; <nl> } <nl> - qemu_vfree ( waiocb -> buf ); <nl> } <nl> + qemu_vfree ( waiocb -> buf ); <nl> } <nl>  <nl> 
static void sch_handle_start_func ( SubchDev * sch , ORB * orb ) <nl> path = 0x80 ; <nl>  <nl> if (!( s -> ctrl & SCSW_ACTL_SUSP )) { <nl> + s -> cstat = 0 ; <nl> + s -> dstat = 0 ; <nl> /* Look at the orb and try to execute the channel program . */ <nl> assert ( orb != NULL ); /* resume does not pass an orb */ <nl> p -> intparm = orb -> intparm ;
static int vscsi_srp_direct_data ( VSCSIState * s , vscsi_req * req , <nl> { <nl> struct srp_direct_buf * md = req -> cur_desc ; <nl> uint32_t llen ; <nl> - int rc ; <nl> + int rc = 0 ; <nl>  <nl> dprintf (" VSCSI : direct segment 0x % x bytes , va = 0x % llx desc len = 0x % x \ n ", <nl> len , ( unsigned long long ) md -> va , md -> len );
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> - g_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
SCSIRequest * scsi_req_ref ( SCSIRequest * req ) <nl>  <nl> void scsi_req_unref ( SCSIRequest * req ) <nl> { <nl> + assert ( req -> refcount > 0 ); <nl> if (-- req -> refcount == 0 ) { <nl> if ( req -> ops -> free_req ) { <nl> req -> ops -> free_req ( req );
static QemuOptsList parallels_runtime_opts = { <nl> . name = PARALLELS_OPT_PREALLOC_SIZE , <nl> . type = QEMU_OPT_SIZE , <nl> . help = " Preallocation size on image expansion ", <nl> - . def_value_str = " 128MiB ", <nl> + . def_value_str = " 128M ", <nl> }, <nl> { <nl> . name = PARALLELS_OPT_PREALLOC_MODE ,
static int img_convert ( int argc , char ** argv ) <nl> ret = bdrv_parse_cache_flags ( cache , & flags ); <nl> if ( ret < 0 ) { <nl> error_report (" Invalid cache option : % s ", cache ); <nl> - return - 1 ; <nl> + goto out ; <nl> } <nl>  <nl> out_bs = bdrv_new_open (" target ", out_filename , out_fmt , flags , true , quiet );
BlockDirtyInfoList * bdrv_query_dirty_bitmaps ( BlockDriverState * bs ) <nl> QLIST_FOREACH ( bm , & bs -> dirty_bitmaps , list ) { <nl> BlockDirtyInfo * info = g_new0 ( BlockDirtyInfo , 1 ); <nl> BlockDirtyInfoList * entry = g_new0 ( BlockDirtyInfoList , 1 ); <nl> - info -> count = bdrv_get_dirty_count ( bm ); <nl> + info -> count = bdrv_get_dirty_count ( bm ) << BDRV_SECTOR_BITS ; <nl> info -> granularity = bdrv_dirty_bitmap_granularity ( bm ); <nl> info -> has_name = !! bm -> name ; <nl> info -> name = g_strdup ( bm -> name );
static void glue ( audio_pcm_hw_gc_ , TYPE ) ( HW ** hwp ) <nl> audio_detach_capture ( hw ); <nl> # endif <nl> QLIST_REMOVE ( hw , entries ); <nl> + glue ( hw -> pcm_ops -> fini_ , TYPE ) ( hw ); <nl> glue ( s -> nb_hw_voices_ , TYPE ) += 1 ; <nl> glue ( audio_pcm_hw_free_resources_ , TYPE ) ( hw ); <nl> - glue ( hw -> pcm_ops -> fini_ , TYPE ) ( hw ); <nl> g_free ( hw ); <nl> * hwp = NULL ; <nl> }
static void drive_uninit ( DriveInfo * dinfo ) <nl> { <nl> qemu_opts_del ( dinfo -> opts ); <nl> bdrv_delete ( dinfo -> bdrv ); <nl> + qemu_free ( dinfo -> id ); <nl> QTAILQ_REMOVE (& drives , dinfo , next ); <nl> qemu_free ( dinfo ); <nl> }
static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl> if ( ret < 0 ) { <nl> error_report (" failed to create inode for snapshot : % s ", <nl> error_get_pretty ( local_err )); <nl> + error_free ( local_err ); <nl> goto cleanup ; <nl> } <nl> 
static void fsl_imx25_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx25_realize ; <nl> - <nl> dc -> desc = " i . MX25 SOC "; <nl> + /* <nl> + * Reason : uses serial_hds in realize and the imx25 board does not <nl> + * support multiple CPUs <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx25_type_info = {
static gboolean qio_channel_websock_handshake_io ( QIOChannel * ioc , <nl> return TRUE ; <nl> } <nl>  <nl> - object_ref ( OBJECT ( task )); <nl> trace_qio_channel_websock_handshake_reply ( ioc ); <nl> qio_channel_add_watch ( <nl> wioc -> master , <nl> G_IO_OUT , <nl> qio_channel_websock_handshake_send , <nl> task , <nl> - ( GDestroyNotify ) object_unref ); <nl> + NULL ); <nl> return FALSE ; <nl> } <nl> 
VHostNetState * get_vhost_net ( NetClientState * nc ) <nl> int vhost_set_vring_enable ( NetClientState * nc , int enable ) <nl> { <nl> VHostNetState * net = get_vhost_net ( nc ); <nl> - const VhostOps * vhost_ops = net -> dev . vhost_ops ; <nl> + const VhostOps * vhost_ops ; <nl> + <nl> + if (! net ) { <nl> + return 0 ; <nl> + } <nl>  <nl> + vhost_ops = net -> dev . vhost_ops ; <nl> if ( vhost_ops -> vhost_set_vring_enable ) { <nl> return vhost_ops -> vhost_set_vring_enable (& net -> dev , enable ); <nl> }
static void xen_pci_passthrough_class_init ( ObjectClass * klass , void * data ) <nl> k -> exit = xen_pt_unregister_device ; <nl> k -> config_read = xen_pt_pci_read_config ; <nl> k -> config_write = xen_pt_pci_write_config ; <nl> + k -> is_express = 1 ; /* We might be */ <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> desc = " Assign an host PCI device with Xen "; <nl> dc -> props = xen_pci_passthrough_properties ;
void mips_malta_init ( MachineState * machine ) <nl> /* Board ID = 0x420 ( Malta Board with CoreLV ) */ <nl> stl_p ( memory_region_get_ram_ptr ( bios_copy ) + 0x10 , 0x00000420 ); <nl>  <nl> - /* Init internal devices */ <nl> - cpu_mips_irq_init_cpu ( env ); <nl> - cpu_mips_clock_init ( env ); <nl> - <nl> /* <nl> * We have a circular dependency problem : pci_bus depends on isa_irq , <nl> * isa_irq is provided by i8259 , i8259 depends on ISA , ISA depends
int glue ( load_elf , SZ )( int fd , int64_t virt_to_phys_addend , <nl> data = NULL ; <nl> } <nl> } <nl> + qemu_free ( phdr ); <nl> return total_size ; <nl> fail : <nl> qemu_free ( data );
bool bdrv_dev_is_medium_locked ( BlockDriverState * bs ) <nl> */ <nl> int bdrv_check ( BlockDriverState * bs , BdrvCheckResult * res , BdrvCheckMode fix ) <nl> { <nl> + if ( bs -> drv == NULL ) { <nl> + return - ENOMEDIUM ; <nl> + } <nl> if ( bs -> drv -> bdrv_check == NULL ) { <nl> return - ENOTSUP ; <nl> }
print_insn ( bfd_vma pc , disassemble_info * info ) <nl> } <nl> } <nl>  <nl> - if ( putop ( dp -> name , sizeflag ) == 0 ) <nl> + if ( dp -> name != NULL && putop ( dp -> name , sizeflag ) == 0 ) <nl> { <nl> for ( i = 0 ; i < MAX_OPERANDS ; ++ i ) <nl> {
static bool ga_open_pidfile ( const char * pidfile ) <nl> goto fail ; <nl> } <nl>  <nl> + /* keep pidfile open & locked forever */ <nl> return true ; <nl>  <nl> fail : <nl> unlink ( pidfile ); <nl> + close ( pidfd ); <nl> return false ; <nl> } <nl> # else /* _WIN32 */
int kvm_init ( void ) <nl> } while ( ret == - EINTR ); <nl>  <nl> if ( ret < 0 ) { <nl> - fprintf ( stderr , " ioctl ( KVM_CREATE_VM ) failed : % d % s \ n ", - s -> vmfd , <nl> + fprintf ( stderr , " ioctl ( KVM_CREATE_VM ) failed : % d % s \ n ", - ret , <nl> strerror (- ret )); <nl>  <nl> # ifdef TARGET_S390X
get_net_error_message ( gint error ) <nl> if ( msg != NULL ) { <nl> nchars = wcslen ( msg ); <nl>  <nl> - if ( nchars > 2 && <nl> + if ( nchars >= 2 && <nl> msg [ nchars - 1 ] == L '\ n ' && <nl> msg [ nchars - 2 ] == L '\ r ') { <nl> msg [ nchars - 2 ] = L '\ 0 ';
static void numa_add ( const char * optarg ) <nl> node_mem [ nodenr ] = 0 ; <nl> } else { <nl> int64_t sval ; <nl> - sval = strtosz ( option , NULL ); <nl> - if ( sval < 0 ) { <nl> + sval = strtosz ( option , & endptr ); <nl> + if ( sval < 0 || * endptr ) { <nl> fprintf ( stderr , " qemu : invalid numa mem size : % s \ n ", optarg ); <nl> exit ( 1 ); <nl> }
void dpy_gfx_replace_surface ( QemuConsole * con , <nl> DisplaySurface * old_surface = con -> surface ; <nl> DisplayChangeListener * dcl ; <nl>  <nl> + assert ( old_surface != surface ); <nl> + <nl> con -> surface = surface ; <nl> QLIST_FOREACH ( dcl , & s -> listeners , next ) { <nl> if ( con != ( dcl -> con ? dcl -> con : active_console )) {
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> + qemu_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
ram_addr_t qemu_ram_alloc_from_ptr ( ram_addr_t size , void * host , <nl>  <nl> qemu_ram_setup_dump ( new_block -> host , size ); <nl> qemu_madvise ( new_block -> host , size , QEMU_MADV_HUGEPAGE ); <nl> + qemu_madvise ( new_block -> host , size , QEMU_MADV_DONTFORK ); <nl>  <nl> if ( kvm_enabled ()) <nl> kvm_setup_guest_memory ( new_block -> host , size );
void ide_dma_cb ( void * opaque , int ret ) <nl> sector_num , n , s -> dma_cmd ); <nl> # endif <nl>  <nl> - if (! ide_sect_range_ok ( s , sector_num , n )) { <nl> + if (( s -> dma_cmd == IDE_DMA_READ || s -> dma_cmd == IDE_DMA_WRITE ) && <nl> + ! ide_sect_range_ok ( s , sector_num , n )) { <nl> dma_buf_commit ( s ); <nl> ide_dma_error ( s ); <nl> return ;
static void test_visitor_in_fuzz ( TestInputVisitorData * data , <nl>  <nl> v = visitor_input_test_init ( data , buf ); <nl> visit_type_intList ( v , NULL , & ilres , NULL ); <nl> + qapi_free_intList ( ilres ); <nl> visitor_input_teardown ( data , NULL ); <nl>  <nl> v = visitor_input_test_init ( data , buf );
static int local_symlink ( FsContext * fs_ctx , const char * oldpath , <nl> return fd ; <nl> } <nl> /* Write the oldpath ( target ) to the file . */ <nl> - oldpath_size = strlen ( oldpath ) + 1 ; <nl> + oldpath_size = strlen ( oldpath ); <nl> do { <nl> write_size = write ( fd , ( void *) oldpath , oldpath_size ); <nl> } while ( write_size == - 1 && errno == EINTR );
static const char * stream_video_names [] = { <nl> [ SPICE_STREAM_VIDEO_FILTER ] = " filter ", <nl> }; <nl> # define parse_stream_video ( _name ) \ <nl> - name2enum ( _name , stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl> + parse_name ( _name , " stream video control ", \ <nl> + stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl>  <nl> static const char * compression_names [] = { <nl> [ SPICE_IMAGE_COMPRESS_OFF ] = " off ",
void ide_exec_cmd ( IDEBus * bus , uint32_t val ) <nl> lba48 = 1 ; <nl> /* fall through */ <nl> case WIN_READ_NATIVE_MAX : <nl> + /* Refuse if no sectors are addressable ( e . g . medium not inserted ) */ <nl> + if ( s -> nb_sectors == 0 ) { <nl> + goto abort_cmd ; <nl> + } <nl> ide_cmd_lba48_transform ( s , lba48 ); <nl> ide_set_sector ( s , s -> nb_sectors - 1 ); <nl> s -> status = READY_STAT | SEEK_STAT ;
static void curses_refresh ( DisplayChangeListener * dcl ) <nl> qemu_input_event_send_key_delay ( 0 ); <nl> } <nl> } else { <nl> - keysym = curses2qemu [ chr ]; <nl> + keysym = - 1 ; <nl> + if ( chr < CURSES_KEYS ) { <nl> + keysym = curses2qemu [ chr ]; <nl> + } <nl> if ( keysym == - 1 ) <nl> keysym = chr ; <nl> 
void cmos_set_s3_resume ( void ) <nl> } <nl>  <nl> static QEMUMachine pc_machine = { <nl> - . name = " pc ", <nl> + . name = " pc - 0 . 11 ", <nl> + . alias = " pc ", <nl> . desc = " Standard PC ", <nl> . init = pc_init_pci , <nl> . max_cpus = 255 ,
static void gen_msa ( CPUMIPSState * env , DisasContext * ctx ) <nl> case OPC_LD_H : <nl> case OPC_LD_W : <nl> case OPC_LD_D : <nl> + save_cpu_state ( ctx , 1 ); <nl> gen_helper_msa_ld_df ( cpu_env , tdf , twd , trs , ts10 ); <nl> break ; <nl> case OPC_ST_B : <nl> case OPC_ST_H : <nl> case OPC_ST_W : <nl> case OPC_ST_D : <nl> + save_cpu_state ( ctx , 1 ); <nl> gen_helper_msa_st_df ( cpu_env , tdf , twd , trs , ts10 ); <nl> break ; <nl> }
void cpu_alpha_store_fpcr ( CPUState * env , uint64_t val ) <nl> round_mode = float_round_nearest_even ; <nl> break ; <nl> case 3 : <nl> + default : /* this avoids a gcc (< 4 . 4 ) warning */ <nl> round_mode = float_round_up ; <nl> break ; <nl> }
static void ccid_handle_data ( USBDevice * dev , USBPacket * p ) <nl> " handle_data : int_in : notify_slot_change % X , " <nl> " requested len % zd \ n ", <nl> s -> bmSlotICCState , p -> iov . size ); <nl> + } else { <nl> + p -> status = USB_RET_NAK ; <nl> } <nl> break ; <nl> default :
static int kvm_physical_sync_dirty_bitmap ( MemoryRegionSection * section ) <nl> { <nl> KVMState * s = kvm_state ; <nl> unsigned long size , allocated_size = 0 ; <nl> - KVMDirtyLog d ; <nl> + KVMDirtyLog d = {}; <nl> KVMSlot * mem ; <nl> int ret = 0 ; <nl> hwaddr start_addr = section -> offset_within_address_space ;
static int target_pread ( int fd , abi_ulong ptr , abi_ulong len , <nl> int ret ; <nl>  <nl> buf = lock_user ( VERIFY_WRITE , ptr , len , 0 ); <nl> + if (! buf ) { <nl> + return - EFAULT ; <nl> + } <nl> ret = pread ( fd , buf , len , offset ); <nl> + if ( ret < 0 ) { <nl> + ret = - errno ; <nl> + } <nl> unlock_user ( buf , ptr , len ); <nl> return ret ; <nl> }
static void test_dummy_createcmdl ( void ) <nl> g_assert ( err == NULL ); <nl> error_free ( err ); <nl>  <nl> + object_unref ( OBJECT ( dobj )); <nl> + <nl> /* <nl> * cmdline - parsing via qemu_opts_parse () results in a QemuOpts entry <nl> * corresponding to the Object ' s ID to be added to the QemuOptsList
static void cpu_openrisc_load_kernel ( ram_addr_t ram_size , <nl> kernel_filename ); <nl> exit ( 1 ); <nl> } <nl> + cpu -> env . pc = entry ; <nl> } <nl> - <nl> - cpu -> env . pc = entry ; <nl> } <nl>  <nl> static void openrisc_sim_init ( QEMUMachineInitArgs * args )
void socket_listen_cleanup ( int fd , Error ** errp ) <nl> } <nl> } <nl>  <nl> - g_free ( addr ); <nl> + qapi_free_SocketAddress ( addr ); <nl> } <nl>  <nl> int socket_dgram ( SocketAddress * remote , SocketAddress * local , Error ** errp )
static void handle_keydown ( SDL_Event * ev ) <nl> case SDL_SCANCODE_7 : <nl> case SDL_SCANCODE_8 : <nl> case SDL_SCANCODE_9 : <nl> + if ( gui_grab ) { <nl> + sdl_grab_end ( scon ); <nl> + } <nl> + <nl> win = ev -> key . keysym . scancode - SDL_SCANCODE_1 ; <nl> if ( win < sdl2_num_outputs ) { <nl> sdl2_console [ win ]. hidden = ! sdl2_console [ win ]. hidden ;
static void rc4030_write ( void * opaque , hwaddr addr , uint64_t data , <nl> break ; <nl> /* Interval timer reload */ <nl> case 0x0228 : <nl> - s -> itr = val ; <nl> + s -> itr = val & 0x01FF ; <nl> qemu_irq_lower ( s -> timer_irq ); <nl> set_next_tick ( s ); <nl> break ;
static void ehci_frame_timer ( void * opaque ) <nl> int need_timer = 0 ; <nl> int64_t expire_time , t_now ; <nl> uint64_t ns_elapsed ; <nl> - int uframes , skipped_uframes ; <nl> + uint64_t uframes , skipped_uframes ; <nl> int i ; <nl>  <nl> t_now = qemu_clock_get_ns ( QEMU_CLOCK_VIRTUAL );
static int init_directory ( BDRVVVFATState * s , const char * dirname ) <nl> memset (&( s -> first_sectors [ 0 ]), 0 , 0x40 * 0x200 ); <nl>  <nl> /* TODO : if FAT32 , this is probably wrong */ <nl> - s -> sectors_per_fat = 0xfc ; <nl> + s -> sectors_per_fat = 0xec ; <nl> s -> sectors_per_cluster = 0x10 ; <nl> s -> cluster_size = s -> sectors_per_cluster * 0x200 ; <nl> s -> cluster = malloc ( s -> cluster_size );
static int pci_add_option_rom ( PCIDevice * pdev , bool is_default_rom ) <nl> pci_patch_ids ( pdev , ptr , size ); <nl> } <nl>  <nl> + qemu_put_ram_ptr ( ptr ); <nl> + <nl> pci_register_bar ( pdev , PCI_ROM_SLOT , size , <nl> 0 , pci_map_option_rom ); <nl> 
static BlockDriverAIOCB * rbd_start_aio ( BlockDriverState * bs , <nl> } <nl>  <nl> if ( r < 0 ) { <nl> - goto failed ; <nl> + goto failed_completion ; <nl> } <nl>  <nl> return & acb -> common ; <nl>  <nl> + failed_completion : <nl> + rbd_aio_release ( c ); <nl> failed : <nl> g_free ( rcb ); <nl> + qemu_vfree ( acb -> bounce ); <nl> qemu_aio_release ( acb ); <nl> return NULL ; <nl> }
SDState * sd_init ( BlockDriverState * bs , bool is_spi ) <nl> { <nl> SDState * sd ; <nl>  <nl> - if ( bdrv_is_read_only ( bs )) { <nl> + if ( bs && bdrv_is_read_only ( bs )) { <nl> fprintf ( stderr , " sd_init : Cannot use read - only drive \ n "); <nl> return NULL ; <nl> }
int qdev_device_help ( QemuOpts * opts ) <nl> } <nl>  <nl> prop_list = qmp_device_list_properties ( driver , & local_err ); <nl> - if (! prop_list ) { <nl> + if ( local_err ) { <nl> error_printf ("% s \ n ", error_get_pretty ( local_err )); <nl> error_free ( local_err ); <nl> return 1 ;
__org_qemu_x_Union1 * qmp___org_qemu_x_command ( __org_qemu_x_EnumList * a , <nl> ret -> type = ORG_QEMU_X_UNION1_KIND___ORG_QEMU_X_BRANCH ; <nl> ret -> u . __org_qemu_x_branch = strdup (" blah1 "); <nl>  <nl> + /* Also test that ' wchar - t ' was munged to ' q_wchar_t ' */ <nl> + if ( b && b -> value && ! b -> value -> has_q_wchar_t ) { <nl> + b -> value -> q_wchar_t = 1 ; <nl> + } <nl> return ret ; <nl> } <nl> 
restart : <nl> QLIST_REMOVE ( elem , all ); <nl> /* Read state before ret . */ <nl> smp_rmb (); <nl> + <nl> + /* Schedule ourselves in case elem -> common . cb () calls aio_poll () to <nl> + * wait for another request that completed at the same time . <nl> + */ <nl> + qemu_bh_schedule ( pool -> completion_bh ); <nl> + <nl> elem -> common . cb ( elem -> common . opaque , elem -> ret ); <nl> qemu_aio_release ( elem ); <nl> goto restart ;
static struct iovec * lock_iovec ( int type , abi_ulong target_addr , <nl> return vec ; <nl>  <nl> fail : <nl> + while (-- i >= 0 ) { <nl> + if ( tswapal ( target_vec [ i ]. iov_len ) > 0 ) { <nl> + unlock_user ( vec [ i ]. iov_base , tswapal ( target_vec [ i ]. iov_base ), 0 ); <nl> + } <nl> + } <nl> unlock_user ( target_vec , target_addr , 0 ); <nl> fail2 : <nl> free ( vec );
QObject * json_parser_parse ( QList * tokens , va_list * ap ) <nl> QObject * json_parser_parse_err ( QList * tokens , va_list * ap , Error ** errp ) <nl> { <nl> JSONParserContext ctxt = {}; <nl> - QList * working = qlist_copy ( tokens ); <nl> + QList * working ; <nl> QObject * result ; <nl>  <nl> + if (! tokens ) { <nl> + return NULL ; <nl> + } <nl> + working = qlist_copy ( tokens ); <nl> result = parse_value (& ctxt , & working , ap ); <nl>  <nl> QDECREF ( working );
static int open_self_cmdline ( void * cpu_env , int fd ) <nl>  <nl> if ( word_skipped ) { <nl> if ( write ( fd , cp_buf , nb_read ) != nb_read ) { <nl> + close ( fd_orig ); <nl> return - 1 ; <nl> } <nl> }
static void qemu_input_queue_process ( void * opaque ) <nl> item = QTAILQ_FIRST ( queue ); <nl> g_assert ( item -> type == QEMU_INPUT_QUEUE_DELAY ); <nl> QTAILQ_REMOVE ( queue , item , node ); <nl> + queue_count --; <nl> g_free ( item ); <nl>  <nl> while (! QTAILQ_EMPTY ( queue )) {
static void print_block_info ( Monitor * mon , BlockInfo * info , <nl> inserted -> iops_size ); <nl> } <nl>  <nl> - if ( verbose ) { <nl> + /* TODO : inserted -> image should never be null */ <nl> + if ( verbose && inserted -> image ) { <nl> monitor_printf ( mon , "\ nImages :\ n "); <nl> image_info = inserted -> image ; <nl> while ( 1 ) {
static int vmdk_init_tables ( BlockDriverState * bs , VmdkExtent * extent , <nl> Error ** errp ) <nl> { <nl> int ret ; <nl> - int l1_size , i ; <nl> + size_t l1_size ; <nl> + int i ; <nl>  <nl> /* read the L1 table */ <nl> l1_size = extent -> l1_size * sizeof ( uint32_t );
static void gen_sse ( CPUX86State * env , DisasContext * s , int b , <nl> break ; <nl> # ifdef TARGET_X86_64 <nl> case MO_64 : <nl> - tcg_gen_mulu2_i64 ( cpu_regs [ s -> vex_v ], cpu_regs [ reg ], <nl> + tcg_gen_mulu2_i64 ( cpu_T [ 0 ], cpu_T [ 1 ], <nl> cpu_T [ 0 ], cpu_regs [ R_EDX ]); <nl> + tcg_gen_mov_i64 ( cpu_regs [ s -> vex_v ], cpu_T [ 0 ]); <nl> + tcg_gen_mov_i64 ( cpu_regs [ reg ], cpu_T [ 1 ]); <nl> break ; <nl> # endif <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl>  <nl> if (! lock_user_struct ( VERIFY_WRITE , target_st , arg2 , 0 )) <nl> goto efault ; <nl> + memset ( target_st , 0 , sizeof (* target_st )); <nl> __put_user ( st . st_dev , & target_st -> st_dev ); <nl> __put_user ( st . st_ino , & target_st -> st_ino ); <nl> __put_user ( st . st_mode , & target_st -> st_mode );
void slirp_input ( const uint8_t * pkt , int pkt_len ) <nl> if (! m ) <nl> return ; <nl> /* Note : we add to align the IP header */ <nl> + if ( M_FREEROOM ( m ) < pkt_len + 2 ) { <nl> + m_inc ( m , pkt_len + 2 ); <nl> + } <nl> m -> m_len = pkt_len + 2 ; <nl> memcpy ( m -> m_data + 2 , pkt , pkt_len ); <nl> 
QEMUOptionParameter * append_option_parameters ( QEMUOptionParameter * dest , <nl> num_options += count_option_parameters ( list ); <nl>  <nl> dest = qemu_realloc ( dest , ( num_options + 1 ) * sizeof ( QEMUOptionParameter )); <nl> + dest [ num_dest_options ]. name = NULL ; <nl>  <nl> while ( list && list -> name ) { <nl> if ( get_option_parameter ( dest , list -> name ) == NULL ) {
static void qxl_reset_state ( PCIQXLDevice * d ) <nl> d -> num_free_res = 0 ; <nl> d -> last_release = NULL ; <nl> memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + qxl_update_irq ( d ); <nl> } <nl>  <nl> static void qxl_soft_reset ( PCIQXLDevice * d )
static uint32_t parse_enumeration ( char * str , <nl> { <nl> uint32_t ret = not_found_value ; <nl>  <nl> + if ( str == NULL ) <nl> + return 0 ; <nl> + <nl> while ( table -> name != NULL ) { <nl> if ( strcmp ( table -> name , str ) == 0 ) { <nl> ret = table -> value ;
if ( cmd == val ) { \ <nl> output_cmd ( IPC_STAT ); <nl> output_cmd ( IPC_INFO ); <nl> /* msgctl () commands */ <nl> - # ifdef __USER_MISC <nl> output_cmd ( MSG_STAT ); <nl> output_cmd ( MSG_INFO ); <nl> - # endif <nl> /* shmctl () commands */ <nl> output_cmd ( SHM_LOCK ); <nl> output_cmd ( SHM_UNLOCK );
void qemu_iovec_concat_iov ( QEMUIOVector * dst , <nl> { <nl> int i ; <nl> size_t done ; <nl> + <nl> + if (! sbytes ) { <nl> + return ; <nl> + } <nl> assert ( dst -> nalloc != - 1 ); <nl> for ( i = 0 , done = 0 ; done < sbytes && i < src_cnt ; i ++) { <nl> if ( soffset < src_iov [ i ]. iov_len ) {
IOCTL ( BLKFLSBUF , 0 , TYPE_NULL ) <nl> IOCTL ( BLKRASET , 0 , TYPE_INT ) <nl> IOCTL ( BLKRAGET , IOC_R , MK_PTR ( TYPE_LONG )) <nl> - IOCTL ( BLKSSZGET , IOC_R , MK_PTR ( TYPE_LONG )) <nl> + IOCTL ( BLKSSZGET , IOC_R , MK_PTR ( TYPE_INT )) <nl> IOCTL ( BLKBSZGET , IOC_R , MK_PTR ( TYPE_INT )) <nl> IOCTL_SPECIAL ( BLKPG , IOC_W , do_ioctl_blkpg , <nl> MK_PTR ( MK_STRUCT ( STRUCT_blkpg_ioctl_arg )))
static int virtio_net_handle_mac ( VirtIONet * n , uint8_t cmd , <nl> goto error ; <nl> } <nl>  <nl> - if ( in_use + mac_data . entries <= MAC_TABLE_ENTRIES ) { <nl> + if ( mac_data . entries <= MAC_TABLE_ENTRIES - in_use ) { <nl> s = iov_to_buf ( iov , iov_cnt , 0 , & macs [ in_use * ETH_ALEN ], <nl> mac_data . entries * ETH_ALEN ); <nl> if ( s != mac_data . entries * ETH_ALEN ) {
static void vfio_ccw_register_io_notifier ( VFIOCCWDevice * vcdev , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> - argsz = sizeof (* irq_set ); <nl> + argsz = sizeof (* irq_info ); <nl> irq_info = g_malloc0 ( argsz ); <nl> irq_info -> index = VFIO_CCW_IO_IRQ_INDEX ; <nl> irq_info -> argsz = argsz ;
static uint64_t alloc_cluster_offset ( BlockDriverState * bs , <nl> /* how many free clusters ? */ <nl>  <nl> while ( i < nb_clusters ) { <nl> - cluster_offset = l2_table [ l2_index + i ]; <nl> + cluster_offset = be64_to_cpu ( l2_table [ l2_index + i ]); <nl> if ( cluster_offset != 0 ) <nl> break ; <nl> i ++;
int qcow2_snapshot_load_tmp ( BlockDriverState * bs , <nl> sn = & s -> snapshots [ snapshot_index ]; <nl>  <nl> /* Allocate and read in the snapshot ' s L1 table */ <nl> - new_l1_bytes = s -> l1_size * sizeof ( uint64_t ); <nl> + new_l1_bytes = sn -> l1_size * sizeof ( uint64_t ); <nl> new_l1_table = g_malloc0 ( align_offset ( new_l1_bytes , 512 )); <nl>  <nl> ret = bdrv_pread ( bs -> file , sn -> l1_table_offset , new_l1_table , new_l1_bytes );
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> } <nl>  <nl> ip_protocol = ip -> ip_p ; <nl> - ip_data_len = be16_to_cpu ( ip -> ip_len ) - hlen ; <nl> + <nl> + ip_data_len = be16_to_cpu ( ip -> ip_len ); <nl> + if ( ip_data_len < hlen || ip_data_len > eth_payload_len ) { <nl> + goto skip_offload ; <nl> + } <nl> + ip_data_len -= hlen ; <nl>  <nl> if ( txdw0 & CP_TX_IPCS ) <nl> {
static uint64_t uart_read ( void * opaque , target_phys_addr_t offset , <nl> uint32_t c = 0 ; <nl>  <nl> offset >>= 2 ; <nl> - if ( offset > R_MAX ) { <nl> + if ( offset >= R_MAX ) { <nl> return 0 ; <nl> } else if ( offset == R_TX_RX ) { <nl> uart_read_rx_fifo ( s , & c );
static int usbredir_post_load ( void * priv , int version_id ) <nl> { <nl> USBRedirDevice * dev = priv ; <nl>  <nl> + if ( dev -> parser == NULL ) { <nl> + return 0 ; <nl> + } <nl> + <nl> switch ( dev -> device_info . speed ) { <nl> case usb_redir_speed_low : <nl> dev -> dev . speed = USB_SPEED_LOW ;
static void qxl_spice_monitors_config_async ( PCIQXLDevice * qxl , int replay ) <nl> } else { <nl> # if SPICE_SERVER_VERSION >= 0x000c06 /* release 0 . 12 . 6 */ <nl> if ( qxl -> max_outputs ) { <nl> - spice_qxl_set_monitors_config_limit (& qxl -> ssd . qxl , <nl> - qxl -> max_outputs ); <nl> + spice_qxl_set_max_monitors (& qxl -> ssd . qxl , qxl -> max_outputs ); <nl> } <nl> # endif <nl> qxl -> guest_monitors_config = qxl -> ram -> monitors_config ;
static coroutine_fn int qcow2_handle_l2meta ( BlockDriverState * bs , <nl> while ( l2meta != NULL ) { <nl> QCowL2Meta * next ; <nl>  <nl> - if (! ret && link_l2 ) { <nl> + if ( link_l2 ) { <nl> ret = qcow2_alloc_cluster_link_l2 ( bs , l2meta ); <nl> if ( ret ) { <nl> goto out ;
static inline uint64_t muldiv64 ( uint64_t a , uint32_t b , uint32_t c ) <nl> return res . ll ; <nl> } <nl>  <nl> +/* Round number down to multiple */ <nl> +# define QEMU_ALIGN_DOWN ( n , m ) (( n ) / ( m ) * ( m )) <nl> + <nl> +/* Round number up to multiple */ <nl> +# define QEMU_ALIGN_UP ( n , m ) QEMU_ALIGN_DOWN (( n ) + ( m ) - 1 , ( m )) <nl> + <nl> # include " module . h " <nl>  <nl> # endif
int qcow2_alloc_clusters_at ( BlockDriverState * bs , uint64_t offset , <nl> BDRVQcowState * s = bs -> opaque ; <nl> uint64_t cluster_index ; <nl> uint64_t old_free_cluster_index ; <nl> - int i , refcount , ret ; <nl> + uint64_t i ; <nl> + int refcount , ret ; <nl> + <nl> + assert ( nb_clusters >= 0 ); <nl> + if ( nb_clusters == 0 ) { <nl> + return 0 ; <nl> + } <nl>  <nl> /* Check how many clusters there are free */ <nl> cluster_index = offset >> s -> cluster_bits ;
int main ( int argc , char ** argv , char ** envp ) <nl> HD_OPTS ); <nl> break ; <nl> case QEMU_OPTION_drive : <nl> - drive_def ( optarg ); <nl> + if ( drive_def ( optarg ) == NULL ) { <nl> + exit ( 1 ); <nl> + } <nl> break ; <nl> case QEMU_OPTION_set : <nl> if ( qemu_set_option ( optarg ) != 0 )
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } <nl> if ( dev -> setup_index < 0 || <nl> dev -> setup_len < 0 || <nl> - dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> - dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> + dev -> setup_index > dev -> setup_len || <nl> + dev -> setup_len > sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> } <nl> return 0 ;
static inline void gen_op_arith_compute_ov ( DisasContext * ctx , TCGv arg0 , <nl> { <nl> TCGv t0 = tcg_temp_new (); <nl>  <nl> - tcg_gen_xor_tl ( cpu_ov , arg0 , arg1 ); <nl> + tcg_gen_xor_tl ( cpu_ov , arg0 , arg2 ); <nl> tcg_gen_xor_tl ( t0 , arg1 , arg2 ); <nl> if ( sub ) { <nl> tcg_gen_and_tl ( cpu_ov , cpu_ov , t0 );
static void create_flash ( const VirtBoardInfo * vbi ) <nl> error_report (" Could not load ROM image '% s '", bios_name ); <nl> exit ( 1 ); <nl> } <nl> - g_free ( fn ); <nl> } <nl>  <nl> create_one_flash (" virt . flash0 ", flashbase , flashsize );
int bdrv_all_delete_snapshot ( const char * name , BlockDriverState ** first_bad_bs , <nl> if ( bdrv_can_snapshot ( bs ) && <nl> bdrv_snapshot_find ( bs , snapshot , name ) >= 0 ) { <nl> ret = bdrv_snapshot_delete_by_id_or_name ( bs , name , err ); <nl> - if ( ret < 0 ) { <nl> - goto fail ; <nl> - } <nl> } <nl> aio_context_release ( ctx ); <nl> if ( ret < 0 ) {
static int cpudef_setfield ( const char * name , const char * str , void * opaque ) <nl> int err = 0 ; <nl>  <nl> if (! strcmp ( name , " name ")) { <nl> + g_free (( void *) def -> name ); <nl> def -> name = g_strdup ( str ); <nl> } else if (! strcmp ( name , " model_id ")) { <nl> strncpy ( def -> model_id , str , sizeof ( def -> model_id ));
int s390_virtio_hypercall ( CPUState * env , uint64_t mem , uint64_t hypercall ) <nl>  <nl> dev = s390_virtio_bus_find_mem ( s390_bus , mem ); <nl> virtio_reset ( dev -> vdev ); <nl> + stb_phys ( dev -> dev_offs + VIRTIO_DEV_OFFS_STATUS , 0 ); <nl> s390_virtio_device_sync ( dev ); <nl> break ; <nl> }
static void core_prop_set_core_id ( Object * obj , Visitor * v , const char * name , <nl> return ; <nl> } <nl>  <nl> + if ( value < 0 ) { <nl> + error_setg ( errp , " Invalid core id %" PRId64 , value ); <nl> + return ; <nl> + } <nl> + <nl> core -> core_id = value ; <nl> } <nl> 
void laio_cleanup ( void * s_ ) <nl> struct qemu_laio_state * s = s_ ; <nl>  <nl> event_notifier_cleanup (& s -> e ); <nl> + <nl> + if ( io_destroy ( s -> ctx ) != 0 ) { <nl> + fprintf ( stderr , "% s : destroy AIO context % p failed \ n ", <nl> + __func__ , & s -> ctx ); <nl> + } <nl> g_free ( s ); <nl> }
static int cpu_post_load ( void * opaque , int version_id ) <nl> ppc_set_compat ( cpu , cpu -> compat_pvr , & local_err ); <nl> if ( local_err ) { <nl> error_report_err ( local_err ); <nl> - error_free ( local_err ); <nl> return - 1 ; <nl> } <nl> } else
static void format_string ( StringOutputVisitor * sov , Range * r , bool next , <nl> { <nl> if ( r -> end - r -> begin > 1 ) { <nl> if ( human ) { <nl> - g_string_append_printf ( sov -> string , " 0x %" PRIx64 "-%" PRIx64 , <nl> + g_string_append_printf ( sov -> string , " 0x %" PRIx64 "- 0x %" PRIx64 , <nl> r -> begin , r -> end - 1 ); <nl>  <nl> } else {
int paio_init ( void ) <nl> s -> first_aio = NULL ; <nl> if ( qemu_pipe ( fds ) == - 1 ) { <nl> fprintf ( stderr , " failed to create pipe \ n "); <nl> + g_free ( s ); <nl> return - 1 ; <nl> } <nl> 
int load_snapshot ( const char * name , Error ** errp ) <nl>  <nl> aio_context_acquire ( aio_context ); <nl> ret = qemu_loadvm_state ( f ); <nl> - qemu_fclose ( f ); <nl> aio_context_release ( aio_context ); <nl>  <nl> migration_incoming_state_destroy ();
static void machvirt_init ( MachineState * machine ) <nl> " secure - memory ", & error_abort ); <nl> } <nl>  <nl> - object_property_set_bool ( cpuobj , true , " realized ", NULL ); <nl> + object_property_set_bool ( cpuobj , true , " realized ", & error_fatal ); <nl> object_unref ( cpuobj ); <nl> } <nl> fdt_add_timer_nodes ( vms );
bool timerlist_expired ( QEMUTimerList * timer_list ) <nl> expire_time = timer_list -> active_timers -> expire_time ; <nl> qemu_mutex_unlock (& timer_list -> active_timers_lock ); <nl>  <nl> - return expire_time < qemu_clock_get_ns ( timer_list -> clock -> type ); <nl> + return expire_time <= qemu_clock_get_ns ( timer_list -> clock -> type ); <nl> } <nl>  <nl> bool qemu_clock_expired ( QEMUClockType type )
static KeyValue * copy_key_value ( KeyValue * src ) <nl> { <nl> KeyValue * dst = g_new ( KeyValue , 1 ); <nl> memcpy ( dst , src , sizeof (* src )); <nl> + if ( dst -> type == KEY_VALUE_KIND_NUMBER ) { <nl> + QKeyCode code = qemu_input_key_number_to_qcode ( dst -> u . number . data ); <nl> + dst -> type = KEY_VALUE_KIND_QCODE ; <nl> + dst -> u . qcode . data = code ; <nl> + } <nl> return dst ; <nl> } <nl> 
static void close_guest_eventfds ( IVShmemState * s , int posn ) <nl> { <nl> int i , guest_curr_max ; <nl>  <nl> + if (! ivshmem_has_feature ( s , IVSHMEM_IOEVENTFD )) { <nl> + return ; <nl> + } <nl> + <nl> guest_curr_max = s -> peers [ posn ]. nb_eventfds ; <nl>  <nl> memory_region_transaction_begin ();
static void do_ext_interrupt ( CPUS390XState * env ) <nl>  <nl> static void do_io_interrupt ( CPUS390XState * env ) <nl> { <nl> - uint64_t mask , addr ; <nl> + uint64_t mask = 0 , addr = 0 ; <nl> LowCore * lowcore ; <nl> IOIntQueue * q ; <nl> uint8_t isc ;
static void tcp_chr_tls_init ( CharDriverState * chr ) <nl> if ( tioc == NULL ) { <nl> error_free ( err ); <nl> tcp_chr_disconnect ( chr ); <nl> + return ; <nl> } <nl> object_unref ( OBJECT ( s -> ioc )); <nl> s -> ioc = QIO_CHANNEL ( tioc );
static CharDriverState * qemu_chr_open_tty ( QemuOpts * opts ) <nl> } <nl> tty_serial_init ( fd , 115200 , ' N ', 8 , 1 ); <nl> chr = qemu_chr_open_fd ( fd , fd ); <nl> - if (! chr ) { <nl> - close ( fd ); <nl> - return NULL ; <nl> - } <nl> chr -> chr_ioctl = tty_serial_ioctl ; <nl> chr -> chr_close = qemu_chr_close_tty ; <nl> return chr ;
static void coroutine_fn mirror_run ( void * opaque ) <nl>  <nl> s -> common . len = bdrv_getlength ( bs ); <nl> if ( s -> common . len <= 0 ) { <nl> - block_job_completed (& s -> common , s -> common . len ); <nl> - return ; <nl> + ret = s -> common . len ; <nl> + goto immediate_exit ; <nl> } <nl>  <nl> length = DIV_ROUND_UP ( s -> common . len , s -> granularity );
static inline void gen_illegal_opcode ( DisasContext * s ) <nl> gen_program_exception ( s , PGM_SPECIFICATION ); <nl> } <nl>  <nl> - static inline void check_privileged ( DisasContext * s ) <nl> +# ifndef CONFIG_USER_ONLY <nl> + static void check_privileged ( DisasContext * s ) <nl> { <nl> if ( s -> tb -> flags & ( PSW_MASK_PSTATE >> 32 )) { <nl> gen_program_exception ( s , PGM_PRIVILEGED ); <nl> } <nl> } <nl> +# endif <nl>  <nl> static TCGv_i64 get_address ( DisasContext * s , int x2 , int b2 , int d2 ) <nl> {
int rom_copy ( uint8_t * dest , hwaddr addr , size_t size ) <nl> if ( rom -> addr + rom -> romsize < addr ) { <nl> continue ; <nl> } <nl> - if ( rom -> addr > end ) { <nl> + if ( rom -> addr > end || rom -> addr < addr ) { <nl> break ; <nl> } <nl> 
int vnc_display_pw_expire ( DisplayState * ds , time_t expires ) <nl> { <nl> VncDisplay * vs = ds ? ( VncDisplay *) ds -> opaque : vnc_display ; <nl>  <nl> + if (! vs ) { <nl> + return - EINVAL ; <nl> + } <nl> + <nl> vs -> expires = expires ; <nl> return 0 ; <nl> }
static uint64_t acpi_pm_tmr_read ( void * opaque , hwaddr addr , unsigned width ) <nl> return acpi_pm_tmr_get ( opaque ); <nl> } <nl>  <nl> + static void acpi_pm_tmr_write ( void * opaque , hwaddr addr , uint64_t val , <nl> + unsigned width ) <nl> +{ <nl> + /* nothing */ <nl> +} <nl> + <nl> static const MemoryRegionOps acpi_pm_tmr_ops = { <nl> . read = acpi_pm_tmr_read , <nl> + . write = acpi_pm_tmr_write , <nl> . valid . min_access_size = 4 , <nl> . valid . max_access_size = 4 , <nl> . endianness = DEVICE_LITTLE_ENDIAN ,
typedef struct VirtQueueElement <nl> struct iovec out_sg [ VIRTQUEUE_MAX_SIZE ]; <nl> } VirtQueueElement ; <nl>  <nl> -# define VIRTIO_QUEUE_MAX 64 <nl> +# define VIRTIO_QUEUE_MAX 1024 <nl>  <nl> # define VIRTIO_NO_VECTOR 0xffff <nl> 
static void wdt_diag288_class_init ( ObjectClass * klass , void * data ) <nl> dc -> realize = wdt_diag288_realize ; <nl> dc -> unrealize = wdt_diag288_unrealize ; <nl> dc -> reset = wdt_diag288_reset ; <nl> + dc -> hotpluggable = false ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> vmsd = & vmstate_diag288 ; <nl> diag288 -> handle_timer = wdt_diag288_handle_timer ;
static void wav_capture_destroy ( void * opaque ) <nl> WAVState * wav = opaque ; <nl>  <nl> AUD_del_capture ( wav -> cap , wav ); <nl> + g_free ( wav ); <nl> } <nl>  <nl> static void wav_capture_info ( void * opaque )
static void * oss_audio_init ( void ) <nl>  <nl> if ( access ( conf -> devpath_in , R_OK | W_OK ) < 0 || <nl> access ( conf -> devpath_out , R_OK | W_OK ) < 0 ) { <nl> + g_free ( conf ); <nl> return NULL ; <nl> } <nl> return conf ;
static int parse_pci_devfn ( DeviceState * dev , Property * prop , const char * str ) <nl> return - EINVAL ; <nl> if ( fn > 7 ) <nl> return - EINVAL ; <nl> + if ( slot > 31 ) <nl> + return - EINVAL ; <nl> * ptr = slot << 3 | fn ; <nl> return 0 ; <nl> }
 <nl> # define DRC_CONTAINER_PATH "/ dr - connector " <nl> # define DRC_INDEX_TYPE_SHIFT 28 <nl> -# define DRC_INDEX_ID_MASK (~(~ 0 << DRC_INDEX_TYPE_SHIFT )) <nl> +# define DRC_INDEX_ID_MASK (( 1ULL << DRC_INDEX_TYPE_SHIFT ) - 1 ) <nl>  <nl> static sPAPRDRConnectorTypeShift get_type_shift ( sPAPRDRConnectorType type ) <nl> {
extern int ram_size ; <nl> void cpu_reset ( CPUSPARCState * env ) <nl> { <nl> memset ( env , 0 , sizeof (* env )); <nl> + tlb_flush ( env , 1 ); <nl> env -> cwp = 0 ; <nl> env -> wim = 1 ; <nl> env -> regwptr = env -> regbase + ( env -> cwp * 16 );
Object * container_get ( Object * root , const char * path ) <nl> if (! child ) { <nl> child = object_new (" container "); <nl> object_property_add_child ( obj , parts [ i ], child , NULL ); <nl> + object_unref ( child ); <nl> } <nl> } <nl> 
static void baum_chr_open ( Chardev * chr , <nl> error_setg ( errp , " brlapi__openConnection : % s ", <nl> brlapi_strerror ( brlapi_error_location ())); <nl> g_free ( handle ); <nl> + baum -> brlapi = NULL ; <nl> return ; <nl> } <nl> baum -> deferred_init = 0 ;
static int kvm_physical_sync_dirty_bitmap ( MemoryRegionSection * section ) <nl>  <nl> d . slot = mem -> slot ; <nl>  <nl> - if ( kvm_vm_ioctl ( s , KVM_GET_DIRTY_LOG , & d ) == - 1 ) { <nl> + if ( kvm_vm_ioctl ( s , KVM_GET_DIRTY_LOG , & d ) < 0 ) { <nl> DPRINTF (" ioctl failed % d \ n ", errno ); <nl> ret = - 1 ; <nl> break ;
m_free ( struct mbuf * m ) <nl> * Either free () it or put it on the free list <nl> */ <nl> if ( m -> m_flags & M_DOFREE ) { <nl> - free ( m ); <nl> m -> slirp -> mbuf_alloced --; <nl> + free ( m ); <nl> } else if (( m -> m_flags & M_FREELIST ) == 0 ) { <nl> insque ( m ,& m -> slirp -> m_freelist ); <nl> m -> m_flags = M_FREELIST ; /* Clobber other flags */
static void checkpoint ( void ) { <nl> return ; <nl> /* avoid compiler warnings : */ <nl> hexdump ( NULL , 100 ); <nl> - remove_mapping ( vvv , NULL ); <nl> + remove_mapping ( vvv , 0 ); <nl> print_mapping ( NULL ); <nl> print_direntry ( NULL ); <nl> }
void cache_insert ( PageCache * cache , uint64_t addr , uint8_t * pdata ) <nl> /* actual update of entry */ <nl> it = cache_get_by_addr ( cache , addr ); <nl>  <nl> + /* free old cached data if any */ <nl> + g_free ( it -> it_data ); <nl> + <nl> if (! it -> it_data ) { <nl> cache -> num_items ++; <nl> }
static int check_directory_consistency ( BDRVVVFATState * s , <nl>  <nl> if ( s -> used_clusters [ cluster_num ] & USED_ANY ) { <nl> fprintf ( stderr , " cluster % d used more than once \ n ", ( int ) cluster_num ); <nl> - return 0 ; <nl> + goto fail ; <nl> } <nl> s -> used_clusters [ cluster_num ] = USED_DIRECTORY ; <nl> 
static const RunStateTransition runstate_transitions_def [] = { <nl>  <nl> { RUN_STATE_PAUSED , RUN_STATE_RUNNING }, <nl> { RUN_STATE_PAUSED , RUN_STATE_FINISH_MIGRATE }, <nl> + { RUN_STATE_PAUSED , RUN_STATE_POSTMIGRATE }, <nl> { RUN_STATE_PAUSED , RUN_STATE_PRELAUNCH }, <nl> { RUN_STATE_PAUSED , RUN_STATE_COLO }, <nl>  <nl> static const RunStateTransition runstate_transitions_def [] = { <nl> { RUN_STATE_PRELAUNCH , RUN_STATE_INMIGRATE }, <nl>  <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_RUNNING }, <nl> + { RUN_STATE_FINISH_MIGRATE , RUN_STATE_PAUSED }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_POSTMIGRATE }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_PRELAUNCH }, <nl> { RUN_STATE_FINISH_MIGRATE , RUN_STATE_COLO },
static void * file_ram_alloc ( RAMBlock * block , <nl> } <nl>  <nl> /* MAP_POPULATE silently ignores failures */ <nl> - for ( i = 0 ; i < ( memory / hpagesize )- 1 ; i ++) { <nl> + for ( i = 0 ; i < ( memory / hpagesize ); i ++) { <nl> memset ( area + ( hpagesize * i ), 0 , 1 ); <nl> } <nl> 
static BlockMeasureInfo * qcow2_measure ( QemuOpts * opts , BlockDriverState * in_bs , <nl> for ( sector_num = 0 ; <nl> sector_num < ssize / BDRV_SECTOR_SIZE ; <nl> sector_num += pnum ) { <nl> - int nb_sectors = MAX ( ssize / BDRV_SECTOR_SIZE - sector_num , <nl> - INT_MAX ); <nl> + int nb_sectors = MIN ( ssize / BDRV_SECTOR_SIZE - sector_num , <nl> + BDRV_REQUEST_MAX_SECTORS ); <nl> BlockDriverState * file ; <nl> int64_t ret ; <nl> 
static uint64_t ich_elrsr_read ( CPUARMState * env , const ARMCPRegInfo * ri ) <nl> uint64_t lr = cs -> ich_lr_el2 [ i ]; <nl>  <nl> if (( lr & ICH_LR_EL2_STATE_MASK ) == 0 && <nl> - (( lr & ICH_LR_EL2_HW ) == 1 || ( lr & ICH_LR_EL2_EOI ) == 0 )) { <nl> + (( lr & ICH_LR_EL2_HW ) != 0 || ( lr & ICH_LR_EL2_EOI ) == 0 )) { <nl> value |= ( 1 << i ); <nl> } <nl> }
static void xio3130_downstream_realize ( PCIDevice * d , Error ** errp ) <nl> pcie_chassis_create ( s -> chassis ); <nl> rc = pcie_chassis_add_slot ( s ); <nl> if ( rc < 0 ) { <nl> + error_setg ( errp , " Can ' t add chassis slot , error % d ", rc ); <nl> goto err_pcie_cap ; <nl> } <nl> 
typedef struct VirtIONet { <nl> uint8_t nobcast ; <nl> uint8_t vhost_started ; <nl> struct { <nl> - int in_use ; <nl> - int first_multi ; <nl> + uint32_t in_use ; <nl> + uint32_t first_multi ; <nl> uint8_t multi_overflow ; <nl> uint8_t uni_overflow ; <nl> uint8_t * macs ;
void usb_desc_create_serial ( USBDevice * dev ) <nl> } <nl> dst += snprintf ( serial + dst , sizeof ( serial )- dst , "-% s ", dev -> port -> path ); <nl> usb_desc_set_string ( dev , index , serial ); <nl> + g_free ( path ); <nl> } <nl>  <nl> const char * usb_desc_get_string ( USBDevice * dev , uint8_t index )
static USBDevice * usb_try_create_simple ( USBBus * bus , const char * name , <nl> object_property_set_bool ( OBJECT ( dev ), true , " realized ", & err ); <nl> if ( err ) { <nl> error_propagate ( errp , err ); <nl> - error_prepend ( errp , " Failed to initialize USB device '% s ': ", <nl> - name ); <nl> - object_unparent ( OBJECT ( dev )); <nl> + error_prepend ( errp , " Failed to initialize USB device '% s ': ", name ); <nl> return NULL ; <nl> } <nl> return dev ;
int socket_connect ( SocketAddress * addr , Error ** errp , <nl> case SOCKET_ADDRESS_KIND_FD : <nl> fd = monitor_get_fd ( cur_mon , addr -> fd -> str , errp ); <nl> if ( callback ) { <nl> + qemu_set_nonblock ( fd ); <nl> callback ( fd , opaque ); <nl> } <nl> break ;
static void pc_init1 ( ram_addr_t ram_size , <nl> pci_bus = i440fx_init (& i440fx_state , & piix3_devfn , isa_irq , ram_size ); <nl> } else { <nl> pci_bus = NULL ; <nl> + i440fx_state = NULL ; <nl> isa_bus_new ( NULL ); <nl> } <nl> isa_bus_irqs ( isa_irq );
static int gicv3_gicd_no_migration_shift_bug_post_load ( void * opaque , <nl> return 0 ; <nl> } <nl>  <nl> + static bool needed_always ( void * opaque ) <nl> +{ <nl> + return true ; <nl> +} <nl> + <nl> const VMStateDescription vmstate_gicv3_gicd_no_migration_shift_bug = { <nl> . name = " arm_gicv3 / gicd_no_migration_shift_bug ", <nl> . version_id = 1 , <nl> . minimum_version_id = 1 , <nl> + . needed = needed_always , <nl> . pre_load = gicv3_gicd_no_migration_shift_bug_pre_load , <nl> . post_load = gicv3_gicd_no_migration_shift_bug_post_load , <nl> . fields = ( VMStateField []) {
void helper_pmon ( int function ) <nl> break ; <nl> case 158 : <nl> { <nl> - unsigned char * fmt = ( void *)( unsigned long ) env -> active_tc . gpr [ 4 ]; <nl> + unsigned char * fmt = ( void *)( uintptr_t ) env -> active_tc . gpr [ 4 ]; <nl> printf ("% s ", fmt ); <nl> } <nl> break ;
static void pci_host_config_write ( void * opaque , target_phys_addr_t addr , <nl>  <nl> PCI_DPRINTF ("% s addr " TARGET_FMT_plx " len % d val %" PRIx64 "\ n ", <nl> __func__ , addr , len , val ); <nl> + if ( addr != 0 || len != 4 ) { <nl> + return ; <nl> + } <nl> s -> config_reg = val ; <nl> } <nl> 
static int qemu_gluster_parse_json ( BlockdevOptionsGluster * gconf , <nl> Error * local_err = NULL ; <nl> char * str = NULL ; <nl> const char * ptr ; <nl> - size_t num_servers ; <nl> - int i , type ; <nl> + int i , type , num_servers ; <nl>  <nl> /* create opts info from runtime_json_opts list */ <nl> opts = qemu_opts_create (& runtime_json_opts , NULL , 0 , & error_abort );
void qemu_input_event_send_key_qcode ( QemuConsole * src , QKeyCode q , bool down ) <nl>  <nl> void qemu_input_event_send_key_delay ( uint32_t delay_ms ) <nl> { <nl> + if (! runstate_is_running () && ! runstate_check ( RUN_STATE_SUSPENDED )) { <nl> + return ; <nl> + } <nl> + <nl> if (! kbd_timer ) { <nl> kbd_timer = timer_new_ms ( QEMU_CLOCK_VIRTUAL , qemu_input_queue_process , <nl> & kbd_queue );
VirtIODevice * virtio_common_init ( const char * name , uint16_t device_id , <nl> vdev -> queue_sel = 0 ; <nl> vdev -> config_vector = VIRTIO_NO_VECTOR ; <nl> vdev -> vq = qemu_mallocz ( sizeof ( VirtQueue ) * VIRTIO_PCI_QUEUE_MAX ); <nl> + vdev -> vm_running = vm_running ; <nl> for ( i = 0 ; i < VIRTIO_PCI_QUEUE_MAX ; i ++) { <nl> vdev -> vq [ i ]. vector = VIRTIO_NO_VECTOR ; <nl> vdev -> vq [ i ]. vdev = vdev ;
static void vga_draw_text ( VGACommonState * s , int full_update ) <nl> cx_min = width ; <nl> cx_max = - 1 ; <nl> for ( cx = 0 ; cx < width ; cx ++) { <nl> + if ( src + sizeof ( uint16_t ) > s -> vram_ptr + s -> vram_size ) { <nl> + break ; <nl> + } <nl> ch_attr = *( uint16_t *) src ; <nl> if ( full_update || ch_attr != * ch_attr_ptr || src == cursor_ptr ) { <nl> if ( cx < cx_min )
int qcow2_update_header ( BlockDriverState * bs ) <nl> ret = sizeof (* header ); <nl> break ; <nl> default : <nl> - return - EINVAL ; <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> } <nl>  <nl> buf += ret ;
static int qcow2_check ( BlockDriverState * bs , BdrvCheckResult * result , <nl> } <nl>  <nl> if ( fix && result -> check_errors == 0 && result -> corruptions == 0 ) { <nl> - return qcow2_mark_clean ( bs ); <nl> + ret = qcow2_mark_clean ( bs ); <nl> + if ( ret < 0 ) { <nl> + return ret ; <nl> + } <nl> + return qcow2_mark_consistent ( bs ); <nl> } <nl> return ret ; <nl> }
static void x86_cpu_register_feature_bit_props ( X86CPU * cpu , <nl>  <nl> for ( i = 1 ; names [ i ]; i ++) { <nl> feat2prop ( names [ i ]); <nl> - object_property_add_alias ( obj , names [ i ], obj , g_strdup ( names [ 0 ]), <nl> + object_property_add_alias ( obj , names [ i ], obj , names [ 0 ], <nl> & error_abort ); <nl> } <nl> 
Object * container_get ( Object * root , const char * path ) <nl> } <nl> } <nl>  <nl> + g_strfreev ( parts ); <nl> + <nl> return obj ; <nl> } <nl> 
static int ata_passthrough_12_xfer_size ( SCSIDevice * dev , uint8_t * buf ) <nl> switch ( length ) { <nl> case 0 : <nl> case 3 : /* USB - specific . */ <nl> + default : <nl> xfer = 0 ; <nl> break ; <nl> case 1 : <nl> static int ata_passthrough_16_xfer_size ( SCSIDevice * dev , uint8_t * buf ) <nl> switch ( length ) { <nl> case 0 : <nl> case 3 : /* USB - specific . */ <nl> + default : <nl> xfer = 0 ; <nl> break ; <nl> case 1 :
void qmp_transaction ( TransactionActionList * dev_list , Error ** errp ) <nl> assert ( dev_info -> kind < ARRAY_SIZE ( actions )); <nl>  <nl> ops = & actions [ dev_info -> kind ]; <nl> + assert ( ops -> instance_size > 0 ); <nl> + <nl> state = g_malloc0 ( ops -> instance_size ); <nl> state -> ops = ops ; <nl> state -> action = dev_info ;
static void vnc_init_timer ( VncDisplay * vd ) <nl> vd -> timer_interval = VNC_REFRESH_INTERVAL_BASE ; <nl> if ( vd -> timer == NULL && ! QTAILQ_EMPTY (& vd -> clients )) { <nl> vd -> timer = qemu_new_timer ( rt_clock , vnc_refresh , vd ); <nl> + vnc_dpy_resize ( vd -> ds ); <nl> vnc_refresh ( vd ); <nl> } <nl> }
static void fsl_imx6_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx6_realize ; <nl> - <nl> dc -> desc = " i . MX6 SOC "; <nl> + /* Reason : Uses serial_hds [] in the realize () function */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx6_type_info = {
static inline void cpu_x86_load_seg_cache ( CPUX86State * env , <nl> } <nl>  <nl> static inline void cpu_x86_load_seg_cache_sipi ( X86CPU * cpu , <nl> - int sipi_vector ) <nl> + uint8_t sipi_vector ) <nl> { <nl> CPUState * cs = CPU ( cpu ); <nl> CPUX86State * env = & cpu -> env ;
static int connect_to_sdog ( const char * addr , const char * port ) <nl> if ( errno == EINTR ) { <nl> goto reconnect ; <nl> } <nl> + close ( fd ); <nl> break ; <nl> } <nl> 
static int64_t try_fiemap ( BlockDriverState * bs , off_t start , off_t * data , <nl>  <nl> f . fm . fm_start = start ; <nl> f . fm . fm_length = ( int64_t ) nb_sectors * BDRV_SECTOR_SIZE ; <nl> - f . fm . fm_flags = 0 ; <nl> + f . fm . fm_flags = FIEMAP_FLAG_SYNC ; <nl> f . fm . fm_extent_count = 1 ; <nl> f . fm . fm_reserved = 0 ; <nl> if ( ioctl ( s -> fd , FS_IOC_FIEMAP , & f ) == - 1 ) {
static int disas_coproc_insn ( DisasContext * s , uint32_t insn ) <nl> break ; <nl> } <nl>  <nl> - gen_set_pc_im ( s , s -> pc ); <nl> + gen_set_pc_im ( s , s -> pc - 4 ); <nl> tmpptr = tcg_const_ptr ( ri ); <nl> tcg_syn = tcg_const_i32 ( syndrome ); <nl> gen_helper_access_check_cp_reg ( cpu_env , tmpptr , tcg_syn );
static void spapr_rtc_class_init ( ObjectClass * oc , void * data ) <nl>  <nl> dc -> realize = spapr_rtc_realize ; <nl> dc -> vmsd = & vmstate_spapr_rtc ; <nl> + /* Reason : This is an internal device only for handling the hypercalls */ <nl> + dc -> user_creatable = false ; <nl>  <nl> spapr_rtas_register ( RTAS_GET_TIME_OF_DAY , " get - time - of - day ", <nl> rtas_get_time_of_day );
static int raw_read_options ( QDict * options , BlockDriverState * bs , <nl>  <nl> /* Make sure size is multiple of BDRV_SECTOR_SIZE to prevent rounding <nl> * up and leaking out of the specified area . */ <nl> - if (! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> + if ( s -> has_size && ! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> error_setg ( errp , " Specified size is not multiple of % llu ", <nl> BDRV_SECTOR_SIZE ); <nl> ret = - EINVAL ;
static void v9fs_attach ( void * opaque ) <nl> s -> root_fid = fid ; <nl> /* disable migration */ <nl> error_set (& s -> migration_blocker , QERR_VIRTFS_FEATURE_BLOCKS_MIGRATION , <nl> - s -> ctx . fs_root , s -> tag ); <nl> + s -> ctx . fs_root ? s -> ctx . fs_root : " NULL ", s -> tag ); <nl> migrate_add_blocker ( s -> migration_blocker ); <nl> out : <nl> put_fid ( pdu , fidp );
static int pci_qdev_init ( DeviceState * qdev ) <nl> pci_dev -> romfile = g_strdup ( pc -> romfile ); <nl> is_default_rom = true ; <nl> } <nl> - pci_add_option_rom ( pci_dev , is_default_rom ); <nl> + <nl> + rc = pci_add_option_rom ( pci_dev , is_default_rom ); <nl> + if ( rc != 0 ) { <nl> + pci_unregister_device ( DEVICE ( pci_dev )); <nl> + return rc ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static void spapr_memory_pre_plug ( HotplugHandler * hotplug_dev , DeviceState * dev , <nl> if ( mem_dev && ! kvmppc_is_mem_backend_page_size_ok ( mem_dev )) { <nl> error_setg ( errp , " Memory backend has bad page size . " <nl> " Use ' memory - backend - file ' with correct mem - path ."); <nl> - return ; <nl> + goto out ; <nl> } <nl> + <nl> + out : <nl> + g_free ( mem_dev ); <nl> } <nl>  <nl> struct sPAPRDIMMState {
bool aio_poll ( AioContext * ctx , bool blocking ) <nl> int count ; <nl> int timeout ; <nl>  <nl> - if ( aio_prepare ( ctx )) { <nl> + have_select_revents = aio_prepare ( ctx ); <nl> + if ( have_select_revents ) { <nl> blocking = false ; <nl> - have_select_revents = true ; <nl> } <nl>  <nl> was_dispatching = ctx -> dispatching ;
static void coroutine_fn mirror_run ( void * opaque ) <nl> } <nl>  <nl> end = s -> common . len >> BDRV_SECTOR_BITS ; <nl> - s -> buf = qemu_blockalign ( bs , s -> buf_size ); <nl> + s -> buf = qemu_try_blockalign ( bs , s -> buf_size ); <nl> + if ( s -> buf == NULL ) { <nl> + ret = - ENOMEM ; <nl> + goto immediate_exit ; <nl> + } <nl> + <nl> sectors_per_chunk = s -> granularity >> BDRV_SECTOR_BITS ; <nl> mirror_free_init ( s ); <nl> 
int qcow2_grow_l1_table ( BlockDriverState * bs , uint64_t min_size , <nl> if ( min_size <= s -> l1_size ) <nl> return 0 ; <nl>  <nl> + /* Do a sanity check on min_size before trying to calculate new_l1_size <nl> + * ( this prevents overflows during the while loop for the calculation of <nl> + * new_l1_size ) */ <nl> + if ( min_size > INT_MAX / sizeof ( uint64_t )) { <nl> + return - EFBIG ; <nl> + } <nl> + <nl> if ( exact_size ) { <nl> new_l1_size = min_size ; <nl> } else {
int kvm_arch_handle_exit ( CPUPPCState * env , struct kvm_run * run ) <nl> dprintf (" handle PAPR hypercall \ n "); <nl> run -> papr_hcall . ret = spapr_hypercall ( env , run -> papr_hcall . nr , <nl> run -> papr_hcall . args ); <nl> - ret = 1 ; <nl> + ret = 0 ; <nl> break ; <nl> # endif <nl> default :
static void handle_windowevent ( SDL_Event * ev ) <nl> { <nl> struct sdl2_console * scon = get_scon_from_window ( ev -> window . windowID ); <nl>  <nl> + if (! scon ) { <nl> + return ; <nl> + } <nl> + <nl> switch ( ev -> window . event ) { <nl> case SDL_WINDOWEVENT_RESIZED : <nl> {
static void usbredir_handle_destroy ( USBDevice * udev ) <nl> USBRedirDevice * dev = DO_UPCAST ( USBRedirDevice , dev , udev ); <nl>  <nl> qemu_chr_delete ( dev -> cs ); <nl> + dev -> cs = NULL ; <nl> /* Note must be done after qemu_chr_close , as that causes a close event */ <nl> qemu_bh_delete ( dev -> chardev_close_bh ); <nl> 
struct BusState { <nl> struct Property { <nl> const char * name ; <nl> PropertyInfo * info ; <nl> - int offset ; <nl> + ptrdiff_t offset ; <nl> uint8_t bitnr ; <nl> qtype_code qtype ; <nl> int64_t defval ;
static void bit_prop_set ( DeviceState * dev , Property * props , bool val ) <nl> uint32_t * p = qdev_get_prop_ptr ( dev , props ); <nl> uint32_t mask = qdev_get_prop_mask ( props ); <nl> if ( val ) <nl> - * p |= ~ mask ; <nl> + * p |= mask ; <nl> else <nl> * p &= ~ mask ; <nl> }
static void report_unavailable_features ( FeatureWord w , uint32_t mask ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < 32 ; ++ i ) { <nl> - if ( 1 << i & mask ) { <nl> + if (( 1UL << i ) & mask ) { <nl> const char * reg = get_register_name_32 ( f -> cpuid_reg ); <nl> assert ( reg ); <nl> fprintf ( stderr , " warning : % s doesn ' t support requested feature : "
static void gen_rot_rm_T1 ( DisasContext * s , int ot , int op1 , int is_right ) <nl> if ( is_right ) { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask - 1 ); <nl> tcg_gen_shri_tl ( cpu_cc_dst , cpu_T [ 0 ], mask ); <nl> + tcg_gen_andi_tl ( cpu_cc_dst , cpu_cc_dst , 1 ); <nl> } else { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask ); <nl> tcg_gen_andi_tl ( cpu_cc_dst , cpu_T [ 0 ], 1 );
static int vmdk_write ( BlockDriverState * bs , int64_t sector_num , <nl> { <nl> BDRVVmdkState * s = bs -> opaque ; <nl> VmdkExtent * extent = NULL ; <nl> - int n , ret ; <nl> - int64_t index_in_cluster ; <nl> + int ret ; <nl> + int64_t index_in_cluster , n ; <nl> uint64_t extent_begin_sector , extent_relative_sector_num ; <nl> uint64_t cluster_offset ; <nl> VmdkMetaData m_data ;
static int pty_chr_write ( CharDriverState * chr , const uint8_t * buf , int len ) <nl> if (! s -> connected ) { <nl> /* guest sends data , check for ( re -) connect */ <nl> pty_chr_update_read_handler_locked ( chr ); <nl> - return 0 ; <nl> + if (! s -> connected ) { <nl> + return 0 ; <nl> + } <nl> } <nl> return io_channel_send ( s -> fd , buf , len ); <nl> }
static void vnc_init_basic_info_from_server_addr ( QIOChannelSocket * ioc , <nl> { <nl> SocketAddress * addr = NULL ; <nl>  <nl> + if (! ioc ) { <nl> + error_setg ( errp , " No listener socket available "); <nl> + return ; <nl> + } <nl> + <nl> addr = qio_channel_socket_get_local_address ( ioc , errp ); <nl> if (! addr ) { <nl> return ;
static int raw_create ( const char * filename , QemuOpts * opts , Error ** errp ) <nl> goto out ; <nl> } <nl>  <nl> - fd = qemu_open ( filename , O_WRONLY | O_CREAT | O_TRUNC | O_BINARY , <nl> + fd = qemu_open ( filename , O_RDWR | O_CREAT | O_TRUNC | O_BINARY , <nl> 0644 ); <nl> if ( fd < 0 ) { <nl> result = - errno ;
udp_input ( register struct mbuf * m , int iphlen ) <nl> * Locate pcb for datagram . <nl> */ <nl> so = slirp -> udp_last_so ; <nl> - if ( so -> so_lport != uh -> uh_sport || <nl> + if ( so == & slirp -> udb || so -> so_lport != uh -> uh_sport || <nl> so -> so_laddr . s_addr != ip -> ip_src . s_addr ) { <nl> struct socket * tmp ; <nl> 
static void pc_fw_add_pflash_drv ( void ) <nl> bios_name = BIOS_FILENAME ; <nl> } <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> + if (! filename ) { <nl> + error_report (" Can ' t open BIOS image % s ", bios_name ); <nl> + exit ( 1 ); <nl> + } <nl>  <nl> opts = drive_add ( IF_PFLASH , - 1 , filename , " readonly = on "); <nl> 
static int check_directory_consistency ( BDRVVVFATState * s , <nl>  <nl> long_file_name lfn ; <nl> int path_len = strlen ( path ); <nl> - char path2 [ PATH_MAX ]; <nl> + char path2 [ PATH_MAX + 1 ]; <nl>  <nl> assert ( path_len < PATH_MAX ); /* len was tested before ! */ <nl> pstrcpy ( path2 , sizeof ( path2 ), path );
static void puv3_load_kernel ( const char * kernel_filename ) <nl> if ( kernel_filename == NULL && qtest_enabled ()) { <nl> return ; <nl> } <nl> - assert ( kernel_filename != NULL ); <nl> + if ( kernel_filename == NULL ) { <nl> + error_report (" kernel parameter cannot be empty "); <nl> + exit ( 1 ); <nl> + } <nl>  <nl> /* only zImage format supported */ <nl> size = load_image_targphys ( kernel_filename , KERNEL_LOAD_ADDR ,
static PCIDevice * qemu_pci_hot_add_storage ( Monitor * mon , <nl> const char * opts ) <nl> { <nl> PCIDevice * dev ; <nl> - DriveInfo * dinfo ; <nl> + DriveInfo * dinfo = NULL ; <nl> int type = - 1 ; <nl> char buf [ 128 ]; <nl> 
static int spapr_fixup_cpu_smt_dt ( void * fdt , int offset , PowerPCCPU * cpu , <nl> int index = ppc_get_vcpu_dt_id ( cpu ); <nl>  <nl> if ( cpu -> cpu_version ) { <nl> - ret = fdt_setprop ( fdt , offset , " cpu - version ", <nl> - & cpu -> cpu_version , sizeof ( cpu -> cpu_version )); <nl> + ret = fdt_setprop_cell ( fdt , offset , " cpu - version ", cpu -> cpu_version ); <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
FWCfgState * pc_memory_init ( MachineState * machine , <nl> exit ( EXIT_FAILURE ); <nl> } <nl>  <nl> + if ( QEMU_ALIGN_UP ( machine -> maxram_size , <nl> + TARGET_PAGE_SIZE ) != machine -> maxram_size ) { <nl> + error_report (" maximum memory size must by aligned to multiple of " <nl> + "% d bytes ", TARGET_PAGE_SIZE ); <nl> + exit ( EXIT_FAILURE ); <nl> + } <nl> + <nl> pcms -> hotplug_memory_base = <nl> ROUND_UP ( 0x100000000ULL + above_4g_mem_size , 1ULL << 30 ); <nl> 
static inline void gen_op_arith_compute_ov ( DisasContext * ctx , TCGv arg0 , <nl> if ( is_isa300 ( ctx )) { <nl> tcg_gen_extract_tl ( cpu_ov32 , cpu_ov , 31 , 1 ); <nl> } <nl> - tcg_gen_extract_tl ( cpu_ov , cpu_ov , 63 , 1 ); <nl> + tcg_gen_extract_tl ( cpu_ov , cpu_ov , TARGET_LONG_BITS - 1 , 1 ); <nl> } <nl> tcg_gen_or_tl ( cpu_so , cpu_so , cpu_ov ); <nl> }
static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> ret = - EINVAL ; <nl> goto fail ; <nl> } <nl> - if ( s -> max_table_entries > ( VHD_MAX_SECTORS * 512 ) / s -> block_size ) { <nl> - ret = - EINVAL ; <nl> - goto fail ; <nl> - } <nl>  <nl> computed_size = ( uint64_t ) s -> max_table_entries * s -> block_size ; <nl> if ( computed_size < bs -> total_sectors * 512 ) {
ip_input ( struct mbuf * m ) <nl> DEBUG_ARG (" m_len = % d ", m -> m_len ); <nl>  <nl> if ( m -> m_len < sizeof ( struct ip )) { <nl> - return ; <nl> + goto bad ; <nl> } <nl>  <nl> ip = mtod ( m , struct ip *);
int cpu_exec ( CPUState * env ) <nl> /* reset soft MMU for next block ( it can currently <nl> only be set by a memory fault ) */ <nl> } /* for (;;) */ <nl> + } else { <nl> + /* Reload env after longjmp - the compiler may have smashed all <nl> + * local variables as longjmp is marked ' noreturn '. */ <nl> + env = cpu_single_env ; <nl> } <nl> } /* for (;;) */ <nl> 
static void vfio_map_bar ( VFIOPCIDevice * vdev , int nr ) <nl> if ( vdev -> msix && vdev -> msix -> table_bar == nr ) { <nl> uint64_t start ; <nl>  <nl> - start = HOST_PAGE_ALIGN ( vdev -> msix -> table_offset + <nl> + start = HOST_PAGE_ALIGN (( uint64_t ) vdev -> msix -> table_offset + <nl> ( vdev -> msix -> entries * PCI_MSIX_ENTRY_SIZE )); <nl>  <nl> size = start < bar -> region . size ? bar -> region . size - start : 0 ;
fail : <nl> /* refcount checking functions */ <nl>  <nl>  <nl> - static size_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> + static uint64_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> { <nl> /* This assertion holds because there is no way we can address more than <nl> * 2 ^( 64 - 9 ) clusters at once ( with cluster size 512 = 2 ^ 9 , and because
 <nl> typedef struct SuperIOConfig <nl> { <nl> - uint8_t config [ 0xff ]; <nl> + uint8_t config [ 0x100 ]; <nl> uint8_t index ; <nl> uint8_t data ; <nl> } SuperIOConfig ;
static int alloc_cluster_link_l2 ( BlockDriverState * bs , uint64_t cluster_offset , <nl> goto err ; <nl>  <nl> for ( i = 0 ; i < j ; i ++) <nl> - free_any_clusters ( bs , old_cluster [ i ], 1 ); <nl> + free_any_clusters ( bs , be64_to_cpu ( old_cluster [ i ]), 1 ); <nl>  <nl> ret = 0 ; <nl> err :
static int vmdk_open_vmfs_sparse ( BlockDriverState * bs , <nl> } <nl> ret = vmdk_add_extent ( bs , file , false , <nl> le32_to_cpu ( header . disk_sectors ), <nl> - le32_to_cpu ( header . l1dir_offset ) << 9 , <nl> + ( int64_t ) le32_to_cpu ( header . l1dir_offset ) << 9 , <nl> 0 , <nl> le32_to_cpu ( header . l1dir_size ), <nl> 4096 ,
static int scsi_req_length ( SCSICommand * cmd , SCSIDevice * dev , uint8_t * buf ) <nl> case VERIFY_16 : <nl> if (( buf [ 1 ] & 2 ) == 0 ) { <nl> cmd -> xfer = 0 ; <nl> - } else if (( buf [ 1 ] & 4 ) == 1 ) { <nl> + } else if (( buf [ 1 ] & 4 ) != 0 ) { <nl> cmd -> xfer = 1 ; <nl> } <nl> cmd -> xfer *= dev -> blocksize ;
int inet_dgram_opts ( QemuOpts * opts ) <nl> if ( 0 != ( rc = getaddrinfo ( addr , port , & ai , & local ))) { <nl> fprintf ( stderr ," getaddrinfo (% s ,% s ): % s \ n ", addr , port , <nl> gai_strerror ( rc )); <nl> - return - 1 ; <nl> + goto err ; <nl> } <nl>  <nl> /* create socket */
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> void * obj ; <nl> int i , j ; <nl>  <nl> + if (! object_dynamic_cast ( qdev_get_machine (), TYPE_SPAPR_MACHINE )) { <nl> + error_setg ( errp , " spapr - cpu - core needs a pseries machine "); <nl> + return ; <nl> + } <nl> + <nl> sc -> threads = g_malloc0 ( size * cc -> nr_threads ); <nl> for ( i = 0 ; i < cc -> nr_threads ; i ++) { <nl> char id [ 32 ];
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> int tcp_hlen = TCP_HEADER_DATA_OFFSET ( p_tcp_hdr ); <nl>  <nl> + /* Invalid TCP data offset ? */ <nl> + if ( tcp_hlen < sizeof ( tcp_header ) || tcp_hlen > ip_data_len ) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ETH_MTU = ip header len + tcp header len + payload */ <nl> int tcp_data_len = ip_data_len - tcp_hlen ; <nl> int tcp_chunk_size = ETH_MTU - hlen - tcp_hlen ;
static int xen_pt_initfn ( PCIDevice * d ) <nl>  <nl> /* Initialize virtualized PCI configuration ( Extended 256 Bytes ) */ <nl> if ( xen_host_pci_get_block (& s -> real_device , 0 , d -> config , <nl> - PCI_CONFIG_SPACE_SIZE ) == - 1 ) { <nl> + PCI_CONFIG_SPACE_SIZE ) < 0 ) { <nl> xen_host_pci_device_put (& s -> real_device ); <nl> return - 1 ; <nl> }
InetSocketAddress * inet_parse ( const char * str , Error ** errp ) <nl> { <nl> InetSocketAddress * addr ; <nl> const char * optstr , * h ; <nl> - char host [ 64 ]; <nl> + char host [ 65 ]; <nl> char port [ 33 ]; <nl> int to ; <nl> int pos ;
static void coroutine_fn verify_entered_step_2 ( void * opaque ) <nl> /* Once more to check it still works after yielding */ <nl> g_assert ( qemu_coroutine_entered ( caller )); <nl> g_assert ( qemu_coroutine_entered ( qemu_coroutine_self ())); <nl> - qemu_coroutine_yield (); <nl> } <nl>  <nl> static void coroutine_fn verify_entered_step_1 ( void * opaque )
void virtio_queue_set_notification ( VirtQueue * vq , int enable ) <nl> } else { <nl> vring_used_flags_set_bit ( vq , VRING_USED_F_NO_NOTIFY ); <nl> } <nl> + if ( enable ) { <nl> + /* Expose avail event / used flags before caller checks the avail idx . */ <nl> + smp_mb (); <nl> + } <nl> } <nl>  <nl> int virtio_queue_ready ( VirtQueue * vq )
static int usb_host_open ( USBHostDevice * dev , int bus_num , <nl>  <nl> dev -> bus_num = bus_num ; <nl> dev -> addr = addr ; <nl> - strcpy ( dev -> port , port ); <nl> + pstrcpy ( dev -> port , sizeof ( dev -> port ), port ); <nl> dev -> fd = fd ; <nl>  <nl> /* read the device description */
static int iscsi_truncate ( BlockDriverState * bs , int64_t offset ) <nl> if ( iscsilun -> allocationmap != NULL ) { <nl> g_free ( iscsilun -> allocationmap ); <nl> iscsilun -> allocationmap = <nl> - bitmap_new ( DIV_ROUND_UP ( bs -> total_sectors , <nl> + bitmap_new ( DIV_ROUND_UP ( sector_lun2qemu ( iscsilun -> num_blocks , <nl> + iscsilun ), <nl> iscsilun -> cluster_sectors )); <nl> } <nl> 
static void write_bootloader ( uint8_t * base , int64_t run_addr , <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> stl_p ( p ++, 0x0ff0021c ); /* jal 870 */ <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> - stl_p ( p ++, 0x08000205 ); /* j 814 */ <nl> + stl_p ( p ++, 0x1000fff9 ); /* b 814 */ <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> stl_p ( p ++, 0x01a00009 ); /* jalr t5 */ <nl> stl_p ( p ++, 0x01602021 ); /* move a0 , t3 */
static void co_read_response ( void * opaque ) <nl> s -> co_recv = qemu_coroutine_create ( aio_read_response , opaque ); <nl> } <nl>  <nl> - aio_co_wake ( s -> co_recv ); <nl> + aio_co_enter ( s -> aio_context , s -> co_recv ); <nl> } <nl>  <nl> static void co_write_request ( void * opaque )
struct target_sigcontext { <nl> /* A Sparc stack frame */ <nl> struct sparc_stackf { <nl> abi_ulong locals [ 8 ]; <nl> - abi_ulong ins [ 6 ]; <nl> - struct sparc_stackf * fp ; <nl> - abi_ulong callers_pc ; <nl> + abi_ulong ins [ 8 ]; <nl> + /* It ' s simpler to treat fp and callers_pc as elements of ins [] <nl> + * since we never need to access them ourselves . <nl> + */ <nl> char * structptr ; <nl> abi_ulong xargs [ 6 ]; <nl> abi_ulong xxargs [ 1 ];
static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl>  <nl> ret = do_sd_create ( s , & new_vid , 1 , & local_err ); <nl> if ( ret < 0 ) { <nl> - error_report_err ( local_err ); <nl> - error_report (" failed to create inode for snapshot . % s ", <nl> - strerror ( errno )); <nl> + error_report (" failed to create inode for snapshot : % s ", <nl> + error_get_pretty ( local_err )); <nl> goto cleanup ; <nl> } <nl> 
static void vscsi_report_luns ( VSCSIState * s , vscsi_req * req ) <nl> len = n + 8 ; <nl>  <nl> resp_data = g_malloc0 ( len ); <nl> - memset ( resp_data , 0 , len ); <nl> stl_be_p ( resp_data , n ); <nl> i = found_lun0 ? 8 : 16 ; <nl> QTAILQ_FOREACH ( kid , & s -> bus . qbus . children , sibling ) {
int main ( int argc , char ** argv ) <nl> return 0 ; <nl> } <nl> argv += optind ; <nl> - optind = 1 ; <nl> + optind = 0 ; <nl>  <nl> if (! trace_init_backends ()) { <nl> exit ( 1 );
static int bdrv_rw_co ( BlockDriverState * bs , int64_t sector_num , uint8_t * buf , <nl> . iov_len = nb_sectors * BDRV_SECTOR_SIZE , <nl> }; <nl>  <nl> + if ( nb_sectors < 0 || nb_sectors > INT_MAX / BDRV_SECTOR_SIZE ) { <nl> + return - EINVAL ; <nl> + } <nl> + <nl> qemu_iovec_init_external (& qiov , & iov , 1 ); <nl> return bdrv_prwv_co ( bs , sector_num << BDRV_SECTOR_BITS , <nl> & qiov , is_write , flags );
static coroutine_fn void nbd_read_reply_entry ( void * opaque ) <nl> { <nl> NBDClientSession * s = opaque ; <nl> uint64_t i ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> Error * local_err = NULL ; <nl>  <nl> while (! s -> quit ) {
static int kvm_set_user_memory_region ( KVMState * s , KVMSlot * slot ) <nl> if ( s -> migration_log ) { <nl> mem . flags |= KVM_MEM_LOG_DIRTY_PAGES ; <nl> } <nl> - if ( mem . flags & KVM_MEM_READONLY ) { <nl> + <nl> + if ( slot -> memory_size && mem . flags & KVM_MEM_READONLY ) { <nl> /* Set the slot size to 0 before setting the slot to the desired <nl> * value . This is needed based on KVM commit 75d61fbc . */ <nl> mem . memory_size = 0 ;
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + vga_dirty_log_start (& d -> vga ); <nl> } <nl>  <nl> static void qxl_exit_vga_mode ( PCIQXLDevice * d ) <nl> static void qxl_exit_vga_mode ( PCIQXLDevice * d ) <nl> return ; <nl> } <nl> trace_qxl_exit_vga_mode ( d -> id ); <nl> + vga_dirty_log_stop (& d -> vga ); <nl> qxl_destroy_primary ( d , QXL_SYNC ); <nl> } <nl> 
static int vfio_msix_vector_do_use ( PCIDevice * pdev , unsigned int nr , <nl> vfio_update_kvm_msi_virq ( vector , * msg , pdev ); <nl> } <nl> } else { <nl> - vfio_add_kvm_msi_virq ( vdev , vector , nr , true ); <nl> + if ( msg ) { <nl> + vfio_add_kvm_msi_virq ( vdev , vector , nr , true ); <nl> + } <nl> } <nl>  <nl> /*
void vnc_display_open ( const char * id , Error ** errp ) <nl> if ( vs -> ws_enabled ) { <nl> vs -> lwebsock = inet_listen_opts ( wsopts , 0 , errp ); <nl> if ( vs -> lwebsock < 0 ) { <nl> - if ( vs -> lsock ) { <nl> + if ( vs -> lsock != - 1 ) { <nl> close ( vs -> lsock ); <nl> vs -> lsock = - 1 ; <nl> }
int spapr_populate_pci_devices ( sPAPRPHBState * phb , <nl> reg [ 0 ]. size = 0 ; <nl>  <nl> n = 0 ; <nl> - for ( i = 0 ; i < PCI_NUM_REGIONS ; ++ i ) { <nl> + for ( i = 0 ; i < ARRAY_SIZE ( bars ); ++ i ) { <nl> if ( 0 == dev -> io_regions [ i ]. size ) { <nl> continue ; <nl> }
static ram_addr_t qxl_rom_size ( void ) <nl> sizeof ( qxl_modes ); <nl> uint32_t rom_size = 8192 ; /* two pages */ <nl>  <nl> - required_rom_size = MAX ( required_rom_size , TARGET_PAGE_SIZE ); <nl> - required_rom_size = msb_mask ( required_rom_size * 2 - 1 ); <nl> - assert ( required_rom_size <= rom_size ); <nl> + QEMU_BUILD_BUG_ON ( required_rom_size > rom_size ); <nl> return rom_size ; <nl> } <nl> 
void qemu_input_event_send_key ( QemuConsole * src , KeyValue * key , bool down ) <nl> } else if ( queue_count < queue_limit ) { <nl> qemu_input_queue_event (& kbd_queue , src , evt ); <nl> qemu_input_queue_sync (& kbd_queue ); <nl> + } else { <nl> + qapi_free_InputEvent ( evt ); <nl> } <nl> } <nl> 
S390CPU * s390x_new_cpu ( const char * typename , uint32_t core_id , Error ** errp ) <nl> object_property_set_bool ( OBJECT ( cpu ), true , " realized ", & err ); <nl>  <nl> out : <nl> + object_unref ( OBJECT ( cpu )); <nl> if ( err ) { <nl> error_propagate ( errp , err ); <nl> - object_unref ( OBJECT ( cpu )); <nl> cpu = NULL ; <nl> } <nl> return cpu ;
static void pc_isa_bios_init ( MemoryRegion * rom_memory , <nl> flash_size = memory_region_size ( flash_mem ); <nl>  <nl> /* map the last 128KB of the BIOS in ISA space */ <nl> - isa_bios_size = flash_size ; <nl> - if ( isa_bios_size > ( 128 * 1024 )) { <nl> - isa_bios_size = 128 * 1024 ; <nl> - } <nl> + isa_bios_size = MIN ( flash_size , 128 * 1024 ); <nl> isa_bios = g_malloc ( sizeof (* isa_bios )); <nl> memory_region_init_ram ( isa_bios , NULL , " isa - bios ", isa_bios_size ); <nl> vmstate_register_ram_global ( isa_bios );
typedef struct CPUX86State { <nl> uint8_t has_error_code ; <nl> uint32_t sipi_vector ; <nl> bool tsc_valid ; <nl> - int tsc_khz ; <nl> + int64_t tsc_khz ; <nl> void * kvm_xsave_buf ; <nl>  <nl> uint64_t mcg_cap ;
fail : <nl> qemu_co_mutex_unlock (& s -> lock ); <nl>  <nl> qemu_iovec_destroy (& hd_qiov ); <nl> + g_free ( cluster_data ); <nl>  <nl> return ret ; <nl> } <nl> fail : <nl> qemu_co_mutex_unlock (& s -> lock ); <nl>  <nl> qemu_iovec_destroy (& hd_qiov ); <nl> + g_free ( cluster_data ); <nl>  <nl> return ret ; <nl> }
void socket_listen_cleanup ( int fd , Error ** errp ) <nl> SocketAddress * addr ; <nl>  <nl> addr = socket_local_address ( fd , errp ); <nl> + if (! addr ) { <nl> + return ; <nl> + } <nl>  <nl> if ( addr -> type == SOCKET_ADDRESS_TYPE_UNIX <nl> && addr -> u . q_unix . path ) {
void block_job_set_speed ( BlockJob * job , int64_t speed , Error ** errp ) <nl> } <nl>  <nl> job -> speed = speed ; <nl> - if ( speed <= old_speed ) { <nl> + if ( speed && speed <= old_speed ) { <nl> return ; <nl> } <nl> 
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> { <nl> DPRINTF ("+++ C + mode offloaded task checksum \ n "); <nl>  <nl> + /* Large enough for Ethernet and IP headers ? */ <nl> + if ( saved_size < ETH_HLEN + sizeof ( ip_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ip packet header */ <nl> ip_header * ip = NULL ; <nl> int hlen = 0 ;
static void spapr_cpu_core_realize_child ( Object * child , Error ** errp ) <nl> Object * obj ; <nl>  <nl> obj = object_new ( spapr -> icp_type ); <nl> - object_property_add_child ( OBJECT ( cpu ), " icp ", obj , NULL ); <nl> + object_property_add_child ( OBJECT ( cpu ), " icp ", obj , & error_abort ); <nl> + object_unref ( obj ); <nl> object_property_add_const_link ( obj , " xics ", OBJECT ( spapr ), & error_abort ); <nl> object_property_set_bool ( obj , true , " realized ", & local_err ); <nl> if ( local_err ) {
typedef struct MirrorBlockJob { <nl>  <nl> unsigned long * in_flight_bitmap ; <nl> int in_flight ; <nl> - int sectors_in_flight ; <nl> + int64_t sectors_in_flight ; <nl> int ret ; <nl> bool unmap ; <nl> bool waiting_for_io ;
static void xlnx_ep108_init ( MachineState * machine ) <nl> machine -> ram_size = EP108_MAX_RAM_SIZE ; <nl> } <nl>  <nl> - if ( machine -> ram_size <= 0x08000000 ) { <nl> + if ( machine -> ram_size < 0x08000000 ) { <nl> qemu_log (" WARNING : RAM size " RAM_ADDR_FMT " is small for EP108 ", <nl> machine -> ram_size ); <nl> }
static target_ulong h_client_architecture_support ( PowerPCCPU * cpu , <nl> error_report_err ( local_err ); <nl> return H_HARDWARE ; <nl> } <nl> + error_free ( local_err ); <nl> local_err = NULL ; <nl> } <nl> }
e1000_link_up ( E1000State * s ) <nl> { <nl> s -> mac_reg [ STATUS ] |= E1000_STATUS_LU ; <nl> s -> phy_reg [ PHY_STATUS ] |= MII_SR_LINK_STATUS ; <nl> + <nl> + /* E1000_STATUS_LU is tested by e1000_can_receive () */ <nl> + qemu_flush_queued_packets ( qemu_get_queue ( s -> nic )); <nl> } <nl>  <nl> static bool
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> } <nl>  <nl> err : <nl> - while ( i >= 0 ) { <nl> + while (-- i >= 0 ) { <nl> obj = sc -> threads + i * size ; <nl> object_unparent ( obj ); <nl> - i --; <nl> } <nl> g_free ( sc -> threads ); <nl> error_propagate ( errp , local_err );
static int get_device_guid ( <nl> & len ); <nl>  <nl> if ( status != ERROR_SUCCESS || name_type != REG_SZ ) { <nl> - return - 1 ; <nl> + ++ i ; <nl> + continue ; <nl> } <nl> else { <nl> if ( is_tap_win32_dev ( enum_name )) {
CharDriverState * qemu_chr_new_from_opts ( QemuOpts * opts , <nl> if ( i == NULL ) { <nl> error_setg ( errp , " chardev : backend \"% s \" not found ", <nl> qemu_opt_get ( opts , " backend ")); <nl> - return NULL ; <nl> + goto err ; <nl> } <nl>  <nl> if (! cd -> open ) {
ObjectClass * object_class_dynamic_cast ( ObjectClass * class , <nl> TypeImpl * type = class -> type ; <nl> ObjectClass * ret = NULL ; <nl>  <nl> + if (! target_type ) { <nl> + /* target class type unknown , so fail the cast */ <nl> + return NULL ; <nl> + } <nl> + <nl> if ( type -> class -> interfaces && <nl> type_is_ancestor ( target_type , type_interface )) { <nl> int found = 0 ;
static void ccid_card_vscard_handle_message ( PassthruState * card , <nl> error_report (" ATR size exceeds spec , ignoring "); <nl> ccid_card_vscard_send_error ( card , scr_msg_header -> reader_id , <nl> VSC_GENERAL_ERROR ); <nl> + break ; <nl> } <nl> memcpy ( card -> atr , data , scr_msg_header -> length ); <nl> card -> atr_length = scr_msg_header -> length ;
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> - if ( chdir ("/") < 0 ) { <nl> - do_perror (" chdir "); <nl> - goto error ; <nl> - } <nl> if ( chroot ( rpath ) < 0 ) { <nl> do_perror (" chroot "); <nl> goto error ; <nl> } <nl> + if ( chdir ("/") < 0 ) { <nl> + do_perror (" chdir "); <nl> + goto error ; <nl> + } <nl>  <nl> get_version = false ; <nl> # ifdef FS_IOC_GETVERSION
static int img_amend ( int argc , char ** argv ) <nl> if (! is_valid_option_list ( optarg )) { <nl> error_report (" Invalid option list : % s ", optarg ); <nl> ret = - 1 ; <nl> - goto out ; <nl> + goto out_no_progress ; <nl> } <nl> if (! options ) { <nl> options = g_strdup ( optarg ); <nl> static int img_amend ( int argc , char ** argv ) <nl> out : <nl> qemu_progress_end (); <nl>  <nl> + out_no_progress : <nl> blk_unref ( blk ); <nl> qemu_opts_del ( opts ); <nl> qemu_opts_free ( create_opts );
static void QEMU_NORETURN force_sig ( int target_sig ) <nl> * it to arrive . */ <nl> sigfillset (& act . sa_mask ); <nl> act . sa_handler = SIG_DFL ; <nl> + act . sa_flags = 0 ; <nl> sigaction ( host_sig , & act , NULL ); <nl>  <nl> /* For some reason raise ( host_sig ) doesn ' t send the signal when
static void disas_arm_insn ( DisasContext * s , unsigned int insn ) <nl> ARCH ( 6T2 ); <nl> shift = ( insn >> 7 ) & 0x1f ; <nl> i = ( insn >> 16 ) & 0x1f ; <nl> + if ( i < shift ) { <nl> + /* UNPREDICTABLE ; we choose to UNDEF */ <nl> + goto illegal_op ; <nl> + } <nl> i = i + 1 - shift ; <nl> if ( rm == 15 ) { <nl> tmp = tcg_temp_new_i32 ();
static int qcow_create ( const char * filename , QemuOpts * opts , Error ** errp ) <nl> header_size += backing_filename_len ; <nl> } else { <nl> /* special backing file for vvfat */ <nl> + g_free ( backing_file ); <nl> backing_file = NULL ; <nl> } <nl> header . cluster_bits = 9 ; /* 512 byte cluster to avoid copying
void qemu_tcg_configure ( QemuOpts * opts , Error ** errp ) <nl> } else if ( use_icount ) { <nl> error_setg ( errp , " No MTTCG when icount is enabled "); <nl> } else { <nl> +# ifndef TARGET_SUPPORT_MTTCG <nl> + error_report (" Guest not yet converted to MTTCG - " <nl> + " you may get unexpected results "); <nl> +# endif <nl> if (! check_tcg_memory_orders_compatible ()) { <nl> error_report (" Guest expects a stronger memory ordering " <nl> " than the host provides ");
static void virtqueue_map_desc ( unsigned int * p_num_sg , hwaddr * addr , struct iove <nl> } <nl>  <nl> iov [ num_sg ]. iov_base = cpu_physical_memory_map ( pa , & len , is_write ); <nl> + if (! iov [ num_sg ]. iov_base ) { <nl> + error_report (" virtio : bogus descriptor or out of resources "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> iov [ num_sg ]. iov_len = len ; <nl> addr [ num_sg ] = pa ; <nl> 
static int vhdx_log_flush_desc ( BlockDriverState * bs , VHDXLogDescriptor * desc , <nl> /* write ' count ' sectors of sector */ <nl> memset ( buffer , 0 , VHDX_LOG_SECTOR_SIZE ); <nl> count = desc -> zero_length / VHDX_LOG_SECTOR_SIZE ; <nl> + } else { <nl> + error_report (" Invalid VHDX log descriptor entry signature 0x %" PRIx32 , <nl> + desc -> signature ); <nl> + ret = - EINVAL ; <nl> + goto exit ; <nl> } <nl>  <nl> file_offset = desc -> file_offset ;
static inline void gen_neon_mull ( TCGv_i64 dest , TCGv a , TCGv b , int size , int u ) <nl> case 4 : <nl> tmp = gen_muls_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> case 5 : <nl> tmp = gen_mulu_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> default : abort (); <nl> }
static int qxl_track_command ( PCIQXLDevice * qxl , struct QXLCommandExt * ext ) <nl> qxl -> guest_cursor = ext -> cmd . data ; <nl> qemu_mutex_unlock (& qxl -> track_lock ); <nl> } <nl> + if ( cmd -> type == QXL_CURSOR_HIDE ) { <nl> + qemu_mutex_lock (& qxl -> track_lock ); <nl> + qxl -> guest_cursor = 0 ; <nl> + qemu_mutex_unlock (& qxl -> track_lock ); <nl> + } <nl> break ; <nl> } <nl> }
static void disas_arm_insn ( CPUARMState * env , DisasContext * s ) <nl> } <nl> ARCH ( 6 ); <nl> gen_srs ( s , ( insn & 0x1f ), ( insn >> 23 ) & 3 , insn & ( 1 << 21 )); <nl> + return ; <nl> } else if (( insn & 0x0e50ffe0 ) == 0x08100a00 ) { <nl> /* rfe */ <nl> int32_t offset ;
static void gen_pusha ( DisasContext * s ) <nl> { <nl> int i ; <nl> gen_op_movl_A0_reg ( R_ESP ); <nl> - gen_op_addl_A0_im (- 8 << s -> dflag ); <nl> + gen_op_addl_A0_im (-( 8 << s -> dflag )); <nl> if (! s -> ss32 ) <nl> tcg_gen_ext16u_tl ( cpu_A0 , cpu_A0 ); <nl> tcg_gen_mov_tl ( cpu_T [ 1 ], cpu_A0 );
int main ( int argc , char * argv []) <nl> const char * arch = qtest_get_arch (); <nl> FILE * f = fopen ( disk , " w "); <nl> int ret ; <nl> + <nl> + if (! f ) { <nl> + fprintf ( stderr , " Couldn ' t open \"% s \": % s ", disk , strerror ( errno )); <nl> + return 1 ; <nl> + } <nl> fwrite ( boot_sector , 1 , sizeof boot_sector , f ); <nl> fclose ( f ); <nl> 
int kvm_arch_release_virq_post ( int virq ) <nl> if ( entry -> virq == virq ) { <nl> trace_kvm_x86_remove_msi_route ( virq ); <nl> QLIST_REMOVE ( entry , list ); <nl> + g_free ( entry ); <nl> break ; <nl> } <nl> }
static int execute_command ( BlockDriverState * bdrv , <nl> r -> io_header . flags |= SG_FLAG_DIRECT_IO ; <nl>  <nl> r -> req . aiocb = bdrv_aio_ioctl ( bdrv , SG_IO , & r -> io_header , complete , r ); <nl> + if ( r -> req . aiocb == NULL ) { <nl> + return - EIO ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static void nvdimm_dsm_set_label_data ( NVDIMMDevice * nvdimm , NvdimmDsmIn * in , <nl> return ; <nl> } <nl>  <nl> - assert ( sizeof (* in ) + sizeof (* set_label_data ) + set_label_data -> length <= <nl> - 4096 ); <nl> + assert ( offsetof ( NvdimmDsmIn , arg3 ) + <nl> + sizeof (* set_label_data ) + set_label_data -> length <= 4096 ); <nl>  <nl> nvc -> write_label_data ( nvdimm , set_label_data -> in_buf , <nl> set_label_data -> length , set_label_data -> offset );
static void become_daemon ( const char * pidfile ) <nl> goto fail ; <nl> } <nl>  <nl> - close ( STDIN_FILENO ); <nl> - close ( STDOUT_FILENO ); <nl> - close ( STDERR_FILENO ); <nl> + reopen_fd_to_null ( STDIN_FILENO ); <nl> + reopen_fd_to_null ( STDOUT_FILENO ); <nl> + reopen_fd_to_null ( STDERR_FILENO ); <nl> return ; <nl>  <nl> fail :
static unsigned hpte_page_shift ( const struct ppc_one_seg_page_size * sps , <nl>  <nl> mask = (( 1ULL << ps -> page_shift ) - 1 ) & HPTE64_R_RPN ; <nl>  <nl> - if (( pte1 & mask ) == ( ps -> pte_enc << HPTE64_R_RPN_SHIFT )) { <nl> + if (( pte1 & mask ) == (( uint64_t ) ps -> pte_enc << HPTE64_R_RPN_SHIFT )) { <nl> return ps -> page_shift ; <nl> } <nl> }
void framebuffer_update_display ( <nl>  <nl> i = * first_row ; <nl> * first_row = - 1 ; <nl> - src_len = src_width * rows ; <nl> + src_len = ( hwaddr ) src_width * rows ; <nl>  <nl> mem = mem_section -> mr ; <nl> if (! mem ) {
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl>  <nl> id_str_size = strlen ( sn -> id_str ); <nl> name_size = strlen ( sn -> name ); <nl> + assert ( id_str_size <= UINT16_MAX && name_size <= UINT16_MAX ); <nl> h . id_str_size = cpu_to_be16 ( id_str_size ); <nl> h . name_size = cpu_to_be16 ( name_size ); <nl> offset = align_offset ( offset , 8 );
static int qio_channel_buffer_close ( QIOChannel * ioc , <nl> QIOChannelBuffer * bioc = QIO_CHANNEL_BUFFER ( ioc ); <nl>  <nl> g_free ( bioc -> data ); <nl> + bioc -> data = NULL ; <nl> bioc -> capacity = bioc -> usage = bioc -> offset = 0 ; <nl>  <nl> return 0 ;
void cpu_dump_state ( CPUPPCState * env , FILE * f , fprintf_function cpu_fprintf , <nl>  <nl> int i ; <nl>  <nl> + cpu_synchronize_state ( env ); <nl> + <nl> cpu_fprintf ( f , " NIP " TARGET_FMT_lx " LR " TARGET_FMT_lx " CTR " <nl> TARGET_FMT_lx " XER " TARGET_FMT_lx "\ n ", <nl> env -> nip , env -> lr , env -> ctr , env -> xer );
VIOsPAPRDevice * vty_lookup ( sPAPRMachineState * spapr , target_ulong reg ) <nl> return spapr_vty_get_default ( spapr -> vio_bus ); <nl> } <nl>  <nl> + if (! object_dynamic_cast ( OBJECT ( sdev ), TYPE_VIO_SPAPR_VTY_DEVICE )) { <nl> + return NULL ; <nl> + } <nl> + <nl> return sdev ; <nl> } <nl> 
void machine_register_compat_props ( MachineState * machine ) <nl>  <nl> for ( i = 0 ; i < mc -> compat_props -> len ; i ++) { <nl> p = g_array_index ( mc -> compat_props , GlobalProperty *, i ); <nl> + /* Machine compat_props must never cause errors : */ <nl> + p -> errp = & error_abort ; <nl> qdev_prop_register_global ( p ); <nl> } <nl> }
int hvf_vcpu_exec ( CPUState * cpu ) <nl> macvm_set_rip ( cpu , rip + ins_len ); <nl> break ; <nl> case VMX_REASON_VMCALL : <nl> - /* TODO : inject # GP fault */ <nl> + env -> exception_injected = EXCP0D_GPF ; <nl> + env -> has_error_code = true ; <nl> + env -> error_code = 0 ; <nl> break ; <nl> default : <nl> error_report ("% llx : unhandled exit % llx \ n ", rip , exit_reason );
void qio_channel_test_run_reader ( QIOChannelTest * test , <nl>  <nl> void qio_channel_test_validate ( QIOChannelTest * test ) <nl> { <nl> + g_assert ( test -> readerr == NULL ); <nl> + g_assert ( test -> writeerr == NULL ); <nl> g_assert_cmpint ( memcmp ( test -> input , <nl> test -> output , <nl> test -> len ), ==, 0 ); <nl> - g_assert ( test -> readerr == NULL ); <nl> - g_assert ( test -> writeerr == NULL ); <nl>  <nl> g_free ( test -> inputv ); <nl> g_free ( test -> outputv );
retry : <nl>  <nl> /* Make sure that all offsets in the " allocated " range are representable <nl> * in an int64_t */ <nl> - if ( s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) { <nl> + if ( s -> free_cluster_index > 0 && <nl> + s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) <nl> + { <nl> return - EFBIG ; <nl> } <nl> 
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> if ( local_err ) { <nl> goto err ; <nl> } <nl> + object_unref ( obj ); <nl> } <nl> object_child_foreach ( OBJECT ( dev ), spapr_cpu_core_realize_child , & local_err ); <nl> if ( local_err ) {
static void cpu_common_reset ( CPUState * cpu ) <nl> log_cpu_state ( cpu , cc -> reset_dump_flags ); <nl> } <nl>  <nl> - cpu -> exit_request = 0 ; <nl> cpu -> interrupt_request = 0 ; <nl> cpu -> current_tb = NULL ; <nl> cpu -> halted = 0 ;
void qemu_system_guest_panicked ( void ) <nl> } <nl> qapi_event_send_guest_panicked ( GUEST_PANIC_ACTION_PAUSE , & error_abort ); <nl> vm_stop ( RUN_STATE_GUEST_PANICKED ); <nl> + if (! no_shutdown ) { <nl> + qapi_event_send_guest_panicked ( GUEST_PANIC_ACTION_POWEROFF , <nl> + & error_abort ); <nl> + qemu_system_shutdown_request (); <nl> + } <nl> } <nl>  <nl> void qemu_system_reset_request ( void )
static void sdhci_sdma_transfer_multi_blocks ( SDHCIState * s ) <nl> boundary_count -= block_size - begin ; <nl> } <nl> dma_memory_read (& address_space_memory , s -> sdmasysad , <nl> - & s -> fifo_buffer [ begin ], s -> data_count ); <nl> + & s -> fifo_buffer [ begin ], s -> data_count - begin ); <nl> s -> sdmasysad += s -> data_count - begin ; <nl> if ( s -> data_count == block_size ) { <nl> for ( n = 0 ; n < block_size ; n ++) {
static void ppc_spapr_init ( MachineState * machine ) <nl>  <nl> /* Set up Interrupt Controller before we create the VCPUs */ <nl> spapr -> icp = xics_system_init ( machine , <nl> - smp_cpus * kvmppc_smt_threads () / smp_threads , <nl> + DIV_ROUND_UP ( smp_cpus * kvmppc_smt_threads (), <nl> + smp_threads ), <nl> XICS_IRQS ); <nl>  <nl> /* init CPUs */
bool address_space_access_valid ( AddressSpace * as , hwaddr addr , int len , bool is_ <nl> if (! memory_access_is_direct ( mr , is_write )) { <nl> l = memory_access_size ( mr , l , addr ); <nl> if (! memory_region_access_valid ( mr , xlat , l , is_write )) { <nl> + rcu_read_unlock (); <nl> return false ; <nl> } <nl> }
static void apic_timer_update ( APICState * s , int64_t current_time ) <nl> d = ( current_time - s -> initial_count_load_time ) >> <nl> s -> count_shift ; <nl> if ( s -> lvt [ APIC_LVT_TIMER ] & APIC_LVT_TIMER_PERIODIC ) { <nl> + if (! s -> initial_count ) <nl> + goto no_timer ; <nl> d = (( d / (( uint64_t ) s -> initial_count + 1 )) + 1 ) * (( uint64_t ) s -> initial_count + 1 ); <nl> } else { <nl> if ( d >= s -> initial_count )
const char * path ( const char * name ) <nl> { <nl> /* Only do absolute paths : quick and dirty , but should mostly be OK . <nl> Could do relative by tracking cwd . */ <nl> - if (! base || name [ 0 ] != '/') <nl> + if (! base || ! name || name [ 0 ] != '/') <nl> return name ; <nl>  <nl> return follow_path ( base , name ) ?: name ;
static void vga_update_memory_access ( VGACommonState * s ) <nl> size = 0x8000 ; <nl> break ; <nl> case 3 : <nl> + default : <nl> base = 0xb8000 ; <nl> size = 0x8000 ; <nl> break ;
static int vmdk_parse_extents ( const char * desc , BlockDriverState * bs , <nl> } else { <nl> ret = vmdk_open_sparse ( bs , extent_file , bs -> open_flags , buf , errp ); <nl> } <nl> + g_free ( buf ); <nl> if ( ret ) { <nl> - g_free ( buf ); <nl> bdrv_unref ( extent_file ); <nl> return ret ; <nl> }
static void hpet_ram_writel ( void * opaque , target_phys_addr_t addr , <nl> ( timer -> config & HPET_TN_SETVAL )) <nl> timer -> cmp = ( timer -> cmp & 0xffffffff00000000ULL ) <nl> | new_val ; <nl> - else { <nl> + if ( timer_is_periodic ( timer )) { <nl> /* <nl> * FIXME : Clamp period to reasonable min value ? <nl> * Clamp period to reasonable max value
static void unix_process_msgfd ( CharDriverState * chr , struct msghdr * msg ) <nl> if ( fd < 0 ) <nl> continue ; <nl>  <nl> + /* O_NONBLOCK is preserved across SCM_RIGHTS so reset it */ <nl> + qemu_set_block ( fd ); <nl> + <nl> # ifndef MSG_CMSG_CLOEXEC <nl> qemu_set_cloexec ( fd ); <nl> # endif
tcp_sockclosed ( struct tcpcb * tp ) <nl> DEBUG_CALL (" tcp_sockclosed "); <nl> DEBUG_ARG (" tp = % p ", tp ); <nl>  <nl> + if (! tp ) { <nl> + return ; <nl> + } <nl> + <nl> switch ( tp -> t_state ) { <nl>  <nl> case TCPS_CLOSED : <nl> tcp_sockclosed ( struct tcpcb * tp ) <nl> tp -> t_state = TCPS_LAST_ACK ; <nl> break ; <nl> } <nl> - if ( tp ) <nl> - tcp_output ( tp ); <nl> + tcp_output ( tp ); <nl> } <nl>  <nl> /*
static void ivshmem_read ( void * opaque , const uint8_t * buf , int size ) <nl> if ( incoming_fd == - 1 ) { <nl> fprintf ( stderr , " could not allocate file descriptor % s \ n ", <nl> strerror ( errno )); <nl> + close ( tmp_fd ); <nl> return ; <nl> } <nl> 
static void fill_prefetch_fifo ( struct omap_gpmc_s * s ) <nl> if ( bytes > s -> prefetch . count ) { <nl> bytes = s -> prefetch . count ; <nl> } <nl> + if ( is16bit ) { <nl> + bytes &= ~ 1 ; <nl> + } <nl> + <nl> s -> prefetch . count -= bytes ; <nl> s -> prefetch . fifopointer += bytes ; <nl> fptr = 64 - s -> prefetch . fifopointer ;
static void tcg_out_movi ( TCGContext * s , TCGType type , <nl> { <nl> tcg_target_long hi , lo = ( int32_t ) arg ; <nl>  <nl> + /* Make sure we test 32 - bit constants for imm13 properly . */ <nl> + if ( type == TCG_TYPE_I32 ) { <nl> + arg = lo ; <nl> + } <nl> + <nl> /* A 13 - bit constant sign - extended to 64 - bits . */ <nl> if ( check_fit_tl ( arg , 13 )) { <nl> tcg_out_movi_imm13 ( s , ret , arg );
static void dec_barrel ( DisasContext * dc ) <nl> tcg_gen_shr_tl ( cpu_R [ dc -> rd ], cpu_R [ dc -> ra ], t0 ); <nl> } <nl> } <nl> + tcg_temp_free ( t0 ); <nl> } <nl>  <nl> static void dec_bit ( DisasContext * dc )
static int img_convert ( int argc , char ** argv ) <nl> goto out ; <nl> } <nl>  <nl> - out_bs = bdrv_new_open ( out_filename , out_fmt , BDRV_O_FLAGS | BDRV_O_RDWR ); <nl> + out_bs = bdrv_new_open ( out_filename , out_fmt , <nl> + BDRV_O_FLAGS | BDRV_O_RDWR | BDRV_O_NO_FLUSH ); <nl> if (! out_bs ) { <nl> ret = - 1 ; <nl> goto out ;
PCIBus * pci_get_bus_devfn ( int * devfnp , PCIBus * root , const char * devaddr ) <nl> int dom , bus ; <nl> unsigned slot ; <nl>  <nl> - assert (! root -> parent_dev ); <nl> - <nl> if (! root ) { <nl> fprintf ( stderr , " No primary PCI bus \ n "); <nl> return NULL ; <nl> } <nl>  <nl> + assert (! root -> parent_dev ); <nl> + <nl> if (! devaddr ) { <nl> * devfnp = - 1 ; <nl> return pci_find_bus_nr ( root , 0 );
static int send_status ( int sockfd , struct iovec * iovec , int status ) <nl> */ <nl> msg_size = proxy_marshal ( iovec , 0 , " ddd ", header . type , <nl> header . size , status ); <nl> + if ( msg_size < 0 ) { <nl> + return msg_size ; <nl> + } <nl> retval = socket_write ( sockfd , iovec -> iov_base , msg_size ); <nl> if ( retval < 0 ) { <nl> return retval ;
int qemu_fsdev_add ( QemuOpts * opts ) <nl>  <nl> if ( fsle -> fse . ops -> parse_opts ) { <nl> if ( fsle -> fse . ops -> parse_opts ( opts , & fsle -> fse )) { <nl> + g_free ( fsle -> fse . fsdev_id ); <nl> + g_free ( fsle ); <nl> return - 1 ; <nl> } <nl> }
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> /* root directory */ <nl> int cur = s -> directory . next ; <nl> array_ensure_allocated (&( s -> directory ), ROOT_ENTRIES - 1 ); <nl> + s -> directory . next = ROOT_ENTRIES ; <nl> memset ( array_get (&( s -> directory ), cur ), 0 , <nl> ( ROOT_ENTRIES - cur ) * sizeof ( direntry_t )); <nl> }
static int stdio_pclose ( void * opaque ) <nl> QEMUFileStdio * s = opaque ; <nl> int ret ; <nl> ret = pclose ( s -> stdio_file ); <nl> + if ( ret == - 1 ) { <nl> + ret = - errno ; <nl> + } <nl> g_free ( s ); <nl> return ret ; <nl> }
uint64_t helper_msub64_q_ssov ( CPUTriCoreState * env , uint64_t r1 , uint32_t r2 , <nl> } else { <nl> result = INT64_MIN ; <nl> } <nl> + } else { <nl> + env -> PSW_USB_V = 0 ; <nl> } <nl> } else { <nl> if ( ovf < 0 ) {
int do_drive_del ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> */ <nl> if ( bdrv_get_attached_dev ( bs )) { <nl> bdrv_make_anon ( bs ); <nl> + <nl> + /* Further I / O must not pause the guest */ <nl> + bdrv_set_on_error ( bs , BLOCKDEV_ON_ERROR_REPORT , <nl> + BLOCKDEV_ON_ERROR_REPORT ); <nl> } else { <nl> drive_uninit ( drive_get_by_blockdev ( bs )); <nl> }
static int create_dynamic_disk ( BlockBackend * blk , uint8_t * buf , <nl> num_bat_entries = ( total_sectors + block_size / 512 ) / ( block_size / 512 ); <nl>  <nl> ret = blk_pwrite ( blk , offset , buf , HEADER_SIZE ); <nl> - if ( ret ) { <nl> + if ( ret < 0 ) { <nl> goto fail ; <nl> } <nl> 
* significant half of a uint64_t struct member . <nl> */ <nl> # ifdef HOST_WORDS_BIGENDIAN <nl> -# define offsetoflow32 ( S , M ) offsetof ( S , M + sizeof ( uint32_t )) <nl> +# define offsetoflow32 ( S , M ) ( offsetof ( S , M ) + sizeof ( uint32_t )) <nl> # else <nl> # define offsetoflow32 ( S , M ) offsetof ( S , M ) <nl> # endif
static int check_refblocks ( BlockDriverState * bs , BdrvCheckResult * res , <nl> * nb_clusters ); <nl> memset (&(* refcount_table )[ old_nb_clusters ], 0 , <nl> (* nb_clusters - old_nb_clusters ) * <nl> - sizeof ( uint16_t )); <nl> + sizeof (** refcount_table )); <nl> } <nl> (* refcount_table )[ cluster ]--; <nl> inc_refcounts ( bs , res , * refcount_table , * nb_clusters ,
static TypeInfo arm_gic_info = { <nl> . parent = TYPE_ARM_GIC_COMMON , <nl> . instance_size = sizeof ( gic_state ), <nl> . class_init = arm_gic_class_init , <nl> + . class_size = sizeof ( ARMGICClass ), <nl> }; <nl>  <nl> static void arm_gic_register_types ( void )
R_API RList * r_io_map_get_maps_in_range ( RIO * io , ut64 addr , ut64 endaddr ) { <nl> RIOMap * map ; <nl> RListIter * iter ; <nl> RList * maps = r_list_new (); <nl> + maps -> free = NULL ; <nl> r_list_foreach ( io -> maps , iter , map ) { <nl> if ( map -> from <= addr && addr < map -> to ) r_list_append ( maps , map ); <nl> // if ( map -> from == addr && endaddr == map -> to ) r_list_append ( maps , map );
R_API void r_core_anal_autoname_all_fcns ( RCore * core ) { <nl> r_flag_rename ( core -> flags , r_flag_get ( core -> flags , fcn -> name ), name ); <nl> free ( fcn -> name ); <nl> fcn -> name = name ; <nl> + } else { <nl> + free ( name ); <nl> } <nl> } <nl> }
R_API bool r_sign_save ( RAnal * a , const char * file ) { <nl> if (! a || ! file ) { <nl> return false ; <nl> } <nl> + <nl> + if ( sdb_count ( a -> sdb_zigns ) == 0 ) { <nl> + eprintf (" WARNING : no zignatures to save \ n "); <nl> + return false ; <nl> + } <nl>  <nl> Sdb * db = sdb_new ( NULL , file , 0 ); <nl> if (! db ) {
static void bin_mach0_versioninfo ( RCore * r ) { <nl> static int bin_versioninfo ( RCore * r , int mode ) { <nl> const RBinInfo * info = r_bin_get_info ( r -> bin ); <nl>  <nl> + if (!( info && info -> rclass )) return false ; <nl> + <nl> if (! strncmp (" pe ", info -> rclass , 2 )) { <nl> bin_pe_versioninfo ( r ); <nl> } else if (! strncmp (" elf ", info -> rclass , 3 )) {
R_API int r_bp_size ( RBreakpoint * bp ) { <nl> int i , bpsize = 8 ; <nl> for ( i = 0 ; bp -> cur -> bps [ i ]. bytes ; i ++) { <nl> bpa = & bp -> cur -> bps [ i ]; <nl> - if ( bpa -> bits != bp -> bits ) { <nl> + if ( bpa -> bits && bpa -> bits != bp -> bits ) { <nl> continue ; <nl> } <nl> if ( bpa -> length < bpsize ) {
static int dex_loadcode ( RBinFile * arch , RBinDexObj * bin ) { <nl> for ( i = 0 ; i < bin -> header . method_size ; i ++) { <nl> // RBinDexMethod * method = & bin -> methods [ i ]; <nl> if (! methods [ i ]) { <nl> + if ( i >= bin -> header . class_size ) continue ; <nl> struct dex_class_t * c = & bin -> classes [ i ]; <nl> char * class_name = dex_class_name ( bin , c ); <nl> if ( class_name ) {
eprintf ("-- % s \ n ", buf ); <nl> } <nl> case ' C ': /* comment */ <nl> if ( input [ 1 ] == '+') { <nl> - const char * text , * newcomment = input + 2 ; <nl> + const char * newcomment = input + 2 ; <nl> + char * text ; <nl> while (* newcomment ==' ') newcomment ++; <nl> char * comment = r_meta_get_string ( <nl> core -> anal , R_META_TYPE_COMMENT , addr );
static int formatDisassembledOperand ( char * strOperand , int operandNum , const dis <nl> char binary [ 9 ]; <nl> int retVal ; <nl>  <nl> + if ( operandNum >= AVR_MAX_NUM_OPERANDS ) <nl> + return 0 ; <nl> + <nl> switch ( dInstruction . instruction -> operandTypes [ operandNum ]) { <nl> case OPERAND_NONE : <nl> case OPERAND_REGISTER_GHOST :
static Sdb * store_versioninfo_gnu_verneed ( struct Elf_ ( r_bin_elf_obj_t ) * bin , Elf <nl> i += entry -> vn_next ; <nl> snprintf ( key , sizeof ( key ), " version % d ", cnt ); <nl> sdb_ns_set ( sdb , key , sdb_version ); <nl> + // if entry -> vn_next is 0 it iterate infinitely <nl> + if (! entry -> vn_next ) break ; <nl> } <nl> free ( need ); <nl> return sdb ;
static void r_bin_file_free ( void /* RBinFile */ * bf_ ) { <nl> if ( a -> curxtr && a -> curxtr -> destroy ) <nl> a -> curxtr -> free_xtr (( void *) ( a -> xtr_obj )); <nl>  <nl> - r_bin_object_free ( a -> o ); <nl> + r_list_free ( a -> objs ); <nl> a -> o = NULL ; <nl> r_buf_free ( a -> buf ); <nl> // TODO : unset related sdb namespaces
riscv_dis ( RAsm * a , RAsmOp * rop , const ut8 * buf , ut64 len ) { <nl> memcpy (& insn , buf , 4 ); <nl> riscv_disassemble ( a , rop , insn , a -> bits ); <nl>  <nl> - return 4 ; <nl> + return riscv_insn_length ( insn ); <nl> }
mgt_run ( int dflag , const char * T_arg ) <nl>  <nl> setproctitle (" Varnish - Mgr % s ", heritage . name ); <nl>  <nl> + memset (& sac , 0 , sizeof sac ); <nl> sac . sa_handler = SIG_IGN ; <nl> sac . sa_flags = SA_RESTART ; <nl> 
dispatch_f ( struct VSL_data * vsl , struct VSL_transaction * const pt [], <nl> for ( t = pt [ 0 ]; t != NULL ; t = *++ pt ) { <nl> CTX . gen ++; <nl> if ( t -> type != VSL_t_req ) <nl> + /* Only look at client requests */ <nl> + continue ; <nl> + if ( t -> reason == VSL_r_esi ) <nl> + /* Skip ESI requests */ <nl> continue ; <nl> CTX . hitmiss = "-"; <nl> CTX . handling = "-";
SES_RefSrcAddr ( struct sess * sp ) <nl> c3 = c ; <nl> continue ; <nl> } <nl> - TAILQ_REMOVE ( ch , c2 , list ); <nl> - free ( c2 ); <nl> + TAILQ_REMOVE ( ch , c , list ); <nl> + free ( c ); <nl> VSL_stats -> n_srcaddr --; <nl> } <nl> if ( c3 == NULL ) {
do_list ( struct cli * cli , struct director * d , void * priv ) <nl> if ( d -> vdir -> admin_health == VDI_AH_DELETED ) <nl> return ( 0 ); <nl>  <nl> + // XXX admin health " probe " for the no - probe case is confusing <nl> VCLI_Out ( cli , "\ n %- 30s %- 7s ", d -> vdir -> cli_name , VDI_Ahealth ( d )); <nl>  <nl> if ( d -> vdir -> methods -> list != NULL )
read_tmo ( int fd , char * ptr , unsigned len , double tmo ) <nl> pfd . events = POLLIN ; <nl> for ( j = 0 ; len > 0 ; ) { <nl> i = poll (& pfd , 1 , to ); <nl> + if ( i < 0 ) { <nl> + errno = EINTR ; <nl> + return (- 1 ); <nl> + } <nl> if ( i == 0 ) { <nl> errno = ETIMEDOUT ; <nl> return (- 1 );
struct mg_context * mg_start ( mg_callback_t user_callback , const char ** options ) { <nl> ctx = calloc ( 1 , sizeof (* ctx )); <nl> ctx -> user_callback = user_callback ; <nl>  <nl> - while (( name = * options ++) != NULL ) { <nl> + while ( options && ( name = * options ++) != NULL ) { <nl> if (( i = get_option_index ( name )) == - 1 ) { <nl> cry ( fc ( ctx ), " Invalid option : % s ", name ); <nl> free_context ( ctx );
struct mg_request_info { <nl> int remote_port ; // Client ' s port <nl> int is_ssl ; // 1 if SSL - ed , 0 if not <nl> void * user_data ; // User data pointer passed to mg_start () <nl> + void * conn_data ; // Connection - specific user data <nl>  <nl> int num_headers ; // Number of HTTP headers <nl> struct mg_header {
struct uwsgi_stats * uwsgi_master_generate_stats () { <nl> uc = uc -> next ; <nl> } <nl>  <nl> + if ( uwsgi_stats_list_close ( us )) <nl> + goto end ; <nl> + <nl> if ( uwsgi_stats_comma ( us )) <nl> goto end ; <nl> }
static char * amqp_simple_get_frame ( int fd , struct amqp_frame_header * fh ) { <nl> while ( len < fh -> size + 1 ) { <nl> rlen = recv ( fd , ptr , ( fh -> size + 1 )- len , 0 ); <nl> if ( rlen <= 0 ) { <nl> - if ( rlen < 0 ) <nl> + if ( rlen < 0 ) { <nl> uwsgi_error (" recv ()"); <nl> + } <nl> + free ( frame ); <nl> return NULL ; <nl> } <nl> len += rlen ;
next : <nl> } <nl>  <nl> // this must be called only by the master !!! <nl> - if ( uwsgi . mywid > 0 ) return ; <nl> + if (! uwsgi . workers ) return ; <nl> + if ( uwsgi . workers [ 0 ]. pid != getpid ()) return ; <nl> uwsgi_legion_announce_death (); <nl> } <nl> 
static void uwsgi_alarm_thread_loop ( struct uwsgi_thread * ut ) { <nl> long ptr = 0 ; <nl> memcpy (& ptr , buf , sizeof ( long )); <nl> struct uwsgi_alarm_instance * uai = ( struct uwsgi_alarm_instance *) ptr ; <nl> - if (! uai ) return ; <nl> + if (! uai ) <nl> + break ; <nl> uwsgi_alarm_run ( uai , msg , msg_size ); <nl> } <nl> } <nl> } <nl> + free ( buf ); <nl> } <nl>  <nl> // initialize alarms , instances and log regexps
int http_parse ( struct http_session * h_session ) { <nl> hv = hv -> next ; <nl> } <nl>  <nl> + // security check <nl> + if ( c >= MAX_HTTP_VEC - 4 ) { <nl> + uwsgi_log (" too much headers in request . skipping it .\ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> return c ; <nl>  <nl> }
void reap_them_all ( int signum ) { <nl> } <nl>  <nl> for ( i = 0 ; i < uwsgi . mules_cnt ; i ++) { <nl> + if (! uwsgi . mules ) break ; <nl> if ( uwsgi . mules [ i ]. pid > 0 ) <nl> kill ( uwsgi . mules [ i ]. pid , SIGKILL ); <nl> }
char * uwsgi_cgi_get_docroot ( char * path_info , uint16_t path_info_len , int * need_f <nl> } <nl>  <nl> if ( choosen_udd -> status == 0 ) { <nl> - char * tmp_udd = realpath ( path , NULL ); <nl> - if (! tmp_udd ) { <nl> + char * tmp_udd = uwsgi_malloc ( PATH_MAX + 1 ); <nl> + if (! realpath ( path , tmp_udd )) { <nl> return NULL ; <nl> } <nl> 
int uwsgi_start ( void * v_argv ) { <nl> # ifndef __OpenBSD__ <nl>  <nl> if ( uwsgi . rl . rlim_max > 0 ) { <nl> + uwsgi . rl . rlim_cur = uwsgi . rl . rlim_max ; <nl> uwsgi_log (" limiting address space of processes ...\ n "); <nl> if ( setrlimit ( RLIMIT_AS , & uwsgi . rl )) { <nl> uwsgi_error (" setrlimit ()");
void uwsgi_python_harakiri ( int wid ) { <nl> char * address = uwsgi_concat2 ( up . tracebacker , uwsgi_num2str ( wid )); <nl>  <nl> int fd = uwsgi_connect ( address , - 1 , 0 ); <nl> - for (;;) { <nl> + while ( fd >= 0 ) { <nl> int ret = uwsgi_waitfd ( fd , uwsgi . shared -> options [ UWSGI_OPTION_SOCKET_TIMEOUT ]); <nl> if ( ret <= 0 ) { <nl> break ;
void uwsgi_opt_set_placeholder ( char * opt , char * value , void * none ) { <nl>  <nl> p [ 0 ] = 0 ; <nl> add_exported_option ( uwsgi_str ( value ), p + 1 , 1 ); <nl> - p [ 1 ] = '='; <nl> + p [ 0 ] = '='; <nl>  <nl> } <nl> 
static v8 :: Handle < v8 :: Value > uwsgi_v8_commonjs_require ( const v8 :: Arguments & <nl> free ( tmp_filename ); <nl> return ret ; <nl> } <nl> + free ( tmp_filename ); <nl> } <nl> - free ( tmp_filename ); <nl> usl = usl -> next ; <nl> } <nl> }
static void uwsgi_pypy_onload () { <nl> # ifdef UWSGI_PYPY_HOME <nl> upypy . home = UWSGI_PYPY_HOME ; <nl> # endif <nl> + uwsgi . has_threads = 1 ; <nl> } <nl>  <nl> static int uwsgi_pypy_mule ( char * opt ) {
static void mongrel2_connect () { <nl> } <nl> char * responder = strchr ( uwsgi_sock -> name , ','); <nl> if (! responder ) { <nl> - uwsgi_log (" invalid zeromq address \ n "); <nl> + uwsgi_log (" invalid zeromq address : % s \ n ", uwsgi_sock -> name ); <nl> exit ( 1 ); <nl> } <nl> uwsgi_sock -> receiver = uwsgi_concat2n ( uwsgi_sock -> name , responder - uwsgi_sock -> name , "", 0 );
error : <nl> if ( urcc -> key ) free ( urcc -> key ); <nl> if ( urcc -> name ) free ( urcc -> name ); <nl> if ( urcc -> expires_str ) free ( urcc -> expires_str ); <nl> + free ( urcc ); <nl> return - 1 ; <nl> } <nl> 
void uwsgi_python_reset_random_seed () { <nl> void uwsgi_python_atexit () { <nl>  <nl> // if hijacked do not run atexit hooks <nl> + if ( uwsgi . workers [ uwsgi . mywid ]. hijacked ) <nl> + return ; <nl>  <nl> // this time we use this higher level function <nl> // as this code can be executed in a signal handler
PyObject * py_uwsgi_gevent_graceful ( PyObject * self , PyObject * args ) { <nl>  <nl> void uwsgi_gevent_gbcw () { <nl>  <nl> - uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> - <nl> py_uwsgi_gevent_graceful ( NULL , NULL ); <nl> + <nl> + uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> + exit ( 0 ); <nl> } <nl>  <nl> struct wsgi_request * uwsgi_gevent_current_wsgi_req ( void ) {
void unit_attack ( <nl> int damage_left = damage ; <nl> while ( damage_left > 0 && ! animator . would_end ()) { <nl> int step_left = ( animator . get_end_time () - animator . get_animation_time () )/ 50 ; <nl> - int removed_hp = damage_left / step_left ; <nl> + int removed_hp = step_left ? damage_left / step_left : 1 ; <nl> if ( removed_hp < 1 ) removed_hp = 1 ; <nl> if ( step_left < 1 ) step_left = 1 ; <nl> defender . take_hit ( removed_hp );
SYNCED_COMMAND_HANDLER_FUNCTION ( attack , child , /* use_undo */, show , error_handler <nl> } <nl> } <nl>  <nl> - if ( size_t ( weapon_num ) >= u -> attacks (). size ()) { <nl> + if ( static_cast < unsigned >( weapon_num ) >= u -> attacks (). size ()) { <nl> error_handler (" illegal weapon type in attack \ n ", true ); <nl> return false ; <nl> }
public : <nl> size_t size () const { return lmap_ . size (); } <nl> size_t num_iters () const ; <nl>  <nl> + bool empty () const { return lmap_ . empty (); } <nl> + <nl> void clear ( bool force = false ); <nl>  <nl> /**
void get_player_info ( const config & cfg , game_state & gamestate , std :: string save_ <nl> LOG_NG << " found gold : '" << gold << "'\ n "; <nl>  <nl> int ngold = lexical_cast_default < int >( gold ); <nl> - if ( player != NULL && player -> gold >= ngold ) { <nl> + if ( ( player != NULL && player -> gold >= ngold ) || snapshot ) { <nl> ngold = player -> gold ; <nl> } <nl> 
void show_about ( display & disp ) <nl> text . push_back ("+ Developers "); <nl> text . push_back ("- Alfredo Beaumont ( ziberpunk )"); <nl> text . push_back ("- Cyril Bouthors ( CyrilB )"); <nl> - text . push_back ("- Guillaume Duwelz - Rebert "); <nl> text . push_back ("- Isaac Clerencia "); <nl> text . push_back ("- J . R . Blain ( Cowboy )"); <nl> text . push_back ("- Justin Zaun ( jzaun )");
game_launcher :: game_launcher ( const commandline_options & cmdline_opts , const char <nl> const std :: string app_basename = filesystem :: base_name ( appname ); <nl> jump_to_editor_ = app_basename . find (" editor ") != std :: string :: npos ; <nl>  <nl> + if ( cmdline_opts_ . core_id ) { <nl> + preferences :: set_core_id (* cmdline_opts_ . core_id ); <nl> + } <nl> if ( cmdline_opts_ . campaign ) { <nl> jump_to_campaign_ . jump_ = true ; <nl> jump_to_campaign_ . campaign_id_ = * cmdline_opts_ . campaign ;
void delete_all_wml_hotkeys () <nl> } <nl> } <nl>  <nl> -// retunrs weather a hotkey was deleted . <nl> +// Returns whether a hotkey was deleted . <nl> bool remove_wml_hotkey ( const std :: string & id ) <nl> { <nl> hotkey :: hotkey_command & command = get_hotkey_command ( id );
bool loadgame :: load_multiplayer_game () <nl> return false ; <nl> } <nl>  <nl> + if ( is_replay_save ( summary_ )) { <nl> + gui2 :: show_transient_message ( video_ , _ (" Load Game "), _ (" Replays are not supported in multiplayer mode .")); <nl> + return false ; <nl> + } <nl> + <nl> if ( gamestate_ . classification (). campaign_type != game_classification :: CAMPAIGN_TYPE :: MULTIPLAYER ) { <nl> gui2 :: show_transient_error_message ( video_ , _ (" This is not a multiplayer save .")); <nl> return false ;
void display :: clear_redraw_observers () <nl>  <nl> void display :: draw ( bool update , bool force ) { <nl> // log_scope (" display :: draw "); <nl> - if ( screen_ . update_locked ()) { <nl> + if ( screen_ . update_locked () || ( SDL_GetAppState () & SDL_APPACTIVE ) == 0 ) { <nl> return ; <nl> } <nl> bool changed = draw_init ();
SOCKET_STATE receive_buf ( TCPsocket sock , std :: vector < char >& buf ) <nl> } <nl> } <nl>  <nl> - const ssize_t res = SDLNet_TCP_Recv ( sock , beg , end - beg ); <nl> + const int res = SDLNet_TCP_Recv ( sock , beg , end - beg ); <nl> if ( res <= 0 ) { <nl> if ( SDLNet_CheckSockets ( set , 15000 ) <= 0 ) { <nl> ERR_NW << " SDLNet_CheckSockets : " << strerror ( errno ) << "\ n ";
namespace { <nl> net_manager_ ( min_thread , max_thread ), <nl> server_manager_ ( load_config ()), <nl> hooks_ (), <nl> - input_ ( 0 ) <nl> + input_ ( 0 ), <nl> + compress_level_ ( 0 ) <nl> { <nl> if ( cfg_ . child (" campaigns ") == NULL ) { <nl> cfg_ . add_child (" campaigns ");
namespace <nl> char * endptr ; <nl> int res = strtol ( index_str , & endptr , 10 ); <nl>  <nl> - if (* endptr != ']' || res > int ( game_config :: max_loop )) <nl> + if (* endptr != ']' || res > int ( game_config :: max_loop ) || endptr == index_str ) <nl> { <nl> throw invalid_variablename_exception (); <nl> }
void room_manager :: load_config ( const config & cfg ) <nl> { <nl> filename_ = cfg [" room_save_file "]; <nl> compress_stored_rooms_ = utils :: string_bool ( cfg [" compress_stored_rooms "], true ); <nl> - new_room_policy_ = pp_from_string ( cfg [" new_room_policy "]); <nl> + PRIVILEGE_POLICY pp = pp_from_string ( cfg [" new_room_policy "]); <nl> + if ( pp != PP_COUNT ) new_room_policy_ = pp ; <nl> } <nl>  <nl> const std :: string & room_manager :: storage_filename () const
void wait :: start_game () <nl>  <nl> LOG_NW << " starting game \ n "; <nl> sound :: play_UI_sound ( game_config :: sounds :: mp_game_begins ); <nl> + game_display :: get_singleton ()-> send_notification ( _ (" Wesnoth "), _ (" Game has begun !")); <nl> } <nl>  <nl> void wait :: layout_children ( const SDL_Rect & rect )
void widget :: set_visible ( const visibility visible ) <nl> visible_ = visible ; <nl>  <nl> if ( need_resize ) { <nl> - if ( new_widgets ) { <nl> + if ( visible == visibility :: visible && new_widgets ) { <nl> event :: message message ; <nl> fire ( event :: REQUEST_PLACEMENT , * this , message ); <nl> } else {
public : <nl> static void set_sunset ( const unsigned interval ) <nl> { sunset_ = interval ? interval : 5 ; } <nl>  <nl> + bool get_need_layout () const { return need_layout_ ; } <nl> + <nl> private : <nl>  <nl> /** Needed so we can change what ' s drawn on the screen . */
int battle_context :: choose_attacker_weapon ( const unit & attacker , <nl> attacker_combatant_ = new combatant (* attacker_stats_ ); <nl> defender_combatant_ = new combatant (* defender_stats_ , prev_def ); <nl> attacker_combatant_ -> fight (* defender_combatant_ ); <nl> + } else { <nl> + if ( attacker_stats_ -> disable ) { <nl> + delete attacker_stats_ ; <nl> + attacker_stats_ = nullptr ; <nl> + continue ; <nl> + } <nl> } <nl> if (! best_att_comb || better_combat (* attacker_combatant_ , * defender_combatant_ , <nl> * best_att_comb , * best_def_comb , harm_weight )) {
time_of_day :: time_of_day ( const config & cfg ) <nl> time_of_day :: time_of_day () <nl> : lawful_bonus ( 0 ) <nl> , bonus_modified ( 0 ) <nl> +, image () <nl> , name (" NULL_TOD ") <nl> , id (" nulltod ") <nl> +, image_mask () <nl> , red ( 0 ) <nl> , green ( 0 ) <nl> , blue ( 0 ) <nl> +, sounds () <nl> { <nl> } <nl> 
bool game :: describe_slots () { <nl> std :: string descr = buf . str (); <nl>  <nl> if ((* description_ )[" slots "] != descr ) { <nl> - description_ -> set_attr_dup (" slots ", descr ); <nl> + description_ -> set_attr_dup (" slots ", descr . c_str ()); <nl> return true ; <nl> } else { <nl> return false ;
tdata_t <nl> _TIFFCheckMalloc ( TIFF * tif , size_t nmemb , size_t elem_size , const char * what ) <nl> { <nl> - tdata_t * cp = NULL ; <nl> + tdata_t cp = NULL ; <nl> tsize_t bytes = nmemb * elem_size ; <nl>  <nl> /*
process ( register int code , unsigned char ** fill ) <nl> } <nl>  <nl> if ( oldcode == - 1 ) { <nl> + if ( code >= clear ) { <nl> + fprintf ( stderr , " bad input : code =% d is larger than clear =% d \ n ", code , clear ); <nl> + return 0 ; <nl> + } <nl> *(* fill )++ = suffix [ code ]; <nl> firstchar = oldcode = code ; <nl> return 1 ;
static int readContigStripsIntoBuffer ( TIFF * in , uint8 * buf ) <nl> ( unsigned long ) strip , ( unsigned long ) rows ); <nl> return 0 ; <nl> } <nl> - bufp += bytes_read ; <nl> + bufp += stripsize ; <nl> } <nl>  <nl> return 1 ;
EstimateStripByteCounts ( TIFF * tif , TIFFDirEntry * dir , uint16 dircount ) <nl> td -> td_stripbytecount = ( uint64 *) <nl> _TIFFCheckMalloc ( tif , td -> td_nstrips , sizeof ( uint64 ), <nl> " for \" StripByteCounts \" array "); <nl> + if ( td -> td_stripbytecount == NULL ) <nl> + return - 1 ; <nl> + <nl> if ( td -> td_compression != COMPRESSION_NONE ) { <nl> uint64 space ; <nl> uint64 filesize ;
tsize_t t2p_readwrite_pdf_image_tile ( T2P * t2p , TIFF * input , TIFF * output , ttile_ <nl> return ( 0 ); <nl> } <nl> if ( TIFFGetField ( input , TIFFTAG_JPEGTABLES , & count , & jpt ) != 0 ) { <nl> - if ( count >= 4 ) { <nl> + if ( count > 4 ) { <nl> int retTIFFReadRawTile ; <nl> /* Ignore EOI marker of JpegTables */ <nl> _TIFFmemcpy ( buffer , jpt , count - 2 );
static void print_usage ( const struct option longopts []) <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_version () <nl> + static void print_version ( void ) <nl> { <nl> printf ("% s \ n ", LXC_VERSION ); <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_help () <nl> + static void print_help ( void ) <nl> { <nl> fprintf ( stderr , "\ <nl> Usage : lxc - init -- name = NAME -- COMMAND \ n \
int lxc_abstract_unix_connect ( const char * path ) <nl>  <nl> if ( connect ( fd , ( struct sockaddr *)& addr , offsetof ( struct sockaddr_un , sun_path ) + len )) { <nl> int tmp = errno ; <nl> + /* special case to connect to older containers */ <nl> + if ( connect ( fd , ( struct sockaddr *)& addr , sizeof ( addr )) == 0 ) <nl> + return fd ; <nl> process_lock (); <nl> close ( fd ); <nl> process_unlock ();
static int keyboard_feed_evdev ( idev_keyboard * k , idev_data * data ) { <nl> /* TODO : update LEDs */ <nl> } <nl>  <nl> - if ( num < 0 ) <nl> + if ( num < 0 ) { <nl> + r = num ; <nl> goto error ; <nl> + } <nl>  <nl> r = keyboard_fill ( k , & k -> evdata , data -> resync , ev -> code , ev -> value , num , keysyms ); <nl> if ( r < 0 )
int dhcp6_option_parse_ip6addrs ( uint8_t * optval , uint16_t optlen , <nl>  <nl> int dhcp6_option_parse_domainname ( const uint8_t * optval , uint16_t optlen , char *** str_arr ) { <nl> size_t pos = 0 , idx = 0 ; <nl> - _cleanup_free_ char ** names = NULL ; <nl> + _cleanup_strv_free_ char ** names = NULL ; <nl> int r ; <nl>  <nl> assert_return ( optlen > 1 , - ENODATA );
int main ( int argc , char * argv [], char * envp []) <nl> udev_init_config (); <nl>  <nl> /* set signal handlers */ <nl> + memset (& act , 0x00 , sizeof ( act )); <nl> act . sa_handler = ( void (*) ( int )) sig_handler ; <nl> sigemptyset (& act . sa_mask ); <nl> act . sa_flags = 0 ;
static int load_group_database ( void ) { <nl> static int make_backup ( const char * target , const char * x ) { <nl> _cleanup_close_ int src = - 1 ; <nl> _cleanup_fclose_ FILE * dst = NULL ; <nl> - char * backup , * temp ; <nl> + _cleanup_free_ char * temp = NULL ; <nl> + char * backup ; <nl> struct timespec ts [ 2 ]; <nl> struct stat st ; <nl> int r ;
int dhcp_server_handle_message ( sd_dhcp_server * server , DHCPMessage * message , <nl> lease -> address = req -> requested_ip ; <nl> lease -> client_id . data = memdup ( req -> client_id . data , <nl> req -> client_id . length ); <nl> - if (! lease -> client_id . data ) <nl> + if (! lease -> client_id . data ) { <nl> + free ( lease ); <nl> return - ENOMEM ; <nl> + } <nl> lease -> client_id . length = req -> client_id . length ; <nl> } else <nl> lease = existing_lease ;
static int create_symlink ( const char * verb , const char * old_path , const char * ne <nl> return 1 ; <nl> } <nl>  <nl> + free ( dest ); <nl> return 0 ; <nl> } <nl> 
int unit_file_get_list ( <nl> } <nl> } <nl>  <nl> - return r ; <nl> + return 0 ; <nl> } <nl>  <nl> static const char * const unit_file_state_table [ _UNIT_FILE_STATE_MAX ] = {
static int unit_file_search ( <nl>  <nl> _cleanup_free_ char * template = NULL ; <nl> _cleanup_strv_free_ char ** dirs = NULL ; <nl> - _cleanup_free_ char ** files = NULL ; <nl> + _cleanup_strv_free_ char ** files = NULL ; <nl> const char * dropin_dir_name = NULL ; <nl> const char * dropin_template_dir_name = NULL ; <nl> 
_public_ int sd_pid_get_machine_name ( pid_t pid , char ** name ) { <nl>  <nl> _public_ int sd_pid_get_owner_uid ( pid_t pid , uid_t * uid ) { <nl> int r ; <nl> - _cleanup_free_ char * root = NULL , * cgroup = NULL , * p = NULL , * cc = NULL ; <nl> + _cleanup_free_ char * root = NULL , * cgroup = NULL , * cc = NULL ; <nl> + char * p ; <nl> struct stat st ; <nl>  <nl> if ( pid < 0 )
const SyscallFilterSet syscall_filter_sets [ _SYSCALL_FILTER_SET_MAX ] = { <nl> " reboot \ 0 " <nl> }, <nl> [ SYSCALL_FILTER_SET_RESOURCES ] = { <nl> - /* Alter resource settings */ <nl> . name = "@ resources ", <nl> + . help = " Alter resource settings ", <nl> . value = <nl> " sched_setparam \ 0 " <nl> " sched_setscheduler \ 0 "
static int get_key ( struct udev * udev , char ** line , char ** key , enum operation_ty <nl> char * temp ; <nl>  <nl> linepos = * line ; <nl> - if ( linepos == NULL && linepos [ 0 ] == '\ 0 ') <nl> + if ( linepos == NULL || linepos [ 0 ] == '\ 0 ') <nl> return - 1 ; <nl>  <nl> /* skip whitespace */
int main ( int argc , char * argv []) { <nl> return EXIT_FAILURE ; <nl> } <nl>  <nl> - if ( streq ( argv [ 1 ], " load ") && shall_restore_state ()) { <nl> + if ( streq ( argv [ 1 ], " load ")) { <nl> _cleanup_free_ char * value = NULL ; <nl>  <nl> + if (! shall_restore_state ()) <nl> + return EXIT_SUCCESS ; <nl> + <nl> r = read_one_line_file ( saved , & value ); <nl> if ( r < 0 ) { <nl> 
static int idev_evdev_io ( idev_evdev * evdev ) { <nl>  <nl> error : <nl> idev_evdev_hup ( evdev ); <nl> - return r ; <nl> + return 0 ; /* idev_evdev_hup () handles the error so discard it */ <nl> } <nl>  <nl> static int idev_evdev_event_fn ( sd_event_source * s , int fd , uint32_t revents , void * userdata ) {
static int service_dispatch_timer ( sd_event_source * source , usec_t usec , void * us <nl>  <nl> case SERVICE_RELOAD : <nl> log_unit_warning ( UNIT ( s ), " Reload operation timed out . Stopping ."); <nl> + service_unwatch_control_pid ( s ); <nl> + service_kill_control_processes ( s ); <nl> s -> reload_result = SERVICE_FAILURE_TIMEOUT ; <nl> service_enter_running ( s , SERVICE_SUCCESS ); <nl> break ;
static int parse_line ( const char * fname , unsigned line , const char * buffer , bool <nl> } <nl> } else { <nl> existing = new0 ( ItemArray , 1 ); <nl> + if (! existing ) <nl> + return log_oom (); <nl> + <nl> r = ordered_hashmap_put ( h , i . path , existing ); <nl> if ( r < 0 ) <nl> return log_oom ();
int bus_exec_context_set_transient_property ( <nl> } else { <nl> _cleanup_free_ char * joined = NULL ; <nl>  <nl> + r = strv_extend_strv (& c -> pass_environment , l , true ); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + <nl> /* We write just the new settings out to file , with unresolved specifiers . */ <nl> joined = unit_concat_strv ( l , UNIT_ESCAPE_SPECIFIERS ); <nl> if (! joined )
static uint32_t term_color_to_argb32 ( const term_color * color , const term_attr * a <nl> case TERM_CCODE_BLACK ... TERM_CCODE_LIGHT_WHITE : <nl> t = color -> ccode - TERM_CCODE_BLACK ; <nl>  <nl> - /* bold causes light colors */ <nl> - if ( t < 8 && attr -> bold ) <nl> + /* bold causes light colors ( only for foreground colors ) */ <nl> + if ( t < 8 && attr -> bold && color == & attr -> fg ) <nl> t += 8 ; <nl>  <nl> r = palette [ t * 3 + 0 ];
static int unit_find_paths ( <nl> _cleanup_free_ char * template = NULL ; <nl>  <nl> r = unit_name_template ( unit_name , & template ); <nl> - if ( r != - EINVAL ) <nl> + if ( r < 0 && r != - EINVAL ) <nl> return log_error_errno ( r , " Failed to determine template name : % m "); <nl> if ( r >= 0 ) { <nl> r = unit_file_find_path ( lp , template , & path );
static int str_compare ( const void * _a , const void * _b ) { <nl> } <nl>  <nl> char ** strv_sort ( char ** l ) { <nl> - <nl> - if ( strv_isempty ( l )) <nl> - return l ; <nl> - <nl> - qsort ( l , strv_length ( l ), sizeof ( char *), str_compare ); <nl> + qsort_safe ( l , strv_length ( l ), sizeof ( char *), str_compare ); <nl> return l ; <nl> } <nl> 
static int event_make_signal_data ( <nl> d -> priority = priority ; <nl>  <nl> r = hashmap_put ( e -> signal_data , & d -> priority , d ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( d ); <nl> return r ; <nl> + } <nl>  <nl> added = true ; <nl> }
static int search_and_fopen_internal ( const char * path , const char * mode , const c <nl> _cleanup_free_ char * p = NULL ; <nl> FILE * f ; <nl>  <nl> - p = strjoin (* i , "/", path , NULL ); <nl> + if ( root ) <nl> + p = strjoin ( root , * i , "/", path , NULL ); <nl> + else <nl> + p = strjoin (* i , "/", path , NULL ); <nl> if (! p ) <nl> return - ENOMEM ; <nl> 
int manager_handle_action ( <nl>  <nl> n = manager_count_displays ( m ); <nl> if ( n != 1 ) { <nl> - log_debug (" Ignoring lid switch request , % s displays connected ."); <nl> + log_debug (" Ignoring lid switch request , % i displays connected .", n ); <nl> return 0 ; <nl> } <nl> }
static void output_unit_file_list ( const UnitFileList * units , unsigned c ) { <nl> } else <nl> id_cols = max_id_len ; <nl>  <nl> - if (! arg_no_legend ) <nl> + if (! arg_no_legend && c > 0 ) <nl> printf ("%-* s %-* s \ n ", <nl> id_cols , " UNIT FILE ", <nl> state_cols , " STATE ");
int main ( int argc , char ** argv ) { <nl> } <nl> printf ("% s ... ", name ); <nl> fflush ( stdout ); <nl> - ( void ) LLVMFuzzerTestOneInput (( uint8_t *) buf , size ); <nl> + ( void ) LLVMFuzzerTestOneInput (( uint8_t *) buf , size ); <nl> printf (" ok \ n "); <nl> } <nl> return EXIT_SUCCESS ;
static int get_file_to_edit ( <nl> return log_oom (); <nl>  <nl> if ( arg_runtime ) { <nl> - run = strjoin ( paths -> runtime_config , name , NULL ); <nl> + run = strjoin ( paths -> runtime_config , "/", name , NULL ); <nl> if (! run ) <nl> return log_oom (); <nl> }
char * <nl> utf8_prev_char ( const char * p ) <nl> { <nl> - while ( 1 ) <nl> + for (;;) <nl> { <nl> p --; <nl> if ((* p & 0xc0 ) != 0x80 )
static int client_receive_message_udp ( <nl> if ( buflen < 0 ) <nl> return buflen ; <nl>  <nl> + if ( buflen == 0 ) <nl> + buflen = 1 ; <nl> + <nl> message = malloc0 ( buflen ); <nl> if (! message ) <nl> return - ENOMEM ;
fallback : <nl> return - errno ; <nl> } <nl>  <nl> + free ( parent ); <nl> + parent = NULL ; <nl> + <nl> r = path_get_parent ( t , & parent ); <nl> if ( r < 0 ) <nl> return r ;
int bus_message_print_all_properties ( <nl> return log_oom (); <nl>  <nl> r = set_put (* found_properties , name ); <nl> - if ( r < 0 && r != EEXIST ) <nl> + if ( r < 0 && r != - EEXIST ) <nl> return log_oom (); <nl> } <nl> 
static struct node * bus_node_allocate ( sd_bus * bus , const char * path ) { <nl> e = strrchr ( path , '/'); <nl> assert ( e ); <nl>  <nl> - p = strndupa ( path , MAX ( 1 , path - e )); <nl> + p = strndupa ( path , MAX ( 1 , e - path )); <nl>  <nl> parent = bus_node_allocate ( bus , p ); <nl> if (! parent )
int fstab_find_pri ( const char * options , int * ret ) { <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> + if (( int ) pri < 0 ) <nl> + return - ERANGE ; <nl> + <nl> * ret = ( int ) r ; <nl> return 1 ; <nl> }
static void mount_set_state ( Mount * m , MountState state ) { <nl> state == MOUNT_REMOUNTING_SIGKILL || <nl> state == MOUNT_UNMOUNTING_SIGTERM || <nl> state == MOUNT_UNMOUNTING_SIGKILL || <nl> - state == MOUNT_FAILED ) <nl> - mount_notify_automount ( m , - ENODEV ); <nl> + state == MOUNT_FAILED ) { <nl> + if ( state != old_state ) <nl> + mount_notify_automount ( m , - ENODEV ); <nl> + } <nl>  <nl> if ( state != old_state ) <nl> log_debug ("% s changed % s -> % s ",
int fopen_temporary ( const char * path , FILE ** _f , char ** _temp_path ) { <nl>  <nl> f = fdopen ( fd , " we "); <nl> if (! f ) { <nl> - unlink ( t ); <nl> + unlink_noerrno ( t ); <nl> free ( t ); <nl> safe_close ( fd ); <nl> return - errno ;
static int adm_settle ( struct udev * udev , int argc , char * argv []) { <nl> break ; <nl> } <nl>  <nl> - if ( timeout > 0 && now ( CLOCK_MONOTONIC ) >= deadline ) <nl> + if ( now ( CLOCK_MONOTONIC ) >= deadline ) <nl> break ; <nl>  <nl> /* wake up when queue is empty */
int cg_is_empty_recursive ( const char * controller , const char * path ) { <nl>  <nl> assert ( path ); <nl>  <nl> + /* The root cgroup is always populated */ <nl> + if ( controller && ( isempty ( path ) || path_equal ( path , "/"))) <nl> + return 0 ; <nl> + <nl> r = cg_is_empty ( controller , path ); <nl> if ( r <= 0 ) <nl> return r ;
_public_ int sd_bus_try_close ( sd_bus * bus ) { <nl> assert_return (! bus_pid_changed ( bus ), - ECHILD ); <nl> assert_return ( bus -> is_kernel , - ENOTSUP ); <nl>  <nl> + if ( bus -> rqueue_size > 0 ) <nl> + return - EBUSY ; <nl> + <nl> r = bus_kernel_try_close ( bus ); <nl> if ( r < 0 ) <nl> return r ;
static DBusHandlerResult locale_message_handler ( <nl> " Locale \ 0 "); <nl> if (! changed ) <nl> goto oom ; <nl> - } <nl> + } else <nl> + strv_free ( l ); <nl> + <nl> } else if ( dbus_message_is_method_call ( message , " org . freedesktop . locale1 ", " SetVConsoleKeyboard ")) { <nl>  <nl> const char * keymap , * keymap_toggle ;
static void output_draw ( Output * o , bool menu , term_screen * screen ) { <nl> */ <nl>  <nl> static void terminal_dirty ( Terminal * t ) { <nl> - uint64_t usec ; <nl> + usec_t usec ; <nl> int r ; <nl>  <nl> assert ( t );
int base_filesystem_create ( const char * root ) { <nl> const char * target = NULL ; <nl> const char * s ; <nl>  <nl> + if ( faccessat ( fd , table [ i ]. dir , F_OK , AT_SYMLINK_NOFOLLOW ) >= 0 ) <nl> + continue ; <nl> + <nl> /* check if one of the targets exists */ <nl> NULSTR_FOREACH ( s , table [ i ]. target ) { <nl> if ( faccessat ( fd , s , F_OK , AT_SYMLINK_NOFOLLOW ) < 0 )
static int map_netif ( sd_bus * bus , const char * member , sd_bus_message * m , sd_bus_ <nl> r = sd_bus_message_read_array ( m , SD_BUS_TYPE_INT32 , & v , & l ); <nl> if ( r < 0 ) <nl> return r ; <nl> + if ( r == 0 ) <nl> + return - EBADMSG ; <nl>  <nl> i -> n_netif = l / sizeof ( int32_t ); <nl> i -> netif = memdup ( v , l );
int switch_root ( const char * new_root ) { <nl> snprintf ( new_mount , sizeof ( new_mount ), "% s % s ", new_root , i ); <nl> char_array_0 ( new_mount ); <nl>  <nl> - mkdir_parents ( new_mount , 0755 ); <nl> + mkdir_p ( new_mount , 0755 ); <nl>  <nl> if (( stat ( new_mount , & sb ) < 0 ) || <nl> sb . st_dev != new_root_stat . st_dev ) {
bool path_is_safe ( const char * p ) { <nl> if ( streq ( p , "..") || startswith ( p , "../") || endswith ( p , "/..") || strstr ( p , "/../")) <nl> return false ; <nl>  <nl> - if ( strlen ( p ) > PATH_MAX ) <nl> + if ( strlen ( p )+ 1 > PATH_MAX ) <nl> return false ; <nl>  <nl> /* The following two checks are not really dangerous , but hey , they still are confusing */
static int output_cat ( sd_journal * j , OutputMode mode , unsigned line , <nl>  <nl> r = sd_journal_get_data ( j , " MESSAGE ", & data , & l ); <nl> if ( r < 0 ) { <nl> + /* An entry without MESSAGE =? */ <nl> + if ( r == - ENOENT ) <nl> + return 0 ; <nl> + <nl> log_error (" Failed to get data : % s ", strerror (- r )); <nl> return r ; <nl> }
static int json_parse_tokens ( JsonVariant ** tokens , size_t ntokens , JsonVariant * <nl> size_t it = 0 ; <nl> int r ; <nl> JsonVariant * e ; <nl> - _cleanup_jsonunref_ JsonVariant * p ; <nl> + _cleanup_jsonunref_ JsonVariant * p = NULL ; <nl>  <nl> assert ( tokens ); <nl> assert ( ntokens );
int server_flush_to_var ( Server * s , bool require_flag_file ) { <nl> r = 0 ; <nl>  <nl> finish : <nl> - journal_file_post_change ( s -> system_journal ); <nl> + if ( s -> system_journal ) <nl> + journal_file_post_change ( s -> system_journal ); <nl>  <nl> s -> runtime_journal = journal_file_close ( s -> runtime_journal ); <nl> 
void initialize_srand ( void ) { <nl>  <nl> auxv = ( void *) getauxval ( AT_RANDOM ); <nl> if ( auxv ) { <nl> - assert_cc ( sizeof ( x ) < 16 ); <nl> + assert_cc ( sizeof ( x ) <= 16 ); <nl> memcpy (& x , auxv , sizeof ( x )); <nl> } else <nl> # endif
static void manager_clear_jobs_and_units ( Manager * m ) { <nl>  <nl> m -> n_on_console = 0 ; <nl> m -> n_running_jobs = 0 ; <nl> + m -> n_installed_jobs = 0 ; <nl> + m -> n_failed_jobs = 0 ; <nl> } <nl>  <nl> Manager * manager_free ( Manager * m ) {
int button_open ( Button * b ) { <nl> } <nl>  <nl> ( void ) button_set_mask ( b ); <nl> - <nl> + <nl> + b -> io_event_source = sd_event_source_unref ( b -> io_event_source ); <nl> r = sd_event_add_io ( b -> manager -> event , & b -> io_event_source , b -> fd , EPOLLIN , button_dispatch , b ); <nl> if ( r < 0 ) { <nl> log_error_errno ( r , " Failed to add button event : % m ");
int dns_packet_is_reply_for ( DnsPacket * p , const DnsResourceKey * key ) { <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> + if (! p -> question ) <nl> + return 0 ; <nl> + <nl> if ( p -> question -> n_keys != 1 ) <nl> return 0 ; <nl> 
static int parse_date ( const char ** p , CalendarSpec * c ) { <nl> c -> month = first ; <nl> c -> day = second ; <nl> return 0 ; <nl> - } else if ( c -> end_of_month ) <nl> + } else if ( c -> end_of_month ) { <nl> + free_chain ( first ); <nl> + free_chain ( second ); <nl> return - EINVAL ; <nl> + } <nl>  <nl> if (* t == '~') <nl> c -> end_of_month = true ;
static int client_initialize_time_events ( sd_dhcp_client * client ) { <nl>  <nl> r = sd_event_source_set_priority ( client -> timeout_resend , <nl> client -> event_priority ); <nl> + if ( r < 0 ) <nl> + goto error ; <nl>  <nl> r = sd_event_source_set_description ( client -> timeout_resend , " dhcp4 - resend - timer "); <nl> if ( r < 0 )
int udev_monitor_filter_update ( struct udev_monitor * udev_monitor ) <nl> bpf_stmt ( ins , & i , BPF_RET | BPF_K , 0xffffffff ); <nl>  <nl> /* install filter */ <nl> + memset (& filter , 0x00 , sizeof ( filter )); <nl> filter . len = i ; <nl> filter . filter = ins ; <nl> err = setsockopt ( udev_monitor -> sock , SOL_SOCKET , SO_ATTACH_FILTER , & filter , sizeof ( filter ));
static int handle_response ( sd_resolve * resolve , const Packet * packet , size_t len <nl>  <nl> if ( ni_resp -> hostlen > DNS_HOSTNAME_MAX || <nl> ni_resp -> servlen > DNS_HOSTNAME_MAX || <nl> - sizeof ( NameInfoResponse ) + ni_resp -> hostlen + ni_resp -> servlen > length + 2 ) <nl> + sizeof ( NameInfoResponse ) + ni_resp -> hostlen + ni_resp -> servlen > length ) <nl> ASSIGN_ERRNO ( q , EAI_SYSTEM , EIO , 0 ); <nl>  <nl> else {
int bus_event_loop_with_idle ( <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> - if ( r == 0 && ! exiting ) { <nl> + if ( r == 0 && ! exiting && idle ) { <nl>  <nl> r = sd_bus_try_close ( bus ); <nl> if ( r == - EBUSY )
static int set_usb_mass_storage_ifsubtype ( char * to , const char * from , size_t len <nl> type = " floppy "; <nl> break ; <nl> case 1 : /* RBC devices */ <nl> + type = " rbc "; <nl> + break ; <nl> case 6 : /* Transparent SPC - 2 devices */ <nl> - type = " disk "; <nl> + type = " scsi "; <nl> break ; <nl> default : <nl> break ;
void _logsys_log_printf ( <nl> subsysid = LOGSYS_MAX_SUBSYS_COUNT ; <nl> } <nl>  <nl> + if (( level > logsys_loggers [ subsysid ]. syslog_priority ) && <nl> + ( level > logsys_loggers [ subsysid ]. logfile_priority )) { <nl> + return ; <nl> + } <nl> + <nl> va_start ( ap , format ); <nl> len = vsprintf ( logsys_print_buffer , format , ap ); <nl> va_end ( ap );
void main_deliver_fn ( <nl> log_printf ( instance -> totemsrp_log_level_security , " Received message is too short ... ignoring % d .\ n ", msg_len ); <nl> return ; <nl> } <nl> - <nl> + <nl> + if (( int ) message_header -> type >= totemsrp_message_handlers . count ) { <nl> + log_printf ( instance -> totemsrp_log_level_security , " Type of received message is wrong ... ignoring % d .\ n ", ( int ) message_header -> type ); <nl> + return ; <nl> + } <nl> + <nl> /* <nl> * Handle incoming message <nl> */
saEvtEventAttributesSet ( <nl> struct event_data_instance * edi ; <nl> int i ; <nl>  <nl> + if ( priority < SA_EVT_HIGHEST_PRIORITY || <nl> + priority > SA_EVT_LOWEST_PRIORITY ) { <nl> + return SA_AIS_ERR_INVALID_PARAM ; <nl> + } <nl> + <nl> error = saHandleInstanceGet (& event_handle_db , eventHandle , <nl> ( void *)& edi ); <nl> if ( error != SA_AIS_OK ) {
poll_handle poll_create ( <nl> poll_instance -> poll_entries = 0 ; <nl> poll_instance -> ufds = 0 ; <nl> poll_instance -> poll_entry_count = 0 ; <nl> - poll_instance -> serialize_lock_fn = serialize_unlock_fn ; <nl> + poll_instance -> serialize_lock_fn = serialize_lock_fn ; <nl> poll_instance -> serialize_unlock_fn = serialize_unlock_fn ; <nl> timerlist_init (& poll_instance -> timerlist ); <nl> 
static void unlink_all_completed ( void ) <nl> * here <nl> */ <nl> serialize_unlock (); <nl> + api -> timer_delete ( corosync_stats_timer_handle ); <nl> poll_stop ( corosync_poll_handle ); <nl> totempg_finalize (); <nl> 
int amfReadNetwork ( char ** error_string , <nl> int res = 0 ; <nl> int line_number = 0 ; <nl>  <nl> + memset ( mcast_addr , 0 , sizeof ( struct sockaddr_in )); <nl> + memset ( bindnet_addr , 0 , sizeof ( struct sockaddr_in )); <nl>  <nl> mcast_addr -> sin_family = AF_INET ; <nl> fp = fopen ("/ etc / ais / network . conf ", " r ");
static void memb_state_gather_enter ( <nl>  <nl> instance -> memb_state = MEMB_STATE_GATHER ; <nl> instance -> stats . gather_entered ++; <nl> - instance -> stats . continuous_gather ++; <nl> + <nl> + if ( gather_from == 3 ) { <nl> + /* <nl> + * State 3 means gather , so we are continuously gathering . <nl> + */ <nl> + instance -> stats . continuous_gather ++; <nl> + } <nl>  <nl> if ( instance -> stats . continuous_gather > MAX_NO_CONT_GATHER ) { <nl> log_printf ( instance -> totemsrp_log_level_warning ,
ev_document_misc_get_screen_dpi ( GdkScreen * screen ) <nl>  <nl> /* diagonal in pixels */ <nl> dp = hypot ( gdk_screen_get_width ( screen ), gdk_screen_get_height ( screen )); <nl> + if ( dp == 0 ) <nl> + return 96 ; <nl>  <nl> /* diagonal in inches */ <nl> di = hypot ( gdk_screen_get_width_mm ( screen ), gdk_screen_get_height_mm ( screen )) / 25 . 4 ; <nl> + if ( di == 0 ) <nl> + return 96 ; <nl>  <nl> return ( dp / di ); <nl> }
DllMain ( HINSTANCE hinstDLL , <nl> static const gchar * <nl> _ev_win32_get_locale_dir ( HMODULE module ) <nl> { <nl> + if ( locale_dir ) <nl> + return locale_dir ; <nl> + <nl> gchar * install_dir = NULL , * utf8_locale_dir ; <nl> gchar * retval = NULL ; <nl> 
ev_view_accessible_focus_changed ( GtkWidget * widget , <nl> g_return_val_if_fail ( EV_IS_VIEW ( widget ), FALSE ); <nl> g_return_val_if_fail ( EV_IS_VIEW_ACCESSIBLE ( self ), FALSE ); <nl>  <nl> - if ( self -> priv -> children == NULL ) <nl> + if ( self -> priv -> children == NULL || self -> priv -> children -> len == 0 ) <nl> return FALSE ; <nl>  <nl> page_accessible = g_ptr_array_index ( self -> priv -> children ,
ev_view_button_release_event ( GtkWidget * widget , <nl> view -> pressed_button = - 1 ; <nl>  <nl> return TRUE ; <nl> - } <nl> + } <nl> + <nl> + if ( view -> pressed_button == 1 && event -> state & GDK_CONTROL_MASK ) { <nl> + view -> pressed_button = - 1 ; <nl> + return TRUE ; <nl> + } <nl>  <nl> if ( view -> drag_info . in_drag ) { <nl> view -> drag_info . release_timeout_id =
ev_view_select_all ( EvView * view ) <nl> } <nl>  <nl> merge_selection_region ( view , g_list_reverse ( selections )); <nl> - gtk_widget_queue_draw ( GTK_WIDGET ( view )); <nl> } <nl>  <nl> gboolean
end_element_handler ( GMarkupParseContext * markup_context , <nl> case STATE_INSIDE_ALT : <nl> case STATE_INSIDE_BAG : <nl> case STATE_INSIDE_SEQ : <nl> + if ( context -> property && context -> prop_cur_value < 0 ) <nl> + { <nl> + g_free ( context -> property ); <nl> + context -> property = NULL ; <nl> + } <nl> context -> state = STATE_INSIDE_PROPERTY ; <nl> break ; <nl> 
gimp_prop_widget_new_from_pspec ( GObject * config , <nl>  <nl> buffer = gimp_prop_text_buffer_new ( config , pspec -> name , - 1 ); <nl> view = gtk_text_view_new_with_buffer ( buffer ); <nl> + g_object_unref ( buffer ); <nl>  <nl> widget = gtk_scrolled_window_new ( NULL , NULL ); <nl> gtk_scrolled_window_set_shadow_type ( GTK_SCROLLED_WINDOW ( widget ),
gimp_config_path_expand_only ( const gchar * path , <nl> s = gimp_plug_in_directory (); <nl> else if ( strcmp ( token , " gimp_sysconf_dir ") == 0 ) <nl> s = gimp_sysconf_directory (); <nl> + else if ( strcmp ( token , " gimp_installation_dir ") == 0 ) <nl> + s = gimp_installation_directory (); <nl>  <nl> if (! s ) <nl> s = g_getenv ( token );
choose_format ( GeglBuffer * buffer , <nl> break ; <nl>  <nl> case GIMP_SELECT_CRITERION_LCH_L : <nl> + format = babl_format (" CIE L alpha float "); <nl> + break ; <nl> + <nl> case GIMP_SELECT_CRITERION_LCH_C : <nl> case GIMP_SELECT_CRITERION_LCH_H : <nl> format = babl_format (" CIE LCH ( ab ) alpha float ");
wilber_get_extents ( cairo_t * cr ) <nl> cairo_fill_extents ( cr , & wilber_x1 , & wilber_y1 , & wilber_x2 , & wilber_y2 ); <nl>  <nl> wilber_cairo_path = cairo_copy_path ( cr ); <nl> + cairo_new_path ( cr ); <nl>  <nl> cairo_restore ( cr ); <nl> }
d_load_object ( gchar * desc , <nl> if ( sscanf ( buf , "% d ", & new_obj -> type_data ) != 1 ) <nl> { <nl> g_message (" Error while loading object ( no type data )"); <nl> + g_free ( new_obj ); <nl> return NULL ; <nl> } <nl>  <nl> d_load_object ( gchar * desc , <nl> if ( strcmp ("</ EXTRA >", buf )) <nl> { <nl> g_message (" Syntax error while loading object "); <nl> + g_free ( new_obj ); <nl> return NULL ; <nl> } <nl> /* Go around and read the last line */
gimp_levels_tool_config_notify ( GimpFilterTool * filter_tool , <nl> GIMP_FILTER_TOOL_CLASS ( parent_class )-> config_notify ( filter_tool , <nl> config , pspec ); <nl>  <nl> - if (! levels_tool -> channel_menu ) <nl> + if (! levels_tool -> channel_menu || <nl> + ! levels_tool -> histogram_view ) <nl> return ; <nl>  <nl> if (! strcmp ( pspec -> name , " linear "))
save_layer ( const gchar * filename , <nl> gimp_filename_to_utf8 ( filename )); <nl>  <nl> /* Attempt to open the output file */ <nl> - if (( outfile = g_fopen ( filename , " wb +")) == NULL ) <nl> + if (( outfile = g_fopen ( filename , " w + b ")) == NULL ) <nl> { <nl> g_set_error ( error , G_FILE_ERROR , <nl> g_file_error_from_errno ( errno ),
xcf_init ( Gimp * gimp ) <nl> " Filename ", <nl> " The name of the file " <nl> " to save the image in , " <nl> - " in the on - disk " <nl> - " character set and " <nl> - " encoding ", <nl> + " in URI format and " <nl> + " UTF - 8 encoding ", <nl> TRUE , FALSE , TRUE , <nl> NULL , <nl> GIMP_PARAM_READWRITE ));
gimp_container_tree_view_constructor ( GType type , <nl> tree_view -> main_column = gtk_tree_view_column_new (); <nl> gtk_tree_view_insert_column ( tree_view -> view , tree_view -> main_column , 0 ); <nl>  <nl> + gtk_tree_view_set_expander_column ( tree_view -> view , tree_view -> main_column ); <nl> + <nl> tree_view -> renderer_cell = gimp_cell_renderer_viewable_new (); <nl> gtk_tree_view_column_pack_start ( tree_view -> main_column , <nl> tree_view -> renderer_cell ,
gimp_tile_backend_plugin_command ( GeglTileSource * tile_store , <nl> break ; <nl>  <nl> default : <nl> - g_assert ( command < GEGL_TILE_LAST_COMMAND && command >= 0 ); <nl> + /* g_assert ( command < GEGL_TILE_LAST_COMMAND && command >= 0 ); */ <nl> + break ; <nl> } <nl>  <nl> return result ;
p_vertical_bend ( BenderDialog * cd , <nl> } <nl> } <nl> } <nl> + <nl> + g_free ( last_arr ); <nl> + g_free ( first_arr ); <nl> } <nl>  <nl> /* ============================================================================
metadata_message_dialog ( GtkMessageType type , <nl> { <nl> GtkWidget * dlg ; <nl>  <nl> - dlg = gtk_message_dialog_new ( parent , 0 , type , GTK_BUTTONS_OK , message ); <nl> + dlg = gtk_message_dialog_new ( parent , 0 , type , GTK_BUTTONS_OK , "% s ", message ); <nl>  <nl> if ( title ) <nl> gtk_window_set_title ( GTK_WINDOW ( dlg ), title );
gimp_histogram_view_notify ( GimpHistogram * histogram , <nl> static void <nl> gimp_histogram_view_update_bins ( GimpHistogramView * view ) <nl> { <nl> - gint new_bins ; <nl> + gint new_bins = 256 ; <nl>  <nl> if ( view -> histogram ) <nl> new_bins = gimp_histogram_n_bins ( view -> histogram );
gui_unique_win32_idle_open ( IdleOpenData * data ) <nl> if ( data -> file ) <nl> { <nl> file_open_from_command_line ( unique_gimp , data -> file , <nl> - data -> as_new , NULL , 0 ); <nl> + data -> as_new , NULL ); <nl> } <nl> else <nl> { <nl> gui_unique_quartz_idle_open ( GFile * file ) <nl>  <nl> if ( file ) <nl> { <nl> - file_open_from_command_line ( unique_gimp , file , FALSE , NULL , 0 ); <nl> + file_open_from_command_line ( unique_gimp , file , FALSE , NULL ); <nl> } <nl>  <nl> return FALSE ;
gimp_drawable_sync_fs_filter ( GimpDrawable * drawable , <nl> GeglNode * fs_source ; <nl>  <nl> private -> fs_filter = gimp_filter_new (" Floating Selection "); <nl> + gimp_viewable_set_stock_id ( GIMP_VIEWABLE ( private -> fs_filter ), <nl> + " gimp - floating - selection "); <nl>  <nl> node = gimp_filter_get_node ( private -> fs_filter ); <nl> 
save_dialog ( void ) <nl> g_signal_connect ( toggle , " toggled ", <nl> G_CALLBACK ( gimp_toggle_button_update ), <nl> & jsvals . save_xmp ); <nl> + g_signal_connect ( toggle , " toggled ", <nl> + G_CALLBACK ( make_preview ), <nl> + NULL ); <nl>  <nl> gtk_toggle_button_set_active ( GTK_TOGGLE_BUTTON ( toggle ), <nl> jsvals . save_xmp && has_metadata );
gimp_prop_kelvin_presets_new ( GObject * config , <nl> menu = gtk_menu_new (); <nl> gtk_menu_attach_to_widget ( GTK_MENU ( menu ), button , NULL ); <nl>  <nl> + gimp_help_set_help_data ( button , <nl> + _ (" Choose from a list of common " <nl> + " color temperatures "), NULL ); <nl> + <nl> g_signal_connect ( button , " button - press - event ", <nl> G_CALLBACK ( gimp_prop_kelvin_presets_button_press ), <nl> menu );
load_image ( const gchar * filename , <nl> gint32 volatile image_ID = - 1 ; <nl> gint32 layer_ID ; <nl> int fd ; /* File descriptor */ <nl> - char buf [ BUFLEN ]; /* buffer for random things like scanning */ <nl> + char buf [ BUFLEN + 4 ]; /* buffer for random things like scanning */ <nl> PNMInfo * pnminfo ; <nl> PNMScanner * volatile scan ; <nl> int ctr ;
gimp_image_map_tool_initialize ( GimpTool * tool , <nl>  <nl> window = gimp_display_shell_get_window ( display_shell ); <nl>  <nl> - image_map_tool -> overlay = gimp_image_window_get_fullscreen ( window ); <nl> + /* disabled for at least GIMP 2 . 8 */ <nl> + image_map_tool -> overlay = FALSE ; <nl>  <nl> if ( image_map_tool -> overlay ) <nl> {
gimp_transform_tool_commit ( GimpTransformTool * tr_tool ) <nl> if ( tr_tool -> gui ) <nl> gimp_tool_gui_hide ( tr_tool -> gui ); <nl>  <nl> - if ( gimp_matrix3_is_identity (& tr_tool -> transform )) <nl> + if ( GIMP_TRANSFORM_TOOL_GET_CLASS ( tr_tool )-> recalc_matrix && <nl> + gimp_matrix3_is_identity (& tr_tool -> transform )) <nl> { <nl> /* No need to commit an identity transformation ! */ <nl> return ;
struct hb_ot_face_metrics_accelerator_t <nl>  <nl> this -> blob = OT :: Sanitizer < OT :: _mtx >:: sanitize ( face -> reference_table ( _mtx_tag )); <nl> if ( unlikely (! this -> num_advances || <nl> - 2 * ( this -> num_advances + this -> num_metrics ) < hb_blob_get_length ( this -> blob ))) <nl> + 2 * ( this -> num_advances + this -> num_metrics ) > hb_blob_get_length ( this -> blob ))) <nl> { <nl> this -> num_metrics = this -> num_advances = 0 ; <nl> hb_blob_destroy ( this -> blob );
static HB_Error Load_Mark2Array ( HB_Mark2Array * m2a , <nl>  <nl> FORGET_Frame (); <nl>  <nl> + if ( new_offset == base_offset ) { <nl> + /* Anchor table not provided . Skip loading . <nl> + * Some versions of FreeSans hit this . */ <nl> + m2an [ n ]. PosFormat = 0 ; <nl> + continue ; <nl> + } <nl> + <nl> cur_offset = FILE_Pos (); <nl> if ( FILE_Seek ( new_offset ) || <nl> ( error = Load_Anchor ( & m2an [ n ], stream ) ) != HB_Err_Ok )
void <nl> _hb_ot_shape_complex_override_features_indic ( hb_ot_map_builder_t * map , <nl> const hb_segment_properties_t * props HB_UNUSED ) <nl> { <nl> + /* Uniscribe does not apply ' kern '. */ <nl> + if ( indic_options (). uniscribe_bug_compatible ) <nl> + map -> add_feature ( HB_TAG (' k ',' e ',' r ',' n '), 0 , true ); <nl> } <nl>  <nl> 
my_bool <nl> my_net_write ( NET * net , const char * packet , ulong len ) <nl> { <nl> uchar buff [ NET_HEADER_SIZE ]; <nl> - if ( unlikely (! net -> vio )) // nowhere to write <nl> + if ( unlikely (! net -> vio )) /* nowhere to write */ <nl> return 0 ; <nl> /* <nl> Big packets are handled by splitting them in packets of MAX_PACKET_LENGTH
int merge_many_buff ( Sort_param * param , uchar * sort_buffer , <nl> ulong read_to_buffer ( IO_CACHE * fromfile , BUFFPEK * buffpek , <nl> uint rec_length ) <nl> { <nl> - register ulong count ; <nl> + ulong count ; <nl> ulong length = 0 ; <nl>  <nl> if (( count = ( ulong ) MY_MIN (( ha_rows ) buffpek -> max_keys , buffpek -> count )))
static int connect_assisted_discovery ( handlerton *, THD * thd , <nl> break ; <nl> # if defined ( MONGO_SUPPORT ) <nl> case TAB_MONGO : <nl> + if (! topt -> tabname ) <nl> + topt -> tabname = tab ; <nl> + <nl> ok = true ; <nl> break ; <nl> # endif // MONGO_SUPPORT
btr_search_guess_on_hash ( <nl> } <nl>  <nl> block = buf_block_align ( rec ); <nl> - page = buf_block_get_frame ( block ); <nl> + page = page_align ( rec ); <nl>  <nl> if ( UNIV_LIKELY (! has_search_latch )) { <nl> 
static int run_query ( const char * query , DYNAMIC_STRING * ds_res , <nl> NULL ); <nl>  <nl> my_close ( fd , MYF ( 0 )); <nl> + my_delete ( query_file_path , MYF ( 0 )); <nl>  <nl> DBUG_RETURN ( ret ); <nl> }
int ha_create_table_from_engine ( THD * thd , <nl> } <nl>  <nl> err_end : <nl> - my_free (( char *) frmblob , MYF ( MY_ALLOW_ZERO )); <nl> + my_free (( char *) frmblob , MYF ( MY_ALLOW_ZERO_PTR )); <nl> DBUG_RETURN ( error ); <nl> } <nl> 
int serialize_brtnode_to ( int fd , diskoff off , diskoff size , BRTNODE node ) { <nl>  <nl> // printf ("% s :% d wrote % d bytes for % lld size =% lld \ n ", __FILE__ , __LINE__ , w . ndone , off , size ); <nl> assert ( w . ndone <= size ); <nl> - toku_free ( w . buf ); <nl> return 0 ; <nl> } <nl> 
ha_innobase :: add_index ( <nl>  <nl> func_exit : <nl> mem_heap_free ( heap ); <nl> + innobase_commit_low ( trx );/* work around a bug in mysql_alter_table () */ <nl>  <nl> /* There might be work for utility threads .*/ <nl> srv_active_wake_master_thread ();
btr_cur_optimistic_insert ( <nl> buf_block_get_page_no ( block ), max_size , <nl> rec_size + PAGE_DIR_SLOT_SIZE , index -> type ); <nl> # endif <nl> - if ( leaf <nl> - && ! dict_index_is_clust ( index ) <nl> - && ! dict_index_is_ibuf ( index )) { <nl> + if ( leaf && ! dict_index_is_clust ( index )) { <nl> /* Update the free bits of the B - tree page in the <nl> insert buffer bitmap . */ <nl> 
mysql_init ( MYSQL * mysql ) <nl> bzero (( char *) ( mysql ), sizeof (*( mysql ))); <nl> mysql -> options . connect_timeout = CONNECT_TIMEOUT ; <nl> mysql -> last_used_con = mysql -> next_slave = mysql -> master = mysql ; <nl> + mysql -> charset = default_charset_info ; <nl> strmov ( mysql -> net . sqlstate , not_error_sqlstate ); <nl> /* <nl> By default , we are a replication pivot . The caller must reset it
buf_page_init_low ( <nl> /*==============*/ <nl> buf_page_t * bpage ) /* in : block to init */ <nl> { <nl> + bpage -> flush_type = BUF_FLUSH_LRU ; <nl> bpage -> accessed = FALSE ; <nl> bpage -> io_fix = BUF_IO_NONE ; <nl> bpage -> buf_fix_count = 0 ;
pstack_install_segv_action ( const char * path_format_ ) <nl> if (( abfd = load_bfd ( pid ))== 0 ) <nl> fprintf ( stderr , " BFD load failed ..\ n "); <nl> else { <nl> - long storage_needed = bfd_get_symtab_upper_bound ( abfd ); <nl> + long storage_needed = ( bfd_get_file_flags ( abfd ) & HAS_SYMS ) ? <nl> + bfd_get_symtab_upper_bound ( abfd ) : 0 ; <nl> long i ; <nl> ( void ) i ; <nl> 
sync_array_signal_object ( <nl>  <nl> if ( cell_count == cell_max_count ) { <nl> sync_cell_t ** old_cell_ptr = cell_ptr ; <nl> - size_t old_size = cell_max_count * <nl> + size_t old_size , new_size ; <nl> + <nl> + old_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl> cell_max_count *= 2 ; <nl> - <nl> - size_t new_size = cell_max_count * <nl> + new_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl>  <nl> cell_ptr = malloc ( new_size );
btr_pcur_move_to_prev_page ( <nl>  <nl> page_cur_set_after_last ( prev_block , btr_pcur_get_page_cur ( cursor )); <nl>  <nl> - page_check_dir ( prev_page ); <nl> + ut_d ( page_check_dir ( prev_page )); <nl> } <nl>  <nl> /*********************************************************//**
row_merge_create_temporary_table ( <nl>  <nl> if ( error != DB_SUCCESS ) { <nl> trx -> error_state = error ; <nl> + dict_mem_table_free ( new_table ); <nl> new_table = NULL ; <nl> } <nl> 
ulong my_scan_8bit ( CHARSET_INFO * cs , const char * str , const char * end , int sq ) <nl> return 0 ; <nl>  <nl> case MY_SEQ_SPACES : <nl> - for ( str ++ ; str != end ; str ++) <nl> + for (; str != end ; str ++) <nl> { <nl> if (! my_isspace ( cs ,* str )) <nl> break ;
row_sel_sec_rec_is_for_clust_rec ( <nl> clust_index -> table ))) { <nl> goto inequal ; <nl> } <nl> + <nl> + continue ; <nl> } <nl> } <nl> 
i_s_zip_fill_low ( <nl>  <nl> DBUG_ENTER (" i_s_zip_fill_low "); <nl>  <nl> + /* deny access to non - superusers */ <nl> + if ( check_global_access ( thd , SUPER_ACL )) { <nl> + <nl> + DBUG_RETURN ( 0 ); <nl> + } <nl> + <nl> /* Determine log2 ( PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ). */ <nl> for ( uint r = PAGE_ZIP_MIN_SIZE / 2 / BUF_BUDDY_LOW ; r >>= 1 ; y ++); <nl> 
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
bool check_change_password ( THD * thd , const char * host , const char * user , <nl> return ( 1 ); <nl> } <nl> uint len = strlen ( new_password ); <nl> - if ( len != SCRAMBLED_PASSWORD_CHAR_LENGTH && <nl> + if ( len && len != SCRAMBLED_PASSWORD_CHAR_LENGTH && <nl> len != SCRAMBLED_PASSWORD_CHAR_LENGTH_323 ) <nl> { <nl> net_printf ( thd , 0 ,
extern void bitmap_set_prefix ( MY_BITMAP * map , uint prefix_size ); <nl> extern void bitmap_intersect ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> extern void bitmap_subtract ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> extern void bitmap_union ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> + extern void bitmap_xor ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> + extern void bitmap_invert ( MY_BITMAP * map ); <nl>  <nl> extern uint bitmap_lock_set_next ( MY_BITMAP * map ); <nl> extern void bitmap_lock_clear_bit ( MY_BITMAP * map , uint bitmap_bit );
bool rollback_log_node_cache :: give_rollback_log_node ( TOKUTXN txn , ROLLBACK_LOG_N <nl> if ( m_num_avail < m_max_num_avail ) { <nl> retval = true ; <nl> uint32_t index = m_first + m_num_avail ; <nl> - if ( index > m_max_num_avail ) { <nl> + if ( index >= m_max_num_avail ) { <nl> index -= m_max_num_avail ; <nl> } <nl> m_avail_blocknums [ index ]. b = log -> blocknum . b ;
int create_cachetable ( CACHETABLE * result , int n_entries ) { <nl> int i ; <nl> t -> n_in_table = 0 ; <nl> t -> table_size = n_entries ; <nl> - t -> table = toku_calloc ( t -> table_size , sizeof ( struct ctpair )); <nl> + MALLOC_N ( t -> table_size , t -> table ); <nl> assert ( t -> table ); <nl> t -> head = t -> tail = 0 ; <nl> for ( i = 0 ; i < t -> table_size ; i ++) {
toku_minicron_change_period ( struct minicron * p , u_int32_t new_period ) <nl> int <nl> toku_minicron_shutdown ( struct minicron * p ) { <nl> int r = toku_pthread_mutex_lock (& p -> mutex ); assert ( r == 0 ); <nl> + assert (! p -> do_shutdown ); <nl> p -> do_shutdown = TRUE ; <nl> // printf ("% s :% d signalling \ n ", __FILE__ , __LINE__ ); <nl> r = toku_pthread_cond_signal (& p -> condvar ); assert ( r == 0 );
btr_create ( <nl> PAGE_HEADER + PAGE_BTR_SEG_LEAF , mtr )) { <nl> /* Not enough space for new segment , free root <nl> segment before return . */ <nl> - fseg_free ( space , page_no , <nl> - PAGE_HEADER + PAGE_BTR_SEG_TOP ); <nl> + btr_free_root ( space , page_no , mtr ); <nl>  <nl> return ( FIL_NULL ); <nl> }
sp_head :: check_unresolved_goto () <nl> if ( m_backpatch_goto . elements > 0 ) <nl> { <nl> List_iterator_fast < bp_t > li ( m_backpatch_goto ); <nl> - bp_t * bp ; <nl> - while (( bp = li ++)) <nl> + while ( bp_t * bp = li ++) <nl> { <nl> - if (( bp -> instr_type == GOTO )) <nl> + if ( bp -> instr_type == GOTO ) <nl> { <nl> my_error ( ER_SP_LILABEL_MISMATCH , MYF ( 0 ), " GOTO ", bp -> lab -> name . str ); <nl> has_unresolved_label = true ;
buf_LRU_block_remove_hashed_page ( <nl> void * data = bpage -> zip . data ; <nl> bpage -> zip . data = NULL ; <nl>  <nl> + ut_ad (! bpage -> in_free_list ); <nl> + ut_ad (! bpage -> in_flush_list ); <nl> + ut_ad (! bpage -> in_LRU_list ); <nl> mutex_exit (&(( buf_block_t *) bpage )-> mutex ); <nl> buf_pool_mutex_exit_forbid (); <nl> buf_buddy_free ( data , page_zip_get_size (& bpage -> zip ));
trx_undo_parse_page_header ( <nl> mtr_t * mtr ) /*!< in : mtr or NULL */ <nl> { <nl> trx_id_t trx_id ; <nl> + /* Silence a GCC warning about possibly uninitialized variable <nl> + when mach_ull_parse_compressed () is not inlined . */ <nl> + ut_d ( trx_id = 0 ); <nl> + /* Declare the variable uninitialized in Valgrind , so that the <nl> + above initialization will not mask any bugs . */ <nl> + UNIV_MEM_INVALID (& trx_id , sizeof trx_id ); <nl>  <nl> ptr = mach_ull_parse_compressed ( ptr , end_ptr , & trx_id ); <nl> 
int ha_tokudb :: delete_all_rows () { <nl> txn <nl> ); <nl> if ( error ) { goto cleanup ; } <nl> + error = share -> key_file [ i ]-> pre_acquire_table_lock ( <nl> + share -> key_file [ i ], <nl> + txn <nl> + ); <nl> + if ( error ) { goto cleanup ; } <nl> } <nl> for ( uint i = 0 ; i < curr_num_DBs ; i ++) { <nl> error = truncate_dictionary ( i , txn );
bool ZIPUTIL :: WildMatch ( PSZ pat , PSZ str ) { <nl> if (!*++ pat ) return TRUE ; <nl> goto loopStart ; <nl> default : <nl> - if ( mapCaseTable [( unsigned )* s ] != mapCaseTable [( unsigned )* p ]) <nl> + if ( mapCaseTable [( uchar )* s ] != mapCaseTable [( uchar )* p ]) <nl> goto starCheck ; <nl> break ; <nl> } /* endswitch */
row_create_prebuilt ( <nl> prebuilt -> ins_node = NULL ; <nl>  <nl> prebuilt -> ins_upd_rec_buff = NULL ; <nl> - <nl> + <nl> + prebuilt -> hint_need_to_fetch_extra_cols = 0 ; <nl> + <nl> prebuilt -> upd_node = NULL ; <nl> prebuilt -> ins_graph = NULL ; <nl> prebuilt -> upd_graph = NULL ;
innobase_xa_prepare ( <nl> int error = 0 ; <nl> trx_t * trx = check_trx_exists ( thd ); <nl>  <nl> - if ( thd -> lex -> sql_command != SQLCOM_XA_PREPARE ) { <nl> + if ( thd -> lex -> sql_command != SQLCOM_XA_PREPARE && <nl> + ( all || !( thd -> options & ( OPTION_NOT_AUTOCOMMIT | OPTION_BEGIN )))) <nl> + { <nl>  <nl> /* For ibbackup to work the order of transactions in binlog <nl> and InnoDB must be the same . Consider the situation
referenced_xids_note_snapshot_txn_end_iter ( OMTVALUE live_xidv , u_int32_t UU ( inde <nl> if (-- tuple -> references == 0 ) { <nl> r = toku_omt_delete_at ( referenced_xids , idx ); <nl> lazy_assert_zero ( r ); <nl> + toku_free ( tuple ); <nl> } <nl> done : <nl> return 0 ;
dict_index_add_to_cache ( <nl>  <nl> if (! dict_index_find_cols ( table , index )) { <nl>  <nl> + dict_mem_index_free ( index ); <nl> return ( DB_CORRUPTION ); <nl> } <nl> 
enum tablespace_op_type <nl> e ||+-------------------------+ || <nl> V | neighbor | V | <nl> unit1 . 1 <+==================> unit1 . 2 unit2 . 1 <nl> - fake1 . 1 fake2 . 1 <nl> - select1 . 1 . 1 select 1 . 1 . 2 select1 . 2 . 1 select2 . 1 . 1 select2 . 1 . 2 <nl> + fake1 . 1 <nl> + select1 . 1 . 1 select 1 . 1 . 2 select1 . 2 . 1 select2 . 1 . 1 <nl> |^ <nl> || <nl> V |
void mysql_client_binlog_statement ( THD * thd ) <nl>  <nl> const char * error = 0 ; <nl> char * buf = ( char *) my_malloc ( event_len , MYF ( MY_WME )); <nl> - Log_event * ev ; <nl> + Log_event * ev = 0 ; <nl> int res ; <nl>  <nl> /*
int decimal_round ( decimal * from , decimal * to , int scale , decimal_round_mode mode <nl> error = E_DEC_TRUNCATED ; <nl> } <nl>  <nl> - if ( scale + from -> intg < 0 ) <nl> + if ( scale + from -> intg <= 0 ) <nl> { <nl> decimal_make_zero ( to ); <nl> return E_DEC_OK ;
TABLE * Delayed_insert :: get_local_table ( THD * client_thd ) <nl> goto error ; <nl> dfield_ptr = copy -> default_field ; <nl> } <nl> + copy -> expr_arena = NULL ; <nl>  <nl> /* Ensure we don ' t use the table list of the original table */ <nl> copy -> pos_in_table_list = 0 ;
static opj_bool t2_read_packet_data ( <nl>  <nl> # endif /* USE_JPWL */ <nl>  <nl> + if (( l_cblk -> len + l_seg -> newlen ) > 8192 ) { <nl> + return OPJ_FALSE ; <nl> + } <nl> + <nl> memcpy ( l_cblk -> data + l_cblk -> len , l_current_data , l_seg -> newlen ); <nl>  <nl> if ( l_seg -> numpasses == 0 ) {
buf_shrink_freelists ( int free_all ) <nl> -- n_to_free ; <nl> } <nl> tor_assert (! n_to_free ); <nl> - freelists [ i ]. lowest_length = freelists [ i ]. cur_length = n_to_skip ; <nl> + freelists [ i ]. cur_length = n_to_skip ; <nl> } <nl> + freelists [ i ]. lowest_length = freelists [ i ]. cur_length ; <nl> assert_freelist_ok (& freelists [ i ]); <nl> } <nl> }
generate_v2_networkstatus ( void ) <nl> or_options_t * options = get_options (); <nl> char fingerprint [ FINGERPRINT_LEN + 1 ]; <nl> char ipaddr [ INET_NTOA_BUF_LEN + 1 ]; <nl> - char published [ ISO_TIME_LEN ]; <nl> + char published [ ISO_TIME_LEN + 1 ]; <nl> char digest [ DIGEST_LEN ]; <nl> struct in_addr in ; <nl> uint32_t addr ;
rend_service_set_connection_addr_port ( connection_t * conn , circuit_t * circ ) <nl> serviceid , circ -> n_circ_id ); <nl> circuit_mark_for_close ( circ ); <nl> connection_mark_for_close ( conn , 0 /* XXX */); <nl> + return - 1 ; <nl> } <nl> for ( i = 0 ; i < smartlist_len ( service -> ports ); ++ i ) { <nl> p = smartlist_get ( service -> ports , i );
rend_config_services ( const or_options_t * options , int validate_only ) <nl> log_warn ( LD_CONFIG , <nl> " HiddenServiceAllowUnknownPorts should be 0 or 1 , not % s ", <nl> line -> value ); <nl> - smartlist_free ( temp_service_list ); <nl> goto free_and_return ; <nl> } <nl> log_info ( LD_CONFIG ,
parse_server_transport_line ( const char * line , int validate_only ) <nl> done : <nl> SMARTLIST_FOREACH ( items , char *, s , tor_free ( s )); <nl> smartlist_free ( items ); <nl> - SMARTLIST_FOREACH ( transport_list , char *, s , tor_free ( s )); <nl> - smartlist_free ( transport_list ); <nl> + if ( transport_list ) { <nl> + SMARTLIST_FOREACH ( transport_list , char *, s , tor_free ( s )); <nl> + smartlist_free ( transport_list ); <nl> + } <nl>  <nl> return r ; <nl> }
entry_guard_register_connect_status ( const char * digest , int succeeded , <nl> entry -> nickname , buf , tbuf ); <nl> entry -> last_attempted = now ; <nl> } <nl> - entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> + if ( entry ) <nl> + entry -> can_retry = 0 ; /* We gave it an early chance ; no good . */ <nl> } <nl>  <nl> if ( first_contact ) {
MOCK_IMPL ( STATIC X509 *, <nl> goto error ; <nl> if (! X509_set_pubkey ( x509 , pkey )) <nl> goto error ; <nl> - if (! X509_sign ( x509 , sign_pkey , EVP_sha1 ())) <nl> + <nl> + if (! X509_sign ( x509 , sign_pkey , EVP_sha256 ())) <nl> goto error ; <nl>  <nl> goto done ;
tor_tls_check_lifetime ( int severity , tor_tls_t * tls , <nl> * < b > future_tolerance </ b > seconds . If it is live , return 0 . If it is not <nl> * live , log a message and return - 1 . */ <nl> static int <nl> - check_cert_lifetime_internal ( int severity , const X509 * cert , int past_tolerance , <nl> - int future_tolerance ) <nl> + check_cert_lifetime_internal ( int severity , const X509 * cert , <nl> + int past_tolerance , int future_tolerance ) <nl> { <nl> time_t now , t ; <nl> 
router_dump_router_to_string ( char * s , size_t maxlen , routerinfo_t * router , <nl> if ( result < 0 ) <nl> return - 1 ; <nl> written += result ; <nl> - if ( written < maxlen + 2 ) <nl> + if ( written + 2 > maxlen ) <nl> return - 1 ; <nl> s [ written ++] = '\ n '; <nl> }
directory_remove_invalid ( void ) <nl> directory_set_dirty (); <nl>  <nl> routerlist_assert_ok ( rl ); <nl> + smartlist_free ( nodes ); <nl> } <nl>  <nl> /** Mark the directory as < b > dirty </ b > -- when we ' re next asked for a
consensus_queue_compression_work ( const char * consensus , <nl> config_line_prepend (& job -> labels_in , LABEL_SIGNATORIES , signers ); <nl> tor_free ( signers ); <nl> SMARTLIST_FOREACH ( hexvoters , char *, cp , tor_free ( cp )); <nl> + smartlist_free ( hexvoters ); <nl> } <nl>  <nl> if ( background_compression ) {
add_an_entry_guard ( routerinfo_t * chosen ) <nl>  <nl> again : <nl> if (-- tries_left <= 0 ) { <nl> - log_warn ( LD_CIRC , " Tried finding a new entry , but failed . Bad news . XXX ."); <nl> + log_warn ( LD_CIRC , " Tried finding a new entry guard , but failed . " <nl> + " Can you reach the Tor network ?"); <nl> return NULL ; <nl> } <nl> if ( chosen )
static int parse_redirect_line ( or_options_t * options , <nl> tor_assert ( line ); <nl>  <nl> r = tor_malloc_zero ( sizeof ( exit_redirect_t )); <nl> + elements = smartlist_create (); <nl> smartlist_split_string ( elements , line -> value , " ", <nl> SPLIT_SKIP_SPACE | SPLIT_IGNORE_BLANK , 0 ); <nl> if ( smartlist_len ( elements ) != 2 ) {
tor_spawn_background ( const char * const filename , const char ** argv , <nl> process_environment_t * env , <nl> process_handle_t ** process_handle_out ) <nl> { <nl> - if ( may_spawn_background_process == 0 ) <nl> + if ( BUG ( may_spawn_background_process == 0 )) { <nl> + /* We should never reach this point if we ' re forbidden to spawn <nl> + * processes . Instead we should have caught the attempt earlier . */ <nl> return PROCESS_STATUS_ERROR ; <nl> + } <nl>  <nl> # ifdef _WIN32 <nl> HANDLE stdout_pipe_read = NULL ;
# error " Sorry ; we don ' t support building with NDEBUG ." <nl> # else <nl> # ifdef __GNUC__ <nl> -# define PREDICT_FALSE ( x ) PREDICT (( x ) != (( typeof ( x )) 0 ), 0 ) <nl> +# define PREDICT_FALSE ( x ) PREDICT (( x ) == (( typeof ( x )) 0 ), 0 ) <nl> # else <nl> # define PREDICT_FALSE ( x ) !( x ) <nl> # endif
tor_zlib_process ( tor_zlib_state_t * state , <nl> return Z_OK ; <nl> return TOR_ZLIB_BUF_FULL ; <nl> case Z_OK : <nl> - if ( state -> stream . avail_out == 0 ) <nl> + if ( state -> stream . avail_out == 0 || finish ) <nl> return TOR_ZLIB_BUF_FULL ; <nl> return TOR_ZLIB_OK ; <nl> default :
imalloc ( size_t size ) <nl> ptralloc = 1 ; <nl> size = malloc_pagesize ; <nl> } <nl> - if (( size + malloc_pagesize ) < size ) { /* Check for overflow */ <nl> + if ( size > SIZE_MAX - malloc_pagesize ) { /* Check for overflow */ <nl> result = NULL ; <nl> errno = ENOMEM ; <nl> } else if ( size <= malloc_maxsize )
NAMESPACE_BEGIN ( CryptoPP ) <nl> RandomPool :: RandomPool () <nl> : m_pCipher ( new AES :: Encryption ), m_keySet ( false ) <nl> { <nl> + memset ( m_key , 0 , m_key . SizeInBytes ()); <nl> + memset ( m_seed , 0 , m_seed . SizeInBytes ()); <nl> } <nl>  <nl> void RandomPool :: IncorporateEntropy ( const byte * input , size_t length )
ECP :: ECP ( BufferedTransformation & bt ) <nl> GetField (). BERDecodeElement ( seq , m_b ); <nl> // skip optional seed <nl> if (! seq . EndReached ()) <nl> - BERDecodeOctetString ( seq , TheBitBucket ()); <nl> + { <nl> + SecByteBlock seed ; <nl> + unsigned int unused ; <nl> + BERDecodeBitString ( seq , seed , unused ); <nl> + } <nl> seq . MessageEnd (); <nl> } <nl> 
# include < algorithm > <nl> # include < functional > <nl>  <nl> +// R - value references and std :: move <nl> +# if defined ( __cplusplus >= 201103L ) <nl> +# include < utility > <nl> +# endif <nl> + <nl> # ifdef CRYPTOPP_INCLUDE_VECTOR_CC <nl> // workaround needed on Sun Studio 12u1 Sun C ++ 5 . 10 SunOS_i386 128229 - 02 2009 / 09 / 21 <nl> # include < vector . cc >
typedef unsigned int word32 ; <nl> # if defined ( _MSC_VER ) || defined ( __BORLANDC__ ) <nl> typedef unsigned __int64 word64 ; <nl> # define W64LIT ( x ) x ## ui64 <nl> -# elif ( _LP64 || __LP64__ ) && ! defined ( __SUNPRO_CC ) <nl> +# elif ( _LP64 || __LP64__ ) <nl> typedef unsigned long word64 ; <nl> # define W64LIT ( x ) x ## UL <nl> # else
void jshFlashErasePage ( uint32_t addr ) { <nl> if (! f ) return ; <nl> uint32_t startAddr , pageSize ; <nl> if ( jshFlashGetPage ( addr , & startAddr , & pageSize )) { <nl> + startAddr -= FLASH_START ; <nl> fseek ( f , startAddr , SEEK_SET ); <nl> char * buf = malloc ( pageSize ); <nl> memset ( buf , 0xFF , pageSize );
size_t jsvGetString ( const JsVar * v , char * str , size_t len ) { <nl> * want to pad the entire buffer with zeros */ <nl> len --; <nl> int l = 0 ; <nl> - while (* s && l < len ) { <nl> + while ( s [ l ] && l < len ) { <nl> str [ l ] = s [ l ]; <nl> l ++; <nl> }
JsVar * jspeFactor () { <nl> return jspeFactorTypeOf (); <nl> } else if ( execInfo . lex -> tk == LEX_R_VOID ) { <nl> JSP_MATCH ( LEX_R_VOID ); <nl> - jsvUnLock ( jspeBase ()); <nl> + jsvUnLock ( jspeFactor ()); <nl> return 0 ; <nl> } <nl> // Nothing we can do here ... just hope it ' s the end ...
void jsiSemiInit ( bool autoLoad ) { <nl> "| __ | _ -| . | _ | | | | | . |\ n " <nl> "| _____ | ___ | _ | _ | | ___ | _ | _ | _ | ___ |\ n " <nl> " | _ | http :// espruino . com \ n " <nl> - " " JS_VERSION " Copyright 2015 G . Williams \ n "); <nl> + " " JS_VERSION " Copyright 2016 G . Williams \ n "); <nl> # ifdef ESP8266 <nl> jshPrintBanner (); <nl> # endif
void jswrap_wifi_restore ( void ) { <nl> ap_config . ssid_hidden = jsvGetInteger ( v ); <nl> jsvUnLock ( v ); <nl>  <nl> - v = jsvObjectGetChild ( o ," ssid ", 0 ); <nl> + v = jsvObjectGetChild ( o ," ssidAP ", 0 ); <nl> jsvGetString ( v , ( char *) ap_config . ssid , sizeof ( ap_config . ssid )); <nl>  <nl> ap_config . ssid_len = jsvGetStringLength ( v );
slurmd_task_info_t * task_info_create ( int taskid , int gtaskid , <nl> static inline slurmd_task_info_t * <nl> job_task_info_by_pid ( slurmd_job_t * job , pid_t pid ) <nl> { <nl> - int i ; <nl> + uint32_t i ; <nl> for ( i = 0 ; i < job -> node_tasks ; i ++) { <nl> if ( job -> task [ i ]-> pid == pid ) <nl> return ( job -> task [ i ]);
static int _move_account ( mysql_conn_t * mysql_conn , uint32_t lft , uint32_t rgt , <nl> } <nl> xfree ( query ); <nl> if (!( row = mysql_fetch_row ( result ))) { <nl> - error (" no row "); <nl> + debug4 (" Can ' t move a none existant association "); <nl> mysql_free_result ( result ); <nl> - return SLURM_ERROR ; <nl> + return SLURM_SUCCESS ; <nl> } <nl> par_left = atoi ( row [ 0 ]); <nl> mysql_free_result ( result );
int slurm_step_launch ( slurm_step_ctx ctx , <nl> char ** env = NULL ; <nl>  <nl> debug (" Entering slurm_step_launch "); <nl> + memset (& launch , 0 , sizeof ( launch )); <nl> + <nl> if ( ctx == NULL || ctx -> magic != STEP_CTX_MAGIC ) { <nl> error (" Not a valid slurm_step_ctx !"); <nl> 
void slurm_step_launch_wait_finish ( slurm_step_ctx_t * ctx ) <nl>  <nl> /* Then shutdown the message handler thread */ <nl> eio_signal_shutdown ( sls -> msg_handle ); <nl> + pthread_mutex_unlock (& sls -> lock ); <nl> pthread_join ( sls -> msg_thread , NULL ); <nl> + pthread_mutex_lock (& sls -> lock ); <nl> eio_handle_destroy ( sls -> msg_handle ); <nl>  <nl> /* Then wait for the IO thread to finish */ <nl> void slurm_step_launch_wait_finish ( slurm_step_ctx_t * ctx ) <nl> } <nl>  <nl> mpi_hook_client_fini ( sls -> mpi_state ); <nl> - <nl> pthread_mutex_unlock (& sls -> lock ); <nl> } <nl> 
static int _load_job_state ( Buf buffer ) <nl> jobacct_storage_g_job_complete ( acct_db_conn , job_ptr ); <nl> } <nl>  <nl> - if ( job_ptr -> qos ) { <nl> + if ( job_ptr -> qos && ( accounting_enforce & ACCOUNTING_ENFORCE_ASSOCS )) { <nl> memset (& qos_rec , 0 , sizeof ( acct_qos_rec_t )); <nl> qos_rec . id = job_ptr -> qos ; <nl> if ( _determine_and_validate_qos ( job_ptr , & qos_rec )
extern int assoc_mgr_fill_in_assoc ( void * db_conn , <nl> ret_assoc = found_assoc ; <nl> debug3 (" found association " <nl> " for no partition "); <nl> + continue ; <nl> } else if ( strcasecmp ( assoc -> partition , <nl> - found_assoc -> partition )) <nl> + found_assoc -> partition )) { <nl> debug3 (" not the right partition "); <nl> - continue ; <nl> + continue ; <nl> + } <nl> } <nl> } <nl> ret_assoc = found_assoc ;
extern int make_batch_job_cred ( batch_job_launch_msg_t * launch_msg_ptr , <nl> xassert ( job_ptr -> job_resrcs ); <nl> job_resrcs_ptr = job_ptr -> job_resrcs ; <nl>  <nl> + if ( job_ptr -> job_resrcs == NULL ) { <nl> + error ("% s : job % u is missing job_resrcs info ", <nl> + __func__ , job_ptr -> job_id ); <nl> + return SLURM_ERROR ; <nl> + } <nl> + <nl> memset (& cred_arg , 0 , sizeof ( slurm_cred_arg_t )); <nl>  <nl> cred_arg . jobid = launch_msg_ptr -> job_id ;
static int _load_jobs ( slurm_msg_t * req_msg , job_info_msg_t ** job_info_msg_pptr , <nl> int i , j , rc = SLURM_SUCCESS ; <nl> int local_job_cnt ; <nl> slurm_msg_t resp_msg , resp_msg_fed ; <nl> - job_info_msg_t * orig_msg = NULL , * new_msg ; <nl> + job_info_msg_t * orig_msg = NULL , * new_msg = NULL ; <nl> uint32_t new_rec_cnt ; <nl> uint32_t hash_inx , * hash_tbl_size = NULL , ** hash_job_id = NULL ; <nl> slurmdb_cluster_rec_t * cluster ;
extern void gres_plugin_node_state_log ( List gres_list , char * node_name ) <nl> ListIterator gres_iter ; <nl> gres_node_state_t * gres_ptr ; <nl>  <nl> + if ( gres_list == NULL ) <nl> + return ; <nl> + <nl> ( void ) gres_plugin_init (); <nl>  <nl> slurm_mutex_lock (& gres_context_lock );
extern void addto_qos_char_list ( List char_list , List qos_list , char * names ) <nl> bad : <nl> i ++; <nl> start = i ; <nl> + if (! names [ i ]) { <nl> + info (" There is a problem with " <nl> + " your line . It appears you " <nl> + " have spaces inside your list ."); <nl> + break ; <nl> + } <nl> } <nl> i ++; <nl> }
static void * _agent ( void * x ) <nl> free_buf ( buffer ); <nl> fail_time = 0 ; <nl> } else { <nl> + /* We still need to free a mult_msg even if we <nl> + got a failure . <nl> + */ <nl> + if ( list_msg . my_list ) { <nl> + list_msg . my_list = NULL ; <nl> + free_buf ( buffer ); <nl> + } <nl> + <nl> fail_time = time ( NULL ); <nl> } <nl> slurm_mutex_unlock (& agent_lock );
static uint32_t _update_weighted_freq ( struct jobacctinfo * jobacct , <nl> jobacct -> current_weighted_freq = <nl> jobacct -> current_weighted_freq + <nl> ( uint32_t ) jobacct -> this_sampled_cputime * thisfreq ; <nl> - if ( jobacct -> tot_cpu ) { <nl> + if ( jobacct -> tot_cpu >= 1 ) { <nl> return ( jobacct -> current_weighted_freq / <nl> ( uint32_t ) jobacct -> tot_cpu ); <nl> } else
static void _purge_missing_jobs ( int node_inx , time_t now ) <nl> requeue = true ; <nl> info (" Batch JobId =% u missing from node 0 ", <nl> job_ptr -> job_id ); <nl> + job_ptr -> exit_code = 1 ; <nl> job_complete ( job_ptr -> job_id , 0 , requeue , true , NO_VAL ); <nl> } else { <nl> _notify_srun_missing_step ( job_ptr , node_inx ,
bool slurm_container_has_pid ( uint32_t cont_id , pid_t pid ) <nl>  <nl> int slurm_container_wait ( uint32_t id ) <nl> { <nl> - if ( _job_waitjid (( jid_t ) id , NULL , 0 ) == ( jid_t )- 1 ) <nl> + int status ; <nl> + if ( _job_waitjid (( jid_t ) id , & status , 0 ) == ( jid_t )- 1 ) <nl> return SLURM_ERROR ; <nl>  <nl> return SLURM_SUCCESS ;
static void _purge_missing_jobs ( int node_inx , time_t now ) <nl> node_boot_time = node_ptr -> boot_time - ( msg_timeout + 5 ); <nl> } <nl> batch_startup_time = now - batch_start_timeout ; <nl> - batch_startup_time -= msg_timeout ; <nl> + batch_startup_time -= MIN ( DEFAULT_MSG_TIMEOUT , msg_timeout ); <nl>  <nl> job_iterator = list_iterator_create ( job_list ); <nl> while (( job_ptr = ( struct job_record *) list_next ( job_iterator ))) {
char * slurm_sprint_partition_info ( partition_info_t * part_ptr , <nl> if ( part_ptr -> disable_root_jobs ) <nl> sprintf ( tmp_line , " DisableRootJobs = YES "); <nl> else <nl> - sprintf ( tmp_line , " DisableRootJobs = NO "); <nl> + sprintf ( tmp_line , " DisableRootJobs = NO "); <nl> xstrcat ( out , tmp_line ); <nl>  <nl> if ( part_ptr -> hidden )
geocache_cfg * geocache_configuration_create ( apr_pool_t * pool ) { <nl> grid -> srs = apr_pstrdup ( pool ," epsg : 4326 "); <nl> grid -> unit = GEOCACHE_UNIT_DEGREES ; <nl> grid -> tile_sx = grid -> tile_sy = 256 ; <nl> - grid -> nlevels = 16 ; <nl> + grid -> nlevels = 19 ; <nl> grid -> extent [ 0 ] = wgs84_extent [ 0 ]; <nl> grid -> extent [ 1 ] = wgs84_extent [ 1 ]; <nl> grid -> extent [ 2 ] = wgs84_extent [ 2 ];
styleObj * msRemoveStyle ( classObj * class , int nStyleIndex ) { <nl> return NULL ; <nl> } <nl> msCopyStyle ( style , &( class -> styles [ nStyleIndex ])); <nl> + style -> isachild = MS_FALSE ; <nl> for ( i = nStyleIndex ; i < class -> numstyles - 1 ; i ++) { <nl> msCopyStyle (& class -> styles [ i ], & class -> styles [ i + 1 ]); <nl> }
int loadLayer ( layerObj * layer , mapObj * map ) <nl> if ( getString (& layer -> tileindex ) == MS_FAILURE ) return (- 1 ); <nl> break ; <nl> case ( TILEITEM ): <nl> + free ( layer -> tileitem ); layer -> tileitem = NULL ; // erase default <nl> if ( getString (& layer -> tileitem ) == MS_FAILURE ) return (- 1 ); <nl> break ; <nl> case ( TOLERANCE ):
MS_DLL_EXPORT int freeClass ( classObj * ); <nl> MS_DLL_EXPORT void initLabel ( labelObj * label ); <nl> MS_DLL_EXPORT void resetClassStyle ( classObj * _class ); <nl> MS_DLL_EXPORT int initStyle ( styleObj * style ); <nl> + MS_DLL_EXPORT int freeStyle ( styleObj * style ); <nl> MS_DLL_EXPORT void initReferenceMap ( referenceMapObj * ref ); <nl> MS_DLL_EXPORT void initScalebar ( scalebarObj * scalebar ); <nl> MS_DLL_EXPORT void initGrid ( graticuleObj * pGraticule );
int msDumpLayer ( mapObj * map , layerObj * lp , int nVersion , const char * script_url_ <nl> free ( nestedGroups ); <nl> free ( numNestedGroups ); <nl> free ( isUsedInNestedGroup ); <nl> + free ( group_layers ); <nl> } <nl> } <nl> }
int msGetGDALGeoTransform ( GDALDatasetH hDS , mapObj * map , layerObj * layer , <nl> } <nl> /* fullPath has a filename included , so get the extension */ <nl> else { <nl> - fileExtension = strrchr ( szPath ,'.') + 1 ; <nl> + fileExtension = msStrdup ( strrchr ( szPath ,'.') + 1 ); <nl> } <nl> } <nl> /* common behaviour with worldfile generated from basename + . wld */
int initStyle ( styleObj * style ) { <nl> style -> sizescaled = style -> size = 1 ; // in SIZEUNITS ( layerObj ) <nl> style -> minsize = MS_MINSYMBOLSIZE ; <nl> style -> maxsize = MS_MAXSYMBOLSIZE ; <nl> - style -> offsetx = style -> offsety = - 1 ; <nl> + style -> offsetx = style -> offsety = 0 ; <nl>  <nl> return MS_SUCCESS ; <nl> }
static gboolean wsq_tlskey_inited = FALSE ; <nl> void <nl> mono_wsq_init () <nl> { <nl> + if ( wsq_tlskey_inited ) <nl> + return ; <nl> + <nl> mono_native_tls_alloc ( wsq_tlskey , NULL ); <nl> + wsq_tlskey_inited = TRUE ; <nl> } <nl>  <nl> void
mono_threads_summarize ( MonoContext * ctx , gchar ** out , MonoStackHash * hashes ) <nl> int count = 4 ; <nl>  <nl> while ( old_num_summarized == num_threads_summarized && count > 0 ) { <nl> + if ( thread -> state & ( ThreadState_Unstarted | ThreadState_Aborted | ThreadState_Stopped )) <nl> + break ; <nl> + <nl> sleep ( 1 ); <nl> mono_memory_barrier (); <nl> const char * name = thread_summary_state_to_str ( summarizing_thread_state );
read_pipes ( int outfd , gchar ** out_str , int errfd , gchar ** err_str ) <nl> err_closed = ( nread <= 0 ); <nl> } <nl> } <nl> - <nl> - } while ( res == - 1 && errno == EINTR ); <nl> + } while ( res > 0 || ( res == - 1 && errno == EINTR )); <nl>  <nl> g_free ( buffer ); <nl> if ( out_str )
mono_handle_stack_free ( HandleStack * stack ) <nl> c = next ; <nl> } <nl> g_free ( c ); <nl> + g_free ( stack ); <nl> } <nl>  <nl> void
mono_arch_create_specific_trampoline ( gpointer arg1 , MonoTrampolineType tramp_ty <nl>  <nl> code = buf = mono_domain_code_reserve_align ( domain , size , 1 ); <nl>  <nl> - if ((( gint64 ) tramp - ( gint64 ) code ) >> 31 != 0 ) { <nl> + if ((( gint64 ) tramp - ( gint64 ) code ) >> 31 != 0 && (( gint64 ) tramp - ( gint64 ) code ) >> 31 != - 1 ) { <nl> # ifndef MONO_ARCH_NOMAP32BIT <nl> g_assert_not_reached (); <nl> # endif
mono_local_emulate_ops ( MonoCompile * cfg ) <nl> * at IR level , instead of inlining the icall wrapper . FIXME <nl> */ <nl> if ( inlined_wrapper ) { <nl> - mono_decompose_long_opts ( cfg ); <nl> + if (! COMPILE_LLVM ( cfg )) <nl> + mono_decompose_long_opts ( cfg ); <nl> if ( cfg -> opt & ( MONO_OPT_CONSPROP | MONO_OPT_COPYPROP )) <nl> mono_local_cprop ( cfg ); <nl> }
continuation_store ( MonoContinuation * cont , int state , MonoException ** e ) <nl> /* clear to avoid GC retention */ <nl> if ( num_bytes < cont -> stack_used_size ) { <nl> memset (( char *) cont -> saved_stack + num_bytes , 0 , cont -> stack_used_size - num_bytes ); <nl> - cont -> stack_used_size = num_bytes ; <nl> } <nl> + cont -> stack_used_size = num_bytes ; <nl> } else { <nl> tasklets_lock (); <nl> internal_init ();
mono_jit_walk_stack_from_ctx_in_thread ( MonoJitStackWalk func , MonoDomain * domai <nl> /* <nl> * FIXME : These frames show up twice , and ctx could refer to native code . <nl> */ <nl> + ctx = new_ctx ; <nl> continue ; <nl> } <nl> frame . actual_method = get_method_from_stack_frame ( frame . ji , get_generic_info_from_stack_frame ( frame . ji , & ctx ));
retry : <nl> * between 1 and 2 , the object is still live ) <nl> */ <nl> * objslot = NULL ; <nl> + SET_OWNER ( top , idx ); <nl> + SET_SP ( handles , top , idx ); <nl> mono_memory_write_barrier (); <nl> top -> size ++; <nl> mono_memory_write_barrier (); <nl> * objslot = obj ; <nl> - SET_OWNER ( top , idx ); <nl> - SET_SP ( handles , top , idx ); <nl> return objslot ; <nl> } <nl> if ( G_LIKELY ( top -> next )) {
load_aot_module ( MonoAssembly * assembly , gpointer user_data ) <nl> mono_trace ( G_LOG_LEVEL_INFO , MONO_TRACE_AOT , " AOT : image '% s ' not found : % s ", aot_name , err ); <nl> g_free ( err ); <nl> } <nl> + g_free ( aot_name ); <nl> } <nl> if (! sofile ) { <nl> GList * l ;
 <nl> typedef struct _MSBlockInfo MSBlockInfo ; <nl> struct _MSBlockInfo { <nl> - int obj_size ; <nl> - int obj_size_index ; <nl> + guint16 obj_size ; <nl> + /* <nl> + * FIXME : Do we even need this ? It ' s only used during sweep and might be worth <nl> + * recalculating to save the space . <nl> + */ <nl> + guint16 obj_size_index ; <nl> unsigned int pinned : 1 ; <nl> unsigned int has_references : 1 ; <nl> unsigned int has_pinned : 1 ; /* means cannot evacuate */
acfg_free ( MonoAotCompile * acfg ) <nl> g_hash_table_destroy ( acfg -> image_hash ); <nl> g_hash_table_destroy ( acfg -> unwind_info_offsets ); <nl> g_hash_table_destroy ( acfg -> method_label_hash ); <nl> - if (! acfg -> typespec_classes ) <nl> + if ( acfg -> typespec_classes ) <nl> g_hash_table_destroy ( acfg -> typespec_classes ); <nl> g_hash_table_destroy ( acfg -> export_names ); <nl> g_hash_table_destroy ( acfg -> plt_entry_debug_sym_cache );
mono_llvm_emit_method ( MonoCompile * cfg ) <nl> if ( cfg -> method -> save_lmf && ! cfg -> llvm_only ) <nl> LLVM_FAILURE ( ctx , " lmf "); <nl>  <nl> - if ( sig -> pinvoke && cfg -> method -> wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE ) <nl> + if ( sig -> pinvoke && cfg -> method -> wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE && ! cfg -> llvm_only ) <nl> LLVM_FAILURE ( ctx , " pinvoke signature "); <nl>  <nl> header = cfg -> header ;
coverage_filter ( MonoProfiler * prof , MonoMethod * method ) <nl> MonoLockFreeQueue * image_methods , * class_methods ; <nl> MonoLockFreeQueueNode * node ; <nl>  <nl> - if (! coverage_initialized ) <nl> - return FALSE ; <nl> + g_assert ( coverage_initialized && " Why are we being asked for coverage filter info when we ' re not doing coverage ?"); <nl>  <nl> COVERAGE_DEBUG ( fprintf ( stderr , " Coverage filter for % s \ n ", mono_method_get_name ( method ));) <nl> 
sgen_stop_world ( int generation ) <nl> { <nl> int count , dead ; <nl>  <nl> - /* XXX this is the right stop , thought might not be the nicest place to put it */ <nl> - sgen_process_togglerefs (); <nl> - <nl> mono_profiler_gc_event ( MONO_GC_EVENT_PRE_STOP_WORLD , generation ); <nl> MONO_GC_WORLD_STOP_BEGIN (); <nl> acquire_gc_locks (); <nl>  <nl> + /* We start to scan after locks are taking , this ensures we won ' t be interrupted . */ <nl> + sgen_process_togglerefs (); <nl> + <nl> update_current_thread_stack (& count ); <nl>  <nl> sgen_global_stop_count ++;
mono_gc_pthread_detach ( pthread_t thread ) <nl> void <nl> mono_gc_pthread_exit ( void * retval ) <nl> { <nl> + mono_thread_info_dettach (); <nl> pthread_exit ( retval ); <nl> } <nl> 
void check_object ( char * start ); <nl> */ <nl>  <nl> const char * descriptor_types [] = { <nl> + " INVALID ", <nl> " run_length ", <nl> " small_bitmap ", <nl> - " string ", <nl> " complex ", <nl> " vector ", <nl> - " array ", <nl> " large_bitmap ", <nl> - " complex_arr " <nl> + " complex_arr ", <nl> + " complex_ptrfree " <nl> }; <nl>  <nl> static char * describe_nursery_ptr ( char * ptr , gboolean need_setup );
mini_method_compile ( MonoMethod * method , guint32 opts , MonoDomain * domain , JitFl <nl> // g_free ( nm ); <nl> } <nl> if ( cfg -> llvm_only ) { <nl> + g_free ( cfg -> exception_message ); <nl> cfg -> disable_aot = TRUE ; <nl> return cfg ; <nl> }
mono_mempool_alloc ( MonoMemPool * pool , guint size ) <nl>  <nl> # ifdef MALLOC_ALLOCATION <nl> { <nl> - Chunk * c = g_malloc ( size ); <nl> + Chunk * c = g_malloc ( size + sizeof ( Chunk )); <nl>  <nl> c -> next = pool -> chunks ; <nl> pool -> chunks = c ;
seq_point_info_add_seq_point ( MonoSeqPointInfo * info , SeqPoint * sp , SeqPoint * la <nl> guint8 buffer [ 4 ]; <nl> guint8 len ; <nl>  <nl> + if (! info -> has_debug_data && <nl> + ( sp -> il_offset == METHOD_ENTRY_IL_OFFSET || sp -> il_offset == METHOD_EXIT_IL_OFFSET )) <nl> + return FALSE ; <nl> + <nl> /* check that data can be added to the arrays */ <nl> g_assert ( info -> alloc_arrays ); <nl> 
void <nl> mono_llvm_free_domain_info ( MonoDomain * domain ) <nl> { <nl> /* This is called even when llvm is not enabled */ <nl> - if ( mono_llvm_free_domain_info_fptr ) <nl> + if ( backend . free_domain_info ) <nl> backend . free_domain_info ( domain ); <nl> } <nl> 
mono_trace_set_printerr_handler ( MonoPrintCallback callback ) <nl> { <nl> g_assert ( callback ); <nl> printerr_callback = callback ; <nl> - g_set_print_handler ( printerr_handler ); <nl> + g_set_printerr_handler ( printerr_handler ); <nl> }
add_wrappers ( MonoAotCompile * acfg ) <nl> if ( export_name ) <nl> g_hash_table_insert ( acfg -> export_names , wrapper , export_name ); <nl> } <nl> + g_free ( cattr ); <nl> } <nl>  <nl> if (( method -> flags & METHOD_ATTRIBUTE_PINVOKE_IMPL ) ||
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> } <nl>  <nl> /* Add a sequence point for method entry / exit events */ <nl> - if ( cfg -> gen_seq_points_debug_data ) { <nl> + if ( seq_points && cfg -> gen_seq_points_debug_data ) { <nl> NEW_SEQ_POINT ( cfg , ins , METHOD_ENTRY_IL_OFFSET , FALSE ); <nl> MONO_ADD_INS ( init_localsbb , ins ); <nl> NEW_SEQ_POINT ( cfg , ins , METHOD_EXIT_IL_OFFSET , FALSE );
mono_allocate_stack_slots2 ( MonoCompile * cfg , gboolean backward , guint32 * stack_ <nl> if ( cfg -> disable_reuse_stack_slots ) <nl> reuse_slot = FALSE ; <nl>  <nl> + t = mini_get_underlying_type ( cfg , t ); <nl> switch ( t -> type ) { <nl> case MONO_TYPE_GENERICINST : <nl> if (! mono_type_generic_inst_is_valuetype ( t )) {
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> MonoMethod * cil_method ; <nl>  <nl> if ( method -> wrapper_type != MONO_WRAPPER_NONE ) { <nl> - if ( cfg -> verbose_level > 2 ) <nl> + if ( constrained_call && cfg -> verbose_level > 2 ) <nl> printf (" DM Constrained call to % s \ n ", mono_type_get_full_name ( constrained_call )); <nl> cmethod = ( MonoMethod *) mono_method_get_wrapper_data ( method , token ); <nl> cil_method = cmethod ;
decode_exception_debug_info ( MonoAotModule * amodule , MonoDomain * domain , <nl> mono_error_cleanup (& error ); /* FIXME don ' t swallow the error */ <nl> } <nl>  <nl> - gi -> generic_sharing_context = g_new0 ( MonoGenericSharingContext , 1 ); <nl> + gi -> generic_sharing_context = alloc0_jit_info_data ( domain , sizeof ( MonoGenericSharingContext ), async ); <nl> if ( decode_value ( p , & p )) { <nl> /* gsharedvt */ <nl> MonoGenericSharingContext * gsctx = gi -> generic_sharing_context ;
try <nl> rccS . send ( command ); <nl> string receive = rccS . recv (); <nl> cout << receive ; <nl> - exit ( 0 ); <nl> + return 0 ; <nl> } <nl> catch ( AhuException & ae ) <nl> { <nl> cerr <<" Fatal : "<< ae . reason <<"\ n "; <nl> - exit ( 1 ); <nl> + return 1 ; <nl> }
bool isEDNSOptionInOpt ( const std :: string & packet , const size_t optStart , const s <nl> size_t p = optStart + 9 ; <nl> uint16_t rdLen = ( 0x100 * packet . at ( p ) + packet . at ( p + 1 )); <nl> p += sizeof ( rdLen ); <nl> - if ( 11 + rdLen > optLen ) { <nl> + if ( rdLen > ( optLen - 11 )) { <nl> return false ; <nl> } <nl> 
void doClient ( ComboAddress server , const std :: string & command ) <nl> msg . assign ( resp . get (), len ); <nl> msg = sodDecryptSym ( msg , g_key , theirs ); <nl> cout << msg ; <nl> + cout . flush (); <nl> } <nl> } <nl> else {
try <nl>  <nl> qtype = mdp . d_qtype ; <nl> qclass = mdp . d_qclass ; <nl> + <nl> + d_trc = TSIGRecordContent (); <nl> + <nl> return 0 ; <nl> } <nl> catch ( std :: exception & e ) {
vector < std :: function < void ( void )>> setupLua ( bool client , const std :: string & confi <nl> return std :: shared_ptr < DNSAction >( new NoRecurseAction ); <nl> }); <nl>  <nl> + g_lua . writeFunction (" DropAction ", []() { <nl> + return std :: shared_ptr < DNSAction >( new DropAction ); <nl> + }); <nl> + <nl> + <nl> g_lua . writeFunction (" MaxQPSIPRule ", []( unsigned int qps ) { <nl> return std :: shared_ptr < DNSRule >( new MaxQPSIPRule ( qps )); <nl> });
RemoteBackend :: RemoteBackend ( const std :: string & suffix ) <nl> { <nl> setArgPrefix (" remote "+ suffix ); <nl> build ( getArg (" connection - string ")); <nl> + this -> d_result = NULL ; <nl> this -> d_dnssec = mustDo (" dnssec "); <nl> this -> d_index = - 1 ; <nl> this -> d_trxid = 0 ;
HTTPserver :: HTTPserver ( const char * _docs_dir , const char * _scripts_dir ) { <nl> bool use_http = true ; <nl> struct stat statsBuf ; <nl> int stat_rc ; <nl> + struct timeval tv ; <nl>  <nl> + /* Randomize data */ <nl> + gettimeofday (& tv , NULL ); <nl> + srand ( tv . tv_sec + tv . tv_usec ); <nl> + <nl> static char * http_options [] = { <nl> ( char *)" listening_ports ", ports , <nl> ( char *)" enable_directory_listing ", ( char *)" no ",
int MySQLDB :: exec_sql_query ( MYSQL * conn , char * sql , <nl> // than a simple 0 <nl> if (( result = mysql_store_result (& mysql )) == NULL ) <nl> rc = 0 ; // unable to retrieve the result but still the query succeded <nl> - else <nl> + else { <nl> + mysql_free_result ( result ); <nl> rc = mysql_num_rows ( result ); <nl> + } <nl> } <nl>  <nl> if ( doLock && m ) m -> unlock ( __FILE__ , __LINE__ );
int pem_read_buffer ( pem_context * ctx , char * header , char * footer , const unsigne <nl> return ( POLARSSL_ERR_PEM_PASSWORD_MISMATCH ); <nl> } <nl> # else <nl> + free ( buf ); <nl> return ( POLARSSL_ERR_PEM_FEATURE_UNAVAILABLE ); <nl> # endif <nl> }
int ssl_get_ciphersuite_id ( const char * ciphersuite_name ) <nl>  <nl> const char * ssl_get_ciphersuite ( const ssl_context * ssl ) <nl> { <nl> + if ( ssl == NULL || ssl -> session == NULL ) <nl> + return NULL ; <nl> + <nl> return ssl_get_ciphersuite_name ( ssl -> session -> ciphersuite ); <nl> } <nl> 
location_cell_data_func ( GtkTreeViewColumn * column , <nl> NAUTILUS_LIST_MODEL_FILE_COLUMN , & file , <nl> - 1 ); <nl>  <nl> + /* The file might be NULL if we just toggled an expander <nl> + * and we ' re still loading the subdirectory . <nl> + */ <nl> + if ( file == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> if ( show_trash_orig && nautilus_file_is_in_trash ( file )) { <nl> NautilusFile * orig_file ; <nl> 
on_widget_destroyed ( GtkWidget * widget , <nl> self -> details -> change_idle_id = 0 ; <nl> } <nl>  <nl> + free_fade ( self ); <nl> self -> details -> widget = NULL ; <nl> } <nl> 
eazel_install_install_packages ( EazelInstall * service , GList * categories ) <nl> if ( install_new_packages ( service , categories )== FALSE ) { <nl> g_warning ( _ (" Install failed ")); <nl> } <nl> - eazel_install_emit_done ( service ); <nl> if ( eazel_install_emit_delete_files ( service )) { <nl> GList * item ; <nl> GList * cat ; <nl> eazel_install_install_packages ( EazelInstall * service , GList * categories ) <nl> } <nl> } <nl> } <nl> + eazel_install_emit_done ( service ); <nl> } <nl>  <nl> void
struct _NautilusPathBarClass <nl> { <nl> GtkContainerClass parent_class ; <nl>  <nl> - void (* path_clicked ) ( NautilusPathBar * path_bar , <nl> - GFile * location ); <nl> - void (* path_event ) ( NautilusPathBar * path_bar , <nl> - GdkEventButton * event , <nl> - GFile * location ); <nl> + void (* path_clicked ) ( NautilusPathBar * path_bar , <nl> + GFile * location ); <nl> + gboolean (* path_event ) ( NautilusPathBar * path_bar , <nl> + GdkEventButton * event , <nl> + GFile * location ); <nl> }; <nl>  <nl> GType nautilus_path_bar_get_type ( void ) G_GNUC_CONST ;
nautilus_link_get_link_uri_from_desktop ( GKeyFile * key_file , const char * desktop <nl> g_object_unref ( parent ); <nl> } <nl> } <nl> + g_free ( scheme ); <nl> } <nl>  <nl> return retval ;
nautilus_window_slot_content_view_matches ( NautilusWindowSlot * self , <nl> return FALSE ; <nl> } <nl>  <nl> - if ( id != NAUTILUS_VIEW_INVALID_ID && NAUTILUS_IS_FILES_VIEW ( priv -> content_view )){ <nl> + if ( id != NAUTILUS_VIEW_INVALID_ID && NAUTILUS_IS_FILES_VIEW ( priv -> content_view )) { <nl> return nautilus_files_view_get_view_id ( NAUTILUS_FILES_VIEW ( priv -> content_view )) == id ; <nl> } else { <nl> return FALSE ;
clicked_within_double_click_interval ( NautilusIconContainer * container ) <nl> /* Stash time for next compare */ <nl> last_click_time = current_time ; <nl>  <nl> - return ( click_count > 0 ); <nl> + /* Only allow double click */ <nl> + return ( click_count == 1 ); <nl> } <nl>  <nl> static void
pk_proxy_appeared_cb ( GObject * source , <nl>  <nl> /* show an unhelpful dialog */ <nl> show_unhandled_type_error ( parameters_install ); <nl> - /* The callback wasn ' t started , so we have to free the parameters */ <nl> - activate_parameters_install_free ( parameters_install ); <nl>  <nl> return ; <nl> }
done_loading ( NautilusFilesView * view , <nl> } else if ( pending_selection != NULL && all_files_seen ) { <nl> view -> details -> pending_selection = NULL ; <nl>  <nl> - nautilus_files_view_call_set_selection ( view , selection ); <nl> + nautilus_files_view_call_set_selection ( view , pending_selection ); <nl> do_reveal = TRUE ; <nl> } <nl> 
nautilus_search_hit_compute_scores ( NautilusSearchHit * hit , <nl> proximity_bonus , recent_bonus , match_bonus ); <nl>  <nl> g_date_time_unref ( now ); <nl> + g_free ( query_uri ); <nl> } <nl>  <nl> const char *
eel_canvas_button ( GtkWidget * widget , GdkEventButton * event ) <nl>  <nl> canvas = EEL_CANVAS ( widget ); <nl>  <nl> + /* Don ' t handle extra mouse button events */ <nl> + if ( event -> button > 5 ) <nl> + return FALSE ; <nl> + <nl> /* <nl> * dispatch normally regardless of the event ' s window if an item has <nl> * has a pointer grab in effect
notebook_create_window_cb ( GtkNotebook * notebook , <nl> g_object_set_data ( G_OBJECT ( slot ), " dnd - window - slot ", <nl> GINT_TO_POINTER ( TRUE )); <nl>  <nl> + gtk_window_set_position ( GTK_WINDOW ( new_window ), GTK_WIN_POS_MOUSE ); <nl> + <nl> return GTK_NOTEBOOK ( new_window -> details -> notebook ); <nl> } <nl> 
static GF_Err gf_isom_parse_movie_boxes_internal ( GF_ISOFile * mov , u32 * boxType , <nl> if ( pos < 0 ) pos = 0 ; <nl> gf_list_insert ( mov -> TopBoxes , brand , pos ); <nl> } <nl> + gf_isom_box_del ( a ); <nl> } <nl> break ; <nl> 
GF_Err gf_bin128_parse ( const char * string , bin128 value ) <nl> break ; <nl> sprintf ( szV , "% c % c ", string [ j ], string [ j + 1 ]); <nl> sscanf ( szV , "% x ", & v ); <nl> + if ( i > 15 ) { <nl> + // force error check below <nl> + i ++; <nl> + break ; <nl> + } <nl> value [ i ] = v ; <nl> i ++; <nl> + <nl> } <nl> } <nl> if ( i != 16 ) {
static GF_Err ft_set_font ( GF_FontReader * dr , const char * OrigFontName , u32 style <nl> opt = gf_modules_get_option (( GF_BaseInterface *) dr , " FontEngine ", fname ); <nl>  <nl> if ( opt ) { <nl> - gf_free ( fname ); <nl> FT_Face face ; <nl> + gf_free ( fname ); <nl> if ( FT_New_Face ( ftpriv -> library , opt , 0 , & face )) return GF_IO_ERR ; <nl> if (! face ) return GF_IO_ERR ; <nl> gf_list_add ( ftpriv -> loaded_fonts , face );
GF_Err MergeFragment ( GF_MovieFragmentBox * moof , GF_ISOFile * mov ) <nl>  <nl> static void FixTrackID ( GF_ISOFile * mov ) <nl> { <nl> + if (! mov -> moov ) return ; <nl> + <nl> if ( gf_list_count ( mov -> moov -> trackList ) == 1 && gf_list_count ( mov -> moof -> TrackList ) == 1 ) { <nl> GF_TrackFragmentBox * traf = ( GF_TrackFragmentBox *) gf_list_get ( mov -> moof -> TrackList , 0 ); <nl> GF_TrackBox * trak = ( GF_TrackBox *) gf_list_get ( mov -> moov -> trackList , 0 );
GF_Err dump_isom_xml ( GF_ISOFile * file , char * inName , Bool is_final_name , Bool do <nl> if ( e ) { <nl> fprintf ( stderr , " Error dumping ISO structure \ n "); <nl> } <nl> - if ( gf_isom_get_track_count ( file ) == 0 ) <nl> - do_track_dump = GF_FALSE ; <nl>  <nl> if ( do_track_dump ) { <nl> u32 i , j ;
s32 AVC_ReadSeqInfo ( char * sps_data , u32 sps_size , AVCState * avc , u32 subseq_sps , <nl>  <nl> pcomp = gf_bs_read_int ( bs , 8 ); <nl> /* sanity checks */ <nl> - if ( pcomp && 0x3 ) <nl> - goto exit ; <nl> + // JLF commented - breaks SVC import and no time to investigate <nl> +// if ( pcomp && 0x3 ) goto exit ; <nl>  <nl> level_idc = gf_bs_read_int ( bs , 8 ); <nl> 
static Bool enum_dir_fct ( void * cbck , char * file_name , char * file_path ) <nl> JSObject * obj ; <nl> enum_dir_cbk * cbk = ( enum_dir_cbk *) cbck ; <nl>  <nl> + if ( file_name && ( file_name [ 0 ]=='.')) return 0 ; <nl> + <nl> obj = JS_NewObject ( cbk -> c , 0 , 0 , 0 ); <nl> s = JS_NewStringCopyZ ( cbk -> c , file_name ); <nl> JS_DefineProperty ( cbk -> c , obj , " name ", STRING_TO_JSVAL ( s ), 0 , 0 , JSPROP_READONLY | JSPROP_PERMANENT );
GF_Err infe_Read ( GF_Box * s , GF_BitStream * bs ) <nl> } <nl> string_start += string_len ; <nl> string_len = 0 ; <nl> + if ( ptr -> content_encoding && ptr -> version == 1 ) { <nl> + break ; <nl> + } <nl> } <nl> string_len ++; <nl> }
GF_Err adts_dmx_process ( GF_Filter * filter ) <nl> break ; <nl> } <nl>  <nl> + if ( ctx -> hdr . frame_size < ctx -> hdr . hdr_size ) { <nl> + GF_LOG ( GF_LOG_WARNING , GF_LOG_PARSER , ("[ ADTSDmx ] Corrupted ADTS frame header , resyncing \ n ")); <nl> + ctx -> nb_frames = 0 ; <nl> + goto drop_byte ; <nl> + } <nl> + <nl> adts_dmx_check_pid ( filter , ctx ); <nl>  <nl> if (! ctx -> is_playing ) {
static void parse_sec_attr_44 ( sc_file_t * file , const u8 * buf , size_t len ) <nl> } <nl>  <nl> /* Encryption key present ? */ <nl> - iPinCount = iACLen - 1 ; <nl> + iPinCount = iACLen > 0 ? iACLen - 1 : 0 ; <nl>  <nl> if ( buf [ iOffset ] & 0x20 ) { <nl> int iSC ;
static int jcop_set_security_env ( sc_card_t * card , <nl> tmp . algorithm_ref |= 0x10 ; <nl> if ( tmp . algorithm_flags & SC_ALGORITHM_RSA_HASH_MD5 ) <nl> tmp . algorithm_ref |= 0x20 ; <nl> - env =& tmp ; <nl> + <nl> + memcpy ( env , & tmp , sizeof ( struct sc_security_env )); <nl> } <nl>  <nl> sc_format_apdu ( card , & apdu , SC_APDU_CASE_3_SHORT , 0x22 , 0xC1 , 0 );
jpki_finish ( sc_card_t * card ) <nl> struct jpki_private_data * drvdata = JPKI_DRVDATA ( card ); <nl>  <nl> LOG_FUNC_CALLED ( card -> ctx ); <nl> - <nl> + if ( drvdata -> mf ) { <nl> + free ( drvdata -> mf ); <nl> + drvdata -> mf = NULL ; <nl> + } <nl> if ( drvdata ) { <nl> free ( drvdata ); <nl> card -> drv_data = NULL ;
sc_pkcs15_free_pubkey_info ( sc_pkcs15_pubkey_info_t * info ) <nl> { <nl> if ( info -> subject . value ) <nl> free ( info -> subject . value ); <nl> + if ( info -> direct . spki . value ) <nl> + free ( info -> direct . spki . value ); <nl> + if ( info -> direct . raw . value ) <nl> + free ( info -> direct . raw . value ); <nl> sc_pkcs15_free_key_params (& info -> params ); <nl> free ( info ); <nl> }
parse_dir_record ( sc_card_t * card , u8 ** buf , size_t * buflen , int rec_nr ) <nl> else <nl> app -> label = NULL ; <nl>  <nl> - if ( asn1_dirrecord [ 2 ]. flags & SC_ASN1_PRESENT ) { <nl> + if ( asn1_dirrecord [ 2 ]. flags & SC_ASN1_PRESENT && path_len > 0 ) { <nl> /* application path present : ignore AID */ <nl> if ( path_len > SC_MAX_PATH_SIZE ) { <nl> free ( app );
pgp_finish ( sc_card_t * card ) <nl> pgp_iterate_blobs ( priv -> mf , 99 , pgp_free_blob ); <nl>  <nl> free ( priv ); <nl> + card -> drv_data = NULL ; <nl> return 0 ; <nl> } <nl> 
pkcs15_pubkey_get_attribute ( struct sc_pkcs11_session * session , void * object , CK_ <nl> *( CK_OBJECT_CLASS *) attr -> pValue = CKO_PUBLIC_KEY ; <nl> break ; <nl> case CKA_TOKEN : <nl> + check_attribute_buffer ( attr , sizeof ( CK_BBOOL )); <nl> + *( CK_BBOOL *) attr -> pValue = TRUE ; <nl> + break ; <nl> case CKA_SENSITIVE : <nl> /* By PKCS # 11 v2 . 20 public key cannot have SENSITIVE attr TRUE */ <nl> check_attribute_buffer ( attr , sizeof ( CK_BBOOL ));
sm_encrypt_des_cbc3 ( struct sc_context * ctx , unsigned char * key , <nl>  <nl> * out_len = data_len ; <nl> * out = malloc ( data_len + 8 ); <nl> - if (* out == NULL ) <nl> + if (* out == NULL ) { <nl> + free ( data ); <nl> LOG_TEST_RET ( ctx , SC_ERROR_OUT_OF_MEMORY , " SM encrypt_des_cbc3 : failure "); <nl> + } <nl>  <nl> memcpy (& kk , key , 8 ); <nl> memcpy (& k2 , key + 8 , 8 );
static int part10_build_modify_pin_block ( struct sc_reader * reader , u8 * buf , siz <nl> pin_modify -> bInsertionOffsetNew = 0x00 ; <nl> } <nl>  <nl> - if (! data -> pin1 . min_length || ! data -> pin1 . max_length ) <nl> + if (!( data -> flags & SC_PIN_CMD_IMPLICIT_CHANGE ) <nl> + && (! data -> pin1 . min_length || ! data -> pin1 . max_length )) <nl> return SC_ERROR_INVALID_ARGUMENTS ; <nl>  <nl> tmp16 = ( data -> pin1 . min_length << 8 ) + data -> pin1 . max_length ;
coolkey_add_object ( coolkey_private_data_t * priv , unsigned long object_id , const <nl> new_object . id = object_id ; <nl> new_object . length = object_length ; <nl>  <nl> + /* The object ID needs to be unique */ <nl> + if ( coolkey_find_object_by_id (& priv -> objects_list , object_id ) != NULL ) { <nl> + return SC_ERROR_INTERNAL ; <nl> + } <nl> + <nl> if ( object_data ) { <nl> new_object . data = malloc ( object_length + add_v1_record ); <nl> if ( new_object . data == NULL ) {
iso7816_select_file ( struct sc_card * card , const struct sc_path * in_path , struct <nl> pathlen = in_path -> len ; <nl> pathtype = in_path -> type ; <nl>  <nl> + if ( file_out != NULL ) { <nl> + * file_out = NULL ; <nl> + } <nl> if ( in_path -> aid . len ) { <nl> if (! pathlen ) { <nl> memcpy ( path , in_path -> aid . value , in_path -> aid . len );
int sc_pkcs15_parse_tokeninfo ( sc_context_t * ctx , <nl> sprintf ( byte , "% 02X ", serial [ ii ]); <nl> strcat ( ti -> serial_number , byte ); <nl> } <nl> + sc_log ( ctx , " TokenInfo . serialNunmber '% s '", ti -> serial_number ); <nl> } <nl>  <nl> if ( ti -> manufacturer_id == NULL ) {
static int transform_pace_output ( u8 * rbuf , size_t rbuflen , <nl> if ( parsed + 2 > rbuflen ) <nl> return SC_ERROR_UNKNOWN_DATA_RECEIVED ; <nl> pace_output -> mse_set_at_sw1 = rbuf [ parsed + 0 ]; <nl> - pace_output -> mse_set_at_sw1 = rbuf [ parsed + 1 ]; <nl> + pace_output -> mse_set_at_sw2 = rbuf [ parsed + 1 ]; <nl> parsed += 2 ; <nl>  <nl> /* length_CardAccess */
sc_parse_ef_atr_content ( struct sc_card * card , unsigned char * buf , size_t buflen ) <nl>  <nl> category = * buf ; <nl>  <nl> + memset (& ef_atr , 0 , sizeof ( struct sc_ef_atr )); <nl> /* IAS / ECC specific : skip second ' zero ' byte */ <nl> if (*(++ buf ) == 0x00 ) <nl> ++ buf ;
static int tcos_decipher ( sc_card_t * card , const u8 * crgram , size_t crgram_len , <nl> apdu . data = sbuf ; <nl> apdu . lc = apdu . datalen = crgram_len + 1 ; <nl> sbuf [ 0 ] = tcos3 ? 0x00 : (( data -> pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1 ) ? 0x81 : 0x02 ); <nl> + if ( sizeof sbuf - 1 < crgram_len ) <nl> + return SC_ERROR_INVALID_ARGUMENTS ; <nl> memcpy ( sbuf + 1 , crgram , crgram_len ); <nl>  <nl> r = sc_transmit_apdu ( card , & apdu );
static int westcos_sign_decipher ( int mode , sc_card_t * card , <nl> BIO * mem = BIO_new ( BIO_s_mem ()); <nl> # endif <nl>  <nl> - if ( card == NULL ) <nl> + if ( card == NULL ) { <nl> + if ( keyfile ) <nl> + sc_file_free ( keyfile ); <nl> return SC_ERROR_INVALID_ARGUMENTS ; <nl> + } <nl> sc_debug ( card -> ctx , SC_LOG_DEBUG_NORMAL , <nl> " westcos_sign_decipher outlen =% d \ n ", outlen ); <nl> 
CK_RV attr_extract ( CK_ATTRIBUTE_PTR pAttr , void * ptr , size_t * sizep ) <nl> size = sizeof ( CK_KEY_TYPE ); break ; <nl> case CKA_PRIVATE : <nl> size = sizeof ( CK_BBOOL ); break ; <nl> + case CKA_CERTIFICATE_TYPE : <nl> + size = sizeof ( CKA_CERTIFICATE_TYPE ); break ; <nl> default : <nl> return CKR_FUNCTION_FAILED ; <nl> }
print_aligned_vertical ( const printTableContent * cont , FILE * fout ) <nl> if ( cont -> cells [ 0 ] == NULL && cont -> opt -> start_table && <nl> cont -> opt -> stop_table ) <nl> { <nl> - if (! opt_tuples_only ) <nl> + if (! opt_tuples_only && cont -> opt -> default_footer ) <nl> fprintf ( fout , _ ("( No rows )\ n ")); <nl> return ; <nl> }
pltcl_returnnext ( ClientData cdata , Tcl_Interp * interp , <nl> Datum retval ; <nl> bool isNull = false ; <nl>  <nl> + /* for paranoia ' s sake , check that tupdesc has exactly one column */ <nl> + if ( call_state -> ret_tupdesc -> natts != 1 ) <nl> + elog ( ERROR , " wrong result type supplied in return_next "); <nl> + <nl> retval = InputFunctionCall (& prodesc -> result_in_func , <nl> utf_u2e (( char *) Tcl_GetString ( objv [ 1 ])), <nl> prodesc -> result_typioparam ,
initHyperLogLog ( hyperLogLogState * cState , uint8 bwidth ) <nl> elog ( ERROR , " bit width must be between 4 and 16 inclusive "); <nl>  <nl> cState -> registerWidth = bwidth ; <nl> - cState -> nRegisters = 1 << bwidth ; <nl> + cState -> nRegisters = ( Size ) 1 << bwidth ; <nl> cState -> arrSize = sizeof ( uint8 ) * cState -> nRegisters + 1 ; <nl>  <nl> /*
void c_SimpleXMLElement :: t___construct ( CStrRef data , int64 options /* = 0 */, <nl> m_attributes = collect_attributes ( m_node , ns , is_prefix ); <nl> } <nl> } else { <nl> - raise_error (" String could not be parsed as XML "); <nl> + throw ( Object ) p_Exception ( NEW ( c_Exception )())-> create ( <nl> + " String could not be parsed as XML "); <nl> } <nl> } <nl> 
void Translator :: handleAssertionEffects ( Tracelet & t , <nl> */ <nl> if ( tas . m_changeSet . count ( dl -> location )) { <nl> auto const src = findInputSrc ( tas . m_t -> m_instrStream . last , dl ); <nl> - if ( src -> outputPredicted ) src -> outputPredicted = false ; <nl> + if ( src && src -> outputPredicted ) src -> outputPredicted = false ; <nl> } <nl> } <nl> 
static int exif_scan_JPEG_header ( image_info_type * ImageInfo ) { <nl> case M_SOF13 : <nl> case M_SOF14 : <nl> case M_SOF15 : <nl> + if (( itemlen - 2 ) < 6 ) { <nl> + return 0 ; <nl> + } <nl> + <nl> exif_process_SOFn ( Data , marker , & sof_info ); <nl> ImageInfo -> Width = sof_info . width ; <nl> ImageInfo -> Height = sof_info . height ;
static bool php_mb_parse_encoding ( const Variant & encoding , <nl> } <nl> if (! ret ) { <nl> if ( return_list && * return_list ) { <nl> - free (* return_list ); <nl> + req :: free (* return_list ); <nl> * return_list = nullptr ; <nl> } <nl> return_size = 0 ;
void LibEventTransport :: sendImpl ( const void * data , int size , int code , <nl> } else { <nl> if ( m_method != HEAD ) { <nl> evbuffer_add ( m_request -> output_buffer , data , size ); <nl> + } else { <nl> + char buf [ 11 ]; <nl> + snprintf ( buf , sizeof ( buf ), "% d ", size ); <nl> + addHeaderImpl (" Content - Length ", buf ); <nl> } <nl> m_server -> onResponse ( m_workerId , m_request , code ); <nl> m_sendEnded = true ;
TranslatorX64 :: smash ( X64Assembler & a , TCA src , TCA dest , bool isCall ) { <nl> */ <nl> CodeCursor cg ( a , src ); <nl> assert ( isSmashable ( a . code . frontier , kJmpLen )); <nl> - if ( dest > src && dest - src <= 7 ) { <nl> + if ( dest > src && dest - src <= kJmpLen ) { <nl> assert (! isCall ); <nl> a . emitNop ( dest - src ); <nl> } else if (! isCall ) {
SAL_DLLPUBLIC void SAL_CALL rtl_uString_newReplaceAllAsciiL ( <nl> @ param to pointer to the replacing substring ; must not be null and must <nl> point to memory of at least \ p toLength ASCII bytes <nl>  <nl> - @ param fromLength the length of the \ p to substring ; must be non - negative <nl> + @ param toLength the length of the \ p to substring ; must be non - negative <nl>  <nl> @ since LibreOffice 5 . 1 <nl> */
static LibreOfficeKit * lok_init_2 ( const char * install_path , const char * user_p <nl> LokHookFunction2 * pSym2 ; <nl>  <nl> dlhandle = lok_dlopen ( install_path , & imp_lib ); <nl> + if (! dlhandle ) <nl> + return NULL ; <nl>  <nl> pSym2 = ( LokHookFunction2 *) lok_dlsym ( dlhandle , " libreofficekit_hook_2 "); <nl> if (! pSym2 )
namespace XrdCl <nl>  <nl> XrdSysPwd pwdHandler ; <nl> passwd * pwd = pwdHandler . Get ( getuid () ); <nl> + if ( ! pwd ) return ; <nl> std :: string userPlugIns = pwd -> pw_dir ; <nl> userPlugIns += "/. xrootd / client . plugins . d "; <nl> ProcessConfigDir ( userPlugIns );
int XrdHttpReq :: PostProcessHTTPReq ( bool final_ ) { <nl>  <nl> } else <nl> for ( int i = 0 ; i < iovN ; i ++) { <nl> - prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len ); <nl> + if ( prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len )) return 1 ; <nl> writtenbytes += iovP [ i ]. iov_len ; <nl> } <nl> 
do { while ( optind < Argc && Legacy ( optind )) {} <nl> switch ( opC ) <nl> { case OpCksum : defCks ( optarg ); <nl> break ; <nl> + case OpCoerce : OpSpec |= DoCoerce ; <nl> + break ; <nl> case OpDebug : OpSpec |= DoDebug ; <nl> if (! a2i ( optarg , & Dlvl , 0 , 3 )) Usage ( 22 ); <nl> break ;
namespace XrdCl <nl> XRDCL_SMART_PTR_T < Message > msgPtr ( pResponse ); <nl> pResponse = 0 ; <nl>  <nl> - if ( rsp -> hdr . dlen < 4 ) <nl> + if ( rsp -> hdr . dlen <= 4 ) <nl> { <nl> log -> Error ( XRootDMsg , "[% s ] Got invalid redirect response .", <nl> pUrl . GetHostId (). c_str () );
void DoIt () { myMutex . Lock (); <nl> virtual void Finished ( XrdSsiRequest & rqstR , <nl> const XrdSsiRespInfo & rInfo , <nl> bool cancel = false ) <nl> - { myMutex . Lock (); <nl> + { UnBindRequest (); <nl> + myMutex . Lock (); <nl> if (! isActive ) delete this ; <nl> else { isActive = false ; <nl> myMutex . UnLock ();
static void do_chanuser_sync ( mychan_t * mc , chanuser_t * cu , chanacs_t * ca , <nl> } <nl>  <nl> try_kick ( chansvs . me -> me , mc -> chan , cu -> user , " You are not authorized to be on this channel "); <nl> + return ; <nl> } <nl> if ( fl & CA_AKICK && !( fl & CA_EXEMPT )) <nl> {
void write_accounts ( void ) <nl> for ( i = 0 ; i < 1024 ; i ++) { <nl> for ( nc = nclists [ i ]; nc ; nc = nc -> next ) { <nl> athemeflags = 0 ; <nl> + if ( nc -> aliases . count == 0 ) <nl> + continue ; <nl> na = nc -> aliases . list [ 0 ]; <nl> registered = na -> time_registered ; <nl> for ( ii = 1 ; ii < nc -> aliases . count ; ii ++)
void help_display_as_subcmd ( sourceinfo_t * si , service_t * service , const char * su <nl> if (! help_file ) <nl> { <nl> command_fail ( si , fault_nosuch_target , _ (" Could not get help file for \ 2 % s \ 2 ."), command ); <nl> + free ( ccommand ); <nl> return ; <nl> } <nl> 
static void can_register ( hook_channel_register_check_t * req ) <nl>  <nl> return_if_fail ( req != NULL ); <nl>  <nl> + /* no point in moderating registrations from those who have PRIV_CHAN_ADMIN since they can <nl> + * approve them anyway . -- nenolod <nl> + */ <nl> + if ( has_priv ( req -> si , PRIV_CHAN_ADMIN )) <nl> + return ; <nl> + <nl> req -> approved ++; <nl>  <nl> cs = csreq_create ( req -> name , entity ( req -> si -> smu ));
PHP_FUNCTION ( swoole_server_sendfile ) <nl> memcpy ( buffer , filename , send_data . info . len ); <nl> buffer [ send_data . info . len ] = 0 ; <nl> send_data . info . len ++; <nl> + send_data . length = 0 ; <nl>  <nl> send_data . data = buffer ; <nl> SW_CHECK_RETURN ( serv -> factory . finish (& serv -> factory , & send_data ));
PHP_MINFO_FUNCTION ( swoole ) <nl> # ifdef HAVE_PTHREAD_BARRIER <nl> php_info_print_table_row ( 2 , " pthread_barrier ", " enabled "); <nl> # endif <nl> - <nl> +# ifdef SW_USE_JEMALLOC <nl> + php_info_print_table_row ( 2 , " jemalloc ", " enabled "); <nl> +# endif <nl> +# ifdef SW_USE_TCMALLOC <nl> + php_info_print_table_row ( 2 , " tcmalloc ", " enabled "); <nl> +# endif <nl> +# ifdef SW_USE_HUGEPAGE <nl> + php_info_print_table_row ( 2 , " hugepage ", " enabled "); <nl> +# endif <nl> php_info_print_table_end (); <nl>  <nl> DISPLAY_INI_ENTRIES ();
class ModuleSSLGnuTLS : public Module <nl>  <nl> // This may be on a large ( once a day or week ) timer eventually . <nl> GenerateDHParams (); <nl> + <nl> + delete Conf ; <nl> } <nl>  <nl> void GenerateDHParams () <nl> class ModuleSSLGnuTLS : public Module <nl>  <nl> virtual ~ ModuleSSLGnuTLS () <nl> { <nl> - delete Conf ; <nl> gnutls_dh_params_deinit ( dh_params ); <nl> gnutls_certificate_free_credentials ( x509_cred ); <nl> gnutls_global_deinit ();
class SilenceMessage : public ClientProtocol :: Message <nl> : ClientProtocol :: Message (" SILENCE ") <nl> { <nl> PushParam ( mask ); <nl> - PushParamRef ( flags ); <nl> + PushParam ( flags ); <nl> } <nl> }; <nl> 
int InspIRCd :: Run () <nl>  <nl> int main ( int argc , char ** argv ) <nl> { <nl> - InspIRCd TittyBiscuits = new InspIRCd ( argc , argv ); <nl> + InspIRCd * TittyBiscuits = new InspIRCd ( argc , argv ); <nl> TittyBiscuits -> Run (); <nl> delete TittyBiscuits ; <nl> return 0 ;
class cmd_spynames : public command_t <nl> return CMD_FAILURE ; <nl> } <nl>  <nl> - if ( ServerInstance -> Parser -> LoopCall ( user , this , parameters , pcnt , 1 )) <nl> + if ( ServerInstance -> Parser -> LoopCall ( user , this , parameters , pcnt , 0 )) <nl> return CMD_FAILURE ; <nl>  <nl> c = ServerInstance -> FindChan ( parameters [ 0 ]);
bool LoadConf ( const char * filename , std :: stringstream * target , std :: stringstream <nl> std :: string newstuff = merge . str (); <nl> * target << newstuff ; <nl> } <nl> + else <nl> + { <nl> + // the error propogates up to its parent recursively <nl> + // causing the config reader to bail at the top level . <nl> + fclose ( conf ); <nl> + return false ; <nl> + } <nl> } <nl> else <nl> {
void NativeWindow :: TitleWasSet ( content :: NavigationEntry * entry , <nl> FOR_EACH_OBSERVER ( NativeWindowObserver , <nl> observers_ , <nl> OnPageTitleUpdated (& prevent_default , text )); <nl> - if (! prevent_default ) <nl> + if (! prevent_default && ! is_closed_ ) <nl> SetTitle ( text ); <nl> } <nl> 
NativeWindowWin :: NativeWindowWin ( content :: WebContents * web_contents , <nl> OnViewWasResized (); <nl>  <nl> if ( g_exe_icon == NULL ) <nl> - g_exe_icon = :: LoadImage ( GetModuleHandle ( NULL ), L " IDR_MAINFRAME ", <nl> + g_exe_icon = :: LoadImage ( GetModuleHandle ( NULL ), MAKEINTRESOURCE ( 1 ), <nl> IMAGE_ICON , 0 , 0 , 0 ); <nl> :: SendMessage ( window_ -> GetNativeWindow (), <nl> WM_SETICON ,
gfx :: Image Clipboard :: ReadImage ( mate :: Arguments * args ) { <nl> void Clipboard :: WriteImage ( const gfx :: Image & image , mate :: Arguments * args ) { <nl> ui :: ScopedClipboardWriter writer ( GetClipboardType ( args )); <nl> SkBitmap bmp ; <nl> + // TODO ( ferreus ): Replace with sk_tools_utils :: copy_to ( chrome60 ) <nl> if ( image . AsBitmap (). deepCopyTo (& bmp )) { <nl> writer . WriteImage ( bmp ); <nl> } else {
Archive ::~ Archive () { <nl> file_ . TakePlatformFile (); <nl> } <nl> # endif <nl> - base :: PostTaskWithTraits ( <nl> - FROM_HERE , <nl> - { base :: MayBlock (), base :: TaskPriority :: BACKGROUND , <nl> - base :: TaskShutdownBehavior :: CONTINUE_ON_SHUTDOWN }, <nl> - base :: Bind ([]( base :: File file ) { file . Close (); }, Passed (& file_ ))); <nl> + base :: ThreadRestrictions :: ScopedAllowIO allow_io ; <nl> + file_ . Close (); <nl> } <nl>  <nl> bool Archive :: Init () {
/* unzip . c -- IO for uncompress . zip files using zlib <nl> + <nl> + Modified for Quake III Arena to use the Z_Malloc () memory pool ; <nl> + this means a system copy of minizip is not a suitable replacement . <nl> + <nl> + Based on minizip : <nl> + <nl> Version 1 . 01e , February 12th , 2005 <nl>  <nl> Copyright ( C ) 1998 - 2005 Gilles Vollant
void AsmCall ( void ) { <nl> " doret : \ n \ t " \ <nl> " ret \ n \ t " \ <nl> : "= rm " ( callSyscallNum ), "= rm " ( callProgramStack ), "= rm " ( callOpStack ) \ <nl> - : " rm " ( instructionPointers ) \ <nl> + : " m " ( instructionPointers ) \ <nl> : " ax ", " di ", " si ", " cx " \ <nl> ); <nl> }
void compute_subject_downtime_times ( time_t start_time , time_t end_time , avail_su <nl> } <nl> saved_status = temp_as -> entry_type ; <nl> saved_stamp = temp_as -> time_stamp ; <nl> + <nl> + /* check if first time is before schedule downtime */ <nl> + if ( saved_stamp < start_time ) <nl> + saved_stamp = start_time ; <nl> + <nl> } <nl> } <nl> 
rtadv_read ( struct thread * thread ) <nl> /* Register myself . */ <nl> rtadv_event ( zvrf , RTADV_READ , sock ); <nl>  <nl> - len = rtadv_recv_packet ( sock , buf , BUFSIZ , & from , & ifindex , & hoplimit ); <nl> + len = rtadv_recv_packet ( sock , buf , sizeof ( buf ), & from , & ifindex , & hoplimit ); <nl>  <nl> if ( len < 0 ) <nl> {
static struct static_route * static_route_alloc () <nl>  <nl> s_route = XCALLOC ( MTYPE_PIM_STATIC_ROUTE , sizeof (* s_route )); <nl> if (! s_route ) { <nl> - zlog_err (" PIM XCALLOC (% u ) failure ", sizeof (* s_route )); <nl> + zlog_err (" PIM XCALLOC (% zu ) failure ", sizeof (* s_route )); <nl> return 0 ; <nl> } <nl> return s_route ;
ospf_recv_packet ( int fd , struct interface ** ifp , struct stream * ibuf ) <nl>  <nl> ip_len = iph -> ip_len ; <nl>  <nl> -# if ! defined ( GNU_LINUX ) && ( OpenBSD < 200311 ) <nl> +# if ! defined ( GNU_LINUX ) && ( OpenBSD < 200311 ) && ( __FreeBSD_version < 1000000 ) <nl> /* <nl> * Kernel network code touches incoming IP header parameters , <nl> * before protocol specific processing .
void ospf6_spf_reason_string ( unsigned int reason , char * buf , int size ) <nl> if (! buf ) <nl> return ; <nl>  <nl> - for ( bit = 0 ; bit <= ( sizeof ( ospf6_spf_reason_str ) / sizeof ( char *)); bit ++) <nl> + for ( bit = 0 ; bit < array_size ( ospf6_spf_reason_str ); bit ++) <nl> { <nl> if (( reason & ( 1 << bit )) && ( len < size )) <nl> {
bgp_address_del ( struct prefix * p ) <nl> tmp . addr = p -> u . prefix4 ; <nl>  <nl> addr = hash_lookup ( bgp_address_hash , & tmp ); <nl> + /* may have been deleted earlier by bgp_interface_down () */ <nl> + if ( addr == NULL ) <nl> + return ; <nl> + <nl> addr -> refcnt --; <nl>  <nl> if ( addr -> refcnt == 0 )
int transform_save ( struct augeas * aug , struct tree * xfm , <nl> goto done ; <nl> } <nl>  <nl> + text = append_newline ( text , strlen ( text )); <nl> + <nl> // FIXME : We might have to create intermediary directories <nl> // to be able to write augnew , but we have no idea what permissions <nl> // etc . they should get . Just the process default ?
static const char * init_root ( const char * root0 ) { <nl> if ( root0 == NULL ) <nl> root0 = "/"; <nl> root = strdup ( root0 ); <nl> + if ( root == NULL ) <nl> + return NULL ; <nl> if ( root [ strlen ( root )- 1 ] != SEP ) { <nl> if ( REALLOC_N ( root , strlen ( root ) + 2 ) == - 1 ) { <nl> FREE ( root );
int main ( int argc , char ** argv ) { <nl> } <nl> } <nl> putchar ('\ n '); <nl> + free ( rx ); <nl> } <nl>  <nl> return 0 ;
# include " fish_version . h " <nl>  <nl> /** Command used to start fishd */ <nl> -# define FISHD_CMD L " fishd ^ / tmp / fishd . log .% s " <nl> +# define FISHD_CMD L " fishd ^ / dev / null " <nl>  <nl> // Version for easier debugging <nl> //# define FISHD_CMD L " fishd "
universal_notifier_t :: notifier_strategy_t universal_notifier_t :: resolve_default_ <nl> } <nl> # if FISH_NOTIFYD_AVAILABLE <nl> return strategy_notifyd ; <nl> +# elif defined ( __CYGWIN__ ) <nl> + return strategy_shmem_polling ; <nl> # else <nl> return strategy_named_pipe ; <nl> # endif
static int indent ( string_buffer_t * out , wchar_t * in , int flags ) <nl> { <nl> indent --; <nl> } <nl> + /* case should have the same indent level as switch */ <nl> + else if ( wcscmp ( unesc , L " case " ) == 0 ) <nl> + { <nl> + indent --; <nl> + } <nl> else if ( wcscmp ( unesc , L " end " ) == 0 ) <nl> { <nl> indent --;
namespace tnt <nl> explicit HttpReply ( std :: ostream & s , bool sendStatusLine = true ); <nl>  <nl> void setContentType ( const char * t ) { setHeader ( httpheader :: contentType , t ); } <nl> + void setContentType ( const std :: string & t ) { setHeader ( httpheader :: contentType , t ); } <nl> const char * getContentType () const { return getHeader ( httpheader :: contentType ); } <nl>  <nl> void setHeadRequest ( bool sw = true ) { headRequest = sw ; }
namespace tnt <nl> { <nl> log_debug (" worker - process "); <nl>  <nl> + stop = false ; <nl> + <nl> if ( listeners . empty ()) <nl> { <nl> unsigned short int port = ( getuid () == 0 ? 80 : 8000 );
ecall ( mrb_state * mrb , int i ) <nl> mrb_value * self = mrb -> c -> stack ; <nl> struct RObject * exc ; <nl>  <nl> + if ( i < 0 ) return ; <nl> p = mrb -> c -> ensure [ i ]; <nl> if (! p ) return ; <nl> if ( mrb -> c -> ci -> eidx > i )
mrb_init_class ( mrb_state * mrb ) <nl> mrb_define_method ( mrb , mod , " define_method ", mod_define_method , ARGS_REQ ( 1 )); <nl>  <nl> mrb_define_method ( mrb , mod , "===", mrb_mod_eqq , ARGS_REQ ( 1 )); <nl> + mrb_undef_method ( mrb , cls , " append_features "); <nl> }
mrb_str_inspect ( mrb_state * mrb , mrb_value str ) <nl> buf [ i ] = p [ i ]; <nl> } <nl> mrb_str_cat ( mrb , result , buf , clen ); <nl> - p += clen ; <nl> + p += clen - 1 ; <nl> continue ; <nl> } <nl> # endif
void restart_connections_by_peer ( struct connection * const c ) <nl> host_addr = c -> spd . that . host_addr ; <nl> } <nl>  <nl> - if ( hp_next == NULL ) { <nl> + if ( c_kind == CK_INSTANCE && hp_next == NULL ) { <nl> + /* in simple cases this is a dangling hp */ <nl> DBG ( DBG_CONTROL , <nl> DBG_log (" no connection to restart after termination ")); <nl> } else {
aggr_inI1_outR1_continue1 ( struct pluto_crypto_req_cont * pcrc <nl> /* unpack first calculation */ <nl> unpack_KE ( st , r , & st -> st_gr ); <nl>  <nl> + /* unpack nonce too */ <nl> + unpack_nonce (& st -> st_nr , r ); <nl> + <nl> /* NOTE : the " r " reply will get freed by our caller */ <nl>  <nl> /* set up second calculation */
whack_handle ( int whackctlfd ) <nl> close ( whackfd ); <nl> return ; <nl> } <nl> + memset (& msg , 0 , sizeof ( msg )); <nl> n = read ( whackfd , & msg , sizeof ( msg )); <nl> if ( n <= 0 ) <nl> {
static bool setup_half_ipsec_sa ( struct state * st , bool inbound ) <nl> case IKEv2_ENCR_AES_GCM_8 : <nl> case IKEv2_ENCR_AES_GCM_12 : <nl> case IKEv2_ENCR_AES_GCM_16 : <nl> + /* keymat contains 4 bytes of salt */ <nl> + enc_key_len += AES_GCM_SALT_BYTES ; <nl> + break ; <nl> case IKEv2_ENCR_AES_CCM_8 : <nl> case IKEv2_ENCR_AES_CCM_12 : <nl> case IKEv2_ENCR_AES_CCM_16 : <nl> - /* keymat contains 4 bytes of salt */ <nl> + /* keymat contains 3 bytes of salt */ <nl> enc_key_len += AES_CCM_SALT_BYTES ; <nl> break ; <nl> }
 <nl> # define LSW_NFDBITS ( 8 * sizeof ( long int )) <nl> # define LSW_FDELT ( d ) (( d ) / LSW_NFDBITS ) <nl> -# define LSW_FDMASK ( d ) (( long int ) 1 << (( d ) % LSW_NFDBITS )) <nl> +# define LSW_FDMASK ( d ) (( long int ) ( 1UL << (( d ) % LSW_NFDBITS ))) <nl> # define LSW_FD_SETCOUNT (( LSW_FD_SETSIZE + LSW_NFDBITS - 1 ) / LSW_NFDBITS ) <nl>  <nl> typedef struct {
size_t format_end ( char * buf , <nl> send_cert = "+ S = C "; <nl> break ; <nl> } <nl> - p = add_str ( endopts , sizeof ( endopts ), p , send_cert ); <nl> + add_str ( endopts , sizeof ( endopts ), p , send_cert ); <nl> } <nl> } <nl> 
static lsw_cert_ret pluto_process_certs ( struct state * st , <nl> add_crl_fetch_request_nss (& fdn , end_cert_dp ); <nl> wake_fetch_thread ( __FUNCTION__ ); <nl> } <nl> + DBGF ( DBG_X509 , " releasing end_cert_dp sent to crl fetch "); <nl> + free_generalNames ( end_cert_dp , false /* shallow */); <nl> } <nl> # endif <nl> 
ipsec_xmit_sanity_check_dev ( struct ipsec_xmit_state * ixs ) <nl> } <nl>  <nl> ixs -> physmtu = ixs -> physdev -> mtu ; <nl> - ixs -> cur_mtu = ixs -> physdev -> mtu ; <nl> + ixs -> cur_mtu = ixs -> dev -> mtu ; <nl>  <nl> ixs -> stats = ( struct net_device_stats *) &( ixs -> prv -> mystats ); <nl> 
static void retransmit_v1_msg ( struct state * st ) <nl> delay_ms = c -> r_interval ; <nl> } <nl>  <nl> - if ( delay_ms != 0 ) { <nl> + if ( delay_ms != 0 ) <nl> delay_ms = retrans_delay ( st , delay_ms ); <nl>  <nl> if ( delay_ms != 0 ) {
parse ( cherokee_handler_ssi_t * hdl , <nl>  <nl> ret = parse ( hdl , & file_content , out ); <nl> if ( unlikely ( ret != ret_ok )) { <nl> + cherokee_buffer_mrproper (& file_content ); <nl> return ret_error ; <nl> } <nl> 
cherokee_config_node_add ( cherokee_config_node_t * conf , const char * key , cheroke <nl> } <nl>  <nl> if ( final ) { <nl> + cherokee_buffer_clean (& child -> val ); <nl> cherokee_buffer_add_buffer (& child -> val , val ); <nl> } <nl> 
entry_free ( cherokee_nonce_table_t * nonces , <nl> { <nl> cherokee_list_del (& entry -> listed ); <nl> cherokee_avl_del (& nonces -> table , & entry -> nonce , NULL ); <nl> + cherokee_buffer_mrproper (& entry -> nonce ); <nl> free ( entry ); <nl> } <nl> 
local_file_exists ( cherokee_rule_extensions_t * rule , <nl> ret = cherokee_io_stat ( srv -> iocache , tmp , rule -> use_iocache , <nl> & nocache_info , & io_entry , & info ); <nl>  <nl> - is_file = S_ISREG ( info -> st_mode ); <nl> + if ( ret == ret_ok ) { <nl> + is_file = S_ISREG ( info -> st_mode ); <nl> + } <nl>  <nl> if ( io_entry ) { <nl> cherokee_iocache_entry_unref (& io_entry );
skip_more_whitespace_and_comments : <nl>  <nl> /* LHS Keysym . */ <nl> if ( chr ( s , '<')) { <nl> - while ( peek ( s ) != '>' && ! eol ( s )) <nl> + while ( peek ( s ) != '>' && ! eol ( s ) && ! eof ( s )) <nl> buf_append ( s , next ( s )); <nl> if (! chr ( s , '>')) { <nl> scanner_err ( s , " unterminated keysym literal ");
bool <nl> map_file ( FILE * file , char ** string_out , size_t * size_out ) <nl> { <nl> struct stat stat_buf ; <nl> - const int fd = fileno ( file ); <nl> + int fd ; <nl> char * string ; <nl>  <nl> /* Make sure to keep the errno on failure ! */ <nl> + fd = fileno ( file ); <nl> + if ( fd < 0 ) <nl> + return false ; <nl>  <nl> if ( fstat ( fd , & stat_buf ) != 0 ) <nl> return false ;
LookupModMask ( struct xkb_context * ctx , const void * priv , xkb_atom_t field , <nl> return false ; <nl>  <nl> str = xkb_atom_text ( ctx , field ); <nl> + if (! str ) <nl> + return false ; <nl>  <nl> if ( istreq ( str , " all ")) { <nl> * val_rtrn = MOD_REAL_MASK_ALL ;
ExprAppendMultiKeysymList ( ExprDef * expr , ExprDef * append ) <nl> darray_append ( expr -> keysym_list . symsNumEntries , numEntries ); <nl> darray_concat ( expr -> keysym_list . syms , append -> keysym_list . syms ); <nl>  <nl> - FreeStmt (( ParseCommon *) & append ); <nl> + FreeStmt (( ParseCommon *) append ); <nl>  <nl> return expr ; <nl> }
ExprResolveLhs ( struct xkb_context * ctx , const ExprDef * expr , <nl> * elem_rtrn = NULL ; <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> ident . ident ); <nl> * index_rtrn = NULL ; <nl> - return true ; <nl> + return (* field_rtrn != NULL ); <nl> case EXPR_FIELD_REF : <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> field_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> field_ref . field );
frexpf ( float x , int * eptr ) <nl> } <nl> * eptr += ( ix >> 23 )- 126 ; <nl> hx = ( hx & 0x807fffff )| 0x3f000000 ; <nl> - *( int *)& x = hx ; <nl> + SET_FLOAT_WORD ( x , hx ); <nl> return x ; <nl> }
extern char * ptsname ( int ); <nl> extern int ptsname_r ( int , char *, size_t ); <nl> extern int getpt ( void ); <nl>  <nl> - static __inline__ int grantpt ( int __fd ) <nl> + static __inline__ int grantpt ( int __fd __attribute (( unused ))) <nl> { <nl> ( void ) __fd ; <nl> return 0 ; /* devpts does this all for us ! */
void * mmap64 ( void * addr , size_t size , int prot , int flags , int fd , off64_t offse <nl>  <nl> // prevent allocations large enough for ` end - start ` to overflow <nl> size_t rounded = BIONIC_ALIGN ( size , PAGE_SIZE ); <nl> - if ( rounded < size || size > PTRDIFF_MAX ) { <nl> + if ( rounded < size || rounded > PTRDIFF_MAX ) { <nl> errno = ENOMEM ; <nl> return MAP_FAILED ; <nl> }
void get_malloc_leak_info ( uint8_t ** info , size_t * overallSize , <nl> // debug_log (" info = % p \ n ", info ); <nl> if (* info == NULL ) { <nl> * overallSize = 0 ; <nl> - goto done ; <nl> + goto out_nomem_info ; <nl> } <nl>  <nl> // debug_log (" sorting list ...\ n "); <nl> void get_malloc_leak_info ( uint8_t ** info , size_t * overallSize , <nl> head += * infoSize ; <nl> } <nl>  <nl> + out_nomem_info : <nl> dlfree ( list ); <nl>  <nl> done :
struct android_namespace_link_t { <nl> } <nl>  <nl> bool is_accessible ( const char * soname ) const { <nl> + if ( soname == nullptr ) { <nl> + return false ; <nl> + } <nl> return allow_all_shared_libs_ || shared_lib_sonames_ . find ( soname ) != shared_lib_sonames_ . end (); <nl> } <nl> 
namespace Js <nl> const EquivalentPropertyEntry * refInfo = & properties [ pi ]; <nl> if (! this -> NullTypeHandlerBase :: IsObjTypeSpecEquivalent ( type , refInfo )) <nl> { <nl> + failedPropertyIndex = pi ; <nl> return false ; <nl> } <nl> }
HRESULT ChakraRTInterface :: ParseConfigFlags () <nl> else <nl> { <nl> hr = WideStringToNarrowDynamic ( fileNameWide , & m_argInfo -> filename ); <nl> + SysFreeString ( fileNameWide ); <nl> if ( FAILED ( hr )) <nl> { <nl> Assert ( hr == E_OUTOFMEMORY );
namespace Js <nl> Output :: Print ( _u (" ObjectHeaderInlining : Moving inlined properties out of the object header .\ n ")); <nl> Output :: Flush (); <nl> } <nl> - Var * const newInlineSlots = reinterpret_cast < Var *>( object + 1 ); <nl> + Field ( Var ) * const newInlineSlots = reinterpret_cast < Field ( Var ) *>( object + 1 ); <nl> PropertyIndex i = newInlineSlotCapacity ; <nl> do <nl> {
File * FileRef :: create ( FileName fileName , bool readAudioProperties , <nl> File * file = new Ogg :: FLAC :: File ( fileName , readAudioProperties , audioPropertiesStyle ); <nl> if ( file -> isValid ()) <nl> return file ; <nl> + delete file ; <nl> return new Ogg :: Vorbis :: File ( fileName , readAudioProperties , audioPropertiesStyle ); <nl> } <nl> if ( ext == " FLAC ")
time_t raptor_parse_date ( char * p , time_t * now ); <nl> int raptor_stringbuffer_append_turtle_string ( raptor_stringbuffer * stringbuffer , unsigned char * text , size_t len , int delim , raptor_simple_message_handler error_handler , void * error_data ); <nl>  <nl>  <nl> +/* raptor_xsd . c */ <nl> + raptor_identifier * raptor_new_identifier_from_double ( double d ); <nl> + <nl> /* end of RAPTOR_INTERNAL */ <nl> # endif <nl> 
static MagickBooleanType WriteOnePNGImage ( MngInfo * mng_info , <nl> image_colors =( int ) image -> colors ; <nl> image_matte = image -> alpha_trait == BlendPixelTrait ? MagickTrue : MagickFalse ; <nl>  <nl> - if ( mng_info -> write_png_colortype > 4 ) <nl> + if ( mng_info -> write_png_colortype < 5 ) <nl> mng_info -> IsPalette = image -> storage_class == PseudoClass && <nl> image_colors <= 256 && image -> colormap != NULL ; <nl> else
static Image * ReadTIFFImage ( const ImageInfo * image_info , <nl> /* <nl> Convert stripped TIFF image . <nl> */ <nl> - extent = 4 *( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> + extent = 4 *(( image -> depth + 7 )/ 8 )*( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> strip_pixels =( unsigned char *) AcquireQuantumMemory ( extent , <nl> sizeof (* strip_pixels )); <nl> if ( strip_pixels == ( unsigned char *) NULL )
static Image * ReadMPCImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> if ( LocaleCompare ( keyword ," number - meta - channels ") == 0 ) <nl> { <nl> image -> number_meta_channels = StringToUnsignedLong ( options ); <nl> + if ( image -> number_meta_channels > MaxPixelChannels ) <nl> + ThrowReaderException ( CorruptImageError , <nl> + " ImproperImageHeader "); <nl> break ; <nl> } <nl> break ;
static MagickBooleanType DrawStrokePolygon ( Image * image , <nl> for ( p = primitive_info ; p -> primitive != UndefinedPrimitive ; p += p -> coordinates ) <nl> { <nl> stroke_polygon = TraceStrokePolygon ( draw_info , p ); <nl> + if ( stroke_polygon == ( PrimitiveInfo *) NULL ) <nl> + { <nl> + status = 0 ; <nl> + break ; <nl> + } <nl> status &= DrawPolygonPrimitive ( image , clone_info , stroke_polygon , exception ); <nl> if ( status == 0 ) <nl> break ;
static void WriteTo8BimProfile ( Image * image , const char * name , <nl> count =( ssize_t ) value ; <nl> if (( count & 0x01 ) != 0 ) <nl> count ++; <nl> - if (( p > ( datum + length - count )) || ( count > ( ssize_t ) length )) <nl> + if (( count < 0 ) || ( p > ( datum + length - count )) || <nl> + ( count > ( ssize_t ) length )) <nl> break ; <nl> if ( id != profile_id ) <nl> p += count ;
void Magick :: Image :: read ( MagickCore :: Image * image , <nl> if (! quiet ()) <nl> throwExceptionExplicit ( MagickCore :: ImageWarning , <nl> " No image was loaded ."); <nl> + return ; <nl> } <nl> ThrowImageException ; <nl> }
MagickExport Image * WaveletDenoiseImage ( const Image * image , <nl> difference ; <nl>  <nl> difference = pixels [ low_pass ]- pixels [ high_pass ]; <nl> - pixels [ i ]+= copysignf ( fmaxf ( fabsf ( difference )- magnitude , 0 . 0f ), <nl> - difference ); <nl> + pixels [ i ]+= copysignf ( fmaxf ( fabsf ( difference )- magnitude - <nl> + softness * magnitude , 0 . 0f ), difference ); <nl> } <nl> } <nl> /*
static void CL_API_CALL DestroyMagickCLCacheInfoAndPixels ( <nl> } <nl> } <nl> pixels = info -> pixels ; <nl> + RelinquishMagickResource ( MemoryResource , info -> length ); <nl> DestroyMagickCLCacheInfo ( info ); <nl> ( void ) RelinquishAlignedMemory ( pixels ); <nl> }
static MagickBooleanType ReadPSDChannelPixels ( Image * image , <nl> SetPixelIndex ( image ,((( unsigned char ) pixel ) & <nl> ( 0x01 << ( 7 - bit ))) != 0 ? 0 : 255 , q ); <nl> SetPixelViaPixelInfo ( image , image -> colormap +( ssize_t ) <nl> - GetPixelIndex ( image , q ), q ); <nl> + ConstrainColormapIndex ( image , GetPixelIndex ( image , q ), <nl> + exception ), q ); <nl> q += GetPixelChannels ( image ); <nl> x ++; <nl> }
static Image * ReadOneJNGImage ( MngInfo * mng_info , <nl> ( void ) LogMagickEvent ( CoderEvent , GetMagickModule (), <nl> " Copying JDAT chunk data to color_blob ."); <nl>  <nl> - ( void ) WriteBlob ( color_image , length , chunk ); <nl> - <nl> if ( length != 0 ) <nl> - chunk =( unsigned char *) RelinquishMagickMemory ( chunk ); <nl> + { <nl> + ( void ) WriteBlob ( color_image , length , chunk ); <nl> + chunk =( unsigned char *) RelinquishMagickMemory ( chunk ); <nl> + } <nl>  <nl> continue ; <nl> }
MagickExport Image * CloneImage ( const Image * image , const size_t columns , <nl> sizeof (* clone_image -> colormap )); <nl> if ( clone_image -> colormap == ( PixelInfo *) NULL ) <nl> { <nl> - clone_image = DestroyImage ( clone_image ); <nl> + image =( Image *) RelinquishMagickMemory ( image ); <nl> ThrowImageException ( ResourceLimitError ," MemoryAllocationFailed "); <nl> } <nl> ( void ) CopyMagickMemory ( clone_image -> colormap , image -> colormap , length *
static Image * ReadPSDImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> image -> alpha_trait = UndefinedPixelTrait ; <nl> } <nl> } <nl> + if (( image -> depth == 1 ) && ( image -> storage_class != PseudoClass )) <nl> + ThrowReaderException ( CorruptImageError , " ImproperImageHeader "); <nl> has_merged_image = MagickTrue ; <nl> length = ReadBlobMSBLong ( image ); <nl> if ( length != 0 )
static inline MagickSizeType GetPSDSize ( const PSDInfo * psd_info , Image * image ) <nl> static inline size_t GetPSDRowSize ( Image * image ) <nl> { <nl> if ( image -> depth == 1 ) <nl> - return (( image -> columns + 7 )/ 8 ); <nl> + return ((( image -> columns + 7 )/ 8 )* GetPSDPacketSize ( image )); <nl> else <nl> return ( image -> columns * GetPSDPacketSize ( image )); <nl> }
private : <nl> } <nl> } <nl>  <nl> + UPDATE_TRACE_POINT (); <nl> APR_BRIGADE_INSERT_TAIL ( bb , b ); <nl>  <nl> b = apr_bucket_eos_create ( r -> connection -> bucket_alloc );
void CCountryFlags :: OnInit () <nl> CCountryFlag DummyEntry ; <nl> DummyEntry . m_CountryCode = - 1 ; <nl> DummyEntry . m_Texture = - 1 ; <nl> + mem_zero ( DummyEntry . m_aCountryCodeString , sizeof ( DummyEntry . m_aCountryCodeString )); <nl> m_aCountryFlags . add ( DummyEntry ); <nl> } <nl> }
void CServer :: ProcessClientPacket ( CNetChunk * pPacket ) <nl> return ; <nl>  <nl> int Chunk = Unpacker . GetInt (); <nl> - int ChunkSize = 1024 - 128 ; <nl> - int Offset = Chunk * ChunkSize ; <nl> + unsigned int ChunkSize = 1024 - 128 ; <nl> + unsigned int Offset = Chunk * ChunkSize ; <nl> int Last = 0 ; <nl>  <nl> // drop faulty map data requests
evrpc_resume_request ( void * vbase , void * ctx , enum EVRPC_HOOK_RESULT res ) <nl>  <nl> (* pause -> cb )( pause -> ctx , res ); <nl> TAILQ_REMOVE ( head , pause , next ); <nl> + mm_free ( pause ); <nl> return ( 0 ); <nl> } <nl> 
evhttp_send_reply_start ( struct evhttp_request * req , int code , <nl> evhttp_add_header ( req -> output_headers , " Transfer - Encoding ", <nl> " chunked "); <nl> req -> chunked = 1 ; <nl> + } else { <nl> + req -> chunked = 0 ; <nl> } <nl> evhttp_make_header ( req -> evcon , req ); <nl> evhttp_write_buffer ( req -> evcon , NULL , NULL );
epoll_init ( struct event_base * base ) <nl> fd = epollop -> timerfd = timerfd_create ( CLOCK_MONOTONIC , TFD_NONBLOCK | TFD_CLOEXEC ); <nl> if ( epollop -> timerfd >= 0 ) { <nl> struct epoll_event epev ; <nl> + memset (& epev , 0 , sizeof ( epev )); <nl> epev . data . fd = epollop -> timerfd ; <nl> epev . events = EPOLLIN ; <nl> if ( epoll_ctl ( epollop -> epfd , EPOLL_CTL_ADD , fd , & epev ) < 0 ) {
struct evbuffer_overlapped { <nl> static inline struct evbuffer_overlapped * <nl> upcast_evbuffer ( struct evbuffer * buf ) <nl> { <nl> - if (! buf || buf -> is_overlapped ) <nl> + if (! buf || ! buf -> is_overlapped ) <nl> return NULL ; <nl> return EVUTIL_UPCAST ( buf , struct evbuffer_overlapped , buffer ); <nl> }
encode_int_internal ( ev_uint8_t * data , ev_uint32_t number ) <nl> { <nl> int off = 1 , nibbles = 0 ; <nl>  <nl> - memset ( data , 0 , sizeof ( uint32_t )+ 1 ); <nl> + memset ( data , 0 , sizeof ( ev_uint32_t )+ 1 ); <nl> while ( number ) { <nl> if ( off & 0x1 ) <nl> data [ off / 2 ] = ( data [ off / 2 ] & 0xf0 ) | ( number & 0x0f );
event_set ( struct event * ev , int fd , short events , <nl> ev -> ev_arg = arg ; <nl> ev -> ev_fd = fd ; <nl> ev -> ev_events = events ; <nl> + ev -> ev_res = 0 ; <nl> ev -> ev_flags = EVLIST_INIT ; <nl> ev -> ev_ncalls = 0 ; <nl> ev -> ev_pncalls = NULL ;
int Ftp :: Do () <nl> if ( state != CONNECTED_STATE || Error ()) <nl> return MOVED ; <nl>  <nl> - if ( expect -> Has ( Expect :: FEAT )) <nl> + if ( expect -> Has ( Expect :: FEAT ) || conn -> quit_sent ) <nl> goto usual_return ; <nl>  <nl> # if USE_SSL
int FileCopy :: Do () <nl> } <nl> if ( get -> Error () && get -> Size ()== 0 ) <nl> { <nl> - put -> PutEOF (); <nl> - Roll ( put ); <nl> + if ( put -> GetPos ()> 0 ) <nl> + { <nl> + put -> PutEOF (); <nl> + Roll ( put ); <nl> + } <nl> get_error : <nl> SetError ( get -> ErrorText ()); <nl> return MOVED ;
public : <nl> if ( o || ! n ) <nl> return ; <nl> downloader = new const TorrentPeer *[ blk_count ]; <nl> - for ( int i = 0 ; i < blk_count ; i ++) <nl> + for ( unsigned i = 0 ; i < blk_count ; i ++) <nl> downloader [ i ]= 0 ; <nl> } <nl> const TorrentPeer *& d = downloader [ block ];
_win32_read_file ( void * state , void * data , zip_uint64_t len , zip_source_cmd_t cmd <nl> zip_error_set (& ctx -> error , ZIP_ER_RENAME , _zip_set_win32_error ( GetLastError (), & ctx -> win32err )); <nl> return - 1 ; <nl> } <nl> + free ( ctx -> tmpname ); <nl> + ctx -> tmpname = NULL ; <nl> return 0 ; <nl> } <nl> 
int uv__stdio_create ( uv_loop_t * loop , <nl>  <nl> case FILE_TYPE_PIPE : <nl> CHILD_STDIO_CRT_FLAGS ( buffer , i ) = FOPEN | FPIPE ; <nl> + break ; <nl>  <nl> case FILE_TYPE_CHAR : <nl> case FILE_TYPE_REMOTE :
static void uv_loop_init ( uv_loop_t * loop ) { <nl> loop -> active_udp_streams = 0 ; <nl>  <nl> loop -> last_err = uv_ok_ ; <nl> + <nl> + memset (& loop -> counters , 0 , sizeof loop -> counters ); <nl> } <nl>  <nl> 
int uv_run ( uv_loop_t * loop , uv_run_mode mode ) { <nl>  <nl> uv__update_time ( loop ); <nl> uv__run_timers ( loop ); <nl> + uv__run_pending ( loop ); <nl> uv__run_idle ( loop ); <nl> uv__run_prepare ( loop ); <nl> - uv__run_pending ( loop ); <nl>  <nl> timeout = 0 ; <nl> if (( mode & UV_RUN_NOWAIT ) == 0 )
int uv_write ( uv_write_t * req , uv_stream_t * stream , uv_buf_t bufs [], int bufcnt , <nl> req -> type = UV_WRITE ; <nl> ngx_queue_init (& req -> queue ); <nl>  <nl> - if ( bufcnt < UV_REQ_BUFSML_SIZE ) { <nl> + if ( bufcnt <= UV_REQ_BUFSML_SIZE ) { <nl> req -> bufs = req -> bufsml ; <nl> } <nl> else {
static int uv__ifaddr_exclude ( struct ifaddrs * ent ) { <nl> return 1 ; <nl> if ( ent -> ifa_addr == NULL ) <nl> return 1 ; <nl> - if ( ent -> ifa_addr -> sa_family == PF_PACKET ) <nl> + if ( ent -> ifa_addr -> sa_family != AF_INET && <nl> + ent -> ifa_addr -> sa_family != AF_INET6 ) <nl> return 1 ; <nl> return 0 ; <nl> }
start : <nl> } <nl>  <nl> if ( n < 0 ) { <nl> - if ( errno != EAGAIN ) { <nl> + if ( errno != EAGAIN && errno != EWOULDBLOCK ) { <nl> /* Error */ <nl> req -> error = errno ; <nl> stream -> write_queue_size -= uv__write_req_size ( req );
static const char * static_camera_list [] = { <nl> " FujiFilm X - E1 ", <nl> " FujiFilm X - E2 ", <nl> " FujiFilm X - E2S ", <nl> + " FujiFilm X - E3 ", <nl> " FujiFilm X - M1 ", <nl> " FujiFilm XF1 ", <nl> " FujiFilm X - T1 ",
int MK_EXPORT _mkp_network_io_create_socket ( int domain , int type , int protocol ); <nl> int MK_EXPORT _mkp_network_io_bind ( int socket_fd , const struct sockaddr * addr , <nl> socklen_t addrlen , int backlog ); <nl> int MK_EXPORT _mkp_network_io_server ( int port , char * listen_addr , int reuse_port ); <nl> + int MK_EXPORT _mkp_network_io_buffer_size (); <nl> int MK_EXPORT _mkp_event_read ( int sockfd ); <nl> int MK_EXPORT _mkp_event_write ( int sockfd ); <nl> int MK_EXPORT _mkp_event_error ( int sockfd );
static int get_port_by_socket ( int fd ) <nl> socklen_t len = sizeof ( struct sockaddr_in ); <nl> struct sockaddr_in m_addr ; <nl>  <nl> - getpeername ( fd , ( struct sockaddr *) & m_addr , & len ); <nl> + int ret = getpeername ( fd , ( struct sockaddr *) & m_addr , & len ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> return ( int ) m_addr . sin_port ; <nl> } <nl> 
# include < assert . h > <nl> # include < string . h > <nl>  <nl> -# if defined ( __linux__ ) <nl> -# include < ucontext . h > <nl> -# elif defined ( __APPLE__ ) <nl> +# if defined ( __APPLE__ ) <nl> # include < sys / ucontext . h > <nl> +# else <nl> +# include < ucontext . h > <nl> # endif <nl>  <nl> # include < limits . h >
int sql_dump ( sqlite3 * db , const char * query , sqlite3_stmt ** handle ) <nl> ret = sqlite3_prepare ( db , query , - 1 , handle , NULL ); <nl> if ( ret != SQLITE_OK || ! handle ) { <nl> printf (" Error : sql_dump ()=% d % s \ n ", ret , sqlite3_errmsg ( db )); <nl> + return - 1 ; <nl> } <nl>  <nl> return ret ;
time_t mk_utils_gmt2utime ( char * date ) <nl> { <nl> time_t new_unix_time ; <nl> struct tm t_data ; <nl> + memset (& t_data , 0 , sizeof ( struct tm )); <nl>  <nl> if (! strptime ( date , GMT_DATEFORMAT , ( struct tm *) & t_data )) { <nl> return - 1 ;
int mk_sched_check_timeouts ( struct sched_list_node * sched ) <nl>  <nl> mk_sched_remove_client ( sched , cs_node -> socket ); <nl> mk_session_remove ( cs_node -> socket ); <nl> + <nl> + /* This removal invalidated our iterator . Start over from the beginning . */ <nl> + node = rb_first ( cs_list ); <nl> + if (! node ) break ; <nl> } <nl> } <nl> }
static inline void mk_stream_set ( struct mk_stream * stream , int type , <nl> * performance and aim to make things easier . The COPYBUF type is not <nl> * used by Monkey core , at the moment the only caller is the CGI plugin . <nl> */ <nl> - if (! stream && type == MK_STREAM_COPYBUF ) { <nl> + if ( type == MK_STREAM_COPYBUF ) { <nl> stream = mk_mem_malloc ( sizeof ( struct mk_stream )); <nl> } <nl> 
shell_doc_system_open ( ShellDocSystem * system , <nl> app_exec_quoted = g_regex_replace ( regex , app_exec , - 1 , 0 , "%%", 0 , NULL ); <nl> g_regex_unref ( regex ); <nl>  <nl> - app_info = g_app_info_create_from_commandline ( app_exec , NULL , 0 , NULL ); <nl> + app_info = g_app_info_create_from_commandline ( app_exec_quoted , NULL , 0 , NULL ); <nl> + g_free ( app_exec_quoted ); <nl>  <nl> /* The point of passing an app launch context to <nl> launch () is mostly to get startup notification and
recorder_record_frame ( ShellRecorder * recorder ) <nl>  <nl> size = recorder -> area . width * recorder -> area . height * 4 ; <nl>  <nl> - data = g_malloc ( recorder -> area . width * 4 * recorder -> area . height ); <nl> + data = g_malloc ( size ); <nl> cogl_framebuffer_read_pixels ( cogl_get_draw_framebuffer (), <nl> recorder -> area . x , <nl> recorder -> area . y ,
st_box_layout_get_paint_volume ( ClutterActor * actor , <nl> ClutterActorBox content_box ; <nl> ClutterVertex origin ; <nl>  <nl> + /* Setting the paint volume does not make sense when we don ' t have any allocation */ <nl> + if (! clutter_actor_has_allocation ( actor )) <nl> + return FALSE ; <nl> + <nl> /* When have an adjustment we are clipped to the content box , so base <nl> * our paint volume on that . */ <nl> if ( priv -> hadjustment || priv -> vadjustment )
NPP_GetValue ( NPP instance , <nl>  <nl> *( NPObject **) value = funcs . createobject ( instance , & plugin_class ); <nl> break ; <nl> + <nl> + case NPPVpluginNeedsXEmbed : <nl> + *( bool *) value = TRUE ; <nl> + break ; <nl> + <nl> default : <nl> ; <nl> }
_shell_app_remove_window ( ShellApp * app , <nl> g_object_unref ( window ); <nl> app -> windows = g_slist_remove ( app -> windows , window ); <nl>  <nl> + g_signal_emit ( app , shell_app_signals [ WINDOWS_CHANGED ], 0 ); <nl> + <nl> if ( app -> windows == NULL ) <nl> disconnect_workspace_switch ( app ); <nl> }
shell_global_get_runtime_state ( ShellGlobal * global , <nl> else <nl> { <nl> GBytes * bytes = g_mapped_file_get_bytes ( mfile ); <nl> - res = g_variant_new_from_bytes (( GVariantType *) property_type , bytes , TRUE ); <nl> + res = g_variant_new_from_bytes ( G_VARIANT_TYPE ( property_type ), bytes , TRUE ); <nl> g_bytes_unref ( bytes ); <nl> g_mapped_file_unref ( mfile ); <nl> }
static inline int SCSigGetFlowintType ( Signature * sig ) <nl> fi -> modifier == FLOWINT_MODIFIER_NE || <nl> fi -> modifier == FLOWINT_MODIFIER_GE || <nl> fi -> modifier == FLOWINT_MODIFIER_GT || <nl> + fi -> modifier == FLOWINT_MODIFIER_NOTSET || <nl> fi -> modifier == FLOWINT_MODIFIER_ISSET ) { <nl> read ++; <nl> } else {
void StreamTcpReassembleMemuseCounter ( ThreadVars * tv , TcpReassemblyThreadCtx * rt <nl> * \ retval 0 if not in bounds <nl> */ <nl> int StreamTcpReassembleCheckMemcap ( uint32_t size ) { <nl> - if ( stream_config . reassembly_memcap == 0 || size + SC_ATOMIC_GET ( ra_memuse ) <= stream_config . reassembly_memcap ) <nl> + if ( stream_config . reassembly_memcap == 0 || <nl> + ( uint64_t )(( uint64_t ) size + SC_ATOMIC_GET ( ra_memuse )) <= stream_config . reassembly_memcap ) <nl> return 1 ; <nl> return 0 ; <nl> }
static int SSHParseBanner ( SshState * state , SshHeader * header , const uint8_t * inp <nl> uint32_t line_len = input_len ; <nl>  <nl> /* is it the version line ? */ <nl> - if ( SCMemcmp (" SSH -", line_ptr , 4 ) != 0 ) { <nl> + if ( line_len >= 4 && SCMemcmp (" SSH -", line_ptr , 4 ) != 0 ) { <nl> SCReturnInt (- 1 ); <nl> } <nl> 
int DetectEngineInspectHttpStatCode ( DetectEngineCtx * de_ctx , <nl> } <nl>  <nl> # ifdef DEBUG <nl> - SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSMDMATCH ]; <nl> + SigMatch * sm = s -> sm_lists [ DETECT_SM_LIST_HSCDMATCH ]; <nl> DetectContentData * co = ( DetectContentData *) sm -> ctx ; <nl> SCLogDebug (" co -> id %" PRIu32 , co -> id ); <nl> # endif
static int DNP3CheckStartBytes ( const DNP3LinkHeader * header ) <nl> */ <nl> static int DNP3ContainsBanner ( const uint8_t * input , uint32_t len ) <nl> { <nl> - return memmem ( input , len , banner , strlen ( banner )) != NULL ; <nl> + return BasicSearch ( input , len , ( uint8_t *) banner , strlen ( banner )) != NULL ; <nl> } <nl>  <nl> /**
int SCCudaHlGetCudaModule ( CUmodule * p_module , const char * ptx_image , int handle ) <nl> if ( unlikely ( image == NULL )) { <nl> exit ( EXIT_FAILURE ); <nl> } <nl> - memset ( image , 0x0 , sizeof ( image )); <nl> + memset ( image , 0x0 , strlen ( ptx_image )+ 15 ); <nl>  <nl> int major = INT_MAX ; <nl> int minor = INT_MAX ;
insert : <nl>  <nl> Frag * frag ; <nl> TAILQ_FOREACH ( frag , & tracker -> frags , next ) { <nl> - if ( frag_offset < frag -> offset ) <nl> + if ( new -> offset < frag -> offset ) <nl> break ; <nl> } <nl> if ( frag == NULL ) {
static int StatsOutput ( ThreadVars * tv ) <nl> const StatsCounter * pc = NULL ; <nl> void * td = stats_thread_data ; <nl>  <nl> + if ( counters_global_id == 0 ) <nl> + return - 1 ; <nl> + <nl> if ( stats_table . nstats == 0 ) { <nl> StatsThreadRegister (" Global ", & stats_ctx -> global_counter_ctx ); <nl> 
static int DetectFlowvarSetup ( DetectEngineCtx * de_ctx , Signature * s , char * raws <nl> fd = SCMalloc ( sizeof ( DetectFlowvarData )); <nl> if ( unlikely ( fd == NULL )) <nl> goto error ; <nl> + memset ( fd , 0x00 , sizeof (* fd )); <nl>  <nl> fd -> content = SCMalloc ( contentlen ); <nl> if ( unlikely ( fd -> content == NULL ))
int SigAddressPrepareStage4 ( DetectEngineCtx * de_ctx ) <nl>  <nl> SigGroupHeadBuildNonPrefilterArray ( de_ctx , sgh ); <nl>  <nl> + SigGroupHeadInitDataFree ( sgh -> init ); <nl> + sgh -> init = NULL ; <nl> + <nl> sgh -> id = idx ; <nl> cnt ++; <nl> }
static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> uint8_t flags , <nl> uint32_t * buffer_len ) <nl> { <nl> + uint8_t * headers_buffer = NULL ; <nl> int index = 0 ; <nl> * buffer_len = 0 ; <nl>  <nl> static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> goto end ; <nl>  <nl> htp_header_t * h = NULL ; <nl> - uint8_t * headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> + headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> size_t headers_buffer_len = 0 ; <nl>  <nl> table_iterator_reset ( headers );
DetectPcreData * DetectPcreParse ( char * regexstr ) <nl> SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' I '"); <nl> goto error ; <nl> } <nl> + if ( pd -> flags & DETECT_PCRE_RAWBYTES ) { <nl> + SCLogError ( SC_ERR_INVALID_SIGNATURE , " regex modifier ' U ' inconsistent with ' B '"); <nl> + goto error ; <nl> + } <nl> pd -> flags |= DETECT_PCRE_URI ; <nl> break ; <nl> case ' H ': /* snort ' s option */
IMPORTED_FUNCTION * pe_parse_import_descriptor ( <nl> // I ' ve seen binaries where OriginalFirstThunk is zero . In this case <nl> // use FirstThunk . <nl>  <nl> - if ( offset < 0 ) <nl> + if ( offset <= 0 ) <nl> offset = pe_rva_to_offset ( pe , import_descriptor -> FirstThunk ); <nl>  <nl> if ( offset < 0 )
ivec_reserve ( rust_task * task , type_desc * ty , rust_ivec * v , size_t n_elems ) <nl> } else { <nl> // On heap ; resize . <nl> heap_part = ( rust_ivec_heap *) task -> realloc ( v -> payload . ptr , <nl> - new_alloc ); <nl> + new_alloc + sizeof ( size_t )); <nl> v -> payload . ptr = heap_part ; <nl> } <nl> 
SOFTWARE , EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE . <nl> */ <nl>  <nl> +// transition helper <nl> +# ifdef FMT_FORMAT_PROVIDE_PRINTF <nl> +# include " printf . h " <nl> +# endif <nl> + <nl> # ifndef FMT_FORMAT_H_ <nl> # define FMT_FORMAT_H_ <nl> 
void FormatDecimal ( char * buffer , uint64_t value , unsigned num_digits ) { <nl> # ifdef _MSC_VER <nl> int signbit ( double value ) { <nl> if ( value < 0 ) return 1 ; <nl> - if (! isnan ( value )) return 0 ; <nl> + if ( value == value ) return 0 ; <nl> int dec = 0 , sign = 0 ; <nl> ecvt ( value , 0 , & dec , & sign ); <nl> return sign ;
CURLcode Curl_readwrite ( struct connectdata * conn , <nl> time_t secs = time ( NULL ); <nl> k -> timeofdoc = curl_getdate ( k -> p + strlen (" Last - Modified :"), <nl> & secs ); <nl> - if ( data -> set . get_filetime >= 0 ) <nl> + if ( data -> set . get_filetime ) <nl> data -> info . filetime = k -> timeofdoc ; <nl> } <nl> else if (( k -> httpcode >= 300 && k -> httpcode < 400 ) &&
static ssize_t data_source_read_callback ( nghttp2_session * session , <nl> memcpy ( buf , stream -> upload_mem , nread ); <nl> stream -> upload_mem += nread ; <nl> stream -> upload_len -= nread ; <nl> - stream -> upload_left -= nread ; <nl> + if ( data_s -> state . infilesize != - 1 ) <nl> + stream -> upload_left -= nread ; <nl> } <nl>  <nl> if ( stream -> upload_left == 0 )
static CURLcode ssh_block_statemach ( struct connectdata * conn , <nl>  <nl> while (( sshc -> state != SSH_STOP ) && ! result ) { <nl> bool block ; <nl> - time_t left ; <nl> + time_t left = 1000 ; <nl> struct timeval now = Curl_tvnow (); <nl>  <nl> result = ssh_statemach_act ( conn , & block );
CURLcode get_url_file_name ( char ** filename , const char * url ) <nl> Curl_safefree (* filename ); <nl> * filename = strdup ( buffer ); /* clone the buffer */ <nl> curl_free ( tdir ); <nl> + if (!* filename ) <nl> + return CURLE_OUT_OF_MEMORY ; <nl> } <nl> } <nl> # endif
static CURLMcode multi_runsingle ( struct Curl_multi * multi , <nl>  <nl> case CURLM_STATE_TOOFAST : /* limit - rate exceeded in either direction */ <nl> /* if both rates are within spec , resume transfer */ <nl> + Curl_pgrsUpdate ( easy -> easy_conn ); <nl> if ( ( ( data -> set . max_send_speed == 0 ) || <nl> ( data -> progress . ulspeed < data -> set . max_send_speed )) && <nl> ( ( data -> set . max_recv_speed == 0 ) ||
static bool imap_endofresp ( struct connectdata * conn , char * line , size_t len , <nl> wordlen ++; <nl>  <nl> /* Does the server support the STARTTLS capability ? */ <nl> - if ( len >= 8 && ! memcmp ( line , " STARTTLS ", 8 )) <nl> + if ( wordlen >= 8 && ! memcmp ( line , " STARTTLS ", 8 )) <nl> imapc -> tls_supported = TRUE ; <nl>  <nl> /* Has the server explicitly disabled clear text authentication ? */
int Curl_resolv_timeout ( struct connectdata * conn , <nl>  <nl> * entry = NULL ; <nl>  <nl> + if ( timeoutms < 0 ) <nl> + /* got an already expired timeout */ <nl> + return CURLRESOLV_TIMEDOUT ; <nl> + <nl> # ifdef USE_ALARM_TIMEOUT <nl> if ( data -> set . no_signal ) <nl> /* Ignore the timeout when signals are disabled */
void curl_mime_free ( curl_mime * mime ) <nl> curl_mimepart * part ; <nl>  <nl> if ( mime ) { <nl> + mime_subparts_unbind ( mime ); /* Be sure it ' s not referenced anymore . */ <nl> while ( mime -> firstpart ) { <nl> part = mime -> firstpart ; <nl> mime -> firstpart = part -> nextpart ;
void ourWriteOut ( CURL * curl , struct OutStruct * outs , const char * writeinfo ) <nl> double doubleinfo ; <nl>  <nl> while ( ptr && * ptr ) { <nl> - if ('%' == * ptr ) { <nl> + if ('%' == * ptr && ptr [ 1 ]) { <nl> if ('%' == ptr [ 1 ]) { <nl> /* an escaped %- letter */ <nl> fputc ('%', stream );
CURLcode Curl_proxyCONNECT ( struct connectdata * conn , <nl> else <nl> for ( i = 0 ; i < gotbytes ; ptr ++, i ++) { <nl> perline ++; /* amount of bytes in this line so far */ <nl> - if (* ptr =='\ x0a ') { <nl> + if (* ptr == 0x0a ) { <nl> char letter ; <nl> int writetype ; <nl> 
int main ( int argc , char ** argv ) <nl> curl = curl_easy_init (); <nl> if ( curl ) { <nl> /* what call to write : */ <nl> - curl_easy_setopt ( curl , CURLOPT_URL , " HTTPS :// curl . haxx . se "); <nl> + curl_easy_setopt ( curl , CURLOPT_URL , " HTTPS :// your . favourite . ssl . site "); <nl> curl_easy_setopt ( curl , CURLOPT_WRITEHEADER , headerfile ); <nl>  <nl> while ( 1 ) /* do some ugly short cut ... */
gtls_connect_step3 ( struct connectdata * conn , <nl> infof ( data , "\ t common name : WARNING couldn ' t obtain \ n "); <nl> } <nl>  <nl> - if ( data -> set . ssl . certinfo ) { <nl> + if ( data -> set . ssl . certinfo && chainp ) { <nl> unsigned int i ; <nl>  <nl> result = Curl_ssl_init_certinfo ( data , cert_list_size );
libc5 - based Linux systems . Only include it on system that are known to <nl> require it ! */ <nl> # if defined ( _AIX ) || defined ( __NOVELL_LIBC__ ) || defined ( __NetBSD__ ) || \ <nl> - defined ( __minix ) || defined ( __SYMBIAN32__ ) || defined ( __INTEGRITY ) <nl> + defined ( __minix ) || defined ( __SYMBIAN32__ ) || defined ( __INTEGRITY ) || \ <nl> + defined ( ANDROID ) <nl> # include < sys / select . h > <nl> # endif <nl> 
int main ( void ) <nl> /* the DEBUGFUNCTION has no effect until we enable VERBOSE */ <nl> curl_easy_setopt ( curl , CURLOPT_VERBOSE , 1L ); <nl>  <nl> + /* example . com is redirected , so we tell libcurl to follow redirection */ <nl> + curl_easy_setopt ( curl , CURLOPT_FOLLOWLOCATION , 1L ); <nl> + <nl> curl_easy_setopt ( curl , CURLOPT_URL , " http :// example . com /"); <nl> res = curl_easy_perform ( curl ); <nl> /* Check for errors */
static void voutf ( struct GlobalConfig * config , <nl> ( void ) fwrite ( ptr , cut + 1 , 1 , config -> errors ); <nl> fputs ("\ n ", config -> errors ); <nl> ptr += cut + 1 ; /* skip the space too */ <nl> - len -= cut ; <nl> + len -= cut + 1 ; <nl> } <nl> else { <nl> fputs ( ptr , config -> errors );
static ParameterError str2double ( double * val , const char * str , long max ) <nl> num = strtod ( str , & endptr ); <nl> if ( errno == ERANGE ) <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> - if (( long ) num > max ) { <nl> + if ( num > max ) { <nl> /* too large */ <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> }
void NPC_ChoosePainAnimation ( gentity_t * self , gentity_t * other , vec3_t point , i <nl> || PM_RollingAnim ( self -> client -> ps . legsAnim ) <nl> || ( BG_FlippingAnim ( self -> client -> ps . legsAnim )&&! PM_InCartwheel ( self -> client -> ps . legsAnim )) ) <nl> {// strong attacks , rolls , knockdowns , flips and spins cannot be interrupted by pain <nl> + return ; <nl> } <nl> else <nl> {// play an anim
static void GLimp_InitExtensions ( void ) <nl> // Find out how many general combiners they have . <nl> # define GL_MAX_GENERAL_COMBINERS_NV 0x854D <nl> GLint iNumGeneralCombiners = 0 ; <nl> - qglGetIntegerv ( GL_MAX_GENERAL_COMBINERS_NV , & iNumGeneralCombiners ); <nl> + if ( bNVRegisterCombiners ) <nl> + qglGetIntegerv ( GL_MAX_GENERAL_COMBINERS_NV , & iNumGeneralCombiners ); <nl>  <nl> // Only allow dynamic glows / flares if they have the hardware <nl> if ( bTexRectSupported && bARBVertexProgram && qglActiveTextureARB && glConfig . maxActiveTextures >= 4 &&
# define GLOBAL_METHOD_CACHE_SIZE 0x800 <nl> # endif <nl> # define LSB_ONLY ( x ) (( x ) & ~(( x ) - 1 )) <nl> -# define POWOR_OF_2_P ( x ) (( x ) == LSB_ONLY ( x )) <nl> -# if ! POWOR_OF_2_P ( GLOBAL_METHOD_CACHE_SIZE ) <nl> +# define POWER_OF_2_P ( x ) (( x ) == LSB_ONLY ( x )) <nl> +# if ! POWER_OF_2_P ( GLOBAL_METHOD_CACHE_SIZE ) <nl> # error GLOBAL_METHOD_CACHE_SIZE must be power of 2 <nl> # endif <nl> # ifndef GLOBAL_METHOD_CACHE_MASK
onig_vsnprintf_with_pattern ( UChar buf [], int bufsize , OnigEncoding enc , <nl> need = ( pat_end - pat ) * 4 + 4 ; <nl>  <nl> if ( n + need < ( size_t ) bufsize ) { <nl> - xstrcat (( char * ) buf , ": /", bufsize ); <nl> + static const char sep [] = ": /"; <nl> + memcpy (( char * ) buf + n , sep , sizeof ( sep )); <nl> s = buf + onigenc_str_bytelen_null ( ONIG_ENCODING_ASCII , buf ); <nl>  <nl> p = pat ;
syck_hdlr_add_alias ( SyckParser * p , char * a ) <nl> { <nl> SyckNode * n ; <nl>  <nl> - if ( st_lookup ( p -> anchors , ( st_data_t ) a , & n ) ) <nl> + if ( st_lookup ( p -> anchors , ( st_data_t ) a , ( st_data_t *)& n ) ) <nl> { <nl> return n ; <nl> }
zstream_run_func ( void * ptr ) <nl> struct zstream * z = args -> z ; <nl> uInt n ; <nl>  <nl> + err = Z_OK ; <nl> while (! args -> interrupt ) { <nl> n = z -> stream . avail_out ; <nl> err = z -> func -> run (& z -> stream , flush );
unix_recv_io ( int argc , VALUE * argv , VALUE sock ) <nl> enum { <nl> GC_REASON_EMSGSIZE = 0x1 , <nl> GC_REASON_TRUNCATE = 0x2 , <nl> - GC_REASON_ENOMEM = 0x4 , <nl> + GC_REASON_ENOMEM = 0x4 <nl> }; <nl>  <nl> int fd ;
rb_str_cat_conv_enc_opts ( VALUE newstr , long ofs , const char * ptr , long len , <nl> long olen ; <nl>  <nl> olen = RSTRING_LEN ( newstr ); <nl> - if ( ofs < - olen || olen <= ofs ) <nl> + if ( ofs < - olen || olen < ofs ) <nl> rb_raise ( rb_eIndexError , " index % ld out of string ", ofs ); <nl> if ( ofs < 0 ) ofs += olen ; <nl> if (! from ) {
condvar_ptr ( VALUE self ) <nl>  <nl> /* forked children can ' t reach into parent thread stacks */ <nl> if ( cv -> fork_gen != fork_gen ) { <nl> + cv -> fork_gen = fork_gen ; <nl> list_head_init (& cv -> waitq ); <nl> } <nl> 
st_insert2 ( st_table * tab , st_data_t key , st_data_t value , <nl> if ( tab -> bins == NULL ) { <nl> bin = find_entry ( tab , hash_value , key ); <nl> new_p = bin == UNDEFINED_ENTRY_IND ; <nl> + if ( new_p ) <nl> + tab -> num_entries ++; <nl> bin_ind = UNDEFINED_BIN_IND ; <nl> } <nl> else {
valid_hostname ( const char * hostname ) <nl> if ( hostname == NULL ) <nl> return NO ; <nl>  <nl> + if (! strcmp ( hostname , " localhost ")) <nl> + return YES ; <nl> + <nl> if ('.' == * p || ':' == * p || '/' == * p ) <nl> return NO ; <nl> 
load_a_module ( const char * path , int warn , int core ) <nl> } <nl> } <nl> } <nl> + <nl> + break ; <nl> default : <nl> ilog ( L_MAIN , " Module % s has unknown / unsupported MAPI version % d .", <nl> mod_basename , MAPI_VERSION (* mapi_version ));
stats_dnsbl ( struct Client * source_p ) <nl> rb_dictionary_iter iter ; <nl> struct BlacklistStats * stats ; <nl>  <nl> + if ( bl_stats == NULL ) <nl> + return ; <nl> + <nl> RB_DICTIONARY_FOREACH ( stats , & iter , bl_stats ) <nl> { <nl> /* use RPL_STATSDEBUG for now -- jilles */
blacklist_dns_callback ( const char * result , bool status , query_type type , void * d <nl> { <nl> /* Done here */ <nl> notice_client ( auth -> cid , "*** IP not found in DNS blacklist % s ", <nl> - rb_dlink_list_length (& blacklist_list ) > 1 : " s " : ""); <nl> + rb_dlink_list_length (& blacklist_list ) > 1 ? " s " : ""); <nl> rb_free ( bluser ); <nl> auth -> data [ PROVIDER_BLACKLIST ] = NULL ; <nl> provider_done ( auth , PROVIDER_BLACKLIST );
delete_opm_scanner ( const char * key __unused , int parc __unused , const char ** par <nl>  <nl> rb_dlinkDelete (& proxy -> node , & proxy_scanners ); <nl> rb_free ( proxy ); <nl> + <nl> + if (! rb_dlink_list_length ( proxy_scanners )) <nl> + opm_enable = false ; <nl> } <nl>  <nl> static void <nl> delete_opm_scanner_all ( const char * key __unused , int parc __unused , const char * <nl> { <nl> opm_cancel ( auth ); <nl> } <nl> + <nl> + opm_enable = false ; <nl> } <nl>  <nl> 
static INLINE OPJ_BOOL opj_dwt_encode_procedure ( opj_tcd_tilecomp_t * tilec , void <nl>  <nl> l_data_size = opj_dwt_max_resolution ( tilec -> resolutions , tilec -> numresolutions ) * ( OPJ_UINT32 ) sizeof ( OPJ_INT32 ); <nl> bj = ( OPJ_INT32 *) opj_malloc (( size_t ) l_data_size ); <nl> - if (! bj ) { <nl> + if ( l_data_size != 0 && ! bj ) { <nl> return OPJ_FALSE ; <nl> } <nl> i = l ;
opj_pi_iterator_t * opj_pi_create_decode ( opj_image_t * p_image , <nl> l_current_pi -> include = 00 ; <nl> if ( l_step_l <= ( SIZE_MAX / ( l_tcp -> numlayers + 1U ))) <nl> { <nl> - l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( l_tcp -> numlayers + 1 ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> + l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( size_t )( l_tcp -> numlayers + 1U ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> } <nl>  <nl> if
void pureftpd_register_simple_auth_callback ( int (* callback )( const char * account , <nl> static AuthResult embedded_simple_pw_check ( const char * account , const char * password ) <nl> { <nl> AuthResult authresult ; <nl> - <nl> + <nl> + memset (& authresult , 0 , sizeof authresult ); <nl> if ( simple_auth_callback == NULL || <nl> account == NULL || * account == 0 || password == NULL ) { <nl> authresult . auth_ok = 0 ;
public : <nl> inline Iterator end () const { return Iterator ( value , count ); } <nl>  <nl> inline size_t size () const { return count ; } <nl> + inline T operator []( ptrdiff_t ) const { return value ; } <nl>  <nl> private : <nl> T value ;
foptoas ( int op , Type * t , int flg ) <nl> { <nl> int et , a ; <nl>  <nl> + a = AGOK ; <nl> et = simtype [ t -> etype ]; <nl>  <nl> if ( use_sse )
Dconv ( Fmt * fp ) <nl> break ; <nl>  <nl> case D_BRANCH : <nl> - snprint ( str , sizeof ( str ), "% d ", a -> u . branch -> loc ); <nl> + if ( a -> u . branch == nil ) <nl> + snprint ( str , sizeof ( str ), "< nil >"); <nl> + else <nl> + snprint ( str , sizeof ( str ), "% d ", a -> u . branch -> loc ); <nl> break ; <nl>  <nl> case D_EXTERN :
static struct DWAbbrev { <nl> DW_TAG_subrange_type , DW_CHILDREN_no , <nl> // No name ! <nl> DW_AT_type , DW_FORM_ref_addr , <nl> - DW_AT_upper_bound , DW_FORM_data1 , <nl> + DW_AT_upper_bound , DW_FORM_udata , <nl> 0 , 0 <nl> }, <nl> 
runtime  Goexit ( void ) <nl> rundefer (); <nl> runtime  goexit (); <nl> } <nl> + <nl> + void <nl> + runtime  panicdivide ( void ) <nl> +{ <nl> + runtime  panicstring (" integer divide by zero "); <nl> +}
dumpbv ( BitVector * bv , uintptr offset ) <nl> for ( i = 0 ; i < bv -> n ; i += BitsPerPointer ) { <nl> switch ( bv -> bytedata [ i / 8 ] >> i % 8 & 3 ) { <nl> case BitsDead : <nl> - return ; <nl> + // BitsDead has already been processed in makeheapobjbv . <nl> + // We should only see it in stack maps , in which case we should continue processing . <nl> + break ; <nl> case BitsScalar : <nl> break ; <nl> case BitsPointer :
runtime  equal ( Type * t , ...) <nl> uintptr ret ; <nl>  <nl> x = ( byte *)(& t + 1 ); <nl> - y = x + ROUND ( t -> size , t -> align ); <nl> + y = x + t -> size ; <nl> ret = ( uintptr )( y + t -> size ); <nl> ret = ROUND ( ret , Structrnd ); <nl> t -> alg -> equal (( bool *) ret , t -> size , x , y );
static void list_type ( FUNC_TYPE ft , int one ) <nl> { <nl> FUNCTION * fp ; <nl> int i = 0 ; <nl> - DISPLAY_COLUMNS dc ; <nl> + DISPLAY_COLUMNS dc = { 0 }; <nl>  <nl> if (! one ) <nl> calculate_columns (& dc );
DECLARE_ASN1_FUNCTIONS ( X509_CINF ) <nl> DECLARE_ASN1_FUNCTIONS ( X509 ) <nl> DECLARE_ASN1_FUNCTIONS ( X509_CERT_AUX ) <nl>  <nl> -# define X509_new_index ( l , p , newf , dupf , freef ) \ <nl> +# define X509_get_ex_new_index ( l , p , newf , dupf , freef ) \ <nl> CRYPTO_get_ex_new_index ( CRYPTO_EX_INDEX_X509 , l , p , newf , dupf , freef ) <nl> int X509_set_ex_data ( X509 * r , int idx , void * arg ); <nl> void * X509_get_ex_data ( X509 * r , int idx );
int tls1_mac ( SSL * ssl , SSL3_RECORD * rec , unsigned char * md , int sending ) <nl> mac_ctx = hash ; <nl> } else { <nl> hmac = EVP_MD_CTX_new (); <nl> - if ( hmac == NULL || ! EVP_MD_CTX_copy ( hmac , hash )) <nl> + if ( hmac == NULL || ! EVP_MD_CTX_copy ( hmac , hash )) { <nl> + EVP_MD_CTX_free ( hmac ); <nl> return 0 ; <nl> + } <nl> mac_ctx = hmac ; <nl> } <nl> 
int ossl_policy_cache_set_mapping ( X509 * x , POLICY_MAPPINGS * maps ) <nl>  <nl> ret = 1 ; <nl> bad_mapping : <nl> - if ( ret == - 1 && CRYPTO_THREAD_write_lock ( x -> lock )) { <nl> - x -> ex_flags |= EXFLAG_INVALID_POLICY ; <nl> - CRYPTO_THREAD_unlock ( x -> lock ); <nl> - } <nl> sk_POLICY_MAPPING_pop_free ( maps , POLICY_MAPPING_free ); <nl> return ret ; <nl> 
int asn1parse_main ( int argc , char ** argv ) <nl> ASN1_TYPE * atmp ; <nl> int typ ; <nl> j = atoi ( sk_OPENSSL_STRING_value ( osk , i )); <nl> - if ( j == 0 ) { <nl> + if ( j <= 0 || j >= tmplen ) { <nl> BIO_printf ( bio_err , "'% s ' is an invalid number \ n ", <nl> sk_OPENSSL_STRING_value ( osk , i )); <nl> continue ;
int dsa_builtin_paramgen2 ( DSA * ret , size_t L , size_t N , <nl> } else { <nl> p = BN_CTX_get ( ctx ); <nl> q = BN_CTX_get ( ctx ); <nl> + if ( q == NULL ) <nl> + goto err ; <nl> } <nl>  <nl> if (! BN_lshift ( test , BN_value_one (), L - 1 ))
int OBJ_create ( const char * oid , const char * sn , const char * ln ) <nl>  <nl> /* Convert numerical OID string to an ASN1_OBJECT structure */ <nl> tmpoid = OBJ_txt2obj ( oid , 1 ); <nl> + if ( tmpoid == NULL ) <nl> + return 0 ; <nl>  <nl> /* If NID is not NID_undef then object already exists */ <nl> if ( OBJ_obj2nid ( tmpoid ) != NID_undef ) {
static int pkey_gost_ctrl ( EVP_PKEY_CTX * ctx , int type , int p1 , void * p2 ) <nl> return 1 ; <nl> case EVP_PKEY_CTRL_SET_IV : <nl> pctx -> shared_ukm = OPENSSL_malloc (( int ) p1 ); <nl> + if ( pctx -> shared_ukm == NULL ) <nl> + { <nl> + GOSTerr ( GOST_F_PKEY_GOST_CTRL , ERR_R_MALLOC_FAILURE ); <nl> + return 0 ; <nl> + } <nl> memcpy ( pctx -> shared_ukm , p2 ,( int ) p1 ); <nl> return 1 ; <nl> case EVP_PKEY_CTRL_PEER_KEY :
int OCSP_parse_url ( char * url , char ** phost , char ** pport , char ** ppath , int * pss <nl>  <nl>  <nl> err : <nl> + if ( buf ) OPENSSL_free ( buf ); <nl> if (* ppath ) OPENSSL_free (* ppath ); <nl> if (* pport ) OPENSSL_free (* pport ); <nl> if (* phost ) OPENSSL_free (* phost );
static int tree_init ( X509_POLICY_TREE ** ptree , STACK_OF ( X509 ) * certs , <nl> X509_check_purpose ( x , - 1 , 0 ); <nl>  <nl> /* If cache is NULL , likely ENOMEM : return immediately */ <nl> - if (( cache = policy_cache_set ( x )) == NULL ) <nl> + if ( policy_cache_set ( x ) == NULL ) <nl> return X509_PCY_TREE_INTERNAL ; <nl> } <nl> 
static void cms_env_set_version ( CMS_EnvelopedData * env ) <nl> env -> version = 2 ; <nl> } <nl> } <nl> - if ( env -> version == 2 ) <nl> - return ; <nl> if ( env -> originatorInfo || env -> unprotectedAttrs ) <nl> env -> version = 2 ; <nl> + if ( env -> version == 2 ) <nl> + return ; <nl> env -> version = 0 ; <nl> } <nl> 
static CAPI_KEY * capi_get_key ( CAPI_CTX * ctx , const TCHAR * contname , TCHAR * provn <nl> CAPI_KEY * key ; <nl> DWORD dwFlags = 0 ; <nl> key = OPENSSL_malloc ( sizeof ( CAPI_KEY )); <nl> + if ( key == NULL ) <nl> + return NULL ; <nl> if ( sizeof ( TCHAR )== sizeof ( char )) <nl> CAPI_trace ( ctx , " capi_get_key , contname =% s , provname =% s , type =% d \ n ", <nl> contname , provname , ptype );
int SSL_check_private_key ( SSL * ssl ) <nl> SSLerr ( SSL_F_SSL_CHECK_PRIVATE_KEY , ERR_R_PASSED_NULL_PARAMETER ); <nl> return ( 0 ); <nl> } <nl> + if ( ssl -> cert == NULL ) <nl> + return 0 ; <nl> if ( ssl -> cert -> key -> x509 == NULL ) <nl> { <nl> SSLerr ( SSL_F_SSL_CHECK_PRIVATE_KEY , SSL_R_NO_CERTIFICATE_ASSIGNED );
static struct file_st * win32_splitter ( DSO * dso , const char * filename , <nl> DSOerr ( DSO_F_WIN32_SPLITTER , <nl> DSO_R_INCORRECT_FILE_SYNTAX ); <nl> /* goto err ;*/ <nl> + OPENSSL_free ( result ); <nl> return ( NULL ); <nl> } <nl> result -> device = start ; <nl> static char * win32_merger ( DSO * dso , const char * filespec1 , const char * filespec2 <nl>  <nl> merged = win32_joiner ( dso , filespec1_split ); <nl> } <nl> + OPENSSL_free ( filespec1_split ); <nl> + OPENSSL_free ( filespec2_split ); <nl> return ( merged ); <nl> } <nl> 
int PEM_read_bio ( BIO * bp , char ** name , char ** header , unsigned char ** data , <nl> dataB = BUF_MEM_new (); <nl> if (( nameB == NULL ) || ( headerB == NULL ) || ( dataB == NULL )) <nl> { <nl> + BUF_MEM_free ( nameB ); <nl> + BUF_MEM_free ( headerB ); <nl> + BUF_MEM_free ( dataB ); <nl> PEMerr ( PEM_F_PEM_READ_BIO , ERR_R_MALLOC_FAILURE ); <nl> return ( 0 ); <nl> }
# ifndef HEADER_STORE_H <nl> # define HEADER_STORE_H <nl>  <nl> +# include < openssl / opensslconf . h > <nl> + <nl> +# ifdef OPENSSL_NO_STORE <nl> +# error STORE is disabled . <nl> +# endif <nl> + <nl> # include < openssl / ossl_typ . h > <nl> # ifndef OPENSSL_NO_DEPRECATED <nl> # include < openssl / evp . h >
iperf_new_test () <nl> memset ( test , 0 , sizeof ( struct iperf_test )); <nl>  <nl> test -> settings = ( struct iperf_settings *) malloc ( sizeof ( struct iperf_settings )); <nl> + if (! test -> settings ) { <nl> + free ( test ); <nl> + i_errno = IENEWTEST ; <nl> + return NULL ; <nl> + } <nl> memset ( test -> settings , 0 , sizeof ( struct iperf_settings )); <nl>  <nl> return test ;
static void newstats ( struct cgpu_info * cgpu ) <nl> void update_usb_stats ( __maybe_unused struct cgpu_info * cgpu ) <nl> { <nl> # if DO_USB_STATS <nl> + if ( cgpu -> usbstat < 1 ) <nl> + newstats ( cgpu ); <nl> + <nl> // we don ' t know the device_id until after add_cgpu () <nl> usb_stats [ cgpu -> usbstat - 1 ]. device_id = cgpu -> device_id ; <nl> # endif
static bool klondike_get_stats ( struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= slaves ; dev ++) { <nl> char * reply = SendCmdGetReply ( klncgpu , ' S ', dev , 0 , NULL ); <nl> if ( reply != NULL ) <nl> - klninfo -> status [ dev ] = *( WORKSTATUS *)( reply + 2 ); <nl> + memcpy (( void *)(&( klninfo -> status [ dev ])), reply + 2 , sizeof ( klninfo -> status [ dev ])); <nl> } <nl> wr_unlock (&( klninfo -> stat_lock )); <nl> 
static void * miner_thread ( void * userdata ) <nl> nonce_inc = next_inc ; <nl> } else if (! diff . tv_sec ) <nl> nonce_inc = hashes_done * 2 ; <nl> + if ( nonce_inc < 4 ) <nl> + nonce_inc = 0xffffff ; <nl> max64 = work -> blk . nonce + nonce_inc ; <nl> if ( max64 > 0xfffffffaULL ) <nl> max64 = 0xfffffffaULL ;
CBlock * CreateNewBlock ( CReserveKey & reservekey ) <nl> int64 nValueIn = coins . vout [ txin . prevout . n ]. nValue ; <nl> nTotalIn += nValueIn ; <nl>  <nl> - int nConf = pindexPrev -> nHeight - coins . nHeight ; <nl> + int nConf = pindexPrev -> nHeight - coins . nHeight + 1 ; <nl>  <nl> dPriority += ( double ) nValueIn * nConf ; <nl> }
read_gif ( Gif_Reader * grr , int read_flags , <nl> Gif_DeleteArray ( gfc . suffix ); <nl> Gif_DeleteArray ( gfc . length ); <nl> gfc . gfi = 0 ; <nl> + last_name = 0 ; <nl>  <nl> if ( gfs ) <nl> gfs -> errors = gfc . errors [ 1 ];
particular purpose .\ n "); <nl> # ifdef DMALLOC <nl> dmalloc_report (); <nl> # endif <nl> + Clp_DeleteParser ( clp ); <nl> return ( error_count ? EXIT_ERR : EXIT_OK ); <nl> }
static int file_to_data ( const char * path , char ** data , size_t * len ) <nl> path , strerror ( errno )); <nl> goto err ; <nl> } <nl> + if (! sb . st_size ) { <nl> + * len = 0 ; <nl> + return 0 ; <nl> + } <nl>  <nl> * data = mmap ( NULL , sb . st_size , PROT_READ , MAP_PRIVATE , fd , 0 ); <nl> if (* data == MAP_FAILED ) {
int main ( int argc , char ** argv ) <nl> printf ("% s \ n ", context ); <nl> freecon ( context ); <nl> } <nl> + free ( pc [ i ]); <nl> } <nl>  <nl> printf ("\ nFile contexts :\ n "); <nl> int main ( int argc , char ** argv ) <nl> freecon ( context ); <nl> } <nl> } <nl> + free ( fc [ i ]); <nl> } <nl>  <nl> return 0 ;
static void cil_reset_class ( struct cil_class * class ) <nl>  <nl> static void cil_reset_perm ( struct cil_perm * perm ) <nl> { <nl> - cil_reset_classperms_list ( perm -> classperms ); <nl> + cil_list_destroy (& perm -> classperms , CIL_FALSE ); <nl> } <nl>  <nl> static inline void cil_reset_classperms ( struct cil_classperms * cp )
static void cil_reset_classpermission ( struct cil_classpermission * cp ) <nl> return ; <nl> } <nl>  <nl> - cil_reset_classperms_list ( cp -> classperms ); <nl> + cil_list_destroy (& cp -> classperms , CIL_FALSE ); <nl> } <nl>  <nl> static void cil_reset_classperms_set ( struct cil_classperms_set * cp_set )
static void cil_reset_classpermission ( struct cil_classpermission * cp ) <nl>  <nl> static void cil_reset_classperms_set ( struct cil_classperms_set * cp_set ) <nl> { <nl> - cil_reset_classpermission ( cp_set -> set ); <nl> + if ( cp_set == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> + cp_set -> set = NULL ; <nl> } <nl>  <nl> static inline void cil_reset_classperms_list ( struct cil_list * cp_list )
rsRetVal actionDestruct ( action_t * pThis ) <nl> pthread_mutex_destroy (& pThis -> mutActExec ); <nl> d_free ( pThis -> pszName ); <nl> d_free ( pThis -> ppTpl ); <nl> + d_free ( pThis -> peParamPassing ); <nl>  <nl> finalize_it : <nl> d_free ( pThis );
BEGINrunInput <nl> # endif <nl>  <nl> CODESTARTrunInput <nl> + CHKmalloc ( pReadfds ); <nl> if ( runModConf -> bOmitLocalLogging && nfd == 1 ) <nl> ABORT_FINALIZE ( RS_RET_OK ); <nl> /* this is an endless loop - it is terminated when the thread is
scriptExec ( struct cnfstmt * root , msg_t * pMsg , wti_t * pWti ) <nl> struct cnfstmt * stmt ; <nl>  <nl> for ( stmt = root ; stmt != NULL ; stmt = stmt -> next ) { <nl> + if (* pWti -> pbShutdownImmediate ) { <nl> + DBGPRINTF (" scriptExec : ShutdownImmediate set , " <nl> + " force terminating \ n "); <nl> + goto done ; <nl> + } <nl> if ( Debug ) { <nl> cnfstmtPrintOnly ( stmt , 2 , 0 ); <nl> }
buildSeverityMapping ( instanceData * pData ) <nl> uchar pszSevCode [ 512 ]; <nl> int sevCode ; <nl> uchar * mapping ; <nl> - struct severMap_s * node ; <nl> + struct severMap_s * node = NULL ; <nl> DEFiRet ; <nl>  <nl> mapping = cs . pszSeverityMapping ; <nl> buildSeverityMapping ( instanceData * pData ) <nl> } <nl>  <nl> finalize_it : <nl> + if ( iRet != RS_RET_OK ) { <nl> + if ( node != NULL ) <nl> + free ( node ); <nl> + } <nl> RETiRet ; <nl> } <nl> 
doRetry ( nsd_gtls_t * pNsd ) <nl> break ; <nl> default : <nl> assert ( 0 ); /* this shall not happen ! */ <nl> + dbgprintf (" ERROR : pNsd -> rtryCall invalid in nsdsel_gtls . c :% d \ n ", __LINE__ ); <nl> + gnuRet = 0 ; /* if it happens , we have at least a defined behaviour ... ;) */ <nl> break ; <nl> } <nl> 
CODESTARTfreeWrkrInstance <nl> pWrkrData -> curlHandle = NULL ; <nl> } <nl> free ( pWrkrData -> restURL ); <nl> + es_deleteStr ( pWrkrData -> batch . data ); <nl> ENDfreeWrkrInstance <nl>  <nl> BEGINdbgPrintInstInfo
irc_ctcp_dcc_filename_without_quotes ( const char * filename ) <nl> int length ; <nl>  <nl> length = strlen ( filename ); <nl> - if ( length > 0 ) <nl> + if ( length > 1 ) <nl> { <nl> if (( filename [ 0 ] == '\"') && ( filename [ length - 1 ] == '\"')) <nl> return weechat_strndup ( filename + 1 , length - 2 );
relay_irc_recv ( struct t_relay_client * client , const char * data ) <nl> /* server capabilities */ <nl> if ( irc_command && ( weechat_strcasecmp ( irc_command , " cap ") == 0 )) <nl> { <nl> - if (( irc_argc > 0 ) && irc_argv ) <nl> + if ( irc_argc > 0 ) <nl> { <nl> relay_irc_recv_command_capab ( client , <nl> irc_argc , irc_argv , irc_argv_eol );
archive_acl_from_text_l ( struct archive_acl * acl , const char * text , <nl> st = field [ n ]. start + 1 ; <nl> len = field [ n ]. end - field [ n ]. start ; <nl>  <nl> + if ( len == 0 ) { <nl> + ret = ARCHIVE_WARN ; <nl> + continue ; <nl> + } <nl> + <nl> switch (* s ) { <nl> case ' u ': <nl> if ( len == 1 || ( len == 4
parse_codes ( struct archive_read * a ) <nl> new_size = DICTIONARY_MAX_SIZE ; <nl> else <nl> new_size = rar_fls (( unsigned int ) rar -> unp_size ) << 1 ; <nl> + if ( new_size == 0 ) { <nl> + archive_set_error (& a -> archive , ARCHIVE_ERRNO_FILE_FORMAT , <nl> + " Zero window size is invalid ."); <nl> + return ( ARCHIVE_FATAL ); <nl> + } <nl> new_window = realloc ( rar -> lzss . window , new_size ); <nl> if ( new_window == NULL ) { <nl> archive_set_error (& a -> archive , ENOMEM ,
parse_codes ( struct archive_read * a ) <nl> rar -> range_dec . Stream = & rar -> bytein ; <nl> __archive_ppmd7_functions . Ppmd7_Construct (& rar -> ppmd7_context ); <nl>  <nl> + if ( rar -> dictionary_size == 0 ) { <nl> + archive_set_error (& a -> archive , ARCHIVE_ERRNO_FILE_FORMAT , <nl> + " Invalid zero dictionary size "); <nl> + return ( ARCHIVE_FATAL ); <nl> + } <nl> + <nl> if (! __archive_ppmd7_functions . Ppmd7_Alloc (& rar -> ppmd7_context , <nl> rar -> dictionary_size , & g_szalloc )) <nl> {
_warc_read ( struct archive_read * a , const void ** buf , size_t * bsz , int64_t * off ) <nl> return ( ARCHIVE_EOF ); <nl> } <nl>  <nl> + if ( w -> unconsumed ) { <nl> + __archive_read_consume ( a , w -> unconsumed ); <nl> + w -> unconsumed = 0U ; <nl> + } <nl> + <nl> rab = __archive_read_ahead ( a , 1U , & nrd ); <nl> if ( nrd < 0 ) { <nl> * bsz = 0U ;
setup_current_filesystem ( struct archive_read_disk * a ) <nl> if (! GetVolumePathName ( tree_current_access_path ( t ), vol , sizeof ( vol ))) { <nl> t -> current_filesystem -> remote = - 1 ; <nl> archive_set_error (& a -> archive , ARCHIVE_ERRNO_MISC , <nl> - " GetVolumePathName failed : % d ", GetLastError ()); <nl> + " GetVolumePathName failed : % d ", ( int ) GetLastError ()); <nl> return ( ARCHIVE_FAILED ); <nl> } <nl> switch ( GetDriveType ( vol )) {
static EC_KEY * ec_key_new ( ErlNifEnv * env , ERL_NIF_TERM curve_arg ) <nl> } else <nl> goto out_err ; <nl>  <nl> + if (! group ) <nl> + goto out_err ; <nl> + <nl> if ( enif_inspect_binary ( env , prime [ 2 ], & seed )) { <nl> EC_GROUP_set_seed ( group , seed . data , seed . size ); <nl> }
# define HAVE_EC <nl> # endif <nl>  <nl> -// ( test for == 1 . 1 . 1pre8 ) <nl> -# if OPENSSL_VERSION_NUMBER == ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> +// ( test for >= 1 . 1 . 1pre8 ) <nl> +# if OPENSSL_VERSION_NUMBER >= ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> && ! defined ( HAS_LIBRESSL ) \ <nl> && defined ( HAVE_EC ) <nl> // EXPERIMENTAL :
Port * erts_get_heart_port () { <nl>  <nl> for ( ix = 0 ; ix < erts_max_ports ; ix ++) { <nl> port = & erts_port [ ix ]; <nl> - /* immediate compare */ <nl> + /* only examine undead or alive ports */ <nl> + if ( port -> status & ERTS_PORT_SFLGS_DEAD ) <nl> + continue ; <nl> + /* immediate atom compare */ <nl> if ( port -> reg && port -> reg -> name == am_heart_port ) { <nl> return port ; <nl> }
gen_select_val ( LoaderState * stp , GenOpArg S , GenOpArg Fail , <nl> op -> a [ j + size ] = Fail ; <nl>  <nl> # ifdef DEBUG <nl> - for ( i = 0 ; i < size ; i ++) { <nl> + for ( i = 0 ; i < size - 1 ; i ++) { <nl> ASSERT ( op -> a [ i + 3 ]. val <= op -> a [ i + 4 ]. val ); <nl> } <nl> # endif
bool set_default_value ( const char * name , intptr_t value ) <nl> return false ; <nl>  <nl> int idx = mutt_option_index ( name ); <nl> - if (! idx ) <nl> + if ( idx < 0 ) <nl> return false ; <nl>  <nl> MuttVars [ idx ]. initial = value ;
int mutt_pattern_func ( int op , char * prompt ) <nl> simple = safe_strdup ( buf ); <nl> mutt_check_simple ( buf , sizeof ( buf ), NONULL ( SimpleSearch )); <nl>  <nl> + memset (& err , 0 , sizeof ( err )); <nl> err . data = error ; <nl> err . dsize = sizeof ( error ); <nl> if (( pat = mutt_pattern_comp ( buf , M_FULL_MSG , & err )) == NULL )
static int cmd_handle_untagged ( struct ImapData * idata ) <nl> mutt_debug ( 2 , " Handling untagged NO \ n "); <nl>  <nl> /* Display the warning message from the server */ <nl> - mutt_error ("% s ", s + 3 ); <nl> + mutt_error ("% s ", s + 2 ); <nl> } <nl>  <nl> return 0 ;
static int label_message ( HEADER * hdr , char * new ) <nl> mutt_free_list (& hdr -> env -> labels ); <nl> } <nl>  <nl> - if ( new == NULL ) <nl> - hdr -> env -> labels = NULL ; <nl> - else <nl> + if (( new != NULL ) && (* new != '\ 0 ')) <nl> { <nl> char * last , * label ; <nl> 
int mutt_index_menu ( void ) <nl> imap_allow_reopen ( Context ); <nl> # endif <nl>  <nl> - index_hint = ( Context -> vcount && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl> + index_hint = ( Context -> vcount && menu -> current >= 0 && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl>  <nl> if (( check = mx_check_mailbox ( Context , & index_hint , 0 )) < 0 ) <nl> {
void imap_quote_string ( char * dest , size_t dlen , const char * src , bool quote_back <nl> const char * s = src ; <nl>  <nl> * pt ++ = '"'; <nl> - /* save room for trailing quote - char */ <nl> - dlen -= 2 ; <nl> + /* save room for quote - chars */ <nl> + dlen -= 3 ; <nl>  <nl> for (; * s && dlen ; s ++) <nl> {
int imap_fetch_message ( MESSAGE * msg , CONTEXT * ctx , int msgno ) <nl> } <nl> } <nl>  <nl> - mutt_message _ (" Fetching message ..."); <nl> + if (! isendwin ()) <nl> + mutt_message _ (" Fetching message ..."); <nl>  <nl> cache -> uid = HEADER_DATA ( h )-> uid ; <nl> mutt_mktemp ( path );
static void * klondike_get_replies ( void * userdata ) <nl> case KLN_CMD_ABORT : <nl> // We can ' t do / check this until it ' s initialised <nl> if ( klninfo -> initialised ) { <nl> + isc = 0 ; <nl> dev = kitem -> kline . ws . dev ; <nl> wr_lock (&( klninfo -> stat_lock )); <nl> klninfo -> jobque [ dev ]. workqc = ( int )( kitem -> kline . ws . workqc );
serial_open ( const char * devpath , unsigned long baud , signed short timeout , bool p <nl> switch ( baud ) { <nl> case 0 : <nl> break ; <nl> + case 57600 : <nl> + cfsetispeed ( & my_termios , B57600 ); <nl> + cfsetospeed ( & my_termios , B57600 ); <nl> + break ; <nl> case 115200 : <nl> cfsetispeed ( & my_termios , B115200 ); <nl> cfsetospeed ( & my_termios , B115200 );
static void set_work_target ( struct work * work , int diff ) <nl> free ( htarget ); <nl> } <nl> } <nl> - memcpy ( work -> target , target , 256 ); <nl> + memcpy ( work -> target , target , 32 ); <nl> } <nl>  <nl> static void gen_stratum_work ( struct pool * pool , struct work * work )
retry : <nl> struct stat statbuf ; <nl>  <nl> strcpy ( filename , str ); <nl> + free ( str ); <nl> if (! stat ( filename , & statbuf )) { <nl> wlogprint (" File exists , overwrite ?\ n "); <nl> input = getch (); <nl> retry : <nl> goto retry ; <nl> } <nl> } <nl> + else <nl> + free ( str ); <nl> fcfg = fopen ( filename , " w "); <nl> if (! fcfg ) { <nl> wlogprint (" Cannot open or create file \ n ");
bool initiate_stratum ( struct pool * pool ) <nl> if ( pool -> sessionid ) <nl> applog ( LOG_DEBUG , " Pool % d stratum session id : % s ", pool -> pool_no , pool -> sessionid ); <nl> else <nl> - applog ( LOG_DEBUG , " Pool % d stratum session id does not exist "); <nl> + applog ( LOG_DEBUG , " Pool % d stratum session id does not exist ", pool -> pool_no ); <nl>  <nl> ret = true ; <nl> out :
void _simplelog ( int prio , const char * str , bool force ) <nl> { <nl> # ifdef HAVE_SYSLOG_H <nl> if ( use_syslog ) { <nl> - syslog ( prio , "% s ", str ); <nl> + syslog ( LOG_LOCAL0 | prio , "% s ", str ); <nl> } <nl> # else <nl> if ( 0 ) {}
struct cg_usb_tmo { <nl> struct cg_usb_info { <nl> uint8_t bus_number ; <nl> uint8_t device_address ; <nl> - int which_intinfo ; <nl> int usbstat ; <nl> bool nodev ; <nl> int nodev_count ;
static bool klondike_get_stats ( struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= slaves ; dev ++) { <nl> char * reply = SendCmdGetReply ( klncgpu , ' S ', dev , 0 , NULL ); <nl> if ( reply != NULL ) <nl> - klninfo -> status [ dev ] = *( WORKSTATUS *)( reply + 2 ); <nl> + memcpy (( void *)(&( klninfo -> status [ dev ])), reply + 2 , sizeof ( klninfo -> status [ dev ])); <nl> } <nl> wr_unlock (&( klninfo -> stat_lock )); <nl> 
bindresvport_sa ( int sd , struct sockaddr * sa ) <nl> if ( sa == NULL ) { <nl> memset (& myaddr , 0 , sizeof ( myaddr )); <nl> sa = ( struct sockaddr *)& myaddr ; <nl> + salen = sizeof ( myaddr ); <nl>  <nl> if ( getsockname ( sd , sa , & salen ) == - 1 ) <nl> return - 1 ; /* errno is correctly set */
static int tcmu_rbd_lock_break ( struct tcmu_device * dev , char ** orig_owner ) <nl> tcmu_dev_err ( dev , " Could not break lock from % s . ( Err % d )\ n ", <nl> owners [ 0 ], ret ); <nl> if ( ret == - ETIMEDOUT ) <nl> - return ret ; <nl> + goto free_owners ; <nl>  <nl> ret = - EAGAIN ; <nl> if (!* orig_owner ) {
int main ( int argc , char ** argv ) <nl> { <nl> char * rootdir = get_rootdir ( pid ); <nl>  <nl> - dd_create_basic_files ( dd , fsuid , ( rootdir && strcmp ( rootdir , "/") != 0 ) ? rootdir : NULL ); <nl> + dd_create_basic_files ( dd , fsuid , NULL ); <nl>  <nl> char source_filename [ sizeof ("/ proc /% lu / somewhat_long_name ") + sizeof ( long )* 3 ]; <nl> int source_base_ofs = sprintf ( source_filename , "/ proc /% lu / smaps ", ( long ) pid );
static off_t copyfd_sparse ( int src_fd , int dst_fd1 , int dst_fd2 , off_t size2 ) <nl> size2 -= rd ; <nl> if ( size2 < 0 ) <nl> dst_fd2 = - 1 ; <nl> +// TODO : truncate to 0 or even delete the second file <nl> +//( currently we delete the file later ) <nl> } <nl> out : <nl> 
static int open_user_core ( uid_t uid , uid_t fsuid , pid_t pid , char ** percent_valu <nl>  <nl> static bool dump_fd_info ( const char * dest_filename , char * source_filename , int source_base_ofs , uid_t uid , gid_t gid ) <nl> { <nl> - FILE * fp = fopen ( dest_filename , " w "); <nl> + FILE * fp = fopen ( dest_filename , " wx "); <nl> if (! fp ) <nl> return false ; <nl> 
GtkWidget * create_main_window ( void ) <nl> gtk_container_add ( GTK_CONTAINER ( halign ), hbox_report_delete ); <nl>  <nl> GtkWidget * hbox_help_close = gtk_hbutton_box_new (); <nl> - GtkWidget * btn_online_help = gtk_button_new_with_mnemonic ( _ (" _Online Help ")); <nl> + GtkWidget * btn_online_help = gtk_button_new_with_mnemonic ( _ (" Online _Help ")); <nl> GtkWidget * btn_close = gtk_button_new_from_stock ( GTK_STOCK_CLOSE ); <nl> gtk_box_pack_end ( GTK_BOX ( hbox_help_close ), btn_online_help , false , false , 0 ); <nl> gtk_box_pack_end ( GTK_BOX ( hbox_help_close ), btn_close , false , false , 0 );
static void server_recv_cb ( EV_P_ ev_io * w , int revents ) <nl>  <nl> if ( remote_ctx != NULL ) { <nl> if ( memcmp (& src_addr , & remote_ctx -> src_addr , sizeof ( src_addr )) <nl> - || strcmp ( addr_header , remote_ctx -> addr_header ) != 0 ) { <nl> + || remote_ctx -> addr_header_len != addr_header_len <nl> + || memcmp ( addr_header , remote_ctx -> addr_header , addr_header_len ) != 0 ) { <nl> remote_ctx = NULL ; <nl> } <nl> }
static void remote_recv_cb ( EV_P_ ev_io * w , int revents ) { <nl>  <nl> ssize_t r = recv ( remote -> fd , server -> buf , BUF_SIZE , 0 ); <nl>  <nl> - if ( verbose ) { <nl> - LOGD (" remote recv : % d byte ", ( int ) r ); <nl> - } <nl> - <nl> if ( r == 0 ) { <nl> // connection closed <nl> close_and_free_remote ( EV_A_ remote );
void MultiLineEdit :: keyPressEvent ( QKeyEvent * event ) { <nl> case Qt :: Key_Greater : <nl> moveCursor ( QTextCursor :: End ); <nl> return ; <nl> + <nl> + // modify <nl> + case Qt :: Key_D : <nl> + moveCursor ( QTextCursor :: WordRight , QTextCursor :: KeepAnchor ); <nl> + cut (); <nl> + return ; <nl> } <nl> } <nl> }
QString MultiLineEdit :: convertMircCodesToHtml ( const QString & text ) { <nl> words [ i ] = "< span style =\"" + style + "\">" + words [ i ] + "</ span >"; <nl> } <nl> } <nl> - return words . join (""); <nl> + return words . join (""). replace ("\ n ","< br />"); <nl> } <nl>  <nl> void MultiLineEdit :: on_returnPressed () {
# include " common / xX . h " <nl> # include " common / tcpdump . h " <nl> # include " common / timer . h " <nl> + <nl> + extern char SVN_Version []; <nl> + const char * svn_version ( void ); /* svn_version . c */ <nl> + <nl> # endif
exif_subchunk_parse ( SF_PRIVATE * psf , uint32_t length ) <nl> case olym_MARKER : <nl> bytesread += psf_binheader_readf ( psf , " 4 ", & dword ) ; <nl> psf_log_printf ( psf , "% M : % u \ n ", marker , dword ) ; <nl> + if ( bytesread + dword > length ) <nl> + break ; <nl> dword += ( dword & 1 ) ; <nl> bytesread += psf_binheader_readf ( psf , " j ", dword ) ; <nl> break ;
aiff_read_chanmap ( SF_PRIVATE * psf , unsigned dword ) <nl> psf_binheader_readf ( psf , " j ", dword - bytesread ) ; <nl>  <nl> if ( map_info -> channel_map != NULL ) <nl> - { size_t chanmap_size = psf -> sf . channels * sizeof ( psf -> channel_map [ 0 ]) ; <nl> + { size_t chanmap_size = SF_MIN ( psf -> sf . channels , layout_tag & 0xffff ) * sizeof ( psf -> channel_map [ 0 ]) ; <nl>  <nl> free ( psf -> channel_map ) ; <nl> 
pcm_init ( SF_PRIVATE * psf ) <nl> { int chars = 0 ; <nl>  <nl> if ( psf -> bytewidth == 0 || psf -> sf . channels == 0 ) <nl> + { psf_log_printf ( psf , " pcm_init : internal error : bytewitdh = % d , channels = % d \ n ", psf -> bytewidth , psf -> sf . channels ) ; <nl> return SFE_INTERNAL ; <nl> + } ; <nl>  <nl> psf -> blockwidth = psf -> bytewidth * psf -> sf . channels ; <nl> 
php_http_info_t * php_http_info_parse ( php_http_info_t * info , const char * pre_head <nl> } else { <nl> PHP_HTTP_INFO ( info ). request . url = php_http_url_parse_authority ( url , http - url , ~ 0 TSRMLS_CC ); <nl> } <nl> + if (! PHP_HTTP_INFO ( info ). request . url ) { <nl> + PTR_SET ( PHP_HTTP_INFO ( info ). request . method , NULL ); <nl> + return NULL ; <nl> + } <nl> } else { <nl> PTR_SET ( PHP_HTTP_INFO ( info ). request . method , NULL ); <nl> return NULL ;
OPJ_BOOL opj_jp2_read_boxhdr_char ( opj_jp2_box_t * box , <nl> opj_event_msg ( p_manager , EVT_ERROR , " Cannot handle box of undefined sizes \ n "); <nl> return OPJ_FALSE ; <nl> } <nl> - <nl> + if ( box -> length < * p_number_bytes_read ) { <nl> + opj_event_msg ( p_manager , EVT_ERROR , " Box length is inconsistent .\ n "); <nl> + return OPJ_FALSE ; <nl> + } <nl> return OPJ_TRUE ; <nl> } <nl> 
OPJ_BOOL opj_j2k_update_image_data ( opj_tcd_t * p_tcd , OPJ_BYTE * p_data , opj_im <nl> if ( ( l_offset_x0_src < 0 ) || ( l_offset_y0_src < 0 ) || ( l_offset_x1_src < 0 ) || ( l_offset_y1_src < 0 ) ){ <nl> return OPJ_FALSE ; <nl> } <nl> + /* testcase 2977 . pdf . asan . 67 . 2198 */ <nl> + if (( OPJ_INT32 ) l_width_dest < 0 || ( OPJ_INT32 ) l_height_dest < 0 ) { <nl> + return OPJ_FALSE ; <nl> + } <nl> /*-----*/ <nl>  <nl> /* Compute the input buffer offset */
int main ( int argc , char ** argv ) <nl>  <nl> io_sources . Register ( thread_mgr , true ); <nl>  <nl> - if ( io_sources . Size () > 0 || have_pending_timers ) <nl> + if ( io_sources . Size () > 0 || have_pending_timers || BifConst :: exit_only_after_terminate ) <nl> { <nl> if ( profiling_logger ) <nl> profiling_logger -> Log ();
void h2o_http2_scheduler_rebind ( h2o_http2_scheduler_node_t * parent , h2o_http2_sc <nl> decr_active_cnt ( ref -> super . _parent ); <nl> incr_active_cnt ( parent ); <nl> } <nl> + /* update the backlinks */ <nl> + ref -> super . _parent = parent ; <nl> + ref -> super . _slot = new_slot ; <nl>  <nl> if ( exclusive ) <nl> convert_to_exclusive ( parent , ref );
xmlrpc_value * rhbz_get_member ( const char * member , xmlrpc_value * xml ) <nl> xmlrpc_struct_find_value (& env , xml , member , & value ); <nl> if ( env . fault_occurred ) <nl> abrt_xmlrpc_error (& env ); <nl> + if (! value ) <nl> + error_msg_and_die (" fatal : There is no member named '% s '", member ); <nl>  <nl> return value ; <nl> }
char * strtrimch ( char * str , int ch ) <nl> while (* tmp == ch ) <nl> ++ tmp ; <nl>  <nl> - memmove ( str , tmp , strlen ( str )); <nl> + memmove ( str , tmp , strlen ( tmp )); <nl>  <nl> // Remove trailing spaces . <nl> int i = strlen ( str );
static void write_unicode ( GByteArray * bplist , gunichar2 * val , uint64_t size ) <nl> for ( i = 0 ; i < size ; i ++) <nl> byte_convert ( buff + i * sizeof ( gunichar2 ), sizeof ( gunichar2 )); <nl> write_raw_data ( bplist , BPLIST_UNICODE , buff , size ); <nl> + free ( buff ); <nl> } <nl>  <nl> static void write_array ( GByteArray * bplist , GNode * node , GHashTable * ref_table , uint8_t dict_param_size )
static void mp_resolve_color ( GVJ_t * job , gvcolor_t * color ) <nl> color -> u . rgba [ 2 ]); <nl> color -> u . index = i ; <nl> break ; <nl> + case HSVA_DOUBLE : /* TODO : implement color conversion */ <nl> + color -> u . index = 0 ; <nl> + break ; <nl> default : <nl> assert ( 0 ); /* internal error */ <nl> }
static void view_bookmarks_check ( TEXT_BUFFER_VIEW_REC * view , LINE_REC * line ) <nl> if ( new_line != NULL ) { <nl> g_hash_table_insert ( view -> bookmarks , <nl> tmp -> data , new_line ); <nl> + } else { <nl> + g_free ( tmp -> data ); <nl> } <nl> } <nl> g_slist_free ( rec . remove_list );
static int view_scroll ( TEXT_BUFFER_VIEW_REC * view , GList ** lines , int * subline , <nl> { <nl> int linecount , realcount , scroll_visible ; <nl>  <nl> + if (* lines == NULL ) <nl> + return 0 ; <nl> + <nl> /* scroll down */ <nl> scroll_visible = lines == & view -> startline ; <nl> 
static int sig_autoremove ( void ) <nl>  <nl> /* Close only logs with private messages */ <nl> logitem = log -> items -> data ; <nl> + if ( logitem -> servertag == NULL ) <nl> + continue ; <nl> + <nl> server = server_find_tag ( logitem -> servertag ); <nl> if ( logitem -> type == LOG_ITEM_TARGET && <nl> server != NULL && ! server -> ischannel (* logitem -> name ))
tls_ctx_personalise_random ( struct tls_root_ctx * ctx ) <nl> const md_kt_t * sha256_kt = md_kt_get (" SHA256 "); <nl> mbedtls_x509_crt * cert = ctx -> crt_chain ; <nl>  <nl> - if ( 0 != md_full ( sha256_kt , cert -> tbs . p , cert -> tbs . len , sha256_hash )) <nl> + if (! md_full ( sha256_kt , cert -> tbs . p , cert -> tbs . len , sha256_hash )) <nl> { <nl> msg ( M_WARN , " WARNING : failed to personalise random "); <nl> }
verify_callback ( int preverify_ok , X509_STORE_CTX * ctx ) <nl> if ( opt -> verify_export_cert ) <nl> { <nl> gc = gc_new (); <nl> - if ( tmp_file = get_peer_cert ( ctx , opt -> verify_export_cert ,& gc )) <nl> + if (( tmp_file = get_peer_cert ( ctx , opt -> verify_export_cert ,& gc ))) <nl> { <nl> setenv_str ( opt -> es , " peer_cert ", tmp_file ); <nl> }
ecma_create_arguments_object ( ecma_object_t * func_obj_p , /**< callee function */ <nl> prop_desc , <nl> false ); <nl> JERRY_ASSERT ( ecma_is_completion_value_normal_true ( completion )); <nl> + ecma_deref_ecma_string ( length_magic_string_p ); <nl>  <nl> ecma_dealloc_number ( len_p ); <nl> 
run_int_from_pos ( struct __int_data * int_data ) <nl> { <nl> const OPCODE * curr = & __program [ int_data -> pos ]; <nl> completion = __opfuncs [ curr -> op_idx ](* curr , int_data ); <nl> + <nl> + JERRY_ASSERT ( ! ecma_is_completion_value_normal ( completion ) <nl> + || ecma_is_completion_value_normal_simple_value ( completion , <nl> + ECMA_SIMPLE_VALUE_EMPTY ) ); <nl> } while ( completion . type == ECMA_COMPLETION_TYPE_NORMAL ); <nl>  <nl> if ( completion . type == ECMA_COMPLETION_TYPE_BREAK )
handle_pap ( netdissect_options * ndo , <nl>  <nl> switch ( code ) { <nl> case PAP_AREQ : <nl> + /* A valid Authenticate - Request is 6 or more octets long . */ <nl> + if ( len < 6 ) <nl> + goto trunc ; <nl> if ( length - ( p - p0 ) < 1 ) <nl> return ; <nl> ND_TCHECK (* p );
local size_t compressed_suffix ( char * nm ) <nl> if ( len > 4 ) { <nl> nm += len - 4 ; <nl> len = 4 ; <nl> - if ( strcmp ( nm , ". zip ") == 0 || strcmp ( nm , ". ZIP ") == 0 ) <nl> + if ( strcmp ( nm , ". zip ") == 0 || strcmp ( nm , ". ZIP ") == 0 || strcmp ( nm , ". spli ") == 0 ) <nl> return 4 ; <nl> } <nl> if ( len > 3 ) {
public : <nl> // base methods <nl> virtual ~ AP4_AtomParent (); <nl> AP4_List < AP4_Atom >& GetChildren () { return m_Children ; } <nl> + AP4_Result CopyChildren ( AP4_AtomParent & destination ) const ; <nl> virtual AP4_Result AddChild ( AP4_Atom * child , int position = - 1 ); <nl> virtual AP4_Result RemoveChild ( AP4_Atom * child ); <nl> virtual AP4_Result DeleteChild ( AP4_Atom :: Type type , AP4_Ordinal index = 0 );
static void start_auth_request ( PgSocket * client , const char * username ) <nl> int res ; <nl> PktBuf * buf ; <nl>  <nl> - client -> auth_user = client -> db -> auth_user ; <nl> /* have to fetch user info from db */ <nl> client -> pool = get_pool ( client -> db , client -> db -> auth_user ); <nl> if (! find_server ( client )) {
static void zone_timer ( int fd , short flg , void * arg ) <nl> z = container_of ( el , struct DNSZone , lnode ); <nl> ctx -> zone_state = 1 ; <nl> ctx -> cur_zone = z ; <nl> + ctx -> active ++; <nl> impl_query_soa_serial ( ctx , z -> zonename ); <nl> } <nl> 
static bool check_client_passwd ( PgSocket * client , const char * passwd ) <nl> const char * correct ; <nl> PgUser * user = client -> auth_user ; <nl>  <nl> + /* auth_user may be missing */ <nl> + if (! user ) { <nl> + slog_error ( client , " Password packet before auth packet ?"); <nl> + return false ; <nl> + } <nl> + <nl> /* disallow empty passwords */ <nl> if (!* passwd || !* user -> passwd ) <nl> return false ;
close_uzbl ( WebKitWebView * page , GArray * argv , GString * result ) { <nl> if ( uzbl . gui . main_window ) <nl> gtk_widget_destroy ( uzbl . gui . main_window ); <nl> else if ( uzbl . gui . plug ) <nl> - gtk_widget_destroy ( uzbl . gui . plug ); <nl> + gtk_widget_destroy ( GTK_WIDGET ( uzbl . gui . plug )); <nl>  <nl> gtk_main_quit (); <nl> }
key_to_event ( guint keyval , gint mode ) { <nl> utf_conv = g_convert ( byte , - 1 , " UTF - 8 ", " ISO - 8859 - 1 ", NULL , NULL , NULL ); <nl>  <nl> send_event ( mode == GDK_KEY_PRESS ? KEY_PRESS : KEY_RELEASE , utf_conv ? utf_conv : byte , NULL ); <nl> + g_free ( utf_conv ); <nl> } <nl> else <nl> send_event ( mode == GDK_KEY_PRESS ? KEY_PRESS : KEY_RELEASE ,
request_starting_cb ( WebKitWebView * view , WebKitWebFrame * frame , WebKitWebResour <nl>  <nl> g_object_ref ( decision -> request ); <nl> request_decision ( uri , decision ); <nl> + <nl> + g_free ( decision ); <nl> } <nl>  <nl> void
static int send_ra ( int sock , struct Interface * iface , struct in6_addr const * des <nl> flog ( LOG_WARNING , " sendmsg : % s ", strerror ( errno )); <nl> else <nl> dlog ( LOG_DEBUG , 3 , " sendmsg : % s ", strerror ( errno )); <nl> + return - 1 ; <nl> } <nl>  <nl> return 0 ;
set_interface_var ( const char * iface , <nl> if ( snprintf ( spath , sizeof ( spath ), var , iface ) >= sizeof ( spath )) <nl> return - 1 ; <nl>  <nl> + /* No path traversal */ <nl> + if ( strstr ( name , "..") || strchr ( name , '/')) <nl> + return - 1 ; <nl> + <nl> if ( access ( spath , F_OK ) != 0 ) <nl> return - 1 ; <nl> 
static int inflate ( struct mszipd_stream * zip ) { <nl>  <nl> /* read in block type */ <nl> READ_BITS ( block_type , 2 ); <nl> - D ((" block_type =% u last_block =% u ", block_type , last_block )) <nl>  <nl> if ( block_type == 0 ) { <nl> /* uncompressed block */
public : <nl> y += secVnc . height (); <nl> secPlain . move ( xPad , y ); <nl> y += secPlain . height (); <nl> + <nl> + xPad -= SECOND_COL_XPAD ; <nl> # endif <nl>  <nl> /* Render " OK " and " Cancel " buttons */
void mg_send_digest_auth_request ( struct mg_connection * c ) { <nl> " realm =\"% s \", nonce =\"% lu \"\ r \ n \ r \ n ", <nl> conn -> server -> config_options [ AUTH_DOMAIN ], <nl> ( unsigned long ) time ( NULL )); <nl> + close_local_endpoint ( conn ); <nl> } <nl>  <nl> // Use the global passwords file , if specified by auth_gpass option ,
enum { <nl>  <nl> // Macros for enabling compiler - specific checks for printf - like arguments . <nl> # undef PRINTF_FORMAT_STRING <nl> -# if _MSC_VER >= 1400 <nl> +# if defined ( _MSC_VER ) && _MSC_VER >= 1400 <nl> # include < sal . h > <nl> -# if _MSC_VER > 1400 <nl> +# if defined ( _MSC_VER ) && _MSC_VER > 1400 <nl> # define PRINTF_FORMAT_STRING ( s ) _Printf_format_string_ s <nl> # else <nl> # define PRINTF_FORMAT_STRING ( s ) __format_string s
inf_text_fixline_buffer_buffer_insert_text ( InfTextBuffer * buffer , <nl>  <nl> priv -> keep = g_realloc ( <nl> priv -> keep , <nl> - priv -> n_keep + inf_text_chunk_get_length ( chunk ) <nl> + ( priv -> n_keep + inf_text_chunk_get_length ( chunk )) * sizeof ( guint ) <nl> ); <nl>  <nl> if ( pos - buf_len < priv -> n_keep )
static st_ret_t _st_db_put ( st_driver_t drv , const char * type , const char * owner , <nl> DB_TXN * t ; <nl> st_ret_t ret ; <nl>  <nl> + if ( dbd == NULL ) { <nl> + return st_FAILED ; <nl> + } <nl> + <nl> if ( os_count ( os ) == 0 ) <nl> return st_SUCCESS ; <nl> 
ImagingNewBlock ( const char * mode , int xsize , int ysize ) <nl> if ( im -> linesize && <nl> im -> ysize > INT_MAX / im -> linesize ) { <nl> /* punt if we ' re going to overflow */ <nl> - return NULL ; <nl> + ImagingDelete ( im ); <nl> + return ( Imaging ) ImagingError_MemoryError (); <nl> } <nl>  <nl> if ( im -> ysize * im -> linesize <= 0 ) {
getfont ( PyObject * self_ , PyObject * args , PyObject * kw ) <nl> } <nl>  <nl> if ( filename && font_bytes_size <= 0 ) { <nl> + self -> font_bytes = NULL ; <nl> error = FT_New_Face ( library , filename , index , & self -> face ); <nl> } else { <nl> /* need to have allocated storage for font_bytes for the life of the object .*/
files_out : <nl> afc_file_lock ( afc , lockfile , AFC_LOCK_UN ); <nl> afc_file_close ( afc , lockfile ); <nl> lockfile = 0 ; <nl> - do_post_notification ( NP_SYNC_DID_FINISH ); <nl> + if ( cmd == CMD_BACKUP ) <nl> + do_post_notification ( NP_SYNC_DID_FINISH ); <nl> } <nl> } else { <nl> printf (" ERROR : Could not start service % s .\ n ", MOBILEBACKUP2_SERVICE_NAME );
afc_file_read ( afc_client_t client , uint64_t handle , char * data , int length , uint <nl> const int MAXIMUM_READ_SIZE = 1 << 16 ; <nl> afc_error_t ret = AFC_E_SUCCESS ; <nl>  <nl> - if (! client || ! client -> afc_packet || ! client -> connection || handle == 0 ) <nl> + if (! client || ! client -> afc_packet || ! client -> connection || handle == 0 || ( length < 0 )) <nl> return AFC_E_INVALID_ARGUMENT ; <nl> log_debug_msg ("% s : called for length % i \ n ", __func__ , length ); <nl> 
char * format_string ( const char * buf , int cols , int depth ) <nl> int i = 0 ; <nl> int j = 0 ; <nl>  <nl> - assert ( cols >= 0 ); <nl> + assert ( cols > 0 ); <nl> assert ( depth >= 0 ); <nl>  <nl> // Inserts new lines and tabs at appropriate locations
mobilesync_error_t mobilesync_ready_to_send_changes_from_computer ( mobilesync_cli <nl> err = MOBILESYNC_E_SUCCESS ; <nl>  <nl> out : <nl> + if ( response_type ) { <nl> + free ( response_type ); <nl> + response_type = NULL ; <nl> + } <nl> if ( msg ) { <nl> plist_free ( msg ); <nl> msg = NULL ;
static int jpc_dec_tileinit ( jpc_dec_t * dec , jpc_dec_tile_t * tile ) <nl> uint_fast32_t tmpxend ; <nl> uint_fast32_t tmpyend ; <nl> jpc_dec_cp_t * cp ; <nl> - jpc_tsfb_band_t bnds [ 64 ]; <nl> + jpc_tsfb_band_t bnds [ JPC_MAXBANDS ]; <nl> jpc_pchg_t * pchg ; <nl> int pchgno ; <nl> jpc_dec_cmpt_t * cmpt ;
int JPC_SEGPASSCNT ( int passno , int firstpassno , int numpasses , int bypass , int t <nl> } else { <nl> ret = JPC_PREC * 3 - 2 ; <nl> } <nl> - ret = JAS_MIN ( ret , numpasses - passno ); <nl> + if ( passno < numpasses ) <nl> + ret = JAS_MIN ( ret , numpasses - passno ); <nl> return ret ; <nl> } <nl> 
namespace SDDM { <nl> if ( index != - 1 ) <nl> env . insert ( s . left ( index ), s . mid ( index + 1 )); <nl> } <nl> +# else <nl> + // we strdup ' d the string before in this branch <nl> + free ( mapped ); <nl> # endif <nl> env . insert (" HOME ", pw -> pw_dir ); <nl> env . insert (" PWD ", pw -> pw_dir );
namespace SDDM { <nl> } <nl>  <nl> bool PamBackend :: closeSession () { <nl> - if ( m_pam -> isOpen ()) <nl> + if ( m_pam -> isOpen ()) { <nl> + qDebug () << "[ PAM ] Closing session "; <nl> return m_pam -> closeSession (); <nl> + } <nl> + qWarning () << "[ PAM ] Asked to close the session but it wasn ' t previously open "; <nl> return Backend :: closeSession (); <nl> } <nl> 
array_index ( PyArrayObject * v ) <nl> " one element can be converted to an index "); <nl> return NULL ; <nl> } <nl> + if ( PyArray_NDIM ( v ) != 0 ) { <nl> + if ( DEPRECATE (" converting an array with ndim > 0 to an index " <nl> + " will result in an error in the future ") < 0 ) { <nl> + return NULL ; <nl> + } <nl> + } <nl> return PyArray_DESCR ( v )-> f -> getitem ( PyArray_DATA ( v ), v ); <nl> } <nl> # endif
arr_insert ( PyObject * NPY_UNUSED ( self ), PyObject * args , PyObject * kwdict ) <nl> } else { <nl> Py_XDECREF ( values ); <nl> Py_XDECREF ( mask ); <nl> + Py_XDECREF ( array ); <nl> Py_RETURN_NONE ; <nl> } <nl> }
prepare_index ( PyArrayObject * self , PyObject * index , <nl> if ( indices [ i ]. value != PyArray_DIM ( self , used_ndim )) { <nl> static PyObject * warning ; <nl>  <nl> - char * err_msg [ 174 ]; <nl> - sprintf ( err_msg , <nl> + char err_msg [ 174 ]; <nl> + PyOS_snprintf ( err_msg , sizeof ( err_msg ), <nl> " boolean index did not match indexed array along " <nl> " dimension % d ; dimension is %" NPY_INTP_FMT <nl> " but corresponding boolean dimension is %" NPY_INTP_FMT ,
const TSS2_TCTI_INFO * tpm2_tcti_ldr_getinfo ( void ) { <nl> bool tpm2_tcti_ldr_is_tcti_present ( const char * name ) { <nl>  <nl> char path [ PATH_MAX ]; <nl> - snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so ", name ); <nl> + snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so . 0 ", name ); <nl>  <nl> void * handle = dlopen ( path , RTLD_LAZY ); <nl> if ( handle ) {
pid_t read_pid ( char * pidfile ) <nl> { <nl> FILE * f ; <nl> - long pid = 0 ; <nl> + long pid ; <nl>  <nl> if (!( f = fopen ( pidfile ," r "))) <nl> return 0 ; <nl> - fscanf ( f ,"% ld ", & pid ); <nl> + if ( fscanf ( f ,"% ld ", & pid ) != 1 ) <nl> + pid = 0 ; <nl> fclose ( f ); <nl> return pid ; <nl> }
bool M68K_getInstruction ( csh ud , const uint8_t * code , size_t code_len , MCInst * i <nl> cs_struct * handle = instr -> csh ; <nl> m68k_info * info ; <nl>  <nl> - if ( inst_info == NULL ) { <nl> + if ( handle -> printer_info == NULL ) { <nl> info = cs_mem_malloc ( sizeof ( m68k_info )); <nl> if (! info ) { <nl> handle -> errnum = CS_ERR_MEM ;
static void printPCRelOperand ( MCInst * MI , int OpNum , SStream * O ) <nl> SStream_concat ( O , "% u ", imm ); <nl> } else { <nl> if ( imm < - HEX_THRESHOLD ) <nl> - SStream_concat ( O , "- 0x % x ", - imm ); <nl> + SStream_concat ( O , "- 0x % x ", ( unsigned int )- imm ); <nl> else <nl> SStream_concat ( O , "-% u ", - imm ); <nl> }
static DecodeStatus DecodeINSVE_DF_4 ( MCInst * MI , uint32_t insn , <nl> } // else llvm_unreachable (" Invalid encoding "); <nl>  <nl> // assert ( NSize != 0 && RegDecoder != nullptr ); <nl> + if ( NSize == 0 || RegDecoder == NULL ) <nl> + return MCDisassembler_Fail ; <nl>  <nl> if ( RegDecoder == NULL ) <nl> return MCDisassembler_Fail ;
static DecodeStatus Mips64Disassembler_getInstruction ( int mode , MCInst * instr , <nl> { <nl> uint32_t Insn ; <nl>  <nl> + if ( code_len < 4 ) <nl> + // not enough data <nl> + return MCDisassembler_Fail ; <nl> + <nl> + if ( instr -> flat_insn -> detail ) { <nl> + memset ( instr -> flat_insn -> detail , 0 , sizeof ( cs_detail )); <nl> + } <nl> + <nl> DecodeStatus Result = readInstruction32 (( unsigned char *) code , & Insn , isBigEndian , false ); <nl> if ( Result == MCDisassembler_Fail ) <nl> return MCDisassembler_Fail ;
CAMLprim value caml_bytes_set ( value str , value index , value newval ) <nl> */ <nl> CAMLprim value caml_string_set ( value str , value index , value newval ) <nl> { <nl> - return caml_string_set ( str , index , newval ); <nl> + return caml_bytes_set ( str , index , newval ); <nl> } <nl>  <nl> 
static inline void drop_trailing_newlines ( char * s ) <nl> static void dorealloc ( char ** mem , size_t oldlen , size_t newlen ) <nl> { <nl> int batches ; <nl> - if ( newlen % BATCH_SIZE <= oldlen % BATCH_SIZE ) <nl> + if ( newlen <= oldlen ) <nl> return ; <nl> - batches = ( newlen % BATCH_SIZE ) + 1 ; <nl> + batches = ( newlen / BATCH_SIZE ) + 1 ; <nl> if (!* mem ) { <nl> do { <nl> * mem = malloc ( batches * BATCH_SIZE );
HwProbe :: hd2value ( hd_t * hd ) <nl> out -> add ( YCPString (" sub_vendor "), YCPString ( s )); <nl> } <nl>  <nl> + // HAL udi <nl> + <nl> + s = hd -> udi ; <nl> + if ( s ) <nl> + { <nl> + out -> add ( YCPString (" udi "), YCPString ( s )); <nl> + } <nl> + <nl> // unique key <nl>  <nl> s = hd -> unique_id ;
YEBuiltin :: toStream ( std :: ostream & str ) const <nl> std :: ostream & <nl> YEBuiltin :: toXml ( std :: ostream & str , int indent ) const <nl> { <nl> - str << "< builtin name =\"" << m_decl -> name << "\""; <nl> + str << "< builtin name =\"" << StaticDeclaration :: Decl2String ( m_decl ) << "\""; <nl>  <nl> if ( m_parameterblock != 0 ) <nl> {
__no_resolve : <nl> if (( off = lseek ( _state . s_wpafd , 0 , SEEK_CUR )) == ( off_t ) - 1 ) <nl> err ( 1 , " lseek ()"); <nl>  <nl> + if ( lseek ( _state . s_wpafd , 0 , SEEK_SET ) == ( off_t ) - 1 ) <nl> + err ( 1 , " lseek ()"); <nl> + <nl> while ( tot ) { <nl> int l = tot ; <nl> 
static int net_get_nopacket ( struct priv_net * pn , void * arg , int * len ) <nl> while ( 1 ) { <nl> l = sizeof ( buf ); <nl> c = net_get ( pn -> pn_s , buf , & l ); <nl> + if ( c < 0 ) <nl> + return c ; <nl>  <nl> if ( c != NET_PACKET && c > 0 ) <nl> break ;
static int resolve_deps ( const char * src ) <nl> if ( strstr ( buf , " not regular file ")) <nl> break ; <nl>  <nl> + if ( strstr ( buf , " cannot read header ")) <nl> + break ; <nl> + <nl> + if ( strstr ( buf , destrootdir )) <nl> + break ; <nl> + <nl> p = strstr ( buf , "/"); <nl> if ( p ) { <nl> int r ;
int main ( int argc , char ** argv ) <nl> info . subusers . erase ( uiter ); <nl> if ( purge_keys ) { <nl> map < string , RGWAccessKey > * keys_map ; <nl> - access_key = subuser ; <nl> + access_key = info . user_id ; <nl> access_key . append (":"); <nl> access_key . append ( subuser ); <nl> keys_map = & info . swift_keys ;
extern " C " int ceph_create ( struct ceph_mount_info ** cmount , const char * const i <nl> extern " C " void ceph_shutdown ( struct ceph_mount_info * cmount ) <nl> { <nl> cmount -> shutdown (); <nl> + delete cmount ; <nl> } <nl>  <nl> extern " C " int ceph_conf_read_file ( struct ceph_mount_info * cmount , const char * path )
bool Monitor :: ms_get_authorizer ( int dest_type , AuthAuthorizer ** authorizer , bool <nl> } <nl>  <nl> CephXTicketBlob blob ; <nl> - ret = cephx_build_service_ticket_blob ( info , blob ); <nl> - if ( ret < 0 ) <nl> + if (! cephx_build_service_ticket_blob ( info , blob )) <nl> return false ; <nl> bufferlist ticket_data ; <nl> :: encode ( blob , ticket_data );
void Server :: handle_client_readdir ( MDRequest * mdr ) <nl> if (! dir -> is_complete ()) { <nl> if ( dir -> is_frozen ()) { <nl> dout ( 7 ) << " dir is frozen " << * dir << dendl ; <nl> + mds -> locker -> drop_locks ( mdr ); <nl> + mdr -> drop_local_auth_pins (); <nl> dir -> add_waiter ( CDir :: WAIT_UNFREEZE , new C_MDS_RetryRequest ( mdcache , mdr )); <nl> return ; <nl> }
public : <nl> set_tid ( rtid ); <nl> } <nl> MOSDSubOp () {} <nl> + private : <nl> + ~ MOSDSubOp () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " osd_sub_op "; } <nl> void print ( ostream & out ) { <nl> out << " osd_sub_op (" << reqid
private : <nl> item_dirty_dirfrag_nest ( this ), <nl> item_dirty_dirfrag_dirfragtree ( this ), <nl> auth_pins ( 0 ), nested_auth_pins ( 0 ), <nl> + auth_pin_freeze_allowance ( 0 ), <nl> nested_anchors ( 0 ), <nl> pop ( ceph_clock_now ( g_ceph_context )), <nl> versionlock ( this , & versionlock_type ),
void Elector :: start () <nl> return ; <nl> } <nl> dout ( 5 ) << " start -- can i be leader ?" << dendl ; <nl> + <nl> + acked_me . clear (); <nl>  <nl> // start by trying to elect me <nl> if ( epoch % 2 == 0 )
public : <nl> uint64_t get_required_features () const { <nl> return required_features ; <nl> } <nl> + mon_feature_t get_required_mon_features () const { <nl> + return monmap -> get_required_features (); <nl> + } <nl> void apply_quorum_to_compatset_features (); <nl> void apply_compatset_features_to_quorum_requirements (); <nl> 
ssize_t AsyncConnection :: _try_send ( bool send , bool more ) <nl> // trim already sent for outcoming_bl <nl> if ( sent_bytes ) { <nl> if ( sent_bytes < outcoming_bl . length ()) { <nl> - bufferlist bl ; <nl> - outcoming_bl . splice ( sent_bytes , outcoming_bl . length ()- sent_bytes , & bl ); <nl> - bl . swap ( outcoming_bl ); <nl> + outcoming_bl . splice ( 0 , sent_bytes ); <nl> } else { <nl> outcoming_bl . clear (); <nl> }
class MonMap { <nl> void calc_ranks () { <nl> rank_name . resize ( mon_addr . size ()); <nl> rank_addr . resize ( mon_addr . size ()); <nl> + addr_name . clear (); <nl> for ( map < string , entity_addr_t >:: iterator p = mon_addr . begin (); <nl> p != mon_addr . end (); <nl> p ++) {
inline ostream & operator <<( ostream & out , const sockaddr_storage & ss ) <nl>  <nl> inline ostream & operator <<( ostream & out , const sockaddr_in & ss ) <nl> { <nl> - char buf [ NI_MAXHOST ]; <nl> + char buf [ NI_MAXHOST ] = { 0 }; <nl> getnameinfo (( struct sockaddr *)& ss , sizeof ( ss ), buf , sizeof ( buf ), 0 , 0 , NI_NUMERICHOST ); <nl> return out << buf ; <nl> }
void PG :: _compare_scrubmaps ( const map < int , ScrubMap *> & maps , <nl> set < int > cur_missing ; <nl> set < int > cur_inconsistent ; <nl> for ( j = maps . begin (); j != maps . end (); ++ j ) { <nl> + if ( j == auth ) <nl> + continue ; <nl> if ( j -> second -> objects . count (* k )) { <nl> // Compare <nl> stringstream ss ;
public : <nl> */ <nl> void get_leaves_under ( frag_t x , std :: list < frag_t >& ls ) const { <nl> std :: list < frag_t > q ; <nl> - q . push_back ( get_branch ( x )); <nl> + q . push_back ( get_branch_or_leaf ( x )); <nl> while (! q . empty ()) { <nl> frag_t t = q . front (); <nl> q . pop_front ();
void JournalingObjectStore :: _op_journal_transactions ( list < ObjectStore :: Transacti <nl> data_len = t -> get_data_length (); <nl> data_align = ( t -> get_data_alignment () - tbl . length ()) & ~ CEPH_PAGE_MASK ; <nl> } <nl> - t -> encode ( tbl ); <nl> + :: encode (* t , tbl ); <nl> } <nl> journal -> submit_entry ( op , tbl , data_align , onjournal ); <nl> } else if ( onjournal )
void CInode :: validate_disk_state ( CInode :: validated_data * results , <nl> } else if ( fin ) { <nl> fin -> complete ( get_rval ()); <nl> } <nl> + delete this ; <nl> } <nl> }; <nl> 
Context * create_async_context_callback ( I & image_ctx , Context * on_finish ) { <nl> image_ctx . op_work_queue , on_finish ); <nl> } <nl>  <nl> + template < typename WQ > <nl> + Context * create_async_context_callback ( WQ * work_queue , Context * on_finish ) { <nl> + // use async callback to acquire a clean lock context <nl> + return new detail :: C_AsyncCallback < WQ >( work_queue , on_finish ); <nl> +} <nl> + <nl> // TODO : temporary until AioCompletion supports templated ImageCtx <nl> inline ImageCtx * get_image_ctx ( ImageCtx * image_ctx ) { <nl> return image_ctx ;
CInode * Server :: rdlock_path_pin_ref ( MDRequest * mdr , bool want_auth ) <nl> // open ref inode <nl> CInode * ref = 0 ; <nl> if ( trace . empty ()) <nl> - ref = mdcache -> get_root (); <nl> + ref = mdcache -> get_inode ( refpath . get_ino ()); <nl> else { <nl> CDentry * dn = trace [ trace . size ()- 1 ]; <nl> 
int FileJournal :: prepare_single_write ( bufferlist & bl , off64_t & queue_pos , uint64 <nl>  <nl> // add it this entry <nl> entry_header_t h ; <nl> + memset (& h , 0 , sizeof ( h )); <nl> h . seq = seq ; <nl> h . pre_pad = pre_pad ; <nl> h . len = ebl . length ();
int aio_bench ( Rados & rados , rados_pool_t pool , int secondsToRun , int concurrenti <nl> time (& initialTime ); <nl> stringstream initialTimeS (""); <nl> initialTimeS << initialTime ; <nl> - const char * iTime = initialTimeS . str (). c_str (); <nl> + char iTime [ 100 ]; <nl> + strcpy ( iTime , initialTimeS . str (). c_str ()); <nl> maxLatency . set_from_double ( 0 ); <nl> // set up writes so I can start them together <nl> for ( int i = 0 ; i < concurrentios ; ++ i ) {
void PG :: scrub () <nl> ss << ", " << fixed << " fixed "; <nl> osd -> get_logclient ()-> log ( errors ? LOG_ERROR : LOG_INFO , ss ); <nl>  <nl> - if (!( errors - fixed ) && repair ) <nl> + if ( errors == 0 || ( repair && ( errors - fixed ) == 0 )) <nl> state_clear ( PG_STATE_INCONSISTENT ); <nl> state_clear ( PG_STATE_REPAIR ); <nl> 
void Server :: early_reply ( MDRequest * mdr , CInode * tracei , CDentry * tracedn ) <nl> mds -> logger -> inc ( l_mds_reply ); <nl> double lat = g_clock . now () - mdr -> client_request -> get_recv_stamp (); <nl> mds -> logger -> favg ( l_mds_replyl , lat ); <nl> - dout ( 0 ) << " lat " << lat << dendl ; <nl> + dout ( 20 ) << " lat " << lat << dendl ; <nl> } <nl>  <nl> /*
int ceph_do_lookup ( struct super_block * sb , struct dentry * dentry , int mask ) <nl> struct ceph_mds_request_head * rhead ; <nl> int err ; <nl>  <nl> + if ( dentry -> d_name . len > NAME_MAX ) <nl> + return - ENAMETOOLONG ; <nl> + <nl> dout ( 10 , " do_lookup % p mask % d \ n ", dentry , CEPH_STAT_MASK_INODE_ALL ); <nl> path = ceph_build_dentry_path ( dentry , & pathlen ); <nl> if ( IS_ERR ( path ))
void Locker :: xlock_finish ( SimpleLock * lock , Mutation * mut ) <nl> lock -> get_num_client_lease () == 0 ) { <nl> assert (! lock -> is_stable ()); <nl> lock -> get_parent ()-> auth_unpin ( lock ); <nl> - lock -> set_state ( LOCK_LOCK ); <nl> + if ( lock -> get_type () != CEPH_LOCK_DN && (( CInode *) lock -> get_parent ())-> get_loner () >= 0 ) <nl> + lock -> set_state ( LOCK_EXCL ); <nl> + else <nl> + lock -> set_state ( LOCK_LOCK ); <nl> } <nl>  <nl> // others waiting ?
reprotect_and_return_err : <nl> req_comp ); <nl> } else { <nl> if ( ictx -> cct -> _conf -> rbd_skip_partial_discard ) { <nl> + delete req_comp ; <nl> continue ; <nl> } <nl> req = new AioZero ( ictx , p -> oid . name , p -> objectno , p -> offset , p -> length ,
PG :: RecoveryState :: Peering :: react ( const AdvMap & advmap ) { <nl> dout ( 10 ) << " Peering advmap " << dendl ; <nl> if ( pg -> prior_set_affected (* prior_set . get (), & advmap . osdmap )) { <nl> dout ( 1 ) << " Peering , priors_set_affected , going to Reset " << dendl ; <nl> + pg -> state_clear ( PG_STATE_PEERING ); <nl> post_event ( advmap ); <nl> return transit < Reset >(); <nl> }
namespace ceph { <nl> assert ( s == SECSuccess ); <nl> } <nl> void Update ( const byte * input , size_t length ) { <nl> - SECStatus s ; <nl> - s = PK11_DigestOp ( ctx , input , length ); <nl> - assert ( s == SECSuccess ); <nl> + if ( length ) { <nl> + SECStatus s ; <nl> + s = PK11_DigestOp ( ctx , input , length ); <nl> + assert ( s == SECSuccess ); <nl> + } <nl> } <nl> void Final ( byte * digest ) { <nl> SECStatus s ;
void PrimaryLogPG :: on_shutdown () <nl> cancel_log_updates (); <nl> // we must remove PGRefs , so do this this prior to release_backoffs () callers <nl> clear_backoffs (); <nl> + // clean up snap trim references <nl> + snap_trimmer_machine . process_event ( Reset ()); <nl>  <nl> pgbackend -> on_change (); <nl> 
void MgrMonitor :: drop_active () <nl> pending_map . active_gid = 0 ; <nl> pending_map . available = false ; <nl> pending_map . active_addr = entity_addr_t (); <nl> + <nl> + // So that when new active mgr subscribes to mgrdigest , it will <nl> + // get an immediate response instead of waiting for next timer <nl> + cancel_timer (); <nl> } <nl>  <nl> void MgrMonitor :: drop_standby ( uint64_t gid )
int rgw_bucket_init_index ( cls_method_context_t hctx , bufferlist * in , bufferlist <nl>  <nl> if ( header_bl . length () != 0 ) { <nl> CLS_LOG (" ERROR : index already initialized \ n "); <nl> + return - EINVAL ; <nl> } <nl>  <nl> rgw_bucket_dir dir ;
int RGWPostObj_ObjStore :: read_form_part_header ( struct post_form_part * const part <nl> } <nl>  <nl> r = read_line ( bl , chunk_size , reached_boundary , done ); <nl> + if ( r < 0 ) { <nl> + return r ; <nl> + } <nl> } <nl>  <nl> return 0 ;
int namespace_add ( cls_method_context_t hctx , bufferlist * in , bufferlist * out ) <nl> } <nl>  <nl> /** <nl> - * Add a namespace to the namespace directory . <nl> + * Remove a namespace from the namespace directory . <nl> * <nl> * Input : <nl> * @ param name the name of the namespace
void PG :: start_peering_interval ( <nl> state_clear ( PG_STATE_REMAPPED ); <nl>  <nl> int role = osdmap -> calc_pg_role ( osd -> whoami , acting , acting . size ()); <nl> - if ( role == pg_whoami . shard ) <nl> + if ( pool . info . is_replicated () || role == pg_whoami . shard ) <nl> set_role ( role ); <nl> else <nl> set_role (- 1 );
static int do_disk_usage ( librbd :: RBD & rbd , librados :: IoCtx & io_ctx , <nl> ++ count ; <nl> } <nl> } <nl> - if (! found ) { <nl> + if ( imgname != nullptr && ! found ) { <nl> std :: cerr << " specified image " << imgname << " is not found ." << std :: endl ; <nl> return - ENOENT ; <nl> }
int buffer :: list :: read_file ( const char * fn ) <nl> :: fstat ( fd , & st ); <nl> int s = ROUND_UP_TO ( st . st_size , PAGE_SIZE ); <nl> bufferptr bp = buffer :: create_page_aligned ( s ); <nl> + bp . set_length ( st . st_size ); <nl> append ( bp ); <nl> :: read ( fd , ( void *) c_str (), length ()); <nl> :: close ( fd );
int Journal < I >:: remove ( librados :: IoCtx & io_ctx , const std :: string & image_id ) { <nl> return r ; <nl> } <nl>  <nl> - r = journaler . remove ( false ); <nl> + r = journaler . remove ( true ); <nl> if ( r < 0 ) { <nl> lderr ( cct ) << " failed to remove journal : " << cpp_strerror ( r ) << dendl ; <nl> return r ;
string render_log_object_name ( const string & format , <nl> break ; <nl>  <nl> case ' i ': <nl> - sprintf ( buf , "% lld ", bucket_id ); <nl> + sprintf ( buf , "% lld ", ( long long ) bucket_id ); <nl> break ; <nl> case ' n ': <nl> o += bucket_name ;
int OSD :: mkfs ( const char * dev , ceph_fsid fsid , int whoami ) <nl> int OSD :: peek_super ( const char * dev , nstring & magic , ceph_fsid & fsid , int & whoami ) <nl> { <nl> ObjectStore * store = create_object_store ( dev ); <nl> + if (! store ) <nl> + return - ENODEV ; <nl> int err = store -> mount (); <nl> if ( err < 0 ) <nl> return err ;
void PG :: do_peer ( ObjectStore :: Transaction & t , list < Context *>& tfin , <nl> osd -> queue_generate_backlog ( this ); <nl> return ; <nl> } <nl> - for ( unsigned i = 0 ; i < acting . size (); i ++) { <nl> + for ( unsigned i = 1 ; i < acting . size (); i ++) { <nl> int o = acting [ i ]; <nl> Info & pi = peer_info [ o ]; <nl> if ( pi . last_complete < pi . log_tail && ! pi . log_backlog &&
bool MDS :: _dispatch ( Message * m ) <nl> } <nl>  <nl> switch ( m -> get_type ()) { <nl> + <nl> + case CEPH_MSG_MON_MAP : <nl> + delete m ; <nl> + break ; <nl> + <nl> // MDS <nl> case CEPH_MSG_MDS_MAP : <nl> handle_mds_map (( MMDSMap *) m );
void Server :: _rename_prepare ( MDRequestRef & mdr , <nl> } <nl> if ( tpi ) { <nl> tpi -> ctime = mdr -> get_op_stamp (); <nl> + destdn -> make_path_string ( tpi -> stray_prior_path ); <nl> tpi -> nlink --; <nl> if ( tpi -> nlink == 0 ) <nl> oldin -> state_set ( CInode :: STATE_ORPHAN );
PG :: RecoveryState :: Stray :: Stray ( my_context ctx ) <nl> assert (! pg -> is_active ()); <nl> assert (! pg -> is_peering ()); <nl> assert (! pg -> is_primary ()); <nl> - pg -> state_set ( PG_STATE_PEERING ); <nl> } <nl>  <nl> boost :: statechart :: result
public : <nl> RBD (); <nl> ~ RBD (); <nl>  <nl> + // This must be dynamically allocated with new , and <nl> + // must be released with release (). <nl> + // Do not use delete . <nl> struct AioCompletion { <nl> void * pc ; <nl> AioCompletion ( void * cb_arg , callback_t complete_cb );
bool CInode :: encode_inodestat ( bufferlist & bl , Session * session , <nl> int allowed = get_caps_allowed_for_client ( client ); <nl> int issue = ( cap -> wanted () | likes ) & allowed ; <nl> cap -> issue_norevoke ( issue ); <nl> + issue = cap -> pending (); <nl> cap -> set_last_issue_stamp ( g_clock . recent_now ()); <nl> cap -> touch (); // move to back of session cap LRU <nl> e . cap . caps = issue ;
protected : <nl> public : <nl> Message () { <nl> memset (& header , 0 , sizeof ( header )); <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> }; <nl> Message ( int t ) { <nl> memset (& header , 0 , sizeof ( header )); <nl> header . type = t ; <nl> header . priority = 0 ; // undef <nl> header . data_off = 0 ; <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> } <nl> virtual ~ Message () { } <nl> 
struct MOSDScrub : public Message { <nl> MOSDScrub () {} <nl> MOSDScrub ( ceph_fsid & f ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> - fsid ( f ) {} <nl> + fsid ( f ), repair ( false ) {} <nl> MOSDScrub ( ceph_fsid & f , vector < pg_t >& pgs , bool r ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> fsid ( f ), scrub_pgs ( pgs ), repair ( r ) {}
void RGWAbortMultipart :: execute () <nl> // and also remove the metadata obj <nl> op_ret = del_op . delete_obj (); <nl> if ( op_ret == - ENOENT ) { <nl> - op_ret = - ERR_NO_SUCH_BUCKET ; <nl> + op_ret = - ERR_NO_SUCH_UPLOAD ; <nl> } <nl> } <nl> 
void ReplicatedPG :: handle_watch_timeout ( WatchRef watch ) <nl>  <nl> // obc ref swallowed by repop ! <nl> simple_repop_submit ( repop ); <nl> + <nl> + // apply new object state . <nl> + ctx -> obc -> obs = ctx -> new_obs ; <nl> } <nl>  <nl> ObjectContextRef ReplicatedPG :: create_object_context ( const object_info_t & oi ,
int RGWRados :: flush_read_list ( struct get_obj_data * d ) <nl> bufferlist & bl = * iter ; <nl> r = d -> client_cb -> handle_data ( bl , 0 , bl . length ()); <nl> if ( r < 0 ) { <nl> - dout ( 0 ) << " ERROR : flush_read_list (): d -> client_c -> handle_data () returned " << r << dendl ; <nl> + dout ( 0 ) << " ERROR : flush_read_list (): d -> client_cb -> handle_data () returned " << r << dendl ; <nl> break ; <nl> } <nl> }
void OSD :: start_recovery_op ( PG * pg , const hobject_t & soid ) <nl>  <nl> void OSD :: finish_recovery_op ( PG * pg , const hobject_t & soid , bool dequeue ) <nl> { <nl> + recovery_wq . lock (); <nl> dout ( 10 ) << " finish_recovery_op " << * pg << " " << soid <nl> << " dequeue =" << dequeue <nl> << " (" << recovery_ops_active << "/" << g_conf -> osd_recovery_max_active << " rops )" <nl> << dendl ; <nl> - recovery_wq . lock (); <nl>  <nl> // adjust count <nl> recovery_ops_active --;
void ObjectCopyRequest < I >:: send_update_object_map () { <nl> m_local_image_ctx -> snap_lock . put_read (); <nl> finish ( 0 ); <nl> return ; <nl> + } else if ( m_local_image_ctx -> object_map == nullptr ) { <nl> + // possible that exclusive lock was lost in background <nl> + derr << ": object map is not initialized " << dendl ; <nl> + <nl> + m_local_image_ctx -> snap_lock . put_read (); <nl> + finish (- EINVAL ); <nl> + return ; <nl> } <nl>  <nl> assert ( m_local_image_ctx -> object_map != nullptr );
void ReplicatedPG :: sub_op_modify_ondisk ( MOSDSubOp * op , int ackerosd , eversion_t <nl> commit -> set_pg_complete_thru ( last_complete ); <nl> commit -> set_peer_stat ( osd -> get_my_stat_for ( g_clock . now (), ackerosd )); <nl> osd -> messenger -> send_message ( commit , osd -> osdmap -> get_inst ( ackerosd )); <nl> - delete op ; <nl> } <nl> + <nl> + delete op ; <nl> } <nl>  <nl> void ReplicatedPG :: sub_op_modify_reply ( MOSDSubOpReply * r )
sr_t * CInode :: project_snaprealm ( snapid_t snapid ) <nl> } else { <nl> new_srnode = new sr_t (); <nl> new_srnode -> created = snapid ; <nl> - new_srnode -> current_parent_since = snapid ; <nl> + new_srnode -> current_parent_since = get_oldest_snap (); <nl> } <nl> dout ( 10 ) << " project_snaprealm " << new_srnode << dendl ; <nl> projected_nodes . back ()-> snapnode = new_srnode ;
done : <nl> handler -> put_op ( op ); <nl> rgwstore -> destroy_context ( s -> obj_ctx ); <nl> FCGX_Finish_r ( fcgx ); <nl> - delete req ; <nl>  <nl> dout ( 1 ) << "====== req done req =" << hex << req << dec << " http_status =" << http_ret << " ======" << dendl ; <nl> + delete req ; <nl> } <nl>  <nl> class C_InitTimeout : public Context {
void take_min_markers ( IterIn first , IterIn last , IterOut dest ) <nl> } <nl> } <nl>  <nl> +} // anonymous namespace <nl> + <nl> class DataLogTrimCR : public RGWCoroutine { <nl> RGWRados * store ; <nl> RGWHTTPManager * http ; <nl> int DataLogTrimPollCR :: operate () <nl> return 0 ; <nl> } <nl>  <nl> -} // anonymous namespace <nl> - <nl> RGWCoroutine * create_data_log_trim_cr ( RGWRados * store , <nl> RGWHTTPManager * http , <nl> int num_shards , utime_t interval )
struct C_MDC_TruncateFinish : public Context { <nl>  <nl> void MDCache :: _truncate_inode ( CInode * in , LogSegment * ls ) <nl> { <nl> - inode_t * pi = in -> get_projected_inode (); <nl> + inode_t * pi = & in -> inode ; <nl> dout ( 10 ) << " _truncate_inode " <nl> << pi -> truncate_from << " -> " << pi -> truncate_size <nl> << " on " << * in << dendl ;
class MHeartbeat : public Message { <nl> this -> load = load ; <nl> this -> beat = beat ; <nl> } <nl> + private : <nl> + ~ MHeartbeat () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " HB "; } <nl>  <nl> void encode_payload () {
void ceph_handle_caps ( struct ceph_mds_client * mdsc , <nl> up_write (& mdsc -> snap_rwsem ); <nl> check_caps = 1 ; /* we may have sent a RELEASE to the old auth */ <nl> goto done ; <nl> - <nl> } <nl>  <nl> /* preallocate space for xattrs ? */ <nl> bad : <nl> return ; <nl>  <nl> release : <nl> + up_write (& mdsc -> snap_rwsem ); <nl> send_cap_msg ( mdsc , vino . ino , CEPH_CAP_OP_RELEASE , <nl> 0 , 0 , 0 , <nl> seq , 0 ,
public : <nl> monc_lock (" MonClient :: monc_lock "), <nl> timer ( monc_lock ), <nl> hunting ( false ), <nl> + want_monmap ( false ), <nl> + want_keys ( 0 ), <nl> mounting ( 0 ), mount_err ( 0 ), <nl> auth ( NULL ) { } <nl> ~ MonClient () {
const char * ceph_osd_flag_name ( unsigned flag ) <nl> case CEPH_OSD_FLAG_READ : return " read "; <nl> case CEPH_OSD_FLAG_WRITE : return " write "; <nl> case CEPH_OSD_FLAG_ORDERSNAP : return " ordersnap "; <nl> + case CEPH_OSD_FLAG_PEERSTAT_OLD : return " peerstat_old "; <nl> + case CEPH_OSD_FLAG_BALANCE_READS : return " balance_reads "; <nl> + case CEPH_OSD_FLAG_PARALLELEXEC : return " parallelexec "; <nl> case CEPH_OSD_FLAG_PGOP : return " pgop "; <nl> case CEPH_OSD_FLAG_EXEC : return " exec "; <nl> case CEPH_OSD_FLAG_EXEC_PUBLIC : return " exec_public ";
static int do_bench_write ( librbd :: Image & image , uint64_t io_size , <nl>  <nl> printf (" SEC OPS OPS / SEC BYTES / SEC \ n "); <nl> uint64_t off ; <nl> - for ( off = 0 ; off < io_bytes ; off += io_size ) { <nl> + for ( off = 0 ; off < io_bytes ; ) { <nl> b . wait_for ( io_threads - 1 ); <nl> i = 0 ; <nl> while ( i < io_threads && off < io_bytes &&
OPTION ( osd_mon_shutdown_timeout , OPT_DOUBLE , 5 ) <nl> OPTION ( osd_max_object_size , OPT_U64 , 100 * 1024L * 1024L * 1024L ) // OSD ' s maximum object size <nl> OPTION ( osd_max_attr_size , OPT_U64 , 0 ) <nl>  <nl> - OPTION ( filestore , OPT_BOOL , false ) <nl> - <nl> /// filestore wb throttle limits <nl> OPTION ( filestore_wbthrottle_enable , OPT_BOOL , true ) <nl> OPTION ( filestore_wbthrottle_btrfs_bytes_start_flusher , OPT_U64 , 41943040 )
void CDir :: _omap_fetched ( bufferlist & hdrbl , map < string , bufferlist >& omap , <nl>  <nl> void CDir :: go_bad () <nl> { <nl> + if ( get_version () == 0 ) <nl> + set_version ( 1 ); <nl> state_set ( STATE_BADFRAG ); <nl> // mark complete , ! fetching <nl> mark_complete (); <nl> state_clear ( STATE_FETCHING ); <nl> auth_unpin ( this ); <nl> - <nl> + <nl> // kick waiters <nl> finish_waiting ( WAIT_COMPLETE , 0 ); <nl> }
class MMDSMap : public Message { <nl> epoch = mm -> get_epoch (); <nl> mm -> encode ( encoded ); <nl> } <nl> + private : <nl> + ~ MMDSMap () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " mdsmap "; } <nl> void print ( ostream & out ) { <nl> out << " mdsmap ( e " << epoch << ")";
int execute ( const po :: variables_map & vm ) { <nl> return r ; <nl> } <nl>  <nl> + io_ctx . set_osdmap_full_try (); <nl> + <nl> librbd :: RBD rbd ; <nl> r = do_delete ( rbd , io_ctx , image_name . c_str (), <nl> vm [ at :: NO_PROGRESS ]. as < bool >());
class FileLock : public SimpleLock { <nl> } <nl> bool can_rdlock_soon () { <nl> if ( parent -> is_auth ()) <nl> - return ( state == LOCK_GLOCKL ); <nl> + return <nl> + ( state == LOCK_GLOCKL ) || <nl> + ( state == LOCK_LOCK && xlock_by ); <nl> else <nl> return false ; <nl> }
void Mgr :: init () <nl> cluster_state . notify_osdmap ( osd_map ); <nl> }); <nl>  <nl> + // Subscribe to OSDMap update to pass on to ClusterState <nl> + objecter -> maybe_request_map (); <nl> + <nl> monc -> sub_want (" mgrdigest ", 0 , 0 ); <nl>  <nl> // Prepare to receive FSMap and request it <nl> bool Mgr :: ms_dispatch ( Message * m ) <nl> m -> put (); <nl> break ; <nl> case CEPH_MSG_OSD_MAP : <nl> - <nl> handle_osd_map (); <nl>  <nl> py_modules . notify_all (" osd_map ", "");
int FileStore :: _detect_fs () <nl> dout ( 0 ) << " mount FIEMAP ioctl is disabled via ' filestore fiemap ' config option " << dendl ; <nl> ioctl_fiemap = false ; <nl> } <nl> + free ( fiemap ); <nl>  <nl> struct statfs st ; <nl> r = :: fstatfs ( fd , & st );
int RGWRados :: bi_list ( rgw_bucket & bucket , const string & obj_name , const string & <nl> } <nl>  <nl> ret = cls_rgw_bi_list ( bs . index_ctx , bs . bucket_obj , obj_name , marker , max , entries , is_truncated ); <nl> + if ( ret == - ENOENT ) { <nl> + * is_truncated = false ; <nl> + } <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
int ceph_setxattr ( struct dentry * dentry , const char * name , <nl> if ( strncmp ( name , " user .", 5 ) != 0 ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( _ceph_match_vir_xattr ( name ) != NULL ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> /* copy value into some pages */ <nl> nr_pages = calc_pages_for ( 0 , size ); <nl> if ( nr_pages ) {
void Server :: handle_client_rename ( MDRequestRef & mdr ) <nl> & remote_wrlocks , auth_pin_freeze )) <nl> return ; <nl>  <nl> + if (! check_access ( mdr , srcdn -> get_dir ()-> get_inode (), MAY_WRITE )) <nl> + return ; <nl> + <nl> + if (! check_access ( mdr , destdn -> get_dir ()-> get_inode (), MAY_WRITE )) <nl> + return ; <nl> + <nl> + if (! check_access ( mdr , srci , MAY_WRITE )) <nl> + return ; <nl> + <nl> if ( oldin && <nl> oldin -> is_dir () && <nl> _dir_is_nonempty ( mdr , oldin )) {
static inline std :: string pg_state_string ( int state ) <nl> if ( state & PG_STATE_SCANNING ) <nl> oss << " scanning +"; <nl> string ret ( oss . str ()); <nl> - return ( ret . length () == 0 ) ? " inactive " : ret ; <nl> + if ( ret . length () > 0 ) <nl> + ret . resize ( ret . length () - 1 ); <nl> + else <nl> + ret = " inactive "; <nl> + return ret ; <nl> } <nl>  <nl> 
struct ObjectOperation { <nl> string mname = " filter "; <nl> :: encode ( cname , osd_op . indata ); <nl> :: encode ( mname , osd_op . indata ); <nl> - :: encode ( cookie , osd_op . indata ); <nl> osd_op . indata . append ( filter ); <nl> + :: encode ( cookie , osd_op . indata ); <nl> } <nl> void add_alloc_hint ( int op , uint64_t expected_object_size , <nl> uint64_t expected_write_size ) {
int OSD :: init_op_flags ( OpRequestRef op ) <nl> } <nl> } <nl>  <nl> + if ( op -> rmw_flags == 0 ) <nl> + return - EINVAL ; <nl> + <nl> return 0 ; <nl> }
void send_mds_reconnect ( struct ceph_mds_client * mdsc , int mds ) <nl> session = __get_session ( mdsc , mds ); <nl> if ( session ) { <nl> session -> s_state = CEPH_MDS_SESSION_RECONNECTING ; <nl> + session -> s_cap_seq = 0 ; <nl>  <nl> /* estimate needed space */ <nl> len += session -> s_nr_caps *
void OSD :: build_initial_pg_history ( <nl> h -> last_epoch_split = e ; <nl> } <nl> lastmap = osdmap ; <nl> + up_primary = new_up_primary ; <nl> + acting_primary = new_acting_primary ; <nl> + up = new_up ; <nl> + acting = new_acting ; <nl> } <nl> dout ( 20 ) << __func__ << " " << debug . str () << dendl ; <nl> dout ( 10 ) << __func__ << " " << * h << " " << * pi
public : <nl> pg_temp . reset ( new map < pg_t , vector < int32_t > >(* o . pg_temp )); <nl> osd_uuid . reset ( new vector < uuid_d >(* o . osd_uuid )); <nl>  <nl> + if ( o . osd_primary_affinity ) <nl> + osd_primary_affinity . reset ( new vector < __u32 >(* o . osd_primary_affinity )); <nl> + <nl> // NOTE : this still references shared entity_addr_t ' s . <nl> osd_addrs . reset ( new addrs_s (* o . osd_addrs )); <nl> 
void OSD :: handle_osd_map ( MOSDMap * m ) <nl> << " but failed to encode full with correct crc ; requesting " <nl> << dendl ; <nl> clog -> warn () << " failed to encode map e " << e << " with expected crc \ n "; <nl> + delete o ; <nl> MMonGetOSDMap * req = new MMonGetOSDMap ; <nl> req -> request_full ( e , last ); <nl> monc -> send_mon_message ( req );
int main ( int argc , const char ** argv ) <nl> derr << " ERROR : failed initializing frontend " << dendl ; <nl> return - r ; <nl> } <nl> - fe -> run (); <nl> + r = fe -> run (); <nl> + if ( r < 0 ) { <nl> + derr << " ERROR : failed run " << dendl ; <nl> + return - r ; <nl> + } <nl>  <nl> fes . push_back ( fe ); <nl> }
void Monitor :: handle_probe ( MonOpRequestRef op ) <nl> } <nl> } <nl>  <nl> -/** <nl> - * @ todo fix this . This is going to cause trouble . <nl> - */ <nl> void Monitor :: handle_probe_probe ( MonOpRequestRef op ) <nl> { <nl> MMonProbe * m = static_cast < MMonProbe *>( op -> get_req ());
int ReplicatedPG :: do_osd_ops ( OpContext * ctx , vector < OSDOp >& ops ) <nl> case CEPH_OSD_OP_WATCH : <nl> ++ ctx -> num_write ; <nl> { <nl> + if (! obs . exists ) { <nl> + result = - ENOENT ; <nl> + break ; <nl> + } <nl> uint64_t cookie = op . watch . cookie ; <nl> bool do_watch = op . watch . flag & 1 ; <nl> entity_name_t entity = ctx -> reqid . name ;
bool CephXAuthorizer :: verify_reply ( bufferlist :: iterator & indata ) <nl> } <nl> } catch ( buffer :: error * e ) { <nl> dout ( 0 ) << " verify_authorizer_reply exception in decode_decrypt with " << session_key << dendl ; <nl> + delete e ; <nl> return false ; <nl> } <nl> 
void Server :: handle_client_setlayout ( MDRequest * mdr ) <nl> // validate layout <nl> // FIXME : only set striping parameters , for now . <nl> ceph_file_layout layout ; <nl> + memset (& layout , 0 , sizeof ( layout )); <nl>  <nl> if ( req -> head . args . setlayout . layout . fl_object_size > 0 ) <nl> layout . fl_object_size = req -> head . args . setlayout . layout . fl_object_size ;
struct MClientLease : public Message { <nl> h . first = sf ; <nl> h . last = sl ; <nl> } <nl> + private : <nl> + ~ MClientLease () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " client_lease "; } <nl> void print ( ostream & out ) { <nl> out << " client_lease ( a =" << ceph_lease_op_name ( get_action ())
OPTION ( cluster_network , OPT_STR , "") <nl> OPTION ( num_client , OPT_INT , 1 ) <nl> OPTION ( monmap , OPT_STR , "") <nl> OPTION ( mon_host , OPT_STR , "") <nl> + OPTION ( mon_dns_srv_name , OPT_STR , " ceph - mon ") <nl> OPTION ( lockdep , OPT_BOOL , false ) <nl> OPTION ( lockdep_force_backtrace , OPT_BOOL , false ) // always gather current backtrace at every lock <nl> OPTION ( run_dir , OPT_STR , "/ var / run / ceph ") // the "/ var / run / ceph " dir , created on daemon startup
void SaveVCalendar ( TNEFStruct TNEF , int isMtgReq ) { <nl> if ( isMtgReq ) { <nl> CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " MtgReq ", " ics ", filepath ); <nl> } else { <nl> - CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " calendar ", " vcf ", filepath ); <nl> + CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " calendar ", " ics ", filepath ); <nl> } <nl>  <nl> printf ("% s \ n ", ifilename );
void virtio_delete_queues ( VirtIODevice * vdev ) <nl> struct virtqueue * vq ; <nl> unsigned i ; <nl>  <nl> + if ( vdev -> info == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < vdev -> maxQueues ; i ++) { <nl> vq = vdev -> info [ i ]. vq ; <nl> if ( vq != NULL ) {
extern <nl> /* Sadly we can ' t include < malloc . h > as it causes a redefinition error */ <nl> size_t malloc_usable_size ( void *); <nl> # elif defined ( __APPLE__ ) <nl> - # include < malloc . h > <nl> + # if TARGET_OS_IPHONE <nl> + # include < malloc / malloc . h > <nl> + # else <nl> + # include < malloc . h > <nl> + # endif <nl> # else <nl> # error Do not know what to do here <nl> # endif
int der_decode_raw_bit_string ( const unsigned char * in , unsigned long inlen , <nl> blen = (( dlen - 1 ) << 3 ) - ( in [ x ++] & 7 ); <nl>  <nl> /* too many bits ? */ <nl> - if ( blen > * outlen ) { <nl> + if ( blen / 8 > * outlen ) { <nl> * outlen = blen ; <nl> return CRYPT_BUFFER_OVERFLOW ; <nl> }
ONIG_EXTERN int onigenc_unicode_apply_all_case_fold P_ (( OnigCaseFoldType flag , O <nl> addr = OnigUnicodeFolds2 + ( buk )-> index ;\ <nl> else if (( buk )-> fold_len == 3 )\ <nl> addr = OnigUnicodeFolds3 + ( buk )-> index ;\ <nl> + else \ <nl> + addr = 0 ;\ <nl> } while ( 0 ) <nl>  <nl> extern OnigCodePoint OnigUnicodeFolds1 [];
static int jpeg_size ( unsigned char * data , unsigned int data_size , <nl> return 0 ; <nl> } <nl> i += 2 ; <nl> - block_length = data [ i ] * 256 + data [ i + 1 ]; <nl> + if ( i + 1 < data_size ) <nl> + block_length = data [ i ] * 256 + data [ i + 1 ]; <nl> } <nl> } <nl> }
+/* vim : set tabstop = 8 shiftwidth = 4 softtabstop = 4 smarttab expandtab autoindent : */ <nl> /** <nl> * The following sites have various bits & pieces about PDF document <nl> * generation
uint64 ReadCodedSizeValue ( const binary * InBuffer , uint32 & BufferSize , uint64 & <nl> // ID found <nl> PossibleSizeLength = SizeIdx + 1 ; <nl> SizeBitMask >>= SizeIdx ; <nl> + <nl> + // Guard against invalid memory accesses with incomplete IDs . <nl> + if ( PossibleSizeLength > BufferSize ) <nl> + break ; <nl> + <nl> for ( SizeIdx = 0 ; SizeIdx < PossibleSizeLength ; SizeIdx ++) { <nl> PossibleSize [ SizeIdx ] = InBuffer [ SizeIdx ]; <nl> }
jv jq_next ( jq_state * jq ) { <nl> if ( opcode != ON_BACKTRACK ( DESTRUCTURE_ALT )) { <nl> jv_free ( stack_pop ( jq )); // free the input <nl> stack_push ( jq , jv_invalid_get_msg ( jq -> error )); // push the error ' s message <nl> + } else { <nl> + jv_free ( jq -> error ); <nl> } <nl> jq -> error = jv_null (); <nl> uint16_t offset = * pc ++;
int32_t ByteArray :: SetFilledLength ( int32_t filled_length ) { <nl> } <nl>  <nl> int32_t ByteArray :: Get ( int32_t index ) { <nl> + if ( index < 0 || index >= Length ()) <nl> + return - 1 ; <nl> return InternalGet ( index ) & 0xff ; <nl> } <nl> 
l_uint32 * line ; <nl> pos = ( qpos + i ) % 8 ; <nl> npx = px + xpostab [ pos ]; <nl> npy = py + ypostab [ pos ]; <nl> + if ( npx < 0 || npx >= w || npy < 0 || npy >= h ) <nl> + continue ; <nl> line = data + npy * wpl ; <nl> val = GET_DATA_BIT ( line , npx ); <nl> if ( val ) {
static char * check_dir_or_file ( const char * name ) { <nl> if ( ptr && strlen ( ptr ) == strlen ("/ firejail ")) { <nl> if ( arg_debug ) <nl> printf (" firejail exec symlink detected \ n "); <nl> + free ( actual_path ); <nl> free ( fname ); <nl> fname = NULL ; <nl> i ++; <nl> continue ; <nl> } <nl> + free ( actual_path ); <nl> } <nl>  <nl> }
void * poller (){ <nl> cmd_stdout = popen ( entry -> command , " r "); <nl> if ( cmd_stdout != NULL ) fgets ( cmd_result , 64 , cmd_stdout ); <nl> if ( is_number ( cmd_result )) result = atoll ( cmd_result ); <nl> - break ; <nl> + pclose ( cmd_stdout ); <nl> + break ; <nl> default : <nl> printf (" Unknown Action !\ n "); <nl> result = 0 ;
rsvg_filter_primitive_free ( gpointer impl ) <nl> { <nl> RsvgFilterPrimitive * primitive = impl ; <nl>  <nl> - g_string_free ( primitive -> in , TRUE ); <nl> - g_string_free ( primitive -> result , TRUE ); <nl> + if ( primitive -> in ) { <nl> + g_string_free ( primitive -> in , TRUE ); <nl> + } <nl> + <nl> + if ( primitive -> result ) { <nl> + g_string_free ( primitive -> result , TRUE ); <nl> + } <nl>  <nl> g_free ( primitive ); <nl> }
rsvg_characters_impl ( RsvgHandle * ctx , const xmlChar * ch , int len ) <nl>  <nl> if ( ctx -> priv -> currentnode ) <nl> rsvg_node_add_child ( ctx -> priv -> currentnode , node ); <nl> + <nl> + node = rsvg_node_unref ( node ); <nl> } <nl>  <nl> static void
int main ( int argc , char * argv []) <nl> " dosfslabel : labels can be no longer than 11 characters \ n "); <nl> exit ( 1 ); <nl> } <nl> - for ( i = 0 ; i < 11 ; i ++) <nl> + for ( i = 0 ; label [ i ] && i < 11 ; i ++) <nl> /* don ' t know if here should be more strict ! uppercase ( label [ i ])*/ <nl> if ( islower ( label [ i ])) { <nl> fprintf ( stderr ,
typedef int int32_t ; <nl>  <nl> # endif /* HAVE_CONFIG_H */ <nl>  <nl> - int utf8_encode ( int codepoint , char * buffer , size_t * size ); <nl> + int utf8_encode ( int32_t codepoint , char * buffer , size_t * size ); <nl>  <nl> size_t utf8_check_first ( char byte ); <nl> size_t utf8_check_full ( const char * buffer , size_t size , int32_t * codepoint );
struct iw_rr_ctx { <nl> }; <nl>  <nl>  <nl> - static IW_INLINE double iw_sinc ( double x ) <nl> + static double iw_sinc ( double x ) <nl> { <nl> - if ( x == 0 . 0 ) return 1 . 0 ; <nl> + if ( x <= 0 . 000000005 ) return 1 . 0 ; <nl> return sin ( M_PI * x )/( M_PI * x ); <nl> } <nl> 
static int iwgif_read_image ( struct iwgifrcontext * rctx ) <nl>  <nl> rctx -> image_width = ( int ) iw_get_ui16le (& rctx -> rbuf [ 4 ]); <nl> rctx -> image_height = ( int ) iw_get_ui16le (& rctx -> rbuf [ 6 ]); <nl> + if ( rctx -> image_width < 1 || rctx -> image_height < 1 ) { <nl> + iw_set_error ( rctx -> ctx , " Invalid image dimensions "); <nl> + goto done ; <nl> + } <nl>  <nl> rctx -> interlaced = ( int )(( rctx -> rbuf [ 8 ]>> 6 )& 0x01 ); <nl> 
DialInstance :: DialResult DialInstance :: execute () <nl>  <nl> void DialInstance :: prepareAddress () <nl> { <nl> - if ( mTargetUri . scheme () == Symbols :: Sip ) { <nl> + if ( mTargetUri . scheme () == Symbols :: Sip || <nl> + mTargetUri . scheme () == Symbols :: Sips ) { <nl> mFullTarget = mTargetUri ; <nl> return ; <nl> }
int ares_parse_a_reply ( const unsigned char * abuf , int alen , <nl> rr_class = DNS_RR_CLASS ( aptr ); <nl> rr_len = DNS_RR_LEN ( aptr ); <nl> aptr += RRFIXEDSZ ; <nl> + if ( aptr + rr_len > abuf + alen ) <nl> + { <nl> + free ( rr_name ); <nl> + status = ARES_EBADRESP ; <nl> + break ; <nl> + } <nl>  <nl> if ( rr_class == C_IN && rr_type == T_A <nl> && rr_len == sizeof ( struct in_addr )
int ares_init_options_with_socket_function ( ares_channel * channelptr , struct ares <nl> channel -> ndomains = - 1 ; <nl> channel -> nsort = - 1 ; <nl> channel -> lookups = NULL ; <nl> + channel -> servers = NULL ; <nl>  <nl> /* Initialize configuration by each of the four sources , from highest <nl> * precedence to lowest .
int main ( gint argc , gchar ** argv ) { <nl> bind ("/", prefix ); <nl>  <nl> fail_if ( chroot ( prefix )); <nl> + fail_if ( chdir ("/")); <nl> fail_if ( execvp (* argv , argv )); <nl> } <nl> 
char * argv [],* envp []; <nl> else <nl> program_name = argv [ 0 ]; <nl>  <nl> - if (!( eargv = ( char **) malloc (( argc + 3 ) * sizeof ( char *)))) <nl> + if (!( eargv = ( char **) malloc (( argc + 4 ) * sizeof ( char *)))) <nl> { <nl> fprintf ( stderr , "% s : Failed to obtain memory .\ n ", program_name ); <nl> exit ( 1 ); /* Trap core dump ! */
check_user_token ( const char * authfile , <nl> { <nl> if ( verbose ) <nl> D ( debug_file , " Match user / token as % s /% s ", username , otp_id ); <nl> + <nl> + fclose ( opwfile ); <nl> return AUTH_FOUND ; <nl> } <nl> }
static int create_zone_index ( const char * directory , timelib_tzdb * db ) <nl> db_index [ index_next ]. pos = data_size ; <nl> data_size += length ; <nl> free ( tzfile_data ); <nl> + <nl> + index_next ++; <nl> + } else { <nl> + free ( db_index [ index_next ]. id ); <nl> } <nl> } <nl> - <nl> - index_next ++; <nl> } <nl> } <nl> 
exit_on_inactivity ( gpointer user_data ) <nl> extern gboolean in_shutdown ; <nl>  <nl> if (! in_shutdown ) <nl> - exit ( 0 ); <nl> + { <nl> + GDBusConnection * session_bus ; <nl> + <nl> + session_bus = g_bus_get_sync ( G_BUS_TYPE_SESSION , NULL , NULL ); <nl> + g_dbus_connection_flush_sync ( session_bus , NULL , NULL ); <nl> + g_object_unref ( session_bus ); <nl> + <nl> + exit ( 0 ); <nl> + } <nl>  <nl> return FALSE ; <nl> }
S_study_chunk ( pTHX_ RExC_state_t * pRExC_state , regnode ** scanp , <nl> RExC_precomp ))); <nl> } <nl>  <nl> + if ( ( minnext > 0 && mincount >= SSize_t_MAX / minnext ) <nl> + || min >= SSize_t_MAX - minnext * mincount ) <nl> + { <nl> + FAIL (" Regexp out of space "); <nl> + } <nl> + <nl> min += minnext * mincount ; <nl> is_inf_internal |= deltanext == SSize_t_MAX <nl> || ( maxcount == REG_INFTY && minnext + deltanext > 0 );
int charconv_buffer ( const char * tocode , const char * fromcode , <nl> * result_len = result_length ; <nl> } <nl> } <nl> + else { <nl> + mmap_string_free ( mmapstr ); <nl> + } <nl> } <nl> + else { <nl> + mmap_string_free ( mmapstr ); <nl> + } <nl> return res ; <nl> } <nl> /* else , let ' s try with iconv , if available */
static Image * ReadTIFFImage ( const ImageInfo * image_info , <nl> /* <nl> Convert stripped TIFF image . <nl> */ <nl> - extent = 4 *( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> + extent = 4 *(( image -> depth + 7 )/ 8 )*( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> strip_pixels =( unsigned char *) AcquireQuantumMemory ( extent , <nl> sizeof (* strip_pixels )); <nl> if ( strip_pixels == ( unsigned char *) NULL )
rfbSendServerCutText ( rfbScreenInfoPtr rfbScreen , char * str , int len ) <nl> rfbServerCutTextMsg sct ; <nl> rfbClientIteratorPtr iterator ; <nl>  <nl> + memset (( char *)& sct , 0 , sizeof ( sct )); <nl> + <nl> iterator = rfbGetClientIterator ( rfbScreen ); <nl> while (( cl = rfbClientIteratorNext ( iterator )) != NULL ) { <nl> sct . type = rfbServerCutText ;
void rfbClientCleanup ( rfbClient * client ) { <nl> client -> clientData = next ; <nl> } <nl>  <nl> + free ( client -> vncRec ); <nl> + <nl> if ( client -> sock != RFB_INVALID_SOCKET ) <nl> rfbCloseSocket ( client -> sock ); <nl> if ( client -> listenSock != RFB_INVALID_SOCKET )
ConnectClientToUnixSock ( const char * sockFile ) <nl> int sock ; <nl> struct sockaddr_un addr ; <nl> addr . sun_family = AF_UNIX ; <nl> + if ( strlen ( sockFile ) + 1 > sizeof ( addr . sun_path )) { <nl> + rfbClientErr (" ConnectToUnixSock : socket file name too long \ n "); <nl> + return - 1 ; <nl> + } <nl> strcpy ( addr . sun_path , sockFile ); <nl>  <nl> sock = socket ( AF_UNIX , SOCK_STREAM , 0 );
im_vips2dz ( IMAGE * in , const char * filename ) <nl> * p = '\ 0 '; <nl> im_strncpy ( mode , p + 1 , FILENAME_MAX ); <nl> } <nl> + else <nl> + strcpy ( mode , "" ); <nl>  <nl> strcpy ( buf , mode ); <nl> p = & buf [ 0 ];
static void ctrycatchfinally ( JF , js_Ast * trystm , js_Ast * catchvar , js_Ast * catch <nl> emitstring ( J , F , OP_CATCH , catchvar -> string ); <nl> cstm ( J , F , catchstm ); <nl> emit ( J , F , OP_ENDCATCH ); <nl> + emit ( J , F , OP_ENDTRY ); <nl> L3 = emitjump ( J , F , OP_JUMP ); /* skip past the try block to the finally block */ <nl> } <nl> label ( J , F , L1 );
qemuProcessHandleMonitorEOF ( qemuMonitorPtr mon , <nl> /* We don ' t want this EOF handler to be called over and over while the <nl> * thread is waiting for a job . <nl> */ <nl> + virObjectLock ( mon ); <nl> qemuMonitorUnregister ( mon ); <nl> + virObjectUnlock ( mon ); <nl>  <nl> /* We don ' t want any cleanup from EOF handler ( or any other <nl> * thread ) to enter qemu namespace . */
ofproto_rule_insert__ ( struct ofproto * ofproto , struct rule * rule ) <nl> const struct rule_actions * actions = rule_get_actions ( rule ); <nl>  <nl> /* A rule may not be reinserted . */ <nl> - ovs_assert ( rule -> state == RULE_INITIALIZED ); <nl> + ovs_assert ( rule -> state != RULE_INSERTED ); <nl>  <nl> if ( rule -> hard_timeout || rule -> idle_timeout ) { <nl> ovs_list_insert (& ofproto -> expirable , & rule -> expirable );
ipf_extract_frags_from_batch ( struct ipf * ipf , struct dp_packet_batch * pb , <nl> ovs_mutex_lock (& ipf -> ipf_lock ); <nl> if (! ipf_handle_frag ( ipf , pkt , dl_type , zone , now , hash_basis )) { <nl> dp_packet_batch_refill ( pb , pkt , pb_idx ); <nl> + } else { <nl> + dp_packet_delete ( pkt ); <nl> } <nl> ovs_mutex_unlock (& ipf -> ipf_lock ); <nl> } else {
status WAVEFile :: parseFormat ( const Tag & id , uint32_t size ) <nl>  <nl> /* numCoefficients should be at least 7 . */ <nl> assert ( numCoefficients >= 7 && numCoefficients <= 255 ); <nl> + if ( numCoefficients < 7 || numCoefficients > 255 ) <nl> + { <nl> + _af_error ( AF_BAD_HEADER , <nl> + " Bad number of coefficients "); <nl> + return AF_FAIL ; <nl> + } <nl>  <nl> m_msadpcmNumCoefficients = numCoefficients ; <nl> 
_rsvg_io_get_file_path ( const gchar * filename , <nl> { <nl> gchar * absolute_filename ; <nl>  <nl> - if ( g_file_test ( filename , G_FILE_TEST_EXISTS ) || g_path_is_absolute ( filename )) { <nl> + if ( g_path_is_absolute ( filename )) { <nl> absolute_filename = g_strdup ( filename ); <nl> } else { <nl> gchar * tmpcdir ;
NS_ASSUME_NONNULL_BEGIN <nl> @ interface OTRXMPPMessageYapStroage : XMPPModule <nl>  <nl> @ property ( nonatomic , strong , readonly ) YapDatabaseConnection * databaseConnection ; <nl> +@ property ( nonatomic , readonly ) dispatch_queue_t moduleDelegateQueue ; <nl>  <nl> /** This connection is only used for readWrites */ <nl> - ( instancetype ) initWithDatabaseConnection :( YapDatabaseConnection *) databaseConnection ;
int fs_mount ( spiffs * spf , uint32_t addr , uint32_t size , uint8_t * workbuf , <nl> cfg . hal_erase_f = esp_spiffs_erase ; <nl>  <nl> if ( SPIFFS_mount ( spf , & cfg , workbuf , fds , fds_size , 0 , 0 , 0 ) != SPIFFS_OK ) { <nl> + LOG ( LL_ERROR , (" SPIFFS_mount failed : % d ", SPIFFS_errno ( spf ))); <nl> return SPIFFS_errno ( spf ); <nl> } <nl> 
static const char * stbvox_vertex_program = <nl> " uniform vec3 normal_table [ 32 ];\ n " <nl>  <nl> # ifndef STBVOX_CONFIG_OPENGL_MODELVIEW <nl> - " uniform mat44 model_view ;\ n " <nl> + " uniform mat4x4 model_view ;\ n " <nl> # endif <nl>  <nl> // fragment output data
void stb_leakcheck_free ( void * ptr ) <nl> if ( mi -> next ) <nl> mi -> next -> prev = mi -> prev ; <nl> # endif <nl> + free ( ptr ); <nl> } <nl> } <nl> 
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
static int ndbcluster_reset_logs ( THD * thd ) <nl> if (! ndb_binlog_running ) <nl> return 0 ; <nl>  <nl> + /* only reset master should reset logs */ <nl> + if (!( thd -> lex -> type & REFRESH_MASTER )) <nl> + return 0 ; <nl> + <nl> DBUG_ENTER (" ndbcluster_reset_logs "); <nl>  <nl> /*
innobase_next_autoinc ( <nl> } else { <nl> next_value = current + increment ; <nl> } <nl> - } else { <nl> + } else if ( max_value > current ) { <nl> if ( current > offset ) { <nl> next_value = (( current - offset ) / increment ) + 1 ; <nl> } else { <nl> innobase_next_autoinc ( <nl> next_value += offset ; <nl> } <nl> } <nl> + } else { <nl> + next_value = max_value ; <nl> } <nl>  <nl> ut_a ( next_value <= max_value );
innodb_initialize ( <nl>  <nl> my_eng_config = ( eng_config_info_t *) config_str ; <nl>  <nl> + /* If no call back function registered ( InnoDB engine failed to load ), <nl> + load InnoDB Memcached engine should fail too */ <nl> + if (! my_eng_config -> cb_ptr ) { <nl> + return ( ENGINE_TMPFAIL ); <nl> + } <nl> + <nl> /* Register the call back function */ <nl> register_innodb_cb (( void *) my_eng_config -> cb_ptr ); <nl> 
lock_table_locks_check ( <nl> ut_a ( table != NULL ); <nl> ut_ad ( lock_mutex_own ()); <nl>  <nl> + rw_lock_s_lock (& trx_sys -> lock ); <nl> + <nl> for ( trx = UT_LIST_GET_FIRST ( trx_sys -> trx_list ); <nl> trx != NULL ; <nl> trx = UT_LIST_GET_NEXT ( trx_list , trx )) { <nl> lock_table_locks_check ( <nl> } <nl> } <nl>  <nl> + rw_lock_s_unlock (& trx_sys -> lock ); <nl> + <nl> return ( NULL ); <nl> } <nl> # endif /* UNIV_DEBUG */
Ndb :: internalize_index_name ( const NdbTableImpl * table , <nl> if (! table ) <nl> { <nl> DBUG_PRINT (" error ", ("! table ")); <nl> - return ret ; <nl> + DBUG_RETURN ( ret ); <nl> } <nl>  <nl> if ( fullyQualifiedNames )
multi_delete :: multi_delete ( THD * thd_arg , TABLE_LIST * dt , <nl> table -> used_keys = 0 ; <nl> tempfiles [ counter ] = new Unique ( refposcmp2 , <nl> ( void *) & table -> file -> ref_length , <nl> - table -> file -> ref_length + 1 , <nl> + table -> file -> ref_length , <nl> MEM_STRIP_BUF_SIZE ); <nl> } <nl> }
__weak_alias ( vis , _vis ) <nl> # define MAXEXTRAS 5 <nl>  <nl>  <nl> - char * MAKEEXTRALIST ( uint flag , const char * orig ) <nl> + char * MAKEEXTRALIST ( unsigned int flag , const char * orig ) <nl> { <nl> const char * o = orig ; <nl> char * e , * extra ;
TABLE_COUNTER_TYPE Query_cache :: is_cacheable ( THD * thd , uint32 query_len , <nl> ( tables_used -> db_length == 5 && <nl> # ifdef FN_NO_CASE_SENCE <nl> // TODO : latin1 charset should be replaced with system charset <nl> - my_strncasecmp ( my_charset_latin1 , tables_used -> db ," mysql ", 5 ) == 0 <nl> + my_strncasecmp (& my_charset_latin1 , <nl> + tables_used -> db , <nl> + " mysql ", 5 ) == 0 <nl> # else <nl> tables_used -> db [ 0 ]==' m ' && <nl> tables_used -> db [ 1 ]==' y ' &&
Guardian :: Guardian ( Thread_registry * thread_registry_arg , <nl> monitoring_interval ( monitoring_interval_arg ), <nl> thread_registry ( thread_registry_arg ), <nl> instance_map ( instance_map_arg ), <nl> + guarded_instances ( 0 ), <nl> shutdown_requested ( FALSE ) <nl> { <nl> pthread_mutex_init (& LOCK_guardian , 0 );
inline void setup_table_map ( TABLE * table , TABLE_LIST * table_list , uint tablenr ) <nl> table -> tablenr = tablenr ; <nl> table -> map = ( table_map ) 1 << tablenr ; <nl> table -> force_index = table_list -> force_index ; <nl> + table -> force_index_order = table -> force_index_group = 0 ; <nl> table -> covering_keys = table -> s -> keys_for_keyread ; <nl> table -> merge_keys . clear_all (); <nl> }
int <nl> Tablespace_client :: get_tablespace_info ( CreateFilegroupImplReq * rep ) <nl> { <nl> Ptr < Tsman :: Tablespace > ts_ptr ; <nl> - if ( m_tsman -> m_tablespace_hash . find ( ts_ptr , m_tablespace_id )); <nl> + if ( m_tsman -> m_tablespace_hash . find ( ts_ptr , m_tablespace_id )) <nl> { <nl> rep -> tablespace . extent_size = ts_ptr . p -> m_extent_size ; <nl> rep -> tablespace . logfile_group_id =
history_save ( History * h , const char * fname ) <nl> retval = HPREV ( h , & ev ), i ++) { <nl> len = strlen ( ev . str ) * 4 ; <nl> if ( len >= max_size ) { <nl> - max_size = ( len + 1023 ) & 1023 ; <nl> + max_size = ( len + 1023 ) & ~ 1023 ; <nl> ptr = h_realloc ( ptr , max_size ); <nl> } <nl> ( void ) strvis ( ptr , ev . str , VIS_WHITE );
pstack_install_segv_action ( const char * path_format_ ) <nl> if (( abfd = load_bfd ( pid ))== 0 ) <nl> fprintf ( stderr , " BFD load failed ..\ n "); <nl> else { <nl> - long storage_needed = bfd_get_symtab_upper_bound ( abfd ); <nl> + long storage_needed = ( bfd_get_file_flags ( abfd ) & HAS_SYMS ) ? <nl> + bfd_get_symtab_upper_bound ( abfd ) : 0 ; <nl> long i ; <nl> ( void ) i ; <nl> 
static void trace_table_dependencies ( Opt_trace_context * trace , <nl> } <nl> Opt_trace_array depends_on ( trace , " depends_on_map_bits "); <nl> // RAND_TABLE_BIT may be in join_tabs [ i ]. dependent , so we test all 64 bits <nl> - compile_time_assert ( sizeof ( TABLE :: map ) <= 64 ); <nl> + compile_time_assert ( sizeof ( table -> map ) <= 64 ); <nl> for ( uint j = 0 ; j < 64 ; j ++) <nl> { <nl> if ( join_tabs [ i ]. dependent & ( 1ULL << j ))
void Dblqh :: writeSinglePage ( Signal * signal , Uint32 pageNo , <nl> signal -> theData [ 7 ] = pageNo ; <nl> sendSignal ( NDBFS_REF , GSN_FSWRITEREQ , signal , 8 , JBA ); <nl>  <nl> + if ( logFilePtr . p -> fileRef == RNIL ) <nl> + { <nl> + signal -> theData [ 0 ] = 2305 ; <nl> + execDUMP_STATE_ORD ( signal ); <nl> + } <nl> ndbrequire ( logFilePtr . p -> fileRef != RNIL ); <nl>  <nl> logPartPtr . p -> m_io_tracker . send_io ( 32768 );
MgmtSrvr :: dumpStateSelf ( const Uint32 args [], Uint32 no ) <nl> if ( no < 1 ) <nl> return - 1 ; <nl>  <nl> +# ifdef ERROR_INSERT <nl> switch ( args [ 0 ]) <nl> { <nl> case 9994 : <nl> MgmtSrvr :: dumpStateSelf ( const Uint32 args [], Uint32 no ) <nl> break ; <nl> } <nl>  <nl> -# ifdef ERROR_INSERT <nl> case 9996 : <nl> { <nl> /* Sendbuffer consumption */
int mysql_create_function ( THD * thd , udf_func * udf ) <nl> } <nl>  <nl> rw_wrlock (& THR_LOCK_udf ); <nl> - if (( hash_search (& udf_hash ,( byte *) & udf -> name . str , udf -> name . length ))) <nl> + if (( hash_search (& udf_hash ,( byte *) udf -> name . str , udf -> name . length ))) <nl> { <nl> net_printf ( thd , ER_UDF_EXISTS , udf -> name ); <nl> goto err ;
buf_get_latched_pages_number ( void ) <nl>  <nl> block = buf_pool_get_nth_block ( buf_pool , i ); <nl>  <nl> - if (( block -> buf_fix_count != 0 ) || ( block -> io_fix != 0 )) <nl> + if ((( block -> buf_fix_count != 0 ) || ( block -> io_fix != 0 )) && <nl> + block -> magic_n == BUF_BLOCK_MAGIC_N ) <nl> fixed_pages_number ++; <nl> } <nl> 
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
Prepared_statement :: execute_loop ( String * expanded_query , <nl> int reprepare_attempt = 0 ; <nl>  <nl> /* Check if we got an error when sending long data */ <nl> - if ( state == Query_arena :: ERROR ) <nl> + if ( state == Query_arena :: STMT_ERROR ) <nl> { <nl> my_message ( last_errno , last_error , MYF ( 0 )); <nl> return TRUE ;
int SetCipherList ( Suites * s , const char * list ) <nl> byte b ; <nl> byte compression ; <nl> ProtocolVersion pv ; <nl> - word16 extSz ; <nl> word32 i = * inOutIdx ; <nl> word32 begin = i ; <nl> 
int TLSX_ValidateEllipticCurves ( CYASSL * ssl , byte first , byte second ) { <nl> int sig = 0 ; /* valitade signature */ <nl> int key = 0 ; /* validate key */ <nl>  <nl> + ( void ) oid ; <nl> + ( void ) octets ; <nl> + <nl> if (! extension ) <nl> return 1 ; /* no suite restriction */ <nl> 
void AesCtrEncrypt ( Aes * aes , byte * out , const byte * in , word32 sz ) <nl> word32 blocks = sz / AES_BLOCK_SIZE ; <nl>  <nl> while ( blocks --) { <nl> - AesEncrypt ( aes , aes -> reg , out ); <nl> + AesEncrypt ( aes , ( byte *) aes -> reg , out ); <nl> IncrementAesCounter (( byte *) aes -> reg ); <nl> xorbuf ( out , in , AES_BLOCK_SIZE ); <nl> 
int SendCertificateRequest ( WOLFSSL * ssl ) <nl> /* write to output */ <nl> output [ i ++] = ( byte ) typeTotal ; /* # of types */ <nl> # ifdef HAVE_ECC <nl> - if ( ssl -> options . cipherSuite0 == ECC_BYTE && <nl> + if (( ssl -> options . cipherSuite0 == ECC_BYTE || <nl> + ssl -> options . cipherSuite0 == CHACHA_BYTE ) && <nl> ssl -> specs . sig_algo == ecc_dsa_sa_algo ) { <nl> output [ i ++] = ecdsa_sign ; <nl> } else
static int DecodePolicyOID ( char * out , word32 outSz , byte * in , word32 inSz ) <nl> # endif <nl> } <nl> idx += policy_length ; <nl> - } while (( int ) idx < total_length && cert -> extCertPoliciesNb < MAX_CERTPOL_NB ); <nl> + } while (( int ) idx < total_length <nl> + # if defined ( WOLFSSL_CERT_EXT ) <nl> + && cert -> extCertPoliciesNb < MAX_CERTPOL_NB <nl> + # endif <nl> + ); <nl>  <nl> WOLFSSL_LEAVE (" DecodeCertPolicy ", 0 ); <nl> return 0 ;
void * memchr_inv ( const void * start , int c , size_t bytes ) <nl>  <nl> value64 = value ; <nl> # if defined ( CONFIG_ARCH_HAS_FAST_MULTIPLIER ) && BITS_PER_LONG == 64 <nl> - value64 *= 0x0101010101010101 ; <nl> + value64 *= 0x0101010101010101ULL ; <nl> # elif defined ( CONFIG_ARCH_HAS_FAST_MULTIPLIER ) <nl> value64 *= 0x01010101 ; <nl> value64 |= value64 << 32 ;
static void mga_connector_destroy ( struct drm_connector * connector ) <nl> kfree ( connector ); <nl> } <nl>  <nl> - struct drm_connector_helper_funcs mga_vga_connector_helper_funcs = { <nl> + static const struct drm_connector_helper_funcs mga_vga_connector_helper_funcs = { <nl> . get_modes = mga_vga_get_modes , <nl> . mode_valid = mga_vga_mode_valid , <nl> . best_encoder = mga_connector_best_encoder , <nl> }; <nl>  <nl> - struct drm_connector_funcs mga_vga_connector_funcs = { <nl> + static const struct drm_connector_funcs mga_vga_connector_funcs = { <nl> . dpms = drm_helper_connector_dpms , <nl> . detect = mga_vga_detect , <nl> . fill_modes = drm_helper_probe_single_connector_modes ,
void * dma_generic_alloc_coherent ( struct device * dev , size_t size , <nl> void * ret , * ret_nocache ; <nl> int order = get_order ( size ); <nl>  <nl> + gfp |= __GFP_ZERO ; <nl> + <nl> ret = ( void *) __get_free_pages ( gfp , order ); <nl> if (! ret ) <nl> return NULL ; <nl>  <nl> - memset ( ret , 0 , size ); <nl> /* <nl> * Pages from the page allocator may have data present in <nl> * cache . So flush the cache before using uncached memory .
static ssize_t xenbus_file_write ( struct file * filp , <nl> goto out ; <nl>  <nl> /* Can ' t write a xenbus message larger we can buffer */ <nl> - if (( len + u -> len ) > sizeof ( u -> u . buffer )) { <nl> + if ( len > sizeof ( u -> u . buffer ) - u -> len ) { <nl> /* On error , dump existing buffer */ <nl> u -> len = 0 ; <nl> rc = - EINVAL ;
void ath6kl_destroy ( struct net_device * dev , unsigned int unregister ) <nl>  <nl> wlan_node_table_cleanup (& ar -> scan_table ); <nl>  <nl> + kfree ( ar -> fw_board ); <nl> + kfree ( ar -> fw_otp ); <nl> + kfree ( ar -> fw ); <nl> + kfree ( ar -> fw_patch ); <nl> + <nl> ath6kl_cfg80211_deinit ( ar ); <nl> }
static int xemaclite_send ( struct sk_buff * orig_skb , struct net_device * dev ) <nl> skb_tx_timestamp ( new_skb ); <nl>  <nl> dev -> stats . tx_bytes += len ; <nl> - dev_kfree_skb ( new_skb ); <nl> + dev_consume_skb_any ( new_skb ); <nl>  <nl> return 0 ; <nl> }
static void xiic_fill_tx_fifo ( struct xiic_i2c * i2c ) <nl> /* last message in transfer -> STOP */ <nl> data |= XIIC_TX_DYN_STOP_MASK ; <nl> dev_dbg ( i2c -> adap . dev . parent , "% s TX STOP \ n ", __func__ ); <nl> - <nl> - xiic_setreg16 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> - } else <nl> - xiic_setreg8 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> + } <nl> + xiic_setreg16 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> } <nl> } <nl> 
accept_err : <nl> return result ; <nl> } <nl>  <nl> - int sctp_accept_from_sock ( struct connection * con ) <nl> + static int sctp_accept_from_sock ( struct connection * con ) <nl> { <nl> /* Check that the new node is in the lockspace */ <nl> struct sctp_prim prim ;
int __init init_dmars ( void ) <nl> deferred_flush = kzalloc ( g_num_of_iommus * <nl> sizeof ( struct deferred_flush_tables ), GFP_KERNEL ); <nl> if (! deferred_flush ) { <nl> - kfree ( g_iommus ); <nl> ret = - ENOMEM ; <nl> goto error ; <nl> }
static void <nl> intel_disable_cursor_plane ( struct drm_plane * plane , <nl> struct drm_crtc * crtc ) <nl> { <nl> + struct intel_crtc * intel_crtc = to_intel_crtc ( crtc ); <nl> + <nl> + intel_crtc -> cursor_addr = 0 ; <nl> intel_crtc_update_cursor ( crtc , false ); <nl> } <nl> 
static struct usb_function * geth_alloc ( struct usb_function_instance * fi ) <nl> sizeof ( geth -> ethaddr )); <nl> if ( status < 12 ) { <nl> kfree ( geth ); <nl> + mutex_unlock (& opts -> lock ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> geth_string_defs [ 1 ]. s = geth -> ethaddr ;
static const struct file_operations bm_register_operations = { <nl> static ssize_t <nl> bm_status_read ( struct file * file , char __user * buf , size_t nbytes , loff_t * ppos ) <nl> { <nl> - char * s = enabled ? " enabled " : " disabled "; <nl> + char * s = enabled ? " enabled \ n " : " disabled \ n "; <nl>  <nl> return simple_read_from_buffer ( buf , nbytes , ppos , s , strlen ( s )); <nl> }
static int skfp_ioctl ( struct net_device * dev , struct ifreq * rq , int cmd ) <nl> break ; <nl> case SKFP_CLR_STATS : /* Zero out the driver statistics */ <nl> if (! capable ( CAP_NET_ADMIN )) { <nl> - memset (& lp -> MacStat , 0 , sizeof ( lp -> MacStat )); <nl> - } else { <nl> status = - EPERM ; <nl> + } else { <nl> + memset (& lp -> MacStat , 0 , sizeof ( lp -> MacStat )); <nl> } <nl> break ; <nl> default :
static void switched_to_dl ( struct rq * rq , struct task_struct * p ) <nl> if ( unlikely ( p -> dl . dl_throttled )) <nl> return ; <nl>  <nl> - if ( p -> on_rq || rq -> curr != p ) { <nl> + if ( p -> on_rq && rq -> curr != p ) { <nl> # ifdef CONFIG_SMP <nl> if ( rq -> dl . overloaded && push_dl_task ( rq ) && rq != task_rq ( p )) <nl> /* Only reschedule if pushing failed */
static int __init fcoe_init ( void ) <nl> /* Setup link change notification */ <nl> fcoe_dev_setup (); <nl>  <nl> - fcoe_if_init (); <nl> + rc = fcoe_if_init (); <nl> + if ( rc ) <nl> + goto out_free ; <nl>  <nl> return 0 ; <nl> 
static int mptsas_smp_handler ( struct Scsi_Host * shost , struct sas_rphy * rphy , <nl> smprep = ( SmpPassthroughReply_t *) ioc -> sas_mgmt . reply ; <nl> memcpy ( req -> sense , smprep , sizeof (* smprep )); <nl> req -> sense_len = sizeof (* smprep ); <nl> + req -> data_len = 0 ; <nl> + rsp -> data_len -= smprep -> ResponseDataLength ; <nl> } else { <nl> printk ( MYIOC_s_ERR_FMT "% s : smp passthru reply failed to be returned \ n ", <nl> ioc -> name , __FUNCTION__ );
static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
struct page * kmap_to_page ( void * vaddr ) <nl>  <nl> return virt_to_page ( addr ); <nl> } <nl> + EXPORT_SYMBOL ( kmap_to_page ); <nl>  <nl> static void flush_all_zero_pkmaps ( void ) <nl> {
static ssize_t auerchar_write ( struct file * file , const char __user * buf , size_t <nl> int ret ; <nl> wait_queue_t wait ; <nl>  <nl> - dbg (" auerchar_write % d bytes ", len ); <nl> + dbg (" auerchar_write % zd bytes ", len ); <nl>  <nl> /* Error checking */ <nl> if (! ccp )
static void xfrm4_dst_destroy ( struct dst_entry * dst ) <nl>  <nl> if ( likely ( xdst -> u . rt . idev )) <nl> in_dev_put ( xdst -> u . rt . idev ); <nl> + if ( likely ( xdst -> u . rt . peer )) <nl> + inet_putpeer ( xdst -> u . rt . peer ); <nl> xfrm_dst_destroy ( xdst ); <nl> } <nl> 
static int initialize_usbh1_port ( struct platform_device * pdev ) <nl>  <nl> mdelay ( 10 ); <nl>  <nl> - return mx51_initialize_usb_hw ( 0 , MXC_EHCI_ITC_NO_THRESHOLD ); <nl> + return mx51_initialize_usb_hw ( pdev -> id , MXC_EHCI_ITC_NO_THRESHOLD ); <nl> } <nl>  <nl> static struct mxc_usbh_platform_data usbh1_config = {
void rtl92e_set_key ( struct net_device * dev , u8 EntryNo , u8 KeyIndex , <nl> } <nl> } <nl> priv -> rtllib -> is_set_key = true ; <nl> - if ( EntryNo >= TOTAL_CAM_ENTRY ) <nl> + if ( EntryNo >= TOTAL_CAM_ENTRY ) { <nl> netdev_info ( dev , "% s (): Invalid CAM entry \ n ", __func__ ); <nl> + return ; <nl> + } <nl>  <nl> RT_TRACE ( COMP_SEC , <nl> "====> to rtl92e_set_key (), dev :% p , EntryNo :% d , KeyIndex :% d , KeyType :% d , MacAddr % pM \ n ",
e1000_set_tso ( struct net_device * netdev , u32 data ) <nl> else <nl> netdev -> features &= ~ NETIF_F_TSO ; <nl>  <nl> - if ( data ) <nl> + if ( data && ( adapter -> hw . mac_type > e1000_82547_rev_2 )) <nl> netdev -> features |= NETIF_F_TSO6 ; <nl> else <nl> netdev -> features &= ~ NETIF_F_TSO6 ;
void free_user_ns ( struct kref * kref ) <nl> struct user_namespace * ns ; <nl>  <nl> ns = container_of ( kref , struct user_namespace , kref ); <nl> + free_uid ( ns -> root_user ); <nl> kfree ( ns ); <nl> } <nl> 
static void * arm64_swiotlb_alloc_coherent ( struct device * dev , size_t size , <nl> if ( IS_ENABLED ( CONFIG_DMA_CMA )) { <nl> struct page * page ; <nl>  <nl> + size = PAGE_ALIGN ( size ); <nl> page = dma_alloc_from_contiguous ( dev , size >> PAGE_SHIFT , <nl> get_order ( size )); <nl> if (! page )
static int clk_wzrd_probe ( struct platform_device * pdev ) <nl> reg = ( readl ( clk_wzrd -> base + WZRD_CLK_CFG_REG ( 0 )) & <nl> WZRD_DIVCLK_DIVIDE_MASK ) >> WZRD_DIVCLK_DIVIDE_SHIFT ; <nl> clk_name = kasprintf ( GFP_KERNEL , "% s_mul_div ", dev_name (& pdev -> dev )); <nl> + if (! clk_name ) { <nl> + ret = - ENOMEM ; <nl> + goto err_rm_int_clk ; <nl> + } <nl> + <nl> clk_wzrd -> clks_internal [ wzrd_clk_mul_div ] = clk_register_fixed_factor ( <nl> & pdev -> dev , clk_name , <nl> __clk_get_name ( clk_wzrd -> clks_internal [ wzrd_clk_mul ]),
static inline bool ipv4_is_local_multicast ( __be32 addr ) <nl> static inline bool ipv4_is_lbcast ( __be32 addr ) <nl> { <nl> /* limited broadcast */ <nl> - return addr == INADDR_BROADCAST ; <nl> + return addr == htonl ( INADDR_BROADCAST ); <nl> } <nl>  <nl> static inline bool ipv4_is_zeronet ( __be32 addr )
static int nand_scan_bbt ( struct mtd_info * mtd , struct nand_bbt_descr * bd ) <nl> struct nand_bbt_descr * td = this -> bbt_td ; <nl> struct nand_bbt_descr * md = this -> bbt_md ; <nl>  <nl> - len = mtd -> size >> ( this -> bbt_erase_shift + 2 ); <nl> + len = ( mtd -> size >> ( this -> bbt_erase_shift + 2 )) ? : 1 ; <nl> /* <nl> * Allocate memory ( 2bit per block ) and clear the memory bad block <nl> * table .
radeon_atom_encoder_mode_set ( struct drm_encoder * encoder , <nl>  <nl> radeon_encoder -> pixel_clock = adjusted_mode -> clock ; <nl>  <nl> - if ( ASIC_IS_AVIVO ( rdev )) { <nl> + if ( ASIC_IS_AVIVO ( rdev ) && ! ASIC_IS_DCE4 ( rdev )) { <nl> if ( radeon_encoder -> active_device & ( ATOM_DEVICE_CV_SUPPORT | ATOM_DEVICE_TV_SUPPORT )) <nl> atombios_yuv_setup ( encoder , true ); <nl> else
static void kvm_mmu_remove_some_alloc_mmu_pages ( struct kvm * kvm , <nl> { <nl> struct kvm_mmu_page * page ; <nl>  <nl> + if ( list_empty (& kvm -> arch . active_mmu_pages )) <nl> + return ; <nl> + <nl> page = container_of ( kvm -> arch . active_mmu_pages . prev , <nl> struct kvm_mmu_page , link ); <nl> kvm_mmu_prepare_zap_page ( kvm , page , invalid_list );
static int __init sm_it87_init ( void ) <nl>  <nl> static void __exit sm_it87_exit ( void ) <nl> { <nl> - i2c_isa_del_driver (& it87_isa_driver ); <nl> + if ( isa_address ) <nl> + i2c_isa_del_driver (& it87_isa_driver ); <nl> i2c_del_driver (& it87_driver ); <nl> } <nl> 
static int pcmuio_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl>  <nl> /* save the ioport address for each ' port ' of 8 channels in the <nl> subdevice */ <nl> - for ( byte_no = 0 ; byte_no < PORTS_PER_SUBDEV ; ++ byte_no , ++ port ) { <nl> + for ( byte_no = 0 ; byte_no < PORTS_PER_SUBDEV ; <nl> + ++ byte_no , ++ port ) { <nl> if ( port >= PORTS_PER_ASIC ) { <nl> port = 0 ; <nl> ++ asic ;
static struct drm_display_mode * drm_mode_detailed ( struct drm_device * dev , <nl> return NULL ; <nl> } <nl>  <nl> + /* Some EDIDs have bogus h / vtotal values */ <nl> + if ( mode -> hsync_end > mode -> htotal ) <nl> + mode -> htotal = mode -> hsync_end + 1 ; <nl> + if ( mode -> vsync_end > mode -> vtotal ) <nl> + mode -> vtotal = mode -> vsync_end + 1 ; <nl> + <nl> drm_mode_set_name ( mode ); <nl>  <nl> if ( pt -> misc & DRM_EDID_PT_INTERLACED )
static inline int local_sid_lookup ( struct id * entry ) <nl> return - 1 ; <nl> } <nl>  <nl> -/* Invalidate all id mappings on local core */ <nl> +/* Invalidate all id mappings on local core -- call with preempt disabled */ <nl> static inline void local_sid_destroy_all ( void ) <nl> { <nl> - preempt_disable (); <nl> __get_cpu_var ( pcpu_last_used_sid ) = 0 ; <nl> memset (& __get_cpu_var ( pcpu_sids ), 0 , sizeof ( __get_cpu_var ( pcpu_sids ))); <nl> - preempt_enable (); <nl> } <nl>  <nl> static void * kvmppc_e500_id_table_alloc ( struct kvmppc_vcpu_e500 * vcpu_e500 )
con3270_init ( void ) <nl> return PTR_ERR ( rp ); <nl>  <nl> condev = kzalloc ( sizeof ( struct con3270 ), GFP_KERNEL | GFP_DMA ); <nl> + if (! condev ) <nl> + return - ENOMEM ; <nl> condev -> view . dev = rp ; <nl>  <nl> condev -> read = raw3270_request_alloc ( 0 );
int xen_pcibk_enable_msix ( struct xen_pcibk_device * pdev , <nl> /* <nl> * PCI_COMMAND_MEMORY must be enabled , otherwise we may not be able <nl> * to access the BARs where the MSI - X entries reside . <nl> + * But VF devices are unique in which the PF needs to be checked . <nl> */ <nl> - pci_read_config_word ( dev , PCI_COMMAND , & cmd ); <nl> + pci_read_config_word ( pci_physfn ( dev ), PCI_COMMAND , & cmd ); <nl> if ( dev -> msi_enabled || !( cmd & PCI_COMMAND_MEMORY )) <nl> return - ENXIO ; <nl> 
static int do_lo_send_aops ( struct loop_device * lo , struct bio_vec * bvec , <nl> if ( ret ) <nl> goto fail ; <nl>  <nl> + file_update_time ( file ); <nl> + <nl> transfer_result = lo_do_transfer ( lo , WRITE , page , offset , <nl> bvec -> bv_page , bv_offs , size , IV ); <nl> copied = size ;
static long gntalloc_ioctl_alloc ( struct gntalloc_file_private_data * priv , <nl> goto out ; <nl> } <nl>  <nl> - gref_ids = kzalloc ( sizeof ( gref_ids [ 0 ]) * op . count , GFP_TEMPORARY ); <nl> + gref_ids = kcalloc ( op . count , sizeof ( gref_ids [ 0 ]), GFP_TEMPORARY ); <nl> if (! gref_ids ) { <nl> rc = - ENOMEM ; <nl> goto out ;
static int __init orion_nand_probe ( struct platform_device * pdev ) <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) { <nl> - err = - ENODEV ; <nl> + ret = - ENODEV ; <nl> goto no_res ; <nl> } <nl> 
static int iwl_read_ucode ( struct iwl_priv * priv ) <nl> priv -> ucode_data_backup . len = data_size ; <nl> iwl_alloc_fw_desc ( priv -> pci_dev , & priv -> ucode_data_backup ); <nl>  <nl> + if (! priv -> ucode_code . v_addr || ! priv -> ucode_data . v_addr || <nl> + ! priv -> ucode_data_backup . v_addr ) <nl> + goto err_pci_alloc ; <nl> + <nl> /* Initialization instructions and data */ <nl> if ( init_size && init_data_size ) { <nl> priv -> ucode_init . len = init_size ;
static void br_multicast_port_query_expired ( unsigned long data ) <nl> struct net_bridge * br = port -> br ; <nl>  <nl> spin_lock (& br -> multicast_lock ); <nl> - if ( port && ( port -> state == BR_STATE_DISABLED || <nl> - port -> state == BR_STATE_BLOCKING )) <nl> + if ( port -> state == BR_STATE_DISABLED || <nl> + port -> state == BR_STATE_BLOCKING ) <nl> goto out ; <nl>  <nl> if ( port -> multicast_startup_queries_sent <
static void atmel_sha_get_cap ( struct atmel_sha_dev * dd ) <nl>  <nl> /* keep only major version number */ <nl> switch ( dd -> hw_version & 0xff0 ) { <nl> + case 0x420 : <nl> + dd -> caps . has_dma = 1 ; <nl> + dd -> caps . has_dualbuff = 1 ; <nl> + dd -> caps . has_sha224 = 1 ; <nl> + dd -> caps . has_sha_384_512 = 1 ; <nl> + break ; <nl> case 0x410 : <nl> dd -> caps . has_dma = 1 ; <nl> dd -> caps . has_dualbuff = 1 ;
int regcache_sync_block ( struct regmap * map , void * block , <nl> unsigned int block_base , unsigned int start , <nl> unsigned int end ) <nl> { <nl> - if ( regmap_can_raw_write ( map )) <nl> + if ( regmap_can_raw_write ( map ) && ! map -> use_single_rw ) <nl> return regcache_sync_block_raw ( map , block , cache_present , <nl> block_base , start , end ); <nl> else
int rxrpc_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( copy > len - copied ) <nl> copy = len - copied ; <nl>  <nl> - if ( skb -> ip_summed == CHECKSUM_UNNECESSARY ) { <nl> + if ( skb -> ip_summed == CHECKSUM_UNNECESSARY || <nl> + skb -> ip_summed == CHECKSUM_PARTIAL ) { <nl> ret = skb_copy_datagram_iovec ( skb , offset , <nl> msg -> msg_iov , copy ); <nl> } else {
static void vmw_user_surface_base_release ( struct ttm_base_object ** p_base ) <nl> struct vmw_resource * res = & user_srf -> srf . res ; <nl>  <nl> * p_base = NULL ; <nl> - ttm_base_object_unref (& user_srf -> backup_base ); <nl> + if ( user_srf -> backup_base ) <nl> + ttm_base_object_unref (& user_srf -> backup_base ); <nl> vmw_resource_unreference (& res ); <nl> } <nl> 
static int radeon_surface_free ( DRM_IOCTL_ARGS ) <nl> return DRM_ERR ( EINVAL ); <nl> } <nl>  <nl> - DRM_COPY_FROM_USER_IOCTL ( memfree , ( drm_radeon_mem_free_t __user *) data , <nl> + DRM_COPY_FROM_USER_IOCTL ( memfree , ( drm_radeon_surface_free_t __user *) data , <nl> sizeof ( memfree )); <nl>  <nl> if ( free_surface ( filp , dev_priv , memfree . address ))
static void incoming_packet ( struct sasem_context * context , <nl> } <nl>  <nl> if ( debug ) { <nl> - printk ( KERN_INFO " Incoming data : "); <nl> + pr_info (" Incoming data : "); <nl> for ( i = 0 ; i < 8 ; ++ i ) <nl> - printk ( KERN_CONT "% 02x ", buf [ i ]); <nl> - printk ( KERN_CONT "\ n "); <nl> + pr_cont ("% 02x ", buf [ i ]); <nl> + pr_cont ("\ n "); <nl> } <nl>  <nl> /*
int dwc2_hcd_init ( struct dwc2_hsotg * hsotg , int irq , <nl> if (! hcd ) <nl> goto error1 ; <nl>  <nl> + if ( hsotg -> core_params -> dma_enable <= 0 ) <nl> + hcd -> self . uses_dma = 0 ; <nl> + <nl> hcd -> has_tt = 1 ; <nl>  <nl> spin_lock_init (& hsotg -> lock );
static void zfcp_erp_rports_del ( struct zfcp_adapter * adapter ) <nl> { <nl> struct zfcp_port * port ; <nl> list_for_each_entry ( port , & adapter -> port_list_head , list ) { <nl> + if (! port -> rport ) <nl> + continue ; <nl> fc_remote_port_delete ( port -> rport ); <nl> port -> rport = NULL ; <nl> }
static int ds1374_probe ( struct i2c_adapter * adap , int addr , int kind ) <nl> client -> driver = & ds1374_driver ; <nl>  <nl> ds1374_workqueue = create_singlethread_workqueue (" ds1374 "); <nl> + if (! ds1374_workqueue ) { <nl> + kfree ( client ); <nl> + return - ENOMEM ; /* most expected reason */ <nl> + } <nl>  <nl> if (( rc = i2c_attach_client ( client )) != 0 ) { <nl> kfree ( client );
static int dw_mci_data_complete ( struct dw_mci * host , struct mmc_data * data ) <nl> data -> error = - EIO ; <nl> } <nl>  <nl> - dev_err ( host -> dev , " data error , status 0x % 08x \ n ", status ); <nl> + dev_dbg ( host -> dev , " data error , status 0x % 08x \ n ", status ); <nl>  <nl> /* <nl> * After an error , there may be data lingering
static int __devinit twl_rtc_probe ( struct platform_device * pdev ) <nl> if ( ret < 0 ) <nl> goto out1 ; <nl>  <nl> - ret = request_irq ( irq , twl_rtc_interrupt , <nl> + ret = request_threaded_irq ( irq , NULL , twl_rtc_interrupt , <nl> IRQF_TRIGGER_RISING , <nl> dev_name (& rtc -> dev ), rtc ); <nl> if ( ret < 0 ) {
static LIST_HEAD ( pinctrl_maps ); <nl> list_for_each_entry ( _maps_node_ , & pinctrl_maps , node ) \ <nl> for ( _i_ = 0 , _map_ = & _maps_node_ -> maps [ _i_ ]; \ <nl> _i_ < _maps_node_ -> num_maps ; \ <nl> - i ++, _map_ = & _maps_node_ -> maps [ _i_ ]) <nl> + _i_ ++, _map_ = & _maps_node_ -> maps [ _i_ ]) <nl>  <nl> /** <nl> * pinctrl_provide_dummies () - indicate if pinctrl provides dummy state support
static void um_new_card ( DESCRIPTOR * d ) <nl> } else { <nl> DBG_ERR ((" could not create user mode idi card % d ", <nl> adapter_nr )); <nl> + diva_os_free ( 0 , card ); <nl> } <nl> } <nl> 
void __devinit bttv_init_card2 ( struct bttv * btv ) <nl> } <nl> btv -> pll . pll_current = - 1 ; <nl>  <nl> - bttv_reset_audio ( btv ); <nl> - <nl> /* tuner configuration ( from card list / autodetect / insmod option ) */ <nl> if ( UNSET != bttv_tvcards [ btv -> c . type ]. tuner_type ) <nl> if ( UNSET == btv -> tuner_type )
static long logger_ioctl ( struct file * file , unsigned int cmd , unsigned long arg ) <nl> ret = - EBADF ; <nl> break ; <nl> } <nl> + if (!( in_egroup_p ( file -> f_dentry -> d_inode -> i_gid ) || <nl> + capable ( CAP_SYSLOG ))) { <nl> + ret = - EPERM ; <nl> + break ; <nl> + } <nl> list_for_each_entry ( reader , & log -> readers , list ) <nl> reader -> r_off = log -> w_off ; <nl> log -> head = log -> w_off ;
qla24xx_chip_diag ( scsi_qla_host_t * ha ) <nl> /* Perform RISC reset . */ <nl> qla24xx_reset_risc ( ha ); <nl>  <nl> - ha -> fw_transfer_size = REQUEST_ENTRY_SIZE * 1024 ; <nl> + ha -> fw_transfer_size = REQUEST_ENTRY_SIZE * ha -> request_q_length ; <nl>  <nl> rval = qla2x00_mbx_reg_test ( ha ); <nl> if ( rval ) {
int of_dma_controller_register ( struct device_node * np , <nl> if (! nbcells ) { <nl> pr_err ("% s : # dma - cells property is missing or invalid \ n ", <nl> __func__ ); <nl> + kfree ( ofdma ); <nl> return - EINVAL ; <nl> } <nl> 
bnx2_init_5709_context ( struct bnx2 * bp ) <nl> for ( i = 0 ; i < bp -> ctx_pages ; i ++) { <nl> int j ; <nl>  <nl> + if ( bp -> ctx_blk [ i ]) <nl> + memset ( bp -> ctx_blk [ i ], 0 , BCM_PAGE_SIZE ); <nl> + else <nl> + return - ENOMEM ; <nl> + <nl> REG_WR ( bp , BNX2_CTX_HOST_PAGE_TBL_DATA0 , <nl> ( bp -> ctx_blk_mapping [ i ] & 0xffffffff ) | <nl> BNX2_CTX_HOST_PAGE_TBL_DATA0_VALID );
void hw_cursor_setData ( struct lynx_cursor * cursor , <nl> iowrite16 ( data , pbuffer ); <nl>  <nl> /* assume pitch is 1 , 2 , 4 , 8 ,...*/ <nl> - if (( i + 1 ) % pitch == 0 ) <nl> - { <nl> + if (( i + 1 ) % pitch == 0 ) { <nl> /* need a return */ <nl> pstart += offset ; <nl> pbuffer = pstart ;
static void clear_subscriber_list ( struct snd_seq_client * client , <nl> list_del (& subs -> dest_list ); <nl> else <nl> list_del (& subs -> src_list ); <nl> + up_write (& agrp -> list_mutex ); <nl> unsubscribe_port ( c , aport , agrp , & subs -> info , 1 ); <nl> kfree ( subs ); <nl> - up_write (& agrp -> list_mutex ); <nl> snd_seq_port_unlock ( aport ); <nl> snd_seq_client_unlock ( c ); <nl> }
static int pagemap_pte_range ( pmd_t * pmd , unsigned long addr , unsigned long end , <nl>  <nl> /* find the first VMA at or above ' addr ' */ <nl> vma = find_vma ( walk -> mm , addr ); <nl> - if ( pmd_trans_huge_lock ( pmd , vma ) == 1 ) { <nl> + if ( vma && pmd_trans_huge_lock ( pmd , vma ) == 1 ) { <nl> for (; addr != end ; addr += PAGE_SIZE ) { <nl> unsigned long offset ; <nl> 
static void _ceph_msgr_exit ( void ) <nl> ceph_msgr_slab_exit (); <nl>  <nl> BUG_ON ( zero_page == NULL ); <nl> - kunmap ( zero_page ); <nl> page_cache_release ( zero_page ); <nl> zero_page = NULL ; <nl> }
ext4_move_extents ( struct file * o_filp , struct file * d_filp , <nl> orig_inode -> i_ino , donor_inode -> i_ino ); <nl> return - EINVAL ; <nl> } <nl> - <nl> + /* TODO : This is non obvious task to swap blocks for inodes with full <nl> + jornaling enabled */ <nl> + if ( ext4_should_journal_data ( orig_inode ) || <nl> + ext4_should_journal_data ( donor_inode )) { <nl> + return - EINVAL ; <nl> + } <nl> /* Protect orig and donor inodes against a truncate */ <nl> mext_inode_double_lock ( orig_inode , donor_inode ); <nl> 
static unsigned long __meminit compute_pernodesize ( int node ) <nl> pernodesize += node * L1_CACHE_BYTES ; <nl> pernodesize += L1_CACHE_ALIGN ( sizeof ( pg_data_t )); <nl> pernodesize += L1_CACHE_ALIGN ( sizeof ( struct ia64_node_data )); <nl> + pernodesize += L1_CACHE_ALIGN ( sizeof ( pg_data_t )); <nl> pernodesize = PAGE_ALIGN ( pernodesize ); <nl> return pernodesize ; <nl> }
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> * queued ( again ) by ceph_update_snap_trace () <nl> * below . Queue it _now_ , under the old context . <nl> */ <nl> + spin_lock (& realm -> inodes_with_caps_lock ); <nl> list_del_init (& ci -> i_snap_realm_item ); <nl> + spin_unlock (& realm -> inodes_with_caps_lock ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> ceph_queue_cap_snap ( ci ,
void arch_release_hugepage ( struct page * page ) <nl> ptep = ( pte_t *) page [ 1 ]. index ; <nl> if (! ptep ) <nl> return ; <nl> + clear_table (( unsigned long *) ptep , _PAGE_TYPE_EMPTY , <nl> + PTRS_PER_PTE * sizeof ( pte_t )); <nl> page_table_free (& init_mm , ( unsigned long *) ptep ); <nl> page [ 1 ]. index = 0 ; <nl> }
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl>  <nl> hw -> wiphy -> max_remain_on_channel_duration = 10000 ; <nl> hw -> max_listen_interval = IWL_CONN_MAX_LISTEN_INTERVAL ; <nl> + /* we can compensate an offset of up to 3 channels = 15 MHz */ <nl> + hw -> wiphy -> max_adj_channel_rssi_comp = 3 * 5 ; <nl>  <nl> /* Extract MAC address */ <nl> memcpy ( mvm -> addresses [ 0 ]. addr , mvm -> nvm_data -> hw_addr , ETH_ALEN );
mext_check_arguments ( struct inode * orig_inode , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( IS_IMMUTABLE ( donor_inode ) || IS_APPEND ( donor_inode )) <nl> + return - EPERM ; <nl> + <nl> /* Ext4 move extent does not support swapfile */ <nl> if ( IS_SWAPFILE ( orig_inode ) || IS_SWAPFILE ( donor_inode )) { <nl> ext4_debug (" ext4 move extent : The argument files should "
static s32 igb_get_invariants_82575 ( struct e1000_hw * hw ) <nl> * for setting word_size . <nl> */ <nl> size += NVM_WORD_SIZE_BASE_SHIFT ; <nl> + <nl> + /* EEPROM access above 16k is unsupported */ <nl> + if ( size > 14 ) <nl> + size = 14 ; <nl> nvm -> word_size = 1 << size ; <nl>  <nl> /* setup PHY parameters */
xfrm_state_find ( const xfrm_address_t * daddr , const xfrm_address_t * saddr , <nl> xfrm_state_look_at ( pol , x , fl , encap_family , <nl> & best , & acquire_in_progress , & error ); <nl> } <nl> - if ( best ) <nl> + if ( best || acquire_in_progress ) <nl> goto found ; <nl>  <nl> h_wildcard = xfrm_dst_hash ( net , daddr , & saddr_wildcard , tmpl -> reqid , encap_family );
static int mthca_alloc_qp_common ( struct mthca_dev * dev , <nl> int i ; <nl>  <nl> atomic_set (& qp -> refcount , 1 ); <nl> + init_waitqueue_head (& qp -> wait ); <nl> qp -> state = IB_QPS_RESET ; <nl> qp -> atomic_rd_en = 0 ; <nl> qp -> resp_depth = 0 ;
special_insn : <nl> case 0x88 ... 0x8b : /* mov */ <nl> goto mov ; <nl> case 0x8d : /* lea r16 / r32 , m */ <nl> - c -> dst . val = c -> modrm_val ; <nl> + c -> dst . val = c -> modrm_ea ; <nl> break ; <nl> case 0x8f : /* pop ( sole member of Grp1a ) */ <nl> rc = emulate_grp1a ( ctxt , ops );
static struct k_itimer * __lock_timer ( timer_t timer_id , unsigned long * flags ) <nl> { <nl> struct k_itimer * timr ; <nl>  <nl> + /* <nl> + * timer_t could be any type >= int and we want to make sure any <nl> + * @ timer_id outside positive int range fails lookup . <nl> + */ <nl> + if (( unsigned long long ) timer_id > INT_MAX ) <nl> + return NULL ; <nl> + <nl> rcu_read_lock (); <nl> timr = idr_find (& posix_timers_id , ( int ) timer_id ); <nl> if ( timr ) {
static void set_tracepoint ( struct tracepoint_entry ** entry , <nl> static void disable_tracepoint ( struct tracepoint * elem ) <nl> { <nl> elem -> state = 0 ; <nl> + rcu_assign_pointer ( elem -> funcs , NULL ); <nl> } <nl>  <nl> /**
static void vmid_reference ( struct snd_soc_codec * codec ) <nl> WM8994_LINEOUT2_DISCH , <nl> WM8994_LINEOUT_VMID_BUF_ENA ); <nl>  <nl> + wm_hubs_vmid_ena ( codec ); <nl> + <nl> /* Startup bias , VMID ramp & buffer */ <nl> snd_soc_update_bits ( codec , WM8994_ANTIPOP_2 , <nl> WM8994_BIAS_SRC | <nl> static void vmid_reference ( struct snd_soc_codec * codec ) <nl> WM8994_VMID_BUF_ENA | <nl> ( 0x2 << WM8994_VMID_RAMP_SHIFT )); <nl>  <nl> - wm_hubs_vmid_ena ( codec ); <nl> - <nl> /* Main bias enable , VMID = 2x40k */ <nl> snd_soc_update_bits ( codec , WM8994_POWER_MANAGEMENT_1 , <nl> WM8994_BIAS_ENA |
static int __devinit gen_74x164_probe ( struct spi_device * spi ) <nl> } <nl>  <nl> chip -> gpio_chip . ngpio = GEN_74X164_NUMBER_GPIOS * chip -> registers ; <nl> - chip -> buffer = devm_kzalloc (& spi -> dev , chip -> gpio_chip . ngpio , GFP_KERNEL ); <nl> + chip -> buffer = devm_kzalloc (& spi -> dev , chip -> registers , GFP_KERNEL ); <nl> if (! chip -> buffer ) { <nl> ret = - ENOMEM ; <nl> goto exit_destroy ;
int usb_alloc_streams ( struct usb_interface * interface , <nl> return - EINVAL ; <nl> if ( dev -> speed != USB_SPEED_SUPER ) <nl> return - EINVAL ; <nl> + if ( dev -> state < USB_STATE_CONFIGURED ) <nl> + return - ENODEV ; <nl>  <nl> for ( i = 0 ; i < num_eps ; i ++) { <nl> /* Streams only apply to bulk endpoints . */
static int __devinit corgipm_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if (! machine_is_corgi () && ! machine_is_shepherd () <nl> + && ! machine_is_husky ()) <nl> + return - ENODEV ; <nl> + <nl> corgipm_device = platform_device_alloc (" sharpsl - pm ", - 1 ); <nl> if (! corgipm_device ) <nl> return - ENOMEM ;
static void reg_process_pending_hints ( void ) <nl> } <nl>  <nl> reg_process_hint ( reg_request ); <nl> + <nl> + lr = get_last_request (); <nl> + <nl> + spin_lock (& reg_requests_lock ); <nl> + if (! list_empty (& reg_requests_list ) && lr && lr -> processed ) <nl> + schedule_work (& reg_work ); <nl> + spin_unlock (& reg_requests_lock ); <nl> } <nl>  <nl> /* Processes beacon hints -- this has nothing to do with country IEs */
struct pcmcia_device * pcmcia_device_add ( struct pcmcia_socket * s , unsigned int f <nl> p_dev -> socket = s ; <nl> p_dev -> device_no = ( s -> device_count ++); <nl> p_dev -> func = function ; <nl> - if ( s -> functions < function ) <nl> - s -> functions = function ; <nl> + if ( s -> functions <= function ) <nl> + s -> functions = function + 1 ; <nl>  <nl> p_dev -> dev . bus = & pcmcia_bus_type ; <nl> p_dev -> dev . parent = s -> dev . dev ;
static DEFINE_PCI_DEVICE_TABLE ( rt2800pci_device_table ) = { <nl> { PCI_DEVICE ( 0x1814 , 0x5390 ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x5392 ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x539a ) }, <nl> + { PCI_DEVICE ( 0x1814 , 0x539b ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x539f ) }, <nl> # endif <nl> { 0 , }
static int __init parse_memopt ( char * p ) <nl>  <nl> userdef = 1 ; <nl> mem_size = memparse ( p , & p ); <nl> + /* don ' t remove all of memory when handling " mem ={ invalid }" param */ <nl> + if ( mem_size == 0 ) <nl> + return - EINVAL ; <nl> e820_remove_range ( mem_size , ULLONG_MAX - mem_size , E820_RAM , 1 ); <nl>  <nl> return 0 ;
static int __ath10k_pci_hif_power_up ( struct ath10k * ar , bool cold_reset ) <nl> irq_mode = " legacy "; <nl>  <nl> if (! test_bit ( ATH10K_FLAG_FIRST_BOOT_DONE , & ar -> dev_flags )) <nl> - ath10k_info (" pci irq % s \ n ", irq_mode ); <nl> + ath10k_info (" pci irq % s irq_mode % d reset_mode % d \ n ", <nl> + irq_mode , ath10k_pci_irq_mode , <nl> + ath10k_pci_reset_mode ); <nl>  <nl> return 0 ; <nl> 
static enum blk_eh_timer_return mtip_cmd_timeout ( struct request * req , <nl> bool reserved ) <nl> { <nl> struct driver_data * dd = req -> q -> queuedata ; <nl> - int ret = BLK_EH_RESET_TIMER ; <nl>  <nl> if ( reserved ) <nl> goto exit_handler ; <nl> static enum blk_eh_timer_return mtip_cmd_timeout ( struct request * req , <nl>  <nl> wake_up_interruptible (& dd -> port -> svc_wait ); <nl> exit_handler : <nl> - return ret ; <nl> + return BLK_EH_RESET_TIMER ; <nl> } <nl>  <nl> static struct blk_mq_ops mtip_mq_ops = {
static int vpbe_display_g_register ( struct file * file , void * priv , <nl> struct v4l2_dbg_register * reg ) <nl> { <nl> struct v4l2_dbg_match * match = & reg -> match ; <nl> + struct vpbe_fh * fh = file -> private_data ; <nl> + struct vpbe_device * vpbe_dev = fh -> disp_dev -> vpbe_dev ; <nl>  <nl> if ( match -> type >= 2 ) { <nl> v4l2_subdev_call ( vpbe_dev -> venc ,
static int unqueue_me ( struct futex_q * q ) <nl> /* In the common case we don ' t take the spinlock , which is nice . */ <nl> retry : <nl> lock_ptr = q -> lock_ptr ; <nl> + barrier (); <nl> if ( lock_ptr != 0 ) { <nl> spin_lock ( lock_ptr ); <nl> /*
static void hpet_msi_capability_lookup ( unsigned int start_timer ) <nl> continue ; <nl>  <nl> irq = hpet_assign_irq ( hpet_domain , hdev , hdev -> num ); <nl> - if ( irq < 0 ) <nl> + if ( irq <= 0 ) <nl> continue ; <nl>  <nl> sprintf ( hdev -> name , " hpet % d ", i );
more : <nl> goto out_unlock ; <nl> } <nl> if (! d_unhashed ( dentry ) && dentry -> d_inode && <nl> + ceph_snap ( dentry -> d_inode ) != CEPH_SNAPDIR && <nl> filp -> f_pos <= di -> offset ) <nl> break ; <nl> dout (" skipping % p %.* s at % llu (% llu )% s % s \ n ", dentry ,
xfs_file_last_byte ( <nl> * necessary . <nl> */ <nl> if ( ip -> i_df . if_flags & XFS_IFEXTENTS ) { <nl> + xfs_ilock ( ip , XFS_ILOCK_SHARED ); <nl> error = xfs_bmap_last_offset ( NULL , ip , & last_block , <nl> XFS_DATA_FORK ); <nl> + xfs_iunlock ( ip , XFS_ILOCK_SHARED ); <nl> if ( error ) { <nl> last_block = 0 ; <nl> }
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) { <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) { <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& priv -> lock , flags );
static ssize_t ath6kl_fwlog_block_read ( struct file * file , <nl>  <nl> ret = wait_for_completion_interruptible ( <nl> & ar -> debug . fwlog_completion ); <nl> - if ( ret == - ERESTARTSYS ) <nl> + if ( ret == - ERESTARTSYS ) { <nl> + vfree ( buf ); <nl> return ret ; <nl> + } <nl>  <nl> spin_lock (& ar -> debug . fwlog_queue . lock ); <nl> }
static unsigned int br_nf_post_routing ( unsigned int hook , struct sk_buff * skb , <nl> if (! nf_bridge ) <nl> return NF_ACCEPT ; <nl>  <nl> + if (!( nf_bridge -> mask & ( BRNF_BRIDGED | BRNF_BRIDGED_DNAT ))) <nl> + return NF_ACCEPT ; <nl> + <nl> if (! realoutdev ) <nl> return NF_DROP ; <nl> 
void __init ehv_pic_init ( void ) <nl>  <nl> if (! ehv_pic -> irqhost ) { <nl> of_node_put ( np ); <nl> + kfree ( ehv_pic ); <nl> return ; <nl> } <nl> 
static int do_pages_stat ( struct mm_struct * mm , unsigned long nr_pages , <nl> int err ; <nl>  <nl> for ( i = 0 ; i < nr_pages ; i += chunk_nr ) { <nl> - if ( chunk_nr + i > nr_pages ) <nl> + if ( chunk_nr > nr_pages - i ) <nl> chunk_nr = nr_pages - i ; <nl>  <nl> err = copy_from_user ( chunk_pages , & pages [ i ],
static inline int gred_change_vq ( struct Qdisc * sch , int dp , <nl> struct gred_sched_data * q ; <nl>  <nl> if ( table -> tab [ dp ] == NULL ) { <nl> - table -> tab [ dp ] = kzalloc ( sizeof (* q ), GFP_KERNEL ); <nl> + table -> tab [ dp ] = kzalloc ( sizeof (* q ), GFP_ATOMIC ); <nl> if ( table -> tab [ dp ] == NULL ) <nl> return - ENOMEM ; <nl> }
struct drm_mm_node * drm_mm_search_free_in_range ( const struct drm_mm * mm , <nl> wasted += alignment - tmp ; <nl> } <nl>  <nl> - if ( entry -> size >= size + wasted ) { <nl> + if ( entry -> size >= size + wasted && <nl> + ( entry -> start + wasted + size ) <= end ) { <nl> if (! best_match ) <nl> return entry ; <nl> if ( entry -> size < best_size ) {
static void brcmf_fws_dequeue_worker ( struct work_struct * worker ) <nl> fws = container_of ( worker , struct brcmf_fws_info , fws_dequeue_work ); <nl>  <nl> brcmf_fws_lock ( fws -> drvr , flags ); <nl> - for ( fifo = NL80211_NUM_ACS ; fifo >= 0 && ! fws -> bus_flow_blocked ; <nl> + for ( fifo = BRCMF_FWS_FIFO_BCMC ; fifo >= 0 && ! fws -> bus_flow_blocked ; <nl> fifo --) { <nl> while (( fws -> fifo_credit [ fifo ]) || ((! fws -> bcmc_credit_check ) && <nl> ( fifo == BRCMF_FWS_FIFO_BCMC ))) {
static int fuse_rename ( struct inode * olddir , struct dentry * oldent , <nl> fuse_invalidate_attr ( newdir ); <nl>  <nl> /* newent will end up negative */ <nl> - if ( newent -> d_inode ) <nl> + if ( newent -> d_inode ) { <nl> + fuse_invalidate_attr ( newent -> d_inode ); <nl> fuse_invalidate_entry_cache ( newent ); <nl> + } <nl> } else if ( err == - EINTR ) { <nl> /* If request was interrupted , DEITY only knows if the <nl> rename actually took place . If the invalidation
static struct net_device * setup_pre_routing ( struct sk_buff * skb ) <nl> else if ( skb -> protocol == htons ( ETH_P_PPP_SES )) <nl> nf_bridge -> mask |= BRNF_PPPoE ; <nl>  <nl> + /* Must drop socket now because of tproxy . */ <nl> + skb_orphan ( skb ); <nl> return skb -> dev ; <nl> } <nl> 
static int usbduxsub_probe ( struct usb_interface * uinterf , <nl> usbduxsub [ index ]. dux_commands = kzalloc ( SIZEOFDUXBUFFER , GFP_KERNEL ); <nl> if (! usbduxsub [ index ]. dux_commands ) { <nl> dev_err ( dev , " comedi_ : usbdux : " <nl> - " error alloc space for dac commands \ n "); <nl> + " error alloc space for dux commands \ n "); <nl> tidy_up (&( usbduxsub [ index ])); <nl> up (& start_stop_sem ); <nl> return - ENOMEM ;
uint brcms_reset ( struct brcms_info * wl ) <nl> /* dpc will not be rescheduled */ <nl> wl -> resched = false ; <nl>  <nl> + /* inform publicly that interface is down */ <nl> + wl -> pub -> up = false ; <nl> + <nl> return 0 ; <nl> } <nl> 
lnet_ping ( lnet_process_id_t id , int timeout_ms , lnet_process_id_t * ids , int n_i <nl>  <nl> rc = - EFAULT ; /* If I SEGV ... */ <nl>  <nl> + memset (& tmpid , 0 , sizeof ( tmpid )); <nl> for ( i = 0 ; i < n_ids ; i ++) { <nl> tmpid . pid = info -> pi_pid ; <nl> tmpid . nid = info -> pi_ni [ i ]. ns_nid ;
static struct vmap_area * alloc_vmap_area ( unsigned long size , <nl>  <nl> BUG_ON ( size & ~ PAGE_MASK ); <nl>  <nl> - addr = ALIGN ( vstart , align ); <nl> - <nl> va = kmalloc_node ( sizeof ( struct vmap_area ), <nl> gfp_mask & GFP_RECLAIM_MASK , node ); <nl> if ( unlikely (! va )) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> retry : <nl> + addr = ALIGN ( vstart , align ); <nl> + <nl> spin_lock (& vmap_area_lock ); <nl> /* XXX : could have a last_hole cache */ <nl> n = vmap_area_root . rb_node ;
int radeon_vm_bo_update_pte ( struct radeon_device * rdev , <nl> return - ENOMEM ; <nl>  <nl> r = radeon_ib_get ( rdev , R600_RING_TYPE_DMA_INDEX , & ib , NULL , ndw * 4 ); <nl> + if ( r ) <nl> + return r ; <nl> ib . length_dw = 0 ; <nl>  <nl> r = radeon_vm_update_pdes ( rdev , vm , & ib , bo_va -> soffset , bo_va -> eoffset );
int wm8350_device_init ( struct wm8350 * wm8350 , int irq , <nl> return ret ; <nl> } <nl>  <nl> - if ( pdata -> init ) { <nl> + if ( pdata && pdata -> init ) { <nl> ret = pdata -> init ( wm8350 ); <nl> if ( ret != 0 ) { <nl> dev_err ( wm8350 -> dev , " Platform init () failed : % d \ n ",
static inline int fls64 ( unsigned long x ) <nl> { <nl> unsigned long t , a , r ; <nl>  <nl> - t = __kernel_cmpbge ( x , 0x0101010101010101 ); <nl> + t = __kernel_cmpbge ( x , 0x0101010101010101UL ); <nl> a = __flsm1_tab [ t ]; <nl> t = __kernel_extbl ( x , a ); <nl> r = a * 8 + __flsm1_tab [ t ] + ( x != 0 );
static void __init read_obp_translations ( void ) <nl> for ( i = 0 ; i < prom_trans_ents ; i ++) <nl> prom_trans [ i ]. data &= ~ 0x0003fe0000000000UL ; <nl> } <nl> + <nl> + /* Force execute bit on . */ <nl> + for ( i = 0 ; i < prom_trans_ents ; i ++) <nl> + prom_trans [ i ]. data |= ( tlb_type == hypervisor ? <nl> + _PAGE_EXEC_4V : _PAGE_EXEC_4U ); <nl> } <nl>  <nl> static void __init hypervisor_tlb_lock ( unsigned long vaddr ,
bfa_ioc_mbox_isr ( struct bfa_ioc_s * ioc ) <nl> return ; <nl> } <nl>  <nl> - if (( mc > BFI_MC_MAX ) || ( mod -> mbhdlr [ mc ]. cbfn == NULL )) <nl> + if (( mc >= BFI_MC_MAX ) || ( mod -> mbhdlr [ mc ]. cbfn == NULL )) <nl> return ; <nl>  <nl> mod -> mbhdlr [ mc ]. cbfn ( mod -> mbhdlr [ mc ]. cbarg , & m );
bool ROUTEbRelay ( PSDevice pDevice , unsigned char * pbySkbData , unsigned int uData <nl> pHeadTD = pHeadTD -> next ; <nl> } <nl>  <nl> - pLastTD -> pTDInfo -> skb = 0 ; <nl> + pLastTD -> pTDInfo -> skb = NULL ; <nl> pLastTD -> pTDInfo -> byFlags = 0 ; <nl>  <nl> pDevice -> apCurrTD [ TYPE_AC0DMA ] = pHeadTD ;
static irqreturn_t pcf8563_irq ( int irq , void * dev_id ) <nl>  <nl> err = pcf8563_get_alarm_mode ( pcf8563 -> client , NULL , & pending ); <nl> if ( err ) <nl> - return err ; <nl> + return IRQ_NONE ; <nl>  <nl> if ( pending ) { <nl> rtc_update_irq ( pcf8563 -> rtc , 1 , RTC_IRQF | RTC_AF );
unlock : <nl>  <nl> for ( skb = segs ; skb ; skb = skb -> next ) { <nl> ipv6h = skb -> nh . ipv6h ; <nl> - ipv6h -> payload_len = htons ( skb -> len - skb -> mac_len ); <nl> + ipv6h -> payload_len = htons ( skb -> len - skb -> mac_len - <nl> + sizeof (* ipv6h )); <nl> } <nl>  <nl> out :
static int pm2fb_check_var ( struct fb_var_screeninfo * var , struct fb_info * info ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + var -> transp . offset = 0 ; <nl> + var -> transp . length = 0 ; <nl> switch ( var -> bits_per_pixel ) { <nl> case 8 : <nl> var -> red . length = var -> green . length = var -> blue . length = 8 ;
ia64_fault ( unsigned long vector , unsigned long isr , unsigned long ifa , <nl> printk ( KERN_ERR " iip - 0x % lx , ifa - 0x % lx , isr - 0x % lx \ n ", <nl> iip , ifa , isr ); <nl> force_sig ( SIGSEGV , current ); <nl> - break ; <nl> + return ; <nl>  <nl> case 46 : <nl> printk ( KERN_ERR " Unexpected IA - 32 intercept trap ( Trap 46 )\ n ");
device_receive_frame ( <nl> } <nl>  <nl> ev . src_addr . sa_family = ARPHRD_ETHER ; <nl> - memcpy ( ev . src_addr . sa_data , pMACHeader -> abyAddr2 , ETH_ALEN ); <nl> + ether_addr_copy ( ev . src_addr . sa_data , <nl> + pMACHeader -> abyAddr2 ); <nl> memset (& wrqu , 0 , sizeof ( wrqu )); <nl> wrqu . data . length = sizeof ( ev ); <nl> wireless_send_event ( pDevice -> dev , IWEVMICHAELMICFAILURE , & wrqu , ( char *)& ev );
int mdiobus_register ( struct mii_bus * bus ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> + put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
c4_add_dev ( hdw_info_t * hi , int brdno , unsigned long f0 , unsigned long f1 , <nl> hi -> devname , irq1 ); <nl> unregister_netdev ( ndev ); <nl> free_irq ( irq0 , ndev ); <nl> - OS_kfree ( ndev -> priv ); <nl> + OS_kfree ( netdev_priv ( ndev )); <nl> OS_kfree ( ndev ); <nl> error_flag = EIO ; <nl> return 0 ;
# define CR8_RESERVED_BITS (~( unsigned long ) X86_CR8_TPR ) <nl>  <nl> # define KVM_MAX_MCE_BANKS 32 <nl> -# define KVM_MCE_CAP_SUPPORTED MCG_CTL_P <nl> +# define KVM_MCE_CAP_SUPPORTED ( MCG_CTL_P | MCG_SER_P ) <nl>  <nl> /* EFER defaults : <nl> * - enable syscall per default because its emulated by KVM
static inline long snd_ctl_ioctl_compat ( struct file * file , unsigned int cmd , uns <nl> case SNDRV_CTL_IOCTL_POWER_STATE : <nl> case SNDRV_CTL_IOCTL_ELEM_LOCK : <nl> case SNDRV_CTL_IOCTL_ELEM_UNLOCK : <nl> + case SNDRV_CTL_IOCTL_ELEM_REMOVE : <nl> + case SNDRV_CTL_IOCTL_TLV_READ : <nl> + case SNDRV_CTL_IOCTL_TLV_WRITE : <nl> + case SNDRV_CTL_IOCTL_TLV_COMMAND : <nl> return snd_ctl_ioctl ( file , cmd , ( unsigned long ) argp ); <nl> case SNDRV_CTL_IOCTL_ELEM_LIST32 : <nl> return snd_ctl_elem_list_compat ( ctl -> card , argp );
static ssize_t enable_store ( <nl> struct timed_output_dev * tdev = dev_get_drvdata ( dev ); <nl> int value ; <nl>  <nl> - sscanf ( buf , "% d ", & value ); <nl> + if ( sscanf ( buf , "% d ", & value ) != 1 ) <nl> + return - EINVAL ; <nl> + <nl> tdev -> enable ( tdev , value ); <nl>  <nl> return size ;
static int sirfsoc_gpio_probe ( struct device_node * np ) <nl> if ( err ) { <nl> dev_err (& pdev -> dev , <nl> " could not connect irqchip to gpiochip \ n "); <nl> - goto out ; <nl> + goto out_banks ; <nl> } <nl>  <nl> for ( i = 0 ; i < SIRFSOC_GPIO_NO_OF_BANKS ; i ++) {
static void rtl8180_beacon_work ( struct work_struct * work ) <nl>  <nl> /* grab a fresh beacon */ <nl> skb = ieee80211_beacon_get ( dev , vif ); <nl> + if (! skb ) <nl> + goto resched ; <nl>  <nl> /* <nl> * update beacon timestamp w / TSF value
vhost_scsi_handle_vq ( struct vhost_scsi * vs , struct vhost_virtqueue * vq ) <nl> break ; <nl> } <nl>  <nl> + /* virtio - scsi spec requires byte 0 of the lun to be 1 */ <nl> + if ( unlikely ( v_req . lun [ 0 ] != 1 )) { <nl> + vhost_scsi_send_bad_target ( vs , vq , head , out ); <nl> + continue ; <nl> + } <nl> + <nl> /* Extract the tpgt */ <nl> target = v_req . lun [ 1 ]; <nl> tpg = ACCESS_ONCE ( vs_tpg [ target ]);
u64 perf_evsel__intval ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> value = *( u32 *) ptr ; <nl> break ; <nl> case 8 : <nl> - value = *( u64 *) ptr ; <nl> + memcpy (& value , ptr , sizeof ( u64 )); <nl> break ; <nl> default : <nl> return 0 ;
MODULE_DEVICE_TABLE ( pnp , smsc_ircc_pnp_table ); <nl> static int pnp_driver_registered ; <nl>  <nl> # ifdef CONFIG_PNP <nl> - static int __init smsc_ircc_pnp_probe ( struct pnp_dev * dev , <nl> + static int __devinit smsc_ircc_pnp_probe ( struct pnp_dev * dev , <nl> const struct pnp_device_id * dev_id ) <nl> { <nl> unsigned int firbase , sirbase ;
skip_msix : <nl> } else <nl> ql_log ( ql_log_warn , vha , 0x0039 , <nl> " MSI - X ; Falling back - to INTa mode -- % d .\ n ", ret ); <nl> + <nl> + /* Skip INTx on ISP82xx . */ <nl> + if (! ha -> flags . msi_enabled && IS_QLA82XX ( ha )) <nl> + return QLA_FUNCTION_FAILED ; <nl> + <nl> skip_msi : <nl>  <nl> ret = request_irq ( ha -> pdev -> irq , ha -> isp_ops -> intr_handler ,
static int exynos_dsi_parse_dt ( struct exynos_dsi * dsi ) <nl>  <nl> ep = of_graph_get_next_endpoint ( node , NULL ); <nl> if (! ep ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl>  <nl> dsi -> bridge_node = of_graph_get_remote_port_parent ( ep ); <nl> if (! dsi -> bridge_node ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl> end :
static __cpuinit int mce_device_create ( unsigned int cpu ) <nl> if (! mce_available (& boot_cpu_data )) <nl> return - EIO ; <nl>  <nl> - memset (& dev -> kobj , 0 , sizeof ( struct kobject )); <nl> + memset ( dev , 0 , sizeof ( struct device )); <nl> dev -> id = cpu ; <nl> dev -> bus = & mce_subsys ; <nl> 
do_last : <nl> goto exit ; <nl> } <nl>  <nl> + if ( IS_ERR ( nd -> intent . open . file )) { <nl> + mutex_unlock (& dir -> d_inode -> i_mutex ); <nl> + error = PTR_ERR ( nd -> intent . open . file ); <nl> + goto exit_dput ; <nl> + } <nl> + <nl> /* Negative dentry , just create the file */ <nl> if (! path . dentry -> d_inode ) { <nl> if (! IS_POSIXACL ( dir -> d_inode ))
int iwl_mvm_scan_request ( struct iwl_mvm * mvm , <nl> else <nl> cmd -> type = cpu_to_le32 ( SCAN_TYPE_FORCED ); <nl>  <nl> + /* <nl> + * TODO : This is a WA due to a bug in the FW AUX framework that does not <nl> + * properly handle time events that fail to be scheduled <nl> + */ <nl> + cmd -> type = cpu_to_le32 ( SCAN_TYPE_FORCED ); <nl> + <nl> cmd -> repeats = cpu_to_le32 ( 1 ); <nl>  <nl> /*
int mmc_send_if_cond ( struct mmc_host * host , u32 ocr ) <nl> static const u8 test_pattern = 0xAA ; <nl> u8 result_pattern ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( struct mmc_command )); <nl> + <nl> /* <nl> * To support SD 2 . 0 cards , we must always invoke SD_SEND_IF_COND <nl> * before SD_APP_OP_COND . This command will harmlessly fail for
static int sep_construct_dma_tables_from_lli ( <nl> table_data_size ); <nl>  <nl> /* If info entry is null - this is the first table built */ <nl> - if ( info_in_entry_ptr == NULL ) { <nl> + if ( info_in_entry_ptr == NULL || info_out_entry_ptr == NULL ) { <nl> /* Set the output parameters to physical addresses */ <nl> * lli_table_in_ptr = <nl> sep_shared_area_virt_to_bus ( sep , dma_in_lli_table_ptr );
u64 gfs2_ri_total ( struct gfs2_sbd * sdp ) <nl> for ( rgrps = 0 ;; rgrps ++) { <nl> loff_t pos = rgrps * sizeof ( struct gfs2_rindex ); <nl>  <nl> - if ( pos + sizeof ( struct gfs2_rindex ) >= i_size_read ( inode )) <nl> + if ( pos + sizeof ( struct gfs2_rindex ) > i_size_read ( inode )) <nl> break ; <nl> error = gfs2_internal_read ( ip , & ra_state , buf , & pos , <nl> sizeof ( struct gfs2_rindex ));
static void picolcd_debug_out_report ( struct picolcd_data * data , <nl> report -> id , raw_size ); <nl> hid_debug_event ( hdev , buff ); <nl> if ( raw_size + 5 > sizeof ( raw_data )) { <nl> + kfree ( buff ); <nl> hid_debug_event ( hdev , " TOO BIG \ n "); <nl> return ; <nl> } else {
pid_t pid_vnr ( struct pid * pid ); <nl> hlist_for_each_entry_rcu (( task ), pos___ , \ <nl> & pid -> tasks [ type ], pids [ type ]. node ) { <nl>  <nl> + /* <nl> + * Both old and new leaders may be attached to <nl> + * the same pid in the middle of de_thread (). <nl> + */ <nl> # define while_each_pid_task ( pid , type , task ) \ <nl> + if ( type == PIDTYPE_PID ) \ <nl> + break ; \ <nl> } \ <nl> } while ( 0 ) <nl> 
i830_dispatch_execbuffer ( struct intel_engine_cs * ring , <nl> */ <nl> intel_ring_emit ( ring , SRC_COPY_BLT_CMD | BLT_WRITE_RGBA ); <nl> intel_ring_emit ( ring , BLT_DEPTH_32 | BLT_ROP_SRC_COPY | 4096 ); <nl> - intel_ring_emit ( ring , DIV_ROUND_UP ( len , 4096 ) << 16 | 1024 ); <nl> + intel_ring_emit ( ring , DIV_ROUND_UP ( len , 4096 ) << 16 | 4096 ); <nl> intel_ring_emit ( ring , cs_offset ); <nl> intel_ring_emit ( ring , 4096 ); <nl> intel_ring_emit ( ring , offset );
bool __init early_can_reuse_p2m_middle ( unsigned long set_pfn , unsigned long set_ <nl> if ( p2m_index ( set_pfn )) <nl> return false ; <nl>  <nl> - for ( pfn = 0 ; pfn <= MAX_DOMAIN_PAGES ; pfn += P2M_PER_PAGE ) { <nl> + for ( pfn = 0 ; pfn < MAX_DOMAIN_PAGES ; pfn += P2M_PER_PAGE ) { <nl> topidx = p2m_top_index ( pfn ); <nl>  <nl> if (! p2m_top [ topidx ])
void __init spear13xx_l2x0_init ( void ) <nl> * write alloc and ' Full line of zero ' options <nl> * <nl> */ <nl> + if (! IS_ENABLED ( CONFIG_CACHE_L2X0 )) <nl> + return ; <nl>  <nl> writel_relaxed ( 0x06 , VA_L2CC_BASE + L2X0_PREFETCH_CTRL ); <nl> 
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 81 " <nl> -# define DRV_MODULE_RELDATE " September 5 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 82 " <nl> +# define DRV_MODULE_RELDATE " October 5 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
retry : <nl> printk (" locked it .\ n "); <nl>  <nl> do_each_thread ( g , p ) { <nl> + /* <nl> + * It ' s not reliable to print a task ' s held locks <nl> + * if it ' s not sleeping ( or if it ' s not the current <nl> + * task ): <nl> + */ <nl> + if ( p -> state == TASK_RUNNING && p != current ) <nl> + continue ; <nl> if ( p -> lockdep_depth ) <nl> lockdep_print_held_locks ( p ); <nl> if (! unlock )
static int tiadc_buffer_preenable ( struct iio_dev * indio_dev ) <nl> for ( i = 0 ; i < fifo1count ; i ++) <nl> read = tiadc_readl ( adc_dev , REG_FIFO1 ); <nl>  <nl> - return iio_sw_buffer_preenable ( indio_dev ); <nl> + return 0 ; <nl> } <nl>  <nl> static int tiadc_buffer_postenable ( struct iio_dev * indio_dev )
static int hih6130_probe ( struct i2c_client * client , <nl> hih6130 -> client = client ; <nl> mutex_init (& hih6130 -> lock ); <nl>  <nl> + if (! i2c_check_functionality ( client -> adapter , I2C_FUNC_SMBUS_QUICK )) <nl> + hih6130 -> write_length = 1 ; <nl> + <nl> hwmon_dev = devm_hwmon_device_register_with_groups ( dev , client -> name , <nl> hih6130 , <nl> hih6130_groups );
tvp514x_probe ( struct i2c_client * client , const struct i2c_device_id * id ) <nl> if ( ret < 0 ) { <nl> v4l2_err ( sd , "% s decoder driver failed to register !!\ n ", <nl> sd -> name ); <nl> - kfree ( decoder ); <nl> return ret ; <nl> } <nl> # endif
static int sbus_do_settimeofday ( struct timespec * tv ) <nl> static int set_rtc_mmss ( unsigned long secs ) <nl> { <nl> struct rtc_device * rtc = rtc_class_open (" rtc0 "); <nl> + int err = - 1 ; <nl>  <nl> - if ( rtc ) <nl> - return rtc_set_mmss ( rtc , secs ); <nl> + if ( rtc ) { <nl> + err = rtc_set_mmss ( rtc , secs ); <nl> + rtc_class_close ( rtc ); <nl> + } <nl>  <nl> - return - 1 ; <nl> + return err ; <nl> }
static int __block_prepare_write ( struct inode * inode , struct page * page , <nl> unmap_underlying_metadata ( bh -> b_bdev , <nl> bh -> b_blocknr ); <nl> if ( PageUptodate ( page )) { <nl> + clear_buffer_new ( bh ); <nl> set_buffer_uptodate ( bh ); <nl> + mark_buffer_dirty ( bh ); <nl> continue ; <nl> } <nl> if ( block_end > to || block_start < from ) {
store_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
static inline struct device_node * nand_get_flash_node ( struct nand_chip * chip ) <nl>  <nl> static inline struct nand_chip * mtd_to_nand ( struct mtd_info * mtd ) <nl> { <nl> - return mtd -> priv ; <nl> + return container_of ( mtd , struct nand_chip , mtd ); <nl> } <nl>  <nl> static inline struct mtd_info * nand_to_mtd ( struct nand_chip * chip )
static int bcm2835_dma_terminate_all ( struct dma_chan * chan ) <nl> * c -> desc is NULL and exit .) <nl> */ <nl> if ( c -> desc ) { <nl> + bcm2835_dma_desc_free (& c -> desc -> vd ); <nl> c -> desc = NULL ; <nl> bcm2835_dma_abort ( c -> chan_base ); <nl> 
bool ai_deviceremoved ( struct si_pub * sih ) <nl>  <nl> sii = ( struct si_info *) sih ; <nl>  <nl> + if ( sii -> icbus -> hosttype != BCMA_HOSTTYPE_PCI ) <nl> + return false ; <nl> + <nl> pci_read_config_dword ( sii -> pcibus , PCI_VENDOR_ID , & w ); <nl> if (( w & 0xFFFF ) != PCI_VENDOR_ID_BROADCOM ) <nl> return true ;
ecryptfs_setxattr ( struct dentry * dentry , const char * name , const void * value , <nl> } <nl>  <nl> rc = vfs_setxattr ( lower_dentry , name , value , size , flags ); <nl> - if (! rc ) <nl> + if (! rc && dentry -> d_inode ) <nl> fsstack_copy_attr_all ( dentry -> d_inode , lower_dentry -> d_inode ); <nl> out : <nl> return rc ;
static int tun_chr_open ( struct inode * inode , struct file * file ) <nl> set_bit ( SOCK_EXTERNALLY_ALLOCATED , & tfile -> socket . flags ); <nl> INIT_LIST_HEAD (& tfile -> next ); <nl>  <nl> + sock_set_flag (& tfile -> sk , SOCK_ZEROCOPY ); <nl> + <nl> return 0 ; <nl> } <nl> 
EXPORT_SYMBOL ( genphy_config_advert ); <nl> */ <nl> int genphy_setup_forced ( struct phy_device * phydev ) <nl> { <nl> - int ctl = BMCR_RESET ; <nl> + int ctl = 0 ; <nl>  <nl> phydev -> pause = phydev -> asym_pause = 0 ; <nl> 
void uf_send_pkt_to_encrypt ( struct work_struct * work ) <nl>  <nl> if ( pktBulkDataLength > 0 ) { <nl> pktBulkData = kmalloc ( pktBulkDataLength , GFP_KERNEL ); <nl> - memset ( pktBulkData , 0 , pktBulkDataLength ); <nl> } else { <nl> unifi_error ( priv , " uf_send_pkt_to_encrypt () : invalid buffer \ n "); <nl> return ;
int rtw_tkip_encrypt23a ( struct rtw_adapter * padapter , <nl> arcfour_encrypt (& mycontext , payload , payload , length ); <nl> arcfour_encrypt (& mycontext , payload + length , crc , 4 ); <nl>  <nl> - pframe += pxmitpriv -> frag_len ; <nl> - pframe = PTR_ALIGN ( pframe , 4 ); <nl> + pframe += pxmitpriv -> frag_len ; <nl> + pframe = PTR_ALIGN ( pframe , 4 ); <nl> } <nl> } <nl> 
bnad_cb_tx_resume ( struct bnad * bnad , struct bna_tcb * tcb ) <nl> static void <nl> bnad_cb_tx_cleanup ( struct bnad * bnad , struct bna_tcb * tcb ) <nl> { <nl> - struct bnad_unmap_q * unmap_q = tcb -> unmap_q ; <nl> + struct bnad_unmap_q * unmap_q ; <nl>  <nl> if (! tcb || (! tcb -> unmap_q )) <nl> return ; <nl>  <nl> + unmap_q = tcb -> unmap_q ; <nl> if (! unmap_q -> unmap_array ) <nl> return ; <nl> 
void __init qe_ic_init ( struct device_node * node , unsigned int flags ) <nl> return ; <nl>  <nl> memset ( qe_ic , 0 , sizeof ( struct qe_ic )); <nl> - qe_ic -> of_node = node ? of_node_get ( node ) : NULL ; <nl> + qe_ic -> of_node = of_node_get ( node ); <nl>  <nl> qe_ic -> irqhost = irq_alloc_host ( IRQ_HOST_MAP_LINEAR , <nl> NR_QE_IC_INTS , & qe_ic_host_ops , 0 );
 <nl> # define DRV_MODULE_NAME " bnx2 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 1 . 6 . 7 " <nl> -# define DRV_MODULE_RELDATE " October 10 , 2007 " <nl> +# define DRV_MODULE_VERSION " 1 . 6 . 8 " <nl> +# define DRV_MODULE_RELDATE " October 17 , 2007 " <nl>  <nl> # define RUN_AT ( x ) ( jiffies + ( x )) <nl> 
xfs_qm_reset_dqcounts ( <nl> */ <nl> xfs_dqcheck ( mp , ddq , id + j , type , XFS_QMOPT_DQREPAIR , <nl> " xfs_quotacheck "); <nl> + /* <nl> + * Reset type in case we are reusing group quota file for <nl> + * project quotas or vice versa <nl> + */ <nl> + ddq -> d_flags = type ; <nl> ddq -> d_bcount = 0 ; <nl> ddq -> d_icount = 0 ; <nl> ddq -> d_rtbcount = 0 ;
static void kfd_process_destroy_delayed ( struct rcu_head * rcu ) <nl> mmdrop ( p -> mm ); <nl>  <nl> work = ( struct kfd_process_release_work *) <nl> - kmalloc ( sizeof ( struct kfd_process_release_work ), GFP_KERNEL ); <nl> + kmalloc ( sizeof ( struct kfd_process_release_work ), GFP_ATOMIC ); <nl>  <nl> if ( work ) { <nl> INIT_WORK (( struct work_struct *) work , kfd_process_wq_release );
static int stmmac_init_phy ( struct net_device * dev ) <nl> interface ); <nl> } <nl>  <nl> - if ( IS_ERR ( phydev )) { <nl> + if ( IS_ERR_OR_NULL ( phydev )) { <nl> pr_err ("% s : Could not attach to PHY \ n ", dev -> name ); <nl> + if (! phydev ) <nl> + return - ENODEV ; <nl> + <nl> return PTR_ERR ( phydev ); <nl> } <nl> 
static void btrfs_submit_direct ( int rw , struct bio * bio , struct inode * inode , <nl> if (! skip_sum ) { <nl> dip -> csums = kmalloc ( sizeof ( u32 ) * bio -> bi_vcnt , GFP_NOFS ); <nl> if (! dip -> csums ) { <nl> + kfree ( dip ); <nl> ret = - ENOMEM ; <nl> goto free_ordered ; <nl> }
done : <nl> "", ( margins ) ? " with margins " : "", ( interlace ) ? <nl> " interlaced " : ""); <nl>  <nl> + memset (& cvt_mode , 0 , sizeof ( cvt_mode )); <nl> cvt_mode . xres = xres ; <nl> cvt_mode . yres = yres ; <nl> cvt_mode . refresh = ( refresh ) ? refresh : 60 ;
int drm_resctx ( struct inode * inode , struct file * filp , <nl> for ( i = 0 ; i < DRM_RESERVED_CONTEXTS ; i ++ ) { <nl> ctx . handle = i ; <nl> if ( copy_to_user ( & res . contexts [ i ], <nl> - & i , sizeof ( i ) ) ) <nl> + & ctx , sizeof ( ctx ) ) ) <nl> return - EFAULT ; <nl> } <nl> }
static struct snd_soc_codec_driver soc_codec_dev_wm8962 = { <nl> . remove = wm8962_remove , <nl> . resume = wm8962_resume , <nl> . set_bias_level = wm8962_set_bias_level , <nl> - . reg_cache_size = WM8962_MAX_REGISTER , <nl> + . reg_cache_size = WM8962_MAX_REGISTER + 1 , <nl> . reg_word_size = sizeof ( u16 ), <nl> . reg_cache_default = wm8962_reg , <nl> . volatile_register = wm8962_volatile_register ,
mv64xxx_of_config ( struct mv64xxx_i2c_data * drv_data , <nl> } <nl> tclk = clk_get_rate ( drv_data -> clk ); <nl>  <nl> - rc = of_property_read_u32 ( np , " clock - frequency ", & bus_freq ); <nl> - if ( rc ) <nl> + if ( of_property_read_u32 ( np , " clock - frequency ", & bus_freq )) <nl> bus_freq = 100000 ; /* 100kHz by default */ <nl>  <nl> if (! mv64xxx_find_baud_factors ( bus_freq , tclk ,
static int em_gio_probe ( struct platform_device * pdev ) <nl> gpio_chip -> request = em_gio_request ; <nl> gpio_chip -> free = em_gio_free ; <nl> gpio_chip -> label = name ; <nl> + gpio_chip -> dev = & pdev -> dev ; <nl> gpio_chip -> owner = THIS_MODULE ; <nl> gpio_chip -> base = pdata -> gpio_base ; <nl> gpio_chip -> ngpio = pdata -> number_of_pins ;
extern void integrator_secondary_startup ( void ); <nl> * control for which core is the next to come out of the secondary <nl> * boot " holding pen " <nl> */ <nl> - volatile int __initdata pen_release = - 1 ; <nl> - unsigned long __initdata phys_pen_release = 0 ; <nl> + volatile int __cpuinitdata pen_release = - 1 ; <nl> + unsigned long __cpuinitdata phys_pen_release = 0 ; <nl>  <nl> static DEFINE_SPINLOCK ( boot_lock ); <nl> 
# include " myri10ge_mcp . h " <nl> # include " myri10ge_mcp_gen_header . h " <nl>  <nl> -# define MYRI10GE_VERSION_STR " 1 . 0 . 0 " <nl> +# define MYRI10GE_VERSION_STR " 1 . 1 . 0 " <nl>  <nl> MODULE_DESCRIPTION (" Myricom 10G driver ( 10GbE )"); <nl> MODULE_AUTHOR (" Maintainer : help @ myri . com ");
struct ioat_dma_chan { <nl> struct delayed_work work ; <nl>  <nl> int pending ; <nl> - int dmacount ; <nl> - int desccount ; <nl> + u16 dmacount ; <nl> + u16 desccount ; <nl>  <nl> struct ioatdma_device * device ; <nl> struct dma_chan common ;
static int map_sg_data ( struct scsi_cmnd * cmd , <nl> sdev_printk ( KERN_ERR , cmd -> device , <nl> " Can ' t allocate memory " <nl> " for indirect table \ n "); <nl> + scsi_dma_unmap ( cmd ); <nl> return 0 ; <nl> } <nl> }
void read_persistent_clock ( struct timespec * ts ) <nl> year += 100 ; <nl>  <nl> ts -> tv_sec = mktime ( year , mon , day , hour , min , sec ); <nl> + ts -> tv_nsec = 0 ; <nl> } <nl>  <nl> 
static int ams_delta_led_remove ( struct platform_device * pdev ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < ARRAY_SIZE ( ams_delta_leds ); i --) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( ams_delta_leds ); i ++) <nl> led_classdev_unregister (& ams_delta_leds [ i ]. cdev ); <nl>  <nl> return 0 ;
static void mt_feature_mapping ( struct hid_device * hdev , <nl> td -> is_buttonpad = true ; <nl>  <nl> break ; <nl> + case 0xff0000c5 : <nl> + /* Retrieve the Win8 blob once to enable some devices */ <nl> + if ( usage -> usage_index == 0 ) <nl> + mt_get_feature ( hdev , field -> report ); <nl> + break ; <nl> } <nl> } <nl> 
static int GLOB_SBD_init ( void ) <nl> int i ; <nl>  <nl> /* Set debug output level ( 0 ~ 3 ) here . 3 is most verbose */ <nl> - nand_debug_level = 0 ; <nl> - <nl> printk ( KERN_ALERT " Spectra : % s \ n ", GLOB_version ); <nl>  <nl> mutex_init (& spectra_lock );
cifs_ucs2_bytes ( const __le16 * from , int maxbytes , <nl> int maxwords = maxbytes / 2 ; <nl> char tmp [ NLS_MAX_CHARSET_SIZE ]; <nl>  <nl> - for ( i = 0 ; from [ i ] && i < maxwords ; i ++) { <nl> + for ( i = 0 ; i < maxwords && from [ i ]; i ++) { <nl> charlen = codepage -> uni2char ( le16_to_cpu ( from [ i ]), tmp , <nl> NLS_MAX_CHARSET_SIZE ); <nl> if ( charlen > 0 )
int mc13xxx_common_init ( struct mc13xxx * mc13xxx , <nl> err_mask : <nl> err_revision : <nl> mc13xxx_unlock ( mc13xxx ); <nl> - kfree ( mc13xxx ); <nl> return ret ; <nl> } <nl> 
static int br_multicast_ipv6_rcv ( struct net_bridge * br , <nl> ip6h -> payload_len == 0 ) <nl> return 0 ; <nl>  <nl> - len = ntohs ( ip6h -> payload_len ); <nl> + len = ntohs ( ip6h -> payload_len ) + sizeof (* ip6h ); <nl> if ( skb -> len < len ) <nl> return - EINVAL ; <nl> 
asmlinkage long sys_ioprio_set ( int which , int who , int ioprio ) <nl> continue ; <nl> ret = set_task_ioprio ( p , ioprio ); <nl> if ( ret ) <nl> - break ; <nl> + goto free_uid ; <nl> } while_each_thread ( g , p ); <nl> - <nl> + free_uid : <nl> if ( who ) <nl> free_uid ( user ); <nl> break ;
static int lmv_iocontrol ( unsigned int cmd , struct obd_export * exp , <nl> __u32 index ; <nl>  <nl> memcpy (& index , data -> ioc_inlbuf2 , sizeof ( __u32 )); <nl> - if (( index >= count )) <nl> + if ( index >= count ) <nl> return - ENODEV ; <nl>  <nl> if ( lmv -> tgts [ index ] == NULL ||
bnx2_test_loopback ( struct bnx2 * bp ) <nl>  <nl> pkt_size = 1514 ; <nl> skb = dev_alloc_skb ( pkt_size ); <nl> + if (! skb ) <nl> + return - ENOMEM ; <nl> packet = skb_put ( skb , pkt_size ); <nl> memcpy ( packet , bp -> mac_addr , 6 ); <nl> memset ( packet + 6 , 0x0 , 8 );
static void fsg_unbind ( struct usb_configuration * c , struct usb_function * f ) <nl> fsg_common_put ( common ); <nl> usb_free_descriptors ( fsg -> function . descriptors ); <nl> usb_free_descriptors ( fsg -> function . hs_descriptors ); <nl> + usb_free_descriptors ( fsg -> function . ss_descriptors ); <nl> kfree ( fsg ); <nl> } <nl> 
static bool tcp_fastopen_create_child ( struct sock * sk , <nl> struct dst_entry * dst , <nl> struct request_sock * req ) <nl> { <nl> - struct tcp_sock * tp = tcp_sk ( sk ); <nl> + struct tcp_sock * tp ; <nl> struct request_sock_queue * queue = & inet_csk ( sk )-> icsk_accept_queue ; <nl> struct sock * child ; <nl> 
allocate_trace_buffer ( struct trace_array * tr , struct trace_buffer * buf , int size <nl>  <nl> rb_flags = trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0 ; <nl>  <nl> + buf -> tr = tr ; <nl> + <nl> buf -> buffer = ring_buffer_alloc ( size , rb_flags ); <nl> if (! buf -> buffer ) <nl> return - ENOMEM ;
int snd_soc_dapm_set_endpoint ( struct snd_soc_codec * codec , <nl> list_for_each_entry ( w , & codec -> dapm_widgets , list ) { <nl> if (! strcmp ( w -> name , endpoint )) { <nl> w -> connected = status ; <nl> + return 0 ; <nl> } <nl> } <nl>  <nl> - return 0 ; <nl> + return - ENODEV ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_set_endpoint ); <nl> 
static int add_std_chmaps ( struct hda_codec * codec ) <nl> struct snd_pcm_chmap * chmap ; <nl> const struct snd_pcm_chmap_elem * elem ; <nl>  <nl> - if (! pcm || ! pcm -> pcm || pcm -> own_chmap || <nl> - ! hinfo -> substreams ) <nl> + if (! pcm -> pcm || pcm -> own_chmap || ! hinfo -> substreams ) <nl> continue ; <nl> elem = hinfo -> chmap ? hinfo -> chmap : snd_pcm_std_chmaps ; <nl> err = snd_pcm_add_chmap_ctls ( pcm -> pcm , str , elem ,
handle_transaction ( struct link_transaction * t ) <nl> struct subaction * sa ; <nl> int i ; <nl>  <nl> + if (! t -> request ) { <nl> + printf (" BUG in handle_transaction \ n "); <nl> + return ; <nl> + } <nl> + <nl> for ( i = 0 ; i < array_length ( protocol_decoders ); i ++) <nl> if ( protocol_decoders [ i ]. decode ( t )) <nl> break ;
static int ath9k_sta_state ( struct ieee80211_hw * hw , <nl> } <nl>  <nl> if ( ath9k_is_chanctx_enabled ()) { <nl> - if ( old_state == IEEE80211_STA_ASSOC && <nl> - new_state == IEEE80211_STA_AUTHORIZED ) <nl> - ath_chanctx_event ( sc , vif , <nl> - ATH_CHANCTX_EVENT_AUTHORIZED ); <nl> + if ( vif -> type == NL80211_IFTYPE_STATION ) { <nl> + if ( old_state == IEEE80211_STA_ASSOC && <nl> + new_state == IEEE80211_STA_AUTHORIZED ) <nl> + ath_chanctx_event ( sc , vif , <nl> + ATH_CHANCTX_EVENT_AUTHORIZED ); <nl> + } <nl> } <nl>  <nl> return ret ;
static int eql_g_master_cfg ( struct net_device * dev , master_config_t __user * mcp ) <nl> equalizer_t * eql ; <nl> master_config_t mc ; <nl>  <nl> + memset (& mc , 0 , sizeof ( master_config_t )); <nl> + <nl> if ( eql_is_master ( dev )) { <nl> eql = netdev_priv ( dev ); <nl> mc . max_slaves = eql -> max_slaves ;
static cycle_t sb1250_hpt_read ( void ) <nl> } <nl>  <nl> struct clocksource bcm1250_clocksource = { <nl> - . name = " MIPS ", <nl> + . name = " bcm1250 - counter - 3 ", <nl> . rating = 200 , <nl> . read = sb1250_hpt_read , <nl> . mask = CLOCKSOURCE_MASK ( 23 ),
struct fsnotify_group * fsnotify_obtain_group ( unsigned int group_num , __u32 mask , <nl> struct fsnotify_group * group , * tgroup ; <nl>  <nl> /* very low use , simpler locking if we just always alloc */ <nl> - group = kmalloc ( sizeof ( struct fsnotify_group ), GFP_KERNEL ); <nl> + group = kzalloc ( sizeof ( struct fsnotify_group ), GFP_KERNEL ); <nl> if (! group ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
static void do_interrupt_requests ( struct kvm_vcpu * vcpu , <nl> vmx_update_window_states ( vcpu ); <nl>  <nl> if ( vcpu -> arch . nmi_pending && ! vcpu -> arch . nmi_injected ) { <nl> - if ( vcpu -> arch . nmi_window_open ) { <nl> + if ( vcpu -> arch . interrupt . pending ) { <nl> + enable_nmi_window ( vcpu ); <nl> + } else if ( vcpu -> arch . nmi_window_open ) { <nl> vcpu -> arch . nmi_pending = false ; <nl> vcpu -> arch . nmi_injected = true ; <nl> } else {
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> */ <nl> if ( skb_shared ( skb )) { <nl> tmp_skb = skb ; <nl> - skb = skb_copy ( skb , GFP_ATOMIC ); <nl> + skb = skb_clone ( skb , GFP_ATOMIC ); <nl> kfree_skb ( tmp_skb ); <nl>  <nl> if (! skb ) {
static int egalax_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> struct egalax_data * td ; <nl> struct hid_report * report ; <nl>  <nl> - td = kmalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> + td = kzalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> if (! td ) { <nl> dev_err (& hdev -> dev , " cannot allocate eGalax data \ n "); <nl> return - ENOMEM ;
static int sdhci_esdhc_imx_probe ( struct platform_device * pdev ) <nl> * to something insane . Change it back here . <nl> */ <nl> if ( esdhc_is_usdhc ( imx_data )) { <nl> - writel ( 0x08100810 , host -> ioaddr + ESDHC_WTMK_LVL ); <nl> + writel ( 0x10401040 , host -> ioaddr + ESDHC_WTMK_LVL ); <nl> + <nl> host -> quirks2 |= SDHCI_QUIRK2_PRESET_VALUE_BROKEN ; <nl> host -> mmc -> caps |= MMC_CAP_1_8V_DDR ; <nl> 
int i915_vma_unbind ( struct i915_vma * vma ) <nl> * cause memory corruption through use - after - free . <nl> */ <nl>  <nl> + /* Throw away the active reference before moving to the unbound list */ <nl> + i915_gem_object_retire ( obj ); <nl> + <nl> if ( i915_is_ggtt ( vma -> vm )) { <nl> i915_gem_object_finish_gtt ( obj ); <nl> 
static int map_lookup_elem ( union bpf_attr * attr ) <nl> if ( copy_from_user ( key , ukey , map -> key_size ) != 0 ) <nl> goto free_key ; <nl>  <nl> - err = - ESRCH ; <nl> + err = - ENOENT ; <nl> rcu_read_lock (); <nl> value = map -> ops -> map_lookup_elem ( map , key ); <nl> if (! value )
static int mv643xx_eth_stop ( struct net_device * dev ) <nl> struct mv643xx_eth_private * mp = netdev_priv ( dev ); <nl> int i ; <nl>  <nl> + wrlp ( mp , INT_MASK_EXT , 0x00000000 ); <nl> wrlp ( mp , INT_MASK , 0x00000000 ); <nl> rdlp ( mp , INT_MASK ); <nl> 
static int __exit usba_udc_remove ( struct platform_device * pdev ) <nl> usba_ep_cleanup_debugfs (& usba_ep [ i ]); <nl> usba_cleanup_debugfs ( udc ); <nl>  <nl> - if ( gpio_is_valid ( udc -> vbus_pin )) <nl> + if ( gpio_is_valid ( udc -> vbus_pin )) { <nl> + free_irq ( gpio_to_irq ( udc -> vbus_pin ), udc ); <nl> gpio_free ( udc -> vbus_pin ); <nl> + } <nl>  <nl> free_irq ( udc -> irq , udc ); <nl> kfree ( usba_ep );
static void ath_ant_comb_scan ( struct ath_softc * sc , struct ath_rx_status * rs ) <nl> main_ant_conf = ( rs -> rs_rssi_ctl2 >> ATH_ANT_RX_MAIN_SHIFT ) & <nl> ATH_ANT_RX_MASK ; <nl>  <nl> - /* Record packet only when alt_rssi is positive */ <nl> - if ( alt_rssi > 0 ) { <nl> + /* Record packet only when both main_rssi and alt_rssi is positive */ <nl> + if ( main_rssi > 0 && alt_rssi > 0 ) { <nl> antcomb -> total_pkt_count ++; <nl> antcomb -> main_total_rssi += main_rssi ; <nl> antcomb -> alt_total_rssi += alt_rssi ;
static int chd_dec_fetch_cdata ( struct crystalhd_adp * adp , <nl> if ( rc ) { <nl> BCMLOG_ERR (" failed to pull add_cdata sz :% x ua_off :% x \ n ", <nl> io -> add_cdata_sz , ( unsigned int ) ua_off ); <nl> - kfree ( io -> add_cdata ); <nl> + vfree ( io -> add_cdata ); <nl> io -> add_cdata = NULL ; <nl> return - ENODATA ; <nl> }
static int irda_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> struct sock * sk = sock -> sk ; <nl> struct irda_sock * self = irda_sk ( sk ); <nl>  <nl> + memset (& saddr , 0 , sizeof ( saddr )); <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED ) <nl> return - ENOTCONN ;
static struct svc_xprt * svc_rdma_create ( struct svc_serv * serv , <nl> int ret ; <nl>  <nl> dprintk (" svcrdma : Creating RDMA socket \ n "); <nl> - <nl> + if ( sa -> sa_family != AF_INET ) { <nl> + dprintk (" svcrdma : Address family % d is not supported .\ n ", sa -> sa_family ); <nl> + return ERR_PTR (- EAFNOSUPPORT ); <nl> + } <nl> cma_xprt = rdma_create_xprt ( serv , 1 ); <nl> if (! cma_xprt ) <nl> return ERR_PTR (- ENOMEM );
static void collapse_huge_page ( struct mm_struct * mm , <nl> set_pmd_at ( mm , address , pmd , _pmd ); <nl> spin_unlock (& mm -> page_table_lock ); <nl> anon_vma_unlock ( vma -> anon_vma ); <nl> - mem_cgroup_uncharge_page ( new_page ); <nl> goto out ; <nl> } <nl>  <nl> out_up_write : <nl> return ; <nl>  <nl> out : <nl> + mem_cgroup_uncharge_page ( new_page ); <nl> # ifdef CONFIG_NUMA <nl> put_page ( new_page ); <nl> # endif
int wm8350_device_init ( struct wm8350 * wm8350 , int irq , <nl> wm8350 -> power . rev_g_coeff = 1 ; <nl> break ; <nl>  <nl> + case 1 : <nl> + dev_info ( wm8350 -> dev , " WM8351 Rev B \ n "); <nl> + wm8350 -> power . rev_g_coeff = 1 ; <nl> + break ; <nl> + <nl> default : <nl> dev_err ( wm8350 -> dev , " Unknown WM8351 CHIP_REV \ n "); <nl> ret = - ENODEV ;
iscsi_get_host_stats ( struct iscsi_transport * transport , struct nlmsghdr * nlh ) <nl> memset ( buf , 0 , host_stats_size ); <nl>  <nl> err = transport -> get_host_stats ( shost , buf , host_stats_size ); <nl> + if ( err ) { <nl> + kfree ( skbhost_stats ); <nl> + goto exit_host_stats ; <nl> + } <nl>  <nl> actual_size = nlmsg_total_size ( sizeof (* ev ) + host_stats_size ); <nl> skb_trim ( skbhost_stats , NLMSG_ALIGN ( actual_size ));
static struct pci_controller cobalt_pci_controller = { <nl> . mem_resource = & cobalt_mem_resource , <nl> . io_resource = & cobalt_io_resource , <nl> . io_offset = 0 - GT_DEF_PCI0_IO_BASE , <nl> + . io_map_base = CKSEG1ADDR ( GT_DEF_PCI0_IO_BASE ), <nl> }; <nl>  <nl> static int __init cobalt_pci_init ( void )
static int __devinit cpmac_probe ( struct platform_device * pdev ) <nl> priv -> dev = dev ; <nl> priv -> ring_size = 64 ; <nl> priv -> msg_enable = netif_msg_init ( debug_level , 0xff ); <nl> - memcpy ( dev -> dev_addr , pdata -> dev_addr , sizeof ( dev -> dev_addr )); <nl> + memcpy ( dev -> dev_addr , pdata -> dev_addr , sizeof ( pdata -> dev_addr )); <nl>  <nl> snprintf ( priv -> phy_name , MII_BUS_ID_SIZE , PHY_ID_FMT , mdio_bus_id , phy_id ); <nl> 
static struct usb_request * isp1760_ep_alloc_request ( struct usb_ep * ep , <nl> struct isp1760_request * req ; <nl>  <nl> req = kzalloc ( sizeof (* req ), gfp_flags ); <nl> + if (! req ) <nl> + return NULL ; <nl>  <nl> return & req -> req ; <nl> }
int tcp_disconnect ( struct sock * sk , int flags ) <nl> tp -> snd_ssthresh = TCP_INFINITE_SSTHRESH ; <nl> tp -> snd_cwnd_cnt = 0 ; <nl> tp -> bytes_acked = 0 ; <nl> + tp -> window_clamp = 0 ; <nl> tcp_set_ca_state ( sk , TCP_CA_Open ); <nl> tcp_clear_retrans ( tp ); <nl> inet_csk_delack_init ( sk );
# define VI6_DISP_IRQ_ENB 0x0078 <nl> # define VI6_DISP_IRQ_ENB_DSTE ( 1 << 8 ) <nl> # define VI6_DISP_IRQ_ENB_MAEE ( 1 << 5 ) <nl> -# define VI6_DISP_IRQ_ENB_LNEE ( n ) ( 1 << (( n ) + 4 )) <nl> +# define VI6_DISP_IRQ_ENB_LNEE ( n ) ( 1 << ( n )) <nl>  <nl> # define VI6_DISP_IRQ_STA 0x007c <nl> # define VI6_DISP_IRQ_STA_DSE ( 1 << 8 )
static int fc_user_scan ( struct Scsi_Host * shost , uint channel , <nl> if ( rport -> scsi_target_id == - 1 ) <nl> continue ; <nl>  <nl> + if ( rport -> port_state != FC_PORTSTATE_ONLINE ) <nl> + continue ; <nl> + <nl> if (( channel == SCAN_WILD_CARD || channel == rport -> channel ) && <nl> ( id == SCAN_WILD_CARD || id == rport -> scsi_target_id )) { <nl> scsi_scan_target (& rport -> dev , rport -> channel ,
void * consistent_alloc ( gfp_t gfp , size_t size , dma_addr_t * handle ) <nl> split_page ( page , order ); <nl>  <nl> ret = page_address ( page ); <nl> + memset ( ret , 0 , size ); <nl> * handle = virt_to_phys ( ret ); <nl>  <nl> /*
static struct config_item * uvcg_frame_make ( struct config_group * group , <nl> h -> fmt_type = UVCG_MJPEG ; <nl> } else { <nl> mutex_unlock (& opts -> lock ); <nl> + kfree ( h ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> ++ fmt -> num_frames ;
static int iio_read_first_n_kfifo ( struct iio_buffer * r , <nl> int ret , copied ; <nl> struct iio_kfifo * kf = iio_to_kfifo ( r ); <nl>  <nl> - if ( n < r -> bytes_per_datum ) <nl> + if ( n < r -> bytes_per_datum || r -> bytes_per_datum == 0 ) <nl> return - EINVAL ; <nl>  <nl> ret = kfifo_to_user (& kf -> kf , buf , n , & copied ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> return copied ; <nl> }
nouveau_cli_destroy ( struct nouveau_cli * cli ) <nl> nvkm_vm_ref ( NULL , & nvxx_client (& cli -> base )-> vm , NULL ); <nl> nvif_client_fini (& cli -> base ); <nl> usif_client_fini ( cli ); <nl> + kfree ( cli ); <nl> } <nl>  <nl> static void
extern int ext4_init_inode_table ( struct super_block * sb , ext4_group_t group , <nl> group , used_blks , <nl> ext4_itable_unused_count ( sb , gdp )); <nl> ret = 1 ; <nl> - goto out ; <nl> + goto err_out ; <nl> } <nl>  <nl> blk = ext4_inode_table ( sb , gdp ) + used_blks ;
asmlinkage void math_emulate ( long arg ) <nl> entry_sel_off . offset = FPU_ORIG_EIP ; <nl> entry_sel_off . selector = FPU_CS ; <nl> entry_sel_off . opcode = ( byte1 << 8 ) | FPU_modrm ; <nl> + entry_sel_off . empty = 0 ; <nl>  <nl> FPU_rm = FPU_modrm & 7 ; <nl> 
enum { <nl> ATA_REG_IRQ = ATA_REG_NSECT , <nl>  <nl> /* ATA device commands */ <nl> + ATA_CMD_DEV_RESET = 0x08 , /* ATAPI device reset */ <nl> ATA_CMD_CHK_POWER = 0xE5 , /* check power mode */ <nl> ATA_CMD_STANDBY = 0xE2 , /* place in standby power mode */ <nl> ATA_CMD_IDLE = 0xE3 , /* place in idle power mode */
static int _search_rsb ( struct dlm_ls * ls , char * name , int len , int b , <nl> kref_get (& r -> res_ref ); <nl> goto out ; <nl> } <nl> + if ( error == - ENOTBLK ) <nl> + goto out ; <nl> + <nl> error = dlm_search_rsb_tree (& ls -> ls_rsbtbl [ b ]. toss , name , len , flags , & r ); <nl> if ( error ) <nl> goto out ;
static int ncp_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> case 0x00 : <nl> ncp_dbg ( 1 , " renamed % pd -> % pd \ n ", <nl> old_dentry , new_dentry ); <nl> + ncp_d_prune ( old_dentry ); <nl> + ncp_d_prune ( new_dentry ); <nl> break ; <nl> case 0x9E : <nl> error = - ENAMETOOLONG ;
/* <nl> * Literals <nl> */ <nl> -# define IPR_DRIVER_VERSION " 2 . 1 . 2 " <nl> -# define IPR_DRIVER_DATE "( February 8 , 2006 )" <nl> +# define IPR_DRIVER_VERSION " 2 . 1 . 3 " <nl> +# define IPR_DRIVER_DATE "( March 29 , 2006 )" <nl>  <nl> /* <nl> * IPR_MAX_CMD_PER_LUN : This defines the maximum number of outstanding
* differently than other MIPS interrupts . <nl> */ <nl>  <nl> - static void gt64120_irq ( int irq , void * dev_id ) <nl> + static irqreturn_t gt64120_irq ( int irq , void * dev_id ) <nl> { <nl> unsigned int irq_src , int_high_src , irq_src_mask , int_high_src_mask ; <nl> int handled = 0 ; <nl> static void gt64120_irq ( int irq , void * dev_id ) <nl>  <nl> GT_WRITE ( GT_INTRCAUSE_OFS , 0 ); <nl> GT_WRITE ( GT_HINTRCAUSE_OFS , 0 ); <nl> + <nl> + return IRQ_HANDLED ; <nl> } <nl>  <nl> /*
static void __init tegra210_pll_init ( void __iomem * clk_base , <nl>  <nl> /* PLLU_VCO */ <nl> val = readl ( clk_base + pll_u_vco_params . base_reg ); <nl> - val &= ~ BIT ( 24 ); /* disable PLLU_OVERRIDE */ <nl> + val &= ~ PLLU_BASE_OVERRIDE ; /* disable PLLU_OVERRIDE */ <nl> writel ( val , clk_base + pll_u_vco_params . base_reg ); <nl>  <nl> clk = tegra_clk_register_pllre (" pll_u_vco ", " pll_ref ", clk_base , pmc ,
retry_snap : <nl> goto retry_snap ; <nl> } <nl> } else { <nl> + loff_t old_size = inode -> i_size ; <nl> /* <nl> * No need to acquire the i_truncate_mutex . Because <nl> * the MDS revokes Fwb caps before sending truncate <nl> retry_snap : <nl> written = generic_file_buffered_write ( iocb , iov , nr_segs , <nl> pos , & iocb -> ki_pos , <nl> count , 0 ); <nl> + if ( inode -> i_size > old_size ) <nl> + ceph_fscache_update_objectsize ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> } <nl> 
static void iwl_ucode_callback ( const struct firmware * ucode_raw , void * context ) <nl> const struct iwl_op_mode_ops * ops = op -> ops ; <nl> drv -> op_mode = ops -> start ( drv -> trans , drv -> cfg , & drv -> fw ); <nl>  <nl> - if (! drv -> op_mode ) <nl> + if (! drv -> op_mode ) { <nl> + mutex_unlock (& iwlwifi_opmode_table_mtx ); <nl> goto out_unbind ; <nl> + } <nl> } else { <nl> load_module = true ; <nl> }
ath5k_deinit_softc ( struct ath5k_softc * sc ) <nl> * state and potentially want to use them . <nl> */ <nl> ath5k_hw_deinit ( sc -> ah ); <nl> + kfree ( sc -> ah ); <nl> free_irq ( sc -> irq , sc ); <nl> } <nl> 
struct dst_entry * fib6_rule_lookup ( struct net * net , struct flowi * fl , <nl> { <nl> struct fib_lookup_arg arg = { <nl> . lookup_ptr = lookup , <nl> + . flags = FIB_LOOKUP_NOREF , <nl> }; <nl>  <nl> fib_rules_lookup ( net -> ipv6 . fib6_rules_ops , fl , flags , & arg ); <nl> - if ( arg . rule ) <nl> - fib_rule_put ( arg . rule ); <nl>  <nl> if ( arg . result ) <nl> return arg . result ;
int kvm_ioapic_set_irq ( struct kvm_ioapic * ioapic , int irq , int level ) <nl> if (( edge && old_irr != ioapic -> irr ) || <nl> (! edge && ! entry . fields . remote_irr )) <nl> ret = ioapic_service ( ioapic , irq ); <nl> + else <nl> + ret = 0 ; /* report coalesced interrupt */ <nl> } <nl> trace_kvm_ioapic_set_irq ( entry . bits , irq , ret == 0 ); <nl> }
static int ceph_link ( struct dentry * old_dentry , struct inode * dir , <nl> req -> r_locked_dir = dir ; <nl> req -> r_dentry_drop = CEPH_CAP_FILE_SHARED ; <nl> req -> r_dentry_unless = CEPH_CAP_FILE_EXCL ; <nl> + /* release LINK_SHARED on source inode ( mds will lock it ) */ <nl> + req -> r_old_inode_drop = CEPH_CAP_LINK_SHARED ; <nl> err = ceph_mdsc_do_request ( mdsc , dir , req ); <nl> if ( err ) { <nl> d_drop ( dentry );
try_again : <nl> * based cpu - clock - tick sw counter , which <nl> * is always available even if no PMU support : <nl> */ <nl> - if ( attr -> type == PERF_TYPE_HARDWARE <nl> + if ( err == ENOENT && attr -> type == PERF_TYPE_HARDWARE <nl> && attr -> config == PERF_COUNT_HW_CPU_CYCLES ) { <nl>  <nl> if ( verbose )
int xhci_add_endpoint ( struct usb_hcd * hcd , struct usb_device * udev , <nl> * for usb_set_interface () and usb_set_configuration () claim ). <nl> */ <nl> if ( xhci_endpoint_init ( xhci , xhci -> devs [ udev -> slot_id ], <nl> - udev , ep , GFP_KERNEL ) < 0 ) { <nl> + udev , ep , GFP_NOIO ) < 0 ) { <nl> dev_dbg (& udev -> dev , "% s - could not initialize ep %# x \ n ", <nl> __func__ , ep -> desc . bEndpointAddress ); <nl> return - ENOMEM ;
int wlan_unsetup ( wlandevice_t * wlandev ) <nl>  <nl> if ( wlandev -> netdev ) { <nl> wdev = netdev_priv ( wlandev -> netdev ); <nl> - if ( wdev -> wiphy ) wlan_free_wiphy ( wdev -> wiphy ); <nl> + if ( wdev -> wiphy ) <nl> + wlan_free_wiphy ( wdev -> wiphy ); <nl> free_netdev ( wlandev -> netdev ); <nl> wlandev -> netdev = NULL ; <nl> }
static inline void x86_assign_hw_event ( struct perf_event * event , <nl> } else { <nl> hwc -> config_base = x86_pmu_config_addr ( hwc -> idx ); <nl> hwc -> event_base = x86_pmu_event_addr ( hwc -> idx ); <nl> - hwc -> event_base_rdpmc = x86_pmu_addr_offset ( hwc -> idx ); <nl> + hwc -> event_base_rdpmc = hwc -> idx ; <nl> } <nl> } <nl> 
void dump_stack ( void ) <nl> EXPORT_SYMBOL ( dump_stack ); <nl>  <nl> /* Stolen from arch / i386 / kernel / traps . c */ <nl> - static int kstack_depth_to_print = 24 ; <nl> + static const int kstack_depth_to_print = 24 ; <nl>  <nl> /* This recently started being used in arch - independent code too , as in <nl> * kernel / sched . c .*/
static int device_authorization ( struct hdpvr_device * dev ) <nl> hex_dump_to_buffer ( response , 8 , 16 , 1 , print_buf , 5 * buf_size + 1 , 0 ); <nl> v4l2_dbg ( MSG_INFO , hdpvr_debug , & dev -> v4l2_dev , " response : % s \ n ", <nl> print_buf ); <nl> + kfree ( print_buf ); <nl> # endif <nl>  <nl> msleep ( 100 );
static int hostap_enable_hostapd ( PSDevice pDevice , int rtnl_locked ) <nl> if ( ret ) { <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO "% s : register_netdevice ( AP ) failed !\ n ", <nl> dev -> name ); <nl> + free_netdev ( pDevice -> apdev ); <nl> + pDevice -> apdev = NULL ; <nl> return - 1 ; <nl> } <nl> 
static void __cpuinit init_amd ( struct cpuinfo_x86 * c ) <nl> level = cpuid_eax ( 1 ); <nl> if ( c -> x86 == 15 && (( level >= 0x0f48 && level < 0x0f50 ) || level >= 0x0f58 )) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl> - if ( c -> x86 == 0x10 ) <nl> + if ( c -> x86 == 0x10 || c -> x86 == 0x11 ) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl>  <nl> /* Enable workaround for FXSAVE leak */
__setup (" noclflush ", setup_noclflush ); <nl> void __cpuinit print_cpu_info ( struct cpuinfo_x86 * c ) <nl> { <nl> if ( c -> x86_model_id [ 0 ]) <nl> - printk ( KERN_INFO "% s ", c -> x86_model_id ); <nl> + printk ( KERN_CONT "% s ", c -> x86_model_id ); <nl>  <nl> if ( c -> x86_mask || c -> cpuid_level >= 0 ) <nl> printk ( KERN_CONT " stepping % 02x \ n ", c -> x86_mask );
int tpm_open ( struct inode * inode , struct file * file ) <nl> return - EBUSY ; <nl> } <nl>  <nl> - chip -> data_buffer = kmalloc ( TPM_BUFSIZE * sizeof ( u8 ), GFP_KERNEL ); <nl> + chip -> data_buffer = kzalloc ( TPM_BUFSIZE , GFP_KERNEL ); <nl> if ( chip -> data_buffer == NULL ) { <nl> clear_bit ( 0 , & chip -> is_open ); <nl> put_device ( chip -> dev );
static int __do_index ( struct sw842_param * p , u8 size , u8 bits , u64 fsize ) <nl> /* this is where the current fifo is */ <nl> u64 section = round_down ( total , fsize ); <nl> /* the current pos in the fifo */ <nl> - u64 pos = total % fsize ; <nl> + u64 pos = total - section ; <nl>  <nl> /* if the offset is past / at the pos , we need to <nl> * go back to the last fifo section
static int cb_pcidas_attach ( struct comedi_device * dev , <nl>  <nl> /* 8255 */ <nl> s = dev -> subdevices + 2 ; <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + ret = subdev_8255_init ( dev , s , NULL , <nl> + devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* serial EEPROM , */ <nl> s = dev -> subdevices + 3 ;
static int create_i2c_bus ( struct i2c_adapter * adapter , <nl> algo -> setscl = via_i2c_setscl ; <nl> algo -> getsda = via_i2c_getsda ; <nl> algo -> getscl = via_i2c_getscl ; <nl> - algo -> udelay = 40 ; <nl> - algo -> timeout = 20 ; <nl> + algo -> udelay = 10 ; <nl> + algo -> timeout = 2 ; <nl> algo -> data = adap_cfg ; <nl>  <nl> sprintf ( adapter -> name , " viafb i2c io_port idx 0x % 02x ",
static const struct of_device_id spear13xx_pcie_of_match [] = { <nl> }; <nl> MODULE_DEVICE_TABLE ( of , spear13xx_pcie_of_match ); <nl>  <nl> - static struct platform_driver spear13xx_pcie_driver = { <nl> + static struct platform_driver spear13xx_pcie_driver __initdata = { <nl> . probe = spear13xx_pcie_probe , <nl> . remove = spear13xx_pcie_remove , <nl> . driver = {
void afu_release_irqs ( struct cxl_context * ctx , void * cookie ) <nl>  <nl> afu_irq_name_free ( ctx ); <nl> cxl_release_irq_ranges (& ctx -> irqs , ctx -> afu -> adapter ); <nl> + <nl> + kfree ( ctx -> irq_bitmap ); <nl> + ctx -> irq_bitmap = NULL ; <nl> + ctx -> irq_count = 0 ; <nl> }
static int get_skb_hdr ( struct sk_buff * skb , void ** iphdr , <nl> * tcph = tcp_hdr ( skb ); <nl>  <nl> /* check if ip header and tcp header are complete */ <nl> - if ( iph -> tot_len < ip_len + tcp_hdrlen ( skb )) <nl> + if ( ntohs ( iph -> tot_len ) < ip_len + tcp_hdrlen ( skb )) <nl> return - 1 ; <nl>  <nl> * hdr_flags = LRO_IPV4 | LRO_TCP ;
static void deallocate_vmid ( struct device_queue_manager * dqm , <nl> { <nl> int bit = qpd -> vmid - KFD_VMID_START_OFFSET ; <nl>  <nl> + /* Release the vmid mapping */ <nl> + set_pasid_vmid_mapping ( dqm , 0 , qpd -> vmid ); <nl> + <nl> set_bit ( bit , ( unsigned long *)& dqm -> vmid_bitmap ); <nl> qpd -> vmid = 0 ; <nl> q -> properties . vmid = 0 ;
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
free_work : <nl> out_hang : <nl> intel_crtc_wait_for_pending_flips ( crtc ); <nl> ret = intel_pipe_set_base ( crtc , crtc -> x , crtc -> y , fb ); <nl> - if ( ret == 0 && event ) <nl> + if ( ret == 0 && event ) { <nl> + spin_lock_irqsave (& dev -> event_lock , flags ); <nl> drm_send_vblank_event ( dev , pipe , event ); <nl> + spin_unlock_irqrestore (& dev -> event_lock , flags ); <nl> + } <nl> } <nl> return ret ; <nl> }
int __init omap_mux_init ( u32 mux_pbase , u32 mux_size , <nl> } <nl>  <nl> # ifdef CONFIG_OMAP_MUX <nl> - omap_mux_package_fixup ( package_subset , superset ); <nl> - omap_mux_package_init_balls ( package_balls , superset ); <nl> + if ( package_subset ) <nl> + omap_mux_package_fixup ( package_subset , superset ); <nl> + if ( package_balls ) <nl> + omap_mux_package_init_balls ( package_balls , superset ); <nl> omap_mux_set_cmdline_signals (); <nl> omap_mux_set_board_signals ( board_mux ); <nl> # endif
static int check_in_drive_lists ( ide_drive_t * drive , const char ** list ) <nl> static u8 svwks_ratemask ( ide_drive_t * drive ) <nl> { <nl> struct pci_dev * dev = HWIF ( drive )-> pci_dev ; <nl> - u8 mode ; <nl> + u8 mode = 0 ; <nl>  <nl> if (! svwks_revision ) <nl> pci_read_config_byte ( dev , PCI_REVISION_ID , & svwks_revision );
static void __init_memblock memblock_merge_regions ( struct memblock_type * type ) <nl> } <nl>  <nl> this -> size += next -> size ; <nl> - memmove ( next , next + 1 , ( type -> cnt - ( i + 1 )) * sizeof (* next )); <nl> + /* move forward from next + 1 , index of which is i + 2 */ <nl> + memmove ( next , next + 1 , ( type -> cnt - ( i + 2 )) * sizeof (* next )); <nl> type -> cnt --; <nl> } <nl> }
static int __devinit ab8500_ponkey_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_free_dbr_irq : <nl> - free_irq ( ponkey -> irq_dbf , ponkey ); <nl> + free_irq ( ponkey -> irq_dbr , ponkey ); <nl> err_free_dbf_irq : <nl> free_irq ( ponkey -> irq_dbf , ponkey ); <nl> err_free_mem :
static void ixgbe_reset_subtask ( struct ixgbe_adapter * adapter ) <nl> netdev_err ( adapter -> netdev , " Reset adapter \ n "); <nl> adapter -> tx_timeout_count ++; <nl>  <nl> + rtnl_lock (); <nl> ixgbe_reinit_locked ( adapter ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> /**
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , u32 features ) <nl> skb -> ip_summed = CHECKSUM_NONE ; <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> - if (( skb_headroom ( skb ) < frag_hdr_sz ) && <nl> + if (( skb_mac_header ( skb ) < skb -> head + frag_hdr_sz ) && <nl> pskb_expand_head ( skb , frag_hdr_sz , 0 , GFP_ATOMIC )) <nl> goto out ; <nl> 
int genwqe_device_create ( struct genwqe_dev * cd ) <nl> genwqe_attribute_groups , <nl> GENWQE_DEVNAME "% u_card ", <nl> cd -> card_idx ); <nl> - if ( cd -> dev == NULL ) { <nl> - rc = - ENODEV ; <nl> + if ( IS_ERR ( cd -> dev )) { <nl> + rc = PTR_ERR ( cd -> dev ); <nl> goto err_cdev ; <nl> } <nl> 
static int md_open ( struct block_device * bdev , fmode_t mode ) <nl> struct mddev * mddev = mddev_find ( bdev -> bd_dev ); <nl> int err ; <nl>  <nl> + if (! mddev ) <nl> + return - ENODEV ; <nl> + <nl> if ( mddev -> gendisk != bdev -> bd_disk ) { <nl> /* we are racing with mddev_put which is discarding this <nl> * bd_disk .
static struct clk_lookup lookups [] = { <nl>  <nl> /* MSTP32 clocks */ <nl> CLKDEV_DEV_ID (" sh_mmcif ", & mstp_clks [ MSTP331 ]), /* MMC */ <nl> + CLKDEV_DEV_ID (" ffe4e000 . mmcif ", & mstp_clks [ MSTP331 ]), /* MMC */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 0 ", & mstp_clks [ MSTP323 ]), /* SDHI0 */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 1 ", & mstp_clks [ MSTP322 ]), /* SDHI1 */ <nl> CLKDEV_DEV_ID (" sh_mobile_sdhi . 2 ", & mstp_clks [ MSTP321 ]), /* SDHI2 */
static int imx6q_revision ( void ) <nl> } <nl> } <nl>  <nl> - void imx6q_restart ( char mode , const char * cmd ) <nl> + static void imx6q_restart ( char mode , const char * cmd ) <nl> { <nl> struct device_node * np ; <nl> void __iomem * wdog_base ; <nl> put_node : <nl> of_node_put ( np ); <nl> } <nl>  <nl> - struct platform_device imx6q_cpufreq_pdev = { <nl> + static struct platform_device imx6q_cpufreq_pdev = { <nl> . name = " imx6q - cpufreq ", <nl> }; <nl> 
static int uio_pdrv_remove ( struct platform_device * pdev ) <nl>  <nl> uio_unregister_device ( pdata -> uioinfo ); <nl>  <nl> + kfree ( pdata ); <nl> + <nl> return 0 ; <nl> } <nl> 
struct i810fb_par { <nl> struct i810fb_i2c_chan chan [ 3 ]; <nl> struct mutex open_lock ; <nl> unsigned int use_count ; <nl> - u32 pseudo_palette [ 17 ]; <nl> + u32 pseudo_palette [ 16 ]; <nl> unsigned long mmio_start_phys ; <nl> u8 __iomem * mmio_start_virtual ; <nl> u8 * edid ;
int ieee80211_master_start_xmit ( struct sk_buff * skb , <nl> return 0 ; <nl> } <nl>  <nl> + if ( osdata -> vif . type == NL80211_IFTYPE_AP_VLAN ) <nl> + osdata = container_of ( osdata -> bss , <nl> + struct ieee80211_sub_if_data , <nl> + u . ap ); <nl> info -> control . vif = & osdata -> vif ; <nl> ret = ieee80211_tx ( odev , skb ); <nl> dev_put ( odev );
void btrfsic_unmount ( struct btrfs_root * root , <nl> btrfsic_block_link_free ( l ); <nl> } <nl>  <nl> - if ( b_all -> is_iodone ) <nl> + if ( b_all -> is_iodone || b_all -> never_written ) <nl> btrfsic_block_free ( b_all ); <nl> else <nl> printk ( KERN_INFO " btrfs : attempt to free % c - block "
void dump_trace ( struct task_struct * task , struct pt_regs * regs , <nl> if ( UNW_SP (& info ) >= PAGE_OFFSET ) { <nl> ops -> warning ( data , " Leftover inexact backtrace :\ n "); <nl> stack = ( void *) UNW_SP (& info ); <nl> + if (! stack ) <nl> + return ; <nl> } else <nl> ops -> warning ( data , " Full inexact backtrace again :\ n "); <nl> } else if ( call_trace >= 1 )
static int snd_timer_s_stop ( struct snd_timer * timer ) <nl> timer -> sticks = priv -> last_expires - jiff ; <nl> else <nl> timer -> sticks = 1 ; <nl> + priv -> correction = 0 ; <nl> return 0 ; <nl> } <nl> 
s32 e1000e_setup_fiber_serdes_link ( struct e1000_hw * hw ) <nl> e_dbg (" No signal detected \ n "); <nl> } <nl>  <nl> - return 0 ; <nl> + return ret_val ; <nl> } <nl>  <nl> /**
static int __devinit adm8211_probe ( struct pci_dev * pdev , <nl>  <nl> priv -> channel = 1 ; <nl>  <nl> + dev -> wiphy -> bands [ IEEE80211_BAND_2GHZ ] = & priv -> band ; <nl> + <nl> err = ieee80211_register_hw ( dev ); <nl> if ( err ) { <nl> printk ( KERN_ERR "% s ( adm8211 ): Cannot register device \ n ",
static void __devinit pci_read_bridge_io ( struct pci_bus * child ) <nl>  <nl> if ( base && base <= limit ) { <nl> res -> flags = ( io_base_lo & PCI_IO_RANGE_TYPE_MASK ) | IORESOURCE_IO ; <nl> + res2 . flags = res -> flags ; <nl> region . start = base ; <nl> region . end = limit + 0xfff ; <nl> pcibios_bus_to_resource ( dev , & res2 , & region );
static int dispatch_procfs_write ( struct file * file , <nl>  <nl> if (! ibm || ! ibm -> write ) <nl> return - EINVAL ; <nl> + if ( count > PAGE_SIZE - 2 ) <nl> + return - EINVAL ; <nl>  <nl> kernbuf = kmalloc ( count + 2 , GFP_KERNEL ); <nl> if (! kernbuf )
ret_orig : <nl> kfree_skb ( clone ); <nl> return skb ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( nf_ct_frag6_gather ); <nl>  <nl> void nf_ct_frag6_consume_orig ( struct sk_buff * skb ) <nl> {
intel_dp_max_link_bw ( struct intel_dp * intel_dp ) <nl> case DP_LINK_BW_1_62 : <nl> case DP_LINK_BW_2_7 : <nl> break ; <nl> + case DP_LINK_BW_5_4 : /* 1 . 2 capable displays may advertise higher bw */ <nl> + max_link_bw = DP_LINK_BW_2_7 ; <nl> + break ; <nl> default : <nl> + WARN ( 1 , " invalid max DP link bw val % x , using 1 . 62Gbps \ n ", <nl> + max_link_bw ); <nl> max_link_bw = DP_LINK_BW_1_62 ; <nl> break ; <nl> }
 <nl> # include < linux / seq_file . h > <nl> # include < linux / list . h > <nl> +# include < linux / vmalloc . h > <nl> # include " debug . h " <nl> # include " ath5k . h " <nl> # include " reg . h "
void (* mach_beep )( unsigned int , unsigned int ); <nl> # if defined ( CONFIG_ISA ) && defined ( MULTI_ISA ) <nl> int isa_type ; <nl> int isa_sex ; <nl> + EXPORT_SYMBOL ( isa_type ); <nl> + EXPORT_SYMBOL ( isa_sex ); <nl> # endif <nl>  <nl> extern int amiga_parse_bootinfo ( const struct bi_record *);
void speakup_remove_virtual_keyboard ( void ) <nl> { <nl> if ( virt_keyboard != NULL ) { <nl> input_unregister_device ( virt_keyboard ); <nl> - input_free_device ( virt_keyboard ); <nl> virt_keyboard = NULL ; <nl> } <nl> }
static int zcache_new_pool ( uint16_t cli_id , uint32_t flags ) <nl> if ( cli == NULL ) <nl> goto out ; <nl> atomic_inc (& cli -> refcount ); <nl> - pool = kmalloc ( sizeof ( struct tmem_pool ), GFP_KERNEL ); <nl> + pool = kmalloc ( sizeof ( struct tmem_pool ), GFP_ATOMIC ); <nl> if ( pool == NULL ) { <nl> pr_info (" zcache : pool creation failed : out of memory \ n "); <nl> goto out ;
static u32 __init allocate_aperture ( void ) <nl> printk (" Cannot allocate aperture memory hole (% p ,% uK )\ n ", <nl> p , aper_size >> 10 ); <nl> if ( p ) <nl> - free_bootmem_node ( nd0 , ( unsigned long ) p , aper_size ); <nl> + free_bootmem_node ( nd0 , __pa ( p ), aper_size ); <nl> return 0 ; <nl> } <nl> printk (" Mapping aperture over % d KB of RAM @ % lx \ n ",
static void __init ima_add_boot_aggregate ( void ) <nl> result = ima_calc_boot_aggregate (& hash . hdr ); <nl> if ( result < 0 ) { <nl> audit_cause = " hashing_error "; <nl> - kfree ( entry ); <nl> goto err_out ; <nl> } <nl> }
static int wm8731_check_osc ( struct snd_soc_dapm_widget * source , <nl> { <nl> struct wm8731_priv * wm8731 = snd_soc_codec_get_drvdata ( source -> codec ); <nl>  <nl> - return wm8731 -> sysclk_type == WM8731_SYSCLK_MCLK ; <nl> + return wm8731 -> sysclk_type == WM8731_SYSCLK_XTAL ; <nl> } <nl>  <nl> static const struct snd_soc_dapm_route wm8731_intercon [] = {
static int mt9t112_probe ( struct i2c_client * client , <nl> v4l2_i2c_subdev_init (& priv -> subdev , client , & mt9t112_subdev_ops ); <nl>  <nl> ret = mt9t112_camera_probe ( client ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> kfree ( priv ); <nl> + return ret ; <nl> + } <nl>  <nl> /* Cannot fail : using the default supported pixel code */ <nl> mt9t112_set_params ( priv , & rect , V4L2_MBUS_FMT_UYVY8_2X8 );
SYSCALL_DEFINE5 ( perf_event_open , <nl> } <nl> } <nl>  <nl> - if ( pid != - 1 ) <nl> + if ( pid != - 1 ) { <nl> task = find_lively_task_by_vpid ( pid ); <nl> + if ( IS_ERR ( task )) { <nl> + err = PTR_ERR ( task ); <nl> + goto err_group_fd ; <nl> + } <nl> + } <nl>  <nl> /* <nl> * Get the target context ( task or percpu ):
static int clone_backref_node ( struct btrfs_trans_handle * trans , <nl> new_node -> bytenr = dest -> node -> start ; <nl> new_node -> level = node -> level ; <nl> new_node -> lowest = node -> lowest ; <nl> + new_node -> checked = 1 ; <nl> new_node -> root = dest ; <nl>  <nl> if (! node -> lowest ) {
static int omap_pcm_open ( struct snd_pcm_substream * substream ) <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> - prtd = kzalloc ( sizeof ( prtd ), GFP_KERNEL ); <nl> + prtd = kzalloc ( sizeof (* prtd ), GFP_KERNEL ); <nl> if ( prtd == NULL ) { <nl> ret = - ENOMEM ; <nl> goto out ;
int datagram_send_ctl ( struct net * net , struct sock * sk , <nl>  <nl> if ( addr_type != IPV6_ADDR_ANY ) { <nl> int strict = __ipv6_addr_src_scope ( addr_type ) <= IPV6_ADDR_SCOPE_LINKLOCAL ; <nl> - if (! inet_sk ( sk )-> transparent && <nl> + if (!( inet_sk ( sk )-> freebind || inet_sk ( sk )-> transparent ) && <nl> ! ipv6_chk_addr ( net , & src_info -> ipi6_addr , <nl> strict ? dev : NULL , 0 )) <nl> err = - EINVAL ;
iscsi_tcp_state_change ( struct sock * sk ) <nl> conn = ( struct iscsi_conn *) sk -> sk_user_data ; <nl> session = conn -> session ; <nl>  <nl> - if ( sk -> sk_state == TCP_CLOSE_WAIT || <nl> - sk -> sk_state == TCP_CLOSE ) { <nl> + if (( sk -> sk_state == TCP_CLOSE_WAIT || <nl> + sk -> sk_state == TCP_CLOSE ) && <nl> + ! atomic_read (& sk -> sk_rmem_alloc )) { <nl> debug_tcp (" iscsi_tcp_state_change : TCP_CLOSE | TCP_CLOSE_WAIT \ n "); <nl> iscsi_conn_failure ( conn , ISCSI_ERR_CONN_FAILED ); <nl> }
static int omap_sham_finup ( struct ahash_request * req ) <nl> ctx -> flags |= FLAGS_FINUP ; <nl>  <nl> err1 = omap_sham_update ( req ); <nl> - if ( err1 == - EINPROGRESS ) <nl> + if ( err1 == - EINPROGRESS || err1 == - EBUSY ) <nl> return err1 ; <nl> /* <nl> * final () has to be always called to cleanup resources
void batadv_gw_node_update ( struct batadv_priv * bat_priv , <nl> * gets dereferenced . <nl> */ <nl> spin_lock_bh (& bat_priv -> gw . list_lock ); <nl> - hlist_del_init_rcu (& gw_node -> list ); <nl> + if (! hlist_unhashed (& gw_node -> list )) { <nl> + hlist_del_init_rcu (& gw_node -> list ); <nl> + batadv_gw_node_free_ref ( gw_node ); <nl> + } <nl> spin_unlock_bh (& bat_priv -> gw . list_lock ); <nl>  <nl> - batadv_gw_node_free_ref ( gw_node ); <nl> - <nl> curr_gw = batadv_gw_get_selected_gw_node ( bat_priv ); <nl> if ( gw_node == curr_gw ) <nl> batadv_gw_reselect ( bat_priv );
static int perf_session_deliver_event ( struct perf_session * session , <nl> dump_sample ( session , event , sample ); <nl> if ( evsel == NULL ) { <nl> ++ session -> hists . stats . nr_unknown_id ; <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl> if ( machine == NULL ) { <nl> ++ session -> hists . stats . nr_unprocessable_samples ; <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl> return tool -> sample ( tool , event , sample , evsel , machine ); <nl> case PERF_RECORD_MMAP :
static void scsi_sysfs_add_devices ( struct Scsi_Host * shost ) <nl> /* target removed before the device could be added */ <nl> if ( sdev -> sdev_state == SDEV_DEL ) <nl> continue ; <nl> + /* If device is already visible , skip adding it to sysfs */ <nl> + if ( sdev -> is_visible ) <nl> + continue ; <nl> if (! scsi_host_scan_allowed ( shost ) || <nl> scsi_sysfs_add_sdev ( sdev ) != 0 ) <nl> __scsi_remove_device ( sdev );
__rb_reserve_next ( struct ring_buffer_per_cpu * cpu_buffer , <nl> write &= RB_WRITE_MASK ; <nl> tail = write - length ; <nl>  <nl> + /* <nl> + * If this is the first commit on the page , then it has the same <nl> + * timestamp as the page itself . <nl> + */ <nl> + if (! tail ) <nl> + delta = 0 ; <nl> + <nl> /* See if we shot pass the end of this buffer page */ <nl> if ( unlikely ( write > BUF_PAGE_SIZE )) <nl> return rb_move_tail ( cpu_buffer , length , tail ,
alloc_new_skb : <nl> * because we have no idea what fragment will be <nl> * the last . <nl> */ <nl> - if ( datalen == length ) <nl> + if ( datalen == length + fraggap ) <nl> alloclen += rt -> u . dst . trailer_len ; <nl>  <nl> if ( transhdrlen ) {
static void paging_new_cr3 ( struct kvm_vcpu * vcpu ) <nl> { <nl> pgprintk ("% s : cr3 % lx \ n ", __FUNCTION__ , vcpu -> cr3 ); <nl> mmu_free_roots ( vcpu ); <nl> + if ( unlikely ( vcpu -> kvm -> n_free_mmu_pages < KVM_MIN_FREE_MMU_PAGES )) <nl> + kvm_mmu_free_some_pages ( vcpu ); <nl> mmu_alloc_roots ( vcpu ); <nl> kvm_mmu_flush_tlb ( vcpu ); <nl> kvm_arch_ops -> set_cr3 ( vcpu , vcpu -> mmu . root_hpa );
int drbd_al_begin_io_nonblock ( struct drbd_conf * mdev , struct drbd_interval * i ) <nl> if ( unlikely ( tmp != NULL )) { <nl> struct bm_extent * bm_ext = lc_entry ( tmp , struct bm_extent , lce ); <nl> if ( test_bit ( BME_NO_WRITES , & bm_ext -> flags )) { <nl> - if (! test_and_set_bit ( BME_PRIORITY , & bm_ext -> flags )); <nl> + if (! test_and_set_bit ( BME_PRIORITY , & bm_ext -> flags )) <nl> return - EBUSY ; <nl> return - EWOULDBLOCK ; <nl> }
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> tnl_hlen = skb_tnl_header_len ( skb ); <nl> - if ( skb_headroom ( skb ) < ( tnl_hlen + frag_hdr_sz )) { <nl> + if ( skb -> mac_header < ( tnl_hlen + frag_hdr_sz )) { <nl> if ( gso_pskb_expand_head ( skb , tnl_hlen + frag_hdr_sz )) <nl> goto out ; <nl> }
static int pm860x_led_dt_init ( struct platform_device * pdev , <nl> of_property_read_u32 ( np , " marvell , 88pm860x - iset ", <nl> & iset ); <nl> data -> iset = PM8606_LED_CURRENT ( iset ); <nl> + of_node_put ( np ); <nl> break ; <nl> } <nl> }
static int virtnet_set_channels ( struct net_device * dev , <nl> if ( channels -> rx_count || channels -> tx_count || channels -> other_count ) <nl> return - EINVAL ; <nl>  <nl> - if ( queue_pairs > vi -> max_queue_pairs ) <nl> + if ( queue_pairs > vi -> max_queue_pairs || queue_pairs == 0 ) <nl> return - EINVAL ; <nl>  <nl> get_online_cpus ();
const struct imx_imx_ssi_data imx35_imx_ssi_data [] __initconst = { <nl> # ifdef CONFIG_SOC_IMX51 <nl> const struct imx_imx_ssi_data imx51_imx_ssi_data [] __initconst = { <nl> # define imx51_imx_ssi_data_entry ( _id , _hwid ) \ <nl> - imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_4K ) <nl> + imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_16K ) <nl> imx51_imx_ssi_data_entry ( 0 , 1 ), <nl> imx51_imx_ssi_data_entry ( 1 , 2 ), <nl> imx51_imx_ssi_data_entry ( 2 , 3 ),
static const struct nla_policy ifla_vf_policy [ IFLA_VF_MAX + 1 ] = { <nl> . len = sizeof ( struct ifla_vf_vlan ) }, <nl> [ IFLA_VF_TX_RATE ] = { . type = NLA_BINARY , <nl> . len = sizeof ( struct ifla_vf_tx_rate ) }, <nl> + [ IFLA_VF_SPOOFCHK ] = { . type = NLA_BINARY , <nl> + . len = sizeof ( struct ifla_vf_spoofchk ) }, <nl> }; <nl>  <nl> static const struct nla_policy ifla_port_policy [ IFLA_PORT_MAX + 1 ] = {
int ipoib_vlan_delete ( struct net_device * pdev , unsigned short pkey ) <nl> if ( priv -> pkey == pkey ) { <nl> unregister_netdev ( priv -> dev ); <nl> ipoib_dev_cleanup ( priv -> dev ); <nl> - <nl> list_del (& priv -> list ); <nl> - <nl> - kfree ( priv ); <nl> + free_netdev ( priv -> dev ); <nl>  <nl> ret = 0 ; <nl> break ;
static int wanxl_pci_init_one ( struct pci_dev * pdev , <nl> if ( pci_set_consistent_dma_mask ( pdev , DMA_BIT_MASK ( 28 )) || <nl> pci_set_dma_mask ( pdev , DMA_BIT_MASK ( 28 ))) { <nl> pr_err (" No usable DMA configuration \ n "); <nl> + pci_disable_device ( pdev ); <nl> return - EIO ; <nl> } <nl> 
static void put_pages ( struct drm_gem_object * obj ) <nl>  <nl> if ( iommu_present (& platform_bus_type )) <nl> drm_gem_put_pages ( obj , msm_obj -> pages , true , false ); <nl> - else <nl> + else { <nl> drm_mm_remove_node ( msm_obj -> vram_node ); <nl> + drm_free_large ( msm_obj -> pages ); <nl> + } <nl>  <nl> msm_obj -> pages = NULL ; <nl> }
static bool linkwatch_urgent_event ( struct net_device * dev ) <nl> if ( dev -> ifindex != dev -> iflink ) <nl> return true ; <nl>  <nl> + if ( dev -> priv_flags & IFF_TEAM_PORT ) <nl> + return true ; <nl> + <nl> return netif_carrier_ok ( dev ) && qdisc_tx_changing ( dev ); <nl> } <nl> 
static void fc_rport_plogi_resp ( struct fc_seq * sp , struct fc_frame * fp , <nl>  <nl> tov = ntohl ( plp -> fl_csp . sp_e_d_tov ); <nl> if ( ntohs ( plp -> fl_csp . sp_features ) & FC_SP_FT_EDTR ) <nl> - tov /= 1000 ; <nl> + tov /= 1000000 ; <nl> if ( tov > rdata -> e_d_tov ) <nl> rdata -> e_d_tov = tov ; <nl> csp_seq = ntohs ( plp -> fl_csp . sp_tot_seq );
static int md_notify_reboot ( struct notifier_block * this , <nl> if ( mddev_trylock ( mddev )) { <nl> if ( mddev -> pers ) <nl> __md_stop_writes ( mddev ); <nl> - mddev -> safemode = 2 ; <nl> + if ( mddev -> persistent ) <nl> + mddev -> safemode = 2 ; <nl> mddev_unlock ( mddev ); <nl> } <nl> need_delay = 1 ;
UNUSUAL_DEV ( 0x090a , 0x1200 , 0x0000 , 0x9999 , <nl> USB_SC_RBC , USB_PR_BULK , NULL , <nl> 0 ), <nl>  <nl> +/* Feiya QDI U2 DISK , reported by Hans de Goede < hdegoede @ redhat . com > */ <nl> + UNUSUAL_DEV ( 0x090c , 0x1000 , 0x0000 , 0xffff , <nl> + " Feiya ", <nl> + " QDI U2 DISK ", <nl> + USB_SC_DEVICE , USB_PR_DEVICE , NULL , <nl> + US_FL_NO_READ_CAPACITY_16 ), <nl> + <nl> /* aeb */ <nl> UNUSUAL_DEV ( 0x090c , 0x1132 , 0x0000 , 0xffff , <nl> " Feiya ",
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> struct oz_app_hdr * app_hdr ; <nl> struct oz_serial_ctx * ctx ; <nl>  <nl> + if ( count > sizeof ( ei -> data ) - sizeof (* elt ) - sizeof (* app_hdr )) <nl> + return - EINVAL ; <nl> + <nl> spin_lock_bh (& g_cdev . lock ); <nl> pd = g_cdev . active_pd ; <nl> if ( pd )
int ath9k_hw_fill_cap_info ( struct ath_hw * ah ) <nl>  <nl> if ( AR_SREV_9485 ( ah ) || AR_SREV_9285 ( ah ) || AR_SREV_9330 ( ah )) <nl> chip_chainmask = 1 ; <nl> + else if ( AR_SREV_9462 ( ah )) <nl> + chip_chainmask = 3 ; <nl> else if (! AR_SREV_9280_20_OR_LATER ( ah )) <nl> chip_chainmask = 7 ; <nl> else if (! AR_SREV_9300_20_OR_LATER ( ah ) || AR_SREV_9340 ( ah ))
static void prepare_dma ( struct s3c64xx_spi_dma_data * dma , <nl> struct scatterlist sg ; <nl> struct dma_async_tx_descriptor * desc ; <nl>  <nl> + memset (& config , 0 , sizeof ( config )); <nl> + <nl> if ( dma -> direction == DMA_DEV_TO_MEM ) { <nl> sdd = container_of (( void *) dma , <nl> struct s3c64xx_spi_driver_data , rx_dma );
int btrfs_drop_snapshot ( struct btrfs_root * root , <nl> int level ; <nl>  <nl> path = btrfs_alloc_path (); <nl> - BUG_ON (! path ); <nl> + if (! path ) <nl> + return - ENOMEM ; <nl>  <nl> wc = kzalloc ( sizeof (* wc ), GFP_NOFS ); <nl> - BUG_ON (! wc ); <nl> + if (! wc ) { <nl> + btrfs_free_path ( path ); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> trans = btrfs_start_transaction ( tree_root , 0 ); <nl> BUG_ON ( IS_ERR ( trans ));
static void stmmac_clk_csr_set ( struct stmmac_priv * priv ) <nl> priv -> clk_csr = STMMAC_CSR_100_150M ; <nl> else if (( clk_rate >= CSR_F_150M ) && ( clk_rate < CSR_F_250M )) <nl> priv -> clk_csr = STMMAC_CSR_150_250M ; <nl> - else if (( clk_rate >= CSR_F_250M ) && ( clk_rate < CSR_F_300M )) <nl> + else if (( clk_rate >= CSR_F_250M ) && ( clk_rate <= CSR_F_300M )) <nl> priv -> clk_csr = STMMAC_CSR_250_300M ; <nl> } <nl> }
static int __cpuinit iucv_cpu_notify ( struct notifier_block * self , <nl> return NOTIFY_BAD ; <nl> iucv_param [ cpu ] = kmalloc_node ( sizeof ( union iucv_param ), <nl> GFP_KERNEL | GFP_DMA , cpu_to_node ( cpu )); <nl> - if (! iucv_param [ cpu ]) <nl> + if (! iucv_param [ cpu ]) { <nl> + kfree ( iucv_irq_data [ cpu ]); <nl> + iucv_irq_data [ cpu ] = NULL ; <nl> return NOTIFY_BAD ; <nl> + } <nl> break ; <nl> case CPU_UP_CANCELED : <nl> case CPU_UP_CANCELED_FROZEN :
int __scm_send ( struct socket * sock , struct msghdr * msg , struct scm_cookie * p ) <nl> goto error ; <nl>  <nl> cred -> uid = cred -> euid = p -> creds . uid ; <nl> - cred -> gid = cred -> egid = p -> creds . uid ; <nl> + cred -> gid = cred -> egid = p -> creds . gid ; <nl> put_cred ( p -> cred ); <nl> p -> cred = cred ; <nl> }
static void dmaengine_pcm_request_chan_of ( struct dmaengine_pcm * pcm , <nl> return ; <nl>  <nl> if ( pcm -> flags & SND_DMAENGINE_PCM_FLAG_HALF_DUPLEX ) { <nl> - pcm -> chan [ 0 ] = of_dma_request_slave_channel ( of_node , " tx_rx "); <nl> + pcm -> chan [ 0 ] = of_dma_request_slave_channel ( of_node , " rx - tx "); <nl> pcm -> chan [ 1 ] = pcm -> chan [ 0 ]; <nl> } else { <nl> for ( i = SNDRV_PCM_STREAM_PLAYBACK ; i <= SNDRV_PCM_STREAM_CAPTURE ; i ++) {
void iwl_rx_reply_rx ( struct iwl_priv * priv , <nl> iwl_dbg_report_frame ( priv , rx_start , len , header , 1 ); <nl> # endif <nl> IWL_DEBUG_STATS_LIMIT ( priv , " Rssi % d , noise % d , qual % d , TSF % llu \ n ", <nl> - rx_status . signal , rx_status . noise , rx_status . signal , <nl> + rx_status . signal , rx_status . noise , rx_status . qual , <nl> ( unsigned long long ) rx_status . mactime ); <nl>  <nl> /*
static void ovfx2_pkt_scan ( struct gspca_dev * gspca_dev , <nl> gspca_frame_add ( gspca_dev , INTER_PACKET , data , len ); <nl>  <nl> /* A short read signals EOF */ <nl> - if ( len < OVFX2_BULK_SIZE ) { <nl> + if ( len < gspca_dev -> cam . bulk_size ) { <nl> /* If the frame is short , and it is one of the first ones <nl> the sensor and bridge are still syncing , so drop it . */ <nl> if ( sd -> first_frame ) {
nv_printk_ ( struct nouveau_object * object , int level , const char * fmt , ...) <nl> char obuf [ 64 ], * ofmt = ""; <nl>  <nl> if ( object -> engine ) { <nl> - snprintf ( obuf , sizeof ( obuf ), "[ 0x % 08x ][% p ]", <nl> - nv_hclass ( object ), object ); <nl> + snprintf ( obuf , sizeof ( obuf ), "[ 0x % 08x ]", <nl> + nv_hclass ( object )); <nl> ofmt = obuf ; <nl> subdev = object -> engine ; <nl> device = object -> engine ;
static int __exit usba_udc_remove ( struct platform_device * pdev ) <nl> gpio_free ( udc -> vbus_pin ); <nl>  <nl> free_irq ( udc -> irq , udc ); <nl> + kfree ( usba_ep ); <nl> iounmap ( udc -> fifo ); <nl> iounmap ( udc -> regs ); <nl> clk_put ( udc -> hclk );
static inline bool __rpc_copy_addr6 ( struct sockaddr * dst , <nl>  <nl> dsin6 -> sin6_family = ssin6 -> sin6_family ; <nl> dsin6 -> sin6_addr = ssin6 -> sin6_addr ; <nl> + dsin6 -> sin6_scope_id = ssin6 -> sin6_scope_id ; <nl> return true ; <nl> } <nl> # else /* !( IS_ENABLED ( CONFIG_IPV6 ) */
static netdev_tx_t tg3_start_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> struct iphdr * iph ; <nl> u32 tcp_opt_len , hdr_len ; <nl>  <nl> - if ( skb_header_cloned ( skb ) && <nl> - pskb_expand_head ( skb , 0 , 0 , GFP_ATOMIC )) <nl> + if ( skb_cow_head ( skb , 0 )) <nl> goto drop ; <nl>  <nl> iph = ip_hdr ( skb );
static int process_counter ( struct perf_evsel * counter ) <nl> int i , ret ; <nl>  <nl> aggr -> val = aggr -> ena = aggr -> run = 0 ; <nl> - memset ( ps -> res_stats , 0 , sizeof ( ps -> res_stats )); <nl> + init_stats ( ps -> res_stats ); <nl>  <nl> if ( counter -> per_pkg ) <nl> zero_per_pkg ( counter );
out : <nl> full_bio -> bi_private = pe -> full_bio_private ; <nl> atomic_inc (& full_bio -> bi_remaining ); <nl> } <nl> - free_pending_exception ( pe ); <nl> - <nl> increment_pending_exceptions_done_count (); <nl>  <nl> up_write (& s -> lock ); <nl> out : <nl> } <nl>  <nl> retry_origin_bios ( s , origin_bios ); <nl> + <nl> + free_pending_exception ( pe ); <nl> } <nl>  <nl> static void commit_callback ( void * context , int success )
int snd_soc_dapm_stream_event ( struct snd_soc_codec * codec , <nl> { <nl> struct snd_soc_dapm_widget * w ; <nl>  <nl> + if ( stream == NULL ) <nl> + return 0 ; <nl> + <nl> mutex_lock (& codec -> mutex ); <nl> list_for_each_entry ( w , & codec -> dapm_widgets , list ) <nl> {
retry : <nl> * and pretend the write failed ... */ <nl> ext3_truncate_failed_direct_write ( inode ); <nl> ret = PTR_ERR ( handle ); <nl> + if ( inode -> i_nlink ) <nl> + ext3_orphan_del ( NULL , inode ); <nl> goto out ; <nl> } <nl> if ( inode -> i_nlink )
static int afiucv_hs_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> break ; <nl> case 0 : <nl> /* plain data frame */ <nl> + memcpy ( CB_TRGCLS ( skb ), & trans_hdr -> iucv_hdr . class , <nl> + CB_TRGCLS_LEN ); <nl> err = afiucv_hs_callback_rx ( sk , skb ); <nl> break ; <nl> default :
struct mmc_fixup { <nl> # define CID_OEMID_ANY (( unsigned short ) - 1 ) <nl> # define CID_NAME_ANY ( NULL ) <nl>  <nl> -# define END_FIXUP { 0 } <nl> +# define END_FIXUP { NULL } <nl>  <nl> # define _FIXUP_EXT ( _name , _manfid , _oemid , _rev_start , _rev_end , \ <nl> _cis_vendor , _cis_device , \
static int __fimc_md_create_flite_source_links ( struct fimc_md * fmd ) <nl> { <nl> struct media_entity * source , * sink ; <nl> unsigned int flags = MEDIA_LNK_FL_ENABLED ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> for ( i = 0 ; i < FIMC_LITE_MAX_DEVS ; i ++) { <nl> struct fimc_lite * fimc = fmd -> fimc_lite [ i ];
void __init paging_init ( void ) <nl> map_mem (); <nl> fixup_executable (); <nl>  <nl> - /* <nl> - * Finally flush the caches and tlb to ensure that we ' re in a <nl> - * consistent state . <nl> - */ <nl> - flush_cache_all (); <nl> - flush_tlb_all (); <nl> - <nl> /* allocate the zero page . */ <nl> zero_page = early_alloc ( PAGE_SIZE ); <nl> 
static __inline__ struct page * drm_do_vm_shm_nopage ( struct vm_area_struct * vma , <nl>  <nl> offset = address - vma -> vm_start ; <nl> i = ( unsigned long ) map -> handle + offset ; <nl> - page = vmalloc_to_page (( void *) i ); <nl> + page = ( map -> type == _DRM_CONSISTENT ) ? <nl> + virt_to_page (( void *) i ) : vmalloc_to_page (( void *) i ); <nl> if (! page ) <nl> return NOPAGE_OOM ; <nl> get_page ( page );
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static pci_ers_result_t ixgbe_io_error_detected ( struct pci_dev * pdev , <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + if ( state == pci_channel_io_perm_failure ) <nl> + return PCI_ERS_RESULT_DISCONNECT ; <nl> + <nl> if ( netif_running ( netdev )) <nl> ixgbe_down ( adapter ); <nl> pci_disable_device ( pdev );
static int bnx2x_issue_dmae_with_comp ( struct bnx2x * bp , <nl> struct dmae_command * dmae ) <nl> { <nl> u32 * wb_comp = bnx2x_sp ( bp , wb_comp ); <nl> - int cnt = CHIP_REV_IS_SLOW ( bp ) ? ( 400000 ) : 40 ; <nl> + int cnt = CHIP_REV_IS_SLOW ( bp ) ? ( 400000 ) : 4000 ; <nl> int rc = 0 ; <nl>  <nl> DP ( BNX2X_MSG_OFF , " data before [ 0x % 08x 0x % 08x 0x % 08x 0x % 08x ]\ n ",
void sb1250_time_init ( void ) <nl> /* Disable the timer and set up the count */ <nl> __raw_writeq ( 0 , IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_CFG ))); <nl> # ifdef CONFIG_SIMULATION <nl> - __raw_writeq ( 50000 / HZ , <nl> + __raw_writeq (( 50000 / HZ ) - 1 , <nl> IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_INIT ))); <nl> # else <nl> - __raw_writeq ( 1000000 / HZ , <nl> + __raw_writeq (( V_SCD_TIMER_FREQ / HZ ) - 1 , <nl> IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_INIT ))); <nl> # endif <nl> 
int usb_sg_init ( struct usb_sg_request * io , struct usb_device * dev , <nl> } <nl>  <nl> /* initialize all the urbs we ' ll use */ <nl> - io -> urbs = kmalloc ( io -> entries * sizeof * io -> urbs , mem_flags ); <nl> + io -> urbs = kmalloc ( io -> entries * sizeof (* io -> urbs ), mem_flags ); <nl> if (! io -> urbs ) <nl> goto nomem ; <nl> 
static int handle_invalid_guest_state ( struct kvm_vcpu * vcpu ) <nl> if ( intr_window_requested && vmx_interrupt_allowed ( vcpu )) <nl> return handle_interrupt_window (& vmx -> vcpu ); <nl>  <nl> + if ( test_bit ( KVM_REQ_EVENT , & vcpu -> requests )) <nl> + return 1 ; <nl> + <nl> err = emulate_instruction ( vcpu , 0 ); <nl>  <nl> if ( err == EMULATE_DO_MMIO ) {
static int at76_iw_handler_get_scan ( struct net_device * netdev , <nl> if (! iwe ) <nl> return - ENOMEM ; <nl>  <nl> - if ( priv -> scan_state != SCAN_COMPLETED ) <nl> + if ( priv -> scan_state != SCAN_COMPLETED ) { <nl> /* scan not yet finished */ <nl> + kfree ( iwe ); <nl> return - EAGAIN ; <nl> + } <nl>  <nl> spin_lock_irqsave (& priv -> bss_list_spinlock , flags ); <nl> 
static struct mtd_partition * newpart ( char * s , <nl> s ++; <nl> } else { <nl> size = memparse ( s , & s ); <nl> - if ( size < PAGE_SIZE ) { <nl> - printk ( KERN_ERR ERRP " partition size too small (% llx )\ n ", <nl> - size ); <nl> + if (! size ) { <nl> + printk ( KERN_ERR ERRP " partition has size 0 \ n "); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> }
int i915_gpu_idle ( struct drm_device * dev ) <nl> /* Is the device fubar ? */ <nl> if ( WARN_ON (! list_empty (& ring -> gpu_write_list ))) <nl> return - EBUSY ; <nl> + <nl> + ret = i915_switch_context ( ring , NULL , DEFAULT_CONTEXT_ID ); <nl> + if ( ret ) <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static int fuse_dentry_revalidate ( struct dentry * entry , struct nameidata * nd ) <nl> { <nl> struct inode * inode ; <nl>  <nl> - if ( nd -> flags & LOOKUP_RCU ) <nl> + if ( nd && nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl>  <nl> inode = entry -> d_inode ;
static int submit_extent_page ( int rw , struct extent_io_tree * tree , <nl> return 0 ; <nl> } <nl> } <nl> - nr = min_t ( int , max_pages , bio_get_nr_vecs ( bdev )); <nl> + nr = bio_get_nr_vecs ( bdev ); <nl> bio = extent_bio_alloc ( bdev , sector , nr , GFP_NOFS | __GFP_HIGH ); <nl> if (! bio ) { <nl> printk (" failed to allocate bio nr % d \ n ", nr );
static int ext4_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> EXT4_FEATURE_INCOMPAT_FILETYPE )) <nl> new_de -> file_type = old_de -> file_type ; <nl> new_dir -> i_version ++; <nl> + new_dir -> i_ctime = new_dir -> i_mtime = <nl> + ext4_current_time ( new_dir ); <nl> + ext4_mark_inode_dirty ( handle , new_dir ); <nl> BUFFER_TRACE ( new_bh , " call ext4_journal_dirty_metadata "); <nl> ext4_journal_dirty_metadata ( handle , new_bh ); <nl> brelse ( new_bh );
static int ep93xx_gpio_irq_type ( struct irq_data * d , unsigned int type ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - __irq_set_handler_locked ( d -> irq , handler ); <nl> + irq_set_handler_locked ( d , handler ); <nl>  <nl> gpio_int_enabled [ port ] |= port_mask ; <nl> 
out : <nl> return err ; <nl> no_route : <nl> kfree_skb ( nskb ); <nl> - IP_INC_STATS_BH ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl> + IP_INC_STATS ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl>  <nl> /* FIXME : Returning the ' err ' will effect all the associations <nl> * associated with a socket , although only one of the paths of the
static int img_spfi_start_dma ( struct spi_master * master , <nl> dma_async_issue_pending ( spfi -> rx_ch ); <nl> } <nl>  <nl> + spfi_start ( spfi ); <nl> + <nl> if ( xfer -> tx_buf ) { <nl> spfi -> tx_dma_busy = true ; <nl> dmaengine_submit ( txdesc ); <nl> dma_async_issue_pending ( spfi -> tx_ch ); <nl> } <nl>  <nl> - spfi_start ( spfi ); <nl> - <nl> return 1 ; <nl>  <nl> stop_dma :
extern long __put_user_asm_l ( void *, long ); <nl> extern long __put_user_asm_q ( void *, long ); <nl> extern void __put_user_unknown ( void ); <nl>  <nl> + extern long __strnlen_user ( const char * __s , long __n ); <nl> + <nl> # endif /* __ASM_SH_UACCESS_64_H */
static void notify_ring ( struct drm_device * dev , <nl> struct intel_ring_buffer * ring ) <nl> { <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> - u32 seqno = ring -> get_seqno ( ring ); <nl> + u32 seqno ; <nl> + <nl> + if ( ring -> obj == NULL ) <nl> + return ; <nl>  <nl> + seqno = ring -> get_seqno ( ring ); <nl> trace_i915_gem_request_complete ( dev , seqno ); <nl>  <nl> ring -> irq_seqno = seqno ;
int twl4030_madc_conversion ( struct twl4030_madc_request * req ) <nl> u8 ch_msb , ch_lsb ; <nl> int ret ; <nl>  <nl> - if (! req ) <nl> + if (! req || ! twl4030_madc ) <nl> return - EINVAL ; <nl> + <nl> mutex_lock (& twl4030_madc -> lock ); <nl> if ( req -> method < TWL4030_MADC_RT || req -> method > TWL4030_MADC_SW2 ) { <nl> ret = - EINVAL ;
static int soc_post_component_init ( struct snd_soc_card * card , <nl>  <nl> # ifdef CONFIG_DEBUG_FS <nl> /* add DPCM sysfs entries */ <nl> - if (! dai_link -> dynamic ) <nl> + if (! dailess && ! dai_link -> dynamic ) <nl> goto out ; <nl>  <nl> ret = soc_dpcm_debugfs_add ( rtd );
static int rocker_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl> goto err_probe_ports ; <nl> } <nl>  <nl> - dev_info (& pdev -> dev , " Rocker switch with id % 016llx \ n ", rocker -> hw . id ); <nl> + dev_info (& pdev -> dev , " Rocker switch with id %* phN \ n ", <nl> + ( int ) sizeof ( rocker -> hw . id ), & rocker -> hw . id ); <nl>  <nl> return 0 ; <nl> 
static struct perf_pmu * pmu_lookup ( const char * name ) <nl> LIST_HEAD ( aliases ); <nl> __u32 type ; <nl>  <nl> + /* No support for intel_bts or intel_pt so disallow them */ <nl> + if (! strcmp ( name , " intel_bts ") || ! strcmp ( name , " intel_pt ")) <nl> + return NULL ; <nl> + <nl> /* <nl> * The pmu data we store & need consists of the pmu <nl> * type value and format definitions . Load both right
struct e820entry ; <nl> char * __init machine_specific_memory_setup ( void ); <nl> char * memory_setup ( void ); <nl>  <nl> - int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> - int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> + int __init copy_e820_map ( struct e820entry * biosmap , int nr_map ); <nl> + int __init sanitize_e820_map ( struct e820entry * biosmap , char * pnr_map ); <nl> void __init add_memory_region ( unsigned long long start , <nl> unsigned long long size , int type ); <nl> 
static int remove_and_add_spares ( struct mddev * mddev , <nl> ! test_bit ( Bitmap_sync , & rdev -> flags ))) <nl> continue ; <nl>  <nl> - rdev -> recovery_offset = 0 ; <nl> + if ( rdev -> saved_raid_disk < 0 ) <nl> + rdev -> recovery_offset = 0 ; <nl> if ( mddev -> pers -> <nl> hot_add_disk ( mddev , rdev ) == 0 ) { <nl> if ( sysfs_link_rdev ( mddev , rdev ))
static int wacom_set_device_mode ( struct hid_device * hdev , int report_id , <nl> if ( error >= 0 ) <nl> error = wacom_get_report ( hdev , HID_FEATURE_REPORT , <nl> rep_data , length , 1 ); <nl> - } while (( error < 0 || rep_data [ 1 ] != mode ) && limit ++ < WAC_MSG_RETRIES ); <nl> + } while ( error >= 0 && rep_data [ 1 ] != mode && limit ++ < WAC_MSG_RETRIES ); <nl>  <nl> kfree ( rep_data ); <nl> 
SYSCALL_DEFINE3 ( sched_setattr , pid_t , pid , struct sched_attr __user *, uattr , <nl> if ( retval ) <nl> return retval ; <nl>  <nl> + if ( attr . sched_policy < 0 ) <nl> + return - EINVAL ; <nl> + <nl> rcu_read_lock (); <nl> retval = - ESRCH ; <nl> p = find_process_by_pid ( pid );
static int dlfb_realloc_framebuffer ( struct dlfb_data * dev , struct fb_info * info ) <nl> int new_len ; <nl> unsigned char * old_fb = info -> screen_base ; <nl> unsigned char * new_fb ; <nl> - unsigned char * new_back = 0 ; <nl> + unsigned char * new_back = NULL ; <nl>  <nl> pr_warn (" Reallocating framebuffer . Addresses will change !\ n "); <nl> 
static int annotate_browser__run ( struct annotate_browser * browser , <nl> nd = browser -> curr_hot ; <nl> break ; <nl> case K_UNTAB : <nl> - if ( nd != NULL ) <nl> + if ( nd != NULL ) { <nl> nd = rb_next ( nd ); <nl> if ( nd == NULL ) <nl> nd = rb_first (& browser -> entries ); <nl> - else <nl> + } else <nl> nd = browser -> curr_hot ; <nl> break ; <nl> case K_F1 :
static int f2fs_write_end ( struct file * file , <nl> if ( pos + copied > i_size_read ( inode )) { <nl> i_size_write ( inode , pos + copied ); <nl> mark_inode_dirty ( inode ); <nl> - update_inode_page ( inode ); <nl> } <nl>  <nl> f2fs_put_page ( page , 1 );
int hists__browse ( struct hists * self , const char * helpline , const char * ev_name ) <nl>  <nl> switch ( key ) { <nl> case ' a ': <nl> - if ( browser -> selection -> map == NULL && <nl> + if ( browser -> selection -> map == NULL || <nl> browser -> selection -> map -> dso -> annotate_warned ) <nl> continue ; <nl> goto do_annotate ;
static int snd_pcm_drain ( struct snd_pcm_substream * substream ) <nl>  <nl> snd_pcm_stream_lock_irq ( substream ); <nl> /* resume pause */ <nl> - if ( runtime -> status -> state == SNDRV_PCM_STATE_PAUSED ) <nl> + if ( substream -> runtime -> status -> state == SNDRV_PCM_STATE_PAUSED ) <nl> snd_pcm_pause ( substream , 0 ); <nl>  <nl> /* pre - start / stop - all running streams are changed to DRAINING state */
static void request_key_auth_destroy ( struct key * key ) <nl> kenter ("{% d }", key -> serial ); <nl>  <nl> key_put ( rka -> target_key ); <nl> + kfree ( rka ); <nl>  <nl> } /* end request_key_auth_destroy () */ <nl> 
# undef __get_str <nl>  <nl> # undef TP_printk <nl> -# define TP_printk ( fmt , args ...) "% s , % s \ n ", # fmt , __stringify ( args ) <nl> +# define TP_printk ( fmt , args ...) "\"% s \", % s \ n ", fmt , __stringify ( args ) <nl>  <nl> # undef TP_fast_assign <nl> # define TP_fast_assign ( args ...) args
hpet_ioctl_common ( struct hpet_dev * devp , int cmd , unsigned long arg , <nl> break ; <nl> case HPET_INFO : <nl> { <nl> + memset ( info , 0 , sizeof (* info )); <nl> if ( devp -> hd_ireqfreq ) <nl> info -> hi_ireqfreq = <nl> hpet_time_div ( hpetp , devp -> hd_ireqfreq ); <nl> - else <nl> - info -> hi_ireqfreq = 0 ; <nl> info -> hi_flags = <nl> readq (& timer -> hpet_config ) & Tn_PER_INT_CAP_MASK ; <nl> info -> hi_hpet = hpetp -> hp_which ;
int ext4_mb_find_by_goal ( struct ext4_allocation_context * ac , <nl> int max ; <nl> int err ; <nl> struct ext4_sb_info * sbi = EXT4_SB ( ac -> ac_sb ); <nl> + struct ext4_group_info * grp = ext4_get_group_info ( ac -> ac_sb , group ); <nl> struct ext4_free_extent ex ; <nl>  <nl> if (!( ac -> ac_flags & EXT4_MB_HINT_TRY_GOAL )) <nl> return 0 ; <nl> + if ( grp -> bb_free == 0 ) <nl> + return 0 ; <nl>  <nl> err = ext4_mb_load_buddy ( ac -> ac_sb , group , e4b ); <nl> if ( err )
static int adp5588_gpio_probe ( struct i2c_client * client , <nl> } <nl>  <nl> dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> - if ( dev == NULL ) { <nl> - dev_err (& client -> dev , " failed to alloc memory \ n "); <nl> + if ( dev == NULL ) <nl> return - ENOMEM ; <nl> - } <nl>  <nl> dev -> client = client ; <nl> 
static int drm_helper_probe_single_connector_modes_merge_bits ( struct drm_connect <nl> mode -> status = MODE_UNVERIFIED ; <nl>  <nl> if ( connector -> force ) { <nl> - if ( connector -> force == DRM_FORCE_ON ) <nl> + if ( connector -> force == DRM_FORCE_ON || <nl> + connector -> force == DRM_FORCE_ON_DIGITAL ) <nl> connector -> status = connector_status_connected ; <nl> else <nl> connector -> status = connector_status_disconnected ;
static long btrfs_ioctl_qgroup_assign ( struct file * file , void __user * arg ) <nl> sa -> src , sa -> dst ); <nl> } <nl>  <nl> + /* update qgroup status and info */ <nl> + err = btrfs_run_qgroups ( trans , root -> fs_info ); <nl> + if ( err < 0 ) <nl> + btrfs_error ( root -> fs_info , ret , <nl> + " failed to update qgroup status and info \ n "); <nl> err = btrfs_end_transaction ( trans , root ); <nl> if ( err && ! ret ) <nl> ret = err ;
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
int ath10k_wmi_event_mgmt_rx ( struct ath10k * ar , struct sk_buff * skb ) <nl> ret = ath10k_wmi_pull_mgmt_rx ( ar , skb , & arg ); <nl> if ( ret ) { <nl> ath10k_warn ( ar , " failed to parse mgmt rx event : % d \ n ", ret ); <nl> + dev_kfree_skb ( skb ); <nl> return ret ; <nl> } <nl> 
long drm_ioctl ( struct file * filp , <nl> goto err_i1 ; <nl> } <nl> } <nl> + if ( asize > usize ) <nl> + memset ( kdata + usize , 0 , asize - usize ); <nl> } <nl>  <nl> if ( cmd & IOC_IN ) {
next_button : <nl> if ( dev -> num_button_polling_addresses ) { <nl> memset ( dev -> button_polling_last_values , 0 , <nl> EM28XX_NUM_BUTTON_ADDRESSES_MAX ); <nl> - INIT_DELAYED_WORK (& dev -> buttons_query_work , <nl> - em28xx_query_buttons ); <nl> schedule_delayed_work (& dev -> buttons_query_work , <nl> msecs_to_jiffies ( dev -> button_polling_interval )); <nl> } <nl> static int em28xx_ir_init ( struct em28xx * dev ) <nl> } <nl>  <nl> kref_get (& dev -> ref ); <nl> + INIT_DELAYED_WORK (& dev -> buttons_query_work , em28xx_query_buttons ); <nl>  <nl> if ( dev -> board . buttons ) <nl> em28xx_init_buttons ( dev );
static int machines__deliver_event ( struct machines * machines , <nl>  <nl> switch ( event -> header . type ) { <nl> case PERF_RECORD_SAMPLE : <nl> - dump_sample ( evsel , event , sample ); <nl> if ( evsel == NULL ) { <nl> ++ evlist -> stats . nr_unknown_id ; <nl> return 0 ; <nl> } <nl> + dump_sample ( evsel , event , sample ); <nl> if ( machine == NULL ) { <nl> ++ evlist -> stats . nr_unprocessable_samples ; <nl> return 0 ;
static int t7l66xb_probe ( struct platform_device * dev ) <nl> t7l66xb_cells [ T7L66XB_CELL_NAND ]. data_size = <nl> sizeof ( t7l66xb_cells [ T7L66XB_CELL_NAND ]); <nl>  <nl> + t7l66xb_cells [ T7L66XB_CELL_MMC ]. platform_data = <nl> + & t7l66xb_cells [ T7L66XB_CELL_MMC ]; <nl> + t7l66xb_cells [ T7L66XB_CELL_MMC ]. data_size = <nl> + sizeof ( t7l66xb_cells [ T7L66XB_CELL_MMC ]); <nl> + <nl> ret = mfd_add_devices (& dev -> dev , dev -> id , <nl> t7l66xb_cells , ARRAY_SIZE ( t7l66xb_cells ), <nl> iomem , t7l66xb -> irq_base );
static int parse_raid_params ( struct raid_set * rs , char ** argv , <nl> rs -> ti -> error = " write_mostly option is only valid for RAID1 "; <nl> return - EINVAL ; <nl> } <nl> - if ( value > rs -> md . raid_disks ) { <nl> + if ( value >= rs -> md . raid_disks ) { <nl> rs -> ti -> error = " Invalid write_mostly drive index given "; <nl> return - EINVAL ; <nl> }
void __init at91_add_device_serial ( void ) <nl> printk ( KERN_INFO " AT91 : No default serial console defined .\ n "); <nl> } <nl> # else <nl> - void __init __deprecated at91_init_serial ( struct at91_uart_config * config ) {} <nl> void __init at91_register_uart ( unsigned id , unsigned portnr , unsigned pins ) {} <nl> void __init at91_set_serial_console ( unsigned portnr ) {} <nl> void __init at91_add_device_serial ( void ) {}
unsigned int qe_get_num_of_snums ( void ) <nl> if (( num_of_snums < 28 ) || ( num_of_snums > QE_NUM_OF_SNUM )) { <nl> /* No QE ever has fewer than 28 SNUMs */ <nl> pr_err (" QE : number of snum is invalid \ n "); <nl> + of_node_put ( qe ); <nl> return - EINVAL ; <nl> } <nl> }
static netdev_tx_t r6040_start_xmit ( struct sk_buff * skb , <nl> /* Set TX descriptor & Transmit it */ <nl> lp -> tx_free_desc --; <nl> descptr = lp -> tx_insert_ptr ; <nl> - if ( skb -> len < MISR ) <nl> - descptr -> len = MISR ; <nl> + if ( skb -> len < ETH_ZLEN ) <nl> + descptr -> len = ETH_ZLEN ; <nl> else <nl> descptr -> len = skb -> len ; <nl> 
static int __init fusb300_probe ( struct platform_device * pdev ) <nl>  <nl> fusb300 -> ep0_req = fusb300_alloc_request (& fusb300 -> ep [ 0 ]-> ep , <nl> GFP_KERNEL ); <nl> - if ( fusb300 -> ep0_req == NULL ) <nl> + if ( fusb300 -> ep0_req == NULL ) { <nl> + ret = - ENOMEM ; <nl> goto clean_up3 ; <nl> + } <nl>  <nl> init_controller ( fusb300 ); <nl> ret = usb_add_gadget_udc (& pdev -> dev , & fusb300 -> gadget );
static inline int get_ni_value ( int mclk , int rate ) <nl> if ( ni_div [ i ]. mclk >= mclk ) <nl> break ; <nl> } <nl> + if ( i == ARRAY_SIZE ( ni_div )) <nl> + return - EINVAL ; <nl>  <nl> switch ( rate ) { <nl> case 8000 :
static int setup_routing_entry ( struct kvm_irq_routing_table * rt , <nl> */ <nl> hlist_for_each_entry ( ei , n , & rt -> map [ ue -> gsi ], link ) <nl> if ( ei -> type == KVM_IRQ_ROUTING_MSI || <nl> + ue -> type == KVM_IRQ_ROUTING_MSI || <nl> ue -> u . irqchip . irqchip == ei -> irqchip . irqchip ) <nl> return r ; <nl> 
static int dma_set_runtime_config ( struct dma_chan * chan , <nl> u32 cctl = 0 ; <nl> int i ; <nl>  <nl> + if (! plchan -> slave ) <nl> + return - EINVAL ; <nl> + <nl> /* Transfer direction */ <nl> plchan -> runtime_direction = config -> direction ; <nl> if ( config -> direction == DMA_TO_DEVICE ) {
void btrfs_add_ordered_operation ( struct btrfs_trans_handle * trans , <nl> * if this file hasn ' t been changed since the last transaction <nl> * commit , we can safely return without doing anything <nl> */ <nl> - if ( last_mod < root -> fs_info -> last_trans_committed ) <nl> + if ( last_mod <= root -> fs_info -> last_trans_committed ) <nl> return ; <nl>  <nl> spin_lock (& root -> fs_info -> ordered_root_lock );
int __init init_dmars ( void ) <nl> deferred_flush = kzalloc ( g_num_of_iommus * <nl> sizeof ( struct deferred_flush_tables ), GFP_KERNEL ); <nl> if (! deferred_flush ) { <nl> - kfree ( g_iommus ); <nl> ret = - ENOMEM ; <nl> goto error ; <nl> }
static int pxa2xx_pcm_hw_free ( struct snd_pcm_substream * substream ) <nl> return 0 ; <nl> } <nl>  <nl> - struct snd_pcm_ops pxa2xx_pcm_ops = { <nl> + static struct snd_pcm_ops pxa2xx_pcm_ops = { <nl> . open = __pxa2xx_pcm_open , <nl> . close = __pxa2xx_pcm_close , <nl> . ioctl = snd_pcm_lib_ioctl ,
static int ata_dev_read_id ( struct ata_port * ap , struct ata_device * dev , <nl> err_out : <nl> printk ( KERN_WARNING " ata % u : dev % u failed to IDENTIFY (% s )\ n ", <nl> ap -> id , dev -> devno , reason ); <nl> - kfree ( id ); <nl> return rc ; <nl> } <nl> 
static int __devexit mxcnd_remove ( struct platform_device * pdev ) <nl> static struct platform_driver mxcnd_driver = { <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> + . owner = THIS_MODULE , <nl> }, <nl> . remove = __devexit_p ( mxcnd_remove ), <nl> };
static int arizona_dai_set_sysclk ( struct snd_soc_dai * dai , <nl> routes [ 1 ]. source = arizona_dai_clk_str ( clk_id ); <nl> snd_soc_dapm_add_routes (& codec -> dapm , routes , ARRAY_SIZE ( routes )); <nl>  <nl> + dai_priv -> clk = clk_id ; <nl> + <nl> return snd_soc_dapm_sync (& codec -> dapm ); <nl> } <nl> 
DECLARE_PCI_FIXUP_HEADER ( PCI_VENDOR_ID_ATI , PCI_DEVICE_ID_ATI_SBX00_SMBUS , <nl>  <nl> # if defined ( CONFIG_PCI ) && defined ( CONFIG_NUMA ) <nl> /* Set correct numa_node information for AMD NB functions */ <nl> - static void __init quirk_amd_nb_node ( struct pci_dev * dev ) <nl> + static void __devinit quirk_amd_nb_node ( struct pci_dev * dev ) <nl> { <nl> struct pci_dev * nb_ht ; <nl> unsigned int devfn ;
static void __init sanity_check_meminfo ( void ) <nl> bank -> size = VMALLOC_MIN - __va ( bank -> start ); <nl> } <nl> # else <nl> + bank -> highmem = highmem ; <nl> + <nl> /* <nl> * Check whether this memory bank would entirely overlap <nl> * the vmalloc area .
static inline u32 skb_mstamp_us_delta ( const struct skb_mstamp * t1 , <nl> return delta_us ; <nl> } <nl>  <nl> + static inline bool skb_mstamp_after ( const struct skb_mstamp * t1 , <nl> + const struct skb_mstamp * t0 ) <nl> +{ <nl> + s32 diff = t1 -> stamp_jiffies - t0 -> stamp_jiffies ; <nl> + <nl> + if (! diff ) <nl> + diff = t1 -> stamp_us - t0 -> stamp_us ; <nl> + return diff > 0 ; <nl> +} <nl>  <nl> /** <nl> * struct sk_buff - socket buffer
static inline pgtable_t pte_alloc_one ( struct mm_struct * mm , unsigned long addres <nl> { <nl> struct page * pte ; <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO , 0 ); <nl> + if (! pte ) <nl> + return NULL ; <nl> pgtable_page_ctor ( pte ); <nl> return pte ; <nl> }
static int mmc_blk_ioctl_cmd ( struct block_device * bdev , <nl> md = mmc_blk_get ( bdev -> bd_disk ); <nl> if (! md ) { <nl> err = - EINVAL ; <nl> - goto cmd_done ; <nl> + goto cmd_err ; <nl> } <nl>  <nl> card = md -> queue . card ; <nl> cmd_rel_host : <nl>  <nl> cmd_done : <nl> mmc_blk_put ( md ); <nl> + cmd_err : <nl> kfree ( idata -> buf ); <nl> kfree ( idata ); <nl> return err ;
static int __init d40_lcla_allocate ( struct d40_base * base ) <nl>  <nl> d40_err ( base -> dev , " Failed to allocate % d pages .\ n ", <nl> base -> lcla_pool . pages ); <nl> + ret = - ENOMEM ; <nl>  <nl> for ( j = 0 ; j < i ; j ++) <nl> free_pages ( page_list [ j ], base -> lcla_pool . pages );
static int gpmc_probe_dt ( struct platform_device * pdev ) <nl> of_node_cmp ( child -> name , " nor ") == 0 ) <nl> ret = gpmc_probe_generic_child ( pdev , child ); <nl>  <nl> - if ( ret < 0 ) { <nl> + if ( WARN ( ret < 0 , "% s : probing gpmc child % s failed \ n ", <nl> + __func__ , child -> full_name )) <nl> of_node_put ( child ); <nl> - return ret ; <nl> - } <nl> } <nl>  <nl> return 0 ;
static int wm8995_probe ( struct snd_soc_codec * codec ) <nl>  <nl> if ( ret != 0x8995 ) { <nl> dev_err ( codec -> dev , " Invalid device ID : %# x \ n ", ret ); <nl> + ret = - EINVAL ; <nl> goto err_reg_enable ; <nl> } <nl> 
static int hpb_dmae_chan_probe ( struct hpb_dmae_device * hpbdev , int id ) <nl> } <nl>  <nl> schan = & new_hpb_chan -> shdma_chan ; <nl> + schan -> max_xfer_len = HPB_DMA_TCR_MAX ; <nl> + <nl> shdma_chan_probe ( sdev , schan , id ); <nl>  <nl> if ( pdev -> id >= 0 )
ohci_enable_phys_dma ( struct fw_card * card , int node_id , int generation ) <nl> 1 << ( node_id - 32 )); <nl> } <nl> flush_writes ( ohci ); <nl> - <nl> - spin_unlock_irqrestore (& ohci -> lock , flags ); <nl> - <nl> out : <nl> + spin_unlock_irqrestore (& ohci -> lock , flags ); <nl> return retval ; <nl> } <nl> 
sctp_disposition_t sctp_sf_backbeat_8_3 ( const struct sctp_endpoint * ep , <nl> commands ); <nl>  <nl> hbinfo = ( sctp_sender_hb_info_t *) chunk -> skb -> data ; <nl> + /* Make sure that the length of the parameter is what we expect */ <nl> + if ( ntohs ( hbinfo -> param_hdr . length ) != <nl> + sizeof ( sctp_sender_hb_info_t )) { <nl> + return SCTP_DISPOSITION_DISCARD ; <nl> + } <nl> + <nl> from_addr = hbinfo -> daddr ; <nl> link = sctp_assoc_lookup_paddr ( asoc , & from_addr ); <nl> 
static int pipe_buffer_setting ( struct m66592 * m66592 , <nl> break ; <nl> case M66592_BULK : <nl> /* isochronous pipes may be used as bulk pipes */ <nl> - if ( info -> pipe > M66592_BASE_PIPENUM_BULK ) <nl> + if ( info -> pipe >= M66592_BASE_PIPENUM_BULK ) <nl> bufnum = info -> pipe - M66592_BASE_PIPENUM_BULK ; <nl> else <nl> bufnum = info -> pipe - M66592_BASE_PIPENUM_ISOC ;
int usb_serial_register_drivers ( struct usb_serial_driver * const serial_drivers [] <nl>  <nl> /* we only set the reset_resume field if the serial_driver has one */ <nl> for ( sd = serial_drivers ; * sd ; ++ sd ) { <nl> - if ((* sd )-> reset_resume ) <nl> + if ((* sd )-> reset_resume ) { <nl> udriver -> reset_resume = usb_serial_reset_resume ; <nl> break ; <nl> + } <nl> } <nl>  <nl> rc = usb_register ( udriver );
static int i40evf_request_misc_irq ( struct i40evf_adapter * adapter ) <nl> int err ; <nl>  <nl> snprintf ( adapter -> misc_vector_name , <nl> - sizeof ( adapter -> misc_vector_name ) - 1 , " i40evf : mbx "); <nl> + sizeof ( adapter -> misc_vector_name ) - 1 , " i40evf -% s : mbx ", <nl> + dev_name (& adapter -> pdev -> dev )); <nl> err = request_irq ( adapter -> msix_entries [ 0 ]. vector , <nl> & i40evf_msix_aq , 0 , <nl> adapter -> misc_vector_name , netdev );
static int nr_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> sax -> fsa_ax25 . sax25_family = AF_NETROM ; <nl> sax -> fsa_ax25 . sax25_ndigis = 1 ; <nl> sax -> fsa_ax25 . sax25_call = nr -> user_addr ; <nl> + memset ( sax -> fsa_digipeater , 0 , sizeof ( sax -> fsa_digipeater )); <nl> sax -> fsa_digipeater [ 0 ] = nr -> dest_addr ; <nl> * uaddr_len = sizeof ( struct full_sockaddr_ax25 ); <nl> } else {
struct ieee80211_tx_info { <nl> } control ; <nl> struct { <nl> struct ieee80211_tx_rate rates [ IEEE80211_TX_MAX_RATES ]; <nl> - int ack_signal ; <nl> + s32 ack_signal ; <nl> u8 ampdu_ack_len ; <nl> u8 ampdu_len ; <nl> u8 antenna ; <nl> - /* 21 bytes free */ <nl> + void * status_driver_data [ 21 / sizeof ( void *)]; <nl> } status ; <nl> struct { <nl> struct ieee80211_tx_rate driver_rates [
no_firmware : <nl> "% s : please contact support @ connecttech . com \ n ", <nl> serial -> type -> description ); <nl> kfree ( result ); <nl> + kfree ( command ); <nl> return - ENODEV ; <nl>  <nl> no_command_private :
int ath9k_cmn_rx_skb_preprocess ( struct ath_common * common , <nl> { <nl> struct ath_hw * ah = common -> ah ; <nl>  <nl> + memset ( rx_status , 0 , sizeof ( struct ieee80211_rx_status )); <nl> if (! ath9k_rx_accept ( common , skb , rx_status , rx_stats , decrypt_error )) <nl> return - EINVAL ; <nl> 
match1 : <nl> ndoms_cur = 0 ; <nl> doms_new = & fallback_doms ; <nl> cpus_andnot ( doms_new [ 0 ], cpu_online_map , cpu_isolated_map ); <nl> - dattr_new = NULL ; <nl> + WARN_ON_ONCE ( dattr_new ); <nl> } <nl>  <nl> /* Build new domains */
static int wl1271_plt_init ( struct wl1271 * wl ) <nl> if ( ret < 0 ) <nl> goto out_free_memmap ; <nl>  <nl> + ret = wl1271_acx_sta_mem_cfg ( wl ); <nl> + if ( ret < 0 ) <nl> + goto out_free_memmap ; <nl> + <nl> /* Default fragmentation threshold */ <nl> ret = wl1271_acx_frag_threshold ( wl , wl -> conf . tx . frag_threshold ); <nl> if ( ret < 0 )
static int ax_probe ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_IRQ , 0 ); <nl> if ( res == NULL ) { <nl> dev_err (& pdev -> dev , " no IRQ specified \ n "); <nl> + ret = - ENXIO ; <nl> goto exit_mem ; <nl> } <nl> 
static int ext4_mb_init_per_dev_proc ( struct super_block * sb ) <nl> struct proc_dir_entry * proc ; <nl> char devname [ 64 ]; <nl>  <nl> + if ( proc_root_ext4 == NULL ) { <nl> + sbi -> s_mb_proc = NULL ; <nl> + return - EINVAL ; <nl> + } <nl> bdevname ( sb -> s_bdev , devname ); <nl> sbi -> s_mb_proc = proc_mkdir ( devname , proc_root_ext4 ); <nl> 
iwl_op_mode_mvm_start ( struct iwl_trans * trans , const struct iwl_cfg * cfg , <nl> } <nl> mvm -> sf_state = SF_UNINIT ; <nl> mvm -> low_latency_agg_frame_limit = 6 ; <nl> + mvm -> cur_ucode = IWL_UCODE_INIT ; <nl>  <nl> mutex_init (& mvm -> mutex ); <nl> mutex_init (& mvm -> d0i3_suspend_mutex );
static const u32 cipher_suites [] = { <nl> }; <nl>  <nl> static const struct ieee80211_txrx_stypes <nl> - wilc_wfi_cfg80211_mgmt_types [ NL80211_IFTYPE_MAX ] = { <nl> + wilc_wfi_cfg80211_mgmt_types [ NUM_NL80211_IFTYPES ] = { <nl> [ NL80211_IFTYPE_STATION ] = { <nl> . tx = 0xffff , <nl> . rx = BIT ( IEEE80211_STYPE_ACTION >> 4 ) |
static int ioctl_send_response ( struct client * client , void * buffer ) <nl> if ( copy_from_user ( r -> data , u64_to_uptr ( request -> data ), <nl> r -> length )) { <nl> ret = - EFAULT ; <nl> + kfree ( r -> request ); <nl> goto out ; <nl> } <nl> fw_send_response ( client -> device -> card , r -> request ,
static int lpc18xx_pconf_set_i2c0 ( struct pinctrl_dev * pctldev , <nl> break ; <nl>  <nl> case PIN_CONFIG_INPUT_SCHMITT_ENABLE : <nl> - if ( param ) <nl> + if ( param_val ) <nl> * reg &= ~( LPC18XX_SCU_I2C0_ZIF << shift ); <nl> else <nl> * reg |= ( LPC18XX_SCU_I2C0_ZIF << shift ); <nl> static int lpc18xx_pconf_set_pin ( struct pinctrl_dev * pctldev , <nl> break ; <nl>  <nl> case PIN_CONFIG_INPUT_SCHMITT_ENABLE : <nl> - if ( param ) <nl> + if ( param_val ) <nl> * reg &= ~ LPC18XX_SCU_PIN_ZIF ; <nl> else <nl> * reg |= LPC18XX_SCU_PIN_ZIF ;
static int sysfs_add_link ( struct dentry * parent , const char * name , struct kobj <nl> if (! error ) <nl> return 0 ; <nl>  <nl> + kobject_put ( target ); <nl> kfree ( sl -> link_name ); <nl> exit2 : <nl> kfree ( sl );
int drm_vblank_get ( struct drm_device * dev , int crtc ) <nl> unsigned long irqflags ; <nl> int ret = 0 ; <nl>  <nl> + if (! dev -> num_crtcs ) <nl> + return - EINVAL ; <nl> + <nl> if ( WARN_ON ( crtc >= dev -> num_crtcs )) <nl> return - EINVAL ; <nl> 
static struct config_item_type printer_func_type = { <nl>  <nl> static inline int gprinter_get_minor ( void ) <nl> { <nl> - return ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + int ret ; <nl> + <nl> + ret = ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + if ( ret >= PRINTER_MINORS ) { <nl> + ida_simple_remove (& printer_ida , ret ); <nl> + ret = - ENODEV ; <nl> + } <nl> + <nl> + return ret ; <nl> } <nl>  <nl> static inline void gprinter_put_minor ( int minor )
static int ath9k_start ( struct ieee80211_hw * hw ) <nl> DPRINTF ( sc , ATH_DBG_CONFIG , "% s : Starting driver with " <nl> " initial channel : % d MHz \ n ", __func__ , curchan -> center_freq ); <nl>  <nl> + memset (& sc -> sc_ht_info , 0 , sizeof ( struct ath_ht_info )); <nl> + <nl> /* setup initial channel */ <nl>  <nl> pos = ath_get_channel ( sc , curchan );
static int resizer_configure_output_win ( struct vpfe_resizer_device * resizer ) <nl>  <nl> outformat = & resizer -> resizer_a . formats [ RESIZER_PAD_SOURCE ]; <nl>  <nl> + memset (& output_specs , 0x0 , sizeof ( struct vpfe_rsz_output_spec )); <nl> output_specs . vst_y = param -> user_config . vst ; <nl> if ( outformat -> code == MEDIA_BUS_FMT_YDYUYDYV8_1X16 ) <nl> output_specs . vst_c = param -> user_config . vst ;
static int __devinit rtsx_probe ( struct pci_dev * pci , <nl> th = kthread_create ( rtsx_scan_thread , dev , " rtsx - scan "); <nl> if ( IS_ERR ( th )) { <nl> printk ( KERN_ERR " Unable to start the device - scanning thread \ n "); <nl> + complete (& dev -> scanning_done ); <nl> quiesce_and_remove_host ( dev ); <nl> err = PTR_ERR ( th ); <nl> goto errout ;
static struct of_device_id octeon_cf_match [] = { <nl> }, <nl> {}, <nl> }; <nl> - MODULE_DEVICE_TABLE ( of , octeon_i2c_match ); <nl> + MODULE_DEVICE_TABLE ( of , octeon_cf_match ); <nl>  <nl> static struct platform_driver octeon_cf_driver = { <nl> . probe = octeon_cf_probe ,
static __init int samsung_gpiolib_init ( void ) <nl> s5p_register_gpioint_bank ( IRQ_GPIO_XA , 0 , IRQ_GPIO1_NR_GROUPS ); <nl> s5p_register_gpioint_bank ( IRQ_GPIO_XB , IRQ_GPIO1_NR_GROUPS , IRQ_GPIO2_NR_GROUPS ); <nl> # endif <nl> + } else { <nl> + WARN ( 1 , " Unknown SoC in gpio - samsung , no GPIOs added \ n "); <nl> + return - ENODEV ; <nl> } <nl>  <nl> return 0 ;
restart : <nl> goto out_free ; <nl> } <nl>  <nl> + if ( sk_filter ( other , skb ) < 0 ) { <nl> + /* Toss the packet but do not return any error to the sender */ <nl> + err = len ; <nl> + goto out_free ; <nl> + } <nl> + <nl> unix_state_lock ( other ); <nl> err = - EPERM ; <nl> if (! unix_may_send ( sk , other ))
again : <nl> key . offset = found_key . offset - 1 ; <nl> wc . replay_dest -> log_root = NULL ; <nl> free_extent_buffer ( log -> node ); <nl> + free_extent_buffer ( log -> commit_root ); <nl> kfree ( log ); <nl>  <nl> if ( found_key . offset == 0 )
static int pvscsi_queue_ring ( struct pvscsi_adapter * adapter , <nl> memcpy ( e -> cdb , cmd -> cmnd , e -> cdbLen ); <nl>  <nl> e -> tag = SIMPLE_QUEUE_TAG ; <nl> - if ( sdev -> tagged_supported && <nl> - ( cmd -> tag == HEAD_OF_QUEUE_TAG || <nl> - cmd -> tag == ORDERED_QUEUE_TAG )) <nl> - e -> tag = cmd -> tag ; <nl>  <nl> if ( cmd -> sc_data_direction == DMA_FROM_DEVICE ) <nl> e -> flags = PVSCSI_FLAG_CMD_DIR_TOHOST ;
done : <nl>  <nl> out : <nl> if ( unlikely ( frozen_buffer )) /* It ' s usually NULL */ <nl> - kfree ( frozen_buffer ); <nl> + jbd_slab_free ( frozen_buffer , bh -> b_size ); <nl>  <nl> JBUFFER_TRACE ( jh , " exit "); <nl> return error ;
int amdgpu_gem_userptr_ioctl ( struct drm_device * dev , void * data , <nl> AMDGPU_GEM_USERPTR_REGISTER )) <nl> return - EINVAL ; <nl>  <nl> - if (!( args -> flags & AMDGPU_GEM_USERPTR_ANONONLY ) || <nl> - !( args -> flags & AMDGPU_GEM_USERPTR_REGISTER )) { <nl> + if (!( args -> flags & AMDGPU_GEM_USERPTR_READONLY ) && ( <nl> + !( args -> flags & AMDGPU_GEM_USERPTR_ANONONLY ) || <nl> + !( args -> flags & AMDGPU_GEM_USERPTR_REGISTER ))) { <nl>  <nl> /* if we want to write to it we must require anonymous <nl> memory and install a MMU notifier */
pca963x_dt_init ( struct i2c_client * client , struct pca963x_chipdef * chip ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> for_each_child_of_node ( np , child ) { <nl> - struct led_info led ; <nl> + struct led_info led = {}; <nl> u32 reg ; <nl> int res ; <nl> 
static int f2fs_xattr_advise_set ( struct dentry * dentry , const char * name , <nl> return - EINVAL ; <nl>  <nl> F2FS_I ( inode )-> i_advise |= *( char *) value ; <nl> + mark_inode_dirty ( inode ); <nl> return 0 ; <nl> } <nl> 
asmlinkage long compat_sys_ppoll ( struct pollfd __user * ufds , <nl> } <nl>  <nl> if ( sigmask ) { <nl> - if ( sigsetsize |= sizeof ( compat_sigset_t )) <nl> + if ( sigsetsize != sizeof ( compat_sigset_t )) <nl> return - EINVAL ; <nl> if ( copy_from_user (& ss32 , sigmask , sizeof ( ss32 ))) <nl> return - EFAULT ;
int __init musb_platform_init ( struct musb * musb , void * board_data ) <nl>  <nl> usb_nop_xceiv_register (); <nl> musb -> xceiv = otg_get_transceiver (); <nl> - if (! musb -> xceiv ) <nl> + if (! musb -> xceiv ) { <nl> + gpio_free ( musb -> config -> gpio_vrsel ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> if ( ANOMALY_05000346 ) { <nl> bfin_write_USB_APHY_CALIB ( ANOMALY_05000346_value );
again : <nl> */ <nl> if ( type != TRANS_JOIN_NOLOCK && <nl> ! __sb_start_write ( root -> fs_info -> sb , SB_FREEZE_FS , false )) { <nl> - if ( type == TRANS_JOIN_FREEZE ) <nl> + if ( type == TRANS_JOIN_FREEZE ) { <nl> + kmem_cache_free ( btrfs_trans_handle_cachep , h ); <nl> return ERR_PTR (- EPERM ); <nl> + } <nl> sb_start_intwrite ( root -> fs_info -> sb ); <nl> } <nl> 
static void ext4_mb_group_or_file ( struct ext4_allocation_context * ac ) <nl> return ; <nl> } <nl>  <nl> + if ( sbi -> s_mb_group_prealloc <= 0 ) { <nl> + ac -> ac_flags |= EXT4_MB_STREAM_ALLOC ; <nl> + return ; <nl> + } <nl> + <nl> /* don ' t use group allocation for large files */ <nl> size = max ( size , isize ); <nl> if ( size > sbi -> s_mb_stream_request ) {
acpi_ns_lookup ( union acpi_generic_state * scope_info , <nl> * segments ). <nl> */ <nl> if ( this_node -> type == ACPI_TYPE_LOCAL_ALIAS ) { <nl> + if (! this_node -> object ) { <nl> + return_ACPI_STATUS ( AE_NOT_EXIST ); <nl> + } <nl> + <nl> if ( acpi_ns_opens_scope <nl> ((( struct acpi_namespace_node *) this_node -> <nl> object )-> type )) {
static ssize_t ac_read ( struct file * filp , char __user * buf , size_t count , loff_ <nl> struct mailbox mailbox ; <nl>  <nl> /* Got a packet for us */ <nl> + memset (& st_loc , 0 , sizeof ( st_loc )); <nl> ret = do_ac_read ( i , buf , & st_loc , & mailbox ); <nl> spin_unlock_irqrestore (& apbs [ i ]. mutex , flags ); <nl> set_current_state ( TASK_RUNNING );
static int rcar_vin_videobuf_setup ( struct vb2_queue * vq , <nl> unsigned int bytes_per_line ; <nl> int ret ; <nl>  <nl> + if ( fmt -> fmt . pix . sizeimage < icd -> sizeimage ) <nl> + return - EINVAL ; <nl> + <nl> xlate = soc_camera_xlate_by_fourcc ( icd , <nl> fmt -> fmt . pix . pixelformat ); <nl> if (! xlate )
out : <nl> if ( mem_tight != 0 ) <nl> cfs_memory_pressure_restore ( mpflag ); <nl>  <nl> - if ( crattr != NULL ) { <nl> - kfree ( crattr ); <nl> - } <nl> + kfree ( crattr ); <nl>  <nl> if ( rc != 0 ) { <nl> LASSERT ( req == NULL );
static int nokia_modem_probe ( struct device * dev ) <nl> return - ENOMEM ; <nl> } <nl> dev_set_drvdata ( dev , modem ); <nl> + modem -> device = dev ; <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> if (! irq ) {
nvc0_fifo_init ( struct nouveau_object * object ) <nl> nv_wr32 ( priv , 0x002a00 , 0xffffffff ); /* clears PFIFO . INTR bit 30 */ <nl> nv_wr32 ( priv , 0x002100 , 0xffffffff ); <nl> nv_wr32 ( priv , 0x002140 , 0x3fffffff ); <nl> + nv_wr32 ( priv , 0x002628 , 0x00000001 ); /* makes mthd 0x20 work */ <nl> return 0 ; <nl> } <nl> 
static int ext4_fill_super ( struct super_block * sb , void * data , int silent ) <nl> * of the filesystem . <nl> */ <nl> if ( le32_to_cpu ( es -> s_first_data_block ) >= ext4_blocks_count ( es )) { <nl> - ext4_msg ( sb , KERN_WARNING , " bad geometry : first data " <nl> + ext4_msg ( sb , KERN_WARNING , " bad geometry : first data " <nl> " block % u is beyond end of filesystem (% llu )", <nl> le32_to_cpu ( es -> s_first_data_block ), <nl> ext4_blocks_count ( es ));
int add_extent_mapping ( struct extent_map_tree * tree , <nl> rb = tree_insert (& tree -> map , em -> start , & em -> rb_node ); <nl> if ( rb ) { <nl> ret = - EEXIST ; <nl> - free_extent_map ( merge ); <nl> goto out ; <nl> } <nl> atomic_inc (& em -> refs );
static inline struct ata_link * ata_port_next_link ( struct ata_link * link ) <nl> return ap -> pmp_link ; <nl> } <nl>  <nl> - if (++ link - ap -> pmp_link < ap -> nr_pmp_links ) <nl> + if (++ link < ap -> nr_pmp_links + ap -> pmp_link ) <nl> return link ; <nl> return NULL ; <nl> }
static const struct snd_pci_quirk alc269_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x103c , 0x228a , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228b , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228c , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> + SND_PCI_QUIRK ( 0x103c , 0x228d , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22c5 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22c6 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ),
void local_touch_nmi ( void ) <nl> { <nl> __this_cpu_write ( last_nmi_rip , 0 ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( local_touch_nmi );
static int sst_platform_pcm_trigger ( struct snd_pcm_substream * substream , <nl> struct snd_soc_pcm_runtime * rtd = substream -> private_data ; <nl>  <nl> dev_dbg ( rtd -> dev , " sst_platform_pcm_trigger called \ n "); <nl> + if ( substream -> pcm -> internal ) <nl> + return 0 ; <nl> stream = substream -> runtime -> private_data ; <nl> str_id = stream -> stream_info . str_id ; <nl> switch ( cmd ) {
int snd_hda_create_dig_out_ctls ( struct hda_codec * codec , <nl> return - EBUSY ; <nl> } <nl> spdif = snd_array_new (& codec -> spdif_out ); <nl> + if (! spdif ) <nl> + return - ENOMEM ; <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> if (! kctl )
int wl1271_init_ieee80211 ( struct wl1271 * wl ) <nl> wl -> hw -> wiphy -> max_scan_ie_len = WL1271_CMD_TEMPL_DFLT_SIZE - <nl> sizeof ( struct ieee80211_header ); <nl>  <nl> + wl -> hw -> wiphy -> max_sched_scan_ie_len = WL1271_CMD_TEMPL_DFLT_SIZE - <nl> + sizeof ( struct ieee80211_header ); <nl> + <nl> wl -> hw -> wiphy -> flags |= WIPHY_FLAG_AP_UAPSD ; <nl>  <nl> /* make sure all our channels fit in the scanned_ch bitmask */
static inline void * __alloc_percpu ( size_t size , size_t align ) <nl> * percpu sections on SMP for which this path isn ' t used . <nl> */ <nl> WARN_ON_ONCE ( align > __alignof__ ( unsigned long long )); <nl> - return kzalloc ( size , gfp ); <nl> + return kzalloc ( size , GFP_KERNEL ); <nl> } <nl>  <nl> static inline void free_percpu ( void * p )
int kvm_vgic_create ( struct kvm * kvm , u32 type ) <nl> * emulation . So check this here again . KVM_CREATE_DEVICE does <nl> * the proper checks already . <nl> */ <nl> - if ( type == KVM_DEV_TYPE_ARM_VGIC_V2 && ! vgic -> can_emulate_gicv2 ) <nl> - return - ENODEV ; <nl> + if ( type == KVM_DEV_TYPE_ARM_VGIC_V2 && ! vgic -> can_emulate_gicv2 ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl>  <nl> /* <nl> * Any time a vcpu is run , vcpu_load is called which tries to grab the
unsigned int solo_eeprom_ewen ( struct solo_dev * solo_dev , int w_en ) <nl> __be16 solo_eeprom_read ( struct solo_dev * solo_dev , int loc ) <nl> { <nl> int read_cmd = loc | ( EE_READ_CMD << ADDR_LEN ); <nl> - unsigned short retval = 0 ; <nl> + u16 retval = 0 ; <nl> int i ; <nl>  <nl> solo_eeprom_cmd ( solo_dev , read_cmd );
static int k3_dma_probe ( struct platform_device * op ) <nl> d -> slave . device_issue_pending = k3_dma_issue_pending ; <nl> d -> slave . device_control = k3_dma_control ; <nl> d -> slave . copy_align = DMA_ALIGN ; <nl> - d -> slave . chancnt = d -> dma_requests ; <nl>  <nl> /* init virtual channel */ <nl> d -> chans = devm_kzalloc (& op -> dev ,
static int snd_asihpi_cmode_info ( struct snd_kcontrol * kcontrol , <nl> valid_modes ++; <nl> } <nl>  <nl> + if (! valid_modes ) <nl> + return - EINVAL ; <nl> + <nl> uinfo -> type = SNDRV_CTL_ELEM_TYPE_ENUMERATED ; <nl> uinfo -> count = 1 ; <nl> uinfo -> value . enumerated . items = valid_modes ;
int __init dc21285_setup ( int nr , struct pci_sys_data * sys ) <nl>  <nl> sys -> mem_offset = DC21285_PCI_MEM ; <nl>  <nl> - pci_ioremap_io ( 0 , DC21285_PCI_IO ); <nl> - <nl> pci_add_resource_offset (& sys -> resources , & res [ 0 ], sys -> mem_offset ); <nl> pci_add_resource_offset (& sys -> resources , & res [ 1 ], sys -> mem_offset ); <nl> 
static int mwifiex_cfg80211_start_ap ( struct wiphy * wiphy , <nl> case NL80211_HIDDEN_SSID_ZERO_CONTENTS : <nl> /* firmware doesn ' t support this type of hidden SSID */ <nl> default : <nl> + kfree ( bss_cfg ); <nl> return - EINVAL ; <nl> } <nl> 
struct simple_xattr * simple_xattr_alloc ( const void * value , size_t size ) <nl>  <nl> /* wrap around ? */ <nl> len = sizeof (* new_xattr ) + size ; <nl> - if ( len <= sizeof (* new_xattr )) <nl> + if ( len < sizeof (* new_xattr )) <nl> return NULL ; <nl>  <nl> new_xattr = kmalloc ( len , GFP_KERNEL );
static void __devinit pci_fixed_bar_fixup ( struct pci_dev * dev ) <nl> u32 size ; <nl> int i ; <nl>  <nl> + /* Must have extended configuration space */ <nl> + if ( dev -> cfg_size < PCIE_CAP_OFFSET + 4 ) <nl> + return ; <nl> + <nl> /* Fixup the BAR sizes for fixed BAR devices and make them unmoveable */ <nl> offset = fixed_bar_cap ( dev -> bus , dev -> devfn ); <nl> if (! offset || PCI_DEVFN ( 2 , 0 ) == dev -> devfn ||
int rt2x00mac_tx ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> */ <nl> if (! test_bit ( DEVICE_PRESENT , & rt2x00dev -> flags )) { <nl> ieee80211_stop_queues ( hw ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
static int sprom_extract ( struct ssb_bus * bus , struct ssb_sprom * out , <nl> ssb_printk ( KERN_WARNING PFX " Unsupported SPROM " <nl> " revision % d detected . Will extract " <nl> " v1 \ n ", out -> revision ); <nl> + out -> revision = 1 ; <nl> sprom_extract_r123 ( out , in ); <nl> } <nl> }
static int bnx2x_get_hwinfo ( struct bnx2x * bp ) <nl> } else <nl> BNX2X_DEV_INFO (" illegal OV for SD \ n "); <nl> break ; <nl> + case SHARED_FEAT_CFG_FORCE_SF_MODE_FORCED_SF : <nl> + bp -> mf_config [ vn ] = 0 ; <nl> + break ; <nl> default : <nl> /* Unknown configuration : reset mf_config */ <nl> bp -> mf_config [ vn ] = 0 ;
static int drm_crtc_convert_umode ( struct drm_display_mode * out , <nl> if ( in -> clock > INT_MAX || in -> vrefresh > INT_MAX ) <nl> return - ERANGE ; <nl>  <nl> - /* At most , 1 set bit describing the 3D layout of the mode */ <nl> - if ( hweight32 ( in -> flags & DRM_MODE_FLAG_3D_MASK ) > 1 ) <nl> - return - EINVAL ; <nl> - <nl> out -> clock = in -> clock ; <nl> out -> hdisplay = in -> hdisplay ; <nl> out -> hsync_start = in -> hsync_start ;
static long pmcraid_ioctl_passthrough ( <nl> rc = - EFAULT ; <nl> goto out_free_buffer ; <nl> } <nl> + } else if ( request_size < 0 ) { <nl> + rc = - EINVAL ; <nl> + goto out_free_buffer ; <nl> } <nl>  <nl> /* check if we have any additional command parameters */
static int clk_div16_get_divider ( unsigned long parent_rate , unsigned long rate ) <nl> if ( divider_u16 - 1 < 0 ) <nl> return 0 ; <nl>  <nl> - if ( divider_u16 - 1 > 255 ) <nl> + if ( divider_u16 - 1 > 0xFFFF ) <nl> return - EINVAL ; <nl>  <nl> return divider_u16 - 1 ;
static int bond_xmit_roundrobin ( struct sk_buff * skb , struct net_device * bond_dev <nl> * send the join / membership reports . The curr_active_slave found <nl> * will send all of this type of traffic . <nl> */ <nl> - if (( iph -> protocol == htons ( IPPROTO_IGMP )) && <nl> + if (( iph -> protocol == IPPROTO_IGMP ) && <nl> ( skb -> protocol == htons ( ETH_P_IP ))) { <nl>  <nl> read_lock (& bond -> curr_slave_lock );
static int blkvsc_do_operation ( struct block_device_context * blkdev , <nl>  <nl> page_buf = alloc_page ( GFP_KERNEL ); <nl> if (! page_buf ) { <nl> - kmem_cache_free ( blkvsc_req -> dev -> request_pool , blkvsc_req ); <nl> + kmem_cache_free ( blkdev -> request_pool , blkvsc_req ); <nl> return - ENOMEM ; <nl> } <nl>  <nl> cleanup : <nl>  <nl> __free_page ( page_buf ); <nl>  <nl> - kmem_cache_free ( blkvsc_req -> dev -> request_pool , blkvsc_req ); <nl> + kmem_cache_free ( blkdev -> request_pool , blkvsc_req ); <nl>  <nl> return ret ; <nl> }
static bool g4x_compute_wm0 ( struct drm_device * dev , <nl> int entries , tlb_miss ; <nl>  <nl> crtc = intel_get_crtc_for_plane ( dev , plane ); <nl> - if ( crtc -> fb == NULL || ! crtc -> enabled ) <nl> + if ( crtc -> fb == NULL || ! crtc -> enabled ) { <nl> + * cursor_wm = cursor -> guard_size ; <nl> + * plane_wm = display -> guard_size ; <nl> return false ; <nl> + } <nl>  <nl> htotal = crtc -> mode . htotal ; <nl> hdisplay = crtc -> mode . hdisplay ;
static void * alloc_qos_entry ( void ) <nl> } <nl> spin_unlock_irqrestore (& qos_free_list . lock , flags ); <nl>  <nl> - entry = kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> - return entry ; <nl> + return kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> } <nl>  <nl> static void free_qos_entry ( void * entry )
static int be_get_config ( struct be_adapter * adapter ) <nl> if ( status ) <nl> return status ; <nl> memcpy ( adapter -> netdev -> dev_addr , mac , ETH_ALEN ); <nl> + memcpy ( adapter -> netdev -> perm_addr , mac , ETH_ALEN ); <nl>  <nl> return 0 ; <nl> }
static int __devinit td_probe ( struct platform_device * pdev ) <nl> pdata -> channels + i ; <nl>  <nl> /* even channels are RX , odd are TX */ <nl> - if ((( i % 2 ) && pchan -> rx ) || (!( i % 2 ) && ! pchan -> rx )) { <nl> + if (( i % 2 ) == pchan -> rx ) { <nl> dev_err (& pdev -> dev , " Wrong channel configuration \ n "); <nl> err = - EINVAL ; <nl> goto err_tasklet_kill ;
static inline void calculate_imbalance ( struct lb_env * env , struct sd_lb_stats * s <nl> * max load less than avg load ( as we skip the groups at or below <nl> * its cpu_power , while calculating max_load ..) <nl> */ <nl> - if ( busiest -> avg_load < sds -> avg_load ) { <nl> + if ( busiest -> avg_load <= sds -> avg_load || <nl> + local -> avg_load >= sds -> avg_load ) { <nl> env -> imbalance = 0 ; <nl> return fix_small_imbalance ( env , sds ); <nl> }
int __kvm_set_memory_region ( struct kvm * kvm , <nl> goto out_free ; <nl> } <nl>  <nl> - kvm_free_physmem_slot (& old , & new ); <nl> + kvm_free_physmem_slot (& old , npages ? & new : NULL ); <nl> + /* Slot deletion case : we have to update the current slot */ <nl> + if (! npages ) <nl> + * memslot = old ; <nl> # ifdef CONFIG_DMAR <nl> /* map the pages in iommu page table */ <nl> r = kvm_iommu_map_pages ( kvm , base_gfn , npages );
static int mlx4_en_complete_rx_desc ( struct mlx4_en_priv * priv , <nl> PCI_DMA_FROMDEVICE ); <nl> } <nl> /* Adjust size of last fragment to match actual length */ <nl> - skb_frags_rx [ nr - 1 ]. size = length - <nl> - priv -> frag_info [ nr - 1 ]. frag_prefix_size ; <nl> + if ( nr > 0 ) <nl> + skb_frags_rx [ nr - 1 ]. size = length - <nl> + priv -> frag_info [ nr - 1 ]. frag_prefix_size ; <nl> return nr ; <nl>  <nl> fail :
int intel_framebuffer_init ( struct drm_device * dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> + /* FIXME need to adjust LINOFF / TILEOFF accordingly . */ <nl> + if ( mode_cmd -> offsets [ 0 ] != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = drm_framebuffer_init ( dev , & intel_fb -> base , & intel_fb_funcs ); <nl> if ( ret ) { <nl> DRM_ERROR (" framebuffer init failed % d \ n ", ret );
static u8 * __init alloc_event_buffer ( struct amd_iommu * iommu ) <nl> if ( iommu -> evt_buf == NULL ) <nl> return NULL ; <nl>  <nl> + iommu -> evt_buf_size = EVT_BUFFER_SIZE ; <nl> + <nl> return iommu -> evt_buf ; <nl> } <nl> 
static int rt2x00mac_tx_rts_cts ( struct rt2x00_dev * rt2x00dev , <nl> ( struct ieee80211_rts *)( skb -> data )); <nl>  <nl> if ( rt2x00queue_write_tx_frame ( queue , skb )) { <nl> + dev_kfree_skb_any ( skb ); <nl> WARNING ( rt2x00dev , " Failed to send RTS / CTS frame .\ n "); <nl> return NETDEV_TX_BUSY ; <nl> }
void rt2x00link_update_stats ( struct rt2x00_dev * rt2x00dev , <nl> struct link_ant * ant = & rt2x00dev -> link . ant ; <nl> struct ieee80211_hdr * hdr = ( struct ieee80211_hdr *) skb -> data ; <nl>  <nl> + /* <nl> + * No need to update the stats for != STA interfaces <nl> + */ <nl> + if (! rt2x00dev -> intf_sta_count ) <nl> + return ; <nl> + <nl> /* <nl> * Frame was received successfully since non - succesfull <nl> * frames would have been dropped by the hardware .
static inline void update_sd_power_savings_stats ( struct sched_group * group , <nl> * capacity but still has some space to pick up some load <nl> * from other group and save more power <nl> */ <nl> - if ( sgs -> sum_nr_running > sgs -> group_capacity - 1 ) <nl> + if ( sgs -> sum_nr_running + 1 > sgs -> group_capacity ) <nl> return ; <nl>  <nl> if ( sgs -> sum_nr_running > sds -> leader_nr_running ||
static int ip_vs_wrr_init_svc ( struct ip_vs_service * svc ) <nl> /* <nl> * Allocate the mark variable for WRR scheduling <nl> */ <nl> - mark = kmalloc ( sizeof ( struct ip_vs_wrr_mark ), GFP_ATOMIC ); <nl> + mark = kmalloc ( sizeof ( struct ip_vs_wrr_mark ), GFP_KERNEL ); <nl> if ( mark == NULL ) <nl> return - ENOMEM ; <nl> 
int cx25821_openfile_audio ( struct cx25821_dev * dev , <nl> vfs_read_retval = <nl> vfs_read ( myfile , mybuf , line_size , & pos ); <nl>  <nl> - if ( vfs_read_retval > 0 <nl> - && vfs_read_retval == line_size <nl> - && dev -> _audiodata_buf_virt_addr != NULL ) { <nl> + if ( vfs_read_retval > 0 && <nl> + vfs_read_retval == line_size && <nl> + dev -> _audiodata_buf_virt_addr != NULL ) { <nl> memcpy (( void *)( dev -> <nl> _audiodata_buf_virt_addr <nl> + offset / 4 ), mybuf ,
static int efx_init_lm87 ( struct efx_nic * efx , struct i2c_board_info * info , <nl> if (! client ) <nl> return - EIO ; <nl>  <nl> + /* Read - to - clear alarm / interrupt status */ <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS1 ); <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS2 ); <nl> + <nl> rc = efx_poke_lm87 ( client , reg_values ); <nl> if ( rc ) <nl> goto err ;
static ssize_t __iscsi_ ## prefix ## _store_ ## name ( \ <nl> \ <nl> if (! capable ( CAP_SYS_ADMIN )) \ <nl> return - EPERM ; \ <nl> - \ <nl> + if ( count >= sizeof ( auth -> name )) \ <nl> + return - EINVAL ; \ <nl> snprintf ( auth -> name , sizeof ( auth -> name ), "% s ", page ); \ <nl> if (! strncmp (" NULL ", auth -> name , 4 )) \ <nl> auth -> naf_flags &= ~ flags ; \
int mccic_irq ( struct mcam_camera * cam , unsigned int irqs ) <nl> if ( irqs & ( IRQ_EOF0 << frame )) { <nl> mcam_frame_complete ( cam , frame ); <nl> handled = 1 ; <nl> + if ( cam -> buffer_mode == B_DMA_sg ) <nl> + break ; <nl> } <nl> /* <nl> * If a frame starts , note that we have DMA active . This
static ssize_t set_pwm_mode ( struct device * dev , struct device_attribute * attr , <nl> if (!( val == 0 || val == 1 )) <nl> return - EINVAL ; <nl>  <nl> + /* F75373 does not support DC ( linear voltage ) fan control mode */ <nl> + if ( data -> kind == f75373 && val == 0 ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& data -> update_lock ); <nl> conf = f75375_read8 ( client , F75375_REG_CONFIG1 ); <nl> conf &= ~( 1 << FAN_CTRL_LINEAR ( nr ));
static int do_swap_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> } else if ( PageHWPoison ( page )) { <nl> ret = VM_FAULT_HWPOISON ; <nl> delayacct_clear_flag ( DELAYACCT_PF_SWAPIN ); <nl> - goto out ; <nl> + goto out_release ; <nl> } <nl>  <nl> lock_page ( page ); <nl> out_nomap : <nl> pte_unmap_unlock ( page_table , ptl ); <nl> out_page : <nl> unlock_page ( page ); <nl> + out_release : <nl> page_cache_release ( page ); <nl> return ret ; <nl> }
error3 : atomic_dec (& cm_id_priv -> refcount ); <nl> cm_deref_id ( listen_cm_id_priv ); <nl> cm_cleanup_timewait ( cm_id_priv -> timewait_info ); <nl> error2 : kfree ( cm_id_priv -> timewait_info ); <nl> + cm_id_priv -> timewait_info = NULL ; <nl> error1 : ib_destroy_cm_id (& cm_id_priv -> id ); <nl> return ret ; <nl> }
ath5k_ani_init ( struct ath5k_hw * ah , enum ath5k_ani_mode mode ) <nl> if ( ah -> ah_version < AR5K_AR5212 ) <nl> return ; <nl>  <nl> + if ( mode < ATH5K_ANI_MODE_OFF || mode > ATH5K_ANI_MODE_AUTO ) { <nl> + ATH5K_ERR ( ah -> ah_sc , " ANI mode % d out of range ", mode ); <nl> + return ; <nl> + } <nl> + <nl> /* clear old state information */ <nl> memset (& ah -> ah_sc -> ani_state , 0 , sizeof ( ah -> ah_sc -> ani_state )); <nl> 
static int new_term ( struct parse_events_term ** _term , int type_val , <nl> term -> val . str = str ; <nl> break ; <nl> default : <nl> + free ( term ); <nl> return - EINVAL ; <nl> } <nl> 
out_err : <nl> * errors we try again until the max number of retries is reached . <nl> */ <nl> if ( result != - EHOSTUNREACH && result != - ENETUNREACH && <nl> - result != - ENETDOWN && result != EINVAL <nl> + result != - ENETDOWN && result != - EINVAL <nl> && result != - EPROTONOSUPPORT ) { <nl> lowcomms_connect_sock ( con ); <nl> result = 0 ;
static void ath9k_hw_read_revisions ( struct ath_hw * ah ) <nl> val = REG_READ ( ah , AR_SREV ); <nl> ah -> hw_version . macRev = MS ( val , AR_SREV_REVISION2 ); <nl> return ; <nl> + case AR9300_DEVID_QCA955X : <nl> + ah -> hw_version . macVersion = AR_SREV_VERSION_9550 ; <nl> + return ; <nl> } <nl>  <nl> val = REG_READ ( ah , AR_SREV ) & AR_SREV_ID ;
static int setup_p6_watchdog ( unsigned nmi_hz ) <nl> perfctr_msr = MSR_P6_PERFCTR0 ; <nl> evntsel_msr = MSR_P6_EVNTSEL0 ; <nl>  <nl> - wrmsrl ( perfctr_msr , 0UL ); <nl> + /* KVM doesn ' t implement this MSR */ <nl> + if ( wrmsr_safe ( perfctr_msr , 0 , 0 ) < 0 ) <nl> + return 0 ; <nl>  <nl> evntsel = P6_EVNTSEL_INT <nl> | P6_EVNTSEL_OS
static void setup_frame_info ( struct ieee80211_hw * hw , <nl> fi -> keyix = ATH9K_TXKEYIX_INVALID ; <nl> fi -> keytype = keytype ; <nl> fi -> framelen = framelen ; <nl> + <nl> + if (! rate ) <nl> + return ; <nl> fi -> rtscts_rate = rate -> hw_value ; <nl> if ( short_preamble ) <nl> fi -> rtscts_rate |= rate -> hw_value_short ;
netdev_tx_t ixgbe_xmit_frame_ring ( struct sk_buff * skb , <nl> tx_flags |= IXGBE_TX_FLAGS_SW_VLAN ; <nl> } <nl>  <nl> + skb_tx_timestamp ( skb ); <nl> + <nl> # ifdef CONFIG_IXGBE_PTP <nl> if ( unlikely ( skb_shinfo ( skb )-> tx_flags & SKBTX_HW_TSTAMP )) { <nl> skb_shinfo ( skb )-> tx_flags |= SKBTX_IN_PROGRESS ;
static int i915_pipe_crc_open ( struct inode * inode , struct file * filep ) <nl> struct drm_i915_private * dev_priv = info -> dev -> dev_private ; <nl> struct intel_pipe_crc * pipe_crc = & dev_priv -> pipe_crc [ info -> pipe ]; <nl>  <nl> + if ( info -> pipe >= INTEL_INFO ( info -> dev )-> num_pipes ) <nl> + return - ENODEV ; <nl> + <nl> spin_lock_irq (& pipe_crc -> lock ); <nl>  <nl> if ( pipe_crc -> opened ) {
static const struct file_operations vfio_device_fops = { <nl> */ <nl> static char * vfio_devnode ( struct device * dev , umode_t * mode ) <nl> { <nl> - if ( MINOR ( dev -> devt ) == 0 ) <nl> + if ( mode && ( MINOR ( dev -> devt ) == 0 )) <nl> * mode = S_IRUGO | S_IWUGO ; <nl>  <nl> return kasprintf ( GFP_KERNEL , " vfio /% s ", dev_name ( dev ));
EXPORT_SYMBOL_GPL ( rcu_sched_lock_map ); <nl> # endif <nl>  <nl> int rcu_scheduler_active __read_mostly ; <nl> + EXPORT_SYMBOL_GPL ( rcu_scheduler_active ); <nl>  <nl> /* <nl> * This function is invoked towards the end of the scheduler ' s initialization
void intel_panel_enable_backlight ( struct intel_connector * connector ) <nl>  <nl> WARN_ON ( panel -> backlight . max == 0 ); <nl>  <nl> - if ( panel -> backlight . level == 0 ) { <nl> + if ( panel -> backlight . level <= panel -> backlight . min ) { <nl> panel -> backlight . level = panel -> backlight . max ; <nl> if ( panel -> backlight . device ) <nl> panel -> backlight . device -> props . brightness =
vxge_probe ( struct pci_dev * pdev , const struct pci_device_id * pre ) <nl> driver_config -> config_dev_cnt = 0 ; <nl> driver_config -> total_dev_cnt = 0 ; <nl> driver_config -> g_no_cpus = 0 ; <nl> - driver_config -> vpath_per_dev = max_config_vpath ; <nl> } <nl>  <nl> + driver_config -> vpath_per_dev = max_config_vpath ; <nl> + <nl> driver_config -> total_dev_cnt ++; <nl> if (++ driver_config -> config_dev_cnt > max_config_dev ) { <nl> ret = 0 ;
static int btrfs_remount ( struct super_block * sb , int * flags , char * data ) <nl> struct btrfs_root * root = btrfs_sb ( sb ); <nl> int ret ; <nl>  <nl> + ret = btrfs_parse_options ( root , data ); <nl> + if ( ret ) <nl> + return - EINVAL ; <nl> + <nl> if ((* flags & MS_RDONLY ) == ( sb -> s_flags & MS_RDONLY )) <nl> return 0 ; <nl> 
int perf_event__parse_sample ( const union perf_event * event , u64 type , <nl> u32 val32 [ 2 ]; <nl> } u ; <nl>  <nl> - <nl> + memset ( data , 0 , sizeof (* data )); <nl> data -> cpu = data -> pid = data -> tid = - 1 ; <nl> data -> stream_id = data -> id = data -> time = - 1ULL ; <nl> 
static int init_volumes ( struct ubi_device * ubi , <nl>  <nl> /* Static volumes only */ <nl> av = ubi_find_av ( ai , i ); <nl> - if (! av ) { <nl> + if (! av || ! av -> leb_count ) { <nl> /* <nl> * No eraseblocks belonging to this volume found . We <nl> * don ' t actually know whether this static volume is
static void __unregister_request ( struct ceph_osd_client * osdc , <nl> */ <nl> static void __cancel_request ( struct ceph_osd_request * req ) <nl> { <nl> - if ( req -> r_sent ) { <nl> + if ( req -> r_sent && req -> r_osd ) { <nl> ceph_con_revoke (& req -> r_osd -> o_con , req -> r_request ); <nl> req -> r_sent = 0 ; <nl> }
void i915_debugfs_cleanup ( struct drm_minor * minor ); <nl> int i915_debugfs_connector_add ( struct drm_connector * connector ); <nl> void intel_display_crc_init ( struct drm_device * dev ); <nl> # else <nl> - static inline int i915_debugfs_connector_add ( struct drm_connector * connector ) {} <nl> + static inline int i915_debugfs_connector_add ( struct drm_connector * connector ) <nl> +{ return 0 ; } <nl> static inline void intel_display_crc_init ( struct drm_device * dev ) {} <nl> # endif <nl> 
static int sh_eth_drv_probe ( struct platform_device * pdev ) <nl> } <nl> mdp -> tsu_addr = ioremap ( rtsu -> start , <nl> resource_size ( rtsu )); <nl> + if ( mdp -> tsu_addr == NULL ) { <nl> + ret = - ENOMEM ; <nl> + dev_err (& pdev -> dev , " TSU ioremap failed .\ n "); <nl> + goto out_release ; <nl> + } <nl> mdp -> port = devno % 2 ; <nl> ndev -> features = NETIF_F_HW_VLAN_FILTER ; <nl> }
struct iwl_cfg iwl6000i_2bg_cfg = { <nl> . fw_name_pre = IWL6050_FW_PRE , \ <nl> . ucode_api_max = IWL6050_UCODE_API_MAX , \ <nl> . ucode_api_min = IWL6050_UCODE_API_MIN , \ <nl> + . valid_tx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> + . valid_rx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> . ops = & iwl6050_ops , \ <nl> . eeprom_ver = EEPROM_6050_EEPROM_VERSION , \ <nl> . eeprom_calib_ver = EEPROM_6050_TX_POWER_VERSION , \
int mls_context_to_sid ( char oldc , <nl>  <nl> if (! selinux_mls_enabled ) { <nl> if ( def_sid != SECSID_NULL && oldc ) <nl> - * scontext += strlen (* scontext ); <nl> + * scontext += strlen (* scontext )+ 1 ; <nl> return 0 ; <nl> } <nl> 
static struct tgfx __init * tgfx_probe ( int parport , int * n_buttons , int n_devs ) <nl> if ( n_buttons [ i ] < 1 ) <nl> continue ; <nl>  <nl> - if ( n_buttons [ i ] > 6 ) { <nl> + if ( n_buttons [ i ] > ARRAY_SIZE ( tgfx_buttons )) { <nl> printk ( KERN_ERR " turbografx . c : Invalid number of buttons % d \ n ", n_buttons [ i ]); <nl> err = - EINVAL ; <nl> goto err_unreg_devs ;
rpcrdma_register_frmr_external ( struct rpcrdma_mr_seg * seg , <nl> if ( rc ) { <nl> dprintk (" RPC : % s : failed ib_post_send for register ," <nl> " status % i \ n ", __func__ , rc ); <nl> + ib_update_fast_reg_key ( mr , -- key ); <nl> goto out_err ; <nl> } else { <nl> seg1 -> mr_rkey = mr -> rkey ;
static void cx24120_check_cmd ( struct cx24120_state * state , u8 id ) <nl> case CMD_DISEQC_MSG2 : <nl> case CMD_SETVOLTAGE : <nl> case CMD_SETTONE : <nl> + case CMD_DISEQC_BURST : <nl> cx24120_msg_mpeg_output_global_config ( state , 0 ); <nl> /* Old driver would do a msleep ( 100 ) here */ <nl> default :
static int __cpuinit comp_pool_callback ( struct notifier_block * nfb , <nl> ehca_gen_dbg (" CPU : % x ( CPU_PREPARE )", cpu ); <nl> if (! create_comp_task ( pool , cpu )) { <nl> ehca_gen_err (" Can ' t create comp_task for cpu : % x ", cpu ); <nl> - return NOTIFY_BAD ; <nl> + return notifier_from_errno (- ENOMEM ); <nl> } <nl> break ; <nl> case CPU_UP_CANCELED :
int eeh_dn_check_failure ( struct device_node * dn , struct pci_dev * dev ) <nl> no_dn ++; <nl> return 0 ; <nl> } <nl> + dn = find_device_pe ( dn ); <nl> pdn = PCI_DN ( dn ); <nl>  <nl> /* Access to IO BARs might get this far and still not want checking . */
ieee80211_rx_h_michael_mic_verify ( struct ieee80211_rx_data * rx ) <nl> if ( status -> flag & RX_FLAG_MMIC_ERROR ) <nl> goto mic_fail ; <nl>  <nl> - if (!( status -> flag & RX_FLAG_IV_STRIPPED )) <nl> + if (!( status -> flag & RX_FLAG_IV_STRIPPED ) && rx -> key ) <nl> goto update_iv ; <nl>  <nl> return RX_CONTINUE ;
static int alps_enter_command_mode ( struct psmouse * psmouse , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( param [ 0 ] != 0x88 && param [ 1 ] != 0x07 ) { <nl> + if ( param [ 0 ] != 0x88 || ( param [ 1 ] != 0x07 && param [ 1 ] != 0x08 )) { <nl> psmouse_dbg ( psmouse , <nl> " unknown response while entering command mode \ n "); <nl> return - 1 ;
static void cyberjack_read_int_callback ( struct urb * urb ) <nl>  <nl> old_rdtodo = priv -> rdtodo ; <nl>  <nl> - if ( old_rdtodo + size < old_rdtodo ) { <nl> + if ( old_rdtodo > SHRT_MAX - size ) { <nl> dev_dbg ( dev , " To many bulk_in urbs to do .\ n "); <nl> spin_unlock (& priv -> lock ); <nl> goto resubmit ;
static int time_cpufreq_notifier ( struct notifier_block * nb , unsigned long val , <nl> tsc_khz = cpufreq_scale ( tsc_khz_ref , ref_freq , freq -> new ); <nl> if (!( freq -> flags & CPUFREQ_CONST_LOOPS )) <nl> mark_tsc_unstable (" cpufreq changes "); <nl> - } <nl>  <nl> - set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int fc2580_set_params ( struct dvb_frontend * fe ) <nl> { <nl> struct fc2580_priv * priv = fe -> tuner_priv ; <nl> struct dtv_frontend_properties * c = & fe -> dtv_property_cache ; <nl> - int ret , i ; <nl> + int ret = 0 , i ; <nl> unsigned int r_val , n_val , k_val , k_val_reg , f_ref ; <nl> u8 tmp_val , r18_val ; <nl> u64 f_vco ;
static void __init create_mapping ( phys_addr_t phys , unsigned long virt , <nl> void __iomem * __init early_io_map ( phys_addr_t phys , unsigned long virt ) <nl> { <nl> unsigned long size , mask ; <nl> - bool page64k = IS_ENABLED ( ARM64_64K_PAGES ); <nl> + bool page64k = IS_ENABLED ( CONFIG_ARM64_64K_PAGES ); <nl> pgd_t * pgd ; <nl> pud_t * pud ; <nl> pmd_t * pmd ;
static void * vb2_dc_alloc ( void * alloc_ctx , unsigned long size ) <nl> if (! buf ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> + /* align image size to PAGE_SIZE */ <nl> + size = PAGE_ALIGN ( size ); <nl> + <nl> buf -> vaddr = dma_alloc_coherent ( dev , size , & buf -> dma_addr , GFP_KERNEL ); <nl> if (! buf -> vaddr ) { <nl> dev_err ( dev , " dma_alloc_coherent of size % ld failed \ n ", size );
int pinconf_generic_parse_dt_config ( struct device_node * np , <nl> ncfg ++; <nl> } <nl>  <nl> + /* no configs found at all */ <nl> + if ( ncfg == 0 ) { <nl> + * configs = NULL ; <nl> + * nconfigs = 0 ; <nl> + return 0 ; <nl> + } <nl> + <nl> /* <nl> * Now limit the number of configs to the real number of <nl> * found properties .
static void do_ubd_request ( struct request_queue * q ) <nl> " errno = % d \ n ", - n ); <nl> else if ( list_empty (& dev -> restart )) <nl> list_add (& dev -> restart , & restart ); <nl> + kfree ( io_req ); <nl> return ; <nl> } <nl> 
static u32 tcp_yeah_ssthresh ( struct sock * sk ) <nl> yeah -> fast_count = 0 ; <nl> yeah -> reno_count = max ( yeah -> reno_count >> 1 , 2U ); <nl>  <nl> - return tp -> snd_cwnd - reduction ; <nl> + return max_t ( int , tp -> snd_cwnd - reduction , 2 ); <nl> } <nl>  <nl> static struct tcp_congestion_ops tcp_yeah __read_mostly = {
void ieee80211_beacon_connection_loss_work ( struct work_struct * work ) <nl> struct sta_info * sta ; <nl>  <nl> if ( ifmgd -> associated ) { <nl> + rcu_read_lock (); <nl> sta = sta_info_get ( sdata , ifmgd -> bssid ); <nl> if ( sta ) <nl> sta -> beacon_loss_count ++; <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> if ( sdata -> local -> hw . flags & IEEE80211_HW_CONNECTION_MONITOR )
static ssize_t read_vmcore ( struct file * file , char __user * buffer , <nl>  <nl> static const struct file_operations proc_vmcore_operations = { <nl> . read = read_vmcore , <nl> - . llseek = generic_file_llseek , <nl> + . llseek = default_llseek , <nl> }; <nl>  <nl> static struct vmcore * __init get_new_element ( void )
static void kswapd_try_to_sleep ( pg_data_t * pgdat , int order , int classzone_idx ) <nl> * them before going back to sleep . <nl> */ <nl> set_pgdat_percpu_threshold ( pgdat , calculate_normal_threshold ); <nl> - schedule (); <nl> + <nl> + if (! kthread_should_stop ()) <nl> + schedule (); <nl> + <nl> set_pgdat_percpu_threshold ( pgdat , calculate_pressure_threshold ); <nl> } else { <nl> if ( remaining )
static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl>  <nl> mutex_lock (& dev -> lock ); <nl>  <nl> - if ( dev -> open ++) <nl> + if ( dev -> open ) <nl> goto unlock ; <nl>  <nl> kref_get (& dev -> ref ); <nl> static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl> goto error_release ; <nl>  <nl> unlock : <nl> + dev -> open ++; <nl> mutex_unlock (& dev -> lock ); <nl> blktrans_dev_put ( dev ); <nl> return ret ;
static struct usb_device_id usb_ids [] = { <nl> { USB_DEVICE ( 0x157e , 0x300d ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x079b , 0x0062 ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x1582 , 0x6003 ), . driver_info = DEVICE_ZD1211B }, <nl> + { USB_DEVICE ( 0x050d , 0x705c ), . driver_info = DEVICE_ZD1211B }, <nl> /* " Driverless " devices that need ejecting */ <nl> { USB_DEVICE ( 0x0ace , 0x2011 ), . driver_info = DEVICE_INSTALLER }, <nl> {}
int radeon_info_ioctl ( struct drm_device * dev , void * data , struct drm_file * filp ) <nl> */ <nl> int radeon_driver_firstopen_kms ( struct drm_device * dev ) <nl> { <nl> + struct radeon_device * rdev = dev -> dev_private ; <nl> + <nl> + if ( rdev -> powered_down ) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static void dwc2_hsotg_start_req ( struct dwc2_hsotg * hsotg , <nl> /* If endpoint is stalled , we will restart request later */ <nl> ctrl = dwc2_readl ( hsotg -> regs + epctrl_reg ); <nl>  <nl> - if ( ctrl & DXEPCTL_STALL ) { <nl> + if ( index && ctrl & DXEPCTL_STALL ) { <nl> dev_warn ( hsotg -> dev , "% s : ep % d is stalled \ n ", __func__ , index ); <nl> return ; <nl> }
static struct mount * clone_mnt ( struct mount * old , struct dentry * root , <nl> } <nl>  <nl> /* Don ' t allow unprivileged users to reveal what is under a mount */ <nl> - if (( flag & CL_UNPRIVILEGED ) && list_empty (& old -> mnt_expire )) <nl> + if (( flag & CL_UNPRIVILEGED ) && <nl> + (!( flag & CL_EXPIRE ) || list_empty (& old -> mnt_expire ))) <nl> mnt -> mnt . mnt_flags |= MNT_LOCKED ; <nl>  <nl> atomic_inc (& sb -> s_active );
static void gc_attach ( struct parport * pp ) <nl> pads = gc_cfg [ port_idx ]. args + 1 ; <nl> n_pads = gc_cfg [ port_idx ]. nargs - 1 ; <nl>  <nl> + memset (& gc_parport_cb , 0 , sizeof ( gc_parport_cb )); <nl> gc_parport_cb . flags = PARPORT_FLAG_EXCL ; <nl>  <nl> pd = parport_register_dev_model ( pp , " gamecon ", & gc_parport_cb ,
int fib_nh_match ( struct fib_config * cfg , struct fib_info * fi ) <nl> return 1 ; <nl>  <nl> attrlen = rtnh_attrlen ( rtnh ); <nl> - if ( attrlen < 0 ) { <nl> + if ( attrlen > 0 ) { <nl> struct nlattr * nla , * attrs = rtnh_attrs ( rtnh ); <nl>  <nl> nla = nla_find ( attrs , attrlen , RTA_GATEWAY );
void kvm_check_async_pf_completion ( struct kvm_vcpu * vcpu ) <nl> spin_unlock (& vcpu -> async_pf . lock ); <nl>  <nl> kvm_arch_async_page_ready ( vcpu , work ); <nl> - kvm_arch_async_page_present ( vcpu , work ); <nl> + kvm_async_page_present_async ( vcpu , work ); <nl>  <nl> list_del (& work -> queue ); <nl> vcpu -> async_pf . queued --;
static void ipw_handle_data_packet ( struct ipw_priv * priv , <nl> IPW_DEBUG_RX (" Rx packet of % d bytes .\ n ", rxb -> skb -> len ); <nl>  <nl> /* HW decrypt will not clear the WEP bit , MIC , PN , etc . */ <nl> - if (! priv -> ieee -> host_decrypt ) <nl> + if (! priv -> ieee -> host_decrypt && priv -> ieee -> iw_mode != IW_MODE_MONITOR ) <nl> ipw_rebuild_decrypted_skb ( priv , rxb -> skb ); <nl>  <nl> if (! ieee80211_rx ( priv -> ieee , rxb -> skb , stats ))
static int dma40_memcpy_channels [] = { <nl> }; <nl>  <nl> /* Default configuration for physcial memcpy */ <nl> - struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> + static struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> . mode = STEDMA40_MODE_PHYSICAL , <nl> . dir = DMA_MEM_TO_MEM , <nl>  <nl> struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> }; <nl>  <nl> /* Default configuration for logical memcpy */ <nl> - struct stedma40_chan_cfg dma40_memcpy_conf_log = { <nl> + static struct stedma40_chan_cfg dma40_memcpy_conf_log = { <nl> . mode = STEDMA40_MODE_LOGICAL , <nl> . dir = DMA_MEM_TO_MEM , <nl> 
static void __init atmci_get_cap ( struct atmel_mci * host ) <nl> /* keep only major version number */ <nl> switch ( version & 0xf00 ) { <nl> case 0x100 : <nl> + host -> caps . has_pdc = 1 ; <nl> + break ; <nl> case 0x200 : <nl> host -> caps . has_pdc = 1 ; <nl> host -> caps . has_rwproof = 1 ;
int of_gpio_simple_xlate ( struct gpio_chip * gc , <nl> if ( WARN_ON ( gpiospec -> args_count < gc -> of_gpio_n_cells )) <nl> return - EINVAL ; <nl>  <nl> - if ( gpiospec -> args [ 0 ] > gc -> ngpio ) <nl> + if ( gpiospec -> args [ 0 ] >= gc -> ngpio ) <nl> return - EINVAL ; <nl>  <nl> if ( flags )
void kvm_hv_process_stimers ( struct kvm_vcpu * vcpu ) <nl> for ( i = 0 ; i < ARRAY_SIZE ( hv_vcpu -> stimer ); i ++) <nl> if ( test_and_clear_bit ( i , hv_vcpu -> stimer_pending_bitmap )) { <nl> stimer = & hv_vcpu -> stimer [ i ]; <nl> - stimer_stop ( stimer ); <nl> if ( stimer -> config & HV_STIMER_ENABLE ) { <nl> time_now = get_time_ref_counter ( vcpu -> kvm ); <nl> if ( time_now >= stimer -> exp_time )
static int chaoskey_rng_read ( struct hwrng * rng , void * data , <nl> if ( this_time > max ) <nl> this_time = max ; <nl>  <nl> - memcpy ( data , dev -> buf , this_time ); <nl> + memcpy ( data , dev -> buf + dev -> used , this_time ); <nl>  <nl> dev -> used += this_time ; <nl> 
static int mxs_dcp_start_dma ( struct dcp_async_ctx * actx ) <nl> struct dcp * sdcp = global_sdcp ; <nl> const int chan = actx -> chan ; <nl> uint32_t stat ; <nl> - int ret ; <nl> + unsigned long ret ; <nl> struct dcp_dma_desc * desc = & sdcp -> coh -> desc [ actx -> chan ]; <nl>  <nl> dma_addr_t desc_phys = dma_map_single ( sdcp -> dev , desc , sizeof (* desc ),
static int add_munmap ( unsigned long addr , unsigned long len , <nl> struct host_vm_op * last ; <nl> int ret = 0 ; <nl>  <nl> + if (( addr >= STUB_START ) && ( addr < STUB_END )) <nl> + return - EINVAL ; <nl> + <nl> if ( hvc -> index != 0 ) { <nl> last = & hvc -> ops [ hvc -> index - 1 ]; <nl> if (( last -> type == MUNMAP ) &&
static int rxq_process ( struct ieee80211_hw * hw , int index , int limit ) <nl> rmb (); <nl>  <nl> skb = rxq -> rx_skb [ rxq -> rx_head ]; <nl> + if ( skb == NULL ) <nl> + break ; <nl> rxq -> rx_skb [ rxq -> rx_head ] = NULL ; <nl>  <nl> rxq -> rx_head = ( rxq -> rx_head + 1 ) % MWL8K_RX_DESCS ;
static int amdgpu_vm_clear_bo ( struct amdgpu_device * adev , <nl> if ( r ) <nl> return r ; <nl>  <nl> + r = reservation_object_reserve_shared ( bo -> tbo . resv ); <nl> + if ( r ) <nl> + return r ; <nl> + <nl> r = ttm_bo_validate (& bo -> tbo , & bo -> placement , true , false ); <nl> if ( r ) <nl> goto error_unreserve ;
SYSCALL_DEFINE6 ( sparc_ipc , unsigned int , call , int , first , unsigned long , second <nl> long err ; <nl>  <nl> /* No need for backward compatibility . We can start fresh ... */ <nl> - if ( call <= SEMCTL ) { <nl> + if ( call <= SEMTIMEDOP ) { <nl> switch ( call ) { <nl> case SEMOP : <nl> err = sys_semtimedop ( first , ptr ,
static void __init exynos_reserve ( void ) <nl> " samsung , mfc - v5 ", <nl> " samsung , mfc - v6 ", <nl> " samsung , mfc - v7 ", <nl> + " samsung , mfc - v8 ", <nl> }; <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( mfc_mem ); i ++)
at86rf230_tx_complete ( void * context ) <nl> { <nl> struct at86rf230_state_change * ctx = context ; <nl> struct at86rf230_local * lp = ctx -> lp ; <nl> - struct sk_buff * skb = lp -> tx_skb ; <nl>  <nl> enable_irq ( lp -> spi -> irq ); <nl>  <nl> - ieee802154_xmit_complete ( lp -> hw , skb , ! lp -> tx_aret ); <nl> + ieee802154_xmit_complete ( lp -> hw , lp -> tx_skb , ! lp -> tx_aret ); <nl> } <nl>  <nl> static void
recheck : <nl> */ <nl> if (! capable ( CAP_SYS_NICE )) { <nl> /* can ' t change policy */ <nl> - if ( policy != p -> policy ) <nl> + if ( policy != p -> policy && <nl> + ! p -> signal -> rlim [ RLIMIT_RTPRIO ]. rlim_cur ) <nl> return - EPERM ; <nl> /* can ' t increase priority */ <nl> if ( policy != SCHED_NORMAL &&
static int __devinit adp8870_probe ( struct i2c_client * client , <nl> mutex_init (& data -> lock ); <nl>  <nl> memset (& props , 0 , sizeof ( props )); <nl> + props . type = BACKLIGHT_RAW ; <nl> props . max_brightness = props . brightness = ADP8870_MAX_BRIGHTNESS ; <nl> bl = backlight_device_register ( dev_driver_string (& client -> dev ), <nl> & client -> dev , data , & adp8870_bl_ops , & props );
ldebugfs_fid_write_common ( const char __user * buffer , size_t count , <nl> rc = sscanf ( kernbuf , "[% llx - % llx ]\ n ", <nl> ( unsigned long long *)& tmp . lsr_start , <nl> ( unsigned long long *)& tmp . lsr_end ); <nl> + if ( rc != 2 ) <nl> + return - EINVAL ; <nl> if (! range_is_sane (& tmp ) || range_is_zero (& tmp ) || <nl> tmp . lsr_start < range -> lsr_start || tmp . lsr_end > range -> lsr_end ) <nl> return - EINVAL ;
static enum print_line_t trace_stack_print ( struct trace_iterator * iter , <nl> trace_assign_type ( field , iter -> ent ); <nl>  <nl> for ( i = 0 ; i < FTRACE_STACK_ENTRIES ; i ++) { <nl> + if (! field -> caller [ i ]) <nl> + break ; <nl> if ( i ) { <nl> if (! trace_seq_puts ( s , " <= ")) <nl> goto partial ;
static void moxa_start ( struct tty_struct * tty ) <nl> if ( ch == NULL ) <nl> return ; <nl>  <nl> - if (!( ch -> statusflags & TXSTOPPED )) <nl> + if (! test_bit ( TXSTOPPED , & ch -> statusflags )) <nl> return ; <nl>  <nl> MoxaPortTxEnable ( ch );
static void tg3_adjust_link ( struct net_device * dev ) <nl>  <nl> if ( phydev -> speed == SPEED_100 || phydev -> speed == SPEED_10 ) <nl> mac_mode |= MAC_MODE_PORT_MODE_MII ; <nl> - else <nl> + else if ( phydev -> speed == SPEED_1000 || <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) != ASIC_REV_5785 ) <nl> mac_mode |= MAC_MODE_PORT_MODE_GMII ; <nl> + else <nl> + mac_mode |= MAC_MODE_PORT_MODE_MII ; <nl>  <nl> if ( phydev -> duplex == DUPLEX_HALF ) <nl> mac_mode |= MAC_MODE_HALF_DUPLEX ;
static void rtl8150_disconnect ( struct usb_interface * intf ) <nl> if ( dev ) { <nl> set_bit ( RTL8150_UNPLUG , & dev -> flags ); <nl> tasklet_disable (& dev -> tl ); <nl> + tasklet_kill (& dev -> tl ); <nl> unregister_netdev ( dev -> netdev ); <nl> unlink_all_urbs ( dev ); <nl> free_all_urbs ( dev );
static int vmw_cotable_unbind ( struct vmw_resource * res , <nl> struct vmw_private * dev_priv = res -> dev_priv ; <nl> struct ttm_buffer_object * bo = val_buf -> bo ; <nl> struct vmw_fence_obj * fence ; <nl> - int ret ; <nl>  <nl> if ( list_empty (& res -> mob_head )) <nl> return 0 ; <nl> static int vmw_cotable_unbind ( struct vmw_resource * res , <nl> if ( likely ( fence != NULL )) <nl> vmw_fence_obj_unreference (& fence ); <nl>  <nl> - return ret ; <nl> + return 0 ; <nl> } <nl>  <nl> /**
static int hostap_set_generic_element ( PSDevice pDevice , <nl> { <nl> PSMgmtObject pMgmt = pDevice -> pMgmt ; <nl>  <nl> + if ( param -> u . generic_elem . len > sizeof ( pMgmt -> abyWPAIE )) <nl> + return - EINVAL ; <nl> + <nl> memcpy ( pMgmt -> abyWPAIE , <nl> param -> u . generic_elem . data , <nl> param -> u . generic_elem . len
static int sh_mdio_release ( struct net_device * ndev ) <nl> /* remove mdio bus info from net_device */ <nl> dev_set_drvdata (& ndev -> dev , NULL ); <nl>  <nl> + /* free interrupts memory */ <nl> + kfree ( bus -> irq ); <nl> + <nl> /* free bitbang info */ <nl> free_mdio_bitbang ( bus ); <nl> 
retry : <nl> handle = ext3_journal_start ( inode , 2 ); <nl> if ( IS_ERR ( handle )) { <nl> /* This is really bad luck . We ' ve written the data <nl> - * but cannot extend i_size . Bail out and pretend <nl> - * the write failed ... */ <nl> + * but cannot extend i_size . Truncate allocated blocks <nl> + * and pretend the write failed ... */ <nl> + ext3_truncate ( inode ); <nl> ret = PTR_ERR ( handle ); <nl> goto out ; <nl> }
static int do_insnlist_ioctl ( struct comedi_device * dev , <nl> goto error ; <nl> } <nl>  <nl> + if ( sizeof ( struct comedi_insn ) * insnlist . n_insns < insnlist . n_insns ) { <nl> + ret = - EINVAL ; <nl> + goto error ; <nl> + } <nl> + <nl> insns = <nl> kmalloc ( sizeof ( struct comedi_insn ) * insnlist . n_insns , GFP_KERNEL ); <nl> if (! insns ) {
_base_sas_log_info ( struct MPT2SAS_ADAPTER * ioc , u32 log_info ) <nl> return ; <nl>  <nl> /* eat the loginfos associated with task aborts */ <nl> - if ( ioc -> ignore_loginfos && ( log_info == 30050000 || log_info == <nl> + if ( ioc -> ignore_loginfos && ( log_info == 0x30050000 || log_info == <nl> 0x31140000 || log_info == 0x31130000 )) <nl> return ; <nl> 
static SENSOR_DEVICE_ATTR ( temp4_input , S_IRUGO , show_temp , NULL , 3 ); <nl> REG : count of 90kHz pulses / revolution */ <nl> static int fan_from_reg ( u16 reg ) <nl> { <nl> + if ( reg == 0 || reg == 0xffff ) <nl> + return 0 ; <nl> return 90000 * 60 / reg ; <nl> } <nl> 
void ata_bmdma_error_handler ( struct ata_port * ap ) <nl> */ <nl> void ata_bmdma_post_internal_cmd ( struct ata_queued_cmd * qc ) <nl> { <nl> - ata_bmdma_stop ( qc ); <nl> + if ( qc -> ap -> ioaddr . bmdma_addr ) <nl> + ata_bmdma_stop ( qc ); <nl> } <nl>  <nl> # ifdef CONFIG_PCI
static int nl80211_start_radar_detection ( struct sk_buff * skb , <nl> struct net_device * dev = info -> user_ptr [ 1 ]; <nl> struct wireless_dev * wdev = dev -> ieee80211_ptr ; <nl> struct cfg80211_chan_def chandef ; <nl> + enum nl80211_dfs_regions dfs_region ; <nl> int err ; <nl>  <nl> + dfs_region = reg_get_dfs_region ( wdev -> wiphy ); <nl> + if ( dfs_region == NL80211_DFS_UNSET ) <nl> + return - EINVAL ; <nl> + <nl> err = nl80211_parse_chandef ( rdev , info , & chandef ); <nl> if ( err ) <nl> return err ;
static int mxs_mmc_probe ( struct platform_device * pdev ) <nl> if (! ssp -> dmach ) { <nl> dev_err ( mmc_dev ( host -> mmc ), <nl> "% s : failed to request dma \ n ", __func__ ); <nl> + ret = - ENODEV ; <nl> goto out_clk_put ; <nl> } <nl> 
static void blade_image_blit ( struct tridentfb_par * par , const char * data , <nl> writemmr ( par , DST1 , point ( x , y )); <nl> writemmr ( par , DST2 , point ( x + w - 1 , y + h - 1 )); <nl>  <nl> - memcpy ( par -> io_virt + 0x10000 , data , 4 * size ); <nl> + iowrite32_rep ( par -> io_virt + 0x10000 , data , size ); <nl> } <nl>  <nl> static void blade_copy_rect ( struct tridentfb_par * par ,
static int wm5100_set_sysclk ( struct snd_soc_codec * codec , int clk_id , <nl> fval = 1 ; <nl> break ; <nl> case 22579200 : <nl> - case 2457600 : <nl> + case 24576000 : <nl> fval = 2 ; <nl> break ; <nl> default : <nl> static int wm5100_set_sysclk ( struct snd_soc_codec * codec , int clk_id , <nl>  <nl> case 6144000 : <nl> case 12288000 : <nl> - case 2457600 : <nl> + case 24576000 : <nl> audio_rate = 48000 ; <nl> break ; <nl> 
static int init_timers_cpu ( int cpu ) <nl> /* <nl> * The APs use this path later in boot <nl> */ <nl> - base = kmalloc_node ( sizeof (* base ), <nl> - GFP_KERNEL | __GFP_ZERO , <nl> - cpu_to_node ( cpu )); <nl> + base = kzalloc_node ( sizeof (* base ), GFP_KERNEL , <nl> + cpu_to_node ( cpu )); <nl> if (! base ) <nl> return - ENOMEM ; <nl> 
asmlinkage long sys_rt_sigreturn ( struct pt_regs * regs ) <nl>  <nl> /* It is more difficult to avoid calling this function than to <nl> call it and ignore errors . */ <nl> - if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> r1 )) <nl> + if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> r1 ) == - EFAULT ) <nl> goto badframe ; <nl>  <nl> return rval ;
iscsi_sendpage ( struct iscsi_conn * conn , struct iscsi_buf * buf , <nl> BUG_ON ( buf -> sent + size > buf -> sg . length ); <nl> if ( size > * count ) <nl> size = * count ; <nl> - if ( buf -> sent + size != buf -> sg . length ) <nl> + if ( buf -> sent + size != buf -> sg . length || * count != size ) <nl> flags |= MSG_MORE ; <nl>  <nl> res = iscsi_send ( sk , buf , size , flags );
int irq_domain_simple_dt_translate ( struct irq_domain * d , <nl> return - EINVAL ; <nl> if ( intsize < 1 ) <nl> return - EINVAL ; <nl> + if ( d -> nr_irq && (( intspec [ 0 ] < d -> hwirq_base ) || <nl> + ( intspec [ 0 ] >= d -> hwirq_base + d -> nr_irq ))) <nl> + return - EINVAL ; <nl>  <nl> * out_hwirq = intspec [ 0 ]; <nl> * out_type = IRQ_TYPE_NONE ;
static int tcp_tso_should_defer ( struct sock * sk , struct tcp_sock * tp , struct sk_ <nl> if ( TCP_SKB_CB ( skb )-> flags & TCPCB_FLAG_FIN ) <nl> return 0 ; <nl>  <nl> + if ( tp -> ca_state != TCP_CA_Open ) <nl> + return 0 ; <nl> + <nl> in_flight = tcp_packets_in_flight ( tp ); <nl>  <nl> BUG_ON ( tcp_skb_pcount ( skb ) <= 1 ||
static int snd_compr_allocate_buffer ( struct snd_compr_stream * stream , <nl> unsigned int buffer_size ; <nl> void * buffer ; <nl>  <nl> + if ( params -> buffer . fragment_size == 0 || <nl> + params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + return - EINVAL ; <nl> + <nl> buffer_size = params -> buffer . fragment_size * params -> buffer . fragments ; <nl> if ( stream -> ops -> copy ) { <nl> buffer = NULL ;
static void slob_free ( void * block , int size ) <nl> sp -> units += units ; <nl>  <nl> if ( b < sp -> free ) { <nl> + if ( b + units == sp -> free ) { <nl> + units += slob_units ( sp -> free ); <nl> + sp -> free = slob_next ( sp -> free ); <nl> + } <nl> set_slob ( b , units , sp -> free ); <nl> sp -> free = b ; <nl> } else {
retry : <nl> goto found ; <nl> } <nl>  <nl> - while ( addr + size >= first -> va_start && addr + size <= vend ) { <nl> + while ( addr + size > first -> va_start && addr + size <= vend ) { <nl> addr = ALIGN ( first -> va_end + PAGE_SIZE , align ); <nl>  <nl> n = rb_next (& first -> rb_node );
int intel_framebuffer_init ( struct drm_device * dev , <nl> case DRM_FORMAT_UYVY : <nl> case DRM_FORMAT_YVYU : <nl> case DRM_FORMAT_VYUY : <nl> - if ( INTEL_INFO ( dev )-> gen < 6 ) <nl> + if ( INTEL_INFO ( dev )-> gen < 5 ) <nl> return - EINVAL ; <nl> break ; <nl> default :
int iwl_mvm_tof_responder_cmd ( struct iwl_mvm * mvm , <nl> if (! fw_has_capa (& mvm -> fw -> ucode_capa , IWL_UCODE_TLV_CAPA_TOF_SUPPORT )) <nl> return - EINVAL ; <nl>  <nl> - if ( vif -> p2p || vif -> type != NL80211_IFTYPE_AP ) { <nl> + if ( vif -> p2p || vif -> type != NL80211_IFTYPE_AP || <nl> + ! mvmvif -> ap_ibss_active ) { <nl> IWL_ERR ( mvm , " Cannot start responder , not in AP mode \ n "); <nl> return - EIO ; <nl> }
ecryptfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> ecryptfs_copy_attr_all ( old_dir , lower_old_dir_dentry -> d_inode ); <nl> out_lock : <nl> unlock_rename ( lower_old_dir_dentry , lower_new_dir_dentry ); <nl> + dput ( lower_new_dentry -> d_parent ); <nl> + dput ( lower_old_dentry -> d_parent ); <nl> dput ( lower_new_dentry ); <nl> dput ( lower_old_dentry ); <nl> return rc ;
struct fbcon_ops { <nl> # define attr_fgcol ( fgshift , s ) \ <nl> ((( s ) >> ( fgshift )) & 0x0f ) <nl> # define attr_bgcol ( bgshift , s ) \ <nl> - ((( s ) >> ( bgshift )) & 0x0f ) <nl> + ((( s ) >> ( bgshift )) & 0x07 ) <nl>  <nl> /* Monochrome */ <nl> # define attr_bold ( s ) \
static int ccmp_encrypt_skb ( struct ieee80211_tx_data * tx , struct sk_buff * skb ) <nl> memmove ( pos , pos + CCMP_HDR_LEN , hdrlen ); <nl>  <nl> /* the HW only needs room for the IV , but not the actual IV */ <nl> - if ( info -> control . hw_key -> flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE ) <nl> + if ( info -> control . hw_key && <nl> + ( info -> control . hw_key -> flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE )) <nl> return 0 ; <nl>  <nl> hdr = ( struct ieee80211_hdr *) pos ;
ia64_global_tlb_purge ( struct mm_struct * mm , unsigned long start , <nl> { <nl> static DEFINE_SPINLOCK ( ptcg_lock ); <nl>  <nl> - if ( mm != current -> active_mm ) { <nl> + if ( mm != current -> active_mm || ! current -> mm ) { <nl> flush_tlb_all (); <nl> return ; <nl> }
void __init setup_per_cpu_areas ( void ) <nl> fc = __alloc_bootmem ( unit_size , PAGE_SIZE , __pa ( MAX_DMA_ADDRESS )); <nl> if (! ai || ! fc ) <nl> panic (" Failed to allocate memory for percpu areas ."); <nl> + /* kmemleak tracks the percpu allocations separately */ <nl> + kmemleak_free ( fc ); <nl>  <nl> ai -> dyn_size = unit_size ; <nl> ai -> unit_size = unit_size ;
int MoxaDriverIoctl ( unsigned int cmd , unsigned long arg , int port ) <nl> case MOXA_FIND_BOARD : <nl> case MOXA_LOAD_C320B : <nl> case MOXA_LOAD_CODE : <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> break ; <nl> } <nl> 
static int exynos_drm_crtc_mode_set_base ( struct drm_crtc * crtc , int x , int y , <nl>  <nl> DRM_DEBUG_KMS ("% s \ n ", __FILE__ ); <nl>  <nl> + /* when framebuffer changing is requested , crtc ' s dpms should be on */ <nl> + if ( exynos_crtc -> dpms > DRM_MODE_DPMS_ON ) { <nl> + DRM_ERROR (" failed framebuffer changing request .\ n "); <nl> + return - EPERM ; <nl> + } <nl> + <nl> crtc_w = crtc -> fb -> width - x ; <nl> crtc_h = crtc -> fb -> height - y ; <nl> 
static int dmx_section_feed_release_filter ( struct dmx_section_feed * feed , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( feed -> is_filtering ) <nl> + if ( feed -> is_filtering ) { <nl> + /* release dvbdmx -> mutex as far as <nl> + it is acquired by stop_filtering () itself */ <nl> + mutex_unlock (& dvbdmx -> mutex ); <nl> feed -> stop_filtering ( feed ); <nl> + mutex_lock (& dvbdmx -> mutex ); <nl> + } <nl>  <nl> spin_lock_irq (& dvbdmx -> lock ); <nl> f = dvbdmxfeed -> filter ;
void usb_serial_generic_process_read_urb ( struct urb * urb ) <nl> char * ch = ( char *) urb -> transfer_buffer ; <nl> int i ; <nl>  <nl> + if (! urb -> actual_length ) <nl> + return ; <nl> + <nl> tty = tty_port_tty_get (& port -> port ); <nl> if (! tty ) <nl> return ;
static int hba_setup_cid_tbls ( struct beiscsi_hba * phba ) <nl> kfree ( phba -> ep_array ); <nl> phba -> ep_array = NULL ; <nl> ret = - ENOMEM ; <nl> + <nl> + goto free_memory ; <nl> } <nl>  <nl> for ( i = 0 ; i < phba -> params . cxns_per_ctrl ; i ++) {
handle_locking_key ( struct input_dev * visorinput_dev , <nl> int keycode , int desired_state ) <nl> { <nl> int led ; <nl> - char * sled ; <nl>  <nl> switch ( keycode ) { <nl> case KEY_CAPSLOCK : <nl> led = LED_CAPSL ; <nl> - sled = " CAP "; <nl> break ; <nl> case KEY_SCROLLLOCK : <nl> led = LED_SCROLLL ; <nl> - sled = " SCR "; <nl> break ; <nl> case KEY_NUMLOCK : <nl> led = LED_NUML ; <nl> - sled = " NUM "; <nl> break ; <nl> default : <nl> led = - 1 ;
void iscsi_boot_destroy_kset ( struct iscsi_boot_kset * boot_kset ) <nl> iscsi_boot_remove_kobj ( boot_kobj ); <nl>  <nl> kset_unregister ( boot_kset -> kset ); <nl> + kfree ( boot_kset ); <nl> } <nl> EXPORT_SYMBOL_GPL ( iscsi_boot_destroy_kset );
static void intel_dp_commit ( struct drm_encoder * encoder ) <nl>  <nl> if ( IS_eDP ( intel_dp ) || IS_PCH_eDP ( intel_dp )) <nl> ironlake_edp_backlight_on ( dev ); <nl> + intel_dp -> dpms_mode = DRM_MODE_DPMS_ON ; <nl> } <nl>  <nl> static void
static int __init parse_crashkernel_mem ( char * cmdline , <nl> } while (* cur ++ == ','); <nl>  <nl> if (* crash_size > 0 ) { <nl> - while (* cur != ' ' && * cur != '@') <nl> + while (* cur && * cur != ' ' && * cur != '@') <nl> cur ++; <nl> if (* cur == '@') { <nl> cur ++;
extern struct page ** ceph_get_direct_page_vector ( const void __user * data , <nl> bool write_page ); <nl> extern void ceph_put_page_vector ( struct page ** pages , int num_pages , <nl> bool dirty ); <nl> - extern void ceph_release_page_vector ( struct page ** pages , int num_pages ); <nl> extern struct page ** ceph_alloc_page_vector ( int num_pages , gfp_t flags ); <nl> extern int ceph_copy_user_to_page_vector ( struct page ** pages , <nl> const void __user * data ,
static struct ib_mr * mthca_reg_phys_mr ( struct ib_pd * pd , <nl> convert_access ( acc ), mr ); <nl>  <nl> if ( err ) { <nl> + kfree ( page_list ); <nl> kfree ( mr ); <nl> return ERR_PTR ( err ); <nl> }
int rsnd_dai_connect ( struct rsnd_mod * mod , <nl> if (! mod ) <nl> return - EIO ; <nl>  <nl> + if ( io -> mod [ type ]) <nl> + return - EINVAL ; <nl> + <nl> priv = rsnd_mod_to_priv ( mod ); <nl> dev = rsnd_priv_to_dev ( priv ); <nl> 
again : <nl> * refill the WL pool synchronous . */ <nl> if ( pool -> used == pool -> size || wl_pool -> used == wl_pool -> size ) { <nl> spin_unlock (& ubi -> wl_lock ); <nl> - ubi_update_fastmap ( ubi ); <nl> + ret = ubi_update_fastmap ( ubi ); <nl> + if ( ret ) { <nl> + ubi_msg ( ubi , " Unable to write a new fastmap : % i ", ret ); <nl> + return - ENOSPC ; <nl> + } <nl> spin_lock (& ubi -> wl_lock ); <nl> } <nl> 
skip_create_disk : <nl> blk_queue_max_hw_sectors ( dd -> queue , 0xffff ); <nl> blk_queue_max_segment_size ( dd -> queue , 0x400000 ); <nl> blk_queue_io_min ( dd -> queue , 4096 ); <nl> + blk_queue_bounce_limit ( dd -> queue , dd -> pdev -> dma_mask ); <nl>  <nl> /* <nl> * write back cache is not supported in the device . FUA depends on
static int mmu_topup_memory_cache_page ( struct kvm_mmu_memory_cache * cache , <nl> static void mmu_free_memory_cache_page ( struct kvm_mmu_memory_cache * mc ) <nl> { <nl> while ( mc -> nobjs ) <nl> - __free_page ( mc -> objects [-- mc -> nobjs ]); <nl> + free_page (( unsigned long ) mc -> objects [-- mc -> nobjs ]); <nl> } <nl>  <nl> static int __mmu_topup_memory_caches ( struct kvm_vcpu * vcpu , gfp_t gfp_flags )
void blkcg_drain_queue ( struct request_queue * q ) <nl> { <nl> lockdep_assert_held ( q -> queue_lock ); <nl>  <nl> + /* <nl> + * @ q could be exiting and already have destroyed all blkgs as <nl> + * indicated by NULL root_blkg . If so , don ' t confuse policies . <nl> + */ <nl> + if (! q -> root_blkg ) <nl> + return ; <nl> + <nl> blk_throtl_drain ( q ); <nl> } <nl> 
static int snapshot_status ( struct dm_target * ti , status_type_t type , <nl> unsigned sz = 0 ; <nl> struct dm_snapshot * snap = ti -> private ; <nl>  <nl> + down_write (& snap -> lock ); <nl> + <nl> switch ( type ) { <nl> case STATUSTYPE_INFO : <nl> if (! snap -> valid ) <nl> static int snapshot_status ( struct dm_target * ti , status_type_t type , <nl> break ; <nl> } <nl>  <nl> + up_write (& snap -> lock ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int __init efi_rtc_probe ( struct platform_device * dev ) <nl> if ( IS_ERR ( rtc )) <nl> return PTR_ERR ( rtc ); <nl>  <nl> + rtc -> uie_unsupported = 1 ; <nl> platform_set_drvdata ( dev , rtc ); <nl>  <nl> return 0 ;
int pci_mmap_page_range ( struct pci_dev * dev , struct vm_area_struct * vma , <nl> */ <nl> prot |= _PAGE_CACHE_UC_MINUS ; <nl>  <nl> + prot |= _PAGE_IOMAP ; /* creating a mapping for IO */ <nl> + <nl> vma -> vm_page_prot = __pgprot ( prot ); <nl>  <nl> if ( io_remap_pfn_range ( vma , vma -> vm_start , vma -> vm_pgoff ,
static struct platform_driver gef_wdt_driver = { <nl> . of_match_table = gef_wdt_ids , <nl> }, <nl> . probe = gef_wdt_probe , <nl> + . remove = gef_wdt_remove , <nl> }; <nl>  <nl> static int __init gef_wdt_init ( void )
static int send_reply ( struct svcxprt_rdma * rdma , <nl> " svcrdma : could not post a receive buffer , err =% d ." <nl> " Closing transport % p .\ n ", ret , rdma ); <nl> set_bit ( XPT_CLOSE , & rdma -> sc_xprt . xpt_flags ); <nl> - return 0 ; <nl> + svc_rdma_put_context ( ctxt , 0 ); <nl> + return - ENOTCONN ; <nl> } <nl>  <nl> /* Prepare the context */
struct inode * ovl_d_select_inode ( struct dentry * dentry , unsigned file_flags ) <nl> ovl_path_upper ( dentry , & realpath ); <nl> } <nl>  <nl> + if ( realpath . dentry -> d_flags & DCACHE_OP_SELECT_INODE ) <nl> + return realpath . dentry -> d_op -> d_select_inode ( realpath . dentry , file_flags ); <nl> + <nl> return d_backing_inode ( realpath . dentry ); <nl> } <nl> 
static int cgroup_release_agent_show ( struct seq_file * seq , void * v ) <nl> { <nl> struct cgroup * cgrp = seq_css ( seq )-> cgroup ; <nl>  <nl> - if (! cgroup_lock_live_group ( cgrp )) <nl> - return - ENODEV ; <nl> + spin_lock (& release_agent_path_lock ); <nl> seq_puts ( seq , cgrp -> root -> release_agent_path ); <nl> + spin_unlock (& release_agent_path_lock ); <nl> seq_putc ( seq , '\ n '); <nl> - mutex_unlock (& cgroup_mutex ); <nl> return 0 ; <nl> } <nl> 
int clockevents_unbind_device ( struct clock_event_device * ced , int cpu ) <nl> mutex_unlock (& clockevents_mutex ); <nl> return ret ; <nl> } <nl> - EXPORT_SYMBOL_GPL ( clockevents_unbind ); <nl> + EXPORT_SYMBOL_GPL ( clockevents_unbind_device ); <nl>  <nl> /** <nl> * clockevents_register_device - register a clock event device
void ath9k_htc_rx_msg ( struct htc_target * htc_handle , <nl> return ; <nl> } <nl>  <nl> - if ( epid >= ENDPOINT_MAX ) { <nl> + if ( epid < 0 || epid >= ENDPOINT_MAX ) { <nl> if ( pipe_id != USB_REG_IN_PIPE ) <nl> dev_kfree_skb_any ( skb ); <nl> else
int irlan_extract_param ( __u8 * buf , char * name , char * value , __u16 * len ) <nl> memcpy (& val_len , buf + n , 2 ); /* To avoid alignment problems */ <nl> le16_to_cpus (& val_len ); n += 2 ; <nl>  <nl> - if ( val_len > 1016 ) { <nl> + if ( val_len >= 1016 ) { <nl> IRDA_DEBUG ( 2 , "% s (), parameter length to long \ n ", __func__ ); <nl> return - RSP_INVALID_COMMAND_FORMAT ; <nl> }
int ixgbe_ndo_set_vf_spoofchk ( struct net_device * netdev , int vf , bool setting ) <nl> struct ixgbe_hw * hw = & adapter -> hw ; <nl> u32 regval ; <nl>  <nl> + if ( vf >= adapter -> num_vfs ) <nl> + return - EINVAL ; <nl> + <nl> adapter -> vfinfo [ vf ]. spoofchk_enabled = setting ; <nl>  <nl> regval = IXGBE_READ_REG ( hw , IXGBE_PFVFSPOOF ( vf_target_reg ));
int read_log ( struct tpm_bios_log * log ) <nl> log -> bios_event_log_end = log -> bios_event_log + len ; <nl>  <nl> virt = acpi_os_map_memory ( start , len ); <nl> + if (! virt ) { <nl> + kfree ( log -> bios_event_log ); <nl> + printk ("% s : ERROR - Unable to map memory \ n ", __func__ ); <nl> + return - EIO ; <nl> + } <nl>  <nl> memcpy ( log -> bios_event_log , virt , len ); <nl> 
static int mmc_sdio_resume ( struct mmc_host * host ) <nl> mmc_claim_host ( host ); <nl> err = mmc_sdio_init_card ( host , host -> ocr , host -> card , <nl> ( host -> pm_flags & MMC_PM_KEEP_POWER )); <nl> + if (! err && host -> sdio_irqs ) <nl> + mmc_signal_sdio_irq ( host ); <nl> mmc_release_host ( host ); <nl>  <nl> /*
static void binder_transaction ( struct binder_proc * proc , <nl> proc -> pid , thread -> pid , <nl> ( u64 ) fp -> binder , node -> debug_id , <nl> ( u64 ) fp -> cookie , ( u64 ) node -> cookie ); <nl> + return_error = BR_FAILED_REPLY ; <nl> goto err_binder_get_ref_for_node_failed ; <nl> } <nl> ref = binder_get_ref_for_node ( target_proc , node );
int ip6_mr_input ( struct sk_buff * skb ) <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> cache = ip6mr_cache_find ( mrt ,
void dump_stack ( void ) <nl>  <nl> show_stack ( current , & stack ); <nl> } <nl> + EXPORT_SYMBOL ( dump_stack ); <nl>  <nl> void show_registers ( struct pt_regs * regs ) <nl> {
out_unlock : <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
static void ftdi_process_read ( struct work_struct * work ) <nl> spin_unlock_irqrestore (& priv -> rx_lock , flags ); <nl> dbg ("% s - deferring remainder until unthrottled ", <nl> __func__ ); <nl> - return ; <nl> + goto out ; <nl> } <nl> spin_unlock_irqrestore (& priv -> rx_lock , flags ); <nl> /* if the port is closed stop trying to read */
static int beiscsi_eh_device_reset ( struct scsi_cmnd * sc ) <nl> if (! abrt_task -> sc || abrt_task -> state == ISCSI_TASK_FREE ) <nl> continue ; <nl>  <nl> - if ( abrt_task -> sc -> device -> lun != abrt_task -> sc -> device -> lun ) <nl> + if ( sc -> device -> lun != abrt_task -> sc -> device -> lun ) <nl> continue ; <nl>  <nl> /* Invalidate WRB Posted for this Task */
void wl1271_tx_work_locked ( struct wl1271 * wl ) <nl>  <nl> /* if rates have changed , re - configure the rate policy */ <nl> if ( unlikely ( sta_rates )) { <nl> + ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + woken_up = true ; <nl> + <nl> wl -> rate_set = wl1271_tx_enabled_rates_get ( wl , sta_rates ); <nl> wl1271_acx_rate_policies ( wl ); <nl> }
void card_send_command ( struct ft1000_device * ft1000dev , void * ptempbuffer , <nl>  <nl> DEBUG (" card_send_command : enter card_send_command ... size =% d \ n ", size ); <nl>  <nl> - commandbuf = ( unsigned char *) kmalloc ( size + 2 , GFP_KERNEL ); <nl> + commandbuf = kmalloc ( size + 2 , GFP_KERNEL ); <nl> memcpy (( void *) commandbuf + 2 , ( void *) ptempbuffer , size ); <nl>  <nl> ft1000_read_register ( ft1000dev , & temp , FT1000_REG_DOORBELL );
static int hyp_init_cpu_notify ( struct notifier_block * self , <nl> switch ( action ) { <nl> case CPU_STARTING : <nl> case CPU_STARTING_FROZEN : <nl> - cpu_init_hyp_mode ( NULL ); <nl> + if ( __hyp_get_vectors () == hyp_default_vectors ) <nl> + cpu_init_hyp_mode ( NULL ); <nl> break ; <nl> } <nl> 
static int gfar_spauseparam ( struct net_device * dev , <nl> struct gfar __iomem * regs = priv -> gfargrp [ 0 ]. regs ; <nl> u32 oldadv , newadv ; <nl>  <nl> + if (! phydev ) <nl> + return - ENODEV ; <nl> + <nl> if (!( phydev -> supported & SUPPORTED_Pause ) || <nl> (!( phydev -> supported & SUPPORTED_Asym_Pause ) && <nl> ( epause -> rx_pause != epause -> tx_pause )))
static int pmic_irq_type ( unsigned irq , unsigned type ) <nl> u32 gpio = irq - pg -> irq_base ; <nl> unsigned long flags ; <nl>  <nl> - if ( gpio > pg -> chip . ngpio ) <nl> + if ( gpio >= pg -> chip . ngpio ) <nl> return - EINVAL ; <nl>  <nl> spin_lock_irqsave (& pg -> irqtypes . lock , flags );
int rt2x00mac_set_key ( struct ieee80211_hw * hw , enum set_key_cmd cmd , <nl> crypto . cipher = rt2x00crypto_key_to_cipher ( key ); <nl> if ( crypto . cipher == CIPHER_NONE ) <nl> return - EOPNOTSUPP ; <nl> + if ( crypto . cipher == CIPHER_TKIP && rt2x00_is_usb ( rt2x00dev )) <nl> + return - EOPNOTSUPP ; <nl>  <nl> crypto . cmd = cmd ; <nl> 
int ide_noacpitfs = 1 ; <nl> int ide_noacpionboot = 1 ; <nl> # endif <nl>  <nl> -/* <nl> - * This is declared extern in ide . h , for access by other IDE modules : <nl> - */ <nl> ide_hwif_t ide_hwifs [ MAX_HWIFS ]; /* master data repository */ <nl>  <nl> - EXPORT_SYMBOL ( ide_hwifs ); <nl> - <nl> static void ide_port_init_devices_data ( ide_hwif_t *); <nl>  <nl> /*
static int pcnet32_open ( struct net_device * dev ) <nl> lp -> rx_dma_addr [ i ] = 0 ; <nl> } <nl>  <nl> - pcnet32_free_ring ( dev ); <nl> - <nl> /* <nl> * Switch back to 16bit mode to avoid problems with dumb <nl> * DOS packet driver after a warm reboot
static int sep_get_time_handler ( unsigned long arg ) <nl> struct sep_driver_get_time_t command_args ; <nl>  <nl> error = sep_set_time (& command_args . time_physical_address , & command_args . time_value ); <nl> - error = copy_to_user (( void *) arg , ( void *) & command_args , sizeof ( struct sep_driver_get_time_t )); <nl> + if ( error == 0 ) <nl> + error = copy_to_user (( void __user *) arg , <nl> + & command_args , sizeof ( struct sep_driver_get_time_t )); <nl> return error ; <nl>  <nl> }
void hists__output_recalc_col_len ( struct hists * hists , int max_rows ) <nl>  <nl> while ( next && row ++ < max_rows ) { <nl> n = rb_entry ( next , struct hist_entry , rb_node ); <nl> - hists__calc_col_len ( hists , n ); <nl> + if (! n -> filtered ) <nl> + hists__calc_col_len ( hists , n ); <nl> next = rb_next (& n -> rb_node ); <nl> } <nl> }
out : <nl> if ( rc != MIGRATEPAGE_SUCCESS && put_new_page ) <nl> put_new_page ( new_hpage , private ); <nl> else <nl> - put_page ( new_hpage ); <nl> + putback_active_hugepage ( new_hpage ); <nl>  <nl> if ( result ) { <nl> if ( rc )
dma_addr_t xhci_trb_virt_to_dma ( struct xhci_segment * seg , <nl> return 0 ; <nl> /* offset in TRBs */ <nl> segment_offset = trb - seg -> trbs ; <nl> - if ( segment_offset > TRBS_PER_SEGMENT ) <nl> + if ( segment_offset >= TRBS_PER_SEGMENT ) <nl> return 0 ; <nl> return seg -> dma + ( segment_offset * sizeof (* trb )); <nl> }
void sctp_transport_lower_cwnd ( struct sctp_transport * transport , <nl> transport -> ssthresh = max ( transport -> cwnd / 2 , <nl> 4 * transport -> asoc -> pathmtu ); <nl> transport -> cwnd = transport -> asoc -> pathmtu ; <nl> + <nl> + /* T3 - rtx also clears fast recovery on the transport */ <nl> + transport -> fast_recovery = 0 ; <nl> break ; <nl>  <nl> case SCTP_LOWER_CWND_FAST_RTX :
static void cirrus_connector_destroy ( struct drm_connector * connector ) <nl> kfree ( connector ); <nl> } <nl>  <nl> - struct drm_connector_helper_funcs cirrus_vga_connector_helper_funcs = { <nl> + static const struct drm_connector_helper_funcs cirrus_vga_connector_helper_funcs = { <nl> . get_modes = cirrus_vga_get_modes , <nl> . best_encoder = cirrus_connector_best_encoder , <nl> }; <nl>  <nl> - struct drm_connector_funcs cirrus_vga_connector_funcs = { <nl> + static const struct drm_connector_funcs cirrus_vga_connector_funcs = { <nl> . dpms = drm_helper_connector_dpms , <nl> . detect = cirrus_vga_detect , <nl> . fill_modes = drm_helper_probe_single_connector_modes ,
static unsigned long ramfs_nommu_get_unmapped_area ( struct file * file , <nl>  <nl> /* gang - find the pages */ <nl> ret = - ENOMEM ; <nl> - pages = kzalloc ( lpages * sizeof ( struct page *), GFP_KERNEL ); <nl> + pages = kcalloc ( lpages , sizeof ( struct page *), GFP_KERNEL ); <nl> if (! pages ) <nl> goto out_free ; <nl> 
static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> } <nl> logical = re -> logical ; <nl>  <nl> - reada_extent_put ( fs_info , re ); <nl> - <nl> atomic_inc (& dev -> reada_in_flight ); <nl> ret = reada_tree_block_flagged ( fs_info -> extent_root , logical , <nl> mirror_num , & eb ); <nl> static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> if ( eb ) <nl> free_extent_buffer ( eb ); <nl>  <nl> + reada_extent_put ( fs_info , re ); <nl> + <nl> return 1 ; <nl>  <nl> }
static int ethtool_phys_id ( struct net_device * dev , void __user * useraddr ) <nl> if ( rc == 0 ) { <nl> /* Driver will handle this itself */ <nl> schedule_timeout_interruptible ( <nl> - id . data ? id . data : MAX_SCHEDULE_TIMEOUT ); <nl> + id . data ? ( id . data * HZ ) : MAX_SCHEDULE_TIMEOUT ); <nl> } else { <nl> /* Driver expects to be called periodically */ <nl> do {
static int scsi_eh_completed_normally ( struct scsi_cmnd * scmd ) <nl> scsi_handle_queue_full ( scmd -> device ); <nl> /* fall through */ <nl> case BUSY : <nl> + return NEEDS_RETRY ; <nl> default : <nl> return FAILED ; <nl> }
static int econet_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> if ( peer ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset ( sec , 0 , sizeof (* sec )); <nl> mutex_lock (& econet_mutex ); <nl>  <nl> sk = sock -> sk ;
void hugetlb_unreserve_pages ( struct inode * inode , long offset , long freed ) <nl> long chg = region_truncate (& inode -> i_mapping -> private_list , offset ); <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> - inode -> i_blocks -= blocks_per_huge_page ( h ); <nl> + inode -> i_blocks -= ( blocks_per_huge_page ( h ) * freed ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> hugetlb_put_quota ( inode -> i_mapping , ( chg - freed ));
int __init main ( int argc , char ** argv , char ** envp ) <nl> # endif <nl>  <nl> do_uml_initcalls (); <nl> + change_sig ( SIGPIPE , 0 ); <nl> ret = linux_main ( argc , argv ); <nl>  <nl> /*
ath_reg_apply_active_scan_flags ( struct wiphy * wiphy , <nl> int r ; <nl>  <nl> sband = wiphy -> bands [ IEEE80211_BAND_2GHZ ]; <nl> + if (! sband ) <nl> + return ; <nl>  <nl> /* <nl> * If no country IE has been received always enable active scan
static int omap3_onenand_read_bufferram ( struct mtd_info * mtd , int area , <nl> if ( bram_offset & 3 || ( size_t ) buf & 3 || count < 384 ) <nl> goto out_copy ; <nl>  <nl> + /* panic_write () may be in an interrupt context */ <nl> + if ( in_interrupt ()) <nl> + goto out_copy ; <nl> + <nl> if ( buf >= high_memory ) { <nl> struct page * p1 ; <nl> 
static void __init xen_rebuild_p2m_list ( unsigned long * p2m ) <nl> p2m_missing_pte : p2m_identity_pte ; <nl> for ( i = 0 ; i < PMDS_PER_MID_PAGE ; i ++) { <nl> pmdp = populate_extra_pmd ( <nl> - ( unsigned long )( p2m + pfn + i * PTRS_PER_PTE )); <nl> + ( unsigned long )( p2m + pfn ) + i * PMD_SIZE ); <nl> set_pmd ( pmdp , __pmd ( __pa ( ptep ) | _KERNPG_TABLE )); <nl> } <nl> }
static int vfio_set_trigger ( struct vfio_platform_device * vdev , int index , <nl> int ret ; <nl>  <nl> if ( irq -> trigger ) { <nl> + irq_clear_status_flags ( irq -> hwirq , IRQ_NOAUTOEN ); <nl> free_irq ( irq -> hwirq , irq ); <nl> kfree ( irq -> name ); <nl> eventfd_ctx_put ( irq -> trigger );
int max1363_single_channel_from_ring ( long mask , struct max1363_state * st ) <nl> ret = - EBUSY ; <nl> goto error_ret ; <nl> } <nl> - numvals = hweight_long ( st -> current_mode -> modemask ); <nl>  <nl> - ring_data = kmalloc ( numvals * 2 , GFP_KERNEL ); <nl> + ring_data = kmalloc ( ring -> access . get_bytes_per_datum ( ring ), GFP_KERNEL ); <nl> if ( ring_data == NULL ) { <nl> ret = - ENOMEM ; <nl> goto error_ret ;
void kvm_ioapic_calculate_eoi_exitmap ( struct kvm_vcpu * vcpu , <nl> if (! e -> fields . mask && <nl> ( e -> fields . trig_mode == IOAPIC_LEVEL_TRIG || <nl> kvm_irq_has_notifier ( ioapic -> kvm , KVM_IRQCHIP_IOAPIC , <nl> - index ))) { <nl> + index ) || index == RTC_GSI )) { <nl> if ( kvm_apic_match_dest ( vcpu , NULL , 0 , <nl> e -> fields . dest_id , e -> fields . dest_mode )) <nl> __set_bit ( e -> fields . vector , ( unsigned long *) eoi_exit_bitmap );
DECLARE_EVENT_CLASS ( xhci_log_event , <nl> __field ( u64 , dma ) <nl> __field ( u32 , status ) <nl> __field ( u32 , flags ) <nl> - __dynamic_array ( __le32 , trb , 4 ) <nl> + __dynamic_array ( u8 , trb , sizeof ( struct xhci_generic_trb )) <nl> ), <nl> TP_fast_assign ( <nl> __entry -> va = trb_va ;
static int validate_region_size ( struct raid_set * rs , unsigned long region_size ) <nl> static int validate_raid_redundancy ( struct raid_set * rs ) <nl> { <nl> unsigned i , rebuild_cnt = 0 ; <nl> - unsigned rebuilds_per_group , copies , d ; <nl> + unsigned rebuilds_per_group = 0 , copies , d ; <nl> unsigned group_size , last_group_start ; <nl>  <nl> for ( i = 0 ; i < rs -> md . raid_disks ; i ++)
retry : <nl> ! atomic_inc_not_zero (& ctx -> refcount )) { <nl> raw_spin_unlock (& ctx -> lock ); <nl> ctx = NULL ; <nl> + } else { <nl> + WARN_ON_ONCE ( ctx -> task != task ); <nl> } <nl> - <nl> - WARN_ON_ONCE ( ctx -> task != task ); <nl> } <nl> rcu_read_unlock (); <nl> if (! ctx )
static void tpacket_destruct_skb ( struct sk_buff * skb ) <nl> struct packet_sock * po = pkt_sk ( skb -> sk ); <nl> void * ph ; <nl>  <nl> - BUG_ON ( skb == NULL ); <nl> - <nl> if ( likely ( po -> tx_ring . pg_vec )) { <nl> ph = skb_shinfo ( skb )-> destructor_arg ; <nl> BUG_ON ( __packet_get_status ( po , ph ) != TP_STATUS_SENDING );
static int crypto_ccm_auth ( struct aead_request * req , struct scatterlist * plain , <nl> if ( assoclen ) { <nl> pctx -> ilen = format_adata ( idata , assoclen ); <nl> get_data_to_compute ( cipher , pctx , req -> assoc , req -> assoclen ); <nl> + } else { <nl> + pctx -> ilen = 0 ; <nl> } <nl>  <nl> /* compute plaintext into mac */
int vnt_init ( struct vnt_private * priv ) <nl>  <nl> priv -> mac_hw = true ; <nl>  <nl> + vnt_radio_power_off ( priv ); <nl> + <nl> return 0 ; <nl> } <nl> 
int eprintf ( int level , const char * fmt , ...) <nl>  <nl> if ( verbose >= level ) { <nl> va_start ( args , fmt ); <nl> - if ( use_browser > 1 ) <nl> + if ( use_browser >= 1 ) <nl> ui_helpline__vshow ( fmt , args ); <nl> else <nl> ret = vfprintf ( stderr , fmt , args );
static void sbp2_prep_command_orb_sg ( struct sbp2_command_orb * orb , <nl>  <nl> /* loop through and fill out our SBP - 2 page tables <nl> * ( and split up anything too large ) */ <nl> - for ( i = 0 , sg_count = 0 ; i < count ; i ++, sgpnt ++) { <nl> + for ( i = 0 , sg_count = 0 ; i < count ; i ++, sgpnt = sg_next ( sgpnt )) { <nl> sg_len = sg_dma_len ( sgpnt ); <nl> sg_addr = sg_dma_address ( sgpnt ); <nl> while ( sg_len ) {
scsi_internal_device_unblock ( struct scsi_device * sdev , <nl> * Try to transition the scsi device to SDEV_RUNNING or one of the <nl> * offlined states and goose the device queue if successful . <nl> */ <nl> - if ( sdev -> sdev_state == SDEV_BLOCK ) <nl> + if (( sdev -> sdev_state == SDEV_BLOCK ) || <nl> + ( sdev -> sdev_state == SDEV_TRANSPORT_OFFLINE )) <nl> sdev -> sdev_state = new_state ; <nl> else if ( sdev -> sdev_state == SDEV_CREATED_BLOCK ) { <nl> if ( new_state == SDEV_TRANSPORT_OFFLINE ||
void ixgbe_update_stats ( struct ixgbe_adapter * adapter ) <nl> u32 i , missed_rx = 0 , mpc , bprc , lxon , lxoff , xon_off_tot ; <nl> u64 non_eop_descs = 0 , restart_queue = 0 ; <nl>  <nl> + if ( test_bit ( __IXGBE_DOWN , & adapter -> state ) || <nl> + test_bit ( __IXGBE_RESETTING , & adapter -> state )) <nl> + return ; <nl> + <nl> if ( adapter -> flags2 & IXGBE_FLAG2_RSC_ENABLED ) { <nl> u64 rsc_count = 0 ; <nl> u64 rsc_flush = 0 ;
__perf_counter_exit_task ( struct task_struct * child , <nl> } <nl> } <nl>  <nl> - kfree ( child_counter ); <nl> + if (! child_counter -> filp || ! atomic_long_read (& child_counter -> filp -> f_count )) <nl> + kfree ( child_counter ); <nl> } <nl>  <nl> /*
DEFINE_PER_CPU_READ_MOSTLY ( cpumask_var_t , cpu_llc_shared_map ); <nl> DEFINE_PER_CPU_SHARED_ALIGNED ( struct cpuinfo_x86 , cpu_info ); <nl> EXPORT_PER_CPU_SYMBOL ( cpu_info ); <nl>  <nl> - static DEFINE_PER_CPU ( struct completion , die_complete ); <nl> - <nl> atomic_t init_deasserted ; <nl>  <nl> /* <nl> void cpu_disable_common ( void ) <nl> fixup_irqs (); <nl> } <nl>  <nl> + static DEFINE_PER_CPU ( struct completion , die_complete ); <nl> + <nl> int native_cpu_disable ( void ) <nl> { <nl> int ret ;
int iwl_mvm_sched_scan_start ( struct iwl_mvm * mvm , <nl> return - EBUSY ; <nl> } <nl>  <nl> + /* we don ' t support " match all " in the firmware */ <nl> + if (! req -> n_match_sets ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> ret = iwl_mvm_check_running_scans ( mvm , type ); <nl> if ( ret ) <nl> return ret ;
static int __init wb_module_init ( void ) <nl>  <nl> err = map_bios (); <nl> if ( err ) <nl> - return err ; <nl> + goto err_free_keymap ; <nl>  <nl> err = platform_driver_register (& wistron_driver ); <nl> if ( err ) <nl> static int __init wb_module_init ( void ) <nl> platform_driver_unregister (& wistron_driver ); <nl> err_unmap_bios : <nl> unmap_bios (); <nl> + err_free_keymap : <nl> + kfree ( keymap ); <nl>  <nl> return err ; <nl> }
static unsigned long cfq_slice_offset ( struct cfq_data * cfqd , <nl> /* <nl> * just an approximation , should be ok . <nl> */ <nl> - return (( cfqd -> busy_queues - 1 ) * cfq_prio_slice ( cfqd , 1 , 0 )); <nl> + return ( cfqd -> busy_queues - 1 ) * ( cfq_prio_slice ( cfqd , 1 , 0 ) - <nl> + cfq_prio_slice ( cfqd , cfq_cfqq_sync ( cfqq ), cfqq -> ioprio )); <nl> } <nl>  <nl> /*
static IIO_DEV_ATTR_SAMP_FREQ ( S_IWUSR | S_IRUGO , <nl> adis16400_read_frequency , <nl> adis16400_write_frequency ); <nl>  <nl> - static IIO_CONST_ATTR_SAMP_FREQ_AVAIL (" 409 546 819 1638 "); <nl> - <nl> static const u8 adis16400_addresses [] = { <nl> [ ADIS16400_SCAN_GYRO_X ] = ADIS16400_XGYRO_OFF , <nl> [ ADIS16400_SCAN_GYRO_Y ] = ADIS16400_YGYRO_OFF , <nl> static const struct iio_chan_spec adis16334_channels [] = { <nl>  <nl> static struct attribute * adis16400_attributes [] = { <nl> & iio_dev_attr_sampling_frequency . dev_attr . attr , <nl> - & iio_const_attr_sampling_frequency_available . dev_attr . attr , <nl> NULL <nl> }; <nl> 
static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
static void tg3_timer ( unsigned long __opaque ) <nl> * resets . <nl> */ <nl> if (!-- tp -> asf_counter ) { <nl> - if ( tp -> tg3_flags & TG3_FLAG_ENABLE_ASF ) { <nl> + if (( tp -> tg3_flags & TG3_FLAG_ENABLE_ASF ) && <nl> + !( tp -> tg3_flags3 & TG3_FLG3_ENABLE_APE )) { <nl> u32 val ; <nl>  <nl> tg3_wait_for_event_ack ( tp );
static int zoran_dqbuf ( struct file * file , void * __fh , struct v4l2_buffer * buf ) <nl> res = - EAGAIN ; <nl> goto dqbuf_unlock_and_return ; <nl> } <nl> + bs . frame = 0 ; /* suppress compiler warning */ <nl> res = jpg_sync ( fh , & bs ); <nl> if ( res ) <nl> goto dqbuf_unlock_and_return ;
static irqreturn_t intel_sst_interrupt ( int irq , void * context ) <nl> unsigned int size = 0 , str_id ; <nl> struct stream_info * stream ; <nl>  <nl> + /* Do not handle interrupt in suspended state */ <nl> + if ( drv -> sst_state == SST_SUSPENDED ) <nl> + return IRQ_NONE ; <nl> /* Interrupt arrived , check src */ <nl> isr . full = sst_shim_read ( drv -> shim , SST_ISRX ); <nl> 
static unsigned long iommu_range_alloc ( struct iommu_table * tbl , <nl> /* This allocator was derived from x86_64 ' s bit string search */ <nl>  <nl> /* Sanity check */ <nl> - if ( unlikely ( npages ) == 0 ) { <nl> + if ( unlikely ( npages == 0 )) { <nl> if ( printk_ratelimit ()) <nl> WARN_ON ( 1 ); <nl> return DMA_ERROR_CODE ;
static int era_is_congested ( struct dm_target_callbacks * cb , int bdi_bits ) <nl>  <nl> static void era_destroy ( struct era * era ) <nl> { <nl> - metadata_close ( era -> md ); <nl> + if ( era -> md ) <nl> + metadata_close ( era -> md ); <nl>  <nl> if ( era -> wq ) <nl> destroy_workqueue ( era -> wq );
__xfrm4_bundle_create ( struct xfrm_policy * policy , struct xfrm_state ** xfrm , int <nl> afinfo = xfrm_state_get_afinfo ( dst_prev -> xfrm -> props . family ); <nl> if (! afinfo ) { <nl> dst = * dst_p ; <nl> + err = - EAFNOSUPPORT ; <nl> goto error ; <nl> } <nl> dst_prev -> output = afinfo -> output ;
store_priv_session_ ## field ( struct device * dev , \ <nl> # define iscsi_priv_session_rw_attr ( field , format ) \ <nl> iscsi_priv_session_attr_show ( field , format ) \ <nl> iscsi_priv_session_attr_store ( field ) \ <nl> - static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUGO , \ <nl> + static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUSR , \ <nl> show_priv_session_ ## field , \ <nl> store_priv_session_ ## field ) <nl> iscsi_priv_session_rw_attr ( recovery_tmo , "% d ");
static int dr_interception ( struct vcpu_svm * svm ) <nl> kvm_register_write (& svm -> vcpu , reg , val ); <nl> } <nl>  <nl> + skip_emulated_instruction (& svm -> vcpu ); <nl> + <nl> return 1 ; <nl> } <nl> 
EXPORT_SYMBOL ( of_find_node_with_property ); <nl> const struct of_device_id * of_match_node ( const struct of_device_id * matches , <nl> const struct device_node * node ) <nl> { <nl> + if (! matches ) <nl> + return NULL ; <nl> + <nl> while ( matches -> name [ 0 ] || matches -> type [ 0 ] || matches -> compatible [ 0 ]) { <nl> int match = 1 ; <nl> if ( matches -> name [ 0 ])
static void pxa3xx_nand_cmdfunc ( struct mtd_info * mtd , unsigned command , <nl> /* disable HW ECC to get all the OOB data */ <nl> info -> buf_count = mtd -> writesize + mtd -> oobsize ; <nl> info -> buf_start = mtd -> writesize + column ; <nl> + memset ( info -> data_buff , 0xFF , info -> buf_count ); <nl>  <nl> if ( prepare_read_prog_cmd ( info , cmdset -> read1 , column , page_addr )) <nl> break ;
static struct fileIdentDesc * udf_find_entry ( struct inode * dir , <nl> } <nl>  <nl> if (( cfi -> fileCharacteristics & FID_FILE_CHAR_PARENT ) && <nl> - isdotdot ) { <nl> - brelse ( epos . bh ); <nl> - return fi ; <nl> - } <nl> + isdotdot ) <nl> + goto out_ok ; <nl>  <nl> if (! lfi ) <nl> continue ;
int sctp_sysctl_net_register ( struct net * net ) <nl> table [ i ]. data += ( char *)(& net -> sctp ) - ( char *)& init_net . sctp ; <nl>  <nl> net -> sctp . sysctl_header = register_net_sysctl ( net , " net / sctp ", table ); <nl> + if ( net -> sctp . sysctl_header == NULL ) { <nl> + kfree ( table ); <nl> + return - ENOMEM ; <nl> + } <nl> return 0 ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> + if ( ret != sizeof ( event )) { <nl> + printf (" Reading event failed !\ n "); <nl> + ret = - EIO ; <nl> + break ; <nl> + } <nl> + <nl> print_event (& event ); <nl> } <nl> 
static int mc13783_probe ( struct snd_soc_codec * codec ) <nl> { <nl> struct mc13783_priv * priv = snd_soc_codec_get_drvdata ( codec ); <nl>  <nl> - codec -> control_data = priv -> mc13xxx ; <nl> - <nl> mc13xxx_lock ( priv -> mc13xxx ); <nl>  <nl> /* these are the reset values */
static const struct attribute_group isl29108_group = { <nl> static int isl29018_chip_init ( struct isl29018_chip * chip ) <nl> { <nl> int status ; <nl> - int new_adc_bit ; <nl> + unsigned int new_adc_bit ; <nl> unsigned int new_range ; <nl>  <nl> /* Code added per Intersil Application Note 1534 :
static int ov7670_read ( struct i2c_client * c , unsigned char reg , <nl> int ret ; <nl>  <nl> ret = i2c_smbus_read_byte_data ( c , reg ); <nl> - if ( ret >= 0 ) <nl> + if ( ret >= 0 ) { <nl> * value = ( unsigned char ) ret ; <nl> + ret = 0 ; <nl> + } <nl> return ret ; <nl> } <nl> 
static ssize_t mtd_write ( struct file * file , const char __user * buf , size_t count <nl> ops . mode = MTD_OOB_RAW ; <nl> ops . datbuf = kbuf ; <nl> ops . oobbuf = NULL ; <nl> + ops . ooboffs = 0 ; <nl> ops . len = len ; <nl>  <nl> ret = mtd -> write_oob ( mtd , * ppos , & ops );
struct ion_device * ion_device_create ( long (* custom_ioctl ) <nl> ret = misc_register (& idev -> dev ); <nl> if ( ret ) { <nl> pr_err (" ion : failed to register misc device .\ n "); <nl> + kfree ( idev ); <nl> return ERR_PTR ( ret ); <nl> } <nl> 
static enum odd_mech_type zpodd_get_mech_type ( struct ata_device * dev ) <nl> static bool odd_can_poweroff ( struct ata_device * ata_dev ) <nl> { <nl> acpi_handle handle ; <nl> - acpi_status status ; <nl> struct acpi_device * acpi_dev ; <nl>  <nl> handle = ata_dev_acpi_handle ( ata_dev ); <nl> if (! handle ) <nl> return false ; <nl>  <nl> - status = acpi_bus_get_device ( handle , & acpi_dev ); <nl> - if ( ACPI_FAILURE ( status )) <nl> + if ( acpi_bus_get_device ( handle , & acpi_dev )) <nl> return false ; <nl>  <nl> return acpi_device_can_poweroff ( acpi_dev );
static int corsair_input_mapping ( struct hid_device * dev , <nl> { <nl> int gkey ; <nl>  <nl> + if (( usage -> hid & HID_USAGE_PAGE ) != HID_UP_KEYBOARD ) <nl> + return 0 ; <nl> + <nl> gkey = corsair_usage_to_gkey ( usage -> hid & HID_USAGE ); <nl> if ( gkey != 0 ) { <nl> hid_map_usage_clear ( input , usage , bit , max , EV_KEY ,
remove_write ( struct device_driver * drv , const char * buf , size_t count ) <nl> count = IFNAMSIZ - 1 ; <nl>  <nl> for ( i = 0 , p =( char *) buf ; i < count && * p ; i ++, p ++) { <nl> - if ((* p == '\ n ') | (* p == ' ')) { <nl> + if ((* p == '\ n ') || (* p == ' ')) { <nl> /* trailing lf , grr */ <nl> break ; <nl> } else {
void radeon_compute_pll ( struct radeon_pll * pll , <nl> * frac_fb_div_p = best_frac_feedback_div ; <nl> * ref_div_p = best_ref_div ; <nl> * post_div_p = best_post_div ; <nl> + DRM_DEBUG_KMS ("% d % d , pll dividers - fb : % d .% d ref : % d , post % d \ n ", <nl> + freq , best_freq / 1000 , best_feedback_div , best_frac_feedback_div , <nl> + best_ref_div , best_post_div ); <nl> + <nl> } <nl>  <nl> static void radeon_user_framebuffer_destroy ( struct drm_framebuffer * fb )
static __devinit int vpbe_probe ( struct platform_device * pdev ) <nl>  <nl> if ( cfg -> outputs -> num_modes > 0 ) <nl> vpbe_dev -> current_timings = vpbe_dev -> cfg -> outputs [ 0 ]. modes [ 0 ]; <nl> - else <nl> + else { <nl> + kfree ( vpbe_dev ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> /* set the driver data in platform device */ <nl> platform_set_drvdata ( pdev , vpbe_dev );
static int bnx2x_init_dev ( struct bnx2x * bp , struct pci_dev * pdev , <nl> pci_write_config_dword ( bp -> pdev , PCICFG_GRC_ADDRESS , <nl> PCICFG_VENDOR_ID_OFFSET ); <nl>  <nl> + /* Set PCIe reset type to fundamental for EEH recovery */ <nl> + pdev -> needs_freset = 1 ; <nl> + <nl> /* AER ( Advanced Error reporting ) configuration */ <nl> rc = pci_enable_pcie_error_reporting ( pdev ); <nl> if (! rc )
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
static int intel_crtc_page_flip ( struct drm_crtc * crtc , <nl> to_intel_plane ( primary )-> frontbuffer_bit ); <nl>  <nl> intel_fbc_disable ( dev ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> intel_frontbuffer_flip_prepare ( dev , <nl> to_intel_plane ( primary )-> frontbuffer_bit ); <nl> - mutex_unlock (& dev -> struct_mutex ); <nl>  <nl> trace_i915_flip_request ( intel_crtc -> plane , obj ); <nl> 
befs_fill_super ( struct super_block * sb , void * data , int silent ) <nl> brelse ( bh ); <nl>  <nl> unacquire_priv_sbp : <nl> + kfree ( befs_sb -> mount_opts . iocharset ); <nl> kfree ( sb -> s_fs_info ); <nl>  <nl> unacquire_none :
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static int hdmi_pcm_open ( struct hda_pcm_stream * hinfo , <nl> * codec_pars = * hinfo ; <nl>  <nl> eld = & spec -> sink_eld [ idx ]; <nl> - if ( eld -> sad_count > 0 ) { <nl> + if ( eld -> eld_valid && eld -> sad_count > 0 ) { <nl> hdmi_eld_update_pcm_info ( eld , hinfo , codec_pars ); <nl> if ( hinfo -> channels_min > hinfo -> channels_max || <nl> ! hinfo -> rates || ! hinfo -> formats )
# define SEEK_MAX SEEK_END <nl>  <nl> struct fstrim_range { <nl> - uint64_t start ; <nl> - uint64_t len ; <nl> - uint64_t minlen ; <nl> + __u64 start ; <nl> + __u64 len ; <nl> + __u64 minlen ; <nl> }; <nl>  <nl> /* And dynamically - tunable limits and defaults : */
static int do_end_io ( struct multipath * m , struct request * clone , <nl> if (! error && ! clone -> errors ) <nl> return 0 ; /* I / O complete */ <nl>  <nl> - if ( error == - EOPNOTSUPP || error == - EREMOTEIO ) <nl> + if ( error == - EOPNOTSUPP || error == - EREMOTEIO || error == - EILSEQ ) <nl> return error ; <nl>  <nl> if ( mpio -> pgpath )
static int m5mols_s_stream ( struct v4l2_subdev * sd , int enable ) <nl> if ( enable ) { <nl> if ( is_code ( code , M5MOLS_RESTYPE_MONITOR )) <nl> ret = m5mols_start_monitor ( info ); <nl> - if ( is_code ( code , M5MOLS_RESTYPE_CAPTURE )) <nl> + else if ( is_code ( code , M5MOLS_RESTYPE_CAPTURE )) <nl> ret = m5mols_start_capture ( info ); <nl> else <nl> ret = - EINVAL ;
static int snd_hdsp_get_adat_sync_check ( struct snd_kcontrol * kcontrol , struct sn <nl> struct hdsp * hdsp = snd_kcontrol_chip ( kcontrol ); <nl>  <nl> offset = ucontrol -> id . index - 1 ; <nl> - snd_BUG_ON ( offset < 0 ); <nl> + if ( snd_BUG_ON ( offset < 0 )) <nl> + return - EINVAL ; <nl>  <nl> switch ( hdsp -> io_type ) { <nl> case Digiface :
int snd_hda_parse_pin_def_config ( struct hda_codec * codec , <nl> cfg -> input_pins [ AUTO_PIN_AUX ] = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_OUT : <nl> + case AC_JACK_DIG_OTHER_OUT : <nl> cfg -> dig_out_pin = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_IN : <nl> + case AC_JACK_DIG_OTHER_IN : <nl> cfg -> dig_in_pin = nid ; <nl> break ; <nl> }
static const struct mfd_cell s5m8767_devs [] = { <nl> static const struct mfd_cell s2mps11_devs [] = { <nl> { <nl> . name = " s2mps11 - pmic ", <nl> + }, { <nl> + . name = " s2mps14 - rtc ", <nl> }, { <nl> . name = " s2mps11 - clk ", <nl> . of_compatible = " samsung , s2mps11 - clk ",
out_unlock : <nl> * because the core might be gone away while we unlocked the mutex . */ <nl> static struct b43_wldev * b43_wireless_core_stop ( struct b43_wldev * dev ) <nl> { <nl> - struct b43_wl * wl = dev -> wl ; <nl> + struct b43_wl * wl ; <nl> struct b43_wldev * orig_dev ; <nl> u32 mask ; <nl>  <nl> + if (! dev ) <nl> + return NULL ; <nl> + wl = dev -> wl ; <nl> redo : <nl> if (! dev || b43_status ( dev ) < B43_STAT_STARTED ) <nl> return dev ;
extern int scsi_execute_async ( struct scsi_device * sdev , <nl> void (* done )( void *, char *, int , int ), <nl> gfp_t gfp ); <nl>  <nl> - static inline void scsi_device_reprobe ( struct scsi_device * sdev ) <nl> + static inline int __must_check scsi_device_reprobe ( struct scsi_device * sdev ) <nl> { <nl> - device_reprobe (& sdev -> sdev_gendev ); <nl> + return device_reprobe (& sdev -> sdev_gendev ); <nl> } <nl>  <nl> static inline unsigned int sdev_channel ( struct scsi_device * sdev )
int regmap_register_patch ( struct regmap * map , const struct reg_default * regs , <nl> int i , ret ; <nl> bool bypass ; <nl>  <nl> + if ( WARN_ONCE ( num_regs <= 0 , " invalid registers number (% d )\ n ", <nl> + num_regs )) <nl> + return 0 ; <nl> + <nl> map -> lock ( map -> lock_arg ); <nl>  <nl> bypass = map -> cache_bypass ;
int __init ip_rt_init ( void ) <nl> 0 , <nl> & rt_hash_log , <nl> & rt_hash_mask , <nl> - 0 ); <nl> + rhash_entries ? 0 : 512 * 1024 ); <nl> memset ( rt_hash_table , 0 , ( rt_hash_mask + 1 ) * sizeof ( struct rt_hash_bucket )); <nl> rt_hash_lock_init (); <nl> 
static dma_cookie_t imxdma_tx_submit ( struct dma_async_tx_descriptor * tx ) <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& imxdma -> lock , flags ); <nl> + list_move_tail ( imxdmac -> ld_free . next , & imxdmac -> ld_queue ); <nl> cookie = dma_cookie_assign ( tx ); <nl> spin_unlock_irqrestore (& imxdma -> lock , flags ); <nl> 
int snd_hda_input_jack_add ( struct hda_codec * codec , hda_nid_t nid , int type , <nl> err = snd_jack_new ( codec -> bus -> card , name , type , & jack -> jack ); <nl> if ( err < 0 ) <nl> return err ; <nl> + jack -> type = type ; <nl> jack -> jack -> private_data = jack ; <nl> jack -> jack -> private_free = hda_free_jack_priv ; <nl> return 0 ;
static void parse_dacl ( struct cifs_acl * pdacl , char * end_of_acl , <nl> umode_t group_mask = S_IRWXG ; <nl> umode_t other_mask = S_IRWXU | S_IRWXG | S_IRWXO ; <nl>  <nl> + if ( num_aces > ULONG_MAX / sizeof ( struct cifs_ace *)) <nl> + return ; <nl> ppace = kmalloc ( num_aces * sizeof ( struct cifs_ace *), <nl> GFP_KERNEL ); <nl> if (! ppace ) {
out : <nl>  <nl> static int jfs_ci_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> { <nl> - if ( nd -> flags & LOOKUP_RCU ) <nl> + if ( nd && nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl> /* <nl> * This is not negative dentry . Always valid .
static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
static int igb_set_eee ( struct net_device * netdev , <nl> ( hw -> phy . media_type != e1000_media_type_copper )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset (& eee_curr , 0 , sizeof ( struct ethtool_eee )); <nl> + <nl> ret_val = igb_get_eee ( netdev , & eee_curr ); <nl> if ( ret_val ) <nl> return ret_val ;
cputime_to_timeval ( const cputime_t cputime , struct timeval * value ) <nl> value -> tv_usec = rp . subreg . even / 4096 ; <nl> value -> tv_sec = rp . subreg . odd ; <nl> # else <nl> - value -> tv_usec = cputime % 4096000000ULL ; <nl> + value -> tv_usec = ( cputime % 4096000000ULL ) / 4096 ; <nl> value -> tv_sec = cputime / 4096000000ULL ; <nl> # endif <nl> }
void iio_disable_all_buffers ( struct iio_dev * indio_dev ) <nl> indio_dev -> currentmode = INDIO_DIRECT_MODE ; <nl> if ( indio_dev -> setup_ops -> postdisable ) <nl> indio_dev -> setup_ops -> postdisable ( indio_dev ); <nl> + <nl> + if ( indio_dev -> available_scan_masks == NULL ) <nl> + kfree ( indio_dev -> active_scan_mask ); <nl> } <nl>  <nl> int iio_update_buffers ( struct iio_dev * indio_dev ,
int __nvme_submit_sync_cmd ( struct request_queue * q , struct nvme_command * cmd , <nl> return PTR_ERR ( req ); <nl>  <nl> req -> cmd_type = REQ_TYPE_DRV_PRIV ; <nl> + req -> cmd_flags = REQ_FAILFAST_DRIVER ; <nl> req -> __data_len = 0 ; <nl> req -> __sector = ( sector_t ) - 1 ; <nl> req -> bio = req -> biotail = NULL ;
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> /* Device config is special , because it must work on <nl> * an unconfigured device . */ <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> + if ( minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> + /* Device config not appropriate on non - board minors . */ <nl> + rc = - ENOTTY ; <nl> + goto done ; <nl> + } <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> if ( rc == 0 )
static void handle_swbp ( struct pt_regs * regs ) <nl> if ( unlikely (! test_bit ( UPROBE_COPY_INSN , & uprobe -> flags ))) <nl> goto out ; <nl>  <nl> + /* Tracing handlers use -> utask to communicate with fetch methods */ <nl> + if (! get_utask ()) <nl> + goto out ; <nl> + <nl> handler_chain ( uprobe , regs ); <nl> if ( can_skip_sstep ( uprobe , regs )) <nl> goto out ;
static int emc1403_probe ( struct i2c_client * client , <nl> res = sysfs_create_group (& client -> dev . kobj , & m_thermal_gr ); <nl> if ( res ) { <nl> dev_warn (& client -> dev , " create group failed \ n "); <nl> - hwmon_device_unregister ( data -> hwmon_dev ); <nl> goto thermal_error1 ; <nl> } <nl> data -> hwmon_dev = hwmon_device_register (& client -> dev );
static int pcc_cpufreq_target ( struct cpufreq_policy * policy , <nl> return 0 ; <nl>  <nl> cmd_incomplete : <nl> + freqs . new = freqs . old ; <nl> + cpufreq_notify_transition ( policy , & freqs , CPUFREQ_POSTCHANGE ); <nl> iowrite16 ( 0 , & pcch_hdr -> status ); <nl> spin_unlock (& pcc_lock ); <nl> return - EINVAL ;
int snd_pcm_status ( struct snd_pcm_substream * substream , <nl> runtime -> status -> audio_tstamp ; <nl> goto _tstamp_end ; <nl> } <nl> + } else { <nl> + /* get tstamp only in fallback mode and only if enabled */ <nl> + if ( runtime -> tstamp_mode == SNDRV_PCM_TSTAMP_ENABLE ) <nl> + snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> } <nl> - snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> _tstamp_end : <nl> status -> appl_ptr = runtime -> control -> appl_ptr ; <nl> status -> hw_ptr = runtime -> status -> hw_ptr ;
spider_net_stop ( struct net_device * netdev ) <nl> /* release chains */ <nl> spider_net_release_tx_chain ( card , 1 ); <nl>  <nl> + spider_net_free_rx_chain_contents ( card ); <nl> + <nl> spider_net_free_chain ( card , & card -> tx_chain ); <nl> spider_net_free_chain ( card , & card -> rx_chain ); <nl> 
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> req -> pages [ req -> num_pages ] = page ; <nl> req -> num_pages ++; <nl>  <nl> + offset = 0 ; <nl> num -= this_num ; <nl> total_len += this_num ; <nl> index ++;
xfs_fs_geometry ( <nl> xfs_fsop_geom_t * geo , <nl> int new_version ) <nl> { <nl> + <nl> + memset ( geo , 0 , sizeof (* geo )); <nl> + <nl> geo -> blocksize = mp -> m_sb . sb_blocksize ; <nl> geo -> rtextsize = mp -> m_sb . sb_rextsize ; <nl> geo -> agblocks = mp -> m_sb . sb_agblocks ;
static bool hist_browser__toggle_fold ( struct hist_browser * browser ) <nl> struct callchain_list * cl = container_of ( ms , struct callchain_list , ms ); <nl> bool has_children ; <nl>  <nl> + if (! he || ! ms ) <nl> + return false ; <nl> + <nl> if ( ms == & he -> ms ) <nl> has_children = hist_entry__toggle_fold ( he ); <nl> else
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl> ! iwlwifi_mod_params . sw_crypto ) <nl> hw -> flags |= IEEE80211_HW_MFP_CAPABLE ; <nl>  <nl> - if ( mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> + if ( 0 && mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> hw -> flags |= IEEE80211_HW_SUPPORTS_UAPSD ; <nl> hw -> uapsd_queues = IWL_UAPSD_AC_INFO ; <nl> hw -> uapsd_max_sp_len = IWL_UAPSD_MAX_SP ;
int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> which_atu = 0 ; <nl> } <nl>  <nl> - if (! which_atu ) <nl> + if (! which_atu ) { <nl> + kfree ( res ); <nl> return 0 ; <nl> + } <nl>  <nl> switch ( which_atu ) { <nl> case IOP13XX_INIT_ATU_ATUX : <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> sys -> map_irq = iop13xx_pcie_map_irq ; <nl> break ; <nl> default : <nl> + kfree ( res ); <nl> return 0 ; <nl> } <nl> 
void start_tty ( struct tty_struct * tty ) <nl>  <nl> /* If we have a running line discipline it may need kicking */ <nl> tty_wakeup ( tty ); <nl> - wake_up_interruptible (& tty -> write_wait ); <nl> } <nl>  <nl> EXPORT_SYMBOL ( start_tty );
static int generic_set_freq ( struct dvb_frontend * fe , <nl> goto err ; <nl>  <nl> rc = r820t_sysfreq_sel ( priv , freq , type , std , delsys ); <nl> + if ( rc < 0 ) <nl> + goto err ; <nl> + <nl> + tuner_dbg ("% s : PLL locked on frequency % d Hz , gain =% d \ n ", <nl> + __func__ , freq , r820t_read_gain ( priv )); <nl> + <nl> err : <nl>  <nl> if ( rc < 0 )
xfs_rename ( <nl> if ( unlikely (( target_dp -> i_d . di_flags & XFS_DIFLAG_PROJINHERIT ) && <nl> ( target_dp -> i_d . di_projid != src_ip -> i_d . di_projid ))) { <nl> error = XFS_ERROR ( EXDEV ); <nl> - xfs_rename_unlock4 ( inodes , XFS_ILOCK_SHARED ); <nl> + xfs_rename_unlock4 ( inodes , XFS_ILOCK_EXCL ); <nl> xfs_trans_cancel ( tp , cancel_flags ); <nl> goto std_return ; <nl> }
SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> if ( which > PRIO_USER || which < PRIO_PROCESS ) <nl> return - EINVAL ; <nl>  <nl> + rcu_read_lock (); <nl> read_lock (& tasklist_lock ); <nl> switch ( which ) { <nl> case PRIO_PROCESS : <nl> SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> } <nl> out_unlock : <nl> read_unlock (& tasklist_lock ); <nl> + rcu_read_unlock (); <nl>  <nl> return retval ; <nl> }
i915_gem_object_create_from_data ( struct drm_device * dev , <nl> i915_gem_object_pin_pages ( obj ); <nl> sg = obj -> pages ; <nl> bytes = sg_copy_from_buffer ( sg -> sgl , sg -> nents , ( void *) data , size ); <nl> + obj -> dirty = 1 ; /* Backing store is now out of date */ <nl> i915_gem_object_unpin_pages ( obj ); <nl>  <nl> if ( WARN_ON ( bytes != size )) {
static void ttm_tt_clear_mapping ( struct ttm_tt * ttm ) <nl> pgoff_t i ; <nl> struct page ** page = ttm -> pages ; <nl>  <nl> + if ( ttm -> page_flags & TTM_PAGE_FLAG_SG ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < ttm -> num_pages ; ++ i ) { <nl> (* page )-> mapping = NULL ; <nl> (* page ++)-> index = 0 ;
int get_dnode_of_data ( struct dnode_of_data * dn , pgoff_t index , int mode ) <nl>  <nl> /* if inline_data is set , should not report any block indices */ <nl> if ( f2fs_has_inline_data ( dn -> inode ) && index ) { <nl> - err = - EINVAL ; <nl> + err = - ENOENT ; <nl> f2fs_put_page ( npage [ 0 ], 1 ); <nl> goto release_out ; <nl> }
static int amd_xgbe_phy_set_mode ( struct phy_device * phydev , <nl> static enum amd_xgbe_phy_an amd_xgbe_an_tx_training ( struct phy_device * phydev , <nl> enum amd_xgbe_phy_rx * state ) <nl> { <nl> + struct amd_xgbe_phy_priv * priv = phydev -> priv ; <nl> int ad_reg , lp_reg , ret ; <nl>  <nl> * state = AMD_XGBE_RX_COMPLETE ;
brcmf_notify_sched_scan_results ( struct brcmf_if * ifp , <nl> } <nl>  <nl> set_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status ); <nl> + cfg -> escan_info . run = brcmf_run_escan ; <nl> err = brcmf_do_escan ( cfg , wiphy , ifp , request ); <nl> if ( err ) { <nl> clear_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status );
int f2fs_getxattr ( struct inode * inode , int name_index , const char * name , <nl> if ( name == NULL ) <nl> return - EINVAL ; <nl> name_len = strlen ( name ); <nl> + if ( name_len > F2FS_NAME_LEN ) <nl> + return - ERANGE ; <nl>  <nl> base_addr = read_all_xattrs ( inode , NULL ); <nl> if (! base_addr )
static int ocfs2_initialize_super ( struct super_block * sb , <nl> cbits = le32_to_cpu ( di -> id2 . i_super . s_clustersize_bits ); <nl> bbits = le32_to_cpu ( di -> id2 . i_super . s_blocksize_bits ); <nl> sb -> s_maxbytes = ocfs2_max_file_offset ( bbits , cbits ); <nl> + memcpy ( sb -> s_uuid , di -> id2 . i_super . s_uuid , <nl> + sizeof ( di -> id2 . i_super . s_uuid )); <nl>  <nl> osb -> osb_dx_mask = ( 1 << ( cbits - bbits )) - 1 ; <nl> 
static ssize_t bonding_store_slaves ( struct device * d , <nl>  <nl> if ( command [ 0 ] == '-') { <nl> dev = NULL ; <nl> + original_mtu = 0 ; <nl> bond_for_each_slave ( bond , slave , i ) <nl> if ( strnicmp ( slave -> dev -> name , ifname , IFNAMSIZ ) == 0 ) { <nl> dev = slave -> dev ;
static void hsmmc_command_incomplete ( struct omap_hsmmc_host * host , <nl> if ( host -> data ) { <nl> omap_hsmmc_reset_controller_fsm ( host , SRD ); <nl> omap_hsmmc_dma_cleanup ( host , err ); <nl> - } <nl> - <nl> + } else if ( host -> mrq && host -> mrq -> cmd ) <nl> + host -> mrq -> cmd -> error = err ; <nl> } <nl>  <nl> static void omap_hsmmc_do_irq ( struct omap_hsmmc_host * host , int status )
int iwl_enqueue_hcmd ( struct iwl_priv * priv , struct iwl_host_cmd * cmd ) <nl> return - EIO ; <nl> } <nl>  <nl> + if (( priv -> ucode_owner == IWL_OWNERSHIP_TM ) && <nl> + !( cmd -> flags & CMD_ON_DEMAND )) { <nl> + IWL_DEBUG_HC ( priv , " tm own the uCode , no regular hcmd send \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> copy_size = sizeof ( out_cmd -> hdr ); <nl> cmd_size = sizeof ( out_cmd -> hdr ); <nl> 
static void ext4_orphan_cleanup ( struct super_block * sb , <nl> jbd_debug ( 2 , " truncating inode % lu to % lld bytes \ n ", <nl> inode -> i_ino , inode -> i_size ); <nl> mutex_lock (& inode -> i_mutex ); <nl> + truncate_inode_pages ( inode -> i_mapping , inode -> i_size ); <nl> ext4_truncate ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> nr_truncates ++;
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
int radeonfb_create ( struct drm_device * dev , <nl> goto out_unref ; <nl> } <nl>  <nl> + rdev -> fbdev_info = info ; <nl> rfbdev = info -> par ; <nl> rfbdev -> helper . funcs = & radeon_fb_helper_funcs ; <nl> rfbdev -> helper . dev = dev ;
static int ehci_mxc_drv_remove ( struct platform_device * pdev ) <nl> if ( pdata && pdata -> exit ) <nl> pdata -> exit ( pdev ); <nl>  <nl> - if ( pdata -> otg ) <nl> + if ( pdata && pdata -> otg ) <nl> usb_phy_shutdown ( pdata -> otg ); <nl>  <nl> clk_disable_unprepare ( priv -> usbclk );
static int dwc3_ep0_set_config ( struct dwc3 * dwc , struct usb_ctrlrequest * ctrl ) <nl> case DWC3_ADDRESS_STATE : <nl> ret = dwc3_ep0_delegate_req ( dwc , ctrl ); <nl> /* if the cfg matches and the cfg is non zero */ <nl> - if (! ret && cfg ) <nl> + if ( cfg && (! ret || ( ret == USB_GADGET_DELAYED_STATUS ))) <nl> dwc -> dev_state = DWC3_CONFIGURED_STATE ; <nl> break ; <nl> 
int mthca_init_db_tab ( struct mthca_dev * dev ) <nl>  <nl> init_MUTEX (& dev -> db_tab -> mutex ); <nl>  <nl> - dev -> db_tab -> npages = dev -> uar_table . uarc_size / PAGE_SIZE ; <nl> + dev -> db_tab -> npages = dev -> uar_table . uarc_size / 4096 ; <nl> dev -> db_tab -> max_group1 = 0 ; <nl> dev -> db_tab -> min_group2 = dev -> db_tab -> npages - 1 ; <nl> 
cfq_should_preempt ( struct cfq_data * cfqd , struct cfq_queue * new_cfqq , <nl> if ( cfq_class_idle ( cfqq )) <nl> return true ; <nl>  <nl> + /* <nl> + * Don ' t allow a non - RT request to preempt an ongoing RT cfqq timeslice . <nl> + */ <nl> + if ( cfq_class_rt ( cfqq ) && ! cfq_class_rt ( new_cfqq )) <nl> + return false ; <nl> + <nl> /* <nl> * if the new request is sync , but the currently running queue is <nl> * not , let the sync request have priority .
void assert_pipe ( struct drm_i915_private * dev_priv , <nl> u32 val ; <nl> bool cur_state ; <nl>  <nl> + /* if we need the pipe A quirk it must be always on */ <nl> + if ( pipe == PIPE_A && dev_priv -> quirks & QUIRK_PIPEA_FORCE ) <nl> + state = true ; <nl> + <nl> reg = PIPECONF ( pipe ); <nl> val = I915_READ ( reg ); <nl> cur_state = !!( val & PIPECONF_ENABLE );
free_interfaces : <nl> intf -> dev . bus_id , ret ); <nl> continue ; <nl> } <nl> - usb_create_sysfs_intf_files ( intf ); <nl> + <nl> + /* The driver ' s probe method can call usb_set_interface (), <nl> + * which would mean the interface ' s sysfs files are already <nl> + * created . Just in case , we ' ll remove them first . <nl> + */ <nl> + usb_remove_sysfs_intf_files ( intf ); <nl> + usb_create_sysfs_intf_files ( intf ); <nl> } <nl>  <nl> usb_autosuspend_device ( dev );
static int __access_remote_vm ( struct task_struct * tsk , struct mm_struct * mm , <nl> */ <nl> # ifdef CONFIG_HAVE_IOREMAP_PROT <nl> vma = find_vma ( mm , addr ); <nl> - if (! vma ) <nl> + if (! vma || vma -> vm_start > addr ) <nl> break ; <nl> if ( vma -> vm_ops && vma -> vm_ops -> access ) <nl> ret = vma -> vm_ops -> access ( vma , addr , buf ,
static int pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> free_press_irq : <nl> - free_irq ( key_press_irq , NULL ); <nl> + free_irq ( key_press_irq , pwrkey ); <nl> unreg_input_dev : <nl> input_unregister_device ( pwr ); <nl> pwr = NULL ;
static int mcp230xx_probe ( struct i2c_client * client , <nl> pdata = devm_kzalloc (& client -> dev , <nl> sizeof ( struct mcp23s08_platform_data ), <nl> GFP_KERNEL ); <nl> + if (! pdata ) <nl> + return - ENOMEM ; <nl> pdata -> base = - 1 ; <nl> } <nl> }
int __init mem_reserve ( unsigned long start , unsigned long end , int must_exist ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( it && start - it -> start < bank_sz ) { <nl> + if ( it && start - it -> start <= bank_sz ) { <nl> if ( start == it -> start ) { <nl> if ( end - it -> start < bank_sz ) { <nl> it -> start = end ;
void laptop_mode_timer_fn ( unsigned long data ) <nl> if (! bdi_has_dirty_io (& q -> backing_dev_info )) <nl> return ; <nl>  <nl> + rcu_read_lock (); <nl> bdi_for_each_wb ( wb , & q -> backing_dev_info , & iter , 0 ) <nl> if ( wb_has_dirty_io ( wb )) <nl> wb_start_writeback ( wb , nr_pages , true , <nl> WB_REASON_LAPTOP_TIMER ); <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> /*
static int hfsplus_fill_super ( struct super_block * sb , void * data , int silent ) <nl> u64 last_fs_block , last_fs_page ; <nl> int err ; <nl>  <nl> - err = - EINVAL ; <nl> + err = - ENOMEM ; <nl> sbi = kzalloc ( sizeof (* sbi ), GFP_KERNEL ); <nl> if (! sbi ) <nl> goto out ;
static acpi_status intel_menlow_register_sensor ( acpi_handle handle , u32 lvl , <nl> return AE_ERROR ; <nl> } <nl>  <nl> + return AE_OK ; <nl> + <nl> aux1_not_found : <nl> if ( status == AE_NOT_FOUND ) <nl> return AE_OK ;
static struct usb_device_id usb_ids [] = { <nl> { USB_DEVICE ( 0x0411 , 0x00da ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x2019 , 0x5303 ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x129b , 0x1667 ), . driver_info = DEVICE_ZD1211B }, <nl> + { USB_DEVICE ( 0x0cde , 0x001a ), . driver_info = DEVICE_ZD1211B }, <nl> /* " Driverless " devices that need ejecting */ <nl> { USB_DEVICE ( 0x0ace , 0x2011 ), . driver_info = DEVICE_INSTALLER }, <nl> { USB_DEVICE ( 0x0ace , 0x20ff ), . driver_info = DEVICE_INSTALLER },
void led_trigger_unregister ( struct led_trigger * trigger ) <nl>  <nl> void led_trigger_unregister_simple ( struct led_trigger * trigger ) <nl> { <nl> - led_trigger_unregister ( trigger ); <nl> + if ( trigger ) <nl> + led_trigger_unregister ( trigger ); <nl> kfree ( trigger ); <nl> } <nl> 
 <nl> # define BUILD_IRQ ( nr ) \ <nl> asmlinkage void IRQ_NAME ( nr ); \ <nl> - asm ("\ n . p2align \ n " \ <nl> + asm ("\ n . text \ n . p2align \ n " \ <nl> " IRQ " # nr " _interrupt :\ n \ t " \ <nl> " push $~(" # nr ") ; " \ <nl> " jmp common_interrupt ");
static int pm860x_probe ( struct snd_soc_codec * codec ) <nl> } <nl> } <nl>  <nl> - pm860x_set_bias_level ( codec , SND_SOC_BIAS_STANDBY ); <nl> - <nl> return 0 ; <nl>  <nl> out : <nl> static int pm860x_remove ( struct snd_soc_codec * codec ) <nl>  <nl> for ( i = 3 ; i >= 0 ; i --) <nl> free_irq ( pm860x -> irq [ i ], pm860x ); <nl> - pm860x_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> return 0 ; <nl> } <nl> 
asmlinkage long compat_sys_nfsservctl ( int cmd , struct compat_nfsctl_arg __user * <nl>  <nl> default : <nl> err = - EINVAL ; <nl> - goto done ; <nl> + break ; <nl> } <nl>  <nl> + if ( err ) <nl> + goto done ; <nl> + <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> /* The __user pointer casts are valid because of the set_fs () */
static int tpm_tis_init ( struct device * dev , struct tpm_info * tpm_info , <nl> iowrite32 ( intmask , <nl> chip -> vendor . iobase + <nl> TPM_INT_ENABLE ( chip -> vendor . locality )); <nl> + <nl> + devm_free_irq ( dev , i , chip ); <nl> } <nl> } <nl> if ( chip -> vendor . irq ) {
struct map * machine__new_module ( struct machine * machine , u64 start , <nl> if ( kmod_path__parse_name (& m , filename )) <nl> return NULL ; <nl>  <nl> + map = map_groups__find_by_name (& machine -> kmaps , MAP__FUNCTION , <nl> + m . name ); <nl> + if ( map ) <nl> + goto out ; <nl> + <nl> dso = machine__module_dso ( machine , & m , filename ); <nl> if ( dso == NULL ) <nl> goto out ;
static int visornic_probe ( struct visor_device * dev ) <nl> goto cleanup_netdev ; <nl> } <nl>  <nl> - devdata -> rcvbuf = kzalloc ( sizeof ( struct sk_buff *) * <nl> - devdata -> num_rcv_bufs , GFP_KERNEL ); <nl> + devdata -> rcvbuf = kcalloc ( devdata -> num_rcv_bufs , <nl> + sizeof ( struct sk_buff *), GFP_KERNEL ); <nl> if (! devdata -> rcvbuf ) { <nl> err = - ENOMEM ; <nl> goto cleanup_rcvbuf ;
static int ieee80211_prep_connection ( struct ieee80211_sub_if_data * sdata , <nl> chanctx_conf = rcu_dereference ( sdata -> vif . chanctx_conf ); <nl> if ( WARN_ON (! chanctx_conf )) { <nl> rcu_read_unlock (); <nl> + sta_info_free ( local , new_sta ); <nl> return - EINVAL ; <nl> } <nl> rate_flags = ieee80211_chandef_rate_flags (& chanctx_conf -> def );
nouveau_bo_move_flips ( struct ttm_buffer_object * bo , bool evict , bool intr , <nl> if ( ret ) <nl> goto out ; <nl>  <nl> - ret = nouveau_bo_move_m2mf ( bo , true , intr , no_wait , new_mem ); <nl> + ret = nouveau_bo_move_m2mf ( bo , evict , intr , no_wait , new_mem ); <nl> if ( ret ) <nl> goto out ; <nl> 
static int add_new_gdb ( handle_t * handle , struct inode * inode , <nl> return err ; <nl>  <nl> exit_inode : <nl> + kfree ( n_group_desc ); <nl> /* ext4_handle_release_buffer ( handle , iloc . bh ); */ <nl> brelse ( iloc . bh ); <nl> exit_dindj :
static int at91_rtc_resume ( struct device * dev ) <nl>  <nl> static SIMPLE_DEV_PM_OPS ( at91_rtc_pm_ops , at91_rtc_suspend , at91_rtc_resume ); <nl>  <nl> +# ifdef CONFIG_OF <nl> static const struct of_device_id at91_rtc_dt_ids [] = { <nl> { . compatible = " atmel , at91rm9200 - rtc " }, <nl> { /* sentinel */ } <nl> }; <nl> MODULE_DEVICE_TABLE ( of , at91_rtc_dt_ids ); <nl> +# endif <nl>  <nl> static struct platform_driver at91_rtc_driver = { <nl> . remove = __exit_p ( at91_rtc_remove ),
int sdma_init ( struct hfi1_devdata * dd , u8 port ) <nl>  <nl> sde -> progress_check_head = 0 ; <nl>  <nl> - init_timer (& sde -> err_progress_check_timer ); <nl> - sde -> err_progress_check_timer . function = <nl> - sdma_err_progress_check ; <nl> - sde -> err_progress_check_timer . data = ( unsigned long ) sde ; <nl> + setup_timer (& sde -> err_progress_check_timer , <nl> + sdma_err_progress_check , ( unsigned long ) sde ); <nl>  <nl> sde -> descq = dma_zalloc_coherent ( <nl> & dd -> pcidev -> dev ,
static int ibmvfc_get_err_result ( struct ibmvfc_cmd * vfc_cmd ) <nl> int fc_rsp_len = rsp -> fcp_rsp_len ; <nl>  <nl> if (( rsp -> flags & FCP_RSP_LEN_VALID ) && <nl> - ((! fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> + (( fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> rsp -> data . info . rsp_code )) <nl> return DID_ERROR << 16 ; <nl> 
static int mwl8k_tx_wait_empty ( struct ieee80211_hw * hw ) <nl>  <nl> rc = - ETIMEDOUT ; <nl> } <nl> + priv -> tx_wait = NULL ; <nl> spin_unlock_bh (& priv -> tx_lock ); <nl>  <nl> return rc ;
static int snd_card_asihpi_trigger ( struct snd_pcm_substream * substream , <nl> VPRINTK1 ( KERN_INFO " start \ n "); <nl> /* start the master stream */ <nl> snd_card_asihpi_pcm_timer_start ( substream ); <nl> - if ( substream -> stream == SNDRV_PCM_STREAM_CAPTURE ) <nl> + if (( substream -> stream == SNDRV_PCM_STREAM_CAPTURE ) || <nl> + ! card -> support_mmap ) <nl> hpi_handle_error ( hpi_stream_start ( dpcm -> h_stream )); <nl> break ; <nl> 
static void do_writes ( struct mirror_set * ms , struct bio_list * writes ) <nl> /* <nl> * Dispatch io . <nl> */ <nl> - if ( unlikely ( ms -> log_failure )) { <nl> + if ( unlikely ( ms -> log_failure ) && errors_handled ( ms )) { <nl> spin_lock_irq (& ms -> lock ); <nl> bio_list_merge (& ms -> failures , & sync ); <nl> spin_unlock_irq (& ms -> lock );
static int of_mpc8xxx_spi_get_chipselects ( struct device * dev ) <nl> gpio = of_get_gpio_flags ( np , i , & flags ); <nl> if (! gpio_is_valid ( gpio )) { <nl> dev_err ( dev , " invalid gpio #% d : % d \ n ", i , gpio ); <nl> + ret = gpio ; <nl> goto err_loop ; <nl> } <nl> 
__acquires ( musb -> lock ) <nl> musb -> g . a_alt_hnp_support = 1 ; <nl> break ; <nl> # endif <nl> + case USB_DEVICE_DEBUG_MODE : <nl> + handled = 0 ; <nl> + break ; <nl> stall : <nl> default : <nl> handled = - EINVAL ;
static unsigned int cpg_div6_clock_calc_div ( unsigned long rate , <nl> { <nl> unsigned int div ; <nl>  <nl> + if (! rate ) <nl> + rate = 1 ; <nl> + <nl> div = DIV_ROUND_CLOSEST ( parent_rate , rate ); <nl> return clamp_t ( unsigned int , div , 1 , 64 ); <nl> }
static void reset_connection ( struct ceph_connection * con ) <nl> con -> out_msg = NULL ; <nl> } <nl> con -> in_seq = 0 ; <nl> + con -> in_seq_acked = 0 ; <nl> } <nl>  <nl> /*
static int __devinit fs_enet_mdio_probe ( struct of_device * ofdev , <nl>  <nl> ret = of_address_to_resource ( ofdev -> node , 0 , & res ); <nl> if ( ret ) <nl> - return ret ; <nl> + goto out_res ; <nl>  <nl> snprintf ( new_bus -> id , MII_BUS_ID_SIZE , "% x ", res . start ); <nl>  <nl> out_free_irqs : <nl> kfree ( new_bus -> irq ); <nl> out_unmap_regs : <nl> iounmap ( fec -> fecp ); <nl> + out_res : <nl> out_fec : <nl> kfree ( fec ); <nl> out_mii :
static void __exit powernv_cpufreq_exit ( void ) <nl> unregister_reboot_notifier (& powernv_cpufreq_reboot_nb ); <nl> opal_message_notifier_unregister ( OPAL_MSG_OCC , <nl> & powernv_cpufreq_opal_nb ); <nl> + kfree ( chips ); <nl> cpufreq_unregister_driver (& powernv_cpufreq_driver ); <nl> } <nl> module_exit ( powernv_cpufreq_exit );
intel_sdvo_tv_init ( struct intel_sdvo * intel_sdvo , int type ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl>  <nl> intel_sdvo_lvds_init ( struct intel_sdvo * intel_sdvo , int device ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl> 
static DEFINE_MUTEX ( nb_smu_ind_mutex ); <nl> * Control ] <nl> */ <nl> # define F15H_M60H_REPORTED_TEMP_CTRL_OFFSET 0xd8200ca4 <nl> -# define PCI_DEVICE_ID_AMD_15H_M60H_NB_F3 0x1573 <nl>  <nl> static void amd_nb_smu_index_read ( struct pci_dev * pdev , unsigned int devfn , <nl> int offset , u32 * val )
static void bmc150_accel_unregister_triggers ( struct bmc150_accel_data * data , <nl> { <nl> int i ; <nl>  <nl> - for ( i = from ; i >= 0 ; i ++) { <nl> + for ( i = from ; i >= 0 ; i --) { <nl> if ( data -> triggers [ i ]. indio_trig ) { <nl> iio_trigger_unregister ( data -> triggers [ i ]. indio_trig ); <nl> data -> triggers [ i ]. indio_trig = NULL ;
static int parse_gfp_flags ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> . size = sample -> raw_size , <nl> }; <nl> struct trace_seq seq ; <nl> - char * str , * pos ; <nl> + char * str , * pos = NULL ; <nl>  <nl> if ( nr_gfps ) { <nl> struct gfp_flag key = {
xlog_recover_do_reg_buffer ( <nl> stale_buf = 1 ; <nl> break ; <nl> } <nl> - if ( be16_to_cpu ( dip -> di_core . di_mode )) <nl> + if ( dip -> di_core . di_mode ) <nl> mode_count ++; <nl> - if ( be16_to_cpu ( dip -> di_core . di_gen )) <nl> + if ( dip -> di_core . di_gen ) <nl> gen_count ++; <nl> } <nl> 
int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
static void unlock_two_stripes ( struct stripe_head * sh1 , struct stripe_head * sh2 ) <nl> /* Only freshly new full stripe normal write stripe can be added to a batch list */ <nl> static bool stripe_can_batch ( struct stripe_head * sh ) <nl> { <nl> + struct r5conf * conf = sh -> raid_conf ; <nl> + <nl> + if ( conf -> log ) <nl> + return false ; <nl> return test_bit ( STRIPE_BATCH_READY , & sh -> state ) && <nl> ! test_bit ( STRIPE_BITMAP_PENDING , & sh -> state ) && <nl> is_full_stripe_write ( sh );
void fuse_put_request ( struct fuse_conn * fc , struct fuse_req * req ) <nl> spin_unlock (& fc -> lock ); <nl> } <nl>  <nl> - if ( req -> waiting ) <nl> + if ( req -> waiting ) { <nl> atomic_dec (& fc -> num_waiting ); <nl> + req -> waiting = 0 ; <nl> + } <nl>  <nl> if ( req -> stolen_file ) <nl> put_reserved_req ( fc , req );
static struct dma_chan * of_dma_sirfsoc_xlate ( struct of_phandle_args * dma_spec , <nl> struct sirfsoc_dma * sdma = ofdma -> of_dma_data ; <nl> unsigned int request = dma_spec -> args [ 0 ]; <nl>  <nl> - if ( request > SIRFSOC_DMA_CHANNELS ) <nl> + if ( request >= SIRFSOC_DMA_CHANNELS ) <nl> return NULL ; <nl>  <nl> return dma_get_slave_channel (& sdma -> channels [ request ]. chan );
static void __init of_omap2_apll_setup ( struct device_node * node ) <nl> const char * parent_name ; <nl> u32 val ; <nl>  <nl> - ad = kzalloc ( sizeof (* clk_hw ), GFP_KERNEL ); <nl> + ad = kzalloc ( sizeof (* ad ), GFP_KERNEL ); <nl> clk_hw = kzalloc ( sizeof (* clk_hw ), GFP_KERNEL ); <nl> init = kzalloc ( sizeof (* init ), GFP_KERNEL ); <nl> 
static int __devinit ntc_thermistor_probe ( struct platform_device * pdev ) <nl>  <nl> data -> dev = & pdev -> dev ; <nl> data -> pdata = pdata ; <nl> - strncpy ( data -> name , pdev -> id_entry -> name , PLATFORM_NAME_SIZE ); <nl> + strlcpy ( data -> name , pdev -> id_entry -> name , sizeof ( data -> name )); <nl>  <nl> switch ( pdev -> id_entry -> driver_data ) { <nl> case TYPE_NCPXXWB473 :
parahotplug_request_create ( struct controlvm_message * msg ) <nl> { <nl> struct parahotplug_request * req ; <nl>  <nl> - req = kmalloc ( sizeof (* req ), GFP_KERNEL | __GFP_NORETRY ); <nl> + req = kmalloc ( sizeof (* req ), GFP_KERNEL | __GFP_NORETRY ); <nl> if (! req ) <nl> return NULL ; <nl> 
u32 ath_calcrxfilter ( struct ath_softc * sc ) <nl> rfilt |= ATH9K_RX_FILTER_COMP_BAR ; <nl>  <nl> if ( sc -> nvifs > 1 || ( sc -> rx . rxfilter & FIF_OTHER_BSS )) { <nl> - /* The following may also be needed for other older chips */ <nl> - if ( sc -> sc_ah -> hw_version . macVersion == AR_SREV_VERSION_9160 ) <nl> + /* This is needed for older chips */ <nl> + if ( sc -> sc_ah -> hw_version . macVersion <= AR_SREV_VERSION_9160 ) <nl> rfilt |= ATH9K_RX_FILTER_PROM ; <nl> rfilt |= ATH9K_RX_FILTER_MCAST_BCAST_ALL ; <nl> }
static int ieee80211_stop ( struct net_device * dev ) <nl> case IEEE80211_IF_TYPE_STA : <nl> case IEEE80211_IF_TYPE_IBSS : <nl> sdata -> u . sta . state = IEEE80211_DISABLED ; <nl> + memset ( sdata -> u . sta . bssid , 0 , ETH_ALEN ); <nl> del_timer_sync (& sdata -> u . sta . timer ); <nl> /* <nl> * When we get here , the interface is marked down .
void bcm43xx_phy_set_baseband_attenuation ( struct bcm43xx_private * bcm , <nl> return ; <nl> } <nl>  <nl> - if ( phy -> analog > 1 ) { <nl> + if ( phy -> analog == 1 ) { <nl> value = bcm43xx_phy_read ( bcm , 0x0060 ) & ~ 0x003C ; <nl> value |= ( baseband_attenuation << 2 ) & 0x003C ; <nl> } else {
static struct dmi_system_id acer_quirks [] = { <nl> }, <nl> . driver_data = & quirk_lenovo_ideapad_s205 , <nl> }, <nl> + { <nl> + . callback = dmi_matched , <nl> + . ident = " Lenovo Ideapad S205 - 1038DPG ", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " LENOVO "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME , " 1038DPG "), <nl> + }, <nl> + . driver_data = & quirk_lenovo_ideapad_s205 , <nl> + }, <nl> {} <nl> }; <nl> 
static irqreturn_t s6000_pcm_irq ( int irq , void * data ) <nl> substream -> runtime && <nl> snd_pcm_running ( substream )) { <nl> dev_dbg ( pcm -> dev , " xrun \ n "); <nl> + snd_pcm_stream_lock ( substream ); <nl> snd_pcm_stop ( substream , SNDRV_PCM_STATE_XRUN ); <nl> + snd_pcm_stream_unlock ( substream ); <nl> ret = IRQ_HANDLED ; <nl> } <nl> 
acpi_ds_build_internal_package_obj ( struct acpi_walk_state * walk_state , <nl> arg = arg -> common . next ; <nl> } <nl>  <nl> - ACPI_ERROR (( AE_INFO , <nl> + ACPI_WARNING (( AE_INFO , <nl> " Package List length (% X ) larger than NumElements count (% X ), truncated \ n ", <nl> i , element_count )); <nl> } else if ( i < element_count ) {
device_release_WPADEV ( pDevice ); <nl> free_netdev ( pDevice -> dev ); <nl> } <nl>  <nl> - kfree ( pDevice ); <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO " device_disconnect3 .. \ n "); <nl> } <nl> 
static int process_pool_aeb ( struct ubi_device * ubi , struct ubi_attach_info * ai , <nl> av = tmp_av ; <nl> else { <nl> ubi_err (" orphaned volume in fastmap pool !"); <nl> + kmem_cache_free ( ai -> aeb_slab_cache , new_aeb ); <nl> return UBI_BAD_FASTMAP ; <nl> } <nl> 
static int snd_pcm_update_hw_ptr0 ( struct snd_pcm_substream * substream , <nl> * the elapsed time to detect xruns . <nl> */ <nl> jdelta = jiffies - runtime -> hw_ptr_jiffies ; <nl> + if ( jdelta < runtime -> hw_ptr_buffer_jiffies / 2 ) <nl> + goto no_delta_check ; <nl> hdelta = jdelta - delta * HZ / runtime -> rate ; <nl> while ( hdelta > runtime -> hw_ptr_buffer_jiffies / 2 + 1 ) { <nl> delta += runtime -> buffer_size ;
static struct xfrm_policy * xfrm_compile_policy ( u16 family , int opt , <nl> if ( nr > XFRM_MAX_DEPTH ) <nl> return NULL ; <nl>  <nl> + if ( p -> dir > XFRM_POLICY_OUT ) <nl> + return NULL ; <nl> + <nl> xp = xfrm_policy_alloc ( GFP_KERNEL ); <nl> if ( xp == NULL ) { <nl> * dir = - ENOBUFS ;
void dump_trace ( struct task_struct * task , <nl> unsigned used = 0 ; <nl> struct thread_info * tinfo ; <nl> int graph = 0 ; <nl> + unsigned long dummy ; <nl> unsigned long bp ; <nl>  <nl> if (! task ) <nl> task = current ; <nl>  <nl> if (! stack ) { <nl> - unsigned long dummy ; <nl> stack = & dummy ; <nl> if ( task && task != current ) <nl> stack = ( unsigned long *) task -> thread . sp ;
void dynamic_irq_cleanup ( unsigned int irq ) <nl> desc -> chip_data = NULL ; <nl> desc -> handle_irq = handle_bad_irq ; <nl> desc -> chip = & no_irq_chip ; <nl> + desc -> name = NULL ; <nl> spin_unlock_irqrestore (& desc -> lock , flags ); <nl> } <nl> 
static int special_clk_ctl_put ( struct snd_kcontrol * kctl , <nl> struct special_params * params = bebob -> maudio_special_quirk ; <nl> int err , id ; <nl>  <nl> - mutex_lock (& bebob -> mutex ); <nl> - <nl> id = uval -> value . enumerated . item [ 0 ]; <nl> if ( id >= ARRAY_SIZE ( special_clk_labels )) <nl> return 0 ; <nl>  <nl> + mutex_lock (& bebob -> mutex ); <nl> + <nl> err = avc_maudio_set_special_clk ( bebob , id , <nl> params -> dig_in_fmt , <nl> params -> dig_out_fmt ,
static int cpsw_poll ( struct napi_struct * napi , int budget ) <nl> cpdma_ctlr_eoi ( priv -> dma , CPDMA_EOI_RX ); <nl> prim_cpsw = cpsw_get_slave_priv ( priv , 0 ); <nl> if ( prim_cpsw -> irq_enabled == false ) { <nl> - cpsw_enable_irq ( priv ); <nl> prim_cpsw -> irq_enabled = true ; <nl> + cpsw_enable_irq ( priv ); <nl> } <nl> } <nl> 
static int sti_drm_platform_probe ( struct platform_device * pdev ) <nl> master = platform_device_register_resndata ( dev , <nl> DRIVER_NAME " __master ", - 1 , <nl> NULL , 0 , NULL , 0 ); <nl> - if (! master ) <nl> - return - EINVAL ; <nl> + if ( IS_ERR ( master )) <nl> + return PTR_ERR ( master ); <nl>  <nl> platform_set_drvdata ( pdev , master ); <nl> return 0 ;
static int cgroup_attach_proc ( struct cgroup * cgrp , struct task_struct * leader ) <nl> if (! group ) <nl> return - ENOMEM ; <nl> /* pre - allocate to guarantee space while iterating in rcu read - side . */ <nl> - retval = flex_array_prealloc ( group , 0 , group_size - 1 , GFP_KERNEL ); <nl> + retval = flex_array_prealloc ( group , 0 , group_size , GFP_KERNEL ); <nl> if ( retval ) <nl> goto out_free_group_list ; <nl> 
static void tty_audit_buf_push ( struct task_struct * tsk , uid_t loginuid , <nl> get_task_comm ( name , tsk ); <nl> audit_log_untrustedstring ( ab , name ); <nl> audit_log_format ( ab , " data ="); <nl> - audit_log_n_untrustedstring ( ab , buf -> data , buf -> valid ); <nl> + audit_log_n_hex ( ab , buf -> data , buf -> valid ); <nl> audit_log_end ( ab ); <nl> } <nl> buf -> valid = 0 ;
void flush_thread ( void ) <nl>  <nl> void start_thread ( struct pt_regs * regs , unsigned long eip , unsigned long esp ) <nl> { <nl> + get_safe_registers ( regs -> regs . gp , regs -> regs . fp ); <nl> PT_REGS_IP ( regs ) = eip ; <nl> PT_REGS_SP ( regs ) = esp ; <nl> current -> ptrace &= ~ PT_DTRACE ;
int bnx2i_send_iscsi_nopout ( struct bnx2i_conn * bnx2i_conn , <nl> bnx2i_cmd = ( struct bnx2i_cmd *) task -> dd_data ; <nl> nopout_hdr = ( struct iscsi_nopout *) task -> hdr ; <nl> nopout_wqe = ( struct bnx2i_nop_out_request *) ep -> qp . sq_prod_qe ; <nl> + <nl> + memset ( nopout_wqe , 0x00 , sizeof ( struct bnx2i_nop_out_request )); <nl> + <nl> nopout_wqe -> op_code = nopout_hdr -> opcode ; <nl> nopout_wqe -> op_attr = ISCSI_FLAG_CMD_FINAL ; <nl> memcpy ( nopout_wqe -> lun , nopout_hdr -> lun , 8 );
static void cleanup_one_si ( struct smi_info * to_clean ) <nl> if (! to_clean ) <nl> return ; <nl>  <nl> + if ( to_clean -> dev ) <nl> + dev_set_drvdata ( to_clean -> dev , NULL ); <nl> + <nl> list_del (& to_clean -> link ); <nl>  <nl> /* Tell the driver that we are shutting down . */
batadv_purge_outstanding_packets ( struct batadv_priv * bat_priv , <nl> * we delete only packets belonging to the given interface <nl> */ <nl> if (( hard_iface ) && <nl> - ( forw_packet -> if_incoming != hard_iface )) <nl> + ( forw_packet -> if_incoming != hard_iface ) && <nl> + ( forw_packet -> if_outgoing != hard_iface )) <nl> continue ; <nl>  <nl> spin_unlock_bh (& bat_priv -> forw_bcast_list_lock );
struct Qdisc_ops <nl>  <nl> int (* enqueue )( struct sk_buff *, struct Qdisc *); <nl> struct sk_buff * (* dequeue )( struct Qdisc *); <nl> + struct sk_buff * (* peek )( struct Qdisc *); <nl> int (* requeue )( struct sk_buff *, struct Qdisc *); <nl> unsigned int (* drop )( struct Qdisc *); <nl> 
static ssize_t hid_debug_events_read ( struct file * file , char __user * buffer , <nl>  <nl> if (! list -> hdev || ! list -> hdev -> debug ) { <nl> ret = - EIO ; <nl> - break ; <nl> + set_current_state ( TASK_RUNNING ); <nl> + goto out ; <nl> } <nl>  <nl> /* allow O_NONBLOCK from other threads */
static int load_segment_descriptor ( struct x86_emulate_ctxt * ctxt , <nl> seg_desc . type = 3 ; <nl> seg_desc . p = 1 ; <nl> seg_desc . s = 1 ; <nl> + if ( ctxt -> mode == X86EMUL_MODE_VM86 ) <nl> + seg_desc . dpl = 3 ; <nl> goto load ; <nl> } <nl> 
static int musb_gadget_stop ( struct usb_gadget * g , <nl> dev_dbg ( musb -> controller , " unregistering driver % s \ n ", driver -> function ); <nl>  <nl> musb -> is_active = 0 ; <nl> + musb -> gadget_driver = NULL ; <nl> musb_platform_try_idle ( musb , 0 ); <nl> spin_unlock_irqrestore (& musb -> lock , flags ); <nl> 
struct clk * icst_clk_register ( struct device * dev , <nl>  <nl> pclone = kmemdup ( desc -> params , sizeof (* pclone ), GFP_KERNEL ); <nl> if (! pclone ) { <nl> + kfree ( icst ); <nl> pr_err (" could not clone ICST params \ n "); <nl> return ERR_PTR (- ENOMEM ); <nl> }
static long clk_divider_round_rate ( struct clk_hw * hw , unsigned long rate , <nl> bestdiv = readl ( divider -> reg ) >> divider -> shift ; <nl> bestdiv &= div_mask ( divider -> width ); <nl> bestdiv = _get_div ( divider -> table , bestdiv , divider -> flags ); <nl> - return bestdiv ; <nl> + return DIV_ROUND_UP (* prate , bestdiv ); <nl> } <nl>  <nl> return divider_round_rate ( hw , rate , prate , divider -> table ,
int r100_cs_parse ( struct radeon_cs_parser * p ) <nl> int r ; <nl>  <nl> track = kzalloc ( sizeof (* track ), GFP_KERNEL ); <nl> + if (! track ) <nl> + return - ENOMEM ; <nl> r100_cs_track_clear ( p -> rdev , track ); <nl> p -> track = track ; <nl> do {
static int __init ubi_mtd_param_parse ( const char * val , struct kernel_param * kp ) <nl> char * pbuf = & buf [ 0 ]; <nl> char * tokens [ 3 ] = { NULL , NULL , NULL }; <nl>  <nl> + if (! val ) <nl> + return - EINVAL ; <nl> + <nl> if ( mtd_devs == UBI_MAX_DEVICES ) { <nl> printk (" UBI error : too many parameters , max . is % d \ n ", <nl> UBI_MAX_DEVICES );
int svc_create_xprt ( struct svc_serv * serv , const char * xprt_name , <nl> list_add (& newxprt -> xpt_list , & serv -> sv_permsocks ); <nl> spin_unlock_bh (& serv -> sv_lock ); <nl> newport = svc_xprt_local_port ( newxprt ); <nl> - clear_bit ( XPT_BUSY , & newxprt -> xpt_flags ); <nl> + svc_xprt_received ( newxprt ); <nl> return newport ; <nl> } <nl> err :
static int ci_udc_pullup ( struct usb_gadget * _gadget , int is_on ) <nl> { <nl> struct ci_hdrc * ci = container_of ( _gadget , struct ci_hdrc , gadget ); <nl>  <nl> + if (! ci -> vbus_active ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> if ( is_on ) <nl> hw_write ( ci , OP_USBCMD , USBCMD_RS , USBCMD_RS ); <nl> else
static int readable ( struct pcmcia_socket * s , struct resource * res , <nl> destroy_cis_cache ( s ); <nl> } <nl> s -> cis_mem . res = NULL ; <nl> - if (( ret != 0 ) || ( count == 0 )) <nl> + if (( ret != 0 ) || (* count == 0 )) <nl> return 0 ; <nl> return 1 ; <nl> }
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
void __init init_bsp_APIC ( void ) <nl> /** <nl> * setup_local_APIC - setup the local APIC <nl> */ <nl> - void __devinit setup_local_APIC ( void ) <nl> + void __cpuinit setup_local_APIC ( void ) <nl> { <nl> unsigned long oldvalue , value , maxlvt , integrated ; <nl> int i , j ;
static struct hw_breakpoint { <nl> unsigned long addr ; <nl> int len ; <nl> int type ; <nl> - struct perf_event ** pev ; <nl> + struct perf_event * __percpu * pev ; <nl> } breakinfo [ HBP_NUM ]; <nl>  <nl> static unsigned long early_dr7 ;
static void vlv_display_power_well_deinit ( struct drm_i915_private * dev_priv ) <nl> valleyview_disable_display_irqs ( dev_priv ); <nl> spin_unlock_irq (& dev_priv -> irq_lock ); <nl>  <nl> + /* make sure we ' re done processing display irqs */ <nl> + synchronize_irq ( dev_priv -> dev -> irq ); <nl> + <nl> vlv_power_sequencer_reset ( dev_priv ); <nl> } <nl> 
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
cache_type_store ( struct device * dev , struct device_attribute * attr , <nl> buffer_data [ 2 ] &= ~ 0x05 ; <nl> buffer_data [ 2 ] |= wce << 2 | rcd ; <nl> sp = buffer_data [ 0 ] & 0x80 ? 1 : 0 ; <nl> + buffer_data [ 0 ] &= ~ 0x80 ; <nl>  <nl> if ( scsi_mode_select ( sdp , 1 , sp , 8 , buffer_data , len , SD_TIMEOUT , <nl> SD_MAX_RETRIES , & data , & sshdr )) {
static int ad1836_register ( struct ad1836_priv * ad1836 ) <nl>  <nl> if ( ad1836_codec ) { <nl> dev_err ( codec -> dev , " Another ad1836 is registered \ n "); <nl> + kfree ( ad1836 ); <nl> return - EINVAL ; <nl> } <nl> 
static void tgfx_attach ( struct parport * pp ) <nl> n_buttons = tgfx_cfg [ port_idx ]. args + 1 ; <nl> n_devs = tgfx_cfg [ port_idx ]. nargs - 1 ; <nl>  <nl> + memset (& tgfx_parport_cb , 0 , sizeof ( tgfx_parport_cb )); <nl> tgfx_parport_cb . flags = PARPORT_FLAG_EXCL ; <nl>  <nl> pd = parport_register_dev_model ( pp , " turbografx ", & tgfx_parport_cb ,
static int vq_memory_access_ok ( void __user * log_base , struct vhost_memory * mem , <nl> int log_all ) <nl> { <nl> int i ; <nl> + <nl> + if (! mem ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < mem -> nregions ; ++ i ) { <nl> struct vhost_memory_region * m = mem -> regions + i ; <nl> unsigned long a = m -> userspace_addr ;
int ide_device_add ( u8 idx [ 4 ]) <nl>  <nl> hwif = & ide_hwifs [ idx [ i ]]; <nl>  <nl> - if ( hwif -> present ) <nl> + if ( hwif -> present ) { <nl> + if ( hwif -> chipset == ide_unknown || <nl> + hwif -> chipset == ide_forced ) <nl> + hwif -> chipset = ide_generic ; <nl> hwif_register_devices ( hwif ); <nl> + } <nl> } <nl>  <nl> for ( i = 0 ; i < 4 ; i ++) {
int __max730x_remove ( struct device * dev ) <nl> ts -> write ( dev , 0x04 , 0x00 ); <nl> gpiochip_remove (& ts -> chip ); <nl> mutex_destroy (& ts -> lock ); <nl> - kfree ( ts ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( __max730x_remove );
void led_blink_set ( struct led_classdev * led_cdev , <nl> unsigned long * delay_on , <nl> unsigned long * delay_off ) <nl> { <nl> + del_timer_sync (& led_cdev -> blink_timer ); <nl> + <nl> if ( led_cdev -> blink_set && <nl> ! led_cdev -> blink_set ( led_cdev , delay_on , delay_off )) { <nl> led_cdev -> blink_delay_on = * delay_on ;
int madvise_free_huge_pmd ( struct mmu_gather * tlb , struct vm_area_struct * vma , <nl> int ret = 0 ; <nl>  <nl> if (! pmd_trans_huge_lock ( pmd , vma , & ptl )) <nl> - goto out ; <nl> + goto out_unlocked ; <nl>  <nl> orig_pmd = * pmd ; <nl> if ( is_huge_zero_pmd ( orig_pmd )) {
int ath_cmn_process_fft ( struct ath_spec_scan_priv * spec_priv , struct ieee80211_h <nl> } <nl>  <nl> /* Process a normal frame */ <nl> - if ( sample_bytes == sample_len ) { <nl> - memcpy ( sample_buf , sample_start , sample_len ); <nl> - ret = fft_handler ( rs , spec_priv , sample_buf , <nl> + if ( sample_bytes == sample_len ) <nl> + ret = fft_handler ( rs , spec_priv , sample_start , <nl> tsf , freq , chan_type ); <nl> - } <nl>  <nl> /* Short report processed , break out of the <nl> * loop .
static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
static inline void check_for_tasks ( int cpu ) <nl>  <nl> write_lock_irq (& tasklist_lock ); <nl> for_each_process ( p ) { <nl> - if ( task_cpu ( p ) == cpu && <nl> + if ( task_cpu ( p ) == cpu && p -> state == TASK_RUNNING && <nl> (! cputime_eq ( p -> utime , cputime_zero ) || <nl> ! cputime_eq ( p -> stime , cputime_zero ))) <nl> printk ( KERN_WARNING " Task % s ( pid = % d ) is on cpu % d \
static int generic_hdmi_build_controls ( struct hda_codec * codec ) <nl> struct snd_pcm_chmap * chmap ; <nl> struct snd_kcontrol * kctl ; <nl> int i ; <nl> + <nl> + if (! codec -> pcm_info [ pin_idx ]. pcm ) <nl> + break ; <nl> err = snd_pcm_add_chmap_ctls ( codec -> pcm_info [ pin_idx ]. pcm , <nl> SNDRV_PCM_STREAM_PLAYBACK , <nl> NULL , 0 , pin_idx , & chmap );
static ctl_table vm_table [] = { <nl> . extra2 = & one_hundred , <nl> }, <nl> # endif <nl> -# ifdef CONFIG_X86_32 <nl> +# if defined ( CONFIG_X86_32 ) || \ <nl> + ( defined ( CONFIG_SUPERH ) && defined ( CONFIG_VSYSCALL )) <nl> { <nl> . ctl_name = VM_VDSO_ENABLED , <nl> . procname = " vdso_enabled ",
static irqreturn_t edt_ft5x06_ts_isr ( int irq , void * dev_id ) <nl> if ( type == TOUCH_EVENT_RESERVED ) <nl> continue ; <nl>  <nl> + /* ignore TOUCH_DOWN events , might have bogus coordinates */ <nl> + if ( type == TOUCH_EVENT_DOWN ) <nl> + continue ; <nl> + <nl> x = (( buf [ 0 ] << 8 ) | buf [ 1 ]) & 0x0fff ; <nl> y = (( buf [ 2 ] << 8 ) | buf [ 3 ]) & 0x0fff ; <nl> id = ( buf [ 2 ] >> 4 ) & 0x0f ;
static void ni_write_caldac ( struct comedi_device * dev , int addr , int val ) <nl> addr -= caldacs [ type ]. n_chans ; <nl> } <nl>  <nl> + /* bits will be 0 if there is no caldac for the given addr */ <nl> + if ( bits == 0 ) <nl> + return ; <nl> + <nl> for ( bit = 1 << ( bits - 1 ); bit ; bit >>= 1 ) { <nl> ni_writeb ( dev , (( bit & bitstring ) ? 0x02 : 0 ), Serial_Command ); <nl> udelay ( 1 );
static int rtas_excl_open ( struct inode * inode , struct file * file ) <nl>  <nl> /* Enforce exclusive open with use count of PDE */ <nl> spin_lock (& flash_file_open_lock ); <nl> - if ( atomic_read (& dp -> count ) > 1 ) { <nl> + if ( atomic_read (& dp -> count ) > 2 ) { <nl> spin_unlock (& flash_file_open_lock ); <nl> return - EBUSY ; <nl> }
static inline void of_pci_check_probe_only ( void ) { } <nl> int of_pci_get_host_bridge_resources ( struct device_node * dev , <nl> unsigned char busno , unsigned char bus_max , <nl> struct list_head * resources , resource_size_t * io_base ); <nl> +# else <nl> + static inline int of_pci_get_host_bridge_resources ( struct device_node * dev , <nl> + unsigned char busno , unsigned char bus_max , <nl> + struct list_head * resources , resource_size_t * io_base ) <nl> +{ <nl> + return - EINVAL ; <nl> +} <nl> # endif <nl>  <nl> # if defined ( CONFIG_OF ) && defined ( CONFIG_PCI_MSI )
static int initializing = 1 ; <nl>  <nl> static int pmac_late_init ( void ) <nl> { <nl> + if (! machine_is ( powermac )) <nl> + return - ENODEV ; <nl> + <nl> initializing = 0 ; <nl> /* this is udbg ( which is __init ) and we can later use it during <nl> * cpu hotplug ( in smp_core99_kick_cpu ) */
static void ath6kl_recovery_work ( struct work_struct * work ) <nl>  <nl> ar -> fw_recovery . err_reason = 0 ; <nl>  <nl> - if ( ar -> fw_recovery . enable ) <nl> + if ( ar -> fw_recovery . hb_poll ) <nl> mod_timer (& ar -> fw_recovery . hb_timer , jiffies + <nl> msecs_to_jiffies ( ar -> fw_recovery . hb_poll )); <nl> }
MODULE_LICENSE (" GPL v2 "); <nl>  <nl> static int debug ; <nl> module_param ( debug , int , 0644 ); <nl> - MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on \ n "); <nl> + MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on "); <nl>  <nl> /* # define MPX_DEBUG */ <nl> 
wl12xx_scan_sched_scan_ssid_list ( struct wl1271 * wl , <nl> * so they ' re used in probe requests . <nl> */ <nl> for ( i = 0 ; i < req -> n_ssids ; i ++) { <nl> + if (! req -> ssids [ i ]. ssid_len ) <nl> + continue ; <nl> + <nl> for ( j = 0 ; j < cmd -> n_ssids ; j ++) <nl> if (! memcmp ( req -> ssids [ i ]. ssid , <nl> cmd -> ssids [ j ]. ssid ,
PyMODINIT_FUNC initperf ( void ) <nl> pyrf_cpu_map__setup_types () < 0 ) <nl> return ; <nl>  <nl> + page_size = sysconf ( _SC_PAGE_SIZE ); <nl> + <nl> Py_INCREF (& pyrf_evlist__type ); <nl> PyModule_AddObject ( module , " evlist ", ( PyObject *)& pyrf_evlist__type ); <nl> 
static void wlcore_op_stop_locked ( struct wl1271 * wl ) <nl>  <nl> /* <nl> * FW channels must be re - calibrated after recovery , <nl> - * clear the last Reg - Domain channel configuration . <nl> + * save current Reg - Domain channel configuration and clear it . <nl> */ <nl> + memcpy ( wl -> reg_ch_conf_pending , wl -> reg_ch_conf_last , <nl> + sizeof ( wl -> reg_ch_conf_pending )); <nl> memset ( wl -> reg_ch_conf_last , 0 , sizeof ( wl -> reg_ch_conf_last )); <nl> } <nl> 
struct hda_bus { <nl>  <nl> /* codec linked list */ <nl> struct list_head codec_list ; <nl> - struct hda_codec * caddr_tbl [ HDA_MAX_CODEC_ADDRESS ]; /* caddr -> codec */ <nl> + struct hda_codec * caddr_tbl [ HDA_MAX_CODEC_ADDRESS + 1 ]; /* caddr -> codec */ <nl>  <nl> struct semaphore cmd_mutex ; <nl> 
bool i40e_dcb_need_reconfig ( struct i40e_pf * pf , <nl> /* Check if APP Table has changed */ <nl> if ( memcmp (& new_cfg -> app , <nl> & old_cfg -> app , <nl> - sizeof ( new_cfg -> app ))) <nl> + sizeof ( new_cfg -> app ))) { <nl> need_reconfig = true ; <nl> dev_info (& pf -> pdev -> dev , " APP Table change detected .\ n "); <nl> + } <nl>  <nl> return need_reconfig ; <nl> }
static void intel_pin_eld_notify ( void * audio_ptr , int port ) <nl> */ <nl> if ( snd_power_get_state ( codec -> card ) != SNDRV_CTL_POWER_D0 ) <nl> return ; <nl> + /* ditto during suspend / resume process itself */ <nl> + if ( atomic_read (&( codec )-> core . in_pm )) <nl> + return ; <nl>  <nl> check_presence_and_report ( codec , pin_nid ); <nl> }
# define OP_31_XOP_SLBIA 498 <nl> # define OP_31_XOP_MFSR 595 <nl> # define OP_31_XOP_MFSRIN 659 <nl> +# define OP_31_XOP_DCBA 758 <nl> # define OP_31_XOP_SLBMFEV 851 <nl> # define OP_31_XOP_EIOIO 854 <nl> # define OP_31_XOP_SLBMFEE 915 <nl> int kvmppc_core_emulate_op ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> kvmppc_set_gpr ( vcpu , get_rt ( inst ), t ); <nl> } <nl> break ; <nl> + case OP_31_XOP_DCBA : <nl> + /* Gets treated as NOP */ <nl> + break ; <nl> case OP_31_XOP_DCBZ : <nl> { <nl> ulong rb = kvmppc_get_gpr ( vcpu , get_rb ( inst ));
static int perl_start_script ( const char * script , int argc , const char ** argv ) <nl> goto error ; <nl> } <nl>  <nl> - perl_run ( my_perl ); <nl> + if ( perl_run ( my_perl )) { <nl> + err = - 1 ; <nl> + goto error ; <nl> + } <nl> + <nl> if ( SvTRUE ( ERRSV )) { <nl> err = - 1 ; <nl> goto error ;
void __init clocksource_of_init ( void ) <nl> clocksource_of_init_fn init_func ; <nl>  <nl> for_each_matching_node_and_match ( np , __clksrc_of_table , & match ) { <nl> + if (! of_device_is_available ( np )) <nl> + continue ; <nl> + <nl> init_func = match -> data ; <nl> init_func ( np ); <nl> }
do_open_permission ( struct svc_rqst * rqstp , struct svc_fh * current_fh , struct nfs <nl>  <nl> if ( open -> op_share_access & NFS4_SHARE_ACCESS_READ ) <nl> accmode |= MAY_READ ; <nl> - if ( open -> op_share_deny & NFS4_SHARE_ACCESS_WRITE ) <nl> + if ( open -> op_share_access & NFS4_SHARE_ACCESS_WRITE ) <nl> accmode |= ( MAY_WRITE | MAY_TRUNC ); <nl> + if ( open -> op_share_deny & NFS4_SHARE_DENY_WRITE ) <nl> + accmode |= MAY_WRITE ; <nl>  <nl> status = fh_verify ( rqstp , current_fh , S_IFREG , accmode ); <nl> 
static void __save_error_info ( struct super_block * sb , const char * func , <nl> struct ext4_super_block * es = EXT4_SB ( sb )-> s_es ; <nl>  <nl> EXT4_SB ( sb )-> s_mount_state |= EXT4_ERROR_FS ; <nl> + if ( bdev_read_only ( sb -> s_bdev )) <nl> + return ; <nl> es -> s_state |= cpu_to_le16 ( EXT4_ERROR_FS ); <nl> es -> s_last_error_time = cpu_to_le32 ( get_seconds ()); <nl> strncpy ( es -> s_last_error_func , func , sizeof ( es -> s_last_error_func ));
static int sprom_extract ( struct ssb_bus * bus , struct ssb_sprom * out , <nl> ssb_printk ( KERN_WARNING PFX " Unsupported SPROM " <nl> " revision % d detected . Will extract " <nl> " v1 \ n ", out -> revision ); <nl> + out -> revision = 1 ; <nl> sprom_extract_r123 ( out , in ); <nl> } <nl> }
static void cxl_pci_disable_device ( struct pci_dev * dev ) <nl> dev_err (& dev -> dev , " Default context started \ n "); <nl> return ; <nl> } <nl> + dev -> dev . archdata . cxl_ctx = NULL ; <nl> cxl_release_context ( ctx ); <nl> } <nl> }
static int add_tracepoint_multi ( struct list_head ** list , int * idx , <nl> ret = add_tracepoint ( list , idx , sys_name , evt_ent -> d_name ); <nl> } <nl>  <nl> + closedir ( evt_dir ); <nl> return ret ; <nl> } <nl> 
static int balloon ( void * _vballoon ) <nl> try_to_freeze (); <nl> wait_event_interruptible ( vb -> config_change , <nl> ( diff = towards_target ( vb )) != 0 <nl> - || kthread_should_stop ()); <nl> + || kthread_should_stop () <nl> + || freezing ( current )); <nl> if ( diff > 0 ) <nl> fill_balloon ( vb , diff ); <nl> else if ( diff < 0 )
done : <nl> atomic_dec (& urb -> use_count ); <nl> if ( urb -> reject ) <nl> wake_up (& usb_kill_urb_queue ); <nl> - usb_put_urb ( urb ); <nl> usbmon_urb_submit_error (& hcd -> self , urb , status ); <nl> + usb_put_urb ( urb ); <nl> } <nl> return status ; <nl> }
static int atomic_open ( struct nameidata * nd , struct dentry * dentry , <nl> dput ( dentry ); <nl> dentry = file -> f_path . dentry ; <nl> } <nl> + if ( create_error && dentry -> d_inode == NULL ) { <nl> + error = create_error ; <nl> + goto out ; <nl> + } <nl> goto looked_up ; <nl> } <nl> 
struct vm_struct * alloc_vm_area ( size_t size ) <nl> return NULL ; <nl> } <nl>  <nl> - /* Make sure the pagetables are constructed in process kernel <nl> - mappings */ <nl> - vmalloc_sync_all (); <nl> - <nl> return area ; <nl> } <nl> EXPORT_SYMBOL_GPL ( alloc_vm_area );
fail : <nl> kvm_unregister_irq_mask_notifier ( kvm , 0 , & pit -> mask_notifier ); <nl> kvm_unregister_irq_ack_notifier ( kvm , & pit_state -> irq_ack_notifier ); <nl> kvm_free_irq_source_id ( kvm , pit -> irq_source_id ); <nl> - <nl> + destroy_workqueue ( pit -> wq ); <nl> kfree ( pit ); <nl> return NULL ; <nl> }
static int imx1_pinctrl_parse_functions ( struct device_node * np , <nl> /* Initialise function */ <nl> func -> name = np -> name ; <nl> func -> num_groups = of_get_child_count ( np ); <nl> - if ( func -> num_groups <= 0 ) <nl> + if ( func -> num_groups == 0 ) <nl> return - EINVAL ; <nl>  <nl> func -> groups = devm_kzalloc ( info -> dev ,
static void mousevsc_on_channel_callback ( void * context ) <nl> static int mousevsc_connect_to_vsp ( struct hv_device * device ) <nl> { <nl> int ret = 0 ; <nl> - int t ; <nl> + unsigned long t ; <nl> struct mousevsc_dev * input_dev = hv_get_drvdata ( device ); <nl> struct mousevsc_prt_msg * request ; <nl> struct mousevsc_prt_msg * response ;
store_dh_state ( struct device * dev , struct device_attribute * attr , <nl> struct scsi_device_handler * scsi_dh ; <nl> int err = - EINVAL ; <nl>  <nl> + if ( sdev -> sdev_state == SDEV_CANCEL || <nl> + sdev -> sdev_state == SDEV_DEL ) <nl> + return - ENODEV ; <nl> + <nl> if (! sdev -> scsi_dh_data ) { <nl> /* <nl> * Attach to a device handler
int radeon_atombios_init ( struct radeon_device * rdev ) <nl>  <nl> void radeon_atombios_fini ( struct radeon_device * rdev ) <nl> { <nl> - kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> - kfree ( rdev -> mode_info . atom_context ); <nl> + if ( rdev -> mode_info . atom_context ) { <nl> + kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> + kfree ( rdev -> mode_info . atom_context ); <nl> + } <nl> kfree ( rdev -> mode_info . atom_card_info ); <nl> } <nl> 
int hmc5843_common_probe ( struct device * dev , struct regmap * regmap , <nl> mutex_init (& data -> lock ); <nl>  <nl> indio_dev -> dev . parent = dev ; <nl> + indio_dev -> name = dev -> driver -> name ; <nl> indio_dev -> info = & hmc5843_info ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl> indio_dev -> channels = data -> variant -> channels ;
int tf_tonga_thermal_setup_fan_table ( struct pp_hwmgr * hwmgr , void * input , void * <nl> int res ; <nl> uint64_t tmp64 ; <nl>  <nl> + if (! phm_cap_enabled ( hwmgr -> platform_descriptor . platformCaps , PHM_PlatformCaps_MicrocodeFanControl )) <nl> + return 0 ; <nl> + <nl> if ( 0 == data -> fan_table_start ) { <nl> phm_cap_unset ( hwmgr -> platform_descriptor . platformCaps , PHM_PlatformCaps_MicrocodeFanControl ); <nl> return 0 ;
static int do_ip_getsockopt ( struct sock * sk , int level , int optname , <nl> case IP_HDRINCL : <nl> val = inet -> hdrincl ; <nl> break ; <nl> + case IP_NODEFRAG : <nl> + val = inet -> nodefrag ; <nl> + break ; <nl> case IP_MTU_DISCOVER : <nl> val = inet -> pmtudisc ; <nl> break ;
static int ath10k_pci_probe ( struct pci_dev * pdev , <nl> if (! ath10k_pci_chip_is_supported ( pdev -> device , chip_id )) { <nl> ath10k_err ( ar , " device % 04x with chip_id % 08x isn ' t supported \ n ", <nl> pdev -> device , chip_id ); <nl> - goto err_sleep ; <nl> + goto err_free_irq ; <nl> } <nl>  <nl> ret = ath10k_core_register ( ar , chip_id );
struct btrfs_root * btrfs_read_fs_root_no_radix ( struct btrfs_root * tree_root , <nl> } <nl> btrfs_free_path ( path ); <nl> if ( ret ) { <nl> + kfree ( root ); <nl> if ( ret > 0 ) <nl> ret = - ENOENT ; <nl> return ERR_PTR ( ret );
qla2x00_mailbox_command ( scsi_qla_host_t * vha , mbx_cmd_t * mcp ) <nl> " mb [ 0 ] = 0x % x .\ n ", mb0 ); <nl> ql_dump_regs ( ql_dbg_mbx + ql_dbg_buffer , vha , 0x1019 ); <nl>  <nl> + /* <nl> + * Attempt to capture a firmware dump for further analysis <nl> + * of the current firmware state <nl> + */ <nl> + ha -> isp_ops -> fw_dump ( vha , 0 ); <nl> + <nl> rval = QLA_FUNCTION_TIMEOUT ; <nl> } <nl> 
int i915_vma_unbind ( struct i915_vma * vma ) <nl>  <nl> i915_gem_gtt_finish_object ( obj ); <nl>  <nl> - list_del (& vma -> mm_list ); <nl> + list_del_init (& vma -> mm_list ); <nl> /* Avoid an unnecessary call to unbind on rebind . */ <nl> if ( i915_is_ggtt ( vma -> vm )) <nl> obj -> map_and_fenceable = true ;
static int xfrm_get_policy ( struct sk_buff * skb , struct nlmsghdr * nlh , void ** xfr <nl> MSG_DONTWAIT ); <nl> } <nl> } else { <nl> + c . data . byid = p -> index ; <nl> c . event = XFRM_SAP_DELETED ; <nl> c . seq = nlh -> nlmsg_seq ; <nl> c . pid = nlh -> nlmsg_pid ;
int shpchprm_set_hpp ( <nl> pci_bus -> number = func -> bus ; <nl> devfn = PCI_DEVFN ( func -> device , func -> function ); <nl>  <nl> - ab = find_acpi_bridge_by_bus ( acpi_bridges_head , ctrl -> seg , ctrl -> bus ); <nl> + ab = find_acpi_bridge_by_bus ( acpi_bridges_head , ctrl -> seg , ctrl -> slot_bus ); <nl>  <nl> if ( ab ) { <nl> if ( ab -> _hpp ) {
static noinline_for_stack int ethtool_get_rxnfc ( struct net_device * dev , <nl>  <nl> if ( info . cmd == ETHTOOL_GRXCLSRLALL ) { <nl> if ( info . rule_cnt > 0 ) { <nl> - rule_buf = kmalloc ( info . rule_cnt * sizeof ( u32 ), <nl> - GFP_USER ); <nl> + if ( info . rule_cnt <= KMALLOC_MAX_SIZE / sizeof ( u32 )) <nl> + rule_buf = kmalloc ( info . rule_cnt * sizeof ( u32 ), <nl> + GFP_USER ); <nl> if (! rule_buf ) <nl> return - ENOMEM ; <nl> }
int cfhsi_probe ( struct platform_device * pdev ) <nl> dev_err (& ndev -> dev , "% s : Registration error : % d .\ n ", <nl> __func__ , res ); <nl> free_netdev ( ndev ); <nl> + return - ENODEV ; <nl> } <nl> /* Add CAIF HSI device to list . */ <nl> spin_lock (& cfhsi_list_lock );
static int rt2800_init_registers ( struct rt2x00_dev * rt2x00dev ) <nl>  <nl> rt2800_register_read ( rt2x00dev , MM40_PROT_CFG , & reg ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_RATE , 0x4084 ); <nl> - rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , <nl> - ! rt2x00_is_usb ( rt2x00dev )); <nl> + rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_CTRL , 0 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_PROTECT_NAV , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_CCK , 1 ); <nl> rt2x00_set_field32 (& reg , MM40_PROT_CFG_TX_OP_ALLOW_OFDM , 1 );
# include < linux / module . h > /* Modules */ <nl> # include < linux / init . h > /* Initdata */ <nl> # include < linux / ioport . h > /* request_region */ <nl> +# include < linux / delay . h > /* msleep */ <nl> # include < linux / videodev2 . h > /* kernel radio structs */ <nl> # include < linux / version . h > /* for KERNEL_VERSION MACRO */ <nl> # include < linux / io . h > /* outb , outb_p */
static hda_nid_t path_power_update ( struct hda_codec * codec , <nl>  <nl> for ( i = 0 ; i < path -> depth ; i ++) { <nl> nid = path -> path [ i ]; <nl> + if (!( get_wcaps ( codec , nid ) & AC_WCAP_POWER )) <nl> + continue ; <nl> if ( nid == codec -> core . afg ) <nl> continue ; <nl> if (! allow_powerdown || is_active_nid_for_any ( codec , nid ))
int btrfs_orphan_add ( struct btrfs_trans_handle * trans , struct inode * inode ) <nl> /* insert an orphan item to track this unlinked / truncated file */ <nl> if ( insert >= 1 ) { <nl> ret = btrfs_insert_orphan_item ( trans , root , btrfs_ino ( inode )); <nl> - BUG_ON ( ret ); <nl> + BUG_ON ( ret && ret != - EEXIST ); <nl> } <nl>  <nl> /* insert an orphan item to track subvolume contains orphan files */
static void wait_for_writer ( struct btrfs_trans_handle * trans , <nl> mutex_unlock (& root -> log_mutex ); <nl> if ( atomic_read (& root -> log_writers )) <nl> schedule (); <nl> - mutex_lock (& root -> log_mutex ); <nl> finish_wait (& root -> log_writer_wait , & wait ); <nl> + mutex_lock (& root -> log_mutex ); <nl> } <nl> } <nl> 
static void storvsc_handle_error ( struct vmscsi_request * vm_srb , <nl> do_work = true ; <nl> process_err_fn = storvsc_remove_lun ; <nl> break ; <nl> - case ( SRB_STATUS_ABORTED | SRB_STATUS_AUTOSENSE_VALID ): <nl> - if (( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> + case SRB_STATUS_ABORTED : <nl> + if ( vm_srb -> srb_status & SRB_STATUS_AUTOSENSE_VALID && <nl> + ( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> do_work = true ; <nl> process_err_fn = storvsc_device_scan ; <nl> /*
static struct iommu_table * vio_build_iommu_table ( struct vio_dev * dev ) <nl> return NULL ; <nl>  <nl> tbl = kmalloc ( sizeof (* tbl ), GFP_KERNEL ); <nl> + if ( tbl == NULL ) <nl> + return NULL ; <nl>  <nl> of_parse_dma_window ( dev -> dev . archdata . of_node , dma_window , <nl> & tbl -> it_index , & offset , & size );
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) <nl> vnt_mac_set_bssid_addr ( priv , ( u8 *) conf -> bssid ); <nl>  <nl> 
static int nl80211_trigger_scan ( struct sk_buff * skb , struct genl_info * info ) <nl> tmp ) { <nl> enum ieee80211_band band = nla_type ( attr ); <nl>  <nl> - if ( band < 0 || band > IEEE80211_NUM_BANDS ) { <nl> + if ( band < 0 || band >= IEEE80211_NUM_BANDS ) { <nl> err = - EINVAL ; <nl> goto out_free ; <nl> }
int ext3_group_add ( struct super_block * sb , struct ext3_new_group_data * input ) <nl> if ( input -> group != EXT3_SB ( sb )-> s_groups_count ) { <nl> ext3_warning ( sb , __FUNCTION__ , <nl> " multiple resizers run on filesystem !\ n "); <nl> + err = - EBUSY ; <nl> goto exit_journal ; <nl> } <nl> 
static enum BC_STATUS bc_cproc_download_fw ( struct crystalhd_cmd * ctx , <nl> sts = crystalhd_download_fw ( ctx -> adp , ( uint8_t *) idata -> add_cdata , <nl> idata -> add_cdata_sz ); <nl>  <nl> - if ( sts != BC_STS_SUCCESS ) { <nl> + if ( sts != BC_STS_SUCCESS ) <nl> BCMLOG_ERR (" Firmware Download Failure !! - % d \ n ", sts ); <nl> - } else <nl> + else <nl> ctx -> state |= BC_LINK_INIT ; <nl>  <nl> return sts ;
struct sk_buff * ieee80211_beacon_get ( struct ieee80211_hw * hw , <nl> " no rate found \ n ", <nl> wiphy_name ( local -> hw . wiphy )); <nl> } <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> skb = NULL ; <nl> goto out ; <nl> }
int devres_release_all ( struct device * dev ) <nl> { <nl> unsigned long flags ; <nl>  <nl> + /* Looks like an uninitialized device structure */ <nl> + if ( WARN_ON ( dev -> devres_head . next == NULL )) <nl> + return - ENODEV ; <nl> spin_lock_irqsave (& dev -> devres_lock , flags ); <nl> return release_nodes ( dev , dev -> devres_head . next , & dev -> devres_head , <nl> flags );
static int ibmvfc_map_sg_data ( struct scsi_cmnd * scmd , <nl> & evt -> ext_list_token ); <nl>  <nl> if (! evt -> ext_list ) { <nl> - scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> + scsi_dma_unmap ( scmd ); <nl> + if ( vhost -> log_level > IBMVFC_DEFAULT_LOG_LEVEL ) <nl> + scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> return - ENOMEM ; <nl> } <nl> }
static int __devinit tc35815_read_plat_dev_addr ( struct net_device * dev ) <nl> return - ENODEV ; <nl> } <nl> # else <nl> - static int __devinit tc35815_read_plat_dev_addr ( struct device * dev ) <nl> + static int __devinit tc35815_read_plat_dev_addr ( struct net_device * dev ) <nl> { <nl> return - ENODEV ; <nl> }
static int perf_exclude_event ( struct perf_event * event , <nl> struct pt_regs * regs ) <nl> { <nl> if ( event -> hw . state & PERF_HES_STOPPED ) <nl> - return 0 ; <nl> + return 1 ; <nl>  <nl> if ( regs ) { <nl> if ( event -> attr . exclude_user && user_mode ( regs ))
hw_perf_counter_init ( struct perf_counter * counter ) <nl> return NULL ; <nl> } <nl> events [ n ] = ev ; <nl> + ctrs [ n ] = counter ; <nl> if ( check_excludes ( ctrs , n , 1 )) <nl> return NULL ; <nl> if ( power_check_constraints ( events , n + 1 ))
cfq_update_io_seektime ( struct cfq_data * cfqd , struct cfq_io_context * cic , <nl> sector_t sdist ; <nl> u64 total ; <nl>  <nl> - if ( cic -> last_request_pos < rq -> sector ) <nl> + if (! cic -> last_request_pos ) <nl> + sdist = 0 ; <nl> + else if ( cic -> last_request_pos < rq -> sector ) <nl> sdist = rq -> sector - cic -> last_request_pos ; <nl> else <nl> sdist = cic -> last_request_pos - rq -> sector ;
flush_thread ( void ) <nl> ia32_drop_ia64_partial_page_list ( current ); <nl> current -> thread . task_size = IA32_PAGE_OFFSET ; <nl> set_fs ( USER_DS ); <nl> + memset ( current -> thread . tls_array , 0 , sizeof ( current -> thread . tls_array )); <nl> } <nl> # endif <nl> }
DT_MACHINE_START ( R8A7794_DT , " Generic R8A7794 ( Flattened Device Tree )") <nl> . init_early = shmobile_init_delay , <nl> . init_late = shmobile_init_late , <nl> . init_time = rcar_gen2_timer_init , <nl> + . reserve = rcar_gen2_reserve , <nl> . dt_compat = r8a7794_boards_compat_dt , <nl> MACHINE_END
int n_tty_ioctl ( struct tty_struct * tty , struct file * file , <nl> ld = tty_ldisc_ref ( tty ); <nl> switch ( arg ) { <nl> case TCIFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> break ; <nl> case TCIOFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> /* fall through */ <nl> case TCOFLUSH :
acpi_ex_load_op ( union acpi_operand_object * obj_desc , <nl>  <nl> /* table_ptr was deallocated above */ <nl>  <nl> + acpi_ut_remove_reference ( ddb_handle ); <nl> return_ACPI_STATUS ( status ); <nl> } <nl> 
static int sixpack_ioctl ( struct tty_struct * tty , struct file * file , <nl> unsigned int cmd , unsigned long arg ) <nl> { <nl> struct sixpack * sp = sp_get ( tty ); <nl> - struct net_device * dev = sp -> dev ; <nl> + struct net_device * dev ; <nl> unsigned int tmp , err ; <nl>  <nl> if (! sp ) <nl> return - ENXIO ; <nl> + dev = sp -> dev ; <nl>  <nl> switch ( cmd ) { <nl> case SIOCGIFNAME :
int vb2_mmap ( struct vb2_queue * q , struct vm_area_struct * vma ) <nl> { <nl> unsigned long off = vma -> vm_pgoff << PAGE_SHIFT ; <nl> struct vb2_buffer * vb ; <nl> - unsigned int buffer , plane ; <nl> + unsigned int buffer = 0 , plane = 0 ; <nl> int ret ; <nl> unsigned long length ; <nl> 
static int __init sh_pm_runtime_init ( void ) <nl> ! of_machine_is_compatible (" renesas , r8a7779 ") && <nl> ! of_machine_is_compatible (" renesas , r8a7790 ") && <nl> ! of_machine_is_compatible (" renesas , r8a7791 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7792 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7793 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7794 ") && <nl> ! of_machine_is_compatible (" renesas , sh7372 ") && <nl> ! of_machine_is_compatible (" renesas , sh73a0 ")) <nl> return 0 ;
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> - err = do_migrate_pages ( mm , & old , & new , MPOL_MF_MOVE ); <nl> + err = do_migrate_pages ( mm , & old , & new , <nl> + capable ( CAP_SYS_ADMIN ) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE ); <nl> out : <nl> mmput ( mm ); <nl> return err ;
static struct ieee80211_ops agnx_ops = { <nl> static void __devexit agnx_pci_remove ( struct pci_dev * pdev ) <nl> { <nl> struct ieee80211_hw * dev = pci_get_drvdata ( pdev ); <nl> - struct agnx_priv * priv = dev -> priv ; <nl> + struct agnx_priv * priv ; <nl> AGNX_TRACE ; <nl>  <nl> if (! dev ) <nl> return ; <nl> + priv = dev -> priv ; <nl> ieee80211_unregister_hw ( dev ); <nl> pci_iounmap ( pdev , priv -> ctl ); <nl> pci_iounmap ( pdev , priv -> data );
static void parse_system_parameter_string ( struct seq_file * m ) <nl> if (! workbuffer ) { <nl> printk ( KERN_ERR "% s % s kmalloc failure at line % d \ n ", <nl> __FILE__ , __FUNCTION__ , __LINE__ ); <nl> + kfree ( local_buffer ); <nl> return ; <nl> } <nl> # ifdef LPARCFG_DEBUG
int wl12xx_allocate_link ( struct wl1271 * wl , struct wl12xx_vif * wlvif , u8 * hlid ) <nl> __set_bit ( link , wl -> links_map ); <nl> __set_bit ( link , wlvif -> links_map ); <nl> spin_unlock_irqrestore (& wl -> wl_lock , flags ); <nl> + <nl> + /* take the last " freed packets " value from the current FW status */ <nl> + wl -> links [ link ]. prev_freed_pkts = <nl> + wl -> fw_status_2 -> counters . tx_lnk_free_pkts [ link ]; <nl> * hlid = link ; <nl> return 0 ; <nl> }
Configuration options : <nl> # define BCD 0x01 <nl>  <nl> # define ATAO_2_RTSISHFT 0x06 /* W 8 */ <nl> -# define RSI 0x01 <nl> +# define ATAO_RTSISHFT_RSI ( 1 << 0 ) <nl>  <nl> # define ATAO_2_RTSISTRB 0x07 /* W 8 */ <nl> 
static inline void dev_set_cma_area ( struct device * dev , struct cma * cma ) <nl> { <nl> if ( dev ) <nl> dev -> cma_area = cma ; <nl> - if (! dev || ! dma_contiguous_default_area ) <nl> + if (! dev && ! dma_contiguous_default_area ) <nl> dma_contiguous_default_area = cma ; <nl> } <nl> 
static const struct usb_device_id hso_ids [] = { <nl> { USB_DEVICE ( 0x0af0 , 0x8600 )}, <nl> { USB_DEVICE ( 0x0af0 , 0x8800 )}, <nl> { USB_DEVICE ( 0x0af0 , 0x8900 )}, <nl> + { USB_DEVICE ( 0x0af0 , 0x9000 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd035 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd055 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd155 )},
static void snd_als4000_configure ( struct snd_sb * chip ) <nl> /* SPECS_PAGE : 39 */ <nl> for ( i = ALS4K_GCR91_DMA0_ADDR ; i <= ALS4K_GCR96_DMA3_MODE_COUNT ; ++ i ) <nl> snd_als4k_gcr_write ( chip , i , 0 ); <nl> - <nl> + /* enable burst mode to prevent dropouts during high PCI bus usage */ <nl> snd_als4k_gcr_write ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL , <nl> - snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL )); <nl> + snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL ) | 0x04 ); <nl> spin_unlock_irq (& chip -> reg_lock ); <nl> } <nl> 
static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> int err ; <nl> unsigned int mss ; <nl>  <nl> + if ( packets == 0 ) <nl> + return ; <nl> + <nl> WARN_ON ( packets > tp -> packets_out ); <nl> if ( tp -> lost_skb_hint ) { <nl> skb = tp -> lost_skb_hint ;
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
int prism2_scan ( struct wiphy * wiphy , struct net_device * dev , <nl> msg1 . msgcode = DIDmsg_dot11req_scan ; <nl> msg1 . bsstype . data = P80211ENUM_bsstype_any ; <nl>  <nl> - memset (&( msg1 . bssid . data ), 0xFF , sizeof ( p80211item_pstr6_t )); <nl> + memset (& msg1 . bssid . data , 0xFF , sizeof ( msg1 . bssid . data )); <nl> msg1 . bssid . data . len = 6 ; <nl>  <nl> if ( request -> n_ssids > 0 ) {
static struct dmar_domain * dmar_insert_one_dev_info ( struct intel_iommu * iommu , <nl>  <nl> if ( ret ) { <nl> spin_unlock_irqrestore (& device_domain_lock , flags ); <nl> + free_devinfo_mem ( info ); <nl> return NULL ; <nl> } <nl> 
static void * i915_gem_dmabuf_vmap ( struct dma_buf * dma_buf ) <nl> goto error ; <nl>  <nl> i = 0 ; <nl> - for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ); <nl> + for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ) <nl> pages [ i ++] = sg_page_iter_page (& sg_iter ); <nl>  <nl> obj -> dma_buf_vmapping = vmap ( pages , i , 0 , PAGE_KERNEL );
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
xfs_log_force_umount ( <nl> * before we mark the filesystem SHUTDOWN and wake <nl> * everybody up to tell the bad news . <nl> */ <nl> - spin_lock (& log -> l_grant_lock ); <nl> spin_lock (& log -> l_icloglock ); <nl> + spin_lock (& log -> l_grant_lock ); <nl> mp -> m_flags |= XFS_MOUNT_FS_SHUTDOWN ; <nl> XFS_BUF_DONE ( mp -> m_sb_bp ); <nl> /*
static const __u8 uvc_camera_guid [ 16 ] = UVC_GUID_UVC_CAMERA ; <nl> static const __u8 uvc_media_transport_input_guid [ 16 ] = <nl> UVC_GUID_UVC_MEDIA_TRANSPORT_INPUT ; <nl>  <nl> - static int uvc_entity_match_guid ( struct uvc_entity * entity , __u8 guid [ 16 ]) <nl> + static int uvc_entity_match_guid ( const struct uvc_entity * entity , <nl> + const __u8 guid [ 16 ]) <nl> { <nl> switch ( UVC_ENTITY_TYPE ( entity )) { <nl> case UVC_ITT_CAMERA :
int seq_bitmap ( struct seq_file * m , const unsigned long * bits , <nl> unsigned int nr_bits ); <nl> static inline int seq_cpumask ( struct seq_file * m , const struct cpumask * mask ) <nl> { <nl> - return seq_bitmap ( m , mask -> bits , NR_CPUS ); <nl> + return seq_bitmap ( m , mask -> bits , nr_cpu_ids ); <nl> } <nl>  <nl> static inline int seq_nodemask ( struct seq_file * m , nodemask_t * mask )
void __init pxa_init_irq_gpio ( int gpio_nr ) <nl> set_irq_flags ( irq , IRQF_VALID | IRQF_PROBE ); <nl> } <nl>  <nl> - for ( irq = IRQ_GPIO ( 2 ); irq <= IRQ_GPIO ( gpio_nr ); irq ++) { <nl> + for ( irq = IRQ_GPIO ( 2 ); irq < IRQ_GPIO ( gpio_nr ); irq ++) { <nl> set_irq_chip ( irq , & pxa_muxed_gpio_chip ); <nl> set_irq_handler ( irq , handle_edge_irq ); <nl> set_irq_flags ( irq , IRQF_VALID | IRQF_PROBE );
static int soc_hw_match_params ( struct snd_pcm_substream * substream , <nl> /* normalise cpu bfs div & codec const mult */ <nl> codec_bfs = soc_bfs_rate_to_div ( codec_dai_mode -> bfs , rate , <nl> mclk , rtd -> codec_dai -> dai_runtime . pcmfmt , chn ); <nl> - if ( codec_dai_mode -> bfs & codec_bfs ) { <nl> + if ( cpu_dai_mode -> bfs & codec_bfs ) { <nl> rtd -> cpu_dai -> dai_runtime . bfs = codec_bfs ; <nl> rtd -> codec_dai -> dai_runtime . bfs = codec_dai_mode -> bfs ; <nl> } else
void __update_cache ( struct vm_area_struct * vma , <nl> return ; <nl>  <nl> page = pfn_to_page ( pfn ); <nl> - if ( pfn_valid ( pfn ) && page_mapping ( page )) { <nl> + if ( pfn_valid ( pfn )) { <nl> int dirty = test_and_clear_bit ( PG_dcache_dirty , & page -> flags ); <nl> if ( dirty ) { <nl> unsigned long addr = ( unsigned long ) page_address ( page );
static void w9966_term ( struct w9966 * cam ) <nl> parport_unregister_device ( cam -> pdev ); <nl> w9966_set_state ( cam , W9966_STATE_PDEV , 0 ); <nl> } <nl> + memset ( cam , 0 , sizeof (* cam )); <nl> } <nl>  <nl> 
static ssize_t cpufv_store ( struct device * dev , <nl> return rv ; <nl> if ( value < 0 || value >= c . num ) <nl> return - EINVAL ; <nl> - set_acpi ( eeepc , CM_ASL_CPUFV , value ); <nl> + rv = set_acpi ( eeepc , CM_ASL_CPUFV , value ); <nl> + if ( rv ) <nl> + return rv ; <nl> return count ; <nl> } <nl> 
static __init void nested_vmx_setup_ctls_msrs ( void ) <nl> /* nested EPT : emulate EPT also to L1 */ <nl> nested_vmx_secondary_ctls_high |= SECONDARY_EXEC_ENABLE_EPT ; <nl> nested_vmx_ept_caps = VMX_EPT_PAGE_WALK_4_BIT | <nl> - VMX_EPTP_WB_BIT | VMX_EPT_INVEPT_BIT ; <nl> + VMX_EPTP_WB_BIT | VMX_EPT_2MB_PAGE_BIT | <nl> + VMX_EPT_INVEPT_BIT ; <nl> nested_vmx_ept_caps &= vmx_capability . ept ; <nl> /* <nl> * Since invept is completely emulated we support both global
static int option_inquiry ( struct us_data * us ) <nl>  <nl> result = memcmp ( buffer + 8 , " Option ", 6 ); <nl>  <nl> + if ( result != 0 ) <nl> + result = memcmp ( buffer + 8 , " ZCOPTION ", 8 ); <nl> + <nl> /* Read the CSW */ <nl> usb_stor_bulk_transfer_buf ( us , <nl> us -> recv_bulk_pipe ,
static void dwc3_endpoint_interrupt ( struct dwc3 * dwc , <nl>  <nl> dep = dwc -> eps [ epnum ]; <nl>  <nl> + if (!( dep -> flags & DWC3_EP_ENABLED )) <nl> + return ; <nl> + <nl> dev_vdbg ( dwc -> dev , "% s : % s \ n ", dep -> name , <nl> dwc3_ep_event_string ( event -> endpoint_event )); <nl> 
struct e1000_nvm_operations { <nl>  <nl> struct e1000_mac_info { <nl> struct e1000_mac_operations ops ; <nl> - <nl> - u8 addr [ 6 ]; <nl> - u8 perm_addr [ 6 ]; <nl> + u8 addr [ ETH_ALEN ]; <nl> + u8 perm_addr [ ETH_ALEN ]; <nl>  <nl> enum e1000_mac_type type ; <nl> 
static unsigned int br_nf_pre_routing ( unsigned int hook , struct sk_buff * skb , <nl>  <nl> pskb_trim_rcsum ( skb , len ); <nl>  <nl> + /* BUG : Should really parse the IP options here . */ <nl> + memset ( IPCB ( skb ), 0 , sizeof ( struct inet_skb_parm )); <nl> + <nl> nf_bridge_put ( skb -> nf_bridge ); <nl> if (! nf_bridge_alloc ( skb )) <nl> return NF_DROP ;
static void das16m1_handler ( struct comedi_device * dev , unsigned int status ) <nl> num_samples = FIFO_SIZE ; <nl> insw ( dev -> iobase , devpriv -> ai_buffer , num_samples ); <nl> munge_sample_array ( devpriv -> ai_buffer , num_samples ); <nl> - cfc_write_array_to_buffer ( s , devpriv -> ai_buffer , <nl> - num_samples * sizeof ( short )); <nl> + comedi_buf_write_samples ( s , devpriv -> ai_buffer , num_samples ); <nl> devpriv -> adc_count += num_samples ; <nl>  <nl> if ( cmd -> stop_src == TRIG_COUNT ) {
void thread__delete ( struct thread * thread ) <nl> { <nl> struct comm * comm , * tmp ; <nl>  <nl> - map_groups__put ( thread -> mg ); <nl> - thread -> mg = NULL ; <nl> + if ( thread -> mg ) { <nl> + map_groups__put ( thread -> mg ); <nl> + thread -> mg = NULL ; <nl> + } <nl> list_for_each_entry_safe ( comm , tmp , & thread -> comm_list , list ) { <nl> list_del (& comm -> list ); <nl> comm__free ( comm );
static int tda10071_get_frontend ( struct dvb_frontend * fe ) <nl> if ( ret ) <nl> goto error ; <nl>  <nl> - c -> symbol_rate = ( buf [ 0 ] << 16 ) | ( buf [ 1 ] << 8 ) | ( buf [ 2 ] << 0 ); <nl> + c -> symbol_rate = (( buf [ 0 ] << 16 ) | ( buf [ 1 ] << 8 ) | ( buf [ 2 ] << 0 )) * 1000 ; <nl>  <nl> return ret ; <nl> error :
void * __init alloc_large_system_hash ( const char * tablename , <nl> table = __vmalloc ( size , GFP_ATOMIC , PAGE_KERNEL ); <nl> else { <nl> unsigned long order = get_order ( size ); <nl> - table = ( void *) __get_free_pages ( GFP_ATOMIC , order ); <nl> + <nl> + if ( order < MAX_ORDER ) <nl> + table = ( void *) __get_free_pages ( GFP_ATOMIC , <nl> + order ); <nl> /* <nl> * If bucketsize is not a power - of - two , we may free <nl> * some pages at the end of hash table .
void sctp_transport_burst_limited ( struct sctp_transport * t ) <nl> u32 old_cwnd = t -> cwnd ; <nl> u32 max_burst_bytes ; <nl>  <nl> - if ( t -> burst_limited ) <nl> + if ( t -> burst_limited || asoc -> max_burst == 0 ) <nl> return ; <nl>  <nl> max_burst_bytes = t -> flight_size + ( asoc -> max_burst * asoc -> pathmtu );
int do_sys_settimeofday ( struct timespec * tv , struct timezone * tz ) <nl> static int firsttime = 1 ; <nl> int error = 0 ; <nl>  <nl> + if (! timespec_valid ( tv )) <nl> + return - EINVAL ; <nl> + <nl> error = security_settime ( tv , tz ); <nl> if ( error ) <nl> return error ;
do { \ <nl> if ( EXT4_FITS_IN_INODE ( raw_inode , einode , xtime )) \ <nl> ( einode )-> xtime . tv_sec = \ <nl> ( signed ) le32_to_cpu (( raw_inode )-> xtime ); \ <nl> + else \ <nl> + ( einode )-> xtime . tv_sec = 0 ; \ <nl> if ( EXT4_FITS_IN_INODE ( raw_inode , einode , xtime ## _extra )) \ <nl> ext4_decode_extra_time (&( einode )-> xtime , \ <nl> raw_inode -> xtime ## _extra ); \
static int snd_pcm_update_hw_ptr0 ( struct snd_pcm_substream * substream , <nl> new_hw_ptr = hw_base + pos ; <nl> } <nl> __delta : <nl> - delta = ( new_hw_ptr - old_hw_ptr ) % runtime -> boundary ; <nl> + delta = new_hw_ptr - old_hw_ptr ; <nl> + if ( delta < 0 ) <nl> + delta += runtime -> boundary ; <nl> if ( xrun_debug ( substream , in_interrupt ? <nl> XRUN_DEBUG_PERIODUPDATE : XRUN_DEBUG_HWPTRUPDATE )) { <nl> char name [ 16 ];
static void vmlinux_path__exit ( void ) <nl> { <nl> while (-- vmlinux_path__nr_entries >= 0 ) <nl> zfree (& vmlinux_path [ vmlinux_path__nr_entries ]); <nl> + vmlinux_path__nr_entries = 0 ; <nl>  <nl> zfree (& vmlinux_path ); <nl> }
static void netlink_rcv_cb ( struct sk_buff * skb ) <nl> if ( skb -> len >= NLMSG_HDRLEN ) { <nl> nlh = ( struct nlmsghdr *) skb -> data ; <nl>  <nl> - if ( skb -> len < nlh -> nlmsg_len || <nl> + if ( nlh -> nlmsg_len < ND_IFINDEX_LEN || <nl> + nlh -> nlmsg_len > skb -> len || <nl> nlh -> nlmsg_len > ND_MAX_MSG_LEN ) { <nl> netdev_err ( skb -> dev , " Invalid length (% d ,% d )\ n ", <nl> skb -> len , nlh -> nlmsg_len );
static void __devexit slic_entry_remove ( struct pci_dev * pcidev ) <nl> } <nl> free_netdev ( dev ); <nl> pci_release_regions ( pcidev ); <nl> + pci_disable_device ( pcidev ); <nl> } <nl>  <nl> static int slic_entry_halt ( struct net_device * dev )
static int parse_opts ( char * params , struct p9_fd_opts * opts ) <nl> opts -> port = P9_PORT ; <nl> opts -> rfd = ~ 0 ; <nl> opts -> wfd = ~ 0 ; <nl> + opts -> privport = 0 ; <nl>  <nl> if (! params ) <nl> return 0 ;
static inline void run_hrtimer_queue ( struct hrtimer_base * base ) <nl> { <nl> struct rb_node * node ; <nl>  <nl> + if (! base -> first ) <nl> + return ; <nl> + <nl> if ( base -> get_softirq_time ) <nl> base -> softirq_time = base -> get_softirq_time (); <nl> 
static struct sk_buff * be_insert_vlan_in_pkt ( struct be_adapter * adapter , <nl>  <nl> if ( vlan_tx_tag_present ( skb )) { <nl> vlan_tag = be_get_tx_vlan_tag ( adapter , skb ); <nl> - __vlan_put_tag ( skb , vlan_tag ); <nl> - skb -> vlan_tci = 0 ; <nl> + skb = __vlan_put_tag ( skb , vlan_tag ); <nl> + if ( skb ) <nl> + skb -> vlan_tci = 0 ; <nl> } <nl>  <nl> return skb ;
int ieee80211_request_sched_scan_start ( struct ieee80211_sub_if_data * sdata , <nl> for ( i = 0 ; i < IEEE80211_NUM_BANDS ; i ++) { <nl> local -> sched_scan_ies . ie [ i ] = kzalloc ( 2 + <nl> IEEE80211_MAX_SSID_LEN + <nl> - local -> scan_ies_len , <nl> + local -> scan_ies_len + <nl> + req -> ie_len , <nl> GFP_KERNEL ); <nl> if (! local -> sched_scan_ies . ie [ i ]) { <nl> ret = - ENOMEM ;
mt7601u_extra_power_over_mac ( struct mt7601u_dev * dev ) <nl> static void <nl> mt7601u_set_power_rate ( struct power_per_rate * rate , s8 delta , u8 value ) <nl> { <nl> + /* Invalid ? Note : vendor driver does not handle this */ <nl> + if ( value == 0xff ) <nl> + return ; <nl> + <nl> rate -> raw = s6_validate ( value ); <nl> rate -> bw20 = s6_to_int ( value ); <nl> /* Note : vendor driver does cap the value to s6 right away */
INTEL_VGA_DEVICE ( 0x191D , info ) /* WKS GT2 */ <nl>  <nl> # define INTEL_SKL_GT3_IDS ( info ) \ <nl> + INTEL_VGA_DEVICE ( 0x1923 , info ), /* ULT GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x1926 , info ), /* ULT GT3 */ \ <nl> + INTEL_VGA_DEVICE ( 0x1927 , info ), /* ULT GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x192B , info ), /* Halo GT3 */ \ <nl> INTEL_VGA_DEVICE ( 0x192A , info ) /* SRV GT3 */ <nl> 
void print2byte ( int input , struct iio_channel_info * info ) <nl> * Shift before conversion to avoid sign extension <nl> * of left aligned data <nl> */ <nl> - input = input >> info -> shift ; <nl> + input >>= info -> shift ; <nl> if ( info -> is_signed ) { <nl> int16_t val = input ; <nl> 
void omap_gem_init ( struct drm_device * dev ) <nl> } <nl>  <nl> usergart = kzalloc ( 3 * sizeof (* usergart ), GFP_KERNEL ); <nl> + if (! usergart ) { <nl> + dev_warn ( dev -> dev , " could not allocate usergart \ n "); <nl> + return ; <nl> + } <nl>  <nl> /* reserve 4k aligned / wide regions for userspace mappings : */ <nl> for ( i = 0 ; i < ARRAY_SIZE ( fmts ); i ++) {
int spu_acquire_runnable ( struct spu_context * ctx ) <nl>  <nl> if ( ctx -> state == SPU_STATE_SAVED ) { <nl> ret = spu_activate ( ctx , 0 ); <nl> + if ( ret ) <nl> + goto out ; <nl> ctx -> state = SPU_STATE_RUNNABLE ; <nl> } <nl> - if ( ret ) <nl> - goto out ; <nl>  <nl> downgrade_write (& ctx -> state_sema ); <nl> /* On success , we return holding the lock */
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> + if ( count > XFS_ACL_MAX_ENTRIES ) <nl> + return ERR_PTR (- EFSCORRUPTED ); <nl>  <nl> acl = posix_acl_alloc ( count , GFP_KERNEL ); <nl> if (! acl )
nvc0_fb_init ( struct drm_device * dev ) <nl> priv = dev_priv -> engine . fb . priv ; <nl>  <nl> nv_wr32 ( dev , 0x100c10 , priv -> r100c10 >> 8 ); <nl> + nv_mask ( dev , 0x17e820 , 0x00100000 , 0x00000000 ); /* NV_PLTCG_INTR_EN */ <nl> return 0 ; <nl> } <nl> 
static struct sk_buff * ieee80211_build_hdr ( struct ieee80211_sub_if_data * sdata , <nl> authorized = test_sta_flag ( sta , WLAN_STA_AUTHORIZED ); <nl> wme_sta = sta -> sta . wme ; <nl> have_station = true ; <nl> + } else if ( sdata -> wdev . use_4addr ) { <nl> + ret = - ENOLINK ; <nl> + goto free ; <nl> } <nl> ap_sdata = container_of ( sdata -> bss , struct ieee80211_sub_if_data , <nl> u . ap );
int cx25821_video_register ( struct cx25821_dev * dev ) <nl>  <nl> spin_lock_init (& dev -> slock ); <nl>  <nl> - for ( i = 0 ; i < MAX_VID_CHANNEL_NUM - 1 ; ++ i ) { <nl> + for ( i = 0 ; i < VID_CHANNEL_NUM ; ++ i ) { <nl> cx25821_init_controls ( dev , i ); <nl>  <nl> cx25821_risc_stopper ( dev -> pci , & dev -> channels [ i ]. vidq . stopper ,
static int dtv_property_legacy_params_sync ( struct dvb_frontend * fe , <nl>  <nl> static bool has_get_frontend ( struct dvb_frontend * fe ) <nl> { <nl> - return fe -> ops . get_frontend ; <nl> + return fe -> ops . get_frontend != NULL ; <nl> } <nl>  <nl> /*
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> AAC_OPT_NEW_COMM ) ? <nl> ( dev -> scsi_host_ptr -> max_sectors << 9 ) : <nl> 65536 )) { <nl> + kfree ( usg ); <nl> rcode = - EINVAL ; <nl> goto cleanup ; <nl> }
static void __call_console_drivers ( unsigned long start , unsigned long end ) <nl>  <nl> static int __read_mostly ignore_loglevel ; <nl>  <nl> - int __init ignore_loglevel_setup ( char * str ) <nl> + static int __init ignore_loglevel_setup ( char * str ) <nl> { <nl> ignore_loglevel = 1 ; <nl> printk ( KERN_INFO " debug : ignoring loglevel setting .\ n ");
static void __cpuinit early_init_intel ( struct cpuinfo_x86 * c ) <nl> { <nl> /* Unmask CPUID levels if masked : */ <nl> - if ( c -> x86 == 6 && c -> x86_model >= 15 ) { <nl> + if ( c -> x86 > 6 || ( c -> x86 == 6 && c -> x86_model >= 0xd )) { <nl> u64 misc_enable ; <nl>  <nl> rdmsrl ( MSR_IA32_MISC_ENABLE , misc_enable );
static int snd_rme32_capture_close ( struct snd_pcm_substream * substream ) <nl> spin_lock_irq (& rme32 -> lock ); <nl> rme32 -> capture_substream = NULL ; <nl> rme32 -> capture_periodsize = 0 ; <nl> - spin_unlock (& rme32 -> lock ); <nl> + spin_unlock_irq (& rme32 -> lock ); <nl> return 0 ; <nl> } <nl> 
static int xhci_setup_device ( struct usb_hcd * hcd , struct usb_device * udev , <nl>  <nl> mutex_lock (& xhci -> mutex ); <nl>  <nl> + if ( xhci -> xhc_state ) /* dying or halted */ <nl> + goto out ; <nl> + <nl> if (! udev -> slot_id ) { <nl> xhci_dbg_trace ( xhci , trace_xhci_dbg_address , <nl> " Bad Slot ID % d ", udev -> slot_id );
static void r600_cs_parser_fini ( struct radeon_cs_parser * parser , int error ) <nl> kfree ( parser -> relocs ); <nl> for ( i = 0 ; i < parser -> nchunks ; i ++) { <nl> kfree ( parser -> chunks [ i ]. kdata ); <nl> + kfree ( parser -> chunks [ i ]. kpage [ 0 ]); <nl> + kfree ( parser -> chunks [ i ]. kpage [ 1 ]); <nl> } <nl> kfree ( parser -> chunks ); <nl> kfree ( parser -> chunks_array );
static int show_numa_map ( struct seq_file * m , void * v , int is_pid ) <nl> walk . mm = mm ; <nl>  <nl> pol = get_vma_policy ( task , vma , vma -> vm_start ); <nl> - mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> + n = mpol_to_str ( buffer , sizeof ( buffer ), pol ); <nl> mpol_cond_put ( pol ); <nl> + if ( n < 0 ) <nl> + return n ; <nl>  <nl> seq_printf ( m , "% 08lx % s ", vma -> vm_start , buffer ); <nl> 
static int __init omapdss_hdmihw_probe ( struct platform_device * pdev ) <nl> hdmi . ip_data . pll_offset = HDMI_PLLCTRL ; <nl> hdmi . ip_data . phy_offset = HDMI_PHY ; <nl>  <nl> + hdmi_init_output ( pdev ); <nl> + <nl> r = hdmi_panel_init (); <nl> if ( r ) { <nl> DSSERR (" can ' t init panel \ n "); <nl> static int __init omapdss_hdmihw_probe ( struct platform_device * pdev ) <nl>  <nl> dss_debugfs_create_file (" hdmi ", hdmi_dump_regs ); <nl>  <nl> - hdmi_init_output ( pdev ); <nl> - <nl> hdmi_probe_pdata ( pdev ); <nl>  <nl> return 0 ;
static int pl2303_tiocmset ( struct tty_struct * tty , <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl>  <nl> mutex_lock (& serial -> disc_mutex ); <nl> - if (! serial -> disconnected ) <nl> + if (! serial -> disconnected ) { <nl> ret = pl2303_set_control_lines ( port , control ); <nl> - else <nl> + if ( ret ) <nl> + ret = usb_translate_errors ( ret ); <nl> + } else { <nl> ret = - ENODEV ; <nl> + } <nl> mutex_unlock (& serial -> disc_mutex ); <nl>  <nl> return ret ;
int ip6_del_rt ( struct rt6_info * rt , struct nlmsghdr * nlh , void * _rtattr , struct <nl> int err ; <nl> struct fib6_table * table ; <nl>  <nl> + if ( rt == & ip6_null_entry ) <nl> + return - ENOENT ; <nl> + <nl> table = rt -> rt6i_table ; <nl> write_lock_bh (& table -> tb6_lock ); <nl> 
int perf_event_init_context ( struct task_struct * child , int ctxn ) <nl> * swapped under us . <nl> */ <nl> parent_ctx = perf_pin_task_context ( parent , ctxn ); <nl> + if (! parent_ctx ) <nl> + return 0 ; <nl>  <nl> /* <nl> * No need to check if parent_ctx != NULL here ; since we saw
recurse : <nl> if ( err ) <nl> goto out_free ; <nl>  <nl> - kobject_uevent (& b -> kobj , KOBJ_ADD ); <nl> + if ( b ) <nl> + kobject_uevent (& b -> kobj , KOBJ_ADD ); <nl>  <nl> return err ; <nl> 
static int get_info ( struct net * net , void __user * user , <nl> private = & tmp ; <nl> } <nl> # endif <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . valid_hooks = t -> valid_hooks ; <nl> memcpy ( info . hook_entry , private -> hook_entry , <nl> sizeof ( info . hook_entry ));
int ehci_suspend ( struct usb_hcd * hcd , bool do_wakeup ) <nl> clear_bit ( HCD_FLAG_HW_ACCESSIBLE , & hcd -> flags ); <nl> spin_unlock_irq (& ehci -> lock ); <nl>  <nl> + synchronize_irq ( hcd -> irq ); <nl> + <nl> + /* Check for race with a wakeup request */ <nl> + if ( do_wakeup && HCD_WAKEUP_PENDING ( hcd )) { <nl> + ehci_resume ( hcd , false ); <nl> + return - EBUSY ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( ehci_suspend );
static int ath9k_htc_aggr_oper ( struct ath9k_htc_priv * priv , <nl> int ret = 0 ; <nl> u8 cmd_rsp ; <nl>  <nl> - if ( tid > ATH9K_HTC_MAX_TID ) <nl> + if ( tid >= ATH9K_HTC_MAX_TID ) <nl> return - EINVAL ; <nl>  <nl> memset (& aggr , 0 , sizeof ( struct ath9k_htc_target_aggr ));
int lapic_watchdog_init ( unsigned nmi_hz ) <nl> probe_nmi_watchdog (); <nl> if (! wd_ops ) <nl> return - 1 ; <nl> + <nl> + if (! wd_ops -> reserve ()) { <nl> + printk ( KERN_ERR <nl> + " NMI watchdog : cannot reserve perfctrs \ n "); <nl> + return - 1 ; <nl> + } <nl> } <nl>  <nl> if (!( wd_ops -> setup ( nmi_hz ))) {
struct btrfs_root * btrfs_create_tree ( struct btrfs_trans_handle * trans , <nl> fail : <nl> if ( leaf ) { <nl> btrfs_tree_unlock ( leaf ); <nl> + free_extent_buffer ( root -> commit_root ); <nl> free_extent_buffer ( leaf ); <nl> } <nl> kfree ( root );
static struct crypto_instance * crypto_cts_alloc ( struct rtattr ** tb ) <nl> if (! is_power_of_2 ( alg -> cra_blocksize )) <nl> goto out_put_alg ; <nl>  <nl> + if ( strncmp ( alg -> cra_name , " cbc (", 4 )) <nl> + goto out_put_alg ; <nl> + <nl> inst = crypto_alloc_instance (" cts ", alg ); <nl> if ( IS_ERR ( inst )) <nl> goto out_put_alg ;
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> loff_t file_size ; <nl> unsigned int num ; <nl> unsigned int offset ; <nl> - size_t total_len ; <nl> + size_t total_len = 0 ; <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req ))
int ip6_xmit ( struct sock * sk , struct sk_buff * skb , struct flowi * fl , <nl> skb_reset_network_header ( skb ); <nl> hdr = ipv6_hdr ( skb ); <nl>  <nl> + /* Allow local fragmentation . */ <nl> + if ( ipfragok ) <nl> + skb -> local_df = 1 ; <nl> + <nl> /* <nl> * Fill in the IPv6 header <nl> */
proc_dodebug ( ctl_table * table , int write , struct file * file , <nl> left --, s ++; <nl> *( unsigned int *) table -> data = value ; <nl> /* Display the RPC tasks on writing to rpc_debug */ <nl> - if ( table -> ctl_name == CTL_RPCDEBUG ) { <nl> + if ( strcmp ( table -> procname , " rpc_debug ") == 0 ) <nl> rpc_show_tasks (); <nl> - } <nl> } else { <nl> if (! access_ok ( VERIFY_WRITE , buffer , left )) <nl> return - EFAULT ;
struct ieee80211_hw * ieee80211_alloc_hw ( size_t priv_data_len , <nl> local -> uapsd_max_sp_len = IEEE80211_DEFAULT_MAX_SP_LEN ; <nl>  <nl> INIT_LIST_HEAD (& local -> interfaces ); <nl> + <nl> + __hw_addr_init (& local -> mc_list ); <nl> + <nl> mutex_init (& local -> iflist_mtx ); <nl> mutex_init (& local -> scan_mtx ); <nl> 
static void resample_shrink ( struct snd_pcm_plugin * plugin , <nl> while ( dst_frames1 > 0 ) { <nl> S1 = S2 ; <nl> if ( src_frames1 -- > 0 ) { <nl> - S1 = * src ; <nl> + S2 = * src ; <nl> src += src_step ; <nl> } <nl> if ( pos & ~ R_MASK ) {
static int sh_eth_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> skb -> len + 2 ); <nl> txdesc -> addr = dma_map_single (& ndev -> dev , skb -> data , skb -> len , <nl> DMA_TO_DEVICE ); <nl> + if ( dma_mapping_error (& ndev -> dev , txdesc -> addr )) { <nl> + kfree_skb ( skb ); <nl> + return NETDEV_TX_OK ; <nl> + } <nl> txdesc -> buffer_length = skb -> len ; <nl>  <nl> if ( entry >= mdp -> num_tx_ring - 1 )
static int snd_ctl_elem_add ( struct snd_ctl_file * file , <nl>  <nl> if ( card -> user_ctl_count >= MAX_USER_CONTROLS ) <nl> return - ENOMEM ; <nl> - if ( info -> count > 1024 ) <nl> + if ( info -> count < 1 ) <nl> return - EINVAL ; <nl> access = info -> access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE : <nl> ( info -> access & ( SNDRV_CTL_ELEM_ACCESS_READWRITE |
int intel_framebuffer_init ( struct drm_device * dev , <nl> switch ( mode_cmd -> bpp ) { <nl> case 8 : <nl> case 16 : <nl> + /* Only pre - ILK can handle 5 : 5 : 5 */ <nl> + if ( mode_cmd -> depth == 15 && ! HAS_PCH_SPLIT ( dev )) <nl> + return - EINVAL ; <nl> + break ; <nl> + <nl> case 24 : <nl> case 32 : <nl> break ;
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
static int filter_ack ( struct ieee80211_hw * hw , struct ieee80211_hdr * rx_hdr , <nl> struct ieee80211_hdr * tx_hdr ; <nl>  <nl> tx_hdr = ( struct ieee80211_hdr *) skb -> data ; <nl> - if ( likely (! compare_ether_addr ( tx_hdr -> addr2 , rx_hdr -> addr1 ))) <nl> + if ( likely (! memcmp ( tx_hdr -> addr2 , rx_hdr -> addr1 , ETH_ALEN ))) <nl> { <nl> __skb_unlink ( skb , q ); <nl> tx_status ( hw , skb , IEEE80211_TX_STAT_ACK , stats -> signal , 1 );
static int __init lart_flash_init ( void ) <nl> mtd . name = module_name ; <nl> mtd . type = MTD_NORFLASH ; <nl> mtd . writesize = 1 ; <nl> + mtd . writebufsize = 4 ; <nl> mtd . flags = MTD_CAP_NORFLASH ; <nl> mtd . size = FLASH_BLOCKSIZE_PARAM * FLASH_NUMBLOCKS_16m_PARAM + FLASH_BLOCKSIZE_MAIN * FLASH_NUMBLOCKS_16m_MAIN ; <nl> mtd . erasesize = FLASH_BLOCKSIZE_MAIN ;
int radeon_ring_alloc ( struct radeon_device * rdev , struct radeon_ring * ring , unsi <nl> if ( ndw < ring -> ring_free_dw ) { <nl> break ; <nl> } <nl> + mutex_unlock (& ring -> mutex ); <nl> r = radeon_fence_wait_next ( rdev , radeon_ring_index ( rdev , ring )); <nl> + mutex_lock (& ring -> mutex ); <nl> if ( r ) <nl> return r ; <nl> }
static int ixgbe_rcv_msg_from_vf ( struct ixgbe_adapter * adapter , u32 vf ) <nl> } <nl> break ; <nl> case IXGBE_VF_SET_MACVLAN : <nl> + if ( adapter -> vfinfo [ vf ]. pf_set_mac ) { <nl> + e_warn ( drv , " VF % d requested MACVLAN filter but is " <nl> + " administratively denied \ n ", vf ); <nl> + retval = - 1 ; <nl> + break ; <nl> + } <nl> index = ( msgbuf [ 0 ] & IXGBE_VT_MSGINFO_MASK ) >> <nl> IXGBE_VT_MSGINFO_SHIFT ; <nl> /*
__visible void prepare_exit_to_usermode ( struct pt_regs * regs ) <nl> READ_ONCE ( pt_regs_to_thread_info ( regs )-> flags ); <nl>  <nl> if (!( cached_flags & ( _TIF_SIGPENDING | _TIF_NOTIFY_RESUME | <nl> - _TIF_UPROBE | _TIF_NEED_RESCHED ))) <nl> + _TIF_UPROBE | _TIF_NEED_RESCHED | <nl> + _TIF_USER_RETURN_NOTIFY ))) <nl> break ; <nl>  <nl> /* We have work to do . */
again : <nl> case STAC_DELL_M4_3 : <nl> spec -> num_dmics = 1 ; <nl> spec -> num_smuxes = 0 ; <nl> - spec -> num_dmuxes = 0 ; <nl> + spec -> num_dmuxes = 1 ; <nl> break ; <nl> default : <nl> spec -> num_dmics = STAC92HD71BXX_NUM_DMICS ;
static int __init usba_udc_probe ( struct platform_device * pdev ) <nl> usba_writel ( udc , CTRL , USBA_DISABLE_MASK ); <nl> clk_disable ( pclk ); <nl>  <nl> - usba_ep = kmalloc ( sizeof ( struct usba_ep ) * pdata -> num_ep , <nl> + usba_ep = kzalloc ( sizeof ( struct usba_ep ) * pdata -> num_ep , <nl> GFP_KERNEL ); <nl> if (! usba_ep ) <nl> goto err_alloc_ep ;
vpfe_get_pdata ( struct platform_device * pdev ) <nl> pdata -> asd [ i ] = devm_kzalloc (& pdev -> dev , <nl> sizeof ( struct v4l2_async_subdev ), <nl> GFP_KERNEL ); <nl> + if (! pdata -> asd [ i ]) { <nl> + of_node_put ( rem ); <nl> + pdata = NULL ; <nl> + goto done ; <nl> + } <nl> + <nl> pdata -> asd [ i ]-> match_type = V4L2_ASYNC_MATCH_OF ; <nl> pdata -> asd [ i ]-> match . of . node = rem ; <nl> of_node_put ( endpoint );
static bool snd_soc_set_cache_val ( void * base , unsigned int idx , <nl> static unsigned int snd_soc_get_cache_val ( const void * base , unsigned int idx , <nl> unsigned int word_size ) <nl> { <nl> + if (! base ) <nl> + return - 1 ; <nl> + <nl> switch ( word_size ) { <nl> case 1 : { <nl> const u8 * cache = base ;
static int btrfs_show_devname ( struct seq_file * m , struct dentry * root ) <nl> list_for_each_entry ( dev , head , dev_list ) { <nl> if ( dev -> missing ) <nl> continue ; <nl> + if (! dev -> name ) <nl> + continue ; <nl> if (! first_dev || dev -> devid < first_dev -> devid ) <nl> first_dev = dev ; <nl> }
EXPORT_SYMBOL_GPL ( regmap_irq_chip_get_base ); <nl> */ <nl> int regmap_irq_get_virq ( struct regmap_irq_chip_data * data , int irq ) <nl> { <nl> + /* Handle holes in the IRQ list */ <nl> + if (! data -> chip -> irqs [ irq ]. mask ) <nl> + return - EINVAL ; <nl> + <nl> return irq_create_mapping ( data -> domain , irq ); <nl> } <nl> EXPORT_SYMBOL_GPL ( regmap_irq_get_virq );
static int drbd_accept ( const char ** what , struct socket * sock , struct socket ** n <nl> goto out ; <nl> } <nl> (* newsock )-> ops = sock -> ops ; <nl> + __module_get ((* newsock )-> ops -> owner ); <nl>  <nl> out : <nl> return err ;
static __inline__ int rt6_check_expired ( const struct rt6_info * rt ) <nl> static inline int rt6_need_strict ( struct in6_addr * daddr ) <nl> { <nl> return ( ipv6_addr_type ( daddr ) & <nl> - ( IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL )); <nl> + ( IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL | IPV6_ADDR_LOOPBACK )); <nl> } <nl>  <nl> /*
static u64 bpf_get_current_comm ( u64 r1 , u64 size , u64 r3 , u64 r4 , u64 r5 ) <nl> if (! task ) <nl> return - EINVAL ; <nl>  <nl> - memcpy ( buf , task -> comm , min_t ( size_t , size , sizeof ( task -> comm ))); <nl> + strlcpy ( buf , task -> comm , min_t ( size_t , size , sizeof ( task -> comm ))); <nl> return 0 ; <nl> } <nl> 
static int __init amijoy_init ( void ) <nl> int i , j ; <nl> int err ; <nl>  <nl> + if (! MACH_IS_AMIGA ) <nl> + return - ENODEV ; <nl> + <nl> for ( i = 0 ; i < 2 ; i ++) { <nl> if (! amijoy [ i ]) <nl> continue ;
scsi_reset_provider ( struct scsi_device * dev , int flag ) <nl> rtn = FAILED ; <nl> } <nl>  <nl> - scsi_delete_timer ( scmd ); <nl> scsi_next_command ( scmd ); <nl> return rtn ; <nl> }
int lustre_start_mgc ( struct super_block * sb ) <nl>  <nl> /* Random uuid for MGC allows easier reconnects */ <nl> OBD_ALLOC_PTR ( uuid ); <nl> + if (! uuid ) { <nl> + rc = - ENOMEM ; <nl> + goto out_free ; <nl> + } <nl> + <nl> ll_generate_random_uuid ( uuidc ); <nl> class_uuid_unparse ( uuidc , uuid ); <nl> 
static ssize_t dlpar_cpu_probe ( const char * buf , size_t count ) <nl> return - ENODEV ; <nl>  <nl> dn = dlpar_configure_connector ( cpu_to_be32 ( drc_index ), parent ); <nl> + of_node_put ( parent ); <nl> if (! dn ) <nl> return - EINVAL ; <nl>  <nl> - of_node_put ( parent ); <nl> - <nl> rc = dlpar_attach_node ( dn ); <nl> if ( rc ) { <nl> dlpar_release_drc ( drc_index );
static void _rtl_usb_tx_preprocess ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> u8 tid = 0 ; <nl> u16 seq_number = 0 ; <nl>  <nl> + memset (& tcb_desc , 0 , sizeof ( struct rtl_tcb_desc )); <nl> if ( ieee80211_is_auth ( fc )) { <nl> RT_TRACE ( rtlpriv , COMP_SEND , DBG_DMESG , (" MAC80211_LINKING \ n ")); <nl> rtl_ips_nic_on ( hw );
static void posix_acl_fix_xattr_userns ( <nl> break ; <nl> case ACL_GROUP : <nl> gid = make_kgid ( from , le32_to_cpu ( entry -> e_id )); <nl> - entry -> e_id = cpu_to_le32 ( from_kuid ( to , uid )); <nl> + entry -> e_id = cpu_to_le32 ( from_kgid ( to , gid )); <nl> break ; <nl> default : <nl> break ;
static int ch7006_encoder_create_resources ( struct drm_encoder * encoder , <nl> drm_mode_create_tv_properties ( dev , NUM_TV_NORMS , ch7006_tv_norm_names ); <nl>  <nl> priv -> scale_property = drm_property_create_range ( dev , 0 , " scale ", 0 , 2 ); <nl> + if (! priv -> scale_property ) <nl> + return - ENOMEM ; <nl>  <nl> drm_object_attach_property (& connector -> base , conf -> tv_select_subconnector_property , <nl> priv -> select_subconnector );
void odm_DIGInit ( struct odm_dm_struct * pDM_Odm ) <nl> struct adapter * adapter = pDM_Odm -> Adapter ; <nl> struct rtw_dig * pDM_DigTable = & pDM_Odm -> DM_DigTable ; <nl>  <nl> - pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG ( IGI_A , pDM_Odm ), ODM_BIT ( IGI , pDM_Odm )); <nl> + pDM_DigTable -> CurIGValue = ( u8 ) PHY_QueryBBReg ( adapter , ODM_REG_IGI_A_11N , ODM_BIT_IGI_11N ); <nl> pDM_DigTable -> RssiLowThresh = DM_DIG_THRESH_LOW ; <nl> pDM_DigTable -> RssiHighThresh = DM_DIG_THRESH_HIGH ; <nl> pDM_DigTable -> FALowThresh = DM_false_ALARM_THRESH_LOW ;
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> + free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
static inline void i2c_pnx_arm_timer ( struct i2c_adapter * adap ) <nl> struct timer_list * timer = & data -> mif . timer ; <nl> int expires = I2C_PNX_TIMEOUT / ( 1000 / HZ ); <nl>  <nl> + if ( expires <= 1 ) <nl> + expires = 2 ; <nl> + <nl> del_timer_sync ( timer ); <nl>  <nl> dev_dbg (& adap -> dev , " Timer armed at % lu plus % u jiffies .\ n ",
service_in_request ( struct musb * musb , const struct usb_ctrlrequest * ctrlrequest ) <nl> static void musb_g_ep0_giveback ( struct musb * musb , struct usb_request * req ) <nl> { <nl> musb_g_giveback (& musb -> endpoints [ 0 ]. ep_in , req , 0 ); <nl> - musb -> ep0_state = MUSB_EP0_STAGE_SETUP ; <nl> } <nl>  <nl> /*
static const struct usb_device_id usbtouch_devices [] = { <nl>  <nl> # ifdef CONFIG_TOUCHSCREEN_USB_ITM <nl> { USB_DEVICE ( 0x0403 , 0xf9e9 ), . driver_info = DEVTYPE_ITM }, <nl> + { USB_DEVICE ( 0x16e3 , 0xf9e9 ), . driver_info = DEVTYPE_ITM }, <nl> # endif <nl>  <nl> # ifdef CONFIG_TOUCHSCREEN_USB_ETURBO
static int __devinit palmas_i2c_probe ( struct i2c_client * i2c , <nl> goto err ; <nl> } <nl>  <nl> + children [ PALMAS_PMIC_ID ]. platform_data = pdata -> pmic_pdata ; <nl> + children [ PALMAS_PMIC_ID ]. pdata_size = sizeof (* pdata -> pmic_pdata ); <nl> + <nl> ret = mfd_add_devices ( palmas -> dev , - 1 , <nl> children , ARRAY_SIZE ( palmas_children ), <nl> NULL , regmap_irq_chip_get_base ( palmas -> irq_data ));
static int parse_addr ( const struct nf_conn * ct , const char * cp , <nl> int family = ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . l3num ; <nl> int ret = 0 ; <nl>  <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> switch ( family ) { <nl> case AF_INET : <nl> ret = in4_pton ( cp , limit - cp , ( u8 *)& addr -> ip , - 1 , & end );
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN ); <nl> sta -> local = local ; <nl> sta -> sdata = sdata ; <nl> + sta -> last_rx = jiffies ; <nl>  <nl> ewma_init (& sta -> avg_signal , 1024 , 8 ); <nl> 
out_dio : <nl> * async dio is going to do it in the future or an end_io after an <nl> * error has already done it . <nl> */ <nl> - if ( ret == - EIOCBQUEUED || ! ocfs2_iocb_is_rw_locked ( iocb )) { <nl> + if (( ret == - EIOCBQUEUED ) || (! ocfs2_iocb_is_rw_locked ( iocb ))) { <nl> rw_level = - 1 ; <nl> have_alloc_sem = 0 ; <nl> }
read_again : <nl> skb = xgbe_create_skb ( pdata , rdata , & put_len ); <nl> if (! skb ) { <nl> error = 1 ; <nl> - goto read_again ; <nl> + goto skip_data ; <nl> } <nl> } <nl>  <nl> read_again : <nl> } <nl> } <nl>  <nl> + skip_data : <nl> if ( incomplete || context_next ) <nl> goto read_again ; <nl>  <nl> - /* Stray Context Descriptor ? */ <nl> if (! skb ) <nl> goto next_packet ; <nl> 
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
int pci_vpd_truncate ( struct pci_dev * dev , size_t size ) <nl> return - EINVAL ; <nl>  <nl> dev -> vpd -> len = size ; <nl> - dev -> vpd -> attr -> size = size ; <nl> + if ( dev -> vpd -> attr ) <nl> + dev -> vpd -> attr -> size = size ; <nl>  <nl> return 0 ; <nl> }
struct clk * imx_clk_fixup_mux ( const char * name , void __iomem * reg , <nl> init . ops = & clk_fixup_mux_ops ; <nl> init . parent_names = parents ; <nl> init . num_parents = num_parents ; <nl> + init . flags = 0 ; <nl>  <nl> fixup_mux -> mux . reg = reg ; <nl> fixup_mux -> mux . shift = shift ;
void nfs_inode_return_delegation_noreclaim ( struct inode * inode ) <nl>  <nl> delegation = nfs_inode_detach_delegation ( inode ); <nl> if ( delegation != NULL ) <nl> - nfs_do_return_delegation ( inode , delegation , 0 ); <nl> + nfs_do_return_delegation ( inode , delegation , 1 ); <nl> } <nl>  <nl> /**
static int uas_queuecommand_lck ( struct scsi_cmnd * cmnd , <nl> return SCSI_MLQUEUE_DEVICE_BUSY ; <nl> } <nl>  <nl> + memset ( cmdinfo , 0 , sizeof (* cmdinfo )); <nl> + <nl> if ( blk_rq_tagged ( cmnd -> request )) { <nl> cmdinfo -> stream = cmnd -> request -> tag + 2 ; <nl> } else {
luan_setup_hoses ( void ) <nl>  <nl> /* Allocate hoses for PCIX1 and PCIX2 */ <nl> hose1 = pcibios_alloc_controller (); <nl> + if (! hose1 ) <nl> + return ; <nl> + <nl> hose2 = pcibios_alloc_controller (); <nl> - if (! hose1 || ! hose2 ) <nl> + if (! hose2 ) { <nl> + pcibios_free_controller ( hose1 ); <nl> return ; <nl> + } <nl>  <nl> /* Setup PCIX1 */ <nl> hose1 -> first_busno = 0 ;
static struct urb * usbtv_setup_iso_transfer ( struct usbtv * usbtv ) <nl> ip -> transfer_flags = URB_ISO_ASAP ; <nl> ip -> transfer_buffer = kzalloc ( size * USBTV_ISOC_PACKETS , <nl> GFP_KERNEL ); <nl> + if (! ip -> transfer_buffer ) { <nl> + usb_free_urb ( ip ); <nl> + return NULL ; <nl> + } <nl> ip -> complete = usbtv_iso_cb ; <nl> ip -> number_of_packets = USBTV_ISOC_PACKETS ; <nl> ip -> transfer_buffer_length = size * USBTV_ISOC_PACKETS ;
struct request { <nl>  <nl> unsigned short ioprio ; <nl>  <nl> + unsigned int timeout ; <nl> + <nl> void * special ; /* opaque pointer available for LLD use */ <nl>  <nl> int errors ; <nl> struct request { <nl>  <nl> unsigned long deadline ; <nl> struct list_head timeout_list ; <nl> - unsigned int timeout ; <nl>  <nl> /* <nl> * completion callback .
nouveau_gem_object_open ( struct drm_gem_object * gem , struct drm_file * file_priv ) <nl> } <nl>  <nl> ret = pm_runtime_get_sync ( dev ); <nl> - if ( ret < 0 && ret != - EACCES ) <nl> + if ( ret < 0 && ret != - EACCES ) { <nl> + kfree ( vma ); <nl> goto out ; <nl> + } <nl>  <nl> ret = nouveau_bo_vma_add ( nvbo , cli -> vm , vma ); <nl> if ( ret )
static struct drm_driver driver = { <nl> . minor = DRIVER_MINOR , <nl> . patchlevel = DRIVER_PATCHLEVEL , <nl>  <nl> - . gem_free_object = ast_gem_free_object , <nl> + . gem_free_object_unlocked = ast_gem_free_object , <nl> . dumb_create = ast_dumb_create , <nl> . dumb_map_offset = ast_dumb_mmap_offset , <nl> . dumb_destroy = drm_gem_dumb_destroy ,
int mac_partition ( struct parsed_partitions * state , struct block_device * bdev ) <nl> be32_to_cpu ( part -> start_block ) * ( secsize / 512 ), <nl> be32_to_cpu ( part -> block_count ) * ( secsize / 512 )); <nl>  <nl> + if (! strnicmp ( part -> type , " Linux_RAID ", 10 )) <nl> + state -> parts [ slot ]. flags = 1 ; <nl> # ifdef CONFIG_PPC_PMAC <nl> /* <nl> * If this is the first bootable partition , tell the
qla24xx_reset_adapter ( scsi_qla_host_t * vha ) <nl> WRT_REG_DWORD (& reg -> hccr , HCCRX_REL_RISC_PAUSE ); <nl> RD_REG_DWORD (& reg -> hccr ); <nl> spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> + <nl> + if ( IS_NOPOLLING_TYPE ( ha )) <nl> + ha -> isp_ops -> enable_intrs ( ha ); <nl> } <nl>  <nl> /* On sparc systems , obtain port and node WWN from firmware
int dlpar_detach_node ( struct device_node * dn ) <nl> if ( rc ) <nl> return rc ; <nl>  <nl> - of_node_put ( dn ); /* Must decrement the refcount */ <nl> return 0 ; <nl> } <nl> 
static int atmel_serial_remove ( struct platform_device * pdev ) <nl> struct atmel_uart_port * atmel_port = to_atmel_uart_port ( port ); <nl> int ret = 0 ; <nl>  <nl> + tasklet_kill (& atmel_port -> tasklet ); <nl> + <nl> device_init_wakeup (& pdev -> dev , 0 ); <nl>  <nl> ret = uart_remove_one_port (& atmel_uart , port ); <nl>  <nl> - tasklet_kill (& atmel_port -> tasklet ); <nl> kfree ( atmel_port -> rx_ring . buf ); <nl>  <nl> /* " port " is allocated statically , so we shouldn ' t free it */
static void _gb_power_supplies_release ( struct gb_power_supplies * supplies ) <nl> { <nl> int i ; <nl>  <nl> + if (! supplies -> supply ) <nl> + return ; <nl> + <nl> mutex_lock (& supplies -> supplies_lock ); <nl> for ( i = 0 ; i < supplies -> supplies_count ; i ++) <nl> _gb_power_supply_release (& supplies -> supply [ i ]);
static int bnx2x_cnic_handle_cfc_del ( struct bnx2x * bp , u32 cid , <nl> union event_ring_elem * elem ) <nl> { <nl> if (! bp -> cnic_eth_dev . starting_cid || <nl> - cid < bp -> cnic_eth_dev . starting_cid ) <nl> + ( cid < bp -> cnic_eth_dev . starting_cid && <nl> + cid != bp -> cnic_eth_dev . iscsi_l2_cid )) <nl> return 1 ; <nl>  <nl> DP ( BNX2X_MSG_SP , " got delete ramrod for CNIC CID % d \ n ", cid );
static enum dlm_status dlmunlock_common ( struct dlm_ctxt * dlm , <nl> else <nl> status = dlm_get_unlock_actions ( dlm , res , lock , lksb , & actions ); <nl>  <nl> - if ( status != DLM_NORMAL && status != DLM_CANCELGRANT ) <nl> + if ( status != DLM_NORMAL && ( status != DLM_CANCELGRANT || ! master_node )) <nl> goto leave ; <nl>  <nl> /* By now this has been masked out of cancel requests . */
static void ati_remote_input_report ( struct urb * urb , struct pt_regs * regs ) <nl> input_regs ( dev , regs ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 1 ); <nl> + input_sync ( dev ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 0 ); <nl> input_sync ( dev );
static int __devinit plat_nand_probe ( struct platform_device * pdev ) <nl> struct resource * res ; <nl> int err = 0 ; <nl>  <nl> + if ( pdata -> chip . nr_chips < 1 ) { <nl> + dev_err (& pdev -> dev , " invalid number of chips specified \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) <nl> return - ENXIO ;
static void isp1760_hub_descriptor ( struct isp1760_hcd * priv , <nl> int ports = HCS_N_PORTS ( priv -> hcs_params ); <nl> u16 temp ; <nl>  <nl> - desc -> bDescriptorType = 0x29 ; <nl> + desc -> bDescriptorType = USB_DT_HUB ; <nl> /* priv 1 . 0 , 2 . 3 . 9 says 20ms max */ <nl> desc -> bPwrOn2PwrGood = 10 ; <nl> desc -> bHubContrCurrent = 0 ;
static struct task_struct * dup_task_struct ( struct task_struct * orig ) <nl>  <nl> /* One for us , one for whoever does the " release_task ()" ( usually parent ) */ <nl> atomic_set (& tsk -> usage , 2 ); <nl> + atomic_set (& tsk -> fs_excl , 0 ); <nl> return tsk ; <nl> } <nl> 
static int do_proc_dointvec_jiffies_conv ( bool * negp , unsigned long * lvalp , <nl> int write , void * data ) <nl> { <nl> if ( write ) { <nl> - if (* lvalp > LONG_MAX / HZ ) <nl> + if (* lvalp > INT_MAX / HZ ) <nl> return 1 ; <nl> * valp = * negp ? -(* lvalp * HZ ) : (* lvalp * HZ ); <nl> } else {
struct page * kmap_to_page ( void * vaddr ) <nl>  <nl> return virt_to_page ( addr ); <nl> } <nl> + EXPORT_SYMBOL ( kmap_to_page ); <nl>  <nl> static void flush_all_zero_pkmaps ( void ) <nl> {
void rtl92e_set_key ( struct net_device * dev , u8 EntryNo , u8 KeyIndex , <nl> } <nl> } <nl> priv -> rtllib -> is_set_key = true ; <nl> - if ( EntryNo >= TOTAL_CAM_ENTRY ) <nl> + if ( EntryNo >= TOTAL_CAM_ENTRY ) { <nl> netdev_info ( dev , "% s (): Invalid CAM entry \ n ", __func__ ); <nl> + return ; <nl> + } <nl>  <nl> RT_TRACE ( COMP_SEC , <nl> "====> to rtl92e_set_key (), dev :% p , EntryNo :% d , KeyIndex :% d , KeyType :% d , MacAddr % pM \ n ",
enum { <nl> # define DM_DEV_SET_GEOMETRY _IOWR ( DM_IOCTL , DM_DEV_SET_GEOMETRY_CMD , struct dm_ioctl ) <nl>  <nl> # define DM_VERSION_MAJOR 4 <nl> -# define DM_VERSION_MINOR 31 <nl> +# define DM_VERSION_MINOR 32 <nl> # define DM_VERSION_PATCHLEVEL 0 <nl> -# define DM_VERSION_EXTRA "- ioctl ( 2015 - 3 - 12 )" <nl> +# define DM_VERSION_EXTRA "- ioctl ( 2015 - 6 - 26 )" <nl>  <nl> /* Status bits */ <nl> # define DM_READONLY_FLAG ( 1 << 0 ) /* In / Out */
static void bit_putcs ( struct vc_data * vc , struct fb_info * info , <nl> image . depth = 1 ; <nl>  <nl> if ( attribute ) { <nl> - buf = kmalloc ( cellsize , GFP_KERNEL ); <nl> + buf = kmalloc ( cellsize , GFP_ATOMIC ); <nl> if (! buf ) <nl> return ; <nl> }
static int qat_alg_sgl_to_bufl ( struct qat_crypto_instance * inst , <nl> goto err ; <nl>  <nl> for_each_sg ( assoc , sg , assoc_n , i ) { <nl> + if (! sg -> length ) <nl> + continue ; <nl> bufl -> bufers [ bufs ]. addr = dma_map_single ( dev , <nl> sg_virt ( sg ), <nl> sg -> length ,
static void i9xx_enable_pll ( struct intel_crtc * crtc ) <nl> I915_READ ( DPLL (! crtc -> pipe )) | DPLL_DVO_2X_MODE ); <nl> } <nl>  <nl> + I915_WRITE ( reg , dpll ); <nl> + <nl> /* Wait for the clocks to stabilize . */ <nl> POSTING_READ ( reg ); <nl> udelay ( 150 );
u32 bond_xmit_hash ( struct bonding * bond , struct sk_buff * skb ) <nl> struct flow_keys flow ; <nl> u32 hash ; <nl>  <nl> + if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_ENCAP34 && <nl> + skb -> l4_hash ) <nl> + return skb -> hash ; <nl> + <nl> if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_LAYER2 || <nl> ! bond_flow_dissect ( bond , skb , & flow )) <nl> return bond_eth_hash ( skb );
struct drm_gem_object * msm_gem_new ( struct drm_device * dev , <nl>  <nl> fail : <nl> if ( obj ) <nl> - drm_gem_object_unreference_unlocked ( obj ); <nl> + drm_gem_object_unreference ( obj ); <nl>  <nl> return ERR_PTR ( ret ); <nl> }
omap_i2c_probe ( struct platform_device * pdev ) <nl> struct omap_i2c_dev * dev ; <nl> struct i2c_adapter * adap ; <nl> struct resource * mem , * irq , * ioarea ; <nl> - struct omap_i2c_bus_platform_data * pdata = pdev -> dev . platform_data ; <nl> + const struct omap_i2c_bus_platform_data * pdata = <nl> + pdev -> dev . platform_data ; <nl> struct device_node * node = pdev -> dev . of_node ; <nl> const struct of_device_id * match ; <nl> irq_handler_t isr ;
static int __init musb_probe ( struct platform_device * pdev ) <nl> void __iomem * base ; <nl>  <nl> iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - if (! iomem || irq == 0 ) <nl> + if (! iomem || irq <= 0 ) <nl> return - ENODEV ; <nl>  <nl> base = ioremap ( iomem -> start , resource_size ( iomem ));
static s16 * read_rds_samples ( struct cx88_core * core , u32 * N ) <nl> current_address , <nl> current_address - srch -> fifo_start , sample_count , <nl> cx_read ( MO_AUD_INTSTAT )); <nl> - <nl> - samples = kmalloc ( sizeof ( s16 )* sample_count , GFP_KERNEL ); <nl> + samples = kmalloc_array ( sample_count , sizeof (* samples ), GFP_KERNEL ); <nl> if (! samples ) <nl> return NULL ; <nl> 
static size_t jffs2_trusted_listxattr ( struct dentry * dentry , char * list , <nl> { <nl> size_t retlen = XATTR_TRUSTED_PREFIX_LEN + name_len + 1 ; <nl>  <nl> + if (! capable ( CAP_SYS_ADMIN )) <nl> + return 0 ; <nl> + <nl> if ( list && retlen <= list_size ) { <nl> strcpy ( list , XATTR_TRUSTED_PREFIX ); <nl> strcpy ( list + XATTR_TRUSTED_PREFIX_LEN , name );
static struct drm_driver driver = { <nl> . minor = DRIVER_MINOR , <nl> . patchlevel = DRIVER_PATCHLEVEL , <nl>  <nl> - . gem_free_object = mgag200_gem_free_object , <nl> + . gem_free_object_unlocked = mgag200_gem_free_object , <nl> . dumb_create = mgag200_dumb_create , <nl> . dumb_map_offset = mgag200_dumb_mmap_offset , <nl> . dumb_destroy = drm_gem_dumb_destroy ,
static int synaptics_board_id ( struct psmouse * psmouse ) <nl> struct synaptics_data * priv = psmouse -> private ; <nl> unsigned char bid [ 3 ]; <nl>  <nl> + /* firmwares prior 7 . 5 have no board_id encoded */ <nl> + if ( SYN_ID_FULL ( priv -> identity ) < 0x705 ) <nl> + return 0 ; <nl> + <nl> if ( synaptics_send_cmd ( psmouse , SYN_QUE_MODES , bid )) <nl> return - 1 ; <nl> priv -> board_id = (( bid [ 0 ] & 0xfc ) << 6 ) | bid [ 1 ];
static void conf_message_callback ( const char * fmt , va_list ap ) <nl>  <nl> static void show_help ( struct menu * menu ) <nl> { <nl> - struct gstr help = str_new (); <nl> + struct gstr help ; <nl> + <nl> + if (! menu ) <nl> + return ; <nl> + <nl> + help = str_new (); <nl> menu_get_ext_help ( menu , & help ); <nl> show_scroll_win ( main_window , _ ( menu_get_prompt ( menu )), str_get (& help )); <nl> str_free (& help );
retry : <nl> return 0 ; <nl>  <nl> n_group = ext4_get_group_number ( sb , n_blocks_count - 1 ); <nl> + if ( n_group > ( 0xFFFFFFFFUL / EXT4_INODES_PER_GROUP ( sb ))) { <nl> + ext4_warning ( sb , " resize would cause inodes_count overflow "); <nl> + return - EINVAL ; <nl> + } <nl> ext4_get_group_no_and_offset ( sb , o_blocks_count - 1 , & o_group , & offset ); <nl>  <nl> n_desc_blocks = num_desc_blocks ( sb , n_group + 1 );
int kvmppc_handle_exit ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> } <nl> break ; <nl>  <nl> + case BOOKE_INTERRUPT_FP_UNAVAIL : <nl> + kvmppc_queue_exception ( vcpu , exit_nr ); <nl> + r = RESUME_GUEST ; <nl> + break ; <nl> + <nl> case BOOKE_INTERRUPT_DATA_STORAGE : <nl> vcpu -> arch . dear = vcpu -> arch . fault_dear ; <nl> vcpu -> arch . esr = vcpu -> arch . fault_esr ;
static struct eg20t_port * pch_uart_init_port ( struct pci_dev * pdev , <nl> priv -> port . line = num ++; <nl> priv -> trigger = PCH_UART_HAL_TRIGGER_M ; <nl>  <nl> + spin_lock_init (& priv -> port . lock ); <nl> + <nl> pci_set_drvdata ( pdev , priv ); <nl> pch_uart_hal_request ( pdev , fifosize , base_baud ); <nl> 
out_device_destroy : <nl> scsi_device_set_state ( sdev , SDEV_DEL ); <nl> transport_destroy_device (& sdev -> sdev_gendev ); <nl> put_device (& sdev -> sdev_dev ); <nl> + scsi_free_queue ( sdev -> request_queue ); <nl> put_device (& sdev -> sdev_gendev ); <nl> out : <nl> if ( display_failure_msg )
void btrfs_apply_pending_changes ( struct btrfs_fs_info * fs_info ) <nl> unsigned long prev ; <nl> unsigned long bit ; <nl>  <nl> - prev = cmpxchg (& fs_info -> pending_changes , 0 , 0 ); <nl> + prev = xchg (& fs_info -> pending_changes , 0 ); <nl> if (! prev ) <nl> return ; <nl> 
static void udf_sb_free_partitions ( struct super_block * sb ) <nl> { <nl> struct udf_sb_info * sbi = UDF_SB ( sb ); <nl> int i ; <nl> - <nl> + if ( sbi -> s_partmaps == NULL ) <nl> + return ; <nl> for ( i = 0 ; i < sbi -> s_partitions ; i ++) <nl> udf_free_partition (& sbi -> s_partmaps [ i ]); <nl> kfree ( sbi -> s_partmaps );
static ssize_t stm_char_write ( struct file * file , const char __user * buf , <nl> char * kbuf ; <nl> int err ; <nl>  <nl> + if ( count + 1 > PAGE_SIZE ) <nl> + count = PAGE_SIZE - 1 ; <nl> + <nl> /* <nl> * if no m / c have been assigned to this writer up to this <nl> * point , use " default " policy entry
void btrfs_evict_inode ( struct inode * inode ) <nl> btrfs_orphan_del ( NULL , inode ); <nl> goto no_delete ; <nl> } <nl> + rsv -> size = min_size ; <nl>  <nl> btrfs_i_size_write ( inode , 0 ); <nl>  <nl> static int btrfs_truncate ( struct inode * inode ) <nl> rsv = btrfs_alloc_block_rsv ( root ); <nl> if (! rsv ) <nl> return - ENOMEM ; <nl> + rsv -> size = min_size ; <nl>  <nl> /* <nl> * 1 for the truncate slack space
int hfsplus_find_cat ( struct super_block * sb , u32 cnid , <nl> return - EIO ; <nl> } <nl>  <nl> + if ( be16_to_cpu ( tmp . thread . nodeName . length ) > 255 ) { <nl> + printk ( KERN_ERR " hfs : catalog name length corrupted \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> hfsplus_cat_build_key_uni ( fd -> search_key , be32_to_cpu ( tmp . thread . parentID ), <nl> & tmp . thread . nodeName ); <nl> return hfs_brec_find ( fd );
xfs_destroy_ioend ( <nl> } <nl>  <nl> if ( ioend -> io_iocb ) { <nl> + inode_dio_done ( ioend -> io_inode ); <nl> if ( ioend -> io_isasync ) { <nl> aio_complete ( ioend -> io_iocb , ioend -> io_error ? <nl> ioend -> io_error : ioend -> io_result , 0 ); <nl> } <nl> - inode_dio_done ( ioend -> io_inode ); <nl> } <nl>  <nl> mempool_free ( ioend , xfs_ioend_pool );
static int llc_ui_create ( struct net * net , struct socket * sock , int protocol ) <nl> struct sock * sk ; <nl> int rc = - ESOCKTNOSUPPORT ; <nl>  <nl> + if (! capable ( CAP_NET_RAW )) <nl> + return - EPERM ; <nl> + <nl> if ( net != & init_net ) <nl> return - EAFNOSUPPORT ; <nl> 
static int __init scx200_create_isa ( const char * text , unsigned long base , <nl> if ( iface == NULL ) <nl> return - ENOMEM ; <nl>  <nl> - if ( request_region ( base , 8 , iface -> adapter . name ) == 0 ) { <nl> + if (! request_region ( base , 8 , iface -> adapter . name )) { <nl> printk ( KERN_ERR NAME ": can ' t allocate io 0x % lx - 0x % lx \ n ", <nl> base , base + 8 - 1 ); <nl> rc = - EBUSY ;
static void line6_destruct ( struct snd_card * card ) <nl> /* Free buffer memory first . We cannot depend on the existence of private <nl> * data from the ( podhd ) module , it may be gone already during this call <nl> */ <nl> - if ( line6 -> buffer_message ) <nl> - kfree ( line6 -> buffer_message ); <nl> + kfree ( line6 -> buffer_message ); <nl>  <nl> kfree ( line6 -> buffer_listen ); <nl> 
struct vp_config_entry_24xx { <nl> uint16_t id ; <nl> uint16_t reserved_4 ; <nl> uint16_t hopct ; <nl> - uint8_t reserved_5 ; <nl> + uint8_t reserved_5 [ 2 ]; <nl> }; <nl>  <nl> # define VP_RPT_ID_IOCB_TYPE 0x32 /* Report ID Acquisition entry . */
static ktime_t tick_nohz_stop_sched_tick ( struct tick_sched * ts , <nl> */ <nl> if ( delta == 0 ) { <nl> tick_nohz_restart ( ts , now ); <nl> + /* <nl> + * Make sure next tick stop doesn ' t get fooled by past <nl> + * clock deadline <nl> + */ <nl> + ts -> next_tick = 0 ; <nl> goto out ; <nl> } <nl> }
static struct task_struct * first_tid ( struct task_struct * leader , <nl> pos = NULL ; <nl> if ( nr && nr >= get_nr_threads ( leader )) <nl> goto out ; <nl> + /* It could be unhashed before we take rcu lock */ <nl> + if (! pid_alive ( leader )) <nl> + goto out ; <nl>  <nl> /* If we haven ' t found our starting place yet start <nl> * with the leader and walk nr threads forward .
i915_gem_wait_ioctl ( struct drm_device * dev , void * data , struct drm_file * file ) <nl> u32 seqno = 0 ; <nl> int ret = 0 ; <nl>  <nl> + if ( args -> flags != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = i915_mutex_lock_interruptible ( dev ); <nl> if ( ret ) <nl> return ret ;
static int br_afspec ( struct net_bridge * br , <nl> if ( nla_len ( attr ) != sizeof ( struct bridge_vlan_info )) <nl> return - EINVAL ; <nl> vinfo = nla_data ( attr ); <nl> + if (! vinfo -> vid || vinfo -> vid >= VLAN_VID_MASK ) <nl> + return - EINVAL ; <nl> if ( vinfo -> flags & BRIDGE_VLAN_INFO_RANGE_BEGIN ) { <nl> if ( vinfo_start ) <nl> return - EINVAL ;
static void wacom_i4_parse_pen_report ( struct wacom_data * wdata , <nl>  <nl> switch ( data [ 1 ]) { <nl> case 0x80 : /* Out of proximity report */ <nl> - wdata -> tool = 0 ; <nl> input_report_key ( input , BTN_TOUCH , 0 ); <nl> input_report_abs ( input , ABS_PRESSURE , 0 ); <nl> input_report_key ( input , wdata -> tool , 0 ); <nl> + wdata -> tool = 0 ; <nl> input_sync ( input ); <nl> break ; <nl> case 0xC2 : /* Tool report */
int dw_mci_probe ( struct dw_mci * host ) <nl> } <nl> } <nl>  <nl> - if ( host -> pdata -> num_slots > 1 ) { <nl> + if ( host -> pdata -> num_slots < 1 ) { <nl> dev_err ( host -> dev , <nl> " Platform data must supply num_slots .\ n "); <nl> return - ENODEV ;
static int __cpufreq_set_policy ( struct cpufreq_policy * data , <nl> memcpy (& policy -> cpuinfo , & data -> cpuinfo , <nl> sizeof ( struct cpufreq_cpuinfo )); <nl>  <nl> - if ( policy -> min > data -> min && policy -> min > policy -> max ) { <nl> + if ( policy -> min > data -> max || policy -> max < data -> min ) { <nl> ret = - EINVAL ; <nl> goto error_out ; <nl> }
static int enum_fmt ( void * priv , struct v4l2_fmtdesc * f , <nl> fmt = & formats [ i ]; <nl> strlcpy ( f -> description , fmt -> name , sizeof ( f -> description )); <nl> f -> pixelformat = fmt -> fourcc ; <nl> + if (! coda_format_is_yuv ( fmt -> fourcc )) <nl> + f -> flags |= V4L2_FMT_FLAG_COMPRESSED ; <nl> return 0 ; <nl> } <nl> 
static bool new_idmap_permitted ( const struct file * file , <nl> u32 id = new_map -> extent [ 0 ]. lower_first ; <nl> if ( cap_setid == CAP_SETUID ) { <nl> kuid_t uid = make_kuid ( ns -> parent , id ); <nl> - if ( uid_eq ( uid , file -> f_cred -> fsuid )) <nl> + if ( uid_eq ( uid , file -> f_cred -> euid )) <nl> return true ; <nl> } <nl> }
static int kvm_vcpu_ioctl_x86_set_vcpu_events ( struct kvm_vcpu * vcpu , <nl> | KVM_VCPUEVENT_VALID_SMM )) <nl> return - EINVAL ; <nl>  <nl> + if ( events -> exception . injected && <nl> + ( events -> exception . nr > 31 || events -> exception . nr == NMI_VECTOR )) <nl> + return - EINVAL ; <nl> + <nl> process_nmi ( vcpu ); <nl> vcpu -> arch . exception . pending = events -> exception . injected ; <nl> vcpu -> arch . exception . nr = events -> exception . nr ;
static struct sock * tcp_fastopen_create_child ( struct sock * sk , <nl> tcp_fastopen_add_skb ( child , skb ); <nl>  <nl> tcp_rsk ( req )-> rcv_nxt = tp -> rcv_nxt ; <nl> + tp -> rcv_wup = tp -> rcv_nxt ; <nl> /* tcp_conn_request () is sending the SYNACK , <nl> * and queues the child into listener accept queue . <nl> */
static void hidinput_configure_usage ( struct hid_input * hidinput , struct hid_fiel <nl> break ; <nl>  <nl> case HID_UP_BUTTON : <nl> - code = (( usage -> hid - 1 ) & 0xf ); <nl> + code = (( usage -> hid - 1 ) & HID_USAGE ); <nl>  <nl> switch ( field -> application ) { <nl> case HID_GD_MOUSE :
ext4_move_extents ( struct file * o_filp , struct file * d_filp , <nl> orig_inode -> i_ino , donor_inode -> i_ino ); <nl> return - EINVAL ; <nl> } <nl> - <nl> + /* TODO : This is non obvious task to swap blocks for inodes with full <nl> + jornaling enabled */ <nl> + if ( ext4_should_journal_data ( orig_inode ) || <nl> + ext4_should_journal_data ( donor_inode )) { <nl> + return - EINVAL ; <nl> + } <nl> /* Protect orig and donor inodes against a truncate */ <nl> mext_inode_double_lock ( orig_inode , donor_inode ); <nl> 
static int lzo_compress_pages ( struct list_head * ws , <nl> } <nl>  <nl> /* we ' re making it bigger , give up */ <nl> - if ( tot_in > 8192 && tot_in < tot_out ) <nl> + if ( tot_in > 8192 && tot_in < tot_out ) { <nl> + ret = - 1 ; <nl> goto out ; <nl> + } <nl>  <nl> /* we ' re all done */ <nl> if ( tot_in >= len )
static ssize_t tcmu_set_configfs_dev_params ( struct se_device * dev , <nl> default : <nl> break ; <nl> } <nl> + <nl> + if ( ret ) <nl> + break ; <nl> } <nl>  <nl> kfree ( orig );
void eth_header_cache_update ( struct hh_cache * hh , struct net_device * dev , <nl> static int eth_mac_addr ( struct net_device * dev , void * p ) <nl> { <nl> struct sockaddr * addr = p ; <nl> + <nl> if ( netif_running ( dev )) <nl> return - EBUSY ; <nl> + if (! is_valid_ether_addr ( addr -> sa_data )) <nl> + return - EADDRNOTAVAIL ; <nl> memcpy ( dev -> dev_addr , addr -> sa_data , dev -> addr_len ); <nl> return 0 ; <nl> }
enum acer_wmi_event_ids { <nl> static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x01 , { KEY_WLAN } }, /* WiFi */ <nl> { KE_KEY , 0x03 , { KEY_WLAN } }, /* WiFi */ <nl> + { KE_KEY , 0x04 , { KEY_WLAN } }, /* WiFi */ <nl> { KE_KEY , 0x12 , { KEY_BLUETOOTH } }, /* BT */ <nl> { KE_KEY , 0x21 , { KEY_PROG1 } }, /* Backup */ <nl> { KE_KEY , 0x22 , { KEY_PROG2 } }, /* Arcade */
static int knav_setup_queue_range ( struct knav_device * kdev , <nl>  <nl> range -> num_irqs ++; <nl>  <nl> - if ( oirq . args_count == 3 ) <nl> + if ( IS_ENABLED ( CONFIG_SMP ) && oirq . args_count == 3 ) <nl> range -> irqs [ i ]. cpu_map = <nl> ( oirq . args [ 2 ] & 0x0000ff00 ) >> 8 ; <nl> }
static int find_first_block_group ( struct btrfs_root * root , <nl> } else { <nl> ret = 0 ; <nl> } <nl> + free_extent_map ( em ); <nl> goto out ; <nl> } <nl> path -> slots [ 0 ]++;
static int skl_tplg_mixer_dapm_post_pmd_event ( struct snd_soc_dapm_widget * w , <nl> int ret = 0 ; <nl>  <nl> skl_tplg_free_pipe_mcps ( skl , mconfig ); <nl> + skl_tplg_free_pipe_mem ( skl , mconfig ); <nl>  <nl> list_for_each_entry ( w_module , & s_pipe -> w_list , node ) { <nl> dst_module = w_module -> w -> priv ; <nl> static int skl_tplg_mixer_dapm_post_pmd_event ( struct snd_soc_dapm_widget * w , <nl> } <nl>  <nl> ret = skl_delete_pipe ( ctx , mconfig -> pipe ); <nl> - skl_tplg_free_pipe_mem ( skl , mconfig ); <nl>  <nl> return ret ; <nl> }
int __init mxc_register_gpios ( void ) <nl> # ifdef CONFIG_MACH_MX21 <nl> static struct resource mx21_usbhc_resources [] = { <nl> { <nl> - . start = MX21_BASE_ADDR , <nl> - . end = MX21_BASE_ADDR + 0x1FFF , <nl> + . start = MX21_USBOTG_BASE_ADDR , <nl> + . end = MX21_USBOTG_BASE_ADDR + SZ_8K - 1 , <nl> . flags = IORESOURCE_MEM , <nl> }, <nl> {
static s32 igb_get_invariants_82575 ( struct e1000_hw * hw ) <nl> * for setting word_size . <nl> */ <nl> size += NVM_WORD_SIZE_BASE_SHIFT ; <nl> + <nl> + /* EEPROM access above 16k is unsupported */ <nl> + if ( size > 14 ) <nl> + size = 14 ; <nl> nvm -> word_size = 1 << size ; <nl>  <nl> /* setup PHY parameters */
static int p9_virtio_probe ( struct virtio_device * vdev ) <nl> int err ; <nl> struct virtio_chan * chan ; <nl>  <nl> + if (! vdev -> config -> get ) { <nl> + dev_err (& vdev -> dev , "% s failure : config access disabled \ n ", <nl> + __func__ ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> chan = kmalloc ( sizeof ( struct virtio_chan ), GFP_KERNEL ); <nl> if (! chan ) { <nl> pr_err (" Failed to allocate virtio 9P channel \ n ");
static struct page * alloc_misplaced_dst_page ( struct page * page , <nl> __GFP_NOMEMALLOC | __GFP_NORETRY | <nl> __GFP_NOWARN ) & <nl> ~ GFP_IOFS , 0 ); <nl> + if ( newpage ) <nl> + page_xchg_last_nid ( newpage , page_last_nid ( page )); <nl> + <nl> return newpage ; <nl> } <nl> 
# include < linux / list . h > <nl> # include < linux / slab . h > <nl> # include < linux / export . h > <nl> +# include < linux / vmalloc . h > <nl> # include < net / net_namespace . h > <nl> # include < net / ip . h > <nl> # include < net / protocol . h >
static struct packet_type ip_packet_type __read_mostly = { <nl>  <nl> static int __init inet_init ( void ) <nl> { <nl> - struct sk_buff * dummy_skb ; <nl> struct inet_protosw * q ; <nl> struct list_head * r ; <nl> int rc = - EINVAL ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct inet_skb_parm ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct inet_skb_parm ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> sysctl_local_reserved_ports = kzalloc ( 65536 / 8 , GFP_KERNEL ); <nl> if (! sysctl_local_reserved_ports )
static int ads7846_remove ( struct spi_device * spi ) <nl>  <nl> ads784x_hwmon_unregister ( spi , ts ); <nl>  <nl> - regulator_disable ( ts -> reg ); <nl> regulator_put ( ts -> reg ); <nl>  <nl> if (! ts -> get_pendown_state ) {
dev_config ( struct file * fd , const char __user * buf , size_t len , loff_t * ptr ) <nl>  <nl> spin_lock_irq (& dev -> lock ); <nl> value = - EINVAL ; <nl> - if ( dev -> buf ) <nl> + if ( dev -> buf ) { <nl> + kfree ( kbuf ); <nl> goto fail ; <nl> + } <nl> dev -> buf = kbuf ; <nl>  <nl> /* full or low speed config */
print_graph_entry ( struct ftrace_graph_ent_entry * field , struct trace_seq * s , <nl>  <nl> /* Proc */ <nl> if ( tracer_flags . val & TRACE_GRAPH_PRINT_PROC ) { <nl> - ret = print_graph_proc ( s , pid ); <nl> + ret = print_graph_proc ( s , ent -> pid ); <nl> if ( ret == TRACE_TYPE_PARTIAL_LINE ) <nl> return TRACE_TYPE_PARTIAL_LINE ; <nl> 
static void __exit cleanup_nsc ( void ) <nl> if ( pdev ) { <nl> tpm_nsc_remove (& pdev -> dev ); <nl> platform_device_unregister ( pdev ); <nl> - kfree ( pdev ); <nl> - pdev = NULL ; <nl> } <nl>  <nl> platform_driver_unregister (& nsc_drv );
sid_to_id ( struct cifs_sb_info * cifs_sb , struct cifs_sid * psid , <nl> * probably a safe assumption but might be better to check based on <nl> * sidtype . <nl> */ <nl> + BUILD_BUG_ON ( sizeof ( uid_t ) != sizeof ( gid_t )); <nl> if ( sidkey -> datalen != sizeof ( uid_t )) { <nl> rc = - EIO ; <nl> cFYI ( 1 , "% s : Downcall contained malformed key "
static void set_times ( struct tca6507_chip * tca , int bank ) <nl> int result ; <nl>  <nl> result = choose_times ( tca -> bank [ bank ]. ontime , & c1 , & c2 ); <nl> + if ( result < 0 ) <nl> + return ; <nl> dev_dbg (& tca -> client -> dev , <nl> " Chose on times % d (% d ) % d (% d ) for % dms \ n ", <nl> c1 , time_codes [ c1 ],
void mdfld_dbi_dsr_exit ( struct drm_device * dev ) <nl> struct drm_psb_private * dev_priv = dev -> dev_private ; <nl> struct mdfld_dbi_dsr_info * dsr_info = dev_priv -> dbi_dsr_info ; <nl>  <nl> - if (! dsr_info ) { <nl> + if ( dsr_info ) { <nl> del_timer_sync (& dsr_info -> dsr_timer ); <nl> kfree ( dsr_info ); <nl> dev_priv -> dbi_dsr_info = NULL ;
# include < linux / version . h > <nl>  <nl> /* Simplified build - specific string for starting entropy . */ <nl> - static const char * build_str = UTS_RELEASE " (" LINUX_COMPILE_BY "@" <nl> + static const char build_str [] = UTS_RELEASE " (" LINUX_COMPILE_BY "@" <nl> LINUX_COMPILE_HOST ") (" LINUX_COMPILER ") " UTS_VERSION ; <nl>  <nl> # define I8254_PORT_CONTROL 0x43
static int ecryptfs_setattr ( struct dentry * dentry , struct iattr * ia ) <nl> } <nl> } <nl> mutex_unlock (& crypt_stat -> cs_mutex ); <nl> + if ( S_ISREG ( inode -> i_mode )) { <nl> + rc = filemap_write_and_wait ( inode -> i_mapping ); <nl> + if ( rc ) <nl> + goto out ; <nl> + fsstack_copy_attr_all ( inode , lower_inode ); <nl> + } <nl> memcpy (& lower_ia , ia , sizeof ( lower_ia )); <nl> if ( ia -> ia_valid & ATTR_FILE ) <nl> lower_ia . ia_file = ecryptfs_file_to_lower ( ia -> ia_file );
static int __devinit sdhci_esdhc_imx_probe ( struct platform_device * pdev ) <nl> clk_prepare_enable ( clk ); <nl> pltfm_host -> clk = clk ; <nl>  <nl> - if (! is_imx25_esdhc ( imx_data )) <nl> - host -> quirks |= SDHCI_QUIRK_BROKEN_TIMEOUT_VAL ; <nl> + host -> quirks |= SDHCI_QUIRK_BROKEN_TIMEOUT_VAL ; <nl>  <nl> if ( is_imx25_esdhc ( imx_data ) || is_imx35_esdhc ( imx_data )) <nl> /* Fix errata ENGcm07207 present on i . MX25 and i . MX35 */
static u32 asle_set_backlight ( struct drm_device * dev , u32 bclp ) <nl> return ASLE_BACKLIGHT_FAILED ; <nl>  <nl> intel_panel_set_backlight ( dev , bclp , 255 ); <nl> - iowrite32 (( bclp * 0x64 )/ 0xff | ASLE_CBLV_VALID , & asle -> cblv ); <nl> + iowrite32 ( DIV_ROUND_UP ( bclp * 100 , 255 ) | ASLE_CBLV_VALID , & asle -> cblv ); <nl>  <nl> return 0 ; <nl> }
static void rt2800_config_channel_rf55xx ( struct rt2x00_dev * rt2x00dev , <nl> rt2800_rfcsr_write ( rt2x00dev , 49 , rfcsr ); <nl>  <nl> rt2800_rfcsr_read ( rt2x00dev , 50 , & rfcsr ); <nl> - if ( info -> default_power1 > power_bound ) <nl> + if ( info -> default_power2 > power_bound ) <nl> rt2x00_set_field8 (& rfcsr , RFCSR50_TX , power_bound ); <nl> else <nl> rt2x00_set_field8 (& rfcsr , RFCSR50_TX , info -> default_power2 );
static int snd_hdspm_playback_open ( struct snd_pcm_substream * substream ) <nl> snd_pcm_hw_constraint_minmax ( runtime , <nl> SNDRV_PCM_HW_PARAM_PERIOD_SIZE , <nl> 64 , 8192 ); <nl> + snd_pcm_hw_constraint_minmax ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS , <nl> + 2 , 2 ); <nl> break ; <nl> } <nl>  <nl> static int snd_hdspm_capture_open ( struct snd_pcm_substream * substream ) <nl> snd_pcm_hw_constraint_minmax ( runtime , <nl> SNDRV_PCM_HW_PARAM_PERIOD_SIZE , <nl> 64 , 8192 ); <nl> + snd_pcm_hw_constraint_minmax ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS , <nl> + 2 , 2 ); <nl> break ; <nl> } <nl> 
static int fd_ioctl ( struct block_device * bdev , fmode_t mode , unsigned int cmd , <nl> (( cmd & 0x80 ) && ! capable ( CAP_SYS_ADMIN ))) <nl> return - EPERM ; <nl>  <nl> + if ( WARN_ON ( size < 0 || size > sizeof ( inparam ))) <nl> + return - EINVAL ; <nl> + <nl> /* copyin */ <nl> CLEARSTRUCT (& inparam ); <nl> if ( _IOC_DIR ( cmd ) & _IOC_WRITE )
static int handshake_on_error_set_halt ( struct ehci_hcd * ehci , void __iomem * ptr , <nl> if ( error ) { <nl> ehci_halt ( ehci ); <nl> ehci_to_hcd ( ehci )-> state = HC_STATE_HALT ; <nl> - ehci_err ( ehci , " force halt ; handhake % p % 08x % 08x -> % d \ n ", <nl> + ehci_err ( ehci , " force halt ; handshake % p % 08x % 08x -> % d \ n ", <nl> ptr , mask , done , error ); <nl> } <nl> 
static bool vgic_its_check_device_id ( struct kvm * kvm , struct vgic_its * its , <nl> & indirect_ptr , sizeof ( indirect_ptr ))) <nl> return false ; <nl>  <nl> + indirect_ptr = le64_to_cpu ( indirect_ptr ); <nl> + <nl> /* check the valid bit of the first level entry */ <nl> if (!( indirect_ptr & BIT_ULL ( 63 ))) <nl> return false ;
static int of_fsl_spi_get_chipselects ( struct device * dev ) <nl> return 0 ; <nl> } <nl>  <nl> - pinfo -> gpios = kmalloc ( ngpios * sizeof (* pinfo -> gpios ), GFP_KERNEL ); <nl> + pinfo -> gpios = kmalloc_array ( ngpios , sizeof (* pinfo -> gpios ), <nl> + GFP_KERNEL ); <nl> if (! pinfo -> gpios ) <nl> return - ENOMEM ; <nl> memset ( pinfo -> gpios , - 1 , ngpios * sizeof (* pinfo -> gpios ));
static int pn533_target_found ( struct pn533 * dev , <nl> if ( resp -> tg != 1 ) <nl> return - EPROTO ; <nl>  <nl> + memset (& nfc_tgt , 0 , sizeof ( struct nfc_target )); <nl> + <nl> target_data_len = resp_len - sizeof ( struct pn533_poll_response ); <nl>  <nl> switch ( dev -> poll_mod_curr ) {
static int cp2112_gpio_direction_input ( struct gpio_chip * chip , unsigned offset ) <nl>  <nl> exit : <nl> mutex_unlock (& dev -> lock ); <nl> - return ret <= 0 ? ret : - EIO ; <nl> + return ret < 0 ? ret : - EIO ; <nl> } <nl>  <nl> static void cp2112_gpio_set ( struct gpio_chip * chip , unsigned offset , int value )
static void uniphier_pctl_pin_dbg_show ( struct pinctrl_dev * pctldev , <nl> case UNIPHIER_PIN_PULL_DOWN : <nl> pull_dir = " DOWN "; <nl> break ; <nl> + case UNIPHIER_PIN_PULL_UP_FIXED : <nl> + pull_dir = " UP ( FIXED )"; <nl> + break ; <nl> + case UNIPHIER_PIN_PULL_DOWN_FIXED : <nl> + pull_dir = " DOWN ( FIXED )"; <nl> + break ; <nl> case UNIPHIER_PIN_PULL_NONE : <nl> pull_dir = " NONE "; <nl> break ;
static int mmci_probe ( struct amba_device * dev , <nl> dev_dbg ( mmc_dev ( mmc ), " clocking block at % u Hz \ n ", mmc -> f_max ); <nl>  <nl> /* Get regulators and the supported OCR mask */ <nl> - mmc_regulator_get_supply ( mmc ); <nl> + ret = mmc_regulator_get_supply ( mmc ); <nl> + if ( ret == - EPROBE_DEFER ) <nl> + goto clk_disable ; <nl> + <nl> if (! mmc -> ocr_avail ) <nl> mmc -> ocr_avail = plat -> ocr_mask ; <nl> else if ( plat -> ocr_mask )
static struct print_arg * make_bprint_args ( char * fmt , void * data , int size , struc <nl> goto process_again ; <nl> case '.': <nl> goto process_again ; <nl> + case ' z ': <nl> + case ' Z ': <nl> + ls = 1 ; <nl> + goto process_again ; <nl> case ' p ': <nl> ls = 1 ; <nl> /* fall through */
static int ath10k_start ( struct ieee80211_hw * hw ) <nl> goto err_core_stop ; <nl> } <nl>  <nl> + ret = ath10k_wmi_pdev_set_param ( ar , <nl> + ar -> wmi . pdev_param -> ani_enable , 1 ); <nl> + if ( ret ) { <nl> + ath10k_warn ( ar , " failed to enable ani by default : % d \ n ", <nl> + ret ); <nl> + goto err_core_stop ; <nl> + } <nl> + <nl> ar -> num_started_vdevs = 0 ; <nl> ath10k_regd_update ( ar ); <nl> 
static int pb0100_start ( struct sd * sd ) <nl>  <nl> intf = usb_ifnum_to_if ( sd -> gspca_dev . dev , sd -> gspca_dev . iface ); <nl> alt = usb_altnum_to_altsetting ( intf , sd -> gspca_dev . alt ); <nl> + if (! alt ) <nl> + return - ENODEV ; <nl> packet_size = le16_to_cpu ( alt -> endpoint [ 0 ]. desc . wMaxPacketSize ); <nl>  <nl> /* If we don ' t have enough bandwidth use a lower framerate */
acpi_db_walk_for_execute ( acpi_handle obj_handle , <nl>  <nl> status = acpi_get_object_info ( obj_handle , & obj_info ); <nl> if ( ACPI_FAILURE ( status )) { <nl> + ACPI_FREE ( pathname ); <nl> return ( status ); <nl> } <nl> 
static int ml26124_hw_params ( struct snd_pcm_substream * substream , <nl> struct ml26124_priv * priv = snd_soc_codec_get_drvdata ( codec ); <nl> int i = get_coeff ( priv -> mclk , params_rate ( hw_params )); <nl>  <nl> + if ( i < 0 ) <nl> + return i ; <nl> priv -> substream = substream ; <nl> priv -> rate = params_rate ( hw_params ); <nl> 
void radeon_compute_pll_avivo ( struct radeon_pll * pll , <nl> ref_div_min = pll -> reference_div ; <nl> else <nl> ref_div_min = pll -> min_ref_div ; <nl> - ref_div_max = pll -> max_ref_div ; <nl> + <nl> + if ( pll -> flags & RADEON_PLL_USE_FRAC_FB_DIV && <nl> + pll -> flags & RADEON_PLL_USE_REF_DIV ) <nl> + ref_div_max = pll -> reference_div ; <nl> + else <nl> + ref_div_max = pll -> max_ref_div ; <nl>  <nl> /* determine allowed post divider range */ <nl> if ( pll -> flags & RADEON_PLL_USE_POST_DIV ) {
int cfg80211_mgd_wext_connect ( struct cfg80211_registered_device * rdev , <nl> if ( wdev -> wext . keys ) { <nl> wdev -> wext . keys -> def = wdev -> wext . default_key ; <nl> wdev -> wext . keys -> defmgmt = wdev -> wext . default_mgmt_key ; <nl> - wdev -> wext . connect . privacy = true ; <nl> + if ( wdev -> wext . default_key != - 1 ) <nl> + wdev -> wext . connect . privacy = true ; <nl> } <nl>  <nl> if (! wdev -> wext . connect . ssid_len )
static void cleanup_single_sta ( struct sta_info * sta ) <nl> * directly by station destruction . <nl> */ <nl> for ( i = 0 ; i < IEEE80211_NUM_TIDS ; i ++) { <nl> + kfree ( sta -> ampdu_mlme . tid_start_tx [ i ]); <nl> tid_tx = rcu_dereference_raw ( sta -> ampdu_mlme . tid_tx [ i ]); <nl> if (! tid_tx ) <nl> continue ;
void btrfs_cleanup_one_transaction ( struct btrfs_transaction * cur_trans , <nl>  <nl> btrfs_destroy_marked_extents ( root , & cur_trans -> dirty_pages , <nl> EXTENT_DIRTY ); <nl> + btrfs_destroy_pinned_extent ( root , <nl> + root -> fs_info -> pinned_extents ); <nl>  <nl> /* <nl> memset ( cur_trans , 0 , sizeof (* cur_trans ));
unsigned long acpi_realmode_flags ; <nl> static unsigned long acpi_realmode ; <nl>  <nl> # if defined ( CONFIG_SMP ) && defined ( CONFIG_64BIT ) <nl> - static char temp_stack [ 10240 ]; <nl> + static char temp_stack [ 4096 ]; <nl> # endif <nl>  <nl> /**
static void mce_async_callback ( struct urb * urb , struct pt_regs * regs ) <nl> mceusb_dev_printdata ( ir , urb -> transfer_buffer , 0 , len , true ); <nl> } <nl>  <nl> + /* the transfer buffer and urb were allocated in mce_request_packet */ <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> } <nl>  <nl> /* request incoming or send outgoing usb packet - used to initialize remote */
static void __del_gref ( struct gntalloc_gref * gref ) <nl>  <nl> if (! gnttab_end_foreign_access_ref ( gref -> gref_id , 0 )) <nl> return ; <nl> + <nl> + gnttab_free_grant_reference ( gref -> gref_id ); <nl> } <nl>  <nl> gref_size --;
i915_gem_execbuffer2 ( struct drm_device * dev , void * data , <nl> struct drm_i915_gem_exec_object2 * exec2_list = NULL ; <nl> int ret ; <nl>  <nl> - if ( args -> buffer_count < 1 ) { <nl> + if ( args -> buffer_count < 1 || <nl> + args -> buffer_count > UINT_MAX / sizeof (* exec2_list )) { <nl> DRM_DEBUG (" execbuf2 with % d buffers \ n ", args -> buffer_count ); <nl> return - EINVAL ; <nl> }
static int s3c64xx_setparent_clksrc ( struct clk * clk , struct clk * parent ) <nl> clksrc |= src_nr << sclk -> shift ; <nl>  <nl> __raw_writel ( clksrc , S3C_CLK_SRC ); <nl> + <nl> + clk -> parent = parent ; <nl> return 0 ; <nl> } <nl> 
void intel_setup_bios ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl>  <nl> /* Set the Panel Power On / Off timings if uninitialized . */ <nl> - if (( I915_READ ( PP_ON_DELAYS ) == 0 ) && ( I915_READ ( PP_OFF_DELAYS ) == 0 )) { <nl> + if (! HAS_PCH_SPLIT ( dev ) && <nl> + I915_READ ( PP_ON_DELAYS ) == 0 && I915_READ ( PP_OFF_DELAYS ) == 0 ) { <nl> /* Set T2 to 40ms and T5 to 200ms */ <nl> I915_WRITE ( PP_ON_DELAYS , 0x019007d0 ); <nl> 
static void acm_waker ( struct work_struct * waker ) <nl> static int acm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> { <nl> struct acm * acm ; <nl> - int rv = - EINVAL ; <nl> + int rv = - ENODEV ; <nl> int i ; <nl> dbg (" Entering acm_tty_open ."); <nl> 
static void b43_request_firmware ( struct work_struct * work ) <nl> for ( i = 0 ; i < B43_NR_FWTYPES ; i ++) { <nl> errmsg = ctx -> errors [ i ]; <nl> if ( strlen ( errmsg )) <nl> - b43err ( dev -> wl , errmsg ); <nl> + b43err ( dev -> wl , "% s ", errmsg ); <nl> } <nl> b43_print_fw_helptext ( dev -> wl , 1 ); <nl> goto out ;
int __init omap2_clk_provider_init ( struct device_node * parent , int index , <nl> clocks_node_ptr [ index ] = clocks ; <nl>  <nl> io = kzalloc ( sizeof (* io ), GFP_KERNEL ); <nl> + if (! io ) <nl> + return - ENOMEM ; <nl>  <nl> io -> regmap = syscon ; <nl> io -> mem = mem ;
static int dw_i2c_probe ( struct platform_device * pdev ) <nl> adap = & dev -> adapter ; <nl> i2c_set_adapdata ( adap , dev ); <nl> adap -> owner = THIS_MODULE ; <nl> - adap -> class = I2C_CLASS_HWMON | I2C_CLASS_DEPRECATED ; <nl> + adap -> class = I2C_CLASS_DEPRECATED ; <nl> strlcpy ( adap -> name , " Synopsys DesignWare I2C adapter ", <nl> sizeof ( adap -> name )); <nl> adap -> algo = & i2c_dw_algo ;
static int sa1111_resume ( struct platform_device * dev ) <nl> # define sa1111_resume NULL <nl> # endif <nl>  <nl> - static int sa1111_probe ( struct platform_device * pdev ) <nl> + static int __devinit sa1111_probe ( struct platform_device * pdev ) <nl> { <nl> struct resource * mem ; <nl> int irq ;
static unsigned int bsg_poll ( struct file * file , poll_table * wait ) <nl> spin_lock_irq (& bd -> lock ); <nl> if (! list_empty (& bd -> done_list )) <nl> mask |= POLLIN | POLLRDNORM ; <nl> - if ( bd -> queued_cmds >= bd -> max_queue ) <nl> + if ( bd -> queued_cmds < bd -> max_queue ) <nl> mask |= POLLOUT ; <nl> spin_unlock_irq (& bd -> lock ); <nl> 
static int gfx_v9_0_set_clockgating_state ( void * handle , <nl> { <nl> struct amdgpu_device * adev = ( struct amdgpu_device *) handle ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> switch ( adev -> asic_type ) { <nl> case CHIP_VEGA10 : <nl> gfx_v9_0_update_gfx_clock_gating ( adev ,
void __init s3c6400_init_irq ( void ) <nl> s3c64xx_init_irq (~ 0 & ~( 0xf << 5 ), ~ 0 ); <nl> } <nl>  <nl> - struct sysdev_class s3c6400_sysclass = { <nl> + static struct sysdev_class s3c6400_sysclass = { <nl> . name = " s3c6400 - core ", <nl> }; <nl> 
static void sky2_restart ( struct work_struct * work ) <nl>  <nl> rtnl_lock (); <nl>  <nl> - napi_disable (& hw -> napi ); <nl> - synchronize_irq ( hw -> pdev -> irq ); <nl> imask = sky2_read32 ( hw , B0_IMSK ); <nl> sky2_write32 ( hw , B0_IMSK , 0 ); <nl> + synchronize_irq ( hw -> pdev -> irq ); <nl> + napi_disable (& hw -> napi ); <nl>  <nl> for ( i = 0 ; i < hw -> ports ; i ++) { <nl> struct net_device * dev = hw -> dev [ i ];
void ath_detach ( struct ath_softc * sc ) <nl>  <nl> ath9k_hw_detach ( sc -> sc_ah ); <nl> ath9k_exit_debug ( sc ); <nl> - ath9k_ps_restore ( sc ); <nl> } <nl>  <nl> static int ath9k_reg_notifier ( struct wiphy * wiphy ,
static void bcm2048_rds_fifo_receive ( struct bcm2048_device * bdev ) <nl> bdev -> rds_info . radio_text , bdev -> fifo_size ); <nl> if ( err != 2 ) { <nl> dev_err (& bdev -> client -> dev , " RDS Read problem \ n "); <nl> + mutex_unlock (& bdev -> mutex ); <nl> return ; <nl> } <nl> 
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> ( rdev -> pdev -> device == 0x6667 )) { <nl> max_sclk = 75000 ; <nl> } <nl> + } else if ( rdev -> family == CHIP_OLAND ) { <nl> + if (( rdev -> pdev -> device == 0x6604 ) && <nl> + ( rdev -> pdev -> subsystem_vendor == 0x1028 ) && <nl> + ( rdev -> pdev -> subsystem_device == 0x066F )) { <nl> + max_sclk = 75000 ; <nl> + } <nl> } <nl>  <nl> if ( rps -> vce_active ) {
static void usbhsh_pipe_detach ( struct usbhsh_hpriv * hpriv , <nl> struct device * dev = usbhs_priv_to_dev ( priv ); <nl> unsigned long flags ; <nl>  <nl> + if ( unlikely (! uep )) { <nl> + dev_err ( dev , " no uep \ n "); <nl> + return ; <nl> + } <nl> + <nl> /******************** spin lock ********************/ <nl> usbhs_lock ( priv , flags ); <nl> 
static void efx_pci_remove ( struct pci_dev * pci_dev ) <nl> efx_dissociate ( efx ); <nl> dev_close ( efx -> net_dev ); <nl> efx_disable_interrupts ( efx ); <nl> + efx -> state = STATE_UNINIT ; <nl> rtnl_unlock (); <nl>  <nl> if ( efx -> type -> sriov_fini )
static int __devexit gpio_keys_remove ( struct platform_device * pdev ) <nl> } <nl>  <nl> input_unregister_device ( input ); <nl> + kfree ( ddata ); <nl>  <nl> return 0 ; <nl> }
__ieee80211_get_channel_mode ( struct ieee80211_local * local , <nl> if (! sdata -> u . ap . beacon ) <nl> continue ; <nl> break ; <nl> + case NL80211_IFTYPE_MESH_POINT : <nl> + if (! sdata -> wdev . mesh_id_len ) <nl> + continue ; <nl> + break ; <nl> default : <nl> break ; <nl> }
int i915_gem_stolen_setup_compression ( struct drm_device * dev , int size , int fb_c <nl> if (! drm_mm_initialized (& dev_priv -> mm . stolen )) <nl> return - ENODEV ; <nl>  <nl> - if ( size < dev_priv -> fbc . uncompressed_size ) <nl> + if ( size <= dev_priv -> fbc . uncompressed_size ) <nl> return 0 ; <nl>  <nl> /* Release any current block */
restore_state : <nl>  <nl> return err ; <nl> } <nl> + EXPORT_SYMBOL ( xfrm_migrate ); <nl> # endif <nl> 
MPI mpi_read_raw_data ( const void * xbuffer , size_t nbytes ) <nl> mpi_limb_t a ; <nl> MPI val = NULL ; <nl>  <nl> - while ( nbytes >= 0 && buffer [ 0 ] == 0 ) { <nl> + while ( nbytes > 0 && buffer [ 0 ] == 0 ) { <nl> buffer ++; <nl> nbytes --; <nl> }
int snd_soc_dapm_stream_event ( struct snd_soc_codec * codec , <nl> } <nl> } <nl> } <nl> - mutex_unlock (& codec -> mutex ); <nl>  <nl> dapm_power_widgets ( codec , event ); <nl> + mutex_unlock (& codec -> mutex ); <nl> dump_dapm ( codec , __func__ ); <nl> return 0 ; <nl> }
nfsd_cross_mnt ( struct svc_rqst * rqstp , struct dentry ** dpp , <nl>  <nl> exp2 = rqst_exp_get_by_name ( rqstp , mnt , mounts ); <nl> if ( IS_ERR ( exp2 )) { <nl> - err = PTR_ERR ( exp2 ); <nl> + if ( PTR_ERR ( exp2 ) != - ENOENT ) <nl> + err = PTR_ERR ( exp2 ); <nl> dput ( mounts ); <nl> mntput ( mnt ); <nl> goto out ;
journal_t * jbd2_journal_init_dev ( struct block_device * bdev , <nl> __FUNCTION__ ); <nl> kfree ( journal ); <nl> journal = NULL ; <nl> + goto out ; <nl> } <nl> journal -> j_dev = bdev ; <nl> journal -> j_fs_dev = fs_dev ; <nl> journal_t * jbd2_journal_init_dev ( struct block_device * bdev , <nl> J_ASSERT ( bh != NULL ); <nl> journal -> j_sb_buffer = bh ; <nl> journal -> j_superblock = ( journal_superblock_t *) bh -> b_data ; <nl> - <nl> + out : <nl> return journal ; <nl> } <nl> 
cifs_put_tcon ( struct cifsTconInfo * tcon ) <nl> CIFSSMBTDis ( xid , tcon ); <nl> _FreeXid ( xid ); <nl>  <nl> - tconInfoFree ( tcon ); <nl> cifs_fscache_release_super_cookie ( tcon ); <nl> + tconInfoFree ( tcon ); <nl> cifs_put_smb_ses ( ses ); <nl> } <nl> 
int snd_es1688_pcm ( struct snd_card * card , struct snd_es1688 * chip , int device ) <nl>  <nl> pcm -> private_data = chip ; <nl> pcm -> info_flags = SNDRV_PCM_INFO_HALF_DUPLEX ; <nl> - sprintf ( pcm -> name , snd_es1688_chip_id ( chip )); <nl> + strcpy ( pcm -> name , snd_es1688_chip_id ( chip )); <nl> chip -> pcm = pcm ; <nl>  <nl> snd_pcm_lib_preallocate_pages_for_all ( pcm , SNDRV_DMA_TYPE_DEV ,
rerun_vcpu : <nl> if ( rc == SIE_INTERCEPT_RERUNVCPU ) <nl> goto rerun_vcpu ; <nl>  <nl> - if ( signal_pending ( current ) && ! rc ) <nl> + if ( signal_pending ( current ) && ! rc ) { <nl> + kvm_run -> exit_reason = KVM_EXIT_INTR ; <nl> rc = - EINTR ; <nl> + } <nl>  <nl> if ( rc == - ENOTSUPP ) { <nl> /* intercept cannot be handled in - kernel , prepare kvm - run */
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 81 " <nl> -# define DRV_MODULE_RELDATE " September 5 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 82 " <nl> +# define DRV_MODULE_RELDATE " October 5 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static int rose_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> skb_copy_datagram_iovec ( skb , 0 , msg -> msg_iov , copied ); <nl>  <nl> if ( srose != NULL ) { <nl> + memset ( srose , 0 , msg -> msg_namelen ); <nl> srose -> srose_family = AF_ROSE ; <nl> srose -> srose_addr = rose -> dest_addr ; <nl> srose -> srose_call = rose -> dest_call ;
static int __open_dso ( struct dso * dso , struct machine * machine ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if (! is_regular_file ( name )) <nl> + return - EINVAL ; <nl> + <nl> fd = do_open ( name ); <nl> free ( name ); <nl> return fd ;
 <nl> bool rtw_IOL_applied ( struct adapter * adapter ) <nl> { <nl> - if ( 1 == adapter -> registrypriv . fw_iol ) <nl> + if ( adapter -> registrypriv . fw_iol == 1 ) <nl> return true ; <nl>  <nl> - if (( 2 == adapter -> registrypriv . fw_iol ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> + if (( adapter -> registrypriv . fw_iol == 2 ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> return true ; <nl> return false ; <nl> }
static struct pxamci_platform_data magician_mci_info = { <nl>  <nl> static struct pxaohci_platform_data magician_ohci_info = { <nl> . port_mode = PMM_PERPORT_MODE , <nl> - . flags = ENABLE_PORT1 | ENABLE_PORT3 | POWER_CONTROL_LOW , <nl> + /* port1 : CSR Bluetooth , port2 : OTG with UDC */ <nl> + . flags = ENABLE_PORT1 | ENABLE_PORT2 | POWER_CONTROL_LOW , <nl> . power_budget = 0 , <nl> + . power_on_delay = 100 , <nl> }; <nl>  <nl> /*
struct mapped_device * dm_get_from_kobject ( struct kobject * kobj ) <nl> if (& md -> kobj != kobj ) <nl> return NULL ; <nl>  <nl> + if ( test_bit ( DMF_FREEING , & md -> flags ) || <nl> + test_bit ( DMF_DELETING , & md -> flags )) <nl> + return NULL ; <nl> + <nl> dm_get ( md ); <nl> return md ; <nl> }
int extcon_register_interest ( struct extcon_specific_cable_nb * obj , <nl>  <nl> obj -> cable_index = extcon_find_cable_index ( obj -> edev , cable_name ); <nl> if ( obj -> cable_index < 0 ) <nl> - return - ENODEV ; <nl> + return obj -> cable_index ; <nl>  <nl> obj -> user_nb = nb ; <nl> 
static u32 __seccomp_phase1_filter ( int this_syscall , struct seccomp_data * sd ) <nl>  <nl> switch ( action ) { <nl> case SECCOMP_RET_ERRNO : <nl> - /* Set the low - order 16 - bits as a errno . */ <nl> + /* Set low - order bits as an errno , capped at MAX_ERRNO . */ <nl> + if ( data > MAX_ERRNO ) <nl> + data = MAX_ERRNO ; <nl> syscall_set_return_value ( current , task_pt_regs ( current ), <nl> - data , 0 ); <nl> goto skip ;
static ssize_t numa_node_store ( struct device * dev , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - if (! node_online ( node )) <nl> + if ( node >= MAX_NUMNODES || ! node_online ( node )) <nl> return - EINVAL ; <nl>  <nl> add_taint ( TAINT_FIRMWARE_WORKAROUND , LOCKDEP_STILL_OK );
static void __init s3c24xx_gpiolib_add_chips ( struct samsung_gpio_chip * chip , <nl> struct gpio_chip * gc = & chip -> chip ; <nl>  <nl> for ( i = 0 ; i < nr_chips ; i ++, chip ++) { <nl> + /* skip banks not present on SoC */ <nl> + if ( chip -> chip . base >= S3C_GPIO_END ) <nl> + continue ; <nl> + <nl> if (! chip -> config ) <nl> chip -> config = & s3c24xx_gpiocfg_default ; <nl> if (! chip -> pm )
static void maxiradio_remove ( struct pci_dev * pdev ) <nl> outb ( 0 , dev -> io ); <nl> v4l2_device_unregister ( v4l2_dev ); <nl> release_region ( pci_resource_start ( pdev , 0 ), pci_resource_len ( pdev , 0 )); <nl> + kfree ( dev ); <nl> } <nl>  <nl> static struct pci_device_id maxiradio_pci_tbl [] = {
int snd_timer_new ( struct snd_card * card , char * id , struct snd_timer_id * tid , <nl> timer -> tmr_subdevice = tid -> subdevice ; <nl> if ( id ) <nl> strlcpy ( timer -> id , id , sizeof ( timer -> id )); <nl> + timer -> sticks = 1 ; <nl> INIT_LIST_HEAD (& timer -> device_list ); <nl> INIT_LIST_HEAD (& timer -> open_list_head ); <nl> INIT_LIST_HEAD (& timer -> active_list_head );
static const char * const cw1200_debug_link_id [] = { <nl> " REQ ", <nl> " SOFT ", <nl> " HARD ", <nl> + " RESET ", <nl> + " RESET_REMAP ", <nl> }; <nl>  <nl> static const char * cw1200_debug_mode ( int mode )
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> * anyway ( it holds no special properties of the bond device ), <nl> * so we can change it without calling change_active_interface () <nl> */ <nl> - if (! bond -> curr_active_slave ) <nl> + if (! bond -> curr_active_slave && new_slave -> link == BOND_LINK_UP ) <nl> bond -> curr_active_slave = new_slave ; <nl>  <nl> break ;
# define SCLK_MACREF_OUT 106 <nl> # define SCLK_VOP0_PWM 107 <nl> # define SCLK_VOP1_PWM 108 <nl> -# define SCLK_RGA 109 <nl> +# define SCLK_RGA_CORE 109 <nl> # define SCLK_ISP0 110 <nl> # define SCLK_ISP1 111 <nl> # define SCLK_HDMI_CEC 112
static int vtg_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl> } <nl> vtg -> regs = devm_ioremap_nocache ( dev , res -> start , resource_size ( res )); <nl> + if (! vtg -> regs ) { <nl> + DRM_ERROR (" failed to remap I / O memory \ n "); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> np = of_parse_phandle ( pdev -> dev . of_node , " st , slave ", 0 ); <nl> if ( np ) {
static int __init mod_init ( void ) <nl> if ( err ) { <nl> printk ( KERN_ERR PFX " RNG registering failed (% d )\ n ", <nl> err ); <nl> - goto out ; <nl> + goto err_unmap ; <nl> } <nl> out : <nl> return err ;
static int inet6_rtm_getroute ( struct sk_buff * in_skb , struct nlmsghdr * nlh ) <nl> rt = ( struct rt6_info *) ip6_route_output ( net , NULL , & fl6 ); <nl> } <nl>  <nl> + if ( rt == net -> ipv6 . ip6_null_entry ) { <nl> + err = rt -> dst . error ; <nl> + ip6_rt_put ( rt ); <nl> + goto errout ; <nl> + } <nl> + <nl> skb = alloc_skb ( NLMSG_GOODSIZE , GFP_KERNEL ); <nl> if (! skb ) { <nl> ip6_rt_put ( rt );
static int i915_context_status ( struct seq_file * m , void * unused ) <nl> } <nl>  <nl> list_for_each_entry ( ctx , & dev_priv -> context_list , link ) { <nl> + if ( ctx -> obj == NULL ) <nl> + continue ; <nl> + <nl> seq_puts ( m , " HW context "); <nl> describe_ctx ( m , ctx ); <nl> for_each_ring ( ring , dev_priv , i )
void rtl_8821ae_c2h_command_handle ( struct ieee80211_hw * hw ) <nl> rtl_write_byte ( rtlpriv , 0x1AF , 0x00 ); <nl> return ; <nl> } <nl> - ptmp_buf = ( u8 *) kmalloc ( c2h_event . cmd_len , GFP_KERNEL ); <nl> + ptmp_buf = kmalloc ( c2h_event . cmd_len , GFP_KERNEL ); <nl> if ( ptmp_buf == NULL ) { <nl> RT_TRACE ( COMP_FW , DBG_TRACE , (" malloc cmd buf failed \ n ")); <nl> return ;
void tipc_link_delete ( struct link * l_ptr ) <nl>  <nl> static void link_start ( struct link * l_ptr ) <nl> { <nl> + tipc_node_lock ( l_ptr -> owner ); <nl> link_state_event ( l_ptr , STARTING_EVT ); <nl> + tipc_node_unlock ( l_ptr -> owner ); <nl> } <nl>  <nl> /**
static int bnx2x_set_pauseparam ( struct net_device * dev , <nl> bp -> link_params . req_flow_ctrl [ cfg_idx ] = <nl> BNX2X_FLOW_CTRL_AUTO ; <nl> } <nl> + bp -> link_params . req_fc_auto_adv = BNX2X_FLOW_CTRL_NONE ; <nl> + if ( epause -> rx_pause ) <nl> + bp -> link_params . req_fc_auto_adv |= BNX2X_FLOW_CTRL_RX ; <nl> + <nl> + if ( epause -> tx_pause ) <nl> + bp -> link_params . req_fc_auto_adv |= BNX2X_FLOW_CTRL_TX ; <nl> } <nl>  <nl> DP ( BNX2X_MSG_ETHTOOL ,
static struct tty_struct * receive_chars ( struct uart_port * port , struct pt_regs * <nl> break ; <nl>  <nl> if ( c == CON_BREAK ) { <nl> + if ( uart_handle_break ( port )) <nl> + continue ; <nl> saw_console_brk = 1 ; <nl> c = 0 ; <nl> }
static void __devexit pm2fb_remove ( struct pci_dev * pdev ) <nl> release_mem_region ( fix -> mmio_start , fix -> mmio_len ); <nl>  <nl> pci_set_drvdata ( pdev , NULL ); <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> kfree ( info -> pixmap . addr ); <nl> kfree ( info ); <nl> }
static void add_pin_to_irq_node ( struct irq_cfg * cfg , int node , int apic , int pin <nl> } <nl>  <nl> entry = get_one_free_irq_2_pin ( node ); <nl> + if (! entry ) { <nl> + printk ( KERN_ERR " can not alloc irq_pin_list \ n "); <nl> + BUG_ON ( 1 ); <nl> + } <nl> entry -> apic = apic ; <nl> entry -> pin = pin ; <nl> 
static struct irqaction psurge_irqaction = { <nl>  <nl> static void __init smp_psurge_setup_cpu ( int cpu_nr ) <nl> { <nl> - if ( cpu_nr != 0 ) <nl> + if ( cpu_nr != 0 || ! psurge_start ) <nl> return ; <nl>  <nl> /* reset the entry point so if we get another intr we won ' t
xfs_mountfs ( <nl> * Allocate and initialize the per - ag data . <nl> */ <nl> spin_lock_init (& mp -> m_perag_lock ); <nl> - INIT_RADIX_TREE (& mp -> m_perag_tree , GFP_NOFS ); <nl> + INIT_RADIX_TREE (& mp -> m_perag_tree , GFP_ATOMIC ); <nl> error = xfs_initialize_perag ( mp , sbp -> sb_agcount , & mp -> m_maxagi ); <nl> if ( error ) { <nl> cmn_err ( CE_WARN , " XFS : Failed per - ag init : % d ", error );
static int saa7134_try_get_set_fmt_vbi_cap ( struct file * file , void * priv , <nl> struct saa7134_dev * dev = fh -> dev ; <nl> struct saa7134_tvnorm * norm = dev -> tvnorm ; <nl>  <nl> + memset (& f -> fmt . vbi . reserved , 0 , sizeof ( f -> fmt . vbi . reserved )); <nl> f -> fmt . vbi . sampling_rate = 6750000 * 4 ; <nl> f -> fmt . vbi . samples_per_line = 2048 /* VBI_LINE_LENGTH */; <nl> f -> fmt . vbi . sample_format = V4L2_PIX_FMT_GREY ;
struct uts_namespace ; <nl>  <nl> extern cpumask_var_t cpu_isolated_map ; <nl>  <nl> - extern int runqueue_is_locked ( int cpu ); <nl> - <nl> extern void scheduler_tick ( void ); <nl>  <nl> # define MAX_SCHEDULE_TIMEOUT LONG_MAX
static int cp2112_read ( struct cp2112_device * dev , u8 * data , size_t size ) <nl> struct cp2112_force_read_report report ; <nl> int ret ; <nl>  <nl> + if ( size > sizeof ( dev -> read_data )) <nl> + size = sizeof ( dev -> read_data ); <nl> report . report = CP2112_DATA_READ_FORCE_SEND ; <nl> report . length = cpu_to_be16 ( size ); <nl> 
struct net_device * alloc_rtllib ( int sizeof_priv ) <nl> rtllib_softmac_init ( ieee ); <nl>  <nl> ieee -> pHTInfo = kzalloc ( sizeof ( struct rt_hi_throughput ), GFP_KERNEL ); <nl> - if ( ieee -> pHTInfo == NULL ) <nl> + if (! ieee -> pHTInfo ) <nl> return NULL ; <nl>  <nl> HTUpdateDefaultSetting ( ieee );
static hda_nid_t set_path_power ( struct hda_codec * codec , hda_nid_t nid , <nl>  <nl> for ( n = 0 ; n < spec -> paths . used ; n ++) { <nl> path = snd_array_elem (& spec -> paths , n ); <nl> + if (! path -> depth ) <nl> + continue ; <nl> if ( path -> path [ 0 ] == nid || <nl> path -> path [ path -> depth - 1 ] == nid ) { <nl> bool pin_old = path -> pin_enabled ;
u32 mp_query_psd ( struct adapter * pAdapter , u8 * data ) <nl> sscanf ( data , " pts =% d , start =% d , stop =% d ", & psd_pts , & psd_start , & psd_stop ); <nl> } <nl>  <nl> - _rtw_memset ( data , '\ 0 ', sizeof ( data )); <nl> + _rtw_memset ( data , '\ 0 ', sizeof (* data )); <nl>  <nl> i = psd_start ; <nl> while ( i < psd_stop ) {
static void o2hb_region_release ( struct config_item * item ) <nl> debugfs_remove ( reg -> hr_debug_dir ); <nl> kfree ( reg -> hr_db_livenodes ); <nl> kfree ( reg -> hr_db_regnum ); <nl> - kfree ( reg -> hr_debug_elapsed_time ); <nl> - kfree ( reg -> hr_debug_pinned ); <nl> + kfree ( reg -> hr_db_elapsed_time ); <nl> + kfree ( reg -> hr_db_pinned ); <nl>  <nl> spin_lock (& o2hb_live_lock ); <nl> list_del (& reg -> hr_all_item );
# define PAGE_SIZE ( _AC ( 1 , UL ) << PAGE_SHIFT ) <nl> # define PAGE_MASK (~( PAGE_SIZE - 1 )) <nl>  <nl> -# define __PHYSICAL_MASK (( phys_addr_t )( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> +# define __PHYSICAL_MASK (( phys_addr_t )(( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 )) <nl> # define __VIRTUAL_MASK (( 1UL << __VIRTUAL_MASK_SHIFT ) - 1 ) <nl>  <nl> /* Cast PAGE_MASK to a signed type so that it is sign - extended if
int hfsplus_parse_options ( char * input , struct hfsplus_sb_info * sbi ) <nl> return 0 ; <nl> } <nl> p = match_strdup (& args [ 0 ]); <nl> - sbi -> nls = load_nls ( p ); <nl> + if ( p ) <nl> + sbi -> nls = load_nls ( p ); <nl> if (! sbi -> nls ) { <nl> printk ( KERN_ERR " hfs : unable to load nls mapping \"% s \"\ n ", p ); <nl> kfree ( p );
static int map_sg_data ( struct scsi_cmnd * cmd , <nl> sdev_printk ( KERN_ERR , cmd -> device , <nl> " Can ' t allocate memory " <nl> " for indirect table \ n "); <nl> + scsi_dma_unmap ( cmd ); <nl> return 0 ; <nl> } <nl> }
try_offline_again : <nl> */ <nl> ata_msleep ( ap , 1 ); <nl>  <nl> + sata_set_spd ( link ); <nl> + <nl> /* <nl> * Now , bring the host controller online again , this can take time <nl> * as PHY reset and communication establishment , 1st D2H FIS and
static void mt_feature_mapping ( struct hid_device * hdev , <nl> td -> is_buttonpad = true ; <nl>  <nl> break ; <nl> + case 0xff0000c5 : <nl> + /* Retrieve the Win8 blob once to enable some devices */ <nl> + if ( usage -> usage_index == 0 ) <nl> + mt_get_feature ( hdev , field -> report ); <nl> + break ; <nl> } <nl> } <nl> 
int mc13xxx_common_init ( struct mc13xxx * mc13xxx , <nl> err_mask : <nl> err_revision : <nl> mc13xxx_unlock ( mc13xxx ); <nl> - kfree ( mc13xxx ); <nl> return ret ; <nl> } <nl> 
static int mt9t112_probe ( struct i2c_client * client , <nl> v4l2_i2c_subdev_init (& priv -> subdev , client , & mt9t112_subdev_ops ); <nl>  <nl> ret = mt9t112_camera_probe ( client ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> kfree ( priv ); <nl> + return ret ; <nl> + } <nl>  <nl> /* Cannot fail : using the default supported pixel code */ <nl> mt9t112_set_params ( priv , & rect , V4L2_MBUS_FMT_UYVY8_2X8 );
int ath5k_hw_attach ( struct ath5k_softc * sc ) <nl> ah -> ah_limit_tx_retries = AR5K_INIT_TX_RETRY ; <nl> ah -> ah_software_retry = false ; <nl> ah -> ah_ant_mode = AR5K_ANTMODE_DEFAULT ; <nl> + ah -> ah_noise_floor = - 95 ; /* until first NF calibration is run */ <nl>  <nl> /* <nl> * Find the mac version
int kernfs_rename_ns ( struct kernfs_node * kn , struct kernfs_node * new_parent , <nl> if (! new_name ) <nl> goto out ; <nl>  <nl> - kfree ( kn -> name ); <nl> + if ( kn -> flags & KERNFS_STATIC_NAME ) <nl> + kn -> flags &= ~ KERNFS_STATIC_NAME ; <nl> + else <nl> + kfree ( kn -> name ); <nl> + <nl> kn -> name = new_name ; <nl> } <nl> 
static int mt8173_nor_write_single_byte ( struct mt8173_nor * mt8173_nor , <nl> mt8173_nor_set_addr ( mt8173_nor , addr ); <nl>  <nl> for ( i = 0 ; i < length ; i ++) { <nl> + writeb (* data ++, mt8173_nor -> base + MTK_NOR_WDATA_REG ); <nl> ret = mt8173_nor_execute_cmd ( mt8173_nor , MTK_NOR_PIO_WR_CMD ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> - writeb (* data ++, mt8173_nor -> base + MTK_NOR_WDATA_REG ); <nl> } <nl> return 0 ; <nl> }
static int __devinit xencons_probe ( struct xenbus_device * dev , <nl> if ( devid == 0 ) <nl> return - ENODEV ; <nl>  <nl> - info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL | __GFP_ZERO ); <nl> + info = kzalloc ( sizeof ( struct xencons_info ), GFP_KERNEL ); <nl> if (! info ) <nl> - goto error_nomem ; <nl> + return - ENOMEM ; <nl> dev_set_drvdata (& dev -> dev , info ); <nl> info -> xbdev = dev ; <nl> info -> vtermno = xenbus_devid_to_vtermno ( devid );
static long privcmd_ioctl ( struct file * file , <nl> # ifndef HAVE_ARCH_PRIVCMD_MMAP <nl> static int privcmd_fault ( struct vm_area_struct * vma , struct vm_fault * vmf ) <nl> { <nl> + printk ( KERN_DEBUG " privcmd_fault : vma =% p % lx -% lx , pgoff =% lx , uv =% p \ n ", <nl> + vma , vma -> vm_start , vma -> vm_end , <nl> + vmf -> pgoff , vmf -> virtual_address ); <nl> + <nl> return VM_FAULT_SIGBUS ; <nl> } <nl> 
__ip_vs_get_dest_entries ( struct net * net , const struct ip_vs_get_dests * get , <nl> struct ip_vs_dest * dest ; <nl> struct ip_vs_dest_entry entry ; <nl>  <nl> + memset (& entry , 0 , sizeof ( entry )); <nl> list_for_each_entry ( dest , & svc -> destinations , n_list ) { <nl> if ( count >= get -> num_dests ) <nl> break ;
static int serio_raw_connect ( struct serio * serio , struct serio_driver * drv ) <nl>  <nl> serio_raw -> dev . minor = PSMOUSE_MINOR ; <nl> serio_raw -> dev . name = serio_raw -> name ; <nl> + serio_raw -> dev . dev = & serio -> dev ; <nl> serio_raw -> dev . fops = & serio_raw_fops ; <nl>  <nl> err = misc_register (& serio_raw -> dev );
static int ixgbe_resume ( struct pci_dev * pdev ) <nl>  <nl> pci_wake_from_d3 ( pdev , false ); <nl>  <nl> + rtnl_lock (); <nl> err = ixgbe_init_interrupt_scheme ( adapter ); <nl> + rtnl_unlock (); <nl> if ( err ) { <nl> e_dev_err (" Cannot initialize interrupts for device \ n "); <nl> return err ;
int snd_usb_caiaq_audio_init ( struct snd_usb_caiaqdev * cdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( cdev -> n_streams < 2 ) { <nl> + dev_err ( dev , " bogus number of streams : % d \ n ", cdev -> n_streams ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> ret = snd_pcm_new ( cdev -> chip . card , cdev -> product_name , 0 , <nl> cdev -> n_audio_out , cdev -> n_audio_in , & cdev -> pcm ); <nl> 
static bool radeon_msi_ok ( struct radeon_device * rdev ) <nl> ( rdev -> pdev -> subsystem_device == 0x0185 )) <nl> return true ; <nl>  <nl> + /* try and enable MSIs by default on all RS690s */ <nl> + if ( rdev -> family == CHIP_RS690 ) <nl> + return true ; <nl> + <nl> /* RV515 seems to have MSI issues where it loses <nl> * MSI rearms occasionally . This leads to lockups and freezes . <nl> * disable it by default .
struct spi_message { <nl> void * state ; <nl> }; <nl>  <nl> + static inline void spi_message_init_no_memset ( struct spi_message * m ) <nl> +{ <nl> + INIT_LIST_HEAD (& m -> transfers ); <nl> +} <nl> + <nl> static inline void spi_message_init ( struct spi_message * m ) <nl> { <nl> memset ( m , 0 , sizeof * m ); <nl> - INIT_LIST_HEAD (& m -> transfers ); <nl> + spi_message_init_no_memset ( m ); <nl> } <nl>  <nl> static inline void
static int xfrm_add_pol_expire ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl> if ( err ) <nl> return err ; <nl>  <nl> + err = verify_policy_dir ( p -> dir ); <nl> + if ( err ) <nl> + return err ; <nl> + <nl> if ( p -> index ) <nl> xp = xfrm_policy_byid ( net , mark , type , p -> dir , p -> index , 0 , & err ); <nl> else {
static int usb_audio_probe ( struct usb_interface * intf , <nl> goto __error ; <nl> } <nl> chip = usb_chip [ i ]; <nl> - dev_set_drvdata (& dev -> dev , chip ); <nl> atomic_inc (& chip -> active ); /* avoid autopm */ <nl> break ; <nl> } <nl> static int usb_audio_probe ( struct usb_interface * intf , <nl> goto __error ; <nl> } <nl> } <nl> + dev_set_drvdata (& dev -> dev , chip ); <nl>  <nl> /* <nl> * For devices with more than one control interface , we assume the
static long gpio_ioctl ( struct file * filp , unsigned int cmd , unsigned long arg ) <nl> if ( cmd == GPIO_GET_CHIPINFO_IOCTL ) { <nl> struct gpiochip_info chipinfo ; <nl>  <nl> + memset (& chipinfo , 0 , sizeof ( chipinfo )); <nl> + <nl> strncpy ( chipinfo . name , dev_name (& gdev -> dev ), <nl> sizeof ( chipinfo . name )); <nl> chipinfo . name [ sizeof ( chipinfo . name )- 1 ] = '\ 0 ';
static void mmci_post_request ( struct mmc_host * mmc , struct mmc_request * mrq , <nl> chan = host -> dma_tx_channel ; <nl> dmaengine_terminate_all ( chan ); <nl>  <nl> + if ( host -> dma_desc_current == next -> dma_desc ) <nl> + host -> dma_desc_current = NULL ; <nl> + <nl> + if ( host -> dma_current == next -> dma_chan ) <nl> + host -> dma_current = NULL ; <nl> + <nl> next -> dma_desc = NULL ; <nl> next -> dma_chan = NULL ; <nl> + data -> host_cookie = 0 ; <nl> } <nl> } <nl> 
static int __init amd64_edac_init ( void ) <nl> mcis = kzalloc ( amd_nb_num () * sizeof ( mcis [ 0 ]), GFP_KERNEL ); <nl> ecc_stngs = kzalloc ( amd_nb_num () * sizeof ( ecc_stngs [ 0 ]), GFP_KERNEL ); <nl> if (!( mcis && ecc_stngs )) <nl> - goto err_ret ; <nl> + goto err_free ; <nl>  <nl> msrs = msrs_alloc (); <nl> if (! msrs )
static int pcs_parse_one_pinctrl_entry ( struct pcs_device * pcs , <nl> (* map )-> data . mux . function = np -> name ; <nl>  <nl> if ( pcs -> is_pinconf ) { <nl> - if ( pcs_parse_pinconf ( pcs , np , function , map )) <nl> + res = pcs_parse_pinconf ( pcs , np , function , map ); <nl> + if ( res ) <nl> goto free_pingroups ; <nl> * num_maps = 2 ; <nl> } else {
static int fsl_pcie_check_link ( struct pci_controller * hose ) <nl> if ( hose -> indirect_type & PPC_INDIRECT_TYPE_FSL_CFG_REG_LINK ) { <nl> if ( hose -> ops -> read == fsl_indirect_read_config ) { <nl> struct pci_bus bus ; <nl> - bus . number = 0 ; <nl> + bus . number = hose -> first_busno ; <nl> bus . sysdata = hose ; <nl> bus . ops = hose -> ops ; <nl> indirect_read_config (& bus , 0 , PCIE_LTSSM , 4 , & val );
err_dvb_unregister_frontend : <nl>  <nl> err_dvb_frontend_detach : <nl> for ( i = MAX_NO_OF_FE_PER_ADAP - 1 ; i >= 0 ; i --) { <nl> - if ( adap -> fe [ i ]) <nl> + if ( adap -> fe [ i ]) { <nl> dvb_frontend_detach ( adap -> fe [ i ]); <nl> + adap -> fe [ i ] = NULL ; <nl> + } <nl> } <nl>  <nl> err :
static struct sst_acpi_mach sst_acpi_chv [] = { <nl> & chv_platform_data }, <nl> {" 10EC3276 ", " bytcr_rt5640 ", " intel / fw_sst_22a8 . bin ", " bytcr_rt5640 ", NULL , <nl> & chv_platform_data }, <nl> - <nl> + /* some CHT - T platforms rely on RT5651 , use Baytrail machine driver */ <nl> + {" 10EC5651 ", " bytcr_rt5651 ", " intel / fw_sst_22a8 . bin ", " bytcr_rt5651 ", NULL , <nl> + & chv_platform_data }, <nl> {}, <nl> }; <nl> 
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> */ <nl> if ( skb_shared ( skb )) { <nl> tmp_skb = skb ; <nl> - skb = skb_copy ( skb , GFP_ATOMIC ); <nl> + skb = skb_clone ( skb , GFP_ATOMIC ); <nl> kfree_skb ( tmp_skb ); <nl>  <nl> if (! skb ) {
static int bcma_get_next_core ( struct bcma_bus * bus , u32 __iomem ** eromptr , <nl> switch ( core -> id . id ) { <nl> case BCMA_CORE_4706_MAC_GBIT_COMMON : <nl> case BCMA_CORE_NS_CHIPCOMMON_B : <nl> + case BCMA_CORE_PMU : <nl> + case BCMA_CORE_GCI : <nl> /* Not used yet : case BCMA_CORE_OOB_ROUTER : */ <nl> break ; <nl> default :
static int dwc3_cleanup_done_reqs ( struct dwc3 * dwc , struct dwc3_ep * dep , <nl> for_each_sg ( sg , s , pending , i ) { <nl> trb = & dep -> trb_pool [ dep -> trb_dequeue ]; <nl>  <nl> + if ( trb -> ctrl & DWC3_TRB_CTRL_HWO ) <nl> + break ; <nl> + <nl> req -> sg = sg_next ( s ); <nl> req -> num_pending_sgs --; <nl> 
static long kvm_vm_ioctl ( struct file * filp , <nl> routing . nr * sizeof (* entries ))) <nl> goto out_free_irq_routing ; <nl> } <nl> + /* avoid races with KVM_CREATE_IRQCHIP on x86 */ <nl> + mutex_lock (& kvm -> lock ); <nl> r = kvm_set_irq_routing ( kvm , entries , routing . nr , <nl> routing . flags ); <nl> + mutex_unlock (& kvm -> lock ); <nl> out_free_irq_routing : <nl> vfree ( entries ); <nl> break ;
static struct omap_board_mux board_mux [] __initdata = { <nl> OMAP4_MUX ( DPM_EMU18 , OMAP_PIN_OUTPUT | OMAP_MUX_MODE5 ), <nl> /* dispc2_data0 */ <nl> OMAP4_MUX ( DPM_EMU19 , OMAP_PIN_OUTPUT | OMAP_MUX_MODE5 ), <nl> + /* NIRQ2 for twl6040 */ <nl> + OMAP4_MUX ( SYS_NIRQ2 , OMAP_MUX_MODE0 | <nl> + OMAP_PIN_INPUT_PULLUP | OMAP_PIN_OFF_WAKEUPENABLE ), <nl> { . reg_offset = OMAP_MUX_TERMINATOR }, <nl> }; <nl> 
static int lov_add_target ( struct obd_device * obd , struct obd_uuid * uuidp , <nl> struct lov_tgt_desc ** newtgts , ** old = NULL ; <nl> __u32 newsize , oldsize = 0 ; <nl>  <nl> - newsize = max ( lov -> lov_tgt_size , ( __u32 ) 2 ); <nl> + newsize = max_t ( __u32 , lov -> lov_tgt_size , 2 ); <nl> while ( newsize < index + 1 ) <nl> newsize = newsize << 1 ; <nl> OBD_ALLOC ( newtgts , sizeof (* newtgts ) * newsize );
static irqreturn_t s3c64xx_dma_irq ( int irq , void * pw ) <nl>  <nl> s3c64xx_dma_bufffdone ( chan , buff , res ); <nl>  <nl> + /* Free the node and update curr , if non - circular queue */ <nl> + if (!( chan -> flags & S3C2410_DMAF_CIRCULAR )) { <nl> + chan -> curr = buff -> next ; <nl> + s3c64xx_dma_freebuff ( buff ); <nl> + } <nl> + <nl> /* Update ' next ' */ <nl> buff = chan -> next ; <nl> if ( chan -> next == chan -> end ) {
# define OMAP4430_PRM_BASE 0x4a306000 <nl> # define OMAP44XX_GPMC_BASE 0x50000000 <nl> # define OMAP443X_SCM_BASE 0x4a002000 <nl> -# define OMAP443X_CTRL_BASE OMAP443X_SCM_BASE <nl> +# define OMAP443X_CTRL_BASE 0x4a100000 <nl> # define OMAP44XX_IC_BASE 0x48200000 <nl> # define OMAP44XX_IVA_INTC_BASE 0x40000000 <nl> # define IRQ_SIR_IRQ 0x0040
void ieee80211_scan_work ( struct work_struct * work ) <nl>  <nl> rc = __ieee80211_start_scan ( sdata , req ); <nl> if ( rc ) { <nl> + /* need to complete scan in cfg80211 */ <nl> + local -> scan_req = req ; <nl> aborted = true ; <nl> goto out_complete ; <nl> } else
EXPORT_SYMBOL ( get_trigger_mask ); <nl> EXPORT_SYMBOL ( global_trigger_mask ); <nl> # endif <nl>  <nl> + EXPORT_SYMBOL ( clear_page ); <nl> + EXPORT_SYMBOL ( copy_page ); <nl> EXPORT_SYMBOL ( empty_zero_page ); <nl>  <nl> EXPORT_SYMBOL ( pfn_base );
static struct usb_request * isp1760_ep_alloc_request ( struct usb_ep * ep , <nl> struct isp1760_request * req ; <nl>  <nl> req = kzalloc ( sizeof (* req ), gfp_flags ); <nl> + if (! req ) <nl> + return NULL ; <nl>  <nl> return & req -> req ; <nl> }
static int ceph_con_in_msg_alloc ( struct ceph_connection * con , int * skip ) <nl> msg = con -> ops -> alloc_msg ( con , hdr , skip ); <nl> mutex_lock (& con -> mutex ); <nl> if ( con -> state != CON_STATE_OPEN ) { <nl> - ceph_msg_put ( msg ); <nl> + if ( msg ) <nl> + ceph_msg_put ( msg ); <nl> return - EAGAIN ; <nl> } <nl> con -> in_msg = msg ;
static int dummy_udc_probe ( struct platform_device * pdev ) <nl> int rc ; <nl>  <nl> dum = *(( void **) dev_get_platdata (& pdev -> dev )); <nl> + /* Clear usb_gadget region for new registration to udc - core */ <nl> + memzero_explicit (& dum -> gadget , sizeof ( struct usb_gadget )); <nl> dum -> gadget . name = gadget_name ; <nl> dum -> gadget . ops = & dummy_ops ; <nl> dum -> gadget . max_speed = USB_SPEED_SUPER ;
struct ti_ohci { <nl>  <nl> static inline int cross_bound ( unsigned long addr , unsigned int size ) <nl> { <nl> + if ( size == 0 ) <nl> + return 0 ; <nl> + <nl> if ( size > PAGE_SIZE ) <nl> return 1 ; <nl> 
static void hw_init ( void ) <nl> break ; <nl> } <nl>  <nl> + /* magic required on VX900 for correct modesetting on IGA1 */ <nl> + via_write_reg_mask ( VIACR , 0x45 , 0x00 , 0x01 ); <nl> + <nl> /* probably this should go to the scaling code one day */ <nl> via_write_reg_mask ( VIACR , 0xFD , 0 , 0x80 ); /* VX900 hw scale on IGA2 */ <nl> viafb_write_regx ( scaling_parameters , ARRAY_SIZE ( scaling_parameters ));
static struct resource * res_pci_find_mem ( u_long base , u_long num , <nl>  <nl> static int res_pci_init ( struct pcmcia_socket * s ) <nl> { <nl> - if (! s -> cb_dev || (! s -> features & SS_CAP_PAGE_REGS )) { <nl> + if (! s -> cb_dev || !( s -> features & SS_CAP_PAGE_REGS )) { <nl> dev_err (& s -> dev , " not supported by res_pci \ n "); <nl> return - EOPNOTSUPP ; <nl> }
static int piix_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl>  <nl> static void piix_host_stop ( struct ata_host_set * host_set ) <nl> { <nl> + struct piix_host_priv * hpriv = host_set -> private_data ; <nl> + <nl> ata_host_stop ( host_set ); <nl> + <nl> + kfree ( hpriv ); <nl> } <nl>  <nl> static int __init piix_init ( void )
static unsigned long super_cache_scan ( struct shrinker * shrink , <nl> inodes = list_lru_count_node (& sb -> s_inode_lru , sc -> nid ); <nl> dentries = list_lru_count_node (& sb -> s_dentry_lru , sc -> nid ); <nl> total_objects = dentries + inodes + fs_objects + 1 ; <nl> + if (! total_objects ) <nl> + total_objects = 1 ; <nl>  <nl> /* proportion the scan between the caches */ <nl> dentries = mult_frac ( sc -> nr_to_scan , dentries , total_objects );
static struct shash_alg alg = { <nl> . export = md5_export , <nl> . import = md5_import , <nl> . descsize = sizeof ( struct md5_state ), <nl> + . statesize = sizeof ( struct md5_state ), <nl> . base = { <nl> . cra_name = " md5 ", <nl> . cra_flags = CRYPTO_ALG_TYPE_SHASH ,
static int __devinit cpm_i2c_setup ( struct cpm_i2c * cpm ) <nl> init_waitqueue_head (& cpm -> i2c_wait ); <nl>  <nl> cpm -> irq = of_irq_to_resource ( ofdev -> node , 0 , NULL ); <nl> - if ( cpm -> irq == NO_IRQ ) <nl> + if (! cpm -> irq ) <nl> return - EINVAL ; <nl>  <nl> /* Install interrupt handler . */
static int __init ion_dummy_init ( void ) <nl> int i , err ; <nl>  <nl> idev = ion_device_create ( NULL ); <nl> - heaps = kzalloc ( sizeof ( struct ion_heap *) * dummy_ion_pdata . nr , <nl> + heaps = kcalloc ( dummy_ion_pdata . nr , sizeof ( struct ion_heap *), <nl> GFP_KERNEL ); <nl> if (! heaps ) <nl> return - ENOMEM ;
static inline pte_t pfn_pte ( unsigned long pfn , pgprot_t prot ) <nl> sz_bits = 0UL ; <nl> if ( _PAGE_SZBITS_4U != 0UL || _PAGE_SZBITS_4V != 0UL ) { <nl> __asm__ __volatile__ ( <nl> - "\ n661 : sethi % uhi (% 1 ), % 0 \ n " <nl> + "\ n661 : sethi %% uhi (% 1 ), % 0 \ n " <nl> " sllx % 0 , 32 , % 0 \ n " <nl> " . section . sun4v_2insn_patch , \" ax \"\ n " <nl> " . word 661b \ n "
static ssize_t xenbus_file_read ( struct file * filp , <nl> int ret ; <nl>  <nl> mutex_lock (& u -> reply_mutex ); <nl> + again : <nl> while ( list_empty (& u -> read_buffers )) { <nl> mutex_unlock (& u -> reply_mutex ); <nl> if ( filp -> f_flags & O_NONBLOCK ) <nl> static ssize_t xenbus_file_read ( struct file * filp , <nl> struct read_buffer , list ); <nl> } <nl> } <nl> + if ( i == 0 ) <nl> + goto again ; <nl>  <nl> out : <nl> mutex_unlock (& u -> reply_mutex );
static int pppoe_rcv_core ( struct sock * sk , struct sk_buff * skb ) <nl> * can ' t change . <nl> */ <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto abort_kfree ; <nl> + <nl> if ( sk -> sk_state & PPPOX_BOUND ) { <nl> ppp_input (& po -> chan , skb ); <nl> } else if ( sk -> sk_state & PPPOX_RELAY ) {
xfer_exit : <nl>  <nl> static void dma_pl330_rqcb ( struct dma_pl330_desc * desc , enum pl330_op_err err ) <nl> { <nl> - struct dma_pl330_chan * pch = desc -> pchan ; <nl> + struct dma_pl330_chan * pch ; <nl> unsigned long flags ; <nl>  <nl> + if (! desc ) <nl> + return ; <nl> + <nl> + pch = desc -> pchan ; <nl> + <nl> /* If desc aborted */ <nl> if (! pch ) <nl> return ;
void flush_thread ( void ) <nl>  <nl> void start_thread ( struct pt_regs * regs , unsigned long eip , unsigned long esp ) <nl> { <nl> + get_safe_registers ( regs -> regs . gp , regs -> regs . fp ); <nl> PT_REGS_IP ( regs ) = eip ; <nl> PT_REGS_SP ( regs ) = esp ; <nl> current -> ptrace &= ~ PT_DTRACE ;
static int rs_get_tbl_info_from_mcs ( const u32 rate_n_flags , <nl> u8 num_of_ant = get_num_of_ant_from_rate ( rate_n_flags ); <nl> u8 mcs ; <nl>  <nl> - memset ( tbl , 0 , sizeof ( struct iwl_scale_tbl_info )); <nl> + memset ( tbl , 0 , offsetof ( struct iwl_scale_tbl_info , win )); <nl> * rate_idx = iwl_hwrate_to_plcp_idx ( rate_n_flags ); <nl>  <nl> if (* rate_idx == IWL_RATE_INVALID ) {
void falcon_remove_nic ( struct efx_nic * efx ) <nl> struct falcon_nic_data * nic_data = efx -> nic_data ; <nl> int rc ; <nl>  <nl> + /* Remove I2C adapter and clear it in preparation for a retry */ <nl> rc = i2c_del_adapter (& efx -> i2c_adap ); <nl> BUG_ON ( rc ); <nl> + memset (& efx -> i2c_adap , 0 , sizeof ( efx -> i2c_adap )); <nl>  <nl> falcon_remove_spi_devices ( efx ); <nl> falcon_free_buffer ( efx , & efx -> irq_status );
static int __devinit snd_hdspm_create_hwdep ( struct snd_card * card , <nl>  <nl> hw -> ops . open = snd_hdspm_hwdep_dummy_op ; <nl> hw -> ops . ioctl = snd_hdspm_hwdep_ioctl ; <nl> + hw -> ops . ioctl_compat = snd_hdspm_hwdep_ioctl ; <nl> hw -> ops . release = snd_hdspm_hwdep_dummy_op ; <nl>  <nl> return 0 ;
vpif_enum_dv_timings ( struct file * file , void * priv , <nl> int ret ; <nl>  <nl> ret = v4l2_subdev_call ( ch -> sd , video , enum_dv_timings , timings ); <nl> - if ( ret == - ENOIOCTLCMD && ret == - ENODEV ) <nl> + if ( ret == - ENOIOCTLCMD || ret == - ENODEV ) <nl> return - EINVAL ; <nl> return ret ; <nl> }
mxm_sor_map ( struct nvkm_bios * bios , u8 conn ) <nl> u16 map = nvbios_rd16 ( bios , mxm + 4 ); <nl> if ( map ) { <nl> ver = nvbios_rd08 ( bios , map ); <nl> - if ( ver == 0x10 ) { <nl> + if ( ver == 0x10 || ver == 0x11 ) { <nl> if ( conn < nvbios_rd08 ( bios , map + 3 )) { <nl> map += nvbios_rd08 ( bios , map + 1 ); <nl> map += conn ;
long do_shmat ( int shmid , char __user * shmaddr , int shmflg , ulong * raddr , <nl> down_write (& current -> mm -> mmap_sem ); <nl> if ( addr && !( shmflg & SHM_REMAP )) { <nl> err = - EINVAL ; <nl> + if ( addr + size < addr ) <nl> + goto invalid ; <nl> + <nl> if ( find_vma_intersection ( current -> mm , addr , addr + size )) <nl> goto invalid ; <nl> /*
void kvm_lapic_reset ( struct kvm_vcpu * vcpu , bool init_event ) <nl> apic_set_reg ( apic , APIC_DFR , 0xffffffffU ); <nl> apic_set_spiv ( apic , 0xff ); <nl> apic_set_reg ( apic , APIC_TASKPRI , 0 ); <nl> - kvm_apic_set_ldr ( apic , 0 ); <nl> + if (! apic_x2apic_mode ( apic )) <nl> + kvm_apic_set_ldr ( apic , 0 ); <nl> apic_set_reg ( apic , APIC_ESR , 0 ); <nl> apic_set_reg ( apic , APIC_ICR , 0 ); <nl> apic_set_reg ( apic , APIC_ICR2 , 0 );
static int da9055_rtc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> alm_irq = platform_get_irq_byname ( pdev , " ALM "); <nl> - alm_irq = regmap_irq_get_virq ( rtc -> da9055 -> irq_data , alm_irq ); <nl> + if ( alm_irq < 0 ) <nl> + return alm_irq ; <nl> + <nl> ret = devm_request_threaded_irq (& pdev -> dev , alm_irq , NULL , <nl> da9055_rtc_alm_irq , <nl> IRQF_TRIGGER_HIGH | IRQF_ONESHOT ,
static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
retry_snap : <nl> goto retry_snap ; <nl> } <nl> } else { <nl> + loff_t old_size = inode -> i_size ; <nl> /* <nl> * No need to acquire the i_truncate_mutex . Because <nl> * the MDS revokes Fwb caps before sending truncate <nl> retry_snap : <nl> written = generic_file_buffered_write ( iocb , iov , nr_segs , <nl> pos , & iocb -> ki_pos , <nl> count , 0 ); <nl> + if ( inode -> i_size > old_size ) <nl> + ceph_fscache_update_objectsize ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> } <nl> 
static int dgrp_net_release ( struct inode * inode , struct file * file ) <nl>  <nl> spin_unlock_irqrestore (& dgrp_poll_data . poll_lock , lock_flags ); <nl>  <nl> - done : <nl> down (& nd -> nd_net_semaphore ); <nl>  <nl> dgrp_monitor_message ( nd , " Net Close "); <nl>  <nl> up (& nd -> nd_net_semaphore ); <nl>  <nl> + done : <nl> module_put ( THIS_MODULE ); <nl> file -> private_data = NULL ; <nl> return 0 ;
static int cpuhp_invoke_ap_callback ( int cpu , enum cpuhp_state state , <nl> if (! cpu_online ( cpu )) <nl> return 0 ; <nl>  <nl> + /* <nl> + * If we are up and running , use the hotplug thread . For early calls <nl> + * we invoke the thread function directly . <nl> + */ <nl> + if (! st -> thread ) <nl> + return cpuhp_invoke_callback ( cpu , state , cb ); <nl> + <nl> st -> cb_state = state ; <nl> st -> cb = cb ; <nl> /*
static void reparent_leader ( struct task_struct * father , struct task_struct * p , <nl> { <nl> list_move_tail (& p -> sibling , & p -> real_parent -> children ); <nl>  <nl> - if ( task_detached ( p )) <nl> + if ( p -> exit_state == EXIT_DEAD ) <nl> return ; <nl> /* <nl> * If this is a threaded reparent there is no need to
static int usb_serial_probe ( struct usb_interface * interface , <nl> num_ports = type -> num_ports ; <nl> } <nl>  <nl> + if ( num_ports > MAX_NUM_PORTS ) { <nl> + dev_warn ( ddev , " too many ports requested : % d \ n ", num_ports ); <nl> + num_ports = MAX_NUM_PORTS ; <nl> + } <nl> + <nl> serial -> num_ports = num_ports ; <nl> serial -> num_bulk_in = num_bulk_in ; <nl> serial -> num_bulk_out = num_bulk_out ;
lookup_pi_state ( u32 uval , struct futex_hash_bucket * hb , <nl> if (! p ) <nl> return - ESRCH ; <nl>  <nl> + if (! p -> mm ) { <nl> + put_task_struct ( p ); <nl> + return - EPERM ; <nl> + } <nl> + <nl> /* <nl> * We need to look at the task state flags to figure out , <nl> * whether the task is exiting . To protect against the do_exit
int usb_disable_lpm ( struct usb_device * udev ) <nl> return 0 ; <nl>  <nl> udev -> lpm_disable_count ++; <nl> - if (( udev -> u1_params . timeout == 0 && udev -> u1_params . timeout == 0 )) <nl> + if (( udev -> u1_params . timeout == 0 && udev -> u2_params . timeout == 0 )) <nl> return 0 ; <nl>  <nl> /* If LPM is enabled , attempt to disable it . */
struct fib_table * fib_trie_unmerge ( struct fib_table * oldtb ) <nl> local_l = fib_find_node ( lt , & local_tp , l -> key ); <nl>  <nl> if ( fib_insert_alias ( lt , local_tp , local_l , new_fa , <nl> - NULL , l -> key )) <nl> + NULL , l -> key )) { <nl> + kmem_cache_free ( fn_alias_kmem , new_fa ); <nl> goto out ; <nl> + } <nl> } <nl>  <nl> /* stop loop if key wrapped back to 0 */
static int tilcdc_irq_postinstall ( struct drm_device * dev ) <nl> struct tilcdc_drm_private * priv = dev -> dev_private ; <nl>  <nl> /* enable FIFO underflow irq : */ <nl> - if ( priv -> rev == 1 ) { <nl> + if ( priv -> rev == 1 ) <nl> tilcdc_set ( dev , LCDC_RASTER_CTRL_REG , LCDC_V1_UNDERFLOW_INT_ENA ); <nl> - } else { <nl> + else <nl> tilcdc_set ( dev , LCDC_INT_ENABLE_SET_REG , LCDC_V2_UNDERFLOW_INT_ENA ); <nl> - } <nl>  <nl> return 0 ; <nl> }
static void __cpuinit init_amd ( struct cpuinfo_x86 * c ) <nl> level = cpuid_eax ( 1 ); <nl> if ( c -> x86 == 15 && (( level >= 0x0f48 && level < 0x0f50 ) || level >= 0x0f58 )) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl> - if ( c -> x86 == 0x10 ) <nl> + if ( c -> x86 == 0x10 || c -> x86 == 0x11 ) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl>  <nl> /* Enable workaround for FXSAVE leak */
have_busfreq : <nl> static int powernow_k6_cpu_exit ( struct cpufreq_policy * policy ) <nl> { <nl> unsigned int i ; <nl> - for ( i = 0 ; i < 8 ; i ++) { <nl> - if ( i == max_multiplier ) <nl> + <nl> + for ( i = 0 ; ( clock_ratio [ i ]. frequency != CPUFREQ_TABLE_END ); i ++) { <nl> + if ( clock_ratio [ i ]. driver_data == max_multiplier ) <nl> powernow_k6_target ( policy , i ); <nl> } <nl> return 0 ;
perf_callchain_kernel ( struct perf_callchain_entry_ctx * entry , struct pt_regs * re <nl> return ; <nl> } <nl>  <nl> - perf_callchain_store ( entry , regs -> ip ); <nl> + if ( perf_callchain_store ( entry , regs -> ip )) <nl> + return ; <nl>  <nl> dump_trace ( NULL , regs , NULL , 0 , & backtrace_ops , entry ); <nl> }
static struct priority_group * parse_priority_group ( struct arg_set * as , <nl>  <nl> if ( as -> argc < nr_params ) { <nl> ti -> error = " not enough path parameters "; <nl> + r = - EINVAL ; <nl> goto bad ; <nl> } <nl> 
static int snd_line6_pcm_free ( struct snd_device * device ) <nl> */ <nl> static void pcm_disconnect_substream ( struct snd_pcm_substream * substream ) <nl> { <nl> - if ( substream -> runtime && snd_pcm_running ( substream )) <nl> + if ( substream -> runtime && snd_pcm_running ( substream )) { <nl> + snd_pcm_stream_lock_irq ( substream ); <nl> snd_pcm_stop ( substream , SNDRV_PCM_STATE_DISCONNECTED ); <nl> + snd_pcm_stream_unlock_irq ( substream ); <nl> + } <nl> } <nl>  <nl> /*
static void fsl_ssi_config ( struct fsl_ssi_private * ssi_private , bool enable , <nl> * ( online configuration ) <nl> */ <nl> if ( enable ) { <nl> - regmap_update_bits ( regs , CCSR_SSI_SIER , vals -> sier , vals -> sier ); <nl> regmap_update_bits ( regs , CCSR_SSI_SRCR , vals -> srcr , vals -> srcr ); <nl> regmap_update_bits ( regs , CCSR_SSI_STCR , vals -> stcr , vals -> stcr ); <nl> + regmap_update_bits ( regs , CCSR_SSI_SIER , vals -> sier , vals -> sier ); <nl> } else { <nl> u32 sier ; <nl> u32 srcr ;
static int set_agc_rf ( struct drxk_state * state , <nl> } <nl>  <nl> /* Set TOP , only if IF - AGC is in AUTO mode */ <nl> - if ( p_if_agc_settings -> ctrl_mode == DRXK_AGC_CTRL_AUTO ) <nl> + if ( p_if_agc_settings -> ctrl_mode == DRXK_AGC_CTRL_AUTO ) { <nl> status = write16 ( state , <nl> SCU_RAM_AGC_IF_IACCU_HI_TGT_MAX__A , <nl> p_agc_cfg -> top ); <nl> if ( status < 0 ) <nl> goto error ; <nl> + } <nl>  <nl> /* Cut - Off current */ <nl> status = write16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A ,
int xhci_urb_enqueue ( struct usb_hcd * hcd , struct urb * urb , gfp_t mem_flags ) <nl> if (! xhci -> devs || ! xhci -> devs [ slot_id ]) { <nl> if (! in_interrupt ()) <nl> dev_warn (& urb -> dev -> dev , " WARN : urb submitted for dev with no Slot ID \ n "); <nl> - return - EINVAL ; <nl> + ret = - EINVAL ; <nl> + goto exit ; <nl> } <nl> if (! test_bit ( HCD_FLAG_HW_ACCESSIBLE , & hcd -> flags )) { <nl> if (! in_interrupt ())
static int qat_hal_init_esram ( struct icp_qat_fw_loader_handle * handle ) <nl> unsigned int csr_val ; <nl> int times = 30 ; <nl>  <nl> + if ( handle -> pci_dev -> device == ADF_C3XXX_PCI_DEVICE_ID ) <nl> + return 0 ; <nl> + <nl> csr_val = ADF_CSR_RD ( csr_addr , 0 ); <nl> if (( csr_val & ESRAM_AUTO_TINIT ) && ( csr_val & ESRAM_AUTO_TINIT_DONE )) <nl> return 0 ;
static struct spi_board_info mc13783_dev __initdata = { <nl> . bus_num = 1 , <nl> . chip_select = 0 , <nl> . platform_data = & mc13783_pdata , <nl> + . irq = IOMUX_TO_IRQ ( MX31_PIN_GPIO1_3 ), <nl> }; <nl>  <nl> static int mx31lilly_baseboard ;
static struct phy * tegra_xusb_padctl_xlate ( struct device * dev , <nl> if ( args -> args_count <= 0 ) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> - if ( index > ARRAY_SIZE ( padctl -> phys )) <nl> + if ( index >= ARRAY_SIZE ( padctl -> phys )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> return padctl -> phys [ index ];
static int __devinit w83627ehf_probe ( struct platform_device * pdev ) <nl> mutex_init (& data -> lock ); <nl> mutex_init (& data -> update_lock ); <nl> data -> name = w83627ehf_device_names [ sio_data -> kind ]; <nl> + data -> bank = 0xff ; /* Force initial bank selection */ <nl> platform_set_drvdata ( pdev , data ); <nl>  <nl> /* 627EHG and 627EHF have 10 voltage inputs ; 627DHG and 667HG have 9 */
int btrfs_commit_transaction ( struct btrfs_trans_handle * trans , <nl> spin_unlock (& root -> fs_info -> trans_lock ); <nl> } <nl>  <nl> - if ( now < cur_trans -> start_time || now - cur_trans -> start_time < 1 ) <nl> + if (! btrfs_test_opt ( root , SSD ) && <nl> + ( now < cur_trans -> start_time || now - cur_trans -> start_time < 1 )) <nl> should_grow = 1 ; <nl>  <nl> do {
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
static dma_addr_t intel_map_page ( struct device * dev , struct page * page , <nl> struct dma_attrs * attrs ) <nl> { <nl> return __intel_map_single ( dev , page_to_phys ( page ) + offset , size , <nl> - dir , to_pci_dev ( dev )-> dma_mask ); <nl> + dir , * dev -> dma_mask ); <nl> } <nl>  <nl> static void flush_unmaps ( void )
static void ironlake_irq_uninstall ( struct drm_device * dev ) <nl> I915_WRITE ( GTIMR , 0xffffffff ); <nl> I915_WRITE ( GTIER , 0x0 ); <nl> I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> + <nl> + I915_WRITE ( SDEIMR , 0xffffffff ); <nl> + I915_WRITE ( SDEIER , 0x0 ); <nl> + I915_WRITE ( SDEIIR , I915_READ ( SDEIIR )); <nl> } <nl>  <nl> static void i915_driver_irq_uninstall ( struct drm_device * dev )
static struct pci_driver adv_pci1710_pci_driver = { <nl> module_comedi_pci_driver ( adv_pci1710_driver , adv_pci1710_pci_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi : Advantech PCI - 1710 Series Multifunction DAS Cards "); <nl> MODULE_LICENSE (" GPL ");
i915_gem_execbuffer ( struct drm_device * dev , void * data , <nl> ( int ) args -> buffers_ptr , args -> buffer_count , args -> batch_len ); <nl> # endif <nl>  <nl> + if ( args -> buffer_count < 1 ) { <nl> + DRM_ERROR (" execbuf with % d buffers \ n ", args -> buffer_count ); <nl> + return - EINVAL ; <nl> + } <nl> /* Copy in the exec list from userland */ <nl> exec_list = drm_calloc ( sizeof (* exec_list ), args -> buffer_count , <nl> DRM_MEM_DRIVER );
int scrub_enumerate_chunks ( struct scrub_ctx * sctx , <nl> btrfs_put_block_group ( cache ); <nl> if ( ret ) <nl> break ; <nl> - if ( atomic64_read (& dev_replace -> num_write_errors ) > 0 ) { <nl> + if ( is_dev_replace && <nl> + atomic64_read (& dev_replace -> num_write_errors ) > 0 ) { <nl> ret = - EIO ; <nl> break ; <nl> }
data_sock_getname ( struct socket * sock , struct sockaddr * addr , <nl> lock_sock ( sk ); <nl>  <nl> * addr_len = sizeof (* maddr ); <nl> + maddr -> family = AF_ISDN ; <nl> maddr -> dev = _pms ( sk )-> dev -> id ; <nl> maddr -> channel = _pms ( sk )-> ch . nr ; <nl> maddr -> sapi = _pms ( sk )-> ch . addr & 0xff ;
static int rv3029c2_rtc_i2c_set_alarm ( struct i2c_client * client , <nl> dev_dbg (& client -> dev , " alarm IRQ armed \ n "); <nl> } else { <nl> /* disable AIE irq */ <nl> - ret = rv3029c2_rtc_i2c_alarm_set_irq ( client , 1 ); <nl> + ret = rv3029c2_rtc_i2c_alarm_set_irq ( client , 0 ); <nl> if ( ret ) <nl> return ret ; <nl> 
static int __init init_balloon_drv ( void ) <nl> return vmbus_driver_register (& balloon_drv ); <nl> } <nl>  <nl> - static void exit_balloon_drv ( void ) <nl> -{ <nl> - <nl> - vmbus_driver_unregister (& balloon_drv ); <nl> -} <nl> - <nl> module_init ( init_balloon_drv ); <nl> - module_exit ( exit_balloon_drv ); <nl>  <nl> MODULE_DESCRIPTION (" Hyper - V Balloon "); <nl> MODULE_VERSION ( HV_DRV_VERSION );
void pci_bus_add_device ( struct pci_dev * dev ) <nl>  <nl> dev -> match_driver = true ; <nl> retval = device_attach (& dev -> dev ); <nl> - if ( retval < 0 ) { <nl> + if ( retval < 0 && retval != - EPROBE_DEFER ) { <nl> dev_warn (& dev -> dev , " device attach failed (% d )\ n ", retval ); <nl> pci_proc_detach_device ( dev ); <nl> pci_remove_sysfs_dev_files ( dev );
void ieee80211_check_fast_xmit ( struct sta_info * sta ) <nl> goto out ; <nl>  <nl> /* fast - xmit doesn ' t handle fragmentation at all */ <nl> - if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 ) <nl> + if ( local -> hw . wiphy -> frag_threshold != ( u32 )- 1 && <nl> + ! local -> ops -> set_frag_threshold ) <nl> goto out ; <nl>  <nl> rcu_read_lock ();
static irqreturn_t ads7846_irq ( int irq , void * handle ) <nl> msecs_to_jiffies ( TS_POLL_PERIOD )); <nl> } <nl>  <nl> - if ( ts -> pendown ) { <nl> + if ( ts -> pendown && ! ts -> stopped ) { <nl> struct input_dev * input = ts -> input ; <nl>  <nl> input_report_key ( input , BTN_TOUCH , 0 );
nouveau_sgdma_populate ( struct ttm_backend * be , unsigned long num_pages , <nl> return - ENOMEM ; <nl>  <nl> nvbe -> ttm_alloced = kmalloc ( sizeof ( bool ) * num_pages , GFP_KERNEL ); <nl> - if (! nvbe -> ttm_alloced ) <nl> + if (! nvbe -> ttm_alloced ) { <nl> + kfree ( nvbe -> pages ); <nl> + nvbe -> pages = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> nvbe -> nr_pages = 0 ; <nl> while ( num_pages --) {
void irq_domain_free_irqs_common ( struct irq_domain * domain , unsigned int virq , <nl> } <nl> irq_domain_free_irqs_parent ( domain , virq , nr_irqs ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( irq_domain_free_irqs_common ); <nl>  <nl> /** <nl> * irq_domain_free_irqs_top - Clear handler and handler data , clear irqdata and free parent
static int <nl> lpfc_parse_vpd ( struct lpfc_hba * phba , uint8_t * vpd , int len ) <nl> { <nl> uint8_t lenlo , lenhi ; <nl> - uint32_t Length ; <nl> + int Length ; <nl> int i , j ; <nl> int finished = 0 ; <nl> int index = 0 ;
static int gb_interface_resume ( struct device * dev ) <nl> return ret ; <nl> } <nl>  <nl> + ret = gb_timesync_schedule_synchronous ( intf ); <nl> + if ( ret ) { <nl> + dev_err ( dev , " failed to synchronize FrameTime : % d \ n ", ret ); <nl> + return ret ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static void ipu_plane_dpms ( struct ipu_plane * ipu_plane , int mode ) <nl>  <nl> ipu_idmac_put ( ipu_plane -> ipu_ch ); <nl> ipu_dmfc_put ( ipu_plane -> dmfc ); <nl> - ipu_dp_put ( ipu_plane -> dp ); <nl> + if ( ipu_plane -> dp ) <nl> + ipu_dp_put ( ipu_plane -> dp ); <nl> } <nl> } <nl> 
int mei_cl_notify_get ( struct mei_cl * cl , bool block , bool * notify_ev ) <nl>  <nl> dev = cl -> dev ; <nl>  <nl> + if (! dev -> hbm_f_ev_supported ) { <nl> + cl_dbg ( dev , cl , " notifications not supported \ n "); <nl> + return - EOPNOTSUPP ; <nl> + } <nl> + <nl> if (! mei_cl_is_connected ( cl )) <nl> return - ENODEV ; <nl> 
void clear_local_APIC ( void ) <nl> } <nl>  <nl> /* lets not touch this if we didn ' t frob it */ <nl> -# if defined ( CONFIG_X86_MCE_P4THERMAL ) || defined ( X86_MCE_INTEL ) <nl> +# if defined ( CONFIG_X86_MCE_P4THERMAL ) || defined ( CONFIG_X86_MCE_INTEL ) <nl> if ( maxlvt >= 5 ) { <nl> v = apic_read ( APIC_LVTTHMR ); <nl> apic_write ( APIC_LVTTHMR , v | APIC_LVT_MASKED );
static int r820t_signal ( struct dvb_frontend * fe , u16 * strength ) <nl>  <nl> /* A higher gain at LNA means a lower signal strength */ <nl> * strength = ( 45 - rc ) << 4 | 0xff ; <nl> + if (* strength == 0xff ) <nl> + * strength = 0 ; <nl> } else { <nl> * strength = 0 ; <nl> }
static void gfs2_write_super ( struct super_block * sb ) <nl> static int gfs2_sync_fs ( struct super_block * sb , int wait ) <nl> { <nl> sb -> s_dirt = 0 ; <nl> - if ( wait ) <nl> + if ( wait && sb -> s_fs_info ) <nl> gfs2_log_flush ( sb -> s_fs_info , NULL ); <nl> return 0 ; <nl> }
static void _rtl92s_phy_get_txpower_index ( struct ieee80211_hw * hw , u8 channel , <nl> /* Read HT 40 OFDM TX power */ <nl> ofdmpowerLevel [ 0 ] = rtlefuse -> txpwrlevel_ht40_2s [ 0 ][ index ]; <nl> ofdmpowerLevel [ 1 ] = rtlefuse -> txpwrlevel_ht40_2s [ 1 ][ index ]; <nl> + } else { <nl> + ofdmpowerLevel [ 0 ] = 0 ; <nl> + ofdmpowerLevel [ 1 ] = 0 ; <nl> } <nl> } <nl> 
static void fc_exch_rrq_resp ( struct fc_seq * sp , struct fc_frame * fp , void * arg ) <nl> if ( IS_ERR ( fp )) { <nl> int err = PTR_ERR ( fp ); <nl>  <nl> - if ( err == - FC_EX_CLOSED ) <nl> + if ( err == - FC_EX_CLOSED || err == - FC_EX_TIMEOUT ) <nl> goto cleanup ; <nl> FC_DBG (" Cannot process RRQ , because of frame error % d \ n ", err ); <nl> return ;
static int uio_pdrv_remove ( struct platform_device * pdev ) <nl>  <nl> uio_unregister_device ( pdata -> uioinfo ); <nl>  <nl> + kfree ( pdata ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int soc15_common_set_clockgating_state ( void * handle , <nl> { <nl> struct amdgpu_device * adev = ( struct amdgpu_device *) handle ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> switch ( adev -> asic_type ) { <nl> case CHIP_VEGA10 : <nl> nbio_v6_1_update_medium_grain_clock_gating ( adev ,
static int ibmvscsis_drop_nexus ( struct ibmvscsis_tport * tport ) <nl> /* <nl> * Release the SCSI I_T Nexus to the emulated ibmvscsis Target Port <nl> */ <nl> + target_wait_for_sess_cmds ( se_sess ); <nl> + transport_deregister_session_configfs ( se_sess ); <nl> transport_deregister_session ( se_sess ); <nl> tport -> ibmv_nexus = NULL ; <nl> kfree ( nexus );
static int davinci_pcm_open ( struct snd_pcm_substream * substream ) <nl> int ret = 0 ; <nl>  <nl> snd_soc_set_runtime_hwparams ( substream , & davinci_pcm_hardware ); <nl> + /* ensure that buffer size is a multiple of period size */ <nl> + ret = snd_pcm_hw_constraint_integer ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> prtd = kzalloc ( sizeof ( struct davinci_runtime_data ), GFP_KERNEL ); <nl> if ( prtd == NULL )
int ocfs2_fiemap ( struct inode * inode , struct fiemap_extent_info * fieinfo , <nl> & hole_size , & rec , & is_last ); <nl> if ( ret ) { <nl> mlog_errno ( ret ); <nl> - goto out ; <nl> + goto out_unlock ; <nl> } <nl>  <nl> if ( rec . e_blkno == 0ULL ) {
struct io_context { <nl> /* <nl> * For request batching <nl> */ <nl> - unsigned long last_waited ; /* Time last woken after wait for request */ <nl> int nr_batch_requests ; /* Number of requests left in the batch */ <nl> + unsigned long last_waited ; /* Time last woken after wait for request */ <nl>  <nl> struct radix_tree_root radix_root ; <nl> struct hlist_head cic_list ;
int udf_ioctl ( struct inode * inode , struct file * filp , unsigned int cmd , <nl> static int udf_release_file ( struct inode * inode , struct file * filp ) <nl> { <nl> if ( filp -> f_mode & FMODE_WRITE ) { <nl> + mutex_lock (& inode -> i_mutex ); <nl> lock_kernel (); <nl> udf_discard_prealloc ( inode ); <nl> unlock_kernel (); <nl> + mutex_unlock (& inode -> i_mutex ); <nl> } <nl> return 0 ; <nl> }
int netvsc_send ( struct hv_device * device , <nl> if (! net_device ) <nl> return - ENODEV ; <nl>  <nl> + /* We may race with netvsc_connect_vsp ()/ netvsc_init_buf () and get <nl> + * here before the negotiation with the host is finished and <nl> + * send_section_map may not be allocated yet . <nl> + */ <nl> + if (! net_device -> send_section_map ) <nl> + return - EAGAIN ; <nl> + <nl> out_channel = net_device -> chn_table [ q_idx ]; <nl>  <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ;
static int __init r8a66597_probe ( struct platform_device * pdev ) <nl>  <nl> r8a66597 -> ep0_req = r8a66597_alloc_request (& r8a66597 -> ep [ 0 ]. ep , <nl> GFP_KERNEL ); <nl> - if ( r8a66597 -> ep0_req == NULL ) <nl> + if ( r8a66597 -> ep0_req == NULL ) { <nl> + ret = - ENOMEM ; <nl> goto clean_up3 ; <nl> + } <nl> r8a66597 -> ep0_req -> complete = nop_completion ; <nl>  <nl> ret = usb_add_gadget_udc (& pdev -> dev , & r8a66597 -> gadget );
static int spi_qup_remove ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> ret = pm_runtime_get_sync (& pdev -> dev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> return ret ; <nl>  <nl> ret = spi_qup_set_state ( controller , QUP_STATE_RESET );
static struct pci_device_id azx_ids [] = { <nl> { PCI_DEVICE ( 0x10de , 0x044b ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x055c ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x055d ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> + { PCI_DEVICE ( 0x10de , 0x0590 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0774 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0775 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0776 ), . driver_data = AZX_DRIVER_NVIDIA },
unsigned fuse_file_poll ( struct file * file , poll_table * wait ) <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req )) <nl> - return PTR_ERR ( req ); <nl> + return POLLERR ; <nl>  <nl> req -> in . h . opcode = FUSE_POLL ; <nl> req -> in . h . nodeid = ff -> nodeid ;
static int cx22700_set_tps ( struct cx22700_state * state , <nl>  <nl> cx22700_writereg ( state , 0x04 , val ); <nl>  <nl> + if ( p -> code_rate_HP - FEC_1_2 >= sizeof ( fec_tab ) || <nl> + p -> code_rate_LP - FEC_1_2 >= sizeof ( fec_tab )) <nl> + return - EINVAL ; <nl> val = fec_tab [ p -> code_rate_HP - FEC_1_2 ] << 3 ; <nl> val |= fec_tab [ p -> code_rate_LP - FEC_1_2 ]; <nl> 
static void cfq_dispatch_insert ( struct request_queue * q , struct request * rq ) <nl>  <nl> cfq_log_cfqq ( cfqd , cfqq , " dispatch_insert "); <nl>  <nl> + cfqq -> next_rq = cfq_find_next_rq ( cfqd , cfqq , rq ); <nl> cfq_remove_request ( rq ); <nl> cfqq -> dispatched ++; <nl> elv_dispatch_sort ( q , rq );
void ceph_handle_caps ( struct ceph_mds_session * session , <nl> cap -> cap_id = le64_to_cpu ( h -> cap_id ); <nl> cap -> mseq = mseq ; <nl> cap -> seq = seq ; <nl> + cap -> issue_seq = seq ; <nl> spin_lock (& session -> s_cap_lock ); <nl> list_add_tail (& cap -> session_caps , <nl> & session -> s_cap_releases );
static int st_fdma_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> fdev -> slim_rproc = st_slim_rproc_alloc ( pdev , fdev -> fw_name ); <nl> - if (! fdev -> slim_rproc ) { <nl> + if ( IS_ERR ( fdev -> slim_rproc )) { <nl> ret = PTR_ERR ( fdev -> slim_rproc ); <nl> dev_err (& pdev -> dev , " slim_rproc_alloc failed (% d )\ n ", ret ); <nl> goto err ;
static int vpfe_enum_input ( struct file * file , void * priv , <nl> return - EINVAL ; <nl> } <nl> sdinfo = & vpfe_dev -> cfg -> sub_devs [ subdev ]; <nl> - memcpy ( inp , & sdinfo -> inputs [ index ], sizeof ( struct v4l2_input )); <nl> + * inp = sdinfo -> inputs [ index ]; <nl> return 0 ; <nl> } <nl> 
acpi_ex_load_op ( union acpi_operand_object * obj_desc , <nl> */ <nl> status = acpi_tb_add_table ( table_ptr , & table_index ); <nl> if ( ACPI_FAILURE ( status )) { <nl> - return_ACPI_STATUS ( status ); <nl> + goto cleanup ; <nl> } <nl>  <nl> status =
static int mount_ubifs ( struct ubifs_info * c ) <nl> if ( err ) <nl> goto out_orphans ; <nl> err = ubifs_rcvry_gc_commit ( c ); <nl> + if ( err ) <nl> + goto out_orphans ; <nl> } else { <nl> err = take_gc_lnum ( c ); <nl> if ( err )
static unsigned int pxa2xx_ssp_get_clk_div ( struct driver_data * drv_data , <nl> switch ( drv_data -> ssp_type ) { <nl> case QUARK_X1000_SSP : <nl> clk_div = quark_x1000_get_clk_div ( rate , & chip -> dds_rate ); <nl> + break ; <nl> default : <nl> clk_div = ssp_get_clk_div ( drv_data , rate ); <nl> + break ; <nl> } <nl> return clk_div << 8 ; <nl> }
int datagram_send_ctl ( struct net * net , struct sock * sk , <nl>  <nl> if ( addr_type != IPV6_ADDR_ANY ) { <nl> int strict = __ipv6_addr_src_scope ( addr_type ) <= IPV6_ADDR_SCOPE_LINKLOCAL ; <nl> - if (! inet_sk ( sk )-> transparent && <nl> + if (!( inet_sk ( sk )-> freebind || inet_sk ( sk )-> transparent ) && <nl> ! ipv6_chk_addr ( net , & src_info -> ipi6_addr , <nl> strict ? dev : NULL , 0 )) <nl> err = - EINVAL ;
listxattr ( struct dentry * d , char __user * list , size_t size ) <nl> error = d -> d_inode -> i_op -> listxattr ( d , klist , size ); <nl> } else { <nl> error = security_inode_listsecurity ( d -> d_inode , klist , size ); <nl> - if ( size && error >= size ) <nl> + if ( size && error > size ) <nl> error = - ERANGE ; <nl> } <nl> if ( error > 0 ) {
nfulnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl> ret = - EINVAL ; <nl> break ; <nl> } <nl> + <nl> + if (! inst ) <nl> + goto out ; <nl> } else { <nl> if (! inst ) { <nl> UDEBUG (" no config command , and no instance for " <nl> nfulnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl>  <nl> out_put : <nl> instance_put ( inst ); <nl> + out : <nl> return ret ; <nl> } <nl> 
static int ieee802154_dev_ioctl ( struct sock * sk , struct ifreq __user * arg , <nl> dev_load ( sock_net ( sk ), ifr . ifr_name ); <nl> dev = dev_get_by_name ( sock_net ( sk ), ifr . ifr_name ); <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( dev -> type == ARPHRD_IEEE802154 && dev -> netdev_ops -> ndo_do_ioctl ) <nl> ret = dev -> netdev_ops -> ndo_do_ioctl ( dev , & ifr , cmd ); <nl> 
static void dma_pte_free_level ( struct dmar_domain * domain , int level , <nl>  <nl> /* If range covers entire pagetable , free it */ <nl> if (!( start_pfn > level_pfn || <nl> - last_pfn < level_pfn + level_size ( level ))) { <nl> + last_pfn < level_pfn + level_size ( level ) - 1 )) { <nl> dma_clear_pte ( pte ); <nl> domain_flush_cache ( domain , pte , sizeof (* pte )); <nl> free_pgtable_page ( level_pte );
static int max31790_read_pwm ( struct device * dev , u32 attr , int channel , <nl> long * val ) <nl> { <nl> struct max31790_data * data = max31790_update_device ( dev ); <nl> - u8 fan_config = data -> fan_config [ channel ]; <nl> + u8 fan_config ; <nl>  <nl> if ( IS_ERR ( data )) <nl> return PTR_ERR ( data ); <nl>  <nl> + fan_config = data -> fan_config [ channel ]; <nl> + <nl> switch ( attr ) { <nl> case hwmon_pwm_input : <nl> * val = data -> pwm [ channel ] >> 8 ;
static int logger_release ( struct inode * ignored , struct file * file ) <nl> { <nl> if ( file -> f_mode & FMODE_READ ) { <nl> struct logger_reader * reader = file -> private_data ; <nl> + struct logger_log * log = reader -> log ; <nl> + <nl> + mutex_lock (& log -> mutex ); <nl> list_del (& reader -> list ); <nl> + mutex_unlock (& log -> mutex ); <nl> + <nl> kfree ( reader ); <nl> } <nl> 
struct gen_pool * devm_gen_pool_create ( struct device * dev , int min_alloc_order , <nl> struct gen_pool ** ptr , * pool ; <nl>  <nl> ptr = devres_alloc ( devm_gen_pool_release , sizeof (* ptr ), GFP_KERNEL ); <nl> + if (! ptr ) <nl> + return NULL ; <nl>  <nl> pool = gen_pool_create ( min_alloc_order , nid ); <nl> if ( pool ) {
static void skl_power_well_post_enable ( struct drm_i915_private * dev_priv , <nl> 1 << PIPE_C | 1 << PIPE_B ); <nl> } <nl>  <nl> - if ( power_well -> data == SKL_DISP_PW_1 ) <nl> + if ( power_well -> data == SKL_DISP_PW_1 ) { <nl> + intel_prepare_ddi ( dev ); <nl> gen8_irq_power_well_post_enable ( dev_priv , 1 << PIPE_A ); <nl> + } <nl> } <nl>  <nl> static void hsw_set_power_well ( struct drm_i915_private * dev_priv ,
const struct imx_imx_ssi_data imx35_imx_ssi_data [] __initconst = { <nl> # ifdef CONFIG_SOC_IMX51 <nl> const struct imx_imx_ssi_data imx51_imx_ssi_data [] __initconst = { <nl> # define imx51_imx_ssi_data_entry ( _id , _hwid ) \ <nl> - imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_4K ) <nl> + imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_16K ) <nl> imx51_imx_ssi_data_entry ( 0 , 1 ), <nl> imx51_imx_ssi_data_entry ( 1 , 2 ), <nl> imx51_imx_ssi_data_entry ( 2 , 3 ),
static void amdgpu_gem_va_update_vm ( struct amdgpu_device * adev , <nl> if ( domain == AMDGPU_GEM_DOMAIN_CPU ) <nl> goto error_unreserve ; <nl> } <nl> + r = amdgpu_vm_update_page_directory ( adev , bo_va -> vm ); <nl> + if ( r ) <nl> + goto error_unreserve ; <nl>  <nl> r = amdgpu_vm_clear_freed ( adev , bo_va -> vm ); <nl> if ( r )
int dmar_set_interrupt ( struct intel_iommu * iommu ) <nl> return 0 ; <nl>  <nl> irq = create_irq (); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> pr_err (" IOMMU : no free vectors \ n "); <nl> return - EINVAL ; <nl> }
int vmbus_open ( struct vmbus_channel * newchannel , u32 send_ringbuffer_size , <nl> ret = vmbus_post_msg ( open_msg , <nl> sizeof ( struct vmbus_channel_open_channel )); <nl>  <nl> - if ( ret != 0 ) <nl> + if ( ret != 0 ) { <nl> + err = ret ; <nl> goto error1 ; <nl> + } <nl>  <nl> t = wait_for_completion_timeout (& open_info -> waitevent , 5 * HZ ); <nl> if ( t == 0 ) {
 <nl> int intel_sanitize_enable_execlists ( struct drm_device * dev , int enable_execlists ) <nl> { <nl> + WARN_ON ( i915 . enable_ppgtt == - 1 ); <nl> + <nl> if ( enable_execlists == 0 ) <nl> return 0 ; <nl> 
static int p9_mux_poll_start ( struct p9_conn * m ) <nl> } <nl>  <nl> if ( i >= ARRAY_SIZE ( p9_mux_poll_tasks )) { <nl> - if ( vptlast == NULL ) <nl> + if ( vptlast == NULL ) { <nl> + mutex_unlock (& p9_mux_task_lock ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> P9_DPRINTK ( P9_DEBUG_MUX , " put in proc % d \ n ", i ); <nl> list_add (& m -> mux_list , & vptlast -> mux_list );
static int perf_top_config ( const char * var , const char * value , void * cb ) <nl>  <nl> if (! strcmp ( var , " top . call - graph ")) <nl> return record_parse_callchain ( value , & top -> record_opts ); <nl> + if (! strcmp ( var , " top . children ")) { <nl> + symbol_conf . cumulate_callchain = perf_config_bool ( var , value ); <nl> + return 0 ; <nl> + } <nl>  <nl> return perf_default_config ( var , value , cb ); <nl> }
static void ixgbe_free_irq ( struct ixgbe_adapter * adapter ) <nl>  <nl> i --; <nl> for (; i >= 0 ; i --) { <nl> + /* free only the irqs that were actually requested */ <nl> + if (! adapter -> q_vector [ i ]-> rxr_count && <nl> + ! adapter -> q_vector [ i ]-> txr_count ) <nl> + continue ; <nl> + <nl> free_irq ( adapter -> msix_entries [ i ]. vector , <nl> adapter -> q_vector [ i ]); <nl> }
static int xhci_plat_probe ( struct platform_device * pdev ) <nl> if ( ret ) <nl> goto dealloc_usb2_hcd ; <nl>  <nl> + device_enable_async_suspend (& pdev -> dev ); <nl> + <nl> return 0 ; <nl>  <nl> 
void blk_mq_delay_queue ( struct blk_mq_hw_ctx * hctx , unsigned long msecs ) <nl> if ( unlikely (! blk_mq_hw_queue_mapped ( hctx ))) <nl> return ; <nl>  <nl> + blk_mq_stop_hw_queue ( hctx ); <nl> kblockd_schedule_delayed_work_on ( blk_mq_hctx_next_cpu ( hctx ), <nl> & hctx -> delay_work , msecs_to_jiffies ( msecs )); <nl> }
static enum dvbfe_search cxd2820r_search ( struct dvb_frontend * fe ) <nl> /* frontend lock wait loop count */ <nl> switch ( priv -> delivery_system ) { <nl> case SYS_DVBT : <nl> + case SYS_DVBC_ANNEX_A : <nl> i = 20 ; <nl> break ; <nl> case SYS_DVBT2 :
static ssize_t port_fops_read ( struct file * filp , char __user * ubuf , <nl> if ( ret < 0 ) <nl> return ret ; <nl> } <nl> + /* Port got hot - unplugged . */ <nl> + if (! port -> guest_connected ) <nl> + return - ENODEV ; <nl> /* <nl> * We could ' ve received a disconnection message while we were <nl> * waiting for more data .
static void ieee80211_do_stop ( struct ieee80211_sub_if_data * sdata , <nl> rcu_barrier (); <nl> sta_info_flush_cleanup ( sdata ); <nl>  <nl> - skb_queue_purge (& sdata -> skb_queue ); <nl> - <nl> /* <nl> * Free all remaining keys , there shouldn ' t be any , <nl> * except maybe in WDS mode ? <nl> */ <nl> ieee80211_free_keys ( sdata ); <nl>  <nl> + /* fall through */ <nl> + case NL80211_IFTYPE_AP : <nl> + skb_queue_purge (& sdata -> skb_queue ); <nl> + <nl> drv_remove_interface_debugfs ( local , sdata ); <nl>  <nl> if ( going_down )
static void sil_host_intr ( struct ata_port * ap , u32 bmdma2 ) <nl> u8 status ; <nl>  <nl> if ( unlikely ( bmdma2 & SIL_DMA_SATA_IRQ )) { <nl> - u32 serror ; <nl> + u32 serror = 0xffffffff ; <nl>  <nl> /* SIEN doesn ' t mask SATA IRQs on some 3112s . Those <nl> * controllers continue to assert IRQ as long as
static __be32 nfsd_set_fh_dentry ( struct svc_rqst * rqstp , struct svc_fh * fhp ) <nl> * fix that case easily . <nl> */ <nl> struct cred * new = prepare_creds (); <nl> - if (! new ) <nl> - return nfserrno (- ENOMEM ); <nl> + if (! new ) { <nl> + error = nfserrno (- ENOMEM ); <nl> + goto out ; <nl> + } <nl> new -> cap_effective = <nl> cap_raise_nfsd_set ( new -> cap_effective , <nl> new -> cap_permitted );
pptp_inbound_pkt ( struct sk_buff ** pskb , <nl> DEBUGP ("% s but no session \ n ", pptp_msg_name [ msg ]); <nl> break ; <nl> } <nl> - if ( info -> sstate != PPTP_CALL_IN_REP <nl> - && info -> sstate != PPTP_CALL_IN_CONF ) { <nl> + if ( info -> cstate != PPTP_CALL_IN_REP <nl> + && info -> cstate != PPTP_CALL_IN_CONF ) { <nl> DEBUGP ("% s but never sent IN_CALL_REPLY \ n ", <nl> pptp_msg_name [ msg ]); <nl> break ;
pte_t xen_make_pte ( unsigned long long pte ) <nl> if ( pte & 1 ) <nl> pte = phys_to_machine ( XPADDR ( pte )). maddr ; <nl>  <nl> + pte &= ~ _PAGE_PCD ; <nl> + <nl> return ( pte_t ){ pte , pte >> 32 }; <nl> } <nl>  <nl> pte_t xen_make_pte ( unsigned long pte ) <nl> if ( pte & _PAGE_PRESENT ) <nl> pte = phys_to_machine ( XPADDR ( pte )). maddr ; <nl>  <nl> + pte &= ~ _PAGE_PCD ; <nl> + <nl> return ( pte_t ){ pte }; <nl> } <nl> 
static int open ( struct tty_struct * tty , struct file * filp ) <nl> if ( info -> port . count == 1 ) { <nl> /* 1st open on this device , init hardware */ <nl> retval = startup ( info ); <nl> - if ( retval < 0 ) <nl> + if ( retval < 0 ) { <nl> + mutex_unlock (& info -> port . mutex ); <nl> goto cleanup ; <nl> + } <nl> } <nl> mutex_unlock (& info -> port . mutex ); <nl> retval = block_til_ready ( tty , filp , info );
static int i2c_register_adapter ( struct i2c_adapter * adap ) <nl>  <nl> dev_dbg (& adap -> dev , " adapter [% s ] registered \ n ", adap -> name ); <nl>  <nl> + pm_runtime_no_callbacks (& adap -> dev ); <nl> + <nl> # ifdef CONFIG_I2C_COMPAT <nl> res = class_compat_create_link ( i2c_adapter_compat_class , & adap -> dev , <nl> adap -> dev . parent );
static struct sg_table * omap_gem_map_dma_buf ( <nl> /* this should be after _get_paddr () to ensure we have pages attached */ <nl> omap_gem_dma_sync ( obj , dir ); <nl>  <nl> - out : <nl> - if ( ret ) <nl> - return ERR_PTR ( ret ); <nl> return sg ; <nl> + out : <nl> + kfree ( sg ); <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> static void omap_gem_unmap_dma_buf ( struct dma_buf_attachment * attachment ,
static int bsd_compress ( void * state , struct sk_buff * skb_in , struct sk_buff * skb <nl> db -> n_bits ++; <nl>  <nl> /* If output length is too large then this is an incompressible frame . */ <nl> - if (! skb_out || ( skb_out && skb_out -> len >= skb_in -> len )) { <nl> + if (! skb_out || skb_out -> len >= skb_in -> len ) { <nl> ++ db -> incomp_count ; <nl> db -> incomp_bytes += isize ; <nl> return 0 ;
static int ioc4_serial_remove_one ( struct ioc4_driver_data * idd ) <nl> if ( soft ) { <nl> free_irq ( control -> ic_irq , soft ); <nl> if ( soft -> is_ioc4_serial_addr ) { <nl> + iounmap ( soft -> is_ioc4_serial_addr ); <nl> release_region (( unsigned long ) <nl> soft -> is_ioc4_serial_addr , <nl> sizeof ( struct ioc4_serial )); <nl> out4 : <nl> out3 : <nl> kfree ( control ); <nl> out2 : <nl> + if ( serial ) <nl> + iounmap ( serial ); <nl> release_region ( tmp_addr1 , sizeof ( struct ioc4_serial )); <nl> out1 : <nl> 
static int get_strength ( struct drxk_state * state , u64 * strength ) <nl> return status ; <nl>  <nl> /* SCU c . o . c . */ <nl> - read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> + status = read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> if ( status < 0 ) <nl> return status ; <nl> 
static int musb_probe ( struct platform_device * pdev ) <nl> struct resource * iomem ; <nl> void __iomem * base ; <nl>  <nl> - iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - if (! iomem || irq <= 0 ) <nl> + if ( irq <= 0 ) <nl> return - ENODEV ; <nl>  <nl> + iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> base = devm_ioremap_resource ( dev , iomem ); <nl> if ( IS_ERR ( base )) <nl> return PTR_ERR ( base );
static int ir_rc5_decode ( struct rc_dev * dev , struct ir_raw_event ev ) <nl> u32 scancode ; <nl> enum rc_type protocol ; <nl>  <nl> - if (!( dev -> enabled_protocols & ( RC_BIT_RC5 | RC_BIT_RC5X ))) <nl> + if (!( dev -> enabled_protocols & ( RC_BIT_RC5 | RC_BIT_RC5X | RC_BIT_RC5_SZ ))) <nl> return 0 ; <nl>  <nl> if (! is_timing_event ( ev )) {
static irqreturn_t interrupt_pcl816 ( int irq , void * d ) <nl> } <nl>  <nl> outb ( 0 , dev -> iobase + PCL816_CLRINT ); /* clear INT request */ <nl> - if (! dev -> irq || ! devpriv -> ai_cmd_running || ! devpriv -> int816_mode ) { <nl> + if (! devpriv -> ai_cmd_running || ! devpriv -> int816_mode ) { <nl> if ( devpriv -> irq_was_now_closed ) { <nl> devpriv -> irq_was_now_closed = 0 ; <nl> /* comedi_error ( dev ," last IRQ .."); */
static int cramfs_readpage ( struct file * file , struct page * page ) <nl> pgdata = kmap ( page ); <nl> if ( compr_len == 0 ) <nl> ; /* hole */ <nl> + else if ( compr_len > ( PAGE_CACHE_SIZE << 1 )) <nl> + printk ( KERN_ERR " cramfs : bad compressed blocksize % u \ n ", compr_len ); <nl> else { <nl> mutex_lock (& read_mutex ); <nl> bytes_filled = cramfs_uncompress_block ( pgdata ,
err_out_unregister : <nl>  <nl> err_unlock_policy : <nl> unlock_policy_rwsem_write ( cpu ); <nl> + free_cpumask_var ( policy -> related_cpus ); <nl> err_free_cpumask : <nl> free_cpumask_var ( policy -> cpus ); <nl> err_free_policy :
nvkm_ltc_tags_clear ( struct nvkm_ltc * ltc , u32 first , u32 count ) <nl>  <nl> BUG_ON (( first > limit ) || ( limit >= ltc -> num_tags )); <nl>  <nl> + mutex_lock (& ltc -> subdev . mutex ); <nl> ltc -> func -> cbc_clear ( ltc , first , limit ); <nl> ltc -> func -> cbc_wait ( ltc ); <nl> + mutex_unlock (& ltc -> subdev . mutex ); <nl> } <nl>  <nl> int
loff_t mem_lseek ( struct file * file , loff_t offset , int orig ) <nl> static int mem_release ( struct inode * inode , struct file * file ) <nl> { <nl> struct mm_struct * mm = file -> private_data ; <nl> - <nl> - mmput ( mm ); <nl> + if ( mm ) <nl> + mmput ( mm ); <nl> return 0 ; <nl> } <nl> 
static int pl330_dma_device_slave_caps ( struct dma_chan * dchan , <nl> caps -> directions = BIT ( DMA_DEV_TO_MEM ) | BIT ( DMA_MEM_TO_DEV ); <nl> caps -> cmd_pause = false ; <nl> caps -> cmd_terminate = true ; <nl> + caps -> residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR ; <nl>  <nl> return 0 ; <nl> }
static void vmbus_process_offer ( struct work_struct * work ) <nl> spin_lock_irqsave (& vmbus_connection . channel_lock , flags ); <nl> list_del (& newchannel -> listentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channel_lock , flags ); <nl> + kfree ( newchannel -> device_obj ); <nl>  <nl> free_channel ( newchannel ); <nl> } else {
void inet_initpeers ( void ) __init ; <nl> static inline void inetpeer_set_addr_v4 ( struct inetpeer_addr * iaddr , __be32 ip ) <nl> { <nl> iaddr -> a4 . addr = ip ; <nl> + iaddr -> a4 . vif = 0 ; <nl> iaddr -> family = AF_INET ; <nl> } <nl> 
static void bus_reset_work ( struct work_struct * work ) <nl> { <nl> struct fw_ohci * ohci = <nl> container_of ( work , struct fw_ohci , bus_reset_work ); <nl> - int self_id_count , i , j , reg ; <nl> - int generation , new_generation ; <nl> + int self_id_count , generation , new_generation , i , j ; <nl> + u32 reg ; <nl> unsigned long flags ; <nl> void * free_rom = NULL ; <nl> dma_addr_t free_rom_bus = 0 ;
rockchip_drm_framebuffer_init ( struct drm_device * dev , <nl>  <nl> rockchip_fb = rockchip_fb_alloc ( dev , mode_cmd , & obj , 1 ); <nl> if ( IS_ERR ( rockchip_fb )) <nl> - return NULL ; <nl> + return ERR_CAST ( rockchip_fb ); <nl>  <nl> return & rockchip_fb -> fb ; <nl> }
EXPORT_SYMBOL_GPL ( usbnet_resume ); <nl>  <nl> static int __init usbnet_init ( void ) <nl> { <nl> - /* compiler should optimize this out */ <nl> - BUILD_BUG_ON ( sizeof ((( struct sk_buff *) 0 )-> cb ) <nl> - < sizeof ( struct skb_data )); <nl> + /* Compiler should optimize this out . */ <nl> + BUILD_BUG_ON ( <nl> + FIELD_SIZEOF ( struct sk_buff , cb ) < sizeof ( struct skb_data )); <nl>  <nl> random_ether_addr ( node_id ); <nl> return 0 ;
static int i915_drm_freeze ( struct drm_device * dev ) <nl> * Disable CRTCs directly since we want to preserve sw state <nl> * for _thaw . <nl> */ <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> list_for_each_entry ( crtc , & dev -> mode_config . crtc_list , head ) <nl> dev_priv -> display . crtc_disable ( crtc ); <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl>  <nl> intel_modeset_suspend_hw ( dev ); <nl> }
static int __devexit i2c_hid_remove ( struct i2c_client * client ) <nl>  <nl> free_irq ( client -> irq , ihid ); <nl>  <nl> + if ( ihid -> bufsize ) <nl> + i2c_hid_free_buffers ( ihid ); <nl> + <nl> kfree ( ihid ); <nl>  <nl> return 0 ;
static ssize_t mdc_kuc_write ( struct file * file , <nl> /* for mockup below */ 2 * cfs_size_round ( sizeof (* hai )); <nl>  <nl> OBD_ALLOC ( lh , len ); <nl> + if (! lh ) <nl> + return - ENOMEM ; <nl>  <nl> lh -> kuc_magic = KUC_MAGIC ; <nl> lh -> kuc_transport = KUC_TRANSPORT_HSM ;
s32 fm10k_disable_queues_generic ( struct fm10k_hw * hw , u16 q_cnt ) <nl> /* clear tx_ready to prevent any false hits for reset */ <nl> hw -> mac . tx_ready = false ; <nl>  <nl> + if ( FM10K_REMOVED ( hw -> hw_addr )) <nl> + return 0 ; <nl> + <nl> /* clear the enable bit for all rings */ <nl> for ( i = 0 ; i < q_cnt ; i ++) { <nl> reg = fm10k_read_reg ( hw , FM10K_TXDCTL ( i ));
static struct rfkill * amilo_rfkill_dev ; <nl>  <nl> static int __devinit amilo_rfkill_probe ( struct platform_device * device ) <nl> { <nl> + int rc ; <nl> const struct dmi_system_id * system_id = <nl> dmi_first_match ( amilo_rfkill_id_table ); <nl> - int rc ; <nl> + <nl> + if (! system_id ) <nl> + return - ENXIO ; <nl>  <nl> amilo_rfkill_dev = rfkill_alloc ( KBUILD_MODNAME , & device -> dev , <nl> RFKILL_TYPE_WLAN ,
struct mmc_host * mmc_alloc_host ( int extra , struct device * dev ) <nl>  <nl> if ( mmc_gpio_alloc ( host )) { <nl> put_device (& host -> class_dev ); <nl> + ida_simple_remove (& mmc_host_ida , host -> index ); <nl> + kfree ( host ); <nl> return NULL ; <nl> } <nl> 
static struct pci_driver cciss_pci_driver = { <nl> */ <nl> static int __init cciss_init ( void ) <nl> { <nl> + /* <nl> + * The hardware requires that commands are aligned on a 64 - bit <nl> + * boundary . Given that we use pci_alloc_consistent () to allocate an <nl> + * array of them , the size must be a multiple of 8 bytes . <nl> + */ <nl> + BUILD_BUG_ON ( sizeof ( CommandList_struct ) % 8 ); <nl> + <nl> printk ( KERN_INFO DRIVER_NAME "\ n "); <nl>  <nl> /* Register for our PCI devices */
xfs_uuid_mount ( <nl> uuid_t * uuid = & mp -> m_sb . sb_uuid ; <nl> int hole , i ; <nl>  <nl> + /* Publish UUID in struct super_block */ <nl> + BUILD_BUG_ON ( sizeof ( mp -> m_super -> s_uuid ) != sizeof ( uuid_t )); <nl> + memcpy (& mp -> m_super -> s_uuid , uuid , sizeof ( uuid_t )); <nl> + <nl> if ( mp -> m_flags & XFS_MOUNT_NOUUID ) <nl> return 0 ; <nl> 
void kvm_arch_vcpu_load ( struct kvm_vcpu * vcpu , int cpu ) <nl> if ( check_tsc_unstable ()) { <nl> kvm_x86_ops -> adjust_tsc_offset ( vcpu , - tsc_delta ); <nl> vcpu -> arch . tsc_catchup = 1 ; <nl> - kvm_make_request ( KVM_REQ_CLOCK_UPDATE , vcpu ); <nl> } <nl> + kvm_make_request ( KVM_REQ_CLOCK_UPDATE , vcpu ); <nl> if ( vcpu -> cpu != cpu ) <nl> kvm_migrate_timers ( vcpu ); <nl> vcpu -> cpu = cpu ;
static int find_variable ( Dwarf_Die * sc_die , struct probe_finder * pf ) <nl> if (! die_find_variable_at (& pf -> cu_die , pf -> pvar -> var , 0 , & vr_die )) <nl> ret = - ENOENT ; <nl> } <nl> - if ( ret == 0 ) <nl> + if ( ret >= 0 ) <nl> ret = convert_variable (& vr_die , pf ); <nl>  <nl> if ( ret < 0 )
static struct scsi_device * scsi_alloc_sdev ( struct scsi_target * starget , <nl> /* release fn is set up in scsi_sysfs_device_initialise , so <nl> * have to free and put manually here */ <nl> put_device (& starget -> dev ); <nl> + kfree ( sdev ); <nl> goto out ; <nl> } <nl> 
EXPORT_SYMBOL ( clk_add_alias ); <nl> */ <nl> void clkdev_drop ( struct clk_lookup * cl ) <nl> { <nl> + struct clk_lookup_alloc * cla = container_of ( cl , struct clk_lookup_alloc , cl ); <nl> + <nl> mutex_lock (& clocks_mutex ); <nl> list_del (& cl -> node ); <nl> mutex_unlock (& clocks_mutex ); <nl> - kfree ( cl ); <nl> + kfree ( cla ); <nl> } <nl> EXPORT_SYMBOL ( clkdev_drop );
static int marvell_pre_reset ( struct ata_port * ap ) <nl> switch ( ap -> port_no ) <nl> { <nl> case 0 : <nl> - /* Might be backward , docs unclear */ <nl> if ( inb ( ap -> ioaddr . bmdma_addr + 1 ) & 1 ) <nl> - ap -> cbl = ATA_CBL_PATA80 ; <nl> - else <nl> ap -> cbl = ATA_CBL_PATA40 ; <nl> + else <nl> + ap -> cbl = ATA_CBL_PATA80 ; <nl> + break ; <nl>  <nl> case 1 : /* Legacy SATA port */ <nl> ap -> cbl = ATA_CBL_SATA ;
static int sha384_neon_final ( struct shash_desc * desc , u8 * hash ) <nl> sha512_neon_final ( desc , D ); <nl>  <nl> memcpy ( hash , D , SHA384_DIGEST_SIZE ); <nl> - memset ( D , 0 , SHA512_DIGEST_SIZE ); <nl> + memzero_explicit ( D , SHA512_DIGEST_SIZE ); <nl>  <nl> return 0 ; <nl> }
static void r8a66597_check_detect_child ( struct r8a66597 * r8a66597 , <nl>  <nl> memset ( now_map , 0 , sizeof ( now_map )); <nl>  <nl> + mutex_lock (& usb_bus_idr_lock ); <nl> bus = idr_find (& usb_bus_idr , hcd -> self . busnum ); <nl> if ( bus && bus -> root_hub ) { <nl> collect_usb_address_map ( bus -> root_hub , now_map ); <nl> update_usb_address_map ( r8a66597 , bus -> root_hub , now_map ); <nl> } <nl> + mutex_unlock (& usb_bus_idr_lock ); <nl> } <nl>  <nl> static int r8a66597_hub_status_data ( struct usb_hcd * hcd , char * buf )
static int usbhsg_ep_disable ( struct usb_ep * ep ) <nl> struct usbhsg_uep * uep = usbhsg_ep_to_uep ( ep ); <nl> struct usbhs_pipe * pipe = usbhsg_uep_to_pipe ( uep ); <nl>  <nl> + if (! pipe ) <nl> + return - EINVAL ; <nl> + <nl> usbhsg_pipe_disable ( uep ); <nl> usbhs_pipe_free ( pipe ); <nl> 
static void acm_softint ( struct work_struct * work ) <nl> static void acm_waker ( struct work_struct * waker ) <nl> { <nl> struct acm * acm = container_of ( waker , struct acm , waker ); <nl> - long flags ; <nl> + unsigned long flags ; <nl> int rv ; <nl>  <nl> rv = usb_autopm_get_interface ( acm -> control );
struct device_node * v4l2_of_get_next_endpoint ( const struct device_node * parent , <nl> if (! endpoint ) <nl> pr_err ("% s (): no endpoint nodes specified for % s \ n ", <nl> __func__ , parent -> full_name ); <nl> + of_node_put ( node ); <nl> } else { <nl> port = of_get_parent ( prev ); <nl> if (! port )
static int __devinit tmiofb_probe ( struct platform_device * dev ) <nl> dev_err (& dev -> dev , " NULL platform data !\ n "); <nl> return - EINVAL ; <nl> } <nl> + if ( ccr == NULL || lcr == NULL || vram == NULL || irq < 0 ) { <nl> + dev_err (& dev -> dev , " missing resources \ n "); <nl> + return - EINVAL ; <nl> + } <nl>  <nl> info = framebuffer_alloc ( sizeof ( struct tmiofb_par ), & dev -> dev ); <nl> 
static int malidp_bind ( struct device * dev ) <nl> if ( ret < 0 ) <nl> goto irq_init_fail ; <nl>  <nl> + drm -> irq_enabled = true ; <nl> + <nl> ret = drm_vblank_init ( drm , drm -> mode_config . num_crtc ); <nl> if ( ret < 0 ) { <nl> DRM_ERROR (" failed to initialise vblank \ n "); <nl> fbdev_fail : <nl> vblank_fail : <nl> malidp_se_irq_fini ( drm ); <nl> malidp_de_irq_fini ( drm ); <nl> + drm -> irq_enabled = false ; <nl> irq_init_fail : <nl> component_unbind_all ( dev , drm ); <nl> bind_fail :
dma_alloc_coherent ( struct device * dev , size_t size , dma_addr_t * dma_handle , <nl> goto again ; <nl> } <nl>  <nl> + /* Let low level make its own zone decisions */ <nl> + gfp &= ~( GFP_DMA32 | GFP_DMA ); <nl> + <nl> if ( dma_ops -> alloc_coherent ) <nl> return dma_ops -> alloc_coherent ( dev , size , <nl> dma_handle , gfp );
new_slab : <nl> stat ( s , ALLOC_SLAB ); <nl> c -> node = page_to_nid ( page ); <nl> c -> page = page ; <nl> + <nl> + if ( kmem_cache_debug ( s )) <nl> + goto debug ; <nl> goto load_freelist ; <nl> } <nl> if (!( gfpflags & __GFP_NOWARN ) && printk_ratelimit ())
static int ecryptfs_open ( struct inode * inode , struct file * file ) <nl> file , ecryptfs_inode_to_private ( inode )-> lower_file ); <nl> if ( S_ISDIR ( ecryptfs_dentry -> d_inode -> i_mode )) { <nl> ecryptfs_printk ( KERN_DEBUG , " This is a directory \ n "); <nl> + mutex_lock (& crypt_stat -> cs_mutex ); <nl> crypt_stat -> flags &= ~( ECRYPTFS_ENCRYPTED ); <nl> + mutex_unlock (& crypt_stat -> cs_mutex ); <nl> rc = 0 ; <nl> goto out ; <nl> }
int i965_reset ( struct drm_device * dev , u8 flags ) <nl> } <nl> } else { <nl> DRM_ERROR (" Error occurred . Don ' t know how to reset this chip .\ n "); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return - ENODEV ; <nl> } <nl> 
static int __cmd_record ( struct perf_record * rec , int argc , const char ** argv ) <nl> return err ; <nl> } <nl>  <nl> - if (!! rec -> no_buildid <nl> + if (! rec -> no_buildid <nl> && ! perf_header__has_feat (& session -> header , HEADER_BUILD_ID )) { <nl> - pr_err (" Couldn ' t generating buildids . " <nl> + pr_err (" Couldn ' t generate buildids . " <nl> " Use -- no - buildid to profile anyway .\ n "); <nl> return - 1 ; <nl> }
static int at76_iw_handler_get_scan ( struct net_device * netdev , <nl> if (! iwe ) <nl> return - ENOMEM ; <nl>  <nl> - if ( priv -> scan_state != SCAN_COMPLETED ) <nl> + if ( priv -> scan_state != SCAN_COMPLETED ) { <nl> /* scan not yet finished */ <nl> + kfree ( iwe ); <nl> return - EAGAIN ; <nl> + } <nl>  <nl> spin_lock_irqsave (& priv -> bss_list_spinlock , flags ); <nl> 
int btrfs_commit_transaction ( struct btrfs_trans_handle * trans , <nl> spin_unlock (& root -> fs_info -> trans_lock ); <nl>  <nl> wait_for_commit ( root , prev_trans ); <nl> + ret = prev_trans -> aborted ; <nl>  <nl> btrfs_put_transaction ( prev_trans ); <nl> + if ( ret ) <nl> + goto cleanup_transaction ; <nl> } else { <nl> spin_unlock (& root -> fs_info -> trans_lock ); <nl> }
static int hdmi_display_check_timing ( struct omap_dss_device * dssdev , <nl> { <nl> struct omap_dss_device * out = & hdmi . output ; <nl>  <nl> + /* TODO : proper interlace support */ <nl> + if ( timings -> interlace ) <nl> + return - EINVAL ; <nl> + <nl> if (! dispc_mgr_timings_ok ( out -> dispc_channel , timings )) <nl> return - EINVAL ; <nl> 
static int sanitize_enable_ppgtt ( struct drm_device * dev , int enable_ppgtt ) <nl> } <nl> # endif <nl>  <nl> + /* Early VLV doesn ' t have this */ <nl> + if ( IS_VALLEYVIEW ( dev ) && dev -> pdev -> revision < 0xb ) { <nl> + DRM_DEBUG_DRIVER (" disabling PPGTT on pre - B3 step VLV \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> return HAS_ALIASING_PPGTT ( dev ) ? 1 : 0 ; <nl> } <nl> 
static inline pte_t * pte_alloc_one_kernel ( struct mm_struct * mm , <nl> static inline pgtable_t pte_alloc_one ( struct mm_struct * mm , <nl> unsigned long addr ) <nl> { <nl> + pte_t * pte ; <nl> struct page * page ; <nl>  <nl> - page = virt_to_page ( pte_alloc_one_kernel ( mm , addr )); <nl> + pte = pte_alloc_one_kernel ( mm , addr ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + page = virt_to_page ( pte ); <nl> pgtable_page_ctor ( page ); <nl> return page ; <nl> }
static int pcmcia_device_query ( struct pcmcia_device * p_dev ) <nl> tmp = vers1 -> str + vers1 -> ofs [ i ]; <nl>  <nl> length = strlen ( tmp ) + 1 ; <nl> - if (( length < 3 ) || ( length > 255 )) <nl> + if (( length < 2 ) || ( length > 255 )) <nl> continue ; <nl>  <nl> p_dev -> prod_id [ i ] = kmalloc ( sizeof ( char ) * length ,
static const struct usb_device_id usb_quirk_list [] = { <nl> /* Creative SB Audigy 2 NX */ <nl> { USB_DEVICE ( 0x041e , 0x3020 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl>  <nl> + /* Microsoft LifeCam - VX700 v2 . 0 */ <nl> + { USB_DEVICE ( 0x045e , 0x0770 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl> + <nl> /* Logitech Quickcam Fusion */ <nl> { USB_DEVICE ( 0x046d , 0x08c1 ), . driver_info = USB_QUIRK_RESET_RESUME }, <nl> 
static unsigned long si5351_clkout_recalc_rate ( struct clk_hw * hw , <nl> unsigned char reg ; <nl> unsigned char rdiv ; <nl>  <nl> - if ( hwdata -> num > 5 ) <nl> + if ( hwdata -> num <= 5 ) <nl> reg = si5351_msynth_params_address ( hwdata -> num ) + 2 ; <nl> else <nl> reg = SI5351_CLK6_7_OUTPUT_DIVIDER ;
static void sdma_add_scripts ( struct sdma_engine * sdma , <nl> s32 * saddr_arr = ( u32 *) sdma -> script_addrs ; <nl> int i ; <nl>  <nl> + /* use the default firmware in ROM if missing external firmware */ <nl> + if (! sdma -> script_number ) <nl> + sdma -> script_number = SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1 ; <nl> + <nl> for ( i = 0 ; i < sdma -> script_number ; i ++) <nl> if ( addr_arr [ i ] > 0 ) <nl> saddr_arr [ i ] = addr_arr [ i ];
retry : <nl>  <nl> blkcg = bio_blkcg ( bio ); <nl> cfqg = cfq_lookup_create_cfqg ( cfqd , blkcg ); <nl> + if (! cfqg ) { <nl> + cfqq = & cfqd -> oom_cfqq ; <nl> + goto out ; <nl> + } <nl> + <nl> cfqq = cic_to_cfqq ( cic , is_sync ); <nl>  <nl> /* <nl> retry : <nl> } else <nl> cfqq = & cfqd -> oom_cfqq ; <nl> } <nl> - <nl> + out : <nl> if ( new_cfqq ) <nl> kmem_cache_free ( cfq_pool , new_cfqq ); <nl> 
EXPORT_SYMBOL ( smp_num_siblings ); <nl>  <nl> /* Last level cache ID of each logical CPU */ <nl> u8 cpu_llc_id [ NR_CPUS ] __cpuinitdata = {[ 0 ... NR_CPUS - 1 ] = BAD_APICID }; <nl> - EXPORT_SYMBOL ( cpu_llc_id ); <nl>  <nl> /* Bitmask of currently online CPUs */ <nl> cpumask_t cpu_online_map __read_mostly ;
static bool have_cpu_die ( void ) <nl> # ifdef CONFIG_HOTPLUG_CPU <nl> int any_cpu = raw_smp_processor_id (); <nl>  <nl> - if ( cpu_ops [ any_cpu ]-> cpu_die ) <nl> + if ( cpu_ops [ any_cpu ] && cpu_ops [ any_cpu ]-> cpu_die ) <nl> return true ; <nl> # endif <nl> return false ;
static int hdmi_remove ( struct platform_device * pdev ) <nl>  <nl> pm_runtime_disable ( dev ); <nl>  <nl> - free_irq ( hdata -> irq , hdata ); <nl> + free_irq ( hdata -> irq , ctx ); <nl>  <nl>  <nl> /* hdmiphy i2c driver */
__setup (" vga =", sm712vga_setup ); <nl> * Original init function changed to probe method to be used by pci_drv <nl> * process used to detect chips replaced with kernel process in pci_drv <nl> */ <nl> - static int __init smtcfb_pci_probe ( struct pci_dev * pdev , <nl> + static int __devinit smtcfb_pci_probe ( struct pci_dev * pdev , <nl> const struct pci_device_id * ent ) <nl> { <nl> struct smtcfb_info * sfb ;
static int eseqiv_givencrypt ( struct skcipher_givcrypt_request * req ) <nl> if ( err ) <nl> goto out ; <nl>  <nl> - eseqiv_complete2 ( req ); <nl> + if ( giv != req -> giv ) <nl> + eseqiv_complete2 ( req ); <nl>  <nl> out : <nl> return err ;
int intel_setup_gmbus ( struct drm_device * dev ) <nl> bus -> reg0 = i | GMBUS_RATE_100KHZ ; <nl>  <nl> /* XXX force bit banging until GMBUS is fully debugged */ <nl> - bus -> force_bit = intel_gpio_create ( dev_priv , i ); <nl> + if ( IS_GEN2 ( dev )) <nl> + bus -> force_bit = intel_gpio_create ( dev_priv , i ); <nl> } <nl>  <nl> intel_i2c_reset ( dev_priv -> dev );
intel_user_framebuffer_create ( struct drm_device * dev , <nl>  <nl> ret = intel_framebuffer_create ( dev , mode_cmd , & fb , obj ); <nl> if ( ret ) { <nl> + mutex_lock (& dev -> struct_mutex ); <nl> drm_gem_object_unreference ( obj ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return NULL ; <nl> } <nl> 
int ceph_atomic_open ( struct inode * dir , struct dentry * dentry , <nl> } <nl> err = finish_open ( file , dentry , ceph_open , opened ); <nl> } <nl> - <nl> out_err : <nl> + if (! req -> r_err && req -> r_target_inode ) <nl> + ceph_put_fmode ( ceph_inode ( req -> r_target_inode ), req -> r_fmode ); <nl> ceph_mdsc_put_request ( req ); <nl> dout (" atomic_open result =% d \ n ", err ); <nl> return err ;
int twl4030_madc_conversion ( struct twl4030_madc_request * req ) <nl> u8 ch_msb , ch_lsb ; <nl> int ret ; <nl>  <nl> - if (! req ) <nl> + if (! req || ! twl4030_madc ) <nl> return - EINVAL ; <nl> + <nl> mutex_lock (& twl4030_madc -> lock ); <nl> if ( req -> method < TWL4030_MADC_RT || req -> method > TWL4030_MADC_SW2 ) { <nl> ret = - EINVAL ;
static void * ixgbe_fwd_add ( struct net_device * pdev , struct net_device * vdev ) <nl> ( adapter -> num_rx_pools > IXGBE_MAX_MACVLANS )) <nl> return ERR_PTR (- EBUSY ); <nl>  <nl> - fwd_adapter = kcalloc ( 1 , sizeof ( struct ixgbe_fwd_adapter ), GFP_KERNEL ); <nl> + fwd_adapter = kzalloc ( sizeof (* fwd_adapter ), GFP_KERNEL ); <nl> if (! fwd_adapter ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
static int lstats_open ( struct inode * inode , struct file * file ) <nl> struct seq_file * m ; <nl> struct task_struct * task = get_proc_task ( inode ); <nl>  <nl> + if (! task ) <nl> + return - ENOENT ; <nl> ret = single_open ( file , lstats_show_proc , NULL ); <nl> if (! ret ) { <nl> m = file -> private_data ;
static int skge_down ( struct net_device * dev ) <nl> struct skge_hw * hw = skge -> hw ; <nl> int port = skge -> port ; <nl>  <nl> - if ( skge -> mem == NULL ) <nl> + if (! skge -> mem ) <nl> return 0 ; <nl>  <nl> netif_info ( skge , ifdown , skge -> netdev , " disabling interface \ n ");
static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
static long tce_iommu_register_pages ( struct tce_container * container , <nl> return ret ; <nl>  <nl> tcemem = kzalloc ( sizeof (* tcemem ), GFP_KERNEL ); <nl> + if (! tcemem ) { <nl> + mm_iommu_put ( container -> mm , mem ); <nl> + return - ENOMEM ; <nl> + } <nl> + <nl> tcemem -> mem = mem ; <nl> list_add (& tcemem -> next , & container -> prereg_list ); <nl> 
int __devinit snd_emu10k1_mixer ( struct snd_emu10k1 * emu , <nl> if ( emu -> ac97 -> id == AC97_ID_STAC9758 ) { <nl> emu -> rear_ac97 = 1 ; <nl> snd_emu10k1_ptr_write ( emu , AC97SLOT , 0 , AC97SLOT_CNTR | AC97SLOT_LFE | AC97SLOT_REAR_LEFT | AC97SLOT_REAR_RIGHT ); <nl> + snd_ac97_write_cache ( emu -> ac97 , AC97_HEADPHONE , 0x0202 ); <nl> } <nl> /* remove unused AC97 controls */ <nl> snd_ac97_write_cache ( emu -> ac97 , AC97_SURROUND_MASTER , 0x0202 );
static int ds1374_probe ( struct i2c_adapter * adap , int addr , int kind ) <nl> client -> driver = & ds1374_driver ; <nl>  <nl> ds1374_workqueue = create_singlethread_workqueue (" ds1374 "); <nl> + if (! ds1374_workqueue ) { <nl> + kfree ( client ); <nl> + return - ENOMEM ; /* most expected reason */ <nl> + } <nl>  <nl> if (( rc = i2c_attach_client ( client )) != 0 ) { <nl> kfree ( client );
int btrfs_ordered_update_i_size ( struct inode * inode , u64 offset , <nl>  <nl> if ( ordered ) <nl> offset = entry_end ( ordered ); <nl> + else <nl> + offset = ALIGN ( offset , BTRFS_I ( inode )-> root -> sectorsize ); <nl>  <nl> mutex_lock (& tree -> mutex ); <nl> disk_i_size = BTRFS_I ( inode )-> disk_i_size ;
static int noinline dirty_and_release_pages ( struct btrfs_trans_handle * trans , <nl> */ <nl> inline_size = end_pos ; <nl> if ( isize >= BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> - inline_size > 32768 || <nl> + inline_size > 8192 || <nl> inline_size >= BTRFS_MAX_INLINE_DATA_SIZE ( root )) { <nl> u64 last_end ; <nl> u64 existing_delalloc = 0 ;
static int fbcon_blank ( struct vc_data * vc , int blank , int mode_switch ) <nl> update_screen ( vc ); <nl> } <nl>  <nl> - if ( fbcon_is_inactive ( vc , info ) || <nl> + if ( mode_switch || fbcon_is_inactive ( vc , info ) || <nl> ops -> blank_state != FB_BLANK_UNBLANK ) <nl> fbcon_del_cursor_timer ( info ); <nl> else
static inline uint8_t elf_sym__type ( const GElf_Sym * sym ) <nl>  <nl> static inline int elf_sym__is_function ( const GElf_Sym * sym ) <nl> { <nl> - return elf_sym__type ( sym ) == STT_FUNC && <nl> + return ( elf_sym__type ( sym ) == STT_FUNC || <nl> + elf_sym__type ( sym ) == STT_GNU_IFUNC ) && <nl> sym -> st_name != 0 && <nl> sym -> st_shndx != SHN_UNDEF ; <nl> }
static int rr_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> return 0 ; <nl>  <nl> out : <nl> + if ( rrpriv -> evt_ring ) <nl> + pci_free_consistent ( pdev , EVT_RING_SIZE , rrpriv -> evt_ring , <nl> + rrpriv -> evt_ring_dma ); <nl> if ( rrpriv -> rx_ring ) <nl> pci_free_consistent ( pdev , RX_TOTAL_SIZE , rrpriv -> rx_ring , <nl> rrpriv -> rx_ring_dma );
static int gb_connection_hd_cport_quiesce ( struct gb_connection * connection ) <nl> if ( connection -> mode_switch ) <nl> peer_space += sizeof ( struct gb_operation_msg_hdr ); <nl>  <nl> + if (! hd -> driver -> cport_quiesce ) <nl> + return 0 ; <nl> + <nl> ret = hd -> driver -> cport_quiesce ( hd , connection -> hd_cport_id , <nl> peer_space , <nl> GB_CONNECTION_CPORT_QUIESCE_TIMEOUT );
static int davinci_gpio_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& chips [ i ]. lock ); <nl>  <nl> regs = gpio2regs ( base ); <nl> + if (! regs ) <nl> + return - ENXIO ; <nl> chips [ i ]. regs = regs ; <nl> chips [ i ]. set_data = & regs -> set_data ; <nl> chips [ i ]. clr_data = & regs -> clr_data ;
static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh , <nl>  <nl> if (! buffer_dirty ( bh )) { <nl> /* bdflush has written it . We can drop it now */ <nl> + __jbd2_journal_remove_checkpoint ( jh ); <nl> goto zap_buffer ; <nl> } <nl>  <nl> static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh , <nl> /* The orphan record ' s transaction has <nl> * committed . We can cleanse this buffer */ <nl> clear_buffer_jbddirty ( bh ); <nl> + __jbd2_journal_remove_checkpoint ( jh ); <nl> goto zap_buffer ; <nl> } <nl> }
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
static int btree_gc_coalesce ( struct btree * b , struct btree_op * op , <nl> BUG_ON ( btree_bset_first ( new_nodes [ 0 ])-> keys ); <nl> btree_node_free ( new_nodes [ 0 ]); <nl> rw_unlock ( true , new_nodes [ 0 ]); <nl> + new_nodes [ 0 ] = NULL ; <nl>  <nl> for ( i = 0 ; i < nodes ; i ++) { <nl> if ( __bch_keylist_realloc (& keylist , bkey_u64s (& r [ i ]. b -> key )))
static int __devinit snd_card_ad1816a_pnp ( int dev , struct snd_card_ad1816a * acar <nl> struct pnp_resource_table * cfg = kmalloc ( sizeof (* cfg ), GFP_KERNEL ); <nl> int err ; <nl>  <nl> + if (! cfg ) <nl> + return - ENOMEM ; <nl> acard -> dev = pnp_request_card_device ( card , id -> devs [ 0 ]. id , NULL ); <nl> if ( acard -> dev == NULL ) { <nl> kfree ( cfg );
static int w1_attach_slave_device ( struct w1_master * dev , struct w1_reg_num * rn ) <nl> atomic_set (& sl -> refcnt , 0 ); <nl> init_completion (& sl -> released ); <nl>  <nl> + /* slave modules need to be loaded in a context with unlocked mutex */ <nl> + mutex_unlock (& dev -> mutex ); <nl> request_module (" w1 - family - 0x % 0x ", rn -> family ); <nl> + mutex_lock (& dev -> mutex ); <nl>  <nl> spin_lock (& w1_flock ); <nl> f = w1_family_registered ( rn -> family );
static struct platform_driver axp288_fuel_gauge_driver = { <nl>  <nl> module_platform_driver ( axp288_fuel_gauge_driver ); <nl>  <nl> + MODULE_AUTHOR (" Ramakrishna Pallala < ramakrishna . pallala @ intel . com >"); <nl> MODULE_AUTHOR (" Todd Brandt < todd . e . brandt @ linux . intel . com >"); <nl> MODULE_DESCRIPTION (" Xpower AXP288 Fuel Gauge Driver "); <nl> MODULE_LICENSE (" GPL ");
static int __init aha152x_init ( void ) <nl>  <nl> static void __exit aha152x_exit ( void ) <nl> { <nl> - struct aha152x_hostdata * hd ; <nl> + struct aha152x_hostdata * hd , * tmp ; <nl>  <nl> - list_for_each_entry ( hd , & aha152x_host_list , host_list ) { <nl> + list_for_each_entry_safe ( hd , tmp , & aha152x_host_list , host_list ) { <nl> struct Scsi_Host * shost = container_of (( void *) hd , struct Scsi_Host , hostdata ); <nl>  <nl> aha152x_release ( shost );
static struct ip_conntrack_expect * find_expect ( struct ip_conntrack * ct , <nl> tuple . dst . protonum = IPPROTO_TCP ; <nl>  <nl> exp = __ip_conntrack_expect_find (& tuple ); <nl> - if ( exp -> master == ct ) <nl> + if ( exp && exp -> master == ct ) <nl> return exp ; <nl> return NULL ; <nl> }
out : <nl> static int si476x_codec_probe ( struct snd_soc_codec * codec ) <nl> { <nl> codec -> control_data = dev_get_regmap ( codec -> dev -> parent , NULL ); <nl> - return 0 ; <nl> + return snd_soc_codec_set_cache_io ( codec , 0 , 0 , SND_SOC_REGMAP ); <nl> } <nl>  <nl> static struct snd_soc_dai_ops si476x_dai_ops = {
static int __init sdma_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + ret = dma_coerce_mask_and_coherent (& pdev -> dev , DMA_BIT_MASK ( 32 )); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> sdma = kzalloc ( sizeof (* sdma ), GFP_KERNEL ); <nl> if (! sdma ) <nl> return - ENOMEM ;
static int ipmmu_domain_init_context ( struct ipmmu_vmsa_domain * domain ) <nl> domain -> cfg . ias = 32 ; <nl> domain -> cfg . oas = 40 ; <nl> domain -> cfg . tlb = & ipmmu_gather_ops ; <nl> + domain -> io_domain . geometry . aperture_end = DMA_BIT_MASK ( 32 ); <nl> + domain -> io_domain . geometry . force_aperture = true ; <nl> /* <nl> * TODO : Add support for coherent walk through CCI with DVM and remove <nl> * cache handling . For now , delegate it to the io - pgtable code .
static void pci_dio_detach ( struct comedi_device * dev ) <nl> if ( devpriv ) { <nl> if ( devpriv -> valid ) <nl> pci_dio_reset ( dev ); <nl> + } <nl> + if ( dev -> subdevices ) { <nl> for ( i = 0 ; i < dev -> n_subdevices ; i ++) { <nl> s = dev -> subdevices + i ; <nl> if ( s -> type == COMEDI_SUBD_DIO )
static struct pernet_operations __net_initdata proc_net_ns_ops = { <nl>  <nl> int __init proc_net_init ( void ) <nl> { <nl> - proc_symlink (" net ", NULL , " self / net "); <nl> + proc_symlink (" net ", NULL , " thread - self / net "); <nl>  <nl> return register_pernet_subsys (& proc_net_ns_ops ); <nl> }
lpfc_send_els_event ( struct lpfc_vport * vport , <nl> sizeof ( struct lpfc_name )); <nl> break ; <nl> default : <nl> + kfree ( els_data ); <nl> return ; <nl> } <nl> memcpy ( els_data -> wwpn , & ndlp -> nlp_portname , sizeof ( struct lpfc_name ));
static struct page * page_chain_del ( struct page ** head , int n ) <nl> BUG_ON (! head ); <nl>  <nl> page = * head ; <nl> + <nl> + if (! page ) <nl> + return NULL ; <nl> + <nl> while ( page ) { <nl> tmp = page_chain_next ( page ); <nl> if (-- n == 0 )
static struct usb_driver go7007_usb_driver = { <nl> }; <nl>  <nl> module_usb_driver ( go7007_usb_driver ); <nl> + MODULE_LICENSE (" GPL v2 ");
static void k8_map_sysaddr_to_csrow ( struct mem_ctl_info * mci , <nl> * different from the node that detected the error . <nl> */ <nl> src_mci = find_mc_by_sys_addr ( mci , SystemAddress ); <nl> - if ( src_mci ) { <nl> + if (! src_mci ) { <nl> amd64_mc_printk ( mci , KERN_ERR , <nl> " failed to map error address 0x % lx to a node \ n ", <nl> ( unsigned long ) SystemAddress );
void __devinit pci_read_bridge_bases ( struct pci_bus * child ) <nl> struct resource * res ; <nl> int i ; <nl>  <nl> - if (! child -> parent ) /* It ' s a host bus , nothing to read */ <nl> + if ( pci_is_root_bus ( child )) /* It ' s a host bus , nothing to read */ <nl> return ; <nl>  <nl> if ( dev -> transparent ) {
static void gpiodevice_release ( struct device * dev ) <nl> cdev_del (& gdev -> chrdev ); <nl> list_del (& gdev -> list ); <nl> ida_simple_remove (& gpio_ida , gdev -> id ); <nl> + kfree ( gdev ); <nl> } <nl>  <nl> /**
void __init create_boot_cache ( struct kmem_cache * s , const char * name , size_t siz <nl> err = __kmem_cache_create ( s , flags ); <nl>  <nl> if ( err ) <nl> - panic (" Creation of kmalloc slab % s size =% zd failed . Reason % d \ n ", <nl> + panic (" Creation of kmalloc slab % s size =% zu failed . Reason % d \ n ", <nl> name , size , err ); <nl>  <nl> s -> refcount = - 1 ; /* Exempt from merging for now */
static ssize_t dgap_driver_pollrate_show ( struct device_driver * ddp , char * buf ) <nl>  <nl> static ssize_t dgap_driver_pollrate_store ( struct device_driver * ddp , const char * buf , size_t count ) <nl> { <nl> - sscanf ( buf , "% d \ n ", & dgap_poll_tick ); <nl> + if ( sscanf ( buf , "% d \ n ", & dgap_poll_tick ) != 1 ) <nl> + return - EINVAL ; <nl> return count ; <nl> } <nl> static DRIVER_ATTR ( pollrate , ( S_IRUSR | S_IWUSR ), dgap_driver_pollrate_show , dgap_driver_pollrate_store );
struct sctp_cookie { <nl> __u32 adaptation_ind ; <nl>  <nl> __u8 auth_random [ sizeof ( sctp_paramhdr_t ) + SCTP_AUTH_RANDOM_LENGTH ]; <nl> - __u8 auth_hmacs [ SCTP_AUTH_NUM_HMACS + 2 ]; <nl> + __u8 auth_hmacs [ SCTP_AUTH_NUM_HMACS * sizeof ( __u16 ) + 2 ]; <nl> __u8 auth_chunks [ sizeof ( sctp_paramhdr_t ) + SCTP_AUTH_MAX_CHUNKS ]; <nl>  <nl> /* This is a shim for my peer ' s INIT packet , followed by
static int macronix_quad_enable ( struct spi_nor * nor ) <nl> val = read_sr ( nor ); <nl> if ( val < 0 ) <nl> return val ; <nl> + if ( val & SR_QUAD_EN_MX ) <nl> + return 0 ; <nl> + <nl> write_enable ( nor ); <nl>  <nl> write_sr ( nor , val | SR_QUAD_EN_MX );
scsi_reset_provider ( struct scsi_device * dev , int flag ) <nl> } <nl>  <nl> scmd = scsi_get_command ( dev , GFP_KERNEL ); <nl> + if (! scmd ) { <nl> + rtn = FAILED ; <nl> + put_device (& dev -> sdev_gendev ); <nl> + goto out_put_autopm_host ; <nl> + } <nl> + <nl> blk_rq_init ( NULL , & req ); <nl> scmd -> request = & req ; <nl> 
static int __devexit max77693_muic_remove ( struct platform_device * pdev ) <nl> free_irq ( muic_irqs [ i ]. virq , info ); <nl> cancel_work_sync (& info -> irq_work ); <nl> extcon_dev_unregister ( info -> edev ); <nl> + kfree ( info -> edev ); <nl> kfree ( info ); <nl>  <nl> return 0 ;
static void hdmi5_core_audio_config ( struct hdmi_core_data * core , <nl>  <nl> /* Source number */ <nl> val = cfg -> iec60958_cfg -> status [ 2 ] & IEC958_AES2_CON_SOURCE ; <nl> - REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 2 ), val , 3 , 4 ); <nl> + REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 2 ), val , 3 , 0 ); <nl>  <nl> /* Channel number right 0 */ <nl> REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 3 ), 2 , 3 , 0 );
int cfg80211_wext_siwscan ( struct net_device * dev , <nl> wext_freq_not_found : ; <nl> } <nl> } <nl> + /* No channels found ? */ <nl> + if (! i ) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl>  <nl> /* Set real number of channels specified in creq -> channels [] */ <nl> creq -> n_channels = i ;
int tipc_nl_node_get_monitor ( struct sk_buff * skb , struct genl_info * info ) <nl> int err ; <nl>  <nl> msg . skb = nlmsg_new ( NLMSG_GOODSIZE , GFP_KERNEL ); <nl> + if (! msg . skb ) <nl> + return - ENOMEM ; <nl> msg . portid = info -> snd_portid ; <nl> msg . seq = info -> snd_seq ; <nl> 
static void sahara_decode_status ( struct sahara_dev * dev , unsigned int status ) <nl> if ( status & SAHARA_STATUS_MODE_BATCH ) <nl> dev_dbg ( dev -> device , " - Batch Mode .\ n "); <nl> else if ( status & SAHARA_STATUS_MODE_DEDICATED ) <nl> - dev_dbg ( dev -> device , " - Decidated Mode .\ n "); <nl> + dev_dbg ( dev -> device , " - Dedicated Mode .\ n "); <nl> else if ( status & SAHARA_STATUS_MODE_DEBUG ) <nl> dev_dbg ( dev -> device , " - Debug Mode .\ n "); <nl> 
static int sierra_resume ( struct usb_serial * serial ) <nl> if ( err < 0 ) { <nl> intfdata -> in_flight --; <nl> usb_unanchor_urb ( urb ); <nl> - usb_scuttle_anchored_urbs (& portdata -> delayed ); <nl> - break ; <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> + spin_lock (& portdata -> lock ); <nl> + portdata -> outstanding_urbs --; <nl> + spin_unlock (& portdata -> lock ); <nl> + continue ; <nl> } <nl> } <nl> 
static int input_open_polled_device ( struct input_dev * input ) <nl> dev -> open ( dev ); <nl>  <nl> /* Only start polling if polling is enabled */ <nl> - if ( dev -> poll_interval > 0 ) <nl> - queue_delayed_work ( system_freezable_wq , & dev -> work , 0 ); <nl> + if ( dev -> poll_interval > 0 ) { <nl> + dev -> poll ( dev ); <nl> + input_polldev_queue_work ( dev ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void rtl8152_get_drvinfo ( struct net_device * netdev , <nl> { <nl> struct r8152 * tp = netdev_priv ( netdev ); <nl>  <nl> - strncpy ( info -> driver , MODULENAME , ETHTOOL_BUSINFO_LEN ); <nl> - strncpy ( info -> version , DRIVER_VERSION , ETHTOOL_BUSINFO_LEN ); <nl> + strlcpy ( info -> driver , MODULENAME , sizeof ( info -> driver )); <nl> + strlcpy ( info -> version , DRIVER_VERSION , sizeof ( info -> version )); <nl> usb_make_path ( tp -> udev , info -> bus_info , sizeof ( info -> bus_info )); <nl> } <nl> 
void btrfs_rm_dev_replace_srcdev ( struct btrfs_fs_info * fs_info , <nl> fs_devices -> num_devices --; <nl> if ( srcdev -> missing ) { <nl> fs_devices -> missing_devices --; <nl> - fs_devices -> rw_devices ++; <nl> + if (! fs_devices -> seeding ) <nl> + fs_devices -> rw_devices ++; <nl> } <nl> if ( srcdev -> can_discard ) <nl> fs_devices -> num_can_discard --;
static void v4l2_ctrl_del_event ( struct v4l2_subscribed_event * sev ) <nl> { <nl> struct v4l2_ctrl * ctrl = v4l2_ctrl_find ( sev -> fh -> ctrl_handler , sev -> id ); <nl>  <nl> + if ( ctrl == NULL ) <nl> + return ; <nl> + <nl> v4l2_ctrl_lock ( ctrl ); <nl> list_del (& sev -> node ); <nl> v4l2_ctrl_unlock ( ctrl );
int perf_evlist__parse_mmap_pages ( const struct option * opt , const char * str , <nl> unsigned long max = UINT_MAX ; <nl> long pages ; <nl>  <nl> - if ( max < SIZE_MAX / page_size ) <nl> + if ( max > SIZE_MAX / page_size ) <nl> max = SIZE_MAX / page_size ; <nl>  <nl> pages = parse_pages_arg ( str , 1 , max );
static struct sh_eth_cpu_data sh7734_data = { <nl> . tsu = 1 , <nl> . hw_checksum = 1 , <nl> . select_mii = 1 , <nl> + . magic = 1 , <nl> }; <nl>  <nl> /* SH7763 */
void disable_lapic_nmi_watchdog ( void ) <nl> return ; <nl>  <nl> on_each_cpu ( stop_apic_nmi_watchdog , NULL , 0 , 1 ); <nl> - wd_ops -> unreserve (); <nl> + <nl> + if ( wd_ops ) <nl> + wd_ops -> unreserve (); <nl>  <nl> BUG_ON ( atomic_read (& nmi_active ) != 0 ); <nl> }
static int proc_thermal_add ( struct device * dev , <nl> int ret ; <nl>  <nl> adev = ACPI_COMPANION ( dev ); <nl> + if (! adev ) <nl> + return - ENODEV ; <nl>  <nl> status = acpi_evaluate_object ( adev -> handle , " PPCC ", NULL , & buf ); <nl> if ( ACPI_FAILURE ( status ))
int cpufreq_update_policy ( unsigned int cpu ) <nl> */ <nl> if ( cpufreq_driver -> get ) { <nl> new_policy . cur = cpufreq_driver -> get ( cpu ); <nl> + if ( WARN_ON (! new_policy . cur )) { <nl> + ret = - EIO ; <nl> + goto no_policy ; <nl> + } <nl> + <nl> if (! policy -> cur ) { <nl> pr_debug (" Driver did not initialize current freq "); <nl> policy -> cur = new_policy . cur ;
int diAlloc ( struct inode * pip , bool dir , struct inode * ip ) <nl> /* mask any prior bits for the starting words of the <nl> * summary map . <nl> */ <nl> - mask = ONES << ( EXTSPERSUM - bitno ); <nl> + mask = ( bitno == 0 ) ? 0 : ( ONES << ( EXTSPERSUM - bitno )); <nl> inosmap = le32_to_cpu ( iagp -> inosmap [ sword ]) | mask ; <nl> extsmap = le32_to_cpu ( iagp -> extsmap [ sword ]) | mask ; <nl> 
static int encode_caps_cb ( struct inode * inode , struct ceph_cap * cap , <nl> spin_lock (& ci -> i_ceph_lock ); <nl> cap -> seq = 0 ; /* reset cap seq */ <nl> cap -> issue_seq = 0 ; /* and issue_seq */ <nl> + cap -> mseq = 0 ; /* and migrate_seq */ <nl>  <nl> if ( recon_state -> flock ) { <nl> rec . v2 . cap_id = cpu_to_le64 ( cap -> cap_id );
static int xenbus_write_transaction ( unsigned msg_type , <nl> return xenbus_command_reply ( u , XS_ERROR , " ENOENT "); <nl>  <nl> rc = xenbus_dev_request_and_reply (& u -> u . msg , u ); <nl> - if ( rc ) <nl> + if ( rc && trans ) { <nl> + list_del (& trans -> list ); <nl> kfree ( trans ); <nl> + } <nl>  <nl> out : <nl> return rc ;
static int mcp23s08_irq_setup ( struct mcp23s08 * mcp ) <nl> return - ENODEV ; <nl>  <nl> err = devm_request_threaded_irq ( chip -> dev , mcp -> irq , NULL , mcp23s08_irq , <nl> - IRQF_TRIGGER_LOW | IRQF_ONESHOT , <nl> + IRQF_TRIGGER_LOW | IRQF_ONESHOT | <nl> + IRQF_SHARED , <nl> dev_name ( chip -> dev ), mcp ); <nl> if ( err != 0 ) { <nl> dev_err ( chip -> dev , " unable to request IRQ #% d : % d \ n ",
static void ad1884_fixup_thinkpad ( struct hda_codec * codec , <nl> { <nl> struct ad198x_spec * spec = codec -> spec ; <nl>  <nl> - if ( action == HDA_FIXUP_ACT_PRE_PROBE ) <nl> + if ( action == HDA_FIXUP_ACT_PRE_PROBE ) { <nl> spec -> gen . keep_eapd_on = 1 ; <nl> + spec -> gen . vmaster_mute . hook = ad_vmaster_eapd_hook ; <nl> + spec -> eapd_nid = 0x12 ; <nl> + } <nl> } <nl>  <nl> /* set magic COEFs for dmic */
static void fan_watchdog_reset ( void ) <nl> { <nl> static int fan_watchdog_active = 0 ; <nl>  <nl> + if ( fan_control_access_mode == TPACPI_FAN_WR_NONE ) <nl> + return ; <nl> + <nl> if ( fan_watchdog_active ) <nl> cancel_delayed_work (& fan_watchdog_task ); <nl> 
static int ak8975_probe ( struct i2c_client * client , <nl> indio_dev -> channels = ak8975_channels ; <nl> indio_dev -> num_channels = ARRAY_SIZE ( ak8975_channels ); <nl> indio_dev -> info = & ak8975_info ; <nl> + indio_dev -> name = id -> name ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl>  <nl> err = iio_device_register ( indio_dev );
STATIC int INIT gunzip ( unsigned char * buf , int len , <nl> strm -> next_in ++; <nl> strm -> next_in ++; <nl> } <nl> - strm -> avail_in = len - 10 ; <nl> + strm -> avail_in = len - ( strm -> next_in - zbuf ); <nl>  <nl> strm -> next_out = out_buf ; <nl> strm -> avail_out = out_len ;
static int ext4_mb_init_per_dev_proc ( struct super_block * sb ) <nl> struct proc_dir_entry * proc ; <nl> char devname [ 64 ]; <nl>  <nl> + if ( proc_root_ext4 == NULL ) { <nl> + sbi -> s_mb_proc = NULL ; <nl> + return - EINVAL ; <nl> + } <nl> bdevname ( sb -> s_bdev , devname ); <nl> sbi -> s_mb_proc = proc_mkdir ( devname , proc_root_ext4 ); <nl> 
static int ti_qspi_dma_xfer ( struct ti_qspi * qspi , dma_addr_t dma_dst , <nl> tx -> callback = ti_qspi_dma_callback ; <nl> tx -> callback_param = qspi ; <nl> cookie = tx -> tx_submit ( tx ); <nl> + reinit_completion (& qspi -> transfer_complete ); <nl>  <nl> ret = dma_submit_error ( cookie ); <nl> if ( ret ) {
xfs_setattr_nonsize ( <nl> } <nl> if (! gid_eq ( igid , gid )) { <nl> if ( XFS_IS_QUOTA_RUNNING ( mp ) && XFS_IS_GQUOTA_ON ( mp )) { <nl> - ASSERT (! XFS_IS_PQUOTA_ON ( mp )); <nl> + ASSERT ( xfs_sb_version_has_pquotino (& mp -> m_sb ) || <nl> + ! XFS_IS_PQUOTA_ON ( mp )); <nl> ASSERT ( mask & ATTR_GID ); <nl> ASSERT ( gdqp ); <nl> olddquot2 = xfs_qm_vop_chown ( tp , ip ,
static struct config_item_type printer_func_type = { <nl>  <nl> static inline int gprinter_get_minor ( void ) <nl> { <nl> - return ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + int ret ; <nl> + <nl> + ret = ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + if ( ret >= PRINTER_MINORS ) { <nl> + ida_simple_remove (& printer_ida , ret ); <nl> + ret = - ENODEV ; <nl> + } <nl> + <nl> + return ret ; <nl> } <nl>  <nl> static inline void gprinter_put_minor ( int minor )
void oz_apps_term ( void ) <nl> void oz_handle_app_elt ( struct oz_pd * pd , u8 app_id , struct oz_elt * elt ) <nl> { <nl> struct oz_app_if * ai ; <nl> - if ( app_id > OZ_APPID_MAX ) <nl> + if ( app_id == 0 || app_id > OZ_APPID_MAX ) <nl> return ; <nl> ai = & g_app_if [ app_id - 1 ]; <nl> ai -> rx ( pd , elt );
static int spi_map_buf ( struct spi_master * master , struct device * dev , <nl> } <nl>  <nl> ret = dma_map_sg ( dev , sgt -> sgl , sgt -> nents , dir ); <nl> + if (! ret ) <nl> + ret = - ENOMEM ; <nl> if ( ret < 0 ) { <nl> sg_free_table ( sgt ); <nl> return ret ;
send : <nl> ret = batadv_send_skb_via_tt ( bat_priv , skb , dst_hint , <nl> vid ); <nl> } <nl> - if ( ret == NET_XMIT_DROP ) <nl> + if ( ret != NET_XMIT_SUCCESS ) <nl> goto dropped_freed ; <nl> } <nl> 
cfq_rq_enqueued ( struct cfq_data * cfqd , struct cfq_queue * cfqq , <nl> if ( blk_rq_bytes ( rq ) > PAGE_CACHE_SIZE || <nl> cfqd -> busy_queues > 1 ) { <nl> del_timer (& cfqd -> idle_slice_timer ); <nl> - __blk_run_queue ( cfqd -> queue ); <nl> - } <nl> - cfq_mark_cfqq_must_dispatch ( cfqq ); <nl> + __blk_run_queue ( cfqd -> queue ); <nl> + } else <nl> + cfq_mark_cfqq_must_dispatch ( cfqq ); <nl> } <nl> } else if ( cfq_should_preempt ( cfqd , cfqq , rq )) { <nl> /*
__xfs_printk ( <nl> const struct xfs_mount * mp , <nl> struct va_format * vaf ) <nl> { <nl> - if ( mp && mp -> m_fsname ) <nl> + if ( mp && mp -> m_fsname ) { <nl> printk ("% sXFS (% s ): % pV \ n ", level , mp -> m_fsname , vaf ); <nl> + return ; <nl> + } <nl> printk ("% sXFS : % pV \ n ", level , vaf ); <nl> } <nl> 
static void b43_supported_bands ( struct b43_wldev * dev , bool * have_2ghz_phy , <nl> * have_5ghz_phy = true ; <nl> return ; <nl> case 0x4321 : /* BCM4306 */ <nl> + /* There are 14e4 : 4321 PCI devs with 2 . 4 GHz BCM4321 ( N - PHY ) */ <nl> + if ( dev -> phy . type != B43_PHYTYPE_G ) <nl> + break ; <nl> + /* fall through */ <nl> case 0x4313 : /* BCM4311 */ <nl> case 0x431a : /* BCM4318 */ <nl> case 0x432a : /* BCM4321 */
static ssize_t spufs_mfc_write ( struct file * file , const char __user * buffer , <nl> if ( ret ) <nl> goto out ; <nl>  <nl> - spu_acquire_runnable ( ctx , 0 ); <nl> + ret = spu_acquire_runnable ( ctx , 0 ); <nl> + if ( ret ) <nl> + goto out ; <nl> + <nl> if ( file -> f_flags & O_NONBLOCK ) { <nl> ret = ctx -> ops -> send_mfc_command ( ctx , & cmd ); <nl> } else {
static int do_calculate_time ( int status , enum apm_source source ) <nl> return - 1 ; <nl> } <nl>  <nl> + if (! I . intval ) <nl> + return 0 ; <nl> + <nl> switch ( source ) { <nl> case SOURCE_CHARGE : <nl> full_prop = POWER_SUPPLY_PROP_CHARGE_FULL ;
static const struct comedi_lrange * dac_range_table [] = { <nl>  <nl> static const struct comedi_lrange * dac_range_lkup ( int opt ) <nl> { <nl> - if ( opt < 0 || opt > 5 ) <nl> + if ( opt < 0 || opt >= 5 ) <nl> return & range_unknown ; <nl> return dac_range_table [ opt ]; <nl> }
static int mtk_pmx_gpio_request_enable ( struct pinctrl_dev * pctldev , <nl> } <nl>  <nl> mtk_pmx_set_mode ( pctldev , offset , muxval ); <nl> + mtk_pconf_set_ies_smt ( pctl , offset , 1 , PIN_CONFIG_INPUT_ENABLE ); <nl>  <nl> return 0 ; <nl> }
set_v4l_control ( struct inode * inode , <nl>  <nl> /* ----------------------------------------------------------------- */ <nl>  <nl> - const static unsigned int palette2pixelformat [] = { <nl> + static const unsigned int palette2pixelformat [] = { <nl> [ VIDEO_PALETTE_GREY ] = V4L2_PIX_FMT_GREY , <nl> [ VIDEO_PALETTE_RGB555 ] = V4L2_PIX_FMT_RGB555 , <nl> [ VIDEO_PALETTE_RGB565 ] = V4L2_PIX_FMT_RGB565 ,
static void ixgbe_watchdog_link_is_up ( struct ixgbe_adapter * adapter ) <nl> ( flow_tx ? " TX " : " None ")))); <nl>  <nl> netif_carrier_on ( netdev ); <nl> -# ifdef HAVE_IPLINK_VF_CONFIG <nl> ixgbe_check_vf_rate_limit ( adapter ); <nl> -# endif /* HAVE_IPLINK_VF_CONFIG */ <nl> } <nl>  <nl> /**
restart : <nl> } else { <nl> spin_unlock (& gl -> gl_spin ); <nl>  <nl> - new_gh = gfs2_holder_get ( gl , state , LM_FLAG_TRY , GFP_KERNEL ); <nl> + new_gh = gfs2_holder_get ( gl , state , LM_FLAG_TRY , GFP_NOFS ); <nl> if (! new_gh ) <nl> return ; <nl> set_bit ( HIF_DEMOTE , & new_gh -> gh_iflags );
static inline void update_sd_power_savings_stats ( struct sched_group * group , <nl> * capacity but still has some space to pick up some load <nl> * from other group and save more power <nl> */ <nl> - if ( sgs -> sum_nr_running > sgs -> group_capacity - 1 ) <nl> + if ( sgs -> sum_nr_running + 1 > sgs -> group_capacity ) <nl> return ; <nl>  <nl> if ( sgs -> sum_nr_running > sds -> leader_nr_running ||
static int __init early_get_pnodeid ( void ) <nl> break ; <nl> case UV3_HUB_PART_NUMBER : <nl> case UV3_HUB_PART_NUMBER_X : <nl> - uv_min_hub_revision_id += UV3_HUB_REVISION_BASE - 1 ; <nl> + uv_min_hub_revision_id += UV3_HUB_REVISION_BASE ; <nl> break ; <nl> } <nl> 
static int __init gc_setup_pad ( struct gc * gc , int idx , int pad_type ) <nl> int i ; <nl> int err ; <nl>  <nl> - if ( pad_type < 1 || pad_type > GC_MAX ) { <nl> + if ( pad_type < 1 || pad_type >= GC_MAX ) { <nl> pr_err (" Pad type % d unknown \ n ", pad_type ); <nl> return - EINVAL ; <nl> }
static int sd_sdr_tuning ( struct rtsx_chip * chip ) <nl> int retval ; <nl>  <nl> retval = sd_tuning_tx ( chip ); <nl> - if ( retval != STATUS_SUCCESS ) { <nl> + if ( retval != STATUS_SUCCESS ) <nl> TRACE_RET ( chip , STATUS_FAIL ); <nl> - } <nl>  <nl> retval = sd_tuning_rx ( chip ); <nl> - if ( retval != STATUS_SUCCESS ) { <nl> + if ( retval != STATUS_SUCCESS ) <nl> TRACE_RET ( chip , STATUS_FAIL ); <nl> - } <nl>  <nl> return STATUS_SUCCESS ; <nl> }
static int __init gt641xx_timer0_clockevent_init ( void ) <nl>  <nl> cd = & gt641xx_timer0_clockevent ; <nl> cd -> rating = 200 + gt641xx_base_clock / 10000000 ; <nl> + clockevent_set_clock ( cd , gt641xx_base_clock ); <nl> cd -> max_delta_ns = clockevent_delta2ns ( 0x7fffffff , cd ); <nl> cd -> min_delta_ns = clockevent_delta2ns ( 0x300 , cd ); <nl> - clockevent_set_clock ( cd , gt641xx_base_clock ); <nl>  <nl> clockevents_register_device (& gt641xx_timer0_clockevent ); <nl> 
i915_gem_object_get_pages_gtt ( struct drm_i915_gem_object * obj ) <nl>  <nl> page_count = obj -> base . size / PAGE_SIZE ; <nl> if ( sg_alloc_table ( st , page_count , GFP_KERNEL )) { <nl> - sg_free_table ( st ); <nl> kfree ( st ); <nl> return - ENOMEM ; <nl> }
int drm_mode_create_dumb_ioctl ( struct drm_device * dev , <nl> return - EINVAL ; <nl>  <nl> /* overflow checks for 32bit size calculations */ <nl> + /* NOTE : DIV_ROUND_UP () can overflow */ <nl> cpp = DIV_ROUND_UP ( args -> bpp , 8 ); <nl> - if ( cpp > 0xffffffffU / args -> width ) <nl> + if (! cpp || cpp > 0xffffffffU / args -> width ) <nl> return - EINVAL ; <nl> stride = cpp * args -> width ; <nl> if ( args -> height > 0xffffffffU / stride )
static void igb_update_ring_itr ( struct igb_q_vector * q_vector ) <nl> else <nl> new_val = avg_wire_size / 2 ; <nl>  <nl> + /* when in itr mode 3 do not exceed 20K ints / sec */ <nl> + if ( adapter -> rx_itr_setting == 3 && new_val < 196 ) <nl> + new_val = 196 ; <nl> + <nl> set_itr_val : <nl> if ( new_val != q_vector -> itr_val ) { <nl> q_vector -> itr_val = new_val ;
static int rcar_vin_videobuf_setup ( struct vb2_queue * vq , <nl> unsigned int bytes_per_line ; <nl> int ret ; <nl>  <nl> + if ( fmt -> fmt . pix . sizeimage < icd -> sizeimage ) <nl> + return - EINVAL ; <nl> + <nl> xlate = soc_camera_xlate_by_fourcc ( icd , <nl> fmt -> fmt . pix . pixelformat ); <nl> if (! xlate )
static int be_get_config ( struct be_adapter * adapter ) <nl> if ( status ) <nl> return status ; <nl>  <nl> - /* primary mac needs 1 pmac entry */ <nl> - adapter -> pmac_id = kcalloc ( be_max_uc ( adapter ) + 1 , sizeof ( u32 ), <nl> - GFP_KERNEL ); <nl> + adapter -> pmac_id = kcalloc ( be_max_uc ( adapter ), <nl> + sizeof (* adapter -> pmac_id ), GFP_KERNEL ); <nl> if (! adapter -> pmac_id ) <nl> return - ENOMEM ; <nl> 
static void qlcnic_get_ethtool_stats ( struct net_device * dev , <nl> tx_ring = & adapter -> tx_ring [ ring ]; <nl> data = qlcnic_fill_tx_queue_stats ( data , tx_ring ); <nl> qlcnic_update_stats ( adapter ); <nl> + } else { <nl> + data += QLCNIC_TX_STATS_LEN ; <nl> } <nl> } <nl> 
static int _sp2d_alloc ( unsigned pages_in_unit , unsigned group_width , <nl> num_a1pa = min_t ( unsigned , PAGE_SIZE / sizeof__a1pa , <nl> pages_in_unit - i ); <nl>  <nl> - __a1pa = kzalloc ( num_a1pa * sizeof__a1pa , GFP_KERNEL ); <nl> + __a1pa = kcalloc ( num_a1pa , sizeof__a1pa , GFP_KERNEL ); <nl> if ( unlikely (! __a1pa )) { <nl> ORE_DBGMSG ("!! Failed to _alloc_1p_arrays =% d \ n ", <nl> num_a1pa );
static struct comedi_driver pcl726_driver = { <nl> module_comedi_driver ( pcl726_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for Advantech PCL - 726 & compatibles "); <nl> MODULE_LICENSE (" GPL ");
static struct drm_driver tegra_drm_driver = { <nl> . debugfs_cleanup = tegra_debugfs_cleanup , <nl> # endif <nl>  <nl> - . gem_free_object = tegra_bo_free_object , <nl> + . gem_free_object_unlocked = tegra_bo_free_object , <nl> . gem_vm_ops = & tegra_bo_vm_ops , <nl>  <nl> . prime_handle_to_fd = drm_gem_prime_handle_to_fd ,
static long ft1000_ChIoctl ( struct file * File , unsigned int Command , <nl> break ; <nl> case IOCTL_GET_DSP_STAT_CMD : <nl> // DEBUG (" FT1000 : ft1000_ChIoctl : IOCTL_FT1000_GET_DSP_STAT called \ n "); <nl> - <nl> + memset (& get_stat_data , 0 , sizeof ( get_stat_data )); <nl> memcpy ( get_stat_data . DspVer , info -> DspVer , DSPVERSZ ); <nl> memcpy ( get_stat_data . HwSerNum , info -> HwSerNum , HWSERNUMSZ ); <nl> memcpy ( get_stat_data . Sku , info -> Sku , SKUSZ );
int add_extent_mapping ( struct extent_map_tree * tree , <nl> rb = tree_insert (& tree -> map , em -> start , & em -> rb_node ); <nl> if ( rb ) { <nl> ret = - EEXIST ; <nl> - free_extent_map ( merge ); <nl> goto out ; <nl> } <nl> atomic_inc (& em -> refs );
static void esdhc_writew_le ( struct sdhci_host * host , u16 val , int reg ) <nl> new_val |= ESDHC_VENDOR_SPEC_FRC_SDCLK_ON ; <nl> else <nl> new_val &= ~ ESDHC_VENDOR_SPEC_FRC_SDCLK_ON ; <nl> - writel ( new_val , host -> ioaddr + ESDHC_VENDOR_SPEC ); <nl> + writel ( new_val , host -> ioaddr + ESDHC_VENDOR_SPEC ); <nl> return ; <nl> case SDHCI_HOST_CONTROL2 : <nl> new_val = readl ( host -> ioaddr + ESDHC_VENDOR_SPEC );
static void ext3_put_super ( struct super_block * sb ) <nl> } <nl> sb -> s_fs_info = NULL ; <nl> kfree ( sbi -> s_blockgroup_lock ); <nl> + mutex_destroy (& sbi -> s_orphan_lock ); <nl> + mutex_destroy (& sbi -> s_resize_lock ); <nl> kfree ( sbi ); <nl> } <nl> 
static inline void bio_list_add ( struct bio_list * bl , struct bio * bio ) <nl>  <nl> static inline void bio_list_merge ( struct bio_list * bl , struct bio_list * bl2 ) <nl> { <nl> + if (! bl2 -> head ) <nl> + return ; <nl> + <nl> if ( bl -> tail ) <nl> bl -> tail -> bi_next = bl2 -> head ; <nl> else
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) { <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) { <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& priv -> lock , flags );
static int omap_mcbsp_dai_startup ( struct snd_pcm_substream * substream , <nl> * smaller buffer than the FIFO size to avoid underruns <nl> */ <nl> snd_pcm_hw_rule_add ( substream -> runtime , 0 , <nl> - SNDRV_PCM_HW_PARAM_CHANNELS , <nl> + SNDRV_PCM_HW_PARAM_BUFFER_SIZE , <nl> omap_mcbsp_hwrule_min_buffersize , <nl> mcbsp , <nl> - SNDRV_PCM_HW_PARAM_BUFFER_SIZE , - 1 ); <nl> + SNDRV_PCM_HW_PARAM_CHANNELS , - 1 ); <nl>  <nl> /* Make sure , that the period size is always even */ <nl> snd_pcm_hw_constraint_step ( substream -> runtime , 0 ,
static const struct key_entry eeepc_wmi_keymap [] = { <nl> { KE_KEY , 0xcc , { KEY_SWITCHVIDEOMODE } }, <nl> { KE_KEY , 0xe0 , { KEY_PROG1 } }, <nl> { KE_KEY , 0xe1 , { KEY_F14 } }, <nl> - { KE_KEY , 0xe9 , { KEY_DISPLAY_OFF } }, <nl> + { KE_KEY , 0xe9 , { KEY_BRIGHTNESS_ZERO } }, <nl> { KE_END , 0 }, <nl> }; <nl> 
parse_dcb15_entry ( struct drm_device * dev , struct dcb_table * dcb , <nl> entry -> tvconf . has_component_output = false ; <nl> break ; <nl> case OUTPUT_LVDS : <nl> - if (( conn & 0x00003f00 ) != 0x10 ) <nl> + if (( conn & 0x00003f00 ) >> 8 != 0x10 ) <nl> entry -> lvdsconf . use_straps_for_mode = true ; <nl> entry -> lvdsconf . use_power_scripts = true ; <nl> break ;
struct snd_soc_card { <nl> /* <nl> * Card - specific routes and widgets . <nl> */ <nl> - struct snd_soc_dapm_widget * dapm_widgets ; <nl> + const struct snd_soc_dapm_widget * dapm_widgets ; <nl> int num_dapm_widgets ; <nl> - struct snd_soc_dapm_route * dapm_routes ; <nl> + const struct snd_soc_dapm_route * dapm_routes ; <nl> int num_dapm_routes ; <nl>  <nl> struct work_struct deferred_resume_work ;
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> * queued ( again ) by ceph_update_snap_trace () <nl> * below . Queue it _now_ , under the old context . <nl> */ <nl> + spin_lock (& realm -> inodes_with_caps_lock ); <nl> list_del_init (& ci -> i_snap_realm_item ); <nl> + spin_unlock (& realm -> inodes_with_caps_lock ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> ceph_queue_cap_snap ( ci ,
long drm_ioctl ( struct file * filp , <nl> usize = asize = _IOC_SIZE ( cmd ); <nl> if ( drv_size > asize ) <nl> asize = drv_size ; <nl> + cmd = ioctl -> cmd_drv ; <nl> } <nl> else if (( nr >= DRM_COMMAND_END ) || ( nr < DRM_COMMAND_BASE )) { <nl> ioctl = & drm_ioctls [ nr ];
static int mxt_lookup_bootloader_address ( struct mxt_data * data ) <nl> switch ( appmode ) { <nl> case 0x4a : <nl> case 0x4b : <nl> + /* Chips after 1664S use different scheme */ <nl> + if ( data -> info . family_id >= 0xa2 ) { <nl> + bootloader = appmode - 0x24 ; <nl> + break ; <nl> + } <nl> + /* Fall through for normal case */ <nl> case 0x4c : <nl> case 0x4d : <nl> case 0x5a :
xfs_allocbt_free_block ( <nl> xfs_extent_busy_insert ( cur -> bc_tp , be32_to_cpu ( agf -> agf_seqno ), bno , 1 , <nl> XFS_EXTENT_BUSY_SKIP_DISCARD ); <nl> xfs_trans_agbtree_delta ( cur -> bc_tp , - 1 ); <nl> + <nl> + xfs_trans_binval ( cur -> bc_tp , bp ); <nl> return 0 ; <nl> } <nl> 
card_probe_error : <nl> if ( card -> remove ) <nl> card -> remove ( card ); <nl>  <nl> + snd_soc_dapm_free (& card -> dapm ); <nl> soc_cleanup_card_debugfs ( card ); <nl> snd_card_free ( card -> snd_card ); <nl> 
late_initcall ( geneve_init_module ); <nl> static void __exit geneve_cleanup_module ( void ) <nl> { <nl> destroy_workqueue ( geneve_wq ); <nl> + unregister_pernet_subsys (& geneve_net_ops ); <nl> } <nl> module_exit ( geneve_cleanup_module ); <nl> 
static int bnx2x_get_hwinfo ( struct bnx2x * bp ) <nl> } else <nl> BNX2X_DEV_INFO (" illegal OV for SD \ n "); <nl> break ; <nl> + case SHARED_FEAT_CFG_FORCE_SF_MODE_FORCED_SF : <nl> + bp -> mf_config [ vn ] = 0 ; <nl> + break ; <nl> default : <nl> /* Unknown configuration : reset mf_config */ <nl> bp -> mf_config [ vn ] = 0 ;
int w1_process ( void * data ) <nl> jremain = 1 ; <nl> } <nl>  <nl> - try_to_freeze (); <nl> __set_current_state ( TASK_INTERRUPTIBLE ); <nl>  <nl> /* hold list_mutex until after interruptible to prevent loosing
out : <nl>  <nl> static void __init zynq_timer_init ( void ) <nl> { <nl> - zynq_early_slcr_init (); <nl> - <nl> zynq_clock_init (); <nl> of_clk_init ( NULL ); <nl> clocksource_probe (); <nl> static void __init zynq_map_io ( void ) <nl>  <nl> static void __init zynq_irq_init ( void ) <nl> { <nl> + zynq_early_slcr_init (); <nl> irqchip_init (); <nl> } <nl> 
pkttype_mt ( const struct sk_buff * skb , const struct net_device * in , <nl> const struct xt_pkttype_info * info = matchinfo ; <nl>  <nl> if ( skb -> pkt_type == PACKET_LOOPBACK ) <nl> - type = ipv4_is_multicast ( ip_hdr ( skb )-> daddr ) <nl> + type = match -> family == AF_INET && <nl> + ipv4_is_multicast ( ip_hdr ( skb )-> daddr ) <nl> ? PACKET_MULTICAST <nl> : PACKET_BROADCAST ; <nl> else
static ssize_t btrfs_direct_IO ( int rw , struct kiocb * iocb , <nl> btrfs_submit_direct , 0 ); <nl> } <nl>  <nl> +# define BTRFS_FIEMAP_FLAGS ( FIEMAP_FLAG_SYNC ) <nl> + <nl> static int btrfs_fiemap ( struct inode * inode , struct fiemap_extent_info * fieinfo , <nl> __u64 start , __u64 len ) <nl> { <nl> + int ret ; <nl> + <nl> + ret = fiemap_check_flags ( fieinfo , BTRFS_FIEMAP_FLAGS ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> return extent_fiemap ( inode , fieinfo , start , len , btrfs_get_extent_fiemap ); <nl> } <nl> 
get_more_pages : <nl> ci -> i_truncate_seq , <nl> ci -> i_truncate_size , <nl> & inode -> i_mtime , true , 1 , 0 ); <nl> + <nl> + if (! req ) { <nl> + rc = - ENOMEM ; <nl> + unlock_page ( page ); <nl> + break ; <nl> + } <nl> + <nl> max_pages = req -> r_num_pages ; <nl>  <nl> alloc_page_vec ( fsc , req );
static struct page * balloon_next_page ( struct page * page ) <nl>  <nl> static enum bp_state update_schedule ( enum bp_state state ) <nl> { <nl> + if ( state == BP_ECANCELED ) <nl> + return BP_ECANCELED ; <nl> + <nl> if ( state == BP_DONE ) { <nl> balloon_stats . schedule_delay = 1 ; <nl> balloon_stats . retry_count = 1 ;
static void __init ati_bugs_contd ( int num , int slot , int func ) <nl> if ( rev >= 0x40 ) <nl> acpi_fix_pin2_polarity = 1 ; <nl>  <nl> - if ( rev > 0x13 ) <nl> + /* <nl> + * SB600 : revisions 0x11 , 0x12 , 0x13 , 0x14 , ... <nl> + * SB700 : revisions 0x39 , 0x3a , ... <nl> + * SB800 : revisions 0x40 , 0x41 , ... <nl> + */ <nl> + if ( rev >= 0x39 ) <nl> return ; <nl>  <nl> if ( acpi_use_timer_override )
static void __init _set_omap_chip ( void ) <nl>  <nl> } <nl>  <nl> - void __init omap2_check_revision ( void ) <nl> + void __init omap24xx_check_revision ( void ) <nl> { <nl> int i , j ; <nl> u32 idcode ; <nl> void __init omap2_check_revision ( void ) <nl>  <nl> } <nl>  <nl> + void __init omap2_check_revision ( void ) <nl> +{ <nl> + omap24xx_check_revision (); <nl> +} <nl> + <nl> void __init omap2_set_globals_tap ( struct omap_globals * omap2_globals ) <nl> { <nl> class = omap2_globals -> class ;
ieee80211_tx_h_rate_ctrl ( struct ieee80211_tx_data * tx ) <nl> IEEE80211_TX_RC_USE_RTS_CTS ; <nl>  <nl> /* RC is busted */ <nl> - if ( WARN_ON ( info -> control . rates [ i ]. idx >= <nl> - sband -> n_bitrates )) { <nl> + if ( WARN_ON_ONCE ( info -> control . rates [ i ]. idx >= <nl> + sband -> n_bitrates )) { <nl> info -> control . rates [ i ]. idx = - 1 ; <nl> continue ; <nl> }
static void intel_sanitize_crtc ( struct intel_crtc * crtc ) <nl> * ... */ <nl> plane = crtc -> plane ; <nl> crtc -> plane = ! plane ; <nl> + crtc -> primary_enabled = true ; <nl> dev_priv -> display . crtc_disable (& crtc -> base ); <nl> crtc -> plane = plane ; <nl> 
static inline void calculate_imbalance ( struct lb_env * env , struct sd_lb_stats * s <nl> * max load less than avg load ( as we skip the groups at or below <nl> * its cpu_power , while calculating max_load ..) <nl> */ <nl> - if ( busiest -> avg_load < sds -> avg_load ) { <nl> + if ( busiest -> avg_load <= sds -> avg_load || <nl> + local -> avg_load >= sds -> avg_load ) { <nl> env -> imbalance = 0 ; <nl> return fix_small_imbalance ( env , sds ); <nl> }
static void discard_cap_releases ( struct ceph_mds_client * mdsc , <nl> num = le32_to_cpu ( head -> num ); <nl> dout (" discard_cap_releases mds % d % p % u \ n ", session -> s_mds , msg , num ); <nl> head -> num = cpu_to_le32 ( 0 ); <nl> + msg -> front . iov_len = sizeof (* head ); <nl> session -> s_num_cap_releases += num ; <nl>  <nl> /* requeue completed messages */
void rv770_set_uvd_clock_before_set_eng_clock ( struct radeon_device * rdev , <nl> if ( new_state -> high . sclk >= current_state -> high . sclk ) <nl> return ; <nl>  <nl> - radeon_set_uvd_clocks ( rdev , new_ps -> vclk , old_ps -> dclk ); <nl> + radeon_set_uvd_clocks ( rdev , new_ps -> vclk , new_ps -> dclk ); <nl> } <nl>  <nl> void rv770_set_uvd_clock_after_set_eng_clock ( struct radeon_device * rdev ,
retry : <nl> /* <nl> * Recalculate credits when extent tree depth changes . <nl> */ <nl> - if ( depth >= 0 && depth != ext_depth ( inode )) { <nl> + if ( depth != ext_depth ( inode )) { <nl> credits = ext4_chunk_trans_blocks ( inode , len ); <nl> depth = ext_depth ( inode ); <nl> }
static int mv643xx_eth_receive_queue ( struct net_device * dev ) <nl> netif_rx ( skb ); <nl> # endif <nl> } <nl> + dev -> last_rx = jiffies ; <nl> } <nl>  <nl> return received_packets ;
int inet_recv_error ( struct sock * sk , struct msghdr * msg , int len , <nl>  <nl> static inline void inet_ctl_sock_destroy ( struct sock * sk ) <nl> { <nl> - sock_release ( sk -> sk_socket ); <nl> + if ( sk ) <nl> + sock_release ( sk -> sk_socket ); <nl> } <nl>  <nl> # endif
# endif <nl> # include " igb . h " <nl>  <nl> -# define DRV_VERSION " 2 . 4 . 13 - k2 " <nl> +# define MAJ 3 <nl> +# define MIN 0 <nl> +# define BUILD 6 <nl> +# define KFIX 2 <nl> +# define DRV_VERSION __stringify ( MAJ ) "." __stringify ( MIN ) "." \ <nl> + __stringify ( BUILD ) "- k " __stringify ( KFIX ) <nl> char igb_driver_name [] = " igb "; <nl> char igb_driver_version [] = DRV_VERSION ; <nl> static const char igb_driver_string [] =
static int ascot2e_write_regs ( struct ascot2e_priv * priv , <nl> } <nl> }; <nl>  <nl> - if ( len + 1 >= sizeof ( buf )) { <nl> + if ( len + 1 > sizeof ( buf )) { <nl> dev_warn (& priv -> i2c -> dev ," wr reg =% 04x : len =% d is too big !\ n ", <nl> reg , len + 1 ); <nl> return - E2BIG ;
static int __diag_ipl_functions ( struct kvm_vcpu * vcpu ) <nl>  <nl> VCPU_EVENT ( vcpu , 5 , " diag ipl functions , subcode % lx ", subcode ); <nl> switch ( subcode ) { <nl> + case 0 : <nl> + case 1 : <nl> + page_table_reset_pgste ( current -> mm , 0 , TASK_SIZE ); <nl> + return - EOPNOTSUPP ; <nl> case 3 : <nl> vcpu -> run -> s390_reset_flags = KVM_S390_RESET_CLEAR ; <nl> page_table_reset_pgste ( current -> mm , 0 , TASK_SIZE );
static int __devinit isp1761_pci_probe ( struct pci_dev * dev , <nl> hcd = isp1760_register ( pci_mem_phy0 , length , dev -> irq , <nl> IRQF_SHARED | IRQF_DISABLED , & dev -> dev , dev_name (& dev -> dev ), <nl> devflags ); <nl> - pci_set_drvdata ( dev , hcd ); <nl> - if (! hcd ) <nl> + if (! IS_ERR ( hcd )) { <nl> + pci_set_drvdata ( dev , hcd ); <nl> return 0 ; <nl> + } <nl> clean : <nl> status = - ENODEV ; <nl> iounmap ( iobase );
void perf_hpp__column_disable ( unsigned col ) <nl>  <nl> void perf_hpp__cancel_cumulate ( void ) <nl> { <nl> + if ( field_order ) <nl> + return ; <nl> + <nl> perf_hpp__column_disable ( PERF_HPP__OVERHEAD_ACC ); <nl> perf_hpp__format [ PERF_HPP__OVERHEAD ]. header = hpp__header_overhead ; <nl> }
int mccic_irq ( struct mcam_camera * cam , unsigned int irqs ) <nl> if ( irqs & ( IRQ_EOF0 << frame )) { <nl> mcam_frame_complete ( cam , frame ); <nl> handled = 1 ; <nl> + if ( cam -> buffer_mode == B_DMA_sg ) <nl> + break ; <nl> } <nl> /* <nl> * If a frame starts , note that we have DMA active . This
static ssize_t set_pwm_mode ( struct device * dev , struct device_attribute * attr , <nl> if (!( val == 0 || val == 1 )) <nl> return - EINVAL ; <nl>  <nl> + /* F75373 does not support DC ( linear voltage ) fan control mode */ <nl> + if ( data -> kind == f75373 && val == 0 ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& data -> update_lock ); <nl> conf = f75375_read8 ( client , F75375_REG_CONFIG1 ); <nl> conf &= ~( 1 << FAN_CTRL_LINEAR ( nr ));
static void bfin_musb_try_idle ( struct musb * musb , unsigned long timeout ) <nl> mod_timer (& musb_conn_timer , jiffies + TIMER_DELAY ); <nl> } <nl>  <nl> - static int bfin_musb_get_vbus_status ( struct musb * musb ) <nl> + static int bfin_musb_vbus_status ( struct musb * musb ) <nl> { <nl> return 0 ; <nl> }
static int tcp_ack_update_window ( struct sock * sk , struct tcp_sock * tp , <nl> /* Note , it is the only place , where <nl> * fast path is recovered for sending TCP . <nl> */ <nl> + tp -> pred_flags = 0 ; <nl> tcp_fast_path_check ( sk , tp ); <nl>  <nl> if ( nwin > tp -> max_window ) {
static int ocfs2_rename ( struct inode * old_dir , <nl> * <nl> * And that ' s why , just like the VFS , we need a file system <nl> * rename lock . */ <nl> - if ( old_dentry != new_dentry ) { <nl> + if ( old_dir != new_dir && S_ISDIR ( old_inode -> i_mode )) { <nl> status = ocfs2_rename_lock ( osb ); <nl> if ( status < 0 ) { <nl> mlog_errno ( status );
bool gw_out_of_range ( struct bat_priv * bat_priv , <nl> } <nl>  <nl> neigh_old = find_router ( bat_priv , orig_dst_node , NULL ); <nl> - if (!! neigh_old ) <nl> + if (! neigh_old ) <nl> goto out ; <nl>  <nl> if ( curr_tq_avg - neigh_old -> tq_avg > GW_THRESHOLD )
static struct dma_chan * zx_of_dma_simple_xlate ( struct of_phandle_args * dma_spec , <nl> struct dma_chan * chan ; <nl> struct zx_dma_chan * c ; <nl>  <nl> - if ( request > d -> dma_requests ) <nl> + if ( request >= d -> dma_requests ) <nl> return NULL ; <nl>  <nl> chan = dma_get_any_slave_channel (& d -> slave );
static int sil24_softreset ( struct ata_port * ap , unsigned int * class ) <nl> goto err ; <nl> } <nl>  <nl> - /* <nl> - * XXX : Not sure whether the following sleep is needed or not . <nl> - * The original driver had it . So .... <nl> - */ <nl> - msleep ( 10 ); <nl> - <nl> + /* do SRST */ <nl> prb -> ctrl = PRB_CTRL_SRST ; <nl> prb -> fis [ 1 ] = 0 ; /* no PM yet */ <nl> 
static int __rfcomm_dlc_close ( struct rfcomm_dlc * d , int err ) <nl> rfcomm_dlc_unlock ( d ); <nl>  <nl> skb_queue_purge (& d -> tx_queue ); <nl> - rfcomm_session_put ( s ); <nl> - <nl> rfcomm_dlc_unlink ( d ); <nl> } <nl>  <nl> static struct rfcomm_session * rfcomm_session_create ( bdaddr_t * src , bdaddr_t * dst <nl> goto failed ; <nl> } <nl>  <nl> - rfcomm_session_hold ( s ); <nl> - <nl> s -> initiator = 1 ; <nl>  <nl> bacpy (& addr . l2_bdaddr , dst );
struct kvm_vcpu * kvm_arch_vcpu_create ( struct kvm * kvm , unsigned int id ) <nl> int err ; <nl> struct kvm_vcpu * vcpu ; <nl>  <nl> + if ( irqchip_in_kernel ( kvm ) && vgic_initialized ( kvm )) { <nl> + err = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> vcpu = kmem_cache_zalloc ( kvm_vcpu_cache , GFP_KERNEL ); <nl> if (! vcpu ) { <nl> err = - ENOMEM ;
static ssize_t dfs_file_write ( struct file * file , const char __user * user_buf , <nl> } else if ( dent == d -> dfs_emulate_power_cut ) { <nl> if ( kstrtoint ( buf , 0 , & val ) != 0 ) <nl> count = - EINVAL ; <nl> - d -> emulate_power_cut = val ; <nl> + else <nl> + d -> emulate_power_cut = val ; <nl> goto out ; <nl> } <nl> 
cy_ioctl ( struct tty_struct * tty , <nl> break ; <nl> # ifndef CONFIG_CYZ_INTR <nl> case CYZSETPOLLCYCLE : <nl> + if ( arg > LONG_MAX / HZ ) <nl> + return - ENODEV ; <nl> cyz_polling_cycle = ( arg * HZ ) / 1000 ; <nl> break ; <nl> case CYZGETPOLLCYCLE :
static int em28xx_i2c_xfer ( struct i2c_adapter * i2c_adap , <nl> if ( dev -> disconnected ) <nl> return - ENODEV ; <nl>  <nl> - rc = rt_mutex_trylock (& dev -> i2c_bus_lock ); <nl> - if ( rc < 0 ) <nl> - return rc ; <nl> + if (! rt_mutex_trylock (& dev -> i2c_bus_lock )) <nl> + return - EAGAIN ; <nl>  <nl> /* Switch I2C bus if needed */ <nl> if ( bus != dev -> cur_i2c_bus &&
EXPORT_SYMBOL_GPL ( omap_dm_timer_enable ); <nl>  <nl> void omap_dm_timer_disable ( struct omap_dm_timer * timer ) <nl> { <nl> - pm_runtime_put (& timer -> pdev -> dev ); <nl> + pm_runtime_put_sync (& timer -> pdev -> dev ); <nl> } <nl> EXPORT_SYMBOL_GPL ( omap_dm_timer_disable ); <nl> 
static void free_sa_defrag_extent ( struct new_sa_defrag_extent * new ) <nl> return ; <nl>  <nl> list_for_each_entry_safe ( old , tmp , & new -> head , list ) { <nl> - list_del (& old -> list ); <nl> kfree ( old ); <nl> } <nl> kfree ( new );
static int load_twl4030_script ( struct twl4030_script * tscript , <nl> goto out ; <nl> } <nl> if ( tscript -> flags & TWL4030_WAKEUP12_SCRIPT ) { <nl> + /* Reset any existing sleep script to avoid hangs on reboot */ <nl> + err = twl_i2c_write_u8 ( TWL_MODULE_PM_MASTER , END_OF_SCRIPT , <nl> + R_SEQ_ADD_A2S ); <nl> + if ( err ) <nl> + goto out ; <nl> + <nl> err = twl4030_config_wakeup12_sequence ( address ); <nl> if ( err ) <nl> goto out ;
static pci_ers_result_t atl1c_io_error_detected ( struct pci_dev * pdev , <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + if ( state == pci_channel_io_perm_failure ) <nl> + return PCI_ERS_RESULT_DISCONNECT ; <nl> + <nl> if ( netif_running ( netdev )) <nl> atl1c_down ( adapter ); <nl> 
lpfc_els_retry ( struct lpfc_hba * phba , struct lpfc_iocbq * cmdiocb , <nl> /* FLOGI retry policy */ <nl> retry = 1 ; <nl> /* retry FLOGI forever */ <nl> - maxretry = 0 ; <nl> + if ( phba -> link_flag != LS_LOOPBACK_MODE ) <nl> + maxretry = 0 ; <nl> + else <nl> + maxretry = 2 ; <nl> + <nl> if ( cmdiocb -> retry >= 100 ) <nl> delay = 5000 ; <nl> else if ( cmdiocb -> retry >= 32 )
static bool gfar_add_rx_frag ( struct gfar_rx_buff * rxb , u32 lstatus , <nl> } <nl>  <nl> /* try reuse page */ <nl> - if ( unlikely ( page_count ( page ) != 1 )) <nl> + if ( unlikely ( page_count ( page ) != 1 || page_is_pfmemalloc ( page ))) <nl> return false ; <nl>  <nl> /* change offset to the other half */
static void tilcdc_crtc_destroy ( struct drm_crtc * crtc ) <nl> struct tilcdc_crtc * tilcdc_crtc = to_tilcdc_crtc ( crtc ); <nl> struct tilcdc_drm_private * priv = crtc -> dev -> dev_private ; <nl>  <nl> + drm_modeset_lock_crtc ( crtc , NULL ); <nl> tilcdc_crtc_disable ( crtc ); <nl> + drm_modeset_unlock_crtc ( crtc ); <nl>  <nl> flush_workqueue ( priv -> wq ); <nl> 
static struct sk_buff * fill_packet_ipv4 ( struct net_device * odev , <nl> /* Eth + IPh + UDPh + mpls */ <nl> datalen = pkt_dev -> cur_pkt_size - 14 - 20 - 8 - <nl> pkt_dev -> pkt_overhead ; <nl> - if ( datalen < sizeof ( struct pktgen_hdr )) <nl> + if ( datalen < 0 || datalen < sizeof ( struct pktgen_hdr )) <nl> datalen = sizeof ( struct pktgen_hdr ); <nl>  <nl> udph -> source = htons ( pkt_dev -> cur_udp_src );
int __init acpi_parse_mcfg ( unsigned long phys_addr , unsigned long size ) <nl> if ( mcfg -> config [ i ]. base_reserved ) { <nl> printk ( KERN_ERR PREFIX <nl> " MMCONFIG not in low 4GB of memory \ n "); <nl> + kfree ( pci_mmcfg_config ); <nl> + pci_mmcfg_config_num = 0 ; <nl> return - ENODEV ; <nl> } <nl> }
static int jr3_pci_auto_attach ( struct comedi_device * dev , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if ( pci_resource_len ( pcidev , 0 ) < board -> n_subdevs * sizeof (* block )) <nl> + return - ENXIO ; <nl> + <nl> dev -> mmio = pci_ioremap_bar ( pcidev , 0 ); <nl> if (! dev -> mmio ) <nl> return - ENOMEM ;
static int create_device ( struct ramzswap * rzs , int device_id ) <nl> * or set equal to backing swap device ( if provided ) <nl> */ <nl> set_capacity ( rzs -> disk , 0 ); <nl> + <nl> + blk_queue_physical_block_size ( rzs -> disk -> queue , PAGE_SIZE ); <nl> + blk_queue_logical_block_size ( rzs -> disk -> queue , PAGE_SIZE ); <nl> + <nl> add_disk ( rzs -> disk ); <nl>  <nl> rzs -> init_done = 0 ;
static struct platform_driver snvs_rtc_driver = { <nl> . name = " snvs_rtc ", <nl> . owner = THIS_MODULE , <nl> . pm = & snvs_rtc_pm_ops , <nl> - . of_match_table = of_match_ptr ( snvs_dt_ids ), <nl> + . of_match_table = snvs_dt_ids , <nl> }, <nl> . probe = snvs_rtc_probe , <nl> };
static int slic_mcast_add_list ( struct adapter * adapter , char * address ) <nl> } <nl>  <nl> /* Doesn ' t already exist . Allocate a structure to hold it */ <nl> - mcaddr = kmalloc ( sizeof ( struct mcast_address ), GFP_ATOMIC ); <nl> + mcaddr = kmalloc ( sizeof (* mcaddr ), GFP_ATOMIC ); <nl> if ( mcaddr == NULL ) <nl> return 1 ; <nl> 
int dns_query ( const char * type , const char * name , size_t namelen , <nl> if (!* _result ) <nl> goto put ; <nl>  <nl> - memcpy (* _result , upayload -> data , len + 1 ); <nl> + memcpy (* _result , upayload -> data , len ); <nl> + * _result [ len ] = '\ 0 '; <nl> + <nl> if ( _expiry ) <nl> * _expiry = rkey -> expiry ; <nl> 
static void broxton_phy_init ( struct drm_i915_private * dev_priv , <nl> DRM_DEBUG_DRIVER (" DDI PHY % d already enabled , " <nl> " won ' t reprogram it \ n ", phy ); <nl> /* Still read out the GRC value for state verification */ <nl> - if ( phy == DPIO_PHY1 ) <nl> + if ( phy == DPIO_PHY0 ) <nl> dev_priv -> bxt_phy_grc = broxton_get_grc ( dev_priv , phy ); <nl>  <nl> return ;
static int evm_protect_xattr ( struct dentry * dentry , const char * xattr_name , <nl> goto out ; <nl> } <nl> evm_status = evm_verify_current_integrity ( dentry ); <nl> + if ( evm_status == INTEGRITY_NOXATTRS ) { <nl> + struct integrity_iint_cache * iint ; <nl> + <nl> + iint = integrity_iint_find ( dentry -> d_inode ); <nl> + if ( iint && ( iint -> flags & IMA_NEW_FILE )) <nl> + return 0 ; <nl> + } <nl> out : <nl> if ( evm_status != INTEGRITY_PASS ) <nl> integrity_audit_msg ( AUDIT_INTEGRITY_METADATA , dentry -> d_inode ,
static struct hv_pci_dev * new_pcichild_device ( struct hv_pcibus_device * hbus , <nl> struct hv_pci_dev * hpdev ; <nl> struct pci_child_message * res_req ; <nl> struct q_res_req_compl comp_pkt ; <nl> - union { <nl> - struct pci_packet init_packet ; <nl> - u8 buffer [ 0x100 ]; <nl> + struct { <nl> + struct pci_packet init_packet ; <nl> + u8 buffer [ sizeof ( struct pci_child_message )]; <nl> } pkt ; <nl> unsigned long flags ; <nl> int ret ;
static int i2s_pll_clk_probe ( struct platform_device * pdev ) <nl> if ( IS_ERR ( pll_clk -> base )) <nl> return PTR_ERR ( pll_clk -> base ); <nl>  <nl> + memset (& init , 0 , sizeof ( init )); <nl> clk_name = node -> name ; <nl> init . name = clk_name ; <nl> init . ops = & i2s_pll_ops ;
static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x23 , { KEY_PROG3 } }, /* P_Key */ <nl> { KE_KEY , 0x24 , { KEY_PROG4 } }, /* Social networking_Key */ <nl> { KE_KEY , 0x64 , { KEY_SWITCHVIDEOMODE } }, /* Display Switch */ <nl> - { KE_KEY , 0x82 , { KEY_F22 } }, /* Touch Pad On / Off */ <nl> + { KE_KEY , 0x82 , { KEY_TOUCHPAD_TOGGLE } }, /* Touch Pad On / Off */ <nl> { KE_END , 0 } <nl> }; <nl> 
int symsrc__init ( struct symsrc * ss , struct dso * dso __maybe_unused , <nl> if (! ss -> name ) <nl> goto out_close ; <nl>  <nl> + ss -> fd = fd ; <nl> ss -> type = type ; <nl>  <nl> return 0 ;
retry : <nl> return UBI_IO_BITFLIPS ; <nl> } <nl>  <nl> - if ( read != len && retries ++ < UBI_IO_RETRIES ) { <nl> + if ( retries ++ < UBI_IO_RETRIES ) { <nl> dbg_io (" error % d % s while reading % d bytes from PEB % d :% d ," <nl> " read only % zd bytes , retry ", <nl> err , errstr , len , pnum , offset , read );
static ssize_t iio_ev_value_store ( struct device * dev , <nl> unsigned long val ; <nl> int ret ; <nl>  <nl> + if (! indio_dev -> info -> write_event_value ) <nl> + return - EINVAL ; <nl> + <nl> ret = strict_strtoul ( buf , 10 , & val ); <nl> if ( ret ) <nl> return ret ;
cifs_set_file_info ( struct inode * inode , struct iattr * attrs , int xid , <nl> struct cifsTconInfo * pTcon = cifs_sb -> tcon ; <nl> FILE_BASIC_INFO info_buf ; <nl>  <nl> + if ( attrs == NULL ) <nl> + return - EINVAL ; <nl> + <nl> if ( attrs -> ia_valid & ATTR_ATIME ) { <nl> set_time = true ; <nl> info_buf . LastAccessTime =
static void EChannel_proc_rcv ( struct hisax_d_if * d_if ) <nl> # ifdef CONFIG_PCI <nl> # include < linux / pci . h > <nl>  <nl> - static struct pci_device_id hisax_pci_tbl [] __devinitdata = { <nl> + static struct pci_device_id hisax_pci_tbl [] __devinitdata __used = { <nl> # ifdef CONFIG_HISAX_FRITZPCI <nl> { PCI_VDEVICE ( AVM , PCI_DEVICE_ID_AVM_A1 ) }, <nl> # endif
static void frontend_init ( struct dvb_bt8xx_card * card , u32 type ) <nl> /* DST is not a frontend , attaching the ASIC */ <nl> if ( dvb_attach ( dst_attach , state , & card -> dvb_adapter ) == NULL ) { <nl> pr_err ("% s : Could not find a Twinhan DST \ n ", __func__ ); <nl> + kfree ( state ); <nl> break ; <nl> } <nl> /* Attach other DST peripherals if any */
static void logfs_put_super ( struct super_block * sb ) <nl> { <nl> struct logfs_super * super = logfs_super ( sb ); <nl> /* kill the meta - inodes */ <nl> - iput ( super -> s_master_inode ); <nl> iput ( super -> s_segfile_inode ); <nl> + iput ( super -> s_master_inode ); <nl> iput ( super -> s_mapping_inode ); <nl> } <nl> 
static int si_common_early_init ( void * handle ) <nl> AMD_CG_SUPPORT_HDP_LS | <nl> AMD_CG_SUPPORT_HDP_MGCG ; <nl> adev -> pg_flags = 0 ; <nl> + adev -> external_rev_id = 60 ; <nl> break ; <nl> case CHIP_HAINAN : <nl> adev -> cg_flags =
static int wm8731_register ( struct wm8731_priv * wm8731 ) <nl>  <nl> memcpy ( codec -> reg_cache , wm8731_reg , sizeof ( wm8731_reg )); <nl>  <nl> + ret = wm8731_reset ( codec ); <nl> + if ( ret < 0 ) { <nl> + dev_err ( codec -> dev , " Failed to issue reset \ n "); <nl> + return ret ; <nl> + } <nl> + <nl> wm8731_dai . dev = codec -> dev ; <nl>  <nl> - wm8731_reset ( codec ); <nl> wm8731_set_bias_level ( codec , SND_SOC_BIAS_STANDBY ); <nl>  <nl> /* Latch the update bits */
int mwifiex_bss_start ( struct mwifiex_private * priv , struct cfg80211_bss * bss , <nl> mwifiex_dbg ( adapter , ERROR , <nl> " Attempt to reconnect on csa closed chan (% d )\ n ", <nl> bss_desc -> channel ); <nl> + ret = - 1 ; <nl> goto done ; <nl> } <nl> 
out_destroy : <nl> out_free : <nl> pr_info ("% s : failed to register PMU devices !\ n ", <nl> of_node_full_name ( node )); <nl> + kfree ( pmu -> irq_affinity ); <nl> kfree ( pmu ); <nl> return ret ; <nl> }
int vty_init ( const struct file_operations * console_fops ); <nl>  <nl> static inline bool vt_force_oops_output ( struct vc_data * vc ) <nl> { <nl> - if ( oops_in_progress && vc -> vc_panic_force_write ) <nl> + if ( oops_in_progress && vc -> vc_panic_force_write && panic_timeout >= 0 ) <nl> return true ; <nl> return false ; <nl> }
int eeh_dn_check_failure ( struct device_node * dn , struct pci_dev * dev ) <nl> no_dn ++; <nl> return 0 ; <nl> } <nl> + dn = find_device_pe ( dn ); <nl> pdn = PCI_DN ( dn ); <nl>  <nl> /* Access to IO BARs might get this far and still not want checking . */
struct dvb_frontend * dib9000_attach ( struct i2c_adapter * i2c_adap , u8 i2c_addr , c <nl> if ( st == NULL ) <nl> return NULL ; <nl> fe = kzalloc ( sizeof ( struct dvb_frontend ), GFP_KERNEL ); <nl> - if ( fe == NULL ) <nl> + if ( fe == NULL ) { <nl> + kfree ( st ); <nl> return NULL ; <nl> + } <nl>  <nl> memcpy (& st -> chip . d9 . cfg , cfg , sizeof ( struct dib9000_config )); <nl> st -> i2c . i2c_adap = i2c_adap ;
static int assign_guest_irq ( struct kvm * kvm , <nl> dev -> irq_requested_type |= guest_irq_type ; <nl> if ( dev -> ack_notifier . gsi != - 1 ) <nl> kvm_register_irq_ack_notifier ( kvm , & dev -> ack_notifier ); <nl> - } else <nl> + } else { <nl> kvm_free_irq_source_id ( kvm , dev -> irq_source_id ); <nl> + dev -> irq_source_id = - 1 ; <nl> + } <nl>  <nl> return r ; <nl> }
static int get_sig_strength ( struct drx_demod_instance * demod , u16 * sig_strength ) <nl> * sig_strength = ( 20 * if_gain / if_agc_sns ); <nl> } <nl>  <nl> + if (* sig_strength <= 7 ) <nl> + * sig_strength = 0 ; <nl> + <nl> return 0 ; <nl> rw_error : <nl> return - EIO ;
void snd_trident_write_voice_regs ( trident_t * trident , <nl> break ; <nl> default : <nl> snd_BUG (); <nl> + return ; <nl> } <nl>  <nl> outb ( voice -> number , TRID_REG ( trident , T4D_LFO_GC_CIR ));
unsigned int ata_sff_qc_issue ( struct ata_queued_cmd * qc ) <nl> break ; <nl>  <nl> default : <nl> - WARN_ON_ONCE ( 1 ); <nl> return AC_ERR_SYSTEM ; <nl> } <nl> 
static void cyberjack_read_int_callback ( struct urb * urb ) <nl>  <nl> old_rdtodo = priv -> rdtodo ; <nl>  <nl> - if ( old_rdtodo + size < old_rdtodo ) { <nl> + if ( old_rdtodo > SHRT_MAX - size ) { <nl> dev_dbg ( dev , " To many bulk_in urbs to do .\ n "); <nl> spin_unlock (& priv -> lock ); <nl> goto resubmit ;
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> - free ( ptr ); <nl> + kmem_free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
struct wm8994_ldo_pdata { <nl> int enable ; <nl>  <nl> const char * supply ; <nl> - struct regulator_init_data * init_data ; <nl> + const struct regulator_init_data * init_data ; <nl> }; <nl>  <nl> # define WM8994_CONFIGURE_GPIO 0x10000
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> if (!( src_file -> f_mode & FMODE_READ )) <nl> goto out_fput ; <nl>  <nl> + /* don ' t make the dst file partly checksummed */ <nl> + if (( BTRFS_I ( src )-> flags & BTRFS_INODE_NODATASUM ) != <nl> + ( BTRFS_I ( inode )-> flags & BTRFS_INODE_NODATASUM )) <nl> + goto out_fput ; <nl> + <nl> ret = - EISDIR ; <nl> if ( S_ISDIR ( src -> i_mode ) || S_ISDIR ( inode -> i_mode )) <nl> goto out_fput ;
static int get_cac_tdp_table ( <nl>  <nl> hwmgr -> dyn_state . cac_dtp_table = kzalloc ( table_size , GFP_KERNEL ); <nl>  <nl> - if ( NULL == hwmgr -> dyn_state . cac_dtp_table ) <nl> + if ( NULL == hwmgr -> dyn_state . cac_dtp_table ) { <nl> + kfree ( tdp_table ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> memset ( hwmgr -> dyn_state . cac_dtp_table , 0x00 , table_size ); <nl> 
void r8712_joinbss_event_callback ( struct _adapter * adapter , u8 * pbuf ) <nl>  <nl> if ( sizeof ( struct list_head ) == 4 * sizeof ( u32 )) { <nl> pnetwork = kmalloc ( sizeof ( struct wlan_network ), GFP_ATOMIC ); <nl> + if (! pnetwork ) <nl> + return ; <nl> memcpy (( u8 *) pnetwork + 16 , ( u8 *) pbuf + 8 , <nl> sizeof ( struct wlan_network ) - 16 ); <nl> } else
static void do_ubd_request ( struct request_queue * q ) <nl> " errno = % d \ n ", - n ); <nl> else if ( list_empty (& dev -> restart )) <nl> list_add (& dev -> restart , & restart ); <nl> + kfree ( io_req ); <nl> return ; <nl> } <nl> 
static int smsc95xx_suspend ( struct usb_interface * intf , pm_message_t message ) <nl> ret = smsc95xx_enter_suspend0 ( dev ); <nl>  <nl> done : <nl> - if ( ret ) <nl> + /* <nl> + * TODO : resume () might need to handle the suspend failure <nl> + * in system sleep <nl> + */ <nl> + if ( ret && PMSG_IS_AUTO ( message )) <nl> usbnet_resume ( intf ); <nl> return ret ; <nl> }
static int imx_thermal_probe ( struct platform_device * pdev ) <nl> data -> tempmon = map ; <nl>  <nl> data -> socdata = of_device_get_match_data (& pdev -> dev ); <nl> + if (! data -> socdata ) { <nl> + dev_err (& pdev -> dev , " no device match found \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> /* make sure the IRQ flag is clear before enabling irq on i . MX6SX */ <nl> if ( data -> socdata -> version == TEMPMON_IMX6SX ) {
static void sci_io_request_build_ssp_command_iu ( struct isci_request * ireq ) <nl> cmd_iu -> _r_c = 0 ; <nl>  <nl> sci_swab32_cpy (& cmd_iu -> cdb , task -> ssp_task . cmd -> cmnd , <nl> - task -> ssp_task . cmd -> cmd_len / sizeof ( u32 )); <nl> + ( task -> ssp_task . cmd -> cmd_len + 3 ) / sizeof ( u32 )); <nl> } <nl>  <nl> static void sci_task_request_build_ssp_task_iu ( struct isci_request * ireq )
static void kvmppc_fast_vcpu_kick_hv ( struct kvm_vcpu * vcpu ) <nl> ++ vcpu -> stat . halt_wakeup ; <nl> } <nl>  <nl> - if ( kvmppc_ipi_thread ( vcpu -> arch . thread_cpu )) <nl> + cpu = READ_ONCE ( vcpu -> arch . thread_cpu ); <nl> + if ( cpu >= 0 && kvmppc_ipi_thread ( cpu )) <nl> return ; <nl>  <nl> /* CPU points to the first thread of the core */
# define DOC_ECCCONF1 0x1042 <nl> # define DOC_ECCPRESET 0x1044 <nl> # define DOC_HAMMINGPARITY 0x1046 <nl> -# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 1 )) <nl> +# define DOC_BCH_SYNDROM ( idx ) ( 0x1048 + ( idx << 0 )) <nl>  <nl> # define DOC_PROTECTION 0x1056 <nl> # define DOC_DPS0_ADDRLOW 0x1060
int fsl_rio_setup_rmu ( struct rio_mport * mport , struct device_node * node ) <nl> if (! msg_addr ) { <nl> pr_err ("% s : unable to find ' reg ' property of message - unit \ n ", <nl> node -> full_name ); <nl> + kfree ( rmu ); <nl> return - ENOMEM ; <nl> } <nl> msg_start = of_read_number ( msg_addr , aw );
static int st33zp24_spi_evaluate_latency ( void * phy_id ) <nl> & data , 1 ); <nl> latency ++; <nl> } <nl> + if ( status < 0 ) <nl> + return status ; <nl> + if ( latency == MAX_SPI_LATENCY ) <nl> + return - ENODEV ; <nl> + <nl> return latency - 1 ; <nl> } /* evaluate_latency () */ <nl> 
static void snd_card_asihpi_timer_function ( unsigned long data ) <nl> s -> number ); <nl> ds -> drained_count ++; <nl> if ( ds -> drained_count > 20 ) { <nl> + unsigned long flags ; <nl> + snd_pcm_stream_lock_irqsave ( s , flags ); <nl> snd_pcm_stop ( s , SNDRV_PCM_STATE_XRUN ); <nl> + snd_pcm_stream_unlock_irqrestore ( s , flags ); <nl> continue ; <nl> } <nl> } else {
static ide_startstop_t cdrom_do_block_pc ( ide_drive_t * drive , struct request * rq ) <nl> /* <nl> * check if dma is safe <nl> */ <nl> - if (( rq -> data_len & mask ) || ( addr & mask )) <nl> + if (( rq -> data_len & 3 ) || ( addr & mask )) <nl> info -> dma = 0 ; <nl> } <nl> 
int kvm_dev_ioctl_check_extension ( long ext ) <nl> case KVM_CAP_SYNC_REGS : <nl> r = 1 ; <nl> break ; <nl> + case KVM_CAP_NR_VCPUS : <nl> + case KVM_CAP_MAX_VCPUS : <nl> + r = KVM_MAX_VCPUS ; <nl> + break ; <nl> default : <nl> r = 0 ; <nl> }
static void proc_flush_task_mnt ( struct vfsmount * mnt , pid_t pid , pid_t tgid ) <nl> dput ( dentry ); <nl> } <nl>  <nl> + if ( pid == tgid ) <nl> + return ; <nl> + <nl> name . name = buf ; <nl> name . len = snprintf ( buf , sizeof ( buf ), "% d ", tgid ); <nl> leader = d_hash_and_lookup ( mnt -> mnt_root , & name );
int gpiochip_add_data ( struct gpio_chip * chip , void * data ) <nl> * First : allocate and populate the internal stat container , and <nl> * set up the struct device . <nl> */ <nl> - gdev = kmalloc ( sizeof (* gdev ), GFP_KERNEL ); <nl> + gdev = kzalloc ( sizeof (* gdev ), GFP_KERNEL ); <nl> if (! gdev ) <nl> return - ENOMEM ; <nl> gdev -> dev . bus = & gpio_bus_type ;
static void lkdtm_do_action ( enum ctype which ) <nl> break ; <nl>  <nl> val = kmalloc ( 1024 , GFP_KERNEL ); <nl> - if (! val ) <nl> + if (! val ) { <nl> + free_page ( p ); <nl> break ; <nl> + } <nl>  <nl> base = ( int *) p ; <nl> 
union acpi_parse_object ; <nl>  <nl> static char * acpi_gbl_mutex_names [ ACPI_NUM_MUTEX ] = { <nl> " ACPI_MTX_Interpreter ", <nl> - " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Namespace ", <nl> + " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Events ", <nl> " ACPI_MTX_Caches ", <nl> " ACPI_MTX_Memory ",
static inline void task_state ( struct seq_file * m , struct pid_namespace * ns , <nl> " FDSize :\ t % d \ n " <nl> " Groups :\ t ", <nl> fdt ? fdt -> max_fds : 0 ); <nl> + task_unlock ( p ); <nl> rcu_read_unlock (); <nl>  <nl> group_info = cred -> group_info ; <nl> - task_unlock ( p ); <nl> - <nl> for ( g = 0 ; g < group_info -> ngroups ; g ++) <nl> seq_printf ( m , "% d ", <nl> from_kgid_munged ( user_ns , GROUP_AT ( group_info , g )));
u32 crypto4xx_build_pd ( struct crypto_async_request * req , <nl>  <nl> /* figure how many gd is needed */ <nl> num_gd = sg_nents_for_len ( src , datalen ); <nl> + if (( int ) num_gd < 0 ) { <nl> + dev_err ( dev -> core_dev -> device , " Invalid number of src SG .\ n "); <nl> + return - EINVAL ; <nl> + } <nl> if ( num_gd == 1 ) <nl> num_gd = 0 ; <nl> 
cont : <nl> PAGE_SET_WRITEBACK | <nl> page_error_op | <nl> PAGE_END_WRITEBACK ); <nl> - btrfs_free_reserved_data_space_noquota ( inode , start , <nl> - end - start + 1 ); <nl> + if ( ret == 0 ) <nl> + btrfs_free_reserved_data_space_noquota ( inode , <nl> + start , <nl> + end - start + 1 ); <nl> goto free_pages_out ; <nl> } <nl> }
static void virtcons_remove ( struct virtio_device * vdev ) <nl> /* Disable interrupts for vqs */ <nl> vdev -> config -> reset ( vdev ); <nl> /* Finish up work that ' s lined up */ <nl> - cancel_work_sync (& portdev -> control_work ); <nl> + if ( use_multiport ( portdev )) <nl> + cancel_work_sync (& portdev -> control_work ); <nl>  <nl> list_for_each_entry_safe ( port , port2 , & portdev -> ports , list ) <nl> unplug_port ( port );
static int tsi721_rio_map_inb_mem ( struct rio_mport * mport , dma_addr_t lstart , <nl> } else if ( ibw_start < ( ib_win -> rstart + ib_win -> size ) && <nl> ( ibw_start + ibw_size ) > ib_win -> rstart ) { <nl> /* Return error if address translation involved */ <nl> - if ( direct && ib_win -> xlat ) { <nl> + if (! direct || ib_win -> xlat ) { <nl> ret = - EFAULT ; <nl> break ; <nl> }
acpi_parse_lapic ( struct acpi_subtable_header * header , const unsigned long end ) <nl>  <nl> acpi_table_print_madt_entry ( header ); <nl>  <nl> + /* Ignore invalid ID */ <nl> + if ( processor -> id == 0xff ) <nl> + return 0 ; <nl> + <nl> /* <nl> * We need to register disabled CPU as well to permit <nl> * counting disabled CPUs . This allows us to size
static int et131x_phy_mii_read ( struct et131x_adapter * adapter , u8 addr , <nl> mii_indicator ); <nl>  <nl> status = - EIO ; <nl> + goto out ; <nl> } <nl>  <nl> /* If we hit here we were able to read the register and we need to <nl> static int et131x_phy_mii_read ( struct et131x_adapter * adapter , u8 addr , <nl> */ <nl> * value = readl (& mac -> mii_mgmt_stat ) & ET_MAC_MIIMGMT_STAT_PHYCRTL_MASK ; <nl>  <nl> + out : <nl> /* Stop the read operation */ <nl> writel ( 0 , & mac -> mii_mgmt_cmd ); <nl> 
static int fusbh200_hcd_fusbh200_probe ( struct platform_device * pdev ) <nl>  <nl> retval = fusbh200_setup ( hcd ); <nl> if ( retval ) <nl> - return retval ; <nl> + goto fail_add_hcd ; <nl>  <nl> fusbh200_init ( fusbh200 ); <nl> 
static void intel_hdmi_mode_set ( struct intel_encoder * encoder ) <nl> else <nl> hdmi_val |= SDVO_COLOR_FORMAT_8bpc ; <nl>  <nl> - /* Required on CPT */ <nl> - if ( intel_hdmi -> has_hdmi_sink && HAS_PCH_CPT ( dev )) <nl> + if ( intel_hdmi -> has_hdmi_sink && <nl> + ( HAS_PCH_CPT ( dev ) || IS_VALLEYVIEW ( dev ))) <nl> hdmi_val |= HDMI_MODE_SELECT_HDMI ; <nl>  <nl> if ( intel_hdmi -> has_audio ) {
static struct talitos_crypto_alg * talitos_alg_alloc ( struct device * dev , <nl> break ; <nl> default : <nl> dev_err ( dev , " unknown algorithm type % d \ n ", t_alg -> algt . type ); <nl> + kfree ( t_alg ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> 
static int uevent_net_init ( struct net * net ) <nl> if (! ue_sk -> sk ) { <nl> printk ( KERN_ERR <nl> " kobject_uevent : unable to create netlink socket !\ n "); <nl> + kfree ( ue_sk ); <nl> return - ENODEV ; <nl> } <nl> mutex_lock (& uevent_sock_mutex );
static int __devinit snd_gusextreme_probe ( struct device * dev , unsigned int n ) <nl> " detected at 0x % lx \ n ", dev -> bus_id , gus -> gf1 . port ); <nl> goto out ; <nl> } <nl> + gus -> codec_flag = 1 ; <nl>  <nl> error = snd_es1688_pcm ( es1688 , 0 , NULL ); <nl> if ( error < 0 )
xfs_find_handle ( <nl> int hsize ; <nl> xfs_handle_t handle ; <nl> struct inode * inode ; <nl> - struct fd f ; <nl> + struct fd f = { 0 }; <nl> struct path path ; <nl> int error ; <nl> struct xfs_inode * ip ;
static int nsp_cs_config ( struct pcmcia_device * link ) <nl>  <nl> nsp_dbg ( NSP_DEBUG_INIT , " in "); <nl>  <nl> - cfg_mem = kzalloc ( sizeof ( cfg_mem ), GFP_KERNEL ); <nl> + cfg_mem = kzalloc ( sizeof (* cfg_mem ), GFP_KERNEL ); <nl> if (! cfg_mem ) <nl> return - ENOMEM ; <nl> cfg_mem -> data = data ;
void pcmcia_disable_device ( struct pcmcia_device * p_dev ) { <nl> pcmcia_release_configuration ( p_dev ); <nl> pcmcia_release_io ( p_dev , & p_dev -> io ); <nl> pcmcia_release_irq ( p_dev , & p_dev -> irq ); <nl> - if (& p_dev -> win ) <nl> + if ( p_dev -> win ) <nl> pcmcia_release_window ( p_dev -> win ); <nl> } <nl> EXPORT_SYMBOL ( pcmcia_disable_device );
static int sep_register_driver_with_fs ( struct sep_device * sep ) <nl> if ( ret_val ) { <nl> dev_warn (& sep -> pdev -> dev , " sysfs attribute1 fails for SEP % x \ n ", <nl> ret_val ); <nl> + misc_deregister (& sep -> miscdev_sep ); <nl> return ret_val ; <nl> } <nl> 
static int palmas_i2c_probe ( struct i2c_client * i2c , <nl> ret = - ENOMEM ; <nl> goto err ; <nl> } <nl> + palmas -> i2c_clients [ i ]-> dev . of_node = of_node_get ( node ); <nl> } <nl> palmas -> regmap [ i ] = devm_regmap_init_i2c ( palmas -> i2c_clients [ i ], <nl> & palmas_regmap_config [ i ]);
# include < syslog . h > <nl> # endif <nl>  <nl> -# define S8 int8_t <nl> # define S16 int16_t <nl> # define S32 int32_t <nl> # define S64 int64_t
static int __init camellia_aesni_init ( void ) <nl> { <nl> const char * feature_name ; <nl>  <nl> + if (! cpu_has_avx || ! cpu_has_aes || ! cpu_has_osxsave ) { <nl> + pr_info (" AVX or AES - NI instructions are not detected .\ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> if (! cpu_has_xfeatures ( XSTATE_SSE | XSTATE_YMM , & feature_name )) { <nl> pr_info (" CPU feature '% s ' is not supported .\ n ", feature_name ); <nl> return - ENODEV ;
static void pci_acpi_cleanup ( struct device * dev ) <nl>  <nl> static bool pci_acpi_bus_match ( struct device * dev ) <nl> { <nl> - return dev -> bus == & pci_bus_type ; <nl> + return dev_is_pci ( dev ); <nl> } <nl>  <nl> static struct acpi_bus_type acpi_pci_bus = {
static int rt5645_irq_detection ( struct rt5645_priv * rt5645 ) <nl> { <nl> int val , btn_type , gpio_state = 0 , report = 0 ; <nl>  <nl> + if (! rt5645 -> codec ) <nl> + return - EINVAL ; <nl> + <nl> switch ( rt5645 -> pdata . jd_mode ) { <nl> case 0 : /* Not using rt5645 JD */ <nl> if ( rt5645 -> gpiod_hp_det ) {
static void blade_image_blit ( struct tridentfb_par * par , const char * data , <nl> writemmr ( par , DST1 , point ( x , y )); <nl> writemmr ( par , DST2 , point ( x + w - 1 , y + h - 1 )); <nl>  <nl> - memcpy ( par -> io_virt + 0x10000 , data , 4 * size ); <nl> + iowrite32_rep ( par -> io_virt + 0x10000 , data , size ); <nl> } <nl>  <nl> static void blade_copy_rect ( struct tridentfb_par * par ,
* XXX We need to find a better place for these things ... <nl> */ <nl> bool perf_host = true ; <nl> - bool perf_guest = true ; <nl> + bool perf_guest = false ; <nl>  <nl> void event_attr_init ( struct perf_event_attr * attr ) <nl> {
static void squashfs_put_super ( struct super_block * sb ) <nl> kfree ( sbi -> id_table ); <nl> kfree ( sbi -> fragment_index ); <nl> kfree ( sbi -> meta_index ); <nl> + kfree ( sbi -> inode_lookup_table ); <nl> kfree ( sb -> s_fs_info ); <nl> sb -> s_fs_info = NULL ; <nl> }
out_attach : <nl> static void ocfs2_drop_dentry_lock ( struct ocfs2_super * osb , <nl> struct ocfs2_dentry_lock * dl ) <nl> { <nl> + iput ( dl -> dl_inode ); <nl> ocfs2_simple_drop_lockres ( osb , & dl -> dl_lockres ); <nl> ocfs2_lock_res_free (& dl -> dl_lockres ); <nl> - iput ( dl -> dl_inode ); <nl> kfree ( dl ); <nl> } <nl> 
i915_gem_set_tiling ( struct drm_device * dev , void * data , <nl> } <nl>  <nl> mutex_lock (& dev -> struct_mutex ); <nl> - if ( i915_gem_obj_is_pinned ( obj ) || obj -> framebuffer_references ) { <nl> + if ( obj -> pin_display || obj -> framebuffer_references ) { <nl> ret = - EBUSY ; <nl> goto err ; <nl> }
static void v9fs_fd_close ( struct v9fs_transport * trans ) <nl> if (! trans ) <nl> return ; <nl>  <nl> - trans -> status = Disconnected ; <nl> - ts = trans -> priv ; <nl> + ts = xchg (& trans -> priv , NULL ); <nl>  <nl> if (! ts ) <nl> return ; <nl>  <nl> + trans -> status = Disconnected ; <nl> if ( ts -> in_file ) <nl> fput ( ts -> in_file ); <nl> 
_scsih_error_recovery_delete_devices ( struct MPT2SAS_ADAPTER * ioc ) <nl>  <nl> if ( ioc -> is_driver_loading ) <nl> return ; <nl> + <nl> + fw_event = kzalloc ( sizeof ( struct fw_event_work ), GFP_ATOMIC ); <nl> + if (! fw_event ) <nl> + return ; <nl> + <nl> fw_event -> event = MPT2SAS_REMOVE_UNRESPONDING_DEVICES ; <nl> fw_event -> ioc = ioc ; <nl> _scsih_fw_event_add ( ioc , fw_event );
static void vmw_fb_dirty_flush ( struct vmw_fb_par * par ) <nl> SVGAFifoCmdUpdate body ; <nl> } * cmd ; <nl>  <nl> + if ( vmw_priv -> suspended ) <nl> + return ; <nl> + <nl> spin_lock_irqsave (& par -> dirty . lock , flags ); <nl> if (! par -> dirty . active ) { <nl> spin_unlock_irqrestore (& par -> dirty . lock , flags );
static const struct pinmux_cfg_reg pinmux_config_regs [] = { <nl> FN_MSIOF0_SCK_B , 0 , <nl> /* IP5_23_21 [ 3 ] */ <nl> FN_WE1_N , FN_IERX , FN_CAN1_RX , FN_VI1_G4 , <nl> - FN_VI1_G4_B , FN_VI2_R6 , FN_SCIFA0_CTS_N_B , <nl> - FN_IERX_C , 0 , <nl> + FN_VI1_G4_B , FN_VI2_R6 , FN_SCIFA0_CTS_N_B , FN_IERX_C , <nl> /* IP5_20_18 [ 3 ] */ <nl> FN_WE0_N , FN_IECLK , FN_CAN_CLK , <nl> FN_VI2_VSYNC_N , FN_SCIFA0_TXD_B , FN_VI2_VSYNC_N_B , 0 , 0 ,
# define IRQ_PXA168_DDR_INT 26 <nl> # define IRQ_PXA168_UART1 27 <nl> # define IRQ_PXA168_UART2 28 <nl> +# define IRQ_PXA168_UART3 29 <nl> # define IRQ_PXA168_WDT 35 <nl> +# define IRQ_PXA168_MAIN_PMU 36 <nl> # define IRQ_PXA168_FRQ_CHANGE 38 <nl> # define IRQ_PXA168_SDH1 39 <nl> # define IRQ_PXA168_SDH2 40 <nl> # define IRQ_PXA168_USB2 51 <nl> # define IRQ_PXA168_AC97 57 <nl> # define IRQ_PXA168_TWSI1 58 <nl> -# define IRQ_PXA168_PMU 60 <nl> +# define IRQ_PXA168_AP_PMU 60 <nl> # define IRQ_PXA168_SM_INT 63 <nl>  <nl> /*
int mmap_min_addr_handler ( struct ctl_table * table , int write , <nl> { <nl> int ret ; <nl>  <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> + <nl> ret = proc_doulongvec_minmax ( table , write , buffer , lenp , ppos ); <nl>  <nl> update_mmap_min_addr ();
static ssize_t bonding_show_mii_status ( struct device * d , <nl> char * buf ) <nl> { <nl> struct bonding * bond = to_bond ( d ); <nl> + bool active = !! rcu_access_pointer ( bond -> curr_active_slave ); <nl>  <nl> - return sprintf ( buf , "% s \ n ", bond -> curr_active_slave ? " up " : " down "); <nl> + return sprintf ( buf , "% s \ n ", active ? " up " : " down "); <nl> } <nl> static DEVICE_ATTR ( mii_status , S_IRUGO , bonding_show_mii_status , NULL ); <nl> 
static int exynos_drm_crtc_mode_set_base ( struct drm_crtc * crtc , int x , int y , <nl>  <nl> DRM_DEBUG_KMS ("% s \ n ", __FILE__ ); <nl>  <nl> + /* when framebuffer changing is requested , crtc ' s dpms should be on */ <nl> + if ( exynos_crtc -> dpms > DRM_MODE_DPMS_ON ) { <nl> + DRM_ERROR (" failed framebuffer changing request .\ n "); <nl> + return - EPERM ; <nl> + } <nl> + <nl> crtc_w = crtc -> fb -> width - x ; <nl> crtc_h = crtc -> fb -> height - y ; <nl> 
static int dmx_section_feed_release_filter ( struct dmx_section_feed * feed , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( feed -> is_filtering ) <nl> + if ( feed -> is_filtering ) { <nl> + /* release dvbdmx -> mutex as far as <nl> + it is acquired by stop_filtering () itself */ <nl> + mutex_unlock (& dvbdmx -> mutex ); <nl> feed -> stop_filtering ( feed ); <nl> + mutex_lock (& dvbdmx -> mutex ); <nl> + } <nl>  <nl> spin_lock_irq (& dvbdmx -> lock ); <nl> f = dvbdmxfeed -> filter ;
int savagefb_probe_i2c_connector ( struct fb_info * info , u8 ** out_edid ) <nl> } <nl> } <nl>  <nl> - if ( out_edid ) <nl> - * out_edid = edid ; <nl> + * out_edid = edid ; <nl>  <nl> return ( edid ) ? 0 : 1 ; <nl> }
static void ironlake_enable_pch_transcoder ( struct drm_i915_private * dev_priv , <nl> val |= TRANS_PROGRESSIVE ; <nl>  <nl> I915_WRITE ( reg , val | TRANS_ENABLE ); <nl> - if ( wait_for ( I915_READ ( reg ) & TRANS_STATE_ENABLE , 100 )) <nl> + if ( intel_wait_for_register ( dev_priv , <nl> + reg , TRANS_STATE_ENABLE , TRANS_STATE_ENABLE , <nl> + 100 )) <nl> DRM_ERROR (" failed to enable transcoder % c \ n ", pipe_name ( pipe )); <nl> } <nl> 
static struct inotify_watch * create_watch ( struct inotify_device * dev , <nl> return ERR_PTR ( ret ); <nl> } <nl>  <nl> - dev -> last_wd = ret ; <nl> + dev -> last_wd = watch -> wd ; <nl> watch -> mask = mask ; <nl> atomic_set (& watch -> count , 0 ); <nl> INIT_LIST_HEAD (& watch -> d_list );
intel_dp_check_link_status ( struct intel_dp * intel_dp ) <nl> if (! to_intel_crtc ( intel_encoder -> base . crtc )-> active ) <nl> return ; <nl>  <nl> + /* FIXME : we need to synchronize this sort of stuff with hardware <nl> + * readout */ <nl> + if ( WARN_ON_ONCE (! intel_dp -> lane_count )) <nl> + return ; <nl> + <nl> /* if link training is requested we should perform it always */ <nl> if (( intel_dp -> compliance_test_type == DP_TEST_LINK_TRAINING ) || <nl> (! drm_dp_channel_eq_ok ( link_status , intel_dp -> lane_count ))) {
void <nl> trace_printk_seq ( struct trace_seq * s ) <nl> { <nl> /* Probably should print a warning here . */ <nl> - if ( s -> len >= 1000 ) <nl> - s -> len = 1000 ; <nl> + if ( s -> len >= TRACE_MAX_PRINT ) <nl> + s -> len = TRACE_MAX_PRINT ; <nl>  <nl> /* should be zero ended , but we are paranoid . */ <nl> s -> buffer [ s -> len ] = 0 ;
static void alc283_shutup ( struct hda_codec * codec ) <nl>  <nl> alc_write_coef_idx ( codec , 0x43 , 0x9004 ); <nl>  <nl> + /* depop hp during suspend */ <nl> + alc_write_coef_idx ( codec , 0x06 , 0x2100 ); <nl> + <nl> snd_hda_codec_write ( codec , hp_pin , 0 , <nl> AC_VERB_SET_AMP_GAIN_MUTE , AMP_OUT_MUTE ); <nl> 
static struct iwl_power_vec_entry range_2 [ IWL_POWER_MAX ] = { <nl> /* set card power command */ <nl> static int iwl_set_power ( struct iwl_priv * priv , void * cmd ) <nl> { <nl> - return iwl_send_cmd_pdu_async ( priv , POWER_TABLE_CMD , <nl> - sizeof ( struct iwl_powertable_cmd ), <nl> - cmd , NULL ); <nl> + return iwl_send_cmd_pdu ( priv , POWER_TABLE_CMD , <nl> + sizeof ( struct iwl_powertable_cmd ), cmd ); <nl> } <nl> /* decide the right power level according to association status <nl> * and battery status
static int ata_bus_probe ( struct ata_port * ap ) <nl>  <nl> /* reset */ <nl> if ( ap -> ops -> probe_reset ) { <nl> + for ( i = 0 ; i < ATA_MAX_DEVICES ; i ++) <nl> + classes [ i ] = ATA_DEV_UNKNOWN ; <nl> + <nl> rc = ap -> ops -> probe_reset ( ap , classes ); <nl> if ( rc ) { <nl> printk (" ata % u : reset failed ( errno =% d )\ n ", ap -> id , rc );
int rsnd_dai_connect ( struct rsnd_mod * mod , <nl> if (! mod ) <nl> return - EIO ; <nl>  <nl> + if ( io -> mod [ type ]) <nl> + return - EINVAL ; <nl> + <nl> priv = rsnd_mod_to_priv ( mod ); <nl> dev = rsnd_priv_to_dev ( priv ); <nl> 
static u64 tg_prfill_cpu_rwstat ( struct seq_file * sf , <nl> struct blkg_rwstat rwstat = { }, tmp ; <nl> int i , cpu ; <nl>  <nl> + if ( tg -> stats_cpu == NULL ) <nl> + return 0 ; <nl> + <nl> for_each_possible_cpu ( cpu ) { <nl> struct tg_stats_cpu * sc = per_cpu_ptr ( tg -> stats_cpu , cpu ); <nl> 
static inline int phy_set_mode ( struct phy * phy , enum phy_mode mode ) <nl> return - ENOSYS ; <nl> } <nl>  <nl> + static inline int phy_reset ( struct phy * phy ) <nl> +{ <nl> + if (! phy ) <nl> + return 0 ; <nl> + return - ENOSYS ; <nl> +} <nl> + <nl> static inline int phy_get_bus_width ( struct phy * phy ) <nl> { <nl> return - ENOSYS ;
unmap_intr_base : <nl> iounmap ( priv -> avs_intr_base ); <nl> unmap_base : <nl> iounmap ( priv -> base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return ret ; <nl> } <nl> static int brcm_avs_cpufreq_remove ( struct platform_device * pdev ) <nl> priv = platform_get_drvdata ( pdev ); <nl> iounmap ( priv -> base ); <nl> iounmap ( priv -> avs_intr_base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return 0 ; <nl> }
static void rt6_free_pcpu ( struct rt6_info * non_pcpu_rt ) <nl> } <nl> } <nl>  <nl> + free_percpu ( non_pcpu_rt -> rt6i_pcpu ); <nl> non_pcpu_rt -> rt6i_pcpu = NULL ; <nl> } <nl> 
static int __devinit fealnx_init_one ( struct pci_dev * pdev , <nl> if ( np -> flags == HAS_MII_XCVR ) { <nl> int phy , phy_idx = 0 ; <nl>  <nl> - for ( phy = 1 ; phy < 32 && phy_idx < 4 ; phy ++) { <nl> + for ( phy = 1 ; phy < 32 && phy_idx < ARRAY_SIZE ( np -> phys ); <nl> + phy ++) { <nl> int mii_status = mdio_read ( dev , phy , 1 ); <nl>  <nl> if ( mii_status != 0xffff && mii_status != 0x0000 ) {
int mac802154_llsec_key_del ( struct mac802154_llsec * sec , <nl> mkey = container_of ( pos -> key , struct mac802154_llsec_key , key ); <nl>  <nl> if ( llsec_key_id_equal (& pos -> id , key )) { <nl> + list_del_rcu (& pos -> list ); <nl> llsec_key_put ( mkey ); <nl> return 0 ; <nl> }
static unsigned int get_max_cost ( struct f2fs_sb_info * sbi , <nl> if ( p -> alloc_mode == SSR ) <nl> return sbi -> blocks_per_seg ; <nl> if ( p -> gc_mode == GC_GREEDY ) <nl> - return sbi -> blocks_per_seg * p -> ofs_unit ; <nl> + return 2 * sbi -> blocks_per_seg * p -> ofs_unit ; <nl> else if ( p -> gc_mode == GC_CB ) <nl> return UINT_MAX ; <nl> else /* No other gc_mode */
static int send_reply ( struct svcxprt_rdma * rdma , <nl> " svcrdma : could not post a receive buffer , err =% d ." <nl> " Closing transport % p .\ n ", ret , rdma ); <nl> set_bit ( XPT_CLOSE , & rdma -> sc_xprt . xpt_flags ); <nl> - return 0 ; <nl> + svc_rdma_put_context ( ctxt , 0 ); <nl> + return - ENOTCONN ; <nl> } <nl>  <nl> /* Prepare the context */
static int TSS_authhmac ( unsigned char * digest , const unsigned char * key , <nl> if ( dlen == 0 ) <nl> break ; <nl> data = va_arg ( argp , unsigned char *); <nl> + if (! data ) { <nl> + ret = - EINVAL ; <nl> + va_end ( argp ); <nl> + goto out ; <nl> + } <nl> ret = crypto_shash_update (& sdesc -> shash , data , dlen ); <nl> if ( ret < 0 ) { <nl> va_end ( argp );
int free_stream_context ( struct intel_sst_drv * ctx , unsigned int str_id ) <nl> if ( ret ) <nl> sst_clean_stream (& ctx -> streams [ str_id ]); <nl> return ret ; <nl> + } else { <nl> + dev_err ( ctx -> dev , " we tried to free stream context % d which was freed !!!\ n ", str_id ); <nl> } <nl> return ret ; <nl> }
static int __inode_security_revalidate ( struct inode * inode , <nl>  <nl> might_sleep_if ( may_sleep ); <nl>  <nl> - if ( isec -> initialized != LABEL_INITIALIZED ) { <nl> + if ( ss_initialized && isec -> initialized != LABEL_INITIALIZED ) { <nl> if (! may_sleep ) <nl> return - ECHILD ; <nl> 
pte_t * huge_pte_offset ( struct mm_struct * mm , unsigned long addr ) <nl> pmd_t * pmd = NULL ; <nl>  <nl> pgd = pgd_offset ( mm , addr ); <nl> - pud = pud_offset ( pgd , addr ); <nl> - pmd = pmd_offset ( pud , addr ); <nl> + if ( pgd_present (* pgd )) { <nl> + pud = pud_offset ( pgd , addr ); <nl> + if ( pud_present (* pud )) <nl> + pmd = pmd_offset ( pud , addr ); <nl> + } <nl> return ( pte_t *) pmd ; <nl> } <nl> 
irq_handler ( int irq , void * device ) <nl>  <nl> pci_int_status = reg_read ( lynx , PCI_INT_STATUS ); <nl>  <nl> + if ( pci_int_status == ~ 0 ) <nl> + /* Card was ejected . */ <nl> + return IRQ_NONE ; <nl> + <nl> if (( pci_int_status & PCI_INT_INT_PEND ) == 0 ) <nl> /* Not our interrupt , bail out quickly . */ <nl> return IRQ_NONE ;
struct bcm2835_audio_instance { <nl> short peer_version ; <nl> }; <nl>  <nl> - bool force_bulk = false ; <nl> + static bool force_bulk ; <nl>  <nl> /* ---- Private Variables ---------------------------------------------------- */ <nl> 
static void gb_tty_set_termios ( struct tty_struct * tty , <nl>  <nl> if ( C_BAUD ( tty ) == B0 ) { <nl> newline . rate = gb_tty -> line_coding . rate ; <nl> - newctrl &= GB_UART_CTRL_DTR ; <nl> + newctrl &= ~ GB_UART_CTRL_DTR ; <nl> } else if ( termios_old && ( termios_old -> c_cflag & CBAUD ) == B0 ) { <nl> newctrl |= GB_UART_CTRL_DTR ; <nl> }
int mlx5_core_access_reg ( struct mlx5_core_dev * dev , void * data_in , <nl> in -> arg = cpu_to_be32 ( arg ); <nl> in -> register_id = cpu_to_be16 ( reg_num ); <nl> err = mlx5_cmd_exec ( dev , in , sizeof (* in ) + size_in , out , <nl> - sizeof ( out ) + size_out ); <nl> + sizeof (* out ) + size_out ); <nl> if ( err ) <nl> goto ex2 ; <nl> 
do_kern_mount ( const char * fstype , int flags , const char * name , void * data ) <nl> mnt -> mnt_parent = mnt ; <nl> mnt -> mnt_namespace = current -> namespace ; <nl> up_write (& sb -> s_umount ); <nl> + free_secdata ( secdata ); <nl> put_filesystem ( type ); <nl> return mnt ; <nl> out_sb :
static bool nested_vmx_exit_handled_msr ( struct kvm_vcpu * vcpu , <nl> u32 msr_index = vcpu -> arch . regs [ VCPU_REGS_RCX ]; <nl> gpa_t bitmap ; <nl>  <nl> - if (! nested_cpu_has ( get_vmcs12 ( vcpu ), CPU_BASED_USE_MSR_BITMAPS )) <nl> + if (! nested_cpu_has ( vmcs12 , CPU_BASED_USE_MSR_BITMAPS )) <nl> return 1 ; <nl>  <nl> /*
radeon_user_framebuffer_create ( struct drm_device * dev , <nl> return ERR_PTR (- ENOENT ); <nl> } <nl>  <nl> + /* Handle is imported dma - buf , so cannot be migrated to VRAM for scanout */ <nl> + if ( obj -> import_attach ) { <nl> + DRM_DEBUG_KMS (" Cannot create framebuffer from imported dma_buf \ n "); <nl> + return ERR_PTR (- EINVAL ); <nl> + } <nl> + <nl> radeon_fb = kzalloc ( sizeof (* radeon_fb ), GFP_KERNEL ); <nl> if ( radeon_fb == NULL ) { <nl> drm_gem_object_unreference_unlocked ( obj );
static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> ctl_dir = container_of ( head , struct ctl_dir , header ); <nl>  <nl> if (! dir_emit_dots ( file , ctx )) <nl> - return 0 ; <nl> + goto out ; <nl>  <nl> pos = 2 ; <nl>  <nl> static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> break ; <nl> } <nl> } <nl> + out : <nl> sysctl_head_finish ( head ); <nl> return 0 ; <nl> }
int ip6_mr_input ( struct sk_buff * skb ) <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> cache = ip6mr_cache_find ( mrt ,
imsttfb_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> default : <nl> printk ( KERN_INFO " imsttfb : Device 0x % x unknown , " <nl> " contact maintainer .\ n ", pdev -> device ); <nl> + release_mem_region ( addr , size ); <nl> + framebuffer_release ( info ); <nl> return - ENODEV ; <nl> } <nl> 
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . fuse = gm107_fuse_new , <nl> . gpio = gk104_gpio_new , <nl> + . i2c = gm200_i2c_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> . mmu = gf100_mmu_new ,
__cmpxchg_u32 ( volatile unsigned int * p , unsigned int old , unsigned int new ) <nl> " bra 2f ; \ n " <nl> " . fillinsn \ n " <nl> " 1 :" <nl> - M32R_UNLOCK " % 2 , @% 1 ; \ n " <nl> + M32R_UNLOCK " % 0 , @% 1 ; \ n " <nl> " . fillinsn \ n " <nl> " 2 :" <nl> : "=& r " ( retval )
void __init smp_prepare_boot_cpu ( void ) <nl>  <nl> static void send_ipi_message ( const struct cpumask * mask , enum ipi_msg_type msg ) <nl> { <nl> - unsigned long flags ; <nl> - <nl> - local_irq_save ( flags ); <nl> - <nl> /* <nl> * Call the platform specific cross - CPU call function . <nl> */ <nl> smp_cross_call ( mask , msg ); <nl> - <nl> - local_irq_restore ( flags ); <nl> } <nl>  <nl> void arch_send_call_function_ipi_mask ( const struct cpumask * mask )
static struct collection collections [] = { <nl>  <nl> /* Iterate over all benchmarks within a collection : */ <nl> # define for_each_bench ( coll , bench ) \ <nl> - for ( bench = coll -> benchmarks ; bench -> name ; bench ++) <nl> + for ( bench = coll -> benchmarks ; bench && bench -> name ; bench ++) <nl>  <nl> static void dump_benchmarks ( struct collection * coll ) <nl> {
static int si_dpm_init_microcode ( struct amdgpu_device * adev ) <nl> ( adev -> pdev -> revision == 0x80 ) || <nl> ( adev -> pdev -> revision == 0x81 ) || <nl> ( adev -> pdev -> revision == 0x83 ) || <nl> + ( adev -> pdev -> revision == 0x87 ) || <nl> ( adev -> pdev -> device == 0x6604 ) || <nl> ( adev -> pdev -> device == 0x6605 )) <nl> chip_name = " oland_k ";
s32 igb_get_phy_id ( struct e1000_hw * hw ) <nl> u16 phy_id ; <nl>  <nl> /* ensure PHY page selection to fix misconfigured i210 */ <nl> - if ( hw -> mac . type == e1000_i210 ) <nl> + if (( hw -> mac . type == e1000_i210 ) || ( hw -> mac . type == e1000_i211 )) <nl> phy -> ops . write_reg ( hw , I347AT4_PAGE_SELECT , 0 ); <nl>  <nl> ret_val = phy -> ops . read_reg ( hw , PHY_ID1 , & phy_id );
int fcoe_ctlr_els_send ( struct fcoe_ctlr * fip , struct fc_lport * lport , <nl>  <nl> if ( fip -> state == FIP_ST_NON_FIP ) <nl> return 0 ; <nl> + if (! fip -> sel_fcf ) <nl> + goto drop ; <nl>  <nl> switch ( op ) { <nl> case ELS_FLOGI :
static int bcm_parse_target_params ( PMINI_ADAPTER Adapter ) <nl> if (! buff ) <nl> return - ENOMEM ; <nl>  <nl> - if (( Adapter -> pstargetparams = kmalloc ( sizeof ( STARGETPARAMS ), GFP_KERNEL )) == NULL ) { <nl> + Adapter -> pstargetparams = kmalloc ( sizeof ( STARGETPARAMS ), GFP_KERNEL ); <nl> + if ( Adapter -> pstargetparams == NULL ) { <nl> kfree ( buff ); <nl> return - ENOMEM ; <nl> }
static int i82875p_setup_overfl_dev ( struct pci_dev * pdev , <nl> "% s (): pci_bus_add_device () Failed \ n ", <nl> __func__ ); <nl> } <nl> + pci_bus_assign_resources ( dev -> bus ); <nl> } <nl>  <nl> * ovrfl_pdev = dev ;
static void sppp_lcp_input ( struct sppp * sp , struct sk_buff * skb ) <nl> struct net_device * dev = sp -> pp_if ; <nl> int len = skb -> len ; <nl> u8 * p , opt [ 6 ]; <nl> - u32 rmagic ; <nl> + u32 rmagic = 0 ; <nl>  <nl> if (! pskb_may_pull ( skb , sizeof ( struct lcp_header ))) { <nl> if ( sp -> pp_flags & PP_DEBUG )
static void e1000_set_rx_mode ( struct net_device * netdev ) <nl> e1000_rar_set ( hw , ha -> addr , i ++); <nl> } <nl>  <nl> - WARN_ON ( i == rar_entries ); <nl> - <nl> netdev_for_each_mc_addr ( ha , netdev ) { <nl> if ( i == rar_entries ) { <nl> /* load any remaining addresses into the hash table */
static int qxl_palette_create_1bit ( struct qxl_bo * palette_bo , <nl> * correctly globaly , since that would require <nl> * tracking all of our palettes . */ <nl> ret = qxl_bo_kmap ( palette_bo , ( void **)& pal ); <nl> + if ( ret ) <nl> + return ret ; <nl> pal -> num_ents = 2 ; <nl> pal -> unique = unique ++; <nl> if ( visual == FB_VISUAL_TRUECOLOR || visual == FB_VISUAL_DIRECTCOLOR ) {
struct usb_sevsegdev { <nl> * if str commands are used , we would assume the end of string <nl> * so mem commands are used . <nl> */ <nl> - inline size_t my_memlen ( const char * buf , size_t count ) <nl> + static inline size_t my_memlen ( const char * buf , size_t count ) <nl> { <nl> if ( count > 0 && buf [ count - 1 ] == '\ n ') <nl> return count - 1 ;
static void tcp_init_metrics ( struct sock * sk ) <nl> } <nl> if ( dst_metric ( dst , RTAX_RTTVAR ) > tp -> mdev ) { <nl> tp -> mdev = dst_metric ( dst , RTAX_RTTVAR ); <nl> - tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , TCP_RTO_MIN ); <nl> + tp -> mdev_max = tp -> rttvar = max ( tp -> mdev , tcp_rto_min ( sk )); <nl> } <nl> tcp_set_rto ( sk ); <nl> tcp_bound_rto ( sk );
void usb_buffer_unmap_sg ( struct usb_device * dev , unsigned pipe , <nl>  <nl> static int verify_suspended ( struct device * dev , void * unused ) <nl> { <nl> + if ( dev -> driver == NULL ) <nl> + return 0 ; <nl> return ( dev -> power . power_state . event == PM_EVENT_ON ) ? - EBUSY : 0 ; <nl> } <nl> 
static int mmpcam_probe ( struct platform_device * pdev ) <nl>  <nl> out_unregister : <nl> mccic_shutdown ( mcam ); <nl> - mmpcam_power_down ( mcam ); <nl> out_gpio2 : <nl> + mmpcam_power_down ( mcam ); <nl> gpio_free ( pdata -> sensor_reset_gpio ); <nl> out_gpio : <nl> gpio_free ( pdata -> sensor_power_gpio );
static int pwm_setup_backlight ( struct intel_connector * connector , <nl> return - ENODEV ; <nl> } <nl>  <nl> + /* <nl> + * FIXME : pwm_apply_args () should be removed when switching to <nl> + * the atomic PWM API . <nl> + */ <nl> + pwm_apply_args ( panel -> backlight . pwm ); <nl> + <nl> retval = pwm_config ( panel -> backlight . pwm , CRC_PMIC_PWM_PERIOD_NS , <nl> CRC_PMIC_PWM_PERIOD_NS ); <nl> if ( retval < 0 ) {
static void ceph_x_destroy ( struct ceph_auth_client * ac ) <nl> remove_ticket_handler ( ac , th ); <nl> } <nl>  <nl> + if ( xi -> auth_authorizer . buf ) <nl> + ceph_buffer_put ( xi -> auth_authorizer . buf ); <nl> + <nl> kfree ( ac -> private ); <nl> ac -> private = NULL ; <nl> }
static int shmem_unuse_inode ( struct shmem_inode_info * info , swp_entry_t entry , s <nl> if ( size > ENTRIES_PER_PAGE ) <nl> size = ENTRIES_PER_PAGE ; <nl> offset = shmem_find_swp ( entry , ptr , ptr + size ); <nl> + shmem_swp_unmap ( ptr ); <nl> if ( offset >= 0 ) { <nl> shmem_dir_unmap ( dir ); <nl> + ptr = shmem_swp_map ( subdir ); <nl> goto found ; <nl> } <nl> - shmem_swp_unmap ( ptr ); <nl> } <nl> } <nl> lost1 :
static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
static int ath9k_add_interface ( struct ieee80211_hw * hw , <nl> } <nl> } <nl>  <nl> - if (( vif -> type == NL80211_IFTYPE_ADHOC ) && <nl> - sc -> nvifs > 0 ) { <nl> + if (( ah -> opmode == NL80211_IFTYPE_ADHOC ) || <nl> + (( vif -> type == NL80211_IFTYPE_ADHOC ) && <nl> + sc -> nvifs > 0 )) { <nl> ath_err ( common , " Cannot create ADHOC interface when other " <nl> " interfaces already exist .\ n "); <nl> ret = - EINVAL ;
int perf_session__cpu_bitmap ( struct perf_session * session , <nl> } <nl>  <nl> map = cpu_map__new ( cpu_list ); <nl> + if ( map == NULL ) { <nl> + pr_err (" Invalid cpu_list \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> for ( i = 0 ; i < map -> nr ; i ++) { <nl> int cpu = map -> map [ i ];
unsigned int create_irq_nr ( unsigned int irq_want , int node ) <nl> continue ; <nl>  <nl> desc_new = move_irq_desc ( desc_new , node ); <nl> + cfg_new = desc_new -> chip_data ; <nl>  <nl> if ( __assign_irq_vector ( new , cfg_new , apic -> target_cpus ()) == 0 ) <nl> irq = new ;
static int selinux_setprocattr ( struct task_struct * p , <nl> return error ; <nl>  <nl> /* Obtain a SID for the context , if one was specified . */ <nl> - if ( size && str [ 1 ] && str [ 1 ] != '\ n ') { <nl> + if ( size && str [ 0 ] && str [ 0 ] != '\ n ') { <nl> if ( str [ size - 1 ] == '\ n ') { <nl> str [ size - 1 ] = 0 ; <nl> size --;
static __be32 encode_cb_sequence_res ( struct svc_rqst * rqstp , <nl> if ( unlikely ( status != 0 )) <nl> goto out ; <nl>  <nl> - encode_sessionid ( xdr , & res -> csr_sessionid ); <nl> + status = encode_sessionid ( xdr , & res -> csr_sessionid ); <nl> + if ( status ) <nl> + goto out ; <nl>  <nl> p = xdr_reserve_space ( xdr , 4 * sizeof ( uint32_t )); <nl> if ( unlikely ( p == NULL ))
static unsigned long ramfs_nommu_get_unmapped_area ( struct file * file , <nl>  <nl> /* gang - find the pages */ <nl> ret = - ENOMEM ; <nl> - pages = kzalloc ( lpages * sizeof ( struct page *), GFP_KERNEL ); <nl> + pages = kcalloc ( lpages , sizeof ( struct page *), GFP_KERNEL ); <nl> if (! pages ) <nl> goto out_free ; <nl> 
static int ltr501_write_event_config ( struct iio_dev * indio_dev , <nl> int ret ; <nl>  <nl> /* only 1 and 0 are valid inputs */ <nl> - if ( state != 1 || state != 0 ) <nl> + if ( state != 1 && state != 0 ) <nl> return - EINVAL ; <nl>  <nl> switch ( chan -> type ) {
int call_usermodehelper_exec ( struct subprocess_info * sub_info , int wait ) <nl> DECLARE_COMPLETION_ONSTACK ( done ); <nl> int retval = 0 ; <nl>  <nl> + if (! sub_info -> path ) { <nl> + call_usermodehelper_freeinfo ( sub_info ); <nl> + return - EINVAL ; <nl> + } <nl> helper_lock (); <nl> if (! khelper_wq || usermodehelper_disabled ) { <nl> retval = - EBUSY ;
static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> } <nl> logical = re -> logical ; <nl>  <nl> - reada_extent_put ( fs_info , re ); <nl> - <nl> atomic_inc (& dev -> reada_in_flight ); <nl> ret = reada_tree_block_flagged ( fs_info -> extent_root , logical , <nl> mirror_num , & eb ); <nl> static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> if ( eb ) <nl> free_extent_buffer ( eb ); <nl>  <nl> + reada_extent_put ( fs_info , re ); <nl> + <nl> return 1 ; <nl>  <nl> }
static int spi_imx_setupxfer ( struct spi_device * spi , <nl> config . bpw = t ? t -> bits_per_word : spi -> bits_per_word ; <nl> config . speed_hz = t ? t -> speed_hz : spi -> max_speed_hz ; <nl> config . mode = spi -> mode ; <nl> + config . cs = spi_imx -> chipselect [ spi -> chip_select ]; <nl>  <nl> if (! config . speed_hz ) <nl> config . speed_hz = spi -> max_speed_hz ;
static enum ucode_state request_microcode_fw ( int cpu , struct device * device ) <nl> return UCODE_NFOUND ; <nl> } <nl>  <nl> + if (*( u32 *) firmware -> data != UCODE_MAGIC ) { <nl> + printk ( KERN_ERR " microcode : invalid UCODE_MAGIC ( 0x % 08x )\ n ", <nl> + *( u32 *) firmware -> data ); <nl> + return UCODE_ERROR ; <nl> + } <nl> + <nl> ret = generic_load_microcode ( cpu , firmware -> data , firmware -> size ); <nl>  <nl> release_firmware ( firmware );
static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
static irqreturn_t sdhci_irq ( int irq , void * dev_id ) <nl>  <nl> intmask = readl ( host -> ioaddr + SDHCI_INT_STATUS ); <nl>  <nl> - if (! intmask ) { <nl> + if (! intmask || intmask == 0xffffffff ) { <nl> result = IRQ_NONE ; <nl> goto out ; <nl> }
int del_mtd_blktrans_dev ( struct mtd_blktrans_dev * old ) <nl> BUG (); <nl> } <nl>  <nl> - /* Stop new requests to arrive */ <nl> - del_gendisk ( old -> disk ); <nl> - <nl> if ( old -> disk_attributes ) <nl> sysfs_remove_group (& disk_to_dev ( old -> disk )-> kobj , <nl> old -> disk_attributes ); <nl>  <nl> + /* Stop new requests to arrive */ <nl> + del_gendisk ( old -> disk ); <nl> + <nl> + <nl> /* Stop the thread */ <nl> kthread_stop ( old -> thread ); <nl> 
void dlm_lowcomms_stop ( void ) <nl> con = __nodeid2con ( i , 0 ); <nl> if ( con ) { <nl> close_connection ( con , true ); <nl> + if ( con -> othercon ) <nl> + kmem_cache_free ( con_cache , con -> othercon ); <nl> kmem_cache_free ( con_cache , con ); <nl> } <nl> }
static int __devinit mei_probe ( struct pci_dev * pdev , <nl> err = request_threaded_irq ( pdev -> irq , <nl> NULL , <nl> mei_interrupt_thread_handler , <nl> - 0 , mei_driver_name , dev ); <nl> + IRQF_ONESHOT , mei_driver_name , dev ); <nl> else <nl> err = request_threaded_irq ( pdev -> irq , <nl> mei_interrupt_quick_handler ,
int c4iw_register_device ( struct c4iw_dev * dev ) <nl> dev -> ibdev . iwcm -> add_ref = c4iw_qp_add_ref ; <nl> dev -> ibdev . iwcm -> rem_ref = c4iw_qp_rem_ref ; <nl> dev -> ibdev . iwcm -> get_qp = c4iw_get_qp ; <nl> + memcpy ( dev -> ibdev . iwcm -> ifname , dev -> rdev . lldi . ports [ 0 ]-> name , <nl> + sizeof ( dev -> ibdev . iwcm -> ifname )); <nl>  <nl> ret = ib_register_device (& dev -> ibdev , NULL ); <nl> if ( ret )
static void get_total_mem ( struct mv64x60_mc_pdata * pdata ) <nl> if (! np ) <nl> return ; <nl>  <nl> - reg = get_property ( np , " reg ", NULL ); <nl> + reg = of_get_property ( np , " reg ", NULL ); <nl>  <nl> pdata -> total_mem = reg [ 1 ]; <nl> }
static void i40iw_cm_disconn_true ( struct i40iw_qp * iwqp ) <nl> /* Flush the queues */ <nl> i40iw_flush_wqes ( iwdev , iwqp ); <nl>  <nl> - if ( qp -> term_flags ) { <nl> + if ( qp -> term_flags && iwqp -> ibqp . event_handler ) { <nl> ibevent . device = iwqp -> ibqp . device ; <nl> ibevent . event = ( qp -> eventtype == TERM_EVENT_QP_FATAL ) ? <nl> IB_EVENT_QP_FATAL : IB_EVENT_QP_ACCESS_ERR ;
static int threshold_create_bank ( unsigned int cpu , unsigned int bank ) <nl> const char * name = get_name ( bank , NULL ); <nl> int err = 0 ; <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( is_shared_bank ( bank )) { <nl> nb = node_to_amd_nb ( amd_get_nb_id ( cpu )); <nl> 
int dwc3_send_gadget_generic_command ( struct dwc3 * dwc , unsigned cmd , u32 param ) <nl> dwc3_trace ( trace_dwc3_gadget , <nl> " Command Complete --> % d ", <nl> DWC3_DGCMD_STATUS ( reg )); <nl> + if ( DWC3_DGCMD_STATUS ( reg )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static int rohm_ts_load_firmware ( struct i2c_client * client , <nl> break ; <nl>  <nl> error = - EIO ; <nl> - } while (++ retry >= FIRMWARE_RETRY_MAX ); <nl> + } while (++ retry <= FIRMWARE_RETRY_MAX ); <nl>  <nl> out : <nl> error2 = i2c_smbus_write_byte_data ( client , INT_MASK , INT_ALL );
void bpf_jit_compile ( struct bpf_prog * fp ) <nl>  <nl> memset (& ctx , 0 , sizeof ( ctx )); <nl>  <nl> - ctx . offsets = kcalloc ( fp -> len , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> + ctx . offsets = kcalloc ( fp -> len + 1 , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> if ( ctx . offsets == NULL ) <nl> return ; <nl> 
static void igb_reuse_rx_page ( struct igb_ring * rx_ring , <nl> rx_ring -> next_to_alloc = ( nta < rx_ring -> count ) ? nta : 0 ; <nl>  <nl> /* transfer page from old buffer to new buffer */ <nl> - memcpy ( new_buff , old_buff , sizeof ( struct igb_rx_buffer )); <nl> + * new_buff = * old_buff ; <nl>  <nl> /* sync the buffer for use by the device */ <nl> dma_sync_single_range_for_device ( rx_ring -> dev , old_buff -> dma ,
int macvlan_common_newlink ( struct net * src_net , struct net_device * dev , <nl>  <nl> list_add_tail_rcu (& vlan -> list , & port -> vlans ); <nl> netif_stacked_transfer_operstate ( lowerdev , dev ); <nl> + linkwatch_fire_event ( dev ); <nl>  <nl> return 0 ; <nl>  <nl> static int macvlan_device_event ( struct notifier_block * unused , <nl> port = macvlan_port_get_rtnl ( dev ); <nl>  <nl> switch ( event ) { <nl> + case NETDEV_UP : <nl> case NETDEV_CHANGE : <nl> list_for_each_entry ( vlan , & port -> vlans , list ) <nl> netif_stacked_transfer_operstate ( vlan -> lowerdev ,
static ssize_t rbd_add ( struct bus_type * bus , <nl> if (! try_module_get ( THIS_MODULE )) <nl> return - ENODEV ; <nl>  <nl> - mon_dev_name = kmalloc ( RBD_MAX_OPT_LEN , GFP_KERNEL ); <nl> + mon_dev_name = kmalloc ( count , GFP_KERNEL ); <nl> if (! mon_dev_name ) <nl> goto err_out_mod ; <nl>  <nl> - options = kmalloc ( RBD_MAX_OPT_LEN , GFP_KERNEL ); <nl> + options = kmalloc ( count , GFP_KERNEL ); <nl> if (! options ) <nl> goto err_mon_dev ; <nl> 
static struct platform_driver i2c_mux_reg_driver = { <nl> . remove = i2c_mux_reg_remove , <nl> . driver = { <nl> . name = " i2c - mux - reg ", <nl> + . of_match_table = of_match_ptr ( i2c_mux_reg_of_match ), <nl> }, <nl> }; <nl> 
static int __init b44_init ( void ) <nl>  <nl> /* Setup paramaters for syncing RX / TX DMA descriptors */ <nl> dma_desc_align_mask = ~( dma_desc_align_size - 1 ); <nl> - dma_desc_sync_size = max ( dma_desc_align_size , sizeof ( struct dma_desc )); <nl> + dma_desc_sync_size = max_t ( unsigned int , dma_desc_align_size , sizeof ( struct dma_desc )); <nl>  <nl> return pci_module_init (& b44_driver ); <nl> }
static int p54u_probe ( struct usb_interface * intf , <nl> priv -> upload_fw = p54u_upload_firmware_net2280 ; <nl> } <nl> err = p54u_load_firmware ( dev , intf ); <nl> + if ( err ) { <nl> + usb_put_dev ( udev ); <nl> + p54_free_common ( dev ); <nl> + } <nl> return err ; <nl> } <nl> 
static int omap_hdq_remove ( struct platform_device * pdev ) <nl>  <nl> if ( hdq_data -> hdq_usecount ) { <nl> dev_dbg (& pdev -> dev , " removed when use count is not zero \ n "); <nl> + mutex_unlock (& hdq_data -> hdq_mutex ); <nl> return - EBUSY ; <nl> } <nl> 
static int exynos_drm_fbdev_update ( struct drm_fb_helper * helper , <nl>  <nl> fbi -> screen_base = buffer -> kvaddr + offset ; <nl> fbi -> screen_size = size ; <nl> + fbi -> fix . smem_len = size ; <nl>  <nl> return 0 ; <nl> }
snd_pmac_burgundy_busy_wait ( struct snd_pmac * chip ) <nl> int timeout = 50 ; <nl> while (( in_le32 (& chip -> awacs -> codec_ctrl ) & MASK_NEWECMD ) && timeout --) <nl> udelay ( 1 ); <nl> - if (! timeout ) <nl> + if ( timeout < 0 ) <nl> printk ( KERN_DEBUG " burgundy_busy_wait : timeout \ n "); <nl> } <nl> 
static void tpacket_destruct_skb ( struct sk_buff * skb ) <nl> struct packet_sock * po = pkt_sk ( skb -> sk ); <nl> void * ph ; <nl>  <nl> - BUG_ON ( skb == NULL ); <nl> - <nl> if ( likely ( po -> tx_ring . pg_vec )) { <nl> ph = skb_shinfo ( skb )-> destructor_arg ; <nl> BUG_ON ( __packet_get_status ( po , ph ) != TP_STATUS_SENDING );
void __init exynos5_init_irq ( void ) <nl> * Theses parameters should be NULL and 0 because EXYNOS4 <nl> * uses GIC instead of VIC . <nl> */ <nl> - s5p_init_irq ( NULL , 0 ); <nl> + if (! of_machine_is_compatible (" samsung , exynos5440 ")) <nl> + s5p_init_irq ( NULL , 0 ); <nl>  <nl> gic_arch_extn . irq_set_wake = s3c_irq_wake ; <nl> }
static void vss_on_reset ( void ) <nl> int <nl> hv_vss_init ( struct hv_util_service * srv ) <nl> { <nl> + if ( vmbus_proto_version < VERSION_WIN8_1 ) { <nl> + pr_warn (" Integration service ' Backup ( volume snapshot )'" <nl> + " not supported on this host version .\ n "); <nl> + return - ENOTSUPP ; <nl> + } <nl> recv_buffer = srv -> recv_buffer ; <nl>  <nl> /*
void destroy_preds ( struct ftrace_event_call * call ) <nl> filter_free_pred ( filter -> preds [ i ]); <nl> } <nl> kfree ( filter -> preds ); <nl> + kfree ( filter -> filter_string ); <nl> kfree ( filter ); <nl> call -> filter = NULL ; <nl> }
static const struct of_device_id bcm_kona_i2c_of_match [] = { <nl> {. compatible = " brcm , kona - i2c ",}, <nl> {}, <nl> }; <nl> - MODULE_DEVICE_TABLE ( of , kona_i2c_of_match ); <nl> + MODULE_DEVICE_TABLE ( of , bcm_kona_i2c_of_match ); <nl>  <nl> static struct platform_driver bcm_kona_i2c_driver = { <nl> . driver = {
int snd_soc_dapm_device_event ( struct snd_soc_device * socdev , int event ) <nl> struct snd_soc_machine * machine = socdev -> machine ; <nl>  <nl> if ( machine -> dapm_event ) <nl> - machine -> dapm_event ( machine , event ); <nl> + machine -> dapm_event ( machine , event ); <nl> if ( codec -> dapm_event ) <nl> - codec -> dapm_event ( codec , event ); <nl> + codec -> dapm_event ( codec , event ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_device_event );
static int ceph_con_in_msg_alloc ( struct ceph_connection * con , int * skip ) <nl> msg = con -> ops -> alloc_msg ( con , hdr , skip ); <nl> mutex_lock (& con -> mutex ); <nl> if ( con -> state != CON_STATE_OPEN ) { <nl> - ceph_msg_put ( msg ); <nl> + if ( msg ) <nl> + ceph_msg_put ( msg ); <nl> return - EAGAIN ; <nl> } <nl> con -> in_msg = msg ;
coh901318_tx_status ( struct dma_chan * chan , dma_cookie_t cookie , <nl> enum dma_status ret ; <nl>  <nl> ret = dma_cookie_status ( chan , cookie , txstate ); <nl> - if ( ret == DMA_COMPLETE ) <nl> + if ( ret == DMA_COMPLETE || ! txstate ) <nl> return ret ; <nl>  <nl> dma_set_residue ( txstate , coh901318_get_bytes_left ( chan ));
int ssb_bus_scan ( struct ssb_bus * bus , <nl> /* Ignore PCI cores on PCI - E cards . <nl> * Ignore PCI - E cores on PCI cards . */ <nl> if ( dev -> id . coreid == SSB_DEV_PCI ) { <nl> - if ( bus -> host_pci -> is_pcie ) <nl> + if ( pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } else { <nl> - if (! bus -> host_pci -> is_pcie ) <nl> + if (! pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } <nl> }
TLan_FinishReset ( struct net_device * dev ) <nl> TLan_SetTimer ( dev , ( 10 * HZ ), TLAN_TIMER_FINISH_RESET ); <nl> return ; <nl> } <nl> + TLan_SetMulticastList ( dev ); <nl>  <nl> } /* TLan_FinishReset */ <nl> 
vmw_execbuf_copy_fence_user ( struct vmw_private * dev_priv , <nl> if ( user_fence_rep == NULL ) <nl> return ; <nl>  <nl> + memset (& fence_rep , 0 , sizeof ( fence_rep )); <nl> + <nl> fence_rep . error = ret ; <nl> if ( ret == 0 ) { <nl> BUG_ON ( fence == NULL );
retry : <nl> if ( local_flags & __GFP_WAIT ) <nl> local_irq_enable (); <nl> kmem_flagcheck ( cache , flags ); <nl> - obj = kmem_getpages ( cache , flags , - 1 ); <nl> + obj = kmem_getpages ( cache , local_flags , - 1 ); <nl> if ( local_flags & __GFP_WAIT ) <nl> local_irq_disable (); <nl> if ( obj ) {
static void atombios_crtc_program_ss ( struct drm_crtc * crtc , <nl> return ; <nl> } <nl> args . v2 . ucEnable = enable ; <nl> - if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK )) <nl> + if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK ) || ASIC_IS_DCE41 ( rdev )) <nl> args . v2 . ucEnable = ATOM_DISABLE ; <nl> } else if ( ASIC_IS_DCE3 ( rdev )) { <nl> args . v1 . usSpreadSpectrumPercentage = cpu_to_le16 ( ss -> percentage );
static int __pppoe_xmit ( struct sock * sk , struct sk_buff * skb ) <nl> * give dev_queue_xmit something it can free . <nl> */ <nl> skb2 = skb_clone ( skb , GFP_ATOMIC ); <nl> + <nl> + if ( skb2 == NULL ) <nl> + goto abort ; <nl> } <nl>  <nl> ph = ( struct pppoe_hdr *) skb_push ( skb2 , sizeof ( struct pppoe_hdr ));
fastcall void __kprobes do_general_protection ( struct pt_regs * regs , <nl> tss -> io_bitmap_max - thread -> io_bitmap_max ); <nl> tss -> io_bitmap_max = thread -> io_bitmap_max ; <nl> tss -> io_bitmap_base = IO_BITMAP_OFFSET ; <nl> + tss -> io_bitmap_owner = thread ; <nl> put_cpu (); <nl> return ; <nl> }
static struct davinci_nand_pdata davinci_nand_data = { <nl> . nr_parts = ARRAY_SIZE ( davinci_nand_partitions ), <nl> . ecc_mode = NAND_ECC_HW , <nl> . options = NAND_USE_FLASH_BBT , <nl> + . ecc_bits = 4 , <nl> }; <nl>  <nl> static struct resource davinci_nand_resources [] = {
static int __init create_setup_data_nodes ( struct dentry * parent ) <nl> if ( PageHighMem ( pg )) { <nl> data = ioremap_cache ( pa_data , sizeof (* data )); <nl> if (! data ) { <nl> + kfree ( node ); <nl> error = - ENXIO ; <nl> goto err_dir ; <nl> }
static int __init list_sort_test ( void ) <nl> } <nl> count ++; <nl> } <nl> + if ( head . prev != cur ) { <nl> + printk ( KERN_ERR " list_sort_test : error : list is corrupted \ n "); <nl> + goto exit ; <nl> + } <nl> + <nl>  <nl> if ( count != TEST_LIST_LEN ) { <nl> printk ( KERN_ERR " list_sort_test : error : bad list length % d ",
static int cipso_v4_map_cat_rbm_hton ( const struct cipso_v4_doi * doi_def , <nl>  <nl> switch ( doi_def -> type ) { <nl> case CIPSO_V4_MAP_PASS : <nl> - net_spot_max = host_cat_len - 1 ; <nl> - while ( net_spot_max > 0 && host_cat [ net_spot_max ] == 0 ) <nl> + net_spot_max = host_cat_len ; <nl> + while ( net_spot_max > 0 && host_cat [ net_spot_max - 1 ] == 0 ) <nl> net_spot_max --; <nl> if ( net_spot_max > net_cat_len ) <nl> return - EINVAL ;
static int __devinit pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> unsigned int delay ; <nl> u8 pon_cntl ; <nl> struct pmic8xxx_pwrkey * pwrkey ; <nl> - const struct pm8xxx_pwrkey_platform_data * pdata = mfd_get_data ( pdev ); <nl> + const struct pm8xxx_pwrkey_platform_data * pdata = <nl> + dev_get_platdata (& pdev -> dev ); <nl>  <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " power key platform data not supplied \ n ");
static irqreturn_t ad7298_trigger_handler ( int irq , void * p ) <nl> struct iio_dev * indio_dev = pf -> indio_dev ; <nl> struct ad7298_state * st = iio_priv ( indio_dev ); <nl> struct iio_buffer * ring = indio_dev -> buffer ; <nl> - s64 time_ns ; <nl> + s64 time_ns = 0 ; <nl> __u16 buf [ 16 ]; <nl> int b_sent , i ; <nl> 
static int uvc_v4l2_open ( struct file * file ) <nl> if ( atomic_inc_return (& stream -> dev -> users ) == 1 ) { <nl> ret = uvc_status_start ( stream -> dev ); <nl> if ( ret < 0 ) { <nl> - usb_autopm_put_interface ( stream -> dev -> intf ); <nl> atomic_dec (& stream -> dev -> users ); <nl> + usb_autopm_put_interface ( stream -> dev -> intf ); <nl> kfree ( handle ); <nl> return ret ; <nl> }
static void bnx2x_set_one_mac_e1x ( struct bnx2x * bp , <nl> /* Reset the ramrod data buffer */ <nl> memset ( config , 0 , sizeof (* config )); <nl>  <nl> - bnx2x_vlan_mac_set_rdata_e1x ( bp , o , BNX2X_FILTER_MAC_PENDING , <nl> + bnx2x_vlan_mac_set_rdata_e1x ( bp , o , raw -> state , <nl> cam_offset , add , <nl> elem -> cmd_data . vlan_mac . u . mac . mac , 0 , <nl> ETH_VLAN_FILTER_ANY_VLAN , config );
static ssize_t yurex_write ( struct file * file , const char * user_buffer , size_t co <nl> goto error ; <nl>  <nl> mutex_lock (& dev -> io_mutex ); <nl> - if (! dev -> interface ) { /* alreaday disconnected */ <nl> + if (! dev -> interface ) { /* already disconnected */ <nl> mutex_unlock (& dev -> io_mutex ); <nl> retval = - ENODEV ; <nl> goto error ;
int hfsplus_get_block ( struct inode * inode , sector_t iblock , <nl> goto done ; <nl> } <nl>  <nl> + if ( inode -> i_ino == HFSPLUS_EXT_CNID ) <nl> + return - EIO ; <nl> + <nl> mutex_lock (& HFSPLUS_I ( inode ). extents_lock ); <nl> res = hfsplus_ext_read_extent ( inode , ablock ); <nl> if (! res ) {
struct rxrpc_call * rxrpc_new_incoming_call ( struct rxrpc_local * local , <nl>  <nl> /* Get the socket providing the service */ <nl> rx = rcu_dereference ( local -> service ); <nl> - if ( service_id == rx -> srx . srx_service ) <nl> + if ( rx && service_id == rx -> srx . srx_service ) <nl> goto found_service ; <nl>  <nl> trace_rxrpc_abort (" INV ", sp -> hdr . cid , sp -> hdr . callNumber , sp -> hdr . seq ,
static int atalk_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> return - ENOBUFS ; <nl>  <nl> * uaddr_len = sizeof ( struct sockaddr_at ); <nl> + memset (& sat . sat_zero , 0 , sizeof ( sat . sat_zero )); <nl>  <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED )
int trace_parser_get_init ( struct trace_parser * parser , int size ) <nl> void trace_parser_put ( struct trace_parser * parser ) <nl> { <nl> kfree ( parser -> buffer ); <nl> + parser -> buffer = NULL ; <nl> } <nl>  <nl> /*
found : <nl>  <nl> if ( codec -> reg_cache ) <nl> kfree ( codec -> reg_cache ); <nl> + kfree ( codec -> name ); <nl> kfree ( codec ); <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_unregister_codec );
int cvmx_usb_initialize ( struct cvmx_usb_state * state , int usb_port_number , <nl> } <nl> } <nl>  <nl> - memset ( usb , 0 , sizeof ( usb )); <nl> + memset ( usb , 0 , sizeof (* usb )); <nl> usb -> init_flags = flags ; <nl>  <nl> /* Initialize the USB state structure */
struct comedi_device * comedi_open ( const char * filename ) <nl> if ( strncmp ( filename , "/ dev / comedi ", 11 ) != 0 ) <nl> return NULL ; <nl>  <nl> - minor = simple_strtoul ( filename + 11 , NULL , 0 ); <nl> + if ( kstrtouint ( filename + 11 , 0 , & minor )) <nl> + return NULL ; <nl>  <nl> if ( minor >= COMEDI_NUM_BOARD_MINORS ) <nl> return NULL ;
struct rtnl_link_stats64 * e1000e_get_stats64 ( struct net_device * netdev , <nl> static int e1000_change_mtu ( struct net_device * netdev , int new_mtu ) <nl> { <nl> struct e1000_adapter * adapter = netdev_priv ( netdev ); <nl> - int max_frame = new_mtu + ETH_HLEN + ETH_FCS_LEN ; <nl> + int max_frame = new_mtu + VLAN_HLEN + ETH_HLEN + ETH_FCS_LEN ; <nl>  <nl> /* Jumbo frame support */ <nl> if (( max_frame > ETH_FRAME_LEN + ETH_FCS_LEN ) &&
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
 <nl> void btrfs_tree_lock ( struct extent_buffer * eb ); <nl> void btrfs_tree_unlock ( struct extent_buffer * eb ); <nl> - int btrfs_try_spin_lock ( struct extent_buffer * eb ); <nl>  <nl> void btrfs_tree_read_lock ( struct extent_buffer * eb ); <nl> void btrfs_tree_read_unlock ( struct extent_buffer * eb );
static struct array_cache ** alloc_alien_cache ( int node , int limit ) <nl> } <nl> ac_ptr [ i ] = alloc_arraycache ( node , limit , 0xbaadf00d ); <nl> if (! ac_ptr [ i ]) { <nl> - for ( i --; i <= 0 ; i --) <nl> + for ( i --; i >= 0 ; i --) <nl> kfree ( ac_ptr [ i ]); <nl> kfree ( ac_ptr ); <nl> return NULL ;
int ttm_bo_pipeline_move ( struct ttm_buffer_object * bo , <nl> */ <nl>  <nl> spin_lock (& from -> move_lock ); <nl> - if (! from -> move || fence_is_later ( from -> move , fence )) { <nl> + if (! from -> move || fence_is_later ( fence , from -> move )) { <nl> fence_put ( from -> move ); <nl> from -> move = fence_get ( fence ); <nl> }
err_iounmap : <nl> err_free_mem_region : <nl> release_mem_region ( res -> start , resource_size ( res )); <nl> err_free_mem : <nl> - input_free_device ( kbc -> idev ); <nl> + input_free_device ( input_dev ); <nl> kfree ( kbc ); <nl>  <nl> return err ;
static int sunxi_pctrl_dt_node_to_map ( struct pinctrl_dev * pctldev , <nl> configlen ++; <nl>  <nl> pinconfig = kzalloc ( configlen * sizeof (* pinconfig ), GFP_KERNEL ); <nl> + if (! pinconfig ) { <nl> + kfree (* map ); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> if (! of_property_read_u32 ( node , " allwinner , drive ", & val )) { <nl> u16 strength = ( val + 1 ) * 10 ;
static int dbgp_control_msg ( unsigned devnum , int requesttype , <nl> int ret ; <nl>  <nl> read = ( requesttype & USB_DIR_IN ) != 0 ; <nl> - if ( size > ( read ? DBGP_MAX_PACKET : 0 )) <nl> + if ( size > ( read ? DBGP_MAX_PACKET : 0 )) <nl> return - 1 ; <nl>  <nl> /* Compute the control message */
* ( you will need to reboot afterwards ) */ <nl> /* # define BNX2X_STOP_ON_ERROR */ <nl>  <nl> -# define DRV_MODULE_VERSION " 1 . 72 . 10 - 0 " <nl> -# define DRV_MODULE_RELDATE " 2012 / 02 / 20 " <nl> +# define DRV_MODULE_VERSION " 1 . 72 . 17 - 0 " <nl> +# define DRV_MODULE_RELDATE " 2012 / 04 / 02 " <nl> # define BNX2X_BC_VER 0x040200 <nl>  <nl> # if defined ( CONFIG_DCB )
static ssize_t do_generic_file_read ( struct file * filp , loff_t * ppos , <nl>  <nl> cond_resched (); <nl> find_page : <nl> + if ( fatal_signal_pending ( current )) { <nl> + error = - EINTR ; <nl> + goto out ; <nl> + } <nl> + <nl> page = find_get_page ( mapping , index ); <nl> if (! page ) { <nl> page_cache_sync_readahead ( mapping ,
static int ad799x_read_event_value ( struct iio_dev * indio_dev , <nl> if ( ret < 0 ) <nl> return ret ; <nl> * val = ( ret >> chan -> scan_type . shift ) & <nl> - GENMASK ( chan -> scan_type . realbits - 1 , 0 ); <nl> + GENMASK ( chan -> scan_type . realbits - 1 , 0 ); <nl>  <nl> return IIO_VAL_INT ; <nl> }
struct tcp_md5sig_pool * __tcp_get_md5sig_pool ( int cpu ) <nl>  <nl> EXPORT_SYMBOL ( __tcp_get_md5sig_pool ); <nl>  <nl> - void __tcp_put_md5sig_pool ( void ) { <nl> - __tcp_free_md5sig_pool ( tcp_md5sig_pool ); <nl> + void __tcp_put_md5sig_pool ( void ) <nl> +{ <nl> + tcp_free_md5sig_pool (); <nl> } <nl>  <nl> EXPORT_SYMBOL ( __tcp_put_md5sig_pool );
static int bdc_udc_set_selfpowered ( struct usb_gadget * gadget , <nl> unsigned long flags ; <nl>  <nl> dev_dbg ( bdc -> dev , "% s ()\ n ", __func__ ); <nl> + gadget -> is_selfpowered = ( is_self != 0 ); <nl> spin_lock_irqsave (& bdc -> lock , flags ); <nl> if (! is_self ) <nl> bdc -> devstatus |= 1 << USB_DEVICE_SELF_POWERED ;
static bool _is_valid_div ( struct clk_divider * divider , unsigned int div ) <nl> static int _round_up_table ( const struct clk_div_table * table , int div ) <nl> { <nl> const struct clk_div_table * clkt ; <nl> - int up = _get_table_maxdiv ( table ); <nl> + int up = INT_MAX ; <nl>  <nl> for ( clkt = table ; clkt -> div ; clkt ++) { <nl> if ( clkt -> div == div )
static int __ocfs2_change_file_space ( struct file * file , struct inode * inode , <nl> if ( ret < 0 ) <nl> mlog_errno ( ret ); <nl>  <nl> + if ( file -> f_flags & O_SYNC ) <nl> + handle -> h_sync = 1 ; <nl> + <nl> ocfs2_commit_trans ( osb , handle ); <nl>  <nl> out_inode_unlock :
int cap_bprm_set_creds ( struct linux_binprm * bprm ) <nl> { <nl> const struct cred * old = current_cred (); <nl> struct cred * new = bprm -> cred ; <nl> - bool effective , has_cap ; <nl> + bool effective , has_cap = false ; <nl> int ret ; <nl>  <nl> effective = false ;
void usb_del_gadget_udc ( struct usb_gadget * gadget ) <nl> flush_work (& gadget -> work ); <nl> device_unregister (& udc -> dev ); <nl> device_unregister (& gadget -> dev ); <nl> + memset (& gadget -> dev , 0x00 , sizeof ( gadget -> dev )); <nl> } <nl> EXPORT_SYMBOL_GPL ( usb_del_gadget_udc ); <nl> 
static ssize_t i40e_dbg_command_write ( struct file * filp , <nl> if (! cmd_buf ) <nl> return count ; <nl> bytes_not_copied = copy_from_user ( cmd_buf , buffer , count ); <nl> - if ( bytes_not_copied < 0 ) <nl> + if ( bytes_not_copied < 0 ) { <nl> + kfree ( cmd_buf ); <nl> return bytes_not_copied ; <nl> + } <nl> if ( bytes_not_copied > 0 ) <nl> count -= bytes_not_copied ; <nl> cmd_buf [ count ] = '\ 0 ';
struct tpm_chip * tpm_register_hardware ( struct device * dev , const struct tpm_vend <nl> put_device ( dev ); <nl> clear_bit ( chip -> dev_num , dev_mask ); <nl> kfree ( chip ); <nl> + kfree ( devname ); <nl> return NULL ; <nl> } <nl> 
void drm_mm_remove_node ( struct drm_mm_node * node ) <nl> struct drm_mm * mm = node -> mm ; <nl> struct drm_mm_node * prev_node ; <nl>  <nl> + if ( WARN_ON (! node -> allocated )) <nl> + return ; <nl> + <nl> BUG_ON ( node -> scanned_block || node -> scanned_prev_free <nl> || node -> scanned_next_free ); <nl> 
static int audio_set_pcm_format ( struct snd_pcm_hardware * pcm_hw , <nl> if ( cfg -> subbuffer_size != 1 ) <nl> goto error ; <nl> pr_info (" PCM format is 8 - bit mono \ n "); <nl> + pcm_hw -> channels_min = 1 ; <nl> + pcm_hw -> channels_max = 1 ; <nl> pcm_hw -> formats = SNDRV_PCM_FMTBIT_S8 ; <nl> } else if (! strcmp ( pcm_format , " 2x16 ")) { <nl> if ( cfg -> subbuffer_size != 4 )
void ceph_mdsc_sync ( struct ceph_mds_client * mdsc ) <nl> { <nl> u64 want_tid , want_flush ; <nl>  <nl> + if ( mdsc -> client -> mount_state == CEPH_MOUNT_SHUTDOWN ) <nl> + return ; <nl> + <nl> dout (" sync \ n "); <nl> mutex_lock (& mdsc -> mutex ); <nl> want_tid = mdsc -> last_tid ;
static inline int dev_hard_header ( struct sk_buff * skb , struct net_device * dev , <nl> const void * daddr , const void * saddr , <nl> unsigned len ) <nl> { <nl> - if (! dev -> header_ops ) <nl> + if (! dev -> header_ops || ! dev -> header_ops -> create ) <nl> return 0 ; <nl>  <nl> return dev -> header_ops -> create ( skb , dev , type , daddr , saddr , len );
static struct rt6_info * ip6_route_redirect ( struct in6_addr * dest , <nl> }, <nl> }, <nl> }, <nl> - . gateway = * gateway , <nl> }; <nl>  <nl> + ipv6_addr_copy (& rdfl . gateway , gateway ); <nl> + <nl> if ( rt6_need_strict ( dest )) <nl> flags |= RT6_LOOKUP_F_IFACE ; <nl> 
__ieee80211_tx_prepare ( struct ieee80211_tx_data * tx , <nl> tx -> local = local ; <nl> tx -> sdata = IEEE80211_DEV_TO_SUB_IF ( dev ); <nl> tx -> channel = local -> hw . conf . channel ; <nl> + tx -> rate_idx = - 1 ; <nl> + tx -> last_frag_rate_idx = - 1 ; <nl> /* <nl> * Set this flag ( used below to indicate " automatic fragmentation "), <nl> * it will be cleared / left by radiotap as desired .
static void bnx2x_tpa_stop ( struct bnx2x * bp , struct bnx2x_fastpath * fp , <nl>  <nl> return ; <nl> } <nl> - bnx2x_frag_free ( fp , new_data ); <nl> + if ( new_data ) <nl> + bnx2x_frag_free ( fp , new_data ); <nl> drop : <nl> /* drop the packet and keep the buffer in the bin */ <nl> DP ( NETIF_MSG_RX_STATUS ,
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> /* Device config is special , because it must work on <nl> * an unconfigured device . */ <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> + if ( minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> + /* Device config not appropriate on non - board minors . */ <nl> + rc = - ENOTTY ; <nl> + goto done ; <nl> + } <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> if ( rc == 0 )
static void queue_event ( struct client * client , struct event * event , <nl> event -> v [ 1 ]. size = size1 ; <nl>  <nl> spin_lock_irqsave (& client -> lock , flags ); <nl> - <nl> list_add_tail (& event -> link , & client -> event_list ); <nl> - wake_up_interruptible (& client -> wait ); <nl> - <nl> spin_unlock_irqrestore (& client -> lock , flags ); <nl> + <nl> + wake_up_interruptible (& client -> wait ); <nl> } <nl>  <nl> static int
static int ina2xx_buffer_enable ( struct iio_dev * indio_dev ) <nl> chip -> prev_ns = iio_get_time_ns (); <nl>  <nl> chip -> task = kthread_run ( ina2xx_capture_thread , ( void *) indio_dev , <nl> - " ina2xx -% uus ", sampling_us ); <nl> + "% s :% d -% uus ", indio_dev -> name , indio_dev -> id , <nl> + sampling_us ); <nl>  <nl> return PTR_ERR_OR_ZERO ( chip -> task ); <nl> }
static int emc1403_probe ( struct i2c_client * client , <nl> res = sysfs_create_group (& client -> dev . kobj , & m_thermal_gr ); <nl> if ( res ) { <nl> dev_warn (& client -> dev , " create group failed \ n "); <nl> - hwmon_device_unregister ( data -> hwmon_dev ); <nl> goto thermal_error1 ; <nl> } <nl> data -> hwmon_dev = hwmon_device_register (& client -> dev );
static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath_softc * sc = hw -> priv ; <nl> int ret = 0 ; <nl>  <nl> - local_bh_disable (); <nl> + mutex_lock (& sc -> mutex ); <nl>  <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( sc -> sc_ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> - local_bh_enable (); <nl> + mutex_unlock (& sc -> mutex ); <nl>  <nl> return ret ; <nl> }
static int virtnet_probe ( struct virtio_device * vdev ) <nl> dev -> mtu = mtu ; <nl> dev -> max_mtu = mtu ; <nl> } <nl> + <nl> + /* TODO : size buffers correctly in this case . */ <nl> + if ( dev -> mtu > ETH_DATA_LEN ) <nl> + vi -> big_packets = true ; <nl> } <nl>  <nl> if ( vi -> any_header_sg )
int sc_ioctl ( int card , scs_ioctl * data ) <nl>  <nl> case SCIOCSTART : <nl> { <nl> + kfree ( rcvmsg ); <nl> pr_debug ("% s : SCIOSTART : ioctl received \ n ", <nl> sc_adapter [ card ]-> devicename ); <nl> if ( sc_adapter [ card ]-> EngineUp ) {
int blkcg_init_queue ( struct request_queue * q ) <nl> if ( preloaded ) <nl> radix_tree_preload_end (); <nl>  <nl> - if ( IS_ERR ( blkg )) { <nl> - blkg_free ( new_blkg ); <nl> + if ( IS_ERR ( blkg )) <nl> return PTR_ERR ( blkg ); <nl> - } <nl>  <nl> q -> root_blkg = blkg ; <nl> q -> root_rl . blkg = blkg ;
static int s3c24xx_eint_init ( struct samsung_pinctrl_drv_data * d ) <nl> irq = bank -> eint_offset ; <nl> mask = bank -> eint_mask ; <nl> for ( pin = 0 ; mask ; ++ pin , mask >>= 1 ) { <nl> - if ( irq > NUM_EINT ) <nl> + if ( irq >= NUM_EINT ) <nl> break ; <nl> if (!( mask & 1 )) <nl> continue ;
int snd_pcm_status ( struct snd_pcm_substream * substream , <nl> runtime -> status -> audio_tstamp ; <nl> goto _tstamp_end ; <nl> } <nl> + } else { <nl> + /* get tstamp only in fallback mode and only if enabled */ <nl> + if ( runtime -> tstamp_mode == SNDRV_PCM_TSTAMP_ENABLE ) <nl> + snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> } <nl> - snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> _tstamp_end : <nl> status -> appl_ptr = runtime -> control -> appl_ptr ; <nl> status -> hw_ptr = runtime -> status -> hw_ptr ;
void usbnet_skb_return ( struct usbnet * dev , struct sk_buff * skb ) <nl> return ; <nl> } <nl>  <nl> - skb -> protocol = eth_type_trans ( skb , dev -> net ); <nl> + /* only update if unset to allow minidriver rx_fixup override */ <nl> + if ( skb -> protocol == 0 ) <nl> + skb -> protocol = eth_type_trans ( skb , dev -> net ); <nl> + <nl> dev -> net -> stats . rx_packets ++; <nl> dev -> net -> stats . rx_bytes += skb -> len ; <nl> 
static int raid10_add_disk ( mddev_t * mddev , mdk_rdev_t * rdev ) <nl> if (! enough ( conf )) <nl> return - EINVAL ; <nl>  <nl> - if ( rdev -> raid_disk ) <nl> + if ( rdev -> raid_disk >= 0 ) <nl> first = last = rdev -> raid_disk ; <nl>  <nl> if ( rdev -> saved_raid_disk >= 0 &&
static int tiadc_read_raw ( struct iio_dev * indio_dev , <nl> return - EAGAIN ; <nl> } <nl> } <nl> - map_val = chan -> channel + TOTAL_CHANNELS ; <nl> + map_val = adc_dev -> channel_step [ chan -> scan_index ]; <nl>  <nl> /* <nl> * We check the complete FIFO . We programmed just one entry but in case
static inline void ftrace_dump ( enum ftrace_dump_mode oops_dump_mode ) { } <nl> # define COMPACTION_BUILD 0 <nl> # endif <nl>  <nl> +/* This helps us to avoid # ifdef CONFIG_SYMBOL_PREFIX */ <nl> +# ifdef CONFIG_SYMBOL_PREFIX <nl> +# define SYMBOL_PREFIX CONFIG_SYMBOL_PREFIX <nl> +# else <nl> +# define SYMBOL_PREFIX "" <nl> +# endif <nl> + <nl> /* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */ <nl> # ifdef CONFIG_FTRACE_MCOUNT_RECORD <nl> # define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD
static void __init sanity_check_meminfo ( void ) <nl> * Check whether this memory bank would entirely overlap <nl> * the vmalloc area . <nl> */ <nl> - if ( __va ( bank -> start ) >= VMALLOC_MIN ) { <nl> + if ( __va ( bank -> start ) >= VMALLOC_MIN || <nl> + __va ( bank -> start ) < PAGE_OFFSET ) { <nl> printk ( KERN_NOTICE " Ignoring RAM at %. 8lx -%. 8lx " <nl> "( vmalloc region overlap ).\ n ", <nl> bank -> start , bank -> start + bank -> size - 1 );
static umode_t lm3533_attr_is_visible ( struct kobject * kobj , <nl> struct device_attribute * dattr = to_dev_attr ( attr ); <nl> struct lm3533_device_attribute * lattr = to_lm3533_dev_attr ( dattr ); <nl> enum lm3533_attribute_type type = lattr -> type ; <nl> - mode_t mode = attr -> mode ; <nl> + umode_t mode = attr -> mode ; <nl>  <nl> if (! lm3533 -> have_backlights && type == LM3533_ATTR_TYPE_BACKLIGHT ) <nl> mode = 0 ;
static void unfreeze_partials ( struct kmem_cache * s ) <nl>  <nl> new . frozen = 0 ; <nl>  <nl> - if (! new . inuse && (! n || n -> nr_partial < s -> min_partial )) <nl> + if (! new . inuse && (! n || n -> nr_partial > s -> min_partial )) <nl> m = M_FREE ; <nl> else { <nl> struct kmem_cache_node * n2 = get_node ( s ,
EXPORT_SYMBOL ( writeback_in_progress ); <nl>  <nl> struct backing_dev_info * inode_to_bdi ( struct inode * inode ) <nl> { <nl> - struct super_block * sb = inode -> i_sb ; <nl> + struct super_block * sb ; <nl> + <nl> + if (! inode ) <nl> + return & noop_backing_dev_info ; <nl> + <nl> + sb = inode -> i_sb ; <nl> # ifdef CONFIG_BLOCK <nl> if ( sb_is_blkdev_sb ( sb )) <nl> return blk_get_backing_dev_info ( I_BDEV ( inode ));
static inline void __cache_free ( struct kmem_cache * cachep , void * objp ) <nl> check_irq_off (); <nl> objp = cache_free_debugcheck ( cachep , objp , __builtin_return_address ( 0 )); <nl>  <nl> - if ( use_alien_caches && cache_free_alien ( cachep , objp )) <nl> + if ( cache_free_alien ( cachep , objp )) <nl> return ; <nl>  <nl> if ( likely ( ac -> avail < ac -> limit )) {
int aix_partition ( struct parsed_partitions * state ) <nl> numlvs = be16_to_cpu ( p -> numlvs ); <nl> put_dev_sector ( sect ); <nl> } <nl> - lvip = kzalloc ( sizeof ( struct lv_info ) * state -> limit , GFP_KERNEL ); <nl> + lvip = kcalloc ( state -> limit , sizeof ( struct lv_info ), GFP_KERNEL ); <nl> if (! lvip ) <nl> return 0 ; <nl> if ( numlvs && ( d = read_part_sector ( state , vgda_sector + 1 , & sect ))) {
static int exynos_iommu_add_device ( struct device * dev ) <nl> struct iommu_group * group ; <nl> int ret ; <nl>  <nl> + if (! has_sysmmu ( dev )) <nl> + return - ENODEV ; <nl> + <nl> group = iommu_group_get ( dev ); <nl>  <nl> if (! group ) { <nl> static int exynos_iommu_add_device ( struct device * dev ) <nl>  <nl> static void exynos_iommu_remove_device ( struct device * dev ) <nl> { <nl> + if (! has_sysmmu ( dev )) <nl> + return ; <nl> + <nl> iommu_group_remove_device ( dev ); <nl> } <nl> 
static const struct mfd_cell cros_devs [] = { <nl> . id = 2 , <nl> . of_compatible = " google , cros - ec - i2c - tunnel ", <nl> }, <nl> + { <nl> + . name = " cros - ec - ctl ", <nl> + . id = 3 , <nl> + }, <nl> }; <nl>  <nl> int cros_ec_register ( struct cros_ec_device * ec_dev )
static int __init cy_detect_isa ( void ) <nl> continue ; <nl> } <nl> # ifdef MODULE <nl> - if ( isparam && irq [ i ]) <nl> + if ( isparam && i < NR_CARDS && irq [ i ]) <nl> cy_isa_irq = irq [ i ]; <nl> else <nl> # endif
skbfree ( struct sk_buff * skb ) <nl> return ; <nl> while ( atomic_read (& skb_shinfo ( skb )-> dataref ) != 1 && i -- > 0 ) <nl> msleep ( Sms ); <nl> - if ( i <= 0 ) { <nl> + if ( i < 0 ) { <nl> printk ( KERN_ERR <nl> " aoe : % s holds ref : % s \ n ", <nl> skb -> dev ? skb -> dev -> name : " netif ",
::" a " ( rw ) : " memory ") <nl>  <nl> # define __build_write_lock_const ( rw , helper ) \ <nl> - asm volatile ( LOCK " subl $" RW_LOCK_BIAS_STR ",(% 0 )\ n \ t " \ <nl> + asm volatile ( LOCK " subl $" RW_LOCK_BIAS_STR ",% 0 \ n \ t " \ <nl> " jnz 2f \ n " \ <nl> " 1 :\ n " \ <nl> LOCK_SECTION_START ("") \
SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> if ( which > PRIO_USER || which < PRIO_PROCESS ) <nl> return - EINVAL ; <nl>  <nl> + rcu_read_lock (); <nl> read_lock (& tasklist_lock ); <nl> switch ( which ) { <nl> case PRIO_PROCESS : <nl> SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> } <nl> out_unlock : <nl> read_unlock (& tasklist_lock ); <nl> + rcu_read_unlock (); <nl>  <nl> return retval ; <nl> }
static int __init init_spkm3_module ( void ) <nl> status = gss_mech_register (& gss_spkm3_mech ); <nl> if ( status ) <nl> printk (" Failed to register spkm3 gss mechanism !\ n "); <nl> - return 0 ; <nl> + return status ; <nl> } <nl>  <nl> static void __exit cleanup_spkm3_module ( void )
static int clip_constructor ( struct neighbour * neigh ) <nl>  <nl> static int clip_encap ( struct atm_vcc * vcc , int mode ) <nl> { <nl> + if (! CLIP_VCC ( vcc )) <nl> + return - EBADFD ; <nl> + <nl> CLIP_VCC ( vcc )-> encap = mode ; <nl> return 0 ; <nl> }
static int newseg ( struct ipc_namespace * ns , struct ipc_params * params ) <nl> if ( size < SHMMIN || size > ns -> shm_ctlmax ) <nl> return - EINVAL ; <nl>  <nl> - if ( ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> + if ( ns -> shm_tot + numpages < ns -> shm_tot || <nl> + ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> return - ENOSPC ; <nl>  <nl> shp = ipc_rcu_alloc ( sizeof (* shp ));
drm_atomic_helper_wait_for_vblanks ( struct drm_device * dev , <nl> for_each_crtc_in_state ( old_state , crtc , old_crtc_state , i ) { <nl> struct drm_crtc_state * new_crtc_state = crtc -> state ; <nl>  <nl> - if (! new_crtc_state -> active ) <nl> - continue ; <nl> - <nl> - if (! drm_atomic_helper_framebuffer_changed ( dev , <nl> - old_state , crtc )) <nl> + if (! new_crtc_state -> active || ! new_crtc_state -> planes_changed ) <nl> continue ; <nl>  <nl> ret = drm_crtc_vblank_get ( crtc );
int vmw_kms_update_layout_ioctl ( struct drm_device * dev , void * data , <nl> ret = copy_from_user ( rects , user_rects , rects_size ); <nl> if ( unlikely ( ret != 0 )) { <nl> DRM_ERROR (" Failed to get rects .\ n "); <nl> + ret = - EFAULT ; <nl> goto out_free ; <nl> } <nl> 
static struct clk * cp110_register_gate ( const char * name , <nl> if (! gate ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> + memset (& init , 0 , sizeof ( init )); <nl> + <nl> init . name = name ; <nl> init . ops = & cp110_gate_ops ; <nl> init . parent_names = & parent_name ;
int __vma_adjust ( struct vm_area_struct * vma , unsigned long start , <nl> * If next doesn ' t have anon_vma , import from vma after <nl> * next , if the vma overlaps with it . <nl> */ <nl> - if ( remove_next == 2 && next && ! next -> anon_vma ) <nl> + if ( remove_next == 2 && ! next -> anon_vma ) <nl> exporter = next -> vm_next ; <nl>  <nl> } else if ( end > next -> vm_start ) {
static int pty_set_pktmode ( struct tty_struct * tty , int __user * arg ) <nl> spin_lock_irq (& tty -> ctrl_lock ); <nl> if ( pktmode ) { <nl> if (! tty -> packet ) { <nl> - tty -> packet = 1 ; <nl> tty -> link -> ctrl_status = 0 ; <nl> + smp_mb (); <nl> + tty -> packet = 1 ; <nl> } <nl> } else <nl> tty -> packet = 0 ;
static int tcp_ack_update_window ( struct sock * sk , struct tcp_sock * tp , <nl> static void tcp_conservative_spur_to_response ( struct tcp_sock * tp ) <nl> { <nl> tp -> snd_cwnd = min ( tp -> snd_cwnd , tp -> snd_ssthresh ); <nl> + tp -> snd_cwnd_cnt = 0 ; <nl> tcp_moderate_cwnd ( tp ); <nl> } <nl> 
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> + if ( rc == 0 ) <nl> + /* Evade comedi_auto_unconfig (). */ <nl> + dev_file_info -> hardware_device = NULL ; <nl> goto done ; <nl> } <nl> 
static void map_cpu_to_logical_apicid ( void ) <nl> { <nl> int cpu = smp_processor_id (); <nl> int apicid = logical_smp_processor_id (); <nl> + int node = apicid_to_node ( apicid ); <nl> + <nl> + if (! node_online ( node )) <nl> + node = first_online_node ; <nl>  <nl> cpu_2_logical_apicid [ cpu ] = apicid ; <nl> - map_cpu_to_node ( cpu , apicid_to_node ( apicid )); <nl> + map_cpu_to_node ( cpu , node ); <nl> } <nl>  <nl> static void unmap_cpu_to_logical_apicid ( int cpu )
static int kvm_dev_ioctl_get_supported_cpuid ( struct kvm_cpuid2 * cpuid , <nl> for ( func = 0x80000001 ; func <= limit && nent < cpuid -> nent ; ++ func ) <nl> do_cpuid_ent (& cpuid_entries [ nent ], func , 0 , <nl> & nent , cpuid -> nent ); <nl> + r = - E2BIG ; <nl> + if ( nent >= cpuid -> nent ) <nl> + goto out_free ; <nl> + <nl> r = - EFAULT ; <nl> if ( copy_to_user ( entries , cpuid_entries , <nl> nent * sizeof ( struct kvm_cpuid_entry2 )))
static int unix_dgram_recvmsg ( struct socket * sock , struct msghdr * msg , <nl> goto out_unlock ; <nl> } <nl>  <nl> - wake_up_interruptible_sync_poll (& u -> peer_wait , <nl> - POLLOUT | POLLWRNORM | POLLWRBAND ); <nl> + if ( wq_has_sleeper (& u -> peer_wait )) <nl> + wake_up_interruptible_sync_poll (& u -> peer_wait , <nl> + POLLOUT | POLLWRNORM | <nl> + POLLWRBAND ); <nl>  <nl> if ( msg -> msg_name ) <nl> unix_copy_addr ( msg , skb -> sk );
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static inline struct sk_buff * sk_stream_alloc_pskb ( struct sock * sk , <nl> { <nl> struct sk_buff * skb ; <nl>  <nl> + /* The TCP header must be at least 32 - bit aligned . */ <nl> + size = ALIGN ( size , 4 ); <nl> + <nl> skb = alloc_skb_fclone ( size + sk -> sk_prot -> max_header , gfp ); <nl> if ( skb ) { <nl> skb -> truesize += mem ;
static void ieee80211_handle_filtered_frame ( struct ieee80211_local * local , <nl> struct ieee80211_hdr * hdr = ( void *) skb -> data ; <nl> int ac ; <nl>  <nl> + if ( info -> flags & IEEE80211_TX_CTL_NO_PS_BUFFER ) { <nl> + ieee80211_free_txskb (& local -> hw , skb ); <nl> + return ; <nl> + } <nl> + <nl> /* <nl> * This skb ' survived ' a round - trip through the driver , and <nl> * hopefully the driver didn ' t mangle it too badly . However ,
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
int pl320_ipc_unregister_notifier ( struct notifier_block * nb ) <nl> } <nl> EXPORT_SYMBOL_GPL ( pl320_ipc_unregister_notifier ); <nl>  <nl> - static int __init pl320_probe ( struct amba_device * adev , <nl> - const struct amba_id * id ) <nl> + static int pl320_probe ( struct amba_device * adev , const struct amba_id * id ) <nl> { <nl> int ret ; <nl> 
static int dwc3_ep0_set_config ( struct dwc3 * dwc , struct usb_ctrlrequest * ctrl ) <nl> case DWC3_ADDRESS_STATE : <nl> ret = dwc3_ep0_delegate_req ( dwc , ctrl ); <nl> /* if the cfg matches and the cfg is non zero */ <nl> - if (! ret && cfg ) <nl> + if ( cfg && (! ret || ( ret == USB_GADGET_DELAYED_STATUS ))) <nl> dwc -> dev_state = DWC3_CONFIGURED_STATE ; <nl> break ; <nl> 
static int intel_pstate_set_policy ( struct cpufreq_policy * policy ) <nl> if ( policy -> policy == CPUFREQ_POLICY_PERFORMANCE ) { <nl> limits . min_perf_pct = 100 ; <nl> limits . min_perf = int_tofp ( 1 ); <nl> + limits . max_policy_pct = 100 ; <nl> limits . max_perf_pct = 100 ; <nl> limits . max_perf = int_tofp ( 1 ); <nl> limits . no_turbo = limits . turbo_disabled ;
populate_shared_memory : <nl> */ <nl> if ( ret == - EAGAIN && op_state_purged ( new_op )) { <nl> orangefs_bufmap_put ( bufmap , buffer_index ); <nl> + buffer_index = - 1 ; <nl> gossip_debug ( GOSSIP_FILE_DEBUG , <nl> "% s : going to repopulate_shared_memory .\ n ", <nl> __func__ );
static int vmx_set_msr ( struct kvm_vcpu * vcpu , u32 msr_index , u64 data ) <nl> msr = find_msr_entry ( vcpu , msr_index ); <nl> if ( msr ) <nl> msr -> data = data ; <nl> - load_msrs ( vcpu -> guest_msrs , NR_BAD_MSRS ); <nl> + if ( vcpu -> vmx_host_state . loaded ) <nl> + load_msrs ( vcpu -> guest_msrs , NR_BAD_MSRS ); <nl> break ; <nl> # endif <nl> case MSR_IA32_SYSENTER_CS :
static int mv88e6xxx_set_port_state ( struct dsa_switch * ds , int port , u8 state ) <nl> mutex_lock (& ps -> smi_mutex ); <nl>  <nl> reg = _mv88e6xxx_reg_read ( ds , REG_PORT ( port ), PORT_CONTROL ); <nl> - if ( reg < 0 ) <nl> + if ( reg < 0 ) { <nl> + ret = reg ; <nl> goto abort ; <nl> + } <nl>  <nl> oldstate = reg & PORT_CONTROL_STATE_MASK ; <nl> if ( oldstate != state ) {
static void visual_init ( struct vc_data * vc , int num , int init ) <nl> __module_get ( vc -> vc_sw -> owner ); <nl> vc -> vc_num = num ; <nl> vc -> vc_display_fg = & master_display_fg ; <nl> + if ( vc -> vc_uni_pagedir_loc ) <nl> + con_free_unimap ( vc ); <nl> vc -> vc_uni_pagedir_loc = & vc -> vc_uni_pagedir ; <nl> vc -> vc_uni_pagedir = NULL ; <nl> vc -> vc_hi_font_mask = 0 ;
int rtl8188eu_init_recv_priv ( struct adapter * padapter ) <nl> _rtw_init_queue (& precvpriv -> free_recv_buf_queue ); <nl>  <nl> precvpriv -> pallocated_recv_buf = <nl> - kzalloc ( NR_RECVBUFF * sizeof ( struct recv_buf ), GFP_KERNEL ); <nl> + kcalloc ( NR_RECVBUFF , sizeof ( struct recv_buf ), GFP_KERNEL ); <nl> if ( precvpriv -> pallocated_recv_buf == NULL ) { <nl> res = _FAIL ; <nl> RT_TRACE ( _module_rtl871x_recv_c_ , _drv_err_ ,
int drm_mode_page_flip_ioctl ( struct drm_device * dev , <nl> goto out ; <nl> crtc = obj_to_crtc ( obj ); <nl>  <nl> + if ( crtc -> fb == NULL ) { <nl> + /* The framebuffer is currently unbound , presumably <nl> + * due to a hotplug event , that userspace has not <nl> + * yet discovered . <nl> + */ <nl> + ret = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> if ( crtc -> funcs -> page_flip == NULL ) <nl> goto out ; <nl> 
int ath9k_hw_reset ( struct ath_hw * ah , struct ath9k_channel * chan , <nl>  <nl> if ( ah -> hw -> conf . radar_enabled ) { <nl> /* set HW specific DFS configuration */ <nl> + ah -> radar_conf . ext_channel = IS_CHAN_HT40 ( chan ); <nl> ath9k_hw_set_radar_params ( ah ); <nl> } <nl> 
bool i40e_is_vsi_in_vlan ( struct i40e_vsi * vsi ) <nl> * so we have to go through all the list in order to make sure <nl> */ <nl> list_for_each_entry ( f , & vsi -> mac_filter_list , list ) { <nl> - if ( f -> vlan >= 0 ) <nl> + if ( f -> vlan >= 0 || vsi -> info . pvid ) <nl> return true ; <nl> } <nl> 
static struct platform_device * crag6410_devices [] __initdata = { <nl> & s3c_device_fb , <nl> & s3c_device_ohci , <nl> & s3c_device_usb_hsotg , <nl> - & s3c_device_adc , <nl> - & s3c_device_rtc , <nl> - & s3c_device_ts , <nl> & s3c_device_timer [ 0 ], <nl> & s3c64xx_device_iis0 , <nl> & s3c64xx_device_iis1 ,
static int parse_scriptname ( const struct option * opt __used , <nl> script ++; <nl> } else { <nl> script = str ; <nl> - ext = strchr ( script , '.'); <nl> + ext = strrchr ( script , '.'); <nl> if (! ext ) { <nl> fprintf ( stderr , " invalid script extension "); <nl> return - 1 ;
static int __devinit snd_hdspm_create ( struct snd_card * card , <nl> hdspm -> card_name = " RME AES32 "; <nl> hdspm -> midiPorts = 2 ; <nl> break ; <nl> + default : <nl> + snd_printk ( KERN_ERR " HDSPM : unknown firmware revision % x \ n ", <nl> + hdspm -> firmware_rev ); <nl> + return - ENODEV ; <nl> } <nl>  <nl> err = pci_enable_device ( pci );
static int ks7010_upload_firmware ( struct ks_sdio_card * card ) <nl> unsigned char * rom_buf ; <nl> unsigned char rw_data = 0 ; <nl> int ret ; <nl> - int length ; <nl> + unsigned int length ; <nl> const struct firmware * fw_entry = NULL ; <nl>  <nl> /* buffer allocate */
struct nfs_server * nfs4_create_referral_server ( struct nfs_clone_mount * data , <nl> parent_server -> client -> cl_xprt -> prot , <nl> parent_client -> retrans_timeo , <nl> parent_client -> retrans_count ); <nl> + if ( error < 0 ) <nl> + goto error ; <nl>  <nl> /* Initialise the client representation from the parent server */ <nl> nfs_server_copy_userdata ( server , parent_server );
static int __init mpf_checksum ( unsigned char * mp , int len ) <nl> return sum & 0xFF ; <nl> } <nl>  <nl> - static void __cpuinit MP_processor_info ( struct mpc_config_processor * m ) <nl> + static void __init MP_processor_info ( struct mpc_config_processor * m ) <nl> { <nl> int apicid ; <nl> char * bootup_cpu = "";
struct ipoib_neigh * ipoib_neigh_alloc ( struct neighbour * neighbour , <nl>  <nl> neigh -> neighbour = neighbour ; <nl> neigh -> dev = dev ; <nl> + memset (& neigh -> dgid . raw , 0 , sizeof ( union ib_gid )); <nl> * to_ipoib_neigh ( neighbour ) = neigh ; <nl> skb_queue_head_init (& neigh -> queue ); <nl> ipoib_cm_set ( neigh , NULL );
again : <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> + page = compound_head ( page ); <nl> lock_page ( page ); <nl> if (! page -> mapping ) { <nl> unlock_page ( page );
static int atmel_pdmic_cpu_dai_startup ( struct snd_pcm_substream * substream , <nl> return ret ; <nl>  <nl> ret = clk_prepare_enable ( dd -> pclk ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + clk_disable_unprepare ( dd -> gclk ); <nl> return ret ; <nl> + } <nl>  <nl> /* Clear all bits in the Control Register ( PDMIC_CR ) */ <nl> regmap_write ( dd -> regmap , PDMIC_CR , 0 );
static int gb_loopback_transfer ( struct gb_loopback * gb , u32 len ) <nl> return - ENOMEM ; <nl> } <nl>  <nl> + memset ( request -> data , 0x5A , len ); <nl> + <nl> request -> len = cpu_to_le32 ( len ); <nl>  <nl> do_gettimeofday (& ts );
static int max77686_clk_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + drv_data -> num_clks = num_clks ; <nl> drv_data -> max_clk_data = devm_kcalloc ( dev , num_clks , <nl> sizeof (* drv_data -> max_clk_data ), <nl> GFP_KERNEL );
static inline void rtsx_exclusive_enter_ss ( struct rtsx_chip * chip ) <nl> { <nl> struct rtsx_dev * dev = chip -> rtsx ; <nl>  <nl> - spin_lock (&( dev -> reg_lock )); <nl> + spin_lock (& dev -> reg_lock ); <nl> rtsx_enter_ss ( chip ); <nl> - spin_unlock (&( dev -> reg_lock )); <nl> + spin_unlock (& dev -> reg_lock ); <nl> } <nl>  <nl> static inline void rtsx_reset_detected_cards ( struct rtsx_chip * chip , int flag )
static int bnx2x_populate_ext_phy ( struct bnx2x * bp , <nl> return - EINVAL ; <nl> default : <nl> * phy = phy_null ; <nl> + /* In case external PHY wasn ' t found */ <nl> + if (( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_DIRECT ) && <nl> + ( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_NOT_CONN )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static int ath9k_tx ( struct ieee80211_hw * hw , <nl> struct ath_tx_control txctl ; <nl> int hdrlen , padsize ; <nl>  <nl> + if ( aphy -> state != ATH_WIPHY_ACTIVE ) { <nl> + printk ( KERN_DEBUG " ath9k : % s : TX in unexpected wiphy state " <nl> + "% d \ n ", wiphy_name ( hw -> wiphy ), aphy -> state ); <nl> + goto exit ; <nl> + } <nl> + <nl> memset (& txctl , 0 , sizeof ( struct ath_tx_control )); <nl>  <nl> /*
noinline int btrfs_truncate_inode_items ( struct btrfs_trans_handle * trans , <nl> if ( root -> ref_cows ) <nl> btrfs_drop_extent_cache ( inode , new_size & (~ mask ), ( u64 )- 1 , 0 ); <nl> path = btrfs_alloc_path (); <nl> - path -> reada = - 1 ; <nl> BUG_ON (! path ); <nl> + path -> reada = - 1 ; <nl>  <nl> /* FIXME , add redo link to tree so we don ' t leak on crash */ <nl> key . objectid = inode -> i_ino ;
long vt_compat_ioctl ( struct tty_struct * tty , struct file * file , <nl>  <nl> case PIO_UNIMAP : <nl> case GIO_UNIMAP : <nl> - ret = do_unimap_ioctl ( cmd , up , perm , vc ); <nl> + ret = compat_unimap_ioctl ( cmd , up , perm , vc ); <nl> break ; <nl>  <nl> /*
int fscrypt_ioctl_set_policy ( struct file * filp , const void __user * arg ) <nl> printk ( KERN_WARNING <nl> "% s : Policy inconsistent with encryption context \ n ", <nl> __func__ ); <nl> - ret = - EINVAL ; <nl> + ret = - EEXIST ; <nl> } <nl>  <nl> inode_unlock ( inode );
static int acpi_cpufreq_cpu_init ( struct cpufreq_policy * policy ) <nl>  <nl> switch ( perf -> control_register . space_id ) { <nl> case ACPI_ADR_SPACE_SYSTEM_IO : <nl> + if ( boot_cpu_data . x86_vendor == X86_VENDOR_AMD && <nl> + boot_cpu_data . x86 == 0xf ) { <nl> + pr_debug (" AMD K8 systems must use native drivers .\ n "); <nl> + result = - ENODEV ; <nl> + goto err_unreg ; <nl> + } <nl> pr_debug (" SYSTEM IO addr space \ n "); <nl> data -> cpu_feature = SYSTEM_IO_CAPABLE ; <nl> break ;
struct map * machine__new_module ( struct machine * machine , u64 start , <nl> if ( kmod_path__parse_name (& m , filename )) <nl> return NULL ; <nl>  <nl> + map = map_groups__find_by_name (& machine -> kmaps , MAP__FUNCTION , <nl> + m . name ); <nl> + if ( map ) <nl> + goto out ; <nl> + <nl> dso = machine__module_dso ( machine , & m , filename ); <nl> if ( dso == NULL ) <nl> goto out ;
static void falcon_handle_rx_event ( struct efx_channel * channel , <nl> * UDP / IPv4 , then we can rely on the hardware checksum . <nl> */ <nl> checksummed = <nl> - rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_TCP || <nl> - rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_UDP ; <nl> + efx -> rx_checksum_enabled && <nl> + ( rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_TCP || <nl> + rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_UDP ); <nl> } else { <nl> falcon_handle_rx_not_ok ( rx_queue , event , & rx_ev_pkt_ok , <nl> & discard );
static int rtl8152_close ( struct net_device * netdev ) <nl> netif_stop_queue ( netdev ); <nl>  <nl> res = usb_autopm_get_interface ( tp -> intf ); <nl> - if ( res < 0 ) { <nl> + if ( res < 0 || test_bit ( RTL8152_UNPLUG , & tp -> flags )) { <nl> rtl_drop_queued_tx ( tp ); <nl> rtl_stop_rx ( tp ); <nl> } else {
static ssize_t spufs_mbox_read ( struct file * file , char __user * buf , <nl> udata = ( void __user *) buf ; <nl>  <nl> spu_acquire ( ctx ); <nl> - for ( count = 0 ; count <= len ; count += 4 , udata ++) { <nl> + for ( count = 0 ; ( count + 4 ) <= len ; count += 4 , udata ++) { <nl> int ret ; <nl> ret = ctx -> ops -> mbox_read ( ctx , & mbox_data ); <nl> if ( ret == 0 )
static int dax_pmem_probe ( struct device * dev ) <nl> nsio = to_nd_namespace_io (& ndns -> dev ); <nl>  <nl> /* parse the ' pfn ' info block via -> rw_bytes */ <nl> - devm_nsio_enable ( dev , nsio ); <nl> + rc = devm_nsio_enable ( dev , nsio ); <nl> + if ( rc ) <nl> + return rc ; <nl> altmap = nvdimm_setup_pfn ( nd_pfn , & res , & __altmap ); <nl> if ( IS_ERR ( altmap )) <nl> return PTR_ERR ( altmap );
static void doc_delay ( struct docg3 * docg3 , int nbNOPs ) <nl> { <nl> int i ; <nl>  <nl> - doc_dbg (" NOP x % d \ n ", nbNOPs ); <nl> + doc_vdbg (" NOP x % d \ n ", nbNOPs ); <nl> for ( i = 0 ; i < nbNOPs ; i ++) <nl> doc_writeb ( docg3 , 0 , DOC_NOP ); <nl> }
static int be_resume ( struct pci_dev * pdev ) <nl> pci_set_power_state ( pdev , PCI_D0 ); <nl> pci_restore_state ( pdev ); <nl>  <nl> + status = be_fw_wait_ready ( adapter ); <nl> + if ( status ) <nl> + return status ; <nl> + <nl> /* tell fw we ' re ready to fire cmds */ <nl> status = be_cmd_fw_init ( adapter ); <nl> if ( status )
relookup : <nl> p -> rate_last = 0 ; <nl> p -> pmtu_expires = 0 ; <nl> p -> pmtu_orig = 0 ; <nl> + p -> redirect_genid = 0 ; <nl> memset (& p -> redirect_learned , 0 , sizeof ( p -> redirect_learned )); <nl>  <nl> 
ieee80211softmac_assoc_req ( struct ieee80211_assoc_request ** pkt , <nl> return 0 ; <nl> ieee80211softmac_hdr_3addr ( mac , &((* pkt )-> header ), IEEE80211_STYPE_ASSOC_REQ , net -> bssid , net -> bssid ); <nl>  <nl> + /* Fill in the capabilities */ <nl> + (* pkt )-> capability = ieee80211softmac_capabilities ( mac , net ); <nl> + <nl> /* Fill in Listen Interval (?) */ <nl> (* pkt )-> listen_interval = cpu_to_le16 ( 10 ); <nl> 
static int i2s_startup ( struct snd_pcm_substream * substream , <nl> /* Enforce set_sysclk in Master mode */ <nl> i2s -> rclk_srcrate = 0 ; <nl>  <nl> + if (! any_active ( i2s ) && ( i2s -> quirks & QUIRK_NEED_RSTCLR )) <nl> + writel ( CON_RSTCLR , i2s -> addr + I2SCON ); <nl> + <nl> spin_unlock_irqrestore (& lock , flags ); <nl>  <nl> return 0 ;
int mxc_initialize_usb_hw ( int port , unsigned int flags ) <nl> # ifdef CONFIG_ARCH_MX51 <nl> if ( cpu_is_mx51 ()) { <nl> void __iomem * usb_base ; <nl> - u32 usbotg_base ; <nl> - u32 usbother_base ; <nl> + void __iomem * usbotg_base ; <nl> + void __iomem * usbother_base ; <nl> int ret = 0 ; <nl>  <nl> usb_base = ioremap ( MX51_OTG_BASE_ADDR , SZ_4K );
int iwl_mvm_scan_request ( struct iwl_mvm * mvm , <nl> basic_ssid ? 1 : 0 ); <nl>  <nl> cmd -> tx_cmd . tx_flags = cpu_to_le32 ( TX_CMD_FLG_SEQ_CTL | <nl> - TX_CMD_FLG_BT_DIS ); <nl> + 3 << TX_CMD_FLG_BT_PRIO_POS ); <nl> + <nl> cmd -> tx_cmd . sta_id = mvm -> aux_sta . sta_id ; <nl> cmd -> tx_cmd . life_time = cpu_to_le32 ( TX_CMD_LIFE_TIME_INFINITE ); <nl> cmd -> tx_cmd . rate_n_flags =
static int digi_read_oob_callback ( struct urb * urb ) <nl> return - 1 ; <nl>  <nl> /* handle each oob command */ <nl> - for ( i = 0 ; i < urb -> actual_length - 4 ; i += 4 ) { <nl> + for ( i = 0 ; i < urb -> actual_length - 3 ; i += 4 ) { <nl> opcode = buf [ i ]; <nl> line = buf [ i + 1 ]; <nl> status = buf [ i + 2 ];
static int fsg_setup ( struct usb_function * f , <nl> *( u8 *) req -> buf = fsg -> common -> nluns - 1 ; <nl>  <nl> /* Respond with data / status */ <nl> - req -> length = min ( 1 , w_length ); <nl> + req -> length = min (( u16 ) 1 , w_length ); <nl> fsg -> common -> ep0req_name = <nl> ctrl -> bRequestType & USB_DIR_IN ? " ep0 - in " : " ep0 - out "; <nl> return ep0_queue ( fsg -> common );
void exynos4_jpeg_set_enc_out_fmt ( void __iomem * base , unsigned int out_fmt ) <nl>  <nl> void exynos4_jpeg_set_interrupt ( void __iomem * base ) <nl> { <nl> - unsigned int reg ; <nl> - <nl> - reg = readl ( base + EXYNOS4_INT_EN_REG ) & ~ EXYNOS4_INT_EN_MASK ; <nl> writel ( EXYNOS4_INT_EN_ALL , base + EXYNOS4_INT_EN_REG ); <nl> } <nl> 
static int sep_prepare_input_output_dma_table_in_dcb ( struct sep_device * sep , <nl> } <nl> } <nl> if ( tail_size ) { <nl> + if ( tail_size > sizeof ( dcb_table_ptr -> tail_data )) <nl> + return - EINVAL ; <nl> if ( is_kva == true ) { <nl> memcpy ( dcb_table_ptr -> tail_data , <nl> ( void *)( app_in_address + data_in_size -
static const char * sata_pmp_spec_rev_str ( const u32 * gscr ) <nl> { <nl> u32 rev = gscr [ SATA_PMP_GSCR_REV ]; <nl>  <nl> + if ( rev & ( 1 << 3 )) <nl> + return " 1 . 2 "; <nl> if ( rev & ( 1 << 2 )) <nl> return " 1 . 1 "; <nl> if ( rev & ( 1 << 1 ))
static int of_mpc8xxx_spi_get_chipselects ( struct device * dev ) <nl> gpio = of_get_gpio_flags ( np , i , & flags ); <nl> if (! gpio_is_valid ( gpio )) { <nl> dev_err ( dev , " invalid gpio #% d : % d \ n ", i , gpio ); <nl> + ret = gpio ; <nl> goto err_loop ; <nl> } <nl> 
void radeon_atom_backlight_init ( struct radeon_encoder * radeon_encoder , <nl> u8 backlight_level ; <nl> char bl_name [ 16 ]; <nl>  <nl> + /* Mac laptops with multiple GPUs use the gmux driver for backlight <nl> + * so don ' t register a backlight device <nl> + */ <nl> + if (( rdev -> pdev -> subsystem_vendor == PCI_VENDOR_ID_APPLE ) && <nl> + ( rdev -> pdev -> device == 0x6741 )) <nl> + return ; <nl> + <nl> if (! radeon_encoder -> enc_priv ) <nl> return ; <nl> 
static int rt298_i2c_probe ( struct i2c_client * i2c , <nl>  <nl> /* enable jack combo mode on supported devices */ <nl> acpiid = acpi_match_device ( dev -> driver -> acpi_match_table , dev ); <nl> - if ( acpiid ) { <nl> + if ( acpiid && acpiid -> driver_data ) { <nl> rt298 -> pdata = *( struct rt298_platform_data *) <nl> acpiid -> driver_data ; <nl> }
static int dvb_register ( struct cx23885_tsport * port ) <nl>  <nl> fe = dvb_attach ( xc4000_attach , fe0 -> dvb . frontend , <nl> & dev -> i2c_bus [ 1 ]. i2c_adap , & cfg ); <nl> + if (! fe ) { <nl> + printk ( KERN_ERR "% s / 2 : xc4000 attach failed \ n ", <nl> + dev -> name ); <nl> + goto frontend_detach ; <nl> + } <nl> } <nl> break ; <nl> case CX23885_BOARD_TBS_6920 :
static void reset_connection ( struct ceph_connection * con ) <nl> con -> out_msg = NULL ; <nl> } <nl> con -> in_seq = 0 ; <nl> + con -> in_seq_acked = 0 ; <nl> } <nl>  <nl> /*
int snd_hda_parse_pin_def_config ( struct hda_codec * codec , <nl> memmove ( sequences_hp + i , sequences_hp + i + 1 , <nl> sizeof ( sequences_hp [ 0 ]) * ( cfg -> hp_outs - i )); <nl> } <nl> + memset ( cfg -> hp_pins + cfg -> hp_outs , 0 , <nl> + sizeof ( hda_nid_t ) * ( AUTO_CFG_MAX_OUTS - cfg -> hp_outs )); <nl> } <nl>  <nl> /* sort by sequence */
static void sta_ps_start ( struct sta_info * sta ) <nl> for ( tid = 0 ; tid < ARRAY_SIZE ( sta -> sta . txq ); tid ++) { <nl> struct txq_info * txqi = to_txq_info ( sta -> sta . txq [ tid ]); <nl>  <nl> - if (! txqi -> tin . backlog_packets ) <nl> + if ( txqi -> tin . backlog_packets ) <nl> set_bit ( tid , & sta -> txq_buffered_tids ); <nl> else <nl> clear_bit ( tid , & sta -> txq_buffered_tids );
static int __devinit virtcons_probe ( struct virtio_device * dev ) <nl> struct virtqueue * vqs [ 2 ]; <nl> int err ; <nl>  <nl> + if ( vdev ) { <nl> + dev_warn (& vdev -> dev , <nl> + " Multiple virtio - console devices not supported yet \ n "); <nl> + return - EEXIST ; <nl> + } <nl> vdev = dev ; <nl>  <nl> /* This is the scratch page we use to receive console input */
extern void __qdisc_run ( struct Qdisc * q ); <nl>  <nl> static inline void qdisc_run ( struct Qdisc * q ) <nl> { <nl> - if (! test_and_set_bit ( __QDISC_STATE_RUNNING , & q -> state )) <nl> + struct netdev_queue * txq = q -> dev_queue ; <nl> + <nl> + if (! netif_tx_queue_stopped ( txq ) && <nl> + ! test_and_set_bit ( __QDISC_STATE_RUNNING , & q -> state )) <nl> __qdisc_run ( q ); <nl> } <nl> 
try_mount_again : <nl>  <nl> remote_path_check : <nl> /* check if a whole path ( including prepath ) is not remote */ <nl> - if (! rc && cifs_sb -> prepathlen && tcon ) { <nl> + if (! rc && tcon ) { <nl> /* build_path_to_root works only when we have a valid tcon */ <nl> full_path = cifs_build_path_to_root ( cifs_sb , tcon ); <nl> if ( full_path == NULL ) {
static int alloc_iommu ( struct dmar_drhd_unit * drhd ) <nl> intel_iommu_groups , <nl> "% s ", iommu -> name ); <nl>  <nl> + if ( IS_ERR ( iommu -> iommu_dev )) { <nl> + drhd -> iommu = NULL ; <nl> + err = PTR_ERR ( iommu -> iommu_dev ); <nl> + goto err_unmap ; <nl> + } <nl> + <nl> return 0 ; <nl>  <nl> err_unmap :
static int device_change_notifier ( struct notifier_block * nb , <nl> case BUS_NOTIFY_UNBOUND_DRIVER : <nl> if (! domain ) <nl> goto out ; <nl> + if ( iommu_pass_through ) <nl> + break ; <nl> detach_device ( domain , devid ); <nl> break ; <nl> case BUS_NOTIFY_ADD_DEVICE :
static int __of_iio_channel_get ( struct iio_channel * channel , <nl> channel -> indio_dev = indio_dev ; <nl> index = iiospec . args_count ? iiospec . args [ 0 ] : 0 ; <nl> if ( index >= indio_dev -> num_channels ) { <nl> - return - EINVAL ; <nl> + err = - EINVAL ; <nl> goto err_put ; <nl> } <nl> channel -> channel = & indio_dev -> channels [ index ];
static int uas_find_endpoints ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < n_endpoints ; i ++) { <nl> unsigned char * extra = endpoint [ i ]. extra ; <nl> int len = endpoint [ i ]. extralen ; <nl> - while ( len > 1 ) { <nl> + while ( len >= 3 ) { <nl> if ( extra [ 1 ] == USB_DT_PIPE_USAGE ) { <nl> unsigned pipe_id = extra [ 2 ]; <nl> if ( pipe_id > 0 && pipe_id < 5 )
static int skl_probe ( struct pci_dev * pci , <nl> if ( err < 0 ) <nl> goto out_free ; <nl>  <nl> + skl -> nhlt = skl_nhlt_init ( bus -> dev ); <nl> + <nl> + if ( skl -> nhlt == NULL ) <nl> + goto out_free ; <nl> + <nl> pci_set_drvdata ( skl -> pci , ebus ); <nl>  <nl> /* check if dsp is there */
static int __inject_sigp_stop ( struct kvm_s390_local_interrupt * li , int action ) <nl> inti -> type = KVM_S390_SIGP_STOP ; <nl>  <nl> spin_lock_bh (& li -> lock ); <nl> - if (( atomic_read ( li -> cpuflags ) & CPUSTAT_STOPPED )) <nl> + if (( atomic_read ( li -> cpuflags ) & CPUSTAT_STOPPED )) { <nl> + kfree ( inti ); <nl> goto out ; <nl> + } <nl> list_add_tail (& inti -> list , & li -> list ); <nl> atomic_set (& li -> active , 1 ); <nl> atomic_set_mask ( CPUSTAT_STOP_INT , li -> cpuflags );
i915_error_object_create_sized ( struct drm_i915_private * dev_priv , <nl> goto unwind ; <nl>  <nl> local_irq_save ( flags ); <nl> - if ( reloc_offset < dev_priv -> gtt . mappable_end && <nl> + if ( src -> cache_level == I915_CACHE_NONE && <nl> + reloc_offset < dev_priv -> gtt . mappable_end && <nl> src -> has_global_gtt_mapping && <nl> i915_is_ggtt ( vm )) { <nl> void __iomem * s ;
static void alloc_mem ( void ** dst , void ** src , size_t length ) <nl> * src = zalloc ( length ); <nl> if (!* src ) <nl> die (" memory allocation failed - maybe length is too large ?\ n "); <nl> + /* Make sure to always replace the zero pages even if MMAP_THRESH is crossed */ <nl> + memset (* src , 0 , length ); <nl> } <nl>  <nl> static u64 do_memcpy_cycle ( memcpy_t fn , size_t len , bool prefault )
static void __init setup_hwcaps ( void ) <nl> void __init <nl> setup_arch ( char ** cmdline_p ) <nl> { <nl> + /* set up preferred console */ <nl> + add_preferred_console (" ttyS ", 0 , NULL ); <nl> + <nl> /* <nl> * print what head . S has found out about the machine <nl> */
static int cciss_bigpassthru ( ctlr_info_t * h , void __user * argp ) <nl> return - EINVAL ; <nl> if (! capable ( CAP_SYS_RAWIO )) <nl> return - EPERM ; <nl> - ioc = ( BIG_IOCTL_Command_struct *) <nl> - kmalloc ( sizeof (* ioc ), GFP_KERNEL ); <nl> + ioc = kmalloc ( sizeof (* ioc ), GFP_KERNEL ); <nl> if (! ioc ) { <nl> status = - ENOMEM ; <nl> goto cleanup1 ;
static int vhost_scsi_set_endpoint ( <nl> /* Flushing the vhost_work acts as synchronize_rcu */ <nl> mutex_lock (& vq -> mutex ); <nl> rcu_assign_pointer ( vq -> private_data , vs_tpg ); <nl> + vhost_init_used ( vq ); <nl> mutex_unlock (& vq -> mutex ); <nl> } <nl> ret = 0 ;
static int dw_msi_setup_irq ( struct msi_controller * chip , struct pci_dev * pdev , <nl> struct msi_msg msg ; <nl> struct pcie_port * pp = sys_to_pcie ( pdev -> bus -> sysdata ); <nl>  <nl> + if ( desc -> msi_attrib . is_msix ) <nl> + return - EINVAL ; <nl> + <nl> irq = assign_irq ( 1 , desc , & pos ); <nl> if ( irq < 0 ) <nl> return irq ;
static void __init vpac270_onenand_init ( void ) {} <nl> # if defined ( CONFIG_MMC_PXA ) || defined ( CONFIG_MMC_PXA_MODULE ) <nl> static struct pxamci_platform_data vpac270_mci_platform_data = { <nl> . ocr_mask = MMC_VDD_32_33 | MMC_VDD_33_34 , <nl> + . gpio_power = - 1 , <nl> . gpio_card_detect = GPIO53_VPAC270_SD_DETECT_N , <nl> . gpio_card_ro = GPIO52_VPAC270_SD_READONLY , <nl> . detect_delay_ms = 200 ,
BYTE byPwr = pDevice -> byCCKPwr ; <nl> return TRUE ; <nl> } <nl>  <nl> + if ( uCH == 0 ) <nl> + return - EINVAL ; <nl> + <nl> switch ( uRATE ) { <nl> case RATE_1M : <nl> case RATE_2M :
static int alc_cap_getput_caller ( struct snd_kcontrol * kcontrol , <nl> { <nl> struct hda_codec * codec = snd_kcontrol_chip ( kcontrol ); <nl> struct alc_spec * spec = codec -> spec ; <nl> - int i , err ; <nl> + int i , err = 0 ; <nl>  <nl> mutex_lock (& codec -> control_mutex ); <nl> if ( check_adc_switch && spec -> dual_adc_switch ) {
static void acm_tty_flush_chars ( struct tty_struct * tty ) <nl> int err ; <nl> unsigned long flags ; <nl>  <nl> + if (! cur ) /* nothing to do */ <nl> + return ; <nl> + <nl> acm -> putbuffer = NULL ; <nl> err = usb_autopm_get_interface_async ( acm -> control ); <nl> spin_lock_irqsave (& acm -> write_lock , flags ); <nl> if ( err < 0 ) { <nl> cur -> use = 0 ; <nl> + acm -> putbuffer = cur ; <nl> goto out ; <nl> } <nl> 
static int digi_read_oob_callback ( struct urb * urb ) <nl> return - 1 ; <nl>  <nl> /* handle each oob command */ <nl> - for ( i = 0 ; i < urb -> actual_length - 4 ; i += 4 ) { <nl> + for ( i = 0 ; i < urb -> actual_length - 3 ; i += 4 ) { <nl> opcode = buf [ i ]; <nl> line = buf [ i + 1 ]; <nl> status = buf [ i + 2 ];
parse_dcb20_entry ( struct drm_device * dev , struct dcb_table * dcb , <nl> case 0 : <nl> entry -> dpconf . link_bw = 162000 ; <nl> break ; <nl> - default : <nl> + case 1 : <nl> entry -> dpconf . link_bw = 270000 ; <nl> break ; <nl> + default : <nl> + entry -> dpconf . link_bw = 540000 ; <nl> + break ; <nl> } <nl> switch (( conf & 0x0f000000 ) >> 24 ) { <nl> case 0xf :
static int econet_sendmsg ( struct kiocb * iocb , struct socket * sock , <nl> udpdest . sin_addr . s_addr = htonl ( network | addr . station ); <nl> } <nl>  <nl> + memset (& ah , 0 , sizeof ( ah )); <nl> ah . port = port ; <nl> ah . cb = cb & 0x7f ; <nl> ah . code = 2 ; /* magic */ <nl> - ah . pad = 0 ; <nl>  <nl> /* tack our header on the front of the iovec */ <nl> size = sizeof ( struct aunhdr );
static void scrub_print_warning ( const char * errstr , struct scrub_block * sblock ) <nl> u64 flags = 0 ; <nl> u64 ref_root ; <nl> u32 item_size ; <nl> - u8 ref_level ; <nl> + u8 ref_level = 0 ; <nl> int ret ; <nl>  <nl> WARN_ON ( sblock -> page_count < 1 );
static int ucc_hdlc_probe ( struct platform_device * pdev ) <nl>  <nl> err_hdlc_init : <nl> err_miss_tsa_property : <nl> - kfree ( uhdlc_priv ); <nl> if ( uhdlc_priv -> tsa ) <nl> kfree ( utdm ); <nl> err_alloc_utdm :
done : <nl> /* Timeouts occur when the device isn ' t connected , so they ' re <nl> * " normal " -- don ' t fill the kernel log with these */ <nl> if ( status & DP_AUX_CH_CTL_TIME_OUT_ERROR ) { <nl> - DRM_DEBUG_KMS (" dp_aux_ch timeout status 0x % 08x \ n ", status ); <nl> + DRM_DEBUG_KMS_RATELIMITED (" dp_aux_ch timeout status 0x % 08x \ n ", <nl> + status ); <nl> ret = - ETIMEDOUT ; <nl> goto out ; <nl> }
void flush_dcache_page ( struct page * page ) <nl> } <nl> EXPORT_SYMBOL ( flush_dcache_page ); <nl>  <nl> + void flush_kernel_dcache_page ( struct page * page ) <nl> +{ <nl> + __cpuc_flush_dcache_area ( page_address ( page ), PAGE_SIZE ); <nl> +} <nl> + EXPORT_SYMBOL ( flush_kernel_dcache_page ); <nl> + <nl> void copy_to_user_page ( struct vm_area_struct * vma , struct page * page , <nl> unsigned long uaddr , void * dst , const void * src , <nl> unsigned long len )
static int sctp_send_asconf_del_ip ( struct sock * sk , <nl> continue ; <nl> asoc -> asconf_addr_del_pending = <nl> kzalloc ( sizeof ( union sctp_addr ), GFP_ATOMIC ); <nl> + if ( asoc -> asconf_addr_del_pending == NULL ) { <nl> + retval = - ENOMEM ; <nl> + goto out ; <nl> + } <nl> asoc -> asconf_addr_del_pending -> sa . sa_family = <nl> addrs -> sa_family ; <nl> asoc -> asconf_addr_del_pending -> v4 . sin_port =
static sint _init_mlme_priv ( struct _adapter * padapter ) <nl> _init_queue (&( pmlmepriv -> scanned_queue )); <nl> set_scanned_network_val ( pmlmepriv , 0 ); <nl> memset (& pmlmepriv -> assoc_ssid , 0 , sizeof ( struct ndis_802_11_ssid )); <nl> - pbuf = kmalloc ( MAX_BSS_CNT * ( sizeof ( struct wlan_network )), <nl> - GFP_ATOMIC ); <nl> + pbuf = kmalloc_array ( MAX_BSS_CNT , sizeof ( struct wlan_network ), <nl> + GFP_ATOMIC ); <nl> if ( pbuf == NULL ) <nl> return _FAIL ; <nl> pmlmepriv -> free_bss_buf = pbuf ;
static void line6_stream_stop ( struct snd_line6_pcm * line6pcm , int direction , <nl> spin_lock_irqsave (& pstr -> lock , flags ); <nl> clear_bit ( type , & pstr -> running ); <nl> if (! pstr -> running ) { <nl> + spin_unlock_irqrestore (& pstr -> lock , flags ); <nl> line6_unlink_audio_urbs ( line6pcm , pstr ); <nl> + spin_lock_irqsave (& pstr -> lock , flags ); <nl> if ( direction == SNDRV_PCM_STREAM_CAPTURE ) { <nl> line6pcm -> prev_fbuf = NULL ; <nl> line6pcm -> prev_fsize = 0 ;
static void rockchip_drm_unbind ( struct device * dev ) <nl> rockchip_drm_fbdev_fini ( drm_dev ); <nl> drm_kms_helper_poll_fini ( drm_dev ); <nl>  <nl> + drm_atomic_helper_shutdown ( drm_dev ); <nl> drm_vblank_cleanup ( drm_dev ); <nl> component_unbind_all ( dev , drm_dev ); <nl> drm_mode_config_cleanup ( drm_dev );
enum mtd_file_modes { <nl> MTD_FILE_MODE_RAW , <nl> }; <nl>  <nl> + static inline int mtd_type_is_nand_user ( const struct mtd_info_user * mtd ) <nl> +{ <nl> + return mtd -> type == MTD_NANDFLASH || mtd -> type == MTD_MLCNANDFLASH ; <nl> +} <nl> + <nl> # endif /* __MTD_ABI_H__ */
int tipc_nl_add_monitor_peer ( struct net * net , struct tipc_nl_msg * msg , <nl> u32 bearer_id , u32 * prev_node ) <nl> { <nl> struct tipc_monitor * mon = tipc_monitor ( net , bearer_id ); <nl> - struct tipc_peer * peer = mon -> self ; <nl> + struct tipc_peer * peer ; <nl>  <nl> if (! mon ) <nl> return - EINVAL ; <nl>  <nl> read_lock_bh (& mon -> lock ); <nl> + peer = mon -> self ; <nl> do { <nl> if (* prev_node ) { <nl> if ( peer -> addr == * prev_node )
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> fail_workqueue : <nl> wiphy_unregister ( local -> hw . wiphy ); <nl> fail_wiphy_register : <nl> - kfree ( local -> int_scan_req -> channels ); <nl> + kfree ( local -> int_scan_req ); <nl> return result ; <nl> } <nl> EXPORT_SYMBOL ( ieee80211_register_hw );
qlcnic_setup_netdev ( struct qlcnic_adapter * adapter , struct net_device * netdev , <nl> if ( err ) <nl> return err ; <nl>  <nl> + qlcnic_dcb_init_dcbnl_ops ( adapter -> dcb ); <nl> + <nl> err = register_netdev ( netdev ); <nl> if ( err ) { <nl> dev_err (& pdev -> dev , " failed to register net device \ n "); <nl> return err ; <nl> } <nl>  <nl> - qlcnic_dcb_init_dcbnl_ops ( adapter -> dcb ); <nl> - <nl> return 0 ; <nl> } <nl> 
void sync_timeline_signal ( struct sync_timeline * obj ) <nl> list_for_each_entry_safe ( pt , next , & obj -> active_list_head , <nl> active_list ) { <nl> if ( fence_is_signaled_locked (& pt -> base )) <nl> - list_del (& pt -> active_list ); <nl> + list_del_init (& pt -> active_list ); <nl> } <nl>  <nl> spin_unlock_irqrestore (& obj -> child_list_lock , flags );
u16 hpi_entity_alloc_and_pack ( const enum e_entity_type type , <nl> if ( hE ) <nl> return hE ; <nl>  <nl> - HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_ROLE ); <nl> + HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_TYPE ); <nl>  <nl> bytes_to_copy = entity_type_to_size [ type ] * item_count ; <nl> total_size = hpi_entity_header_size (* entity ) + bytes_to_copy ;
static int orinoco_ioctl_set_auth ( struct net_device * dev , <nl> */ <nl> if ( param -> value ) { <nl> priv -> tkip_cm_active = 1 ; <nl> - ret = hermes_enable_port ( hw , 0 ); <nl> + ret = hermes_disable_port ( hw , 0 ); <nl> } else { <nl> priv -> tkip_cm_active = 0 ; <nl> - ret = hermes_disable_port ( hw , 0 ); <nl> + ret = hermes_enable_port ( hw , 0 ); <nl> } <nl> break ; <nl> 
static struct platform_driver td_driver = { <nl> . owner = THIS_MODULE , <nl> }, <nl> . probe = td_probe , <nl> - . remove = __exit_p ( td_remove ), <nl> + . remove = td_remove , <nl> }; <nl>  <nl> module_platform_driver ( td_driver );
void vmbus_close ( struct vmbus_channel * channel ) <nl> free_channel ( channel ); <nl> } <nl> } <nl> + EXPORT_SYMBOL_GPL ( vmbus_close ); <nl>  <nl> /** <nl> * vmbus_sendpacket () - Send the specified buffer on the given channel
int mmc_send_app_op_cond ( struct mmc_host * host , u32 ocr , u32 * rocr ) <nl> mmc_delay ( 10 ); <nl> } <nl>  <nl> + if (! i ) <nl> + pr_err ("% s : card never left busy state \ n ", mmc_hostname ( host )); <nl> + <nl> if ( rocr && ! mmc_host_is_spi ( host )) <nl> * rocr = cmd . resp [ 0 ]; <nl> 
static void carl9170_ps_beacon ( struct ar9170 * ar , void * data , unsigned int len ) <nl> cam = ieee80211_check_tim ( tim_ie , tim_len , ar -> common . curaid ); <nl>  <nl> /* 2 . Maybe the AP wants to send multicast / broadcast data ? */ <nl> - cam = !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl> + cam |= !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl>  <nl> if (! cam ) { <nl> /* back to low - power land . */
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> int bios_reserved = 0 ; <nl>  <nl> + if ( dev_priv -> gtt . stolen_size == 0 ) <nl> + return 0 ; <nl> + <nl> dev_priv -> mm . stolen_base = i915_stolen_to_physical ( dev ); <nl> if ( dev_priv -> mm . stolen_base == 0 ) <nl> return 0 ;
nv130_chipset = { <nl> static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> + . bios = nvkm_bios_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
static void blk_stat_flush_batch ( struct blk_rq_stat * stat ) <nl>  <nl> static void blk_stat_sum ( struct blk_rq_stat * dst , struct blk_rq_stat * src ) <nl> { <nl> + blk_stat_flush_batch ( src ); <nl> + <nl> if (! src -> nr_samples ) <nl> return ; <nl>  <nl> - blk_stat_flush_batch ( src ); <nl> - <nl> dst -> min = min ( dst -> min , src -> min ); <nl> dst -> max = max ( dst -> max , src -> max ); <nl> 
void x86_pci_root_bus_res_quirks ( struct pci_bus * b ) <nl> int j ; <nl> struct pci_root_info * info ; <nl>  <nl> + /* don ' t go for it if _CRS is used */ <nl> + if ( pci_probe & PCI_USE__CRS ) <nl> + return ; <nl> + <nl> /* if only one root bus , don ' t need to anything */ <nl> if ( pci_root_num < 2 ) <nl> return ;
static int fill_buffer ( struct debug_buffer * buf ) <nl> int ret = 0 ; <nl>  <nl> if (! buf -> output_buf ) <nl> - buf -> output_buf = ( char *) vmalloc ( buf -> alloc_size ); <nl> + buf -> output_buf = vmalloc ( buf -> alloc_size ); <nl>  <nl> if (! buf -> output_buf ) { <nl> ret = - ENOMEM ;
long amdgpu_fence_wait_seq_timeout ( struct amdgpu_device * adev , u64 * target_seq , <nl> bool signaled ; <nl> int i , r ; <nl>  <nl> + if ( timeout == 0 ) { <nl> + return amdgpu_fence_any_seq_signaled ( adev , target_seq ); <nl> + } <nl> + <nl> while (! amdgpu_fence_any_seq_signaled ( adev , target_seq )) { <nl>  <nl> /* Save current sequence values , used to check for GPU lockups */
u32 ft_get_task_tag ( struct se_cmd * se_cmd ) <nl> { <nl> struct ft_cmd * cmd = container_of ( se_cmd , struct ft_cmd , se_cmd ); <nl>  <nl> + if ( cmd -> aborted ) <nl> + return ~ 0 ; <nl> return fc_seq_exch ( cmd -> seq )-> rxid ; <nl> } <nl> 
ecryptfs_add_global_auth_tok ( struct ecryptfs_mount_crypt_stat * mount_crypt_stat , <nl> struct ecryptfs_global_auth_tok * new_auth_tok ; <nl> int rc = 0 ; <nl>  <nl> - new_auth_tok = kmem_cache_alloc ( ecryptfs_global_auth_tok_cache , <nl> + new_auth_tok = kmem_cache_zalloc ( ecryptfs_global_auth_tok_cache , <nl> GFP_KERNEL ); <nl> if (! new_auth_tok ) { <nl> rc = - ENOMEM ;
asmlinkage long sys_oabi_semtimedop ( int semid , <nl> long err ; <nl> int i ; <nl>  <nl> - if ( nsops < 1 ) <nl> + if ( nsops < 1 || nsops > SEMOPM ) <nl> return - EINVAL ; <nl> sops = kmalloc ( sizeof (* sops ) * nsops , GFP_KERNEL ); <nl> if (! sops )
next_file : <nl> mnt_drop_write_file ( dst_file ); <nl> next_loop : <nl> fdput ( dst_fd ); <nl> + <nl> + if ( fatal_signal_pending ( current )) <nl> + goto out ; <nl> } <nl>  <nl> out :
static int gfx_v9_0_rlc_resume ( struct amdgpu_device * adev ) <nl> { <nl> int r ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> gfx_v9_0_rlc_stop ( adev ); <nl>  <nl> /* disable CG */
static int port_fops_open ( struct inode * inode , struct file * filp ) <nl>  <nl> /* We get the port with a kref here */ <nl> port = find_port_by_devt ( cdev -> dev ); <nl> + if (! port ) { <nl> + /* Port was unplugged before we could proceed */ <nl> + return - ENXIO ; <nl> + } <nl> filp -> private_data = port ; <nl>  <nl> /*
static int rbd_get_client ( struct rbd_device * rbd_dev , const char * mon_addr , <nl> rbdc = __rbd_client_find ( opt ); <nl> if ( rbdc ) { <nl> ceph_destroy_options ( opt ); <nl> + kfree ( rbd_opts ); <nl>  <nl> /* using an existing client */ <nl> kref_get (& rbdc -> kref );
static int __init ubi_mtd_param_parse ( const char * val , struct kernel_param * kp ) <nl> char * pbuf = & buf [ 0 ]; <nl> char * tokens [ 3 ] = { NULL , NULL , NULL }; <nl>  <nl> + if (! val ) <nl> + return - EINVAL ; <nl> + <nl> if ( mtd_devs == UBI_MAX_DEVICES ) { <nl> printk (" UBI error : too many parameters , max . is % d \ n ", <nl> UBI_MAX_DEVICES );
static ssize_t sn2_ptc_proc_write ( struct file * file , const char __user * user , si <nl> int cpu ; <nl> char optstr [ 64 ]; <nl>  <nl> + if ( count > sizeof ( optstr )) <nl> + return - EINVAL ; <nl> if ( copy_from_user ( optstr , user , count )) <nl> return - EFAULT ; <nl> optstr [ count - 1 ] = '\ 0 ';
static int moxart_gpio_probe ( struct platform_device * pdev ) <nl> gc -> parent = dev ; <nl> gc -> owner = THIS_MODULE ; <nl>  <nl> - ret = gpiochip_add_data ( gc , NULL ); <nl> + ret = devm_gpiochip_add_data ( dev , gc , NULL ); <nl> if ( ret ) { <nl> dev_err ( dev , "% s : gpiochip_add failed \ n ", <nl> dev -> of_node -> full_name );
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
static void WILC_WFI_mon_setup ( struct net_device * dev ) <nl> ether_setup ( dev ); <nl> dev -> tx_queue_len = 0 ; <nl> dev -> type = ARPHRD_IEEE80211_RADIOTAP ; <nl> - memset ( dev -> dev_addr , 0 , ETH_ALEN ); <nl> + eth_zero_addr ( dev -> dev_addr ); <nl>  <nl> # ifdef USE_WIRELESS <nl> {
static struct sysfs_dirent * sysfs_new_dirent ( struct sysfs_dirent * parent_sd , <nl>  <nl> memset ( sd , 0 , sizeof (* sd )); <nl> atomic_set (& sd -> s_count , 1 ); <nl> - atomic_set (& sd -> s_event , 0 ); <nl> + atomic_set (& sd -> s_event , 1 ); <nl> INIT_LIST_HEAD (& sd -> s_children ); <nl> list_add (& sd -> s_sibling , & parent_sd -> s_children ); <nl> sd -> s_element = element ;
static int hdsp_dds_offset ( struct hdsp * hdsp ) <nl> unsigned int dds_value = hdsp -> dds_value ; <nl> int system_sample_rate = hdsp -> system_sample_rate ; <nl>  <nl> + if (! dds_value ) <nl> + return 0 ; <nl> + <nl> n = DDS_NUMERATOR ; <nl> /* <nl> * dds_value = n / rate
static bool davinci_spi_can_dma ( struct spi_master * master , <nl>  <nl> if ( spicfg ) <nl> can_dma = ( spicfg -> io_type == SPI_IO_TYPE_DMA ) && <nl> - ( xfer -> len >= DMA_MIN_BYTES ); <nl> + ( xfer -> len >= DMA_MIN_BYTES ) && <nl> + ! is_vmalloc_addr ( xfer -> rx_buf ) && <nl> + ! is_vmalloc_addr ( xfer -> tx_buf ); <nl>  <nl> return can_dma ; <nl> }
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
enum stb0899_status stb0899_dvbs2_algo ( struct stb0899_state * state ) <nl> else <nl> internal -> inversion = IQ_SWAP_OFF ; <nl>  <nl> - offsetfreq *= internal -> inversion ; <nl> - <nl> - internal -> freq = internal -> freq - offsetfreq ; <nl> + internal -> freq = internal -> freq + offsetfreq ; <nl> internal -> srate = stb0899_dvbs2_get_srate ( state ); <nl>  <nl> reg = STB0899_READ_S2REG ( STB0899_S2DEMOD , UWP_STAT2 );
static int get_sb_mtd_aux ( struct file_system_type * fs_type , int flags , <nl> DEBUG ( 1 , " MTDSB : New superblock for device % d (\"% s \")\ n ", <nl> mtd -> index , mtd -> name ); <nl>  <nl> + sb -> s_flags = flags ; <nl> + <nl> ret = fill_super ( sb , data , flags & MS_SILENT ? 1 : 0 ); <nl> if ( ret < 0 ) { <nl> up_write (& sb -> s_umount );
static int do_fault_around ( struct fault_env * fe , pgoff_t start_pgoff ) <nl>  <nl> if ( pmd_none (* fe -> pmd )) { <nl> fe -> prealloc_pte = pte_alloc_one ( fe -> vma -> vm_mm , fe -> address ); <nl> + if (! fe -> prealloc_pte ) <nl> + goto out ; <nl> smp_wmb (); /* See comment in __pte_alloc () */ <nl> } <nl> 
static void iwlagn_bt_traffic_change_work ( struct work_struct * work ) <nl> priv -> cfg -> ops -> lib -> update_chain_flags ( priv ); <nl>  <nl> if ( smps_request != - 1 ) { <nl> + priv -> current_ht_config . smps = smps_request ; <nl> for_each_context ( priv , ctx ) { <nl> if ( ctx -> vif && ctx -> vif -> type == NL80211_IFTYPE_STATION ) <nl> ieee80211_request_smps ( ctx -> vif , smps_request );
static int __init caam_rng_init ( void ) <nl> rng_ctx = kmalloc ( sizeof ( struct caam_rng_ctx ), GFP_DMA ); <nl> if (! rng_ctx ) <nl> return - ENOMEM ; <nl> - caam_init_rng (& rng_ctx , dev ); <nl> + caam_init_rng ( rng_ctx , dev ); <nl>  <nl> dev_info ( dev , " registering rng - caam \ n "); <nl> return hwrng_register (& caam_rng );
static int create_trace_kprobe ( int argc , char ** argv ) <nl> pr_info (" Failed to parse symbol .\ n "); <nl> return ret ; <nl> } <nl> + if ( offset && is_return && <nl> + ! arch_function_offset_within_entry ( offset )) { <nl> + pr_info (" Given offset is not valid for return probe .\ n "); <nl> + return - EINVAL ; <nl> + } <nl> } <nl> argc -= 2 ; argv += 2 ; <nl> 
static int hostap_enable_hostapd ( PSDevice pDevice , int rtnl_locked ) <nl>  <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO "% s : Enabling hostapd mode \ n ", dev -> name ); <nl>  <nl> - pDevice -> apdev = kzalloc ( sizeof ( struct net_device ), GFP_KERNEL ); <nl> + pDevice -> apdev = alloc_etherdev ( sizeof (* apdev_priv )); <nl> if ( pDevice -> apdev == NULL ) <nl> return - ENOMEM ; <nl> 
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> + if ( device_name == NULL ) <nl> + return - 1 ; <nl> + <nl> /* Find the device requested */ <nl> dev_num = find_type_by_name ( device_name , " device "); <nl> if ( dev_num < 0 ) {
*/ <nl> # define AT91_BASE_SYS 0xffffc000 <nl>  <nl> +# endif <nl> + <nl> /* <nl> * On sama5d4 there is no system controller , we map some needed peripherals <nl> */ <nl> # define AT91_ALT_BASE_SYS 0xfc069000 <nl> -# endif <nl>  <nl> /* <nl> * On all at91 have the Advanced Interrupt Controller starts at address <nl> */ <nl> # define AT91_IO_PHYS_BASE AT91_BASE_SYS <nl> # define AT91_IO_VIRT_BASE IOMEM ( AT91_IO_PHYS_BASE ) <nl> + <nl> +# define AT91_ALT_IO_PHYS_BASE AT91_ALT_BASE_SYS <nl> +# define AT91_ALT_IO_VIRT_BASE IOMEM ( AT91_ALT_BASE_SYS ) <nl> # endif <nl>  <nl> # define AT91_IO_SIZE ( 0xFFFFFFFF - AT91_IO_PHYS_BASE + 1 )
qla2xxx_queuecommand ( struct Scsi_Host * host , struct scsi_cmnd * cmd ) <nl> * Return target busy if we ' ve received a non - zero retry_delay_timer <nl> * in a FCP_RSP . <nl> */ <nl> - if ( time_after ( jiffies , fcport -> retry_delay_timestamp )) <nl> + if ( fcport -> retry_delay_timestamp == 0 ) { <nl> + /* retry delay not set */ <nl> + } else if ( time_after ( jiffies , fcport -> retry_delay_timestamp )) <nl> fcport -> retry_delay_timestamp = 0 ; <nl> else <nl> goto qc24_target_busy ;
static int edma_config_pset ( struct dma_chan * chan , struct edmacc_param * pset , <nl> int absync ; <nl>  <nl> acnt = dev_width ; <nl> + <nl> + /* src / dst_maxburst == 0 is the same case as src / dst_maxburst == 1 */ <nl> + if (! burst ) <nl> + burst = 1 ; <nl> /* <nl> * If the maxburst is equal to the fifo width , use <nl> * A - synced transfers . This allows for large contiguous
static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
int ocfs2_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> } <nl>  <nl> if ( size_change && attr -> ia_size != i_size_read ( inode )) { <nl> - if ( attr -> ia_size > sb -> s_maxbytes ) { <nl> - status = - EFBIG ; <nl> + status = inode_newsize_ok ( inode , attr -> ia_size ); <nl> + if ( status ) <nl> goto bail_unlock ; <nl> - } <nl>  <nl> if ( i_size_read ( inode ) > attr -> ia_size ) { <nl> if ( ocfs2_should_order_data ( inode )) {
int tm6000_reset ( struct tm6000_core * dev ) <nl>  <nl> msleep ( 5 ); <nl>  <nl> + /* <nl> + * Not all devices have int_in defined <nl> + */ <nl> + if (! dev -> int_in . endp ) <nl> + return 0 ; <nl> + <nl> err = usb_set_interface ( dev -> udev , dev -> isoc_in . bInterfaceNumber , 2 ); <nl> if ( err < 0 ) { <nl> tm6000_err (" failed to select interface % d , alt . setting 2 \ n ",
static void nvmet_execute_rw ( struct nvmet_req * req ) <nl>  <nl> if ( req -> cmd -> rw . opcode == nvme_cmd_write ) { <nl> op = REQ_OP_WRITE ; <nl> + op_flags = WRITE_ODIRECT ; <nl> if ( req -> cmd -> rw . control & cpu_to_le16 ( NVME_RW_FUA )) <nl> op_flags |= REQ_FUA ; <nl> } else {
static int s3c2440_nand_calculate_ecc ( struct mtd_info * mtd , const u_char * dat , u <nl> ecc_code [ 1 ] = ecc >> 8 ; <nl> ecc_code [ 2 ] = ecc >> 16 ; <nl>  <nl> - pr_debug ("% s : returning ecc % 06x \ n ", __func__ , ecc ); <nl> + pr_debug ("% s : returning ecc % 06lx \ n ", __func__ , ecc ); <nl>  <nl> return 0 ; <nl> }
static void s6000_pcm_enqueue_dma ( struct snd_pcm_substream * substream ) <nl> return ; <nl> } <nl>  <nl> - BUG_ON ( period_size & 15 ); <nl> + if ( WARN_ON ( period_size & 15 )) <nl> + return ; <nl> s6dmac_put_fifo ( DMA_MASK_DMAC ( channel ), DMA_INDEX_CHNL ( channel ), <nl> src , dst , period_size ); <nl> 
err : <nl> */ <nl> void fcoe_ctlr_recv ( struct fcoe_ctlr * fip , struct sk_buff * skb ) <nl> { <nl> + skb = skb_share_check ( skb , GFP_ATOMIC ); <nl> + if (! skb ) <nl> + return ; <nl> skb_queue_tail (& fip -> fip_recv_list , skb ); <nl> schedule_work (& fip -> recv_work ); <nl> }
static ssize_t ab8500_subscribe_write ( struct file * file , <nl> */ <nl> dev_attr [ irq_index ] = kmalloc ( sizeof ( struct device_attribute ), <nl> GFP_KERNEL ); <nl> + if (! dev_attr [ irq_index ]) <nl> + return - ENOMEM ; <nl> + <nl> event_name [ irq_index ] = kmalloc ( count , GFP_KERNEL ); <nl> sprintf ( event_name [ irq_index ], "% lu ", user_val ); <nl> dev_attr [ irq_index ]-> show = show_irq ;
int blkdev_ioctl ( struct block_device * bdev , fmode_t mode , unsigned cmd , <nl> * We need to set the startsect first , the driver may <nl> * want to override it . <nl> */ <nl> + memset (& geo , 0 , sizeof ( geo )); <nl> geo . start = get_start_sect ( bdev ); <nl> ret = disk -> fops -> getgeo ( bdev , & geo ); <nl> if ( ret )
module_exit ( visornic_cleanup ); <nl>  <nl> MODULE_AUTHOR (" Unisys "); <nl> MODULE_LICENSE (" GPL "); <nl> - MODULE_DESCRIPTION (" sPAR nic driver for sparlinux : ver 1 . 0 . 0 . 0 "); <nl> - MODULE_VERSION (" 1 . 0 . 0 . 0 "); <nl> + MODULE_DESCRIPTION (" sPAR nic driver for sparlinux ");
# include < libunwind . h > <nl> # include " perf_regs . h " <nl> # include "../../ util / unwind . h " <nl> +# include "../../ util / debug . h " <nl>  <nl> int libunwind__arch_reg_id ( int regnum ) <nl> {
of_at91_clk_master_setup ( struct device_node * np , struct at91_pmc * pmc , <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> if (! irq ) <nl> - return ; <nl> + goto out_free_characteristics ; <nl>  <nl> clk = at91_clk_register_master ( pmc , irq , name , num_parents , <nl> parent_names , layout ,
err1 : <nl>  <nl> static void treo680_irda_shutdown ( struct device * dev ) <nl> { <nl> - gpio_free ( GPIO_NR_TREO680_AMP_EN ); <nl> + gpio_free ( GPIO_NR_TREO680_IR_EN ); <nl> } <nl>  <nl> static struct pxaficp_platform_data treo680_ficp_info = {
static int tegra_dma_probe ( struct platform_device * pdev ) <nl> if ( ret ) { <nl> dev_err (& pdev -> dev , <nl> " request_irq failed with err % d channel % d \ n ", <nl> - i , ret ); <nl> + ret , i ); <nl> goto err_irq ; <nl> } <nl> 
struct hisi_clock_data * hisi_clk_init ( struct device_node * np , <nl> goto err ; <nl> } <nl> clk_data -> base = base ; <nl> - <nl> - clk_table = kzalloc ( sizeof ( struct clk *) * nr_clks , GFP_KERNEL ); <nl> + clk_table = kcalloc ( nr_clks , sizeof (* clk_table ), GFP_KERNEL ); <nl> if (! clk_table ) { <nl> pr_err ("% s : could not allocate clock lookup table \ n ", __func__ ); <nl> goto err_data ;
unsigned long do_mremap ( unsigned long addr , <nl> map_flags |= MAP_SHARED ; <nl>  <nl> new_addr = get_unmapped_area ( vma -> vm_file , 0 , new_len , <nl> - vma -> vm_pgoff , map_flags ); <nl> + vma -> vm_pgoff + <nl> + (( addr - vma -> vm_start ) >> PAGE_SHIFT ), <nl> + map_flags ); <nl> if ( new_addr & ~ PAGE_MASK ) { <nl> ret = new_addr ; <nl> goto out ;
static int o2cb_cluster_check ( void ) <nl> set_bit ( node_num , netmap ); <nl> if (! memcmp ( hbmap , netmap , sizeof ( hbmap ))) <nl> return 0 ; <nl> - if ( i < O2CB_MAP_STABILIZE_COUNT ) <nl> + if ( i < O2CB_MAP_STABILIZE_COUNT - 1 ) <nl> msleep ( 1000 ); <nl> } <nl> 
EXPORT_SYMBOL ( hwsw_unmap_sg ); <nl> EXPORT_SYMBOL ( hwsw_dma_supported ); <nl> EXPORT_SYMBOL ( hwsw_alloc_coherent ); <nl> EXPORT_SYMBOL ( hwsw_free_coherent ); <nl> + EXPORT_SYMBOL ( hwsw_sync_single_for_cpu ); <nl> + EXPORT_SYMBOL ( hwsw_sync_single_for_device ); <nl> + EXPORT_SYMBOL ( hwsw_sync_sg_for_cpu ); <nl> + EXPORT_SYMBOL ( hwsw_sync_sg_for_device );
static void irq_domain_disassociate_many ( struct irq_domain * domain , <nl> while ( count --) { <nl> int irq = irq_base + count ; <nl> struct irq_data * irq_data = irq_get_irq_data ( irq ); <nl> - irq_hw_number_t hwirq = irq_data -> hwirq ; <nl> + irq_hw_number_t hwirq ; <nl>  <nl> if ( WARN_ON (! irq_data || irq_data -> domain != domain )) <nl> continue ; <nl>  <nl> + hwirq = irq_data -> hwirq ; <nl> irq_set_status_flags ( irq , IRQ_NOREQUEST ); <nl>  <nl> /* remove chip and handler */
# define OP_31_XOP_SLBIA 498 <nl> # define OP_31_XOP_MFSR 595 <nl> # define OP_31_XOP_MFSRIN 659 <nl> +# define OP_31_XOP_DCBA 758 <nl> # define OP_31_XOP_SLBMFEV 851 <nl> # define OP_31_XOP_EIOIO 854 <nl> # define OP_31_XOP_SLBMFEE 915 <nl> int kvmppc_core_emulate_op ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> kvmppc_set_gpr ( vcpu , get_rt ( inst ), t ); <nl> } <nl> break ; <nl> + case OP_31_XOP_DCBA : <nl> + /* Gets treated as NOP */ <nl> + break ; <nl> case OP_31_XOP_DCBZ : <nl> { <nl> ulong rb = kvmppc_get_gpr ( vcpu , get_rb ( inst ));
int cgroup_path ( const struct cgroup * cgrp , char * buf , int buflen ) <nl> return 0 ; <nl> } <nl>  <nl> - start = buf + buflen ; <nl> + start = buf + buflen - 1 ; <nl>  <nl> - *-- start = '\ 0 '; <nl> + * start = '\ 0 '; <nl> for (;;) { <nl> int len = dentry -> d_name . len ; <nl> 
d40_get_dev_addr ( struct d40_chan * chan , enum dma_data_direction direction ) <nl> { <nl> struct stedma40_platform_data * plat = chan -> base -> plat_data ; <nl> struct stedma40_chan_cfg * cfg = & chan -> dma_cfg ; <nl> - dma_addr_t addr ; <nl> + dma_addr_t addr = 0 ; <nl>  <nl> if ( chan -> runtime_addr ) <nl> return chan -> runtime_addr ;
nv134_chipset = { <nl> . name = " GP104 ", <nl> . bar = gf100_bar_new , <nl> . bios = nvkm_bios_new , <nl> + . bus = gf100_bus_new , <nl> . devinit = gm200_devinit_new , <nl> . fb = gp104_fb_new , <nl> . imem = nv50_instmem_new ,
static void mpc_push ( struct atm_vcc * vcc , struct sk_buff * skb ) <nl> eg -> packets_rcvd ++; <nl> mpc -> eg_ops -> put ( eg ); <nl>  <nl> - memset ( ATM_SKB ( skb ), 0 , sizeof ( struct atm_skb_data )); <nl> + memset ( ATM_SKB ( new_skb ), 0 , sizeof ( struct atm_skb_data )); <nl> netif_rx ( new_skb ); <nl> } <nl> 
int xfrm_init_replay ( struct xfrm_state * x ) <nl> replay_esn -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> return - EINVAL ; <nl>  <nl> + if (( x -> props . flags & XFRM_STATE_ESN ) && replay_esn -> replay_window == 0 ) <nl> + return - EINVAL ; <nl> + <nl> if (( x -> props . flags & XFRM_STATE_ESN ) && x -> replay_esn ) <nl> x -> repl = & xfrm_replay_esn ; <nl> else
static void free_migration ( struct dm_cache_migration * mg ) <nl> wake_up (& cache -> migration_wait ); <nl>  <nl> mempool_free ( mg , cache -> migration_pool ); <nl> - wake_worker ( cache ); <nl> } <nl>  <nl> static int prealloc_data_structs ( struct cache * cache , struct prealloc * p )
int recover_inode_page ( struct f2fs_sb_info * sbi , struct page * page ) <nl> new_ni = old_ni ; <nl> new_ni . ino = ino ; <nl>  <nl> + if (! inc_valid_node_count ( sbi , NULL , 1 )) <nl> + WARN_ON ( 1 ); <nl> set_node_addr ( sbi , & new_ni , NEW_ADDR ); <nl> inc_valid_inode_count ( sbi ); <nl> 
int xhci_halt ( struct xhci_hcd * xhci ) <nl> STS_HALT , STS_HALT , XHCI_MAX_HALT_USEC ); <nl> if (! ret ) <nl> xhci -> xhc_state |= XHCI_STATE_HALTED ; <nl> + else <nl> + xhci_warn ( xhci , " Host not halted after % u microseconds .\ n ", <nl> + XHCI_MAX_HALT_USEC ); <nl> return ret ; <nl> } <nl> 
unapply_new_state : <nl> pinmux_enable_setting ( setting ); <nl> } <nl> } <nl> + <nl> + p -> state = old_state ; <nl> return ret ; <nl> } <nl> 
static int dtl1_hci_send_frame ( struct sk_buff * skb ) <nl> nsh . len = skb -> len ; <nl>  <nl> s = bt_skb_alloc ( NSHL + skb -> len + 1 , GFP_ATOMIC ); <nl> + if (! s ) <nl> + return - ENOMEM ; <nl> + <nl> skb_reserve ( s , NSHL ); <nl> memcpy ( skb_put ( s , skb -> len ), skb -> data , skb -> len ); <nl> if ( skb -> len & 0x0001 )
void __init tcp_tasklet_init ( void ) <nl> * We cant xmit new skbs from this context , as we might already <nl> * hold qdisc lock . <nl> */ <nl> - void tcp_wfree ( struct sk_buff * skb ) <nl> + static void tcp_wfree ( struct sk_buff * skb ) <nl> { <nl> struct sock * sk = skb -> sk ; <nl> struct tcp_sock * tp = tcp_sk ( sk );
static ssize_t chars_in_buffer ( struct tty_struct * tty ) <nl>  <nl> static ssize_t n_tty_chars_in_buffer ( struct tty_struct * tty ) <nl> { <nl> + WARN_ONCE ( 1 , "% s is deprecated and scheduled for removal .", __func__ ); <nl> return chars_in_buffer ( tty ); <nl> } <nl> 
static struct neighbour * fake_neigh_lookup ( const struct dst_entry * dst , const vo <nl> return NULL ; <nl> } <nl>  <nl> + static unsigned int fake_mtu ( const struct dst_entry * dst ) <nl> +{ <nl> + return dst -> dev -> mtu ; <nl> +} <nl> + <nl> static struct dst_ops fake_dst_ops = { <nl> . family = AF_INET , <nl> . protocol = cpu_to_be16 ( ETH_P_IP ), <nl> . update_pmtu = fake_update_pmtu , <nl> . cow_metrics = fake_cow_metrics , <nl> . neigh_lookup = fake_neigh_lookup , <nl> + . mtu = fake_mtu , <nl> }; <nl>  <nl> /*
struct gpio_chip * gpiochip_find ( void * data , <nl>  <nl> spin_lock_irqsave (& gpio_lock , flags ); <nl> list_for_each_entry ( gdev , & gpio_devices , list ) <nl> - if ( match ( gdev -> chip , data )) <nl> + if ( gdev -> chip && match ( gdev -> chip , data )) <nl> break ; <nl>  <nl> /* No match ? */
static int __init pasic3_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> if ( pdata && pdata -> led_pdata ) { <nl> + led_cell . platform_data = pdata -> led_pdata ; <nl> + led_cell . pdata_size = sizeof ( struct pasic3_leds_machinfo ); <nl> ret = mfd_add_devices (& pdev -> dev , pdev -> id , & led_cell , 1 , r , 0 ); <nl> if ( ret < 0 ) <nl> dev_warn ( dev , " failed to register LED device \ n ");
static void rtl8150_disconnect ( struct usb_interface * intf ) <nl> usb_set_intfdata ( intf , NULL ); <nl> if ( dev ) { <nl> set_bit ( RTL8150_UNPLUG , & dev -> flags ); <nl> - tasklet_disable (& dev -> tl ); <nl> tasklet_kill (& dev -> tl ); <nl> unregister_netdev ( dev -> netdev ); <nl> unlink_all_urbs ( dev );
static int tg3_init_one ( struct pci_dev * pdev , <nl>  <nl> tg3_timer_init ( tp ); <nl>  <nl> + tg3_carrier_off ( tp ); <nl> + <nl> err = register_netdev ( dev ); <nl> if ( err ) { <nl> dev_err (& pdev -> dev , " Cannot register net device , aborting \ n ");
static int cgroup_get_sb ( struct file_system_type * fs_type , <nl> BUG_ON ( root -> number_of_cgroups != 1 ); <nl>  <nl> cgroup_populate_dir ( root_cgrp ); <nl> - mutex_unlock (& inode -> i_mutex ); <nl> mutex_unlock (& cgroup_mutex ); <nl> + mutex_unlock (& inode -> i_mutex ); <nl> } <nl>  <nl> simple_set_mnt ( mnt , sb );
struct fpgaimage { <nl> char part [ MAX_STR ]; <nl> char date [ MAX_STR ]; <nl> char time [ MAX_STR ]; <nl> - int32_t lendata ; <nl> + int lendata ; <nl> char * fpgadata ; <nl> };
int symbol__init ( void ) <nl> if ( symbol_conf . initialized ) <nl> return 0 ; <nl>  <nl> + symbol_conf . priv_size = ALIGN ( symbol_conf . priv_size , sizeof ( u64 )); <nl> + <nl> elf_version ( EV_CURRENT ); <nl> if ( symbol_conf . sort_by_name ) <nl> symbol_conf . priv_size += ( sizeof ( struct symbol_name_rb_node ) -
gen6_read ## x ( struct drm_i915_private * dev_priv , off_t reg , bool trace ) { \ <nl> static u ## x \ <nl> vlv_read ## x ( struct drm_i915_private * dev_priv , off_t reg , bool trace ) { \ <nl> unsigned fwengine = 0 ; \ <nl> - unsigned * fwcount = 0 ; \ <nl> + unsigned * fwcount ; \ <nl> REG_READ_HEADER ( x ); \ <nl> if ( FORCEWAKE_VLV_RENDER_RANGE_OFFSET ( reg )) { \ <nl> fwengine = FORCEWAKE_RENDER ; \
static int xfrm_get_policy ( struct sk_buff * skb , struct nlmsghdr * nlh , void ** xfr <nl> MSG_DONTWAIT ); <nl> } <nl> } else { <nl> + c . data . byid = p -> index ; <nl> c . event = XFRM_SAP_DELETED ; <nl> c . seq = nlh -> nlmsg_seq ; <nl> c . pid = nlh -> nlmsg_pid ;
struct vpif_disp_buffer { <nl> }; <nl>  <nl> struct common_obj { <nl> - /* Buffer specific parameters */ <nl> - u8 * fbuffers [ VIDEO_MAX_FRAME ]; /* List of buffer pointers for <nl> - * storing frames */ <nl> u32 numbuffers ; /* number of buffers */ <nl> struct vpif_disp_buffer * cur_frm ; /* Pointer pointing to current <nl> * vb2_buffer */
int msm_proc_comm ( unsigned cmd , unsigned * data1 , unsigned * data2 ) <nl> * and unknown state . This function should be called early to <nl> * wait on the ARM9 . <nl> */ <nl> - void __init proc_comm_boot_wait ( void ) <nl> + void __devinit proc_comm_boot_wait ( void ) <nl> { <nl> void __iomem * base = MSM_SHARED_RAM_BASE ; <nl> 
void free_reloc_roots ( struct list_head * list ) <nl> while (! list_empty ( list )) { <nl> reloc_root = list_entry ( list -> next , struct btrfs_root , <nl> root_list ); <nl> + free_extent_buffer ( reloc_root -> node ); <nl> + free_extent_buffer ( reloc_root -> commit_root ); <nl> + reloc_root -> node = NULL ; <nl> + reloc_root -> commit_root = NULL ; <nl> __del_reloc_root ( reloc_root ); <nl> } <nl> }
int cfhsi_probe ( struct platform_device * pdev ) <nl> dev_err (& ndev -> dev , "% s : Registration error : % d .\ n ", <nl> __func__ , res ); <nl> free_netdev ( ndev ); <nl> + return - ENODEV ; <nl> } <nl> /* Add CAIF HSI device to list . */ <nl> spin_lock (& cfhsi_list_lock );
drm_display_mode_from_vic_index ( struct drm_connector * connector , <nl> return NULL ; <nl>  <nl> newmode = drm_mode_duplicate ( dev , & edid_cea_modes [ cea_mode ]); <nl> + if (! newmode ) <nl> + return NULL ; <nl> + <nl> newmode -> vrefresh = 0 ; <nl>  <nl> return newmode ;
static struct ad5755_platform_data * ad5755_parse_dt ( struct device * dev ) <nl>  <nl> devnr = 0 ; <nl> for_each_child_of_node ( np , pp ) { <nl> - if ( devnr > AD5755_NUM_CHANNELS ) { <nl> + if ( devnr >= AD5755_NUM_CHANNELS ) { <nl> dev_err ( dev , <nl> " There is to many channels defined in DT \ n "); <nl> goto error_out ;
static int s5h1420_send_master_cmd ( struct dvb_frontend * fe , <nl> int result = 0 ; <nl>  <nl> dprintk (" enter % s \ n ", __func__ ); <nl> - if ( cmd -> msg_len > 8 ) <nl> + if ( cmd -> msg_len > sizeof ( cmd -> msg )) <nl> return - EINVAL ; <nl>  <nl> /* setup for DISEQC */
static int __devinit iic_probe ( struct ocp_device * ocp ){ <nl> strcpy ( adap -> name , " IBM IIC "); <nl> i2c_set_adapdata ( adap , dev ); <nl> adap -> id = I2C_HW_OCP ; <nl> + adap -> class = I2C_CLASS_HWMON ; <nl> adap -> algo = & iic_algo ; <nl> adap -> client_register = NULL ; <nl> adap -> client_unregister = NULL ;
static char * hidpp_get_unifying_name ( struct hidpp_device * hidpp_dev ) <nl>  <nl> len = response . rap . params [ 1 ]; <nl>  <nl> + if ( 2 + len > sizeof ( response . rap . params )) <nl> + return NULL ; <nl> + <nl> name = kzalloc ( len + 1 , GFP_KERNEL ); <nl> if (! name ) <nl> return NULL ;
static int tty_fasync ( int fd , struct file * filp , int on ) <nl> pid = task_pid ( current ); <nl> type = PIDTYPE_PID ; <nl> } <nl> - spin_unlock_irqrestore (& tty -> ctrl_lock , flags ); <nl> retval = __f_setown ( filp , pid , type , 0 ); <nl> + spin_unlock_irqrestore (& tty -> ctrl_lock , flags ); <nl> if ( retval ) <nl> goto out ; <nl> } else {
static const struct ethtool_ops dnet_ethtool_ops = { <nl> . set_settings = dnet_set_settings , <nl> . get_drvinfo = dnet_get_drvinfo , <nl> . get_link = ethtool_op_get_link , <nl> + . get_ts_info = ethtool_op_get_ts_info , <nl> }; <nl>  <nl> static const struct net_device_ops dnet_netdev_ops = {
int mesh_allocated ; <nl> static struct kmem_cache * rm_cache ; <nl>  <nl> -# ifdef CONFIG_MAC80211_MESH <nl> bool mesh_action_is_path_sel ( struct ieee80211_mgmt * mgmt ) <nl> { <nl> return ( mgmt -> u . action . u . mesh_action . action_code == <nl> WLAN_MESH_ACTION_HWMP_PATH_SELECTION ); <nl> } <nl> -# else <nl> - bool mesh_action_is_path_sel ( struct ieee80211_mgmt * mgmt ) <nl> -{ return false ; } <nl> -# endif <nl>  <nl> void ieee80211s_init ( void ) <nl> {
retry : <nl> } <nl>  <nl> /* Check info buffer */ <nl> - info = ( void *)& msg [ 1 ]; <nl> + info = ( void *)& bcdc -> buf [ 0 ]; <nl>  <nl> /* Copy info buffer */ <nl> if ( buf ) {
static const struct of_device_id regulator_haptic_dt_match [] = { <nl> { . compatible = " regulator - haptic " }, <nl> { /* sentinel */ }, <nl> }; <nl> + MODULE_DEVICE_TABLE ( of , regulator_haptic_dt_match ); <nl>  <nl> static struct platform_driver regulator_haptic_driver = { <nl> . probe = regulator_haptic_probe ,
/* <nl> * Literals <nl> */ <nl> -# define IPR_DRIVER_VERSION " 2 . 6 . 1 " <nl> -# define IPR_DRIVER_DATE "( March 12 , 2015 )" <nl> +# define IPR_DRIVER_VERSION " 2 . 6 . 2 " <nl> +# define IPR_DRIVER_DATE "( June 11 , 2015 )" <nl>  <nl> /* <nl> * IPR_MAX_CMD_PER_LUN : This defines the maximum number of outstanding
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
static int pfkey_spdadd ( struct sock * sk , struct sk_buff * skb , struct sadb_msg * h <nl> return 0 ; <nl>  <nl> out : <nl> + xp -> dead = 1 ; <nl> xfrm_policy_destroy ( xp ); <nl> return err ; <nl> }
i915_gem_object_pin ( struct drm_i915_gem_object * obj , <nl> uint32_t alignment , <nl> unsigned flags ) <nl> { <nl> + struct drm_i915_private * dev_priv = obj -> base . dev -> dev_private ; <nl> struct i915_vma * vma ; <nl> int ret ; <nl>  <nl> + if ( WARN_ON ( vm == & dev_priv -> mm . aliasing_ppgtt -> base )) <nl> + return - ENODEV ; <nl> + <nl> if ( WARN_ON ( flags & ( PIN_GLOBAL | PIN_MAPPABLE ) && ! i915_is_ggtt ( vm ))) <nl> return - EINVAL ; <nl> 
struct ulpi_info { <nl> /* ULPI hardcoded IDs , used for probing */ <nl> static struct ulpi_info ulpi_ids [] = { <nl> ULPI_INFO ( ULPI_ID ( 0x04cc , 0x1504 ), " NXP ISP1504 "), <nl> - ULPI_INFO ( ULPI_ID ( 0x0424 , 0x0006 ), " SMSC USB3319 "), <nl> + ULPI_INFO ( ULPI_ID ( 0x0424 , 0x0006 ), " SMSC USB331x "), <nl> }; <nl>  <nl> static int ulpi_set_otg_flags ( struct otg_transceiver * otg )
static int nokia_modem_probe ( struct device * dev ) <nl> dev_set_drvdata ( dev , modem ); <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> - if ( irq < 0 ) { <nl> + if (! irq ) { <nl> dev_err ( dev , " Invalid rst_ind interrupt (% d )\ n ", irq ); <nl> - return irq ; <nl> + return - EINVAL ; <nl> } <nl> modem -> nokia_modem_rst_ind_irq = irq ; <nl> pflags = irq_get_trigger_type ( irq );
static int snd_fm801_free ( struct fm801 * chip ) <nl> cmdw |= 0x00c3 ; <nl> fm801_writew ( chip , IRQ_MASK , cmdw ); <nl>  <nl> + devm_free_irq (& chip -> pci -> dev , chip -> irq , chip ); <nl> + <nl> __end_hw : <nl> # ifdef CONFIG_SND_FM801_TEA575X_BOOL <nl> if (!( chip -> tea575x_tuner & TUNER_DISABLED )) {
vector_setup_out : <nl> **/ <nl> static struct i40e_vsi * i40e_vsi_reinit_setup ( struct i40e_vsi * vsi ) <nl> { <nl> - struct i40e_pf * pf = vsi -> back ; <nl> + struct i40e_pf * pf ; <nl> u8 enabled_tc ; <nl> int ret ; <nl>  <nl> + if (! vsi ) <nl> + return NULL ; <nl> + <nl> + pf = vsi -> back ; <nl> + <nl> i40e_put_lump ( pf -> qp_pile , vsi -> base_queue , vsi -> idx ); <nl> i40e_vsi_clear_rings ( vsi ); <nl> 
minstrel_aggr_check ( struct minstrel_priv * mp , struct ieee80211_sta * pubsta , stru <nl> if ( likely ( sta -> ampdu_mlme . tid_tx [ tid ])) <nl> return ; <nl>  <nl> + if ( skb_get_queue_mapping ( skb ) == IEEE80211_AC_VO ) <nl> + return ; <nl> + <nl> ieee80211_start_tx_ba_session ( pubsta , tid ); <nl> } <nl> 
static int __ixgbe_shutdown ( struct pci_dev * pdev , bool * enable_wake ) <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + rtnl_lock (); <nl> if ( netif_running ( netdev )) { <nl> - rtnl_lock (); <nl> ixgbe_down ( adapter ); <nl> ixgbe_free_irq ( adapter ); <nl> ixgbe_free_all_tx_resources ( adapter ); <nl> ixgbe_free_all_rx_resources ( adapter ); <nl> - rtnl_unlock (); <nl> } <nl> + rtnl_unlock (); <nl>  <nl> ixgbe_clear_interrupt_scheme ( adapter ); <nl> 
int __init ks_dw_pcie_host_init ( struct keystone_pcie * ks_pcie , <nl>  <nl> /* Index 0 is the config reg . space address */ <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - pci -> dbi_base = devm_ioremap_resource ( dev , res ); <nl> + pci -> dbi_base = devm_pci_remap_cfg_resource ( dev , res ); <nl> if ( IS_ERR ( pci -> dbi_base )) <nl> return PTR_ERR ( pci -> dbi_base ); <nl> 
static noinline void init_btrfs_i ( struct inode * inode ) <nl> bi -> flags = 0 ; <nl> bi -> index_cnt = ( u64 )- 1 ; <nl> bi -> last_unlink_trans = 0 ; <nl> + bi -> ordered_data_close = 0 ; <nl> extent_map_tree_init (& BTRFS_I ( inode )-> extent_tree , GFP_NOFS ); <nl> extent_io_tree_init (& BTRFS_I ( inode )-> io_tree , <nl> inode -> i_mapping , GFP_NOFS );
static int bcm2835_sdhci_probe ( struct platform_device * pdev ) <nl> goto err ; <nl> } <nl>  <nl> - return sdhci_add_host ( host ); <nl> + ret = sdhci_add_host ( host ); <nl> + if ( ret ) <nl> + goto err ; <nl>  <nl> + return 0 ; <nl> err : <nl> sdhci_pltfm_free ( pdev ); <nl> return ret ;
void blk_mq_free_request ( struct request * rq ) <nl> hctx = q -> mq_ops -> map_queue ( q , ctx -> cpu ); <nl> __blk_mq_free_request ( hctx , ctx , rq ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( blk_mq_free_request ); <nl>  <nl> inline void __blk_mq_end_request ( struct request * rq , int error ) <nl> {
static int ibmvfc_map_sg_data ( struct scsi_cmnd * scmd , <nl> & evt -> ext_list_token ); <nl>  <nl> if (! evt -> ext_list ) { <nl> - scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> + scsi_dma_unmap ( scmd ); <nl> + if ( vhost -> log_level > IBMVFC_DEFAULT_LOG_LEVEL ) <nl> + scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> return - ENOMEM ; <nl> } <nl> }
static int omap_i2c_remove ( struct platform_device * pdev ) <nl> return ret ; <nl>  <nl> omap_i2c_write_reg ( omap , OMAP_I2C_CON_REG , 0 ); <nl> - pm_runtime_put (& pdev -> dev ); <nl> + pm_runtime_put_sync (& pdev -> dev ); <nl> pm_runtime_disable (& pdev -> dev ); <nl> return 0 ; <nl> }
static int osc_extent_wait ( const struct lu_env * env , struct osc_extent * ext , <nl> "% s : wait ext to % d timedout , recovery in progress ?\ n ", <nl> osc_export ( obj )-> exp_obd -> obd_name , state ); <nl>  <nl> - lwi = LWI_INTR ( LWI_ON_SIGNAL_NOOP , NULL ); <nl> + lwi = LWI_INTR ( NULL , NULL ); <nl> rc = l_wait_event ( ext -> oe_waitq , extent_wait_cb ( ext , state ), <nl> & lwi ); <nl> }
static struct cphy * my3126_phy_create ( adapter_t * adapter , <nl> { <nl> struct cphy * cphy = kzalloc ( sizeof (* cphy ), GFP_KERNEL ); <nl>  <nl> - if ( cphy ) <nl> - cphy_init ( cphy , adapter , phy_addr , & my3126_ops , mdio_ops ); <nl> + if (! cphy ) <nl> + return NULL ; <nl>  <nl> + cphy_init ( cphy , adapter , phy_addr , & my3126_ops , mdio_ops ); <nl> INIT_DELAYED_WORK (& cphy -> phy_update , my3216_poll ); <nl> cphy -> bmsr = 0 ; <nl> 
static void dce3_2_afmt_write_sad_regs ( struct drm_encoder * encoder ) <nl> } <nl>  <nl> sad_count = drm_edid_to_sad ( radeon_connector -> edid , & sads ); <nl> - if ( sad_count < 0 ) { <nl> + if ( sad_count <= 0 ) { <nl> DRM_ERROR (" Couldn ' t read SADs : % d \ n ", sad_count ); <nl> return ; <nl> }
int dm_split_args ( int * argc , char *** argvp , char * input ) <nl> unsigned array_size = 0 ; <nl>  <nl> * argc = 0 ; <nl> + <nl> + if (! input ) { <nl> + * argvp = NULL ; <nl> + return 0 ; <nl> + } <nl> + <nl> argv = realloc_argv (& array_size , argv ); <nl> if (! argv ) <nl> return - ENOMEM ;
int reiserfs_acl_chmod ( struct inode * inode ) <nl> return 0 ; <nl> } <nl>  <nl> + reiserfs_write_unlock ( inode -> i_sb ); <nl> acl = reiserfs_get_acl ( inode , ACL_TYPE_ACCESS ); <nl> + reiserfs_write_lock ( inode -> i_sb ); <nl> if (! acl ) <nl> return 0 ; <nl> if ( IS_ERR ( acl ))
safe_sig_queue_validate ( struct signal_queue_header * psafe_sqh , <nl> struct signal_queue_header * punsafe_sqh , <nl> u32 * phead , u32 * ptail ) <nl> { <nl> - if ((* phead >= psafe_sqh -> max_slots ) <nl> - || (* ptail >= psafe_sqh -> max_slots )) { <nl> + if ((* phead >= psafe_sqh -> max_slots ) || <nl> + (* ptail >= psafe_sqh -> max_slots )) { <nl> /* Choose 0 or max , maybe based on current tail value */ <nl> * phead = 0 ; <nl> * ptail = 0 ;
static int gp8psk_fe_read_signal_strength ( struct dvb_frontend * fe , u16 * strength <nl>  <nl> static int gp8psk_fe_get_tune_settings ( struct dvb_frontend * fe , struct dvb_frontend_tune_settings * tune ) <nl> { <nl> - tune -> min_delay_ms = 200 ; <nl> + tune -> min_delay_ms = 800 ; <nl> return 0 ; <nl> } <nl> 
static long intel_vgpu_ioctl ( struct mdev_device * mdev , unsigned int cmd , <nl> sparse -> areas [ 0 ]. offset = <nl> PAGE_ALIGN ( vgpu_aperture_offset ( vgpu )); <nl> sparse -> areas [ 0 ]. size = vgpu_aperture_sz ( vgpu ); <nl> - if (! caps . buf ) { <nl> - kfree ( caps . buf ); <nl> - caps . buf = NULL ; <nl> - caps . size = 0 ; <nl> - } <nl> break ; <nl>  <nl> case VFIO_PCI_BAR3_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX :
static inline void dec_valid_block_count ( struct f2fs_sb_info * sbi , <nl> static inline void inc_page_count ( struct f2fs_sb_info * sbi , int count_type ) <nl> { <nl> percpu_counter_inc (& sbi -> nr_pages [ count_type ]); <nl> + <nl> + if ( count_type == F2FS_DIRTY_DATA || count_type == F2FS_INMEM_PAGES ) <nl> + return ; <nl> + <nl> set_sbi_flag ( sbi , SBI_IS_DIRTY ); <nl> } <nl> 
static int brcmf_sdbrcm_write_vars ( struct brcmf_sdio * bus ) <nl> /* Verify NVRAM bytes */ <nl> brcmf_dbg ( INFO , " Compare NVRAM dl & ul ; varsize =% d \ n ", varsize ); <nl> nvram_ularray = kmalloc ( varsize , GFP_ATOMIC ); <nl> - if (! nvram_ularray ) <nl> + if (! nvram_ularray ) { <nl> + kfree ( vbuffer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> /* Upload image to verify downloaded contents . */ <nl> memset ( nvram_ularray , 0xaa , varsize );
spider_net_prepare_rx_descr ( struct spider_net_card * card , <nl> /* and we need to have it 128 byte aligned , therefore we allocate a <nl> * bit more */ <nl> /* allocate an skb */ <nl> - descr -> skb = dev_alloc_skb ( bufsize + SPIDER_NET_RXBUF_ALIGN - 1 ); <nl> + descr -> skb = netdev_alloc_skb ( card -> netdev , <nl> + bufsize + SPIDER_NET_RXBUF_ALIGN - 1 ); <nl> if (! descr -> skb ) { <nl> if ( netif_msg_rx_err ( card ) && net_ratelimit ()) <nl> pr_err (" Not enough memory to allocate rx buffer \ n ");
static long lineevent_ioctl ( struct file * filep , unsigned int cmd , <nl> if ( cmd == GPIOHANDLE_GET_LINE_VALUES_IOCTL ) { <nl> int val ; <nl>  <nl> + memset (& ghd , 0 , sizeof ( ghd )); <nl> + <nl> val = gpiod_get_value_cansleep ( le -> desc ); <nl> if ( val < 0 ) <nl> return val ;
static int __vhost_add_used_n ( struct vhost_virtqueue * vq , <nl>  <nl> start = vq -> last_used_idx % vq -> num ; <nl> used = vq -> used -> ring + start ; <nl> - if ( copy_to_user ( used , heads , count * sizeof * used )) { <nl> + if ( __copy_to_user ( used , heads , count * sizeof * used )) { <nl> vq_err ( vq , " Failed to write used "); <nl> return - EFAULT ; <nl> }
static void __init request_standard_resources ( void ) <nl> res = alloc_bootmem_low ( sizeof (* res )); <nl> if ( memblock_is_nomap ( region )) { <nl> res -> name = " reserved "; <nl> - res -> flags = IORESOURCE_MEM | IORESOURCE_BUSY ; <nl> + res -> flags = IORESOURCE_MEM ; <nl> } else { <nl> res -> name = " System RAM "; <nl> res -> flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY ;
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> - err = do_migrate_pages ( mm , & old , & new , MPOL_MF_MOVE ); <nl> + err = do_migrate_pages ( mm , & old , & new , <nl> + capable ( CAP_SYS_ADMIN ) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE ); <nl> out : <nl> mmput ( mm ); <nl> return err ;
jme_alloc_and_feed_skb ( struct jme_adapter * jme , int idx ) <nl> jme -> jme_vlan_rx ( skb , jme -> vlgrp , <nl> le16_to_cpu ( rxdesc -> descwb . vlan )); <nl> NET_STAT ( jme ). rx_bytes += 4 ; <nl> + } else { <nl> + dev_kfree_skb ( skb ); <nl> } <nl> } else { <nl> jme -> jme_rx ( skb );
static struct tx_agg * r8152_get_tx_agg ( struct r8152 * tp ) <nl> struct tx_agg * agg = NULL ; <nl> unsigned long flags ; <nl>  <nl> + if ( list_empty (& tp -> tx_free )) <nl> + return NULL ; <nl> + <nl> spin_lock_irqsave (& tp -> tx_lock , flags ); <nl> if (! list_empty (& tp -> tx_free )) { <nl> struct list_head * cursor ;
static int macvtap_ioctl_set_queue ( struct file * file , unsigned int flags ) <nl> ret = macvtap_enable_queue ( vlan -> dev , file , q ); <nl> else if ( flags & IFF_DETACH_QUEUE ) <nl> ret = macvtap_disable_queue ( q ); <nl> + else <nl> + ret = - EINVAL ; <nl>  <nl> macvtap_put_vlan ( vlan ); <nl> return ret ;
__acquires (& pool -> lock ) <nl> * kernels , where a requeueing work item waiting for something to <nl> * happen could deadlock with stop_machine as such work item could <nl> * indefinitely requeue itself while all other CPUs are trapped in <nl> - * stop_machine . <nl> + * stop_machine . At the same time , report a quiescent RCU state so <nl> + * the same condition doesn ' t freeze RCU . <nl> */ <nl> + rcu_note_voluntary_context_switch ( current ); <nl> cond_resched (); <nl>  <nl> spin_lock_irq (& pool -> lock );
ssize_t tcp_splice_read ( struct socket * sock , loff_t * ppos , <nl> ssize_t spliced ; <nl> int ret ; <nl>  <nl> + sock_rps_record_flow ( sk ); <nl> /* <nl> * We can ' t seek on a socket input <nl> */
static void raid10d ( mddev_t * mddev ) <nl> sl --; <nl> d = r10_bio -> devs [ sl ]. devnum ; <nl> rdev = conf -> mirrors [ d ]. rdev ; <nl> - atomic_add ( s , & rdev -> corrected_errors ); <nl> if ( rdev && <nl> test_bit ( In_sync , & rdev -> flags )) { <nl> + atomic_add ( s , & rdev -> corrected_errors ); <nl> if ( sync_page_io ( rdev -> bdev , <nl> r10_bio -> devs [ sl ]. addr + <nl> sect + rdev -> data_offset ,
static ssize_t btrfs_label_store ( struct kobject * kobj , <nl> int ret ; <nl> size_t p_len ; <nl>  <nl> + if ( fs_info -> sb -> s_flags & MS_RDONLY ) <nl> + return - EROFS ; <nl> + <nl> /* <nl> * p_len is the len until the first occurrence of either <nl> * '\ n ' or '\ 0 '
static int arche_platform_probe ( struct platform_device * pdev ) <nl> arche_pdata -> wake_detect_gpio = of_get_named_gpio ( np , " svc , wake - detect - gpio ", 0 ); <nl> if ( arche_pdata -> wake_detect_gpio < 0 ) { <nl> dev_err ( dev , " failed to get wake detect gpio \ n "); <nl> - ret = arche_pdata -> wake_detect_gpio ; <nl> - return ret ; <nl> + return arche_pdata -> wake_detect_gpio ; <nl> } <nl>  <nl> ret = devm_gpio_request ( dev , arche_pdata -> wake_detect_gpio , " wake detect ");
static int dspi_request_dma ( struct fsl_dspi * dspi , phys_addr_t phy_addr ) <nl> return 0 ; <nl>  <nl> err_slave_config : <nl> - devm_kfree ( dev , dma -> rx_dma_buf ); <nl> + dma_free_coherent ( dev , DSPI_DMA_BUFSIZE , <nl> + dma -> rx_dma_buf , dma -> rx_dma_phys ); <nl> err_rx_dma_buf : <nl> - devm_kfree ( dev , dma -> tx_dma_buf ); <nl> + dma_free_coherent ( dev , DSPI_DMA_BUFSIZE , <nl> + dma -> tx_dma_buf , dma -> tx_dma_phys ); <nl> err_tx_dma_buf : <nl> dma_release_channel ( dma -> chan_tx ); <nl> err_tx_channel :
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
int btrfs_open_devices ( struct btrfs_fs_devices * fs_devices , <nl> goto error_brelse ; <nl>  <nl> transid = btrfs_super_generation ( disk_super ); <nl> - if ( transid > latest_transid ) { <nl> + if (! latest_transid || transid > latest_transid ) { <nl> latest_devid = devid ; <nl> latest_transid = transid ; <nl> latest_bdev = bdev ;
int test_range_bit ( struct extent_io_tree * tree , u64 start , u64 end , <nl> bitset = 0 ; <nl> break ; <nl> } <nl> + <nl> + if ( state -> end == ( u64 )- 1 ) <nl> + break ; <nl> + <nl> start = state -> end + 1 ; <nl> if ( start > end ) <nl> break ;
static int fsl_pq_mdio_probe ( struct of_device * ofdev , <nl> dev_set_drvdata (& ofdev -> dev , new_bus ); <nl>  <nl> if ( of_device_is_compatible ( np , " fsl , gianfar - mdio ") || <nl> + of_device_is_compatible ( np , " fsl , gianfar - tbi ") || <nl> of_device_is_compatible ( np , " gianfar ")) { <nl> # ifdef CONFIG_GIANFAR <nl> tbipa = get_gfar_tbipa ( regs );
extern void numa_initmem_init ( unsigned long start_pfn , unsigned long end_pfn ); <nl> extern unsigned long numa_free_all_bootmem ( void ); <nl>  <nl> extern void reserve_bootmem_generic ( unsigned long phys , unsigned len ); <nl> - extern void free_bootmem_generic ( unsigned long phys , unsigned len ); <nl>  <nl> extern void load_gs_index ( unsigned gs ); <nl> 
void usbip_stop_eh ( struct usbip_device * ud ) <nl> { <nl> struct usbip_task * eh = & ud -> eh ; <nl>  <nl> + if ( eh -> thread == current ) <nl> + return ; /* do not wait for myself */ <nl> + <nl> wait_for_completion (& eh -> thread_done ); <nl> usbip_dbg_eh (" usbip_eh has finished \ n "); <nl> }
static s32 brcmf_p2p_run_escan ( struct brcmf_cfg80211_info * cfg , <nl> } <nl> err = brcmf_p2p_escan ( p2p , num_nodfs , chanspecs , search_state , <nl> action , P2PAPI_BSSCFG_DEVICE ); <nl> + kfree ( chanspecs ); <nl> } <nl> exit : <nl> if ( err )
void host1x_set_drm_data ( struct device * dev , void * data ) <nl> void * host1x_get_drm_data ( struct device * dev ) <nl> { <nl> struct host1x * host1x = dev_get_drvdata ( dev ); <nl> - return host1x -> drm_data ; <nl> + return host1x ? host1x -> drm_data : NULL ; <nl> } <nl>  <nl> void host1x_sync_writel ( struct host1x * host1x , u32 v , u32 r )
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
static int idmouse_probe ( struct usb_interface * interface , <nl> if ( iface_desc -> desc . bInterfaceClass != 0x0A ) <nl> return - ENODEV ; <nl>  <nl> + if ( iface_desc -> desc . bNumEndpoints < 1 ) <nl> + return - ENODEV ; <nl> + <nl> /* allocate memory for our device state and initialize it */ <nl> dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> if ( dev == NULL )
static int ocfs2_truncate_file ( struct inode * inode , <nl> ( unsigned long long ) OCFS2_I ( inode )-> ip_blkno , <nl> ( unsigned long long ) new_i_size ); <nl>  <nl> + unmap_mapping_range ( inode -> i_mapping , new_i_size + PAGE_SIZE - 1 , 0 , 1 ); <nl> truncate_inode_pages ( inode -> i_mapping , new_i_size ); <nl>  <nl> fe = ( struct ocfs2_dinode *) di_bh -> b_data ;
static u32 crc32c_vpmsum ( u32 crc , unsigned char const * p , size_t len ) <nl> } <nl>  <nl> if ( len & ~ VMX_ALIGN_MASK ) { <nl> + preempt_disable (); <nl> pagefault_disable (); <nl> enable_kernel_altivec (); <nl> crc = __crc32c_vpmsum ( crc , p , len & ~ VMX_ALIGN_MASK ); <nl> + disable_kernel_altivec (); <nl> pagefault_enable (); <nl> + preempt_enable (); <nl> } <nl>  <nl> tail = len & VMX_ALIGN_MASK ;
int switch_ssc_clock ( struct rtsx_chip * chip , int clk ) <nl> return STATUS_FAIL ; <nl> } <nl>  <nl> - mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> + mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> if ( mcu_cnt > 7 ) <nl> mcu_cnt = 7 ; <nl> 
static void iwl_mvm_stat_iterator ( void * _data , u8 * mac , <nl> if ( vif -> type != NL80211_IFTYPE_STATION ) <nl> return ; <nl>  <nl> + if ( sig == 0 ) { <nl> + IWL_DEBUG_RX ( mvm , " RSSI is 0 - skip signal based decision \ n "); <nl> + return ; <nl> + } <nl> + <nl> mvmvif -> bf_data . ave_beacon_signal = sig ; <nl>  <nl> /* BT Coex */
static int qla4xxx_fw_ready ( struct scsi_qla_host * ha ) <nl> DEBUG2 ( printk (" scsi % ld : % s : FW initialized , but " <nl> " auto - discovery still in process \ n ", <nl> ha -> host_no , __func__ )); <nl> + ready = 1 ; <nl> } <nl>  <nl> return ready ;
static void __cpuinit put_core_offline ( unsigned int cpu ) <nl>  <nl> indx = TO_ATTR_NO ( cpu ); <nl>  <nl> + /* The core id is too big , just return */ <nl> + if ( indx > MAX_CORE_DATA - 1 ) <nl> + return ; <nl> + <nl> if ( pdata -> core_data [ indx ] && pdata -> core_data [ indx ]-> cpu == cpu ) <nl> coretemp_remove_core ( pdata , & pdev -> dev , indx ); <nl> 
static int sysfs_get_sb ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb ) || sb -> s_fs_info != info ) <nl> kfree ( info ); <nl> if ( IS_ERR ( sb )) { <nl> - kfree ( info ); <nl> error = PTR_ERR ( sb ); <nl> goto out ; <nl> }
static int coredump_wait ( int exit_code , struct core_state * core_state ) <nl> core_state -> dumper . task = tsk ; <nl> core_state -> dumper . next = NULL ; <nl>  <nl> - down_write (& mm -> mmap_sem ); <nl> + if ( down_write_killable (& mm -> mmap_sem )) <nl> + return - EINTR ; <nl> + <nl> if (! mm -> core_state ) <nl> core_waiters = zap_threads ( tsk , mm , core_state , exit_code ); <nl> up_write (& mm -> mmap_sem );
static struct usbip_imported_device * imported_device_init ( struct usbip_imported_ <nl> goto err ; <nl>  <nl> memcpy ( new_cdev , cdev , sizeof (* new_cdev )); <nl> - dlist_unshift ( idev -> cdev_list , ( void *) new_cdev ); <nl> + dlist_unshift ( idev -> cdev_list , ( void *) new_cdev ); <nl> } <nl> } <nl> 
static bool attempt_plug_merge ( struct request_queue * q , struct bio * bio , <nl> struct request * rq ; <nl> bool ret = false ; <nl>  <nl> + if ( blk_queue_nomerges ( q )) <nl> + goto out ; <nl> + <nl> plug = current -> plug ; <nl> if (! plug ) <nl> goto out ;
static long pnv_pci_ioda2_table_alloc_pages ( int nid , __u64 bus_offset , <nl> level_shift = entries_shift + 3 ; <nl> level_shift = max_t ( unsigned , level_shift , PAGE_SHIFT ); <nl>  <nl> + if (( level_shift - 3 ) * levels + page_shift >= 60 ) <nl> + return - EINVAL ; <nl> + <nl> /* Allocate TCE table */ <nl> addr = pnv_pci_ioda2_table_do_alloc_pages ( nid , level_shift , <nl> levels , tce_table_size , & offset , & total_allocated );
static void intel_agp_insert_sg_entries ( struct agp_memory * mem , <nl> off_t pg_start , int mask_type ) <nl> { <nl> int i , j ; <nl> + u32 cache_bits = 0 ; <nl> + <nl> + if ( agp_bridge -> dev -> device == PCI_DEVICE_ID_INTEL_SANDYBRIDGE_HB ) { <nl> + cache_bits = I830_PTE_SYSTEM_CACHED ; <nl> + } <nl>  <nl> for ( i = 0 , j = pg_start ; i < mem -> page_count ; i ++, j ++) { <nl> writel ( agp_bridge -> driver -> mask_memory ( agp_bridge ,
static u32 * vgic_bytemap_get_reg ( struct vgic_bytemap * x , int cpuid , u32 offset ) <nl> { <nl> offset >>= 2 ; <nl> BUG_ON ( offset > ( VGIC_NR_IRQS / 4 )); <nl> - if ( offset < 4 ) <nl> + if ( offset < 8 ) <nl> return x -> percpu [ cpuid ] + offset ; <nl> else <nl> return x -> shared + offset - 8 ;
static struct tty_audit_buf * tty_audit_buf_alloc ( int major , int minor , <nl> { <nl> struct tty_audit_buf * buf ; <nl>  <nl> - buf = kmalloc ( sizeof (* buf ), GFP_KERNEL ); <nl> + buf = kmalloc ( sizeof (* buf ), GFP_KERNEL ); <nl> if (! buf ) <nl> goto err ; <nl> if ( PAGE_SIZE != N_TTY_BUF_SIZE )
static void fimd_dp_clock_enable ( struct exynos_drm_crtc * crtc , bool enable ) <nl> * clock . On these SoCs the bootloader may enable it but any <nl> * power domain off / on will reset it to disable state . <nl> */ <nl> - if ( ctx -> driver_data != & exynos5_fimd_driver_data || <nl> + if ( ctx -> driver_data != & exynos5_fimd_driver_data && <nl> ctx -> driver_data != & exynos5420_fimd_driver_data ) <nl> return ; <nl> 
static int generic_set_freq ( struct dvb_frontend * fe , u32 freq /* in HZ */, <nl> offset += 200000 ; <nl> } <nl> # endif <nl> + break ; <nl> default : <nl> tuner_err (" Unsupported tuner type % d .\ n ", new_type ); <nl> break ;
MODULE_DEVICE_TABLE ( spi , mcp320x_id ); <nl> static struct spi_driver mcp320x_driver = { <nl> . driver = { <nl> . name = " mcp320x ", <nl> + . of_match_table = of_match_ptr ( mcp320x_dt_ids ), <nl> . owner = THIS_MODULE , <nl> }, <nl> . probe = mcp320x_probe ,
int sst_hsw_stream_get_volume ( struct sst_hsw * hsw , struct sst_hsw_stream * stream <nl> return - EINVAL ; <nl>  <nl> sst_dsp_read ( hsw -> dsp , volume , <nl> - stream -> reply . volume_register_address [ channel ], sizeof ( volume )); <nl> + stream -> reply . volume_register_address [ channel ], <nl> + sizeof (* volume )); <nl>  <nl> return 0 ; <nl> }
static int setup_blkring ( struct xenbus_device * dev , <nl> if ( err ) <nl> goto fail ; <nl>  <nl> - err = bind_evtchn_to_irqhandler ( info -> evtchn , <nl> - blkif_interrupt , <nl> - IRQF_SAMPLE_RANDOM , " blkif ", info ); <nl> + err = bind_evtchn_to_irqhandler ( info -> evtchn , blkif_interrupt , 0 , <nl> + " blkif ", info ); <nl> if ( err <= 0 ) { <nl> xenbus_dev_fatal ( dev , err , <nl> " bind_evtchn_to_irqhandler failed ");
struct sched_domain * build_sched_domain ( struct sched_domain_topology_level * tl , <nl> sd -> level = child -> level + 1 ; <nl> sched_domain_level_max = max ( sched_domain_level_max , sd -> level ); <nl> child -> parent = sd ; <nl> + sd -> child = child ; <nl> } <nl> - sd -> child = child ; <nl> set_domain_attribute ( sd , attr ); <nl>  <nl> return sd ;
static int do_garbage_collect ( struct f2fs_sb_info * sbi , <nl>  <nl> for ( segno = start_segno ; segno < end_segno ; segno ++) { <nl>  <nl> - if ( get_valid_blocks ( sbi , segno , 1 ) == 0 ) <nl> + if ( get_valid_blocks ( sbi , segno , 1 ) == 0 || <nl> + unlikely ( f2fs_cp_error ( sbi ))) <nl> goto next ; <nl>  <nl> /* find segment summary of victim */
static int zcache_comp_init ( void ) <nl> # else <nl> if (* zcache_comp_name != '\ 0 ') { <nl> ret = crypto_has_comp ( zcache_comp_name , 0 , 0 ); <nl> - if (! ret ) <nl> + if (! ret ) { <nl> pr_info (" zcache : % s not supported \ n ", <nl> zcache_comp_name ); <nl> - goto out ; <nl> + ret = 1 ; <nl> + goto out ; <nl> + } <nl> } <nl> if (! ret ) <nl> strcpy ( zcache_comp_name , " lzo ");
static int elm_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> pm_runtime_enable (& pdev -> dev ); <nl> - if ( pm_runtime_get_sync (& pdev -> dev )) { <nl> + if ( pm_runtime_get_sync (& pdev -> dev ) < 0 ) { <nl> ret = - EINVAL ; <nl> pm_runtime_disable (& pdev -> dev ); <nl> dev_err (& pdev -> dev , " can ' t enable clock \ n ");
static int __devinit mwl8k_probe ( struct pci_dev * pdev , <nl> if ( rc ) { <nl> printk ( KERN_ERR "% s : Cannot register device \ n ", <nl> wiphy_name ( hw -> wiphy )); <nl> - goto err_free_irq ; <nl> + goto err_free_queues ; <nl> } <nl>  <nl> printk ( KERN_INFO "% s : % s v % d , % pM , % s firmware % u .% u .% u .% u \ n ",
put_clk : <nl>  <nl> static int g2d_remove ( struct platform_device * pdev ) <nl> { <nl> - struct g2d_dev * dev = ( struct g2d_dev *) platform_get_drvdata ( pdev ); <nl> + struct g2d_dev * dev = platform_get_drvdata ( pdev ); <nl>  <nl> v4l2_info (& dev -> v4l2_dev , " Removing " G2D_NAME ); <nl> v4l2_m2m_release ( dev -> m2m_dev );
static int wm8962_readable_register ( unsigned int reg ) <nl>  <nl> static int wm8962_reset ( struct snd_soc_codec * codec ) <nl> { <nl> - return snd_soc_write ( codec , WM8962_SOFTWARE_RESET , 0 ); <nl> + return snd_soc_write ( codec , WM8962_SOFTWARE_RESET , 0x6243 ); <nl> } <nl>  <nl> static const DECLARE_TLV_DB_SCALE ( inpga_tlv , - 2325 , 75 , 0 );
static void tcp_reinit_congestion_control ( struct sock * sk , <nl> icsk -> icsk_ca_ops = ca ; <nl> icsk -> icsk_ca_setsockopt = 1 ; <nl>  <nl> - if ( sk -> sk_state != TCP_CLOSE ) <nl> + if ( sk -> sk_state != TCP_CLOSE ) { <nl> + memset ( icsk -> icsk_ca_priv , 0 , sizeof ( icsk -> icsk_ca_priv )); <nl> tcp_init_congestion_control ( sk ); <nl> + } <nl> } <nl>  <nl> /* Manage refcounts on socket close . */
void i40evf_virtchnl_completion ( struct i40evf_adapter * adapter , <nl> sizeof ( struct i40e_virtchnl_vsi_resource ); <nl> memcpy ( adapter -> vf_res , msg , min ( msglen , len )); <nl> i40e_vf_parse_hw_config (& adapter -> hw , adapter -> vf_res ); <nl> + /* restore current mac address */ <nl> + ether_addr_copy ( adapter -> hw . mac . addr , netdev -> dev_addr ); <nl> i40evf_process_config ( adapter ); <nl> } <nl> break ;
static void gb_tty_set_termios ( struct tty_struct * tty , <nl> send_control ( gb_tty , newctrl ); <nl> } <nl>  <nl> - if ( memcpy (& gb_tty -> line_coding , & newline , sizeof ( newline ))) { <nl> + if ( memcmp (& gb_tty -> line_coding , & newline , sizeof ( newline ))) { <nl> memcpy (& gb_tty -> line_coding , & newline , sizeof ( newline )); <nl> send_line_coding ( gb_tty ); <nl> }
static void dwc2_hc_set_even_odd_frame ( struct dwc2_hsotg * hsotg , <nl> if ( chan -> ep_type == USB_ENDPOINT_XFER_INT || <nl> chan -> ep_type == USB_ENDPOINT_XFER_ISOC ) { <nl> /* 1 if _next_ frame is odd , 0 if it ' s even */ <nl> - if ( dwc2_hcd_get_frame_number ( hsotg ) & 0x1 ) <nl> + if (!( dwc2_hcd_get_frame_number ( hsotg ) & 0x1 )) <nl> * hcchar |= HCCHAR_ODDFRM ; <nl> } <nl> }
static int ade7854_set_irq ( struct device * dev , bool enable ) <nl> else <nl> irqen &= ~ BIT ( 17 ); <nl>  <nl> - ret = st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> - <nl> - return ret ; <nl> + return st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> } <nl>  <nl> static int ade7854_initial_setup ( struct iio_dev * indio_dev )
static void * slob_alloc ( size_t size , gfp_t gfp , int align , int node ) <nl> /* Improve fragment distribution and reduce our average <nl> * search time by starting our next search here . ( see <nl> * Knuth vol 1 , sec 2 . 5 , pg 449 ) */ <nl> - if ( free_slob_pages . next != prev -> next ) <nl> + if ( prev != free_slob_pages . prev && <nl> + free_slob_pages . next != prev -> next ) <nl> list_move_tail (& free_slob_pages , prev -> next ); <nl> break ; <nl> }
static void add_new_bitmap ( struct btrfs_block_group_cache * block_group , <nl> BUG_ON ( block_group -> total_bitmaps >= max_bitmaps ); <nl>  <nl> info -> offset = offset_to_bitmap ( block_group , offset ); <nl> + info -> bytes = 0 ; <nl> link_free_space ( block_group , info ); <nl> block_group -> total_bitmaps ++; <nl> 
extern unsigned long wall_jiffies ; <nl> */ <nl> unsigned long long sched_clock ( void ) <nl> { <nl> - return (( get_clock () - jiffies_timer_cc ) * 1000 ) >> 12 ; <nl> + return (( get_clock () - jiffies_timer_cc ) * 125 ) >> 9 ; <nl> } <nl>  <nl> void tod_to_timeval ( __u64 todval , struct timespec * xtime )
static int p54_tx_qos_accounting_alloc ( struct p54_common * priv , <nl> struct p54_tx_queue_stats * queue ; <nl> unsigned long flags ; <nl>  <nl> - if ( WARN_ON ( p54_queue > P54_QUEUE_NUM )) <nl> + if ( WARN_ON ( p54_queue >= P54_QUEUE_NUM )) <nl> return - EINVAL ; <nl>  <nl> queue = & priv -> tx_stats [ p54_queue ];
static int mxsfb_attach_endpoint ( struct drm_device * drm , <nl>  <nl> int mxsfb_create_output ( struct drm_device * drm ) <nl> { <nl> + struct mxsfb_drm_private * mxsfb = drm -> dev_private ; <nl> struct device_node * ep_np = NULL ; <nl> struct of_endpoint ep ; <nl> int ret ; <nl> int mxsfb_create_output ( struct drm_device * drm ) <nl> } <nl> } <nl>  <nl> + if (! mxsfb -> panel ) <nl> + return - EPROBE_DEFER ; <nl> + <nl> return 0 ; <nl> }
static void omap_pcm_limit_supported_formats ( void ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < SNDRV_PCM_FORMAT_LAST ; i ++) { <nl> + for ( i = 0 ; i <= SNDRV_PCM_FORMAT_LAST ; i ++) { <nl> switch ( snd_pcm_format_physical_width ( i )) { <nl> case 8 : <nl> case 16 :
get_matching_model_microcode ( int cpu , unsigned long start , <nl> unsigned int mc_saved_count = mc_saved_data -> mc_saved_count ; <nl> int i ; <nl>  <nl> - while ( leftover ) { <nl> + while ( leftover && mc_saved_count < ARRAY_SIZE ( mc_saved_tmp )) { <nl> mc_header = ( struct microcode_header_intel *) ucode_ptr ; <nl>  <nl> mc_size = get_totalsize ( mc_header );
acpi_table_parse_entries ( char * id , <nl> unsigned long table_end ; <nl> acpi_size tbl_size ; <nl>  <nl> + if ( acpi_disabled ) <nl> + return - ENODEV ; <nl> + <nl> if (! handler ) <nl> return - EINVAL ; <nl>  <nl> int __init acpi_table_parse ( char * id , acpi_table_handler handler ) <nl> struct acpi_table_header * table = NULL ; <nl> acpi_size tbl_size ; <nl>  <nl> + if ( acpi_disabled ) <nl> + return - ENODEV ; <nl> + <nl> if (! handler ) <nl> return - EINVAL ; <nl> 
static void rtl8169_hw_phy_config ( struct net_device * dev ) <nl> return ; <nl> } <nl>  <nl> - /* phy config for RTL8169s mac_version C chip */ <nl> + if (( tp -> mac_version != RTL_GIGA_MAC_VER_02 ) && <nl> + ( tp -> mac_version != RTL_GIGA_MAC_VER_03 )) <nl> + return ; <nl> + <nl> mdio_write ( ioaddr , 31 , 0x0001 ); // w 31 2 0 1 <nl> mdio_write ( ioaddr , 21 , 0x1000 ); // w 21 15 0 1000 <nl> mdio_write ( ioaddr , 24 , 0x65c7 ); // w 24 15 0 65c7
static int vxlan_newlink ( struct net * net , struct net_device * dev , <nl>  <nl> if (! tb [ IFLA_MTU ]) <nl> dev -> mtu = lowerdev -> mtu - VXLAN_HEADROOM ; <nl> + <nl> + /* update header length based on lower device */ <nl> + dev -> hard_header_len = lowerdev -> hard_header_len + <nl> + VXLAN_HEADROOM ; <nl> } <nl>  <nl> if ( data [ IFLA_VXLAN_TOS ])
static void magicmouse_setup_input ( struct input_dev * input , struct hid_device * h <nl> __set_bit ( BTN_TOOL_TRIPLETAP , input -> keybit ); <nl> __set_bit ( BTN_TOOL_QUADTAP , input -> keybit ); <nl> __set_bit ( BTN_TOUCH , input -> keybit ); <nl> + __set_bit ( INPUT_PROP_POINTER , input -> propbit ); <nl> + __set_bit ( INPUT_PROP_BUTTONPAD , input -> propbit ); <nl> } <nl>  <nl> if ( report_touches ) {
out0 : <nl>  <nl> static int __devexit nuc900_ac97_drvremove ( struct platform_device * pdev ) <nl> { <nl> - <nl> snd_soc_unregister_dai (& pdev -> dev ); <nl>  <nl> clk_put ( nuc900_ac97_data -> clk ); <nl> static int __devexit nuc900_ac97_drvremove ( struct platform_device * pdev ) <nl> release_mem_region ( nuc900_ac97_data -> res -> start , <nl> resource_size ( nuc900_ac97_data -> res )); <nl>  <nl> + kfree ( nuc900_ac97_data ); <nl> nuc900_ac97_data = NULL ; <nl>  <nl> return 0 ;
void __init m68k_setup_node ( int node ) <nl> */ <nl>  <nl> void * empty_zero_page ; <nl> + EXPORT_SYMBOL ( empty_zero_page ); <nl>  <nl> void show_mem ( void ) <nl> {
static int cgroup_rmdir ( struct kernfs_node * kn ) <nl> cgrp = cgroup_kn_lock_live ( kn ); <nl> if (! cgrp ) <nl> return 0 ; <nl> - cgroup_get ( cgrp ); /* for @ kn -> priv clearing */ <nl>  <nl> ret = cgroup_destroy_locked ( cgrp ); <nl>  <nl> cgroup_kn_unlock ( kn ); <nl> - <nl> - cgroup_put ( cgrp ); <nl> return ret ; <nl> } <nl> 
static int __devinit dwc3_omap_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> /* enable all IRQs */ <nl> - dwc3_writel ( omap -> base , USBOTGSS_IRQENABLE_SET_0 , 0x01 ); <nl> + reg = USBOTGSS_IRQO_COREIRQ_ST ; <nl> + dwc3_writel ( omap -> base , USBOTGSS_IRQENABLE_SET_0 , reg ); <nl>  <nl> reg = ( USBOTGSS_IRQ1_OEVT | <nl> USBOTGSS_IRQ1_DRVVBUS_RISE |
static struct xfrm_state * pfkey_msg2xfrm_state ( struct net * net , <nl> x -> aalg -> alg_key_len = key -> sadb_key_bits ; <nl> memcpy ( x -> aalg -> alg_key , key + 1 , keysize ); <nl> } <nl> + x -> aalg -> alg_trunc_len = a -> uinfo . auth . icv_truncbits ; <nl> x -> props . aalgo = sa -> sadb_sa_auth ; <nl> /* x -> algo . flags = sa -> sadb_sa_flags ; */ <nl> }
static int davinci_spi_bufs_dma ( struct spi_device * spi , struct spi_transfer * t ) <nl>  <nl> data1_reg_val = ioread32 ( davinci_spi -> base + SPIDAT1 ); <nl>  <nl> - INIT_COMPLETION ( davinci_spi -> done ); <nl> - <nl> init_completion (& davinci_spi_dma -> dma_rx_completion ); <nl> init_completion (& davinci_spi_dma -> dma_tx_completion ); <nl> 
static int find_probes ( int fd , struct probe_finder * pf ) <nl> . file = pp -> file , <nl> . cu_die = & pf -> cu_die , <nl> . sp_die = & pf -> sp_die , <nl> + . found = 0 , <nl> }; <nl> struct dwarf_callback_param probe_param = { <nl> . data = pf ,
static int mwl8k_request_firmware ( struct mwl8k_priv * priv ) <nl> return 0 ; <nl> } <nl>  <nl> + MODULE_FIRMWARE (" mwl8k / helper_8687 . fw "); <nl> + MODULE_FIRMWARE (" mwl8k / fmimage_8687 . fw "); <nl> + <nl> struct mwl8k_cmd_pkt { <nl> __le16 code ; <nl> __le16 length ;
static int child_wait_callback ( wait_queue_t * wait , unsigned mode , <nl> if (! eligible_child ( wo , p )) <nl> return 0 ; <nl>  <nl> + if (( wo -> wo_flags & __WNOTHREAD ) && wait -> private != p -> parent ) <nl> + return 0 ; <nl> + <nl> return default_wake_function ( wait , mode , sync , key ); <nl> } <nl> 
struct omap_dss_output * omap_dss_get_output ( enum omap_dss_output_id id ) <nl>  <nl> return NULL ; <nl> } <nl> + EXPORT_SYMBOL ( omap_dss_get_output ); <nl>  <nl> static const struct dss_mgr_ops * dss_mgr_ops ; <nl> 
bnad_get_strings ( struct net_device * netdev , u32 stringset , u8 * string ) <nl> for ( i = 0 ; i < BNAD_ETHTOOL_STATS_NUM ; i ++) { <nl> BUG_ON (!( strlen ( bnad_net_stats_strings [ i ]) < <nl> ETH_GSTRING_LEN )); <nl> - memcpy ( string , bnad_net_stats_strings [ i ], <nl> - ETH_GSTRING_LEN ); <nl> + strncpy ( string , bnad_net_stats_strings [ i ], <nl> + ETH_GSTRING_LEN ); <nl> string += ETH_GSTRING_LEN ; <nl> } <nl> bmap = bna_tx_rid_mask (& bnad -> bna );
static int selinux_is_sblabel_mnt ( struct super_block * sb ) <nl> return sbsec -> behavior == SECURITY_FS_USE_XATTR || <nl> sbsec -> behavior == SECURITY_FS_USE_TRANS || <nl> sbsec -> behavior == SECURITY_FS_USE_TASK || <nl> + sbsec -> behavior == SECURITY_FS_USE_NATIVE || <nl> /* Special handling . Genfs but also in - core setxattr handler */ <nl> ! strcmp ( sb -> s_type -> name , " sysfs ") || <nl> ! strcmp ( sb -> s_type -> name , " pstore ") ||
static int asoc_simple_card_dai_link_of ( struct device_node * node , <nl> strlen ( dai_link -> cpu_dai_name ) + <nl> strlen ( dai_link -> codec_dai_name ) + 2 , <nl> GFP_KERNEL ); <nl> + if (! name ) { <nl> + ret = - ENOMEM ; <nl> + goto dai_link_of_err ; <nl> + } <nl> + <nl> sprintf ( name , "% s -% s ", dai_link -> cpu_dai_name , <nl> dai_link -> codec_dai_name ); <nl> dai_link -> name = dai_link -> stream_name = name ;
static void vp_del_vq ( struct virtqueue * vq ) <nl> { <nl> struct virtio_pci_device * vp_dev = to_vp_device ( vq -> vdev ); <nl> struct virtio_pci_vq_info * info = vq -> priv ; <nl> - unsigned long size ; <nl> + unsigned long flags , size ; <nl> + <nl> + spin_lock_irqsave (& vp_dev -> lock , flags ); <nl> + list_del (& info -> node ); <nl> + spin_unlock_irqrestore (& vp_dev -> lock , flags ); <nl>  <nl> iowrite16 ( info -> queue_index , vp_dev -> ioaddr + VIRTIO_PCI_QUEUE_SEL ); <nl> 
static struct irq_chip puv3_low_gpio_chip = { <nl> * irq_controller_lock held , and IRQs disabled . Decode the IRQ <nl> * and call the handler . <nl> */ <nl> - static void <nl> - puv3_gpio_handler ( unsigned int irq , struct irq_desc * desc ) <nl> + static void puv3_gpio_handler ( unsigned int __irq , struct irq_desc * desc ) <nl> { <nl> - unsigned int mask ; <nl> + unsigned int mask , irq ; <nl>  <nl> mask = readl ( GPIO_GEDR ); <nl> do {
static int trusted_update ( struct key * key , const void * data , size_t datalen ) <nl> ret = datablob_parse ( datablob , new_p , new_o ); <nl> if ( ret != Opt_update ) { <nl> ret = - EINVAL ; <nl> + kfree ( new_p ); <nl> goto out ; <nl> } <nl> /* copy old key values , and reseal with new pcrs */
static ssize_t hugetlb_cgroup_write ( struct kernfs_open_file * of , <nl> ret = res_counter_memparse_write_strategy ( buf , & val ); <nl> if ( ret ) <nl> break ; <nl> + val = ALIGN ( val , 1ULL << huge_page_shift (& hstates [ idx ])); <nl> ret = res_counter_set_limit (& h_cg -> hugepage [ idx ], val ); <nl> break ; <nl> default :
static int get_info ( struct net * net , void __user * user , <nl> private = & tmp ; <nl> } <nl> # endif <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . valid_hooks = t -> valid_hooks ; <nl> memcpy ( info . hook_entry , private -> hook_entry , <nl> sizeof ( info . hook_entry ));
out : <nl> static int wl1271_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> { <nl> struct wl1271 * wl = hw -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> mutex_lock (& wl -> mutex ); <nl>  <nl> + if ( unlikely ( wl -> state == WL1271_STATE_OFF )) <nl> + goto out ; <nl> + <nl> ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> if ( ret < 0 ) <nl> goto out ;
static void i40evf_remove ( struct pci_dev * pdev ) <nl> i40evf_reset_interrupt_capability ( adapter ); <nl> } <nl>  <nl> - del_timer_sync (& adapter -> watchdog_timer ); <nl> + if ( adapter -> watchdog_timer . function ) <nl> + del_timer_sync (& adapter -> watchdog_timer ); <nl> + <nl> flush_scheduled_work (); <nl>  <nl> if ( hw -> aq . asq . count )
static int cache_create ( struct cache_args * ca , struct cache ** result ) <nl> atomic_set (& cache -> nr_migrations , 0 ); <nl> init_waitqueue_head (& cache -> migration_wait ); <nl>  <nl> + r = - ENOMEM ; <nl> cache -> nr_dirty = 0 ; <nl> cache -> dirty_bitset = alloc_bitset ( from_cblock ( cache -> cache_size )); <nl> if (! cache -> dirty_bitset ) {
static int dpcm_add_paths ( struct snd_soc_pcm_runtime * fe , int stream , <nl>  <nl> switch ( list -> widgets [ i ]-> id ) { <nl> case snd_soc_dapm_dai_in : <nl> + if ( stream != SNDRV_PCM_STREAM_PLAYBACK ) <nl> + continue ; <nl> + break ; <nl> case snd_soc_dapm_dai_out : <nl> + if ( stream != SNDRV_PCM_STREAM_CAPTURE ) <nl> + continue ; <nl> break ; <nl> default : <nl> continue ;
static int ath9k_htc_aggr_oper ( struct ath9k_htc_priv * priv , <nl> int ret = 0 ; <nl> u8 cmd_rsp ; <nl>  <nl> - if ( tid > ATH9K_HTC_MAX_TID ) <nl> + if ( tid >= ATH9K_HTC_MAX_TID ) <nl> return - EINVAL ; <nl>  <nl> memset (& aggr , 0 , sizeof ( struct ath9k_htc_target_aggr ));
static void usb_hcd_ppc_soc_remove ( struct usb_hcd * hcd , <nl>  <nl> iounmap ( hcd -> regs ); <nl> release_mem_region ( hcd -> rsrc_start , hcd -> rsrc_len ); <nl> - usb_hcd_put ( hcd ); <nl> + usb_put_hcd ( hcd ); <nl> } <nl>  <nl> static int __devinit
struct btrfs_root * btrfs_create_tree ( struct btrfs_trans_handle * trans , <nl> fail : <nl> if ( leaf ) { <nl> btrfs_tree_unlock ( leaf ); <nl> + free_extent_buffer ( root -> commit_root ); <nl> free_extent_buffer ( leaf ); <nl> } <nl> kfree ( root );
static int sony_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> } <nl>  <nl> ret = sony_set_operational ( hdev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_stop ; <nl>  <nl> return 0 ;
static int vpfe_open ( struct file * file ) <nl> if (! vpfe_dev -> initialized ) { <nl> if ( vpfe_initialize_device ( vpfe_dev )) { <nl> mutex_unlock (& vpfe_dev -> lock ); <nl> + v4l2_fh_exit (& fh -> fh ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> }
static int machine__process_kernel_mmap_event ( struct machine * machine , <nl> if ( __machine__create_kernel_maps ( machine , kernel ) < 0 ) <nl> goto out_problem ; <nl>  <nl> + if ( strstr ( dso -> long_name , " vmlinux ")) <nl> + dso__set_short_name ( dso , "[ kernel . vmlinux ]", false ); <nl> + <nl> machine__set_kernel_mmap_len ( machine , event ); <nl>  <nl> /*
static void atl1_free_ring_resources ( struct atl1_adapter * adapter ) <nl>  <nl> rrd_ring -> desc = NULL ; <nl> rrd_ring -> dma = 0 ; <nl> + <nl> + adapter -> cmb . dma = 0 ; <nl> + adapter -> cmb . cmb = NULL ; <nl> + <nl> + adapter -> smb . dma = 0 ; <nl> + adapter -> smb . smb = NULL ; <nl> } <nl>  <nl> static void atl1_setup_mac_ctrl ( struct atl1_adapter * adapter )
void pcibios_free_controller ( struct pci_controller * phb ) <nl> if ( phb -> is_dynamic ) <nl> kfree ( phb ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( pcibios_free_controller ); <nl>  <nl> /* <nl> * The function is used to return the minimal alignment
hauppauge_tuner [] = <nl> { TUNER_ABSENT , " MaxLinear 301 "}, <nl> { TUNER_ABSENT , " Mirics MSi001 "}, <nl> { TUNER_ABSENT , " MaxLinear MxL241SF "}, <nl> - { TUNER_ABSENT , " Xceive XC5000C "}, <nl> + { TUNER_XC5000C , " Xceive XC5000C "}, <nl> { TUNER_ABSENT , " Montage M68TS2020 "}, <nl> }; <nl> 
static int sh_eth_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> skb -> len + 2 ); <nl> txdesc -> addr = dma_map_single (& ndev -> dev , skb -> data , skb -> len , <nl> DMA_TO_DEVICE ); <nl> + if ( dma_mapping_error (& ndev -> dev , txdesc -> addr )) { <nl> + kfree_skb ( skb ); <nl> + return NETDEV_TX_OK ; <nl> + } <nl> txdesc -> buffer_length = skb -> len ; <nl>  <nl> if ( entry >= mdp -> num_tx_ring - 1 )
static void alc269_fill_coef ( struct hda_codec * codec ) <nl>  <nl> if ( spec -> codec_variant != ALC269_TYPE_ALC269VB ) <nl> return ; <nl> + /* ALC271X doesn ' t seem to support these COEFs ( bko # 52181 ) */ <nl> + if (! strcmp ( codec -> chip_name , " ALC271X ")) <nl> + return ; <nl>  <nl> if (( alc_get_coef0 ( codec ) & 0x00ff ) < 0x015 ) { <nl> alc_write_coef_idx ( codec , 0xf , 0x960b );
svga3dsurface_get_mip_size ( surf_size_struct base_level , u32 mip_level ) <nl> size . width = max_t ( u32 , base_level . width >> mip_level , 1 ); <nl> size . height = max_t ( u32 , base_level . height >> mip_level , 1 ); <nl> size . depth = max_t ( u32 , base_level . depth >> mip_level , 1 ); <nl> + size . pad64 = 0 ; <nl> + <nl> return size ; <nl> } <nl> 
static int soc_camera_close ( struct file * file ) <nl> pm_runtime_suspend (& icd -> vdev -> dev ); <nl> pm_runtime_disable (& icd -> vdev -> dev ); <nl>  <nl> - ici -> ops -> remove ( icd ); <nl> if ( ici -> ops -> init_videobuf2 ) <nl> vb2_queue_release (& icd -> vb2_vidq ); <nl> + ici -> ops -> remove ( icd ); <nl>  <nl> soc_camera_power_off ( icd , icl ); <nl> }
static int __init s3c2410fb_probe ( struct platform_device * pdev ) <nl>  <nl> info = fbinfo -> par ; <nl> info -> fb = fbinfo ; <nl> + info -> dev = & pdev -> dev ; <nl> + <nl> platform_set_drvdata ( pdev , fbinfo ); <nl>  <nl> dprintk (" devinit \ n ");
mwifiex_11n_aggregate_pkt ( struct mwifiex_private * priv , <nl> tx_info_aggr -> flags |= MWIFIEX_BUF_FLAG_TDLS_PKT ; <nl> tx_info_aggr -> flags |= MWIFIEX_BUF_FLAG_AGGR_PKT ; <nl> skb_aggr -> priority = skb_src -> priority ; <nl> + skb_aggr -> tstamp = skb_src -> tstamp ; <nl>  <nl> do_gettimeofday (& tv ); <nl> skb_aggr -> tstamp = timeval_to_ktime ( tv );
static int do_tcp_getsockopt ( struct sock * sk , int level , <nl> val = tp -> mss_cache ; <nl> if (! val && (( 1 << sk -> sk_state ) & ( TCPF_CLOSE | TCPF_LISTEN ))) <nl> val = tp -> rx_opt . user_mss ; <nl> + if ( tp -> repair ) <nl> + val = tp -> rx_opt . mss_clamp ; <nl> break ; <nl> case TCP_NODELAY : <nl> val = !!( tp -> nonagle & TCP_NAGLE_OFF );
static int __init longhaul_cpu_init ( struct cpufreq_policy * policy ) <nl> if ( pr == NULL ) goto err_acpi ; <nl>  <nl> cx = & pr -> power . states [ ACPI_STATE_C3 ]; <nl> - if ( cx == NULL || cx -> latency > 1000 ) goto err_acpi ; <nl> + if ( cx -> address == 0 || cx -> latency > 1000 ) goto err_acpi ; <nl>  <nl> /* Now check what we have on this motherboard */ <nl> switch ( c -> x86_model ) {
ssize_t lirc_dev_fop_read ( struct file * file , <nl> return - ENODEV ; <nl> } <nl>  <nl> + if (! LIRC_CAN_REC ( ir -> d . features )) <nl> + return - EINVAL ; <nl> + <nl> dev_dbg ( ir -> d . dev , LOGHEAD " read called \ n ", ir -> d . name , ir -> d . minor ); <nl>  <nl> buf = kzalloc ( ir -> chunk_size , GFP_KERNEL );
static int wm8750_set_bias_level ( struct snd_soc_codec * codec , <nl> case SND_SOC_BIAS_PREPARE : <nl> break ; <nl> case SND_SOC_BIAS_STANDBY : <nl> - if ( codec -> dapm . bias_level == SND_SOC_BIAS_OFF ) { <nl> + if ( snd_soc_codec_get_bias_level ( codec ) == SND_SOC_BIAS_OFF ) { <nl> snd_soc_cache_sync ( codec ); <nl>  <nl> /* Set VMID to 5k */
static void usb_alphatrack_disconnect ( struct usb_interface * intf ) <nl> mutex_unlock (& dev -> mtx ); <nl> usb_alphatrack_delete ( dev ); <nl> } else { <nl> + atomic_set (& dev -> writes_pending , 0 ); <nl> dev -> intf = NULL ; <nl> mutex_unlock (& dev -> mtx ); <nl> } <nl>  <nl> - atomic_set (& dev -> writes_pending , 0 ); <nl> mutex_unlock (& disconnect_mutex ); <nl>  <nl> dev_info (& intf -> dev , " Alphatrack Surface #% d now disconnected \ n ",
static int xhci_plat_probe ( struct platform_device * pdev ) <nl> ret = clk_prepare_enable ( clk ); <nl> if ( ret ) <nl> goto put_hcd ; <nl> + } else if ( PTR_ERR ( clk ) == - EPROBE_DEFER ) { <nl> + ret = - EPROBE_DEFER ; <nl> + goto put_hcd ; <nl> } <nl>  <nl> xhci = hcd_to_xhci ( hcd );
static int super_written ( struct bio * bio , unsigned int bytes_done , int error ) <nl>  <nl> if ( atomic_dec_and_test (& rdev -> mddev -> pending_writes )) <nl> wake_up (& rdev -> mddev -> sb_wait ); <nl> + bio_put ( bio ); <nl> return 0 ; <nl> } <nl> 
static struct pci_device_id agp_sis_pci_table [] = { <nl> . subvendor = PCI_ANY_ID , <nl> . subdevice = PCI_ANY_ID , <nl> }, <nl> - { <nl> - . class = ( PCI_CLASS_BRIDGE_HOST << 8 ), <nl> - . class_mask = ~ 0 , <nl> - . vendor = PCI_VENDOR_ID_SI , <nl> - . device = PCI_DEVICE_ID_SI_760 , <nl> - . subvendor = PCI_ANY_ID , <nl> - . subdevice = PCI_ANY_ID , <nl> - }, <nl> { } <nl> }; <nl> 
EXPORT_SYMBOL ( set_memory_array_uc ); <nl>  <nl> int _set_memory_wc ( unsigned long addr , int numpages ) <nl> { <nl> - return change_page_attr_set (& addr , numpages , <nl> + int ret ; <nl> + ret = change_page_attr_set (& addr , numpages , <nl> + __pgprot ( _PAGE_CACHE_UC_MINUS ), 0 ); <nl> + <nl> + if (! ret ) { <nl> + ret = change_page_attr_set (& addr , numpages , <nl> __pgprot ( _PAGE_CACHE_WC ), 0 ); <nl> + } <nl> + return ret ; <nl> } <nl>  <nl> int set_memory_wc ( unsigned long addr , int numpages )
struct virtio_device_id { <nl>  <nl> struct i2c_device_id { <nl> char name [ I2C_NAME_SIZE ]; <nl> - kernel_ulong_t driver_data ; /* Data private to the driver */ <nl> + kernel_ulong_t driver_data /* Data private to the driver */ <nl> + __attribute__ (( aligned ( sizeof ( kernel_ulong_t )))); <nl> }; <nl>  <nl> 
void iwl_irq_tasklet ( struct iwl_trans * trans ) <nl> } <nl> # endif <nl>  <nl> - spin_unlock_irqrestore (& trans -> shrd -> lock , flags ); <nl> - <nl> /* saved interrupt in inta variable now we can reset trans_pcie -> inta */ <nl> trans_pcie -> inta = 0 ; <nl>  <nl> + spin_unlock_irqrestore (& trans -> shrd -> lock , flags ); <nl> + <nl> /* Now service all interrupt bits discovered above . */ <nl> if ( inta & CSR_INT_BIT_HW_ERR ) { <nl> IWL_ERR ( trans , " Hardware error detected . Restarting .\ n ");
void sun4c_update_mmu_cache ( struct vm_area_struct * vma , unsigned long address , p <nl> unsigned long flags ; <nl> int pseg ; <nl>  <nl> + if ( vma -> vm_mm -> context == NO_CONTEXT ) <nl> + return ; <nl> + <nl> local_irq_save ( flags ); <nl> address &= PAGE_MASK ; <nl> if (( pseg = sun4c_get_segmap ( address )) == invalid_segment ) {
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> } <nl> ++ p ; <nl> } <nl> + /* limit mclk on all R7 370 parts for stability */ <nl> + if ( rdev -> pdev -> device == 0x6811 && <nl> + rdev -> pdev -> revision == 0x81 ) <nl> + max_mclk = 120000 ; <nl>  <nl> if ( rps -> vce_active ) { <nl> rps -> evclk = rdev -> pm . dpm . vce_states [ rdev -> pm . dpm . vce_level ]. evclk ;
static int onenand_write_ops_nolock ( struct mtd_info * mtd , loff_t to , <nl> } <nl>  <nl> /* Only check verify write turn on */ <nl> - ret = onenand_verify ( mtd , ( u_char *) wbuf , to , thislen ); <nl> + ret = onenand_verify ( mtd , buf , to , thislen ); <nl> if ( ret ) { <nl> printk ( KERN_ERR " onenand_write_ops_nolock : verify failed % d \ n ", ret ); <nl> break ;
static int dac33_set_bias_level ( struct snd_soc_codec * codec , <nl> } <nl> break ; <nl> case SND_SOC_BIAS_OFF : <nl> + /* Do not power off , when the codec is already off */ <nl> + if ( codec -> bias_level == SND_SOC_BIAS_OFF ) <nl> + return 0 ; <nl> ret = dac33_hard_power ( codec , 0 ); <nl> if ( ret != 0 ) <nl> return ret ;
static netdev_tx_t eth_start_xmit ( struct sk_buff * skb , <nl> /* Multi frame CDC protocols may store the frame for <nl> * later which is not a dropped frame . <nl> */ <nl> - if ( dev -> port_usb -> supports_multi_frame ) <nl> + if ( dev -> port_usb && <nl> + dev -> port_usb -> supports_multi_frame ) <nl> goto multiframe ; <nl> goto drop ; <nl> }
static struct machine * machines__find_for_cpumode ( struct machines * machines , <nl>  <nl> machine = machines__find ( machines , pid ); <nl> if (! machine ) <nl> - machine = machines__find ( machines , DEFAULT_GUEST_KERNEL_ID ); <nl> + machine = machines__findnew ( machines , DEFAULT_GUEST_KERNEL_ID ); <nl> return machine ; <nl> } <nl> 
out_disable_phy : <nl> out_unregister_bus : <nl> phy_exit ( host -> generic_phy ); <nl> out_host_free : <nl> - devm_kfree ( dev , host ); <nl> ufshcd_set_variant ( hba , NULL ); <nl> out : <nl> return err ;
static int ixgbe_open ( struct net_device * netdev ) <nl> int err ; <nl> u32 num_rx_queues = adapter -> num_rx_queues ; <nl>  <nl> + /* disallow open during test */ <nl> + if ( test_bit ( __IXGBE_TESTING , & adapter -> state )) <nl> + return - EBUSY ; <nl> + <nl> try_intr_reinit : <nl> /* allocate transmit descriptors */ <nl> err = ixgbe_setup_all_tx_resources ( adapter );
cifs_setlk ( struct file * file , struct file_lock * flock , __u32 type , <nl> rc = server -> ops -> mand_unlock_range ( cfile , flock , xid ); <nl>  <nl> out : <nl> - if ( flock -> fl_flags & FL_POSIX ) <nl> - posix_lock_file_wait ( file , flock ); <nl> + if ( flock -> fl_flags & FL_POSIX && ! rc ) <nl> + rc = posix_lock_file_wait ( file , flock ); <nl> return rc ; <nl> } <nl> 
static int socrates_nand_probe ( struct platform_device * ofdev ) <nl> nand_release ( mtd ); <nl>  <nl> out : <nl> - dev_set_drvdata (& ofdev -> dev , NULL ); <nl> iounmap ( host -> io_base ); <nl> kfree ( host ); <nl> return res ; <nl> static int socrates_nand_remove ( struct platform_device * ofdev ) <nl>  <nl> nand_release ( mtd ); <nl>  <nl> - dev_set_drvdata (& ofdev -> dev , NULL ); <nl> iounmap ( host -> io_base ); <nl> kfree ( host ); <nl> 
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> max_sclk = 75000 ; <nl> max_mclk = 80000 ; <nl> } <nl> + /* limit clocks on HD8600 series */ <nl> + if ( rdev -> pdev -> device == 0x6660 && <nl> + rdev -> pdev -> revision == 0x83 ) { <nl> + max_sclk = 75000 ; <nl> + max_mclk = 80000 ; <nl> + } <nl>  <nl> if ( rps -> vce_active ) { <nl> rps -> evclk = rdev -> pm . dpm . vce_states [ rdev -> pm . dpm . vce_level ]. evclk ;
static int ch7006_encoder_create_resources ( struct drm_encoder * encoder , <nl> drm_mode_create_tv_properties ( dev , NUM_TV_NORMS , ch7006_tv_norm_names ); <nl>  <nl> priv -> scale_property = drm_property_create_range ( dev , 0 , " scale ", 0 , 2 ); <nl> + if (! priv -> scale_property ) <nl> + return - ENOMEM ; <nl>  <nl> drm_object_attach_property (& connector -> base , conf -> tv_select_subconnector_property , <nl> priv -> select_subconnector );
static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
SYSCALL_DEFINE2 ( delete_module , const char __user *, name_user , <nl> return - EFAULT ; <nl> name [ MODULE_NAME_LEN - 1 ] = '\ 0 '; <nl>  <nl> + audit_log_kern_module ( name ); <nl> + <nl> if ( mutex_lock_interruptible (& module_mutex ) != 0 ) <nl> return - EINTR ; <nl> 
static int get_exec_file ( struct cfg_devnode * dev_node_obj , <nl> if (! drv_datap || ! drv_datap -> base_img ) <nl> return - EFAULT ; <nl>  <nl> - if ( strlen ( drv_datap -> base_img ) > size ) <nl> + if ( strlen ( drv_datap -> base_img ) >= size ) <nl> return - EINVAL ; <nl>  <nl> strcpy ( exec_file , drv_datap -> base_img );
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> + free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
static int ohci_init ( struct ohci_hcd * ohci ) <nl> return 0 ; <nl>  <nl> ohci -> hcca = dma_alloc_coherent ( hcd -> self . controller , <nl> - sizeof * ohci -> hcca , & ohci -> hcca_dma , 0 ); <nl> + sizeof (* ohci -> hcca ), & ohci -> hcca_dma , GFP_KERNEL ); <nl> if (! ohci -> hcca ) <nl> return - ENOMEM ; <nl> 
static void dump_dev_cap_flags ( struct mlx4_dev * dev , u32 flags ) <nl> int i ; <nl>  <nl> mlx4_dbg ( dev , " DEV_CAP flags :\ n "); <nl> - for ( i = 0 ; i < 32 ; ++ i ) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( fname ); ++ i ) <nl> if ( fname [ i ] && ( flags & ( 1 << i ))) <nl> mlx4_dbg ( dev , " % s \ n ", fname [ i ]); <nl> }
int hardif_add_interface ( char * dev , int if_num ) <nl> return 1 ; <nl>  <nl> out : <nl> - if ( batman_if -> packet_buff ) <nl> - kfree ( batman_if -> packet_buff ); <nl> + kfree ( batman_if -> packet_buff ); <nl> kfree ( batman_if ); <nl> kfree ( dev ); <nl> return - 1 ;
static int em28xx_dvb_init ( struct em28xx * dev ) <nl> dvb -> i2c_client_demod = client ; <nl>  <nl> /* attach tuner */ <nl> + memset (& si2157_config , 0 , sizeof ( si2157_config )); <nl> si2157_config . fe = dvb -> fe [ 0 ]; <nl> memset (& info , 0 , sizeof ( struct i2c_board_info )); <nl> strlcpy ( info . type , " si2157 ", I2C_NAME_SIZE );
static int __init pwrdms_setup ( struct powerdomain * pwrdm ) <nl> if (! pwrdm -> pwrsts ) <nl> return 0 ; <nl>  <nl> - pwrst = kmalloc ( sizeof ( struct power_state ), GFP_KERNEL ); <nl> + pwrst = kmalloc ( sizeof ( struct power_state ), GFP_ATOMIC ); <nl> if (! pwrst ) <nl> return - ENOMEM ; <nl> pwrst -> pwrdm = pwrdm ;
static int parse_addr ( const struct nf_conn * ct , const char * cp , <nl> int family = ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . l3num ; <nl> int ret = 0 ; <nl>  <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> switch ( family ) { <nl> case AF_INET : <nl> ret = in4_pton ( cp , limit - cp , ( u8 *)& addr -> ip , - 1 , & end );
static loff_t fuse_file_llseek ( struct file * file , loff_t offset , int origin ) <nl> mutex_lock (& inode -> i_mutex ); <nl> switch ( origin ) { <nl> case SEEK_END : <nl> + retval = fuse_update_attributes ( inode , NULL , file , NULL ); <nl> + if ( retval ) <nl> + return retval ; <nl> offset += i_size_read ( inode ); <nl> break ; <nl> case SEEK_CUR :
int ceph_init_dentry ( struct dentry * dentry ) <nl> return - ENOMEM ; /* oh well */ <nl>  <nl> spin_lock (& dentry -> d_lock ); <nl> - if ( dentry -> d_fsdata ) /* lost a race */ <nl> + if ( dentry -> d_fsdata ) { <nl> + /* lost a race */ <nl> + kmem_cache_free ( ceph_dentry_cachep , di ); <nl> goto out_unlock ; <nl> + } <nl> di -> dentry = dentry ; <nl> di -> lease_session = NULL ; <nl> dentry -> d_fsdata = di ;
static int wm8904_remove ( struct snd_soc_codec * codec ) <nl>  <nl> wm8904_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8904 -> supplies ), wm8904 -> supplies ); <nl> + kfree ( wm8904 -> retune_mobile_texts ); <nl> + kfree ( wm8904 -> drc_texts ); <nl>  <nl> return 0 ; <nl> }
static int hgcm_call_preprocess_linaddr ( <nl> if (! bounce_buf ) <nl> return - ENOMEM ; <nl>  <nl> + * bounce_buf_ret = bounce_buf ; <nl> + <nl> if ( copy_in ) { <nl> ret = copy_from_user ( bounce_buf , ( void __user *) buf , len ); <nl> if ( ret ) <nl> static int hgcm_call_preprocess_linaddr ( <nl> memset ( bounce_buf , 0 , len ); <nl> } <nl>  <nl> - * bounce_buf_ret = bounce_buf ; <nl> hgcm_call_add_pagelist_size ( bounce_buf , len , extra ); <nl> return 0 ; <nl> }
static int drm_queue_vblank_event ( struct drm_device * dev , int pipe , <nl> if (( vblwait -> request . type & _DRM_VBLANK_NEXTONMISS ) && <nl> ( seq - vblwait -> request . sequence ) <= ( 1 << 23 )) { <nl> vblwait -> request . sequence = seq + 1 ; <nl> + vblwait -> reply . sequence = vblwait -> request . sequence ; <nl> } <nl>  <nl> DRM_DEBUG (" event on vblank count % d , current % d , crtc % d \ n ",
int activate_fd ( int irq , int fd , int type , void * dev_id ) <nl> . events = events , <nl> . current_events = 0 } ); <nl>  <nl> + err = - EBUSY ; <nl> spin_lock_irqsave (& irq_lock , flags ); <nl> for ( irq_fd = active_fds ; irq_fd != NULL ; irq_fd = irq_fd -> next ) { <nl> if (( irq_fd -> fd == fd ) && ( irq_fd -> type == type )) {
void __iomem * __ioremap ( unsigned long phys_addr , unsigned long size , unsigned l <nl> */ <nl> offset = phys_addr & ~ PAGE_MASK ; <nl> phys_addr &= PAGE_MASK ; <nl> - size = PAGE_ALIGN ( last_addr ) - phys_addr ; <nl> + size = PAGE_ALIGN ( last_addr + 1 ) - phys_addr ; <nl>  <nl> /* <nl> * Ok , go for it ..
static enum dma_status rcar_dmac_tx_status ( struct dma_chan * chan , <nl> residue = rcar_dmac_chan_get_residue ( rchan , cookie ); <nl> spin_unlock_irqrestore (& rchan -> lock , flags ); <nl>  <nl> + /* if there ' s no residue , the cookie is complete */ <nl> + if (! residue ) <nl> + return DMA_COMPLETE ; <nl> + <nl> dma_set_residue ( txstate , residue ); <nl>  <nl> return status ;
static int ssm4567_set_power ( struct ssm4567 * ssm4567 , bool enable ) <nl> regcache_cache_only ( ssm4567 -> regmap , ! enable ); <nl>  <nl> if ( enable ) { <nl> + ret = regmap_write ( ssm4567 -> regmap , SSM4567_REG_SOFT_RESET , <nl> + 0x00 ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> ret = regmap_update_bits ( ssm4567 -> regmap , <nl> SSM4567_REG_POWER_CTRL , <nl> SSM4567_POWER_SPWDN , 0x00 );
static inline struct kvm_vcpu * kvm_get_vcpu_by_id ( struct kvm * kvm , int id ) <nl> struct kvm_vcpu * vcpu ; <nl> int i ; <nl>  <nl> + if ( id < 0 || id >= KVM_MAX_VCPUS ) <nl> + return NULL ; <nl> + vcpu = kvm_get_vcpu ( kvm , id ); <nl> + if ( vcpu && vcpu -> vcpu_id == id ) <nl> + return vcpu ; <nl> kvm_for_each_vcpu ( i , vcpu , kvm ) <nl> if ( vcpu -> vcpu_id == id ) <nl> return vcpu ;
int megasas_alloc_cmds ( struct megasas_instance * instance ) <nl> if ( megasas_create_frame_pool ( instance )) { <nl> dev_printk ( KERN_DEBUG , & instance -> pdev -> dev , " Error creating frame DMA pool \ n "); <nl> megasas_free_cmds ( instance ); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> return 0 ;
int ext4_collapse_range ( struct inode * inode , loff_t offset , loff_t len ) <nl> if (! S_ISREG ( inode -> i_mode )) <nl> return - EINVAL ; <nl>  <nl> + if ( EXT4_SB ( inode -> i_sb )-> s_cluster_ratio > 1 ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> trace_ext4_collapse_range ( inode , offset , len ); <nl>  <nl> punch_start = offset >> EXT4_BLOCK_SIZE_BITS ( sb );
static int perf_trace_event_perm ( struct ftrace_event_call * tp_event , <nl> { <nl> /* The ftrace function trace is allowed only for root . */ <nl> if ( ftrace_event_is_function ( tp_event ) && <nl> - perf_paranoid_kernel () && ! capable ( CAP_SYS_ADMIN )) <nl> + perf_paranoid_tracepoint_raw () && ! capable ( CAP_SYS_ADMIN )) <nl> return - EPERM ; <nl>  <nl> /* No tracing , just counting , so no obvious leak */
void free_user_ns ( struct kref * kref ) <nl> struct user_namespace * ns ; <nl>  <nl> ns = container_of ( kref , struct user_namespace , kref ); <nl> + free_uid ( ns -> root_user ); <nl> kfree ( ns ); <nl> } <nl> 
void __init paging_init ( void ) <nl>  <nl> mem_map = NODE_DATA ( 0 )-> node_mem_map ; <nl>  <nl> - memset ( zero_page , 0 , PAGE_SIZE ); <nl> empty_zero_page = virt_to_page ( zero_page ); <nl> flush_dcache_page ( empty_zero_page ); <nl> }
static void free_pcppages_bulk ( struct zone * zone , int count , <nl> list = & pcp -> lists [ migratetype ]; <nl> } while ( list_empty ( list )); <nl>  <nl> + /* This is the only non - empty list . Free them all . */ <nl> + if ( batch_free == MIGRATE_PCPTYPES ) <nl> + batch_free = to_free ; <nl> + <nl> do { <nl> page = list_entry ( list -> prev , struct page , lru ); <nl> /* must delete as __free_one_page list manipulates */
static int do_ipv6_setsockopt ( struct sock * sk , int level , int optname , <nl> break ; <nl>  <nl> case IPV6_TRANSPARENT : <nl> + if (! capable ( CAP_NET_ADMIN )) { <nl> + retv = - EPERM ; <nl> + break ; <nl> + } <nl> if ( optlen < sizeof ( int )) <nl> goto e_inval ; <nl> /* we don ' t have a separate transparent bit for IPV6 we use the one in the IPv4 socket */
static noinline int cow_file_range_inline ( struct btrfs_root * root , <nl> data_len = compressed_size ; <nl>  <nl> if ( start > 0 || <nl> - actual_end >= PAGE_CACHE_SIZE || <nl> - data_len >= BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> + actual_end > PAGE_CACHE_SIZE || <nl> + data_len > BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> (! compressed_size && <nl> ( actual_end & ( root -> sectorsize - 1 )) == 0 ) || <nl> end + 1 < isize ||
static int xl_open ( struct net_device * dev ) <nl> if ( i == 0 ) { <nl> printk ( KERN_WARNING "% s : Not enough memory to allocate rx buffers . Adapter disabled \ n ", dev -> name ) ; <nl> free_irq ( dev -> irq , dev ) ; <nl> + kfree ( xl_priv -> xl_tx_ring ); <nl> + kfree ( xl_priv -> xl_rx_ring ); <nl> return - EIO ; <nl> } <nl> 
static int __do_request ( struct ceph_mds_client * mdsc , <nl> int mds = - 1 ; <nl> int err = - EAGAIN ; <nl>  <nl> - if ( req -> r_err || req -> r_got_result ) <nl> + if ( req -> r_err || req -> r_got_result ) { <nl> + if ( req -> r_aborted ) <nl> + __unregister_request ( mdsc , req ); <nl> goto out ; <nl> + } <nl>  <nl> if ( req -> r_timeout && <nl> time_after_eq ( jiffies , req -> r_started + req -> r_timeout )) {
static ssize_t cifsFYI_proc_write ( struct file * file , const char __user * buffer , <nl> cifsFYI = bv ; <nl> else if (( c [ 0 ] > ' 1 ') && ( c [ 0 ] <= ' 9 ')) <nl> cifsFYI = ( int ) ( c [ 0 ] - ' 0 '); /* see cifs_debug . h for meanings */ <nl> + else <nl> + return - EINVAL ; <nl>  <nl> return count ; <nl> }
static struct kioctx * ioctx_alloc ( unsigned nr_events ) <nl> err_cleanup : <nl> aio_nr_sub ( ctx -> max_reqs ); <nl> err : <nl> - aio_free_ring ( ctx ); <nl> free_percpu ( ctx -> cpu ); <nl> free_percpu ( ctx -> reqs . pcpu_count ); <nl> free_percpu ( ctx -> users . pcpu_count );
void __init init_IRQ ( void ) <nl> struct irq_desc * desc ; <nl> int irq ; <nl>  <nl> - for ( irq = 0 ; irq < nr_irqs ; irq ++) <nl> + for ( irq = 0 ; irq < nr_irqs ; irq ++) { <nl> + desc = irq_to_desc_alloc_node ( irq , 0 ); <nl> desc -> status |= IRQ_NOREQUEST | IRQ_NOPROBE ; <nl> + } <nl>  <nl> init_arch_irq (); <nl> }
static int yam_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> break ; <nl>  <nl> case SIOCYAMGCFG : <nl> + memset (& yi , 0 , sizeof ( yi )); <nl> yi . cfg . mask = 0xffffffff ; <nl> yi . cfg . iobase = yp -> iobase ; <nl> yi . cfg . irq = yp -> irq ;
static int mv643xx_eth_shared_probe ( struct platform_device * pdev ) <nl> * Detect hardware parameters . <nl> */ <nl> msp -> t_clk = ( pd != NULL && pd -> t_clk != 0 ) ? pd -> t_clk : 133000000 ; <nl> - msp -> tx_csum_limit = pd -> tx_csum_limit ? pd -> tx_csum_limit : 9 * 1024 ; <nl> + msp -> tx_csum_limit = ( pd != NULL && pd -> tx_csum_limit ) ? <nl> + pd -> tx_csum_limit : 9 * 1024 ; <nl> infer_hw_params ( msp ); <nl>  <nl> platform_set_drvdata ( pdev , msp );
static void igmp_heard_query ( struct in_device * in_dev , struct sk_buff * skb , <nl> * to be intended in a v3 query . <nl> */ <nl> max_delay = IGMPV3_MRC ( ih3 -> code )*( HZ / IGMP_TIMER_SCALE ); <nl> + if (! max_delay ) <nl> + max_delay = 1 ; /* can ' t mod w / 0 */ <nl> } else { /* v3 */ <nl> if (! pskb_may_pull ( skb , sizeof ( struct igmpv3_query ))) <nl> return ;
static void clear_subscriber_list ( struct snd_seq_client * client , <nl> list_del (& subs -> dest_list ); <nl> else <nl> list_del (& subs -> src_list ); <nl> + up_write (& agrp -> list_mutex ); <nl> unsubscribe_port ( c , aport , agrp , & subs -> info , 1 ); <nl> kfree ( subs ); <nl> - up_write (& agrp -> list_mutex ); <nl> snd_seq_port_unlock ( aport ); <nl> snd_seq_client_unlock ( c ); <nl> }
int BcmGetSectionValEndOffset ( struct bcm_mini_adapter * Adapter , enum bcm_flash2x <nl> case CONTROL_SECTION : <nl> /* Not Clear So Putting failure . confirm and fix it . */ <nl> SectEndOffset = STATUS_FAILURE ; <nl> + break ; <nl> case ISO_IMAGE1_PART2 : <nl> if ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End != UNINIT_PTR_IN_CS ) <nl> SectEndOffset = ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End );
static long swap_inode_boot_loader ( struct super_block * sb , <nl> handle = ext4_journal_start ( inode_bl , EXT4_HT_MOVE_EXTENTS , 2 ); <nl> if ( IS_ERR ( handle )) { <nl> err = - EINVAL ; <nl> - goto swap_boot_out ; <nl> + goto journal_err_out ; <nl> } <nl>  <nl> /* Protect extent tree against block allocations via delalloc */ <nl> static long swap_inode_boot_loader ( struct super_block * sb , <nl>  <nl> ext4_double_up_write_data_sem ( inode , inode_bl ); <nl>  <nl> + journal_err_out : <nl> ext4_inode_resume_unlocked_dio ( inode ); <nl> ext4_inode_resume_unlocked_dio ( inode_bl ); <nl> 
static int __init net_ns_init ( void ) <nl>  <nl> register_pernet_subsys (& net_ns_ops ); <nl>  <nl> - rtnl_register ( PF_UNSPEC , RTM_NEWNSID , rtnl_net_newid , NULL , 0 ); <nl> + rtnl_register ( PF_UNSPEC , RTM_NEWNSID , rtnl_net_newid , NULL , <nl> + RTNL_FLAG_DOIT_UNLOCKED ); <nl> rtnl_register ( PF_UNSPEC , RTM_GETNSID , rtnl_net_getid , rtnl_net_dumpid , <nl> - 0 ); <nl> + RTNL_FLAG_DOIT_UNLOCKED ); <nl>  <nl> return 0 ; <nl> }
static int tridentfb_pan_display ( struct fb_var_screeninfo * var , <nl> unsigned int offset ; <nl>  <nl> debug (" enter \ n "); <nl> - offset = ( var -> xoffset + ( var -> yoffset * var -> xres_virtual )) <nl> - * var -> bits_per_pixel / 32 ; <nl> + offset = ( var -> xoffset + ( var -> yoffset * info -> var . xres_virtual )) <nl> + * info -> var . bits_per_pixel / 32 ; <nl> set_screen_start ( par , offset ); <nl> debug (" exit \ n "); <nl> return 0 ;
struct fb_info * fbtft_framebuffer_alloc ( struct fbtft_display * display , <nl> /* Transmit buffer */ <nl> if ( txbuflen == - 1 ) <nl> txbuflen = vmem_size + 2 ; /* add in case startbyte is used */ <nl> + if ( txbuflen >= vmem_size + 2 ) <nl> + txbuflen = 0 ; <nl>  <nl> # ifdef __LITTLE_ENDIAN <nl> if ((! txbuflen ) && ( bpp > 8 ))
long kernel_wait4 ( pid_t upid , int __user * stat_addr , int options , <nl> __WNOTHREAD | __WCLONE | __WALL )) <nl> return - EINVAL ; <nl>  <nl> + /* - INT_MIN is not defined */ <nl> + if ( upid == INT_MIN ) <nl> + return - ESRCH ; <nl> + <nl> if ( upid == - 1 ) <nl> type = PIDTYPE_MAX ; <nl> else if ( upid < 0 ) {
static int ccdc_config_vdfc ( struct ccdc_vertical_dft * dfc ) <nl> */ <nl> static void ccdc_config_csc ( struct ccdc_csc * csc ) <nl> { <nl> - u32 val1 , val2 ; <nl> + u32 val1 = 0 , val2 ; <nl> int i ; <nl>  <nl> if (! csc -> enable )
static int snd_seq_ioctl_remove_events ( struct snd_seq_client * client , <nl> * No restrictions so for a user client we can clear <nl> * the whole fifo <nl> */ <nl> - if ( client -> type == USER_CLIENT ) <nl> + if ( client -> type == USER_CLIENT && client -> data . user . fifo ) <nl> snd_seq_fifo_clear ( client -> data . user . fifo ); <nl> } <nl> 
static int vc4_plane_mode_set ( struct drm_plane * plane , <nl> /* Control word */ <nl> vc4_dlist_write ( vc4_state , <nl> SCALER_CTL0_VALID | <nl> + VC4_SET_FIELD ( SCALER_CTL0_RGBA_EXPAND_ROUND , SCALER_CTL0_RGBA_EXPAND ) | <nl> ( format -> pixel_order << SCALER_CTL0_ORDER_SHIFT ) | <nl> ( format -> hvs << SCALER_CTL0_PIXEL_FORMAT_SHIFT ) | <nl> VC4_SET_FIELD ( tiling , SCALER_CTL0_TILING ) |
static int fd_ioctl ( struct block_device * bdev , fmode_t mode , unsigned int cmd , <nl> (( cmd & 0x80 ) && ! capable ( CAP_SYS_ADMIN ))) <nl> return - EPERM ; <nl>  <nl> + if ( WARN_ON ( size < 0 || size > sizeof ( inparam ))) <nl> + return - EINVAL ; <nl> + <nl> /* copyin */ <nl> CLEARSTRUCT (& inparam ); <nl> if ( _IOC_DIR ( cmd ) & _IOC_WRITE )
do_sigbus ( struct pt_regs * regs , unsigned long error_code , unsigned long address , <nl> up_read (& mm -> mmap_sem ); <nl>  <nl> /* Kernel mode ? Handle exceptions or die : */ <nl> - if (!( error_code & PF_USER )) <nl> + if (!( error_code & PF_USER )) { <nl> no_context ( regs , error_code , address ); <nl> + return ; <nl> + } <nl>  <nl> /* User - space => ok to do another page fault : */ <nl> if ( is_prefetch ( regs , error_code , address ))
static struct platform_device_id armpmu_plat_device_ids [] = { <nl>  <nl> static int __devinit armpmu_device_probe ( struct platform_device * pdev ) <nl> { <nl> + if (! cpu_pmu ) <nl> + return - ENODEV ; <nl> + <nl> cpu_pmu -> plat_device = pdev ; <nl> return 0 ; <nl> }
static int pn533_target_found ( struct pn533 * dev , <nl> if ( resp -> tg != 1 ) <nl> return - EPROTO ; <nl>  <nl> + memset (& nfc_tgt , 0 , sizeof ( struct nfc_target )); <nl> + <nl> target_data_len = resp_len - sizeof ( struct pn533_poll_response ); <nl>  <nl> switch ( dev -> poll_mod_curr ) {
struct ehci_hcd { /* one per controller */ <nl> # ifdef DEBUG <nl> struct dentry * debug_dir ; <nl> # endif <nl> + <nl> + /* platform - specific data -- must come last */ <nl> + unsigned long priv [ 0 ] __aligned ( sizeof ( s64 )); <nl> }; <nl>  <nl> /* convert between an HCD pointer and the corresponding EHCI_HCD */
static int scsi_report_lun_scan ( struct scsi_target * starget , int bflags , <nl> out_err : <nl> kfree ( lun_data ); <nl> out : <nl> - scsi_device_put ( sdev ); <nl> if ( scsi_device_created ( sdev )) <nl> /* <nl> * the sdev we used didn ' t appear in the report luns scan <nl> */ <nl> __scsi_remove_device ( sdev ); <nl> + scsi_device_put ( sdev ); <nl> return ret ; <nl> } <nl> 
static int do_pages_stat ( struct mm_struct * mm , unsigned long nr_pages , <nl> int err ; <nl>  <nl> for ( i = 0 ; i < nr_pages ; i += chunk_nr ) { <nl> - if ( chunk_nr + i > nr_pages ) <nl> + if ( chunk_nr > nr_pages - i ) <nl> chunk_nr = nr_pages - i ; <nl>  <nl> err = copy_from_user ( chunk_pages , & pages [ i ],
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
static void recalibrate ( struct dp83640_clock * clock ) <nl> u16 cal_gpio , cfg0 , evnt , ptp_trig , trigger , val ; <nl>  <nl> trigger = CAL_TRIGGER ; <nl> - cal_gpio = gpio_tab [ CALIBRATE_GPIO ]; <nl> + cal_gpio = 1 + ptp_find_pin ( clock -> ptp_clock , PTP_PF_PHYSYNC , 0 ); <nl> + if ( cal_gpio < 1 ) { <nl> + pr_err (" PHY calibration pin not avaible - PHY is not calibrated ."); <nl> + return ; <nl> + } <nl>  <nl> mutex_lock (& clock -> extreg_lock ); <nl> 
static int wm8731_hw_params ( struct snd_pcm_substream * substream , <nl> case 24 : <nl> iface |= 0x0008 ; <nl> break ; <nl> + case 32 : <nl> + iface |= 0x000c ; <nl> + break ; <nl> } <nl>  <nl> wm8731_set_deemph ( codec ); <nl> static int wm8731_startup ( struct snd_pcm_substream * substream , <nl> # define WM8731_RATES SNDRV_PCM_RATE_8000_96000 <nl>  <nl> # define WM8731_FORMATS ( SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S20_3LE |\ <nl> - SNDRV_PCM_FMTBIT_S24_LE ) <nl> + SNDRV_PCM_FMTBIT_S24_LE | SNDRV_PCM_FMTBIT_S32_LE ) <nl>  <nl> static const struct snd_soc_dai_ops wm8731_dai_ops = { <nl> . startup = wm8731_startup ,
static inline void __fpu_invalidate_fpregs_state ( struct fpu * fpu ) <nl>  <nl> static inline int fpregs_state_valid ( struct fpu * fpu , unsigned int cpu ) <nl> { <nl> - return fpu == this_cpu_read_stable ( fpu_fpregs_owner_ctx ) && cpu == fpu -> last_cpu ; <nl> + return fpu == this_cpu_read ( fpu_fpregs_owner_ctx ) && cpu == fpu -> last_cpu ; <nl> } <nl>  <nl> /*
static ssize_t bundle_class_show ( struct device * dev , <nl> { <nl> struct gb_bundle * bundle = to_gb_bundle ( dev ); <nl>  <nl> - return sprintf ( buf , "% d \ n ", bundle -> class ); <nl> + return sprintf ( buf , " 0x % 02x \ n ", bundle -> class ); <nl> } <nl> static DEVICE_ATTR_RO ( bundle_class ); <nl> 
# define CR8_RESERVED_BITS (~( unsigned long ) X86_CR8_TPR ) <nl>  <nl> # define KVM_MAX_MCE_BANKS 32 <nl> -# define KVM_MCE_CAP_SUPPORTED MCG_CTL_P <nl> +# define KVM_MCE_CAP_SUPPORTED ( MCG_CTL_P | MCG_SER_P ) <nl>  <nl> /* EFER defaults : <nl> * - enable syscall per default because its emulated by KVM
int rtl92ce_hw_init ( struct ieee80211_hw * hw ) <nl> local_save_flags ( flags ); <nl> local_irq_enable (); <nl>  <nl> + rtlhal -> fw_ready = false ; <nl> rtlpriv -> intf_ops -> disable_aspm ( hw ); <nl> rtstatus = _rtl92ce_init_mac ( hw ); <nl> if (! rtstatus ) { <nl> int rtl92ce_hw_init ( struct ieee80211_hw * hw ) <nl> goto exit ; <nl> } <nl>  <nl> + rtlhal -> fw_ready = true ; <nl> rtlhal -> last_hmeboxnum = 0 ; <nl> rtl92c_phy_mac_config ( hw ); <nl> /* because last function modify RCR , so we update
static int set_link_state ( struct ibmvnic_adapter * adapter , u8 link_state ) <nl> /* Partuial success , delay and re - send */ <nl> mdelay ( 1000 ); <nl> resend = true ; <nl> + } else if ( adapter -> init_done_rc ) { <nl> + netdev_warn ( netdev , " Unable to set link state , rc =% d \ n ", <nl> + adapter -> init_done_rc ); <nl> + return adapter -> init_done_rc ; <nl> } <nl> } while ( resend ); <nl> 
static int cdrom_read_cdda_bpc ( struct cdrom_device_info * cdi , __u8 __user * ubuf , <nl> if (! q ) <nl> return - ENXIO ; <nl>  <nl> + if (! blk_queue_scsi_passthrough ( q )) { <nl> + WARN_ONCE ( true , <nl> + " Attempt read CDDA info through a non - SCSI queue \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> cdi -> last_sense = 0 ; <nl>  <nl> while ( nframes ) {
static int svm_check_intercept ( struct kvm_vcpu * vcpu , <nl> if ( info -> intercept == x86_intercept_in || <nl> info -> intercept == x86_intercept_ins ) { <nl> exit_info |= SVM_IOIO_TYPE_MASK ; <nl> - bytes = info -> src_bytes ; <nl> - } else { <nl> bytes = info -> dst_bytes ; <nl> + } else { <nl> + bytes = info -> src_bytes ; <nl> } <nl>  <nl> if ( info -> intercept == x86_intercept_outs ||
int fib_dump_info ( struct sk_buff * skb , u32 portid , u32 seq , int event , <nl> IN_DEV_IGNORE_ROUTES_WITH_LINKDOWN ( in_dev )) <nl> rtm -> rtm_flags |= RTNH_F_DEAD ; <nl> } <nl> + if ( fi -> fib_nh -> nh_flags & RTNH_F_OFFLOAD ) <nl> + rtm -> rtm_flags |= RTNH_F_OFFLOAD ; <nl> # ifdef CONFIG_IP_ROUTE_CLASSID <nl> if ( fi -> fib_nh [ 0 ]. nh_tclassid && <nl> nla_put_u32 ( skb , RTA_FLOW , fi -> fib_nh [ 0 ]. nh_tclassid ))
static struct radio_si4713_platform_data rx51_si4713_data __initdata_or_module = <nl> . subdev_board_info = & rx51_si4713_board_info , <nl> }; <nl>  <nl> - static struct platform_device rx51_si4713_dev = { <nl> + static struct platform_device rx51_si4713_dev __initdata_or_module = { <nl> . name = " radio - si4713 ", <nl> . id = - 1 , <nl> . dev = {
int fb_find_mode ( struct fb_var_screeninfo * var , <nl> "", ( margins ) ? " with margins " : "", ( interlace ) ? <nl> " interlaced " : ""); <nl>  <nl> + memset (& cvt_mode , 0 , sizeof ( cvt_mode )); <nl> cvt_mode . xres = xres ; <nl> cvt_mode . yres = yres ; <nl> cvt_mode . refresh = ( refresh ) ? refresh : 60 ;
static int storage_probe ( struct usb_interface * intf , <nl> return - ENOMEM ; <nl> } <nl>  <nl> + /* <nl> + * Allow 16 - byte CDBs and thus > 2TB <nl> + */ <nl> + host -> max_cmd_len = 16 ; <nl> us = host_to_us ( host ); <nl> memset ( us , 0 , sizeof ( struct us_data )); <nl> mutex_init (&( us -> dev_mutex ));
static int i2c_hid_hwreset ( struct i2c_client * client ) <nl> static void i2c_hid_get_input ( struct i2c_hid * ihid ) <nl> { <nl> int ret , ret_size ; <nl> - int size = le16_to_cpu ( ihid -> hdesc . wMaxInputLength ); <nl> + int size = ihid -> bufsize ; <nl>  <nl> ret = i2c_master_recv ( ihid -> client , ihid -> inbuf , size ); <nl> if ( ret != size ) {
SYSCALL_DEFINE4 ( epoll_ctl , int , epfd , int , op , int , fd , <nl> if ( op == EPOLL_CTL_ADD ) { <nl> if ( is_file_epoll ( tfile )) { <nl> error = - ELOOP ; <nl> - if ( ep_loop_check ( ep , tfile ) != 0 ) <nl> + if ( ep_loop_check ( ep , tfile ) != 0 ) { <nl> + clear_tfile_check_list (); <nl> goto error_tgt_fput ; <nl> + } <nl> } else <nl> list_add (& tfile -> f_tfile_llink , & tfile_check_list ); <nl> }
static int mspro_block_read_attributes ( struct memstick_dev * card ) <nl> snprintf ( s_attr -> name , sizeof ( s_attr -> name ), <nl> " attr_x % 02x ", attr -> entries [ cnt ]. id ); <nl>  <nl> + sysfs_attr_init (& s_attr -> dev_attr . attr ); <nl> s_attr -> dev_attr . attr . name = s_attr -> name ; <nl> s_attr -> dev_attr . attr . mode = S_IRUGO ; <nl> s_attr -> dev_attr . show = mspro_block_attr_show ( s_attr -> id );
ff_layout_alloc_lseg ( struct pnfs_layout_hdr * lh , <nl> goto out_err_free ; <nl>  <nl> /* fh */ <nl> + rc = - EIO ; <nl> p = xdr_inline_decode (& stream , 4 ); <nl> if (! p ) <nl> goto out_err_free ;
static bool device_init_rings ( struct vnt_private * priv ) <nl> CB_MAX_BUF_SIZE , <nl> & priv -> tx_bufs_dma0 , <nl> GFP_ATOMIC ); <nl> - if ( priv -> tx0_bufs == NULL ) { <nl> + if (! priv -> tx0_bufs ) { <nl> dev_err (& priv -> pcid -> dev , " allocate buf dma memory failed \ n "); <nl>  <nl> dma_free_coherent (& priv -> pcid -> dev ,
struct ahash_request { <nl> void * __ctx [] CRYPTO_MINALIGN_ATTR ; <nl> }; <nl>  <nl> +# define AHASH_REQUEST_ON_STACK ( name , ahash ) \ <nl> + char __ ## name ## _desc [ sizeof ( struct ahash_request ) + \ <nl> + crypto_ahash_reqsize ( ahash )] CRYPTO_MINALIGN_ATTR ; \ <nl> + struct ahash_request * name = ( void *) __ ## name ## _desc <nl> + <nl> /** <nl> * struct ahash_alg - asynchronous message digest definition <nl> * @ init : Initialize the transformation context . Intended only to initialize the
static int ttm_bo_cleanup_refs_and_unlock ( struct ttm_buffer_object * bo , <nl> } <nl>  <nl> ttm_bo_del_from_lru ( bo ); <nl> + if (! list_empty (& bo -> ddestroy ) && ( bo -> resv != & bo -> ttm_resv )) <nl> + reservation_object_fini (& bo -> ttm_resv ); <nl> list_del_init (& bo -> ddestroy ); <nl> kref_put (& bo -> list_kref , ttm_bo_ref_bug ); <nl> 
static int match_session ( struct cifs_ses * ses , struct smb_vol * vol ) <nl> vol -> username ? vol -> username : "", <nl> CIFS_MAX_USERNAME_LEN )) <nl> return 0 ; <nl> - if ( strlen ( vol -> username ) != 0 && <nl> + if (( vol -> username && strlen ( vol -> username ) != 0 ) && <nl> ses -> password != NULL && <nl> strncmp ( ses -> password , <nl> vol -> password ? vol -> password : "",
int __init mon_text_init ( void ) <nl> { <nl> struct dentry * mondir ; <nl>  <nl> - mondir = debugfs_create_dir (" usbmon ", NULL ); <nl> + mondir = debugfs_create_dir (" usbmon ", usb_debug_root ); <nl> if ( IS_ERR ( mondir )) { <nl> printk ( KERN_NOTICE TAG ": debugfs is not available \ n "); <nl> return - ENODEV ;
int omap3isp_csiphy_acquire ( struct isp_csiphy * phy ) <nl> if ( rval < 0 ) <nl> goto done ; <nl>  <nl> - omap3isp_csi2_reset ( phy -> csi2 ); <nl> + rval = omap3isp_csi2_reset ( phy -> csi2 ); <nl> + if ( rval < 0 ) <nl> + goto done ; <nl>  <nl> csiphy_dphy_config ( phy ); <nl> csiphy_lanes_config ( phy );
struct qmp * qmp_get ( struct device * dev ) <nl>  <nl> qmp = platform_get_drvdata ( pdev ); <nl>  <nl> - return qmp ? qmp : ERR_PTR (- EPROBE_DEFER ); <nl> + if (! qmp ) { <nl> + put_device (& pdev -> dev ); <nl> + return ERR_PTR (- EPROBE_DEFER ); <nl> + } <nl> + return qmp ; <nl> } <nl> EXPORT_SYMBOL ( qmp_get ); <nl> 
static void smsc_ircc_sir_wait_hw_transmitter_finish ( struct smsc_ircc_cb * self ) <nl> while ( count -- > 0 && !( inb ( iobase + UART_LSR ) & UART_LSR_TEMT )) <nl> udelay ( 1 ); <nl>  <nl> - if ( count == 0 ) <nl> + if ( count < 0 ) <nl> IRDA_DEBUG ( 0 , "% s (): stuck transmitter \ n ", __func__ ); <nl> } <nl> 
static long kfd_ioctl_create_queue ( struct file * filep , struct kfd_process * p , <nl> p -> pasid , <nl> dev -> id ); <nl>  <nl> - err = pqm_create_queue (& p -> pqm , dev , filep , & q_properties , 0 , <nl> - KFD_QUEUE_TYPE_COMPUTE , & queue_id ); <nl> + err = pqm_create_queue (& p -> pqm , dev , filep , & q_properties , <nl> + 0 , q_properties . type , & queue_id ); <nl> if ( err != 0 ) <nl> goto err_create_queue ; <nl> 
static int ad5686_write_raw ( struct iio_dev * indio_dev , <nl>  <nl> switch ( mask ) { <nl> case 0 : <nl> - if ( val > ( 1 << chan -> scan_type . realbits )) <nl> + if ( val > ( 1 << chan -> scan_type . realbits ) || val < 0 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& indio_dev -> mlock );
static const struct hda_fixup alc269_fixups [] = { <nl> [ ALC269_FIXUP_HEADSET_MODE ] = { <nl> . type = HDA_FIXUP_FUNC , <nl> . v . func = alc_fixup_headset_mode , <nl> + . chained = true , <nl> + . chain_id = ALC255_FIXUP_DELL_WMI_MIC_MUTE_LED <nl> }, <nl> [ ALC269_FIXUP_HEADSET_MODE_NO_HP_MIC ] = { <nl> . type = HDA_FIXUP_FUNC ,
bool dc_stream_set_cursor_position ( <nl> ! pipe_ctx -> ipp || ! pipe_ctx -> surface ) <nl> continue ; <nl>  <nl> + if ( pipe_ctx -> surface -> public . address . type <nl> + == PLN_ADDR_TYPE_VIDEO_PROGRESSIVE ) <nl> + pos_cpy . enable = false ; <nl> + <nl> if ( pipe_ctx -> top_pipe && pipe_ctx -> surface != pipe_ctx -> top_pipe -> surface ) <nl> pos_cpy . enable = false ; <nl> 
nv140_chipset = { <nl> . i2c = gm200_i2c_new , <nl> . ibus = gm200_ibus_new , <nl> . imem = nv50_instmem_new , <nl> + . ltc = gp102_ltc_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
int snd_usbmidi_create ( struct snd_card * card , <nl> else <nl> err = snd_usbmidi_create_endpoints ( umidi , endpoints ); <nl> if ( err < 0 ) { <nl> - snd_usbmidi_free ( umidi ); <nl> return err ; <nl> } <nl> 
iblock_execute_rw ( struct se_cmd * cmd , struct scatterlist * sgl , u32 sgl_nents , <nl> sg_num --; <nl> } <nl>  <nl> - if ( cmd -> prot_type ) { <nl> + if ( cmd -> prot_type && dev -> dev_attrib . pi_prot_type ) { <nl> int rc = iblock_alloc_bip ( cmd , bio_start ); <nl> if ( rc ) <nl> goto fail_put_bios ;
static void cpufreq_policy_free ( struct cpufreq_policy * policy ) <nl>  <nl> static void update_policy_cpu ( struct cpufreq_policy * policy , unsigned int cpu ) <nl> { <nl> + if ( cpu == policy -> cpu ) <nl> + return ; <nl> + <nl> policy -> last_cpu = policy -> cpu ; <nl> policy -> cpu = cpu ; <nl> 
static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
static int goldfish_tty_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_tty_register_device_failed : <nl> - free_irq ( irq , pdev ); <nl> + free_irq ( irq , qtty ); <nl> err_request_irq_failed : <nl> goldfish_tty_current_line_count --; <nl> if ( goldfish_tty_current_line_count == 0 )
static int strip_open ( struct tty_struct * tty ) <nl> * We need a write method . <nl> */ <nl>  <nl> - if ( tty -> ops -> write == NULL ) <nl> + if ( tty -> ops -> write == NULL || tty -> ops -> set_termios == NULL ) <nl> return - EOPNOTSUPP ; <nl>  <nl> /*
static int agpioc_info_wrap ( struct agp_file_private * priv , void __user * arg ) <nl>  <nl> agp_copy_info ( agp_bridge , & kerninfo ); <nl>  <nl> + memset (& userinfo , 0 , sizeof ( userinfo )); <nl> userinfo . version . major = kerninfo . version . major ; <nl> userinfo . version . minor = kerninfo . version . minor ; <nl> userinfo . bridge_id = kerninfo . device -> vendor |
static int _regmap_read ( struct regmap * map , unsigned int reg , <nl> if ( map -> cache_only ) <nl> return - EBUSY ; <nl>  <nl> + if (! regmap_readable ( map , reg )) <nl> + return - EIO ; <nl> + <nl> ret = map -> reg_read ( context , reg , val ); <nl> if ( ret == 0 ) { <nl> # ifdef LOG_DEVICE
static int device_authorization ( struct hdpvr_device * dev ) <nl> hex_dump_to_buffer ( response , 8 , 16 , 1 , print_buf , 5 * buf_size + 1 , 0 ); <nl> v4l2_dbg ( MSG_INFO , hdpvr_debug , & dev -> v4l2_dev , " response : % s \ n ", <nl> print_buf ); <nl> + kfree ( print_buf ); <nl> # endif <nl>  <nl> msleep ( 100 );
int CIFSFindNext ( const int xid , struct cifs_tcon * tcon , <nl> T2_FNEXT_RSP_PARMS * parms ; <nl> char * response_data ; <nl> int rc = 0 ; <nl> - int bytes_returned , name_len ; <nl> + int bytes_returned ; <nl> + unsigned int name_len ; <nl> __u16 params , byte_count ; <nl>  <nl> cFYI ( 1 , " In FindNext ");
int tpm_open ( struct inode * inode , struct file * file ) <nl> return - EBUSY ; <nl> } <nl>  <nl> - chip -> data_buffer = kmalloc ( TPM_BUFSIZE * sizeof ( u8 ), GFP_KERNEL ); <nl> + chip -> data_buffer = kzalloc ( TPM_BUFSIZE , GFP_KERNEL ); <nl> if ( chip -> data_buffer == NULL ) { <nl> clear_bit ( 0 , & chip -> is_open ); <nl> put_device ( chip -> dev );
static void qusb2_phy_set_tune2_param ( struct qusb2_phy * qphy ) <nl> const struct qusb2_phy_cfg * cfg = qphy -> cfg ; <nl> u8 * val ; <nl>  <nl> + /* efuse register is optional */ <nl> + if (! qphy -> cell ) <nl> + return ; <nl> + <nl> /* <nl> * Read efuse register having TUNE2 / 1 parameter ' s high nibble . <nl> * If efuse register shows value as 0x0 , or if we fail to find
static int cb_pcidas_attach ( struct comedi_device * dev , <nl>  <nl> /* 8255 */ <nl> s = dev -> subdevices + 2 ; <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + ret = subdev_8255_init ( dev , s , NULL , <nl> + devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* serial EEPROM , */ <nl> s = dev -> subdevices + 3 ;
static int hwsim_new_radio_nl ( struct sk_buff * msg , struct genl_info * info ) <nl> if ( info -> attrs [ HWSIM_ATTR_CHANNELS ]) <nl> param . channels = nla_get_u32 ( info -> attrs [ HWSIM_ATTR_CHANNELS ]); <nl>  <nl> + if ( param . channels < 1 ) { <nl> + GENL_SET_ERR_MSG ( info , " must have at least one channel "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( param . channels > CFG80211_MAX_NUM_DIFFERENT_CHANNELS ) { <nl> GENL_SET_ERR_MSG ( info , " too many channels specified "); <nl> return - EINVAL ;
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
snd_ali_playback_pointer ( struct snd_pcm_substream * substream ) <nl> spin_unlock (& codec -> reg_lock ); <nl> dev_dbg ( codec -> card -> dev , " playback pointer returned cso =% xh .\ n ", cso ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl>  <nl> static snd_pcm_uframes_t snd_ali_pointer ( struct snd_pcm_substream * substream ) <nl> cso = inw ( ALI_REG ( codec , ALI_CSO_ALPHA_FMS + 2 )); <nl> spin_unlock (& codec -> reg_lock ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl> 
static int iwl_nvm_read_section ( struct iwl_mvm * mvm , u16 section , <nl> offset += ret ; <nl> } <nl>  <nl> - IWL_INFO ( mvm , " NVM section % d read completed \ n ", section ); <nl> + IWL_DEBUG_EEPROM ( mvm -> trans -> dev , <nl> + " NVM section % d read completed \ n ", section ); <nl> return offset ; <nl> } <nl> 
int __init omap_mux_init ( u32 mux_pbase , u32 mux_size , <nl> } <nl>  <nl> # ifdef CONFIG_OMAP_MUX <nl> - omap_mux_package_fixup ( package_subset , superset ); <nl> - omap_mux_package_init_balls ( package_balls , superset ); <nl> + if ( package_subset ) <nl> + omap_mux_package_fixup ( package_subset , superset ); <nl> + if ( package_balls ) <nl> + omap_mux_package_init_balls ( package_balls , superset ); <nl> omap_mux_set_cmdline_signals (); <nl> omap_mux_set_board_signals ( board_mux ); <nl> # endif
kvm_irqfd ( struct kvm * kvm , struct kvm_irqfd * args ) <nl> { <nl> if ( args -> flags & ~( KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE )) <nl> return - EINVAL ; <nl> + if ( args -> gsi >= KVM_MAX_IRQ_ROUTES ) <nl> + return - EINVAL ; <nl>  <nl> if ( args -> flags & KVM_IRQFD_FLAG_DEASSIGN ) <nl> return kvm_irqfd_deassign ( kvm , args );
static int goldfish_audio_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_misc_register_failed : <nl> + free_irq ( data -> irq , data ); <nl> err_request_irq_failed : <nl> dma_free_coherent (& pdev -> dev , COMBINED_BUFFER_SIZE , <nl> data -> buffer_virt , data -> buffer_phys );
static int atmel_ssc_set_dai_clkdiv ( struct snd_soc_dai * cpu_dai , <nl> * transmit and receive , so if a value has already <nl> * been set , it must match this value . <nl> */ <nl> - if ( ssc_p -> cmr_div == 0 ) <nl> + if ( ssc_p -> dir_mask != <nl> + ( SSC_DIR_MASK_PLAYBACK | SSC_DIR_MASK_CAPTURE )) <nl> + ssc_p -> cmr_div = div ; <nl> + else if ( ssc_p -> cmr_div == 0 ) <nl> ssc_p -> cmr_div = div ; <nl> else <nl> if ( div != ssc_p -> cmr_div )
int __init acpi_table_init ( void ) <nl>  <nl> static int __init acpi_parse_apic_instance ( char * str ) <nl> { <nl> + if (! str ) <nl> + return - EINVAL ; <nl>  <nl> acpi_apic_instance = simple_strtoul ( str , NULL , 0 ); <nl> 
static void arm_ccn_pmu_xp_dt_config ( struct perf_event * event , int enable ) <nl> struct arm_ccn_component * xp ; <nl> u32 val , dt_cfg ; <nl>  <nl> + /* Nothing to do for cycle counter */ <nl> + if ( hw -> idx == CCN_IDX_PMU_CYCLE_COUNTER ) <nl> + return ; <nl> + <nl> if ( CCN_CONFIG_TYPE ( event -> attr . config ) == CCN_TYPE_XP ) <nl> xp = & ccn -> xp [ CCN_CONFIG_XP ( event -> attr . config )]; <nl> else
static void ip6gre_tnl_link_config ( struct ip6_tnl * t , int set_mtu ) <nl> dev -> mtu = rt -> dst . dev -> mtu - addend ; <nl> if (!( t -> parms . flags & IP6_TNL_F_IGN_ENCAP_LIMIT )) <nl> dev -> mtu -= 8 ; <nl> + if ( dev -> type == ARPHRD_ETHER ) <nl> + dev -> mtu -= ETH_HLEN ; <nl>  <nl> if ( dev -> mtu < IPV6_MIN_MTU ) <nl> dev -> mtu = IPV6_MIN_MTU ;
static void copy_from_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy <nl>  <nl> static void copy_to_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy_info * p , int dir ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> sel , & xp -> selector , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & xp -> lft , sizeof ( p -> lft )); <nl> memcpy (& p -> curlft , & xp -> curlft , sizeof ( p -> curlft ));
int brcmf_fweh_activate_events ( struct brcmf_if * ifp ) <nl> int i , err ; <nl> s8 eventmask [ BRCMF_EVENTING_MASK_LEN ]; <nl>  <nl> + memset ( eventmask , 0 , sizeof ( eventmask )); <nl> for ( i = 0 ; i < BRCMF_E_LAST ; i ++) { <nl> if ( ifp -> drvr -> fweh . evt_handler [ i ]) { <nl> brcmf_dbg ( EVENT , " enable event % s \ n ",
static int usbdev_open ( struct inode * inode , struct file * file ) <nl> if (! ps ) <nl> goto out ; <nl>  <nl> - ret = - ENOENT ; <nl> + ret = - ENODEV ; <nl>  <nl> /* usbdev device - node */ <nl> if ( imajor ( inode ) == USB_DEVICE_MAJOR )
static int ov5642_set_fmt ( struct v4l2_subdev * sd , <nl> mf -> field = V4L2_FIELD_NONE ; <nl>  <nl> if ( format -> which == V4L2_SUBDEV_FORMAT_ACTIVE ) <nl> - priv -> fmt = ov5642_find_datafmt ( mf -> code ); <nl> + priv -> fmt = fmt ; <nl> else <nl> cfg -> try_fmt = * mf ; <nl> return 0 ;
static int usbat_probe ( struct usb_interface * intf , <nl> us -> transport_name = " Shuttle USBAT "; <nl> us -> transport = usbat_flash_transport ; <nl> us -> transport_reset = usb_stor_CB_reset ; <nl> - us -> max_lun = 1 ; <nl> + us -> max_lun = 0 ; <nl>  <nl> result = usb_stor_probe2 ( us ); <nl> return result ;
void blk_execute_rq_nowait ( struct request_queue * q , struct gendisk * bd_disk , <nl> spin_lock_irq ( q -> queue_lock ); <nl> __elv_add_request ( q , rq , where , 1 ); <nl> __generic_unplug_device ( q ); <nl> + /* the queue is stopped so it won ' t be plugged + unplugged */ <nl> + if ( blk_pm_resume_request ( rq )) <nl> + q -> request_fn ( q ); <nl> spin_unlock_irq ( q -> queue_lock ); <nl> } <nl> EXPORT_SYMBOL_GPL ( blk_execute_rq_nowait );
struct snd_usb_endpoint * snd_usb_add_endpoint ( struct snd_usb_audio * chip , <nl> struct snd_usb_endpoint * ep ; <nl> int is_playback = direction == SNDRV_PCM_STREAM_PLAYBACK ; <nl>  <nl> + if ( WARN_ON (! alts )) <nl> + return NULL ; <nl> + <nl> mutex_lock (& chip -> mutex ); <nl>  <nl> list_for_each_entry ( ep , & chip -> ep_list , list ) {
static int ioat_pci_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> device -> version = readb ( device -> reg_base + IOAT_VER_OFFSET ); <nl> if ( device -> version >= IOAT_VER_3_0 ) { <nl> + if ( is_skx_ioat ( pdev )) <nl> + device -> version = IOAT_VER_3_2 ; <nl> err = ioat3_dma_probe ( device , ioat_dca_enabled ); <nl>  <nl> if ( device -> version >= IOAT_VER_3_3 )
mwifiex_cmd_802_11_ad_hoc_start ( struct mwifiex_private * priv , <nl>  <nl> memset ( adhoc_start -> ssid , 0 , IEEE80211_MAX_SSID_LEN ); <nl>  <nl> + if ( req_ssid -> ssid_len > IEEE80211_MAX_SSID_LEN ) <nl> + req_ssid -> ssid_len = IEEE80211_MAX_SSID_LEN ; <nl> memcpy ( adhoc_start -> ssid , req_ssid -> ssid , req_ssid -> ssid_len ); <nl>  <nl> mwifiex_dbg ( adapter , INFO , " info : ADHOC_S_CMD : SSID = % s \ n ",
static void __devinit dfx_bus_init ( struct net_device * dev ) <nl> * Interrupts are disabled at the adapter bus - specific logic . <nl> */ <nl>  <nl> - static void __devinit dfx_bus_uninit ( struct net_device * dev ) <nl> + static void __devexit dfx_bus_uninit ( struct net_device * dev ) <nl> { <nl> DFX_board_t * bp = netdev_priv ( dev ); <nl> struct device * bdev = bp -> bus_dev ;
static void __devinit pnv_pci_ioda_setup_seg ( void ) <nl> } <nl> } <nl>  <nl> + static void __devinit pnv_pci_ioda_setup_DMA ( void ) <nl> +{ <nl> + struct pci_controller * hose , * tmp ; <nl> + <nl> + list_for_each_entry_safe ( hose , tmp , & hose_list , list_node ) { <nl> + pnv_ioda_setup_dma ( hose -> private_data ); <nl> + } <nl> +} <nl> + <nl> static void __devinit pnv_pci_ioda_fixup ( void ) <nl> { <nl> pnv_pci_ioda_setup_PEs (); <nl> pnv_pci_ioda_setup_seg (); <nl> + pnv_pci_ioda_setup_DMA (); <nl> } <nl>  <nl> /*
int xhci_queue_bulk_tx ( struct xhci_hcd * xhci , gfp_t mem_flags , <nl> send_addr = addr ; <nl>  <nl> /* Queue the TRBs , even if they are zero - length */ <nl> - for ( enqd_len = 0 ; enqd_len < full_len ; enqd_len += trb_buff_len ) { <nl> + for ( enqd_len = 0 ; first_trb || enqd_len < full_len ; <nl> + enqd_len += trb_buff_len ) { <nl> field = TRB_TYPE ( TRB_NORMAL ); <nl>  <nl> /* TRB buffer should not cross 64KB boundaries */
static int usb_hcd_fsl_probe ( const struct hc_driver * driver , <nl> */ <nl> if ( pdata -> init && pdata -> init ( pdev )) { <nl> retval = - ENODEV ; <nl> - goto err3 ; <nl> + goto err4 ; <nl> } <nl>  <nl> /* Enable USB controller , 83xx or 8536 */
static int __open_dso ( struct dso * dso , struct machine * machine ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (! is_regular_file ( name )) <nl> + if (! is_regular_file ( name )) { <nl> + free ( name ); <nl> return - EINVAL ; <nl> + } <nl>  <nl> fd = do_open ( name ); <nl> free ( name );
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( ieee80211_vif_is_mesh (& sdata -> vif ) && <nl> ! sdata -> u . mesh . user_mpm ) <nl> init_timer (& sta -> plink_timer ); <nl> + sta -> nonpeer_pm = NL80211_MESH_POWER_ACTIVE ; <nl> # endif <nl>  <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN );
static int ad7877_rx ( struct ad7877 * ts ) <nl> Rt /= z1 ; <nl> Rt = ( Rt + 2047 ) >> 12 ; <nl>  <nl> + /* <nl> + * Sample found inconsistent , pressure is beyond <nl> + * the maximum . Don ' t report it to user space . <nl> + */ <nl> + if ( Rt > ts -> pressure_max ) <nl> + return - EINVAL ; <nl> + <nl> if (! timer_pending (& ts -> timer )) <nl> input_report_key ( input_dev , BTN_TOUCH , 1 ); <nl> 
static struct wlan_bssid_ex * collect_bss_info ( struct rtw_adapter * padapter , <nl> memcpy ( bssid -> Ssid . ssid , p + 2 , p [ 1 ]); <nl> bssid -> Ssid . ssid_len = p [ 1 ]; <nl>  <nl> - memset ( bssid -> SupportedRates , 0 , NDIS_802_11_LENGTH_RATES_EX ); <nl> - <nl> /* checking rate info ... */ <nl> i = 0 ; <nl> p = cfg80211_find_ie ( WLAN_EID_SUPP_RATES , bssid -> IEs + ie_offset ,
int assoc_array_gc ( struct assoc_array * array , <nl> shortcut = assoc_array_ptr_to_shortcut ( ptr ); <nl> slot = shortcut -> parent_slot ; <nl> cursor = shortcut -> back_pointer ; <nl> + if (! cursor ) <nl> + goto gc_complete ; <nl> } else { <nl> slot = node -> parent_slot ; <nl> cursor = ptr ; <nl> } <nl> - BUG_ON (! ptr ); <nl> + BUG_ON (! cursor ); <nl> node = assoc_array_ptr_to_node ( cursor ); <nl> slot ++; <nl> goto continue_node ;
static unsigned char GetXG27FPBits ( struct vb_device_info * pVBInfo ) <nl> /* enable GPIOA / B / C read */ <nl> xgifb_reg_and_or ( pVBInfo -> P3d4 , 0x4A , ~ 0x03 , 0x03 ); <nl> temp = xgifb_reg_get ( pVBInfo -> P3d4 , 0x48 ); <nl> - if ( temp <= 2 ) <nl> - temp &= 0x03 ; <nl> - else <nl> + if ( temp > 2 ) <nl> temp = (( temp & 0x04 ) >> 1 ) | ((~ temp ) & 0x01 ); <nl>  <nl> xgifb_reg_set ( pVBInfo -> P3d4 , 0x4A , CR4A );
static int virtnet_set_channels ( struct net_device * dev , <nl> if ( channels -> rx_count || channels -> tx_count || channels -> other_count ) <nl> return - EINVAL ; <nl>  <nl> - if ( queue_pairs > vi -> max_queue_pairs ) <nl> + if ( queue_pairs > vi -> max_queue_pairs || queue_pairs == 0 ) <nl> return - EINVAL ; <nl>  <nl> get_online_cpus ();
enum dc_status dce110_apply_ctx_to_hw ( <nl> if ( pipe_ctx -> stream == pipe_ctx_old -> stream ) <nl> continue ; <nl>  <nl> + if ( pipe_ctx -> stream && pipe_ctx_old -> stream <nl> + && ! pipe_need_reprogram ( pipe_ctx_old , pipe_ctx )) <nl> + continue ; <nl> + <nl> if ( pipe_ctx -> top_pipe ) <nl> continue ; <nl> 
void ieee80211_csa_finalize_work ( struct work_struct * work ) <nl> container_of ( work , struct ieee80211_sub_if_data , <nl> csa_finalize_work ); <nl> struct ieee80211_local * local = sdata -> local ; <nl> - int err , changed ; <nl> + int err , changed = 0 ; <nl>  <nl> if (! ieee80211_sdata_running ( sdata )) <nl> return ;
static int acm_probe ( struct usb_interface * intf , <nl> } <nl>  <nl>  <nl> - if ( data_interface -> cur_altsetting -> desc . bNumEndpoints < 2 ) <nl> + if ( data_interface -> cur_altsetting -> desc . bNumEndpoints < 2 || <nl> + control_interface -> cur_altsetting -> desc . bNumEndpoints == 0 ) <nl> return - EINVAL ; <nl>  <nl> epctrl = & control_interface -> cur_altsetting -> endpoint [ 0 ]. desc ;
static int __init sa1110_clk_init ( void ) <nl> struct sdram_params * sdram ; <nl> const char * name = sdram_name ; <nl>  <nl> + if (! cpu_is_sa1110 ()) <nl> + return - ENODEV ; <nl> + <nl> if (! name [ 0 ]) { <nl> if ( machine_is_assabet ()) <nl> name = " TC59SM716 - CL3 ";
static int validate_user_key ( struct fscrypt_info * crypt_info , <nl> goto out ; <nl> } <nl> ukp = user_key_payload_locked ( keyring_key ); <nl> + if (! ukp ) { <nl> + /* key was revoked before we acquired its semaphore */ <nl> + res = - EKEYREVOKED ; <nl> + goto out ; <nl> + } <nl> if ( ukp -> datalen != sizeof ( struct fscrypt_key )) { <nl> res = - EINVAL ; <nl> goto out ;
static int annotate_browser__run ( struct annotate_browser * self , int evidx , <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' H ': <nl> + case ' h ': <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' S ': <nl> + case ' s ': <nl> if ( annotate_browser__toggle_source ( self )) <nl> ui_helpline__puts ( help ); <nl> continue ;
int ath10k_wmi_start_scan ( struct ath10k * ar , <nl> return ret ; <nl>  <nl> if ( test_bit ( ATH10K_FW_FEATURE_WMI_10X , ar -> fw_features )) <nl> - len = sizeof ( struct wmi_start_scan_cmd ) + <nl> + len = sizeof ( struct wmi_10x_start_scan_cmd ) + <nl> ath10k_wmi_start_scan_tlvs_len ( arg ); <nl> else <nl> - len = sizeof ( struct wmi_10x_start_scan_cmd ) + <nl> + len = sizeof ( struct wmi_start_scan_cmd ) + <nl> ath10k_wmi_start_scan_tlvs_len ( arg ); <nl>  <nl> skb = ath10k_wmi_alloc_skb ( ar , len );
static int saa7134_s_fmt_overlay ( struct file * file , void * priv , <nl> struct saa7134_fh * fh = priv ; <nl> struct saa7134_dev * dev = fh -> dev ; <nl> int err ; <nl> - unsigned int flags ; <nl> + unsigned long flags ; <nl>  <nl> if ( saa7134_no_overlay > 0 ) { <nl> printk ( KERN_ERR " V4L2_BUF_TYPE_VIDEO_OVERLAY : no_overlay \ n ");
static long pmcraid_ioctl_passthrough ( <nl> pmcraid_err (" couldn ' t build passthrough ioadls \ n "); <nl> goto out_free_buffer ; <nl> } <nl> + } else if ( request_size < 0 ) { <nl> + rc = - EINVAL ; <nl> + goto out_free_buffer ; <nl> } <nl>  <nl> /* If data is being written into the device , copy the data from user
static irqreturn_t rtl8169_interrupt ( int irq , void * dev_instance ) <nl> handled = 1 ; <nl>  <nl> rtl_irq_disable ( tp ); <nl> - napi_schedule (& tp -> napi ); <nl> + napi_schedule_irqoff (& tp -> napi ); <nl> } <nl> } <nl> return IRQ_RETVAL ( handled );
static void m_stop ( struct seq_file * m , void * v ) <nl> struct proc_maps_private * priv = m -> private ; <nl> struct vm_area_struct * vma = v ; <nl>  <nl> - vma_stop ( priv , vma ); <nl> + if (! IS_ERR ( vma )) <nl> + vma_stop ( priv , vma ); <nl> if ( priv -> task ) <nl> put_task_struct ( priv -> task ); <nl> }
void hpsb_packet_received ( struct hpsb_host * host , quadlet_t * data , size_t size , <nl> /* return the index ( within a minor number block ) of a file */ <nl> static inline unsigned char ieee1394_file_to_instance ( struct file * file ) <nl> { <nl> - return file -> f_dentry -> d_inode -> i_cindex ; <nl> + return file -> f_path . dentry -> d_inode -> i_cindex ; <nl> } <nl>  <nl> extern int hpsb_disable_irm ;
static inline int uinput_request_reserve_slot ( struct uinput_device * udev , struct <nl>  <nl> static void uinput_request_done ( struct uinput_device * udev , struct uinput_request * request ) <nl> { <nl> - complete (& request -> done ); <nl> - <nl> /* Mark slot as available */ <nl> udev -> requests [ request -> id ] = NULL ; <nl> wake_up_interruptible (& udev -> requests_waitq ); <nl> + <nl> + complete (& request -> done ); <nl> } <nl>  <nl> static int uinput_request_submit ( struct input_dev * dev , struct uinput_request * request )
void arch_update_cpu_topology ( void ); <nl> . busy_idx = 3 , \ <nl> . idle_idx = 3 , \ <nl> . flags = SD_LOAD_BALANCE \ <nl> - | SD_SERIALIZE , \ <nl> + | SD_BALANCE_NEWIDLE \ <nl> + | SD_WAKE_AFFINE \ <nl> + | SD_SERIALIZE , \ <nl> . last_balance = jiffies , \ <nl> . balance_interval = 64 , \ <nl> }
ufs_extend_tail ( struct inode * inode , u64 writes_to , <nl>  <nl> p = ufs_get_direct_data_ptr ( uspi , ufsi , block ); <nl> tmp = ufs_new_fragments ( inode , p , lastfrag , ufs_data_ptr_to_cpu ( sb , p ), <nl> - new_size , err , locked_page ); <nl> + new_size - ( lastfrag & uspi -> s_fpbmask ), err , <nl> + locked_page ); <nl> return tmp != 0 ; <nl> } <nl> 
static ssize_t unix_stream_sendpage ( struct socket * socket , struct page * page , <nl> * this - does no harm <nl> */ <nl> consume_skb ( newskb ); <nl> + newskb = NULL ; <nl> } <nl>  <nl> if ( skb_append_pagefrags ( skb , page , offset , size )) {
+// SPDX - License - Identifier : GPL - 2 . 0 <nl> /* <nl> - * linux / drivers / pinctrl / pinctrl - rt2880 . c <nl> - * <nl> - * This program is free software ; you can redistribute it and / or modify <nl> - * it under the terms of the GNU General Public License version 2 as <nl> - * publishhed by the Free Software Foundation . <nl> - * <nl> * Copyright ( C ) 2013 John Crispin < blogic @ openwrt . org > <nl> */ <nl> 
int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> cur_devices -> num_devices --; <nl> cur_devices -> total_devices --; <nl> + /* Update total_devices of the parent fs_devices if it ' s seed */ <nl> + if ( cur_devices != fs_devices ) <nl> + fs_devices -> total_devices --; <nl>  <nl> if ( test_bit ( BTRFS_DEV_STATE_MISSING , & device -> dev_state )) <nl> cur_devices -> missing_devices --;
static int eseqiv_givencrypt ( struct skcipher_givcrypt_request * req ) <nl> if ( err ) <nl> goto out ; <nl>  <nl> - eseqiv_complete2 ( req ); <nl> + if ( giv != req -> giv ) <nl> + eseqiv_complete2 ( req ); <nl>  <nl> out : <nl> return err ;
static struct pci_driver jr3_pci_pci_driver = { <nl> module_comedi_pci_driver ( jr3_pci_driver , jr3_pci_pci_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for JR3 / PCI force sensor board "); <nl> MODULE_LICENSE (" GPL "); <nl> MODULE_FIRMWARE (" comedi / jr3pci . idm ");
int sas_smp_get_phy_events ( struct sas_phy * phy ) <nl> phy -> phy_reset_problem_count = scsi_to_u32 (& resp [ 24 ]); <nl>  <nl> out : <nl> + kfree ( req ); <nl> kfree ( resp ); <nl> return res ; <nl> 
static int remove_and_add_spares ( struct mddev * mddev , <nl> ! test_bit ( Bitmap_sync , & rdev -> flags ))) <nl> continue ; <nl>  <nl> - rdev -> recovery_offset = 0 ; <nl> + if ( rdev -> saved_raid_disk < 0 ) <nl> + rdev -> recovery_offset = 0 ; <nl> if ( mddev -> pers -> <nl> hot_add_disk ( mddev , rdev ) == 0 ) { <nl> if ( sysfs_link_rdev ( mddev , rdev ))
static void ath9k_hw_ar9300_set_txpower ( struct ath_hw * ah , <nl> regulatory -> max_power_level = targetPowerValT2 [ i ]; <nl> } <nl>  <nl> + ath9k_hw_update_regulatory_maxpower ( ah ); <nl> + <nl> if ( test ) <nl> return ; <nl> 
static int davinci_gpio_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& chips [ i ]. lock ); <nl>  <nl> regs = gpio2regs ( base ); <nl> + if (! regs ) <nl> + return - ENXIO ; <nl> chips [ i ]. regs = regs ; <nl> chips [ i ]. set_data = & regs -> set_data ; <nl> chips [ i ]. clr_data = & regs -> clr_data ;
void usb_serial_generic_resubmit_read_urb ( struct usb_serial_port * port , <nl> (( serial -> type -> read_bulk_callback ) ? <nl> serial -> type -> read_bulk_callback : <nl> usb_serial_generic_read_bulk_callback ), port ); <nl> + <nl> result = usb_submit_urb ( urb , mem_flags ); <nl> - if ( result ) <nl> + if ( result && result != - EPERM ) { <nl> dev_err (& port -> dev , <nl> "% s - failed resubmitting read urb , error % d \ n ", <nl> __func__ , result ); <nl> + } <nl> } <nl> EXPORT_SYMBOL_GPL ( usb_serial_generic_resubmit_read_urb ); <nl> 
static int sym_prepare_setting ( struct Scsi_Host * shost , struct sym_hcb * np , stru <nl>  <nl> tp -> usrflags |= ( SYM_DISC_ENABLED | SYM_TAGS_ENABLED ); <nl> tp -> usrtags = SYM_SETUP_MAX_TAG ; <nl> + tp -> usr_width = np -> maxwide ; <nl> + tp -> usr_period = 9 ; <nl>  <nl> sym_nvram_setup_target ( tp , i , nvram ); <nl> 
static int eb_relocate_vma ( struct i915_execbuffer * eb , struct i915_vma * vma ) <nl> * to read . However , if the array is not writable the user loses <nl> * the updated relocation values . <nl> */ <nl> - if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof ( urelocs )))) <nl> + if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof (* urelocs )))) <nl> return - EFAULT ; <nl>  <nl> do {
struct ata_host * ata_host_alloc ( struct device * dev , int max_ports ) <nl> return NULL ; <nl>  <nl> if (! devres_open_group ( dev , NULL , GFP_KERNEL )) <nl> - return NULL ; <nl> + goto err_free ; <nl>  <nl> dr = devres_alloc ( ata_devres_release , 0 , GFP_KERNEL ); <nl> if (! dr ) <nl> struct ata_host * ata_host_alloc ( struct device * dev , int max_ports ) <nl>  <nl> err_out : <nl> devres_release_group ( dev , NULL ); <nl> + err_free : <nl> + kfree ( host ); <nl> return NULL ; <nl> } <nl> 
static int iwl_store_ucode_sec ( struct iwl_firmware_pieces * pieces , <nl>  <nl> sec -> offset = le32_to_cpu ( sec_parse -> offset ); <nl> sec -> data = sec_parse -> data ; <nl> + sec -> size = size - sizeof ( sec_parse -> offset ); <nl>  <nl> ++ img -> sec_counter ; <nl> 
static u32 slic_card_locate ( struct adapter * adapter ) <nl> if (! physcard ) { <nl> /* no structure allocated for this physical card yet */ <nl> physcard = kzalloc ( sizeof ( struct physcard ), GFP_ATOMIC ); <nl> - if (! physcard ) <nl> + if (! physcard ) { <nl> + if ( card_hostid == SLIC_HOSTID_DEFAULT ) <nl> + kfree ( card ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> physcard -> next = slic_global . phys_card ; <nl> slic_global . phys_card = physcard ;
qla2x00_do_dpc ( void * data ) <nl> } else { <nl> fcport -> login_retry = 0 ; <nl> } <nl> - if ( fcport -> login_retry == 0 ) <nl> + if ( fcport -> login_retry == 0 && status != QLA_SUCCESS ) <nl> fcport -> loop_id = FC_NO_LOOP_ID ; <nl> } <nl> if ( test_bit ( LOOP_RESYNC_NEEDED , & ha -> dpc_flags ))
struct ttm_tt * ttm_tt_create ( struct ttm_bo_device * bdev , unsigned long size , <nl> ttm -> dummy_read_page = dummy_read_page ; <nl>  <nl> ttm_tt_alloc_page_directory ( ttm ); <nl> - if (! ttm -> pages ) { <nl> + if (! ttm -> pages || ! ttm -> dma_address ) { <nl> ttm_tt_destroy ( ttm ); <nl> printk ( KERN_ERR TTM_PFX " Failed allocating page table \ n "); <nl> return NULL ;
struct extent_buffer * read_tree_block ( struct btrfs_root * root , u64 bytenr , <nl> return NULL ; <nl>  <nl> ret = btree_read_extent_buffer_pages ( root , buf , 0 , parent_transid ); <nl> + if ( ret ) { <nl> + free_extent_buffer ( buf ); <nl> + return NULL ; <nl> + } <nl> return buf ; <nl>  <nl> }
int acpi_bus_start ( struct acpi_device * device ) <nl> { <nl> struct acpi_bus_ops ops ; <nl>  <nl> + if (! device ) <nl> + return - EINVAL ; <nl> + <nl> memset (& ops , 0 , sizeof ( ops )); <nl> ops . acpi_op_start = 1 ; <nl> 
extern struct debug_obj_descr rcuhead_debug_descr ; <nl>  <nl> static inline void debug_rcu_head_queue ( struct rcu_head * head ) <nl> { <nl> + WARN_ON_ONCE (( unsigned long ) head & 0x3 ); <nl> debug_object_activate ( head , & rcuhead_debug_descr ); <nl> debug_object_active_state ( head , & rcuhead_debug_descr , <nl> STATE_RCU_HEAD_READY ,
static int acpi_battery_technology ( struct acpi_battery * battery ) <nl> return POWER_SUPPLY_TECHNOLOGY_NiMH ; <nl> if (! strcasecmp (" LION ", battery -> type )) <nl> return POWER_SUPPLY_TECHNOLOGY_LION ; <nl> + if (! strcasecmp (" LI - ION ", battery -> type )) <nl> + return POWER_SUPPLY_TECHNOLOGY_LION ; <nl> if (! strcasecmp (" LiP ", battery -> type )) <nl> return POWER_SUPPLY_TECHNOLOGY_LIPO ; <nl> return POWER_SUPPLY_TECHNOLOGY_UNKNOWN ;
static u32 Handle_ListenStateExpired ( struct host_if_drv * hif_drv , <nl> wid . size = 2 ; <nl> wid . val = kmalloc ( wid . size , GFP_KERNEL ); <nl>  <nl> - if (! wid . val ) <nl> + if (! wid . val ) { <nl> PRINT_ER (" Failed to allocate memory \ n "); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> wid . val [ 0 ] = u8remain_on_chan_flag ; <nl> wid . val [ 1 ] = FALSE_FRMWR_CHANNEL ;
qla2x00_configure_fabric ( scsi_qla_host_t * vha ) <nl> fcport -> d_id . b . domain , <nl> fcport -> d_id . b . area , <nl> fcport -> d_id . b . al_pa ); <nl> - fcport -> loop_id = FC_NO_LOOP_ID ; <nl> + qla2x00_clear_loop_id ( fcport ); <nl> } <nl> } <nl> }
static int ixgbe_xdp ( struct net_device * dev , struct netdev_xdp * xdp ) <nl> return ixgbe_xdp_setup ( dev , xdp -> prog ); <nl> case XDP_QUERY_PROG : <nl> xdp -> prog_attached = !!( adapter -> xdp_prog ); <nl> + xdp -> prog_id = adapter -> xdp_prog ? <nl> + adapter -> xdp_prog -> aux -> id : 0 ; <nl> return 0 ; <nl> default : <nl> return - EINVAL ;
static void ax25_kill_by_device ( struct net_device * dev ) <nl> ax25_for_each ( s , & ax25_list ) { <nl> if ( s -> ax25_dev == ax25_dev ) { <nl> sk = s -> sk ; <nl> + if (! sk ) { <nl> + spin_unlock_bh (& ax25_list_lock ); <nl> + s -> ax25_dev = NULL ; <nl> + ax25_disconnect ( s , ENETUNREACH ); <nl> + spin_lock_bh (& ax25_list_lock ); <nl> + goto again ; <nl> + } <nl> sock_hold ( sk ); <nl> spin_unlock_bh (& ax25_list_lock ); <nl> lock_sock ( sk );
static void option_instat_callback ( struct urb * urb ) <nl> dev_dbg ( dev , "% s : type % x req % x \ n ", __func__ , <nl> req_pkt -> bRequestType , req_pkt -> bRequest ); <nl> } <nl> + } else if ( status == - ENOENT || status == - ESHUTDOWN ) { <nl> + dev_dbg ( dev , "% s : urb stopped : % d \ n ", __func__ , status ); <nl> } else <nl> dev_err ( dev , "% s : error % d \ n ", __func__ , status ); <nl> 
void snd_pcm_period_elapsed ( struct snd_pcm_substream * substream ) <nl> snd_timer_interrupt ( substream -> timer , 1 ); <nl> # endif <nl> _end : <nl> - snd_pcm_stream_unlock_irqrestore ( substream , flags ); <nl> kill_fasync (& runtime -> fasync , SIGIO , POLL_IN ); <nl> + snd_pcm_stream_unlock_irqrestore ( substream , flags ); <nl> } <nl>  <nl> EXPORT_SYMBOL ( snd_pcm_period_elapsed );
static struct btrfs_trans_handle * start_transaction ( struct btrfs_root * root , <nl> */ <nl> if ( type != TRANS_JOIN_NOLOCK && <nl> ! __sb_start_write ( root -> fs_info -> sb , SB_FREEZE_FS , false )) { <nl> - if ( type == TRANS_JOIN_FREEZE ) <nl> + if ( type == TRANS_JOIN_FREEZE ) { <nl> + kmem_cache_free ( btrfs_trans_handle_cachep , h ); <nl> return ERR_PTR (- EPERM ); <nl> + } <nl> sb_start_intwrite ( root -> fs_info -> sb ); <nl> } <nl> 
static int usbdev_open ( struct inode * inode , struct file * file ) <nl> struct dev_state * ps ; <nl> int ret ; <nl>  <nl> + lock_kernel (); <nl> /* Protect against simultaneous removal or release */ <nl> mutex_lock (& usbfs_mutex ); <nl>  <nl> static int usbdev_open ( struct inode * inode , struct file * file ) <nl> if ( ret ) <nl> kfree ( ps ); <nl> mutex_unlock (& usbfs_mutex ); <nl> + unlock_kernel (); <nl> return ret ; <nl> } <nl> 
static int rndis_wlan_bind ( struct usbnet * usbdev , struct usb_interface * intf ) <nl>  <nl> /* because rndis_command () sleeps we need to use workqueue */ <nl> priv -> workqueue = create_singlethread_workqueue (" rndis_wlan "); <nl> + if (! priv -> workqueue ) { <nl> + wiphy_free ( wiphy ); <nl> + return - ENOMEM ; <nl> + } <nl> INIT_WORK (& priv -> work , rndis_wlan_worker ); <nl> INIT_DELAYED_WORK (& priv -> dev_poller_work , rndis_device_poller ); <nl> INIT_DELAYED_WORK (& priv -> scan_work , rndis_get_scan_results );
static int nvm_get_dir_info ( struct net_device * dev , u32 * entries , u32 * length ) <nl>  <nl> static int bnxt_get_eeprom_len ( struct net_device * dev ) <nl> { <nl> + struct bnxt * bp = netdev_priv ( dev ); <nl> + <nl> + if ( BNXT_VF ( bp )) <nl> + return 0 ; <nl> + <nl> /* The - 1 return value allows the entire 32 - bit range of offsets to be <nl> * passed via the ethtool command - line utility . <nl> */
static void gfx_v8_0_ring_emit_vm_flush ( struct amdgpu_ring * ring , <nl>  <nl> amdgpu_ring_write ( ring , PACKET3 ( PACKET3_WAIT_REG_MEM , 5 )); <nl> amdgpu_ring_write ( ring , ( WAIT_REG_MEM_MEM_SPACE ( 1 ) | /* memory */ <nl> - WAIT_REG_MEM_FUNCTION ( 3 ))); /* equal */ <nl> + WAIT_REG_MEM_FUNCTION ( 3 ) | /* equal */ <nl> + WAIT_REG_MEM_ENGINE ( usepfp ))); /* pfp or me */ <nl> amdgpu_ring_write ( ring , addr & 0xfffffffc ); <nl> amdgpu_ring_write ( ring , upper_32_bits ( addr ) & 0xffffffff ); <nl> amdgpu_ring_write ( ring , seq );
static int apparmor_setprocattr ( struct task_struct * task , char * name , <nl> sa . aad . op = OP_SETPROCATTR ; <nl> sa . aad . info = name ; <nl> sa . aad . error = - EINVAL ; <nl> - return aa_audit ( AUDIT_APPARMOR_DENIED , NULL , GFP_KERNEL , <nl> + return aa_audit ( AUDIT_APPARMOR_DENIED , <nl> + __aa_current_profile (), GFP_KERNEL , <nl> & sa , NULL ); <nl> } <nl> } else if ( strcmp ( name , " exec ") == 0 ) {
static void init_once ( void * foo ) <nl>  <nl> static struct inode * befs_iget ( struct super_block * sb , unsigned long ino ) <nl> { <nl> - struct buffer_head * bh = NULL ; <nl> + struct buffer_head * bh ; <nl> befs_inode * raw_inode = NULL ; <nl> struct befs_sb_info * befs_sb = BEFS_SB ( sb ); <nl> struct befs_inode_info * befs_ino = NULL ;
static struct proc_dir_entry * proc_sn2_ptc ; <nl>  <nl> static int __init sn2_ptc_init ( void ) <nl> { <nl> + if (! ia64_platform_is (" sn2 ")) <nl> + return - ENOSYS ; <nl> + <nl> if (!( proc_sn2_ptc = create_proc_entry ( PTC_BASENAME , 0444 , NULL ))) { <nl> printk ( KERN_ERR " unable to create % s proc entry ", PTC_BASENAME ); <nl> return - EINVAL ;
static void cciss_geometry_inquiry ( int ctlr , int logvol , <nl> " does not support reading geometry \ n "); <nl> drv -> heads = 255 ; <nl> drv -> sectors = 32 ; // Sectors per track <nl> + drv -> raid_level = RAID_UNKNOWN ; <nl> } else { <nl> drv -> heads = inq_buff -> data_byte [ 6 ]; <nl> drv -> sectors = inq_buff -> data_byte [ 7 ];
static void t3_io_resume ( struct pci_dev * pdev ) <nl> CH_ALERT ( adapter , " adapter recovering , PEX ERR 0x % x \ n ", <nl> t3_read_reg ( adapter , A_PCIE_PEX_ERR )); <nl>  <nl> + rtnl_lock (); <nl> t3_resume_ports ( adapter ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> static const struct pci_error_handlers t3_err_handler = {
dev_config ( struct file * fd , const char __user * buf , size_t len , loff_t * ptr ) <nl> spin_lock_irq (& dev -> lock ); <nl> value = - EINVAL ; <nl> if ( dev -> buf ) { <nl> + spin_unlock_irq (& dev -> lock ); <nl> kfree ( kbuf ); <nl> - goto fail ; <nl> + return value ; <nl> } <nl> dev -> buf = kbuf ; <nl> 
validate_event ( struct pmu_hw_events * hw_events , <nl> struct arm_pmu * armpmu = to_arm_pmu ( event -> pmu ); <nl> struct pmu * leader_pmu = event -> group_leader -> pmu ; <nl>  <nl> + if ( is_software_event ( event )) <nl> + return 1 ; <nl> + <nl> if ( event -> pmu != leader_pmu || event -> state < PERF_EVENT_STATE_OFF ) <nl> return 1 ; <nl> 
static void __split_and_process_bio ( struct mapped_device * md , <nl> } <nl>  <nl> /* drop the extra reference count */ <nl> - dec_pending ( ci . io , error ); <nl> + dec_pending ( ci . io , errno_to_blk_status ( error )); <nl> } <nl> /*----------------------------------------------------------------- <nl> * CRUD END
static int ascot2e_write_regs ( struct ascot2e_priv * priv , <nl> } <nl> }; <nl>  <nl> - if ( len + 1 >= sizeof ( buf )) { <nl> + if ( len + 1 > sizeof ( buf )) { <nl> dev_warn (& priv -> i2c -> dev ," wr reg =% 04x : len =% d is too big !\ n ", <nl> reg , len + 1 ); <nl> return - E2BIG ;
int ext4_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> /* ( user + group )*( old + new ) structure , inode write ( sb , <nl> * inode block , ? - but truncate inode update has it ) */ <nl> handle = ext4_journal_start ( inode , ( EXT4_MAXQUOTAS_INIT_BLOCKS ( inode -> i_sb )+ <nl> - EXT4_QUOTA_DEL_BLOCKS ( inode -> i_sb ))+ 3 ); <nl> + EXT4_MAXQUOTAS_DEL_BLOCKS ( inode -> i_sb ))+ 3 ); <nl> if ( IS_ERR ( handle )) { <nl> error = PTR_ERR ( handle ); <nl> goto err_out ;
static int fill_inode ( struct inode * inode , struct page * locked_page , <nl> } <nl>  <nl> /* finally update i_version */ <nl> - ci -> i_version = le64_to_cpu ( info -> version ); <nl> + if ( le64_to_cpu ( info -> version ) > ci -> i_version ) <nl> + ci -> i_version = le64_to_cpu ( info -> version ); <nl>  <nl> inode -> i_mapping -> a_ops = & ceph_aops ; <nl> 
tracing_iter_ctrl_write ( struct file * filp , const char __user * ubuf , <nl> break ; <nl> } <nl> } <nl> + /* <nl> + * If no option could be set , return an error : <nl> + */ <nl> + if (! trace_options [ i ]) <nl> + return - EINVAL ; <nl>  <nl> filp -> f_pos += cnt ; <nl> 
drm_atomic_plane_get_property ( struct drm_plane * plane , <nl> * val = state -> src_w ; <nl> } else if ( property == config -> prop_src_h ) { <nl> * val = state -> src_h ; <nl> + } else if ( property == config -> rotation_property ) { <nl> + * val = state -> rotation ; <nl> } else if ( plane -> funcs -> atomic_get_property ) { <nl> return plane -> funcs -> atomic_get_property ( plane , state , property , val ); <nl> } else {
lpfc_els_retry ( struct lpfc_hba * phba , struct lpfc_iocbq * cmdiocb , <nl> /* FLOGI retry policy */ <nl> retry = 1 ; <nl> /* retry FLOGI forever */ <nl> - maxretry = 0 ; <nl> + if ( phba -> link_flag != LS_LOOPBACK_MODE ) <nl> + maxretry = 0 ; <nl> + else <nl> + maxretry = 2 ; <nl> + <nl> if ( cmdiocb -> retry >= 100 ) <nl> delay = 5000 ; <nl> else if ( cmdiocb -> retry >= 32 )
static int snd_ctl_elem_add ( struct snd_ctl_file * file , <nl> if ( ue == NULL ) <nl> return - ENOMEM ; <nl> ue -> info = * info ; <nl> + ue -> info . access = 0 ; <nl> ue -> elem_data = ( char *) ue + sizeof (* ue ); <nl> ue -> elem_data_size = private_size ; <nl> kctl . private_free = snd_ctl_elem_user_free ;
static int __devexit wl1271_remove ( struct spi_device * spi ) <nl>  <nl> static struct spi_driver wl1271_spi_driver = { <nl> . driver = { <nl> - . name = " wl1271 ", <nl> + . name = " wl1271_spi ", <nl> . bus = & spi_bus_type , <nl> . owner = THIS_MODULE , <nl> },
xfs_dialloc_ag_inobt ( <nl>  <nl> /* free inodes to the left ? */ <nl> if ( useleft && trec . ir_freecount ) { <nl> - rec = trec ; <nl> xfs_btree_del_cursor ( cur , XFS_BTREE_NOERROR ); <nl> cur = tcur ; <nl>  <nl> pag -> pagl_leftrec = trec . ir_startino ; <nl> pag -> pagl_rightrec = rec . ir_startino ; <nl> pag -> pagl_pagino = pagino ; <nl> + rec = trec ; <nl> goto alloc_inode ; <nl> } <nl> 
static u32 atombios_adjust_pll ( struct drm_crtc * crtc , <nl> if ( radeon_crtc -> ss . refdiv ) { <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_REF_DIV ; <nl> radeon_crtc -> pll_reference_div = radeon_crtc -> ss . refdiv ; <nl> - if ( rdev -> family >= CHIP_RV770 ) <nl> + if ( ASIC_IS_AVIVO ( rdev ) && <nl> + rdev -> family != CHIP_RS780 && <nl> + rdev -> family != CHIP_RS880 ) <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_FRAC_FB_DIV ; <nl> } <nl> }
static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh ) <nl> clear_buffer_mapped ( bh ); <nl> clear_buffer_req ( bh ); <nl> clear_buffer_new ( bh ); <nl> + clear_buffer_delay ( bh ); <nl> + clear_buffer_unwritten ( bh ); <nl> bh -> b_bdev = NULL ; <nl> return may_free ; <nl> }
int fcoe_ctlr_els_send ( struct fcoe_ctlr * fip , struct fc_lport * lport , <nl> fip -> send ( fip , skb ); <nl> return - EINPROGRESS ; <nl> drop : <nl> - kfree_skb ( skb ); <nl> LIBFCOE_FIP_DBG ( fip , " drop els_send op % u d_id % x \ n ", <nl> op , ntoh24 ( fh -> fh_d_id )); <nl> + kfree_skb ( skb ); <nl> return - EINVAL ; <nl> } <nl> EXPORT_SYMBOL ( fcoe_ctlr_els_send );
static int dvb_ca_ioctl ( struct file * file , unsigned int cmd , void * parg ) <nl> { <nl> ca_slot_info_t * info =( ca_slot_info_t *) parg ; <nl>  <nl> - if ( info -> num > 1 ) <nl> + if ( info -> num < 0 || info -> num > 1 ) <nl> return - EINVAL ; <nl> av7110 -> ci_slot [ info -> num ]. num = info -> num ; <nl> av7110 -> ci_slot [ info -> num ]. type = FW_CI_LL_SUPPORT ( av7110 -> arm_app ) ?
static inline void __fsnotify_update_dcache_flags ( struct dentry * dentry ) <nl> assert_spin_locked (& dentry -> d_lock ); <nl>  <nl> parent = dentry -> d_parent ; <nl> - if ( fsnotify_inode_watches_children ( parent -> d_inode )) <nl> + if ( parent -> d_inode && fsnotify_inode_watches_children ( parent -> d_inode )) <nl> dentry -> d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED ; <nl> else <nl> dentry -> d_flags &= ~ DCACHE_FSNOTIFY_PARENT_WATCHED ;
static int __init hp_wmi_input_setup ( void ) <nl> int err ; <nl>  <nl> hp_wmi_input_dev = input_allocate_device (); <nl> + if (! hp_wmi_input_dev ) <nl> + return - ENOMEM ; <nl>  <nl> hp_wmi_input_dev -> name = " HP WMI hotkeys "; <nl> hp_wmi_input_dev -> phys = " wmi / input0 ";
static int alc662_parse_auto_config ( struct hda_codec * codec ) <nl> if ( codec -> vendor_id == 0x10ec0663 ) <nl> spec -> init_verbs [ spec -> num_init_verbs ++] = <nl> alc663_auto_init_verbs ; <nl> + <nl> + err = alc_auto_add_mic_boost ( codec ); <nl> + if ( err < 0 ) <nl> + return err ; <nl> + <nl> spec -> mixers [ spec -> num_mixers ] = alc662_capture_mixer ; <nl> spec -> num_mixers ++; <nl> return 1 ;
void __init mem_init ( void ) <nl>  <nl> pci_iommu_alloc (); <nl>  <nl> - /* clear the zero - page */ <nl> - memset ( empty_zero_page , 0 , PAGE_SIZE ); <nl> + /* clear_bss () already clear the empty_zero_page */ <nl>  <nl> reservedpages = 0 ; <nl> 
static void __orinoco_ev_info ( struct net_device * dev , hermes_t * hw ) <nl> /* Read scan data */ <nl> err = hermes_bap_pread ( hw , IRQ_BAP , ( void *) buf , len , <nl> infofid , sizeof ( info )); <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree ( buf ); <nl> break ; <nl> + } <nl>  <nl> # ifdef ORINOCO_DEBUG <nl> {
static int lmp91000_probe ( struct i2c_client * client , <nl> indio_dev -> channels = lmp91000_channels ; <nl> indio_dev -> num_channels = ARRAY_SIZE ( lmp91000_channels ); <nl> indio_dev -> name = LMP91000_DRV_NAME ; <nl> + indio_dev -> dev . parent = & client -> dev ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl> i2c_set_clientdata ( client , indio_dev ); <nl> 
static int rtl2832u_get_rc_config ( struct dvb_usb_device * d , <nl> rc -> allowed_protos = RC_BIT_ALL ; <nl> rc -> driver_type = RC_DRIVER_IR_RAW ; <nl> rc -> query = rtl2832u_rc_query ; <nl> - rc -> interval = 400 ; <nl> + rc -> interval = 200 ; <nl>  <nl> return 0 ; <nl> }
nouveau_fbcon_destroy ( struct drm_device * dev , struct nouveau_fbdev * fbcon ) <nl> drm_fb_helper_unregister_fbi (& fbcon -> helper ); <nl> drm_fb_helper_fini (& fbcon -> helper ); <nl>  <nl> - if ( nouveau_fb -> nvbo ) { <nl> + if ( nouveau_fb && nouveau_fb -> nvbo ) { <nl> nouveau_vma_del (& nouveau_fb -> vma ); <nl> nouveau_bo_unmap ( nouveau_fb -> nvbo ); <nl> nouveau_bo_unpin ( nouveau_fb -> nvbo );
static int ucb1400_ts_remove ( struct platform_device * dev ) <nl> # ifdef CONFIG_PM <nl> static int ucb1400_ts_resume ( struct platform_device * dev ) <nl> { <nl> - struct ucb1400_ts * ucb = platform_get_drvdata ( dev ); <nl> + struct ucb1400_ts * ucb = dev -> dev . platform_data ; <nl>  <nl> if ( ucb -> ts_task ) { <nl> /*
struct tpm_chip * tpmm_chip_alloc ( struct device * dev , <nl>  <nl> device_initialize (& chip -> dev ); <nl>  <nl> - chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> cdev_init (& chip -> cdev , & tpm_fops ); <nl> + chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> + chip -> cdev . kobj . parent = & chip -> dev . kobj ; <nl>  <nl> return chip ; <nl> }
void kvm_check_async_pf_completion ( struct kvm_vcpu * vcpu ) <nl> spin_unlock (& vcpu -> async_pf . lock ); <nl>  <nl> kvm_arch_async_page_ready ( vcpu , work ); <nl> - kvm_arch_async_page_present ( vcpu , work ); <nl> + kvm_async_page_present_async ( vcpu , work ); <nl>  <nl> list_del (& work -> queue ); <nl> vcpu -> async_pf . queued --;
static ssize_t fuse_dev_splice_read ( struct file * in , loff_t * ppos , <nl> * code can Oops if the buffer persists after module unload . <nl> */ <nl> bufs [ page_nr ]. ops = & nosteal_pipe_buf_ops ; <nl> + bufs [ page_nr ]. flags = 0 ; <nl> ret = add_to_pipe ( pipe , & bufs [ page_nr ++]); <nl> if ( unlikely ( ret < 0 )) <nl> break ;
static int complete_emulated_mmio ( struct kvm_vcpu * vcpu ) <nl> frag -> len -= len ; <nl> } <nl>  <nl> - if ( vcpu -> mmio_cur_fragment == vcpu -> mmio_nr_fragments ) { <nl> + if ( vcpu -> mmio_cur_fragment >= vcpu -> mmio_nr_fragments ) { <nl> vcpu -> mmio_needed = 0 ; <nl>  <nl> /* FIXME : return into emulator if single - stepping . */
static int octeon_usb_hub_status_data ( struct usb_hcd * hcd , char * buf ) <nl> buf [ 0 ] = 0 ; <nl> buf [ 0 ] = port_status . connect_change << 1 ; <nl>  <nl> - return ( buf [ 0 ] != 0 ); <nl> + return buf [ 0 ] != 0 ; <nl> } <nl>  <nl> static int octeon_usb_hub_control ( struct usb_hcd * hcd , u16 typeReq , u16 wValue , u16 wIndex , char * buf , u16 wLength )
void mctp_dev_hold ( struct mctp_dev * mdev ) <nl> void mctp_dev_put ( struct mctp_dev * mdev ) <nl> { <nl> if ( mdev && refcount_dec_and_test (& mdev -> refs )) { <nl> + kfree ( mdev -> addrs ); <nl> dev_put ( mdev -> dev ); <nl> kfree_rcu ( mdev , rcu ); <nl> } <nl> static void mctp_unregister ( struct net_device * dev ) <nl>  <nl> mctp_route_remove_dev ( mdev ); <nl> mctp_neigh_remove_dev ( mdev ); <nl> - kfree ( mdev -> addrs ); <nl>  <nl> mctp_dev_put ( mdev ); <nl> }
void update_mmu_cache ( struct vm_area_struct * vma , unsigned long address , <nl> * called with either mm -> page_table_lock held or ptl lock held <nl> */ <nl> unsigned long access = 0 , trap ; <nl> + if ( radix_enabled ()) <nl> + return ; <nl>  <nl> /* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */ <nl> if (! pte_young (* ptep ) || address >= TASK_SIZE )
static int pcc_get_offset ( int cpu ) <nl> pr = per_cpu ( processors , cpu ); <nl> pcc_cpu_data = per_cpu_ptr ( pcc_cpu_info , cpu ); <nl>  <nl> + if (! pr ) <nl> + return - ENODEV ; <nl> + <nl> status = acpi_evaluate_object ( pr -> handle , " PCCP ", NULL , & buffer ); <nl> if ( ACPI_FAILURE ( status )) <nl> return - ENODEV ;
static void kone_keep_values_up_to_date ( struct kone_device * kone , <nl> { <nl> switch ( event -> event ) { <nl> case kone_mouse_event_switch_profile : <nl> + kone -> actual_dpi = kone -> profiles [ event -> value - 1 ]. <nl> + startup_dpi ; <nl> case kone_mouse_event_osd_profile : <nl> kone -> actual_profile = event -> value ; <nl> - kone -> actual_dpi = kone -> profiles [ kone -> actual_profile - 1 ]. <nl> - startup_dpi ; <nl> break ; <nl> case kone_mouse_event_switch_dpi : <nl> case kone_mouse_event_osd_dpi :
static int ti_pipe3_exit ( struct phy * x ) <nl> u32 val ; <nl> unsigned long timeout ; <nl>  <nl> + /* SATA DPLL can ' t be powered down due to Errata i783 */ <nl> + if ( of_device_is_compatible ( phy -> dev -> of_node , " ti , phy - pipe3 - sata ")) <nl> + return 0 ; <nl> + <nl> /* Put DPLL in IDLE mode */ <nl> val = ti_pipe3_readl ( phy -> pll_ctrl_base , PLL_CONFIGURATION2 ); <nl> val |= PLL_IDLE ;
int mdfld_dsi_send_gen_short ( struct mdfld_dsi_pkg_sender * sender , u8 param0 , <nl> unsigned long flags ; <nl> u8 data_type ; <nl>  <nl> - if (! sender || param_num < 0 || param_num > 2 ) { <nl> + if (! sender || param_num > 2 ) { <nl> DRM_ERROR (" Invalid parameter \ n "); <nl> return - EINVAL ; <nl> }
struct vmxnet3_adapter { <nl> struct net_device * netdev ; <nl> struct pci_dev * pdev ; <nl>  <nl> - u8 * hw_addr0 ; /* for BAR 0 */ <nl> - u8 * hw_addr1 ; /* for BAR 1 */ <nl> + u8 __iomem * hw_addr0 ; /* for BAR 0 */ <nl> + u8 __iomem * hw_addr1 ; /* for BAR 1 */ <nl>  <nl> /* feature control */ <nl> bool rxcsum ;
static inline pte_t huge_ptep_get_and_clear ( struct mm_struct * mm , <nl> static inline void huge_ptep_clear_flush ( struct vm_area_struct * vma , <nl> unsigned long addr , pte_t * ptep ) <nl> { <nl> + ptep_clear_flush ( vma , addr , ptep ); <nl> } <nl>  <nl> static inline int huge_pte_none ( pte_t pte )
void intel_display_power_get ( struct drm_i915_private * dev_priv , <nl> struct i915_power_well * power_well ; <nl> int i ; <nl>  <nl> + intel_runtime_pm_get ( dev_priv ); <nl> + <nl> power_domains = & dev_priv -> power_domains ; <nl>  <nl> mutex_lock (& power_domains -> lock ); <nl> void intel_display_power_put ( struct drm_i915_private * dev_priv , <nl> } <nl>  <nl> mutex_unlock (& power_domains -> lock ); <nl> + <nl> + intel_runtime_pm_put ( dev_priv ); <nl> } <nl>  <nl> static struct i915_power_domains * hsw_pwr ;
static int caif_seqpkt_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( m -> msg_flags & MSG_OOB ) <nl> goto read_error ; <nl>  <nl> + m -> msg_namelen = 0 ; <nl> + <nl> skb = skb_recv_datagram ( sk , flags , 0 , & ret ); <nl> if (! skb ) <nl> goto read_error ;
static int fuse_notify_inval_entry ( struct fuse_conn * fc , unsigned int size , <nl> if ( outarg . namelen > FUSE_NAME_MAX ) <nl> goto err ; <nl>  <nl> + err = - EINVAL ; <nl> + if ( size != sizeof ( outarg ) + outarg . namelen + 1 ) <nl> + goto err ; <nl> + <nl> name . name = buf ; <nl> name . len = outarg . namelen ; <nl> err = fuse_copy_one ( cs , buf , outarg . namelen + 1 );
static void rtsx_pci_shutdown ( struct pci_dev * pcidev ) <nl> rtsx_pci_power_off ( pcr , HOST_ENTER_S1 ); <nl>  <nl> pci_disable_device ( pcidev ); <nl> + free_irq ( pcr -> irq , ( void *) pcr ); <nl> + if ( pcr -> msi_en ) <nl> + pci_disable_msi ( pcr -> pci ); <nl> } <nl>  <nl> # else /* CONFIG_PM */
static int get_dma_channel ( struct device_node * ssi_np , <nl> * dai -> platform name should already point to an allocated buffer . <nl> */ <nl> ret = of_address_to_resource ( dma_channel_np , 0 , & res ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + of_node_put ( dma_channel_np ); <nl> return ret ; <nl> + } <nl> snprintf (( char *) dai -> platform_name , DAI_NAME_SIZE , "% llx .% s ", <nl> ( unsigned long long ) res . start , dma_channel_np -> name ); <nl> 
static int udf_encode_fh ( struct inode * inode , __u32 * fh , int * lenp , <nl> * lenp = 3 ; <nl> fid -> udf . block = location . logicalBlockNum ; <nl> fid -> udf . partref = location . partitionReferenceNum ; <nl> + fid -> udf . parent_partref = 0 ; <nl> fid -> udf . generation = inode -> i_generation ; <nl>  <nl> if ( parent ) {
int __init acpi_debugfs_init ( void ) <nl> if (! acpi_dir ) <nl> goto err ; <nl>  <nl> - cm_dentry = debugfs_create_file (" custom_method ", S_IWUGO , <nl> + cm_dentry = debugfs_create_file (" custom_method ", S_IWUSR , <nl> acpi_dir , NULL , & cm_fops ); <nl> if (! cm_dentry ) <nl> goto err ;
static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> ctl_dir = container_of ( head , struct ctl_dir , header ); <nl>  <nl> if (! dir_emit_dots ( file , ctx )) <nl> - return 0 ; <nl> + goto out ; <nl>  <nl> pos = 2 ; <nl>  <nl> static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> break ; <nl> } <nl> } <nl> + out : <nl> sysctl_head_finish ( head ); <nl> return 0 ; <nl> }
static void save_microcode_patch ( void * data , unsigned int size ) <nl> p = memdup_patch ( data , size ); <nl> if (! p ) <nl> pr_err (" Error allocating buffer % p \ n ", data ); <nl> - else <nl> + else { <nl> list_replace (& iter -> plist , & p -> plist ); <nl> + kfree ( iter -> data ); <nl> + kfree ( iter ); <nl> + } <nl> } <nl> } <nl> 
int btrfs_read_sys_array ( struct btrfs_root * root ) <nl> if ( ret ) <nl> break ; <nl> } else { <nl> + printk ( KERN_ERR <nl> + " BTRFS : unexpected item type % u in sys_array at offset % u \ n ", <nl> + ( u32 ) key . type , cur_offset ); <nl> ret = - EIO ; <nl> break ; <nl> }
static void devm_pci_release_host_bridge_dev ( struct device * dev ) <nl>  <nl> if ( bridge -> release_fn ) <nl> bridge -> release_fn ( bridge ); <nl> + <nl> + pci_free_resource_list (& bridge -> windows ); <nl> } <nl>  <nl> static void pci_release_host_bridge_dev ( struct device * dev ) <nl> { <nl> devm_pci_release_host_bridge_dev ( dev ); <nl> - pci_free_host_bridge ( to_pci_host_bridge ( dev )); <nl> + kfree ( to_pci_host_bridge ( dev )); <nl> } <nl>  <nl> struct pci_host_bridge * pci_alloc_host_bridge ( size_t priv )
int nvdimm_namespace_attach_btt ( struct nd_namespace_common * ndns ) <nl> } <nl>  <nl> btt_sb = devm_kzalloc (& nd_btt -> dev , sizeof (* btt_sb ), GFP_KERNEL ); <nl> + if (! btt_sb ) <nl> + return - ENOMEM ; <nl>  <nl> /* <nl> * If this returns < 0 , that is ok as it just means there wasn ' t
s32 igb_get_phy_id ( struct e1000_hw * hw ) <nl> u16 phy_id ; <nl>  <nl> /* ensure PHY page selection to fix misconfigured i210 */ <nl> - if ( hw -> mac . type == e1000_i210 ) <nl> + if (( hw -> mac . type == e1000_i210 ) || ( hw -> mac . type == e1000_i211 )) <nl> phy -> ops . write_reg ( hw , I347AT4_PAGE_SELECT , 0 ); <nl>  <nl> ret_val = phy -> ops . read_reg ( hw , PHY_ID1 , & phy_id );
static void alc_auto_init_extra_out ( struct hda_codec * codec ) <nl> int i ; <nl> hda_nid_t pin , dac ; <nl>  <nl> - for ( i = 0 ; i < spec -> autocfg . speaker_outs ; i ++) { <nl> + for ( i = 0 ; i < spec -> autocfg . hp_outs ; i ++) { <nl> pin = spec -> autocfg . hp_pins [ i ]; <nl> if (! pin ) <nl> break ;
static void spear_smi_hw_init ( struct spear_smi * dev ) <nl> val = HOLD1 | BANK_EN | DSEL_TIME | ( prescale << 8 ); <nl>  <nl> mutex_lock (& dev -> lock ); <nl> + /* clear all interrupt conditions */ <nl> + writel ( 0 , dev -> io_base + SMI_SR ); <nl> + <nl> writel ( val , dev -> io_base + SMI_CR1 ); <nl> mutex_unlock (& dev -> lock ); <nl> }
int ath_tx_aggr_start ( struct ath_softc * sc , struct ieee80211_sta * sta , <nl> txtid -> paused = true ; <nl> * ssn = txtid -> seq_start = txtid -> seq_next ; <nl>  <nl> + memset ( txtid -> tx_buf , 0 , sizeof ( txtid -> tx_buf )); <nl> + txtid -> baw_head = txtid -> baw_tail = 0 ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int omap_system_dma_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl>  <nl> if ( dma_omap2plus ()) { <nl> - dma_linked_lch = kzalloc ( sizeof ( struct dma_link_info ) * <nl> - dma_lch_count , GFP_KERNEL ); <nl> + dma_linked_lch = kcalloc ( dma_lch_count , <nl> + sizeof (* dma_linked_lch ), <nl> + GFP_KERNEL ); <nl> if (! dma_linked_lch ) { <nl> ret = - ENOMEM ; <nl> goto exit_dma_lch_fail ;
static void gdm_usb_disconnect ( struct usb_interface * intf ) <nl> { <nl> struct phy_dev * phy_dev ; <nl> struct lte_udev * udev ; <nl> - u16 idVendor , idProduct ; <nl> struct usb_device * usbdev ; <nl>  <nl> usbdev = interface_to_usbdev ( intf ); <nl> - <nl> - idVendor = __le16_to_cpu ( usbdev -> descriptor . idVendor ); <nl> - idProduct = __le16_to_cpu ( usbdev -> descriptor . idProduct ); <nl> - <nl> phy_dev = usb_get_intfdata ( intf ); <nl>  <nl> udev = phy_dev -> priv_dev ;
static int pwm_setup_backlight ( struct intel_connector * connector , <nl> return - ENODEV ; <nl> } <nl>  <nl> + /* <nl> + * FIXME : pwm_apply_args () should be removed when switching to <nl> + * the atomic PWM API . <nl> + */ <nl> + pwm_apply_args ( panel -> backlight . pwm ); <nl> + <nl> retval = pwm_config ( panel -> backlight . pwm , CRC_PMIC_PWM_PERIOD_NS , <nl> CRC_PMIC_PWM_PERIOD_NS ); <nl> if ( retval < 0 ) {
static int check_vmentry_postreqs ( struct kvm_vcpu * vcpu , struct vmcs12 * vmcs12 , <nl> return 1 ; <nl> } <nl>  <nl> + if (( vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS ) && <nl> + ( is_noncanonical_address ( vmcs12 -> guest_bndcfgs & PAGE_MASK , vcpu ) || <nl> + ( vmcs12 -> guest_bndcfgs & MSR_IA32_BNDCFGS_RSVD ))) <nl> + return 1 ; <nl> + <nl> return 0 ; <nl> } <nl> 
int kvm_timer_hyp_init ( void ) <nl> info = arch_timer_get_kvm_info (); <nl> timecounter = & info -> timecounter ; <nl>  <nl> + if (! timecounter -> cc ) { <nl> + kvm_err (" kvm_arch_timer : uninitialized timecounter \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> if ( info -> virtual_irq <= 0 ) { <nl> kvm_err (" kvm_arch_timer : invalid virtual timer IRQ : % d \ n ", <nl> info -> virtual_irq );
static int iterate_dir_item ( struct btrfs_root * root , struct btrfs_path * path , <nl> buf = tmp ; <nl> } <nl> if (! buf ) { <nl> - buf = vmalloc ( buf_len ); <nl> + buf = kvmalloc ( buf_len , GFP_KERNEL ); <nl> if (! buf ) { <nl> ret = - ENOMEM ; <nl> goto out ;
static int __inet6_check_established ( struct inet_timewait_death_row * death_row , <nl>  <nl> if ( twp != NULL ) { <nl> * twp = tw ; <nl> - NET_INC_STATS_BH ( twsk_net ( tw ), LINUX_MIB_TIMEWAITRECYCLED ); <nl> + NET_INC_STATS_BH ( net , LINUX_MIB_TIMEWAITRECYCLED ); <nl> } else if ( tw != NULL ) { <nl> /* Silly . Should hash - dance instead ... */ <nl> inet_twsk_deschedule ( tw , death_row ); <nl> - NET_INC_STATS_BH ( twsk_net ( tw ), LINUX_MIB_TIMEWAITRECYCLED ); <nl> + NET_INC_STATS_BH ( net , LINUX_MIB_TIMEWAITRECYCLED ); <nl>  <nl> inet_twsk_put ( tw ); <nl> }
static int threshold_create_bank ( unsigned int cpu , unsigned int bank ) <nl> const char * name = get_name ( bank , NULL ); <nl> int err = 0 ; <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( is_shared_bank ( bank )) { <nl> nb = node_to_amd_nb ( amd_get_nb_id ( cpu )); <nl> 
tegra_xusb_find_port_node ( struct tegra_xusb_padctl * padctl , const char * type , <nl> name = kasprintf ( GFP_KERNEL , "% s -% u ", type , index ); <nl> if (! name ) { <nl> of_node_put ( ports ); <nl> - return ERR_PTR (- ENOMEM ); <nl> + return NULL ; <nl> } <nl> np = of_get_child_by_name ( ports , name ); <nl> kfree ( name );
static int max1363_probe ( struct i2c_client * client , <nl> goto error_out ; <nl> } <nl>  <nl> + indio_dev -> dev . of_node = client -> dev . of_node ; <nl> ret = iio_map_array_register ( indio_dev , client -> dev . platform_data ); <nl> if ( ret < 0 ) <nl> goto error_free_device ;
static int cnic_start_hw ( struct cnic_dev * dev ) <nl> return 0 ; <nl>  <nl> err1 : <nl> - cp -> free_resc ( dev ); <nl> + if ( ethdev -> drv_state & CNIC_DRV_STATE_HANDLES_IRQ ) <nl> + cp -> stop_hw ( dev ); <nl> + else <nl> + cp -> free_resc ( dev ); <nl> pci_dev_put ( dev -> pcidev ); <nl> return err ; <nl> }
static int snd_sst_fill_kernel_list ( struct stream_info * stream , <nl> return - ENOMEM ; <nl> if ( copy_from_user (( void *) & rar_handle , <nl> iovec [ index ]. iov_base , <nl> - sizeof ( __u32 ))) <nl> + sizeof ( __u32 ))) { <nl> + kfree ( stream_bufs ); <nl> return - EFAULT ; <nl> + } <nl> stream_bufs -> addr = ( char *) rar_handle ; <nl> stream_bufs -> in_use = false ; <nl> stream_bufs -> size = iovec [ 0 ]. iov_len ;
static int intel_ring_context_pin ( struct intel_engine_cs * engine , <nl> ret = context_pin ( ctx ); <nl> if ( ret ) <nl> goto error ; <nl> + <nl> + ce -> state -> obj -> mm . dirty = true ; <nl> } <nl>  <nl> /* The kernel context is only used as a placeholder for flushing the
void reload_ucode_amd ( void ) <nl> if ( mc && rev < mc -> hdr . patch_id ) { <nl> if (! __apply_microcode_amd ( mc )) { <nl> ucode_new_rev = mc -> hdr . patch_id ; <nl> - pr_info (" microcode : reload patch_level = 0x % 08x \ n ", <nl> - ucode_new_rev ); <nl> + pr_info (" reload patch_level = 0x % 08x \ n ", ucode_new_rev ); <nl> } <nl> } <nl> }
static struct task_struct * producer ; <nl> static struct task_struct * consumer ; <nl> static unsigned long read ; <nl>  <nl> - static int disable_reader ; <nl> + static unsigned int disable_reader ; <nl> module_param ( disable_reader , uint , 0644 ); <nl> MODULE_PARM_DESC ( disable_reader , " only run producer "); <nl>  <nl> - static int write_iteration = 50 ; <nl> + static unsigned int write_iteration = 50 ; <nl> module_param ( write_iteration , uint , 0644 ); <nl> MODULE_PARM_DESC ( write_iteration , "# of writes between timestamp readings "); <nl> 
int hugetlb_mcopy_atomic_pte ( struct mm_struct * dst_mm , <nl> return ret ; <nl> out_release_unlock : <nl> spin_unlock ( ptl ); <nl> - out_release_nounlock : <nl> if ( vm_shared ) <nl> unlock_page ( page ); <nl> + out_release_nounlock : <nl> put_page ( page ); <nl> goto out ; <nl> }
int iwl_mvm_sched_scan_start ( struct iwl_mvm * mvm , <nl> return - EBUSY ; <nl> } <nl>  <nl> + /* we don ' t support " match all " in the firmware */ <nl> + if (! req -> n_match_sets ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> ret = iwl_mvm_check_running_scans ( mvm , type ); <nl> if ( ret ) <nl> return ret ;
static const struct file_operations evtchn_fops = { <nl>  <nl> static struct miscdevice evtchn_miscdev = { <nl> . minor = MISC_DYNAMIC_MINOR , <nl> - . name = " evtchn ", <nl> + . name = " xen / evtchn ", <nl> . fops = & evtchn_fops , <nl> }; <nl> static int __init evtchn_init ( void )
static int __init kvmppc_e500_init ( void ) <nl> return kvm_init ( NULL , sizeof ( struct kvmppc_vcpu_e500 ), 0 , THIS_MODULE ); <nl> } <nl>  <nl> - static void __init kvmppc_e500_exit ( void ) <nl> + static void __exit kvmppc_e500_exit ( void ) <nl> { <nl> kvmppc_booke_exit (); <nl> }
static int __devinit pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> unsigned int delay ; <nl> u8 pon_cntl ; <nl> struct pmic8xxx_pwrkey * pwrkey ; <nl> - const struct pm8xxx_pwrkey_platform_data * pdata = mfd_get_data ( pdev ); <nl> + const struct pm8xxx_pwrkey_platform_data * pdata = <nl> + dev_get_platdata (& pdev -> dev ); <nl>  <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " power key platform data not supplied \ n ");
static int dpi_check_timings ( struct omap_dss_device * dssdev , <nl> struct dpi_clk_calc_ctx ctx ; <nl> bool ok ; <nl>  <nl> + if ( timings -> x_res % 8 != 0 ) <nl> + return - EINVAL ; <nl> + <nl> if ( mgr && ! dispc_mgr_timings_ok ( mgr -> id , timings )) <nl> return - EINVAL ; <nl> 
static int omninet_port_remove ( struct usb_serial_port * port ) <nl>  <nl> static int omninet_open ( struct tty_struct * tty , struct usb_serial_port * port ) <nl> { <nl> - struct usb_serial * serial = port -> serial ; <nl> - struct usb_serial_port * wport ; <nl> - <nl> - wport = serial -> port [ 1 ]; <nl> - tty_port_tty_set (& wport -> port , tty ); <nl> - <nl> return usb_serial_generic_open ( tty , port ); <nl> } <nl> 
EXPORT_SYMBOL ( clk_enable ); <nl>  <nl> void clk_disable ( struct clk * clk ) <nl> { <nl> + if (! clk ) <nl> + return ; <nl> + <nl> if ( clk -> ops && clk -> ops -> disable ) <nl> clk -> ops -> disable ( clk ); <nl> }
static int list_devices ( struct file * filp , struct dm_ioctl * param , size_t param_ <nl> * Grab our output buffer . <nl> */ <nl> nl = orig_nl = get_result_buffer ( param , param_size , & len ); <nl> - if ( len < needed ) { <nl> + if ( len < needed || len < sizeof ( nl -> dev )) { <nl> param -> flags |= DM_BUFFER_FULL_FLAG ; <nl> goto out ; <nl> }
static inline void callchain_init ( struct callchain_node * node ) <nl> INIT_LIST_HEAD (& node -> children ); <nl> INIT_LIST_HEAD (& node -> val ); <nl>  <nl> + node -> children_hit = 0 ; <nl> node -> parent = NULL ; <nl> node -> hit = 0 ; <nl> }
int rtllib_rx ( struct rtllib_device * ieee , struct sk_buff * skb , <nl> return ret ; <nl>  <nl> rx_dropped : <nl> - ieee -> stats . rx_dropped ++; <nl> + if ( ieee ) <nl> + ieee -> stats . rx_dropped ++; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( rtllib_rx );
static netdev_tx_t ip6erspan_tunnel_xmit ( struct sk_buff * skb , <nl> truncate = true ; <nl> } <nl>  <nl> + if ( skb_cow_head ( skb , dev -> needed_headroom )) <nl> + goto tx_err ; <nl> + <nl> t -> parms . o_flags &= ~ TUNNEL_KEY ; <nl> IPCB ( skb )-> flags = 0 ; <nl> 
struct dst_entry * icmp6_dst_alloc ( struct net_device * dev , <nl> struct net * net = dev_net ( dev ); <nl>  <nl> if ( unlikely (! idev )) <nl> - return NULL ; <nl> + return ERR_PTR (- ENODEV ); <nl>  <nl> rt = ip6_dst_alloc (& net -> ipv6 . ip6_dst_ops , dev , 0 ); <nl> if ( unlikely (! rt )) {
struct sock * inet_csk_clone_lock ( const struct sock * sk , <nl> /* listeners have SOCK_RCU_FREE , not the children */ <nl> sock_reset_flag ( newsk , SOCK_RCU_FREE ); <nl>  <nl> + inet_sk ( newsk )-> mc_list = NULL ; <nl> + <nl> newsk -> sk_mark = inet_rsk ( req )-> ir_mark ; <nl> atomic64_set (& newsk -> sk_cookie , <nl> atomic64_read (& inet_rsk ( req )-> ir_cookie ));
static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
static void solo_enc_free ( struct solo_enc_dev * solo_enc ) <nl> if ( solo_enc == NULL ) <nl> return ; <nl>  <nl> + pci_free_consistent ( solo_enc -> solo_dev -> pdev , <nl> + sizeof ( struct solo_p2m_desc ) * solo_enc -> desc_nelts , <nl> + solo_enc -> desc_items , solo_enc -> desc_dma ); <nl> video_unregister_device ( solo_enc -> vfd ); <nl> v4l2_ctrl_handler_free (& solo_enc -> hdl ); <nl> kfree ( solo_enc );
void spu_deactivate ( struct spu_context * ctx ) <nl> */ <nl> void spu_yield ( struct spu_context * ctx ) <nl> { <nl> - mutex_lock (& ctx -> state_mutex ); <nl> - __spu_deactivate ( ctx , 0 , MAX_PRIO ); <nl> - mutex_unlock (& ctx -> state_mutex ); <nl> + if (!( ctx -> flags & SPU_CREATE_NOSCHED )) { <nl> + mutex_lock (& ctx -> state_mutex ); <nl> + __spu_deactivate ( ctx , 0 , MAX_PRIO ); <nl> + mutex_unlock (& ctx -> state_mutex ); <nl> + } <nl> } <nl>  <nl> void spu_sched_tick ( struct work_struct * work )
nouveau_connector_set_encoder ( struct drm_connector * connector , <nl> return ; <nl> nv_connector -> detected_encoder = nv_encoder ; <nl>  <nl> + if ( dev_priv -> card_type >= NV_50 ) { <nl> + connector -> interlace_allowed = true ; <nl> + connector -> doublescan_allowed = true ; <nl> + } else <nl> if ( nv_encoder -> dcb -> type == OUTPUT_LVDS || <nl> nv_encoder -> dcb -> type == OUTPUT_TMDS ) { <nl> connector -> doublescan_allowed = false ;
static inline void omap3xxx_restart ( enum reboot_mode mode , const char * cmd ) <nl> } <nl> # endif <nl>  <nl> -# if defined ( CONFIG_ARCH_OMAP4 ) || defined ( CONFIG_SOC_OMAP5 ) <nl> +# if defined ( CONFIG_ARCH_OMAP4 ) || defined ( CONFIG_SOC_OMAP5 ) || \ <nl> + defined ( CONFIG_SOC_DRA7XX ) || defined ( CONFIG_SOC_AM43XX ) <nl> void omap44xx_restart ( enum reboot_mode mode , const char * cmd ); <nl> # else <nl> static inline void omap44xx_restart ( enum reboot_mode mode , const char * cmd )
static int sunxi_pinctrl_probe ( struct platform_device * pdev ) <nl> goto gpiochip_error ; <nl> } <nl>  <nl> - clk_prepare_enable ( clk ); <nl> + ret = clk_prepare_enable ( clk ); <nl> + if ( ret ) <nl> + goto gpiochip_error ; <nl>  <nl> pctl -> irq = irq_of_parse_and_map ( node , 0 ); <nl> if (! pctl -> irq ) {
static int fsl_elbc_chip_remove ( struct fsl_elbc_mtd * priv ) <nl>  <nl> elbc_fcm_ctrl -> chips [ priv -> bank ] = NULL ; <nl> kfree ( priv ); <nl> - kfree ( elbc_fcm_ctrl ); <nl> return 0 ; <nl> } <nl> 
xfs_qm_freelist_destroy ( xfs_frlist_t * ql ) <nl> xfs_qm_dqdestroy ( dqp ); <nl> dqp = nextdqp ; <nl> } <nl> - /* <nl> - * Don ' t bother about unlocking . <nl> - */ <nl> + mutex_unlock (& ql -> qh_lock ); <nl> mutex_destroy (& ql -> qh_lock ); <nl>  <nl> ASSERT ( ql -> qh_nelems == 0 );
brcmf_notify_sched_scan_results ( struct brcmf_if * ifp , <nl> } <nl>  <nl> set_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status ); <nl> + cfg -> escan_info . run = brcmf_run_escan ; <nl> err = brcmf_do_escan ( cfg , wiphy , ifp , request ); <nl> if ( err ) { <nl> clear_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status );
static void mem_cgroup_usage_unregister_event ( struct cgroup * cgrp , <nl> */ <nl> BUG_ON (! thresholds ); <nl>  <nl> + if (! thresholds -> primary ) <nl> + goto unlock ; <nl> + <nl> usage = mem_cgroup_usage ( memcg , type == _MEMSWAP ); <nl>  <nl> /* Check if a threshold crossed before removing */ <nl> static void mem_cgroup_usage_unregister_event ( struct cgroup * cgrp , <nl>  <nl> /* To be sure that nobody uses thresholds */ <nl> synchronize_rcu (); <nl> - <nl> + unlock : <nl> mutex_unlock (& memcg -> thresholds_lock ); <nl> } <nl> 
int cap_bprm_set_creds ( struct linux_binprm * bprm ) <nl> } <nl> skip : <nl>  <nl> + /* if we have fs caps , clear dangerous personality flags */ <nl> + if (! cap_issubset ( new -> cap_permitted , old -> cap_permitted )) <nl> + bprm -> per_clear |= PER_CLEAR_ON_SETID ; <nl> + <nl> + <nl> /* Don ' t let someone trace a set [ ug ] id / setpcap binary with the revised <nl> * credentials unless they have the appropriate permit <nl> */
void __init setup_arch ( char ** cmdline_p ) <nl> init_mm . brk = ( unsigned long ) & _end ; <nl>  <nl> * cmdline_p = m68k_command_line ; <nl> - memcpy ( saved_command_line , * cmdline_p , CL_SIZE ); <nl> + memcpy ( boot_command_line , * cmdline_p , CL_SIZE ); <nl>  <nl> /* Parse the command line for arch - specific options . <nl> * For the m68k , this is currently only " debug = xxx " to enable printing
static int __exit twl4030_usb_remove ( struct platform_device * pdev ) <nl> /* disable complete OTG block */ <nl> twl4030_usb_clear_bits ( twl , POWER_CTRL , POWER_CTRL_OTG_ENAB ); <nl>  <nl> - twl4030_phy_power ( twl , 0 ); <nl> + if (! twl -> asleep ) <nl> + twl4030_phy_power ( twl , 0 ); <nl> regulator_put ( twl -> usb1v5 ); <nl> regulator_put ( twl -> usb1v8 ); <nl> regulator_put ( twl -> usb3v1 );
void * ion_heap_map_kernel ( struct ion_heap * heap , <nl> struct page ** tmp = pages ; <nl>  <nl> if (! pages ) <nl> - return NULL ; <nl> + return ERR_PTR (- ENOMEM ); <nl>  <nl> if ( buffer -> flags & ION_FLAG_CACHED ) <nl> pgprot = PAGE_KERNEL ;
static int ext4_file_open ( struct inode * inode , struct file * filp ) <nl> path . dentry = mnt -> mnt_root ; <nl> cp = d_path (& path , buf , sizeof ( buf )); <nl> if (! IS_ERR ( cp )) { <nl> - memcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> - sizeof ( sbi -> s_es -> s_last_mounted )); <nl> + strlcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> + sizeof ( sbi -> s_es -> s_last_mounted )); <nl> ext4_mark_super_dirty ( sb ); <nl> } <nl> }
void __kvm_migrate_pit_timer ( struct kvm_vcpu * vcpu ) <nl> return ; <nl>  <nl> timer = & pit -> pit_state . timer ; <nl> + mutex_lock (& pit -> pit_state . lock ); <nl> if ( hrtimer_cancel ( timer )) <nl> hrtimer_start_expires ( timer , HRTIMER_MODE_ABS ); <nl> + mutex_unlock (& pit -> pit_state . lock ); <nl> } <nl>  <nl> static void destroy_pit_timer ( struct kvm_pit * pit )
static int amdgpu_cs_wait_any_fence ( struct amdgpu_device * adev , <nl> wait -> out . status = ( r > 0 ); <nl> wait -> out . first_signaled = first ; <nl>  <nl> - if ( array [ first ]) <nl> + if ( first < fence_count && array [ first ]) <nl> r = array [ first ]-> error ; <nl> else <nl> r = 0 ;
int ext4_orphan_del ( handle_t * handle , struct inode * inode ) <nl> struct ext4_iloc iloc ; <nl> int err = 0 ; <nl>  <nl> - if (! EXT4_SB ( inode -> i_sb )-> s_journal ) <nl> + if ((! EXT4_SB ( inode -> i_sb )-> s_journal ) && <nl> + !( EXT4_SB ( inode -> i_sb )-> s_mount_state & EXT4_ORPHAN_FS )) <nl> return 0 ; <nl>  <nl> mutex_lock (& EXT4_SB ( inode -> i_sb )-> s_orphan_lock );
static int create_queue_cpsch ( struct device_queue_manager * dqm , struct queue * q , <nl> return retval ; <nl> } <nl>  <nl> - int fence_wait_timeout ( unsigned int * fence_addr , unsigned int fence_value , <nl> - unsigned long timeout ) <nl> + static int fence_wait_timeout ( unsigned int * fence_addr , <nl> + unsigned int fence_value , <nl> + unsigned long timeout ) <nl> { <nl> BUG_ON (! fence_addr ); <nl> timeout += jiffies ;
static int of_iommu_xlate ( struct device * dev , <nl> * a proper probe - ordering dependency mechanism in future . <nl> */ <nl> if (! ops ) <nl> - return - EPROBE_DEFER ; <nl> + return driver_deferred_probe_check_state ( dev ); <nl>  <nl> return ops -> of_xlate ( dev , iommu_spec ); <nl> }
get_futex_key ( u32 __user * uaddr , int fshared , union futex_key * key , int rw ) <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> + page = compound_head ( page ); <nl> lock_page ( page ); <nl> if (! page -> mapping ) { <nl> unlock_page ( page );
int cx23885_tuner_callback ( void * priv , int component , int command , int arg ) <nl> struct cx23885_dev * dev = port -> dev ; <nl> u32 bitmask = 0 ; <nl>  <nl> - if ( command == XC2028_RESET_CLK ) <nl> + if (( command == XC2028_RESET_CLK ) || ( command == XC2028_I2C_FLUSH )) <nl> return 0 ; <nl>  <nl> if ( command != 0 ) {
static void mmu_set_spte ( struct kvm_vcpu * vcpu , u64 * sptep , <nl>  <nl> child = page_header ( pte & PT64_BASE_ADDR_MASK ); <nl> mmu_page_remove_parent_pte ( child , sptep ); <nl> + __set_spte ( sptep , shadow_trap_nonpresent_pte ); <nl> + kvm_flush_remote_tlbs ( vcpu -> kvm ); <nl> } else if ( pfn != spte_to_pfn (* sptep )) { <nl> pgprintk (" hfn old % lx new % lx \ n ", <nl> spte_to_pfn (* sptep ), pfn );
int dso__load_sym ( struct dso * dso , struct map * map , const char * name , int fd , <nl> goto new_symbol ; <nl> } <nl>  <nl> - if ( curr_dso -> adjust_symbols ) { <nl> + if ( curr_dso -> adjust_symbols && sym . st_value ) { <nl> pr_debug4 ("% s : adjusting symbol : st_value : %#" PRIx64 " " <nl> " sh_addr : %#" PRIx64 " sh_offset : %#" PRIx64 "\ n ", __func__ , <nl> ( u64 ) sym . st_value , ( u64 ) shdr . sh_addr ,
static const struct regmap_config rt5677_regmap = { <nl> static const struct i2c_device_id rt5677_i2c_id [] = { <nl> { " rt5677 ", RT5677 }, <nl> { " rt5676 ", RT5676 }, <nl> + { " RT5677CE : 00 ", RT5677 }, <nl> { } <nl> }; <nl> MODULE_DEVICE_TABLE ( i2c , rt5677_i2c_id );
static void sw_perf_event_destroy ( struct perf_event * event ) <nl>  <nl> static int perf_swevent_init ( struct perf_event * event ) <nl> { <nl> - int event_id = event -> attr . config ; <nl> + u64 event_id = event -> attr . config ; <nl>  <nl> if ( event -> attr . type != PERF_TYPE_SOFTWARE ) <nl> return - ENOENT ;
void bnx2x_disable_sriov ( struct bnx2x * bp ) <nl> static int bnx2x_vf_ndo_sanity ( struct bnx2x * bp , int vfidx , <nl> struct bnx2x_virtf * vf ) <nl> { <nl> + if ( bp -> state != BNX2X_STATE_OPEN ) { <nl> + BNX2X_ERR (" vf ndo called though PF is down \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if (! IS_SRIOV ( bp )) { <nl> BNX2X_ERR (" vf ndo called though sriov is disabled \ n "); <nl> return - EINVAL ;
void gfs2_set_iop ( struct inode * inode ) <nl> inode -> i_op = & gfs2_symlink_iops ; <nl> } else { <nl> inode -> i_op = & gfs2_file_iops ; <nl> + init_special_inode ( inode , inode -> i_mode , inode -> i_rdev ); <nl> } <nl>  <nl> unlock_new_inode ( inode );
static int rds_ib_laddr_check ( __be32 addr ) <nl> ret = rdma_bind_addr ( cm_id , ( struct sockaddr *)& sin ); <nl> /* due to this , we will claim to support iWARP devices unless we <nl> check node_type . */ <nl> - if ( ret || cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> + if ( ret || ! cm_id -> device || <nl> + cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> ret = - EADDRNOTAVAIL ; <nl>  <nl> rdsdebug (" addr % pI4 ret % d node type % d \ n ",
int __cfs_fail_timeout_set ( __u32 id , __u32 value , int ms , int set ) <nl> int ret ; <nl>  <nl> ret = __cfs_fail_check_set ( id , value , set ); <nl> - if ( ret ) { <nl> + if ( ret && likely ( ms > 0 )) { <nl> CERROR (" cfs_fail_timeout id % x sleeping for % dms \ n ", <nl> id , ms ); <nl> set_current_state ( TASK_UNINTERRUPTIBLE );
static int i965_do_reset ( struct drm_device * dev ) <nl> { <nl> int ret ; <nl>  <nl> + /* FIXME : i965g / gm need a display save / restore for gpu reset . */ <nl> + return - ENODEV ; <nl> + <nl> /* <nl> * Set the domains we want to reset ( GRDOM / bits 2 and 3 ) as <nl> * well as the reset bit ( GR / bit 0 ). Setting the GR bit
static int uas_find_endpoints ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < n_endpoints ; i ++) { <nl> unsigned char * extra = endpoint [ i ]. extra ; <nl> int len = endpoint [ i ]. extralen ; <nl> - while ( len > 1 ) { <nl> + while ( len >= 3 ) { <nl> if ( extra [ 1 ] == USB_DT_PIPE_USAGE ) { <nl> unsigned pipe_id = extra [ 2 ]; <nl> if ( pipe_id > 0 && pipe_id < 5 )
static int skl_probe ( struct pci_dev * pci , <nl> if ( err < 0 ) <nl> goto out_free ; <nl>  <nl> + skl -> nhlt = skl_nhlt_init ( bus -> dev ); <nl> + <nl> + if ( skl -> nhlt == NULL ) <nl> + goto out_free ; <nl> + <nl> pci_set_drvdata ( skl -> pci , ebus ); <nl>  <nl> /* check if dsp is there */
bool seg6_validate_srh ( struct ipv6_sr_hdr * srh , int len ) <nl> struct sr6_tlv * tlv ; <nl> unsigned int tlv_len ; <nl>  <nl> + if ( trailing < sizeof (* tlv )) <nl> + return false ; <nl> + <nl> tlv = ( struct sr6_tlv *)(( unsigned char *) srh + tlv_offset ); <nl> tlv_len = sizeof (* tlv ) + tlv -> len ; <nl> 
void xgbe_debugfs_init ( struct xgbe_prv_data * pdata ) <nl> pdata -> xgbe_debugfs = debugfs_create_dir ( buf , NULL ); <nl> if (! pdata -> xgbe_debugfs ) { <nl> netdev_err ( pdata -> netdev , " debugfs_create_dir failed \ n "); <nl> + kfree ( buf ); <nl> return ; <nl> } <nl> 
struct gb_interface * gb_interface_create ( struct greybus_host_device * hd , <nl>  <nl> free_intf : <nl> put_device (& intf -> dev ); <nl> - kfree ( intf ); <nl> put_module : <nl> put_device (& module -> dev ); <nl> return NULL ;
static int bio_readpage_error ( struct bio * failed_bio , u64 phy_offset , <nl> ret = tree -> ops -> submit_bio_hook ( inode , read_mode , bio , <nl> failrec -> this_mirror , <nl> failrec -> bio_flags , 0 ); <nl> + if ( ret ) { <nl> + free_io_failure ( inode , failrec , 0 ); <nl> + bio_put ( bio ); <nl> + } <nl> + <nl> return ret ; <nl> } <nl> 
static void azx_pcm_free ( struct snd_pcm * pcm ) <nl>  <nl> # define MAX_PREALLOC_SIZE ( 32 * 1024 * 1024 ) <nl>  <nl> - int azx_attach_pcm_stream ( struct hda_bus * bus , struct hda_codec * codec , <nl> - struct hda_pcm * cpcm ) <nl> + static int azx_attach_pcm_stream ( struct hda_bus * bus , struct hda_codec * codec , <nl> + struct hda_pcm * cpcm ) <nl> { <nl> struct azx * chip = bus -> private_data ; <nl> struct snd_pcm * pcm ;
int fix_alignment ( struct pt_regs * regs ) <nl>  <nl> type = op . type & INSTR_TYPE_MASK ; <nl> if (! OP_IS_LOAD_STORE ( type )) { <nl> - if ( type != CACHEOP + DCBZ ) <nl> + if ( op . type != CACHEOP + DCBZ ) <nl> return - EINVAL ; <nl> PPC_WARN_ALIGNMENT ( dcbz , regs ); <nl> r = emulate_dcbz ( op . ea , regs );
static int raw_cmd_copyout ( int cmd , void __user * param , <nl> int ret ; <nl>  <nl> while ( ptr ) { <nl> - ret = copy_to_user ( param , ptr , sizeof (* ptr )); <nl> + struct floppy_raw_cmd cmd = * ptr ; <nl> + cmd . next = NULL ; <nl> + cmd . kernel_data = NULL ; <nl> + ret = copy_to_user ( param , & cmd , sizeof ( cmd )); <nl> if ( ret ) <nl> return - EFAULT ; <nl> param += sizeof ( struct floppy_raw_cmd );
static inline void create_debug_files ( struct ehci_hcd * ehci ) <nl> & debug_registers_fops )) <nl> goto file_error ; <nl>  <nl> - if (! debugfs_create_file (" lpm ", S_IRUGO | S_IWUGO , ehci -> debug_dir , bus , <nl> + if (! debugfs_create_file (" lpm ", S_IRUGO | S_IWUSR , ehci -> debug_dir , bus , <nl> & debug_lpm_fops )) <nl> goto file_error ; <nl> 
static int ieee80211_fragment ( struct ieee80211_tx_data * tx , <nl> } <nl>  <nl> /* adjust first fragment ' s length */ <nl> - skb -> len = hdrlen + per_fragm ; <nl> + skb_trim ( skb , hdrlen + per_fragm ); <nl> return 0 ; <nl> } <nl> 
i915_gem_detect_bit_6_swizzle ( struct drm_device * dev ) <nl> } <nl> } <nl>  <nl> + /* FIXME : check with memory config on IGDNG */ <nl> + if ( IS_IGDNG ( dev )) { <nl> + DRM_ERROR (" disable tiling on IGDNG ...\ n "); <nl> + swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + } <nl> + <nl> dev_priv -> mm . bit_6_swizzle_x = swizzle_x ; <nl> dev_priv -> mm . bit_6_swizzle_y = swizzle_y ; <nl> }
static int ipoib_mcast_join_complete ( int status , <nl> } <nl>  <nl> if ( mcast -> logcount ++ < 20 ) { <nl> - if ( status == - ETIMEDOUT ) { <nl> + if ( status == - ETIMEDOUT || status == - EAGAIN ) { <nl> ipoib_dbg_mcast ( priv , " multicast join failed for % pI6 , status % d \ n ", <nl> mcast -> mcmember . mgid . raw , status ); <nl> } else {
static int uwbd ( void * param ) <nl> HZ ); <nl> if ( should_stop ) <nl> break ; <nl> - try_to_freeze (); <nl>  <nl> spin_lock_irqsave (& rc -> uwbd . event_list_lock , flags ); <nl> if (! list_empty (& rc -> uwbd . event_list )) {
parser_name_get ( struct parser_context * ctx ) <nl> struct spar_controlvm_parameters_header * phdr = NULL ; <nl>  <nl> phdr = ( struct spar_controlvm_parameters_header *)( ctx -> data ); <nl> + <nl> + if ( phdr -> name_offset + phdr -> name_length > ctx -> param_bytes ) <nl> + return NULL ; <nl> + <nl> ctx -> curr = ctx -> data + phdr -> name_offset ; <nl> ctx -> bytes_remaining = phdr -> name_length ; <nl> return parser_string_get ( ctx );
struct clk * icst_clk_register ( struct device * dev , <nl>  <nl> pclone = kmemdup ( desc -> params , sizeof (* pclone ), GFP_KERNEL ); <nl> if (! pclone ) { <nl> + kfree ( icst ); <nl> pr_err (" could not clone ICST params \ n "); <nl> return ERR_PTR (- ENOMEM ); <nl> }
static int host_start ( struct ci13xxx * ci ) <nl> else <nl> ci -> hcd = hcd ; <nl>  <nl> + if ( ci -> platdata -> flags & CI13XXX_DISABLE_STREAMING ) <nl> + hw_write ( ci , OP_USBMODE , USBMODE_CI_SDIS , USBMODE_CI_SDIS ); <nl> + <nl> return ret ; <nl> } <nl> 
int inet_recv_error ( struct sock * sk , struct msghdr * msg , int len , <nl>  <nl> static inline void inet_ctl_sock_destroy ( struct sock * sk ) <nl> { <nl> - sock_release ( sk -> sk_socket ); <nl> + if ( sk ) <nl> + sock_release ( sk -> sk_socket ); <nl> } <nl>  <nl> # endif
static int moxart_gpio_probe ( struct platform_device * pdev ) <nl> gc -> parent = dev ; <nl> gc -> owner = THIS_MODULE ; <nl>  <nl> - ret = gpiochip_add_data ( gc , NULL ); <nl> + ret = devm_gpiochip_add_data ( dev , gc , NULL ); <nl> if ( ret ) { <nl> dev_err ( dev , "% s : gpiochip_add failed \ n ", <nl> dev -> of_node -> full_name );
int devm_request_irq ( struct device * dev , unsigned int irq , <nl>  <nl> rc = request_irq ( irq , handler , irqflags , devname , dev_id ); <nl> if ( rc ) { <nl> - kfree ( dr ); <nl> + devres_free ( dr ); <nl> return rc ; <nl> } <nl> 
struct io_reg CX700_ModeXregs [] = { { VIASR , SR10 , 0xFF , 0x01 }, <nl> { VIACR , CR96 , 0xFF , 0x00 }, <nl> { VIACR , CR97 , 0xFF , 0x00 }, <nl> { VIACR , CR99 , 0xFF , 0x00 }, <nl> -{ VIACR , CR9B , 0xFF , 0x00 }, <nl> -{ VIACR , CRD2 , 0xFF , 0xFF } /* TMDS / LVDS control register . */ <nl> +{ VIACR , CR9B , 0xFF , 0x00 } <nl> }; <nl>  <nl> /* Video Mode Table */
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
static void atmel_spi_next_xfer ( struct spi_master * master , <nl> xfer , xfer -> len , xfer -> tx_buf , xfer -> tx_dma , <nl> xfer -> rx_buf , xfer -> rx_dma , spi_readl ( as , IMR )); <nl>  <nl> - spi_writel ( as , TCR , len ); <nl> spi_writel ( as , RCR , len ); <nl> + spi_writel ( as , TCR , len ); <nl> spi_writel ( as , PTCR , SPI_BIT ( TXTEN ) | SPI_BIT ( RXTEN )); <nl> } <nl> 
static int twl4030_kp_probe ( struct platform_device * pdev ) <nl> err3 : <nl> /* mask all events - we don ' t care about the result */ <nl> ( void ) twl4030_kpwrite_u8 ( kp , 0xff , KEYP_IMR1 ); <nl> - free_irq ( kp -> irq , NULL ); <nl> + free_irq ( kp -> irq , kp ); <nl> err2 : <nl> input_unregister_device ( input ); <nl> input = NULL ;
extern int for_each_subchannel ( int (* fn )( struct subchannel_id , void *), void *); <nl> struct channel_subsystem { <nl> u8 cssid ; <nl> int valid ; <nl> - struct channel_path * chps [ __MAX_CHPID ]; <nl> + struct channel_path * chps [ __MAX_CHPID + 1 ]; <nl> struct device device ; <nl> struct pgid global_pgid ; <nl> };
static void atiixp_set_dmamode ( struct ata_port * ap , struct ata_device * adev ) <nl> * We must now look at the PIO mode situation . We may need to <nl> * adjust the PIO mode to keep the timings acceptable <nl> */ <nl> - if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> - wanted_pio = 4 ; <nl> + if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> + wanted_pio = 4 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_1 ) <nl> wanted_pio = 3 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_0 )
void hostif_data_indication ( struct ks_wlan_private * priv ) <nl> { <nl> unsigned int rx_ind_size ; /* indicate data size */ <nl> struct sk_buff * skb ; <nl> - unsigned short auth_type ; <nl> + u16 auth_type ; <nl> unsigned char temp [ 256 ]; <nl> struct ether_hdr * eth_hdr ; <nl> unsigned short eth_proto ;
static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
static int tower_probe ( struct usb_interface * interface , const struct usb_device <nl> USB_MAJOR , dev -> minor ); <nl>  <nl> exit : <nl> + kfree ( get_version_reply ); <nl> return retval ; <nl>  <nl> error :
static struct sctp_auth_bytes * sctp_auth_create_key ( __u32 key_len , gfp_t gfp ) <nl> struct sctp_auth_bytes * key ; <nl>  <nl> /* Verify that we are not going to overflow INT_MAX */ <nl> - if (( INT_MAX - key_len ) < sizeof ( struct sctp_auth_bytes )) <nl> + if ( key_len > ( INT_MAX - sizeof ( struct sctp_auth_bytes ))) <nl> return NULL ; <nl>  <nl> /* Allocate the shared key */
static inline void ata_tf_init ( struct ata_device * dev , struct ata_taskfile * tf ) <nl>  <nl> static inline void ata_qc_reinit ( struct ata_queued_cmd * qc ) <nl> { <nl> + qc -> dma_dir = DMA_NONE ; <nl> qc -> __sg = NULL ; <nl> qc -> flags = 0 ; <nl> qc -> cursect = qc -> cursg = qc -> cursg_ofs = 0 ;
static struct sock * sctp_v6_create_accept_sk ( struct sock * sk , <nl> newnp = inet6_sk ( newsk ); <nl>  <nl> memcpy ( newnp , np , sizeof ( struct ipv6_pinfo )); <nl> + newnp -> ipv6_mc_list = NULL ; <nl> + newnp -> ipv6_ac_list = NULL ; <nl> + newnp -> ipv6_fl_list = NULL ; <nl>  <nl> rcu_read_lock (); <nl> opt = rcu_dereference ( np -> opt );
int evtchn_get ( unsigned int evtchn ) <nl> struct irq_info * info ; <nl> int err = - ENOENT ; <nl>  <nl> + if ( evtchn >= NR_EVENT_CHANNELS ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& irq_mapping_update_lock ); <nl>  <nl> irq = evtchn_to_irq [ evtchn ];
bool i40e_dcb_need_reconfig ( struct i40e_pf * pf , <nl> /* Check if APP Table has changed */ <nl> if ( memcmp (& new_cfg -> app , <nl> & old_cfg -> app , <nl> - sizeof ( new_cfg -> app ))) <nl> + sizeof ( new_cfg -> app ))) { <nl> need_reconfig = true ; <nl> dev_info (& pf -> pdev -> dev , " APP Table change detected .\ n "); <nl> + } <nl>  <nl> return need_reconfig ; <nl> }
int radeon_atombios_init ( struct radeon_device * rdev ) <nl>  <nl> void radeon_atombios_fini ( struct radeon_device * rdev ) <nl> { <nl> - kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> - kfree ( rdev -> mode_info . atom_context ); <nl> + if ( rdev -> mode_info . atom_context ) { <nl> + kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> + kfree ( rdev -> mode_info . atom_context ); <nl> + } <nl> kfree ( rdev -> mode_info . atom_card_info ); <nl> } <nl> 
static int ca8210_probe ( struct spi_device * spi_device ) <nl> goto error ; <nl> } <nl>  <nl> + priv -> spi -> dev . platform_data = pdata ; <nl> ret = ca8210_get_platform_data ( priv -> spi , pdata ); <nl> if ( ret ) { <nl> dev_crit (& spi_device -> dev , " ca8210_get_platform_data failed \ n "); <nl> goto error ; <nl> } <nl> - priv -> spi -> dev . platform_data = pdata ; <nl>  <nl> ret = ca8210_dev_com_init ( priv ); <nl> if ( ret ) {
static inline int gro_cells_init ( struct gro_cells * gcells , struct net_device * de <nl> int i ; <nl>  <nl> gcells -> gro_cells_mask = roundup_pow_of_two ( netif_get_num_default_rss_queues ()) - 1 ; <nl> - gcells -> cells = kcalloc ( sizeof ( struct gro_cell ), <nl> - gcells -> gro_cells_mask + 1 , <nl> + gcells -> cells = kcalloc ( gcells -> gro_cells_mask + 1 , <nl> + sizeof ( struct gro_cell ), <nl> GFP_KERNEL ); <nl> if (! gcells -> cells ) <nl> return - ENOMEM ;
static struct scatterlist * alloc_sgtable ( int size ) <nl> if ( new_page ) <nl> __free_page ( new_page ); <nl> } <nl> + kfree ( table ); <nl> return NULL ; <nl> } <nl> alloc_size = min_t ( int , size , PAGE_SIZE );
static int audit_log_single_execve_arg ( struct audit_context * context , <nl> * so we can be sure nothing was lost . <nl> */ <nl> if (( i == 0 ) && ( too_long )) <nl> - audit_log_format (* ab , " a % d_len =% ld ", arg_num , <nl> + audit_log_format (* ab , " a % d_len =% zu ", arg_num , <nl> has_cntl ? 2 * len : len ); <nl>  <nl> /*
static int do_video_set_spu_palette ( unsigned int fd , unsigned int cmd , <nl>  <nl> err = get_user ( palp , & up -> palette ); <nl> err |= get_user ( length , & up -> length ); <nl> + if ( err ) <nl> + return - EFAULT ; <nl>  <nl> up_native = compat_alloc_user_space ( sizeof ( struct video_spu_palette )); <nl> err = put_user ( compat_ptr ( palp ), & up_native -> palette );
static int perf_sched__read_events ( struct perf_sched * sched ) <nl> struct perf_data_file file = { <nl> . path = input_name , <nl> . mode = PERF_DATA_MODE_READ , <nl> + . force = sched -> force , <nl> }; <nl> int rc = - 1 ; <nl> 
/* to align the pointer to the ( next ) page boundary */ <nl> # define PAGE_ALIGN ( addr ) ((( addr )+ PAGE_SIZE - 1 )& PAGE_MASK ) <nl>  <nl> -# define __PHYSICAL_MASK (((( phys_addr_t ) 1 ) << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> +# define __PHYSICAL_MASK (( phys_addr_t )( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> # define __VIRTUAL_MASK (( 1UL << __VIRTUAL_MASK_SHIFT ) - 1 ) <nl>  <nl> # ifndef __ASSEMBLY__
static int io_files_update_with_index_alloc ( struct io_kiocb * req , <nl> struct file * file ; <nl> int ret , fd ; <nl>  <nl> + if (! req -> ctx -> file_data ) <nl> + return - ENXIO ; <nl> + <nl> for ( done = 0 ; done < req -> rsrc_update . nr_args ; done ++) { <nl> if ( copy_from_user (& fd , & fds [ done ], sizeof ( fd ))) { <nl> ret = - EFAULT ;
static int truncate_data_node ( const struct ubifs_info * c , const struct inode * in <nl> int err , dlen , compr_type , out_len , old_dlen ; <nl>  <nl> out_len = le32_to_cpu ( dn -> size ); <nl> - buf = kmalloc ( out_len * WORST_COMPR_FACTOR , GFP_NOFS ); <nl> + buf = kmalloc_array ( out_len , WORST_COMPR_FACTOR , GFP_NOFS ); <nl> if (! buf ) <nl> return - ENOMEM ; <nl> 
static void xiic_reinit ( struct xiic_i2c * i2c ) <nl> /* Enable interrupts */ <nl> xiic_setreg32 ( i2c , XIIC_DGIER_OFFSET , XIIC_GINTR_ENABLE_MASK ); <nl>  <nl> - xiic_irq_clr_en ( i2c , XIIC_INTR_AAS_MASK | XIIC_INTR_ARB_LOST_MASK ); <nl> + xiic_irq_clr_en ( i2c , XIIC_INTR_ARB_LOST_MASK ); <nl> } <nl>  <nl> static void xiic_deinit ( struct xiic_i2c * i2c )
int exynos_eint_wkup_init ( struct samsung_pinctrl_drv_data * d ) <nl> if ( match ) { <nl> irq_chip = kmemdup ( match -> data , <nl> sizeof (* irq_chip ), GFP_KERNEL ); <nl> + if (! irq_chip ) <nl> + return - ENOMEM ; <nl> wkup_np = np ; <nl> break ; <nl> }
static inline int sctp_frag_point ( const struct sctp_association * asoc , int pmtu ) <nl> if ( asoc -> user_frag ) <nl> frag = min_t ( int , frag , asoc -> user_frag ); <nl>  <nl> - frag = min_t ( int , frag , SCTP_MAX_CHUNK_LEN ); <nl> + frag = WORD_TRUNC ( min_t ( int , frag , SCTP_MAX_CHUNK_LEN )); <nl>  <nl> return frag ; <nl> }
nouveau_framebuffer_init ( struct drm_device * dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( nvbo -> tile_flags & NOUVEAU_GEM_TILE_NONCONTIG ) { <nl> + NV_ERROR ( drm , " framebuffer requires contiguous bo \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( nv_device ( drm -> device )-> chipset == 0x50 ) <nl> nv_fb -> r_format |= ( tile_flags << 8 ); <nl> 
struct btrfs_root * open_ctree ( struct super_block * sb , <nl> kfree ( tree_root ); <nl> bdi_destroy (& fs_info -> bdi ); <nl> kfree ( fs_info ); <nl> + kfree ( chunk_root ); <nl> + kfree ( dev_root ); <nl> return ERR_PTR ( err ); <nl> } <nl> 
static int sctp_wait_for_sndbuf ( struct sctp_association * asoc , long * timeo_p , <nl> */ <nl> release_sock ( sk ); <nl> current_timeo = schedule_timeout ( current_timeo ); <nl> - BUG_ON ( sk != asoc -> base . sk ); <nl> + if ( sk != asoc -> base . sk ) <nl> + goto do_error ; <nl> lock_sock ( sk ); <nl>  <nl> * timeo_p = current_timeo ;
static int atl2_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl>  <nl> err = - EIO ; <nl>  <nl> - netdev -> hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX ; <nl> + netdev -> hw_features = NETIF_F_HW_VLAN_CTAG_RX ; <nl> netdev -> features |= ( NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX ); <nl>  <nl> /* Init PHY as early as possible due to power saving issue */
int ext4_insert_range ( struct inode * inode , loff_t offset , loff_t len ) <nl> up_write (& EXT4_I ( inode )-> i_data_sem ); <nl> goto out_stop ; <nl> } <nl> + } else { <nl> + ext4_ext_drop_refs ( path ); <nl> + kfree ( path ); <nl> } <nl>  <nl> ret = ext4_es_remove_extent ( inode , offset_lblk ,
struct se_portal_group * tcm_loop_make_naa_tpg ( <nl> tpgt_str += 5 ; /* Skip ahead of " tpgt_ " */ <nl> tpgt = ( unsigned short int ) simple_strtoul ( tpgt_str , & end_ptr , 0 ); <nl>  <nl> - if ( tpgt > TL_TPGS_PER_HBA ) { <nl> + if ( tpgt >= TL_TPGS_PER_HBA ) { <nl> printk ( KERN_ERR " Passed tpgt : % hu exceeds TL_TPGS_PER_HBA :" <nl> " % u \ n ", tpgt , TL_TPGS_PER_HBA ); <nl> return ERR_PTR (- EINVAL );
do { \ <nl> # define this_cpu_generic_read ( pcp ) \ <nl> ({ \ <nl> typeof ( pcp ) __ret ; \ <nl> - preempt_disable (); \ <nl> + preempt_disable_notrace (); \ <nl> __ret = raw_cpu_generic_read ( pcp ); \ <nl> - preempt_enable (); \ <nl> + preempt_enable_notrace (); \ <nl> __ret ; \ <nl> }) <nl> 
smb_proc_setattr_unix ( struct dentry * d , struct iattr * attr , <nl> LSET ( data , 32 , SMB_TIME_NO_CHANGE ); <nl> LSET ( data , 40 , SMB_UID_NO_CHANGE ); <nl> LSET ( data , 48 , SMB_GID_NO_CHANGE ); <nl> - LSET ( data , 56 , smb_filetype_from_mode ( attr -> ia_mode )); <nl> + DSET ( data , 56 , smb_filetype_from_mode ( attr -> ia_mode )); <nl> LSET ( data , 60 , major ); <nl> LSET ( data , 68 , minor ); <nl> LSET ( data , 76 , 0 );
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int io_rw_init_file ( struct io_kiocb * req , fmode_t mode ) <nl> if (!( kiocb -> ki_flags & IOCB_DIRECT ) || ! file -> f_op -> iopoll ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + kiocb -> private = NULL ; <nl> kiocb -> ki_flags |= IOCB_HIPRI | IOCB_ALLOC_CACHE ; <nl> kiocb -> ki_complete = io_complete_rw_iopoll ; <nl> req -> iopoll_completed = 0 ;
int v4l2_async_notifier_register ( struct v4l2_device * v4l2_dev , <nl> struct v4l2_async_subdev * asd ; <nl> int i ; <nl>  <nl> - if (! notifier -> num_subdevs || notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> + if (! v4l2_dev || ! notifier -> num_subdevs || <nl> + notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> return - EINVAL ; <nl>  <nl> notifier -> v4l2_dev = v4l2_dev ;
static int rave_sp_probe ( struct serdev_device * serdev ) <nl> return ret ; <nl>  <nl> serdev_device_set_baudrate ( serdev , baud ); <nl> + serdev_device_set_flow_control ( serdev , false ); <nl> + <nl> + ret = serdev_device_set_parity ( serdev , SERDEV_PARITY_NONE ); <nl> + if ( ret ) { <nl> + dev_err ( dev , " Failed to set parity \ n "); <nl> + return ret ; <nl> + } <nl>  <nl> ret = rave_sp_get_status ( sp ); <nl> if ( ret ) {
int mlxsw_sp_router_init ( struct mlxsw_sp * mlxsw_sp ) <nl> return err ; <nl> mlxsw_sp_lpm_init ( mlxsw_sp ); <nl> mlxsw_sp_vrs_init ( mlxsw_sp ); <nl> - return mlxsw_sp_neigh_init ( mlxsw_sp ); <nl> + err = mlxsw_sp_neigh_init ( mlxsw_sp ); <nl> + if ( err ) <nl> + goto err_neigh_init ; <nl> + return 0 ; <nl> + <nl> + err_neigh_init : <nl> + __mlxsw_sp_router_fini ( mlxsw_sp ); <nl> + return err ; <nl> } <nl>  <nl> void mlxsw_sp_router_fini ( struct mlxsw_sp * mlxsw_sp )
int tcp_rcv_state_process ( struct sock * sk , struct sk_buff * skb , <nl> goto discard ; <nl>  <nl> if ( th -> syn ) { <nl> + if ( th -> fin ) <nl> + goto discard ; <nl> if ( icsk -> icsk_af_ops -> conn_request ( sk , skb ) < 0 ) <nl> return 1 ; <nl> 
static int eb_lookup_vmas ( struct i915_execbuffer * eb ) <nl>  <nl> err = radix_tree_insert ( handles_vma , handle , vma ); <nl> if ( unlikely ( err )) { <nl> - kfree ( lut ); <nl> + kmem_cache_free ( eb -> i915 -> luts , lut ); <nl> goto err_obj ; <nl> } <nl> 
static int __devinit ad7879_probe ( struct spi_device * spi ) <nl> kfree ( ts ); <nl> } <nl>  <nl> - return 0 ; <nl> + return error ; <nl> } <nl>  <nl> static int __devexit ad7879_remove ( struct spi_device * spi ) <nl> static int __devinit ad7879_probe ( struct i2c_client * client , <nl> kfree ( ts ); <nl> } <nl>  <nl> - return 0 ; <nl> + return error ; <nl> } <nl>  <nl> static int __devexit ad7879_remove ( struct i2c_client * client )
static void handle_keypress ( int c ) <nl> switch ( c ) { <nl> case ' d ': <nl> prompt_integer (& delay_secs , " Enter display delay "); <nl> + if ( delay_secs < 1 ) <nl> + delay_secs = 1 ; <nl> break ; <nl> case ' e ': <nl> prompt_integer (& print_entries , " Enter display entries ( lines )");
int vcc_recvmsg ( struct kiocb * iocb , struct socket * sock , struct msghdr * msg , <nl> struct sk_buff * skb ; <nl> int copied , error = - EINVAL ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> if ( sock -> state != SS_CONNECTED ) <nl> return - ENOTCONN ; <nl> 
static int exec_drive_taskfile ( struct driver_data * dd , <nl> fis . device ); <nl>  <nl> /* check for erase mode support during secure erase .*/ <nl> - if (( fis . command == ATA_CMD_SEC_ERASE_UNIT ) <nl> - && ( outbuf [ 0 ] & MTIP_SEC_ERASE_MODE )) { <nl> + if (( fis . command == ATA_CMD_SEC_ERASE_UNIT ) && outbuf && <nl> + ( outbuf [ 0 ] & MTIP_SEC_ERASE_MODE )) { <nl> erasemode = 1 ; <nl> } <nl> 
static void destroy_eps ( struct ci_hdrc * ci ) <nl> for ( i = 0 ; i < ci -> hw_ep_max ; i ++) { <nl> struct ci_hw_ep * hwep = & ci -> ci_hw_ep [ i ]; <nl>  <nl> + if ( hwep -> pending_td ) <nl> + free_pending_td ( hwep ); <nl> dma_pool_free ( ci -> qh_pool , hwep -> qh . ptr , hwep -> qh . dma ); <nl> } <nl> }
static int sco_sock_bind ( struct socket * sock , struct sockaddr * addr , <nl> if (! addr || addr -> sa_family != AF_BLUETOOTH ) <nl> return - EINVAL ; <nl>  <nl> + if ( addr_len < sizeof ( struct sockaddr_sco )) <nl> + return - EINVAL ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state != BT_OPEN ) {
struct reset_control * of_reset_control_get ( struct device_node * node , <nl> { <nl> int index = 0 ; <nl>  <nl> - if ( id ) <nl> + if ( id ) { <nl> index = of_property_match_string ( node , <nl> " reset - names ", id ); <nl> + if ( index < 0 ) <nl> + return ERR_PTR (- ENOENT ); <nl> + } <nl> return of_reset_control_get_by_index ( node , index ); <nl> } <nl> EXPORT_SYMBOL_GPL ( of_reset_control_get );
getname_kernel ( const char * filename ) <nl> if ( len <= EMBEDDED_NAME_MAX ) { <nl> result -> name = ( char *) result -> iname ; <nl> } else if ( len <= PATH_MAX ) { <nl> + const size_t size = offsetof ( struct filename , iname [ 1 ]); <nl> struct filename * tmp ; <nl>  <nl> - tmp = kmalloc ( sizeof (* tmp ), GFP_KERNEL ); <nl> + tmp = kmalloc ( size , GFP_KERNEL ); <nl> if ( unlikely (! tmp )) { <nl> __putname ( result ); <nl> return ERR_PTR (- ENOMEM );
i2c_dw_xfer ( struct i2c_adapter * adap , struct i2c_msg msgs [], int num ) <nl> i2c_dw_xfer_init ( dev ); <nl>  <nl> /* wait for tx to complete */ <nl> - if (! wait_for_completion_timeout (& dev -> cmd_complete , HZ )) { <nl> + if (! wait_for_completion_timeout (& dev -> cmd_complete , adap -> timeout )) { <nl> dev_err ( dev -> dev , " controller timed out \ n "); <nl> /* i2c_dw_init implicitly disables the adapter */ <nl> i2c_dw_init ( dev );
static int i40e_setup_macvlans ( struct i40e_vsi * vsi , u16 macvlan_cnt , u16 qcnt , <nl> ch -> num_queue_pairs = qcnt ; <nl> if (! i40e_setup_channel ( pf , vsi , ch )) { <nl> ret = - EINVAL ; <nl> + kfree ( ch ); <nl> goto err_free ; <nl> } <nl> ch -> parent_vsi = vsi ;
int fscrypt_process_policy ( struct inode * inode , <nl> return - EINVAL ; <nl>  <nl> if (! inode_has_encryption_context ( inode )) { <nl> + if (! S_ISDIR ( inode -> i_mode )) <nl> + return - EINVAL ; <nl> if (! inode -> i_sb -> s_cop -> empty_dir ) <nl> return - EOPNOTSUPP ; <nl> if (! inode -> i_sb -> s_cop -> empty_dir ( inode ))
get_matching_model_microcode ( int cpu , unsigned long start , <nl> unsigned int mc_saved_count = mc_saved_data -> mc_saved_count ; <nl> int i ; <nl>  <nl> - while ( leftover ) { <nl> + while ( leftover && mc_saved_count < ARRAY_SIZE ( mc_saved_tmp )) { <nl> mc_header = ( struct microcode_header_intel *) ucode_ptr ; <nl>  <nl> mc_size = get_totalsize ( mc_header );
static int mcp3422_probe ( struct i2c_client * client , <nl> | MCP3422_CHANNEL_VALUE ( 0 ) <nl> | MCP3422_PGA_VALUE ( MCP3422_PGA_1 ) <nl> | MCP3422_SAMPLE_RATE_VALUE ( MCP3422_SRATE_240 )); <nl> - mcp3422_update_config ( adc , config ); <nl> + err = mcp3422_update_config ( adc , config ); <nl> + if ( err < 0 ) <nl> + return err ; <nl>  <nl> err = devm_iio_device_register (& client -> dev , indio_dev ); <nl> if ( err < 0 )
static void imc_common_cpuhp_mem_free ( struct imc_pmu * pmu_ptr ) <nl> } <nl>  <nl> /* Only free the attr_groups which are dynamically allocated */ <nl> - kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]-> attrs ); <nl> + if ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]) <nl> + kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]-> attrs ); <nl> kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]); <nl> kfree ( pmu_ptr ); <nl> return ;
struct drm_i915_file_private { <nl>  <nl> # define HAS_FORCE_WAKE ( dev ) ( INTEL_INFO ( dev )-> has_force_wake ) <nl>  <nl> -# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev )) <nl> +# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev ) || IS_HASWELL ( dev )) <nl>  <nl> # include " i915_trace . h " <nl> 
static struct dm_region * __rh_alloc ( struct dm_region_hash * rh , region_t region ) <nl>  <nl> nreg = mempool_alloc ( rh -> region_pool , GFP_ATOMIC ); <nl> if ( unlikely (! nreg )) <nl> - nreg = kmalloc ( sizeof (* nreg ), GFP_NOIO ); <nl> + nreg = kmalloc ( sizeof (* nreg ), GFP_NOIO | __GFP_NOFAIL ); <nl>  <nl> nreg -> state = rh -> log -> type -> in_sync ( rh -> log , region , 1 ) ? <nl> DM_RH_CLEAN : DM_RH_NOSYNC ;
static int blan_mdlm_bind ( struct usbnet * dev , struct usb_interface * intf ) <nl> } <nl> /* expect bcdVersion 1 . 0 , ignore */ <nl> if ( memcmp (& desc -> bGUID , blan_guid , 16 ) <nl> - && memcmp (& desc -> bGUID , blan_guid , 16 ) ) { <nl> + && memcmp (& desc -> bGUID , safe_guid , 16 ) ) { <nl> /* hey , this one might _really_ be MDLM ! */ <nl> dev_dbg (& intf -> dev , " MDLM guid \ n "); <nl> goto bad_desc ;
static int pl2303_tiocmset ( struct tty_struct * tty , <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl>  <nl> mutex_lock (& serial -> disc_mutex ); <nl> - if (! serial -> disconnected ) <nl> + if (! serial -> disconnected ) { <nl> ret = pl2303_set_control_lines ( port , control ); <nl> - else <nl> + if ( ret ) <nl> + ret = usb_translate_errors ( ret ); <nl> + } else { <nl> ret = - ENODEV ; <nl> + } <nl> mutex_unlock (& serial -> disc_mutex ); <nl>  <nl> return ret ;
static int sdio_read_cis ( struct mmc_card * card , struct sdio_func * func ) <nl> if ( tpl_code == 0xff ) <nl> break ; <nl>  <nl> + /* null entries have no link field or data */ <nl> + if ( tpl_code == 0x00 ) <nl> + continue ; <nl> + <nl> ret = mmc_io_rw_direct ( card , 0 , 0 , ptr ++, 0 , & tpl_link ); <nl> if ( ret ) <nl> break ;
static int ac100_rtc_probe ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> chip = devm_kzalloc (& pdev -> dev , sizeof (* chip ), GFP_KERNEL ); <nl> + if (! chip ) <nl> + return - ENOMEM ; <nl> + <nl> platform_set_drvdata ( pdev , chip ); <nl> chip -> dev = & pdev -> dev ; <nl> chip -> regmap = ac100 -> regmap ;
int __ceph_caps_used ( struct ceph_inode_info * ci ) <nl> used |= CEPH_CAP_PIN ; <nl> if ( ci -> i_rd_ref ) <nl> used |= CEPH_CAP_FILE_RD ; <nl> - if ( ci -> i_rdcache_ref || ci -> i_rdcache_gen ) <nl> + if ( ci -> i_rdcache_ref || ci -> vfs_inode . i_data . nrpages ) <nl> used |= CEPH_CAP_FILE_CACHE ; <nl> if ( ci -> i_wr_ref ) <nl> used |= CEPH_CAP_FILE_WR ;
int ip6_xmit ( struct sock * sk , struct sk_buff * skb , struct flowi * fl , <nl> skb_reset_network_header ( skb ); <nl> hdr = ipv6_hdr ( skb ); <nl>  <nl> + /* Allow local fragmentation . */ <nl> + if ( ipfragok ) <nl> + skb -> local_df = 1 ; <nl> + <nl> /* <nl> * Fill in the IPv6 header <nl> */
static long __video_do_ioctl ( struct file * file , <nl> break ; <nl>  <nl> ret = 0 ; <nl> + p -> parm . capture . readbuffers = 2 ; <nl> if ( ops -> vidioc_g_std ) <nl> ret = ops -> vidioc_g_std ( file , fh , & std ); <nl> if ( ret == 0 )
static int do_loopback ( struct path * path , const char * old_name , <nl>  <nl> if ( IS_ERR ( mnt )) { <nl> err = PTR_ERR ( mnt ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> err = graft_tree ( mnt , path );
static int adis16400_read_raw ( struct iio_dev * indio_dev , <nl> * val = st -> variant -> temp_scale_nano / 1000000 ; <nl> * val2 = ( st -> variant -> temp_scale_nano % 1000000 ); <nl> return IIO_VAL_INT_PLUS_MICRO ; <nl> + case IIO_PRESSURE : <nl> + /* 20 uBar = 0 . 002kPascal */ <nl> + * val = 0 ; <nl> + * val2 = 2000 ; <nl> + return IIO_VAL_INT_PLUS_MICRO ; <nl> default : <nl> return - EINVAL ; <nl> }
__append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
xfsbufd ( <nl>  <nl> current -> flags |= PF_MEMALLOC ; <nl>  <nl> + set_freezable (); <nl> + <nl> do { <nl> if ( unlikely ( freezing ( current ))) { <nl> set_bit ( XBT_FORCE_SLEEP , & target -> bt_flags );
static bool snd_soc_set_cache_val ( void * base , unsigned int idx , <nl> static unsigned int snd_soc_get_cache_val ( const void * base , unsigned int idx , <nl> unsigned int word_size ) <nl> { <nl> + if (! base ) <nl> + return - 1 ; <nl> + <nl> switch ( word_size ) { <nl> case 1 : { <nl> const u8 * cache = base ;
static void usb_alphatrack_disconnect ( struct usb_interface * intf ) <nl> mutex_unlock (& dev -> mtx ); <nl> usb_alphatrack_delete ( dev ); <nl> } else { <nl> + atomic_set (& dev -> writes_pending , 0 ); <nl> dev -> intf = NULL ; <nl> mutex_unlock (& dev -> mtx ); <nl> } <nl>  <nl> - atomic_set (& dev -> writes_pending , 0 ); <nl> mutex_unlock (& disconnect_mutex ); <nl>  <nl> dev_info (& intf -> dev , " Alphatrack Surface #% d now disconnected \ n ",
static struct irq_chip amd_gpio_irqchip = { <nl> . irq_set_type = amd_gpio_irq_set_type , <nl> }; <nl>  <nl> - static void amd_gpio_irq_handler ( unsigned int irq , struct irq_desc * desc ) <nl> + static void amd_gpio_irq_handler ( unsigned int __irq , struct irq_desc * desc ) <nl> { <nl> + unsigned int irq = irq_desc_get_irq ( desc ); <nl> u32 i ; <nl> u32 off ; <nl> u32 reg ;
unsigned long slice_get_unmapped_area ( unsigned long addr , unsigned long len , <nl> unsigned long high_limit ; <nl>  <nl> high_limit = DEFAULT_MAP_WINDOW ; <nl> - if ( addr >= high_limit ) <nl> + if ( addr >= high_limit || ( fixed && ( addr + len > high_limit ))) <nl> high_limit = TASK_SIZE ; <nl>  <nl> if ( len > high_limit )
static int ubifs_get_sb ( struct file_system_type * fs_type , int flags , <nl> */ <nl> ubi = open_ubi ( name , UBI_READONLY ); <nl> if ( IS_ERR ( ubi )) { <nl> - ubifs_err (" cannot open \"% s \", error % d ", <nl> - name , ( int ) PTR_ERR ( ubi )); <nl> + dbg_err (" cannot open \"% s \", error % d ", <nl> + name , ( int ) PTR_ERR ( ubi )); <nl> return PTR_ERR ( ubi ); <nl> } <nl> ubi_get_volume_info ( ubi , & vi );
int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl> if ( ret != - EPROBE_DEFER ) <nl> list_del (& driver -> pending ); <nl> if ( ret ) <nl> - goto err4 ; <nl> + goto err5 ; <nl> break ; <nl> } <nl> } <nl> int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl>  <nl> return 0 ; <nl>  <nl> + err5 : <nl> + device_del (& udc -> dev ); <nl> + <nl> err4 : <nl> list_del (& udc -> list ); <nl> mutex_unlock (& udc_lock );
void scsi_io_completion ( struct scsi_cmnd * cmd , unsigned int good_bytes ) <nl> */ <nl> req -> next_rq -> resid_len = scsi_in ( cmd )-> resid ; <nl>  <nl> + scsi_release_buffers ( cmd ); <nl> blk_end_request_all ( req , 0 ); <nl>  <nl> - scsi_release_buffers ( cmd ); <nl> scsi_next_command ( cmd ); <nl> return ; <nl> }
int i2400m_msg_check_status ( const struct i2400m_l3l4_hdr * l3l4_hdr , <nl>  <nl> if ( status == 0 ) <nl> return 0 ; <nl> - if ( status > ARRAY_SIZE ( ms_to_errno )) { <nl> + if ( status >= ARRAY_SIZE ( ms_to_errno )) { <nl> str = " unknown status code "; <nl> result = - EBADR ; <nl> } else {
static int r8a66597_hub_status_data ( struct usb_hcd * hcd , char * buf ) <nl> static void r8a66597_hub_descriptor ( struct r8a66597 * r8a66597 , <nl> struct usb_hub_descriptor * desc ) <nl> { <nl> - desc -> bDescriptorType = 0x29 ; <nl> + desc -> bDescriptorType = USB_DT_HUB ; <nl> desc -> bHubContrCurrent = 0 ; <nl> desc -> bNbrPorts = r8a66597 -> max_root_hub ; <nl> desc -> bDescLength = 9 ;
static struct urb * usbtv_setup_iso_transfer ( struct usbtv * usbtv ) <nl> ip -> transfer_flags = URB_ISO_ASAP ; <nl> ip -> transfer_buffer = kzalloc ( size * USBTV_ISOC_PACKETS , <nl> GFP_KERNEL ); <nl> + if (! ip -> transfer_buffer ) { <nl> + usb_free_urb ( ip ); <nl> + return NULL ; <nl> + } <nl> ip -> complete = usbtv_iso_cb ; <nl> ip -> number_of_packets = USBTV_ISOC_PACKETS ; <nl> ip -> transfer_buffer_length = size * USBTV_ISOC_PACKETS ;
static int atmel_serial_remove ( struct platform_device * pdev ) <nl> struct atmel_uart_port * atmel_port = to_atmel_uart_port ( port ); <nl> int ret = 0 ; <nl>  <nl> + tasklet_kill (& atmel_port -> tasklet ); <nl> + <nl> device_init_wakeup (& pdev -> dev , 0 ); <nl>  <nl> ret = uart_remove_one_port (& atmel_uart , port ); <nl>  <nl> - tasklet_kill (& atmel_port -> tasklet ); <nl> kfree ( atmel_port -> rx_ring . buf ); <nl>  <nl> /* " port " is allocated statically , so we shouldn ' t free it */
static void load_render_mocs ( struct drm_i915_private * dev_priv ) <nl> }; <nl> int ring_id , i ; <nl>  <nl> - for ( ring_id = 0 ; ring_id < I915_NUM_ENGINES ; ring_id ++) { <nl> + for ( ring_id = 0 ; ring_id < ARRAY_SIZE ( regs ); ring_id ++) { <nl> offset . reg = regs [ ring_id ]; <nl> for ( i = 0 ; i < 64 ; i ++) { <nl> gen9_render_mocs . control_table [ ring_id ][ i ] =
flush_signal_handlers ( struct task_struct * t , int force_default ) <nl> if ( force_default || ka -> sa . sa_handler != SIG_IGN ) <nl> ka -> sa . sa_handler = SIG_DFL ; <nl> ka -> sa . sa_flags = 0 ; <nl> +# ifdef SA_RESTORER <nl> + ka -> sa . sa_restorer = NULL ; <nl> +# endif <nl> sigemptyset (& ka -> sa . sa_mask ); <nl> ka ++; <nl> }
static int cpufreq_governor_dbs ( struct cpufreq_policy * policy , <nl> if ( latency == 0 ) <nl> latency = 1 ; <nl>  <nl> - def_sampling_rate = latency * <nl> + def_sampling_rate = 10 * latency * <nl> DEF_SAMPLING_RATE_LATENCY_MULTIPLIER ; <nl>  <nl> if ( def_sampling_rate < MIN_STAT_SAMPLING_RATE )
qdio_check_ccq ( struct qdio_q * q , unsigned int ccq ) <nl> { <nl> char dbf_text [ 15 ]; <nl>  <nl> - if ( ccq == 0 || ccq == 32 || ccq == 96 ) <nl> + if ( ccq == 0 || ccq == 32 ) <nl> return 0 ; <nl> - if ( ccq == 97 ) <nl> + if ( ccq == 96 || ccq == 97 ) <nl> return 1 ; <nl> /* notify devices immediately */ <nl> sprintf ( dbf_text ,"% d ", ccq );
static void ssb_pmu_resources_init ( struct ssb_chipcommon * cc ) <nl>  <nl> switch ( bus -> chip_id ) { <nl> case 0x4312 : <nl> + min_msk = 0xCBB ; <nl> + break ; <nl> case 0x4322 : <nl> /* We keep the default settings : <nl> * min_msk = 0xCBB
int btrfs_read_sys_array ( struct btrfs_root * root ) <nl> sb_array_offset += len ; <nl> cur_offset += len ; <nl> } <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return ret ; <nl>  <nl> out_short_read : <nl> printk ( KERN_ERR " BTRFS : sys_array too short to read % u bytes at offset % u \ n ", <nl> len , cur_offset ); <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return - EIO ; <nl> }
static void t3e3_remove_card ( struct pci_dev * pdev ) <nl> struct channel * channel0 = pci_get_drvdata ( pdev ); <nl> struct card * card = channel0 -> card ; <nl>  <nl> - del_timer (& card -> timer ); <nl> + del_timer_sync (& card -> timer ); <nl> if ( has_two_ports ( channel0 -> pdev )) { <nl> t3e3_remove_channel (& card -> channels [ 1 ]); <nl> pci_dev_put ( card -> channels [ 1 ]. pdev );
static int quota_setinfo ( struct super_block * sb , int type , void __user * addr ) <nl>  <nl> static void copy_to_if_dqblk ( struct if_dqblk * dst , struct fs_disk_quota * src ) <nl> { <nl> + memset ( dst , 0 , sizeof (* dst )); <nl> dst -> dqb_bhardlimit = src -> d_blk_hardlimit ; <nl> dst -> dqb_bsoftlimit = src -> d_blk_softlimit ; <nl> dst -> dqb_curspace = src -> d_bcount ;
struct fman_mac * memac_config ( struct fman_mac_params * params ) <nl> /* Save FMan revision */ <nl> fman_get_revision ( memac -> fm , & memac -> fm_rev_info ); <nl>  <nl> - if ( memac -> phy_if == PHY_INTERFACE_MODE_SGMII ) { <nl> + if ( memac -> phy_if == PHY_INTERFACE_MODE_SGMII || <nl> + memac -> phy_if == PHY_INTERFACE_MODE_QSGMII ) { <nl> if (! params -> internal_phy_node ) { <nl> pr_err (" PCS PHY node is not available \ n "); <nl> memac_free ( memac );
static int start_ap ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> priv = wiphy_priv ( wiphy ); <nl> vif = netdev_priv ( dev ); <nl> - wl = vif -> wilc ; <nl> + wl = vif -> wilc ; <nl>  <nl> s32Error = set_channel ( wiphy , & settings -> chandef ); <nl> 
static int bpa10x_probe ( struct usb_interface * intf , const struct usb_device_id * <nl> if ( ignore ) <nl> return - ENODEV ; <nl>  <nl> + if ( intf -> cur_altsetting -> desc . bInterfaceNumber > 0 ) <nl> + return - ENODEV ; <nl> + <nl> data = kmalloc ( sizeof (* data ), GFP_KERNEL ); <nl> if (! data ) { <nl> BT_ERR (" Can ' t allocate data structure ");
static void pptp_expectfn ( struct nf_conn * ct , <nl>  <nl> rcu_read_lock (); <nl> nf_nat_pptp_expectfn = rcu_dereference ( nf_nat_pptp_hook_expectfn ); <nl> - if ( nf_nat_pptp_expectfn && ct -> status & IPS_NAT_MASK ) <nl> + if ( nf_nat_pptp_expectfn && ct -> master -> status & IPS_NAT_MASK ) <nl> nf_nat_pptp_expectfn ( ct , exp ); <nl> else { <nl> struct nf_conntrack_tuple inv_t ;
static int pn_send ( struct sk_buff * skb , struct net_device * dev , <nl> struct phonethdr * ph ; <nl> int err ; <nl>  <nl> - if ( skb -> len + 2 > 0xffff ) { <nl> - /* Phonet length field would overflow */ <nl> + if ( skb -> len + 2 > 0xffff /* Phonet length field limit */ || <nl> + skb -> len + sizeof ( struct phonethdr ) > dev -> mtu ) { <nl> err = - EMSGSIZE ; <nl> goto drop ; <nl> }
void rtsx_add_cmd ( struct rtsx_chip * chip , <nl> void rtsx_send_cmd_no_wait ( struct rtsx_chip * chip ); <nl> int rtsx_send_cmd ( struct rtsx_chip * chip , u8 card , int timeout ); <nl>  <nl> - extern inline u8 * rtsx_get_cmd_data ( struct rtsx_chip * chip ) <nl> + static inline u8 * rtsx_get_cmd_data ( struct rtsx_chip * chip ) <nl> { <nl> # ifdef CMD_USING_SG <nl> return ( u8 *)( chip -> host_sg_tbl_ptr );
static int device_rx_srv ( struct vnt_private * pDevice , unsigned int uIdx ) <nl> pRD = pRD -> next ) { <nl> if ( works ++ > 15 ) <nl> break ; <nl> + <nl> + if (! pRD -> pRDInfo -> skb ) <nl> + break ; <nl> + <nl> if ( vnt_receive_frame ( pDevice , pRD )) { <nl> if (! device_alloc_rx_buf ( pDevice , pRD )) { <nl> dev_err (& pDevice -> pcid -> dev ,
# include < linux / interrupt . h > <nl> # include < linux / pci . h > <nl> # include < linux / firmware . h > <nl> +# include < linux / vmalloc . h > <nl> # include < asm / io . h > <nl> # include < sound / core . h > <nl> # include " mixart . h "
void rcu_check_callbacks ( int cpu , int user ) <nl> static void rcu_init_percpu_data ( int cpu , struct rcu_ctrlblk * rcp , <nl> struct rcu_data * rdp ) <nl> { <nl> - long flags ; <nl> + unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& rcp -> lock , flags ); <nl> memset ( rdp , 0 , sizeof (* rdp ));
composite_setup ( struct usb_gadget * gadget , const struct usb_ctrlrequest * ctrl ) <nl> if ( w_index != 0x5 || ( w_value >> 8 )) <nl> break ; <nl> interface = w_value & 0xFF ; <nl> + if ( interface >= MAX_CONFIG_INTERFACES || <nl> + ! os_desc_cfg -> interface [ interface ]) <nl> + break ; <nl> buf [ 6 ] = w_index ; <nl> count = count_ext_prop ( os_desc_cfg , <nl> interface );
static int ilk_compute_pipe_wm ( struct intel_crtc * intel_crtc , <nl> return PTR_ERR ( cstate ); <nl>  <nl> pipe_wm = & cstate -> wm . optimal . ilk ; <nl> + memset ( pipe_wm , 0 , sizeof (* pipe_wm )); <nl>  <nl> for_each_intel_plane_on_crtc ( dev , intel_crtc , intel_plane ) { <nl> ps = drm_atomic_get_plane_state ( state ,
static const struct snd_pci_quirk alc269_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x103c , 0x226e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x2271 , " HP ", ALC286_FIXUP_HP_GPIO_LED ), <nl> SND_PCI_QUIRK ( 0x103c , 0x2272 , " HP ", ALC280_FIXUP_HP_DOCK_PINS ), <nl> + SND_PCI_QUIRK ( 0x103c , 0x2273 , " HP ", ALC280_FIXUP_HP_DOCK_PINS ), <nl> SND_PCI_QUIRK ( 0x103c , 0x229e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22b2 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22b7 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ),
int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> + mutex_lock (& gpu -> lock ); <nl> + <nl> fence = etnaviv_gpu_fence_alloc ( gpu ); <nl> if (! fence ) { <nl> event_free ( gpu , event ); <nl> int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> - mutex_lock (& gpu -> lock ); <nl> - <nl> gpu -> event [ event ]. fence = fence ; <nl> submit -> fence = fence -> seqno ; <nl> gpu -> active_fence = submit -> fence ;
xfs_fs_remount ( <nl>  <nl> /* ro -> rw */ <nl> if (( mp -> m_flags & XFS_MOUNT_RDONLY ) && !(* flags & MS_RDONLY )) { <nl> + if ( mp -> m_flags & XFS_MOUNT_NORECOVERY ) { <nl> + xfs_warn ( mp , <nl> + " ro -> rw transition prohibited on norecovery mount "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> mp -> m_flags &= ~ XFS_MOUNT_RDONLY ; <nl>  <nl> /*
static int drbg_generate_long ( struct drbg_state * drbg , <nl> if ( 0 >= tmplen ) <nl> return tmplen ; <nl> len += tmplen ; <nl> - } while ( slice > 0 ); <nl> + } while ( slice > 0 && ( len < buflen )); <nl> return len ; <nl> } <nl> 
struct kvm_s390_float_interrupt { <nl> struct list_head list ; <nl> atomic_t active ; <nl> int next_rr_cpu ; <nl> - unsigned long idle_mask [( 64 + sizeof ( long ) - 1 ) / sizeof ( long )]; <nl> - struct kvm_s390_local_interrupt * local_int [ 64 ]; <nl> + unsigned long idle_mask [( KVM_MAX_VCPUS + sizeof ( long ) - 1 ) <nl> + / sizeof ( long )]; <nl> + struct kvm_s390_local_interrupt * local_int [ KVM_MAX_VCPUS ]; <nl> }; <nl>  <nl> 
static void scsi_finish_async_scan ( struct async_scan_data * data ) <nl> printk ("% s called twice for host % d ", __FUNCTION__ , <nl> shost -> host_no ); <nl> dump_stack (); <nl> + mutex_unlock (& shost -> scan_mutex ); <nl> return ; <nl> } <nl> 
void spuctx_switch_state ( struct spu_context * ctx , <nl> node = spu -> node ; <nl> if ( old_state == SPU_UTIL_USER ) <nl> atomic_dec (& cbe_spu_info [ node ]. busy_spus ); <nl> - if ( new_state == SPU_UTIL_USER ); <nl> + if ( new_state == SPU_UTIL_USER ) <nl> atomic_inc (& cbe_spu_info [ node ]. busy_spus ); <nl> } <nl> }
static int create_filter ( struct trace_event_call * call , <nl> if ( err && set_str ) <nl> append_filter_err ( ps , filter ); <nl> } <nl> + if ( err && ! set_str ) { <nl> + free_event_filter ( filter ); <nl> + filter = NULL ; <nl> + } <nl> create_filter_finish ( ps ); <nl>  <nl> * filterp = filter ;
static int snapshot_open ( struct inode * inode , struct file * filp ) <nl> if ( error ) <nl> pm_notifier_call_chain ( PM_POST_RESTORE ); <nl> } <nl> - if ( error ) <nl> + if ( error ) { <nl> + free_basic_memory_bitmaps (); <nl> atomic_inc (& snapshot_device_available ); <nl> + } <nl> data -> frozen = 0 ; <nl> data -> ready = 0 ; <nl> data -> platform_support = 0 ;
fill_write_buffer ( struct sysfs_buffer * buffer , const char __user * buf , size_t <nl> return - ENOMEM ; <nl>  <nl> if ( count >= PAGE_SIZE ) <nl> - count = PAGE_SIZE ; <nl> + count = PAGE_SIZE - 1 ; <nl> error = copy_from_user ( buffer -> page , buf , count ); <nl> buffer -> needs_read_fill = 1 ; <nl> return error ? - EFAULT : count ;
static int ipw2100_get_firmware ( struct ipw2100_priv * priv , <nl> return 0 ; <nl> } <nl>  <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("- i ")); <nl> +# ifdef CONFIG_IPW2100_MONITOR <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("- p ")); <nl> +# endif <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("")); <nl> + <nl> static void ipw2100_release_firmware ( struct ipw2100_priv * priv , <nl> struct ipw2100_fw * fw ) <nl> {
int kvm_set_msr_common ( struct kvm_vcpu * vcpu , struct msr_data * msr_info ) <nl> /* ... but clean it before doing the actual write */ <nl> vcpu -> arch . time_offset = data & ~( PAGE_MASK | 1 ); <nl>  <nl> + /* Check that the address is 32 - byte aligned . */ <nl> + if ( vcpu -> arch . time_offset & <nl> + ( sizeof ( struct pvclock_vcpu_time_info ) - 1 )) <nl> + break ; <nl> + <nl> vcpu -> arch . time_page = <nl> gfn_to_page ( vcpu -> kvm , data >> PAGE_SHIFT ); <nl> 
static long __write_once initfree = 1 ; <nl> static int __init set_initfree ( char * str ) <nl> { <nl> long val ; <nl> - if ( strict_strtol ( str , 0 , & val )) { <nl> + if ( strict_strtol ( str , 0 , & val ) == 0 ) { <nl> initfree = val ; <nl> pr_info (" initfree : % s free init pages \ n ", <nl> initfree ? " will " : " won ' t ");
static int rtnl_fill_ifinfo ( struct sk_buff * skb , struct net_device * dev , <nl> * report anything . <nl> */ <nl> ivi . spoofchk = - 1 ; <nl> + memset ( ivi . mac , 0 , sizeof ( ivi . mac )); <nl> if ( dev -> netdev_ops -> ndo_get_vf_config ( dev , i , & ivi )) <nl> break ; <nl> vf_mac . vf =
set_pte_phys ( unsigned long vaddr , unsigned long phys , pgprot_t prot ) <nl> new_pte = pfn_pte ( phys >> PAGE_SHIFT , prot ); <nl>  <nl> pte = pte_offset_kernel ( pmd , vaddr ); <nl> - if (! pte_none (* pte ) && <nl> + if (! pte_none (* pte ) && pte_val ( new_pte ) && <nl> pte_val (* pte ) != ( pte_val ( new_pte ) & __supported_pte_mask )) <nl> pte_ERROR (* pte ); <nl> set_pte ( pte , new_pte );
i915_gem_execbuffer2 ( struct drm_device * dev , void * data , <nl> struct drm_i915_gem_exec_object2 * exec2_list = NULL ; <nl> int ret ; <nl>  <nl> - if ( args -> buffer_count < 1 ) { <nl> + if ( args -> buffer_count < 1 || <nl> + args -> buffer_count > UINT_MAX / sizeof (* exec2_list )) { <nl> DRM_DEBUG (" execbuf2 with % d buffers \ n ", args -> buffer_count ); <nl> return - EINVAL ; <nl> }
static int ax25_release ( struct socket * sock ) <nl> ax25_destroy_socket ( ax25 ); <nl> } <nl> if ( ax25_dev ) { <nl> + del_timer_sync (& ax25 -> timer ); <nl> + del_timer_sync (& ax25 -> t1timer ); <nl> + del_timer_sync (& ax25 -> t2timer ); <nl> + del_timer_sync (& ax25 -> t3timer ); <nl> + del_timer_sync (& ax25 -> idletimer ); <nl> dev_put_track ( ax25_dev -> dev , & ax25_dev -> dev_tracker ); <nl> ax25_dev_put ( ax25_dev ); <nl> }
static int ttm_mem_init_dma32_zone ( struct ttm_mem_global * glob , <nl> * No special dma32 zone needed . <nl> */ <nl>  <nl> - if ( mem <= (( uint64_t ) 1ULL << 32 )) <nl> + if ( mem <= (( uint64_t ) 1ULL << 32 )) { <nl> + kfree ( zone ); <nl> return 0 ; <nl> + } <nl>  <nl> /* <nl> * Limit max dma32 memory to 4GB for now
static void cnl_display_core_init ( struct drm_i915_private * dev_priv , bool resume <nl>  <nl> /* 6 . Enable DBUF */ <nl> gen9_dbuf_enable ( dev_priv ); <nl> + <nl> + if ( resume && dev_priv -> csr . dmc_payload ) <nl> + intel_csr_load_program ( dev_priv ); <nl> } <nl>  <nl> # undef CNL_PROCMON_IDX
static int get_port_device_capability ( struct pci_dev * dev ) <nl> if ( reg32 & SLOT_HP_CAPABLE_MASK ) <nl> services |= PCIE_PORT_SERVICE_HP ; <nl> } <nl> - /* PME Capable */ <nl> - pos = pci_find_capability ( dev , PCI_CAP_ID_PME ); <nl> - if ( pos ) <nl> + /* PME Capable - root port capability */ <nl> + if ((( reg16 >> 4 ) & PORT_TYPE_MASK ) == PCIE_RC_PORT ) <nl> services |= PCIE_PORT_SERVICE_PME ; <nl>  <nl> pos = PCI_CFG_SPACE_SIZE ;
static int llc_ui_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> int target ; /* Read at least this many bytes */ <nl> long timeo ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl> copied = - ENOTCONN ; <nl> if ( unlikely ( sk -> sk_type == SOCK_STREAM && sk -> sk_state == TCP_LISTEN ))
void e1000e_reset ( struct e1000_adapter * adapter ) <nl> e1000e_reset_adaptive ( hw ); <nl> e1000_get_phy_info ( hw ); <nl>  <nl> - if (!( adapter -> flags & FLAG_SMART_POWER_DOWN )) { <nl> + if (( adapter -> flags & FLAG_HAS_SMART_POWER_DOWN ) && <nl> + !( adapter -> flags & FLAG_SMART_POWER_DOWN )) { <nl> u16 phy_data = 0 ; <nl> /* <nl> * speed up time to link by disabling smart power down , ignore
static ssize_t environ_read ( struct file * file , char __user * buf , <nl> struct mm_struct * mm = file -> private_data ; <nl> unsigned long env_start , env_end ; <nl>  <nl> - if (! mm ) <nl> + /* Ensure the process spawned far enough to have an environment . */ <nl> + if (! mm || ! mm -> env_end ) <nl> return 0 ; <nl>  <nl> page = ( char *) __get_free_page ( GFP_TEMPORARY );
static int tegra_output_hdmi_check_mode ( struct tegra_output * output , <nl> parent = clk_get_parent ( hdmi -> clk_parent ); <nl>  <nl> err = clk_round_rate ( parent , pclk * 4 ); <nl> - if ( err < 0 ) <nl> + if ( err <= 0 ) <nl> * status = MODE_NOCLOCK ; <nl> else <nl> * status = MODE_OK ;
static int rose_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> skb_copy_datagram_iovec ( skb , 0 , msg -> msg_iov , copied ); <nl>  <nl> if ( srose != NULL ) { <nl> + memset ( srose , 0 , msg -> msg_namelen ); <nl> srose -> srose_family = AF_ROSE ; <nl> srose -> srose_addr = rose -> dest_addr ; <nl> srose -> srose_call = rose -> dest_call ;
static long DAC960_gam_ioctl ( struct file * file , unsigned int Request , <nl> else <nl> ErrorCode = 0 ; <nl> } <nl> + break ; <nl> default : <nl> ErrorCode = - ENOTTY ; <nl> }
unsigned long parse_tag_value ( const char * str , struct parse_tag * tags ) <nl> if ( s != endptr ) <nl> break ; <nl>  <nl> + if ( value > ULONG_MAX / i -> mult ) <nl> + break ; <nl> value *= i -> mult ; <nl> return value ; <nl> }
static ssize_t pubek_show ( struct device * dev , struct device_attribute * attr , <nl> ssize_t err ; <nl> int i , rc ; <nl> char * str = buf ; <nl> - <nl> struct tpm_chip * chip = to_tpm_chip ( dev ); <nl>  <nl> + memset (& tpm_cmd , 0 , sizeof ( tpm_cmd )); <nl> + <nl> tpm_cmd . header . in = tpm_readpubek_header ; <nl> err = tpm_transmit_cmd ( chip , NULL , & tpm_cmd , READ_PUBEK_RESULT_SIZE , <nl> READ_PUBEK_RESULT_MIN_BODY_SIZE , 0 ,
static int msm_otg_read_dt ( struct platform_device * pdev , struct msm_otg * motg ) <nl> motg -> pdata = pdata ; <nl>  <nl> id = of_match_device ( msm_otg_dt_match , & pdev -> dev ); <nl> - pdata -> phy_type = ( int ) id -> data ; <nl> + pdata -> phy_type = ( enum msm_usb_phy_type ) id -> data ; <nl>  <nl> motg -> link_rst = devm_reset_control_get (& pdev -> dev , " link "); <nl> if ( IS_ERR ( motg -> link_rst ))
xfs_rmap_convert_shared ( <nl> */ <nl> error = xfs_rmap_lookup_le_range ( cur , bno , owner , offset , flags , <nl> & PREV , & i ); <nl> + if ( error ) <nl> + goto done ; <nl> XFS_WANT_CORRUPTED_GOTO ( mp , i == 1 , done ); <nl>  <nl> ASSERT ( PREV . rm_offset <= offset );
static ssize_t bat_socket_read ( struct file * file , char __user * buf , <nl>  <nl> spin_unlock_bh (& socket_client -> lock ); <nl>  <nl> - error = copy_to_user ( buf , & socket_packet -> icmp_packet , <nl> - socket_packet -> icmp_len ); <nl> + packet_len = min ( count , socket_packet -> icmp_len ); <nl> + error = copy_to_user ( buf , & socket_packet -> icmp_packet , packet_len ); <nl>  <nl> - packet_len = socket_packet -> icmp_len ; <nl> kfree ( socket_packet ); <nl>  <nl> if ( error )
int dccp_disconnect ( struct sock * sk , int flags ) <nl> sk -> sk_err = ECONNRESET ; <nl>  <nl> dccp_clear_xmit_timers ( sk ); <nl> + <nl> __skb_queue_purge (& sk -> sk_receive_queue ); <nl> + __skb_queue_purge (& sk -> sk_write_queue ); <nl> if ( sk -> sk_send_head != NULL ) { <nl> __kfree_skb ( sk -> sk_send_head ); <nl> sk -> sk_send_head = NULL ;
static int crypto_report ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
static int sgtl5000_set_clock ( struct snd_soc_codec * codec , int frame_rate ) <nl> } else { <nl> dev_err ( codec -> dev , <nl> " PLL not supported in slave mode \ n "); <nl> + dev_err ( codec -> dev , "% d ratio is not supported . " <nl> + " SYS_MCLK needs to be 256 , 384 or 512 * fs \ n ", <nl> + sgtl5000 -> sysclk / sys_fs ); <nl> return - EINVAL ; <nl> } <nl> }
ecryptfs_decode_from_filename ( unsigned char * dst , size_t * dst_size , <nl> break ; <nl> case 2 : <nl> dst [ dst_byte_offset ++] |= ( src_byte ); <nl> - dst [ dst_byte_offset ] = 0 ; <nl> current_bit_offset = 0 ; <nl> break ; <nl> }
static int __xen_pcibk_add_pci_dev ( struct xen_pcibk_device * pdev , <nl> /* Publish this device . */ <nl> if (! err ) <nl> err = publish_cb ( pdev , 0 , 0 , PCI_DEVFN ( slot , func ), devid ); <nl> + else <nl> + kfree ( dev_entry ); <nl>  <nl> out : <nl> return err ;
static int ath9k_htc_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath9k_htc_sta * ista ; <nl> int ret = 0 ; <nl>  <nl> + mutex_lock (& priv -> mutex ); <nl> + <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> break ; <nl> static int ath9k_htc_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( priv -> ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> + mutex_unlock (& priv -> mutex ); <nl> + <nl> return ret ; <nl> } <nl> 
void xhci_mem_cleanup ( struct xhci_hcd * xhci ) <nl>  <nl> if ( xhci -> lpm_command ) <nl> xhci_free_command ( xhci , xhci -> lpm_command ); <nl> + xhci -> lpm_command = NULL ; <nl> if ( xhci -> cmd_ring ) <nl> xhci_ring_free ( xhci , xhci -> cmd_ring ); <nl> xhci -> cmd_ring = NULL ;
int vcc_getsockopt ( struct socket * sock , int level , int optname , <nl>  <nl> if (! vcc -> dev || ! test_bit ( ATM_VF_ADDR , & vcc -> flags )) <nl> return - ENOTCONN ; <nl> + memset (& pvc , 0 , sizeof ( pvc )); <nl> pvc . sap_family = AF_ATMPVC ; <nl> pvc . sap_addr . itf = vcc -> dev -> number ; <nl> pvc . sap_addr . vpi = vcc -> vpi ;
static void kvm_write_wall_clock ( struct kvm * kvm , gpa_t wall_clock ) <nl> */ <nl> getboottime (& boot ); <nl>  <nl> + if ( kvm -> arch . kvmclock_offset ) { <nl> + struct timespec ts = ns_to_timespec ( kvm -> arch . kvmclock_offset ); <nl> + boot = timespec_sub ( boot , ts ); <nl> + } <nl> wc . sec = boot . tv_sec ; <nl> wc . nsec = boot . tv_nsec ; <nl> wc . version = version ;
static netdev_tx_t ipip6_tunnel_xmit ( struct sk_buff * skb , <nl> if ( tunnel -> parms . iph . daddr && skb_dst ( skb )) <nl> skb_dst ( skb )-> ops -> update_pmtu ( skb_dst ( skb ), NULL , skb , mtu ); <nl>  <nl> - if ( skb -> len > mtu ) { <nl> + if ( skb -> len > mtu && ! skb_is_gso ( skb )) { <nl> icmpv6_send ( skb , ICMPV6_PKT_TOOBIG , 0 , mtu ); <nl> ip_rt_put ( rt ); <nl> goto tx_error ;
static void pty_close ( struct tty_struct * tty , struct file * filp ) <nl> mutex_unlock (& devpts_mutex ); <nl> } <nl> # endif <nl> + tty_unlock ( tty ); <nl> tty_vhangup ( tty -> link ); <nl> + tty_lock ( tty ); <nl> } <nl> } <nl> 
static int translate_desc ( struct vhost_dev * dev , u64 addr , u32 len , <nl> } <nl> _iov = iov + ret ; <nl> size = reg -> memory_size - addr + reg -> guest_phys_addr ; <nl> - _iov -> iov_len = min (( u64 ) len , size ); <nl> + _iov -> iov_len = min (( u64 ) len - s , size ); <nl> _iov -> iov_base = ( void __user *)( unsigned long ) <nl> ( reg -> userspace_addr + addr - reg -> guest_phys_addr ); <nl> s += size ;
i915_gem_create ( struct drm_file * file , <nl> u32 handle ; <nl>  <nl> size = roundup ( size , PAGE_SIZE ); <nl> + if ( size == 0 ) <nl> + return - EINVAL ; <nl>  <nl> /* Allocate the new object */ <nl> obj = i915_gem_alloc_object ( dev , size );
int x86_emulate_instruction ( struct kvm_vcpu * vcpu , <nl> if ( reexecute_instruction ( vcpu , cr2 , write_fault_to_spt , <nl> emulation_type )) <nl> return EMULATE_DONE ; <nl> + if ( ctxt -> have_exception && inject_emulated_exception ( vcpu )) <nl> + return EMULATE_DONE ; <nl> if ( emulation_type & EMULTYPE_SKIP ) <nl> return EMULATE_FAIL ; <nl> return handle_emulation_failure ( vcpu );
brcmf_cfg80211_start_ap ( struct wiphy * wiphy , struct net_device * ndev , <nl> ( u8 *)& settings -> beacon . head [ ie_offset ], <nl> settings -> beacon . head_len - ie_offset , <nl> WLAN_EID_SSID ); <nl> - if (! ssid_ie ) <nl> + if (! ssid_ie || ssid_ie -> len > IEEE80211_MAX_SSID_LEN ) <nl> return - EINVAL ; <nl>  <nl> memcpy ( ssid_le . SSID , ssid_ie -> data , ssid_ie -> len );
static void nhmex_uncore_msr_enable_event ( struct intel_uncore_box * box , struct p <nl> { <nl> struct hw_perf_event * hwc = & event -> hw ; <nl>  <nl> - if ( hwc -> idx >= UNCORE_PMC_IDX_FIXED ) <nl> + if ( hwc -> idx == UNCORE_PMC_IDX_FIXED ) <nl> wrmsrl ( hwc -> config_base , NHMEX_PMON_CTL_EN_BIT0 ); <nl> else if ( box -> pmu -> type -> event_mask & NHMEX_PMON_CTL_EN_BIT0 ) <nl> wrmsrl ( hwc -> config_base , hwc -> config | NHMEX_PMON_CTL_EN_BIT22 );
mountpoint_last ( struct nameidata * nd , struct path * path ) <nl> goto out ; <nl> } <nl> path -> dentry = dentry ; <nl> - path -> mnt = mntget ( nd -> path . mnt ); <nl> + path -> mnt = nd -> path . mnt ; <nl> if ( should_follow_link ( dentry , nd -> flags & LOOKUP_FOLLOW )) <nl> return 1 ; <nl> + mntget ( path -> mnt ); <nl> follow_mount ( path ); <nl> error = 0 ; <nl> out :
static int exofs_read_lookup_dev_table ( struct exofs_sb_info ** psbi , <nl> } <nl>  <nl> od = osduld_info_lookup (& odi ); <nl> - if ( unlikely ( IS_ERR ( od ))) { <nl> + if ( IS_ERR ( od )) { <nl> ret = PTR_ERR ( od ); <nl> EXOFS_ERR (" ERROR : device requested is not found " <nl> " osd_name -% s =>% d \ n ", odi . osdname , ret );
static int fs_enet_rx_napi ( struct napi_struct * napi , int budget ) <nl> u16 pkt_len , sc ; <nl> int curidx ; <nl>  <nl> + if ( budget <= 0 ) <nl> + return received ; <nl> + <nl> /* <nl> * First , grab all of the stats for the incoming packet . <nl> * These get messed up if we get called due to a busy condition .
void msm_gem_free_object ( struct drm_gem_object * obj ) <nl> if ( msm_obj -> pages ) <nl> drm_free_large ( msm_obj -> pages ); <nl>  <nl> + drm_prime_gem_destroy ( obj , msm_obj -> sgt ); <nl> } else { <nl> vunmap ( msm_obj -> vaddr ); <nl> put_pages ( obj );
static void pwmled_brightness ( struct led_classdev * cdev , enum led_brightness b ) <nl> * NOTE : we reuse the platform_data structure of GPIO leds , <nl> * but repurpose its " gpio " number as a PWM channel number . <nl> */ <nl> - static int __init pwmled_probe ( struct platform_device * pdev ) <nl> + static int __devinit pwmled_probe ( struct platform_device * pdev ) <nl> { <nl> const struct gpio_led_platform_data * pdata ; <nl> struct pwmled * leds ;
static int uas_slave_configure ( struct scsi_device * sdev ) <nl> if ( devinfo -> flags & US_FL_BROKEN_FUA ) <nl> sdev -> broken_fua = 1 ; <nl>  <nl> + scsi_change_queue_depth ( sdev , devinfo -> qdepth - 2 ); <nl> return 0 ; <nl> } <nl> 
asmlinkage void math_emulate ( long arg ) <nl> entry_sel_off . offset = FPU_ORIG_EIP ; <nl> entry_sel_off . selector = FPU_CS ; <nl> entry_sel_off . opcode = ( byte1 << 8 ) | FPU_modrm ; <nl> + entry_sel_off . empty = 0 ; <nl>  <nl> FPU_rm = FPU_modrm & 7 ; <nl> 
static inline void get_page ( struct page * page ) <nl> page_ref_inc ( page ); <nl> } <nl>  <nl> + static inline __must_check bool try_get_page ( struct page * page ) <nl> +{ <nl> + page = compound_head ( page ); <nl> + if ( WARN_ON_ONCE ( page_ref_count ( page ) <= 0 )) <nl> + return false ; <nl> + page_ref_inc ( page ); <nl> + return true ; <nl> +} <nl> + <nl> static inline void put_page ( struct page * page ) <nl> { <nl> page = compound_head ( page );
void scatterwalk_map_and_copy ( void * buf , struct scatterlist * sg , <nl> struct scatter_walk walk ; <nl> unsigned int offset = 0 ; <nl>  <nl> + if (! nbytes ) <nl> + return ; <nl> + <nl> for (;;) { <nl> scatterwalk_start (& walk , sg ); <nl> 
static void __init tegra210_pll_init ( void __iomem * clk_base , <nl>  <nl> /* PLLU_VCO */ <nl> val = readl ( clk_base + pll_u_vco_params . base_reg ); <nl> - val &= ~ BIT ( 24 ); /* disable PLLU_OVERRIDE */ <nl> + val &= ~ PLLU_BASE_OVERRIDE ; /* disable PLLU_OVERRIDE */ <nl> writel ( val , clk_base + pll_u_vco_params . base_reg ); <nl>  <nl> clk = tegra_clk_register_pllre (" pll_u_vco ", " pll_ref ", clk_base , pmc ,
static void apc_agent_timeout ( unsigned long data ) <nl> configure_phy_mask = ~ port_agent -> phy_configured_mask & port_agent -> phy_ready_mask ; <nl>  <nl> if (! configure_phy_mask ) <nl> - return ; <nl> + goto done ; <nl>  <nl> for ( index = 0 ; index < SCI_MAX_PHYS ; index ++) { <nl> if (( configure_phy_mask & ( 1 << index )) == 0 )
static int smsdvb_hotplug ( struct smscore_device_t * coredev , <nl> switch ( smscore_get_device_mode ( coredev )) { <nl> case DEVICE_MODE_DVBT : <nl> case DEVICE_MODE_DVBT_BDA : <nl> - smsdvb_fe_ops . delsys [ 0 ] = SYS_DVBT ; <nl> + client -> frontend . ops . delsys [ 0 ] = SYS_DVBT ; <nl> break ; <nl> case DEVICE_MODE_ISDBT : <nl> case DEVICE_MODE_ISDBT_BDA : <nl> - smsdvb_fe_ops . delsys [ 0 ] = SYS_ISDBT ; <nl> + client -> frontend . ops . delsys [ 0 ] = SYS_ISDBT ; <nl> break ; <nl> } <nl> 
xfs_iext_remove_node ( <nl> node -> ptrs [ nr_entries ] = NULL ; <nl>  <nl> if ( pos == 0 && nr_entries > 0 ) { <nl> - xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , <nl> - node ); <nl> + xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , node ); <nl> offset = node -> keys [ 0 ]; <nl> } <nl> 
enum xgbe_conn_type { <nl> XGBE_CONN_TYPE_NONE = 0 , <nl> XGBE_CONN_TYPE_SFP , <nl> XGBE_CONN_TYPE_MDIO , <nl> + XGBE_CONN_TYPE_RSVD1 , <nl> XGBE_CONN_TYPE_BACKPLANE , <nl> XGBE_CONN_TYPE_MAX , <nl> }; <nl> static int xgbe_phy_init ( struct xgbe_prv_data * pdata ) <nl> if ( xgbe_phy_conn_type_mismatch ( pdata )) { <nl> dev_err ( pdata -> dev , " phy mode / connection mismatch (%# x /%# x )\ n ", <nl> phy_data -> port_mode , phy_data -> conn_type ); <nl> + return - EINVAL ; <nl> } <nl>  <nl> /* Validate the mode requested */
static void binder_send_failed_reply ( struct binder_transaction * t , <nl> if ( target_thread -> return_error == BR_OK ) { <nl> binder_debug ( BINDER_DEBUG_FAILED_TRANSACTION , <nl> " send failed reply for transaction % d to % d :% d \ n ", <nl> - t -> debug_id , target_thread -> proc -> pid , <nl> + t -> debug_id , <nl> + target_thread -> proc -> pid , <nl> target_thread -> pid ); <nl>  <nl> binder_pop_transaction ( target_thread , t );
static int qat_hal_init_esram ( struct icp_qat_fw_loader_handle * handle ) <nl> unsigned int csr_val ; <nl> int times = 30 ; <nl>  <nl> + if ( handle -> pci_dev -> device == ADF_C3XXX_PCI_DEVICE_ID ) <nl> + return 0 ; <nl> + <nl> csr_val = ADF_CSR_RD ( csr_addr , 0 ); <nl> if (( csr_val & ESRAM_AUTO_TINIT ) && ( csr_val & ESRAM_AUTO_TINIT_DONE )) <nl> return 0 ;
static int safexcel_probe ( struct platform_device * pdev ) <nl> snprintf ( irq_name , 6 , " ring % d ", i ); <nl> irq = safexcel_request_ring_irq ( pdev , irq_name , safexcel_irq_ring , <nl> ring_irq ); <nl> - <nl> - if ( irq < 0 ) <nl> + if ( irq < 0 ) { <nl> + ret = irq ; <nl> goto err_clk ; <nl> + } <nl>  <nl> priv -> ring [ i ]. work_data . priv = priv ; <nl> priv -> ring [ i ]. work_data . ring = i ;
static u32 apic_get_tmcct ( struct kvm_lapic * apic ) <nl> ASSERT ( apic != NULL ); <nl>  <nl> /* if initial count is 0 , current count should also be 0 */ <nl> - if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 ) <nl> + if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 || <nl> + apic -> lapic_timer . period == 0 ) <nl> return 0 ; <nl>  <nl> remaining = hrtimer_get_remaining (& apic -> lapic_timer . timer );
int kvm_arch_vcpu_init ( struct kvm_vcpu * vcpu ) <nl> } <nl> vcpu -> arch . mcg_cap = KVM_MAX_MCE_BANKS ; <nl>  <nl> - if (! zalloc_cpumask_var (& vcpu -> arch . wbinvd_dirty_mask , GFP_KERNEL )) <nl> + if (! zalloc_cpumask_var (& vcpu -> arch . wbinvd_dirty_mask , GFP_KERNEL )) { <nl> + r = - ENOMEM ; <nl> goto fail_free_mce_banks ; <nl> + } <nl>  <nl> r = fx_init ( vcpu ); <nl> if ( r )
static void walk_linearmapping ( struct pg_state * st ) <nl> unsigned long psize = 1 << mmu_psize_defs [ mmu_linear_psize ]. shift ; <nl>  <nl> for ( addr = PAGE_OFFSET ; addr < PAGE_OFFSET + <nl> - memblock_phys_mem_size (); addr += psize ) <nl> + memblock_end_of_DRAM (); addr += psize ) <nl> hpte_find ( st , addr , mmu_linear_psize ); <nl> } <nl> 
bool wil_fw_verify_file_exists ( struct wil6210_priv * wil , const char * name ) <nl> rc = request_firmware (& fw , name , wil_to_dev ( wil )); <nl> if (! rc ) <nl> release_firmware ( fw ); <nl> - return rc != - ENOENT ; <nl> + else <nl> + wil_dbg_fw ( wil , "<% s > not available : % d \ n ", name , rc ); <nl> + return ! rc ; <nl> }
static int sun8i_vi_layer_atomic_check ( struct drm_plane * plane , <nl> clip . x2 = crtc_state -> adjusted_mode . hdisplay ; <nl> clip . y2 = crtc_state -> adjusted_mode . vdisplay ; <nl>  <nl> + min_scale = DRM_PLANE_HELPER_NO_SCALING ; <nl> + max_scale = DRM_PLANE_HELPER_NO_SCALING ; <nl> + <nl> if ( layer -> mixer -> cfg -> scaler_mask & BIT ( layer -> channel )) { <nl> min_scale = SUN8I_VI_SCALER_SCALE_MIN ; <nl> max_scale = SUN8I_VI_SCALER_SCALE_MAX ;
static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
int blk_mq_alloc_tag_set ( struct blk_mq_tag_set * set ) <nl> return - EINVAL ; <nl>  <nl>  <nl> - set -> tags = kmalloc_node ( set -> nr_hw_queues * sizeof ( struct blk_mq_tags ), <nl> + set -> tags = kmalloc_node ( set -> nr_hw_queues * <nl> + sizeof ( struct blk_mq_tags *), <nl> GFP_KERNEL , set -> numa_node ); <nl> if (! set -> tags ) <nl> goto out ;
static long ppc_set_hwdebug ( struct task_struct * child , <nl>  <nl> brk . address = bp_info -> addr & ~ 7UL ; <nl> brk . type = HW_BRK_TYPE_TRANSLATE ; <nl> + brk . len = 8 ; <nl> if ( bp_info -> trigger_type & PPC_BREAKPOINT_TRIGGER_READ ) <nl> brk . type |= HW_BRK_TYPE_READ ; <nl> if ( bp_info -> trigger_type & PPC_BREAKPOINT_TRIGGER_WRITE )
static int davinci_pcm_open ( struct snd_pcm_substream * substream ) <nl> int ret = 0 ; <nl>  <nl> snd_soc_set_runtime_hwparams ( substream , & davinci_pcm_hardware ); <nl> + /* ensure that buffer size is a multiple of period size */ <nl> + ret = snd_pcm_hw_constraint_integer ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> prtd = kzalloc ( sizeof ( struct davinci_runtime_data ), GFP_KERNEL ); <nl> if ( prtd == NULL )
static int coda_try_fmt ( struct coda_ctx * ctx , struct coda_codec * codec , <nl> BUG (); <nl> } <nl>  <nl> + f -> fmt . pix . priv = 0 ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int create_in_format_blob ( struct drm_device * dev , struct drm_plane * plane <nl> plane -> format_types [ j ], <nl> plane -> modifiers [ i ])) { <nl>  <nl> - mod -> formats |= 1 << j ; <nl> + mod -> formats |= 1ULL << j ; <nl> } <nl> } <nl> 
static int gbcodec_trigger ( struct snd_pcm_substream * substream , int cmd , <nl> dev_err ( dai -> dev , "% d : Error during % s stream \ n ", ret , <nl> start ? " Start " : " Stop "); <nl>  <nl> + /* in case device removed , return 0 for stop trigger */ <nl> + if ( stop && ( ret == - ENODEV )) <nl> + ret = 0 ; <nl> + <nl> func_exit : <nl> mutex_unlock (& gb -> lock ); <nl> return ret ;
static int picolcd_raw_event ( struct hid_device * hdev , <nl> if (! data ) <nl> return 1 ; <nl>  <nl> + if ( size > 64 ) { <nl> + hid_warn ( hdev , " invalid size value (% d ) for picolcd raw event \ n ", <nl> + size ); <nl> + return 0 ; <nl> + } <nl> + <nl> if ( report -> id == REPORT_KEY_STATE ) { <nl> if ( data -> input_keys ) <nl> ret = picolcd_raw_keypad ( data , report , raw_data + 1 , size - 1 );
static int omap_pcm_open ( struct snd_pcm_substream * substream ) <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> - prtd = kzalloc ( sizeof ( prtd ), GFP_KERNEL ); <nl> + prtd = kzalloc ( sizeof (* prtd ), GFP_KERNEL ); <nl> if ( prtd == NULL ) { <nl> ret = - ENOMEM ; <nl> goto out ;
+// SPDX - License - Identifier : GPL - 2 . 0 <nl> /* <nl> * Copyright ( C ) Maxime Coquelin 2015 <nl> + * Copyright ( C ) STMicroelectronics 2017 <nl> * Author : Maxime Coquelin < mcoquelin . stm32 @ gmail . com > <nl> - * License terms : GNU General Public License ( GPL ), version 2 <nl> */ <nl>  <nl> # include < linux / kernel . h >
int batadv_recv_unicast_packet ( struct sk_buff * skb , <nl> batadv_dbg ( BATADV_DBG_BLA , bat_priv , <nl> " recv_unicast_packet (): Dropped unicast pkt received from another backbone gw % pM .\ n ", <nl> orig_addr_gw ); <nl> - return NET_RX_DROP ; <nl> + goto free_skb ; <nl> } <nl> } <nl> 
EXPORT_SYMBOL_GPL ( crypto_givcipher_type ); <nl>  <nl> const char * crypto_default_geniv ( const struct crypto_alg * alg ) <nl> { <nl> + if ((( alg -> cra_flags & CRYPTO_ALG_TYPE_MASK ) == <nl> + CRYPTO_ALG_TYPE_BLKCIPHER ? alg -> cra_blkcipher . ivsize : <nl> + alg -> cra_ablkcipher . ivsize ) != <nl> + alg -> cra_blocksize ) <nl> + return " chainiv "; <nl> + <nl> return alg -> cra_flags & CRYPTO_ALG_ASYNC ? <nl> " eseqiv " : skcipher_default_geniv ; <nl> }
int ___ieee80211_stop_tx_ba_session ( struct sta_info * sta , u16 tid , <nl>  <nl> spin_lock_bh (& sta -> lock ); <nl>  <nl> + /* free struct pending for start , if present */ <nl> + tid_tx = sta -> ampdu_mlme . tid_start_tx [ tid ]; <nl> + kfree ( tid_tx ); <nl> + sta -> ampdu_mlme . tid_start_tx [ tid ] = NULL ; <nl> + <nl> tid_tx = rcu_dereference_protected_tid_tx ( sta , tid ); <nl> if (! tid_tx ) { <nl> spin_unlock_bh (& sta -> lock );
static int bnxt_get_nvram_item ( struct net_device * dev , u32 index , u32 offset , <nl> dma_addr_t dma_handle ; <nl> struct hwrm_nvm_read_input req = { 0 }; <nl>  <nl> + if (! length ) <nl> + return - EINVAL ; <nl> + <nl> buf = dma_alloc_coherent (& bp -> pdev -> dev , length , & dma_handle , <nl> GFP_KERNEL ); <nl> if (! buf ) {
static __devexit int wm831x_power_remove ( struct platform_device * pdev ) <nl> power_supply_unregister (& wm831x_power -> battery ); <nl> power_supply_unregister (& wm831x_power -> wall ); <nl> power_supply_unregister (& wm831x_power -> usb ); <nl> + kfree ( wm831x_power ); <nl> return 0 ; <nl> } <nl> 
int virtio_gpu_object_create ( struct virtio_gpu_device * vgdev , <nl> return - ENOMEM ; <nl> size = roundup ( size , PAGE_SIZE ); <nl> ret = drm_gem_object_init ( vgdev -> ddev , & bo -> gem_base , size ); <nl> - if ( ret != 0 ) <nl> + if ( ret != 0 ) { <nl> + kfree ( bo ); <nl> return ret ; <nl> + } <nl> bo -> dumb = false ; <nl> virtio_gpu_init_ttm_placement ( bo , pinned ); <nl> 
iwl_parse_nvm_data ( struct device * dev , const struct iwl_cfg * cfg , <nl> if (! nvm_calib ) { <nl> IWL_ERR_DEV ( dev , <nl> " Can ' t parse empty Calib NVM sections \ n "); <nl> + kfree ( data ); <nl> return NULL ; <nl> } <nl> /* in family 8000 Xtal calibration values moved to OTP */
brcmf_cfg80211_add_key ( struct wiphy * wiphy , struct net_device * ndev , <nl> if (! check_vif_up ( ifp -> vif )) <nl> return - EIO ; <nl>  <nl> - if ( mac_addr ) { <nl> + if ( mac_addr && <nl> + ( params -> cipher != WLAN_CIPHER_SUITE_WEP40 ) && <nl> + ( params -> cipher != WLAN_CIPHER_SUITE_WEP104 )) { <nl> brcmf_dbg ( TRACE , " Exit "); <nl> return brcmf_add_keyext ( wiphy , ndev , key_idx , mac_addr , params ); <nl> }
int i915_gem_freeze_late ( struct drm_i915_private * dev_priv ) <nl> */ <nl>  <nl> i915_gem_shrink ( dev_priv , - 1UL , I915_SHRINK_UNBOUND ); <nl> + i915_gem_drain_freed_objects ( dev_priv ); <nl>  <nl> mutex_lock (& dev_priv -> drm . struct_mutex ); <nl> for ( p = phases ; * p ; p ++) {
snd_nm256_mixer ( struct nm256 * chip ) <nl> . read = snd_nm256_ac97_read , <nl> }; <nl>  <nl> - chip -> ac97_regs = kcalloc ( sizeof ( short ), <nl> - ARRAY_SIZE ( nm256_ac97_init_val ), GFP_KERNEL ); <nl> + chip -> ac97_regs = kcalloc ( ARRAY_SIZE ( nm256_ac97_init_val ), <nl> + sizeof ( short ), GFP_KERNEL ); <nl> if (! chip -> ac97_regs ) <nl> return - ENOMEM ; <nl> 
static int pfkey_register ( struct sock * sk , struct sk_buff * skb , const struct sad <nl> pfk -> registered |= ( 1 << hdr -> sadb_msg_satype ); <nl> } <nl>  <nl> + mutex_lock (& pfkey_mutex ); <nl> xfrm_probe_algs (); <nl>  <nl> supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL | __GFP_ZERO ); <nl> + mutex_unlock (& pfkey_mutex ); <nl> + <nl> if (! supp_skb ) { <nl> if ( hdr -> sadb_msg_satype != SADB_SATYPE_UNSPEC ) <nl> pfk -> registered &= ~( 1 << hdr -> sadb_msg_satype );
ath10k_wmi_10_4_gen_update_fw_tdls_state ( struct ath10k * ar , u32 vdev_id , <nl> if (! skb ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> - if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map )) <nl> + if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map ) && <nl> + state == WMI_TDLS_ENABLE_ACTIVE ) <nl> state = WMI_TDLS_ENABLE_PASSIVE ; <nl>  <nl> if ( test_bit ( WMI_SERVICE_TDLS_UAPSD_BUFFER_STA , ar -> wmi . svc_map ))
static int max30102_probe ( struct i2c_client * client , <nl> dev_err (& client -> dev , " regmap initialization failed \ n "); <nl> return PTR_ERR ( data -> regmap ); <nl> } <nl> - max30102_set_powermode ( data , false ); <nl> + <nl> + ret = max30102_set_powermode ( data , false ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = max30102_chip_init ( data ); <nl> if ( ret )
static int parse_features ( struct dm_arg_set * as , struct flakey_c * fc , <nl> arg_name = dm_shift_arg ( as ); <nl> argc --; <nl>  <nl> + if (! arg_name ) { <nl> + ti -> error = " Insufficient feature arguments "; <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* <nl> * drop_writes <nl> */
int blk_init_allocated_queue ( struct request_queue * q ) <nl> q -> exit_rq_fn ( q , q -> fq -> flush_rq ); <nl> out_free_flush_queue : <nl> blk_free_flush_queue ( q -> fq ); <nl> + q -> fq = NULL ; <nl> return - ENOMEM ; <nl> } <nl> EXPORT_SYMBOL ( blk_init_allocated_queue );
void sctp_association_free ( struct sctp_association * asoc ) <nl> /* Only real associations count against the endpoint , so <nl> * don ' t bother for if this is a temporary association . <nl> */ <nl> - if (! asoc -> temp ) { <nl> + if (! list_empty (& asoc -> asocs )) { <nl> list_del (& asoc -> asocs ); <nl>  <nl> /* Decrement the backlog value for a TCP - style listening
static int visor_thread_start ( struct visor_thread_info * thrinfo , <nl> void * thrcontext , char * name ) <nl> { <nl> /* used to stop the thread */ <nl> - thrinfo -> task = kthread_run ( threadfn , thrcontext , name ); <nl> + thrinfo -> task = kthread_run ( threadfn , thrcontext , "% s ", name ); <nl> if ( IS_ERR ( thrinfo -> task )) { <nl> pr_debug ("% s failed (% ld )\ n ", <nl> __func__ , PTR_ERR ( thrinfo -> task ));
xfs_error_get_cfg ( <nl> { <nl> struct xfs_error_cfg * cfg ; <nl>  <nl> + if ( error < 0 ) <nl> + error = - error ; <nl> + <nl> switch ( error ) { <nl> case EIO : <nl> cfg = & mp -> m_error_cfg [ error_class ][ XFS_ERR_EIO ];
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> return - EBUSY ; <nl> } <nl>  <nl> + if ( bond_dev == slave_dev ) { <nl> + pr_err ("% s : cannot enslave bond to itself .\ n ", bond_dev -> name ); <nl> + return - EPERM ; <nl> + } <nl> + <nl> /* vlan challenged mutual exclusion */ <nl> /* no need to lock since we ' re protected by rtnl_lock */ <nl> if ( slave_dev -> features & NETIF_F_VLAN_CHALLENGED ) {
int cpuidle_enter_state ( struct cpuidle_device * dev , struct cpuidle_driver * drv , <nl>  <nl> time_end = ktime_get (); <nl>  <nl> - local_irq_enable (); <nl> + if (! cpuidle_state_is_coupled ( dev , drv , entered_state )) <nl> + local_irq_enable (); <nl>  <nl> diff = ktime_to_us ( ktime_sub ( time_end , time_start )); <nl> if ( diff > INT_MAX )
static int spi_ppc4xx_of_probe ( struct platform_device * op ) <nl> if ( num_gpios > 0 ) { <nl> int i ; <nl>  <nl> - hw -> gpios = kzalloc ( sizeof ( int ) * num_gpios , GFP_KERNEL ); <nl> + hw -> gpios = kcalloc ( num_gpios , sizeof (* hw -> gpios ), GFP_KERNEL ); <nl> if (! hw -> gpios ) { <nl> ret = - ENOMEM ; <nl> goto free_master ;
static void test_acipher_speed ( const char * algo , int enc , unsigned int secs , <nl> goto out_free_req ; <nl> } <nl>  <nl> - sg_init_table ( sg , TVMEMSIZE ); <nl> - <nl> k = * keysize + * b_size ; <nl> + sg_init_table ( sg , DIV_ROUND_UP ( k , PAGE_SIZE )); <nl> + <nl> if ( k > PAGE_SIZE ) { <nl> sg_set_buf ( sg , tvmem [ 0 ] + * keysize , <nl> PAGE_SIZE - * keysize );
static bool fib6_rule_suppress ( struct fib_rule * rule , struct fib_lookup_arg * arg <nl> return false ; <nl>  <nl> suppress_route : <nl> - ip6_rt_put ( rt ); <nl> + if (!( arg -> flags & FIB_LOOKUP_NOREF )) <nl> + ip6_rt_put ( rt ); <nl> return true ; <nl> } <nl> 
static int deprecated_sysctl_warning ( struct __sysctl_args * args ) <nl> int name [ CTL_MAXNAME ]; <nl> int i ; <nl>  <nl> + /* Check args -> nlen . */ <nl> + if ( args -> nlen < 0 || args -> nlen > CTL_MAXNAME ) <nl> + return - ENOTDIR ; <nl> + <nl> /* Read in the sysctl name for better debug message logging */ <nl> for ( i = 0 ; i < args -> nlen ; i ++) <nl> if ( get_user ( name [ i ], args -> name + i ))
static struct domain_device * sas_ex_discover_expander ( <nl>  <nl> res = sas_discover_expander ( child ); <nl> if ( res ) { <nl> + spin_lock_irq (& parent -> port -> dev_list_lock ); <nl> + list_del (& child -> dev_list_node ); <nl> + spin_unlock_irq (& parent -> port -> dev_list_lock ); <nl> kfree ( child ); <nl> return NULL ; <nl> }
static const struct ieee80211_channel ath10k_5ghz_channels [] = { <nl> CHAN5G ( 132 , 5660 , 0 ), <nl> CHAN5G ( 136 , 5680 , 0 ), <nl> CHAN5G ( 140 , 5700 , 0 ), <nl> + CHAN5G ( 144 , 5720 , 0 ), <nl> CHAN5G ( 149 , 5745 , 0 ), <nl> CHAN5G ( 153 , 5765 , 0 ), <nl> CHAN5G ( 157 , 5785 , 0 ),
extern long __put_user_asm_l ( void *, long ); <nl> extern long __put_user_asm_q ( void *, long ); <nl> extern void __put_user_unknown ( void ); <nl>  <nl> + extern long __strnlen_user ( const char * __s , long __n ); <nl> + <nl> # endif /* __ASM_SH_UACCESS_64_H */
static void __init sun6i_rtc_clk_init ( struct device_node * node ) <nl>  <nl> clk_data = kzalloc ( sizeof (* clk_data ) + ( sizeof (* clk_data -> hws ) * 2 ), <nl> GFP_KERNEL ); <nl> - if (! clk_data ) <nl> + if (! clk_data ) { <nl> + kfree ( rtc ); <nl> return ; <nl> + } <nl>  <nl> spin_lock_init (& rtc -> lock ); <nl> 
static long tce_iommu_register_pages ( struct tce_container * container , <nl> return ret ; <nl>  <nl> tcemem = kzalloc ( sizeof (* tcemem ), GFP_KERNEL ); <nl> + if (! tcemem ) { <nl> + mm_iommu_put ( container -> mm , mem ); <nl> + return - ENOMEM ; <nl> + } <nl> + <nl> tcemem -> mem = mem ; <nl> list_add (& tcemem -> next , & container -> prereg_list ); <nl> 
static void zfcp_fc_adisc_handler ( unsigned long data ) <nl> struct zfcp_port * port = adisc -> els . port ; <nl> struct zfcp_ls_adisc_acc * ls_adisc = & adisc -> ls_adisc_acc ; <nl>  <nl> - if (! adisc -> els . status ) { <nl> + if ( adisc -> els . status ) { <nl> /* request rejected or timed out */ <nl> zfcp_erp_port_forced_reopen ( port , 0 , 63 , NULL ); <nl> goto out ;
int journal_create ( journal_t * journal ) <nl> if ( err ) <nl> return err ; <nl> bh = __getblk ( journal -> j_dev , blocknr , journal -> j_blocksize ); <nl> + if ( unlikely (! bh )) <nl> + return - ENOMEM ; <nl> lock_buffer ( bh ); <nl> memset ( bh -> b_data , 0 , journal -> j_blocksize ); <nl> BUFFER_TRACE ( bh , " marking dirty ");
static int sti_cpufreq_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if ((! of_machine_is_compatible (" st , stih407 ")) && <nl> + (! of_machine_is_compatible (" st , stih410 "))) <nl> + return - ENODEV ; <nl> + <nl> ddata . cpu = get_cpu_device ( 0 ); <nl> if (! ddata . cpu ) { <nl> dev_err ( ddata . cpu , " Failed to get device for CPU0 \ n ");
static void pch_gpio_setup ( struct pch_gpio * chip ) <nl> static int pch_irq_type ( struct irq_data * d , unsigned int type ) <nl> { <nl> u32 im ; <nl> - u32 * im_reg ; <nl> + u32 __iomem * im_reg ; <nl> u32 ien ; <nl> u32 im_pos ; <nl> int ch ;
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> __le16 fc ; <nl> struct ieee80211_hdr hdr ; <nl> struct ieee80211s_hdr mesh_hdr __maybe_unused ; <nl> - struct mesh_path * mppath = NULL ; <nl> + struct mesh_path __maybe_unused * mppath = NULL ; <nl> const u8 * encaps_data ; <nl> int encaps_len , skip_header_bytes ; <nl> int nh_pos , h_pos ;
static struct glink_channel * qcom_glink_alloc_channel ( struct qcom_glink * glink , <nl>  <nl> init_completion (& channel -> open_req ); <nl> init_completion (& channel -> open_ack ); <nl> + init_completion (& channel -> intent_req_comp ); <nl>  <nl> INIT_LIST_HEAD (& channel -> done_intents ); <nl> INIT_WORK (& channel -> intent_work , qcom_glink_rx_done_work );
static int intel_pt_walk_to_ip ( struct intel_pt_decoder * decoder ) <nl> break ; <nl>  <nl> case INTEL_PT_PSB : <nl> + intel_pt_clear_stack (& decoder -> stack ); <nl> err = intel_pt_walk_psb ( decoder ); <nl> if ( err ) <nl> return err ;
subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> err = filter_add_subsystem_pred ( system , pred ); <nl> if ( err < 0 ) { <nl> - filter_free_subsystem_preds ( system ); <nl> filter_free_pred ( pred ); <nl> return err ; <nl> }
static int sharpsl_nand_probe ( struct platform_device * pdev ) <nl> /* Register the partitions */ <nl> mtd -> name = " sharpsl - nand "; <nl>  <nl> - err = mtd_device_parse_register ( mtd , NULL , NULL , <nl> + err = mtd_device_parse_register ( mtd , data -> part_parsers , NULL , <nl> data -> partitions , data -> nr_partitions ); <nl> if ( err ) <nl> goto err_add ;
int i915_gem_init_stolen ( struct drm_i915_private * dev_priv ) <nl>  <nl> mutex_init (& dev_priv -> mm . stolen_lock ); <nl>  <nl> + if ( intel_vgpu_active ( dev_priv )) { <nl> + DRM_INFO (" iGVT - g active , disabling use of stolen memory \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> # ifdef CONFIG_INTEL_IOMMU <nl> if ( intel_iommu_gfx_mapped && INTEL_GEN ( dev_priv ) < 8 ) { <nl> DRM_INFO (" DMAR active , disabling use of stolen memory \ n ");
void handle_ra_miss ( struct address_space * mapping , <nl> { <nl> ra -> flags |= RA_FLAG_MISS ; <nl> ra -> flags &= ~ RA_FLAG_INCACHE ; <nl> + ra -> cache_hit = 0 ; <nl> } <nl>  <nl> /*
static int __devinit snd_miro_probe ( struct snd_card * card ) <nl>  <nl> error = snd_card_miro_aci_detect ( card , miro ); <nl> if ( error < 0 ) { <nl> - snd_card_free ( card ); <nl> snd_printk ( KERN_ERR " unable to detect aci chip \ n "); <nl> return - ENODEV ; <nl> }
# define DRIVER_AUTHOR " Maksim Salau < maksim . salau @ gmail . com >" <nl>  <nl> static const struct usb_device_id id_table [] = { <nl> - { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x0409 , 0x0063 ) }, /* V850ESJX3 - STICK */ <nl> + { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x064B , 0x7825 ) }, /* Analog Devices EVAL - ADXL362Z - DB */ <nl> {} <nl> };
static int cuse_channel_release ( struct inode * inode , struct file * file ) <nl> unregister_chrdev_region ( cc -> cdev -> dev , 1 ); <nl> cdev_del ( cc -> cdev ); <nl> } <nl> + /* Base reference is now owned by " fud " */ <nl> + fuse_conn_put (& cc -> fc ); <nl>  <nl> rc = fuse_dev_release ( inode , file ); /* puts the base reference */ <nl> 
struct sta_info * ieee80211_ibss_add_sta ( struct ieee80211_sub_if_data * sdata , <nl> if (! sta ) <nl> return NULL ; <nl>  <nl> + sta -> last_rx = jiffies ; <nl> set_sta_flags ( sta , WLAN_STA_AUTHORIZED ); <nl>  <nl> /* make sure mandatory rates are always added */
static int fsi_resume ( struct device * dev ) <nl> return 0 ; <nl> } <nl>  <nl> - static struct dev_pm_ops fsi_pm_ops = { <nl> + static const struct dev_pm_ops fsi_pm_ops = { <nl> . suspend = fsi_suspend , <nl> . resume = fsi_resume , <nl> };
static int spi_map_buf ( struct spi_master * master , struct device * dev , <nl> } <nl>  <nl> ret = dma_map_sg ( dev , sgt -> sgl , sgt -> nents , dir ); <nl> + if (! ret ) <nl> + ret = - ENOMEM ; <nl> if ( ret < 0 ) { <nl> sg_free_table ( sgt ); <nl> return ret ;
int btrfs_search_slot_for_read ( struct btrfs_root * root , <nl> if ( ret < 0 ) <nl> return ret ; <nl> if (! ret ) { <nl> - p -> slots [ 0 ] = btrfs_header_nritems ( leaf ) - 1 ; <nl> + leaf = p -> nodes [ 0 ]; <nl> + if ( p -> slots [ 0 ] == btrfs_header_nritems ( leaf )) <nl> + p -> slots [ 0 ]--; <nl> return 0 ; <nl> } <nl> if (! return_any )
static int ext4_ext_journal_restart ( handle_t * handle , int needed ) <nl> if ( handle -> h_buffer_credits > needed ) <nl> return 0 ; <nl> err = ext4_journal_extend ( handle , needed ); <nl> - if ( err ) <nl> + if ( err <= 0 ) <nl> return err ; <nl> return ext4_journal_restart ( handle , needed ); <nl> }
static int btrfs_add_system_chunk ( struct btrfs_root * root , <nl> u8 * ptr ; <nl>  <nl> array_size = btrfs_super_sys_array_size ( super_copy ); <nl> - if ( array_size + item_size > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> + if ( array_size + item_size + sizeof ( disk_key ) <nl> + > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> return - EFBIG ; <nl>  <nl> ptr = super_copy -> sys_chunk_array + array_size ;
static int tomoyo_mount_acl ( struct tomoyo_request_info * r , char * dev_name , <nl> } <nl> if ( need_dev ) { <nl> /* Get mount point or device file . */ <nl> - if ( kern_path ( dev_name , LOOKUP_FOLLOW , & path )) { <nl> + if (! dev_name || kern_path ( dev_name , LOOKUP_FOLLOW , & path )) { <nl> error = - ENOENT ; <nl> goto out ; <nl> }
static const char * alc_get_line_out_pfx ( const struct auto_pin_cfg * cfg , <nl>  <nl> switch ( cfg -> line_out_type ) { <nl> case AUTO_PIN_SPEAKER_OUT : <nl> - return " Speaker "; <nl> + if ( cfg -> line_outs == 1 ) <nl> + return " Speaker "; <nl> + break ; <nl> case AUTO_PIN_HP_OUT : <nl> return " Headphone "; <nl> default :
struct extent_map * btrfs_get_extent ( struct inode * inode , struct page * page , <nl> goto not_found ; <nl> if ( start + len <= found_key . offset ) <nl> goto not_found ; <nl> + if ( start > found_key . offset ) <nl> + goto next ; <nl> em -> start = start ; <nl> em -> orig_start = start ; <nl> em -> len = found_key . offset - start ;
static ssize_t iwl_dbgfs_fw_rx_stats_read ( struct file * file , <nl>  <nl> mutex_lock (& mvm -> mutex ); <nl>  <nl> + if ( iwl_mvm_firmware_running ( mvm )) <nl> + iwl_mvm_request_statistics ( mvm , false ); <nl> + <nl> pos += scnprintf ( buf + pos , bufsz - pos , fmt_header , <nl> " Statistics_Rx - OFDM "); <nl> if (! iwl_mvm_has_new_rx_stats_api ( mvm )) {
static struct drm_driver tegra_drm_driver = { <nl> . debugfs_cleanup = tegra_debugfs_cleanup , <nl> # endif <nl>  <nl> - . gem_free_object = tegra_bo_free_object , <nl> + . gem_free_object_unlocked = tegra_bo_free_object , <nl> . gem_vm_ops = & tegra_bo_vm_ops , <nl>  <nl> . prime_handle_to_fd = drm_gem_prime_handle_to_fd ,
enum { none , prepare , done , } __init_state ; <nl> static void init_preload ( void ); <nl> static void try_init_preload ( void ) <nl> { <nl> - if (! __init_state != done ) <nl> + if ( __init_state != done ) <nl> init_preload (); <nl> } <nl> 
static int xfrm6_tunnel_rcv ( struct sk_buff * skb ) <nl> __be32 spi ; <nl>  <nl> spi = xfrm6_tunnel_spi_lookup (( xfrm_address_t *)& iph -> saddr ); <nl> - return xfrm6_rcv_spi ( skb , spi ); <nl> + return xfrm6_rcv_spi ( skb , spi ) > 0 ? : 0 ; <nl> } <nl>  <nl> static int xfrm6_tunnel_err ( struct sk_buff * skb , struct inet6_skb_parm * opt ,
int snd_hda_multi_out_analog_open ( struct hda_codec * codec , <nl> if ( mout -> spdif_maxbps < hinfo -> maxbps ) <nl> hinfo -> maxbps = mout -> spdif_maxbps ; <nl> } <nl> + mutex_unlock (& codec -> spdif_mutex ); <nl> } <nl> - mutex_unlock (& codec -> spdif_mutex ); <nl> return snd_pcm_hw_constraint_step ( substream -> runtime , 0 , <nl> SNDRV_PCM_HW_PARAM_CHANNELS , 2 ); <nl> }
int jbd2__journal_restart ( handle_t * handle , int nblocks , gfp_t gfp_mask ) <nl>  <nl> rwsem_release (& journal -> j_trans_commit_map , 1 , _THIS_IP_ ); <nl> handle -> h_buffer_credits = nblocks ; <nl> + /* <nl> + * Restore the original nofs context because the journal restart <nl> + * is basically the same thing as journal stop and start . <nl> + * start_this_handle will start a new nofs context . <nl> + */ <nl> + memalloc_nofs_restore ( handle -> saved_alloc_context ); <nl> ret = start_this_handle ( journal , handle , gfp_mask ); <nl> return ret ; <nl> }
static int futex_requeue ( u32 __user * uaddr1 , unsigned int flags , <nl> struct futex_q * this , * next ; <nl> DEFINE_WAKE_Q ( wake_q ); <nl>  <nl> + if ( nr_wake < 0 || nr_requeue < 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * When PI not supported : return - ENOSYS if requeue_pi is true , <nl> * consequently the compiler knows requeue_pi is always false past
static int snd_soc_dapm_set_bias_level ( struct snd_soc_dapm_context * dapm , <nl> if ( dapm -> codec -> driver -> set_bias_level ) <nl> ret = dapm -> codec -> driver -> set_bias_level ( dapm -> codec , <nl> level ); <nl> - else <nl> - dapm -> bias_level = level ; <nl> - } <nl> + } else <nl> + dapm -> bias_level = level ; <nl> + <nl> if ( ret != 0 ) <nl> goto out ; <nl> 
static int ipw_load ( struct ipw_priv * priv ) <nl> ipw_rx_queue_reset ( priv , priv -> rxq ); <nl> if (! priv -> rxq ) { <nl> IPW_ERROR (" Unable to initialize Rx queue \ n "); <nl> + rc = - ENOMEM ; <nl> goto error ; <nl> } <nl> 
static void __init kvm_guest_init ( void ) <nl> } <nl>  <nl> if ( kvm_para_has_feature ( KVM_FEATURE_PV_TLB_FLUSH ) && <nl> + ! kvm_para_has_hint ( KVM_HINTS_DEDICATED ) && <nl> ! kvm_para_has_feature ( KVM_FEATURE_STEAL_TIME )) <nl> pv_mmu_ops . flush_tlb_others = kvm_flush_tlb_others ; <nl>  <nl> static __init int kvm_setup_pv_tlb_flush ( void ) <nl> int cpu ; <nl>  <nl> if ( kvm_para_has_feature ( KVM_FEATURE_PV_TLB_FLUSH ) && <nl> + ! kvm_para_has_hint ( KVM_HINTS_DEDICATED ) && <nl> ! kvm_para_has_feature ( KVM_FEATURE_STEAL_TIME )) { <nl> for_each_possible_cpu ( cpu ) { <nl> zalloc_cpumask_var_node ( per_cpu_ptr (& __pv_tlb_mask , cpu ),
aoenet_xmit ( struct sk_buff_head * queue ) <nl> { <nl> struct sk_buff * skb , * tmp ; <nl>  <nl> - skb_queue_walk_safe ( queue , skb , tmp ) <nl> + skb_queue_walk_safe ( queue , skb , tmp ) { <nl> + __skb_unlink ( skb , queue ); <nl> dev_queue_xmit ( skb ); <nl> + } <nl> } <nl>  <nl> /*
static int fuse_fill_super ( struct super_block * sb , void * data , int silent ) <nl> err_put_root : <nl> dput ( root_dentry ); <nl> err_put_conn : <nl> + bdi_destroy (& fc -> bdi ); <nl> fuse_conn_put ( fc ); <nl> err_fput : <nl> fput ( file );
static int adv7511_get_modes ( struct adv7511 * adv7511 , <nl> adv7511_set_config_csc ( adv7511 , connector , adv7511 -> rgb , <nl> drm_detect_hdmi_monitor ( edid )); <nl>  <nl> - kfree ( edid ); <nl> - <nl> cec_s_phys_addr_from_edid ( adv7511 -> cec_adap , edid ); <nl>  <nl> + kfree ( edid ); <nl> + <nl> return count ; <nl> } <nl> 
static u16 ar9003_hw_get_max_edge_power ( struct ar9300_eeprom * eep , <nl> break ; <nl> } <nl> } <nl> + <nl> + if ( is2GHz && ! twiceMaxEdgePower ) <nl> + twiceMaxEdgePower = 60 ; <nl> + <nl> return twiceMaxEdgePower ; <nl> } <nl> 
static int rockchip_i2s_remove ( struct platform_device * pdev ) <nl> if (! pm_runtime_status_suspended (& pdev -> dev )) <nl> i2s_runtime_suspend (& pdev -> dev ); <nl>  <nl> - clk_disable_unprepare ( i2s -> mclk ); <nl> clk_disable_unprepare ( i2s -> hclk ); <nl>  <nl> return 0 ;
static void * alloc_qos_entry ( void ) <nl> } <nl> spin_unlock_irqrestore (& qos_free_list . lock , flags ); <nl>  <nl> - entry = kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> - return entry ; <nl> + return kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> } <nl>  <nl> static void free_qos_entry ( void * entry )
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> + . mmu = gf100_mmu_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new , <nl> . top = gk104_top_new ,
int ll_fid2path ( struct inode * inode , void __user * arg ) <nl> if ( get_user ( pathlen , & gfin -> gf_pathlen )) <nl> return - EFAULT ; <nl>  <nl> + if ( pathlen > PATH_MAX ) <nl> + return - EINVAL ; <nl> + <nl> outsize = sizeof (* gfout ) + pathlen ; <nl>  <nl> OBD_ALLOC ( gfout , outsize );
static int cxgb4vf_open ( struct net_device * dev ) <nl> if ( err ) <nl> return err ; <nl> set_bit ( pi -> port_id , & adapter -> open_device_map ); <nl> - link_start ( dev ); <nl> + err = link_start ( dev ); <nl> + if ( err ) <nl> + return err ; <nl> netif_tx_start_all_queues ( dev ); <nl> return 0 ; <nl> }
static struct platform_driver mxc_w1_driver = { <nl> . name = " mxc_w1 ", <nl> }, <nl> . probe = mxc_w1_probe , <nl> - . remove = __devexit_p ( mxc_w1_remove ), <nl> + . remove = mxc_w1_remove , <nl> }; <nl> module_platform_driver ( mxc_w1_driver ); <nl> 
static void clear_huge_page ( struct page * page , <nl> { <nl> int i ; <nl>  <nl> - if ( unlikely ( sz > MAX_ORDER_NR_PAGES )) { <nl> + if ( unlikely ( sz / PAGE_SIZE > MAX_ORDER_NR_PAGES )) { <nl> clear_gigantic_page ( page , addr , sz ); <nl> return ; <nl> }
static int ina2xx_probe ( struct i2c_client * client , <nl> data -> config = & ina2xx_config [ data -> kind ]; <nl> data -> client = client ; <nl>  <nl> - if ( data -> rshunt <= 0 ) <nl> + if ( data -> rshunt <= 0 || <nl> + data -> rshunt > data -> config -> calibration_factor ) <nl> return - ENODEV ; <nl>  <nl> ret = ina2xx_init ( data );
static int new_term ( struct parse_events_term ** _term , int type_val , <nl> term -> val . str = str ; <nl> break ; <nl> default : <nl> + free ( term ); <nl> return - EINVAL ; <nl> } <nl> 
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
void rxrpc_discard_prealloc ( struct rxrpc_sock * rx ) <nl> tail = b -> call_backlog_tail ; <nl> while ( CIRC_CNT ( head , tail , size ) > 0 ) { <nl> struct rxrpc_call * call = b -> call_backlog [ tail ]; <nl> + call -> socket = rx ; <nl> if ( rx -> discard_new_call ) { <nl> _debug (" discard % lx ", call -> user_call_ID ); <nl> rx -> discard_new_call ( call , call -> user_call_ID );
int core_kernel_text ( unsigned long addr ) <nl> addr <= ( unsigned long ) _etext ) <nl> return 1 ; <nl>  <nl> - if ( addr >= ( unsigned long ) _sinittext && <nl> + if ( system_state == SYSTEM_BOOTING && <nl> + addr >= ( unsigned long ) _sinittext && <nl> addr <= ( unsigned long ) _einittext ) <nl> return 1 ; <nl> return 0 ;
static void __exit ams_delta_serio_exit ( void ) <nl> free_irq ( OMAP_GPIO_IRQ ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ), 0 ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_DATA ); <nl> - kfree ( ams_delta_serio ); <nl> } <nl> module_exit ( ams_delta_serio_exit );
void snd_timer_interrupt ( struct snd_timer * timer , unsigned long ticks_left ) <nl> } else { <nl> ti -> flags &= ~ SNDRV_TIMER_IFLG_RUNNING ; <nl> if (-- timer -> running ) <nl> - list_del (& ti -> active_list ); <nl> + list_del_init (& ti -> active_list ); <nl> } <nl> if (( timer -> hw . flags & SNDRV_TIMER_HW_TASKLET ) || <nl> ( ti -> flags & SNDRV_TIMER_IFLG_FAST ))
static int ovl_copy_up_locked ( struct dentry * workdir , struct dentry * upperdir , <nl>  <nl> out_cleanup : <nl> ovl_cleanup ( wdir , newdentry ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> /*
int dvb_create_media_graph ( struct dvb_adapter * adap , <nl> return - ENOMEM ; <nl> adap -> conn = conn ; <nl>  <nl> - adap -> conn_pads = kcalloc ( 1 , sizeof (* adap -> conn_pads ), <nl> - GFP_KERNEL ); <nl> + adap -> conn_pads = kzalloc ( sizeof (* adap -> conn_pads ), GFP_KERNEL ); <nl> if (! adap -> conn_pads ) <nl> return - ENOMEM ; <nl> 
int vty_init ( const struct file_operations * console_fops ); <nl>  <nl> static inline bool vt_force_oops_output ( struct vc_data * vc ) <nl> { <nl> - if ( oops_in_progress && vc -> vc_panic_force_write ) <nl> + if ( oops_in_progress && vc -> vc_panic_force_write && panic_timeout >= 0 ) <nl> return true ; <nl> return false ; <nl> }
vnet_select_queue ( struct net_device * dev , struct sk_buff * skb , <nl> struct vnet * vp = netdev_priv ( dev ); <nl> struct vnet_port * port = __tx_port_find ( vp , skb ); <nl>  <nl> + if ( port == NULL ) <nl> + return 0 ; <nl> return port -> q_index ; <nl> } <nl> 
struct thread_struct { <nl> . pgdir = swapper_pg_dir , \ <nl> } <nl>  <nl> -/* Do necessary setup to start up a newly executed thread . */ <nl> - void start_thread ( struct pt_regs * regs , <nl> - unsigned long pc , unsigned long usp ); <nl>  <nl> /* Free all resources held by a thread . */ <nl> extern inline void release_thread ( struct task_struct * dead_task )
void ieee80211_beacon_connection_loss_work ( struct work_struct * work ) <nl> struct sta_info * sta ; <nl>  <nl> if ( ifmgd -> associated ) { <nl> + rcu_read_lock (); <nl> sta = sta_info_get ( sdata , ifmgd -> bssid ); <nl> if ( sta ) <nl> sta -> beacon_loss_count ++; <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> if ( sdata -> local -> hw . flags & IEEE80211_HW_CONNECTION_MONITOR )
int __mdiobus_register ( struct mii_bus * bus , struct module * owner ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> - put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
int mwifiex_ret_802_11_scan ( struct mwifiex_private * priv , <nl>  <nl> pmatch = adapter -> nd_info -> matches [ idx ]; <nl>  <nl> - if (! pmatch ) { <nl> + if ( pmatch ) { <nl> memset ( pmatch , 0 , sizeof (* pmatch )); <nl> if ( chan_band_tlv ) { <nl> pmatch -> n_channels = 1 ;
static int st33zp24_spi_evaluate_latency ( void * phy_id ) <nl> & data , 1 ); <nl> latency ++; <nl> } <nl> + if ( status < 0 ) <nl> + return status ; <nl> + if ( latency == MAX_SPI_LATENCY ) <nl> + return - ENODEV ; <nl> + <nl> return latency - 1 ; <nl> } /* evaluate_latency () */ <nl> 
static int __init alloc_node_page_cgroup ( int nid ) <nl> start_pfn = NODE_DATA ( nid )-> node_start_pfn ; <nl> nr_pages = NODE_DATA ( nid )-> node_spanned_pages ; <nl>  <nl> + if (! nr_pages ) <nl> + return 0 ; <nl> + <nl> table_size = sizeof ( struct page_cgroup ) * nr_pages ; <nl>  <nl> base = __alloc_bootmem_node_nopanic ( NODE_DATA ( nid ),
static void proc_flush_task_mnt ( struct vfsmount * mnt , pid_t pid , pid_t tgid ) <nl> dput ( dentry ); <nl> } <nl>  <nl> + if ( pid == tgid ) <nl> + return ; <nl> + <nl> name . name = buf ; <nl> name . len = snprintf ( buf , sizeof ( buf ), "% d ", tgid ); <nl> leader = d_hash_and_lookup ( mnt -> mnt_root , & name );
static void flush_tlb_others_ipi ( const struct cpumask * cpumask , <nl> * We have to send the IPI only to <nl> * CPUs affected . <nl> */ <nl> - send_IPI_mask ( cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl> + send_IPI_mask ( f -> flush_cpumask , INVALIDATE_TLB_VECTOR_START + sender ); <nl>  <nl> while (! cpumask_empty ( to_cpumask ( f -> flush_cpumask ))) <nl> cpu_relax ();
static int mxs_phy_probe ( struct platform_device * pdev ) <nl> mxs_phy -> phy . set_suspend = mxs_phy_suspend ; <nl> mxs_phy -> phy . notify_connect = mxs_phy_on_connect ; <nl> mxs_phy -> phy . notify_disconnect = mxs_phy_on_disconnect ; <nl> + mxs_phy -> phy . type = USB_PHY_TYPE_USB2 ; <nl>  <nl> ATOMIC_INIT_NOTIFIER_HEAD (& mxs_phy -> phy . notifier ); <nl> 
static int tsi721_rio_map_inb_mem ( struct rio_mport * mport , dma_addr_t lstart , <nl> } else if ( ibw_start < ( ib_win -> rstart + ib_win -> size ) && <nl> ( ibw_start + ibw_size ) > ib_win -> rstart ) { <nl> /* Return error if address translation involved */ <nl> - if ( direct && ib_win -> xlat ) { <nl> + if (! direct || ib_win -> xlat ) { <nl> ret = - EFAULT ; <nl> break ; <nl> }
static int prepare_for_handlers ( struct ieee80211_rx_data * rx , <nl> * and location updates . Note that mac80211 <nl> * itself never looks at these frames . <nl> */ <nl> + if (! multicast && <nl> + ! ether_addr_equal ( sdata -> vif . addr , hdr -> addr1 )) <nl> + return 0 ; <nl> if ( ieee80211_is_public_action ( hdr , skb -> len )) <nl> return 1 ; <nl> if (! ieee80211_is_beacon ( hdr -> frame_control ))
static void sm501_free_init_fb ( struct sm501fb_info * info , <nl> { <nl> struct fb_info * fbi = info -> fb [ head ]; <nl>  <nl> + if (! fbi ) <nl> + return ; <nl> + <nl> fb_dealloc_cmap (& fbi -> cmap ); <nl> } <nl> 
static int sep_register_driver_with_fs ( struct sep_device * sep ) <nl> if ( ret_val ) { <nl> dev_warn (& sep -> pdev -> dev , " sysfs attribute1 fails for SEP % x \ n ", <nl> ret_val ); <nl> + misc_deregister (& sep -> miscdev_sep ); <nl> return ret_val ; <nl> } <nl> 
static long vbg_misc_device_ioctl ( struct file * filp , unsigned int req , <nl> if (! buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( copy_from_user ( buf , ( void *) arg , hdr . size_in )) { <nl> + *(( struct vbg_ioctl_hdr *) buf ) = hdr ; <nl> + if ( copy_from_user ( buf + sizeof ( hdr ), ( void *) arg + sizeof ( hdr ), <nl> + hdr . size_in - sizeof ( hdr ))) { <nl> ret = - EFAULT ; <nl> goto out ; <nl> }
static int ath10k_add_interface ( struct ieee80211_hw * hw , <nl> goto err_vdev_delete ; <nl> } <nl>  <nl> - if ( ar -> cfg_tx_chainmask ) { <nl> + /* Configuring number of spatial stream for monitor interface is causing <nl> + * target assert in qca9888 and qca6174 . <nl> + */ <nl> + if ( ar -> cfg_tx_chainmask && ( vif -> type != NL80211_IFTYPE_MONITOR )) { <nl> u16 nss = get_nss_from_chainmask ( ar -> cfg_tx_chainmask ); <nl>  <nl> vdev_param = ar -> wmi . vdev_param -> nss ;
cy_put_char ( struct tty_struct * tty , unsigned char ch ) <nl> if ( serial_paranoia_check ( info , tty -> name , " cy_put_char ")) <nl> return ; <nl>  <nl> - if (! tty || ! info -> xmit_buf ) <nl> + if (! info -> xmit_buf ) <nl> return ; <nl>  <nl> local_irq_save ( flags ); <nl> cy_write ( struct tty_struct * tty , <nl> return 0 ; <nl> } <nl>  <nl> - if (! tty || ! info -> xmit_buf ){ <nl> + if (! info -> xmit_buf ){ <nl> return 0 ; <nl> } <nl> 
static int sx150x_regmap_reg_width ( struct sx150x_pinctrl * pctl , <nl> reg == data -> pri . x123 . reg_advanced ) <nl> || <nl> ( data -> model == SX150X_456 && <nl> + data -> pri . x456 . reg_advanced && <nl> reg == data -> pri . x456 . reg_advanced )) { <nl> return 8 ; <nl> } else {
static void dpp1_cm_set_regamma_pwl ( <nl> struct dpp * dpp_base , const struct pwl_params * params , enum opp_regamma mode ) <nl> { <nl> struct dcn10_dpp * dpp = TO_DCN10_DPP ( dpp_base ); <nl> - uint32_t re_mode ; <nl> + uint32_t re_mode = 0 ; <nl>  <nl> switch ( mode ) { <nl> case OPP_REGAMMA_BYPASS :
static int gart_map_sg ( struct device * dev , struct scatterlist * sg , int nents , <nl>  <nl> error : <nl> flush_gart (); <nl> - gart_unmap_sg ( dev , sg , nents , dir ); <nl> + gart_unmap_sg ( dev , sg , out , dir ); <nl> /* When it was forced or merged try again in a dumb way */ <nl> if ( force_iommu || iommu_merge ) { <nl> out = dma_map_sg_nonforce ( dev , sg , nents , dir );
int cfg80211_get_station ( struct net_device * dev , const u8 * mac_addr , <nl> if (! rdev -> ops -> get_station ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset ( sinfo , 0 , sizeof (* sinfo )); <nl> + <nl> return rdev_get_station ( rdev , dev , mac_addr , sinfo ); <nl> } <nl> EXPORT_SYMBOL ( cfg80211_get_station );
int kvm_arch_vcpu_ioctl_set_sregs ( struct kvm_vcpu * vcpu , <nl> kvm_set_segment ( vcpu , & sregs -> tr , VCPU_SREG_TR ); <nl> kvm_set_segment ( vcpu , & sregs -> ldt , VCPU_SREG_LDTR ); <nl>  <nl> + update_cr8_intercept ( vcpu ); <nl> + <nl> /* Older userspace won ' t unhalt the vcpu on reset . */ <nl> if ( kvm_vcpu_is_bsp ( vcpu ) && kvm_rip_read ( vcpu ) == 0xfff0 && <nl> sregs -> cs . selector == 0xf000 && sregs -> cs . base == 0xffff0000 &&
static int igb_ptp_feature_enable_i210 ( struct ptp_clock_info * ptp , <nl> ts . tv_nsec = rq -> perout . period . nsec ; <nl> ns = timespec64_to_ns (& ts ); <nl> ns = ns >> 1 ; <nl> - if ( on && ns <= 70000000LL ) { <nl> + if ( on && (( ns <= 70000000LL ) || ( ns == 125000000LL ) || <nl> + ( ns == 250000000LL ) || ( ns == 500000000LL ))) { <nl> if ( ns < 8LL ) <nl> return - EINVAL ; <nl> use_freq = 1 ;
static int hba_setup_cid_tbls ( struct beiscsi_hba * phba ) <nl> kfree ( phba -> ep_array ); <nl> phba -> ep_array = NULL ; <nl> ret = - ENOMEM ; <nl> + <nl> + goto free_memory ; <nl> } <nl>  <nl> for ( i = 0 ; i < phba -> params . cxns_per_ctrl ; i ++) {
static int pci_create_attr ( struct pci_dev * pdev , int num , int write_combine ) <nl> res_attr -> size = pci_resource_len ( pdev , num ); <nl> res_attr -> private = & pdev -> resource [ num ]; <nl> retval = sysfs_create_bin_file (& pdev -> dev . kobj , res_attr ); <nl> + if ( retval ) <nl> + kfree ( res_attr ); <nl> } else <nl> retval = - ENOMEM ; <nl> 
static int __init tegra_pmc_early_init ( void ) <nl> bool invert ; <nl> u32 value ; <nl>  <nl> + mutex_init (& pmc -> powergates_lock ); <nl> + <nl> np = of_find_matching_node_and_match ( NULL , tegra_pmc_match , & match ); <nl> if (! np ) { <nl> /* <nl> static int __init tegra_pmc_early_init ( void ) <nl> return - ENXIO ; <nl> } <nl>  <nl> - mutex_init (& pmc -> powergates_lock ); <nl> - <nl> if ( np ) { <nl> pmc -> soc = match -> data ; <nl> 
static struct pci_resource * get_max_resource ( struct pci_resource ** head , u32 siz <nl> temp = temp -> next ; <nl> } <nl>  <nl> - temp -> next = max -> next ; <nl> + if ( temp ) <nl> + temp -> next = max -> next ; <nl> } <nl>  <nl> max -> next = NULL ;
static int llc_ui_sendmsg ( struct socket * sock , struct msghdr * msg , size_t len ) <nl> if ( size > llc -> dev -> mtu ) <nl> size = llc -> dev -> mtu ; <nl> copied = size - hdrlen ; <nl> + rc = - EINVAL ; <nl> + if ( copied < 0 ) <nl> + goto release ; <nl> release_sock ( sk ); <nl> skb = sock_alloc_send_skb ( sk , size , noblock , & rc ); <nl> lock_sock ( sk );
static int kvm_vcpu_ioctl_enable_cap ( struct kvm_vcpu * vcpu , <nl>  <nl> switch ( cap -> cap ) { <nl> case KVM_CAP_HYPERV_SYNIC : <nl> + if (! irqchip_in_kernel ( vcpu -> kvm )) <nl> + return - EINVAL ; <nl> return kvm_hv_activate_synic ( vcpu ); <nl> default : <nl> return - EINVAL ;
static int hist_entry__srcline_snprintf ( struct hist_entry * self , char * bf , <nl> if ( path != NULL ) <nl> goto out_path ; <nl>  <nl> + if (! self -> ms . map ) <nl> + goto out_ip ; <nl> + <nl> snprintf ( cmd , sizeof ( cmd ), " addr2line - e % s % 016 " PRIx64 , <nl> self -> ms . map -> dso -> long_name , self -> ip ); <nl> fp = popen ( cmd , " r ");
static void igb_reset_q_vector ( struct igb_adapter * adapter , int v_idx ) <nl> { <nl> struct igb_q_vector * q_vector = adapter -> q_vector [ v_idx ]; <nl>  <nl> + /* Coming from igb_set_interrupt_capability , the vectors are not yet <nl> + * allocated . So , q_vector is NULL so we should stop here . <nl> + */ <nl> + if (! q_vector ) <nl> + return ; <nl> + <nl> if ( q_vector -> tx . ring ) <nl> adapter -> tx_ring [ q_vector -> tx . ring -> queue_index ] = NULL ; <nl> 
static inline void clear_operand_string ( struct filter_parse_state * ps ) <nl>  <nl> static inline int append_operand_char ( struct filter_parse_state * ps , char c ) <nl> { <nl> - if ( ps -> operand . tail == MAX_FILTER_STR_VAL ) <nl> + if ( ps -> operand . tail == MAX_FILTER_STR_VAL - 1 ) <nl> return - EINVAL ; <nl>  <nl> ps -> operand . string [ ps -> operand . tail ++] = c ;
int cx23885_dvb_unregister ( struct cx23885_tsport * port ) <nl> * implement MFE support . <nl> */ <nl> fe0 = videobuf_dvb_get_frontend (& port -> frontends , 1 ); <nl> - if ( fe0 -> dvb . frontend ) <nl> + if ( fe0 && fe0 -> dvb . frontend ) <nl> videobuf_dvb_unregister_bus (& port -> frontends ); <nl>  <nl> switch ( port -> dev -> board ) {
int cvm_oct_common_init ( struct net_device * dev ) <nl> mac = of_get_mac_address ( priv -> of_node ); <nl>  <nl> if ( mac ) <nl> - memcpy ( dev -> dev_addr , mac , ETH_ALEN ); <nl> + ether_addr_copy ( dev -> dev_addr , mac ); <nl> else <nl> eth_hw_addr_random ( dev ); <nl> 
static void efx_filter_rfs_work ( struct work_struct * data ) <nl> struct efx_channel * channel = efx_get_channel ( efx , req -> rxq_index ); <nl> int rc ; <nl>  <nl> - rc = efx -> type -> filter_insert ( efx , & req -> spec , false ); <nl> + rc = efx -> type -> filter_insert ( efx , & req -> spec , true ); <nl> if ( rc >= 0 ) { <nl> /* Remember this so we can check whether to expire the filter <nl> * later .
static int igb_find_enabled_vfs ( struct igb_adapter * adapter ) <nl> vf_devfn = pdev -> devfn + 0x80 ; <nl> pvfdev = pci_get_device ( hw -> vendor_id , device_id , NULL ); <nl> while ( pvfdev ) { <nl> - if ( pvfdev -> devfn == vf_devfn ) <nl> + if ( pvfdev -> devfn == vf_devfn && <nl> + ( pvfdev -> bus -> number >= pdev -> bus -> number )) <nl> vfs_found ++; <nl> vf_devfn += vf_stride ; <nl> pvfdev = pci_get_device ( hw -> vendor_id ,
static int vidioc_querycap ( struct file * file , void * priv , <nl> strcpy ( cap -> driver , " vivi "); <nl> strcpy ( cap -> card , " vivi "); <nl> strlcpy ( cap -> bus_info , dev -> v4l2_dev . name , sizeof ( cap -> bus_info )); <nl> - cap -> capabilities = V4L2_CAP_VIDEO_CAPTURE | V4L2_CAP_STREAMING | \ <nl> - V4L2_CAP_READWRITE ; <nl> + cap -> capabilities = V4L2_CAP_VIDEO_CAPTURE | V4L2_CAP_STREAMING | <nl> + V4L2_CAP_READWRITE | V4L2_CAP_DEVICE_CAPS ; <nl> + cap -> device_caps = cap -> capabilities ; <nl> return 0 ; <nl> } <nl> 
static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
void dlm_lowcomms_stop ( void ) <nl> con = __nodeid2con ( i , 0 ); <nl> if ( con ) { <nl> close_connection ( con , true ); <nl> + if ( con -> othercon ) <nl> + kmem_cache_free ( con_cache , con -> othercon ); <nl> kmem_cache_free ( con_cache , con ); <nl> } <nl> }
int saa7164_cmd_send ( struct saa7164_dev * dev , u8 id , enum tmComResCmd command , <nl> dprintk ( DBGLVL_CMD , <nl> "% s () UNKNOWN OR INVALID CONTROL \ n ", <nl> __func__ ); <nl> + ret = SAA_ERR_NOT_SUPPORTED ; <nl> + break ; <nl> default : <nl> dprintk ( DBGLVL_CMD , "% s () UNKNOWN \ n ", __func__ ); <nl> ret = SAA_ERR_NOT_SUPPORTED ;
static void i40iw_cm_disconn_true ( struct i40iw_qp * iwqp ) <nl> /* Flush the queues */ <nl> i40iw_flush_wqes ( iwdev , iwqp ); <nl>  <nl> - if ( qp -> term_flags ) { <nl> + if ( qp -> term_flags && iwqp -> ibqp . event_handler ) { <nl> ibevent . device = iwqp -> ibqp . device ; <nl> ibevent . event = ( qp -> eventtype == TERM_EVENT_QP_FATAL ) ? <nl> IB_EVENT_QP_FATAL : IB_EVENT_QP_ACCESS_ERR ;
void gspca_frame_add ( struct gspca_dev * gspca_dev , <nl> } else { <nl> switch ( gspca_dev -> last_packet_type ) { <nl> case DISCARD_PACKET : <nl> - if ( packet_type == LAST_PACKET ) <nl> + if ( packet_type == LAST_PACKET ) { <nl> gspca_dev -> last_packet_type = packet_type ; <nl> + gspca_dev -> image = NULL ; <nl> + gspca_dev -> image_len = 0 ; <nl> + } <nl> return ; <nl> case LAST_PACKET : <nl> return ;
static int aead_setkey ( struct crypto_aead * tfm , const u8 * key , <nl> ctx -> authkey_len = keys . authkeylen ; <nl> ctx -> enckey_len = keys . enckeylen ; <nl>  <nl> + memzero_explicit (& keys , sizeof ( keys )); <nl> return aead_setup ( tfm , crypto_aead_authsize ( tfm )); <nl> badkey : <nl> crypto_aead_set_flags ( tfm , CRYPTO_TFM_RES_BAD_KEY_LEN ); <nl> + memzero_explicit (& keys , sizeof ( keys )); <nl> return - EINVAL ; <nl> } <nl> 
void __init exynos5_init_irq ( void ) <nl> * Theses parameters should be NULL and 0 because EXYNOS4 <nl> * uses GIC instead of VIC . <nl> */ <nl> - s5p_init_irq ( NULL , 0 ); <nl> + if (! of_machine_is_compatible (" samsung , exynos5440 ")) <nl> + s5p_init_irq ( NULL , 0 ); <nl>  <nl> gic_arch_extn . irq_set_wake = s3c_irq_wake ; <nl> }
int snd_timer_stop ( struct snd_timer_instance * timeri ) <nl> if ( err < 0 ) <nl> return err ; <nl> timer = timeri -> timer ; <nl> + if (! timer ) <nl> + return - EINVAL ; <nl> spin_lock_irqsave (& timer -> lock , flags ); <nl> timeri -> cticks = timeri -> ticks ; <nl> timeri -> pticks = 0 ;
int intel_gvt_init ( struct drm_i915_private * dev_priv ) <nl> goto bail ; <nl> } <nl>  <nl> + if (! i915 . enable_execlists ) { <nl> + DRM_INFO (" GPU guest virtualisation [ GVT - g ] disabled due to disabled execlist submission [ i915 . enable_execlists module parameter ]\ n "); <nl> + goto bail ; <nl> + } <nl> + <nl> /* <nl> * We ' re not in host or fail to find a MPT module , disable GVT - g <nl> */
static struct attribute * module_attrs [] = { <nl> }; <nl> ATTRIBUTE_GROUPS ( module ); <nl>  <nl> - static void greybus_module_release ( struct device * dev ) <nl> + static void gb_module_release ( struct device * dev ) <nl> { <nl> struct gb_module * module = to_gb_module ( dev ); <nl>  <nl> static void greybus_module_release ( struct device * dev ) <nl>  <nl> struct device_type greybus_module_type = { <nl> . name = " greybus_module ", <nl> - . release = greybus_module_release , <nl> + . release = gb_module_release , <nl> }; <nl>  <nl> struct module_find {
static int do_replace ( struct net * net , const void __user * user , <nl> if ( tmp . num_counters >= INT_MAX / sizeof ( struct ebt_counter )) <nl> return - ENOMEM ; <nl>  <nl> + tmp . name [ sizeof ( tmp . name ) - 1 ] = 0 ; <nl> + <nl> countersize = COUNTER_OFFSET ( tmp . nentries ) * nr_cpu_ids ; <nl> newinfo = vmalloc ( sizeof (* newinfo ) + countersize ); <nl> if (! newinfo )
static void aead_release ( void * private ) <nl> struct aead_tfm * tfm = private ; <nl>  <nl> crypto_free_aead ( tfm -> aead ); <nl> + crypto_put_default_null_skcipher2 (); <nl> kfree ( tfm ); <nl> } <nl>  <nl> static void aead_sock_destruct ( struct sock * sk ) <nl> unsigned int ivlen = crypto_aead_ivsize ( tfm ); <nl>  <nl> af_alg_pull_tsgl ( sk , ctx -> used , NULL , 0 ); <nl> - crypto_put_default_null_skcipher2 (); <nl> sock_kzfree_s ( sk , ctx -> iv , ivlen ); <nl> sock_kfree_s ( sk , ctx , ctx -> len ); <nl> af_alg_release_parent ( sk );
static int usbduxsigma_auto_attach ( struct comedi_device * dev , <nl> } <nl>  <nl> ret = usbduxsigma_alloc_usb_buffers ( dev ); <nl> - if ( ret ) { <nl> - tidy_up ( devpriv ); <nl> + if ( ret ) <nl> return ret ; <nl> - } <nl>  <nl> ret = comedi_load_firmware ( dev , & usb -> dev , FIRMWARE , <nl> usbduxsigma_firmware_upload , 0 );
static struct fileIdentDesc * udf_find_entry ( struct inode * dir , <nl> } <nl>  <nl> if (( cfi -> fileCharacteristics & FID_FILE_CHAR_PARENT ) && <nl> - isdotdot ) { <nl> - brelse ( epos . bh ); <nl> - return fi ; <nl> - } <nl> + isdotdot ) <nl> + goto out_ok ; <nl>  <nl> if (! lfi ) <nl> continue ;
extern pgprot_t phys_mem_access_prot ( struct file * file , unsigned long pfn , <nl> # define pmd_sect ( pmd ) (( pmd_val ( pmd ) & PMD_TYPE_MASK ) == \ <nl> PMD_TYPE_SECT ) <nl>  <nl> -# ifdef CONFIG_ARM64_64K_PAGES <nl> +# if defined ( CONFIG_ARM64_64K_PAGES ) || CONFIG_PGTABLE_LEVELS < 3 <nl> # define pud_sect ( pud ) ( 0 ) <nl> # define pud_table ( pud ) ( 1 ) <nl> # else
acpi_status acpi_os_initialize1 ( void ); <nl> int init_acpi_device_notify ( void ); <nl> int acpi_scan_init ( void ); <nl> -# ifdef CONFIG_ACPI_PCI_SLOT <nl> - void acpi_pci_slot_init ( void ); <nl> -# else <nl> - static inline void acpi_pci_slot_init ( void ) { } <nl> -# endif <nl> void acpi_pci_root_init ( void ); <nl> void acpi_pci_link_init ( void ); <nl> void acpi_pci_root_hp_init ( void );
static void acpi_ac_notify ( struct acpi_device * device , u32 event ) <nl> acpi_bus_generate_netlink_event ( device -> pnp . device_class , <nl> dev_name (& device -> dev ), event , <nl> ( u32 ) ac -> state ); <nl> + acpi_notifier_call_chain ( device , event , ( u32 ) ac -> state ); <nl> # ifdef CONFIG_ACPI_SYSFS_POWER <nl> kobject_uevent (& ac -> charger . dev -> kobj , KOBJ_CHANGE ); <nl> # endif
static int wm8753_register ( struct wm8753_priv * wm8753 ) <nl> codec -> reg_cache = & wm8753 -> reg_cache ; <nl> codec -> private_data = wm8753 ; <nl>  <nl> - memcpy ( codec -> reg_cache , wm8753_reg , sizeof ( codec -> reg_cache )); <nl> + memcpy ( codec -> reg_cache , wm8753_reg , sizeof ( wm8753 -> reg_cache )); <nl> INIT_DELAYED_WORK (& codec -> delayed_work , wm8753_work ); <nl>  <nl> ret = wm8753_reset ( codec );
static int __devinit e100_probe ( struct pci_dev * pdev , <nl>  <nl> e100_get_defaults ( nic ); <nl>  <nl> + /* D100 MAC doesn ' t allow rx of vlan packets with normal MTU */ <nl> + if ( nic -> mac < mac_82558_D101_A4 ) <nl> + netdev -> features |= NETIF_F_VLAN_CHALLENGED ; <nl> + <nl> /* locks must be initialized before calling hw_reset */ <nl> spin_lock_init (& nic -> cb_lock ); <nl> spin_lock_init (& nic -> cmd_lock );
int p9_client_remove ( struct p9_fid * fid ) <nl> P9_DPRINTK ( P9_DEBUG_9P , "<<< RREMOVE fid % d \ n ", fid -> fid ); <nl>  <nl> p9_free_req ( clnt , req ); <nl> - p9_fid_destroy ( fid ); <nl> - <nl> error : <nl> + p9_fid_destroy ( fid ); <nl> return err ; <nl> } <nl> EXPORT_SYMBOL ( p9_client_remove );
static void ieee80211_xmit ( struct ieee80211_sub_if_data * sdata , <nl> list ) { <nl> if (! ieee80211_sdata_running ( tmp_sdata )) <nl> continue ; <nl> - if ( tmp_sdata -> vif . type != NL80211_IFTYPE_AP ) <nl> + if ( tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_MONITOR || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_AP_VLAN || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_WDS ) <nl> continue ; <nl> if ( compare_ether_addr ( tmp_sdata -> vif . addr , <nl> hdr -> addr2 ) == 0 ) {
static int mem_cgroup_hierarchical_reclaim ( struct mem_cgroup * root_mem , <nl> excess = res_counter_soft_limit_excess (& root_mem -> res ) >> PAGE_SHIFT ; <nl>  <nl> /* If memsw_is_minimum == 1 , swap - out is of - no - use . */ <nl> - if ( root_mem -> memsw_is_minimum ) <nl> + if (! check_soft && root_mem -> memsw_is_minimum ) <nl> noswap = true ; <nl>  <nl> while ( 1 ) {
static const struct amdgpu_irq_src_funcs dm_hpd_irq_funcs = { <nl>  <nl> void amdgpu_dm_set_irq_funcs ( struct amdgpu_device * adev ) <nl> { <nl> - if ( adev -> mode_info . num_crtc > 0 ) <nl> - adev -> crtc_irq . num_types = AMDGPU_CRTC_IRQ_VLINE1 + adev -> mode_info . num_crtc ; <nl> - else <nl> - adev -> crtc_irq . num_types = 0 ; <nl> + <nl> + adev -> crtc_irq . num_types = adev -> mode_info . num_crtc ; <nl> adev -> crtc_irq . funcs = & dm_crtc_irq_funcs ; <nl>  <nl> adev -> pageflip_irq . num_types = adev -> mode_info . num_crtc ;
static int intel_crtc_mode_set ( struct drm_crtc * crtc , <nl> } <nl> dpll |= DPLL_DVO_HIGH_SPEED ; <nl> } <nl> - if ( is_dp ) <nl> + if ( is_dp || intel_encoder_is_pch_edp (& has_edp_encoder -> base )) <nl> dpll |= DPLL_DVO_HIGH_SPEED ; <nl>  <nl> /* compute bitmask from p1 value */
int dma_async_device_register ( struct dma_device * device ) <nl> } <nl> chan -> client_count = 0 ; <nl> } <nl> + <nl> + if (! chancnt ) { <nl> + dev_err ( device -> dev , "% s : device has no channels !\ n ", __func__ ); <nl> + rc = - ENODEV ; <nl> + goto err_out ; <nl> + } <nl> + <nl> device -> chancnt = chancnt ; <nl>  <nl> mutex_lock (& dma_list_mutex );
int main ( int argc , char ** argv ) <nl> "% s - dev % d ", device_name , dev_num ); <nl> if ( ret < 0 ) { <nl> ret = - ENOMEM ; <nl> - goto error_ret ; <nl> + goto error_free_dev_dir_name ; <nl> } <nl> } <nl>  <nl> int main ( int argc , char ** argv ) <nl> error_free_triggername : <nl> if ( datardytrigger ) <nl> free ( trigger_name ); <nl> + error_free_dev_dir_name : <nl> + free ( dev_dir_name ); <nl> error_ret : <nl> return ret ; <nl> }
__ieee80211_tx_prepare ( struct ieee80211_tx_data * tx , <nl> tx -> local = local ; <nl> tx -> sdata = IEEE80211_DEV_TO_SUB_IF ( dev ); <nl> tx -> channel = local -> hw . conf . channel ; <nl> + tx -> rate_idx = - 1 ; <nl> + tx -> last_frag_rate_idx = - 1 ; <nl> /* <nl> * Set this flag ( used below to indicate " automatic fragmentation "), <nl> * it will be cleared / left by radiotap as desired .
static void bnx2x_tpa_stop ( struct bnx2x * bp , struct bnx2x_fastpath * fp , <nl>  <nl> return ; <nl> } <nl> - bnx2x_frag_free ( fp , new_data ); <nl> + if ( new_data ) <nl> + bnx2x_frag_free ( fp , new_data ); <nl> drop : <nl> /* drop the packet and keep the buffer in the bin */ <nl> DP ( NETIF_MSG_RX_STATUS ,
static int asus_laptop_get_info ( struct asus_laptop * asus ) <nl> } <nl> } <nl> asus -> name = kstrdup ( string , GFP_KERNEL ); <nl> - if (! asus -> name ) <nl> + if (! asus -> name ) { <nl> + kfree ( buffer . pointer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if (* string ) <nl> pr_notice (" % s model detected \ n ", string );
static const struct dmi_system_id min_max_dmi_table [] __initconst = { <nl> }, <nl> . driver_data = ( int []){ 1024 , 5052 , 2258 , 4832 }, <nl> }, <nl> + { <nl> + /* Lenovo ThinkPad X240 */ <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " LENOVO "), <nl> + DMI_MATCH ( DMI_PRODUCT_VERSION , " ThinkPad X240 "), <nl> + }, <nl> + . driver_data = ( int []){ 1232 , 5710 , 1156 , 4696 }, <nl> + }, <nl> { <nl> /* Lenovo ThinkPad T440s */ <nl> . matches = {
static int disable_periodic ( struct ehci_hcd * ehci ) <nl> ehci_writel ( ehci , cmd , & ehci -> regs -> command ); <nl> /* posted write ... */ <nl>  <nl> + free_cached_itd_list ( ehci ); <nl> + <nl> ehci -> next_uframe = - 1 ; <nl> return 0 ; <nl> }
static int __devinit eni_start ( struct atm_dev * dev ) <nl> kfree ( eni_dev -> free_list ); <nl>  <nl> free_irq : <nl> - free_irq ( eni_dev -> irq , eni_dev ); <nl> + free_irq ( eni_dev -> irq , dev ); <nl>  <nl> out : <nl> return error ;
static int intel_atomic_check ( struct drm_device * dev , <nl> struct intel_crtc_state * pipe_config = <nl> to_intel_crtc_state ( crtc_state ); <nl>  <nl> + memset (& to_intel_crtc ( crtc )-> atomic , 0 , <nl> + sizeof ( struct intel_crtc_atomic_commit )); <nl> + <nl> /* Catch I915_MODE_FLAG_INHERITED */ <nl> if ( crtc_state -> mode . private_flags != crtc -> state -> mode . private_flags ) <nl> crtc_state -> mode_changed = true ;
nvmet_fc_find_target_queue ( struct nvmet_fc_tgtport * tgtport , <nl> u16 qid = nvmet_fc_getqueueid ( connection_id ); <nl> unsigned long flags ; <nl>  <nl> + if ( qid > NVMET_NR_QUEUES ) <nl> + return NULL ; <nl> + <nl> spin_lock_irqsave (& tgtport -> lock , flags ); <nl> list_for_each_entry ( assoc , & tgtport -> assoc_list , a_list ) { <nl> if ( association_id == assoc -> association_id ) {
static int wm_adsp_fw_put ( struct snd_kcontrol * kcontrol , <nl> if ( adsp [ e -> shift_l ]. running ) <nl> return - EBUSY ; <nl>  <nl> - adsp -> fw = ucontrol -> value . integer . value [ 0 ]; <nl> + adsp [ e -> shift_l ]. fw = ucontrol -> value . integer . value [ 0 ]; <nl>  <nl> return 0 ; <nl> }
smb2_query_symlink ( const unsigned int xid , struct cifs_tcon * tcon , <nl> & resp_buftype ); <nl> if (! rc || ! err_iov . iov_base ) { <nl> rc = - ENOENT ; <nl> - goto querty_exit ; <nl> + goto free_path ; <nl> } <nl>  <nl> err_buf = err_iov . iov_base ; <nl> smb2_query_symlink ( const unsigned int xid , struct cifs_tcon * tcon , <nl>  <nl> querty_exit : <nl> free_rsp_buf ( resp_buftype , err_buf ); <nl> + free_path : <nl> kfree ( utf16_path ); <nl> return rc ; <nl> }
static void atomic_switch_perf_msrs ( struct vcpu_vmx * vmx ) <nl> msrs [ i ]. host ); <nl> } <nl>  <nl> - void vmx_arm_hv_timer ( struct kvm_vcpu * vcpu ) <nl> + static void vmx_arm_hv_timer ( struct kvm_vcpu * vcpu ) <nl> { <nl> struct vcpu_vmx * vmx = to_vmx ( vcpu ); <nl> u64 tscl ;
int __vma_adjust ( struct vm_area_struct * vma , unsigned long start , <nl> * If next doesn ' t have anon_vma , import from vma after <nl> * next , if the vma overlaps with it . <nl> */ <nl> - if ( remove_next == 2 && next && ! next -> anon_vma ) <nl> + if ( remove_next == 2 && ! next -> anon_vma ) <nl> exporter = next -> vm_next ; <nl>  <nl> } else if ( end > next -> vm_start ) {
static int imx_ldb_connector_get_modes ( struct drm_connector * connector ) <nl> struct drm_display_mode * mode ; <nl>  <nl> mode = drm_mode_create ( connector -> dev ); <nl> + if (! mode ) <nl> + return - EINVAL ; <nl> drm_mode_copy ( mode , & imx_ldb_ch -> mode ); <nl> mode -> type |= DRM_MODE_TYPE_DRIVER | DRM_MODE_TYPE_PREFERRED ; <nl> drm_mode_probed_add ( connector , mode );
static __init void parse_cmdline_early ( char ** cmdline_p ) <nl> if (! memcmp ( from , " noapic ", 6 )) <nl> skip_ioapic_setup = 1 ; <nl>  <nl> - if (! memcmp ( from , " apic ", 4 )) { <nl> + /* Make sure to not confuse with apic = */ <nl> + if (! memcmp ( from , " apic ", 4 ) && <nl> + ( from [ 4 ] == ' ' || from [ 4 ] == 0 )) { <nl> skip_ioapic_setup = 0 ; <nl> ioapic_force = 1 ; <nl> }
static int __devinit mc13783_led_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - led = kzalloc ( sizeof (* led ) * pdata -> num_leds , GFP_KERNEL ); <nl> + led = kcalloc ( pdata -> num_leds , sizeof (* led ), GFP_KERNEL ); <nl> if ( led == NULL ) { <nl> dev_err (& pdev -> dev , " failed to alloc memory \ n "); <nl> return - ENOMEM ;
int kprobe_fault_handler ( struct pt_regs * regs , unsigned long cause ); <nl> void kretprobe_trampoline ( void ); <nl> void trap_is_kprobe ( unsigned long address , struct pt_regs * regs ); <nl> # else <nl> - static void trap_is_kprobe ( unsigned long address , struct pt_regs * regs ) <nl> -{ <nl> -} <nl> +# define trap_is_kprobe ( address , regs ) <nl> # endif /* CONFIG_KPROBES */ <nl>  <nl> # endif /* _ARC_KPROBES_H */
static void iwl_mvm_scan_fill_ssids ( struct iwl_ssid_ie * cmd_ssid , <nl> static u16 iwl_mvm_get_active_dwell ( enum ieee80211_band band , int n_ssids ) <nl> { <nl> if ( band == IEEE80211_BAND_2GHZ ) <nl> - return 30 + 3 * ( n_ssids + 1 ); <nl> - return 20 + 2 * ( n_ssids + 1 ); <nl> + return 20 + 3 * ( n_ssids + 1 ); <nl> + return 10 + 2 * ( n_ssids + 1 ); <nl> } <nl>  <nl> static u16 iwl_mvm_get_passive_dwell ( enum ieee80211_band band )
static bool gen8_ppgtt_clear_pt ( struct i915_address_space * vm , <nl> GEM_BUG_ON ( pte_end > GEN8_PTES ); <nl>  <nl> bitmap_clear ( pt -> used_ptes , pte , num_entries ); <nl> - <nl> - if ( bitmap_empty ( pt -> used_ptes , GEN8_PTES )) <nl> - return true ; <nl> + if ( USES_FULL_PPGTT ( vm -> i915 )) { <nl> + if ( bitmap_empty ( pt -> used_ptes , GEN8_PTES )) <nl> + return true ; <nl> + } <nl>  <nl> pt_vaddr = kmap_px ( pt ); <nl> 
static int __devinit ipr_probe_ioa ( struct pci_dev * pdev , <nl> uproc = readl ( ioa_cfg -> regs . sense_uproc_interrupt_reg32 ); <nl> if (( mask & IPR_PCII_HRRQ_UPDATED ) == 0 || ( uproc & IPR_UPROCI_RESET_ALERT )) <nl> ioa_cfg -> needs_hard_reset = 1 ; <nl> - if ( interrupts & IPR_PCII_ERROR_INTERRUPTS ) <nl> + if (( interrupts & IPR_PCII_ERROR_INTERRUPTS ) || reset_devices ) <nl> ioa_cfg -> needs_hard_reset = 1 ; <nl> if ( interrupts & IPR_PCII_IOA_UNIT_CHECKED ) <nl> ioa_cfg -> ioa_unit_checked = 1 ;
void uncore_perf_event_update ( struct intel_uncore_box * box , struct perf_event * e <nl> u64 prev_count , new_count , delta ; <nl> int shift ; <nl>  <nl> - if ( event -> hw . idx >= UNCORE_PMC_IDX_FIXED ) <nl> + if ( event -> hw . idx == UNCORE_PMC_IDX_FIXED ) <nl> shift = 64 - uncore_fixed_ctr_bits ( box ); <nl> else <nl> shift = 64 - uncore_perf_ctr_bits ( box );
static int __access_remote_vm ( struct task_struct * tsk , struct mm_struct * mm , <nl> */ <nl> # ifdef CONFIG_HAVE_IOREMAP_PROT <nl> vma = find_vma ( mm , addr ); <nl> - if (! vma ) <nl> + if (! vma || vma -> vm_start > addr ) <nl> break ; <nl> if ( vma -> vm_ops && vma -> vm_ops -> access ) <nl> ret = vma -> vm_ops -> access ( vma , addr , buf ,
static struct davinci_aemif_timing da830_evm_nandflash_timing = { <nl> }; <nl>  <nl> static struct davinci_nand_pdata da830_evm_nand_pdata = { <nl> + . core_chipsel = 1 , <nl> . parts = da830_evm_nand_partitions , <nl> . nr_parts = ARRAY_SIZE ( da830_evm_nand_partitions ), <nl> . ecc_mode = NAND_ECC_HW ,
static enum dma_status sun6i_dma_tx_status ( struct dma_chan * chan , <nl> size_t bytes = 0 ; <nl>  <nl> ret = dma_cookie_status ( chan , cookie , state ); <nl> - if ( ret == DMA_COMPLETE ) <nl> + if ( ret == DMA_COMPLETE || ! state ) <nl> return ret ; <nl>  <nl> spin_lock_irqsave (& vchan -> vc . lock , flags );
__tree_mod_log_free_eb ( struct btrfs_fs_info * fs_info , struct extent_buffer * eb ) <nl> u32 nritems ; <nl> int ret ; <nl>  <nl> + if ( btrfs_header_level ( eb ) == 0 ) <nl> + return ; <nl> + <nl> nritems = btrfs_header_nritems ( eb ); <nl> for ( i = nritems - 1 ; i >= 0 ; i --) { <nl> ret = tree_mod_log_insert_key_locked ( fs_info , eb , i ,
static void seq_set_overflow ( struct seq_file * m ) <nl>  <nl> static void * seq_buf_alloc ( unsigned long size ) <nl> { <nl> + if ( unlikely ( size > MAX_RW_COUNT )) <nl> + return NULL ; <nl> + <nl> return kvmalloc ( size , GFP_KERNEL_ACCOUNT ); <nl> } <nl> 
static int nvme_nvm_submit_user_cmd ( struct request_queue * q , <nl>  <nl> rq -> timeout = timeout ? timeout : ADMIN_TIMEOUT ; <nl>  <nl> - rq -> cmd_flags &= ~ REQ_FAILFAST_DRIVER ; <nl> - <nl> if ( ppa_buf && ppa_len ) { <nl> ppa_list = dma_pool_alloc ( dev -> dma_pool , GFP_KERNEL , & ppa_dma ); <nl> if (! ppa_list ) {
static int btrfs_get_name ( struct dentry * parent , char * name , <nl> name_len = btrfs_inode_ref_name_len ( leaf , iref ); <nl> } <nl>  <nl> + ret = btrfs_is_name_len_valid ( leaf , path -> slots [ 0 ], name_ptr , name_len ); <nl> + if (! ret ) { <nl> + btrfs_free_path ( path ); <nl> + return - EIO ; <nl> + } <nl> read_extent_buffer ( leaf , name , name_ptr , name_len ); <nl> btrfs_free_path ( path ); <nl> 
int wilc_netdev_init ( void ) <nl>  <nl> /* create the common structure */ <nl> g_linux_wlan = kzalloc ( sizeof ( linux_wlan_t ), GFP_KERNEL ); <nl> + if (! g_linux_wlan ) <nl> + return - ENOMEM ; <nl>  <nl> /* Reset interrupt count debug */ <nl> int_rcvdU = 0 ;
int ath9k_wmi_cmd ( struct wmi * wmi , enum wmi_cmd_id cmd_id , <nl> ath_dbg ( common , WMI , " Timeout waiting for WMI command : % s \ n ", <nl> wmi_cmd_to_name ( cmd_id )); <nl> mutex_unlock (& wmi -> op_mutex ); <nl> + kfree_skb ( skb ); <nl> return - ETIMEDOUT ; <nl> } <nl> 
acornfb_pan_display ( struct fb_var_screeninfo * var , struct fb_info * info ) <nl> if (!( var -> vmode & FB_VMODE_YWRAP )) <nl> y_bottom += var -> yres ; <nl>  <nl> - BUG_ON ( y_bottom > var -> yres_virtual ); <nl> + if ( y_bottom > var -> yres_virtual ) <nl> + return - EINVAL ; <nl>  <nl> acornfb_update_dma ( info , var ); <nl> 
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> dev -> name ); <nl> break ; <nl> } <nl> + <nl> + if ( idev ) <nl> + idev -> if_flags |= IF_READY ; <nl> } else { <nl> if (! netif_carrier_ok ( dev )) { <nl> /* device is still not ready . */
static long evdev_do_ioctl ( struct file * file , unsigned int cmd , <nl> return - EFAULT ; <nl>  <nl> error = input_ff_upload ( dev , & effect , file ); <nl> + if ( error ) <nl> + return error ; <nl>  <nl> if ( put_user ( effect . id , &((( struct ff_effect __user *) p )-> id ))) <nl> return - EFAULT ; <nl>  <nl> - return error ; <nl> + return 0 ; <nl> } <nl>  <nl> /* Multi - number variable - length handlers */
static void __exit cmdq_drv_exit ( void ) <nl>  <nl> subsys_initcall ( cmdq_drv_init ); <nl> module_exit ( cmdq_drv_exit ); <nl> + <nl> + MODULE_LICENSE (" GPL v2 ");
static void fbcon_deinit ( struct vc_data * vc ) <nl> finished : <nl>  <nl> fbcon_free_font ( p , free_font ); <nl> + if ( free_font ) <nl> + vc -> vc_font . data = NULL ; <nl>  <nl> if (! con_is_bound (& fb_con )) <nl> fbcon_exit ();
static struct platform_driver wm8505fb_driver = { <nl> . driver = { <nl> . owner = THIS_MODULE , <nl> . name = DRIVER_NAME , <nl> - . of_match_table = of_match_ptr ( wmt_dt_ids ), <nl> + . of_match_table = wmt_dt_ids , <nl> }, <nl> }; <nl> 
static void qedi_get_boot_tgt_info ( struct nvm_iscsi_block * block , <nl> ipv6_en = !!( block -> generic . ctrl_flags & <nl> NVM_ISCSI_CFG_GEN_IPV6_ENABLED ); <nl>  <nl> - snprintf ( tgt -> iscsi_name , NVM_ISCSI_CFG_ISCSI_NAME_MAX_LEN , "% s \ n ", <nl> + snprintf ( tgt -> iscsi_name , sizeof ( tgt -> iscsi_name ), "% s \ n ", <nl> block -> target [ index ]. target_name . byte ); <nl>  <nl> tgt -> ipv6_en = ipv6_en ;
static void mtk_drm_unbind ( struct device * dev ) <nl> { <nl> struct mtk_drm_private * private = dev_get_drvdata ( dev ); <nl>  <nl> - drm_put_dev ( private -> drm ); <nl> + drm_dev_unregister ( private -> drm ); <nl> + drm_dev_unref ( private -> drm ); <nl> private -> drm = NULL ; <nl> } <nl> 
static int do_umount ( struct mount * mnt , int flags ) <nl> * Special case for " unmounting " root ... <nl> * we just try to remount it readonly . <nl> */ <nl> + if (! capable ( CAP_SYS_ADMIN )) <nl> + return - EPERM ; <nl> down_write (& sb -> s_umount ); <nl> if (!( sb -> s_flags & MS_RDONLY )) <nl> retval = do_remount_sb ( sb , MS_RDONLY , NULL , 0 );
ccp_run_sha_cmd ( struct ccp_cmd_queue * cmd_q , struct ccp_cmd * cmd ) <nl> LSB_ITEM_SIZE ); <nl> break ; <nl> default : <nl> + kfree ( hmac_buf ); <nl> ret = - EINVAL ; <nl> - goto e_ctx ; <nl> + goto e_data ; <nl> } <nl>  <nl> memset (& hmac_cmd , 0 , sizeof ( hmac_cmd ));
int i915_gem_init ( struct drm_device * dev ) <nl> i915_gem_init_global_gtt ( dev ); <nl>  <nl> ret = i915_gem_context_init ( dev ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return ret ; <nl> + } <nl>  <nl> ret = i915_gem_init_hw ( dev ); <nl> mutex_unlock (& dev -> struct_mutex );
static int orinoco_ioctl_set_auth ( struct net_device * dev , <nl> */ <nl> if ( param -> value ) { <nl> priv -> tkip_cm_active = 1 ; <nl> - ret = hermes_enable_port ( hw , 0 ); <nl> + ret = hermes_disable_port ( hw , 0 ); <nl> } else { <nl> priv -> tkip_cm_active = 0 ; <nl> - ret = hermes_disable_port ( hw , 0 ); <nl> + ret = hermes_enable_port ( hw , 0 ); <nl> } <nl> break ; <nl> 
int class_process_proc_param ( char * prefix , struct lprocfs_vars * lvars , <nl>  <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> - rc = var -> fops -> write (& fakefile , sval , <nl> + rc = var -> fops -> write (& fakefile , <nl> + ( const char __user *) sval , <nl> vallen , NULL ); <nl> set_fs ( oldfs ); <nl> }
int imx1_pinctrl_core_probe ( struct platform_device * pdev , <nl>  <nl> ipctl -> base = devm_ioremap_nocache (& pdev -> dev , res -> start , <nl> resource_size ( res )); <nl> - if ( IS_ERR ( ipctl -> base )) <nl> - return PTR_ERR ( ipctl -> base ); <nl> + if (! ipctl -> base ) <nl> + return - ENOMEM ; <nl>  <nl> pctl_desc = & imx1_pinctrl_desc ; <nl> pctl_desc -> name = dev_name (& pdev -> dev );
static void assert_can_enable_dc9 ( struct drm_i915_private * dev_priv ) <nl> " DC9 already programmed to be enabled .\ n "); <nl> WARN_ONCE ( I915_READ ( DC_STATE_EN ) & DC_STATE_EN_UPTO_DC5 , <nl> " DC5 still not disabled to enable DC9 .\ n "); <nl> - WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ), " Power well on .\ n "); <nl> + WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ) & <nl> + SKL_POWER_WELL_REQ ( SKL_DISP_PW_2 ), <nl> + " Power well 2 on .\ n "); <nl> WARN_ONCE ( intel_irqs_enabled ( dev_priv ), <nl> " Interrupts not disabled yet .\ n "); <nl> 
static int __init setup_maxnodemem ( char * str ) <nl> { <nl> char * endp ; <nl> unsigned long long maxnodemem ; <nl> - long node ; <nl> + unsigned long node ; <nl>  <nl> node = str ? simple_strtoul ( str , & endp , 0 ) : INT_MAX ; <nl> if ( node >= MAX_NUMNODES || * endp != ':')
static int ftrace_function_set_regexp ( struct ftrace_ops * ops , int filter , <nl> static int __ftrace_function_set_filter ( int filter , char * buf , int len , <nl> struct function_filter_data * data ) <nl> { <nl> - int i , re_cnt , ret ; <nl> + int i , re_cnt , ret = - EINVAL ; <nl> int * reset ; <nl> char ** re ; <nl> 
void drm_sysfs_destroy ( void ) <nl> */ <nl> static void drm_sysfs_device_release ( struct device * dev ) <nl> { <nl> + memset ( dev , 0 , sizeof ( struct device )); <nl> return ; <nl> } <nl> 
static struct sysfs_dirent * sysfs_new_dirent ( struct sysfs_dirent * parent_sd , <nl>  <nl> memset ( sd , 0 , sizeof (* sd )); <nl> atomic_set (& sd -> s_count , 1 ); <nl> - atomic_set (& sd -> s_event , 0 ); <nl> + atomic_set (& sd -> s_event , 1 ); <nl> INIT_LIST_HEAD (& sd -> s_children ); <nl> list_add (& sd -> s_sibling , & parent_sd -> s_children ); <nl> sd -> s_element = element ;
static int dgap_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> { <nl> int rc ; <nl>  <nl> + if ( dgap_NumBoards >= MAXBOARDS ) <nl> + return - EPERM ; <nl> + <nl> /* wake up and enable device */ <nl> rc = pci_enable_device ( pdev ); <nl> 
int arm_pmu_acpi_probe ( armpmu_init_fn init_fn ) <nl> ret = armpmu_register ( pmu ); <nl> if ( ret ) { <nl> pr_warn (" Failed to register PMU for CPU % d \ n ", cpu ); <nl> + kfree ( pmu -> name ); <nl> return ret ; <nl> } <nl> }
static int snd_timer_user_params ( struct file * file , <nl> if ( tu -> timeri -> flags & SNDRV_TIMER_IFLG_EARLY_EVENT ) { <nl> if ( tu -> tread ) { <nl> struct snd_timer_tread tread ; <nl> + memset (& tread , 0 , sizeof ( tread )); <nl> tread . event = SNDRV_TIMER_EVENT_EARLY ; <nl> tread . tstamp . tv_sec = 0 ; <nl> tread . tstamp . tv_nsec = 0 ;
static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> struct gfs2_holder * t_gh ) <nl> { <nl> struct gfs2_inode * ip ; <nl> - struct gfs2_holder ji_gh ; <nl> struct gfs2_jdesc * jd ; <nl> struct lfcc * lfcc ; <nl> LIST_HEAD ( list ); <nl> static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> gfs2_glock_dq_uninit (& lfcc -> gh ); <nl> kfree ( lfcc ); <nl> } <nl> - gfs2_glock_dq_uninit (& ji_gh ); <nl> return error ; <nl> } <nl> 
static int sfb_dump ( struct Qdisc * sch , struct sk_buff * skb ) <nl>  <nl> sch -> qstats . backlog = q -> qdisc -> qstats . backlog ; <nl> opts = nla_nest_start ( skb , TCA_OPTIONS ); <nl> + if ( opts == NULL ) <nl> + goto nla_put_failure ; <nl> if ( nla_put ( skb , TCA_SFB_PARMS , sizeof ( opt ), & opt )) <nl> goto nla_put_failure ; <nl> return nla_nest_end ( skb , opts );
static int skl_manifest_load ( struct snd_soc_component * cmpnt , <nl> struct skl * skl = ebus_to_skl ( ebus ); <nl> int ret = 0 ; <nl>  <nl> + /* proceed only if we have private data defined */ <nl> + if ( manifest -> priv . size == 0 ) <nl> + return 0 ; <nl> + <nl> minfo = & skl -> skl_sst -> manifest ; <nl>  <nl> skl_tplg_get_manifest_data ( manifest , bus -> dev , minfo );
static void s6000_pcm_enqueue_dma ( struct snd_pcm_substream * substream ) <nl> return ; <nl> } <nl>  <nl> - BUG_ON ( period_size & 15 ); <nl> + if ( WARN_ON ( period_size & 15 )) <nl> + return ; <nl> s6dmac_put_fifo ( DMA_MASK_DMAC ( channel ), DMA_INDEX_CHNL ( channel ), <nl> src , dst , period_size ); <nl> 
static void stv0900_set_search_standard ( struct stv0900_internal * intp , <nl> break ; <nl> case STV0900_SEARCH_DSS : <nl> dprintk (" Search Standard = DSS \ n "); <nl> - case STV0900_SEARCH_DVBS2 : <nl> break ; <nl> + case STV0900_SEARCH_DVBS2 : <nl> dprintk (" Search Standard = DVBS2 \ n "); <nl> + break ; <nl> case STV0900_AUTO_SEARCH : <nl> default : <nl> dprintk (" Search Standard = AUTO \ n ");
static void ar5008_hw_init_bb ( struct ath_hw * ah , <nl> else <nl> synthDelay /= 10 ; <nl>  <nl> + if ( IS_CHAN_HALF_RATE ( chan )) <nl> + synthDelay *= 2 ; <nl> + else if ( IS_CHAN_QUARTER_RATE ( chan )) <nl> + synthDelay *= 4 ; <nl> + <nl> REG_WRITE ( ah , AR_PHY_ACTIVE , AR_PHY_ACTIVE_EN ); <nl>  <nl> udelay ( synthDelay + BASE_ACTIVATE_DELAY );
void __aa_fs_profile_migrate_dents ( struct aa_profile * old , <nl>  <nl> for ( i = 0 ; i < AAFS_PROF_SIZEOF ; i ++) { <nl> new -> dents [ i ] = old -> dents [ i ]; <nl> + if ( new -> dents [ i ]) <nl> + new -> dents [ i ]-> d_inode -> i_mtime = CURRENT_TIME ; <nl> old -> dents [ i ] = NULL ; <nl> } <nl> }
i915_gem_execbuffer_relocate_entry ( struct drm_i915_gem_object * obj , <nl> else <nl> ret = relocate_entry_gtt ( obj , reloc ); <nl>  <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* and update the user ' s relocation entry */ <nl> reloc -> presumed_offset = target_offset ; <nl> 
__cpuinit int unsynchronized_tsc ( void ) <nl> if ( boot_cpu_data . x86_vendor == X86_VENDOR_INTEL ) { <nl> # ifdef CONFIG_ACPI <nl> /* But TSC doesn ' t tick in C3 so don ' t use it there */ <nl> - if ( acpi_fadt . length > 0 && acpi_fadt . plvl3_lat < 100 ) <nl> + if ( acpi_fadt . length > 0 && acpi_fadt . plvl3_lat < 1000 ) <nl> return 1 ; <nl> # endif <nl> return 0 ;
static int perl_start_script ( const char * script , int argc , const char ** argv ) <nl> goto error ; <nl> } <nl>  <nl> - perl_run ( my_perl ); <nl> + if ( perl_run ( my_perl )) { <nl> + err = - 1 ; <nl> + goto error ; <nl> + } <nl> + <nl> if ( SvTRUE ( ERRSV )) { <nl> err = - 1 ; <nl> goto error ;
static int qlcnic_82xx_setup_intr ( struct qlcnic_adapter * adapter ) <nl> qlcnic_disable_multi_tx ( adapter ); <nl>  <nl> err = qlcnic_enable_msi_legacy ( adapter ); <nl> - if (! err ) <nl> + if ( err ) <nl> return err ; <nl> } <nl> }
static int pvc_getname ( struct socket * sock , struct sockaddr * sockaddr , <nl> return - ENOTCONN ; <nl> * sockaddr_len = sizeof ( struct sockaddr_atmpvc ); <nl> addr = ( struct sockaddr_atmpvc *) sockaddr ; <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> addr -> sap_family = AF_ATMPVC ; <nl> addr -> sap_addr . itf = vcc -> dev -> number ; <nl> addr -> sap_addr . vpi = vcc -> vpi ;
static int dtl1_hci_send_frame ( struct sk_buff * skb ) <nl> nsh . len = skb -> len ; <nl>  <nl> s = bt_skb_alloc ( NSHL + skb -> len + 1 , GFP_ATOMIC ); <nl> + if (! s ) <nl> + return - ENOMEM ; <nl> + <nl> skb_reserve ( s , NSHL ); <nl> memcpy ( skb_put ( s , skb -> len ), skb -> data , skb -> len ); <nl> if ( skb -> len & 0x0001 )
static int mpc5121_nfc_probe ( struct platform_device * op ) <nl> chip = & prv -> chip ; <nl>  <nl> mtd -> priv = chip ; <nl> + mtd -> dev . parent = dev ; <nl> chip -> priv = prv ; <nl> prv -> dev = dev ; <nl> 
static inline void ttwu_post_activation ( struct task_struct * p , struct rq * rq , <nl> if ( p -> sched_class -> task_woken ) <nl> p -> sched_class -> task_woken ( rq , p ); <nl>  <nl> - if ( unlikely ( rq -> idle_stamp )) { <nl> + if ( rq -> idle_stamp ) { <nl> u64 delta = rq -> clock - rq -> idle_stamp ; <nl> u64 max = 2 * sysctl_sched_migration_cost ; <nl> 
static int video_open ( struct file * file ) <nl>  <nl> if ( NULL == dev ) { <nl> mutex_unlock (& cx25821_devlist_mutex ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> 
static __inline__ void atomic64_set_mask ( unsigned long mask , atomic64_t * v ) <nl> __CSG_LOOP ( v , mask , " ogr "); <nl> } <nl>  <nl> +# define atomic64_xchg ( v , new ) ( xchg (&(( v )-> counter ), new )) <nl> + <nl> static __inline__ long long atomic64_cmpxchg ( atomic64_t * v , <nl> long long old , long long new ) <nl> {
static int __devinit sta2x11_mfd_probe ( struct pci_dev * pdev , <nl> sta2x11_mfd_setup ( pdev , setup_data ); <nl>  <nl> /* Record this pdev before mfd_add_devices : their probe looks for it */ <nl> - sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl> + if (! sta2x11_mfd_find ( pdev )) <nl> + sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl>  <nl> /* Just 2 bars for all mfd ' s at present */ <nl> for ( i = 0 ; i < 2 ; i ++) {
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) <nl> vnt_mac_set_bssid_addr ( priv , ( u8 *) conf -> bssid ); <nl>  <nl> 
static inline void TCP_ECN_send ( struct sock * sk , struct sk_buff * skb , <nl> */ <nl> static void tcp_init_nondata_skb ( struct sk_buff * skb , u32 seq , u8 flags ) <nl> { <nl> + skb -> ip_summed = CHECKSUM_PARTIAL ; <nl> skb -> csum = 0 ; <nl>  <nl> TCP_SKB_CB ( skb )-> flags = flags ;
static int ethtool_get_eeprom ( struct net_device * dev , void __user * useraddr ) <nl> bytes_remaining -= eeprom . len ; <nl> } <nl>  <nl> + eeprom . len = userbuf - ( useraddr + sizeof ( eeprom )); <nl> + eeprom . offset -= eeprom . len ; <nl> + if ( copy_to_user ( useraddr , & eeprom , sizeof ( eeprom ))) <nl> + ret = - EFAULT ; <nl> + <nl> kfree ( data ); <nl> return ret ; <nl> }
int vb2_mmap ( struct vb2_queue * q , struct vm_area_struct * vma ) <nl> { <nl> unsigned long off = vma -> vm_pgoff << PAGE_SHIFT ; <nl> struct vb2_buffer * vb ; <nl> - unsigned int buffer , plane ; <nl> + unsigned int buffer = 0 , plane = 0 ; <nl> int ret ; <nl> unsigned long length ; <nl> 
static const struct file_operations __fops = { \ <nl> . release = simple_attr_release , \ <nl> . read = debugfs_attr_read , \ <nl> . write = debugfs_attr_write , \ <nl> - . llseek = generic_file_llseek , \ <nl> + . llseek = no_llseek , \ <nl> } <nl>  <nl> # if defined ( CONFIG_DEBUG_FS )
void ip_tunnel_xmit ( struct sk_buff * skb , struct net_device * dev , <nl> tunnel -> err_time + IPTUNNEL_ERR_TIMEO )) { <nl> tunnel -> err_count --; <nl>  <nl> + memset ( IPCB ( skb ), 0 , sizeof (* IPCB ( skb ))); <nl> dst_link_failure ( skb ); <nl> } else <nl> tunnel -> err_count = 0 ;
int test_range_bit ( struct extent_io_tree * tree , u64 start , u64 end , <nl> bitset = 0 ; <nl> break ; <nl> } <nl> + <nl> + if ( state -> end == ( u64 )- 1 ) <nl> + break ; <nl> + <nl> start = state -> end + 1 ; <nl> if ( start > end ) <nl> break ;
__build_packet_message ( struct nfulnl_instance * inst , <nl> } <nl>  <nl> nlh -> nlmsg_len = inst -> skb -> tail - old_tail ; <nl> + inst -> lastnlh = nlh ; <nl> return 0 ; <nl>  <nl> nlmsg_failure :
int evm_update_evmxattr ( struct dentry * dentry , const char * xattr_name , <nl> rc = __vfs_setxattr_noperm ( dentry , XATTR_NAME_EVM , <nl> & xattr_data , <nl> sizeof ( xattr_data ), 0 ); <nl> - } <nl> - else if ( rc == - ENODATA ) <nl> + } else if ( rc == - ENODATA && inode -> i_op -> removexattr ) { <nl> rc = inode -> i_op -> removexattr ( dentry , XATTR_NAME_EVM ); <nl> + } <nl> return rc ; <nl> } <nl> 
void ceph_check_caps ( struct ceph_inode_info * ci , int flags , <nl>  <nl> if ( cap == ci -> i_auth_cap && ci -> i_dirty_caps ) <nl> flushing = __mark_caps_flushing ( inode , session ); <nl> + else <nl> + flushing = 0 ; <nl>  <nl> mds = cap -> mds ; /* remember mds , so we don ' t repeat */ <nl> sent ++;
static void __init setup_bootmem ( void ) <nl> } <nl> memset ( pfnnid_map , 0xff , sizeof ( pfnnid_map )); <nl>  <nl> - for ( i = 0 ; i < npmem_ranges ; i ++) <nl> + for ( i = 0 ; i < npmem_ranges ; i ++) { <nl> + node_set_state ( i , N_NORMAL_MEMORY ); <nl> node_set_online ( i ); <nl> + } <nl> # endif <nl>  <nl> /*
static int get_free_pipe_id_locked ( struct goldfish_pipe_dev * dev ) <nl> /* Reallocate the array */ <nl> u32 new_capacity = 2 * dev -> pipes_capacity ; <nl> struct goldfish_pipe ** pipes = <nl> - kcalloc ( new_capacity , sizeof (* pipes ), GFP_KERNEL ); <nl> + kcalloc ( new_capacity , sizeof (* pipes ), GFP_ATOMIC ); <nl> if (! pipes ) <nl> return - ENOMEM ; <nl> memcpy ( pipes , dev -> pipes , sizeof (* pipes ) * dev -> pipes_capacity );
int bxt_sst_dsp_init ( struct device * dev , void __iomem * mmio_base , int irq , <nl> sst_dsp_mailbox_init ( sst , ( BXT_ADSP_SRAM0_BASE + SKL_ADSP_W0_STAT_SZ ), <nl> SKL_ADSP_W0_UP_SZ , BXT_ADSP_SRAM1_BASE , SKL_ADSP_W1_SZ ); <nl>  <nl> + INIT_LIST_HEAD (& sst -> module_list ); <nl> ret = skl_ipc_init ( dev , skl ); <nl> if ( ret ) <nl> return ret ;
static inline bool kvm_apic_vid_enabled ( struct kvm * kvm ) <nl>  <nl> static inline bool kvm_apic_has_events ( struct kvm_vcpu * vcpu ) <nl> { <nl> - return vcpu -> arch . apic -> pending_events ; <nl> + return kvm_vcpu_has_lapic ( vcpu ) && vcpu -> arch . apic -> pending_events ; <nl> } <nl>  <nl> static inline bool kvm_lowest_prio_delivery ( struct kvm_lapic_irq * irq )
int uwb_rsv_find_best_allocation ( struct uwb_rsv * rsv , struct uwb_mas_bm * availab <nl> int bit_index ; <nl>  <nl> ai = kzalloc ( sizeof ( struct uwb_rsv_alloc_info ), GFP_KERNEL ); <nl> - <nl> + if (! ai ) <nl> + return UWB_RSV_ALLOC_NOT_FOUND ; <nl> ai -> min_mas = rsv -> min_mas ; <nl> ai -> max_mas = rsv -> max_mas ; <nl> ai -> max_interval = rsv -> max_interval ;
static acpi_status setup_res ( struct acpi_resource * acpi_res , void * data ) <nl> struct resource * res = data ; <nl> struct resource_win win ; <nl>  <nl> + /* <nl> + * We might assign this to ' res ' later , make sure all pointers are <nl> + * cleared before the resource is added to the global list <nl> + */ <nl> + memset (& win , 0 , sizeof ( win )); <nl> + <nl> res -> flags = 0 ; <nl> if ( acpi_dev_filter_resource_type ( acpi_res , IORESOURCE_MEM )) <nl> return AE_OK ;
static void atmel_aes_get_cap ( struct atmel_aes_dev * dd ) <nl>  <nl> /* keep only major version number */ <nl> switch ( dd -> hw_version & 0xff0 ) { <nl> + case 0x200 : <nl> + dd -> caps . has_dualbuff = 1 ; <nl> + dd -> caps . has_cfb64 = 1 ; <nl> + dd -> caps . max_burst_size = 4 ; <nl> + break ; <nl> case 0x130 : <nl> dd -> caps . has_dualbuff = 1 ; <nl> dd -> caps . has_cfb64 = 1 ;
static ssize_t hiddev_read ( struct file * file , char __user * buffer , size_t coun <nl> } <nl>  <nl> schedule (); <nl> + set_current_state ( TASK_INTERRUPTIBLE ); <nl> } <nl>  <nl> set_current_state ( TASK_RUNNING );
void brcmf_fweh_process_event ( struct brcmf_pub * drvr , <nl> if ( code != BRCMF_E_IF && ! fweh -> evt_handler [ code ]) <nl> return ; <nl>  <nl> - if ( datalen > BRCMF_DCMD_MAXLEN ) <nl> + if ( datalen > BRCMF_DCMD_MAXLEN || <nl> + datalen + sizeof (* event_packet ) > packet_len ) <nl> return ; <nl>  <nl> if ( in_interrupt ())
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> + if ( count > XFS_ACL_MAX_ENTRIES ) <nl> + return ERR_PTR (- EFSCORRUPTED ); <nl>  <nl> acl = posix_acl_alloc ( count , GFP_KERNEL ); <nl> if (! acl )
int rhashtable_walk_start_check ( struct rhashtable_iter * iter ) <nl> skip ++; <nl> if ( list == iter -> list ) { <nl> iter -> p = p ; <nl> - skip = skip ; <nl> + iter -> skip = skip ; <nl> goto found ; <nl> } <nl> }
int skl_bind_modules ( struct skl_sst * ctx , <nl>  <nl> skl_dump_bind_info ( ctx , src_mcfg , dst_mcfg ); <nl>  <nl> - if ( src_mcfg -> m_state < SKL_MODULE_INIT_DONE && <nl> + if ( src_mcfg -> m_state < SKL_MODULE_INIT_DONE || <nl> dst_mcfg -> m_state < SKL_MODULE_INIT_DONE ) <nl> return 0 ; <nl> 
static int s5p64x0_alloc_gc ( void ) <nl> } <nl>  <nl> ct = gc -> chip_types ; <nl> - ct -> chip . irq_ack = irq_gc_ack ; <nl> + ct -> chip . irq_ack = irq_gc_ack_set_bit ; <nl> ct -> chip . irq_mask = irq_gc_mask_set_bit ; <nl> ct -> chip . irq_unmask = irq_gc_mask_clr_bit ; <nl> ct -> chip . irq_set_type = s5p64x0_irq_eint_set_type ;
static void channel_swdemux_tsklet ( unsigned long data ) <nl> writel ( channel -> back_buffer_busaddr , channel -> irec + <nl> DMA_PRDS_BUSRP_TP ( 0 )); <nl> else <nl> - writel ( wp , channel -> irec + DMA_PRDS_BUSWP_TP ( 0 )); <nl> + writel ( wp , channel -> irec + DMA_PRDS_BUSRP_TP ( 0 )); <nl> } <nl>  <nl> static int c8sectpfe_start_feed ( struct dvb_demux_feed * dvbdmxfeed )
struct mmc_host_ops { <nl>  <nl> int (* start_signal_voltage_switch )( struct mmc_host * host , struct mmc_ios * ios ); <nl>  <nl> + /* Check if the card is pulling dat [ 0 : 3 ] low */ <nl> + int (* card_busy )( struct mmc_host * host ); <nl> + <nl> /* The tuning command opcode value is different for SD and eMMC cards */ <nl> int (* execute_tuning )( struct mmc_host * host , u32 opcode ); <nl> void (* enable_preset_value )( struct mmc_host * host , bool enable );
static void __reada_start_machine ( struct btrfs_fs_info * fs_info ) <nl>  <nl> do { <nl> enqueued = 0 ; <nl> + mutex_lock (& fs_devices -> device_list_mutex ); <nl> list_for_each_entry ( device , & fs_devices -> devices , dev_list ) { <nl> if ( atomic_read (& device -> reada_in_flight ) < <nl> MAX_IN_FLIGHT ) <nl> enqueued += reada_start_machine_dev ( fs_info , <nl> device ); <nl> } <nl> + mutex_unlock (& fs_devices -> device_list_mutex ); <nl> total += enqueued ; <nl> } while ( enqueued && total < 10000 ); <nl> 
static void orinoco_process_scan_results ( struct work_struct * work ) <nl>  <nl> spin_lock_irqsave (& priv -> scan_lock , flags ); <nl> list_for_each_entry_safe ( sd , temp , & priv -> scan_list , list ) { <nl> - spin_unlock_irqrestore (& priv -> scan_lock , flags ); <nl>  <nl> buf = sd -> buf ; <nl> len = sd -> len ; <nl> type = sd -> type ; <nl>  <nl> list_del (& sd -> list ); <nl> + spin_unlock_irqrestore (& priv -> scan_lock , flags ); <nl> kfree ( sd ); <nl>  <nl> if ( len > 0 ) {
static int adxrs450_read_raw ( struct iio_dev * indio_dev , <nl> * val = t ; <nl> ret = IIO_VAL_INT ; <nl> break ; <nl> + case IIO_CHAN_INFO_CALIBBIAS : <nl> + ret = adxrs450_spi_read_reg_16 ( indio_dev , ADXRS450_DNC1 , & t ); <nl> + if ( ret ) <nl> + break ; <nl> + * val = t ; <nl> + ret = IIO_VAL_INT ; <nl> + break ; <nl> default : <nl> ret = - EINVAL ; <nl> break ;
int debug_log ( struct bat_priv * bat_priv , char * fmt , ...) <nl>  <nl> va_start ( args , fmt ); <nl> vscnprintf ( tmp_log_buf , sizeof ( tmp_log_buf ), fmt , args ); <nl> - fdebug_log ( bat_priv -> debug_log , "[% 10u ] % s ", <nl> + fdebug_log ( bat_priv -> debug_log , "[% 10lu ] % s ", <nl> ( jiffies / HZ ), tmp_log_buf ); <nl> va_end ( args ); <nl> 
static int tipc_nl_compat_link_dump ( struct tipc_nl_compat_msg * msg , <nl>  <nl> link_info . dest = nla_get_flag ( link [ TIPC_NLA_LINK_DEST ]); <nl> link_info . up = htonl ( nla_get_flag ( link [ TIPC_NLA_LINK_UP ])); <nl> - strcpy ( link_info . str , nla_data ( link [ TIPC_NLA_LINK_NAME ])); <nl> + nla_strlcpy ( link_info . str , nla_data ( link [ TIPC_NLA_LINK_NAME ]), <nl> + TIPC_MAX_LINK_NAME ); <nl>  <nl> return tipc_add_tlv ( msg -> rep , TIPC_TLV_LINK_INFO , <nl> & link_info , sizeof ( link_info ));
int asn1_ber_decoder ( const struct asn1_decoder * decoder , <nl> if ( unlikely ( len > datalen - dp )) <nl> goto data_overrun_error ; <nl> } <nl> + } else { <nl> + if ( unlikely ( len > datalen - dp )) <nl> + goto data_overrun_error ; <nl> } <nl>  <nl> if ( flags & FLAG_CONS ) {
scsi_reset_provider ( struct scsi_device * dev , int flag ) <nl> rtn = FAILED ; <nl> } <nl>  <nl> - scsi_delete_timer ( scmd ); <nl> scsi_next_command ( scmd ); <nl> return rtn ; <nl> }
static int xgifb_probe ( struct pci_dev * pdev , <nl>  <nl> if ( xgifb_info -> mode_idx < 0 ) { <nl> dev_err (& pdev -> dev , " No supported video mode found \ n "); <nl> + ret = - EINVAL ; <nl> goto error_1 ; <nl> } <nl> 
static int mxs_lradc_ts_register ( struct mxs_lradc * lradc ) <nl> __set_bit ( EV_ABS , input -> evbit ); <nl> __set_bit ( EV_KEY , input -> evbit ); <nl> __set_bit ( BTN_TOUCH , input -> keybit ); <nl> + __set_bit ( INPUT_PROP_DIRECT , input -> propbit ); <nl> input_set_abs_params ( input , ABS_X , 0 , LRADC_SINGLE_SAMPLE_MASK , 0 , 0 ); <nl> input_set_abs_params ( input , ABS_Y , 0 , LRADC_SINGLE_SAMPLE_MASK , 0 , 0 ); <nl> input_set_abs_params ( input , ABS_PRESSURE , 0 , LRADC_SINGLE_SAMPLE_MASK ,
cifs_setlk ( struct file * file , struct file_lock * flock , __u32 type , <nl> rc = server -> ops -> mand_unlock_range ( cfile , flock , xid ); <nl>  <nl> out : <nl> - if ( flock -> fl_flags & FL_POSIX ) <nl> - posix_lock_file_wait ( file , flock ); <nl> + if ( flock -> fl_flags & FL_POSIX && ! rc ) <nl> + rc = posix_lock_file_wait ( file , flock ); <nl> return rc ; <nl> } <nl> 
static int fnic_fcpio_fw_reset_cmpl_handler ( struct fnic * fnic , <nl>  <nl> atomic64_set (& fnic -> fnic_stats . fw_stats . active_fw_reqs , 0 ); <nl> atomic64_set (& fnic -> fnic_stats . io_stats . active_ios , 0 ); <nl> + atomic64_set (& fnic -> io_cmpl_skip , 0 ); <nl>  <nl> spin_lock_irqsave (& fnic -> fnic_lock , flags ); <nl> 
static struct rpmsg_device * rpmsg_virtio_add_ctrl_dev ( struct virtio_device * vdev <nl>  <nl> err = rpmsg_ctrldev_register_device ( rpdev_ctrl ); <nl> if ( err ) { <nl> - kfree ( vch ); <nl> + /* vch will be free in virtio_rpmsg_release_device () */ <nl> return ERR_PTR ( err ); <nl> } <nl> 
static int nodemgr_host_thread ( void * data ) <nl> g = get_hpsb_generation ( host ); <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> msleep_interruptible ( 63 ); <nl> + try_to_freeze (); <nl> if ( kthread_should_stop ()) <nl> goto exit ; <nl>  <nl> static int nodemgr_host_thread ( void * data ) <nl> /* Sleep 3 seconds */ <nl> for ( i = 3000 / 200 ; i ; i --) { <nl> msleep_interruptible ( 200 ); <nl> + try_to_freeze (); <nl> if ( kthread_should_stop ()) <nl> goto exit ; <nl> 
static int crypt_alloc_tfms ( struct crypt_config * cc , char * ciphermode ) <nl> unsigned i ; <nl> int err ; <nl>  <nl> - cc -> tfms = kmalloc ( cc -> tfms_count * sizeof ( struct crypto_skcipher *), <nl> + cc -> tfms = kzalloc ( cc -> tfms_count * sizeof ( struct crypto_skcipher *), <nl> GFP_KERNEL ); <nl> if (! cc -> tfms ) <nl> return - ENOMEM ;
static void ixgbe_clean_rx_irq ( struct ixgbe_q_vector * q_vector , <nl> if ( ixgbe_rx_is_fcoe ( adapter , rx_desc )) { <nl> ddp_bytes = ixgbe_fcoe_ddp ( adapter , rx_desc , skb , <nl> staterr ); <nl> - if (! ddp_bytes ) <nl> + if (! ddp_bytes ) { <nl> + dev_kfree_skb_any ( skb ); <nl> goto next_desc ; <nl> + } <nl> } <nl> # endif /* IXGBE_FCOE */ <nl> ixgbe_receive_skb ( q_vector , skb , staterr , rx_ring , rx_desc );
static void intel_i9xx_setup_flush ( void ) <nl> intel_private . ifp_resource . flags = IORESOURCE_MEM ; <nl>  <nl> /* Setup chipset flush for 915 */ <nl> - if ( IS_I965 || IS_G33 ) { <nl> + if ( IS_I965 || IS_G33 || IS_G4X ) { <nl> intel_i965_g33_setup_chipset_flush (); <nl> } else { <nl> intel_i915_setup_chipset_flush ();
static loff_t fuse_file_llseek ( struct file * file , loff_t offset , int origin ) <nl> mutex_lock (& inode -> i_mutex ); <nl> switch ( origin ) { <nl> case SEEK_END : <nl> + retval = fuse_update_attributes ( inode , NULL , file , NULL ); <nl> + if ( retval ) <nl> + return retval ; <nl> offset += i_size_read ( inode ); <nl> break ; <nl> case SEEK_CUR :
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
static void __init clk_sp810_of_setup ( struct device_node * node ) <nl>  <nl> if ( of_clk_parent_fill ( node , parent_names , num ) != num ) { <nl> pr_warn (" Failed to obtain parent clocks for SP810 !\ n "); <nl> + kfree ( sp810 ); <nl> return ; <nl> } <nl> 
int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
iscsi_if_rx ( struct sk_buff * skb ) <nl> uint32_t group ; <nl>  <nl> nlh = nlmsg_hdr ( skb ); <nl> - if ( nlh -> nlmsg_len < sizeof (* nlh ) || <nl> + if ( nlh -> nlmsg_len < sizeof (* nlh ) + sizeof (* ev ) || <nl> skb -> len < nlh -> nlmsg_len ) { <nl> break ; <nl> }
static int mv643xx_eth_set_mac_address ( struct net_device * dev , void * addr ) <nl> { <nl> struct sockaddr * sa = addr ; <nl>  <nl> + if (! is_valid_ether_addr ( sa -> sa_data )) <nl> + return - EINVAL ; <nl> + <nl> memcpy ( dev -> dev_addr , sa -> sa_data , ETH_ALEN ); <nl>  <nl> netif_addr_lock_bh ( dev );
static void imx_pinconf_group_dbg_show ( struct pinctrl_dev * pctldev , <nl> const char * name ; <nl> int i , ret ; <nl>  <nl> - if ( group > pctldev -> num_groups ) <nl> + if ( group >= pctldev -> num_groups ) <nl> return ; <nl>  <nl> seq_puts ( s , "\ n ");
static int f2fs_write_data_pages ( struct address_space * mapping , <nl> if (! mapping -> a_ops -> writepage ) <nl> return 0 ; <nl>  <nl> + /* skip writing if there is no dirty page in this inode */ <nl> + if (! get_dirty_pages ( inode ) && wbc -> sync_mode == WB_SYNC_NONE ) <nl> + return 0 ; <nl> + <nl> if ( S_ISDIR ( inode -> i_mode ) && wbc -> sync_mode == WB_SYNC_NONE && <nl> get_dirty_pages ( inode ) < nr_pages_to_skip ( sbi , DATA ) && <nl> available_free_memory ( sbi , DIRTY_DENTS ))
static int __iwl_mvm_suspend ( struct ieee80211_hw * hw , <nl> out : <nl> if ( ret < 0 ) { <nl> iwl_mvm_ref ( mvm , IWL_MVM_REF_UCODE_DOWN ); <nl> - ieee80211_restart_hw ( mvm -> hw ); <nl> + if ( mvm -> restart_fw > 0 ) { <nl> + mvm -> restart_fw --; <nl> + ieee80211_restart_hw ( mvm -> hw ); <nl> + } <nl> iwl_mvm_free_nd ( mvm ); <nl> } <nl> out_noreset :
static int btrfs_real_readdir ( struct file * filp , void * dirent , <nl>  <nl> /* Reached end of directory / root . Bump pos past the last item . */ <nl> if ( key_type == BTRFS_DIR_INDEX_KEY ) <nl> - filp -> f_pos = INT_LIMIT ( off_t ); <nl> + /* <nl> + * 32 - bit glibc will use getdents64 , but then strtol - <nl> + * so the last number we can serve is this . <nl> + */ <nl> + filp -> f_pos = 0x7fffffff ; <nl> else <nl> filp -> f_pos ++; <nl> nopos :
static int sdhci_st_probe ( struct platform_device * pdev ) <nl> if ( IS_ERR ( icnclk )) <nl> icnclk = NULL ; <nl>  <nl> - rstc = devm_reset_control_get (& pdev -> dev , NULL ); <nl> + rstc = devm_reset_control_get_exclusive (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( rstc )) <nl> rstc = NULL ; <nl> else
int snd_soc_register_dais ( struct device * dev , <nl> pr_debug (" Registered DAI '% s '\ n ", dai -> name ); <nl> } <nl>  <nl> + mutex_lock (& client_mutex ); <nl> snd_soc_instantiate_cards (); <nl> + mutex_unlock (& client_mutex ); <nl> return 0 ; <nl>  <nl> err :
static void conf_message_callback ( const char * fmt , va_list ap ) <nl>  <nl> static void show_help ( struct menu * menu ) <nl> { <nl> - struct gstr help = str_new (); <nl> + struct gstr help ; <nl> + <nl> + if (! menu ) <nl> + return ; <nl> + <nl> + help = str_new (); <nl> menu_get_ext_help ( menu , & help ); <nl> show_scroll_win ( main_window , _ ( menu_get_prompt ( menu )), str_get (& help )); <nl> str_free (& help );
static int logi_dj_ll_raw_request ( struct hid_device * hid , <nl> if (! out_buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( count < DJREPORT_SHORT_LENGTH - 2 ) <nl> + if ( count > DJREPORT_SHORT_LENGTH - 2 ) <nl> count = DJREPORT_SHORT_LENGTH - 2 ; <nl>  <nl> out_buf [ 0 ] = REPORT_ID_DJ_SHORT ;
struct el3_private { <nl> spinlock_t lock ; <nl> }; <nl>  <nl> - static const char * if_names [] = { " auto ", " 10baseT ", " 10base2 ", " AUI " }; <nl> + static const char * if_names [] = { " auto ", " 10base2 ", " 10baseT ", " AUI " }; <nl>  <nl> /*====================================================================*/ <nl> 
static int sc16is7xx_probe ( struct device * dev , <nl> else <nl> return PTR_ERR ( s -> clk ); <nl> } else { <nl> + clk_prepare_enable ( s -> clk ); <nl> freq = clk_get_rate ( s -> clk ); <nl> } <nl> 
static void udf_sb_free_partitions ( struct super_block * sb ) <nl> { <nl> struct udf_sb_info * sbi = UDF_SB ( sb ); <nl> int i ; <nl> - <nl> + if ( sbi -> s_partmaps == NULL ) <nl> + return ; <nl> for ( i = 0 ; i < sbi -> s_partitions ; i ++) <nl> udf_free_partition (& sbi -> s_partmaps [ i ]); <nl> kfree ( sbi -> s_partmaps );
int dw_hdmi_probe ( struct platform_device * pdev , <nl> const struct dw_hdmi_plat_data * plat_data ) <nl> { <nl> struct dw_hdmi * hdmi ; <nl> - int ret ; <nl>  <nl> hdmi = __dw_hdmi_probe ( pdev , plat_data ); <nl> if ( IS_ERR ( hdmi )) <nl> return PTR_ERR ( hdmi ); <nl>  <nl> - ret = drm_bridge_add (& hdmi -> bridge ); <nl> - if ( ret < 0 ) { <nl> - __dw_hdmi_remove ( hdmi ); <nl> - return ret ; <nl> - } <nl> + drm_bridge_add (& hdmi -> bridge ); <nl>  <nl> return 0 ; <nl> }
extern int vdso_enabled ; <nl>  <nl> # endif /* ! CONFIG_X86_32 */ <nl>  <nl> +# define CORE_DUMP_USE_REGSET <nl> # define USE_ELF_CORE_DUMP <nl> # define ELF_EXEC_PAGESIZE 4096 <nl> 
static inline int logfs_get_sb_bdev ( struct logfs_super * s , <nl>  <nl> /* dev_mtd . c */ <nl> # ifdef CONFIG_MTD <nl> - int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> + int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ); <nl> # else <nl> static inline int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> {
int kvm_timer_enable ( struct kvm_vcpu * vcpu ) <nl> return ret ; <nl>  <nl> no_vgic : <nl> + preempt_disable (); <nl> timer -> enabled = 1 ; <nl> + kvm_timer_vcpu_load_vgic ( vcpu ); <nl> + preempt_enable (); <nl> + <nl> return 0 ; <nl> } <nl> 
int hfsplus_find_cat ( struct super_block * sb , u32 cnid , <nl> return - EIO ; <nl> } <nl>  <nl> + if ( be16_to_cpu ( tmp . thread . nodeName . length ) > 255 ) { <nl> + printk ( KERN_ERR " hfs : catalog name length corrupted \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> hfsplus_cat_build_key_uni ( fd -> search_key , be32_to_cpu ( tmp . thread . parentID ), <nl> & tmp . thread . nodeName ); <nl> return hfs_brec_find ( fd );
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> goto cleanup ; <nl> } <nl>  <nl> - user_srbcmd = kmalloc ( GFP_KERNEL , fibsize ); <nl> + user_srbcmd = kmalloc ( fibsize , GFP_KERNEL ); <nl> if (! user_srbcmd ) { <nl> dprintk (( KERN_DEBUG " aacraid : Could not make a copy of the srb \ n ")); <nl> rcode = - ENOMEM ;
static void zynqmp_gqspi_selectslave ( struct zynqmp_qspi * instanceptr , <nl> case GQSPI_SELECT_FLASH_CS_BOTH : <nl> instanceptr -> genfifocs = GQSPI_GENFIFO_CS_LOWER | <nl> GQSPI_GENFIFO_CS_UPPER ; <nl> + break ; <nl> case GQSPI_SELECT_FLASH_CS_UPPER : <nl> instanceptr -> genfifocs = GQSPI_GENFIFO_CS_UPPER ; <nl> break ;
static void raise_dtr ( struct isi_port * port ) <nl> } <nl>  <nl> /* card -> lock HAS to be held */ <nl> - static inline void drop_dtr ( struct isi_port * port ) <nl> + static void drop_dtr ( struct isi_port * port ) <nl> { <nl> struct isi_board * card = port -> card ; <nl> unsigned long base = card -> base ;
static void ath9k_htc_send_buffered ( struct ath9k_htc_priv * priv , <nl> } <nl>  <nl> tx_slot = ath9k_htc_tx_get_slot ( priv ); <nl> - if ( tx_slot != 0 ) { <nl> + if ( tx_slot < 0 ) { <nl> ath_dbg ( common , ATH_DBG_XMIT , " No free CAB slot \ n "); <nl> dev_kfree_skb_any ( skb ); <nl> goto next ;
static ktime_t tick_nohz_stop_sched_tick ( struct tick_sched * ts , <nl> */ <nl> if ( delta == 0 ) { <nl> tick_nohz_restart ( ts , now ); <nl> + /* <nl> + * Make sure next tick stop doesn ' t get fooled by past <nl> + * clock deadline <nl> + */ <nl> + ts -> next_tick = 0 ; <nl> goto out ; <nl> } <nl> }
static void pvr2_hdw_state_log_state ( struct pvr2_hdw * hdw ) <nl> printk ( KERN_INFO "% s %.* s \ n ", hdw -> name , ccnt , buf ); <nl> } <nl> ccnt = pvr2_hdw_report_clients ( hdw , buf , sizeof ( buf )); <nl> + if ( ccnt >= sizeof ( buf )) <nl> + ccnt = sizeof ( buf ); <nl> + <nl> ucnt = 0 ; <nl> while ( ucnt < ccnt ) { <nl> lcnt = 0 ;
static noinline long btrfs_ioctl_start_sync ( struct file * file , void __user * argp <nl> return PTR_ERR ( trans ); <nl> transid = trans -> transid ; <nl> ret = btrfs_commit_transaction_async ( trans , root , 0 ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + btrfs_end_transaction ( trans , root ); <nl> return ret ; <nl> + } <nl>  <nl> if ( argp ) <nl> if ( copy_to_user ( argp , & transid , sizeof ( transid )))
static noinline int btrfs_ioctl_resize ( struct btrfs_root * root , <nl> } <nl> ret = btrfs_grow_device ( trans , device , new_size ); <nl> btrfs_commit_transaction ( trans , root ); <nl> - } else { <nl> + } else if ( new_size < old_size ) { <nl> ret = btrfs_shrink_device ( device , new_size ); <nl> } <nl> 
static int tegra_pcie_probe ( struct platform_device * pdev ) <nl> struct pci_bus * child ; <nl> int err ; <nl>  <nl> - host = pci_alloc_host_bridge ( sizeof (* pcie )); <nl> + host = devm_pci_alloc_host_bridge ( dev , sizeof (* pcie )); <nl> if (! host ) <nl> return - ENOMEM ; <nl> 
long drm_ioctl ( struct file * filp , <nl> retcode = - EFAULT ; <nl> goto err_i1 ; <nl> } <nl> - } <nl> + } else <nl> + memset ( kdata , 0 , _IOC_SIZE ( cmd )); <nl> + <nl> if ( ioctl -> flags & DRM_UNLOCKED ) <nl> retcode = func ( dev , kdata , file_priv ); <nl> else {
int perf_cpu_time_max_percent_handler ( struct ctl_table * table , int write , <nl> void __user * buffer , size_t * lenp , <nl> loff_t * ppos ) <nl> { <nl> - int ret = proc_dointvec ( table , write , buffer , lenp , ppos ); <nl> + int ret = proc_dointvec_minmax ( table , write , buffer , lenp , ppos ); <nl>  <nl> if ( ret || ! write ) <nl> return ret ;
int kvm_vm_ioctl_check_extension ( struct kvm * kvm , long ext ) <nl> break ; <nl> # endif <nl> case KVM_CAP_PPC_HTM : <nl> - r = cpu_has_feature ( CPU_FTR_TM_COMP ) && <nl> - is_kvmppc_hv_enabled ( kvm ); <nl> + r = cpu_has_feature ( CPU_FTR_TM_COMP ) && hv_enabled ; <nl> break ; <nl> default : <nl> r = 0 ;
static int fn_hash_insert ( struct fib_table * tb , struct fib_config * cfg ) <nl> struct fib_info * fi_drop ; <nl> u8 state ; <nl>  <nl> + if ( fi -> fib_treeref > 1 ) <nl> + goto out ; <nl> + <nl> write_lock_bh (& fib_hash_lock ); <nl> fi_drop = fa -> fa_info ; <nl> fa -> fa_info = fi ;
static int atmel_hlcdc_plane_init_properties ( struct atmel_hlcdc_plane * plane , <nl> drm_object_attach_property (& plane -> base . base , <nl> props -> alpha , 255 ); <nl>  <nl> - if ( desc -> layout . xstride && desc -> layout . pstride ) { <nl> + if ( desc -> layout . xstride [ 0 ] && desc -> layout . pstride [ 0 ]) { <nl> int ret ; <nl>  <nl> ret = drm_plane_create_rotation_property (& plane -> base ,
static int vpbe_display_g_register ( struct file * file , void * priv , <nl> struct v4l2_dbg_register * reg ) <nl> { <nl> struct v4l2_dbg_match * match = & reg -> match ; <nl> + struct vpbe_fh * fh = file -> private_data ; <nl> + struct vpbe_device * vpbe_dev = fh -> disp_dev -> vpbe_dev ; <nl>  <nl> if ( match -> type >= 2 ) { <nl> v4l2_subdev_call ( vpbe_dev -> venc ,
static int wcn36xx_start ( struct ieee80211_hw * hw ) <nl> wcn36xx_smd_stop ( wcn ); <nl> out_free_smd_buf : <nl> kfree ( wcn -> hal_buf ); <nl> - out_free_dxe_pool : <nl> - wcn36xx_dxe_free_mem_pools ( wcn ); <nl> out_free_dxe_ctl : <nl> wcn36xx_dxe_free_ctl_blks ( wcn ); <nl> + out_free_dxe_pool : <nl> + wcn36xx_dxe_free_mem_pools ( wcn ); <nl> out_smd_close : <nl> wcn36xx_smd_close ( wcn ); <nl> out_err :
static int stmmac_open ( struct net_device * dev ) <nl> if ( ret ) { <nl> pr_err ("% s : Cannot attach to PHY ( error : % d )\ n ", <nl> __func__ , ret ); <nl> - goto phy_error ; <nl> + return ret ; <nl> } <nl> } <nl>  <nl> static int stmmac_open ( struct net_device * dev ) <nl> dma_desc_error : <nl> if ( priv -> phydev ) <nl> phy_disconnect ( priv -> phydev ); <nl> - phy_error : <nl> - clk_disable_unprepare ( priv -> stmmac_clk ); <nl>  <nl> return ret ; <nl> }
static int mmci_probe ( struct amba_device * dev , <nl> dev_dbg ( mmc_dev ( mmc ), " clocking block at % u Hz \ n ", mmc -> f_max ); <nl>  <nl> /* Get regulators and the supported OCR mask */ <nl> - mmc_regulator_get_supply ( mmc ); <nl> + ret = mmc_regulator_get_supply ( mmc ); <nl> + if ( ret == - EPROBE_DEFER ) <nl> + goto clk_disable ; <nl> + <nl> if (! mmc -> ocr_avail ) <nl> mmc -> ocr_avail = plat -> ocr_mask ; <nl> else if ( plat -> ocr_mask )
static struct hardwall_info * hardwall_create ( <nl> /* Allocate a new rectangle optimistically . */ <nl> rect = kmalloc ( sizeof ( struct hardwall_info ), <nl> GFP_KERNEL | __GFP_ZERO ); <nl> + if ( rect == NULL ) <nl> + return ERR_PTR (- ENOMEM ); <nl> INIT_LIST_HEAD (& rect -> task_head ); <nl>  <nl> /* Compute the rectangle size and validate that it ' s plausible . */
int host_int_set_wep_default_key ( struct host_if_drv * hif_drv , u8 index ); <nl> * @ date 8 March 2012 <nl> * @ version 1 . 0 <nl> */ <nl> - int host_int_add_wep_key_bss_sta ( struct host_if_drv * hWFIDrv , const u8 * pu8WepKey , u8 u8WepKeylen , u8 u8Keyidx ); <nl> + int host_int_add_wep_key_bss_sta ( struct host_if_drv * hif_drv , const u8 * pu8WepKey , u8 u8WepKeylen , u8 u8Keyidx ); <nl> /** <nl> * @ brief host_int_add_wep_key_bss_ap <nl> * @ details valid only in AP mode if External Supplicant support is enabled .
static ssize_t hidraw_get_report ( struct file * file , char __user * buffer , size_t <nl> int ret = 0 , len ; <nl> unsigned char report_number ; <nl>  <nl> + if (! hidraw_table [ minor ] || ! hidraw_table [ minor ]-> exist ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl> + <nl> dev = hidraw_table [ minor ]-> hid ; <nl>  <nl> if (! dev -> ll_driver -> raw_request ) {
static struct gfs2_leaf * new_leaf ( struct inode * inode , struct buffer_head ** pbh , <nl> leaf = ( struct gfs2_leaf *) bh -> b_data ; <nl> leaf -> lf_depth = cpu_to_be16 ( depth ); <nl> leaf -> lf_entries = 0 ; <nl> - leaf -> lf_dirent_format = cpu_to_be16 ( GFS2_FORMAT_DE ); <nl> + leaf -> lf_dirent_format = cpu_to_be32 ( GFS2_FORMAT_DE ); <nl> leaf -> lf_next = 0 ; <nl> memset ( leaf -> lf_reserved , 0 , sizeof ( leaf -> lf_reserved )); <nl> dent = ( struct gfs2_dirent *)( leaf + 1 );
static int arizona_runtime_resume ( struct device * dev ) <nl> return ret ; <nl> } <nl>  <nl> - regcache_sync ( arizona -> regmap ); <nl> + ret = regcache_sync ( arizona -> regmap ); <nl> + if ( ret != 0 ) { <nl> + dev_err ( arizona -> dev , " Failed to restore register cache \ n "); <nl> + regulator_disable ( arizona -> dcvdd ); <nl> + return ret ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static int find_data_references ( struct reloc_control * rc , <nl> } <nl>  <nl> key . objectid = ref_objectid ; <nl> - key . offset = ref_offset ; <nl> key . type = BTRFS_EXTENT_DATA_KEY ; <nl> + if ( ref_offset > (( u64 )- 1 << 32 )) <nl> + key . offset = 0 ; <nl> + else <nl> + key . offset = ref_offset ; <nl>  <nl> path -> search_commit_root = 1 ; <nl> path -> skip_locking = 1 ;
static inline struct dma_chan <nl> if ( chan ) <nl> return chan ; <nl>  <nl> + if (! fn || ! fn_param ) <nl> + return NULL ; <nl> + <nl> return __dma_request_channel ( mask , fn , fn_param ); <nl> } <nl> # endif /* DMAENGINE_H */
static void _rtl_usb_rx_process_noagg ( struct ieee80211_hw * hw , <nl> ieee80211_rx ( hw , skb ); <nl> else <nl> dev_kfree_skb_any ( skb ); <nl> + } else { <nl> + dev_kfree_skb_any ( skb ); <nl> } <nl> } <nl> 
static void picolcd_debug_out_report ( struct picolcd_data * data , <nl> report -> id , raw_size ); <nl> hid_debug_event ( hdev , buff ); <nl> if ( raw_size + 5 > sizeof ( raw_data )) { <nl> + kfree ( buff ); <nl> hid_debug_event ( hdev , " TOO BIG \ n "); <nl> return ; <nl> } else {
void ath6kl_rx ( struct htc_target * target , struct htc_packet * packet ) <nl> /* aggregation code will handle the skb */ <nl> return ; <nl> } <nl> - } <nl> + } else if (! is_broadcast_ether_addr ( datap -> h_dest )) <nl> + vif -> net_stats . multicast ++; <nl>  <nl> ath6kl_deliver_frames_to_nw_stack ( vif -> ndev , skb ); <nl> }
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> - memset ( sax , 0 , sizeof ( sax )); <nl> + memset ( sax , 0 , sizeof (* sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
static void unicast_arp_send ( struct sk_buff * skb , struct net_device * dev , <nl> skb_push ( skb , sizeof * phdr ); <nl> __skb_queue_tail (& path -> queue , skb ); <nl>  <nl> - if ( path_rec_start ( dev , path )) { <nl> + if (! path -> query && path_rec_start ( dev , path )) { <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl> path_free ( dev , path ); <nl> return ;
typedef __s64 int64_t ; <nl> # endif <nl>  <nl> /* this is a special 64bit data type that is 8 - byte aligned */ <nl> -# define aligned_u64 unsigned long long __attribute__ (( aligned ( 8 ))) <nl> +# define aligned_u64 __u64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_be64 __be64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_le64 __le64 __attribute__ (( aligned ( 8 ))) <nl> 
void __init kmem_cache_init ( void ) <nl> /* Allocate two kmem_caches from the page allocator */ <nl> kmalloc_size = ALIGN ( kmem_size , cache_line_size ()); <nl> order = get_order ( 2 * kmalloc_size ); <nl> - kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT , order ); <nl> + kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT | __GFP_ZERO , order ); <nl>  <nl> /* <nl> * Must first have the slab cache available for the allocations of the
static long compat_ipmi_ioctl ( struct file * filep , unsigned int cmd , <nl> struct ipmi_recv __user * precv64 ; <nl> struct ipmi_recv recv64 ; <nl>  <nl> + memset (& recv64 , 0 , sizeof ( recv64 )); <nl> if ( get_compat_ipmi_recv (& recv64 , compat_ptr ( arg ))) <nl> return - EFAULT ; <nl> 
static int replace_map_fd_with_map_ptr ( struct verifier_env * env ) <nl> if ( IS_ERR ( map )) { <nl> verbose (" fd % d is not pointing to valid bpf_map \ n ", <nl> insn -> imm ); <nl> - fdput ( f ); <nl> return PTR_ERR ( map ); <nl> } <nl> 
static void tcm_loop_submission_work ( struct work_struct * work ) <nl> return ; <nl>  <nl> out_done : <nl> + kmem_cache_free ( tcm_loop_cmd_cache , tl_cmd ); <nl> sc -> scsi_done ( sc ); <nl> return ; <nl> }
static int GLOB_SBD_init ( void ) <nl> int i ; <nl>  <nl> /* Set debug output level ( 0 ~ 3 ) here . 3 is most verbose */ <nl> - nand_debug_level = 0 ; <nl> - <nl> printk ( KERN_ALERT " Spectra : % s \ n ", GLOB_version ); <nl>  <nl> mutex_init (& spectra_lock );
static long gpio_ioctl ( struct file * filp , unsigned int cmd , unsigned long arg ) <nl> if ( cmd == GPIO_GET_CHIPINFO_IOCTL ) { <nl> struct gpiochip_info chipinfo ; <nl>  <nl> + memset (& chipinfo , 0 , sizeof ( chipinfo )); <nl> + <nl> strncpy ( chipinfo . name , dev_name (& gdev -> dev ), <nl> sizeof ( chipinfo . name )); <nl> chipinfo . name [ sizeof ( chipinfo . name )- 1 ] = '\ 0 ';
int fib_nh_match ( struct fib_config * cfg , struct fib_info * fi , <nl> fi -> fib_nh , cfg , extack )) <nl> return 1 ; <nl> } <nl> +# ifdef CONFIG_IP_ROUTE_CLASSID <nl> + if ( cfg -> fc_flow && <nl> + cfg -> fc_flow != fi -> fib_nh -> nh_tclassid ) <nl> + return 1 ; <nl> +# endif <nl> if ((! cfg -> fc_oif || cfg -> fc_oif == fi -> fib_nh -> nh_oif ) && <nl> (! cfg -> fc_gw || cfg -> fc_gw == fi -> fib_nh -> nh_gw )) <nl> return 0 ;
static void ufs_mtk_init_va09_pwr_ctrl ( struct ufs_hba * hba ) <nl> struct ufs_mtk_host * host = ufshcd_get_variant ( hba ); <nl>  <nl> host -> reg_va09 = regulator_get ( hba -> dev , " va09 "); <nl> - if (! host -> reg_va09 ) <nl> + if ( IS_ERR ( host -> reg_va09 )) <nl> dev_info ( hba -> dev , " failed to get va09 "); <nl> else <nl> host -> caps |= UFS_MTK_CAP_VA09_PWR_CTRL ;
static unsigned int xdr_set_page_base ( struct xdr_stream * xdr , <nl> void * kaddr ; <nl>  <nl> maxlen = xdr -> buf -> page_len ; <nl> - if ( base >= maxlen ) { <nl> - base = maxlen ; <nl> - maxlen = 0 ; <nl> - } else <nl> + if ( base >= maxlen ) <nl> + return 0 ; <nl> + else <nl> maxlen -= base ; <nl> if ( len > maxlen ) <nl> len = maxlen ;
drm_est3_modes ( struct drm_connector * connector , struct detailed_timing * timing ) <nl> u8 * est = (( u8 *) timing ) + 5 ; <nl>  <nl> for ( i = 0 ; i < 6 ; i ++) { <nl> - for ( j = 7 ; j > 0 ; j --) { <nl> + for ( j = 7 ; j >= 0 ; j --) { <nl> m = ( i * 8 ) + ( 7 - j ); <nl> if ( m >= ARRAY_SIZE ( est3_modes )) <nl> break ;
static int trunc_start ( struct gfs2_inode * ip , u64 size ) <nl>  <nl> if ( gfs2_is_stuffed ( ip )) { <nl> u64 dsize = size + sizeof ( struct gfs2_inode ); <nl> + ip -> i_disksize = size ; <nl> ip -> i_inode . i_mtime = ip -> i_inode . i_ctime = CURRENT_TIME ; <nl> gfs2_trans_add_bh ( ip -> i_gl , dibh , 1 ); <nl> gfs2_dinode_out ( ip , dibh -> b_data );
ips_link_to_i915_driver ( void ) <nl> EXPORT_SYMBOL_GPL ( ips_link_to_i915_driver ); <nl>  <nl> static const struct pci_device_id ips_id_table [] = { <nl> - { PCI_DEVICE ( PCI_VENDOR_ID_INTEL , <nl> - PCI_DEVICE_ID_INTEL_THERMAL_SENSOR ), }, <nl> + { PCI_VDEVICE ( INTEL , PCI_DEVICE_ID_INTEL_THERMAL_SENSOR ), }, <nl> { 0 , } <nl> }; <nl> 
static int __init cell_iommu_fixed_mapping_init ( void ) <nl> fbase = _ALIGN_UP ( fbase , 1 << IO_SEGMENT_SHIFT ); <nl> fsize = lmb_phys_mem_size (); <nl>  <nl> - if (( fbase + fsize ) <= 0x800000000 ) <nl> + if (( fbase + fsize ) <= 0x800000000ul ) <nl> hbase = 0 ; /* use the device tree window */ <nl> else { <nl> /* If we ' re over 32 GB we need to cheat . We can ' t map all of
struct rxrpc_connection * rxrpc_find_service_conn_rcu ( struct rxrpc_peer * peer , <nl> else if ( conn -> proto . index_key > k . index_key ) <nl> p = rcu_dereference_raw ( p -> rb_right ); <nl> else <nl> - goto done ; <nl> + break ; <nl> conn = NULL ; <nl> } <nl> } while ( need_seqretry (& peer -> service_conn_lock , seq )); <nl>  <nl> - done : <nl> done_seqretry (& peer -> service_conn_lock , seq ); <nl> _leave (" = % d ", conn ? conn -> debug_id : - 1 ); <nl> return conn ;
void dlm_add_cb ( struct dlm_lkb * lkb , uint32_t flags , int mode , int status , <nl>  <nl> spin_lock (& dlm_cb_seq_spin ); <nl> new_seq = ++ dlm_cb_seq ; <nl> + if (! dlm_cb_seq ) <nl> + new_seq = ++ dlm_cb_seq ; <nl> spin_unlock (& dlm_cb_seq_spin ); <nl>  <nl> if ( lkb -> lkb_flags & DLM_IFL_USER ) {
receive_buf ( struct tty_struct * tty , struct tty_buffer * head , int count ) <nl> count = disc -> ops -> receive_buf2 ( tty , p , f , count ); <nl> else { <nl> count = min_t ( int , count , tty -> receive_room ); <nl> - if ( count ) <nl> + if ( count && disc -> ops -> receive_buf ) <nl> disc -> ops -> receive_buf ( tty , p , f , count ); <nl> } <nl> return count ;
static void ax25_kill_by_device ( struct net_device * dev ) <nl> lock_sock ( sk ); <nl> s -> ax25_dev = NULL ; <nl> ax25_dev_put ( ax25_dev ); <nl> - release_sock ( sk ); <nl> ax25_disconnect ( s , ENETUNREACH ); <nl> + release_sock ( sk ); <nl> spin_lock_bh (& ax25_list_lock ); <nl> sock_put ( sk ); <nl> /* The entry could have been deleted from the
struct inet_connection_sock { <nl>  <nl> u32 probe_timestamp ; <nl> } icsk_mtup ; <nl> - u32 icsk_ca_priv [ 16 ]; <nl> u32 icsk_user_timeout ; <nl> -# define ICSK_CA_PRIV_SIZE ( 16 * sizeof ( u32 )) <nl> + <nl> + u64 icsk_ca_priv [ 64 / sizeof ( u64 )]; <nl> +# define ICSK_CA_PRIV_SIZE ( 8 * sizeof ( u64 )) <nl> }; <nl>  <nl> # define ICSK_TIME_RETRANS 1 /* Retransmit timer */
int perf_uprobe_init ( struct perf_event * p_event , bool is_retprobe ) <nl> return - ENOMEM ; <nl> ret = strncpy_from_user ( <nl> path , u64_to_user_ptr ( p_event -> attr . uprobe_path ), PATH_MAX ); <nl> + if ( ret == PATH_MAX ) <nl> + return - E2BIG ; <nl> if ( ret < 0 ) <nl> goto out ; <nl> if ( path [ 0 ] == '\ 0 ') {
static struct shash_alg alg = { <nl> . export = md5_export , <nl> . import = md5_import , <nl> . descsize = sizeof ( struct md5_state ), <nl> + . statesize = sizeof ( struct md5_state ), <nl> . base = { <nl> . cra_name = " md5 ", <nl> . cra_flags = CRYPTO_ALG_TYPE_SHASH ,
static int handle_conflicting_encoders ( struct drm_atomic_state * state , <nl>  <nl> if ( funcs -> atomic_best_encoder ) <nl> new_encoder = funcs -> atomic_best_encoder ( connector , conn_state ); <nl> - else <nl> + else if ( funcs -> best_encoder ) <nl> new_encoder = funcs -> best_encoder ( connector ); <nl> + else <nl> + new_encoder = drm_atomic_helper_best_encoder ( connector ); <nl>  <nl> if ( new_encoder ) { <nl> if ( encoder_mask & ( 1 << drm_encoder_index ( new_encoder ))) {
* under normal circumstances , used to verify that nobody uses <nl> * non - initialized list entries . <nl> */ <nl> -# define LIST_POISON1 (( void *) 0x00100100 + POISON_POINTER_DELTA ) <nl> -# define LIST_POISON2 (( void *) 0x00200200 + POISON_POINTER_DELTA ) <nl> +# define LIST_POISON1 (( void *) 0x100 + POISON_POINTER_DELTA ) <nl> +# define LIST_POISON2 (( void *) 0x200 + POISON_POINTER_DELTA ) <nl>  <nl> /********** include / linux / timer . h **********/ <nl> /*
xfs_dir2_leafn_remove ( <nl> /* <nl> * One less used entry in the free table . <nl> */ <nl> - free -> hdr . nused = cpu_to_be32 (- 1 ); <nl> + be32_add (& free -> hdr . nused , - 1 ); <nl> xfs_dir2_free_log_header ( tp , fbp ); <nl> /* <nl> * If this was the last entry in the table , we can
static int ibmvscsi_probe ( struct vio_dev * vdev , const struct vio_device_id * id ) <nl> host -> max_lun = 8 ; <nl> host -> max_id = max_id ; <nl> host -> max_channel = max_channel ; <nl> + host -> max_cmd_len = 16 ; <nl>  <nl> if ( scsi_add_host ( hostdata -> host , hostdata -> dev )) <nl> goto add_host_failed ;
static int perf_event__repipe_attr ( struct perf_tool * tool , <nl> union perf_event * event , <nl> struct perf_evlist ** pevlist ) <nl> { <nl> + struct perf_inject * inject = container_of ( tool , struct perf_inject , <nl> + tool ); <nl> int ret ; <nl>  <nl> ret = perf_event__process_attr ( tool , event , pevlist ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if (! inject -> pipe_output ) <nl> + return 0 ; <nl> + <nl> return perf_event__repipe_synth ( tool , event ); <nl> } <nl> 
ath5k_deinit_softc ( struct ath5k_softc * sc ) <nl> * state and potentially want to use them . <nl> */ <nl> ath5k_hw_deinit ( sc -> ah ); <nl> + kfree ( sc -> ah ); <nl> free_irq ( sc -> irq , sc ); <nl> } <nl> 
static int ieee80211_get_key ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> if ( pairwise && key_idx < NUM_DEFAULT_KEYS ) <nl> key = rcu_dereference ( sta -> ptk [ key_idx ]); <nl> - else if (! pairwise && key_idx < NUM_DEFAULT_KEYS ) <nl> + else if (! pairwise && <nl> + key_idx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ) <nl> key = rcu_dereference ( sta -> gtk [ key_idx ]); <nl> } else <nl> key = rcu_dereference ( sdata -> keys [ key_idx ]);
static void i40e_service_task ( struct work_struct * work ) <nl> service_task ); <nl> unsigned long start_time = jiffies ; <nl>  <nl> + /* don ' t bother with service tasks if a reset is in progress */ <nl> + if ( test_bit ( __I40E_RESET_RECOVERY_PENDING , & pf -> state )) { <nl> + i40e_service_event_complete ( pf ); <nl> + return ; <nl> + } <nl> + <nl> i40e_reset_subtask ( pf ); <nl> i40e_handle_mdd_event ( pf ); <nl> i40e_vc_process_vflr_event ( pf );
static int poll_select_copy_remaining ( struct timespec * end_time , void __user * p , <nl> rts . tv_sec = rts . tv_nsec = 0 ; <nl>  <nl> if ( timeval ) { <nl> + if ( sizeof ( rtv ) > sizeof ( rtv . tv_sec ) + sizeof ( rtv . tv_usec )) <nl> + memset (& rtv , 0 , sizeof ( rtv )); <nl> rtv . tv_sec = rts . tv_sec ; <nl> rtv . tv_usec = rts . tv_nsec / NSEC_PER_USEC ; <nl> 
unsigned long perf_instruction_pointer ( struct pt_regs * regs ) <nl> bool use_siar = regs_use_siar ( regs ); <nl> unsigned long siar = mfspr ( SPRN_SIAR ); <nl>  <nl> - if ( ppmu -> flags & PPMU_P10_DD1 ) { <nl> + if ( ppmu && ( ppmu -> flags & PPMU_P10_DD1 )) { <nl> if ( siar ) <nl> return siar ; <nl> else
int vt_do_kdskled ( int console , int cmd , unsigned long arg , int perm ) <nl> kbd -> default_ledflagstate = (( arg >> 4 ) & 7 ); <nl> set_leds (); <nl> spin_unlock_irqrestore (& kbd_event_lock , flags ); <nl> - break ; <nl> + return 0 ; <nl>  <nl> /* the ioctls below only set the lights , not the functions */ <nl> /* for those , see KDGKBLED and KDSKBLED above */
struct dentry * ovl_lookup ( struct inode * dir , struct dentry * dentry , <nl> } <nl>  <nl> if ( d . redirect ) { <nl> + err = - ENOMEM ; <nl> upperredirect = kstrdup ( d . redirect , GFP_KERNEL ); <nl> if (! upperredirect ) <nl> goto out_put_upper ;
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl>  <nl> void oprofile_arch_exit ( void ) <nl> { <nl> - model -> exit (); <nl> + if ( model ) <nl> + model -> exit (); <nl> }
int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> if ( IS_ERR ( device )) { <nl> if ( PTR_ERR ( device ) == - ENOENT && <nl> - strcmp ( device_path , " missing ") == 0 ) <nl> + device_path && strcmp ( device_path , " missing ") == 0 ) <nl> ret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND ; <nl> else <nl> ret = PTR_ERR ( device );
static int pch_dma_probe ( struct pci_dev * pdev , <nl>  <nl> if (!( pci_resource_flags ( pdev , 1 ) & IORESOURCE_MEM )) { <nl> dev_err (& pdev -> dev , " Cannot find proper base address \ n "); <nl> + err = - ENODEV ; <nl> goto err_disable_pdev ; <nl> } <nl> 
static int ocfs2_remove_inode_range ( struct inode * inode , <nl> ocfs2_truncate_cluster_pages ( inode , byte_start , byte_len ); <nl>  <nl> out : <nl> + ocfs2_free_path ( path ); <nl> ocfs2_schedule_truncate_log_flush ( osb , 1 ); <nl> ocfs2_run_deallocs ( osb , & dealloc ); <nl> 
uint16_t fixed_point_to_int_frac ( <nl> arg )); <nl>  <nl> if ( d <= ( uint16_t )( 1 << integer_bits ) - ( 1 / ( uint16_t ) divisor )) <nl> - numerator = ( uint16_t ) dal_fixed31_32_floor ( <nl> + numerator = ( uint16_t ) dal_fixed31_32_round ( <nl> dal_fixed31_32_mul_int ( <nl> arg , <nl> divisor ));
static noinline struct module * load_module ( void __user * umod , <nl> free_unload : <nl> module_unload_free ( mod ); <nl> # if defined ( CONFIG_MODULE_UNLOAD ) && defined ( CONFIG_SMP ) <nl> - free_init : <nl> percpu_modfree ( mod -> refptr ); <nl> + free_init : <nl> # endif <nl> module_free ( mod , mod -> module_init ); <nl> free_core :
static void xilinx_msi_teardown_irq ( struct msi_controller * chip , <nl> unsigned int irq ) <nl> { <nl> xilinx_pcie_destroy_msi ( irq ); <nl> + irq_dispose_mapping ( irq ); <nl> } <nl>  <nl> /**
sg_unlink_reserve ( Sg_fd * sfp , Sg_request * srp ) <nl> req_schp -> page_order = 0 ; <nl> req_schp -> sglist_len = 0 ; <nl> srp -> res_used = 0 ; <nl> + /* Called without mutex lock to avoid deadlock */ <nl> + sfp -> res_in_use = 0 ; <nl> } <nl>  <nl> static Sg_request *
static int clk_fetch_parent_index ( struct clk_core * core , <nl> { <nl> int i ; <nl>  <nl> + if (! parent ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * find index of new parent clock using cached parent ptrs , <nl> * or if not yet cached , use string name comparison and cache
static int imx_ssi_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> ssi -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( ssi -> irq < 0 ) { <nl> + dev_err (& pdev -> dev , " Failed to get IRQ : % d \ n ", ssi -> irq ); <nl> + return ssi -> irq ; <nl> + } <nl>  <nl> ssi -> clk = devm_clk_get (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( ssi -> clk )) {
struct perf_evsel * perf_evlist__id2evsel ( struct perf_evlist * evlist , u64 id ) <nl> hlist_for_each_entry ( sid , pos , head , node ) <nl> if ( sid -> id == id ) <nl> return sid -> evsel ; <nl> + <nl> + if (! perf_evlist__sample_id_all ( evlist )) <nl> + return list_entry ( evlist -> entries . next , struct perf_evsel , node ); <nl> + <nl> return NULL ; <nl> } <nl> 
static int soc_tplg_dai_create ( struct soc_tplg * tplg , <nl> set_stream_info ( stream , caps ); <nl> } <nl>  <nl> + if ( pcm -> compress ) <nl> + dai_drv -> compress_new = snd_soc_new_compress ; <nl> + <nl> /* pass control to component driver for optional further init */ <nl> ret = soc_tplg_dai_load ( tplg , dai_drv , pcm , NULL ); <nl> if ( ret < 0 ) {
i915_gem_execbuffer_reserve ( struct intel_ring_buffer * ring , <nl>  <nl> obj -> base . pending_read_domains = 0 ; <nl> obj -> base . pending_write_domain = 0 ; <nl> + obj -> pending_fenced_gpu_access = false ; <nl> } <nl> list_splice (& ordered_objects , objects ); <nl> 
static int spi_qup_remove ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> ret = pm_runtime_get_sync (& pdev -> dev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> return ret ; <nl>  <nl> ret = spi_qup_set_state ( controller , QUP_STATE_RESET );
static void * etm_setup_aux ( int event_cpu , void ** pages , <nl> if (! sink_ops ( sink )-> alloc_buffer ) <nl> goto err ; <nl>  <nl> + cpu = cpumask_first ( mask ); <nl> /* Get the AUX specific data from the sink buffer */ <nl> event_data -> snk_config = <nl> sink_ops ( sink )-> alloc_buffer ( sink , cpu , pages ,
void speakup_remove_virtual_keyboard ( void ) <nl> { <nl> if ( virt_keyboard != NULL ) { <nl> input_unregister_device ( virt_keyboard ); <nl> - input_free_device ( virt_keyboard ); <nl> virt_keyboard = NULL ; <nl> } <nl> }
static int dpaa2_eth_poll ( struct napi_struct * napi , int budget ) <nl> err = dpaa2_io_service_rearm ( NULL , & ch -> nctx ); <nl> cpu_relax (); <nl> } while ( err == - EBUSY ); <nl> + WARN_ONCE ( err , " CDAN notifications rearm failed on core % d ", <nl> + ch -> nctx . desired_cpu ); <nl> } <nl>  <nl> ch -> stats . frames += cleaned ;
static int das16m1_attach ( struct comedi_device * dev , <nl>  <nl> s = & dev -> subdevices [ 3 ]; <nl> /* 8255 */ <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> extra_iobase ); <nl> + ret = subdev_8255_init ( dev , s , NULL , devpriv -> extra_iobase ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* disable upper half of hardware conversion counter so it doesn ' t mess with us */ <nl> outb ( TOTAL_CLEAR , dev -> iobase + DAS16M1_8254_FIRST_CNTRL );
struct usb_function * ecm_alloc ( struct usb_function_instance * fi ) <nl> sizeof ( ecm -> ethaddr )); <nl> if ( status < 12 ) { <nl> kfree ( ecm ); <nl> + mutex_unlock (& opts -> lock ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> ecm_string_defs [ 1 ]. s = ecm -> ethaddr ;
static int __devinit wl1271_probe ( struct sdio_func * func , <nl> goto out_free ; <nl> } <nl>  <nl> + enable_irq_wake ( wl -> irq ); <nl> + <nl> disable_irq ( wl -> irq ); <nl>  <nl> ret = wl1271_init_ieee80211 ( wl ); <nl> static void __devexit wl1271_remove ( struct sdio_func * func ) <nl> pm_runtime_get_noresume (& func -> dev ); <nl>  <nl> wl1271_unregister_hw ( wl ); <nl> + disable_irq_wake ( wl -> irq ); <nl> free_irq ( wl -> irq , wl ); <nl> wl1271_free_hw ( wl ); <nl> }
void drbd_start_resync ( struct drbd_device * device , enum drbd_conns side ) <nl> return ; <nl> } <nl>  <nl> + if (! connection ) { <nl> + drbd_err ( device , " No connection to peer , aborting !\ n "); <nl> + return ; <nl> + } <nl> + <nl> if (! test_bit ( B_RS_H_DONE , & device -> flags )) { <nl> if ( side == C_SYNC_TARGET ) { <nl> /* Since application IO was locked out during C_WF_BITMAP_T and
static int mv_udc_get_frame ( struct usb_gadget * gadget ) <nl>  <nl> udc = container_of ( gadget , struct mv_udc , gadget ); <nl>  <nl> - retval = readl ( udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl> + retval = readl (& udc -> op_regs -> frindex ) & USB_FRINDEX_MASKS ; <nl>  <nl> return retval ; <nl> }
static int check_stack_boundary ( struct bpf_verifier_env * env , int regno , <nl> tnum_strn ( tn_buf , sizeof ( tn_buf ), regs [ regno ]. var_off ); <nl> verbose ( env , " invalid variable stack read R % d var_off =% s \ n ", <nl> regno , tn_buf ); <nl> + return - EACCES ; <nl> } <nl> off = regs [ regno ]. off + regs [ regno ]. var_off . value ; <nl> if ( off >= 0 || off < - MAX_BPF_STACK || off + access_size > 0 ||
static struct mfd_cell db8500_prcmu_devs [] = { <nl> . pdata_size = sizeof ( db8500_regulators ), <nl> }, <nl> { <nl> - . name = " cpufreq - u8500 ", <nl> - . of_compatible = " stericsson , cpufreq - u8500 ", <nl> + . name = " cpufreq - ux500 ", <nl> + . of_compatible = " stericsson , cpufreq - ux500 ", <nl> . platform_data = & db8500_cpufreq_table , <nl> . pdata_size = sizeof ( db8500_cpufreq_table ), <nl> },
static int __devinit i2o_pci_probe ( struct pci_dev * pdev , <nl> i2o_pci_free ( c ); <nl>  <nl> free_controller : <nl> - i2o_iop_free ( c ); <nl> put_device ( c -> device . parent ); <nl> + i2o_iop_free ( c ); <nl>  <nl> disable : <nl> pci_disable_device ( pdev );
static bool mem_cgroup_out_of_memory ( struct mem_cgroup * memcg , gfp_t gfp_mask , <nl> mem_cgroup_iter_break ( memcg , iter ); <nl> if ( chosen ) <nl> put_task_struct ( chosen ); <nl> + /* Set a dummy value to return " true ". */ <nl> + chosen = ( void *) 1 ; <nl> goto unlock ; <nl> case OOM_SCAN_OK : <nl> break ;
static int dcp_probe ( struct platform_device * pdev ) <nl>  <nl> r = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> dev -> dcp_regs_base = devm_ioremap_resource (& pdev -> dev , r ); <nl> + if ( IS_ERR ( dev -> dcp_regs_base )) <nl> + return PTR_ERR ( dev -> dcp_regs_base ); <nl>  <nl> dcp_set ( dev , DCP_CTRL_SFRST , DCP_REG_CTRL ); <nl> udelay ( 10 );
int mei_cl_disconnect ( struct mei_cl * cl ) <nl> cl_err ( dev , cl , " failed to disconnect .\ n "); <nl> goto free ; <nl> } <nl> + cl -> timer_count = MEI_CONNECT_TIMEOUT ; <nl> mdelay ( 10 ); /* Wait for hardware disconnection ready */ <nl> list_add_tail (& cb -> list , & dev -> ctrl_rd_list . list ); <nl> } else {
static void bus_reset_work ( struct work_struct * work ) <nl> { <nl> struct fw_ohci * ohci = <nl> container_of ( work , struct fw_ohci , bus_reset_work ); <nl> - int self_id_count , i , j , reg ; <nl> - int generation , new_generation ; <nl> + int self_id_count , generation , new_generation , i , j ; <nl> + u32 reg ; <nl> unsigned long flags ; <nl> void * free_rom = NULL ; <nl> dma_addr_t free_rom_bus = 0 ;
vsock_stream_recvmsg ( struct kiocb * kiocb , <nl> vsk = vsock_sk ( sk ); <nl> err = 0 ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state != SS_CONNECTED ) {
static int amdgpu_dm_atomic_check ( struct drm_device * dev , <nl> } <nl> } else { <nl> for_each_oldnew_crtc_in_state ( state , crtc , old_crtc_state , new_crtc_state , i ) { <nl> - if (! drm_atomic_crtc_needs_modeset ( new_crtc_state )) <nl> + if (! drm_atomic_crtc_needs_modeset ( new_crtc_state ) && <nl> + ! new_crtc_state -> color_mgmt_changed ) <nl> continue ; <nl>  <nl> if (! new_crtc_state -> enable )
int bnxt_re_create_srq ( struct ib_srq * ib_srq , <nl> dev_err ( rdev_to_dev ( rdev ), " SRQ copy to udata failed !"); <nl> bnxt_qplib_destroy_srq (& rdev -> qplib_res , <nl> & srq -> qplib_srq ); <nl> - goto exit ; <nl> + goto fail ; <nl> } <nl> } <nl> if ( nq )
s32 host_int_del_station ( struct host_if_drv * hif_drv , const u8 * pu8MacAddr ) <nl> msg . drv = hif_drv ; <nl>  <nl> if (! pu8MacAddr ) <nl> - memset ( pstrDelStationMsg -> mac_addr , 255 , ETH_ALEN ); <nl> + eth_broadcast_addr ( pstrDelStationMsg -> mac_addr ); <nl> else <nl> memcpy ( pstrDelStationMsg -> mac_addr , pu8MacAddr , ETH_ALEN ); <nl> 
static struct stmmac_axi * stmmac_axi_setup ( struct platform_device * pdev ) <nl> if (! np ) <nl> return NULL ; <nl>  <nl> - axi = kzalloc ( sizeof ( axi ), GFP_KERNEL ); <nl> + axi = kzalloc ( sizeof (* axi ), GFP_KERNEL ); <nl> if (! axi ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
void exit_shm ( struct task_struct * task ) <nl> { <nl> struct ipc_namespace * ns = task -> nsproxy -> ipc_ns ; <nl>  <nl> + if ( shm_ids ( ns ). in_use == 0 ) <nl> + return ; <nl> + <nl> /* Destroy all already created segments , but not mapped yet */ <nl> down_write (& shm_ids ( ns ). rw_mutex ); <nl> if ( shm_ids ( ns ). in_use )
static struct file * do_last ( struct nameidata * nd , struct path * path , <nl> if ( path -> dentry -> d_inode -> i_op -> follow_link ) <nl> return NULL ; <nl> error = - ENOTDIR ; <nl> - if (* want_dir & ! path -> dentry -> d_inode -> i_op -> lookup ) <nl> + if (* want_dir && ! path -> dentry -> d_inode -> i_op -> lookup ) <nl> goto exit_dput ; <nl> path_to_nameidata ( path , nd ); <nl> audit_inode ( pathname , nd -> path . dentry );
static __inline__ struct page * drm_do_vm_shm_nopage ( struct vm_area_struct * vma , <nl>  <nl> offset = address - vma -> vm_start ; <nl> i = ( unsigned long ) map -> handle + offset ; <nl> - page = vmalloc_to_page (( void *) i ); <nl> + page = ( map -> type == _DRM_CONSISTENT ) ? <nl> + virt_to_page (( void *) i ) : vmalloc_to_page (( void *) i ); <nl> if (! page ) <nl> return NOPAGE_OOM ; <nl> get_page ( page );
void intel_engines_mark_idle ( struct drm_i915_private * i915 ) <nl> for_each_engine ( engine , i915 , id ) { <nl> intel_engine_disarm_breadcrumbs ( engine ); <nl> i915_gem_batch_pool_fini (& engine -> batch_pool ); <nl> + tasklet_kill (& engine -> irq_tasklet ); <nl> engine -> no_priolist = false ; <nl> } <nl> }
static int __init d40_of_probe ( struct platform_device * pdev , <nl> list = of_get_property ( np , " disabled - channels ", & num_disabled ); <nl> num_disabled /= sizeof (* list ); <nl>  <nl> - if ( num_disabled > STEDMA40_MAX_PHYS || num_disabled < 0 ) { <nl> + if ( num_disabled >= STEDMA40_MAX_PHYS || num_disabled < 0 ) { <nl> d40_err (& pdev -> dev , <nl> " Invalid number of disabled channels specified (% d )\ n ", <nl> num_disabled );
int __cpuinit local_timer_setup ( struct clock_event_device * evt ) <nl>  <nl> /* Use existing clock_event for cpu 0 */ <nl> if (! smp_processor_id ()) <nl> - return ; <nl> + return 0 ; <nl>  <nl> writel ( DGT_CLK_CTL_DIV_4 , MSM_TMR_BASE + DGT_CLK_CTL ); <nl> 
static const struct drm_i915_gem_object_ops i915_gem_phys_ops = { <nl> . release = i915_gem_object_release_phys , <nl> }; <nl>  <nl> - int <nl> - i915_gem_object_unbind ( struct drm_i915_gem_object * obj ) <nl> + int i915_gem_object_unbind ( struct drm_i915_gem_object * obj ) <nl> { <nl> struct i915_vma * vma ; <nl> LIST_HEAD ( still_in_list ); <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> /* The vma will only be freed if it is marked as closed , and if we wait <nl> * upon rendering to the vma , we may unbind anything in the list .
int batadv_bla_tx ( struct batadv_priv * bat_priv , struct sk_buff * skb , <nl> if (! atomic_read (& bat_priv -> bridge_loop_avoidance )) <nl> goto allow ; <nl>  <nl> - /* in VLAN case , the mac header might not be set . */ <nl> - skb_reset_mac_header ( skb ); <nl> - <nl> if ( batadv_bla_process_claim ( bat_priv , primary_if , skb )) <nl> goto handled ; <nl> 
static void perf_callchain_user_64 ( struct perf_callchain_entry * entry , <nl> sp = regs -> gpr [ 1 ]; <nl> perf_callchain_store ( entry , next_ip ); <nl>  <nl> - for (;;) { <nl> + while ( entry -> nr < PERF_MAX_STACK_DEPTH ) { <nl> fp = ( unsigned long __user *) sp ; <nl> if (! valid_user_sp ( sp , 1 ) || read_user_stack_64 ( fp , & next_sp )) <nl> return ;
static void hidinput_configure_usage ( struct hid_input * hidinput , struct hid_fiel <nl> case 0x0b6 : map_key_clear ( KEY_PREVIOUSSONG ); break ; <nl> case 0x0b7 : map_key_clear ( KEY_STOPCD ); break ; <nl> case 0x0b8 : map_key_clear ( KEY_EJECTCD ); break ; <nl> + case 0x0bc : map_key_clear ( KEY_MEDIA_REPEAT ); break ; <nl>  <nl> case 0x0cd : map_key_clear ( KEY_PLAYPAUSE ); break ; <nl> case 0x0e0 : map_abs_clear ( ABS_VOLUME ); break ;
static void * ixgbe_fwd_add ( struct net_device * pdev , struct net_device * vdev ) <nl> ( adapter -> num_rx_pools > IXGBE_MAX_MACVLANS )) <nl> return ERR_PTR (- EBUSY ); <nl>  <nl> - fwd_adapter = kcalloc ( 1 , sizeof ( struct ixgbe_fwd_adapter ), GFP_KERNEL ); <nl> + fwd_adapter = kzalloc ( sizeof (* fwd_adapter ), GFP_KERNEL ); <nl> if (! fwd_adapter ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
void rds_inc_info_copy ( struct rds_incoming * inc , <nl> minfo . fport = inc -> i_hdr . h_dport ; <nl> } <nl>  <nl> + minfo . flags = 0 ; <nl> + <nl> rds_info_copy ( iter , & minfo , sizeof ( minfo )); <nl> }
static int si21_writeregs ( struct si21xx_state * state , u8 reg1 , <nl> . len = len + 1 <nl> }; <nl>  <nl> + if ( len > sizeof ( buf ) - 1 ) <nl> + return - EINVAL ; <nl> + <nl> msg . buf [ 0 ] = reg1 ; <nl> memcpy ( msg . buf + 1 , data , len ); <nl> 
static struct rds_connection * __rds_conn_create ( struct net * net , <nl> } <nl> } <nl>  <nl> + if ( trans == NULL ) { <nl> + kmem_cache_free ( rds_conn_slab , conn ); <nl> + conn = ERR_PTR (- ENODEV ); <nl> + goto out ; <nl> + } <nl> + <nl> conn -> c_trans = trans ; <nl>  <nl> ret = trans -> conn_alloc ( conn , gfp );
static int ptrace_bts_config ( struct task_struct * child , <nl> if (! cfg . signal ) <nl> return - EINVAL ; <nl>  <nl> - return - EOPNOTSUPP ; <nl> - <nl> child -> thread . bts_ovfl_signal = cfg . signal ; <nl> + return - EOPNOTSUPP ; <nl> } <nl>  <nl> if (( cfg . flags & PTRACE_BTS_O_ALLOC ) &&
static int wanxl_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> ifr -> ifr_settings . size = size ; /* data size wanted */ <nl> return - ENOBUFS ; <nl> } <nl> + memset (& line , 0 , sizeof ( line )); <nl> line . clock_type = get_status ( port )-> clocking ; <nl> line . clock_rate = 0 ; <nl> line . loopback = 0 ;
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
__init int intel_pmu_init ( void ) <nl> if ( version > 1 ) <nl> x86_pmu . num_counters_fixed = max (( int ) edx . split . num_counters_fixed , 3 ); <nl>  <nl> - /* <nl> - * v2 and above have a perf capabilities MSR <nl> - */ <nl> - if ( version > 1 ) { <nl> + if ( boot_cpu_has ( X86_FEATURE_PDCM )) { <nl> u64 capabilities ; <nl>  <nl> rdmsrl ( MSR_IA32_PERF_CAPABILITIES , capabilities );
typedef struct kl_config_hdr { <nl> /* --- New Macros for the changed kl_config_hdr_t structure --- */ <nl>  <nl> # define PTR_CH_MALLOC_HDR ( _k ) (( klc_malloc_hdr_t *)\ <nl> - ( unsigned long ) _k + ( _k -> ch_malloc_hdr_off ))) <nl> + (( unsigned long ) _k + ( _k -> ch_malloc_hdr_off ))) <nl>  <nl> # define KL_CONFIG_CH_MALLOC_HDR ( _n ) PTR_CH_MALLOC_HDR ( KL_CONFIG_HDR ( _n )) <nl> 
static int ismt_access ( struct i2c_adapter * adap , u16 addr , <nl>  <nl> case I2C_SMBUS_BLOCK_PROC_CALL : <nl> dev_dbg ( dev , " I2C_SMBUS_BLOCK_PROC_CALL \ n "); <nl> + if ( data -> block [ 0 ] > I2C_SMBUS_BLOCK_MAX ) <nl> + return - EINVAL ; <nl> + <nl> dma_size = I2C_SMBUS_BLOCK_MAX ; <nl> desc -> tgtaddr_rw = ISMT_DESC_ADDR_RW ( addr , 1 ); <nl> desc -> wr_len_cmd = data -> block [ 0 ] + 1 ;
unsigned int kstat_irqs ( unsigned int irq ) <nl> */ <nl> unsigned int kstat_irqs_usr ( unsigned int irq ) <nl> { <nl> - int sum ; <nl> + unsigned int sum ; <nl>  <nl> irq_lock_sparse (); <nl> sum = kstat_irqs ( irq );
static void process_init_reply ( struct fuse_conn * fc , struct fuse_req * req ) <nl> int i ; <nl> struct fuse_init_out * arg = & req -> misc . init_out ; <nl>  <nl> - if ( arg -> major != FUSE_KERNEL_VERSION ) <nl> + if ( req -> out . h . error || arg -> major != FUSE_KERNEL_VERSION ) <nl> fc -> conn_error = 1 ; <nl> else { <nl> fc -> minor = arg -> minor ;
int __init pci_legacy_init ( void ) <nl>  <nl> return 0 ; <nl> } <nl> - EXPORT_SYMBOL_GPL ( pci_legacy_init ); <nl>  <nl> void pcibios_scan_specific_bus ( int busn ) <nl> {
static int zd1201_resume ( struct usb_interface * interface ) <nl> { <nl> struct zd1201 * zd = usb_get_intfdata ( interface ); <nl>  <nl> + if (! zd || ! zd -> dev ) <nl> + return - ENODEV ; <nl> + <nl> netif_device_attach ( zd -> dev ); <nl>  <nl> if ( zd -> was_enabled )
static int cqspi_setup_flash ( struct cqspi_st * cqspi , struct device_node * np ) <nl> goto err ; <nl> } <nl>  <nl> - if ( cs > CQSPI_MAX_CHIPSELECT ) { <nl> + if ( cs >= CQSPI_MAX_CHIPSELECT ) { <nl> dev_err ( dev , " Chip select % d out of range .\ n ", cs ); <nl> goto err ; <nl> }
int azx_codec_configure ( struct azx * chip ) <nl> list_for_each_codec_safe ( codec , next , & chip -> bus ) { <nl> snd_hda_codec_configure ( codec ); <nl> } <nl> + <nl> + if (! azx_bus ( chip )-> num_codecs ) <nl> + return - ENODEV ; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( azx_codec_configure );
static struct net_device * fs_init_instance ( struct device * dev , <nl> if ( registered ) <nl> unregister_netdev ( ndev ); <nl>  <nl> - if ( fep != NULL ) { <nl> + if ( fep && fep -> ops ) { <nl> (* fep -> ops -> free_bd )( ndev ); <nl> (* fep -> ops -> cleanup_data )( ndev ); <nl> }
static struct sk_buff * isdn_ppp_decompress ( struct sk_buff * skb , struct ippp_struc <nl> rsparm . maxdlen = IPPP_RESET_MAXDATABYTES ; <nl>  <nl> skb_out = dev_alloc_skb ( is -> mru + PPP_HDRLEN ); <nl> + if (! skb_out ) { <nl> + kfree_skb ( skb ); <nl> + printk ( KERN_ERR " ippp : decomp memory allocation failure \ n "); <nl> + return NULL ; <nl> + } <nl> len = ipc -> decompress ( stat , skb , skb_out , & rsparm ); <nl> kfree_skb ( skb ); <nl> if ( len <= 0 ) {
static void pcie_disable_notification ( struct controller * ctrl ) <nl> u16 mask ; <nl> mask = ( PCI_EXP_SLTCTL_PDCE | PCI_EXP_SLTCTL_ABPE | <nl> PCI_EXP_SLTCTL_MRLSCE | PCI_EXP_SLTCTL_PFDE | <nl> - PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE ); <nl> + PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE | <nl> + PCI_EXP_SLTCTL_DLLSCE ); <nl> if ( pcie_write_cmd ( ctrl , 0 , mask )) <nl> ctrl_warn ( ctrl , " Cannot disable software notification \ n "); <nl> }
static int __init twl4030_configure_resource ( struct twl4030_resconfig * rconfig ) <nl> return err ; <nl> } <nl>  <nl> - if ( rconfig -> remap_off >= 0 ) { <nl> + if ( rconfig -> remap_off != TWL4030_RESCONFIG_UNDEF ) { <nl> remap &= ~ OFF_STATE_MASK ; <nl> remap |= rconfig -> remap_off << OFF_STATE_SHIFT ; <nl> } <nl>  <nl> - if ( rconfig -> remap_sleep >= 0 ) { <nl> + if ( rconfig -> remap_sleep != TWL4030_RESCONFIG_UNDEF ) { <nl> remap &= ~ SLEEP_STATE_MASK ; <nl> remap |= rconfig -> remap_off << SLEEP_STATE_SHIFT ; <nl> }
static ssize_t write_ports ( struct file * file , char * buf , size_t size ) <nl> /* Decrease the count , but don ' t shutdown the <nl> * the service <nl> */ <nl> + lock_kernel (); <nl> nfsd_serv -> sv_nrthreads --; <nl> + unlock_kernel (); <nl> } <nl> return err ; <nl> }
enum vnt_cmd_state { <nl>  <nl> struct vnt_private ; <nl>  <nl> - void vnt_reset_command_timer ( struct vnt_private *); <nl> + void vnt_reset_command_timer ( struct vnt_private * priv ); <nl>  <nl> - int vnt_schedule_command ( struct vnt_private *, enum vnt_cmd ); <nl> + int vnt_schedule_command ( struct vnt_private * priv , enum vnt_cmd ); <nl>  <nl> void vnt_run_command ( struct work_struct * work ); <nl> 
int i2400m_op_rfkill_sw_toggle ( struct wimax_dev * wimax_dev , <nl> "% d \ n ", result ); <nl> result = 0 ; <nl> error_cmd : <nl> - kfree ( cmd ); <nl> kfree_skb ( ack_skb ); <nl> error_msg_to_dev : <nl> error_alloc : <nl> d_fnend ( 4 , dev , "( wimax_dev % p state % d ) = % d \ n ", <nl> wimax_dev , state , result ); <nl> + kfree ( cmd ); <nl> return result ; <nl> } <nl> 
static int iscsit_build_sendtargets_response ( struct iscsi_cmd * cmd ) <nl> if (! text_ptr ) { <nl> pr_err (" Unable to locate '=' string in text_in :" <nl> " % s \ n ", text_in ); <nl> + kfree ( payload ); <nl> return - EINVAL ; <nl> } <nl> /*
struct inode * reiserfs_iget ( struct super_block * s , const struct cpu_key * key ) <nl>  <nl> args . objectid = key -> on_disk_key . k_objectid ; <nl> args . dirid = key -> on_disk_key . k_dir_id ; <nl> + reiserfs_write_unlock ( s ); <nl> inode = iget5_locked ( s , key -> on_disk_key . k_objectid , <nl> reiserfs_find_actor , reiserfs_init_locked_inode , <nl> ( void *)(& args )); <nl> + reiserfs_write_lock ( s ); <nl> if (! inode ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
int __ieee80211_suspend ( struct ieee80211_hw * hw , struct cfg80211_wowlan * wowlan ) <nl> int err = drv_suspend ( local , wowlan ); <nl> if ( err < 0 ) { <nl> local -> quiescing = false ; <nl> + local -> wowlan = false ; <nl> return err ; <nl> } else if ( err > 0 ) { <nl> WARN_ON ( err != 1 );
unsigned int solo_eeprom_ewen ( struct solo_dev * solo_dev , int w_en ) <nl> __be16 solo_eeprom_read ( struct solo_dev * solo_dev , int loc ) <nl> { <nl> int read_cmd = loc | ( EE_READ_CMD << ADDR_LEN ); <nl> - unsigned short retval = 0 ; <nl> + u16 retval = 0 ; <nl> int i ; <nl>  <nl> solo_eeprom_cmd ( solo_dev , read_cmd );
static LIST_HEAD ( dev_map_list ); <nl>  <nl> static u64 dev_map_bitmap_size ( const union bpf_attr * attr ) <nl> { <nl> - return BITS_TO_LONGS ( attr -> max_entries ) * sizeof ( unsigned long ); <nl> + return BITS_TO_LONGS (( u64 ) attr -> max_entries ) * sizeof ( unsigned long ); <nl> } <nl>  <nl> static struct bpf_map * dev_map_alloc ( union bpf_attr * attr )
static int sgtl5000_i2c_probe ( struct i2c_client * client , <nl> if ( IS_ERR ( sgtl5000 -> mclk )) { <nl> ret = PTR_ERR ( sgtl5000 -> mclk ); <nl> dev_err (& client -> dev , " Failed to get mclock : % d \ n ", ret ); <nl> + /* Defer the probe to see if the clk will be provided later */ <nl> + if ( ret == - ENOENT ) <nl> + return - EPROBE_DEFER ; <nl> return ret ; <nl> } <nl> 
struct simple_xattr * simple_xattr_alloc ( const void * value , size_t size ) <nl>  <nl> /* wrap around ? */ <nl> len = sizeof (* new_xattr ) + size ; <nl> - if ( len <= sizeof (* new_xattr )) <nl> + if ( len < sizeof (* new_xattr )) <nl> return NULL ; <nl>  <nl> new_xattr = kmalloc ( len , GFP_KERNEL );
static ssize_t btrfs_label_store ( struct kobject * kobj , <nl> struct btrfs_fs_info * fs_info = to_fs_info ( kobj ); <nl> size_t p_len ; <nl>  <nl> + if (! fs_info ) <nl> + return - EPERM ; <nl> + <nl> if ( fs_info -> sb -> s_flags & MS_RDONLY ) <nl> return - EROFS ; <nl> 
static int wm97xx_init_pen_irq ( struct wm97xx * wm ) <nl> * provided . */ <nl> BUG_ON (! wm -> mach_ops -> irq_enable ); <nl>  <nl> - if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , IRQF_SHARED , <nl> + if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , <nl> + IRQF_SHARED | IRQF_SAMPLE_RANDOM , <nl> " wm97xx - pen ", wm )) { <nl> dev_err ( wm -> dev , <nl> " Failed to register pen down interrupt , polling ");
static void chase_port ( struct edgeport_port * port , unsigned long timeout , <nl> wait_queue_t wait ; <nl> unsigned long flags ; <nl>  <nl> + if (! tty ) <nl> + return ; <nl> + <nl> if (! timeout ) <nl> timeout = ( HZ * EDGE_CLOSING_WAIT )/ 100 ; <nl> 
static void task_fork_fair ( struct task_struct * p ) <nl>  <nl> update_rq_clock ( rq ); <nl>  <nl> - if ( unlikely ( task_cpu ( p ) != this_cpu )) <nl> + if ( unlikely ( task_cpu ( p ) != this_cpu )) { <nl> + rcu_read_lock (); <nl> __set_task_cpu ( p , this_cpu ); <nl> + rcu_read_unlock (); <nl> + } <nl>  <nl> update_curr ( cfs_rq ); <nl> 
static int ov965x_enum_frame_sizes ( struct v4l2_subdev * sd , <nl> { <nl> int i = ARRAY_SIZE ( ov965x_formats ); <nl>  <nl> - if ( fse -> index > ARRAY_SIZE ( ov965x_framesizes )) <nl> + if ( fse -> index >= ARRAY_SIZE ( ov965x_framesizes )) <nl> return - EINVAL ; <nl>  <nl> while (-- i )
static int i915_context_status ( struct seq_file * m , void * unused ) <nl> } <nl>  <nl> list_for_each_entry ( ctx , & dev_priv -> context_list , link ) { <nl> + if ( ctx -> obj == NULL ) <nl> + continue ; <nl> + <nl> seq_puts ( m , " HW context "); <nl> describe_ctx ( m , ctx ); <nl> for_each_ring ( ring , dev_priv , i )
static int alloc_wbufs ( struct ubifs_info * c ) <nl> { <nl> int i , err ; <nl>  <nl> - c -> jheads = kzalloc ( c -> jhead_cnt * sizeof ( struct ubifs_jhead ), <nl> - GFP_KERNEL ); <nl> + c -> jheads = kcalloc ( c -> jhead_cnt , sizeof ( struct ubifs_jhead ), <nl> + GFP_KERNEL ); <nl> if (! c -> jheads ) <nl> return - ENOMEM ; <nl> 
static ssize_t cxlflash_show_port_status ( struct device * dev , <nl> u64 * fc_regs ; <nl>  <nl> rc = kstrtouint (( attr -> attr . name + 4 ), 10 , & port ); <nl> - if ( rc || ( port > NUM_FC_PORTS )) <nl> + if ( rc || ( port >= NUM_FC_PORTS )) <nl> return 0 ; <nl>  <nl> fc_regs = & afu -> afu_map -> global . fc_regs [ port ][ 0 ];
qla27xx_fwdt_entry_t270 ( struct scsi_qla_host * vha , <nl> qla27xx_write_reg ( reg , 0xc0 , addr | 0x80000000 , buf ); <nl> qla27xx_insert32 ( addr , buf , len ); <nl> qla27xx_read_off ( reg , 0xc4 , buf , len ); <nl> - addr ++; <nl> + addr += sizeof ( uint32_t ); <nl> } <nl>  <nl> return false ;
static int shash_update_unaligned ( struct shash_desc * desc , const u8 * data , <nl> u8 buf [ shash_align_buffer_size ( unaligned_len , alignmask )] <nl> __attribute__ (( aligned )); <nl>  <nl> + if ( unaligned_len > len ) <nl> + unaligned_len = len ; <nl> + <nl> memcpy ( buf , data , unaligned_len ); <nl>  <nl> return shash -> update ( desc , buf , unaligned_len ) ?:
int key_reject_and_link ( struct key * key , <nl>  <nl> mutex_unlock (& key_construction_mutex ); <nl>  <nl> - if ( keyring ) <nl> + if ( keyring && link_ret == 0 ) <nl> __key_link_end ( keyring , & key -> index_key , edit ); <nl>  <nl> /* wake up anyone waiting for a key to be constructed */
static ssize_t dfs_file_write ( struct file * file , const char __user * user_buf , <nl> } else if ( dent == d -> dfs_emulate_power_cut ) { <nl> if ( kstrtoint ( buf , 0 , & val ) != 0 ) <nl> count = - EINVAL ; <nl> - d -> emulate_power_cut = val ; <nl> + else <nl> + d -> emulate_power_cut = val ; <nl> goto out ; <nl> } <nl> 
struct kvm_vcpu * kvm_arch_vcpu_create ( struct kvm * kvm , unsigned int id ) <nl> int err ; <nl> struct kvm_vcpu * vcpu ; <nl>  <nl> + if ( irqchip_in_kernel ( kvm ) && vgic_initialized ( kvm )) { <nl> + err = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> vcpu = kmem_cache_zalloc ( kvm_vcpu_cache , GFP_KERNEL ); <nl> if (! vcpu ) { <nl> err = - ENOMEM ;
static int uvesafb_setcmap ( struct fb_cmap * cmap , struct fb_info * info ) <nl> info -> cmap . len || cmap -> start < info -> cmap . start ) <nl> return - EINVAL ; <nl>  <nl> - entries = kmalloc ( sizeof (* entries ) * cmap -> len , GFP_KERNEL ); <nl> + entries = kmalloc_array ( cmap -> len , sizeof (* entries ), <nl> + GFP_KERNEL ); <nl> if (! entries ) <nl> return - ENOMEM ; <nl> 
i915_gem_userptr_ioctl ( struct drm_device * dev , <nl> I915_USERPTR_UNSYNCHRONIZED )) <nl> return - EINVAL ; <nl>  <nl> + if (! args -> user_size ) <nl> + return - EINVAL ; <nl> + <nl> if ( offset_in_page ( args -> user_ptr | args -> user_size )) <nl> return - EINVAL ; <nl> 
struct lock_chain { <nl> }; <nl>  <nl> # define MAX_LOCKDEP_KEYS_BITS 11 <nl> -# define MAX_LOCKDEP_KEYS ( 1UL << MAX_LOCKDEP_KEYS_BITS ) <nl> +/* <nl> + * Subtract one because we offset hlock -> class_idx by 1 in order <nl> + * to make 0 mean no class . This avoids overflowing the class_idx <nl> + * bitfield and hitting the BUG in hlock_class (). <nl> + */ <nl> +# define MAX_LOCKDEP_KEYS (( 1UL << MAX_LOCKDEP_KEYS_BITS ) - 1 ) <nl>  <nl> struct held_lock { <nl> /*
static inline int verify_replay ( struct xfrm_usersa_info * p , <nl> if (! rt ) <nl> return 0 ; <nl>  <nl> + if ( p -> id . proto != IPPROTO_ESP ) <nl> + return - EINVAL ; <nl> + <nl> if ( p -> replay_window != 0 ) <nl> return - EINVAL ; <nl> 
static int ipxitf_ioctl ( unsigned int cmd , void __user * arg ) <nl> sipx -> sipx_network = ipxif -> if_netnum ; <nl> memcpy ( sipx -> sipx_node , ipxif -> if_node , <nl> sizeof ( sipx -> sipx_node )); <nl> - rc = - EFAULT ; <nl> + rc = 0 ; <nl> if ( copy_to_user ( arg , & ifr , sizeof ( ifr ))) <nl> - break ; <nl> + rc = - EFAULT ; <nl> ipxitf_put ( ipxif ); <nl> - rc = 0 ; <nl> break ; <nl> } <nl> case SIOCAIPXITFCRT :
void dccp_close ( struct sock * sk , long timeout ) <nl> __kfree_skb ( skb ); <nl> } <nl>  <nl> + /* If socket has been already reset kill it . */ <nl> + if ( sk -> sk_state == DCCP_CLOSED ) <nl> + goto adjudge_to_death ; <nl> + <nl> if ( data_was_unread ) { <nl> /* Unread data was tossed , send an appropriate Reset Code */ <nl> DCCP_WARN (" ABORT with % u bytes unread \ n ", data_was_unread );
static int amd_gpio_remove ( struct platform_device * pdev ) <nl> gpio_dev = platform_get_drvdata ( pdev ); <nl>  <nl> gpiochip_remove (& gpio_dev -> gc ); <nl> - pinctrl_unregister ( gpio_dev -> pctrl ); <nl>  <nl> return 0 ; <nl> }
static struct rds_connection * __rds_conn_create ( struct net * net , <nl> * should end up here , but if it <nl> * does , reset / destroy the connection . <nl> */ <nl> + kfree ( conn -> c_path ); <nl> kmem_cache_free ( rds_conn_slab , conn ); <nl> conn = ERR_PTR (- EOPNOTSUPP ); <nl> goto out ;
struct timekeeper { <nl> u32 mult ; <nl> }; <nl>  <nl> - struct timekeeper timekeeper ; <nl> + static struct timekeeper timekeeper ; <nl>  <nl> /** <nl> * timekeeper_setup_internals - Set up internals to use clocksource clock . <nl> static struct timespec total_sleep_time ; <nl> /* <nl> * The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock . <nl> */ <nl> - struct timespec raw_time ; <nl> + static struct timespec raw_time ; <nl>  <nl> /* flag for if timekeeping is suspended */ <nl> int __read_mostly timekeeping_suspended ;
static int omap2_onenand_probe ( struct platform_device * pdev ) <nl> } <nl> } <nl>  <nl> - if ( pdata -> skip_initial_unlocking ) <nl> - this -> options |= ONENAND_SKIP_INITIAL_UNLOCKING ; <nl> - <nl> if (( r = onenand_scan (& c -> mtd , 1 )) < 0 ) <nl> goto err_release_dma ; <nl> 
int ubi_io_read ( const struct ubi_device * ubi , void * buf , int pnum , int offset , <nl> return UBI_IO_BITFLIPS ; <nl> } <nl>  <nl> - if ( read != len && retries ++ < UBI_IO_RETRIES ) { <nl> + if ( retries ++ < UBI_IO_RETRIES ) { <nl> dbg_io (" error % d % s while reading % d bytes from PEB % d :% d ," <nl> " read only % zd bytes , retry ", <nl> err , errstr , len , pnum , offset , read );
static int ci_hdrc_remove ( struct platform_device * pdev ) <nl> dbg_remove_files ( ci ); <nl> free_irq ( ci -> irq , ci ); <nl> ci_role_destroy ( ci ); <nl> + kfree ( ci -> hw_bank . regmap ); <nl>  <nl> return 0 ; <nl> }
static int __unioxx5_subdev_init ( struct comedi_device * dev , <nl> return - ENOMEM ; <nl>  <nl> ret = __comedi_request_region ( dev , iobase , UNIOXX5_SIZE ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + kfree ( usp ); <nl> return ret ; <nl> + } <nl> usp -> usp_iobase = iobase ; <nl>  <nl> /* defining modules types */
static int das16cs_attach ( struct comedi_device * dev , <nl> dev -> driver -> driver_name , dev -> board_name , <nl> dev -> iobase , dev -> irq ); <nl>  <nl> - return 1 ; <nl> + return 0 ; <nl> } <nl>  <nl> static void das16cs_detach ( struct comedi_device * dev )
fst_get_iface ( struct fst_card_info * card , struct fst_port_info * port , <nl> } <nl>  <nl> i = port -> index ; <nl> + memset (& sync , 0 , sizeof ( sync )); <nl> sync . clock_rate = FST_RDL ( card , portConfig [ i ]. lineSpeed ); <nl> /* Lucky card and linux use same encoding here */ <nl> sync . clock_type = FST_RDB ( card , portConfig [ i ]. internalClock ) ==
# include " tg3 . h " <nl>  <nl> # define DRV_MODULE_NAME " tg3 " <nl> -# define DRV_MODULE_VERSION " 3 . 108 " <nl> -# define DRV_MODULE_RELDATE " February 17 , 2010 " <nl> +# define DRV_MODULE_VERSION " 3 . 109 " <nl> +# define DRV_MODULE_RELDATE " April 2 , 2010 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
int vfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> bool new_is_dir = false ; <nl> unsigned max_links = new_dir -> i_sb -> s_max_links ; <nl>  <nl> - if ( source == target ) <nl> + /* <nl> + * Check source == target . <nl> + * On overlayfs need to look at underlying inodes . <nl> + */ <nl> + if ( vfs_select_inode ( old_dentry , 0 ) == vfs_select_inode ( new_dentry , 0 )) <nl> return 0 ; <nl>  <nl> error = may_delete ( old_dir , old_dentry , is_dir );
i915_gem_shrink ( struct drm_i915_private * dev_priv , <nl> SINGLE_DEPTH_NESTING ); <nl> if (! obj -> mm . pages ) { <nl> __i915_gem_object_invalidate ( obj ); <nl> + list_del_init (& obj -> global_list ); <nl> count += obj -> base . size >> PAGE_SHIFT ; <nl> } <nl> mutex_unlock (& obj -> mm . lock );
static void release_one_tty ( struct work_struct * work ) <nl> list_del_init (& tty -> tty_files ); <nl> file_list_unlock (); <nl>  <nl> + put_pid ( tty -> pgrp ); <nl> + put_pid ( tty -> session ); <nl> free_tty_struct ( tty ); <nl> } <nl> 
static const struct snd_pci_quirk stac92hd73xx_cfg_tbl [] = { <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02bd , <nl> " Dell Studio 1557 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02fe , <nl> - " Dell Studio XPS 1645 ", STAC_DELL_M6_BOTH ), <nl> + " Dell Studio XPS 1645 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x0413 , <nl> " Dell Studio 1558 ", STAC_DELL_M6_DMIC ), <nl> {} /* terminator */
static int mp_wait_modem_status ( struct sb_uart_state * state , unsigned long arg ) <nl>  <nl> static int mp_get_count ( struct sb_uart_state * state , struct serial_icounter_struct * icnt ) <nl> { <nl> - struct serial_icounter_struct icount ; <nl> + struct serial_icounter_struct icount = {}; <nl> struct sb_uart_icount cnow ; <nl> struct sb_uart_port * port = state -> port ; <nl> 
static int vhci_hub_status ( struct usb_hcd * hcd , char * buf ) <nl>  <nl> pr_info (" changed % d \ n ", changed ); <nl>  <nl> - if ( hcd -> state == HC_STATE_SUSPENDED ) <nl> + if (( hcd -> state == HC_STATE_SUSPENDED ) && ( changed == 1 )) <nl> usb_hcd_resume_root_hub ( hcd ); <nl>  <nl> done :
vfs_removexattr ( struct dentry * dentry , const char * name ) <nl> if ( error ) <nl> return error ; <nl>  <nl> + mutex_lock (& inode -> i_mutex ); <nl> error = security_inode_removexattr ( dentry , name ); <nl> - if ( error ) <nl> + if ( error ) { <nl> + mutex_unlock (& inode -> i_mutex ); <nl> return error ; <nl> + } <nl>  <nl> - mutex_lock (& inode -> i_mutex ); <nl> error = inode -> i_op -> removexattr ( dentry , name ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> 
static long vop_ioctl ( struct file * f , unsigned int cmd , unsigned long arg ) <nl> ret = - EFAULT ; <nl> goto free_ret ; <nl> } <nl> + /* Ensure desc has not changed between the two reads */ <nl> + if ( memcmp (& dd , dd_config , sizeof ( dd ))) { <nl> + ret = - EINVAL ; <nl> + goto free_ret ; <nl> + } <nl> mutex_lock (& vdev -> vdev_mutex ); <nl> mutex_lock (& vi -> vop_mutex ); <nl> ret = vop_virtio_add_device ( vdev , dd_config );
static int gbaudio_codec_probe ( struct gb_connection * connection ) <nl> kfree ( topology ); <nl> base_error : <nl> gbcodec -> mgmt_connection = NULL ; <nl> + gbaudio_free_codec ( dev , gbcodec ); <nl> return ret ; <nl> } <nl> 
static int goldfish_fb_remove ( struct platform_device * pdev ) <nl> dma_free_coherent (& pdev -> dev , framesize , ( void *) fb -> fb . screen_base , <nl> fb -> fb . fix . smem_start ); <nl> iounmap ( fb -> reg_base ); <nl> + kfree ( fb ); <nl> return 0 ; <nl> } <nl> 
static int tty_open ( struct inode * inode , struct file * filp ) <nl> if ( IS_ERR ( tty )) { <nl> tty_unlock (); <nl> mutex_unlock (& tty_mutex ); <nl> + tty_driver_kref_put ( driver ); <nl> return PTR_ERR ( tty ); <nl> } <nl> }
bfad_im_get_stats ( struct Scsi_Host * shost ) <nl> rc = bfa_port_get_stats ( BFA_FCPORT (& bfad -> bfa ), <nl> fcstats , bfad_hcb_comp , & fcomp ); <nl> spin_unlock_irqrestore (& bfad -> bfad_lock , flags ); <nl> - if ( rc != BFA_STATUS_OK ) <nl> + if ( rc != BFA_STATUS_OK ) { <nl> + kfree ( fcstats ); <nl> return NULL ; <nl> + } <nl>  <nl> wait_for_completion (& fcomp . comp ); <nl> 
static int is_valid_state_transition ( struct drbd_conf * mdev , <nl> os . conn < C_CONNECTED ) <nl> rv = SS_NEED_CONNECTION ; <nl>  <nl> + if (( ns . conn == C_SYNC_TARGET || ns . conn == C_SYNC_SOURCE ) <nl> + && os . conn < C_WF_REPORT_PARAMS ) <nl> + rv = SS_NEED_CONNECTION ; /* No NetworkFailure -> SyncTarget etc ... */ <nl> + <nl> return rv ; <nl> } <nl> 
void global_dirty_limits ( unsigned long * pbackground , unsigned long * pdirty ) <nl> { <nl> unsigned long background ; <nl> unsigned long dirty ; <nl> - unsigned long available_memory = determine_dirtyable_memory (); <nl> + unsigned long uninitialized_var ( available_memory ); <nl> struct task_struct * tsk ; <nl>  <nl> + if (! vm_dirty_bytes || ! dirty_background_bytes ) <nl> + available_memory = determine_dirtyable_memory (); <nl> + <nl> if ( vm_dirty_bytes ) <nl> dirty = DIV_ROUND_UP ( vm_dirty_bytes , PAGE_SIZE ); <nl> else
static void intel_hdmi_mode_set ( struct intel_encoder * encoder ) <nl> else <nl> hdmi_val |= SDVO_COLOR_FORMAT_8bpc ; <nl>  <nl> - /* Required on CPT */ <nl> - if ( intel_hdmi -> has_hdmi_sink && HAS_PCH_CPT ( dev )) <nl> + if ( intel_hdmi -> has_hdmi_sink && <nl> + ( HAS_PCH_CPT ( dev ) || IS_VALLEYVIEW ( dev ))) <nl> hdmi_val |= HDMI_MODE_SELECT_HDMI ; <nl>  <nl> if ( intel_hdmi -> has_audio ) {
static int davinci_wdt_probe ( struct platform_device * pdev ) <nl> return PTR_ERR ( davinci_wdt -> clk ); <nl> } <nl>  <nl> - clk_prepare_enable ( davinci_wdt -> clk ); <nl> + ret = clk_prepare_enable ( davinci_wdt -> clk ); <nl> + if ( ret ) { <nl> + dev_err (& pdev -> dev , " failed to prepare clock \ n "); <nl> + return ret ; <nl> + } <nl>  <nl> platform_set_drvdata ( pdev , davinci_wdt ); <nl> 
EXPORT_SYMBOL ( vio_unregister_driver ); <nl> /* vio_dev refcount hit 0 */ <nl> static void __devinit vio_dev_release ( struct device * dev ) <nl> { <nl> - if ( dev -> archdata . of_node ) { <nl> - /* XXX should free TCE table */ <nl> - of_node_put ( dev -> archdata . of_node ); <nl> - } <nl> + /* XXX should free TCE table */ <nl> + of_node_put ( dev -> archdata . of_node ); <nl> kfree ( to_vio_dev ( dev )); <nl> } <nl> 
static int wil_cfg80211_stop_ap ( struct wiphy * wiphy , <nl> wil6210_bus_request ( wil , WIL_DEFAULT_BUS_REQUEST_KBPS ); <nl> wil_set_recovery_state ( wil , fw_recovery_idle ); <nl>  <nl> + set_bit ( wil_status_resetting , wil -> status ); <nl> + <nl> mutex_lock (& wil -> mutex ); <nl>  <nl> wmi_pcp_stop ( wil );
static void mwifiex_tdls_add_aid ( struct mwifiex_private * priv , <nl> pos = ( void *) skb_put ( skb , 4 ); <nl> * pos ++ = WLAN_EID_AID ; <nl> * pos ++ = 2 ; <nl> - * pos ++ = le16_to_cpu ( assoc_rsp -> a_id ); <nl> + memcpy ( pos , & assoc_rsp -> a_id , sizeof ( assoc_rsp -> a_id )); <nl>  <nl> return ; <nl> }
struct pci_dn * handle_eeh_events ( struct eeh_event * event ) <nl> } <nl>  <nl> /* All devices should claim they have recovered by now . */ <nl> - if ( result != PCI_ERS_RESULT_RECOVERED ) { <nl> + if (( result != PCI_ERS_RESULT_RECOVERED ) && <nl> + ( result != PCI_ERS_RESULT_NONE )) { <nl> printk ( KERN_WARNING " EEH : Not recovered \ n "); <nl> goto hard_fail ; <nl> }
ieee80211_sta_process_chanswitch ( struct ieee80211_sub_if_data * sdata , <nl> case - 1 : <nl> cfg80211_chandef_create (& new_chandef , new_chan , <nl> NL80211_CHAN_NO_HT ); <nl> + /* keep width for 5 / 10 MHz channels */ <nl> + switch ( sdata -> vif . bss_conf . chandef . width ) { <nl> + case NL80211_CHAN_WIDTH_5 : <nl> + case NL80211_CHAN_WIDTH_10 : <nl> + new_chandef . width = sdata -> vif . bss_conf . chandef . width ; <nl> + break ; <nl> + default : <nl> + break ; <nl> + } <nl> break ; <nl> } <nl> 
static int pcie_find_smpss ( struct pci_dev * dev , void * data ) <nl> * will occur as normal . <nl> */ <nl> if ( dev -> is_hotplug_bridge && (! list_is_singular (& dev -> bus -> devices ) || <nl> - dev -> bus -> self -> pcie_type != PCI_EXP_TYPE_ROOT_PORT )) <nl> + ( dev -> bus -> self && <nl> + dev -> bus -> self -> pcie_type != PCI_EXP_TYPE_ROOT_PORT ))) <nl> * smpss = 0 ; <nl>  <nl> if (* smpss > dev -> pcie_mpss )
static int __build_sched_domains ( const cpumask_t * cpu_map , <nl> error : <nl> free_sched_groups ( cpu_map , tmpmask ); <nl> SCHED_CPUMASK_FREE (( void *) allmasks ); <nl> + kfree ( rd ); <nl> return - ENOMEM ; <nl> # endif <nl> }
static int snd_compr_allocate_buffer ( struct snd_compr_stream * stream , <nl> unsigned int buffer_size ; <nl> void * buffer ; <nl>  <nl> + if ( params -> buffer . fragment_size == 0 || <nl> + params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + return - EINVAL ; <nl> + <nl> buffer_size = params -> buffer . fragment_size * params -> buffer . fragments ; <nl> if ( stream -> ops -> copy ) { <nl> buffer = NULL ;
static void dwc2_hcd_cleanup_channels ( struct dwc2_hsotg * hsotg ) <nl> */ <nl> channel -> qh = NULL ; <nl> } <nl> + /* All channels have been freed , mark them available */ <nl> + if ( hsotg -> core_params -> uframe_sched > 0 ) { <nl> + hsotg -> available_host_channels = <nl> + hsotg -> core_params -> host_channels ; <nl> + } else { <nl> + hsotg -> non_periodic_channels = 0 ; <nl> + hsotg -> periodic_channels = 0 ; <nl> + } <nl> } <nl>  <nl> /**
static void oz_usb_handle_ep_data ( struct oz_usb_ctx * usb_ctx , <nl> struct oz_multiple_fixed * body = <nl> ( struct oz_multiple_fixed *) data_hdr ; <nl> u8 * data = body -> data ; <nl> - int n = ( len - sizeof ( struct oz_multiple_fixed )+ 1 ) <nl> + int n ; <nl> + if (! body -> unit_size ) <nl> + break ; <nl> + n = ( len - sizeof ( struct oz_multiple_fixed )+ 1 ) <nl> / body -> unit_size ; <nl> while ( n --) { <nl> oz_hcd_data_ind ( usb_ctx -> hport , body -> endpoint ,
static const struct pinmux_ops sunxi_pmx_ops = { <nl> . get_function_groups = sunxi_pmx_get_func_groups , <nl> . set_mux = sunxi_pmx_set_mux , <nl> . gpio_set_direction = sunxi_pmx_gpio_set_direction , <nl> + . strict = true , <nl> }; <nl>  <nl> static int sunxi_pinctrl_gpio_direction_input ( struct gpio_chip * chip ,
int MoxaDriverIoctl ( unsigned int cmd , unsigned long arg , int port ) <nl> case MOXA_FIND_BOARD : <nl> case MOXA_LOAD_C320B : <nl> case MOXA_LOAD_CODE : <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> break ; <nl> } <nl> 
static int emc1403_detect ( struct i2c_client * client , <nl> } <nl>  <nl> id = i2c_smbus_read_byte_data ( client , THERMAL_REVISION_REG ); <nl> - if ( id != 0x01 ) <nl> + if ( id < 0x01 || id > 0x04 ) <nl> return - ENODEV ; <nl>  <nl> return 0 ;
static int spear_cpufreq_target ( struct cpufreq_policy * policy , <nl> } <nl>  <nl> newfreq = clk_round_rate ( srcclk , newfreq * mult ); <nl> - if ( newfreq < 0 ) { <nl> + if ( newfreq <= 0 ) { <nl> pr_err (" clk_round_rate failed for cpu src clock \ n "); <nl> return newfreq ; <nl> }
static void compat_input ( struct dlm_write_request * kb , <nl> static void compat_output ( struct dlm_lock_result * res , <nl> struct dlm_lock_result32 * res32 ) <nl> { <nl> + memset ( res32 , 0 , sizeof (* res32 )); <nl> + <nl> res32 -> version [ 0 ] = res -> version [ 0 ]; <nl> res32 -> version [ 1 ] = res -> version [ 1 ]; <nl> res32 -> version [ 2 ] = res -> version [ 2 ];
static inline void intel_ring_emit_wa ( struct intel_engine_cs * ring , <nl> struct drm_device * dev = ring -> dev ; <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl>  <nl> - if ( dev_priv -> num_wa_regs > I915_MAX_WA_REGS ) <nl> + if ( dev_priv -> num_wa_regs >= I915_MAX_WA_REGS ) <nl> return ; <nl>  <nl> intel_ring_emit ( ring , MI_LOAD_REGISTER_IMM ( 1 ));
static void ar9003_hw_prog_ini ( struct ath_hw * ah , <nl> u32 val = INI_RA ( iniArr , i , column ); <nl>  <nl> REG_WRITE ( ah , reg , val ); <nl> - <nl> - /* <nl> - * Determine if this is a shift register value , and insert the <nl> - * configured delay if so . <nl> - */ <nl> - if ( reg >= 0x16000 && reg < 0x17000 <nl> - && ah -> config . analog_shiftreg ) <nl> - udelay ( 100 ); <nl> - <nl> DO_DELAY ( regWrites ); <nl> } <nl> }
EXPORT_SYMBOL_GPL ( mnt_clone_write ); <nl> */ <nl> int mnt_want_write_file ( struct file * file ) <nl> { <nl> - if (!( file -> f_mode & FMODE_WRITE )) <nl> + struct inode * inode = file -> f_dentry -> d_inode ; <nl> + if (!( file -> f_mode & FMODE_WRITE ) || special_file ( inode -> i_mode )) <nl> return mnt_want_write ( file -> f_path . mnt ); <nl> else <nl> return mnt_clone_write ( file -> f_path . mnt );
static noinline int __btrfs_cow_block ( struct btrfs_trans_handle * trans , <nl> btrfs_set_node_ptr_generation ( parent , parent_slot , <nl> trans -> transid ); <nl> btrfs_mark_buffer_dirty ( parent ); <nl> - tree_mod_log_free_eb ( root -> fs_info , buf ); <nl> + if ( last_ref ) <nl> + tree_mod_log_free_eb ( root -> fs_info , buf ); <nl> btrfs_free_tree_block ( trans , root , buf , parent_start , <nl> last_ref ); <nl> }
static int do_ip_setsockopt ( struct sock * sk , int level , <nl> * Check the arguments are allowable <nl> */ <nl>  <nl> + if ( optlen < sizeof ( struct in_addr )) <nl> + goto e_inval ; <nl> + <nl> err = - EFAULT ; <nl> if ( optlen >= sizeof ( struct ip_mreqn )) { <nl> if ( copy_from_user (& mreq , optval , sizeof ( mreq )))
struct hid_device * hid_allocate_device ( void ) <nl> device_initialize (& hdev -> dev ); <nl> hdev -> dev . release = hid_device_release ; <nl> hdev -> dev . bus = & hid_bus_type ; <nl> + device_enable_async_suspend (& hdev -> dev ); <nl>  <nl> hid_close_report ( hdev ); <nl> 
do_send_specific ( pid_t tgid , pid_t pid , int sig , struct siginfo * info ) <nl>  <nl> static int do_tkill ( pid_t tgid , pid_t pid , int sig ) <nl> { <nl> - struct siginfo info ; <nl> + struct siginfo info = {}; <nl>  <nl> info . si_signo = sig ; <nl> info . si_errno = 0 ;
int nvdimm_has_flush ( struct nd_region * nd_region ) <nl> { <nl> int i ; <nl>  <nl> - /* no nvdimm == flushing capability unknown */ <nl> - if ( nd_region -> ndr_mappings == 0 ) <nl> + /* no nvdimm or pmem api == flushing capability unknown */ <nl> + if ( nd_region -> ndr_mappings == 0 <nl> + || ! IS_ENABLED ( CONFIG_ARCH_HAS_PMEM_API )) <nl> return - ENXIO ; <nl>  <nl> for ( i = 0 ; i < nd_region -> ndr_mappings ; i ++) {
static int pulse8_cec_adap_transmit ( struct cec_adapter * adap , u8 attempts , <nl> int err ; <nl>  <nl> cmd [ 0 ] = MSGCODE_TRANSMIT_IDLETIME ; <nl> - cmd [ 1 ] = 3 ; <nl> + cmd [ 1 ] = signal_free_time ; <nl> err = pulse8_send_and_wait ( pulse8 , cmd , 2 , <nl> MSGCODE_COMMAND_ACCEPTED , 1 ); <nl> cmd [ 0 ] = MSGCODE_TRANSMIT_ACK_POLARITY ;
int smb2_tree_disconnect ( struct ksmbd_work * work ) <nl>  <nl> ksmbd_close_tree_conn_fds ( work ); <nl> ksmbd_tree_conn_disconnect ( sess , tcon ); <nl> + work -> tcon = NULL ; <nl> return 0 ; <nl> } <nl> 
static int upgrade_fw ( struct adapter * adap ) <nl> if (! ret ) <nl> dev_info ( dev , " firmware upgraded to version % pI4 from " <nl> FW_FNAME "\ n ", & hdr -> fw_ver ); <nl> + } else { <nl> + /* <nl> + * Tell our caller that we didn ' t upgrade the firmware . <nl> + */ <nl> + ret = - EINVAL ; <nl> } <nl> + <nl> out : release_firmware ( fw ); <nl> return ret ; <nl> }
static void __ccw_device_pm_restore ( struct ccw_device * cdev ) <nl> * available again . Kick re - detection . <nl> */ <nl> cdev -> private -> flags . resuming = 1 ; <nl> + cdev -> private -> path_new_mask = LPM_ANYPATH ; <nl> css_schedule_eval ( sch -> schid ); <nl> spin_unlock_irq ( sch -> lock ); <nl> css_complete_work ();
e1000_up ( struct e1000_adapter * adapter ) <nl> return err ; <nl>  <nl> mod_timer (& adapter -> watchdog_timer , jiffies ); <nl> - e1000_irq_enable ( adapter ); <nl>  <nl> # ifdef CONFIG_E1000_NAPI <nl> netif_poll_enable ( netdev ); <nl> # endif <nl> + e1000_irq_enable ( adapter ); <nl> + <nl> return 0 ; <nl> } <nl> 
static inline void activate_mm ( struct mm_struct * old , struct mm_struct * new ) <nl> * possible . <nl> */ <nl> if ( old != new && ( current -> flags & PF_BORROWED_MM )) <nl> - force_flush_all (); <nl> + CHOOSE_MODE ( force_flush_all (), <nl> + switch_mm_skas (& new -> context . skas . id )); <nl> } <nl>  <nl> static inline void switch_mm ( struct mm_struct * prev , struct mm_struct * next ,
static int efx_ef10_probe ( struct efx_nic * efx ) <nl> EFX_MAX_CHANNELS , <nl> resource_size (& efx -> pci_dev -> resource [ EFX_MEM_BAR ]) / <nl> ( EFX_VI_PAGE_SIZE * EFX_TXQ_TYPES )); <nl> - BUG_ON ( efx -> max_channels == 0 ); <nl> + if ( WARN_ON ( efx -> max_channels == 0 )) <nl> + return - EIO ; <nl>  <nl> nic_data = kzalloc ( sizeof (* nic_data ), GFP_KERNEL ); <nl> if (! nic_data )
static void amdgpu_gem_va_update_vm ( struct amdgpu_device * adev , <nl> goto error ; <nl>  <nl> if ( operation == AMDGPU_VA_OP_MAP || <nl> - operation == AMDGPU_VA_OP_REPLACE ) <nl> + operation == AMDGPU_VA_OP_REPLACE ) { <nl> r = amdgpu_vm_bo_update ( adev , bo_va , false ); <nl> + if ( r ) <nl> + goto error ; <nl> + } <nl>  <nl> r = amdgpu_vm_update_directories ( adev , vm ); <nl> - if ( r ) <nl> - goto error ; <nl>  <nl> error : <nl> if ( r && r != - ERESTARTSYS )
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> + memset ( sax , 0 , sizeof ( sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
u64 snd_usb_interface_dsd_format_quirks ( struct snd_usb_audio * chip , <nl> } <nl> } <nl> break ; <nl> + case USB_ID ( 0x16d0 , 0x0a23 ): <nl> + if ( fp -> altsetting == 2 ) <nl> + return SNDRV_PCM_FMTBIT_DSD_U32_BE ; <nl> + break ; <nl>  <nl> default : <nl> break ;
static int treo_attach ( struct usb_serial * serial ) <nl> ( serial -> num_interrupt_in == 0 )) <nl> return 0 ; <nl>  <nl> + if ( serial -> num_bulk_in < 2 || serial -> num_interrupt_in < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> /* <nl> * It appears that Treos and Kyoceras want to use the <nl> * 1st bulk in endpoint to communicate with the 2nd bulk out endpoint ,
static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
static long media_device_enum_entities ( struct media_device * mdev , <nl> struct media_entity * ent ; <nl> struct media_entity_desc u_ent ; <nl>  <nl> + memset (& u_ent , 0 , sizeof ( u_ent )); <nl> if ( copy_from_user (& u_ent . id , & uent -> id , sizeof ( u_ent . id ))) <nl> return - EFAULT ; <nl> 
static int easycap_usb_probe ( struct usb_interface * intf , <nl> /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */ <nl> pdata_urb = kzalloc ( sizeof ( struct data_urb ), GFP_KERNEL ); <nl> if (! pdata_urb ) { <nl> + usb_free_urb ( purb ); <nl> SAM (" ERROR : Could not allocate struct data_urb .\ n "); <nl> return - ENOMEM ; <nl> }
static const struct { <nl> . bDescriptorType = USB_DT_ENDPOINT , <nl> . bEndpointAddress = 1 | USB_DIR_IN , <nl> . bmAttributes = USB_ENDPOINT_XFER_BULK , <nl> + . wMaxPacketSize = htole16 ( 512 ), <nl> }, <nl> . bulk_source = { <nl> . bLength = sizeof ( descriptors . hs_descs . bulk_source ), <nl> . bDescriptorType = USB_DT_ENDPOINT , <nl> . bEndpointAddress = 2 | USB_DIR_OUT , <nl> . bmAttributes = USB_ENDPOINT_XFER_BULK , <nl> + . wMaxPacketSize = htole16 ( 512 ), <nl> }, <nl> }, <nl> };
void ide_do_drive_cmd ( ide_drive_t * drive , struct request * rq ) <nl>  <nl> spin_lock_irqsave ( q -> queue_lock , flags ); <nl> __elv_add_request ( q , rq , ELEVATOR_INSERT_FRONT , 0 ); <nl> - blk_start_queueing ( q ); <nl> spin_unlock_irqrestore ( q -> queue_lock , flags ); <nl> } <nl> EXPORT_SYMBOL ( ide_do_drive_cmd );
int brcmf_fws_hdrpull ( struct brcmf_pub * drvr , int ifidx , s16 signal_len , <nl> if (! signal_len ) <nl> return 0 ; <nl> /* if flow control disabled , skip to packet data and leave */ <nl> - if (! fws -> fw_signals ) { <nl> + if ((! fws ) || (! fws -> fw_signals )) { <nl> skb_pull ( skb , signal_len ); <nl> return 0 ; <nl> }
static struct dma_async_tx_descriptor * sdma_prep_slave_sg ( <nl>  <nl> param = BD_DONE | BD_EXTD | BD_CONT ; <nl>  <nl> - if ( i + 1 == sg_len ) <nl> + if ( i + 1 == sg_len ) { <nl> param |= BD_INTR ; <nl> + param |= BD_LAST ; <nl> + param &= ~ BD_CONT ; <nl> + } <nl>  <nl> dev_dbg ( sdma -> dev , " entry % d : count : % d dma : 0x % 08x % s % s \ n ", <nl> i , count , sg -> dma_address ,
static int count_fastmap_pebs ( struct ubi_attach_info * ai ) <nl> list_for_each_entry ( aeb , & ai -> free , u . list ) <nl> n ++; <nl>  <nl> - ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> + ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> ubi_rb_for_each_entry ( rb2 , aeb , & av -> root , u . rb ) <nl> n ++; <nl> 
static int may_commit_transaction ( struct btrfs_fs_info * fs_info , <nl>  <nl> spin_lock (& delayed_rsv -> lock ); <nl> if ( percpu_counter_compare (& space_info -> total_bytes_pinned , <nl> - bytes - delayed_rsv -> size ) >= 0 ) { <nl> + bytes - delayed_rsv -> size ) < 0 ) { <nl> spin_unlock (& delayed_rsv -> lock ); <nl> return - ENOSPC ; <nl> }
static int gen6_drpc_info ( struct seq_file * m ) <nl>  <nl> rpmodectl1 = I915_READ ( GEN6_RP_CONTROL ); <nl> rcctl1 = I915_READ ( GEN6_RC_CONTROL ); <nl> - sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> mutex_unlock (& dev -> struct_mutex ); <nl> + mutex_lock (& dev_priv -> rps . hw_lock ); <nl> + sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> + mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> seq_printf ( m , " Video Turbo Mode : % s \ n ", <nl> yesno ( rpmodectl1 & GEN6_RP_MEDIA_TURBO ));
static int adu_release ( struct inode * inode , struct file * file ) <nl> retval = adu_release_internal ( dev ); <nl>  <nl> exit : <nl> - up (& dev -> sem ); <nl> + if ( dev ) <nl> + up (& dev -> sem ); <nl> dbg ( 2 ," % s : leave , return value % d ", __FUNCTION__ , retval ); <nl> return retval ; <nl> }
static int ipmi_fasync ( int fd , struct file * file , int on ) <nl> struct ipmi_file_private * priv = file -> private_data ; <nl> int result ; <nl>  <nl> + lock_kernel (); /* could race against open () otherwise */ <nl> result = fasync_helper ( fd , file , on , & priv -> fasync_queue ); <nl> + unlock_kernel (); <nl>  <nl> return ( result ); <nl> }
int cdc_parse_cdc_header ( struct usb_cdc_parsed_header * hdr , <nl> elength = 1 ; <nl> goto next_desc ; <nl> } <nl> + if (( buflen < elength ) || ( elength < 3 )) { <nl> + dev_err (& intf -> dev , " invalid descriptor buffer length \ n "); <nl> + break ; <nl> + } <nl> if ( buffer [ 1 ] != USB_DT_CS_INTERFACE ) { <nl> dev_err (& intf -> dev , " skipping garbage \ n "); <nl> goto next_desc ;
void __init tegra_super_clk_gen4_init ( void __iomem * clk_base , <nl> ARRAY_SIZE ( cclk_lp_parents ), <nl> CLK_SET_RATE_PARENT , <nl> clk_base + CCLKLP_BURST_POLICY , <nl> - 0 , 4 , 8 , 9 , NULL ); <nl> + TEGRA_DIVIDER_2 , 4 , 8 , 9 , NULL ); <nl> * dt_clk = clk ; <nl> } <nl> 
static const struct iio_chan_spec st_press_1_channels [] = { <nl> }, <nl> . info_mask_separate = <nl> BIT ( IIO_CHAN_INFO_RAW ) | BIT ( IIO_CHAN_INFO_SCALE ), <nl> + . info_mask_shared_by_all = BIT ( IIO_CHAN_INFO_SAMP_FREQ ), <nl> }, <nl> { <nl> . type = IIO_TEMP , <nl> static const struct iio_chan_spec st_press_1_channels [] = { <nl> BIT ( IIO_CHAN_INFO_RAW ) | <nl> BIT ( IIO_CHAN_INFO_SCALE ) | <nl> BIT ( IIO_CHAN_INFO_OFFSET ), <nl> + . info_mask_shared_by_all = BIT ( IIO_CHAN_INFO_SAMP_FREQ ), <nl> }, <nl> IIO_CHAN_SOFT_TIMESTAMP ( 2 ) <nl> };
struct qcom_glink * qcom_glink_native_probe ( struct device * dev , <nl> idr_init (& glink -> rcids ); <nl>  <nl> glink -> mbox_client . dev = dev ; <nl> + glink -> mbox_client . knows_txdone = true ; <nl> glink -> mbox_chan = mbox_request_channel (& glink -> mbox_client , 0 ); <nl> if ( IS_ERR ( glink -> mbox_chan )) { <nl> if ( PTR_ERR ( glink -> mbox_chan ) != - EPROBE_DEFER )
efx_mcdi_mon_add_attr ( struct efx_nic * efx , const char * name , <nl> attr -> index = index ; <nl> attr -> type = type ; <nl> attr -> limit_value = limit_value ; <nl> + sysfs_attr_init (& attr -> dev_attr . attr ); <nl> attr -> dev_attr . attr . name = attr -> name ; <nl> attr -> dev_attr . attr . mode = S_IRUGO ; <nl> attr -> dev_attr . show = reader ;
static int mxcnd_probe ( struct platform_device * pdev ) <nl> init_completion (& host -> op_completion ); <nl>  <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq < 0 ) <nl> + return host -> irq ; <nl>  <nl> /* <nl> * Use host -> devtype_data -> irq_control () here instead of irq_control ()
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> write_unlock_bh (& bond -> curr_slave_lock ); <nl> read_unlock (& bond -> lock ); <nl> } <nl> + slave_disable_netpoll ( new_slave ); <nl>  <nl> err_close : <nl> slave_dev -> priv_flags &= ~ IFF_BONDING ;
static ssize_t i40e_dbg_command_write ( struct file * filp , <nl> if (! cmd_buf ) <nl> return count ; <nl> bytes_not_copied = copy_from_user ( cmd_buf , buffer , count ); <nl> - if ( bytes_not_copied < 0 ) <nl> + if ( bytes_not_copied < 0 ) { <nl> + kfree ( cmd_buf ); <nl> return bytes_not_copied ; <nl> + } <nl> if ( bytes_not_copied > 0 ) <nl> count -= bytes_not_copied ; <nl> cmd_buf [ count ] = '\ 0 ';
static void __init __e820_add_region ( struct e820map * e820x , u64 start , u64 size , <nl> { <nl> int x = e820x -> nr_map ; <nl>  <nl> - if ( x == ARRAY_SIZE ( e820x -> map )) { <nl> + if ( x >= ARRAY_SIZE ( e820x -> map )) { <nl> printk ( KERN_ERR " Ooops ! Too many entries in the memory map !\ n "); <nl> return ; <nl> }
static int rdma_cma_handler ( struct rdma_cm_id * cma_id , <nl> if ( xprt ) { <nl> set_bit ( XPT_CLOSE , & xprt -> xpt_flags ); <nl> svc_xprt_enqueue ( xprt ); <nl> + svc_xprt_put ( xprt ); <nl> } <nl> break ; <nl> case RDMA_CM_EVENT_DEVICE_REMOVAL :
static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath_softc * sc = hw -> priv ; <nl> int ret = 0 ; <nl>  <nl> - local_bh_disable (); <nl> + mutex_lock (& sc -> mutex ); <nl>  <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( sc -> sc_ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> - local_bh_enable (); <nl> + mutex_unlock (& sc -> mutex ); <nl>  <nl> return ret ; <nl> }
static int virtnet_probe ( struct virtio_device * vdev ) <nl> dev -> mtu = mtu ; <nl> dev -> max_mtu = mtu ; <nl> } <nl> + <nl> + /* TODO : size buffers correctly in this case . */ <nl> + if ( dev -> mtu > ETH_DATA_LEN ) <nl> + vi -> big_packets = true ; <nl> } <nl>  <nl> if ( vi -> any_header_sg )
int blkcg_init_queue ( struct request_queue * q ) <nl> if ( preloaded ) <nl> radix_tree_preload_end (); <nl>  <nl> - if ( IS_ERR ( blkg )) { <nl> - blkg_free ( new_blkg ); <nl> + if ( IS_ERR ( blkg )) <nl> return PTR_ERR ( blkg ); <nl> - } <nl>  <nl> q -> root_blkg = blkg ; <nl> q -> root_rl . blkg = blkg ;
static int ext4_quota_enable ( struct super_block * sb , int type , int format_id , <nl> return PTR_ERR ( qf_inode ); <nl> } <nl>  <nl> + /* Don ' t account quota for quota files to avoid recursion */ <nl> + qf_inode -> i_flags |= S_NOQUOTA ; <nl> err = dquot_enable ( qf_inode , type , format_id , flags ); <nl> iput ( qf_inode ); <nl> 
# include < linux / hdmi . h > <nl> # include " hdmi . h " <nl>  <nl> - <nl> -/* Supported HDMI Audio channels */ <nl> -# define MSM_HDMI_AUDIO_CHANNEL_2 0 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_4 1 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_6 2 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_8 3 <nl> - <nl> /* maps MSM_HDMI_AUDIO_CHANNEL_n consts used by audio driver to # of channels : */ <nl> static int nchannels [] = { 2 , 4 , 6 , 8 }; <nl> 
int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> which_atu = 0 ; <nl> } <nl>  <nl> - if (! which_atu ) <nl> + if (! which_atu ) { <nl> + kfree ( res ); <nl> return 0 ; <nl> + } <nl>  <nl> switch ( which_atu ) { <nl> case IOP13XX_INIT_ATU_ATUX : <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> sys -> map_irq = iop13xx_pcie_map_irq ; <nl> break ; <nl> default : <nl> + kfree ( res ); <nl> return 0 ; <nl> } <nl> 
static int __init cy_detect_isa ( void ) <nl> continue ; <nl> } <nl> # ifdef MODULE <nl> - if ( isparam && irq [ i ]) <nl> + if ( isparam && i < NR_CARDS && irq [ i ]) <nl> cy_isa_irq = irq [ i ]; <nl> else <nl> # endif
static int dlm_add_member ( struct dlm_ls * ls , int nodeid ) <nl> return - ENOMEM ; <nl>  <nl> w = dlm_node_weight ( ls -> ls_name , nodeid ); <nl> - if ( w < 0 ) <nl> + if ( w < 0 ) { <nl> + kfree ( memb ); <nl> return w ; <nl> + } <nl>  <nl> memb -> nodeid = nodeid ; <nl> memb -> weight = w ;
static int vesafb_setcolreg ( unsigned regno , unsigned red , unsigned green , <nl>  <nl> static void vesafb_destroy ( struct fb_info * info ) <nl> { <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> if ( info -> screen_base ) <nl> iounmap ( info -> screen_base ); <nl> release_mem_region ( info -> apertures -> ranges [ 0 ]. base , info -> apertures -> ranges [ 0 ]. size );
int intel_svm_bind_mm ( struct device * dev , int * pasid , int flags , struct svm_dev_ <nl> pasid_max - 1 , GFP_KERNEL ); <nl> if ( ret < 0 ) { <nl> kfree ( svm ); <nl> + kfree ( sdev ); <nl> goto out ; <nl> } <nl> svm -> pasid = ret ;
static int acm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> else <nl> rv = 0 ; <nl>  <nl> + set_bit ( TTY_NO_WRITE_SPLIT , & tty -> flags ); <nl> tty -> driver_data = acm ; <nl> acm -> tty = tty ; <nl> 
bool kvm_vgic_map_is_active ( struct kvm_vcpu * vcpu , struct irq_phys_map * map ) <nl> return true ; <nl> } <nl>  <nl> - return dist_active_irq ( vcpu ); <nl> + return vgic_irq_is_active ( vcpu , map -> virt_irq ); <nl> } <nl>  <nl> /*
int x86_decode_insn ( struct x86_emulate_ctxt * ctxt , void * insn , int insn_len ) <nl> /* Decode and fetch the destination operand : register or memory . */ <nl> rc = decode_operand ( ctxt , & ctxt -> dst , ( ctxt -> d >> DstShift ) & OpMask ); <nl>  <nl> - done : <nl> if ( ctxt -> rip_relative ) <nl> ctxt -> memopp -> addr . mem . ea += ctxt -> _eip ; <nl>  <nl> + done : <nl> return ( rc != X86EMUL_CONTINUE ) ? EMULATION_FAILED : EMULATION_OK ; <nl> } <nl> 
void rtl8188eu_set_hal_ops ( struct adapter * adapt ) <nl>  <nl>  <nl> adapt -> HalData = kzalloc ( sizeof ( struct hal_data_8188e ), GFP_KERNEL ); <nl> - if ( adapt -> HalData == NULL ) <nl> + if (! adapt -> HalData ) <nl> DBG_88E (" cant not alloc memory for HAL DATA \ n "); <nl>  <nl> halfunc -> hal_power_on = rtl8188eu_InitPowerOn ;
static int ext4_valid_extent ( struct inode * inode , struct ext4_extent * ext ) <nl> ext4_fsblk_t block = ext4_ext_pblock ( ext ); <nl> int len = ext4_ext_get_actual_len ( ext ); <nl>  <nl> + if ( len == 0 ) <nl> + return 0 ; <nl> return ext4_data_block_valid ( EXT4_SB ( inode -> i_sb ), block , len ); <nl> } <nl> 
static inline struct sk_buff * sk_stream_alloc_pskb ( struct sock * sk , <nl> { <nl> struct sk_buff * skb ; <nl>  <nl> + /* The TCP header must be at least 32 - bit aligned . */ <nl> + size = ALIGN ( size , 4 ); <nl> + <nl> skb = alloc_skb_fclone ( size + sk -> sk_prot -> max_header , gfp ); <nl> if ( skb ) { <nl> skb -> truesize += mem ;
static int __devinit lpc32xx_nand_probe ( struct platform_device * pdev ) <nl> dev_err (& pdev -> dev , " Missing platform data \ n "); <nl> return - ENOENT ; <nl> } <nl> + if ( host -> ncfg -> wp_gpio == - EPROBE_DEFER ) <nl> + return - EPROBE_DEFER ; <nl> if ( gpio_is_valid ( host -> ncfg -> wp_gpio ) && <nl> gpio_request ( host -> ncfg -> wp_gpio , " NAND WP ")) { <nl> dev_err (& pdev -> dev , " GPIO not available \ n ");
static struct console usbcons = { <nl>  <nl> void usb_serial_console_disconnect ( struct usb_serial * serial ) <nl> { <nl> - if ( serial -> port [ 0 ] == usbcons_info . port ) { <nl> + if ( serial -> port [ 0 ] && serial -> port [ 0 ] == usbcons_info . port ) { <nl> usb_serial_console_exit (); <nl> usb_serial_put ( serial ); <nl> }
skl_tplg_init_pipe_modules ( struct skl * skl , struct skl_pipe * pipe ) <nl> if ( mconfig -> id . module_id < 0 ) { <nl> struct skl_dfw_module * dfw_config ; <nl>  <nl> - dfw_config = kzalloc ( sizeof ( dfw_config ), GFP_KERNEL ); <nl> + dfw_config = kzalloc ( sizeof (* dfw_config ), GFP_KERNEL ); <nl> if (! dfw_config ) <nl> return - ENOMEM ; <nl> 
int fscrypt_has_permitted_context ( struct inode * parent , struct inode * child ) <nl> BUG_ON ( 1 ); <nl> } <nl>  <nl> + /* No restrictions on file types which are never encrypted */ <nl> + if (! S_ISREG ( child -> i_mode ) && ! S_ISDIR ( child -> i_mode ) && <nl> + ! S_ISLNK ( child -> i_mode )) <nl> + return 1 ; <nl> + <nl> /* no restrictions if the parent directory is not encrypted */ <nl> if (! parent -> i_sb -> s_cop -> is_encrypted ( parent )) <nl> return 1 ;
static int write_vmem ( struct fbtft_par * par , size_t offset , size_t len ) <nl> signed short * convert_buf = kmalloc ( par -> info -> var . xres * <nl> par -> info -> var . yres * sizeof ( signed short ), GFP_NOIO ); <nl>  <nl> + if (! convert_buf ) <nl> + return - ENOMEM ; <nl> + <nl> fbtft_par_dbg ( DEBUG_WRITE_VMEM , par , "% s ()\ n ", __func__ ); <nl>  <nl> /* converting to grayscale16 */
sctp_disposition_t sctp_sf_eat_auth ( const struct sctp_endpoint * ep , <nl> struct sctp_chunk * err_chunk ; <nl> sctp_ierror_t error ; <nl>  <nl> + /* Make sure that the peer has AUTH capable */ <nl> + if (! asoc -> peer . auth_capable ) <nl> + return sctp_sf_unk_chunk ( ep , asoc , type , arg , commands ); <nl> + <nl> if (! sctp_vtag_verify ( chunk , asoc )) { <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_REPORT_BAD_TAG , <nl> SCTP_NULL ());
int be_cmd_if_create ( struct be_adapter * adapter , u32 cap_flags , u32 en_flags , <nl> if (! status ) { <nl> struct be_cmd_resp_if_create * resp = embedded_payload ( wrb ); <nl> * if_handle = le32_to_cpu ( resp -> interface_id ); <nl> + <nl> + /* Hack to retrieve VF ' s pmac - id on BE3 */ <nl> + if ( BE3_chip ( adapter ) && ! be_physfn ( adapter )) <nl> + adapter -> pmac_id [ 0 ] = le32_to_cpu ( resp -> pmac_id ); <nl> } <nl>  <nl> err :
static int qeth_query_card_info ( struct qeth_card * card , <nl>  <nl> static inline int qeth_get_qdio_q_format ( struct qeth_card * card ) <nl> { <nl> - switch ( card -> info . type ) { <nl> - case QETH_CARD_TYPE_IQD : <nl> - return 2 ; <nl> - default : <nl> - return 0 ; <nl> - } <nl> + if ( card -> info . type == QETH_CARD_TYPE_IQD ) <nl> + return QDIO_IQDIO_QFMT ; <nl> + else <nl> + return QDIO_QETH_QFMT ; <nl> } <nl>  <nl> static void qeth_determine_capabilities ( struct qeth_card * card )
static int perf_sched__process_tracepoint_sample ( struct perf_tool * tool __maybe_ <nl> struct perf_evsel * evsel , <nl> struct machine * machine ) <nl> { <nl> - struct thread * thread = machine__findnew_thread ( machine , sample -> pid ); <nl> + struct thread * thread = machine__findnew_thread ( machine , sample -> tid ); <nl> int err = 0 ; <nl>  <nl> if ( thread == NULL ) {
struct gb_host_device * gb_hd_create ( struct gb_hd_driver * driver , <nl> return ERR_PTR (- EINVAL ); <nl> } <nl>  <nl> - if ( num_cports == 0 || num_cports > CPORT_ID_MAX ) { <nl> + if ( num_cports == 0 || num_cports > CPORT_ID_MAX + 1 ) { <nl> dev_err ( parent , " Invalid number of CPorts : % zu \ n ", num_cports ); <nl> return ERR_PTR (- EINVAL ); <nl> }
static struct sock * pep_sock_accept ( struct sock * sk , int flags , int * errp , <nl>  <nl> err = pep_accept_conn ( newsk , skb ); <nl> if ( err ) { <nl> + __sock_put ( sk ); <nl> sock_put ( newsk ); <nl> newsk = NULL ; <nl> goto drop ;
static int rsc_parse ( struct cache_detail * cd , <nl> /* number of additional gid ' s */ <nl> if ( get_int (& mesg , & N )) <nl> goto out ; <nl> + if ( N < 0 || N > NGROUPS_MAX ) <nl> + goto out ; <nl> status = - ENOMEM ; <nl> rsci . cred . cr_group_info = groups_alloc ( N ); <nl> if ( rsci . cred . cr_group_info == NULL )
static void __init setup_lowcore ( void ) <nl> + PAGE_SIZE - STACK_FRAME_OVERHEAD - sizeof ( struct pt_regs ); <nl> lc -> current_task = ( unsigned long ) init_thread_union . thread_info . task ; <nl> lc -> thread_info = ( unsigned long ) & init_thread_union ; <nl> + lc -> lpp = LPP_MAGIC ; <nl> lc -> machine_flags = S390_lowcore . machine_flags ; <nl> lc -> stfl_fac_list = S390_lowcore . stfl_fac_list ; <nl> memcpy ( lc -> stfle_fac_list , S390_lowcore . stfle_fac_list ,
static inline int init_new_context ( struct task_struct * tsk , <nl> mm -> context . execute_only_pkey = - 1 ; <nl> } <nl> # endif <nl> - init_new_context_ldt ( tsk , mm ); <nl> - <nl> - return 0 ; <nl> + return init_new_context_ldt ( tsk , mm ); <nl> } <nl> static inline void destroy_context ( struct mm_struct * mm ) <nl> {
# include " commands . h " <nl> # include " power . h " <nl>  <nl> - static bool force_cam ; <nl> + static bool force_cam = true ; <nl> module_param ( force_cam , bool , 0644 ); <nl> MODULE_PARM_DESC ( force_cam , " force continuously aware mode ( no power saving at all )"); <nl> 
int selinux_task_prlimit ( const struct cred * cred , const struct cred * tcred , <nl> { <nl> u32 av = 0 ; <nl>  <nl> + if (! flags ) <nl> + return 0 ; <nl> if ( flags & LSM_PRLIMIT_WRITE ) <nl> av |= PROCESS__SETRLIMIT ; <nl> if ( flags & LSM_PRLIMIT_READ )
static void scan_requests ( struct ceph_osd * osd , <nl> list_add_tail (& lreq -> scan_item , need_resend_linger ); <nl> break ; <nl> case CALC_TARGET_POOL_DNE : <nl> + list_del_init (& lreq -> scan_item ); <nl> check_linger_pool_dne ( lreq ); <nl> break ; <nl> }
xlog_recover_do_reg_buffer ( <nl> stale_buf = 1 ; <nl> break ; <nl> } <nl> - if ( be16_to_cpu ( dip -> di_core . di_mode )) <nl> + if ( dip -> di_core . di_mode ) <nl> mode_count ++; <nl> - if ( be16_to_cpu ( dip -> di_core . di_gen )) <nl> + if ( dip -> di_core . di_gen ) <nl> gen_count ++; <nl> } <nl> 
struct nvkm_pll_vals ; <nl>  <nl> struct nv04_devinit_priv { <nl> struct nvkm_devinit base ; <nl> - u8 owner ; <nl> + int owner ; <nl> }; <nl>  <nl> int nv04_devinit_ctor ( struct nvkm_object *, struct nvkm_object *,
static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> err = rpmsg_ns_register_device ( rpdev_ns ); <nl> if ( err ) <nl> - goto free_vch ; <nl> + /* vch will be free in virtio_rpmsg_release_device () */ <nl> + goto free_ctrldev ; <nl> } <nl>  <nl> /* <nl> static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> return 0 ; <nl>  <nl> - free_vch : <nl> - kfree ( vch ); <nl> free_ctrldev : <nl> rpmsg_virtio_del_ctrl_dev ( rpdev_ctrl ); <nl> free_coherent :
static void restore_custom_reg_settings ( struct wiphy * wiphy ) <nl> chan -> flags = chan -> orig_flags ; <nl> chan -> max_antenna_gain = chan -> orig_mag ; <nl> chan -> max_power = chan -> orig_mpwr ; <nl> + chan -> beacon_found = false ; <nl> } <nl> } <nl> }
intel_ioapic_set_affinity ( struct irq_data * data , const struct cpumask * mask , <nl>  <nl> err = apic -> cpu_mask_to_apicid_and ( cfg -> domain , mask , & dest ); <nl> if ( err ) { <nl> - if ( assign_irq_vector ( irq , cfg , data -> affinity )); <nl> + if ( assign_irq_vector ( irq , cfg , data -> affinity )) <nl> pr_err (" Failed to recover vector for irq % d \ n ", irq ); <nl> return err ; <nl> }
void intel_lrc_irq_handler ( struct intel_engine_cs * ring ) <nl>  <nl> spin_unlock (& ring -> execlist_lock ); <nl>  <nl> - WARN ( submit_contexts > 2 , " More than two context complete events ?\ n "); <nl> + if ( unlikely ( submit_contexts > 2 )) <nl> + DRM_ERROR (" More than two context complete events ?\ n "); <nl> + <nl> ring -> next_context_status_buffer = write_pointer % GEN8_CSB_ENTRIES ; <nl>  <nl> /* Update the read pointer to the old write pointer . Manual ringbuffer
static int kcm_sendmsg ( struct socket * sock , struct msghdr * msg , size_t len ) <nl> } else { <nl> /* Message not complete , save state */ <nl> partial_message : <nl> - kcm -> seq_skb = head ; <nl> - kcm_tx_msg ( head )-> last_skb = skb ; <nl> + if ( head ) { <nl> + kcm -> seq_skb = head ; <nl> + kcm_tx_msg ( head )-> last_skb = skb ; <nl> + } <nl> } <nl>  <nl> KCM_STATS_ADD ( kcm -> stats . tx_bytes , copied );
compat_ptr ( compat_uptr_t uptr ) <nl> return ( void __user *) ( unsigned long ) uptr ; <nl> } <nl>  <nl> + static inline compat_uptr_t <nl> + ptr_to_compat ( void __user * uptr ) <nl> +{ <nl> + return ( u32 )( unsigned long ) uptr ; <nl> +} <nl> + <nl> static __inline__ void __user * <nl> compat_alloc_user_space ( long len ) <nl> {
nv130_chipset = { <nl> static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> + . bios = nvkm_bios_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
static struct input_dev * gb_svc_input_create ( struct gb_svc * svc ) <nl> return input_dev ; <nl>  <nl> err_free_input : <nl> - input_free_device ( svc -> input ); <nl> + input_free_device ( input_dev ); <nl> return ERR_PTR (- ENOMEM ); <nl> } <nl> 
static int ttusbdecfe_dvbs_diseqc_send_master_cmd ( struct dvb_frontend * fe , struc <nl> 0x00 , 0x00 , 0x00 , 0x00 , <nl> 0x00 , 0x00 }; <nl>  <nl> + if ( cmd -> msg_len > sizeof ( b ) - 4 ) <nl> + return - EINVAL ; <nl> + <nl> memcpy (& b [ 4 ], cmd -> msg , cmd -> msg_len ); <nl>  <nl> state -> config -> send_command ( fe , 0x72 ,
xfs_qm_dquot_walk ( <nl> skipped = 0 ; <nl> break ; <nl> } <nl> + /* we ' re done if id overflows back to zero */ <nl> + if (! next_index ) <nl> + break ; <nl> } <nl>  <nl> if ( skipped ) {
EXPORT_SYMBOL ( ipmi_get_smi_info ); <nl> static void free_user ( struct kref * ref ) <nl> { <nl> struct ipmi_user * user = container_of ( ref , struct ipmi_user , refcount ); <nl> + cleanup_srcu_struct (& user -> release_barrier ); <nl> kfree ( user ); <nl> } <nl>  <nl> int ipmi_destroy_user ( struct ipmi_user * user ) <nl> { <nl> _ipmi_destroy_user ( user ); <nl>  <nl> - cleanup_srcu_struct (& user -> release_barrier ); <nl> kref_put (& user -> refcount , free_user ); <nl>  <nl> return 0 ;
int iwlagn_mac_setup_register ( struct iwl_priv * priv , <nl> ARRAY_SIZE ( iwlagn_iface_combinations_dualmode ); <nl> } <nl>  <nl> - hw -> wiphy -> max_remain_on_channel_duration = 1000 ; <nl> + hw -> wiphy -> max_remain_on_channel_duration = 500 ; <nl>  <nl> hw -> wiphy -> flags |= WIPHY_FLAG_CUSTOM_REGULATORY | <nl> WIPHY_FLAG_DISABLE_BEACON_HINTS |
static bool is_fullscreen ( struct drm_crtc_state * cstate , <nl> (( pstate -> crtc_y + pstate -> crtc_h ) >= cstate -> mode . vdisplay ); <nl> } <nl>  <nl> - enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> + static enum mdp_mixer_stage_id get_start_stage ( struct drm_crtc * crtc , <nl> struct drm_crtc_state * new_crtc_state , <nl> struct drm_plane_state * bpstate ) <nl> {
static struct task_struct * select_bad_process ( unsigned long * ppoints ) <nl> unsigned long points ; <nl> int releasing ; <nl>  <nl> + /* skip kernel threads */ <nl> + if (! p -> mm ) <nl> + continue ; <nl> /* skip the init task with pid == 1 */ <nl> if ( p -> pid == 1 ) <nl> continue ;
static int acm_probe ( struct usb_interface * intf , <nl> if ( quirks == NO_UNION_NORMAL ) { <nl> data_interface = usb_ifnum_to_if ( usb_dev , 1 ); <nl> control_interface = usb_ifnum_to_if ( usb_dev , 0 ); <nl> + /* we would crash */ <nl> + if (! data_interface || ! control_interface ) <nl> + return - ENODEV ; <nl> goto skip_normal_probe ; <nl> } <nl> 
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
static int wm8731_probe ( struct snd_soc_codec * codec ) <nl> err_regulator_get : <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8731 -> supplies ), wm8731 -> supplies ); <nl>  <nl> - kfree ( wm8731 ); <nl> return ret ; <nl> } <nl> 
static struct snd_soc_codec_driver soc_codec_device_ak4104 = { <nl> . probe = ak4104_probe , <nl> . remove = ak4104_remove , <nl> . reg_cache_size = AK4104_NUM_REGS , <nl> - . reg_word_size = sizeof ( u16 ), <nl> + . reg_word_size = sizeof ( u8 ), <nl> }; <nl>  <nl> static int ak4104_spi_probe ( struct spi_device * spi )
static unsigned long ext4_get_stripe_size ( struct ext4_sb_info * sbi ) <nl>  <nl> if ( sbi -> s_stripe && sbi -> s_stripe <= sbi -> s_blocks_per_group ) <nl> ret = sbi -> s_stripe ; <nl> - else if ( stripe_width <= sbi -> s_blocks_per_group ) <nl> + else if ( stripe_width && stripe_width <= sbi -> s_blocks_per_group ) <nl> ret = stripe_width ; <nl> - else if ( stride <= sbi -> s_blocks_per_group ) <nl> + else if ( stride && stride <= sbi -> s_blocks_per_group ) <nl> ret = stride ; <nl> else <nl> ret = 0 ;
static SIMPLE_DEV_PM_OPS ( ds1374_pm , ds1374_suspend , ds1374_resume ); <nl> static struct i2c_driver ds1374_driver = { <nl> . driver = { <nl> . name = " rtc - ds1374 ", <nl> + . of_match_table = of_match_ptr ( ds1374_of_match ), <nl> . pm = & ds1374_pm , <nl> }, <nl> . probe = ds1374_probe ,
static int iio_device_add_event ( struct iio_dev * indio_dev , <nl> & indio_dev -> event_interface -> dev_attr_list ); <nl> kfree ( postfix ); <nl>  <nl> + if (( ret == - EBUSY ) && ( shared_by != IIO_SEPARATE )) <nl> + continue ; <nl> + <nl> if ( ret ) <nl> return ret ; <nl> 
static void finish_csr_load ( const struct firmware * fw , void * context ) <nl> } <nl> csr -> mmio_count = dmc_header -> mmio_count ; <nl> for ( i = 0 ; i < dmc_header -> mmio_count ; i ++) { <nl> - if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE && <nl> + if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE || <nl> dmc_header -> mmioaddr [ i ] > CSR_MMIO_END_RANGE ) { <nl> DRM_ERROR (" Firmware has wrong mmio address 0x % x \ n ", <nl> dmc_header -> mmioaddr [ i ]);
static int nvmet_rdma_queue_connect ( struct rdma_cm_id * cm_id , <nl> } <nl> queue -> port = cm_id -> context ; <nl>  <nl> + if ( queue -> host_qid == 0 ) { <nl> + /* Let inflight controller teardown complete */ <nl> + flush_scheduled_work (); <nl> + } <nl> + <nl> ret = nvmet_rdma_cm_accept ( cm_id , queue , & event -> param . conn ); <nl> if ( ret ) <nl> goto release_queue ;
static netdev_tx_t reg_vif_xmit ( struct sk_buff * skb , <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> dev -> stats . tx_bytes += skb -> len ;
struct hisi_clock_data * hisi_clk_init ( struct device_node * np , <nl> goto err ; <nl> } <nl> clk_data -> base = base ; <nl> - <nl> - clk_table = kzalloc ( sizeof ( struct clk *) * nr_clks , GFP_KERNEL ); <nl> + clk_table = kcalloc ( nr_clks , sizeof (* clk_table ), GFP_KERNEL ); <nl> if (! clk_table ) { <nl> pr_err ("% s : could not allocate clock lookup table \ n ", __func__ ); <nl> goto err_data ;
static int usb_console_setup ( struct console * co , char * options ) <nl> tty_kref_put ( tty ); <nl> reset_open_count : <nl> port -> port . count = 0 ; <nl> + info -> port = NULL ; <nl> usb_autopm_put_interface ( serial -> interface ); <nl> error_get_interface : <nl> usb_serial_put ( serial );
static inline struct crypto_kpp * crypto_kpp_reqtfm ( struct kpp_request * req ) <nl> return __crypto_kpp_tfm ( req -> base . tfm ); <nl> } <nl>  <nl> + static inline u32 crypto_kpp_get_flags ( struct crypto_kpp * tfm ) <nl> +{ <nl> + return crypto_tfm_get_flags ( crypto_kpp_tfm ( tfm )); <nl> +} <nl> + <nl> + static inline void crypto_kpp_set_flags ( struct crypto_kpp * tfm , u32 flags ) <nl> +{ <nl> + crypto_tfm_set_flags ( crypto_kpp_tfm ( tfm ), flags ); <nl> +} <nl> + <nl> /** <nl> * crypto_free_kpp () - free KPP tfm handle <nl> *
xfs_growfs_rt ( <nl> /* <nl> * Initial error checking . <nl> */ <nl> - if ( mp -> m_rtdev_targp || mp -> m_rbmip == NULL || <nl> + if ( mp -> m_rtdev_targp == NULL || mp -> m_rbmip == NULL || <nl> ( nrblocks = in -> newblocks ) <= sbp -> sb_rblocks || <nl> ( sbp -> sb_rblocks && ( in -> extsize != sbp -> sb_rextsize ))) <nl> return XFS_ERROR ( EINVAL );
static int ucode_init ( loader_block * lb , amb_dev * dev ) <nl> const struct firmware * fw ; <nl> unsigned long start_address ; <nl> const struct ihex_binrec * rec ; <nl> - const char * errmsg = 0 ; <nl> + const char * errmsg = NULL ; <nl> int res ; <nl>  <nl> res = request_ihex_firmware (& fw , " atmsar11 . fw ", & dev -> pci_dev -> dev );
static int pty_write ( struct tty_struct * tty , const unsigned char * buf , <nl>  <nl> static int pty_write_room ( struct tty_struct * tty ) <nl> { <nl> + if ( tty -> stopped ) <nl> + return 0 ; <nl> return pty_space ( tty -> link ); <nl> } <nl> 
static void ks_wlan_hw_rx ( struct ks_wlan_private * priv , uint16_t size ) <nl> int ret ; <nl> struct rx_device_buffer * rx_buffer ; <nl> struct hostif_hdr * hdr ; <nl> - unsigned short event = 0 ; <nl> + u16 event = 0 ; <nl>  <nl> /* receive data */ <nl> if ( rxq_count ( priv ) >= ( RX_DEVICE_BUFF_SIZE - 1 )) {
static struct kset * ipl_kset ; <nl>  <nl> static void __ipl_run ( void * unused ) <nl> { <nl> + if ( MACHINE_IS_LPAR && ipl_info . type == IPL_TYPE_CCW ) <nl> + diag308 ( DIAG308_LOAD_NORMAL_DUMP , NULL ); <nl> diag308 ( DIAG308_LOAD_CLEAR , NULL ); <nl> if ( MACHINE_IS_VM ) <nl> __cpcmd (" IPL ", NULL , 0 , NULL );
static void after_state_ch ( struct drbd_conf * mdev , union drbd_state os , <nl> drbd_free_bc ( mdev -> ldev ); <nl> mdev -> ldev = NULL ;); <nl>  <nl> - if ( mdev -> md_io_tmpp ) <nl> + if ( mdev -> md_io_tmpp ) { <nl> __free_page ( mdev -> md_io_tmpp ); <nl> + mdev -> md_io_tmpp = NULL ; <nl> + } <nl> } <nl>  <nl> /* Disks got bigger while they were detached */
vbuschannel_itoa ( char * p , int remain , int num ) <nl> } <nl> /* form a backwards decimal ascii string in < s > */ <nl> while ( num > 0 ) { <nl> - if ( digits >= ( int ) sizeof ( s )) <nl> + if ( digits >= ( int ) sizeof ( s )) <nl> return 0 ; <nl> s [ digits ++] = ( num % 10 ) + ' 0 '; <nl> num = num / 10 ;
static struct ext4_new_flex_group_data * alloc_flex_gd ( unsigned long flexbg_size ) <nl> if ( flex_gd == NULL ) <nl> goto out3 ; <nl>  <nl> - if ( flexbg_size >= UINT_MAX / sizeof ( struct ext4_new_flex_group_data )) <nl> + if ( flexbg_size >= UINT_MAX / sizeof ( struct ext4_new_group_data )) <nl> goto out2 ; <nl> flex_gd -> count = flexbg_size ; <nl> 
int ipv6_find_hdr ( const struct sk_buff * skb , unsigned int * offset , <nl> found = ( nexthdr == target ); <nl>  <nl> if ((! ipv6_ext_hdr ( nexthdr )) || nexthdr == NEXTHDR_NONE ) { <nl> - if ( target < 0 ) <nl> + if ( target < 0 || found ) <nl> break ; <nl> return - ENOENT ; <nl> }
static void snd_soc_instantiate_card ( struct snd_soc_card * card ) <nl> snd_soc_dapm_add_routes (& card -> dapm , card -> dapm_routes , <nl> card -> num_dapm_routes ); <nl>  <nl> + snd_soc_dapm_new_widgets (& card -> dapm ); <nl> + <nl> for ( i = 0 ; i < card -> num_links ; i ++) { <nl> dai_link = & card -> dai_link [ i ]; <nl> 
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
int inode_init_always ( struct super_block * sb , struct inode * inode ) <nl> mapping -> a_ops = & empty_aops ; <nl> mapping -> host = inode ; <nl> mapping -> flags = 0 ; <nl> + mapping -> wb_err = 0 ; <nl> atomic_set (& mapping -> i_mmap_writable , 0 ); <nl> mapping_set_gfp_mask ( mapping , GFP_HIGHUSER_MOVABLE ); <nl> mapping -> private_data = NULL ;
static int pit_ioport_read ( struct kvm_io_device * this , <nl> return - EOPNOTSUPP ; <nl>  <nl> addr &= KVM_PIT_CHANNEL_MASK ; <nl> + if ( addr == 3 ) <nl> + return 0 ; <nl> + <nl> s = & pit_state -> channels [ addr ]; <nl>  <nl> mutex_lock (& pit_state -> lock );
static int ieee80211_set_cqm_rssi_config ( struct wiphy * wiphy , <nl>  <nl> bss_conf -> cqm_rssi_thold = rssi_thold ; <nl> bss_conf -> cqm_rssi_hyst = rssi_hyst ; <nl> + sdata -> u . mgd . last_cqm_event_signal = 0 ; <nl>  <nl> /* tell the driver upon association , unless already associated */ <nl> if ( sdata -> u . mgd . associated &&
static int nfs4_stat_to_errno ( int ); <nl> 2 + encode_verifier_maxsz + 5 + \ <nl> nfs4_label_maxsz ) <nl> # define decode_readdir_maxsz ( op_decode_hdr_maxsz + \ <nl> - decode_verifier_maxsz + \ <nl> - nfs4_label_maxsz + nfs4_fattr_maxsz ) <nl> + decode_verifier_maxsz ) <nl> # define encode_readlink_maxsz ( op_encode_hdr_maxsz ) <nl> # define decode_readlink_maxsz ( op_decode_hdr_maxsz + 1 ) <nl> # define encode_write_maxsz ( op_encode_hdr_maxsz + \
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> b43_set_phytxctl_defaults ( dev ); <nl>  <nl> /* Minimum Contention Window */ <nl> - if ( phy -> type == B43_PHYTYPE_B ) { <nl> + if ( phy -> type == B43_PHYTYPE_B ) <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0x1F ); <nl> - } else { <nl> + else <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0xF ); <nl> - } <nl> /* Maximum Contention Window */ <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MAXCONT , 0x3FF ); <nl> 
static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> mutex_lock (& minors_lock ); <nl> dev = hidraw_table [ minor ]; <nl> + if (! dev ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl>  <nl> switch ( cmd ) { <nl> case HIDIOCGRDESCSIZE : <nl> static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> ret = - ENOTTY ; <nl> } <nl> + out : <nl> mutex_unlock (& minors_lock ); <nl> return ret ; <nl> }
static int sdhci_probe ( struct platform_device * pdev ) <nl> host -> hw_name = " sdhci "; <nl> host -> ops = & sdhci_pltfm_ops ; <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq <= 0 ) { <nl> + ret = - EINVAL ; <nl> + goto err_host ; <nl> + } <nl> host -> quirks = SDHCI_QUIRK_BROKEN_ADMA ; <nl>  <nl> sdhci = sdhci_priv ( host );
long kvm_vm_ioctl_create_spapr_tce ( struct kvm * kvm , <nl> int ret = - ENOMEM ; <nl> int i ; <nl>  <nl> - if (! args -> size ) <nl> + if (! args -> size || args -> page_shift < 12 || args -> page_shift > 34 || <nl> + ( args -> offset + args -> size > ( ULLONG_MAX >> args -> page_shift ))) <nl> return - EINVAL ; <nl>  <nl> size = _ALIGN_UP ( args -> size , PAGE_SIZE >> 3 );
void ieee80211_sta_rx_notify ( struct ieee80211_sub_if_data * sdata , <nl> if ( is_multicast_ether_addr ( hdr -> addr1 )) <nl> return ; <nl>  <nl> + /* <nl> + * In case we receive frames after disassociation . <nl> + */ <nl> + if (! sdata -> u . mgd . associated ) <nl> + return ; <nl> + <nl> ieee80211_sta_reset_conn_monitor ( sdata ); <nl> } <nl> 
struct rchan * relay_open ( const char * base_filename , <nl>  <nl> kref_put (& chan -> kref , relay_destroy_channel ); <nl> mutex_unlock (& relay_channels_mutex ); <nl> + kfree ( chan ); <nl> return NULL ; <nl> } <nl> EXPORT_SYMBOL_GPL ( relay_open );
static int aic3x_set_power ( struct snd_soc_codec * codec , int power ) <nl>  <nl> /* Sync reg_cache with the hardware */ <nl> codec -> cache_only = 0 ; <nl> - for ( i = 0 ; i < ARRAY_SIZE ( aic3x_reg ); i ++) <nl> + for ( i = AIC3X_SAMPLE_RATE_SEL_REG ; i < ARRAY_SIZE ( aic3x_reg ); i ++) <nl> snd_soc_write ( codec , i , cache [ i ]); <nl> if ( aic3x -> model == AIC3X_MODEL_3007 ) <nl> aic3x_init_3007 ( codec );
int snd_ctl_add ( struct snd_card * card , struct snd_kcontrol * kcontrol ) <nl> if ( snd_BUG_ON (! card || ! kcontrol -> info )) <nl> goto error ; <nl> id = kcontrol -> id ; <nl> + if ( id . index > UINT_MAX - kcontrol -> count ) <nl> + goto error ; <nl> + <nl> down_write (& card -> controls_rwsem ); <nl> if ( snd_ctl_find_id ( card , & id )) { <nl> up_write (& card -> controls_rwsem );
static void __ibmvnic_reset ( struct work_struct * work ) <nl>  <nl> if ( rc ) { <nl> free_all_rwi ( adapter ); <nl> + mutex_unlock (& adapter -> reset_lock ); <nl> return ; <nl> } <nl> 
int bdi_register ( struct backing_dev_info * bdi , struct device * parent , <nl> int ret = 0 ; <nl> struct device * dev ; <nl>  <nl> + if ( WARN_ON ( bdi -> dev )) <nl> + goto exit ; <nl> + <nl> va_start ( args , fmt ); <nl> dev = device_create_vargs ( bdi_class , parent , MKDEV ( 0 , 0 ), bdi , fmt , args ); <nl> va_end ( args );
void stmmac_remove_config_dt ( struct platform_device * pdev , <nl> if ( of_phy_is_fixed_link ( np )) <nl> of_phy_deregister_fixed_link ( np ); <nl> of_node_put ( plat -> phy_node ); <nl> + of_node_put ( plat -> mdio_node ); <nl> } <nl> # else <nl> struct plat_stmmacenet_data *
static inline int _loop ( unsigned dry_run , u8 buf [], <nl> unsigned lcnt0 , lcnt1 , ljmp0 , ljmp1 ; <nl> struct _arg_LPEND lpend ; <nl>  <nl> + if (* bursts == 1 ) <nl> + return _bursts ( dry_run , buf , pxs , 1 ); <nl> + <nl> /* Max iterations possible in DMALP is 256 */ <nl> if (* bursts >= 256 * 256 ) { <nl> lcnt1 = 256 ;
struct dev_pm_domain omap_device_pm_domain = { <nl> USE_PLATFORM_PM_SLEEP_OPS <nl> . suspend_noirq = _od_suspend_noirq , <nl> . resume_noirq = _od_resume_noirq , <nl> + . freeze_noirq = _od_suspend_noirq , <nl> + . thaw_noirq = _od_resume_noirq , <nl> + . restore_noirq = _od_resume_noirq , <nl> } <nl> }; <nl> 
static const struct ich_laptop ich_laptop [] = { <nl> { 0x27DF , 0x0005 , 0x0280 }, /* ICH7 on Acer 5602WLMi */ <nl> { 0x27DF , 0x1025 , 0x0110 }, /* ICH7 on Acer 3682WLMi */ <nl> { 0x27DF , 0x1043 , 0x1267 }, /* ICH7 on Asus W5F */ <nl> + { 0x24CA , 0x1025 , 0x0061 }, /* ICH4 on ACER Aspire 2023WLMi */ <nl> /* end marker */ <nl> { 0 , } <nl> };
static void tg3_phy_toggle_apd ( struct tg3 * tp , bool enable ) <nl> { <nl> u32 reg ; <nl>  <nl> - if (!( tp -> tg3_flags2 & TG3_FLG2_5705_PLUS )) <nl> + if (!( tp -> tg3_flags2 & TG3_FLG2_5705_PLUS ) || <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5906 ) <nl> return ; <nl>  <nl> reg = MII_TG3_MISC_SHDW_WREN |
static bool malidp_check_pages_threshold ( struct malidp_plane_state * ms , <nl> else <nl> sgt = obj -> funcs -> get_sg_table ( obj ); <nl>  <nl> - if (! sgt ) <nl> + if ( IS_ERR ( sgt )) <nl> return false ; <nl>  <nl> sgl = sgt -> sgl ;
static int mon_bin_mmap ( struct file * filp , struct vm_area_struct * vma ) <nl> { <nl> /* don ' t do anything here : " fault " will set up page table entries */ <nl> vma -> vm_ops = & mon_bin_vm_ops ; <nl> + <nl> + if ( vma -> vm_flags & VM_WRITE ) <nl> + return - EPERM ; <nl> + <nl> + vma -> vm_flags &= ~ VM_MAYWRITE ; <nl> vma -> vm_flags |= VM_DONTEXPAND | VM_DONTDUMP ; <nl> vma -> vm_private_data = filp -> private_data ; <nl> mon_bin_vma_open ( vma );
static void cx_auto_check_auto_mic ( struct hda_codec * codec ) <nl> int pset [ INPUT_PIN_ATTR_NORMAL + 1 ]; <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < INPUT_PIN_ATTR_NORMAL ; i ++) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( pset ); i ++) <nl> pset [ i ] = - 1 ; <nl> for ( i = 0 ; i < spec -> private_imux . num_items ; i ++) { <nl> hda_nid_t pin = spec -> imux_info [ i ]. pin ;
static int __ext4_ext_check ( const char * function , unsigned int line , <nl> error_msg = " invalid extent entries "; <nl> goto corrupted ; <nl> } <nl> + if ( unlikely ( depth > 32 )) { <nl> + error_msg = " too large eh_depth "; <nl> + goto corrupted ; <nl> + } <nl> /* Verify checksum on non - root extent tree nodes */ <nl> if ( ext_depth ( inode ) != depth && <nl> ! ext4_extent_block_csum_verify ( inode , eh )) {
int ext4_expand_extra_isize_ea ( struct inode * inode , int new_extra_isize , <nl> goto cleanup ; <nl> kfree ( b_entry_name ); <nl> kfree ( buffer ); <nl> + b_entry_name = NULL ; <nl> + buffer = NULL ; <nl> brelse ( is -> iloc . bh ); <nl> kfree ( is ); <nl> kfree ( bs );
# ifndef HPSB_DEBUG_TLABELS <nl> static <nl> # endif <nl> - spinlock_t hpsb_tlabel_lock = SPIN_LOCK_UNLOCKED ; <nl> + DEFINE_SPINLOCK ( hpsb_tlabel_lock ); <nl>  <nl> static DECLARE_WAIT_QUEUE_HEAD ( tlabel_wq ); <nl> 
int imx_drm_add_crtc ( struct drm_crtc * crtc , <nl>  <nl> mutex_lock (& imxdrm -> mutex ); <nl>  <nl> + /* <nl> + * The vblank arrays are dimensioned by MAX_CRTC - we can ' t <nl> + * pass IDs greater than this to those functions . <nl> + */ <nl> + if ( imxdrm -> pipes >= MAX_CRTC ) { <nl> + ret = - EINVAL ; <nl> + goto err_busy ; <nl> + } <nl> + <nl> if ( imxdrm -> drm -> open_count ) { <nl> ret = - EBUSY ; <nl> goto err_busy ;
static int parse_mount_options ( struct ceph_mount_options ** pfsopt , <nl>  <nl> fsopt -> rsize = CEPH_MOUNT_RSIZE_DEFAULT ; <nl> fsopt -> snapdir_name = kstrdup ( CEPH_SNAPDIRNAME_DEFAULT , GFP_KERNEL ); <nl> + fsopt -> caps_wanted_delay_min = CEPH_CAPS_WANTED_DELAY_MIN_DEFAULT ; <nl> + fsopt -> caps_wanted_delay_max = CEPH_CAPS_WANTED_DELAY_MAX_DEFAULT ; <nl> fsopt -> cap_release_safety = CEPH_CAP_RELEASE_SAFETY_DEFAULT ; <nl> fsopt -> max_readdir = CEPH_MAX_READDIR_DEFAULT ; <nl> fsopt -> max_readdir_bytes = CEPH_MAX_READDIR_BYTES_DEFAULT ;
asmlinkage long sys_rt_sigreturn_wrapper ( void ); <nl> * The sys_call_table array must be 4K aligned to be accessible from <nl> * kernel / entry . S . <nl> */ <nl> - void * sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> + void * const sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> [ 0 ... __NR_syscalls - 1 ] = sys_ni_syscall , <nl> # include < asm / unistd . h > <nl> };
static int rtl8xxxu_submit_int_urb ( struct ieee80211_hw * hw ) <nl> ret = usb_submit_urb ( urb , GFP_KERNEL ); <nl> if ( ret ) { <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> goto error ; <nl> } <nl> 
long dgnc_mgmt_ioctl ( struct file * file , unsigned int cmd , unsigned long arg ) <nl>  <nl> spin_lock_irqsave (& dgnc_global_lock , flags ); <nl>  <nl> + memset (& ddi , 0 , sizeof ( ddi )); <nl> ddi . dinfo_nboards = dgnc_NumBoards ; <nl> sprintf ( ddi . dinfo_version , "% s ", DG_PART ); <nl> 
static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> cnt += tcp_skb_pcount ( skb ); <nl>  <nl> if ( cnt > packets ) { <nl> - if ( tcp_is_sack ( tp ) || ( oldcnt >= packets )) <nl> + if (( tcp_is_sack ( tp ) && ! tcp_is_fack ( tp )) || <nl> + ( oldcnt >= packets )) <nl> break ; <nl>  <nl> mss = skb_shinfo ( skb )-> gso_size ;
static int __devinit snd_interwave_pnp ( int dev , struct snd_interwave * iwcard , <nl> struct pnp_resource_table * cfg = kmalloc ( sizeof ( struct pnp_resource_table ), GFP_KERNEL ); <nl> int err ; <nl>  <nl> + if (! cfg ) <nl> + return - ENOMEM ; <nl> iwcard -> dev = pnp_request_card_device ( card , id -> devs [ 0 ]. id , NULL ); <nl> if ( iwcard -> dev == NULL ) { <nl> kfree ( cfg );
static int init_volumes ( struct ubi_device * ubi , const struct ubi_scan_info * si , <nl> if ( ubi -> autoresize_vol_id != - 1 ) { <nl> ubi_err (" more then one auto - resize volume (% d " <nl> " and % d )", ubi -> autoresize_vol_id , i ); <nl> + kfree ( vol ); <nl> return - EINVAL ; <nl> } <nl> 
static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
static int rt6_score_route ( struct rt6_info * rt , int oif , <nl> m |= IPV6_DECODE_PREF ( IPV6_EXTRACT_PREF ( rt -> rt6i_flags )) << 2 ; <nl> # endif <nl> n = rt6_check_neigh ( rt ); <nl> - if ( n > 1 ) <nl> - m |= 16 ; <nl> - else if (! n && strict & RT6_LOOKUP_F_REACHABLE ) <nl> + if (! n && ( strict & RT6_LOOKUP_F_REACHABLE )) <nl> return - 1 ; <nl> return m ; <nl> }
static void zynqmp_dma_chan_remove ( struct zynqmp_dma_chan * chan ) <nl> if (! chan ) <nl> return ; <nl>  <nl> - devm_free_irq ( chan -> zdev -> dev , chan -> irq , chan ); <nl> + if ( chan -> irq ) <nl> + devm_free_irq ( chan -> zdev -> dev , chan -> irq , chan ); <nl> tasklet_kill (& chan -> tasklet ); <nl> list_del (& chan -> common . device_node ); <nl> }
static int btrfs_extent_same ( struct inode * src , u64 loff , u64 olen , <nl> inode_lock ( src ); <nl>  <nl> ret = extent_same_check_offsets ( src , loff , & len , olen ); <nl> + if ( ret ) <nl> + goto out_unlock ; <nl> + ret = extent_same_check_offsets ( src , dst_loff , & len , olen ); <nl> if ( ret ) <nl> goto out_unlock ; <nl> 
static int rpmsg_dev_probe ( struct device * dev ) <nl> goto out ; <nl> } <nl>  <nl> - if ( rpdev -> ops -> announce_create ) <nl> + if ( ept && rpdev -> ops -> announce_create ) <nl> err = rpdev -> ops -> announce_create ( rpdev ); <nl> out : <nl> return err ;
static int e1000_set_ringparam ( struct net_device * netdev , <nl> err_alloc_rx : <nl> kfree ( txdr ); <nl> err_alloc_tx : <nl> - e1000_up ( adapter ); <nl> + if ( netif_running ( adapter -> netdev )) <nl> + e1000_up ( adapter ); <nl> err_setup : <nl> clear_bit ( __E1000_RESETTING , & adapter -> flags ); <nl> return err ;
static int __ipmi_bmc_register ( struct ipmi_smi * intf , <nl> bmc -> pdev . name = " ipmi_bmc "; <nl>  <nl> rv = ida_simple_get (& ipmi_bmc_ida , 0 , 0 , GFP_KERNEL ); <nl> - if ( rv < 0 ) <nl> + if ( rv < 0 ) { <nl> + kfree ( bmc ); <nl> goto out ; <nl> + } <nl> + <nl> bmc -> pdev . dev . driver = & ipmidriver . driver ; <nl> bmc -> pdev . id = rv ; <nl> bmc -> pdev . dev . release = release_bmc_device ;
static int lpc32xx_adc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if (( irq < 0 ) || ( irq >= NR_IRQS )) { <nl> + if ( irq <= 0 ) { <nl> dev_err (& pdev -> dev , " failed getting interrupt resource \ n "); <nl> return - EINVAL ; <nl> }
static long aac_compat_do_ioctl ( struct aac_dev * dev , unsigned cmd , unsigned long <nl> static int aac_compat_ioctl ( struct scsi_device * sdev , int cmd , void __user * arg ) <nl> { <nl> struct aac_dev * dev = ( struct aac_dev *) sdev -> host -> hostdata ; <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> return aac_compat_do_ioctl ( dev , cmd , ( unsigned long ) arg ); <nl> } <nl> 
static int i40e_set_channels ( struct net_device * dev , <nl> * class queue mapping <nl> */ <nl> new_count = i40e_reconfig_rss_queues ( pf , count ); <nl> - if ( new_count > 1 ) <nl> + if ( new_count > 0 ) <nl> return 0 ; <nl> else <nl> return - EINVAL ;
static void exynos5_powerdown_conf ( enum sys_powerdown mode ) <nl> void exynos_sys_powerdown_conf ( enum sys_powerdown mode ) <nl> { <nl> unsigned int i ; <nl> + const struct exynos_pmu_data * pmu_data ; <nl> + <nl> + if (! pmu_context ) <nl> + return ; <nl>  <nl> - const struct exynos_pmu_data * pmu_data = pmu_context -> pmu_data ; <nl> + pmu_data = pmu_context -> pmu_data ; <nl>  <nl> if ( pmu_data -> powerdown_conf ) <nl> pmu_data -> powerdown_conf ( mode );
static isolate_migrate_t isolate_migratepages ( struct zone * zone , <nl> low_pfn = isolate_migratepages_block ( cc , low_pfn , end_pfn , <nl> isolate_mode ); <nl>  <nl> - if (! low_pfn || cc -> contended ) <nl> + if (! low_pfn || cc -> contended ) { <nl> + acct_isolated ( zone , cc ); <nl> return ISOLATE_ABORT ; <nl> + } <nl>  <nl> /* <nl> * Either we isolated something and proceed with migration . Or
void DisableVGA ( volatile STG4000REG __iomem * pSTGReg ) <nl> { <nl> u32 tmp ; <nl> - volatile u32 count , i ; <nl> + volatile u32 count = 0 , i ; <nl>  <nl> /* Reset the VGA registers */ <nl> tmp = STG_READ_REG ( SoftwareReset );
static int __posix_lock_file ( struct inode * inode , struct file_lock * request , str <nl> } <nl> locks_copy_lock ( new_fl , request ); <nl> locks_insert_lock_ctx ( new_fl , & fl -> fl_list ); <nl> + fl = new_fl ; <nl> new_fl = NULL ; <nl> } <nl> if ( right ) {
long ext4_fallocate ( struct file * file , int mode , loff_t offset , loff_t len ) <nl> blkbits ) >> blkbits )) <nl> new_size = offset + len ; <nl> else <nl> - new_size = ( map . m_lblk + ret ) << blkbits ; <nl> + new_size = (( loff_t ) map . m_lblk + ret ) << blkbits ; <nl>  <nl> ext4_falloc_update_inode ( inode , mode , new_size , <nl> ( map . m_flags & EXT4_MAP_NEW ));
static struct sg_table * omap_gem_map_dma_buf ( <nl> /* this should be after _get_paddr () to ensure we have pages attached */ <nl> omap_gem_dma_sync ( obj , dir ); <nl>  <nl> - out : <nl> - if ( ret ) <nl> - return ERR_PTR ( ret ); <nl> return sg ; <nl> + out : <nl> + kfree ( sg ); <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> static void omap_gem_unmap_dma_buf ( struct dma_buf_attachment * attachment ,
static void setup_mailboxes ( int base_io , struct Scsi_Host * shpnt ); <nl> static int aha1542_restart ( struct Scsi_Host * shost ); <nl> static void aha1542_intr_handle ( struct Scsi_Host * shost ); <nl>  <nl> -# define aha1542_intr_reset ( base ) outb ( IRST , CONTROL ( base )) <nl> + static inline void aha1542_intr_reset ( u16 base ) <nl> +{ <nl> + outb ( IRST , CONTROL ( base )); <nl> +} <nl>  <nl> # define WAIT ( port , mask , allof , noneof ) \ <nl> { register int WAITbits ; \
char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> * string to the length of the original string to allow for worst case . <nl> */ <nl> md_len = strlen ( sb_mountdata ) + INET6_ADDRSTRLEN ; <nl> - mountdata = kzalloc ( md_len + 1 , GFP_KERNEL ); <nl> + mountdata = kzalloc ( md_len + sizeof (" ip =") + 1 , GFP_KERNEL ); <nl> if ( mountdata == NULL ) { <nl> rc = - ENOMEM ; <nl> goto compose_mount_options_err ;
void ath_init_leds ( struct ath_softc * sc ) <nl> { <nl> int ret ; <nl>  <nl> + if ( AR_SREV_9100 ( sc -> sc_ah )) <nl> + return ; <nl> + <nl> if ( sc -> sc_ah -> led_pin < 0 ) { <nl> if ( AR_SREV_9287 ( sc -> sc_ah )) <nl> sc -> sc_ah -> led_pin = ATH_LED_PIN_9287 ;
static int ocores_i2c_of_probe ( struct platform_device * pdev , <nl> & i2c -> reg_io_width ); <nl>  <nl> match = of_match_node ( ocores_i2c_match , pdev -> dev . of_node ); <nl> - if ( match && ( int ) match -> data == TYPE_GRLIB ) { <nl> + if ( match && ( long ) match -> data == TYPE_GRLIB ) { <nl> dev_dbg (& pdev -> dev , " GRLIB variant of i2c - ocores \ n "); <nl> i2c -> setreg = oc_setreg_grlib ; <nl> i2c -> getreg = oc_getreg_grlib ;
static void ohci_stop ( struct usb_hcd * hcd ) <nl>  <nl> ohci_usb_reset ( ohci ); <nl> ohci_writel ( ohci , OHCI_INTR_MIE , & ohci -> regs -> intrdisable ); <nl> - <nl> + free_irq ( hcd -> irq , hcd ); <nl> + hcd -> irq = - 1 ; <nl> + <nl> remove_debug_files ( ohci ); <nl> ohci_mem_cleanup ( ohci ); <nl> if ( ohci -> hcca ) {
int udp_ioctl ( struct sock * sk , int cmd , unsigned long arg ) <nl> { <nl> unsigned int amount = first_packet_length ( sk ); <nl>  <nl> - if ( amount ) <nl> - /* <nl> - * We will only return the amount <nl> - * of this packet since that is all <nl> - * that will be read . <nl> - */ <nl> return put_user ( amount , ( int __user *) arg ); <nl> } <nl> 
static int nand_default_block_markbad ( struct mtd_info * mtd , loff_t ofs ) <nl> int block , ret ; <nl>  <nl> /* Get block number */ <nl> - block = (( int ) ofs ) >> chip -> bbt_erase_shift ; <nl> + block = ( int )( ofs >> chip -> bbt_erase_shift ); <nl> if ( chip -> bbt ) <nl> chip -> bbt [ block >> 2 ] |= 0x01 << (( block & 0x03 ) << 1 ); <nl> 
int __kprobes arch_prepare_kprobe ( struct kprobe * p ) <nl> if (! ret ) { <nl> memcpy ( p -> ainsn . insn , p -> addr , MAX_INSN_SIZE * sizeof ( kprobe_opcode_t )); <nl> p -> opcode = * p -> addr ; <nl> + flush_icache_range (( unsigned long ) p -> ainsn . insn , <nl> + ( unsigned long ) p -> ainsn . insn + sizeof ( kprobe_opcode_t )); <nl> } <nl>  <nl> return ret ;
static int meson_sar_adc_lock ( struct iio_dev * indio_dev ) <nl> regmap_read ( priv -> regmap , MESON_SAR_ADC_DELAY , & val ); <nl> } while ( val & MESON_SAR_ADC_DELAY_BL30_BUSY && timeout --); <nl>  <nl> - if ( timeout < 0 ) <nl> + if ( timeout < 0 ) { <nl> + mutex_unlock (& indio_dev -> mlock ); <nl> return - ETIMEDOUT ; <nl> + } <nl> } <nl>  <nl> return 0 ;
int install_user_keyrings ( void ) <nl>  <nl> kenter ("% p {% u }", user , uid ); <nl>  <nl> - if ( user -> uid_keyring ) { <nl> + if ( user -> uid_keyring && user -> session_keyring ) { <nl> kleave (" = 0 [ exist ]"); <nl> return 0 ; <nl> }
static void tcp_mtu_probing ( struct inet_connection_sock * icsk , struct sock * sk ) <nl> struct tcp_sock * tp = tcp_sk ( sk ); <nl> int mss ; <nl>  <nl> - mss = tcp_mtu_to_mss ( sk , icsk -> icsk_mtup . search_low )/ 2 ; <nl> + mss = tcp_mtu_to_mss ( sk , icsk -> icsk_mtup . search_low ) >> 1 ; <nl> mss = min ( sysctl_tcp_base_mss , mss ); <nl> mss = max ( mss , 68 - tp -> tcp_header_len ); <nl> icsk -> icsk_mtup . search_low = tcp_mss_to_mtu ( sk , mss );
static int ems_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> goto err ; <nl> } <nl>  <nl> - emsff_init ( hdev ); <nl> + ret = emsff_init ( hdev ); <nl> + if ( ret ) { <nl> + dev_err (& hdev -> dev , " force feedback init failed \ n "); <nl> + hid_hw_stop ( hdev ); <nl> + goto err ; <nl> + } <nl>  <nl> return 0 ; <nl> err :
isofs_export_encode_fh ( struct inode * inode , <nl> len = 3 ; <nl> fh32 [ 0 ] = ei -> i_iget5_block ; <nl> fh16 [ 2 ] = ( __u16 ) ei -> i_iget5_offset ; /* fh16 [ sic ] */ <nl> + fh16 [ 3 ] = 0 ; /* avoid leaking uninitialized data */ <nl> fh32 [ 2 ] = inode -> i_generation ; <nl> if ( parent ) { <nl> struct iso_inode_info * eparent ;
int ext4_expand_extra_isize_ea ( struct inode * inode , int new_extra_isize , <nl> s_min_extra_isize ) { <nl> tried_min_extra_isize ++; <nl> new_extra_isize = s_min_extra_isize ; <nl> + kfree ( is ); is = NULL ; <nl> + kfree ( bs ); bs = NULL ; <nl> goto retry ; <nl> } <nl> error = - 1 ;
static int mwifiex_pcie_init_evt_ring ( struct mwifiex_adapter * adapter ) <nl> skb_put ( skb , MAX_EVENT_SIZE ); <nl>  <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MAX_EVENT_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> + kfree ( card -> evtbd_ring_vbase ); <nl> return - 1 ; <nl> + } <nl>  <nl> buf_pa = MWIFIEX_SKB_DMA_ADDR ( skb ); <nl> 
static bool lm3533_readable_register ( struct device * dev , unsigned int reg ) <nl> static bool lm3533_volatile_register ( struct device * dev , unsigned int reg ) <nl> { <nl> switch ( reg ) { <nl> - case 0x34 : /* zone */ <nl> + case 0x34 ... 0x36 : /* zone */ <nl> case 0x37 ... 0x38 : /* adc */ <nl> case 0xb0 ... 0xb1 : /* fault */ <nl> return true ;
static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl> } <nl>  <nl> err = percpu_ref_init (& css -> refcnt , css_release ); <nl> - if ( err ) <nl> + if ( err ) { <nl> + ss -> css_free ( cgrp ); <nl> goto err_free_all ; <nl> + } <nl>  <nl> init_cgroup_css ( css , ss , cgrp ); <nl> 
static int tun_set_iff ( struct net * net , struct file * file , struct ifreq * ifr ) <nl>  <nl> err_detach : <nl> tun_detach_all ( dev ); <nl> + /* register_netdevice () already called tun_free_netdev () */ <nl> + goto err_free_dev ; <nl> + <nl> err_free_flow : <nl> tun_flow_uninit ( tun ); <nl> security_tun_dev_free_security ( tun -> security );
static int mb86s70_gpio_request ( struct gpio_chip * gc , unsigned gpio ) <nl> spin_lock_irqsave (& gchip -> lock , flags ); <nl>  <nl> val = readl ( gchip -> base + PFR ( gpio )); <nl> + if (!( val & OFFSET ( gpio ))) { <nl> + spin_unlock_irqrestore (& gchip -> lock , flags ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> val &= ~ OFFSET ( gpio ); <nl> writel ( val , gchip -> base + PFR ( gpio )); <nl> 
int extent_range_uptodate ( struct extent_io_tree * tree , <nl> while ( start <= end ) { <nl> index = start >> PAGE_CACHE_SHIFT ; <nl> page = find_get_page ( tree -> mapping , index ); <nl> + if (! page ) <nl> + return 1 ; <nl> uptodate = PageUptodate ( page ); <nl> page_cache_release ( page ); <nl> if (! uptodate ) {
void * __raw_xsave_addr ( struct xregs_state * xsave , int xstate_feature_mask ) <nl> { <nl> int feature_nr = fls64 ( xstate_feature_mask ) - 1 ; <nl>  <nl> + if (! xfeature_enabled ( feature_nr )) { <nl> + WARN_ON_FPU ( 1 ); <nl> + return NULL ; <nl> + } <nl> + <nl> return ( void *) xsave + xstate_comp_offsets [ feature_nr ]; <nl> } <nl> /*
static int valid_kprobe_addr ( int template , int slot , unsigned long addr ) <nl> addr ); <nl> return - EINVAL ; <nl> } <nl> + <nl> + if ( slot == 1 && bundle_encoding [ template ][ 1 ] != L ) { <nl> + printk ( KERN_WARNING " Inserting kprobes on slot # 1 " <nl> + " is not supported \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static long __validate_layout ( struct ceph_mds_client * mdsc , <nl> /* validate striping parameters */ <nl> if (( l -> object_size & ~ PAGE_MASK ) || <nl> ( l -> stripe_unit & ~ PAGE_MASK ) || <nl> - (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit )) <nl> + ( l -> stripe_unit != 0 && <nl> + (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit ))) <nl> return - EINVAL ; <nl>  <nl> /* make sure it ' s a valid data pool */
xmaddr_t arbitrary_virt_to_machine ( void * vaddr ) <nl> } <nl> EXPORT_SYMBOL_GPL ( arbitrary_virt_to_machine ); <nl>  <nl> - void xen_flush_tlb_all ( void ) <nl> + static void xen_flush_tlb_all ( void ) <nl> { <nl> struct mmuext_op * op ; <nl> struct multicall_space mcs ;
int ubi_wl_get_peb ( struct ubi_device * ubi ) <nl> peb = __wl_get_peb ( ubi ); <nl> spin_unlock (& ubi -> wl_lock ); <nl>  <nl> + if ( peb < 0 ) <nl> + return peb ; <nl> + <nl> err = ubi_self_check_all_ff ( ubi , peb , ubi -> vid_hdr_aloffset , <nl> ubi -> peb_size - ubi -> vid_hdr_aloffset ); <nl> if ( err ) {
int tusb6010_platform_retime ( unsigned is_refclk ) <nl> unsigned sysclk_ps ; <nl> int status ; <nl>  <nl> - if (! refclk_psec || sysclk_ps == 0 ) <nl> + if (! refclk_psec || fclk_ps == 0 ) <nl> return - ENODEV ; <nl>  <nl> sysclk_ps = is_refclk ? refclk_psec : TUSB6010_OSCCLK_60 ;
int iwl_mvm_rx_card_state_notif ( struct iwl_mvm * mvm , <nl> ( flags & CT_KILL_CARD_DISABLED ) ? <nl> " Reached " : " Not reached "); <nl>  <nl> - if ( flags & CARD_DISABLED_MSK ) <nl> - iwl_write32 ( mvm -> trans , CSR_UCODE_DRV_GP1_SET , <nl> - CSR_UCODE_DRV_GP1_BIT_CMD_BLOCKED ); <nl> - <nl> return 0 ; <nl> } <nl> 
static int ath10k_usb_hif_tx_sg ( struct ath10k * ar , u8 pipe_id , <nl> ath10k_dbg ( ar , ATH10K_DBG_USB_BULK , <nl> " usb bulk transmit failed : % d \ n ", ret ); <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> ret = - EINVAL ; <nl> goto err_free_urb_to_pipe ; <nl> }
static int __devinit ks8851_probe ( struct spi_device * spi ) <nl>  <nl>  <nl> err_netdev : <nl> - free_irq ( ndev -> irq , ndev ); <nl> + free_irq ( ndev -> irq , ks ); <nl>  <nl> err_id : <nl> err_irq :
ssize_t tomoyo_write_control ( struct tomoyo_io_buffer * head , <nl> return - EFAULT ; <nl> if ( mutex_lock_interruptible (& head -> io_sem )) <nl> return - EINTR ; <nl> + head -> read_user_buf_avail = 0 ; <nl> idx = tomoyo_read_lock (); <nl> /* Read a line and dispatch it to the policy handler . */ <nl> while ( avail_len > 0 ) {
static void f2fs_submit_discard_endio ( struct bio * bio ) <nl>  <nl> dc -> error = bio -> bi_error ; <nl> dc -> state = D_DONE ; <nl> - complete (& dc -> wait ); <nl> + complete_all (& dc -> wait ); <nl> bio_put ( bio ); <nl> } <nl> 
int wusbhc_chid_set ( struct wusbhc * wusbhc , const struct wusb_ckhdid * chid ) <nl> { <nl> int result = 0 ; <nl>  <nl> - if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof ( chid )) == 0 ) <nl> + if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof (* chid )) == 0 ) <nl> chid = NULL ; <nl>  <nl> mutex_lock (& wusbhc -> mutex );
static int annotate_browser__run ( struct annotate_browser * self , int evidx , <nl> timer , arg , delay_secs ); <nl> } <nl> break ; <nl> + case NEWT_KEY_LEFT : <nl> + case NEWT_KEY_ESCAPE : <nl> case ' q ': <nl> case CTRL (' c '): <nl> goto out ;
struct ring_buffer * __ring_buffer_alloc ( unsigned long size , unsigned flags , <nl> buffer -> reader_lock_key = key ; <nl>  <nl> /* need at least two pages */ <nl> - if ( buffer -> pages == 1 ) <nl> - buffer -> pages ++; <nl> + if ( buffer -> pages < 2 ) <nl> + buffer -> pages = 2 ; <nl>  <nl> /* <nl> * In case of non - hotplug cpu , if the ring - buffer is allocated
static int reset_one_sub_crq_queue ( struct ibmvnic_adapter * adapter , <nl> scrq -> irq = 0 ; <nl> } <nl>  <nl> - memset ( scrq -> msgs , 0 , 2 * PAGE_SIZE ); <nl> + memset ( scrq -> msgs , 0 , 4 * PAGE_SIZE ); <nl> scrq -> cur = 0 ; <nl>  <nl> rc = h_reg_sub_crq ( adapter -> vdev -> unit_address , scrq -> msg_token ,
static int hdmi_get_edid ( void * ctx , struct drm_connector * connector , <nl> DRM_DEBUG_KMS ("% s : width [% d ] x height [% d ]\ n ", <nl> ( hdata -> dvi_mode ? " dvi monitor " : " hdmi monitor "), <nl> raw_edid -> width_cm , raw_edid -> height_cm ); <nl> + kfree ( raw_edid ); <nl> } else { <nl> return - ENODEV ; <nl> }
static int wait_for_connected ( struct usb_device * udev , <nl> while ( delay_ms < 2000 ) { <nl> if ( status || * portstatus & USB_PORT_STAT_CONNECTION ) <nl> break ; <nl> + if (! port_is_power_on ( hub , * portstatus )) { <nl> + status = - ENODEV ; <nl> + break ; <nl> + } <nl> msleep ( 20 ); <nl> delay_ms += 20 ; <nl> status = hub_port_status ( hub , * port1 , portstatus , portchange );
static void qlcnic_83xx_mailbox_worker ( struct work_struct * work ) <nl> __func__ , cmd -> cmd_op , cmd -> type , ahw -> pci_func , <nl> ahw -> op_mode ); <nl> clear_bit ( QLC_83XX_MBX_READY , & mbx -> status ); <nl> + qlcnic_dump_mbx ( adapter , cmd ); <nl> qlcnic_83xx_idc_request_reset ( adapter , <nl> QLCNIC_FORCE_FW_DUMP_KEY ); <nl> cmd -> rsp_opcode = QLCNIC_RCODE_TIMEOUT ;
static int fcoe_rcv ( struct sk_buff * skb , struct net_device * netdev , <nl> skb_tail_pointer ( skb ), skb_end_pointer ( skb ), <nl> skb -> csum , skb -> dev ? skb -> dev -> name : "< NULL >"); <nl>  <nl> + <nl> + skb = skb_share_check ( skb , GFP_ATOMIC ); <nl> + <nl> + if ( skb == NULL ) <nl> + return NET_RX_DROP ; <nl> + <nl> eh = eth_hdr ( skb ); <nl>  <nl> if ( is_fip_mode ( ctlr ) &&
static void tcp_cwnd_reduction ( struct sock * sk , const int prior_unsacked , <nl> int newly_acked_sacked = prior_unsacked - <nl> ( tp -> packets_out - tp -> sacked_out ); <nl>  <nl> + if ( newly_acked_sacked <= 0 || WARN_ON_ONCE (! tp -> prior_cwnd )) <nl> + return ; <nl> + <nl> tp -> prr_delivered += newly_acked_sacked ; <nl> if ( delta < 0 ) { <nl> u64 dividend = ( u64 ) tp -> snd_ssthresh * tp -> prr_delivered +
void rtl88e_dm_watchdog ( struct ieee80211_hw * hw ) <nl> if ( ppsc -> p2p_ps_info . p2p_ps_mode ) <nl> fw_ps_awake = false ; <nl>  <nl> + spin_lock (& rtlpriv -> locks . rf_ps_lock ); <nl> if (( ppsc -> rfpwr_state == ERFON ) && <nl> ((! fw_current_inpsmode ) && fw_ps_awake ) && <nl> (! ppsc -> rfchange_inprogress )) { <nl> void rtl88e_dm_watchdog ( struct ieee80211_hw * hw ) <nl> rtl88e_dm_check_edca_turbo ( hw ); <nl> rtl88e_dm_antenna_diversity ( hw ); <nl> } <nl> + spin_unlock (& rtlpriv -> locks . rf_ps_lock ); <nl> }
struct kvm_pit * kvm_create_pit ( struct kvm * kvm ) <nl> mutex_lock (& kvm -> lock ); <nl> pit -> irq_source_id = kvm_request_irq_source_id ( kvm ); <nl> mutex_unlock (& kvm -> lock ); <nl> - if ( pit -> irq_source_id < 0 ) <nl> + if ( pit -> irq_source_id < 0 ) { <nl> + kfree ( pit ); <nl> return NULL ; <nl> + } <nl>  <nl> mutex_init (& pit -> pit_state . lock ); <nl> mutex_lock (& pit -> pit_state . lock );
static int l2tp_ip6_recvmsg ( struct kiocb * iocb , struct sock * sk , <nl> lsa -> l2tp_addr = ipv6_hdr ( skb )-> saddr ; <nl> lsa -> l2tp_flowinfo = 0 ; <nl> lsa -> l2tp_scope_id = 0 ; <nl> + lsa -> l2tp_conn_id = 0 ; <nl> if ( ipv6_addr_type (& lsa -> l2tp_addr ) & IPV6_ADDR_LINKLOCAL ) <nl> lsa -> l2tp_scope_id = IP6CB ( skb )-> iif ; <nl> }
static ssize_t state_store ( struct subsystem * subsys , const char * buf , size_t n <nl> if (* s && ! strncmp ( buf , * s , len )) <nl> break ; <nl> } <nl> - if (* s ) <nl> + if ( state < PM_SUSPEND_MAX && * s ) <nl> error = enter_state ( state ); <nl> else <nl> error = - EINVAL ;
static void scan_add_host ( struct soc_camera_host * ici ) <nl>  <nl> list_for_each_entry ( icd , & devices , list ) { <nl> if ( icd -> iface == ici -> nr ) { <nl> - int ret ; <nl> - <nl> icd -> parent = ici -> v4l2_dev . dev ; <nl> - ret = soc_camera_probe ( icd ); <nl> + soc_camera_probe ( icd ); <nl> } <nl> } <nl> 
struct bio * bch_bio_split ( struct bio * bio , int sectors , <nl>  <nl> if ( bio -> bi_rw & REQ_DISCARD ) { <nl> ret = bio_alloc_bioset ( gfp , 1 , bs ); <nl> + if (! ret ) <nl> + return NULL ; <nl> idx = 0 ; <nl> goto out ; <nl> }
int svc_register ( const struct svc_serv * serv , struct net * net , <nl> if ( vers -> vs_hidden ) <nl> continue ; <nl>  <nl> + /* <nl> + * Don ' t register a UDP port if we need congestion <nl> + * control . <nl> + */ <nl> + if ( vers -> vs_need_cong_ctrl && proto == IPPROTO_UDP ) <nl> + continue ; <nl> + <nl> error = __svc_register ( net , progp -> pg_name , progp -> pg_prog , <nl> i , family , proto , port ); <nl> 
static struct mtd_partition davinci_ntosd2_nandflash_partition [] = { <nl> }; <nl>  <nl> static struct davinci_nand_pdata davinci_ntosd2_nandflash_data = { <nl> + . core_chipsel = 0 , <nl> . parts = davinci_ntosd2_nandflash_partition , <nl> . nr_parts = ARRAY_SIZE ( davinci_ntosd2_nandflash_partition ), <nl> . ecc_mode = NAND_ECC_HW ,
verbose_printk (" btrfs : send_create_inode % llu \ n ", ino ); <nl> TLV_PUT_PATH ( sctx , BTRFS_SEND_A_PATH_LINK , p ); <nl> } else if ( S_ISCHR ( mode ) || S_ISBLK ( mode ) || <nl> S_ISFIFO ( mode ) || S_ISSOCK ( mode )) { <nl> - TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , rdev ); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , new_encode_dev ( rdev )); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_MODE , mode ); <nl> } <nl>  <nl> ret = send_cmd ( sctx );
static void <nl> iscsi_tcp_conn_stop ( struct iscsi_cls_conn * cls_conn , int flag ) <nl> { <nl> struct iscsi_conn * conn = cls_conn -> dd_data ; <nl> + struct iscsi_tcp_conn * tcp_conn = conn -> dd_data ; <nl>  <nl> iscsi_conn_stop ( cls_conn , flag ); <nl> iscsi_tcp_release_conn ( conn ); <nl> + tcp_conn -> hdr_size = sizeof ( struct iscsi_hdr ); <nl> } <nl>  <nl> static int
static int __devinit max17042_probe ( struct i2c_client * client , <nl> reg |= CONFIG_ALRT_BIT_ENBL ; <nl> max17042_write_reg ( client , MAX17042_CONFIG , reg ); <nl> max17042_set_soc_threshold ( chip , 1 ); <nl> - } else <nl> + } else { <nl> + client -> irq = 0 ; <nl> dev_err (& client -> dev , "% s (): cannot get IRQ \ n ", <nl> __func__ ); <nl> + } <nl> } <nl>  <nl> reg = max17042_read_reg ( chip -> client , MAX17042_STATUS );
static int ccs811_start_sensor_application ( struct i2c_client * client ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (( ret & CCS811_STATUS_FW_MODE_APPLICATION )) <nl> + return 0 ; <nl> + <nl> if (( ret & CCS811_STATUS_APP_VALID_MASK ) != <nl> CCS811_STATUS_APP_VALID_LOADED ) <nl> return - EIO ;
static int proc_do_submiturb ( struct usb_dev_state * ps , struct usbdevfs_urb * uurb <nl> u = ( is_in ? URB_DIR_IN : URB_DIR_OUT ); <nl> if ( uurb -> flags & USBDEVFS_URB_ISO_ASAP ) <nl> u |= URB_ISO_ASAP ; <nl> - if ( uurb -> flags & USBDEVFS_URB_SHORT_NOT_OK ) <nl> + if ( uurb -> flags & USBDEVFS_URB_SHORT_NOT_OK && is_in ) <nl> u |= URB_SHORT_NOT_OK ; <nl> if ( uurb -> flags & USBDEVFS_URB_NO_FSBR ) <nl> u |= URB_NO_FSBR ;
static int ni_660x_request_mite_channel ( struct comedi_device * dev , <nl> struct mite_channel * mite_chan ; <nl>  <nl> spin_lock_irqsave (& devpriv -> mite_channel_lock , flags ); <nl> - BUG_ON ( counter -> mite_chan ); <nl> mite_chan = mite_request_channel ( devpriv -> mite , <nl> mite_ring ( devpriv , counter )); <nl> if (! mite_chan ) {
static int at_dma_remove ( struct platform_device * pdev ) <nl>  <nl> /* Disable interrupts */ <nl> atc_disable_chan_irq ( atdma , chan -> chan_id ); <nl> - tasklet_disable (& atchan -> tasklet ); <nl>  <nl> tasklet_kill (& atchan -> tasklet ); <nl> list_del (& chan -> device_node );
static int __devinit tg3_get_invariants ( struct tg3 * tp ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5717 && <nl> + ( tp -> pci_chip_rev_id != CHIPREV_ID_5717_A0 || <nl> + ( tp -> tg3_flags2 & TG3_FLG2_MII_SERDES ))) <nl> + return - ENOTSUPP ; <nl> + <nl> /* Initialize data / descriptor byte / word swapping . */ <nl> val = tr32 ( GRC_MODE ); <nl> val &= GRC_MODE_HOST_STACKUP ;
enum batadv_dbg_level { <nl> BATADV_DBG_NC = BIT ( 5 ), <nl> BATADV_DBG_MCAST = BIT ( 6 ), <nl> BATADV_DBG_TP_METER = BIT ( 7 ), <nl> - BATADV_DBG_ALL = 127 , <nl> + BATADV_DBG_ALL = 255 , <nl> }; <nl>  <nl> # ifdef CONFIG_BATMAN_ADV_DEBUG
struct rockchip_thermal_data { <nl> # define TSADCV2_HIGHT_TSHUT_DEBOUNCE_COUNT 4 <nl> # define TSADCV2_AUTO_PERIOD_TIME 250 /* 250ms */ <nl> # define TSADCV2_AUTO_PERIOD_HT_TIME 50 /* 50ms */ <nl> -# define TSADCV3_AUTO_PERIOD_TIME 187500 /* 250ms */ <nl> -# define TSADCV3_AUTO_PERIOD_HT_TIME 37500 /* 50ms */ <nl> +# define TSADCV3_AUTO_PERIOD_TIME 1875 /* 2 . 5ms */ <nl> +# define TSADCV3_AUTO_PERIOD_HT_TIME 1875 /* 2 . 5ms */ <nl>  <nl> # define TSADCV2_USER_INTER_PD_SOC 0x340 /* 13 clocks */ <nl> 
static int inet6_rtm_getroute ( struct sk_buff * in_skb , struct nlmsghdr * nlh ) <nl> rt = ( struct rt6_info *) ip6_route_output ( net , NULL , & fl6 ); <nl> } <nl>  <nl> + if ( rt == net -> ipv6 . ip6_null_entry ) { <nl> + err = rt -> dst . error ; <nl> + ip6_rt_put ( rt ); <nl> + goto errout ; <nl> + } <nl> + <nl> skb = alloc_skb ( NLMSG_GOODSIZE , GFP_KERNEL ); <nl> if (! skb ) { <nl> ip6_rt_put ( rt );
int cx231xx_set_mode ( struct cx231xx * dev , enum cx231xx_mode set_mode ) <nl> } <nl> } <nl>  <nl> - return errCode ? - EINVAL : 0 ; <nl> + if ( errCode < 0 ) { <nl> + dev_err ( dev -> dev , " Failed to set devmode to % s : error : % i ", <nl> + dev -> mode == CX231XX_DIGITAL_MODE ? " digital " : " analog ", <nl> + errCode ); <nl> + return errCode ; <nl> + } <nl> + <nl> + return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( cx231xx_set_mode ); <nl> 
void rtl92se_disable_interrupt ( struct ieee80211_hw * hw ) <nl> rtl_write_dword ( rtlpriv , INTA_MASK + 4 , 0 ); <nl>  <nl> rtlpci -> irq_enabled = false ; <nl> + synchronize_irq ( rtlpci -> pdev -> irq ); <nl> } <nl>  <nl> 
static inline void wilc_wfi_cfg_parse_ch_attr ( u8 * buf , u32 len , u8 sta_ch ) <nl> if ( index + sizeof (* e ) + attr_size > len ) <nl> return ; <nl>  <nl> - if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST ) <nl> + if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST && <nl> + attr_size >= ( sizeof ( struct wilc_attr_ch_list ) - sizeof (* e ))) <nl> ch_list_idx = index ; <nl> else if ( e -> attr_type == IEEE80211_P2P_ATTR_OPER_CHANNEL && <nl> attr_size == ( sizeof ( struct wilc_attr_oper_ch ) - sizeof (* e )))
void fuse_conn_kill ( struct fuse_conn * fc ) <nl> kill_fasync (& fc -> fasync , SIGIO , POLL_IN ); <nl> wake_up_all (& fc -> waitq ); <nl> wake_up_all (& fc -> blocked_waitq ); <nl> - wake_up_all (& fc -> reserved_req_waitq ); <nl> } <nl> EXPORT_SYMBOL_GPL ( fuse_conn_kill ); <nl> 
static void __init offb_init_fb ( const char * name , const char * full_name , <nl> return ; <nl> } <nl>  <nl> - size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 17 ; <nl> + size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 16 ; <nl>  <nl> info = kmalloc ( size , GFP_ATOMIC ); <nl> 
static void ccwgroup_ungroup_callback ( struct device * dev ) <nl> struct ccwgroup_device * gdev = to_ccwgroupdev ( dev ); <nl>  <nl> mutex_lock (& gdev -> reg_mutex ); <nl> - __ccwgroup_remove_symlinks ( gdev ); <nl> - device_unregister ( dev ); <nl> + if ( device_is_registered (& gdev -> dev )) { <nl> + __ccwgroup_remove_symlinks ( gdev ); <nl> + device_unregister ( dev ); <nl> + } <nl> mutex_unlock (& gdev -> reg_mutex ); <nl> } <nl> 
int ext3_sync_file ( struct file * file , struct dentry * dentry , int datasync ) <nl> goto out ; <nl> } <nl>  <nl> + if ( datasync && !( inode -> i_state & I_DIRTY_DATASYNC )) <nl> + goto out ; <nl> + <nl> /* <nl> * The VFS has written the file data . If the inode is unaltered <nl> * then we need not start a commit .
static int si_common_early_init ( void * handle ) <nl> AMD_CG_SUPPORT_HDP_LS | <nl> AMD_CG_SUPPORT_HDP_MGCG ; <nl> adev -> pg_flags = 0 ; <nl> + adev -> external_rev_id = 70 ; <nl> break ; <nl>  <nl> default :
xfs_errortag_add ( int error_tag , xfs_mount_t * mp ) <nl> int len ; <nl> int64_t fsid ; <nl>  <nl> + if ( error_tag >= XFS_ERRTAG_MAX ) <nl> + return - EINVAL ; <nl> + <nl> memcpy (& fsid , mp -> m_fixedfsid , sizeof ( xfs_fsid_t )); <nl>  <nl> for ( i = 0 ; i < XFS_NUM_INJECT_ERROR ; i ++) {
static int whiteheat_attach ( struct usb_serial * serial ) <nl> err ("% s : Unable to retrieve firmware version , try replugging \ n ", serial -> type -> description ); <nl> err ("% s : If the firmware is not running ( status led not blinking )\ n ", serial -> type -> description ); <nl> err ("% s : please contact support @ connecttech . com \ n ", serial -> type -> description ); <nl> + kfree ( result ); <nl> return - ENODEV ; <nl>  <nl> no_command_private :
static void snd_timer_user_ccallback ( struct snd_timer_instance * timeri , <nl> tu -> tstamp = * tstamp ; <nl> if (( tu -> filter & ( 1 << event )) == 0 || ! tu -> tread ) <nl> return ; <nl> + memset (& r1 , 0 , sizeof ( r1 )); <nl> r1 . event = event ; <nl> r1 . tstamp = * tstamp ; <nl> r1 . val = resolution ;
static cycle_t sb1250_hpt_read ( void ) <nl> } <nl>  <nl> struct clocksource bcm1250_clocksource = { <nl> - . name = " MIPS ", <nl> + . name = " bcm1250 - counter - 3 ", <nl> . rating = 200 , <nl> . read = sb1250_hpt_read , <nl> . mask = CLOCKSOURCE_MASK ( 23 ),
static int xive_spapr_get_ipi ( unsigned int cpu , struct xive_cpu * xc ) <nl>  <nl> static void xive_spapr_put_ipi ( unsigned int cpu , struct xive_cpu * xc ) <nl> { <nl> + if (! xc -> hw_ipi ) <nl> + return ; <nl> + <nl> xive_irq_bitmap_free ( xc -> hw_ipi ); <nl> + xc -> hw_ipi = 0 ; <nl> } <nl> # endif /* CONFIG_SMP */ <nl> 
static int bcma_get_next_core ( struct bcma_bus * bus , u32 __iomem ** eromptr , <nl> switch ( core -> id . id ) { <nl> case BCMA_CORE_4706_MAC_GBIT_COMMON : <nl> case BCMA_CORE_NS_CHIPCOMMON_B : <nl> + case BCMA_CORE_PMU : <nl> + case BCMA_CORE_GCI : <nl> /* Not used yet : case BCMA_CORE_OOB_ROUTER : */ <nl> break ; <nl> default :
static void start_apic_timer ( struct kvm_lapic * apic ) <nl> { <nl> ktime_t now = apic -> lapic_timer . timer . base -> get_time (); <nl>  <nl> - apic -> lapic_timer . period = apic_get_reg ( apic , APIC_TMICT ) * <nl> + apic -> lapic_timer . period = ( u64 ) apic_get_reg ( apic , APIC_TMICT ) * <nl> APIC_BUS_CYCLE_NS * apic -> divide_count ; <nl> atomic_set (& apic -> lapic_timer . pending , 0 ); <nl> 
static int omap_gpio_irq_type ( struct irq_data * d , unsigned type ) <nl>  <nl> spin_lock_irqsave (& bank -> lock , flags ); <nl> retval = omap_set_gpio_triggering ( bank , offset , type ); <nl> - if ( retval ) <nl> + if ( retval ) { <nl> + spin_unlock_irqrestore (& bank -> lock , flags ); <nl> goto error ; <nl> + } <nl> omap_gpio_init_irq ( bank , offset ); <nl> if (! omap_gpio_is_input ( bank , offset )) { <nl> spin_unlock_irqrestore (& bank -> lock , flags );
void jfs_evict_inode ( struct inode * inode ) <nl> dquot_initialize ( inode ); <nl>  <nl> if ( JFS_IP ( inode )-> fileset == FILESYSTEM_I ) { <nl> + struct inode * ipimap = JFS_SBI ( inode -> i_sb )-> ipimap ; <nl> truncate_inode_pages_final (& inode -> i_data ); <nl>  <nl> if ( test_cflag ( COMMIT_Freewmap , inode )) <nl> jfs_free_zero_link ( inode ); <nl>  <nl> - if ( JFS_SBI ( inode -> i_sb )-> ipimap ) <nl> + if ( ipimap && JFS_IP ( ipimap )-> i_imap ) <nl> diFree ( inode ); <nl>  <nl> /*
static void collapse_huge_page ( struct mm_struct * mm , <nl> set_pmd_at ( mm , address , pmd , _pmd ); <nl> spin_unlock (& mm -> page_table_lock ); <nl> anon_vma_unlock ( vma -> anon_vma ); <nl> - mem_cgroup_uncharge_page ( new_page ); <nl> goto out ; <nl> } <nl>  <nl> static void collapse_huge_page ( struct mm_struct * mm , <nl> return ; <nl>  <nl> out : <nl> + mem_cgroup_uncharge_page ( new_page ); <nl> # ifdef CONFIG_NUMA <nl> put_page ( new_page ); <nl> # endif
int extract_param ( <nl> if ( len < 0 ) <nl> return - 1 ; <nl>  <nl> - if ( len > max_length ) { <nl> + if ( len >= max_length ) { <nl> pr_err (" Length of input : % d exceeds max_length :" <nl> " % d \ n ", len , max_length ); <nl> return - 1 ;
int __cpufreq_driver_target ( struct cpufreq_policy * policy , <nl>  <nl> pr_debug (" target for CPU % u : % u kHz , relation % u \ n ", policy -> cpu , <nl> target_freq , relation ); <nl> + <nl> + if ( target_freq == policy -> cur ) <nl> + return 0 ; <nl> + <nl> if ( cpu_online ( policy -> cpu ) && cpufreq_driver -> target ) <nl> retval = cpufreq_driver -> target ( policy , target_freq , relation ); <nl> 
static int ata_eh_recover ( struct ata_port * ap , ata_prereset_fn_t prereset , <nl> down_xfermask = 0 ; <nl> rc = 0 ; <nl>  <nl> + /* if UNLOADING , finish immediately */ <nl> + if ( ap -> flags & ATA_FLAG_UNLOADING ) <nl> + goto out ; <nl> + <nl> /* skip EH if possible . */ <nl> if ( ata_eh_skip_recovery ( ap )) <nl> ehc -> i . action = 0 ;
# include < asm / bitops . h > <nl> # include < asm / uaccess . h > <nl>  <nl> -# include " mxser . h " <nl> +# include " mxser_new . h " <nl>  <nl> # define MXSER_VERSION " 1 . 8 " <nl> # define MXSERMAJOR 174
static int pppoe_rcv_core ( struct sock * sk , struct sk_buff * skb ) <nl> * can ' t change . <nl> */ <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto abort_kfree ; <nl> + <nl> if ( sk -> sk_state & PPPOX_BOUND ) { <nl> ppp_input (& po -> chan , skb ); <nl> } else if ( sk -> sk_state & PPPOX_RELAY ) {
xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> struct posix_acl_entry * acl_e ; <nl> struct posix_acl * acl ; <nl> struct xfs_acl_entry * ace ; <nl> - int count , i ; <nl> + unsigned int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> if ( count > XFS_ACL_MAX_ENTRIES )
static int uas_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl>  <nl> shost -> max_cmd_len = 16 + 252 ; <nl> shost -> max_id = 1 ; <nl> + shost -> max_lun = 256 ; <nl> + shost -> max_channel = 0 ; <nl> shost -> sg_tablesize = udev -> bus -> sg_tablesize ; <nl>  <nl> devinfo -> intf = intf ;
static inline struct page * pte_alloc_one ( struct mm_struct * mm , <nl> struct page * pte ; <nl>  <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT , PTE_ORDER ); <nl> - if ( pte ) { <nl> - clear_highpage ( pte ); <nl> - pgtable_page_ctor ( pte ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + clear_highpage ( pte ); <nl> + if (! pgtable_page_ctor ( pte )) { <nl> + __free_page ( pte ); <nl> + return NULL ; <nl> } <nl> return pte ; <nl> }
static int _search_rsb ( struct dlm_ls * ls , char * name , int len , int b , <nl> kref_get (& r -> res_ref ); <nl> goto out ; <nl> } <nl> + if ( error == - ENOTBLK ) <nl> + goto out ; <nl> + <nl> error = dlm_search_rsb_tree (& ls -> ls_rsbtbl [ b ]. toss , name , len , flags , & r ); <nl> if ( error ) <nl> goto out ;
static int bf5xx_i2s_hw_params ( struct snd_pcm_substream * substream , <nl> bf5xx_i2s -> tcr2 |= 7 ; <nl> bf5xx_i2s -> rcr2 |= 7 ; <nl> sport_handle -> wdsize = 1 ; <nl> + break ; <nl> case SNDRV_PCM_FORMAT_S16_LE : <nl> bf5xx_i2s -> tcr2 |= 15 ; <nl> bf5xx_i2s -> rcr2 |= 15 ;
int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = dev_alloc_name ( ndev , ndev -> name ); <nl> if ( ret < 0 ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl>  <nl> int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = register_netdevice ( ndev ); <nl> if ( ret ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl> }
usba_ep_enable ( struct usb_ep * _ep , const struct usb_endpoint_descriptor * desc ) <nl>  <nl> spin_lock_irqsave (& ep -> udc -> lock , flags ); <nl>  <nl> - if ( ep -> ep . desc ) { <nl> - spin_unlock_irqrestore (& ep -> udc -> lock , flags ); <nl> - DBG ( DBG_ERR , " ep % d already enabled \ n ", ep -> index ); <nl> - return - EBUSY ; <nl> - } <nl> - <nl> ep -> ep . desc = desc ; <nl> ep -> ep . maxpacket = maxpacket ; <nl> 
xfs_attr3_rmt_verify ( <nl> if ( be32_to_cpu ( rmt -> rm_bytes ) > fsbsize - sizeof (* rmt )) <nl> return false ; <nl> if ( be32_to_cpu ( rmt -> rm_offset ) + <nl> - be32_to_cpu ( rmt -> rm_bytes ) >= XATTR_SIZE_MAX ) <nl> + be32_to_cpu ( rmt -> rm_bytes ) > XATTR_SIZE_MAX ) <nl> return false ; <nl> if ( rmt -> rm_owner == 0 ) <nl> return false ;
static int get_bitmap_file ( struct mddev * mddev , void __user * arg ) <nl> char * ptr ; <nl> int err ; <nl>  <nl> - file = kmalloc ( sizeof (* file ), GFP_NOIO ); <nl> + file = kzalloc ( sizeof (* file ), GFP_NOIO ); <nl> if (! file ) <nl> return - ENOMEM ; <nl> 
static int vga_video_font_height ; <nl> static int vga_scan_lines __read_mostly ; <nl> static unsigned int vga_rolled_over ; <nl>  <nl> - int vgacon_text_mode_force = 0 ; <nl> + static int vgacon_text_mode_force ; <nl>  <nl> bool vgacon_text_force ( void ) <nl> {
static int bcap_probe ( struct platform_device * pdev ) <nl> q -> mem_ops = & vb2_dma_contig_memops ; <nl> q -> timestamp_type = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC ; <nl>  <nl> - vb2_queue_init ( q ); <nl> + ret = vb2_queue_init ( q ); <nl> + if ( ret ) <nl> + goto err_free_handler ; <nl>  <nl> mutex_init (& bcap_dev -> mutex ); <nl> init_completion (& bcap_dev -> comp );
static int blkcg_print_stat ( struct seq_file * sf , void * v ) <nl> struct cftype blkcg_files [] = { <nl> { <nl> . name = " stat ", <nl> + . flags = CFTYPE_NOT_ON_ROOT , <nl> . seq_show = blkcg_print_stat , <nl> }, <nl> { } /* terminate */
static int stts751_read_chip_config ( struct stts751_priv * priv ) <nl> ret = i2c_smbus_read_byte_data ( priv -> client , STTS751_REG_RATE ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + if ( ret >= ARRAY_SIZE ( stts751_intervals )) { <nl> + dev_err ( priv -> dev , " Unrecognized conversion rate 0x % x \ n ", ret ); <nl> + return - ENODEV ; <nl> + } <nl> priv -> interval = ret ; <nl>  <nl> ret = stts751_read_reg16 ( priv , & priv -> event_max ,
static int i7core_register_mci ( struct i7core_dev * i7core_dev , <nl> } <nl>  <nl> fail : <nl> - edac_mc_free ( mci ); <nl> + if ( rc < 0 ) <nl> + edac_mc_free ( mci ); <nl> return rc ; <nl> } <nl> 
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
struct request_list * __blk_queue_next_rl ( struct request_list * rl , <nl> */ <nl> if ( rl == & q -> root_rl ) { <nl> ent = & q -> blkg_list ; <nl> + /* There are no more block groups , hence no request lists */ <nl> + if ( list_empty ( ent )) <nl> + return NULL ; <nl> } else { <nl> blkg = container_of ( rl , struct blkcg_gq , rl ); <nl> ent = & blkg -> q_node ;
static void __devinit tg3_get_eeprom_hw_cfg ( struct tg3 * tp ) <nl> ( cfg2 & NIC_SRAM_DATA_CFG_2_APD_EN )) <nl> tp -> tg3_flags3 |= TG3_FLG3_PHY_ENABLE_APD ; <nl>  <nl> - if ( tp -> tg3_flags2 & TG3_FLG2_PCI_EXPRESS ) { <nl> + if (( tp -> tg3_flags2 & TG3_FLG2_PCI_EXPRESS ) && <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) != ASIC_REV_5785 && <nl> + !( tp -> tg3_flags3 & TG3_FLG3_5717_PLUS )) { <nl> u32 cfg3 ; <nl>  <nl> tg3_read_mem ( tp , NIC_SRAM_DATA_CFG_3 , & cfg3 );
struct thread_map * thread_map__new_by_uid ( uid_t uid ) <nl> { <nl> DIR * proc ; <nl> int max_threads = 32 , items , i ; <nl> - char path [ 256 ]; <nl> + char path [ NAME_MAX + 1 + 6 ]; <nl> struct dirent * dirent , ** namelist = NULL ; <nl> struct thread_map * threads = thread_map__alloc ( max_threads ); <nl> 
static int doDevConfig ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> int sdev = - 1 , nchans , tmp ; <nl> struct BondedDevice * bdev = NULL ; <nl>  <nl> - if ( minor < 0 || minor > COMEDI_NUM_BOARD_MINORS ) { <nl> + if ( minor < 0 || minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> ERROR (" Minor % d is invalid !\ n ", minor ); <nl> return 0 ; <nl> }
static int tun_set_iff ( struct net * net , struct file * file , struct ifreq * ifr ) <nl> if (! dev ) <nl> return - ENOMEM ; <nl> err = dev_get_valid_name ( net , dev , name ); <nl> - if ( err ) <nl> + if ( err < 0 ) <nl> goto err_free_dev ; <nl>  <nl> dev_net_set ( dev , net );
static int load_module ( struct load_info * info , const char __user * uargs , <nl> mod_sysfs_teardown ( mod ); <nl> coming_cleanup : <nl> mod -> state = MODULE_STATE_GOING ; <nl> + destroy_params ( mod -> kp , mod -> num_kp ); <nl> blocking_notifier_call_chain (& module_notify_list , <nl> MODULE_STATE_GOING , mod ); <nl> klp_module_going ( mod );
static int gdma_dma_remove ( struct platform_device * pdev ) <nl> struct gdma_dma_dev * dma_dev = platform_get_drvdata ( pdev ); <nl>  <nl> tasklet_kill (& dma_dev -> task ); <nl> - of_dma_controller_free ( pdev -> dev . of_node ); <nl> + of_dma_controller_free ( pdev -> dev . of_node ); <nl> dma_async_device_unregister (& dma_dev -> ddev ); <nl>  <nl> return 0 ;
BPF_PROG_TYPE_FNS ( tracepoint , BPF_PROG_TYPE_TRACEPOINT ); <nl> BPF_PROG_TYPE_FNS ( xdp , BPF_PROG_TYPE_XDP ); <nl> BPF_PROG_TYPE_FNS ( perf_event , BPF_PROG_TYPE_PERF_EVENT ); <nl>  <nl> -# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ), type } <nl> +# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ) - 1 , type } <nl> static const struct { <nl> const char * sec ; <nl> size_t len ;
int policydb_read ( struct policydb * p , void * fp ) <nl> } else <nl> tr -> tclass = p -> process_class ; <nl>  <nl> + rc = - EINVAL ; <nl> if (! policydb_role_isvalid ( p , tr -> role ) || <nl> ! policydb_type_isvalid ( p , tr -> type ) || <nl> ! policydb_class_isvalid ( p , tr -> tclass ) ||
static void cgroup_enable_task_cg_lists ( void ) <nl> } <nl>  <nl> void cgroup_iter_start ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __acquires ( css_set_lock ) <nl> { <nl> /* <nl> * The first time anyone tries to iterate across a cgroup , <nl> struct task_struct * cgroup_iter_next ( struct cgroup * cgrp , <nl> } <nl>  <nl> void cgroup_iter_end ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __releases ( css_set_lock ) <nl> { <nl> read_unlock (& css_set_lock ); <nl> }
static int __init mvebu_soc_id_init ( void ) <nl>  <nl> res_ioremap : <nl> clk_disable_unprepare ( clk ); <nl> + clk_put ( clk ); <nl>  <nl> clk_err : <nl> of_node_put ( child );
int uprobe_write_opcode ( struct mm_struct * mm , unsigned long vaddr , <nl>  <nl> retry : <nl> /* Read the page with vaddr into memory */ <nl> - ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , FOLL_FORCE , & old_page , <nl> - & vma , NULL ); <nl> + ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , <nl> + FOLL_FORCE | FOLL_SPLIT , & old_page , & vma , NULL ); <nl> if ( ret <= 0 ) <nl> return ret ; <nl> 
static int nokia_modem_gpio_probe ( struct device * dev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - modem -> gpios = devm_kzalloc ( dev , gpio_count * <nl> - sizeof ( struct nokia_modem_gpio ), GFP_KERNEL ); <nl> + modem -> gpios = devm_kcalloc ( dev , gpio_count , sizeof (* modem -> gpios ), <nl> + GFP_KERNEL ); <nl> if (! modem -> gpios ) { <nl> dev_err ( dev , " Could not allocate memory for gpios \ n "); <nl> return - ENOMEM ;
static netdev_tx_t ipgre_tunnel_xmit ( struct sk_buff * skb , struct net_device * dev <nl> if ( skb_headroom ( skb ) < max_headroom || skb_shared ( skb )|| <nl> ( skb_cloned ( skb ) && ! skb_clone_writable ( skb , 0 ))) { <nl> struct sk_buff * new_skb = skb_realloc_headroom ( skb , max_headroom ); <nl> + if ( max_headroom > dev -> needed_headroom ) <nl> + dev -> needed_headroom = max_headroom ; <nl> if (! new_skb ) { <nl> ip_rt_put ( rt ); <nl> dev -> stats . tx_dropped ++;
minstrel_get_rate ( void * priv , struct ieee80211_sta * sta , <nl> return ; <nl> # endif <nl>  <nl> + /* Don ' t use EAPOL frames for sampling on non - mrr hw */ <nl> + if ( mp -> hw -> max_rates == 1 && <nl> + ( info -> control . flags & IEEE80211_TX_CTRL_PORT_CTRL_PROTO )) <nl> + return ; <nl> + <nl> delta = ( mi -> total_packets * sampling_ratio / 100 ) - <nl> ( mi -> sample_packets + mi -> sample_deferred / 2 ); <nl> 
i915_gem_pwrite_ioctl ( struct drm_device * dev , void * data , <nl>  <nl> if ( obj -> gtt_space && <nl> obj -> cache_level == I915_CACHE_NONE && <nl> + obj -> tiling_mode == I915_TILING_NONE && <nl> obj -> map_and_fenceable && <nl> obj -> base . write_domain != I915_GEM_DOMAIN_CPU ) { <nl> ret = i915_gem_gtt_pwrite_fast ( dev , obj , args , file );
static int wm5102_sysclk_ev ( struct snd_soc_dapm_widget * w , <nl> struct snd_kcontrol * kcontrol , int event ) <nl> { <nl> struct snd_soc_codec * codec = w -> codec ; <nl> - struct arizona * arizona = dev_get_drvdata ( codec -> dev ); <nl> + struct arizona * arizona = dev_get_drvdata ( codec -> dev -> parent ); <nl> struct regmap * regmap = codec -> control_data ; <nl> const struct reg_default * patch = NULL ; <nl> int i , patch_size ;
xfrm_init_tempstate ( struct xfrm_state * x , const struct flowi * fl , <nl> { <nl> struct xfrm_state_afinfo * afinfo = xfrm_state_afinfo_get_rcu ( family ); <nl>  <nl> - if ( afinfo ) <nl> - afinfo -> init_tempsel (& x -> sel , fl ); <nl> + if (! afinfo ) <nl> + return ; <nl> + <nl> + afinfo -> init_tempsel (& x -> sel , fl ); <nl>  <nl> if ( family != tmpl -> encap_family ) { <nl> afinfo = xfrm_state_afinfo_get_rcu ( tmpl -> encap_family );
static bool ironlake_get_pipe_config ( struct intel_crtc * crtc , <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> uint32_t tmp ; <nl>  <nl> + if (! intel_display_power_enabled ( dev_priv , <nl> + POWER_DOMAIN_PIPE ( crtc -> pipe ))) <nl> + return false ; <nl> + <nl> pipe_config -> cpu_transcoder = ( enum transcoder ) crtc -> pipe ; <nl> pipe_config -> shared_dpll = DPLL_ID_PRIVATE ; <nl> 
void cs46xx_dsp_destroy_pcm_channel ( struct snd_cs46xx * chip , <nl> if (! pcm_channel -> src_scb -> ref_count ) { <nl> cs46xx_dsp_remove_scb ( chip , pcm_channel -> src_scb ); <nl>  <nl> - snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot <= DSP_MAX_SRC_NR , <nl> + snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot < DSP_MAX_SRC_NR , <nl> return ); <nl>  <nl> ins -> src_scb_slots [ pcm_channel -> src_slot ] = 0 ;
ext4_mb_load_buddy ( struct super_block * sb , ext4_group_t group , <nl> grp = ext4_get_group_info ( sb , group ); <nl>  <nl> e4b -> bd_blkbits = sb -> s_blocksize_bits ; <nl> - e4b -> bd_info = ext4_get_group_info ( sb , group ); <nl> + e4b -> bd_info = grp ; <nl> e4b -> bd_sb = sb ; <nl> e4b -> bd_group = group ; <nl> e4b -> bd_buddy_page = NULL ;
void rt2x00ht_create_tx_descriptor ( struct queue_entry * entry , <nl> txdesc -> mpdu_density = 0 ; <nl>  <nl> txdesc -> ba_size = 7 ; /* FIXME : What value is needed ? */ <nl> - txdesc -> stbc = 0 ; /* FIXME : What value is needed ? */ <nl> + <nl> + txdesc -> stbc = <nl> + ( tx_info -> flags & IEEE80211_TX_CTL_STBC ) >> IEEE80211_TX_CTL_STBC_SHIFT ; <nl>  <nl> txdesc -> mcs = rt2x00_get_rate_mcs ( hwrate -> mcs ); <nl> if ( txrate -> flags & IEEE80211_TX_RC_USE_SHORT_PREAMBLE )
int adf_create_ring ( struct adf_accel_dev * accel_dev , const char * section , <nl> dev_err (& GET_DEV ( accel_dev ), " Can ' t get ring number \ n "); <nl> return - EFAULT ; <nl> } <nl> + if ( ring_num >= ADF_ETR_MAX_RINGS_PER_BANK ) { <nl> + dev_err (& GET_DEV ( accel_dev ), " Invalid ring number \ n "); <nl> + return - EFAULT ; <nl> + } <nl>  <nl> bank = & transport_data -> banks [ bank_num ]; <nl> if ( adf_reserve_ring ( bank , ring_num )) {
static int ieee80211_change_bss ( struct wiphy * wiphy , <nl> sdata -> flags &= ~ IEEE80211_SDATA_DONT_BRIDGE_PACKETS ; <nl> } <nl>  <nl> + if ( params -> ht_opmode >= 0 ) { <nl> + sdata -> vif . bss_conf . ht_operation_mode = <nl> + ( u16 ) params -> ht_opmode ; <nl> + changed |= BSS_CHANGED_HT ; <nl> + } <nl> + <nl> ieee80211_bss_info_change_notify ( sdata , changed ); <nl>  <nl> return 0 ;
int cpqhp_configure_device ( struct controller * ctrl , struct pci_func * func ) <nl> } <nl>  <nl> if ( func -> pci_dev -> hdr_type == PCI_HEADER_TYPE_BRIDGE ) { <nl> + int max ; <nl> pci_read_config_byte ( func -> pci_dev , PCI_SECONDARY_BUS , & bus ); <nl> child = ( struct pci_bus *) pci_add_new_bus ( func -> pci_dev -> bus , ( func -> pci_dev ), bus ); <nl> - pci_do_scan_bus ( child ); <nl> + max = pci_do_scan_bus ( child ); <nl> + pci_bus_update_busn_res_end ( child , max ); <nl> } <nl>  <nl> pci_dev_put ( func -> pci_dev );
static void pl011_dma_probe ( struct uart_amba_port * uap ) <nl> /* Optionally make use of an RX channel as well */ <nl> chan = dma_request_slave_channel ( dev , " rx "); <nl>  <nl> - if (! chan && plat -> dma_rx_param ) { <nl> + if (! chan && plat && plat -> dma_rx_param ) { <nl> chan = dma_request_channel ( mask , plat -> dma_filter , plat -> dma_rx_param ); <nl>  <nl> if (! chan ) {
static void cpufreq_stats_free_table ( unsigned int cpu ) <nl> static void cpufreq_stats_free_sysfs ( unsigned int cpu ) <nl> { <nl> struct cpufreq_policy * policy = cpufreq_cpu_get ( cpu ); <nl> + <nl> + if (! cpufreq_frequency_get_table ( cpu )) <nl> + return ; <nl> + <nl> if ( policy && ! policy_is_shared ( policy )) { <nl> pr_debug ("% s : Free sysfs stat \ n ", __func__ ); <nl> sysfs_remove_group (& policy -> kobj , & stats_attr_group );
int i40e_ndo_get_vf_config ( struct net_device * netdev , <nl> else <nl> ivi -> linkstate = IFLA_VF_LINK_STATE_DISABLE ; <nl> ivi -> spoofchk = vf -> spoofchk ; <nl> + ivi -> trusted = vf -> trusted ; <nl> ret = 0 ; <nl>  <nl> error_param :
static int xadc_parse_dt ( struct iio_dev * indio_dev , struct device_node * np , <nl> chan -> address = XADC_REG_VPVN ; <nl> } else { <nl> chan -> scan_index = 15 + reg ; <nl> - chan -> scan_index = XADC_REG_VAUX ( reg - 1 ); <nl> + chan -> address = XADC_REG_VAUX ( reg - 1 ); <nl> } <nl> num_channels ++; <nl> chan ++;
static int __init dw_probe ( struct platform_device * pdev ) <nl> dma_writel ( dw , CFG , DW_CFG_DMA_EN ); <nl>  <nl> printk ( KERN_INFO "% s : DesignWare DMA Controller , % d channels \ n ", <nl> - pdev -> dev . bus_id , dw -> dma . chancnt ); <nl> + dev_name (& pdev -> dev ), dw -> dma . chancnt ); <nl>  <nl> dma_async_device_register (& dw -> dma ); <nl> 
struct dst_entry * fib6_rule_lookup ( struct net * net , struct flowi6 * fl6 , <nl> if ( rt -> dst . error == - EAGAIN ) { <nl> ip6_rt_put_flags ( rt , flags ); <nl> rt = net -> ipv6 . ip6_null_entry ; <nl> - if (!( flags | RT6_LOOKUP_F_DST_NOREF )) <nl> + if (!( flags & RT6_LOOKUP_F_DST_NOREF )) <nl> dst_hold (& rt -> dst ); <nl> } <nl> 
static int key_notify_policy_flush ( const struct km_event * c ) <nl> hdr -> sadb_msg_pid = c -> portid ; <nl> hdr -> sadb_msg_version = PF_KEY_V2 ; <nl> hdr -> sadb_msg_errno = ( uint8_t ) 0 ; <nl> + hdr -> sadb_msg_satype = SADB_SATYPE_UNSPEC ; <nl> hdr -> sadb_msg_len = ( sizeof ( struct sadb_msg ) / sizeof ( uint64_t )); <nl> pfkey_broadcast ( skb_out , GFP_ATOMIC , BROADCAST_ALL , NULL , c -> net ); <nl> return 0 ;
void do_exit ( long code ) <nl>  <nl> module_put ( task_thread_info ( tsk )-> exec_domain -> module ); <nl>  <nl> - proc_exit_connector ( tsk ); <nl> /* <nl> * FIXME : do that only when needed , using sched_exit tracepoint <nl> */ <nl> flush_ptrace_hw_breakpoint ( tsk ); <nl>  <nl> exit_notify ( tsk , group_dead ); <nl> + proc_exit_connector ( tsk ); <nl> # ifdef CONFIG_NUMA <nl> task_lock ( tsk ); <nl> mpol_put ( tsk -> mempolicy );
struct lcd_device * lcd_device_register ( const char * name , struct device * parent , <nl>  <nl> rc = device_register (& new_ld -> dev ); <nl> if ( rc ) { <nl> - kfree ( new_ld ); <nl> + put_device (& new_ld -> dev ); <nl> return ERR_PTR ( rc ); <nl> } <nl> 
static void disk_seqf_stop ( struct seq_file * seqf , void * v ) <nl> if ( iter ) { <nl> class_dev_iter_exit ( iter ); <nl> kfree ( iter ); <nl> + seqf -> private = NULL ; <nl> } <nl> } <nl> 
u32 __tcp_select_window ( struct sock * sk ) <nl> */ <nl> if ( window <= free_space - mss || window > free_space ) <nl> window = ( free_space / mss )* mss ; <nl> + else if ( mss == full_space && <nl> + free_space > window + full_space / 2 ) <nl> + window = free_space ; <nl> } <nl>  <nl> return window ;
int sctp_packet_transmit ( struct sctp_packet * packet ) <nl> return err ; <nl> no_route : <nl> kfree_skb ( nskb ); <nl> - IP_INC_STATS_BH ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl> + IP_INC_STATS ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl>  <nl> /* FIXME : Returning the ' err ' will effect all the associations <nl> * associated with a socket , although only one of the paths of the
void intel_irq_init ( struct drm_device * dev ) <nl> dev -> driver -> get_vblank_counter = gm45_get_vblank_counter ; <nl> } <nl>  <nl> - <nl> - dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + if ( drm_core_check_feature ( dev , DRIVER_MODESET )) <nl> + dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + else <nl> + dev -> driver -> get_vblank_timestamp = NULL ; <nl> dev -> driver -> get_scanout_position = i915_get_crtc_scanoutpos ; <nl>  <nl> if ( IS_IVYBRIDGE ( dev )) {
int dss_mgr_enable ( struct omap_overlay_manager * mgr ) <nl> if (! mgr_manual_update ( mgr )) <nl> mp -> updating = true ; <nl>  <nl> + if (! dss_data . irq_enabled && need_isr ()) <nl> + dss_register_vsync_isr (); <nl> + <nl> spin_unlock_irqrestore (& data_lock , flags ); <nl>  <nl> if (! mgr_manual_update ( mgr ))
static struct perf_pmu * pmu_lookup ( const char * name ) <nl> LIST_HEAD ( aliases ); <nl> __u32 type ; <nl>  <nl> + /* No support for intel_bts or intel_pt so disallow them */ <nl> + if (! strcmp ( name , " intel_bts ") || ! strcmp ( name , " intel_pt ")) <nl> + return NULL ; <nl> + <nl> /* <nl> * The pmu data we store & need consists of the pmu <nl> * type value and format definitions . Load both right
int hists__browse ( struct hists * self , const char * helpline , const char * ev_name ) <nl>  <nl> switch ( key ) { <nl> case ' a ': <nl> - if ( browser -> selection -> map == NULL && <nl> + if ( browser -> selection -> map == NULL || <nl> browser -> selection -> map -> dso -> annotate_warned ) <nl> continue ; <nl> goto do_annotate ;
static int load_firmware ( struct octeon_device * oct ) <nl> char fw_name [ LIO_MAX_FW_FILENAME_LEN ]; <nl> char * tmp_fw_type ; <nl>  <nl> - if ( fw_type_is_auto ()) <nl> + if ( fw_type_is_auto ()) { <nl> tmp_fw_type = LIO_FW_NAME_TYPE_NIC ; <nl> - else <nl> + strncpy ( fw_type , tmp_fw_type , sizeof ( fw_type )); <nl> + } else { <nl> tmp_fw_type = fw_type ; <nl> + } <nl>  <nl> sprintf ( fw_name , "% s % s % s_ % s % s ", LIO_FW_DIR , LIO_FW_BASE_NAME , <nl> octeon_get_conf ( oct )-> card_name , tmp_fw_type ,
static int btree_gc_coalesce ( struct btree * b , struct btree_op * op , <nl> BUG_ON ( btree_bset_first ( new_nodes [ 0 ])-> keys ); <nl> btree_node_free ( new_nodes [ 0 ]); <nl> rw_unlock ( true , new_nodes [ 0 ]); <nl> + new_nodes [ 0 ] = NULL ; <nl>  <nl> for ( i = 0 ; i < nodes ; i ++) { <nl> if ( __bch_keylist_realloc (& keylist , bkey_u64s (& r [ i ]. b -> key )))
void __detach_mounts ( struct dentry * dentry ) <nl>  <nl> namespace_lock (); <nl> mp = lookup_mountpoint ( dentry ); <nl> - if (! mp ) <nl> + if ( IS_ERR_OR_NULL ( mp )) <nl> goto out_unlock ; <nl>  <nl> lock_mount_hash ();
static int handle_vmcall ( struct kvm_vcpu * vcpu ) <nl> return 1 ; <nl> } <nl>  <nl> - static int handle_vmx_insn ( struct kvm_vcpu * vcpu ) <nl> -{ <nl> - kvm_queue_exception ( vcpu , UD_VECTOR ); <nl> - return 1 ; <nl> -} <nl> - <nl> static int handle_invd ( struct kvm_vcpu * vcpu ) <nl> { <nl> return emulate_instruction ( vcpu , 0 ) == EMULATE_DONE ;
static cycle_t e1000e_cyclecounter_read ( const struct cyclecounter * cc ) <nl> systimeh = er32 ( SYSTIMH ); <nl> systimel_2 = er32 ( SYSTIML ); <nl> /* Check for overflow . If there was no overflow , use the values */ <nl> - if ( systimel_1 < systimel_2 ) { <nl> + if ( systimel_1 <= systimel_2 ) { <nl> systim = ( cycle_t ) systimel_1 ; <nl> systim |= ( cycle_t ) systimeh << 32 ; <nl> } else {
int __init dmar_parse_dev_scope ( void * start , void * end , int * cnt , <nl> if ( scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT || <nl> scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE ) <nl> (* cnt )++; <nl> - else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC ) { <nl> + else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC && <nl> + scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_HPET ) { <nl> pr_warn (" Unsupported device scope \ n "); <nl> } <nl> start += scope -> length ;
int vmw_get_cap_3d_ioctl ( struct drm_device * dev , void * data , <nl> memcpy_fromio ( bounce , & fifo_mem [ SVGA_FIFO_3D_CAPS ], size ); <nl>  <nl> ret = copy_to_user ( buffer , bounce , size ); <nl> + if ( ret ) <nl> + ret = - EFAULT ; <nl> vfree ( bounce ); <nl>  <nl> if ( unlikely ( ret != 0 ))
static int try_smi_init ( struct smi_info * new_smi ) <nl> */ <nl> new_smi -> pdev = platform_device_alloc (" ipmi_si ", <nl> new_smi -> intf_num ); <nl> - if ( rv ) { <nl> + if (! new_smi -> pdev ) { <nl> printk ( KERN_ERR <nl> " ipmi_si_intf :" <nl> " Unable to allocate platform device \ n ");
static struct dmi_system_id __initdata acpisleep_dmi_table [] = { <nl> }, <nl> { <nl> . callback = init_nvs_nosave , <nl> + . ident = " Sony Vaio VGN - FW41E_H ", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " Sony Corporation "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME , " VGN - FW41E_H "), <nl> + }, <nl> + }, <nl> + { <nl> + . callback = init_nvs_nosave , <nl> . ident = " Sony Vaio VGN - FW21E ", <nl> . matches = { <nl> DMI_MATCH ( DMI_SYS_VENDOR , " Sony Corporation "),
static int parse_audio_selector_unit ( struct mixer_build * state , int unitid , unsi <nl> kctl = snd_ctl_new1 (& mixer_selectunit_ctl , cval ); <nl> if (! kctl ) { <nl> snd_printk ( KERN_ERR " cannot malloc kcontrol \ n "); <nl> + kfree ( namelist ); <nl> kfree ( cval ); <nl> return - ENOMEM ; <nl> }
static int handle_emulation_failure ( struct kvm_vcpu * vcpu ) <nl> vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> vcpu -> run -> internal . ndata = 0 ; <nl> - r = EMULATE_FAIL ; <nl> + r = EMULATE_USER_EXIT ; <nl> } <nl> kvm_queue_exception ( vcpu , UD_VECTOR ); <nl> 
static int wm8903_probe ( struct snd_soc_codec * codec ) <nl> /* power down chip */ <nl> static int wm8903_remove ( struct snd_soc_codec * codec ) <nl> { <nl> + struct wm8903_priv * wm8903 = snd_soc_codec_get_drvdata ( codec ); <nl> + <nl> wm8903_free_gpio ( codec ); <nl> wm8903_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> + if ( wm8903 -> irq ) <nl> + free_irq ( wm8903 -> irq , codec ); <nl> + <nl> return 0 ; <nl> } <nl> 
static void i40evf_adminq_task ( struct work_struct * work ) <nl>  <nl> /* check for error indications */ <nl> val = rd32 ( hw , hw -> aq . arq . len ); <nl> + if ( val == 0xdeadbeef ) /* indicates device in reset */ <nl> + goto freedom ; <nl> oldval = val ; <nl> if ( val & I40E_VF_ARQLEN1_ARQVFE_MASK ) { <nl> dev_info (& adapter -> pdev -> dev , " ARQ VF Error detected \ n ");
static int ccid3_hc_tx_getsockopt ( struct sock * sk , const int optname , int len , <nl> case DCCP_SOCKOPT_CCID_TX_INFO : <nl> if ( len < sizeof ( tfrc )) <nl> return - EINVAL ; <nl> + memset (& tfrc , 0 , sizeof ( tfrc )); <nl> tfrc . tfrctx_x = hc -> tx_x ; <nl> tfrc . tfrctx_x_recv = hc -> tx_x_recv ; <nl> tfrc . tfrctx_x_calc = hc -> tx_x_calc ;
static int fsl_lpspi_probe ( struct platform_device * pdev ) <nl> ret = pm_runtime_get_sync ( fsl_lpspi -> dev ); <nl> if ( ret < 0 ) { <nl> dev_err ( fsl_lpspi -> dev , " failed to enable clock \ n "); <nl> - return ret ; <nl> + goto out_controller_put ; <nl> } <nl>  <nl> temp = readl ( fsl_lpspi -> base + IMX7ULP_PARAM );
int afu_register_irqs ( struct cxl_context * ctx , u32 count ) <nl> */ <nl> INIT_LIST_HEAD (& ctx -> irq_names ); <nl> for ( r = 1 ; r < CXL_IRQ_RANGES ; r ++) { <nl> - for ( i = 0 ; i < ctx -> irqs . range [ r ]; hwirq ++, i ++) { <nl> + for ( i = 0 ; i < ctx -> irqs . range [ r ]; i ++) { <nl> irq_name = kmalloc ( sizeof ( struct cxl_irq_name ), <nl> GFP_KERNEL ); <nl> if (! irq_name )
static void __init cpg_mssr_register_mod_clk ( const struct mssr_mod_clk * mod , <nl> # else <nl> dev_dbg ( dev , " Ignoring MSTP % s to prevent disabling \ n ", <nl> mod -> name ); <nl> + kfree ( clock ); <nl> return ; <nl> # endif <nl> }
static int ioctl_send_response ( struct client * client , void * buffer ) <nl> if ( copy_from_user ( r -> data , u64_to_uptr ( request -> data ), <nl> r -> length )) { <nl> ret = - EFAULT ; <nl> + kfree ( r -> request ); <nl> goto out ; <nl> } <nl> fw_send_response ( client -> device -> card , r -> request ,
static void __init reset_all_timers ( void ) <nl> * In other cases ( such as with VSAless OpenFirmware ), the system firmware <nl> * leaves timers available for us to use . <nl> */ <nl> - static int __init scan_timers ( struct cs5535_mfgpt_chip * mfgpt ) <nl> + static int __devinit scan_timers ( struct cs5535_mfgpt_chip * mfgpt ) <nl> { <nl> struct cs5535_mfgpt_timer timer = { . chip = mfgpt }; <nl> unsigned long flags ;
static void drm_cleanup ( struct drm_device * dev ) <nl> DRM_ERROR (" Cannot unload module \ n "); <nl> } <nl>  <nl> - int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> + static int drm_minors_cleanup ( int id , void * ptr , void * data ) <nl> { <nl> struct drm_minor * minor = ptr ; <nl> struct drm_device * dev ;
static int iscsi_login_zero_tsih_s1 ( <nl> if ( IS_ERR ( sess -> se_sess )) { <nl> iscsit_tx_login_rsp ( conn , ISCSI_STATUS_CLS_TARGET_ERR , <nl> ISCSI_LOGIN_STATUS_NO_RESOURCES ); <nl> + kfree ( sess -> sess_ops ); <nl> kfree ( sess ); <nl> return - ENOMEM ; <nl> }
static int btrfs_extent_same ( struct inode * src , u64 loff , u64 len , <nl> if ( src == dst ) <nl> return - EINVAL ; <nl>  <nl> + if ( len == 0 ) <nl> + return 0 ; <nl> + <nl> btrfs_double_lock ( src , loff , dst , dst_loff , len ); <nl>  <nl> ret = extent_same_check_offsets ( src , loff , len );
int i40e_ndo_set_vf_port_vlan ( struct net_device * netdev , int vf_id , <nl> VLAN_VID_MASK )); <nl> } <nl>  <nl> + spin_unlock_bh (& vsi -> mac_filter_hash_lock ); <nl> if ( vlan_id || qos ) <nl> ret = i40e_vsi_add_pvid ( vsi , vlanprio ); <nl> else <nl> i40e_vsi_remove_pvid ( vsi ); <nl> + spin_lock_bh (& vsi -> mac_filter_hash_lock ); <nl>  <nl> if ( vlan_id ) { <nl> dev_info (& pf -> pdev -> dev , " Setting VLAN % d , QOS 0x % x on VF % d \ n ",
int __init musb_platform_init ( struct musb * musb , void * board_data ) <nl>  <nl> usb_nop_xceiv_register (); <nl> musb -> xceiv = otg_get_transceiver (); <nl> - if (! musb -> xceiv ) <nl> + if (! musb -> xceiv ) { <nl> + gpio_free ( musb -> config -> gpio_vrsel ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> if ( ANOMALY_05000346 ) { <nl> bfin_write_USB_APHY_CALIB ( ANOMALY_05000346_value );
static int create_tracing_map_fields ( struct hist_trigger_data * hist_data ) <nl> struct tracing_map * map = hist_data -> map ; <nl> struct ftrace_event_field * field ; <nl> struct hist_field * hist_field ; <nl> - int i , idx ; <nl> + int i , idx = 0 ; <nl>  <nl> for_each_hist_field ( i , hist_data ) { <nl> hist_field = hist_data -> fields [ i ];
static void remap_cell_to_origin_clear_discard ( struct cache * cache , <nl> remap_to_origin ( cache , bio ); <nl> issue ( cache , bio ); <nl> } <nl> + <nl> + free_prison_cell ( cache , cell ); <nl> } <nl>  <nl> static void remap_cell_to_cache_dirty ( struct cache * cache , struct dm_bio_prison_cell * cell , <nl> static void remap_cell_to_cache_dirty ( struct cache * cache , struct dm_bio_prison_ <nl> remap_to_cache ( cache , bio , cblock ); <nl> issue ( cache , bio ); <nl> } <nl> + <nl> + free_prison_cell ( cache , cell ); <nl> } <nl>  <nl> /*----------------------------------------------------------------*/
static int rcar_gen2_usb_phy_probe ( struct platform_device * pdev ) <nl> priv -> phy . shutdown = rcar_gen2_usb_phy_shutdown ; <nl> priv -> phy . set_suspend = rcar_gen2_usb_phy_set_suspend ; <nl>  <nl> - retval = usb_add_phy (& priv -> phy , USB_PHY_TYPE_USB2 ); <nl> + retval = usb_add_phy_dev (& priv -> phy ); <nl> if ( retval < 0 ) { <nl> dev_err ( dev , " Failed to add USB phy \ n "); <nl> return retval ;
static inline int stack_map_data_size ( struct bpf_map * map ) <nl>  <nl> static int prealloc_elems_and_freelist ( struct bpf_stack_map * smap ) <nl> { <nl> - u32 elem_size = sizeof ( struct stack_map_bucket ) + smap -> map . value_size ; <nl> + u64 elem_size = sizeof ( struct stack_map_bucket ) + <nl> + ( u64 ) smap -> map . value_size ; <nl> int err ; <nl>  <nl> smap -> elems = bpf_map_area_alloc ( elem_size * smap -> map . max_entries ,
static inline void tpg_s_bytesperline ( struct tpg_data * tpg , unsigned plane , unsi <nl>  <nl> tpg -> bytesperline [ p ] = plane_w / tpg -> hdownsampling [ p ]; <nl> } <nl> + if ( tpg_g_interleaved ( tpg )) <nl> + tpg -> bytesperline [ 1 ] = tpg -> bytesperline [ 0 ]; <nl> } <nl>  <nl> 
int rds_cmsg_atomic ( struct rds_sock * rs , struct rds_message * rm , <nl> err : <nl> if ( page ) <nl> put_page ( page ); <nl> + rm -> atomic . op_active = 0 ; <nl> kfree ( rm -> atomic . op_notifier ); <nl>  <nl> return ret ;
static struct page ** __iommu_alloc_buffer ( struct device * dev , size_t size , <nl> int i = 0 ; <nl>  <nl> if ( array_size <= PAGE_SIZE ) <nl> - pages = kzalloc ( array_size , gfp ); <nl> + pages = kzalloc ( array_size , GFP_KERNEL ); <nl> else <nl> pages = vzalloc ( array_size ); <nl> if (! pages )
static int cpufreq_add_dev ( struct sys_device * sys_dev ) <nl>  <nl> spin_lock_irqsave (& cpufreq_driver_lock , flags ); <nl> for_each_cpu ( j , policy -> cpus ) { <nl> + if (! cpu_online ( j )) <nl> + continue ; <nl> per_cpu ( cpufreq_cpu_data , j ) = policy ; <nl> per_cpu ( policy_cpu , j ) = policy -> cpu ; <nl> }
static int find_parent_nodes ( struct btrfs_trans_handle * trans , <nl> } <nl> ret = find_extent_in_eb ( eb , bytenr , <nl> * extent_item_pos , & eie ); <nl> - ref -> inode_list = eie ; <nl> free_extent_buffer ( eb ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + ref -> inode_list = eie ; <nl> } <nl> ret = ulist_add_merge ( refs , ref -> parent , <nl> ( uintptr_t ) ref -> inode_list ,
static acpi_status WMID_set_capabilities ( void ) <nl> devices = *(( u32 *) obj -> buffer . pointer ); <nl> } else if ( obj -> type == ACPI_TYPE_INTEGER ) { <nl> devices = ( u32 ) obj -> integer . value ; <nl> + } else { <nl> + kfree ( out . pointer ); <nl> + return AE_ERROR ; <nl> } <nl> } else { <nl> kfree ( out . pointer );
static inline int __mkroute_input ( struct sk_buff * skb , <nl> # endif <nl> if ( in_dev -> cnf . no_policy ) <nl> rth -> u . dst . flags |= DST_NOPOLICY ; <nl> - if ( in_dev -> cnf . no_xfrm ) <nl> + if ( out_dev -> cnf . no_xfrm ) <nl> rth -> u . dst . flags |= DST_NOXFRM ; <nl> rth -> fl . fl4_dst = daddr ; <nl> rth -> rt_dst = daddr ;
static bool radeon_atom_apply_quirks ( struct drm_device * dev , <nl> if (( supported_device == ATOM_DEVICE_CRT1_SUPPORT ) || <nl> ( supported_device == ATOM_DEVICE_DFP2_SUPPORT )) <nl> return false ; <nl> + if ( supported_device == ATOM_DEVICE_CRT2_SUPPORT ) <nl> + * line_mux = 0x90 ; <nl> } <nl>  <nl> /* ASUS HD 3600 XT board lists the DVI port as HDMI */
static int futex_unlock_pi ( u32 __user * uaddr , struct rw_semaphore * fshared ) <nl> attempt ); <nl> if ( ret ) <nl> goto out ; <nl> + uval = 0 ; <nl> goto retry_unlocked ; <nl> } <nl> 
static void local_exit ( void ) <nl> DMINFO (" cleaned up "); <nl> } <nl>  <nl> - int (* _inits [])( void ) __initdata = { <nl> + static int (* _inits [])( void ) __initdata = { <nl> local_init , <nl> dm_target_init , <nl> dm_linear_init , <nl> int (* _inits [])( void ) __initdata = { <nl> dm_interface_init , <nl> }; <nl>  <nl> - void (* _exits [])( void ) = { <nl> + static void (* _exits [])( void ) = { <nl> local_exit , <nl> dm_target_exit , <nl> dm_linear_exit ,
static void change_port_settings ( struct tty_struct * tty , <nl> if (! baud ) { <nl> /* pick a default , any default ... */ <nl> baud = 9600 ; <nl> - } else <nl> + } else { <nl> + /* Avoid a zero divisor . */ <nl> + baud = min ( baud , 461550 ); <nl> tty_encode_baud_rate ( tty , baud , baud ); <nl> + } <nl>  <nl> edge_port -> baud_rate = baud ; <nl> config -> wBaudRate = ( __u16 )(( 461550L + baud / 2 ) / baud );
int skl_init_module ( struct skl_sst * ctx , <nl> return ret ; <nl> } <nl> mconfig -> m_state = SKL_MODULE_INIT_DONE ; <nl> - <nl> + kfree ( param_data ); <nl> return ret ; <nl> } <nl> 
static void setup_frame_info ( struct ieee80211_hw * hw , <nl> fi -> keyix = ATH9K_TXKEYIX_INVALID ; <nl> fi -> keytype = keytype ; <nl> fi -> framelen = framelen ; <nl> + <nl> + if (! rate ) <nl> + return ; <nl> fi -> rtscts_rate = rate -> hw_value ; <nl> if ( short_preamble ) <nl> fi -> rtscts_rate |= rate -> hw_value_short ;
static void iwl4965_rx_reply_tx ( struct iwl_priv * priv , <nl> struct ieee80211_tx_info * info ; <nl> struct iwl4965_tx_resp * tx_resp = ( void *)& pkt -> u . raw [ 0 ]; <nl> u32 status = le32_to_cpu ( tx_resp -> u . status ); <nl> - int tid = MAX_TID_COUNT ; <nl> + int tid = MAX_TID_COUNT - 1 ; <nl> int sta_id ; <nl> int freed ; <nl> u8 * qc = NULL ;
int v4l2_m2m_streamoff ( struct file * file , struct v4l2_m2m_ctx * m2m_ctx , <nl> /* Drop queue , since streamoff returns device to the same state as after <nl> * calling reqbufs . */ <nl> INIT_LIST_HEAD (& q_ctx -> rdy_queue ); <nl> + q_ctx -> num_rdy = 0 ; <nl> spin_unlock_irqrestore (& q_ctx -> rdy_spinlock , flags ); <nl>  <nl> if ( m2m_dev -> curr_ctx == m2m_ctx ) {
static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x23 , { KEY_PROG3 } }, /* P_Key */ <nl> { KE_KEY , 0x24 , { KEY_PROG4 } }, /* Social networking_Key */ <nl> { KE_KEY , 0x64 , { KEY_SWITCHVIDEOMODE } }, /* Display Switch */ <nl> - { KE_KEY , 0x82 , { KEY_F22 } }, /* Touch Pad On / Off */ <nl> + { KE_KEY , 0x82 , { KEY_TOUCHPAD_TOGGLE } }, /* Touch Pad On / Off */ <nl> { KE_END , 0 } <nl> }; <nl> 
static int rt5514_dsp_voice_wake_up_put ( struct snd_kcontrol * kcontrol , <nl> # else <nl> dev_err ( component -> dev , " There is no SPI driver for " <nl> " loading the firmware \ n "); <nl> + memset ( buf , 0 , sizeof ( buf )); <nl> # endif <nl> rt5514 -> pll3_cal_value = buf [ 0 ] | buf [ 1 ] << 8 | <nl> buf [ 2 ] << 16 | buf [ 3 ] << 24 ;
static int powermate_probe ( struct usb_interface * intf , const struct usb_device_i <nl> int error = - ENOMEM ; <nl>  <nl> interface = intf -> cur_altsetting ; <nl> + if ( interface -> desc . bNumEndpoints < 1 ) <nl> + return - EINVAL ; <nl> + <nl> endpoint = & interface -> endpoint [ 0 ]. desc ; <nl> if (! usb_endpoint_is_int_in ( endpoint )) <nl> return - EIO ;
static int alps_enter_command_mode ( struct psmouse * psmouse , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( param [ 0 ] != 0x88 && param [ 1 ] != 0x07 ) { <nl> + if ( param [ 0 ] != 0x88 || ( param [ 1 ] != 0x07 && param [ 1 ] != 0x08 )) { <nl> psmouse_dbg ( psmouse , <nl> " unknown response while entering command mode \ n "); <nl> return - 1 ;
static int irda_recvmsg_dgram ( struct kiocb * iocb , struct socket * sock , <nl>  <nl> IRDA_DEBUG ( 4 , "% s ()\ n ", __func__ ); <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> skb = skb_recv_datagram ( sk , flags & ~ MSG_DONTWAIT , <nl> flags & MSG_DONTWAIT , & err ); <nl> if (! skb )
static void __devinit bnx2x_link_settings_supported ( struct bnx2x * bp , <nl> break ; <nl>  <nl> case PORT_HW_CFG_XGXS_EXT_PHY_TYPE_BCM8481 : <nl> - BNX2X_DEV_INFO (" ext_phy_type 0x % x ( BCM8481 )\ n ", <nl> + case PORT_HW_CFG_XGXS_EXT_PHY_TYPE_BCM84823 : <nl> + BNX2X_DEV_INFO (" ext_phy_type 0x % x ( BCM848xx )\ n ", <nl> ext_phy_type ); <nl>  <nl> bp -> port . supported |= ( SUPPORTED_10baseT_Half |
static int xen_add_device ( struct device * dev ) <nl>  <nl> # ifdef CONFIG_ACPI <nl> handle = DEVICE_ACPI_HANDLE (& pci_dev -> dev ); <nl> - if (! handle ) <nl> + if (! handle && pci_dev -> bus -> bridge ) <nl> handle = DEVICE_ACPI_HANDLE ( pci_dev -> bus -> bridge ); <nl> # ifdef CONFIG_PCI_IOV <nl> if (! handle && pci_dev -> is_virtfn )
static int ar9003_hw_set_channel ( struct ath_hw * ah , struct ath9k_channel * chan ) <nl> u32 chan_frac ; <nl>  <nl> channelSel = ( freq * 2 ) / 75 ; <nl> - chan_frac = (( freq % 75 ) * 0x20000 ) / 75 ; <nl> + chan_frac = ((( freq * 2 ) % 75 ) * 0x20000 ) / 75 ; <nl> channelSel = ( channelSel << 17 ) | chan_frac ; <nl> } else { <nl> channelSel = CHANSEL_5G ( freq );
int iomap_fiemap ( struct inode * inode , struct fiemap_extent_info * fi , <nl> while ( len > 0 ) { <nl> ret = iomap_apply ( inode , start , len , 0 , ops , & ctx , <nl> iomap_fiemap_actor ); <nl> + /* inode with no ( attribute ) mapping will give ENOENT */ <nl> + if ( ret == - ENOENT ) <nl> + break ; <nl> if ( ret < 0 ) <nl> return ret ; <nl> if ( ret == 0 )
struct platform_device * __init imx_add_platform_device_dmamask ( <nl> ret = platform_device_add ( pdev ); <nl> if ( ret ) { <nl> err : <nl> + if ( dmamask ) <nl> + kfree ( pdev -> dev . dma_mask ); <nl> platform_device_put ( pdev ); <nl> return ERR_PTR ( ret ); <nl> }
static inline int ptr_ring_consume_batched_bh ( struct ptr_ring * r , <nl>  <nl> static inline void ** __ptr_ring_init_queue_alloc ( unsigned int size , gfp_t gfp ) <nl> { <nl> + if ( size * sizeof ( void *) > KMALLOC_MAX_SIZE ) <nl> + return NULL ; <nl> return kcalloc ( size , sizeof ( void *), gfp ); <nl> } <nl> 
static void mmu_pte_write_zap_pte ( struct kvm_vcpu * vcpu , <nl> mmu_page_remove_parent_pte ( child , spte ); <nl> } <nl> } <nl> - * spte = 0 ; <nl> + set_shadow_pte ( spte , 0 ); <nl> kvm_flush_remote_tlbs ( vcpu -> kvm ); <nl> } <nl> 
static int intel_pstate_set_policy ( struct cpufreq_policy * policy ) <nl> limits -> max_sysfs_pct ); <nl> limits -> max_perf_pct = max ( limits -> min_policy_pct , <nl> limits -> max_perf_pct ); <nl> + limits -> max_perf = round_up ( limits -> max_perf , 8 ); <nl>  <nl> /* Make sure min_perf_pct <= max_perf_pct */ <nl> limits -> min_perf_pct = min ( limits -> max_perf_pct , limits -> min_perf_pct );
static int clie_5_attach ( struct usb_serial * serial ) <nl> */ <nl>  <nl> /* some sanity check */ <nl> - if ( serial -> num_ports < 2 ) <nl> - return - 1 ; <nl> + if ( serial -> num_bulk_out < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing bulk out endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> /* port 0 now uses the modified endpoint Address */ <nl> port = serial -> port [ 0 ];
static ssize_t input_dev_show_modalias ( struct class_device * dev , char * buf ) <nl>  <nl> len = input_print_modalias ( buf , PAGE_SIZE , id , 1 ); <nl>  <nl> - return max_t ( int , len , PAGE_SIZE ); <nl> + return min_t ( int , len , PAGE_SIZE ); <nl> } <nl> static CLASS_DEVICE_ATTR ( modalias , S_IRUGO , input_dev_show_modalias , NULL ); <nl> 
ecryptfs_setxattr ( struct dentry * dentry , const char * name , const void * value , <nl> } <nl>  <nl> rc = vfs_setxattr ( lower_dentry , name , value , size , flags ); <nl> + if (! rc ) <nl> + fsstack_copy_attr_all ( dentry -> d_inode , lower_dentry -> d_inode ); <nl> out : <nl> return rc ; <nl> }
static void do_config_file ( const char * filename ) <nl> perror ( filename ); <nl> exit ( 2 ); <nl> } <nl> - fstat ( fd , & st ); <nl> + if ( fstat ( fd , & st ) < 0 ) { <nl> + fprintf ( stderr , " fixdep : error fstat ' ing config file : "); <nl> + perror ( filename ); <nl> + exit ( 2 ); <nl> + } <nl> if ( st . st_size == 0 ) { <nl> close ( fd ); <nl> return ;
static int stmmac_hw_init ( struct stmmac_priv * priv ) <nl> struct mac_device_info * mac ; <nl>  <nl> /* Identify the MAC HW device */ <nl> - if ( priv -> plat -> has_gmac ) <nl> + if ( priv -> plat -> has_gmac ) { <nl> + priv -> dev -> priv_flags |= IFF_UNICAST_FLT ; <nl> mac = dwmac1000_setup ( priv -> ioaddr ); <nl> - else <nl> + } else { <nl> mac = dwmac100_setup ( priv -> ioaddr ); <nl> + } <nl> if (! mac ) <nl> return - ENOMEM ; <nl> 
static int msr_open ( struct inode * inode , struct file * file ) <nl> unsigned int cpu ; <nl> struct cpuinfo_x86 * c ; <nl>  <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> + <nl> cpu = iminor ( file -> f_path . dentry -> d_inode ); <nl> if ( cpu >= nr_cpu_ids || ! cpu_online ( cpu )) <nl> return - ENXIO ; /* No such CPU */
int qla24xx_async_notify_ack ( scsi_qla_host_t * vha , fc_port_t * fcport , <nl> qla2x00_init_timer ( sp , qla2x00_get_async_timeout ( vha )+ 2 ); <nl>  <nl> sp -> u . iocb_cmd . u . nack . ntfy = ntfy ; <nl> - <nl> + sp -> u . iocb_cmd . timeout = qla2x00_async_iocb_timeout ; <nl> sp -> done = qla2x00_async_nack_sp_done ; <nl>  <nl> rval = qla2x00_start_sp ( sp );
static int synic_set_irq ( struct kvm_vcpu_hv_synic * synic , u32 sint ) <nl> struct kvm_lapic_irq irq ; <nl> int ret , vector ; <nl>  <nl> + if ( KVM_BUG_ON (! lapic_in_kernel ( vcpu ), vcpu -> kvm )) <nl> + return - EINVAL ; <nl> + <nl> if ( sint >= ARRAY_SIZE ( synic -> sint )) <nl> return - EINVAL ; <nl> 
void check_and_switch_context ( struct mm_struct * mm , unsigned int cpu ) <nl> raw_spin_unlock_irqrestore (& cpu_asid_lock , flags ); <nl>  <nl> switch_mm_fastpath : <nl> + <nl> + arm64_apply_bp_hardening (); <nl> + <nl> /* <nl> * Defer TTBR0_EL1 setting for user threads to uaccess_enable () when <nl> * emulating PAN . <nl> asmlinkage void post_ttbr_update_workaround ( void ) <nl> " ic iallu ; dsb nsh ; isb ", <nl> ARM64_WORKAROUND_CAVIUM_27456 , <nl> CONFIG_CAVIUM_ERRATUM_27456 )); <nl> - <nl> - arm64_apply_bp_hardening (); <nl> } <nl>  <nl> static int asids_init ( void )
void dma_run_dependencies ( struct dma_async_tx_descriptor * tx ) <nl> if (! dep ) <nl> return ; <nl>  <nl> + /* we ' ll submit tx -> next now , so clear the link */ <nl> + tx -> next = NULL ; <nl> chan = dep -> chan ; <nl>  <nl> /* keep submitting up until a channel switch is detected
static void vmw_fb_dirty_flush ( struct vmw_fb_par * par ) <nl> SVGAFifoCmdUpdate body ; <nl> } * cmd ; <nl>  <nl> + if ( vmw_priv -> suspended ) <nl> + return ; <nl> + <nl> spin_lock_irqsave (& par -> dirty . lock , flags ); <nl> if (! par -> dirty . active ) { <nl> spin_unlock_irqrestore (& par -> dirty . lock , flags );
static void edge_bulk_in_callback ( struct urb * urb ) <nl>  <nl> port_number = edge_port -> port -> port_number ; <nl>  <nl> - if ( edge_port -> lsr_event ) { <nl> + if ( urb -> actual_length > 0 && edge_port -> lsr_event ) { <nl> edge_port -> lsr_event = 0 ; <nl> dev_dbg ( dev , "% s ===== Port % u LSR Status = % 02x , Data = % 02x ======\ n ", <nl> __func__ , port_number , edge_port -> lsr_mask , * data );
static int process_ctrl_td ( struct xhci_hcd * xhci , struct xhci_td * td , <nl> case TRB_NORMAL : <nl> td -> urb -> actual_length = requested - remaining ; <nl> goto finish_td ; <nl> + case TRB_STATUS : <nl> + td -> urb -> actual_length = requested ; <nl> + goto finish_td ; <nl> default : <nl> xhci_warn ( xhci , " WARN : unexpected TRB Type % d \ n ", <nl> trb_type );
__update_curr ( struct cfs_rq * cfs_rq , struct sched_entity * curr , <nl> schedstat_set ( curr -> exec_max , max (( u64 ) delta_exec , curr -> exec_max )); <nl>  <nl> curr -> sum_exec_runtime += delta_exec ; <nl> - cfs_rq -> exec_clock += delta_exec ; <nl> + schedstat_add ( cfs_rq , exec_clock , delta_exec ); <nl> delta_exec_weighted = delta_exec ; <nl> if ( unlikely ( curr -> load . weight != NICE_0_LOAD )) { <nl> delta_exec_weighted = calc_delta_fair ( delta_exec_weighted ,
static struct snd_emu_chip_details emu_chip_details [] = { <nl> . ca0151_chip = 1 , <nl> . spk71 = 1 , <nl> . spdif_bug = 1 , <nl> + . invert_shared_spdif = 1 , /* digital / analog switch swapped */ <nl> . ac97_chip = 1 } , <nl> {. vendor = 0x1102 , . device = 0x0004 , . subsystem = 0x10021102 , <nl> . driver = " Audigy2 ", . name = " SB Audigy 2 Platinum [ SB0240P ]",
int rds_rdma_extra_size ( struct rds_rdma_args * args ) <nl>  <nl> local_vec = ( struct rds_iovec __user *)( unsigned long ) args -> local_vec_addr ; <nl>  <nl> + if ( args -> nr_local == 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* figure out the number of pages in the vector */ <nl> for ( i = 0 ; i < args -> nr_local ; i ++) { <nl> if ( copy_from_user (& vec , & local_vec [ i ],
int free_stream_context ( struct intel_sst_drv * ctx , unsigned int str_id ) <nl> if ( ret ) <nl> sst_clean_stream (& ctx -> streams [ str_id ]); <nl> return ret ; <nl> + } else { <nl> + dev_err ( ctx -> dev , " we tried to free stream context % d which was freed !!!\ n ", str_id ); <nl> } <nl> return ret ; <nl> }
int genl_register_family ( struct genl_family * family ) <nl> start , end + 1 , GFP_KERNEL ); <nl> if ( family -> id < 0 ) { <nl> err = family -> id ; <nl> - goto errout_locked ; <nl> + goto errout_free ; <nl> } <nl>  <nl> err = genl_validate_assign_mc_groups ( family ); <nl> int genl_register_family ( struct genl_family * family ) <nl>  <nl> errout_remove : <nl> idr_remove (& genl_fam_idr , family -> id ); <nl> + errout_free : <nl> kfree ( family -> attrbuf ); <nl> errout_locked : <nl> genl_unlock_all ();
int mwifiex_bss_start ( struct mwifiex_private * priv , struct cfg80211_bss * bss , <nl> goto done ; <nl> } <nl>  <nl> - if ( priv -> bss_mode == NL80211_IFTYPE_STATION ) { <nl> + if ( priv -> bss_mode == NL80211_IFTYPE_STATION || <nl> + priv -> bss_mode == NL80211_IFTYPE_P2P_CLIENT ) { <nl> u8 config_bands ; <nl>  <nl> - /* Infra mode */ <nl> ret = mwifiex_deauthenticate ( priv , NULL ); <nl> if ( ret ) <nl> goto done ;
static int elo_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl>  <nl> return 0 ; <nl> err_free : <nl> + usb_put_dev ( udev ); <nl> kfree ( priv ); <nl> return ret ; <nl> }
static int db8500_prcmu_probe ( struct platform_device * pdev ) <nl> } <nl> tcdm_base = devm_ioremap (& pdev -> dev , res -> start , <nl> resource_size ( res )); <nl> + if (! tcdm_base ) { <nl> + dev_err (& pdev -> dev , <nl> + " failed to ioremap prcmu - tcdm register memory \ n "); <nl> + return - ENOENT ; <nl> + } <nl>  <nl> /* Clean up the mailbox interrupts after pre - kernel code . */ <nl> writel ( ALL_MBOX_BITS , PRCM_ARM_IT1_CLR );
sg_start_req ( Sg_request * srp , unsigned char * cmd ) <nl> md -> from_user = 0 ; <nl> } <nl>  <nl> + if ( unlikely ( iov_count > MAX_UIOVEC )) <nl> + return - EINVAL ; <nl> + <nl> if ( iov_count ) { <nl> int size = sizeof ( struct iovec ) * iov_count ; <nl> struct iovec * iov ;
static int destroy_queue_nocpsch ( struct device_queue_manager * dqm , <nl> } <nl> dqm -> sdma_queue_count --; <nl> deallocate_sdma_queue ( dqm , q -> sdma_id ); <nl> + } else { <nl> + pr_debug (" q -> properties . type is invalid (% d )\ n ", <nl> + q -> properties . type ); <nl> + retval = - EINVAL ; <nl> + goto out ; <nl> } <nl>  <nl> retval = mqd -> destroy_mqd ( mqd , q -> mqd ,
struct stedma40_platform_data dma40_plat_data = { <nl> struct platform_device u8500_dma40_device = { <nl> . dev = { <nl> . platform_data = & dma40_plat_data , <nl> + . coherent_dma_mask = DMA_BIT_MASK ( 32 ), <nl> }, <nl> . name = " dma40 ", <nl> . id = 0 ,
int add_mtd_partitions ( struct mtd_info * master , <nl>  <nl> for ( i = 0 ; i < nbparts ; i ++) { <nl> slave = allocate_partition ( master , parts + i , i , cur_offset ); <nl> - if ( IS_ERR ( slave )) <nl> + if ( IS_ERR ( slave )) { <nl> + del_mtd_partitions ( master ); <nl> return PTR_ERR ( slave ); <nl> + } <nl>  <nl> mutex_lock (& mtd_partitions_mutex ); <nl> list_add (& slave -> list , & mtd_partitions );
static void audit_log_feature_change ( int which , u32 old_feature , u32 new_feature <nl> { <nl> struct audit_buffer * ab ; <nl>  <nl> + if ( audit_enabled == AUDIT_OFF ) <nl> + return ; <nl> + <nl> ab = audit_log_start ( NULL , GFP_KERNEL , AUDIT_FEATURE_CHANGE ); <nl> audit_log_format ( ab , " feature =% s old =% d new =% d old_lock =% d new_lock =% d res =% d ", <nl> audit_feature_names [ which ], !! old_feature , !! new_feature ,
static int bcm7038_wdt_probe ( struct platform_device * pdev ) <nl> wdt -> clk = devm_clk_get ( dev , NULL ); <nl> /* If unable to get clock , use default frequency */ <nl> if (! IS_ERR ( wdt -> clk )) { <nl> - clk_prepare_enable ( wdt -> clk ); <nl> + err = clk_prepare_enable ( wdt -> clk ); <nl> + if ( err ) <nl> + return err ; <nl> wdt -> rate = clk_get_rate ( wdt -> clk ); <nl> /* Prevent divide - by - zero exception */ <nl> if (! wdt -> rate )
static int ethtool_phys_id ( struct net_device * dev , void __user * useraddr ) <nl> if ( rc == 0 ) { <nl> /* Driver will handle this itself */ <nl> schedule_timeout_interruptible ( <nl> - id . data ? id . data : MAX_SCHEDULE_TIMEOUT ); <nl> + id . data ? ( id . data * HZ ) : MAX_SCHEDULE_TIMEOUT ); <nl> } else { <nl> /* Driver expects to be called periodically */ <nl> do {
static int asoc_simple_card_dai_link_of ( struct device_node * node , <nl> strlen ( dai_link -> cpu_dai_name ) + <nl> strlen ( dai_link -> codec_dai_name ) + 2 , <nl> GFP_KERNEL ); <nl> + if (! name ) { <nl> + ret = - ENOMEM ; <nl> + goto dai_link_of_err ; <nl> + } <nl> + <nl> sprintf ( name , "% s -% s ", dai_link -> cpu_dai_name , <nl> dai_link -> codec_dai_name ); <nl> dai_link -> name = dai_link -> stream_name = name ;
static int find_probe_functions ( struct map * map , char * name ) <nl> struct symbol * sym ; <nl> struct rb_node * tmp ; <nl>  <nl> + if ( map__load ( map , NULL ) < 0 ) <nl> + return 0 ; <nl> + <nl> map__for_each_symbol ( map , sym , tmp ) { <nl> if ( strglobmatch ( sym -> name , name )) <nl> found ++;
static int llog_process_thread ( void * arg ) <nl> else <nl> last_index = LLOG_BITMAP_BYTES * 8 - 1 ; <nl>  <nl> + /* Record is not in this buffer . */ <nl> + if ( index > last_index ) <nl> + goto out ; <nl> + <nl> while ( rc == 0 ) { <nl> struct llog_rec_hdr * rec ; <nl> 
static int do_recover_data ( struct f2fs_sb_info * sbi , struct inode * inode , <nl> # endif <nl> /* We should not get - ENOSPC */ <nl> f2fs_bug_on ( sbi , err ); <nl> + if ( err ) <nl> + goto err ; <nl> } <nl>  <nl> /* Check the previous node page having this index */
static int omap_pcm_new ( struct snd_soc_pcm_runtime * rtd ) <nl> } <nl>  <nl> out : <nl> + /* free preallocated buffers in case of error */ <nl> + if ( ret ) <nl> + omap_pcm_free_dma_buffers ( pcm ); <nl> + <nl> return ret ; <nl> } <nl> 
static struct xfrm_state * pfkey_msg2xfrm_state ( struct net * net , <nl> ext_hdrs [ SADB_X_EXT_NAT_T_DPORT - 1 ]; <nl> natt -> encap_dport = n_port -> sadb_x_nat_t_port_port ; <nl> } <nl> + memset (& natt -> encap_oa , 0 , sizeof ( natt -> encap_oa )); <nl> } <nl>  <nl> err = xfrm_init_state ( x );
static void __ieee80211_scan_completed ( struct ieee80211_hw * hw , bool aborted , <nl> if ( local -> scan_req != local -> int_scan_req ) <nl> cfg80211_scan_done ( local -> scan_req , aborted ); <nl> local -> scan_req = NULL ; <nl> - local -> scan_sdata = NULL ; <nl> + rcu_assign_pointer ( local -> scan_sdata , NULL ); <nl>  <nl> local -> scanning = 0 ; <nl> local -> scan_channel = NULL ;
static int __init macide_init ( void ) <nl> int irq ; <nl> hw_regs_t hw ; <nl>  <nl> + if (! MACH_IS_MAC ) <nl> + return - ENODEV ; <nl> + <nl> switch ( macintosh_config -> ide_type ) { <nl> case MAC_IDE_QUADRA : <nl> base = IDE_BASE ;
static int crypt_iv_tcw_whitening ( struct crypt_config * cc , <nl> for ( i = 0 ; i < (( 1 << SECTOR_SHIFT ) / 8 ); i ++) <nl> crypto_xor ( data + i * 8 , buf , 8 ); <nl> out : <nl> - memset ( buf , 0 , sizeof ( buf )); <nl> + memzero_explicit ( buf , sizeof ( buf )); <nl> return r ; <nl> } <nl> 
void psb_intel_crtc_init ( struct drm_device * dev , int pipe , <nl> ( struct drm_connector **) ( psb_intel_crtc + 1 ); <nl> psb_intel_crtc -> mode_set . num_connectors = 0 ; <nl> psb_intel_cursor_init ( dev , psb_intel_crtc ); <nl> + <nl> + /* Set to true so that the pipe is forced off on initial config . */ <nl> + psb_intel_crtc -> active = true ; <nl> } <nl>  <nl> int psb_intel_get_pipe_from_crtc_id ( struct drm_device * dev , void * data ,
static void oz_add_farewell ( struct oz_pd * pd , u8 ep_num , u8 index , <nl> return ; <nl> f -> ep_num = ep_num ; <nl> f -> index = index ; <nl> + f -> len = len ; <nl> memcpy ( f -> report , report , len ); <nl> oz_dbg ( ON , " RX : Adding farewell report \ n "); <nl> spin_lock (& g_polling_lock );
__setup (" mce =", mcheck_enable ); <nl> static int mce_resume ( struct sys_device * dev ) <nl> { <nl> mce_init ( NULL ); <nl> + mce_cpu_features (& current_cpu_data ); <nl> return 0 ; <nl> } <nl> 
static long ioctl_file_dedupe_range ( struct file * file , void __user * arg ) <nl> goto out ; <nl> } <nl>  <nl> + same -> dest_count = count ; <nl> ret = vfs_dedupe_file_range ( file , same ); <nl> if ( ret ) <nl> goto out ;
long join_session_keyring ( const char * name ) <nl> ret = PTR_ERR ( keyring ); <nl> goto error2 ; <nl> } else if ( keyring == new -> session_keyring ) { <nl> + key_put ( keyring ); <nl> ret = 0 ; <nl> goto error2 ; <nl> }
static int intel_backlight_device_update_status ( struct backlight_device * bd ) <nl> */ <nl> if ( panel -> backlight . enabled ) { <nl> if ( panel -> backlight_power ) { <nl> - bool enable = bd -> props . power == FB_BLANK_UNBLANK ; <nl> + bool enable = bd -> props . power == FB_BLANK_UNBLANK && <nl> + bd -> props . brightness != 0 ; <nl> panel -> backlight_power ( connector , enable ); <nl> } <nl> } else {
int main ( int argc , char ** argv ) <nl> read_relocs ( fp ); <nl> if ( show_absolute_syms ) { <nl> print_absolute_symbols (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> if ( show_absolute_relocs ) { <nl> print_absolute_relocs (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> emit_relocs ( as_text , use_real_mode ); <nl> + out : <nl> + fclose ( fp ); <nl> return 0 ; <nl> }
void drm_helper_resume_force_mode ( struct drm_device * dev ) <nl> int encoder_dpms ; <nl> bool ret ; <nl>  <nl> + drm_modeset_lock_all ( dev ); <nl> list_for_each_entry ( crtc , & dev -> mode_config . crtc_list , head ) { <nl>  <nl> if (! crtc -> enabled ) <nl> void drm_helper_resume_force_mode ( struct drm_device * dev ) <nl>  <nl> /* disable the unused connectors while restoring the modesetting */ <nl> __drm_helper_disable_unused_functions ( dev ); <nl> + drm_modeset_unlock_all ( dev ); <nl> } <nl> EXPORT_SYMBOL ( drm_helper_resume_force_mode ); <nl> 
static int uvc_v4l2_open ( struct file * file ) <nl> if ( atomic_inc_return (& stream -> dev -> users ) == 1 ) { <nl> ret = uvc_status_start ( stream -> dev ); <nl> if ( ret < 0 ) { <nl> - usb_autopm_put_interface ( stream -> dev -> intf ); <nl> atomic_dec (& stream -> dev -> users ); <nl> + usb_autopm_put_interface ( stream -> dev -> intf ); <nl> kfree ( handle ); <nl> return ret ; <nl> }
static int asoc_simple_card_probe ( struct platform_device * pdev ) <nl> snd_soc_card_set_drvdata ( card , priv ); <nl>  <nl> ret = devm_snd_soc_register_card ( dev , card ); <nl> - if ( ret >= 0 ) <nl> - return ret ; <nl> + if ( ret < 0 ) <nl> + goto err ; <nl> + <nl> + return 0 ; <nl> err : <nl> asoc_simple_card_clean_reference ( card ); <nl> 
# define USB_REQ_LOOPBACK_DATA_READ 0x16 <nl> # define USB_REQ_SET_INTERFACE_DS 0x17 <nl>  <nl> +/* specific requests for USB Power Delivery */ <nl> +# define USB_REQ_GET_PARTNER_PDO 20 <nl> +# define USB_REQ_GET_BATTERY_STATUS 21 <nl> +# define USB_REQ_SET_PDO 22 <nl> +# define USB_REQ_GET_VDM 23 <nl> +# define USB_REQ_SEND_VDM 24 <nl> + <nl> /* The Link Power Management ( LPM ) ECN defines USB_REQ_TEST_AND_SET command , <nl> * used by hubs to put ports into a new L1 suspend state , except that it <nl> * forgot to define its number ...
static int hpsa_get_pdisk_of_ioaccel2 ( struct ctlr_info * h , <nl>  <nl> /* Get the list of physical devices */ <nl> physicals = kzalloc ( reportsize , GFP_KERNEL ); <nl> + if ( physicals == NULL ) <nl> + return 0 ; <nl> if ( hpsa_scsi_do_report_phys_luns ( h , ( struct ReportLUNdata *) physicals , <nl> reportsize , extended )) { <nl> dev_err (& h -> pdev -> dev ,
static int mem_cgroup_hierarchical_reclaim ( struct mem_cgroup * root_mem , <nl> ret = try_to_free_mem_cgroup_pages ( root_mem , gfp_mask , noswap ); <nl> if ( mem_cgroup_check_under_limit ( root_mem )) <nl> return 0 ; <nl> + if (! root_mem -> use_hierarchy ) <nl> + return ret ; <nl>  <nl> next_mem = mem_cgroup_get_first_node ( root_mem ); <nl> 
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static void gb_svc_remove_modules ( struct gb_svc * svc ) <nl>  <nl> void gb_svc_del ( struct gb_svc * svc ) <nl> { <nl> - gb_connection_disable ( svc -> connection ); <nl> + gb_connection_disable_rx ( svc -> connection ); <nl>  <nl> /* <nl> * The SVC device and input device may have been registered <nl> void gb_svc_del ( struct gb_svc * svc ) <nl> flush_workqueue ( svc -> wq ); <nl>  <nl> gb_svc_remove_modules ( svc ); <nl> + <nl> + gb_connection_disable ( svc -> connection ); <nl> } <nl>  <nl> void gb_svc_put ( struct gb_svc * svc )
static int pxad_probe ( struct platform_device * op ) <nl> pdev -> slave . dst_addr_widths = widths ; <nl> pdev -> slave . directions = BIT ( DMA_MEM_TO_DEV ) | BIT ( DMA_DEV_TO_MEM ); <nl> pdev -> slave . residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR ; <nl> + pdev -> slave . descriptor_reuse = true ; <nl>  <nl> pdev -> slave . dev = & op -> dev ; <nl> ret = pxad_init_dmadev ( op , pdev , dma_channels );
static int tc6393xb_resume ( struct platform_device * dev ) <nl> int ret ; <nl> int i ; <nl>  <nl> - clk_prepare_enable ( tc6393xb -> clk ); <nl> + ret = clk_prepare_enable ( tc6393xb -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = tcpd -> resume ( dev ); <nl> if ( ret )
static s32 ixgbe_reset_hw_X550em ( struct ixgbe_hw * hw ) <nl> hw -> phy . sfp_setup_needed = false ; <nl> } <nl>  <nl> + if ( status == IXGBE_ERR_SFP_NOT_SUPPORTED ) <nl> + return status ; <nl> + <nl> /* Reset PHY */ <nl> if (! hw -> phy . reset_disable && hw -> phy . ops . reset ) <nl> hw -> phy . ops . reset ( hw );
static int cfs_wi_scheduler ( void * arg ) <nl>  <nl> spin_unlock (& sched -> ws_lock ); <nl> rc = wait_event_interruptible_exclusive ( sched -> ws_waitq , <nl> - ! cfs_wi_sched_cansleep ( sched )); <nl> + ! cfs_wi_sched_cansleep ( sched )); <nl> spin_lock (& sched -> ws_lock ); <nl> } <nl> 
static int alloc_reserved_tree_block ( struct btrfs_trans_handle * trans , <nl> ret = btrfs_insert_empty_item ( trans , fs_info -> extent_root , path , <nl> ins , size ); <nl> if ( ret ) { <nl> + btrfs_free_path ( path ); <nl> btrfs_free_and_pin_reserved_extent ( root , ins -> objectid , <nl> root -> nodesize ); <nl> - btrfs_free_path ( path ); <nl> return ret ; <nl> } <nl> 
int amdgpu_device_ip_suspend ( struct amdgpu_device * adev ) <nl> if ( amdgpu_sriov_vf ( adev )) <nl> amdgpu_virt_request_full_gpu ( adev , false ); <nl>  <nl> + /* ungate SMC block powergating */ <nl> + if ( adev -> powerplay . pp_feature & PP_GFXOFF_MASK ) <nl> + amdgpu_device_ip_set_powergating_state ( adev , <nl> + AMD_IP_BLOCK_TYPE_SMC , <nl> + AMD_CG_STATE_UNGATE ); <nl> + <nl> /* ungate SMC block first */ <nl> r = amdgpu_device_ip_set_clockgating_state ( adev , AMD_IP_BLOCK_TYPE_SMC , <nl> AMD_CG_STATE_UNGATE );
batadv_frag_merge_packets ( struct hlist_head * chain , struct sk_buff * skb ) <nl> kfree ( entry ); <nl>  <nl> /* Make room for the rest of the fragments . */ <nl> - if ( pskb_expand_head ( skb_out , 0 , size - skb -> len , GFP_ATOMIC ) < 0 ) { <nl> + if ( pskb_expand_head ( skb_out , 0 , size - skb_out -> len , GFP_ATOMIC ) < 0 ) { <nl> kfree_skb ( skb_out ); <nl> skb_out = NULL ; <nl> goto free ;
static void ni6527_reset ( struct comedi_device * dev ) <nl> /* disable deglitch filters on all channels */ <nl> ni6527_set_filter_enable ( dev , 0 ); <nl>  <nl> + /* disable edge detection */ <nl> + ni6527_set_edge_detection ( dev , 0xffffffff , 0 , 0 ); <nl> + <nl> writeb ( NI6527_CLR_IRQS | NI6527_CLR_RESET_FILT , <nl> mmio + NI6527_CLR_REG ); <nl> writeb ( NI6527_CTRL_DISABLE_IRQS , mmio + NI6527_CTRL_REG );
static int __devinit bq20z75_probe ( struct i2c_client * client , <nl>  <nl> INIT_DELAYED_WORK (& bq20z75_device -> work , bq20z75_delayed_work ); <nl>  <nl> + bq20z75_device -> enable_detection = true ; <nl> + <nl> return 0 ; <nl>  <nl> exit_psupply :
nvkm_pmu_reset ( struct nvkm_pmu * pmu ) <nl> ); <nl>  <nl> /* Reset . */ <nl> - pmu -> func -> reset ( pmu ); <nl> + if ( pmu -> func -> reset ) <nl> + pmu -> func -> reset ( pmu ); <nl>  <nl> /* Wait for IMEM / DMEM scrubbing to be complete . */ <nl> nvkm_msec ( device , 2000 ,
static int of_platform_serial_setup ( struct platform_device * ofdev , <nl> port -> line = ret ; <nl>  <nl> port -> irq = irq_of_parse_and_map ( np , 0 ); <nl> + if (! port -> irq ) { <nl> + ret = - EPROBE_DEFER ; <nl> + goto err_unprepare ; <nl> + } <nl>  <nl> info -> rst = devm_reset_control_get_optional_shared (& ofdev -> dev , NULL ); <nl> if ( IS_ERR ( info -> rst )) {
static int change_memory_common ( unsigned long addr , int numpages , <nl> if (! size ) <nl> return 0 ; <nl>  <nl> - if (! in_range ( start , size , MODULES_VADDR , MODULES_END )) <nl> + if (! in_range ( start , size , MODULES_VADDR , MODULES_END ) && <nl> + ! in_range ( start , size , VMALLOC_START , VMALLOC_END )) <nl> return - EINVAL ; <nl>  <nl> data . set_mask = set_mask ;
EXPORT_SYMBOL ( vprintk_emit ); <nl>  <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> { <nl> - return vprintk_emit ( 0 , LOGLEVEL_DEFAULT , NULL , 0 , fmt , args ); <nl> + return vprintk_func ( fmt , args ); <nl> } <nl> EXPORT_SYMBOL ( vprintk ); <nl> 
static int dgnc_found_board ( struct pci_dev * pdev , int id ) <nl> return - ENOMEM ; <nl>  <nl> /* make a temporary message buffer for the boot messages */ <nl> - brd -> msgbuf_head = kzalloc ( sizeof ( u8 ) * 8192 , GFP_KERNEL ); <nl> + brd -> msgbuf_head = kcalloc ( 8192 , sizeof ( u8 ), GFP_KERNEL ); <nl> brd -> msgbuf = brd -> msgbuf_head ; <nl>  <nl> if (! brd -> msgbuf ) {
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
int wpa_ioctl ( PSDevice pDevice , struct iw_point * p ) <nl> default : <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO " wpa_ioctl : unknown cmd =% d \ n ", <nl> param -> cmd ); <nl> + kfree ( param ); <nl> return - EOPNOTSUPP ; <nl> } <nl> 
int iwl_enqueue_hcmd ( struct iwl_priv * priv , struct iwl_host_cmd * cmd ) <nl> return - EIO ; <nl> } <nl>  <nl> + if (( priv -> ucode_owner == IWL_OWNERSHIP_TM ) && <nl> + !( cmd -> flags & CMD_ON_DEMAND )) { <nl> + IWL_DEBUG_HC ( priv , " tm own the uCode , no regular hcmd send \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> copy_size = sizeof ( out_cmd -> hdr ); <nl> cmd_size = sizeof ( out_cmd -> hdr ); <nl> 
int smb2_handle_negotiate ( struct ksmbd_work * work ) <nl> status ); <nl> rsp -> hdr . Status = status ; <nl> rc = - EINVAL ; <nl> + kfree ( conn -> preauth_info ); <nl> + conn -> preauth_info = NULL ; <nl> goto err_out ; <nl> } <nl>  <nl> rc = init_smb3_11_server ( conn ); <nl> if ( rc < 0 ) { <nl> rsp -> hdr . Status = STATUS_INVALID_PARAMETER ; <nl> + kfree ( conn -> preauth_info ); <nl> + conn -> preauth_info = NULL ; <nl> goto err_out ; <nl> } <nl> 
static loff_t mtd_lseek ( struct file * file , loff_t offset , int orig ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( offset >= 0 && offset < mtd -> size ) <nl> + if ( offset >= 0 && offset <= mtd -> size ) <nl> return file -> f_pos = offset ; <nl>  <nl> return - EINVAL ;
static inline void rt2x00lib_set_if_combinations ( struct rt2x00_dev * rt2x00dev ) <nl> */ <nl> if_limit = & rt2x00dev -> if_limits_ap ; <nl> if_limit -> max = rt2x00dev -> ops -> max_ap_intf ; <nl> - if_limit -> types = BIT ( NL80211_IFTYPE_AP ); <nl> + if_limit -> types = BIT ( NL80211_IFTYPE_AP ) | <nl> + BIT ( NL80211_IFTYPE_MESH_POINT ); <nl>  <nl> /* <nl> * Build up AP interface combinations structure .
EXPORT_SYMBOL ( param_set_copystring ); <nl> int param_get_string ( char * buffer , const struct kernel_param * kp ) <nl> { <nl> const struct kparam_string * kps = kp -> str ; <nl> - return strlcpy ( buffer , kps -> string , kps -> maxlen ); <nl> + return strlcpy ( buffer , kps -> string , PAGE_SIZE ); <nl> } <nl> EXPORT_SYMBOL ( param_get_string ); <nl> 
static int __devinit nmk_gpio_probe ( struct platform_device * dev ) <nl> struct clk * clk ; <nl> int secondary_irq ; <nl> void __iomem * base ; <nl> - int irq_start = - 1 ; <nl> + int irq_start = 0 ; <nl> int irq ; <nl> int ret ; <nl> 
static const struct omap_video_timings tpo_td043_timings = { <nl> static int tpo_td043_power_on ( struct tpo_td043_device * tpo_td043 ) <nl> { <nl> int nreset_gpio = tpo_td043 -> nreset_gpio ; <nl> + int r ; <nl>  <nl> if ( tpo_td043 -> powered_on ) <nl> return 0 ; <nl>  <nl> - regulator_enable ( tpo_td043 -> vcc_reg ); <nl> + r = regulator_enable ( tpo_td043 -> vcc_reg ); <nl> + if ( r != 0 ) <nl> + return r ; <nl>  <nl> /* wait for regulator to stabilize */ <nl> msleep ( 160 );
ath5k_set_key ( struct ieee80211_hw * hw , enum set_key_cmd cmd , <nl> if ( modparam_nohwcrypt ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( sc -> opmode == NL80211_IFTYPE_AP ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> switch ( key -> alg ) { <nl> case ALG_WEP : <nl> case ALG_TKIP :
struct powerdomain * omap_hwmod_get_pwrdm ( struct omap_hwmod * oh ) <nl> c = oh -> slaves [ oh -> _mpu_port_index ]-> _clk ; <nl> } <nl>  <nl> + if (! c -> clkdm ) <nl> + return NULL ; <nl> + <nl> return c -> clkdm -> pwrdm . ptr ; <nl>  <nl> }
static int regcache_default_sync ( struct regmap * map , unsigned int min , <nl> unsigned int val ; <nl> int ret ; <nl>  <nl> - if ( regmap_volatile ( map , reg )) <nl> + if ( regmap_volatile ( map , reg ) || <nl> + ! regmap_writeable ( map , reg )) <nl> continue ; <nl>  <nl> ret = regcache_read ( map , reg , & val );
void radeon_atom_backlight_init ( struct radeon_encoder * radeon_encoder , <nl> u8 backlight_level ; <nl> char bl_name [ 16 ]; <nl>  <nl> + /* Mac laptops with multiple GPUs use the gmux driver for backlight <nl> + * so don ' t register a backlight device <nl> + */ <nl> + if (( rdev -> pdev -> subsystem_vendor == PCI_VENDOR_ID_APPLE ) && <nl> + ( rdev -> pdev -> device == 0x6741 )) <nl> + return ; <nl> + <nl> if (! radeon_encoder -> enc_priv ) <nl> return ; <nl> 
void dasd_int_handler ( struct ccw_device * cdev , unsigned long intparm , <nl> if ( cqr -> status == DASD_CQR_CLEAR_PENDING && <nl> scsw_fctl (& irb -> scsw ) & SCSW_FCTL_CLEAR_FUNC ) { <nl> cqr -> status = DASD_CQR_CLEARED ; <nl> - if ( cqr -> callback_data == DASD_SLEEPON_START_TAG ) <nl> - cqr -> callback_data = DASD_SLEEPON_END_TAG ; <nl> dasd_device_clear_timer ( device ); <nl> wake_up (& dasd_flush_wq ); <nl> - wake_up (& generic_waitq ); <nl> dasd_schedule_device_bh ( device ); <nl> return ; <nl> }
void ping_unhash ( struct sock * sk ) <nl> if ( sk_hashed ( sk )) { <nl> write_lock_bh (& ping_table . lock ); <nl> hlist_nulls_del (& sk -> sk_nulls_node ); <nl> + sk_nulls_node_init (& sk -> sk_nulls_node ); <nl> sock_put ( sk ); <nl> isk -> inet_num = 0 ; <nl> isk -> inet_sport = 0 ;
int iwl_run_init_mvm_ucode ( struct iwl_mvm * mvm , bool read_nvm ) <nl> ret = iwl_nvm_check_version ( mvm -> nvm_data , mvm -> trans ); <nl> WARN_ON ( ret ); <nl>  <nl> + /* Send TX valid antennas before triggering calibrations */ <nl> + ret = iwl_send_tx_ant_cfg ( mvm , mvm -> nvm_data -> valid_tx_ant ); <nl> + if ( ret ) <nl> + goto error ; <nl> + <nl> /* Override the calibrations from TLV and the const of fw */ <nl> iwl_set_default_calib_trigger ( mvm ); <nl> 
static void <nl> mt76x2_phy_adjust_vga_gain ( struct mt76x2_dev * dev ) <nl> { <nl> u32 false_cca ; <nl> - u8 limit = dev -> cal . low_gain > 1 ? 4 : 16 ; <nl> + u8 limit = dev -> cal . low_gain > 0 ? 16 : 4 ; <nl>  <nl> false_cca = FIELD_GET ( MT_RX_STAT_1_CCA_ERRORS , mt76_rr ( dev , MT_RX_STAT_1 )); <nl> if ( false_cca > 800 && dev -> cal . agc_gain_adjust < limit )
event_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_preds ( call ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl>  <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_subsystem_preds ( system ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl> 
static int adis_update_scan_mode_burst ( struct iio_dev * indio_dev , <nl> return - ENOMEM ; <nl>  <nl> adis -> buffer = kzalloc ( burst_length + sizeof ( u16 ), GFP_KERNEL ); <nl> - if (! adis -> buffer ) <nl> + if (! adis -> buffer ) { <nl> + kfree ( adis -> xfer ); <nl> + adis -> xfer = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> tx = adis -> buffer + burst_length ; <nl> tx [ 0 ] = ADIS_READ_REG ( adis -> burst -> reg_cmd );
do_ip_vs_get_ctl ( struct sock * sk , int cmd , void __user * user , int * len ) <nl> { <nl> struct ip_vs_timeout_user t ; <nl>  <nl> + memset (& t , 0 , sizeof ( t )); <nl> __ip_vs_get_timeouts ( net , & t ); <nl> if ( copy_to_user ( user , & t , sizeof ( t )) != 0 ) <nl> ret = - EFAULT ;
static const struct mssr_mod_clk r8a7795_mod_clks [] __initconst = { <nl> DEF_MOD (" scif2 ", 310 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" pcie1 ", 318 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" pcie0 ", 319 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if1 ", 327 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if0 ", 328 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" intc - ap ", 408 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" audmac0 ", 502 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" audmac1 ", 501 , R8A7795_CLK_S3D4 ),
u16 hpi_entity_alloc_and_pack ( const enum e_entity_type type , <nl> if ( hE ) <nl> return hE ; <nl>  <nl> - HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_ROLE ); <nl> + HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_TYPE ); <nl>  <nl> bytes_to_copy = entity_type_to_size [ type ] * item_count ; <nl> total_size = hpi_entity_header_size (* entity ) + bytes_to_copy ;
static enum print_line_t trace_ctxwake_bin ( struct trace_iterator * iter , <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_pid ); <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_prio ); <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_state ); <nl> + SEQ_PUT_FIELD_RET ( s , field -> next_cpu ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_pid ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_prio ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_state );
__alloc_pages_slowpath ( gfp_t gfp_mask , unsigned int order , <nl> if ( p -> flags & PF_MEMALLOC ) <nl> goto nopage ; <nl>  <nl> + /* Avoid allocations with no watermarks from looping endlessly */ <nl> + if ( test_thread_flag ( TIF_MEMDIE ) && !( gfp_mask & __GFP_NOFAIL )) <nl> + goto nopage ; <nl> + <nl> /* Try direct reclaim and then allocating */ <nl> page = __alloc_pages_direct_reclaim ( gfp_mask , order , <nl> zonelist , high_zoneidx ,
static int hwsim_new_radio_nl ( struct sk_buff * msg , struct genl_info * info ) <nl> if ( info -> attrs [ HWSIM_ATTR_REG_CUSTOM_REG ]) { <nl> u32 idx = nla_get_u32 ( info -> attrs [ HWSIM_ATTR_REG_CUSTOM_REG ]); <nl>  <nl> - if ( idx >= ARRAY_SIZE ( hwsim_world_regdom_custom )) <nl> + if ( idx >= ARRAY_SIZE ( hwsim_world_regdom_custom )) { <nl> + kfree ( hwname ); <nl> return - EINVAL ; <nl> + } <nl> param . regd = hwsim_world_regdom_custom [ idx ]; <nl> } <nl> 
EXPORT_SYMBOL_GPL ( memory_add_physaddr_to_nid ); <nl> void __init acpi_numa_slit_init ( struct acpi_table_slit * slit ) <nl> { <nl> } <nl> + <nl> + void __init <nl> + acpi_numa_processor_affinity_init ( struct acpi_srat_cpu_affinity * pa ) <nl> +{ <nl> +} <nl> # endif
struct gpio_desc * devm_get_gpiod_from_child ( struct device * dev , <nl> suffixes [ i ]); <nl>  <nl> desc = fwnode_get_named_gpiod ( child , prop_name ); <nl> - if (! IS_ERR ( desc ) || ( PTR_ERR ( desc ) == - EPROBE_DEFER )) <nl> + if (! IS_ERR ( desc ) || ( PTR_ERR ( desc ) != - ENOENT )) <nl> break ; <nl> } <nl> if ( IS_ERR ( desc )) {
static void ilk_pipe_wm_get_hw_state ( struct drm_crtc * crtc ) <nl> if ( IS_HASWELL ( dev ) || IS_BROADWELL ( dev )) <nl> hw -> wm_linetime [ pipe ] = I915_READ ( PIPE_WM_LINETIME ( pipe )); <nl>  <nl> + memset ( active , 0 , sizeof (* active )); <nl> + <nl> active -> pipe_enabled = intel_crtc -> active ; <nl>  <nl> if ( active -> pipe_enabled ) {
static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> ch . band = BRCMU_CHAN_BAND_2G ; <nl> ch . bw = BRCMU_CHAN_BW_40 ; <nl> + ch . sb = BRCMU_CHAN_SB_NONE ; <nl> ch . chnum = 0 ; <nl> cfg -> d11inf . encchspec (& ch ); <nl>  <nl> static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> brcmf_update_bw40_channel_flag (& band -> channels [ j ], & ch ); <nl> } <nl> + kfree ( pbuf ); <nl> } <nl> return err ; <nl> }
static int prepare_vmcs02 ( struct kvm_vcpu * vcpu , struct vmcs12 * vmcs12 , <nl> if ( exec_control & CPU_BASED_TPR_SHADOW ) { <nl> vmcs_write64 ( VIRTUAL_APIC_PAGE_ADDR , - 1ull ); <nl> vmcs_write32 ( TPR_THRESHOLD , vmcs12 -> tpr_threshold ); <nl> + } else { <nl> +# ifdef CONFIG_X86_64 <nl> + exec_control |= CPU_BASED_CR8_LOAD_EXITING | <nl> + CPU_BASED_CR8_STORE_EXITING ; <nl> +# endif <nl> } <nl>  <nl> /*
static int fsl_ifc_chip_init ( struct fsl_ifc_mtd * priv ) <nl> chip -> ecc . algo = NAND_ECC_HAMMING ; <nl> } <nl>  <nl> - if ( ctrl -> version == FSL_IFC_VERSION_1_1_0 ) <nl> + if ( ctrl -> version >= FSL_IFC_VERSION_1_1_0 ) <nl> fsl_ifc_sram_init ( priv ); <nl>  <nl> return 0 ;
static int dell_wmi_events_set_enabled ( bool enable ) <nl> int ret ; <nl>  <nl> buffer = kzalloc ( sizeof ( struct calling_interface_buffer ), GFP_KERNEL ); <nl> + if (! buffer ) <nl> + return - ENOMEM ; <nl> buffer -> cmd_class = CLASS_INFO ; <nl> buffer -> cmd_select = SELECT_APP_REGISTRATION ; <nl> buffer -> input [ 0 ] = 0x10000 ;
static int __devinit cas_init_one ( struct pci_dev * pdev , <nl> INIT_WORK (& cp -> reset_task , cas_reset_task ); <nl>  <nl> /* Default link parameters */ <nl> - if ( link_mode >= 0 && link_mode <= 6 ) <nl> + if ( link_mode >= 0 && link_mode < 6 ) <nl> cp -> link_cntl = link_modes [ link_mode ]; <nl> else <nl> cp -> link_cntl = BMCR_ANENABLE ;
void radeon_compute_pll_avivo ( struct radeon_pll * pll , <nl>  <nl> /* avoid high jitter with small fractional dividers */ <nl> if ( pll -> flags & RADEON_PLL_USE_FRAC_FB_DIV && ( fb_div % 10 )) { <nl> - fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 60 ); <nl> + fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 50 ); <nl> if ( fb_div < fb_div_min ) { <nl> unsigned tmp = DIV_ROUND_UP ( fb_div_min , fb_div ); <nl> fb_div *= tmp ;
static struct snd_soc_dai_link da8xx_evm_dai = { <nl> . stream_name = " AIC3X ", <nl> . cpu_dai_name = " davinci - mcasp . 0 ", <nl> . codec_dai_name = " tlv320aic3x - hifi ", <nl> - . codec_name = " tlv320aic3x - codec . 0 - 001a ", <nl> + . codec_name = " tlv320aic3x - codec . 1 - 0018 ", <nl> . platform_name = " davinci - pcm - audio ", <nl> . init = evm_aic3x_init , <nl> . ops = & evm_ops ,
static int sh_pfc_dt_node_to_map ( struct pinctrl_dev * pctldev , <nl> for_each_child_of_node ( np , child ) { <nl> ret = sh_pfc_dt_subnode_to_map ( pctldev , child , map , num_maps , <nl> & index ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + of_node_put ( child ); <nl> goto done ; <nl> + } <nl> } <nl>  <nl> /* If no mapping has been found in child nodes try the config node . */
mlxsw_sp_lpm_tree_get ( struct mlxsw_sp * mlxsw_sp , <nl>  <nl> for ( i = 0 ; i < MLXSW_SP_LPM_TREE_COUNT ; i ++) { <nl> lpm_tree = & mlxsw_sp -> router . lpm_trees [ i ]; <nl> - if ( lpm_tree -> proto == proto && <nl> + if ( lpm_tree -> ref_count != 0 && <nl> + lpm_tree -> proto == proto && <nl> mlxsw_sp_prefix_usage_eq (& lpm_tree -> prefix_usage , <nl> prefix_usage )) <nl> goto inc_ref_count ;
void blk_mq_wake_waiters ( struct request_queue * q ) <nl> queue_for_each_hw_ctx ( q , hctx , i ) <nl> if ( blk_mq_hw_queue_mapped ( hctx )) <nl> blk_mq_tag_wakeup_all ( hctx -> tags , true ); <nl> + <nl> + /* <nl> + * If we are called because the queue has now been marked as <nl> + * dying , we need to ensure that processes currently waiting on <nl> + * the queue are notified as well . <nl> + */ <nl> + wake_up_all (& q -> mq_freeze_wq ); <nl> } <nl>  <nl> bool blk_mq_can_queue ( struct blk_mq_hw_ctx * hctx )
static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
static void kvm_vcpu_init ( struct kvm_vcpu * vcpu , struct kvm * kvm , unsigned id ) <nl>  <nl> static void kvm_vcpu_destroy ( struct kvm_vcpu * vcpu ) <nl> { <nl> - kvm_dirty_ring_free (& vcpu -> dirty_ring ); <nl> kvm_arch_vcpu_destroy ( vcpu ); <nl> + kvm_dirty_ring_free (& vcpu -> dirty_ring ); <nl>  <nl> /* <nl> * No need for rcu_read_lock as VCPU_RUN is the only place that changes
static inline u64 btrfs_ino ( struct inode * inode ) <nl> { <nl> u64 ino = BTRFS_I ( inode )-> location . objectid ; <nl>  <nl> - if ( ino <= BTRFS_FIRST_FREE_OBJECTID ) <nl> + /* <nl> + * ! ino : btree_inode <nl> + * type == BTRFS_ROOT_ITEM_KEY : subvol dir <nl> + */ <nl> + if (! ino || BTRFS_I ( inode )-> location . type == BTRFS_ROOT_ITEM_KEY ) <nl> ino = inode -> i_ino ; <nl> return ino ; <nl> }
ieee80211_deliver_skb ( struct ieee80211_rx_data * rx ) <nl> } <nl>  <nl> if ( xmit_skb ) { <nl> - /* send to wireless media */ <nl> + /* <nl> + * Send to wireless media and increase priority by 256 to <nl> + * keep the received priority instead of reclassifying <nl> + * the frame ( see cfg80211_classify8021d ). <nl> + */ <nl> + xmit_skb -> priority += 256 ; <nl> xmit_skb -> protocol = htons ( ETH_P_802_3 ); <nl> skb_reset_network_header ( xmit_skb ); <nl> skb_reset_mac_header ( xmit_skb );
MODULE_LICENSE (" GPL v2 "); <nl>  <nl> static int debug ; <nl> module_param ( debug , int , 0644 ); <nl> - MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on \ n "); <nl> + MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on "); <nl>  <nl> /* # define MPX_DEBUG */ <nl> 
static void mmc_deselect_cards ( struct mmc_host * host ) <nl>  <nl> static inline void mmc_delay ( unsigned int ms ) <nl> { <nl> - if ( ms < HZ / 1000 ) { <nl> - yield (); <nl> + if ( ms < 1000 / HZ ) { <nl> + cond_resched (); <nl> mdelay ( ms ); <nl> } else { <nl> - msleep_interruptible ( ms ); <nl> + msleep ( ms ); <nl> } <nl> } <nl> 
static int f2fs_write_begin ( struct file * file , struct address_space * mapping , <nl>  <nl> /* check inline_data */ <nl> ipage = get_node_page ( sbi , inode -> i_ino ); <nl> - if ( IS_ERR ( ipage )) <nl> + if ( IS_ERR ( ipage )) { <nl> + err = PTR_ERR ( ipage ); <nl> goto unlock_fail ; <nl> + } <nl>  <nl> set_new_dnode (& dn , inode , ipage , ipage , 0 ); <nl> 
static void cpu_ready_for_interrupts ( void ) <nl> * If we are not in hypervisor mode the job is done once for <nl> * the whole partition in configure_exceptions (). <nl> */ <nl> - if ( early_cpu_has_feature ( CPU_FTR_HVMODE ) && <nl> - early_cpu_has_feature ( CPU_FTR_ARCH_207S )) { <nl> + if ( cpu_has_feature ( CPU_FTR_HVMODE ) && <nl> + cpu_has_feature ( CPU_FTR_ARCH_207S )) { <nl> unsigned long lpcr = mfspr ( SPRN_LPCR ); <nl> mtspr ( SPRN_LPCR , lpcr | LPCR_AIL_3 ); <nl> }
static int __proc_dobitmasks ( void * data , int write , <nl> } else { <nl> rc = cfs_trace_copyin_string ( tmpstr , tmpstrlen , buffer , nob ); <nl> if ( rc < 0 ) { <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl>  <nl> static int __proc_dobitmasks ( void * data , int write , <nl> * mask |= D_EMERG ; <nl> } <nl>  <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl> 
void cec_received_msg ( struct cec_adapter * adap , struct cec_msg * msg ) <nl> if (! valid_la || msg -> len <= 1 ) <nl> return ; <nl>  <nl> + if ( adap -> log_addrs . log_addr_mask == 0 ) <nl> + return ; <nl> + <nl> /* <nl> * Process the message on the protocol level . If is_reply is true , <nl> * then cec_receive_notify () won ' t pass on the reply to the listener ( s )
int iioutils_get_type ( unsigned * is_signed , <nl> ret = - errno ; <nl> printf (" failed to pass scan type description \ n "); <nl> goto error_close_sysfsfp ; <nl> + } else if ( ret != 5 ) { <nl> + ret = - EIO ; <nl> + printf (" scan type description didn ' t match \ n "); <nl> + goto error_close_sysfsfp ; <nl> } <nl> * be = ( endianchar == ' b '); <nl> * bytes = padint / 8 ;
efi_initialize_iomem_resources ( struct resource * code_resource , <nl> if ( md -> attribute & EFI_MEMORY_WP ) { <nl> name = " System ROM "; <nl> flags |= IORESOURCE_READONLY ; <nl> - } else { <nl> + } else if ( md -> attribute == EFI_MEMORY_UC ) <nl> + name = " Uncached RAM "; <nl> + else <nl> name = " System RAM "; <nl> - } <nl> break ; <nl>  <nl> case EFI_ACPI_MEMORY_NVS :
static void tce_buildmulti_pSeriesLP ( struct iommu_table * tbl , long tcenum , <nl> union tce_entry tce , * tcep ; <nl> long l , limit ; <nl>  <nl> - if ( npages == 1 ) <nl> + if ( TCE_PAGE_FACTOR == 0 && npages == 1 ) <nl> return tce_build_pSeriesLP ( tbl , tcenum , npages , uaddr , <nl> direction ); <nl> 
static int ipw_wx_set_retry ( struct net_device * dev , <nl> if (!( wrqu -> retry . flags & IW_RETRY_LIMIT )) <nl> return 0 ; <nl>  <nl> - if ( wrqu -> retry . value < 0 || wrqu -> retry . value > 255 ) <nl> + if ( wrqu -> retry . value < 0 || wrqu -> retry . value >= 255 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& priv -> mutex );
static int get_task_ioprio ( struct task_struct * p ) <nl> if ( ret ) <nl> goto out ; <nl> ret = IOPRIO_PRIO_VALUE ( IOPRIO_CLASS_NONE , IOPRIO_NORM ); <nl> + task_lock ( p ); <nl> if ( p -> io_context ) <nl> ret = p -> io_context -> ioprio ; <nl> + task_unlock ( p ); <nl> out : <nl> return ret ; <nl> }
static void mousevsc_on_channel_callback ( void * context ) <nl> static int mousevsc_connect_to_vsp ( struct hv_device * device ) <nl> { <nl> int ret = 0 ; <nl> - int t ; <nl> + unsigned long t ; <nl> struct mousevsc_dev * input_dev = hv_get_drvdata ( device ); <nl> struct mousevsc_prt_msg * request ; <nl> struct mousevsc_prt_msg * response ;
static int kjournald2 ( void * arg ) <nl> goto loop ; <nl>  <nl> end_loop : <nl> - write_unlock (& journal -> j_state_lock ); <nl> del_timer_sync (& journal -> j_commit_timer ); <nl> journal -> j_task = NULL ; <nl> wake_up (& journal -> j_wait_done_commit ); <nl> jbd_debug ( 1 , " Journal thread exiting .\ n "); <nl> + write_unlock (& journal -> j_state_lock ); <nl> return 0 ; <nl> } <nl> 
static int ath10k_abort_scan ( struct ath10k * ar ) <nl> ret = ath10k_wmi_stop_scan ( ar , & arg ); <nl> if ( ret ) { <nl> ath10k_warn (" could not submit wmi stop scan (% d )\ n ", ret ); <nl> + spin_lock_bh (& ar -> data_lock ); <nl> + ar -> scan . in_progress = false ; <nl> + ath10k_offchan_tx_purge ( ar ); <nl> + spin_unlock_bh (& ar -> data_lock ); <nl> return - EIO ; <nl> } <nl> 
static int img_prl_out_set_fmt ( struct snd_soc_dai * dai , unsigned int fmt ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + pm_runtime_get_sync ( prl -> dev ); <nl> reg = img_prl_out_readl ( prl , IMG_PRL_OUT_CTL ); <nl> reg = ( reg & ~ IMG_PRL_OUT_CTL_EDGE_MASK ) | control_set ; <nl> img_prl_out_writel ( prl , reg , IMG_PRL_OUT_CTL ); <nl> + pm_runtime_put ( prl -> dev ); <nl>  <nl> return 0 ; <nl> }
static int patch_alc269 ( struct hda_codec * codec ) <nl>  <nl> spec = codec -> spec ; <nl> spec -> gen . shared_mic_vref_pin = 0x18 ; <nl> + codec -> power_save_node = 1 ; <nl>  <nl> snd_hda_pick_fixup ( codec , alc269_fixup_models , <nl> alc269_fixup_tbl , alc269_fixups );
static sctp_xmit_t sctp_packet_bundle_auth ( struct sctp_packet * pkt , <nl> /* See if this is an auth chunk we are bundling or if <nl> * auth is already bundled . <nl> */ <nl> - if ( chunk -> chunk_hdr -> type == SCTP_CID_AUTH || pkt -> auth ) <nl> + if ( chunk -> chunk_hdr -> type == SCTP_CID_AUTH || pkt -> has_auth ) <nl> return retval ; <nl>  <nl> /* if the peer did not request this chunk to be authenticated ,
extern int debug_locks_off ( void ); <nl> ({ \ <nl> int __ret = 0 ; \ <nl> \ <nl> - if ( unlikely ( c )) { \ <nl> + if (! oops_in_progress && unlikely ( c )) { \ <nl> if ( debug_locks_off () && ! debug_locks_silent ) \ <nl> WARN_ON ( 1 ); \ <nl> __ret = 1 ; \
static int do_dentry_open ( struct file * f , <nl> return 0 ; <nl>  <nl> cleanup_all : <nl> + if ( WARN_ON_ONCE ( error > 0 )) <nl> + error = - EINVAL ; <nl> fops_put ( f -> f_op ); <nl> if ( f -> f_mode & FMODE_WRITER ) { <nl> put_write_access ( inode );
static int ar9002_hw_calibrate ( struct ath_hw * ah , struct ath9k_channel * chan , <nl> return 0 ; <nl>  <nl> ah -> cal_list_curr = currCal = currCal -> calNext ; <nl> - if ( currCal -> calState == CAL_WAITING ) { <nl> + if ( currCal -> calState == CAL_WAITING ) <nl> ath9k_hw_reset_calibration ( ah , currCal ); <nl> - return 0 ; <nl> - } <nl> + <nl> + return 0 ; <nl> } <nl>  <nl> /* Do NF cal only at longer intervals */
static int check_ptr_alignment ( struct bpf_verifier_env * env , <nl> break ; <nl> case PTR_TO_STACK : <nl> pointer_desc = " stack "; <nl> + /* The stack spill tracking logic in check_stack_write () <nl> + * and check_stack_read () relies on stack accesses being <nl> + * aligned . <nl> + */ <nl> + strict = true ; <nl> break ; <nl> default : <nl> break ;
xfs_inumbers ( <nl> return error ; <nl>  <nl> bcount = MIN ( left , ( int )( PAGE_SIZE / sizeof (* buffer ))); <nl> - buffer = kmem_alloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> + buffer = kmem_zalloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> do { <nl> struct xfs_inobt_rec_incore r ; <nl> int stat ;
int crypto_reportstat ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
static void storvsc_handle_error ( struct vmscsi_request * vm_srb , <nl> do_work = true ; <nl> process_err_fn = storvsc_remove_lun ; <nl> break ; <nl> - case ( SRB_STATUS_ABORTED | SRB_STATUS_AUTOSENSE_VALID ): <nl> - if (( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> + case SRB_STATUS_ABORTED : <nl> + if ( vm_srb -> srb_status & SRB_STATUS_AUTOSENSE_VALID && <nl> + ( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> do_work = true ; <nl> process_err_fn = storvsc_device_scan ; <nl> /*
static int l2tp_ip6_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> lsa -> l2tp_family = AF_INET6 ; <nl> lsa -> l2tp_flowinfo = 0 ; <nl> lsa -> l2tp_scope_id = 0 ; <nl> + lsa -> l2tp_unused = 0 ; <nl> if ( peer ) { <nl> if (! lsk -> peer_conn_id ) <nl> return - ENOTCONN ;
extern void to_tm ( int tim , struct rtc_time * tm ); <nl> extern void tick_broadcast_ipi_handler ( void ); <nl>  <nl> extern void generic_calibrate_decr ( void ); <nl> + extern void hdec_interrupt ( struct pt_regs * regs ); <nl>  <nl> /* Some sane defaults : 125 MHz timebase , 1GHz processor */ <nl> extern unsigned long ppc_proc_freq ;
static int ipmmu_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& mmu -> lock ); <nl> bitmap_zero ( mmu -> ctx , IPMMU_CTX_MAX ); <nl> mmu -> features = of_device_get_match_data (& pdev -> dev ); <nl> + dma_set_mask_and_coherent (& pdev -> dev , DMA_BIT_MASK ( 40 )); <nl>  <nl> /* Map I / O memory and request IRQ . */ <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 );
# define crisv10_mask_irq ( irq_nr ) (* R_VECT_MASK_CLR = 1 << ( irq_nr )); <nl> # define crisv10_unmask_irq ( irq_nr ) (* R_VECT_MASK_SET = 1 << ( irq_nr )); <nl>  <nl> + extern void kgdb_init ( void ); <nl> + extern void breakpoint ( void ); <nl> + <nl> /* don ' t use set_int_vector , it bypasses the linux interrupt handlers . it is <nl> * global just so that the kernel gdb can use it . <nl> */
static int special_clk_ctl_put ( struct snd_kcontrol * kctl , <nl> struct special_params * params = bebob -> maudio_special_quirk ; <nl> int err , id ; <nl>  <nl> - mutex_lock (& bebob -> mutex ); <nl> - <nl> id = uval -> value . enumerated . item [ 0 ]; <nl> if ( id >= ARRAY_SIZE ( special_clk_labels )) <nl> return 0 ; <nl>  <nl> + mutex_lock (& bebob -> mutex ); <nl> + <nl> err = avc_maudio_set_special_clk ( bebob , id , <nl> params -> dig_in_fmt , <nl> params -> dig_out_fmt ,
static ssize_t dm_dp_aux_transfer ( struct drm_dp_aux * aux , <nl> enum ddc_result res ; <nl> ssize_t read_bytes ; <nl>  <nl> + if ( WARN_ON ( msg -> size > 16 )) <nl> + return - E2BIG ; <nl> + <nl> switch ( msg -> request & ~ DP_AUX_I2C_MOT ) { <nl> case DP_AUX_NATIVE_READ : <nl> read_bytes = dal_ddc_service_read_dpcd_data (
static int falcon_mtd_probe ( struct efx_nic * efx ) <nl>  <nl> /* Allocate space for maximum number of partitions */ <nl> parts = kcalloc ( 2 , sizeof (* parts ), GFP_KERNEL ); <nl> + if (! parts ) <nl> + return - ENOMEM ; <nl> n_parts = 0 ; <nl>  <nl> spi = & nic_data -> spi_flash ;
void exit_io_context ( void ) <nl> ioc -> aic -> exit ( ioc -> aic ); <nl> cfq_exit ( ioc ); <nl>  <nl> - put_io_context ( ioc ); <nl> } <nl> + put_io_context ( ioc ); <nl> } <nl>  <nl> struct io_context * alloc_io_context ( gfp_t gfp_flags , int node )
static int crypt_iv_essiv_ctr ( struct crypt_config * cc , struct dm_target * ti , <nl>  <nl> if ( err ) { <nl> ti -> error = " Error calculating hash in ESSIV "; <nl> + kfree ( salt ); <nl> return err ; <nl> } <nl> 
static int mt9t112_init_pll ( const struct i2c_client * client ) <nl> * I2C Master Clock Divider <nl> */ <nl> mt9t112_reg_write ( ret , client , 0x0014 , 0x3046 ); <nl> + mt9t112_reg_write ( ret , client , 0x0016 , 0x0400 ); /* JPEG initialization workaround */ <nl> mt9t112_reg_write ( ret , client , 0x0022 , 0x0190 ); <nl> mt9t112_reg_write ( ret , client , 0x3B84 , 0x0212 ); <nl> 
static int ext2_fill_super ( struct super_block * sb , void * data , int silent ) <nl> if ( EXT2_INODE_SIZE ( sb ) == 0 ) <nl> goto cantfind_ext2 ; <nl> sbi -> s_inodes_per_block = sb -> s_blocksize / EXT2_INODE_SIZE ( sb ); <nl> - if ( sbi -> s_inodes_per_block == 0 ) <nl> + if ( sbi -> s_inodes_per_block == 0 || sbi -> s_inodes_per_group == 0 ) <nl> goto cantfind_ext2 ; <nl> sbi -> s_itb_per_group = sbi -> s_inodes_per_group / <nl> sbi -> s_inodes_per_block ;
int mwifiex_del_mgmt_ies ( struct mwifiex_private * priv ) <nl> ar_ie , & priv -> assocresp_idx ); <nl>  <nl> done : <nl> + kfree ( gen_ie ); <nl> kfree ( beacon_ie ); <nl> kfree ( pr_ie ); <nl> kfree ( ar_ie );
void task_tick_numa ( struct rq * rq , struct task_struct * curr ) <nl> now = curr -> se . sum_exec_runtime ; <nl> period = ( u64 ) curr -> numa_scan_period * NSEC_PER_MSEC ; <nl>  <nl> - if ( now - curr -> node_stamp > period ) { <nl> + if ( now > curr -> node_stamp + period ) { <nl> if (! curr -> node_stamp ) <nl> curr -> numa_scan_period = task_scan_min ( curr ); <nl> curr -> node_stamp += period ;
void snd_seq_device_load_drivers ( void ) <nl> flush_work (& autoload_work ); <nl> } <nl> EXPORT_SYMBOL ( snd_seq_device_load_drivers ); <nl> +# define cancel_autoload_drivers () cancel_work_sync (& autoload_work ) <nl> # else <nl> # define queue_autoload_drivers () /* NOP */ <nl> +# define cancel_autoload_drivers () /* NOP */ <nl> # endif <nl>  <nl> /* <nl> static int snd_seq_device_dev_free ( struct snd_device * device ) <nl> { <nl> struct snd_seq_device * dev = device -> device_data ; <nl>  <nl> + cancel_autoload_drivers (); <nl> put_device (& dev -> dev ); <nl> return 0 ; <nl> }
xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
cpufreq_stat_notifier_trans ( struct notifier_block * nb , unsigned long val , <nl> return 0 ; <nl> } <nl>  <nl> - static int __cpuinit cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> + static int cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> unsigned long action , void * hcpu ) <nl> { <nl> unsigned int cpu = ( unsigned long ) hcpu ;
static int create_fixed_stream_quirk ( struct snd_usb_audio * chip , <nl> } <nl> alts = & iface -> altsetting [ fp -> altset_idx ]; <nl> altsd = get_iface_desc ( alts ); <nl> + if ( altsd -> bNumEndpoints < 1 ) { <nl> + kfree ( fp ); <nl> + kfree ( rate_table ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> fp -> protocol = altsd -> bInterfaceProtocol ; <nl>  <nl> if ( fp -> datainterval == 0 )
static int log_one_block ( struct log_writes_c * lc , <nl> goto out ; <nl> sector ++; <nl>  <nl> + atomic_inc (& lc -> io_blocks ); <nl> bio = bio_alloc ( GFP_KERNEL , block -> vec_cnt ); <nl> if (! bio ) { <nl> DMERR (" Couldn ' t alloc log bio "); <nl> goto error ; <nl> } <nl> - atomic_inc (& lc -> io_blocks ); <nl> bio -> bi_iter . bi_size = 0 ; <nl> bio -> bi_iter . bi_sector = sector ; <nl> bio -> bi_bdev = lc -> logdev -> bdev ;
static void iwl_req_fw_callback ( const struct firmware * ucode_raw , void * context ) <nl> op -> name , err ); <nl> # endif <nl> } <nl> + kfree ( pieces ); <nl> return ; <nl>  <nl> try_again :
static int __devinit bnx2x_init_one ( struct pci_dev * pdev , <nl> bp = netdev_priv ( dev ); <nl> bp -> msglevel = debug ; <nl>  <nl> + pci_set_drvdata ( pdev , dev ); <nl> + <nl> rc = bnx2x_init_dev ( pdev , dev ); <nl> if ( rc < 0 ) { <nl> free_netdev ( dev ); <nl> return rc ; <nl> } <nl>  <nl> - pci_set_drvdata ( pdev , dev ); <nl> - <nl> rc = bnx2x_init_bp ( bp ); <nl> if ( rc ) <nl> goto init_one_exit ;
static ssize_t gt_max_freq_mhz_show ( struct device * kdev , struct device_attribute <nl> int ret ; <nl>  <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl> - ret = dev_priv -> rps . hw_max * GT_FREQUENCY_MULTIPLIER ; <nl> + ret = dev_priv -> rps . max_delay * GT_FREQUENCY_MULTIPLIER ; <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> return snprintf ( buf , PAGE_SIZE , "% d \ n ", ret );
static int ade7854_set_irq ( struct device * dev , bool enable ) <nl> else <nl> irqen &= ~ BIT ( 17 ); <nl>  <nl> - ret = st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> - <nl> - return ret ; <nl> + return st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> } <nl>  <nl> static int ade7854_initial_setup ( struct iio_dev * indio_dev )
static int __devinit snd_hdspm_create ( struct snd_card * card , <nl> hdspm -> io_type = AES32 ; <nl> hdspm -> card_name = " RME AES32 "; <nl> hdspm -> midiPorts = 2 ; <nl> - } else if (( hdspm -> firmware_rev == 0xd5 ) || <nl> + } else if (( hdspm -> firmware_rev == 0xd2 ) || <nl> (( hdspm -> firmware_rev >= 0xc8 ) && <nl> ( hdspm -> firmware_rev <= 0xcf ))) { <nl> hdspm -> io_type = MADI ;
int snd_midi_event_encode_byte ( struct snd_midi_event * dev , int c , <nl> ev -> type = status_event [ ST_SPECIAL + c - 0xf0 ]. event ; <nl> ev -> flags &= ~ SNDRV_SEQ_EVENT_LENGTH_MASK ; <nl> ev -> flags |= SNDRV_SEQ_EVENT_LENGTH_FIXED ; <nl> - return 1 ; <nl> + return ev -> type != SNDRV_SEQ_EVENT_NONE ; <nl> } <nl>  <nl> spin_lock_irqsave (& dev -> lock , flags );
# include " viosrp . h " <nl>  <nl> # define IBMVFC_NAME " ibmvfc " <nl> -# define IBMVFC_DRIVER_VERSION " 1 . 0 . 3 " <nl> -# define IBMVFC_DRIVER_DATE "( October 28 , 2008 )" <nl> +# define IBMVFC_DRIVER_VERSION " 1 . 0 . 4 " <nl> +# define IBMVFC_DRIVER_DATE "( November 14 , 2008 )" <nl>  <nl> # define IBMVFC_DEFAULT_TIMEOUT 15 <nl> # define IBMVFC_INIT_TIMEOUT 30
int ip6_forward ( struct sk_buff * skb ) <nl> if ( mtu < IPV6_MIN_MTU ) <nl> mtu = IPV6_MIN_MTU ; <nl>  <nl> - if ( skb -> len > mtu ) { <nl> + if ( skb -> len > mtu && ! skb_is_gso ( skb )) { <nl> /* Again , force OUTPUT device used as source address */ <nl> skb -> dev = dst -> dev ; <nl> icmpv6_send ( skb , ICMPV6_PKT_TOOBIG , 0 , mtu );
struct inode { <nl> struct timespec i_atime ; <nl> struct timespec i_mtime ; <nl> struct timespec i_ctime ; <nl> - unsigned int i_blkbits ; <nl> blkcnt_t i_blocks ; <nl> + unsigned int i_blkbits ; <nl> unsigned short i_bytes ; <nl> umode_t i_mode ; <nl> spinlock_t i_lock ; /* i_blocks , i_bytes , maybe i_size */
void btrfs_destroy_workqueue ( struct btrfs_workqueue * wq ) <nl> if ( wq -> high ) <nl> __btrfs_destroy_workqueue ( wq -> high ); <nl> __btrfs_destroy_workqueue ( wq -> normal ); <nl> + kfree ( wq ); <nl> } <nl>  <nl> void btrfs_workqueue_set_max ( struct btrfs_workqueue * wq , int max )
int seq_buf_putmem_hex ( struct seq_buf * s , const void * mem , <nl>  <nl> WARN_ON ( s -> size == 0 ); <nl>  <nl> + BUILD_BUG_ON ( MAX_MEMHEX_BYTES * 2 >= HEX_CHARS ); <nl> + <nl> while ( len ) { <nl> - start_len = min ( len , HEX_CHARS - 1 ); <nl> + start_len = min ( len , MAX_MEMHEX_BYTES ); <nl> # ifdef __BIG_ENDIAN <nl> for ( i = 0 , j = 0 ; i < start_len ; i ++) { <nl> # else
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
static const struct fm10k_stats fm10k_gstrings_pf_stats [] = { <nl>  <nl> static const struct fm10k_stats fm10k_gstrings_mbx_stats [] = { <nl> FM10K_MBX_STAT (" mbx_tx_busy ", tx_busy ), <nl> - FM10K_MBX_STAT (" mbx_tx_oversized ", tx_dropped ), <nl> + FM10K_MBX_STAT (" mbx_tx_dropped ", tx_dropped ), <nl> FM10K_MBX_STAT (" mbx_tx_messages ", tx_messages ), <nl> FM10K_MBX_STAT (" mbx_tx_dwords ", tx_dwords ), <nl> FM10K_MBX_STAT (" mbx_tx_mbmem_pulled ", tx_mbmem_pulled ),
static int qe_ep_enable ( struct usb_ep * _ep , <nl> ep = container_of ( _ep , struct qe_ep , ep ); <nl>  <nl> /* catch various bogus parameters */ <nl> - if (! _ep || ! desc || ep -> ep . desc || _ep -> name == ep_name [ 0 ] || <nl> + if (! _ep || ! desc || _ep -> name == ep_name [ 0 ] || <nl> ( desc -> bDescriptorType != USB_DT_ENDPOINT )) <nl> return - EINVAL ; <nl> 
static int __rds_rdma_map ( struct rds_sock * rs , struct rds_get_mr_args * args , <nl> long i ; <nl> int ret ; <nl>  <nl> - if ( rs -> rs_bound_addr == 0 ) { <nl> + if ( rs -> rs_bound_addr == 0 || ! rs -> rs_transport ) { <nl> ret = - ENOTCONN ; /* XXX not a great errno */ <nl> goto out ; <nl> }
static int ptlrpc_main ( void * arg ) <nl> /* Process all incoming reqs before handling any */ <nl> if ( ptlrpc_server_request_incoming ( svcpt )) { <nl> lu_context_enter (& env -> le_ctx ); <nl> + env -> le_ses = NULL ; <nl> ptlrpc_server_handle_req_in ( svcpt , thread ); <nl> lu_context_exit (& env -> le_ctx ); <nl> 
int inet6_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl> */ <nl> v4addr = LOOPBACK4_IPV6 ; <nl> if (!( addr_type & IPV6_ADDR_MULTICAST )) { <nl> - if (! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> + if (! inet -> transparent && <nl> + ! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> dev , 0 )) { <nl> err = - EADDRNOTAVAIL ; <nl> goto out_unlock ;
struct tracepoint { <nl> do { \ <nl> void ** it_func ; \ <nl> \ <nl> - rcu_read_lock_sched (); \ <nl> + rcu_read_lock_sched_notrace (); \ <nl> it_func = rcu_dereference (( tp )-> funcs ); \ <nl> if ( it_func ) { \ <nl> do { \ <nl> (( void (*)( proto ))(* it_func ))( args ); \ <nl> } while (*(++ it_func )); \ <nl> } \ <nl> - rcu_read_unlock_sched (); \ <nl> + rcu_read_unlock_sched_notrace (); \ <nl> } while ( 0 ) <nl>  <nl> /*
int dm_kcopyd_copy ( struct dm_kcopyd_client * kc , struct dm_io_region * from , <nl> job -> fn = fn ; <nl> job -> context = context ; <nl>  <nl> - if ( job -> source . count < SUB_JOB_SIZE ) <nl> + if ( job -> source . count <= SUB_JOB_SIZE ) <nl> dispatch_job ( job ); <nl>  <nl> else {
void ath10k_htt_tx_free ( struct ath10k_htt * htt ) <nl> { <nl> int size ; <nl>  <nl> + tasklet_kill (& htt -> txrx_compl_task ); <nl> + <nl> idr_for_each (& htt -> pending_tx , ath10k_htt_tx_clean_up_pending , htt -> ar ); <nl> idr_destroy (& htt -> pending_tx ); <nl> 
int __init igafb_init ( void ) <nl> iounmap ( info -> screen_base ); <nl> kfree ( par -> mmap_map ); <nl> kfree ( info ); <nl> + return - ENODEV ; <nl> } <nl>  <nl> # ifdef CONFIG_SPARC
static int onenand_write_ops_nolock ( struct mtd_info * mtd , loff_t to , <nl> } <nl>  <nl> /* Only check verify write turn on */ <nl> - ret = onenand_verify ( mtd , ( u_char *) wbuf , to , thislen ); <nl> + ret = onenand_verify ( mtd , buf , to , thislen ); <nl> if ( ret ) { <nl> printk ( KERN_ERR " onenand_write_ops_nolock : verify failed % d \ n ", ret ); <nl> break ;
unsigned long hugetlb_get_unmapped_area ( struct file * file , unsigned long addr , <nl> { <nl> struct hstate * hstate = hstate_file ( file ); <nl> int mmu_psize = shift_to_mmu_psize ( huge_page_shift ( hstate )); <nl> + <nl> + if (! mmu_huge_psizes [ mmu_psize ]) <nl> + return - EINVAL ; <nl> return slice_get_unmapped_area ( addr , len , flags , mmu_psize , 1 , 0 ); <nl> } <nl> 
static int beagle_twl_gpio_setup ( struct device * dev , <nl>  <nl> /* TWL4030_GPIO_MAX + 0 == ledA , EHCI nEN_USB_PWR ( out , active low ) */ <nl> gpio_request ( gpio + TWL4030_GPIO_MAX , " nEN_USB_PWR "); <nl> - gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 1 ); <nl> + gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 0 ); <nl>  <nl> /* TWL4030_GPIO_MAX + 1 == ledB , PMU_STAT ( out , active low LED ) */ <nl> gpio_leds [ 2 ]. gpio = gpio + TWL4030_GPIO_MAX + 1 ;
struct vm_area_struct * vb2_get_vma ( struct vm_area_struct * vma ) <nl> { <nl> struct vm_area_struct * vma_copy ; <nl>  <nl> - vma_copy = kmalloc ( sizeof (* vma_copy ), GFP_KERNEL ); <nl> + vma_copy = kmem_cache_alloc ( vm_area_cachep , GFP_KERNEL ); <nl> if ( vma_copy == NULL ) <nl> return NULL ; <nl>  <nl> void vb2_put_vma ( struct vm_area_struct * vma ) <nl> if ( vma -> vm_file ) <nl> fput ( vma -> vm_file ); <nl>  <nl> - kfree ( vma ); <nl> + kmem_cache_free ( vm_area_cachep , vma ); <nl> } <nl> EXPORT_SYMBOL_GPL ( vb2_put_vma ); <nl> 
static struct rpc_task * nfs4_do_unlck ( struct file_lock * fl , <nl> { <nl> struct nfs4_unlockdata * data ; <nl>  <nl> + /* Ensure this is an unlock - when canceling a lock , the <nl> + * canceled lock is passed in , and it won ' t be an unlock . <nl> + */ <nl> + fl -> fl_type = F_UNLCK ; <nl> + <nl> data = nfs4_alloc_unlockdata ( fl , ctx , lsp , seqid ); <nl> if ( data == NULL ) { <nl> nfs_free_seqid ( seqid );
static int yam_siocdevprivate ( struct net_device * dev , struct ifreq * ifr , void __ <nl> ym = memdup_user ( data , sizeof ( struct yamdrv_ioctl_mcs )); <nl> if ( IS_ERR ( ym )) <nl> return PTR_ERR ( ym ); <nl> - if ( ym -> cmd != SIOCYAMSMCS ) <nl> - return - EINVAL ; <nl> - if ( ym -> bitrate > YAM_MAXBITRATE ) { <nl> + if ( ym -> cmd != SIOCYAMSMCS || ym -> bitrate > YAM_MAXBITRATE ) { <nl> kfree ( ym ); <nl> return - EINVAL ; <nl> }
struct platform_device * __init imx_add_mxc_mmc ( <nl> struct resource res [] = { <nl> { <nl> . start = data -> iobase , <nl> - . end = data -> iobase + SZ_4K - 1 , <nl> + . end = data -> iobase + data -> iosize - 1 , <nl> . flags = IORESOURCE_MEM , <nl> }, { <nl> . start = data -> irq ,
int bpf_jit_enable __read_mostly ; <nl>  <nl> static void bpf_jit_fill_ill_insns ( void * area , unsigned int size ) <nl> { <nl> - int * p = area ; <nl> - <nl> - /* Fill whole space with trap instructions */ <nl> - while ( p < ( int *)(( char *) area + size )) <nl> - * p ++ = BREAKPOINT_INSTRUCTION ; <nl> + memset32 ( area , BREAKPOINT_INSTRUCTION , size / 4 ); <nl> } <nl>  <nl> static inline void bpf_flush_icache ( void * start , void * end )
static int tile_net_poll ( struct napi_struct * napi , int budget ) <nl> struct info_mpipe * info_mpipe = <nl> container_of ( napi , struct info_mpipe , napi ); <nl>  <nl> + if ( budget <= 0 ) <nl> + goto done ; <nl> + <nl> instance = info_mpipe -> instance ; <nl> while (( n = gxio_mpipe_iqueue_try_peek ( <nl> & info_mpipe -> iqueue ,
static int acl_permission_check ( struct inode * inode , int mask ) <nl> if ( current_user_ns () != inode_userns ( inode )) <nl> goto other_perms ; <nl>  <nl> - if ( current_fsuid () == inode -> i_uid ) <nl> + if ( likely ( current_fsuid () == inode -> i_uid )) <nl> mode >>= 6 ; <nl> else { <nl> if ( IS_POSIXACL ( inode ) && ( mode & S_IRWXG )) {
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
static void nvmet_execute_identify_ctrl ( struct nvmet_req * req ) <nl> id -> vid = 0 ; <nl> id -> ssvid = 0 ; <nl>  <nl> + memset ( id -> sn , ' ', sizeof ( id -> sn )); <nl> bin2hex ( id -> sn , & ctrl -> subsys -> serial , <nl> min ( sizeof ( ctrl -> subsys -> serial ), sizeof ( id -> sn ) / 2 )); <nl> memcpy_and_pad ( id -> mn , sizeof ( id -> mn ), model , sizeof ( model ) - 1 , ' ');
static struct obstack obstack_for_string ; <nl> # define STRING_1GROW ( Char ) \ <nl> obstack_1grow (& obstack_for_string , Char ) <nl>  <nl> -# define STRING_FREE () \ <nl> +# ifdef NDEBUG <nl> +# define STRING_FREE () \ <nl> obstack_free (& obstack_for_string , last_string ) <nl> +# else <nl> +# define STRING_FREE () \ <nl> + do { \ <nl> + obstack_free (& obstack_for_string , last_string ); \ <nl> + last_string = NULL ; \ <nl> + } while ( 0 ) <nl> +# endif <nl>  <nl> # endif
static SQInteger thread_call ( HSQUIRRELVM v ) <nl> SQObjectPtr o = stack_get ( v , 1 ); <nl> if ( sq_type ( o ) == OT_THREAD ) { <nl> SQInteger nparams = sq_gettop ( v ); <nl> + sq_reservestack ( _thread ( o ), nparams + 3 ); <nl> _thread ( o )-> Push ( _thread ( o )-> _roottable ); <nl> for ( SQInteger i = 2 ; i <( nparams + 1 ); i ++) <nl> sq_move ( _thread ( o ), v , i );
MultiPartInputFile :: initialize () <nl> // Perform usual check on headers . <nl> // <nl>  <nl> + if ( _data -> _headers . size () == 0 ) <nl> + { <nl> + throw IEX_NAMESPACE :: ArgExc (" Files must contain at least one header "); <nl> + } <nl> + <nl> for ( size_t i = 0 ; i < _data -> _headers . size (); i ++) <nl> { <nl> //
crun_command_exec ( struct crun_global_arguments * global_args , int argc , char ** a <nl> capabilities -> effective = exec_options . cap ; <nl> capabilities -> effective_len = exec_options . cap_size ; <nl>  <nl> - capabilities -> inheritable = dup_array ( exec_options . cap , exec_options . cap_size ); <nl> - capabilities -> inheritable_len = exec_options . cap_size ; <nl> + capabilities -> inheritable = NULL ; <nl> + capabilities -> inheritable_len = 0 ; <nl>  <nl> capabilities -> bounding = dup_array ( exec_options . cap , exec_options . cap_size ); <nl> capabilities -> bounding_len = exec_options . cap_size ;
DU_getStringDOElement ( DcmItem * obj , DcmTagKey t , char * s , size_t bufsize ) <nl> s [ 0 ] = '\ 0 '; <nl> } else { <nl> ec = elem -> getString ( aString ); <nl> - OFStandard :: strlcpy ( s , aString , bufsize ); <nl> + if ( ec == EC_Normal ) <nl> + OFStandard :: strlcpy ( s , aString , bufsize ); <nl> } <nl> } <nl> return ( ec == EC_Normal );
vhost_user_check_and_alloc_queue_pair ( struct virtio_net * dev , <nl> case VHOST_USER_SET_VRING_ADDR : <nl> vring_idx = ctx -> msg . payload . addr . index ; <nl> break ; <nl> + case VHOST_USER_SET_INFLIGHT_FD : <nl> + vring_idx = ctx -> msg . payload . inflight . num_queues - 1 ; <nl> + break ; <nl> default : <nl> return 0 ; <nl> }
class McAsciiParserBase { <nl> const char * posStart , <nl> const char * posEnd ); <nl>  <nl> + // limit the value size . <nl> + static constexpr uint32_t maxValueBytes = 1 * 1024 * 1024 * 1024 ; // 1GB <nl> + <nl> std :: string currentErrorDescription_ ; <nl>  <nl> uint64_t currentUInt_ { 0 };
BaseType_t xQueueGenericReset ( QueueHandle_t xQueue , <nl> /* Check for multiplication overflow . */ <nl> configASSERT ( ( uxItemSize == 0 ) || ( uxQueueLength == ( xQueueSizeInBytes / uxItemSize ) ) ); <nl>  <nl> + /* Check for addition overflow . */ <nl> + configASSERT ( ( sizeof ( Queue_t ) + xQueueSizeInBytes ) > xQueueSizeInBytes ); <nl> + <nl> /* Allocate the queue and storage area . Justification for MISRA <nl> * deviation as follows : pvPortMalloc () always ensures returned memory <nl> * blocks are aligned per the requirements of the MCU stack . In this case
hermesBuiltinApply ( void *, Runtime * runtime , NativeArgs args ) { <nl>  <nl> ScopedNativeCallFrame newFrame { <nl> runtime , len , * fn , isConstructor , thisVal . getHermesValue ()}; <nl> + if ( LLVM_UNLIKELY ( newFrame . overflowed ())) <nl> + return runtime -> raiseStackOverflow ( Runtime :: StackOverflowKind :: NativeStack ); <nl> + <nl> for ( uint32_t i = 0 ; i < len ; ++ i ) { <nl> newFrame -> getArgRef ( i ) = argArray -> at ( runtime , i ); <nl> }
gnutls_ocsp_resp_check_crt ( gnutls_ocsp_resp_t resp , <nl> gnutls_assert (); <nl> goto cleanup ; <nl> } <nl> + cserial . size = t ; <nl>  <nl> if ( rserial . size != cserial . size <nl> || memcmp ( cserial . data , rserial . data , rserial . size ) != 0 ) {
_gnutls_x509_dn_to_string ( const char * oid , void * value , <nl> if ( ret < 0 ) { <nl> gnutls_assert (); <nl> gnutls_free ( str -> data ); <nl> + str -> data = NULL ; <nl> return ret ; <nl> } <nl> str -> size = size ;
int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl>  <nl> if ( ret < 0 && ret != GNUTLS_E_REQUESTED_DATA_NOT_AVAILABLE ) { <nl> gnutls_assert (); <nl> - gnutls_free ( san . data ); <nl> goto cleanup ; <nl> } <nl> 
lzw_result lzw_decode ( struct lzw_ctx * ctx , <nl> /* Code is invalid */ <nl> return LZW_BAD_CODE ; <nl>  <nl> + } else if ( code_new >= 1 << LZW_CODE_MAX ) { <nl> + /* Don ' t access out of bound */ <nl> + return LZW_BAD_CODE ; <nl> + <nl> } else if ( code_new < current_entry ) { <nl> /* Code is in table */ <nl> code_out = code_new ;
static int prctl_set_vma_anon_name ( unsigned long start , unsigned long end , <nl> tmp = end ; <nl>  <nl> /* Here vma -> vm_start <= start < tmp <= ( end | vma -> vm_end ). */ <nl> - error = prctl_update_vma_anon_name ( vma , & prev , start , end , <nl> + error = prctl_update_vma_anon_name ( vma , & prev , start , tmp , <nl> ( const char __user *) arg ); <nl> if ( error ) <nl> return error ;
static int stszin ( int size ) <nl> u32in (); <nl> // Number of entries <nl> mp4config . frame . ents = u32in (); <nl> - // fixme : check atom size <nl> + <nl> + if (!( mp4config . frame . ents + 1 )) <nl> + return ERR_FAIL ; <nl> + <nl> mp4config . frame . data = malloc ( sizeof (* mp4config . frame . data ) <nl> * ( mp4config . frame . ents + 1 )); <nl> 
static bool check_allocations ( ASS_Shaper * shaper , size_t new_size ) <nl> ! ASS_REALLOC_ARRAY ( shaper -> emblevels , new_size ) || <nl> ! ASS_REALLOC_ARRAY ( shaper -> cmap , new_size )) <nl> return false ; <nl> + shaper -> n_glyphs = new_size ; <nl> } <nl> return true ; <nl> }
 <nl> # include " idn2 . h " <nl>  <nl> +# include < sys / types . h > <nl> # include < stdbool . h > <nl>  <nl> # include " bidi . h " <nl> static bool <nl> _isBidi ( const uint32_t * label , size_t llen ) <nl> { <nl> - while ( llen -- > 0 ) { <nl> + for (; ( ssize_t ) llen > 0 ; llen --) { <nl> int bc = uc_bidi_category (* label ++); <nl>  <nl> if ( bc == UC_BIDI_R || bc == UC_BIDI_AL || bc == UC_BIDI_AN )
static bool env_var_checked_p ; <nl>  <nl> # define FIELD_2DD_VECTOR ( nam , size , dxf ) \ <nl> OVERFLOW_CHECK ( nam , _obj -> size ) \ <nl> - FIELD_2RD ( nam [ 0 ], dxf ); \ <nl> + if ( _obj -> size ) \ <nl> + FIELD_2RD ( nam [ 0 ], dxf ); \ <nl> for ( vcount = 1 ; vcount < ( BITCODE_BL ) _obj -> size ; vcount ++) \ <nl> { \ <nl> FIELD_2DD ( nam [ vcount ], FIELD_VALUE ( nam [ vcount - 1 ]. x ), \
parserule ( struct scanner * s , struct environment * env ) <nl> var = scanname ( s ); <nl> parselet ( s , & val ); <nl> ruleaddvar ( r , var , val ); <nl> + if (! val ) <nl> + continue ; <nl> if ( strcmp ( var , " command ") == 0 ) <nl> hascommand = true ; <nl> else if ( strcmp ( var , " rspfile ") == 0 )
static int cbor2json ( OSCTXT * pCborCtxt , OSCTXT * pJsonCtxt ) <nl> case OSRTCBOR_UTF8STR : { <nl> OSUTF8CHAR * utf8str ; <nl> ret = rtCborDecDynUTF8Str ( pCborCtxt , ub , ( char **)& utf8str ); <nl> + if ( 0 != ret ) return LOG_RTERR ( pCborCtxt , ret ); <nl>  <nl> ret = rtJsonEncStringValue ( pJsonCtxt , utf8str ); <nl> rtxMemFreePtr ( pCborCtxt , utf8str );
int update_timestamp ( const char * key , const struct efi_time * timestamp , char * la <nl> static uint64_t unpack_timestamp ( const struct efi_time * timestamp ) <nl> { <nl> uint64_t val = 0 ; <nl> - uint16_t year = le32_to_cpu ( timestamp -> year ); <nl> + uint16_t year = le16_to_cpu ( timestamp -> year ); <nl>  <nl> /* pad1 , nanosecond , timezone , daylight and pad2 are meant to be zero */ <nl> val |= (( uint64_t ) timestamp -> pad1 & 0xFF ) << 0 ;
static void DetectRunCleanup ( DetectEngineThreadCtx * det_ctx , <nl>  <nl> if ( pflow != NULL ) { <nl> /* update inspected tracker for raw reassembly */ <nl> - if ( p -> proto == IPPROTO_TCP && pflow -> protoctx != NULL ) { <nl> + if ( p -> proto == IPPROTO_TCP && pflow -> protoctx != NULL && <nl> + ( p -> flags & PKT_STREAM_EST )) <nl> + { <nl> StreamReassembleRawUpdateProgress ( pflow -> protoctx , p , <nl> det_ctx -> raw_stream_progress ); <nl> 
njs_module_path ( njs_vm_t * vm , const njs_str_t * dir , njs_module_info_t * info ) <nl> length = info -> name . length ; <nl>  <nl> if ( dir != NULL ) { <nl> - length = dir -> length ; <nl> + length += dir -> length ; <nl>  <nl> if ( length == 0 ) { <nl> return NJS_DECLINED ;
njs_function_frame_save ( njs_vm_t * vm , njs_frame_t * frame , u_char * pc ) <nl> njs_native_frame_t * active , * native ; <nl>  <nl> * frame = * vm -> active_frame ; <nl> + <nl> frame -> previous_active_frame = NULL ; <nl>  <nl> native = & frame -> native ; <nl> + native -> size = 0 ; <nl> + native -> free = NULL ; <nl> + native -> free_size = 0 ; <nl>  <nl> active = & vm -> active_frame -> native ; <nl> value_count = njs_function_frame_value_count ( active );
static char * clean_path ( char * path ) <nl> char * ch ; <nl> char * ch2 ; <nl> char * str ; <nl> - str = xmalloc ( strlen ( path )); <nl> + str = xmalloc ( strlen ( path ) + 1 ); <nl> ch = path ; <nl> ch2 = str ; <nl> while ( true ) {
static const unsigned char * parse_object ( cJSON * item , const unsigned char * value <nl> fail : <nl> if ( item -> child != NULL ) <nl> { <nl> - cJSON_Delete ( child ); <nl> + cJSON_Delete ( item -> child ); <nl> item -> child = NULL ; <nl> } <nl> 
int delete_sdp_line ( struct sip_msg * msg , char * s , struct sdp_stream_cell * str <nl>  <nl> while (* end != '\ n ' && end < ( stream -> body . s + stream -> body . len ) ) <nl> end ++; <nl> - end ++; <nl> + if ( * end == '\ n ') <nl> + end ++; <nl>  <nl> /* delete the entry */ <nl> if ( del_lump ( msg , start - msg -> buf , end - start , 0 ) == NULL )
static int stream_process ( struct sip_msg * msg , struct sdp_stream_cell * cell , <nl> /* when trimming the very last payload , avoid trailing ws */ <nl> if ( cur == lmp -> u . value + lmp -> len ) { <nl> tmp = found . s ; <nl> - while (*(-- tmp ) == ' ') { <nl> + while ( tmp > lmp -> u . value && *(-- tmp ) == ' ') { <nl> found . s --; <nl> found . len ++; <nl> }
parse_again : <nl>  <nl> case ':': <nl> switch ( state ){ <nl> - case F_HOST : <nl> case F_IP6HOST : <nl> state = P_IP6HOST ; <nl> break ;
otError Commissioner :: GeneratePskc ( const char * aPassPhrase , <nl> uint16_t saltLen = 0 ; <nl>  <nl> VerifyOrExit (( strlen ( aPassPhrase ) >= OT_COMMISSIONING_PASSPHRASE_MIN_SIZE ) && <nl> - ( strlen ( aPassPhrase ) <= OT_COMMISSIONING_PASSPHRASE_MAX_SIZE ), <nl> + ( strlen ( aPassPhrase ) <= OT_COMMISSIONING_PASSPHRASE_MAX_SIZE ) && <nl> + ( strlen ( aNetworkName ) <= OT_NETWORK_NAME_MAX_SIZE ), <nl> error = OT_ERROR_INVALID_ARGS ); <nl>  <nl> memset ( salt , 0 , sizeof ( salt ));
WORD32 ih264d_decode_gaps_in_frame_num ( dec_struct_t * ps_dec , <nl>  <nl> ps_cur_slice = ps_dec -> ps_cur_slice ; <nl> ps_pic_params = ps_dec -> ps_cur_pps ; <nl> - ps_cur_slice -> u1_field_pic_flag = 0 ; <nl>  <nl> i4_frame_gaps = 0 ; <nl> ps_dpb_mgr = ps_dec -> ps_dpb_mgr ;
WORD32 ih264d_mark_err_slice_skip ( dec_struct_t * ps_dec , <nl> ih264d_err_pic_dispbuf_mgr ( ps_dec ); <nl> return 0 ; <nl> } <nl> - <nl> + ps_dec -> ps_dpb_cmds -> u1_long_term_reference_flag = 0 ; <nl> if ( prev_slice_err == 1 ) <nl> { <nl> /* first slice - missing / header corruption */
WORD32 ih264d_video_decode ( iv_obj_t * dec_hdl , void * pv_api_ip , void * pv_api_op ) <nl> else <nl> prev_slice_err = 2 ; <nl>  <nl> + if ( ps_dec -> u4_first_slice_in_pic && ( ps_dec -> u2_total_mbs_coded == 0 )) <nl> + prev_slice_err = 1 ; <nl> + <nl> ret1 = ih264d_mark_err_slice_skip ( ps_dec , num_mb_skipped , ps_dec -> u1_nal_unit_type == IDR_SLICE_NAL , ps_dec -> ps_cur_slice -> u2_frame_num , <nl> & temp_poc , prev_slice_err ); <nl> 
WORD32 ih264d_process_intra_mb ( dec_struct_t * ps_dec , <nl> + ( pu1_rem_intra4x4_pred_mode [ u1_cur_sub_block ] <nl> >= i1_intra_pred ); <nl> } <nl> + i1_intra_pred = CLIP3 ( 0 , 8 , i1_intra_pred ); <nl> { <nl> UWORD8 u1_err_code = pu1_intra_err_codes [ i1_intra_pred ]; <nl>  <nl> WORD32 ih264d_process_intra_mb ( dec_struct_t * ps_dec , <nl> + ( pu1_rem_intra4x4_pred_mode [ u1_sub_mb_num ] <nl> >= i1_intra_pred ); <nl> } <nl> + i1_intra_pred = CLIP3 ( 0 , 8 , i1_intra_pred ); <nl> { <nl> UWORD8 u1_err_code = pu1_intra_err_codes [ i1_intra_pred ]; <nl> 
WORD32 ih264d_start_of_pic ( dec_struct_t * ps_dec , <nl> ps_cur_pic -> pu1_col_zero_flag = ( UWORD8 *) ps_col_mv -> pv_col_zero_flag ; <nl> ps_cur_pic -> ps_mv = ( mv_pred_t *) ps_col_mv -> pv_mv ; <nl> ps_dec -> au1_pic_buf_ref_flag [ cur_pic_buf_id ] = 0 ; <nl> - if ( ps_dec -> u1_first_slice_in_stream ) <nl> + <nl> { <nl> /* make first entry of list0 point to cur pic , so that if first Islice is in error , ref pic struct will have valid entries */ <nl> ps_dec -> ps_ref_pic_buf_lx [ 0 ] = ps_dec -> ps_dpb_mgr -> ps_init_dpb [ 0 ];
native_handle * Parcel :: readNativeHandle () const <nl> if ( err != NO_ERROR ) return 0 ; <nl>  <nl> native_handle * h = native_handle_create ( numFds , numInts ); <nl> + if (! h ) { <nl> + return 0 ; <nl> + } <nl> + <nl> for ( int i = 0 ; err == NO_ERROR && i < numFds ; i ++) { <nl> h -> data [ i ] = dup ( readFileDescriptor ()); <nl> if ( h -> data [ i ] < 0 ) err = BAD_VALUE ;
status_t BnGraphicBufferConsumer :: onTransact ( <nl> CHECK_INTERFACE ( IGraphicBufferConsumer , data , reply ); <nl> sp < GraphicBuffer > buffer = new GraphicBuffer (); <nl> data . read (* buffer . get ()); <nl> - int slot ; <nl> + int slot = - 1 ; <nl> int result = attachBuffer (& slot , buffer ); <nl> reply -> writeInt32 ( slot ); <nl> reply -> writeInt32 ( result );
uint8_t rfc_parse_data ( tRFC_MCB * p_mcb , MX_FRAME * p_frame , BT_HDR * p_buf ) { <nl>  <nl> eal = *( p_data )& RFCOMM_EA ; <nl> len = *( p_data )++ >> RFCOMM_SHIFT_LENGTH1 ; <nl> - if ( eal == 0 && p_buf -> len < RFCOMM_CTRL_FRAME_LEN ) { <nl> + if ( eal == 0 && p_buf -> len > RFCOMM_CTRL_FRAME_LEN ) { <nl> len += (*( p_data )++ << RFCOMM_SHIFT_LENGTH2 ); <nl> } else if ( eal == 0 ) { <nl> RFCOMM_TRACE_ERROR (" Bad Length when EAL = 0 : % d ", p_buf -> len );
int CGIFFF DGifSlurp ( CGIFFF GifFileType * GifFile ) <nl> memcpy ( ep -> Bytes , ExtData , ep -> ByteCount * sizeof ( char )); <nl> # else <nl> if ( ExtData == NULL ) break ; <nl> - AddExtensionBlock ( sp , ExtData [ 0 ], ExtData + 1 ); <nl> + AddExtensionBlock (& ext , ExtData [ 0 ], ExtData + 1 ); <nl> + ext . ExtensionBlocks [ ext . ExtensionBlockCount - 1 ]. code = ext_code ; <nl> # endif <nl> } <nl> }
void NuPlayer :: GenericSource :: notifyPreparedAndCleanup ( status_t err ) { <nl> { <nl> Mutex :: Autolock _l ( mDisconnectLock ); <nl> mDataSource . clear (); <nl> + mDrmManagerClient = NULL ; <nl> mCachedSource . clear (); <nl> mHttpSource . clear (); <nl> }
status_t OMXCodec :: allocateBuffersOnPort ( OMX_U32 portIndex ) { <nl>  <nl> for ( OMX_U32 i = 0 ; i < def . nBufferCountActual ; ++ i ) { <nl> sp < IMemory > mem = mDealer [ portIndex ]-> allocate ( def . nBufferSize ); <nl> - CHECK ( mem . get () != NULL ); <nl> + if ( mem == NULL || mem -> pointer () == NULL ) { <nl> + return NO_MEMORY ; <nl> + } <nl>  <nl> BufferInfo info ; <nl> info . mData = NULL ;
int64_t NuPlayer :: GenericSource :: getLastReadPosition () { <nl>  <nl> status_t NuPlayer :: GenericSource :: setBuffers ( <nl> bool audio , Vector < MediaBuffer *> & buffers ) { <nl> - if ( mIsWidevine && ! audio && mVideoTrack . mSource != NULL ) { <nl> + if ( mIsSecure && ! audio && mVideoTrack . mSource != NULL ) { <nl> return mVideoTrack . mSource -> setBuffers ( buffers ); <nl> } <nl> return INVALID_OPERATION ;
static bool MR_primality_test ( UnsignedBigInteger n , const Vector < UnsignedBigInte <nl> return n == 2 ; <nl> } <nl>  <nl> - for ( auto a : tests ) { <nl> + for ( auto & a : tests ) { <nl> // Technically : ASSERT ( 2 <= a && a <= n - 2 ) <nl> ASSERT ( a < n ); <nl> auto x = ModularPower ( a , d , n );
static int MP4_ReadBox_String ( stream_t * p_stream , MP4_Box_t * p_box ) <nl> { <nl> MP4_READBOX_ENTER ( MP4_Box_data_string_t ); <nl>  <nl> + if ( p_box -> i_size < 8 || p_box -> i_size > SIZE_MAX ) <nl> + MP4_READBOX_EXIT ( 0 ); <nl> + <nl> p_box -> data . p_string -> psz_text = malloc ( p_box -> i_size + 1 - 8 ); /* +\ 0 , - name , - size */ <nl> if ( p_box -> data . p_string -> psz_text == NULL ) <nl> MP4_READBOX_EXIT ( 0 );
static bool GetUpdateFile ( update_t * p_update ) <nl> } <nl>  <nl> const int64_t i_read = stream_Size ( p_stream ); <nl> + <nl> + if ( i_read < 0 || i_read >= UINT16_MAX ) <nl> + { <nl> + msg_Err ( p_update -> p_libvlc , " Status file too large "); <nl> + goto error ; <nl> + } <nl> + <nl> psz_update_data = malloc ( i_read + 1 ); /* terminating '\ 0 ' */ <nl> if ( ! psz_update_data ) <nl> goto error ;
class c_single_allocator <nl>  <nl> std :: size_t size () const { return _buf_size ; } <nl>  <nl> - void resize ( std :: size_t new_size_ ) { _buf_size = new_size_ ; } <nl> + // This buffer is fixed , size must not be changed <nl> + void resize ( std :: size_t new_size_ ) { LIBZMQ_UNUSED ( new_size_ ); } <nl>  <nl> private : <nl> std :: size_t _buf_size ;
void csync_daemon_session () <nl> goto conn_without_ssl_ok ; <nl> } <nl> cmd_error = conn_response ( CR_ERR_SSL_EXPECTED ); <nl> + peer = NULL ; <nl> } <nl> conn_without_ssl_ok :; <nl> # endif
resolve_iffeature ( struct lys_iffeature * expr ) <nl> { <nl> int index_e = 0 , index_f = 0 ; <nl>  <nl> - if ( expr -> expr ) { <nl> + if ( expr -> expr && expr -> features [ 0 ]) { <nl> return resolve_iffeature_recursive ( expr , & index_e , & index_f ); <nl> } <nl> return 0 ;
REGISTER_OP (" Dequantize ") <nl> if (! s . ok () && s . code () != error :: NOT_FOUND ) { <nl> return s ; <nl> } <nl> + if ( axis < - 1 ) { <nl> + return errors :: InvalidArgument (" axis should be at least - 1 , got ", <nl> + axis ); <nl> + } <nl> const int minmax_rank = ( axis == - 1 ) ? 0 : 1 ; <nl> TF_RETURN_IF_ERROR ( shape_inference :: UnchangedShape ( c )); <nl> ShapeHandle minmax ;
TfLiteStatus Prepare ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_TYPES_EQ ( context , input -> type , output -> type ); <nl>  <nl> const int block_size = params -> block_size ; <nl> + TF_LITE_ENSURE ( context , block_size > 0 ); <nl> const int input_height = input -> dims -> data [ 1 ]; <nl> const int input_width = input -> dims -> data [ 2 ]; <nl> int output_height = input_height / block_size ;
TfLiteStatus ExpandTensorDim ( TfLiteContext * context , const TfLiteTensor & input , <nl> axis = input_dims . size + 1 + axis ; <nl> } <nl> TF_LITE_ENSURE ( context , axis <= input_dims . size ); <nl> + TF_LITE_ENSURE ( context , axis >= 0 ); <nl>  <nl> TfLiteIntArray * output_dims = TfLiteIntArrayCreate ( input_dims . size + 1 ); <nl> for ( int i = 0 ; i < output_dims -> size ; ++ i ) {
TfLiteStatus Eval ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_OK ( context , <nl> GetOutputSafe ( context , node , kOutputTensor , & output )); <nl>  <nl> + // Prevent division by 0 in the helper <nl> + TF_LITE_ENSURE ( context , NumElements ( params ) > 0 ); <nl> + <nl> switch ( indices -> type ) { <nl> case kTfLiteInt32 : <nl> return EvalGatherNd < int32_t >( context , params , indices , output );
TfLiteStatus Prepare ( TfLiteContext * context , TfLiteNode * node ) { <nl>  <nl> TF_LITE_ENSURE_EQ ( context , NumDimensions ( input ), 4 ); <nl> TF_LITE_ENSURE_EQ ( context , NumDimensions ( filter ), 4 ); <nl> + TF_LITE_ENSURE ( context , params -> dilation_height_factor > 0 ); <nl> + TF_LITE_ENSURE ( context , params -> dilation_width_factor > 0 ); <nl>  <nl> const TfLiteType data_type = input -> type ; <nl> 
TfLiteStatus Eval ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_OK ( context , GetInputSafe ( context , node , 2 , & value )); <nl>  <nl> const int num_rows = SizeOfDimension ( value , 0 ); <nl> + TF_LITE_ENSURE ( context , num_rows != 0 ); <nl> const int row_bytes = value -> bytes / num_rows ; <nl> void * pointer = nullptr ; <nl> DynamicBuffer buf ;
class RaggedTensorToVariantOp : public OpKernel { <nl> batched_ragged_input . mutable_nested_splits ()-> reserve ( <nl> ragged_nested_splits_len ); <nl> for ( int i = 0 ; i < ragged_nested_splits_len ; i ++) { <nl> + OP_REQUIRES ( context , ragged_nested_splits_in [ i ]. dims () == 1 , <nl> + errors :: InvalidArgument (" Requires nested_row_splits [", i , "]", <nl> + " to be rank 1 but is rank ", <nl> + ragged_nested_splits_in [ i ]. dims ())); <nl> batched_ragged_input . append_splits ( ragged_nested_splits_in [ i ]); <nl> } <nl> 
inline void BinaryBroadcastFiveFold ( const ArithmeticParams & unswitched_params , <nl> // We have broadcast y2 * y3 * y4 of input2 data y1 times , and now move on . <nl> input2_data_reset = input2_data_ptr ; <nl> } <nl> - } else { <nl> + } else if ( input1_data_ptr != nullptr ) { <nl> // Special case of y4 == 1 , in which the innermost loop is a single <nl> // element and can be combined with the next ( y3 ) as an inner broadcast . <nl> //
class QuantizeAndDequantizeV2Op : public OpKernel { <nl>  <nl> void Compute ( OpKernelContext * ctx ) override { <nl> const Tensor & input = ctx -> input ( 0 ); <nl> + OP_REQUIRES ( <nl> + ctx , axis_ >= - 1 , <nl> + errors :: InvalidArgument (" Axis must be at least - 1 . Found ", axis_ )); <nl> OP_REQUIRES ( <nl> ctx , ( axis_ == - 1 || axis_ < input . shape (). dims ()), <nl> errors :: InvalidArgument (" Shape must be at least rank ", axis_ + 1 ,
int fmt_mtm_load_song ( song_t * song , slurp_t * fp , unsigned int lflags ) <nl>  <nl> song -> patterns [ pat ] = csf_allocate_pattern ( MAX ( rows , 32 )); <nl> song -> pattern_size [ pat ] = song -> pattern_alloc_size [ pat ] = 64 ; <nl> - tracknote = trackdata [ n ]; <nl> for ( chan = 0 ; chan < 32 ; chan ++) { <nl> slurp_read ( fp , & tmp , 2 ); <nl> tmp = bswapLE16 ( tmp );
static void _jbn_add_item ( JBL_NODE parent , JBL_NODE node ); <nl>  <nl> void iwjson_ftoa ( long double val , char buf [ static IWNUMBUF_SIZE ], size_t * out_len ) { <nl> // TODO : review <nl> - int len = snprintf ( buf , 64 , "%. 8Lf ", val ); <nl> + int len = snprintf ( buf , IWNUMBUF_SIZE , "%. 8Lf ", val ); <nl> if ( len <= 0 ) { <nl> buf [ 0 ] = '\ 0 '; <nl> * out_len = 0 ;
static int tls_new_ciphertext ( struct tls_connection * tls , <nl> if ( is_block_cipher ( cipher ) ) { <nl> pad_len = tls_verify_padding ( tls , last ); <nl> if ( pad_len < 0 ) { <nl> - rc = pad_len ; <nl> - return rc ; <nl> + /* Assume zero padding length to avoid timing attacks */ <nl> + pad_len = 0 ; <nl> } <nl> iob_unput ( last , pad_len ); <nl> len -= pad_len ;
future < fragmented_temporary_buffer > cql_server :: connection :: read_and_decompress_ <nl> if ( ret < 0 ) { <nl> throw std :: runtime_error (" CQL frame LZ4 uncompression failure "); <nl> } <nl> - return out . size (); <nl> + if ( ret != out . size ()) { <nl> + throw std :: runtime_error (" Malformed CQL frame - provided uncompressed size different than real uncompressed size "); <nl> + } <nl> + return static_cast < size_t >( ret ); <nl> }); <nl> on_compression_buffer_use (); <nl> return uncomp ;
namespace drachtio { <nl> if ( complete ) { <nl> m_os . flush () ; <nl> m_sipMessage = m_os . str () ; <nl> - m_sipMessage . resize ( m_sipMessage . length () - 1 ) ; <nl> + if ( m_sipMessage . length () > 1 ) m_sipMessage . resize ( m_sipMessage . length () - 1 ) ; <nl> boost :: replace_all ( m_sipMessage , "\ n ", DR_CRLF ); <nl> } <nl> else if ( 0 == strcmp ( szLine , "\ n ") ) {
void hexdump ( msg_info msg_info , const char * mem , unsigned int len ) <nl> } <nl> str [ c ++] = '\ n '; <nl> str [ c ++] = 0 ; <nl> - print_message ( msg_info , str ); <nl> + print_message ( msg_info , "% s ", str ); <nl> c = 0 ; <nl> } <nl> }
tport_t * tport_tsend ( tport_t * self , <nl> tp_name_t tpn [ 1 ]; <nl> struct sigcomp_compartment * cc ; <nl>  <nl> - assert ( self ); <nl> - <nl> if (! self || ! msg || ! _tpn ) { <nl> msg_set_errno ( msg , EINVAL ); <nl> return NULL ;
static int parse_packet ( const char * payload , struct ncrx_msg * msg ) <nl> goto einval ; <nl> if (! msg -> text_len || <nl> nf_len >= NCRX_LINE_MAX || <nl> + nf_off >= nf_len || <nl> nf_off + msg -> text_len > nf_len ) <nl> goto einval ; <nl> 
BZIP3_API s32 bz3_decode_block ( struct bz3_state * state , u8 * buffer , s32 data_s <nl> } <nl>  <nl> if ( bwt_idx == - 1 ) { <nl> - if ( data_size - 8 > 64 ) { <nl> + if ( data_size - 8 > 64 || data_size < 8 ) { <nl> state -> last_error = BZ3_ERR_MALFORMED_HEADER ; <nl> return - 1 ; <nl> }
PerformanceNavigationTiming :: PerformanceNavigationTiming ( <nl> ResourceTimingInfo * info , <nl> TimeTicks time_origin , <nl> const WebVector < WebServerTimingInfo >& server_timing ) <nl> - : PerformanceResourceTiming ( info ? info -> InitialURL (). GetString () : "", <nl> - " navigation ", <nl> - time_origin , <nl> - server_timing ), <nl> + : PerformanceResourceTiming ( <nl> + info ? info -> FinalResponse (). Url (). GetString () : "", <nl> + " navigation ", <nl> + time_origin , <nl> + server_timing ), <nl> ContextClient ( frame ), <nl> resource_timing_info_ ( info ) { <nl> DCHECK ( frame );
void RendererSchedulerImpl :: OnShutdownTaskQueue ( <nl> case MainThreadTaskQueue :: QueueClass :: kTimer : <nl> task_queue -> RemoveTaskObserver ( <nl> & main_thread_only (). timer_task_cost_estimator ); <nl> + break ; <nl> case MainThreadTaskQueue :: QueueClass :: kLoading : <nl> task_queue -> RemoveTaskObserver ( <nl> & main_thread_only (). loading_task_cost_estimator ); <nl> + break ; <nl> default : <nl> break ; <nl> }
void FeatureInfo :: EnableOESTextureHalfFloatLinear () { <nl> return ; <nl> AddExtensionString (" GL_OES_texture_half_float_linear "); <nl> feature_flags_ . enable_texture_half_float_linear = true ; <nl> + <nl> + // TODO ( capn ) : Re - enable this once we have ANGLE + SwiftShader supporting <nl> + // IOSurfaces . <nl> + if ( workarounds_ . disable_half_float_for_gmb ) <nl> + return ; <nl> feature_flags_ . gpu_memory_buffer_formats . Add ( gfx :: BufferFormat :: RGBA_F16 ); <nl> } <nl> 
ChildProcessTerminationInfo ChildProcessLauncherHelper :: GetTerminationInfo ( <nl> if (! java_peer_avaiable_on_client_thread_ ) <nl> return info ; <nl>  <nl> - Java_ChildProcessLauncherHelperImpl_getTerminationInfo ( <nl> + Java_ChildProcessLauncherHelperImpl_getTerminationInfoAndStop ( <nl> AttachCurrentThread (), java_peer_ , reinterpret_cast < intptr_t >(& info )); <nl>  <nl> base :: android :: ApplicationState app_state =
class TouchActionBrowserTest : public ContentBrowserTest { <nl> // Expect that the compositor scrolled at least one pixel while the <nl> // main thread was in a busy loop . <nl> while ( wait_until_scrolled && <nl> - frame_watcher -> LastMetadata (). root_scroll_offset . y () <= 0 ) { <nl> + frame_watcher -> LastMetadata (). root_scroll_offset . y () < <nl> + distance . y ()) { <nl> frame_watcher -> WaitFrames ( 1 ); <nl> } <nl> 
void GpuDataManager :: UpdateGpuInfo ( const GPUInfo & gpu_info ) { <nl> base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> if (! gpu_info_ . Merge ( gpu_info )) <nl> return ; <nl> + } <nl> + <nl> + RunGpuInfoUpdateCallbacks (); <nl>  <nl> - RunGpuInfoUpdateCallbacks (); <nl> + { <nl> + base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> content :: GetContentClient ()-> SetGpuInfo ( gpu_info_ ); <nl> } <nl> 
void InterstitialPage :: Observe ( NotificationType type , <nl> // request won ' t be blocked if the same RenderViewHost was used for the <nl> // new navigation . <nl> Disable (); <nl> - DCHECK (! resource_dispatcher_host_notified_ ); <nl> TakeActionOnResourceDispatcher ( CANCEL ); <nl> break ; <nl> case NotificationType :: RENDER_WIDGET_HOST_DESTROYED :
xmlParseAttValueComplex ( xmlParserCtxtPtr ctxt , int * attlen , int normalize ) { <nl> c = CUR_CHAR ( l ); <nl> } <nl> if (( in_space ) && ( normalize )) { <nl> - while ( buf [ len - 1 ] == 0x20 ) len --; <nl> + while (( len > 0 ) && ( buf [ len - 1 ] == 0x20 )) len --; <nl> } <nl> buf [ len ] = 0 ; <nl> if ( RAW == '<') {
std :: string ProcessRawBytesWithSeparators ( const unsigned char * data , <nl> // except for the last byte . <nl> std :: string ret ; <nl> size_t kMin = 0U ; <nl> + <nl> + if (! data_length ) <nl> + return ""; <nl> + <nl> ret . reserve ( std :: max ( kMin , data_length * 3 - 1 )); <nl>  <nl> for ( size_t i = 0 ; i < data_length ; ++ i ) {
void OpenPDFInReaderView :: Update ( content :: WebContents * web_contents ) { <nl> } <nl>  <nl> SetVisible (!! model_ ); <nl> + <nl> + // Hide the bubble if it is currently shown and the icon is hidden . <nl> + if (! model_ && bubble_ ) <nl> + bubble_ -> GetWidget ()-> Hide (); <nl> } <nl>  <nl> void OpenPDFInReaderView :: ShowBubble () {
IN_PROC_BROWSER_TEST_F ( PanelBrowserTest , MAYBE_FocusLostOnMinimize ) { <nl> panel -> Close (); <nl> } <nl>  <nl> -// TODO ( dimich ): try / enable on other platforms . <nl> -# if defined ( OS_MACOSX ) || defined ( OS_WIN ) <nl> +// TODO ( dimich ): try / enable on other platforms . See bug 103253 for details on <nl> +// why this is disabled on windows . <nl> +# if defined ( OS_MACOSX ) <nl> # define MAYBE_MinimizeTwoPanelsWithoutTabbedWindow \ <nl> MinimizeTwoPanelsWithoutTabbedWindow <nl> # else
class ActionBoxTest : public InProcessBrowserTest , <nl> case content :: NOTIFICATION_WEB_CONTENTS_DESTROYED : <nl> case chrome :: NOTIFICATION_TAB_PARENTED : <nl> case chrome :: NOTIFICATION_AUTOCOMPLETE_CONTROLLER_RESULT_READY : <nl> - case chrome :: NOTIFICATION_BOOKMARK_MODEL_LOADED : <nl> case chrome :: NOTIFICATION_HISTORY_LOADED : <nl> case chrome :: NOTIFICATION_HISTORY_URLS_MODIFIED : <nl> case chrome :: NOTIFICATION_TEMPLATE_URL_SERVICE_LOADED :
SessionStartupPref StartupBrowserCreator :: GetSessionStartupPref ( <nl> pref . type = SessionStartupPref :: LAST ; <nl> } <nl>  <nl> - if ( pref . type == SessionStartupPref :: LAST && <nl> - IncognitoModePrefs :: ShouldLaunchIncognito ( command_line , prefs )) { <nl> + if ( pref . type == SessionStartupPref :: LAST && profile -> IsOffTheRecord ()) { <nl> // We don ' t store session information when incognito . If the user has <nl> // chosen to restore last session and launched incognito , fallback to <nl> // default launch behavior .
class InotifyReaderTask : public Task { <nl> : reader_ ( reader ), <nl> inotify_fd_ ( inotify_fd ), <nl> shutdown_fd_ ( shutdown_fd ) { <nl> + // Make sure the file descriptors are good for use with select (). <nl> + CHECK_LE ( 0 , inotify_fd_ ); <nl> + CHECK_GT ( FD_SETSIZE , inotify_fd_ ); <nl> + CHECK_LE ( 0 , shutdown_fd_ ); <nl> + CHECK_GT ( FD_SETSIZE , shutdown_fd_ ); <nl> } <nl>  <nl> virtual void Run () {
WebContents * DevToolsWindow :: OpenURLFromTab ( <nl> DCHECK ( source == main_web_contents_ ); <nl> if (! params . url . SchemeIs ( content :: kChromeDevToolsScheme )) { <nl> WebContents * inspected_web_contents = GetInspectedWebContents (); <nl> - return inspected_web_contents ? <nl> - inspected_web_contents -> OpenURL ( params ) : NULL ; <nl> + if (! inspected_web_contents ) <nl> + return nullptr ; <nl> + content :: OpenURLParams modified = params ; <nl> + modified . referrer = content :: Referrer (); <nl> + return inspected_web_contents -> OpenURL ( modified ); <nl> } <nl> bindings_ -> Reload (); <nl> return main_web_contents_ ;
int PDFiumEngine :: GetMostVisiblePage () { <nl> if ( in_flight_visible_page_ ) <nl> return * in_flight_visible_page_ ; <nl>  <nl> + // We can call GetMostVisiblePage through a callback from PDFium . We have <nl> + // to defer the page deletion otherwise we could potentially delete the page <nl> + // that originated the calling JS request and destroy the objects that are <nl> + // currently being used . <nl> + defer_page_unload_ = true ; <nl> CalculateVisiblePages (); <nl> + defer_page_unload_ = false ; <nl> return most_visible_page_ ; <nl> } <nl> 
xsltCompileLocationPathPattern ( xsltParserContextPtr ctxt , int novar ) { <nl> SKIP_BLANKS ; <nl> if (( CUR == '(') && ! xmlXPathIsNodeType ( name )) { <nl> xsltCompileIdKeyPattern ( ctxt , name , 1 , novar , 0 ); <nl> + if ( ctxt -> error ) <nl> + return ; <nl> if (( CUR == '/') && ( NXT ( 1 ) == '/')) { <nl> PUSH ( XSLT_OP_ANCESTOR , NULL , NULL , novar ); <nl> NEXT ;
class LocalHttpTestServer : public TestServer { <nl> FilePath ()) {} <nl> }; <nl>  <nl> - TEST_F ( URLRequestTest , DelayedCookieCallback ) { <nl> + TEST_F ( URLRequestTest , FLAKY_DelayedCookieCallback ) { <nl> LocalHttpTestServer test_server ; <nl> ASSERT_TRUE ( test_server . Start ()); <nl>  <nl> class URLRequestTestFTP : public URLRequestTest { <nl> }; <nl>  <nl> // Make sure an FTP request using an unsafe ports fails . <nl> - TEST_F ( URLRequestTestFTP , UnsafePort ) { <nl> + TEST_F ( URLRequestTestFTP , FLAKY_UnsafePort ) { <nl> ASSERT_TRUE ( test_server_ . Start ()); <nl>  <nl> URLRequestJobFactoryImpl job_factory ;
TEST ( VideoFrameMac , CheckWrapperFrame ) { <nl> { PIXEL_FORMAT_NV12 , kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange }, <nl> }; <nl>  <nl> - const gfx :: Size size ( kWidth , kHeight ); <nl> for ( const auto & format_pair : format_pairs ) { <nl> base :: ScopedCFTypeRef < CVPixelBufferRef > pb ; <nl> CVPixelBufferCreate ( nullptr , kWidth , kHeight , format_pair . corevideo ,
void AudioOutputController :: DoFlush () { <nl> if (! sync_reader_ ) { <nl> if ( state_ != kPaused ) <nl> return ; <nl> + AutoLock auto_lock ( lock_ ); <nl> buffer_ . Clear (); <nl> } <nl> }
void WebGLRenderingContextBase :: TexImageHelperImageData ( <nl> adjusted_source_image_rect . Height (), format , type , bytes ); <nl> } else { <nl> GLint upload_height = adjusted_source_image_rect . Height (); <nl> - if ( unpack_image_height ) { <nl> - // GL_UNPACK_IMAGE_HEIGHT overrides the passed - in height . <nl> - upload_height = unpack_image_height ; <nl> - } <nl> if ( function_id == kTexImage3D ) { <nl> ContextGL ()-> TexImage3D ( target , level , internalformat , <nl> adjusted_source_image_rect . Width (), upload_height ,
bool SimplifiedBackwardsTextIterator :: handleTextNode () <nl> m_textLength = m_positionEndOffset - m_positionStartOffset ; <nl> m_textCharacters = text . characters () + ( m_positionStartOffset - offsetInNode ); <nl> ASSERT ( m_textCharacters >= text . characters ()); <nl> - ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl> + RELEASE_ASSERT ( m_textCharacters + m_textLength <= text . characters () + static_cast < int >( text . length ())); <nl>  <nl> m_lastCharacter = text [ m_positionEndOffset - 1 ]; <nl> 
bool HTMLFormElement :: validateInteractively ( Event * event ) <nl>  <nl> bool HTMLFormElement :: prepareForSubmission ( Event * event ) <nl> { <nl> + RefPtr < HTMLFormElement > protector ( this ); <nl> Frame * frame = document (). frame (); <nl> if ( m_isSubmittingOrPreparingForSubmission || ! frame ) <nl> return m_isSubmittingOrPreparingForSubmission ;
SK_API void SkDebugf_FileLine ( const char * file , int line , bool fatal , <nl> # define SK_USE_LEGACY_DISTANCE_FIELDS <nl> # endif <nl>  <nl> -// To stage layout result changes ( minor ) related to convexity calculations <nl> -# ifndef SK_SUPPORT_LEGACY_CACHE_CONVEXITY <nl> -# define SK_SUPPORT_LEGACY_CACHE_CONVEXITY <nl> -# endif <nl> - <nl> // Skia is enabling this feature soon . Chrome probably does <nl> // not want it for M64 <nl> # ifndef SK_DISABLE_EXPLICIT_GPU_RESOURCE_ALLOCATION
void PaintLayerScrollableArea :: UpdateCompositingLayersAfterScroll () { <nl> DCHECK ( Layer ()-> HasCompositedLayerMapping ()); <nl> ScrollingCoordinator * scrolling_coordinator = GetScrollingCoordinator (); <nl> bool handled_scroll = <nl> - Layer ()-> IsRootLayer () && scrolling_coordinator && <nl> + ( Layer ()-> IsRootLayer () || <nl> + RuntimeEnabledFeatures :: BlinkGenPropertyTreesEnabled ()) && <nl> + scrolling_coordinator && <nl> scrolling_coordinator -> UpdateCompositedScrollOffset ( this ); <nl>  <nl> if (! handled_scroll ) {
class PowerPopupView : public views :: Label { <nl> public : <nl> PowerPopupView () { <nl> SetHorizontalAlignment ( ALIGN_RIGHT ); <nl> + SetMultiLine ( true ); <nl> UpdateText (); <nl> } <nl> 
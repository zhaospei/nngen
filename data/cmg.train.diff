start_play_tune ( GstSidDec * siddec ) <nl>  <nl> gst_segment_init (& segment , GST_FORMAT_TIME ); <nl> gst_pad_push_event ( siddec -> srcpad , gst_event_new_segment (& segment )); <nl> + siddec -> total_bytes = 0 ; <nl>  <nl> res = gst_pad_start_task ( siddec -> srcpad , <nl> ( GstTaskFunction ) play_loop , siddec -> srcpad , NULL );
init_failed : <nl> static gboolean <nl> plugin_init ( GstPlugin * plugin ) <nl> { <nl> - if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_MARGINAL , <nl> + if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_PRIMARY , <nl> GST_TYPE_MPEG2DEC )) <nl> return FALSE ; <nl> 
gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl> gst_buffer_ref ( buf ); <nl> if (! gst_buffer_map ( buf , & minfo , GST_MAP_READ )) { <nl> GST_ERROR_OBJECT ( mpeg2dec , " Failed to map input buffer "); <nl> + gst_buffer_unref ( buf ); <nl> return GST_FLOW_ERROR ; <nl> } <nl> 
gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl> frame -> system_frame_number , <nl> GST_TIME_ARGS ( frame -> pts ), GST_TIME_ARGS ( frame -> duration )); <nl>  <nl> + gst_buffer_ref ( buf ); <nl> if (! gst_buffer_map ( buf , & minfo , GST_MAP_READ )) { <nl> GST_ERROR_OBJECT ( mpeg2dec , " Failed to map input buffer "); <nl> return GST_FLOW_ERROR ; <nl> gst_mpeg2dec_handle_frame ( GstVideoDecoder * decoder , <nl>  <nl> done : <nl> gst_buffer_unmap ( buf , & minfo ); <nl> + gst_buffer_unref ( buf ); <nl> return ret ; <nl> } <nl> 
create_request_failed : <nl> { <nl> GST_ELEMENT_ERROR ( ctx , LIBRARY , INIT , <nl> (" Could not create request ."), ( NULL )); <nl> + g_free ( req_url ); <nl> goto reset ; <nl> } <nl> send_error :
gst_rdt_depay_push ( GstRDTDepay * rdtdepay , GstBuffer * buffer ) <nl> rdtdepay -> need_newsegment = FALSE ; <nl> } <nl>  <nl> + buffer = gst_buffer_make_metadata_writable ( buffer ); <nl> gst_buffer_set_caps ( buffer , GST_PAD_CAPS ( rdtdepay -> srcpad )); <nl>  <nl> if ( rdtdepay -> discont ) {
free_tree ( struct tree * t ) <nl> { <nl> size_t i ; <nl>  <nl> + if ( t == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < t -> nr_files ; ++ i ) { <nl> free ( t -> files [ i ]. path ); <nl> guestfs_free_statns ( t -> files [ i ]. stat );
main ( int argc , char * argv []) <nl> name = strrchr ( drvs -> a . filename , '/'); <nl> if ( name == NULL ) <nl> name = drvs -> a . filename ; <nl> + else <nl> + name ++; /* skip '/' character */ <nl> break ; <nl> case drv_d : <nl> name = drvs -> d . guest ;
do_part_get_gpt_type ( const char * device , int partnum ) <nl> char * <nl> do_part_get_name ( const char * device , int partnum ) <nl> { <nl> - CLEANUP_FREE char * parttype = do_part_get_parttype ( device ); <nl> + CLEANUP_FREE char * parttype ; <nl> + <nl> + parttype = do_part_get_parttype ( device ); <nl> + if ( parttype == NULL ) <nl> + return NULL ; <nl>  <nl> if ( STREQ ( parttype , " gpt ")) { <nl> int parted_has_m_opt = test_parted_m_opt ();
static int ovs_events_plugin_config ( oconfig_item_t * ci ) { <nl> ovs_events_config_free (); <nl> return - 1 ; <nl> } <nl> - strncpy ( ovs_events_ctx . config . ovs_db_serv , service , <nl> - sizeof ( ovs_events_ctx . config . ovs_db_serv )); <nl> + sstrncpy ( ovs_events_ctx . config . ovs_db_serv , service , <nl> + sizeof ( ovs_events_ctx . config . ovs_db_serv )); <nl> sfree ( service ); <nl> } else if ( strcasecmp (" Socket ", child -> key ) == 0 ) { <nl> if ( cf_util_get_string_buffer (
static int mb_config_add_host ( oconfig_item_t * ci ) /* {{{ */ <nl>  <nl> status = cf_util_get_string_buffer ( ci , host -> host , sizeof ( host -> host )); <nl> if ( status != 0 ) <nl> + { <nl> + sfree ( host ); <nl> return ( status ); <nl> + } <nl> if ( host -> host [ 0 ] == 0 ) <nl> + { <nl> + sfree ( host ); <nl> return ( EINVAL ); <nl> + } <nl>  <nl> for ( i = 0 ; i < ci -> children_num ; i ++) <nl> {
static int bind_config ( oconfig_item_t * ci ) /* {{{ */ <nl> return (- 1 ); <nl> } <nl>  <nl> + sfree ( url ); <nl> url = strdup ( child -> values [ 0 ]. value . string ); <nl> } else if ( strcasecmp (" OpCodes ", child -> key ) == 0 ) <nl> bind_config_set_bool (" OpCodes ", & global_opcodes , child );
static int perl_config_includedir ( oconfig_item_t * ci ) <nl> || ( OCONFIG_TYPE_STRING != ci -> values [ 0 ]. type )) <nl> return 1 ; <nl>  <nl> + if ( NULL == aTHX ) { <nl> + log_warn (" EnableDebugger has no effects if used after LoadPlugin ."); <nl> + return 1 ; <nl> + } <nl> + <nl> value = ci -> values [ 0 ]. value . string ; <nl>  <nl> if ( NULL == perl ) {
c_avl_tree_t * c_avl_create ( int (* compare ) ( const void *, const void *)) <nl>  <nl> void c_avl_destroy ( c_avl_tree_t * t ) <nl> { <nl> + if ( t == NULL ) <nl> + return ; <nl> free_node ( t -> root ); <nl> free ( t ); <nl> }
int lcc_getval ( lcc_connection_t * c , lcc_identifier_t * ident , /* {{{ */ <nl> if ( ret_values_names != NULL ) <nl> * ret_values_names = values_names ; <nl>  <nl> + lcc_response_free (& res ); <nl> + <nl> return ( 0 ); <nl> } /* }}} int lcc_getval */ <nl> 
static int tss2_add_vserver ( int vserver_port ) <nl> } <nl>  <nl> /* Allocate memory */ <nl> - entry = malloc ( sizeof (* entry )); <nl> + entry = calloc ( 1 , sizeof (* entry )); <nl> if ( entry == NULL ) <nl> { <nl> - ERROR (" teamspeak2 plugin : malloc failed ."); <nl> + ERROR (" teamspeak2 plugin : calloc failed ."); <nl> return (- 1 ); <nl> } <nl> - memset ( entry , 0 , sizeof ( vserver_list_t )); <nl>  <nl> /* Save data */ <nl> entry -> port = vserver_port ;
submit_counters ( struct thread_data * t , struct core_data * c , struct pkg_data * p ) <nl>  <nl> /* SMI */ <nl> if ( do_smi ) <nl> - turbostat_submit ( name , " current ", NULL , t -> smi_count ); <nl> + turbostat_submit ( name , " count ", NULL , t -> smi_count ); <nl>  <nl> /* submit per - core data only for 1st thread in core */ <nl> if (!( t -> flags & CPU_IS_FIRST_THREAD_IN_CORE ))
static int varnish_read ( user_data_t * ud ) /* {{{ */ <nl>  <nl> vd = VSM_New (); <nl> VSC_Setup ( vd ); <nl> + if ( VSM_n_Arg ( vd , conf -> instance ) == - 1 ) <nl> + { <nl> + ERROR (" Varnish plugin : unable to load statistics from instance "); <nl> + return (- 1 ); <nl> + } <nl> if ( VSC_Open ( vd , /* diag = */ 1 )) <nl> { <nl> ERROR (" varnish plugin : Unable to load statistics .");
refresh_lists ( void ) <nl>  <nl> /* Get list of domains . */ <nl> domids = malloc ( sizeof (* domids ) * n ); <nl> - if ( domids == 0 ) { <nl> + if ( domids == NULL ) { <nl> ERROR ( PLUGIN_NAME " plugin : malloc failed ."); <nl> return - 1 ; <nl> }
static int wh_write_command ( const data_set_t * ds , const value_list_t * vl , /* {{ <nl> } <nl> assert ( command_len < cb -> send_buffer_free ); <nl>  <nl> + /* Make scan - build happy . */ <nl> + assert ( cb -> send_buffer != NULL ); <nl> + <nl> /* ` command_len + 1 ' because ` command_len ' does not include the <nl> * trailing null byte . Neither does ` send_buffer_fill '. */ <nl> memcpy ( cb -> send_buffer + cb -> send_buffer_fill ,
static int c_psql_config_query ( oconfig_item_t * ci ) <nl> else <nl> log_warn (" Ignoring unknown config key \"% s \".", c -> key ); <nl> } <nl> + <nl> + if ( NULL == query -> query ) { <nl> + log_err (" Query \"% s \" does not include an SQL query string - " <nl> + " please check your configuration .", query -> name ); <nl> + c_psql_query_delete ( query ); <nl> + -- queries_num ; <nl> + return 1 ; <nl> + } <nl> return 0 ; <nl> } /* c_psql_config_query */ <nl> 
int main ( int argc , char ** argv ) <nl> return 1 ; <nl> } <nl> krb5_klog_syslog ( LOG_INFO , " commencing operation "); <nl> + if ( nofork ) <nl> + fprintf ( stderr , "% s : starting ...\ n ", kdc_progname ); <nl> if (( retval = listen_and_process ())) { <nl> kdc_err ( kcontext , retval , " while processing network requests "); <nl> errout ++;
krb5_lcc_close ( krb5_context context , krb5_ccache id ) <nl>  <nl> if ( data ) { <nl> LsaDeregisterLogonProcess ( data -> LogonHandle ); <nl> + if ( data -> cc_name ) <nl> + free ( data -> cc_name ); <nl> free ( data ); <nl> } <nl> free ( id );
strdup ( str ) <nl> int len ; <nl> char * copy ; <nl>  <nl> + if (! str ) <nl> + return (( char *) 0 ); <nl> len = strlen ( str ) + 1 ; <nl> if (!( copy = malloc (( u_int ) len ))) <nl> return (( char *) 0 );
kadm5_setkey_principal_3 ( void * server_handle , <nl> goto done ; <nl> } <nl> tptr = & kdb . key_data [ i ]; <nl> + tptr -> key_data_ver = tmp_key_data . key_data_ver ; <nl> + tptr -> key_data_kvno = tmp_key_data . key_data_kvno ; <nl> for ( k = 0 ; k < tmp_key_data . key_data_ver ; k ++) { <nl> tptr -> key_data_type [ k ] = tmp_key_data . key_data_type [ k ]; <nl> tptr -> key_data_length [ k ] = tmp_key_data . key_data_length [ k ];
try_one_princ ( krb5_context context , const krb5_ap_req * req , <nl> if ( ret ) <nl> return ret ; <nl> ret = try_one_entry ( context , req , & ent , keyblock_out ); <nl> + if ( ret == 0 ) <nl> + TRACE_RD_REQ_DECRYPT_SPECIFIC ( context , ent . principal , & ent . key ); <nl> ( void ) krb5_free_keytab_entry_contents ( context , & ent ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - TRACE_RD_REQ_DECRYPT_SPECIFIC ( context , ent . principal , & ent . key ); <nl> return 0 ; <nl> } <nl> 
template int SSLWrap < TLSCallbacks >:: TLSExtStatusCallback ( SSL * s , void * arg ); <nl>  <nl>  <nl> static void crypto_threadid_cb ( CRYPTO_THREADID * tid ) { <nl> - static_assert ( sizeof ( uv_thread_t ) <= sizeof ( void *), <nl> + static_assert ( sizeof ( uv_thread_t ) <= sizeof ( void *), // NOLINT ( runtime / sizeof ) <nl> " uv_thread_t does not fit in a pointer "); <nl> CRYPTO_THREADID_set_pointer ( tid , reinterpret_cast < void *>( uv_thread_self ())); <nl> }
static struct { <nl> } <nl>  <nl> void StopTracingAgent () { <nl> - tracing_agent_ -> Stop (); <nl> + if ( tracing_agent_ ) <nl> + tracing_agent_ -> Stop (); <nl> } <nl>  <nl> tracing :: Agent * GetTracingAgent () const {
static void GetStringWidth ( const FunctionCallbackInfo < Value >& args ) { <nl> TwoByteValue value ( env -> isolate (), args [ 0 ]); <nl> // reinterpret_cast is required by windows to compile <nl> UChar * str = reinterpret_cast < UChar *>(* value ); <nl> - UChar32 c ; <nl> + static_assert ( sizeof (* str ) == sizeof (** value ), <nl> + " sizeof (* str ) == sizeof (** value )"); <nl> + UChar32 c = 0 ; <nl> UChar32 p ; <nl> size_t n = 0 ; <nl> uint32_t width = 0 ;
class CipherBase : public BaseObject { <nl> private : <nl> EVP_CIPHER_CTX ctx_ ; /* coverity [ member_decl ] */ <nl> bool initialised_ ; <nl> - CipherKind kind_ ; <nl> + const CipherKind kind_ ; <nl> unsigned int auth_tag_len_ ; <nl> char auth_tag_ [ EVP_GCM_TLS_TAG_LEN ]; <nl> };
void RandomBytesCheck ( RandomBytesRequest * req , Local < Value > argv [ 2 ]) { <nl> Buffer * buffer = Buffer :: New ( req -> data_ , req -> size_ , RandomBytesFree , NULL ); <nl> argv [ 0 ] = Local < Value >:: New ( Null ()); <nl> argv [ 1 ] = Local < Object >:: New ( buffer -> handle_ ); <nl> + req -> data_ = NULL ; <nl> } <nl> + free ( req -> data_ ); <nl> } <nl>  <nl> 
void SetupProcessObject ( Environment * env , <nl> READONLY_PROPERTY ( process , <nl> " _preload_modules ", <nl> array ); <nl> + <nl> + delete [] preload_modules ; <nl> + preload_modules = nullptr ; <nl> + preload_module_count = 0 ; <nl> } <nl>  <nl> // -- no - deprecation
void CipherBase :: Final ( const FunctionCallbackInfo < Value >& args ) { <nl>  <nl> args . GetReturnValue (). Set ( <nl> Buffer :: New ( env , reinterpret_cast < char *>( out_value ), out_len )); <nl> + delete [] out_value ; <nl> } <nl>  <nl> 
static SRes SzFolder_Decode2 ( const CSzFolder * folder , const UInt64 * packSizes , <nl> else <nl> return SZ_ERROR_UNSUPPORTED ; <nl> } <nl> + if (! packSizes ) <nl> + return SZ_ERROR_FAIL ; <nl> offset = GetSum ( packSizes , si ); <nl> inSize = packSizes [ si ]; <nl> RINOK ( LookInStream_SeekTo ( inStream , startPos + offset ));
const char * cl_strerror ( int clerror ) <nl> return " Null argument passed while initialized is required "; <nl> case CL_EIO : <nl> return " Input / Output error "; <nl> + case CL_EFORMAT : <nl> + return " Bad format or broken data "; <nl> default : <nl> return " Unknown error code "; <nl> }
int unrar_open ( int fd , const char * dirname , unrar_state_t * state ) <nl> unrar_dbgmsg (" UNRAR : Offset : % x \ n ", offset ); <nl> if ( offset < 0 ){ <nl> unrar_dbgmsg (" UNRAR : Error Offset : % d \ n ", offset ); <nl> - offset = 0 ; <nl> + free ( main_hdr ); <nl> + free ( state -> comment_dir ); <nl> + free ( unpack_data ); <nl> + return UNRAR_ERR ; <nl> } <nl> comment_header = read_header ( fd , COMM_HEAD ); <nl> if ( comment_header ) {
static int pdf_extract_obj ( struct pdf_struct * pdf , struct pdf_obj * obj ) <nl> n -= q2 - q ; <nl> q = q2 ; <nl> } <nl> - } while ( n > 0 && q2 && q2 [- 1 ] == '\\'); <nl> + } while ( n > 0 && q2 && q2 [- 2 ] == '\\'); <nl> if ( q2 ) <nl> end = q2 - 1 ; <nl> n = end - out ;
abort : <nl> } <nl> if ( file_tmp_o1 ) { <nl> html_output_flush ( file_tmp_o1 ); <nl> - close ( file_tmp_o1 -> fd ); <nl> + if ( file_buff_text -> fd != - 1 ) <nl> + close ( file_tmp_o1 -> fd ); <nl> free ( file_tmp_o1 ); <nl> } <nl> return retval ;
static char * sha256file ( const char * file , unsigned int * size ) <nl> sha256_final (& ctx , digest ); <nl> sha = ( char *) malloc ( 65 ); <nl> if (! sha ) <nl> + { <nl> + fclose ( fh ); <nl> return NULL ; <nl> + } <nl> for ( i = 0 ; i < 32 ; i ++) <nl> sprintf ( sha + i * 2 , "% 02x ", digest [ i ]); <nl> + <nl> + fclose ( fh ); <nl> return sha ; <nl> } <nl> 
inline static int ac_special_altstr ( const char * hexpr , uint8_t sigopts , struct c <nl> /* allocate reusable subexpr */ <nl> if (!( subexpr = cli_calloc ( slen + 1 , sizeof ( char )))) { <nl> cli_errmsg (" ac_special_altstr : Can ' t allocate subexpr container \ n "); <nl> + free ( hexprcpy ); <nl> return CL_EMEM ; <nl> } <nl> 
int cli_bm_scanbuff ( const unsigned char * buffer , uint32_t length , const char ** v <nl> memset (& info , 0 , sizeof ( info )); <nl> i = BM_MIN_LENGTH - BM_BLOCK_SIZE ; <nl> if ( offdata ) { <nl> + if (! offdata -> cnt ) <nl> + return CL_CLEAN ; <nl> for (; offdata -> pos && offdata -> offtab [ offdata -> pos ] > offset ; offdata -> pos --); <nl> if ( offdata -> offtab [ offdata -> pos ] < offset ) <nl> offdata -> pos ++;
static int updatedb ( const char * dbname , const char * hostname , char * ip , int * sig <nl> } <nl>  <nl> if ( rename ( newfile , newdb ) == - 1 ) { <nl> - logg ("! Can ' t rename % s to % s \ n ", newfile , newdb ); <nl> + logg ("! Can ' t rename % s to % s : % s \ n ", newfile , newdb , strerror ( errno )); <nl> unlink ( newfile ); <nl> free ( newfile ); <nl> return 57 ;
static int url_hash_match ( const struct regex_matcher * rlist , const char * inurl , <nl> size_t j , k , ji , ki ; <nl> int rc ; <nl>  <nl> - if (! rlist -> md5_hashes . bm_patterns ) { <nl> + if (! rlist || ! rlist -> md5_hashes . bm_patterns ) { <nl> return CL_SUCCESS ; <nl> } <nl> if (! inurl )
char * pdf_finalize_string ( struct pdf_struct * pdf , struct pdf_obj * obj , const cha <nl> /* TODO : replace the escape sequences directly in the wrkstr */ <nl> if ( strchr ( wrkstr , '\\')) { <nl> output = cli_calloc ( wrklen + 1 , sizeof ( char )); <nl> - if (! output ) <nl> + if (! output ) { <nl> + free ( wrkstr ); <nl> return NULL ; <nl> + } <nl>  <nl> outlen = 0 ; <nl> for ( i = 0 ; i < wrklen ; ++ i ) {
cib_perform_op ( const char * op , int call_options , cib_op_t * fn , gboolean is_query <nl>  <nl> if ( rc == cib_ok && scratch ) { <nl> const char * new_version = crm_element_value ( scratch , XML_ATTR_CRM_VERSION ); <nl> - if ( new_version && compare_version ( new_version , CRM_FEATURE_SET ) < 0 ) { <nl> + if ( new_version && compare_version ( new_version , CRM_FEATURE_SET ) > 0 ) { <nl> crm_err (" Discarding update with feature set '% s ' greater than our own '% s '", <nl> new_version , CRM_FEATURE_SET ); <nl> rc = cib_NOTSUPPORTED ;
do_cl_join_finalize_respond ( long long action , <nl> erase_status_tag ( fsa_our_uname , XML_CIB_TAG_LRM ); <nl>  <nl> /* Just in case attrd was still around too */ <nl> - if ( is_not_set ( input_register , R_SHUTDOWN )) { <nl> + if ( is_not_set ( fsa_input_register , R_SHUTDOWN )) { <nl> update_attrd ( fsa_our_uname , " terminate ", NULL ); <nl> update_attrd ( fsa_our_uname , XML_CIB_ATTR_SHUTDOWN , NULL ); <nl> }
lrmd_init_remote_tls_server () <nl> if ( rc != 0 ) { <nl> crm_warn (" A cluster connection will not be possible until the key is available "); <nl> } <nl> + gnutls_free ( psk_key . data ); <nl>  <nl> memset (& hints , 0 , sizeof ( struct addrinfo )); <nl> /* Bind to the wildcard address ( INADDR_ANY or IN6ADDR_ANY_INIT ).
void native_rsc_order_rh ( <nl> rsc -> actions , order -> rh_action_task , NULL ); <nl> } <nl>  <nl> - if ( rh_actions == NULL && lh_action != NULL ) { <nl> + if ( rh_actions == NULL ) { <nl> crm_debug_4 (" No RH - Side (% s /% s ) found for constraint ..." <nl> " ignoring ", rsc -> id , order -> rh_action_task ); <nl> if ( lh_action ) {
need_abort ( crm_data_t * update ) <nl> return NULL ; <nl> } <nl>  <nl> + xml_prop_iter ( update , name , value , <nl> + if ( safe_str_eq ( name , XML_ATTR_ID ) == FALSE ) { <nl> + return update ; <nl> + } <nl> + ); <nl> + <nl> section = XML_CIB_TAG_NODES ; <nl> section_xml = get_object_root ( section , update ); <nl> xml_child_iter ( section_xml , child ,
send_smtp_trap ( const char * node , const char * rsc , const char * task , int target_r <nl> char crm_mail_body [ BODY_MAX ]; <nl> char * crm_mail_subject = NULL ; <nl>  <nl> + memset (& sa , 0 , sizeof ( struct sigaction )); <nl> + <nl> if ( node == NULL ) { <nl> node = "-"; <nl> }
cib_ha_peer_callback ( HA_Message * msg , void * private_data ) <nl> { <nl> xmlNode * xml = convert_ha_message ( NULL , msg , __FUNCTION__ ); <nl> cib_peer_callback ( xml , private_data ); <nl> + free_xml ( xml ); <nl> } <nl>  <nl> void
synthesize_lrmd_failure ( lrm_state_t * lrm_state , xmlNode * action , int rc ) <nl> lrmd_free_rsc_info ( rsc_info ); <nl> process_lrm_event ( lrm_state , op , NULL ); <nl>  <nl> - } else { <nl> + } else if ( controld_action_is_recordable ( op -> op_type )) { <nl> /* If we can ' t process the result normally , at least write it to the CIB <nl> * if possible , so the scheduler can act on it . <nl> */
prepare_cmd_parameters ( const char * rsc_type , const char * op_type , <nl> * Add the teminating NULL pointer . <nl> */ <nl> params_argv [ 2 ] = NULL ; <nl> - if ( ht_size != 0 ) { <nl> + if ( ( ht_size != 0 ) && ( 0 != STRNCMP_CONST ( op_type , " status ")) ) { <nl> cl_log ( LOG_WARNING , " For LSB init script , no additional " <nl> " parameters are needed ."); <nl> }
write_cib_contents ( gpointer p ) <nl> crm_free ( tmp2 ); <nl> crm_free ( tmp1 ); <nl>  <nl> + free_xml ( local_cib ); <nl> + <nl> if ( p == NULL ) { <nl> /* exit () could potentially affect the parent by closing things it shouldn ' t <nl> * Use _exit instead
process_pe_message ( xmlNode * msg , xmlNode * xml_data , qb_ipcs_connection_t * send <nl> crm_xml_add_int ( data_set . graph , " transition_id ", 0 ); <nl> crm_xml_add_int ( data_set . graph , " cluster - delay ", 0 ); <nl> process = FALSE ; <nl> + free ( digest ); <nl>  <nl> } else if ( safe_str_eq ( digest , last_digest )) { <nl> crm_info (" Input has not changed since last time , not saving to disk "); <nl> is_repoke = TRUE ; <nl> + free ( digest ); <nl>  <nl> } else { <nl> free ( last_digest );
update_attr ( cib_t * the_cib , int call_options , <nl> CRM_CHECK ( set_name != NULL , return cib_missing ); <nl>  <nl> if ( attr_value == NULL ) { <nl> + free_xml ( xml_obj ); <nl> return cib_missing_data ; <nl> } <nl>  <nl> update_attr ( cib_t * the_cib , int call_options , <nl> xml_obj = create_xml_node ( xml_obj , XML_TAG_ATTRS ); <nl> crm_free ( local_set_name ); <nl> } else { <nl> + free_xml ( xml_obj ); <nl> xml_obj = NULL ; <nl> } <nl> 
attrd_local_callback ( xmlNode * msg ) <nl> goto set_unexpanded ; <nl> } <nl>  <nl> - int_value = char2score ( value ); <nl> + int_value = char2score ( hash_entry -> value ); <nl> if ( value [ plus_plus_len + 1 ] != '+') { <nl> const char * offset_s = value +( plus_plus_len + 2 ); <nl> offset = char2score ( offset_s );
common_apply_stickiness ( resource_t * rsc , node_t * node , pe_working_set_t * data_se <nl> value = g_hash_table_lookup ( node -> details -> attrs , fail_attr ); <nl> if ( value != NULL ) { <nl> crm_debug ("% s : % s ", fail_attr , value ); <nl> - fail_count = crm_parse_int ( value , " 0 "); <nl> + fail_count = char2score ( value ); <nl> } <nl> crm_free ( fail_attr ); <nl> 
void BitcoinGUI :: message ( const QString & title , const QString & message , unsigned <nl>  <nl> showNormalIfMinimized (); <nl> QMessageBox mBox ( static_cast < QMessageBox :: Icon >( nMBoxIcon ), strTitle , message , buttons , this ); <nl> + mBox . setTextFormat ( Qt :: PlainText ); <nl> int r = mBox . exec (); <nl> if ( ret != nullptr ) <nl> * ret = r == QMessageBox :: Ok ;
int git_repository_config__weakptr ( git_config ** out , git_repository * repo ) <nl> git_buf_free (& global_buf ); <nl> git_buf_free (& xdg_buf ); <nl> git_buf_free (& system_buf ); <nl> + git_buf_free (& programdata_buf ); <nl> } <nl>  <nl> * out = repo -> _config ;
static void impl__free ( git_odb_backend * _backend ) <nl> { <nl> struct memory_packer_db * db = ( struct memory_packer_db *) _backend ; <nl>  <nl> + git_mempack_reset ( _backend ); <nl> git_oidmap_free ( db -> objects ); <nl> git__free ( db ); <nl> }
int git_futils_mmap_ro_file ( git_map * out , const char * path ) <nl> if ( fd < 0 ) <nl> return fd ; <nl>  <nl> - len = git_futils_filesize ( fd ); <nl> + if (( len = git_futils_filesize ( fd )) < 0 ) <nl> + return - 1 ; <nl> + <nl> if (! git__is_sizet ( len )) { <nl> giterr_set ( GITERR_OS , " file `% s ` too large to mmap ", path ); <nl> return - 1 ;
static int config_delete ( git_config_file * cfg , const char * name ) <nl> pos = git_strmap_lookup_index ( b -> values , key ); <nl> git__free ( key ); <nl>  <nl> - if (! git_strmap_valid_index ( b -> values , pos )) <nl> + if (! git_strmap_valid_index ( b -> values , pos )) { <nl> + giterr_set ( GITERR_CONFIG , " Could not find key '% s ' to delete ", name ); <nl> return GIT_ENOTFOUND ; <nl> + } <nl>  <nl> var = git_strmap_value_at ( b -> values , pos ); <nl> 
int git_remote_update_tips ( git_remote * remote , int (* cb )( const char * refname , co <nl> for (; i < refs -> length ; ++ i ) { <nl> head = refs -> contents [ i ]; <nl>  <nl> + /* Skip tag annotations */ <nl> + if (! git__suffixcmp ( head -> name , "^{}")) <nl> + continue ; <nl> + <nl> if ( git_refspec_transform_r (& refname , spec , head -> name ) < 0 ) <nl> goto on_error ; <nl> 
static int valid_entry_name ( const char * filename ) <nl> (* filename != '.' || <nl> ( strcmp ( filename , ".") != 0 && <nl> strcmp ( filename , "..") != 0 && <nl> - strcmp ( filename , DOT_GIT ) != 0 )); <nl> + strcasecmp ( filename , DOT_GIT ) != 0 )); <nl> } <nl>  <nl> static int entry_sort_cmp ( const void * a , const void * b )
int git_path_diriter_init ( <nl> unsigned int flags ) <nl> { <nl> git_win32_path path_filter ; <nl> - git_buf hack = { 0 }; <nl>  <nl> static int is_win7_or_later = - 1 ; <nl> if ( is_win7_or_later < 0 )
static void checkout_data_clear ( checkout_data * data ) <nl> git__free ( data -> pfx ); <nl> data -> pfx = NULL ; <nl>  <nl> + git_buf_free (& data -> last_mkdir ); <nl> git_buf_free (& data -> path ); <nl> git_buf_free (& data -> tmp ); <nl> 
int git_repository_config__weakptr ( git_config ** out , git_repository * repo ) <nl> res = load_config (& repo -> _config , repo , global_config_path , xdg_config_path , system_config_path ); <nl>  <nl> git_buf_free (& global_buf ); <nl> + git_buf_free (& xdg_buf ); <nl> git_buf_free (& system_buf ); <nl>  <nl> if ( res < 0 )
static int limit_list ( git_commit_list ** out , git_revwalk * walk , git_commit_list <nl> break ; <nl> } <nl>  <nl> - if (! commit -> uninteresting && walk -> hide_cb && walk -> hide_cb (& commit -> oid , walk -> hide_cb_payload )) <nl> - continue ; <nl> + if ( walk -> hide_cb && walk -> hide_cb (& commit -> oid , walk -> hide_cb_payload )) <nl> + continue ; <nl>  <nl> time = commit -> time ; <nl> p = & git_commit_list_insert ( commit , p )-> next ;
void git_revwalk_reset ( git_revwalk * walk ) <nl> git_commit_list_free (& walk -> iterator_rand ); <nl> git_commit_list_free (& walk -> iterator_reverse ); <nl> git_commit_list_free (& walk -> user_input ); <nl> + walk -> first_parent = 0 ; <nl> walk -> walking = 0 ; <nl> walk -> did_push = walk -> did_hide = 0 ; <nl> }
int git_openssl_stream_global_init ( void ) <nl> * to speak TLSv1 to perform the encryption itself . <nl> */ <nl> git__ssl_ctx = SSL_CTX_new ( SSLv23_method ()); <nl> + if (! git__ssl_ctx ) { <nl> + return - 1 ; <nl> + } <nl> + <nl> SSL_CTX_set_options ( git__ssl_ctx , ssl_opts ); <nl> SSL_CTX_set_mode ( git__ssl_ctx , SSL_MODE_AUTO_RETRY ); <nl> SSL_CTX_set_verify ( git__ssl_ctx , SSL_VERIFY_NONE , NULL );
uint32_t git_pool__system_page_size ( void ) <nl> size_t page_size ; <nl> if ( git__page_size (& page_size ) < 0 ) <nl> page_size = 4096 ; <nl> - size = page_size - 2 * sizeof ( void *); /* allow space for malloc overhead */ <nl> + /* allow space for malloc overhead */ <nl> + size = page_size - ( 2 * sizeof ( void *)) - sizeof ( git_pool_page ); <nl> } <nl>  <nl> return size ;
int git_reference_rename ( git_reference * ref , const char * new_name , int force ) <nl> head_target = git_reference_target ( head ); <nl>  <nl> if ( head_target && ! strcmp ( head_target , ref -> name )) { <nl> + git_reference_free ( head ); <nl> + head = NULL ; <nl> + <nl> if ( git_reference_create_symbolic (& head , ref -> owner , " HEAD ", new_name , 1 ) < 0 ) { <nl> giterr_set ( GITERR_REFERENCE , <nl> " Failed to update HEAD after renaming reference ");
int git_repository_head_unborn ( git_repository * repo ) <nl> error = git_repository_head (& ref , repo ); <nl> git_reference_free ( ref ); <nl>  <nl> - if ( error == GIT_EUNBORNBRANCH ) <nl> + if ( error == GIT_EUNBORNBRANCH ) { <nl> + giterr_clear (); <nl> return 1 ; <nl> + } <nl>  <nl> if ( error < 0 ) <nl> return - 1 ;
static int wait_while_ack ( gitno_buffer * buf ) <nl> ( pkt -> status != GIT_ACK_CONTINUE || <nl> pkt -> status != GIT_ACK_COMMON )) { <nl> git__free ( pkt ); <nl> - break ; <nl> + return 0 ; <nl> } <nl> } <nl> 
cmsHANDLE CMSEXPORT cmsIT8LoadFromMem ( cmsContext ContextID , const void * Ptr , cm <nl>  <nl> it8 = ( cmsIT8 *) hIT8 ; <nl> it8 -> MemoryBlock = ( char *) _cmsMalloc ( ContextID , len + 1 ); <nl> + if ( it8 -> MemoryBlock == NULL ) <nl> + { <nl> + cmsIT8Free ( hIT8 ); <nl> + return FALSE ; <nl> + } <nl>  <nl> strncpy ( it8 -> MemoryBlock , ( const char *) Ptr , len ); <nl> it8 -> MemoryBlock [ len ] = 0 ;
cmsPipeline * DefaultICCintents ( cmsContext ContextID , <nl> // Concatenate to the output LUT <nl> if (! cmsPipelineCat ( Result , Lut )) <nl> goto Error ; <nl> + <nl> cmsPipelineFree ( Lut ); <nl> + Lut = NULL ; <nl>  <nl> // Update current space <nl> CurrentColorSpace = ColorSpaceOut ; <nl> cmsPipeline * DefaultICCintents ( cmsContext ContextID , <nl>  <nl> Error : <nl>  <nl> - cmsPipelineFree ( Lut ); <nl> + if ( Lut != NULL ) cmsPipelineFree ( Lut ); <nl> if ( Result != NULL ) cmsPipelineFree ( Result ); <nl> return NULL ; <nl> 
const char * make_absolute_path ( const char * path ) <nl> char * last_elem = NULL ; <nl> struct stat st ; <nl>  <nl> + /* We ' ve already done it */ <nl> + if ( path == buf || path == next_buf ) <nl> + return path ; <nl> + <nl> if ( strlcpy ( buf , path , PATH_MAX ) >= PATH_MAX ) <nl> die (" Too long path : %.* s ", 60 , path ); <nl> 
# define BLOCKSIZE ( RECORDSIZE * 20 ) <nl>  <nl> static const char tar_tree_usage [] = <nl> -" git - tar - tree [-- remote =< repo >] < ent > [ basedir ]"; <nl> +" git - tar - tree [-- remote =< repo >] < tree - ish > [ basedir ]"; <nl>  <nl> static char block [ BLOCKSIZE ]; <nl> static unsigned long offset ;
static struct ref_entry * search_ref_array ( struct ref_array * array , const char * n <nl> if ( name == NULL ) <nl> return NULL ; <nl>  <nl> + if (! array -> nr ) <nl> + return NULL ; <nl> + <nl> len = strlen ( name ) + 1 ; <nl> e = xmalloc ( sizeof ( struct ref_entry ) + len ); <nl> memcpy ( e -> name , name , len );
static const char * format_time ( unsigned long time , const char * tz_str , <nl> int tz ; <nl>  <nl> if ( show_raw_time ) { <nl> - sprintf ( time_buf , "% lu % s ", time , tz_str ); <nl> + snprintf ( time_buf , sizeof ( time_buf ), "% lu % s ", time , tz_str ); <nl> } <nl> else { <nl> tz = atoi ( tz_str );
*/ <nl>  <nl> typedef struct { <nl> + unsigned long long size ; <nl> unsigned int H [ 5 ]; <nl> unsigned int W [ 16 ]; <nl> - unsigned long long size ; <nl> } blk_SHA_CTX ; <nl>  <nl> void blk_SHA1_Init ( blk_SHA_CTX * ctx );
void diff_setup ( struct diff_options * options ) <nl>  <nl> int diff_setup_done ( struct diff_options * options ) <nl> { <nl> - if (( options -> find_copies_harder && <nl> - options -> detect_rename != DIFF_DETECT_COPY ) || <nl> - ( 0 <= options -> rename_limit && ! options -> detect_rename )) <nl> + if ( options -> find_copies_harder ) <nl> + options -> detect_rename = DIFF_DETECT_COPY ; <nl> + <nl> + if (( 0 <= options -> rename_limit && ! options -> detect_rename ) <nl> return - 1 ; <nl>  <nl> if ( options -> output_format & ( DIFF_FORMAT_NAME |
static void find_deltas ( struct object_entry ** list , unsigned * list_size , <nl> * depth , leaving it in the window is pointless . we <nl> * should evict it first . <nl> */ <nl> - if ( entry -> delta && depth <= n -> depth ) <nl> + if ( entry -> delta && max_depth <= n -> depth ) <nl> continue ; <nl>  <nl> /*
static int prepare_lines ( struct scoreboard * sb ) <nl> bol = 1 ; <nl> } <nl> } <nl> + sb -> lineno = xrealloc ( sb -> lineno , <nl> + sizeof ( int * ) * ( num + incomplete + 1 )); <nl> + sb -> lineno [ num + incomplete ] = buf - sb -> final_buf ; <nl> sb -> num_lines = num + incomplete ; <nl> return sb -> num_lines ; <nl> }
static void setup_progress_signal ( void ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> SHA_CTX ctx ; <nl> - char line [ PATH_MAX + 20 ]; <nl> + char line [ 40 + 1 + PATH_MAX + 2 ]; <nl> int window = 10 , depth = 10 , pack_to_stdout = 0 ; <nl> struct object_entry ** list ; <nl> int num_preferred_base = 0 ;
static int read_patches ( const char * range , struct string_list * list ) <nl> strbuf_addbuf (& buf , & line ); <nl> strbuf_addstr (& buf , "\ n \ n "); <nl> } else if ( starts_with ( line . buf , " ")) { <nl> + strbuf_rtrim (& line ); <nl> strbuf_addbuf (& buf , & line ); <nl> strbuf_addch (& buf , '\ n '); <nl> }
struct http_pack_request * new_http_pack_request ( <nl> return preq ; <nl>  <nl> abort : <nl> - free ( filename ); <nl> free ( preq -> url ); <nl> free ( preq ); <nl> return NULL ;
static void prune_directory ( struct dir_struct * dir , const char ** pathspec , int p <nl> free ( entry ); <nl> continue ; <nl> } <nl> + if ( entry -> ignored_entry ) <nl> + fprintf ( stderr , " warning : '% s ' is an ignored path .\ n ", <nl> + entry -> name ); <nl> * dst ++ = entry ; <nl> } <nl> dir -> nr = dst - dir -> entries ;
static int cmd_log ( int argc , const char ** argv , char ** envp ) <nl> prepare_revision_walk (& rev ); <nl> setup_pager (); <nl> while (( commit = get_revision (& rev )) != NULL ) { <nl> - if ( commit_format != CMIT_FMT_ONELINE && shown ) <nl> + if ( shown && do_diff && commit_format != CMIT_FMT_ONELINE ) <nl> putchar ('\ n '); <nl> fputs ( commit_prefix , stdout ); <nl> if ( abbrev_commit && abbrev )
int main ( int argc , const char ** argv ) <nl> else <nl> revs . header_prefix = " commit "; <nl> } <nl> + else if ( revs . verbose_header ) <nl> + /* Only -- header was specified */ <nl> + revs . commit_format = CMIT_FMT_RAW ; <nl>  <nl> list = revs . commits ; <nl> 
void verify_non_filename ( const char * prefix , const char * arg ) <nl> if (! lstat ( name , & st )) <nl> die (" ambiguous argument '% s ': both revision and filename \ n " <nl> " Use '--' to separate filenames from revisions ", arg ); <nl> - if ( errno != ENOENT ) <nl> + if ( errno != ENOENT && errno != ENOTDIR ) <nl> die ("'% s ': % s ", arg , strerror ( errno )); <nl> } <nl> 
extern int cmd_main ( int , const char **); <nl> */ <nl> # ifdef SUPPRESS_ANNOTATED_LEAKS <nl> extern void unleak_memory ( const void * ptr , size_t len ); <nl> -# define UNLEAK ( var ) unleak_memory (&( var ), sizeof ( var )); <nl> +# define UNLEAK ( var ) unleak_memory (&( var ), sizeof ( var )) <nl> # else <nl> -# define UNLEAK ( var ) <nl> +# define UNLEAK ( var ) do {} while ( 0 ) <nl> # endif <nl>  <nl> # endif
int xmkstemp ( char * template ) <nl> int saved_errno = errno ; <nl> const char * nonrelative_template ; <nl>  <nl> - if (! template [ 0 ]) <nl> + if ( strlen ( template ) != strlen ( origtemplate )) <nl> template = origtemplate ; <nl>  <nl> nonrelative_template = absolute_path ( template );
const char * const local_repo_env [ LOCAL_REPO_ENV_SIZE + 1 ] = { <nl> static void setup_git_env ( void ) <nl> { <nl> git_dir = getenv ( GIT_DIR_ENVIRONMENT ); <nl> - if (! git_dir ) <nl> + if (! git_dir ) { <nl> git_dir = read_gitfile_gently ( DEFAULT_GIT_DIR_ENVIRONMENT ); <nl> + git_dir = git_dir ? xstrdup ( git_dir ) : NULL ; <nl> + } <nl> if (! git_dir ) <nl> git_dir = DEFAULT_GIT_DIR_ENVIRONMENT ; <nl> git_object_dir = getenv ( DB_ENVIRONMENT );
static void print_summary ( const char * prefix , const unsigned char * sha1 ) <nl> rev . show_root_diff = 1 ; <nl> rev . commit_format = get_commit_format (" format :% h : % s "); <nl> rev . always_show_header = 0 ; <nl> + diff_setup_done (& rev . diffopt ); <nl>  <nl> printf (" Created % scommit ", initial_commit ? " initial " : ""); <nl> 
static void copy_templates ( const char * git_dir , int len , char * template_dir ) <nl> } <nl>  <nl> memcpy ( path , git_dir , len ); <nl> + path [ len ] = 0 ; <nl> copy_templates_1 ( path , len , <nl> template_path , template_len , <nl> dir );
static int fetch_object ( struct alt_base * repo , unsigned char * sha1 ) <nl> curl_result = curl_easy_perform ( curl ); <nl> curl_easy_setopt ( curl , CURLOPT_HTTPHEADER , no_range_header ); <nl> if ( curl_result != 0 ) { <nl> - unlink ( tmpfile ); <nl> return error ("% s ", curl_errorstr ); <nl> } <nl> 
arg_new_interval ( struct mail_search_build_context * ctx , <nl> } <nl> sarg -> value . search_flags = MAIL_SEARCH_ARG_FLAG_USE_TZ ; <nl> sarg -> value . time = ioloop_time - interval ; <nl> + sarg -> value . date_type = MAIL_SEARCH_DATE_TYPE_RECEIVED ; <nl> return sarg ; <nl> } <nl> 
static void cmd_user_ver2 ( struct doveadm_cmd_context * cctx ) <nl> ( void ) doveadm_cmd_param_str ( cctx , " field ", & show_field ); <nl> ( void ) doveadm_cmd_param_bool ( cctx , " userdb - only ", & userdb_only ); <nl>  <nl> + memset (& input , 0 , sizeof ( input )); <nl> if ( doveadm_cmd_param_array ( cctx , " auth - info ", & optval )) <nl> for (;* optval != NULL ; optval ++) <nl> auth_user_info_parse (& input . info , * optval );
int fts_expunge_log_uid_count ( struct fts_expunge_log * log , <nl> { <nl> int ret ; <nl>  <nl> - if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) <nl> + if (( ret = fts_expunge_log_reopen_if_needed ( log , FALSE )) <= 0 ) { <nl> + * expunges_r = 0 ; <nl> return ret ; <nl> + } <nl>  <nl> return fts_expunge_log_read_expunge_count ( log , expunges_r ); <nl> }
static void imapc_connection_set_state ( struct imapc_connection * conn , <nl>  <nl> conn -> selecting_box = NULL ; <nl> conn -> selected_box = NULL ; <nl> + <nl> + i_free ( conn -> ips ); <nl> + conn -> ips_count = 0 ; <nl> break ; <nl> default : <nl> break ;
bool passdb_get_credentials ( struct auth_request * auth_request , <nl> /* anything goes . change the credentials_scheme to what we <nl> actually got , so blocking passdbs work . */ <nl> auth_request -> credentials_scheme = <nl> - p_strdup ( auth_request -> pool , input_scheme ); <nl> + p_strdup ( auth_request -> pool , t_strcut ( input_scheme , '.')); <nl> return TRUE ; <nl> } <nl> 
struct ostream * fs_write_stream ( struct fs_file * file ) <nl>  <nl> int fs_write_stream_finish ( struct fs_file * file , struct ostream ** output ) <nl> { <nl> - i_assert (* output == file -> output ); <nl> + i_assert (* output == file -> output || * output == NULL ); <nl>  <nl> * output = NULL ; <nl> return file -> fs -> v . write_stream_finish ( file , TRUE );
void smtp_server_connection_data_chunk_init ( struct smtp_server_cmd_ctx * cmd ) <nl> command -> hook_replied = cmd_data_chunk_replied ; <nl> command -> hook_destroy = cmd_data_destroy ; <nl>  <nl> - if ( conn -> state . data_chain == NULL ) { <nl> + if (! conn -> state . data_failed && conn -> state . data_chain == NULL ) { <nl> i_assert ( data_cmd -> chunk_first ); <nl> i_assert ( conn -> state . data_chain_input == NULL ); <nl> conn -> state . data_chain_input =
imapc_sync_send_commands ( struct imapc_sync_context * ctx , uint32_t first_uid ) <nl> { <nl> string_t * cmd = t_str_new ( 64 ); <nl>  <nl> + if ( ctx -> mbox -> exists_count == 0 ) { <nl> + /* empty mailbox - no point in fetching anything */ <nl> + return ; <nl> + } <nl> + <nl> str_printfa ( cmd , " UID FETCH % u :* ( FLAGS ", first_uid ); <nl> if ( imapc_mailbox_has_modseqs ( ctx -> mbox )) { <nl> str_append ( cmd , " MODSEQ ");
int hash_format_init ( const char * format_string , struct hash_format ** format_r , <nl> } T_END ; <nl> if ( ret < 0 ) { <nl> * error_r = t_strdup (* error_r ); <nl> + pool_unref (& pool ); <nl> return - 1 ; <nl> } <nl> * format_r = format ;
static void o_stream_metawrap_call_callback ( struct metawrap_ostream * mstream ) <nl> if ( write_callback != NULL ) { <nl> mstream -> write_callback = NULL ; <nl> write_callback ( mstream -> context ); <nl> + /* metadata headers aren ' t counted as part of the offset */ <nl> + mstream -> ostream . ostream . offset = 0 ; <nl> } <nl> } <nl> 
static int dbox_sync_index_rebuild_dir ( struct dbox_sync_rebuild_context * ctx , <nl> dir = opendir ( path ); <nl> if ( dir == NULL ) { <nl> if ( errno == ENOENT ) { <nl> + if (! primary ) { <nl> + /* alt directory doesn ' t exist , ignore */ <nl> + return 0 ; <nl> + } <nl> mailbox_set_deleted (& ctx -> mbox -> ibox . box ); <nl> return - 1 ; <nl> }
static int raw_sync ( struct raw_mailbox * mbox ) <nl> MAIL_INDEX_SYNC_FLAG_FLUSH_DIRTY | <nl> MAIL_INDEX_SYNC_FLAG_REQUIRE_CHANGES ; <nl>  <nl> + if ( mail_index_view_get_messages_count ( mbox -> box . view ) > 0 ) { <nl> + /* already - synced index was opened via <nl> + mail - index - alloc - cache . */ <nl> + return 0 ; <nl> + } <nl> + <nl> ret = mail_index_sync_begin ( mbox -> box . index , & index_sync_ctx , <nl> & sync_view , & trans , sync_flags ); <nl> if ( ret <= 0 ) {
static void smtp_server_connection_input ( struct connection * _conn ) <nl> bool smtp_server_connection_pending_command_data ( <nl> struct smtp_server_connection * conn ) <nl> { <nl> + if ( conn -> smtp_parser == NULL ) <nl> + return FALSE ; <nl> return smtp_command_parser_pending_data ( conn -> smtp_parser ); <nl> } <nl> 
fs_list_get_path ( struct mailbox_list * _list , const char * name , <nl> i_assert ( mailbox_list_is_valid_name ( _list , name , & error )); <nl>  <nl> if ( mailbox_list_try_get_absolute_path ( _list , & name )) { <nl> + if ( type == MAILBOX_LIST_PATH_TYPE_INDEX && <nl> + * set -> index_dir == '\ 0 ') <nl> + return 0 ; <nl> * path_r = name ; <nl> return 1 ; <nl> }
static bool pop3_uidl_assign_by_size ( struct mailbox * box ) <nl> struct imap_msg_map * imap_map ; <nl> unsigned int i , pop3_count , imap_count , count ; <nl>  <nl> + if ( mstorage -> skip_size_check ) <nl> + return FALSE ; <nl> + <nl> pop3_map = array_get_modifiable (& mstorage -> pop3_uidl_map , & pop3_count ); <nl> imap_map = array_get_modifiable (& mbox -> imap_msg_map , & imap_count ); <nl> count = I_MIN ( pop3_count , imap_count );
static void tview_lookup_seq_range ( struct mail_index_view * view , <nl> if ( first_uid <= rec -> uid ) <nl> break ; <nl> } <nl> - if ( seq > tview -> t -> last_new_seq ) { <nl> + if ( seq > tview -> t -> last_new_seq || rec -> uid > last_uid ) { <nl> /* no messages in range */ <nl> return ; <nl> }
const struct stat * i_stream_stat ( struct istream * stream ) <nl> { <nl> struct _istream * _stream = stream -> real_stream ; <nl>  <nl> + if ( stream -> closed ) <nl> + return NULL ; <nl> + <nl> return _stream -> stat ( _stream ); <nl> } <nl> 
static bool outofmem = FALSE ; <nl>  <nl> static union { <nl> struct stack_block block ; <nl> - unsigned char data [ 128 ]; <nl> + unsigned char data [ 512 ]; <nl> } outofmem_area ; <nl>  <nl> static void data_stack_last_buffer_reset ( bool preserve_data ATTR_UNUSED ) <nl> static void free_blocks ( struct stack_block * block ) <nl> unused_block = block ; <nl> } else { <nl> # ifndef USE_GC <nl> - free ( block ); <nl> + if ( block != & outofmem_area . block ) <nl> + free ( block ); <nl> # endif <nl> } <nl> 
fts_backend_solr_update_deinit ( struct fts_backend_update_context * _ctx ) <nl> visible to the following search */ <nl> if ( ctx -> expunges ) <nl> fts_backend_solr_expunge_flush ( ctx ); <nl> - str = t_strdup_printf ("< commit waitFlush =\" false \" " <nl> - " waitSearcher =\"% s \"/>", <nl> + str = t_strdup_printf ("< commit waitSearcher =\"% s \"/>", <nl> ctx -> documents_added ? " true " : " false "); <nl> if ( solr_connection_post ( solr_conn , str ) < 0 ) <nl> ret = - 1 ;
int rfc822_parse_phrase ( struct rfc822_parser_context * ctx , string_t * str ) <nl> obs - phrase = word *( word / "." / CFWS ) <nl> */ <nl>  <nl> + if ( ctx -> data == ctx -> end ) <nl> + return 0 ; <nl> if (* ctx -> data == '.') <nl> return - 1 ; <nl> 
dsync_mailbox_find_common_uid ( struct dsync_mailbox_importer * importer , <nl> } <nl> return ; <nl> } <nl> - if ( importer -> revert_local_changes ) { <nl> + if ( importer -> revert_local_changes && <nl> + change -> type != DSYNC_MAIL_CHANGE_TYPE_EXPUNGE ) { <nl> dsync_mailbox_revert_missing ( importer , change ); <nl> * result_r = " Reverting local change by deleting mailbox "; <nl> } else if ( dsync_mailbox_find_common_expunged_uid ( importer , change )) {
void io_loop_handler_init ( struct ioloop * ioloop ) <nl> data -> fd_index = p_new ( ioloop -> pool , struct io_list *, data -> idx_size ); <nl>  <nl> data -> epfd = epoll_create ( INITIAL_EPOLL_EVENTS ); <nl> + if ( data -> epfd < 0 ) <nl> + i_panic (" epoll_create (): % m "); <nl> } <nl>  <nl> void io_loop_handler_deinit ( struct ioloop * ioloop )
static int snarf ( struct mailbox * srcbox , struct mailbox * destbox ) <nl> enum mail_error error ; <nl> int ret ; <nl>  <nl> + /* make sure the destination mailbox has been opened */ <nl> + if ( mailbox_open ( destbox ) < 0 ) <nl> + return - 1 ; <nl> + <nl> if ( mailbox_sync ( srcbox , MAILBOX_SYNC_FLAG_FULL_READ ) < 0 ) <nl> return - 1 ; <nl> 
static void db_ldap_get_fd ( struct ldap_connection * conn ) <nl> i_fatal (" LDAP : Can ' t get connection fd : % s ", <nl> ldap_err2string ( ret )); <nl> } <nl> + if ( conn -> fd <= CLIENT_LISTEN_FD ) { <nl> + /* Solaris LDAP library seems to be broken */ <nl> + i_fatal (" LDAP : Buggy LDAP library returned wrong fd : % d ", <nl> + conn -> fd ); <nl> + } <nl> i_assert ( conn -> fd != - 1 ); <nl> net_set_nonblock ( conn -> fd , TRUE ); <nl> }
db_oauth2_introspect_continue ( struct oauth2_introspection_result * result , <nl>  <nl> if (! result -> success ) { <nl> /* fail here */ <nl> + req -> result = PASSDB_RESULT_INTERNAL_FAILURE ; <nl> req -> failed = TRUE ; <nl> db_oauth2_callback ( req , FALSE , result -> error ); <nl> return ;
penalty_bump_checksum ( struct penalty_rec * rec , unsigned int checksum ) <nl> for ( i = 0 ; i < count ; i ++) { <nl> if ( checksums [ i ] == checksum ) { <nl> if ( i > 0 ) { <nl> - memcpy ( checksums + 1 , checksums , <nl> - sizeof ( checksums [ 0 ]) * i ); <nl> + memmove ( checksums + 1 , checksums , <nl> + sizeof ( checksums [ 0 ]) * i ); <nl> checksums [ 0 ] = checksum ; <nl> } <nl> return TRUE ;
int openssl_cert_match_name ( SSL * ssl , const char * verify_name ) <nl> } <nl> } <nl> sk_GENERAL_NAME_pop_free ( gnames , GENERAL_NAME_free ); <nl> + X509_free ( cert ); <nl> + <nl> /* verify against CommonName only when there wasn ' t any DNS <nl> SubjectAltNames */ <nl> if ( dns_names )
cmd_append_handle_args ( struct client_command_context * cmd , <nl> /* invalid keywords - delay failure */ <nl> client_send_box_error ( cmd , ctx -> box ); <nl> ctx -> failed = TRUE ; <nl> + keywords = NULL ; <nl> } <nl> } <nl> 
int index_storage_search_deinit ( struct mail_search_context * _ctx ) <nl> array_free (& ctx -> mail_ctx . results ); <nl> array_free (& ctx -> mail_ctx . module_contexts ); <nl>  <nl> - array_foreach_modifiable (& ctx -> mails , mailp ) <nl> + array_foreach_modifiable (& ctx -> mails , mailp ) { <nl> + struct index_mail * imail = ( struct index_mail *)* mailp ; <nl> + <nl> + imail -> search_mail = FALSE ; <nl> mail_free ( mailp ); <nl> + } <nl> array_free (& ctx -> mails ); <nl> i_free ( ctx ); <nl> return ret ;
int shared_storage_get_namespace ( struct mail_namespace ** _ns , <nl> /* this user doesn ' t have a usable storage */ <nl> new_ns -> flags |= NAMESPACE_FLAG_UNUSABLE ; <nl> } <nl> + /* mark the shared namespace root as usable , since it now has <nl> + child namespaces */ <nl> + ns -> flags |= NAMESPACE_FLAG_USABLE ; <nl> * _name = mailbox_list_get_storage_name ( new_ns -> list , <nl> t_strconcat ( new_ns -> prefix , name , NULL )); <nl> * _ns = new_ns ;
void sdbox_update_header ( struct sdbox_mailbox * mbox , <nl> mail_index_update_header_ext ( trans , mbox -> hdr_ext_id , 0 , <nl> & new_hdr , sizeof ( new_hdr )); <nl> } <nl> + memcpy ( mbox -> mailbox_guid , new_hdr . mailbox_guid , <nl> + sizeof ( mbox -> mailbox_guid )); <nl> } <nl>  <nl> static int sdbox_mailbox_create_indexes ( struct mailbox * box ,
imapc_command_begin ( imapc_command_callback_t * callback , void * context ) <nl> struct imapc_command * cmd ; <nl> pool_t pool ; <nl>  <nl> + i_assert ( callback != NULL ); <nl> + <nl> pool = pool_alloconly_create (" imapc command ", 2048 ); <nl> cmd = p_new ( pool , struct imapc_command , 1 ); <nl> cmd -> pool = pool ;
ssize_t i_stream_decrypt_read_header_v1 ( struct decrypt_istream * stream , <nl> buffer_t * key = buffer_create_dynamic ( pool_datastack_create (), 256 ); <nl>  <nl> hdr_len = (( data [ 0 ] << 8 ) | data [ 1 ]) + 12 ; <nl> - if ( mlen < hdr_len ) { <nl> + <nl> + if ( mlen < hdr_len - pos ) { <nl> /* try to read more */ <nl> return 0 ; <nl> }
static int auth_master_run_cmd ( struct auth_master_connection * conn , <nl> io_loop_run ( conn -> ioloop ); <nl> } <nl>  <nl> - auth_master_unset_io ( conn , prev_ioloop ); <nl> + if ( prev_ioloop != NULL ) <nl> + auth_master_unset_io ( conn , prev_ioloop ); <nl> if ( conn -> aborted ) { <nl> conn -> aborted = FALSE ; <nl> auth_connection_close ( conn );
int i_getpwnam ( const char * name , struct passwd * pwd_r ) <nl> errno = getpwnam_r ( name , pwd_r , pwbuf , pwbuf_size , & result ); <nl> if ( result != NULL ) <nl> return 1 ; <nl> + if ( errno == EINVAL ) { <nl> + /* FreeBSD fails here when name =" user @ domain " */ <nl> + return 0 ; <nl> + } <nl> return errno == 0 ? 0 : - 1 ; <nl> } <nl> 
ssize_t i_stream_read ( struct istream * stream ) <nl> errno = stream -> stream_errno ; <nl> } else { <nl> i_assert ( stream -> eof ); <nl> + i_assert ( old_size == _stream -> pos - _stream -> skip ); <nl> } <nl> break ; <nl> case 0 :
fts_search_arg_create_or ( const struct mail_search_arg * orig_arg , pool_t pool , <nl> array_foreach ( tokens , tokenp ) { <nl> arg = p_new ( pool , struct mail_search_arg , 1 ); <nl> * arg = * orig_arg ; <nl> + arg -> match_not = FALSE ; /* we copied this to the parent SUB */ <nl> arg -> next = NULL ; <nl> arg -> value . str = p_strdup ( pool , * tokenp ); <nl> 
static void hook_build_update ( struct hook_build_context * ctx , void * _vlast ) <nl> void (** vlast )() = _vlast ; <nl> struct hook_stack * stack ; <nl>  <nl> + if ( ctx -> tail -> vfuncs == vlast ) { <nl> + /* no vfuncs overridden */ <nl> + return ; <nl> + } <nl> + <nl> /* ctx -> vfuncs_stack -> vfuncs points to the root vfuncs , <nl> ctx -> vfuncs_stack -> next -> vfuncs points to the first super function <nl> that is being called , and so on .
uid_range_to_seqs ( struct fts_search_context * fctx , <nl> if (! array_is_created ( seq_range )) <nl> p_array_init ( seq_range , fctx -> result_pool , count ); <nl> for ( i = 0 ; i < count ; i ++) { <nl> + if ( range [ i ]. seq1 > range [ i ]. seq2 ) <nl> + continue ; <nl> mailbox_get_seq_range ( fctx -> box , range [ i ]. seq1 , range [ i ]. seq2 , <nl> & seq1 , & seq2 ); <nl> if ( seq1 != 0 )
mail_index_sync_ext_atomic_inc ( struct mail_index_sync_map_ctx * ctx , <nl> ext -> record_size ); <nl> return - 1 ; <nl> } <nl> - if ( u -> diff < 0 && ( uint32_t )(- u -> diff ) > orig_num ) { <nl> + if ( u -> diff < 0 && ( uint64_t )(- u -> diff ) > orig_num ) { <nl> mail_index_sync_set_corrupted ( ctx , <nl> " Extension record inc drops number below zero " <nl> "( uid =% u , diff =% d , orig =% llu )",
void io_loop_set_current ( struct ioloop * ioloop ) <nl> io_switch_callback_t * const * callbackp ; <nl> struct ioloop * prev_ioloop = current_ioloop ; <nl>  <nl> + if ( ioloop == current_ioloop ) <nl> + return ; <nl> + <nl> current_ioloop = ioloop ; <nl> if ( array_is_created (& io_switch_callbacks )) { <nl> array_foreach (& io_switch_callbacks , callbackp )
void ssl_iostream_destroy ( struct ssl_iostream ** _ssl_io ) <nl> { <nl> struct ssl_iostream * ssl_io = * _ssl_io ; <nl>  <nl> + if ( _ssl_io == NULL || * _ssl_io == NULL ) <nl> + return ; <nl> + <nl> + ssl_io = * _ssl_io ; <nl> * _ssl_io = NULL ; <nl> ssl_vfuncs -> destroy ( ssl_io ); <nl> }
mail_storage_service_init_post ( struct mail_storage_service_ctx * ctx , <nl> if ( errno == EACCES ) { <nl> i_error ("% s ", eacces_error_get (" chdir ", <nl> t_strconcat ( home , "/", NULL ))); <nl> - } if ( errno != ENOENT ) <nl> + } else if ( errno != ENOENT ) <nl> i_error (" chdir (% s ) failed : % m ", home ); <nl> else if ( mail_set -> mail_debug ) <nl> i_debug (" Home dir not found : % s ", home );
static void push_notification_event_mailboxunsubscribe_event ( <nl>  <nl> data = p_new ( ptxn -> pool , <nl> struct push_notification_event_mailboxunsubscribe_data , 1 ); <nl> - data -> subscribe = TRUE ; <nl> + data -> subscribe = FALSE ; <nl>  <nl> push_notification_txn_mbox_set_eventdata ( ptxn , mbox , ec , data ); <nl> }
static void list_send ( struct list_send_context * ctx , struct list_node * node , <nl> name = node -> name ; <nl> send_name = name ; <nl>  <nl> - if ( node -> flags != MAILBOX_PLACEHOLDER ) <nl> + if ( node -> flags != MAILBOX_PLACEHOLDER && <nl> + node -> flags != MAILBOX_NOSELECT ) <nl> match = IMAP_MATCH_YES ; <nl> else { <nl> /* make sure the placeholder matches . */
quota_mailbox_delete_shrink_quota ( struct mailbox * box ) <nl> struct mail * mail ; <nl> struct mail_search_args * search_args ; <nl>  <nl> + if ( mailbox_mark_index_deleted ( box , TRUE ) < 0 ) <nl> + return - 1 ; <nl> + <nl> t = mailbox_transaction_begin ( box , 0 ); <nl> qt = quota_transaction_begin ( box ); <nl> 
password_scheme_detect ( const char * plain_password , const char * crypted_password , <nl> break ; <nl> key = NULL ; <nl> } <nl> + hash_table_iterate_deinit (& ctx ); <nl> return key ; <nl> } <nl> 
int http_header_parse_next_field ( struct http_header_parser * parser , <nl> const uoff_t max_size = parser -> limits . max_size ; <nl> const uoff_t max_field_size = parser -> limits . max_field_size ; <nl> const unsigned char * data ; <nl> - uoff_t size ; <nl> + size_t size ; <nl> int ret ; <nl>  <nl> * error_r = NULL ;
acl_backend_vfile_get_local_dir ( struct acl_backend * backend , const char * name ) <nl> dir = mailbox_list_get_path ( ns -> list , name , <nl> MAILBOX_LIST_PATH_TYPE_MAILBOX ); <nl> } <nl> - if ( name == NULL ) { <nl> + if ( name == NULL && dir != NULL ) { <nl> /* verify that the directory isn ' t same as INBOX ' s directory . <nl> this is mainly for Maildir . */ <nl> inbox = mailbox_list_get_path ( ns -> list , " INBOX ",
imapc_mail_get_stream ( struct mail * _mail , bool get_body , <nl> mail_set_aborted ( _mail ); <nl> return - 1 ; <nl> } <nl> + if ( _mail -> expunged ) { <nl> + /* We already detected that the mail is expunged . <nl> + Don ' t spend time trying to FETCH it again . */ <nl> + mail_set_expunged ( _mail ); <nl> + return - 1 ; <nl> + } <nl> fetch_field = get_body || <nl> ( data -> access_part & READ_BODY ) != 0 ? <nl> MAIL_FETCH_STREAM_BODY : MAIL_FETCH_STREAM_HEADER ;
void i_set_failure_prefix ( const char * prefix ) <nl> { <nl> i_free ( log_prefix ); <nl> log_prefix = i_strdup ( prefix ); <nl> - i_warning (" new prefix =% s ", prefix ); <nl> } <nl>  <nl> static int ATTR_FORMAT ( 2 , 0 )
static int mail_index_open_init ( MailIndex * index , int update_recent , <nl> index -> set_flags |= MAIL_INDEX_FLAG_REBUILD ; <nl> } <nl>  <nl> - return TRUE ; <nl> + /* finally reset the modify log marks , fsck or syncing might have <nl> + deleted some messages , and since we ' re only just opening the <nl> + index , there ' s no need to remember them */ <nl> + return mail_modifylog_mark_synced ( index -> modifylog ); <nl> } <nl>  <nl> static int mail_index_open_file ( MailIndex * index , const char * filename ,
static bool remote_ip_is_usable ( const struct ip_addr * ip ) <nl> return FALSE ; /* 10 / 8 */ <nl> if ( addr >= 3232235520 && addr <= 3232301055 ) <nl> return FALSE ; /* 192 . 168 / 16 */ <nl> + if ( addr >= 2886729728 && addr <= 2887778303 ) <nl> + return FALSE ; /* 172 . 16 / 12 */ <nl> if ( addr >= 2130706432 && addr <= 2147483647 ) <nl> return FALSE ; /* 127 / 8 */ <nl> }
static void <nl> read_ident_reply ( rb_fde_t * F , void * data ) <nl> { <nl> struct auth_client * auth = data ; <nl> - char buf [ IDENT_BUFSIZE + 1 ]; /* buffer to read auth reply into */ <nl> + char buf [ IDENT_BUFSIZE + 1 ] = { 0 }; /* buffer to read auth reply into */ <nl> ident_message message = REPORT_FAIL ; <nl> char * s = NULL ; <nl> char * t = NULL ;
lookup_ip ( const char * addr , int aftype , DNSCB callback , void * data ) <nl> return ( rid ); <nl> } <nl>  <nl> - uint32_t <nl> + static uint32_t <nl> get_nameservers ( DNSLISTCB callback , void * data ) <nl> { <nl> struct dnsstatreq * req = rb_malloc ( sizeof ( struct dnsstatreq ));
add_conf_item ( const char * topconf , const char * name , int type , void (* func ) ( voi <nl> if (( tc = find_top_conf ( topconf )) == NULL ) <nl> return - 1 ; <nl>  <nl> - if (( cf = find_conf_item ( tc , name )) != NULL ) <nl> + if ( find_conf_item ( tc , name ) != NULL ) <nl> return - 1 ; <nl>  <nl> cf = rb_malloc ( sizeof ( struct ConfEntry ));
rb_get_ssl_certfp ( rb_fde_t * F , uint8_t certfp [ RB_SSL_CERTFP_LEN ]) <nl> res == X509_V_ERR_DEPTH_ZERO_SELF_SIGNED_CERT ) <nl> { <nl> memcpy ( certfp , cert -> sha1_hash , RB_SSL_CERTFP_LEN ); <nl> + X509_free ( cert ); <nl> return 1 ; <nl> } <nl> X509_free ( cert );
void Curl_ossl_md5sum ( unsigned char * tmp , /* input */ <nl>  <nl> bool Curl_ossl_cert_status_request ( void ) <nl> { <nl> +# if ! defined ( HAVE_BORINGSSL ) && ! defined ( OPENSSL_NO_TLSEXT ) <nl> return TRUE ; <nl> +# else <nl> + return FALSE ; <nl> +# endif <nl> } <nl> # endif /* USE_SSLEAY */
static int ldap_win_bind_auth ( LDAP * server , const char * user , <nl> const char * passwd , unsigned long authflags ) <nl> { <nl> ULONG method = 0 ; <nl> - SEC_WINNT_AUTH_IDENTITY cred = { 0 , }; <nl> + SEC_WINNT_AUTH_IDENTITY cred ; <nl> int rc = LDAP_AUTH_METHOD_NOT_SUPPORTED ; <nl>  <nl> + memset (& cred , 0 , sizeof ( cred )); <nl> + <nl> # if defined ( USE_SPNEGO ) <nl> if ( authflags & CURLAUTH_NEGOTIATE ) { <nl> method = LDAP_AUTH_NEGOTIATE ;
static CURLcode file_range ( struct connectdata * conn ) <nl> else { <nl> /* X - Y */ <nl> totalsize = to - from ; <nl> + if ( totalsize == CURL_OFF_T_MAX ) <nl> + /* this is too big to increase , so bail out */ <nl> + return CURLE_RANGE_ERROR ; <nl> data -> req . maxdownload = totalsize + 1 ; /* include last byte */ <nl> data -> state . resume_from = from ; <nl> DEBUGF ( infof ( data , " RANGE from %" CURL_FORMAT_CURL_OFF_T
static CURLcode imap_statemach_act ( struct connectdata * conn ) <nl> if ( result ) <nl> return result ; <nl>  <nl> + /* Was there an error parsing the response line ? */ <nl> + if ( imapcode == - 1 ) <nl> + return CURLE_FTP_WEIRD_SERVER_REPLY ; <nl> + <nl> if ( imapcode ) { <nl> /* We have now received a full IMAP server response */ <nl> switch ( imapc -> state ) {
int cert_stuff ( struct connectdata * conn , <nl> EVP_PKEY_free ( pktmp ); <nl> } <nl>  <nl> -# if ! defined ( OPENSSL_NO_RSA ) <nl> +# if ! defined ( OPENSSL_NO_RSA ) && ! defined ( OPENSSL_IS_BORINGSSL ) <nl> { <nl> /* If RSA is used , don ' t check the private key if its flags indicate <nl> * it doesn ' t support it . */
int Curl_resolv_timeout ( struct connectdata * conn , <nl>  <nl> * entry = NULL ; <nl>  <nl> + if ( timeoutms < 0 ) <nl> + /* got an already expired timeout */ <nl> + return CURLRESOLV_TIMEDOUT ; <nl> + <nl> # ifdef USE_ALARM_TIMEOUT <nl> if ( data -> set . no_signal ) <nl> /* Ignore the timeout when signals are disabled */
static CURLcode smb_connect ( struct connectdata * conn , bool * done ) <nl>  <nl> ( void ) done ; <nl>  <nl> + /* Check we have a username and password to authenticate with */ <nl> + if (! conn -> bits . user_passwd ) <nl> + return CURLE_LOGIN_DENIED ; <nl> + <nl> /* Initialize the connection state */ <nl> memset ( smbc , 0 , sizeof (* smbc )); <nl> smbc -> state = SMB_CONNECTING ;
Curl_cookie_add ( struct Curl_easy * data , <nl> /* too long individual name or contents , or too long combination of <nl> name + contents . Chrome and Firefox support 4095 or 4096 bytes <nl> combo . */ <nl> - free ( co ); <nl> + freecookie ( co ); <nl> infof ( data , " oversized cookie dropped , name / val % d + % d bytes \ n ", <nl> nlen , len ); <nl> return NULL ;
_CURL_WARNING ( _curl_easy_getinfo_err_curl_socket , <nl> # endif <nl>  <nl> /* evaluates to true if expr is of type FILE * */ <nl> -# define _curl_is_FILE ( expr ) \ <nl> - ( __builtin_types_compatible_p ( __typeof__ ( expr ), FILE *)) <nl> +# define _curl_is_FILE ( expr ) \ <nl> + ( _curl_is_NULL ( expr ) || \ <nl> + ( __builtin_types_compatible_p ( __typeof__ ( expr ), FILE *))) <nl>  <nl> /* evaluates to true if expr can be passed as POST data ( void * or char *) */ <nl> # define _curl_is_postfields ( expr ) \
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> if ( res == CURLE_OK ) { <nl> bool retry = Curl_retry_request ( conn , & newurl ); <nl>  <nl> - if ( retry ) <nl> + if ( retry ) { <nl> follow = FOLLOW_RETRY ; <nl> + if (! newurl ) <nl> + res = CURLE_OUT_OF_MEMORY ; <nl> + } <nl> else { <nl> /* <nl> * We must duplicate the new URL here as the connection data may
void curl_mime_free ( curl_mime * mime ) <nl> curl_mimepart * part ; <nl>  <nl> if ( mime ) { <nl> + mime_subparts_unbind ( mime ); /* Be sure it ' s not referenced anymore . */ <nl> while ( mime -> firstpart ) { <nl> part = mime -> firstpart ; <nl> mime -> firstpart = part -> nextpart ;
static void read_tcp_data ( ares_channel channel , fd_set * read_fds , time_t now ) <nl> * what ' s left to read of it ). <nl> */ <nl> count = recv ( server -> tcp_socket , <nl> - ( void *)( server -> tcp_lenbuf + server -> tcp_buffer_pos ), <nl> - 2 - server -> tcp_buffer_pos , 0 ); <nl> + ( void *)( server -> tcp_lenbuf + server -> tcp_lenbuf_pos ), <nl> + 2 - server -> tcp_lenbuf_pos , 0 ); <nl> if ( count <= 0 ) <nl> { <nl> handle_error ( channel , i , now );
CURLcode Curl_ntlm_create_type1_message ( const char * userp , <nl> *( dup_domain . tchar_ptr + domlen ) = TEXT ('\ 0 '); <nl> ntlm -> identity . Domain = dup_domain . tbyte_ptr ; <nl> ntlm -> identity . DomainLength = curlx_uztoul ( domlen ); <nl> - free ( dup_domain . tchar_ptr ); <nl> dup_domain . tchar_ptr = NULL ; <nl>  <nl> Curl_unicodefree ( useranddomain . tchar_ptr );
void Curl_sasl_digest_cleanup ( struct digestdata * digest ) <nl> * This is used to generate an already encoded NTLM type - 1 message ready for <nl> * sending to the recipient . <nl> * <nl> -* Note : This is a simple wrapper of the NTLM function which means that any <nl> -* SASL based protocols don ' t have to include the NTLM functions directly . <nl> -* <nl> * Parameters : <nl> * <nl> * userp [ in ] - The user name in the format User or Domain \ User .
# define OS " AmigaOS " <nl>  <nl> # define PACKAGE " curl " <nl> -# define PACKAGE_BUGREPORT " a suitable curl mailing list : https :// curl . haxx . se / mail /" <nl> +# define PACKAGE_BUGREPORT " a suitable mailing list : https :// curl . haxx . se / mail /" <nl> # define PACKAGE_NAME " curl " <nl> # define PACKAGE_STRING " curl -" <nl> # define PACKAGE_TARNAME " curl "
static CURLcode ssh_statemach_act ( struct connectdata * conn , bool * block ) <nl> figure out a " real " bitmask */ <nl> sshc -> orig_waitfor = data -> req . keepon ; <nl>  <nl> + /* since we don ' t really wait for anything at this point , we want the <nl> + state machine to move on as soon as possible so we set a very short <nl> + timeout here */ <nl> + Curl_expire ( data , 1 ); <nl> + <nl> state ( conn , SSH_STOP ); <nl> } <nl> break ;
# undef ssize_t <nl>  <nl> /* Define this to ' int ' if socklen_t is not an available typedefed type */ <nl> -# undef socklen_t size_t <nl> +# define socklen_t size_t <nl>  <nl> /* Define this as a suitable file to read random data from */ <nl> # undef RANDOM_FILE
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> * an error , use the strerror () string or if things are so bad that not <nl> * even that is good , set a bad string that mentions the error code . <nl> */ <nl> - char * str = curl_easy_strerror ( res ); <nl> + const char * str = curl_easy_strerror ( res ); <nl> if (! str ) <nl> failf ( data , " unspecified error % d ", ( int ) res ); <nl> else
CURLMcode Curl_pipeline_set_site_blacklist ( char ** sites , <nl> bool Curl_pipeline_server_blacklisted ( struct SessionHandle * handle , <nl> char * server_name ) <nl> { <nl> - if ( handle -> multi ) { <nl> + if ( handle -> multi && server_name ) { <nl> struct curl_llist * blacklist = <nl> Curl_multi_pipelining_server_bl ( handle -> multi ); <nl> 
CURLcode Curl_is_resolved ( struct connectdata * conn , <nl> if ( conn -> async . done ) { <nl> /* we ' re done , kill the ares handle */ <nl> if (! conn -> async . dns ) { <nl> - failf ( data , " Could not resolve host : % s (% s )", conn -> name , <nl> + failf ( data , " Could not resolve host : % s (% s )", conn -> host . dispname , <nl> ares_strerror ( conn -> async . status )); <nl> return CURLE_COULDNT_RESOLVE_HOST ; <nl> }
void Curl_updateconninfo ( struct connectdata * conn , curl_socket_t sockfd ) <nl> struct SessionHandle * data = conn -> data ; <nl> struct PureInfo * info = & conn -> data -> info ; <nl>  <nl> + if ( conn -> bits . reuse ) <nl> + /* reusing same connection */ <nl> + return ; <nl> + <nl> len = sizeof ( struct Curl_sockaddr_storage ); <nl> if ( getpeername ( sockfd , ( struct sockaddr *) & ssrem , & len )) { <nl> error = SOCKERRNO ;
mbed_connect_step3 ( struct connectdata * conn , <nl>  <nl> ret = mbedtls_ssl_get_session (& connssl -> ssl , our_ssl_sessionid ); <nl> if ( ret ) { <nl> + free ( our_ssl_sessionid ); <nl> failf ( data , " mbedtls_ssl_get_session returned - 0x % x ", - ret ); <nl> return CURLE_SSL_CONNECT_ERROR ; <nl> }
int main ( int argc , char ** argv ) <nl> /* HTTP PUT please */ <nl> curl_easy_setopt ( curl , CURLOPT_PUT , TRUE ); <nl>  <nl> - /* specify target */ <nl> + /* specify target URL , and note that this URL should include a file <nl> + name , not only a directory */ <nl> curl_easy_setopt ( curl , CURLOPT_URL , url ); <nl>  <nl> /* now specify which file to upload */
CURLcode Curl_output_ntlm_wb ( struct connectdata * conn , <nl> conn -> response_header = NULL ; <nl> break ; <nl> case NTLMSTATE_TYPE2 : <nl> - input = aprintf (" TT % s ", conn -> challenge_header ); <nl> + input = aprintf (" TT % s \ n ", conn -> challenge_header ); <nl> if (! input ) <nl> return CURLE_OUT_OF_MEMORY ; <nl> res = ntlm_wb_response ( conn , input , ntlm -> state );
static GlobCode glob_range ( URLGlob * glob , char ** patternp , <nl> } <nl> else <nl> step_n = 1 ; <nl> - if (* endp == ']') { <nl> + if ( endp && (* endp == ']')) { <nl> pattern = endp + 1 ; <nl> } <nl> else
_hb_ot_layout_set_glyph_class ( hb_face_t * face , <nl> unsigned char * new_klasses ; <nl>  <nl> new_len = len == 0 ? 120 : 2 * len ; <nl> - if ( new_len > 65535 ) <nl> - new_len = 65535 ; <nl> + if ( new_len > 65536 ) <nl> + new_len = 65536 ; <nl> new_klasses = ( unsigned char *) realloc ( layout -> new_gdef . klasses , new_len * sizeof ( unsigned char )); <nl>  <nl> if ( HB_UNLIKELY (! new_klasses ))
int TMomFinalizeChild ( <nl> /* Put the script ' s arguments on the command line ( see configure option -- enable - shell - use - argv ). */ <nl> if ( TJE -> is_interactive == FALSE ) <nl> { <nl> - arg [ aindex ] = calloc ( 1 , <nl> + arg [ aindex ] = ( char *) calloc ( 1 , <nl> strlen ( path_jobs ) + <nl> strlen ( pjob -> ji_qs . ji_fileprefix ) + <nl> strlen ( JOB_SCRIPT_SUFFIX ) + 6 );
dynamic_string * prepare_mom_hierarchy () <nl> int fds ; <nl> dynamic_string * send_format = NULL ; <nl>  <nl> + mh = initialize_mom_hierarchy (); <nl> + <nl> if (( fds = open ( path_mom_hierarchy , O_RDONLY , 0 )) < 0 ) <nl> { <nl> if ( errno == ENOENT )
int req_stat_job ( <nl> */ <nl>  <nl> snprintf ( name , sizeof ( name ), "% s ", preq -> rq_ind . rq_status . rq_id ); <nl> - name [( PBS_MAXSVRJOBID > PBS_MAXDEST ? PBS_MAXSVRJOBID : PBS_MAXDEST )] = '\ 0 '; <nl> + name [ sizeof ( name ) - 1 ] = '\ 0 '; <nl>  <nl> if (( name [ 0 ] == '\ 0 ') || ( name [ 0 ] == '@')) <nl> {
START_TEST ( test_should_resend_obit ) <nl> pjob . ji_obit_sent = time_now ; <nl>  <nl> // Running jobs shouldn ' t re - send their obits <nl> - pjob . ji_qs . ji_substate == JOB_SUBSTATE_RUNNING ; <nl> + pjob . ji_qs . ji_substate = JOB_SUBSTATE_RUNNING ; <nl> fail_unless ( should_resend_obit (& pjob , diff ) == false ); <nl> pjob . ji_obit_busy_time = time_now - ( 2 * diff ); <nl> fail_unless ( should_resend_obit (& pjob , diff ) == false );
char * pbse_to_txt ( int err ) <nl> exit ( 1 ); <nl> } <nl>  <nl> + int trq_cg_remove_process_from_accts ( job * pjob ) <nl> + { <nl> + return ( PBSE_NONE ); <nl> + }
void * queue_route ( void * vp ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - void acct_close ( void ) <nl> + void acct_close ( bool ) <nl> { <nl> fprintf ( stderr , " The call to acct_close needs to be mocked !!\ n "); <nl> exit ( 1 );
void check_busy ( double mla ) <nl> exit ( 1 ); <nl> } <nl>  <nl> - void mom_is_request ( struct tcp_chan * chan , int version , int * cmdp ) <nl> + void mom_is_request ( struct tcp_chan * chan , int version , int * cmdp , struct sockaddr_in * pSockAddr ) <nl> { <nl> fprintf ( stderr , " The call to mom_is_request needs to be mocked !!\ n "); <nl> exit ( 1 );
int trq_main ( <nl> { <nl> printf (" Daemon exit requested \ n "); <nl> } <nl> - if ( trq_server_ip != NULL ) <nl> - free ( trq_server_ip ); <nl> if ( the_key != NULL ) <nl> free ( the_key ); <nl> return rc ;
bool Chip :: spread_place_cores ( <nl>  <nl> if ( fits == true ) <nl> { <nl> - int step_count = 1 ; <nl> + int step_count = step ; <nl> + <nl> + if ( lprocs_per_task_remaining == 1 ) <nl> + step_count = 1 ; <nl> + <nl>  <nl> /* cores_placed and cores_to_fill are used because we only want to make sure we <nl> fill the number of cores for this task */
int delete_cpuset ( <nl> */ <nl> else if (! strcmp ( pdirent -> d_name , " tasks ")) <nl> { <nl> + slept = 0 ; <nl> + <nl> do <nl> { <nl> npids = 0 ; <nl> - slept = 0 ; <nl> if (( fd = fopen ( path , " r ")) != NULL ) <nl> { <nl> while (( fgets ( tid , sizeof ( tid ), fd )) != NULL )
job * job_clone ( <nl> pnewjob -> ji_qs . ji_jobid [ PBS_MAXSVRJOBID ] = '\ 0 '; <nl> snprintf ( pnewjob -> ji_qs . ji_jobid , PBS_MAXSVRJOBID ,"% s -% d .% s ", <nl> oldid , taskid , hostname ); <nl> - <nl> + free ( oldid ); <nl> /* update the job filename <nl> * We could optimize the sub - jobs to all use the same file . We would need a <nl> * way to track the number of tasks still using the job file so we know when
void Chip :: calculateStepCounts ( <nl> int & place_count_remaining ) <nl>  <nl> { <nl> + if ( lprocs_per_task == 0 ) <nl> + { <nl> + step = 0 ; <nl> + step_remainder = processing_units_per_task ; <nl> + place_count = 0 ; <nl> + return ; <nl> + } <nl> + <nl> if ( lprocs_per_task == 1 ) <nl> { <nl> step = ( processing_units_per_task / 2 ) + 1 ;
int procs_requested ( <nl> if ( proplist (& str , & prop , & num_procs , & num_gpus , & num_mics )) <nl> { <nl> free ( tmp_spec ); <nl> + if ( prop != NULL ) <nl> + free_prop ( prop ); <nl> return (- 1 ); <nl> } <nl> } <nl> int procs_requested ( <nl> { <nl> /* must be a prop list with no number in front */ <nl> free ( tmp_spec ); <nl> + if ( prop != NULL ) <nl> + free_prop ( prop ); <nl>  <nl> return (- 1 ); <nl> }
 <nl> START_TEST ( test_one ) <nl> { <nl> + /* As this is site specific , there is no implementation in this function */ <nl> + char * user = NULL ; <nl> + char * host = NULL ; <nl> + int rc = - 1 ; <nl> + rc = site_allow_u ( user , host ); <nl> + fail_unless ( rc == 0 , " The return value has changed !!!"); <nl> } <nl> END_TEST <nl> 
work_task * next_task ( <nl> pthread_mutex_lock ( at -> alltasks_mutex ); <nl>  <nl> wt = next_thing ( at -> ra , iter ); <nl> + if ( wt != NULL ) <nl> + pthread_mutex_lock ( wt -> wt_mutex ); <nl>  <nl> pthread_mutex_unlock ( at -> alltasks_mutex ); <nl>  <nl> if ( wt != NULL ) <nl> { <nl> - pthread_mutex_lock ( wt -> wt_mutex ); <nl> - <nl> if ( wt -> wt_being_recycled == TRUE ) <nl> { <nl> pthread_mutex_unlock ( wt -> wt_mutex );
 <nl> # define MAX_UPDATES_BEFORE_SENDING 20 <nl> # define PMOMTCPTIMEOUT 60 /* duration in seconds mom TCP requests will block */ <nl> +# define TCP_READ_PROTO_TIMEOUT 2 <nl>  <nl> /* Global Data Items */ <nl>  <nl> int tcp_read_proto_version ( <nl>  <nl> tmpT = pbs_tcp_timeout ; <nl>  <nl> - pbs_tcp_timeout = 0 ; <nl> + pbs_tcp_timeout = TCP_READ_PROTO_TIMEOUT ; <nl>  <nl> * proto = disrsi ( chan , & rc ); <nl> 
int get_active_pbs_server ( <nl> } <nl> else if (( rc = socket_read_str ( local_socket , & read_buf , & read_buf_len )) != PBSE_NONE ) <nl> { <nl> + if ( read_buf != NULL ) <nl> + free ( read_buf ); <nl> + <nl> close ( local_socket ); <nl> return ( rc ); <nl> }
int read_config ( <nl> return ( 0 ); <nl> } <nl>  <nl> + void free_pwnam ( struct passwd * pwdp , char * buf ) <nl> + {}
void main_func ( <nl> script_tmp , /* O */ <nl> & ji )) != 0 ) <nl> { <nl> + fclose ( script_fp ); <nl> unlink ( script_tmp ); <nl>  <nl> exit ( 1 );
int pres_process_body ( publ_info_t * publ , str ** fin_body , int ver , str ** tuple_pa <nl> { <nl> if ( tuple == NULL ) <nl> { <nl> + if ( strlen ( tuple_id )>= 50 ) { <nl> + LM_ERR (" tuple id is too long : % s \ n ", tuple_id ); <nl> + goto error ; <nl> + } <nl> strcpy ( buf , tuple_id ); <nl> xmlFree ( tuple_id ); <nl> tuple_id = buf ;
__dialog_created ( struct dlg_cell * dlg , int type , struct dlg_cb_params * _params ) <nl> ( include_req_uri )?&( dlg -> req_uri ):&( dlg -> to_uri ), <nl> &( dlg -> callid ), 1 , dlginfo -> lifetime , <nl> 0 , 0 , 0 , 0 , ( send_publish_flag ==- 1 )? 1 : 0 ); <nl> - free_dlginfo_cell ( dlginfo ); <nl>  <nl> } <nl> 
int rx_send_str ( str * rx_session_id ) { <nl> // so just wait for STA or for Grace Timout to happen <nl> LM_DBG (" Hmmm , auth session already in disconnected state \ n "); <nl> cdpb . AAASessionsUnlock ( auth -> hash ); <nl> - CSCF_RETURN_FALSE ; <nl> + return CSCF_RETURN_FALSE ; <nl> } <nl>  <nl> LM_DBG (" Creating STR \ n ");
static int rtpproxy_set_store ( modparam_t type , void * val ){ <nl> return - 1 ; <nl> } <nl> } else {/* realloc to make room for the current set */ <nl> - rtpp_strings = ( char **) pkg_realloc ( rtpp_strings , <nl> + rtpp_strings = ( char **) pkg_reallocxf ( rtpp_strings , <nl> ( rtpp_sets + 1 )* sizeof ( char *)); <nl> if (! rtpp_strings ){ <nl> LM_ERR (" no pkg memory left \ n ");
int th_msg_received ( void * data ) <nl> { <nl> sip_msg_t msg ; <nl> str * obuf ; <nl> - char * nbuf ; <nl> + char * nbuf = NULL ; <nl> int direction ; <nl> int dialog ; <nl>  <nl> int th_msg_received ( void * data ) <nl> obuf -> s [ obuf -> len ] = '\ 0 '; <nl>  <nl> done : <nl> + if ( nbuf != NULL ) <nl> + pkg_free ( nbuf ); <nl> free_sip_msg (& msg ); <nl> return 0 ; <nl> }
dlg_cell_t * dlg_lookup ( unsigned int h_entry , unsigned int h_id ) <nl> dlg_cell_t * dlg ; <nl> dlg_entry_t * d_entry ; <nl>  <nl> + if ( d_table == NULL ) <nl> + return 0 ; <nl> + <nl> if ( h_entry >= d_table -> size ) <nl> goto not_found ; <nl> 
if ( rtplen != prtpstat -> len ) <nl> LM_ERR (" Unable to find RTPSTAT pv !\ n "); <nl> goto initerr ; <nl> } <nl> - prtp_pv = pv_cache_get (& prtpstat [ 0 ]); <nl> + prtp_pv = pv_cache_get ( prtpstat ); <nl> if (! prtp_pv ) <nl> { <nl> LM_ERR (" Unable to find pv spec for RTPSTAT !\ n ");
kz_amqp_zone_ptr kz_primary_zone = NULL ; <nl> amqp_exchange_declare_ok_t * AMQP_CALL kz_amqp_exchange_declare ( amqp_connection_state_t state , amqp_channel_t channel , <nl> amqp_bytes_t exchange , amqp_bytes_t type , <nl> amqp_boolean_t passive , amqp_boolean_t durable , amqp_table_t arguments ) { <nl> -# if AMQP_VERSION_MINOR == 5 <nl> +# if AMQP_VERSION_MAJOR == 0 && AMQP_VERSION_MINOR < 6 <nl> return amqp_exchange_declare ( state , channel , exchange , type , passive , durable , arguments ); <nl> # else <nl> return amqp_exchange_declare ( state , channel , exchange , type , passive , durable , 0 , 0 , arguments );
sl_api_t slb ; <nl> /** module variables */ <nl> str dmq_request_method = str_init (" KDMQ "); <nl> dmq_worker_t * workers ; <nl> - dmq_peer_list_t * peer_list ; <nl> + dmq_peer_list_t * peer_list = 0 ; <nl> /* the list of dmq servers */ <nl> dmq_node_list_t * node_list ; <nl> // the dmq module is a peer itself for receiving notifications regarding nodes
static int init_mi_uptime ( void ) <nl> { <nl> char * p ; <nl>  <nl> + if ( kmi_up_since_ctime . s != 0 ) <nl> + return 0 ; <nl> time (& kmi_up_since ); <nl> p = ctime (& kmi_up_since ); <nl> kmi_up_since_ctime . len = strlen ( p )- 1 ;
int t_load_contacts ( struct sip_msg * msg , char * key , char * value ) <nl> return - 1 ; <nl> } <nl>  <nl> + memset ( next , 0 , sizeof ( struct contact )); <nl> next -> uri . s = branch -> uri ; <nl> next -> uri . len = branch -> len ; <nl> next -> dst_uri . s = branch -> dst_uri ;
int db_postgres_store_result ( const db1_con_t * _con , db1_res_t ** _r ) <nl> } <nl>  <nl> done : <nl> - db_postgres_free_query ( _con ); <nl> return ( rc ); <nl> } <nl> 
static int child_init ( int _rank ) <nl> dlist_t * ptr ; <nl> int i ; <nl>  <nl> + if ( sruid_init (& _ul_sruid , '-', " ulcx ", SRUID_INC )< 0 ) <nl> + return - 1 ; <nl> + <nl> if ( _rank == PROC_MAIN && ul_timer_procs > 0 ) <nl> { <nl> for ( i = 0 ; i < ul_timer_procs ; i ++)
static int mi_child_init ( void ) <nl> return - 1 ; <nl> } <nl> } <nl> + <nl> + if ( sruid_init (& _ul_sruid , '-', " ulcx ", SRUID_INC )< 0 ) <nl> + return - 1 ; <nl> done = 1 ; <nl>  <nl> return 0 ;
void rpc_shv_set ( rpc_t * rpc , void * c ) <nl> rpc -> fault ( c , 500 , " Cannot set shared variable value "); <nl> LM_ERR (" cannot set shv value \ n "); <nl> } else { <nl> - rpc -> printf ( c , " Ok . Variable set to new value ."); <nl> + rpc -> rpl_printf ( c , " Ok . Variable set to new value ."); <nl> } <nl>  <nl> unlock_shvar ( shv );
else <nl> { <nl> LM_ERR ("% sCall (% s ) REFER error (% d )", pfncname , <nl> pcall -> call_from , nreply ); <nl> - pcall -> call_state = CLSTA_INQUEUE ; <nl> - update_call_rec ( pcall ); <nl> + if ( nreply == 481 ) <nl> + { delete_call ( pcall ); } <nl> + else <nl> + { <nl> + pcall -> call_state = CLSTA_INQUEUE ; <nl> + update_call_rec ( pcall ); <nl> + } <nl> } <nl> return ; <nl> }
static struct timeval time_from_string ( str * time_value ) <nl> return time_error ; <nl> } <nl>  <nl> - return ( struct timeval ) { atoi ( zero_terminated_value ), <nl> - atoi ( dot_address + 1 )}; <nl> + time_res -> tv_sec = strtol ( zero_terminated_value , ( char **) NULL , 10 ); <nl> + time_res -> tv_usec = strtol ( dot_address + 1 , ( char **) NULL , 10 ); <nl> + return 0 ; <nl> } <nl>  <nl> /* set the duration in the dialog struct */
struct module_exports exports = { <nl> static int mod_init ( void ) { <nl> struct stat fs ; <nl>  <nl> + if ( register_mi_mod ( exports . name , mi_cmds )!= 0 ) <nl> + { <nl> + LM_ERR (" failed to register MI commands \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> subscriber_table . len = strlen ( subscriber_table . s ); <nl> subscriber_username_col . len = strlen ( subscriber_username_col . s ); <nl> subscriber_domain_col . len = strlen ( subscriber_domain_col . s );
static int do_load_gws ( struct sip_msg * _m , int grp_id ) <nl> } <nl> from_uri = get_from ( _m )-> uri ; <nl> } <nl> - if ( from_uri . len < MAX_FROM_URI_LEN ) { <nl> + if ( from_uri . len <= MAX_FROM_URI_LEN ) { <nl> strncpy ( from_uri_str , from_uri . s , from_uri . len ); <nl> from_uri_str [ from_uri . len ] = '\ 0 '; <nl> } else {
int tmx_check_pretran ( sip_msg_t * msg ) <nl> if ( likely ( vbr != NULL )) { <nl> svbranch = vbr -> value ; <nl> trim (& svbranch ); <nl> - dsize += svbranch . len ; <nl> + dsize += svbranch . len + 1 ; <nl> } <nl> if ( dsize < 256 ) dsize = 256 ; <nl> 
static int load_gws ( struct sip_msg * _m , int argc , action_u_t argv []) <nl> } <nl> /* Do not look further if this matching rule was stopper */ <nl> if ( rule -> stopper == 1 ) goto done ; <nl> + } else { <nl> + LM_DBG (" from uri <%.* s > did not match to from regex <%.* s >", <nl> + from_uri . len , from_uri . s , rule -> from_uri_len , <nl> + rule -> from_uri ); <nl> } <nl> } <nl> rule = rule -> next ;
rs_cms_get_intent ( RS_CMS * cms ) <nl> void * <nl> rs_cms_get_transform ( RS_CMS * cms , CMS_TRANSFORM transform ) <nl> { <nl> + if (! cms -> enabled ) return NULL ; <nl> if ( transform > ( TRANSFORMS - 1 )) return ( NULL ); <nl> return ( cms -> transforms [ transform ]); <nl> }
batch_queue_save ( RS_QUEUE * queue ) <nl> xmlTextWriterWriteFormatElement ( writer , BAD_CAST " filename ", "% s ", filename ); <nl> xmlTextWriterWriteFormatElement ( writer , BAD_CAST " snapshot ", "% d ", setting_id ); <nl> xmlTextWriterEndElement ( writer ); <nl> + g_free ( filename ); <nl> } while ( gtk_tree_model_iter_next ( queue -> list , & iter )); <nl>  <nl> xmlTextWriterEndDocument ( writer );
makernote_nikon ( RAWFILE * rawfile , guint offset , RSMetadata * meta ) <nl> || g_str_equal ( meta -> model_ascii , " NIKON 1 V1 ") <nl> || g_str_equal ( meta -> model_ascii , " NIKON 1 V2 ") <nl> || g_str_equal ( meta -> model_ascii , " NIKON D7000 ") <nl> + || g_str_equal ( meta -> model_ascii , " NIKON D7100 ") <nl> || g_str_equal ( meta -> model_ascii , " COOLPIX P7700 ")) <nl> { <nl> meta -> cam_mul [ 0 ] = get_rational ( rawfile , offset );
gui_drawingarea_motion_callback ( GtkWidget * widget , GdkEventMotion * event , RS_BLO <nl> gint y = ( gint ) event -> y ; <nl> gushort * pixel ; <nl>  <nl> + if (! rs -> photo ) return FALSE ; <nl> + <nl> /* Draw RGB - values at bottom of screen */ <nl> gui_set_values ( rs , x , y ); <nl> 
ACTION ( copy_settings ) <nl>  <nl> ACTION ( paste_settings ) <nl> { <nl> - gint mask ; <nl> + gint mask = 0xffffff ; <nl>  <nl> GtkWidget * dialog , * cb_box ; <nl> GtkWidget * cb_exposure , * cb_saturation , * cb_hue , * cb_contrast , * cb_whitebalance , * cb_curve , * cb_sharpen ;
GtkWidget * <nl> gui_make_scale_from_adj ( RS_BLOB * rs , GCallback cb , GtkObject * adj , gint mask ) <nl> { <nl> GtkWidget * hscale , * box , * rimage , * revent ; <nl> - struct reset_carrier * rc = malloc ( sizeof ( struct reset_carrier )); <nl> + struct reset_carrier * rc = g_malloc ( sizeof ( struct reset_carrier )); <nl> rc -> rs = rs ; <nl> rc -> mask = mask ; <nl> 
gui_save_file_callback ( gpointer callback_data , guint callback_action , GtkWidget <nl> if ( filetype_str ) <nl> if ( g_str_equal ( savers [ n ]. extension , filetype_str )) <nl> gtk_combo_box_set_active ( GTK_COMBO_BOX ( filetype ), n ); <nl> + g_free ( filetype_str ); <nl> n ++; <nl> } <nl> if ( gtk_combo_box_get_active ( GTK_COMBO_BOX ( filetype )) == - 1 )
static void set_format ( WAVEFORMATEXTENSIBLE * wformat , WORD bytepersample , <nl> wformat -> Format . nAvgBytesPerSec = samplerate * block_align ; <nl> wformat -> Format . nBlockAlign = block_align ; <nl> wformat -> Format . wBitsPerSample = bytepersample * 8 ; <nl> - wformat -> Format . cbSize = <nl> - 22 ; /* must be at least 22 for WAVE_FORMAT_EXTENSIBLE */ <nl> + wformat -> Format . cbSize = sizeof ( WAVEFORMATEXTENSIBLE ) - sizeof ( WAVEFORMATEX ); <nl> if ( bytepersample == 4 ) <nl> wformat -> SubFormat = mp_KSDATAFORMAT_SUBTYPE_IEEE_FLOAT ; <nl> else
static void dump_index ( demuxer_t * demuxer , int stream_id ) <nl> if ( verbose <= 1 ) <nl> return ; <nl>  <nl> - if ( stream_id > MAX_STREAMS ) <nl> + if ( stream_id >= MAX_STREAMS ) <nl> return ; <nl>  <nl> index = priv -> index_table [ stream_id ];
void uninit_player ( struct MPContext * mpctx , unsigned int mask ){ <nl> mpctx -> timeline = NULL ; <nl> mpctx -> num_timeline_parts = 0 ; <nl> talloc_free ( mpctx -> chapters ); <nl> + mpctx -> chapters = NULL ; <nl> mpctx -> num_chapters = 0 ; <nl> mpctx -> video_offset = 0 ; <nl> if ( mpctx -> demuxer ){
bool fbotex_change ( struct fbotex * fbo , GL * gl , struct mp_log * log , int w , int h , <nl>  <nl> GLenum filter = fbo -> tex_filter ; <nl>  <nl> + fbotex_uninit ( fbo ); <nl> + <nl> * fbo = ( struct fbotex ) { <nl> . gl = gl , <nl> . rw = w ,
static void handle_stream ( demuxer_t * demuxer , int i ) <nl> sh_sub -> frame_based = 23 . 976 ; <nl> } <nl> } <nl> + <nl> + if ( matches_avinputformat_name ( priv , " ass ")) <nl> + sh_sub -> is_utf8 = true ; <nl> + <nl> break ; <nl> } <nl> case AVMEDIA_TYPE_ATTACHMENT : {
# include " config . h " <nl> # include " mp_msg . h " <nl>  <nl> -# include " fastmemcpy . h " <nl> +# include "../../ libvo / fastmemcpy . h " <nl>  <nl> # include " libmpdemux / nuppelvideo . h " <nl> # include " RTjpegN . h "
static int control ( struct af_instance_s * af , int cmd , void * arg ) <nl> *( float *) arg = s -> scale ; <nl> return AF_OK ; <nl> case AF_CONTROL_COMMAND_LINE :{ <nl> - strarg_t speed ; <nl> + strarg_t speed = {}; <nl> opt_t subopts [] = { <nl> {" scale ", OPT_ARG_FLOAT , & s -> scale_nominal , NULL }, <nl> {" stride ", OPT_ARG_FLOAT , & s -> ms_stride , NULL },
static void drm_egl_uninit ( MPGLContext * ctx ) <nl>  <nl> static int drm_egl_init ( struct MPGLContext * ctx , int flags ) <nl> { <nl> + if ( ctx -> vo -> probing ) { <nl> + MP_VERBOSE ( ctx -> vo , " DRM EGL backend can be activated only manually .\ n "); <nl> + return - 1 ; <nl> + } <nl> struct priv * p = ctx -> priv ; <nl> p -> kms = NULL ; <nl> p -> old_crtc = NULL ;
static void pass_prepare_src_tex ( struct gl_video * p ) <nl> static void render_pass_quad ( struct gl_video * p , int vp_w , int vp_h , <nl> const struct mp_rect * dst , int flags ) <nl> { <nl> - struct vertex va [ 4 ]; <nl> + struct vertex va [ 4 ] = { 0 }; <nl>  <nl> struct gl_transform t ; <nl> gl_transform_ortho (& t , 0 , vp_w , 0 , vp_h );
static mp_cmd_t * interpret_key ( struct input_ctx * ictx , int code ) <nl> code &= ~ KEY_MODIFIER_SHIFT ; <nl>  <nl> if ( code & MP_KEY_DOWN ) { <nl> - if ( ictx -> num_key_down > MP_MAX_KEY_DOWN ) { <nl> + if ( ictx -> num_key_down >= MP_MAX_KEY_DOWN ) { <nl> mp_tmsg ( MSGT_INPUT , MSGL_ERR , " Too many key down events " <nl> " at the same time \ n "); <nl> return NULL ;
static int control ( stream_t * stream , int cmd , void * arg ) <nl> int n = dvdnav_describe_title_chapters ( dvdnav , tit , & parts , & duration ); <nl> if (! parts ) <nl> break ; <nl> - if ( chapter < 0 || chapter + 1 >= n ) <nl> + if ( chapter < 0 || chapter + 1 > n ) <nl> break ; <nl> * ch = chapter > 0 ? parts [ chapter - 1 ] / 90000 . 0 : 0 ; <nl> free ( parts );
static void ao_chain_uninit ( struct ao_chain * ao_c ) <nl> talloc_free ( ao_c -> conv ); <nl> talloc_free ( ao_c -> input_frame ); <nl> talloc_free ( ao_c -> input_format ); <nl> + talloc_free ( ao_c -> output_frame ); <nl> talloc_free ( ao_c -> filter_input_format ); <nl> talloc_free ( ao_c -> ao_buffer ); <nl> talloc_free ( ao_c );
static int find_entrypoint ( int format , VAEntrypoint * ep , int num_ep ) <nl>  <nl> static int is_direct_mapping ( VADisplay display ) <nl> { <nl> - VADisplayAttribute attr ; <nl> + VADisplayAttribute attr = { 0 }; <nl> VAStatus status ; <nl>  <nl> # if VA_CHECK_VERSION ( 0 , 34 , 0 )
void demux_seek_mpg ( demuxer_t * demuxer , float rel_seek_secs , float audio_delay , in <nl> continue ; <nl> } <nl> } <nl> + if (! sh_video ) break ; <nl> i = sync_video_packet ( d_video ); <nl> if ( sh_video -> format == 0x10000004 ) { // mpeg4 <nl> if ( i == 0x1B6 ) { // vop ( frame ) startcode
static void vo_wayland_border ( struct vo * vo ) <nl> static void vo_wayland_fullscreen ( struct vo * vo ) <nl> { <nl> struct vo_wayland_state * wl = vo -> wayland ; <nl> - if (! wl -> display . shell ) <nl> + if (! wl -> display . shell || !! vo -> opts -> fullscreen == wl -> window . is_fullscreen ) <nl> return ; <nl>  <nl> struct wl_output * fs_output = wl -> display . fs_output ;
static int control ( sh_video_t * sh , int cmd , void * arg ,...){ <nl> va_start ( ap , arg ); <nl> value = va_arg ( ap , int ); <nl> va_end ( ap ); <nl> - if ( DS_VideoDecoder_SetValue ( sh -> context , arg , value )== 0 ) <nl> + if ( DS_VideoDecoder_SetValue ( sh -> context , arg , 50 + value / 2 )== 0 ) <nl> return CONTROL_OK ; <nl> return CONTROL_FALSE ; <nl> }
struct mp_csp_equalizer * gl_video_eq_ptr ( struct gl_video * p ) <nl> // Call when the mp_csp_equalizer returned by gl_video_eq_ptr () was changed . <nl> void gl_video_eq_update ( struct gl_video * p ) <nl> { <nl> + gl_video_reset_surfaces ( p ); <nl> } <nl>  <nl> static int validate_scaler_opt ( struct mp_log * log , const m_option_t * opt ,
struct exports exp_msvcr80 []={ <nl> FF ( _initterm_e , - 1 ) <nl> FF ( _initterm , - 1 ) <nl> FF ( _decode_pointer , - 1 ) <nl> +/* needed by KGV1 - VFW . dll */ <nl> + {"?? 2 @ YAPAXI @ Z ", - 1 , expnew }, <nl> + {"?? 3 @ YAXPAX @ Z ", - 1 , expdelete } <nl> }; <nl>  <nl> struct exports exp_msvcp60 []={
d_dvdsub = demuxer -> sub ; <nl> sh_audio = d_audio -> sh ; <nl> sh_video = d_video -> sh ; <nl>  <nl> + if (! sh_video ) <nl> + { <nl> + mp_msg ( MSGT_CPLAYER , MSGL_FATAL ," Video stream is mandatory !\ n "); <nl> + mencoder_exit ( 1 , NULL ); <nl> + } <nl> + <nl> if (! video_read_properties ( sh_video )){ <nl> printf ( MSGTR_CannotReadVideoProperties ); <nl> mencoder_exit ( 1 , NULL );
void gl_video_render_frame ( struct gl_video * p , struct vo_frame * frame , int fbo ) <nl> GL * gl = p -> gl ; <nl> struct video_image * vimg = & p -> image ; <nl>  <nl> + if ( fbo && !( gl -> mpgl_caps & MPGL_CAP_FB )) { <nl> + MP_FATAL ( p , " Rendering to FBO requested , but no FBO extension found !\ n "); <nl> + return ; <nl> + } <nl> + <nl> p -> broken_frame = false ; <nl>  <nl> gl -> BindFramebuffer ( GL_FRAMEBUFFER , fbo );
if ( verbose > 1 ){ <nl> mpi -> stride [ 0 ]= lavc_picture . linesize [ 0 ]; <nl> mpi -> stride [ 1 ]= lavc_picture . linesize [ 1 ]; <nl> mpi -> stride [ 2 ]= lavc_picture . linesize [ 2 ]; <nl> + if ( lavc_context . pix_fmt == PIX_FMT_YUV422P ){ <nl> + mpi -> stride [ 1 ]*= 2 ; <nl> + mpi -> stride [ 2 ]*= 2 ; <nl> + } <nl>  <nl> // stride [ 1 ]= stride [ 2 ]= 0 ; <nl> // stride [ 0 ]/= 2 ;
struct demux_packet * demux_read_packet ( struct sh_stream * sh ) <nl> // packets from the queue . <nl> double demux_get_next_pts ( struct sh_stream * sh ) <nl> { <nl> - if ( sh ) { <nl> + if ( sh && sh -> ds -> selected ) { <nl> ds_get_packets ( sh ); <nl> if ( sh -> ds -> head ) <nl> return sh -> ds -> head -> pts ;
void gl_video_upload_image ( struct gl_video * p , struct mp_image * mpi ) <nl> p -> osd_pts = mpi -> pts ; <nl>  <nl> if ( p -> hwdec_active ) { <nl> + talloc_free ( vimg -> hwimage ); <nl> vimg -> hwimage = mpi ; <nl> p -> have_image = true ; <nl> return ;
const struct ao_driver audio_out_null = { <nl> . priv_size = sizeof ( struct priv ), <nl> . priv_defaults = &( const struct priv ) { <nl> . bufferlen = 0 . 2 , <nl> - . latency_sec = 0 . 5 , <nl> . outburst = 256 , <nl> . speed = 1 , <nl> },
static HRESULT init_session_display ( struct wasapi_state * state ) { <nl> exit_label : <nl> MP_ERR ( state , " Error setting audio session display name : % s ( 0x %" PRIx32 ")\ n ", <nl> wasapi_explain_err ( hr ), ( uint32_t ) hr ); <nl> - return hr ; <nl> + // No reason to abort initialization . <nl> + return S_OK ; <nl> } <nl>  <nl> static HRESULT fix_format ( struct ao * ao )
dvb_config_t * dvb_get_config ( void ) <nl> } <nl>  <nl> if (( access ( conf_file , F_OK | R_OK ) != 0 )) <nl> + { <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> conf_file = get_path (" channels . conf "); <nl> + } <nl>  <nl> list = dvb_get_channels ( conf_file , type ); <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> if ( list == NULL ) <nl> continue ; <nl> 
int streaming_bufferize ( streaming_ctrl_t * streaming_ctrl , char * buffer , int siz <nl> int nop_streaming_read ( int fd , char * buffer , int size , streaming_ctrl_t * stream_ctrl ); <nl> int nop_streaming_seek ( int fd , off_t pos , streaming_ctrl_t * stream_ctrl ); <nl>  <nl> + int connect2Server ( char * host , int port ); <nl> + <nl> # endif
 <nl> static int av_log_level_to_mp_level ( int av_level ) <nl> { <nl> + if ( av_level > AV_LOG_VERBOSE ) <nl> + return MSGL_DBG2 ; <nl> if ( av_level > AV_LOG_INFO ) <nl> return MSGL_V ; <nl> if ( av_level > AV_LOG_WARNING )
static bool resize_d3d ( d3d_priv * priv ) <nl> return 0 ; <nl> } <nl>  <nl> - if (! priv -> d3d_device ) <nl> + if (! priv -> d3d_device || ! priv -> image_format ) <nl> return 1 ; <nl>  <nl> if (! create_d3d_surfaces ( priv ))
static int update_display_size ( struct vo * vo ) <nl> } <nl> p -> sc = gl_sc_create ( p -> egl . gl , vo -> log , vo -> global ), <nl> p -> osd = mpgl_osd_init ( p -> egl . gl , vo -> log , vo -> osd ); <nl> + p -> osd_change_counter = - 1 ; // force initial overlay rendering <nl>  <nl> p -> display_fps = 0 ; <nl> TV_GET_STATE_RESP_T tvstate ;
static int af_find_output_conversion ( struct af_stream * s , struct mp_audio * cfg ) <nl> ! mp_chmap_equals_reordered (& af -> fmt_in . channels , & af -> fmt_out . channels )) <nl> return AF_ERROR ; <nl> } <nl> + // And not if it ' s the only filter . <nl> + if ( conv -> prev == s -> first && conv -> next == s -> last ) <nl> + return AF_ERROR ; <nl>  <nl> * cfg = s -> output ; <nl> return AF_OK ;
static int d_check_file ( struct demuxer * demuxer , enum demux_check check ) <nl> * p = ( struct priv ) { <nl> . track = track , <nl> }; <nl> + demuxer -> priv = p ; <nl>  <nl> struct sh_stream * sh = new_sh_stream ( demuxer , STREAM_SUB ); <nl> sh -> sub -> track = track ;
case 0 : <nl> break ; <nl> case VCODEC_FRAMENO : <nl> mux_v -> buffer =& decoded_frameno ; // tricky <nl> - aviwrite_write_chunk ( muxer , mux_v , muxer_f , sizeof ( int ), 0 ); <nl> + if ( skip_flag <= 0 ) aviwrite_write_chunk ( muxer , mux_v , muxer_f , sizeof ( int ), 0 ); <nl> break ; <nl> case VCODEC_DIVX4 : <nl> blit_frame = decode_video (& video_out , sh_video , start , in_size , 0 );
struct vo * init_best_video_out ( struct MPOpts * opts , <nl> // continue ... <nl> free ( name ); <nl> ++ vo_list ; <nl> - if (!( vo_list [ 0 ])) <nl> + if (!( vo_list [ 0 ])) { <nl> + talloc_free ( vo ); <nl> return NULL ; // do NOT fallback to others <nl> + } <nl> } <nl> // now try the rest ... <nl> vo_subdevice = NULL ;
double get_current_time ( struct MPContext * mpctx ) <nl> return 0 ; <nl> if ( demuxer -> stream_pts != MP_NOPTS_VALUE ) <nl> return demuxer -> stream_pts ; <nl> + if ( mpctx -> hrseek_active ) <nl> + return mpctx -> hrseek_pts ; <nl> double apts = playing_audio_pts ( mpctx ); <nl> if ( apts != MP_NOPTS_VALUE ) <nl> return apts ;
static int seek_to_chapter ( stream_t * stream , ifo_handle_t * vts_file , tt_srpt_t * <nl> if (! vts_file || ! tt_srpt ) <nl> return 0 ; <nl>  <nl> - if ( chapter < 0 || chapter > vts_file -> vts_ptt_srpt -> title [ title_no ]. nr_of_ptts ) // no such chapter <nl> + if ( chapter < 0 || chapter > vts_file -> vts_ptt_srpt -> title [ title_no ]. nr_of_ptts - 1 ) // no such chapter <nl> return 0 ; <nl>  <nl> ptt = vts_file -> vts_ptt_srpt -> title [ title_no ]. ptt [ chapter ];
static void draw_image ( struct vo * vo , struct mp_image * mpi ) <nl> return ; <nl> } <nl> mp_image_copy_attributes ( dst , mpi ); <nl> - mpi = dst ; <nl> + talloc_free ( mpi ); <nl> + mpi = mp_image_new_ref ( dst ); <nl> } <nl>  <nl> talloc_free ( p -> output_surfaces [ p -> output_surface ]);
bool ca_asbd_equals ( const AudioStreamBasicDescription * a , <nl> a -> mBitsPerChannel == b -> mBitsPerChannel && <nl> ca_normalize_formatid ( a -> mFormatID ) == <nl> ca_normalize_formatid ( b -> mFormatID ) && <nl> - a -> mBytesPerPacket == b -> mBytesPerPacket ; <nl> + a -> mBytesPerPacket == b -> mBytesPerPacket && <nl> + a -> mChannelsPerFrame == b -> mChannelsPerFrame && <nl> + a -> mSampleRate == b -> mSampleRate ; <nl> } <nl>  <nl> // Return the AF_FORMAT_ * ( AF_FORMAT_S16 etc .) corresponding to the asbd .
static enum check_result libztex_checkDevice ( struct libusb_device * dev ) <nl> ret = CHECK_RESCAN ; <nl>  <nl> done : <nl> + free ( fw_buf ); <nl> if ( fp ) <nl> fclose ( fp ); <nl> if ( hndl )
void ft232r_close ( struct ft232r_device_handle * dev ) <nl> libusb_release_interface ( dev -> h , 0 ); <nl> libusb_reset_device ( dev -> h ); <nl> libusb_close ( dev -> h ); <nl> + free ( dev ); <nl> } <nl>  <nl> bool ft232r_purge_buffers ( struct ft232r_device_handle * dev , enum ft232r_reset_purge purge )
static bool avalon_detect_one ( libusb_device * dev , struct usb_find_devices * found <nl> info -> temp_sum = 0 ; <nl> info -> temp_old = 0 ; <nl>  <nl> - ret = avalon_reset ( avalon , true ); <nl> - if ( ret && ! configured ) <nl> + if (! add_cgpu ( avalon )) <nl> goto unshin ; <nl>  <nl> - if (! add_cgpu ( avalon )) <nl> + ret = avalon_reset ( avalon , true ); <nl> + if ( ret && ! configured ) <nl> goto unshin ; <nl>  <nl> update_usb_stats ( avalon );
int libztex_prepare_device ( struct libusb_device * dev , struct libztex_device ** z <nl> void libztex_destroy_device ( struct libztex_device * ztex ) { <nl> if ( ztex -> hndl != NULL ) { <nl> libusb_close ( ztex -> hndl ); <nl> + ztex -> hndl = NULL ; <nl> } <nl> if ( ztex -> bitFileName != NULL ) { <nl> free ( ztex -> bitFileName );
void hashmeter2 ( struct thr_info * thr ) <nl>  <nl> gettimeofday (& tv_now , NULL ); <nl> timersub (& tv_now , & thr -> tv_lastupdate , & tv_elapsed ); <nl> - if ( tv_elapsed . tv_sec >= opt_log_interval ) { <nl> + /* Update the hashmeter at most 5 times per second */ <nl> + if ( tv_elapsed . tv_sec > 0 || tv_elapsed . tv_usec > 200 ) { <nl> hashmeter ( thr -> id , & tv_elapsed , thr -> hashes_done ); <nl> thr -> hashes_done = 0 ; <nl> thr -> tv_lastupdate = tv_now ;
static bool setup_stratum_curl ( struct pool * pool ) <nl> quit ( 1 , " Failed to curl_easy_init in initiate_stratum "); <nl> if ( pool -> sockbuf ) <nl> pool -> sockbuf [ 0 ] = '\ 0 '; <nl> - mutex_unlock (& pool -> stratum_lock ); <nl>  <nl> curl = pool -> stratum_curl ; <nl>  <nl> static bool setup_stratum_curl ( struct pool * pool ) <nl> pool -> cgminer_pool_stats . times_sent ++; <nl> pool -> cgminer_pool_stats . times_received ++; <nl>  <nl> + mutex_unlock (& pool -> stratum_lock ); <nl> + <nl> return true ; <nl> } <nl> 
void decay_time ( double * f , double fadd ) <nl> ratio = 1 / ratio ; <nl> } <nl>  <nl> - if ( ratio > 0 . 9 ) <nl> + if ( ratio > 0 . 95 ) <nl> * f = ( fadd * 0 . 1 + * f ) / 1 . 1 ; <nl> else <nl> * f = ( fadd + * f * 0 . 1 ) / 1 . 1 ;
void zero_stats ( void ) <nl> total_ro = 0 ; <nl> total_secs = 1 . 0 ; <nl> total_diff1 = 0 ; <nl> + total_bad_nonces = 0 ; <nl> found_blocks = 0 ; <nl> total_diff_accepted = 0 ; <nl> total_diff_rejected = 0 ;
static inline void string_elist_del ( struct string_elist * item ) <nl> if ( item -> free_me ) <nl> free ( item -> string ); <nl> list_del (& item -> list ); <nl> + free ( item ); <nl> } <nl>  <nl> 
static void avalon_parse_results ( struct cgpu_info * avalon , struct avalon_info * i <nl> } <nl>  <nl> if (! found ) <nl> - spare = * offset - AVALON_READ_SIZE - 1 ; <nl> + spare = * offset - AVALON_READ_SIZE ; <nl> else <nl> spare = AVALON_READ_SIZE + i ; <nl> applog ( LOG_WARNING , " Avalon : Discarding % d bytes from buffer ", spare );
void bfg_waddstr ( WINDOW * win , const char * s ) <nl>  <nl> while ( true ) <nl> { <nl> - while ( likely ( p [ 0 ] >= 0x20 && p [ 0 ] < stop_ascii )) <nl> + while ( likely ( p [ 0 ] == '\ n ' || ( p [ 0 ] >= 0x20 && p [ 0 ] < stop_ascii ))) <nl> { <nl> // Printable ASCII <nl> ++ p ;
void knc_poll ( struct thr_info * const thr ) <nl> if ( KNC_REPLY_NONCE_FOUND == rtype ) <nl> { <nl> nonce = get_u32be (& rxbuf [ 4 ]); <nl> + nonce = le32toh ( nonce ); <nl> inc_hw_errors2 ( mythr , NULL , & nonce ); <nl> } <nl> else
/****************************************************************************** <nl> * <nl> * Module Name : aeexec - Support routines for AcpiExec utility <nl> - * $ Revision : 1 . 107 $ <nl> + * $ Revision : 1 . 108 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> AeMiscellaneousTests ( <nl> AcpiOsPrintf (" Could not get GlobalLock , % X \ n ", Status ); <nl> } <nl>  <nl> + Status = AcpiAcquireGlobalLock ( 0x5 , & LockHandle ); /* Should fail */ <nl> + <nl> Status = AcpiReleaseGlobalLock ( LockHandle ); <nl> if ( ACPI_FAILURE ( Status )) <nl> {
AcpiNsEvaluate ( <nl>  <nl> Status = AE_OK ; <nl> } <nl> + else if ( ACPI_FAILURE ( Status )) <nl> + { <nl> + /* If ReturnObject exists , delete it */ <nl> + <nl> + if ( Info -> ReturnObject ) <nl> + { <nl> + AcpiUtRemoveReference ( Info -> ReturnObject ); <nl> + Info -> ReturnObject = NULL ; <nl> + } <nl> + } <nl>  <nl> ACPI_DEBUG_PRINT (( ACPI_DB_NAMES , <nl> "*** Completed evaluation of object % s ***\ n ",
AmlExecCreateField ( <nl> FieldDesc -> FieldUnit . UpdateRule = ( UINT8 ) UPDATE_Preserve ; <nl> FieldDesc -> FieldUnit . Length = BitCount ; <nl> FieldDesc -> FieldUnit . BitOffset = ( UINT8 ) ( BitOffset % 8 ); <nl> - FieldDesc -> FieldUnit . Offset = BitOffset / 8 ; <nl> + FieldDesc -> FieldUnit . Offset = DIV_8 ( BitOffset ); <nl> FieldDesc -> FieldUnit . Container = SrcDesc ; <nl> FieldDesc -> FieldUnit . Sequence = SrcDesc -> Buffer . Sequence ; <nl> 
AcpiTbInstallTableWithOverride ( <nl> * DESCRIPTION : This function is called to verify and install an ACPI table . <nl> * When this function is called by " Load " or " LoadTable " opcodes , <nl> * or by AcpiLoadTable () API , the " Reload " parameter is set . <nl> - * After sucessfully returning from this function , table is <nl> + * After successfully returning from this function , table is <nl> * " INSTALLED " but not " VALIDATED ". <nl> * <nl> ******************************************************************************/
AtHardwTest0010 ( void ) <nl> return ( Status ); <nl> } <nl>  <nl> - Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress ); <nl> + Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress , 0 ); <nl> if ( ACPI_FAILURE ( Status )) <nl> { <nl> AapiErrors ++; <nl> AtHardwTest0011 ( void ) <nl> return ( Status ); <nl> } <nl>  <nl> - Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress ); <nl> + Status = AcpiSetFirmwareWakingVector (( UINT32 ) PhysicalAddress , 0 ); <nl> if ( Status != AE_NO_ACPI_TABLES ) <nl> { <nl> AapiErrors ++;
/****************************************************************************** <nl> * <nl> * Name : acwin . h - OS specific defines , etc . <nl> - * $ Revision : 1 . 27 $ <nl> + * $ Revision : 1 . 28 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> typedef COMPILER_DEPENDENT_UINT64 u64 ; <nl> # define ACPI_SIMPLE_RETURN_MACROS <nl> # endif <nl>  <nl> +/*! [ End ] no source code translation !*/ <nl> + <nl> /* <nl> * Global Lock acquire / release code <nl> *
ApIsValidHeader ( <nl>  <nl> /* Check for minimum table length */ <nl>  <nl> - if ( Table -> Length <= sizeof ( ACPI_TABLE_HEADER )) <nl> + if ( Table -> Length < sizeof ( ACPI_TABLE_HEADER )) <nl> { <nl> fprintf ( stderr , " Table length ( 0x % 8 . 8X ) is invalid \ n ", <nl> Table -> Length );
/****************************************************************************** <nl> * <nl> * Name : acglobal . h - Declarations for global variables <nl> - * $ Revision : 1 . 153 $ <nl> + * $ Revision : 1 . 154 $ <nl> * <nl> *****************************************************************************/ <nl>  <nl> # define ACPI_INIT_GLOBAL ( a , b ) a <nl> # endif <nl>  <nl> -/* <nl> +/* <nl> * Keep local copies of these FADT - based registers . NOTE : These globals <nl> * are first in this file for alignment reasons on 64 - bit systems . <nl> */
AmlDumpOperand ( <nl> break ; <nl>  <nl>  <nl> + case AML_NAMEPATH_OP : <nl> + DEBUG_PRINT_RAW ( ACPI_INFO , (" Lvalue . Nte -> Name % x \ n ", <nl> + EntryDesc -> Lvalue . Nte -> Name )); <nl> + break ; <nl> + <nl> default : <nl>  <nl> /* unknown opcode */
OpcDoPld ( <nl> { <nl> UINT8 * Buffer ; <nl> ACPI_PARSE_OBJECT * Node ; <nl> - ACPI_PLD_INFO PldInfo = { 0 }; <nl> + ACPI_PLD_INFO PldInfo ; <nl> ACPI_PARSE_OBJECT * NewOp ; <nl>  <nl>  <nl> OpcDoPld ( <nl> return ; <nl> } <nl>  <nl> + ACPI_MEMSET (& PldInfo , 0 , sizeof ( ACPI_PLD_INFO )); <nl> + <nl> Node = Op -> Asl . Child ; <nl> while ( Node ) <nl> {
INT32 <nl> AcpiModeCapabilities ( <nl> void ); <nl>  <nl> - <nl> /* <nl> * Event / System interfaces <nl> */
findso : <nl> so -> so_ti = ti ; <nl> tp -> t_timer [ TCPT_KEEP ] = TCPTV_KEEP_INIT ; <nl> tp -> t_state = TCPS_SYN_RECEIVED ; <nl> + tcp_template ( tp ); <nl> } <nl> return ; <nl> 
static int scsi_disk_initfn ( SCSIDevice * dev ) <nl> } <nl> s -> bs = s -> qdev . conf . dinfo -> bdrv ; <nl>  <nl> + if ( bdrv_is_sg ( s -> bs )) { <nl> + qemu_error (" scsi - disk : unwanted / dev / sg *\ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( bdrv_get_type_hint ( s -> bs ) == BDRV_TYPE_CDROM ) { <nl> s -> cluster_size = 4 ; <nl> } else {
void tlb_fill ( CPUCRISState * env , target_ulong addr , int is_write , int mmu_idx , <nl> int ret ; <nl>  <nl> D_LOG ("% s pc =% x tpc =% x ra =% p \ n ", __func__ , <nl> - env -> pc , env -> debug1 , ( void *) retaddr ); <nl> + env -> pc , env -> pregs [ PR_EDA ], ( void *) retaddr ); <nl> ret = cpu_cris_handle_mmu_fault ( env , addr , is_write , mmu_idx ); <nl> if ( unlikely ( ret )) { <nl> if ( retaddr ) {
void usb_packet_complete ( USBDevice * dev , USBPacket * p ) <nl>  <nl> while (! QTAILQ_EMPTY (& ep -> queue )) { <nl> p = QTAILQ_FIRST (& ep -> queue ); <nl> + if ( p -> state == USB_PACKET_ASYNC ) { <nl> + break ; <nl> + } <nl> assert ( p -> state == USB_PACKET_QUEUED ); <nl> ret = usb_process_one ( p ); <nl> if ( ret == USB_RET_ASYNC ) {
typedef struct { <nl>  <nl> static inline int num_effective_busses ( XilinxSPIPS * s ) <nl> { <nl> - return ( s -> regs [ R_LQSPI_STS ] & LQSPI_CFG_SEP_BUS && <nl> - s -> regs [ R_LQSPI_STS ] & LQSPI_CFG_TWO_MEM ) ? s -> num_busses : 1 ; <nl> + return ( s -> regs [ R_LQSPI_CFG ] & LQSPI_CFG_SEP_BUS && <nl> + s -> regs [ R_LQSPI_CFG ] & LQSPI_CFG_TWO_MEM ) ? s -> num_busses : 1 ; <nl> } <nl>  <nl> static void xilinx_spips_update_cs_lines ( XilinxSPIPS * s )
static int raw_open ( BlockDriverState * bs , const char * filename , int flags ) <nl> } <nl> # endif <nl> # ifdef CONFIG_COCOA <nl> - u_int32_t blockSize = 512 ; <nl> + uint32_t blockSize = 512 ; <nl> if ( ! ioctl ( fd , DKIOCGETBLOCKSIZE , & blockSize ) && blockSize > bufsize ) { <nl> bufsize = blockSize ; <nl> }
out : <nl> static void usb_host_req_abort ( USBHostRequest * r ) <nl> { <nl> USBHostDevice * s = r -> host ; <nl> - bool inflight = ( r -> p && r -> p -> state == USB_RET_ASYNC ); <nl> + bool inflight = ( r -> p && r -> p -> state == USB_PACKET_ASYNC ); <nl>  <nl> if ( inflight ) { <nl> r -> p -> status = USB_RET_NODEV ;
static int send_sub_rect_jpeg ( VncState * vs , int x , int y , int w , int h , <nl> } else { <nl> ret = send_palette_rect ( vs , x , y , w , h , palette ); <nl> } <nl> + } else { <nl> + ret = 0 ; <nl> } <nl> return ret ; <nl> }
bool aio_wait ( AioContext * ctx ) <nl> * Otherwise , if there are no AIO requests , qemu_aio_wait () would <nl> * wait indefinitely . <nl> */ <nl> - if ( node -> io_flush ) { <nl> + if (! node -> deleted && node -> io_flush ) { <nl> if ( node -> io_flush ( node -> opaque ) == 0 ) { <nl> continue ; <nl> }
static void smc91c111_writeb ( void * opaque , hwaddr offset , <nl> return ; <nl> case 12 : /* Early receive . */ <nl> s -> ercv = value & 0x1f ; <nl> + return ; <nl> case 13 : <nl> /* Ignore . */ <nl> return ;
tight_detect_smooth_image ( VncState * vs , int w , int h ) <nl> } else { <nl> errors = tight_detect_smooth_image16 ( vs , w , h ); <nl> } <nl> - if ( quality != - 1 ) { <nl> + if ( quality != ( uint8_t )- 1 ) { <nl> return ( errors < tight_conf [ quality ]. jpeg_threshold ); <nl> } <nl> return ( errors < tight_conf [ compression ]. gradient_threshold );
DMAContext * spapr_tce_new_dma_context ( uint32_t liobn , size_t window_size ) <nl> { <nl> sPAPRTCETable * tcet ; <nl>  <nl> + if ( spapr_tce_find_by_liobn ( liobn )) { <nl> + fprintf ( stderr , " Attempted to create TCE table with duplicate " <nl> + " LIOBN 0x % x \ n ", liobn ); <nl> + return NULL ; <nl> + } <nl> + <nl> if (! window_size ) { <nl> return NULL ; <nl> }
int paio_init ( void ) <nl> s -> first_aio = NULL ; <nl> if ( qemu_pipe ( fds ) == - 1 ) { <nl> fprintf ( stderr , " failed to create pipe \ n "); <nl> + g_free ( s ); <nl> return - 1 ; <nl> } <nl> 
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> + qemu_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
const VMStateDescription vmstate_ahci = { <nl> VMSTATE_UINT32 ( control_regs . impl , AHCIState ), <nl> VMSTATE_UINT32 ( control_regs . version , AHCIState ), <nl> VMSTATE_UINT32 ( idp_index , AHCIState ), <nl> - VMSTATE_INT32 ( ports , AHCIState ), <nl> + VMSTATE_INT32_EQUAL ( ports , AHCIState ), <nl> VMSTATE_END_OF_LIST () <nl> }, <nl> };
petalogix_ml605_init ( QEMUMachineInitArgs * args ) <nl> dma = qdev_create ( NULL , " xlnx . axi - dma "); <nl>  <nl> /* FIXME : attach to the sysbus instead */ <nl> + object_property_add_child ( qdev_get_machine (), " xilinx - eth ", OBJECT ( eth0 ), <nl> + NULL ); <nl> object_property_add_child ( qdev_get_machine (), " xilinx - dma ", OBJECT ( dma ), <nl> NULL ); <nl> 
static void hda_audio_set_amp ( HDAAudioStream * st ) <nl> left = left * 255 / QEMU_HDA_AMP_STEPS ; <nl> right = right * 255 / QEMU_HDA_AMP_STEPS ; <nl>  <nl> + if (! st -> state -> mixer ) { <nl> + return ; <nl> + } <nl> if ( st -> output ) { <nl> AUD_set_volume_out ( st -> voice . out , muted , left , right ); <nl> } else {
static void vring_init ( struct vring * vr , unsigned int num , void * p , <nl> vr -> used -> flags = VRING_USED_F_NO_NOTIFY ; <nl> vr -> used -> idx = 0 ; <nl> vr -> used_idx = 0 ; <nl> + vr -> next_idx = 0 ; <nl>  <nl> debug_print_addr (" init vr ", vr ); <nl> }
out : <nl> g_free ( dummy ); <nl> if ( err ) { <nl> qerror_report_err ( err ); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> return 0 ;
static int64_t coroutine_fn vmdk_co_get_block_status ( BlockDriverState * bs , <nl> break ; <nl> case VMDK_OK : <nl> ret = BDRV_BLOCK_DATA ; <nl> - if ( extent -> file == bs -> file ) { <nl> + if ( extent -> file == bs -> file && ! extent -> compressed ) { <nl> ret |= BDRV_BLOCK_OFFSET_VALID | offset ; <nl> } <nl> 
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> { <nl> DPRINTF ("+++ C + mode offloaded task checksum \ n "); <nl>  <nl> + /* Large enough for Ethernet and IP headers ? */ <nl> + if ( saved_size < ETH_HLEN + sizeof ( ip_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ip packet header */ <nl> ip_header * ip = NULL ; <nl> int hlen = 0 ;
const VMStateDescription vmstate_ide_atapi_gesn_state = { <nl> . fields = ( VMStateField []) { <nl> VMSTATE_BOOL ( events . new_media , IDEState ), <nl> VMSTATE_BOOL ( events . eject_request , IDEState ), <nl> + VMSTATE_END_OF_LIST () <nl> } <nl> }; <nl> 
uint32_t lm4549_write_samples ( lm4549_state * s , uint32_t left , uint32_t right ) <nl> This model supports 16 - bit playback . <nl> */ <nl>  <nl> - if ( s -> buffer_level >= LM4549_BUFFER_SIZE ) { <nl> + if ( s -> buffer_level > LM4549_BUFFER_SIZE - 2 ) { <nl> DPRINTF (" write_sample Buffer full \ n "); <nl> return 0 ; <nl> }
void virtio_cleanup ( VirtIODevice * vdev ) <nl> if ( vdev -> config ) <nl> qemu_free ( vdev -> config ); <nl> qemu_free ( vdev -> vq ); <nl> + qemu_free ( vdev ); <nl> } <nl>  <nl> static void virtio_vmstate_change ( void * opaque , int running , int reason )
void qmp_migrate_set_cache_size ( int64_t value , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> + /* Cache should not be larger than guest ram size */ <nl> + if ( value > ram_bytes_total ()) { <nl> + error_set ( errp , QERR_INVALID_PARAMETER_VALUE , " cache size ", <nl> + " exceeds guest ram size "); <nl> + return ; <nl> + } <nl> + <nl> new_size = xbzrle_cache_resize ( value ); <nl> if ( new_size < 0 ) { <nl> error_set ( errp , QERR_INVALID_PARAMETER_VALUE , " cache size ",
void cpu_loop ( CPUMIPSState * env ) <nl> syscall_num = env -> active_tc . gpr [ 2 ] - 4000 ; <nl> env -> active_tc . PC += 4 ; <nl> if ( syscall_num >= sizeof ( mips_syscall_args )) { <nl> - ret = - ENOSYS ; <nl> + ret = - TARGET_ENOSYS ; <nl> } else { <nl> int nb_args ; <nl> abi_ulong sp_reg ;
static void gen_compute_branch ( DisasContext * ctx , uint32_t opc , int r1 , <nl> break ; <nl> case OPCM_32_BRR_LOOP : <nl> if ( MASK_OP_BRR_OP2 ( ctx -> opcode ) == OPC2_32_BRR_LOOP ) { <nl> - gen_loop ( ctx , r1 , offset * 2 ); <nl> + gen_loop ( ctx , r2 , offset * 2 ); <nl> } else { <nl> /* OPC2_32_BRR_LOOPU */ <nl> gen_goto_tb ( ctx , 0 , ctx -> pc + offset * 2 );
static int sd_create ( const char * filename , QemuOpts * opts , <nl> bdrv_unref ( bs ); <nl> } <nl>  <nl> + s -> aio_context = qemu_get_aio_context (); <nl> ret = do_sd_create ( s , & vid , 0 , errp ); <nl> if ( ret ) { <nl> goto out ;
static void cirrus_init_common ( CirrusVGAState * s , int device_id , int is_pci ) <nl> s -> vga . cursor_draw_line = cirrus_cursor_draw_line ; <nl>  <nl> qemu_register_reset ( cirrus_reset , s ); <nl> - cirrus_reset ( s ); <nl> } <nl>  <nl> /***************************************
static int sd_snapshot_goto ( BlockDriverState * bs , const char * snapshot_id ) <nl> if ( snapid ) { <nl> tag [ 0 ] = 0 ; <nl> } else { <nl> - pstrcpy ( tag , sizeof ( tag ), s -> name ); <nl> + pstrcpy ( tag , sizeof ( tag ), snapshot_id ); <nl> } <nl>  <nl> ret = reload_inode ( s , snapid , tag );
static bool cpu_thread_is_idle ( CPUArchState * env ) <nl> if ( env -> stopped || ! runstate_is_running ()) { <nl> return true ; <nl> } <nl> - if (! env -> halted || qemu_cpu_has_work ( env ) || <nl> - ( kvm_enabled () && kvm_irqchip_in_kernel ())) { <nl> + if (! env -> halted || qemu_cpu_has_work ( env ) || kvm_irqchip_in_kernel ()) { <nl> return false ; <nl> } <nl> return true ;
static int refresh_total_sectors ( BlockDriverState * bs , int64_t hint ) <nl> { <nl> BlockDriver * drv = bs -> drv ; <nl>  <nl> + /* Do not attempt drv -> bdrv_getlength () on scsi - generic devices */ <nl> + if ( bs -> sg ) <nl> + return 0 ; <nl> + <nl> /* query actual device if possible , otherwise just trust the hint */ <nl> if ( drv -> bdrv_getlength ) { <nl> int64_t length = drv -> bdrv_getlength ( bs );
static uint32_t parse_enumeration ( char * str , <nl> { <nl> uint32_t ret = not_found_value ; <nl>  <nl> + if ( str == NULL ) <nl> + return 0 ; <nl> + <nl> while ( table -> name != NULL ) { <nl> if ( strcmp ( table -> name , str ) == 0 ) { <nl> ret = table -> value ;
fail : <nl> qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) table ); <nl> } <nl> s -> l1_table [ l1_index ] = old_l2_offset ; <nl> + if ( l2_offset > 0 ) { <nl> + qcow2_free_clusters ( bs , l2_offset , s -> l2_size * sizeof ( uint64_t ), <nl> + QCOW2_DISCARD_ALWAYS ); <nl> + } <nl> return ret ; <nl> } <nl> 
void qemu_iovec_concat_iov ( QEMUIOVector * dst , <nl> { <nl> int i ; <nl> size_t done ; <nl> + <nl> + if (! sbytes ) { <nl> + return ; <nl> + } <nl> assert ( dst -> nalloc != - 1 ); <nl> for ( i = 0 , done = 0 ; done < sbytes && i < src_cnt ; i ++) { <nl> if ( soffset < src_iov [ i ]. iov_len ) {
void vnc_display_open ( const char * id , Error ** errp ) <nl> /* listen for connects */ <nl> if ( strncmp ( vnc , " unix :", 5 ) == 0 ) { <nl> vs -> lsock = unix_listen ( vnc + 5 , NULL , 0 , errp ); <nl> + if ( vs -> lsock < 0 ) { <nl> + goto fail ; <nl> + } <nl> vs -> is_unix = true ; <nl> } else { <nl> vs -> lsock = inet_listen_opts ( sopts , 5900 , errp );
static const char * const tcg_target_reg_names [ TCG_TARGET_NB_REGS ] = { <nl> # endif <nl>  <nl> static const int tcg_target_reg_alloc_order [] = { <nl> - TCG_REG_EAX , <nl> - TCG_REG_EDX , <nl> - TCG_REG_ECX , <nl> TCG_REG_EBX , <nl> TCG_REG_ESI , <nl> TCG_REG_EDI , <nl> TCG_REG_EBP , <nl> + TCG_REG_ECX , <nl> + TCG_REG_EDX , <nl> + TCG_REG_EAX , <nl> }; <nl>  <nl> static const int tcg_target_call_iarg_regs [ 3 ] = { TCG_REG_EAX , TCG_REG_EDX , TCG_REG_ECX };
static CharDriverState * qemu_chr_open_udp ( QemuOpts * opts ) <nl>  <nl> fd = inet_dgram_opts ( opts , & local_err ); <nl> if ( fd < 0 ) { <nl> + qerror_report_err ( local_err ); <nl> + error_free ( local_err ); <nl> return NULL ; <nl> } <nl> return qemu_chr_open_udp_fd ( fd );
 <nl> typedef struct SuperIOConfig <nl> { <nl> - uint8_t config [ 0xff ]; <nl> + uint8_t config [ 0x100 ]; <nl> uint8_t index ; <nl> uint8_t data ; <nl> } SuperIOConfig ;
static void leon3_generic_hw_init ( MachineState * machine ) <nl> fprintf ( stderr , " Can ' t read bios image % s \ n ", filename ); <nl> exit ( 1 ); <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Can directly load an application . */ <nl> if ( kernel_filename != NULL ) {
bool bdrv_dev_is_medium_locked ( BlockDriverState * bs ) <nl> */ <nl> int bdrv_check ( BlockDriverState * bs , BdrvCheckResult * res , BdrvCheckMode fix ) <nl> { <nl> + if ( bs -> drv == NULL ) { <nl> + return - ENOMEDIUM ; <nl> + } <nl> if ( bs -> drv -> bdrv_check == NULL ) { <nl> return - ENOTSUP ; <nl> }
int virtio_load ( VirtIODevice * vdev , QEMUFile * f ) <nl> qemu_get_8s ( f , & vdev -> status ); <nl> qemu_get_8s ( f , & vdev -> isr ); <nl> qemu_get_be16s ( f , & vdev -> queue_sel ); <nl> + if ( vdev -> queue_sel >= VIRTIO_PCI_QUEUE_MAX ) { <nl> + return - 1 ; <nl> + } <nl> qemu_get_be32s ( f , & features ); <nl>  <nl> if ( virtio_set_features ( vdev , features ) < 0 ) {
int net_init_vhost_user ( const NetClientOptions * opts , const char * name , <nl> } <nl>  <nl> queues = vhost_user_opts -> has_queues ? vhost_user_opts -> queues : 1 ; <nl> + if ( queues < 1 ) { <nl> + error_setg ( errp , <nl> + " vhost - user number of queues must be bigger than zero "); <nl> + return - 1 ; <nl> + } <nl>  <nl> return net_vhost_user_init ( peer , " vhost_user ", name , chr , queues ); <nl> }
static inline void gen_neon_mull ( TCGv_i64 dest , TCGv a , TCGv b , int size , int u ) <nl> case 4 : <nl> tmp = gen_muls_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> case 5 : <nl> tmp = gen_mulu_i64_i32 ( a , b ); <nl> tcg_gen_mov_i64 ( dest , tmp ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> break ; <nl> default : abort (); <nl> }
static int qcow2_read_extensions ( BlockDriverState * bs , uint64_t start_offset , <nl> # ifdef DEBUG_EXT <nl> printf (" ext . magic = 0x % x \ n ", ext . magic ); <nl> # endif <nl> + if ( ext . len > end_offset - offset ) { <nl> + error_report (" Header extension too large "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> switch ( ext . magic ) { <nl> case QCOW2_EXT_MAGIC_END : <nl> return 0 ;
static void disas_s390_insn ( DisasContext * s ) <nl> store_reg32 ( r1 , tmp32_1 ); <nl> tcg_gen_trunc_i64_i32 ( tmp32_2 , tmp2 ); <nl> store_reg32 ( r1 + 1 , tmp32_2 ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> + tcg_temp_free_i64 ( tmp2 ); <nl> break ; <nl> case 0x98 : /* LM R1 , R3 , D2 ( B2 ) [ RS ] */ <nl> case 0x90 : /* STM R1 , R3 , D2 ( B2 ) [ RS ] */
ssize_t v9fs_list_xattr ( FsContext * ctx , const char * path , <nl>  <nl> /* Get the actual len */ <nl> xattr_len = llistxattr ( rpath ( ctx , path ), value , 0 ); <nl> + if ( xattr_len <= 0 ) { <nl> + return xattr_len ; <nl> + } <nl>  <nl> /* Now fetch the xattr and find the actual size */ <nl> orig_value = qemu_malloc ( xattr_len );
static uint32_t pic_poll_read ( PicState * s ) <nl> pic_update_irq ( s -> pics_state ); <nl> } else { <nl> ret = 0x07 ; <nl> - pic_update_irq ( s -> pics_state ); <nl> } <nl>  <nl> return ret ;
void qemu_opts_del ( QemuOpts * opts ) <nl> { <nl> QemuOpt * opt ; <nl>  <nl> + if ( opts == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> for (;;) { <nl> opt = QTAILQ_FIRST (& opts -> head ); <nl> if ( opt == NULL )
ram_addr_t qemu_ram_alloc_from_ptr ( ram_addr_t size , void * host , <nl>  <nl> qemu_ram_setup_dump ( new_block -> host , size ); <nl> qemu_madvise ( new_block -> host , size , QEMU_MADV_HUGEPAGE ); <nl> + qemu_madvise ( new_block -> host , size , QEMU_MADV_DONTFORK ); <nl>  <nl> if ( kvm_enabled ()) <nl> kvm_setup_guest_memory ( new_block -> host , size );
static void xhci_doorbell_write ( void * ptr , hwaddr reg , <nl> } <nl> } <nl>  <nl> + static void xhci_cap_write ( void * opaque , hwaddr addr , uint64_t val , <nl> + unsigned width ) <nl> +{ <nl> + /* nothing */ <nl> +} <nl> + <nl> static const MemoryRegionOps xhci_cap_ops = { <nl> . read = xhci_cap_read , <nl> + . write = xhci_cap_write , <nl> . valid . min_access_size = 1 , <nl> . valid . max_access_size = 4 , <nl> . impl . min_access_size = 4 ,
# ifndef FW_CFG_H <nl> # define FW_CFG_H <nl>  <nl> +# ifndef NO_QEMU_PROTOS <nl> +# include < stdint . h > <nl> +# include < stddef . h > <nl> + <nl> +# include " exec / hwaddr . h " <nl> +# endif <nl> + <nl> # define FW_CFG_SIGNATURE 0x00 <nl> # define FW_CFG_ID 0x01 <nl> # define FW_CFG_UUID 0x02
static void vfio_msi_interrupt ( void * opaque ) <nl> MSIMessage msg ; <nl>  <nl> if ( vdev -> interrupt == VFIO_INT_MSIX ) { <nl> - msg = msi_get_message (& vdev -> pdev , nr ); <nl> - } else if ( vdev -> interrupt == VFIO_INT_MSI ) { <nl> msg = msix_get_message (& vdev -> pdev , nr ); <nl> + } else if ( vdev -> interrupt == VFIO_INT_MSI ) { <nl> + msg = msi_get_message (& vdev -> pdev , nr ); <nl> } else { <nl> abort (); <nl> }
static int ioreq_map ( struct ioreq * ioreq ) <nl> xen_be_printf (& ioreq -> blkdev -> xendev , 0 , <nl> " can ' t map grant ref % d (% s , % d maps )\ n ", <nl> refs [ i ], strerror ( errno ), ioreq -> blkdev -> cnt_map ); <nl> + ioreq -> mapped = 1 ; <nl> ioreq_unmap ( ioreq ); <nl> return - 1 ; <nl> }
static int aio_flush_request ( void * opaque ) <nl> return ! QLIST_EMPTY (& s -> outstanding_aio_head ); <nl> } <nl>  <nl> -# ifdef _WIN32 <nl> +# if ! defined ( SOL_TCP ) || ! defined ( TCP_CORK ) <nl>  <nl> static int set_cork ( int fd , int v ) <nl> {
static void ehci_async_complete_packet ( USBPort * port , USBPacket * packet ) <nl> assert ( p -> async == EHCI_ASYNC_INFLIGHT ); <nl> p -> async = EHCI_ASYNC_FINISHED ; <nl> p -> usb_status = packet -> result ; <nl> + <nl> + if ( p -> queue -> async ) { <nl> + qemu_bh_schedule ( p -> queue -> ehci -> async_bh ); <nl> + } <nl> } <nl>  <nl> static void ehci_execute_complete ( EHCIQueue * q )
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> if (( txdw0 & CP_TX_LGSEN ) && ip_protocol == IP_PROTO_TCP ) <nl> { <nl> + /* Large enough for the TCP header ? */ <nl> + if ( ip_data_len < sizeof ( tcp_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> int large_send_mss = ( txdw0 >> 16 ) & CP_TC_LGSEN_MSS_MASK ; <nl>  <nl> DPRINTF ("+++ C + mode offloaded task TSO MTU =% d IP data % d "
uint32_t gic_acknowledge_irq ( GICState * s , int cpu ) <nl> } <nl> s -> last_active [ irq ][ cpu ] = s -> running_irq [ cpu ]; <nl>  <nl> - if ( s -> revision == REV_11MPCORE ) { <nl> + if ( s -> revision == REV_11MPCORE || s -> revision == REV_NVIC ) { <nl> /* Clear pending flags for both level and edge triggered interrupts . <nl> * Level triggered IRQs will be reasserted once they become inactive . <nl> */
static void decode_opc ( CPUState * env , DisasContext * ctx ) <nl> gen_goto_tb ( ctx , 1 , ctx -> pc + 4 ); <nl> gen_set_label ( l1 ); <nl> } <nl> + <nl> + if ( unlikely ( qemu_loglevel_mask ( CPU_LOG_TB_OP ))) <nl> + tcg_gen_debug_insn_start ( ctx -> pc ); <nl> + <nl> op = MASK_OP_MAJOR ( ctx -> opcode ); <nl> rs = ( ctx -> opcode >> 21 ) & 0x1f ; <nl> rt = ( ctx -> opcode >> 16 ) & 0x1f ;
enum { <nl>  <nl> static inline void flush_icache_range ( unsigned long start , unsigned long stop ) <nl> { <nl> -# if QEMU_GNUC_PREREQ ( 4 , 1 ) <nl> - __builtin___clear_cache (( char *) start , ( char *) stop ); <nl> -# else <nl> -# error not implemented <nl> -# endif <nl> }
static int cpu_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> - return cpu_post_load ( env , version_id ); <nl> + tlb_flush ( env , 1 ); <nl> + return 0 ; <nl> } <nl>  <nl> const VMStateDescription vmstate_cpu = {
static int vmdk_open_vmdk4 ( BlockDriverState * bs , <nl> } <nl> extent -> compressed = <nl> le16_to_cpu ( header . compressAlgorithm ) == VMDK4_COMPRESSION_DEFLATE ; <nl> + if ( extent -> compressed ) { <nl> + g_free ( s -> create_type ); <nl> + s -> create_type = g_strdup (" streamOptimized "); <nl> + } <nl> extent -> has_marker = le32_to_cpu ( header . flags ) & VMDK4_FLAG_MARKER ; <nl> extent -> version = le32_to_cpu ( header . version ); <nl> extent -> has_zero_grain = le32_to_cpu ( header . flags ) & VMDK4_FLAG_ZERO_GRAIN ;
* significant half of a uint64_t struct member . <nl> */ <nl> # ifdef HOST_WORDS_BIGENDIAN <nl> -# define offsetoflow32 ( S , M ) offsetof ( S , M + sizeof ( uint32_t )) <nl> +# define offsetoflow32 ( S , M ) ( offsetof ( S , M ) + sizeof ( uint32_t )) <nl> # else <nl> # define offsetoflow32 ( S , M ) offsetof ( S , M ) <nl> # endif
static int sysbus_device_init ( DeviceState * dev ) <nl> SysBusDevice * sd = SYS_BUS_DEVICE ( dev ); <nl> SysBusDeviceClass * sbc = SYS_BUS_DEVICE_GET_CLASS ( sd ); <nl>  <nl> + if (! sbc -> init ) { <nl> + return 0 ; <nl> + } <nl> return sbc -> init ( sd ); <nl> } <nl> 
static int htab_load ( QEMUFile * f , void * opaque , int version_id ) <nl> if ( section_hdr ) { <nl> /* First section , just the hash shift */ <nl> if ( spapr -> htab_shift != section_hdr ) { <nl> + error_report (" htab_shift mismatch : source % d target % d ", <nl> + section_hdr , spapr -> htab_shift ); <nl> return - EINVAL ; <nl> } <nl> return 0 ;
void * __lzma_wrap_alloc ( void * unused , size_t size ) { <nl> return NULL ; <nl> } <nl>  <nl> - return cli_malloc ( size ); <nl> + return cli_calloc ( 1 , size ); <nl> } <nl> void __lzma_wrap_free ( void * unused , void * freeme ) { <nl> UNUSEDPARAM ( unused );
const char * cl_strerror ( int clerror ) <nl> return " Can ' t map file into memory "; <nl> case CL_EMEM : <nl> return " Can ' t allocate memory "; <nl> + case CL_ETIMEOUT : <nl> + return " Time limit reached "; <nl> /* internal ( needed for debug messages ) */ <nl> case CL_EMAXREC : <nl> return " CL_EMAXREC ";
static struct dconf_module modules [] = { <nl> { " PE ", " MD5SECT ", PE_CONF_MD5SECT , 1 }, <nl> { " PE ", " UPX ", PE_CONF_UPX , 1 }, <nl> { " PE ", " FSG ", PE_CONF_FSG , 1 }, <nl> - { " PE ", " SWIZZOR ", PE_CONF_SWIZZOR , 1 }, <nl> + { " PE ", " SWIZZOR ", PE_CONF_SWIZZOR , 0 }, <nl>  <nl> { " PE ", " PETITE ", PE_CONF_PETITE , 1 }, <nl> { " PE ", " PESPIN ", PE_CONF_PESPIN , 1 },
void cache_add ( unsigned char * md5 , size_t size , cli_ctx * ctx ) { <nl> uint32_t level ; <nl> struct CACHE * c ; <nl>  <nl> - if (! ctx || ! ctx -> engine || ! ctx -> engine -> cache ) <nl> + if (! ctx || ! ctx -> engine || ! ctx -> engine -> cache || ctx -> found_possibly_unwanted ) <nl> return ; <nl>  <nl> level = (* ctx -> fmap && (* ctx -> fmap )-> dont_cache_flag ) ? ctx -> recursion : 0 ;
int cli_fmap_scandesc ( cli_ctx * ctx , cli_file_t ftype , uint8_t ftonly , struct cli <nl> type = ret ; <nl> } <nl>  <nl> - if ( hdb && ! SCAN_ALL ) { <nl> + if ( hdb ) { <nl> const void * data = buff + maxpatlen * ( offset != 0 ); <nl> uint32_t data_len = bytes - maxpatlen * ( offset != 0 ); <nl> 
wwwconnect ( const char * server , const char * proxy , int pport , char * ip , <nl> } <nl> else <nl> i ++; <nl> - mirman_update ( addr , rp -> ai_family , mdat , 2 ); <nl> + if ( mdat ) <nl> + mirman_update ( addr , rp -> ai_family , mdat , 2 ); <nl> continue ; <nl> } <nl> else
int cli_scanhwp3 ( cli_ctx * ctx ) <nl>  <nl> offset += HWP3_DOCSUMMARY_SIZE ; <nl>  <nl> + /* password - protected document - cannot parse */ <nl> + if ( docinfo . di_passwd ) { <nl> + cli_dbgmsg (" HWP3 . x : password - protected file , skip parsing \ n "); <nl> + return CL_SUCCESS ; <nl> + } <nl> + <nl> if ( docinfo . di_infoblksize ) { <nl> /* OPTIONAL TODO : HANDLE OPTIONAL INFORMATION BLOCK # 0 ' s FOR PRECLASS */ <nl> offset += docinfo . di_infoblksize ;
void * cli_jsonarray_nojson ( const char * key ) <nl> int cli_jsonint_array_nojson ( int32_t val ) <nl> { <nl> nojson_func (" nojson : % d \ n ", val ); <nl> + return CL_SUCCESS ; <nl> } <nl>  <nl> # endif
bool read_sequence ( u8 const * & src , u8 const * const end , u8 const * & literal , u <nl> literal = src ; <nl> src += literal_len ; <nl>  <nl> - if ( src > end - 2 ) <nl> + if ( src > end - 2 || src < literal ) <nl> return false ; <nl>  <nl> match_dist = * src ++;
GlyphCache :: GlyphCache ( const Face & face , const uint32 face_options ) <nl> } <nl> delete _glyph_loader ; <nl> _glyph_loader = 0 ; <nl> + // coverity [ leaked_storage : FALSE ] - calling read_glyph on index 0 saved <nl> + // glyphs as _glyphs [ 0 ]. Setting _glyph_loader to nullptr here flags that <nl> + // the dtor needs to call delete [] on _glyphs [ 0 ] to release what was allocated <nl> + // as glyphs <nl> } <nl>  <nl> if ( _glyphs && glyph ( 0 ) == 0 )
static int count_column_width ( struct libscols_table * tb , <nl>  <nl> cl -> width = 0 ; <nl>  <nl> - <nl> - if ( cl -> width_min ) { <nl> + if (! cl -> width_min ) { <nl> if ( cl -> width_hint < 1 && scols_table_is_maxout ( tb )) <nl> cl -> width_min = ( size_t ) ( cl -> width_hint * tb -> termwidth ) - ( is_last_column ( cl ) ? 0 : 1 ); <nl> if ( scols_cell_get_data (& cl -> header )) {
read_extended_partition ( int fd , struct partition * ep , <nl> if (++ loopct > 100 ) <nl> return n ; <nl>  <nl> - bp = getblock ( fd , here ); <nl> + bp = getblock ( fd , here * ssf ); /* in 512 blocks */ <nl> if ( bp == NULL ) <nl> return n ; <nl> 
int main ( int argc , char ** argv ) <nl> break ; <nl> case ' h ': <nl> usage ( stdout ); <nl> - return 0 ; <nl> + return EXIT_SUCCESS ; <nl> default : <nl> usage ( stderr ); <nl> /* Do not exit ! */ <nl> int main ( int argc , char ** argv ) <nl> /* <nl> * User pressed Control - D . <nl> */ <nl> - return 0 ; <nl> + return EXIT_SUCCESS ; <nl> }
static FILE * dump ( FILE * in , const char * filename , int follow , FILE * out ) <nl> struct utmp ut ; <nl>  <nl> if ( follow ) <nl> - fseek ( in , - 10 * sizeof ( ut ), SEEK_END ); <nl> + ignore_result ( fseek ( in , - 10 * sizeof ( ut ), SEEK_END ) ); <nl>  <nl> while ( fread (& ut , sizeof ( ut ), 1 , in ) == 1 ) <nl> print_utline ( ut , out );
static void <nl> xbsd_write_bootstrap ( void ) <nl> { <nl> char * bootdir = BSD_LINUX_BOOTDIR ; <nl> - char path [ MAXPATHLEN ]; <nl> + char path [ sizeof ( BSD_LINUX_BOOTDIR ) + 1 + 2 + 4 ]; /* BSD_LINUX_BOOTDIR + / + { sd , wd } + boot */ <nl> char * dkbasename ; <nl> struct xbsd_disklabel dl ; <nl> char * d , * p , * e ;
int read_hypervisor_dmi ( void ) <nl> if ( rc ) <nl> goto done ; <nl> free ( buf ); <nl> - <nl> + buf = NULL ; <nl> memory_scan : <nl> # if defined ( __x86_64__ ) || defined ( __i386__ ) <nl> /* Fallback to memory scan ( x86 , x86_64 ) */
 <nl> # if defined ( __i386__ ) || defined ( __sparc__ ) || defined ( __arm__ ) || \ <nl> defined ( __mips__ ) || defined ( __s390__ ) || defined ( __sh__ ) || \ <nl> - defined ( __aarch64__ ) || \ <nl> + defined ( __aarch64__ ) || defined ( __xtensa__ ) || \ <nl> defined ( __x86_64__ ) || defined ( __avr32__ ) || defined ( __cris__ ) <nl> # define BSD_LABELSECTOR 1 <nl> # define BSD_LABELOFFSET 0
static int probe_minix ( blkid_probe pr , <nl>  <nl> if ( version <= 2 ) { <nl> struct minix_super_block * sb = ( struct minix_super_block *) data ; <nl> - int zones , ninodes , imaps , zmaps , firstz ; <nl> + unsigned long zones , ninodes , imaps , zmaps ; <nl> + off_t firstz ; <nl>  <nl> if ( sb -> s_imap_blocks == 0 || sb -> s_zmap_blocks == 0 || <nl> sb -> s_log_zone_size != 0 )
static int print_udev_ambivalent ( blkid_probe pr ) <nl>  <nl> if ( count > 1 ) { <nl> *( val + valsz - 1 ) = '\ 0 '; /* rem tailing whitespace */ <nl> - printf (" ID_FS_AMBIVALEN =% s \ n ", val ); <nl> + printf (" ID_FS_AMBIVALENT =% s \ n ", val ); <nl> rc = 0 ; <nl> } <nl> done :
looplist_open ( struct looplist * ll , int flag ) <nl> static void <nl> looplist_close ( struct looplist * ll ) <nl> { <nl> - if ( ll -> minors ) <nl> - free ( ll -> minors ); <nl> + free ( ll -> minors ); <nl> if ( ll -> proc ) <nl> fclose ( ll -> proc ); <nl> ll -> minors = NULL ;
int setpwnam ( struct passwd * pwd ) <nl> } <nl>  <nl> /* Set up the limits so that we ' re not foiled */ <nl> - static void pw_init () <nl> + static void pw_init ( void ) <nl> { <nl> struct rlimit rlim ; <nl> 
int proc_next_tid ( struct proc_tasks * tasks , pid_t * tid ) <nl> struct dirent * d ; <nl> char * end ; <nl>  <nl> + if (! tasks || ! tid ) <nl> + return - 1 ; <nl> + <nl> * tid = 0 ; <nl> errno = 0 ; <nl> 
details_only : <nl> /* <nl> * Gather PART_ENTRY_ * values if the current device is a partition . <nl> */ <nl> - if (( blkid_partitions_get_flags ( pr ) & BLKID_PARTS_ENTRY_DETAILS )) { <nl> + if (! chn -> binary && <nl> + ( blkid_partitions_get_flags ( pr ) & BLKID_PARTS_ENTRY_DETAILS )) { <nl> + <nl> if (! blkid_partitions_probe_partition ( pr )) <nl> rc = 0 ; <nl> }
long old_style_option ( int * argc , char ** argv ) <nl> lines = strtol_or_err ( argv [ i ] + 1 , <nl> _ (" failed to parse number of lines ")); <nl> nargs --; <nl> - memmove ( argv + i , argv + i + 1 , sizeof ( char *) * nargs ); <nl> + if ( nargs - i ) <nl> + memmove ( argv + i , argv + i + 1 , <nl> + sizeof ( char *) * ( nargs - i )); <nl> } else <nl> i ++; <nl> }
wchar_t * buf ; <nl>  <nl> static void sig_handler ( int signo __attribute__ (( __unused__ ))) <nl> { <nl> - free ( buf ); <nl> _exit ( EXIT_SUCCESS ); <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> if (( pwd = getrootpwent ( opt_e )) == NULL ) { <nl> warnx ( _ (" cannot open password database .")); <nl> sleep ( 2 ); <nl> + return EXIT_FAILURE ; <nl> } <nl>  <nl> /*
makemsg ( fname ) <nl> putc ('\ n ', fp ); <nl> cnt = 0 ; <nl> } <nl> - carefulputc ( ch , fp ); <nl> + if ( ch != '\ n ') <nl> + carefulputc ( ch , fp ); <nl> } <nl> } <nl> fprintf ( fp , "% 79s \ r \ n ", " ");
*/ <nl> blkid_loff_t blkid_get_dev_size ( int fd ) <nl> { <nl> - struct stat st ; <nl> unsigned long long bytes ; <nl>  <nl> - if ( fstat ( fd , & st ) == 0 && S_ISREG ( st . st_mode )) <nl> - return st . st_size ; <nl> - <nl> if ( blkdev_get_size ( fd , & bytes )) <nl> return 0 ; <nl> 
void GeoIPManager :: configure () <nl> const bool enabled = Preferences :: instance ()-> resolvePeerCountries (); <nl> if ( m_enabled != enabled ) { <nl> m_enabled = enabled ; <nl> - if ( m_enabled && ! m_geoIPDatabase ) <nl> + if ( m_enabled && ! m_geoIPDatabase ) { <nl> loadDatabase (); <nl> + } <nl> + else if (! m_enabled && m_geoIPDatabase ) { <nl> + delete m_geoIPDatabase ; <nl> + m_geoIPDatabase = 0 ; <nl> + } <nl> } <nl> } <nl> 
void TorrentModel :: removeTorrent ( const QString & hash ) <nl> qDebug () << Q_FUNC_INFO << hash << row ; <nl> if ( row >= 0 ) { <nl> beginRemoveTorrent ( row ); <nl> + delete m_torrents [ row ]; <nl> m_torrents . removeAt ( row ); <nl> endRemoveTorrent (); <nl> }
void MainWindow :: serverDisconnected ( QAbstractSocket :: SocketError err , QString re <nl> if (! Database :: getDigest ( host , port ). isNull ()) { <nl> basereason = tr ("< b > WARNING :</ b > The server presented a certificate that was different from the stored one ."); <nl> } else { <nl> - basereason = tr (" Sever presented a certificate which failed verification ."); <nl> + basereason = tr (" Server presented a certificate which failed verification ."); <nl> } <nl> QStringList qsl ; <nl> foreach ( QSslError e , g . sh -> qlErrors )
void Settings :: save () { <nl> SAVELOAD ( bSuppressMacEventTapWarning , " shortcut / mac / suppresswarning "); <nl> SAVELOAD ( bEnableEvdev , " shortcut / linux / evdev / enable "); <nl> SAVELOAD ( bEnableXInput2 , " shortcut / x11 / xinput2 / enable "); <nl> + SAVELOAD ( bEnableGKey , " shortcut / gkey "); <nl> SAVELOAD ( bEnableXboxInput , " shortcut / windows / xbox / enable "); <nl> SAVELOAD ( bEnableWinHooks , " winhooks "); <nl> SAVELOAD ( bDirectInputVerboseLogging , " shortcut / windows / directinput / verboselogging ");
static int readPackageHeaders ( FD_t fd , /*@ out @*/ struct rpmlead * leadPtr , <nl> int rpmReadPackageInfo ( FD_t fd , Header * sigp , Header * hdrp ) <nl> { <nl> int rc = readPackageHeaders ( fd , NULL , sigp , hdrp ); <nl> + if ( rc ) <nl> + return rc ; <nl> if ( hdrp && * hdrp && sigp && * sigp ) <nl> headerMergeLegacySigs (* hdrp , * sigp ); <nl> return rc ;
int openDatabase ( char * prefix , char * dbpath , rpmdb * rpmdbp , int mode , <nl> int i ; <nl> struct flock lockinfo ; <nl>  <nl> + /* we should accept NULL as a valid prefix */ <nl> + if (! prefix ) prefix =""; <nl> + <nl> i = strlen ( dbpath ); <nl> if ( dbpath [ i - 1 ] != '/') { <nl> filename = alloca ( i + 2 );
static int doSign ( poptContext optCon ) <nl> } <nl>  <nl> exit : <nl> + free ( passPhrase ); <nl> free ( name ); <nl> return rc ; <nl> }
static int rdToken ( ParseState state ) <nl> size_t ts ; <nl>  <nl> p ++; <nl> - for ( ts = 1 ; p [ ts ] && p [ ts ] != '\"'; ts ++); <nl> + for ( ts = 0 ; p [ ts ] && p [ ts ] != '\"'; ts ++); <nl> temp = xmalloc ( ts + 1 ); <nl> memcpy ( temp , p , ts ); <nl> p += ts - 1 ;
int rpmVerifyScript ( char * root , Header h , int err ) { <nl> exit (- 1 ); <nl> } <nl>  <nl> - close ( out ); <nl> - close ( err ); <nl> + if ( out > 2 ) close ( out ); <nl> + if ( err > 2 ) close ( err ); <nl> close ( fd ); <nl> if (! rpmIsVerbose ()) close ( out ); <nl> 
int rpmdbRebuild ( char * rootdir ) { <nl> " to recover ", dbpath , newdbpath ); <nl> return 1 ; <nl> } <nl> - rmdir ( newdbpath ); <nl> + if ( rmdir ( newdbpath )) <nl> + rpmMessage ( RPMERR_RMDIR , " failed to remove % s : % s \ n ", <nl> + newdbpath , strerror ( errno )); <nl> } <nl>  <nl> 
char * headerSprintf ( Header h , const char * origFmt , <nl>  <nl> strcat ( answer , piece ); <nl> answerLength += pieceLength ; <nl> + free ( piece ); <nl> } <nl> } <nl> 
int rpmRunTransactions ( rpmTransactionSet ts , rpmCallbackFunction notify , <nl> ( void *) NULL , & fi -> fc )) { <nl> fi -> fc = 0 ; <nl> fi -> h = alp -> h ; <nl> + hdrs [ alp - al -> list ] = headerLink ( fi -> h ); <nl> continue ; <nl> } <nl> 
static int makeHDRDigest ( Header sigh , const char * file , rpmTagVal sigTag ) <nl> ( void ) rpmDigestUpdate ( ctx , utd . data , utd . count ); <nl> ( void ) rpmDigestFinal ( ctx , ( void **)& SHA1 , NULL , 1 ); <nl> rpmtdFreeData (& utd ); <nl> + } else { <nl> + rpmlog ( RPMLOG_ERR , _ (" Cannot sign RPM v3 packages \ n ")); <nl> + goto exit ; <nl> } <nl>  <nl> if ( SHA1 == NULL )
static int installArchive ( char * prefix , int fd , struct fileToInstall * files , <nl> kill ( SIGTERM , child ); <nl> } <nl>  <nl> - if ( write ( p [ 1 ], buf , bytesRead ) != bytesRead ) { <nl> + if ( bytesRead && write ( p [ 1 ], buf , bytesRead ) != bytesRead ) { <nl> cpioFailed = 1 ; <nl> childDead = 1 ; <nl> kill ( SIGTERM , child );
envelope_err : <nl> void unwrap_flaglist ( strarray_t * strlist , strarray_t ** flaglist , <nl> variable_list_t * variables ) <nl> { <nl> + if (! strlist ) return ; <nl> + <nl> int len = strarray_size ( strlist ); <nl>  <nl> if ( len ) {
EXPORTED void conversations_rename_cidentry ( struct conversations_state * state , <nl> { <nl> struct rename_rock rrock ; <nl>  <nl> + if ( from == to ) return ; <nl> + <nl> memset (& rrock , 0 , sizeof ( rrock )); <nl> rrock . state = state ; <nl> rrock . from_cid = from ;
# include " parseaddr . h " <nl> # include " xmalloc . h " <nl>  <nl> + static char parseaddr_unspecified_domain [] = " unspecified - domain "; <nl> + <nl> static void parseaddr_append (); <nl> static int parseaddr_phrase (); <nl> static int parseaddr_domain (); <nl> char ** freemep ; <nl> newaddr -> mailbox = mailbox ; <nl>  <nl> if ( domain && !* domain ) { <nl> - domain = " unspecified - domain "; <nl> + domain = parseaddr_unspecified_domain ; <nl> } <nl> newaddr -> domain = domain ; <nl> 
void config_read_file ( const char * filename ) <nl> infile = fopen ( filename , " r "); <nl>  <nl> if (! infile ) { <nl> - snprintf ( buf , bufsize , " can ' t open configuration file % s : % m ", <nl> - filename ); <nl> + snprintf ( buf , bufsize , " can ' t open configuration file % s : % s ", <nl> + filename , strerror ( errno )); <nl> fatal ( buf , EC_CONFIG ); <nl> } <nl> 
char ** reply ; <nl> r = connect ( s , ( struct sockaddr *)& srvaddr , sizeof ( srvaddr )); <nl> if ( r == - 1 ) { <nl> * reply = " cannot connect to pwcheck server "; <nl> - return 0 ; <nl> + return 1 ; <nl> } <nl>  <nl> iov [ 0 ]. iov_base = user ;
static int jmap_write_calendarevent ( json_t * event , <nl> } <nl> jmap_calendarevent_to_ical ( comp , event , & rock ); <nl> jmap_timezones_to_ical ( ical , & rock ); <nl> + if ( rock . oldcomp ) { <nl> + icalcomponent_free ( rock . oldcomp ); <nl> + } <nl> calevent_rock_free (& rock ); <nl>  <nl> /* Handle any property errors and bail out . */
static int caldav_delete_cal ( struct transaction_t * txn , <nl> sched_reply ( userid , strarray_nth (& schedule_addresses , 0 ), ical , NULL ); <nl> } <nl>  <nl> + free ( userid ); <nl> strarray_fini (& schedule_addresses ); <nl> } <nl> 
static json_t * jmap_mailbox_from_mbox ( struct mailbox * mbox , <nl> } <nl> } <nl> json_object_set_new ( obj , " sortOrder ", json_integer ( sortOrder )); <nl> + buf_free (& attrib ); <nl> } <nl> if ( _wantprop ( props , " parentId ")) { <nl> json_object_set_new ( obj , " parentId ", parent && parent != inbox ?
static int mboxlist_do_find ( struct find_rock * rock , const strarray_t * patterns ) <nl> int i ; <nl> const char * p ; <nl>  <nl> + if ( patterns -> count < 1 ) return 0 ; /* nothing to do */ <nl> + <nl> for ( i = 0 ; i < patterns -> count ; i ++) { <nl> glob * g = glob_init ( strarray_nth ( patterns , i ), rock -> namespace -> hier_sep ); <nl> ptrarray_append (& rock -> globs , g );
int sync_update_mailbox ( struct sync_folder * local , <nl> flags |= SYNC_FLAG_ISREPEAT ; <nl>  <nl> if ( r == IMAP_AGAIN ) { <nl> + local -> batchsize = 0 ; /* don ' t batch the re - update , means sync to 2 . 4 will still work after fullsync */ <nl> r = mailbox_full_update ( local , reserve_list , sync_be , flags ); <nl> if (! r ) r = update_mailbox_once ( local , remote , topart , <nl> reserve_list , sync_be , flags );
int deliver ( message_data_t * msgdata , char * authuser , <nl> proxy_adddest (& dlist , recip , n , mbentry -> server , authuser ); <nl> status [ n ] = nosieve ; <nl> } <nl> - else { <nl> + else if (! r ) { <nl> /* local mailbox */ <nl> mydata . cur_rcpt = n ; <nl> # ifdef USE_SIEVE
EXPORTED int index_snippets ( struct index_state * state , <nl> int nmatches = 0 ; <nl> struct snippet_rock srock ; <nl>  <nl> + /* reload index */ <nl> + r = index_refresh ( state ); <nl> + if ( r ) return r ; <nl> + <nl> bx = search_begin_search ( state -> mailbox , SEARCH_MULTIPLE ); <nl> if (! bx ) { <nl> r = IMAP_INTERNAL ;
EXPORTED void mbname_free ( mbname_t ** mbnamep ) <nl> free ( mbname -> intname ); <nl> free ( mbname -> extname ); <nl> free ( mbname -> extuserid ); <nl> + free ( mbname -> recipient ); <nl>  <nl> /* thing itself */ <nl> free ( mbname );
int service_main ( int argc , char ** argv , <nl> struct io_count * io_count_stop ; <nl>  <nl> if ( config_iolog ) { <nl> - io_count_start = malloc ( sizeof ( struct io_count )); <nl> - io_count_stop = malloc ( sizeof ( struct io_count )); <nl> + io_count_start = xmalloc ( sizeof ( struct io_count )); <nl> + io_count_stop = xmalloc ( sizeof ( struct io_count )); <nl> read_io_count ( io_count_start ); <nl> } <nl> 
EXPORTED void buf_ensure ( struct buf * buf , size_t n ) <nl> /* protect against wrap */ <nl> assert ( newlen >= buf -> len ); <nl>  <nl> + if ( buf -> alloc >= newlen ) <nl> + return ; <nl> + <nl> if ( buf -> alloc ) { <nl> buf -> s = xrealloc ( buf -> s , newlen ); <nl> }
static int chkchildren ( char * name , <nl> int r ; <nl>  <nl> r = mboxlist_lookup ( name , & mbentry , 0 ); <nl> + /* deleted mailboxes don ' t count as children */ <nl> + if ( r == IMAP_MAILBOX_NONEXISTENT ) return 0 ; <nl> if ( r ) return r ; <nl>  <nl> if (! strcmp ( part , mbentry -> partition ))
static void add_attendees ( icalcomponent * ical , <nl> prop = icalcomponent_get_next_invitee ( comp )) { <nl>  <nl> const char * attendee = icalproperty_get_invitee ( prop ); <nl> + if (! attendee ) continue ; <nl> + <nl> if (! strncasecmp ( attendee , " mailto :", 7 )) attendee += 7 ; <nl>  <nl> /* Skip where attendee == organizer */
int mailbox_ensure_cache ( struct mailbox * mailbox , unsigned offset ) <nl> mailbox -> cache_fd = open ( fname , openflags , 0 ); <nl> if ( mailbox -> cache_fd == - 1 ) <nl> goto fail ; <nl> + <nl> + if ( mailbox -> cache_buf . s ) <nl> + map_free (( const char **)& mailbox -> cache_buf . s , & mailbox -> cache_len ); <nl> + mailbox -> cache_buf . len = 0 ; <nl> } <nl>  <nl> if ( offset >= mailbox -> cache_buf . len ) {
EXPORTED int mboxlist_renamemailbox ( const char * oldname , const char * newname , <nl>  <nl> /* log the rename */ <nl> sync_log_mailbox_double ( oldname , newname ); <nl> + /* and log an append so that squatter indexes it */ <nl> + sync_log_append ( newname ); <nl> } <nl>  <nl> /* free memory */
static int is_incompressible ( const char * p , size_t n ) <nl> */ <nl> EXPORTED int prot_data_boundary ( struct protstream * s ) <nl> { <nl> - s -> boundary = 1 ; <nl> + // XXX - appears to be broken , so just don ' t set the boundary . We ' ll <nl> + // spend trivially more CPU when transferring binary parts . Boo hoo <nl> + // re - enable this once the bug is fixed <nl> + // s -> boundary = 1 ; <nl> return 0 ; <nl> } <nl> 
static int caldav_put ( struct transaction_t * txn , <nl> } <nl> else { <nl> /* Attendee scheduling object resource */ <nl> + if (! oldical ) { <nl> + /* Can ' t reply to a non - existent invitation */ <nl> + ret = HTTP_FORBIDDEN ; <nl> + goto done ; <nl> + } <nl> sched_reply ( userid , oldical , ical ); <nl> } <nl> 
void do_zonedir ( const char * dir , struct hash_table * tzentries , <nl> ical = icalparser_parse_string ( base ); <nl> map_free (& base , & len ); <nl>  <nl> + if (! ical ) continue ; /* skip non - iCalendar files */ <nl> + <nl> comp = icalcomponent_get_first_component ( ical , <nl> ICAL_VTIMEZONE_COMPONENT ); <nl> prop = icalcomponent_get_first_property ( comp , ICAL_TZID_PROPERTY );
static int usersubs_cb ( void * rock , const char * key , size_t keylen , <nl> mboxname_userownsmailbox ( mbrock -> userid , mboxname )) return 0 ; <nl>  <nl> r = mboxlist_lookup ( mboxname , & mbrock -> mbentry , NULL ); <nl> + if ( r == IMAP_MAILBOX_NONEXISTENT ) return 0 ; <nl> + <nl> if ( r ) { <nl> syslog ( LOG_INFO , " mboxlist_lookup (% s ) failed : % s ", <nl> mboxname , error_message ( r ));
int meth_mkcol ( struct transaction_t * txn , void * params ) <nl> /* Start construction of our mkcol / mkcalendar response */ <nl> buf_appendcstr (& txn -> buf , "- response "); <nl> root = init_xml_response ( buf_cstring (& txn -> buf ), NS_REQ_ROOT , root , ns ); <nl> + buf_reset (& txn -> buf ); <nl> if (! root ) { <nl> ret = HTTP_SERVER_ERROR ; <nl> txn -> error . desc = " Unable to create XML response \ r \ n ";
int report_sync_col ( struct transaction_t * txn , <nl>  <nl> /* XXX Handle Depth ( cal - home - set at toplevel ) */ <nl>  <nl> + memset (& istate , 0 , sizeof ( struct index_state )); <nl> istate . map = NULL ; <nl>  <nl> /* Open mailbox for reading */
int cacheScheduleIOPushJobs ( int flags ) { <nl>  <nl> if ( op -> type != REDIS_IO_LOAD && flags & REDIS_IO_ONLYLOADS ) break ; <nl>  <nl> - if (!( flags & REDIS_IO_ASAP ) && <nl> + /* Don ' t execute SAVE before the scheduled time for completion */ <nl> + if ( op -> type == REDIS_IO_SAVE && !( flags & REDIS_IO_ASAP ) && <nl> ( now - op -> ctime ) < server . cache_flush_delay ) break ; <nl>  <nl> /* Don ' t add a SAVE job in the IO thread queue if there is already
void moduleHandleBlockedClients ( void ) { <nl> if ( bc -> privdata && bc -> free_privdata ) <nl> bc -> free_privdata ( bc -> privdata ); <nl> zfree ( bc ); <nl> - if ( c != NULL ) unblockClient ( bc -> client ); <nl> + if ( c != NULL ) unblockClient ( c ); <nl>  <nl> /* Lock again before to iterate the loop . */ <nl> pthread_mutex_lock (& moduleUnblockedClientsMutex );
void sendReplyToClient ( aeEventLoop * el , int fd , void * privdata , int mask ) { <nl>  <nl> /* If the buffer was sent , set bufpos to zero to continue with <nl> * the remainder of the reply . */ <nl> - if ( c -> sentlen == c -> bufpos ) { <nl> + if (( int ) c -> sentlen == c -> bufpos ) { <nl> c -> bufpos = 0 ; <nl> c -> sentlen = 0 ; <nl> }
static void repl ( void ) { <nl> } <nl>  <nl> elapsed = mstime ()- start_time ; <nl> - if ( elapsed >= 500 ) { <nl> + if ( elapsed >= 500 && <nl> + config . output == OUTPUT_STANDARD ) <nl> + { <nl> printf ("(%. 2fs )\ n ",( double ) elapsed / 1000 ); <nl> } <nl> }
void latencyAddSample ( char * event , mstime_t latency ) { <nl> if ( ts -> samples [ prev ]. time == now ) { <nl> if ( latency > ts -> samples [ prev ]. latency ) <nl> ts -> samples [ prev ]. latency = latency ; <nl> + if ( latency > ts -> max ) ts -> max = latency ; <nl> return ; <nl> } <nl> 
void xreadCommand ( client * c ) { <nl> } <nl>  <nl> if ( strcmp ( c -> argv [ i ]-> ptr ,"$") == 0 ) { <nl> + if ( xreadgroup ) { <nl> + addReplyError ( c ," The $ ID can be specified only when calling " <nl> + " XREAD without GROUP option ."); <nl> + goto cleanup ; <nl> + } <nl> if ( o ) { <nl> stream * s = o -> ptr ; <nl> ids [ id_idx ] = s -> last_id ;
size_t zmalloc_size ( void * ptr ) { <nl> return size + PREFIX_SIZE ; <nl> } <nl> size_t zmalloc_usable ( void * ptr ) { <nl> - return zmalloc_usable ( ptr )- PREFIX_SIZE ; <nl> + return zmalloc_size ( ptr )- PREFIX_SIZE ; <nl> } <nl> # endif <nl> 
int luaRedisGenericCommand ( lua_State * lua , int raise_error ) { <nl> luaSortArray ( lua ); <nl> } <nl> sdsfree ( reply ); <nl> + c -> reply_bytes = 0 ; <nl>  <nl> cleanup : <nl> /* Clean up . Command code may have changed argv / argc so we use the
int rdbSaveDoubleValue ( rio * rdb , double val ) { <nl>  <nl> /* For information about double serialization check rdbSaveDoubleValue () */ <nl> int rdbLoadDoubleValue ( rio * rdb , double * val ) { <nl> - char buf [ 128 ]; <nl> + char buf [ 256 ]; <nl> unsigned char len ; <nl>  <nl> if ( rioRead ( rdb ,& len , 1 ) == 0 ) return - 1 ;
static int handle_metadata ( RTMPContext * rt , RTMPPacket * pkt ) <nl> pts = cts ; <nl> ts += cts - pts ; <nl> pts = cts ; <nl> + if ( size + 3 + 4 > pkt -> data + pkt -> size - next ) <nl> + break ; <nl> bytestream_put_byte (& p , type ); <nl> bytestream_put_be24 (& p , size ); <nl> bytestream_put_be24 (& p , ts );
static int fic_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " Could not allocate slice data .\ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( ctx -> slice_data , 0 , nslices * sizeof ( ctx -> slice_data [ 0 ])); <nl>  <nl> for ( slice = 0 ; slice < nslices ; slice ++) { <nl> unsigned slice_off = AV_RB32 ( src + tsize + FIC_HEADER_SIZE + slice * 4 );
int avfilter_graph_add_filter ( AVFilterGraph * graph , AVFilterContext * filter ) <nl> graph -> filters = filters ; <nl> graph -> filters [ graph -> nb_filters ++] = filter ; <nl>  <nl> +# if FF_API_FOO_COUNT <nl> + graph -> filter_count = graph -> nb_filters ; <nl> +# endif <nl> + <nl> return 0 ; <nl> } <nl> 
static int setup_hwaccel ( AVCodecContext * avctx , <nl> return AVERROR ( ENOMEM ); <nl> } <nl>  <nl> + avctx -> hwaccel = hwa ; <nl> if ( hwa -> init ) { <nl> ret = hwa -> init ( avctx ); <nl> if ( ret < 0 ) { <nl> av_freep (& avctx -> internal -> hwaccel_priv_data ); <nl> + avctx -> hwaccel = NULL ; <nl> return ret ; <nl> } <nl> } <nl>  <nl> - avctx -> hwaccel = hwa ; <nl> - <nl> return 0 ; <nl> } <nl> 
static int decode_frame_adu ( AVCodecContext * avctx , void * data , <nl> /* update codec info */ <nl> avctx -> sample_rate = s -> sample_rate ; <nl> avctx -> channels = s -> nb_channels ; <nl> + avctx -> channel_layout = s -> nb_channels == 1 ? AV_CH_LAYOUT_MONO : AV_CH_LAYOUT_STEREO ; <nl> if (! avctx -> bit_rate ) <nl> avctx -> bit_rate = s -> bit_rate ; <nl> 
static int mp3_write_header ( struct AVFormatContext * s ) <nl> char yeartxt [ 10 ]; <nl>  <nl> if ( s -> track ) <nl> - snprintf ( tracktxt , sizeof ( tracktxt ) - 1 , "% d ", s -> track ); <nl> + snprintf ( tracktxt , sizeof ( tracktxt ) , "% d ", s -> track ); <nl> if ( s -> year ) <nl> snprintf ( yeartxt , sizeof ( yeartxt ) , "% d ", s -> year ); <nl> 
static int udp_open ( URLContext * h , const char * uri , int flags ) <nl> goto fail ; <nl> } <nl>  <nl> - if (( s -> is_multicast || ! s -> local_port ) && ( h -> flags & AVIO_FLAG_READ )) <nl> + if (( s -> is_multicast || s -> local_port < 0 ) && ( h -> flags & AVIO_FLAG_READ )) <nl> s -> local_port = port ; <nl>  <nl> if ( localaddr [ 0 ])
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> if (! get_bits1 (& ctx -> gb ) || ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> transform_id = get_bits (& ctx -> gb , 5 ); <nl> - if (! transforms [ transform_id ]. inv_trans ) { <nl> + if ( transform_id >= FF_ARRAY_ELEMS ( transforms ) || <nl> + ! transforms [ transform_id ]. inv_trans ) { <nl> av_log_ask_for_sample ( avctx , " Unimplemented transform : % d !\ n ", transform_id ); <nl> return AVERROR_PATCHWELCOME ; <nl> }
int ffurl_close ( URLContext * h ) <nl> # if CONFIG_NETWORK <nl> ff_network_close (); <nl> # endif <nl> - if ( h -> prot -> priv_data_size ) <nl> + if ( h -> prot -> priv_data_size ) { <nl> + if ( h -> prot -> priv_data_class ) <nl> + av_opt_free ( h -> priv_data ); <nl> av_free ( h -> priv_data ); <nl> + } <nl> av_free ( h ); <nl> return ret ; <nl> }
int main ( int argc , char ** argv ) <nl>  <nl> ret = probe_file ( input_filename ); <nl>  <nl> + uninit_opts (); <nl> + av_dict_free (& fmt_entries_to_show ); <nl> + <nl> avformat_network_deinit (); <nl>  <nl> return ret ;
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> if ( st -> codec -> bit_rate > 0 ) { <nl> - if ( INT_MAX - st -> codec -> bit_rate > bit_rate ) { <nl> + if ( INT_MAX - st -> codec -> bit_rate < bit_rate ) { <nl> bit_rate = 0 ; <nl> break ; <nl> }
static int mjpeg_decode_frame ( AVCodecContext * avctx , <nl> *( dst ++) = x ; <nl> if ( x == 0xff ) <nl> { <nl> - while (* src == 0xff ) src ++; <nl> + while ( src < buf_end && x == 0xff ) <nl> + x = *( src ++); <nl>  <nl> - x = *( src ++); <nl> if ( x >= 0xd0 && x <= 0xd7 ) <nl> *( dst ++) = x ; <nl> else if ( x )
static int video_thread ( void * arg ) <nl> filt_out = is -> out_video_filter ; <nl> # endif <nl>  <nl> + if (! frame ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> for (;;) { <nl> # if CONFIG_AVFILTER <nl> AVRational tb ;
static int mov_read_stss ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> av_dlog ( c -> fc , " keyframe_count = % d \ n ", entries ); <nl>  <nl> + if (! entries ) <nl> + return 0 ; <nl> if ( entries >= UINT_MAX / sizeof ( int )) <nl> return AVERROR_INVALIDDATA ; <nl> sc -> keyframes = av_malloc ( entries * sizeof ( int ));
static int mov_write_udta_tag ( ByteIOContext * pb , MOVMuxContext * mov , <nl> put_be32 ( pb , size + 8 ); <nl> put_tag ( pb , " udta "); <nl> put_buffer ( pb , buf , size ); <nl> - av_free ( buf ); <nl> } <nl> + av_free ( buf ); <nl>  <nl> return 0 ; <nl> }
static void stream_component_close ( VideoState * is , int stream_index ) <nl> SDL_CloseAudio (); <nl>  <nl> packet_queue_end (& is -> audioq ); <nl> + av_free_packet (& is -> audio_pkt ); <nl> if ( is -> reformat_ctx ) <nl> av_audio_convert_free ( is -> reformat_ctx ); <nl> is -> reformat_ctx = NULL ;
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> * q ++ = 0xe0 ; <nl> } else if ( st -> codec -> codec_type == AVMEDIA_TYPE_AUDIO && <nl> ( st -> codec -> codec_id == CODEC_ID_MP2 || <nl> - st -> codec -> codec_id == CODEC_ID_MP3 )) { <nl> + st -> codec -> codec_id == CODEC_ID_MP3 || <nl> + st -> codec -> codec_id == CODEC_ID_AAC )) { <nl> * q ++ = 0xc0 ; <nl> } else { <nl> * q ++ = 0xbd ;
decode_intra_mb : <nl> sl -> intra4x4_pred_mode_cache [ scan8 [ i ]] = decode_cabac_mb_intra4x4_pred_mode ( sl , pred ); <nl>  <nl> ff_dlog ( h -> avctx , " i4x4 pred =% d mode =% d \ n ", pred , <nl> - h -> intra4x4_pred_mode_cache [ scan8 [ i ]]); <nl> + sl -> intra4x4_pred_mode_cache [ scan8 [ i ]]); <nl> } <nl> } <nl> write_back_intra_pred_mode ( h , sl );
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> for ( i = 0 ; i < sc -> chunk_count ; i ++) { <nl> current_offset = sc -> chunk_offsets [ i ]; <nl> - if ( stsc_index + 1 < sc -> stsc_count && <nl> + while ( stsc_index + 1 < sc -> stsc_count && <nl> i + 1 == sc -> stsc_data [ stsc_index + 1 ]. first ) <nl> stsc_index ++; <nl> for ( j = 0 ; j < sc -> stsc_data [ stsc_index ]. count ; j ++) {
static int set_channel_layout ( AVCodecContext * avctx , int channels , int num_core_ <nl> s -> channel_order_tab = ff_dca_channel_reorder_nolfe [ s -> amode ]; <nl> } <nl>  <nl> + if ( channels < ff_dca_channels [ s -> amode ]) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( channels > !! s -> lfe && <nl> s -> channel_order_tab [ channels - 1 - !! s -> lfe ] < 0 ) <nl> return AVERROR_INVALIDDATA ;
static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> char * p ; <nl> int file_idx = strtol ( in -> name , & p , 0 ); <nl>  <nl> - if ( file_idx < 0 || file_idx > nb_input_files ) { <nl> + if ( file_idx < 0 || file_idx >= nb_input_files ) { <nl> av_log ( NULL , AV_LOG_FATAL , " Invalid file index % d in filtegraph description % s .\ n ", <nl> file_idx , fg -> graph_desc ); <nl> exit_program ( 1 );
typedef struct FlacSubframe { <nl> int shift ; <nl> RiceContext rc ; <nl> int32_t samples [ FLAC_MAX_BLOCKSIZE ]; <nl> - int32_t residual [ FLAC_MAX_BLOCKSIZE ]; <nl> + int32_t residual [ FLAC_MAX_BLOCKSIZE + 1 ]; <nl> } FlacSubframe ; <nl>  <nl> typedef struct FlacFrame {
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
static void decode_frame ( SiprContext * ctx , SiprParameters * params , <nl> memcpy ( ctx -> postfilter_syn5k0 , ctx -> postfilter_syn5k0 + frame_size , <nl> LP_FILTER_ORDER * sizeof ( float )); <nl> } <nl> - memcpy ( ctx -> excitation , excitation - PITCH_DELAY_MAX - L_INTERPOL , <nl> + memmove ( ctx -> excitation , excitation - PITCH_DELAY_MAX - L_INTERPOL , <nl> ( PITCH_DELAY_MAX + L_INTERPOL ) * sizeof ( float )); <nl>  <nl> ff_acelp_apply_order_2_transfer_function ( out_data , synth ,
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> break ; <nl>  <nl> arraylen = avio_rb32 ( ioc ); <nl> + if ( arraylen >> 28 ) <nl> + break ; <nl> + <nl> /* <nl> * Expect only ' times ' or ' filepositions ' sub - arrays in other case refuse to use such metadata <nl> * for indexing
static int mkv_write_header ( AVFormatContext * s ) <nl> // currently defined level 1 element <nl> mkv -> main_seekhead = mkv_start_seekhead ( pb , mkv -> segment_offset , 10 ); <nl> mkv -> cluster_seekhead = mkv_start_seekhead ( pb , mkv -> segment_offset , 0 ); <nl> + if ( mkv -> main_seekhead == NULL || mkv -> cluster_seekhead == NULL ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( mkv_add_seekhead_entry ( mkv -> main_seekhead , MATROSKA_ID_INFO , url_ftell ( pb )) < 0 ) <nl> return - 1 ;
static av_cold int qsv_enc_init ( AVCodecContext * avctx ) <nl>  <nl> if ( q -> load_plugin != LOAD_PLUGIN_NONE ) { <nl> static const char * uid_hevcenc_sw = " 2fca99749fdb49aeb121a5b63ef568f7 "; <nl> - static const char * uid_hevcenc_hw = " e5400a06c74d41f5b12d430bbaa23d0b "; <nl> + static const char * uid_hevcenc_hw = " 6fadc791a0c2eb479ab6dcd5ea9da347 "; <nl>  <nl> if ( q -> qsv . load_plugins [ 0 ]) { <nl> av_log ( avctx , AV_LOG_WARNING ,
static av_cold int adpcm_encode_init ( AVCodecContext * avctx ) <nl> } <nl>  <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> - avctx -> coded_frame -> key_frame = 1 ; <nl>  <nl> return 0 ; <nl> error :
static int aac_adtstoasc_filter ( AVBitStreamFilterContext * bsfc , <nl> buf += get_bits_count (& gb )/ 8 ; <nl> } <nl> avctx -> extradata_size = 2 + pce_size ; <nl> - avctx -> extradata = av_malloc ( avctx -> extradata_size ); <nl> + avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> init_put_bits (& pb , avctx -> extradata , avctx -> extradata_size ); <nl> put_bits (& pb , 5 , hdr . object_type );
static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> FLACContext * s = avctx -> priv_data ; <nl> s -> avctx = avctx ; <nl>  <nl> + avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> + <nl> if ( avctx -> extradata_size > 4 ) { <nl> /* initialize based on the demuxer - supplied streamdata header */ <nl> if ( avctx -> extradata_size == FLAC_STREAMINFO_SIZE ) { <nl> static av_cold int flac_decode_init ( AVCodecContext * avctx ) <nl> } <nl> } <nl>  <nl> - avctx -> sample_fmt = SAMPLE_FMT_S16 ; <nl> return 0 ; <nl> } <nl> 
static int tls_open ( URLContext * h , const char * uri , int flags , AVDictionary ** op <nl> struct addrinfo hints = { 0 }, * ai = NULL ; <nl> const char * proxy_path ; <nl> int use_proxy ; <nl> -# if CONFIG_OPENSSL <nl> +# if CONFIG_OPENSSL && ! CONFIG_GNUTLS <nl> BIO * bio ; <nl> # endif <nl> 
static int mov_read_stsd ( MOVContext * c , ByteIOContext * pb , MOVAtom atom ) <nl> # endif <nl> /* no ifdef since parameters are always those */ <nl> case CODEC_ID_QCELP : <nl> + // force sample rate for qcelp when not stored in mov <nl> + if ( st -> codec -> codec_tag != MKTAG (' Q ',' c ',' l ',' p ')) <nl> + st -> codec -> sample_rate = 8000 ; <nl> st -> codec -> frame_size = 160 ; <nl> st -> codec -> channels = 1 ; /* really needed */ <nl> break ;
void mpeg4_encode_mb ( MpegEncContext * s , <nl> cbpy ^= 0xf ; <nl> put_bits ( pb2 , cbpy_tab [ cbpy ][ 1 ], cbpy_tab [ cbpy ][ 0 ]); <nl>  <nl> + if (! s -> progressive_sequence ){ <nl> + if ( cbp ) <nl> + put_bits ( pb2 , 1 , s -> interlaced_dct ); <nl> + } <nl> + <nl> if ( interleaved_stats ){ <nl> bits = get_bit_count (& s -> pb ); <nl> s -> misc_bits += bits - s -> last_bits ;
static int ffm_write_header ( AVFormatContext * s ) <nl> static int ffm_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> { <nl> int64_t dts ; <nl> - uint8_t header [ FRAME_HEADER_SIZE ]; <nl> + uint8_t header [ FRAME_HEADER_SIZE + 4 ]; <nl> int header_size = FRAME_HEADER_SIZE ; <nl>  <nl> dts = s -> timestamp + pkt -> dts ;
static int pva_read_packet ( AVFormatContext * s , AVPacket * pkt ) { <nl> flags = get_byte ( pb ); <nl> length = get_be16 ( pb ); <nl>  <nl> - pts_flag = ( flags & 0x10 ) >> 4 ; <nl> + pts_flag = flags & 0x10 ; <nl>  <nl> if ( syncword != PVA_MAGIC ) { <nl> av_log ( s , AV_LOG_ERROR , " invalid syncword \ n ");
static int vp9_handle_packet ( AVFormatContext * ctx , PayloadContext * rtp_vp9_ctx , <nl> return 0 ; <nl> } <nl>  <nl> + static void vp9_close_context ( PayloadContext * vp9 ) <nl> +{ <nl> + ffio_free_dyn_buf (& vp9 -> buf ); <nl> +} <nl> + <nl> RTPDynamicProtocolHandler ff_vp9_dynamic_handler = { <nl> . enc_name = " VP9 ", <nl> . codec_type = AVMEDIA_TYPE_VIDEO , <nl> . codec_id = AV_CODEC_ID_VP9 , <nl> . priv_data_size = sizeof ( PayloadContext ), <nl> . init = vp9_init , <nl> + . close = vp9_close_context , <nl> . parse_packet = vp9_handle_packet <nl> };
static int decode_wdlt ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> segments = bytestream2_get_le16 ( gb ); <nl> } <nl> line_ptr = frame ; <nl> + if ( frame_end - frame < width ) <nl> + return AVERROR_INVALIDDATA ; <nl> frame += width ; <nl> y ++; <nl> while ( segments --) {
static void vc1_mc_4mv_chroma4 ( VC1Context * v ) <nl> mquant = v -> altpq ; \ <nl> if (( edges & 8 ) && s -> mb_y == ( s -> mb_height - 1 )) \ <nl> mquant = v -> altpq ; \ <nl> + if (! mquant || mquant > 31 ) { \ <nl> + av_log ( v -> s . avctx , AV_LOG_ERROR , \ <nl> + " Overriding invalid mquant % d \ n ", mquant ); \ <nl> + mquant = 1 ; \ <nl> + } \ <nl> } <nl>  <nl> /**
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> break ; <nl> } <nl>  <nl> - if ( last ){ <nl> + if ( last && s -> current_picture_ptr ){ <nl> if ( r -> loop_filter ) <nl> r -> loop_filter ( r , s -> mb_height - 1 ); <nl> ff_er_frame_end ( s );
static int decode_frame ( AVCodecContext * avctx , <nl> } else if ( s -> bit_depth == 16 && <nl> s -> color_type == PNG_COLOR_TYPE_GRAY_ALPHA ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_YA16BE ; <nl> + } else if ( s -> bit_depth == 16 && <nl> + s -> color_type == PNG_COLOR_TYPE_RGB_ALPHA ) { <nl> + avctx -> pix_fmt = AV_PIX_FMT_RGBA64BE ; <nl> } else { <nl> avpriv_report_missing_feature ( avctx , <nl> " Bit depth % d color type % d ",
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
static int find_slice_quant ( AVCodecContext * avctx , const AVFrame * pic , <nl> if ( ctx -> alpha_bits ) <nl> bits += estimate_alpha_plane ( ctx , & error , src , linesize [ 3 ], <nl> mbs_per_slice , q , td -> blocks [ 3 ]); <nl> - if ( bits > 65000 * 8 ) { <nl> + if ( bits > 65000 * 8 ) <nl> error = SCORE_LIMIT ; <nl> - break ; <nl> - } <nl> + <nl> slice_bits [ q ] = bits ; <nl> slice_score [ q ] = error ; <nl> }
const CodecMime ff_id3v2_mime_tags [] = { <nl> {" image / jpg ", CODEC_ID_MJPEG }, <nl> {" image / png " , CODEC_ID_PNG }, <nl> {" image / tiff ", CODEC_ID_TIFF }, <nl> + {" image / bmp ", CODEC_ID_BMP }, <nl> {"", CODEC_ID_NONE }, <nl> }; <nl> 
static inline int op ( uint8_t ** dst , const uint8_t * dst_end , <nl> int striplen = FFMIN ( count , remaining ); <nl> if ( buf ) { <nl> striplen = FFMIN ( striplen , buf_end - * buf ); <nl> + if (* buf >= buf_end ) <nl> + goto exhausted ; <nl> memcpy (* dst , * buf , striplen ); <nl> * buf += striplen ; <nl> } else if ( pixel >= 0 )
int ff_adts_decode_extradata ( AVFormatContext * s , ADTSContext * adts , uint8_t * buf <nl> av_log ( s , AV_LOG_ERROR , " Scalable configurations are not allowed in ADTS \ n "); <nl> return - 1 ; <nl> } <nl> + if ( get_bits (& gb , 1 )) { <nl> + av_log ( s , AV_LOG_ERROR , " Extension flag is not allowed in ADTS \ n "); <nl> + return - 1 ; <nl> + } <nl> if (! adts -> channel_conf ) { <nl> init_put_bits (& pb , adts -> pce_data , MAX_PCE_SIZE ); <nl> 
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl>  <nl> log = av_log2 ( buf ); <nl>  <nl> - if ( log > 31 - 11 ){ <nl> + if ( log - k >= 32 - MIN_CACHE_BITS ){ <nl> buf >>= log - k ; <nl> buf += ( 30 - log )<< k ; <nl> LAST_SKIP_BITS ( re , gb , 32 + k - log );
static int xan_unpack ( uint8_t * dest , const int dest_len , <nl> if ( size + size2 > dest_end - dest ) <nl> break ; <nl> } <nl> - if ( src + size > src_end || dest + size + size2 > dest_end ) <nl> + if ( src + size > src_end || dest + size + size2 > dest_end || <nl> + dest - orig_dest + size < back ) <nl> return - 1 ; <nl> bytestream_get_buffer (& src , dest , size ); <nl> dest += size ;
static int a64_write_header ( AVFormatContext * s ) <nl> 0x00 , // charset_lifetime ( multi only ) <nl> 0x00 // fps in 50 / fps ; <nl> }; <nl> + <nl> + if ( avctx -> extradata_size < 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Missing extradata \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> switch ( avctx -> codec -> id ) { <nl> case AV_CODEC_ID_A64_MULTI : <nl> header [ 2 ] = 0x00 ;
static int decode_channel_transform ( WMAProDecodeCtx * s ) <nl> if ( get_bits1 (& s -> gb )) { <nl> avpriv_request_sample ( s -> avctx , <nl> " Unknown channel transform type "); <nl> + return AVERROR_PATCHWELCOME ; <nl> } <nl> } else { <nl> chgroup -> transform = 1 ;
static void unpack_alpha ( GetBitContext * gb , uint16_t * dst , int num_coeffs , <nl> dst [ idx ++] = alpha_val >> 6 ; <nl> else <nl> dst [ idx ++] = ( alpha_val << 2 ) | ( alpha_val >> 6 ); <nl> - if ( idx == num_coeffs - 1 ) <nl> + if ( idx >= num_coeffs - 1 ) <nl> break ; <nl> } while ( get_bits1 ( gb )); <nl> val = get_bits ( gb , 4 );
static int flush_packet ( AVFormatContext * ctx , int stream_index , <nl>  <nl> if ( stuffing_size < 0 ) <nl> stuffing_size = 0 ; <nl> + <nl> + if ( startcode == PRIVATE_STREAM_1 && id >= 0xa0 ) { <nl> + if ( payload_size < av_fifo_size ( stream -> fifo )) <nl> + stuffing_size += payload_size % stream -> lpcm_align ; <nl> + } <nl> + <nl> if ( stuffing_size > 16 ) { /*<= 16 for MPEG - 1 , <= 32 for MPEG - 2 */ <nl> pad_packet_bytes += stuffing_size ; <nl> packet_size -= stuffing_size ;
int ff_rtp_chain_mux_open ( AVFormatContext ** out , AVFormatContext * s , <nl> /* Get the payload type from the codec */ <nl> if ( st -> id < RTP_PT_PRIVATE ) <nl> rtpctx -> streams [ 0 ]-> id = <nl> - ff_rtp_get_payload_type ( rtpctx , st -> codec , idx ); <nl> + ff_rtp_get_payload_type ( s , st -> codec , idx ); <nl> else <nl> rtpctx -> streams [ 0 ]-> id = st -> id ; <nl> 
int ffurl_open ( URLContext ** puc , const char * filename , int flags , <nl> int ret = ffurl_alloc ( puc , filename , flags , int_cb , protocols ); <nl> if ( ret ) <nl> return ret ; <nl> + if ( options && <nl> + ( ret = av_opt_set_dict (* puc , options )) < 0 ) <nl> + goto fail ; <nl> if ( options && (* puc )-> prot -> priv_data_class && <nl> ( ret = av_opt_set_dict ((* puc )-> priv_data , options )) < 0 ) <nl> goto fail ;
static int probe_buf_write ( void * opaque , uint8_t * buf , int buf_size ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> int ret ; <nl> - uint8_t * buffer = av_malloc ( AVP_BUFFSIZE ); <nl> + uint8_t * buffer = av_mallocz ( AVP_BUFFSIZE ); <nl>  <nl> if (! buffer ) <nl> exit ( 1 );
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + case NAL_IDR_SLICE : <nl> + case NAL_SLICE : <nl> nals_needed = nal_index ; <nl> } <nl> continue ;
static int cyuv_decode_init ( AVCodecContext * avctx ) <nl>  <nl> s -> avctx = avctx ; <nl> s -> width = avctx -> width ; <nl> + /* width needs to be divisible by 4 for this codec to work */ <nl> + if ( s -> width & 0x3 ) <nl> + return - 1 ; <nl> s -> height = avctx -> height ; <nl> avctx -> pix_fmt = PIX_FMT_YUV411P ; <nl> avctx -> has_b_frames = 0 ;
static void truemotion1_decode_24bit ( TrueMotion1Context * s ) <nl> int index ; <nl>  <nl> /* clean out the line buffer */ <nl> - memset ( s -> vert_pred , 0 , s -> avctx -> width * sizeof ( unsigned short )); <nl> + memset ( s -> vert_pred , 0 , s -> avctx -> width * sizeof ( unsigned int )); <nl>  <nl> GET_NEXT_INDEX (); <nl> 
static int mov_read_stsd ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> sc -> stsd_count = entries ; <nl> - sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof ( sc -> extradata_size )); <nl> + sc -> extradata_size = av_mallocz_array ( sc -> stsd_count , sizeof (* sc -> extradata_size )); <nl> if (! sc -> extradata_size ) <nl> return AVERROR ( ENOMEM ); <nl> 
int attribute_align_arg avcodec_encode_audio2 ( AVCodecContext * avctx , <nl> avctx -> time_base ); <nl> } <nl> avpkt -> dts = avpkt -> pts ; <nl> + } else { <nl> + avpkt -> size = 0 ; <nl> } <nl> } else { <nl> /* for compatibility with encoders not supporting encode2 (), we need to
static int sol_read_packet ( AVFormatContext * s , <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR ( EIO ); <nl> ret = av_get_packet ( s -> pb , pkt , MAX_SIZE ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> pkt -> stream_index = 0 ; <nl>  <nl> /* note : we need to modify the packet size here to handle the last
static int decode_vol_header ( MpegEncContext * s , GetBitContext * gb ){ <nl>  <nl> s -> progressive_sequence = <nl> s -> progressive_frame = get_bits1 ( gb )^ 1 ; <nl> + s -> interlaced_dct = 0 ; <nl> if (! get_bits1 ( gb ) && ( s -> avctx -> debug & FF_DEBUG_PICT_INFO )) <nl> av_log ( s -> avctx , AV_LOG_INFO , " MPEG4 OBMC not supported ( very likely buggy encoder )\ n "); /* OBMC Disable */ <nl> if ( vo_ver_id == 1 ) {
void estimate_motion ( MpegEncContext * s , <nl> if ( varc * 2 + 200 > vard ){ <nl> mb_type |= MB_TYPE_INTER ; <nl> halfpel_motion_search ( s , & mx , & my , dmin , xmin , ymin , xmax , ymax , pred_x , pred_y ); <nl> + } else { <nl> + mx = mx * 2 - mb_x * 32 ; <nl> + my = my * 2 - mb_y * 32 ; <nl> } <nl> } else { <nl> if ( vard <= 64 || vard < varc ) {
static int ffm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> FFMContext * ffm = s -> priv_data ; <nl> int duration ; <nl>  <nl> + if ( url_fsize ( s -> pb ) == FFM_PACKET_SIZE ) <nl> + return - 1 ; <nl> + <nl> switch ( ffm -> read_state ) { <nl> case READ_HEADER : <nl> if (! ffm_is_avail_data ( s , FRAME_HEADER_SIZE + 4 )) {
static int mpegts_read_header ( AVFormatContext * s , <nl> /* normal demux */ <nl>  <nl> /* first do a scaning to get all the services */ <nl> - if ( avio_seek ( pb , pos , SEEK_SET ) < 0 ) <nl> + if ( pb -> seekable && avio_seek ( pb , pos , SEEK_SET ) < 0 ) <nl> av_log ( s , AV_LOG_ERROR , " Unable to seek back to the start \ n "); <nl>  <nl> mpegts_open_section_filter ( ts , SDT_PID , sdt_cb , ts , 1 );
static int vaapi_encode_h264_write_extra_header ( AVCodecContext * avctx , <nl>  <nl> if ( priv -> sei_needed ) { <nl> if ( priv -> aud_needed ) { <nl> - vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + err = vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + if ( err < 0 ) <nl> + goto fail ; <nl> priv -> aud_needed = 0 ; <nl> } <nl> 
static int ac3_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> } <nl>  <nl> /* check for crc mismatch */ <nl> - if ( avctx -> error_resilience > 0 ) { <nl> + if ( avctx -> error_resilience >= FF_ER_CAREFUL ) { <nl> if ( av_crc ( av_crc_get_table ( AV_CRC_16_ANSI ), 0 , & buf [ 2 ], s -> frame_size - 2 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame CRC mismatch \ n "); <nl> return - 1 ;
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> /* skip B - frames if we don ' t have reference frames */ <nl> if ( s -> last_picture_ptr == NULL && ( s -> pict_type == AV_PICTURE_TYPE_B || s -> droppable )) { <nl> - goto err ; <nl> + goto end ; <nl> } <nl> if (( avctx -> skip_frame >= AVDISCARD_NONREF && s -> pict_type == AV_PICTURE_TYPE_B ) || <nl> ( avctx -> skip_frame >= AVDISCARD_NONKEY && s -> pict_type != AV_PICTURE_TYPE_I ) ||
static int oma_read_header ( AVFormatContext * s , <nl>  <nl> ff_id3v2_read ( s , ID3v2_EA3_MAGIC ); <nl> ret = avio_read ( s -> pb , buf , EA3_HEADER_SIZE ); <nl> + if ( ret < EA3_HEADER_SIZE ) <nl> + return - 1 ; <nl>  <nl> if ( memcmp ( buf , (( const uint8_t []){' E ', ' A ', ' 3 '}), 3 ) || buf [ 4 ] != 0 || buf [ 5 ] != EA3_HEADER_SIZE ) { <nl> av_log ( s , AV_LOG_ERROR , " Couldn ' t find the EA3 header !\ n ");
static int codebook_sanity_check_for_rate_quarter ( const uint8_t * cbgain ) <nl> * @ param gain array holding the 4 pitch subframe gain values <nl> * @ param cdn_vector array for the generated scaled codebook vector <nl> */ <nl> - static void compute_svector ( const QCELPContext * q , const float * gain , <nl> + static void compute_svector ( QCELPContext * q , const float * gain , <nl> float * cdn_vector ) <nl> { <nl> int i , j , k ;
static int mxf_get_sorted_table_segments ( MXFContext * mxf , int * nb_sorted_segment <nl> if ( mxf -> metadata_sets [ i ]-> type == IndexTableSegment ) <nl> nb_segments ++; <nl>  <nl> + if (! nb_segments ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> * sorted_segments = av_mallocz ( nb_segments * sizeof (** sorted_segments )); <nl> unsorted_segments = av_mallocz ( nb_segments * sizeof (* unsorted_segments )); <nl> if (! sorted_segments || ! unsorted_segments ) {
static int cbs_h2645_write_nal_unit ( CodedBitstreamContext * ctx , <nl> // Overflow but we didn ' t notice . <nl> av_assert0 ( put_bits_count (& pbc ) <= 8 * priv -> write_buffer_size ); <nl>  <nl> + if ( err < 0 ) { <nl> + // Write failed for some other reason . <nl> + return err ; <nl> + } <nl> + <nl> if ( put_bits_count (& pbc ) % 8 ) <nl> unit -> data_bit_padding = 8 - put_bits_count (& pbc ) % 8 ; <nl> else
void avcodec_pix_fmt_string ( char * buf , int buf_size , enum PixelFormat pix_fmt ) <nl> char is_alpha_char = info . is_alpha ? ' y ' : ' n '; <nl>  <nl> snprintf ( buf , buf_size , <nl> - "%- 11s " " % 1d " " % 2d " " % c ", <nl> + "%- 11s % 5d % 9d % 6c ", <nl> info . name , <nl> info . nb_channels , <nl> info . depth ,
static int64_t http_seek ( URLContext * h , int64_t off , int whence ) <nl>  <nl> if ( whence == AVSEEK_SIZE ) <nl> return s -> filesize ; <nl> + else if (( whence == SEEK_CUR && off == 0 ) || <nl> + ( whence == SEEK_SET && off == s -> off )) <nl> + return s -> off ; <nl> else if (( s -> filesize == - 1 && whence == SEEK_END ) || h -> is_streamed ) <nl> return AVERROR ( ENOSYS ); <nl> 
static int avi_read_header ( AVFormatContext * s ) <nl> ast = s -> streams [ 0 ]-> priv_data ; <nl> av_freep (& s -> streams [ 0 ]-> codec -> extradata ); <nl> av_freep (& s -> streams [ 0 ]-> codec ); <nl> + av_freep (& s -> streams [ 0 ]-> info ); <nl> av_freep (& s -> streams [ 0 ]); <nl> s -> nb_streams = 0 ; <nl> if ( CONFIG_DV_DEMUXER ) {
int av_metadata_set ( AVMetadata ** pm , const char * key , const char * value ) <nl> m -> elems [ m -> count ]. value = av_strdup ( value ); <nl> m -> count ++; <nl> } <nl> - if (! m -> count ) <nl> + if (! m -> count ) { <nl> + av_free ( m -> elems ); <nl> av_freep ( pm ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int append_entry ( HLSContext * hls , uint64_t duration ) <nl> if (! en ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - av_get_frame_filename ( en -> name , sizeof ( en -> name ), hls -> basename , <nl> + av_get_frame_filename ( en -> name , sizeof ( en -> name ), <nl> + av_basename ( hls -> basename ), <nl> hls -> number - 1 ); <nl>  <nl> en -> duration = duration ;
static void opt_output_file ( const char * filename ) <nl> fprintf ( stderr , " Not overwriting - exiting \ n "); <nl> av_exit ( 1 ); <nl> } <nl> + while ( c != '\ n ' && c != EOF ) <nl> + c = getchar (); <nl> } <nl> else { <nl> fprintf ( stderr ," File '% s ' already exists . Exiting .\ n ", filename );
static int handle_connection ( HTTPContext * c ) <nl> } <nl> if ( http_send_data ( c ) < 0 ) <nl> return - 1 ; <nl> + /* close connection if trailer sent */ <nl> + if ( c -> state == HTTPSTATE_SEND_DATA_TRAILER ) <nl> + return - 1 ; <nl> break ; <nl> case HTTPSTATE_RECEIVE_DATA : <nl> /* no need to read if no events */
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & bc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
const char * zmq_strerror ( int errnum_ ) <nl> return " Address in use "; <nl> case EADDRNOTAVAIL : <nl> return " Address not available "; <nl> + case ECONNREFUSED : <nl> + return " Connection refused "; <nl> + case EINPROGRESS : <nl> + return " Operation in progress "; <nl> # endif <nl> case EMTHREAD : <nl> return " Number of preallocated application threads exceeded ";
cmd_connect ( gchar ** args , struct cmd_help_t help ) <nl> if ( stream ){ <nl> // Limit to READ_BUF_SIZE bytes to prevent overflows in the case of a poorly chosen command <nl> account -> password = g_malloc ( READ_BUF_SIZE ); <nl> + if (! account -> password ){ <nl> + log_error (" Failed to allocate enough memory to read eval_password output "); <nl> + return TRUE ; <nl> + } <nl> account -> password = fgets ( account -> password , READ_BUF_SIZE , stream ); <nl> pclose ( stream ); <nl> } else {
int read_mainconfig ( int reload ) <nl> if ( mainconfig . reject_delay > mainconfig . cleanup_delay ) { <nl> mainconfig . reject_delay = mainconfig . cleanup_delay ; <nl> } <nl> + if ( mainconfig . reject_delay < 0 ) mainconfig . reject_delay = 0 ; <nl>  <nl> /* <nl> * Initialize the old " bind_address " and " port ", first .
static struct cmp * cmp ; <nl> /* <nl> * Compare 2 attributes . May call the attribute compare function . <nl> */ <nl> - int paircompare ( VALUE_PAIR * request , VALUE_PAIR * check , <nl> + static int paircompare ( VALUE_PAIR * request , VALUE_PAIR * check , <nl> VALUE_PAIR * check_pairs , VALUE_PAIR ** reply_pairs ) <nl> { <nl> int ret = - 2 ;
static int common_socket_parse ( CONF_SECTION * cs , rad_listen_t * this ) <nl> /* <nl> * Try IPv4 first <nl> */ <nl> + memset (& ipaddr , 0 , sizeof ( ipaddr )); <nl> ipaddr . ipaddr . ip4addr . s_addr = htonl ( INADDR_NONE ); <nl> rcode = cf_item_parse ( cs , " ipaddr ", PW_TYPE_IPADDR , <nl> & ipaddr . ipaddr . ip4addr , NULL );
struct groupchat * jabber_chat_join ( struct im_connection * ic , char * room , char * <nl> node = xt_new_node ( " x ", NULL , NULL ); <nl> xt_add_attr ( node , " xmlns ", XMLNS_MUC ); <nl> node = jabber_make_packet ( " presence ", NULL , roomjid , node ); <nl> + if ( password ) <nl> + xt_add_child ( node , xt_new_node ( " password ", password , NULL ) ); <nl> jabber_cache_add ( ic , node , jabber_chat_join_failed ); <nl>  <nl> if ( ! jabber_write_packet ( ic , node ) )
static void msn_close ( struct gaim_connection * gc ) <nl> g_slist_free ( md -> msgq ); <nl> } <nl>  <nl> + g_free ( md -> grouplist ); <nl> + <nl> g_free ( md ); <nl> } <nl> 
file_transfer_t * imcb_file_send_start ( struct im_connection * ic , char * handle , ch <nl> bee_t * bee = ic -> bee ; <nl> bee_user_t * bu = bee_user_by_handle ( bee , ic , handle ); <nl>  <nl> - if ( bee -> ui -> ft_in_start ) { <nl> + if ( bee -> ui -> ft_in_start && bu ) { <nl> return bee -> ui -> ft_in_start ( bee , bu , file_name , file_size ); <nl> } else { <nl> return NULL ;
int jabber_si_handle_request ( struct im_connection * ic , struct xt_node * node , st <nl> requestok = FALSE ; <nl> } <nl>  <nl> - * s = '/'; <nl> + if ( s ) <nl> + * s = '/'; <nl> } <nl> - else <nl> + <nl> + if ( ! requestok ) <nl> { <nl> reply = jabber_make_error_packet ( node , " item - not - found ", " cancel ", NULL ); <nl> if (! jabber_write_packet ( ic , reply ))
dissect_smpp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint32 offset = 0 ; <nl> while ( tvb_reported_length_remaining ( tvb , offset ) > 0 ) { <nl> guint16 pdu_len = tvb_get_ntohl ( tvb , offset ); <nl> + if ( pdu_len < 1 ) <nl> + THROW ( ReportedBoundsError ); <nl> gint pdu_real_len = tvb_length_remaining ( tvb , offset ); <nl> tvbuff_t * pdu_tvb ; <nl> 
proto_reg_handoff_e100 ( void ) <nl> /* Check all UDP traffic , as the specific UDP port is configurable */ <nl> heur_dissector_add (" udp ", dissect_e100 , " E100 over UDP ", " e100_udp ", proto_e100 , HEURISTIC_ENABLE ); <nl> /* e100 traffic encapsulates traffic from the ethernet frame on */ <nl> - eth_handle = find_dissector (" eth "); <nl> + eth_handle = find_dissector (" eth_withoutfcs "); <nl> } <nl>  <nl> /*
capture_start ( capture_options * capture_opts , capture_session * cap_session , void ( <nl> GString * source ; <nl>  <nl> cap_session -> state = CAPTURE_PREPARING ; <nl> + cap_session -> count = 0 ; <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_MESSAGE , " Capture Start ..."); <nl> source = get_iface_list_string ( capture_opts , IFLIST_SHOW_FILTER ); <nl> cf_set_tempfile_source (( capture_file *) cap_session -> cf , source -> str );
capture_all_filter_check_syntax_cb ( GtkWidget * w _U_ , gpointer user_data _U_ ) <nl> } <nl> # ifdef HAVE_EXTCAP <nl> /* Can ' t verify extcap capture filters */ <nl> - if ( device . if_info . extcap != NULL ) <nl> + if ( device . if_info . extcap != NULL && strlen ( device . if_info . extcap ) > 0 ) <nl> continue ; <nl> # endif <nl> filter_text = gtk_combo_box_text_get_active_text ( GTK_COMBO_BOX_TEXT ( filter_cm ));
static void dissect_HEARTBEAT_VIRTUAL ( tvbuff_t * tvb , packet_info * pinfo _U_ , gin <nl> if (!( flags & FLAG_VIRTUAL_HEARTBEAT_N )) { <nl> proto_tree_add_item ( sil_tree_writer , hf_rtps_virtual_heartbeat_num_virtual_guids , tvb , <nl> offset , 4 , little_endian ? ENC_LITTLE_ENDIAN : ENC_BIG_ENDIAN ); <nl> + num_virtual_guids = NEXT_guint32 ( tvb , offset , little_endian ); <nl> offset += 4 ; <nl> } else { <nl> num_virtual_guids = 0 ;
WSLUA_METAMETHOD Dir__call ( lua_State * L ) { <nl> const gchar * filename ; <nl> const char * ext ; <nl>  <nl> - if (! dir ) <nl> + if (! dir ) { <nl> luaL_argerror ( L , 1 ," must be a Dir "); <nl> + return 0 ; <nl> + } <nl>  <nl> if (! dir -> dir ) { <nl> return 0 ;
csv_handle ( GtkTreeModel * model , GtkTreePath * path _U_ , GtkTreeIter * iter , <nl> i == PERCENT_COLUMN || i == PROTECTION_COLUMN ) { <nl> gtk_tree_model_get ( model , iter , i , & table_text , - 1 ); <nl> g_string_append ( CSV_str , table_text ); <nl> + g_free ( table_text ); <nl> } else { <nl> gtk_tree_model_get ( model , iter , i , & table_value , - 1 ); <nl> g_string_append_printf ( CSV_str , "% u ", table_value );
WSLUA_METAMETHOD Columns__newindex ( lua_State * L ) { <nl>  <nl> for ( cn = colnames ; cn -> name ; cn ++) { <nl> if ( g_str_equal ( cn -> name , colname ) ) { <nl> - col_set_str ( cols -> cinfo , cn -> id , text ); <nl> + col_add_str ( cols -> cinfo , cn -> id , text ); <nl> return 0 ; <nl> } <nl> }
dissect_gryphon_message ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> /* <nl> * Indicate what kind of message this is . <nl> */ <nl> - col_set_str ( pinfo -> cinfo , COL_INFO , val_to_str ( frmtyp , frame_type , "- Invalid -")); <nl> + col_set_str ( pinfo -> cinfo , COL_INFO , val_to_str_const ( frmtyp , frame_type , "- Invalid -")); <nl> } <nl>  <nl> if ( tree == NULL )
dissect_data_chunk ( tvbuff_t * chunk_tvb , <nl> */ <nl> if ( b_bit ) <nl> { <nl> - gboolean retval ; <nl> + gboolean retval = FALSE ; <nl>  <nl> /* <nl> * If this particular fragment happens to get a ReportedBoundsError
UAT_CSTRING_CB_DEF ( uat_ue_keys_records , rrcIntegrityKeyString , uat_ue_keys_reco <nl>  <nl> static gboolean global_pdcp_decipher_signalling = FALSE ; <nl> static gboolean global_pdcp_decipher_userplane = FALSE ; <nl> - static gboolean global_pdcp_check_integrity = FALSE ; <nl> # endif <nl> + static gboolean global_pdcp_check_integrity = FALSE ; <nl>  <nl> static const value_string direction_vals [] = <nl> {
PacketList :: PacketList ( QWidget * parent ) : <nl> decode_as_ ( NULL ), <nl> ctx_column_ (- 1 ), <nl> capture_in_progress_ ( false ), <nl> - tail_timer_id_ ( 0 ) <nl> + tail_timer_id_ ( 0 ), <nl> + rows_inserted_ ( false ) <nl> { <nl> QMenu * submenu , * subsubmenu ; <nl> QAction * action ;
void <nl> frame_data_reset ( frame_data * fdata ) <nl> { <nl> fdata -> flags . visited = 0 ; <nl> + fdata -> subnum = 0 ; <nl>  <nl> if ( fdata -> pfd ) { <nl> g_slist_free ( fdata -> pfd );
add_content_type ( proto_tree * tree , tvbuff_t * tvb , guint32 val_start , <nl> So we have to disable that one and become " slow " by pretending that <nl> the tree is " visible ". <nl> */ <nl> - PTREE_DATA ( tree )-> visible = 1 ; <nl> + if ( tree ) <nl> + PTREE_DATA ( tree )-> visible = 1 ; <nl>  <nl> * textual_content = NULL ; <nl> * well_known_content = 0 ;
tvb_captured_length ( const tvbuff_t * tvb ) <nl> static inline gint <nl> _tvb_captured_length_remaining ( const tvbuff_t * tvb , const gint offset ) <nl> { <nl> - guint abs_offset , rem_length ; <nl> + guint abs_offset = 0 , rem_length ; <nl> int exception ; <nl>  <nl> exception = compute_offset_and_remaining ( tvb , offset , & abs_offset , & rem_length );
dissect_gtpv2_mm_context_utms_qq ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tr <nl> proto_tree_add_item ( tree , hf_gtpv2_ik , tvb , offset , 16 , ENC_NA ); <nl> offset += 16 ; <nl>  <nl> - if ( nr_qua ) <nl> - { <nl> - offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qui ); <nl> + if ( nr_qua ) { <nl> + offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qua ); <nl> } <nl>  <nl> if ( nr_qui ) {
save_decode_as_entries ( gchar ** err ) <nl>  <nl> dissector_all_tables_foreach_changed ( decode_as_write_entry , da_file ); <nl> fclose ( da_file ); <nl> + g_free ( daf_path ); <nl> return 0 ; <nl> } <nl> 
static void init_wepkeys ( void ) { <nl>  <nl> # ifdef USE_ENV <nl> buf = ep_alloc ( 128 ); <nl> - sprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> + g_snprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> tmp = getenv ( buf ); <nl> # else <nl> tmp = wep_keystr [ i ];
static int FieldInfo_get_range ( lua_State * L ) { <nl> r -> tvb = ep_new ( struct _wslua_tvb ); <nl>  <nl> r -> tvb -> ws_tvb = fi -> ds_tvb ; <nl> + r -> tvb -> expired = FALSE ; <nl> + r -> tvb -> need_free = FALSE ; <nl> r -> offset = fi -> start ; <nl> r -> len = fi -> length ; <nl> 
clear_node_pr ( stat_node * n ) <nl> clear_node_pr ( c ); <nl> } <nl>  <nl> - if ( n -> pr -> iter ) { <nl> + if ( n -> pr && n -> pr -> iter ) { <nl> gtk_tree_store_remove ( n -> st -> pr -> store , n -> pr -> iter ); <nl> n -> pr -> iter = NULL ; <nl> }
static void parse_outhdr_string ( const guchar * outhdr_string , gint outhdr_string_ <nl> guint d ; <nl>  <nl> /* Find digits */ <nl> - for ( ; n < outhdr_string_len ; n ++) { <nl> + for ( ; ( n < outhdr_string_len ) && ( number_digits < MAX_OUTHDR_VALUES ); n ++) { <nl> if (! g_ascii_isdigit ( outhdr_string [ n ])) { <nl> break ; <nl> }
dissect_sip_contact_item ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , gi <nl> gint current_offset ; <nl> gint queried_offset ; <nl> gint contact_params_start_offset = - 1 ; <nl> - gint contact_param_end_offset = - 1 ; <nl> + /* gint contact_param_end_offset = - 1 ;*/ <nl> uri_offset_info uri_offsets ; <nl>  <nl> /* skip Spaces and Tabs */
dissect_dcm_pdv_fragmented ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> pdv_body_len , <nl> !( pdv -> is_last_fragment )); <nl>  <nl> - if (( head && ( head -> next == NULL )) || pdv -> is_last_fragment ) { <nl> + if ( head && ( head -> next == NULL )) { <nl> /* Was not really fragmented , therefore use ' conventional ' decoding <nl> fragment_add_seq_next () won ' t add any items to the list , when last fragment only <nl> */
ssl_association_remove ( GTree * associations , SslAssociation * assoc ) <nl> if ( assoc -> handle ) <nl> dissector_delete (( assoc -> tcp )?" tcp . port ":" udp . port ", assoc -> ssl_port , assoc -> handle ); <nl>  <nl> + g_free ( assoc -> info ); <nl> + <nl> g_tree_remove ( associations , assoc ); <nl> g_free ( assoc ); <nl> }
fill_list ( GtkWidget * main_w ) <nl> * and use it later without any crashes . This may not be a <nl> * valid assumption . <nl> */ <nl> + g_free ( l_select ); <nl> l_select = ( GtkTreeIter *) g_memdup (& iter , sizeof ( iter )); <nl> } <nl> fl_entry = g_list_next ( fl_entry );
Dot11DecryptDecryptWPABroadcastKey ( const EAPOL_RSN_KEY * pEAPKey , guint8 * decrypt <nl> DEBUG_DUMP (" FullDecrKey :", new_key , 32 ); <nl>  <nl> if ( gcry_cipher_open (& rc4_handle , GCRY_CIPHER_ARCFOUR , GCRY_CIPHER_MODE_STREAM , 0 )) { <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> if ( gcry_cipher_setkey ( rc4_handle , new_key , sizeof ( new_key ))) { <nl> gcry_cipher_close ( rc4_handle ); <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> 
void OverlayScrollBar :: paintEvent ( QPaintEvent * event ) <nl> pm_painter . setPen ( border_color ); <nl> pm_painter . drawLine ( near_dest . topLeft (), near_dest . bottomLeft ()); <nl> pm_painter . drawLine ( near_dest . topRight (), near_dest . bottomRight ()); <nl> + pm_painter . drawLine ( near_dest . bottomLeft (), near_dest . bottomRight ()); <nl> pm_painter . restore (); <nl>  <nl> // Draw the map .
typedef struct _ext_toolbar_update_list_t <nl> GList * entries ; <nl> } ext_toolbar_update_list_t ; <nl>  <nl> - extern gint <nl> + static gint <nl> ext_toolbar_find_item ( gconstpointer a , gconstpointer b ) <nl> { <nl> if ( a == 0 || b == 0 )
static int decode_bgp_link_nlri_prefix_descriptors ( tvbuff_t * tvb , <nl> break ; <nl>  <nl> case BGP_NLRI_TLV_IP_REACHABILITY_INFORMATION : <nl> - decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> - tvb , offset + 4 , 0 , " Reachability "); <nl> + if ( decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> + tvb , offset + 4 , 0 , " Reachability ") == - 1 ) <nl> + return diss_length ; <nl> break ; <nl> } <nl> 
peektagged_read_packet ( wtap * wth , FILE_T fh , struct wtap_pkthdr * phdr , <nl>  <nl> case TAG_PEEKTAGGED_CENTER_FREQUENCY : <nl> /* XXX - also seen in an EtherPeek capture ; value unknown */ <nl> + ieee_802_11 . presence_flags |= PHDR_802_11_HAS_FREQUENCY ; <nl> + ieee_802_11 . frequency = pletoh32 (& tag_value [ 2 ]); <nl> break ; <nl>  <nl> case TAG_PEEKTAGGED_UNKNOWN_0x000E :
do_simple_message_box ( ESD_TYPE_E type , gboolean * notagain , <nl> if ( notagain != NULL ) { <nl> checkbox = gtk_check_button_new_with_label (" Don ' t show this message again ."); <nl> gtk_container_set_border_width ( GTK_CONTAINER ( checkbox ), 12 ); <nl> - gtk_box_pack_start ( GTK_BOX ( gtk_message_dialog_get_message_area ( GTK_MESSAGE_DIALOG ( msg_dialog ))), checkbox , <nl> - TRUE , TRUE , 0 ); <nl> + gtk_box_pack_start ( GTK_BOX ( gtk_dialog_get_content_area ( GTK_DIALOG ( msg_dialog ))), <nl> + checkbox , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( checkbox ); <nl> } <nl> 
static nghttp2_hd_entry * add_hd_table_incremental ( nghttp2_hd_context * context , <nl>  <nl> if ( rv != 0 ) { <nl> -- new_ent -> ref ; <nl> + <nl> + /* nv -> name and nv -> value are managed by caller . */ <nl> + new_ent -> nv . name = NULL ; <nl> + new_ent -> nv . namelen = 0 ; <nl> + new_ent -> nv . value = NULL ; <nl> + new_ent -> nv . valuelen = 0 ; <nl> + <nl> nghttp2_hd_entry_free ( new_ent ); <nl> free ( new_ent ); <nl> 
const int custom_occurrence_col_ = 4 ; <nl> ColumnPreferencesFrame :: ColumnPreferencesFrame ( QWidget * parent ) : <nl> QFrame ( parent ), <nl> ui ( new Ui :: ColumnPreferencesFrame ), <nl> + cur_column_ ( 0 ), <nl> cur_line_edit_ ( NULL ), <nl> cur_combo_box_ ( NULL ), <nl> + saved_combo_idx_ ( 0 ), <nl> saved_custom_combo_idx_ (- 1 ) <nl> { <nl> ui -> setupUi ( this );
extcap_register_preferences_callback ( gpointer key , gpointer value _U_ , gpointer <nl>  <nl> void extcap_register_preferences ( void ) <nl> { <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> module_t * dev_module = prefs_find_module (" extcap "); <nl>  <nl> if (! dev_module ) <nl> extcap_load_interface_list ( void ) <nl> gchar * argv ; <nl> gchar * error ; <nl>  <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> if ( _toolbars ) <nl> { <nl> // Remove existing interface toolbars here instead of in extcap_clear_interfaces ()
decode_pdu_sns_delete ( build_info_t * bi ) { <nl> { NSIP_IE_IP4_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> { NSIP_IE_IP6_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> }; <nl> - decode_iei_transaction_id ( ies , bi , bi -> offset ); <nl> - decode_pdu_general (& ies [ 1 ], 3 , bi ); <nl> + decode_pdu_general ( ies , 1 , bi ); <nl> + decode_iei_transaction_id (& ies [ 1 ], bi , bi -> offset ); <nl> + decode_pdu_general (& ies [ 2 ], 3 , bi ); <nl> } <nl>  <nl> static void
ws_pipe_wait_for_pipe ( HANDLE * pipe_handles , int num_pipe_handles , HANDLE pid ) <nl> int num_waiting_to_connect = 0 ; <nl> int num_handles = num_pipe_handles + 1 ; // PID handle is also added to list of handles . <nl>  <nl> + SecureZeroMemory ( pipeinsts , sizeof ( pipeinsts )); <nl> + <nl> if ( num_pipe_handles == 0 || num_pipe_handles > 3 ) <nl> { <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_DEBUG , " Invalid number of pipes given as argument .");
dissect_qnet6 ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * dat <nl> /* <nl> * data after header <nl> */ <nl> - if ( cklen != 0 ) <nl> + if ( cklen > 0 ) <nl> { <nl> crc = crc32_mpeg2_seed ( tvb_get_ptr ( tvb , 36 + 2 , cklen ), cklen , ~ crc ); <nl> crc = ~ crc ;
void PacketList :: columnsChanged () <nl> setColumnVisibility (); <nl> create_far_overlay_ = true ; <nl> packet_list_model_ -> resetColumns (); <nl> + applyRecentColumnWidths (); <nl> columns_changed_ = false ; <nl> } <nl>  <nl> void PacketList :: setCaptureFile ( capture_file * cf ) <nl> cap_file_ = cf ; <nl> if ( cap_file_ && columns_changed_ ) { <nl> columnsChanged (); <nl> - applyRecentColumnWidths (); <nl> } <nl> packet_list_model_ -> setCaptureFile ( cf ); <nl> create_near_overlay_ = true ;
private Q_SLOTS : <nl> }; <nl>  <nl> Q_DECLARE_METATYPE ( ExtcapArgument ) <nl> + Q_DECLARE_METATYPE ( ExtcapArgument *) <nl>  <nl> class ExtArgText : public ExtcapArgument <nl> {
write_recent ( void ) <nl> g_free ( rf_path ); <nl> return FALSE ; <nl> } <nl> + g_free ( rf_path ); <nl>  <nl> fputs ("# Recent settings file for Wireshark " VERSION ".\ n " <nl> "#\ n "
dissect_rtp_hext_rfc5215_onebyte ( tvbuff_t * tvb , packet_info * pinfo , <nl> return ; <nl>  <nl> ext_length = ( ext_hdr_hdr & 0x0F ) + 1 ; <nl> + <nl> + /* Exit on malformed extension headers */ <nl> + if ( ext_offset + ext_length + 1 > tvb_captured_length ( tvb )) { <nl> + return ; <nl> + } <nl> + <nl> if ( rtp_hext_tree ) { <nl> rtp_hext_rfc5285_tree = proto_tree_add_subtree ( rtp_hext_tree , tvb , ext_offset , ext_length + 1 , <nl> ett_hdr_ext_rfc5285 , NULL , " RFC 5285 Header Extension ( One - Byte Header )");
dissect_dcerpc_cn_bs_body ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> * it was just too short to tell and ask the TCP layer for more <nl> * data . */ <nl> pinfo -> desegment_offset = offset ; <nl> - pinfo -> desegment_len = sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset ); <nl> + pinfo -> desegment_len = ( guint32 )( sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset )); <nl> } else { <nl> /* Really not DCE - RPC */ <nl> break ;
void MulticastStatisticsDialog :: updateMulticastParameters () <nl>  <nl> param = buffer_alarm_threshold_le_ -> text (). toInt (& ok ); <nl> if ( ok && param > 0 ) { <nl> - mcast_stream_trigger = param ; <nl> + mcast_stream_bufferalarm = param ; <nl> } <nl>  <nl> param = stream_empty_speed_le_ -> text (). toInt (& ok );
dissect_attribute_value_pairs ( proto_tree * tree , packet_info * pinfo , tvbuff_t * tv <nl>  <nl> avp_vsa_len -= avp_vsa_header_len ; <nl>  <nl> + memset (& vendor_type , 0 , sizeof ( vendor_type )); <nl> if ( avp_is_extended ) { <nl> vendor_type . u8_code [ 0 ] = avp_type . u8_code [ 0 ]; <nl> vendor_type . u8_code [ 1 ] = avp_vsa_type ;
pcapng_read_section_header_block ( FILE_T fh , pcapng_block_header_t * bh , <nl> bytes_read = pcapng_read_option ( fh , pn , & oh , option_content , opt_cont_buf_len , to_read , err , err_info , " section_header "); <nl> if ( bytes_read <= 0 ) { <nl> pcapng_debug (" pcapng_read_section_header_block : failed to read option "); <nl> + g_free ( option_content ); <nl> return PCAPNG_BLOCK_ERROR ; <nl> } <nl> to_read -= bytes_read ;
bool InterfaceTreeWidgetItem :: operator < ( const QTreeWidgetItem & other ) const { <nl> # include < QComboBox > <nl>  <nl> InterfaceTreeDelegate :: InterfaceTreeDelegate ( QObject * parent ) <nl> - : QStyledItemDelegate ( parent ) <nl> + : QStyledItemDelegate ( parent ), tree_ ( NULL ) <nl> { <nl> } <nl> 
int ieee80211_radiotap_iterator_init ( <nl>  <nl> /* find payload start allowing for extended bitmap ( s ) */ <nl> if ( iterator -> _bitmap_shifter & ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> + if (! ITERATOR_VALID ( iterator , sizeof ( guint32 ))) <nl> + return - EINVAL ; <nl> while ( get_unaligned_le32 ( iterator -> _arg ) & <nl> ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> iterator -> _arg += sizeof ( guint32 );
ZEND_API int zend_register_constant ( zend_constant * c ELS_DC ) <nl>  <nl> zend_str_tolower ( lowercase_name , c -> name_len ); <nl> if ( zend_hash_add ( EG ( zend_constants ), lowercase_name , c -> name_len , ( void *) c , sizeof ( zend_constant ), NULL )== FAILURE ) { <nl> + free ( c -> name ); <nl> zval_dtor (& c -> value ); <nl> zend_error ( E_NOTICE ," Constant % s already defined ", lowercase_name ); <nl> ret = FAILURE ;
ZEND_API int zend_hash_update_current_key_ex ( HashTable * ht , int key_type , const <nl> p -> h = num_index ; <nl> } else { <nl> p -> h = h ; <nl> + p -> nKeyLength = str_length ; <nl> if ( IS_INTERNED ( str_index )) { <nl> p -> arKey = str_index ; <nl> } else {
PHPAPI void php_fgetcsv ( php_stream * stream , /* {{{ */ <nl> /* Types converted , free storage */ <nl> efree ( delim ); <nl> efree ( enc ); <nl> - efree ( buffer ); <nl> } else { <nl> /* Binary stream with binary delimiter / enclosures / prefetch */ <nl> php_fgetcsv_ex ( stream , delim , delim_len , enc , enc_len , "\\", 1 , buffer , buffer_len , return_value TSRMLS_CC );
static php_iconv_err_t _php_iconv_strpos ( unsigned int * pretval , <nl> ndl_buf_left -= GENERIC_SUPERSET_NBYTES ; <nl> if ( ndl_buf_left == 0 ) { <nl> * pretval = match_ofs ; <nl> + ndl_buf_p = ndl_buf ; <nl> + ndl_buf_left = ndl_buf_len ; <nl> + match_ofs = - 1 ; <nl> } <nl> } else { <nl> unsigned int i , j , lim ;
static php_stream * php_stream_url_wrap_rfc2397 ( php_stream_wrapper * wrapper , cha <nl> ts -> mode = mode && mode [ 0 ] == ' r ' ? TEMP_STREAM_READONLY : 0 ; <nl> ts -> meta = meta ; <nl> } <nl> + efree ( comma ); <nl>  <nl> return stream ; <nl> }
PHP_FILEINFO_API zend_object_value finfo_objects_new ( zend_class_entry * class_typ <nl> intern = ecalloc ( 1 , sizeof ( struct finfo_object )); <nl> intern -> zo . ce = class_type ; <nl> intern -> zo . properties = NULL ; <nl> -# if ZEND_EXTENSION_API_NO > 220050000 <nl> +# if ZEND_MODULE_API_NO >= 20050922 <nl> intern -> zo . guards = NULL ; <nl> # else <nl> intern -> zo . in_get = 0 ;
static int create_segments ( size_t requested_size , zend_shared_segment *** shared_ <nl> /* creating segment here */ <nl> * shared_segments_count = 1 ; <nl> * shared_segments_p = ( zend_shared_segment **) calloc ( 1 , sizeof ( zend_shared_segment )+ sizeof ( void *)); <nl> + if (!* shared_segments_p ) { <nl> + zend_win_error_message ( ACCEL_LOG_FATAL , " calloc () failed "); <nl> + * error_in = " calloc "; <nl> + return ALLOC_FAILURE ; <nl> + } <nl> shared_segment = ( zend_shared_segment *)(( char *)(* shared_segments_p ) + sizeof ( void *)); <nl> (* shared_segments_p )[ 0 ] = shared_segment ; <nl> 
ZEND_METHOD ( reflection_class , isSubclassOf ) <nl> case IS_UNICODE : <nl> if ( zend_u_lookup_class ( Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name ), Z_UNILEN_P ( class_name ), & pce TSRMLS_CC ) == FAILURE ) { <nl> zend_throw_exception_ex ( reflection_exception_ptr , 0 TSRMLS_CC , <nl> - " Interface % R does not exist ", Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name )); <nl> + " Class % R does not exist ", Z_TYPE_P ( class_name ), Z_UNIVAL_P ( class_name )); <nl> return ; <nl> } <nl> class_ce = * pce ;
PHP_FUNCTION ( simplexml_import_dom ) <nl>  <nl> if ( object -> node && object -> node -> node ) { <nl> nodep = object -> node -> node ; <nl> + if ( nodep -> doc == NULL ) { <nl> + php_error ( E_WARNING , " Imported Node must have associated Document "); <nl> + RETURN_NULL (); <nl> + } <nl> if ( nodep -> type == XML_DOCUMENT_NODE || nodep -> type == XML_HTML_DOCUMENT_NODE ) { <nl> nodep = xmlDocGetRootElement (( xmlDocPtr ) nodep ); <nl> }
PHPAPI int _php_stream_copy_to_stream_ex ( php_stream * src , php_stream * dest , size <nl>  <nl> * len = didwrite ; <nl>  <nl> - /* read bytes match written */ <nl> - if ( mapped == didwrite ) { <nl> + /* we ' ve got at least 1 byte to read <nl> + * less than 1 is an error <nl> + * AND read bytes match written */ <nl> + if ( mapped > 0 && mapped == didwrite ) { <nl> return SUCCESS ; <nl> } <nl> return FAILURE ;
PHP_METHOD ( Phar , copy ) <nl> } <nl> } <nl>  <nl> - if ( phar_path_check (& newfile , & newfile_len , & pcr_error ) > pcr_is_ok ) { <nl> + if ( phar_path_check (& newfile , ( int *) & newfile_len , & pcr_error ) > pcr_is_ok ) { <nl> zend_throw_exception_ex ( spl_ce_UnexpectedValueException , 0 TSRMLS_CC , <nl> " file \"% s \" contains invalid characters % s , cannot be copied from \"% s \" in phar % s ", newfile , pcr_error , oldfile , phar_obj -> archive -> fname ); <nl> RETURN_FALSE ;
static char * substring_conf ( char * start , int len , char quote TSRMLS_DC ) <nl> # if HAVE_MBSTRING && ! defined ( COMPILE_DL_MBSTRING ) <nl> if ( php_mb_encoding_translation ( TSRMLS_C )) { <nl> size_t j = php_mb_mbchar_bytes ( start + i TSRMLS_CC ); <nl> - while ( j -- > 0 ) { <nl> + while ( j -- > 0 && i < len ) { <nl> * resp ++ = start [ i ++]; <nl> } <nl> -- i ;
int call_user_function_ex ( HashTable * function_table , zval ** object_pp , zval * fun <nl>  <nl> zend_ptr_stack_n_push (& EG ( argument_stack ), 2 , ( void *) ( long ) param_count , NULL ); <nl>  <nl> + EG ( function_state_ptr ) = & function_state ; <nl> + <nl> if ( function_state . function -> type == ZEND_USER_FUNCTION ) { <nl> calling_symbol_table = EG ( active_symbol_table ); <nl> if ( symbol_table ) {
PHP_FUNCTION ( oci_password_change ) <nl> WRONG_PARAM_COUNT ; <nl> } <nl>  <nl> + convert_to_string_ex ( user_param ); <nl> + convert_to_string_ex ( pass_old_param ); <nl> + convert_to_string_ex ( pass_new_param ); <nl> + <nl> user = Z_STRVAL_PP ( user_param ); <nl> pass_old = Z_STRVAL_PP ( pass_old_param ); <nl> pass_new = Z_STRVAL_PP ( pass_new_param );
int zend_register_functions ( zend_class_entry * scope , zend_function_entry * functi <nl> char * lowercase_name ; <nl> int fname_len ; <nl>  <nl> + memset ( internal_function , 0 , sizeof ( zend_function )); <nl> if ( type == MODULE_PERSISTENT ) { <nl> error_type = E_CORE_WARNING ; <nl> } else {
static void ps_files_open ( ps_files * data , const char * key ) <nl> data -> basedir = NULL ; <nl> data -> basedir_len = 0 ; <nl> } <nl> + efree ( data ); <nl> php_error_docref ( NULL , E_WARNING , " The session id is too long or contains illegal characters , valid characters are a - z , A - Z , 0 - 9 and '-,'"); <nl> return ; <nl> }
ZEND_API int zend_restore_ini_entry ( char * name , uint name_length , int stage ) /* <nl> } <nl>  <nl> if ( EG ( modified_ini_directives )) { <nl> - zend_restore_ini_entry_cb ( ini_entry , stage TSRMLS_CC ); <nl> - zend_hash_del ( EG ( modified_ini_directives ), name , name_length ); <nl> + if ( zend_restore_ini_entry_cb ( ini_entry , stage TSRMLS_CC ) == 0 ) { <nl> + zend_hash_del ( EG ( modified_ini_directives ), name , name_length ); <nl> + } else { <nl> + return FAILURE ; <nl> + } <nl> } <nl>  <nl> return SUCCESS ;
int phar_get_entry_data ( phar_entry_data ** ret , char * fname , int fname_len , char <nl> if ( entry -> link ) { <nl> phar_entry_info * link = phar_get_link_source ( entry TSRMLS_CC ); <nl> if (! link ) { <nl> + efree (* ret ); <nl> return FAILURE ; <nl> } <nl> (* ret )-> zero = phar_get_fp_offset ( link TSRMLS_CC );
typedef int32_t zend_off_t ; <nl> # define ZEND_STRTOUL ( s0 , s1 , base ) strtoull (( s0 ), ( s1 ), ( base )) <nl> # define ZEND_STRTOL_PTR strtoll <nl> # define ZEND_STRTOUL_PTR strtoull <nl> -# define ZEND_ABS llabs <nl> +# define ZEND_ABS imaxabs <nl> # endif <nl> # else <nl> # define ZEND_STRTOL ( s0 , s1 , base ) strtol (( s0 ), ( s1 ), ( base ))
apprentice_map ( struct magic_set * ms , const char * fn ) <nl> if ( dbname == NULL ) <nl> goto error ; <nl>  <nl> - stream = php_stream_open_wrapper (( char *) fn , " rb ", REPORT_ERRORS , NULL ); <nl> + stream = php_stream_open_wrapper (( char *) fn , " rb ", REPORT_ERRORS , NULL ); <nl>  <nl> if (! stream ) { <nl> goto error ;
PHP_FUNCTION ( fd_set ) <nl> FD_SET ( fd , & readfd ); <nl> if ( fd > max_fd ) max_fd = fd ; <nl> } <nl> + efree ( args ); <nl> } <nl> RETURN_LONG ( 1 ); <nl> }
PHPAPI php_stream_filter * php_stream_filter_create ( const char * filtername , zval <nl> /* try a wildcard */ <nl> char * wildname ; <nl>  <nl> - wildname = estrdup ( filtername ); <nl> + wildname = emalloc ( n + 3 ); <nl> + memcpy ( wildname , filtername , n + 1 ); <nl> period = wildname + ( period - filtername ); <nl> while ( period && ! filter ) { <nl> * period = '\ 0 ';
PHP_FUNCTION ( imap_utf8 ) <nl> if ( dest . data ) { <nl> free ( dest . data ); <nl> } <nl> + if ( src . data ) { <nl> + free ( src . data ); <nl> + } <nl> } <nl> /* }}} */ <nl> 
static int php_cli_server_poller_iter_on_active ( php_cli_server_poller * poller , v <nl> SOCKET fd ; <nl> int events ; <nl> } entries [ FD_SETSIZE * 2 ]; <nl> - php_socket_t fd = 0 ; <nl> size_t i ; <nl> struct socket_entry * n = entries , * m ; <nl> 
SAPI_API SAPI_POST_HANDLER_FUNC ( rfc1867_post_handler ) <nl> int llen = 0 ; <nl> int upload_cnt = INI_INT (" max_file_uploads "); <nl>  <nl> - if ( SG ( request_info ). content_length > SG ( post_max_size )) { <nl> + if ( SG ( post_max_size ) > 0 && SG ( request_info ). content_length > SG ( post_max_size )) { <nl> sapi_module . sapi_error ( E_WARNING , " POST Content - Length of % ld bytes exceeds the limit of % ld bytes ", SG ( request_info ). content_length , SG ( post_max_size )); <nl> return ; <nl> }
PHPAPI extern const char php_sig_gif [ 3 ]; <nl> PHPAPI extern const char php_sig_jpg [ 3 ]; <nl> PHPAPI extern const char php_sig_png [ 3 ]; <nl> - PHPAPI const char php_sig_gd2 [ 3 ] = {' g ', ' d ', ' 2 '}; <nl> + PHPAPI extern const char php_sig_gd2 [ 3 ] = {' g ', ' d ', ' 2 '}; <nl>  <nl> extern zend_module_entry gd_module_entry ; <nl> # define phpext_gd_ptr & gd_module_entry
SAPI_API size_t sapi_apply_default_charset ( char ** mimetype , size_t len TSRMLS_DC <nl> newtype = emalloc ( newlen + 1 ); <nl> PHP_STRLCPY ( newtype , * mimetype , newlen + 1 , len ); <nl> strlcat ( newtype , "; charset =", newlen + 1 ); <nl> + strlcat ( newtype , charset , newlen + 1 ); <nl> efree (* mimetype ); <nl> * mimetype = newtype ; <nl> return newlen ;
static inline zval * zend_assign_to_variable ( zval * variable_ptr , zval * value TSRM <nl> value = Z_REFVAL_P ( value ); <nl> } <nl> if ( Z_REFCOUNTED_P ( value )) { <nl> + if ( UNEXPECTED ( variable_ptr == value )) { <nl> + return variable_ptr ; <nl> + } <nl> Z_ADDREF_P ( value ); <nl> } <nl> }
static int init_request_info ( TSRMLS_D ) <nl> SG ( request_info ). content_length = LSAPI_GetReqBodyLen (); <nl> SG ( request_info ). path_translated = estrdup ( LSAPI_GetScriptFileName ()); <nl>  <nl> - /* It is not reset by zend engine , set it to 0 . */ <nl> - SG ( sapi_headers ). http_response_code = 0 ; <nl> + /* It is not reset by zend engine , set it to 200 . */ <nl> + SG ( sapi_headers ). http_response_code = 200 ; <nl>  <nl> pAuth = LSAPI_GetHeader ( H_AUTHORIZATION ); <nl> php_handle_auth_data ( pAuth TSRMLS_CC );
ZEND_API zend_mm_heap * zend_mm_startup ( void ) <nl> if ( zend_mm_low_bit ( seg_size ) != zend_mm_high_bit ( seg_size )) { <nl> fprintf ( stderr , " ZEND_MM_SEG_SIZE must be a power of two \ n "); <nl> exit ( 255 ); <nl> + } else if ( seg_size < ZEND_MM_ALIGNED_SEGMENT_SIZE + ZEND_MM_ALIGNED_HEADER_SIZE ) { <nl> + fprintf ( stderr , " ZEND_MM_SEG_SIZE is too small \ n "); <nl> + exit ( 255 ); <nl> } <nl> } else { <nl> seg_size = ZEND_MM_SEG_SIZE ;
static inline int php_tcp_sockop_connect ( php_stream * stream , php_netstream_data_ <nl> if ( xparam -> want_errortext ) { <nl> spprintf (& xparam -> outputs . error_text , 0 , " local_addr context option is not a string ."); <nl> } <nl> + efree ( host ); <nl> return - 1 ; <nl> } <nl> bindto = parse_ip_address_ex ( Z_STRVAL_PP ( tmpzval ), Z_STRLEN_PP ( tmpzval ), & bindport , xparam -> want_errortext , & xparam -> outputs . error_text TSRMLS_CC );
static PHP_INI_MH ( OnTypeLibFileUpdate ) <nl> char * strtok_buf = NULL ; <nl> int cached ; <nl>  <nl> - if (! new_value || ( typelib_file = VCWD_FOPEN ( new_value , " r "))== NULL ) { <nl> + if (! new_value || ! new_value [ 0 ] || ( typelib_file = VCWD_FOPEN ( new_value , " r "))== NULL ) { <nl> return FAILURE ; <nl> } <nl> 
SPL_METHOD ( SplFileInfo , getExtension ) <nl>  <nl> p = zend_memrchr ( ret -> val , '.', ret -> len ); <nl> if ( p ) { <nl> - idx = p - fname ; <nl> + idx = p - ret -> val ; <nl> RETVAL_STRINGL ( ret -> val + idx + 1 , ret -> len - idx - 1 ); <nl> STR_RELEASE ( ret ); <nl> return ;
php_sprintf_appenddouble ( char ** buffer , int * pos , <nl> if (( adjust & ADJ_PRECISION ) == 0 ) { <nl> precision = FLOAT_PRECISION ; <nl> } else if ( precision > MAX_FLOAT_PRECISION ) { <nl> + php_error_docref ( NULL TSRMLS_CC , E_NOTICE , " Requested precision of % d digits was truncated to PHP maximum of % d digits ", precision , MAX_FLOAT_PRECISION ); <nl> precision = MAX_FLOAT_PRECISION ; <nl> } <nl> 
PHP_FUNCTION ( grapheme_substr ) <nl> length += iter_val ; <nl> } <nl>  <nl> - if ( UBRK_DONE == sub_str_end_pos ) { <nl> + if ( UBRK_DONE == sub_str_end_pos && length < 0 ) { <nl>  <nl> intl_error_set ( NULL , U_ILLEGAL_ARGUMENT_ERROR , " grapheme_substr : length not contained in string ", 1 TSRMLS_CC ); <nl> 
static int php_zip_extract_file ( struct zip * za , char * dest , char * file , int fil <nl> * safemode status as its parent folder ? <nl> */ <nl> if ( OPENBASEDIR_CHECKPATH ( fullpath )) { <nl> + efree ( fullpath ); <nl> efree ( file_dirname_fullpath ); <nl> efree ( file_basename ); <nl> return 0 ;
static inline int _php_stream_path_param_encode ( zval ** ppzval , char ** ppath , int <nl> if ( FAILURE == php_stream_path_encode ( NULL , & path , & path_len , Z_USTRVAL_PP ( ppzval ), Z_USTRLEN_PP ( ppzval ), options , context )) { <nl> return FAILURE ; <nl> } <nl> + Z_ADDREF_PP ( ppzval ); /* the conversion removes a refcount */ <nl> MAKE_STD_ZVAL ( zpath ); <nl> ZVAL_STRINGL ( zpath , path , path_len , 0 ); <nl> Z_UNSET_ISREF_P ( zpath );
PHPAPI void php_pcre_match_impl ( pcre_cache_entry * pce , char * subject , int subjec <nl> if ( pcre_get_substring_list ( subject , offsets , count , & stringlist ) < 0 ) { <nl> efree ( subpat_names ); <nl> efree ( offsets ); <nl> + if ( match_sets ) efree ( match_sets ); <nl> php_error_docref ( NULL TSRMLS_CC , E_WARNING , " Get subpatterns list failed "); <nl> RETURN_FALSE ; <nl> }
static HashTable * zend_closure_get_debug_info ( zval * object , int * is_temp TSRMLS_ <nl> } <nl> info_len = zend_spprintf (& info , 0 , "% s ", <nl> i >= required ? "< optional >" : "< required >"); <nl> - add_assoc_stringl_ex (& val , name , name_len , info , info_len , 0 ); <nl> +//??? TODO : avoid reallocation <nl> + add_assoc_stringl_ex (& val , name , name_len , info , info_len , 1 ); <nl> + efree ( info ); <nl> efree ( name ); <nl> arg_info ++; <nl> }
struct _php_stream { <nl> char * orig_path ; <nl>  <nl> zend_resource * ctx ; <nl> - int flags ; /* PHP_STREAM_FLAG_XXX */ <nl> + uint32_t flags ; /* PHP_STREAM_FLAG_XXX */ <nl>  <nl> int eof ; <nl> 
PHPAPI void php_pcre_match_impl ( pcre_cache_entry * pce , char * subject , int subjec <nl> if ( pcre_get_substring_list ( subject , offsets , count , & stringlist ) < 0 ) { <nl> efree ( subpat_names ); <nl> efree ( offsets ); <nl> + if ( match_sets ) efree ( match_sets ); <nl> php_error_docref ( NULL TSRMLS_CC , E_WARNING , " Get subpatterns list failed "); <nl> RETURN_FALSE ; <nl> }
static DWORD tls_key ; <nl>  <nl> # elif defined ( BETHREADS ) <nl> static int32 tls_key ; <nl> -# define tsrm_tls_set ( what ) tls_set ( tls_key , ( void *)( what ) <nl> +# define tsrm_tls_set ( what ) tls_set ( tls_key , ( void *)( what )) <nl> # define tsrm_tls_get () ( tsrm_tls_entry *) tls_get ( tls_key ) <nl>  <nl> # else
# define _FTP_H <nl>  <nl> # include < stdio . h > <nl> -# if HAVE_UINSTD_H <nl> +# if ! PHP_WIN32 <nl> # include < netinet / in . h > <nl> # endif <nl> 
PHP_FUNCTION ( file_put_contents ) <nl> RETURN_FALSE ; <nl> } <nl> switch ( Z_TYPE_P ( data )) { <nl> + case IS_RESOURCE : <nl> + { <nl> + php_stream * srcstream ; <nl> + php_stream_from_zval ( srcstream , & data ); <nl> + <nl> + numbytes = php_stream_copy_to_stream ( srcstream , stream , PHP_STREAM_COPY_ALL ); <nl> + <nl> + break ; <nl> + } <nl> case IS_NULL : <nl> case IS_LONG : <nl> case IS_DOUBLE :
PHP_FUNCTION ( odbc_execute ) <nl>  <nl> /* Check for safe mode . */ <nl> if ( PG ( safe_mode ) && (! php_checkuid ( filename , NULL , CHECKUID_CHECK_FILE_AND_DIR ))) { <nl> - RETURN_FALSE ; <nl> - } <nl> + efree ( filename ); <nl> + efree ( params ); <nl> + RETURN_FALSE ; <nl> + } <nl>  <nl> /* Check the basedir */ <nl> if ( php_check_open_basedir ( filename TSRMLS_CC )) { <nl> + efree ( filename ); <nl> + efree ( params ); <nl> RETURN_FALSE ; <nl> } <nl> 
static zend_object * spl_filesystem_object_clone ( zval * zobject TSRMLS_DC ) <nl> old_object = Z_OBJ_P ( zobject ); <nl> source = ( spl_filesystem_object *) old_object ; <nl> new_object = spl_filesystem_object_new_ex ( old_object -> ce TSRMLS_CC ); <nl> + intern = ( spl_filesystem_object *) new_object ; <nl>  <nl> intern -> flags = source -> flags ; <nl> 
export_desktop_file ( const char * app , <nl> g_string_append_printf ( new_exec , " ", arg ); <nl> else if ( strcasecmp ( arg , "% u ") == 0 ) <nl> g_string_append_printf ( new_exec , " ", arg ); <nl> - else if ( strcmp ( arg , " u ") == 0 ) <nl> + else if ( g_str_has_prefix ( arg , "@@")) <nl> g_print ( _ (" Skipping invalid Exec argument % s \ n "), arg ); <nl> else <nl> g_string_append_printf ( new_exec , " % s ", arg );
setup_seccomp ( FlatpakBwrap * bwrap , <nl>  <nl> /* Don ' t allow subnamespace setups : */ <nl> { SCMP_SYS ( unshare ), EPERM }, <nl> + { SCMP_SYS ( setns ), EPERM }, <nl> { SCMP_SYS ( mount ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ )
main ( int argc , <nl> die_with_error (" Failed to make / slave "); <nl>  <nl> /* Create a tmpfs which we will use as / in the namespace */ <nl> - if ( mount ("", newroot , " tmpfs ", MS_NODEV | MS_NOEXEC | MS_NOSUID , NULL ) != 0 ) <nl> + if ( mount ("", newroot , " tmpfs ", MS_NODEV | MS_NOSUID , NULL ) != 0 ) <nl> die_with_error (" Failed to mount tmpfs "); <nl>  <nl> old_cwd = get_current_dir_name ();
xdg_app_dir_update_appstream ( XdgAppDir * self , <nl> if (! ostree_repo_resolve_rev ( self -> repo , remote_and_branch , TRUE , & new_checksum , error )) <nl> return FALSE ; <nl>  <nl> + if ( new_checksum == NULL ) <nl> + { <nl> + g_warning (" No appstream branch in remote % s \ n ", remote ); <nl> + return TRUE ; <nl> + } <nl> + <nl> appstream_dir = g_file_get_child ( xdg_app_dir_get_path ( self ), " appstream "); <nl> remote_dir = g_file_get_child ( appstream_dir , remote ); <nl> arch_dir = g_file_get_child ( remote_dir , arch );
flatpak_builtin_ls_remote ( int argc , char ** argv , GCancellable * cancellable , GEr <nl> if ( deploy_data == NULL ) <nl> continue ; <nl>  <nl> + if ( g_strcmp0 ( flatpak_deploy_data_get_origin ( deploy_data ), remote ) != 0 ) <nl> + continue ; <nl> + <nl> if ( g_strcmp0 ( flatpak_deploy_data_get_commit ( deploy_data ), checksum ) == 0 ) <nl> continue ; <nl> }
setup_seccomp ( FlatpakBwrap * bwrap , <nl> { SCMP_SYS ( umount ), EPERM }, <nl> { SCMP_SYS ( umount2 ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> + { SCMP_SYS ( chroot ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ ) <nl> /* Architectures with CONFIG_CLONE_BACKWARDS2 : the child stack <nl> * and flags arguments are reversed so the flags come second */
flatpak_number_prompt ( int min , int max , const char * prompt , ...) <nl>  <nl> va_start ( var_args , prompt ); <nl> s = g_strdup_vprintf ( prompt , var_args ); <nl> + va_end ( var_args ); <nl>  <nl> while ( TRUE ) <nl> {
setup_seccomp ( FlatpakBwrap * bwrap , <nl> { SCMP_SYS ( unshare ), EPERM }, <nl> { SCMP_SYS ( setns ), EPERM }, <nl> { SCMP_SYS ( mount ), EPERM }, <nl> + { SCMP_SYS ( umount ), EPERM }, <nl> + { SCMP_SYS ( umount2 ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ ) <nl> /* Architectures with CONFIG_CLONE_BACKWARDS2 : the child stack
add_related ( GHashTable * all_refs , <nl> g_hash_table_insert ( all_refs , g_steal_pointer (& ext_collection_ref ), c_s ); <nl> } <nl>  <nl> + g_list_free_full ( extensions , ( GDestroyNotify ) flatpak_extension_free ); <nl> + <nl> return TRUE ; <nl> } <nl> 
xdp_fuse_init ( GError ** error ) <nl>  <nl> path = xdp_fuse_get_mountpoint (); <nl> if (( stat ( path , & st ) == - 1 && errno == ENOTCONN ) || <nl> - (( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN || <nl> + ((( statfs_res = statfs ( path , & stfs )) == - 1 && errno == ENOTCONN ) || <nl> ( statfs_res == 0 && stfs . f_type == 0x65735546 /* fuse */))) <nl> { <nl> int count ;
static void f_synIDattr ( typval_T * argvars , typval_T * rettv ) <nl> modec = TOLOWER_ASC ( mode [ 0 ]); <nl> if ( modec != ' c ' && modec != ' g ') <nl> modec = 0 ; /* replace invalid with current */ <nl> + } else if ( ui_rgb_attached ()) { <nl> + modec = ' g '; <nl> } else { <nl> modec = ' c '; <nl> }
static void qf_free_stack ( win_T * wp , qf_info_T * qi ) <nl> // If the location list window is open , then create a new empty location <nl> // list <nl> qf_info_T * new_ll = ll_new_list (); <nl> + <nl> + // first free the list reference in the location list window <nl> + ll_free_all (& orig_wp -> w_llist_ref ); <nl> + <nl> orig_wp -> w_llist_ref = new_ll ; <nl> if ( llwin != NULL ) { <nl> llwin -> w_llist = new_ll ;
static inline void add_search_pattern ( PossiblyFreedShadaEntry * const ret_pse , <nl> ? defaults . data . search_pattern . place_cursor_at_end <nl> : pat . off . end ), <nl> . offset = ( is_substitute_pattern <nl> - ? pat . off . off <nl> - : defaults . data . search_pattern . offset ), <nl> + ? defaults . data . search_pattern . offset <nl> + : pat . off . off ), <nl> . is_last_used = ( is_substitute_pattern ^ search_last_used ), <nl> . is_substitute_pattern = is_substitute_pattern , <nl> . highlighted = (( is_substitute_pattern ^ search_last_used )
 <nl> # include " nvim / api / private / defs . h " <nl> # include " nvim / func_attr . h " <nl> +# include " nvim / eval / typval . h " <nl> +# include " nvim / ex_cmds_defs . h " <nl>  <nl> // Generated by msgpack - gen . lua <nl> void nlua_add_api_functions ( lua_State * lstate ) REAL_FATTR_NONNULL_ALL ;
char ** crypto_cert_subject_alt_name ( X509 * xcert , int * count , int ** lengths ) <nl> * lengths = NULL ; <nl> return NULL ; <nl> } <nl> + GENERAL_NAMES_free ( subject_alt_names ); <nl>  <nl> return strings ; <nl> }
BOOL CloseHandle ( HANDLE hObject ) <nl> if ( pipe -> serverfd != - 1 ) <nl> close ( pipe -> serverfd ); <nl>  <nl> - free ( Object ); <nl> + free ( pipe -> lpFileName ); <nl> + free ( pipe -> lpFilePath ); <nl> + free ( pipe -> name ); <nl> + free ( pipe ); <nl>  <nl> return TRUE ; <nl> }
char * crypto_print_name ( X509_NAME * name ) <nl> if ( X509_NAME_print_ex ( outBIO , name , 0 , XN_FLAG_ONELINE ) > 0 ) <nl> { <nl> unsigned long size = BIO_number_written ( outBIO ); <nl> - buffer = xzalloc ( size ); <nl> - memset ( buffer , 0 , size ); <nl> + buffer = xzalloc ( size + 1 ); <nl> + memset ( buffer , 0 , size + 1 ); <nl> BIO_read ( outBIO , buffer , size ); <nl> } <nl> 
void gdi_Glyph_Free ( rdpContext * context , rdpGlyph * glyph ) <nl> gdi_SelectObject ( gdi_glyph -> hdc , ( HGDIOBJECT ) gdi_glyph -> org_bitmap ); <nl> gdi_DeleteObject (( HGDIOBJECT ) gdi_glyph -> bitmap ); <nl> gdi_DeleteDC ( gdi_glyph -> hdc ); <nl> - xfree ( gdi_glyph ); <nl> } <nl> } <nl> 
void update_free ( rdpUpdate * update ) <nl>  <nl> xfree ( update -> bitmap_update . rectangles ); <nl> xfree ( update -> pointer ); <nl> + xfree ( update -> primary -> polyline . points ); <nl> + xfree ( update -> primary -> polygon_sc . points ); <nl> xfree ( update -> primary ); <nl> xfree ( update -> secondary ); <nl> xfree ( update -> altsec );
int send_arp ( u_char type , struct ip_addr * sip , u_int8 * smac , struct ip_addr * tip <nl>  <nl> SEND_LOCK ; <nl>  <nl> + // FIXME <nl> + // why without clearing again the packet I get issue # 245 ? <nl> + libnet_clear_packet ( GBL_IFACE -> lnet ); <nl> + <nl> /* ARP uses 00 : 00 : 00 : 00 : 00 : 00 broadcast */ <nl> if ( type == ARPOP_REQUEST && tmac == MEDIA_BROADCAST ) <nl> tmac = ARP_BROADCAST ;
IODeviceSocket ::~ IODeviceSocket () <nl> proc -> kill (); <nl> } <nl>  <nl> - delete d ; <nl> + d -> deleteLater (); <nl> } <nl>  <nl> bool IODeviceSocket :: canReadLine ()
List :: List ( const Kind _kind , const QByteArray & line , int & start ): <nl>  <nl> ++ start ; <nl>  <nl> - if ( start >= line . size ()) <nl> + if ( start >= line . size () - 2 ) <nl> throw NoData ( line , start ); // no mailbox <nl>  <nl> mailbox = LowLevelParser :: getMailbox ( line , start );
int main ( int argc , char * argv []) { <nl> * the variable name we only support ptys here . */ <nl>  <nl> r = getenv_for_pid ( 1 , " container_ttys ", & container_ttys ); <nl> - if ( r >= 0 ) { <nl> + if ( r > 0 ) { <nl> char * w , * state ; <nl> size_t l ; <nl> 
int devnode_acl_all ( struct udev * udev , <nl> if ( r < 0 ) <nl> goto finish ; <nl>  <nl> - r = udev_enumerate_add_match_tag ( e , seat ); <nl> - if ( r < 0 ) <nl> - goto finish ; <nl> + if (! streq ( seat , " seat0 ")) { <nl> + r = udev_enumerate_add_match_tag ( e , seat ); <nl> + if ( r < 0 ) <nl> + goto finish ; <nl> + } <nl>  <nl> r = udev_enumerate_scan_devices ( e ); <nl> if ( r < 0 )
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl>  <nl> ll = l + strspn ( l , WHITESPACE ); <nl>  <nl> - if ( HAS_FEATURE_MEMORY_SANITIZER && startswith ( ll , " ListenNetlink =")) { <nl> + if ( HAS_FEATURE_MEMORY_SANITIZER && startswith ( ll , " ListenNetlink ")) { <nl> /* ListenNetlink causes a false positive in msan , <nl> * let ' s skip this for now . */ <nl> log_notice (" Skipping test because ListenNetlink = is present ");
end : <nl>  <nl> /* Removing empty dropin dirs */ <nl> if (! arg_full ) { <nl> - _cleanup_free_ char * dir = dirname_malloc (* original ); <nl> + _cleanup_free_ char * dir ; <nl> + <nl> + dir = dirname_malloc (* original ); <nl> + if (! dir ) <nl> + return log_oom (); <nl> + <nl> /* no need to check if the dir is empty , rmdir <nl> * does nothing if it is not the case . <nl> */
int main ( int argc , char * argv []) { <nl> log_error_errno ( r , " Failed to iterate through journal : % m "); <nl> goto finish ; <nl> } <nl> + if ( r == 0 ) { <nl> + printf ("-- No entries --\ n "); <nl> + goto finish ; <nl> + } <nl>  <nl> if (! arg_follow ) <nl> pager_open_if_enabled ();
_public_ int sd_pid_notify_with_fds ( pid_t pid , int unset_environment , const char <nl> goto finish ; <nl> } <nl>  <nl> + if ( strlen ( e ) > sizeof ( sockaddr . un . sun_path )) { <nl> + r = - EINVAL ; <nl> + goto finish ; <nl> + } <nl> + <nl> fd = socket ( AF_UNIX , SOCK_DGRAM | SOCK_CLOEXEC , 0 ); <nl> if ( fd < 0 ) { <nl> r = - errno ;
int main ( int argc , char * argv []) { <nl> const void * data ; <nl> size_t size ; <nl>  <nl> + r = sd_journal_set_data_threshold ( j , 0 ); <nl> + if ( r < 0 ) { <nl> + log_error (" Failed to unset data size threshold "); <nl> + return EXIT_FAILURE ; <nl> + } <nl> + <nl> r = sd_journal_query_unique ( j , arg_field ); <nl> if ( r < 0 ) { <nl> log_error (" Failed to query unique data objects : % s ", strerror (- r ));
static void item_free ( Item * i ) { <nl> free ( i -> uid_path ); <nl> free ( i -> gid_path ); <nl> free ( i -> description ); <nl> + free ( i -> home ); <nl> free ( i ); <nl> } <nl> 
static int check_unit ( DBusConnection * bus , char ** args , unsigned n ) { <nl> if (! arg_quiet ) <nl> puts ( state ); <nl>  <nl> - if ( streq ( state , " active ") || startswith ( state , " active -")) <nl> + if ( streq ( state , " active ") || startswith ( state , " reloading ")) <nl> r = 0 ; <nl>  <nl> dbus_message_unref ( m );
read_again : <nl> return - EINVAL ; <nl> } <nl>  <nl> - transitions = malloc0 ( total_size + tzspec_len ); <nl> + /* leave space for additional zone_names zero terminator */ <nl> + transitions = malloc0 ( total_size + tzspec_len + 1 ); <nl> if ( transitions == NULL ) <nl> return - EINVAL ; <nl> 
static int get_key ( struct udev * udev , char ** line , char ** key , enum operation_ty <nl> char * temp ; <nl>  <nl> linepos = * line ; <nl> - if ( linepos == NULL && linepos [ 0 ] == '\ 0 ') <nl> + if ( linepos == NULL || linepos [ 0 ] == '\ 0 ') <nl> return - 1 ; <nl>  <nl> /* skip whitespace */
static int bus_message_setup_kmsg ( sd_bus_message * m ) { <nl> if (! m -> kdbus ) <nl> return - ENOMEM ; <nl>  <nl> + memset ( m -> kdbus , 0 , sz ); <nl> + <nl> m -> kdbus -> flags = <nl> (( m -> header -> flags & SD_BUS_MESSAGE_NO_REPLY_EXPECTED ) ? 0 : KDBUS_MSG_FLAGS_EXPECT_REPLY ) | <nl> (( m -> header -> flags & SD_BUS_MESSAGE_NO_AUTO_START ) ? KDBUS_MSG_FLAGS_NO_AUTO_START : 0 );
int main ( int argc , char * argv [], char * envp []) <nl> dbg (" error fcntl on write pipe : % s ", strerror ( errno )); <nl>  <nl> /* set signal handlers */ <nl> + memset (& act , 0x00 , sizeof ( struct sigaction )); <nl> act . sa_handler = ( void (*) ( int )) sig_handler ; <nl> sigemptyset (& act . sa_mask ); <nl> act . sa_flags = SA_RESTART ;
int main ( int argc , char * argv []) { <nl> goto finish ; <nl> } <nl>  <nl> + sd_event_get_exit_code ( m -> event , & r ); <nl> + <nl> finish : <nl> sd_notify ( false , " STATUS = Shutting down ..."); <nl> 
static int create_item ( Item * i ) { <nl>  <nl> case CREATE_FILE : <nl> case TRUNCATE_FILE : <nl> + r = write_one_file ( i , i -> path ); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + break ; <nl> case WRITE_FILE : <nl> r = glob_item ( i , write_one_file ); <nl> if ( r < 0 )
finish : <nl>  <nl> if ( n_arguments > 3 ) { <nl> arguments [ n_arguments ] = NULL ; <nl> + strv_uniq ( arguments ); <nl> execv ("/ sbin / modprobe ", arguments ); <nl>  <nl> log_error (" Failed to execute / sbin / modprobe : % m ");
int unit_file_get_list ( <nl> } <nl>  <nl> r = null_or_empty_path ( f -> path ); <nl> - if ( r < 0 ) { <nl> + if ( r < 0 && r != - ENOENT ) { <nl> free ( f -> path ); <nl> free ( f ); <nl> goto finish ;
static int method_set_vc_keyboard ( sd_bus_message * m , void * userdata , sd_bus_erro <nl> } <nl>  <nl> # ifdef HAVE_XKBCOMMON <nl> + _printf_ ( 3 , 0 ) <nl> static void log_xkb ( struct xkb_context * ctx , enum xkb_log_level lvl , const char * format , va_list args ) { <nl> const char * fmt ; <nl> 
static int systemctl_parse_argv ( int argc , char * argv []) { <nl> size_t size ; <nl>  <nl> FOREACH_WORD_SEPARATOR ( word , size , optarg , ",", state ) { <nl> - char * s ; <nl> + _cleanup_free_ char * s = NULL ; <nl>  <nl> s = strndup ( word , size ); <nl> if (! s )
static int unit_find_paths ( <nl> _cleanup_free_ char * template = NULL ; <nl>  <nl> r = unit_name_template ( unit_name , & template ); <nl> - if ( r != - EINVAL ) <nl> + if ( r < 0 && r != - EINVAL ) <nl> return log_error_errno ( r , " Failed to determine template name : % m "); <nl> if ( r >= 0 ) { <nl> r = unit_file_find_path ( lp , template , & path );
static void font_copy_to_all_vcs ( int fd ) { <nl> return ; <nl> } <nl>  <nl> - for ( i = 1 ; i <= 15 ; i ++) { <nl> + for ( i = 1 ; i <= 63 ; i ++) { <nl> char vcname [ strlen ("/ dev / vcs ") + DECIMAL_STR_MAX ( int )]; <nl> _cleanup_close_ int vcfd = - 1 ; <nl> struct console_font_op cfo = {};
int main ( int argc , char ** argv ) { <nl> log_parse_environment (); <nl>  <nl> r = parse_config (); <nl> - if ( r <= 0 ) <nl> + if ( r < 0 ) <nl> goto finish ; <nl>  <nl> r = parse_argv ( argc , argv );
_public_ int sd_bus_path_decode_many ( const char * path , const char * path_template <nl> } <nl> va_end ( list ); <nl>  <nl> - free ( labels ); <nl> - labels = NULL ; <nl> + labels = mfree ( labels ); <nl> return 1 ; <nl> } <nl> 
static void output_unit_file_list ( const UnitFileList * units , unsigned c ) { <nl> } else <nl> id_cols = max_id_len ; <nl>  <nl> - if (! arg_no_legend ) <nl> + if (! arg_no_legend && c > 0 ) <nl> printf ("%-* s %-* s \ n ", <nl> id_cols , " UNIT FILE ", <nl> state_cols , " STATE ");
This file is part of systemd . <nl>  <nl> Copyright 2010 Kay Sievers <nl> + Copyright 2016 Michal Soltys < soltys @ ziu . info > <nl>  <nl> systemd is free software ; you can redistribute it and / or modify it <nl> under the terms of the GNU Lesser General Public License as published by
int columns ( void ) { <nl> struct winsize ws ; <nl> zero ( ws ); <nl>  <nl> - if ( ioctl ( STDIN_FILENO , TIOCGWINSZ , & ws ) >= 0 ) <nl> + if ( ioctl ( STDOUT_FILENO , TIOCGWINSZ , & ws ) >= 0 ) <nl> parsed_columns = ws . ws_col ; <nl> } <nl> 
static int list_machines ( int argc , char * argv [], void * userdata ) { <nl> while (( r = sd_bus_message_read ( reply , "( ssso )", & name , & class , & service , & object )) > 0 ) { <nl> size_t l ; <nl>  <nl> + if ( name [ 0 ] == '.' && ! arg_all ) <nl> + continue ; <nl> + <nl> if (! GREEDY_REALLOC ( machines , n_allocated , n_machines + 1 )) <nl> return log_oom (); <nl> 
static void print_status_info ( <nl>  <nl> printf (" CGroup : % s \ n ", i -> control_group ); <nl>  <nl> - if ( arg_transport == BUS_TRANSPORT_LOCAL ) { <nl> + if ( arg_transport == BUS_TRANSPORT_LOCAL || arg_transport == BUS_TRANSPORT_CONTAINER ) { <nl> unsigned k = 0 ; <nl> pid_t extra [ 2 ]; <nl> char prefix [] = " ";
static int manager_sigusr2 ( sd_event_source * s , const struct signalfd_siginfo * si <nl> assert ( m ); <nl>  <nl> manager_flush_caches ( m ); <nl> - log_info (" Flushed all caches ."); <nl>  <nl> return 0 ; <nl> } <nl> void manager_flush_caches ( Manager * m ) { <nl>  <nl> LIST_FOREACH ( scopes , scope , m -> dns_scopes ) <nl> dns_cache_flush (& scope -> cache ); <nl> + <nl> + log_info (" Flushed all caches ."); <nl> }
int dns_zone_put ( DnsZone * z , DnsScope * s , DnsResourceRecord * rr , bool probe ) { <nl> if ( established ) <nl> i -> state = DNS_ZONE_ITEM_ESTABLISHED ; <nl> else { <nl> + i -> state = DNS_ZONE_ITEM_PROBING ; <nl> + <nl> r = dns_zone_item_probe_start ( i ); <nl> if ( r < 0 ) { <nl> dns_zone_item_remove_and_free ( z , i ); <nl> i = NULL ; <nl> return r ; <nl> } <nl> - <nl> - i -> state = DNS_ZONE_ITEM_PROBING ; <nl> } <nl> } else <nl> i -> state = DNS_ZONE_ITEM_ESTABLISHED ;
int session_set_controller ( Session * s , const char * sender , bool force ) { <nl> * If logind crashes / restarts , we restore the controller during restart <nl> * or reset the VT in case it crashed / exited , too . */ <nl> r = session_prepare_vt ( s ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( t ); <nl> return r ; <nl> + } <nl>  <nl> session_swap_controller ( s , t ); <nl> 
bool ignore_file ( const char * filename ) { <nl> assert ( filename ); <nl>  <nl> if ( endswith ( filename , "~")) <nl> - return false ; <nl> + return true ; <nl>  <nl> return ignore_file_allow_backup ( filename ); <nl> }
int udevdb_add_dev ( const char * path , const struct udevice * dev ) <nl> if (( path == NULL ) || ( dev == NULL )) <nl> return - ENODEV ; <nl>  <nl> - memset ( keystr , 0 , NAME_SIZE ); <nl> + memset ( keystr , 0 , SYSFS_PATH_MAX ); <nl> strfieldcpy ( keystr , path ); <nl> key . dptr = keystr ; <nl> key . dsize = strlen ( keystr ) + 1 ;
int main ( int argc , char * argv []) { <nl> finish : <nl> pager_close (); <nl>  <nl> + strv_free ( arg_file ); <nl> + <nl> return r < 0 ? EXIT_FAILURE : EXIT_SUCCESS ; <nl> }
bool socket_address_equal ( const SocketAddress * a , const SocketAddress * b ) { <nl> break ; <nl>  <nl> case AF_UNIX : <nl> + if ( a -> size <= offsetof ( struct sockaddr_un , sun_path ) || <nl> + b -> size <= offsetof ( struct sockaddr_un , sun_path )) <nl> + return false ; <nl> + <nl> if (( a -> sockaddr . un . sun_path [ 0 ] == 0 ) != ( b -> sockaddr . un . sun_path [ 0 ] == 0 )) <nl> return false ; <nl> 
static void bus_free ( sd_bus * b ) { <nl>  <nl> sd_bus_detach_event ( b ); <nl>  <nl> + if ( b -> default_bus_ptr ) <nl> + * b -> default_bus_ptr = NULL ; <nl> + <nl> bus_close_fds ( b ); <nl>  <nl> if ( b -> kdbus_buffer )
static DBusHandlerResult locale_message_handler ( <nl> " Locale \ 0 "); <nl> if (! changed ) <nl> goto oom ; <nl> - } <nl> + } else <nl> + strv_free ( l ); <nl> + <nl> } else if ( dbus_message_is_method_call ( message , " org . freedesktop . locale1 ", " SetVConsoleKeyboard ")) { <nl>  <nl> const char * keymap , * keymap_toggle ;
struct acpi_fpdt_boot { <nl> }; <nl>  <nl> int acpi_get_boot_usec ( usec_t * loader_start , usec_t * loader_exit ) { <nl> - _cleanup_free_ char * buf ; <nl> + _cleanup_free_ char * buf = NULL ; <nl> struct acpi_table_header * tbl ; <nl> size_t l ; <nl> struct acpi_fpdt_header * rec ;
int base_filesystem_create ( const char * root ) { <nl> const char * target = NULL ; <nl> const char * s ; <nl>  <nl> + if ( faccessat ( fd , table [ i ]. dir , F_OK , AT_SYMLINK_NOFOLLOW ) >= 0 ) <nl> + continue ; <nl> + <nl> /* check if one of the targets exists */ <nl> NULSTR_FOREACH ( s , table [ i ]. target ) { <nl> if ( faccessat ( fd , s , F_OK , AT_SYMLINK_NOFOLLOW ) < 0 )
static int swap_add_device_links ( Swap * s ) { <nl> if (! s -> what ) <nl> return 0 ; <nl>  <nl> + if (! s -> from_fragment ) <nl> + return 0 ; <nl> + <nl> if ( is_device_path ( s -> what )) <nl> return unit_add_node_link ( UNIT ( s ), s -> what , UNIT ( s )-> manager -> running_as == SYSTEMD_SYSTEM ); <nl> else
int message_append_basic ( sd_bus_message * m , char type , const void * p , const void <nl> void * a ; <nl> char * e = NULL ; <nl> int fd = - 1 ; <nl> - uint32_t fdi ; <nl> + uint32_t fdi = 0 ; <nl> int r ; <nl>  <nl> if (! m )
static int unit_create_cgroups ( Unit * u , CGroupControllerMask mask ) { <nl> is_in_hash = true ; <nl>  <nl> if ( r < 0 ) { <nl> - free ( path ); <nl> log_error (" cgroup % s exists already : % s ", path , strerror (- r )); <nl> + free ( path ); <nl> return r ; <nl> } <nl> 
int udev_device_set_syspath ( struct udev_device * udev_device , const char * syspath <nl> } <nl>  <nl> /* trailing number */ <nl> - while ( isdigit ( udev_device -> sysname [-- len ])) <nl> + while ( len > 0 && isdigit ( udev_device -> sysname [-- len ])) <nl> udev_device -> sysnum = & udev_device -> sysname [ len ]; <nl> + <nl> + /* sysname is completely numeric */ <nl> + if ( len == 0 ) <nl> + udev_device -> sysnum = NULL ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int import_program_into_properties ( struct udev_device * dev , const char * p <nl> { <nl> struct udev * udev = udev_device_get_udev ( dev ); <nl> char ** envp ; <nl> - char result [ 2048 ]; <nl> + char result [ 4096 ]; <nl> size_t reslen ; <nl> char * line ; <nl> 
void seat_evict_position ( Seat * s , Session * session ) { <nl> * position ( eg ., during gdm -> session transition ), so let ' s look <nl> * for it and set it on the free slot . */ <nl> LIST_FOREACH ( sessions_by_seat , iter , s -> sessions ) { <nl> - if ( iter -> position == pos ) { <nl> + if ( iter -> position == pos && session_get_state ( iter ) != SESSION_CLOSING ) { <nl> s -> positions [ pos ] = iter ; <nl> break ; <nl> }
int parse_timestamp ( const char * t , usec_t * usec ) { <nl>  <nl> x = time ( NULL ); <nl> assert_se ( localtime_r (& x , & tm )); <nl> + tm . tm_isdst = - 1 ; <nl>  <nl> if ( streq ( t , " now ")) <nl> goto finish ;
static int output_cat ( sd_journal * j , OutputMode mode , unsigned line , <nl>  <nl> r = sd_journal_get_data ( j , " MESSAGE ", & data , & l ); <nl> if ( r < 0 ) { <nl> + /* An entry without MESSAGE =? */ <nl> + if ( r == - ENOENT ) <nl> + return 0 ; <nl> + <nl> log_error (" Failed to get data : % s ", strerror (- r )); <nl> return r ; <nl> }
static int json_parse_tokens ( JsonVariant ** tokens , size_t ntokens , JsonVariant * <nl> size_t it = 0 ; <nl> int r ; <nl> JsonVariant * e ; <nl> - _cleanup_jsonunref_ JsonVariant * p ; <nl> + _cleanup_jsonunref_ JsonVariant * p = NULL ; <nl>  <nl> assert ( tokens ); <nl> assert ( ntokens );
static int detect_vm_xen ( void ) { <nl> r = read_one_line_file ("/ proc / xen / capabilities ", & domcap ); <nl> if ( r == - ENOENT ) <nl> return VIRTUALIZATION_NONE ; <nl> + if ( r < 0 ) <nl> + return r ; <nl>  <nl> i = domcap ; <nl> while (( cap = strsep (& i , ",")))
int bus_socket_read_message ( sd_bus * bus ) { <nl> return - EIO ; <nl> } <nl>  <nl> - f = realloc ( bus -> fds , sizeof ( int ) + ( bus -> n_fds + n )); <nl> + f = realloc ( bus -> fds , sizeof ( int ) * ( bus -> n_fds + n )); <nl> if (! f ) { <nl> close_many (( int *) CMSG_DATA ( cmsg ), n ); <nl> return - ENOMEM ;
void seat_claim_position ( Seat * s , Session * session , unsigned int pos ) { <nl> seat_evict_position ( s , session ); <nl>  <nl> session -> position = pos ; <nl> - if ( pos > 0 && ! s -> positions [ pos ]) <nl> + if ( pos > 0 ) <nl> s -> positions [ pos ] = session ; <nl> } <nl> 
static char ** user_dirs ( <nl> const char * e ; <nl> _cleanup_strv_free_ char ** config_dirs = NULL , ** data_dirs = NULL ; <nl> _cleanup_free_ char * data_home = NULL ; <nl> - _cleanup_free_ char ** res = NULL ; <nl> + _cleanup_strv_free_ char ** res = NULL ; <nl> char ** tmp ; <nl> int r ; <nl> 
static int add_string ( struct udev_rules * rules , const char * str ) <nl> unsigned short node_off ; <nl> unsigned char key ; <nl> size_t len ; <nl> - int depth ; <nl> + unsigned int depth ; <nl> unsigned int off ; <nl>  <nl> len = strlen ( str );
static int server_parse_proc_cmdline ( Server * s ) { <nl>  <nl> p = line ; <nl> for (;;) { <nl> - _cleanup_free_ char * word ; <nl> + _cleanup_free_ char * word = NULL ; <nl>  <nl> r = extract_first_word (& p , & word , NULL , 0 ); <nl> if ( r < 0 )
static int transaction_verify_order_one ( Transaction * tr , Job * j , Job * from , unsi <nl> " Found dependency on % s /% s ", <nl> k -> unit -> id , job_type_to_string ( k -> type )); <nl>  <nl> - if (! delete && <nl> + if (! delete && hashmap_get ( tr -> jobs , k -> unit ) && <nl> ! unit_matters_to_anchor ( k -> unit , k )) { <nl> /* Ok , we can drop this one , so let ' s <nl> * do so . */
SecureVector < byte > generate_dsa_primes ( BigInt & p , BigInt & q , u32bit pbits ) <nl> BigInt random_prime ( u32bit bits , const BigInt & coprime , <nl> u32bit equiv , u32bit modulo ) <nl> { <nl> - if ( bits <= 48 ) <nl> + if ( bits < 48 ) <nl> throw Invalid_Argument (" random_prime : Can ' t make a prime of " + <nl> to_string ( bits ) + " bits "); <nl> 
gss_verify_mic ( OM_uint32 * minor_status , <nl> gss_qop_t * qop_state ) <nl> { <nl> struct _gss_context * ctx = ( struct _gss_context *) context_handle ; <nl> - gssapi_mech_interface m = ctx -> gc_mech ; <nl> + gssapi_mech_interface m ; <nl>  <nl> if ( qop_state ) <nl> * qop_state = 0 ; <nl> gss_verify_mic ( OM_uint32 * minor_status , <nl> return GSS_S_NO_CONTEXT ; <nl> } <nl>  <nl> + m = ctx -> gc_mech ; <nl> + <nl> return ( m -> gm_verify_mic ( minor_status , ctx -> gc_ctx , <nl> message_buffer , token_buffer , qop_state )); <nl> }
KRB5_LIB_FUNCTION krb5_error_code KRB5_LIB_CALL <nl> krb5_ret_int16 ( krb5_storage * sp , <nl> int16_t * value ) <nl> { <nl> - int32_t v ; <nl> + int32_t v = 0 ; <nl> int ret ; <nl> ret = krb5_ret_int ( sp , & v , 2 ); <nl> if ( ret )
imath_rsa_public_decrypt ( int flen , const unsigned char * from , <nl> mp_int_clear (& us ); <nl>  <nl> /* head zero was skipped by mp_int_to_unsigned */ <nl> + if (* p == 0 ) <nl> + return - 7 ; <nl> if (* p != 1 ) <nl> return - 6 ; <nl> size --; p ++;
roken_detach_prep ( int argc , char ** argv , char * special_arg ) <nl> do { <nl> bytes = read ( pipefds [ 0 ], buf , sizeof ( buf )); <nl> } while ( bytes == - 1 && errno == EINTR ); <nl> + ( void ) close ( pipefds [ 0 ]); <nl> + pipefds [ 0 ] = - 1 ; <nl> if ( bytes == - 1 ) { <nl> /* <nl> * No need to wait for the process . We ' ve killed it . If it
create_and_write_cookie ( char * xauthfile , <nl> struct in_addr loopback ; <nl> struct hostent * h ; <nl>  <nl> - k_gethostname ( hostname , sizeof ( hostname )); <nl> + gethostname ( hostname , sizeof ( hostname )); <nl> loopback . s_addr = htonl ( INADDR_LOOPBACK ); <nl>  <nl> auth . family = FamilyLocal ;
add_enc_ts_padata ( krb5_context context , <nl> if ( salt == NULL ) { <nl> /* default to standard salt */ <nl> ret = krb5_get_pw_salt ( context , client , & salt2 ); <nl> + if ( ret ) <nl> + return ret ; <nl> salt = & salt2 ; <nl> } <nl> if (! enctypes ) {
 <nl> extern require_preauth ; <nl> extern sig_atomic_t exit_flag ; <nl> + extern char * keyfile ; <nl>  <nl> extern struct timeval now ; <nl> # define kdc_time ( now . tv_sec ) <nl> void loop ( krb5_context ); <nl>  <nl> void kdc_log ( int , const char * fmt , ...); <nl>  <nl> + Key * unseal_key ( Key * key ); <nl> + <nl> # define ALLOC ( X ) (( X ) = malloc ( sizeof (*( X )))) <nl>  <nl> # endif /* __KDC_LOCL_H__ */
cms_create_sd ( struct cms_create_sd_options * opt , int argc , char ** argv ) <nl> if ( ret ) <nl> hx509_err ( context , 1 , ret , " hx509_certs_find "); <nl> } <nl> + if (! opt -> embedded_certs_flag ) <nl> + flags |= HX509_CMS_SIGNATURE_NO_CERTS ; <nl> + if ( opt -> embed_leaf_only_flag ) <nl> + flags |= HX509_CMS_SIGNATURE_LEAF_ONLY ; <nl>  <nl> ret = rk_undumpdata ( infile , & p , & sz ); <nl> if ( ret )
verify_checksum ( krb5_context context , <nl> if ( keyed_checksum && crypto == NULL ) { <nl> krb5_set_error_message ( context , KRB5_PROG_SUMTYPE_NOSUPP , <nl> N_ (" Checksum type % s is keyed but no " <nl> - " crypto context ( key ) was passed in ", "") <nl> + " crypto context ( key ) was passed in ", ""), <nl> ct -> name ); <nl> return KRB5_PROG_SUMTYPE_NOSUPP ; /* XXX */ <nl> }
# ifndef KRB5_DEPRECATED <nl> # if defined ( __GNUC__ ) && (( __GNUC__ > 3 ) || (( __GNUC__ == 3 ) && ( __GNUC_MINOR__ >= 1 ))) <nl> # define KRB5_DEPRECATED __attribute__ (( deprecated )) <nl> -# elif defined ( _MSC_VER ) <nl> +# elif defined ( _MSC_VER ) && ( _MSC_VER > 1200 ) <nl> # define KRB5_DEPRECATED __declspec ( deprecated ) <nl> # else <nl> # define KRB5_DEPRECATED
get_creds ( krb5_context context , const char * keytab_str , <nl>  <nl> ret = krb5_cc_store_cred ( context , * cache , & creds ); <nl> if ( ret ) krb5_err ( context , 1 , ret , " krb5_cc_store_cred "); <nl> + <nl> + krb5_free_cred_contents ( context , & creds ); <nl> + krb5_free_principal ( context , client ); <nl> } <nl>  <nl> static krb5_error_code
EVP_BytesToKey ( const EVP_CIPHER * type , <nl> void * keydata , <nl> void * ivdata ) <nl> { <nl> + return - 1 ; <nl> } <nl> 
check ( void * opt , int argc , char ** argv ) <nl> p2 = strdup ( realm ); <nl> if ( p2 == NULL ) { <nl> krb5_warn ( context , errno , " malloc "); <nl> - free ( p ); <nl> goto fail ; <nl> } <nl> strlwr ( p2 );
tgs_build_reply ( astgs_request_t priv , <nl>  <nl> s = & adtkt . cname ; <nl> r = adtkt . crealm ; <nl> + } else if ( s == NULL ) { <nl> + ret = KRB5KDC_ERR_S_PRINCIPAL_UNKNOWN ; <nl> + _kdc_set_e_text ( r , " No server in request "); <nl> + goto out ; <nl> } <nl>  <nl> _krb5_principalname2krb5_principal ( context , & sp , * s , r );
del_enctype ( void * opt , int argc , char ** argv ) <nl> goto out2 ; <nl> } <nl>  <nl> + if ( kadm5_all_keys_are_bogus ( princ . n_key_data , princ . key_data )) { <nl> + krb5_warnx ( context , " user lacks get - keys privilege "); <nl> + goto out ; <nl> + } <nl> + <nl> new_key_data = malloc ( princ . n_key_data * sizeof (* new_key_data )); <nl> if ( new_key_data == NULL && princ . n_key_data != 0 ) { <nl> krb5_warnx ( context , " out of memory ");
stats_config ( <nl> rawstats . fp = NULL ; <nl> filegen_setup (& rawstats , now . l_ui ); <nl> } <nl> +# ifdef OPENSSL <nl> + if ( cryptostats . prefix == & statsdir [ 0 ] && <nl> + cryptostats . fp != NULL ) { <nl> + fclose ( cryptostats . fp ); <nl> + cryptostats . fp = NULL ; <nl> + filegen_setup (& cryptostats , now . l_ui ); <nl> + } <nl> +# endif /* OPENSSL */ <nl> } <nl> break ; <nl> 
int rdp_redirection_apply_settings ( rdpRdp * rdp ) <nl> settings -> TargetNetAddresses [ i ] = _strdup ( redirection -> TargetNetAddresses [ i ]); <nl> if (! settings -> TargetNetAddresses [ i ]) <nl> { <nl> - for (; i > 0 ; -- i ) <nl> - free ( settings -> TargetNetAddresses [ i ]); <nl> + UINT32 j ; <nl> + <nl> + for ( j = 0 ; j < i ; j ++) <nl> + free ( settings -> TargetNetAddresses [ j ]); <nl> return - 1 ; <nl> } <nl> }
rdpSettings * freerdp_settings_new ( DWORD flags ) <nl> settings -> SaltedChecksum = TRUE ; <nl> settings -> ServerPort = 3389 ; <nl> settings -> GatewayPort = 443 ; <nl> + settings -> GatewayBypassLocal = TRUE ; <nl> settings -> DesktopResize = TRUE ; <nl> settings -> ToggleFullscreen = TRUE ; <nl> settings -> DesktopPosX = 0 ;
void winpr_HexDump ( const char * tag , int level , const BYTE * data , int length ) <nl> const BYTE * p = data ; <nl> int i , line , offset = 0 ; <nl> const size_t llen = ( length > WINPR_HEXDUMP_LINE_LENGTH ) ? WINPR_HEXDUMP_LINE_LENGTH : length ; <nl> - size_t blen = 5 + llen * 5 ; <nl> + size_t blen = 7 + WINPR_HEXDUMP_LINE_LENGTH * 5 ; <nl> size_t pos = 0 ; <nl> char * buffer = malloc ( blen ); <nl> 
static int cliprdr_server_receive_format_list ( CliprdrServerContext * context , wSt <nl>  <nl> for ( index = 0 ; index < formatList . numFormats ; index ++) <nl> { <nl> - if ( formats [ index ]. formatName ) <nl> - free ( formats [ index ]. formatName ); <nl> + if ( formatList . formats [ index ]. formatName ) <nl> + free ( formatList . formats [ index ]. formatName ); <nl> } <nl>  <nl> - free ( formats ); <nl> + free ( formatList . formats ); <nl>  <nl> return 1 ; <nl> }
int makecert_context_process ( MAKECERT_CONTEXT * context , int argc , char ** argv ) <nl> if (! rsa ) <nl> return - 1 ; <nl>  <nl> + context -> rsa = RSA_new (); <nl> + if (! context -> rsa ) <nl> + { <nl> + BN_clear_free ( rsa ); <nl> + return - 1 ; <nl> + } <nl> BN_set_word ( rsa , RSA_F4 ); <nl> rc = RSA_generate_key_ex ( context -> rsa , key_length , rsa , NULL ); <nl> BN_clear_free ( rsa );
int WLog_ParseFilters () <nl> g_Filters = calloc ( g_FilterCount , sizeof ( wLogFilter )); <nl>  <nl> if (! g_Filters ) <nl> + { <nl> + free ( strs ); <nl> return - 1 ; <nl> + } <nl>  <nl> for ( count = 0 ; count < g_FilterCount ; count ++) <nl> { <nl> status = WLog_ParseFilter (& g_Filters [ count ], strs [ count ]); <nl>  <nl> if ( status < 0 ) <nl> + { <nl> + free ( strs ); <nl> return - 1 ; <nl> + } <nl> } <nl>  <nl> free ( strs );
BOOL xf_event_process ( freerdp * instance , XEvent * event ) <nl> if ( event -> xcookie . type == GenericEvent && <nl> event -> xcookie . extension == xfi -> XInputOpcode ) <nl> { <nl> - switch ( cookie . evtype ) <nl> + switch ( cookie -> evtype ) <nl> { <nl> case XI_ButtonPress : <nl> case XI_Motion :
int rdtk_font_parse_descriptor_buffer ( rdtkFont * font , BYTE * buffer , int size ) <nl> } <nl>  <nl> font -> glyphCount = count ; <nl> - font -> glyphs = ( rdtkGlyph *) calloc ( font -> glyphCount , sizeof ( rdtkGlyph )); <nl> + font -> glyphs = NULL ; <nl> + if ( count > 0 ) <nl> + font -> glyphs = ( rdtkGlyph *) calloc ( font -> glyphCount , sizeof ( rdtkGlyph )); <nl>  <nl> if (! font -> glyphs ) <nl> return - 1 ;
static void set_hints ( primitives_hints_t * hints ) <nl>  <nl> # elif defined ( _M_ARM ) <nl>  <nl> - static UINT32 androidNeon ( void ) <nl> + static UINT32 getNeonSupport ( void ) <nl> { <nl> # ifdef __ANDROID__ <nl> if ( android_getCpuFamily () != ANDROID_CPU_FAMILY_ARM ) return 0 ; <nl> static UINT32 androidNeon ( void ) <nl> static void set_hints ( primitives_hints_t * hints ) <nl> { <nl> /* ARM : TODO */ <nl> - hints -> arm_flags |= androidNeon (); <nl> + hints -> arm_flags |= getNeonSupport (); <nl> } <nl>  <nl> # else
void update_free ( rdpUpdate * update ) <nl>  <nl> xfree ( update -> bitmap_update . rectangles ); <nl> xfree ( update -> pointer ); <nl> + xfree ( update -> primary -> polyline . points ); <nl> + xfree ( update -> primary -> polygon_sc . points ); <nl> xfree ( update -> primary ); <nl> xfree ( update -> secondary ); <nl> xfree ( update -> altsec );
void transport_free ( rdpTransport * transport ) <nl> { <nl> if ( transport ) <nl> { <nl> + if ( transport -> async ) <nl> + { <nl> + assert (! transport -> thread ); <nl> + assert (! transport -> stopEvent ); <nl> + } <nl> + <nl> if ( transport -> ReceiveBuffer ) <nl> Stream_Release ( transport -> ReceiveBuffer ); <nl> 
void tsmf_playback_ack ( IWTSVirtualChannelCallback * pChannelCallback , <nl> if (! callback || ! callback -> channel || ! callback -> channel -> Write ) <nl> { <nl> WLog_ERR ( TAG , " callback =% p , channel =% p , write =% p ", callback , <nl> - callback -> channel , callback -> channel -> Write ); <nl> + callback ? callback -> channel : NULL , <nl> + ( callback && callback -> channel ) ? callback -> channel -> Write : NULL ); <nl> } <nl> else <nl> {
BOOL drive_file_set_information ( DRIVE_FILE * file , UINT32 FsInformationClass , UIN <nl> if (! fullpath ) <nl> { <nl> WLog_ERR ( TAG , " drive_file_combine_fullpath failed !"); <nl> + free ( s ); <nl> return FALSE ; <nl> } <nl> free ( s );
wStream * transport_send_stream_init ( rdpTransport * transport , int size ) <nl>  <nl> void transport_attach ( rdpTransport * transport , int sockfd ) <nl> { <nl> + if (! transport -> TcpIn ) <nl> + transport -> TcpIn = freerdp_tcp_new ( transport -> settings ); <nl> + <nl> freerdp_tcp_attach ( transport -> TcpIn , sockfd ); <nl> + <nl> transport -> SplitInputOutput = FALSE ; <nl> transport -> frontBio = transport -> TcpIn -> bufferedBio ; <nl> }
boolean security_establish_keys ( uint8 * client_random , rdpRdp * rdp ) <nl>  <nl> memcpy ( rdp -> decrypt_update_key , rdp -> decrypt_key , 16 ); <nl> memcpy ( rdp -> encrypt_update_key , rdp -> encrypt_key , 16 ); <nl> + rdp -> decrypt_use_count = 0 ; <nl> + rdp -> decrypt_checksum_use_count = 0 ; <nl> + rdp -> encrypt_use_count = 0 ; <nl> + rdp -> encrypt_checksum_use_count = 0 ; <nl>  <nl> return true ; <nl> }
static BOOL autodetect_recv_bandwidth_measure_results ( rdpRdp * rdp , wStream * s , <nl> return FALSE ; <nl>  <nl> WLog_VRB ( AUTODETECT_TAG , " received Bandwidth Measure Results PDU "); <nl> + if ( Stream_GetRemainingLength ( s ) < 8 ) <nl> + return - 1 ; <nl> Stream_Read_UINT32 ( s , rdp -> autodetect -> bandwidthMeasureTimeDelta ); /* timeDelta ( 4 bytes ) */ <nl> Stream_Read_UINT32 ( s , rdp -> autodetect -> bandwidthMeasureByteCount ); /* byteCount ( 4 bytes ) */ <nl> 
BOOL tls_accept ( rdpTls * tls , const char * cert_file , const char * privatekey_file ) <nl>  <nl> BOOL tls_disconnect ( rdpTls * tls ) <nl> { <nl> + if (! tls ) <nl> + return FALSE ; <nl> + <nl> if ( tls -> ssl ) <nl> SSL_shutdown ( tls -> ssl ); <nl> 
int freerdp_parse_args ( rdpSettings * settings , int argc , char ** argv , <nl> } <nl> else if ( strcmp ("-- plugin ", argv [ index ]) == 0 ) <nl> { <nl> - t = index ; <nl> index ++; <nl> + t = index ; <nl> if ( index == argc ) <nl> { <nl> printf (" missing plugin name \ n ");
static BOOL update_read_bitmap_data ( rdpUpdate * update , wStream * s , BITMAP_DATA * <nl> { <nl> if (!( bitmapData -> flags & NO_BITMAP_COMPRESSION_HDR )) <nl> { <nl> + if ( Stream_GetRemainingLength ( s ) < 8 ) <nl> + return FALSE ; <nl> + <nl> Stream_Read_UINT16 ( s , <nl> bitmapData -> cbCompFirstRowSize ); /* cbCompFirstRowSize ( 2 bytes ) */ <nl> Stream_Read_UINT16 ( s ,
SECURITY_STATUS ntlm_read_NegotiateMessage ( NTLM_CONTEXT * context , PSecBuffer buf <nl> return SEC_E_INVALID_TOKEN ; <nl> } <nl>  <nl> + if ( Stream_GetRemainingLength ( s ) < 4 ) <nl> + { <nl> + Stream_Free ( s , FALSE ); <nl> + return SEC_E_INVALID_TOKEN ; <nl> + } <nl> Stream_Read_UINT32 ( s , message -> NegotiateFlags ); /* NegotiateFlags ( 4 bytes ) */ <nl>  <nl> if (!(( message -> NegotiateFlags & NTLMSSP_REQUEST_TARGET ) &&
BOOL security_fips_decrypt ( BYTE * data , size_t length , rdpRdp * rdp ) <nl> { <nl> size_t olen ; <nl>  <nl> + if (! rdp || ! rdp -> fips_decrypt ) <nl> + return FALSE ; <nl> + <nl> if (! winpr_Cipher_Update ( rdp -> fips_decrypt , data , length , data , & olen )) <nl> return FALSE ; <nl> 
static void rdpsnd_process_receive ( rdpSvcPlugin * plugin , STREAM * data_in ) <nl> if ( rdpsnd -> expectingWave ) <nl> { <nl> rdpsnd_process_message_wave ( rdpsnd , data_in ); <nl> + stream_free ( data_in ); <nl> return ; <nl> } <nl>  <nl> static void rdpsnd_process_receive ( rdpSvcPlugin * plugin , STREAM * data_in ) <nl> DEBUG_WARN (" unknown msgType % d ", msgType ); <nl> break ; <nl> } <nl> + <nl> + stream_free ( data_in ); <nl> } <nl>  <nl> static void rdpsnd_register_device_plugin ( rdpsndPlugin * rdpsnd , rdpsndDevicePlugin * device )
wReference * ReferenceTable_GetFreeEntry ( wReferenceTable * referenceTable ) <nl> new_size = referenceTable -> size * 2 ; <nl> new_ref = ( wReference *) realloc ( referenceTable -> array , <nl> sizeof ( wReference ) * new_size ); <nl> + if (! new_ref ) <nl> + return NULL ; <nl>  <nl> referenceTable -> size = new_size ; <nl> referenceTable -> array = new_ref ;
static BOOL rdp_print_input_capability_set ( wStream * s , UINT16 length ) <nl> static BOOL rdp_read_font_capability_set ( wStream * s , UINT16 length , rdpSettings * settings ) <nl> { <nl> WINPR_UNUSED ( settings ); <nl> - if ( length > 4 ) <nl> + if ( length > 5 ) <nl> Stream_Seek_UINT16 ( s ); /* fontSupportFlags ( 2 bytes ) */ <nl>  <nl> - if ( length > 6 ) <nl> + if ( length > 7 ) <nl> Stream_Seek_UINT16 ( s ); /* pad2Octets ( 2 bytes ) */ <nl>  <nl> return TRUE ;
BOOL ValidFileNameComponent ( LPCWSTR lpFileName ) <nl> { <nl> LPCWSTR c = NULL ; <nl>  <nl> + if (! lpFileName ) <nl> + return FALSE ; <nl> + <nl> /* CON */ <nl> if (( lpFileName [ 0 ] != L '\ 0 ' && ( lpFileName [ 0 ] == L ' C ' || lpFileName [ 0 ] == L ' c ')) && <nl> ( lpFileName [ 1 ] != L '\ 0 ' && ( lpFileName [ 1 ] == L ' O ' || lpFileName [ 1 ] == L ' o ')) &&
struct rdp_freerdp <nl> Must be set to NULL if not needed . */ <nl> UINT64 paddingC [ 47 - 35 ]; /* 35 */ <nl>  <nl> - ALIGN64 ConnectionCallbackState ; /* 48 */ <nl> + ALIGN64 UINT ConnectionCallbackState ; /* 47 */ <nl>  <nl> ALIGN64 pPreConnect PreConnect ; /**< ( offset 48 ) <nl> Callback for pre - connect operations .
rdpSettings * freerdp_settings_new ( DWORD flags ) <nl> settings -> NoBitmapCompressionHeader = TRUE ; <nl> settings -> RefreshRect = TRUE ; <nl> settings -> SuppressOutput = TRUE ; <nl> - settings -> GlyphSupportLevel = GLYPH_SUPPORT_FULL ; <nl> + settings -> GlyphSupportLevel = GLYPH_SUPPORT_NONE ; <nl> settings -> GlyphCache = malloc ( sizeof ( GLYPH_CACHE_DEFINITION ) * 10 ); <nl>  <nl> if (! settings -> GlyphCache )
static void rfx_compose_message_frame_begin ( RFX_CONTEXT * context , STREAM * data_o <nl> stream_write_uint8 ( data_out , 0 ); /* CodecChannelT . channelId */ <nl> stream_write_uint32 ( data_out , context -> frame_idx ); /* frameIdx */ <nl> stream_write_uint16 ( data_out , 1 ); /* numRegions */ <nl> + <nl> + context -> frame_idx ++; <nl> } <nl>  <nl> static void rfx_compose_message_region ( RFX_CONTEXT * context , STREAM * data_out ,
int transport_write ( rdpTransport * transport , wStream * s ) <nl> int status = - 1 ; <nl> int writtenlength = 0 ; <nl>  <nl> + if (! transport ) <nl> + return - 1 ; <nl> + <nl> + if (! transport -> frontBio ) <nl> + { <nl> + transport -> layer = TRANSPORT_LAYER_CLOSED ; <nl> + return - 1 ; <nl> + } <nl> + <nl> EnterCriticalSection (&( transport -> WriteLock )); <nl>  <nl> length = Stream_GetPosition ( s );
BOOL rdp_server_establish_keys ( rdpRdp * rdp , wStream * s ) <nl> if ( rand_len != key_len + 8 ) <nl> { <nl> WLog_ERR ( TAG , " invalid encrypted client random length "); <nl> + free ( client_random ); <nl> goto end ; <nl> } <nl>  <nl> crypt_client_random = calloc ( 1 , rand_len ); <nl> if (! crypt_client_random ) <nl> + { <nl> + free ( client_random ); <nl> goto end ; <nl> + } <nl> + <nl> Stream_Read ( s , crypt_client_random , rand_len ); <nl>  <nl> mod = rdp -> settings -> RdpServerRsaKey -> Modulus ;
public : <nl> return CONTINUE ; <nl> } <nl>  <nl> + void OnNick ( const CNick & Nick , const CString & sNewNick , const std :: vector < CChan *>& vChans ) override { <nl> + for ( CChan * pChan : vChans ) { <nl> + Message (* pChan ); <nl> + } <nl> + } <nl> + <nl> void ShowCommand ( const CString & sLine ) { <nl> PutModule (" Current limit is " + CString ( m_iThresholdMsgs ) + " lines " <nl> " in " + CString ( m_iThresholdSecs ) + " secs .");
gdImageScaleTwoPass ( const gdImagePtr src , const unsigned int new_width , <nl> }/* if */ <nl>  <nl> if ( src != tmp_im ) { <nl> - gdFree ( tmp_im ); <nl> + gdImageDestroy ( tmp_im ); <nl> }/* if */ <nl>  <nl> return dst ;
again : <nl> return false ; <nl> } <nl> if ( x < 0 ) { <nl> - log () << " MessagingPort recv () error " << errno << ' ' << farEnd . toString ()<< endl ; <nl> + log () << " MessagingPort recv () error \"" << strerror ( errno ) << "\" (" << errno << ") " << farEnd . toString ()<< endl ; <nl> m . reset (); <nl> return false ; <nl> }
namespace mongo { <nl> class BalancingWindowUnitTest : public UnitTest { <nl> public : <nl> void run () { <nl> + <nl> + if ( ! cmdLine . isMongos () ) <nl> + return ; <nl> + <nl> // T0 < T1 < now < T2 < T3 and Error <nl> const string T0 = " 9 : 00 "; <nl> const string T1 = " 11 : 00 ";
namespace mongo { <nl>  <nl> virtual void startRequest () {} <nl>  <nl> + virtual void onAddAuthorizedPrincipal ( Principal *) {} <nl> + <nl> + virtual void onLogoutDatabase ( const std :: string & dbname ) {} <nl> + <nl> private : <nl> bool _returnValue ; <nl> };
namespace mongo { <nl> kill_wrapper ( pid , signal , port ); <nl>  <nl> int i = 0 ; <nl> - for ( ; i < 65 ; ++ i ) { <nl> - if ( i == 5 ) { <nl> + for ( ; i < 130 ; ++ i ) { <nl> + if ( i == 30 ) { <nl> char now [ 64 ]; <nl> time_t_to_String ( time ( 0 ), now ); <nl> now [ 20 ] = 0 ;
namespace mongo { <nl> ) , result ) ); <nl> conn . done (); <nl>  <nl> - return result . getObjectField ( " median " ); <nl> + return result . getObjectField ( " median " ). getOwned (); <nl> } <nl>  <nl> Shard * Shard :: split (){
namespace mongo { <nl> void DBConfig :: enableSharding () { <nl> if ( _shardingEnabled ) <nl> return ; <nl> + <nl> + assert ( _name != " config " ); <nl> + <nl> scoped_lock lk ( _lock ); <nl> _shardingEnabled = true ; <nl> _save ();
namespace cling { <nl> // Compile the wrapper code . <nl> // <nl> const llvm :: GlobalValue * GV = 0 ; <nl> + if (! getCodeGenerator ()) <nl> + return 0 ; <nl> + <nl> if ( ifUnique ) <nl> GV = getCodeGenerator ()-> GetModule ()-> getNamedValue ( name ); <nl> 
namespace cling { <nl> Interpreter :: Interpreter ( int argc , const char * const * argv , <nl> const char * llvmdir /*= 0 */) : <nl> m_UniqueCounter ( 0 ), m_PrintDebug ( false ), <nl> - m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ) { <nl> + m_DynamicLookupEnabled ( false ), m_RawInputEnabled ( false ), <nl> + m_LastCustomPragmaDiagPopPoint (){ <nl>  <nl> m_LLVMContext . reset ( new llvm :: LLVMContext ); <nl> std :: vector < unsigned > LeftoverArgsIdx ;
namespace cling { <nl> ///\ param [ in ] file1 - A file to diff <nl> ///\ param [ in ] file2 - A file to diff <nl> ///\ param [ in ] differences - The differences if any between file1 and file2 <nl> + ///\ param [ in ] ignores - A list of differences to ignore . <nl> ///\ returns true if there is difference in the contents . <nl> /// <nl> bool differentContent ( const std :: string & file1 , const std :: string & file2 ,
namespace cling { <nl> if (! CI -> hasTarget ()) { <nl> return 0 ; <nl> } <nl> - CI -> getTarget (). setForcedLangOptions ( CI -> getLangOpts ()); <nl> + CI -> getTarget (). adjust ( CI -> getLangOpts ()); <nl> SetClingTargetLangOpts ( CI -> getLangOpts (), CI -> getTarget ()); <nl> if ( CI -> getTarget (). getTriple (). getOS () == llvm :: Triple :: Cygwin ) { <nl> // clang " forgets " the basic arch part needed by winnt . h :
XrdSysMutex XrdOucAppleBonjour :: SingletonMutex ; <nl>  <nl> XrdOucAppleBonjour :: XrdOucAppleBonjour () <nl> { <nl> - putenv (" AVAHI_COMPAT_NOWARN = 1 "); <nl> + char * env = new char [ 22 ]; <nl> + strcpy ( env , " AVAHI_COMPAT_NOWARN = 1 "); <nl> + putenv ( env ); <nl> } <nl>  <nl> XrdOucAppleBonjour ::~ XrdOucAppleBonjour () { }
namespace utils { <nl> newBody . insert ( newBody . begin () + indexOfLastExpr , DRE ); <nl>  <nl> // Attach the new body ( note : it does dealloc / alloc of all nodes ) <nl> - CS -> setStmts ( S -> getASTContext (), newBody . data (), newBody . size ()); <nl> + CS -> setStmts ( S -> getASTContext (), & newBody . front (), newBody . size ()); <nl> if ( FoundAt ) <nl> * FoundAt = indexOfLastExpr ; <nl> return DRE ;
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> } <nl>  <nl> /* Generic DSP setup */ <nl> + memset (& dsp , 0 , sizeof ( dsp )); <nl> ff_dsputil_init (& dsp , avctx ); <nl> ff_set_cmp (& dsp , dsp . ildct_cmp , avctx -> ildct_cmp ); <nl> s -> get_pixels = dsp . get_pixels ;
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> uint32_t u ; <nl> } value ; <nl>  <nl> - int sign ; <nl> + unsigned int sign ; <nl> int exp = s -> float_max_exp ; <nl>  <nl> if ( s -> got_extra_bits ) {
int av_get_audio_frame_duration ( AVCodecContext * avctx , int frame_bytes ) <nl> return frame_bytes * 8 / bps ; <nl> } <nl>  <nl> - if ( ch > 0 ) { <nl> + if ( ch > 0 && ch < INT_MAX / 16 ) { <nl> /* calc from frame_bytes and channels */ <nl> switch ( id ) { <nl> case AV_CODEC_ID_ADPCM_AFC :
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * insamplesref ) <nl> AVFilterBufferRef * outsamplesref = ff_get_audio_buffer ( outlink , AV_PERM_WRITE , n_out ); <nl> int ret ; <nl>  <nl> + if (! outsamplesref ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> avfilter_copy_buffer_ref_props ( outsamplesref , insamplesref ); <nl> outsamplesref -> format = outlink -> format ; <nl> outsamplesref -> audio -> channel_layout = outlink -> channel_layout ;
static const int prof_h264_high [] = { FF_PROFILE_H264_CONSTRAINED_BASELINE , <nl> FF_PROFILE_UNKNOWN }; <nl> static const int prof_hevc_main [] = { FF_PROFILE_HEVC_MAIN , <nl> FF_PROFILE_UNKNOWN }; <nl> - static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN , <nl> - FF_PROFILE_HEVC_MAIN_10 , <nl> + static const int prof_hevc_main10 [] = { FF_PROFILE_HEVC_MAIN_10 , <nl> FF_PROFILE_UNKNOWN }; <nl>  <nl> static const dxva_mode dxva_modes [] = {
static int mpeg_decode_slice ( Mpeg1Context * s1 , int mb_y , <nl> break ; <nl> } <nl> } <nl> + if ( s -> mb_x >= ( unsigned ) s -> mb_width ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " initial skip overflow \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> s -> resync_mb_x = s -> mb_x ; <nl> s -> resync_mb_y = s -> mb_y = mb_y ;
decode_intra_mb : <nl>  <nl> // We assume these blocks are very rare so we do not optimize it . <nl> h -> intra_pcm_ptr = align_get_bits (& h -> gb ); <nl> + if ( get_bits_left (& h -> gb ) < mb_size ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , " Not enough data for an intra PCM block .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> skip_bits_long (& h -> gb , mb_size ); <nl>  <nl> // In deblocking , the quantizer is 0
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
static int cdg_decode_frame ( AVCodecContext * avctx , <nl> int buf_size = avpkt -> size ; <nl> int ret ; <nl> uint8_t command , inst ; <nl> - uint8_t cdg_data [ CDG_DATA_SIZE ]; <nl> + uint8_t cdg_data [ CDG_DATA_SIZE ] = { 0 }; <nl> AVFrame * frame = data ; <nl> CDGraphicsContext * cc = avctx -> priv_data ; <nl> 
static int vlc_decode_block ( MimicContext * ctx , int num_coeffs , int qscale ) <nl>  <nl> coeff = vlcdec_lookup [ num_bits ][ value ]; <nl> if ( pos < 3 ) <nl> - coeff <<= 4 ; <nl> + coeff *= 16 ; <nl> else /* TODO Use >> 10 instead of / 1001 */ <nl> coeff = ( coeff * qscale ) / 1001 ; <nl> 
static void video_audio_display ( VideoState * s ) <nl> { <nl> int i , i_start , x , y1 , y , ys , delay , n , nb_display_channels ; <nl> int ch , channels , h , h2 , bgcolor , fgcolor ; <nl> - int16_t time_diff ; <nl> + int64_t time_diff ; <nl> int rdft_bits , nb_freq ; <nl>  <nl> for ( rdft_bits = 1 ; ( 1 << rdft_bits ) < 2 * s -> height ; rdft_bits ++)
static void compute_pkt_fields ( AVFormatContext * s , AVStream * st , <nl> if ( pkt -> dts != AV_NOPTS_VALUE && <nl> pkt -> pts == AV_NOPTS_VALUE && <nl> st -> last_IP_duration > 0 && <nl> - ( st -> cur_dts - next_dts ) <= 1 && <nl> + (( uint64_t ) st -> cur_dts - ( uint64_t ) next_dts + 1 ) <= 2 && <nl> next_dts != next_pts && <nl> next_pts != AV_NOPTS_VALUE ) <nl> pkt -> pts = next_dts ;
int ff_vorbis_len2vlc ( uint8_t * bits , uint32_t * codes , unsigned num ) <nl> exit_at_level [ i ] = 0 ; <nl> // construct code ( append 0s to end ) and introduce new exits <nl> for ( j = i + 1 ; j <= bits [ p ]; ++ j ) <nl> - exit_at_level [ j ] = code + ( 1 << ( j - 1 )); <nl> + exit_at_level [ j ] = code + ( 1u << ( j - 1 )); <nl> codes [ p ] = code ; <nl> } <nl> 
static void dca_exss_parse_header ( DCAContext * s ) <nl> } <nl> } <nl>  <nl> + av_assert0 ( num_assets > 0 ); // silence a warning <nl> + <nl> for ( i = 0 ; i < num_assets ; i ++) <nl> asset_size [ i ] = get_bits_long (& s -> gb , 16 + 4 * blownup ); <nl> 
int ff_frame_thread_init ( AVCodecContext * avctx ) <nl> p -> frame = av_frame_alloc (); <nl> if (! p -> frame ) { <nl> err = AVERROR ( ENOMEM ); <nl> + av_freep (& copy ); <nl> goto error ; <nl> } <nl> 
static int yuv4_read_header ( AVFormatContext * s ) <nl> enum AVPixelFormat pix_fmt = AV_PIX_FMT_NONE , alt_pix_fmt = AV_PIX_FMT_NONE ; <nl> enum AVChromaLocation chroma_sample_location = AVCHROMA_LOC_UNSPECIFIED ; <nl> AVStream * st ; <nl> - enum AVFieldOrder field_order ; <nl> + enum AVFieldOrder field_order = AV_FIELD_UNKNOWN ; <nl>  <nl> for ( i = 0 ; i < MAX_YUV4_HEADER ; i ++) { <nl> header [ i ] = avio_r8 ( pb );
av_cold int ff_MPV_encode_init ( AVCodecContext * avctx ) <nl> if ( avctx -> max_b_frames > MAX_B_FRAMES ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Too many B - frames requested , maximum " <nl> " is % d .\ n ", MAX_B_FRAMES ); <nl> + avctx -> max_b_frames = MAX_B_FRAMES ; <nl> } <nl> s -> max_b_frames = avctx -> max_b_frames ; <nl> s -> codec_id = avctx -> codec -> id ;
static int read_header ( AVFormatContext * s , <nl> jvf -> audio_size = avio_rl32 ( pb ); <nl> jvf -> video_size = avio_rl32 ( pb ); <nl> jvf -> palette_size = avio_r8 ( pb ) ? 768 : 0 ; <nl> + jvf -> video_size = FFMIN ( FFMAX ( jvf -> video_size , 0 ), <nl> + INT_MAX - JV_PREAMBLE_SIZE - jvf -> palette_size ); <nl> if ( avio_r8 ( pb )) <nl> av_log ( s , AV_LOG_WARNING , " unsupported audio codec \ n "); <nl> jvf -> video_type = avio_r8 ( pb );
int ff_rate_control_init ( MpegEncContext * s ) <nl> rcc -> pass1_rc_eq_output_sum = 0 . 001 ; <nl> rcc -> pass1_wanted_bits = 0 . 001 ; <nl>  <nl> + if ( s -> avctx -> qblur > 1 . 0 ){ <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " qblur too large \ n "); <nl> + return - 1 ; <nl> + } <nl> /* init stuff with the user specified complexity */ <nl> if ( s -> avctx -> rc_initial_cplx ){ <nl> for ( i = 0 ; i < 60 * 30 ; i ++){
static int r3d_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> case MKTAG (' R ',' E ',' D ',' A '): <nl> if (! r3d -> audio_channels ) <nl> return - 1 ; <nl> - if ( s -> streams [ 1 ]-> discard == AVDISCARD_ALL ) <nl> + if ( s -> nb_streams >= 2 && s -> streams [ 1 ]-> discard == AVDISCARD_ALL ) <nl> goto skip ; <nl> if (!( err = r3d_read_reda ( s , pkt , & atom ))) <nl> return 0 ;
static inline int get_duration ( AVIStream * ast , int len ) <nl> static int get_riff ( AVFormatContext * s , AVIOContext * pb ) <nl> { <nl> AVIContext * avi = s -> priv_data ; <nl> - char header [ 8 ]; <nl> + char header [ 8 ] = { 0 }; <nl> int i ; <nl>  <nl> /* check RIFF header */
static int parse_video_var ( AVFormatContext * avctx , AVStream * st , const char * nam <nl> st -> nb_frames = st -> duration = var_read_int ( pb , size ); <nl> } else if (! strcmp ( name , " COMPRESSION ")) { <nl> char * str = var_read_string ( pb , size ); <nl> + if (! str ) <nl> + return AVERROR_INVALIDDATA ; <nl> if (! strcmp ( str , " 1 ")) { <nl> st -> codec -> codec_id = AV_CODEC_ID_MVC1 ; <nl> } else if (! strcmp ( str , " 2 ")) {
extern void idct_add_altivec ( uint8_t * dest , int line_size , int16_t * block ); <nl>  <nl> void MPV_common_init_altivec ( MpegEncContext * s ) <nl> { <nl> + if ( mm_flags & MM_ALTIVEC == 0 ) return ; <nl> + <nl> if ( s -> avctx -> lowres == 0 ) <nl> { <nl> if (( s -> avctx -> idct_algo == FF_IDCT_AUTO ) ||
void av_opt_freep_ranges ( AVOptionRanges ** rangesp ) <nl> int i ; <nl> AVOptionRanges * ranges = * rangesp ; <nl>  <nl> + if (! ranges ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < ranges -> nb_ranges * ranges -> nb_components ; i ++) { <nl> AVOptionRange * range = ranges -> range [ i ]; <nl> av_freep (& range -> str );
static int read_header ( AVFormatContext * s ) <nl>  <nl> b -> data_start = avio_tell ( s -> pb ); <nl>  <nl> - if (( major != 1 || minor ) && ! bfstm ) <nl> + if (! bfstm && ( major != 1 || minor )) <nl> avpriv_request_sample ( s , " Version % d .% d ", major , minor ); <nl>  <nl> return 0 ;
int av_read_frame ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> ret = add_to_pktbuf (& s -> internal -> packet_buffer , pkt , <nl> & s -> internal -> packet_buffer_end , 1 ); <nl> + av_packet_unref ( pkt ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> }
# include " cabac_functions . h " <nl> # include " hevc . h " <nl>  <nl> -# define CABAC_MAX_BIN 100 <nl> +# define CABAC_MAX_BIN 31 <nl>  <nl> /** <nl> * number of bin by SyntaxElement .
av_cold void ff_fft_fixed_init_arm ( FFTContext * s ) <nl> s -> fft_calc = ff_fft_fixed_calc_neon ; <nl>  <nl> # if CONFIG_MDCT <nl> - if (! s -> inverse && s -> mdct_bits >= 5 ) { <nl> + if (! s -> inverse && s -> nbits >= 3 ) { <nl> s -> mdct_permutation = FF_MDCT_PERM_INTERLEAVE ; <nl> s -> mdct_calc = ff_mdct_fixed_calc_neon ; <nl> s -> mdct_calcw = ff_mdct_fixed_calcw_neon ;
void ff_frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> av_freep (& p -> avctx -> slice_offset ); <nl> } <nl>  <nl> + av_buffer_unref (& p -> avctx -> hw_frames_ctx ); <nl> + <nl> av_freep (& p -> avctx -> internal ); <nl> av_freep (& p -> avctx ); <nl> }
static int decode_frame ( NUTContext * nut , AVPacket * pkt , int frame_code ) <nl> if ( ret != size ) { <nl> if ( ret < 0 ) <nl> return ret ; <nl> - av_shrink_packet ( pkt , nut -> header_len [ header_idx ] + size ); <nl> } <nl> + av_shrink_packet ( pkt , nut -> header_len [ header_idx ] + ret ); <nl>  <nl> pkt -> stream_index = stream_id ; <nl> if ( stc -> last_flags & FLAG_KEY )
static int tcp_write_packet ( AVFormatContext * s , RTSPStream * rtsp_st ) <nl> interleave_header [ 0 ] = '$'; <nl> interleave_header [ 1 ] = id ; <nl> AV_WB16 ( interleave_header + 2 , packet_len ); <nl> - url_write ( rt -> rtsp_hd , interleaved_packet , 4 + packet_len ); <nl> + url_write ( rt -> rtsp_hd_out , interleaved_packet , 4 + packet_len ); <nl> ptr += packet_len ; <nl> size -= packet_len ; <nl> }
static int vorbis_packet ( AVFormatContext * s , int idx ) <nl> s -> streams [ idx ]-> start_time = os -> lastpts + first_duration ; <nl> if ( s -> streams [ idx ]-> duration ) <nl> s -> streams [ idx ]-> duration -= s -> streams [ idx ]-> start_time ; <nl> - s -> streams [ idx ]-> cur_dts = AV_NOPTS_VALUE ; <nl> priv -> final_pts = AV_NOPTS_VALUE ; <nl> avpriv_vorbis_parse_reset (& priv -> vp ); <nl> }
void av_opt_freep_ranges ( AVOptionRanges ** rangesp ) <nl>  <nl> for ( i = 0 ; i < ranges -> nb_ranges * ranges -> nb_components ; i ++) { <nl> AVOptionRange * range = ranges -> range [ i ]; <nl> - av_freep (& range -> str ); <nl> - av_freep (& ranges -> range [ i ]); <nl> + if ( range ) { <nl> + av_freep (& range -> str ); <nl> + av_freep (& ranges -> range [ i ]); <nl> + } <nl> } <nl> av_freep (& ranges -> range ); <nl> av_freep ( rangesp );
static int parse_read_interval ( const char * interval_spec , <nl> } <nl> interval -> end = lli ; <nl> } else { <nl> + interval -> duration_frames = 0 ; <nl> ret = av_parse_time (& us , p , 1 ); <nl> if ( ret < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid interval end / duration specification '% s '\ n ", p );
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> LclDecContext * const c = avctx -> priv_data ; <nl> unsigned int pixel_ptr ; <nl> int row , col ; <nl> - unsigned char * encoded , * outptr ; <nl> + unsigned char * encoded = avpkt -> data , * outptr ; <nl> uint8_t * y_out , * u_out , * v_out ; <nl> unsigned int width = avctx -> width ; // Real image width <nl> unsigned int height = avctx -> height ; // Real image height
static void copy_bits ( PutBitContext * pb , <nl> rmn_bits = rmn_bytes = get_bits_left ( gb ); <nl> if ( rmn_bits < nbits ) <nl> return ; <nl> + if ( nbits > pb -> size_in_bits - put_bits_count ( pb )) <nl> + return ; <nl> rmn_bits &= 7 ; rmn_bytes >>= 3 ; <nl> if (( rmn_bits = FFMIN ( rmn_bits , nbits )) > 0 ) <nl> put_bits ( pb , rmn_bits , get_bits ( gb , rmn_bits ));
static int decode_iccp_chunk ( PNGDecContext * s , int length , AVFrame * f ) <nl> return ret ; <nl>  <nl> av_bprint_finalize (& bp , ( char **)& data ); <nl> + if (! data ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> sd = av_frame_new_side_data ( f , AV_FRAME_DATA_ICC_PROFILE , bp . len ); <nl> if (! sd ) {
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ; <nl>  <nl> - s -> filter_slice_edges = av_malloc ( ctb_count ); <nl> + s -> filter_slice_edges = av_mallocz ( ctb_count ); <nl> s -> tab_slice_address = av_malloc_array ( pic_size_in_ctb , <nl> sizeof (* s -> tab_slice_address )); <nl> s -> qp_y_tab = av_malloc_array ( pic_size_in_ctb ,
typedef struct WmallDecodeCtx { <nl> int8_t mclms_scaling ; <nl> int16_t mclms_coeffs [ 128 ]; <nl> int16_t mclms_coeffs_cur [ 4 ]; <nl> - int16_t mclms_prevvalues [ 64 ]; <nl> - int16_t mclms_updates [ 64 ]; <nl> + int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> + int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ; <nl>  <nl> int movave_scaling ;
static int decode_frame ( AVCodecContext * avctx , void * data , <nl> buf = av_realloc ( s -> packet_buffer , avpkt -> size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! buf ) <nl> return AVERROR ( ENOMEM ); <nl> + memset ( buf + avpkt -> size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> s -> packet_buffer = buf ; <nl> memcpy ( s -> packet_buffer , avpkt -> data , avpkt -> size ); <nl> if (( ret = init_get_bits8 ( gb , s -> packet_buffer , avpkt -> size )) < 0 )
header_fail : <nl> c -> height = 0 ; <nl> c -> tiles_x = <nl> c -> tiles_y = 0 ; <nl> + c -> tile_width = <nl> + c -> tile_height = 0 ; <nl> return ret ; <nl> } <nl> 
int ff_lpc_calc_coefs ( DSPContext * s , <nl> ref [ i ] = fabs ( lpc [ i ][ i ]); <nl> } else { <nl> LLSModel m [ 2 ]; <nl> - double var [ MAX_LPC_ORDER + 1 ], weight ; <nl> + double var [ MAX_LPC_ORDER + 1 ], av_uninit ( weight ); <nl>  <nl> for ( pass = 0 ; pass < use_lpc - 1 ; pass ++){ <nl> av_init_lls (& m [ pass & 1 ], max_order );
void checkasm_stack_clobber ( uint64_t clobber , ...); <nl> }\ <nl> } while ( 0 ) <nl> # else <nl> -# define bench_new (...) <nl> +# define bench_new (...) while ( 0 ) <nl> # endif <nl>  <nl> # endif
static int vqf_read_header ( AVFormatContext * s ) <nl>  <nl> header_size -= len ; <nl>  <nl> - } while ( header_size >= 0 ); <nl> + } while ( header_size >= 0 && ! url_feof ( s -> pb )); <nl>  <nl> switch ( rate_flag ) { <nl> case - 1 :
static void free_tables ( H264Context * h ){ <nl> av_freep (& h -> mb2b_xy ); <nl> av_freep (& h -> mb2b8_xy ); <nl>  <nl> - for ( i = 0 ; i < h -> s . avctx -> thread_count ; i ++) { <nl> + for ( i = 0 ; i < MAX_THREADS ; i ++) { <nl> hx = h -> thread_context [ i ]; <nl> if (! hx ) continue ; <nl> av_freep (& hx -> top_borders [ 1 ]);
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl>  <nl> h -> b_stride = 4 * s -> mb_width ; <nl>  <nl> - ff_h264_alloc_tables ( h ); <nl> + if ( ff_h264_alloc_tables ( h ) < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " svq3 memory allocation failed \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> return 0 ;
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> end = ptr + FFMIN ( 2 + text_length , avpkt -> size ); <nl> ptr += 2 ; <nl>  <nl> + mov_text_cleanup ( m ); <nl> + <nl> tsmb_size = 0 ; <nl> m -> tracksize = 2 + text_length ; <nl> m -> style_entries = 0 ;
void ff_h264_direct_ref_list_init ( H264Context * const h ){ <nl> h -> col_fieldoff = 2 * h -> ref_list [ 1 ][ 0 ]. f . reference - 3 ; <nl> } <nl>  <nl> - if ( cur -> f . pict_type != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> + if ( h -> slice_type_nos != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> return ; <nl>  <nl> for ( list = 0 ; list < 2 ; list ++){
static uint32_t softfloat_mul ( uint32_t x , uint64_t mantissa ) <nl>  <nl> static uint8_t lag_calc_zero_run ( int8_t x ) <nl> { <nl> - return ( x << 1 ) ^ ( x >> 7 ); <nl> + return ( x * 2 ) ^ ( x >> 7 ); <nl> } <nl>  <nl> static int lag_decode_prob ( GetBitContext * gb , uint32_t * value )
AVFilterBufferRef * ff_default_get_audio_buffer ( AVFilterLink * link , int perms , <nl> if (! samplesref ) <nl> goto fail ; <nl>  <nl> + samplesref -> audio -> sample_rate = link -> sample_rate ; <nl> + <nl> av_freep (& data ); <nl>  <nl> fail :
static int url_alloc_for_protocol ( URLContext ** puc , struct URLProtocol * up , <nl> av_log ( uc , AV_LOG_ERROR , " Error parsing options string % s \ n ", start ); <nl> av_freep (& uc -> priv_data ); <nl> av_freep (& uc ); <nl> + err = AVERROR ( EINVAL ); <nl> goto fail ; <nl> } <nl> memmove ( start , key + 1 , strlen ( key ));
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> avio_read ( pb , buf , 4 ); <nl> buf [ 4 ] = 0 ; <nl> } else { <nl> + AV_WL32 ( buf , 0 ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */ <nl> ast -> deint_id = AV_RL32 ( buf ); <nl> get_str8 ( pb , buf , sizeof ( buf )); /* desc */
void avsubtitle_free ( AVSubtitle * sub ) <nl>  <nl> static int do_decode ( AVCodecContext * avctx , AVPacket * pkt ) <nl> { <nl> - int got_frame ; <nl> + int got_frame = 0 ; <nl> int ret ; <nl>  <nl> av_assert0 (! avctx -> internal -> buffer_frame -> buf [ 0 ]);
static int mov_read_stts ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> static void mov_update_dts_shift ( MOVStreamContext * sc , int duration ) <nl> { <nl> if ( duration < 0 ) { <nl> + if ( duration == INT_MIN ) { <nl> + av_log ( NULL , AV_LOG_WARNING , " mov_update_dts_shift (): dts_shift set to % d \ n ", INT_MAX ); <nl> + duration ++; <nl> + } <nl> sc -> dts_shift = FFMAX ( sc -> dts_shift , - duration ); <nl> } <nl> }
static av_cold int encode_init ( AVCodecContext * avctx ) <nl> // case PIX_FMT_YUV444P : <nl> // case PIX_FMT_YUV422P : <nl> case PIX_FMT_YUV420P : <nl> - case PIX_FMT_GRAY8 : <nl> +// case PIX_FMT_GRAY8 : <nl> // case PIX_FMT_YUV411P : <nl> // case PIX_FMT_YUV410P : <nl> s -> colorspace_type = 0 ;
static int v410_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 * avctx -> height * avctx -> width ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> pic -> reference = 0 ; <nl>  <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) {
static int build_table ( VLC * vlc , int table_nb_bits , int nb_codes , <nl> VLC_TYPE (* table )[ 2 ]; <nl>  <nl> table_size = 1 << table_nb_bits ; <nl> + if ( table_nb_bits > 30 ) <nl> + return - 1 ; <nl> table_index = alloc_table ( vlc , table_size , flags & INIT_VLC_USE_NEW_STATIC ); <nl> av_dlog ( NULL , " new table index =% d size =% d \ n ", table_index , table_size ); <nl> if ( table_index < 0 )
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf_size = FFMIN ( buf_size , s -> max_framesize - s -> bitstream_size ); <nl> input_buf_size = buf_size ; <nl>  <nl> - if ( s -> bitstream_index + s -> bitstream_size + buf_size > <nl> + if ( s -> bitstream_index + s -> bitstream_size + buf_size + FF_INPUT_BUFFER_PADDING_SIZE > <nl> s -> allocated_bitstream_size ) { <nl> memmove ( s -> bitstream , & s -> bitstream [ s -> bitstream_index ], <nl> s -> bitstream_size );
static av_cold int truemotion1_decode_init ( AVCodecContext * avctx ) <nl> /* there is a vertical predictor for each pixel in a line ; each vertical <nl> * predictor is 0 to start with */ <nl> av_fast_malloc (& s -> vert_pred , & s -> vert_pred_size , s -> avctx -> width * sizeof ( unsigned int )); <nl> - if (! s -> vert_pred ) <nl> + if (! s -> vert_pred ) { <nl> + av_frame_free (& s -> frame ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int handle_packet ( MpegTSContext * ts , const uint8_t * packet ) <nl> return 0 ; <nl>  <nl> pos = avio_tell ( ts -> stream -> pb ); <nl> - av_assert0 ( pos >= TS_PACKET_SIZE ); <nl> - ts -> pos47_full = pos - TS_PACKET_SIZE ; <nl> + if ( pos >= 0 ) { <nl> + av_assert0 ( pos >= TS_PACKET_SIZE ); <nl> + ts -> pos47_full = pos - TS_PACKET_SIZE ; <nl> + } <nl>  <nl> if ( tss -> type == MPEGTS_SECTION ) { <nl> if ( is_start ) {
static void apply_independent_coupling_fixed ( AACContext * ac , <nl> else { <nl> for ( i = 0 ; i < len ; i ++) { <nl> tmp = ( int )((( int64_t ) src [ i ] * c + ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ i ] += tmp << shift ; <nl> + dest [ i ] += tmp * ( 1 << shift ); <nl> } <nl> } <nl> }
static int v410_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 * avctx -> height * avctx -> width ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> pic -> reference = 0 ; <nl>  <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) {
static int config_props ( AVFilterLink * inlink ) <nl> double res ; <nl>  <nl> /* create the parsed expression */ <nl> + av_expr_free ( s -> comp_expr [ comp ]); <nl> + s -> comp_expr [ comp ] = NULL ; <nl> ret = av_expr_parse (& s -> comp_expr [ comp ], s -> comp_expr_str [ comp ], <nl> var_names , funcs1_names , funcs1 , NULL , NULL , 0 , ctx ); <nl> if ( ret < 0 ) {
static int matroska_parse_rm_audio ( MatroskaDemuxContext * matroska , <nl> } <nl> memcpy ( track -> audio . buf + y * w , data , w ); <nl> } else { <nl> - if ( size < sps * w / sps || h <= 0 ) { <nl> + if ( size < sps * w / sps || h <= 0 || w % sps ) { <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Corrupt generic RM - style audio packet size \ n "); <nl> return AVERROR_INVALIDDATA ;
static int matroska_decode_buffer ( uint8_t ** buf , int * buf_size , <nl> int result = 0 ; <nl> int olen ; <nl>  <nl> + if ( pkt_size >= 10000000 ) <nl> + return - 1 ; <nl> + <nl> switch ( encodings [ 0 ]. compression . algo ) { <nl> case MATROSKA_TRACK_ENCODING_COMP_HEADERSTRIP : <nl> return encodings [ 0 ]. compression . settings . size ;
av_cold int ff_ivi_decode_close ( AVCodecContext * avctx ) <nl> if ( ctx -> mb_vlc . cust_tab . table ) <nl> ff_free_vlc (& ctx -> mb_vlc . cust_tab ); <nl>  <nl> + if ( ctx -> blk_vlc . cust_tab . table ) <nl> + ff_free_vlc (& ctx -> blk_vlc . cust_tab ); <nl> + <nl> av_frame_free (& ctx -> p_frame ); <nl>  <nl> return 0 ;
static int read_interval_packets ( WriterContext * w , AVFormatContext * fmt_ctx , <nl> } <nl>  <nl> frame = av_frame_alloc (); <nl> + if (! frame ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> while (! av_read_frame ( fmt_ctx , & pkt )) { <nl> if ( selected_streams [ pkt . stream_index ]) { <nl> AVRational tb = fmt_ctx -> streams [ pkt . stream_index ]-> time_base ;
typedef struct { <nl>  <nl> int bit_index ; <nl>  <nl> - int16_t curtileno ; <nl> + int curtileno ; <nl>  <nl> J2kTile * tile ; <nl> } J2kDecoderContext ;
AVCodec ff_jpegls_encoder = { <nl> AV_PIX_FMT_GRAY8 , AV_PIX_FMT_GRAY16 , <nl> AV_PIX_FMT_NONE <nl> }, <nl> + . caps_internal = FF_CODEC_CAP_INIT_THREADSAFE | <nl> + FF_CODEC_CAP_INIT_CLEANUP , <nl> };
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> av_log ( avctx , AV_LOG_ERROR , " Custom quant matrix encountered !\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( band -> quant_mat > 21 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid quant matrix encountered !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> /* decode block huffman codebook */
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> sub_type = avio_r8 ( s -> pb ); <nl> type = avio_r8 ( s -> pb ); <nl> size = avio_rl16 ( s -> pb ); <nl> + if ( size < 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> avs -> remaining_frame_size -= size ; <nl>  <nl> switch ( type ) {
static void gif_copy_img_rect ( const uint32_t * src , uint32_t * dst , <nl> const uint32_t * src_px , * src_pr , <nl> * src_py = src + y_start , <nl> * dst_py = dst + y_start ; <nl> - const uint32_t * src_pb = src_py + t * linesize ; <nl> + const uint32_t * src_pb = src_py + h * linesize ; <nl> uint32_t * dst_px ; <nl>  <nl> for (; src_py < src_pb ; src_py += linesize , dst_py += linesize ) {
static void update_stream_timings ( AVFormatContext * ic ) <nl> end_time1 = av_rescale_q_rnd ( st -> duration , st -> time_base , <nl> AV_TIME_BASE_Q , <nl> AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX ); <nl> - if ( end_time1 != AV_NOPTS_VALUE ) { <nl> + if ( end_time1 != AV_NOPTS_VALUE && start_time1 <= INT64_MAX - end_time1 ) { <nl> end_time1 += start_time1 ; <nl> end_time = FFMAX ( end_time , end_time1 ); <nl> }
static void destroy_buffers ( SANMVideoContext * ctx ) <nl> ctx -> frm0_size = <nl> ctx -> frm1_size = <nl> ctx -> frm2_size = 0 ; <nl> + init_sizes ( ctx , 0 , 0 ); <nl> } <nl>  <nl> static av_cold int init_buffers ( SANMVideoContext * ctx )
static int parse_video_info ( AVIOContext * pb , AVStream * st ) <nl> st -> codecpar -> codec_id = ff_codec_get_id ( ff_codec_bmp_tags , tag ); <nl> size_bmp = FFMAX ( size_asf , size_bmp ); <nl>  <nl> - if ( size_bmp > BMP_HEADER_SIZE ) { <nl> + if ( size_bmp > BMP_HEADER_SIZE && <nl> + size_bmp < INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) { <nl> int ret ; <nl> st -> codecpar -> extradata_size = size_bmp - BMP_HEADER_SIZE ; <nl> if (!( st -> codecpar -> extradata = av_malloc ( st -> codecpar -> extradata_size +
int ff_h264_frame_start ( H264Context * h ) <nl>  <nl> if (( ret = alloc_picture ( h , pic )) < 0 ) <nl> return ret ; <nl> - if (! h -> sync && ! h -> avctx -> hwaccel ) <nl> + if (! h -> sync && ! h -> avctx -> hwaccel && <nl> + !( h -> avctx -> codec -> capabilities & CODEC_CAP_HWACCEL_VDPAU )) <nl> avpriv_color_frame (& pic -> f , c ); <nl>  <nl> h -> cur_pic_ptr = pic ;
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> switch ( type ) { <nl> case AVS_PALETTE : <nl> + if ( size - 4 > sizeof ( palette )) <nl> + return AVERROR_INVALIDDATA ; <nl> ret = avio_read ( s -> pb , palette , size - 4 ); <nl> if ( ret < size - 4 ) <nl> return AVERROR ( EIO );
static int pcx_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> ptr = p -> data [ 0 ]; <nl> stride = p -> linesize [ 0 ]; <nl>  <nl> - scanline = av_malloc ( bytes_per_scanline ); <nl> + scanline = av_malloc ( bytes_per_scanline + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! scanline ) <nl> return AVERROR ( ENOMEM ); <nl> 
int ff_MPV_lowest_referenced_row ( MpegEncContext * s , int dir ) <nl> int my_max = INT_MIN , my_min = INT_MAX , qpel_shift = ! s -> quarter_sample ; <nl> int my , off , i , mvs ; <nl>  <nl> - if ( s -> picture_structure != PICT_FRAME ) goto unhandled ; <nl> + if ( s -> picture_structure != PICT_FRAME || s -> mcsel ) goto unhandled ; <nl>  <nl> switch ( s -> mv_type ) { <nl> case MV_TYPE_16X16 :
static int decode_subframe_fixed ( FLACContext * s , int32_t * decoded , <nl> int pred_order , int bps ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> - int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl> + unsigned av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ); <nl> + int i ; <nl> int ret ; <nl>  <nl> /* warm up samples */
void uninit_opts ( void ) <nl> av_freep (& avformat_opts -> key ); <nl> av_freep (& avformat_opts ); <nl> # if CONFIG_SWSCALE <nl> - av_freep (& sws_opts ); <nl> + sws_freeContext ( sws_opts ); <nl> + sws_opts = NULL ; <nl> # endif <nl> for ( i = 0 ; i < opt_name_count ; i ++) { <nl> // opt_values are only stored for codec - specific options in which case
static void opt_output_file ( const char * filename ) <nl>  <nl> audio_enc -> bit_rate = audio_bit_rate ; <nl> audio_enc -> sample_rate = audio_sample_rate ; <nl> + audio_enc -> strict_std_compliance = strict ; <nl> /* For audio codecs other than AC3 we limit */ <nl> /* the number of coded channels to stereo */ <nl> if ( audio_channels > 2 && codec_id != CODEC_ID_AC3 ) {
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl>  <nl> av_freep (& h -> rbsp_buffer [ 0 ]); <nl> av_freep (& h -> rbsp_buffer [ 1 ]); <nl> + ff_h264_unref_picture ( h , & h -> last_pic_for_ec ); <nl> memcpy ( h , h1 , offsetof ( H264Context , intra_pcm_ptr )); <nl> memcpy (& h -> cabac , & h1 -> cabac , <nl> sizeof ( H264Context ) - offsetof ( H264Context , cabac ));
static int fraps2_decode_plane ( FrapsContext * s , uint8_t * dst , int stride , int w , <nl> else if ( Uoff ) dst [ i ] += 0x80 ; <nl> } <nl> dst += stride ; <nl> + if ( get_bits_left (& gb ) < 0 ) <nl> + return - 1 ; <nl> } <nl> free_vlc (& vlc ); <nl> return 0 ;
static int write_adaptation_set ( AVFormatContext * s , int as_index ) <nl> ret = write_representation ( s , s -> streams [ as -> streams [ i ]], <nl> representation_id , ! width_in_as , <nl> ! height_in_as , ! sample_rate_in_as ); <nl> - if ( ret ) return ret ; <nl> av_free ( representation_id ); <nl> + if ( ret ) return ret ; <nl> } <nl> avio_printf ( s -> pb , "</ AdaptationSet >\ n "); <nl> return 0 ;
int ff_dca_xll_decode_audio ( DCAContext * s , AVFrame * frame ) <nl> } <nl> for ( i = 0 ; i < chset -> channels ; i ++) { <nl> int param_index = params -> seg_type ? 0 : i ; <nl> - int bits = params -> pancABIT0 [ param_index ]; <nl> int part0 = params -> nSamplPart0 [ param_index ]; <nl> + int bits = part0 ? params -> pancABIT0 [ param_index ] : 0 ; <nl> int * sample_buf = s -> xll_sample_buf + <nl> ( in_channel + i ) * s -> xll_smpl_in_seg ; <nl> 
static double bessel ( double x ){ <nl> lastv = v ; <nl> t *= x * inv [ i ]; <nl> v += t ; <nl> + av_assert2 ( i < 99 ); <nl> } <nl> return v ; <nl> }
av_cold struct FFPsyPreprocessContext * ff_psy_preprocess_init ( AVCodecContext * av <nl> if (! cutoff_coeff && avctx -> codec_id == AV_CODEC_ID_AAC ) <nl> cutoff_coeff = 2 . 0 * AAC_CUTOFF ( avctx ) / avctx -> sample_rate ; <nl>  <nl> - if ( cutoff_coeff ) <nl> + if ( cutoff_coeff && cutoff_coeff < 0 . 98 ) <nl> ctx -> fcoeffs = ff_iir_filter_init_coeffs ( avctx , FF_FILTER_TYPE_BUTTERWORTH , <nl> FF_FILTER_MODE_LOWPASS , FILT_ORDER , <nl> cutoff_coeff , 0 . 0 , 0 . 0 );
int ff_wms_parse_sdp_a_line ( AVFormatContext * s , const char * p ) <nl> { <nl> int ret = 0 ; <nl> if ( av_strstart ( p , " pgmpu : data : application / vnd . ms . wms - hdr . asfv1 ; base64 ,", & p )) { <nl> - AVIOContext pb ; <nl> + AVIOContext pb = { 0 }; <nl> RTSPState * rt = s -> priv_data ; <nl> AVDictionary * opts = NULL ; <nl> int len = strlen ( p ) * 6 / 8 ;
redo_frame : <nl> || !( height >>( s -> chroma_v_shift + s -> spatial_decomposition_count ))) <nl> s -> spatial_decomposition_count --; <nl>  <nl> + if ( s -> spatial_decomposition_count <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Resolution too low \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> s -> m . pict_type = pic -> pict_type ; <nl> s -> qbias = pic -> pict_type == AV_PICTURE_TYPE_P ? 2 : 0 ; <nl> 
static int get_cookies ( HTTPContext * s , char ** cookies , const char * path , <nl> } <nl>  <nl> done_cookie : <nl> - av_free ( cdomain ); <nl> - av_free ( cpath ); <nl> - av_free ( cvalue ); <nl> + av_freep (& cdomain ); <nl> + av_freep (& cpath ); <nl> + av_freep (& cvalue ); <nl> if ( ret < 0 ) { <nl> if (* cookies ) av_freep ( cookies ); <nl> av_free ( cset_cookies );
typedef struct Jpeg2000Component { <nl> /* misc tools */ <nl> static inline int ff_jpeg2000_ceildivpow2 ( int a , int b ) <nl> { <nl> - return -((( int64_t )(- a )) >> b ); <nl> + return -((-( int64_t ) a ) >> b ); <nl> } <nl>  <nl> static inline int ff_jpeg2000_ceildiv ( int a , int b )
static int decode_subframe_fixed ( FLACContext * s , int32_t * decoded , <nl> int pred_order , int bps ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> - int a , b , c , d , i ; <nl> + int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl>  <nl> /* warm up samples */ <nl> for ( i = 0 ; i < pred_order ; i ++) {
static int set_params ( AVFilterContext * ctx , const char * params ) <nl> Frei0rContext * frei0r = ctx -> priv ; <nl> int i ; <nl>  <nl> + if (! params ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < frei0r -> plugin_info . num_params ; i ++) { <nl> f0r_param_info_t info ; <nl> char * param ;
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> start_subband += start_subband - 7 ; <nl> end_subband = get_bits ( gbc , 3 ) + 5 ; <nl> # if USE_FIXED <nl> - s -> spx_dst_end_freq = end_freq_inv_tab [ end_subband ]; <nl> + s -> spx_dst_end_freq = end_freq_inv_tab [ end_subband - 5 ]; <nl> # endif <nl> if ( end_subband > 7 ) <nl> end_subband += end_subband - 7 ;
int ff_mpeg_update_thread_context ( AVCodecContext * dst , <nl> s -> input_picture_number = s1 -> input_picture_number ; <nl>  <nl> av_assert0 (! s -> picture || s -> picture != s1 -> picture ); <nl> + if ( s -> picture ) <nl> for ( i = 0 ; i < MAX_PICTURE_COUNT ; i ++) { <nl> ff_mpeg_unref_picture ( s , & s -> picture [ i ]); <nl> if ( s1 -> picture [ i ]. f . data [ 0 ] &&
static void filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * samplesref ) <nl> for ( i = 0 ; i < ctx -> nb_outputs ; i ++) <nl> ff_filter_samples ( inlink -> dst -> outputs [ i ], <nl> avfilter_ref_buffer ( samplesref , ~ AV_PERM_WRITE )); <nl> + avfilter_unref_buffer ( samplesref ); <nl> } <nl>  <nl> AVFilter avfilter_af_asplit = {
static int decode_header ( EXRContext * s ) <nl> channel -> xsub = xsub ; <nl> channel -> ysub = ysub ; <nl>  <nl> - s -> current_channel_offset += 1 << current_pixel_type ; <nl> + if ( current_pixel_type == EXR_HALF ) { <nl> + s -> current_channel_offset += 2 ; <nl> + } else {/* Float or UINT32 */ <nl> + s -> current_channel_offset += 4 ; <nl> + } <nl> } <nl>  <nl> /* Check if all channels are set with an offset or if the channels
void av_frame_unref ( AVFrame * frame ) <nl> { <nl> int i ; <nl>  <nl> + if (! frame ) <nl> + return ; <nl> + <nl> wipe_side_data ( frame ); <nl>  <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( frame -> buf ); i ++)
static int matroska_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> ret = matroska_parse_cluster ( matroska ); <nl> } <nl>  <nl> - if ( ret == AVERROR_INVALIDDATA ) { <nl> + if ( ret == AVERROR_INVALIDDATA && pkt -> data ) { <nl> pkt -> flags |= AV_PKT_FLAG_CORRUPT ; <nl> return 0 ; <nl> }
static av_cold int bktr_init ( const char * video_device , int width , int height , <nl> long ioctl_frequency ; <nl> char * arg ; <nl> int c ; <nl> - struct sigaction act = { 0 }, old ; <nl> + struct sigaction act = { { 0 } }, old ; <nl>  <nl> if ( idev < 0 || idev > 4 ) <nl> {
static int encode_audio_frame ( AVFormatContext * s , OutputStream * ost , <nl> pkt . data = NULL ; <nl> pkt . size = 0 ; <nl>  <nl> - if ( buf ) { <nl> + if ( buf && buf_size ) { <nl> if (! ost -> output_frame ) { <nl> ost -> output_frame = avcodec_alloc_frame (); <nl> if (! ost -> output_frame ) {
static av_cold int adpcm_encode_init ( AVCodecContext * avctx ) <nl> goto error ; <nl> } <nl>  <nl> - avctx -> coded_frame = avcodec_alloc_frame (); <nl> + if (!( avctx -> coded_frame = avcodec_alloc_frame ())) <nl> + goto error ; <nl>  <nl> return 0 ; <nl> error :
static const AVOption options [] = { <nl> { " safe ", " enable safe mode ", <nl> OFFSET ( safe ), AV_OPT_TYPE_INT , {. i64 = - 1 }, - 1 , 1 , DEC }, <nl> { " auto_convert ", " automatically convert bitstream format ", <nl> - OFFSET ( auto_convert ), AV_OPT_TYPE_INT , {. i64 = 0 }, 0 , 1 , DEC }, <nl> + OFFSET ( auto_convert ), AV_OPT_TYPE_INT , {. i64 = 1 }, 0 , 1 , DEC }, <nl> { NULL } <nl> }; <nl> 
static int guess_ni_flag ( AVFormatContext * s ){ <nl> if ( last_start > first_end ) <nl> return 1 ; <nl> idx = av_mallocz ( sizeof (* idx ) * s -> nb_streams ); <nl> - for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1 ) { <nl> + for ( min_pos = pos = 0 ; min_pos != INT64_MAX ; pos = min_pos + 1LU ) { <nl> int64_t max_dts = INT64_MIN / 2 , min_dts = INT64_MAX / 2 ; <nl> min_pos = INT64_MAX ; <nl> 
int main ( int argc , char * argv []) <nl> /* keep ftyp atom */ <nl> if ( atom_type == FTYP_ATOM ) { <nl> ftyp_atom_size = atom_size ; <nl> + free ( ftyp_atom ); <nl> ftyp_atom = malloc ( ftyp_atom_size ); <nl> if (! ftyp_atom ) { <nl> printf (" could not allocate %" PRIu64 " byte for ftyp atom \ n ",
int ff_h264_decode_sei ( H264Context * h ){ <nl> size += show_bits (& s -> gb , 8 ); <nl> } while ( get_bits (& s -> gb , 8 ) == 255 ); <nl>  <nl> + if ( s -> avctx -> debug & FF_DEBUG_STARTCODE ) <nl> + av_log ( h -> s . avctx , AV_LOG_DEBUG , " SEI % d len :% d \ n ", type , size ); <nl> + <nl> switch ( type ){ <nl> case SEI_TYPE_PIC_TIMING : // Picture timing SEI <nl> if ( decode_picture_timing ( h ) < 0 )
int av_stream_add_side_data ( AVStream * st , enum AVPacketSideDataType type , <nl> } <nl> } <nl>  <nl> - tmp = av_realloc_array ( st -> side_data , st -> nb_side_data + 1 , sizeof (* tmp )); <nl> + if (( unsigned ) st -> nb_side_data + 1 >= INT_MAX / sizeof (* st -> side_data )) <nl> + return AVERROR ( ERANGE ); <nl> + <nl> + tmp = av_realloc ( st -> side_data , st -> nb_side_data + 1 * sizeof (* tmp )); <nl> if (! tmp ) { <nl> return AVERROR ( ENOMEM ); <nl> }
static void clear_context ( MpegEncContext * s ) <nl>  <nl> s -> parse_context . buffer = NULL ; <nl> s -> parse_context . buffer_size = 0 ; <nl> + s -> parse_context . overread = 0 ; <nl> s -> bitstream_buffer = NULL ; <nl> s -> allocated_bitstream_buffer_size = 0 ; <nl> s -> picture = NULL ;
static inline void dxt1_decode_pixels ( const uint8_t * s , uint32_t * d , <nl> unsigned int qstride , unsigned int flag , <nl> uint64_t alpha ) { <nl> - unsigned int x , y , c0 , c1 , a = (! flag * 255 ) << 24 ; <nl> + unsigned int x , y , c0 , c1 , a = (! flag * 255u ) << 24 ; <nl> unsigned int rb0 , rb1 , rb2 , rb3 , g0 , g1 , g2 , g3 ; <nl> uint32_t colors [ 4 ], pixels ; <nl> 
static int imc_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> IMCContext * q = avctx -> priv_data ; <nl>  <nl> - LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 ]); <nl> + LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 + FF_INPUT_BUFFER_PADDING_SIZE / 2 ]); <nl>  <nl> if ( buf_size < IMC_BLOCK_SIZE * avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame too small !\ n ");
int av_samples_get_buffer_size ( int * linesize , int nb_channels , int nb_samples , <nl>  <nl> /* auto - select alignment if not specified */ <nl> if (! align ) { <nl> + if ( nb_samples > INT_MAX - 31 ) <nl> + return AVERROR ( EINVAL ); <nl> align = 1 ; <nl> nb_samples = FFALIGN ( nb_samples , 32 ); <nl> }
static int decompress_i ( AVCodecContext * avctx , uint32_t * dst , int linesize ) <nl> clr = ( b << 16 ) + ( g << 8 ) + r ; <nl> k += run ; <nl> while ( run -- > 0 ) { <nl> + if ( y >= avctx -> height ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> dst [ y * linesize + x ] = clr ; <nl> lx = x ; <nl> ly = y ;
static ResampleContext * resample_init ( ResampleContext * c , int out_rate , int in_r <nl> av_assert0 ( 0 ); <nl> } <nl>  <nl> + if ( filter_size / factor > INT32_MAX / 256 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " Filter length too large \ n "); <nl> + goto error ; <nl> + } <nl> + <nl> c -> phase_shift = phase_shift ; <nl> c -> phase_mask = phase_count - 1 ; <nl> c -> linear = linear ;
int ff_mpeg1_find_frame_end ( ParseContext * pc , const uint8_t * buf , int buf_size , <nl> pc -> frame_start_found = 4 ; <nl> } <nl> if ( state == SEQ_END_CODE ) { <nl> + pc -> frame_start_found = 0 ; <nl> pc -> state =- 1 ; <nl> return i + 1 ; <nl> }
int avpriv_adx_decode_header ( AVCodecContext * avctx , const uint8_t * buf , <nl>  <nl> /* channels */ <nl> avctx -> channels = buf [ 7 ]; <nl> - if ( avctx -> channels > 2 ) <nl> + if ( avctx -> channels <= 0 || avctx -> channels > 2 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> /* sample rate */
reload : <nl> /* If we need to reload the playlist again below ( if <nl> * there ' s still no more segments ), switch to a reload <nl> * interval of half the target duration . */ <nl> - reload_interval = v -> target_duration * 500000 ; <nl> + reload_interval = v -> target_duration * 500000LL ; <nl> } <nl> if ( v -> cur_seq_no < v -> start_seq_no ) { <nl> av_log ( NULL , AV_LOG_WARNING ,
static int read_header ( ShortenContext * s ) <nl> s -> blocksize = blocksize ; <nl>  <nl> maxnlpc = get_uint ( s , LPCQSIZE ); <nl> + if ( maxnlpc > 1024U ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " maxnlpc is : % d \ n ", maxnlpc ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> nmean = get_uint ( s , 0 ); <nl>  <nl> skip_bytes = get_uint ( s , NSKIPSIZE );
static void decode_nal_sei_decoded_picture_hash ( HEVCContext * s ) <nl> static void decode_nal_sei_frame_packing_arrangement ( HEVCContext * s ) <nl> { <nl> GetBitContext * gb = & s -> HEVClc -> gb ; <nl> - int cancel , type , quincunx , content ; <nl> + int cancel ; <nl> + int quincunx = 0 ; <nl> + int content = - 1 ; <nl> + int type = - 1 ; <nl>  <nl> get_ue_golomb ( gb ); // frame_packing_arrangement_id <nl> cancel = get_bits1 ( gb ); // frame_packing_cancel_flag
static void mp_decode_frame_helper ( MotionPixelsContext * mp , GetBitContext * gb ) <nl> YuvPixel p ; <nl> int y , y0 ; <nl>  <nl> + av_assert1 ( mp -> changes_map [ 0 ]); <nl> + <nl> for ( y = 0 ; y < mp -> avctx -> height ; ++ y ) { <nl> if ( mp -> changes_map [ y * mp -> avctx -> width ] != 0 ) { <nl> memset ( mp -> gradient_scale , 1 , sizeof ( mp -> gradient_scale ));
static int mov_read_close ( AVFormatContext * s ) <nl> av_freep (& sc -> rap_group ); <nl> av_freep (& sc -> display_matrix ); <nl>  <nl> - for ( j = 0 ; j < sc -> stsd_count ; j ++) <nl> - av_free ( sc -> extradata [ j ]); <nl> + if ( sc -> extradata ) <nl> + for ( j = 0 ; j < sc -> stsd_count ; j ++) <nl> + av_free ( sc -> extradata [ j ]); <nl> av_freep (& sc -> extradata ); <nl> av_freep (& sc -> extradata_size ); <nl> 
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> ThreadFrame tframe = { . f = frame }; <nl> WavpackFrameContext * s ; <nl> GetByteContext gb ; <nl> - void * samples_l , * samples_r ; <nl> + void * samples_l = NULL , * samples_r = NULL ; <nl> int ret ; <nl> int got_terms = 0 , got_weights = 0 , got_samples = 0 , <nl> got_entropy = 0 , got_bs = 0 , got_float = 0 , got_hybrid = 0 ;
static int get_cookies ( HTTPContext * s , char ** cookies , const char * path , <nl> snprintf (* cookies , str_size , "% s ; % s =% s ", tmp , cookie_entry -> key , cookie_entry -> value ); <nl> av_free ( tmp ); <nl> } <nl> + av_dict_free (& cookie_params ); <nl> } <nl>  <nl> av_free ( set_cookies );
static int avi_read_seek ( AVFormatContext * s , int stream_index , <nl> continue ; <nl>  <nl> // av_assert1 ( st2 -> codecpar -> block_align ); <nl> - av_assert0 ( fabs ( av_q2d ( st2 -> time_base ) - ast2 -> scale / ( double ) ast2 -> rate ) < av_q2d ( st2 -> time_base ) * 0 . 00000001 ); <nl> index = av_index_search_timestamp ( st2 , <nl> av_rescale_q ( timestamp , <nl> st -> time_base ,
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> ADD_METADATA ( count , " ModelTiepointTag ", NULL ); <nl> break ; <nl> case TIFF_GEO_KEY_DIRECTORY : <nl> + if ( s -> geotag_count ) { <nl> + avpriv_request_sample ( s -> avctx , " Multiple geo key directories \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> ADD_METADATA ( 1 , " GeoTIFF_Version ", NULL ); <nl> ADD_METADATA ( 2 , " GeoTIFF_Key_Revision ", "."); <nl> s -> geotag_count = ff_tget_short (& s -> gb , s -> le );
static AVIOContext * wtvfile_open_sector ( int first_sector , uint64_t length , int <nl> return NULL ; <nl> } <nl>  <nl> - if ( wf -> sectors [ wf -> nb_sectors - 1 ] << WTV_SECTOR_BITS > avio_tell ( s -> pb )) <nl> + if (( int64_t ) wf -> sectors [ wf -> nb_sectors - 1 ] << WTV_SECTOR_BITS > avio_tell ( s -> pb )) <nl> av_log ( s , AV_LOG_WARNING , " truncated file \ n "); <nl>  <nl> /* check length */
static void decode ( AVCodecContext * dec_ctx , AVFrame * frame , AVPacket * pkt , <nl>  <nl> /* the picture is allocated by the decoder . no need to <nl> free it */ <nl> - snprintf ( buf , sizeof ( buf ), filename , dec_ctx -> frame_number ); <nl> + snprintf ( buf , sizeof ( buf ), "% s -% d ", filename , dec_ctx -> frame_number ); <nl> pgm_save ( frame -> data [ 0 ], frame -> linesize [ 0 ], <nl> frame -> width , frame -> height , buf ); <nl> }
static void process_client ( AVIOContext * client , const char * in_uri ) <nl> // may return empty string . <nl> if ( resource && strlen ( resource )) <nl> break ; <nl> + av_freep (& resource ); <nl> } <nl> if ( ret < 0 ) <nl> goto end ; <nl> end : <nl> avio_close ( client ); <nl> fprintf ( stderr , " Closing input \ n "); <nl> avio_close ( input ); <nl> + av_freep (& resource ); <nl> } <nl>  <nl> int main ( int argc , char ** argv )
static int64_t get_bit_rate ( AVCodecContext * ctx ) <nl> break ; <nl> case AVMEDIA_TYPE_AUDIO : <nl> bits_per_sample = av_get_bits_per_sample ( ctx -> codec_id ); <nl> - bit_rate = bits_per_sample ? ctx -> sample_rate * ctx -> channels * bits_per_sample : ctx -> bit_rate ; <nl> + bit_rate = bits_per_sample ? ctx -> sample_rate * ( int64_t ) ctx -> channels * bits_per_sample : ctx -> bit_rate ; <nl> break ; <nl> default : <nl> bit_rate = 0 ;
static int execute_decode_slices ( H264Context * h , int context_count ) <nl> if ( context_count == 1 ) { <nl> return decode_slice ( avctx , & h ); <nl> } else { <nl> + av_assert0 ( context_count > 0 ); <nl> for ( i = 1 ; i < context_count ; i ++) { <nl> hx = h -> thread_context [ i ]; <nl> hx -> s . err_recognition = avctx -> err_recognition ;
static int parse_bsfs ( void * log_ctx , const char * bsfs_spec , <nl> AVBitStreamFilterContext ** bsfs ) <nl> { <nl> char * bsf_name , * buf , * saveptr ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> if (!( buf = av_strdup ( bsfs_spec ))) <nl> return AVERROR ( ENOMEM );
static int ipvideo_decode_block_opcode_0xA ( IpvideoContext * s , AVFrame * frame ) <nl> unsigned char P [ 8 ]; <nl> int flags = 0 ; <nl>  <nl> + if ( bytestream2_get_bytes_left (& s -> stream_ptr ) < 16 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too little data for opcode 0xA \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> bytestream2_get_buffer (& s -> stream_ptr , P , 4 ); <nl>  <nl> /* 4 - color encoding for each 4x4 quadrant , or 4 - color encoding on
static int asf_read_frame_header ( AVFormatContext * s , AVIOContext * pb ){ <nl> case 0x54 : <nl> aspect . num = avio_r8 ( pb ); <nl> aspect . den = avio_r8 ( pb ); <nl> - if ( aspect . num > 0 && aspect . den > 0 ) { <nl> + if ( aspect . num > 0 && aspect . den > 0 && asf -> stream_index >= 0 ) { <nl> s -> streams [ asf -> stream_index ]-> sample_aspect_ratio = aspect ; <nl> } <nl> break ;
static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> next = avio_tell ( s -> pb ) + tlen ; <nl>  <nl> if ( tflags & ID3v2_FLAG_DATALEN ) { <nl> + if ( tlen < 4 ) <nl> + break ; <nl> avio_rb32 ( s -> pb ); <nl> tlen -= 4 ; <nl> }
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> align_get_bits (& ctx -> gb ); <nl>  <nl> + if (! band -> scan ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " band -> scan not set \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
int ff_flac_parse_picture ( AVFormatContext * s , uint8_t * buf , int buf_size ) <nl> if (!( data = av_buffer_alloc ( len + FF_INPUT_BUFFER_PADDING_SIZE ))) { <nl> RETURN_ERROR ( AVERROR ( ENOMEM )); <nl> } <nl> + memset ( data -> data + len , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if ( avio_read ( pb , data -> data , len ) != len ) { <nl> av_log ( s , AV_LOG_ERROR , " Error reading attached picture data .\ n "); <nl> if ( s -> error_recognition & AV_EF_EXPLODE )
static int find_image_range ( int * pfirst_index , int * plast_index , <nl>  <nl> static int image_probe ( AVProbeData * p ) <nl> { <nl> - if ( av_str2id ( img_tags , p -> filename )) { <nl> + if ( p -> filename && av_str2id ( img_tags , p -> filename )) { <nl> if ( av_filename_number_test ( p -> filename )) <nl> return AVPROBE_SCORE_MAX ; <nl> else
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , " Stream discovered after head already parsed \ n "); <nl> st = create_stream ( s , <nl> ( int []){ AVMEDIA_TYPE_VIDEO , AVMEDIA_TYPE_AUDIO , AVMEDIA_TYPE_DATA }[ stream_type ]); <nl> + if (! st ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> } <nl> av_dlog ( s , "% d % X % d \ n ", stream_type , flags , st -> discard );
static int convert_sub_to_old_ass_form ( AVSubtitle * sub , const AVPacket * pkt , AVR <nl> int ts_start , ts_duration = - 1 ; <nl> long int layer ; <nl>  <nl> - if ( rect -> type != SUBTITLE_ASS || ! strncmp ( rect -> ass , " Dialogue ", 10 )) <nl> + if ( rect -> type != SUBTITLE_ASS || ! strncmp ( rect -> ass , " Dialogue : ", 10 )) <nl> continue ; <nl>  <nl> av_bprint_clear (& buf );
static int bmp_decode_frame ( AVCodecContext * avctx , <nl> BiCompression comp ; <nl> unsigned int ihsize ; <nl> int i , j , n , linesize , ret ; <nl> - uint32_t rgb [ 3 ]; <nl> + uint32_t rgb [ 3 ] = { 0 }; <nl> uint32_t alpha = 0 ; <nl> uint8_t * ptr ; <nl> int dsize ;
static av_cold int twin_decode_init ( AVCodecContext * avctx ) <nl> default : avctx -> sample_rate = isampf * 1000 ; break ; <nl> } <nl>  <nl> - if ( avctx -> channels > CHANNELS_MAX ) { <nl> + if ( avctx -> channels <= 0 || avctx -> channels > CHANNELS_MAX ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unsupported number of channels : % i \ n ", <nl> avctx -> channels ); <nl> return - 1 ;
static int tqi_decode_frame ( AVCodecContext * avctx , <nl> for ( s -> mb_x = 0 ; s -> mb_x <( avctx -> width + 15 )/ 16 ; s -> mb_x ++) <nl> { <nl> if ( tqi_decode_mb ( s , t -> block ) < 0 ) <nl> - break ; <nl> + goto end ; <nl> tqi_idct_put ( t , t -> block ); <nl> } <nl> + end : <nl>  <nl> * data_size = sizeof ( AVFrame ); <nl> *( AVFrame *) data = t -> frame ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> # else <nl> if ( s -> xch_present && ! s -> xch_disable ) { <nl> # endif <nl> + if ( avctx -> channel_layout & AV_CH_BACK_CENTER ) { <nl> + avpriv_request_sample ( avctx , " XCh with Back center channel "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> avctx -> channel_layout |= AV_CH_BACK_CENTER ; <nl> if ( s -> lfe ) { <nl> avctx -> channel_layout |= AV_CH_LOW_FREQUENCY ;
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl>  <nl> if ( frames_hwctx -> texture ) <nl> ID3D11Texture2D_Release ( frames_hwctx -> texture ); <nl> + frames_hwctx -> texture = NULL ; <nl>  <nl> if ( s -> staging_texture ) <nl> ID3D11Texture2D_Release ( s -> staging_texture ); <nl> + s -> staging_texture = NULL ; <nl> } <nl>  <nl> static void free_texture ( void * opaque , uint8_t * data )
static inline int mpeg4_decode_block ( MpegEncContext * s , DCTELEM * block , <nl> int n , int coded , int intra , int rvlc ) <nl> { <nl> int level , i , last , run ; <nl> - int dc_pred_dir ; <nl> + int av_uninit ( dc_pred_dir ); <nl> RLTable * rl ; <nl> RL_VLC_ELEM * rl_vlc ; <nl> const uint8_t * scan_table ;
static int decode_vol_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> else <nl> s -> quarter_sample = 0 ; <nl>  <nl> + if ( get_bits_left ( gb ) < 4 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " VOL Header truncated \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (! get_bits1 ( gb )) { <nl> int pos = get_bits_count ( gb ); <nl> int estimation_method = get_bits ( gb , 2 );
static int pcx_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> nplanes = buf [ 65 ]; <nl> bytes_per_scanline = nplanes * bytes_per_line ; <nl>  <nl> - if ( bytes_per_scanline < w * bits_per_pixel * nplanes / 8 || <nl> + if ( bytes_per_scanline < ( w * bits_per_pixel * nplanes + 7 ) / 8 || <nl> (! compressed && bytes_per_scanline > buf_size / h )) { <nl> av_log ( avctx , AV_LOG_ERROR , " PCX data is corrupted \ n "); <nl> return AVERROR_INVALIDDATA ;
static int sap_write_close ( AVFormatContext * s ) <nl> url_fclose ( rtpctx -> pb ); <nl> av_metadata_free (& rtpctx -> streams [ 0 ]-> metadata ); <nl> av_metadata_free (& rtpctx -> metadata ); <nl> + av_free ( rtpctx -> streams [ 0 ]-> info ); <nl> av_free ( rtpctx -> streams [ 0 ]); <nl> av_free ( rtpctx ); <nl> s -> streams [ i ]-> priv_data = NULL ;
int attribute_align_arg avcodec_open2 ( AVCodecContext * avctx , const AVCodec * code <nl> avctx -> time_base = av_inv_q ( av_mul_q ( avctx -> framerate , ( AVRational ){ avctx -> ticks_per_frame , 1 })); <nl> # endif <nl> } <nl> + if ( codec -> priv_data_size > 0 && avctx -> priv_data && codec -> priv_class ) { <nl> + av_assert0 (*( const AVClass **) avctx -> priv_data == codec -> priv_class ); <nl> + } <nl> + <nl> end : <nl> ff_unlock_avcodec (); <nl> if ( options ) {
static av_cold int hevc_decode_free ( AVCodecContext * avctx ) <nl> av_freep (& s -> sList [ i ]); <nl> } <nl> } <nl> + if ( s -> HEVClc == s -> HEVClcList [ 0 ]) <nl> + s -> HEVClc = NULL ; <nl> av_freep (& s -> HEVClcList [ 0 ]); <nl>  <nl> for ( i = 0 ; i < s -> nals_allocated ; i ++)
static int vp8_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> VP8Context * s = avctx -> priv_data ; <nl> int ret , mb_x , mb_y , i , y , referenced ; <nl> enum AVDiscard skip_thresh ; <nl> - AVFrame * curframe ; <nl> + AVFrame * curframe = NULL ; <nl>  <nl> if (( ret = decode_frame_header ( s , avpkt -> data , avpkt -> size )) < 0 ) <nl> return ret ;
static int dvbsub_parse_page_segment ( AVCodecContext * avctx , <nl> region_id = * buf ++; <nl> buf += 1 ; <nl>  <nl> + display = ctx -> display_list ; <nl> + while ( display && display -> region_id != region_id ) { <nl> + display = display -> next ; <nl> + } <nl> + if ( display ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " duplicate region \ n "); <nl> + break ; <nl> + } <nl> + <nl> display = tmp_display_list ; <nl> tmp_ptr = & tmp_display_list ; <nl> 
int ff_wma_init ( AVCodecContext * avctx , int flags2 ) <nl> int coef_vlc_table ; <nl>  <nl> if ( avctx -> sample_rate <= 0 || avctx -> sample_rate > 50000 <nl> - || avctx -> channels <= 0 || avctx -> channels > 8 <nl> + || avctx -> channels <= 0 || avctx -> channels > 2 <nl> || avctx -> bit_rate <= 0 ) <nl> return - 1 ; <nl> 
static void id3v1_create_tag ( AVFormatContext * s , uint8_t * buf ) <nl>  <nl> static int mp3_read_probe ( AVProbeData * p ) <nl> { <nl> - int max_frames , first_frames ; <nl> + int max_frames , first_frames = 0 ; <nl> int fsize , frames , sample_rate ; <nl> uint32_t header ; <nl> uint8_t * buf , * buf2 , * end ;
int av_aes_init ( AVAES * a , const uint8_t * key , int key_bits , int decrypt ) { <nl> uint8_t log8 [ 256 ]; <nl> uint8_t alog8 [ 512 ]; <nl>  <nl> - if (! enc_multbl [ 0 ][ sizeof ( enc_multbl )/ sizeof ( enc_multbl [ 0 ][ 0 ])- 1 ]){ <nl> + if (! enc_multbl [ FF_ARRAY_ELEMS ( enc_multbl )- 1 ][ FF_ARRAY_ELEMS ( enc_multbl [ 0 ])- 1 ]){ <nl> j = 1 ; <nl> for ( i = 0 ; i < 255 ; i ++){ <nl> alog8 [ i ]=
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl> cnt1 = get_bits ( b , nbits ); <nl> } else { <nl> pfx = 14 + (((( uint64_t )( value - 14 )) >> 32 ) & ( value - 14 )); <nl> + if ( pfx < 1 || pfx > 25 ) <nl> + return AVERROR_INVALIDDATA ; <nl> cnt1 *= ( 1 << pfx ) - 1 ; <nl> shbits = show_bits ( b , pfx ); <nl> if ( shbits <= 1 ) {
static void vmd_decode ( VmdVideoContext * s , AVFrame * frame ) <nl> palette32 [ i ] = ( r << 16 ) | ( g << 8 ) | ( b ); <nl> } <nl> } <nl> - s -> size -= ( 256 * 3 + 2 ); <nl> + s -> size -= PALETTE_COUNT * 3 + 2 ; <nl> } <nl> if ( s -> size > 0 ) { <nl> /* originally UnpackFrame in VAG ' s code */
av_cold int ff_mjpeg_decode_init ( AVCodecContext * avctx ) <nl> s -> first_picture = 1 ; <nl> s -> org_height = avctx -> coded_height ; <nl> avctx -> chroma_sample_location = AVCHROMA_LOC_CENTER ; <nl> + avctx -> colorspace = AVCOL_SPC_BT470BG ; <nl>  <nl> build_basic_mjpeg_vlc ( s ); <nl> 
static int cinepak_decode_strip ( CinepakContext * s , <nl> while (( data + 4 ) <= eod ) { <nl> chunk_id = BE_16 (& data [ 0 ]); <nl> chunk_size = BE_16 (& data [ 2 ]) - 4 ; <nl> + if ( chunk_size < 0 ) <nl> + return - 1 ; <nl> + <nl> data += 4 ; <nl> chunk_size = (( data + chunk_size ) > eod ) ? ( eod - data ) : chunk_size ; <nl> 
static int rtsp_read_header ( AVFormatContext * s ) <nl> return ret ; <nl>  <nl> rt -> real_setup_cache = ! s -> nb_streams ? NULL : <nl> - av_mallocz ( 2 * s -> nb_streams * sizeof (* rt -> real_setup_cache )); <nl> + av_mallocz_array ( s -> nb_streams , 2 * sizeof (* rt -> real_setup_cache )); <nl> if (! rt -> real_setup_cache && s -> nb_streams ) <nl> return AVERROR ( ENOMEM ); <nl> rt -> real_setup = rt -> real_setup_cache + s -> nb_streams ;
static void pre_process_video_frame ( InputStream * ist , AVPicture * picture , void * <nl>  <nl> /* create temporary picture */ <nl> size = avpicture_get_size ( dec -> pix_fmt , dec -> width , dec -> height ); <nl> + if ( size < 0 ) <nl> + return ; <nl> buf = av_malloc ( size ); <nl> if (! buf ) <nl> return ;
static av_cold int decode_close_mp3on4 ( AVCodecContext * avctx ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < s -> frames ; i ++) <nl> - av_free ( s -> mp3decctx [ i ]); <nl> + av_freep (& s -> mp3decctx [ i ]); <nl>  <nl> return 0 ; <nl> }
static int ac3_parse_audio_block ( AC3DecodeContext * s , int blk ) <nl> /* coupling in use */ <nl> int cpl_begin_freq , cpl_end_freq ; <nl>  <nl> + if ( channel_mode < AC3_CHMODE_STEREO ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " coupling not allowed in mono or dual - mono \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> /* determine which channels are coupled */ <nl> for ( ch = 1 ; ch <= fbw_channels ; ch ++) <nl> s -> channel_in_cpl [ ch ] = get_bits1 ( gbc );
static int cinvideo_decode_frame ( AVCodecContext * avctx , <nl> bitmap_frame_size = buf_size - 4 ; <nl>  <nl> /* handle palette */ <nl> + if ( bitmap_frame_size < palette_colors_count * ( 3 + ( palette_type != 0 ))) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( palette_type == 0 ) { <nl> for ( i = 0 ; i < palette_colors_count ; ++ i ) { <nl> cin -> palette [ i ] = bytestream_get_le24 (& buf );
static int vqf_probe ( AVProbeData * probe_packet ) <nl> if (! memcmp ( probe_packet -> buf + 4 , " 00052200 ", 8 )) <nl> return AVPROBE_SCORE_MAX ; <nl>  <nl> + if ( AV_RL32 ( probe_packet -> buf + 12 ) > ( 1 << 27 )) <nl> + return AVPROBE_SCORE_EXTENSION / 2 ; <nl> + <nl> return AVPROBE_SCORE_EXTENSION ; <nl> } <nl> 
AVFormatContext * avformat_alloc_context ( void ) <nl> return NULL ; <nl> } <nl> ic -> internal -> offset = AV_NOPTS_VALUE ; <nl> + ic -> internal -> raw_packet_buffer_remaining_size = RAW_PACKET_BUFFER_SIZE ; <nl>  <nl> return ic ; <nl> }
static void fill_coding_method_array ( sb_int8_array tone_level_idx , <nl> for ( j = 0 ; j < 64 ; j ++) <nl> acc += tone_level_idx_temp [ ch ][ sb ][ j ]; <nl>  <nl> - multres = 0x66666667 * ( acc * 10 ); <nl> + multres = 0x66666667LL * ( acc * 10 ); <nl> esp_40 = ( multres >> 32 ) / 8 + (( multres & 0xffffffff ) >> 31 ); <nl> for ( ch = 0 ; ch < nb_channels ; ch ++) <nl> for ( sb = 0 ; sb < 30 ; sb ++)
int ff_wma_init ( AVCodecContext * avctx , int flags2 ) <nl>  <nl> /* compute MDCT block size */ <nl> s -> frame_len_bits = ff_wma_get_frame_len_bits ( s -> sample_rate , s -> version , 0 ); <nl> + s -> next_block_len_bits = s -> frame_len_bits ; <nl> + s -> prev_block_len_bits = s -> frame_len_bits ; <nl> + s -> block_len_bits = s -> frame_len_bits ; <nl>  <nl> s -> frame_len = 1 << s -> frame_len_bits ; <nl> if ( s -> use_variable_block_len ) {
static int asf_read_close ( AVFormatContext * s ) <nl> av_dict_free (& asf -> asf_sd [ i ]. asf_met ); <nl> } <nl>  <nl> + asf -> nb_streams = 0 ; <nl> return 0 ; <nl> } <nl> 
int ff_http_do_new_request ( URLContext * h , const char * uri ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if ( s -> willclose ) <nl> + return AVERROR_EOF ; <nl> + <nl> s -> end_chunked_post = 0 ; <nl> s -> chunkend = 0 ; <nl> s -> off = 0 ;
static int draw_text ( AVFilterContext * ctx , AVFilterBufferRef * picref , <nl> if ( dtext -> tc_opt_string ) { <nl> char tcbuf [ AV_TIMECODE_STR_SIZE ]; <nl> av_timecode_make_string (& dtext -> tc , tcbuf , dtext -> frame_id ++); <nl> + av_free ( buf ); <nl> buf = av_asprintf ("% s % s ", dtext -> text , tcbuf ); <nl> } <nl> 
static int g2m_init_buffers ( G2MContext * c ) <nl> if (! c -> synth_tile || ! c -> jpeg_tile || <nl> c -> old_tile_w < c -> tile_width || <nl> c -> old_tile_h < c -> tile_height ) { <nl> - c -> tile_stride = FFALIGN ( c -> tile_width * 3 , 16 ); <nl> + c -> tile_stride = FFALIGN ( c -> tile_width , 16 ) * 3 ; <nl> aligned_height = FFALIGN ( c -> tile_height , 16 ); <nl> av_free ( c -> synth_tile ); <nl> av_free ( c -> jpeg_tile );
static inline void codeblock ( DiracContext * s , SubBand * b , <nl> } \ <nl>  <nl> INTRA_DC_PRED ( 8 , int16_t ) <nl> - INTRA_DC_PRED ( 10 , int32_t ) <nl> + INTRA_DC_PRED ( 10 , uint32_t ) <nl>  <nl> /** <nl> * Dirac Specification ->
static int oma_read_seek ( struct AVFormatContext * s , <nl> int stream_index , int64_t timestamp , int flags ) <nl> { <nl> OMAContext * oc = s -> priv_data ; <nl> - int err = ff_pcm_read_seek ( s , stream_index , timestamp , flags ); <nl> + int64_t err = ff_pcm_read_seek ( s , stream_index , timestamp , flags ); <nl>  <nl> if (! oc -> encrypted ) <nl> return err ;
static int mov_write_audio_tag ( AVFormatContext * s , AVIOContext * pb , MOVMuxContex <nl> uint32_t tag = track -> tag ; <nl>  <nl> if ( track -> mode == MODE_MOV ) { <nl> - if ( track -> timescale > UINT16_MAX ) { <nl> + if ( track -> timescale > UINT16_MAX || ! track -> par -> channels ) { <nl> if ( mov_get_lpcm_flags ( track -> par -> codec_id )) <nl> tag = AV_RL32 (" lpcm "); <nl> version = 2 ;
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
static int tak_decode_frame ( AVCodecContext * avctx , void * data , <nl> uint8_t * samples = ( uint8_t *) frame -> extended_data [ chan ]; <nl> int32_t * decoded = s -> decoded [ chan ]; <nl> for ( i = 0 ; i < s -> nb_samples ; i ++) <nl> - samples [ i ] = decoded [ i ] + 0x80 ; <nl> + samples [ i ] = decoded [ i ] + 0x80U ; <nl> } <nl> break ; <nl> case AV_SAMPLE_FMT_S16P :
static int sp5x_decode_frame ( AVCodecContext * avctx , <nl> recoded [ j ++] = 0xFF ; <nl> recoded [ j ++] = 0xD9 ; <nl>  <nl> - avctx -> flags &= ~ CODEC_FLAG_EMU_EDGE ; <nl> av_init_packet (& avpkt_recoded ); <nl> avpkt_recoded . data = recoded ; <nl> avpkt_recoded . size = j ; <nl> AVCodec ff_amv_decoder = { <nl> NULL , <nl> ff_mjpeg_decode_end , <nl> sp5x_decode_frame , <nl> - CODEC_CAP_DR1 , <nl> + 0 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" AMV Video "), <nl> };
int avio_close_dyn_buf ( AVIOContext * s , uint8_t ** pbuffer ) <nl> static const char padbuf [ FF_INPUT_BUFFER_PADDING_SIZE ] = { 0 }; <nl> int padding = 0 ; <nl>  <nl> + if (! s ) { <nl> + * pbuffer = NULL ; <nl> + return 0 ; <nl> + } <nl> + <nl> /* don ' t attempt to pad fixed - size packet buffers */ <nl> if (! s -> max_packet_size ) { <nl> avio_write ( s , padbuf , sizeof ( padbuf ));
typedef struct Jpeg2000TgtNode { <nl> } Jpeg2000TgtNode ; <nl>  <nl> typedef struct Jpeg2000CodingStyle { <nl> - uint8_t nreslevels ; // number of resolution levels <nl> - uint8_t nreslevels2decode ; // number of resolution levels to decode <nl> + int nreslevels ; // number of resolution levels <nl> + int nreslevels2decode ; // number of resolution levels to decode <nl> uint8_t log2_cblk_width , <nl> log2_cblk_height ; // exponent of codeblock size <nl> uint8_t transform ; // DWT type
static int asf_read_metadata_obj ( AVFormatContext * s , const GUIDParseTable * g ) <nl> if (( ret = process_metadata ( s , name , name_len , val_len , type , <nl> & asf -> asf_sd [ st_num ]. asf_met )) < 0 ) <nl> break ; <nl> - } <nl> + } else <nl> + av_freep (& name ); <nl> } <nl> } <nl> 
static int mpeg_decode_slice ( MpegEncContext * s , int mb_y , <nl> } <nl> } <nl> eos : // end of slice <nl> + if ( get_bits_left (& s -> gb ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> * buf += ( get_bits_count (& s -> gb )- 1 )/ 8 ; <nl> av_dlog ( s , " y % d % d % d % d \ n ", s -> resync_mb_x , s -> resync_mb_y , s -> mb_x , s -> mb_y ); <nl> return 0 ;
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> break ; <nl>  <nl> arraylen = avio_rb32 ( ioc ); <nl> + if ( arraylen >> 28 ) <nl> + break ; <nl> + <nl> /* <nl> * Expect only ' times ' or ' filepositions ' sub - arrays in other case refuse to use such metadata <nl> * for indexing
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl>  <nl> if ( x >= w ) { <nl> av_log ( NULL , AV_LOG_ERROR , " run overflow \ n "); <nl> + av_assert0 ( x <= w ); <nl> return ; <nl> } <nl> 
int ff_h264_ref_picture ( H264Context * h , H264Picture * dst , H264Picture * src ) <nl> dst -> poc = src -> poc ; <nl> dst -> frame_num = src -> frame_num ; <nl> dst -> mmco_reset = src -> mmco_reset ; <nl> - dst -> pic_id = src -> pic_id ; <nl> dst -> long_ref = src -> long_ref ; <nl> dst -> mbaff = src -> mbaff ; <nl> dst -> field_picture = src -> field_picture ;
static int matroska_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> return - 1 ; <nl> matroska_execute_seekhead ( matroska ); <nl>  <nl> + if (! matroska -> time_scale ) <nl> + matroska -> time_scale = 1000000 ; <nl> if ( matroska -> duration ) <nl> matroska -> ctx -> duration = matroska -> duration * matroska -> time_scale <nl> * 1000 / AV_TIME_BASE ;
static int kalman_smoothen ( WMAVoiceContext * s , int pitch , <nl> float optimal_gain = 0 , dot ; <nl> const float * ptr = & in [- FFMAX ( s -> min_pitch_val , pitch - 3 )], <nl> * end = & in [- FFMIN ( s -> max_pitch_val , pitch + 3 )], <nl> - * best_hist_ptr ; <nl> + * best_hist_ptr = NULL ; <nl>  <nl> /* find best fitting point in history */ <nl> do {
static int read_gab2_sub ( AVFormatContext * s , AVStream * st , AVPacket * pkt ) <nl> if (! sub_demuxer ) <nl> goto error ; <nl>  <nl> + if ( strcmp ( sub_demuxer -> name , " srt ") && strcmp ( sub_demuxer -> name , " ass ")) <nl> + goto error ; <nl> + <nl> if (!( ast -> sub_ctx = avformat_alloc_context ())) <nl> goto error ; <nl> 
static int rm_read_multi ( AVFormatContext * s , AVIOContext * pb , <nl>  <nl> size2 = avio_rb32 ( pb ); <nl> ret = ff_rm_read_mdpr_codecdata ( s , s -> pb , st2 , st2 -> priv_data , <nl> - size2 , mime ); <nl> + size2 , NULL ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> }
enum AVCodecID av_guess_codec ( AVOutputFormat * fmt , const char * short_name , <nl> enum AVMediaType type ) <nl> { <nl> if ( av_match_name (" segment ", fmt -> name ) || av_match_name (" ssegment ", fmt -> name )) { <nl> - fmt = av_guess_format ( NULL , filename , NULL ); <nl> + AVOutputFormat * fmt2 = av_guess_format ( NULL , filename , NULL ); <nl> + if ( fmt2 ) <nl> + fmt = fmt2 ; <nl> } <nl>  <nl> if ( type == AVMEDIA_TYPE_VIDEO ) {
static int ea_read_header ( AVFormatContext * s , <nl> ea -> audio_codec = 0 ; <nl> } <nl>  <nl> + if ( ea -> sample_rate <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Unsupported sample rate : % d \ n ", ea -> sample_rate ); <nl> + ea -> audio_codec = 0 ; <nl> + } <nl> + <nl> if ( ea -> audio_codec ) { <nl> /* initialize the audio decoder stream */ <nl> st = av_new_stream ( s , 0 );
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> s -> picture_structure = last_pic_structure ; <nl> s -> dropable = last_pic_dropable ; <nl> return AVERROR_INVALIDDATA ; <nl> + } else if (! s -> current_picture_ptr ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , <nl> + " unset current_picture_ptr on % d . slice \ n ", <nl> + h0 -> current_slice + 1 ); <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl> } else { <nl> /* Shorten frame num gaps so we don ' t have to allocate reference
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl>  <nl> if ( hdr -> substreamid == info -> num_ind_sub + 1 ) { <nl> // info -> num_ind_sub ++; <nl> - avpriv_request_sample ( track -> par , " Multiple independent substreams "); <nl> + avpriv_request_sample ( mov -> fc , " Multiple independent substreams "); <nl> ret = AVERROR_PATCHWELCOME ; <nl> goto end ; <nl> } else if ( hdr -> substreamid < info -> num_ind_sub ||
static int decode_cblk ( Jpeg2000DecoderContext * s , Jpeg2000CodingStyle * codsty , <nl> ff_mqc_initdec (& t1 -> mqc , cblk -> data ); <nl>  <nl> while ( passno --) { <nl> + if ( bpno < 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " bpno invalid \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> switch ( pass_t ) { <nl> case 0 : <nl> decode_sigpass ( t1 , width , height , bpno + 1 , bandpos ,
static int opus_decode_packet ( AVCodecContext * avctx , void * data , <nl> memset ( frame -> extended_data [ i ], 0 , frame -> linesize [ 0 ]); <nl> } <nl>  <nl> - if ( c -> gain_i ) { <nl> + if ( c -> gain_i && decoded_samples > 0 ) { <nl> c -> fdsp -> vector_fmul_scalar (( float *) frame -> extended_data [ i ], <nl> ( float *) frame -> extended_data [ i ], <nl> c -> gain , FFALIGN ( decoded_samples , 8 ));
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl> vseq -> seq_fields . bits . direct_8x8_inference_flag = 1 ; <nl> vseq -> seq_fields . bits . log2_max_frame_num_minus4 = 4 ; <nl> vseq -> seq_fields . bits . pic_order_cnt_type = 0 ; <nl> + vseq -> seq_fields . bits . log2_max_pic_order_cnt_lsb_minus4 = <nl> + av_clip ( av_log2 ( avctx -> max_b_frames + 1 ) - 2 , 0 , 12 ); <nl>  <nl> if ( avctx -> width != ctx -> surface_width || <nl> avctx -> height != ctx -> surface_height ) {
static int vdpau_vc1_start_frame ( AVCodecContext * avctx , <nl> else <nl> info -> picture_type = s -> pict_type - 1 + s -> pict_type / 3 ; <nl>  <nl> - info -> frame_coding_mode = v -> fcm ; <nl> + info -> frame_coding_mode = v -> fcm ? v -> fcm + 1 : 0 ; <nl> info -> postprocflag = v -> postprocflag ; <nl> info -> pulldown = v -> broadcast ; <nl> info -> interlace = v -> interlace ;
int ff_alsa_get_device_list ( AVDeviceInfoList * device_list , snd_pcm_stream_t stre <nl> & device_list -> nb_devices , new_device )) < 0 ) { <nl> goto fail ; <nl> } <nl> + if (! strcmp ( new_device -> device_name , " default ")) <nl> + device_list -> default_device = device_list -> nb_devices - 1 ; <nl> new_device = NULL ; <nl> } <nl> fail :
static int qdm2_parse_packet ( AVFormatContext * s , PayloadContext * qdm , <nl> * to the decoder that it is OK to initialize . */ <nl> st -> codec -> codec_id = CODEC_ID_QDM2 ; <nl> } <nl> + if ( st -> codec -> codec_id == CODEC_ID_NONE ) <nl> + return AVERROR ( EAGAIN ); <nl>  <nl> /* subpackets */ <nl> while ( end - p >= 4 ) {
static inline void skip_bits1 ( GetBitContext * s ) <nl> */ <nl> static inline unsigned int get_bits_long ( GetBitContext * s , int n ) <nl> { <nl> + av_assert2 ( n >= 0 && n <= 32 ); <nl> if (! n ) { <nl> return 0 ; <nl> } else if ( n <= MIN_CACHE_BITS ) {
static inline int mpeg2_fast_decode_block_non_intra ( MpegEncContext * s , <nl> } <nl>  <nl> block [ j ] = level ; <nl> - if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF ) <nl> + if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF || i >= 64 ) <nl> break ; <nl> + <nl> UPDATE_CACHE ( re , & s -> gb ); <nl> } <nl> end :
int ff_raw_read_partial_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> - pkt -> size = ret ; <nl> + av_shrink_packet ( pkt , ret ); <nl> return ret ; <nl> } <nl> 
int ff_hevc_output_frame ( HEVCContext * s , AVFrame * out , int flush ) <nl> if (( frame -> flags & HEVC_FRAME_FLAG_OUTPUT ) && <nl> frame -> sequence == s -> seq_output ) { <nl> nb_output ++; <nl> - if ( frame -> poc < min_poc ) { <nl> + if ( frame -> poc < min_poc || nb_output == 1 ) { <nl> min_poc = frame -> poc ; <nl> min_idx = i ; <nl> }
static int flv_set_video_codec ( AVFormatContext * s , AVStream * vstream , int flv_co <nl> vcodec -> codec_id = CODEC_ID_VP6A ; <nl> if ( vcodec -> extradata_size != 1 ) { <nl> vcodec -> extradata_size = 1 ; <nl> - vcodec -> extradata = av_malloc ( 1 ); <nl> + vcodec -> extradata = av_malloc ( 1 + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> } <nl> vcodec -> extradata [ 0 ] = avio_r8 ( s -> pb ); <nl> return 1 ; // 1 byte body size adjustment for flv_read_packet ()
static int truemotion2rt_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if ( avctx -> width / s -> hscale * avctx -> height * s -> delta_size > avpkt -> size * 8LL * 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> ret = init_get_bits8 ( gb , avpkt -> data + ret , avpkt -> size - ret ); <nl> if ( ret < 0 ) <nl> return ret ;
static int amovie_request_frame ( AVFilterLink * outlink ) <nl>  <nl> if ( movie -> is_done ) <nl> return AVERROR_EOF ; <nl> - if (( ret = amovie_get_samples ( outlink )) < 0 ) <nl> - return ret ; <nl> + do { <nl> + if (( ret = amovie_get_samples ( outlink )) < 0 ) <nl> + return ret ; <nl> + } while (! movie -> samplesref ); <nl>  <nl> avfilter_filter_samples ( outlink , avfilter_ref_buffer ( movie -> samplesref , ~ 0 )); <nl> avfilter_unref_buffer ( movie -> samplesref );
int ff_thread_decode_frame ( AVCodecContext * avctx , <nl> FrameThreadContext * fctx = avctx -> internal -> thread_ctx ; <nl> int finished = fctx -> next_finished ; <nl> PerThreadContext * p ; <nl> - int err , ret ; <nl> + int err , ret = 0 ; <nl>  <nl> /* release the async lock , permitting blocked hwaccel threads to <nl> * go forward while we are in this function */
static int decode_nal_units ( H264Context * h , uint8_t * buf , int buf_size ){ <nl> if ( ptr == NULL || dst_length < 0 ){ <nl> return - 1 ; <nl> } <nl> - while ( ptr [ dst_length - 1 ] == 0 && dst_length > 1 ) <nl> + while ( ptr [ dst_length - 1 ] == 0 && dst_length > 0 ) <nl> dst_length --; <nl> bit_length = 8 * dst_length - decode_rbsp_trailing ( h , ptr + dst_length - 1 ); <nl> 
static int ffm_write_packet ( AVFormatContext * s , int stream_index , <nl> /* packet size & key_frame */ <nl> header [ 0 ] = stream_index ; <nl> header [ 1 ] = 0 ; <nl> - if ( st -> codec . coded_picture -> key_frame ) <nl> + if ( st -> codec . coded_picture && st -> codec . coded_picture -> key_frame ) <nl> header [ 1 ] |= FLAG_KEY_FRAME ; <nl> header [ 2 ] = ( size >> 16 ) & 0xff ; <nl> header [ 3 ] = ( size >> 8 ) & 0xff ;
static int encode_superframe ( AVCodecContext * avctx , <nl> } <nl> } <nl>  <nl> + if ( buf_size < 2 * MAX_CODED_SUPERFRAME_SIZE ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " output buffer size is too small \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> # if 1 <nl> total_gain = 128 ; <nl> for ( i = 64 ; i ; i >>= 1 ){
static inline void skip_put_bits ( PutBitContext * s , int n ) <nl> */ <nl> static inline void set_put_bits_buffer_size ( PutBitContext * s , int size ) <nl> { <nl> + av_assert0 ( size <= INT_MAX / 8 - 32 ); <nl> s -> buf_end = s -> buf + size ; <nl> s -> size_in_bits = 8 * size ; <nl> }
static int mpeg_mux_write_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> s -> preload *= 2 ; <nl> } <nl> preload = av_rescale ( s -> preload , 90000 , AV_TIME_BASE ); <nl> + av_log ( ctx , AV_LOG_DEBUG , " First SCR : %" PRId64 " First DTS : %" PRId64 "\ n ", s -> last_scr , dts + preload ); <nl> } <nl>  <nl> if ( dts != AV_NOPTS_VALUE ) dts += preload ;
static int ape_read_header ( AVFormatContext * s ) <nl> return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < ape -> seektablelength / sizeof ( uint32_t ); i ++) <nl> ape -> seektable [ i ] = avio_rl32 ( pb ); <nl> + } else { <nl> + av_log ( s , AV_LOG_ERROR , " Missing seektable \ n "); <nl> + return - 1 ; <nl> } <nl>  <nl> ape -> frames [ 0 ]. pos = ape -> firstframe ;
static int mpc8_probe ( AVProbeData * p ) <nl> size = bs_get_v (& bs ); <nl> if ( size < 2 ) <nl> return 0 ; <nl> - if ( bs + size - 2 >= bs_end ) <nl> + if ( size >= bs_end - bs + 2 ) <nl> return AVPROBE_SCORE_EXTENSION - 1 ; // seems to be valid MPC but no header yet <nl> if ( header_found ) { <nl> if ( size < 11 || size > 28 )
int ff_h263_decode_mb ( MpegEncContext * s , <nl> } <nl>  <nl> if ( IS_DIRECT ( mb_type )){ <nl> + if (! s -> pp_time ) <nl> + return AVERROR_INVALIDDATA ; <nl> s -> mv_dir = MV_DIR_FORWARD | MV_DIR_BACKWARD | MV_DIRECT ; <nl> mb_type |= ff_mpeg4_set_direct_mv ( s , 0 , 0 ); <nl> } else {
static int read_extra_header ( FFV1Context * f ) <nl> } <nl>  <nl> f -> quant_table_count = get_symbol ( c , state , 0 ); <nl> - if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES ) <nl> + if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES || ! f -> quant_table_count ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> for ( i = 0 ; i < f -> quant_table_count ; i ++) {
int ff_h264_execute_ref_pic_marking ( H264Context * h , MMCO * mmco , int mmco_count ) <nl> */ <nl> if ( h -> short_ref_count && h -> short_ref [ 0 ] == h -> cur_pic_ptr ) { <nl> /* Just mark the second field valid */ <nl> - h -> cur_pic_ptr -> reference = PICT_FRAME ; <nl> + h -> cur_pic_ptr -> reference |= h -> picture_structure ; <nl> } else if ( h -> cur_pic_ptr -> long_ref ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " illegal short term reference " <nl> " assignment for second field "
static int set_hwframe_ctx ( AVCodecContext * ctx , AVBufferRef * hw_device_ctx ) <nl> if (( err = av_hwframe_ctx_init ( hw_frames_ref )) < 0 ) { <nl> fprintf ( stderr , " Failed to initialize VAAPI frame context ." <nl> " Error code : % s \ n ", av_err2str ( err )); <nl> + av_buffer_unref (& hw_frames_ref ); <nl> return err ; <nl> } <nl> ctx -> hw_frames_ctx = av_buffer_ref ( hw_frames_ref );
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> // update precincts size : 2 ^ n value <nl> reslevel -> log2_prec_width = codsty -> log2_prec_widths [ reslevelno ]; <nl> reslevel -> log2_prec_height = codsty -> log2_prec_heights [ reslevelno ]; <nl> + if (! reslevel -> log2_prec_width || ! reslevel -> log2_prec_height ) { <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> /* Number of bands for each resolution level */ <nl> if ( reslevelno == 0 )
fail : <nl> } <nl>  <nl> if ( ret < 0 ) { <nl> + if ( codec -> object ) { <nl> + (* env )-> DeleteGlobalRef ( env , codec -> object ); <nl> + } <nl> ff_jni_reset_jfields ( env , & codec -> jfields , jni_amediacodec_mapping , 1 , codec ); <nl> av_freep (& codec ); <nl> }
static inline int get_len ( LZOContext * c , int x , int mask ) <nl> { <nl> int cnt = x & mask ; <nl> if (! cnt ) { <nl> - while (!( x = get_byte ( c ))) <nl> + while (!( x = get_byte ( c ))) { <nl> + if ( cnt >= INT_MAX - 1000 ) { <nl> + c -> error |= AV_LZO_ERROR ; <nl> + break ; <nl> + } <nl> cnt += 255 ; <nl> + } <nl> cnt += mask + x ; <nl> } <nl> return cnt ;
static void estimate_timings_from_bit_rate ( AVFormatContext * ic ) <nl> } <nl> } <nl>  <nl> -# define DURATION_MAX_READ_SIZE 250000 <nl> +# define DURATION_MAX_READ_SIZE 250000LL <nl> # define DURATION_MAX_RETRY 4 <nl>  <nl> /* only usable for MPEG - PS streams */
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
static int aasc_decode_frame ( AVCodecContext * avctx , <nl> AascContext * s = avctx -> priv_data ; <nl> int compr , i , stride , psize ; <nl>  <nl> + if ( buf_size < 4 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " frame too short \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> s -> frame . reference = 3 ; <nl> s -> frame . buffer_hints = FF_BUFFER_HINTS_VALID | FF_BUFFER_HINTS_PRESERVE | FF_BUFFER_HINTS_REUSABLE ; <nl> if ( avctx -> reget_buffer ( avctx , & s -> frame )) {
static int read_matrix_params ( MLPDecodeContext * m , unsigned int substr , GetBitCo <nl> if ( get_bits1 ( gbp )) <nl> coeff_val = get_sbits ( gbp , frac_bits + 2 ); <nl>  <nl> - s -> matrix_coeff [ mat ][ ch ] = coeff_val << ( 14 - frac_bits ); <nl> + s -> matrix_coeff [ mat ][ ch ] = coeff_val * ( 1 << ( 14 - frac_bits )); <nl> } <nl>  <nl> if ( s -> noise_type )
static void adpcm_compress_trellis ( AVCodecContext * avctx , <nl> uint8_t * h ;\ <nl> dec_sample = av_clip_int16 ( dec_sample );\ <nl> d = sample - dec_sample ;\ <nl> - ssd = nodes [ j ]-> ssd + d * d ;\ <nl> + ssd = nodes [ j ]-> ssd + d *( unsigned ) d ;\ <nl> /* Check for wraparound , skip such samples completely . \ <nl> * Note , changing ssd to a 64 bit variable would be \ <nl> * simpler , avoiding this check , but it ' s slower on \
int avformat_open_input ( AVFormatContext ** ps , const char * filename , AVInputForma <nl> { <nl> AVFormatContext * s = * ps ; <nl> int ret = 0 ; <nl> - AVFormatParameters ap = { 0 }; <nl> + AVFormatParameters ap = { { 0 } }; <nl> AVDictionary * tmp = NULL ; <nl>  <nl> if (! s && !( s = avformat_alloc_context ()))
int ff_lzw_decode ( LZWState * p , uint8_t * buf , int len ){ <nl> if ((-- l ) == 0 ) <nl> goto the_end ; <nl> } <nl> + if ( s -> ebuf < s -> pbuf ) { <nl> + av_log ( 0 , AV_LOG_ERROR , " lzw overread \ n "); <nl> + goto the_end ; <nl> + } <nl> c = lzw_get_code ( s ); <nl> if ( c == s -> end_code ) { <nl> break ;
static int initFilter ( int16_t ** outFilter , int16_t ** filterPos , int * outFilterSi <nl> int j ; <nl> (* filterPos )[ i ]= xx ; <nl> for ( j = 0 ; j < filterSize ; j ++) { <nl> - int64_t d = (( int64_t ) FFABS (( xx << 17 ) - xDstInSrc ))<< 13 ; <nl> + int64_t d = ( FFABS ((( int64_t ) xx << 17 ) - xDstInSrc ))<< 13 ; <nl> double floatd ; <nl> int64_t coeff ; <nl> 
static av_cold void uninit ( AVFilterContext * ctx ) <nl>  <nl> if ( select -> do_scene_detect ) { <nl> avfilter_unref_bufferp (& select -> prev_picref ); <nl> - avcodec_close ( select -> avctx ); <nl> - av_freep (& select -> avctx ); <nl> + if ( select -> avctx ) { <nl> + avcodec_close ( select -> avctx ); <nl> + av_freep (& select -> avctx ); <nl> + } <nl> } <nl> } <nl> 
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_INIT_VIDEO_BUFFERS : <nl> av_dlog ( NULL , " initialize video buffers \ n "); <nl> - if (( opcode_version > 2 ) || ( opcode_size > 8 )) { <nl> + if (( opcode_version > 2 ) || ( opcode_size > 8 ) || opcode_size < 4 ) { <nl> av_dlog ( NULL , " bad init_video_buffers opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static void mov_parse_stsd_video ( MOVContext * c , AVIOContext * pb , <nl> if (( color_depth == 2 ) || ( color_depth == 4 ) || ( color_depth == 8 )) { <nl> /* for palette traversal */ <nl> unsigned int color_start , color_count , color_end ; <nl> - unsigned char a , r , g , b ; <nl> + unsigned int a , r , g , b ; <nl>  <nl> if ( color_greyscale ) { <nl> int color_index , color_dec ;
static void copy_bits ( PutBitContext * pb , <nl> rmn_bits = rmn_bytes = get_bits_left ( gb ); <nl> if ( rmn_bits < nbits ) <nl> return ; <nl> + if ( nbits > pb -> size_in_bits - put_bits_count ( pb )) <nl> + return ; <nl> rmn_bits &= 7 ; rmn_bytes >>= 3 ; <nl> if (( rmn_bits = FFMIN ( rmn_bits , nbits )) > 0 ) <nl> put_bits ( pb , rmn_bits , get_bits ( gb , rmn_bits ));
av_cold int ff_h264_decode_init ( AVCodecContext * avctx ) <nl>  <nl> if ( avctx -> extradata_size > 0 && avctx -> extradata ) { <nl> ret = ff_h264_decode_extradata ( h ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + ff_h264_free_context ( h ); <nl> return ret ; <nl> + } <nl> } <nl>  <nl> if ( h -> sps . bitstream_restriction_flag &&
static void frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> pthread_cond_signal (& p -> input_cond ); <nl> pthread_mutex_unlock (& p -> mutex ); <nl>  <nl> - pthread_join ( p -> thread , NULL ); <nl> + if ( p -> thread ) <nl> + pthread_join ( p -> thread , NULL ); <nl>  <nl> if ( codec -> close ) <nl> codec -> close ( p -> avctx );
static void vc1_decode_b_mb_intfi ( VC1Context * v ) <nl> int fwd ; <nl> int dmv_x [ 2 ], dmv_y [ 2 ], pred_flag [ 2 ]; <nl> int bmvtype = BMV_TYPE_BACKWARD ; <nl> - int idx_mbmode , interpmvp ; <nl> + int idx_mbmode ; <nl> + int av_uninit ( interpmvp ); <nl>  <nl> mquant = v -> pq ; /* Lossy initialization */ <nl> s -> mb_intra = 0 ;
typedef struct { <nl> static int probe ( AVProbeData * p ) <nl> { <nl> if ( AV_RL16 ( p -> buf ) == 0 && AV_RL16 ( p -> buf + 2 ) == 1 && AV_RL16 ( p -> buf + 4 )) <nl> - return AVPROBE_SCORE_MAX / 3 ; <nl> + return AVPROBE_SCORE_MAX / 4 ; <nl> return 0 ; <nl> } <nl> 
static void alac_linear_predictor ( AlacEncodeContext * s , int ch ) <nl>  <nl> sum >>= lpc . lpc_quant ; <nl> sum += samples [ 0 ]; <nl> - residual [ i ] = samples [ lpc . lpc_order + 1 ] - sum ; <nl> + residual [ i ] = ( samples [ lpc . lpc_order + 1 ] - sum ) << ( 32 - s -> write_sample_size ) >> <nl> + ( 32 - s -> write_sample_size ); <nl> res_val = residual [ i ]; <nl>  <nl> if ( res_val ) {
static void rv34_pred_4x4_block ( RV34DecContext * r , uint8_t * dst , int stride , int <nl> if ( itype == VERT_LEFT_PRED ) itype = VERT_LEFT_PRED_RV40_NODOWN ; <nl> } <nl> if (! right && up ){ <nl> - topleft = dst [- stride + 3 ] * 0x01010101 ; <nl> + topleft = dst [- stride + 3 ] * 0x01010101u ; <nl> prev = ( uint8_t *)& topleft ; <nl> } <nl> r -> h . pred4x4 [ itype ]( dst , prev , stride );
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl> } else { <nl> int i ; <nl> for ( i = 0 ; SHOW_UBITS ( re , gb , 1 ) == 0 ; i ++){ <nl> - if ( get_bits_left ( gb )<= 0 ) <nl> + if ( gb -> size_in_bits <= re_index ) <nl> return - 1 ; <nl> LAST_SKIP_BITS ( re , gb , 1 ); <nl> UPDATE_CACHE ( re , gb );
static int seek_test ( const char * input_filename , const char * start , const char * <nl> return AVERROR ( ENOMEM ); <nl> } <nl>  <nl> - result = compute_crc_of_packets ( fmt_ctx , video_stream , ctx , fr , i , j , 1 ); <nl> + result = compute_crc_of_packets ( fmt_ctx , video_stream , ctx , fr , 0 , 0 , 1 ); <nl> if ( result != 0 ) <nl> return - 1 ; <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> if (! avctx -> rc_initial_buffer_occupancy ) <nl> - avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3 / 4 ; <nl> + avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3LL / 4 ; <nl>  <nl> if ( avctx -> ticks_per_frame && avctx -> time_base . num && <nl> avctx -> ticks_per_frame > INT_MAX / avctx -> time_base . num ) {
static int mxf_read_close ( AVFormatContext * s ) <nl> case MaterialPackage : <nl> av_freep (&(( MXFPackage *) mxf -> metadata_sets [ i ])-> tracks_refs ); <nl> break ; <nl> + case Track : <nl> + mxf -> metadata_sets [ i ] = NULL ; /* will be freed later */ <nl> + break ; <nl> default : <nl> break ; <nl> }
static inline void encode_vlc_codeword ( PutBitContext * pb , unsigned codebook , int <nl> exponent = av_log2 ( val ); <nl>  <nl> put_bits ( pb , exponent - exp_order + switch_bits , 0 ); <nl> - put_bits ( pb , 1 , 1 ); <nl> - put_bits ( pb , exponent , val ); <nl> + put_bits ( pb , exponent + 1 , val ); <nl> } else { <nl> exponent = val >> rice_order ; <nl> 
static av_cold int libwebp_anim_encode_init ( AVCodecContext * avctx ) <nl> int ret = ff_libwebp_encode_init_common ( avctx ); <nl> if (! ret ) { <nl> LibWebPAnimContext * s = avctx -> priv_data ; <nl> - WebPAnimEncoderOptions enc_options ; <nl> + WebPAnimEncoderOptions enc_options = { 0 }; <nl> WebPAnimEncoderOptionsInit (& enc_options ); <nl> // TODO ( urvang ): Expose some options on command - line perhaps . <nl> s -> enc = WebPAnimEncoderNew ( avctx -> width , avctx -> height , & enc_options );
static int aiff_read_packet ( AVFormatContext * s , <nl> if ( max_size <= 0 ) <nl> return AVERROR_EOF ; <nl>  <nl> + if (! st -> codecpar -> block_align ) { <nl> + av_log ( s , AV_LOG_ERROR , " block_align not set \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* Now for that packet */ <nl> switch ( st -> codecpar -> codec_id ) { <nl> case AV_CODEC_ID_ADPCM_IMA_QT :
int ff_thread_init ( AVCodecContext * s ){ <nl> return 0 ; <nl> } <nl>  <nl> - s -> active_thread_type = FF_THREAD_SLICE ; <nl> - <nl> if ( s -> thread_count <= 1 ) <nl> return 0 ; <nl>  <nl> + s -> active_thread_type = FF_THREAD_SLICE ; <nl> + <nl> assert (! s -> thread_opaque ); <nl> c = av_mallocz ( sizeof ( ThreadContext )* s -> thread_count ); <nl> s -> thread_opaque = c ;
static int fileTest ( uint8_t * ref [ 4 ], int refStride [ 4 ], int w , int h , FILE * fp , <nl> struct Results r ; <nl> enum AVPixelFormat srcFormat ; <nl> char srcStr [ 12 ]; <nl> - int srcW , srcH ; <nl> + int srcW = 0 , srcH = 0 ; <nl> enum AVPixelFormat dstFormat ; <nl> char dstStr [ 12 ]; <nl> - int dstW , dstH ; <nl> + int dstW = 0 , dstH = 0 ; <nl> int flags ; <nl> int ret ; <nl> 
static void mov_fix_index ( MOVContext * mov , AVStream * st ) <nl> int first_non_zero_audio_edit = - 1 ; <nl> int packet_skip_samples = 0 ; <nl>  <nl> - if (! msc -> elst_data || msc -> elst_count <= 0 ) { <nl> + if (! msc -> elst_data || msc -> elst_count <= 0 || nb_old <= 0 ) { <nl> return ; <nl> } <nl> // Clean AVStream from traces of old index
AVFilterFormats * avfilter_merge_formats ( AVFilterFormats * a , AVFilterFormats * b ) <nl> AVFilterFormats * ret ; <nl> unsigned i , j , k = 0 , m_count ; <nl>  <nl> + if ( a == b ) <nl> + return a ; <nl> + <nl> ret = av_mallocz ( sizeof (* ret )); <nl>  <nl> /* merge list of formats */
static int amv_encode_picture ( AVCodecContext * avctx , AVPacket * pkt , <nl> return - 1 ; <nl>  <nl> pic = av_frame_alloc (); <nl> + if (! pic ) <nl> + return AVERROR ( ENOMEM ); <nl> av_frame_ref ( pic , pic_arg ); <nl> // picture should be flipped upside - down <nl> for ( i = 0 ; i < 3 ; i ++) {
static void av_update_stream_timings ( AVFormatContext * ic ) <nl> duration = INT64_MIN ; <nl> for ( i = 0 ; i < ic -> nb_streams ; i ++) { <nl> st = ic -> streams [ i ]; <nl> - if ( st -> start_time != AV_NOPTS_VALUE ) { <nl> + if ( st -> start_time != AV_NOPTS_VALUE && st -> time_base . den ) { <nl> start_time1 = av_rescale_q ( st -> start_time , st -> time_base , AV_TIME_BASE_Q ); <nl> if ( start_time1 < start_time ) <nl> start_time = start_time1 ;
static int videotoolbox_buffer_create ( AVCodecContext * avctx , AVFrame * frame ) <nl> vtctx -> cached_hw_frames_ctx = hw_frames_ctx ; <nl> } <nl>  <nl> - av_assert0 (! frame -> hw_frames_ctx ); <nl> + av_buffer_unref (& frame -> hw_frames_ctx ); <nl> frame -> hw_frames_ctx = av_buffer_ref ( vtctx -> cached_hw_frames_ctx ); <nl> if (! frame -> hw_frames_ctx ) <nl> return AVERROR ( ENOMEM );
static int h263p_decode_umotion ( MpegEncContext * s , int pred ) <nl> code += get_bits1 (& s -> gb ); <nl> if ( code >= 32768 ) { <nl> avpriv_request_sample ( s -> avctx , " Huge DMV "); <nl> - return AVERROR_INVALIDDATA ; <nl> + return 0xffff ; <nl> } <nl> } <nl> sign = code & 1 ;
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> av_log ( s , AV_LOG_ERROR , " No seek table at given position \ n "); <nl> return ; <nl> } <nl> + if ( size < 0 || size >= INT_MAX / 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Bad seek table size \ n "); <nl> + return ; <nl> + } <nl> if (!( buf = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ))) <nl> return ; <nl> avio_read ( s -> pb , buf , size );
static int ism_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> SmoothStreamingContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ pkt -> stream_index ]; <nl> - int64_t end_dts = ( c -> nb_fragments + 1 ) * c -> min_frag_duration ; <nl> + int64_t end_dts = ( c -> nb_fragments + 1 ) * ( int64_t ) c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int read_var_block_data ( ALSDecContext * ctx , ALSBlockData * bd ) <nl>  <nl>  <nl> if (! sconf -> rlslms ) { <nl> - if ( sconf -> adapt_order ) { <nl> + if ( sconf -> adapt_order && sconf -> max_order ) { <nl> int opt_order_length = av_ceil_log2 ( av_clip (( bd -> block_length >> 3 ) - 1 , <nl> 2 , sconf -> max_order + 1 )); <nl> * bd -> opt_order = get_bits ( gb , opt_order_length );
static int raw_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> & pal_size ); <nl> int ret ; <nl>  <nl> - if ( pal_size != AVPALETTE_SIZE ) { <nl> + if ( pal && pal_size != AVPALETTE_SIZE ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Palette size % d is wrong \ n ", pal_size ); <nl> pal = NULL ; <nl> }
static int get_channel ( char ** map , uint64_t * ch , char delim ) <nl> static av_cold int channelmap_init ( AVFilterContext * ctx ) <nl> { <nl> ChannelMapContext * s = ctx -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> char * mapping , separator = '|'; <nl> int map_entries = 0 ; <nl> char buf [ 256 ];
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> AVFrame * const p = data ; <nl> int mb_x , mb_y , ret ; <nl>  <nl> + if ( buf_size * 8LL < a -> mb_height2 * a -> mb_width2 * 13LL ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 ) <nl> return ret ; <nl> p -> pict_type = AV_PICTURE_TYPE_I ;
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl>  <nl> if ( frames_hwctx -> texture ) <nl> ID3D11Texture2D_Release ( frames_hwctx -> texture ); <nl> + frames_hwctx -> texture = NULL ; <nl>  <nl> if ( s -> staging_texture ) <nl> ID3D11Texture2D_Release ( s -> staging_texture ); <nl> + s -> staging_texture = NULL ; <nl> } <nl>  <nl> static void free_texture ( void * opaque , uint8_t * data )
static void add_pid_to_pmt ( MpegTSContext * ts , unsigned int programid , <nl> if ( p -> nb_pids >= MAX_PIDS_PER_PROGRAM ) <nl> return ; <nl>  <nl> - for ( i = 0 ; i < MAX_PIDS_PER_PROGRAM ; i ++) <nl> + for ( i = 0 ; i < p -> nb_pids ; i ++) <nl> if ( p -> pids [ i ] == pid ) <nl> return ; <nl> 
static int vaapi_encode_h264_write_extra_header ( AVCodecContext * avctx , <nl>  <nl> if ( priv -> sei_needed ) { <nl> if ( priv -> aud_needed ) { <nl> - vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + err = vaapi_encode_h264_add_nal ( avctx , au , & priv -> aud ); <nl> + if ( err < 0 ) <nl> + goto fail ; <nl> priv -> aud_needed = 0 ; <nl> } <nl> 
static int nist_read_header ( AVFormatContext * s ) <nl> { <nl> char buffer [ 32 ], coding [ 32 ] = " pcm ", format [ 32 ] = " 01 "; <nl> int bps = 0 , be = 0 ; <nl> - int32_t header_size ; <nl> + int32_t header_size = - 1 ; <nl> AVStream * st ; <nl>  <nl> st = avformat_new_stream ( s , NULL );
static int query_formats ( AVFilterContext * ctx ) <nl> main_formats = ff_make_format_list ( main_pix_fmts_rgb ); <nl> overlay_formats = ff_make_format_list ( overlay_pix_fmts_rgb ); <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl>  <nl> ff_formats_ref ( main_formats , & ctx -> inputs [ MAIN ]-> out_formats );
static int read_thread ( void * arg ) <nl> stream_component_close ( is , is -> video_stream ); <nl> if ( is -> subtitle_stream >= 0 ) <nl> stream_component_close ( is , is -> subtitle_stream ); <nl> - if ( is -> ic ) { <nl> - avformat_close_input (& is -> ic ); <nl> + if ( ic ) { <nl> + avformat_close_input (& ic ); <nl> + is -> ic = NULL ; <nl> } <nl>  <nl> if ( ret != 0 ) {
static int ipvideo_decode_block_opcode_0x9 ( IpvideoContext * s , AVFrame * frame ) <nl> int x , y ; <nl> unsigned char P [ 4 ]; <nl>  <nl> + if ( bytestream2_get_bytes_left (& s -> stream_ptr ) < 8 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too little data for opcode 0x9 \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> /* 4 - color encoding */ <nl> bytestream2_get_buffer (& s -> stream_ptr , P , 4 ); <nl> 
static int mpeg4_decode_sprite_trajectory ( Mpeg4DecContext * ctx , GetBitContext * g <nl> int a = 2 << s -> sprite_warping_accuracy ; <nl> int rho = 3 - s -> sprite_warping_accuracy ; <nl> int r = 16 / a ; <nl> - int alpha = 0 ; <nl> + int alpha = 1 ; <nl> int beta = 0 ; <nl> int w = s -> width ; <nl> int h = s -> height ;
static int decode_bmv_frame ( const uint8_t * source , int src_len , uint8_t * frame , <nl> mode += 1 + advance_mode ; <nl> if ( mode >= 4 ) <nl> mode -= 3 ; <nl> - if ( FFABS ( dst_end - dst ) < len ) <nl> + if ( len <= 0 || FFABS ( dst_end - dst ) < len ) <nl> return AVERROR_INVALIDDATA ; <nl> switch ( mode ) { <nl> case 1 :
PCA * ff_pca_init ( int n ){ <nl> if ( n <= 0 ) <nl> return NULL ; <nl>  <nl> - pca = av_mallocz ( sizeof ( PCA )); <nl> + pca = av_mallocz ( sizeof (* pca )); <nl> pca -> n = n ; <nl> pca -> z = av_malloc ( sizeof (* pca -> z ) * n ); <nl> pca -> count = 0 ;
static int v4l2_set_parameters ( AVFormatContext * s1 ) <nl> standard . index = i ; <nl> if ( v4l2_ioctl ( s -> fd , VIDIOC_ENUMSTD , & standard ) < 0 ) { <nl> ret = AVERROR ( errno ); <nl> + if ( ret == AVERROR ( EINVAL )) { <nl> + tpf = & streamparm . parm . capture . timeperframe ; <nl> + break ; <nl> + } <nl> av_log ( s1 , AV_LOG_ERROR , " ioctl ( VIDIOC_ENUMSTD ): % s \ n ", av_err2str ( ret )); <nl> return ret ; <nl> }
static int img_read_header ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl>  <nl> st = av_new_stream ( s1 , 0 ); <nl> if (! st ) { <nl> - av_free ( s ); <nl> return - ENOMEM ; <nl> } <nl>  <nl> static int img_read_header ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl> if (! s -> is_pipe ) <nl> url_fclose ( f ); <nl> fail : <nl> - av_free ( s ); <nl> return AVERROR_IO ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> avio_flush ( probe_out ); <nl> av_freep (& probe_out ); <nl> av_freep (& buffer ); <nl> - <nl> + uninit_opts (); <nl> avformat_network_deinit (); <nl>  <nl> return ret ;
again : <nl> hx -> inter_gb_ptr = & hx -> inter_gb ; <nl>  <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " Partitioned H . 264 support is incomplete \ n "); <nl> - return AVERROR_PATCHWELCOME ; <nl> + break ; <nl>  <nl> if ( hx -> redundant_pic_count == 0 && <nl> hx -> intra_gb_ptr &&
static int rsd_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> par -> channels = avio_rl32 ( pb ); <nl> - if (! par -> channels ) <nl> + if ( par -> channels <= 0 || par -> channels > INT_MAX / 36 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels : % d \ n ", par -> channels ); <nl> return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> avio_skip ( pb , 4 ); // Bit depth <nl> par -> sample_rate = avio_rl32 ( pb );
static void ffmpeg_cleanup ( int ret ) <nl> av_freep (& ost -> audio_channels_map ); <nl> ost -> audio_channels_mapped = 0 ; <nl>  <nl> + av_dict_free (& ost -> sws_dict ); <nl> + <nl> avcodec_free_context (& ost -> enc_ctx ); <nl>  <nl> av_freep (& output_streams [ i ]);
int ff_h264_queue_decode_slice ( H264Context * h , const H2645NAL * nal ) <nl> return ret ; <nl>  <nl> // discard redundant pictures <nl> - if ( sl -> redundant_pic_count > 0 ) <nl> + if ( sl -> redundant_pic_count > 0 ) { <nl> + sl -> ref_count [ 0 ] = sl -> ref_count [ 1 ] = 0 ; <nl> return 0 ; <nl> + } <nl>  <nl> if ( sl -> first_mb_addr == 0 || ! h -> current_slice ) { <nl> if ( h -> setup_finished ) {
static int filter_packet ( void * log_ctx , AVPacket * pkt , <nl> } <nl>  <nl> if ( ret > 0 ) { <nl> + pkt -> side_data = NULL ; <nl> + pkt -> side_data_elems = 0 ; <nl> av_packet_unref ( pkt ); <nl> new_pkt . buf = av_buffer_create ( new_pkt . data , new_pkt . size , <nl> av_buffer_default_free , NULL , 0 );
static int dca_parse ( AVCodecParserContext * s , AVCodecContext * avctx , <nl> /* read the duration and sample rate from the frame header */ <nl> if (! dca_parse_params ( buf , buf_size , & duration , & sample_rate , & pc1 -> framesize )) { <nl> s -> duration = duration ; <nl> - avctx -> sample_rate = sample_rate ; <nl> } else <nl> s -> duration = 0 ; <nl> 
static int encode_init ( AVCodecContext * avctx ){ <nl> if ( avctx -> channels > MAX_CHANNELS ) <nl> return - 1 ; <nl>  <nl> + if ( avctx -> bit_rate < 24 * 1000 ) <nl> + return - 1 ; <nl> + <nl> /* extract flag infos */ <nl> flags1 = 0 ; <nl> flags2 = 1 ;
# define FELEM_MIN INT16_MIN <nl> # define WINDOW_TYPE 9 <nl> # elif ! defined ( CONFIG_RESAMPLE_AUDIOPHILE_KIDDY_MODE ) <nl> -# define FILTER_SHIFT 30 <nl> +# define FILTER_SHIFT 22 <nl>  <nl> # define FELEM int32_t <nl> # define FELEM2 int64_t
end : <nl> free_and_end : <nl> av_dict_free (& tmp ); <nl> av_freep (& avctx -> priv_data ); <nl> - if ( avctx -> internal ) <nl> + if ( avctx -> internal ) { <nl> + av_frame_free (& avctx -> internal -> to_free ); <nl> av_freep (& avctx -> internal -> pool ); <nl> + } <nl> av_freep (& avctx -> internal ); <nl> avctx -> codec = NULL ; <nl> goto end ;
unsigned int avpriv_toupper4 ( unsigned int x ) <nl> return av_toupper ( x & 0xFF ) + <nl> ( av_toupper (( x >> 8 ) & 0xFF ) << 8 ) + <nl> ( av_toupper (( x >> 16 ) & 0xFF ) << 16 ) + <nl> - ( av_toupper (( x >> 24 ) & 0xFF ) << 24 ); <nl> +(( unsigned ) av_toupper (( x >> 24 ) & 0xFF ) << 24 ); <nl> } <nl>  <nl> int ff_thread_ref_frame ( ThreadFrame * dst , ThreadFrame * src )
static int twolame_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> if ( ret < 0 ) // twolame error <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame ) { <nl> + avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame -> pts != AV_NOPTS_VALUE ) <nl> avpkt -> pts = frame -> pts - ff_samples_to_time_base ( avctx , avctx -> initial_padding ); <nl> } else {
static int pmp_header ( AVFormatContext * s ) <nl> avio_skip ( pb , 10 ); <nl> srate = avio_rl32 ( pb ); <nl> channels = avio_rl32 ( pb ) + 1 ; <nl> - pos = avio_tell ( pb ) + 4 * index_cnt ; <nl> + pos = avio_tell ( pb ) + 4LL * index_cnt ; <nl> for ( i = 0 ; i < index_cnt ; i ++) { <nl> uint32_t size = avio_rl32 ( pb ); <nl> int flags = size & 1 ? AVINDEX_KEYFRAME : 0 ;
static int init_input ( AVFormatContext * s , const char * filename ) <nl> int ret ; <nl> AVProbeData pd = { filename , NULL , 0 }; <nl>  <nl> + if ( s -> iformat && ! strlen ( filename )) <nl> + return 0 ; <nl> + <nl> if ( s -> pb ) { <nl> s -> flags |= AVFMT_FLAG_CUSTOM_IO ; <nl> if (! s -> iformat )
static int read_packet ( AVFormatContext * s , uint8_t * buf , int raw_packet_size ) <nl> static int handle_packets ( MpegTSContext * ts , int nb_packets ) <nl> { <nl> AVFormatContext * s = ts -> stream ; <nl> - uint8_t packet [ TS_PACKET_SIZE ]; <nl> + uint8_t packet [ TS_PACKET_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> int packet_num , ret = 0 ; <nl>  <nl> if ( avio_tell ( s -> pb ) != ts -> last_pos ) {
static int ape_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl>  <nl> if ( ape -> seektablelength > 0 ) { <nl> ape -> seektable = av_malloc ( ape -> seektablelength ); <nl> + if (! ape -> seektable ) <nl> + return AVERROR ( ENOMEM ); <nl> for ( i = 0 ; i < ape -> seektablelength / sizeof ( uint32_t ); i ++) <nl> ape -> seektable [ i ] = avio_rl32 ( pb ); <nl> }
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + case NAL_IDR_SLICE : <nl> + case NAL_SLICE : <nl> nals_needed = nal_index ; <nl> } <nl> continue ;
static int concat_read_packet ( AVFormatContext * avf , AVPacket * pkt ) <nl> ( ret = open_next_file ( avf )) < 0 ) <nl> break ; <nl> } <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> delta = av_rescale_q ( cat -> cur_file -> start_time - cat -> avf -> start_time , <nl> AV_TIME_BASE_Q , <nl> cat -> avf -> streams [ pkt -> stream_index ]-> time_base );
static int ff_vp56_decode_mbs ( AVCodecContext * avctx , void * data , <nl> int ret = vp56_decode_mb ( s , mb_row , mb_col , is_alpha ); <nl> if ( ret < 0 ) { <nl> damaged = 1 ; <nl> - if (! s -> have_undamaged_frame ) { <nl> + if (! s -> have_undamaged_frame || ! avctx -> error_concealment ) { <nl> s -> discard_frame = 1 ; <nl> return AVERROR_INVALIDDATA ; <nl> }
static int find_video_stream_info ( AVFormatContext * fmt_ctx , int decode ) <nl> end : <nl> av_packet_unref (& pkt ); <nl>  <nl> + /* close all codecs opened in try_decode_video_frame */ <nl> + for ( i = 0 ; i < fmt_ctx -> nb_streams ; i ++) { <nl> + AVStream * st = fmt_ctx -> streams [ i ]; <nl> + avcodec_close ( st -> codec ); <nl> + } <nl> + <nl> return ret < 0 ; <nl> } <nl> 
int ff_jpegls_decode_picture ( MJpegDecodeContext * s , int near , <nl> else <nl> shift = point_transform + ( 16 - s -> bits ); <nl>  <nl> + if ( shift >= 16 ) { <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto end ; <nl> + } <nl> + <nl> if ( s -> avctx -> debug & FF_DEBUG_PICT_INFO ) { <nl> av_log ( s -> avctx , AV_LOG_DEBUG , <nl> " JPEG - LS params : % ix % i NEAR =% i MV =% i T (% i ,% i ,% i ) "
static int mjpeg_decode_scan_progressive_ac ( MJpegDecodeContext * s , int ss , <nl> } <nl>  <nl> if (! Al ) { <nl> - s -> coefs_finished [ c ] |= ( 2LL << se ) - ( 1LL << ss ); <nl> + s -> coefs_finished [ c ] |= ( 2ULL << se ) - ( 1ULL << ss ); <nl> last_scan = !~ s -> coefs_finished [ c ]; <nl> } <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> s -> frames_out ++; <nl> if ( j > 0 ) s -> dup ++; <nl> } <nl> + av_frame_free (& buf ); <nl> } else { <nl> /* for delta less or equal to 0 , we should drop the frame , <nl> * otherwise , we will have one or more extra frames */
static inline int get_ur_golomb_jpegls ( GetBitContext * gb , int k , int limit , int <nl> } else { <nl> int i ; <nl> for ( i = 0 ; SHOW_UBITS ( re , gb , 1 ) == 0 ; i ++){ <nl> + if ( get_bits_left ( gb )<= 0 ) <nl> + return - 1 ; <nl> LAST_SKIP_BITS ( re , gb , 1 ); <nl> UPDATE_CACHE ( re , gb ); <nl> }
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> break ; <nl> default : <nl> av_log ( c , AV_LOG_ERROR , " Unsupported pixel format .\ n "); <nl> + av_free ( config ); <nl> return NULL ; <nl> } <nl> 
static int config_input ( AVFilterLink * inlink ) <nl> int hsub = desc -> log2_chroma_w ; <nl> int vsub = desc -> log2_chroma_h ; <nl>  <nl> + av_freep (& s -> buf ); <nl> s -> buf = av_mallocz (( FFALIGN ( inlink -> w , 16 ) * ( s -> radius + 1 ) / 2 + 32 ) * sizeof ( uint16_t )); <nl> if (! s -> buf ) <nl> return AVERROR ( ENOMEM );
static int xpm_decode_frame ( AVCodecContext * avctx , void * data , <nl> avctx -> pix_fmt = AV_PIX_FMT_BGRA ; <nl>  <nl> end = avpkt -> data + avpkt -> size ; <nl> - while ( memcmp ( ptr , "/* XPM */\ n ", 10 ) && ptr < end - 10 ) <nl> + while ( memcmp ( ptr , "/* XPM */", 9 ) && ptr < end - 9 ) <nl> ptr ++; <nl>  <nl> if ( ptr >= end ) {
int ff_j2k_dwt_init ( DWTContext * s , uint16_t border [ 2 ][ 2 ], int decomp_levels , int <nl> int i , j , lev = decomp_levels , maxlen , <nl> b [ 2 ][ 2 ]; <nl>  <nl> - if ( decomp_levels >= FF_DWT_MAX_DECLVLS ) <nl> + if (( unsigned ) decomp_levels >= FF_DWT_MAX_DECLVLS ) <nl> return AVERROR_INVALIDDATA ; <nl> s -> ndeclevels = decomp_levels ; <nl> s -> type = type ;
static char * value_string ( char * buf , int buf_size , struct unit_value uv ) <nl> const char * prefix_string = ""; <nl> int l ; <nl>  <nl> - if ( use_value_prefix ) { <nl> + if ( use_value_prefix && vald > 1 ) { <nl> long long int index ; <nl>  <nl> if ( uv . unit == unit_byte_str && use_byte_value_binary_prefix ) {
static int amr_read_packet ( AVFormatContext * s , <nl> AVPacket * pkt ) <nl> { <nl> AVCodecContext * enc = s -> streams [ 0 ]-> codec ; <nl> - int read , size , toc , mode ; <nl> + int read , size = 0 , toc , mode ; <nl>  <nl> if ( url_feof (& s -> pb )) <nl> {
static int pcm_dvd_parse_header ( AVCodecContext * avctx , const uint8_t * header ) <nl> /* early exit if the header didn ' t change apart from the frame number */ <nl> if ( s -> last_header == header_int ) <nl> return 0 ; <nl> + s -> last_header = - 1 ; <nl>  <nl> if ( avctx -> debug & FF_DEBUG_PICT_INFO ) <nl> av_dlog ( avctx , " pcm_dvd_parse_header : header = % 02x % 02x % 02x \ n ",
static int vobsub_read_header ( AVFormatContext * s ) <nl>  <nl> while (* p == ' ') <nl> p ++; <nl> - av_log ( s , AV_LOG_DEBUG , " IDX stream [% d ] name =% s \ n ", st -> id , p ); <nl> + av_log ( s , AV_LOG_DEBUG , " IDX stream [% d ] name =% s \ n ", stream_id , p ); <nl> av_strlcpy ( alt , p , sizeof ( alt )); <nl> header_parsed = 1 ; <nl> 
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_free_packet ( pkt ); <nl> return ret ; <nl> } <nl> + av_shrink_packet ( pkt , CDXL_HEADER_SIZE + ret ); <nl> pkt -> stream_index = cdxl -> video_stream_index ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> pkt -> pos = pos ;
static av_always_inline av_const int avpriv_isnan ( double x ) <nl> uint64_t v = av_double2int ( x ); <nl> if (( v & 0x7ff0000000000000 ) != 0x7ff0000000000000 ) <nl> return 0 ; <nl> - return v & 0x000fffffffffffff ; <nl> + return ( v & 0x000fffffffffffff ) && 1 ; <nl> } <nl>  <nl> # define isnan ( x ) \
static int rtp_mpegts_write_header ( AVFormatContext * s ) <nl> st -> time_base . num = 1 ; <nl> st -> time_base . den = 90000 ; <nl> st -> codec -> codec_id = AV_CODEC_ID_MPEG2TS ; <nl> - chain -> rtp_ctx = rtp_ctx ; <nl> rtp_ctx -> pb = s -> pb ; <nl> if (( ret = avformat_write_header ( rtp_ctx , NULL )) < 0 ) <nl> goto fail ; <nl> - rtp_ctx = NULL ; <nl> + chain -> rtp_ctx = rtp_ctx ; <nl>  <nl> return 0 ; <nl> 
static av_cold int truespeech_decode_init ( AVCodecContext * avctx ) <nl> { <nl> // TSContext * c = avctx -> priv_data ; <nl>  <nl> + if ( avctx -> channels != 1 ) { <nl> + av_log_ask_for_sample ( avctx , " Unsupported channel count : % d \ n ", avctx -> channels ); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ; <nl> return 0 ; <nl> }
static inline int wnv1_get_code ( WNV1Context * w , int base_value ) <nl> if ( v == 15 ) <nl> return ff_reverse [ get_bits (& w -> gb , 8 - w -> shift )]; <nl> else <nl> - return base_value + (( v - 7 ) << w -> shift ); <nl> + return base_value + (( v - 7U ) << w -> shift ); <nl> } <nl>  <nl> static int decode_frame ( AVCodecContext * avctx ,
static int bfi_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> return ret ; <nl>  <nl> pkt -> pts = bfi -> video_frame ; <nl> - bfi -> video_frame += ret / bfi -> video_size ; <nl> + bfi -> video_frame += bfi -> video_size ? ret / bfi -> video_size : 1 ; <nl>  <nl> /* One less frame to read . A cursory decrement . */ <nl> bfi -> nframes --;
static int init_image ( TiffContext * s , AVFrame * frame ) <nl> case 161 : <nl> s -> avctx -> pix_fmt = s -> le ? AV_PIX_FMT_GRAY16LE : AV_PIX_FMT_GRAY16BE ; <nl> break ; <nl> + case 162 : <nl> + s -> avctx -> pix_fmt = AV_PIX_FMT_YA8 ; <nl> + break ; <nl> case 322 : <nl> s -> avctx -> pix_fmt = s -> le ? AV_PIX_FMT_YA16LE : AV_PIX_FMT_YA16BE ; <nl> break ;
static void update_stream_timings ( AVFormatContext * ic ) <nl> end_time1 = av_rescale_q_rnd ( st -> duration , st -> time_base , <nl> AV_TIME_BASE_Q , <nl> AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX ); <nl> - if ( end_time1 != AV_NOPTS_VALUE && start_time1 <= INT64_MAX - end_time1 ) { <nl> + if ( end_time1 != AV_NOPTS_VALUE && ( end_time1 > 0 ? start_time1 <= INT64_MAX - end_time1 : start_time1 >= INT64_MIN - end_time1 )) { <nl> end_time1 += start_time1 ; <nl> end_time = FFMAX ( end_time , end_time1 ); <nl> }
static int opus_header ( AVFormatContext * avf , int idx ) <nl> /* gain = AV_RL16 ( packet + 16 );*/ <nl> /* channel_map = AV_RL8 ( packet + 18 );*/ <nl>  <nl> + av_freep (& st -> codecpar -> extradata ); <nl> if ( ff_alloc_extradata ( st -> codecpar , os -> psize )) <nl> return AVERROR ( ENOMEM ); <nl> 
static int get_video_frame ( VideoState * is , AVFrame * frame , int64_t * pts , AVPacke <nl> return 0 ; <nl> } <nl>  <nl> - avcodec_decode_video2 ( is -> video_st -> codec , frame , & got_picture , pkt ); <nl> + if ( avcodec_decode_video2 ( is -> video_st -> codec , frame , & got_picture , pkt ) < 0 ) <nl> + return - 1 ; <nl>  <nl> if ( got_picture ) { <nl> int ret = 1 ;
void mpeg1_init_vlc ( MpegEncContext * s ) <nl> static int done = 0 ; <nl>  <nl> if (! done ) { <nl> + done = 1 ; <nl>  <nl> init_vlc (& dc_lum_vlc , 9 , 12 , <nl> vlc_dc_lum_bits , 1 , 1 ,
static int load_input_picture ( MpegEncContext * s , const AVFrame * pic_arg ) <nl> EDGE_BOTTOM ); <nl> } <nl> } <nl> + emms_c (); <nl> } <nl> } <nl> ret = av_frame_copy_props ( pic -> f , pic_arg );
static int dnxhd_encode_rdo ( AVCodecContext * avctx , DNXHDEncContext * ctx ) <nl> last_higher = FFMAX ( lambda , last_higher ); <nl> if ( last_lower != INT_MAX ) <nl> lambda = ( lambda + last_lower )>> 1 ; <nl> + else if (( int64_t ) lambda + up_step > INT_MAX ) <nl> + return - 1 ; <nl> else <nl> lambda += up_step ; <nl> - up_step *= 5 ; <nl> + up_step = FFMIN (( int64_t ) up_step * 5 , INT_MAX ); <nl> down_step = 1 << LAMBDA_FRAC_BITS ; <nl> } <nl> }
static int64_t mmsh_seek ( URLContext * h , int64_t pos , int whence ) <nl> MMSContext * mms = & mmsh -> mms ; <nl>  <nl> if ( pos == 0 && whence == SEEK_CUR ) <nl> - return mms -> asf_header_read_size + mms -> remaining_in_len + mmsh -> chunk_seq * mms -> asf_packet_len ; <nl> + return mms -> asf_header_read_size + mms -> remaining_in_len + mmsh -> chunk_seq * ( int64_t ) mms -> asf_packet_len ; <nl> return AVERROR ( ENOSYS ); <nl> } <nl> 
static void vc1_decode_p_blocks ( VC1Context * v ) <nl> if ( s -> mb_y != s -> start_mb_y ) ff_draw_horiz_band ( s , ( s -> mb_y - 1 ) * 16 , 16 ); <nl> s -> first_slice_line = 0 ; <nl> } <nl> - if ( apply_loop_filter ) { <nl> + if ( apply_loop_filter && v -> fcm == PROGRESSIVE ) { <nl> s -> mb_x = 0 ; <nl> ff_init_block_index ( s ); <nl> for (; s -> mb_x < s -> mb_width ; s -> mb_x ++) {
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl> int offset = ( get_bits_count (& gb )+ 7 )>> 3 ; <nl> uint8_t * buf ; <nl>  <nl> - if (( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> + if ( watermark_height <= 0 || ( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> return - 1 ; <nl>  <nl> buf = av_malloc ( buf_len );
static int mxf_read_generic_descriptor ( void * arg , AVIOContext * pb , int tag , int <nl> default : <nl> /* Private uid used by SONY C0023S01 . mxf */ <nl> if ( IS_KLV_KEY ( uid , mxf_sony_mpeg4_extradata )) { <nl> + av_free ( descriptor -> extradata ); <nl> + descriptor -> extradata_size = 0 ; <nl> descriptor -> extradata = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! descriptor -> extradata ) <nl> return AVERROR ( ENOMEM );
int ff_hevc_decode_nal_vps ( HEVCContext * s ) <nl> if ( get_bits_left ( gb ) < 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , <nl> " Overread VPS by % d bits \ n ", - get_bits_left ( gb )); <nl> - goto err ; <nl> + if ( s -> vps_list [ vps_id ]) <nl> + goto err ; <nl> } <nl>  <nl> if ( s -> vps_list [ vps_id ] &&
static int transcode ( OutputFile * output_files , <nl> int64_t ipts_min ; <nl> double opts_min ; <nl>  <nl> - redo : <nl> ipts_min = INT64_MAX ; <nl> opts_min = 1e100 ; <nl>  <nl> static int transcode ( OutputFile * output_files , <nl> if ( exit_on_error ) <nl> exit_program ( 1 ); <nl> av_free_packet (& pkt ); <nl> - goto redo ; <nl> + continue ; <nl> } <nl>  <nl> discard_packet :
int ff_vc1_parse_frame_header ( VC1Context * v , GetBitContext * gb ) <nl> { <nl> int pqindex , lowquant , status ; <nl>  <nl> + v -> field_mode = 0 ; <nl> + v -> fcm = 0 ; <nl> if ( v -> finterpflag ) <nl> v -> interpfrm = get_bits1 ( gb ); <nl> if (! v -> s . avctx -> codec )
static void event_loop ( VideoState * cur_stream ) <nl> } else { <nl> pos = get_master_clock ( cur_stream ); <nl> pos += incr ; <nl> + if ( cur_stream -> ic -> start_time != AV_NOPTS_VALUE && pos < cur_stream -> ic -> start_time / ( double ) AV_TIME_BASE ) <nl> + pos = cur_stream -> ic -> start_time / ( double ) AV_TIME_BASE ; <nl> stream_seek ( cur_stream , ( int64_t )( pos * AV_TIME_BASE ), ( int64_t )( incr * AV_TIME_BASE ), 0 ); <nl> } <nl> break ;
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl> memset (& h -> mb , 0 , sizeof ( h -> mb )); <nl> memset (& h -> mb_luma_dc , 0 , sizeof ( h -> mb_luma_dc )); <nl> memset (& h -> mb_padding , 0 , sizeof ( h -> mb_padding )); <nl> + memset (& h -> cur_pic , 0 , sizeof ( h -> cur_pic )); <nl>  <nl> h -> avctx = dst ; <nl> h -> DPB = NULL ;
static int read_tfra ( MOVContext * mov , AVIOContext * f ) <nl> } <nl> for ( i = 0 ; i < index -> item_count ; i ++) { <nl> int64_t time , offset ; <nl> + <nl> + if ( avio_feof ( f )) { <nl> + index -> item_count = 0 ; <nl> + av_freep (& index -> items ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( version == 1 ) { <nl> time = avio_rb64 ( f ); <nl> offset = avio_rb64 ( f );
start : <nl> c -> seek_timestamp = AV_NOPTS_VALUE ; <nl> break ; <nl> } <nl> + av_free_packet (& var -> pkt ); <nl> + reset_packet (& var -> pkt ); <nl> } <nl> } <nl> /* Check if this stream still is on an earlier segment number , or
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> goto err ; <nl> } <nl>  <nl> + if (( s -> mb_height >> v -> field_mode ) == 0 ) { <nl> + av_log ( v -> s . avctx , AV_LOG_ERROR , " image too short \ n "); <nl> + goto err ; <nl> + } <nl> + <nl> // process pulldown flags <nl> s -> current_picture_ptr -> f . repeat_pict = 0 ; <nl> // Pulldown flags are only valid when ' broadcast ' has been set .
int ff_spatial_idwt_init2 ( DWTContext * d , IDWTELEM * buffer , int width , int height <nl> d -> vertical_compose_l0 = ( void *) vertical_compose_fidelityiL0 ; <nl> d -> vertical_compose_h0 = ( void *) vertical_compose_fidelityiH0 ; <nl> d -> horizontal_compose = horizontal_compose_fidelityi ; <nl> + d -> support = 0 ; // not really used <nl> break ; <nl> case DWT_DIRAC_DAUB9_7 : <nl> d -> spatial_compose = spatial_compose_daub97i_dy ;
static int unpack_vlcs ( Vp3DecodeContext * s , GetBitContext * gb , <nl> if ( blocks_ended ) <nl> dct_tokens [ j ++] = blocks_ended << 2 ; <nl>  <nl> - while ( coeff_i < num_coeffs ) { <nl> + while ( coeff_i < num_coeffs && get_bits_left ( gb ) > 0 ) { <nl> /* decode a VLC into a token */ <nl> token = get_vlc2 ( gb , vlc_table , 5 , 3 ); <nl> /* use the token to get a zero run , a coefficient , and an eob run */
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> av_log ( s , AV_LOG_ERROR , " No seek table at given position \ n "); <nl> return ; <nl> } <nl> + if ( size > INT_MAX / 10 || size <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Seek table size is invalid \ n "); <nl> + return ; <nl> + } <nl> if (!( buf = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ))) <nl> return ; <nl> avio_read ( s -> pb , buf , size );
static int query_formats ( AVFilterContext * ctx ) <nl> EvalContext * eval = ctx -> priv ; <nl> enum AVSampleFormat sample_fmts [] = { AV_SAMPLE_FMT_DBL , AV_SAMPLE_FMT_NONE }; <nl> int64_t chlayouts [] = { eval -> chlayout , - 1 }; <nl> + int sample_rates [] = { eval -> sample_rate , - 1 }; <nl>  <nl> avfilter_set_common_sample_formats ( ctx , avfilter_make_format_list ( sample_fmts )); <nl> ff_set_common_channel_layouts ( ctx , avfilter_make_format64_list ( chlayouts )); <nl> + ff_set_common_samplerates ( ctx , avfilter_make_format_list ( sample_rates )); <nl>  <nl> return 0 ; <nl> }
static int theora_decode_header ( AVCodecContext * avctx , GetBitContext * gb ) <nl>  <nl> fps . num = get_bits_long ( gb , 32 ); <nl> fps . den = get_bits_long ( gb , 32 ); <nl> - if ( fps . num && fps . den ) { <nl> + if ( fps . num > 0 && fps . den > 0 ) { <nl> av_reduce (& avctx -> time_base . num , & avctx -> time_base . den , <nl> fps . den , fps . num , 1 << 30 ); <nl> }
ogm_header ( AVFormatContext * s , int idx ) <nl> size -= 52 ; <nl> if ( bytestream2_get_bytes_left (& p ) < size ) <nl> return AVERROR_INVALIDDATA ; <nl> - ff_alloc_extradata ( st -> codecpar , size ); <nl> + if ( ff_alloc_extradata ( st -> codecpar , size ) < 0 ) <nl> + return AVERROR ( ENOMEM ); <nl> bytestream2_get_buffer (& p , st -> codecpar -> extradata , st -> codecpar -> extradata_size ); <nl> } <nl> }
void ffserver_parse_acl_row ( FFServerStream * stream , FFServerStream * feed , <nl> } <nl>  <nl> nacl = av_mallocz ( sizeof (* nacl )); <nl> + if (! nacl ) { <nl> + fprintf ( stderr , " Failed to allocate FFServerIPAddressACL \ n "); <nl> + goto bail ; <nl> + } <nl> + <nl> naclp = 0 ; <nl>  <nl> acl . next = 0 ;
static int wavpack_encode_block ( WavPackEncodeContext * s , <nl> block_size = bytestream2_tell_p (& pb ); <nl> AV_WL32 ( out + 4 , block_size - 8 ); <nl>  <nl> + av_assert0 ( put_bits_left (& s -> pb ) > 0 ); <nl> + <nl> return block_size ; <nl> } <nl> 
static void check_luma_dc_wht ( void ) <nl> } <nl>  <nl> # define SRC_BUF_STRIDE 32 <nl> -# define SRC_BUF_SIZE (( size + 5 ) * SRC_BUF_STRIDE ) <nl> +# define SRC_BUF_SIZE ((( size << ( size < 16 )) + 5 ) * SRC_BUF_STRIDE ) <nl> // The mc subpixel interpolation filter needs the 2 previous pixels in either <nl> // direction , the + 1 is to make sure the actual load addresses always are <nl> // unaligned .
int ff_mpeg_update_thread_context ( AVCodecContext * dst , <nl> # define UPDATE_PICTURE ( pic )\ <nl> do {\ <nl> ff_mpeg_unref_picture ( s , & s -> pic );\ <nl> - if ( s1 -> pic . f -> buf [ 0 ])\ <nl> + if ( s1 -> pic . f && s1 -> pic . f -> buf [ 0 ])\ <nl> ret = ff_mpeg_ref_picture ( s , & s -> pic , & s1 -> pic );\ <nl> else \ <nl> ret = update_picture_tables (& s -> pic , & s1 -> pic );\
static int vp3_decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> last_frame = s -> golden_frame ; <nl> s -> last_frame . type = FF_BUFFER_TYPE_COPY ; <nl> + ff_thread_report_progress (& s -> last_frame , INT_MAX , 0 ); <nl> } <nl> } <nl> 
int av_write_trailer ( AVFormatContext * s ) <nl> if ( s -> oformat -> write_trailer ) <nl> ret = s -> oformat -> write_trailer ( s ); <nl>  <nl> - if (!( s -> oformat -> flags & AVFMT_NOFILE )) <nl> - avio_flush ( s -> pb ); <nl> - <nl> fail : <nl> if ( s -> pb ) <nl> avio_flush ( s -> pb );
static int speex_header ( AVFormatContext * s , int idx ) { <nl> if ( frames_per_packet ) <nl> spxp -> packet_size *= frames_per_packet ; <nl>  <nl> - ff_alloc_extradata ( st -> codec , os -> psize ); <nl> + if ( ff_alloc_extradata ( st -> codec , os -> psize ) < 0 ) <nl> + return AVERROR ( ENOMEM ); <nl> memcpy ( st -> codec -> extradata , p , st -> codec -> extradata_size ); <nl>  <nl> avpriv_set_pts_info ( st , 64 , 1 , st -> codec -> sample_rate );
static int av_encode ( AVFormatContext ** output_files , <nl> break ; <nl> case CODEC_TYPE_VIDEO : <nl> data_size = ( ist -> st -> codec . width * ist -> st -> codec . height * 3 ) / 2 ; <nl> + /* XXX : allocate picture correctly */ <nl> + memset (& picture , 0 , sizeof ( picture )); <nl> ret = avcodec_decode_video (& ist -> st -> codec , <nl> & picture , & got_picture , ptr , len ); <nl> ist -> st -> quality = picture . quality ;
static inline int decode_subframe ( FLACContext * s , int channel ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( wasted ) { <nl> + if ( wasted && wasted < 32 ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> decoded [ i ] = ( unsigned ) decoded [ i ] << wasted ;
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl>  <nl> flag = 0 ; <nl>  <nl> - if ( state * 4ULL > 0xFF || i >= size ) <nl> + if (( uint64_t ) state > 0xFF / 4 || i >= size ) <nl> continue ; <nl>  <nl> pfx = (( state + 8 ) >> 5 ) + ( state ? ff_clz ( state ): 32 ) - 24 ;
static void term_init ( void ) <nl> # if HAVE_TERMIOS_H <nl> if (! run_as_daemon ){ <nl> struct termios tty ; <nl> - <nl> +# if HAVE_ISATTY <nl> + if ( isatty ( 0 ) && isatty ( 2 )) <nl> +# endif <nl> if ( tcgetattr ( 0 , & tty ) == 0 ) { <nl> oldtty = tty ; <nl> restore_tty = 1 ;
static int64_t asf_read_pts ( AVFormatContext * s , int stream_index , <nl>  <nl> // assert (( asf_st -> packet_pos - s -> data_offset ) % s -> packet_size == 0 ); <nl> pos = asf_st -> packet_pos ; <nl> + av_assert1 ( pkt -> pos == asf_st -> packet_pos ); <nl>  <nl> av_add_index_entry ( s -> streams [ i ], pos , pts , pkt -> size , <nl> pos - start_pos [ i ] + 1 , AVINDEX_KEYFRAME );
av_cold int vaapi_device_init ( const char * device ) <nl> { <nl> int err ; <nl>  <nl> + av_buffer_unref (& hw_device_ctx ); <nl> + <nl> err = av_hwdevice_ctx_create (& hw_device_ctx , AV_HWDEVICE_TYPE_VAAPI , <nl> device , NULL , 0 ); <nl> if ( err < 0 ) {
static int svq1_decode_frame ( AVCodecContext * avctx , <nl> av_dlog ( s -> avctx , " Error in svq1_decode_frame_header % i \ n ", result ); <nl> return result ; <nl> } <nl> + avcodec_set_dimensions ( avctx , s -> width , s -> height ); <nl>  <nl> // FIXME this avoids some confusion for " B frames " without 2 references <nl> // this should be removed after libavcodec can handle more flexible picture types & ordering
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> if (!( buf = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ))) <nl> return ; <nl> avio_read ( s -> pb , buf , size ); <nl> + memset ( buf + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + <nl> init_get_bits (& gb , buf , size * 8 ); <nl> size = gb_get_v (& gb ); <nl> if ( size > UINT_MAX / 4 || size > c -> samples / 1152 ){
decode_intra_mb : <nl> } <nl>  <nl> // The pixels are stored in the same order as levels in h -> mb array . <nl> + if (( int ) ( h -> cabac . bytestream_end - ptr ) < mb_size ) <nl> + return - 1 ; <nl> memcpy ( h -> mb , ptr , mb_size ); ptr += mb_size ; <nl>  <nl> ff_init_cabac_decoder (& h -> cabac , ptr , h -> cabac . bytestream_end - ptr );
static void free_geotags ( TiffContext * const s ) <nl> av_freep (& s -> geotags [ i ]. val ); <nl> } <nl> av_freep (& s -> geotags ); <nl> + s -> geotag_count = 0 ; <nl> } <nl>  <nl> # define RET_GEOKEY ( TYPE , array , element )\
static int caf_write_header ( AVFormatContext * s ) <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl>  <nl> + if ( par -> codec_id == AV_CODEC_ID_OPUS && par -> channels > 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Only mono and stereo are supported for Opus \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (! codec_tag ) { <nl> av_log ( s , AV_LOG_ERROR , " unsupported codec \ n "); <nl> return AVERROR_INVALIDDATA ;
static int smacker_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> smk -> cur_frame ++; <nl> smk -> nextpos = avio_tell ( s -> pb ); <nl> } else { <nl> - if ( smk -> stream_id [ smk -> curstream ] < 0 ) <nl> + if ( smk -> stream_id [ smk -> curstream ] < 0 || ! smk -> bufs [ smk -> curstream ]) <nl> return AVERROR_INVALIDDATA ; <nl> if ( av_new_packet ( pkt , smk -> buf_sizes [ smk -> curstream ])) <nl> return AVERROR ( ENOMEM );
const uint8_t ff_png_pass_mask [ NB_PASSES ] = { <nl>  <nl> void * ff_png_zalloc ( void * opaque , unsigned int items , unsigned int size ) <nl> { <nl> - if ( items >= UINT_MAX / size ) <nl> - return NULL ; <nl> - return av_malloc ( items * size ); <nl> + return av_mallocz_array ( items , size ); <nl> } <nl>  <nl> void ff_png_zfree ( void * opaque , void * ptr )
static int lavfi_read_packet ( AVFormatContext * avctx , AVPacket * pkt ) <nl> int stream_idx , min_pts_sink_idx = 0 ; <nl> AVFilterBufferRef * ref ; <nl> AVPicture pict ; <nl> - int ret , i , size ; <nl> + int ret , i ; <nl> + int size = 0 ; <nl>  <nl> /* iterate through all the graph sinks . Select the sink with the <nl> * minimum PTS */
reconnect : <nl> // audio or video packet arrives . <nl> while (! rt -> has_audio && ! rt -> has_video && ! rt -> received_metadata ) { <nl> if (( ret = get_packet ( s , 0 )) < 0 ) <nl> - return ret ; <nl> + goto fail ; <nl> } <nl>  <nl> // Either after we have read the metadata or ( if there is none ) the
static void set_frag_stream ( MOVFragmentIndex * frag_index , int id ) <nl> static MOVFragmentStreamInfo * get_current_frag_stream_info ( <nl> MOVFragmentIndex * frag_index ) <nl> { <nl> + if ( frag_index -> current < 0 || <nl> + frag_index -> current >= frag_index -> nb_items ) <nl> + return NULL ; <nl> + <nl> MOVFragmentIndexItem * item = & frag_index -> item [ frag_index -> current ]; <nl> if ( item -> current >= 0 && item -> current < item -> nb_stream_info ) <nl> return & item -> stream_info [ item -> current ];
static int wma_decode_block ( WMACodecContext * s ) <nl> coef escape coding */ <nl> total_gain = 1 ; <nl> for (;;) { <nl> + if ( get_bits_left (& s -> gb ) < 7 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " total_gain overread \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> a = get_bits (& s -> gb , 7 ); <nl> total_gain += a ; <nl> if ( a != 127 )
static int read_random ( uint32_t * dst , const char * file ) <nl>  <nl> if ( fd == - 1 ) <nl> return - 1 ; <nl> -# if HAVE_FCNTL && defined ( O_NONBLOCK ) <nl> - if ( fcntl ( fd , F_SETFL , fcntl ( fd , F_GETFL ) | O_NONBLOCK ) != - 1 ) <nl> -# endif <nl> err = read ( fd , dst , sizeof (* dst )); <nl> close ( fd ); <nl> 
static OutputStream * new_video_stream ( OptionsContext * o , AVFormatContext * oc , in <nl> } <nl> /* FIXME realloc failure */ <nl> video_enc -> rc_override = <nl> - av_realloc ( video_enc -> rc_override , <nl> - sizeof ( RcOverride ) * ( i + 1 )); <nl> + av_realloc_array ( video_enc -> rc_override , <nl> + i + 1 , sizeof ( RcOverride )); <nl> video_enc -> rc_override [ i ]. start_frame = start ; <nl> video_enc -> rc_override [ i ]. end_frame = end ; <nl> if ( q > 0 ) {
static void unpack_alpha ( GetBitContext * gb , uint16_t * dst , int num_coeffs , <nl> dst [ idx ++] = alpha_val >> 6 ; <nl> else <nl> dst [ idx ++] = ( alpha_val << 2 ) | ( alpha_val >> 6 ); <nl> - if ( idx == num_coeffs - 1 ) <nl> + if ( idx >= num_coeffs - 1 ) <nl> break ; <nl> } while ( get_bits1 ( gb )); <nl> val = get_bits ( gb , 4 );
static int mov_read_keys ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> avio_skip ( pb , 4 ); <nl> count = avio_rb32 ( pb ); <nl> - if ( count > UINT_MAX / sizeof (* c -> meta_keys )) { <nl> + if ( count > UINT_MAX / sizeof (* c -> meta_keys ) - 1 ) { <nl> av_log ( c -> fc , AV_LOG_ERROR , <nl> " The ' keys ' atom with the invalid key count : % d \ n ", count ); <nl> return AVERROR_INVALIDDATA ;
static int get_audio_frame_size ( AVCodecContext * enc , int size ) <nl> /* used for example by ADPCM codecs */ <nl> if ( enc -> bit_rate == 0 ) <nl> return - 1 ; <nl> - frame_size = ( size * 8 * enc -> sample_rate ) / enc -> bit_rate ; <nl> + frame_size = (( int64_t ) size * 8 * enc -> sample_rate ) / enc -> bit_rate ; <nl> } <nl> } else { <nl> frame_size = enc -> frame_size ;
static int decode_entropy_coded_image ( WebPContext * s , enum ImageRole role , <nl> length = offset + get_bits (& s -> gb , extra_bits ) + 1 ; <nl> } <nl> prefix_code = huff_reader_get_symbol (& hg [ HUFF_IDX_DIST ], & s -> gb ); <nl> - if ( prefix_code > 39 ) { <nl> + if ( prefix_code > 39U ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , <nl> " distance prefix code too large : % d \ n ", prefix_code ); <nl> return AVERROR_INVALIDDATA ;
int av_dict_set ( AVDictionary ** pm , const char * key , const char * value , <nl> m = * pm = av_mallocz ( sizeof (* m )); <nl>  <nl> if ( tag ) { <nl> - if ( flags & AV_DICT_DONT_OVERWRITE ) <nl> + if ( flags & AV_DICT_DONT_OVERWRITE ) { <nl> + if ( flags & AV_DICT_DONT_STRDUP_KEY ) av_free ( key ); <nl> + if ( flags & AV_DICT_DONT_STRDUP_VAL ) av_free ( value ); <nl> return 0 ; <nl> + } <nl> if ( flags & AV_DICT_APPEND ) <nl> oldval = tag -> value ; <nl> else
static int flashsv2_prime ( FlashSVContext * s , uint8_t * src , <nl> s -> zstream . avail_out = s -> block_size * 3 ; <nl> inflate (& s -> zstream , Z_SYNC_FLUSH ); <nl>  <nl> - deflateInit (& zs , 0 ); <nl> + if ( deflateInit (& zs , 0 ) != Z_OK ) <nl> + return - 1 ; <nl> zs . next_in = s -> tmpblock ; <nl> zs . avail_in = s -> block_size * 3 - s -> zstream . avail_out ; <nl> zs . next_out = s -> deflate_block ;
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> * In complex cases , there are style descriptors appended to the string <nl> * so we can ' t just assume the packet size is the string size . <nl> */ <nl> - end = ptr + FFMAX ( 2 + AV_RB16 ( ptr ), avpkt -> size ); <nl> + end = ptr + FFMIN ( 2 + AV_RB16 ( ptr ), avpkt -> size ); <nl> ptr += 2 ; <nl>  <nl> ts_start = av_rescale_q ( avpkt -> pts ,
static int matroska_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> ret = matroska_parse_cluster ( matroska ); <nl> } <nl>  <nl> + if ( ret == AVERROR_INVALIDDATA ) { <nl> + pkt -> flags |= AV_PKT_FLAG_CORRUPT ; <nl> + return 0 ; <nl> + } <nl> + <nl> return ret ; <nl> } <nl> 
typedef struct { <nl>  <nl> typedef struct { <nl> int64_t frames_hdr_strm ; <nl> - int audio_strm_length ; <nl> + int64_t audio_strm_length ; <nl> int packet_count ; <nl> int entry ; <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> for ( i = 0 ; i < SUBFRAMES ; i ++) <nl> put_subframe ( c , i ); <nl>  <nl> + <nl> + for ( i = put_bits_count (& c -> pb ); i < 8 * c -> frame_size ; i ++) <nl> + put_bits (& c -> pb , 1 , 0 ); <nl> + <nl> flush_put_bits (& c -> pb ); <nl>  <nl> avpkt -> pts = frame -> pts ;
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> bytestream_put_le16 (& buf , 0 ); <nl> bytestream_put_le32 (& buf , 0 ); <nl>  <nl> - if (( ret = avio_read ( pb , buf , image -> size )) < 0 ) <nl> + if (( ret = avio_read ( pb , buf , image -> size )) < 0 ) { <nl> + av_packet_unref ( pkt ); <nl> return ret ; <nl> + } <nl>  <nl> st -> codecpar -> bits_per_coded_sample = AV_RL16 ( buf + 14 ); <nl> 
int32_t ff_mlp_pack_output ( int32_t lossless_check_data , <nl> ( 1U << output_shift [ mat_ch ]); <nl> lossless_check_data ^= ( sample & 0xffffff ) << mat_ch ; <nl> if ( is32 ) <nl> - * data_32 ++ = sample << 8 ; <nl> + * data_32 ++ = sample * 256 ; <nl> else <nl> * data_16 ++ = sample >> 8 ; <nl> }
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> h = ( buf [ 1 ] + 1 ) * 8 ; <nl> buf += 2 ; <nl>  <nl> + if ( avpkt -> size < 2 + w * h / 513 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( w != avctx -> width || h != avctx -> height ) { <nl> av_freep (& c -> frame_buffer ); <nl> av_freep (& c -> last_frame_buffer );
int ff_jni_exception_get_summary ( JNIEnv * env , jthrowable exception , char ** error <nl> jclass exception_class = NULL ; <nl> jmethodID get_message_id = NULL ; <nl>  <nl> - jstring string ; <nl> + jstring string = NULL ; <nl>  <nl> av_bprint_init (& bp , 0 , AV_BPRINT_SIZE_AUTOMATIC ); <nl> 
static int sap_write_close ( AVFormatContext * s ) <nl> url_fclose ( rtpctx -> pb ); <nl> av_metadata_free (& rtpctx -> streams [ 0 ]-> metadata ); <nl> av_metadata_free (& rtpctx -> metadata ); <nl> + av_free ( rtpctx -> streams [ 0 ]-> info ); <nl> av_free ( rtpctx -> streams [ 0 ]); <nl> av_free ( rtpctx ); <nl> s -> streams [ i ]-> priv_data = NULL ;
static av_cold int rl2_decode_end ( AVCodecContext * avctx ) <nl> { <nl> Rl2Context * s = avctx -> priv_data ; <nl>  <nl> - av_free ( s -> back_frame ); <nl> + av_freep (& s -> back_frame ); <nl>  <nl> return 0 ; <nl> }
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl>  <nl> int res = avcodec_open2 ( ctx , c , NULL ); <nl> if ( res < 0 ) <nl> - return res ; <nl> + return 0 ; // Failure of avcodec_open2 () does not imply that a issue was found <nl>  <nl> FDBCreate (& buffer ); <nl> int got_frame ;
static int decompress_p ( AVCodecContext * avctx , <nl> return ret ; <nl>  <nl> max += temp << 8 ; <nl> + if ( min > max ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> memset ( s -> blocks , 0 , sizeof (* s -> blocks ) * s -> nbcount ); <nl>  <nl> while ( min <= max ) {
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> memcpy ( c , h -> s . thread_context [ i ], sizeof ( MpegEncContext )); <nl> memset (& c -> s + 1 , 0 , sizeof ( H264Context ) - sizeof ( MpegEncContext )); <nl> c -> h264dsp = h -> h264dsp ; <nl> + c -> h264qpel = h -> h264qpel ; <nl> c -> sps = h -> sps ; <nl> c -> pps = h -> pps ; <nl> c -> pixel_shift = h -> pixel_shift ;
static int video_thread ( void * arg ) <nl> frame -> opaque = picref ; <nl> } <nl>  <nl> - if ( av_cmp_q ( tb , is -> video_st -> time_base )) { <nl> + if ( ret >= 0 && av_cmp_q ( tb , is -> video_st -> time_base )) { <nl> av_unused int64_t pts1 = pts_int ; <nl> pts_int = av_rescale_q ( pts_int , tb , is -> video_st -> time_base ); <nl> av_dlog ( NULL , " video_thread (): "
skip : <nl> // sign extension <nl> int32_t cts = ( avio_rb24 ( s -> pb ) + 0xff800000 ) ^ 0xff800000 ; <nl> pts = dts + cts ; <nl> - if ( cts < 0 ) { // dts are wrong <nl> + if ( cts < 0 && ! flv -> wrong_dts ) { // dts might be wrong <nl> flv -> wrong_dts = 1 ; <nl> av_log ( s , AV_LOG_WARNING , <nl> " Negative cts , previous timestamps might be wrong .\ n ");
static int vorbis_parse_setup_hdr_codebooks ( vorbis_context * vc ) <nl> } <nl>  <nl> // Initialize VLC table <nl> + if ( entries <= 0 ) { <nl> + av_log ( vc -> avctx , AV_LOG_ERROR , " Invalid codebook entry count \ n "); <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto error ; <nl> + } <nl> if ( ff_vorbis_len2vlc ( tmp_vlc_bits , tmp_vlc_codes , entries )) { <nl> av_log ( vc -> avctx , AV_LOG_ERROR , " Invalid code lengths while generating vlcs . \ n "); <nl> ret = AVERROR_INVALIDDATA ;
end : <nl> frame -> height = avctx -> height ; <nl> } <nl>  <nl> + if ( ret < 0 ) <nl> + av_frame_unref ( frame ); <nl> + <nl> return ret ; <nl> } <nl> 
exit_loop : <nl> } <nl> } <nl> ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> + ff_thread_report_progress (& s -> previous_picture , INT_MAX , 0 ); <nl>  <nl> av_frame_set_metadata ( p , metadata ); <nl> metadata = NULL ; <nl> exit_loop : <nl> fail : <nl> av_dict_free (& metadata ); <nl> ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> + ff_thread_report_progress (& s -> previous_picture , INT_MAX , 0 ); <nl> return ret ; <nl> } <nl> 
static int twolame_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> if ( ret < 0 ) // twolame error <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame ) { <nl> + avpkt -> duration = ff_samples_to_time_base ( avctx , frame -> nb_samples ); <nl> if ( frame -> pts != AV_NOPTS_VALUE ) <nl> avpkt -> pts = frame -> pts - ff_samples_to_time_base ( avctx , avctx -> delay ); <nl> } else {
static int bitpacked_decode_yuv422p10 ( AVCodecContext * avctx , AVFrame * frame , <nl> AVPacket * avpkt ) <nl> { <nl> uint64_t frame_size = ( uint64_t ) avctx -> width * ( uint64_t ) avctx -> height * 20 ; <nl> - uint64_t packet_size = avpkt -> size * 8 ; <nl> + uint64_t packet_size = ( uint64_t ) avpkt -> size * 8 ; <nl> GetBitContext bc ; <nl> uint16_t * y , * u , * v ; <nl> int ret , i ;
int ff_thread_get_buffer ( AVCodecContext * avctx , AVFrame * f ) <nl> p -> requested_frame = f ; <nl> p -> state = STATE_GET_BUFFER ; <nl> pthread_mutex_lock (& p -> progress_mutex ); <nl> - pthread_cond_signal (& p -> progress_cond ); <nl> + pthread_cond_broadcast (& p -> progress_cond ); <nl>  <nl> while ( p -> state != STATE_SETTING_UP ) <nl> pthread_cond_wait (& p -> progress_cond , & p -> progress_mutex );
static void compute_chapters_end ( AVFormatContext * s ) <nl> unsigned int i , j ; <nl> int64_t max_time = 0 ; <nl>  <nl> - if ( s -> duration > 0 ) <nl> + if ( s -> duration > 0 && s -> start_time < INT64_MAX - s -> duration ) <nl> max_time = s -> duration + <nl> (( s -> start_time == AV_NOPTS_VALUE ) ? 0 : s -> start_time ); <nl> 
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> } <nl> if ( len > 0xffff ) <nl> len = 0 ; <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO ) { <nl> + len = 0 ; <nl> + } <nl> * q ++ = len >> 8 ; <nl> * q ++ = len ; <nl> val = 0x80 ;
static int ism_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> SmoothStreamingContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ pkt -> stream_index ]; <nl> - int64_t end_dts = ( c -> nb_fragments + 1 ) * c -> min_frag_duration ; <nl> + int64_t end_dts = ( c -> nb_fragments + 1LL ) * c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int channelmap_filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * bu <nl> if ( buf -> extended_data == buf -> data ) { <nl> buf -> extended_data = new_extended_data ; <nl> } else { <nl> - buf -> extended_data = new_extended_data ; <nl> av_free ( buf -> extended_data ); <nl> + buf -> extended_data = new_extended_data ; <nl> } <nl> } else if ( buf -> extended_data != buf -> data ) { <nl> av_free ( buf -> extended_data );
static int find_headers_search_validate ( FLACParseContext * fpc , int offset ) <nl> (* end_handle )-> offset = offset ; <nl> (* end_handle )-> link_penalty = av_malloc ( sizeof ( int ) * <nl> FLAC_MAX_SEQUENTIAL_HEADERS ); <nl> + if (!(* end_handle )-> link_penalty ) { <nl> + av_freep ( end_handle ); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> + <nl> for ( i = 0 ; i < FLAC_MAX_SEQUENTIAL_HEADERS ; i ++) <nl> (* end_handle )-> link_penalty [ i ] = FLAC_HEADER_NOT_PENALIZED_YET ; <nl> 
static int mxf_write_footer ( AVFormatContext * s ) <nl> AVIOContext * pb = s -> pb ; <nl> int err = 0 ; <nl>  <nl> + if (! mxf -> header_written || <nl> + ( s -> oformat == & ff_mxf_opatom_muxer && ! mxf -> body_partition_offset )) { <nl> + /* reason could be invalid options / not supported codec / out of memory */ <nl> + err = AVERROR_UNKNOWN ; <nl> + goto end ; <nl> + } <nl> + <nl> mxf -> duration = mxf -> last_indexed_edit_unit + mxf -> edit_units_count ; <nl>  <nl> mxf_write_klv_fill ( s );
int av_asrc_buffer_add_buffer ( AVFilterContext * ctx , <nl> int sample_fmt , int64_t channel_layout , int planar , <nl> int64_t pts , int av_unused flags ) <nl> { <nl> - uint8_t * data [ 8 ]; <nl> + uint8_t * data [ 8 ] = { 0 }; <nl> int linesize [ 8 ]; <nl> int nb_channels = av_get_channel_layout_nb_channels ( channel_layout ), <nl> nb_samples = buf_size / nb_channels / av_get_bytes_per_sample ( sample_fmt );
void ff_h264_direct_ref_list_init ( H264Context * const h ){ <nl> h -> col_fieldoff = 2 * h -> ref_list [ 1 ][ 0 ]. f . reference - 3 ; <nl> } <nl>  <nl> - if ( cur -> f . pict_type != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> + if ( h -> slice_type_nos != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> return ; <nl>  <nl> for ( list = 0 ; list < 2 ; list ++){
av_cold int ff_dcaadpcm_init ( DCAADPCMEncContext * s ) <nl> return - 1 ; <nl>  <nl> s -> private_data = av_malloc ( sizeof ( premultiplied_coeffs ) * DCA_ADPCM_VQCODEBOOK_SZ ); <nl> + if (! s -> private_data ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> precalc ( s -> private_data ); <nl> return 0 ; <nl> }
void avcodec_get_frame_defaults ( AVFrame * frame ) <nl>  <nl> AVFrame * avcodec_alloc_frame ( void ) <nl> { <nl> - AVFrame * frame = av_malloc ( sizeof ( AVFrame )); <nl> + AVFrame * frame = av_mallocz ( sizeof ( AVFrame )); <nl>  <nl> if ( frame == NULL ) <nl> return NULL ;
static void asfrtp_close_context ( PayloadContext * asf ) <nl> { <nl> ffio_free_dyn_buf (& asf -> pktbuf ); <nl> av_freep (& asf -> buf ); <nl> - av_free ( asf ); <nl> } <nl>  <nl> # define RTP_ASF_HANDLER ( n , s , t ) \
int vc1_parse_frame_header_adv ( VC1Context * v , GetBitContext * gb ) <nl> int scale , shift , i ; /* for initializing LUT for intensity compensation */ <nl>  <nl> v -> numref = 0 ; <nl> + v -> fcm = 0 ; <nl> + v -> field_mode = 0 ; <nl> v -> p_frame_skipped = 0 ; <nl> if ( v -> second_field ) { <nl> v -> s . pict_type = ( v -> fptype & 1 ) ? AV_PICTURE_TYPE_P : AV_PICTURE_TYPE_I ;
static int mov_write_udta_tag ( ByteIOContext * pb , MOVMuxContext * mov , <nl> put_be32 ( pb , size + 8 ); <nl> put_tag ( pb , " udta "); <nl> put_buffer ( pb , buf , size ); <nl> - av_free ( buf ); <nl> } <nl> + av_free ( buf ); <nl>  <nl> return 0 ; <nl> }
static void apply_unsharp ( uint8_t * dst , int dst_stride , <nl>  <nl> int32_t res ; <nl> int x , y , z ; <nl> - const uint8_t * src2 ; <nl> + const uint8_t * src2 = NULL ; // silence a warning <nl>  <nl> if (! fp -> amount ) { <nl> if ( dst_stride == src_stride )
static int rm_write_header ( AVFormatContext * s ) <nl> int n ; <nl> AVCodecContext * codec ; <nl>  <nl> + if ( s -> nb_streams > 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " At most 2 streams are currently supported for muxing in RM \ n "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> + <nl> for ( n = 0 ; n < s -> nb_streams ; n ++) { <nl> s -> streams [ n ]-> id = n ; <nl> codec = s -> streams [ n ]-> codec ;
static int tak_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> if ( s -> sample_shift [ chan ] > 0 ) <nl> for ( i = 0 ; i < s -> nb_samples ; i ++) <nl> - decoded [ i ] <<= s -> sample_shift [ chan ]; <nl> + decoded [ i ] *= 1 << s -> sample_shift [ chan ]; <nl> } <nl> } <nl> 
int ff_mjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> /* EOF */ <nl> if ( start_code < 0 ) { <nl> goto the_end ; <nl> - } else if ( unescaped_buf_size > ( 1U << 29 )) { <nl> + } else if ( unescaped_buf_size > ( 1U << 28 )) { <nl> av_log ( avctx , AV_LOG_ERROR , " MJPEG packet 0x % x too big ( 0x % x / 0x % x ), corrupt data ?\ n ", <nl> start_code , unescaped_buf_size , buf_size ); <nl> return AVERROR_INVALIDDATA ;
static int v4l2_set_parameters ( AVFormatContext * s1 , AVFormatParameters * ap ) <nl> struct v4l2_streamparm streamparm = { 0 }; <nl> struct v4l2_fract * tpf = & streamparm . parm . capture . timeperframe ; <nl> int i , ret ; <nl> - AVRational framerate_q ; <nl> + AVRational framerate_q ={ 0 }; <nl>  <nl> streamparm . type = V4L2_BUF_TYPE_VIDEO_CAPTURE ; <nl> 
int attribute_align_arg av_buffersink_get_frame_flags ( AVFilterContext * ctx , AVFr <nl>  <nl> if ( flags & AV_BUFFERSINK_FLAG_PEEK ) { <nl> cur_frame = *(( AVFrame **) av_fifo_peek2 ( buf -> fifo , 0 )); <nl> - av_frame_ref ( frame , cur_frame ); /* TODO check failure */ <nl> + if (( ret = av_frame_ref ( frame , cur_frame )) < 0 ) <nl> + return ret ; <nl> } else { <nl> av_fifo_generic_read ( buf -> fifo , & cur_frame , sizeof ( cur_frame ), NULL ); <nl> av_frame_move_ref ( frame , cur_frame );
static inline int mxf_read_utf16_string ( AVIOContext * pb , int size , char ** str , i <nl> int ret ; <nl> size_t buf_size ; <nl>  <nl> - if ( size < 0 ) <nl> + if ( size < 0 || size > INT_MAX / 2 ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> buf_size = size + size / 2 + 1 ;
static int mxf_read_primer_pack ( void * arg , AVIOContext * pb , int tag , int size , U <nl> avpriv_request_sample ( pb , " Primer pack item length % d ", item_len ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> - if ( item_num > 65536 ) { <nl> + if ( item_num > 65536 || item_num < 0 ) { <nl> av_log ( mxf -> fc , AV_LOG_ERROR , " item_num % d is too large \ n ", item_num ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> // we just ignore it <nl> bytestream_get_le16 (& buf ); <nl>  <nl> + if ( buf_end - buf < h + 3 * 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> // allocate sub and set values <nl> sub -> rects = av_mallocz ( sizeof (* sub -> rects )); <nl> if (! sub -> rects )
ASSStyle * ff_ass_style_get ( ASSSplitContext * ctx , const char * style ) <nl> if (! style || !* style ) <nl> style = " Default "; <nl> for ( i = 0 ; i < ass -> styles_count ; i ++) <nl> - if (! strcmp ( ass -> styles [ i ]. name , style )) <nl> + if ( ass -> styles [ i ]. name && ! strcmp ( ass -> styles [ i ]. name , style )) <nl> return ass -> styles + i ; <nl> return NULL ; <nl> }
static int ebml_read_binary ( AVIOContext * pb , int length , EbmlBin * bin ) <nl> bin -> pos = avio_tell ( pb ); <nl> if ( avio_read ( pb , bin -> data , length ) != length ) { <nl> av_freep (& bin -> data ); <nl> + bin -> size = 0 ; <nl> return AVERROR ( EIO ); <nl> } <nl> 
static void flush_dpb ( AVCodecContext * avctx ) <nl> h -> parse_context . overread_index = 0 ; <nl> h -> parse_context . index = 0 ; <nl> h -> parse_context . last_index = 0 ; <nl> + <nl> + free_tables ( h , 1 ); <nl> + h -> context_initialized = 0 ; <nl> } <nl>  <nl> int ff_init_poc ( H264Context * h , int pic_field_poc [ 2 ], int * pic_poc )
static int mv_read_packet ( AVFormatContext * avctx , AVPacket * pkt ) <nl> AVStream * st = avctx -> streams [ mv -> stream_index ]; <nl> const AVIndexEntry * index ; <nl> int frame = mv -> frame [ mv -> stream_index ]; <nl> - int ret ; <nl> + int64_t ret ; <nl> uint64_t pos ; <nl>  <nl> if ( frame < st -> nb_index_entries ) {
static int get_siz ( J2kDecoderContext * s ) <nl> s -> tile_offset_y = bytestream_get_be32 (& s -> buf ); // YT0Siz <nl> s -> ncomponents = bytestream_get_be16 (& s -> buf ); // CSiz <nl>  <nl> + if ( s -> tile_width <= 0 || s -> tile_height <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> if ( s -> buf_end - s -> buf < 2 * s -> ncomponents ) <nl> return AVERROR ( EINVAL ); <nl> 
static int read_header ( AVFormatContext * s , <nl> jvf -> audio_size = avio_rl32 ( pb ); <nl> jvf -> video_size = avio_rl32 ( pb ); <nl> jvf -> palette_size = avio_r8 ( pb ) ? 768 : 0 ; <nl> + jvf -> video_size = FFMIN ( FFMAX ( jvf -> video_size , 0 ), <nl> + INT_MAX - JV_PREAMBLE_SIZE - jvf -> palette_size ); <nl> if ( avio_r8 ( pb )) <nl> av_log ( s , AV_LOG_WARNING , " unsupported audio codec \ n "); <nl> jvf -> video_type = avio_r8 ( pb );
static void draw_char ( AVCodecContext * avctx , int c , int a ) <nl> s -> frame . linesize [ 0 ], s -> font , s -> font_height , c , <nl> a & 0x0F , a >> 4 ); <nl> s -> x += FONT_WIDTH ; <nl> - if ( s -> x >= avctx -> width ) { <nl> + if ( s -> x > avctx -> width - FONT_WIDTH ) { <nl> s -> x = 0 ; <nl> s -> y += s -> font_height ; <nl> }
static int nprobe ( AVFormatContext * s , uint8_t * enc_header , unsigned size , <nl> taglen = AV_RB32 (& enc_header [ pos + 32 ]); <nl> datalen = AV_RB32 (& enc_header [ pos + 36 ]) >> 4 ; <nl>  <nl> - pos += 44 + taglen ; <nl> + pos += 44 ; <nl> + if ( size - pos < taglen ) <nl> + return - 1 ; <nl> + <nl> + pos += taglen ; <nl>  <nl> if ( datalen << 4 > size - pos ) <nl> return - 1 ;
static int mpeg_decode_slice ( MpegEncContext * s , int mb_y , <nl> if ( mb_y == 0 && s -> codec_tag == AV_RL32 (" SLIF ")) { <nl> skip_bits1 (& s -> gb ); <nl> } else { <nl> - for (;;) { <nl> + while ( get_bits_left (& s -> gb ) > 0 ) { <nl> int code = get_vlc2 (& s -> gb , mbincr_vlc . table , MBINCR_VLC_BITS , 2 ); <nl> if ( code < 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " first mb_incr damaged \ n ");
static float voice_factor ( float * p_vector , float p_gain , <nl> AMRWB_SFR_SIZE ) * <nl> f_gain * f_gain ; <nl>  <nl> - return ( p_ener - f_ener ) / ( p_ener + f_ener ); <nl> + return ( p_ener - f_ener ) / ( p_ener + f_ener + 0 . 01 ); <nl> } <nl>  <nl> /**
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size , <nl> h -> workaround_bugs |= FF_BUG_TRUNCATED ; <nl>  <nl> if (!( h -> workaround_bugs & FF_BUG_TRUNCATED )) <nl> - while ( ptr [ dst_length - 1 ] == 0 && dst_length > 0 ) <nl> + while ( dst_length > 0 && ptr [ dst_length - 1 ] == 0 ) <nl> dst_length --; <nl> bit_length = ! dst_length ? 0 <nl> : ( 8 * dst_length -
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl>  <nl> vseq -> level_idc = avctx -> level ; <nl>  <nl> - vseq -> max_num_ref_frames = 2 ; <nl> + vseq -> max_num_ref_frames = 1 + ( avctx -> max_b_frames > 0 ); <nl>  <nl> vseq -> picture_width_in_mbs = priv -> mb_width ; <nl> vseq -> picture_height_in_mbs = priv -> mb_height ;
static int daala_header ( AVFormatContext * s , int idx ) <nl> if ( hdr -> gpshift >= 32 ) { <nl> av_log ( s , AV_LOG_ERROR , " Too large gpshift % d (>= 32 ).\ n ", <nl> hdr -> gpshift ); <nl> + hdr -> gpshift = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> hdr -> gpmask = ( 1U << hdr -> gpshift ) - 1 ;
static int decode_frame ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( rbuf + buf_size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 ) { <nl> av_free ( rbuf );
static int decode_exp_vlc ( WMACodecContext * s , int ch ) <nl> } <nl> /* NOTE : this offset is the same as MPEG4 AAC ! */ <nl> last_exp += code - 60 ; <nl> - if (( unsigned ) last_exp + 60 > FF_ARRAY_ELEMS ( pow_tab )) { <nl> + if (( unsigned ) last_exp + 60 >= FF_ARRAY_ELEMS ( pow_tab )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Exponent out of range : % d \ n ", <nl> last_exp ); <nl> return - 1 ;
typedef struct AMRWBContext { <nl> } AMRWBContext ; <nl>  <nl> static const AVOption options [] = { <nl> - { " dtx ", " Allow DTX ( generate comfort noise )", offsetof ( AMRWBContext , allow_dtx ), FF_OPT_TYPE_INT , 0 , 0 , 1 , AV_OPT_FLAG_AUDIO_PARAM | AV_OPT_FLAG_ENCODING_PARAM }, <nl> + { " dtx ", " Allow DTX ( generate comfort noise )", offsetof ( AMRWBContext , allow_dtx ), FF_OPT_TYPE_INT , { 0 }, 0 , 1 , AV_OPT_FLAG_AUDIO_PARAM | AV_OPT_FLAG_ENCODING_PARAM }, <nl> { NULL } <nl> }; <nl> 
static int dirac_decode_picture_header ( DiracContext * s ) <nl> get_buffer_with_edge ( s -> avctx , s -> ref_pics [ i ]-> avframe , AV_GET_BUFFER_FLAG_REF ); <nl> break ; <nl> } <nl> + <nl> + if (! s -> ref_pics [ i ]) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Reference could not be allocated \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> } <nl>  <nl> /* retire the reference frames that are not used anymore */
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> } <nl> break ; <nl> case MKTAG (' s ', ' t ', ' r ', ' d '): <nl> + st = s -> streams [ stream_index ]; <nl> if ( stream_index >= ( unsigned ) s -> nb_streams || st -> codec -> extradata_size ) { <nl> avio_skip ( pb , size ); <nl> } else {
 <nl> FFTContext * av_fft_init ( int nbits , int inverse ) <nl> { <nl> - FFTContext * s = av_malloc ( sizeof (* s )); <nl> + FFTContext * s = av_mallocz ( sizeof (* s )); <nl>  <nl> if ( s && ff_fft_init ( s , nbits , inverse )) <nl> av_freep (& s );
static int lag_decode_prob ( GetBitContext * gb , uint32_t * value ) <nl> } <nl>  <nl> val = get_bits_long ( gb , bits ); <nl> - val |= 1 << bits ; <nl> + val |= 1U << bits ; <nl>  <nl> * value = val - 1 ; <nl> 
static av_cold int vtenc_close ( AVCodecContext * avctx ) <nl>  <nl> if (! vtctx -> session ) return 0 ; <nl>  <nl> - VTCompressionSessionInvalidate ( vtctx -> session ); <nl> pthread_cond_destroy (& vtctx -> cv_sample_sent ); <nl> pthread_mutex_destroy (& vtctx -> lock ); <nl> CFRelease ( vtctx -> session );
static void sample_queue_push ( HintSampleQueue * queue , uint8_t * data , int size , <nl> return ; <nl> if (! queue -> samples || queue -> len >= queue -> size ) { <nl> HintSample * samples ; <nl> - samples = av_realloc ( queue -> samples , sizeof ( HintSample ) * ( queue -> size + 10 )); <nl> + samples = av_realloc_array ( queue -> samples , queue -> size + 10 , sizeof ( HintSample )); <nl> if (! samples ) <nl> return ; <nl> queue -> size += 10 ;
static int decode_codestream ( J2kDecoderContext * s ) <nl> } <nl>  <nl> marker = bytestream_get_be16 (& s -> buf ); <nl> + if ( s -> avctx -> debug & FF_DEBUG_STARTCODE ) <nl> + av_log ( s -> avctx , AV_LOG_DEBUG , " marker 0x %. 4X at pos 0x % x \ n ", marker , s -> buf - s -> buf_start - 4 ); <nl> oldbuf = s -> buf ; <nl>  <nl> if ( marker == J2K_SOD ){
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> m -> size_var = 8 ; <nl> // size_var is equal to 8 or 16 depending on the size of box <nl>  <nl> - if ( m -> tracksize + tsmb_size > avpkt -> size ) <nl> + if ( tsmb_size > avpkt -> size - m -> tracksize ) <nl> break ; <nl>  <nl> for ( size_t i = 0 ; i < box_count ; i ++) {
static int adts_aac_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - return av_append_packet ( s -> pb , pkt , fsize - ADTS_HEADER_SIZE ); <nl> + ret = av_append_packet ( s -> pb , pkt , fsize - ADTS_HEADER_SIZE ); <nl> + if ( ret < 0 ) <nl> + av_packet_unref ( pkt ); <nl> + <nl> + return ret ; <nl> } <nl>  <nl> AVInputFormat ff_aac_demuxer = {
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> int err ; <nl>  <nl> if ( buf_index >= next_avc ) { <nl> - if ( buf_index >= buf_size ) break ; <nl> + if ( buf_index >= buf_size - h -> nal_length_size ) break ; <nl> nalsize = 0 ; <nl> for ( i = 0 ; i < h -> nal_length_size ; i ++) <nl> nalsize = ( nalsize << 8 ) | buf [ buf_index ++];
int av_tempfile ( const char * prefix , char ** filename , int log_offset , void * log_c <nl> if ( fd < 0 ) { <nl> int err = AVERROR ( errno ); <nl> av_log (& file_log_ctx , AV_LOG_ERROR , " ff_tempfile : Cannot open temporary file % s \ n ", * filename ); <nl> + av_freep ( filename ); <nl> return err ; <nl> } <nl> return fd ; /* success */
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> bytestream2_init (& gbc , avpkt -> data , avpkt -> size ); <nl> if ( bytestream2_get_bytes_left (& gbc ) >= 552 <nl> - && ! check_header ( gbc . buffer , bytestream2_get_bytes_left (& gbc )) <nl> && check_header ( gbc . buffer + 512 , bytestream2_get_bytes_left (& gbc ) - 512 ) <nl> ) <nl> bytestream2_skip (& gbc , 512 );
static int decode_dds1 ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> return AVERROR_INVALIDDATA ; <nl> frame += v ; <nl> } else { <nl> - if ( frame_end - frame < width + 3 ) <nl> + if ( frame_end - frame < width + 4 ) <nl> return AVERROR_INVALIDDATA ; <nl> frame [ 0 ] = frame [ 1 ] = <nl> frame [ width ] = frame [ width + 1 ] = bytestream2_get_byte ( gb );
retry : <nl> } else { <nl> level = SHOW_UBITS ( re , & s -> gb , 5 ); <nl> SKIP_CACHE ( re , & s -> gb , 5 ); <nl> - level |= SHOW_SBITS ( re , & s -> gb , 6 )<< 5 ; <nl> + level |= SHOW_SBITS ( re , & s -> gb , 6 ) * ( 1 << 5 ); <nl> SKIP_COUNTER ( re , & s -> gb , 5 + 6 ); <nl> } <nl> }
static int decode_sei ( H264Context * h ){ <nl>  <nl> switch ( type ){ <nl> case 5 : <nl> - if ( decode_unregistered_user_data ( h , size ) < 0 ); <nl> + if ( decode_unregistered_user_data ( h , size ) < 0 ) <nl> return - 1 ; <nl> break ; <nl> default :
int ff_hevc_cu_qp_delta_abs ( HEVCContext * s ) <nl> suffix_val += 1 << k ; <nl> k ++; <nl> } <nl> - if ( k == CABAC_MAX_BIN ) <nl> + if ( k == CABAC_MAX_BIN ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " CABAC_MAX_BIN : % d \ n ", k ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> while ( k --) <nl> suffix_val += get_cabac_bypass (& s -> HEVClc -> cc ) << k ;
static void get_attachment ( AVFormatContext * s , AVIOContext * pb , int length ) <nl> st -> codec -> codec_id = AV_CODEC_ID_MJPEG ; <nl> st -> codec -> codec_type = AVMEDIA_TYPE_ATTACHMENT ; <nl> st -> codec -> extradata = av_mallocz ( filesize ); <nl> + st -> id = - 1 ; <nl> if (! st -> codec -> extradata ) <nl> goto done ; <nl> st -> codec -> extradata_size = filesize ;
static void qdm2_fft_decode_tones ( QDM2Context * q , int duration , GetBitContext * <nl> return ; <nl>  <nl> local_int_14 = ( offset >> local_int_8 ); <nl> + if ( local_int_14 >= FF_ARRAY_ELEMS ( fft_level_index_table )) <nl> + return ; <nl>  <nl> if ( q -> nb_channels > 1 ) { <nl> channel = get_bits1 ( gb );
static int ffm_seek ( AVFormatContext * s , int stream_index , int64_t wanted_pts , in <nl> while ( pos_min <= pos_max ) { <nl> pts_min = get_dts ( s , pos_min ); <nl> pts_max = get_dts ( s , pos_max ); <nl> - if ( pts_min > wanted_pts || pts_max < wanted_pts ) { <nl> + if ( pts_min > wanted_pts || pts_max <= wanted_pts ) { <nl> pos = pts_min > wanted_pts ? pos_min : pos_max ; <nl> goto found ; <nl> }
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> return ret ; <nl> } <nl> } else { <nl> - if (( ret = ff_MPV_common_init ( s ) < 0 )) { <nl> + if (( ret = ff_MPV_common_init ( s )) < 0 ) { <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " ff_MPV_common_init () failed .\ n "); <nl> return ret ; <nl> }
typedef struct A64Context { <nl> AVLFG randctx ; <nl> int mc_lifetime ; <nl> int mc_use_5col ; <nl> - int mc_frame_counter ; <nl> + unsigned mc_frame_counter ; <nl> int * mc_meta_charset ; <nl> int * mc_charmap ; <nl> int * mc_best_cb ;
static int decode_pic_hdr ( IVI45DecContext * ctx , AVCodecContext * avctx ) <nl>  <nl> /* decode subdivision of the planes */ <nl> pic_conf . luma_bands = decode_plane_subdivision (& ctx -> gb ); <nl> + pic_conf . chroma_bands = 0 ; <nl> if ( pic_conf . luma_bands ) <nl> pic_conf . chroma_bands = decode_plane_subdivision (& ctx -> gb ); <nl> ctx -> is_scalable = pic_conf . luma_bands != 1 || pic_conf . chroma_bands != 1 ;
static int xan_decode_frame_type0 ( AVCodecContext * avctx ) <nl> int dec_size ; <nl>  <nl> bytestream2_seek (& s -> gb , 8 + corr_off , SEEK_SET ); <nl> - dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size ); <nl> + dec_size = xan_unpack ( s , s -> scratch_buffer , s -> buffer_size / 2 ); <nl> if ( dec_size < 0 ) <nl> dec_size = 0 ; <nl> for ( i = 0 ; i < dec_size ; i ++)
static int initFilter ( int16_t ** outFilter , int16_t ** filterPos , int * outFilterSi <nl> if ( xInc <= 1 << 16 ) filterSize = 1 + sizeFactor ; // upscale <nl> else filterSize = 1 + ( sizeFactor * srcW + dstW - 1 )/ dstW ; <nl>  <nl> - if ( filterSize > srcW - 2 ) filterSize = srcW - 2 ; <nl> + filterSize = av_clip ( filterSize , 1 , srcW - 2 ); <nl>  <nl> FF_ALLOC_OR_GOTO ( NULL , filter , dstW * sizeof (* filter )* filterSize , fail ); <nl> 
static int read_thread ( void * arg ) <nl> } <nl> if ( is -> queue_attachments_req ) { <nl> if ( is -> video_st && is -> video_st -> disposition & AV_DISPOSITION_ATTACHED_PIC ) { <nl> - AVPacket copy ; <nl> + AVPacket copy = { 0 }; <nl> if (( ret = av_copy_packet (& copy , & is -> video_st -> attached_pic )) < 0 ) <nl> goto fail ; <nl> packet_queue_put (& is -> videoq , & copy );
FF_ENABLE_DEPRECATION_WARNINGS <nl> /* Encode a dummy frame to get the extradata immediately */ <nl> if ( x -> quicktime_format ) { <nl> AVFrame * picture ; <nl> - AVPacket packet ; <nl> + AVPacket packet = { 0 }; <nl> int size , got_packet , ret ; <nl>  <nl> av_init_packet (& packet );
static int update_frame_pool ( AVCodecContext * avctx , AVFrame * frame ) <nl> break ; <nl> } <nl> case AVMEDIA_TYPE_AUDIO : { <nl> - int ch = av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> + int ch = av_frame_get_channels ( frame ); // av_get_channel_layout_nb_channels ( frame -> channel_layout ); <nl> int planar = av_sample_fmt_is_planar ( frame -> format ); <nl> int planes = planar ? ch : 1 ; <nl> 
static int tm2_read_stream ( TM2Context * ctx , const uint8_t * buf , int stream_id , i <nl> if ( len == 0 ) <nl> return 4 ; <nl>  <nl> - if ( len >= INT_MAX / 4 - 1 || len < 0 || len > buf_size ) { <nl> + if ( len >= INT_MAX / 4 - 1 || len < 0 || skip > buf_size ) { <nl> av_log ( ctx -> avctx , AV_LOG_ERROR , " Error , invalid stream size .\ n "); <nl> return - 1 ; <nl> }
static int decode_lowdelay ( DiracContext * s ) <nl> slice_num ++; <nl>  <nl> buf += bytes ; <nl> - bufsize -= bytes * 8 ; <nl> + if ( bufsize / 8 >= bytes ) <nl> + bufsize -= bytes * 8 ; <nl> + else <nl> + bufsize = 0 ; <nl> } <nl>  <nl> avctx -> execute ( avctx , decode_lowdelay_slice , slices , NULL , slice_num ,
static int decode_subframe_fixed ( FLACContext * s , int channel , int pred_order ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> int32_t * decoded = s -> decoded [ channel ]; <nl> - int a , b , c , d , i ; <nl> + int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl>  <nl> /* warm up samples */ <nl> for ( i = 0 ; i < pred_order ; i ++) {
static int xan_huffman_decode ( uint8_t * dest , int dest_len , <nl> return ret ; <nl>  <nl> while ( val != 0x16 ) { <nl> - unsigned idx = val - 0x17 + get_bits1 (& gb ) * byte ; <nl> + unsigned idx ; <nl> + if ( get_bits_left (& gb ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + idx = val - 0x17 + get_bits1 (& gb ) * byte ; <nl> if ( idx >= 2 * byte ) <nl> return AVERROR_INVALIDDATA ; <nl> val = src [ idx ];
static int vmd_read_header ( AVFormatContext * s ) <nl> vst -> codec -> width >>= 1 ; <nl> vst -> codec -> height >>= 1 ; <nl> } <nl> - vst -> codec -> extradata_size = VMD_HEADER_SIZE ; <nl> vst -> codec -> extradata = av_mallocz ( VMD_HEADER_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! vst -> codec -> extradata ) <nl> + return AVERROR ( ENOMEM ); <nl> + vst -> codec -> extradata_size = VMD_HEADER_SIZE ; <nl> memcpy ( vst -> codec -> extradata , vmd -> vmd_header , VMD_HEADER_SIZE ); <nl> } <nl> 
static int start_frame ( AVFilterLink * link , AVFilterBufferRef * picref ) <nl>  <nl> av_assert0 ( picref ); <nl>  <nl> + if ( picref -> video -> h < 3 || picref -> video -> w < 3 ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Video of less than 3 columns or lines is not supported \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> if ( yadif -> frame_pending ) <nl> return_frame ( ctx , 1 ); <nl> 
static int msf_probe ( AVProbeData * p ) <nl> if ( AV_RB32 ( p -> buf + 16 ) <= 0 ) <nl> return 0 ; <nl>  <nl> + if ( AV_RB32 ( p -> buf + 4 ) > 16 ) <nl> + return AVPROBE_SCORE_MAX / 5 ; // unsupported / unknown codec <nl> + <nl> return AVPROBE_SCORE_MAX / 3 * 2 ; <nl> } <nl> 
static int mv_read_header ( AVFormatContext * avctx ) <nl> { <nl> MvContext * mv = avctx -> priv_data ; <nl> AVIOContext * pb = avctx -> pb ; <nl> - AVStream * ast , * vst ; <nl> + AVStream * ast = NULL , * vst = NULL ; // initialization to suppress warning <nl> int version , i ; <nl>  <nl> avio_skip ( pb , 4 );
static void free_stream ( AVStream ** pst ) <nl> av_freep (& st -> index_entries ); <nl> # if FF_API_LAVF_AVCTX <nl> FF_DISABLE_DEPRECATION_WARNINGS <nl> - av_freep (& st -> codec -> extradata ); <nl> - av_freep (& st -> codec -> subtitle_header ); <nl> - av_freep (& st -> codec ); <nl> + avcodec_free_context (& st -> codec ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> # endif <nl> av_freep (& st -> priv_data );
static int decode_frame ( AVCodecContext * avctx , <nl> int prev_y = 0 , prev_u = 0 , prev_v = 0 ; <nl> uint8_t * rbuf ; <nl>  <nl> + if ( buf_size <= 8 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " buf_size % d is too small \ n ", buf_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> rbuf = av_malloc ( buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! rbuf ){ <nl> av_log ( avctx , AV_LOG_ERROR , " Cannot allocate temporary buffer \ n ");
AVIOContext * avio_alloc_context ( <nl> int64_t (* seek )( void * opaque , int64_t offset , int whence )) <nl> { <nl> AVIOContext * s = av_mallocz ( sizeof ( AVIOContext )); <nl> + if (! s ) <nl> + return NULL ; <nl> ffio_init_context ( s , buffer , buffer_size , write_flag , opaque , <nl> read_packet , write_packet , seek ); <nl> return s ;
static int ftp_send_command ( FTPContext * s , const char * command , <nl> if ( response ) <nl> * response = NULL ; <nl>  <nl> + if (! s -> conn_control ) <nl> + return AVERROR ( EIO ); <nl> + <nl> if (( err = ffurl_write ( s -> conn_control , command , strlen ( command ))) < 0 ) <nl> return err ; <nl> if (! err )
void av_dump_format ( AVFormatContext * ic , <nl> int is_output ) <nl> { <nl> int i ; <nl> - uint8_t * printed = av_mallocz ( ic -> nb_streams ); <nl> + uint8_t * printed = ic -> nb_streams ? av_mallocz ( ic -> nb_streams ) : NULL ; <nl> if ( ic -> nb_streams && ! printed ) <nl> return ; <nl> 
error : <nl> static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , <nl> int S ) <nl> { <nl> - int bit ; <nl> + unsigned bit ; <nl>  <nl> if ( s -> extra_bits ) { <nl> S <<= s -> extra_bits ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> frame_len = 1 << frame_len_bits ; <nl>  <nl> - if ( s -> channels > MAX_CHANNELS ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + if ( avctx -> channels > MAX_CHANNELS ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", avctx -> channels ); <nl> return - 1 ; <nl> } <nl> 
int ff_img_read_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> ret = avio_read ( f [ 0 ], header , PROBE_BUF_MIN ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + memset ( header + ret , 0 , sizeof ( header ) - ret ); <nl> avio_skip ( f [ 0 ], - ret ); <nl> pd . buf = header ; <nl> pd . buf_size = ret ;
static int open_file ( AVFormatContext * avf , unsigned fileno ) <nl> if (! cat -> avf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - cat -> avf -> flags |= avf -> flags ; <nl> + cat -> avf -> flags |= avf -> flags & ~ AVFMT_FLAG_CUSTOM_IO ; <nl> cat -> avf -> interrupt_callback = avf -> interrupt_callback ; <nl>  <nl> if (( ret = ff_copy_whiteblacklists ( cat -> avf , avf )) < 0 )
static int opus_decode_frame ( OpusStreamContext * s , const uint8_t * data , int size <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Error resampling SILK data .\ n "); <nl> return samples ; <nl> } <nl> + av_assert2 (( samples & 7 ) == 0 ); <nl> s -> delayed_samples += s -> packet . frame_duration - samples ; <nl> } else <nl> ff_silk_flush ( s -> silk );
static void compute_default_clut ( AVSubtitleRect * rect , int w , int h ) <nl> list_inv [ i ] = bestv ; <nl> } <nl>  <nl> - count = i - 1 ; <nl> + count = FFMAX ( i - 1 , 1 ); <nl> for ( i --; i >= 0 ; i --) { <nl> int v = i * 255 / count ; <nl> AV_WN32 ( rect -> data [ 1 ] + 4 * list_inv [ i ], RGBA ( v / 2 , v , v / 2 , v ));
static void show_packets ( WriterContext * w , AVFormatContext * fmt_ctx ) <nl> if ( do_show_frames && <nl> get_video_frame ( fmt_ctx , & frame , & pkt )) { <nl> show_frame ( w , & frame , fmt_ctx -> streams [ pkt . stream_index ]); <nl> - av_destruct_packet (& pkt ); <nl> + av_free_packet (& pkt ); <nl> } <nl> } <nl> av_init_packet (& pkt );
retry : <nl> if ( c -> itunes_metadata && atom . size > 8 ) { <nl> int data_size = avio_rb32 ( pb ); <nl> int tag = avio_rl32 ( pb ); <nl> - if ( tag == MKTAG (' d ',' a ',' t ',' a ')) { <nl> + if ( tag == MKTAG (' d ',' a ',' t ',' a ') && data_size <= atom . size ) { <nl> data_type = avio_rb32 ( pb ); // type <nl> avio_rb32 ( pb ); // unknown <nl> str_size = data_size - 16 ;
static int applehttp_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> /* If this isn ' t a live stream , calculate the total duration of the <nl> * stream . */ <nl> if ( c -> finished ) { <nl> - int duration = 0 ; <nl> + int64_t duration = 0 ; <nl> for ( i = 0 ; i < c -> variants [ 0 ]-> n_segments ; i ++) <nl> duration += c -> variants [ 0 ]-> segments [ i ]-> duration ; <nl> s -> duration = duration * AV_TIME_BASE ;
static int ivi_init_tiles ( IVIBandDesc * band , IVITile * ref_tile , <nl> band -> mb_size ); <nl>  <nl> av_freep (& tile -> mbs ); <nl> - tile -> mbs = av_malloc ( tile -> num_MBs * sizeof ( IVIMbInfo )); <nl> + tile -> mbs = av_mallocz ( tile -> num_MBs * sizeof ( IVIMbInfo )); <nl> if (! tile -> mbs ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int libopenjpeg_decode_frame ( AVCodecContext * avctx , <nl> // Decode the codestream <nl> image = opj_decode_with_info ( dec , stream , NULL ); <nl> opj_cio_close ( stream ); <nl> + if (! image ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Error decoding codestream .\ n "); <nl> + opj_destroy_decompress ( dec ); <nl> + return - 1 ; <nl> + } <nl>  <nl> pixel_size = av_pix_fmt_descriptors [ avctx -> pix_fmt ]. comp [ 0 ]. step_minus1 + 1 ; <nl> ispacked = libopenjpeg_ispacked ( avctx -> pix_fmt );
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl> } <nl> } else { <nl> - avpriv_request_sample ( s , " Uncompressed image "); <nl> + avpriv_request_sample ( avctx , " Uncompressed image "); <nl> return avpkt -> size ; <nl> } <nl> finish :
av_cold int ff_msmpeg4_decode_init ( AVCodecContext * avctx ) <nl> int i ; <nl> MVTable * mv ; <nl>  <nl> + if ( avctx -> width <= 0 || avctx -> height <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " invalid dimensions \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( ff_h263_decode_init ( avctx ) < 0 ) <nl> return - 1 ; <nl> 
static int g2m_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( got_header ) <nl> c -> got_header = 1 ; <nl>  <nl> - if ( c -> width && c -> height ) { <nl> + if ( c -> width && c -> height && c -> framebuf ) { <nl> if (( ret = ff_get_buffer ( avctx , pic , 0 )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return ret ;
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> flv_same_video_codec ( st -> codec , flags )) { <nl> break ; <nl> } <nl> - } else if ( st -> id == stream_type ) { <nl> - break ; <nl> + } else if ( stream_type == FLV_STREAM_TYPE_DATA ) { <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_DATA ) <nl> + break ; <nl> } <nl> } <nl> if ( i == s -> nb_streams ){
static int h261_decode_mb ( H261Context * h ){ <nl>  <nl> // Read mtype <nl> h -> mtype = get_vlc2 (& s -> gb , h261_mtype_vlc . table , H261_MTYPE_VLC_BITS , 2 ); <nl> + if ( h -> mtype < 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " illegal mtype % d \ n ", h -> mtype ); <nl> + return SLICE_ERROR ; <nl> + } <nl> h -> mtype = h261_mtype_map [ h -> mtype ]; <nl>  <nl> // Read mquant
static int encode_picture_ls ( AVCodecContext * avctx , AVPacket * pkt , <nl>  <nl> ls_store_lse ( state , & pb ); <nl>  <nl> - zero = av_mallocz ( p -> linesize [ 0 ]); <nl> + zero = av_mallocz ( FFABS ( p -> linesize [ 0 ])); <nl> + if (! zero ) <nl> + return AVERROR ( ENOMEM ); <nl> last = zero ; <nl> cur = p -> data [ 0 ]; <nl> if ( avctx -> pix_fmt == PIX_FMT_GRAY8 ){
static void ff_compute_band_indexes ( MPADecodeContext * s , GranuleDef * g ) <nl> else <nl> g -> long_end = 6 ; <nl>  <nl> - g -> short_start = 2 + ( s -> sample_rate_index != 8 ); <nl> + g -> short_start = 3 ; <nl> } else { <nl> g -> long_end = 0 ; <nl> g -> short_start = 0 ;
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> offset = matroska_decode_buffer (& pkt_data ,& pkt_size , track ); <nl> if ( offset < 0 ) <nl> continue ; <nl> + av_assert0 ( offset + pkt_size >= pkt_size ); <nl> } <nl>  <nl> pkt = av_mallocz ( sizeof ( AVPacket ));
static int create_filter ( AVFilterContext ** filt_ctx , AVFilterGraph * ctx , int ind <nl> return ret ; <nl> } <nl>  <nl> - if (! strcmp ( filt_name , " scale ") && ! strstr ( args , " flags ")) { <nl> + if (! strcmp ( filt_name , " scale ") && args && ! strstr ( args , " flags ")) { <nl> snprintf ( tmp_args , sizeof ( tmp_args ), "% s :% s ", <nl> args , ctx -> scale_sws_opts ); <nl> args = tmp_args ;
static int idcin_read_packet ( AVFormatContext * s , <nl> } <nl>  <nl> chunk_size = avio_rl32 ( pb ); <nl> + if ( chunk_size < 4 || chunk_size > INT_MAX - 4 ) { <nl> + av_log ( s , AV_LOG_ERROR , " invalid chunk size : % u \ n ", chunk_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> /* skip the number of decoded bytes ( always equal to width * height ) */ <nl> avio_skip ( pb , 4 ); <nl> chunk_size -= 4 ;
static inline int read_line ( AVFormatContext * s , char * rbuf , const int rbufsize , <nl>  <nl> do { <nl> ret = ffurl_read_complete ( rt -> rtsp_hd , rbuf + idx , 1 ); <nl> - if ( ret < 0 ) <nl> - return ret ; <nl> + if ( ret <= 0 ) <nl> + return ret ? ret : AVERROR_EOF ; <nl> if ( rbuf [ idx ] == '\ r ') { <nl> /* Ignore */ <nl> } else if ( rbuf [ idx ] == '\ n ') {
static int mov_read_stsz ( MOVContext * c , ByteIOContext * pb , MOVAtom atom ) <nl> return - 1 ; <nl> } <nl>  <nl> - if ( entries >= UINT_MAX / sizeof ( int )) <nl> + if ( entries >= UINT_MAX / sizeof ( int ) || entries >= ( UINT_MAX - 4 ) / field_size ) <nl> return - 1 ; <nl> sc -> sample_sizes = av_malloc ( entries * sizeof ( int )); <nl> if (! sc -> sample_sizes )
static int wsvqa_read_packet ( AVFormatContext * s , <nl> switch ( chunk_type ) { <nl> case SND1_TAG : <nl> /* unpacked size is stored in header */ <nl> - pkt -> duration = AV_RL16 ( pkt -> data ) / wsvqa -> channels ; <nl> + if ( pkt -> data ) <nl> + pkt -> duration = AV_RL16 ( pkt -> data ) / wsvqa -> channels ; <nl> break ; <nl> case SND2_TAG : <nl> /* 2 samples / byte , 1 or 2 samples per frame depending on stereo */
FF_ENABLE_DEPRECATION_WARNINGS <nl> if (! avctx -> rc_initial_buffer_occupancy ) <nl> avctx -> rc_initial_buffer_occupancy = avctx -> rc_buffer_size * 3 / 4 ; <nl>  <nl> - if ( avctx -> ticks_per_frame && <nl> + if ( avctx -> ticks_per_frame && avctx -> time_base . num && <nl> avctx -> ticks_per_frame > INT_MAX / avctx -> time_base . num ) { <nl> av_log ( avctx , AV_LOG_ERROR , <nl> " ticks_per_frame % d too large for the timebase % d /% d .",
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> c -> fmt = buf [ 3 ]; <nl> c -> bw = buf [ 4 ]; <nl> c -> bh = buf [ 5 ]; <nl> + c -> decode_intra = NULL ; <nl> + c -> decode_xor = NULL ; <nl>  <nl> buf += 6 ; <nl> len -= 6 ;
static int X264_frame ( AVCodecContext * ctx , AVPacket * pkt , const AVFrame * frame , <nl> X264Context * x4 = ctx -> priv_data ; <nl> x264_nal_t * nal ; <nl> int nnal , i , ret ; <nl> - x264_picture_t pic_out ; <nl> + x264_picture_t pic_out = { 0 }; <nl>  <nl> x264_picture_init ( & x4 -> pic ); <nl> x4 -> pic . img . i_csp = x4 -> params . i_csp ;
static int ra144_decode_frame ( AVCodecContext * avctx , void * data , <nl> do_output_subblock ( ractx , block_coefs [ i ], refl_rms [ i ], & gb ); <nl>  <nl> for ( j = 0 ; j < BLOCKSIZE ; j ++) <nl> - * samples ++ = av_clip_int16 ( ractx -> curr_sblock [ j + 10 ] << 2 ); <nl> + * samples ++ = av_clip_int16 ( ractx -> curr_sblock [ j + 10 ] * ( 1 << 2 )); <nl> } <nl>  <nl> ractx -> old_energy = energy ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> for ( x = 0 ; x < num_possible_block_sizes ; x ++) { <nl> int v = 0 ; <nl> while ( s -> sfb_offsets [ x ][ v + 1 ] << x < offset ) <nl> - ++ v ; <nl> + if (++ v >= MAX_BANDS ) <nl> + return AVERROR_INVALIDDATA ; <nl> s -> sf_offsets [ i ][ x ][ b ] = v ; <nl> } <nl> }
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> goto fail ; <nl>  <nl> s -> cbf_luma = av_malloc ( sps -> min_tb_width * sps -> min_tb_height ); <nl> - s -> tab_ipm = av_malloc ( min_pu_size ); <nl> + s -> tab_ipm = av_mallocz ( min_pu_size ); <nl> s -> is_pcm = av_malloc ( min_pu_size ); <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ;
 <nl> static av_always_inline int even ( uint64_t layout ) <nl> { <nl> - return (! layout || ( layout & ( layout - 1 ))); <nl> + return (! layout || !!( layout & ( layout - 1 ))); <nl> } <nl>  <nl> static int sane_layout ( uint64_t layout )
int av_reallocp_array ( void * ptr , size_t nmemb , size_t size ) <nl> { <nl> void ** ptrptr = ptr ; <nl> * ptrptr = av_realloc_f (* ptrptr , nmemb , size ); <nl> - if (!* ptrptr && !( nmemb && size )) <nl> + if (!* ptrptr && nmemb && size ) <nl> return AVERROR ( ENOMEM ); <nl> return 0 ; <nl> }
static int video_thread ( void * arg ) <nl> ret = queue_picture ( is , frame , pts , duration , frame -> pkt_pos , is -> viddec . pkt_serial ); <nl> av_frame_unref ( frame ); <nl> # if CONFIG_AVFILTER <nl> + if ( is -> videoq . serial != is -> viddec . pkt_serial ) <nl> + break ; <nl> } <nl> # endif <nl> 
fixup_vorbis_headers ( AVFormatContext * as , struct oggvorbis_private * priv , <nl> len = priv -> len [ 0 ] + priv -> len [ 1 ] + priv -> len [ 2 ]; <nl> buf_len = len + len / 255 + 64 ; <nl> ptr = * buf = av_realloc ( NULL , buf_len ); <nl> + if (!* buf ) <nl> + return 0 ; <nl> memset (* buf , '\ 0 ', buf_len ); <nl>  <nl> ptr [ 0 ] = 2 ;
static int64_t find_best_filter ( const DCAADPCMEncContext * s , const int32_t * in , <nl> { <nl> const premultiplied_coeffs * precalc_data = s -> private_data ; <nl> int i , j , k = 0 ; <nl> - int vq ; <nl> + int vq = - 1 ; <nl> int64_t err ; <nl> int64_t min_err = 1ll << 62 ; <nl> int64_t corr [ 15 ];
int ff_init_vlc_sparse ( VLC * vlc , int nb_bits , int nb_codes , <nl> av_dlog ( NULL , " build table nb_codes =% d \ n ", nb_codes ); <nl>  <nl> buf = av_malloc (( nb_codes + 1 ) * sizeof ( VLCcode )); <nl> + if (! buf ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> av_assert0 ( symbols_size <= 2 || ! symbols ); <nl> j = 0 ;
static int dxv_decompress_raw ( AVCodecContext * avctx ) <nl> DXVContext * ctx = avctx -> priv_data ; <nl> GetByteContext * gbc = & ctx -> gbc ; <nl>  <nl> + if ( bytestream2_get_bytes_left ( gbc ) < ctx -> tex_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> bytestream2_get_buffer ( gbc , ctx -> tex_data , ctx -> tex_size ); <nl> return 0 ; <nl> }
static int tm2_build_huff_table ( TM2Context * ctx , TM2Codes * code ) <nl> huff . val_bits , huff . max_bits ); <nl> return - 1 ; <nl> } <nl> - if (( huff . nodes < 0 ) || ( huff . nodes > 0x10000 )) { <nl> + if (( huff . nodes <= 0 ) || ( huff . nodes > 0x10000 )) { <nl> av_log ( ctx -> avctx , AV_LOG_ERROR , " Incorrect number of Huffman tree nodes : % i \ n ", huff . nodes ); <nl> return - 1 ; <nl> }
static int gdv_decode_frame ( AVCodecContext * avctx , void * data , <nl> unsigned r = bytestream2_get_byte ( gb ); <nl> unsigned g = bytestream2_get_byte ( gb ); <nl> unsigned b = bytestream2_get_byte ( gb ); <nl> - gdv -> pal [ i ] = 0xFF << 24 | r << 18 | g << 10 | b << 2 ; <nl> + gdv -> pal [ i ] = 0xFFU << 24 | r << 18 | g << 10 | b << 2 ; <nl> } <nl> break ; <nl> case 3 :
static int filter_frame ( AVFilterLink * inlink , AVFrame * in ) <nl> direct = 1 ; <nl> out = in ; <nl> } else { <nl> + direct = 0 ; <nl> out = ff_get_video_buffer ( outlink , outlink -> w , outlink -> h ); <nl> if (! out ) { <nl> av_frame_free (& in );
pp_context * pp_get_context ( int width , int height , int cpuCaps ){ <nl> int stride = FFALIGN ( width , 16 ); // assumed / will realloc if needed <nl> int qpStride = ( width + 15 )/ 16 + 2 ; // assumed / will realloc if needed <nl>  <nl> + if (! c ) <nl> + return NULL ; <nl> + <nl> c -> av_class = & av_codec_context_class ; <nl> if ( cpuCaps & PP_FORMAT ){ <nl> c -> hChromaSubSample = cpuCaps & 0x3 ;
void mpeg_motion_internal ( MpegEncContext * s , <nl> { <nl> uint8_t * ptr_y , * ptr_cb , * ptr_cr ; <nl> int dxy , uvdxy , mx , my , src_x , src_y , <nl> - uvsrc_x , uvsrc_y , v_edge_pos , uvlinesize , linesize ; <nl> + uvsrc_x , uvsrc_y , v_edge_pos ; <nl> + ptrdiff_t uvlinesize , linesize ; <nl>  <nl> # if 0 <nl> if ( s -> quarter_sample )
static int idcin_read_packet ( AVFormatContext * s , <nl> chunk_size = avio_rl32 ( pb ); <nl> /* skip the number of decoded bytes ( always equal to width * height ) */ <nl> avio_skip ( pb , 4 ); <nl> + if ( chunk_size < 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> chunk_size -= 4 ; <nl> ret = av_get_packet ( pb , pkt , chunk_size ); <nl> if ( ret < 0 )
static av_cold int init ( AVFilterContext * ctx , const char * args ) <nl> av_log ( ctx , AV_LOG_ERROR , " Invalid parameter .\ n "); <nl> return AVERROR ( EINVAL ); <nl> } <nl> - args += strlen ( name )+ 1 ; <nl> + args += strlen ( name ); <nl> + if ( args [ 0 ] == '=') <nl> + args ++; <nl>  <nl> for ( i = 0 ; ; i ++){ <nl> if (! filters [ i ] || ! strcmp ( name , filters [ i ]-> name ))
static void dwt_decode97_int ( DWTContext * s , int32_t * t ) <nl> line += 5 ; <nl>  <nl> for ( i = 0 ; i < w * h ; i ++) <nl> - data [ i ] <<= I_PRESHIFT ; <nl> + data [ i ] *= 1 << I_PRESHIFT ; <nl>  <nl> for ( lev = 0 ; lev < s -> ndeclevels ; lev ++) { <nl> int lh = s -> linelen [ lev ][ 0 ],
int av_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> av_log ( s , AV_LOG_WARNING , <nl> " Dropped corrupted packet ( stream = % d )\ n ", <nl> pkt -> stream_index ); <nl> + av_free_packet ( pkt ); <nl> continue ; <nl> } <nl> 
matroska_read_seek ( AVFormatContext * s , int stream_index , int64_t timestamp , <nl> matroska -> skip_to_keyframe = !( flags & AVSEEK_FLAG_ANY ); <nl> matroska -> skip_to_stream = st ; <nl> matroska -> peek_id = 0 ; <nl> + av_update_cur_dts ( s , st , st -> index_entries [ index ]. timestamp ); <nl> return 0 ; <nl> } <nl> 
static int oma_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> pkt -> stream_index = 0 ; <nl>  <nl> - if ( pos > 0 ) { <nl> + if ( pos > 0 && byte_rate > 0 ) { <nl> pkt -> pts = <nl> pkt -> dts = av_rescale ( pos , st -> time_base . den , <nl> byte_rate * ( int64_t ) st -> time_base . num );
static int decode_lt_rps ( HEVCContext * s , LongTermRPS * rps , GetBitContext * gb ) <nl> static int set_sps ( HEVCContext * s , const HEVCSPS * sps ) <nl> { <nl> int ret ; <nl> - int num = 0 , den = 0 ; <nl> + unsigned num = 0 , den = 0 ; <nl>  <nl> pic_arrays_free ( s ); <nl> ret = pic_arrays_init ( s , sps );
# include " libavcodec / bytestream . h " <nl> # include " libavformat / avformat . h " <nl>  <nl> -# include < FuzzerInterface . h > <nl> + int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ); <nl>  <nl> static void error ( const char * err ) <nl> {
FF_ENABLE_DEPRECATION_WARNINGS <nl> if ( avctx -> level > 0 ) <nl> x4 -> params . i_level_idc = avctx -> level ; <nl>  <nl> - x4 -> params . rc . f_rate_tolerance = <nl> - ( float ) avctx -> bit_rate_tolerance / avctx -> bit_rate ; <nl> + if ( avctx -> bit_rate > 0 ) <nl> + x4 -> params . rc . f_rate_tolerance = <nl> + ( float ) avctx -> bit_rate_tolerance / avctx -> bit_rate ; <nl>  <nl> if (( avctx -> rc_buffer_size ) && <nl> ( avctx -> rc_initial_buffer_occupancy <= avctx -> rc_buffer_size )) {
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> frame_len = 1 << frame_len_bits ; <nl>  <nl> - if ( s -> channels > MAX_CHANNELS ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + if ( avctx -> channels > MAX_CHANNELS ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", avctx -> channels ); <nl> return - 1 ; <nl> } <nl> 
static int apng_read_header ( AVFormatContext * s ) <nl> AVIOContext * pb = s -> pb ; <nl> uint32_t len , tag ; <nl> AVStream * st ; <nl> - int ret = AVERROR_INVALIDDATA , acTL_found = 0 ; <nl> + int acTL_found = 0 ; <nl> + int64_t ret = AVERROR_INVALIDDATA ; <nl>  <nl> /* verify PNGSIG */ <nl> if ( avio_rb64 ( pb ) != PNGSIG )
static inline uint8_t lag_get_rac ( lag_rac * l ) <nl> l -> range -= range_scaled * l -> prob [ 255 ]; <nl> } <nl>  <nl> + if (! l -> range ) <nl> + l -> range = 0x80 ; <nl> + <nl> l -> low -= range_scaled * l -> prob [ val ]; <nl>  <nl> return val ;
static int vp3_decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> last_frame = s -> golden_frame ; <nl> s -> last_frame . type = FF_BUFFER_TYPE_COPY ; <nl> + ff_thread_report_progress (& s -> last_frame , INT_MAX , 0 ); <nl> } <nl> } <nl> 
static int dpcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl> case CODEC_ID_SOL_DPCM : <nl> if ( avctx -> codec_tag != 3 ) { <nl> - uint8_t * output_samples_u8 = data ; <nl> + uint8_t * output_samples_u8 = s -> frame . data [ 0 ]; <nl> while ( buf < buf_end ) { <nl> uint8_t n = * buf ++; <nl> 
static int http_handshake ( URLContext * c ) <nl> av_log ( c , AV_LOG_TRACE , " Lower protocol \ n "); <nl> if (( ret = ffurl_handshake ( cl )) > 0 ) <nl> return 2 + ret ; <nl> - if (( ret < 0 )) <nl> + if ( ret < 0 ) <nl> return ret ; <nl> ch -> handshake_step = READ_HEADERS ; <nl> ch -> is_connected_server = 1 ;
static int xv_write_header ( AVFormatContext * s ) <nl> if ( XvQueryAdaptors ( xv -> display , DefaultRootWindow ( xv -> display ), & num_adaptors , & ai ) != Success ) <nl> return AVERROR_EXTERNAL ; <nl> xv -> xv_port = ai [ 0 ]. base_id ; <nl> + XvFreeAdaptorInfo ( ai ); <nl>  <nl> if ( encctx -> pix_fmt != AV_PIX_FMT_YUV420P ) { <nl> av_log ( s , AV_LOG_ERROR ,
static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> AVIOContext * pbx ; <nl> unsigned char * buffer = NULL ; <nl> int buffer_size = 0 ; <nl> - const ID3v2EMFunc * extra_func ; <nl> + const ID3v2EMFunc * extra_func = NULL ; <nl> unsigned char * compressed_buffer = NULL ; <nl> int compressed_buffer_size = 0 ; <nl> 
static int ac3_decode_frame ( AVCodecContext * avctx , void * data , <nl> avctx -> audio_service_type = AV_AUDIO_SERVICE_TYPE_KARAOKE ; <nl>  <nl> /* get output buffer */ <nl> + avctx -> channels = s -> out_channels ; <nl> s -> frame . nb_samples = s -> num_blocks * 256 ; <nl> if (( ret = avctx -> get_buffer ( avctx , & s -> frame )) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n ");
static int mov_write_single_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int64_t frag_duration = 0 ; <nl> int size = pkt -> size ; <nl>  <nl> + int ret = check_pkt ( s , pkt ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> if ( mov -> flags & FF_MOV_FLAG_FRAG_DISCONT ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> nb_streams ; i ++)
static int parse_cookie ( HTTPContext * s , const char * p , AVDictionary ** cookies ) <nl> } <nl> } <nl> } <nl> + av_dict_free (& new_params ); <nl>  <nl> // duplicate the cookie name ( dict will dupe the value ) <nl> if (!( eql = strchr ( p , '='))) return AVERROR ( EINVAL );
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> int reslevelno , bandno , gbandno = 0 , ret , i , j ; <nl> uint32_t csize = 1 ; <nl>  <nl> + if (! codsty -> nreslevels2decode ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " nreslevels2decode uninitialized \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( ret = ff_jpeg2000_dwt_init (& comp -> dwt , comp -> coord , <nl> codsty -> nreslevels2decode - 1 , <nl> codsty -> transform ))
static int decode_audio_specific_config ( AACContext * ac , <nl> */ <nl> static av_always_inline int lcg_random ( int previous_val ) <nl> { <nl> - return previous_val * 1664525 + 1013904223 ; <nl> + union { unsigned u ; int s ; } v = { previous_val * 1664525u + 1013904223 }; <nl> + return v . s ; <nl> } <nl>  <nl> static av_always_inline void reset_predict_state ( PredictorState * ps )
static int filter_frame ( AVFilterLink * inlink , AVFrame * buf ) <nl> av_frame_free (& buf ); <nl>  <nl> end : <nl> - vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += buf -> nb_samples ; <nl> + vol -> var_values [ VAR_NB_CONSUMED_SAMPLES ] += out_buf -> nb_samples ; <nl> return ff_filter_frame ( outlink , out_buf ); <nl> } <nl> 
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> value = UINT_MAX ; <nl> } <nl> } else { <nl> - if ( type_sizes [ type ] * count > 4 ) { <nl> - off = bytestream2_tell (& s -> gb ); <nl> - } <nl> + off = bytestream2_tell (& s -> gb ); <nl> } <nl>  <nl> switch ( tag ) {
static void qtrle_decode_8bpp ( QtrleContext * s ) <nl> int header ; <nl> int start_line ; <nl> int lines_to_change ; <nl> - signed char rle_code ; <nl> + int rle_code ; <nl> int row_ptr , pixel_ptr ; <nl> int row_inc = s -> frame . linesize [ 0 ]; <nl> unsigned char pi1 , pi2 , pi3 , pi4 ; /* 4 palette indices */
static inline uint64_t v4l2_get_pts ( V4L2Buffer * avbuf ) <nl> int64_t v4l2_pts ; <nl>  <nl> /* convert pts back to encoder timebase */ <nl> - v4l2_pts = avbuf -> buf . timestamp . tv_sec * USEC_PER_SEC + avbuf -> buf . timestamp . tv_usec ; <nl> + v4l2_pts = ( int64_t ) avbuf -> buf . timestamp . tv_sec * USEC_PER_SEC + <nl> + avbuf -> buf . timestamp . tv_usec ; <nl>  <nl> return av_rescale_q ( v4l2_pts , v4l2_timebase , s -> avctx -> time_base ); <nl> }
static int check_fps ( int fps ) <nl>  <nl> static int check_timecode ( void * log_ctx , AVTimecode * tc ) <nl> { <nl> - if ( tc -> fps <= 0 ) { <nl> + if (( int ) tc -> fps <= 0 ) { <nl> av_log ( log_ctx , AV_LOG_ERROR , " Timecode frame rate must be specified \ n "); <nl> return AVERROR ( EINVAL ); <nl> }
vbv_retry : <nl> if ( s -> mb_info ) <nl> av_packet_shrink_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , s -> mb_info_size ); <nl> } else { <nl> - assert (( put_bits_ptr (& s -> pb ) == s -> pb . buf )); <nl> s -> frame_bits = 0 ; <nl> } <nl> assert (( s -> frame_bits & 7 ) == 0 );
static int svag_read_header ( AVFormatContext * s ) <nl> if ( st -> codec -> sample_rate <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl> st -> codec -> channels = avio_rl32 ( s -> pb ); <nl> - if ( st -> codec -> channels <= 0 ) <nl> + if ( st -> codec -> channels <= 0 || st -> codec -> channels > 8 ) <nl> return AVERROR_INVALIDDATA ; <nl> st -> duration = size / ( 16 * st -> codec -> channels ) * 28 ; <nl> align = avio_rl32 ( s -> pb );
static inline void xan_wc3_copy_pixel_run ( XanContext * s , AVFrame * frame , <nl> prevframe_index = ( y + motion_y ) * stride + x + motion_x ; <nl> prevframe_x = x + motion_x ; <nl>  <nl> - if ( prev_palette_plane == palette_plane && FFABS ( curframe_index - prevframe_index ) < pixel_count ) { <nl> + if ( prev_palette_plane == palette_plane && FFABS ( motion_x + width * motion_y ) < pixel_count ) { <nl> avpriv_request_sample ( s -> avctx , " Overlapping copy "); <nl> return ; <nl> }
int64_t av_gcd ( int64_t a , int64_t b ) { <nl> v -= u ; <nl> v >>= ff_ctzll ( v ); <nl> } <nl> - return u << k ; <nl> + return ( uint64_t ) u << k ; <nl> } <nl>  <nl> int64_t av_rescale_rnd ( int64_t a , int64_t b , int64_t c , enum AVRounding rnd )
static int ljpeg_decode_yuv_scan ( MJpegDecodeContext * s , int predictor , <nl> if ( s -> interlaced && s -> bottom_field ) <nl> ptr16 += linesize >> 1 ; <nl> pred &= mask ; <nl> - * ptr16 = pred + ( dc << point_transform ); <nl> + * ptr16 = pred + (( unsigned ) dc << point_transform ); <nl> } <nl> if (++ x == h ) { <nl> x = 0 ;
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> if (! get_bits1 (& ctx -> gb ) || ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> transform_id = get_bits (& ctx -> gb , 5 ); <nl> - if (! transforms [ transform_id ]. inv_trans ) { <nl> + if ( transform_id >= FF_ARRAY_ELEMS ( transforms ) || <nl> + ! transforms [ transform_id ]. inv_trans ) { <nl> av_log_ask_for_sample ( avctx , " Unimplemented transform : % d !\ n ", transform_id ); <nl> return AVERROR_PATCHWELCOME ; <nl> }
static int get_delayed_pic ( DiracContext * s , AVFrame * picture , int * got_frame ) <nl>  <nl> if ( out ) { <nl> out -> reference ^= DELAYED_PIC_REF ; <nl> - * got_frame = 1 ; <nl> if (( ret = av_frame_ref ( picture , out -> avframe )) < 0 ) <nl> return ret ; <nl> + * got_frame = 1 ; <nl> } <nl>  <nl> return 0 ;
static int recode_subtitle ( AVCodecContext * avctx , <nl> goto end ; <nl> } <nl> outpkt -> size -= outl ; <nl> - outpkt -> data [ outpkt -> size - 1 ] = '\ 0 '; <nl> + memset ( outpkt -> data + outpkt -> size , 0 , outl ); <nl>  <nl> end : <nl> if ( cd != ( iconv_t )- 1 )
static av_cold int encode_init ( AVCodecContext * avctx ) <nl> } <nl> } <nl> gob_count = strtol ( p , & next , 0 ); <nl> - if ( next == p || gob_count < 0 ){ <nl> + if ( next == p || gob_count <= 0 ){ <nl> av_log ( avctx , AV_LOG_ERROR , " 2Pass file invalid \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int pmp_header ( AVFormatContext * s ) <nl> uint32_t index_cnt ; <nl> int audio_codec_id = AV_CODEC_ID_NONE ; <nl> int srate , channels ; <nl> - int i ; <nl> + unsigned i ; <nl> uint64_t pos ; <nl> int64_t fsize = avio_size ( pb ); <nl> 
static void apply_tns ( float coef [ 1024 ], TemporalNoiseShaping * tns , <nl> int w , filt , m , i ; <nl> int bottom , top , order , start , end , size , inc ; <nl> float lpc [ TNS_MAX_ORDER ]; <nl> - float tmp [ TNS_MAX_ORDER ]; <nl> + float tmp [ TNS_MAX_ORDER + 1 ]; <nl>  <nl> for ( w = 0 ; w < ics -> num_windows ; w ++) { <nl> bottom = ics -> num_swb ;
ogm_dshow_header ( AVFormatContext * s , int idx ) <nl> if (* p != 1 ) <nl> return 1 ; <nl>  <nl> + if ( os -> psize < 100 ) <nl> + return AVERROR_INVALIDDATA ; <nl> t = AV_RL32 ( p + 96 ); <nl>  <nl> if ( t == 0x05589f80 ){
int64_t av_rescale_rnd ( int64_t a , int64_t b , int64_t c , enum AVRounding rnd ) <nl> else { <nl> int64_t ad = a / c ; <nl> int64_t a2 = ( a % c * b + r ) / c ; <nl> - if ( ad >= INT32_MAX && ad > ( INT64_MAX - a2 ) / b ) <nl> + if ( ad >= INT32_MAX && b && ad > ( INT64_MAX - a2 ) / b ) <nl> return INT64_MIN ; <nl> return ad * b + a2 ; <nl> }
static int read_seek ( AVFormatContext * s , int stream_index , <nl> next_node [ 1 ]-> pos , next_node [ 1 ]-> pos , <nl> next_node [ 0 ]-> ts , next_node [ 1 ]-> ts , <nl> AVSEEK_FLAG_BACKWARD , & ts , nut_read_timestamp ); <nl> + if ( pos < 0 ) <nl> + return pos ; <nl>  <nl> if (!( flags & AVSEEK_FLAG_BACKWARD )) { <nl> dummy . pos = pos + 16 ;
static int update_context_from_thread ( AVCodecContext * dst , AVCodecContext * src , <nl> { <nl> int err = 0 ; <nl>  <nl> - if ( dst != src ) { <nl> + if ( dst != src && ( for_user || !( av_codec_get_codec_descriptor ( src )-> props & AV_CODEC_PROP_INTRA_ONLY ))) { <nl> dst -> time_base = src -> time_base ; <nl> dst -> framerate = src -> framerate ; <nl> dst -> width = src -> width ;
static void ff_compute_band_indexes ( MPADecodeContext * s , GranuleDef * g ) <nl> else <nl> g -> long_end = 6 ; <nl>  <nl> - g -> short_start = 2 + ( s -> sample_rate_index != 8 ); <nl> + g -> short_start = 3 ; <nl> } else { <nl> g -> long_end = 0 ; <nl> g -> short_start = 0 ;
static av_always_inline int wp_exp2 ( int16_t val ) <nl> return neg ? - res : res ; <nl> } <nl>  <nl> - static av_always_inline int wp_log2 ( int32_t val ) <nl> + static av_always_inline int wp_log2 ( uint32_t val ) <nl> { <nl> int bits ; <nl> 
static int decode_frame ( AVCodecContext * avctx , <nl> return ret ; <nl> fail : <nl> av_dict_free (& metadata ); <nl> + ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> ret = AVERROR_INVALIDDATA ; <nl> - ff_thread_release_buffer ( avctx , & s -> picture ); <nl> goto the_end ; <nl> } <nl> 
av_cold static int auto_matrix ( SwrContext * s ) <nl> } else <nl> maxval = INT_MAX ; <nl>  <nl> - if ( maxcoef > maxval ){ <nl> + if ( maxcoef > maxval || s -> rematrix_volume < 0 ){ <nl> maxcoef /= maxval ; <nl> for ( i = 0 ; i < SWR_CH_MAX ; i ++) <nl> for ( j = 0 ; j < SWR_CH_MAX ; j ++){
int main ( int argc , char * argv []) <nl> } <nl> } <nl>  <nl> - max = ( 1 << ( 8 * len )) - 1 ; <nl> + max = ( 1LL << ( 8 * len )) - 1 ; <nl>  <nl> f [ 0 ] = fopen ( argv [ 1 ], " rb "); <nl> f [ 1 ] = fopen ( argv [ 2 ], " rb ");
static int Faac_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl>  <nl> /* add current frame to the queue */ <nl> if ( frame ) { <nl> - if (( ret = ff_af_queue_add (& s -> afq , frame ) < 0 )) <nl> + if (( ret = ff_af_queue_add (& s -> afq , frame )) < 0 ) <nl> return ret ; <nl> } <nl> 
av_cold int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , <nl> } <nl> } <nl>  <nl> -# define USE_MMAP ( HAVE_MMAP && HAVE_MPROTECT && defined MAP_ANONYMOUS ) <nl> +# if HAVE_MMAP && HAVE_MPROTECT && defined ( MAP_ANONYMOUS ) <nl> +# define USE_MMAP 1 <nl> +# else <nl> +# define USE_MMAP 0 <nl> +# endif <nl>  <nl> /* precalculate horizontal scaler filter coefficients */ <nl> {
int ff_wma_run_level_decode ( AVCodecContext * avctx , GetBitContext * gb , <nl> } <nl> /** NOTE : EOB can be omitted */ <nl> if ( offset > num_coefs ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " overflow in spectral RLE , ignoring \ n "); <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " overflow (% d > % d ) in spectral RLE , ignoring \ n ", <nl> + offset , <nl> + num_coefs <nl> + ); <nl> return - 1 ; <nl> } <nl> 
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
static void read_ttag ( AVFormatContext * s , AVIOContext * pb , int taglen , const cha <nl> } <nl> else if (* dst ) <nl> dict_flags |= AV_DICT_DONT_STRDUP_VAL ; <nl> + else <nl> + av_freep (& dst ); <nl>  <nl> if ( dst ) <nl> av_dict_set (& s -> metadata , key , dst , dict_flags );
leave : <nl> av_log ( s , AV_LOG_ERROR , " Packet mismatch % d % d \ n ", last , orig_size + 11 ); <nl> avio_seek ( s -> pb , pos + 1 , SEEK_SET ); <nl> ret = resync ( s ); <nl> + av_free_packet ( pkt ); <nl> if ( ret >= 0 ) { <nl> - av_free_packet ( pkt ); <nl> goto retry ; <nl> } <nl> }
static void dvbsub_parse_page_segment ( AVCodecContext * avctx , <nl>  <nl> av_dlog ( avctx , " Page time out % ds , state % d \ n ", ctx -> time_out , page_state ); <nl>  <nl> - if ( page_state == 2 ) { <nl> + if ( page_state == 1 || page_state == 2 ) { <nl> delete_regions ( ctx ); <nl> delete_objects ( ctx ); <nl> delete_cluts ( ctx );
# define NDEBUG <nl> # endif <nl>  <nl> -# if defined ( DEBUG ) && ! defined ( CHECKED ) <nl> -# define CHECKED <nl> -# endif <nl> +// This can be enabled to allow detection of additional integer overflows with ubsan <nl> +//# define CHECKED <nl>  <nl> # include < limits . h > <nl> # include < stdint . h >
for examples see get_bits , show_bits , skip_bits , get_vlc <nl>  <nl> # define OPEN_READER ( name , gb ) \ <nl> unsigned int name ## _index = ( gb )-> index ; \ <nl> - int name ## _cache = 0 <nl> + unsigned int name ## _cache = 0 <nl>  <nl> # define CLOSE_READER ( name , gb ) ( gb )-> index = name ## _index <nl> 
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> s -> current_picture -> linesize [ 2 ], w >> s -> chroma_h_shift , h >> s -> chroma_v_shift , <nl> EDGE_WIDTH >> s -> chroma_h_shift , EDGE_WIDTH >> s -> chroma_v_shift , EDGE_TOP | EDGE_BOTTOM ); <nl> } <nl> + emms_c (); <nl> } <nl>  <nl> ff_snow_frame_start ( s ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> } <nl> + emms_c (); <nl>  <nl> update_last_header_values ( s ); <nl> 
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int ret , i , type , size , flags , is_audio ; <nl> int64_t next , pos ; <nl> int64_t dts , pts = AV_NOPTS_VALUE ; <nl> - int sample_rate , channels ; <nl> + int sample_rate = 0 , channels = 0 ; <nl> AVStream * st = NULL ; <nl>  <nl> for (;; avio_skip ( s -> pb , 4 )){ /* pkt size is repeated at end . skip it */
int ff_ass_add_rect ( AVSubtitle * sub , const char * dialog , <nl> sub -> rects = rects ; <nl> sub -> end_display_time = FFMAX ( sub -> end_display_time , 10 * duration ); <nl> rects [ sub -> num_rects ] = av_mallocz ( sizeof (* rects [ 0 ])); <nl> + if (! rects [ sub -> num_rects ]) <nl> + goto errnomem ; <nl> rects [ sub -> num_rects ]-> type = SUBTITLE_ASS ; <nl> ret = av_bprint_finalize (& buf , & rects [ sub -> num_rects ]-> ass ); <nl> if ( ret < 0 )
static int transcode ( OutputFile * output_files , <nl> int64_t ipts_min ; <nl> double opts_min ; <nl>  <nl> - redo : <nl> ipts_min = INT64_MAX ; <nl> opts_min = 1e100 ; <nl> /* if ' q ' pressed , exits */ <nl> static int transcode ( OutputFile * output_files , <nl> if ( exit_on_error ) <nl> exit_program ( 1 ); <nl> av_free_packet (& pkt ); <nl> - goto redo ; <nl> + continue ; <nl> } <nl>  <nl> discard_packet :
static int select_reference_stream ( AVFormatContext * s ) <nl> ret = avformat_match_stream_specifier ( s , s -> streams [ i ], <nl> seg -> reference_stream_specifier ); <nl> if ( ret < 0 ) <nl> - break ; <nl> + return ret ; <nl> if ( ret > 0 ) { <nl> seg -> reference_stream_index = i ; <nl> break ;
static int decode_end ( AVCodecContext * avctx ) <nl> common_end ( s ); <nl> av_freep (& s -> bitstream_buffer ); <nl>  <nl> - for ( i = 0 ; i < 3 ; i ++){ <nl> + for ( i = 0 ; i < 6 ; i ++){ <nl> free_vlc (& s -> vlc [ i ]); <nl> } <nl> 
static int rtmp_packet_read_one_chunk ( URLContext * h , RTMPPacket * p , <nl> prev -> data = p -> data ; <nl> prev -> read = p -> read ; <nl> prev -> offset = p -> offset ; <nl> + p -> data = NULL ; <nl> return AVERROR ( EAGAIN ); <nl> } <nl> 
int img_convert ( AVPicture * dst , int dst_pix_fmt , <nl> else <nl> int_pix_fmt = PIX_FMT_RGB24 ; <nl> } <nl> + if ( src_pix_fmt == int_pix_fmt ) <nl> + return - 1 ; <nl> if ( avpicture_alloc ( tmp , int_pix_fmt , dst_width , dst_height ) < 0 ) <nl> return - 1 ; <nl> ret = - 1 ;
static void compute_stereo ( MPADecodeContext * s , GranuleDef * g0 , GranuleDef * g1 ) <nl> { <nl> int i , j , k , l ; <nl> int sf_max , sf , len , non_zero_found ; <nl> - INTFLOAT (* is_tab )[ 16 ], * tab0 , * tab1 , tmp0 , tmp1 , v1 , v2 ; <nl> + INTFLOAT (* is_tab )[ 16 ], * tab0 , * tab1 , v1 , v2 ; <nl> + SUINTFLOAT tmp0 , tmp1 ; <nl> int non_zero_found_short [ 3 ]; <nl>  <nl> /* intensity stereo */
static int configure_output_video_filter ( FilterGraph * fg , OutputFilter * ofilter , <nl> snprintf ( name , sizeof ( name ), " output stream % d :% d ", ost -> file_index , ost -> index ); <nl> ret = avfilter_graph_create_filter (& ofilter -> filter , <nl> avfilter_get_by_name (" buffersink "), <nl> - name , NULL , pix_fmts , fg -> graph ); <nl> + name , NULL , NULL , fg -> graph ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
static FilterGraph * init_simple_filtergraph ( InputStream * ist , OutputStream * ost ) <nl>  <nl> static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> { <nl> - InputStream * ist ; <nl> + InputStream * ist = NULL ; <nl> enum AVMediaType type = in -> filter_ctx -> input_pads [ in -> pad_idx ]. type ; <nl> int i ; <nl> 
static int decode_frame_headers ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl>  <nl> if ( ctx -> data_size == 16 ) <nl> return 4 ; <nl> - if ( ctx -> data_size > buf_size ) <nl> - ctx -> data_size = buf_size ; <nl> + ctx -> data_size = FFMIN ( ctx -> data_size , buf_size - 16 ); <nl>  <nl> bytestream2_skip (& gb , 3 ); // skip reserved byte and checksum <nl> 
static int tta_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> return - 1 ; <nl> } <nl> st -> codec -> extradata = av_mallocz ( st -> codec -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! st -> codec -> extradata ) { <nl> + st -> codec -> extradata_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> avio_seek ( s -> pb , start_offset , SEEK_SET ); <nl> avio_read ( s -> pb , st -> codec -> extradata , st -> codec -> extradata_size ); <nl> 
int av_grow_packet ( AVPacket * pkt , int grow_by ) <nl> pkt -> buf = av_buffer_alloc ( new_size ); <nl> if (! pkt -> buf ) <nl> return AVERROR ( ENOMEM ); <nl> - memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> + if ( pkt -> size > 0 ) <nl> + memcpy ( pkt -> buf -> data , pkt -> data , pkt -> size ); <nl> pkt -> data = pkt -> buf -> data ; <nl> } <nl> pkt -> size += grow_by ;
static int cbs_jpeg_split_fragment ( CodedBitstreamContext * ctx , <nl> if ( marker == JPEG_MARKER_SOS ) { <nl> length = AV_RB16 ( frag -> data + start ); <nl>  <nl> + if ( length > end - start ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> data_ref = NULL ; <nl> data = av_malloc ( end - start + <nl> AV_INPUT_BUFFER_PADDING_SIZE );
static int remove_decoded_packets ( AVFormatContext * ctx , int64_t scr ){ <nl> if ( stream -> buffer_index < pkt_desc -> size || <nl> stream -> predecode_packet == stream -> premux_packet ){ <nl> av_log ( ctx , AV_LOG_ERROR , <nl> - " buffer underflow i =% d bufi =% d size =% d \ n ", <nl> + " buffer underflow st =% d bufi =% d size =% d \ n ", <nl> i , stream -> buffer_index , pkt_desc -> size ); <nl> break ; <nl> }
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> AVFilterBufferRef * out ; <nl> int hsub0 = desc -> log2_chroma_w ; <nl> int vsub0 = desc -> log2_chroma_h ; <nl> - int direct ; <nl> + int direct = 0 ; <nl> int plane ; <nl>  <nl> if (( in -> perms & AV_PERM_WRITE ) && !( in -> perms & AV_PERM_PRESERVE )) {
static inline void ls_decode_line ( JLSState * state , MJpegDecodeContext * s , <nl> while ( x < w ) { <nl> int err , pred ; <nl>  <nl> + if ( get_bits_left (& s -> gb ) <= 0 ) <nl> + return ; <nl> + <nl> /* compute gradients */ <nl> Ra = x ? R ( dst , x - stride ) : R ( last , x ); <nl> Rb = R ( last , x );
static void amr_decode_fix_avctx ( AVCodecContext * avctx ) <nl> { <nl> const int is_amr_wb = 1 + ( avctx -> codec_id == AV_CODEC_ID_AMR_WB ); <nl>  <nl> - avctx -> sample_rate = 8000 * is_amr_wb ; <nl> + if (! avctx -> sample_rate ) <nl> + avctx -> sample_rate = 8000 * is_amr_wb ; <nl>  <nl> if ( avctx -> channels > 1 ) { <nl> av_log_missing_feature ( avctx , " multi - channel AMR ", 0 );
static int decode_frame ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " The answer to life , universe and everything is not correct !\ n "); <nl> return - 1 ; <nl> } <nl> + // Reset these pointers so we can tell if they were set this frame <nl> + s -> stripsizes = s -> stripdata = NULL ; <nl> /* parse image file directory */ <nl> off = tget_long (& buf , le ); <nl> if ( off >= UINT_MAX - 14 || end_buf - orig_buf < off + 14 ) {
static int mpc8_read_header ( AVFormatContext * s ) <nl> while (! avio_feof ( pb )){ <nl> pos = avio_tell ( pb ); <nl> mpc8_get_chunk_header ( pb , & tag , & size ); <nl> + if ( size < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid chunk length \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( tag == TAG_STREAMHDR ) <nl> break ; <nl> mpc8_handle_chunk ( s , tag , pos , size );
static int svq3_decode_frame ( AVCodecContext * avctx , void * data , <nl> h -> mb_x = h -> mb_y = h -> mb_xy = 0 ; <nl>  <nl> if ( s -> watermark_key ) { <nl> - av_fast_malloc (& s -> buf , & s -> buf_size , <nl> - buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + av_fast_padded_malloc (& s -> buf , & s -> buf_size , buf_size ); <nl> if (! s -> buf ) <nl> return AVERROR ( ENOMEM ); <nl> memcpy ( s -> buf , avpkt -> data , buf_size );
int avresample_convert ( AVAudioResampleContext * avr , void ** output , <nl> } <nl> } <nl>  <nl> - return handle_buffered_output ( avr , & output_buffer , current_buffer ); <nl> + return handle_buffered_output ( avr , output ? & output_buffer : NULL , <nl> + current_buffer ); <nl> } <nl>  <nl> int avresample_available ( AVAudioResampleContext * avr )
static void mov_metadata_creation_time ( AVDictionary ** metadata , int64_t time ) <nl> if ( time ) { <nl> if ( time >= 2082844800 ) <nl> time -= 2082844800 ; /* seconds between 1904 - 01 - 01 and Epoch */ <nl> + <nl> + if (( int64_t )( time * 1000000ULL ) / 1000000 != time ) { <nl> + av_log ( NULL , AV_LOG_DEBUG , " creation_time is not representable \ n "); <nl> + return ; <nl> + } <nl> + <nl> avpriv_dict_set_timestamp ( metadata , " creation_time ", time * 1000000 ); <nl> } <nl> }
static void update_stream_timings ( AVFormatContext * ic ) <nl> if ( end_time != INT64_MIN ) <nl> duration = FFMAX ( duration , end_time - start_time ); <nl> } <nl> - if ( duration != INT64_MIN && ic -> duration == AV_NOPTS_VALUE ) { <nl> + if ( duration != INT64_MIN && duration > 0 && ic -> duration == AV_NOPTS_VALUE ) { <nl> ic -> duration = duration ; <nl> } <nl> if ( ic -> pb && ( filesize = avio_size ( ic -> pb )) > 0 && ic -> duration != AV_NOPTS_VALUE ) {
static void stream_pause ( VideoState * is ) <nl>  <nl> static double compute_target_time ( double frame_current_pts , VideoState * is ) <nl> { <nl> - double delay , sync_threshold , diff ; <nl> + double delay , sync_threshold , diff = 0 ; <nl>  <nl> /* compute nominal delay */ <nl> delay = frame_current_pts - is -> frame_last_pts ;
static av_cold int adpcm_decode_init ( AVCodecContext * avctx ) <nl> max_channels = 6 ; <nl> break ; <nl> } <nl> - if ( avctx -> channels > max_channels ){ <nl> - return - 1 ; <nl> + if ( avctx -> channels <= 0 || avctx -> channels > max_channels ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR ( EINVAL ); <nl> } <nl>  <nl> switch ( avctx -> codec -> id ) {
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> entries = avio_rb32 ( pb ); <nl> if ( entries >= UINT_MAX / sizeof (* sc -> drefs )) <nl> return AVERROR_INVALIDDATA ; <nl> + av_free ( sc -> drefs ); <nl> sc -> drefs = av_mallocz ( entries * sizeof (* sc -> drefs )); <nl> if (! sc -> drefs ) <nl> return AVERROR ( ENOMEM );
int ff_img_read_header ( AVFormatContext * s1 ) <nl> break ; <nl> } <nl> } <nl> - ffio_rewind_with_probe_data ( s1 -> pb , & probe_buffer , probe_buffer_size ); <nl> + if ( s1 -> flags & AVFMT_FLAG_CUSTOM_IO ) { <nl> + avio_seek ( s1 -> pb , 0 , SEEK_SET ); <nl> + } else <nl> + ffio_rewind_with_probe_data ( s1 -> pb , & probe_buffer , probe_buffer_size ); <nl> } <nl> if ( st -> codec -> codec_id == AV_CODEC_ID_NONE ) <nl> st -> codec -> codec_id = ff_guess_image2_codec ( s -> path );
static int get_packet ( URLContext * s , int for_header ) <nl> } <nl> } <nl> rt -> bytes_read += ret ; <nl> - if ( rt -> bytes_read > rt -> last_bytes_read + rt -> client_report_size ) { <nl> + if ( rt -> bytes_read - rt -> last_bytes_read > rt -> client_report_size ) { <nl> av_log ( s , AV_LOG_DEBUG , " Sending bytes read report \ n "); <nl> gen_bytes_read ( s , rt , rpkt . timestamp + 1 ); <nl> rt -> last_bytes_read = rt -> bytes_read ;
static int decode_blocks ( SnowContext * s ){ <nl>  <nl> for ( y = 0 ; y < h ; y ++){ <nl> for ( x = 0 ; x < w ; x ++){ <nl> + if ( s -> c . bytestream >= s -> c . bytestream_end ) <nl> + return AVERROR_INVALIDDATA ; <nl> if (( res = decode_q_branch ( s , 0 , x , y )) < 0 ) <nl> return res ; <nl> }
int64_t ff_ape_parse_tag ( AVFormatContext * s ) <nl> return 0 ; <nl> } <nl>  <nl> - tag_start = file_size - tag_bytes - APE_TAG_FOOTER_BYTES ; <nl> - if ( tag_start < 0 ) { <nl> + if ( tag_bytes > file_size - APE_TAG_FOOTER_BYTES ) { <nl> av_log ( s , AV_LOG_ERROR , " Invalid tag size % u .\ n ", tag_bytes ); <nl> return 0 ; <nl> } <nl> + tag_start = file_size - tag_bytes - APE_TAG_FOOTER_BYTES ; <nl>  <nl> fields = avio_rl32 ( pb ); /* number of fields */ <nl> if ( fields > 65536 ) {
static int theora_header ( AVFormatContext * s , int idx ) <nl> st -> codec -> extradata_size = 0 ; <nl> return err ; <nl> } <nl> + memset ( st -> codec -> extradata + cds , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + <nl> cdp = st -> codec -> extradata + st -> codec -> extradata_size ; <nl> * cdp ++ = os -> psize >> 8 ; <nl> * cdp ++ = os -> psize & 0xff ;
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> uint8_t * end = dst + ( width - 2 ); <nl>  <nl> avpriv_request_sample ( l -> avctx , " zero_run_line "); <nl> - return AVERROR_PATCHWELCOME ; <nl> + <nl> + memset ( dst , 0 , width ); <nl>  <nl> output_zeros : <nl> if ( l -> zeros_rem ) {
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> * of get_sr_golomb_shorten (). */ <nl> if ( s -> version == 0 ) <nl> residual_size --; <nl> + if ( residual_size > 30U ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " residual size unsupportd : % d \ n ", residual_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> /* calculate sample offset using means from previous blocks */
static int mpeg_mux_init ( AVFormatContext * ctx ) <nl> if (! s -> mux_rate ) { <nl> /* we increase slightly the bitrate to take into account the <nl> headers . XXX : compute it exactly */ <nl> - bitrate += bitrate * 5 / 100 ; <nl> + bitrate += bitrate * 5LL / 100 ; <nl> bitrate += 10000 ; <nl> s -> mux_rate = ( bitrate + ( 8 * 50 ) - 1 ) / ( 8 * 50 ); <nl> }
int main ( void ){ <nl> put_cabac (& c , state , r [ i ]& 1 ); <nl> } <nl>  <nl> - put_cabac_terminate (& c , 1 ); <nl> + i = put_cabac_terminate (& c , 1 ); <nl> + b [ i ++] = av_lfg_get (& prng ); <nl> + b [ i ] = av_lfg_get (& prng ); <nl>  <nl> ff_init_cabac_decoder (& c , b , SIZE ); <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame_ptr , <nl> uint32_t header ; <nl> int out_size ; <nl>  <nl> - while ( buf_size && !* buf ) <nl> + while ( buf_size && !* buf ){ <nl> buf ++; <nl> + buf_size --; <nl> + } <nl>  <nl> if ( buf_size < HEADER_SIZE ) <nl> return AVERROR_INVALIDDATA ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> cases . */ <nl> static void imdct12 ( INTFLOAT * out , INTFLOAT * in ) <nl> { <nl> - INTFLOAT in0 , in1 , in2 , in3 , in4 , in5 , t1 , t2 ; <nl> + SUINTFLOAT in0 , in1 , in2 , in3 , in4 , in5 , t1 , t2 ; <nl>  <nl> in0 = in [ 0 * 3 ]; <nl> in1 = in [ 1 * 3 ] + in [ 0 * 3 ];
static int get_stats ( AVCodecContext * avctx , int eos ) <nl> // libtheora generates a summary header at the end <nl> memcpy ( h -> stats , buf , bytes ); <nl> avctx -> stats_out = av_malloc ( b64_size ); <nl> + if (! avctx -> stats_out ) <nl> + return AVERROR ( ENOMEM ); <nl> av_base64_encode ( avctx -> stats_out , b64_size , h -> stats , h -> stats_offset ); <nl> } <nl> return 0 ;
retry : <nl>  <nl> if ( next_pkt && next_pkt -> dts - scr > max_delay ) <nl> continue ; <nl> - <nl> + if ( stream -> predecode_packet <nl> + && stream -> predecode_packet -> size > stream -> buffer_index ) <nl> + rel_space += 1 << 28 ; <nl> if ( rel_space > best_score ){ <nl> best_score = rel_space ; <nl> best_i = i ;
static int decode_block ( MJpegDecodeContext * s , int16_t * block , int component , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> val = val * quant_matrix [ 0 ] + s -> last_dc [ component ]; <nl> - val = FFMIN ( val , 32767 ); <nl> + val = av_clip_int16 ( val ); <nl> s -> last_dc [ component ] = val ; <nl> block [ 0 ] = val ; <nl> /* AC coefs */
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> if ( atom . size < 0 ) <nl> atom . size = INT64_MAX ; <nl> - while ( total_size + 8 <= atom . size && ! avio_feof ( pb )) { <nl> + while ( total_size <= atom . size - 8 && ! avio_feof ( pb )) { <nl> int (* parse )( MOVContext *, AVIOContext *, MOVAtom ) = NULL ; <nl> a . size = atom . size ; <nl> a . type = 0 ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> void * tmp_ptr ; <nl> s -> max_framesize = 1024 ; // should hopefully be enough for the first header <nl> tmp_ptr = av_fast_realloc ( s -> bitstream , & s -> allocated_bitstream_size , <nl> - s -> max_framesize ); <nl> + s -> max_framesize + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! tmp_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM );
static int swScale ( SwsContext * c , const uint8_t * src [], <nl> || yuv2planeX == yuv2planeX_9LE_c <nl> || yuv2planeX == yuv2planeX_16BE_c <nl> || yuv2planeX == yuv2planeX_16LE_c <nl> - || yuv2planeX == yuv2planeX_8_c )); <nl> + || yuv2planeX == yuv2planeX_8_c ) || ! ARCH_X86 ); <nl> + <nl> if ( use_mmx_vfilter ){ <nl> vLumFilter = c -> lumMmxFilter ; <nl> vChrFilter = c -> chrMmxFilter ;
static av_cold void init_cplscales_table ( COOKContext * q ) <nl> static inline int decode_bytes ( const uint8_t * inbuffer , uint8_t * out , int bytes ) <nl> { <nl> static const uint32_t tab [ 4 ] = { <nl> - AV_BE2NE32C ( 0x37c511f2 ), AV_BE2NE32C ( 0xf237c511 ), <nl> - AV_BE2NE32C ( 0x11f237c5 ), AV_BE2NE32C ( 0xc511f237 ), <nl> + AV_BE2NE32C ( 0x37c511f2u ), AV_BE2NE32C ( 0xf237c511u ), <nl> + AV_BE2NE32C ( 0x11f237c5u ), AV_BE2NE32C ( 0xc511f237u ), <nl> }; <nl> int i , off ; <nl> uint32_t c ;
static void show_packets ( WriterContext * w , AVFormatContext * fmt_ctx ) <nl> if ( do_show_frames && <nl> get_video_frame ( fmt_ctx , & frame , & pkt )) { <nl> show_frame ( w , & frame , fmt_ctx -> streams [ pkt . stream_index ]); <nl> - av_free_packet (& pkt ); <nl> } <nl> + av_free_packet (& pkt ); <nl> } <nl> av_init_packet (& pkt ); <nl> pkt . data = NULL ;
static int flv_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> unsigned ts ; <nl> int size = pkt -> size ; <nl> uint8_t * data = NULL ; <nl> - int flags , flags_size ; <nl> + int flags = 0 , flags_size ; <nl>  <nl> // av_log ( s , AV_LOG_DEBUG , " type :% d pts : %" PRId64 " size :% d \ n ", <nl> // enc -> codec_type , timestamp , size );
static int writer_open ( WriterContext ** wctx , const Writer * writer , const char * a <nl> { <nl> int i , ret = 0 ; <nl>  <nl> - if (!(* wctx = av_malloc ( sizeof ( WriterContext )))) { <nl> + if (!(* wctx = av_mallocz ( sizeof ( WriterContext )))) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ; <nl> }
static int execute_decode_slices ( H264Context * h , int context_count ) <nl> H264Context * hx ; <nl> int i ; <nl>  <nl> + if ( h -> mb_y >= h -> mb_height ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , <nl> + " Input contains more MB rows than the frame height .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( h -> avctx -> hwaccel ) <nl> return 0 ; <nl> if ( context_count == 1 ) {
static int sonic_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( buf_size == 0 ) return 0 ; <nl>  <nl> - s -> frame . nb_samples = s -> frame_size ; <nl> + s -> frame . nb_samples = s -> frame_size / avctx -> channels ; <nl> if (( ret = ff_get_buffer ( avctx , & s -> frame , 0 )) < 0 ) <nl> return ret ; <nl> samples = ( int16_t *) s -> frame . data [ 0 ];
static int adx_decode_frame ( AVCodecContext * avctx , void * data , <nl> buf += header_size ; <nl> buf_size -= header_size ; <nl> } <nl> + if ( c -> channels <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> /* calculate number of blocks in the packet */ <nl> num_blocks = buf_size / ( BLOCK_SIZE * c -> channels );
static inline int asym_quant ( int c , int e , int qbits ) <nl> { <nl> int m ; <nl>  <nl> - c = ((( c << e ) >> ( 24 - qbits )) + 1 ) >> 1 ; <nl> + c = ((( c * ( 1 << e )) >> ( 24 - qbits )) + 1 ) >> 1 ; <nl> m = ( 1 << ( qbits - 1 )); <nl> if ( c >= m ) <nl> c = m - 1 ;
av_cold int ff_snow_common_init ( AVCodecContext * avctx ){ <nl> ff_scale_mv_ref [ i ][ j ] = 256 *( i + 1 )/( j + 1 ); <nl>  <nl> s -> avctx -> get_buffer ( s -> avctx , & s -> mconly_picture ); <nl> - s -> scratchbuf = av_malloc ( s -> mconly_picture . linesize [ 0 ]* 7 * MB_SIZE ); <nl> + s -> scratchbuf = av_mallocz ( s -> mconly_picture . linesize [ 0 ]* 7 * MB_SIZE ); <nl>  <nl> return 0 ; <nl> }
static void show_packets ( AVFormatContext * fmt_ctx ) <nl>  <nl> av_init_packet (& pkt ); <nl> probe_array_header (" packets ", 0 ); <nl> - while (! av_read_frame ( fmt_ctx , & pkt )) <nl> + while (! av_read_frame ( fmt_ctx , & pkt )) { <nl> show_packet ( fmt_ctx , & pkt ); <nl> + av_packet_unref (& pkt ); <nl> + } <nl> probe_array_footer (" packets ", 0 ); <nl> } <nl> 
static enum CodecID vfw_codecid ( DWORD biCompression ) <nl> switch ( biCompression ) { <nl> case MKTAG (' d ', ' v ', ' s ', ' d '): <nl> return CODEC_ID_DVVIDEO ; <nl> + case MKTAG (' M ', ' J ', ' P ', ' G '): <nl> + case MKTAG (' m ', ' j ', ' p ', ' g '): <nl> + return CODEC_ID_MJPEG ; <nl> } <nl> return CODEC_ID_NONE ; <nl> }
static av_cold int tiff_end ( AVCodecContext * avctx ) <nl>  <nl> ff_lzw_decode_close (& s -> lzw ); <nl> av_freep (& s -> deinvert_buf ); <nl> + s -> deinvert_buf_size = 0 ; <nl> av_freep (& s -> fax_buffer ); <nl> s -> fax_buffer_size = 0 ; <nl> return 0 ;
again : <nl> " SPS decoding failure , trying again with the complete NAL \ n "); <nl> if ( h -> is_avc ) <nl> av_assert0 ( next_avc - buf_index + consumed == nalsize ); <nl> + if (( next_avc - buf_index + consumed - 1 ) >= INT_MAX / 8 ) <nl> + break ; <nl> init_get_bits (& s -> gb , & buf [ buf_index + 1 - consumed ], <nl> 8 *( next_avc - buf_index + consumed - 1 )); <nl> ff_h264_decode_seq_parameter_set ( h );
int main ( int argc , char ** argv ) <nl> k = av_tree_find ( root , ( void *)( j + 1 ), cmp , NULL ); <nl> if ( k ) <nl> av_log ( NULL , AV_LOG_ERROR , " removal failure % d \ n ", i ); <nl> + av_free ( node2 ); <nl> } <nl> } <nl> + av_free ( node ); <nl>  <nl> av_tree_destroy ( root ); <nl> 
typedef struct ListEntry { <nl>  <nl> typedef struct HLSContext { <nl> const AVClass * class ; // Class for private options . <nl> - int number ; <nl> + unsigned number ; <nl> int64_t sequence ; <nl> AVOutputFormat * oformat ; <nl> AVFormatContext * avf ;
int ff_hevc_parse_sps ( HEVCSPS * sps , GetBitContext * gb , unsigned int * sps_id , <nl> return 0 ; <nl>  <nl> err : <nl> - return ret ; <nl> + return ret < 0 ? ret : AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> int ff_hevc_decode_nal_sps ( GetBitContext * gb , AVCodecContext * avctx ,
static int read_len_table ( uint8_t * dst , GetBitContext * gb ){ <nl> if ( repeat == 0 ) <nl> repeat = get_bits ( gb , 8 ); <nl> // printf ("% d % d \ n ", val , repeat ); <nl> - if ( i + repeat > 256 ) { <nl> + if ( i + repeat > 256 || get_bits_left ( gb ) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error reading huffman table \ n "); <nl> return - 1 ; <nl> }
typedef struct MotionEstContext { <nl> int stride ; <nl> int uvstride ; <nl> /* temp variables for picture complexity calculation */ <nl> - int mc_mb_var_sum_temp ; <nl> - int mb_var_sum_temp ; <nl> + int64_t mc_mb_var_sum_temp ; <nl> + int64_t mb_var_sum_temp ; <nl> int scene_change_score ; <nl> /* cmp , chroma_cmp ;*/ <nl> op_pixels_func (* hpel_put )[ 4 ];
static int dss_read_close ( AVFormatContext * s ) <nl> { <nl> DSSDemuxContext * ctx = s -> priv_data ; <nl>  <nl> - av_free ( ctx -> dss_sp_buf ); <nl> + av_freep (& ctx -> dss_sp_buf ); <nl>  <nl> return 0 ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> buf += 5 ; <nl>  <nl> if ( video_size ) { <nl> + if ( video_size < 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " video size % d invalid \ n ", video_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( avctx -> reget_buffer ( avctx , & s -> frame ) < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> return - 1 ;
static int prompeg_write ( URLContext * h , const uint8_t * buf , int size ) { <nl>  <nl> // FEC ( column ) send block - aligned <nl> if (! s -> first && s -> packet_idx % s -> d == 0 ) { <nl> - col_out_idx = s -> packet_idx / s -> l ; <nl> + col_out_idx = s -> packet_idx / s -> d ; <nl> if (( ret = prompeg_write_fec ( h , s -> fec_col [ col_out_idx ], PROMPEG_FEC_COL )) < 0 ) <nl> goto end ; <nl> written += ret ;
static void swap_guid ( ff_asf_guid guid ) <nl>  <nl> static void align_position ( AVIOContext * pb , int64_t offset , uint64_t size ) <nl> { <nl> - if ( avio_tell ( pb ) != offset + size ) <nl> + if ( size < INT64_MAX - offset && avio_tell ( pb ) != offset + size ) <nl> avio_seek ( pb , offset + size , SEEK_SET ); <nl> } <nl> 
static int prepare_packet ( AVPacket * pkt , const FailingMuxerPacketData * pkt_data , <nl> { <nl> int ret ; <nl> FailingMuxerPacketData * data = av_malloc ( sizeof (* data )); <nl> + if (! data ) { <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> memcpy ( data , pkt_data , sizeof ( FailingMuxerPacketData )); <nl> ret = av_packet_from_data ( pkt , ( uint8_t *) data , sizeof (* data )); <nl> 
static int sami_paragraph_to_ass ( AVCodecContext * avctx , const char * src ) <nl> AVBPrint * dst_content = & sami -> encoded_content ; <nl> AVBPrint * dst_source = & sami -> encoded_source ; <nl>  <nl> + if (! dupsrc ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> av_bprint_clear (& sami -> encoded_content ); <nl> av_bprint_clear (& sami -> content ); <nl> av_bprint_clear (& sami -> encoded_source );
static int dnxhd_probe ( AVProbeData * p ) <nl> if (! w || ! h ) <nl> return 0 ; <nl> compression_id = AV_RB32 ( p -> buf + 0x28 ); <nl> - if ( compression_id < 1235 || compression_id > 1258 ) <nl> + if ( compression_id < 1235 || compression_id > 1260 ) <nl> return 0 ; <nl> return AVPROBE_SCORE_MAX ; <nl> }
static void sub_hfyu_median_prediction_int16_c ( uint16_t * dst , const uint16_t * sr <nl> * left_top = lt ; <nl> } <nl>  <nl> - static int add_hfyu_left_prediction_int16_c ( uint16_t * dst , const uint16_t * src , unsigned mask , int w , int acc ){ <nl> + static int add_hfyu_left_prediction_int16_c ( uint16_t * dst , const uint16_t * src , unsigned mask , int w , unsigned acc ){ <nl> int i ; <nl>  <nl> for ( i = 0 ; i < w - 1 ; i ++){
static int read_gab2_sub ( AVFormatContext * s , AVStream * st , AVPacket * pkt ) <nl> goto error ; <nl>  <nl> if (! avformat_open_input (& ast -> sub_ctx , "", sub_demuxer , NULL )) { <nl> + if ( ast -> sub_ctx -> nb_streams != 1 ) <nl> + goto error ; <nl> ff_read_packet ( ast -> sub_ctx , & ast -> sub_pkt ); <nl> avcodec_parameters_copy ( st -> codecpar , ast -> sub_ctx -> streams [ 0 ]-> codecpar ); <nl> time_base = ast -> sub_ctx -> streams [ 0 ]-> time_base ;
static int dsf_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> st -> codecpar -> block_align *= st -> codecpar -> channels ; <nl> + st -> codecpar -> bit_rate = st -> codecpar -> channels * st -> codecpar -> sample_rate * 8LL ; <nl> avio_skip ( pb , 4 ); <nl>  <nl> /* data chunk */
static int read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 4 ] = jvf -> video_type ; <nl> if (( size = avio_read ( pb , pkt -> data + JV_PREAMBLE_SIZE , size )) < 0 ) <nl> return AVERROR ( EIO ); <nl> + memset ( pkt -> data + JV_PREAMBLE_SIZE + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> pkt -> size = size + JV_PREAMBLE_SIZE ; <nl> pkt -> stream_index = 1 ;
static av_cold int tdsc_init ( AVCodecContext * avctx ) <nl> ctx -> jpeg_avctx -> flags2 = avctx -> flags2 ; <nl> ctx -> jpeg_avctx -> dct_algo = avctx -> dct_algo ; <nl> ctx -> jpeg_avctx -> idct_algo = avctx -> idct_algo ;; <nl> - ret = avcodec_open2 ( ctx -> jpeg_avctx , codec , NULL ); <nl> + ret = ff_codec_open2_recursive ( ctx -> jpeg_avctx , codec , NULL ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> 
static int decode_slice ( AVCodecContext * avctx , void * tdata ) <nl>  <nl> /* if V or alpha component size is negative that means that previous <nl> component sizes are too large */ <nl> - if ( v_data_size < 0 || a_data_size < 0 || hdr_size < 6 ) { <nl> + if ( v_data_size < 0 || a_data_size < 0 || hdr_size < 6 || coff [ 3 ] > slice_data_size ) { <nl> av_log ( avctx , AV_LOG_ERROR , " invalid data size \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame_header ( AVCodecContext * ctx , <nl> s -> lf_delta . ref [ 3 ] = - 1 ; <nl> s -> lf_delta . mode [ 0 ] = 0 ; <nl> s -> lf_delta . mode [ 1 ] = 0 ; <nl> + memset ( s -> segmentation . feat , 0 , sizeof ( s -> segmentation . feat )); <nl> } <nl> s -> filter . level = get_bits (& s -> gb , 6 ); <nl> sharp = get_bits (& s -> gb , 3 );
static void mxf_read_pixel_layout ( AVIOContext * pb , MXFDescriptor * descriptor ) <nl> if ( ofs <= 14 ) { <nl> layout [ ofs ++] = code ; <nl> layout [ ofs ++] = value ; <nl> - } <nl> + } else <nl> + break ; /* don ' t read byte by byte on sneaky files filled with lots of non - zeroes */ <nl> } while ( code != 0 ); /* SMPTE 377M E . 2 . 46 */ <nl>  <nl> ff_mxf_decode_pixel_layout ( layout , & descriptor -> pix_fmt );
static av_always_inline int vorbis_residue_decode_internal ( vorbis_context * vc , <nl> } <nl>  <nl> } else if ( vr_type == 2 ) { <nl> - unsigned voffs_div = FASTDIV ( voffset , ch ); <nl> + unsigned voffs_div = ch == 1 ? voffset : FASTDIV ( voffset , ch ); <nl> unsigned voffs_mod = voffset - voffs_div * ch ; <nl>  <nl> for ( k = 0 ; k < step ; ++ k ) {
static int on2avc_decode_band_scales ( On2AVCContext * c , GetBitContext * gb ) <nl> } else { <nl> scale += get_vlc2 ( gb , c -> scale_diff . table , 9 , 3 ) - 60 ; <nl> } <nl> - if ( scale < 0 || scale > 128 ) { <nl> + if ( scale < 0 || scale > 127 ) { <nl> av_log ( c -> avctx , AV_LOG_ERROR , " Invalid scale value % d \ n ", <nl> scale ); <nl> return AVERROR_INVALIDDATA ;
static int check ( AVIOContext * pb , int64_t pos , uint32_t * ret_header ) <nl> return CHECK_SEEK_FAILED ; <nl>  <nl> ret = avio_read ( pb , & header_buf [ 0 ], 4 ); <nl> - if ( ret < 0 ) <nl> + /* We should always find four bytes for a valid mpa header . */ <nl> + if ( ret < 4 ) <nl> return CHECK_SEEK_FAILED ; <nl>  <nl> header = AV_RB32 (& header_buf [ 0 ]);
enum BandType { <nl> INTENSITY_BT = 15 , ///< Scalefactor data are intensity stereo positions . <nl> }; <nl>  <nl> -# define IS_CODEBOOK_UNSIGNED ( x ) (( x - 1 ) & 10 ) <nl> +# define IS_CODEBOOK_UNSIGNED ( x ) ((( x ) - 1 ) & 10 ) <nl>  <nl> enum ChannelPosition { <nl> AAC_CHANNEL_OFF = 0 ,
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl>  <nl> memcpy (& h -> poc , & h1 -> poc , sizeof ( h -> poc )); <nl>  <nl> - memcpy ( h -> default_ref , h1 -> default_ref , sizeof ( h -> default_ref )); <nl> memcpy ( h -> short_ref , h1 -> short_ref , sizeof ( h -> short_ref )); <nl> memcpy ( h -> long_ref , h1 -> long_ref , sizeof ( h -> long_ref )); <nl> memcpy ( h -> delayed_pic , h1 -> delayed_pic , sizeof ( h -> delayed_pic ));
static int vqf_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> pkt -> data [ 1 ] = c -> last_frame_bits ; <nl> ret = avio_read ( s -> pb , pkt -> data + 2 , size ); <nl>  <nl> - if ( ret <= 0 ) { <nl> + if ( ret != size ) { <nl> av_free_packet ( pkt ); <nl> return AVERROR ( EIO ); <nl> }
static int parse_playlist ( AppleHTTPContext * c , const char * url , <nl> enum KeyType key_type = KEY_NONE ; <nl> uint8_t iv [ 16 ] = ""; <nl> int has_iv = 0 ; <nl> - char key [ MAX_URL_SIZE ]; <nl> + char key [ MAX_URL_SIZE ] = ""; <nl> char line [ 1024 ]; <nl> const char * ptr ; <nl> int close_in = 0 ;
static int read_highpass ( AVCodecContext * avctx , uint8_t * ptr , int plane , AVFrame <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( a == INT32_MIN ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> ret = read_high_coeffs ( avctx , ptr + bytestream2_tell (& ctx -> gb ), dest , size , <nl> c , ( b >= FFABS ( a )) ? b : a , d , <nl> ctx -> band [ plane ][ i + 1 ]. width , stride );
static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> av_log ( avctx , AV_LOG_ERROR , " Buffer contains IP frames !\ n "); <nl> } <nl>  <nl> + if ( ctx -> frame_type >= FRAMETYPE_NULL_FIRST ) <nl> + return buf_size ; <nl> + <nl> if ( ctx -> frame . data [ 0 ]) <nl> avctx -> release_buffer ( avctx , & ctx -> frame ); <nl> 
static int svq3_decode_frame ( AVCodecContext * avctx , <nl> s -> next_p_frame_damaged = 0 ; <nl> } <nl>  <nl> - frame_start ( h ); <nl> + if ( frame_start ( h ) < 0 ) <nl> + return - 1 ; <nl>  <nl> if ( s -> pict_type == B_TYPE ) { <nl> h -> frame_num_offset = ( h -> slice_num - h -> prev_frame_num );
int ff_hevc_annexb2mp4 ( AVIOContext * pb , const uint8_t * buf_in , <nl> } <nl>  <nl> end : <nl> - free ( start ); <nl> + av_free ( start ); <nl> if ( ps_count ) <nl> * ps_count = num_ps ; <nl> return ret ; <nl> int ff_hevc_annexb2mp4_buf ( const uint8_t * buf_in , uint8_t ** buf_out , <nl> * size = avio_close_dyn_buf ( pb , buf_out ); <nl>  <nl> end : <nl> - free ( start ); <nl> + av_free ( start ); <nl> if ( ps_count ) <nl> * ps_count = num_ps ; <nl> return ret ;
int ff_mov_read_esds ( AVFormatContext * fc , ByteIOContext * pb , MOVAtom atom ) <nl> dprintf ( fc , " Specific MPEG4 header len =% d \ n ", len ); <nl> if (( uint64_t ) len > ( 1 << 30 )) <nl> return - 1 ; <nl> + av_free ( st -> codec -> extradata ); <nl> st -> codec -> extradata = av_mallocz ( len + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! st -> codec -> extradata ) <nl> return AVERROR ( ENOMEM );
static int ape_read_header ( AVFormatContext * s ) <nl> ape -> seektablelength / sizeof (* ape -> seektable ), ape -> totalframes ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> - ape -> frames = av_malloc ( ape -> totalframes * sizeof ( APEFrame )); <nl> + ape -> frames = av_malloc_array ( ape -> totalframes , sizeof ( APEFrame )); <nl> if (! ape -> frames ) <nl> return AVERROR ( ENOMEM ); <nl> ape -> firstframe = ape -> junklength + ape -> descriptorlength + ape -> headerlength + ape -> seektablelength + ape -> wavheaderlength ;
again : <nl> break ; <nl>  <nl> if ( h -> sei_recovery_frame_cnt >= 0 && ( h -> frame_num != h -> sei_recovery_frame_cnt || hx -> slice_type_nos != AV_PICTURE_TYPE_I )) <nl> - h -> valid_recovery_point ++; <nl> + h -> valid_recovery_point = 1 ; <nl>  <nl> if ( h -> sei_recovery_frame_cnt >= 0 <nl> && ( h -> recovery_frame < 0
static int asfrtp_parse_packet ( AVFormatContext * s , PayloadContext * asf , <nl> int prev_len = out_len ; <nl> out_len += cur_len ; <nl> asf -> buf = av_realloc ( asf -> buf , out_len ); <nl> + if (! asf -> buf || FFMIN ( cur_len , len - off )< 0 ) <nl> + return - 1 ; <nl> memcpy ( asf -> buf + prev_len , buf + off , <nl> FFMIN ( cur_len , len - off )); <nl> avio_skip ( pb , cur_len );
unsigned avutil_version ( void ) <nl> av_assert0 ( LIBAVUTIL_VERSION_MICRO >= 100 ); <nl> av_assert0 ( HAVE_MMX2 == HAVE_MMXEXT ); <nl>  <nl> + if ( av_sat_dadd32 ( 1 , 2 ) != 5 ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Libavutil has been build with a broken binutils , please upgrade binutils and rebuild \ n "); <nl> + abort (); <nl> + } <nl> + <nl> return LIBAVUTIL_VERSION_INT ; <nl> } <nl> 
static float dca_dmix_code ( unsigned code ) <nl> static int scan_for_extensions ( AVCodecContext * avctx ) <nl> { <nl> DCAContext * s = avctx -> priv_data ; <nl> - int core_ss_end , ret ; <nl> + int core_ss_end , ret = 0 ; <nl>  <nl> core_ss_end = FFMIN ( s -> frame_size , s -> dca_buffer_size ) * 8 ; <nl> 
reload : <nl>  <nl> static int hls_read_header ( AVFormatContext * s ) <nl> { <nl> - URLContext * u = s -> pb -> opaque ; <nl> + URLContext * u = ( s -> flags & AVFMT_FLAG_CUSTOM_IO ) ? NULL : s -> pb -> opaque ; <nl> HLSContext * c = s -> priv_data ; <nl> int ret = 0 , i , j , stream_offset = 0 ; <nl> 
int ff_h2645_packet_split ( H2645Packet * pkt , const uint8_t * buf , int length , <nl>  <nl> buf += 3 ; <nl> length -= 3 ; <nl> - extract_length = length ; <nl> + extract_length = FFMIN ( length , next_avc - buf ); <nl>  <nl> if ( buf >= next_avc ) { <nl> /* skip to the start of the next NAL */
static int roq_dpcm_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> context -> input_frames ++; <nl> return 0 ; <nl> } <nl> + } <nl> + if ( context -> input_frames < 8 ) { <nl> in = context -> frame_buffer ; <nl> } <nl> 
int ff_qsv_enc_init ( AVCodecContext * avctx , QSVEncContext * q ) <nl> } <nl>  <nl> ret = MFXVideoENCODE_Init ( q -> session , & q -> param ); <nl> - if ( ret < 0 ) { <nl> + if ( MFX_WRN_PARTIAL_ACCELERATION == ret ) { <nl> + av_log ( avctx , AV_LOG_WARNING , " Encoder will work with partial HW acceleration \ n "); <nl> + } else if ( ret < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Error initializing the encoder \ n "); <nl> return ff_qsv_error ( ret ); <nl> }
static void dvbsub_parse_region_segment ( AVCodecContext * avctx , <nl> } <nl>  <nl> region -> depth = 1 << (((* buf ++) >> 2 ) & 7 ); <nl> + if ( region -> depth < 2 || region -> depth > 8 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " region depth % d is invalid \ n ", region -> depth ); <nl> + region -> depth = 4 ; <nl> + } <nl> region -> clut = * buf ++; <nl>  <nl> if ( region -> depth == 8 )
static int seq_fill_buffer ( SeqDemuxContext * seq , ByteIOContext * pb , int buffer_n <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> seq_buffer = & seq -> frame_buffers [ buffer_num ]; <nl> - if ( seq_buffer -> fill_size + data_size > seq_buffer -> data_size ) <nl> + if ( seq_buffer -> fill_size + data_size > seq_buffer -> data_size || data_size <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> url_fseek ( pb , seq -> current_frame_offs + data_offs , SEEK_SET );
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl>  <nl> if ( h -> ref_count [ 0 ] > max_refs || h -> ref_count [ 1 ] > max_refs ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " reference overflow \ n "); <nl> - h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 1 ; <nl> + h -> ref_count [ 0 ] = h -> ref_count [ 1 ] = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int matroska_parse_laces ( MatroskaDemuxContext * matroska , uint8_t ** buf , <nl> } <nl>  <nl> case 0x2 : /* fixed - size lacing */ <nl> - if ( size != ( size / * laces ) * size ) { <nl> + if ( size % (* laces )) { <nl> res = AVERROR_INVALIDDATA ; <nl> break ; <nl> }
static int latm_decode_audio_specific_config ( struct LATMContext * latmctx , <nl> " config not byte aligned .\ n ", 1 ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( asclen <= 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> bits_consumed = decode_audio_specific_config ( NULL , avctx , & m4ac , <nl> gb -> buffer + ( config_start_bit / 8 ), <nl> asclen , sync_extension );
static int output_frame ( AVFilterLink * outlink , int nb_samples ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> in_buf = ff_get_audio_buffer ( outlink , AV_PERM_WRITE , nb_samples ); <nl> - if (! in_buf ) <nl> + if (! in_buf ) { <nl> + avfilter_unref_buffer ( out_buf ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> for ( i = 0 ; i < s -> nb_inputs ; i ++) { <nl> if ( s -> input_state [ i ] == INPUT_ON ) {
static void opt_list ( void * obj , void * av_log_obj , const char * unit , <nl> av_log ( av_log_obj , AV_LOG_INFO , " ( default "); <nl> switch ( opt -> type ) { <nl> case AV_OPT_TYPE_FLAGS : <nl> - av_log ( av_log_obj , AV_LOG_INFO , "% 0llX ", opt -> default_val . i64 ); <nl> + av_log ( av_log_obj , AV_LOG_INFO , "%" PRIX64 , opt -> default_val . i64 ); <nl> break ; <nl> case AV_OPT_TYPE_DURATION : <nl> case AV_OPT_TYPE_INT :
int vp56_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> int is_alpha , av_uninit ( alpha_offset ); <nl>  <nl> if ( s -> has_alpha ) { <nl> + if ( remaining_buf_size < 3 ) <nl> + return - 1 ; <nl> alpha_offset = bytestream_get_be24 (& buf ); <nl> remaining_buf_size -= 3 ; <nl> + if ( remaining_buf_size < alpha_offset ) <nl> + return - 1 ; <nl> } <nl>  <nl> for ( is_alpha = 0 ; is_alpha < 1 + s -> has_alpha ; is_alpha ++) {
int av_strerror ( int errnum , char * errbuf , size_t errbuf_size ) <nl> av_strlcpy ( errbuf , entry -> str , errbuf_size ); <nl> } else { <nl> # if HAVE_STRERROR_R <nl> - ret = strerror_r ( AVUNERROR ( errnum ), errbuf , errbuf_size ); <nl> + ret = AVERROR ( strerror_r ( AVUNERROR ( errnum ), errbuf , errbuf_size )); <nl> # else <nl> ret = - 1 ; <nl> # endif
static int mov_read_sidx ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> MOVFragmentStreamInfo * si ; <nl> si = & item -> stream_info [ j ]; <nl> if ( si -> sidx_pts != AV_NOPTS_VALUE ) { <nl> - ref_st = c -> fc -> streams [ i ]; <nl> + ref_st = c -> fc -> streams [ j ]; <nl> ref_sc = ref_st -> priv_data ; <nl> break ; <nl> }
int av_bsf_list_parse_str ( const char * str , AVBSFContext ** bsf_lst ) <nl> if (! lst ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - if (!( dup = buf = av_strdup ( str ))) <nl> - return AVERROR ( ENOMEM ); <nl> + if (!( dup = buf = av_strdup ( str ))) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl>  <nl> while ( 1 ) { <nl> bsf_str = av_strtok ( buf , ",", & saveptr );
static int wma_decode_superframe ( AVCodecContext * avctx , void * data , <nl>  <nl> /* read each frame starting from bit_offset */ <nl> pos = bit_offset + 4 + 4 + s -> byte_offset_bits + 3 ; <nl> + if ( pos >= MAX_CODED_SUPERFRAME_SIZE * 8 ) <nl> + return AVERROR_INVALIDDATA ; <nl> init_get_bits (& s -> gb , buf + ( pos >> 3 ), ( MAX_CODED_SUPERFRAME_SIZE - ( pos >> 3 ))* 8 ); <nl> len = pos & 7 ; <nl> if ( len > 0 )
static av_cold int wavpack_encode_init ( AVCodecContext * avctx ) <nl> s -> avctx = avctx ; <nl>  <nl> if ( avctx -> channels > 255 ) { <nl> - av_log ( avctx , AV_LOG_ERROR , " Too many channels \ n ", avctx -> channels ); <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid channel count : % d \ n ", avctx -> channels ); <nl> return AVERROR ( EINVAL ); <nl> } <nl> 
static int iff_read_packet ( AVFormatContext * s , <nl> buf = pkt -> data ; <nl> bytestream_put_be16 (& buf , 2 ); <nl> ret = avio_read ( pb , buf , iff -> body_size ); <nl> + if ( ret >= 0 && ret < iff -> body_size ) <nl> + av_shrink_packet ( pkt , ret + 2 ); <nl> } else { <nl> av_assert0 ( 0 ); <nl> }
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl> escape = av_mod_uintp2 ( 16383 , pfx ); <nl> cnt1 = get_unary ( b , 0 , 8 ); <nl> if ( cnt1 < 8 ) { <nl> + if ( pfx < 1 || pfx > 25 ) <nl> + return AVERROR_INVALIDDATA ; <nl> value = show_bits ( b , pfx ); <nl> if ( value > 1 ) { <nl> skip_bits ( b , pfx );
void ff_ivi_output_plane ( IVIPlaneDesc * plane , uint8_t * dst , int dst_pitch ) <nl> const int16_t * src = plane -> bands [ 0 ]. buf ; <nl> uint32_t pitch = plane -> bands [ 0 ]. pitch ; <nl>  <nl> + if (! src ) <nl> + return ; <nl> + <nl> for ( y = 0 ; y < plane -> height ; y ++) { <nl> for ( x = 0 ; x < plane -> width ; x ++) <nl> dst [ x ] = av_clip_uint8 ( src [ x ] + 128 );
retry : <nl> is -> frame_timer += delay * FFMAX ( 1 , floor (( time - is -> frame_timer ) / delay )); <nl>  <nl> SDL_LockMutex ( is -> pictq_mutex ); <nl> - if (! isnan ( vp -> pts )) <nl> + if (! redisplay && ! isnan ( vp -> pts )) <nl> update_video_pts ( is , vp -> pts , vp -> pos , vp -> serial ); <nl> SDL_UnlockMutex ( is -> pictq_mutex ); <nl> 
av_cold void ff_ac3dsp_init_x86 ( AC3DSPContext * c , int bit_exact ) <nl> c -> ac3_rshift_int32 = ff_ac3_rshift_int32_mmx ; <nl> } <nl> if ( EXTERNAL_AMD3DNOW ( mm_flags )) { <nl> - c -> extract_exponents = ff_ac3_extract_exponents_3dnow ; <nl> if (! bit_exact ) { <nl> c -> float_to_fixed24 = ff_float_to_fixed24_3dnow ; <nl> }
mp_image_t * vf_get_image ( vf_instance_t * vf , unsigned int outfmt , int mp_imgtype , <nl> } <nl>  <nl> mpi -> qscale = NULL ; <nl> - } <nl> mpi -> usage_count ++; <nl> + } <nl> // printf ("\ rVF_MPI : % p % p % p % d % d % d \ n ", <nl> // mpi -> planes [ 0 ], mpi -> planes [ 1 ], mpi -> planes [ 2 ], <nl> // mpi -> stride [ 0 ], mpi -> stride [ 1 ], mpi -> stride [ 2 ]);
static int matroska_read_seek ( AVFormatContext * s , int stream_index , <nl> tracks [ i ]. stream , st -> index_entries [ index ]. timestamp , <nl> AVSEEK_FLAG_BACKWARD ); <nl> while ( index_sub >= 0 && <nl> - index_min >= 0 && <nl> + index_min > 0 && <nl> tracks [ i ]. stream -> index_entries [ index_sub ]. pos < st -> index_entries [ index_min ]. pos && <nl> st -> index_entries [ index ]. timestamp - tracks [ i ]. stream -> index_entries [ index_sub ]. timestamp < 30000000000 / matroska -> time_scale ) <nl> index_min --;
int attribute_align_arg avcodec_encode_audio2 ( AVCodecContext * avctx , <nl> implement encode2 () */ <nl> buf_size = 2 * avctx -> frame_size * avctx -> channels * <nl> av_get_bytes_per_sample ( avctx -> sample_fmt ); <nl> - buf_size += FF_MIN_BUFFER_SIZE ; <nl> + buf_size += 2 * FF_MIN_BUFFER_SIZE ; <nl> } <nl> } <nl> if (( ret = ff_alloc_packet ( avpkt , buf_size )))
fail : <nl> if ( pkt -> stream_index == seg -> reference_stream_index ) <nl> seg -> frame_count ++; <nl>  <nl> - if ( ret < 0 ) { <nl> - if ( seg -> list ) <nl> - avio_close ( seg -> list_pb ); <nl> - avformat_free_context ( oc ); <nl> - } <nl> - <nl> return ret ; <nl> } <nl> 
static int cinepak_decode_vectors ( CinepakContext * s , cvid_strip * strip , <nl> const uint8_t * eod = ( data + size ); <nl> uint32_t flag , mask ; <nl> uint8_t * cb0 , * cb1 , * cb2 , * cb3 ; <nl> - unsigned int x , y ; <nl> + int x , y ; <nl> char * ip0 , * ip1 , * ip2 , * ip3 ; <nl>  <nl> flag = 0 ;
static int hevc_frame_start ( HEVCContext * s ) <nl>  <nl> fail : <nl> if ( s -> ref ) <nl> - ff_thread_report_progress (& s -> ref -> tf , INT_MAX , 0 ); <nl> + ff_hevc_unref_frame ( s , s -> ref , ~ 0 ); <nl> s -> ref = NULL ; <nl> return ret ; <nl> }
static void put_ebml_num ( ByteIOContext * pb , uint64_t num , int bytes ) <nl> static void put_ebml_uint ( ByteIOContext * pb , unsigned int elementid , uint64_t val ) <nl> { <nl> int i , bytes = 1 ; <nl> - while ( val >> bytes * 8 && bytes < 8 ) bytes ++; <nl> + while ( bytes < 8 && val >> bytes * 8 ) bytes ++; <nl>  <nl> put_ebml_id ( pb , elementid ); <nl> put_ebml_num ( pb , bytes , 0 );
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> case XWD_GRAY_SCALE : <nl> if ( bpp != 1 && bpp != 8 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( pixdepth == 1 ) { <nl> + if ( bpp == 1 && pixdepth == 1 ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_MONOWHITE ; <nl> - } else if ( pixdepth == 8 ) { <nl> + } else if ( bpp == 8 && pixdepth == 8 ) { <nl> avctx -> pix_fmt = AV_PIX_FMT_GRAY8 ; <nl> } <nl> break ;
typedef struct WmallDecodeCtx { <nl>  <nl> int8_t mclms_order ; <nl> int8_t mclms_scaling ; <nl> - int16_t mclms_coeffs [ 128 ]; <nl> - int16_t mclms_coeffs_cur [ 4 ]; <nl> + int16_t mclms_coeffs [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS * 32 ]; <nl> + int16_t mclms_coeffs_cur [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS ]; <nl> int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ;
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl>  <nl> if ( hdr -> substreamid == info -> num_ind_sub + 1 ) { <nl> // info -> num_ind_sub ++; <nl> - avpriv_request_sample ( track -> par , " Multiple independent substreams "); <nl> + avpriv_request_sample ( mov -> fc , " Multiple independent substreams "); <nl> return AVERROR_PATCHWELCOME ; <nl> } else if ( hdr -> substreamid < info -> num_ind_sub || <nl> hdr -> substreamid == 0 && info -> substream [ 0 ]. bsid ) {
static inline int convert_frame ( AVAudioResampleContext * avr , <nl>  <nl> static inline int available_samples ( AVFrame * out ) <nl> { <nl> + int samples ; <nl> int bytes_per_sample = av_get_bytes_per_sample ( out -> format ); <nl> - int samples = out -> linesize [ 0 ] / bytes_per_sample ; <nl> + if (! bytes_per_sample ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> + samples = out -> linesize [ 0 ] / bytes_per_sample ; <nl> if ( av_sample_fmt_is_planar ( out -> format )) { <nl> return samples ; <nl> } else {
static int decode_motion ( GetBitContext * gb ) <nl> static int decode_mb ( MadContext * s , AVFrame * frame , int inter ) <nl> { <nl> int mv_map = 0 ; <nl> - int mv_x , mv_y ; <nl> + int av_uninit ( mv_x ), av_uninit ( mv_y ); <nl> int j ; <nl>  <nl> if ( inter ) {
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> S *= 1U << s -> float_shift ; <nl> sign = S < 0 ; <nl> if ( sign ) <nl> - S = - S ; <nl> - if ( S >= 0x1000000 ) { <nl> + S = -( unsigned ) S ; <nl> + if ( S >= 0x1000000U ) { <nl> if ( s -> got_extra_bits && get_bits1 (& s -> gb_extra_bits )) <nl> S = get_bits (& s -> gb_extra_bits , 23 ); <nl> else
static int aiff_read_packet ( AVFormatContext * s , <nl> size = st -> codecpar -> block_align ; <nl> break ; <nl> default : <nl> - size = ( MAX_SIZE / st -> codecpar -> block_align ) * st -> codecpar -> block_align ; <nl> + size = st -> codecpar -> block_align ? ( MAX_SIZE / st -> codecpar -> block_align ) * st -> codecpar -> block_align : MAX_SIZE ; <nl> } <nl> size = FFMIN ( max_size , size ); <nl> res = av_get_packet ( s -> pb , pkt , size );
static int ebml_parse_elem ( MatroskaDemuxContext * matroska , <nl> data = ( char *) data + syntax -> data_offset ; <nl> if ( syntax -> list_elem_size ) { <nl> EbmlList * list = data ; <nl> - newelem = av_realloc ( list -> elem , ( list -> nb_elem + 1 )* syntax -> list_elem_size ); <nl> + newelem = av_realloc_array ( list -> elem , list -> nb_elem + 1 , syntax -> list_elem_size ); <nl> if (! newelem ) <nl> return AVERROR ( ENOMEM ); <nl> list -> elem = newelem ;
static av_cold int init ( AVFilterContext * ctx , const char * args , void * opaque ) <nl> static av_cold void uninit ( AVFilterContext * ctx ) <nl> { <nl> BufferSourceContext * s = ctx -> priv ; <nl> - while ( av_fifo_size ( s -> fifo )) { <nl> + while ( s -> fifo && av_fifo_size ( s -> fifo )) { <nl> AVFilterBufferRef * buf ; <nl> av_fifo_generic_read ( s -> fifo , & buf , sizeof ( buf ), NULL ); <nl> avfilter_unref_buffer ( buf );
static void compute_scale_factors ( unsigned char scale_code [ SBLIMIT ], <nl> vmax = v ; <nl> } <nl> /* compute the scale factor index using log 2 computations */ <nl> - if ( vmax > 0 ) { <nl> + if ( vmax > 1 ) { <nl> n = av_log2 ( vmax ); <nl> /* n is the position of the MSB of vmax . now <nl> use at most 2 compares to find the index */
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> pkt . size = data_size ; <nl>  <nl> ret = ff_vp8_decode_frame ( avctx , p , got_frame , & pkt ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( s -> has_alpha ) { <nl> ret = vp8_lossy_decode_alpha ( avctx , p , s -> alpha_data , <nl> s -> alpha_data_size );
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> avio_skip ( pb , 16 ); <nl>  <nl> for ( type = 0 ; type != - 1 && avio_tell ( pb ) < next ; ) { <nl> + if ( url_feof ( pb )) <nl> + return AVERROR ( EOF ); <nl> type = avio_rb16 ( pb ); <nl> len = avio_rb16 ( pb ); <nl> av_log ( c -> fc , AV_LOG_DEBUG , " type % d , len % d \ n ", type , len );
static int read_sbr_grid ( AACContext * ac , SpectralBandReplication * sbr , <nl> if ( ch_data -> bs_frame_class == FIXFIX ) { <nl> idx = ch_data -> bs_num_env >> 1 ; <nl> } else if ( ch_data -> bs_frame_class & 1 ) { // FIXVAR or VARVAR <nl> - idx = ch_data -> bs_num_env - FFMAX ( bs_pointer - 1 , 1 ); <nl> + idx = ch_data -> bs_num_env - FFMAX (( int ) bs_pointer - 1 , 1 ); <nl> } else { // VARFIX <nl> if (! bs_pointer ) <nl> idx = 1 ;
static int svq1_encode_frame ( AVCodecContext * avctx , unsigned char * buf , <nl> init_put_bits (& s -> pb , buf , buf_size ); <nl>  <nl> * p = * pict ; <nl> - p -> pict_type = avctx -> frame_number % avctx -> gop_size ? P_TYPE : I_TYPE ; <nl> + p -> pict_type = avctx -> gop_size && avctx -> frame_number % avctx -> gop_size ? P_TYPE : I_TYPE ; <nl> p -> key_frame = p -> pict_type == I_TYPE ; <nl>  <nl> svq1_write_header ( s , p -> pict_type );
static int mxf_get_sorted_table_segments ( MXFContext * mxf , int * nb_sorted_segment <nl> if ( mxf -> metadata_sets [ i ]-> type == IndexTableSegment ) <nl> nb_segments ++; <nl>  <nl> + if (! nb_segments ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> * sorted_segments = av_mallocz ( nb_segments * sizeof (** sorted_segments )); <nl> unsorted_segments = av_mallocz ( nb_segments * sizeof (* unsorted_segments )); <nl> if (! sorted_segments || ! unsorted_segments ) {
avs_decode_frame ( AVCodecContext * avctx , <nl> int i , j , x , y , stride , vect_w = 3 , vect_h = 3 ; <nl> AvsVideoSubType sub_type ; <nl> AvsBlockType type ; <nl> - GetBitContext change_map ; <nl> + GetBitContext change_map = { 0 }; // init to silence warning <nl>  <nl> if ( avctx -> reget_buffer ( avctx , p )) { <nl> av_log ( avctx , AV_LOG_ERROR , " reget_buffer () failed \ n ");
static void ebml_free ( EbmlSyntax * syntax , void * data ) <nl> j ++, ptr += syntax [ i ]. list_elem_size ) <nl> ebml_free ( syntax [ i ]. def . n , ptr ); <nl> av_freep (& list -> elem ); <nl> + list -> nb_elem = 0 ; <nl> } else <nl> ebml_free ( syntax [ i ]. def . n , data_off ); <nl> default :
static void park_frame_worker_threads ( FrameThreadContext * fctx , int thread_count <nl> pthread_cond_wait (& p -> output_cond , & p -> progress_mutex ); <nl> pthread_mutex_unlock (& p -> progress_mutex ); <nl> } <nl> + p -> got_frame = 0 ; <nl> } <nl> } <nl> 
static int hls_write_trailer ( struct AVFormatContext * s ) <nl> av_freep (& vs -> baseurl ); <nl> } <nl>  <nl> + ff_format_io_close ( s , & hls -> m3u8_out ); <nl> + ff_format_io_close ( s , & hls -> sub_m3u8_out ); <nl> av_freep (& hls -> key_basename ); <nl> av_freep (& hls -> var_streams ); <nl> av_freep (& hls -> master_m3u8_url );
static void conv411 ( uint8_t * dst , int dst_wrap , <nl> int w , c ; <nl> uint8_t * s1 , * s2 , * d ; <nl>  <nl> + width >>= 1 ; <nl> + <nl> for (; height > 0 ; height --) { <nl> s1 = src ; <nl> s2 = src + src_wrap ;
static int common_end ( AVCodecContext * avctx ){ <nl> PlaneContext * p = & s -> plane [ i ]; <nl>  <nl> av_freep (& p -> state ); <nl> + av_freep (& p -> vlc_state ); <nl> } <nl>  <nl> return 0 ;
void av_dump_format ( AVFormatContext * ic , int index , <nl> av_log ( NULL , AV_LOG_INFO , " Duration : "); <nl> if ( ic -> duration != AV_NOPTS_VALUE ) { <nl> int hours , mins , secs , us ; <nl> - int64_t duration = ic -> duration + 5000 ; <nl> + int64_t duration = ic -> duration + ( ic -> duration <= INT64_MAX - 5000 ? 5000 : 0 ); <nl> secs = duration / AV_TIME_BASE ; <nl> us = duration % AV_TIME_BASE ; <nl> mins = secs / 60 ;
static int allocate_buffers ( ShortenContext * s ) <nl>  <nl> static inline unsigned int get_uint ( ShortenContext * s , int k ) <nl> { <nl> - if ( s -> version != 0 ) <nl> + if ( s -> version != 0 ) { <nl> k = get_ur_golomb_shorten (& s -> gb , ULONGSIZE ); <nl> + if ( k > 31U ) <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> return get_ur_golomb_shorten (& s -> gb , k ); <nl> } <nl> 
static int mxf_read_content_storage ( void * arg , AVIOContext * pb , int tag , int siz <nl> MXFContext * mxf = arg ; <nl> switch ( tag ) { <nl> case 0x1901 : <nl> + if ( mxf -> packages_refs ) <nl> + av_log ( mxf -> fc , AV_LOG_VERBOSE , " Multiple packages_refs \ n "); <nl> + av_free ( mxf -> packages_refs ); <nl> mxf -> packages_count = avio_rb32 ( pb ); <nl> mxf -> packages_refs = av_calloc ( mxf -> packages_count , sizeof ( UID )); <nl> if (! mxf -> packages_refs )
static inline int get_ue_golomb ( GetBitContext * gb ) <nl> int log = 2 * av_log2 ( buf ) - 31 ; <nl> LAST_SKIP_BITS ( re , gb , 32 - log ); <nl> CLOSE_READER ( re , gb ); <nl> - if ( CONFIG_FTRAPV && log < 0 ) { <nl> + if ( log < 7 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid UE golomb code \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int smvjpeg_decode_frame ( AVCodecContext * avctx , void * data , int * data_siz <nl> s -> picture [ 1 ]-> linesize [ i ] = mjpeg_data -> linesize [ i ]; <nl>  <nl> ret = av_frame_ref ( data , s -> picture [ 1 ]); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> } <nl>  <nl> - return ret ; <nl> + return avpkt -> size ; <nl> } <nl>  <nl> static const AVClass smvjpegdec_class = {
static int decode_rle ( uint8_t * bitmap , int linesize , int w , int h , <nl> if ( start >= buf_size ) <nl> return - 1 ; <nl>  <nl> + if ( w <= 0 || h <= 0 ) <nl> + return - 1 ; <nl> + <nl> bit_len = ( buf_size - start ) * 8 ; <nl> init_get_bits (& gb , buf + start , bit_len ); <nl> 
static int idcin_read_seek ( AVFormatContext * s , int stream_index , <nl> IdcinDemuxContext * idcin = s -> priv_data ; <nl>  <nl> if ( idcin -> first_pkt_pos > 0 ) { <nl> - int ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> + int64_t ret = avio_seek ( s -> pb , idcin -> first_pkt_pos , SEEK_SET ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> ff_update_cur_dts ( s , s -> streams [ idcin -> video_stream_index ], 0 );
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> int lower_transport , const char * real_challenge ) <nl> { <nl> RTSPState * rt = s -> priv_data ; <nl> - int rtx , j , i , err , interleave = 0 ; <nl> + int rtx = 0 , j , i , err , interleave = 0 ; <nl> RTSPStream * rtsp_st ; <nl> RTSPMessageHeader reply1 , * reply = & reply1 ; <nl> char cmd [ 2048 ];
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> + modeex . Size = sizeof ( D3DDISPLAYMODEEX ); <nl> hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> if ( FAILED ( hr )) { <nl> IDirect3D9Ex_Release ( d3d9ex );
static int encode_apng ( AVCodecContext * avctx , AVPacket * pkt , <nl> return AVERROR ( ENOMEM ); <nl>  <nl> if ( avctx -> frame_number == 0 ) { <nl> + if (! pict ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> s -> bytestream = avctx -> extradata = av_malloc ( FF_MIN_BUFFER_SIZE ); <nl> if (! avctx -> extradata ) <nl> return AVERROR ( ENOMEM );
static int aac_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> } <nl> start_ch += chans ; <nl> } <nl> - if (( ret = ff_alloc_packet2 ( avctx , avpkt , 768 * s -> channels ))) { <nl> + if (( ret = ff_alloc_packet2 ( avctx , avpkt , 8192 * s -> channels ))) { <nl> av_log ( avctx , AV_LOG_ERROR , " Error getting output packet \ n "); <nl> return ret ; <nl> }
int ff_hevc_decode_nal_sps ( HEVCContext * s ) <nl> if ( s -> sps_list [ sps_id ] && s -> sps == ( HEVCSPS *) s -> sps_list [ sps_id ]-> data ) { <nl> av_buffer_unref (& s -> current_sps ); <nl> s -> current_sps = av_buffer_ref ( s -> sps_list [ sps_id ]); <nl> + if (! s -> current_sps ) <nl> + s -> sps = NULL ; <nl> } <nl> av_buffer_unref (& s -> sps_list [ sps_id ]); <nl> s -> sps_list [ sps_id ] = sps_buf ;
static int mov_write_vmhd_tag ( ByteIOContext * pb ) <nl>  <nl> static int mov_write_hdlr_tag ( ByteIOContext * pb , MOVTrack * track ) <nl> { <nl> - const char * descr , * hdlr , * hdlr_type ; <nl> + const char * hdlr , * descr = NULL , * hdlr_type = NULL ; <nl> int64_t pos = url_ftell ( pb ); <nl>  <nl> if (! track ) { /* no media --> data handler */
static inline void render_line_unrolled ( intptr_t x , unsigned char y , int x1 , <nl> } <nl> } <nl>  <nl> - static void render_line ( int x0 , int y0 , int x1 , int y1 , float * buf ) <nl> + static void render_line ( int x0 , unsigned char y0 , int x1 , int y1 , float * buf ) <nl> { <nl> int dy = y1 - y0 ; <nl> int adx = x1 - x0 ;
static int read_code_table ( CLLCContext * ctx , GetBitContext * gb , VLC * vlc ) <nl>  <nl> count ++; <nl> } <nl> + if ( prefix > ( 65535 - 256 )/ 2 ) { <nl> + vlc -> table = NULL ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> prefix <<= 1 ; <nl> }
static int scale_vector ( int16_t * dst , const int16_t * vector , int length ) <nl> for ( i = 0 ; i < length ; i ++) <nl> max |= FFABS ( vector [ i ]); <nl>  <nl> - max = FFMIN ( max , 0x7FFF ); <nl> bits = normalize_bits ( max , 15 ); <nl>  <nl> if ( bits == 15 )
int ff_audio_interleave_init ( AVFormatContext * s , <nl> aic -> time_base = time_base ; <nl>  <nl> aic -> fifo_size = 100 * * aic -> samples ; <nl> - aic -> fifo = av_fifo_alloc_array ( 100 , * aic -> samples ); <nl> + if (!( aic -> fifo = av_fifo_alloc_array ( 100 , * aic -> samples ))) <nl> + return AVERROR ( ENOMEM ); <nl> } <nl> } <nl> 
static int mov_read_close ( AVFormatContext * s ) <nl> av_freep (& mov -> fragment_index_data ); <nl>  <nl> av_freep (& mov -> aes_decrypt ); <nl> + av_freep (& mov -> chapter_tracks ); <nl>  <nl> return 0 ; <nl> }
static int read_header ( AVFormatContext * s ) <nl> uint8_t root [ WTV_SECTOR_SIZE ]; <nl> AVIOContext * pb ; <nl> int64_t timeline_pos ; <nl> - int ret ; <nl> + int64_t ret ; <nl>  <nl> wtv -> epoch = <nl> wtv -> pts =
static int fourxm_read_packet ( AVFormatContext * s , <nl>  <nl> if ( ret < 0 ) { <nl> av_free_packet ( pkt ); <nl> - } else <nl> + } else { <nl> packet_read = 1 ; <nl> + av_shrink_packet ( pkt , ret + 8 ); <nl> + } <nl> break ; <nl>  <nl> case snd__TAG :
AVStream * avformat_new_stream ( AVFormatContext * s , const AVCodec * c ) <nl> st -> info -> last_dts = AV_NOPTS_VALUE ; <nl>  <nl> st -> codec = avcodec_alloc_context3 ( c ); <nl> + if (! st -> codec ) { <nl> + av_free ( st -> info ); <nl> + av_free ( st ); <nl> + return NULL ; <nl> + } <nl> if ( s -> iformat ) { <nl> /* no default bitrate if decoding */ <nl> st -> codec -> bit_rate = 0 ;
int avformat_seek_file ( AVFormatContext * s , int stream_index , int64_t min_ts , int <nl> { <nl> if ( min_ts > ts || max_ts < ts ) <nl> return - 1 ; <nl> + if ( stream_index < - 1 || stream_index >= ( int ) s -> nb_streams ) <nl> + return AVERROR ( EINVAL ); <nl>  <nl> if ( s -> seek2any > 0 ) <nl> flags |= AVSEEK_FLAG_ANY ;
static av_cold void rnd_table_init ( void ) { <nl>  <nl> static av_cold void init_noise_samples ( void ) { <nl> int i ; <nl> - int random_seed = 0 ; <nl> + unsigned random_seed = 0 ; <nl> float delta = 1 . 0 / 16384 . 0 ; <nl> for ( i = 0 ; i < 128 ; i ++) { <nl> random_seed = random_seed * 214013 + 2531011 ;
int avpriv_mpeg4audio_get_config ( MPEG4AudioConfig * c , const uint8_t * buf , <nl> GetBitContext gb ; <nl> int specific_config_bitindex ; <nl>  <nl> - if ( bit_size <= 0 ) <nl> + if ( bit_size <= 0 || init_get_bits (& gb , buf , bit_size ) < 0 ) <nl> return AVERROR_INVALIDDATA ; <nl> - <nl> - init_get_bits (& gb , buf , bit_size ); <nl> c -> object_type = get_object_type (& gb ); <nl> c -> sample_rate = get_sample_rate (& gb , & c -> sampling_index ); <nl> c -> chan_config = get_bits (& gb , 4 );
av_cold int ff_vp8_decode_free ( AVCodecContext * avctx ) <nl> VP8Context * s = avctx -> priv_data ; <nl> int i ; <nl>  <nl> + if (! s ) <nl> + return 0 ; <nl> + <nl> vp8_decode_flush_impl ( avctx , 1 ); <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( s -> frames ); i ++) <nl> av_frame_free (& s -> frames [ i ]. tf . f );
static int decode_frame ( AVCodecContext * avctx , <nl> int size , offset , start = 0 ; <nl>  <nl> offset = bytestream2_get_le16 ( gb ); <nl> - if ( offset > s -> nb_blocks ) <nl> + if ( offset >= s -> nb_blocks ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> size = bytestream2_get_le16 ( gb );
static int transcode_init ( void ) <nl> AVFormatContext * oc ; <nl> OutputStream * ost ; <nl> InputStream * ist ; <nl> - char error [ 1024 ]; <nl> + char error [ 1024 ] = { 0 }; <nl> int want_sdp = 1 ; <nl>  <nl> for ( i = 0 ; i < nb_filtergraphs ; i ++) {
int ff_jni_init_jfields ( JNIEnv * env , void * jfields , const struct FFJniField * jfi <nl>  <nl> last_clazz = *( jclass *)(( uint8_t *) jfields + jfields_mapping [ i ]. offset ) = <nl> global ? (* env )-> NewGlobalRef ( env , clazz ) : clazz ; <nl> + <nl> + if ( global ) { <nl> + (* env )-> DeleteLocalRef ( env , clazz ); <nl> + } <nl> + <nl> } else { <nl>  <nl> if (! last_clazz ) {
int main ( void ){ <nl> AVPixFmtDescriptor * desc = av_pix_fmt_desc_get ( i ); <nl> if (! desc ) <nl> continue ; <nl> - av_log ( 0 , AV_LOG_INFO , " pix fmt % s % d \ n ", desc -> name , is_yuv_planar ( i )); <nl> + av_log ( 0 , AV_LOG_INFO , " pix fmt % s yuv_plan :% d avg_bpp :% d \ n ", desc -> name , is_yuv_planar ( i ), avg_bits_per_pixel ( i )); <nl> } <nl> return 0 ; <nl> }
static int vorbis_decode_frame ( AVCodecContext * avctx , void * data , <nl> if (! vc -> first_frame ) { <nl> vc -> first_frame = 1 ; <nl> * got_frame_ptr = 0 ; <nl> + av_frame_unref ( frame ); <nl> return buf_size ; <nl> } <nl> 
FF_ENABLE_DEPRECATION_WARNINGS <nl> wipe_side_data ( dst ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + if ( sd_src -> buf ) { <nl> sd_dst -> buf = av_buffer_ref ( sd_src -> buf ); <nl> if (! sd_dst -> buf ) { <nl> wipe_side_data ( dst ); <nl> FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl> sd_dst -> data = sd_dst -> buf -> data ; <nl> sd_dst -> size = sd_dst -> buf -> size ; <nl> + } <nl> } <nl> av_dict_copy (& sd_dst -> metadata , sd_src -> metadata , 0 ); <nl> }
vbv_retry : <nl> if ( s -> mb_info ) <nl> av_packet_shrink_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , s -> mb_info_size ); <nl> } else { <nl> - assert (( put_bits_ptr (& s -> pb ) == s -> pb . buf )); <nl> s -> frame_bits = 0 ; <nl> } <nl> assert (( s -> frame_bits & 7 ) == 0 );
int attribute_align_arg avcodec_encode_audio ( AVCodecContext * avctx , <nl> const short * samples ) <nl> { <nl> AVPacket pkt ; <nl> - AVFrame frame0 ; <nl> + AVFrame frame0 = { 0 }; <nl> AVFrame * frame ; <nl> int ret , samples_size , got_packet ; <nl> 
static int configure_output_audio_filter ( FilterGraph * fg , OutputFilter * ofilter , <nl> pad_idx = 0 ; \ <nl> } while ( 0 ) <nl>  <nl> - if ( audio_sync_method > 0 ) { <nl> + if ( audio_sync_method > 0 && 0 ) { <nl> char args [ 256 ] = { 0 }; <nl>  <nl> av_strlcatf ( args , sizeof ( args ), " min_comp = 0 . 001 : min_hard_comp =% f ", audio_drift_threshold );
static int get_qcd ( Jpeg2000DecoderContext * s , int n , Jpeg2000QuantStyle * q , <nl> Jpeg2000QuantStyle tmp ; <nl> int compno , ret ; <nl>  <nl> + memset (& tmp , 0 , sizeof ( tmp )); <nl> + <nl> if (( ret = get_qcx ( s , n , & tmp )) < 0 ) <nl> return ret ; <nl> for ( compno = 0 ; compno < s -> ncomponents ; compno ++)
uint64_t time = rdtsc (); <nl> s -> workaround_bugs = avctx -> workaround_bugs ; <nl> s -> flags = avctx -> flags ; <nl>  <nl> - /* no supplementary picture */ <nl> + * data_size = 0 ; <nl> + <nl> + /* no supplementary picture */ <nl> if ( buf_size == 0 ) { <nl> - * data_size = 0 ; <nl> return 0 ; <nl> } <nl> 
static void dump_stream_format ( AVFormatContext * ic , int i , <nl> int g = av_gcd ( st -> time_base . num , st -> time_base . den ); <nl> AVDictionaryEntry * lang = av_dict_get ( st -> metadata , " language ", NULL , 0 ); <nl>  <nl> + if (! g ) <nl> + g = 1 ; <nl> + <nl> avcodec_string ( buf , sizeof ( buf ), st -> codec , is_output ); <nl> av_log ( NULL , AV_LOG_INFO , " Stream #% d :% d ", index , i ); <nl> 
static int config_props ( AVFilterLink * inlink ) <nl> s -> hsub = pixdesc -> log2_chroma_w ; <nl> s -> vsub = pixdesc -> log2_chroma_h ; <nl>  <nl> - s -> bpp = av_get_bits_per_pixel ( pixdesc ) >> 3 ; <nl> + s -> bpp = pixdesc -> flags & AV_PIX_FMT_FLAG_PLANAR ? <nl> + 1 : <nl> + av_get_bits_per_pixel ( pixdesc ) >> 3 ; <nl> s -> alpha &= !!( pixdesc -> flags & AV_PIX_FMT_FLAG_ALPHA ); <nl> s -> is_packed_rgb = ff_fill_rgba_map ( s -> rgba_map , inlink -> format ) >= 0 ; <nl> 
static int parse_header ( OutputStream * os , const uint8_t * buf , int buf_size ) <nl> if ( size > buf_size ) <nl> return AVERROR_INVALIDDATA ; <nl> if ( type == 8 || type == 9 ) { <nl> - if ( os -> nb_extra_packets > FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> + if ( os -> nb_extra_packets >= FF_ARRAY_ELEMS ( os -> extra_packets )) <nl> return AVERROR_INVALIDDATA ; <nl> os -> extra_packet_sizes [ os -> nb_extra_packets ] = size ; <nl> os -> extra_packets [ os -> nb_extra_packets ] = av_malloc ( size );
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * insamples ) <nl> av_rescale_q (( p - ( int16_t *) insamples -> data [ 0 ]) / nb_channels , <nl> ( AVRational ){ 1 , inlink -> sample_rate }, <nl> outlink -> time_base ); <nl> - outlink -> out_buf = outpicref ; <nl> linesize = outpicref -> linesize [ 0 ]; <nl> memset ( outpicref -> data [ 0 ], 0 , showwaves -> h * linesize ); <nl> }
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> int best_fps = 0 ; <nl> double best_error = 0 . 01 ; <nl>  <nl> + if ( delta_dts >= INT64_MAX / st -> time_base . num || <nl> + delta_packets >= INT64_MAX / st -> time_base . den ) <nl> + continue ; <nl> av_reduce (& st -> avg_frame_rate . num , & st -> avg_frame_rate . den , <nl> delta_packets *( int64_t ) st -> time_base . den , <nl> delta_dts *( int64_t ) st -> time_base . num , 60000 );
static int dca_convert_bitstream ( uint8_t * src , int src_size , uint8_t * dst , <nl> uint16_t * ssrc = ( uint16_t *) src , * sdst = ( uint16_t *) dst ; <nl> PutBitContext pb ; <nl>  <nl> + if (( unsigned ) src_size > ( unsigned ) max_size ) <nl> + return - 1 ; <nl> + <nl> mrk = AV_RB32 ( src ); <nl> switch ( mrk ) { <nl> case DCA_MARKER_RAW_BE :
static inline int mpeg1_fast_decode_block_inter ( MpegEncContext * s , int16_t * bloc <nl> } <nl>  <nl> block [ j ] = level ; <nl> - if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF ) <nl> + if ((( int32_t ) GET_CACHE ( re , & s -> gb )) <= ( int32_t ) 0xBFFFFFFF || i >= 64 ) <nl> break ; <nl> UPDATE_CACHE ( re , & s -> gb ); <nl> }
static AVFilterContext * create_filter ( AVFilterGraph * ctx , int index , <nl> AVFilter * filt ; <nl> char inst_name [ 30 ]; <nl>  <nl> - snprintf ( inst_name , sizeof ( inst_name ), " Parsed filter % d ", index ); <nl> + snprintf ( inst_name , sizeof ( inst_name ), " Filter % d % s ", index , filt_name ); <nl>  <nl> filt = avfilter_get_by_name ( filt_name ); <nl> 
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * buf ) <nl>  <nl> merge -> frame_requested = 0 ; <nl> draw_frame ( ctx , main_buf , alpha_buf ); <nl> - ff_filter_frame ( ctx -> outputs [ 0 ], avfilter_ref_buffer ( main_buf , ~ 0 )); <nl> + ff_filter_frame ( ctx -> outputs [ 0 ], main_buf ); <nl> avfilter_unref_buffer ( alpha_buf ); <nl> } <nl> return 0 ;
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * insamples ) <nl> } <nl> if ( showwaves -> buf_idx == showwaves -> w ) <nl> push_frame ( outlink ); <nl> + outpicref = showwaves -> outpicref ; <nl> } <nl>  <nl> avfilter_unref_buffer ( insamples );
static void fix_bitshift ( ShortenContext * s , int32_t * buffer ) <nl>  <nl> if ( s -> bitshift != 0 ) <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - buffer [ s -> nwrap + i ] <<= s -> bitshift ; <nl> + buffer [ i ] <<= s -> bitshift ; <nl> } <nl>  <nl> 
pp_mode * pp_get_mode_by_name_and_quality ( const char * name , int quality ) <nl> } <nl>  <nl> ppMode = av_malloc ( sizeof ( PPMode )); <nl> + if (! ppMode ) <nl> + return NULL ; <nl>  <nl> ppMode -> lumMode = 0 ; <nl> ppMode -> chromMode = 0 ;
static const struct { <nl>  <nl> static int webvtt_event_to_ass ( AVBPrint * buf , const char * p ) <nl> { <nl> - int i , again , skip = 0 ; <nl> + int i , again = 0 , skip = 0 ; <nl>  <nl> while (* p ) { <nl> 
static int parse_timestamp ( struct sbg_parser * p , <nl>  <nl> static int parse_fade ( struct sbg_parser * p , struct sbg_fade * fr ) <nl> { <nl> - struct sbg_fade f ; <nl> + struct sbg_fade f = { 0 }; <nl>  <nl> if ( lex_char ( p , '<')) <nl> f . in = SBG_FADE_SILENCE ;
matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , int size , <nl> memcpy ( pkt -> data , matroska -> tracks [ track ]-> encoding_settings , offset ); <nl> memcpy ( pkt -> data + offset , pkt_data , pkt_size ); <nl>  <nl> + if ( pkt_data != data ) <nl> + av_free ( pkt_data ); <nl> + <nl> if ( n == 0 ) <nl> pkt -> flags = is_keyframe ; <nl> pkt -> stream_index = stream_index ;
static int svq1_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> * pict = s -> current_picture . f ; <nl> + pict -> qscale_table = NULL ; <nl>  <nl> ff_MPV_frame_end ( s ); <nl> 
static int dxva2_map_frame ( AVHWFramesContext * ctx , AVFrame * dst , const AVFrame * <nl> } <nl>  <nl> map = av_mallocz ( sizeof (* map )); <nl> - if (! map ) <nl> + if (! map ) { <nl> + err = AVERROR ( ENOMEM ); <nl> goto fail ; <nl> + } <nl>  <nl> err = ff_hwframe_map_create ( src -> hw_frames_ctx , dst , src , <nl> dxva2_unmap_frame , map );
static void do_subtitle_out ( AVFormatContext * s , <nl>  <nl> if (! subtitle_out ) { <nl> subtitle_out = av_malloc ( subtitle_out_max_size ); <nl> + if (! subtitle_out ) { <nl> + av_log ( NULL , AV_LOG_FATAL , " Failed to allocate subtitle_out \ n "); <nl> + exit_program ( 1 ); <nl> + } <nl> } <nl>  <nl> /* Note : DVB subtitle need one packet to draw them and one other
static int decode_frame ( WmallDecodeCtx * s ) <nl> /* decode tile information */ <nl> if (( ret = decode_tilehdr ( s ))) { <nl> s -> packet_loss = 1 ; <nl> + av_frame_unref ( s -> frame ); <nl> return ret ; <nl> } <nl> 
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + avctx -> height * lsize ) { <nl> + if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + ( uint64_t ) avctx -> height * lsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " input buffer too small \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl>  <nl> + if ( avpkt -> size < 20 + avctx -> width * avctx -> height / 16 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Input packet too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( s -> format != format ) { <nl> if ( ret < 0 ) <nl> return ret ;
static int decodeTonalComponents ( GetBitContext * gb , tonal_component * pComponent <nl>  <nl> for ( k = 0 ; k < coded_components ; k ++) { <nl> sfIndx = get_bits ( gb , 6 ); <nl> + if ( component_count >= 64 ) <nl> + return AVERROR_INVALIDDATA ; <nl> pComponent [ component_count ]. pos = j * 64 + ( get_bits ( gb , 6 )); <nl> max_coded_values = SAMPLES_PER_FRAME - pComponent [ component_count ]. pos ; <nl> coded_values = coded_values_per_component + 1 ;
static void read_apic ( AVFormatContext * s , AVIOContext * pb , int taglen , char * tag <nl> goto fail ; <nl> } <nl>  <nl> - apic -> buf = av_buffer_alloc ( taglen ); <nl> + apic -> buf = av_buffer_alloc ( taglen + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + apic -> buf -> size -= FF_INPUT_BUFFER_PADDING_SIZE ; <nl> if (! apic -> buf || ! taglen || avio_read ( pb , apic -> buf -> data , taglen ) != taglen ) <nl> goto fail ; <nl> 
static int decode_header ( SnowContext * s ){ <nl> s -> block_max_depth = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( FFABS ( s -> qbias ) > 127 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " qbias % d is too large \ n ", s -> qbias ); <nl> + s -> qbias = 0 ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> return 0 ; <nl> }
struct ff_timecode { <nl> char * str ; ///< string following the hh : mm : ss [:;.] ff format <nl> int start ; ///< timecode frame start <nl> int drop ; ///< drop flag ( 1 if drop , else 0 ) <nl> - AVRational rate ; ///< Frame rate in rationnal form <nl> + AVRational rate ; ///< Frame rate in rational form <nl> }; <nl>  <nl> /**
static int sdp_probe ( AVProbeData * p1 ) <nl>  <nl> /* we look for a line beginning " c = IN IP " */ <nl> while ( p < p_end && * p != '\ 0 ') { <nl> - if ( p + sizeof (" c = IN IP ") - 1 < p_end && <nl> + if ( sizeof (" c = IN IP ") - 1 < p_end - p && <nl> av_strstart ( p , " c = IN IP ", NULL )) <nl> return AVPROBE_SCORE_EXTENSION ; <nl> 
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i ); <nl> + if ( end - dst < count ) { <nl> + av_log ( l -> avctx , AV_LOG_ERROR , " Too many zeros remaining .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> memset ( dst , 0 , count ); <nl> l -> zeros_rem -= count ; <nl> dst += count ;
static int libopenjpeg_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> case AV_PIX_FMT_GBRP14 : <nl> case AV_PIX_FMT_GBRP16 : <nl> gbrframe = av_frame_alloc (); <nl> + if (! gbrframe ) <nl> + return AVERROR ( ENOMEM ); <nl> av_frame_ref ( gbrframe , frame ); <nl> gbrframe -> data [ 0 ] = frame -> data [ 2 ]; // swap to be rgb <nl> gbrframe -> data [ 1 ] = frame -> data [ 0 ];
static int compat_decode ( AVCodecContext * avctx , AVFrame * frame , <nl> int * got_frame , AVPacket * pkt ) <nl> { <nl> AVCodecInternal * avci = avctx -> internal ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> av_assert0 ( avci -> compat_decode_consumed == 0 ); <nl> 
static int mxg_update_cache ( AVFormatContext * s , unsigned int cache_size ) <nl> /* reallocate internal buffer */ <nl> if ( current_pos > current_pos + cache_size ) <nl> return AVERROR ( ENOMEM ); <nl> - if ( mxg -> soi_ptr ) soi_pos = mxg -> soi_ptr - mxg -> buffer ; <nl> + soi_pos = mxg -> soi_ptr - mxg -> buffer ; <nl> mxg -> buffer = av_fast_realloc ( mxg -> buffer , & mxg -> buffer_size , <nl> current_pos + cache_size + <nl> FF_INPUT_BUFFER_PADDING_SIZE );
static int decode_frame ( WmallDecodeCtx * s ) <nl>  <nl> /* decode all subframes */ <nl> while (! s -> parsed_all_subframes ) { <nl> + int decoded_samples = s -> channel [ 0 ]. decoded_samples ; <nl> if ( decode_subframe ( s ) < 0 ) { <nl> s -> packet_loss = 1 ; <nl> + if ( s -> frame -> nb_samples ) <nl> + s -> frame -> nb_samples = decoded_samples ; <nl> return 0 ; <nl> } <nl> }
static int FUNC ( extension_data )( CodedBitstreamContext * ctx , RWContext * rw , <nl> BitstreamContext start ; <nl> uint8_t bit ; <nl> start = * rw ; <nl> - for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++); <nl> + for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++) <nl> + bitstream_skip ( rw , 1 ); <nl> current -> bit_length = k ; <nl> if ( k > 0 ) { <nl> * rw = start ;
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> if (! s -> context_initialized ) { <nl> if ( ff_msmpeg4_decode_init ( avctx ) < 0 || ff_vc1_decode_init_alloc_tables ( v ) < 0 ) <nl> - return - 1 ; <nl> + goto err ; <nl>  <nl> s -> low_delay = ! avctx -> has_b_frames || v -> res_sprite ; <nl> 
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl> int offset = get_bits_count (& gb ) + 7 >> 3 ; <nl> uint8_t * buf ; <nl>  <nl> - if (( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> + if ( watermark_height > 0 && <nl> + ( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> return - 1 ; <nl>  <nl> buf = av_malloc ( buf_len );
static char * choose_pix_fmts ( OutputStream * ost ) <nl> } <nl> if ( ost -> st -> codec -> pix_fmt != PIX_FMT_NONE ) { <nl> return av_strdup ( av_get_pix_fmt_name ( choose_pixel_fmt ( ost -> st , ost -> enc , ost -> st -> codec -> pix_fmt ))); <nl> - } else if ( ost -> enc -> pix_fmts ) { <nl> + } else if ( ost -> enc && ost -> enc -> pix_fmts ) { <nl> const enum PixelFormat * p ; <nl> AVIOContext * s = NULL ; <nl> uint8_t * ret ;
typedef struct { <nl>  <nl> static const AVOption options [] = { <nl> { " oggpagesize ", " Set preferred Ogg page size .", <nl> - offsetof ( OGGContext , pref_size ), FF_OPT_TYPE_INT , 0 , 0 , MAX_PAGE_SIZE , AV_OPT_FLAG_ENCODING_PARAM }, <nl> + offsetof ( OGGContext , pref_size ), FF_OPT_TYPE_INT , {. dbl = 0 }, 0 , MAX_PAGE_SIZE , AV_OPT_FLAG_ENCODING_PARAM }, <nl> { NULL }, <nl> }; <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ){ <nl>  <nl> } else { <nl> if ( s0 -> current_picture_ptr -> frame_num != h -> frame_num ) { <nl> + ff_thread_report_progress (( AVFrame *) s0 -> current_picture_ptr , INT_MAX , <nl> + s0 -> picture_structure == PICT_BOTTOM_FIELD ); <nl> /* <nl> * This and previous field had <nl> * different frame_nums . Consider this field first in
static int h264_mp4toannexb_filter ( AVBitStreamFilterContext * bsfc , <nl> buf += ctx -> length_size ; <nl> unit_type = * buf & 0x1f ; <nl>  <nl> - if ( buf + nal_size > buf_end || nal_size < 0 ) <nl> + if ( nal_size > buf_end - buf || nal_size < 0 ) <nl> goto fail ; <nl>  <nl> if ( unit_type == 7 )
static int find_and_decode_index ( NUTContext * nut ) <nl> has_keyframe [ n ++] = flag ; <nl> has_keyframe [ n ++] = ! flag ; <nl> } else { <nl> + if ( x <= 1 ) { <nl> + av_log ( s , AV_LOG_ERROR , " index : x %" PRIu64 " is invalid \ n ", x ); <nl> + goto fail ; <nl> + } <nl> while ( x != 1 ) { <nl> if ( n >= syncpoint_count + 1 ) { <nl> av_log ( s , AV_LOG_ERROR , " index overflow B \ n ");
void avcodec_align_dimensions2 ( AVCodecContext * s , int * width , int * height , <nl> case AV_PIX_FMT_YUV422P9BE : <nl> case AV_PIX_FMT_YUV422P10LE : <nl> case AV_PIX_FMT_YUV422P10BE : <nl> + case AV_PIX_FMT_YUVA422P10LE : <nl> + case AV_PIX_FMT_YUVA422P10BE : <nl> case AV_PIX_FMT_YUV444P9LE : <nl> case AV_PIX_FMT_YUV444P9BE : <nl> case AV_PIX_FMT_YUV444P10LE : <nl> case AV_PIX_FMT_YUV444P10BE : <nl> + case AV_PIX_FMT_YUVA444P10LE : <nl> + case AV_PIX_FMT_YUVA444P10BE : <nl> case AV_PIX_FMT_GBRP9LE : <nl> case AV_PIX_FMT_GBRP9BE : <nl> case AV_PIX_FMT_GBRP10LE :
static int mpc7_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> bits = av_malloc ((( buf_size - 1 ) & ~ 3 ) + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! bits ) <nl> + return AVERROR ( ENOMEM ); <nl> c -> dsp . bswap_buf (( uint32_t *) bits , ( const uint32_t *)( buf + 4 ), ( buf_size - 4 ) >> 2 ); <nl> init_get_bits (& gb , bits , ( buf_size - 4 )* 8 ); <nl> skip_bits_long (& gb , buf [ 0 ]);
av_cold void ff_msmpeg4_encode_init ( MpegEncContext * s ) <nl>  <nl> for ( i = 0 ; i < NB_RL_TABLES ; i ++){ <nl> int level ; <nl> - for ( level = 0 ; level <= MAX_LEVEL ; level ++){ <nl> + for ( level = 1 ; level <= MAX_LEVEL ; level ++) { <nl> int run ; <nl> for ( run = 0 ; run <= MAX_RUN ; run ++){ <nl> int last ;
matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , int size , <nl> pkt = av_mallocz ( sizeof ( AVPacket )); <nl> /* XXX : prevent data copy ... */ <nl> if ( av_new_packet ( pkt , pkt_size + offset ) < 0 ) { <nl> + av_free ( pkt ); <nl> res = AVERROR ( ENOMEM ); <nl> n = laces - 1 ; <nl> break ;
static int Stagefright_decode_frame ( AVCodecContext * avctx , void * data , <nl> AVFrame * ret_frame ; <nl>  <nl> if (! s -> thread_started ) { <nl> - pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx ); <nl> + if ( pthread_create (& s -> decode_thread_id , NULL , & decode_thread , avctx )) <nl> + return AVERROR ( ENOMEM ); <nl> s -> thread_started = true ; <nl> } <nl> 
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl> vseq -> seq_fields . bits . direct_8x8_inference_flag = 1 ; <nl> vseq -> seq_fields . bits . log2_max_frame_num_minus4 = 4 ; <nl> vseq -> seq_fields . bits . pic_order_cnt_type = 0 ; <nl> + vseq -> seq_fields . bits . log2_max_pic_order_cnt_lsb_minus4 = <nl> + av_clip ( av_log2 ( avctx -> max_b_frames + 1 ) - 2 , 0 , 12 ); <nl>  <nl> if ( avctx -> width != ctx -> surface_width || <nl> avctx -> height != ctx -> surface_height ) {
static av_cold int vc1_decode_init ( AVCodecContext * avctx ) <nl> avctx -> idct_algo = FF_IDCT_WMV2 ; <nl> } <nl>  <nl> - if ( ff_h263_decode_init ( avctx ) < 0 ) <nl> + if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) <nl> return - 1 ; <nl> if ( vc1_init_common ( v ) < 0 ) return - 1 ; <nl> - // only for ff_msmp4_mb_i_table <nl> - if ( ff_msmpeg4_decode_init ( avctx ) < 0 ) return - 1 ; <nl>  <nl> avctx -> coded_width = avctx -> width ; <nl> avctx -> coded_height = avctx -> height ;
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> if (! comp -> i_data ) <nl> return AVERROR ( ENOMEM ); <nl> } <nl> - comp -> reslevel = av_malloc_array ( codsty -> nreslevels , sizeof (* comp -> reslevel )); <nl> + comp -> reslevel = av_calloc ( codsty -> nreslevels , sizeof (* comp -> reslevel )); <nl> if (! comp -> reslevel ) <nl> return AVERROR ( ENOMEM ); <nl> /* LOOP on resolution levels */
static void ac3_bit_alloc_calc_bap_c ( int16_t * mask , int16_t * psd , <nl> static void ac3_update_bap_counts_c ( uint16_t mant_cnt [ 16 ], uint8_t * bap , <nl> int len ) <nl> { <nl> - while ( len -- >= 0 ) <nl> + while ( len -- > 0 ) <nl> mant_cnt [ bap [ len ]]++; <nl> } <nl> 
static int msf_read_header ( AVFormatContext * s ) <nl> st -> codec -> codec_type = AVMEDIA_TYPE_AUDIO ; <nl> codec = avio_rb32 ( s -> pb ); <nl> st -> codec -> channels = avio_rb32 ( s -> pb ); <nl> - if ( st -> codec -> channels <= 0 ) <nl> + if ( st -> codec -> channels <= 0 || st -> codec -> channels >= INT_MAX / 1024 ) <nl> return AVERROR_INVALIDDATA ; <nl> size = avio_rb32 ( s -> pb ); <nl> st -> codec -> sample_rate = avio_rb32 ( s -> pb );
static int sdp_parse_fmtp_config_h264 ( AVStream * stream , <nl> } <nl> } else if (! strcmp ( attr , " sprop - parameter - sets ")) { <nl> codec -> extradata_size = 0 ; <nl> - codec -> extradata = NULL ; <nl> + av_freep (& codec -> extradata ); <nl>  <nl> while (* value ) { <nl> char base64packet [ 1024 ];
static int http_prepare_data ( HTTPContext * c ) <nl>  <nl> av_freep (& c -> pb_buffer ); <nl> len = avio_close_dyn_buf ( ctx -> pb , & c -> pb_buffer ); <nl> + ctx -> pb = NULL ; <nl> c -> cur_frame_bytes = len ; <nl> c -> buffer_ptr = c -> pb_buffer ; <nl> c -> buffer_end = c -> pb_buffer + len ;
int ff_mjpeg_find_marker ( MJpegDecodeContext * s , <nl> put_bits (& pb , 8 , x ); <nl> if ( x == 0xFF ) { <nl> x = src [ b ++]; <nl> + if ( x & 0x80 ) { <nl> + av_log ( s -> avctx , AV_LOG_WARNING , " Invalid escape sequence \ n "); <nl> + x &= 0x7f ; <nl> + } <nl> put_bits (& pb , 7 , x ); <nl> bit_count --; <nl> }
int ff_dca_lbr_parse ( DCALbrDecoder * s , uint8_t * data , DCAExssAsset * asset ) <nl> LBRChunk hr_grid [ DCA_LBR_CHANNELS / 2 ]; <nl> LBRChunk ts1 [ DCA_LBR_CHANNELS / 2 ]; <nl> LBRChunk ts2 [ DCA_LBR_CHANNELS / 2 ]; <nl> - } chunk = { 0 }; <nl> + } chunk = { { 0 } }; <nl>  <nl> GetByteContext gb ; <nl> 
static int decode_audio_specific_config ( AACContext * ac , <nl> * <nl> * @ return Returns a 32 - bit pseudorandom integer <nl> */ <nl> - static av_always_inline int lcg_random ( int previous_val ) <nl> + static av_always_inline int lcg_random ( unsigned previous_val ) <nl> { <nl> return previous_val * 1664525 + 1013904223 ; <nl> }
typedef struct ScalingList { <nl> } ScalingList ; <nl>  <nl> typedef struct HEVCSPS { <nl> - int vps_id ; <nl> + unsigned vps_id ; <nl> int chroma_format_idc ; <nl> uint8_t separate_colour_plane_flag ; <nl>  <nl> typedef struct HEVCSPS { <nl> } HEVCSPS ; <nl>  <nl> typedef struct HEVCPPS { <nl> - int sps_id ; ///< seq_parameter_set_id <nl> + unsigned sps_id ; ///< seq_parameter_set_id <nl>  <nl> uint8_t sign_data_hiding_flag ; <nl> 
static int mpeg_mux_init ( AVFormatContext * ctx ) <nl> if (! s -> mux_rate ) { <nl> /* we increase slightly the bitrate to take into account the <nl> headers . XXX : compute it exactly */ <nl> - bitrate += bitrate * 5 / 100 ; <nl> + bitrate += bitrate / 20 ; <nl> bitrate += 10000 ; <nl> s -> mux_rate = ( bitrate + ( 8 * 50 ) - 1 ) / ( 8 * 50 ); <nl> }
static int rtsp_read_packet ( AVFormatContext * s , <nl> case RTSP_PROTOCOL_RTP_UDP : <nl> case RTSP_PROTOCOL_RTP_UDP_MULTICAST : <nl> len = udp_read_packet ( s , & rtsp_st , buf , sizeof ( buf )); <nl> - if ( rtsp_st -> rtp_ctx ) <nl> + if ( len >= 0 && rtsp_st -> rtp_ctx ) <nl> rtp_check_and_send_back_rr ( rtsp_st -> rtp_ctx , len ); <nl> break ; <nl> }
int32_t ff_mlp_pack_output ( int32_t lossless_check_data , <nl> ( 1U << output_shift [ mat_ch ]); <nl> lossless_check_data ^= ( sample & 0xffffff ) << mat_ch ; <nl> if ( is32 ) <nl> - * data_32 ++ = sample * 256 ; <nl> + * data_32 ++ = sample * 256U ; <nl> else <nl> * data_16 ++ = sample >> 8 ; <nl> }
resync : <nl> } <nl> ast -> frame_offset += get_duration ( ast , pkt -> size ); <nl> } <nl> - ast -> remaining -= size ; <nl> + ast -> remaining -= err ; <nl> if (! ast -> remaining ){ <nl> avi -> stream_index = - 1 ; <nl> ast -> packet_size = 0 ;
static av_always_inline int coeff_abs_level_remaining_decode ( HEVCContext * s , int <nl> } else { <nl> int prefix_minus3 = prefix - 3 ; <nl>  <nl> - if ( prefix == CABAC_MAX_BIN ) { <nl> + if ( prefix == CABAC_MAX_BIN || prefix_minus3 + rc_rice_param >= 31 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " CABAC_MAX_BIN : % d \ n ", prefix ); <nl> return 0 ; <nl> }
void ff_msmpeg4_encode_mb ( MpegEncContext * s , <nl> static void msmpeg4_encode_dc ( MpegEncContext * s , int level , int n , int * dir_ptr ) <nl> { <nl> int sign , code ; <nl> - int pred , extquant ; <nl> + int pred , av_uninit ( extquant ); <nl> int extrabits = 0 ; <nl>  <nl> int16_t * dc_val ;
AVCodec mjpeg_decoder = { <nl> ff_mjpeg_decode_frame , <nl> CODEC_CAP_DR1 , <nl> NULL , <nl> - . max_lowres = 8 , <nl> + . max_lowres = 4 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" MJPEG ( Motion JPEG )"), <nl> }; <nl> 
SwsFunc ff_yuv2rgb_get_func_ptr ( SwsContext * c ) <nl> } <nl>  <nl> static void fill_table ( uint8_t * table [ 256 + 2 * YUVRGB_TABLE_HEADROOM ], const int elemsize , <nl> - const int inc , void * y_tab ) <nl> + const int64_t inc , void * y_tab ) <nl> { <nl> int i ; <nl> uint8_t * y_table = y_tab ;
void av_image_copy ( uint8_t * dst_data [ 4 ], int dst_linesizes [ 4 ], <nl> for ( i = 0 ; i < planes_nb ; i ++) { <nl> int h = height ; <nl> int bwidth = av_image_get_linesize ( pix_fmt , width , i ); <nl> + if ( bwidth < 0 ) { <nl> + av_log ( 0 , AV_LOG_ERROR , " av_image_get_linesize failed \ n "); <nl> + return ; <nl> + } <nl> if ( i == 1 || i == 2 ) { <nl> h = -((- height )>> desc -> log2_chroma_h ); <nl> }
static void start_frame ( AVFilterLink * inlink , AVFilterBufferRef * inpicref ) <nl> ) <nl> break ; <nl> } <nl> - pad -> needs_copy = plane < 4 && outpicref -> data [ plane ]; <nl> + pad -> needs_copy = plane < 4 && outpicref -> data [ plane ] || !( outpicref -> perms & AV_PERM_WRITE ); <nl> if ( pad -> needs_copy ){ <nl> av_log ( inlink -> dst , AV_LOG_DEBUG , " Direct padding impossible allocating new frame \ n "); <nl> avfilter_unref_buffer ( outpicref );
int ff_hevc_decode_nal_pps ( HEVCContext * s ) <nl> int pps_range_extensions_flag = get_bits1 ( gb ); <nl> /* int pps_extension_7bits = */ get_bits ( gb , 7 ); <nl> if ( sps -> ptl . general_ptl . profile_idc == FF_PROFILE_HEVC_REXT && pps_range_extensions_flag ) { <nl> - pps_range_extensions ( s , pps , sps ); <nl> + if (( ret = pps_range_extensions ( s , pps , sps )) < 0 ) <nl> + goto err ; <nl> } <nl> } <nl> 
static int swf_probe ( AVProbeData * p ) <nl> && AV_RB24 ( p -> buf ) != AV_RB24 (" FWS ")) <nl> return 0 ; <nl>  <nl> + if ( AV_RB24 ( p -> buf ) == AV_RB24 (" CWS ") <nl> + && p -> buf [ 3 ] <= 20 ) <nl> + return AVPROBE_SCORE_MAX / 4 + 1 ; <nl> + <nl> init_get_bits8 (& gb , p -> buf + 3 , p -> buf_size - 3 ); <nl>  <nl> skip_bits (& gb , 40 );
static inline int put_bits_count ( PutBitContext * s ) <nl> static inline void flush_put_bits ( PutBitContext * s ) <nl> { <nl> # ifndef BITSTREAM_WRITER_LE <nl> - s -> bit_buf <<= s -> bit_left ; <nl> + if ( s -> bit_left < 32 ) <nl> + s -> bit_buf <<= s -> bit_left ; <nl> # endif <nl> while ( s -> bit_left < 32 ) { <nl> /* XXX : should test end of buffer */
void av_packet_free ( AVPacket ** pkt ) <nl> static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> { <nl> int ret ; <nl> - if (( unsigned ) size >= ( unsigned ) size + AV_INPUT_BUFFER_PADDING_SIZE ) <nl> + if ( size < 0 || size >= INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> ret = av_buffer_realloc ( buf , size + AV_INPUT_BUFFER_PADDING_SIZE );
static int decode_channel_residues ( WmallDecodeCtx * s , int ch , int tile_size ) <nl> residue = quo ; <nl> else { <nl> rem_bits = av_ceil_log2 ( ave_mean ); <nl> - rem = rem_bits ? get_bits (& s -> gb , rem_bits ) : 0 ; <nl> + rem = rem_bits ? get_bits_long (& s -> gb , rem_bits ) : 0 ; <nl> residue = ( quo << rem_bits ) + rem ; <nl> } <nl> 
static int encode_thread ( AVCodecContext * c , void * arg ){ <nl> int mb_x , mb_y , pdif = 0 ; <nl> int chr_h = 16 >> s -> chroma_y_shift ; <nl> int i , j ; <nl> - MpegEncContext best_s , backup_s ; <nl> + MpegEncContext best_s = { 0 }, backup_s ; <nl> uint8_t bit_buf [ 2 ][ MAX_MB_BYTES ]; <nl> uint8_t bit_buf2 [ 2 ][ MAX_MB_BYTES ]; <nl> uint8_t bit_buf_tex [ 2 ][ MAX_MB_BYTES ];
static av_cold int X264_close ( AVCodecContext * avctx ) <nl> if ( x4 -> enc ) <nl> x264_encoder_close ( x4 -> enc ); <nl>  <nl> - av_free ( x4 -> preset ); <nl> - av_free ( x4 -> tune ); <nl> - av_free ( x4 -> profile ); <nl> - av_free ( x4 -> level ); <nl> - av_free ( x4 -> stats ); <nl> - av_free ( x4 -> weightp ); <nl> - av_free ( x4 -> x264opts ); <nl> - <nl> return 0 ; <nl> } <nl> 
int av_write_frame ( AVFormatContext * s , AVPacket * pkt ) <nl> return 1 ; <nl> } <nl>  <nl> + ret = do_packet_auto_bsf ( s , pkt ); <nl> + if ( ret <= 0 ) <nl> + return ret ; <nl> + <nl> # if FF_API_COMPUTE_PKT_FIELDS2 && FF_API_LAVF_AVCTX <nl> ret = compute_muxer_pkt_fields ( s , s -> streams [ pkt -> stream_index ], pkt ); <nl> 
static inline int decode_subframe ( FLACContext * s , int channel ) <nl> if ( wasted ) { <nl> int i ; <nl> for ( i = 0 ; i < s -> blocksize ; i ++) <nl> - decoded [ i ] <<= wasted ; <nl> + decoded [ i ] = ( unsigned ) decoded [ i ] << wasted ; <nl> } <nl>  <nl> return 0 ;
static int huf_uncompress ( GetByteContext * gb , <nl>  <nl> fail : <nl> for ( i = 0 ; i < HUF_DECSIZE ; i ++) { <nl> - if ( hdec [ i ]. p ) <nl> + if ( hdec ) <nl> av_freep (& hdec [ i ]. p ); <nl> } <nl> 
static int parse_bintree ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_ERROR , " SkipCell procedure not implemented yet !\ n "); <nl>  <nl> CHECK_CELL <nl> + if (! curr_cell . mv_ptr ) <nl> + return AVERROR_INVALIDDATA ; <nl> copy_cell ( ctx , plane , & curr_cell ); <nl> return 0 ; <nl> }
static void fix_coding_method_array ( int sb , int channels , sb_int8_array coding_ <nl> run = 1 ; <nl> case_val = 8 ; <nl> } else { <nl> - switch ( switchtable [ coding_method [ ch ][ sb ][ j ]]) { <nl> + switch ( switchtable [ coding_method [ ch ][ sb ][ j ]- 8 ]) { <nl> case 0 : run = 10 ; case_val = 10 ; break ; <nl> case 1 : run = 1 ; case_val = 16 ; break ; <nl> case 2 : run = 5 ; case_val = 24 ; break ;
static int cin_read_frame_header ( CinDemuxContext * cin , AVIOContext * pb ) { <nl>  <nl> if ( avio_rl32 ( pb ) != 0xAA55AA55 ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( hdr -> video_frame_size < 0 || hdr -> audio_frame_size < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> return 0 ; <nl> }
static inline void mv_pred_direct ( AVSContext * h , cavs_vector * pmv_fw , <nl> cavs_vector * col_mv ) <nl> { <nl> cavs_vector * pmv_bw = pmv_fw + MV_BWD_OFFS ; <nl> - int den = h -> direct_den [ col_mv -> ref ]; <nl> + unsigned den = h -> direct_den [ col_mv -> ref ]; <nl> int m = FF_SIGNBIT ( col_mv -> x ); <nl>  <nl> pmv_fw -> dist = h -> dist [ 1 ];
static int vp9_raw_reorder_make_output ( AVBSFContext * bsf , <nl> "(%" PRId64 ") from slot % d .\ n ", <nl> frame -> sequence , frame -> pts , s ); <nl>  <nl> - frame -> packet = av_packet_alloc (); <nl> - if (! frame -> packet ) <nl> - return AVERROR ( ENOMEM ); <nl> - <nl> err = av_new_packet ( out , 2 ); <nl> if ( err < 0 ) <nl> return err ;
static int vqf_read_seek ( AVFormatContext * s , <nl> { <nl> VqfContext * c = s -> priv_data ; <nl> AVStream * st ; <nl> - int ret ; <nl> + int64_t ret ; <nl> int64_t pos ; <nl>  <nl> st = s -> streams [ stream_index ];
static void jpeg2000_dec_cleanup ( Jpeg2000DecoderContext * s ) <nl> } <nl> } <nl> av_freep (& s -> tile ); <nl> + memset ( s -> codsty , 0 , sizeof ( s -> codsty )); <nl> + memset ( s -> qntsty , 0 , sizeof ( s -> qntsty )); <nl> s -> numXtiles = s -> numYtiles = 0 ; <nl> } <nl> 
static int fdk_aac_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size , 0 ); <nl> + err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size / sizeof ( INT_PCM ), 0 ); <nl> if ( err == AAC_DEC_NOT_ENOUGH_BITS ) { <nl> ret = avpkt -> size - valid ; <nl> goto end ;
static void png_handle_row ( PNGDecContext * s ) <nl> } <nl> s -> y ++; <nl> if ( s -> y == s -> height ) { <nl> + memset ( s -> last_row , 0 , s -> row_size ); <nl> for (;;) { <nl> if ( s -> pass == NB_PASSES - 1 ) { <nl> s -> state |= PNG_ALLIMAGE ;
static int skeleton_header ( AVFormatContext * s , int idx ) <nl> start_num = AV_RL64 ( buf + 12 ); <nl> start_den = AV_RL64 ( buf + 20 ); <nl>  <nl> - if ( start_den ) { <nl> + if ( start_den > 0 && start_num > 0 ) { <nl> int base_den ; <nl> av_reduce (& start_time , & base_den , start_num , start_den , INT_MAX ); <nl> avpriv_set_pts_info ( st , 64 , 1 , base_den );
static int raw_decode ( AVCodecContext * avctx , <nl> AVFrame * frame = ( AVFrame *) data ; <nl> AVPicture * picture = ( AVPicture *) data ; <nl>  <nl> + frame -> pict_type = avctx -> coded_frame -> pict_type ; <nl> frame -> interlaced_frame = avctx -> coded_frame -> interlaced_frame ; <nl> frame -> top_field_first = avctx -> coded_frame -> top_field_first ; <nl> frame -> reordered_opaque = avctx -> reordered_opaque ;
static int film_read_header ( AVFormatContext * s ) <nl> film -> audio_samplerate = AV_RB16 (& scratch [ 24 ]); <nl> film -> audio_channels = scratch [ 21 ]; <nl> film -> audio_bits = scratch [ 22 ]; <nl> - if ( scratch [ 23 ] == 2 ) <nl> + if ( scratch [ 23 ] == 2 && film -> audio_channels > 0 ) <nl> film -> audio_type = AV_CODEC_ID_ADPCM_ADX ; <nl> else if ( film -> audio_channels > 0 ) { <nl> if ( film -> audio_bits == 8 )
static int read_header ( ShortenContext * s ) <nl> s -> channels = get_uint ( s , CHANSIZE ); <nl> if ( s -> channels <= 0 || s -> channels > MAX_CHANNELS ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + s -> channels = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> s -> avctx -> channels = s -> channels ;
static int cmv_decode_frame ( AVCodecContext * avctx , <nl> CmvContext * s = avctx -> priv_data ; <nl> const uint8_t * buf_end = buf + buf_size ; <nl>  <nl> + if ( buf_end - buf < EA_PREAMBLE_SIZE ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( AV_RL32 ( buf )== MVIh_TAG || AV_RB32 ( buf )== MVIh_TAG ) { <nl> cmv_process_header ( s , buf + EA_PREAMBLE_SIZE , buf_end ); <nl> return buf_size ;
typedef struct TargaContext { <nl> } TargaContext ; <nl>  <nl> # define CHECK_BUFFER_SIZE ( buf , buf_end , needed , where ) \ <nl> - if ( buf + needed > buf_end ){ \ <nl> + if ( needed > buf_end - buf ){ \ <nl> av_log ( avctx , AV_LOG_ERROR , " Problem : unexpected end of data while reading " where "\ n "); \ <nl> return - 1 ; \ <nl> } \
static int ape_tag_read_field ( AVFormatContext * s ) <nl> if (! value ) <nl> return AVERROR ( ENOMEM ); <nl> c = avio_read ( pb , value , size ); <nl> + if ( c < 0 ) <nl> + return c ; <nl> value [ c ] = 0 ; <nl> av_dict_set (& s -> metadata , key , value , AV_DICT_DONT_STRDUP_VAL ); <nl> }
void decode_mb_mode ( VP8Context * s , VP8Macroblock * mb , int mb_x , int mb_y , uint8_ <nl>  <nl> if ( s -> segmentation . update_map ) <nl> * segment = vp8_rac_get_tree ( c , vp8_segmentid_tree , s -> prob -> segmentid ); <nl> - else <nl> + else if ( s -> segmentation . enabled ) <nl> * segment = ref ? * ref : * segment ; <nl> s -> segment = * segment ; <nl> 
static int dca_exss_parse_asset_header ( DCAContext * s ) <nl> { <nl> int header_pos = get_bits_count (& s -> gb ); <nl> int header_size ; <nl> - int channels ; <nl> + int channels = 0 ; <nl> int embedded_stereo = 0 ; <nl> int embedded_6ch = 0 ; <nl> int drc_code_present ; <nl> - int extensions_mask ; <nl> + int av_uninit ( extensions_mask ); <nl> int i , j ; <nl>  <nl> if ( get_bits_left (& s -> gb ) < 16 )
static int parse_picture_segment ( AVCodecContext * avctx , <nl> ctx -> pictures [ picture_id ]. w = width ; <nl> ctx -> pictures [ picture_id ]. h = height ; <nl>  <nl> - av_fast_malloc (& ctx -> pictures [ picture_id ]. rle , & ctx -> pictures [ picture_id ]. rle_buffer_size , rle_bitmap_len ); <nl> + av_fast_padded_malloc (& ctx -> pictures [ picture_id ]. rle , & ctx -> pictures [ picture_id ]. rle_buffer_size , rle_bitmap_len ); <nl>  <nl> if (! ctx -> pictures [ picture_id ]. rle ) <nl> return - 1 ;
static int mkv_parse_video_projection ( AVStream * st , const MatroskaTrack * track ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> break ; <nl> + case MATROSKA_VIDEO_PROJECTION_TYPE_RECTANGULAR : <nl> + /* No Spherical metadata */ <nl> + return 0 ; <nl> default : <nl> av_log ( NULL , AV_LOG_WARNING , <nl> " Unknown spherical metadata type %" PRIu64 "\ n ",
static int decode_frame ( AVCodecContext * avctx , <nl> decoded = loco_decode_plane ( l , p -> data [ 0 ] + p -> linesize [ 0 ]*( avctx -> height - 1 ) + 3 , avctx -> width , avctx -> height , <nl> - p -> linesize [ 0 ], buf , buf_size , 4 ); <nl> break ; <nl> + default : <nl> + av_assert0 ( 0 ); <nl> } <nl>  <nl> if ( decoded < 0 || decoded > buf_size )
static int film_probe ( AVProbeData * p ) <nl> if ( AV_RB32 (& p -> buf [ 0 ]) != FILM_TAG ) <nl> return 0 ; <nl>  <nl> + if ( AV_RB32 (& p -> buf [ 16 ]) != FDSC_TAG ) <nl> + return 0 ; <nl> + <nl> return AVPROBE_SCORE_MAX ; <nl> } <nl> 
static int rv20_decode_picture_header ( MpegEncContext * s ) <nl> if ( s -> avctx -> debug & FF_DEBUG_PICT_INFO ){ <nl> av_log ( s -> avctx , AV_LOG_DEBUG , " F % d /% d \ n ", f , rpr_bits ); <nl> } <nl> - } <nl> + } else if ( av_image_check_size ( s -> width , s -> height , 0 , s -> avctx ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl>  <nl> mb_pos = ff_h263_decode_mba ( s ); <nl> 
end : <nl>  <nl> return ret ; <nl> free_and_end : <nl> - if ( avctx -> codec && <nl> + if ( avctx -> codec && avctx -> codec -> close && <nl> ( codec_init_ok || <nl> ( avctx -> codec -> caps_internal & FF_CODEC_CAP_INIT_CLEANUP ))) <nl> avctx -> codec -> close ( avctx );
static int expand_rle_row ( SgiState * s , uint8_t * out_buf , <nl> } <nl>  <nl> /* Check for buffer overflow . */ <nl> - if ( out_buf + pixelstride * ( count - 1 ) >= out_end ) <nl> + if ( out_end - out_buf <= pixelstride * ( count - 1 )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if ( pixel & 0x80 ) {
static int ff_asf_parse_packet ( AVFormatContext * s , AVIOContext * pb , AVPacket * pk <nl> // printf (" packet % d % d \ n ", asf_st -> pkt . size , asf -> packet_frag_size ); <nl> asf_st -> pkt . size = 0 ; <nl> asf_st -> pkt . data = 0 ; <nl> + asf_st -> pkt . side_data_elems = 0 ; <nl> + asf_st -> pkt . side_data = NULL ; <nl> break ; // packet completed <nl> } <nl> }
static int pcm_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR ( EINVAL ); <nl> } <nl>  <nl> + if ( avctx -> channels == 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> n = avctx -> channels * sample_size ; <nl>  <nl> if ( n && buf_size % n ) {
static int mov_read_close ( AVFormatContext * s ) <nl> for ( i = 0 ; i < mov -> total_streams ; i ++) <nl> mov_free_stream_context ( mov -> streams [ i ]); <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) <nl> - av_free ( s -> streams [ i ]); <nl> + av_freep (& s -> streams [ i ]); <nl> return 0 ; <nl> } <nl> 
static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , in <nl> if ( s -> extra_bits ){ <nl> S <<= s -> extra_bits ; <nl>  <nl> - if ( s -> got_extra_bits ){ <nl> + if ( s -> got_extra_bits && get_bits_left (& s -> gb_extra_bits ) >= s -> extra_bits ){ <nl> S |= get_bits (& s -> gb_extra_bits , s -> extra_bits ); <nl> * crc = * crc * 9 + ( S & 0xffff ) * 3 + (( unsigned ) S >> 16 ); <nl> }
static int sol_read_packet ( AVFormatContext * s , <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR ( EIO ); <nl> ret = av_get_packet ( s -> pb , pkt , MAX_SIZE ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> pkt -> stream_index = 0 ; <nl>  <nl> /* note : we need to modify the packet size here to handle the last
static int iff_read_header ( AVFormatContext * s ) <nl> break ; <nl>  <nl> case ID_CMAP : <nl> + if ( data_size < 3 || data_size > 768 || data_size % 3 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid CMAP chunk size % d \ n ", <nl> + data_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> st -> codec -> extradata_size = data_size ; <nl> st -> codec -> extradata = av_malloc ( data_size ); <nl> if (! st -> codec -> extradata )
rdt_new_context ( void ) <nl> { <nl> PayloadContext * rdt = av_mallocz ( sizeof ( PayloadContext )); <nl>  <nl> - avformat_open_input (& rdt -> rmctx , "", & ff_rdt_demuxer , NULL ); <nl> + int ret = avformat_open_input (& rdt -> rmctx , "", & ff_rdt_demuxer , NULL ); <nl> + if ( ret < 0 ) { <nl> + av_free ( rdt ); <nl> + return NULL ; <nl> + } <nl>  <nl> return rdt ; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> if (! fifo -> root . next ) { <nl> if (( ret = ff_request_frame ( outlink -> src -> inputs [ 0 ])) < 0 ) <nl> return ret ; <nl> + av_assert0 ( fifo -> root . next ); <nl> } <nl>  <nl> /* by doing this , we give ownership of the reference to the next filter ,
static int ape_tag_read_field ( AVFormatContext * s ) <nl> { <nl> AVIOContext * pb = s -> pb ; <nl> uint8_t key [ 1024 ], * value ; <nl> - uint32_t size , flags ; <nl> + int64_t size , flags ; <nl> int i , c ; <nl>  <nl> size = avio_rl32 ( pb ); /* field size */
static int transcode_init ( void ) <nl> ost -> filter -> filter -> inputs [ 0 ]-> sample_aspect_ratio ; <nl> codec -> pix_fmt = ost -> filter -> filter -> inputs [ 0 ]-> format ; <nl>  <nl> - if ( codec -> width != icodec -> width || <nl> + if (! icodec || <nl> + codec -> width != icodec -> width || <nl> codec -> height != icodec -> height || <nl> codec -> pix_fmt != icodec -> pix_fmt ) { <nl> codec -> bits_per_raw_sample = frame_bits_per_raw_sample ;
static void hScale16_c ( SwsContext * c , int16_t * _dst , int dstW , const uint8_t * _s <nl> for ( i = 0 ; i < dstW ; i ++) { <nl> int j ; <nl> int srcPos = filterPos [ i ]; <nl> - unsigned int val = 0 ; <nl> + int val = 0 ; <nl>  <nl> for ( j = 0 ; j < filterSize ; j ++) { <nl> val += src [ srcPos + j ] * filter [ filterSize * i + j ];
int ff_img_read_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> } <nl>  <nl> if ( codec -> codec_id == AV_CODEC_ID_NONE ) { <nl> - AVProbeData pd ; <nl> + AVProbeData pd = { 0 }; <nl> AVInputFormat * ifmt ; <nl> uint8_t header [ PROBE_BUF_MIN + AVPROBE_PADDING_SIZE ]; <nl> int ret ;
static int lrc_read_header ( AVFormatContext * s ) <nl> } <nl> ff_subtitles_queue_finalize ( s , & lrc -> q ); <nl> ff_metadata_conv_ctx ( s , NULL , ff_lrc_metadata_conv ); <nl> + av_bprint_finalize (& line , NULL ); <nl> return 0 ; <nl> } <nl> 
static int au_read_header ( AVFormatContext * s ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> + if ( channels == 0 || channels > 64 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid number of channels % d \ n ", channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( size >= 24 ) { <nl> /* skip unused data */ <nl> avio_skip ( pb , size - 24 );
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> + p -> key_frame = 1 ; <nl> * got_frame = 1 ; <nl>  <nl> return buf_size ;
static int mxf_read_primer_pack ( MXFContext * mxf ) <nl>  <nl> static int mxf_add_metadata_set ( MXFContext * mxf , void * metadata_set ) <nl> { <nl> + if ( mxf -> metadata_sets_count + 1 >= UINT_MAX / sizeof (* mxf -> metadata_sets )) <nl> + return AVERROR ( ENOMEM ); <nl> mxf -> metadata_sets = av_realloc ( mxf -> metadata_sets , ( mxf -> metadata_sets_count + 1 ) * sizeof (* mxf -> metadata_sets )); <nl> if (! mxf -> metadata_sets ) <nl> return - 1 ;
static int asf_write_header ( AVFormatContext * s ) <nl> * It is needed to use asf as a streamable format . */ <nl> if ( asf_write_header1 ( s , 0 , DATA_HEADER_SIZE ) < 0 ) { <nl> // av_free ( asf ); <nl> + av_freep (& asf -> index_ptr ); <nl> return - 1 ; <nl> } <nl> 
static void mpc8_parse_seektable ( AVFormatContext * s , int64_t off ) <nl> int i , t , seekd ; <nl> GetBitContext gb ; <nl>  <nl> + if ( s -> nb_streams <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " cannot parse stream table before stream header \ n "); <nl> + return ; <nl> + } <nl> + <nl> avio_seek ( s -> pb , off , SEEK_SET ); <nl> mpc8_get_chunk_header ( s -> pb , & tag , & size ); <nl> if ( tag != TAG_SEEKTABLE ){
static int dvbsub_decode ( AVCodecContext * avctx , <nl> break ; <nl> case DVBSUB_DISPLAYDEFINITION_SEGMENT : <nl> dvbsub_parse_display_definition_segment ( avctx , p , segment_length ); <nl> + break ; <nl> case DVBSUB_DISPLAY_SEGMENT : <nl> * data_size = dvbsub_display_end_segment ( avctx , p , segment_length , sub ); <nl> break ;
typedef struct MpegDemuxContext { <nl> static int mpegps_read_header ( AVFormatContext * s ) <nl> { <nl> MpegDemuxContext * m = s -> priv_data ; <nl> - char buffer [ 7 ]; <nl> + char buffer [ 7 ] = { 0 }; <nl> int64_t last_pos = avio_tell ( s -> pb ); <nl>  <nl> m -> header_state = 0xff ;
static int encode_mode ( CinepakEncContext * s , int h , AVPicture * scratch_pict , AVP <nl> int needs_extra_bit , should_write_temp ; <nl> unsigned char temp [ 64 ]; // 32 / 2 = 16 V4 blocks at 4 B each -> 64 B <nl> mb_info * mb ; <nl> - AVPicture sub_scratch , sub_last ; <nl> + AVPicture sub_scratch = {{ 0 }}, sub_last = {{ 0 }}; <nl>  <nl> // encode codebooks <nl> ////// MacOS vintage decoder compatibility dictates the presence of
static int flv_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( flv -> wrong_dts ) <nl> dts = AV_NOPTS_VALUE ; <nl> } <nl> - if ( type == 0 ) { <nl> + <nl> + if ( type == 0 && ! st -> codec -> extradata ) { <nl> if (( ret = flv_get_extradata ( s , st , size )) < 0 ) <nl> return ret ; <nl> if ( st -> codec -> codec_id == CODEC_ID_AAC ) {
av_cold int ffv1_common_init ( AVCodecContext * avctx ) <nl>  <nl> s -> picture . f = avcodec_alloc_frame (); <nl> s -> last_picture . f = av_frame_alloc (); <nl> + if (! s -> picture . f || ! s -> last_picture . f ) <nl> + return AVERROR ( ENOMEM ); <nl> ff_dsputil_init (& s -> dsp , avctx ); <nl>  <nl> s -> width = avctx -> width ;
static void write_section_data ( MpegTSContext * ts , MpegTSFilter * tss1 , <nl> } else <nl> crc_valid = 2 ; <nl> } <nl> - if ( crc_valid ) <nl> + if ( crc_valid ) { <nl> tss -> section_cb ( tss1 , tss -> section_buf , tss -> section_h_size ); <nl> + if ( crc_valid != 1 ) <nl> + tss -> last_ver = - 1 ; <nl> + } <nl> } <nl> } <nl> 
static int sap_write_header ( AVFormatContext * s ) <nl> freeaddrinfo ( ai ); <nl> } <nl>  <nl> - contexts = av_mallocz ( sizeof ( AVFormatContext *) * s -> nb_streams ); <nl> + contexts = av_mallocz_array ( s -> nb_streams , sizeof ( AVFormatContext *)); <nl> if (! contexts ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ;
static void search_for_quantizers_anmr ( AVCodecContext * avctx , AACEncContext * s , <nl> } <nl> while ( idx ) { <nl> sce -> sf_idx [ bandaddr [ idx ]] = minq + q0 ; <nl> - minq = paths [ idx ][ minq ]. prev ; <nl> + minq = FFMAX ( paths [ idx ][ minq ]. prev , 0 ); <nl> idx --; <nl> } <nl> // set the same quantizers inside window groups
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl>  <nl> if ( wc -> ch_offset + s -> stereo >= avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_WARNING , " Too many channels coded in a packet .\ n "); <nl> - return ( avctx -> err_recognition & AV_EF_EXPLODE ) ? AVERROR_INVALIDDATA : 0 ; <nl> + return (( avctx -> err_recognition & AV_EF_EXPLODE ) || ! wc -> ch_offset ) ? AVERROR_INVALIDDATA : 0 ; <nl> } <nl>  <nl> samples_l = frame -> extended_data [ wc -> ch_offset ];
int ff_srtp_decrypt ( struct SRTPContext * s , uint8_t * buf , int * lenptr ) <nl> { <nl> uint8_t iv [ 16 ] = { 0 }, hmac [ 20 ]; <nl> int len = * lenptr ; <nl> - int ext , seq_largest ; <nl> - uint32_t ssrc , roc ; <nl> + int ext , av_uninit ( seq_largest ); <nl> + uint32_t ssrc , av_uninit ( roc ); <nl> uint64_t index ; <nl> int rtcp ; <nl> 
static int normalize_bits ( int num , int width ) <nl> if ( num < 0 ) <nl> num = ~ num ; <nl>  <nl> - return width - av_log2 ( num ); <nl> + return width - av_log2 ( num ) - 1 ; <nl> } <nl>  <nl> /**
static int au_read_header ( AVFormatContext * s ) <nl> st -> codec -> channels = channels ; <nl> st -> codec -> sample_rate = rate ; <nl> if ( data_size != AU_UNKNOWN_SIZE ) <nl> - st -> duration = ((( int64_t ) data_size )<< 3 ) / ( st -> codec -> channels * bps ); <nl> + st -> duration = ((( int64_t ) data_size )<< 3 ) / ( st -> codec -> channels * ( int64_t ) bps ); <nl> avpriv_set_pts_info ( st , 64 , 1 , rate ); <nl> return 0 ; <nl> }
static float wv_get_value_float ( WavpackFrameContext * s , uint32_t * crc , int S ) <nl> uint32_t u ; <nl> } value ; <nl>  <nl> - int sign ; <nl> + unsigned int sign ; <nl> int exp = s -> float_max_exp ; <nl>  <nl> if ( s -> got_extra_bits ) {
fail : <nl> fail_at_end : <nl> av_freep (& pic -> codec_picture_params ); <nl> av_frame_free (& pic -> recon_image ); <nl> + av_buffer_unref (& pic -> output_buffer_ref ); <nl> + pic -> output_buffer = VA_INVALID_ID ; <nl> return err ; <nl> } <nl> 
static int opt_show_format_entry ( void * optctx , const char * opt , const char * arg ) <nl> char * buf = av_asprintf (" format =% s ", arg ); <nl> int ret ; <nl>  <nl> + if (! buf ) <nl> + return AVERROR ( ENOMEM ); <nl> + <nl> av_log ( NULL , AV_LOG_WARNING , <nl> " Option '% s ' is deprecated , use '- show_entries format =% s ' instead \ n ", <nl> opt , arg );
static int mpegts_push_data ( MpegTSFilter * filter , <nl> pes -> st -> request_probe = 1 ; <nl> } <nl> } else { <nl> + pes -> pes_header_size = 6 ; <nl> pes -> state = MPEGTS_PAYLOAD ; <nl> pes -> data_index = 0 ; <nl> }
static int mov_write_audio_tag ( AVFormatContext * s , AVIOContext * pb , MOVMuxContex <nl> uint32_t tag = track -> tag ; <nl>  <nl> if ( track -> mode == MODE_MOV ) { <nl> - if ( track -> timescale > UINT16_MAX ) { <nl> + if ( track -> timescale > UINT16_MAX || ! track -> par -> channels ) { <nl> if ( mov_get_lpcm_flags ( track -> par -> codec_id )) <nl> tag = AV_RL32 (" lpcm "); <nl> version = 2 ;
static int libshine_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> SHINEContext * s = avctx -> priv_data ; <nl> MPADecodeHeader hdr ; <nl> unsigned char * data ; <nl> - long written ; <nl> + int written ; <nl> int ret , len ; <nl>  <nl> if ( frame )
static int svq1_decode_frame ( AVCodecContext * avctx , <nl> if ( s -> f_code != 0x20 ) { <nl> uint32_t * src = ( uint32_t *) ( buf + 4 ); <nl>  <nl> + if ( buf_size < 36 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> src [ i ] = (( src [ i ] << 16 ) | ( src [ i ] >> 16 )) ^ src [ 7 - i ]; <nl> }
static int decode_frame_packing ( H264Context * h , int size ){ <nl>  <nl> int ff_h264_decode_sei ( H264Context * h ){ <nl> while ( get_bits_left (& h -> gb ) > 16 ) { <nl> - int size , type ; <nl> + int type ; <nl> + unsigned size ; <nl>  <nl> type = 0 ; <nl> do {
static AVStream * init_stream ( AVFormatContext * s ) <nl> avpriv_set_pts_info ( st , 60 , bin -> framerate . den , bin -> framerate . num ); <nl>  <nl> /* simulate tty display speed */ <nl> - bin -> chars_per_frame = FFMAX ( av_q2d ( st -> time_base ) * bin -> chars_per_frame , 1 ); <nl> + bin -> chars_per_frame = av_clip ( av_q2d ( st -> time_base ) * bin -> chars_per_frame , 1 , INT_MAX ); <nl>  <nl> return st ; <nl> }
int ff_get_qtpalette ( int codec_id , AVIOContext * pb , uint32_t * palette ) <nl>  <nl> /* If the depth is 1 , 2 , 4 , or 8 bpp , file is palettized . */ <nl> if (( bit_depth == 1 || bit_depth == 2 || bit_depth == 4 || bit_depth == 8 )) { <nl> - int color_count , color_start , color_end ; <nl> + uint32_t color_count , color_start , color_end ; <nl> uint32_t a , r , g , b ; <nl>  <nl> /* Ignore the greyscale bit for 1 - bit video and sample
av_cold int ff_snow_common_init ( AVCodecContext * avctx ){ <nl>  <nl> s -> avctx = avctx ; <nl> s -> max_ref_frames = 1 ; // just make sure it ' s not an invalid value in case of no initial keyframe <nl> + s -> spatial_decomposition_count = 1 ; <nl>  <nl> ff_me_cmp_init (& s -> mecc , avctx ); <nl> ff_hpeldsp_init (& s -> hdsp , avctx -> flags );
 <nl> static int null_filter_samples ( AVFilterLink * link , AVFilterBufferRef * samplesref ) <nl> { <nl> + avfilter_unref_bufferp (& samplesref ); <nl> return 0 ; <nl> } <nl> 
static int probe ( AVProbeData * p ) <nl> offset = AV_RL32 ( p -> buf + 18 + i * 16 ); <nl> if ( offset < 22 ) <nl> return FFMIN ( i , AVPROBE_SCORE_MAX / 4 ); <nl> - if ( offset + 8 > p -> buf_size ) <nl> + if ( offset > p -> buf_size - 8 ) <nl> continue ; <nl> if ( p -> buf [ offset ] != 40 && AV_RB64 ( p -> buf + offset ) != PNGSIG ) <nl> return FFMIN ( i , AVPROBE_SCORE_MAX / 4 );
int attribute_align_arg avresample_convert ( AVAudioResampleContext * avr , <nl> resample_out = & output_buffer ; <nl> else <nl> resample_out = avr -> resample_out_buffer ; <nl> - av_dlog ( avr , "[ resample ] % s to % s \ n ", current_buffer -> name , <nl> + av_dlog ( avr , "[ resample ] % s to % s \ n ", <nl> + current_buffer ? current_buffer -> name : " null ", <nl> resample_out -> name ); <nl> ret = ff_audio_resample ( avr -> resample , resample_out , <nl> current_buffer );
int ff_get_cpu_flags_x86 ( void ) <nl>  <nl> if ( max_ext_level >= 0x80000001 ){ <nl> cpuid ( 0x80000001 , eax , ebx , ecx , ext_caps ); <nl> - if ( ext_caps & ( 1 << 31 )) <nl> + if ( ext_caps & ( 1U << 31 )) <nl> rval |= AV_CPU_FLAG_3DNOW ; <nl> if ( ext_caps & ( 1 << 30 )) <nl> rval |= AV_CPU_FLAG_3DNOWEXT ;
static int old_codec47 ( SANMVideoContext * ctx , int top , <nl> if ( bytestream2_get_bytes_left (& ctx -> gb ) < width * height ) <nl> return AVERROR_INVALIDDATA ; <nl> for ( j = 0 ; j < height ; j ++) { <nl> - for ( i = 0 ; i < width ; i ++) <nl> - bytestream2_get_bufferu (& ctx -> gb , dst , width ); <nl> + bytestream2_get_bufferu (& ctx -> gb , dst , width ); <nl> dst += stride ; <nl> } <nl> break ;
static int filter_frame ( AVFilterLink * inlink , AVFrame * inpicref ) <nl>  <nl> inpicref -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> ret = ff_filter_frame ( outlink , inpicref ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_frame_free (& second ); <nl> return ret ; <nl> + } <nl>  <nl> second -> pts = outlink -> frame_count * sf -> ts_unit ; <nl> return ff_filter_frame ( outlink , second );
static int rtp_mpegts_write_header ( AVFormatContext * s ) <nl> } <nl> rtp_ctx -> oformat = rtp_format ; <nl> st = avformat_new_stream ( rtp_ctx , NULL ); <nl> + if (! st ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl> st -> time_base . num = 1 ; <nl> st -> time_base . den = 90000 ; <nl> st -> codecpar -> codec_id = AV_CODEC_ID_MPEG2TS ;
static int escape124_decode_frame ( AVCodecContext * avctx , <nl> // This codebook can be cut off at places other than <nl> // powers of 2 , leaving some of the entries undefined . <nl> cb_size = get_bits_long (& gb , 20 ); <nl> + if (! cb_size ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Invalid codebook size 0 .\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> cb_depth = av_log2 ( cb_size - 1 ) + 1 ; <nl> } else { <nl> cb_depth = get_bits (& gb , 4 );
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> for ( ch = 1 ; ch <= s -> channels ; ch ++) { <nl> float gain = s -> mul_bias / 4194304 . 0f ; <nl> if ( s -> channel_mode == AC3_CHMODE_DUALMONO ) { <nl> - gain *= s -> dynamic_range [ ch - 1 ]; <nl> + gain *= s -> dynamic_range [ 2 - ch ]; <nl> } else { <nl> gain *= s -> dynamic_range [ 0 ]; <nl> }
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int i ; <nl>  <nl> + x265_picture_init ( ctx -> params , & x265pic ); <nl> + <nl> if ( pic ) { <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> x265pic . planes [ i ] = pic -> data [ i ];
ebml_read_ascii ( MatroskaDemuxContext * matroska , <nl> offset_t pos = url_ftell ( pb ); <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Read error at pos . %" PRIu64 " ( 0x %" PRIx64 ")\ n ", pos , pos ); <nl> + av_free (* str ); <nl> return AVERROR ( EIO ); <nl> } <nl> (* str )[ size ] = '\ 0 ';
static int ac3_sync ( uint64_t state , AACAC3ParseContext * hdr_info , <nl> int err ; <nl> union { <nl> uint64_t u64 ; <nl> - uint8_t u8 [ 8 ]; <nl> + uint8_t u8 [ 8 + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> } tmp = { av_be2ne64 ( state ) }; <nl> AC3HeaderInfo hdr ; <nl> GetBitContext gbc ;
static int aac_sync ( uint64_t state , AACAC3ParseContext * hdr_info , <nl> int size ; <nl> union { <nl> uint64_t u64 ; <nl> - uint8_t u8 [ 8 ]; <nl> + uint8_t u8 [ 8 + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> } tmp ; <nl>  <nl> tmp . u64 = av_be2ne64 ( state );
static int read_quant_table ( RangeCoder * c , int16_t * quant_table , int scale ) <nl> memset ( state , 128 , sizeof ( state )); <nl>  <nl> for ( v = 0 ; i < 128 ; v ++) { <nl> - unsigned len = get_symbol ( c , state , 0 ) + 1 ; <nl> + unsigned len = get_symbol ( c , state , 0 ) + 1U ; <nl>  <nl> if ( len > 128 - i || ! len ) <nl> return AVERROR_INVALIDDATA ;
static av_cold int vc2_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> int64_t max_frame_bytes , r_bitrate = avctx -> bit_rate >> ( s -> interlaced ); <nl>  <nl> s -> avctx = avctx ; <nl> - s -> size_scaler = 1 ; <nl> + s -> size_scaler = 2 ; <nl> s -> prefix_bytes = 0 ; <nl> s -> last_parse_code = 0 ; <nl> s -> next_parse_offset = 0 ;
static int kempf_decode_tile ( G2MContext * c , int tile_x , int tile_y , <nl> src += 3 ; <nl> } <nl> npal = * src ++ + 1 ; <nl> + if ( src_end - src < npal * 3 ) <nl> + return AVERROR_INVALIDDATA ; <nl> memcpy ( pal , src , npal * 3 ); src += npal * 3 ; <nl> if ( sub_type != 2 ) { <nl> for ( i = 0 ; i < npal ; i ++) {
static int mov_read_udta_string ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> if (! key ) <nl> return 0 ; <nl> - if ( atom . size < 0 ) <nl> + if ( atom . size < 0 || str_size >= INT_MAX / 2 ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> // worst - case requirement for output string in case of utf8 coded input
int ff_vaapi_render_picture ( struct vaapi_context * vactx , VASurfaceID surface ) <nl> VABufferID va_buffers [ 3 ]; <nl> unsigned int n_va_buffers = 0 ; <nl>  <nl> + if (! vactx -> pic_param_buf_id ) <nl> + return 0 ; <nl> + <nl> vaUnmapBuffer ( vactx -> display , vactx -> pic_param_buf_id ); <nl> va_buffers [ n_va_buffers ++] = vactx -> pic_param_buf_id ; <nl> 
static int decodeTonalComponents ( GetBitContext * gb , tonal_component * pComponent <nl>  <nl> for ( k = 0 ; k < coded_components ; k ++) { <nl> sfIndx = get_bits ( gb , 6 ); <nl> + if ( component_count >= 64 ) <nl> + return AVERROR_INVALIDDATA ; <nl> pComponent [ component_count ]. pos = j * 64 + ( get_bits ( gb , 6 )); <nl> max_coded_values = SAMPLES_PER_FRAME - pComponent [ component_count ]. pos ; <nl> coded_values = coded_values_per_component + 1 ;
int ff_ivi_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> } <nl> } <nl> } <nl> + } else { <nl> + if ( ctx -> is_scalable ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> + for ( p = 0 ; p < 3 ; p ++) { <nl> + if (! ctx -> planes [ p ]. bands [ 0 ]. buf ) <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> // STOP_TIMER (" decode_planes "); }
static int lag_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if (! l -> rgb_planes ) { <nl> l -> rgb_stride = FFALIGN ( avctx -> width , 16 ); <nl> - l -> rgb_planes = av_malloc ( l -> rgb_stride * avctx -> height * planes ); <nl> + l -> rgb_planes = av_malloc ( l -> rgb_stride * avctx -> height * planes + 1 ); <nl> if (! l -> rgb_planes ) { <nl> av_log ( avctx , AV_LOG_ERROR , " cannot allocate temporary buffer \ n "); <nl> return AVERROR ( ENOMEM );
static int read_frame ( BVID_DemuxContext * vid , AVIOContext * pb , AVPacket * pkt , <nl> if ( vid -> palette ) { <nl> uint8_t * pdata = av_packet_new_side_data ( pkt , AV_PKT_DATA_PALETTE , <nl> BVID_PALETTE_SIZE ); <nl> - memcpy ( pdata , vid -> palette , BVID_PALETTE_SIZE ); <nl> + if ( pdata ) <nl> + memcpy ( pdata , vid -> palette , BVID_PALETTE_SIZE ); <nl> av_freep (& vid -> palette ); <nl> } <nl> 
static int ape_tag_read_field ( AVFormatContext * s ) <nl> if (! value ) <nl> return AVERROR ( ENOMEM ); <nl> c = avio_read ( pb , value , size ); <nl> - if ( c < 0 ) <nl> + if ( c < 0 ) { <nl> + av_free ( value ); <nl> return c ; <nl> + } <nl> value [ c ] = 0 ; <nl> av_dict_set (& s -> metadata , key , value , AV_DICT_DONT_STRDUP_VAL ); <nl> }
static int matroska_parse_frame ( MatroskaDemuxContext * matroska , <nl> /* XXX : prevent data copy ... */ <nl> if ( av_new_packet ( pkt , pkt_size + offset ) < 0 ) { <nl> av_free ( pkt ); <nl> + av_freep (& pkt_data ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
static inline int ff_fast_malloc ( void * ptr , unsigned int * size , size_t min_size , <nl> void ** p = ptr ; <nl> if ( min_size <= * size && * p ) <nl> return 0 ; <nl> - min_size = FFMAX ( 17 * min_size / 16 + 32 , min_size ); <nl> + min_size = FFMAX ( min_size + min_size / 16 + 32 , min_size ); <nl> av_free (* p ); <nl> * p = zero_realloc ? av_mallocz ( min_size ) : av_malloc ( min_size ); <nl> if (!* p )
 <nl> typedef struct Hnm4VideoContext { <nl> uint8_t version ; <nl> - uint16_t width ; <nl> - uint16_t height ; <nl> + int width ; <nl> + int height ; <nl> uint8_t * current ; <nl> uint8_t * previous ; <nl> uint8_t * buffer1 ;
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl>  <nl> rt -> state = STATE_START ; <nl> if ( rtmp_handshake ( s , rt )) <nl> - return - 1 ; <nl> + goto fail ; <nl>  <nl> rt -> chunk_size = 128 ; <nl> rt -> state = STATE_HANDSHAKED ;
static int opus_packet ( AVFormatContext * avf , int idx ) <nl>  <nl> if (! os -> psize ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( os -> granule > INT64_MAX - UINT32_MAX ) { <nl> + if ( os -> granule > ( 1LL << 62 )) { <nl> av_log ( avf , AV_LOG_ERROR , " Unsupported huge granule pos %" PRId64 "\ n ", os -> granule ); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> bytestream2_init (& gbc , avpkt -> data , avpkt -> size ); <nl>  <nl> + /* PICT images start with a 512 bytes empty header */ <nl> + if ( bytestream2_peek_be32 (& gbc ) == 0 ) <nl> + bytestream2_skip (& gbc , 512 ); <nl> + <nl> /* smallest PICT header */ <nl> if ( bytestream2_get_bytes_left (& gbc ) < 40 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Frame is too small % d \ n ",
av_cold int avcodec_close ( AVCodecContext * avctx ) <nl> avctx -> codec -> close ( avctx ); <nl> avcodec_default_free_buffers ( avctx ); <nl> avctx -> coded_frame = NULL ; <nl> - if ( avctx -> codec -> priv_class ) <nl> + if ( avctx -> codec && avctx -> codec -> priv_class ) <nl> av_opt_free ( avctx -> priv_data ); <nl> av_opt_free ( avctx ); <nl> av_freep (& avctx -> priv_data );
void avformat_free_context ( AVFormatContext * s ) <nl> av_opt_free ( s ); <nl> if ( s -> iformat && s -> iformat -> priv_class && s -> priv_data ) <nl> av_opt_free ( s -> priv_data ); <nl> + if ( s -> oformat && s -> oformat -> priv_class && s -> priv_data ) <nl> + av_opt_free ( s -> priv_data ); <nl>  <nl> for ( i = s -> nb_streams - 1 ; i >= 0 ; i --) { <nl> ff_free_stream ( s , s -> streams [ i ]);
static void opt_output_file ( const char * filename ) <nl> video_enc -> rc_max_rate = video_rc_max_rate ; <nl> video_enc -> rc_min_rate = video_rc_min_rate ; <nl> video_enc -> rc_buffer_size = video_rc_buffer_size ; <nl> + video_enc -> rc_initial_buffer_occupancy = video_rc_buffer_size * 3 / 4 ; <nl> video_enc -> rc_buffer_aggressivity = video_rc_buffer_aggressivity ; <nl> video_enc -> rc_initial_cplx = video_rc_initial_cplx ; <nl> video_enc -> i_quant_factor = video_i_qfactor ;
static int libquvi_read_header ( AVFormatContext * s ) <nl> if ( rc != QUVI_OK ) <nl> goto quvi_fail ; <nl>  <nl> + if (!( qc -> fmtctx = avformat_alloc_context ())) <nl> + goto quvi_fail ; <nl> + <nl> if (( ret = ff_copy_whitelists ( qc -> fmtctx , s )) < 0 ) <nl> goto end ; <nl> 
static int join_request_frame ( AVFilterLink * outlink ) <nl>  <nl> ret = ff_filter_frame ( outlink , frame ); <nl>  <nl> - memset ( s -> input_frames , 0 , sizeof (* s -> input_frames ) * ctx -> nb_inputs ); <nl> + for ( i = 0 ; i < ctx -> nb_inputs ; i ++) <nl> + av_frame_free (& s -> input_frames [ i ]); <nl>  <nl> return ret ; <nl> 
static uint64_t calc_rice_params ( RiceContext * rc , <nl> bits [ pmin ] = UINT32_MAX ; <nl> for ( i = pmax ; ; ) { <nl> bits [ i ] = calc_optimal_rice_params (& tmp_rc , i , sums , n , pred_order , kmax , exact ); <nl> - if ( bits [ i ] < bits [ opt_porder ]) { <nl> + if ( bits [ i ] < bits [ opt_porder ] || pmax == pmin ) { <nl> opt_porder = i ; <nl> * rc = tmp_rc ; <nl> }
retry : <nl> StreamInfo * stream = st -> priv_data ; <nl> const int avail_data = av_fifo_size ( stream -> fifo ); <nl> const int space = stream -> max_buffer_size - stream -> buffer_index ; <nl> - int rel_space = 1024 * space / stream -> max_buffer_size ; <nl> + int rel_space = 1024LL * space / stream -> max_buffer_size ; <nl> PacketDesc * next_pkt = stream -> premux_packet ; <nl>  <nl> /* for subtitle , a single PES packet must be generated ,
static int decode_frame ( AVCodecContext * avctx , <nl> buf += 5 ; <nl>  <nl> if ( video_size ) { <nl> - if ( video_size < 0 ) { <nl> + if ( video_size < 0 || video_size > buf_size ) { <nl> av_log ( avctx , AV_LOG_ERROR , " video size % d invalid \ n ", video_size ); <nl> return AVERROR_INVALIDDATA ; <nl> }
int main ( int argc , char ** argv ) <nl> goto end ; <nl>  <nl> /* read all packets */ <nl> + packet0 . data = NULL ; <nl> packet . data = NULL ; <nl> while ( 1 ) { <nl> if (! packet0 . data ) {
static int cllc_decode_frame ( AVCodecContext * avctx , void * data , <nl> coding_type = ( AV_RL32 ( src ) >> 8 ) & 0xFF ; <nl> av_log ( avctx , AV_LOG_DEBUG , " Frame coding type : % d \ n ", coding_type ); <nl>  <nl> + if ( get_bits_left (& gb ) < avctx -> height * avctx -> width ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> switch ( coding_type ) { <nl> case 0 : <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV422P ;
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> sub_packet_h <= 1 || <nl> ast -> coded_framesize * sub_packet_h > ( 2 + ( sub_packet_h & 1 )) * ast -> audio_framesize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( ast -> coded_framesize * sub_packet_h != 2 * ast -> audio_framesize ) { <nl> + avpriv_request_sample ( s , " mismatching interleaver parameters "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> break ; <nl> case DEINT_ID_GENR : <nl> if ( ast -> sub_packet_size <= 0 ||
static int rtp_write_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> const uint8_t * mb_info = <nl> av_packet_get_side_data ( pkt , AV_PKT_DATA_H263_MB_INFO , <nl> & mb_info_size ); <nl> + if (! mb_info ) { <nl> + av_log ( s1 , AV_LOG_ERROR , " failed to allocate side data \ n "); <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> ff_rtp_send_h263_rfc2190 ( s1 , pkt -> data , size , mb_info , mb_info_size ); <nl> break ; <nl> }
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> int lower_transport , const char * real_challenge ) <nl> { <nl> RTSPState * rt = s -> priv_data ; <nl> - int rtx , j , i , err , interleave = 0 ; <nl> + int rtx = 0 , j , i , err , interleave = 0 ; <nl> RTSPStream * rtsp_st ; <nl> RTSPMessageHeader reply1 , * reply = & reply1 ; <nl> char cmd [ 2048 ];
retry : <nl> es_size -= stream -> premux_packet -> unwritten_size ; <nl> stream -> premux_packet = stream -> premux_packet -> next ; <nl> } <nl> - if ( es_size ) <nl> + if ( stream -> premux_packet && es_size ) <nl> stream -> premux_packet -> unwritten_size -= es_size ; <nl>  <nl> if ( remove_decoded_packets ( ctx , s -> last_scr ) < 0 )
static int mp_decode_frame ( AVCodecContext * avctx , <nl> if ( sz == 0 ) <nl> goto end ; <nl>  <nl> + if ( mp -> max_codes_bits <= 0 ) <nl> + goto end ; <nl> if ( init_vlc (& mp -> vlc , mp -> max_codes_bits , mp -> codes_count , & mp -> codes [ 0 ]. size , sizeof ( HuffCode ), 1 , & mp -> codes [ 0 ]. code , sizeof ( HuffCode ), 4 , 0 )) <nl> goto end ; <nl> mp_decode_frame_helper ( mp , & gb );
static void scale_coefs ( <nl> int dynrng , <nl> int len ) <nl> { <nl> - int i , shift , round ; <nl> - unsigned mul ; <nl> + int i , shift ; <nl> + unsigned mul , round ; <nl> int temp , temp1 , temp2 , temp3 , temp4 , temp5 , temp6 , temp7 ; <nl>  <nl> mul = ( dynrng & 0x1f ) + 0x20 ;
static inline int dirac_get_arith_uint ( DiracArith * c , int follow_ctx , int data_c <nl> { <nl> int ret = 1 ; <nl> while (! dirac_get_arith_bit ( c , follow_ctx )) { <nl> + if ( ret >= 0x40000000 ) { <nl> + av_log ( NULL , AV_LOG_ERROR , " dirac_get_arith_uint overflow \ n "); <nl> + return - 1 ; <nl> + } <nl> ret <<= 1 ; <nl> ret += dirac_get_arith_bit ( c , data_ctx ); <nl> follow_ctx = ff_dirac_next_ctx [ follow_ctx ];
static int crypto_open ( URLContext * h , const char * uri , int flags ) <nl>  <nl> return 0 ; <nl> err : <nl> - av_free ( c -> key ); <nl> - av_free ( c -> iv ); <nl> + av_freep (& c -> key ); <nl> + av_freep (& c -> iv ); <nl> return ret ; <nl> } <nl> 
int ff_h264_frame_start ( H264Context * h ) <nl>  <nl> if (( ret = alloc_picture ( h , pic )) < 0 ) <nl> return ret ; <nl> - if (! h -> sync ) <nl> + if (! h -> sync && ! h -> avctx -> hwaccel ) <nl> avpriv_color_frame (& pic -> f , c ); <nl>  <nl> h -> cur_pic_ptr = pic ;
static int roq_read_packet ( AVFormatContext * s , <nl> pkt -> pos = avio_tell ( pb ); <nl> ret = avio_read ( pb , pkt -> data + RoQ_CHUNK_PREAMBLE_SIZE , <nl> chunk_size ); <nl> - if ( ret != chunk_size ) <nl> + if ( ret != chunk_size ) { <nl> + av_packet_unref ( pkt ); <nl> ret = AVERROR ( EIO ); <nl> + } <nl>  <nl> packet_read = 1 ; <nl> break ;
AVCodec ff_mjpeg_encoder = { <nl> . type = AVMEDIA_TYPE_VIDEO , <nl> . id = AV_CODEC_ID_MJPEG , <nl> . priv_data_size = sizeof ( MpegEncContext ), <nl> - . priv_class = & mjpeg_class , <nl> . init = ff_mpv_encode_init , <nl> . encode2 = ff_mpv_encode_picture , <nl> . close = ff_mpv_encode_end ,
static int udp_close ( URLContext * h ) <nl> ret = pthread_join ( s -> circular_buffer_thread , NULL ); <nl> if ( ret != 0 ) <nl> av_log ( h , AV_LOG_ERROR , " pthread_join (): % s \ n ", strerror ( ret )); <nl> + pthread_mutex_destroy (& s -> mutex ); <nl> + pthread_cond_destroy (& s -> cond ); <nl> } <nl> - <nl> - pthread_mutex_destroy (& s -> mutex ); <nl> - pthread_cond_destroy (& s -> cond ); <nl> # endif <nl> av_fifo_free ( s -> fifo ); <nl> return 0 ;
void ff_h264_free_tables ( H264Context * h , int free_rbsp ) <nl> if ( free_rbsp && h -> DPB ) { <nl> for ( i = 0 ; i < H264_MAX_PICTURE_COUNT ; i ++) <nl> ff_h264_unref_picture ( h , & h -> DPB [ i ]); <nl> + memset ( h -> delayed_pic , 0 , sizeof ( h -> delayed_pic )); <nl> av_freep (& h -> DPB ); <nl> } else if ( h -> DPB ) { <nl> for ( i = 0 ; i < H264_MAX_PICTURE_COUNT ; i ++)
typedef struct VP9Context { <nl> DECLARE_ALIGNED ( 32 , int16_t , uvblock )[ 2 ][ 1024 ]; <nl> uint8_t eob [ 256 ]; <nl> uint8_t uveob [ 2 ][ 64 ]; <nl> - VP56mv min_mv , max_mv ; <nl> + struct { int x , y ; } min_mv , max_mv ; <nl> DECLARE_ALIGNED ( 32 , uint8_t , tmp_y )[ 64 * 64 ]; <nl> DECLARE_ALIGNED ( 32 , uint8_t , tmp_uv )[ 2 ][ 32 * 32 ]; <nl> } VP9Context ;
typedef struct WmallDecodeCtx { <nl>  <nl> int8_t mclms_order ; <nl> int8_t mclms_scaling ; <nl> - int16_t mclms_coeffs [ 128 ]; <nl> - int16_t mclms_coeffs_cur [ 4 ]; <nl> + int16_t mclms_coeffs [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS * 32 ]; <nl> + int16_t mclms_coeffs_cur [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS ]; <nl> int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ;
int avcodec_copy_context ( AVCodecContext * dest , const AVCodecContext * src ) <nl> av_opt_free ( dest ); <nl>  <nl> memcpy ( dest , src , sizeof (* dest )); <nl> + av_opt_copy ( dest , src ); <nl>  <nl> dest -> priv_data = orig_priv_data ; <nl> 
static int handle_metadata ( RTMPContext * rt , RTMPPacket * pkt ) <nl> pts = cts ; <nl> ts += cts - pts ; <nl> pts = cts ; <nl> + if ( size + 3 + 4 > pkt -> data + pkt -> size - next ) <nl> + break ; <nl> bytestream_put_byte (& p , type ); <nl> bytestream_put_be24 (& p , size ); <nl> bytestream_put_be24 (& p , ts );
int ff_rtsp_make_setup_request ( AVFormatContext * s , const char * host , int port , <nl> goto fail ; <nl> } <nl>  <nl> - if ( reply -> timeout > 0 ) <nl> + if ( rt -> nb_rtsp_streams && reply -> timeout > 0 ) <nl> rt -> timeout = reply -> timeout ; <nl>  <nl> if ( rt -> server_type == RTSP_SERVER_REAL )
static int dirac_unpack_prediction_parameters ( DiracContext * s ) <nl> s -> globalmc [ ref ]. perspective [ 0 ] = dirac_get_se_golomb ( gb ); <nl> s -> globalmc [ ref ]. perspective [ 1 ] = dirac_get_se_golomb ( gb ); <nl> } <nl> + if ( s -> globalmc [ ref ]. perspective_exp + ( uint64_t ) s -> globalmc [ ref ]. zrs_exp > 30 ) { <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> } <nl> } <nl> 
int main ( int argc , char ** argv ) <nl>  <nl> if (! print_format ) <nl> print_format = av_strdup (" default "); <nl> + if (! print_format ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> w_name = av_strtok ( print_format , "=", & buf ); <nl> w_args = buf ; <nl> 
static int vp3_update_thread_context ( AVCodecContext * dst , const AVCodecContext * <nl> } <nl>  <nl> if ( s != s1 ) { <nl> + if (! s -> current_frame . f ) <nl> + return AVERROR ( ENOMEM ); <nl> // init tables if the first frame hasn ' t been decoded <nl> if (! s -> current_frame . f -> data [ 0 ]) { <nl> int y_fragment_count , c_fragment_count ;
static int mpegts_read_packet ( AVFormatContext * s , <nl> ts -> pkt = pkt ; <nl> ret = handle_packets ( ts , 0 ); <nl> if ( ret < 0 ) { <nl> + av_free_packet ( ts -> pkt ); <nl> /* flush pes data left */ <nl> for ( i = 0 ; i < NB_PID_MAX ; i ++) { <nl> if ( ts -> pids [ i ] && ts -> pids [ i ]-> type == MPEGTS_PES ) {
static int vp3_decode_init ( AVCodecContext * avctx ) <nl> & superblock_run_length_vlc_table [ 0 ][ 1 ], 4 , 2 , <nl> & superblock_run_length_vlc_table [ 0 ][ 0 ], 4 , 2 , 0 ); <nl>  <nl> - init_vlc (& s -> fragment_run_length_vlc , 5 , 31 , <nl> + init_vlc (& s -> fragment_run_length_vlc , 5 , 30 , <nl> & fragment_run_length_vlc_table [ 0 ][ 1 ], 4 , 2 , <nl> & fragment_run_length_vlc_table [ 0 ][ 0 ], 4 , 2 , 0 ); <nl> 
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int i ; <nl>  <nl> + x265_picture_init ( ctx -> params , & x265pic ); <nl> + <nl> if ( pic ) { <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> x265pic . planes [ i ] = pic -> data [ i ];
static int get_siz ( Jpeg2000DecoderContext * s ) <nl> avpriv_request_sample ( s -> avctx , " Support for image offsets "); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> + if ( s -> width > 32768U || s -> height > 32768U ) { <nl> + avpriv_request_sample ( s -> avctx , " Large Dimensions "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl>  <nl> if ( ncomponents <= 0 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid number of components : % d \ n ",
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> sfb_offsets [ i ][ band - 1 ] = subframe_len ; <nl> s -> num_sfb [ i ] = band - 1 ; <nl> + if ( s -> num_sfb [ i ] <= 0 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " num_sfb invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> 
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> switch ( c -> codec_id ) { <nl> case AV_CODEC_ID_H264 : { <nl> int mode = 1 ; <nl> - if ( fmt && fmt -> oformat -> priv_class && <nl> + if ( fmt && fmt -> oformat && fmt -> oformat -> priv_class && <nl> av_opt_flag_is_set ( fmt -> priv_data , " rtpflags ", " h264_mode0 ")) <nl> mode = 0 ; <nl> if ( c -> extradata_size ) {
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> lace_size [ n ] = lace_size [ n - 1 ] + snum ; <nl> total += lace_size [ n ]; <nl> } <nl> - lace_size [ n ] = size - total ; <nl> + lace_size [ laces - 1 ] = size - total ; <nl> break ; <nl> } <nl> }
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> void * tmp_ptr ; <nl> s -> max_framesize = 8192 ; // should hopefully be enough for the first header <nl> tmp_ptr = av_fast_realloc ( s -> bitstream , & s -> allocated_bitstream_size , <nl> - s -> max_framesize ); <nl> + s -> max_framesize + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! tmp_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM );
static void compute_chapters_end ( AVFormatContext * s ) <nl> if ( j != i && next_start > ch -> start && next_start < end ) <nl> end = next_start ; <nl> } <nl> - ch -> end = ( end == INT64_MAX ) ? ch -> start : end ; <nl> + ch -> end = ( end == INT64_MAX || end < ch -> start ) ? ch -> start : end ; <nl> } <nl> } <nl> 
static void apply_dependent_coupling_fixed ( AACContext * ac , <nl> for ( k = offsets [ i ]; k < offsets [ i + 1 ]; k ++) { <nl> tmp = ( int )((( int64_t ) src [ group * 128 + k ] * c + \ <nl> ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ group * 128 + k ] += tmp * ( 1 << shift ); <nl> + dest [ group * 128 + k ] += tmp * ( 1U << shift ); <nl> } <nl> } <nl> }
static void update_stream_timings ( AVFormatContext * ic ) <nl> st = ic -> streams [ i ]; <nl> if ( st -> start_time != AV_NOPTS_VALUE && st -> time_base . den ) { <nl> start_time1 = av_rescale_q ( st -> start_time , st -> time_base , AV_TIME_BASE_Q ); <nl> - if ( st -> codec -> codec_id == CODEC_ID_DVB_TELETEXT ) { <nl> + if ( st -> codec -> codec_id == CODEC_ID_DVB_TELETEXT || st -> codec -> codec_type == AVMEDIA_TYPE_SUBTITLE ) { <nl> if ( start_time1 < start_time_text ) <nl> start_time_text = start_time1 ; <nl> } else
static inline void xan_wc3_copy_pixel_run ( XanContext * s , <nl>  <nl> palette_plane = s -> current_frame . data [ 0 ]; <nl> prev_palette_plane = s -> last_frame . data [ 0 ]; <nl> + if (! prev_palette_plane ) <nl> + prev_palette_plane = palette_plane ; <nl> stride = s -> current_frame . linesize [ 0 ]; <nl> line_inc = stride - width ; <nl> curframe_index = y * stride + x ;
void ff_frame_thread_free ( AVCodecContext * avctx , int thread_count ) <nl> av_freep (& p -> avctx -> slice_offset ); <nl> } <nl>  <nl> - if ( p -> avctx ) <nl> + if ( p -> avctx ) { <nl> av_freep (& p -> avctx -> internal ); <nl> + av_buffer_unref (& p -> avctx -> hw_frames_ctx ); <nl> + } <nl> + <nl> av_freep (& p -> avctx ); <nl> } <nl> 
static inline void drawbox ( AVFilterBufferRef * picref , unsigned int x , unsigned i <nl> static int draw_glyphs ( DrawTextContext * dtext , AVFilterBufferRef * picref , <nl> int width , int height , const uint8_t rgbcolor [ 4 ], const uint8_t yuvcolor [ 4 ], int x , int y ) <nl> { <nl> - char * text = dtext -> text ; <nl> + char * text = HAVE_LOCALTIME_R ? dtext -> expanded_text : dtext -> text ; <nl> uint32_t code = 0 ; <nl> int i ; <nl> uint8_t * p ;
static int hds_write_header ( AVFormatContext * s ) <nl>  <nl> snprintf ( os -> temp_filename , sizeof ( os -> temp_filename ), <nl> "% s / stream % d_temp ", s -> filename , i ); <nl> - init_file ( s , os , 0 ); <nl> + ret = init_file ( s , os , 0 ); <nl> + if ( ret < 0 ) <nl> + goto fail ; <nl>  <nl> if (! os -> has_video && c -> min_frag_duration <= 0 ) { <nl> av_log ( s , AV_LOG_WARNING ,
static int init_filters ( const char * filters_descr ) <nl> abuffersink_params -> packing_fmts = packing_fmts ; <nl> ret = avfilter_graph_create_filter (& buffersink_ctx , abuffersink , " out ", <nl> NULL , abuffersink_params , filter_graph ); <nl> + av_free ( abuffersink_params ); <nl> if ( ret < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Cannot create audio buffer sink \ n "); <nl> return ret ;
static int filter_samples ( AVFilterLink * inlink , AVFilterBufferRef * buf ) <nl>  <nl> if ( labs ( delta ) > s -> min_delta ) { <nl> av_log ( ctx , AV_LOG_VERBOSE , " Discontinuity - %" PRId64 " samples .\ n ", delta ); <nl> - out_size += delta ; <nl> + out_size = av_clipl_int32 (( int64_t ) out_size + delta ); <nl> } else { <nl> if ( s -> resample ) { <nl> int comp = av_clip ( delta , - s -> max_comp , s -> max_comp );
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_INIT_AUDIO_BUFFERS : <nl> av_dlog ( NULL , " initialize audio buffers \ n "); <nl> - if (( opcode_version > 1 ) || ( opcode_size > 10 )) { <nl> + if (( opcode_version > 1 ) || ( opcode_size > 10 ) || opcode_size < 6 ) { <nl> av_dlog ( NULL , " bad init_audio_buffers opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static int compute_bit_allocation ( AC3EncodeContext * s ) <nl> */ <nl> static inline int sym_quant ( int c , int e , int levels ) <nl> { <nl> - int v = (((( levels * c ) >> ( 24 - e )) + 1 ) >> 1 ) + ( levels >> 1 ); <nl> + int v = ((( levels * c ) >> ( 24 - e )) + levels ) >> 1 ; <nl> av_assert2 ( v >= 0 && v < levels ); <nl> return v ; <nl> }
static int read_ffserver_streams ( OptionsContext * o , AVFormatContext * s , const ch <nl> { <nl> int i , err ; <nl> AVFormatContext * ic = avformat_alloc_context (); <nl> + if (! ic ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> ic -> interrupt_callback = int_cb ; <nl> err = avformat_open_input (& ic , filename , NULL , NULL );
void ff_rtsp_close_streams ( AVFormatContext * s ) <nl> if ( rtsp_st -> dynamic_handler && rtsp_st -> dynamic_protocol_context ) <nl> rtsp_st -> dynamic_handler -> close ( <nl> rtsp_st -> dynamic_protocol_context ); <nl> + av_free ( rtsp_st ); <nl> } <nl> } <nl> av_free ( rt -> rtsp_streams );
static void ipvideo_decode_opcodes ( IpvideoContext * s , AVFrame * frame ) <nl> init_get_bits (& gb , s -> decoding_map , s -> decoding_map_size * 8 ); <nl> for ( y = 0 ; y < s -> avctx -> height ; y += 8 ) { <nl> for ( x = 0 ; x < s -> avctx -> width ; x += 8 ) { <nl> + if ( get_bits_left (& gb ) < 4 ) <nl> + return ; <nl> opcode = get_bits (& gb , 4 ); <nl>  <nl> ff_tlog ( s -> avctx ,
reload : <nl> c -> end_of_segment = 1 ; <nl> c -> cur_seq_no = v -> cur_seq_no ; <nl>  <nl> - if ( v -> ctx ) { <nl> + if ( v -> ctx && v -> ctx -> nb_streams ) { <nl> v -> needed = 0 ; <nl> for ( i = v -> stream_offset ; i < v -> stream_offset + v -> ctx -> nb_streams ; <nl> i ++) {
static int parse_pixel_format ( AVCodecContext * avctx ) <nl> normal_map = flags & DDPF_NORMALMAP ; <nl> fourcc = bytestream2_get_le32 ( gbc ); <nl>  <nl> + if ( ctx -> compressed && ctx -> paletted ) { <nl> + av_log ( avctx , AV_LOG_WARNING , <nl> + " Disabling invalid palette flag for compressed dds .\ n "); <nl> + ctx -> paletted = 0 ; <nl> + } <nl> + <nl> bpp = bytestream2_get_le32 ( gbc ); // rgbbitcount <nl> r = bytestream2_get_le32 ( gbc ); // rbitmask <nl> g = bytestream2_get_le32 ( gbc ); // gbitmask
static OSStatus ffat_decode_callback ( AudioConverterRef converter , UInt32 * nb_pac <nl> return 0 ; <nl> } <nl>  <nl> + av_packet_unref (& at -> in_pkt ); <nl> av_packet_move_ref (& at -> in_pkt , & at -> new_in_pkt ); <nl> at -> new_in_pkt . data = 0 ; <nl> at -> new_in_pkt . size = 0 ;
static int dxva2_device_create9ex ( AVHWDeviceContext * ctx , UINT adapter ) <nl> if ( FAILED ( hr )) <nl> return AVERROR_UNKNOWN ; <nl>  <nl> - IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + hr = IDirect3D9Ex_GetAdapterDisplayModeEx ( d3d9ex , adapter , & modeex , NULL ); <nl> + if ( FAILED ( hr )) { <nl> + IDirect3D9Ex_Release ( d3d9ex ); <nl> + return AVERROR_UNKNOWN ; <nl> + } <nl>  <nl> d3dpp . BackBufferFormat = modeex . Format ; <nl> 
static int rm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> flags = ( seq ++ == 1 ) ? 2 : 0 ; <nl> } else { <nl> len = sync ( s , & timestamp , & flags , & i , & pos ); <nl> - st = s -> streams [ i ]; <nl> + if ( len > 0 ) <nl> + st = s -> streams [ i ]; <nl> } <nl>  <nl> if ( len < 0 || url_feof ( s -> pb ))
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size ){ <nl> switch ( hx -> nal_unit_type ) { <nl> case NAL_SPS : <nl> case NAL_PPS : <nl> + nals_needed = nal_index ; <nl> + break ; <nl> case NAL_IDR_SLICE : <nl> case NAL_SLICE : <nl> - nals_needed = nal_index ; <nl> + init_get_bits (& hx -> s . gb , ptr , bit_length ); <nl> + if (! get_ue_golomb (& hx -> s . gb )) <nl> + nals_needed = nal_index ; <nl> } <nl> continue ; <nl> }
static int mov_codec_id ( AVStream * st , uint32_t format ) <nl> static void mov_parse_stsd_video ( MOVContext * c , AVIOContext * pb , <nl> AVStream * st , MOVStreamContext * sc ) <nl> { <nl> - uint8_t codec_name [ 32 ]; <nl> + uint8_t codec_name [ 32 ] = { 0 }; <nl> int64_t stsd_start ; <nl> unsigned int len ; <nl> 
static av_cold int hevc_decode_free ( AVCodecContext * avctx ) <nl>  <nl> pic_arrays_free ( s ); <nl>  <nl> - av_freep (& lc -> edge_emu_buffer ); <nl> + if ( lc ) <nl> + av_freep (& lc -> edge_emu_buffer ); <nl> av_freep (& s -> md5_ctx ); <nl>  <nl> for ( i = 0 ; i < s -> nals_allocated ; i ++) {
static av_cold void dsputil_init_sse2 ( DSPContext * c , AVCodecContext * avctx , <nl> # if HAVE_SSE2_INLINE <nl> const int high_bit_depth = avctx -> bits_per_raw_sample > 8 ; <nl>  <nl> - if (! high_bit_depth && avctx -> idct_algo == FF_IDCT_XVIDMMX ) { <nl> + if (! high_bit_depth && avctx -> idct_algo == FF_IDCT_XVIDMMX && avctx -> lowres == 0 ) { <nl> c -> idct_put = ff_idct_xvid_sse2_put ; <nl> c -> idct_add = ff_idct_xvid_sse2_add ; <nl> c -> idct = ff_idct_xvid_sse2 ;
int ff_wma_end ( AVCodecContext * avctx ) <nl> free_vlc (& s -> coef_vlc [ i ]); <nl> av_free ( s -> run_table [ i ]); <nl> av_free ( s -> level_table [ i ]); <nl> + av_free ( s -> int_table [ i ]); <nl> } <nl>  <nl> return 0 ;
static int mov_read_extradata ( MOVContext * c , AVIOContext * pb , MOVAtom atom , <nl> av_log ( c -> fc , AV_LOG_WARNING , " truncated extradata \ n "); <nl> st -> codec -> extradata_size -= atom . size - err ; <nl> } <nl> + memset ( buf + 8 + err , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> return 0 ; <nl> } <nl> 
retry_duration : <nl> flv -> last_channels = <nl> channels = st -> codec -> channels ; <nl> } else { <nl> - AVCodecContext ctx ; <nl> + AVCodecContext ctx = { 0 }; <nl> ctx . sample_rate = sample_rate ; <nl> flv_set_audio_codec ( s , st , & ctx , flags & FLV_AUDIO_CODECID_MASK ); <nl> sample_rate = ctx . sample_rate ;
static int rv30_decode_mb_info ( RV34DecContext * r ) <nl> GetBitContext * gb = & s -> gb ; <nl> int code = svq3_get_ue_golomb ( gb ); <nl>  <nl> - if ( code > 11 ){ <nl> + if ( code < 0 || code > 11 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Incorrect MB type code \ n "); <nl> return - 1 ; <nl> }
typedef struct MmContext { <nl> AVCodecContext * avctx ; <nl> AVFrame * frame ; <nl> - int palette [ AVPALETTE_COUNT ]; <nl> + unsigned int palette [ AVPALETTE_COUNT ]; <nl> GetByteContext gb ; <nl> } MmContext ; <nl> 
static av_cold int XAVS_init ( AVCodecContext * avctx ) <nl> if (! x4 -> enc ) <nl> return - 1 ; <nl>  <nl> - if (!( x4 -> pts_buffer = av_mallocz (( avctx -> max_b_frames + 1 ) * sizeof (* x4 -> pts_buffer )))) <nl> + if (!( x4 -> pts_buffer = av_mallocz_array (( avctx -> max_b_frames + 1 ), sizeof (* x4 -> pts_buffer )))) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> avctx -> coded_frame = av_frame_alloc ();
static int mpeg_decode_mb ( MpegEncContext * s , int16_t block [ 12 ][ 64 ]) <nl>  <nl> cbp = get_vlc2 (& s -> gb , ff_mb_pat_vlc . table , MB_PAT_VLC_BITS , 1 ); <nl> if ( mb_block_count > 6 ) { <nl> - cbp <<= mb_block_count - 6 ; <nl> + cbp *= 1 << mb_block_count - 6 ; <nl> cbp |= get_bits (& s -> gb , mb_block_count - 6 ); <nl> s -> bdsp . clear_blocks ( s -> block [ 6 ]); <nl> }
static int vobsub_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> FFDemuxSubtitlesQueue * q ; <nl> AVIOContext * pb = vobsub -> sub_ctx -> pb ; <nl> int ret , psize , total_read = 0 , i ; <nl> - AVPacket idx_pkt ; <nl> + AVPacket idx_pkt = { 0 }; <nl>  <nl> int64_t min_ts = INT64_MAX ; <nl> int sid = 0 ;
static const AVProfile mpeg2_video_profiles [] = { <nl> { FF_PROFILE_MPEG2_SIMPLE , " Simple " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> + { FF_PROFILE_UNKNOWN }, <nl> }; <nl>  <nl> 
AVCodec ff_wmv2_decoder = { <nl> wmv2_decode_end , <nl> ff_h263_decode_frame , <nl> CODEC_CAP_DRAW_HORIZ_BAND | CODEC_CAP_DR1 , <nl> - . max_lowres = 3 , <nl> . long_name = NULL_IF_CONFIG_SMALL (" Windows Media Video 8 "), <nl> . pix_fmts = ff_pixfmt_list_420 , <nl> };
static int set_string_binary ( void * obj , const AVOption * o , const char * val , uint <nl> len /= 2 ; <nl>  <nl> ptr = bin = av_malloc ( len ); <nl> + if (! ptr ) <nl> + return AVERROR ( ENOMEM ); <nl> while (* val ) { <nl> int a = hexchar2int (* val ++); <nl> int b = hexchar2int (* val ++);
static int decode_mb_info ( IVI4DecContext * ctx , IVIBandDesc * band , <nl> } <nl> } <nl> } else { <nl> - if ( band -> inherit_mv ) { <nl> + if ( band -> inherit_mv && ref_mb ) { <nl> mb -> type = ref_mb -> type ; /* copy mb_type from corresponding reference mb */ <nl> } else if ( ctx -> frame_type == FRAMETYPE_INTRA ) { <nl> mb -> type = 0 ; /* mb_type is always INTRA for intra - frames */
enum dirac_subband { <nl> /* magic number division by 3 from schroedinger */ <nl> static inline int divide3 ( int x ) <nl> { <nl> - return (( x + 1 )* 21845 + 10922 ) >> 16 ; <nl> + return ( int )(( x + 1U )* 21845 + 10922 ) >> 16 ; <nl> } <nl>  <nl> static DiracFrame * remove_frame ( DiracFrame * framelist [], int picnum )
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> else { <nl> switch ( type ) { <nl> case TIFF_BYTE : <nl> - s -> bpp = ( off & 0xFF ) + (( off >> 8 ) & 0xFF ) + <nl> - (( off >> 16 ) & 0xFF ) + (( off >> 24 ) & 0xFF ); <nl> - break ; <nl> case TIFF_SHORT : <nl> case TIFF_LONG : <nl> s -> bpp = 0 ;
int av_opt_query_ranges_default ( AVOptionRanges ** ranges_arg , void * obj , const ch <nl> fail : <nl> av_free ( ranges ); <nl> av_free ( range ); <nl> + av_free ( range_array ); <nl> return ret ; <nl> } <nl> 
static int set_string_number ( void * obj , void * target_obj , const AVOption * o , con <nl> } <nl>  <nl> { <nl> - const AVOption * o_named = av_opt_find ( target_obj , buf , o -> unit , 0 , 0 ); <nl> + const AVOption * o_named = av_opt_find ( target_obj , i ? buf : val , o -> unit , 0 , 0 ); <nl> int res ; <nl> int ci = 0 ; <nl> double const_values [ 64 ];
static int parse_keyframes_index ( AVFormatContext * s , AVIOContext * ioc , AVStream <nl> return 0 ; <nl> } <nl>  <nl> + if ( vstream -> nb_index_entries > 0 ){ <nl> + av_log ( s , AV_LOG_WARNING , " Skiping duplicate index \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> while ( avio_tell ( ioc ) < max_pos - 2 && amf_get_string ( ioc , str_val , sizeof ( str_val )) > 0 ) { <nl> int64_t ** current_array ; <nl> unsigned int arraylen ;
static int read_access_unit ( AVCodecContext * avctx , void * data , <nl> substr_header_size += 2 ; <nl> } <nl>  <nl> + if ( length < header_size + substr_header_size ) { <nl> + av_log ( m -> avctx , AV_LOG_ERROR , " Insuffient data for headers \ n "); <nl> + goto error ; <nl> + } <nl> + <nl> if (!( nonrestart_substr ^ m -> is_major_sync_unit )) { <nl> av_log ( m -> avctx , AV_LOG_ERROR , " Invalid nonrestart_substr .\ n "); <nl> goto error ;
void avcodec_align_dimensions2 ( AVCodecContext * s , int * width , int * height , <nl> case AV_PIX_FMT_YUVJ411P : <nl> case AV_PIX_FMT_UYYVYY411 : <nl> w_align = 32 ; <nl> - h_align = 8 ; <nl> + h_align = 16 * 2 ; <nl> break ; <nl> case AV_PIX_FMT_YUV410P : <nl> if ( s -> codec_id == AV_CODEC_ID_SVQ1 ) {
int ff_xvid_rate_control_init ( MpegEncContext * s ){ <nl>  <nl> if ( write ( fd , tmp , strlen ( tmp )) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error % s writing 2pass logfile \ n ", strerror ( errno )); <nl> + av_free ( tmp_name ); <nl> + close ( fd ); <nl> return AVERROR ( errno ); <nl> } <nl> }
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl> static void free_texture ( void * opaque , uint8_t * data ) <nl> { <nl> ID3D11Texture2D_Release (( ID3D11Texture2D *) opaque ); <nl> + av_free ( data ); <nl> } <nl>  <nl> static AVBufferRef * wrap_texture_buf ( ID3D11Texture2D * tex , int index )
static int create_filter ( AVFilterContext ** filt_ctx , AVFilterGraph * ctx , int ind <nl> return ret ; <nl> } <nl>  <nl> - if (! strcmp ( filt_name , " scale ") && ! strstr ( args , " flags ")) { <nl> + if (! strcmp ( filt_name , " scale ") && args && ! strstr ( args , " flags ")) { <nl> snprintf ( tmp_args , sizeof ( tmp_args ), "% s :% s ", <nl> args , ctx -> scale_sws_opts ); <nl> args = tmp_args ;
static int hds_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> HDSContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ s -> streams [ pkt -> stream_index ]-> id ]; <nl> - int64_t end_dts = ( os -> fragment_index ) * c -> min_frag_duration ; <nl> + int64_t end_dts = os -> fragment_index * ( int64_t ) c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static int64_t mxf_essence_container_end ( MXFContext * mxf , int body_sid ) <nl> static int mxf_edit_unit_absolute_offset ( MXFContext * mxf , MXFIndexTable * index_table , int64_t edit_unit , int64_t * edit_unit_out , int64_t * offset_out , int nag ) <nl> { <nl> int i ; <nl> - int offset_temp = 0 ; <nl> + int64_t offset_temp = 0 ; <nl>  <nl> for ( i = 0 ; i < index_table -> nb_segments ; i ++) { <nl> MXFIndexTableSegment * s = index_table -> segments [ i ];
int ff_lpc_calc_coefs ( LPCContext * s , <nl> LLSModel m [ 2 ]; <nl> double var [ MAX_LPC_ORDER + 1 ], av_uninit ( weight ); <nl>  <nl> + if ( lpc_passes <= 0 ) <nl> + lpc_passes = 2 ; <nl> + <nl> for ( pass = 0 ; pass < lpc_passes ; pass ++){ <nl> av_init_lls (& m [ pass & 1 ], max_order ); <nl> 
static inline int read_line ( AVFormatContext * s , char * rbuf , const int rbufsize , <nl>  <nl> do { <nl> ret = ffurl_read_complete ( rt -> rtsp_hd , rbuf + idx , 1 ); <nl> - if ( ret < 0 ) <nl> - return ret ; <nl> + if ( ret <= 0 ) <nl> + return ret ? ret : AVERROR_EOF ; <nl> if ( rbuf [ idx ] == '\ r ') { <nl> /* Ignore */ <nl> } else if ( rbuf [ idx ] == '\ n ') {
static int decode_frame ( AVCodecContext * avctx , <nl> AVFrame * const p = & s -> picture ; <nl> uint8_t * ptr ; <nl>  <nl> - int magic_num , offset , endian ; <nl> + unsigned int offset ; <nl> + int magic_num , endian ; <nl> int x , y ; <nl> int w , h , stride , bits_per_color , descriptor , elements , target_packet_size , source_packet_size ; <nl> 
void avcodec_register_all ( void ) <nl> REGISTER_ENCDEC ( XSUB , xsub ); <nl>  <nl> /* external libraries */ <nl> - REGISTER_ENCODER ( LIBAACPLUS , libaacplus ); <nl> REGISTER_DECODER ( LIBCELT , libcelt ); <nl> REGISTER_DECODER ( LIBDIRAC , libdirac ); <nl> REGISTER_ENCODER ( LIBFAAC , libfaac ); <nl> void avcodec_register_all ( void ) <nl> REGISTER_ENCODER ( LIBX264RGB , libx264rgb ); <nl> REGISTER_ENCODER ( LIBXAVS , libxavs ); <nl> REGISTER_ENCODER ( LIBXVID , libxvid ); <nl> + REGISTER_ENCODER ( LIBAACPLUS , libaacplus ); <nl>  <nl> /* text */ <nl> REGISTER_DECODER ( BINTEXT , bintext );
static int tiff_decode_tag ( TiffContext * s ) <nl> " Samples per pixel requires a single value , many provided \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( value > 4U ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , <nl> + " Samples per pixel % d is too large \ n ", value ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( s -> bppcount == 1 ) <nl> s -> bpp *= value ; <nl> s -> bppcount = value ;
static int process_output_surface ( AVCodecContext * avctx , AVPacket * pkt , NvencSur <nl> NV_ENCODE_API_FUNCTION_LIST * p_nvenc = & dl_fn -> nvenc_funcs ; <nl>  <nl> uint32_t slice_mode_data ; <nl> - uint32_t * slice_offsets ; <nl> + uint32_t * slice_offsets = NULL ; <nl> NV_ENC_LOCK_BITSTREAM lock_params = { 0 }; <nl> NVENCSTATUS nv_status ; <nl> int res = 0 ;
static int vp9_superframe_filter ( AVBSFContext * ctx , AVPacket * out ) <nl> goto done ; <nl> } <nl>  <nl> - av_packet_move_ref ( s -> cache [ s -> n_cache ++], in ); <nl> + res = av_packet_ref ( s -> cache [ s -> n_cache ++], in ); <nl> + if ( res < 0 ) <nl> + goto done ; <nl>  <nl> if ( invisible ) { <nl> res = AVERROR ( EAGAIN );
static av_cold int g726_decode_init ( AVCodecContext * avctx ) <nl> { <nl> G726Context * c = avctx -> priv_data ; <nl>  <nl> + if ( avctx -> channels > 1 ){ <nl> + avpriv_request_sample ( avctx , " Decoding more than one channel "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> avctx -> channels = 1 ; <nl> avctx -> channel_layout = AV_CH_LAYOUT_MONO ; <nl> 
static int tak_read_header ( AVFormatContext * s ) <nl> buffer = av_malloc ( size - 3 + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! buffer ) <nl> return AVERROR ( ENOMEM ); <nl> + memset ( buffer + size - 3 , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> ffio_init_checksum ( pb , tak_check_crc , 0xCE04B7U ); <nl> if ( avio_read ( pb , buffer , size - 3 ) != size - 3 ) {
static int mov_read_dref ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> if ( entries >= UINT_MAX / sizeof (* sc -> drefs )) <nl> return AVERROR_INVALIDDATA ; <nl> av_free ( sc -> drefs ); <nl> + sc -> drefs_count = 0 ; <nl> sc -> drefs = av_mallocz ( entries * sizeof (* sc -> drefs )); <nl> if (! sc -> drefs ) <nl> return AVERROR ( ENOMEM );
static int ea_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( ea -> audio_codec ) { <nl> - if ( ea -> num_channels <= 0 ) { <nl> + if ( ea -> num_channels <= 0 || ea -> num_channels > 2 ) { <nl> av_log ( s , AV_LOG_WARNING , <nl> " Unsupported number of channels : % d \ n ", ea -> num_channels ); <nl> ea -> audio_codec = 0 ;
static av_cold void uninit ( AVFilterContext * ctx ) <nl> avfilter_unref_buffer ( deshake -> ref ); <nl> if ( deshake -> fp ) <nl> fclose ( deshake -> fp ); <nl> - avcodec_close ( deshake -> avctx ); <nl> + if ( deshake -> avctx ) <nl> + avcodec_close ( deshake -> avctx ); <nl> av_freep (& deshake -> avctx ); <nl> } <nl> 
static int txd_read_header ( AVFormatContext * s , AVFormatParameters * ap ) { <nl> st -> codec -> time_base . den = 5 ; <nl> st -> codec -> time_base . num = 1 ; <nl> /* the parameters will be extracted from the compressed bitstream */ <nl> + <nl> + s -> pb -> maxsize = avio_size ( s -> pb ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int http_open_cnx ( URLContext * h ) <nl> { <nl> const char * path , * proxy_path , * lower_proto = " tcp ", * local_path ; <nl> char hostname [ 1024 ], hoststr [ 1024 ], proto [ 10 ]; <nl> - char auth [ 1024 ], proxyauth [ 1024 ]; <nl> + char auth [ 1024 ], proxyauth [ 1024 ] = ""; <nl> char path1 [ 1024 ]; <nl> char buf [ 1024 ], urlbuf [ 1024 ]; <nl> int port , use_proxy , err , location_changed = 0 , redirects = 0 ;
static int vorbis_parse_audio_packet ( vorbis_context * vc ) <nl> ch_left -= ch ; <nl> } <nl>  <nl> + if ( ch_left > 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> // Inverse coupling <nl>  <nl> for ( i = mapping -> coupling_steps - 1 ; i >= 0 ; -- i ) { // warning : i has to be signed
static int bit_allocation ( IMCContext * q , IMCChannel * chctx , <nl> iacc += chctx -> bandWidthT [ i ]; <nl> summa += chctx -> bandWidthT [ i ] * chctx -> flcoeffs4 [ i ]; <nl> } <nl> + <nl> + if (! iacc ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> chctx -> bandWidthT [ BANDS - 1 ] = 0 ; <nl> summa = ( summa * 0 . 5 - freebits ) / iacc ; <nl> 
static av_cold int opus_decode_init ( AVCodecContext * avctx ) <nl>  <nl> /* find out the channel configuration */ <nl> ret = ff_opus_parse_extradata ( avctx , c ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& c -> channel_maps ); <nl> + av_freep (& c -> fdsp ); <nl> return ret ; <nl> + } <nl>  <nl> /* allocate and init each independent decoder */ <nl> c -> streams = av_mallocz_array ( c -> nb_streams , sizeof (* c -> streams ));
* MPEG Audio decoder <nl> */ <nl>  <nl> -# define UNCHECKED_BITSTREAM_READER 1 <nl> - <nl> # include " libavutil / audioconvert . h " <nl> # include " avcodec . h " <nl> # include " get_bits . h "
int ff_jpegls_decode_picture ( MJpegDecodeContext * s , int near , <nl> avpriv_report_missing_feature ( s -> avctx , " Sample interleaved images "); <nl> ret = AVERROR_PATCHWELCOME ; <nl> goto end ; <nl> + } else { /* unknown interleaving */ <nl> + avpriv_report_missing_feature ( s -> avctx , " Unknown interleaved images "); <nl> + ret = AVERROR_PATCHWELCOME ; <nl> + goto end ; <nl> } <nl>  <nl> if ( s -> xfrm && s -> nb_components == 3 ) {
static int get_metadata ( AVFormatContext * s , <nl> if (! buf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> - if ( avio_read ( s -> pb , buf , data_size ) < 0 ) { <nl> + if ( avio_read ( s -> pb , buf , data_size ) != data_size ) { <nl> av_free ( buf ); <nl> return AVERROR ( EIO ); <nl> }
static int check_opcodes ( MMCO * mmco1 , MMCO * mmco2 , int n_mmcos ) <nl> int ff_generate_sliding_window_mmcos ( H264Context * h , int first_slice ) <nl> { <nl> MMCO mmco_temp [ MAX_MMCO_COUNT ], * mmco = first_slice ? h -> mmco : mmco_temp ; <nl> - int mmco_index = 0 , i ; <nl> + int mmco_index = 0 , i = 0 ; <nl>  <nl> assert ( h -> long_ref_count + h -> short_ref_count <= h -> sps . ref_frame_count ); <nl> 
resync : <nl> pkt -> data , pkt -> size ); <nl> pkt -> destruct = dstr ; <nl> pkt -> flags |= AV_PKT_FLAG_KEY ; <nl> + if ( size < 0 ) <nl> + av_free_packet ( pkt ); <nl> } else { <nl> /* XXX : How to handle B - frames in AVI ? */ <nl> pkt -> dts = ast -> frame_offset ;
static int cinvideo_decode_frame ( AVCodecContext * avctx , <nl> if ( bitmap_frame_size < palette_colors_count * ( 3 + ( palette_type != 0 ))) <nl> return AVERROR_INVALIDDATA ; <nl> if ( palette_type == 0 ) { <nl> + if ( palette_colors_count > 256 ) <nl> + return AVERROR_INVALIDDATA ; <nl> for ( i = 0 ; i < palette_colors_count ; ++ i ) { <nl> cin -> palette [ i ] = bytestream_get_le24 (& buf ); <nl> bitmap_frame_size -= 3 ;
static int xwd_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + avctx -> height * lsize ) { <nl> + if ( bytestream2_get_bytes_left (& gb ) < ncolors * XWD_CMAP_SIZE + ( uint64_t ) avctx -> height * lsize ) { <nl> av_log ( avctx , AV_LOG_ERROR , " input buffer too small \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int ea_read_header ( AVFormatContext * s , <nl> st -> codec -> height = ea -> height ; <nl> } <nl>  <nl> + if ( ea -> num_channels <= 0 ) { <nl> + av_log ( s , AV_LOG_WARNING , " Unsupported number of channels : % d \ n ", ea -> num_channels ); <nl> + ea -> audio_codec = 0 ; <nl> + } <nl> + <nl> if ( ea -> audio_codec ) { <nl> /* initialize the audio decoder stream */ <nl> st = av_new_stream ( s , 0 );
uint8_t * av_packet_new_side_data ( AVPacket * pkt , enum AVPacketSideDataType type , <nl> if (! pkt -> side_data ) <nl> return NULL ; <nl>  <nl> - pkt -> side_data [ elems ]. data = av_malloc ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + pkt -> side_data [ elems ]. data = av_mallocz ( size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! pkt -> side_data [ elems ]. data ) <nl> return NULL ; <nl> pkt -> side_data [ elems ]. size = size ;
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * in ) <nl> GradFunContext * gf = inlink -> dst -> priv ; <nl> AVFilterLink * outlink = inlink -> dst -> outputs [ 0 ]; <nl> AVFilterBufferRef * out ; <nl> - int p , direct ; <nl> + int p , direct = 0 ; <nl>  <nl> if (( in -> perms & AV_PERM_WRITE ) && !( in -> perms & AV_PERM_PRESERVE )) { <nl> direct = 1 ;
av_cold int ff_ac3_encode_close ( AVCodecContext * avctx ) <nl> AC3EncodeContext * s = avctx -> priv_data ; <nl>  <nl> av_freep (& s -> windowed_samples ); <nl> + if ( s -> planar_samples ) <nl> for ( ch = 0 ; ch < s -> channels ; ch ++) <nl> av_freep (& s -> planar_samples [ ch ]); <nl> av_freep (& s -> planar_samples );
static int cbs_h264_read_nal_unit ( CodedBitstreamContext * ctx , <nl> err = cbs_h264_read_sei ( ctx , & bc , sei ); <nl> if ( err < 0 ) { <nl> cbs_h264_free_sei ( sei ); <nl> + av_free ( sei ); <nl> return err ; <nl> } <nl> 
typedef struct HEVCPPS { <nl> uint8_t chroma_qp_offset_list_enabled_flag ; <nl> uint8_t diff_cu_chroma_qp_offset_depth ; <nl> uint8_t chroma_qp_offset_list_len_minus1 ; <nl> - int8_t cb_qp_offset_list [ 5 ]; <nl> - int8_t cr_qp_offset_list [ 5 ]; <nl> + int8_t cb_qp_offset_list [ 6 ]; <nl> + int8_t cr_qp_offset_list [ 6 ]; <nl> uint8_t log2_sao_offset_scale_luma ; <nl> uint8_t log2_sao_offset_scale_chroma ; <nl> 
static int gdv_decode_frame ( AVCodecContext * avctx , void * data , <nl> case 1 : <nl> memset ( gdv -> frame + PREAMBLE_SIZE , 0 , gdv -> frame_size - PREAMBLE_SIZE ); <nl> case 0 : <nl> + if ( bytestream2_get_bytes_left ( gb ) < 256 * 3 ) <nl> + return AVERROR_INVALIDDATA ; <nl> for ( i = 0 ; i < 256 ; i ++) { <nl> unsigned r = bytestream2_get_byte ( gb ); <nl> unsigned g = bytestream2_get_byte ( gb );
static int64_t pva_read_timestamp ( struct AVFormatContext * s , int stream_index , <nl> ByteIOContext * pb = s -> pb ; <nl> PVAContext * pvactx = s -> priv_data ; <nl> int length , streamid ; <nl> - int64_t res ; <nl> + int64_t res = AV_NOPTS_VALUE ; <nl>  <nl> pos_limit = FFMIN (* pos + PVA_MAX_PAYLOAD_LENGTH * 8 , ( uint64_t )* pos + pos_limit ); <nl> 
int ff_audio_mix ( AudioMix * am , AudioData * src ) <nl>  <nl> if ( am -> in_matrix_channels && am -> out_matrix_channels ) { <nl> uint8_t ** data ; <nl> - uint8_t * data0 [ AVRESAMPLE_MAX_CHANNELS ]; <nl> + uint8_t * data0 [ AVRESAMPLE_MAX_CHANNELS ] = { NULL }; <nl>  <nl> if ( am -> out_matrix_channels < am -> out_channels || <nl> am -> in_matrix_channels < am -> in_channels ) {
static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> av_buffer_realloc ( buf , size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> - if (! buf ) <nl> + if (!* buf ) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> memset ((* buf )-> data + size , 0 , FF_INPUT_BUFFER_PADDING_SIZE );
static void rac_normalise ( RangeCoder * c ) <nl> c -> low |= * c -> src ++; <nl> } else if (! c -> low ) { <nl> c -> got_error = 1 ; <nl> - return ; <nl> + c -> low = 1 ; <nl> } <nl> if ( c -> range >= RAC_BOTTOM ) <nl> return ;
static int ism_write_header ( AVFormatContext * s ) <nl> goto fail ; <nl> } <nl>  <nl> - c -> streams = av_mallocz ( sizeof (* c -> streams ) * s -> nb_streams ); <nl> + c -> streams = av_mallocz_array ( s -> nb_streams , sizeof (* c -> streams )); <nl> if (! c -> streams ) { <nl> ret = AVERROR ( ENOMEM ); <nl> goto fail ;
static int decode_lowdelay ( DiracContext * s ) <nl> s -> slice_params_buf = av_realloc_f ( s -> slice_params_buf , s -> num_x * s -> num_y , sizeof ( DiracSlice )); <nl> if (! s -> slice_params_buf ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " slice params buffer allocation failure \ n "); <nl> + s -> slice_params_num_buf = 0 ; <nl> return AVERROR ( ENOMEM ); <nl> } <nl> s -> slice_params_num_buf = s -> num_x * s -> num_y ;
static int smush_read_packet ( AVFormatContext * ctx , AVPacket * pkt ) <nl> case MKBETAG (' W ', ' a ', ' v ', ' e '): <nl> if ( size < 13 ) <nl> return AVERROR_INVALIDDATA ; <nl> - if ( av_get_packet ( pb , pkt , size ) < 0 ) <nl> + if ( av_get_packet ( pb , pkt , size ) < 13 ) <nl> return AVERROR ( EIO ); <nl>  <nl> pkt -> stream_index = smush -> audio_stream_index ;
static int encode_thread ( AVCodecContext * c , void * arg ){ <nl> av_fast_padded_malloc (& new_buffer , & new_buffer_size , new_size ); <nl> if ( new_buffer ) { <nl> memcpy ( new_buffer , s -> avctx -> internal -> byte_buffer , s -> avctx -> internal -> byte_buffer_size ); <nl> + av_free ( s -> avctx -> internal -> byte_buffer ); <nl> s -> avctx -> internal -> byte_buffer = new_buffer ; <nl> s -> avctx -> internal -> byte_buffer_size = new_buffer_size ; <nl> rebase_put_bits (& s -> pb , new_buffer , new_buffer_size );
static int unpack_modes ( Vp3DecodeContext * s , GetBitContext * gb ) <nl>  <nl> /* is it a custom coding scheme ? */ <nl> if ( scheme == 0 ) { <nl> + for ( i = 0 ; i < 8 ; i ++) <nl> + custom_mode_alphabet [ i ] = MODE_INTER_NO_MV ; <nl> for ( i = 0 ; i < 8 ; i ++) <nl> custom_mode_alphabet [ get_bits ( gb , 3 )] = i ; <nl> }
void * av_malloc ( size_t size ) <nl> long diff ; <nl> # endif <nl>  <nl> + assert ( size ); <nl> + <nl> /* let ' s disallow possible ambiguous cases */ <nl> - if ( size > ( INT_MAX - 32 ) ) <nl> + if ( size > ( INT_MAX - 32 ) || ! size ) <nl> return NULL ; <nl>  <nl> # if CONFIG_MEMALIGN_HACK
typedef struct { <nl>  <nl>  <nl> static const AVOption options [] = { <nl> - { " use_odml ", " use odml index ", offsetof ( AVIContext , use_odml ), FF_OPT_TYPE_INT , 1 , - 1 , 1 , AV_OPT_FLAG_DECODING_PARAM }, <nl> + { " use_odml ", " use odml index ", offsetof ( AVIContext , use_odml ), FF_OPT_TYPE_INT , {. dbl = 1 }, - 1 , 1 , AV_OPT_FLAG_DECODING_PARAM }, <nl> { NULL }, <nl> }; <nl> 
static int swf_write_trailer ( AVFormatContext * s ) <nl> put_le32 ( pb , file_size ); <nl> url_fseek ( pb , swf -> duration_pos , SEEK_SET ); <nl> put_le16 ( pb , video_enc -> frame_number ); <nl> + url_fseek ( pb , file_size , SEEK_SET ); <nl> } <nl>  <nl> av_free ( swf -> audio_fifo );
int ff_pnm_decode_header ( AVCodecContext * avctx , PNMContext * const s ) <nl>  <nl> avctx -> width = w ; <nl> avctx -> height = h ; <nl> + s -> maxval = maxval ; <nl> if ( depth == 1 ) { <nl> if ( maxval == 1 ) <nl> avctx -> pix_fmt = PIX_FMT_MONOWHITE ;
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl>  <nl> s -> slices [ i ][ j ]. start = offset + header_size ; <nl> - s -> slices [ i ][ j ]. size = avpkt -> size - offset ; <nl> + s -> slices [ i ][ j ]. size = avpkt -> size - s -> slices [ i ][ j ]. start ; <nl> } <nl>  <nl> if ( bytestream2_get_byte (& gb ) != s -> planes )
static int ivf_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> static int ivf_write_trailer ( AVFormatContext * s ) <nl> { <nl> AVIOContext * pb = s -> pb ; <nl> - if ( pb -> seekable ) { <nl> - IVFEncContext * ctx = s -> priv_data ; <nl> + IVFEncContext * ctx = s -> priv_data ; <nl> + <nl> + if ( pb -> seekable && ctx -> frame_cnt > 1 ) { <nl> size_t end = avio_tell ( pb ); <nl>  <nl> avio_seek ( pb , 24 , SEEK_SET );
static av_cold int dnxhd_decode_init ( AVCodecContext * avctx ) <nl>  <nl> ctx -> avctx = avctx ; <nl> ctx -> cid = - 1 ; <nl> - avctx -> colorspace = AVCOL_SPC_BT709 ; <nl> + if ( avctx -> colorspace == AVCOL_SPC_UNSPECIFIED ) { <nl> + avctx -> colorspace = AVCOL_SPC_BT709 ; <nl> + } <nl>  <nl> avctx -> coded_width = FFALIGN ( avctx -> width , 16 ); <nl> avctx -> coded_height = FFALIGN ( avctx -> height , 16 );
static av_cold int vtenc_init ( AVCodecContext * avctx ) <nl> kCFAllocatorDefault , <nl> & has_b_frames_cfbool ); <nl>  <nl> - if (! status ) { <nl> + if (! status && has_b_frames_cfbool ) { <nl> // Some devices don ' t output B - frames for main profile , even if requested . <nl> vtctx -> has_b_frames = CFBooleanGetValue ( has_b_frames_cfbool ); <nl> CFRelease ( has_b_frames_cfbool );
static int mov_write_sidx_tag ( AVIOContext * pb , <nl> } <nl> } else { <nl> entries = track -> nb_frag_info ; <nl> + if ( entries <= 0 ) <nl> + return 0 ; <nl> presentation_time = track -> frag_info [ 0 ]. time ; <nl> } <nl> 
static int tcp_open ( URLContext * h , const char * uri , int flags ) <nl> } <nl> /* test error */ <nl> optlen = sizeof ( ret ); <nl> - getsockopt ( fd , SOL_SOCKET , SO_ERROR , & ret , & optlen ); <nl> + if ( getsockopt ( fd , SOL_SOCKET , SO_ERROR , & ret , & optlen )) <nl> + ret = AVUNERROR ( ff_neterrno ()); <nl> if ( ret != 0 ) { <nl> char errbuf [ 100 ]; <nl> ret = AVERROR ( ret );
void ff_lzw_decode_tail ( LZWState * p ) <nl> struct LZWState * s = ( struct LZWState *) p ; <nl>  <nl> if ( s -> mode == FF_LZW_GIF ) { <nl> - while ( s -> pbuf < s -> ebuf && s -> bs > 0 ){ <nl> + while ( s -> pbuf + s -> bs < s -> ebuf && s -> bs > 0 ){ <nl> s -> pbuf += s -> bs ; <nl> s -> bs = * s -> pbuf ++; <nl> }
static inline int init_get_bits ( GetBitContext * s , const uint8_t * buffer , <nl> int buffer_size ; <nl> int ret = 0 ; <nl>  <nl> - if ( bit_size > INT_MAX - 7 || bit_size <= 0 ) { <nl> + if ( bit_size > INT_MAX - 7 || bit_size < 0 || ! buffer ) { <nl> buffer_size = bit_size = 0 ; <nl> buffer = NULL ; <nl> ret = AVERROR_INVALIDDATA ;
typedef struct TargaContext { <nl> } TargaContext ; <nl>  <nl> # define CHECK_BUFFER_SIZE ( buf , buf_end , needed , where ) \ <nl> - if ( buf + needed > buf_end ){ \ <nl> + if ( needed > buf_end - buf ){ \ <nl> av_log ( avctx , AV_LOG_ERROR , " Problem : unexpected end of data while reading " where "\ n "); \ <nl> return - 1 ; \ <nl> } \
static int flac_parse ( AVCodecParserContext * s , AVCodecContext * avctx , <nl> temp = curr -> next ; <nl> av_freep (& curr -> link_penalty ); <nl> av_free ( curr ); <nl> + fpc -> nb_headers_buffered --; <nl> } <nl> fpc -> headers = fpc -> best_header -> next ; <nl> av_freep (& fpc -> best_header -> link_penalty ); <nl> av_freep (& fpc -> best_header ); <nl> + fpc -> nb_headers_buffered --; <nl> } <nl>  <nl> /* Find and score new headers . */
int av_expr_parse ( AVExpr ** expr , const char * s , <nl> goto end ; <nl> } <nl> e -> var = av_mallocz ( sizeof ( double ) * VARS ); <nl> + if (! e -> var ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> * expr = e ; <nl> e = NULL ; <nl> end :
typedef struct FlacSubframe { <nl> int shift ; <nl> RiceContext rc ; <nl> int32_t samples [ FLAC_MAX_BLOCKSIZE ]; <nl> - int32_t residual [ FLAC_MAX_BLOCKSIZE ]; <nl> + int32_t residual [ FLAC_MAX_BLOCKSIZE + 1 ]; <nl> } FlacSubframe ; <nl>  <nl> typedef struct FlacFrame {
static int read_frame_data ( ALSDecContext * ctx , unsigned int ra_frame ) <nl>  <nl> // TODO : read_diff_float_data <nl>  <nl> + if ( get_bits_left ( gb ) < 0 ) { <nl> + av_log ( ctx -> avctx , AV_LOG_ERROR , " Overread % d \ n ", - get_bits_left ( gb )); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static int h261_probe ( AVProbeData * p ) <nl>  <nl> static int ac3_probe ( AVProbeData * p ) <nl> { <nl> - int max_frames , first_frames , frames ; <nl> + int max_frames , first_frames = 0 , frames ; <nl> uint8_t * buf , * buf2 , * end ; <nl> AC3HeaderInfo hdr ; <nl> 
static int cbs_jpeg_split_fragment ( CodedBitstreamContext * ctx , <nl> if ( marker == JPEG_MARKER_SOS ) { <nl> length = AV_RB16 ( frag -> data + start ); <nl>  <nl> + if ( length > end - start ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> data_ref = NULL ; <nl> data = av_malloc ( end - start + <nl> AV_INPUT_BUFFER_PADDING_SIZE );
static int mjpeg_decode_app ( MJpegDecodeContext * s ) <nl>  <nl> if ( id == AV_RB32 (" JFIF ")) { <nl> int t_w , t_h , v1 , v2 ; <nl> + if ( len < 8 ) <nl> + goto out ; <nl> skip_bits (& s -> gb , 8 ); /* the trailing zero - byte */ <nl> v1 = get_bits (& s -> gb , 8 ); <nl> v2 = get_bits (& s -> gb , 8 );
static int h264_slice_header_init ( H264Context * h , int reinit ) <nl> return ret ; <nl> } <nl> } else { <nl> - if (( ret = ff_MPV_common_init ( s ) < 0 )) { <nl> + if (( ret = ff_MPV_common_init ( s )) < 0 ) { <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " ff_MPV_common_init () failed .\ n "); <nl> return ret ; <nl> }
static void search_for_quantizers_twoloop ( AVCodecContext * avctx , <nl> sce -> sf_idx [ i ] -= qstep ; <nl> } <nl> qstep >>= 1 ; <nl> - if (! qstep && tbits > destbits * 1 . 02 ) <nl> + if (! qstep && tbits > destbits * 1 . 02 && sce -> sf_idx [ 0 ] < 217 ) <nl> qstep = 1 ; <nl> - if ( sce -> sf_idx [ 0 ] >= 217 ) <nl> - break ; <nl> } while ( qstep ); <nl>  <nl> fflag = 0 ;
static int handle_eac3 ( MOVMuxContext * mov , AVPacket * pkt , MOVTrack * track ) <nl> info -> ec3_done = 1 ; <nl> goto concatenate ; <nl> } <nl> + } else { <nl> + if ( hdr -> substreamid != 0 ) { <nl> + avpriv_request_sample ( mov -> fc , " Multiple non EAC3 independent substreams "); <nl> + ret = AVERROR_PATCHWELCOME ; <nl> + goto end ; <nl> + } <nl> } <nl>  <nl> /* fill the info needed for the " dec3 " atom */
static int flashsv_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log_missing_feature ( avctx , " zlibprime_curr ", 1 ); <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl> + if (! s -> blocks && ( s -> zlibprime_curr || s -> zlibprime_prev )) { <nl> + av_log ( avctx , AV_LOG_ERROR , " no data available for zlib " <nl> + " priming \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> size --; // account for flags byte <nl> } <nl> 
static av_cold int mp_decode_init ( AVCodecContext * avctx ) <nl> int w4 = ( avctx -> width + 3 ) & ~ 3 ; <nl> int h4 = ( avctx -> height + 3 ) & ~ 3 ; <nl>  <nl> + if ( avctx -> extradata_size < 2 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " extradata too small \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> motionpixels_tableinit (); <nl> mp -> avctx = avctx ; <nl> ff_dsputil_init (& mp -> dsp , avctx );
ogm_header ( AVFormatContext * s , int idx ) <nl> if ( size > 52 ) { <nl> av_assert0 ( AV_INPUT_BUFFER_PADDING_SIZE <= 52 ); <nl> size -= 52 ; <nl> + if ( bytestream2_get_bytes_left (& p ) < size ) <nl> + return AVERROR_INVALIDDATA ; <nl> ff_alloc_extradata ( st -> codecpar , size ); <nl> bytestream2_get_buffer (& p , st -> codecpar -> extradata , st -> codecpar -> extradata_size ); <nl> }
static int decode_residual_block ( AVSContext * h , GetBitContext * gb , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> esc_code = get_ue_code ( gb , esc_golomb_order ); <nl> + if ( esc_code < 0 || esc_code > 32767 ) { <nl> + av_log ( h -> avctx , AV_LOG_ERROR , " esc_code invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> level = esc_code + ( run > r -> max_run ? 1 : r -> level_add [ run ]); <nl> while ( level > r -> inc_limit ) <nl> r ++;
static int xbm_decode_frame ( AVCodecContext * avctx , void * data , <nl> int number , len ; <nl>  <nl> ptr += strcspn ( ptr , "#"); <nl> - if ( sscanf ( ptr , "# define % 256s % u ", name , & number ) != 2 ) { <nl> + if ( sscanf ( ptr , "# define % 255s % u ", name , & number ) != 2 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Unexpected preprocessor directive \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int decode_wdlt ( GetByteContext * gb , uint8_t * frame , int width , int height <nl> y += skip_lines ; <nl> segments = bytestream2_get_le16 ( gb ); <nl> } <nl> + <nl> + if ( frame_end <= frame ) <nl> + return AVERROR_INVALIDDATA ; <nl> if ( segments & 0x8000 ) { <nl> frame [ width - 1 ] = segments & 0xFF ; <nl> segments = bytestream2_get_le16 ( gb );
void ff_rtsp_undo_setup ( AVFormatContext * s ) <nl> avformat_free_context ( rtpctx ); <nl> } else if ( rt -> transport == RTSP_TRANSPORT_RDT && CONFIG_RTPDEC ) <nl> ff_rdt_parse_close ( rtsp_st -> transport_priv ); <nl> - else if ( rt -> transport == RTSP_TRANSPORT_RAW && CONFIG_RTPDEC ) <nl> + else if ( rt -> transport == RTSP_TRANSPORT_RTP && CONFIG_RTPDEC ) <nl> ff_rtp_parse_close ( rtsp_st -> transport_priv ); <nl> } <nl> rtsp_st -> transport_priv = NULL ;
int ff_raw_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> st -> codec -> width = width ; <nl> st -> codec -> height = height ; <nl> st -> codec -> pix_fmt = pix_fmt ; <nl> - break ; <nl> fail : <nl> av_freep (& s1 -> video_size ); <nl> av_freep (& s1 -> pixel_format );
static int mkv_write_header ( AVFormatContext * s ) <nl> // initialize stream_duration fields <nl> mkv -> stream_durations = av_mallocz ( s -> nb_streams * sizeof ( int64_t )); <nl> mkv -> stream_duration_offsets = av_mallocz ( s -> nb_streams * sizeof ( int64_t )); <nl> + if (! mkv -> stream_durations || ! mkv -> stream_duration_offsets ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto fail ; <nl> + } <nl>  <nl> ret = mkv_write_tracks ( s ); <nl> if ( ret < 0 )
static int find_start_code ( const uint8_t * buf , int buf_size , <nl> buf [ buf_index + 2 ] == 1 ) <nl> break ; <nl>  <nl> - if ( buf_index + 3 >= buf_size ) <nl> + buf_index += 3 ; <nl> + <nl> + if ( buf_index >= buf_size ) <nl> return buf_size ; <nl>  <nl> - return buf_index + 3 ; <nl> + return buf_index ; <nl> } <nl>  <nl> static int get_avc_nalsize ( H264Context * h , const uint8_t * buf ,
fail : <nl> fail_at_end : <nl> av_freep (& pic -> codec_picture_params ); <nl> av_frame_free (& pic -> recon_image ); <nl> + av_buffer_unref (& pic -> output_buffer_ref ); <nl> + pic -> output_buffer = VA_INVALID_ID ; <nl> return err ; <nl> } <nl> 
static int yop_decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> s -> low_nibble = NULL ; <nl>  <nl> is_odd_frame = avpkt -> data [ 0 ]; <nl> + if ( is_odd_frame > 1 ){ <nl> + av_log ( avctx , AV_LOG_ERROR , " frame is too odd % d \ n ", is_odd_frame ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> firstcolor = s -> first_color [ is_odd_frame ]; <nl> palette = ( uint32_t *) s -> frame . data [ 1 ]; <nl> 
static void imc_get_coeffs ( AVCodecContext * avctx , <nl> av_log ( avctx , AV_LOG_WARNING , <nl> " Potential problem on band % i , coefficient % i " <nl> ": cw_len =% i \ n ", i , j , cw_len ); <nl> - } <nl> - <nl> - cw = get_bits (& q -> gb , cw_len ); <nl> + } else <nl> + cw = get_bits (& q -> gb , cw_len ); <nl> } <nl>  <nl> chctx -> codewords [ j ] = cw ;
static int qsv_decode_init ( AVCodecContext * avctx , QSVContext * q ) <nl> const AVPixFmtDescriptor * desc ; <nl> mfxSession session = NULL ; <nl> int iopattern = 0 ; <nl> - mfxVideoParam param = { { 0 } }; <nl> + mfxVideoParam param = { 0 }; <nl> int frame_width = avctx -> coded_width ; <nl> int frame_height = avctx -> coded_height ; <nl> int ret ;
static void guess_mv ( MpegEncContext * s ){ <nl> fixed [ mb_xy ]= f ; <nl> if ( f == MV_FROZEN ) <nl> num_avail ++; <nl> - else if ( s -> last_picture . data [ 0 ]){ <nl> + else if ( s -> last_picture . data [ 0 ] && s -> last_picture . motion_val [ 0 ]){ <nl> const int mb_y = mb_xy / s -> mb_stride ; <nl> const int mb_x = mb_xy % s -> mb_stride ; <nl> const int mot_index = ( mb_x + mb_y * mot_stride ) * mot_step ;
static void cdxl_decode_rgb ( CDXLVideoContext * c ) <nl> { <nl> uint32_t * new_palette = ( uint32_t *) c -> frame . data [ 1 ]; <nl>  <nl> + memset ( c -> frame . data [ 1 ], 0 , AVPALETTE_SIZE ); <nl> import_palette ( c , new_palette ); <nl> import_format ( c , c -> frame . linesize [ 0 ], c -> frame . data [ 0 ]); <nl> }
static int apng_write_packet ( AVFormatContext * format_context , AVPacket * packet ) <nl> int ret ; <nl>  <nl> if (! apng -> prev_packet ) { <nl> - apng -> prev_packet = av_malloc ( sizeof (* apng -> prev_packet )); <nl> + apng -> prev_packet = av_packet_alloc (); <nl> if (! apng -> prev_packet ) <nl> return AVERROR ( ENOMEM ); <nl> 
static int decode_byterun ( uint8_t * dst , int dst_size , <nl> } <nl> x += length ; <nl> } <nl> + if ( x < dst_size ) { <nl> + av_log ( NULL , AV_LOG_WARNING , " decode_byterun ended before plane size \ n "); <nl> + memset ( dst + x , 0 , dst_size - x ); <nl> + } <nl> return buf - buf_start ; <nl> } <nl> 
LF_IFUNC ( v , chroma_intra , depth , avx ) <nl> LF_FUNCS ( uint8_t , 8 ) <nl> LF_FUNCS ( uint16_t , 10 ) <nl>  <nl> -# if ARCH_X86_32 <nl> +# if ARCH_X86_32 && HAVE_YASM <nl> LF_FUNC ( v8 , luma , 8 , mmxext ) <nl> static void ff_deblock_v_luma_8_mmxext ( uint8_t * pix , int stride , int alpha , int beta , int8_t * tc0 ) <nl> {
static int tls_client_handshake_loop ( URLContext * h , int initial ) <nl> TLSContext * c = h -> priv_data ; <nl> TLSShared * s = & c -> tls_shared ; <nl> SECURITY_STATUS sspi_ret ; <nl> - SecBuffer outbuf [ 3 ]; <nl> + SecBuffer outbuf [ 3 ] = { 0 }; <nl> SecBufferDesc outbuf_desc ; <nl> SecBuffer inbuf [ 2 ]; <nl> SecBufferDesc inbuf_desc ;
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> payload_type , config ? config : ""); <nl> break ; <nl> case CODEC_ID_AAC : <nl> - if ( fmt && fmt -> oformat -> priv_class && <nl> + if ( fmt && fmt -> oformat && fmt -> oformat -> priv_class && <nl> av_opt_flag_is_set ( fmt -> priv_data , " rtpflags ", " latm ")) { <nl> config = latm_context2config ( c ); <nl> if (! config )
static int crypto_open ( URLContext * h , const char * uri , int flags ) <nl>  <nl> return 0 ; <nl> err : <nl> - av_free ( c -> key ); <nl> - av_free ( c -> iv ); <nl> + av_freep (& c -> key ); <nl> + av_freep (& c -> iv ); <nl> return ret ; <nl> } <nl> 
int avformat_find_stream_info ( AVFormatContext * ic , AVDictionary ** options ) <nl> least one frame of codec data , this makes sure the codec initializes <nl> the channel configuration and does not only trust the values from the container . <nl> */ <nl> - try_decode_frame ( st , pkt , ( options && i <= orig_nb_streams )? & options [ i ] : NULL ); <nl> + try_decode_frame ( st , pkt , ( options && i < orig_nb_streams )? & options [ i ] : NULL ); <nl>  <nl> st -> codec_info_nb_frames ++; <nl> count ++;
void ff_mpeg_unref_picture ( MpegEncContext * s , Picture * pic ) <nl>  <nl> av_buffer_unref (& pic -> hwaccel_priv_buf ); <nl>  <nl> + if ( pic -> needs_realloc ) <nl> + free_picture_tables ( pic ); <nl> + <nl> memset (( uint8_t *) pic + off , 0 , sizeof (* pic ) - off ); <nl> } <nl> 
static void do_video_out ( AVFormatContext * s , <nl> } else <nl> in_picture = next_picture ; <nl>  <nl> + if (! in_picture ) <nl> + return ; <nl> + <nl> in_picture -> pts = ost -> sync_opts ; <nl>  <nl> # if 1
static int ape_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl> if ( s -> fileversion < 3950 ) // previous versions overread two bytes <nl> buf_size += 2 ; <nl> - av_fast_malloc (& s -> data , & s -> data_size , buf_size ); <nl> + av_fast_padded_malloc (& s -> data , & s -> data_size , buf_size ); <nl> if (! s -> data ) <nl> return AVERROR ( ENOMEM ); <nl> s -> dsp . bswap_buf (( uint32_t *) s -> data , ( const uint32_t *) buf , buf_size >> 2 );
static void init_uni_ac_vlc ( RLTable * rl , uint8_t * uni_ac_vlc_len ){ <nl> for ( i = 0 ; i < 128 ; i ++){ <nl> int level = i - 64 ; <nl> int run ; <nl> + if (! level ) <nl> + continue ; <nl> for ( run = 0 ; run < 64 ; run ++){ <nl> int len , bits , code ; <nl> 
int ff_dirac_golomb_read_16bit ( DiracGolombLUT * lut_ctx , const uint8_t * buf , <nl> if (( c_idx + 1 ) > coeffs ) <nl> return c_idx ; <nl>  <nl> + if ( res_bits >= RSIZE_BITS ) <nl> + res_bits = res = 0 ; <nl> + <nl> if ( res_bits && l -> sign ) { <nl> int32_t coeff = 1 ; <nl> APPEND_RESIDUE ( res , l -> preamble );
decode_intra_mb : <nl>  <nl> dquant = get_se_golomb (& sl -> gb ); <nl>  <nl> - sl -> qscale += dquant ; <nl> + sl -> qscale += ( unsigned ) dquant ; <nl>  <nl> if ((( unsigned ) sl -> qscale ) > max_qp ){ <nl> if ( sl -> qscale < 0 ) sl -> qscale += max_qp + 1 ;
enum AVCodecID av_codec_get_id ( const AVCodecTag * const * tags , unsigned int tag ) <nl> static void compute_chapters_end ( AVFormatContext * s ) <nl> { <nl> unsigned int i , j ; <nl> - int64_t max_time = s -> duration + <nl> + int64_t max_time = 0 ; <nl> + <nl> + if ( s -> duration > 0 ) <nl> + max_time = s -> duration + <nl> (( s -> start_time == AV_NOPTS_VALUE ) ? 0 : s -> start_time ); <nl>  <nl> for ( i = 0 ; i < s -> nb_chapters ; i ++)
static void apply_channel_coupling ( AC3EncodeContext * s ) <nl> # else <nl> int32_t (* fixed_cpl_coords )[ AC3_MAX_CHANNELS ][ 16 ] = cpl_coords ; <nl> # endif <nl> - int blk , ch , bnd , i , j ; <nl> + int av_uninit ( blk ), ch , bnd , i , j ; <nl> CoefSumType energy [ AC3_MAX_BLOCKS ][ AC3_MAX_CHANNELS ][ 16 ] = {{{ 0 }}}; <nl> int cpl_start , num_cpl_coefs ; <nl> 
static int submit_stats ( AVCodecContext * avctx ) <nl> } <nl> h -> stats_size = strlen ( avctx -> stats_in ) * 3 / 4 ; <nl> h -> stats = av_malloc ( h -> stats_size ); <nl> + if (! h -> stats ) { <nl> + h -> stats_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> h -> stats_size = av_base64_decode ( h -> stats , avctx -> stats_in , h -> stats_size ); <nl> } <nl> while ( h -> stats_size - h -> stats_offset > 0 ) {
static int open_slave ( AVFormatContext * avf , char * slave , TeeSlave * tee_slave ) <nl>  <nl> end : <nl> av_free ( format ); <nl> + av_free ( select ); <nl> av_dict_free (& options ); <nl> return ret ; <nl> }
static int dv_write_header ( AVFormatContext * s ) <nl> break ; <nl> } <nl> } <nl> - if ( tcr ) <nl> - return av_timecode_init_from_string (& dvc -> tc , rate , tcr -> value , s ); <nl> + if ( tcr && av_timecode_init_from_string (& dvc -> tc , rate , tcr -> value , s ) >= 0 ) <nl> + return 0 ; <nl> return av_timecode_init (& dvc -> tc , rate , 0 , 0 , s ); <nl> } <nl> 
static inline av_const SoftFloat av_sub_sf ( SoftFloat a , SoftFloat b ){ <nl> */ <nl> static inline av_const SoftFloat av_int2sf ( int v , int frac_bits ){ <nl> int exp_offset = 0 ; <nl> - if ( v == INT_MIN ){ <nl> + if ( v <= INT_MIN + 1 ){ <nl> exp_offset = 1 ; <nl> v >>= 1 ; <nl> }
static int load_apply_palette ( FFFrameSync * fs ) <nl>  <nl> error : <nl> av_frame_free (& master ); <nl> - av_frame_free (& second ); <nl> return ret ; <nl> } <nl> 
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( avctx -> codec_id == AV_CODEC_ID_VC1 || avctx -> codec_id == AV_CODEC_ID_VC1IMAGE ) { <nl> int buf_size2 = 0 ; <nl> buf2 = av_mallocz ( buf_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> + if (! buf2 ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> if ( IS_MARKER ( AV_RB32 ( buf ))) { /* frame starts with marker and needs to be parsed */ <nl> const uint8_t * start , * end , * next ;
av_cold int ff_yuv2rgb_c_init_tables ( SwsContext * c , const int inv_table [ 4 ], int <nl> uint8_t * y_table ; <nl> uint16_t * y_table16 ; <nl> uint32_t * y_table32 ; <nl> - int i , base , rbase , gbase , bbase , abase , needAlpha ; <nl> + int i , base , rbase , gbase , bbase , av_uninit ( abase ), needAlpha ; <nl> const int yoffs = fullRange ? 384 : 326 ; <nl>  <nl> int64_t crv = inv_table [ 0 ];
static av_cold int cook_decode_init ( AVCodecContext * avctx ) <nl> int extradata_size = avctx -> extradata_size ; <nl> int s = 0 ; <nl> unsigned int channel_mask = 0 ; <nl> - int samples_per_frame ; <nl> + int samples_per_frame = 0 ; <nl> int ret ; <nl> q -> avctx = avctx ; <nl> 
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> flush_dpb ( s -> avctx ); <nl> ff_MPV_common_end ( s ); <nl> h -> list_count = 0 ; <nl> + h -> current_slice = 0 ; <nl> } <nl> if (! s -> context_initialized ) { <nl> if ( h != h0 ) {
AVInputFormat ff_ivr_demuxer = { <nl> . read_probe = ivr_probe , <nl> . read_header = ivr_read_header , <nl> . read_packet = ivr_read_packet , <nl> + . read_close = rm_read_close , <nl> . extensions = " ivr ", <nl> };
int ff_thread_get_buffer ( AVCodecContext * avctx , AVFrame * f ) <nl> avctx -> get_buffer == avcodec_default_get_buffer ) { <nl> err = avctx -> get_buffer ( avctx , f ); <nl> } else { <nl> + pthread_mutex_lock (& p -> progress_mutex ); <nl> p -> requested_frame = f ; <nl> p -> state = STATE_GET_BUFFER ; <nl> - pthread_mutex_lock (& p -> progress_mutex ); <nl> pthread_cond_broadcast (& p -> progress_cond ); <nl>  <nl> while ( p -> state != STATE_SETTING_UP )
end : <nl> int ff_get_buffer ( AVCodecContext * avctx , AVFrame * frame , int flags ) <nl> { <nl> int ret = get_buffer_internal ( avctx , frame , flags ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " get_buffer () failed \ n "); <nl> + frame -> width = frame -> height = 0 ; <nl> + } <nl> return ret ; <nl> } <nl> 
static int query_formats ( AVFilterContext * ctx ) <nl>  <nl> static int glyph_enu_free ( void * opaque , void * elem ) <nl> { <nl> + Glyph * glyph = elem ; <nl> + <nl> + FT_Done_Glyph (* glyph -> glyph ); <nl> + av_freep (& glyph -> glyph ); <nl> av_free ( elem ); <nl> return 0 ; <nl> }
static int decode_lt_rps ( HEVCContext * s , LongTermRPS * rps , GetBitContext * gb ) <nl> nb_sps = get_ue_golomb_long ( gb ); <nl> nb_sh = get_ue_golomb_long ( gb ); <nl>  <nl> - if ( nb_sh + nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> + if ( nb_sh + ( uint64_t ) nb_sps > FF_ARRAY_ELEMS ( rps -> poc )) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> rps -> nb_refs = nb_sh + nb_sps ;
AVInputFormat * av_probe_input_format3 ( AVProbeData * pd , int is_opened , int * score <nl> AVProbeData lpd = * pd ; <nl> AVInputFormat * fmt1 = NULL , * fmt ; <nl> int score , nodat = 0 , score_max = 0 ; <nl> + const static uint8_t zerobuffer [ AVPROBE_PADDING_SIZE ]; <nl> + <nl> + if (! lpd . buf ) <nl> + lpd . buf = zerobuffer ; <nl>  <nl> if ( lpd . buf_size > 10 && ff_id3v2_match ( lpd . buf , ID3v2_DEFAULT_MAGIC )) { <nl> int id3len = ff_id3v2_tag_len ( lpd . buf );
static int decode_5 ( SANMVideoContext * ctx ) <nl> # if HAVE_BIGENDIAN <nl> npixels = ctx -> npixels ; <nl> frm = ctx -> frm0 ; <nl> - while ( npixels --) <nl> - * frm ++ = av_bswap16 (* frm ); <nl> + while ( npixels --) { <nl> + * frm = av_bswap16 (* frm ); <nl> + frm ++; <nl> + } <nl> # endif <nl>  <nl> return 0 ;
static int decode_mime_header ( AMRWBContext * ctx , const uint8_t * buf ) <nl> { <nl> /* Decode frame header ( 1st octet ) */ <nl> ctx -> fr_cur_mode = buf [ 0 ] >> 3 & 0x0F ; <nl> - ctx -> fr_quality = ( buf [ 0 ] & 0x4 ) != 0x4 ; <nl> + ctx -> fr_quality = ( buf [ 0 ] & 0x4 ) == 0x4 ; <nl>  <nl> return 1 ; <nl> }
static int ljpeg_decode_rgb_scan ( MJpegDecodeContext * s , int nb_components , int p <nl> int resync_mb_y = 0 ; <nl> int resync_mb_x = 0 ; <nl>  <nl> + if ( s -> nb_components != 3 && s -> nb_components != 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + if ( s -> v_max != 1 || s -> h_max != 1 || ! s -> lossless ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> + <nl> s -> restart_count = s -> restart_interval ; <nl>  <nl> av_fast_malloc (& s -> ljpeg_buffer , & s -> ljpeg_buffer_size ,
intra : <nl> } <nl> end : <nl>  <nl> + if ( get_bits_left (& s -> gb ) < 0 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /* per - MB end of slice check */ <nl> { <nl> int v = show_bits (& s -> gb , 16 );
static av_cold int encode_init ( AVCodecContext * avc_context ) <nl>  <nl> /* Set up the output AVFrame */ <nl> avc_context -> coded_frame = av_frame_alloc (); <nl> + if (! avc_context -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> return 0 ; <nl> }
int avcodec_default_reget_buffer ( AVCodecContext * s , AVFrame * pic ){ <nl> return s -> get_buffer ( s , pic ); <nl> } <nl>  <nl> + assert ( s -> pix_fmt == pic -> pix_fmt ); <nl> + <nl> /* If internal buffer type return the same buffer */ <nl> if ( pic -> type == FF_BUFFER_TYPE_INTERNAL ) { <nl> if ( s -> pkt ) pic -> pkt_pts = s -> pkt -> pts ;
int ff_mp4_read_dec_config_descr ( AVFormatContext * fc , AVStream * st , AVIOContext <nl> return AVERROR ( ENOMEM ); <nl> avio_read ( pb , st -> codec -> extradata , len ); <nl> if ( st -> codec -> codec_id == AV_CODEC_ID_AAC ) { <nl> - MPEG4AudioConfig cfg ; <nl> + MPEG4AudioConfig cfg = { 0 }; <nl> avpriv_mpeg4audio_get_config (& cfg , st -> codec -> extradata , <nl> st -> codec -> extradata_size * 8 , 1 ); <nl> st -> codec -> channels = cfg . channels ;
static int mm_decode_intra ( MmContext * s , int half_horiz , int half_vert ) <nl> */ <nl> static int mm_decode_inter ( MmContext * s , int half_horiz , int half_vert ) <nl> { <nl> - int data_off = bytestream2_get_le16 (& s -> gb ), y ; <nl> + int data_off = bytestream2_get_le16 (& s -> gb ); <nl> + int y = 0 ; <nl> GetByteContext data_ptr ; <nl>  <nl> if ( bytestream2_get_bytes_left (& s -> gb ) < data_off )
static int rtsp_read_header ( AVFormatContext * s , <nl> rt -> real_setup_cache = av_mallocz ( 2 * s -> nb_streams * sizeof (* rt -> real_setup_cache )); <nl> if (! rt -> real_setup_cache ) <nl> return AVERROR ( ENOMEM ); <nl> - rt -> real_setup = rt -> real_setup_cache + s -> nb_streams * sizeof (* rt -> real_setup ); <nl> + rt -> real_setup = rt -> real_setup_cache + s -> nb_streams ; <nl>  <nl> if ( ap -> initial_pause ) { <nl> /* do not start immediately */
static int decode_frame_mp3on4 ( AVCodecContext * avctx , void * data , <nl>  <nl> avpriv_mpegaudio_decode_header (( MPADecodeHeader *) m , header ); <nl>  <nl> - if ( ch + m -> nb_channels > avctx -> channels ) { <nl> + if ( ch + m -> nb_channels > avctx -> channels || s -> coff [ fr ] + m -> nb_channels > avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame channel count exceeds codec " <nl> " channel count \ n "); <nl> return AVERROR_INVALIDDATA ;
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> for ( j = 0 ; j < c -> slices ; j ++) { <nl> slice_end = bytestream2_get_le32u (& gb ); <nl> if ( slice_end < 0 || slice_end < slice_start || <nl> - bytestream2_get_bytes_left (& gb ) < slice_end ) { <nl> + bytestream2_get_bytes_left (& gb ) < slice_end + 1024LL ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Incorrect slice size \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
retry : <nl> if ( snprintf ( str , str_size_alloc , "% f ", val ) >= str_size_alloc ) { <nl> av_log ( c -> fc , AV_LOG_ERROR , <nl> " Failed to store the float32 number (% f ) in string .\ n ", val ); <nl> + av_free ( str ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> } else {
void ff_ivi_recompose53 ( const IVIPlaneDesc * plane , uint8_t * dst , <nl> b3_ptr = plane -> bands [ 3 ]. buf ; <nl>  <nl> for ( y = 0 ; y < plane -> height ; y += 2 ) { <nl> + <nl> + if ( y + 2 >= plane -> height ) <nl> + pitch = 0 ; <nl> /* load storage variables with values */ <nl> if ( num_bands > 0 ) { <nl> b0_1 = b0_ptr [ 0 ];
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> if ( ast -> sub_packet_size <= 0 || <nl> ast -> sub_packet_size > ast -> audio_framesize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( ast -> audio_framesize % ast -> sub_packet_size ) <nl> + return AVERROR_INVALIDDATA ; <nl> break ; <nl> case DEINT_ID_SIPR : <nl> case DEINT_ID_INT0 :
av_cold void ff_psy_preprocess_end ( struct FFPsyPreprocessContext * ctx ) <nl> for ( i = 0 ; i < ctx -> avctx -> channels ; i ++) <nl> ff_iir_filter_free_state ( ctx -> fstate [ i ]); <nl> av_freep (& ctx -> fstate ); <nl> + av_free ( ctx ); <nl> } <nl> 
static av_always_inline void decode_bgr_1 ( HYuvContext * s , int count , <nl> index = SHOW_UBITS ( re , & s -> gb , VLC_BITS ); <nl> VLC_INTERN ( s -> temp [ 0 ][ 4 * i + A ], s -> vlc [ 2 ]. table , <nl> & s -> gb , re , VLC_BITS , 3 ); <nl> - } <nl> + } else <nl> + s -> temp [ 0 ][ 4 * i + A ] = 0 ; <nl> } <nl> } <nl> CLOSE_READER ( re , & s -> gb );
static int fraps2_decode_plane ( FrapsContext * s , uint8_t * dst , int stride , int w , <nl> */ <nl> if ( j ) dst [ i ] += dst [ i - stride ]; <nl> else if ( Uoff ) dst [ i ] += 0x80 ; <nl> + if ( get_bits_left (& gb ) < 0 ) { <nl> + free_vlc (& vlc ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl> dst += stride ; <nl> }
static int ape_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> if ( s -> pb -> eof_reached ) <nl> return AVERROR_EOF ; <nl> - if ( ape -> currentframe > ape -> totalframes ) <nl> + if ( ape -> currentframe >= ape -> totalframes ) <nl> return AVERROR_EOF ; <nl>  <nl> if ( avio_seek ( s -> pb , ape -> frames [ ape -> currentframe ]. pos , SEEK_SET ) < 0 )
 <nl> # include < stdarg . h > <nl> # include " avcodec . h " <nl> -# include " movtext . h " <nl> # include " libavutil / avstring . h " <nl> +# include " libavutil / intreadwrite . h " <nl> # include " ass_split . h " <nl> # include " ass . h " <nl> 
static void add_input_streams ( OptionsContext * o , AVFormatContext * ic ) <nl> if (! ist -> hwaccel_device ) <nl> exit_program ( 1 ); <nl> } <nl> + ist -> hwaccel_pix_fmt = AV_PIX_FMT_NONE ; <nl>  <nl> break ; <nl> case AVMEDIA_TYPE_AUDIO :
static void force_codec_ids ( AVFormatContext * s , AVStream * st ) <nl> break ; <nl> case AVMEDIA_TYPE_DATA : <nl> if ( s -> data_codec_id ) <nl> - st -> codec -> codec_id = s -> data_codec_id ; <nl> + st -> codecpar -> codec_id = s -> data_codec_id ; <nl> break ; <nl> } <nl> }
static av_always_inline SoftFloat autocorr_calc ( int64_t accu ) <nl>  <nl> round = 1U << ( nz - 1 ); <nl> mant = ( int )(( accu + round ) >> nz ); <nl> - mant = ( mant + 0x40 )>> 7 ; <nl> + mant = ( mant + 0x40LL )>> 7 ; <nl> mant *= 64 ; <nl> expo = nz + 15 ; <nl> return av_int2sf ( mant , 30 - expo );
static int mov_read_header ( AVFormatContext * s ) <nl> if ( s -> streams [ j ]-> id == sc -> timecode_track ) <nl> tmcd_st_id = j ; <nl>  <nl> - if ( tmcd_st_id < 0 ) <nl> + if ( tmcd_st_id < 0 || tmcd_st_id == i ) <nl> continue ; <nl> tcr = av_dict_get ( s -> streams [ tmcd_st_id ]-> metadata , " timecode ", NULL , 0 ); <nl> if ( tcr )
static void init_input_filter ( FilterGraph * fg , AVFilterInOut * in ) <nl> char * p ; <nl> int file_idx = strtol ( in -> name , & p , 0 ); <nl>  <nl> - if ( file_idx < 0 || file_idx > nb_input_files ) { <nl> + if ( file_idx < 0 || file_idx >= nb_input_files ) { <nl> av_log ( NULL , AV_LOG_FATAL , " Invalid file index % d in filtegraph description % s .\ n ", <nl> file_idx , fg -> graph_desc ); <nl> exit_program ( 1 );
static int qsv_decode ( AVCodecContext * avctx , QSVContext * q , <nl>  <nl> do { <nl> ret = get_surface ( avctx , q , & insurf ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& sync ); <nl> return ret ; <nl> + } <nl>  <nl> ret = MFXVideoDECODE_DecodeFrameAsync ( q -> session , avpkt -> size ? & bs : NULL , <nl> insurf , & outsurf , sync );
int ff_ass_split_override_codes ( const ASSCodesCallbacks * callbacks , void * priv , <nl> char new_line [ 2 ]; <nl> int text_len = 0 ; <nl>  <nl> - while (* buf ) { <nl> + while ( buf && * buf ) { <nl> if ( text && callbacks -> text && <nl> ( sscanf ( buf , "\\% 1 [ nN ]", new_line ) == 1 || <nl> ! strncmp ( buf , "{\\", 2 ))) {
static int bit_allocation ( IMCContext * q , IMCChannel * chctx , <nl> iacc += chctx -> bandWidthT [ i ]; <nl> summa += chctx -> bandWidthT [ i ] * chctx -> flcoeffs4 [ i ]; <nl> } <nl> + <nl> + if (! iacc ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> chctx -> bandWidthT [ BANDS - 1 ] = 0 ; <nl> summa = ( summa * 0 . 5 - freebits ) / iacc ; <nl> 
static int decode_hq_slice ( DiracContext * s , DiracSlice * slice , uint8_t * tmp_buf ) <nl> skip_bits_long ( gb , 8 * s -> highquality . prefix_bytes ); <nl> quant_idx = get_bits ( gb , 8 ); <nl>  <nl> - if ( quant_idx > DIRAC_MAX_QUANT_INDEX ) { <nl> + if ( quant_idx > DIRAC_MAX_QUANT_INDEX - 1 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid quantization index - % i \ n ", quant_idx ); <nl> return AVERROR_INVALIDDATA ; <nl> }
av_cold int ffv1_init_slice_contexts ( FFV1Context * f ) <nl> int i ; <nl>  <nl> f -> slice_count = f -> num_h_slices * f -> num_v_slices ; <nl> + if ( f -> slice_count <= 0 ) { <nl> + av_log ( f -> avctx , AV_LOG_ERROR , " Invalid number of slices \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl>  <nl> for ( i = 0 ; i < f -> slice_count ; i ++) { <nl> FFV1Context * fs = av_mallocz ( sizeof (* fs ));
int av_cold ff_wma_get_frame_len_bits ( int sample_rate , int version , <nl> } else if ( sample_rate <= 22050 || <nl> ( sample_rate <= 32000 && version == 1 )) { <nl> frame_len_bits = 10 ; <nl> - } else if ( sample_rate <= 48000 ) { <nl> + } else if ( sample_rate <= 48000 || version < 3 ) { <nl> frame_len_bits = 11 ; <nl> } else if ( sample_rate <= 96000 ) { <nl> frame_len_bits = 12 ;
static int dirac_decode_picture_header ( DiracContext * s ) <nl> if (! s -> all_frames [ j ]. avframe . data [ 0 ]) { <nl> s -> ref_pics [ i ] = & s -> all_frames [ j ]; <nl> s -> avctx -> get_buffer ( s -> avctx , & s -> ref_pics [ i ]-> avframe ); <nl> + break ; <nl> } <nl> } <nl> 
void avcodec_free_context ( AVCodecContext ** pavctx ) <nl>  <nl> av_freep (& avctx -> extradata ); <nl> av_freep (& avctx -> subtitle_header ); <nl> + av_freep (& avctx -> intra_matrix ); <nl> + av_freep (& avctx -> inter_matrix ); <nl> + av_freep (& avctx -> rc_override ); <nl>  <nl> av_freep ( pavctx ); <nl> }
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl> fail : <nl> if ( filename != s -> filename ) <nl> av_freep (& filename ); <nl> + if ( rc ) <nl> + RTMP_Close ( r ); <nl> + <nl> return rc ; <nl> } <nl> 
static int expand_rle_row16 ( SgiState * s , uint16_t * out_buf , <nl> break ; <nl>  <nl> /* Check for buffer overflow . */ <nl> - if ( pixelstride * ( count - 1 ) >= len ) { <nl> + if ( out_end - out_buf <= pixelstride * ( count - 1 )) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " Invalid pixel count .\ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
int av_open_input_file ( AVFormatContext ** ic_ptr , const char * filename , <nl> int err ; <nl> AVDictionary * opts = convert_format_parameters ( ap ); <nl>  <nl> - if (! ap -> prealloced_context ) <nl> + if (! ap || ! ap -> prealloced_context ) <nl> * ic_ptr = NULL ; <nl>  <nl> err = avformat_open_input ( ic_ptr , filename , fmt , & opts );
static inline int parse_command_line ( AVFormatContext * s , const char * line , <nl> RTSPState * rt = s -> priv_data ; <nl> const char * linept , * searchlinept ; <nl> linept = strchr ( line , ' '); <nl> + <nl> + if (! linept ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> if ( linept - line > methodsize - 1 ) { <nl> av_log ( s , AV_LOG_ERROR , " Method string too long \ n "); <nl> return AVERROR ( EIO );
static int read_extra_header ( FFV1Context * f ) <nl> f -> quant_table_count = get_symbol ( c , state , 0 ); <nl> if ( f -> quant_table_count > ( unsigned ) MAX_QUANT_TABLES || ! f -> quant_table_count ) { <nl> av_log ( f -> avctx , AV_LOG_ERROR , " quant table count % d is invalid \ n ", f -> quant_table_count ); <nl> + f -> quant_table_count = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
static int film_read_packet ( AVFormatContext * s , <nl> av_free ( film -> stereo_buffer ); <nl> film -> stereo_buffer_size = sample -> sample_size ; <nl> film -> stereo_buffer = av_malloc ( film -> stereo_buffer_size ); <nl> + if (! film -> stereo_buffer ) { <nl> + film -> stereo_buffer_size = 0 ; <nl> + return AVERROR ( ENOMEM ); <nl> + } <nl> } <nl>  <nl> pkt -> pos = avio_tell ( pb );
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> s -> buf = & ptr ; <nl> s -> buf_size = pkt -> size ; <nl>  <nl> - if ( check_size ( s , 8 )) <nl> + if ( check_size ( s , 8 )) { <nl> + ret = AVERROR ( EINVAL ); <nl> goto fail ; <nl> + } <nl>  <nl> // write header <nl> bytestream_put_le16 (& ptr , 0x4949 );
static av_cold int decode_end ( AVCodecContext * avctx ) <nl> { <nl> MadContext * t = avctx -> priv_data ; <nl> av_frame_free (& t -> last_frame ); <nl> - av_free ( t -> bitstream_buf ); <nl> + av_freep (& t -> bitstream_buf ); <nl> return 0 ; <nl> } <nl> 
int ff_mjpeg_find_marker ( MJpegDecodeContext * s , <nl> while ( b < t ) { <nl> uint8_t x = src [ b ++]; <nl> put_bits (& pb , 8 , x ); <nl> - if ( x == 0xFF ) { <nl> + if ( x == 0xFF && b < t ) { <nl> x = src [ b ++]; <nl> if ( x & 0x80 ) { <nl> av_log ( s -> avctx , AV_LOG_WARNING , " Invalid escape sequence \ n ");
int ff_h264_decode_slice_header ( H264Context * h , H264SliceContext * sl ) <nl> } <nl>  <nl> if ( h -> context_initialized && needs_reinit ) { <nl> + h -> context_initialized = 0 ; <nl> if ( sl != h -> slice_ctx ) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , <nl> " changing width % d -> % d / height % d -> % d on "
static int ivr_probe ( AVProbeData * p ) <nl> static int ivr_read_header ( AVFormatContext * s ) <nl> { <nl> unsigned tag , type , len , tlen , value ; <nl> - int i , j , n , count , nb_streams , ret ; <nl> + int i , j , n , count , nb_streams = 0 , ret ; <nl> uint8_t key [ 256 ], val [ 256 ]; <nl> AVIOContext * pb = s -> pb ; <nl> AVStream * st ;
SwsContext * sws_alloc_context ( void ) <nl> { <nl> SwsContext * c = av_mallocz ( sizeof ( SwsContext )); <nl>  <nl> - c -> av_class = & sws_context_class ; <nl> - av_opt_set_defaults ( c ); <nl> + if ( c ) { <nl> + c -> av_class = & sws_context_class ; <nl> + av_opt_set_defaults ( c ); <nl> + } <nl>  <nl> return c ; <nl> }
int ff_h264_build_ref_list ( H264Context * h , H264SliceContext * sl ) <nl> break ; <nl> } <nl> default : <nl> - av_assert1 ( 0 ); <nl> + av_assert0 ( 0 ); <nl> } <nl>  <nl> if ( i < 0 ) {
const char * av_opencl_errstr ( cl_int status ) <nl> static void free_device_list ( AVOpenCLDeviceList * device_list ) <nl> { <nl> int i , j ; <nl> - if (! device_list ) <nl> + if (! device_list || ! device_list -> platform_node ) <nl> return ; <nl> for ( i = 0 ; i < device_list -> platform_num ; i ++) { <nl> if (! device_list -> platform_node [ i ])
static int mv_read_header ( AVFormatContext * avctx ) <nl> uint32_t pos = avio_rb32 ( pb ); <nl> uint32_t asize = avio_rb32 ( pb ); <nl> uint32_t vsize = avio_rb32 ( pb ); <nl> + if ( avio_feof ( pb )) <nl> + return AVERROR_INVALIDDATA ; <nl> avio_skip ( pb , 8 ); <nl> av_add_index_entry ( ast , pos , timestamp , asize , 0 , AVINDEX_KEYFRAME ); <nl> av_add_index_entry ( vst , pos + asize , i , vsize , 0 , AVINDEX_KEYFRAME );
static int get_stream_info ( AVCodecContext * avctx ) <nl>  <nl> for ( i = 0 ; i < info -> numChannels ; i ++) { <nl> AUDIO_CHANNEL_TYPE ctype = info -> pChannelType [ i ]; <nl> - if ( ctype <= ACT_NONE || ctype > FF_ARRAY_ELEMS ( channel_counts )) { <nl> + if ( ctype <= ACT_NONE || ctype >= FF_ARRAY_ELEMS ( channel_counts )) { <nl> av_log ( avctx , AV_LOG_WARNING , " unknown channel type \ n "); <nl> break ; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
void decode_mvs ( VP8Context * s , VP8Macroblock * mb , int mb_x , int mb_y ) <nl>  <nl> AV_ZERO32 (& near_mv [ 0 ]); <nl> AV_ZERO32 (& near_mv [ 1 ]); <nl> + AV_ZERO32 (& near_mv [ 2 ]); <nl>  <nl> /* Process MB on top , left and top - left */ <nl> # define MV_EDGE_CHECK ( n )\
static int cdxl_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> if ( cdxl -> framerate ) <nl> st -> duration = frames ; <nl> else <nl> - st -> duration = frames * audio_size ; <nl> + st -> duration = frames * ( int64_t ) audio_size ; <nl> } <nl> st -> start_time = 0 ; <nl> cdxl -> video_stream_index = st -> index ;
static int mp3_read_probe ( AVProbeData * p ) <nl> const uint8_t * buf , * buf0 , * buf2 , * end ; <nl> AVCodecContext * avctx = avcodec_alloc_context3 ( NULL ); <nl>  <nl> + if (! avctx ) <nl> + return 0 ; <nl> + <nl> buf0 = p -> buf ; <nl> end = p -> buf + p -> buf_size - sizeof ( uint32_t ); <nl> while ( buf0 < end && !* buf0 )
static int matroska_parse_cluster_incremental ( MatroskaDemuxContext * matroska ) <nl> } <nl> } <nl>  <nl> - if ( res < 0 ) matroska -> done = 1 ; <nl> return res ; <nl> } <nl> 
static int mov_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> /* copy frame to create needed atoms */ <nl> trk -> vosLen = size ; <nl> trk -> vosData = av_malloc ( size ); <nl> + if (! trk -> vosData ) <nl> + return AVERROR ( ENOMEM ); <nl> memcpy ( trk -> vosData , pkt -> data , size ); <nl> } <nl> 
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> output = s -> plane [ plane ]. subband [ 0 ]; <nl> for ( i = 0 ; i < lowpass_height * 2 ; i ++) { <nl> for ( j = 0 ; j < lowpass_width * 2 ; j ++) <nl> - output [ j ] <<= 2 ; <nl> + output [ j ] *= 4 ; <nl>  <nl> output += lowpass_width * 2 ; <nl> }
int av_add_index_entry ( AVStream * st , <nl> memmove ( entries + index + 1 , entries + index , sizeof ( AVIndexEntry )*( st -> nb_index_entries - index )); <nl> } <nl> st -> nb_index_entries ++; <nl> + } else { <nl> + if ( ie -> pos == pos && distance < ie -> min_distance ) // dont reduce the distance <nl> + distance = ie -> min_distance ; <nl> } <nl> } else { <nl> index = st -> nb_index_entries ++;
static int decode_frame ( AVCodecContext * avctx , <nl> t = tm2_read_stream ( l , l -> buffer + offset , tm2_stream_order [ i ], <nl> buf_size - offset ); <nl> if ( t < 0 ) { <nl> + int j = tm2_stream_order [ i ]; <nl> + memset ( l -> tokens [ j ], 0 , sizeof (** l -> tokens ) * l -> tok_lens [ j ]); <nl> return t ; <nl> } <nl> offset += t ;
static int decode_cell ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> /* of the predicted cell in order to avoid overflows . */ <nl> if ( vq_index >= 8 && ref_block ) { <nl> for ( x = 0 ; x < cell -> width << 2 ; x ++) <nl> - ref_block [ x ] = requant_tab [ vq_index & 7 ][ ref_block [ x ]]; <nl> + ref_block [ x ] = requant_tab [ vq_index & 7 ][ ref_block [ x ] & 127 ]; <nl> } <nl>  <nl> error = IV3_NOERR ;
static av_cold int MPA_encode_init ( AVCodecContext * avctx ) <nl> s -> freq_index = i ; <nl>  <nl> /* encoding bitrate & frequency */ <nl> - for ( i = 0 ; i < 15 ; i ++) { <nl> + for ( i = 1 ; i < 15 ; i ++) { <nl> if ( avpriv_mpa_bitrate_tab [ s -> lsf ][ 1 ][ i ] == bitrate ) <nl> break ; <nl> }
static inline uint32_t celt_icwrsi ( uint32_t N , uint32_t K , const int * y ) <nl> idx += CELT_PVQ_U ( N - i , sum ) + ( y [ i ] < 0 )* i_s ; <nl> sum += FFABS ( y [ i ]); <nl> } <nl> - av_assert0 ( sum == K ); <nl> return idx ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> AVFrame * const p = data ; <nl> int mb_x , mb_y , ret ; <nl>  <nl> - if ( buf_size * 8LL < a -> mb_height2 * a -> mb_width2 * 13LL ) <nl> + if ( buf_size * 8LL < a -> mb_height * a -> mb_width * 13LL ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 )
read_header : <nl>  <nl> // XXX FIXME factorize , this looks very similar to the EOI code <nl>  <nl> + if (! s -> got_picture ) { <nl> + av_log ( avctx , AV_LOG_WARNING , " no picture \ n "); <nl> + return buf_size ; <nl> + } <nl> + <nl> * picture = * s -> picture_ptr ; <nl> * data_size = sizeof ( AVFrame ); <nl> 
static void ffmpeg_cleanup ( int ret ) <nl> avcodec_free_context (& ost -> enc_ctx ); <nl> avcodec_parameters_free (& ost -> ref_par ); <nl>  <nl> - while ( av_fifo_size ( ost -> muxing_queue )) { <nl> + while ( ost -> muxing_queue && av_fifo_size ( ost -> muxing_queue )) { <nl> AVPacket pkt ; <nl> av_fifo_generic_read ( ost -> muxing_queue , & pkt , sizeof ( pkt ), NULL ); <nl> av_packet_unref (& pkt );
unsigned ff_els_decode_unsigned ( ElsDecCtx * ctx , ElsUnsignedRung * ur ) <nl>  <nl> /* handle the error / overflow case */ <nl> if ( ctx -> err || n >= ELS_EXPGOLOMB_LEN ) { <nl> - ctx -> err = AVERROR ( EOVERFLOW ); <nl> + ctx -> err = AVERROR_INVALIDDATA ; <nl> return 0 ; <nl> } <nl> 
struct vfw_ctx { <nl> static enum PixelFormat vfw_pixfmt ( DWORD biCompression , WORD biBitCount ) <nl> { <nl> switch ( biCompression ) { <nl> + case MKTAG (' U ', ' Y ', ' V ', ' Y '): <nl> + return PIX_FMT_UYVY422 ; <nl> case MKTAG (' Y ', ' U ', ' Y ', ' 2 '): <nl> return PIX_FMT_YUYV422 ; <nl> case MKTAG (' I ', ' 4 ', ' 2 ', ' 0 '):
retry : <nl> } <nl> buf = c -> decomp_buf ; <nl> buf_size = c -> decomp_size - FFMAX ( FF_INPUT_BUFFER_PADDING_SIZE , AV_LZO_OUTPUT_PADDING ) - outlen ; <nl> + memset ( c -> decomp_buf + buf_size , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> } <nl> if ( c -> codec_frameheader ) { <nl> int w , h , q ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> tmp_ptr = av_fast_realloc ( s -> bitstream , & s -> allocated_bitstream_size , <nl> s -> max_framesize + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! tmp_ptr ) { <nl> + s -> max_framesize = 0 ; <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> }
static void aw_pulse_set2 ( WMAVoiceContext * s , GetBitContext * gb , <nl> int excl_range = s -> aw_pulse_range ; // always 16 or 24 <nl> uint16_t * use_mask_ptr = & use_mask [ idx >> 4 ]; <nl> int first_sh = 16 - ( idx & 15 ); <nl> - * use_mask_ptr ++ &= 0xFFFF << first_sh ; <nl> + * use_mask_ptr ++ &= 0xFFFFu << first_sh ; <nl> excl_range -= first_sh ; <nl> if ( excl_range >= 16 ) { <nl> * use_mask_ptr ++ = 0 ;
void ff_ivi_output_plane ( IVIPlaneDesc * plane , uint8_t * dst , int dst_pitch ) <nl> const int16_t * src = plane -> bands [ 0 ]. buf ; <nl> uint32_t pitch = plane -> bands [ 0 ]. pitch ; <nl>  <nl> + if (! src ) <nl> + return ; <nl> + <nl> for ( y = 0 ; y < plane -> height ; y ++) { <nl> for ( x = 0 ; x < plane -> width ; x ++) <nl> dst [ x ] = av_clip_uint8 ( src [ x ] + 128 );
static int vc1_decode_sprites ( VC1Context * v , GetBitContext * gb ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - if (! s -> current_picture . f -> data [ 0 ]) { <nl> + if (! s -> current_picture . f || ! s -> current_picture . f -> data [ 0 ]) { <nl> av_log ( avctx , AV_LOG_ERROR , " Got no sprites \ n "); <nl> return - 1 ; <nl> }
static int mxf_read_close ( AVFormatContext * s ) <nl>  <nl> for ( i = 0 ; i < mxf -> metadata_sets_count ; i ++) { <nl> switch ( mxf -> metadata_sets [ i ]-> type ) { <nl> + case Descriptor : <nl> + av_freep (&(( MXFDescriptor *) mxf -> metadata_sets [ i ])-> extradata ); <nl> + break ; <nl> case MultipleDescriptor : <nl> av_freep (&(( MXFDescriptor *) mxf -> metadata_sets [ i ])-> sub_descriptors_refs ); <nl> break ;
AVFilter avfilter_af_asyncts = { <nl> . config_props = config_props , <nl> . request_frame = request_frame }, <nl> { NULL }}, <nl> - . priv_size = & asyncts_class , <nl> + . priv_class = & asyncts_class , <nl> };
static int start_frame ( AVFilterLink * inlink , AVFilterBufferRef * picref ) <nl> avfilter_unref_buffer ( tinterlace -> cur ); <nl> tinterlace -> cur = tinterlace -> next ; <nl> tinterlace -> next = picref ; <nl> + inlink -> cur_buf = NULL ; <nl> return 0 ; <nl> } <nl> 
static void print_report ( int is_last_report , int64_t timer_start , int64_t cur_ti <nl> AVCodecContext * enc ; <nl> int frame_number , vid , i ; <nl> double bitrate ; <nl> - int64_t pts = INT64_MIN ; <nl> + int64_t pts = INT64_MIN + 1 ; <nl> static int64_t last_time = - 1 ; <nl> static int qp_histogram [ 52 ]; <nl> int hours , mins , secs , us ;
static int transcode_init ( void ) <nl> FilterGraph * fg = filtergraphs [ i ]; <nl> for ( j = 0 ; j < fg -> nb_outputs ; j ++) { <nl> OutputFilter * ofilter = fg -> outputs [ j ]; <nl> - if ( ofilter -> ost -> source_index >= 0 ) <nl> + if (! ofilter -> ost || ofilter -> ost -> source_index >= 0 ) <nl> continue ; <nl> if ( fg -> nb_inputs != 1 ) <nl> continue ;
static av_cold int tta_decode_init ( AVCodecContext * avctx ) <nl> s -> data_length = get_bits_long (& s -> gb , 32 ); <nl> skip_bits (& s -> gb , 32 ); // CRC32 of header <nl>  <nl> + if ( s -> channels == 0 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " Invalid number of channels \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> switch ( s -> bps ) { <nl> case 2 : <nl> avctx -> sample_fmt = AV_SAMPLE_FMT_S16 ;
int ff_vaapi_render_picture ( struct vaapi_context * vactx , VASurfaceID surface ) <nl> VABufferID va_buffers [ 3 ]; <nl> unsigned int n_va_buffers = 0 ; <nl>  <nl> + if (! vactx -> pic_param_buf_id ) <nl> + return 0 ; <nl> + <nl> vaUnmapBuffer ( vactx -> display , vactx -> pic_param_buf_id ); <nl> va_buffers [ n_va_buffers ++] = vactx -> pic_param_buf_id ; <nl> 
static av_cold void uninit ( AVFilterContext * ctx ) <nl> { <nl> MovieContext * movie = ctx -> priv ; <nl>  <nl> - av_free ( movie -> file_name ); <nl> - av_free ( movie -> format_name ); <nl> if ( movie -> codec_ctx ) <nl> avcodec_close ( movie -> codec_ctx ); <nl> if ( movie -> format_ctx )
static av_cold int init ( AVFilterContext * ctx ) <nl> int nb_formats = 1 ; <nl> int i ; <nl>  <nl> + if (! s -> pix_fmts ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " Empty output format string .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> /* count the formats */ <nl> cur = s -> pix_fmts ; <nl> while (( cur = strchr ( cur , '|'))) {
static int matroska_read_header ( AVFormatContext * s ) <nl> break ; <nl> if ( i >= FF_ARRAY_ELEMS ( matroska_doctypes )) { <nl> av_log ( s , AV_LOG_WARNING , " Unknown EBML doctype '% s '\ n ", ebml . doctype ); <nl> + if ( matroska -> ctx -> error_recognition & AV_EF_EXPLODE ) { <nl> + ebml_free ( ebml_syntax , & ebml ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl> ebml_free ( ebml_syntax , & ebml ); <nl> 
static void dwt_decode97_int ( DWTContext * s , int32_t * t ) <nl> line += 5 ; <nl>  <nl> for ( i = 0 ; i < w * h ; i ++) <nl> - data [ i ] *= 1 << I_PRESHIFT ; <nl> + data [ i ] *= 1LL << I_PRESHIFT ; <nl>  <nl> for ( lev = 0 ; lev < s -> ndeclevels ; lev ++) { <nl> int lh = s -> linelen [ lev ][ 0 ],
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> ff_init_range_encoder ( c , pkt -> data , pkt -> size ); <nl> ff_build_rac_states ( c , 0 . 05 * ( 1LL << 32 ), 256 - 8 ); <nl>  <nl> + av_frame_unref ( p ); <nl> av_frame_ref ( p , pict ); <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> 
static int hls_read_header ( AVFormatContext * s ) <nl>  <nl> return 0 ; <nl> fail : <nl> - free_playlist_list ( c ); <nl> - free_variant_list ( c ); <nl> - free_rendition_list ( c ); <nl> + hls_close ( s ); <nl> return ret ; <nl> } <nl> 
void ff_convert_matrix ( MpegEncContext * s , int (* qmat )[ 64 ], <nl> qmat16 [ qscale ][ 0 ][ i ] == 128 * 256 ) <nl> qmat16 [ qscale ][ 0 ][ i ] = 128 * 256 - 1 ; <nl> qmat16 [ qscale ][ 1 ][ i ] = <nl> - ROUNDED_DIV ( bias << ( 16 - QUANT_BIAS_SHIFT ), <nl> + ROUNDED_DIV ( bias * ( 1 <<( 16 - QUANT_BIAS_SHIFT )), <nl> qmat16 [ qscale ][ 0 ][ i ]); <nl> } <nl> }
static int adx_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> AVCodecContext * avctx = s -> streams [ 0 ]-> codec ; <nl> int ret , size ; <nl>  <nl> + if ( avctx -> channels <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " invalid number of channels % d \ n ", avctx -> channels ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> size = BLOCK_SIZE * avctx -> channels ; <nl>  <nl> pkt -> pos = avio_tell ( s -> pb );
static AVCRC av_crc_table [ AV_CRC_MAX ][ 257 ]; <nl> * @ return < 0 on failure <nl> */ <nl> int av_crc_init ( AVCRC * ctx , int le , int bits , uint32_t poly , int ctx_size ){ <nl> - int i , j ; <nl> + unsigned i , j ; <nl> uint32_t c ; <nl>  <nl> if ( bits < 8 || bits > 32 || poly >= ( 1LL << bits ))
av_cold int ff_h264_decode_init ( AVCodecContext * avctx ) <nl> s -> low_delay = 0 ; <nl> } <nl>  <nl> + ff_init_cabac_states (); <nl> + <nl> return 0 ; <nl> } <nl>  <nl> static int decode_slice ( struct AVCodecContext * avctx , void * arg ) <nl> align_get_bits (& s -> gb ); <nl>  <nl> /* init cabac */ <nl> - ff_init_cabac_states (); <nl> ff_init_cabac_decoder (& h -> cabac , <nl> s -> gb . buffer + get_bits_count (& s -> gb ) / 8 , <nl> ( get_bits_left (& s -> gb ) + 7 ) / 8 );
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl> while ( avpkt . size > 0 && it ++ < maxiteration ) { <nl> av_frame_unref ( frame ); <nl> int ret = decode_handler ( ctx , frame , & got_frame , & avpkt ); <nl> + <nl> + if ( it > 20 ) <nl> + ctx -> error_concealment = 0 ; <nl> + <nl> if ( ret <= 0 || ret > avpkt . size ) <nl> break ; <nl> avpkt . data += ret ;
static int mov_read_mac_string ( MOVContext * c , AVIOContext * pb , int len , <nl> uint8_t t , c = avio_r8 ( pb ); <nl> if ( c < 0x80 && p < end ) <nl> * p ++ = c ; <nl> - else <nl> + else if ( p < end ) <nl> PUT_UTF8 ( mac_to_unicode [ c - 0x80 ], t , if ( p < end ) * p ++ = t ;); <nl> } <nl> * p = 0 ;
static int imc_decode_frame ( AVCodecContext * avctx , void * data , <nl>  <nl> IMCContext * q = avctx -> priv_data ; <nl>  <nl> - LOCAL_ALIGNED_16 ( uint16_t , buf16 , [ IMC_BLOCK_SIZE / 2 ]); <nl> + LOCAL_ALIGNED_16 ( uint16_t , buf16 , [( IMC_BLOCK_SIZE + FF_INPUT_BUFFER_PADDING_SIZE ) / 2 ]); <nl>  <nl> if ( buf_size < IMC_BLOCK_SIZE * avctx -> channels ) { <nl> av_log ( avctx , AV_LOG_ERROR , " frame too small !\ n ");
static int idcin_read_packet ( AVFormatContext * s , <nl> return ret ; <nl> else if ( ret != chunk_size ) { <nl> av_log ( s , AV_LOG_ERROR , " incomplete packet \ n "); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( EIO ); <nl> } <nl> if ( command == 1 ) {
static int flashsv_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> if ( has_diff ) { <nl> + if (! s -> keyframe ) { <nl> + av_log ( avctx , AV_LOG_ERROR , <nl> + " inter frame without keyframe \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> diff_start = get_bits (& gb , 8 ); <nl> s -> diff_height = get_bits (& gb , 8 ); <nl> av_log ( avctx , AV_LOG_DEBUG ,
void ff_rtsp_close_streams ( AVFormatContext * s ) <nl> if ( rtsp_st -> dynamic_handler && rtsp_st -> dynamic_protocol_context ) <nl> rtsp_st -> dynamic_handler -> close ( <nl> rtsp_st -> dynamic_protocol_context ); <nl> + av_free ( rtsp_st ); <nl> } <nl> } <nl> av_free ( rt -> rtsp_streams );
retry : <nl> /* If we need to reload the playlist again below ( if <nl> * there ' s still no more segments ), switch to a reload <nl> * interval of half the target duration . */ <nl> - reload_interval = s -> target_duration * 500000 ; <nl> + reload_interval = s -> target_duration * 500000LL ; <nl> } <nl> } <nl> if ( s -> cur_seq_no < s -> start_seq_no ) {
static int mxf_compute_sample_count ( MXFContext * mxf , int stream_index , uint64_t <nl>  <nl> av_assert2 ( size ); <nl>  <nl> - * sample_count = ( mxf -> current_edit_unit / size ) * total ; <nl> + * sample_count = ( mxf -> current_edit_unit / size ) * ( uint64_t ) total ; <nl> for ( i = 0 ; i < mxf -> current_edit_unit % size ; i ++) { <nl> * sample_count += spf -> samples_per_frame [ i ]; <nl> }
static int request_frame ( AVFilterLink * outlink ) <nl> avfilter_start_frame ( outlink , outpicref ); <nl> avfilter_draw_slice ( outlink , 0 , outlink -> h , 1 ); <nl> avfilter_end_frame ( outlink ); <nl> + avfilter_unref_buffer ( movie -> picref ); <nl> + movie -> picref = NULL ; <nl>  <nl> return 0 ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> } <nl> s -> inbuf_ptr = s -> inbuf ; <nl> s -> frame_size = 0 ; <nl> - * data_size = out_size ; <nl> + if ( out_size >= 0 ) <nl> + * data_size = out_size ; <nl> + else <nl> + av_log ( avctx , AV_LOG_DEBUG , " Error while decoding mpeg audio frame \ n "); // FIXME return - 1 / but also return the number of bytes consumed <nl> break ; <nl> } <nl> }
static int a64multi_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int b_width ; <nl>  <nl> int req_size , ret ; <nl> - uint8_t * buf ; <nl> + uint8_t * buf = NULL ; <nl>  <nl> int * charmap = c -> mc_charmap ; <nl> uint8_t * colram = c -> mc_colram ;
static int v4l2_set_parameters ( AVFormatContext * s1 ) <nl> standard . index = i ; <nl> if ( v4l2_ioctl ( s -> fd , VIDIOC_ENUMSTD , & standard ) < 0 ) { <nl> ret = AVERROR ( errno ); <nl> - if ( ret == AVERROR ( EINVAL )) { <nl> + if ( ret == AVERROR ( EINVAL ) || ret == AVERROR ( ENODATA )) { <nl> tpf = & streamparm . parm . capture . timeperframe ; <nl> break ; <nl> }
static av_cold int init ( AVFilterContext * ctx ) <nl> OCVContext * ocv = ctx -> priv ; <nl> int i ; <nl>  <nl> + if (! ocv -> name ) { <nl> + av_log ( ctx , AV_LOG_ERROR , " No libopencv filter name specified \ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> for ( i = 0 ; i < FF_ARRAY_ELEMS ( ocv_filter_entries ); i ++) { <nl> OCVFilterEntry * entry = & ocv_filter_entries [ i ]; <nl> if (! strcmp ( ocv -> name , entry -> name )) {
void ff_h264_direct_ref_list_init ( const H264Context * const h , H264SliceContext * <nl> memcpy ( cur -> ref_poc [ 1 ], cur -> ref_poc [ 0 ], sizeof ( cur -> ref_poc [ 0 ])); <nl> } <nl>  <nl> - cur -> mbaff = FRAME_MBAFF ( h ); <nl> + if ( h -> current_slice == 0 ) { <nl> + cur -> mbaff = FRAME_MBAFF ( h ); <nl> + } else { <nl> + av_assert0 ( cur -> mbaff == FRAME_MBAFF ( h )); <nl> + } <nl>  <nl> sl -> col_fieldoff = 0 ; <nl> 
static void ogg_free ( AVFormatContext * s ) <nl> for ( i = 0 ; i < s -> nb_streams ; i ++) { <nl> AVStream * st = s -> streams [ i ]; <nl> OGGStreamContext * oggstream = st -> priv_data ; <nl> + if (! oggstream ) <nl> + continue ; <nl> if ( st -> codecpar -> codec_id == AV_CODEC_ID_FLAC || <nl> st -> codecpar -> codec_id == AV_CODEC_ID_SPEEX || <nl> st -> codecpar -> codec_id == AV_CODEC_ID_OPUS ||
static av_cold int init_subtitles ( AVFilterContext * ctx , const char * args ) <nl> } <nl>  <nl> end : <nl> - if ( fmt ) <nl> - avformat_close_input (& fmt ); <nl> if ( dec_ctx ) <nl> avcodec_close ( dec_ctx ); <nl> + if ( fmt ) <nl> + avformat_close_input (& fmt ); <nl> return ret ; <nl> } <nl> 
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> else v = buf_p - c -> bytestream_start ; <nl> if ( buf_p - c -> bytestream_start < v ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Slice pointer chain broken \ n "); <nl> + ff_thread_report_progress (& f -> picture , INT_MAX , 0 ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> buf_p -= v ;
static void apply_dependent_coupling_fixed ( AACContext * ac , <nl> for ( k = offsets [ i ]; k < offsets [ i + 1 ]; k ++) { <nl> tmp = ( int )((( int64_t ) src [ group * 128 + k ] * c + \ <nl> ( int64_t ) 0x1000000000 ) >> 37 ); <nl> - dest [ group * 128 + k ] += tmp << shift ; <nl> + dest [ group * 128 + k ] += tmp * ( 1 << shift ); <nl> } <nl> } <nl> }
static int64_t run_opencl_bench ( AVOpenCLExternalEnv * ext_opencl_env ) <nl> cl_int status ; <nl> size_t kernel_len ; <nl> char * inbuf ; <nl> - int * mask ; <nl> + int * mask = NULL ; <nl> int buf_size = width * height * sizeof ( char ); <nl> int mask_size = sizeof ( uint32_t ) * 128 ; <nl> 
static int init_input ( AVFormatContext * s , const char * filename , AVDictionary ** o <nl> int ret ; <nl> AVProbeData pd = { filename , NULL , 0 }; <nl>  <nl> - if ( s -> iformat && ! strlen ( filename )) <nl> - return 0 ; <nl> - <nl> if ( s -> pb ) { <nl> s -> flags |= AVFMT_FLAG_CUSTOM_IO ; <nl> if (! s -> iformat )
return - 1 ; <nl> } <nl> # endif <nl> s -> qscale = get_bits (& s -> gb , 5 ); <nl> + if ( s -> qscale == 0 ){ <nl> + fprintf ( stderr , " invalid qscale \ n "); <nl> + return - 1 ; <nl> + } <nl>  <nl> if ( s -> pict_type == I_TYPE ) { <nl> code = get_bits (& s -> gb , 5 );
av_cold int sws_init_context ( SwsContext * c , SwsFilter * srcFilter , <nl> } <nl> } <nl>  <nl> -# define USE_MMAP ( HAVE_MMAP && HAVE_MPROTECT && defined MAP_ANONYMOUS ) <nl> +# if HAVE_MMAP && HAVE_MPROTECT && defined ( MAP_ANONYMOUS ) <nl> +# define USE_MMAP 1 <nl> +# else <nl> +# define USE_MMAP 0 <nl> +# endif <nl>  <nl> /* precalculate horizontal scaler filter coefficients */ <nl> {
retry : <nl> return psize ; <nl> fail : <nl> av_free_packet ( pkt ); <nl> - av_free ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
static int tak_decode_frame ( AVCodecContext * avctx , void * data , <nl> for ( chan = 0 ; chan < avctx -> channels ; chan ++) { <nl> int32_t * samples = ( int32_t *) frame -> extended_data [ chan ]; <nl> for ( i = 0 ; i < s -> nb_samples ; i ++) <nl> - samples [ i ] <<= 8 ; <nl> + samples [ i ] *= 1 << 8 ; <nl> } <nl> break ; <nl> }
ERROR <nl> # endif <nl>  <nl> void RENAME ( swri_noise_shaping )( SwrContext * s , AudioData * dsts , const AudioData * srcs , const AudioData * noises , int count ){ <nl> - int i , j , pos , ch ; <nl> + int pos = s -> dither . ns_pos ; <nl> + int i , j , ch ; <nl> int taps = s -> dither . ns_taps ; <nl> float S = s -> dither . ns_scale ; <nl> float S_1 = s -> dither . ns_scale_1 ;
static const AVProfile mpeg2_video_profiles [] = { <nl> { FF_PROFILE_MPEG2_SIMPLE , " Simple " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> { FF_PROFILE_RESERVED , " Reserved " }, <nl> + { FF_PROFILE_UNKNOWN }, <nl> }; <nl>  <nl> 
static void celt_pvq_search ( float * X , int * y , int K , int N ) <nl> for ( i = 0 ; i < N ; i ++) <nl> res += FFABS ( X [ i ]); <nl>  <nl> - res = K / res ; <nl> + res = K /( res + FLT_EPSILON ); <nl>  <nl> for ( i = 0 ; i < N ; i ++) { <nl> y [ i ] = lrintf ( res * X [ i ]);
return - 1 ; <nl> # endif <nl>  <nl> if ( s -> msmpeg4_version == 1 ){ <nl> - int start_code ; <nl> - start_code = ( get_bits (& s -> gb , 16 )<< 16 ) | get_bits (& s -> gb , 16 ); <nl> + int start_code = get_bits_long (& s -> gb , 32 ); <nl> if ( start_code != 0x00000100 ){ <nl> av_log ( s -> avctx , AV_LOG_ERROR , " invalid startcode \ n "); <nl> return - 1 ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> + memset ( tmp_ptr , 0 , s -> allocated_bitstream_size ); <nl> s -> bitstream = tmp_ptr ; <nl> } <nl> 
static int lag_decode_zero_run_line ( LagarithContext * l , uint8_t * dst , <nl> output_zeros : <nl> if ( l -> zeros_rem ) { <nl> count = FFMIN ( l -> zeros_rem , width - i ); <nl> + if ( end - dst < count ) { <nl> + av_log ( l -> avctx , AV_LOG_ERROR , " too many zeros remaining \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> memset ( dst , 0 , count ); <nl> l -> zeros_rem -= count ; <nl> dst += count ;
static int aac_decode_frame_int ( AVCodecContext * avctx , void * data , <nl>  <nl> if ( samples ) <nl> ac -> frame -> nb_samples = samples ; <nl> + else <nl> + av_frame_unref ( ac -> frame ); <nl> * got_frame_ptr = !! samples ; <nl>  <nl> if ( is_dmono ) {
static int expand_tseq ( void * log , struct sbg_script * s , int * nb_ev_max , <nl> } else { <nl> ev = alloc_array_elem (( void **)& s -> events , sizeof (* ev ), <nl> & s -> nb_events , nb_ev_max ); <nl> + if (! ev ) <nl> + return AVERROR ( ENOMEM ); <nl> ev -> ts = tseq -> ts . t ; <nl> ev -> elements = def -> elements ; <nl> ev -> nb_elements = def -> nb_elements ;
static int h264_slice_init ( H264Context * h , H264SliceContext * sl , <nl>  <nl> if ( sl -> slice_type_nos == AV_PICTURE_TYPE_B && ! sl -> direct_spatial_mv_pred ) <nl> ff_h264_direct_dist_scale_factor ( h , sl ); <nl> - ff_h264_direct_ref_list_init ( h , sl ); <nl> + if (! h -> setup_finished ) <nl> + ff_h264_direct_ref_list_init ( h , sl ); <nl>  <nl> if ( h -> avctx -> skip_loop_filter >= AVDISCARD_ALL || <nl> ( h -> avctx -> skip_loop_filter >= AVDISCARD_NONKEY &&
static int opt_vstats ( void * optctx , const char * opt , const char * arg ) <nl> time_t today2 = time ( NULL ); <nl> struct tm * today = localtime (& today2 ); <nl>  <nl> + if (! today ) <nl> + return AVERROR ( errno ); <nl> + <nl> snprintf ( filename , sizeof ( filename ), " vstats_ % 02d % 02d % 02d . log ", today -> tm_hour , today -> tm_min , <nl> today -> tm_sec ); <nl> return opt_vstats_file ( NULL , opt , filename );
static int decode_slice_header ( H264Context * h ){ <nl> return - 1 ; <nl> } <nl>  <nl> - if ( h -> dequant_coeff_pps != ( int ) pps_id ){ <nl> - h -> dequant_coeff_pps = ( int ) pps_id ; <nl> + if ( h -> dequant_coeff_pps != pps_id ){ <nl> + h -> dequant_coeff_pps = pps_id ; <nl> init_dequant_tables ( h ); <nl> } <nl> 
resync : <nl> err = av_get_packet ( pb , pkt , size ); <nl> if ( err < 0 ) <nl> return err ; <nl> + size = err ; <nl>  <nl> if ( ast -> has_pal && pkt -> data && pkt -> size <( unsigned ) INT_MAX / 2 ){ <nl> uint8_t * pal ;
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> ret = 0 ; <nl>  <nl> the_end : <nl> - av_free ( crow_base ); <nl> - av_free ( progressive_buf ); <nl> - av_free ( top_buf ); <nl> + av_freep (& crow_base ); <nl> + av_freep (& progressive_buf ); <nl> + av_freep (& top_buf ); <nl> deflateEnd (& s -> zstream ); <nl> return ret ; <nl> fail :
static inline int ape_decode_value_3900 ( APEContext * ctx , APERice * rice ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> x = range_decode_bits ( ctx , tmpk ); <nl> - } else if ( tmpk <= 32 ) { <nl> + } else if ( tmpk <= 31 ) { <nl> x = range_decode_bits ( ctx , 16 ); <nl> x |= ( range_decode_bits ( ctx , tmpk - 16 ) << 16 ); <nl> } else {
static int mov_read_stts ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl> if ( entries >= UINT_MAX / sizeof (* sc -> stts_data )) <nl> return AVERROR ( EINVAL ); <nl>  <nl> + av_free ( sc -> stts_data ); <nl> sc -> stts_data = av_malloc ( entries * sizeof (* sc -> stts_data )); <nl> if (! sc -> stts_data ) <nl> return AVERROR ( ENOMEM );
static const QCELPBitmap qcelp_rate_octave_bitmap [] = { <nl> QCELP_OF ( lspv [ 8 ], 0 , 1 ), // 8 <nl> QCELP_OF ( cbsign [ 15 ], 0 , 1 ), // 7 <nl> QCELP_OF ( lspv [ 9 ], 0 , 1 ), // 6 <nl> - QCELP_OF ( cbgain [ 0 ], 0 , 2 ), // 7 <nl> + QCELP_OF ( cbgain [ 0 ], 0 , 2 ), // 5 <nl> QCELP_OF ( reserved , 0 , 4 ) // 3 <nl> }; <nl> 
uint8_t * av_packet_new_side_data ( AVPacket * pkt , enum AVPacketSideDataType type , <nl> data = av_malloc ( size + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! data ) <nl> return NULL ; <nl> + memset ( data + size , 0 , AV_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> ret = av_packet_add_side_data ( pkt , type , data , size ); <nl> if ( ret < 0 ) {
fail_kernel_arg : <nl> kernel_arg , cle ); <nl> err = AVERROR ( EIO ); <nl> fail : <nl> + av_frame_free (& output ); <nl> return err ; <nl> } <nl> 
static int wv_get_value ( WavpackFrameContext * ctx , GetBitContext * gb , <nl> } else { <nl> t = get_unary_0_33 ( gb ); <nl> if ( t >= 2 ) { <nl> - if ( get_bits_left ( gb ) < t - 1 ) <nl> + if ( t >= 32 || get_bits_left ( gb ) < t - 1 ) <nl> goto error ; <nl> t = get_bits_long ( gb , t - 1 ) | ( 1 << ( t - 1 )); <nl> } else {
static int check_tag ( AVIOContext * s , int offset , unsigned int len ) <nl>  <nl> if ( len > 4 || <nl> avio_seek ( s , offset , SEEK_SET ) < 0 || <nl> - avio_read ( s , tag , len ) < len ) <nl> + avio_read ( s , tag , len ) < ( int ) len ) <nl> return - 1 ; <nl> else if (! AV_RB32 ( tag ) || is_tag ( tag , len )) <nl> return 1 ;
static void vp8_decode_flush ( AVCodecContext * avctx ) <nl> memset ( s -> framep , 0 , sizeof ( s -> framep )); <nl>  <nl> av_freep (& s -> macroblocks_base ); <nl> + av_freep (& s -> filter_strength ); <nl> av_freep (& s -> intra4x4_pred_mode_base ); <nl> av_freep (& s -> top_nnz ); <nl> av_freep (& s -> edge_emu_buffer );
static int libschroedinger_decode_frame ( AVCodecContext * avctx , <nl> /* Grab next frame to be returned from the top of the queue . */ <nl> framewithpts = ff_schro_queue_pop (& p_schro_params -> dec_frame_queue ); <nl>  <nl> - if ( framewithpts && framewithpts -> frame ) { <nl> + if ( framewithpts && framewithpts -> frame && framewithpts -> frame -> components [ 0 ]. stride ) { <nl> int ret ; <nl>  <nl> if (( ret = ff_get_buffer ( avctx , avframe , 0 )) < 0 )
static void mov_build_index ( MOVContext * mov , AVStream * st ) <nl>  <nl> current_dts -= sc -> dts_shift ; <nl>  <nl> - if (! sc -> sample_count ) <nl> + if (! sc -> sample_count || st -> nb_index_entries ) <nl> return ; <nl> if ( sc -> sample_count >= UINT_MAX / sizeof (* st -> index_entries )) <nl> return ;
static void formant_postfilter ( G723_1_Context * p , int16_t * lpc , int16_t * buf ) <nl>  <nl> /* Compensation filter */ <nl> for ( j = 0 ; j < SUBFRAME_LEN ; j ++) { <nl> - buf_ptr [ j ] = av_clipl_int32 ( signal_ptr [ j ] + <nl> + buf_ptr [ j ] = av_clipl_int32 (( int64_t ) signal_ptr [ j ] + <nl> (( signal_ptr [ j - 1 ] >> 16 ) * <nl> temp << 1 )) >> 16 ; <nl> }
static void asf_build_simple_index ( AVFormatContext * s , int stream_index ) <nl> last_pos = pos ; <nl> } <nl> } <nl> - asf -> index_read = 1 ; <nl> + asf -> index_read = ict > 0 ; <nl> } <nl> avio_seek ( s -> pb , current_pos , SEEK_SET ); <nl> }
FF_ENABLE_DEPRECATION_WARNINGS <nl> # endif <nl> { <nl> ret = decode_tiles ( avctx , data , size ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + ff_thread_report_progress (& s -> s . frames [ CUR_FRAME ]. tf , INT_MAX , 0 ); <nl> return ret ; <nl> + } <nl> } <nl>  <nl> // Sum all counts fields into td [ 0 ]. counts for tile threading
static int decode_nal_units ( H264Context * h , const uint8_t * buf , int buf_size , <nl> continue ; <nl>  <nl> again : <nl> - if ( !( avctx -> active_thread_type & FF_THREAD_FRAME ) <nl> - || nals_needed >= nal_index ) <nl> + if ( (!( avctx -> active_thread_type & FF_THREAD_FRAME ) || nals_needed >= nal_index ) <nl> + && ! h -> current_slice ) <nl> h -> au_pps_id = - 1 ; <nl> /* Ignore per frame NAL unit type during extradata <nl> * parsing . Decoding slices is not possible in codec init
static int mm_decode_intra ( MmContext * s , int half_horiz , int half_vert ) <nl> */ <nl> static int mm_decode_inter ( MmContext * s , int half_horiz , int half_vert ) <nl> { <nl> - int data_off = bytestream2_get_le16 (& s -> gb ), y ; <nl> + int data_off = bytestream2_get_le16 (& s -> gb ), y = 0 ; <nl> GetByteContext data_ptr ; <nl>  <nl> if ( bytestream2_get_bytes_left (& s -> gb ) < data_off )
FF_ENABLE_DEPRECATION_WARNINGS <nl> } <nl>  <nl> if ( s -> oformat -> init && ( ret = s -> oformat -> init ( s )) < 0 ) { <nl> - s -> oformat -> deinit ( s ); <nl> + if ( s -> oformat -> deinit ) <nl> + s -> oformat -> deinit ( s ); <nl> goto fail ; <nl> } <nl> 
int ff_j2k_init_component ( J2kComponent * comp , J2kCodingStyle * codsty , J2kQuantSt <nl> band -> cblk = av_malloc ( sizeof ( J2kCblk ) * band -> cblknx * band -> cblkny ); <nl> if (! band -> cblk ) <nl> return AVERROR ( ENOMEM ); <nl> - band -> prec = av_malloc ( reslevel -> num_precincts_x * reslevel -> num_precincts_y * sizeof ( J2kPrec )); <nl> + band -> prec = av_malloc ( sizeof ( J2kCblk ) * reslevel -> num_precincts_x * reslevel -> num_precincts_y ); <nl> if (! band -> prec ) <nl> return AVERROR ( ENOMEM ); <nl> 
enum AVCodecID ff_codec_guid_get_id ( const AVCodecGuid * guids , ff_asf_guid guid ) <nl> static void parse_waveformatex ( AVIOContext * pb , AVCodecParameters * par ) <nl> { <nl> ff_asf_guid subformat ; <nl> - par -> bits_per_coded_sample = avio_rl16 ( pb ); <nl> + int bps ; <nl> + <nl> + bps = avio_rl16 ( pb ); <nl> + if ( bps ) <nl> + par -> bits_per_coded_sample = bps ; <nl> par -> channel_layout = avio_rl32 ( pb ); /* dwChannelMask */ <nl>  <nl> ff_get_guid ( pb , & subformat );
static int codec_get_buffer ( AVCodecContext * s , AVFrame * frame ) <nl> FrameBuffer * buf ; <nl> int ret , i ; <nl>  <nl> + if ( av_image_check_size ( s -> width , s -> height , 0 , s )) <nl> + return - 1 ; <nl> + <nl> if (! ist -> buffer_pool && ( ret = alloc_buffer ( s , ist , & ist -> buffer_pool )) < 0 ) <nl> return ret ; <nl> 
int main ( int argc , char ** argv ) <nl>  <nl> if ( got_frame ) { <nl> /* push the audio data from decoded frame into the filtergraph */ <nl> - if ( av_buffersrc_add_frame_flags ( buffersrc_ctx , frame , AV_BUFFERSRC_FLAG_KEEP_REF ) < 0 ) { <nl> + if ( av_buffersrc_add_frame_flags ( buffersrc_ctx , frame , 0 ) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error while feeding the audio filtergraph \ n "); <nl> break ; <nl> }
static int fourxm_read_header ( AVFormatContext * s ) <nl> } <nl>  <nl> if ( fourcc_tag == std__TAG ) { <nl> + if ( header_size < i + 16 ) { <nl> + av_log ( s , AV_LOG_ERROR , " std TAG truncated \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> fourxm -> fps = av_int2float ( AV_RL32 (& header [ i + 12 ])); <nl> } else if ( fourcc_tag == vtrk_TAG ) { <nl> /* check that there is enough data */
static void create_adapt_vect ( float * vect , const int16_t * cb , int lag ) <nl> static int adaptive_cb_search ( const int16_t * adapt_cb , float * work , <nl> const float * coefs , float * data ) <nl> { <nl> - int i , best_vect ; <nl> - float score , gain , best_score , best_gain ; <nl> + int i , av_uninit ( best_vect ); <nl> + float score , gain , best_score , av_uninit ( best_gain ); <nl> float exc [ BLOCKSIZE ]; <nl>  <nl> gain = best_score = 0 ;
static int decode_subframe_length ( WMAProDecodeCtx * s , int offset ) <nl> if ( offset == s -> samples_per_frame - s -> min_samples_per_subframe ) <nl> return s -> min_samples_per_subframe ; <nl>  <nl> + if ( get_bits_left (& s -> gb ) < 1 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> /** 1 bit indicates if the subframe is of maximum length */ <nl> if ( s -> max_subframe_len_bit ) { <nl> if ( get_bits1 (& s -> gb ))
static int init_pass2 ( MpegEncContext * s ) <nl> double rate_factor = 0 ; <nl> double step ; <nl> const int filter_size = ( int )( a -> qblur * 4 ) | 1 ; <nl> - double expected_bits ; <nl> + double expected_bits = 0 ; // init to silence gcc warning <nl> double * qscale , * blurred_qscale , qscale_sum ; <nl>  <nl> /* find complexity & const_bits & decide the pict_types */
static int decode_frame ( AVCodecContext * avctx , <nl> if ( i ) { <nl> AVRational q = av_d2q ( av_int2float ( i ), 4096 ); <nl> if ( q . num > 0 && q . den > 0 ) <nl> - avctx -> time_base = av_inv_q ( q ); <nl> + avctx -> framerate = q ; <nl> } <nl> } <nl> 
static void update_initial_timestamps ( AVFormatContext * s , int stream_index , <nl> if ( st -> first_dts != AV_NOPTS_VALUE || <nl> dts == AV_NOPTS_VALUE || <nl> st -> cur_dts == AV_NOPTS_VALUE || <nl> + st -> cur_dts < INT_MIN + RELATIVE_TS_BASE || <nl> is_relative ( dts )) <nl> return ; <nl> 
static int applehttp_read_seek ( AVFormatContext * s , int stream_index , <nl> int64_t timestamp , int flags ) <nl> { <nl> AppleHTTPContext * c = s -> priv_data ; <nl> - int pos = 0 , i ; <nl> + int64_t pos = 0 ; <nl> + int i ; <nl> struct variant * var = c -> variants [ 0 ]; <nl>  <nl> if (( flags & AVSEEK_FLAG_BYTE ) || ! c -> finished )
static int opus_packet ( AVFormatContext * avf , int idx ) <nl>  <nl> if (! os -> psize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( os -> granule > INT64_MAX - UINT32_MAX ) { <nl> + av_log ( avf , AV_LOG_ERROR , " Unsupported huge granule pos %" PRId64 "\ n ", os -> granule ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> if ((! os -> lastpts || os -> lastpts == AV_NOPTS_VALUE ) && !( os -> flags & OGG_FLAG_EOS )) { <nl> int seg , d ;
static int try_decode_video_frame ( AVCodecContext * codec_ctx , AVPacket * pkt , int <nl> goto end ; <nl> } <nl>  <nl> - if (! decode && codec_ctx -> codec -> caps_internal & FF_CODEC_CAP_SKIP_FRAME_FILL_PARAM ) { <nl> + if (! decode && avpriv_codec_get_cap_skip_frame_fill_param ( codec_ctx -> codec )) { <nl> codec_ctx -> skip_frame = AVDISCARD_ALL ; <nl> } <nl> 
static int mpc8_decode_frame ( AVCodecContext * avctx , <nl> maxband = c -> last_max_band + get_vlc2 ( gb , band_vlc . table , MPC8_BANDS_BITS , 2 ); <nl> if ( maxband > 32 ) maxband -= 33 ; <nl> } <nl> + if ( maxband > c -> maxbands ) <nl> + return AVERROR_INVALIDDATA ; <nl> c -> last_max_band = maxband ; <nl>  <nl> /* read subband indexes */
static int mp_get_vlc ( MotionPixelsContext * mp , GetBitContext * gb ) <nl> int i ; <nl>  <nl> i = ( mp -> codes_count == 1 ) ? 0 : get_vlc2 ( gb , mp -> vlc . table , mp -> max_codes_bits , 1 ); <nl> + i = FFMIN ( i , FF_ARRAY_ELEMS ( mp -> codes ) - 1 ); <nl> return mp -> codes [ i ]. delta ; <nl> } <nl> 
static int process_line ( URLContext * h , char * line , int line_count , <nl> # ifdef DEBUG <nl> printf (" http_code =% d \ n ", s -> http_code ); <nl> # endif <nl> + /* error codes are 4xx and 5xx */ <nl> + if ( s -> http_code >= 400 && s -> http_code < 600 ) <nl> + return - 1 ; <nl> } else { <nl> while (* p != '\ 0 ' && * p != ':') <nl> p ++;
static int smacker_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> } <nl> flags >>= 1 ; <nl> } <nl> - if ( frame_size < 0 ) <nl> + if ( frame_size < 0 || frame_size >= INT_MAX / 2 ) <nl> return AVERROR_INVALIDDATA ; <nl> if ( av_new_packet ( pkt , frame_size + 769 )) <nl> return AVERROR ( ENOMEM );
static const char * type_string ( int type ) <nl> return "< LINK >"; <nl> case AVIO_ENTRY_SOCKET : <nl> return "< SOCKET >"; <nl> + case AVIO_ENTRY_SERVER : <nl> + return "< SERVER >"; <nl> + case AVIO_ENTRY_SHARE : <nl> + return "< SHARE >"; <nl> + case AVIO_ENTRY_WORKGROUP : <nl> + return "< WORKGROUP >"; <nl> case AVIO_ENTRY_UNKNOWN : <nl> default : <nl> break ;
static int matroska_parse_rm_audio ( MatroskaDemuxContext * matroska , <nl> } <nl> memcpy ( track -> audio . buf + y * w , data , w ); <nl> } else { <nl> - if ( size < sps * w / sps ) { <nl> + if ( size < sps * w / sps || h <= 0 ) { <nl> av_log ( matroska -> ctx , AV_LOG_ERROR , <nl> " Corrupt generic RM - style audio packet size \ n "); <nl> return AVERROR_INVALIDDATA ;
static int avi_read_header ( AVFormatContext * s ) <nl> codec_type = AVMEDIA_TYPE_VIDEO ; <nl>  <nl> ast -> sample_size = 0 ; <nl> + st -> avg_frame_rate = av_inv_q ( st -> time_base ); <nl> break ; <nl> case MKTAG (' a ', ' u ', ' d ', ' s '): <nl> codec_type = AVMEDIA_TYPE_AUDIO ;
static int decode_ics_info ( AACContext * ac , IndividualChannelStream * ics , <nl> if ( aot != AOT_ER_AAC_ELD ) { <nl> if ( get_bits1 ( gb )) { <nl> av_log ( ac -> avctx , AV_LOG_ERROR , " Reserved bit set .\ n "); <nl> - return AVERROR_INVALIDDATA ; <nl> + if ( ac -> avctx -> err_recognition & AV_EF_BITSTREAM ) <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl> ics -> window_sequence [ 1 ] = ics -> window_sequence [ 0 ]; <nl> ics -> window_sequence [ 0 ] = get_bits ( gb , 2 );
static int decode_seq_header ( AVSContext * h ) { <nl> av_log_missing_feature ( s , " Width / height changing in CAVS is ", 0 ); <nl> return - 1 ; <nl> } <nl> + if ( width <= 0 || height <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Dimensions invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> s -> width = width ; <nl> s -> height = height ; <nl> skip_bits (& s -> gb , 2 ); // chroma format
* <nl> */ <nl>  <nl> -# define WTV_SECTOR_BITS 12 <nl> +# define WTV_SECTOR_BITS INT64_C ( 12 ) <nl> # define WTV_SECTOR_SIZE ( 1 << WTV_SECTOR_BITS ) <nl> # define WTV_BIGSECTOR_BITS 18 <nl> 
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - s -> version_b = avctx -> extradata && avctx -> extradata [ 3 ] == ' b '; <nl> + s -> version_b = avctx -> extradata_size >= 4 && avctx -> extradata [ 3 ] == ' b '; <nl>  <nl> if ( avctx -> codec -> id == CODEC_ID_BINKAUDIO_RDFT ) { <nl> // audio is already interleaved for the RDFT format variant
static int has_codec_parameters ( AVStream * st , const char ** errmsg_ptr ) <nl> FAIL (" unspecified sample rate "); <nl> if (! avctx -> channels ) <nl> FAIL (" unspecified number of channels "); <nl> + if ( st -> info -> found_decoder >= 0 && ! st -> nb_decoded_frames && avctx -> codec_id == AV_CODEC_ID_DTS ) <nl> + FAIL (" no decodable DTS frames "); <nl> break ; <nl> case AVMEDIA_TYPE_VIDEO : <nl> if (! avctx -> width )
static AVFilterContext * create_filter_with_args ( const char * filt , void * opaque ) <nl> av_log ( NULL , AV_LOG_ERROR , <nl> " error creating filter \"% s \" with args \"% s \"\ n ", <nl> name , args ? args : "( none )"); <nl> - return NULL ; <nl> } <nl>  <nl> av_free ( filter );
static av_cold void nvenc_setup_rate_control ( AVCodecContext * avctx ) <nl>  <nl> if ( ctx -> flags & NVENC_LOSSLESS ) { <nl> set_lossless ( avctx ); <nl> - } else if ( ctx -> rc > 0 ) { <nl> + } else if ( ctx -> rc >= 0 ) { <nl> nvenc_override_rate_control ( avctx ); <nl> } else { <nl> ctx -> encode_config . rcParams . rateControlMode = NV_ENC_PARAMS_RC_VBR ;
void av_packet_free ( AVPacket ** pkt ) <nl> static int packet_alloc ( AVBufferRef ** buf , int size ) <nl> { <nl> int ret ; <nl> - if (( unsigned ) size >= ( unsigned ) size + AV_INPUT_BUFFER_PADDING_SIZE ) <nl> + if ( size < 0 || size >= INT_MAX - AV_INPUT_BUFFER_PADDING_SIZE ) <nl> return AVERROR ( EINVAL ); <nl>  <nl> ret = av_buffer_realloc ( buf , size + AV_INPUT_BUFFER_PADDING_SIZE );
static void encode_frame ( MpegAudioContext * s , <nl> q1 += 1 << P ; <nl> if ( q1 < 0 ) <nl> q1 = 0 ; <nl> - q [ m ] = ( unsigned )( q1 * steps ) >> ( P + 1 ); <nl> + q [ m ] = ( q1 * ( unsigned ) steps ) >> ( P + 1 ); <nl> } <nl> # endif <nl> if ( q [ m ] >= steps )
av_cold void ff_init_range_decoder ( RangeCoder * c , const uint8_t * buf , <nl> c -> bytestream += 2 ; <nl> if ( c -> low >= 0xFF00 ) { <nl> c -> low = 0xFF00 ; <nl> - c -> bytestream_end = c -> bytestream + 2 ; <nl> + c -> bytestream_end = c -> bytestream ; <nl> } <nl> } <nl> 
RMStream * ff_rm_alloc_rmstream ( void ) <nl>  <nl> void ff_rm_free_rmstream ( RMStream * rms ) <nl> { <nl> - av_free ( rms -> videobuf ); <nl> - av_free ( rms -> audiobuf ); <nl> + av_freep (& rms -> videobuf ); <nl> + av_freep (& rms -> audiobuf ); <nl> } <nl>  <nl> static int rm_read_audio_stream_info ( AVFormatContext * s , ByteIOContext * pb ,
static int xv_write_trailer ( AVFormatContext * s ) <nl> XShmDetach ( xv -> display , & xv -> yuv_shminfo ); <nl> shmdt ( xv -> yuv_image -> data ); <nl> XFree ( xv -> yuv_image ); <nl> + XFreeGC ( xv -> display , xv -> gc ); <nl> XCloseDisplay ( xv -> display ); <nl> return 0 ; <nl> }
static int asf_write_header ( AVFormatContext * s ) <nl> asf -> nb_packets = 0 ; <nl>  <nl> asf -> index_ptr = av_malloc ( sizeof ( ASFIndex ) * ASF_INDEX_BLOCK ); <nl> + if (! asf -> index_ptr ) <nl> + return AVERROR ( ENOMEM ); <nl> asf -> nb_index_memory_alloc = ASF_INDEX_BLOCK ; <nl> asf -> maximum_packet = 0 ; <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> ret = calc_active_inputs ( s ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + } <nl>  <nl> + if ( s -> active_inputs > 1 ) { <nl> available_samples = get_available_samples ( s ); <nl> if (! available_samples ) <nl> return AVERROR ( EAGAIN );
static void compute_rematrixing_strategy ( AC3EncodeContext * s ) <nl> { <nl> int nb_coefs ; <nl> int blk , bnd ; <nl> - AC3Block * block , * block0 ; <nl> + AC3Block * block , * block0 = NULL ; <nl>  <nl> if ( s -> channel_mode != AC3_CHMODE_STEREO ) <nl> return ;
static int concat_read_packet ( AVFormatContext * avf , AVPacket * pkt ) <nl> ConcatStream * cs ; <nl> AVStream * st ; <nl>  <nl> + if (! cat -> avf ) <nl> + return AVERROR ( EIO ); <nl> + <nl> while ( 1 ) { <nl> ret = av_read_frame ( cat -> avf , pkt ); <nl> if ( ret == AVERROR_EOF ) {
static int webm_dash_manifest_read_header ( AVFormatContext * s ) <nl> av_log ( s , AV_LOG_ERROR , " Failed to read file headers \ n "); <nl> return - 1 ; <nl> } <nl> + if (! s -> nb_streams ) { <nl> + matroska_read_close ( s ); <nl> + av_log ( s , AV_LOG_ERROR , " No streams found \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> if (! matroska -> is_live ) { <nl> buf = av_asprintf ("% g ", matroska -> duration );
not_extra : <nl> if (!( s -> flags2 & CODEC_FLAG2_CHUNKS ) && ! s -> current_picture_ptr ){ <nl> if ( avctx -> skip_frame >= AVDISCARD_NONREF || <nl> buf_size >= 4 && ! memcmp (" Q264 ", buf , 4 )) <nl> - return 0 ; <nl> + return buf_size ; <nl> av_log ( avctx , AV_LOG_ERROR , " no frame !\ n "); <nl> return - 1 ; <nl> }
static int vp3_update_thread_context ( AVCodecContext * dst , const AVCodecContext * <nl> || s -> width != s1 -> width <nl> || s -> height != s1 -> height ) { <nl> if ( s != s1 ) <nl> - copy_fields ( s , s1 , golden_frame , current_frame ); <nl> + copy_fields ( s , s1 , golden_frame , keyframe ); <nl> return - 1 ; <nl> } <nl> 
static av_cold int xvid_encode_close ( AVCodecContext * avctx ) { <nl> xvid_encore ( x -> encoder_handle , XVID_ENC_DESTROY , NULL , NULL ); <nl>  <nl> if ( avctx -> extradata != NULL ) <nl> - av_free ( avctx -> extradata ); <nl> + av_freep (& avctx -> extradata ); <nl> if ( x -> twopassbuffer != NULL ) { <nl> av_free ( x -> twopassbuffer ); <nl> av_free ( x -> old_twopassbuffer );
av_cold void ff_init_range_decoder ( RangeCoder * c , const uint8_t * buf , <nl>  <nl> c -> low = AV_RB16 ( c -> bytestream ); <nl> c -> bytestream += 2 ; <nl> + if ( c -> low >= 0xFF00 ) { <nl> + c -> low = 0xFF00 ; <nl> + c -> bytestream_end = c -> bytestream + 2 ; <nl> + } <nl> } <nl>  <nl> void ff_build_rac_states ( RangeCoder * c , int factor , int max_p )
int ff_mjpeg_decode_sof ( MJpegDecodeContext * s ) <nl> s -> avctx -> bits_per_raw_sample = <nl> bits = get_bits (& s -> gb , 8 ); <nl>  <nl> + if ( bits > 16 || bits < 1 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " bits % d is invalid \ n ", bits ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( s -> pegasus_rct ) <nl> bits = 9 ; <nl> if ( bits == 9 && ! s -> pegasus_rct )
typedef struct XMVAudioPacket { <nl> uint16_t bits_per_sample ; ///< Bits per compressed sample . <nl> uint32_t bit_rate ; ///< Bits of compressed data per second . <nl> uint16_t flags ; ///< Flags <nl> - uint16_t block_align ; ///< Bytes per compressed block . <nl> + unsigned block_align ; ///< Bytes per compressed block . <nl> uint16_t block_samples ; ///< Decompressed samples per compressed block . <nl>  <nl> enum AVCodecID codec_id ; ///< The codec ID of the compression scheme .
static int mpeg4_update_thread_context ( AVCodecContext * dst , <nl> s -> time_increment_bits = s1 -> time_increment_bits ; <nl> s -> vol_sprite_usage = s1 -> vol_sprite_usage ; <nl> s -> rvlc = s1 -> rvlc ; <nl> + s -> divx_version = s1 -> divx_version ; <nl> + s -> divx_build = s1 -> divx_build ; <nl> + s -> xvid_build = s1 -> xvid_build ; <nl> + s -> lavc_build = s1 -> lavc_build ; <nl>  <nl> return 0 ; <nl> }
copy_oid ( const oid * from , oid * to ) <nl> to -> components = malloc ( to -> length * sizeof (* to -> components )); <nl> if ( to -> length != 0 && to -> components == NULL ) <nl> return ENOMEM ; <nl> - memcpy ( to -> components , from -> components , to -> length ); <nl> + memcpy ( to -> components , from -> components , to -> length * sizeof (* to -> components )); <nl> return 0 ; <nl> }
wav_new_out ( struct fileops * ops , struct dev * dev , <nl> } <nl> f -> mode = mode ; <nl> f -> pstate = WAV_CFG ; <nl> - f -> endpos = f -> startpos = 0 ; <nl> + f -> mmcpos = f -> endpos = f -> startpos = 0 ; <nl> f -> next = wav_list ; <nl> wav_list = f ; <nl> if ( hdr == HDR_WAV ) {
struct cpu_disklabel { <nl> int cd_dummy ; /* must have one element . */ <nl> }; <nl>  <nl> -# ifdef _KERNEL <nl> - struct disklabel ; <nl> - int bounds_check_with_label __P (( struct buf *, struct disklabel *, int )); <nl> -# endif <nl> - <nl> # endif /* _MACHINE_DISKLABEL_H_ */
API_EXPORT ( int ) ap_call_exec ( request_rec * r , child_info * pinfo , char * argv0 , <nl> else if (( conf -> cgi_command_args == AP_FLAG_OFF ) <nl> || (! r -> args ) || (! r -> args [ 0 ]) <nl> || strchr ( r -> args , '=')) { <nl> - execle ( r -> filename , argv0 , NULL , env ); <nl> + execle ( r -> filename , argv0 , ( void *) NULL , env ); <nl> } <nl>  <nl> else {
# define _PATH_MAN "/ usr / share / man " <nl> # define _PATH_MEM "/ dev / mem " <nl> # define _PATH_NOLOGIN "/ etc / nologin " <nl> +# define _PATH_RSH "/ usr / bin / rsh " <nl> # define _PATH_SENDMAIL "/ usr / sbin / sendmail " <nl> # define _PATH_SHELLS "/ etc / shells " <nl> # define _PATH_TTY "/ dev / tty "
output_data ( const char * format , ...) <nl>  <nl> va_start ( args , format ); <nl> remaining = BUFSIZ - ( nfrontp - netobuf ); <nl> + if ( remaining == 0 ) <nl> + return remaining ; <nl> if (( n = vsnprintf ( nfrontp , remaining , format , args )) >= remaining || n < 0 ) <nl> n = strlen ( nfrontp ); <nl> nfrontp += n ;
-/* $ OpenBSD : file_media . c , v 1 . 13 2016 / 01 / 11 07 : 57 : 54 jasper Exp $ */ <nl> +/* $ OpenBSD : file_media . c , v 1 . 14 2016 / 01 / 11 14 : 27 : 29 jasper Exp $ */ <nl>  <nl> /* <nl> * file_media . c - <nl> compute_block_size ( int fd ) <nl> } <nl> } <nl> } <nl> + free ( buffer ); <nl> return 0 ; <nl> } <nl> 
elf32_arm_size_dynamic_sections ( bfd * output_bfd ATTRIBUTE_UNUSED , <nl> if ( elf_hash_table ( info )-> dynamic_sections_created ) <nl> { <nl> /* Set the contents of the . interp section to the interpreter . */ <nl> - if ( info -> executable ) <nl> + if ( info -> executable && ! info -> static_link ) <nl> { <nl> s = bfd_get_section_by_name ( dynobj , ". interp "); <nl> BFD_ASSERT ( s != NULL );
_dl_lookup_symbol ( const char * undef_name , const Elf32_Sym ** ref , <nl> { <nl> Elf32_Addr a ; <nl> const Elf32_Sym * s ; <nl> - } weak_value = { 0 , NULL }; <nl> + } weak_value ; /* = { 0 , NULL }; breaks GCC 2 . 8 due to implicit memset */ <nl> + <nl> + _dl_memset (& weak_value , 0 , sizeof ( weak_value )); <nl>  <nl> /* Search the relevant loaded objects for a definition . */ <nl> for ( map = symbol_scope ; map ; map = map -> next )
static int uhci_handle_td ( UHCIState * s , uint32_t addr , UHCI_TD * td , uint32_t * in <nl>  <nl> /* Allocate new packet */ <nl> async = uhci_async_alloc ( uhci_queue_get ( s , td ), addr ); <nl> - if (! async ) <nl> - return TD_RESULT_NEXT_QH ; <nl>  <nl> /* valid needs to be large enough to handle 10 frame delay <nl> * for initial isochronous requests
# ifndef TRACE__EVENT_INTERNAL_H <nl> # define TRACE__EVENT_INTERNAL_H <nl>  <nl> -# include " trace / generated - events . h " <nl> - <nl> - <nl> /** <nl> * TraceEvent : <nl> * @ id : Unique event identifier . <nl> * Opaque generic description of a tracing event . <nl> */ <nl> typedef struct TraceEvent { <nl> - TraceEventID id ; <nl> - TraceEventVCPUID vcpu_id ; <nl> + uint32_t id ; <nl> + uint32_t vcpu_id ; <nl> const char * name ; <nl> const bool sstate ; <nl> uint16_t * dstate ;
static int parallels_open ( BlockDriverState * bs , QDict * options , int flags , <nl> if ( local_err != NULL ) { <nl> goto fail_options ; <nl> } <nl> - if (! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> + <nl> + if (!( flags & BDRV_O_RESIZE ) || ! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> bdrv_truncate ( bs -> file , bdrv_getlength ( bs -> file -> bs )) != 0 ) { <nl> s -> prealloc_mode = PRL_PREALLOC_MODE_FALLOCATE ; <nl> }
 <nl> # include " virtio - net . h " <nl> # include " vhost_net . h " <nl> +# include " qemu - error . h " <nl>  <nl> # include " config . h " <nl>  <nl> void vhost_net_cleanup ( struct vhost_net * net ) <nl> struct vhost_net * vhost_net_init ( VLANClientState * backend , int devfd , <nl> bool force ) <nl> { <nl> + error_report (" vhost - net support is not compiled in "); <nl> return NULL ; <nl> } <nl> 
int vhost_set_vring_enable ( NetClientState * nc , int enable ) <nl>  <nl> nc -> vring_enable = enable ; <nl>  <nl> - if ( vhost_ops -> vhost_set_vring_enable ) { <nl> + if ( vhost_ops && vhost_ops -> vhost_set_vring_enable ) { <nl> return vhost_ops -> vhost_set_vring_enable (& net -> dev , enable ); <nl> } <nl> 
static int qed_create ( const char * filename , uint32_t cluster_size , <nl> return ret ; <nl> } <nl>  <nl> + /* File must start empty and grow , check truncate is supported */ <nl> + ret = bdrv_truncate ( bs , 0 ); <nl> + if ( ret < 0 ) { <nl> + goto out ; <nl> + } <nl> + <nl> if ( backing_file ) { <nl> header . features |= QED_F_BACKING_FILE ; <nl> header . backing_filename_offset = sizeof ( le_header );
static void slavio_check_interrupts ( SLAVIO_INTCTLState * s , int set_irqs ) <nl> CPU_IRQ_TIMER_IN ; <nl> if ( i == s -> target_cpu ) { <nl> for ( j = 0 ; j < 32 ; j ++) { <nl> - if (( s -> intregm_pending & ( 1 << j )) && intbit_to_level [ j ]) { <nl> + if (( s -> intregm_pending & ( 1U << j )) && intbit_to_level [ j ]) { <nl> s -> slaves [ i ]. intreg_pending |= 1 << intbit_to_level [ j ]; <nl> } <nl> }
# include < sys / resource . h > <nl> # endif <nl>  <nl> -# if ( defined ( _WIN32 ) || defined RUSAGE_THREAD ) <nl> +# if ( defined ( _WIN32 ) || defined RUSAGE_THREAD ) && \ <nl> + ( defined ( CONFIG_NETTLE_KDF ) || defined ( CONFIG_GCRYPT_KDF )) <nl> # define TEST_LUKS <nl> # else <nl> # undef TEST_LUKS
static int usb_serial_initfn ( USBDevice * dev ) <nl> USBSerialState * s = DO_UPCAST ( USBSerialState , dev , dev ); <nl> s -> dev . speed = USB_SPEED_FULL ; <nl>  <nl> + if (! s -> cs ) { <nl> + error_report (" Property chardev is required "); <nl> + return - 1 ; <nl> + } <nl> + <nl> qemu_chr_add_handlers ( s -> cs , usb_serial_can_read , usb_serial_read , <nl> usb_serial_event , s ); <nl> usb_serial_handle_reset ( dev );
DMAContext * spapr_tce_new_dma_context ( uint32_t liobn , size_t window_size ) <nl> { <nl> sPAPRTCETable * tcet ; <nl>  <nl> + if ( spapr_tce_find_by_liobn ( liobn )) { <nl> + fprintf ( stderr , " Attempted to create TCE table with duplicate " <nl> + " LIOBN 0x % x \ n ", liobn ); <nl> + return NULL ; <nl> + } <nl> + <nl> if (! window_size ) { <nl> return NULL ; <nl> }
int qcow2_grow_l1_table ( BlockDriverState * bs , uint64_t min_size , <nl> } <nl> } <nl>  <nl> - if ( new_l1_size > INT_MAX ) { <nl> + if ( new_l1_size > INT_MAX / sizeof ( uint64_t )) { <nl> return - EFBIG ; <nl> } <nl> 
static int decode_gusa ( DisasContext * ctx , CPUSH4State * env , int * pmax_insns ) <nl> } <nl>  <nl> /* If op_src is not a valid register , then op_arg was a constant . */ <nl> - if ( op_src < 0 ) { <nl> + if ( op_src < 0 && ! TCGV_IS_UNUSED ( op_arg )) { <nl> tcg_temp_free_i32 ( op_arg ); <nl> } <nl> 
int64_t bdrv_getlength ( BlockDriverState * bs ) <nl> { <nl> int64_t ret = bdrv_nb_sectors ( bs ); <nl>  <nl> + ret = ret > INT64_MAX / BDRV_SECTOR_SIZE ? - EFBIG : ret ; <nl> return ret < 0 ? ret : ret * BDRV_SECTOR_SIZE ; <nl> } <nl> 
static void usb_msd_realize_bot ( USBDevice * dev , Error ** errp ) <nl> usb_desc_init ( dev ); <nl> scsi_bus_new (& s -> bus , sizeof ( s -> bus ), DEVICE ( dev ), <nl> & usb_msd_scsi_info_bot , NULL ); <nl> - s -> bus . qbus . allow_hotplug = 0 ; <nl> usb_msd_handle_reset ( dev ); <nl> } <nl> 
static void ide_dma_cb ( void * opaque , int ret ) <nl> } <nl> if ( ret < 0 ) { <nl> if ( ide_handle_rw_error ( s , - ret , ide_dma_cmd_to_retry ( s -> dma_cmd ))) { <nl> + s -> bus -> dma -> aiocb = NULL ; <nl> return ; <nl> } <nl> }
static inline int array_ensure_allocated ( array_t * array , int index ) <nl> array -> pointer = g_realloc ( array -> pointer , new_size ); <nl> if (! array -> pointer ) <nl> return - 1 ; <nl> + memset ( array -> pointer + array -> size , 0 , new_size - array -> size ); <nl> array -> size = new_size ; <nl> array -> next = index + 1 ; <nl> }
static int img_amend ( int argc , char ** argv ) <nl> } <nl>  <nl> if ( optind != argc - 1 ) { <nl> - error_exit (" Expecting one image file name "); <nl> + error_report (" Expecting one image file name "); <nl> + ret = - 1 ; <nl> + goto out ; <nl> } <nl>  <nl> flags = BDRV_O_FLAGS | BDRV_O_RDWR ;
void tb_invalidate_phys_addr ( target_phys_addr_t addr ) <nl>  <nl> static void breakpoint_invalidate ( CPUArchState * env , target_ulong pc ) <nl> { <nl> - tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( env , pc )); <nl> + tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( env , pc ) | <nl> + ( pc & ~ TARGET_PAGE_MASK )); <nl> } <nl> # endif <nl> # endif /* TARGET_HAS_ICE */
PropertyInfo qdev_prop_bit = { <nl> static uint64_t qdev_get_prop_mask64 ( Property * prop ) <nl> { <nl> assert ( prop -> info == & qdev_prop_bit ); <nl> - return 0x1 << prop -> bitnr ; <nl> + return 0x1ull << prop -> bitnr ; <nl> } <nl>  <nl> static void bit64_prop_set ( DeviceState * dev , Property * props , bool val )
static target_ulong disas_insn ( DisasContext * s , target_ulong pc_start ) <nl> break ; <nl> case 5 : /* lfence */ <nl> case 6 : /* mfence */ <nl> - if (( modrm & 0xc7 ) != 0xc0 || !( s -> cpuid_features & CPUID_SSE )) <nl> + if (( modrm & 0xc7 ) != 0xc0 || !( s -> cpuid_features & CPUID_SSE2 )) <nl> goto illegal_op ; <nl> break ; <nl> case 7 : /* sfence / clflush */
static void leon3_generic_hw_init ( MachineState * machine ) <nl> fprintf ( stderr , " Can ' t read bios image % s \ n ", filename ); <nl> exit ( 1 ); <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Can directly load an application . */ <nl> if ( kernel_filename != NULL ) {
static const TypeInfo qemu_s390_skeys_info = { <nl> . instance_init = qemu_s390_skeys_init , <nl> . instance_size = sizeof ( QEMUS390SKeysState ), <nl> . class_init = qemu_s390_skeys_class_init , <nl> - . instance_size = sizeof ( S390SKeysClass ), <nl> + . class_size = sizeof ( S390SKeysClass ), <nl> }; <nl>  <nl> static void s390_storage_keys_save ( QEMUFile * f , void * opaque )
void qemu_chr_be_write ( CharDriverState * s , uint8_t * buf , int len ) <nl> int qemu_chr_fe_get_msgfd ( CharDriverState * s ) <nl> { <nl> int fd ; <nl> - return ( qemu_chr_fe_get_msgfds ( s , & fd , 1 ) >= 0 ) ? fd : - 1 ; <nl> + return ( qemu_chr_fe_get_msgfds ( s , & fd , 1 ) == 1 ) ? fd : - 1 ; <nl> } <nl>  <nl> int qemu_chr_fe_get_msgfds ( CharDriverState * s , int * fds , int len )
void ppce500_init ( MachineState * machine , PPCE500Params * params ) <nl> exit ( 1 ); <nl> } <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Reserve space for dtb */ <nl> dt_base = ( loadaddr + bios_size + DTC_LOAD_PAD ) & ~ DTC_PAD_MASK ;
static int blk_root_inactivate ( BdrvChild * child ) <nl> * this point because the VM is stopped ) and unattached monitor - owned <nl> * BlockBackends . If there is still any other user like a block job , then <nl> * we simply can ' t inactivate the image . */ <nl> - if (! blk -> dev && ! blk -> name [ 0 ]) { <nl> + if (! blk -> dev && ! blk_name ( blk )[ 0 ]) { <nl> return - EPERM ; <nl> } <nl> 
glue ( cirrus_bitblt_rop_fwd_ , ROP_NAME )( CirrusVGAState * s , <nl> dstpitch -= bltwidth ; <nl> srcpitch -= bltwidth ; <nl>  <nl> - if ( dstpitch < 0 || srcpitch < 0 ) { <nl> - /* is 0 valid ? srcpitch == 0 could be useful */ <nl> + if ( bltheight > 1 && ( dstpitch < 0 || srcpitch < 0 )) { <nl> return ; <nl> } <nl> 
file_backend_instance_init ( Object * o ) <nl> set_mem_path , NULL ); <nl> } <nl>  <nl> + static void file_backend_instance_finalize ( Object * o ) <nl> +{ <nl> + HostMemoryBackendFile * fb = MEMORY_BACKEND_FILE ( o ); <nl> + <nl> + g_free ( fb -> mem_path ); <nl> +} <nl> + <nl> static const TypeInfo file_backend_info = { <nl> . name = TYPE_MEMORY_BACKEND_FILE , <nl> . parent = TYPE_MEMORY_BACKEND , <nl> . class_init = file_backend_class_init , <nl> . instance_init = file_backend_instance_init , <nl> + . instance_finalize = file_backend_instance_finalize , <nl> . instance_size = sizeof ( HostMemoryBackendFile ), <nl> }; <nl> 
void qemu_opts_del ( QemuOpts * opts ) <nl> { <nl> QemuOpt * opt ; <nl>  <nl> + if ( opts == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> for (;;) { <nl> opt = QTAILQ_FIRST (& opts -> head ); <nl> if ( opt == NULL )
int vhost_dev_init ( struct vhost_dev * hdev , void * opaque , <nl> if (!( hdev -> features & ( 0x1ULL << VHOST_F_LOG_ALL ))) { <nl> error_setg (& hdev -> migration_blocker , <nl> " Migration disabled : vhost lacks VHOST_F_LOG_ALL feature ."); <nl> - } else if (! qemu_memfd_check ()) { <nl> + } else if ( vhost_dev_log_is_shared ( hdev ) && ! qemu_memfd_check ()) { <nl> error_setg (& hdev -> migration_blocker , <nl> " Migration disabled : failed to allocate shared memory "); <nl> }
void restore_boot_order ( void * opaque ) <nl> return ; <nl> } <nl>  <nl> - qemu_boot_set ( normal_boot_order , NULL ); <nl> + if ( boot_set_handler ) { <nl> + qemu_boot_set ( normal_boot_order , & error_abort ); <nl> + } <nl>  <nl> qemu_unregister_reset ( restore_boot_order , normal_boot_order ); <nl> g_free ( normal_boot_order );
CPUArchState * cpu_copy ( CPUArchState * env ) <nl> { <nl> CPUState * cpu = ENV_GET_CPU ( env ); <nl> CPUState * new_cpu = cpu_init ( cpu_model ); <nl> - CPUArchState * new_env = cpu -> env_ptr ; <nl> + CPUArchState * new_env = new_cpu -> env_ptr ; <nl> CPUBreakpoint * bp ; <nl> CPUWatchpoint * wp ; <nl> 
static int get_cluster_offset ( BlockDriverState * bs , <nl> uint32_t min_count , * l2_table ; <nl> bool zeroed = false ; <nl> int64_t ret ; <nl> - int32_t cluster_sector ; <nl> + int64_t cluster_sector ; <nl>  <nl> if ( m_data ) { <nl> m_data -> valid = 0 ;
static void qemu_cleanup_net_client ( NetClientState * nc ) <nl> { <nl> QTAILQ_REMOVE (& net_clients , nc , next ); <nl>  <nl> - nc -> info -> cleanup ( nc ); <nl> + if ( nc -> info -> cleanup ) { <nl> + nc -> info -> cleanup ( nc ); <nl> + } <nl> } <nl>  <nl> static void qemu_free_net_client ( NetClientState * nc )
static uint32_t vmxnet3_get_interrupt_config ( VMXNET3State * s ) <nl> static void vmxnet3_fill_stats ( VMXNET3State * s ) <nl> { <nl> int i ; <nl> + <nl> + if (! s -> device_active ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < s -> txq_num ; i ++) { <nl> cpu_physical_memory_write ( s -> txq_descr [ i ]. tx_stats_pa , <nl> & s -> txq_descr [ i ]. txq_stats ,
int net_init_l2tpv3 ( const NetClientOptions * opts , <nl> if ( fd == - 1 ) { <nl> fd = - errno ; <nl> error_report (" l2tpv3_open : socket creation failed , errno = % d ", - fd ); <nl> - freeaddrinfo ( result ); <nl> goto outerr ; <nl> } <nl> if ( bind ( fd , ( struct sockaddr *) result -> ai_addr , result -> ai_addrlen )) {
int mmu_translate ( CPUS390XState * env , target_ulong vaddr , int rw , uint64_t asc , <nl> /* Convert real address -> absolute address */ <nl> * raddr = mmu_real2abs ( env , * raddr ); <nl>  <nl> - if (* raddr <= ram_size ) { <nl> + if (* raddr < ram_size ) { <nl> sk = & env -> storage_keys [* raddr / TARGET_PAGE_SIZE ]; <nl> if (* flags & PAGE_READ ) { <nl> * sk |= SK_R ;
static void virtio_device_realize ( DeviceState * dev , Error ** errp ) <nl> virtio_bus_device_plugged ( vdev , & err ); <nl> if ( err != NULL ) { <nl> error_propagate ( errp , err ); <nl> + vdc -> unrealize ( dev , NULL ); <nl> return ; <nl> } <nl> 
static int ioreq_runio_qemu_aio ( struct ioreq * ioreq ) <nl> break ; <nl> case BLKIF_OP_WRITE : <nl> case BLKIF_OP_WRITE_BARRIER : <nl> - ioreq -> aio_inflight ++; <nl> if (! ioreq -> req . nr_segments ) <nl> break ; <nl> + ioreq -> aio_inflight ++; <nl> bdrv_aio_writev ( blkdev -> bs , ioreq -> start / BLOCK_SIZE , <nl> & ioreq -> v , ioreq -> v . size / BLOCK_SIZE , <nl> qemu_aio_complete , ioreq );
static int handle_instruction ( CPUState * env , struct kvm_run * run ) <nl> if ( r < 0 ) { <nl> enter_pgmcheck ( env , 0x0001 ); <nl> } <nl> - return r ; <nl> + return 0 ; <nl> } <nl>  <nl> static int handle_intercept ( CPUState * env )
int rom_add_file ( const char * file , const char * fw_dir , <nl> err : <nl> if ( fd != - 1 ) <nl> close ( fd ); <nl> + <nl> g_free ( rom -> data ); <nl> g_free ( rom -> path ); <nl> g_free ( rom -> name ); <nl> + if ( fw_dir ) { <nl> + g_free ( rom -> fw_dir ); <nl> + g_free ( rom -> fw_file ); <nl> + } <nl> g_free ( rom ); <nl> + <nl> return - 1 ; <nl> } <nl> 
do_check_protect_pse36 : <nl>  <nl> /* the page can be put in the TLB */ <nl> prot = PAGE_READ ; <nl> - if (!( ptep & PG_NX_MASK )) <nl> + if (!( ptep & PG_NX_MASK ) && <nl> + !(( env -> cr [ 4 ] & CR4_SMEP_MASK ) && ( ptep & PG_USER_MASK ))) { <nl> prot |= PAGE_EXEC ; <nl> + } <nl> if ( pte & PG_DIRTY_MASK ) { <nl> /* only set write access if already dirty ... otherwise wait <nl> for dirty access */
void monitor_init ( CharDriverState * hd , int show_banner ) <nl> hide_banner = ! show_banner ; <nl>  <nl> qemu_chr_add_handlers ( hd , term_can_read , term_read , term_event , NULL ); <nl> + <nl> + readline_start ("", 0 , monitor_handle_command1 , NULL ); <nl> } <nl>  <nl> /* XXX : use threads ? */
static int print_block_option_help ( const char * filename , const char * fmt ) <nl> proto_drv = bdrv_find_protocol ( filename , true ); <nl> if (! proto_drv ) { <nl> error_report (" Unknown protocol '% s '", filename ); <nl> + free_option_parameters ( create_options ); <nl> return 1 ; <nl> } <nl> create_options = append_option_parameters ( create_options ,
e1000e_set_pbaclr ( E1000ECore * core , int index , uint32_t val ) <nl>  <nl> core -> mac [ PBACLR ] = val & E1000_PBACLR_VALID_MASK ; <nl>  <nl> - if ( msix_enabled ( core -> owner )) { <nl> + if (! msix_enabled ( core -> owner )) { <nl> return ; <nl> } <nl> 
static void make_dirty ( uint8_t device ) <nl>  <nl> guest_buf = guest_alloc ( guest_malloc , len ); <nl> buf = g_malloc ( len ); <nl> + memset ( buf , rand () % 255 + 1 , len ); <nl> g_assert ( guest_buf ); <nl> g_assert ( buf ); <nl> 
static void realize ( DeviceState * d , Error ** errp ) <nl> error_free ( err ); <nl> object_unref ( OBJECT ( drc )); <nl> } <nl> + g_free ( child_name ); <nl> DPRINTFN (" drc realize complete "); <nl> } <nl> 
static int floppy_probe_device ( const char * filename ) <nl> struct stat st ; <nl>  <nl> if ( strstart ( filename , "/ dev / fd ", NULL ) && <nl> - ! strstart ( filename , "/ dev / fdset /", NULL )) { <nl> + ! strstart ( filename , "/ dev / fdset /", NULL ) && <nl> + ! strstart ( filename , "/ dev / fd /", NULL )) { <nl> prio = 50 ; <nl> } <nl> 
static const RunStateTransition runstate_transitions_def [] = { <nl> { RUN_STATE_WATCHDOG , RUN_STATE_FINISH_MIGRATE }, <nl>  <nl> { RUN_STATE_GUEST_PANICKED , RUN_STATE_PAUSED }, <nl> + { RUN_STATE_GUEST_PANICKED , RUN_STATE_FINISH_MIGRATE }, <nl>  <nl> { RUN_STATE_MAX , RUN_STATE_MAX }, <nl> };
static void calxeda_init ( MachineState * machine , enum cxmachines machine_id ) <nl> if ( bios_name != NULL ) { <nl> sysboot_filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> if ( sysboot_filename != NULL ) { <nl> - uint32_t filesize = get_image_size ( sysboot_filename ); <nl> - if ( load_image_targphys (" sysram . bin ", 0xfff88000 , filesize ) < 0 ) { <nl> + if ( load_image_targphys ( sysboot_filename , 0xfff88000 , 0x8000 ) < 0 ) { <nl> hw_error (" Unable to load % s \ n ", bios_name ); <nl> } <nl> g_free ( sysboot_filename );
static inline void hwsetup_add_tag ( HWSetup * hw , enum hwsetup_tag t ) <nl>  <nl> static inline void hwsetup_add_str ( HWSetup * hw , const char * str ) <nl> { <nl> - strncpy ( hw -> ptr , str , 31 ); /* make sure last byte is zero */ <nl> + pstrcpy ( hw -> ptr , 32 , str ); <nl> hw -> ptr += 32 ; <nl> } <nl> 
static void raise_mmu_exception ( CPUMIPSState * env , target_ulong address , <nl> env -> CP0_Context = ( env -> CP0_Context & ~ 0x007fffff ) | <nl> (( address >> 9 ) & 0x007ffff0 ); <nl> env -> CP0_EntryHi = ( env -> CP0_EntryHi & env -> CP0_EntryHi_ASID_mask ) | <nl> + ( env -> CP0_EntryHi & ( 1 << CP0EnHi_EHINV )) | <nl> ( address & ( TARGET_PAGE_MASK << 1 )); <nl> # if defined ( TARGET_MIPS64 ) <nl> env -> CP0_EntryHi &= env -> SEGMask ;
static inline void elf_core_copy_regs ( target_elf_gregset_t * regs , <nl> int i ; <nl>  <nl> for ( i = 0 ; i < 16 ; i ++) { <nl> - (* regs [ i ]) = tswapreg ( env -> gregs [ i ]); <nl> + (* regs )[ i ] = tswapreg ( env -> gregs [ i ]); <nl> } <nl>  <nl> (* regs )[ TARGET_REG_PC ] = tswapreg ( env -> pc );
static void do_dma_memory_set ( dma_addr_t addr , uint8_t c , dma_addr_t len ) <nl> while ( len > 0 ) { <nl> l = len < FILLBUF_SIZE ? len : FILLBUF_SIZE ; <nl> cpu_physical_memory_rw ( addr , fillbuf , l , true ); <nl> - len -= len ; <nl> - addr += len ; <nl> + len -= l ; <nl> + addr += l ; <nl> } <nl> } <nl> 
static void pmac_ide_atapi_transfer_cb ( void * opaque , int ret ) <nl> s -> io_buffer_size = MIN ( s -> io_buffer_size , io -> len ); <nl> dma_memory_write (& address_space_memory , io -> addr , s -> io_buffer , <nl> s -> io_buffer_size ); <nl> + io -> len = 0 ; <nl> ide_atapi_cmd_ok ( s ); <nl> m -> dma_active = false ; <nl> goto done ;
void * address_space_map ( AddressSpace * as , <nl> if ( bounce . buffer ) { <nl> return NULL ; <nl> } <nl> - bounce . buffer = qemu_memalign ( TARGET_PAGE_SIZE , TARGET_PAGE_SIZE ); <nl> + /* Avoid unbounded allocations */ <nl> + l = MIN ( l , TARGET_PAGE_SIZE ); <nl> + bounce . buffer = qemu_memalign ( TARGET_PAGE_SIZE , l ); <nl> bounce . addr = addr ; <nl> bounce . len = l ; <nl> 
static void sdhci_send_command ( SDHCIState * s ) <nl> ( s -> cmdreg & SDHC_CMD_RESPONSE ) == SDHC_CMD_RSP_WITH_BUSY ) { <nl> s -> norintsts |= SDHC_NIS_TRSCMP ; <nl> } <nl> - } else if ( rlen != 0 && ( s -> errintstsen & SDHC_EISEN_CMDIDX )) { <nl> - s -> errintsts |= SDHC_EIS_CMDIDX ; <nl> - s -> norintsts |= SDHC_NIS_ERR ; <nl> } <nl>  <nl> if ( s -> norintstsen & SDHC_NISEN_CMDCMP ) {
static int load_refcount_block ( BlockDriverState * bs , <nl> static int get_refcount ( BlockDriverState * bs , int64_t cluster_index ) <nl> { <nl> BDRVQcowState * s = bs -> opaque ; <nl> - int refcount_table_index , block_index ; <nl> + uint64_t refcount_table_index , block_index ; <nl> int64_t refcount_block_offset ; <nl> int ret ; <nl> uint16_t * refcount_block ;
void ptimer_set_limit ( ptimer_state * s , uint64_t limit , int reload ) <nl> * on the current generation of host machines . <nl> */ <nl>  <nl> - if ( limit * s -> period < 10000 && s -> period ) { <nl> + if (! use_icount && limit * s -> period < 10000 && s -> period ) { <nl> limit = 10000 / s -> period ; <nl> } <nl> 
static Qcow2BitmapList * bitmap_list_load ( BlockDriverState * bs , uint64_t offset , <nl> goto fail ; <nl> } <nl>  <nl> - bm = g_new ( Qcow2Bitmap , 1 ); <nl> + bm = g_new0 ( Qcow2Bitmap , 1 ); <nl> bm -> table . offset = e -> bitmap_table_offset ; <nl> bm -> table . size = e -> bitmap_table_size ; <nl> bm -> flags = e -> flags ;
static int vhost_user_cleanup ( struct vhost_dev * dev ) <nl>  <nl> u = dev -> opaque ; <nl> if ( u -> slave_fd >= 0 ) { <nl> + qemu_set_fd_handler ( u -> slave_fd , NULL , NULL , NULL ); <nl> close ( u -> slave_fd ); <nl> u -> slave_fd = - 1 ; <nl> }
static void qio_channel_websock_handshake_send_res_ok ( QIOChannelWebsock * ioc , <nl> char combined_key [ QIO_CHANNEL_WEBSOCK_CLIENT_KEY_LEN + <nl> QIO_CHANNEL_WEBSOCK_GUID_LEN + 1 ]; <nl> char * accept = NULL ; <nl> - char * date = qio_channel_websock_date_str (); <nl> + char * date = NULL ; <nl>  <nl> g_strlcpy ( combined_key , key , QIO_CHANNEL_WEBSOCK_CLIENT_KEY_LEN + 1 ); <nl> g_strlcat ( combined_key , QIO_CHANNEL_WEBSOCK_GUID , <nl> static void qio_channel_websock_handshake_send_res_ok ( QIOChannelWebsock * ioc , <nl> return ; <nl> } <nl>  <nl> + date = qio_channel_websock_date_str (); <nl> qio_channel_websock_handshake_send_res ( <nl> ioc , QIO_CHANNEL_WEBSOCK_HANDSHAKE_RES_OK , date , accept ); <nl> 
Object * object_resolve_path_component ( Object * parent , const gchar * part ) <nl> } <nl>  <nl> if ( object_property_is_link ( prop )) { <nl> - return *( Object **) prop -> opaque ; <nl> + LinkProperty * lprop = prop -> opaque ; <nl> + return * lprop -> child ; <nl> } else if ( object_property_is_child ( prop )) { <nl> return prop -> opaque ; <nl> } else {
write_refblocks : <nl> * this will leak that range , but we can easily fix that by running <nl> * a leak - fixing check after this rebuild operation */ <nl> reftable_offset = - 1 ; <nl> + } else { <nl> + assert ( on_disk_reftable ); <nl> } <nl> on_disk_reftable [ refblock_index ] = refblock_offset ; <nl>  <nl> write_refblocks : <nl> goto write_refblocks ; <nl> } <nl>  <nl> - assert ( on_disk_reftable ); <nl> - <nl> for ( refblock_index = 0 ; refblock_index < reftable_size ; refblock_index ++) { <nl> cpu_to_be64s (& on_disk_reftable [ refblock_index ]); <nl> }
void virtio_cleanup ( VirtIODevice * vdev ) <nl> if ( vdev -> config ) <nl> qemu_free ( vdev -> config ); <nl> qemu_free ( vdev -> vq ); <nl> + qemu_free ( vdev ); <nl> } <nl>  <nl> static void virtio_vmstate_change ( void * opaque , int running , int reason )
int v9fs_co_st_gen ( V9fsPDU * pdu , V9fsPath * path , mode_t st_mode , <nl> }); <nl> v9fs_path_unlock ( s ); <nl> } <nl> + /* The ioctl may not be supported depending on the path */ <nl> + if ( err == - ENOTTY ) { <nl> + err = 0 ; <nl> + } <nl> return err ; <nl> } <nl> 
void do_compare_and_swap32 ( void * cpu_env , int num ) <nl> uint32_t * value = ( uint32_t *)(( CPUX86State *) cpu_env )-> regs [ R_ECX ]; <nl> DPRINTF (" commpage : compare_and_swap32 (% x , new ,% p )\ n ", old , value ); <nl>  <nl> - if ( value && old == tswap32 (* value )) <nl> + if ( old == tswap32 (* value )) <nl> { <nl> uint32_t new = (( CPUX86State *) cpu_env )-> regs [ R_EDX ]; <nl> * value = tswap32 ( new );
static struct iovec * lock_iovec ( int type , abi_ulong target_addr , <nl> errno = 0 ; <nl> return NULL ; <nl> } <nl> - if ( count > IOV_MAX ) { <nl> + if ( count < 0 || count > IOV_MAX ) { <nl> errno = EINVAL ; <nl> return NULL ; <nl> }
void do_unassigned_access ( target_phys_addr_t addr , int is_write , int is_exec , <nl> else <nl> raise_exception ( TT_DATA_ACCESS ); <nl> } <nl> - env = saved_env ; <nl>  <nl> /* flush neverland mappings created during no - fault mode , <nl> so the sequential MMU faults report proper fault types */ <nl> if ( env -> mmuregs [ 0 ] & MMU_NF ) { <nl> tlb_flush ( env , 1 ); <nl> } <nl> + <nl> + env = saved_env ; <nl> } <nl> # else <nl> void do_unassigned_access ( target_phys_addr_t addr , int is_write , int is_exec ,
static void vhost_user_cleanup ( NetClientState * nc ) <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> s -> vhost_net = NULL ; <nl> } <nl> + if ( s -> chr ) { <nl> + qemu_chr_add_handlers ( s -> chr , NULL , NULL , NULL , NULL ); <nl> + qemu_chr_fe_release ( s -> chr ); <nl> + s -> chr = NULL ; <nl> + } <nl>  <nl> qemu_purge_queued_packets ( nc ); <nl> }
int pci_bridge_initfn ( PCIDevice * dev ) <nl> br -> bus_name ); <nl> sec_bus -> parent_dev = dev ; <nl> sec_bus -> map_irq = br -> map_irq ; <nl> + /* TODO : use memory API to perform memory filtering . */ <nl> + sec_bus -> address_space_mem = parent -> address_space_mem ; <nl> + sec_bus -> address_space_io = parent -> address_space_io ; <nl>  <nl> QLIST_INIT (& sec_bus -> child ); <nl> QLIST_INSERT_HEAD (& parent -> child , sec_bus , sibling );
static int usb_msd_initfn ( USBDevice * dev ) <nl> usb_msd_handle_reset ( dev ); <nl>  <nl> if ( bdrv_key_required ( s -> conf . dinfo -> bdrv )) { <nl> - if ( s -> dev . qdev . hotplugged ) { <nl> + if ( cur_mon ) { <nl> monitor_read_bdrv_key_start ( cur_mon , s -> conf . dinfo -> bdrv , <nl> usb_msd_password_cb , s ); <nl> s -> dev . auto_attach = 0 ;
static int vmdk_write_extent ( VmdkExtent * extent , int64_t cluster_offset , <nl> goto out ; <nl> } <nl>  <nl> - data -> lba = offset >> BDRV_SECTOR_BITS ; <nl> - data -> size = buf_len ; <nl> + data -> lba = cpu_to_le64 ( offset >> BDRV_SECTOR_BITS ); <nl> + data -> size = cpu_to_le32 ( buf_len ); <nl>  <nl> n_bytes = buf_len + sizeof ( VmdkGrainMarker ); <nl> iov = ( struct iovec ) {
int socket_dgram ( SocketAddress * remote , SocketAddress * local , Error ** errp ) <nl>  <nl> default : <nl> error_setg ( errp , " socket type unsupported for datagram "); <nl> - return - 1 ; <nl> + fd = - 1 ; <nl> } <nl> qemu_opts_del ( opts ); <nl> return fd ;
PCIDevice * virtio_net_init ( PCIBus * bus , NICInfo * nd , int devfn ) <nl> n -> promisc = 1 ; /* for compatibility */ <nl>  <nl> n -> mac_table . macs = qemu_mallocz ( MAC_TABLE_ENTRIES * ETH_ALEN ); <nl> - if (! n -> mac_table . macs ) <nl> - return NULL ; <nl>  <nl> n -> vlans = qemu_mallocz ( MAX_VLAN >> 3 ); <nl> - if (! n -> vlans ) <nl> - return NULL ; <nl>  <nl> register_savevm (" virtio - net ", virtio_net_id ++, VIRTIO_NET_VM_VERSION , <nl> virtio_net_save , virtio_net_load , n );
static void timer_update_irq ( struct fs_timer_t * t ) <nl> qemu_irq_lower ( t -> irq [ 0 ]); <nl> } <nl>  <nl> - static void timer_hit ( struct fs_timer_t * t ) <nl> + static void timer_hit ( void * opaque ) <nl> { <nl> + struct fs_timer_t * t = opaque ; <nl> t -> r_intr |= 1 ; <nl> timer_update_irq ( t ); <nl> }
static int inc_refcounts ( BlockDriverState * bs , <nl> if ( refcount == s -> refcount_max ) { <nl> fprintf ( stderr , " ERROR : overflow cluster offset = 0x %" PRIx64 <nl> "\ n ", cluster_offset ); <nl> + fprintf ( stderr , " Use qemu - img amend to increase the refcount entry " <nl> + " width or qemu - img convert to create a clean copy if the " <nl> + " image cannot be opened for writing \ n "); <nl> res -> corruptions ++; <nl> continue ; <nl> }
static void vnc_dpy_copy ( DisplayChangeListener * dcl , <nl> int i , x , y , pitch , inc , w_lim , s ; <nl> int cmp_bytes ; <nl>  <nl> + if (! vd -> server ) { <nl> + /* no client connected */ <nl> + return ; <nl> + } <nl> + <nl> vnc_refresh_server_surface ( vd ); <nl> QTAILQ_FOREACH_SAFE ( vs , & vd -> clients , next , vn ) { <nl> if ( vnc_has_feature ( vs , VNC_FEATURE_COPYRECT )) {
static void fsl_imx31_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx31_realize ; <nl> - <nl> dc -> desc = " i . MX31 SOC "; <nl> + /* <nl> + * Reason : uses serial_hds in realize and the kzm board does not <nl> + * support multiple CPUs <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx31_type_info = {
int qemu_spice_set_pw_expire ( time_t expires ); <nl> int qemu_spice_migrate_info ( const char * hostname , int port , int tls_port , <nl> const char * subject ); <nl>  <nl> -# define SPICE_NEEDS_SET_MM_TIME \ <nl> - (! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 )) <nl> +# if ! defined ( SPICE_SERVER_VERSION ) || ( SPICE_SERVER_VERSION < 0xc06 ) <nl> +# define SPICE_NEEDS_SET_MM_TIME 1 <nl> +# else <nl> +# define SPICE_NEEDS_SET_MM_TIME 0 <nl> +# endif <nl>  <nl> # if SPICE_SERVER_VERSION >= 0x000c02 <nl> void qemu_spice_register_ports ( void );
static int vdi_create ( const char * filename , QEMUOptionParameter * options ) <nl> static void vdi_close ( BlockDriverState * bs ) <nl> { <nl> BDRVVdiState * s = bs -> opaque ; <nl> + <nl> + g_free ( s -> bmap ); <nl> + <nl> migrate_del_blocker ( s -> migration_blocker ); <nl> error_free ( s -> migration_blocker ); <nl> }
int main ( int argc , char ** argv ) <nl>  <nl> path = g_strdup_printf (" e1000 /% s ", models [ i ]); <nl> qtest_add_data_func ( path , models [ i ], test_device ); <nl> + g_free ( path ); <nl> } <nl>  <nl> return g_test_run ();
static int write_refcount_block_entries ( BlockDriverState * bs , <nl> return 0 ; <nl> } <nl>  <nl> + if ( first_index < 0 ) { <nl> + return 0 ; <nl> + } <nl> + <nl> first_index &= ~( REFCOUNTS_PER_SECTOR - 1 ); <nl> last_index = ( last_index + REFCOUNTS_PER_SECTOR ) <nl> & ~( REFCOUNTS_PER_SECTOR - 1 );
static void virtio_net_set_status ( struct VirtIODevice * vdev , uint8_t status ) <nl> qemu_bh_cancel ( q -> tx_bh ); <nl> } <nl> if (( n -> status & VIRTIO_NET_S_LINK_UP ) == 0 && <nl> - ( queue_status & VIRTIO_CONFIG_S_DRIVER_OK )) { <nl> + ( queue_status & VIRTIO_CONFIG_S_DRIVER_OK ) && <nl> + vdev -> vm_running ) { <nl> /* if tx is waiting we are likely have some packets in tx queue <nl> * and disabled notification */ <nl> q -> tx_waiting = 0 ;
static void isa_irq_handler ( void * opaque , int n , int level ) <nl> if ( n < 16 ) { <nl> qemu_set_irq ( isa -> i8259 [ n ], level ); <nl> } <nl> - qemu_set_irq ( isa -> ioapic [ n ], level ); <nl> + if ( isa -> ioapic ) <nl> + qemu_set_irq ( isa -> ioapic [ n ], level ); <nl> }; <nl>  <nl> static void ioport80_write ( void * opaque , uint32_t addr , uint32_t data )
static void openrisc_pic_cpu_handler ( void * opaque , int irq , int level ) <nl> { <nl> OpenRISCCPU * cpu = ( OpenRISCCPU *) opaque ; <nl> CPUState * cs = CPU ( cpu ); <nl> - uint32_t irq_bit = 1 << irq ; <nl> + uint32_t irq_bit ; <nl>  <nl> if ( irq > 31 || irq < 0 ) { <nl> return ; <nl> } <nl>  <nl> + irq_bit = 1U << irq ; <nl> + <nl> if ( level ) { <nl> cpu -> env . picsr |= irq_bit ; <nl> } else {
void qcow2_free_clusters ( BlockDriverState * bs , <nl> ret = update_refcount ( bs , offset , size , - 1 ); <nl> if ( ret < 0 ) { <nl> fprintf ( stderr , " qcow2_free_clusters failed : % s \ n ", strerror (- ret )); <nl> - abort (); <nl> + /* TODO Remember the clusters to free them later and avoid leaking */ <nl> } <nl> } <nl> 
print_insn_sparc ( bfd_vma memaddr , disassemble_info * info ) <nl> } <nl>  <nl> info -> insn_type = dis_noninsn ; /* Mark as non - valid instruction . */ <nl> - (* info -> fprintf_func ) ( stream , _ (" unknown ")); <nl> + (* info -> fprintf_func ) ( stream , ". long %# 08lx ", insn ); <nl> return sizeof ( buffer ); <nl> }
void hmp_info_local_apic ( Monitor * mon , const QDict * qdict ) <nl>  <nl> void hmp_info_io_apic ( Monitor * mon , const QDict * qdict ) <nl> { <nl> - if ( kvm_irqchip_in_kernel ()) { <nl> + if ( kvm_irqchip_in_kernel () && <nl> + ! kvm_irqchip_is_split ()) { <nl> kvm_ioapic_dump_state ( mon , qdict ); <nl> } else { <nl> ioapic_dump_state ( mon , qdict );
struct TranslationBlock ; <nl> typedef struct TranslationBlock TranslationBlock ; <nl>  <nl> /* XXX : make safe guess about sizes */ <nl> -# if ( HOST_LONG_BITS == 32 ) && ( TARGET_LONG_BITS == 64 ) <nl> -# define MAX_OP_PER_INSTR 128 <nl> -# else <nl> -# define MAX_OP_PER_INSTR 96 <nl> -# endif <nl> +# define MAX_OP_PER_INSTR 208 <nl>  <nl> # if HOST_LONG_BITS == 32 <nl> # define MAX_OPC_PARAM_PER_ARG 2
int main ( int argc , char ** argv ) <nl>  <nl> process_requests ( sock ); <nl> error : <nl> + g_free ( rpath ); <nl> + g_free ( sock_name ); <nl> do_log ( LOG_INFO , " Done \ n "); <nl> closelog (); <nl> return 0 ;
void * qxl_phys2virt ( PCIQXLDevice * qxl , QXLPHYSICAL pqxl , int group_id ) <nl> case MEMSLOT_GROUP_HOST : <nl> return ( void *) offset ; <nl> case MEMSLOT_GROUP_GUEST : <nl> - PANIC_ON ( slot > NUM_MEMSLOTS ); <nl> + PANIC_ON ( slot >= NUM_MEMSLOTS ); <nl> PANIC_ON (! qxl -> guest_slots [ slot ]. active ); <nl> PANIC_ON ( offset < qxl -> guest_slots [ slot ]. delta ); <nl> offset -= qxl -> guest_slots [ slot ]. delta ;
static CharDriverState * create_eventfd_chr_device ( IVShmemState * s , <nl> int vector ) <nl> { <nl> /* create a event character device based on the passed eventfd */ <nl> - PCIDevice * pdev = PCI_DEVICE ( s ); <nl> int eventfd = event_notifier_get_fd ( n ); <nl> CharDriverState * chr ; <nl>  <nl> - s -> msi_vectors [ vector ]. pdev = pdev ; <nl> - <nl> chr = qemu_chr_open_eventfd ( eventfd ); <nl>  <nl> if ( chr == NULL ) {
static void cleanup_unknown_header_ext ( BlockDriverState * bs ) <nl> } <nl> } <nl>  <nl> - static void report_unsupported ( BlockDriverState * bs , const char * fmt , ...) <nl> + static void GCC_FMT_ATTR ( 2 , 3 ) report_unsupported ( BlockDriverState * bs , <nl> + const char * fmt , ...) <nl> { <nl> char msg [ 64 ]; <nl> va_list ap ;
static uint64_t coroutine_fn mirror_iteration ( MirrorBlockJob * s ) <nl> nb_chunks * sectors_per_chunk ); <nl> bitmap_set ( s -> in_flight_bitmap , sector_num / sectors_per_chunk , nb_chunks ); <nl> while ( nb_chunks > 0 && sector_num < end ) { <nl> - int ret ; <nl> + int64_t ret ; <nl> int io_sectors , io_sectors_acct ; <nl> BlockDriverState * file ; <nl> enum MirrorMethod {
static int coroutine_fn bdrv_aligned_preadv ( BlockDriverState * bs , <nl> } <nl>  <nl> max_bytes = ROUND_UP ( MAX ( 0 , total_bytes - offset ), align ); <nl> - if ( bytes < max_bytes ) { <nl> + if ( bytes <= max_bytes ) { <nl> ret = bdrv_driver_preadv ( bs , offset , bytes , qiov , 0 ); <nl> } else if ( max_bytes > 0 ) { <nl> QEMUIOVector local_qiov ;
static uint64_t msix_pba_mmio_read ( void * opaque , hwaddr addr , <nl> return pci_get_long ( dev -> msix_pba + addr ); <nl> } <nl>  <nl> + static void msix_pba_mmio_write ( void * opaque , hwaddr addr , <nl> + uint64_t val , unsigned size ) <nl> +{ <nl> +} <nl> + <nl> static const MemoryRegionOps msix_pba_mmio_ops = { <nl> . read = msix_pba_mmio_read , <nl> + . write = msix_pba_mmio_write , <nl> . endianness = DEVICE_LITTLE_ENDIAN , <nl> . valid = { <nl> . min_access_size = 4 ,
static inline void fimd_swap_data ( unsigned int swap_ctl , uint64_t * data ) <nl> if ( swap_ctl & FIMD_WINCON_SWAP_BITS ) { <nl> res = 0 ; <nl> for ( i = 0 ; i < 64 ; i ++) { <nl> - if ( x & ( 1ULL << ( 64 - i ))) { <nl> + if ( x & ( 1ULL << ( 63 - i ))) { <nl> res |= ( 1ULL << i ); <nl> } <nl> }
void watchdog_perform_action ( void ) <nl> exit ( 0 ); <nl>  <nl> case WDT_PAUSE : /* same as ' stop ' command in monitor */ <nl> + /* In a timer callback , when vm_stop calls qemu_clock_enable <nl> + * you would get a deadlock . Bypass the problem . <nl> + */ <nl> + qemu_system_vmstop_request_prepare (); <nl> qapi_event_send_watchdog ( WATCHDOG_EXPIRATION_ACTION_PAUSE , & error_abort ); <nl> - vm_stop ( RUN_STATE_WATCHDOG ); <nl> + qemu_system_vmstop_request ( RUN_STATE_WATCHDOG ); <nl> break ; <nl>  <nl> case WDT_DEBUG :
static int scsi_disk_emulate_inquiry ( SCSIRequest * req , uint8_t * outbuf ) <nl> } <nl>  <nl> l = strlen ( s -> serial ); <nl> - if ( l > 20 ) { <nl> - l = 20 ; <nl> + if ( l > 36 ) { <nl> + l = 36 ; <nl> } <nl>  <nl> DPRINTF (" Inquiry EVPD [ Serial number ] "
static void qemu_rdma_cleanup ( RDMAContext * rdma ) <nl> rdma_destroy_event_channel ( rdma -> channel ); <nl> rdma -> channel = NULL ; <nl> } <nl> + g_free ( rdma -> host ); <nl> + rdma -> host = NULL ; <nl> } <nl>  <nl> 
static PixelFormat sdl_to_qemu_pixelformat ( SDL_PixelFormat * sdl_pf ) <nl> static DisplaySurface * sdl_create_displaysurface ( int width , int height ) <nl> { <nl> DisplaySurface * surface = ( DisplaySurface *) g_malloc0 ( sizeof ( DisplaySurface )); <nl> - if ( surface == NULL ) { <nl> - fprintf ( stderr , " sdl_create_displaysurface : malloc failed \ n "); <nl> - exit ( 1 ); <nl> - } <nl>  <nl> surface -> width = width ; <nl> surface -> height = height ;
static void tcg_out_op ( TCGContext * s , TCGOpcode opc , const TCGArg * args , <nl> break ; <nl>  <nl> case INDEX_op_ext32u_i64 : <nl> - tcg_out_rld ( s , RLDICR , args [ 0 ], args [ 1 ], 0 , 32 ); <nl> + tcg_out_rld ( s , RLDICL , args [ 0 ], args [ 1 ], 0 , 32 ); <nl> break ; <nl>  <nl> case INDEX_op_setcond_i32 :
void usb_packet_complete ( USBDevice * dev , USBPacket * p ) <nl> { <nl> /* Note : p -> owner != dev is possible in case dev is a hub */ <nl> assert ( p -> owner != NULL ); <nl> - dev -> port -> ops -> complete ( dev -> port , p ); <nl> p -> owner = NULL ; <nl> + dev -> port -> ops -> complete ( dev -> port , p ); <nl> } <nl>  <nl> /* Cancel an active packet . The packed must have been deferred by
static int vnc_display_listen_addr ( VncDisplay * vd , <nl> qio_channel_set_name ( QIO_CHANNEL ( sioc ), name ); <nl> if ( qio_channel_socket_listen_sync ( <nl> sioc , rawaddrs [ i ], listenerr == NULL ? & listenerr : NULL ) < 0 ) { <nl> + object_unref ( OBJECT ( sioc )); <nl> continue ; <nl> } <nl> listening = true ;
static void digic_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = digic_realize ; <nl> + /* Reason : Uses serial_hds in the realize function --> not usable twice */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo digic_type_info = {
BlockDriverAIOCB * dma_bdrv_io ( <nl> dbs -> sg_cur_index = 0 ; <nl> dbs -> sg_cur_byte = 0 ; <nl> dbs -> dir = dir ; <nl> + dbs -> in_cancel = false ; <nl> dbs -> io_func = io_func ; <nl> dbs -> bh = NULL ; <nl> qemu_iovec_init (& dbs -> iov , sg -> nsg );
static uint64_t pci_read ( void * opaque , hwaddr addr , unsigned int size ) <nl> uint32_t val = 0 ; <nl> int bsel = s -> hotplug_select ; <nl>  <nl> - if ( bsel < 0 || bsel > ACPI_PCIHP_MAX_HOTPLUG_BUS ) { <nl> + if ( bsel < 0 || bsel >= ACPI_PCIHP_MAX_HOTPLUG_BUS ) { <nl> return 0 ; <nl> } <nl> 
static inline void setcc ( S390CPU * cpu , uint64_t cc ) <nl>  <nl> env -> psw . mask &= ~( 3ull << 44 ); <nl> env -> psw . mask |= ( cc & 3 ) << 44 ; <nl> + env -> cc_op = cc ; <nl> } <nl>  <nl> typedef struct LowCore
static int spapr_vty_init ( VIOsPAPRDevice * sdev ) <nl> { <nl> VIOsPAPRVTYDevice * dev = ( VIOsPAPRVTYDevice *) sdev ; <nl>  <nl> + if (! dev -> chardev ) { <nl> + fprintf ( stderr , " spapr - vty : Can ' t create vty without a chardev !\ n "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> qemu_chr_add_handlers ( dev -> chardev , vty_can_receive , <nl> vty_receive , NULL , dev ); <nl> 
static void free_test_data ( test_data * data ) <nl> g_free ( temp -> asl_file ); <nl> } <nl>  <nl> - g_array_free ( data -> tables , false ); <nl> + g_array_free ( data -> tables , true ); <nl> } <nl>  <nl> static uint8_t acpi_checksum ( const uint8_t * data , int len )
static void hmp_migrate_status_cb ( void * opaque ) <nl> MigrationInfo * info ; <nl>  <nl> info = qmp_query_migrate ( NULL ); <nl> - if (! info -> has_status || strcmp ( info -> status , " active ") == 0 ) { <nl> + if (! info -> has_status || strcmp ( info -> status , " active ") == 0 || <nl> + strcmp ( info -> status , " setup ") == 0 ) { <nl> if ( info -> has_disk ) { <nl> int progress ; <nl> 
int coroutine_fn bdrv_is_allocated ( BlockDriverState * bs , int64_t sector_num , <nl> if ( ret < 0 ) { <nl> return ret ; <nl> } <nl> - return ( ret & BDRV_BLOCK_ALLOCATED ); <nl> + return !!( ret & BDRV_BLOCK_ALLOCATED ); <nl> } <nl>  <nl> /*
long do_rt_sigreturn ( CPUM68KState * env ) <nl> { <nl> struct target_rt_sigframe * frame ; <nl> abi_ulong frame_addr = env -> aregs [ 7 ] - 4 ; <nl> - target_sigset_t target_set ; <nl> sigset_t set ; <nl>  <nl> trace_user_do_rt_sigreturn ( env , frame_addr ); <nl> if (! lock_user_struct ( VERIFY_READ , frame , frame_addr , 1 )) <nl> goto badframe ; <nl>  <nl> - target_to_host_sigset_internal (& set , & target_set ); <nl> + target_to_host_sigset (& set , & frame -> uc . tuc_sigmask ); <nl> set_sigmask (& set ); <nl>  <nl> /* restore registers */
void target_set_brk ( abi_ulong new_brk ) <nl> abi_long do_brk ( abi_ulong new_brk ) <nl> { <nl> abi_long mapped_addr ; <nl> - int new_alloc_size ; <nl> + abi_ulong new_alloc_size ; <nl>  <nl> DEBUGF_BRK (" do_brk (" TARGET_ABI_FMT_lx ") -> ", new_brk ); <nl> 
static int vvfat_write ( BlockDriverState * bs , int64_t sector_num , <nl>  <nl> DLOG ( checkpoint ()); <nl>  <nl> + /* Check if we ' re operating in read - only mode */ <nl> + if ( s -> qcow == NULL ) { <nl> + return - EACCES ; <nl> + } <nl> + <nl> vvfat_close_current_file ( s ); <nl>  <nl> /*
long do_sigreturn ( CPUPPCState * env ) <nl> { <nl> struct target_sigcontext * sc = NULL ; <nl> struct target_mcontext * sr = NULL ; <nl> - target_ulong sr_addr , sc_addr ; <nl> + target_ulong sr_addr = 0 , sc_addr ; <nl> sigset_t blocked ; <nl> target_sigset_t set ; <nl> 
static void usb_ohci_init ( OHCIState * ohci , DeviceState * dev , <nl>  <nl> ohci -> as = as ; <nl>  <nl> + if ( num_ports > OHCI_MAX_PORTS ) { <nl> + error_setg ( errp , " OHCI num - ports =% d is too big ( limit is % d ports )", <nl> + num_ports , OHCI_MAX_PORTS ); <nl> + return ; <nl> + } <nl> + <nl> if ( usb_frame_time == 0 ) { <nl> # ifdef OHCI_TIME_WARP <nl> usb_frame_time = NANOSECONDS_PER_SECOND ;
static int read_cpuinfo ( const char * field , char * value , int len ) <nl> break ; <nl> } <nl> if (! strncmp ( line , field , field_len )) { <nl> - strncpy ( value , line , len ); <nl> + pstrcpy ( value , len , line ); <nl> ret = 0 ; <nl> break ; <nl> }
int qemu_create_pidfile ( const char * filename ) <nl> return - 1 ; <nl> } <nl>  <nl> - close ( fd ); <nl> + /* keep pidfile open & locked forever */ <nl> return 0 ; <nl> }
ssize_t ne2000_receive ( NetClientState * nc , const uint8_t * buf , size_t size_ ) <nl> if ( index <= s -> stop ) <nl> avail = s -> stop - index ; <nl> else <nl> - avail = 0 ; <nl> + break ; <nl> len = size ; <nl> if ( len > avail ) <nl> len = avail ;
static void spapr_rng_class_init ( ObjectClass * oc , void * data ) <nl> dc -> realize = spapr_rng_realize ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> props = spapr_rng_properties ; <nl> + dc -> hotpluggable = false ; <nl> } <nl>  <nl> static const TypeInfo spapr_rng_info = {
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> update_displaychangelistener (& d -> ssd . dcl , GUI_REFRESH_INTERVAL_DEFAULT ); <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> + qemu_spice_display_switch (& d -> ssd , d -> ssd . ds ); <nl> vga_dirty_log_start (& d -> vga ); <nl> graphic_hw_update ( d -> vga . con ); <nl> }
# endif <nl>  <nl> typedef struct IOHandlerRecord { <nl> - int fd ; <nl> IOCanReadHandler * fd_read_poll ; <nl> IOHandler * fd_read ; <nl> IOHandler * fd_write ; <nl> - int deleted ; <nl> void * opaque ; <nl> QLIST_ENTRY ( IOHandlerRecord ) next ; <nl> + int fd ; <nl> + bool deleted ; <nl> } IOHandlerRecord ; <nl>  <nl> static QLIST_HEAD (, IOHandlerRecord ) io_handlers =
static int aio_write_f ( BlockBackend * blk , int argc , char ** argv ) <nl> int64_t count = cvtnum ( argv [ optind ]); <nl> if ( count < 0 ) { <nl> print_cvtnum_err ( count , argv [ optind ]); <nl> + g_free ( ctx ); <nl> return 0 ; <nl> } <nl> 
static void win_stdio_close ( CharDriverState * chr ) <nl> } <nl>  <nl> g_free ( chr -> opaque ); <nl> - g_free ( chr ); <nl> } <nl>  <nl> static CharDriverState * qemu_chr_open_stdio ( const char * id ,
void microblaze_load_kernel ( MicroBlazeCPU * cpu , hwaddr ddr_base , <nl> big_endian , ELF_MACHINE , 0 ); <nl> } <nl> /* Always boot into physical ram . */ <nl> - boot_info . bootstrap_pc = ddr_base + ( entry & 0x0fffffff ); <nl> + boot_info . bootstrap_pc = ( uint32_t ) entry ; <nl>  <nl> /* If it wasn ' t an ELF image , try an u - boot image . */ <nl> if ( kernel_size < 0 ) {
static int ohci_bus_start ( OHCIState * ohci ) <nl> /* Stop sending SOF tokens on the bus */ <nl> static void ohci_bus_stop ( OHCIState * ohci ) <nl> { <nl> - if ( ohci -> eof_timer ) <nl> + if ( ohci -> eof_timer ) { <nl> timer_del ( ohci -> eof_timer ); <nl> + timer_free ( ohci -> eof_timer ); <nl> + } <nl> ohci -> eof_timer = NULL ; <nl> } <nl> 
static void setup_frame ( int sig , struct target_sigaction * ka , <nl>  <nl> long do_rt_sigreturn ( CPUARMState * env ) <nl> { <nl> - struct target_rt_sigframe * frame ; <nl> + struct target_rt_sigframe * frame = NULL ; <nl> abi_ulong frame_addr = env -> xregs [ 31 ]; <nl>  <nl> if ( frame_addr & 15 ) {
static bool ga_open_pidfile ( const char * pidfile ) <nl> int pidfd ; <nl> char pidstr [ 32 ]; <nl>  <nl> - pidfd = open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> + pidfd = qemu_open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> if ( pidfd == - 1 || lockf ( pidfd , F_TLOCK , 0 )) { <nl> g_critical (" Cannot lock pid file , % s ", strerror ( errno )); <nl> if ( pidfd != - 1 ) {
static void vhost_dev_unassign_memory ( struct vhost_dev * dev , <nl> if ( start_addr <= reg -> guest_phys_addr && memlast >= reglast ) { <nl> -- dev -> mem -> nregions ; <nl> -- to ; <nl> - assert ( to >= 0 ); <nl> ++ overlap_middle ; <nl> continue ; <nl> }
int qcow2_alloc_cluster_offset ( BlockDriverState * bs , uint64_t offset , <nl>  <nl> again : <nl> start = offset ; <nl> - remaining = * num << BDRV_SECTOR_BITS ; <nl> + remaining = ( uint64_t )* num << BDRV_SECTOR_BITS ; <nl> cluster_offset = 0 ; <nl> * host_offset = 0 ; <nl> cur_bytes = 0 ;
static void error_exit ( int err , const char * msg ) <nl> void qemu_mutex_init ( QemuMutex * mutex ) <nl> { <nl> int err ; <nl> - pthread_mutexattr_t mutexattr ; <nl>  <nl> - pthread_mutexattr_init (& mutexattr ); <nl> - pthread_mutexattr_settype (& mutexattr , PTHREAD_MUTEX_ERRORCHECK ); <nl> - err = pthread_mutex_init (& mutex -> lock , & mutexattr ); <nl> - pthread_mutexattr_destroy (& mutexattr ); <nl> + err = pthread_mutex_init (& mutex -> lock , NULL ); <nl> if ( err ) <nl> error_exit ( err , __func__ ); <nl> }
typedef struct SCLPConsole { <nl> /* Return number of bytes that fit into iov buffer */ <nl> static int chr_can_read ( void * opaque ) <nl> { <nl> - int can_read ; <nl> SCLPConsole * scon = opaque ; <nl>  <nl> - can_read = SIZE_BUFFER_VT220 - scon -> iov_data_len ; <nl> - <nl> - return can_read ; <nl> + return scon -> iov ? SIZE_BUFFER_VT220 - scon -> iov_data_len : 0 ; <nl> } <nl>  <nl> /* Receive n bytes from character layer , save in iov buffer ,
static void virtio_net_guest_notifier_mask ( VirtIODevice * vdev , int idx , <nl> void virtio_net_set_config_size ( VirtIONet * n , uint32_t host_features ) <nl> { <nl> int i , config_size = 0 ; <nl> + host_features |= ( 1 << VIRTIO_NET_F_MAC ); <nl> for ( i = 0 ; feature_sizes [ i ]. flags != 0 ; i ++) { <nl> if ( host_features & feature_sizes [ i ]. flags ) { <nl> config_size = MAX ( feature_sizes [ i ]. end , config_size );
static uint64_t pit_ioport_read ( void * opaque , hwaddr addr , <nl> PITChannelState * s ; <nl>  <nl> addr &= 3 ; <nl> + <nl> + if ( addr == 3 ) { <nl> + /* Mode / Command register is write only , read is ignored */ <nl> + return 0 ; <nl> + } <nl> + <nl> s = & pit -> channels [ addr ]; <nl> if ( s -> status_latched ) { <nl> s -> status_latched = 0 ;
TranslationBlock * tb_gen_code ( CPUState * cpu , <nl> /* flush must be done */ <nl> tb_flush ( cpu ); <nl> mmap_unlock (); <nl> + /* Make the execution loop process the flush as soon as possible . */ <nl> + cpu -> exception_index = EXCP_INTERRUPT ; <nl> cpu_loop_exit ( cpu ); <nl> } <nl> 
static void vhost_user_cleanup ( NetClientState * nc ) <nl> s -> vhost_net = NULL ; <nl> } <nl> if ( nc -> queue_index == 0 ) { <nl> + if ( s -> watch ) { <nl> + g_source_remove ( s -> watch ); <nl> + s -> watch = 0 ; <nl> + } <nl> qemu_chr_fe_deinit (& s -> chr , true ); <nl> } <nl> 
static void cg3_realizefn ( DeviceState * dev , Error ** errp ) <nl> if ( fcode_filename ) { <nl> ret = load_image_targphys ( fcode_filename , s -> prom_addr , <nl> FCODE_MAX_ROM_SIZE ); <nl> + g_free ( fcode_filename ); <nl> if ( ret < 0 || ret > FCODE_MAX_ROM_SIZE ) { <nl> error_report (" cg3 : could not load prom '% s '", CG3_ROM_FILE ); <nl> }
static int pfpu_decode_insn ( MilkymistPFPUState * s ) <nl> uint32_t reg_b = ( insn >> 11 ) & 0x7f ; <nl> uint32_t op = ( insn >> 7 ) & 0xf ; <nl> uint32_t reg_d = insn & 0x7f ; <nl> - uint32_t r ; <nl> + uint32_t r = 0 ; <nl> int latency = 0 ; <nl>  <nl> switch ( op ) {
static int ppce500_load_device_tree ( MachineState * machine , <nl> } <nl>  <nl> fdt = load_device_tree ( filename , & fdt_size ); <nl> + g_free ( filename ); <nl> if (! fdt ) { <nl> goto out ; <nl> }
int qemu_devtree_add_subnode ( void * fdt , const char * name ) <nl> int retval ; <nl>  <nl> if (! basename ) { <nl> + g_free ( dupname ); <nl> return - 1 ; <nl> } <nl> 
static int count_contiguous_clusters ( uint64_t nb_clusters , int cluster_size , <nl> uint64_t * l2_table , uint64_t stop_flags ) <nl> { <nl> int i ; <nl> - uint64_t mask = stop_flags | L2E_OFFSET_MASK | QCOW2_CLUSTER_COMPRESSED ; <nl> + uint64_t mask = stop_flags | L2E_OFFSET_MASK | QCOW_OFLAG_COMPRESSED ; <nl> uint64_t first_entry = be64_to_cpu ( l2_table [ 0 ]); <nl> uint64_t offset = first_entry & mask ; <nl> 
TPMVersion tpm_tis_get_tpm_version ( Object * obj ) <nl> { <nl> TPMState * s = TPM ( obj ); <nl>  <nl> + if ( tpm_backend_had_startup_error ( s -> be_driver )) { <nl> + return TPM_VERSION_UNSPEC ; <nl> + } <nl> + <nl> return tpm_backend_get_tpm_version ( s -> be_driver ); <nl> } <nl> 
restart : <nl> aio_context_release ( pool -> ctx ); <nl> elem -> common . cb ( elem -> common . opaque , elem -> ret ); <nl> aio_context_acquire ( pool -> ctx ); <nl> + <nl> + /* We can safely cancel the completion_bh here regardless of someone <nl> + * else having scheduled it meanwhile because we reenter the <nl> + * completion function anyway ( goto restart ). <nl> + */ <nl> + qemu_bh_cancel ( pool -> completion_bh ); <nl> + <nl> qemu_aio_unref ( elem ); <nl> goto restart ; <nl> } else {
static int bad_mode_switch ( CPUARMState * env , int mode ) <nl> return ! arm_feature ( env , ARM_FEATURE_EL2 ) <nl> || arm_current_el ( env ) < 2 || arm_is_secure ( env ); <nl> case ARM_CPU_MODE_MON : <nl> - return ! arm_is_secure ( env ); <nl> + return arm_current_el ( env ) < 3 ; <nl> default : <nl> return 1 ; <nl> }
static int vmxnet3_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> + vmxnet3_validate_interrupts ( s ); <nl> + <nl> return 0 ; <nl> } <nl> 
DeviceState * qdev_device_add ( QemuOpts * opts ) <nl> } <nl> } <nl>  <nl> - if (! oc ) { <nl> + if (! object_class_dynamic_cast ( oc , TYPE_DEVICE )) { <nl> qerror_report ( QERR_INVALID_PARAMETER_VALUE , " driver ", " device type "); <nl> return NULL ; <nl> }
static void do_pci_unregister_device ( PCIDevice * pci_dev ) <nl> pci_dev -> bus -> devices [ pci_dev -> devfn ] = NULL ; <nl> pci_config_free ( pci_dev ); <nl>  <nl> - memory_region_del_subregion (& pci_dev -> bus_master_container_region , <nl> - & pci_dev -> bus_master_enable_region ); <nl> + if ( memory_region_is_mapped (& pci_dev -> bus_master_enable_region )) { <nl> + memory_region_del_subregion (& pci_dev -> bus_master_container_region , <nl> + & pci_dev -> bus_master_enable_region ); <nl> + } <nl> address_space_destroy (& pci_dev -> bus_master_as ); <nl> } <nl> 
static void s390_init ( ram_addr_t ram_size , <nl>  <nl> bios_filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> bios_size = load_image ( bios_filename , qemu_get_ram_ptr ( ZIPL_LOAD_ADDR )); <nl> + qemu_free ( bios_filename ); <nl>  <nl> if (( long ) bios_size < 0 ) { <nl> hw_error (" could not load bootloader '% s '\ n ", bios_name );
static bool ga_open_pidfile ( const char * pidfile ) <nl> pidfd = open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> if ( pidfd == - 1 || lockf ( pidfd , F_TLOCK , 0 )) { <nl> g_critical (" Cannot lock pid file , % s ", strerror ( errno )); <nl> + if ( pidfd != - 1 ) { <nl> + close ( pidfd ); <nl> + } <nl> return false ; <nl> } <nl> 
static void qemu_chr_free_common ( CharDriverState * chr ) <nl> if ( chr -> logfd != - 1 ) { <nl> close ( chr -> logfd ); <nl> } <nl> + qemu_mutex_destroy (& chr -> chr_write_lock ); <nl> g_free ( chr ); <nl> } <nl> 
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl> /* The snapshot list position has not yet been updated , so these clusters <nl> * must indeed be completely free */ <nl> ret = qcow2_pre_write_overlap_check ( bs , QCOW2_OL_DEFAULT , offset , <nl> - s -> snapshots_size ); <nl> + snapshots_size ); <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
bool sysbus_has_irq ( SysBusDevice * dev , int n ) <nl> ObjectProperty * r ; <nl>  <nl> r = object_property_find ( OBJECT ( dev ), prop , NULL ); <nl> + g_free ( prop ); <nl> + <nl> return ( r != NULL ); <nl> } <nl> 
static int css_interpret_ccw ( SubchDev * sch , hwaddr ccw_addr , <nl> if (! ccw_addr ) { <nl> return - EIO ; <nl> } <nl> + /* Check doubleword aligned and 31 or 24 ( fmt 0 ) bit addressable . */ <nl> + if ( ccw_addr & ( sch -> ccw_fmt_1 ? 0x80000007 : 0xff000007 )) { <nl> + return - EINVAL ; <nl> + } <nl>  <nl> /* Translate everything to format - 1 ccws - the information is the same . */ <nl> ccw = copy_ccw_from_guest ( ccw_addr , sch -> ccw_fmt_1 );
static bool blit_is_unsafe ( struct CirrusVGAState * s ) <nl> assert ( s -> cirrus_blt_width > 0 ); <nl> assert ( s -> cirrus_blt_height > 0 ); <nl>  <nl> + if ( s -> cirrus_blt_width > CIRRUS_BLTBUFSIZE ) { <nl> + return true ; <nl> + } <nl> + <nl> if ( blit_region_is_unsafe ( s , s -> cirrus_blt_dstpitch , <nl> s -> cirrus_blt_dstaddr & s -> cirrus_addr_mask )) { <nl> return true ;
static void ipmi_init_sensors_from_sdrs ( IPMIBmcSim * s ) <nl> static int ipmi_register_netfn ( IPMIBmcSim * s , unsigned int netfn , <nl> const IPMINetfn * netfnd ) <nl> { <nl> - if (( netfn & 1 ) || ( netfn > MAX_NETFNS ) || ( s -> netfns [ netfn / 2 ])) { <nl> + if (( netfn & 1 ) || ( netfn >= MAX_NETFNS ) || ( s -> netfns [ netfn / 2 ])) { <nl> return - 1 ; <nl> } <nl> s -> netfns [ netfn / 2 ] = netfnd ;
int load_elf ( const char * filename , uint64_t (* translate_fn )( void *, uint64_t ), <nl> target_data_order = ELFDATA2LSB ; <nl> } <nl>  <nl> - if ( target_data_order != e_ident [ EI_DATA ]) <nl> - return - 1 ; <nl> + if ( target_data_order != e_ident [ EI_DATA ]) { <nl> + goto fail ; <nl> + } <nl>  <nl> lseek ( fd , 0 , SEEK_SET ); <nl> if ( e_ident [ EI_CLASS ] == ELFCLASS64 ) {
char * vnc_display_local_addr ( const char * id ) <nl> { <nl> VncDisplay * vs = vnc_display_find ( id ); <nl>  <nl> + assert ( vs ); <nl> return vnc_socket_local_addr ("% s :% s ", vs -> lsock ); <nl> } <nl> 
static QemuOptsList qemu_vnc_opts = { <nl> },{ <nl> . name = " connections ", <nl> . type = QEMU_OPT_NUMBER , <nl> + },{ <nl> + . name = " to ", <nl> + . type = QEMU_OPT_NUMBER , <nl> + },{ <nl> + . name = " ipv4 ", <nl> + . type = QEMU_OPT_BOOL , <nl> + },{ <nl> + . name = " ipv6 ", <nl> + . type = QEMU_OPT_BOOL , <nl> },{ <nl> . name = " password ", <nl> . type = QEMU_OPT_BOOL ,
hwaddr s390_cpu_get_phys_page_debug ( CPUState * cs , vaddr vaddr ) <nl> vaddr &= 0x7fffffff ; <nl> } <nl>  <nl> - mmu_translate ( env , vaddr , MMU_INST_FETCH , asc , & raddr , & prot , false ); <nl> - <nl> + if ( mmu_translate ( env , vaddr , MMU_INST_FETCH , asc , & raddr , & prot , false )) { <nl> + return - 1 ; <nl> + } <nl> return raddr ; <nl> } <nl> 
static void vnc_display_close ( VncDisplay * vs ) <nl> vs -> subauth = VNC_AUTH_INVALID ; <nl> if ( vs -> tlscreds ) { <nl> object_unparent ( OBJECT ( vs -> tlscreds )); <nl> + vs -> tlscreds = NULL ; <nl> } <nl> g_free ( vs -> tlsaclname ); <nl> vs -> tlsaclname = NULL ;
static void esp_pci_dma_memory_rw ( PCIESPState * pci , uint8_t * buf , int len , <nl> /* update status registers */ <nl> pci -> dma_regs [ DMA_WBC ] -= len ; <nl> pci -> dma_regs [ DMA_WAC ] += len ; <nl> - if ( pci -> dma_regs [ DMA_WBC ] == 0 ) <nl> + if ( pci -> dma_regs [ DMA_WBC ] == 0 ) { <nl> pci -> dma_regs [ DMA_STAT ] |= DMA_STAT_DONE ; <nl> + } <nl> } <nl>  <nl> static void esp_pci_dma_memory_read ( void * opaque , uint8_t * buf , int len )
void nand_setio ( DeviceState * dev , uint32_t value ) <nl>  <nl> if ( s -> ale ) { <nl> unsigned int shift = s -> addrlen * 8 ; <nl> - unsigned int mask = ~( 0xff << shift ); <nl> - unsigned int v = value << shift ; <nl> + uint64_t mask = ~( 0xffull << shift ); <nl> + uint64_t v = ( uint64_t ) value << shift ; <nl>  <nl> s -> addr = ( s -> addr & mask ) | v ; <nl> s -> addrlen ++;
static void tap_cleanup ( VLANClientState * nc ) <nl>  <nl> if ( s -> vhost_net ) { <nl> vhost_net_cleanup ( s -> vhost_net ); <nl> + s -> vhost_net = NULL ; <nl> } <nl>  <nl> qemu_purge_queued_packets ( nc );
static void disas_s390_insn ( DisasContext * s ) <nl> store_reg32 ( r1 , tmp32_1 ); <nl> tcg_gen_trunc_i64_i32 ( tmp32_2 , tmp2 ); <nl> store_reg32 ( r1 + 1 , tmp32_2 ); <nl> + tcg_temp_free_i64 ( tmp ); <nl> + tcg_temp_free_i64 ( tmp2 ); <nl> break ; <nl> case 0x98 : /* LM R1 , R3 , D2 ( B2 ) [ RS ] */ <nl> case 0x90 : /* STM R1 , R3 , D2 ( B2 ) [ RS ] */
static void entropy_available ( void * opaque ) <nl> ssize_t len ; <nl>  <nl> len = read ( s -> fd , buffer , s -> size ); <nl> + if ( len < 0 && errno == EAGAIN ) { <nl> + return ; <nl> + } <nl> g_assert ( len != - 1 ); <nl>  <nl> s -> receive_func ( s -> opaque , buffer , len );
int main ( int argc , char ** argv ) <nl>  <nl> path = g_strdup_printf (" eepro100 /% s ", models [ i ]); <nl> qtest_add_data_func ( path , models [ i ], test_device ); <nl> + g_free ( path ); <nl> } <nl>  <nl> return g_test_run ();
# include " exec / cpu - defs . h " <nl> # define TARGET_PAGE_BITS 12 <nl>  <nl> -# define TARGET_PHYS_ADDR_SPACE_BITS 64 <nl> +/* Actually 64 - bits , limited by the memory API to 62 bits . We <nl> + * never use that much . <nl> + */ <nl> +# define TARGET_PHYS_ADDR_SPACE_BITS 62 <nl> # define TARGET_VIRT_ADDR_SPACE_BITS 64 <nl>  <nl> # include " exec / cpu - all . h "
static int qemu_rdma_broken_ipv6_kernel ( Error ** errp , struct ibv_context * verbs ) <nl>  <nl> for ( x = 0 ; x < num_devices ; x ++) { <nl> verbs = ibv_open_device ( dev_list [ x ]); <nl> + if (! verbs ) { <nl> + if ( errno == EPERM ) { <nl> + continue ; <nl> + } else { <nl> + return - EINVAL ; <nl> + } <nl> + } <nl>  <nl> if ( ibv_query_port ( verbs , 1 , & port_attr )) { <nl> ibv_close_device ( verbs );
static always_inline void gen_store_gpr64 ( int reg , TCGv t ) { <nl> tcg_gen_mov_i64 ( cpu_gpr [ reg ], t ); <nl> # else <nl> tcg_gen_trunc_i64_i32 ( cpu_gpr [ reg ], t ); <nl> - TCGv tmp = tcg_temp_local_new ( TCG_TYPE_I64 ); <nl> + TCGv tmp = tcg_temp_new ( TCG_TYPE_I64 ); <nl> tcg_gen_shri_i64 ( tmp , t , 32 ); <nl> tcg_gen_trunc_i64_i32 ( cpu_gprh [ reg ], tmp ); <nl> tcg_temp_free ( tmp );
static int enable_write_target ( BDRVVVFATState * s , Error ** errp ) <nl> set_option_parameter ( options , BLOCK_OPT_BACKING_FILE , " fat :"); <nl>  <nl> ret = bdrv_create ( bdrv_qcow , s -> qcow_filename , options , errp ); <nl> + free_option_parameters ( options ); <nl> if ( ret < 0 ) { <nl> goto err ; <nl> }
size_t ram_control_save_page ( QEMUFile * f , ram_addr_t block_offset , <nl> offset , size , bytes_sent ); <nl>  <nl> if ( ret != RAM_SAVE_CONTROL_DELAYED ) { <nl> - if (* bytes_sent > 0 ) { <nl> + if ( bytes_sent && * bytes_sent > 0 ) { <nl> qemu_update_position ( f , * bytes_sent ); <nl> } else if ( ret < 0 ) { <nl> qemu_file_set_error ( f , ret );
static void vhost_scsi_unrealize ( DeviceState * dev , Error ** errp ) <nl> /* This will stop vhost backend . */ <nl> vhost_scsi_set_status ( vdev , 0 ); <nl>  <nl> + vhost_dev_cleanup (& s -> dev ); <nl> g_free ( s -> dev . vqs ); <nl>  <nl> virtio_scsi_common_unrealize ( dev , errp );
static inline void bitmap_directory_to_be ( uint8_t * dir , size_t size ) <nl>  <nl> static void bitmap_free ( Qcow2Bitmap * bm ) <nl> { <nl> + if ( bm == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> g_free ( bm -> name ); <nl> g_free ( bm ); <nl> }
static int xen_platform_initfn ( PCIDevice * dev ) <nl> PCIXenPlatformState * d = XEN_PLATFORM ( dev ); <nl> uint8_t * pci_conf ; <nl>  <nl> + /* Device will crash on reset if xen is not initialized */ <nl> + assert ( xen_enabled ()); <nl> + <nl> pci_conf = dev -> config ; <nl>  <nl> pci_set_word ( pci_conf + PCI_COMMAND , PCI_COMMAND_IO | PCI_COMMAND_MEMORY );
void qemu_spice_create_host_primary ( SimpleSpiceDisplay * ssd ) <nl> { <nl> QXLDevSurfaceCreate surface ; <nl>  <nl> + memset (& surface , 0 , sizeof ( surface )); <nl> + <nl> dprint ( 1 , "% s : % dx % d \ n ", __FUNCTION__ , <nl> ds_get_width ( ssd -> ds ), ds_get_height ( ssd -> ds )); <nl> 
static int parse_drive ( DeviceState * dev , Property * prop , const char * str ) <nl> static int print_drive ( DeviceState * dev , Property * prop , char * dest , size_t len ) <nl> { <nl> DriveInfo ** ptr = qdev_get_prop_ptr ( dev , prop ); <nl> - return snprintf ( dest , len , "% s ", (* ptr )-> id ); <nl> + return snprintf ( dest , len , "% s ", (* ptr ) ? (* ptr )-> id : "< null >"); <nl> } <nl>  <nl> PropertyInfo qdev_prop_drive = {
static void spapr_tce_reset ( DeviceState * dev ) <nl> sPAPRTCETable * tcet = SPAPR_TCE_TABLE ( dev ); <nl> size_t table_size = tcet -> nb_table * sizeof ( uint64_t ); <nl>  <nl> - memset ( tcet -> table , 0 , table_size ); <nl> + if ( tcet -> nb_table ) { <nl> + memset ( tcet -> table , 0 , table_size ); <nl> + } <nl> } <nl>  <nl> static target_ulong put_tce_emu ( sPAPRTCETable * tcet , target_ulong ioba ,
CharDriverState * qemu_chr_open ( const char * label , const char * filename , void (* i <nl> if ( chr && qemu_opt_get_bool ( opts , " mux ", 0 )) { <nl> monitor_init ( chr , MONITOR_USE_READLINE ); <nl> } <nl> + qemu_opts_del ( opts ); <nl> return chr ; <nl> } <nl> 
static int64_t seek_to_sector ( BlockDriverState * bs , int64_t sector_num ) <nl> offset = sector_num % s -> tracks ; <nl>  <nl> /* not allocated */ <nl> - if (( index > s -> catalog_size ) || ( s -> catalog_bitmap [ index ] == 0 )) <nl> + if (( index >= s -> catalog_size ) || ( s -> catalog_bitmap [ index ] == 0 )) <nl> return - 1 ; <nl> return <nl> (( uint64_t ) s -> catalog_bitmap [ index ] * s -> off_multiplier + offset ) * 512 ;
glue ( glue ( cirrus_bitblt_rop_bkwd_transp_ , ROP_NAME ), _16 )( CirrusVGAState * s , <nl> srcpitch += bltwidth ; <nl> for ( y = 0 ; y < bltheight ; y ++) { <nl> for ( x = 0 ; x < bltwidth ; x += 2 ) { <nl> - ROP_OP_TR_16 ( s , dstaddr , cirrus_src16 ( s , srcaddr ), transp ); <nl> + ROP_OP_TR_16 ( s , dstaddr - 1 , cirrus_src16 ( s , srcaddr - 1 ), transp ); <nl> dstaddr -= 2 ; <nl> srcaddr -= 2 ; <nl> }
static int32_t scsi_send_command ( SCSIDevice * d , uint32_t tag , <nl> uint8_t * cmd , int lun ) <nl> { <nl> SCSIDeviceState * s = d -> state ; <nl> - uint32_t len ; <nl> - int cmdlen ; <nl> + uint32_t len = 0 ; <nl> + int cmdlen = 0 ; <nl> SCSIRequest * r ; <nl> int ret ; <nl> 
static int scsi_disk_initfn ( SCSIDevice * dev ) <nl> } <nl> s -> bs = s -> qdev . conf . dinfo -> bdrv ; <nl>  <nl> + if ( bdrv_is_sg ( s -> bs )) { <nl> + qemu_error (" scsi - disk : unwanted / dev / sg *\ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> if ( bdrv_get_type_hint ( s -> bs ) == BDRV_TYPE_CDROM ) { <nl> s -> cluster_size = 4 ; <nl> } else {
static VncServerInfo * vnc_server_info_get ( VncDisplay * vd ) <nl> VncServerInfo * info ; <nl> Error * err = NULL ; <nl>  <nl> - info = g_malloc ( sizeof (* info )); <nl> + info = g_malloc0 ( sizeof (* info )); <nl> vnc_init_basic_info_from_server_addr ( vd -> lsock , <nl> qapi_VncServerInfo_base ( info ), & err ); <nl> info -> has_auth = true ;
static void test_smram_lock ( void ) <nl> g_assert ( smram_test_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN ) == false ); <nl> smram_set_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN , true ); <nl> g_assert ( smram_test_bit ( pcidev , MCH_HOST_BRIDGE_SMRAM_D_OPEN ) == true ); <nl> + <nl> + g_free ( pcidev ); <nl> + qpci_free_pc ( pcibus ); <nl> } <nl>  <nl> int main ( int argc , char ** argv )
static void tci_out_label ( TCGContext * s , TCGArg arg ) <nl> assert ( label -> u . value ); <nl> } else { <nl> tcg_out_reloc ( s , s -> code_ptr , sizeof ( tcg_target_ulong ), arg , 0 ); <nl> - tcg_out_i ( s , 0 ); <nl> + s -> code_ptr += sizeof ( tcg_target_ulong ); <nl> } <nl> } <nl> 
static void ccw_machine_class_init ( ObjectClass * oc , void * data ) <nl> mc -> no_parallel = 1 ; <nl> mc -> no_sdcard = 1 ; <nl> mc -> use_sclp = 1 ; <nl> - mc -> max_cpus = 255 ; <nl> + mc -> max_cpus = 248 ; <nl> mc -> get_hotplug_handler = s390_get_hotplug_handler ; <nl> hc -> plug = s390_machine_device_plug ; <nl> nc -> nmi_monitor_handler = s390_nmi ;
static int slirp_smb ( SlirpState * s , const char * exported_dir , <nl> fprintf ( f , <nl> "[ global ]\ n " <nl> " private dir =% s \ n " <nl> - " smb ports = 0 \ n " <nl> " socket address = 127 . 0 . 0 . 1 \ n " <nl> " pid directory =% s \ n " <nl> " lock directory =% s \ n "
retry : <nl> goto retry ; <nl> } <nl> } <nl> + <nl> + /* Make sure that all offsets in the " allocated " range are representable <nl> + * in an int64_t */ <nl> + if ( s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) { <nl> + return - EFBIG ; <nl> + } <nl> + <nl> # ifdef DEBUG_ALLOC2 <nl> fprintf ( stderr , " alloc_clusters : size =%" PRId64 " -> %" PRId64 "\ n ", <nl> size ,
SCSIRequest * scsi_req_new ( SCSIDevice * d , uint32_t tag , uint32_t lun , <nl> } else { <nl> trace_scsi_req_parsed ( d -> id , lun , tag , buf [ 0 ], <nl> cmd . mode , cmd . xfer ); <nl> - if ( req -> cmd . lba != - 1 ) { <nl> + if ( cmd . lba != - 1 ) { <nl> trace_scsi_req_parsed_lba ( d -> id , lun , tag , buf [ 0 ], <nl> cmd . lba ); <nl> }
void hid_reset ( HIDState * hs ) <nl> memset ( hs -> kbd . keycodes , 0 , sizeof ( hs -> kbd . keycodes )); <nl> memset ( hs -> kbd . key , 0 , sizeof ( hs -> kbd . key )); <nl> hs -> kbd . keys = 0 ; <nl> + hs -> kbd . modifiers = 0 ; <nl> break ; <nl> case HID_MOUSE : <nl> case HID_TABLET :
static void omap2_gpio_module_write ( void * opaque , target_phys_addr_t addr , <nl>  <nl> static uint32_t omap2_gpio_module_readp ( void * opaque , target_phys_addr_t addr ) <nl> { <nl> - return omap2_gpio_module_readp ( opaque , addr ) >> (( addr & 3 ) << 3 ); <nl> + return omap2_gpio_module_read ( opaque , addr & ~ 3 ) >> (( addr & 3 ) << 3 ); <nl> } <nl>  <nl> static void omap2_gpio_module_writep ( void * opaque , target_phys_addr_t addr ,
void qbus_free ( BusState * bus ) <nl> QLIST_REMOVE ( bus , sibling ); <nl> bus -> parent -> num_child_bus --; <nl> } <nl> + qemu_free (( void *) bus -> name ); <nl> if ( bus -> qdev_allocated ) { <nl> qemu_free ( bus ); <nl> }
static void do_test_equality ( bool expected , int _ , ...) <nl> g_assert ( qobject_is_equal ( args [ i ], args [ j ]) == expected ); <nl> } <nl> } <nl> + <nl> + g_free ( args ); <nl> } <nl>  <nl> # define check_equal (...) \
static int rtl8139_can_receive ( VLANClientState * nc ) <nl> } else { <nl> avail = MOD2 ( s -> RxBufferSize + s -> RxBufPtr - s -> RxBufAddr , <nl> s -> RxBufferSize ); <nl> - return ( avail == 0 || avail >= 1514 ); <nl> + return ( avail == 0 || avail >= 1514 || ( s -> IntrMask & RxOverflow )); <nl> } <nl> } <nl> 
static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail ; <nl> } <nl>  <nl> - s -> pagetable = qemu_blockalign ( bs , s -> max_table_entries * 4 ); <nl> + s -> pagetable = qemu_try_blockalign ( bs -> file , s -> max_table_entries * 4 ); <nl> + if ( s -> pagetable == NULL ) { <nl> + ret = - ENOMEM ; <nl> + goto fail ; <nl> + } <nl>  <nl> s -> bat_offset = be64_to_cpu ( dyndisk_header -> table_offset ); <nl> 
static target_ulong put_tce_emu ( sPAPRTCETable * tcet , target_ulong ioba , <nl> sPAPRTCE * tcep ; <nl>  <nl> if ( ioba >= tcet -> window_size ) { <nl> - hcall_dprintf (" spapr_vio_put_tce on out - of - boards IOBA 0x " <nl> + hcall_dprintf (" spapr_vio_put_tce on out - of - bounds IOBA 0x " <nl> TARGET_FMT_lx "\ n ", ioba ); <nl> return H_PARAMETER ; <nl> }
static void pci_qdev_unrealize ( DeviceState * dev , Error ** errp ) <nl> pc -> exit ( pci_dev ); <nl> } <nl>  <nl> + pci_device_deassert_intx ( pci_dev ); <nl> do_pci_unregister_device ( pci_dev ); <nl> } <nl> 
static void v9fs_post_lcreate ( V9fsState * s , V9fsLcreateState * vs , int err ) <nl> err = vs -> offset ; <nl> } else { <nl> vs -> fidp -> fid_type = P9_FID_NONE ; <nl> - close ( vs -> fidp -> fs . fd ); <nl> err = - errno ; <nl> + if ( vs -> fidp -> fs . fd > 0 ) { <nl> + close ( vs -> fidp -> fs . fd ); <nl> + } <nl> } <nl>  <nl> complete_pdu ( s , vs -> pdu , err );
static int qcrypto_ivgen_essiv_init ( QCryptoIVGen * ivgen , <nl> & salt , & nhash , <nl> errp ) < 0 ) { <nl> g_free ( essiv ); <nl> + g_free ( salt ); <nl> return - 1 ; <nl> } <nl> 
static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> ( s -> incompatible_features & QCOW2_INCOMPAT_DIRTY )) { <nl> BdrvCheckResult result = { 0 }; <nl>  <nl> - ret = qcow2_check ( bs , & result , BDRV_FIX_ERRORS ); <nl> + ret = qcow2_check ( bs , & result , BDRV_FIX_ERRORS | BDRV_FIX_LEAKS ); <nl> if ( ret < 0 ) { <nl> error_setg_errno ( errp , - ret , " Could not repair dirty image "); <nl> goto fail ;
QGuestAllocator * pc_alloc_init ( void ) <nl> /* Respect PCI hole */ <nl> s -> end = MIN ( ram_size , 0xE0000000 ); <nl>  <nl> + /* clean - up */ <nl> + g_free ( fw_cfg ); <nl> + <nl> return & s -> alloc ; <nl> }
static QObject * qmp_output_pop ( QmpOutputVisitor * qov ) <nl> static QObject * qmp_output_first ( QmpOutputVisitor * qov ) <nl> { <nl> QStackEntry * e = QTAILQ_LAST (& qov -> stack , QStack ); <nl> + <nl> + /* FIXME - find a better way to deal with NULL values */ <nl> + if (! e ) { <nl> + return NULL ; <nl> + } <nl> + <nl> return e -> value ; <nl> } <nl> 
int unix_connect_opts ( QemuOpts * opts ) <nl> snprintf ( un . sun_path , sizeof ( un . sun_path ), "% s ", path ); <nl> if ( connect ( sock , ( struct sockaddr *) & un , sizeof ( un )) < 0 ) { <nl> fprintf ( stderr , " connect ( unix :% s ): % s \ n ", path , strerror ( errno )); <nl> + close ( sock ); <nl> return - 1 ; <nl> } <nl> 
static ssize_t mp_dacl_listxattr ( FsContext * ctx , const char * path , <nl> } <nl>  <nl> /* len includes the trailing NUL */ <nl> - memcpy ( value , ACL_ACCESS , len ); <nl> + memcpy ( value , ACL_DEFAULT , len ); <nl> return 0 ; <nl> } <nl> 
static void handle_rev16 ( DisasContext * s , unsigned int sf , <nl> tcg_gen_shli_i64 ( tcg_rd , tcg_rd , 8 ); <nl> tcg_gen_or_i64 ( tcg_rd , tcg_rd , tcg_tmp ); <nl>  <nl> + tcg_temp_free_i64 ( mask ); <nl> tcg_temp_free_i64 ( tcg_tmp ); <nl> } <nl> 
static int img_bench ( int argc , char ** argv ) <nl> BlockBackend * blk = NULL ; <nl> BenchData data = {}; <nl> int flags = 0 ; <nl> - bool writethrough ; <nl> + bool writethrough = false ; <nl> struct timeval t1 , t2 ; <nl> int i ; <nl> 
static int ide_drive_pio_post_load ( void * opaque , int version_id ) <nl> { <nl> IDEState * s = opaque ; <nl>  <nl> - if ( s -> end_transfer_fn_idx > ARRAY_SIZE ( transfer_end_table )) { <nl> + if ( s -> end_transfer_fn_idx >= ARRAY_SIZE ( transfer_end_table )) { <nl> return - EINVAL ; <nl> } <nl> s -> end_transfer_func = transfer_end_table [ s -> end_transfer_fn_idx ];
static ssize_t rtl8139_do_receive ( NetClientState * nc , const uint8_t * buf , size_t <nl> s -> IntrStatus |= RxOverflow ; <nl> ++ s -> RxMissed ; <nl> rtl8139_update_irq ( s ); <nl> - return size_ ; <nl> + return 0 ; <nl> } <nl>  <nl> packet_header |= RxStatusOK ;
static void gen_rot_rm_im ( DisasContext * s , int ot , int op1 , int op2 , <nl> if ( is_right ) { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask - 1 ); <nl> tcg_gen_shri_tl ( cpu_cc_dst , cpu_T [ 0 ], mask ); <nl> + tcg_gen_andi_tl ( cpu_cc_dst , cpu_cc_dst , 1 ); <nl> } else { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask ); <nl> tcg_gen_andi_tl ( cpu_cc_dst , cpu_T [ 0 ], 1 );
pixman_format_code_t qemu_default_pixman_format ( int bpp , bool native_endian ) <nl> break ; <nl> } <nl> } <nl> - g_assert_not_reached (); <nl> + return 0 ; <nl> } <nl>  <nl> int qemu_pixman_get_type ( int rshift , int gshift , int bshift )
fork_exec ( struct socket * so , const char * ex , int do_pty ) <nl> bind ( s , ( struct sockaddr *)& addr , addrlen ) < 0 || <nl> listen ( s , 1 ) < 0 ) { <nl> error_report (" Error : inet socket : % s ", strerror ( errno )); <nl> - closesocket ( s ); <nl> + if ( s >= 0 ) { <nl> + closesocket ( s ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static char * SocketAddress_to_str ( const char * prefix , SocketAddress * addr , <nl> return g_strdup_printf ("% sfd :% s % s ", prefix , addr -> u . fd . data -> str , <nl> is_listen ? ", server " : ""); <nl> break ; <nl> + case SOCKET_ADDRESS_KIND_VSOCK : <nl> + return g_strdup_printf ("% svsock :% s :% s ", prefix , <nl> + addr -> u . vsock . data -> cid , <nl> + addr -> u . vsock . data -> port ); <nl> default : <nl> abort (); <nl> }
# define TT_DPROT 0x6c <nl> # define TT_SPILL 0x80 <nl> # define TT_FILL 0xc0 <nl> -# define TT_WOTHER 0x10 <nl> +# define TT_WOTHER ( 1 << 5 ) <nl> # define TT_TRAP 0x100 <nl> # endif <nl> 
void HELPER ( mtspr )( CPUOpenRISCState * env , target_ulong spr , target_ulong rb ) <nl> } <nl> break ; <nl> case TO_SPR ( 9 , 0 ): /* PICMR */ <nl> - env -> picmr |= rb ; <nl> + env -> picmr = rb ; <nl> break ; <nl> case TO_SPR ( 9 , 2 ): /* PICSR */ <nl> env -> picsr &= ~ rb ;
int mips_cpu_gdb_write_register ( CPUState * cs , uint8_t * mem_buf , int n ) <nl> return sizeof ( target_ulong ); <nl> } <nl> if ( env -> CP0_Config1 & ( 1 << CP0C1_FP ) <nl> - && n >= 38 && n < 73 ) { <nl> + && n >= 38 && n < 72 ) { <nl> if ( n < 70 ) { <nl> if ( env -> CP0_Status & ( 1 << CP0St_FR )) { <nl> env -> active_fpu . fpr [ n - 38 ]. d = tmp ;
int qemu_acl_insert ( qemu_acl * acl , <nl>  <nl> if ( index <= 0 ) <nl> return - 1 ; <nl> - if ( index >= acl -> nentries ) <nl> + if ( index > acl -> nentries ) { <nl> return qemu_acl_append ( acl , deny , match ); <nl> - <nl> + } <nl>  <nl> entry = g_malloc ( sizeof (* entry )); <nl> entry -> match = g_strdup ( match );
vmxnet3_indicate_packet ( VMXNET3State * s ) <nl> struct Vmxnet3_RxDesc rxd ; <nl> bool is_head = true ; <nl> uint32_t rxd_idx ; <nl> - uint32_t rx_ridx ; <nl> + uint32_t rx_ridx = 0 ; <nl>  <nl> struct Vmxnet3_RxCompDesc rxcd ; <nl> uint32_t new_rxcd_gen = VMXNET3_INIT_GEN ;
void qpci_device_foreach ( QPCIBus * bus , int vendor_id , int device_id , <nl>  <nl> if ( vendor_id != - 1 && <nl> qpci_config_readw ( dev , PCI_VENDOR_ID ) != vendor_id ) { <nl> + g_free ( dev ); <nl> continue ; <nl> } <nl>  <nl> if ( device_id != - 1 && <nl> qpci_config_readw ( dev , PCI_DEVICE_ID ) != device_id ) { <nl> + g_free ( dev ); <nl> continue ; <nl> } <nl> 
fail : <nl> QDECREF ( bs -> options ); <nl> QDECREF ( options ); <nl> bs -> options = NULL ; <nl> + bs -> explicit_options = NULL ; <nl> bdrv_unref ( bs ); <nl> error_propagate ( errp , local_err ); <nl> return NULL ; <nl> static void bdrv_close ( BlockDriverState * bs ) <nl> QDECREF ( bs -> options ); <nl> QDECREF ( bs -> explicit_options ); <nl> bs -> options = NULL ; <nl> + bs -> explicit_options = NULL ; <nl> QDECREF ( bs -> full_open_options ); <nl> bs -> full_open_options = NULL ; <nl> }
PCIDevice * pci_nic_init_nofail ( NICInfo * nd , PCIBus * rootbus , <nl>  <nl> res = pci_nic_init ( nd , rootbus , default_model , default_devaddr , & err ); <nl> if (! res ) { <nl> - error_report_err ( err ); <nl> + if ( err ) { <nl> + error_report_err ( err ); <nl> + } <nl> exit ( 1 ); <nl> } <nl> return res ;
static void pprint_data ( V9fsPDU * pdu , int rx , size_t * offsetp , const char * name ) <nl>  <nl> if ( rx ) { <nl> count = pdu -> elem . in_num ; <nl> - } else <nl> + } else { <nl> count = pdu -> elem . out_num ; <nl> } <nl> 
static int megasas_ctrl_get_info ( MegasasState * s , MegasasCmd * cmd ) <nl> BusChild * kid ; <nl> int num_pd_disks = 0 ; <nl>  <nl> - memset (& info , 0x0 , cmd -> iov_size ); <nl> + memset (& info , 0x0 , dcmd_size ); <nl> if ( cmd -> iov_size < dcmd_size ) { <nl> trace_megasas_dcmd_invalid_xfer_len ( cmd -> index , cmd -> iov_size , <nl> dcmd_size );
void ppc_tb_set_jmp_target ( unsigned long jmp_addr , unsigned long addr ); <nl> static inline void tb_set_jmp_target1 ( uintptr_t jmp_addr , uintptr_t addr ) <nl> { <nl> /* patch the branch destination */ <nl> - *( uint32_t *) jmp_addr = addr - ( jmp_addr + 4 ); <nl> + stl_p (( void *) jmp_addr , addr - ( jmp_addr + 4 )); <nl> /* no need to flush icache explicitly */ <nl> } <nl> # elif defined ( __aarch64__ )
static int mmu_translate_region ( CPUS390XState * env , target_ulong vaddr , <nl> __func__ , origin , offs , new_entry ); <nl>  <nl> if (( new_entry & _REGION_ENTRY_INV ) != 0 ) { <nl> - /* XXX different regions have different faults */ <nl> DPRINTF ("% s : invalid region \ n ", __func__ ); <nl> - trigger_page_fault ( env , vaddr , PGM_SEGMENT_TRANS , asc , rw , exc ); <nl> + trigger_page_fault ( env , vaddr , pchks [ level / 4 ], asc , rw , exc ); <nl> return - 1 ; <nl> } <nl> 
static int get_S2prot ( CPUARMState * env , int s2ap , int xn ) <nl> prot |= PAGE_WRITE ; <nl> } <nl> if (! xn ) { <nl> - prot |= PAGE_EXEC ; <nl> + if ( arm_el_is_aa64 ( env , 2 ) || prot & PAGE_READ ) { <nl> + prot |= PAGE_EXEC ; <nl> + } <nl> } <nl> return prot ; <nl> }
static int usbnet_can_receive ( NetClientState * nc ) <nl> { <nl> USBNetState * s = qemu_get_nic_opaque ( nc ); <nl>  <nl> + if (! s -> dev . config ) { <nl> + return 0 ; <nl> + } <nl> + <nl> if ( is_rndis ( s ) && s -> rndis_state != RNDIS_DATA_INITIALIZED ) { <nl> return 1 ; <nl> }
void qmp_blockdev_change_medium ( const char * device , const char * filename , <nl> } <nl>  <nl> bdrv_flags = blk_get_open_flags_from_root_state ( blk ); <nl> + bdrv_flags &= ~( BDRV_O_TEMPORARY | BDRV_O_SNAPSHOT | BDRV_O_NO_BACKING | <nl> + BDRV_O_PROTOCOL ); <nl>  <nl> if (! has_read_only ) { <nl> read_only = BLOCKDEV_CHANGE_READ_ONLY_MODE_RETAIN ;
void vhost_dev_cleanup ( struct vhost_dev * hdev ) <nl> g_free ( hdev -> mem ); <nl> g_free ( hdev -> mem_sections ); <nl> hdev -> vhost_ops -> vhost_backend_cleanup ( hdev ); <nl> + assert (! hdev -> log ); <nl> QLIST_REMOVE ( hdev , entry ); <nl> } <nl> 
static int proxy_init ( FsContext * ctx ) <nl> sock_id = atoi ( ctx -> fs_root ); <nl> if ( sock_id < 0 ) { <nl> fprintf ( stderr , " socket descriptor not initialized \ n "); <nl> + g_free ( proxy ); <nl> return - 1 ; <nl> } <nl> } <nl> g_free ( ctx -> fs_root ); <nl> + ctx -> fs_root = NULL ; <nl>  <nl> proxy -> in_iovec . iov_base = g_malloc ( PROXY_MAX_IO_SZ + PROXY_HDR_SZ ); <nl> proxy -> in_iovec . iov_len = PROXY_MAX_IO_SZ + PROXY_HDR_SZ ;
static void lan9118_eeprom_cmd ( lan9118_state * s , int cmd , int addr ) <nl> } else { <nl> DPRINTF (" EEPROM Write All ( ignored )\ n "); <nl> } <nl> + break ; <nl> case 5 : /* ERASE */ <nl> if ( s -> eeprom_writable ) { <nl> s -> eeprom [ addr ] = 0xff ;
static int grow_refcount_table ( BlockDriverState * bs , int min_size ) <nl> qemu_free ( s -> refcount_table ); <nl> s -> refcount_table = new_table ; <nl> s -> refcount_table_size = new_table_size ; <nl> + s -> refcount_table_offset = table_offset ; <nl>  <nl> update_refcount ( bs , table_offset , new_table_size2 , 1 ); <nl> return 0 ;
int do_snapshot_blkdev ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> int ret = 0 ; <nl> int flags ; <nl>  <nl> + if (! filename ) { <nl> + qerror_report ( QERR_MISSING_PARAMETER , " snapshot_file "); <nl> + ret = - 1 ; <nl> + goto out ; <nl> + } <nl> + <nl> bs = bdrv_find ( device ); <nl> if (! bs ) { <nl> qerror_report ( QERR_DEVICE_NOT_FOUND , device );
static int ide_dev_initfn ( IDEDevice * dev , IDEDriveKind kind ) <nl> return - 1 ; <nl> } <nl>  <nl> + if ( dev -> conf . logical_block_size != 512 ) { <nl> + error_report (" logical_block_size must be 512 for IDE "); <nl> + return - 1 ; <nl> + } <nl> + <nl> blkconf_serial (& dev -> conf , & dev -> serial ); <nl> if ( kind != IDE_CD ) { <nl> blkconf_geometry (& dev -> conf , & dev -> chs_trans , 65536 , 16 , 255 , & err );
static void vnc_dpy_copy ( DisplayChangeListener * dcl , <nl> } <nl> } <nl>  <nl> + if (! vd -> server ) { <nl> + /* no client connected */ <nl> + return ; <nl> + } <nl> /* do bitblit op on the local surface too */ <nl> pitch = vnc_server_fb_stride ( vd ); <nl> src_row = vnc_server_fb_ptr ( vd , src_x , src_y );
static always_inline int translate_one ( DisasContext * ctx , uint32_t insn ) <nl> break ; <nl> case 0x2C : <nl> /* XXX : incorrect */ <nl> - if ( fn11 == 0x2AC ) { <nl> + if ( fn11 == 0x2AC || fn11 == 0x6AC ) { <nl> /* CVTST */ <nl> gen_farith2 (& helper_cvtst , rb , rc ); <nl> } else {
static int slirp_smb ( SlirpState * s , const char * exported_dir , <nl> " state directory =% s \ n " <nl> " log file =% s / log . smbd \ n " <nl> " smb passwd file =% s / smbpasswd \ n " <nl> - " security = share \ n " <nl> + " security = user \ n " <nl> + " map to guest = Bad User \ n " <nl> "[ qemu ]\ n " <nl> " path =% s \ n " <nl> " read only = no \ n "
const VMStateDescription vmstate_ahci = { <nl> VMSTATE_UINT32 ( control_regs . impl , AHCIState ), <nl> VMSTATE_UINT32 ( control_regs . version , AHCIState ), <nl> VMSTATE_UINT32 ( idp_index , AHCIState ), <nl> - VMSTATE_INT32 ( ports , AHCIState ), <nl> + VMSTATE_INT32_EQUAL ( ports , AHCIState ), <nl> VMSTATE_END_OF_LIST () <nl> }, <nl> };
void cpu_x86_load_seg ( CPUX86State * s , int seg_reg , int selector ) <nl> cpu_x86_load_seg_cache ( env , seg_reg , selector , <nl> ( selector << 4 ), 0xffff , 0 ); <nl> } else { <nl> - load_seg ( seg_reg , selector ); <nl> + helper_load_seg ( seg_reg , selector ); <nl> } <nl> env = saved_env ; <nl> }
build_ssdt ( GArray * table_data , GArray * linker , <nl>  <nl> patch_pci_windows ( pci , ssdt_ptr , sizeof ( ssdp_misc_aml )); <nl>  <nl> - *( uint16_t *)( ssdt_ptr + * ssdt_isa_pest ) = <nl> - cpu_to_le16 ( misc -> pvpanic_port ); <nl> + ACPI_BUILD_SET_LE ( ssdt_ptr , sizeof ( ssdp_misc_aml ), <nl> + ssdt_isa_pest [ 0 ], 16 , misc -> pvpanic_port ); <nl>  <nl> { <nl> GArray * sb_scope = build_alloc_array ();
static int xen_pt_bar_reg_read ( XenPCIPassthroughState * s , XenPTReg * cfg_entry , <nl>  <nl> /* get BAR index */ <nl> index = xen_pt_bar_offset_to_index ( reg -> offset ); <nl> - if ( index < 0 || index >= PCI_NUM_REGIONS ) { <nl> + if ( index < 0 || index >= PCI_NUM_REGIONS - 1 ) { <nl> XEN_PT_ERR (& s -> dev , " Internal error : Invalid BAR index [% d ].\ n ", index ); <nl> return - 1 ; <nl> }
static int tpm_passthrough_unix_write ( int fd , const uint8_t * buf , uint32_t len ) <nl> int ret , remain ; <nl>  <nl> remain = len ; <nl> - while ( len > 0 ) { <nl> + while ( remain > 0 ) { <nl> ret = write ( fd , buf , remain ); <nl> if ( ret < 0 ) { <nl> if ( errno != EINTR && errno != EAGAIN ) {
static void ps2_reset_keyboard ( PS2KbdState * s ) <nl> trace_ps2_reset_keyboard ( s ); <nl> s -> scan_enabled = 1 ; <nl> s -> scancode_set = 2 ; <nl> + ps2_reset_queue (& s -> common ); <nl> ps2_set_ledstate ( s , 0 ); <nl> } <nl> 
static void breakpoint_invalidate ( CPUState * cpu , target_ulong pc ) <nl> # else <nl> static void breakpoint_invalidate ( CPUState * cpu , target_ulong pc ) <nl> { <nl> - tb_invalidate_phys_addr ( cpu_get_phys_page_debug ( cpu , pc ) | <nl> - ( pc & ~ TARGET_PAGE_MASK )); <nl> + hwaddr phys = cpu_get_phys_page_debug ( cpu , pc ); <nl> + if ( phys != - 1 ) { <nl> + tb_invalidate_phys_addr ( phys | ( pc & ~ TARGET_PAGE_MASK )); <nl> + } <nl> } <nl> # endif <nl> # endif /* TARGET_HAS_ICE */
abi_long target_mmap ( abi_ulong start , abi_ulong len , int prot , <nl> goto fail ; <nl> if (!( prot & PROT_WRITE )) { <nl> ret = target_mprotect ( start , len , prot ); <nl> - if ( ret != 0 ) { <nl> - start = ret ; <nl> - goto the_end ; <nl> - } <nl> + assert ( ret == 0 ); <nl> } <nl> goto the_end ; <nl> }
static int blk_send_response_one ( struct ioreq * ioreq ) <nl> break ; <nl> default : <nl> dst = NULL ; <nl> + return 0 ; <nl> } <nl> memcpy ( dst , & resp , sizeof ( resp )); <nl> blkdev -> rings . common . rsp_prod_pvt ++;
void ahci_realize ( AHCIState * s , DeviceState * qdev , AddressSpace * as , int ports ) <nl> ad -> port . dma -> ops = & ahci_dma_ops ; <nl> ide_register_restart_cb (& ad -> port ); <nl> } <nl> + g_free ( irqs ); <nl> } <nl>  <nl> void ahci_uninit ( AHCIState * s )
static void gen_load_fp ( DisasContext * s , int opsize , TCGv addr , TCGv_ptr fp ) <nl> case OS_DOUBLE : <nl> tcg_gen_qemu_ld64 ( t64 , addr , index ); <nl> gen_helper_extf64 ( cpu_env , fp , t64 ); <nl> - tcg_temp_free_i64 ( t64 ); <nl> break ; <nl> case OS_EXTENDED : <nl> if ( m68k_feature ( s -> env , M68K_FEATURE_CF_FPU )) {
static void ioapic_class_init ( ObjectClass * klass , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( klass ); <nl>  <nl> k -> realize = ioapic_realize ; <nl> + /* <nl> + * If APIC is in kernel , we need to update the kernel cache after <nl> + * migration , otherwise first 24 gsi routes will be invalid . <nl> + */ <nl> + k -> post_load = ioapic_update_kvm_routes ; <nl> dc -> reset = ioapic_reset_common ; <nl> dc -> props = ioapic_properties ; <nl> }
int64_t qcow2_alloc_bytes ( BlockDriverState * bs , int size ) <nl> return new_cluster ; <nl> } <nl>  <nl> + if ( new_cluster == 0 ) { <nl> + qcow2_signal_corruption ( bs , true , - 1 , - 1 , " Preventing invalid " <nl> + " allocation of compressed cluster " <nl> + " at offset 0 "); <nl> + return - EIO ; <nl> + } <nl> + <nl> if (! offset || ROUND_UP ( offset , s -> cluster_size ) != new_cluster ) { <nl> offset = new_cluster ; <nl> free_in_cluster = s -> cluster_size ;
int main ( int argc , char ** argv ) <nl> " - device ipmi - bmc - extern , chardev = ipmi0 , id = bmc0 " <nl> " - device isa - ipmi - bt , bmc = bmc0 ", emu_port ); <nl> qtest_start ( cmdline ); <nl> + g_free ( cmdline ); <nl> qtest_irq_intercept_in ( global_qtest , " ioapic "); <nl> qtest_add_func ("/ ipmi / extern / connect ", test_connect ); <nl> qtest_add_func ("/ ipmi / extern / bt_base ", test_bt_base );
static void pc_fw_add_pflash_drv ( void ) <nl> return ; <nl> } <nl>  <nl> - drive_init ( opts , machine -> use_scsi ); <nl> + if (! drive_init ( opts , machine -> use_scsi )) { <nl> + qemu_opts_del ( opts ); <nl> + } <nl> } <nl>  <nl> static void pc_system_flash_init ( MemoryRegion * rom_memory ,
static int cpu_x86_find_by_name ( x86_def_t * x86_cpu_def , const char * cpu_model ) <nl>  <nl> tsc_freq = strtosz_suffix_unit ( val , & err , <nl> STRTOSZ_DEFSUFFIX_B , 1000 ); <nl> - if (!* val || * err ) { <nl> + if ( tsc_freq < 0 || * err ) { <nl> fprintf ( stderr , " bad numerical value % s \ n ", val ); <nl> goto error ; <nl> }
static inline void gen_bcond ( DisasContext * ctx , int type ) <nl> gen_update_nip ( ctx , ctx -> nip ); <nl> tcg_gen_exit_tb ( 0 ); <nl> } <nl> - if ( type == BCOND_LR || type == BCOND_CTR ) { <nl> + if ( type == BCOND_LR || type == BCOND_CTR || type == BCOND_TAR ) { <nl> tcg_temp_free ( target ); <nl> } <nl> }
DeviceState * qdev_device_add ( QemuOpts * opts ) <nl> return NULL ; <nl> } <nl>  <nl> + if ( object_class_is_abstract ( oc )) { <nl> + qerror_report ( QERR_INVALID_PARAMETER_VALUE , " driver ", <nl> + " non - abstract device type "); <nl> + return NULL ; <nl> + } <nl> + <nl> dc = DEVICE_CLASS ( oc ); <nl>  <nl> /* find bus */
int qdev_device_help ( QemuOpts * opts ) <nl> } <nl> } <nl>  <nl> - if (! klass ) { <nl> + if (! object_class_dynamic_cast ( klass , TYPE_DEVICE )) { <nl> return 0 ; <nl> } <nl> do {
static int aio_write_f ( int argc , char ** argv ) <nl> case ' P ': <nl> pattern = parse_pattern ( optarg ); <nl> if ( pattern < 0 ) { <nl> + free ( ctx ); <nl> return 0 ; <nl> } <nl> break ;
int load_image_targphys ( const char * filename , <nl> int size ; <nl>  <nl> size = get_image_size ( filename ); <nl> - if ( size > 0 ) <nl> + if ( size > max_sz ) { <nl> + return - 1 ; <nl> + } <nl> + if ( size > 0 ) { <nl> rom_add_file_fixed ( filename , addr , - 1 ); <nl> + } <nl> return size ; <nl> } <nl> 
int qcow2_pre_write_overlap_check ( BlockDriverState * bs , int ign , int64_t offset , <nl> offset , <nl> true , <nl> size , <nl> + true , <nl> & error_abort ); <nl> g_free ( message ); <nl> 
again : <nl> fail : <nl> qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) & l2_table ); <nl> fail_put : <nl> - if ( nb_clusters > 0 ) { <nl> + if ( m -> nb_clusters > 0 ) { <nl> QLIST_REMOVE ( m , next_in_flight ); <nl> } <nl> return ret ;
static int usb_msd_initfn_storage ( USBDevice * dev ) <nl> s -> conf . bootindex , dev -> serial , <nl> & err ); <nl> if (! scsi_dev ) { <nl> + error_report ("% s ", error_get_pretty ( err )); <nl> + error_free ( err ); <nl> return - 1 ; <nl> } <nl> s -> bus . qbus . allow_hotplug = 0 ;
int qcow2_get_cluster_offset ( BlockDriverState * bs , uint64_t offset , <nl> break ; <nl> case QCOW2_CLUSTER_ZERO : <nl> if ( s -> qcow_version < 3 ) { <nl> + qcow2_cache_put ( bs , s -> l2_table_cache , ( void **) & l2_table ); <nl> return - EIO ; <nl> } <nl> c = count_contiguous_clusters ( nb_clusters , s -> cluster_size ,
static ImageInfoSpecific * qcow2_get_specific_info ( BlockDriverState * bs ) <nl> . lazy_refcounts = s -> compatible_features & <nl> QCOW2_COMPAT_LAZY_REFCOUNTS , <nl> . has_lazy_refcounts = true , <nl> + . corrupt = s -> incompatible_features & <nl> + QCOW2_INCOMPAT_CORRUPT , <nl> + . has_corrupt = true , <nl> }; <nl> } <nl> 
void virtio_reset ( void * opaque ) <nl> vdev -> vq [ i ]. signalled_used_valid = false ; <nl> vdev -> vq [ i ]. notification = true ; <nl> vdev -> vq [ i ]. vring . num = vdev -> vq [ i ]. vring . num_default ; <nl> + vdev -> vq [ i ]. inuse = 0 ; <nl> } <nl> } <nl> 
static uint32_t cc_calc_abs_64 ( int64_t dst ) <nl> if (( uint64_t ) dst == 0x8000000000000000ULL ) { <nl> return 3 ; <nl> } else if ( dst ) { <nl> - return 1 ; <nl> + return 2 ; <nl> } else { <nl> return 0 ; <nl> } <nl> static uint32_t cc_calc_abs_32 ( int32_t dst ) <nl> if (( uint32_t ) dst == 0x80000000UL ) { <nl> return 3 ; <nl> } else if ( dst ) { <nl> - return 1 ; <nl> + return 2 ; <nl> } else { <nl> return 0 ; <nl> }
static void gd_menu_grab_input ( GtkMenuItem * item , void * opaque ) <nl> VirtualConsole * vc = gd_vc_find_current ( s ); <nl>  <nl> if ( gd_is_grab_active ( s )) { <nl> - gd_grab_keyboard ( vc ); <nl> + if (! gd_grab_on_hover ( s )) { <nl> + gd_grab_keyboard ( vc ); <nl> + } <nl> gd_grab_pointer ( vc ); <nl> } else { <nl> gd_ungrab_keyboard ( s );
static int tftp_session_allocate ( Slirp * slirp , struct sockaddr_storage * srcsas , <nl>  <nl> found : <nl> memset ( spt , 0 , sizeof (* spt )); <nl> - spt -> client_addr = * srcsas ; <nl> + memcpy (& spt -> client_addr , srcsas , sockaddr_size ( srcsas )); <nl> spt -> fd = - 1 ; <nl> spt -> block_size = 512 ; <nl> spt -> client_port = tp -> udp . uh_sport ;
static void x86_cpuid_set_model_id ( CPUX86State * env , const char * model_id ) <nl> model_id = ""; <nl> } <nl> len = strlen ( model_id ); <nl> + memset ( env -> cpuid_model , 0 , 48 ); <nl> for ( i = 0 ; i < 48 ; i ++) { <nl> if ( i >= len ) { <nl> c = '\ 0 ';
static void aw_a10_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = aw_a10_realize ; <nl> + /* Reason : Uses serial_hds in realize and nd_table in instance_init */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo aw_a10_type_info = {
void s390_init_cpus ( MachineState * machine ) <nl> machine -> cpu_model = " host "; <nl> } <nl>  <nl> - cpu_states = g_malloc0 ( sizeof ( S390CPU *) * max_cpus ); <nl> + cpu_states = g_new0 ( S390CPU *, max_cpus ); <nl>  <nl> for ( i = 0 ; i < max_cpus ; i ++) { <nl> name = g_strdup_printf (" cpu [% i ]", i );
static inline bool cptype_valid ( int cptype ) <nl> */ <nl> static inline int arm_current_el ( CPUARMState * env ) <nl> { <nl> + if ( arm_feature ( env , ARM_FEATURE_M )) { <nl> + return !(( env -> v7m . exception == 0 ) && ( env -> v7m . control & 1 )); <nl> + } <nl> + <nl> if ( is_a64 ( env )) { <nl> return extract32 ( env -> pstate , 2 , 2 ); <nl> }
void tcg_add_target_add_op_defs ( const TCGTargetOpDef * tdefs ) <nl> if ( tdefs -> op == ( TCGOpcode )- 1 ) <nl> break ; <nl> op = tdefs -> op ; <nl> - assert ( op >= 0 && op < NB_OPS ); <nl> + assert (( unsigned ) op < NB_OPS ); <nl> def = & tcg_op_defs [ op ]; <nl> # if defined ( CONFIG_DEBUG_TCG ) <nl> /* Duplicate entry in op definitions ? */
static inline int test_and_change_bit ( int nr , volatile unsigned long * addr ) <nl> { <nl> unsigned long mask = BIT_MASK ( nr ); <nl> unsigned long * p = (( unsigned long *) addr ) + BIT_WORD ( nr ); <nl> - unsigned long old ; <nl> + unsigned long old = * p ; <nl>  <nl> * p = old ^ mask ; <nl> return ( old & mask ) != 0 ;
int qemu_devtree_add_subnode ( void * fdt , const char * name ) <nl> } <nl>  <nl> retval = fdt_add_subnode ( fdt , parent , basename ); <nl> +# if 0 <nl> if ( retval < 0 ) { <nl> fprintf ( stderr , " FDT : Failed to create subnode % s : % s \ n ", name , <nl> fdt_strerror ( retval )); <nl> exit ( 1 ); <nl> } <nl> +# endif <nl>  <nl> g_free ( dupname ); <nl> return retval ;
static void aspeed_soc_class_init ( ObjectClass * oc , void * data ) <nl>  <nl> sc -> info = ( AspeedSoCInfo *) data ; <nl> dc -> realize = aspeed_soc_realize ; <nl> + /* Reason : Uses serial_hds and nd_table in realize () directly */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo aspeed_soc_type_info = {
static void register_subpage ( MemoryRegionSection * section ) <nl> subpage = container_of ( existing -> mr , subpage_t , iomem ); <nl> } <nl> start = section -> offset_within_address_space & ~ TARGET_PAGE_MASK ; <nl> - end = start + section -> size ; <nl> + end = start + section -> size - 1 ; <nl> subpage_register ( subpage , start , end , phys_section_add ( section )); <nl> } <nl> 
static int qcow2_read_extensions ( BlockDriverState * bs , uint64_t start_offset , <nl> # ifdef DEBUG_EXT <nl> printf (" ext . magic = 0x % x \ n ", ext . magic ); <nl> # endif <nl> - if ( ext . len > end_offset - offset ) { <nl> + if ( offset > end_offset || ext . len > end_offset - offset ) { <nl> error_setg ( errp , " Header extension too large "); <nl> return - EINVAL ; <nl> }
uint32_t lm4549_write_samples ( lm4549_state * s , uint32_t left , uint32_t right ) <nl> This model supports 16 - bit playback . <nl> */ <nl>  <nl> - if ( s -> buffer_level >= LM4549_BUFFER_SIZE ) { <nl> + if ( s -> buffer_level > LM4549_BUFFER_SIZE - 2 ) { <nl> DPRINTF (" write_sample Buffer full \ n "); <nl> return 0 ; <nl> }
static uint64_t serial_ioport_read ( void * opaque , hwaddr addr , unsigned size ) <nl> ret = s -> divider & 0xff ; <nl> } else { <nl> if ( s -> fcr & UART_FCR_FE ) { <nl> - ret = fifo8_is_full (& s -> recv_fifo ) ? <nl> + ret = fifo8_is_empty (& s -> recv_fifo ) ? <nl> 0 : fifo8_pop (& s -> recv_fifo ); <nl> if ( s -> recv_fifo . num == 0 ) { <nl> s -> lsr &= ~( UART_LSR_DR | UART_LSR_BI );
static void cpu_class_init ( ObjectClass * oc , void * data ) <nl> k -> get_receive_mask = receive_mask ; <nl> k -> read_event_data = read_event_data ; <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> + /* <nl> + * Reason : raise_irq_cpu_hotplug () depends on an unique <nl> + * TYPE_SCLP_CPU_HOTPLUG device , which is already created <nl> + * by the sclp event facility <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo sclp_cpu_info = {
static int cryptodev_builtin_create_cipher_session ( <nl> return - 1 ; <nl> } <nl> break ; <nl> + case VIRTIO_CRYPTO_CIPHER_AES_XTS : <nl> + mode = QCRYPTO_CIPHER_MODE_XTS ; <nl> + algo = cryptodev_builtin_get_aes_algo ( sess_info -> key_len , <nl> + mode , errp ); <nl> + if ( algo < 0 ) { <nl> + return - 1 ; <nl> + } <nl> + break ; <nl> case VIRTIO_CRYPTO_CIPHER_DES_ECB : <nl> mode = QCRYPTO_CIPHER_MODE_ECB ; <nl> algo = QCRYPTO_CIPHER_ALG_DES_RFB ;
static void qxl_destroy_primary ( PCIQXLDevice * d ) <nl> dprint ( d , 1 , "% s \ n ", __FUNCTION__ ); <nl>  <nl> d -> mode = QXL_MODE_UNDEFINED ; <nl> + qemu_mutex_unlock_iothread (); <nl> d -> ssd . worker -> destroy_primary_surface ( d -> ssd . worker , 0 ); <nl> + qemu_mutex_lock_iothread (); <nl> } <nl>  <nl> static void qxl_set_mode ( PCIQXLDevice * d , int modenr , int loadvm )
static int vmdk_parent_open ( BlockDriverState * bs , const char * filename ) <nl> p_name += sizeof (" parentFileNameHint ") + 1 ; <nl> if (( end_name = strchr ( p_name ,'\"')) == 0 ) <nl> return - 1 ; <nl> + if (( end_name - p_name ) > sizeof ( s -> hd -> backing_file ) - 1 ) <nl> + return - 1 ; <nl>  <nl> strncpy ( s -> hd -> backing_file , p_name , end_name - p_name ); <nl> if ( stat ( s -> hd -> backing_file , & file_buf ) != 0 ) {
static inline void futex_wait ( QemuEvent * ev , unsigned val ) <nl> # else <nl> static inline void futex_wake ( QemuEvent * ev , int n ) <nl> { <nl> + pthread_mutex_lock (& ev -> lock ); <nl> if ( n == 1 ) { <nl> pthread_cond_signal (& ev -> cond ); <nl> } else { <nl> pthread_cond_broadcast (& ev -> cond ); <nl> } <nl> + pthread_mutex_unlock (& ev -> lock ); <nl> } <nl>  <nl> static inline void futex_wait ( QemuEvent * ev , unsigned val )
# define MAX_TOKEN_COUNT ( 2ULL << 20 ) <nl> # define MAX_NESTING ( 1ULL << 10 ) <nl>  <nl> + static void json_message_free_token ( void * token , void * opaque ) <nl> +{ <nl> + g_free ( token ); <nl> +} <nl> + <nl> static void json_message_free_tokens ( JSONMessageParser * parser ) <nl> { <nl> if ( parser -> tokens ) { <nl> + g_queue_foreach ( parser -> tokens , json_message_free_token , NULL ); <nl> g_queue_free ( parser -> tokens ); <nl> parser -> tokens = NULL ; <nl> }
void * virtqueue_pop ( VirtQueue * vq , size_t sz ) <nl>  <nl> max = vq -> vring . num ; <nl>  <nl> + if ( vq -> inuse >= vq -> vring . num ) { <nl> + error_report (" Virtqueue size exceeded "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> i = head = virtqueue_get_head ( vq , vq -> last_avail_idx ++); <nl> if ( virtio_vdev_has_feature ( vdev , VIRTIO_RING_F_EVENT_IDX )) { <nl> vring_set_avail_event ( vq , vq -> last_avail_idx );
static void acpi_dsdt_add_cpus ( Aml * scope , int smp_cpus ) <nl> uint16_t i ; <nl>  <nl> for ( i = 0 ; i < smp_cpus ; i ++) { <nl> - Aml * dev = aml_device (" C % 03x ", i ); <nl> + Aml * dev = aml_device (" C %. 03X ", i ); <nl> aml_append ( dev , aml_name_decl (" _HID ", aml_string (" ACPI0007 "))); <nl> aml_append ( dev , aml_name_decl (" _UID ", aml_int ( i ))); <nl> aml_append ( scope , dev );
static bool scsi_block_is_passthrough ( SCSIDiskState * s , uint8_t * buf ) <nl> * for the number of logical blocks specified in the length <nl> * field ). For other modes , do not use scatter / gather operation . <nl> */ <nl> - if (( buf [ 1 ] & 6 ) != 2 ) { <nl> + if (( buf [ 1 ] & 6 ) == 2 ) { <nl> return false ; <nl> } <nl> break ;
qemu_irq * i8259_init ( ISABus * bus , qemu_irq parent_irq ) <nl> ISADevice * isadev ; <nl> int i ; <nl>  <nl> - irq_set = g_malloc ( ISA_NUM_IRQS * sizeof ( qemu_irq )); <nl> + irq_set = g_new0 ( qemu_irq , ISA_NUM_IRQS ); <nl>  <nl> isadev = i8259_init_chip ( TYPE_I8259 , bus , true ); <nl> dev = DEVICE ( isadev );
int kvm_cpu_exec ( CPUState * cpu ) <nl> break ; <nl> case KVM_EXIT_MMIO : <nl> DPRINTF (" handle_mmio \ n "); <nl> - qemu_mutex_lock_iothread (); <nl> + /* Called outside BQL */ <nl> address_space_rw (& address_space_memory , <nl> run -> mmio . phys_addr , attrs , <nl> run -> mmio . data , <nl> run -> mmio . len , <nl> run -> mmio . is_write ); <nl> - qemu_mutex_unlock_iothread (); <nl> ret = 0 ; <nl> break ; <nl> case KVM_EXIT_IRQ_WINDOW_OPEN :
static int vhost_user_write ( struct vhost_dev * dev , VhostUserMsg * msg , <nl> return 0 ; <nl> } <nl>  <nl> - qemu_chr_fe_set_msgfds ( chr , fds , fd_num ); <nl> + if ( qemu_chr_fe_set_msgfds ( chr , fds , fd_num ) < 0 ) { <nl> + return - 1 ; <nl> + } <nl>  <nl> return qemu_chr_fe_write_all ( chr , ( const uint8_t *) msg , size ) == size ? <nl> 0 : - 1 ;
void stream_start ( const char * job_id , BlockDriverState * bs , <nl>  <nl> fail : <nl> if ( orig_bs_flags != bdrv_get_flags ( bs )) { <nl> - bdrv_reopen ( bs , s -> bs_flags , NULL ); <nl> + bdrv_reopen ( bs , orig_bs_flags , NULL ); <nl> } <nl> }
static void scsi_cmd_xfer_mode ( SCSICommand * cmd ) <nl> case WRITE_LONG_10 : <nl> case WRITE_SAME_10 : <nl> case WRITE_SAME_16 : <nl> + case UNMAP : <nl> case SEARCH_HIGH_12 : <nl> case SEARCH_EQUAL_12 : <nl> case SEARCH_LOW_12 : <nl> static void scsi_cmd_xfer_mode ( SCSICommand * cmd ) <nl> case SEND_DVD_STRUCTURE : <nl> case PERSISTENT_RESERVE_OUT : <nl> case MAINTENANCE_OUT : <nl> + case ATA_PASSTHROUGH : <nl> cmd -> mode = SCSI_XFER_TO_DEV ; <nl> break ; <nl> default :
qio_channel_socket_accept ( QIOChannelSocket * ioc , <nl> cioc -> fd = qemu_accept ( ioc -> fd , ( struct sockaddr *)& cioc -> remoteAddr , <nl> & cioc -> remoteAddrLen ); <nl> if ( cioc -> fd < 0 ) { <nl> - trace_qio_channel_socket_accept_fail ( ioc ); <nl> if ( errno == EINTR ) { <nl> goto retry ; <nl> } <nl> + error_setg_errno ( errp , errno , " Unable to accept connection "); <nl> + trace_qio_channel_socket_accept_fail ( ioc ); <nl> goto error ; <nl> } <nl> 
struct ICSState { <nl>  <nl> static inline bool ics_valid_irq ( ICSState * ics , uint32_t nr ) <nl> { <nl> - return ( nr >= ics -> offset ) <nl> + return ( ics -> offset != 0 ) && ( nr >= ics -> offset ) <nl> && ( nr < ( ics -> offset + ics -> nr_irqs )); <nl> } <nl> 
int spapr_ovec_populate_dt ( void * fdt , int fdt_offset , <nl> } <nl> } <nl>  <nl> - return fdt_setprop ( fdt , fdt_offset , name , vec , vec_len ); <nl> + return fdt_setprop ( fdt , fdt_offset , name , vec , vec_len + 1 ); <nl> }
fail : <nl>  <nl> if ( use_local_qiov ) { <nl> qemu_iovec_destroy (& local_qiov ); <nl> - qemu_vfree ( head_buf ); <nl> - qemu_vfree ( tail_buf ); <nl> } <nl> + qemu_vfree ( head_buf ); <nl> + qemu_vfree ( tail_buf ); <nl>  <nl> return ret ; <nl> }
static DriveInfo * blockdev_init ( QemuOpts * all_opts , <nl>  <nl> drv = bdrv_find_whitelisted_format ( buf , ro ); <nl> if (! drv ) { <nl> - error_report ("'% s ' invalid format ", buf ); <nl> + if (! ro && bdrv_find_whitelisted_format ( buf , ! ro )) { <nl> + error_report ("'% s ' can be only used as read - only device .", buf ); <nl> + } else { <nl> + error_report ("'% s ' invalid format ", buf ); <nl> + } <nl> return NULL ; <nl> } <nl> }
coroutine_fn iscsi_co_write_zeroes ( BlockDriverState * bs , int64_t sector_num , <nl> nb_blocks = sector_qemu2lun ( nb_sectors , iscsilun ); <nl>  <nl> if ( iscsilun -> zeroblock == NULL ) { <nl> - iscsilun -> zeroblock = g_malloc0 ( iscsilun -> block_size ); <nl> + iscsilun -> zeroblock = g_try_malloc0 ( iscsilun -> block_size ); <nl> + if ( iscsilun -> zeroblock == NULL ) { <nl> + return - ENOMEM ; <nl> + } <nl> } <nl>  <nl> iscsi_co_init_iscsitask ( iscsilun , & iTask );
int qdev_device_help ( QemuOpts * opts ) <nl> return 0 ; <nl> } <nl>  <nl> - if (! object_class_by_name ( driver )) { <nl> - const char * typename = find_typename_by_alias ( driver ); <nl> - <nl> - if ( typename ) { <nl> - driver = typename ; <nl> - } <nl> + qdev_get_device_class (& driver , & local_err ); <nl> + if ( local_err ) { <nl> + goto error ; <nl> } <nl>  <nl> prop_list = qmp_device_list_properties ( driver , & local_err );
SCSIDevice * scsi_bus_legacy_add_drive ( SCSIBus * bus , BlockDriverState * bdrv , <nl> if ( object_property_find ( OBJECT ( dev ), " removable ", NULL )) { <nl> qdev_prop_set_bit ( dev , " removable ", removable ); <nl> } <nl> - if ( serial ) { <nl> + if ( serial && object_property_find ( OBJECT ( dev ), " serial ", NULL )) { <nl> qdev_prop_set_string ( dev , " serial ", serial ); <nl> } <nl> if ( qdev_prop_set_drive ( dev , " drive ", bdrv ) < 0 ) {
static void prop_get_fdt ( Object * obj , Visitor * v , const char * name , <nl> void * fdt ; <nl>  <nl> if (! drc -> fdt ) { <nl> - visit_start_struct ( v , name , NULL , 0 , & err ); <nl> - if (! err ) { <nl> - visit_end_struct ( v , & err ); <nl> - } <nl> - error_propagate ( errp , err ); <nl> + visit_type_null ( v , NULL , errp ); <nl> return ; <nl> } <nl> 
static int usb_xhci_initfn ( struct PCIDevice * dev ) <nl> if ( xhci -> numintrs > MAXINTRS ) { <nl> xhci -> numintrs = MAXINTRS ; <nl> } <nl> + while ( xhci -> numintrs & ( xhci -> numintrs - 1 )) { /* ! power of 2 */ <nl> + xhci -> numintrs ++; <nl> + } <nl> if ( xhci -> numintrs < 1 ) { <nl> xhci -> numintrs = 1 ; <nl> }
DisplayState * init_displaystate ( void ) <nl> gchar * name ; <nl> int i ; <nl>  <nl> - if (! display_state ) { <nl> - display_state = g_new0 ( DisplayState , 1 ); <nl> - } <nl> - <nl> + get_alloc_displaystate (); <nl> for ( i = 0 ; i < nb_consoles ; i ++) { <nl> if ( consoles [ i ]-> console_type != GRAPHIC_CONSOLE && <nl> consoles [ i ]-> ds == NULL ) {
static FeatureWordInfo feature_word_info [ FEATURE_WORDS ] = { <nl> " ibpb ", NULL , NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> - NULL , " virt - ssbd ", NULL , NULL , <nl> + " amd - ssbd ", " virt - ssbd ", NULL , NULL , <nl> NULL , NULL , NULL , NULL , <nl> }, <nl> . cpuid_eax = 0x80000008 ,
static void handle_qmp_command ( JSONMessageParser * parser , QList * tokens ) <nl> obj = qdict_get ( input , " arguments "); <nl> if (! obj ) { <nl> args = qdict_new (); <nl> + } else if ( qobject_type ( obj ) != QTYPE_QDICT ) { <nl> + qerror_report ( QERR_QMP_BAD_INPUT_OBJECT_MEMBER , " arguments ", " object "); <nl> + goto err_input ; <nl> } else { <nl> args = qobject_to_qdict ( obj ); <nl> QINCREF ( args );
static void ahci_reset_port ( AHCIState * s , int port ) <nl> ncq_tfs -> aiocb = NULL ; <nl> } <nl>  <nl> + /* Maybe we just finished the request thanks to bdrv_aio_cancel () */ <nl> + if (! ncq_tfs -> used ) { <nl> + continue ; <nl> + } <nl> + <nl> qemu_sglist_destroy (& ncq_tfs -> sglist ); <nl> ncq_tfs -> used = 0 ; <nl> }
static uint64_t pl011_read ( void * opaque , target_phys_addr_t offset , <nl> if ( s -> read_count == s -> read_trigger - 1 ) <nl> s -> int_level &= ~ PL011_INT_RX ; <nl> pl011_update ( s ); <nl> - qemu_chr_accept_input ( s -> chr ); <nl> + if ( s -> chr ) { <nl> + qemu_chr_accept_input ( s -> chr ); <nl> + } <nl> return c ; <nl> case 1 : /* UARTCR */ <nl> return 0 ;
static inline uint32_t efststeq ( CPUPPCState * env , uint32_t op1 , uint32_t op2 ) <nl> # define HELPER_SINGLE_SPE_CMP ( name ) \ <nl> uint32_t helper_e ## name ( CPUPPCState * env , uint32_t op1 , uint32_t op2 ) \ <nl> { \ <nl> - return e ## name ( env , op1 , op2 ) << 2 ; \ <nl> + return e ## name ( env , op1 , op2 ); \ <nl> } <nl> /* efststlt */ <nl> HELPER_SINGLE_SPE_CMP ( fststlt );
static QTAILQ_HEAD ( CharDriverStateHead , CharDriverState ) chardevs = <nl> CharDriverState * qemu_chr_alloc ( void ) <nl> { <nl> CharDriverState * chr = g_malloc0 ( sizeof ( CharDriverState )); <nl> + qemu_mutex_init (& chr -> chr_write_lock ); <nl> return chr ; <nl> } <nl> 
static void usb_msd_realize_storage ( USBDevice * dev , Error ** errp ) <nl> error_propagate ( errp , err ); <nl> return ; <nl> } <nl> - s -> bus . qbus . allow_hotplug = 0 ; <nl> usb_msd_handle_reset ( dev ); <nl>  <nl> if ( bdrv_key_required ( bs )) {
static void assign_failed_examine ( AssignedDevice * dev ) <nl> goto fail ; <nl> } <nl>  <nl> + driver [ r ] = 0 ; <nl> ns = strrchr ( driver , '/'); <nl> if (! ns ) { <nl> goto fail ;
static void i6300esb_restart_timer ( I6300State * d , int stage ) <nl> * multiply here can exceed 64 - bits , before we divide by 33MHz , so <nl> * we use a higher - precision intermediate result . <nl> */ <nl> - timeout = muldiv64 ( get_ticks_per_sec (), timeout , 33000000 ); <nl> + timeout = muldiv64 ( timeout , get_ticks_per_sec (), 33000000 ); <nl>  <nl> i6300esb_debug (" stage % d , timeout %" PRIi64 "\ n ", d -> stage , timeout ); <nl> 
static SpiceChannelList * qmp_query_spice_channels ( void ) <nl> struct sockaddr * paddr ; <nl> socklen_t plen ; <nl>  <nl> - if (!( item -> info -> flags & SPICE_CHANNEL_EVENT_FLAG_ADDR_EXT )) { <nl> - error_report (" invalid channel event "); <nl> - return NULL ; <nl> - } <nl> + assert ( item -> info -> flags & SPICE_CHANNEL_EVENT_FLAG_ADDR_EXT ); <nl>  <nl> chan = g_malloc0 ( sizeof (* chan )); <nl> chan -> value = g_malloc0 ( sizeof (* chan -> value ));
static int kvmppc_read_host_property ( const char * node_path , const char * prop , <nl> { <nl> char * path ; <nl> FILE * f ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> int pathlen ; <nl>  <nl> pathlen = snprintf ( NULL , 0 , "% s /% s /% s ", PROC_DEVTREE_PATH , node_path , prop )
void unregister_displaychangelistener ( DisplayChangeListener * dcl ) <nl> dcl -> con -> dcls --; <nl> } <nl> QLIST_REMOVE ( dcl , next ); <nl> + dcl -> ds = NULL ; <nl> gui_setup_refresh ( ds ); <nl> } <nl> 
static void qvirtio_9p_pci_stop ( QVirtIO9P * v9p ) <nl> { <nl> qvirtqueue_cleanup ( v9p -> dev -> bus , v9p -> vq , v9p -> qs -> alloc ); <nl> qvirtio_pci_device_disable ( container_of ( v9p -> dev , QVirtioPCIDevice , vdev )); <nl> - g_free ( v9p -> dev ); <nl> + qvirtio_pci_device_free (( QVirtioPCIDevice *) v9p -> dev ); <nl> qvirtio_9p_stop ( v9p ); <nl> } <nl> 
ssize_t pcnet_receive ( NetClientState * nc , const uint8_t * buf , size_t size_ ) <nl> int pktcount = 0 ; <nl>  <nl> if (! s -> looptest ) { <nl> + if ( size > 4092 ) { <nl> +# ifdef PCNET_DEBUG_RMD <nl> + fprintf ( stderr , " pcnet : truncates rx packet .\ n "); <nl> +# endif <nl> + size = 4092 ; <nl> + } <nl> memcpy ( src , buf , size ); <nl> /* no need to compute the CRC */ <nl> src [ size ] = 0 ;
static size_t qcrypto_hash_alg_size [ QCRYPTO_HASH_ALG__MAX ] = { <nl>  <nl> size_t qcrypto_hash_digest_len ( QCryptoHashAlgorithm alg ) <nl> { <nl> - if ( alg >= G_N_ELEMENTS ( qcrypto_hash_alg_size )) { <nl> - return 0 ; <nl> - } <nl> + assert ( alg < G_N_ELEMENTS ( qcrypto_hash_alg_size )); <nl> return qcrypto_hash_alg_size [ alg ]; <nl> } <nl> 
static inline TCGv iwmmxt_load_creg ( int reg ) <nl> static inline void iwmmxt_store_creg ( int reg , TCGv var ) <nl> { <nl> tcg_gen_st_i32 ( var , cpu_env , offsetof ( CPUState , iwmmxt . cregs [ reg ])); <nl> + dead_tmp ( var ); <nl> } <nl>  <nl> static inline void gen_op_iwmmxt_movq_wRn_M0 ( int rn ) <nl> static int disas_iwmmxt_insn ( CPUState * env , DisasContext * s , uint32_t insn ) <nl> } <nl> } <nl> } <nl> + dead_tmp ( addr ); <nl> return 0 ; <nl> } <nl> 
void bdrv_drain_all ( void ) <nl> BlockDriverState * bs ; <nl>  <nl> while ( busy ) { <nl> - /* FIXME : We do not have timer support here , so this is effectively <nl> - * a busy wait . <nl> - */ <nl> QTAILQ_FOREACH ( bs , & bdrv_states , list ) { <nl> - if ( bdrv_start_throttled_reqs ( bs )) { <nl> - busy = true ; <nl> - } <nl> + bdrv_start_throttled_reqs ( bs ); <nl> } <nl>  <nl> busy = bdrv_requests_pending_all ();
static void bmdma_irq ( void * opaque , int n , int level ) <nl> return ; <nl> } <nl>  <nl> - if ( bm ) { <nl> - bm -> status |= BM_STATUS_INT ; <nl> - } <nl> + bm -> status |= BM_STATUS_INT ; <nl>  <nl> /* trigger the real irq */ <nl> qemu_set_irq ( bm -> irq , level );
static int coroutine_fn copy_sectors ( BlockDriverState * bs , <nl> BLKDBG_EVENT ( bs -> file , BLKDBG_COW_READ ); <nl>  <nl> if (! bs -> drv ) { <nl> - return - ENOMEDIUM ; <nl> + ret = - ENOMEDIUM ; <nl> + goto out ; <nl> } <nl>  <nl> /* Call . bdrv_co_readv () directly instead of using the public block - layer
static void json_message_process_token ( JSONLexer * lexer , QString * token , JSONTok <nl> parser -> bracket_count == 0 )) { <nl> goto out_emit ; <nl> } else if ( parser -> token_size > MAX_TOKEN_SIZE || <nl> - parser -> bracket_count > MAX_NESTING || <nl> - parser -> brace_count > MAX_NESTING ) { <nl> + parser -> bracket_count + parser -> brace_count > MAX_NESTING ) { <nl> /* Security consideration , we limit total memory allocated per object <nl> * and the maximum recursion depth that a message can force . <nl> */
void kvm_set_phys_mem ( target_phys_addr_t start_addr , <nl>  <nl> mem = kvm_lookup_slot ( s , start_addr ); <nl> if ( mem ) { <nl> - if ( flags == IO_MEM_UNASSIGNED ) { <nl> + if (( flags == IO_MEM_UNASSIGNED ) || ( flags >= TLB_MMIO )) { <nl> mem -> memory_size = 0 ; <nl> mem -> guest_phys_addr = start_addr ; <nl> mem -> userspace_addr = 0 ;
static void pci_info_device ( PCIBus * bus , PCIDevice * d ) <nl> base , limit ); <nl>  <nl> base = pci_bridge_get_base ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> - limit = pci_config_get_memory_base ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> + limit = pci_bridge_get_limit ( d , PCI_BASE_ADDRESS_SPACE_MEMORY ); <nl> monitor_printf ( mon , <nl> " memory range [ 0x % 08 " PRIx64 ", 0x % 08 " PRIx64 "]\ n ", <nl> base , limit );
static void x86_cpu_realizefn ( DeviceState * dev , Error ** errp ) <nl> env -> cpuid_ext3_features &= TCG_EXT3_FEATURES ; <nl> env -> cpuid_svm_features &= TCG_SVM_FEATURES ; <nl> } else { <nl> -# ifdef CONFIG_KVM <nl> - filter_features_for_kvm ( cpu ); <nl> -# endif <nl> if ( check_cpuid && kvm_check_features_against_host ( cpu ) <nl> && enforce_cpuid ) { <nl> error_setg ( errp , " Host ' s CPU doesn ' t support requested features "); <nl> return ; <nl> } <nl> +# ifdef CONFIG_KVM <nl> + filter_features_for_kvm ( cpu ); <nl> +# endif <nl> } <nl>  <nl> # ifndef CONFIG_USER_ONLY
int kvm_init ( int smp_cpus ) <nl> int ret ; <nl> int i ; <nl>  <nl> - if ( smp_cpus > 1 ) <nl> + if ( smp_cpus > 1 ) { <nl> + fprintf ( stderr , " No SMP KVM support , use '- smp 1 '\ n "); <nl> return - EINVAL ; <nl> + } <nl>  <nl> s = qemu_mallocz ( sizeof ( KVMState )); <nl> 
void hmp_info_block_jobs ( Monitor * mon , const QDict * qdict ) <nl> } <nl> list = list -> next ; <nl> } <nl> + <nl> + qapi_free_BlockJobInfoList ( list ); <nl> } <nl>  <nl> void hmp_info_tpm ( Monitor * mon , const QDict * qdict )
static void hid_keyboard_process_keycode ( HIDState * hs ) <nl> slot = hs -> head & QUEUE_MASK ; QUEUE_INCR ( hs -> head ); hs -> n --; <nl> keycode = hs -> kbd . keycodes [ slot ]; <nl>  <nl> + if (! hs -> n ) { <nl> + trace_hid_kbd_queue_empty (); <nl> + } <nl> + <nl> key = keycode & 0x7f ; <nl> index = key | (( hs -> kbd . modifiers & ( 1 << 8 )) >> 1 ); <nl> hid_code = hid_usage_keys [ index ];
mips_mipssim_init ( MachineState * machine ) <nl> ! kernel_filename && ! qtest_enabled ()) { <nl> /* Bail out if we have neither a kernel image nor boot vector code . */ <nl> error_report (" Could not load MIPS bios '% s ', and no " <nl> - "- kernel argument was specified ", filename ); <nl> + "- kernel argument was specified ", bios_name ); <nl> exit ( 1 ); <nl> } else { <nl> /* We have a boot vector start address . */
static int vfio_populate_device ( VFIODevice * vbasedev ) <nl> return ret ; <nl> } <nl>  <nl> - vdev -> regions = g_malloc0_n ( vbasedev -> num_regions , <nl> - sizeof ( VFIORegion *)); <nl> + vdev -> regions = g_new0 ( VFIORegion *, vbasedev -> num_regions ); <nl>  <nl> for ( i = 0 ; i < vbasedev -> num_regions ; i ++) { <nl> struct vfio_region_info reg_info = { . argsz = sizeof ( reg_info ) };
static void vhost_region_del ( MemoryListener * listener , <nl> == section -> offset_within_address_space ) { <nl> -- dev -> n_mem_sections ; <nl> memmove (& dev -> mem_sections [ i ], & dev -> mem_sections [ i + 1 ], <nl> - dev -> n_mem_sections - i ); <nl> + ( dev -> n_mem_sections - i ) * sizeof (* dev -> mem_sections )); <nl> break ; <nl> } <nl> }
static int qcow2_open ( BlockDriverState * bs , QDict * options , int flags , <nl> } <nl> } <nl>  <nl> + if ( header . backing_file_offset > s -> cluster_size ) { <nl> + error_setg ( errp , " Invalid backing file offset "); <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> + } <nl> + <nl> if ( header . backing_file_offset ) { <nl> ext_end = header . backing_file_offset ; <nl> } else {
# define XLNX_ZYNQ_DEVCFG ( obj ) \ <nl> OBJECT_CHECK ( XlnxZynqDevcfg , ( obj ), TYPE_XLNX_ZYNQ_DEVCFG ) <nl>  <nl> -# define XLNX_ZYNQ_DEVCFG_R_MAX 0x118 <nl> +# define XLNX_ZYNQ_DEVCFG_R_MAX ( 0x100 / 4 ) <nl>  <nl> # define XLNX_ZYNQ_DEVCFG_DMA_CMD_FIFO_LEN 10 <nl> 
static int object_create ( QemuOpts * opts , void * opaque ) <nl>  <nl> obj = object_new ( type ); <nl> if ( qemu_opt_foreach ( opts , object_set_property , obj , 1 ) < 0 ) { <nl> + object_unref ( obj ); <nl> return - 1 ; <nl> } <nl>  <nl> object_property_add_child ( container_get ( object_get_root (), "/ objects "), <nl> id , obj , NULL ); <nl> - <nl> + object_unref ( obj ); <nl> return 0 ; <nl> } <nl> 
static ssize_t qcow2_crypto_hdr_init_func ( QCryptoBlock * block , size_t headerlen , <nl> /* Zero fill remaining space in cluster so it has predictable <nl> * content in case of future spec changes */ <nl> clusterlen = size_to_clusters ( s , headerlen ) * s -> cluster_size ; <nl> + assert ( qcow2_pre_write_overlap_check ( bs , 0 , ret , clusterlen ) == 0 ); <nl> ret = bdrv_pwrite_zeroes ( bs -> file , <nl> ret + headerlen , <nl> clusterlen - headerlen , 0 );
static int iothread_stop ( Object * object , void * opaque ) <nl> IOThread * iothread ; <nl>  <nl> iothread = ( IOThread *) object_dynamic_cast ( object , TYPE_IOTHREAD ); <nl> - if (! iothread || ! iothread -> ctx ) { <nl> + if (! iothread || ! iothread -> ctx || iothread -> stopping ) { <nl> return 0 ; <nl> } <nl> iothread -> stopping = true ;
static Property vhost_scsi_properties [] = { <nl> DEFINE_PROP_STRING (" wwpn ", VirtIOSCSICommon , conf . wwpn ), <nl> DEFINE_PROP_UINT32 (" boot_tpgt ", VirtIOSCSICommon , conf . boot_tpgt , 0 ), <nl> DEFINE_PROP_UINT32 (" num_queues ", VirtIOSCSICommon , conf . num_queues , 1 ), <nl> + DEFINE_PROP_UINT32 (" virtqueue_size ", VirtIOSCSICommon , conf . virtqueue_size , <nl> + 128 ), <nl> DEFINE_PROP_UINT32 (" max_sectors ", VirtIOSCSICommon , conf . max_sectors , <nl> 0xFFFF ), <nl> DEFINE_PROP_UINT32 (" cmd_per_lun ", VirtIOSCSICommon , conf . cmd_per_lun , 128 ),
static void set_cfg_value ( bool is_max , int index , int value ) <nl> { <nl> if ( is_max ) { <nl> cfg . buckets [ index ]. max = value ; <nl> + /* If max is set , avg should never be 0 */ <nl> + cfg . buckets [ index ]. avg = MAX ( cfg . buckets [ index ]. avg , 1 ); <nl> } else { <nl> cfg . buckets [ index ]. avg = value ; <nl> }
udp_listen ( Slirp * slirp , uint32_t haddr , u_int hport , uint32_t laddr , <nl> return NULL ; <nl> } <nl> so -> s = qemu_socket ( AF_INET , SOCK_DGRAM , 0 ); <nl> + if ( so -> s < 0 ) { <nl> + sofree ( so ); <nl> + return NULL ; <nl> + } <nl> so -> so_expire = curtime + SO_EXPIRE ; <nl> insque ( so , & slirp -> udb ); <nl> 
static int get_real_id ( const char * devpath , const char * idname , uint16_t * val ) <nl> if ( fscanf ( f , "% li \ n ", & id ) == 1 ) { <nl> * val = id ; <nl> } else { <nl> + fclose ( f ); <nl> return - 1 ; <nl> } <nl> fclose ( f );
static inline int thunk_type_size ( const argtype * type_ptr , int is_host ) <nl> defined ( HOST_PARISC ) || defined ( HOST_SPARC64 ) <nl> return 4 ; <nl> # elif defined ( HOST_PPC ) <nl> - return TARGET_ABI_BITS / 8 ; <nl> + return sizeof ( void *); <nl> # else <nl> return 2 ; <nl> # endif
static void uart_write ( void * opaque , hwaddr offset , <nl>  <nl> DB_PRINT (" offset :% x data :% 08x \ n ", ( unsigned ) offset , ( unsigned ) value ); <nl> offset >>= 2 ; <nl> + if ( offset >= CADENCE_UART_R_MAX ) { <nl> + return ; <nl> + } <nl> switch ( offset ) { <nl> case R_IER : /* ier ( wts imr ) */ <nl> s -> r [ R_IMR ] |= value ;
static ssize_t gem_receive ( VLANClientState * nc , const uint8_t * buf , size_t size ) <nl> */ <nl>  <nl> memcpy ( rxbuf , buf , size ); <nl> - memset ( rxbuf + size , 0 , sizeof ( rxbuf - size )); <nl> + memset ( rxbuf + size , 0 , sizeof ( rxbuf ) - size ); <nl> rxbuf_ptr = rxbuf ; <nl> crc_val = cpu_to_le32 ( crc32 ( 0 , rxbuf , MAX ( size , 60 ))); <nl> if ( size < 60 ) {
static void acpi_get_pm_info ( AcpiPmInfo * pm ) <nl> Object * obj = NULL ; <nl> QObject * o ; <nl>  <nl> + pm -> cpu_hp_io_base = 0 ; <nl> pm -> pcihp_io_base = 0 ; <nl> pm -> pcihp_io_len = 0 ; <nl> if ( piix ) {
static uint64_t arm_ldq_ptw ( CPUState * cs , hwaddr addr , bool is_secure , <nl> MemTxAttrs attrs = {}; <nl> MemTxResult result = MEMTX_OK ; <nl> AddressSpace * as ; <nl> - uint32_t data ; <nl> + uint64_t data ; <nl>  <nl> attrs . secure = is_secure ; <nl> as = arm_addressspace ( cs , attrs );
static GSList * gd_vc_gfx_init ( GtkDisplayState * s , VirtualConsole * vc , <nl> QemuConsole * con , int idx , <nl> GSList * group , GtkWidget * view_menu ) <nl> { <nl> - Error * local_err = NULL ; <nl> Object * obj ; <nl>  <nl> - obj = object_property_get_link ( OBJECT ( con ), " device ", & local_err ); <nl> + obj = object_property_get_link ( OBJECT ( con ), " device ", NULL ); <nl> if ( obj ) { <nl> vc -> label = g_strdup_printf ("% s ", object_get_typename ( obj )); <nl> } else {
int main ( int argc , char ** argv ) <nl> qtest_add_func ("/ fdc / media_change ", test_media_change ); <nl> qtest_add_func ("/ fdc / sense_interrupt ", test_sense_interrupt ); <nl> qtest_add_func ("/ fdc / relative_seek ", test_relative_seek ); <nl> + qtest_add_func ("/ fdc / media_insert ", test_media_insert ); <nl> qtest_add_func ("/ fdc / fuzz - registers ", fuzz_registers ); <nl>  <nl> ret = g_test_run ();
static void vhost_iommu_region_add ( MemoryListener * listener , <nl> struct vhost_iommu * iommu ; <nl> Int128 end ; <nl> int iommu_idx ; <nl> - IOMMUMemoryRegion * iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + IOMMUMemoryRegion * iommu_mr ; <nl>  <nl> if (! memory_region_is_iommu ( section -> mr )) { <nl> return ; <nl> } <nl>  <nl> + iommu_mr = IOMMU_MEMORY_REGION ( section -> mr ); <nl> + <nl> iommu = g_malloc0 ( sizeof (* iommu )); <nl> end = int128_add ( int128_make64 ( section -> offset_within_region ), <nl> section -> size );
void qemu_ram_remap ( ram_addr_t addr , ram_addr_t length ) <nl> abort (); <nl> } else { <nl> flags = MAP_FIXED ; <nl> - munmap ( vaddr , length ); <nl> if ( block -> fd >= 0 ) { <nl> flags |= ( block -> flags & RAM_SHARED ? <nl> MAP_SHARED : MAP_PRIVATE );
Error * error_copy ( const Error * err ) <nl> err_new = g_malloc0 ( sizeof (* err )); <nl> err_new -> msg = g_strdup ( err -> msg ); <nl> err_new -> err_class = err -> err_class ; <nl> + err_new -> src = err -> src ; <nl> + err_new -> line = err -> line ; <nl> + err_new -> func = err -> func ; <nl> if ( err -> hint ) { <nl> err_new -> hint = g_string_new ( err -> hint -> str ); <nl> }
int qemu_chr_fe_write ( CharDriverState * s , const uint8_t * buf , int len ) <nl> int qemu_chr_fe_write_all ( CharDriverState * s , const uint8_t * buf , int len ) <nl> { <nl> int offset = 0 ; <nl> - int res ; <nl> + int res = 0 ; <nl>  <nl> qemu_mutex_lock (& s -> chr_write_lock ); <nl> while ( offset < len ) {
static void iothread_instance_finalize ( Object * obj ) <nl> iothread_stop ( obj , NULL ); <nl> qemu_cond_destroy (& iothread -> init_done_cond ); <nl> qemu_mutex_destroy (& iothread -> init_done_lock ); <nl> + if (! iothread -> ctx ) { <nl> + return ; <nl> + } <nl> aio_context_unref ( iothread -> ctx ); <nl> } <nl> 
static gboolean ga_channel_open ( GAChannel * c , const gchar * path , GAChannelMethod <nl> ret = ga_channel_client_add ( c , fd ); <nl> if ( ret ) { <nl> g_critical (" error adding channel to main loop "); <nl> + close ( fd ); <nl> return false ; <nl> } <nl> break ;
void do_device_add ( Monitor * mon , const QDict * qdict ) <nl>  <nl> opts = qemu_opts_parse (& qemu_device_opts , <nl> qdict_get_str ( qdict , " config "), " driver "); <nl> - if ( opts && ! qdev_device_help ( opts )) <nl> - qdev_device_add ( opts ); <nl> + if ( opts ) { <nl> + if ( qdev_device_help ( opts ) || qdev_device_add ( opts ) == NULL ) { <nl> + qemu_opts_del ( opts ); <nl> + } <nl> + } <nl> } <nl>  <nl> void do_device_del ( Monitor * mon , const QDict * qdict )
static int xen_pt_bar_reg_write ( XenPCIPassthroughState * s , XenPTReg * cfg_entry , <nl> bar_ro_mask = XEN_PT_BAR_IO_RO_MASK | ( r_size - 1 ); <nl> break ; <nl> case XEN_PT_BAR_FLAG_UPPER : <nl> + assert ( index > 0 ); <nl> + r_size = d -> io_regions [ index - 1 ]. size >> 32 ; <nl> bar_emu_mask = XEN_PT_BAR_ALLF ; <nl> bar_ro_mask = r_size ? r_size - 1 : 0 ; <nl> break ;
int qdev_unplug ( DeviceState * dev ) <nl> dev -> parent_bus -> name ); <nl> return - 1 ; <nl> } <nl> + assert ( dev -> info -> unplug != NULL ); <nl> + <nl> return dev -> info -> unplug ( dev ); <nl> } <nl> 
USBPacket * usb_ep_find_packet_by_id ( USBDevice * dev , int pid , int ep , <nl> struct USBEndpoint * uep = usb_ep_get ( dev , pid , ep ); <nl> USBPacket * p ; <nl>  <nl> - while (( p = QTAILQ_FIRST (& uep -> queue )) != NULL ) { <nl> + QTAILQ_FOREACH ( p , & uep -> queue , queue ) { <nl> if ( p -> id == id ) { <nl> return p ; <nl> }
static int img_convert ( int argc , char ** argv ) <nl>  <nl> if ( options && ! strcmp ( options , "?")) { <nl> print_option_help ( drv -> create_options ); <nl> + free ( bs ); <nl> return 0 ; <nl> } <nl> 
DeviceState * qdev_try_create ( BusState * bus , const char * name ) <nl> { <nl> DeviceState * dev ; <nl>  <nl> + if ( object_class_by_name ( name ) == NULL ) { <nl> + return NULL ; <nl> + } <nl> dev = DEVICE ( object_new ( name )); <nl> if (! dev ) { <nl> return NULL ;
static void vmdk_free_last_extent ( BlockDriverState * bs ) <nl> static uint32_t vmdk_read_cid ( BlockDriverState * bs , int parent ) <nl> { <nl> char desc [ DESC_SIZE ]; <nl> - uint32_t cid ; <nl> + uint32_t cid = 0 ; <nl> const char * p_name , * cid_str ; <nl> size_t cid_str_size ; <nl> BDRVVmdkState * s = bs -> opaque ;
static inline void gdb_continue ( GDBState * s ) <nl> # ifdef CONFIG_USER_ONLY <nl> s -> running_state = 1 ; <nl> # else <nl> - vm_start (); <nl> + if ( runstate_check ( RUN_STATE_DEBUG )) { <nl> + vm_start (); <nl> + } <nl> # endif <nl> } <nl> 
static void arm_cpu_reset ( CPUState * s ) <nl> */ <nl> env -> v7m . ccr = R_V7M_CCR_STKALIGN_MASK ; <nl>  <nl> + /* Unlike A / R profile , M profile defines the reset LR value */ <nl> + env -> regs [ 14 ] = 0xffffffff ; <nl> + <nl> /* Load the initial SP and PC from the vector table at address 0 */ <nl> rom = rom_ptr ( 0 ); <nl> if ( rom ) {
int main ( int argc , char ** argv , char ** envp ) <nl> /* init remote displays */ <nl> qemu_opts_foreach ( qemu_find_opts (" vnc "), vnc_init_func , NULL , 0 ); <nl> if ( show_vnc_port ) { <nl> - printf (" VNC server running on `% s '\ n ", <nl> - vnc_display_local_addr (" default ")); <nl> + char * ret = vnc_display_local_addr (" default "); <nl> + printf (" VNC server running on `% s '\ n ", ret ); <nl> + g_free ( ret ); <nl> } <nl> # endif <nl> # ifdef CONFIG_SPICE
int msix_init ( struct PCIDevice * dev , unsigned short nentries , <nl> return 0 ; <nl>  <nl> err_config : <nl> + dev -> msix_entries_nr = 0 ; <nl> cpu_unregister_io_memory ( dev -> msix_mmio_index ); <nl> err_index : <nl> qemu_free ( dev -> msix_table_page );
qcrypto_tls_session_check_certificate ( QCryptoTLSSession * session , <nl>  <nl> allow = qemu_acl_party_is_allowed ( acl , session -> peername ); <nl>  <nl> - error_setg ( errp , " TLS x509 ACL check for % s is % s ", <nl> - session -> peername , allow ? " allowed " : " denied "); <nl> if (! allow ) { <nl> + error_setg ( errp , " TLS x509 ACL check for % s is denied ", <nl> + session -> peername ); <nl> goto error ; <nl> } <nl> }
size_t qlist_size ( const QList * qlist ) <nl> */ <nl> QList * qobject_to_qlist ( const QObject * obj ) <nl> { <nl> - if ( qobject_type ( obj ) != QTYPE_QLIST ) { <nl> + if (! obj || qobject_type ( obj ) != QTYPE_QLIST ) { <nl> return NULL ; <nl> } <nl> - <nl> return container_of ( obj , QList , base ); <nl> } <nl> 
static void kvm_apic_realize ( DeviceState * dev , Error ** errp ) <nl> { <nl> APICCommonState * s = APIC_COMMON ( dev ); <nl>  <nl> - memory_region_init_io (& s -> io_memory , NULL , & kvm_apic_io_ops , s , " kvm - apic - msi ", <nl> - APIC_SPACE_SIZE ); <nl> + memory_region_init_io (& s -> io_memory , OBJECT ( s ), & kvm_apic_io_ops , s , <nl> + " kvm - apic - msi ", APIC_SPACE_SIZE ); <nl>  <nl> if ( kvm_has_gsi_routing ()) { <nl> msi_nonbroken = true ;
int kvm_arch_init_vcpu ( CPUState * cs ) <nl> cpuid_data . cpuid . nent = cpuid_i ; <nl>  <nl> if ((( env -> cpuid_version >> 8 )& 0xF ) >= 6 <nl> - && ( env -> cpuid_features &( CPUID_MCE | CPUID_MCA )) == ( CPUID_MCE | CPUID_MCA ) <nl> + && ( env -> cpuid_features & ( CPUID_MCE | CPUID_MCA )) == <nl> + ( CPUID_MCE | CPUID_MCA ) <nl> && kvm_check_extension ( cs -> kvm_state , KVM_CAP_MCE ) > 0 ) { <nl> uint64_t mcg_cap ; <nl> int banks ;
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl> return 0 ; <nl>  <nl> fail : <nl> + if ( snapshots_offset > 0 ) { <nl> + qcow2_free_clusters ( bs , snapshots_offset , snapshots_size , <nl> + QCOW2_DISCARD_ALWAYS ); <nl> + } <nl> return ret ; <nl> } <nl> 
static int qemu_event_init ( void ) <nl>  <nl> static void qemu_event_increment ( void ) <nl> { <nl> - SetEvent ( qemu_event_handle ); <nl> + if (! SetEvent ( qemu_event_handle )) { <nl> + fprintf ( stderr , " qemu_event_increment : SetEvent failed : % d \ n ", <nl> + GetLastError ()); <nl> + exit ( 1 ); <nl> + } <nl> } <nl> # endif <nl> 
static int oss_open ( int in , struct oss_params * req , <nl> goto err ; <nl> } <nl>  <nl> + if (! abinfo . fragstotal || ! abinfo . fragsize ) { <nl> + AUD_log ( AUDIO_CAP , " Returned bogus buffer information (% d , % d ) for % s \ n ", <nl> + abinfo . fragstotal , abinfo . fragsize , typ ); <nl> + goto err ; <nl> + } <nl> + <nl> obt -> fmt = fmt ; <nl> obt -> nchannels = nchannels ; <nl> obt -> freq = freq ;
static int alloc_refcount_block ( BlockDriverState * bs , <nl> uint64_t last_table_size ; <nl> uint64_t blocks_clusters ; <nl> do { <nl> - uint64_t table_clusters = size_to_clusters ( s , table_size ); <nl> + uint64_t table_clusters = <nl> + size_to_clusters ( s , table_size * sizeof ( uint64_t )); <nl> blocks_clusters = 1 + <nl> (( table_clusters + refcount_block_clusters - 1 ) <nl> / refcount_block_clusters );
static void xenfb_handle_events ( struct XenFB * xenfb ) <nl>  <nl> prod = page -> out_prod ; <nl> out_cons = page -> out_cons ; <nl> - if ( prod - out_cons >= XENFB_OUT_RING_LEN ) { <nl> + if ( prod - out_cons > XENFB_OUT_RING_LEN ) { <nl> return ; <nl> } <nl> xen_rmb (); /* ensure we see ring contents up to prod */
DriveInfo * drive_init ( QemuOpts * opts , int default_to_scsi , int * fatal_error ) <nl> dinfo -> on_write_error = on_write_error ; <nl> dinfo -> opts = opts ; <nl> if ( serial ) <nl> - strncpy ( dinfo -> serial , serial , sizeof ( serial )); <nl> + strncpy ( dinfo -> serial , serial , sizeof ( dinfo -> serial ) - 1 ); <nl> QTAILQ_INSERT_TAIL (& drives , dinfo , next ); <nl>  <nl> switch ( type ) {
static void vmmouse_reset ( DeviceState * d ) <nl>  <nl> s -> status = 0xffff ; <nl> s -> queue_size = VMMOUSE_QUEUE_SIZE ; <nl> + <nl> + vmmouse_disable ( s ); <nl> } <nl>  <nl> static int vmmouse_initfn ( ISADevice * dev )
static int connect_namedsocket ( const char * path ) <nl> size = strlen ( helper . sun_path ) + sizeof ( helper . sun_family ); <nl> if ( connect ( sockfd , ( struct sockaddr *)& helper , size ) < 0 ) { <nl> fprintf ( stderr , " socket error \ n "); <nl> + close ( sockfd ); <nl> return - 1 ; <nl> } <nl> 
GEN_HANDLER ( dcbtst , 0x1F , 0x16 , 0x07 , 0x03E00001 , PPC_CACHE ) <nl> # define op_dcbz () (* gen_op_dcbz [ ctx -> mem_idx ])() <nl> static GenOpFunc * gen_op_dcbz [] = { <nl> & gen_op_dcbz_user , <nl> + & gen_op_dcbz_user , <nl> + & gen_op_dcbz_kernel , <nl> & gen_op_dcbz_kernel , <nl> }; <nl> # endif
static QString * read_line ( FILE * file , char * key ) <nl> { <nl> char value [ 128 ]; <nl>  <nl> - if ( fscanf ( file , "% s % s ", key , value ) == EOF ) <nl> + if ( fscanf ( file , "% 127s % 127s ", key , value ) == EOF ) { <nl> return NULL ; <nl> + } <nl> remove_dots ( key ); <nl> return qstring_from_str ( value ); <nl> }
static int send_sub_rect_nojpeg ( VncState * vs , int x , int y , int w , int h , <nl> ret = send_mono_rect ( vs , x , y , w , h , bg , fg ); <nl> } else if ( colors <= 256 ) { <nl> ret = send_palette_rect ( vs , x , y , w , h , palette ); <nl> + } else { <nl> + ret = 0 ; <nl> } <nl> return ret ; <nl> }
 <nl> static inline bool virtio_access_is_big_endian ( VirtIODevice * vdev ) <nl> { <nl> +# if defined ( TARGET_IS_BIENDIAN ) <nl> + return virtio_is_big_endian ( vdev ); <nl> +# elif defined ( TARGET_WORDS_BIGENDIAN ) <nl> if ( virtio_vdev_has_feature ( vdev , VIRTIO_F_VERSION_1 )) { <nl> /* Devices conforming to VIRTIO 1 . 0 or later are always LE . */ <nl> return false ; <nl> } <nl> -# if defined ( TARGET_IS_BIENDIAN ) <nl> - return virtio_is_big_endian ( vdev ); <nl> -# elif defined ( TARGET_WORDS_BIGENDIAN ) <nl> return true ; <nl> # else <nl> return false ;
static void tcx_realizefn ( DeviceState * dev , Error ** errp ) <nl> if ( fcode_filename ) { <nl> ret = load_image_targphys ( fcode_filename , s -> prom_addr , <nl> FCODE_MAX_ROM_SIZE ); <nl> + g_free ( fcode_filename ); <nl> if ( ret < 0 || ret > FCODE_MAX_ROM_SIZE ) { <nl> error_report (" tcx : could not load prom '% s '", TCX_ROM_FILE ); <nl> }
static void ppc_spapr_init ( QEMUMachineInitArgs * args ) <nl> NULL , & lowaddr , NULL , 0 , ELF_MACHINE , 0 ); <nl> kernel_le = kernel_size > 0 ; <nl> } <nl> - if ( kernel_size < 0 ) { <nl> - kernel_size = load_image_targphys ( kernel_filename , <nl> - KERNEL_LOAD_ADDR , <nl> - load_limit - KERNEL_LOAD_ADDR ); <nl> - } <nl> if ( kernel_size < 0 ) { <nl> fprintf ( stderr , " qemu : could not load kernel '% s '\ n ", <nl> kernel_filename );
uint64_t pc_dimm_get_free_addr ( uint64_t address_space_start , <nl> uint64_t address_space_end = address_space_start + address_space_size ; <nl>  <nl> g_assert ( QEMU_ALIGN_UP ( address_space_start , align ) == address_space_start ); <nl> - g_assert ( QEMU_ALIGN_UP ( address_space_size , align ) == address_space_size ); <nl>  <nl> if (! address_space_size ) { <nl> error_setg ( errp , " memory hotplug is not enabled , "
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> s -> current_mapping -> path = buffer ; <nl> s -> current_mapping -> read_only = <nl> ( st . st_mode & ( S_IWUSR | S_IWGRP | S_IWOTH )) == 0 ; <nl> - } <nl> + } else { <nl> + g_free ( buffer ); <nl> + } <nl> } <nl> closedir ( dir ); <nl> 
void console_select ( unsigned int index ) <nl> if ( s ) { <nl> DisplayState * ds = s -> ds ; <nl>  <nl> - if ( active_console -> cursor_timer ) { <nl> + if ( active_console && active_console -> cursor_timer ) { <nl> qemu_del_timer ( active_console -> cursor_timer ); <nl> } <nl> active_console = s ;
aio_read_f ( int argc , char ** argv ) <nl> case ' P ': <nl> ctx -> Pflag = 1 ; <nl> ctx -> pattern = parse_pattern ( optarg ); <nl> - if ( ctx -> pattern < 0 ) <nl> + if ( ctx -> pattern < 0 ) { <nl> + free ( ctx ); <nl> return 0 ; <nl> + } <nl> break ; <nl> case ' q ': <nl> ctx -> qflag = 1 ;
static void tcg_reg_alloc_mov ( TCGContext * s , const TCGOpDef * def , <nl> } <nl> ots -> val_type = TEMP_VAL_CONST ; <nl> ots -> val = ts -> val ; <nl> + if ( IS_DEAD_ARG ( 1 )) { <nl> + temp_dead ( s , args [ 1 ]); <nl> + } <nl> } else { <nl> /* The code in the first if block should have moved the <nl> temp to a register . */
* THE SOFTWARE . <nl> */ <nl>  <nl> -# include " sysemu / sysemu . h " <nl> -# include " monitor / monitor . h " <nl> -# include " ui / console . h " <nl> - <nl> -# include " hw / hw . h " <nl> - <nl> +# include " qemu / main - loop . h " <nl> # include " qemu / timer . h " <nl> + <nl> # ifdef CONFIG_POSIX <nl> # include < pthread . h > <nl> # endif
static void x86_cpu_apic_create ( X86CPU * cpu , Error ** errp ) <nl>  <nl> object_property_add_child ( OBJECT ( cpu ), " lapic ", <nl> OBJECT ( cpu -> apic_state ), & error_abort ); <nl> + object_unref ( OBJECT ( cpu -> apic_state )); <nl>  <nl> qdev_prop_set_uint8 ( cpu -> apic_state , " id ", cpu -> apic_id ); <nl> /* TODO : convert to link <> */
static int blkverify_open ( BlockDriverState * bs , QDict * options , int flags , <nl>  <nl> ret = 0 ; <nl> fail : <nl> + qemu_opts_del ( opts ); <nl> return ret ; <nl> } <nl> 
static int ioreq_map ( struct ioreq * ioreq ) <nl> xen_be_printf (& ioreq -> blkdev -> xendev , 0 , <nl> " can ' t map grant ref % d (% s , % d maps )\ n ", <nl> refs [ i ], strerror ( errno ), ioreq -> blkdev -> cnt_map ); <nl> + ioreq -> mapped = 1 ; <nl> ioreq_unmap ( ioreq ); <nl> return - 1 ; <nl> }
void s390_cpu_do_interrupt ( CPUState * cs ) <nl> do_mchk_interrupt ( env ); <nl> break ; <nl> } <nl> + <nl> + /* WAIT PSW during interrupt injection */ <nl> + if ( cs -> exception_index == EXCP_HLT ) { <nl> + /* don ' t trigger a cpu_loop_exit (), use an interrupt instead */ <nl> + cpu_interrupt ( CPU ( cpu ), CPU_INTERRUPT_HALT ); <nl> + } <nl> cs -> exception_index = - 1 ; <nl>  <nl> /* we might still have pending interrupts , but not deliverable */
static void stm32f2xx_timer_write ( void * opaque , hwaddr offset , <nl> return ; <nl> case TIM_PSC : <nl> timer_val = stm32f2xx_ns_to_ticks ( s , now ) - s -> tick_offset ; <nl> - s -> tim_psc = value ; <nl> + s -> tim_psc = value & 0xFFFF ; <nl> value = timer_val ; <nl> break ; <nl> case TIM_CNT :
char * object_property_get_str ( Object * obj , const char * name , <nl> void object_property_set_link ( Object * obj , Object * value , <nl> const char * name , Error ** errp ) <nl> { <nl> - object_property_set_str ( obj , object_get_canonical_path ( value ), <nl> - name , errp ); <nl> + gchar * path = object_get_canonical_path ( value ); <nl> + object_property_set_str ( obj , path , name , errp ); <nl> + g_free ( path ); <nl> } <nl>  <nl> Object * object_property_get_link ( Object * obj , const char * name ,
static void spapr_phb_realize ( DeviceState * dev , Error ** errp ) <nl> } <nl> # endif <nl>  <nl> - memory_region_init_io (& sphb -> msiwindow , NULL , & spapr_msi_ops , spapr , <nl> + memory_region_init_io (& sphb -> msiwindow , OBJECT ( sphb ), & spapr_msi_ops , spapr , <nl> " msi ", msi_window_size ); <nl> memory_region_add_subregion (& sphb -> iommu_root , SPAPR_PCI_MSI_WINDOW , <nl> & sphb -> msiwindow );
int cpu_get_dump_info ( ArchDumpInfo * info , <nl> } else { <nl> info -> d_endian = ELFDATA2LSB ; <nl> } <nl> + /* 64KB is the max page size for pseries kernel */ <nl> + if ( strncmp ( object_get_typename ( qdev_get_machine ()), <nl> + " pseries -", 8 ) == 0 ) { <nl> + info -> page_size = ( 1U << 16 ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static int find_and_clear_dirty_height ( VncState * vs , <nl> static int vnc_update_client ( VncState * vs , int has_dirty , bool sync ) <nl> { <nl> vs -> has_dirty += has_dirty ; <nl> - if ( vs -> need_update && vs -> ioc != NULL ) { <nl> + if ( vs -> need_update && ! vs -> disconnecting ) { <nl> VncDisplay * vd = vs -> vd ; <nl> VncJob * job ; <nl> int y ;
void cpu_physical_memory_write_rom ( target_phys_addr_t addr , <nl> /* ROM / RAM case */ <nl> ptr = qemu_get_ram_ptr ( addr1 ); <nl> memcpy ( ptr , buf , l ); <nl> + if (! cpu_physical_memory_is_dirty ( addr1 )) { <nl> + /* invalidate code */ <nl> + tb_invalidate_phys_page_range ( addr1 , addr1 + l , 0 ); <nl> + /* set dirty bit */ <nl> + cpu_physical_memory_set_dirty_flags ( <nl> + addr1 , ( 0xff & ~ CODE_DIRTY_FLAG )); <nl> + } <nl> qemu_put_ram_ptr ( ptr ); <nl> } <nl> len -= l ;
static void taihu_405ep_init ( ram_addr_t ram_size , <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> if ( filename ) { <nl> bios_size = load_image ( filename , qemu_get_ram_ptr ( bios_offset )); <nl> + qemu_free ( filename ); <nl> } else { <nl> bios_size = - 1 ; <nl> }
static void blk_delete ( BlockBackend * blk ) <nl> assert (! blk -> refcnt ); <nl> assert (! blk -> name ); <nl> assert (! blk -> dev ); <nl> + if ( blk -> public . throttle_state ) { <nl> + blk_io_limits_disable ( blk ); <nl> + } <nl> if ( blk -> root ) { <nl> blk_remove_bs ( blk ); <nl> }
static GIOStatus ga_channel_write ( GAChannel * c , const char * buf , size_t size , <nl> GIOStatus ga_channel_write_all ( GAChannel * c , const char * buf , size_t size ) <nl> { <nl> GIOStatus status = G_IO_STATUS_NORMAL ; <nl> - size_t count ; <nl> + size_t count = 0 ; <nl>  <nl> while ( size ) { <nl> status = ga_channel_write ( c , buf , size , & count );
static int htab_load ( QEMUFile * f , void * opaque , int version_id ) <nl> section_hdr = qemu_get_be32 ( f ); <nl>  <nl> if ( section_hdr ) { <nl> - Error * local_err ; <nl> + Error * local_err = NULL ; <nl>  <nl> /* First section gives the htab size */ <nl> spapr_reallocate_hpt ( spapr , section_hdr , & local_err );
void qmp_drive_mirror ( const char * device , const char * target , <nl> return ; <nl> } <nl>  <nl> - if ( sync == MIRROR_SYNC_MODE_FULL && mode != NEW_IMAGE_MODE_EXISTING ) { <nl> + if (( sync == MIRROR_SYNC_MODE_FULL || ! source ) <nl> + && mode != NEW_IMAGE_MODE_EXISTING ) <nl> + { <nl> /* create new image w / o backing file */ <nl> assert ( format && drv ); <nl> bdrv_img_create ( target , format ,
tcp_listen ( Slirp * slirp , u_int32_t haddr , u_int hport , u_int32_t laddr , <nl> struct socket * so ; <nl> int s , opt = 1 ; <nl> socklen_t addrlen = sizeof ( addr ); <nl> + memset (& addr , 0 , addrlen ); <nl>  <nl> DEBUG_CALL (" tcp_listen "); <nl> DEBUG_ARG (" haddr = % x ", haddr );
reply_maybe_async : <nl> reply_async -> IOCLogInfo = count ; <nl> return ; <nl> } <nl> + g_free ( reply_async ); <nl> reply . TerminationCount = count ; <nl> break ; <nl> 
static int qcrypto_ivgen_essiv_calculate ( QCryptoIVGen * ivgen , <nl> uint8_t * data = g_new ( uint8_t , ndata ); <nl>  <nl> sector = cpu_to_le64 ( sector ); <nl> - memcpy ( data , ( uint8_t *)& sector , ndata ); <nl> + memcpy ( data , ( uint8_t *)& sector , MIN ( sizeof ( sector ), ndata )); <nl> if ( sizeof ( sector ) < ndata ) { <nl> memset ( data + sizeof ( sector ), 0 , ndata - sizeof ( sector )); <nl> }
int qemu_opts_foreach ( QemuOptsList * list , qemu_opts_loopfunc func , void * opaque , <nl> int rc = 0 ; <nl>  <nl> QTAILQ_FOREACH ( opts , & list -> head , next ) { <nl> - rc = func ( opts , opaque ); <nl> + rc |= func ( opts , opaque ); <nl> if ( abort_on_failure && rc != 0 ) <nl> break ; <nl> }
static void nbd_refresh_filename ( BlockDriverState * bs , QDict * options ) <nl> ov = qobject_output_visitor_new (& saddr_qdict ); <nl> visit_type_SocketAddress ( ov , NULL , & s -> saddr , & error_abort ); <nl> visit_complete ( ov , & saddr_qdict ); <nl> + visit_free ( ov ); <nl> assert ( qobject_type ( saddr_qdict ) == QTYPE_QDICT ); <nl>  <nl> qdict_put_obj ( opts , " server ", saddr_qdict );
static void process_event ( JSONMessageParser * parser , QList * tokens ) <nl> error_free ( err ); <nl> } <nl> ret = send_response ( s , QOBJECT ( qdict )); <nl> - if ( ret ) { <nl> - g_warning (" error sending error response : % s ", strerror ( ret )); <nl> + if ( ret < 0 ) { <nl> + g_warning (" error sending error response : % s ", strerror (- ret )); <nl> } <nl> } <nl> 
static CharDriverState * gd_vc_handler ( ChardevVC * vc , Error ** errp ) <nl> chr -> chr_set_echo = gd_vc_chr_set_echo ; <nl>  <nl> /* Temporary , until gd_vc_vte_init runs . */ <nl> - chr -> opaque = g_new ( VirtualConsole , 1 ); <nl> + chr -> opaque = g_new0 ( VirtualConsole , 1 ); <nl>  <nl> /* defer OPENED events until our vc is fully initialized */ <nl> chr -> explicit_be_open = true ;
static void win32_aio_process_completion ( QEMUWin32AIOState * s , <nl> memcpy ( qiov -> iov [ i ]. iov_base , p , qiov -> iov [ i ]. iov_len ); <nl> p += qiov -> iov [ i ]. iov_len ; <nl> } <nl> - qemu_vfree ( waiocb -> buf ); <nl> } <nl> + qemu_vfree ( waiocb -> buf ); <nl> } <nl>  <nl> 
static void sch_handle_start_func ( SubchDev * sch , ORB * orb ) <nl> path = 0x80 ; <nl>  <nl> if (!( s -> ctrl & SCSW_ACTL_SUSP )) { <nl> + s -> cstat = 0 ; <nl> + s -> dstat = 0 ; <nl> /* Look at the orb and try to execute the channel program . */ <nl> assert ( orb != NULL ); /* resume does not pass an orb */ <nl> p -> intparm = orb -> intparm ;
static int vscsi_srp_direct_data ( VSCSIState * s , vscsi_req * req , <nl> { <nl> struct srp_direct_buf * md = req -> cur_desc ; <nl> uint32_t llen ; <nl> - int rc ; <nl> + int rc = 0 ; <nl>  <nl> dprintf (" VSCSI : direct segment 0x % x bytes , va = 0x % llx desc len = 0x % x \ n ", <nl> len , ( unsigned long long ) md -> va , md -> len );
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> - g_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
SCSIRequest * scsi_req_ref ( SCSIRequest * req ) <nl>  <nl> void scsi_req_unref ( SCSIRequest * req ) <nl> { <nl> + assert ( req -> refcount > 0 ); <nl> if (-- req -> refcount == 0 ) { <nl> if ( req -> ops -> free_req ) { <nl> req -> ops -> free_req ( req );
static QemuOptsList parallels_runtime_opts = { <nl> . name = PARALLELS_OPT_PREALLOC_SIZE , <nl> . type = QEMU_OPT_SIZE , <nl> . help = " Preallocation size on image expansion ", <nl> - . def_value_str = " 128MiB ", <nl> + . def_value_str = " 128M ", <nl> }, <nl> { <nl> . name = PARALLELS_OPT_PREALLOC_MODE ,
static int sd_create ( const char * filename , QemuOpts * opts , <nl> bdrv_unref ( bs ); <nl> } <nl>  <nl> + s -> aio_context = qemu_get_aio_context (); <nl> ret = do_sd_create ( s , & vid , 0 , errp ); <nl> if ( ret ) { <nl> goto out ;
BlockDirtyInfoList * bdrv_query_dirty_bitmaps ( BlockDriverState * bs ) <nl> QLIST_FOREACH ( bm , & bs -> dirty_bitmaps , list ) { <nl> BlockDirtyInfo * info = g_new0 ( BlockDirtyInfo , 1 ); <nl> BlockDirtyInfoList * entry = g_new0 ( BlockDirtyInfoList , 1 ); <nl> - info -> count = bdrv_get_dirty_count ( bm ); <nl> + info -> count = bdrv_get_dirty_count ( bm ) << BDRV_SECTOR_BITS ; <nl> info -> granularity = bdrv_dirty_bitmap_granularity ( bm ); <nl> info -> has_name = !! bm -> name ; <nl> info -> name = g_strdup ( bm -> name );
static void glue ( audio_pcm_hw_gc_ , TYPE ) ( HW ** hwp ) <nl> audio_detach_capture ( hw ); <nl> # endif <nl> QLIST_REMOVE ( hw , entries ); <nl> + glue ( hw -> pcm_ops -> fini_ , TYPE ) ( hw ); <nl> glue ( s -> nb_hw_voices_ , TYPE ) += 1 ; <nl> glue ( audio_pcm_hw_free_resources_ , TYPE ) ( hw ); <nl> - glue ( hw -> pcm_ops -> fini_ , TYPE ) ( hw ); <nl> g_free ( hw ); <nl> * hwp = NULL ; <nl> }
static void drive_uninit ( DriveInfo * dinfo ) <nl> { <nl> qemu_opts_del ( dinfo -> opts ); <nl> bdrv_delete ( dinfo -> bdrv ); <nl> + qemu_free ( dinfo -> id ); <nl> QTAILQ_REMOVE (& drives , dinfo , next ); <nl> qemu_free ( dinfo ); <nl> }
static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl> if ( ret < 0 ) { <nl> error_report (" failed to create inode for snapshot : % s ", <nl> error_get_pretty ( local_err )); <nl> + error_free ( local_err ); <nl> goto cleanup ; <nl> } <nl> 
static void fsl_imx25_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx25_realize ; <nl> - <nl> dc -> desc = " i . MX25 SOC "; <nl> + /* <nl> + * Reason : uses serial_hds in realize and the imx25 board does not <nl> + * support multiple CPUs <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx25_type_info = {
VHostNetState * get_vhost_net ( NetClientState * nc ) <nl> int vhost_set_vring_enable ( NetClientState * nc , int enable ) <nl> { <nl> VHostNetState * net = get_vhost_net ( nc ); <nl> - const VhostOps * vhost_ops = net -> dev . vhost_ops ; <nl> + const VhostOps * vhost_ops ; <nl> + <nl> + if (! net ) { <nl> + return 0 ; <nl> + } <nl>  <nl> + vhost_ops = net -> dev . vhost_ops ; <nl> if ( vhost_ops -> vhost_set_vring_enable ) { <nl> return vhost_ops -> vhost_set_vring_enable (& net -> dev , enable ); <nl> }
void mips_malta_init ( MachineState * machine ) <nl> /* Board ID = 0x420 ( Malta Board with CoreLV ) */ <nl> stl_p ( memory_region_get_ram_ptr ( bios_copy ) + 0x10 , 0x00000420 ); <nl>  <nl> - /* Init internal devices */ <nl> - cpu_mips_irq_init_cpu ( env ); <nl> - cpu_mips_clock_init ( env ); <nl> - <nl> /* <nl> * We have a circular dependency problem : pci_bus depends on isa_irq , <nl> * isa_irq is provided by i8259 , i8259 depends on ISA , ISA depends
int glue ( load_elf , SZ )( int fd , int64_t virt_to_phys_addend , <nl> data = NULL ; <nl> } <nl> } <nl> + qemu_free ( phdr ); <nl> return total_size ; <nl> fail : <nl> qemu_free ( data );
# ifndef BITMAP_H <nl> # define BITMAP_H <nl>  <nl> -# include " qemu - common . h " <nl> +# include < glib . h > <nl> +# include < string . h > <nl> +# include < stdlib . h > <nl> + <nl> +# include " qemu / osdep . h " <nl> # include " qemu / bitops . h " <nl>  <nl> /*
print_insn ( bfd_vma pc , disassemble_info * info ) <nl> } <nl> } <nl>  <nl> - if ( putop ( dp -> name , sizeflag ) == 0 ) <nl> + if ( dp -> name != NULL && putop ( dp -> name , sizeflag ) == 0 ) <nl> { <nl> for ( i = 0 ; i < MAX_OPERANDS ; ++ i ) <nl> {
static bool ga_open_pidfile ( const char * pidfile ) <nl> goto fail ; <nl> } <nl>  <nl> + /* keep pidfile open & locked forever */ <nl> return true ; <nl>  <nl> fail : <nl> unlink ( pidfile ); <nl> + close ( pidfd ); <nl> return false ; <nl> } <nl> # else /* _WIN32 */
int kvm_init ( void ) <nl> } while ( ret == - EINTR ); <nl>  <nl> if ( ret < 0 ) { <nl> - fprintf ( stderr , " ioctl ( KVM_CREATE_VM ) failed : % d % s \ n ", - s -> vmfd , <nl> + fprintf ( stderr , " ioctl ( KVM_CREATE_VM ) failed : % d % s \ n ", - ret , <nl> strerror (- ret )); <nl>  <nl> # ifdef TARGET_S390X
get_net_error_message ( gint error ) <nl> if ( msg != NULL ) { <nl> nchars = wcslen ( msg ); <nl>  <nl> - if ( nchars > 2 && <nl> + if ( nchars >= 2 && <nl> msg [ nchars - 1 ] == L '\ n ' && <nl> msg [ nchars - 2 ] == L '\ r ') { <nl> msg [ nchars - 2 ] = L '\ 0 ';
static void numa_add ( const char * optarg ) <nl> node_mem [ nodenr ] = 0 ; <nl> } else { <nl> int64_t sval ; <nl> - sval = strtosz ( option , NULL ); <nl> - if ( sval < 0 ) { <nl> + sval = strtosz ( option , & endptr ); <nl> + if ( sval < 0 || * endptr ) { <nl> fprintf ( stderr , " qemu : invalid numa mem size : % s \ n ", optarg ); <nl> exit ( 1 ); <nl> }
void dpy_gfx_replace_surface ( QemuConsole * con , <nl> DisplaySurface * old_surface = con -> surface ; <nl> DisplayChangeListener * dcl ; <nl>  <nl> + assert ( old_surface != surface ); <nl> + <nl> con -> surface = surface ; <nl> QLIST_FOREACH ( dcl , & s -> listeners , next ) { <nl> if ( con != ( dcl -> con ? dcl -> con : active_console )) {
static int pci_unregister_device ( DeviceState * dev ) <nl>  <nl> pci_unregister_io_regions ( pci_dev ); <nl> pci_del_option_rom ( pci_dev ); <nl> + qemu_free ( pci_dev -> romfile ); <nl> do_pci_unregister_device ( pci_dev ); <nl> return 0 ; <nl> }
static int vmxnet3_post_load ( void * opaque , int version_id ) <nl> } <nl> } <nl>  <nl> + vmxnet3_validate_queues ( s ); <nl> vmxnet3_validate_interrupts ( s ); <nl>  <nl> return 0 ;
void ide_dma_cb ( void * opaque , int ret ) <nl> sector_num , n , s -> dma_cmd ); <nl> # endif <nl>  <nl> - if (! ide_sect_range_ok ( s , sector_num , n )) { <nl> + if (( s -> dma_cmd == IDE_DMA_READ || s -> dma_cmd == IDE_DMA_WRITE ) && <nl> + ! ide_sect_range_ok ( s , sector_num , n )) { <nl> dma_buf_commit ( s ); <nl> ide_dma_error ( s ); <nl> return ;
static void test_visitor_in_fuzz ( TestInputVisitorData * data , <nl>  <nl> v = visitor_input_test_init ( data , buf ); <nl> visit_type_intList ( v , NULL , & ilres , NULL ); <nl> + qapi_free_intList ( ilres ); <nl> visitor_input_teardown ( data , NULL ); <nl>  <nl> v = visitor_input_test_init ( data , buf );
static const char * stream_video_names [] = { <nl> [ SPICE_STREAM_VIDEO_FILTER ] = " filter ", <nl> }; <nl> # define parse_stream_video ( _name ) \ <nl> - name2enum ( _name , stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl> + parse_name ( _name , " stream video control ", \ <nl> + stream_video_names , ARRAY_SIZE ( stream_video_names )) <nl>  <nl> static const char * compression_names [] = { <nl> [ SPICE_IMAGE_COMPRESS_OFF ] = " off ",
void ide_exec_cmd ( IDEBus * bus , uint32_t val ) <nl> lba48 = 1 ; <nl> /* fall through */ <nl> case WIN_READ_NATIVE_MAX : <nl> + /* Refuse if no sectors are addressable ( e . g . medium not inserted ) */ <nl> + if ( s -> nb_sectors == 0 ) { <nl> + goto abort_cmd ; <nl> + } <nl> ide_cmd_lba48_transform ( s , lba48 ); <nl> ide_set_sector ( s , s -> nb_sectors - 1 ); <nl> s -> status = READY_STAT | SEEK_STAT ;
static void curses_refresh ( DisplayChangeListener * dcl ) <nl> qemu_input_event_send_key_delay ( 0 ); <nl> } <nl> } else { <nl> - keysym = curses2qemu [ chr ]; <nl> + keysym = - 1 ; <nl> + if ( chr < CURSES_KEYS ) { <nl> + keysym = curses2qemu [ chr ]; <nl> + } <nl> if ( keysym == - 1 ) <nl> keysym = chr ; <nl> 
void cmos_set_s3_resume ( void ) <nl> } <nl>  <nl> static QEMUMachine pc_machine = { <nl> - . name = " pc ", <nl> + . name = " pc - 0 . 11 ", <nl> + . alias = " pc ", <nl> . desc = " Standard PC ", <nl> . init = pc_init_pci , <nl> . max_cpus = 255 ,
static void gen_msa ( CPUMIPSState * env , DisasContext * ctx ) <nl> case OPC_LD_H : <nl> case OPC_LD_W : <nl> case OPC_LD_D : <nl> + save_cpu_state ( ctx , 1 ); <nl> gen_helper_msa_ld_df ( cpu_env , tdf , twd , trs , ts10 ); <nl> break ; <nl> case OPC_ST_B : <nl> case OPC_ST_H : <nl> case OPC_ST_W : <nl> case OPC_ST_D : <nl> + save_cpu_state ( ctx , 1 ); <nl> gen_helper_msa_st_df ( cpu_env , tdf , twd , trs , ts10 ); <nl> break ; <nl> }
void cpu_alpha_store_fpcr ( CPUState * env , uint64_t val ) <nl> round_mode = float_round_nearest_even ; <nl> break ; <nl> case 3 : <nl> + default : /* this avoids a gcc (< 4 . 4 ) warning */ <nl> round_mode = float_round_up ; <nl> break ; <nl> }
static void ccid_handle_data ( USBDevice * dev , USBPacket * p ) <nl> " handle_data : int_in : notify_slot_change % X , " <nl> " requested len % zd \ n ", <nl> s -> bmSlotICCState , p -> iov . size ); <nl> + } else { <nl> + p -> status = USB_RET_NAK ; <nl> } <nl> break ; <nl> default :
static int kvm_physical_sync_dirty_bitmap ( MemoryRegionSection * section ) <nl> { <nl> KVMState * s = kvm_state ; <nl> unsigned long size , allocated_size = 0 ; <nl> - KVMDirtyLog d ; <nl> + KVMDirtyLog d = {}; <nl> KVMSlot * mem ; <nl> int ret = 0 ; <nl> hwaddr start_addr = section -> offset_within_address_space ;
static int target_pread ( int fd , abi_ulong ptr , abi_ulong len , <nl> int ret ; <nl>  <nl> buf = lock_user ( VERIFY_WRITE , ptr , len , 0 ); <nl> + if (! buf ) { <nl> + return - EFAULT ; <nl> + } <nl> ret = pread ( fd , buf , len , offset ); <nl> + if ( ret < 0 ) { <nl> + ret = - errno ; <nl> + } <nl> unlock_user ( buf , ptr , len ); <nl> return ret ; <nl> }
static void cpu_openrisc_load_kernel ( ram_addr_t ram_size , <nl> kernel_filename ); <nl> exit ( 1 ); <nl> } <nl> + cpu -> env . pc = entry ; <nl> } <nl> - <nl> - cpu -> env . pc = entry ; <nl> } <nl>  <nl> static void openrisc_sim_init ( QEMUMachineInitArgs * args )
void socket_listen_cleanup ( int fd , Error ** errp ) <nl> } <nl> } <nl>  <nl> - g_free ( addr ); <nl> + qapi_free_SocketAddress ( addr ); <nl> } <nl>  <nl> int socket_dgram ( SocketAddress * remote , SocketAddress * local , Error ** errp )
static void handle_keydown ( SDL_Event * ev ) <nl> case SDL_SCANCODE_7 : <nl> case SDL_SCANCODE_8 : <nl> case SDL_SCANCODE_9 : <nl> + if ( gui_grab ) { <nl> + sdl_grab_end ( scon ); <nl> + } <nl> + <nl> win = ev -> key . keysym . scancode - SDL_SCANCODE_1 ; <nl> if ( win < sdl2_num_outputs ) { <nl> sdl2_console [ win ]. hidden = ! sdl2_console [ win ]. hidden ;
static void ehci_frame_timer ( void * opaque ) <nl> int need_timer = 0 ; <nl> int64_t expire_time , t_now ; <nl> uint64_t ns_elapsed ; <nl> - int uframes , skipped_uframes ; <nl> + uint64_t uframes , skipped_uframes ; <nl> int i ; <nl>  <nl> t_now = qemu_clock_get_ns ( QEMU_CLOCK_VIRTUAL );
static int init_directory ( BDRVVVFATState * s , const char * dirname ) <nl> memset (&( s -> first_sectors [ 0 ]), 0 , 0x40 * 0x200 ); <nl>  <nl> /* TODO : if FAT32 , this is probably wrong */ <nl> - s -> sectors_per_fat = 0xfc ; <nl> + s -> sectors_per_fat = 0xec ; <nl> s -> sectors_per_cluster = 0x10 ; <nl> s -> cluster_size = s -> sectors_per_cluster * 0x200 ; <nl> s -> cluster = malloc ( s -> cluster_size );
static int pci_add_option_rom ( PCIDevice * pdev , bool is_default_rom ) <nl> pci_patch_ids ( pdev , ptr , size ); <nl> } <nl>  <nl> + qemu_put_ram_ptr ( ptr ); <nl> + <nl> pci_register_bar ( pdev , PCI_ROM_SLOT , size , <nl> 0 , pci_map_option_rom ); <nl> 
int qdev_device_help ( QemuOpts * opts ) <nl> } <nl>  <nl> prop_list = qmp_device_list_properties ( driver , & local_err ); <nl> - if (! prop_list ) { <nl> + if ( local_err ) { <nl> error_printf ("% s \ n ", error_get_pretty ( local_err )); <nl> error_free ( local_err ); <nl> return 1 ;
__org_qemu_x_Union1 * qmp___org_qemu_x_command ( __org_qemu_x_EnumList * a , <nl> ret -> type = ORG_QEMU_X_UNION1_KIND___ORG_QEMU_X_BRANCH ; <nl> ret -> u . __org_qemu_x_branch = strdup (" blah1 "); <nl>  <nl> + /* Also test that ' wchar - t ' was munged to ' q_wchar_t ' */ <nl> + if ( b && b -> value && ! b -> value -> has_q_wchar_t ) { <nl> + b -> value -> q_wchar_t = 1 ; <nl> + } <nl> return ret ; <nl> } <nl> 
static struct iovec * lock_iovec ( int type , abi_ulong target_addr , <nl> return vec ; <nl>  <nl> fail : <nl> + while (-- i >= 0 ) { <nl> + if ( tswapal ( target_vec [ i ]. iov_len ) > 0 ) { <nl> + unlock_user ( vec [ i ]. iov_base , tswapal ( target_vec [ i ]. iov_base ), 0 ); <nl> + } <nl> + } <nl> unlock_user ( target_vec , target_addr , 0 ); <nl> fail2 : <nl> free ( vec );
QObject * json_parser_parse ( QList * tokens , va_list * ap ) <nl> QObject * json_parser_parse_err ( QList * tokens , va_list * ap , Error ** errp ) <nl> { <nl> JSONParserContext ctxt = {}; <nl> - QList * working = qlist_copy ( tokens ); <nl> + QList * working ; <nl> QObject * result ; <nl>  <nl> + if (! tokens ) { <nl> + return NULL ; <nl> + } <nl> + working = qlist_copy ( tokens ); <nl> result = parse_value (& ctxt , & working , ap ); <nl>  <nl> QDECREF ( working );
void cpu_exec_init ( CPUArchState * env ) <nl> # ifndef CONFIG_USER_ONLY <nl> cpu -> as = & address_space_memory ; <nl> cpu -> thread_id = qemu_get_thread_id (); <nl> + cpu_reload_memory_map ( cpu ); <nl> # endif <nl> QTAILQ_INSERT_TAIL (& cpus , cpu , node ); <nl> # if defined ( CONFIG_USER_ONLY )
int bdrv_aio_multiwrite ( BlockDriverState * bs , BlockRequest * reqs , int num_reqs ) <nl> reqs [ i ]. error = - EIO ; <nl> goto fail ; <nl> } else { <nl> - mcb -> error = - EIO ; <nl> + mcb -> num_requests ++; <nl> + multiwrite_cb ( mcb , - EIO ); <nl> break ; <nl> } <nl> } else {
static int open_self_cmdline ( void * cpu_env , int fd ) <nl>  <nl> if ( word_skipped ) { <nl> if ( write ( fd , cp_buf , nb_read ) != nb_read ) { <nl> + close ( fd_orig ); <nl> return - 1 ; <nl> } <nl> }
static void qemu_input_queue_process ( void * opaque ) <nl> item = QTAILQ_FIRST ( queue ); <nl> g_assert ( item -> type == QEMU_INPUT_QUEUE_DELAY ); <nl> QTAILQ_REMOVE ( queue , item , node ); <nl> + queue_count --; <nl> g_free ( item ); <nl>  <nl> while (! QTAILQ_EMPTY ( queue )) {
static void run_block_job ( BlockJob * job , Error ** errp ) <nl>  <nl> do { <nl> aio_poll ( aio_context , true ); <nl> - qemu_progress_print (( float ) job -> offset / job -> len * 100 . f , 0 ); <nl> + qemu_progress_print ( job -> len ? <nl> + (( float ) job -> offset / job -> len * 100 . f ) : 0 . 0f , 0 ); <nl> } while (! job -> ready ); <nl>  <nl> block_job_complete_sync ( job , errp );
static void print_block_info ( Monitor * mon , BlockInfo * info , <nl> inserted -> iops_size ); <nl> } <nl>  <nl> - if ( verbose ) { <nl> + /* TODO : inserted -> image should never be null */ <nl> + if ( verbose && inserted -> image ) { <nl> monitor_printf ( mon , "\ nImages :\ n "); <nl> image_info = inserted -> image ; <nl> while ( 1 ) {
static void set_year_20xx ( void ) <nl> g_assert_cmpint ( cmos_read ( RTC_YEAR ), ==, 0x11 ); <nl> g_assert_cmpint ( cmos_read ( RTC_CENTURY ), ==, 0x20 ); <nl>  <nl> + if ( sizeof ( time_t ) == 4 ) { <nl> + return ; <nl> + } <nl> + <nl> /* Set a date in 2080 to ensure there is no year - 2038 overflow . */ <nl> cmos_write ( RTC_REG_A , 0x76 ); <nl> cmos_write ( RTC_YEAR , 0x80 );
static void do_sdl_resize ( int new_width , int new_height , int bpp ) <nl>  <nl> // printf (" resizing to % d % d \ n ", w , h ); <nl>  <nl> - flags = SDL_HWSURFACE | SDL_ASYNCBLIT | SDL_HWACCEL | SDL_RESIZABLE ; <nl> - if ( gui_fullscreen ) <nl> + flags = SDL_HWSURFACE | SDL_ASYNCBLIT | SDL_HWACCEL ; <nl> + if ( gui_fullscreen ) { <nl> flags |= SDL_FULLSCREEN ; <nl> + } else { <nl> + flags |= SDL_RESIZABLE ; <nl> + } <nl> if ( gui_noframe ) <nl> flags |= SDL_NOFRAME ; <nl> 
static void gen_sse ( CPUX86State * env , DisasContext * s , int b , <nl> break ; <nl> # ifdef TARGET_X86_64 <nl> case MO_64 : <nl> - tcg_gen_mulu2_i64 ( cpu_regs [ s -> vex_v ], cpu_regs [ reg ], <nl> + tcg_gen_mulu2_i64 ( cpu_T [ 0 ], cpu_T [ 1 ], <nl> cpu_T [ 0 ], cpu_regs [ R_EDX ]); <nl> + tcg_gen_mov_i64 ( cpu_regs [ s -> vex_v ], cpu_T [ 0 ]); <nl> + tcg_gen_mov_i64 ( cpu_regs [ reg ], cpu_T [ 1 ]); <nl> break ; <nl> # endif <nl> }
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl>  <nl> if (! lock_user_struct ( VERIFY_WRITE , target_st , arg2 , 0 )) <nl> goto efault ; <nl> + memset ( target_st , 0 , sizeof (* target_st )); <nl> __put_user ( st . st_dev , & target_st -> st_dev ); <nl> __put_user ( st . st_ino , & target_st -> st_ino ); <nl> __put_user ( st . st_mode , & target_st -> st_mode );
void slirp_input ( const uint8_t * pkt , int pkt_len ) <nl> if (! m ) <nl> return ; <nl> /* Note : we add to align the IP header */ <nl> + if ( M_FREEROOM ( m ) < pkt_len + 2 ) { <nl> + m_inc ( m , pkt_len + 2 ); <nl> + } <nl> m -> m_len = pkt_len + 2 ; <nl> memcpy ( m -> m_data + 2 , pkt , pkt_len ); <nl> 
int spapr_h_cas_compose_response ( sPAPRMachineState * spapr , <nl> return 1 ; <nl> } <nl>  <nl> + if ( size < sizeof ( hdr ) || size > FW_MAX_SIZE ) { <nl> + error_report (" SLOF provided an unexpected CAS buffer size " <nl> + TARGET_FMT_lu " ( min : % zu , max : % u )", <nl> + size , sizeof ( hdr ), FW_MAX_SIZE ); <nl> + exit ( EXIT_FAILURE ); <nl> + } <nl> + <nl> size -= sizeof ( hdr ); <nl>  <nl> /* Create skeleton */
static void qxl_reset_state ( PCIQXLDevice * d ) <nl> d -> num_free_res = 0 ; <nl> d -> last_release = NULL ; <nl> memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + qxl_update_irq ( d ); <nl> } <nl>  <nl> static void qxl_soft_reset ( PCIQXLDevice * d )
static uint32_t parse_enumeration ( char * str , <nl> { <nl> uint32_t ret = not_found_value ; <nl>  <nl> + if ( str == NULL ) <nl> + return 0 ; <nl> + <nl> while ( table -> name != NULL ) { <nl> if ( strcmp ( table -> name , str ) == 0 ) { <nl> ret = table -> value ;
static void kvm_ioapic_put ( IOAPICCommonState * s ) <nl>  <nl> void kvm_ioapic_dump_state ( Monitor * mon , const QDict * qdict ) <nl> { <nl> - IOAPICCommonState s ; <nl> + IOAPICCommonState * s = IOAPIC_COMMON ( object_resolve_path (" ioapic ", NULL )); <nl>  <nl> - kvm_ioapic_get (& s ); <nl> - <nl> - ioapic_print_redtbl ( mon , & s ); <nl> + assert ( s ); <nl> + kvm_ioapic_get ( s ); <nl> + ioapic_print_redtbl ( mon , s ); <nl> } <nl>  <nl> static void kvm_ioapic_reset ( DeviceState * dev )
if ( cmd == val ) { \ <nl> output_cmd ( IPC_STAT ); <nl> output_cmd ( IPC_INFO ); <nl> /* msgctl () commands */ <nl> - # ifdef __USER_MISC <nl> output_cmd ( MSG_STAT ); <nl> output_cmd ( MSG_INFO ); <nl> - # endif <nl> /* shmctl () commands */ <nl> output_cmd ( SHM_LOCK ); <nl> output_cmd ( SHM_UNLOCK );
void qemu_iovec_concat_iov ( QEMUIOVector * dst , <nl> { <nl> int i ; <nl> size_t done ; <nl> + <nl> + if (! sbytes ) { <nl> + return ; <nl> + } <nl> assert ( dst -> nalloc != - 1 ); <nl> for ( i = 0 , done = 0 ; done < sbytes && i < src_cnt ; i ++) { <nl> if ( soffset < src_iov [ i ]. iov_len ) {
static int virtio_net_handle_mac ( VirtIONet * n , uint8_t cmd , <nl> goto error ; <nl> } <nl>  <nl> - if ( in_use + mac_data . entries <= MAC_TABLE_ENTRIES ) { <nl> + if ( mac_data . entries <= MAC_TABLE_ENTRIES - in_use ) { <nl> s = iov_to_buf ( iov , iov_cnt , 0 , & macs [ in_use * ETH_ALEN ], <nl> mac_data . entries * ETH_ALEN ); <nl> if ( s != mac_data . entries * ETH_ALEN ) {
static void vfio_ccw_register_io_notifier ( VFIOCCWDevice * vcdev , Error ** errp ) <nl> return ; <nl> } <nl>  <nl> - argsz = sizeof (* irq_set ); <nl> + argsz = sizeof (* irq_info ); <nl> irq_info = g_malloc0 ( argsz ); <nl> irq_info -> index = VFIO_CCW_IO_IRQ_INDEX ; <nl> irq_info -> argsz = argsz ;
void configure_icount ( const char * option ) <nl>  <nl> void qemu_run_all_timers ( void ) <nl> { <nl> + alarm_timer -> pending = 0 ; <nl> + <nl> /* rearm timer , if not periodic */ <nl> if ( alarm_timer -> expired ) { <nl> alarm_timer -> expired = 0 ; <nl> qemu_rearm_alarm_timer ( alarm_timer ); <nl> } <nl>  <nl> - alarm_timer -> pending = 0 ; <nl> - <nl> /* vm time timers */ <nl> if ( vm_running ) { <nl> qemu_run_timers ( vm_clock );
int qcow2_snapshot_load_tmp ( BlockDriverState * bs , <nl> sn = & s -> snapshots [ snapshot_index ]; <nl>  <nl> /* Allocate and read in the snapshot ' s L1 table */ <nl> - new_l1_bytes = s -> l1_size * sizeof ( uint64_t ); <nl> + new_l1_bytes = sn -> l1_size * sizeof ( uint64_t ); <nl> new_l1_table = g_malloc0 ( align_offset ( new_l1_bytes , 512 )); <nl>  <nl> ret = bdrv_pread ( bs -> file , sn -> l1_table_offset , new_l1_table , new_l1_bytes );
static GIOStatus ga_channel_write ( GAChannel * c , const char * buf , size_t size , <nl> * count = written ; <nl> } <nl>  <nl> + if ( ov . hEvent ) { <nl> + CloseHandle ( ov . hEvent ); <nl> + ov . hEvent = NULL ; <nl> + } <nl> return status ; <nl> } <nl> 
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> } <nl>  <nl> ip_protocol = ip -> ip_p ; <nl> - ip_data_len = be16_to_cpu ( ip -> ip_len ) - hlen ; <nl> + <nl> + ip_data_len = be16_to_cpu ( ip -> ip_len ); <nl> + if ( ip_data_len < hlen || ip_data_len > eth_payload_len ) { <nl> + goto skip_offload ; <nl> + } <nl> + ip_data_len -= hlen ; <nl>  <nl> if ( txdw0 & CP_TX_IPCS ) <nl> {
static uint64_t uart_read ( void * opaque , target_phys_addr_t offset , <nl> uint32_t c = 0 ; <nl>  <nl> offset >>= 2 ; <nl> - if ( offset > R_MAX ) { <nl> + if ( offset >= R_MAX ) { <nl> return 0 ; <nl> } else if ( offset == R_TX_RX ) { <nl> uart_read_rx_fifo ( s , & c );
static void qxl_spice_monitors_config_async ( PCIQXLDevice * qxl , int replay ) <nl> } else { <nl> # if SPICE_SERVER_VERSION >= 0x000c06 /* release 0 . 12 . 6 */ <nl> if ( qxl -> max_outputs ) { <nl> - spice_qxl_set_monitors_config_limit (& qxl -> ssd . qxl , <nl> - qxl -> max_outputs ); <nl> + spice_qxl_set_max_monitors (& qxl -> ssd . qxl , qxl -> max_outputs ); <nl> } <nl> # endif <nl> qxl -> guest_monitors_config = qxl -> ram -> monitors_config ;
static coroutine_fn int qcow2_handle_l2meta ( BlockDriverState * bs , <nl> while ( l2meta != NULL ) { <nl> QCowL2Meta * next ; <nl>  <nl> - if (! ret && link_l2 ) { <nl> + if ( link_l2 ) { <nl> ret = qcow2_alloc_cluster_link_l2 ( bs , l2meta ); <nl> if ( ret ) { <nl> goto out ;
static inline uint64_t muldiv64 ( uint64_t a , uint32_t b , uint32_t c ) <nl> return res . ll ; <nl> } <nl>  <nl> +/* Round number down to multiple */ <nl> +# define QEMU_ALIGN_DOWN ( n , m ) (( n ) / ( m ) * ( m )) <nl> + <nl> +/* Round number up to multiple */ <nl> +# define QEMU_ALIGN_UP ( n , m ) QEMU_ALIGN_DOWN (( n ) + ( m ) - 1 , ( m )) <nl> + <nl> # include " module . h " <nl>  <nl> # endif
int qcow2_alloc_clusters_at ( BlockDriverState * bs , uint64_t offset , <nl> BDRVQcowState * s = bs -> opaque ; <nl> uint64_t cluster_index ; <nl> uint64_t old_free_cluster_index ; <nl> - int i , refcount , ret ; <nl> + uint64_t i ; <nl> + int refcount , ret ; <nl> + <nl> + assert ( nb_clusters >= 0 ); <nl> + if ( nb_clusters == 0 ) { <nl> + return 0 ; <nl> + } <nl>  <nl> /* Check how many clusters there are free */ <nl> cluster_index = offset >> s -> cluster_bits ;
int main ( int argc , char ** argv , char ** envp ) <nl> HD_OPTS ); <nl> break ; <nl> case QEMU_OPTION_drive : <nl> - drive_def ( optarg ); <nl> + if ( drive_def ( optarg ) == NULL ) { <nl> + exit ( 1 ); <nl> + } <nl> break ; <nl> case QEMU_OPTION_set : <nl> if ( qemu_set_option ( optarg ) != 0 )
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } <nl> if ( dev -> setup_index < 0 || <nl> dev -> setup_len < 0 || <nl> - dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> - dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> + dev -> setup_index > dev -> setup_len || <nl> + dev -> setup_len > sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> } <nl> return 0 ;
static inline void gen_op_arith_compute_ov ( DisasContext * ctx , TCGv arg0 , <nl> { <nl> TCGv t0 = tcg_temp_new (); <nl>  <nl> - tcg_gen_xor_tl ( cpu_ov , arg0 , arg1 ); <nl> + tcg_gen_xor_tl ( cpu_ov , arg0 , arg2 ); <nl> tcg_gen_xor_tl ( t0 , arg1 , arg2 ); <nl> if ( sub ) { <nl> tcg_gen_and_tl ( cpu_ov , cpu_ov , t0 );
static void create_flash ( const VirtBoardInfo * vbi ) <nl> error_report (" Could not load ROM image '% s '", bios_name ); <nl> exit ( 1 ); <nl> } <nl> - g_free ( fn ); <nl> } <nl>  <nl> create_one_flash (" virt . flash0 ", flashbase , flashsize );
int bdrv_all_delete_snapshot ( const char * name , BlockDriverState ** first_bad_bs , <nl> if ( bdrv_can_snapshot ( bs ) && <nl> bdrv_snapshot_find ( bs , snapshot , name ) >= 0 ) { <nl> ret = bdrv_snapshot_delete_by_id_or_name ( bs , name , err ); <nl> - if ( ret < 0 ) { <nl> - goto fail ; <nl> - } <nl> } <nl> aio_context_release ( ctx ); <nl> if ( ret < 0 ) {
int kvm_cpu_exec ( CPUState * cpu ) <nl> qemu_system_reset_request (); <nl> ret = EXCP_INTERRUPT ; <nl> break ; <nl> + case KVM_SYSTEM_EVENT_CRASH : <nl> + qemu_mutex_lock_iothread (); <nl> + qemu_system_guest_panicked (); <nl> + qemu_mutex_unlock_iothread (); <nl> + ret = 0 ; <nl> + break ; <nl> default : <nl> DPRINTF (" kvm_arch_handle_exit \ n "); <nl> ret = kvm_arch_handle_exit ( cpu , run );
int s390_virtio_hypercall ( CPUState * env , uint64_t mem , uint64_t hypercall ) <nl>  <nl> dev = s390_virtio_bus_find_mem ( s390_bus , mem ); <nl> virtio_reset ( dev -> vdev ); <nl> + stb_phys ( dev -> dev_offs + VIRTIO_DEV_OFFS_STATUS , 0 ); <nl> s390_virtio_device_sync ( dev ); <nl> break ; <nl> }
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> - if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + if ( dev -> setup_index < 0 || <nl> + dev -> setup_len < 0 || <nl> + dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> }
static int usb_xhci_post_load ( void * opaque , int version_id ) <nl> slot -> uport = xhci_lookup_uport ( xhci , slot_ctx ); <nl> assert ( slot -> uport && slot -> uport -> dev ); <nl>  <nl> - for ( epid = 1 ; epid <= 32 ; epid ++) { <nl> + for ( epid = 1 ; epid <= 31 ; epid ++) { <nl> pctx = slot -> ctx + 32 * epid ; <nl> xhci_dma_read_u32s ( xhci , pctx , ep_ctx , sizeof ( ep_ctx )); <nl> state = ep_ctx [ 0 ] & EP_STATE_MASK ;
static void core_prop_set_core_id ( Object * obj , Visitor * v , const char * name , <nl> return ; <nl> } <nl>  <nl> + if ( value < 0 ) { <nl> + error_setg ( errp , " Invalid core id %" PRId64 , value ); <nl> + return ; <nl> + } <nl> + <nl> core -> core_id = value ; <nl> } <nl> 
void laio_cleanup ( void * s_ ) <nl> struct qemu_laio_state * s = s_ ; <nl>  <nl> event_notifier_cleanup (& s -> e ); <nl> + <nl> + if ( io_destroy ( s -> ctx ) != 0 ) { <nl> + fprintf ( stderr , "% s : destroy AIO context % p failed \ n ", <nl> + __func__ , & s -> ctx ); <nl> + } <nl> g_free ( s ); <nl> }
static void format_string ( StringOutputVisitor * sov , Range * r , bool next , <nl> { <nl> if ( r -> end - r -> begin > 1 ) { <nl> if ( human ) { <nl> - g_string_append_printf ( sov -> string , " 0x %" PRIx64 "-%" PRIx64 , <nl> + g_string_append_printf ( sov -> string , " 0x %" PRIx64 "- 0x %" PRIx64 , <nl> r -> begin , r -> end - 1 ); <nl>  <nl> } else {
int load_snapshot ( const char * name , Error ** errp ) <nl>  <nl> aio_context_acquire ( aio_context ); <nl> ret = qemu_loadvm_state ( f ); <nl> - qemu_fclose ( f ); <nl> aio_context_release ( aio_context ); <nl>  <nl> migration_incoming_state_destroy ();
static void machvirt_init ( MachineState * machine ) <nl> " secure - memory ", & error_abort ); <nl> } <nl>  <nl> - object_property_set_bool ( cpuobj , true , " realized ", NULL ); <nl> + object_property_set_bool ( cpuobj , true , " realized ", & error_fatal ); <nl> object_unref ( cpuobj ); <nl> } <nl> fdt_add_timer_nodes ( vms );
bool timerlist_expired ( QEMUTimerList * timer_list ) <nl> expire_time = timer_list -> active_timers -> expire_time ; <nl> qemu_mutex_unlock (& timer_list -> active_timers_lock ); <nl>  <nl> - return expire_time < qemu_clock_get_ns ( timer_list -> clock -> type ); <nl> + return expire_time <= qemu_clock_get_ns ( timer_list -> clock -> type ); <nl> } <nl>  <nl> bool qemu_clock_expired ( QEMUClockType type )
static uint64_t get_migration_pass ( void ) <nl> } else { <nl> rsp_ram = qdict_get_qdict ( rsp_return , " ram "); <nl> result = qdict_get_try_int ( rsp_ram , " dirty - sync - count ", 0 ); <nl> - QDECREF ( rsp ); <nl> } <nl> + QDECREF ( rsp ); <nl> return result ; <nl> } <nl> 
uint64_t pc_dimm_get_free_addr ( uint64_t address_space_start , <nl> uint64_t new_addr , ret = 0 ; <nl> uint64_t address_space_end = address_space_start + address_space_size ; <nl>  <nl> - assert ( address_space_end > address_space_size ); <nl> + if (! address_space_size ) { <nl> + error_setg ( errp , " memory hotplug is not enabled , " <nl> + " please add maxmem option "); <nl> + goto out ; <nl> + } <nl> + <nl> + assert ( address_space_end > address_space_start ); <nl> object_child_foreach ( qdev_get_machine (), pc_dimm_built_list , & list ); <nl>  <nl> if ( hint ) {
static KeyValue * copy_key_value ( KeyValue * src ) <nl> { <nl> KeyValue * dst = g_new ( KeyValue , 1 ); <nl> memcpy ( dst , src , sizeof (* src )); <nl> + if ( dst -> type == KEY_VALUE_KIND_NUMBER ) { <nl> + QKeyCode code = qemu_input_key_number_to_qcode ( dst -> u . number . data ); <nl> + dst -> type = KEY_VALUE_KIND_QCODE ; <nl> + dst -> u . qcode . data = code ; <nl> + } <nl> return dst ; <nl> } <nl> 
static void close_guest_eventfds ( IVShmemState * s , int posn ) <nl> { <nl> int i , guest_curr_max ; <nl>  <nl> + if (! ivshmem_has_feature ( s , IVSHMEM_IOEVENTFD )) { <nl> + return ; <nl> + } <nl> + <nl> guest_curr_max = s -> peers [ posn ]. nb_eventfds ; <nl>  <nl> memory_region_transaction_begin ();
static void do_ext_interrupt ( CPUS390XState * env ) <nl>  <nl> static void do_io_interrupt ( CPUS390XState * env ) <nl> { <nl> - uint64_t mask , addr ; <nl> + uint64_t mask = 0 , addr = 0 ; <nl> LowCore * lowcore ; <nl> IOIntQueue * q ; <nl> uint8_t isc ;
static CharDriverState * qemu_chr_open_tty ( QemuOpts * opts ) <nl> } <nl> tty_serial_init ( fd , 115200 , ' N ', 8 , 1 ); <nl> chr = qemu_chr_open_fd ( fd , fd ); <nl> - if (! chr ) { <nl> - close ( fd ); <nl> - return NULL ; <nl> - } <nl> chr -> chr_ioctl = tty_serial_ioctl ; <nl> chr -> chr_close = qemu_chr_close_tty ; <nl> return chr ;
static void coroutine_fn mirror_run ( void * opaque ) <nl>  <nl> s -> common . len = bdrv_getlength ( bs ); <nl> if ( s -> common . len <= 0 ) { <nl> - block_job_completed (& s -> common , s -> common . len ); <nl> - return ; <nl> + ret = s -> common . len ; <nl> + goto immediate_exit ; <nl> } <nl>  <nl> length = DIV_ROUND_UP ( s -> common . len , s -> granularity );
int rom_copy ( uint8_t * dest , hwaddr addr , size_t size ) <nl> if ( rom -> addr + rom -> romsize < addr ) { <nl> continue ; <nl> } <nl> - if ( rom -> addr > end ) { <nl> + if ( rom -> addr > end || rom -> addr < addr ) { <nl> break ; <nl> } <nl> 
int vnc_display_pw_expire ( DisplayState * ds , time_t expires ) <nl> { <nl> VncDisplay * vs = ds ? ( VncDisplay *) ds -> opaque : vnc_display ; <nl>  <nl> + if (! vs ) { <nl> + return - EINVAL ; <nl> + } <nl> + <nl> vs -> expires = expires ; <nl> return 0 ; <nl> }
e1000e_init_msix ( E1000EState * s ) <nl> static void <nl> e1000e_cleanup_msix ( E1000EState * s ) <nl> { <nl> - if ( msix_enabled ( PCI_DEVICE ( s ))) { <nl> + if ( msix_present ( PCI_DEVICE ( s ))) { <nl> e1000e_unuse_msix_vectors ( s , E1000E_MSIX_VEC_NUM ); <nl> msix_uninit ( PCI_DEVICE ( s ), & s -> msix , & s -> msix ); <nl> }
static uint64_t acpi_pm_tmr_read ( void * opaque , hwaddr addr , unsigned width ) <nl> return acpi_pm_tmr_get ( opaque ); <nl> } <nl>  <nl> + static void acpi_pm_tmr_write ( void * opaque , hwaddr addr , uint64_t val , <nl> + unsigned width ) <nl> +{ <nl> + /* nothing */ <nl> +} <nl> + <nl> static const MemoryRegionOps acpi_pm_tmr_ops = { <nl> . read = acpi_pm_tmr_read , <nl> + . write = acpi_pm_tmr_write , <nl> . valid . min_access_size = 4 , <nl> . valid . max_access_size = 4 , <nl> . endianness = DEVICE_LITTLE_ENDIAN ,
static int slirp_guestfwd ( SlirpState * s , const char * config_str , <nl> } <nl>  <nl> fwd = qemu_malloc ( sizeof ( struct GuestFwd )); <nl> - snprintf ( buf , sizeof ( buf ), " guestfwd . tcp :% d ", port ); <nl> + snprintf ( buf , sizeof ( buf ), " guestfwd . tcp .% d ", port ); <nl> fwd -> hd = qemu_chr_open ( buf , p , NULL ); <nl> if (! fwd -> hd ) { <nl> error_report (" could not open guest forwarding device '% s '", buf );
static void wav_capture_destroy ( void * opaque ) <nl> WAVState * wav = opaque ; <nl>  <nl> AUD_del_capture ( wav -> cap , wav ); <nl> + g_free ( wav ); <nl> } <nl>  <nl> static void wav_capture_info ( void * opaque )
static int parse_pci_devfn ( DeviceState * dev , Property * prop , const char * str ) <nl> return - EINVAL ; <nl> if ( fn > 7 ) <nl> return - EINVAL ; <nl> + if ( slot > 31 ) <nl> + return - EINVAL ; <nl> * ptr = slot << 3 | fn ; <nl> return 0 ; <nl> }
void gen_intermediate_code ( CPUAlphaState * env , struct TranslationBlock * tb ) <nl> num_insns ++; <nl>  <nl> if ( unlikely ( cpu_breakpoint_test ( cs , ctx . pc , BP_ANY ))) { <nl> - gen_excp (& ctx , EXCP_DEBUG , 0 ); <nl> + ret = gen_excp (& ctx , EXCP_DEBUG , 0 ); <nl> /* The address covered by the breakpoint must be included in <nl> [ tb -> pc , tb -> pc + tb -> size ) in order to for it to be <nl> properly cleared -- thus we increment the PC here so that
static void rtas_ibm_os_term ( PowerPCCPU * cpu , <nl> target_ulong args , <nl> uint32_t nret , target_ulong rets ) <nl> { <nl> - target_ulong ret = 0 ; <nl> + qemu_system_guest_panicked ( NULL ); <nl>  <nl> - qapi_event_send_guest_panicked ( GUEST_PANIC_ACTION_PAUSE , false , NULL , <nl> - & error_abort ); <nl> - <nl> - rtas_st ( rets , 0 , ret ); <nl> + rtas_st ( rets , 0 , RTAS_OUT_SUCCESS ); <nl> } <nl>  <nl> static void rtas_set_power_level ( PowerPCCPU * cpu , sPAPRMachineState * spapr ,
extern int ram_size ; <nl> void cpu_reset ( CPUSPARCState * env ) <nl> { <nl> memset ( env , 0 , sizeof (* env )); <nl> + tlb_flush ( env , 1 ); <nl> env -> cwp = 0 ; <nl> env -> wim = 1 ; <nl> env -> regwptr = env -> regbase + ( env -> cwp * 16 );
int qdev_simple_unplug_cb ( DeviceState * dev ) <nl> way is somewhat unclean , and best avoided . */ <nl> void qdev_init_nofail ( DeviceState * dev ) <nl> { <nl> + const char * typename = object_get_typename ( OBJECT ( dev )); <nl> + <nl> if ( qdev_init ( dev ) < 0 ) { <nl> - error_report (" Initialization of device % s failed ", <nl> - object_get_typename ( OBJECT ( dev ))); <nl> + error_report (" Initialization of device % s failed ", typename ); <nl> exit ( 1 ); <nl> } <nl> }
static void qvirtio_scsi_pci_free ( QVirtIOSCSI * vs ) <nl> qvirtio_pci_device_disable ( container_of ( vs -> dev , QVirtioPCIDevice , vdev )); <nl> g_free ( vs -> dev ); <nl> qpci_free_pc ( vs -> bus ); <nl> + g_free ( vs ); <nl> } <nl>  <nl> static uint64_t qvirtio_scsi_alloc ( QVirtIOSCSI * vs , size_t alloc_size ,
Object * container_get ( Object * root , const char * path ) <nl> if (! child ) { <nl> child = object_new (" container "); <nl> object_property_add_child ( obj , parts [ i ], child , NULL ); <nl> + object_unref ( child ); <nl> } <nl> } <nl> 
int pci_add_capability ( PCIDevice * pdev , uint8_t cap_id , <nl> Error * local_err = NULL ; <nl>  <nl> ret = pci_add_capability2 ( pdev , cap_id , offset , size , & local_err ); <nl> - if ( local_err ) { <nl> - assert ( ret < 0 ); <nl> + if ( ret < 0 ) { <nl> error_report_err ( local_err ); <nl> - } else { <nl> - /* success implies a positive offset in config space */ <nl> - assert ( ret > 0 ); <nl> } <nl> return ret ; <nl> }
static bool is_zero_cluster ( BlockDriverState * bs , int64_t start ) <nl> BlockDriverState * file ; <nl> int64_t res = bdrv_get_block_status_above ( bs , NULL , start , <nl> s -> cluster_sectors , & nr , & file ); <nl> - return res >= 0 && (( res & BDRV_BLOCK_ZERO ) || !( res & BDRV_BLOCK_DATA )); <nl> + return res >= 0 && ( res & BDRV_BLOCK_ZERO ); <nl> } <nl>  <nl> static bool is_zero_cluster_top_locked ( BlockDriverState * bs , int64_t start )
static void spapr_tce_table_class_init ( ObjectClass * klass , void * data ) <nl> dc -> init = spapr_tce_table_realize ; <nl> dc -> reset = spapr_tce_reset ; <nl> dc -> unrealize = spapr_tce_table_unrealize ; <nl> + /* Reason : This is just an internal device for handling the hypercalls */ <nl> + dc -> user_creatable = false ; <nl>  <nl> QLIST_INIT (& spapr_tce_tables ); <nl> 
static void checkpoint ( void ) { <nl> return ; <nl> /* avoid compiler warnings : */ <nl> hexdump ( NULL , 100 ); <nl> - remove_mapping ( vvv , NULL ); <nl> + remove_mapping ( vvv , 0 ); <nl> print_mapping ( NULL ); <nl> print_direntry ( NULL ); <nl> }
void cache_insert ( PageCache * cache , uint64_t addr , uint8_t * pdata ) <nl> /* actual update of entry */ <nl> it = cache_get_by_addr ( cache , addr ); <nl>  <nl> + /* free old cached data if any */ <nl> + g_free ( it -> it_data ); <nl> + <nl> if (! it -> it_data ) { <nl> cache -> num_items ++; <nl> }
static void ehci_advance_state ( EHCIState * ehci , <nl> fprintf ( stderr , " processing error - resetting ehci HC \ n "); <nl> ehci_reset ( ehci ); <nl> again = 0 ; <nl> - assert ( 0 ); <nl> } <nl> } <nl> while ( again );
static void vga_draw_graphic ( VGACommonState * s , int full_update ) <nl> } else if ( is_buffer_shared ( surface ) && <nl> ( full_update || surface_data ( surface ) != s -> vram_ptr <nl> + ( s -> start_addr * 4 ))) { <nl> - DisplaySurface * surface ; <nl> surface = qemu_create_displaysurface_from ( disp_width , <nl> height , depth , s -> line_offset , <nl> s -> vram_ptr + ( s -> start_addr * 4 ), byteswap );
static void * file_ram_alloc ( RAMBlock * block , <nl> } <nl>  <nl> /* MAP_POPULATE silently ignores failures */ <nl> - for ( i = 0 ; i < ( memory / hpagesize )- 1 ; i ++) { <nl> + for ( i = 0 ; i < ( memory / hpagesize ); i ++) { <nl> memset ( area + ( hpagesize * i ), 0 , 1 ); <nl> } <nl> 
static BlockMeasureInfo * qcow2_measure ( QemuOpts * opts , BlockDriverState * in_bs , <nl> for ( sector_num = 0 ; <nl> sector_num < ssize / BDRV_SECTOR_SIZE ; <nl> sector_num += pnum ) { <nl> - int nb_sectors = MAX ( ssize / BDRV_SECTOR_SIZE - sector_num , <nl> - INT_MAX ); <nl> + int nb_sectors = MIN ( ssize / BDRV_SECTOR_SIZE - sector_num , <nl> + BDRV_REQUEST_MAX_SECTORS ); <nl> BlockDriverState * file ; <nl> int64_t ret ; <nl> 
vubr_panic ( VuDev * dev , const char * msg ) <nl> vubr -> quit = 1 ; <nl> } <nl>  <nl> + static bool <nl> + vubr_queue_is_processed_in_order ( VuDev * dev , int qidx ) <nl> +{ <nl> + return true ; <nl> +} <nl> + <nl> static const VuDevIface vuiface = { <nl> . get_features = vubr_get_features , <nl> . set_features = vubr_set_features , <nl> . process_msg = vubr_process_msg , <nl> . queue_set_started = vubr_queue_set_started , <nl> + . queue_is_processed_in_order = vubr_queue_is_processed_in_order , <nl> }; <nl>  <nl> static void
static uint64_t ich_elrsr_read ( CPUARMState * env , const ARMCPRegInfo * ri ) <nl> uint64_t lr = cs -> ich_lr_el2 [ i ]; <nl>  <nl> if (( lr & ICH_LR_EL2_STATE_MASK ) == 0 && <nl> - (( lr & ICH_LR_EL2_HW ) == 1 || ( lr & ICH_LR_EL2_EOI ) == 0 )) { <nl> + (( lr & ICH_LR_EL2_HW ) != 0 || ( lr & ICH_LR_EL2_EOI ) == 0 )) { <nl> value |= ( 1 << i ); <nl> } <nl> }
static void xio3130_downstream_realize ( PCIDevice * d , Error ** errp ) <nl> pcie_chassis_create ( s -> chassis ); <nl> rc = pcie_chassis_add_slot ( s ); <nl> if ( rc < 0 ) { <nl> + error_setg ( errp , " Can ' t add chassis slot , error % d ", rc ); <nl> goto err_pcie_cap ; <nl> } <nl> 
static void usb_xhci_exit ( PCIDevice * dev ) <nl> /* destroy msix memory region */ <nl> if ( dev -> msix_table && dev -> msix_pba <nl> && dev -> msix_entry_used ) { <nl> - memory_region_del_subregion (& xhci -> mem , & dev -> msix_table_mmio ); <nl> - memory_region_del_subregion (& xhci -> mem , & dev -> msix_pba_mmio ); <nl> + msix_uninit ( dev , & xhci -> mem , & xhci -> mem ); <nl> } <nl>  <nl> usb_bus_release (& xhci -> bus );
BlockDriverAIOCB * laio_submit ( BlockDriverState * bs , void * aio_ctx , int fd , <nl> goto out_dec_count ; <nl> return & laiocb -> common ; <nl>  <nl> - out_free_aiocb : <nl> - qemu_aio_release ( laiocb ); <nl> out_dec_count : <nl> s -> count --; <nl> + out_free_aiocb : <nl> + qemu_aio_release ( laiocb ); <nl> return NULL ; <nl> } <nl> 
typedef struct VirtIONet { <nl> uint8_t nobcast ; <nl> uint8_t vhost_started ; <nl> struct { <nl> - int in_use ; <nl> - int first_multi ; <nl> + uint32_t in_use ; <nl> + uint32_t first_multi ; <nl> uint8_t multi_overflow ; <nl> uint8_t uni_overflow ; <nl> uint8_t * macs ;
void usb_desc_create_serial ( USBDevice * dev ) <nl> } <nl> dst += snprintf ( serial + dst , sizeof ( serial )- dst , "-% s ", dev -> port -> path ); <nl> usb_desc_set_string ( dev , index , serial ); <nl> + g_free ( path ); <nl> } <nl>  <nl> const char * usb_desc_get_string ( USBDevice * dev , uint8_t index )
static USBDevice * usb_try_create_simple ( USBBus * bus , const char * name , <nl> object_property_set_bool ( OBJECT ( dev ), true , " realized ", & err ); <nl> if ( err ) { <nl> error_propagate ( errp , err ); <nl> - error_prepend ( errp , " Failed to initialize USB device '% s ': ", <nl> - name ); <nl> - object_unparent ( OBJECT ( dev )); <nl> + error_prepend ( errp , " Failed to initialize USB device '% s ': ", name ); <nl> return NULL ; <nl> } <nl> return dev ;
int socket_connect ( SocketAddress * addr , Error ** errp , <nl> case SOCKET_ADDRESS_KIND_FD : <nl> fd = monitor_get_fd ( cur_mon , addr -> fd -> str , errp ); <nl> if ( callback ) { <nl> + qemu_set_nonblock ( fd ); <nl> callback ( fd , opaque ); <nl> } <nl> break ;
static void pc_init1 ( ram_addr_t ram_size , <nl> pci_bus = i440fx_init (& i440fx_state , & piix3_devfn , isa_irq , ram_size ); <nl> } else { <nl> pci_bus = NULL ; <nl> + i440fx_state = NULL ; <nl> isa_bus_new ( NULL ); <nl> } <nl> isa_bus_irqs ( isa_irq );
static int qcow2_do_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail ; <nl> } <nl>  <nl> + if ( header . refcount_table_clusters == 0 && !( flags & BDRV_O_CHECK )) { <nl> + error_setg ( errp , " Image does not contain a reference count table "); <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> + } <nl> + <nl> ret = validate_table_offset ( bs , s -> refcount_table_offset , <nl> s -> refcount_table_size , sizeof ( uint64_t )); <nl> if ( ret < 0 ) {
static int gicv3_gicd_no_migration_shift_bug_post_load ( void * opaque , <nl> return 0 ; <nl> } <nl>  <nl> + static bool needed_always ( void * opaque ) <nl> +{ <nl> + return true ; <nl> +} <nl> + <nl> const VMStateDescription vmstate_gicv3_gicd_no_migration_shift_bug = { <nl> . name = " arm_gicv3 / gicd_no_migration_shift_bug ", <nl> . version_id = 1 , <nl> . minimum_version_id = 1 , <nl> + . needed = needed_always , <nl> . pre_load = gicv3_gicd_no_migration_shift_bug_pre_load , <nl> . post_load = gicv3_gicd_no_migration_shift_bug_post_load , <nl> . fields = ( VMStateField []) {
static void pci_host_config_write ( void * opaque , target_phys_addr_t addr , <nl>  <nl> PCI_DPRINTF ("% s addr " TARGET_FMT_plx " len % d val %" PRIx64 "\ n ", <nl> __func__ , addr , len , val ); <nl> + if ( addr != 0 || len != 4 ) { <nl> + return ; <nl> + } <nl> s -> config_reg = val ; <nl> } <nl> 
static int qemu_gluster_parse_json ( BlockdevOptionsGluster * gconf , <nl> Error * local_err = NULL ; <nl> char * str = NULL ; <nl> const char * ptr ; <nl> - size_t num_servers ; <nl> - int i , type ; <nl> + int i , type , num_servers ; <nl>  <nl> /* create opts info from runtime_json_opts list */ <nl> opts = qemu_opts_create (& runtime_json_opts , NULL , 0 , & error_abort );
void qemu_input_event_send_key_qcode ( QemuConsole * src , QKeyCode q , bool down ) <nl>  <nl> void qemu_input_event_send_key_delay ( uint32_t delay_ms ) <nl> { <nl> + if (! runstate_is_running () && ! runstate_check ( RUN_STATE_SUSPENDED )) { <nl> + return ; <nl> + } <nl> + <nl> if (! kbd_timer ) { <nl> kbd_timer = timer_new_ms ( QEMU_CLOCK_VIRTUAL , qemu_input_queue_process , <nl> & kbd_queue );
static void aml_free ( gpointer data , gpointer user_data ) <nl> { <nl> Aml * var = data ; <nl> build_free_array ( var -> buf ); <nl> + g_free ( var ); <nl> } <nl>  <nl> Aml * init_aml_allocator ( void )
VirtIODevice * virtio_common_init ( const char * name , uint16_t device_id , <nl> vdev -> queue_sel = 0 ; <nl> vdev -> config_vector = VIRTIO_NO_VECTOR ; <nl> vdev -> vq = qemu_mallocz ( sizeof ( VirtQueue ) * VIRTIO_PCI_QUEUE_MAX ); <nl> + vdev -> vm_running = vm_running ; <nl> for ( i = 0 ; i < VIRTIO_PCI_QUEUE_MAX ; i ++) { <nl> vdev -> vq [ i ]. vector = VIRTIO_NO_VECTOR ; <nl> vdev -> vq [ i ]. vdev = vdev ;
static void vga_draw_text ( VGACommonState * s , int full_update ) <nl> cx_min = width ; <nl> cx_max = - 1 ; <nl> for ( cx = 0 ; cx < width ; cx ++) { <nl> + if ( src + sizeof ( uint16_t ) > s -> vram_ptr + s -> vram_size ) { <nl> + break ; <nl> + } <nl> ch_attr = *( uint16_t *) src ; <nl> if ( full_update || ch_attr != * ch_attr_ptr || src == cursor_ptr ) { <nl> if ( cx < cx_min )
int qcow2_update_header ( BlockDriverState * bs ) <nl> ret = sizeof (* header ); <nl> break ; <nl> default : <nl> - return - EINVAL ; <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> } <nl>  <nl> buf += ret ;
static int qcow2_check ( BlockDriverState * bs , BdrvCheckResult * result , <nl> } <nl>  <nl> if ( fix && result -> check_errors == 0 && result -> corruptions == 0 ) { <nl> - return qcow2_mark_clean ( bs ); <nl> + ret = qcow2_mark_clean ( bs ); <nl> + if ( ret < 0 ) { <nl> + return ret ; <nl> + } <nl> + return qcow2_mark_consistent ( bs ); <nl> } <nl> return ret ; <nl> }
static int ehci_init_transfer ( EHCIPacket * p ) <nl> while ( bytes > 0 ) { <nl> if ( cpage > 4 ) { <nl> fprintf ( stderr , " cpage out of range (% d )\ n ", cpage ); <nl> + qemu_sglist_destroy (& p -> sgl ); <nl> return - 1 ; <nl> } <nl> 
static void disas_ldst_reg_imm9 ( DisasContext * s , uint32_t insn ) <nl> } <nl> } else { <nl> TCGv_i64 tcg_rt = cpu_reg ( s , rt ); <nl> - int memidx = is_unpriv ? 1 : get_mem_index ( s ); <nl> + int memidx = is_unpriv ? MMU_USER_IDX : get_mem_index ( s ); <nl>  <nl> if ( is_store ) { <nl> do_gpr_st_memidx ( s , tcg_rt , tcg_addr , size , memidx );
static void x86_cpu_register_feature_bit_props ( X86CPU * cpu , <nl>  <nl> for ( i = 1 ; names [ i ]; i ++) { <nl> feat2prop ( names [ i ]); <nl> - object_property_add_alias ( obj , names [ i ], obj , g_strdup ( names [ 0 ]), <nl> + object_property_add_alias ( obj , names [ i ], obj , names [ 0 ], <nl> & error_abort ); <nl> } <nl> 
Object * container_get ( Object * root , const char * path ) <nl> } <nl> } <nl>  <nl> + g_strfreev ( parts ); <nl> + <nl> return obj ; <nl> } <nl> 
void qmp_transaction ( TransactionActionList * dev_list , Error ** errp ) <nl> assert ( dev_info -> kind < ARRAY_SIZE ( actions )); <nl>  <nl> ops = & actions [ dev_info -> kind ]; <nl> + assert ( ops -> instance_size > 0 ); <nl> + <nl> state = g_malloc0 ( ops -> instance_size ); <nl> state -> ops = ops ; <nl> state -> action = dev_info ;
static void vnc_init_timer ( VncDisplay * vd ) <nl> vd -> timer_interval = VNC_REFRESH_INTERVAL_BASE ; <nl> if ( vd -> timer == NULL && ! QTAILQ_EMPTY (& vd -> clients )) { <nl> vd -> timer = qemu_new_timer ( rt_clock , vnc_refresh , vd ); <nl> + vnc_dpy_resize ( vd -> ds ); <nl> vnc_refresh ( vd ); <nl> } <nl> }
static inline void cpu_x86_load_seg_cache ( CPUX86State * env , <nl> } <nl>  <nl> static inline void cpu_x86_load_seg_cache_sipi ( X86CPU * cpu , <nl> - int sipi_vector ) <nl> + uint8_t sipi_vector ) <nl> { <nl> CPUState * cs = CPU ( cpu ); <nl> CPUX86State * env = & cpu -> env ;
static int connect_to_sdog ( const char * addr , const char * port ) <nl> if ( errno == EINTR ) { <nl> goto reconnect ; <nl> } <nl> + close ( fd ); <nl> break ; <nl> } <nl> 
static int disas_coproc_insn ( DisasContext * s , uint32_t insn ) <nl> break ; <nl> } <nl>  <nl> - gen_set_pc_im ( s , s -> pc ); <nl> + gen_set_pc_im ( s , s -> pc - 4 ); <nl> tmpptr = tcg_const_ptr ( ri ); <nl> tcg_syn = tcg_const_i32 ( syndrome ); <nl> gen_helper_access_check_cp_reg ( cpu_env , tmpptr , tcg_syn );
static void spapr_rtc_class_init ( ObjectClass * oc , void * data ) <nl>  <nl> dc -> realize = spapr_rtc_realize ; <nl> dc -> vmsd = & vmstate_spapr_rtc ; <nl> + /* Reason : This is an internal device only for handling the hypercalls */ <nl> + dc -> user_creatable = false ; <nl>  <nl> spapr_rtas_register ( RTAS_GET_TIME_OF_DAY , " get - time - of - day ", <nl> rtas_get_time_of_day );
static int raw_read_options ( QDict * options , BlockDriverState * bs , <nl>  <nl> /* Make sure size is multiple of BDRV_SECTOR_SIZE to prevent rounding <nl> * up and leaking out of the specified area . */ <nl> - if (! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> + if ( s -> has_size && ! QEMU_IS_ALIGNED ( s -> size , BDRV_SECTOR_SIZE )) { <nl> error_setg ( errp , " Specified size is not multiple of % llu ", <nl> BDRV_SECTOR_SIZE ); <nl> ret = - EINVAL ;
static void v9fs_attach ( void * opaque ) <nl> s -> root_fid = fid ; <nl> /* disable migration */ <nl> error_set (& s -> migration_blocker , QERR_VIRTFS_FEATURE_BLOCKS_MIGRATION , <nl> - s -> ctx . fs_root , s -> tag ); <nl> + s -> ctx . fs_root ? s -> ctx . fs_root : " NULL ", s -> tag ); <nl> migrate_add_blocker ( s -> migration_blocker ); <nl> out : <nl> put_fid ( pdu , fidp );
bool aio_poll ( AioContext * ctx , bool blocking ) <nl> int count ; <nl> int timeout ; <nl>  <nl> - if ( aio_prepare ( ctx )) { <nl> + have_select_revents = aio_prepare ( ctx ); <nl> + if ( have_select_revents ) { <nl> blocking = false ; <nl> - have_select_revents = true ; <nl> } <nl>  <nl> was_dispatching = ctx -> dispatching ;
static void coroutine_fn mirror_run ( void * opaque ) <nl> } <nl>  <nl> end = s -> common . len >> BDRV_SECTOR_BITS ; <nl> - s -> buf = qemu_blockalign ( bs , s -> buf_size ); <nl> + s -> buf = qemu_try_blockalign ( bs , s -> buf_size ); <nl> + if ( s -> buf == NULL ) { <nl> + ret = - ENOMEM ; <nl> + goto immediate_exit ; <nl> + } <nl> + <nl> sectors_per_chunk = s -> granularity >> BDRV_SECTOR_BITS ; <nl> mirror_free_init ( s ); <nl> 
int qcow2_grow_l1_table ( BlockDriverState * bs , uint64_t min_size , <nl> if ( min_size <= s -> l1_size ) <nl> return 0 ; <nl>  <nl> + /* Do a sanity check on min_size before trying to calculate new_l1_size <nl> + * ( this prevents overflows during the while loop for the calculation of <nl> + * new_l1_size ) */ <nl> + if ( min_size > INT_MAX / sizeof ( uint64_t )) { <nl> + return - EFBIG ; <nl> + } <nl> + <nl> if ( exact_size ) { <nl> new_l1_size = min_size ; <nl> } else {
int kvm_arch_handle_exit ( CPUPPCState * env , struct kvm_run * run ) <nl> dprintf (" handle PAPR hypercall \ n "); <nl> run -> papr_hcall . ret = spapr_hypercall ( env , run -> papr_hcall . nr , <nl> run -> papr_hcall . args ); <nl> - ret = 1 ; <nl> + ret = 0 ; <nl> break ; <nl> # endif <nl> default :
static void set_pci_devfn ( Object * obj , Visitor * v , void * opaque , <nl>  <nl> visit_type_str ( v , & str , name , & local_err ); <nl> if ( local_err ) { <nl> + error_free ( local_err ); <nl> return set_int32 ( obj , v , opaque , name , errp ); <nl> } <nl> 
static void usbredir_handle_destroy ( USBDevice * udev ) <nl> USBRedirDevice * dev = DO_UPCAST ( USBRedirDevice , dev , udev ); <nl>  <nl> qemu_chr_delete ( dev -> cs ); <nl> + dev -> cs = NULL ; <nl> /* Note must be done after qemu_chr_close , as that causes a close event */ <nl> qemu_bh_delete ( dev -> chardev_close_bh ); <nl> 
static void report_unavailable_features ( FeatureWord w , uint32_t mask ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < 32 ; ++ i ) { <nl> - if ( 1 << i & mask ) { <nl> + if (( 1UL << i ) & mask ) { <nl> const char * reg = get_register_name_32 ( f -> cpuid_reg ); <nl> assert ( reg ); <nl> fprintf ( stderr , " warning : % s doesn ' t support requested feature : "
int kvm_init_vcpu ( CPUState * env ) <nl>  <nl> env -> kvm_fd = ret ; <nl> env -> kvm_state = s ; <nl> + env -> kvm_vcpu_dirty = 1 ; <nl>  <nl> mmap_size = kvm_ioctl ( s , KVM_GET_VCPU_MMAP_SIZE , 0 ); <nl> if ( mmap_size < 0 ) {
static void gen_rot_rm_T1 ( DisasContext * s , int ot , int op1 , int is_right ) <nl> if ( is_right ) { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask - 1 ); <nl> tcg_gen_shri_tl ( cpu_cc_dst , cpu_T [ 0 ], mask ); <nl> + tcg_gen_andi_tl ( cpu_cc_dst , cpu_cc_dst , 1 ); <nl> } else { <nl> tcg_gen_shri_tl ( cpu_cc_src2 , cpu_T [ 0 ], mask ); <nl> tcg_gen_andi_tl ( cpu_cc_dst , cpu_T [ 0 ], 1 );
static int vmdk_write ( BlockDriverState * bs , int64_t sector_num , <nl> { <nl> BDRVVmdkState * s = bs -> opaque ; <nl> VmdkExtent * extent = NULL ; <nl> - int n , ret ; <nl> - int64_t index_in_cluster ; <nl> + int ret ; <nl> + int64_t index_in_cluster , n ; <nl> uint64_t extent_begin_sector , extent_relative_sector_num ; <nl> uint64_t cluster_offset ; <nl> VmdkMetaData m_data ;
static int coroutine_fn bdrv_mirror_top_pdiscard ( BlockDriverState * bs , <nl>  <nl> static void bdrv_mirror_top_refresh_filename ( BlockDriverState * bs , QDict * opts ) <nl> { <nl> + if ( bs -> backing == NULL ) { <nl> + /* we can be here after failed bdrv_attach_child in <nl> + * bdrv_set_backing_hd */ <nl> + return ; <nl> + } <nl> bdrv_refresh_filename ( bs -> backing -> bs ); <nl> pstrcpy ( bs -> exact_filename , sizeof ( bs -> exact_filename ), <nl> bs -> backing -> bs -> filename );
static void vnc_init_basic_info_from_server_addr ( QIOChannelSocket * ioc , <nl> { <nl> SocketAddress * addr = NULL ; <nl>  <nl> + if (! ioc ) { <nl> + error_setg ( errp , " No listener socket available "); <nl> + return ; <nl> + } <nl> + <nl> addr = qio_channel_socket_get_local_address ( ioc , errp ); <nl> if (! addr ) { <nl> return ;
static int raw_create ( const char * filename , QemuOpts * opts , Error ** errp ) <nl> goto out ; <nl> } <nl>  <nl> - fd = qemu_open ( filename , O_WRONLY | O_CREAT | O_TRUNC | O_BINARY , <nl> + fd = qemu_open ( filename , O_RDWR | O_CREAT | O_TRUNC | O_BINARY , <nl> 0644 ); <nl> if ( fd < 0 ) { <nl> result = - errno ;
udp_input ( register struct mbuf * m , int iphlen ) <nl> * Locate pcb for datagram . <nl> */ <nl> so = slirp -> udp_last_so ; <nl> - if ( so -> so_lport != uh -> uh_sport || <nl> + if ( so == & slirp -> udb || so -> so_lport != uh -> uh_sport || <nl> so -> so_laddr . s_addr != ip -> ip_src . s_addr ) { <nl> struct socket * tmp ; <nl> 
static void pc_fw_add_pflash_drv ( void ) <nl> bios_name = BIOS_FILENAME ; <nl> } <nl> filename = qemu_find_file ( QEMU_FILE_TYPE_BIOS , bios_name ); <nl> + if (! filename ) { <nl> + error_report (" Can ' t open BIOS image % s ", bios_name ); <nl> + exit ( 1 ); <nl> + } <nl>  <nl> opts = drive_add ( IF_PFLASH , - 1 , filename , " readonly = on "); <nl> 
static int check_directory_consistency ( BDRVVVFATState * s , <nl>  <nl> long_file_name lfn ; <nl> int path_len = strlen ( path ); <nl> - char path2 [ PATH_MAX ]; <nl> + char path2 [ PATH_MAX + 1 ]; <nl>  <nl> assert ( path_len < PATH_MAX ); /* len was tested before ! */ <nl> pstrcpy ( path2 , sizeof ( path2 ), path );
static void puv3_load_kernel ( const char * kernel_filename ) <nl> if ( kernel_filename == NULL && qtest_enabled ()) { <nl> return ; <nl> } <nl> - assert ( kernel_filename != NULL ); <nl> + if ( kernel_filename == NULL ) { <nl> + error_report (" kernel parameter cannot be empty "); <nl> + exit ( 1 ); <nl> + } <nl>  <nl> /* only zImage format supported */ <nl> size = load_image_targphys ( kernel_filename , KERNEL_LOAD_ADDR ,
static PCIDevice * qemu_pci_hot_add_storage ( Monitor * mon , <nl> const char * opts ) <nl> { <nl> PCIDevice * dev ; <nl> - DriveInfo * dinfo ; <nl> + DriveInfo * dinfo = NULL ; <nl> int type = - 1 ; <nl> char buf [ 128 ]; <nl> 
static int spapr_fixup_cpu_smt_dt ( void * fdt , int offset , PowerPCCPU * cpu , <nl> int index = ppc_get_vcpu_dt_id ( cpu ); <nl>  <nl> if ( cpu -> cpu_version ) { <nl> - ret = fdt_setprop ( fdt , offset , " cpu - version ", <nl> - & cpu -> cpu_version , sizeof ( cpu -> cpu_version )); <nl> + ret = fdt_setprop_cell ( fdt , offset , " cpu - version ", cpu -> cpu_version ); <nl> if ( ret < 0 ) { <nl> return ret ; <nl> }
FWCfgState * pc_memory_init ( MachineState * machine , <nl> exit ( EXIT_FAILURE ); <nl> } <nl>  <nl> + if ( QEMU_ALIGN_UP ( machine -> maxram_size , <nl> + TARGET_PAGE_SIZE ) != machine -> maxram_size ) { <nl> + error_report (" maximum memory size must by aligned to multiple of " <nl> + "% d bytes ", TARGET_PAGE_SIZE ); <nl> + exit ( EXIT_FAILURE ); <nl> + } <nl> + <nl> pcms -> hotplug_memory_base = <nl> ROUND_UP ( 0x100000000ULL + above_4g_mem_size , 1ULL << 30 ); <nl> 
static inline void gen_op_arith_compute_ov ( DisasContext * ctx , TCGv arg0 , <nl> if ( is_isa300 ( ctx )) { <nl> tcg_gen_extract_tl ( cpu_ov32 , cpu_ov , 31 , 1 ); <nl> } <nl> - tcg_gen_extract_tl ( cpu_ov , cpu_ov , 63 , 1 ); <nl> + tcg_gen_extract_tl ( cpu_ov , cpu_ov , TARGET_LONG_BITS - 1 , 1 ); <nl> } <nl> tcg_gen_or_tl ( cpu_so , cpu_so , cpu_ov ); <nl> }
void bdrv_image_info_specific_dump ( fprintf_function func_fprintf , void * f , <nl> assert ( qobject_type ( obj ) == QTYPE_QDICT ); <nl> data = qdict_get ( qobject_to_qdict ( obj ), " data "); <nl> dump_qobject ( func_fprintf , f , 1 , data ); <nl> + qobject_decref ( obj ); <nl> visit_free ( v ); <nl> } <nl> 
static int vpc_open ( BlockDriverState * bs , QDict * options , int flags , <nl> ret = - EINVAL ; <nl> goto fail ; <nl> } <nl> - if ( s -> max_table_entries > ( VHD_MAX_SECTORS * 512 ) / s -> block_size ) { <nl> - ret = - EINVAL ; <nl> - goto fail ; <nl> - } <nl>  <nl> computed_size = ( uint64_t ) s -> max_table_entries * s -> block_size ; <nl> if ( computed_size < bs -> total_sectors * 512 ) {
ip_input ( struct mbuf * m ) <nl> DEBUG_ARG (" m_len = % d ", m -> m_len ); <nl>  <nl> if ( m -> m_len < sizeof ( struct ip )) { <nl> - return ; <nl> + goto bad ; <nl> } <nl>  <nl> ip = mtod ( m , struct ip *);
static void vfio_map_bar ( VFIOPCIDevice * vdev , int nr ) <nl> if ( vdev -> msix && vdev -> msix -> table_bar == nr ) { <nl> uint64_t start ; <nl>  <nl> - start = HOST_PAGE_ALIGN ( vdev -> msix -> table_offset + <nl> + start = HOST_PAGE_ALIGN (( uint64_t ) vdev -> msix -> table_offset + <nl> ( vdev -> msix -> entries * PCI_MSIX_ENTRY_SIZE )); <nl>  <nl> size = start < bar -> region . size ? bar -> region . size - start : 0 ;
fail : <nl> /* refcount checking functions */ <nl>  <nl>  <nl> - static size_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> + static uint64_t refcount_array_byte_size ( BDRVQcow2State * s , uint64_t entries ) <nl> { <nl> /* This assertion holds because there is no way we can address more than <nl> * 2 ^( 64 - 9 ) clusters at once ( with cluster size 512 = 2 ^ 9 , and because
 <nl> typedef struct SuperIOConfig <nl> { <nl> - uint8_t config [ 0xff ]; <nl> + uint8_t config [ 0x100 ]; <nl> uint8_t index ; <nl> uint8_t data ; <nl> } SuperIOConfig ;
static int alloc_cluster_link_l2 ( BlockDriverState * bs , uint64_t cluster_offset , <nl> goto err ; <nl>  <nl> for ( i = 0 ; i < j ; i ++) <nl> - free_any_clusters ( bs , old_cluster [ i ], 1 ); <nl> + free_any_clusters ( bs , be64_to_cpu ( old_cluster [ i ]), 1 ); <nl>  <nl> ret = 0 ; <nl> err :
static int vmdk_open_vmfs_sparse ( BlockDriverState * bs , <nl> } <nl> ret = vmdk_add_extent ( bs , file , false , <nl> le32_to_cpu ( header . disk_sectors ), <nl> - le32_to_cpu ( header . l1dir_offset ) << 9 , <nl> + ( int64_t ) le32_to_cpu ( header . l1dir_offset ) << 9 , <nl> 0 , <nl> le32_to_cpu ( header . l1dir_size ), <nl> 4096 ,
static int scsi_req_length ( SCSICommand * cmd , SCSIDevice * dev , uint8_t * buf ) <nl> case VERIFY_16 : <nl> if (( buf [ 1 ] & 2 ) == 0 ) { <nl> cmd -> xfer = 0 ; <nl> - } else if (( buf [ 1 ] & 4 ) == 1 ) { <nl> + } else if (( buf [ 1 ] & 4 ) != 0 ) { <nl> cmd -> xfer = 1 ; <nl> } <nl> cmd -> xfer *= dev -> blocksize ;
int inet_dgram_opts ( QemuOpts * opts ) <nl> if ( 0 != ( rc = getaddrinfo ( addr , port , & ai , & local ))) { <nl> fprintf ( stderr ," getaddrinfo (% s ,% s ): % s \ n ", addr , port , <nl> gai_strerror ( rc )); <nl> - return - 1 ; <nl> + goto err ; <nl> } <nl>  <nl> /* create socket */
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> void * obj ; <nl> int i , j ; <nl>  <nl> + if (! object_dynamic_cast ( qdev_get_machine (), TYPE_SPAPR_MACHINE )) { <nl> + error_setg ( errp , " spapr - cpu - core needs a pseries machine "); <nl> + return ; <nl> + } <nl> + <nl> sc -> threads = g_malloc0 ( size * cc -> nr_threads ); <nl> for ( i = 0 ; i < cc -> nr_threads ; i ++) { <nl> char id [ 32 ];
extern const PropertyInfo qdev_prop_link ; <nl> _arrayfield , _arrayprop , _arraytype ) { \ <nl> . name = ( PROP_ARRAY_LEN_PREFIX _name ), \ <nl> . info = &( qdev_prop_arraylen ), \ <nl> + . defval . u = 0 , \ <nl> . offset = offsetof ( _state , _field ) \ <nl> + type_check ( uint32_t , typeof_field ( _state , _field )), \ <nl> . arrayinfo = &( _arrayprop ), \
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl>  <nl> int tcp_hlen = TCP_HEADER_DATA_OFFSET ( p_tcp_hdr ); <nl>  <nl> + /* Invalid TCP data offset ? */ <nl> + if ( tcp_hlen < sizeof ( tcp_header ) || tcp_hlen > ip_data_len ) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ETH_MTU = ip header len + tcp header len + payload */ <nl> int tcp_data_len = ip_data_len - tcp_hlen ; <nl> int tcp_chunk_size = ETH_MTU - hlen - tcp_hlen ;
static int xen_pt_initfn ( PCIDevice * d ) <nl>  <nl> /* Initialize virtualized PCI configuration ( Extended 256 Bytes ) */ <nl> if ( xen_host_pci_get_block (& s -> real_device , 0 , d -> config , <nl> - PCI_CONFIG_SPACE_SIZE ) == - 1 ) { <nl> + PCI_CONFIG_SPACE_SIZE ) < 0 ) { <nl> xen_host_pci_device_put (& s -> real_device ); <nl> return - 1 ; <nl> }
found : <nl> QTAILQ_REMOVE (& s -> discards , p , next ); <nl> d -> offset = MIN ( d -> offset , p -> offset ); <nl> d -> bytes += p -> bytes ; <nl> + g_free ( p ); <nl> } <nl> } <nl> 
static void coroutine_fn verify_entered_step_2 ( void * opaque ) <nl> /* Once more to check it still works after yielding */ <nl> g_assert ( qemu_coroutine_entered ( caller )); <nl> g_assert ( qemu_coroutine_entered ( qemu_coroutine_self ())); <nl> - qemu_coroutine_yield (); <nl> } <nl>  <nl> static void coroutine_fn verify_entered_step_1 ( void * opaque )
static inline void cpu_physical_memory_set_dirty_lebitmap ( unsigned long * bitmap , <nl> unsigned long page = BIT_WORD ( start >> TARGET_PAGE_BITS ); <nl>  <nl> /* start address is aligned at the start of a word ? */ <nl> - if ((( page * BITS_PER_LONG ) << TARGET_PAGE_BITS ) == start ) { <nl> + if (((( page * BITS_PER_LONG ) << TARGET_PAGE_BITS ) == start ) && <nl> + ( hpratio == 1 )) { <nl> long k ; <nl> long nr = BITS_TO_LONGS ( pages ); <nl> 
static int usb_host_open ( USBHostDevice * dev , int bus_num , <nl>  <nl> dev -> bus_num = bus_num ; <nl> dev -> addr = addr ; <nl> - strcpy ( dev -> port , port ); <nl> + pstrcpy ( dev -> port , sizeof ( dev -> port ), port ); <nl> dev -> fd = fd ; <nl>  <nl> /* read the device description */
static void update_irq ( struct xlx_pic * p ) <nl>  <nl> /* Update the vector register . */ <nl> for ( i = 0 ; i < 32 ; i ++) { <nl> - if ( p -> regs [ R_IPR ] & ( 1 << i )) <nl> + if ( p -> regs [ R_IPR ] & ( 1U << i )) { <nl> break ; <nl> + } <nl> } <nl> if ( i == 32 ) <nl> i = ~ 0 ;
static void write_bootloader ( uint8_t * base , int64_t run_addr , <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> stl_p ( p ++, 0x0ff0021c ); /* jal 870 */ <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> - stl_p ( p ++, 0x08000205 ); /* j 814 */ <nl> + stl_p ( p ++, 0x1000fff9 ); /* b 814 */ <nl> stl_p ( p ++, 0x00000000 ); /* nop */ <nl> stl_p ( p ++, 0x01a00009 ); /* jalr t5 */ <nl> stl_p ( p ++, 0x01602021 ); /* move a0 , t3 */
static void co_read_response ( void * opaque ) <nl> s -> co_recv = qemu_coroutine_create ( aio_read_response , opaque ); <nl> } <nl>  <nl> - aio_co_wake ( s -> co_recv ); <nl> + aio_co_enter ( s -> aio_context , s -> co_recv ); <nl> } <nl>  <nl> static void co_write_request ( void * opaque )
struct target_sigcontext { <nl> /* A Sparc stack frame */ <nl> struct sparc_stackf { <nl> abi_ulong locals [ 8 ]; <nl> - abi_ulong ins [ 6 ]; <nl> - struct sparc_stackf * fp ; <nl> - abi_ulong callers_pc ; <nl> + abi_ulong ins [ 8 ]; <nl> + /* It ' s simpler to treat fp and callers_pc as elements of ins [] <nl> + * since we never need to access them ourselves . <nl> + */ <nl> char * structptr ; <nl> abi_ulong xargs [ 6 ]; <nl> abi_ulong xxargs [ 1 ];
static int sd_snapshot_create ( BlockDriverState * bs , QEMUSnapshotInfo * sn_info ) <nl>  <nl> ret = do_sd_create ( s , & new_vid , 1 , & local_err ); <nl> if ( ret < 0 ) { <nl> - error_report_err ( local_err ); <nl> - error_report (" failed to create inode for snapshot . % s ", <nl> - strerror ( errno )); <nl> + error_report (" failed to create inode for snapshot : % s ", <nl> + error_get_pretty ( local_err )); <nl> goto cleanup ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> return 0 ; <nl> } <nl> argv += optind ; <nl> - optind = 1 ; <nl> + optind = 0 ; <nl>  <nl> if (! trace_init_backends ()) { <nl> exit ( 1 );
static int bdrv_rw_co ( BlockDriverState * bs , int64_t sector_num , uint8_t * buf , <nl> . iov_len = nb_sectors * BDRV_SECTOR_SIZE , <nl> }; <nl>  <nl> + if ( nb_sectors < 0 || nb_sectors > INT_MAX / BDRV_SECTOR_SIZE ) { <nl> + return - EINVAL ; <nl> + } <nl> + <nl> qemu_iovec_init_external (& qiov , & iov , 1 ); <nl> return bdrv_prwv_co ( bs , sector_num << BDRV_SECTOR_BITS , <nl> & qiov , is_write , flags );
static coroutine_fn void nbd_read_reply_entry ( void * opaque ) <nl> { <nl> NBDClientSession * s = opaque ; <nl> uint64_t i ; <nl> - int ret ; <nl> + int ret = 0 ; <nl> Error * local_err = NULL ; <nl>  <nl> while (! s -> quit ) {
static int usb_msd_handle_control ( USBDevice * dev , USBPacket * p , <nl> static void usb_msd_cancel_io ( USBDevice * dev , USBPacket * p ) <nl> { <nl> MSDState * s = DO_UPCAST ( MSDState , dev , dev ); <nl> - scsi_req_cancel ( s -> req ); <nl> + <nl> + if ( s -> req ) { <nl> + scsi_req_cancel ( s -> req ); <nl> + } <nl> } <nl>  <nl> static int usb_msd_handle_data ( USBDevice * dev , USBPacket * p )
static void mmio_interface_realize ( DeviceState * dev , Error ** errp ) <nl>  <nl> if (! s -> host_ptr ) { <nl> error_setg ( errp , " host_ptr property must be set "); <nl> + return ; <nl> } <nl>  <nl> if (! s -> subregion ) { <nl> error_setg ( errp , " subregion property must be set "); <nl> + return ; <nl> } <nl>  <nl> memory_region_init_ram_ptr (& s -> ram_mem , OBJECT ( s ), " ram ",
static void qxl_enter_vga_mode ( PCIQXLDevice * d ) <nl> qemu_spice_create_host_primary (& d -> ssd ); <nl> d -> mode = QXL_MODE_VGA ; <nl> memset (& d -> ssd . dirty , 0 , sizeof ( d -> ssd . dirty )); <nl> + vga_dirty_log_start (& d -> vga ); <nl> } <nl>  <nl> static void qxl_exit_vga_mode ( PCIQXLDevice * d ) <nl> static void qxl_exit_vga_mode ( PCIQXLDevice * d ) <nl> return ; <nl> } <nl> trace_qxl_exit_vga_mode ( d -> id ); <nl> + vga_dirty_log_stop (& d -> vga ); <nl> qxl_destroy_primary ( d , QXL_SYNC ); <nl> } <nl> 
struct ccw1 { <nl> __u8 flags ; <nl> __u16 count ; <nl> __u32 cda ; <nl> -} __attribute__ (( packed )); <nl> +} __attribute__ (( packed , aligned ( 8 ))); <nl>  <nl> # define CCW_FLAG_DC 0x80 <nl> # define CCW_FLAG_CC 0x40
void vnc_display_open ( const char * id , Error ** errp ) <nl> if ( vs -> ws_enabled ) { <nl> vs -> lwebsock = inet_listen_opts ( wsopts , 0 , errp ); <nl> if ( vs -> lwebsock < 0 ) { <nl> - if ( vs -> lsock ) { <nl> + if ( vs -> lsock != - 1 ) { <nl> close ( vs -> lsock ); <nl> vs -> lsock = - 1 ; <nl> }
int spapr_populate_pci_devices ( sPAPRPHBState * phb , <nl> reg [ 0 ]. size = 0 ; <nl>  <nl> n = 0 ; <nl> - for ( i = 0 ; i < PCI_NUM_REGIONS ; ++ i ) { <nl> + for ( i = 0 ; i < ARRAY_SIZE ( bars ); ++ i ) { <nl> if ( 0 == dev -> io_regions [ i ]. size ) { <nl> continue ; <nl> }
static ram_addr_t qxl_rom_size ( void ) <nl> sizeof ( qxl_modes ); <nl> uint32_t rom_size = 8192 ; /* two pages */ <nl>  <nl> - required_rom_size = MAX ( required_rom_size , TARGET_PAGE_SIZE ); <nl> - required_rom_size = msb_mask ( required_rom_size * 2 - 1 ); <nl> - assert ( required_rom_size <= rom_size ); <nl> + QEMU_BUILD_BUG_ON ( required_rom_size > rom_size ); <nl> return rom_size ; <nl> } <nl> 
static void pty_chr_state ( CharDriverState * chr , int connected ) <nl> s -> timer_tag = 0 ; <nl> } <nl> if (! s -> connected ) { <nl> - qemu_chr_be_generic_open ( chr ); <nl> s -> connected = 1 ; <nl> + qemu_chr_be_generic_open ( chr ); <nl> s -> fd_tag = io_add_watch_poll ( s -> fd , pty_chr_read_poll , pty_chr_read , chr ); <nl> } <nl> }
S390CPU * s390x_new_cpu ( const char * typename , uint32_t core_id , Error ** errp ) <nl> object_property_set_bool ( OBJECT ( cpu ), true , " realized ", & err ); <nl>  <nl> out : <nl> + object_unref ( OBJECT ( cpu )); <nl> if ( err ) { <nl> error_propagate ( errp , err ); <nl> - object_unref ( OBJECT ( cpu )); <nl> cpu = NULL ; <nl> } <nl> return cpu ;
static void pc_isa_bios_init ( MemoryRegion * rom_memory , <nl> flash_size = memory_region_size ( flash_mem ); <nl>  <nl> /* map the last 128KB of the BIOS in ISA space */ <nl> - isa_bios_size = flash_size ; <nl> - if ( isa_bios_size > ( 128 * 1024 )) { <nl> - isa_bios_size = 128 * 1024 ; <nl> - } <nl> + isa_bios_size = MIN ( flash_size , 128 * 1024 ); <nl> isa_bios = g_malloc ( sizeof (* isa_bios )); <nl> memory_region_init_ram ( isa_bios , NULL , " isa - bios ", isa_bios_size ); <nl> vmstate_register_ram_global ( isa_bios );
fail : <nl> qemu_co_mutex_unlock (& s -> lock ); <nl>  <nl> qemu_iovec_destroy (& hd_qiov ); <nl> + g_free ( cluster_data ); <nl>  <nl> return ret ; <nl> } <nl> fail : <nl> qemu_co_mutex_unlock (& s -> lock ); <nl>  <nl> qemu_iovec_destroy (& hd_qiov ); <nl> + g_free ( cluster_data ); <nl>  <nl> return ret ; <nl> }
void socket_listen_cleanup ( int fd , Error ** errp ) <nl> SocketAddress * addr ; <nl>  <nl> addr = socket_local_address ( fd , errp ); <nl> + if (! addr ) { <nl> + return ; <nl> + } <nl>  <nl> if ( addr -> type == SOCKET_ADDRESS_TYPE_UNIX <nl> && addr -> u . q_unix . path ) {
static int rtl8139_cplus_transmit_one ( RTL8139State * s ) <nl> { <nl> DPRINTF ("+++ C + mode offloaded task checksum \ n "); <nl>  <nl> + /* Large enough for Ethernet and IP headers ? */ <nl> + if ( saved_size < ETH_HLEN + sizeof ( ip_header )) { <nl> + goto skip_offload ; <nl> + } <nl> + <nl> /* ip packet header */ <nl> ip_header * ip = NULL ; <nl> int hlen = 0 ;
static BlockDriverAIOCB * bdrv_aio_rw_vector ( BlockDriverState * bs , <nl> acb -> is_write = is_write ; <nl> acb -> qiov = qiov ; <nl> acb -> bounce = qemu_blockalign ( bs , qiov -> size ); <nl> - <nl> - if (! acb -> bh ) <nl> - acb -> bh = qemu_bh_new ( bdrv_aio_bh_cb , acb ); <nl> + acb -> bh = qemu_bh_new ( bdrv_aio_bh_cb , acb ); <nl>  <nl> if ( is_write ) { <nl> qemu_iovec_to_buffer ( acb -> qiov , acb -> bounce );
static void spapr_cpu_core_realize_child ( Object * child , Error ** errp ) <nl> Object * obj ; <nl>  <nl> obj = object_new ( spapr -> icp_type ); <nl> - object_property_add_child ( OBJECT ( cpu ), " icp ", obj , NULL ); <nl> + object_property_add_child ( OBJECT ( cpu ), " icp ", obj , & error_abort ); <nl> + object_unref ( obj ); <nl> object_property_add_const_link ( obj , " xics ", OBJECT ( spapr ), & error_abort ); <nl> object_property_set_bool ( obj , true , " realized ", & local_err ); <nl> if ( local_err ) {
typedef struct MirrorBlockJob { <nl>  <nl> unsigned long * in_flight_bitmap ; <nl> int in_flight ; <nl> - int sectors_in_flight ; <nl> + int64_t sectors_in_flight ; <nl> int ret ; <nl> bool unmap ; <nl> bool waiting_for_io ;
static int apic_init_common ( SysBusDevice * dev ) <nl>  <nl> sysbus_init_mmio ( dev , & s -> io_memory ); <nl>  <nl> - if (! vapic && s -> vapic_control & VAPIC_ENABLE_MASK ) { <nl> + /* Note : We need at least 1M to map the VAPIC option ROM */ <nl> + if (! vapic && s -> vapic_control & VAPIC_ENABLE_MASK && <nl> + ram_size >= 1024 * 1024 ) { <nl> vapic = sysbus_create_simple (" kvmvapic ", - 1 , NULL ); <nl> } <nl> s -> vapic = vapic ;
static void xlnx_ep108_init ( MachineState * machine ) <nl> machine -> ram_size = EP108_MAX_RAM_SIZE ; <nl> } <nl>  <nl> - if ( machine -> ram_size <= 0x08000000 ) { <nl> + if ( machine -> ram_size < 0x08000000 ) { <nl> qemu_log (" WARNING : RAM size " RAM_ADDR_FMT " is small for EP108 ", <nl> machine -> ram_size ); <nl> }
static target_ulong h_client_architecture_support ( PowerPCCPU * cpu , <nl> error_report_err ( local_err ); <nl> return H_HARDWARE ; <nl> } <nl> + error_free ( local_err ); <nl> local_err = NULL ; <nl> } <nl> }
e1000_link_up ( E1000State * s ) <nl> { <nl> s -> mac_reg [ STATUS ] |= E1000_STATUS_LU ; <nl> s -> phy_reg [ PHY_STATUS ] |= MII_SR_LINK_STATUS ; <nl> + <nl> + /* E1000_STATUS_LU is tested by e1000_can_receive () */ <nl> + qemu_flush_queued_packets ( qemu_get_queue ( s -> nic )); <nl> } <nl>  <nl> static bool
static void spapr_cpu_core_realize ( DeviceState * dev , Error ** errp ) <nl> } <nl>  <nl> err : <nl> - while ( i >= 0 ) { <nl> + while (-- i >= 0 ) { <nl> obj = sc -> threads + i * size ; <nl> object_unparent ( obj ); <nl> - i --; <nl> } <nl> g_free ( sc -> threads ); <nl> error_propagate ( errp , local_err );
static int get_device_guid ( <nl> & len ); <nl>  <nl> if ( status != ERROR_SUCCESS || name_type != REG_SZ ) { <nl> - return - 1 ; <nl> + ++ i ; <nl> + continue ; <nl> } <nl> else { <nl> if ( is_tap_win32_dev ( enum_name )) {
ObjectClass * object_class_dynamic_cast ( ObjectClass * class , <nl> TypeImpl * type = class -> type ; <nl> ObjectClass * ret = NULL ; <nl>  <nl> + if (! target_type ) { <nl> + /* target class type unknown , so fail the cast */ <nl> + return NULL ; <nl> + } <nl> + <nl> if ( type -> class -> interfaces && <nl> type_is_ancestor ( target_type , type_interface )) { <nl> int found = 0 ;
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> - if ( chdir ("/") < 0 ) { <nl> - do_perror (" chdir "); <nl> - goto error ; <nl> - } <nl> if ( chroot ( rpath ) < 0 ) { <nl> do_perror (" chroot "); <nl> goto error ; <nl> } <nl> + if ( chdir ("/") < 0 ) { <nl> + do_perror (" chdir "); <nl> + goto error ; <nl> + } <nl>  <nl> get_version = false ; <nl> # ifdef FS_IOC_GETVERSION
static int img_amend ( int argc , char ** argv ) <nl> if (! is_valid_option_list ( optarg )) { <nl> error_report (" Invalid option list : % s ", optarg ); <nl> ret = - 1 ; <nl> - goto out ; <nl> + goto out_no_progress ; <nl> } <nl> if (! options ) { <nl> options = g_strdup ( optarg ); <nl> static int img_amend ( int argc , char ** argv ) <nl> out : <nl> qemu_progress_end (); <nl>  <nl> + out_no_progress : <nl> blk_unref ( blk ); <nl> qemu_opts_del ( opts ); <nl> qemu_opts_free ( create_opts );
static void QEMU_NORETURN force_sig ( int target_sig ) <nl> * it to arrive . */ <nl> sigfillset (& act . sa_mask ); <nl> act . sa_handler = SIG_DFL ; <nl> + act . sa_flags = 0 ; <nl> sigaction ( host_sig , & act , NULL ); <nl>  <nl> /* For some reason raise ( host_sig ) doesn ' t send the signal when
static int qcow_create ( const char * filename , QemuOpts * opts , Error ** errp ) <nl> header_size += backing_filename_len ; <nl> } else { <nl> /* special backing file for vvfat */ <nl> + g_free ( backing_file ); <nl> backing_file = NULL ; <nl> } <nl> header . cluster_bits = 9 ; /* 512 byte cluster to avoid copying
static void virtqueue_map_desc ( unsigned int * p_num_sg , hwaddr * addr , struct iove <nl> } <nl>  <nl> iov [ num_sg ]. iov_base = cpu_physical_memory_map ( pa , & len , is_write ); <nl> + if (! iov [ num_sg ]. iov_base ) { <nl> + error_report (" virtio : bogus descriptor or out of resources "); <nl> + exit ( 1 ); <nl> + } <nl> + <nl> iov [ num_sg ]. iov_len = len ; <nl> addr [ num_sg ] = pa ; <nl> 
static int vhdx_log_flush_desc ( BlockDriverState * bs , VHDXLogDescriptor * desc , <nl> /* write ' count ' sectors of sector */ <nl> memset ( buffer , 0 , VHDX_LOG_SECTOR_SIZE ); <nl> count = desc -> zero_length / VHDX_LOG_SECTOR_SIZE ; <nl> + } else { <nl> + error_report (" Invalid VHDX log descriptor entry signature 0x %" PRIx32 , <nl> + desc -> signature ); <nl> + ret = - EINVAL ; <nl> + goto exit ; <nl> } <nl>  <nl> file_offset = desc -> file_offset ;
const char * bdrv_get_device_name ( BlockDriverState * bs ) <nl>  <nl> void bdrv_flush ( BlockDriverState * bs ) <nl> { <nl> + if (! bs -> drv ) <nl> + return ; <nl> if ( bs -> drv -> bdrv_flush ) <nl> bs -> drv -> bdrv_flush ( bs ); <nl> if ( bs -> backing_hd )
static void disas_arm_insn ( CPUARMState * env , DisasContext * s ) <nl> } <nl> ARCH ( 6 ); <nl> gen_srs ( s , ( insn & 0x1f ), ( insn >> 23 ) & 3 , insn & ( 1 << 21 )); <nl> + return ; <nl> } else if (( insn & 0x0e50ffe0 ) == 0x08100a00 ) { <nl> /* rfe */ <nl> int32_t offset ;
void ahci_uninit ( AHCIState * s ) <nl>  <nl> ide_exit ( s ); <nl> } <nl> + object_unparent ( OBJECT (& ad -> port )); <nl> } <nl>  <nl> g_free ( s -> dev );
int main ( int argc , char * argv []) <nl> const char * arch = qtest_get_arch (); <nl> FILE * f = fopen ( disk , " w "); <nl> int ret ; <nl> + <nl> + if (! f ) { <nl> + fprintf ( stderr , " Couldn ' t open \"% s \": % s ", disk , strerror ( errno )); <nl> + return 1 ; <nl> + } <nl> fwrite ( boot_sector , 1 , sizeof boot_sector , f ); <nl> fclose ( f ); <nl> 
static void nvdimm_dsm_set_label_data ( NVDIMMDevice * nvdimm , NvdimmDsmIn * in , <nl> return ; <nl> } <nl>  <nl> - assert ( sizeof (* in ) + sizeof (* set_label_data ) + set_label_data -> length <= <nl> - 4096 ); <nl> + assert ( offsetof ( NvdimmDsmIn , arg3 ) + <nl> + sizeof (* set_label_data ) + set_label_data -> length <= 4096 ); <nl>  <nl> nvc -> write_label_data ( nvdimm , set_label_data -> in_buf , <nl> set_label_data -> length , set_label_data -> offset );
static unsigned hpte_page_shift ( const struct ppc_one_seg_page_size * sps , <nl>  <nl> mask = (( 1ULL << ps -> page_shift ) - 1 ) & HPTE64_R_RPN ; <nl>  <nl> - if (( pte1 & mask ) == ( ps -> pte_enc << HPTE64_R_RPN_SHIFT )) { <nl> + if (( pte1 & mask ) == (( uint64_t ) ps -> pte_enc << HPTE64_R_RPN_SHIFT )) { <nl> return ps -> page_shift ; <nl> } <nl> }
void shpc_cleanup ( PCIDevice * d , MemoryRegion * bar ) <nl> SHPCDevice * shpc = d -> shpc ; <nl> d -> cap_present &= ~ QEMU_PCI_CAP_SHPC ; <nl> memory_region_del_subregion ( bar , & shpc -> mmio ); <nl> + object_unparent ( OBJECT (& shpc -> mmio )); <nl> /* TODO : cleanup config space changes ? */ <nl> g_free ( shpc -> config ); <nl> g_free ( shpc -> cmask );
static int qcow2_write_snapshots ( BlockDriverState * bs ) <nl>  <nl> id_str_size = strlen ( sn -> id_str ); <nl> name_size = strlen ( sn -> name ); <nl> + assert ( id_str_size <= UINT16_MAX && name_size <= UINT16_MAX ); <nl> h . id_str_size = cpu_to_be16 ( id_str_size ); <nl> h . name_size = cpu_to_be16 ( name_size ); <nl> offset = align_offset ( offset , 8 );
static int qio_channel_buffer_close ( QIOChannel * ioc , <nl> QIOChannelBuffer * bioc = QIO_CHANNEL_BUFFER ( ioc ); <nl>  <nl> g_free ( bioc -> data ); <nl> + bioc -> data = NULL ; <nl> bioc -> capacity = bioc -> usage = bioc -> offset = 0 ; <nl>  <nl> return 0 ;
void cpu_dump_state ( CPUPPCState * env , FILE * f , fprintf_function cpu_fprintf , <nl>  <nl> int i ; <nl>  <nl> + cpu_synchronize_state ( env ); <nl> + <nl> cpu_fprintf ( f , " NIP " TARGET_FMT_lx " LR " TARGET_FMT_lx " CTR " <nl> TARGET_FMT_lx " XER " TARGET_FMT_lx "\ n ", <nl> env -> nip , env -> lr , env -> ctr , env -> xer );
VIOsPAPRDevice * vty_lookup ( sPAPRMachineState * spapr , target_ulong reg ) <nl> return spapr_vty_get_default ( spapr -> vio_bus ); <nl> } <nl>  <nl> + if (! object_dynamic_cast ( OBJECT ( sdev ), TYPE_VIO_SPAPR_VTY_DEVICE )) { <nl> + return NULL ; <nl> + } <nl> + <nl> return sdev ; <nl> } <nl> 
void machine_register_compat_props ( MachineState * machine ) <nl>  <nl> for ( i = 0 ; i < mc -> compat_props -> len ; i ++) { <nl> p = g_array_index ( mc -> compat_props , GlobalProperty *, i ); <nl> + /* Machine compat_props must never cause errors : */ <nl> + p -> errp = & error_abort ; <nl> qdev_prop_register_global ( p ); <nl> } <nl> }
static GuestPCIAddress * get_pci_info ( char * guid , Error ** errp ) <nl> } else { <nl> error_setg_win32 ( errp , GetLastError (), <nl> " failed to get device name "); <nl> - goto out ; <nl> + goto free_dev_info ; <nl> } <nl> } <nl>  <nl> static GuestPCIAddress * get_pci_info ( char * guid , Error ** errp ) <nl> pci -> bus = bus ; <nl> break ; <nl> } <nl> + <nl> + free_dev_info : <nl> + SetupDiDestroyDeviceInfoList ( dev_info ); <nl> out : <nl> g_free ( buffer ); <nl> g_free ( name );
int hvf_vcpu_exec ( CPUState * cpu ) <nl> macvm_set_rip ( cpu , rip + ins_len ); <nl> break ; <nl> case VMX_REASON_VMCALL : <nl> - /* TODO : inject # GP fault */ <nl> + env -> exception_injected = EXCP0D_GPF ; <nl> + env -> has_error_code = true ; <nl> + env -> error_code = 0 ; <nl> break ; <nl> default : <nl> error_report ("% llx : unhandled exit % llx \ n ", rip , exit_reason );
void qio_channel_test_run_reader ( QIOChannelTest * test , <nl>  <nl> void qio_channel_test_validate ( QIOChannelTest * test ) <nl> { <nl> + g_assert ( test -> readerr == NULL ); <nl> + g_assert ( test -> writeerr == NULL ); <nl> g_assert_cmpint ( memcmp ( test -> input , <nl> test -> output , <nl> test -> len ), ==, 0 ); <nl> - g_assert ( test -> readerr == NULL ); <nl> - g_assert ( test -> writeerr == NULL ); <nl>  <nl> g_free ( test -> inputv ); <nl> g_free ( test -> outputv );
retry : <nl>  <nl> /* Make sure that all offsets in the " allocated " range are representable <nl> * in an int64_t */ <nl> - if ( s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) { <nl> + if ( s -> free_cluster_index > 0 && <nl> + s -> free_cluster_index - 1 > ( INT64_MAX >> s -> cluster_bits )) <nl> + { <nl> return - EFBIG ; <nl> } <nl> 
void qemu_system_guest_panicked ( void ) <nl> } <nl> qapi_event_send_guest_panicked ( GUEST_PANIC_ACTION_PAUSE , & error_abort ); <nl> vm_stop ( RUN_STATE_GUEST_PANICKED ); <nl> + if (! no_shutdown ) { <nl> + qapi_event_send_guest_panicked ( GUEST_PANIC_ACTION_POWEROFF , <nl> + & error_abort ); <nl> + qemu_system_shutdown_request (); <nl> + } <nl> } <nl>  <nl> void qemu_system_reset_request ( void )
static void sdhci_sdma_transfer_multi_blocks ( SDHCIState * s ) <nl> boundary_count -= block_size - begin ; <nl> } <nl> dma_memory_read (& address_space_memory , s -> sdmasysad , <nl> - & s -> fifo_buffer [ begin ], s -> data_count ); <nl> + & s -> fifo_buffer [ begin ], s -> data_count - begin ); <nl> s -> sdmasysad += s -> data_count - begin ; <nl> if ( s -> data_count == block_size ) { <nl> for ( n = 0 ; n < block_size ; n ++) {
static int iscsi_truncate ( BlockDriverState * bs , int64_t offset ) <nl> if ( iscsilun -> allocationmap != NULL ) { <nl> g_free ( iscsilun -> allocationmap ); <nl> iscsilun -> allocationmap = <nl> - bitmap_new ( DIV_ROUND_UP ( bs -> total_sectors , <nl> + bitmap_new ( DIV_ROUND_UP ( sector_lun2qemu ( iscsilun -> num_blocks , <nl> + iscsilun ), <nl> iscsilun -> cluster_sectors )); <nl> } <nl> 
const char * path ( const char * name ) <nl> { <nl> /* Only do absolute paths : quick and dirty , but should mostly be OK . <nl> Could do relative by tracking cwd . */ <nl> - if (! base || name [ 0 ] != '/') <nl> + if (! base || ! name || name [ 0 ] != '/') <nl> return name ; <nl>  <nl> return follow_path ( base , name ) ?: name ;
sofree ( struct socket * so ) <nl> if ( so -> so_next && so -> so_prev ) <nl> remque ( so ); /* crashes if so is not in a queue */ <nl>  <nl> + if ( so -> so_tcpcb ) { <nl> + free ( so -> so_tcpcb ); <nl> + } <nl> free ( so ); <nl> } <nl> 
static int vmdk_parse_extents ( const char * desc , BlockDriverState * bs , <nl> } else { <nl> ret = vmdk_open_sparse ( bs , extent_file , bs -> open_flags , buf , errp ); <nl> } <nl> + g_free ( buf ); <nl> if ( ret ) { <nl> - g_free ( buf ); <nl> bdrv_unref ( extent_file ); <nl> return ret ; <nl> }
static void hpet_ram_writel ( void * opaque , target_phys_addr_t addr , <nl> ( timer -> config & HPET_TN_SETVAL )) <nl> timer -> cmp = ( timer -> cmp & 0xffffffff00000000ULL ) <nl> | new_val ; <nl> - else { <nl> + if ( timer_is_periodic ( timer )) { <nl> /* <nl> * FIXME : Clamp period to reasonable min value ? <nl> * Clamp period to reasonable max value
tcp_sockclosed ( struct tcpcb * tp ) <nl> DEBUG_CALL (" tcp_sockclosed "); <nl> DEBUG_ARG (" tp = % p ", tp ); <nl>  <nl> + if (! tp ) { <nl> + return ; <nl> + } <nl> + <nl> switch ( tp -> t_state ) { <nl>  <nl> case TCPS_CLOSED : <nl> tcp_sockclosed ( struct tcpcb * tp ) <nl> tp -> t_state = TCPS_LAST_ACK ; <nl> break ; <nl> } <nl> - if ( tp ) <nl> - tcp_output ( tp ); <nl> + tcp_output ( tp ); <nl> } <nl>  <nl> /*
static int grow_refcount_table ( BlockDriverState * bs , int min_size ) <nl> qcow2_free_clusters ( bs , old_table_offset , old_table_size * sizeof ( uint64_t )); <nl> return 0 ; <nl> fail : <nl> - qcow2_free_clusters ( bs , table_offset , new_table_size2 ); <nl> qemu_free ( new_table ); <nl> return - EIO ; <nl> }
static void tcg_out_movi ( TCGContext * s , TCGType type , <nl> { <nl> tcg_target_long hi , lo = ( int32_t ) arg ; <nl>  <nl> + /* Make sure we test 32 - bit constants for imm13 properly . */ <nl> + if ( type == TCG_TYPE_I32 ) { <nl> + arg = lo ; <nl> + } <nl> + <nl> /* A 13 - bit constant sign - extended to 64 - bits . */ <nl> if ( check_fit_tl ( arg , 13 )) { <nl> tcg_out_movi_imm13 ( s , ret , arg );
static void dec_barrel ( DisasContext * dc ) <nl> tcg_gen_shr_tl ( cpu_R [ dc -> rd ], cpu_R [ dc -> ra ], t0 ); <nl> } <nl> } <nl> + tcg_temp_free ( t0 ); <nl> } <nl>  <nl> static void dec_bit ( DisasContext * dc )
static void virtio_net_vhost_status ( VirtIONet * n , uint8_t status ) <nl> return ; <nl> } <nl>  <nl> - if (!! n -> vhost_started == virtio_net_started ( n , status ) && <nl> - ! nc -> peer -> link_down ) { <nl> + if (!! n -> vhost_started == <nl> + ( virtio_net_started ( n , status ) && ! nc -> peer -> link_down )) { <nl> return ; <nl> } <nl> if (! n -> vhost_started ) {
static void sd_reset ( SDState * sd , BlockDriverState * bdrv ) <nl> } else { <nl> sect = 0 ; <nl> } <nl> - sect <<= 9 ; <nl> - <nl> - size = sect + 1 ; <nl> + size = sect << 9 ; <nl>  <nl> sect = ( size >> ( HWBLOCK_SHIFT + SECTOR_SHIFT + WPGROUP_SHIFT )) + 1 ; <nl> 
int main ( int argc , char ** argv ) <nl> root = tmpfs ; <nl> } <nl>  <nl> - socket_path = g_strdup_printf ("/ tmp / vhost -% d . sock ", getpid ()); <nl> + socket_path = g_strdup_printf ("% s / vhost . sock ", tmpfs ); <nl>  <nl> /* create char dev and add read handlers */ <nl> qemu_add_opts (& qemu_chardev_opts );
static int img_convert ( int argc , char ** argv ) <nl> goto out ; <nl> } <nl>  <nl> - out_bs = bdrv_new_open ( out_filename , out_fmt , BDRV_O_FLAGS | BDRV_O_RDWR ); <nl> + out_bs = bdrv_new_open ( out_filename , out_fmt , <nl> + BDRV_O_FLAGS | BDRV_O_RDWR | BDRV_O_NO_FLUSH ); <nl> if (! out_bs ) { <nl> ret = - 1 ; <nl> goto out ;
PCIBus * pci_get_bus_devfn ( int * devfnp , PCIBus * root , const char * devaddr ) <nl> int dom , bus ; <nl> unsigned slot ; <nl>  <nl> - assert (! root -> parent_dev ); <nl> - <nl> if (! root ) { <nl> fprintf ( stderr , " No primary PCI bus \ n "); <nl> return NULL ; <nl> } <nl>  <nl> + assert (! root -> parent_dev ); <nl> + <nl> if (! devaddr ) { <nl> * devfnp = - 1 ; <nl> return pci_find_bus_nr ( root , 0 );
int qemu_fsdev_add ( QemuOpts * opts ) <nl>  <nl> if ( fsle -> fse . ops -> parse_opts ) { <nl> if ( fsle -> fse . ops -> parse_opts ( opts , & fsle -> fse )) { <nl> + g_free ( fsle -> fse . fsdev_id ); <nl> + g_free ( fsle ); <nl> return - 1 ; <nl> } <nl> }
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> /* root directory */ <nl> int cur = s -> directory . next ; <nl> array_ensure_allocated (&( s -> directory ), ROOT_ENTRIES - 1 ); <nl> + s -> directory . next = ROOT_ENTRIES ; <nl> memset ( array_get (&( s -> directory ), cur ), 0 , <nl> ( ROOT_ENTRIES - cur ) * sizeof ( direntry_t )); <nl> }
static int stdio_pclose ( void * opaque ) <nl> QEMUFileStdio * s = opaque ; <nl> int ret ; <nl> ret = pclose ( s -> stdio_file ); <nl> + if ( ret == - 1 ) { <nl> + ret = - errno ; <nl> + } <nl> g_free ( s ); <nl> return ret ; <nl> }
uint64_t helper_msub64_q_ssov ( CPUTriCoreState * env , uint64_t r1 , uint32_t r2 , <nl> } else { <nl> result = INT64_MIN ; <nl> } <nl> + } else { <nl> + env -> PSW_USB_V = 0 ; <nl> } <nl> } else { <nl> if ( ovf < 0 ) {
static int parallels_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail_options ; <nl> } <nl>  <nl> - if (!( flags & BDRV_O_RESIZE ) || ! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> - bdrv_truncate ( bs -> file , bdrv_getlength ( bs -> file -> bs ), <nl> - PREALLOC_MODE_OFF , NULL ) != 0 ) { <nl> + if (! bdrv_has_zero_init ( bs -> file -> bs )) { <nl> s -> prealloc_mode = PRL_PREALLOC_MODE_FALLOCATE ; <nl> } <nl> 
int do_drive_del ( Monitor * mon , const QDict * qdict , QObject ** ret_data ) <nl> */ <nl> if ( bdrv_get_attached_dev ( bs )) { <nl> bdrv_make_anon ( bs ); <nl> + <nl> + /* Further I / O must not pause the guest */ <nl> + bdrv_set_on_error ( bs , BLOCKDEV_ON_ERROR_REPORT , <nl> + BLOCKDEV_ON_ERROR_REPORT ); <nl> } else { <nl> drive_uninit ( drive_get_by_blockdev ( bs )); <nl> }
static int create_dynamic_disk ( BlockBackend * blk , uint8_t * buf , <nl> num_bat_entries = ( total_sectors + block_size / 512 ) / ( block_size / 512 ); <nl>  <nl> ret = blk_pwrite ( blk , offset , buf , HEADER_SIZE ); <nl> - if ( ret ) { <nl> + if ( ret < 0 ) { <nl> goto fail ; <nl> } <nl> 
* significant half of a uint64_t struct member . <nl> */ <nl> # ifdef HOST_WORDS_BIGENDIAN <nl> -# define offsetoflow32 ( S , M ) offsetof ( S , M + sizeof ( uint32_t )) <nl> +# define offsetoflow32 ( S , M ) ( offsetof ( S , M ) + sizeof ( uint32_t )) <nl> # else <nl> # define offsetoflow32 ( S , M ) offsetof ( S , M ) <nl> # endif
static int check_refblocks ( BlockDriverState * bs , BdrvCheckResult * res , <nl> * nb_clusters ); <nl> memset (&(* refcount_table )[ old_nb_clusters ], 0 , <nl> (* nb_clusters - old_nb_clusters ) * <nl> - sizeof ( uint16_t )); <nl> + sizeof (** refcount_table )); <nl> } <nl> (* refcount_table )[ cluster ]--; <nl> inc_refcounts ( bs , res , * refcount_table , * nb_clusters ,
static TypeInfo arm_gic_info = { <nl> . parent = TYPE_ARM_GIC_COMMON , <nl> . instance_size = sizeof ( gic_state ), <nl> . class_init = arm_gic_class_init , <nl> + . class_size = sizeof ( ARMGICClass ), <nl> }; <nl>  <nl> static void arm_gic_register_types ( void )
int main ( int argc , char ** argv ) { <nl> datalen = 0 ; <nl> } <nl> code = malloc ( strlen ( p )+ 1 ); <nl> + if (! code ) { <nl> + return 1 ; <nl> + } <nl> codelen = r_hex_str2bin ( p , code ); <nl> if (! arch ) arch = " x86 "; <nl> if (! bits ) bits = 32 ;
R_API RList * r_io_map_get_maps_in_range ( RIO * io , ut64 addr , ut64 endaddr ) { <nl> RIOMap * map ; <nl> RListIter * iter ; <nl> RList * maps = r_list_new (); <nl> + maps -> free = NULL ; <nl> r_list_foreach ( io -> maps , iter , map ) { <nl> if ( map -> from <= addr && addr < map -> to ) r_list_append ( maps , map ); <nl> // if ( map -> from == addr && endaddr == map -> to ) r_list_append ( maps , map );
static void create_dummy_nodes ( RAGraph * g ) { <nl> const RListIter * it ; <nl> const RGraphEdge * e ; <nl>  <nl> - g -> long_edges = r_list_new (); <nl> + g -> long_edges = r_list_newf (( RListFree ) free ); <nl> dummy_vis . data = g -> long_edges ; <nl> dummy_vis . tree_edge = ( RGraphEdgeCallback ) view_dummy ; <nl> dummy_vis . fcross_edge = ( RGraphEdgeCallback ) view_dummy ;
R_API bool r_sign_save ( RAnal * a , const char * file ) { <nl> if (! a || ! file ) { <nl> return false ; <nl> } <nl> + <nl> + if ( sdb_count ( a -> sdb_zigns ) == 0 ) { <nl> + eprintf (" WARNING : no zignatures to save \ n "); <nl> + return false ; <nl> + } <nl>  <nl> Sdb * db = sdb_new ( NULL , file , 0 ); <nl> if (! db ) {
static void bin_mach0_versioninfo ( RCore * r ) { <nl> static int bin_versioninfo ( RCore * r , int mode ) { <nl> const RBinInfo * info = r_bin_get_info ( r -> bin ); <nl>  <nl> + if (!( info && info -> rclass )) return false ; <nl> + <nl> if (! strncmp (" pe ", info -> rclass , 2 )) { <nl> bin_pe_versioninfo ( r ); <nl> } else if (! strncmp (" elf ", info -> rclass , 3 )) {
R_API int r_bp_size ( RBreakpoint * bp ) { <nl> int i , bpsize = 8 ; <nl> for ( i = 0 ; bp -> cur -> bps [ i ]. bytes ; i ++) { <nl> bpa = & bp -> cur -> bps [ i ]; <nl> - if ( bpa -> bits != bp -> bits ) { <nl> + if ( bpa -> bits && bpa -> bits != bp -> bits ) { <nl> continue ; <nl> } <nl> if ( bpa -> length < bpsize ) {
static int dex_loadcode ( RBinFile * arch , RBinDexObj * bin ) { <nl> for ( i = 0 ; i < bin -> header . method_size ; i ++) { <nl> // RBinDexMethod * method = & bin -> methods [ i ]; <nl> if (! methods [ i ]) { <nl> + if ( i >= bin -> header . class_size ) continue ; <nl> struct dex_class_t * c = & bin -> classes [ i ]; <nl> char * class_name = dex_class_name ( bin , c ); <nl> if ( class_name ) {
static char * r_debug_bf_reg_profile ( RDebug * dbg ) { <nl> ); <nl> } <nl>  <nl> - static int r_debug_bf_breakpoint ( void * bp , RBreakpointItem * b , bool set ) { <nl> + static int r_debug_bf_breakpoint ( struct r_bp_t * bp , RBreakpointItem * b , bool set ) { <nl> // r_io_system ( dbg -> iob . io , " db "); <nl> return false ; <nl> }
eprintf ("-- % s \ n ", buf ); <nl> } <nl> case ' C ': /* comment */ <nl> if ( input [ 1 ] == '+') { <nl> - const char * text , * newcomment = input + 2 ; <nl> + const char * newcomment = input + 2 ; <nl> + char * text ; <nl> while (* newcomment ==' ') newcomment ++; <nl> char * comment = r_meta_get_string ( <nl> core -> anal , R_META_TYPE_COMMENT , addr );
static int formatDisassembledOperand ( char * strOperand , int operandNum , const dis <nl> char binary [ 9 ]; <nl> int retVal ; <nl>  <nl> + if ( operandNum >= AVR_MAX_NUM_OPERANDS ) <nl> + return 0 ; <nl> + <nl> switch ( dInstruction . instruction -> operandTypes [ operandNum ]) { <nl> case OPERAND_NONE : <nl> case OPERAND_REGISTER_GHOST :
static Sdb * store_versioninfo_gnu_verneed ( struct Elf_ ( r_bin_elf_obj_t ) * bin , Elf <nl> i += entry -> vn_next ; <nl> snprintf ( key , sizeof ( key ), " version % d ", cnt ); <nl> sdb_ns_set ( sdb , key , sdb_version ); <nl> + // if entry -> vn_next is 0 it iterate infinitely <nl> + if (! entry -> vn_next ) break ; <nl> } <nl> free ( need ); <nl> return sdb ;
struct r_bin_zimg_obj_t * r_bin_zimg_new_buf ( RBuffer * buf ) { <nl> goto fail ; <nl> } <nl>  <nl> + if ( r_buf_size ( bin -> b ) < sizeof ( struct zimg_header_t )) { <nl> + goto fail ; <nl> + } <nl> bin -> header = (*( struct zimg_header_t *) bin -> b -> buf ); <nl>  <nl> return bin ;
static void r_bin_file_free ( void /* RBinFile */ * bf_ ) { <nl> if ( a -> curxtr && a -> curxtr -> destroy ) <nl> a -> curxtr -> free_xtr (( void *) ( a -> xtr_obj )); <nl>  <nl> - r_bin_object_free ( a -> o ); <nl> + r_list_free ( a -> objs ); <nl> a -> o = NULL ; <nl> r_buf_free ( a -> buf ); <nl> // TODO : unset related sdb namespaces
riscv_dis ( RAsm * a , RAsmOp * rop , const ut8 * buf , ut64 len ) { <nl> memcpy (& insn , buf , 4 ); <nl> riscv_disassemble ( a , rop , insn , a -> bits ); <nl>  <nl> - return 4 ; <nl> + return riscv_insn_length ( insn ); <nl> }
static int __plugin_open ( RIO * io , const char * file , ut8 many ) { <nl>  <nl> static RIODesc * __open ( RIO * io , const char * file , int rw , int mode ) { <nl> int ret , pid = getpid (); <nl> + if ( r_sandbox_enable ( 0 )) <nl> + return NULL ; <nl> io -> va = R_TRUE ; // nop <nl> ret = update_self_regions ( pid ); <nl> if ( ret ) {
static void get_chain_data ( proxy_data * pd , unsigned int * proxy_count , chain_typ <nl> } <nl> } <nl> fclose ( file ); <nl> + if (! count ) { <nl> + fprintf ( stderr , " error : no valid proxy found in config \ n "); <nl> + exit ( 1 ); <nl> + } <nl> * proxy_count = count ; <nl> proxychains_got_chain_data = 1 ; <nl> }
dispatch_f ( struct VSL_data * vsl , struct VSL_transaction * const pt [], <nl> for ( t = pt [ 0 ]; t != NULL ; t = *++ pt ) { <nl> CTX . gen ++; <nl> if ( t -> type != VSL_t_req ) <nl> + /* Only look at client requests */ <nl> + continue ; <nl> + if ( t -> reason == VSL_r_esi ) <nl> + /* Skip ESI requests */ <nl> continue ; <nl> CTX . hitmiss = "-"; <nl> CTX . handling = "-";
parse_new ( struct vcc * tl ) <nl> vcc_ErrWhere ( tl , tl -> t ); <nl> return ; <nl> } <nl> - XXXAZ ( sy1 ); <nl>  <nl> sy1 = VCC_AddSymbolTok ( tl , tl -> t , SYM_NONE ); // XXX : NONE ? <nl> XXXAN ( sy1 );
do_list ( struct cli * cli , struct director * d , void * priv ) <nl> if ( d -> vdir -> admin_health == VDI_AH_DELETED ) <nl> return ( 0 ); <nl>  <nl> + // XXX admin health " probe " for the no - probe case is confusing <nl> VCLI_Out ( cli , "\ n %- 30s %- 7s ", d -> vdir -> cli_name , VDI_Ahealth ( d )); <nl>  <nl> if ( d -> vdir -> methods -> list != NULL )
read_tmo ( int fd , char * ptr , unsigned len , double tmo ) <nl> pfd . events = POLLIN ; <nl> for ( j = 0 ; len > 0 ; ) { <nl> i = poll (& pfd , 1 , to ); <nl> + if ( i < 0 ) { <nl> + errno = EINTR ; <nl> + return (- 1 ); <nl> + } <nl> if ( i == 0 ) { <nl> errno = ETIMEDOUT ; <nl> return (- 1 );
struct mg_request_info { <nl> int remote_port ; // Client ' s port <nl> int is_ssl ; // 1 if SSL - ed , 0 if not <nl> void * user_data ; // User data pointer passed to mg_start () <nl> + void * conn_data ; // Connection - specific user data <nl>  <nl> int num_headers ; // Number of HTTP headers <nl> struct mg_header {
void logto ( char * logfile ) { <nl> uwsgi . logfile = logfile ; <nl>  <nl> if ( uwsgi . chmod_logfile_value ) { <nl> - if ( chmod ( uwsgi . logfile , uwsgi . chmod_logfile_value )) { <nl> - uwsgi_error (" chmod ()"); <nl> + if ( fchmod ( fd , uwsgi . chmod_logfile_value )) { <nl> + uwsgi_error (" fchmod ()"); <nl> } <nl> } <nl> }
static char * amqp_simple_get_frame ( int fd , struct amqp_frame_header * fh ) { <nl> while ( len < fh -> size + 1 ) { <nl> rlen = recv ( fd , ptr , ( fh -> size + 1 )- len , 0 ); <nl> if ( rlen <= 0 ) { <nl> - if ( rlen < 0 ) <nl> + if ( rlen < 0 ) { <nl> uwsgi_error (" recv ()"); <nl> + } <nl> + free ( frame ); <nl> return NULL ; <nl> } <nl> len += rlen ;
next : <nl> } <nl>  <nl> // this must be called only by the master !!! <nl> - if ( uwsgi . mywid > 0 ) return ; <nl> + if (! uwsgi . workers ) return ; <nl> + if ( uwsgi . workers [ 0 ]. pid != getpid ()) return ; <nl> uwsgi_legion_announce_death (); <nl> } <nl> 
static void uwsgi_alarm_thread_loop ( struct uwsgi_thread * ut ) { <nl> long ptr = 0 ; <nl> memcpy (& ptr , buf , sizeof ( long )); <nl> struct uwsgi_alarm_instance * uai = ( struct uwsgi_alarm_instance *) ptr ; <nl> - if (! uai ) return ; <nl> + if (! uai ) <nl> + break ; <nl> uwsgi_alarm_run ( uai , msg , msg_size ); <nl> } <nl> } <nl> } <nl> + free ( buf ); <nl> } <nl>  <nl> // initialize alarms , instances and log regexps
int http_parse ( struct http_session * h_session ) { <nl> hv = hv -> next ; <nl> } <nl>  <nl> + // security check <nl> + if ( c >= MAX_HTTP_VEC - 4 ) { <nl> + uwsgi_log (" too much headers in request . skipping it .\ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> return c ; <nl>  <nl> }
char * uwsgi_cgi_get_docroot ( char * path_info , uint16_t path_info_len , int * need_f <nl> } <nl>  <nl> if ( choosen_udd -> status == 0 ) { <nl> - char * tmp_udd = realpath ( path , NULL ); <nl> - if (! tmp_udd ) { <nl> + char * tmp_udd = uwsgi_malloc ( PATH_MAX + 1 ); <nl> + if (! realpath ( path , tmp_udd )) { <nl> return NULL ; <nl> } <nl> 
int uwsgi_start ( void * v_argv ) { <nl> # ifndef __OpenBSD__ <nl>  <nl> if ( uwsgi . rl . rlim_max > 0 ) { <nl> + uwsgi . rl . rlim_cur = uwsgi . rl . rlim_max ; <nl> uwsgi_log (" limiting address space of processes ...\ n "); <nl> if ( setrlimit ( RLIMIT_AS , & uwsgi . rl )) { <nl> uwsgi_error (" setrlimit ()");
void uwsgi_python_harakiri ( int wid ) { <nl> char * address = uwsgi_concat2 ( up . tracebacker , uwsgi_num2str ( wid )); <nl>  <nl> int fd = uwsgi_connect ( address , - 1 , 0 ); <nl> - for (;;) { <nl> + while ( fd >= 0 ) { <nl> int ret = uwsgi_waitfd ( fd , uwsgi . shared -> options [ UWSGI_OPTION_SOCKET_TIMEOUT ]); <nl> if ( ret <= 0 ) { <nl> break ;
void uwsgi_opt_set_placeholder ( char * opt , char * value , void * none ) { <nl>  <nl> p [ 0 ] = 0 ; <nl> add_exported_option ( uwsgi_str ( value ), p + 1 , 1 ); <nl> - p [ 1 ] = '='; <nl> + p [ 0 ] = '='; <nl>  <nl> } <nl> 
static v8 :: Handle < v8 :: Value > uwsgi_v8_commonjs_require ( const v8 :: Arguments & <nl> free ( tmp_filename ); <nl> return ret ; <nl> } <nl> + free ( tmp_filename ); <nl> } <nl> - free ( tmp_filename ); <nl> usl = usl -> next ; <nl> } <nl> }
static void uwsgi_pypy_onload () { <nl> # ifdef UWSGI_PYPY_HOME <nl> upypy . home = UWSGI_PYPY_HOME ; <nl> # endif <nl> + uwsgi . has_threads = 1 ; <nl> } <nl>  <nl> static int uwsgi_pypy_mule ( char * opt ) {
static void mongrel2_connect () { <nl> } <nl> char * responder = strchr ( uwsgi_sock -> name , ','); <nl> if (! responder ) { <nl> - uwsgi_log (" invalid zeromq address \ n "); <nl> + uwsgi_log (" invalid zeromq address : % s \ n ", uwsgi_sock -> name ); <nl> exit ( 1 ); <nl> } <nl> uwsgi_sock -> receiver = uwsgi_concat2n ( uwsgi_sock -> name , responder - uwsgi_sock -> name , "", 0 );
error : <nl> if ( urcc -> key ) free ( urcc -> key ); <nl> if ( urcc -> name ) free ( urcc -> name ); <nl> if ( urcc -> expires_str ) free ( urcc -> expires_str ); <nl> + free ( urcc ); <nl> return - 1 ; <nl> } <nl> 
void * uwsgi_python_tracebacker_thread ( void * foobar ) { <nl> uwsgi . no_defer_accept = current_defer_accept ; <nl>  <nl> PyObject * traceback_module = PyImport_ImportModule (" traceback "); <nl> - if (! traceback_module ) return NULL ; <nl> + if (! traceback_module ) { <nl> + free ( str_wid ); <nl> + free ( sock_path ); <nl> + close ( fd ); <nl> + return NULL ; <nl> + } <nl> PyObject * traceback_dict = PyModule_GetDict ( traceback_module ); <nl> PyObject * extract_stack = PyDict_GetItemString ( traceback_dict , " extract_stack "); <nl> 
void uwsgi_python_reset_random_seed () { <nl> void uwsgi_python_atexit () { <nl>  <nl> // if hijacked do not run atexit hooks <nl> + if ( uwsgi . workers [ uwsgi . mywid ]. hijacked ) <nl> + return ; <nl>  <nl> // this time we use this higher level function <nl> // as this code can be executed in a signal handler
PyObject * py_uwsgi_gevent_graceful ( PyObject * self , PyObject * args ) { <nl>  <nl> void uwsgi_gevent_gbcw () { <nl>  <nl> - uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> - <nl> py_uwsgi_gevent_graceful ( NULL , NULL ); <nl> + <nl> + uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> + exit ( 0 ); <nl> } <nl>  <nl> struct wsgi_request * uwsgi_gevent_current_wsgi_req ( void ) {
void unit_attack ( <nl> int damage_left = damage ; <nl> while ( damage_left > 0 && ! animator . would_end ()) { <nl> int step_left = ( animator . get_end_time () - animator . get_animation_time () )/ 50 ; <nl> - int removed_hp = damage_left / step_left ; <nl> + int removed_hp = step_left ? damage_left / step_left : 1 ; <nl> if ( removed_hp < 1 ) removed_hp = 1 ; <nl> if ( step_left < 1 ) step_left = 1 ; <nl> defender . take_hit ( removed_hp );
SYNCED_COMMAND_HANDLER_FUNCTION ( attack , child , /* use_undo */, show , error_handler <nl> } <nl> } <nl>  <nl> - if ( size_t ( weapon_num ) >= u -> attacks (). size ()) { <nl> + if ( static_cast < unsigned >( weapon_num ) >= u -> attacks (). size ()) { <nl> error_handler (" illegal weapon type in attack \ n ", true ); <nl> return false ; <nl> }
public : <nl> size_t size () const { return lmap_ . size (); } <nl> size_t num_iters () const ; <nl>  <nl> + bool empty () const { return lmap_ . empty (); } <nl> + <nl> void clear ( bool force = false ); <nl>  <nl> /**
void get_player_info ( const config & cfg , game_state & gamestate , std :: string save_ <nl> LOG_NG << " found gold : '" << gold << "'\ n "; <nl>  <nl> int ngold = lexical_cast_default < int >( gold ); <nl> - if ( player != NULL && player -> gold >= ngold ) { <nl> + if ( ( player != NULL && player -> gold >= ngold ) || snapshot ) { <nl> ngold = player -> gold ; <nl> } <nl> 
void team :: team_info :: write ( config & cfg ) const <nl> cfg [" hidden "] = hidden ; <nl> cfg [" suppress_end_turn_confirmation "] = no_turn_confirmation ; <nl> cfg [" scroll_to_leader "] = scroll_to_leader ; <nl> - cfg [" controller "] = controller_string (); <nl> + cfg [" controller "] = ( controller == IDLE ? " human " : controller_string ()); <nl>  <nl> std :: stringstream can_recruit_str ; <nl> for ( std :: set < std :: string >:: const_iterator cr = can_recruit . begin (); cr != can_recruit . end (); ++ cr ) {
void show_about ( display & disp ) <nl> text . push_back ("+ Developers "); <nl> text . push_back ("- Alfredo Beaumont ( ziberpunk )"); <nl> text . push_back ("- Cyril Bouthors ( CyrilB )"); <nl> - text . push_back ("- Guillaume Duwelz - Rebert "); <nl> text . push_back ("- Isaac Clerencia "); <nl> text . push_back ("- J . R . Blain ( Cowboy )"); <nl> text . push_back ("- Justin Zaun ( jzaun )");
void textbox :: set_text ( std :: string text ) <nl> { <nl> text_ = string_to_wstring ( text ); <nl> cursor_ = text_ . size (); <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> } <nl> void textbox :: clear () <nl> cursor_ = 0 ; <nl> cursor_pos_ = 0 ; <nl> text_pos_ = 0 ; <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> }
game_launcher :: game_launcher ( const commandline_options & cmdline_opts , const char <nl> const std :: string app_basename = filesystem :: base_name ( appname ); <nl> jump_to_editor_ = app_basename . find (" editor ") != std :: string :: npos ; <nl>  <nl> + if ( cmdline_opts_ . core_id ) { <nl> + preferences :: set_core_id (* cmdline_opts_ . core_id ); <nl> + } <nl> if ( cmdline_opts_ . campaign ) { <nl> jump_to_campaign_ . jump_ = true ; <nl> jump_to_campaign_ . campaign_id_ = * cmdline_opts_ . campaign ;
bool loadgame :: load_multiplayer_game () <nl> return false ; <nl> } <nl>  <nl> + if ( is_replay_save ( summary_ )) { <nl> + gui2 :: show_transient_message ( video_ , _ (" Load Game "), _ (" Replays are not supported in multiplayer mode .")); <nl> + return false ; <nl> + } <nl> + <nl> if ( gamestate_ . classification (). campaign_type != game_classification :: CAMPAIGN_TYPE :: MULTIPLAYER ) { <nl> gui2 :: show_transient_error_message ( video_ , _ (" This is not a multiplayer save .")); <nl> return false ;
void unit :: write ( config & cfg ) const <nl> break ; <nl> case unit_type :: LIMINAL : <nl> cfg [" alignment "] = " liminal "; <nl> + break ; <nl> default : <nl> cfg [" alignment "] = " neutral "; <nl> }
WML_HANDLER_FUNCTION ( replace_schedule , /* event_info */, cfg ) <nl> ERR_NG << " attempted to to replace ToD schedule with empty schedule \ n "; <nl> } else { <nl> resources :: tod_manager -> replace_schedule ( cfg . get_parsed_config ()); <nl> + resources :: screen -> new_turn (); <nl> LOG_NG << " replaced ToD schedule \ n "; <nl> } <nl> }
void display :: clear_redraw_observers () <nl>  <nl> void display :: draw ( bool update , bool force ) { <nl> // log_scope (" display :: draw "); <nl> - if ( screen_ . update_locked ()) { <nl> + if ( screen_ . update_locked () || ( SDL_GetAppState () & SDL_APPACTIVE ) == 0 ) { <nl> return ; <nl> } <nl> bool changed = draw_init ();
SOCKET_STATE receive_buf ( TCPsocket sock , std :: vector < char >& buf ) <nl> } <nl> } <nl>  <nl> - const ssize_t res = SDLNet_TCP_Recv ( sock , beg , end - beg ); <nl> + const int res = SDLNet_TCP_Recv ( sock , beg , end - beg ); <nl> if ( res <= 0 ) { <nl> if ( SDLNet_CheckSockets ( set , 15000 ) <= 0 ) { <nl> ERR_NW << " SDLNet_CheckSockets : " << strerror ( errno ) << "\ n ";
namespace { <nl> net_manager_ ( min_thread , max_thread ), <nl> server_manager_ ( load_config ()), <nl> hooks_ (), <nl> - input_ ( 0 ) <nl> + input_ ( 0 ), <nl> + compress_level_ ( 0 ) <nl> { <nl> if ( cfg_ . child (" campaigns ") == NULL ) { <nl> cfg_ . add_child (" campaigns ");
namespace game_config <nl> namespace sounds { <nl> const std :: string turn_bell = " bell . wav ", <nl> timer_bell = " timer . wav ", <nl> - receive_message = " chat - 3 . ogg ", <nl> + receive_message = " chat - 1 . ogg , chat - 2 . ogg , chat - 3 . ogg , chat - 4 . ogg ", <nl> receive_message_highlight = " chat - highlight . ogg ", <nl> receive_message_friend = " chat - 4 . ogg ", <nl> receive_message_server = " receive . wav ",
loadscreen :: loadscreen ( CVideo & screen , const int & percent ): <nl> setconfig_counter ( 0 ), <nl> parser_counter ( 0 ), <nl> screen_ ( screen ), <nl> + textarea_ (), <nl> + logo_surface_ ( NULL ), <nl> logo_drawn_ ( false ), <nl> pby_offset_ ( 0 ), <nl> prcnt_ ( percent )
void wait :: start_game () <nl>  <nl> LOG_NW << " starting game \ n "; <nl> sound :: play_UI_sound ( game_config :: sounds :: mp_game_begins ); <nl> + game_display :: get_singleton ()-> send_notification ( _ (" Wesnoth "), _ (" Game has begun !")); <nl> } <nl>  <nl> void wait :: layout_children ( const SDL_Rect & rect )
version_info :: version_info ( unsigned int major , unsigned int minor , unsigned int <nl> } <nl>  <nl> version_info :: version_info ( const std :: string & str ) <nl> - : special_ (""), special_separator_ ('\ 0 '), sane_ ( true ) <nl> + : nums_ () <nl> + , special_ ("") <nl> + , special_separator_ ('\ 0 ') <nl> + , sane_ ( true ) <nl> { <nl> const std :: vector < std :: string > string_parts = utils :: split ( str ,'.'); <nl> // first two components are required to be valid numbers , though
void widget :: set_visible ( const visibility visible ) <nl> visible_ = visible ; <nl>  <nl> if ( need_resize ) { <nl> - if ( new_widgets ) { <nl> + if ( visible == visibility :: visible && new_widgets ) { <nl> event :: message message ; <nl> fire ( event :: REQUEST_PLACEMENT , * this , message ); <nl> } else {
public : <nl> static void set_sunset ( const unsigned interval ) <nl> { sunset_ = interval ? interval : 5 ; } <nl>  <nl> + bool get_need_layout () const { return need_layout_ ; } <nl> + <nl> private : <nl>  <nl> /** Needed so we can change what ' s drawn on the screen . */
time_of_day :: time_of_day ( const config & cfg ) <nl> time_of_day :: time_of_day () <nl> : lawful_bonus ( 0 ) <nl> , bonus_modified ( 0 ) <nl> +, image () <nl> , name (" NULL_TOD ") <nl> , id (" nulltod ") <nl> +, image_mask () <nl> , red ( 0 ) <nl> , green ( 0 ) <nl> , blue ( 0 ) <nl> +, sounds () <nl> { <nl> } <nl> 
bool game :: describe_slots () { <nl> std :: string descr = buf . str (); <nl>  <nl> if ((* description_ )[" slots "] != descr ) { <nl> - description_ -> set_attr_dup (" slots ", descr ); <nl> + description_ -> set_attr_dup (" slots ", descr . c_str ()); <nl> return true ; <nl> } else { <nl> return false ;
struct tiff { <nl> */ <nl> # ifndef ReadOK <nl> # define ReadOK ( tif , buf , size ) \ <nl> - ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t ) size ) == ( tsize_t ) size ) <nl> + ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t )( size )) == ( tsize_t )( size )) <nl> # endif <nl> # ifndef SeekOK <nl> # define SeekOK ( tif , off ) \
tdata_t <nl> _TIFFCheckMalloc ( TIFF * tif , size_t nmemb , size_t elem_size , const char * what ) <nl> { <nl> - tdata_t * cp = NULL ; <nl> + tdata_t cp = NULL ; <nl> tsize_t bytes = nmemb * elem_size ; <nl>  <nl> /*
EstimateStripByteCounts ( TIFF * tif , TIFFDirEntry * dir , uint16 dircount ) <nl> td -> td_stripbytecount = ( uint64 *) <nl> _TIFFCheckMalloc ( tif , td -> td_nstrips , sizeof ( uint64 ), <nl> " for \" StripByteCounts \" array "); <nl> + if ( td -> td_stripbytecount == NULL ) <nl> + return - 1 ; <nl> + <nl> if ( td -> td_compression != COMPRESSION_NONE ) { <nl> uint64 space ; <nl> uint64 filesize ;
tsize_t t2p_readwrite_pdf_image_tile ( T2P * t2p , TIFF * input , TIFF * output , ttile_ <nl> return ( 0 ); <nl> } <nl> if ( TIFFGetField ( input , TIFFTAG_JPEGTABLES , & count , & jpt ) != 0 ) { <nl> - if ( count >= 4 ) { <nl> + if ( count > 4 ) { <nl> int retTIFFReadRawTile ; <nl> /* Ignore EOI marker of JpegTables */ <nl> _TIFFmemcpy ( buffer , jpt , count - 2 );
static int lxc_spawn ( struct lxc_handler * handler ) <nl> int preserve_mask = 0 , i ; <nl> int netpipepair [ 2 ], nveths ; <nl>  <nl> + netpipe = - 1 ; <nl> + <nl> for ( i = 0 ; i < LXC_NS_MAX ; i ++) <nl> if ( handler -> conf -> inherit_ns_fd [ i ] != - 1 ) <nl> preserve_mask |= ns_info [ i ]. clone_flag ;
static void print_usage ( const struct option longopts []) <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_version () <nl> + static void print_version ( void ) <nl> { <nl> printf ("% s \ n ", LXC_VERSION ); <nl> exit ( 0 ); <nl> } <nl>  <nl> - static void print_help () <nl> + static void print_help ( void ) <nl> { <nl> fprintf ( stderr , "\ <nl> Usage : lxc - init -- name = NAME -- COMMAND \ n \
int lxc_abstract_unix_connect ( const char * path ) <nl>  <nl> if ( connect ( fd , ( struct sockaddr *)& addr , offsetof ( struct sockaddr_un , sun_path ) + len )) { <nl> int tmp = errno ; <nl> + /* special case to connect to older containers */ <nl> + if ( connect ( fd , ( struct sockaddr *)& addr , sizeof ( addr )) == 0 ) <nl> + return fd ; <nl> process_lock (); <nl> close ( fd ); <nl> process_unlock ();
static int keyboard_feed_evdev ( idev_keyboard * k , idev_data * data ) { <nl> /* TODO : update LEDs */ <nl> } <nl>  <nl> - if ( num < 0 ) <nl> + if ( num < 0 ) { <nl> + r = num ; <nl> goto error ; <nl> + } <nl>  <nl> r = keyboard_fill ( k , & k -> evdata , data -> resync , ev -> code , ev -> value , num , keysyms ); <nl> if ( r < 0 )
int dhcp_packet_verify_headers ( DHCPPacket * packet , size_t len , bool checksum ) { <nl>  <nl> /* UDP */ <nl>  <nl> + if ( packet -> ip . protocol != IPPROTO_UDP ) { <nl> + log_dhcp_client ( client , " ignoring packet : not UDP "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( len < DHCP_IP_UDP_SIZE ) { <nl> log_dhcp_client ( client , " ignoring packet : packet (% zu bytes ) " <nl> " smaller than IP + UDP header (% u bytes )", len ,
int dhcp6_option_parse_ip6addrs ( uint8_t * optval , uint16_t optlen , <nl>  <nl> int dhcp6_option_parse_domainname ( const uint8_t * optval , uint16_t optlen , char *** str_arr ) { <nl> size_t pos = 0 , idx = 0 ; <nl> - _cleanup_free_ char ** names = NULL ; <nl> + _cleanup_strv_free_ char ** names = NULL ; <nl> int r ; <nl>  <nl> assert_return ( optlen > 1 , - ENODATA );
int main ( int argc , char * argv [], char * envp []) <nl> udev_init_config (); <nl>  <nl> /* set signal handlers */ <nl> + memset (& act , 0x00 , sizeof ( act )); <nl> act . sa_handler = ( void (*) ( int )) sig_handler ; <nl> sigemptyset (& act . sa_mask ); <nl> act . sa_flags = 0 ;
static int load_group_database ( void ) { <nl> static int make_backup ( const char * target , const char * x ) { <nl> _cleanup_close_ int src = - 1 ; <nl> _cleanup_fclose_ FILE * dst = NULL ; <nl> - char * backup , * temp ; <nl> + _cleanup_free_ char * temp = NULL ; <nl> + char * backup ; <nl> struct timespec ts [ 2 ]; <nl> struct stat st ; <nl> int r ;
static int create_symlink ( const char * verb , const char * old_path , const char * ne <nl> return 1 ; <nl> } <nl>  <nl> + free ( dest ); <nl> return 0 ; <nl> } <nl> 
int unit_file_get_list ( <nl> } <nl> } <nl>  <nl> - return r ; <nl> + return 0 ; <nl> } <nl>  <nl> static const char * const unit_file_state_table [ _UNIT_FILE_STATE_MAX ] = {
int main ( int argc , char * argv [], char * envp []) <nl>  <nl> reload_config = 1 ; <nl> buf = malloc ( nbytes ); <nl> - if ( buf != NULL ) { <nl> + if ( buf == NULL ) { <nl> err (" error getting buffer for inotify , disable watching "); <nl> close ( inotify_fd ); <nl> inotify_fd = - 1 ;
const SyscallFilterSet syscall_filter_sets [ _SYSCALL_FILTER_SET_MAX ] = { <nl> " reboot \ 0 " <nl> }, <nl> [ SYSCALL_FILTER_SET_RESOURCES ] = { <nl> - /* Alter resource settings */ <nl> . name = "@ resources ", <nl> + . help = " Alter resource settings ", <nl> . value = <nl> " sched_setparam \ 0 " <nl> " sched_setscheduler \ 0 "
int main ( int argc , char * argv []) { <nl> return EXIT_FAILURE ; <nl> } <nl>  <nl> - if ( streq ( argv [ 1 ], " load ") && shall_restore_state ()) { <nl> + if ( streq ( argv [ 1 ], " load ")) { <nl> _cleanup_free_ char * value = NULL ; <nl>  <nl> + if (! shall_restore_state ()) <nl> + return EXIT_SUCCESS ; <nl> + <nl> r = read_one_line_file ( saved , & value ); <nl> if ( r < 0 ) { <nl> 
static int idev_evdev_io ( idev_evdev * evdev ) { <nl>  <nl> error : <nl> idev_evdev_hup ( evdev ); <nl> - return r ; <nl> + return 0 ; /* idev_evdev_hup () handles the error so discard it */ <nl> } <nl>  <nl> static int idev_evdev_event_fn ( sd_event_source * s , int fd , uint32_t revents , void * userdata ) {
static int dispatch_wqueue ( sd_bus * bus ) { <nl> * it got full , then all bets are off <nl> * anyway . */ <nl>  <nl> - sd_bus_message_unref ( bus -> wqueue [ 0 ]); <nl> bus -> wqueue_size --; <nl> + sd_bus_message_unref ( bus -> wqueue [ 0 ]); <nl> memmove ( bus -> wqueue , bus -> wqueue + 1 , sizeof ( sd_bus_message *) * bus -> wqueue_size ); <nl> bus -> windex = 0 ; <nl> 
static int service_dispatch_timer ( sd_event_source * source , usec_t usec , void * us <nl>  <nl> case SERVICE_RELOAD : <nl> log_unit_warning ( UNIT ( s ), " Reload operation timed out . Stopping ."); <nl> + service_unwatch_control_pid ( s ); <nl> + service_kill_control_processes ( s ); <nl> s -> reload_result = SERVICE_FAILURE_TIMEOUT ; <nl> service_enter_running ( s , SERVICE_SUCCESS ); <nl> break ;
int main ( int argc , char * argv []) { <nl>  <nl> umask ( 0022 ); <nl>  <nl> + /* Refuse to run unless we are in an initrd () */ <nl> + if (! in_initrd ()) <nl> + return EXIT_SUCCESS ; <nl> + <nl> device = argv [ 1 ]; <nl>  <nl> if ( stat ( device , & st ) < 0 ) {
int config_parse_exec_nice ( <nl> assert ( rvalue ); <nl> assert ( data ); <nl>  <nl> + if ( isempty ( rvalue )) { <nl> + c -> nice_set = false ; <nl> + return 0 ; <nl> + } <nl> + <nl> r = parse_nice ( rvalue , & priority ); <nl> if ( r < 0 ) { <nl> if ( r == - ERANGE )
static int parse_line ( const char * fname , unsigned line , const char * buffer , bool <nl> } <nl> } else { <nl> existing = new0 ( ItemArray , 1 ); <nl> + if (! existing ) <nl> + return log_oom (); <nl> + <nl> r = ordered_hashmap_put ( h , i . path , existing ); <nl> if ( r < 0 ) <nl> return log_oom ();
int bus_exec_context_set_transient_property ( <nl> } else { <nl> _cleanup_free_ char * joined = NULL ; <nl>  <nl> + r = strv_extend_strv (& c -> pass_environment , l , true ); <nl> + if ( r < 0 ) <nl> + return r ; <nl> + <nl> /* We write just the new settings out to file , with unresolved specifiers . */ <nl> joined = unit_concat_strv ( l , UNIT_ESCAPE_SPECIFIERS ); <nl> if (! joined )
static int unit_find_paths ( <nl> _cleanup_free_ char * template = NULL ; <nl>  <nl> r = unit_name_template ( unit_name , & template ); <nl> - if ( r != - EINVAL ) <nl> + if ( r < 0 && r != - EINVAL ) <nl> return log_error_errno ( r , " Failed to determine template name : % m "); <nl> if ( r >= 0 ) { <nl> r = unit_file_find_path ( lp , template , & path );
static int event_make_signal_data ( <nl> d -> priority = priority ; <nl>  <nl> r = hashmap_put ( e -> signal_data , & d -> priority , d ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( d ); <nl> return r ; <nl> + } <nl>  <nl> added = true ; <nl> }
static int search_and_fopen_internal ( const char * path , const char * mode , const c <nl> _cleanup_free_ char * p = NULL ; <nl> FILE * f ; <nl>  <nl> - p = strjoin (* i , "/", path , NULL ); <nl> + if ( root ) <nl> + p = strjoin ( root , * i , "/", path , NULL ); <nl> + else <nl> + p = strjoin (* i , "/", path , NULL ); <nl> if (! p ) <nl> return - ENOMEM ; <nl> 
int manager_handle_action ( <nl>  <nl> n = manager_count_displays ( m ); <nl> if ( n != 1 ) { <nl> - log_debug (" Ignoring lid switch request , % s displays connected ."); <nl> + log_debug (" Ignoring lid switch request , % i displays connected .", n ); <nl> return 0 ; <nl> } <nl> }
static void output_unit_file_list ( const UnitFileList * units , unsigned c ) { <nl> } else <nl> id_cols = max_id_len ; <nl>  <nl> - if (! arg_no_legend ) <nl> + if (! arg_no_legend && c > 0 ) <nl> printf ("%-* s %-* s \ n ", <nl> id_cols , " UNIT FILE ", <nl> state_cols , " STATE ");
static int get_file_to_edit ( <nl> return log_oom (); <nl>  <nl> if ( arg_runtime ) { <nl> - run = strjoin ( paths -> runtime_config , name , NULL ); <nl> + run = strjoin ( paths -> runtime_config , "/", name , NULL ); <nl> if (! run ) <nl> return log_oom (); <nl> }
char * <nl> utf8_prev_char ( const char * p ) <nl> { <nl> - while ( 1 ) <nl> + for (;;) <nl> { <nl> p --; <nl> if ((* p & 0xc0 ) != 0x80 )
static int logind_check_inhibitors ( enum action a ) { <nl> if (! on_tty ()) <nl> return 0 ; <nl>  <nl> + if ( arg_transport != BUS_TRANSPORT_LOCAL ) <nl> + return 0 ; <nl> + <nl> r = acquire_bus ( BUS_FULL , & bus ); <nl> if ( r < 0 ) <nl> return r ;
static int client_receive_message_udp ( <nl> if ( buflen < 0 ) <nl> return buflen ; <nl>  <nl> + if ( buflen == 0 ) <nl> + buflen = 1 ; <nl> + <nl> message = malloc0 ( buflen ); <nl> if (! message ) <nl> return - ENOMEM ;
fallback : <nl> return - errno ; <nl> } <nl>  <nl> + free ( parent ); <nl> + parent = NULL ; <nl> + <nl> r = path_get_parent ( t , & parent ); <nl> if ( r < 0 ) <nl> return r ;
int bus_message_print_all_properties ( <nl> return log_oom (); <nl>  <nl> r = set_put (* found_properties , name ); <nl> - if ( r < 0 && r != EEXIST ) <nl> + if ( r < 0 && r != - EEXIST ) <nl> return log_oom (); <nl> } <nl> 
struct udev_device * udev_monitor_receive_device ( struct udev_monitor * udev_monito <nl> udev_device_unref ( udev_device ); <nl> return NULL ; <nl> } <nl> - udev_device_set_devnum ( udev_device , makedev ( maj , min )); <nl> + if ( maj > 0 ) <nl> + udev_device_set_devnum ( udev_device , makedev ( maj , min )); <nl> udev_device_set_info_loaded ( udev_device ); <nl> return udev_device ; <nl> }
int main ( int argc , char * argv []) { <nl> } <nl> } <nl>  <nl> + free ( arg_root_what ); <nl> + <nl> return r < 0 ? EXIT_FAILURE : EXIT_SUCCESS ; <nl> }
int fopen_temporary ( const char * path , FILE ** _f , char ** _temp_path ) { <nl>  <nl> f = fdopen ( fd , " we "); <nl> if (! f ) { <nl> - unlink ( t ); <nl> + unlink_noerrno ( t ); <nl> free ( t ); <nl> safe_close ( fd ); <nl> return - errno ;
int cg_is_empty_recursive ( const char * controller , const char * path ) { <nl>  <nl> assert ( path ); <nl>  <nl> + /* The root cgroup is always populated */ <nl> + if ( controller && ( isempty ( path ) || path_equal ( path , "/"))) <nl> + return 0 ; <nl> + <nl> r = cg_is_empty ( controller , path ); <nl> if ( r <= 0 ) <nl> return r ;
static DBusHandlerResult locale_message_handler ( <nl> " Locale \ 0 "); <nl> if (! changed ) <nl> goto oom ; <nl> - } <nl> + } else <nl> + strv_free ( l ); <nl> + <nl> } else if ( dbus_message_is_method_call ( message , " org . freedesktop . locale1 ", " SetVConsoleKeyboard ")) { <nl>  <nl> const char * keymap , * keymap_toggle ;
static void output_draw ( Output * o , bool menu , term_screen * screen ) { <nl> */ <nl>  <nl> static void terminal_dirty ( Terminal * t ) { <nl> - uint64_t usec ; <nl> + usec_t usec ; <nl> int r ; <nl>  <nl> assert ( t );
subst : <nl> cpos ++; <nl> while ( isspace ( cpos [ 0 ])) <nl> cpos ++; <nl> + if ( cpos [ 0 ] == '\ 0 ') <nl> + break ; <nl> } <nl> if ( i > 0 ) { <nl> log_error (" requested part of result string not found ");
static int map_netif ( sd_bus * bus , const char * member , sd_bus_message * m , sd_bus_ <nl> r = sd_bus_message_read_array ( m , SD_BUS_TYPE_INT32 , & v , & l ); <nl> if ( r < 0 ) <nl> return r ; <nl> + if ( r == 0 ) <nl> + return - EBADMSG ; <nl>  <nl> i -> n_netif = l / sizeof ( int32_t ); <nl> i -> netif = memdup ( v , l );
int cg_pid_get_path ( const char * controller , pid_t pid , char ** path ) { <nl> if (! p ) <nl> return - ENOMEM ; <nl>  <nl> + /* Truncate suffix indicating the process is a zombie */ <nl> + e = endswith ( p , " ( deleted )"); <nl> + if ( e ) <nl> + * e = 0 ; <nl> + <nl> * path = p ; <nl> return 0 ; <nl> }
int switch_root ( const char * new_root ) { <nl> snprintf ( new_mount , sizeof ( new_mount ), "% s % s ", new_root , i ); <nl> char_array_0 ( new_mount ); <nl>  <nl> - mkdir_parents ( new_mount , 0755 ); <nl> + mkdir_p ( new_mount , 0755 ); <nl>  <nl> if (( stat ( new_mount , & sb ) < 0 ) || <nl> sb . st_dev != new_root_stat . st_dev ) {
static int export_legacy_dbus_address ( <nl> _cleanup_free_ char * s = NULL ; <nl> int r ; <nl>  <nl> + /* skip export if kdbus is not active */ <nl> + if ( access ("/ dev / kdbus ", F_OK ) < 0 ) <nl> + return PAM_SUCCESS ; <nl> + <nl> if ( asprintf (& s , KERNEL_USER_BUS_FMT ";" UNIX_USER_BUS_FMT , <nl> ( unsigned long ) uid , runtime ) < 0 ) { <nl> pam_syslog ( handle , LOG_ERR , " Failed to set bus variable .");
bool path_is_safe ( const char * p ) { <nl> if ( streq ( p , "..") || startswith ( p , "../") || endswith ( p , "/..") || strstr ( p , "/../")) <nl> return false ; <nl>  <nl> - if ( strlen ( p ) > PATH_MAX ) <nl> + if ( strlen ( p )+ 1 > PATH_MAX ) <nl> return false ; <nl>  <nl> /* The following two checks are not really dangerous , but hey , they still are confusing */
static int mount_load_proc_self_mountinfo ( Manager * m , bool set_flags ) { <nl> options = mnt_fs_get_options ( fs ); <nl> fstype = mnt_fs_get_fstype ( fs ); <nl>  <nl> + if (! device || ! path ) <nl> + continue ; <nl> + <nl> if ( cunescape ( device , UNESCAPE_RELAX , & d ) < 0 ) <nl> return log_oom (); <nl> 
static int output_cat ( sd_journal * j , OutputMode mode , unsigned line , <nl>  <nl> r = sd_journal_get_data ( j , " MESSAGE ", & data , & l ); <nl> if ( r < 0 ) { <nl> + /* An entry without MESSAGE =? */ <nl> + if ( r == - ENOENT ) <nl> + return 0 ; <nl> + <nl> log_error (" Failed to get data : % s ", strerror (- r )); <nl> return r ; <nl> }
static int json_parse_tokens ( JsonVariant ** tokens , size_t ntokens , JsonVariant * <nl> size_t it = 0 ; <nl> int r ; <nl> JsonVariant * e ; <nl> - _cleanup_jsonunref_ JsonVariant * p ; <nl> + _cleanup_jsonunref_ JsonVariant * p = NULL ; <nl>  <nl> assert ( tokens ); <nl> assert ( ntokens );
int bus_socket_read_message ( sd_bus * bus ) { <nl> return - EIO ; <nl> } <nl>  <nl> - f = realloc ( bus -> fds , sizeof ( int ) + ( bus -> n_fds + n )); <nl> + f = realloc ( bus -> fds , sizeof ( int ) * ( bus -> n_fds + n )); <nl> if (! f ) { <nl> close_many (( int *) CMSG_DATA ( cmsg ), n ); <nl> return - ENOMEM ;
int server_flush_to_var ( Server * s , bool require_flag_file ) { <nl> r = 0 ; <nl>  <nl> finish : <nl> - journal_file_post_change ( s -> system_journal ); <nl> + if ( s -> system_journal ) <nl> + journal_file_post_change ( s -> system_journal ); <nl>  <nl> s -> runtime_journal = journal_file_close ( s -> runtime_journal ); <nl> 
void initialize_srand ( void ) { <nl>  <nl> auxv = ( void *) getauxval ( AT_RANDOM ); <nl> if ( auxv ) { <nl> - assert_cc ( sizeof ( x ) < 16 ); <nl> + assert_cc ( sizeof ( x ) <= 16 ); <nl> memcpy (& x , auxv , sizeof ( x )); <nl> } else <nl> # endif
static void manager_clear_jobs_and_units ( Manager * m ) { <nl>  <nl> m -> n_on_console = 0 ; <nl> m -> n_running_jobs = 0 ; <nl> + m -> n_installed_jobs = 0 ; <nl> + m -> n_failed_jobs = 0 ; <nl> } <nl>  <nl> Manager * manager_free ( Manager * m ) {
int button_open ( Button * b ) { <nl> } <nl>  <nl> ( void ) button_set_mask ( b ); <nl> - <nl> + <nl> + b -> io_event_source = sd_event_source_unref ( b -> io_event_source ); <nl> r = sd_event_add_io ( b -> manager -> event , & b -> io_event_source , b -> fd , EPOLLIN , button_dispatch , b ); <nl> if ( r < 0 ) { <nl> log_error_errno ( r , " Failed to add button event : % m ");
int dns_packet_is_reply_for ( DnsPacket * p , const DnsResourceKey * key ) { <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> + if (! p -> question ) <nl> + return 0 ; <nl> + <nl> if ( p -> question -> n_keys != 1 ) <nl> return 0 ; <nl> 
static int parse_date ( const char ** p , CalendarSpec * c ) { <nl> c -> month = first ; <nl> c -> day = second ; <nl> return 0 ; <nl> - } else if ( c -> end_of_month ) <nl> + } else if ( c -> end_of_month ) { <nl> + free_chain ( first ); <nl> + free_chain ( second ); <nl> return - EINVAL ; <nl> + } <nl>  <nl> if (* t == '~') <nl> c -> end_of_month = true ;
static int service_collect_fds ( Service * s , int ** fds , unsigned * n_fds ) { <nl> p = manager_get_unit ( UNIT ( s )-> meta . manager , k ); <nl> free ( k ); <nl>  <nl> + if (! p ) <nl> + continue ; <nl> + <nl> if (( r = socket_collect_fds ( SOCKET ( p ), & cfds , & cn_fds )) < 0 ) <nl> goto fail ; <nl> 
static int client_initialize_time_events ( sd_dhcp_client * client ) { <nl>  <nl> r = sd_event_source_set_priority ( client -> timeout_resend , <nl> client -> event_priority ); <nl> + if ( r < 0 ) <nl> + goto error ; <nl>  <nl> r = sd_event_source_set_description ( client -> timeout_resend , " dhcp4 - resend - timer "); <nl> if ( r < 0 )
int udev_monitor_filter_update ( struct udev_monitor * udev_monitor ) <nl> bpf_stmt ( ins , & i , BPF_RET | BPF_K , 0xffffffff ); <nl>  <nl> /* install filter */ <nl> + memset (& filter , 0x00 , sizeof ( filter )); <nl> filter . len = i ; <nl> filter . filter = ins ; <nl> err = setsockopt ( udev_monitor -> sock , SOL_SOCKET , SO_ATTACH_FILTER , & filter , sizeof ( filter ));
static int handle_response ( sd_resolve * resolve , const Packet * packet , size_t len <nl>  <nl> if ( ni_resp -> hostlen > DNS_HOSTNAME_MAX || <nl> ni_resp -> servlen > DNS_HOSTNAME_MAX || <nl> - sizeof ( NameInfoResponse ) + ni_resp -> hostlen + ni_resp -> servlen > length + 2 ) <nl> + sizeof ( NameInfoResponse ) + ni_resp -> hostlen + ni_resp -> servlen > length ) <nl> ASSIGN_ERRNO ( q , EAI_SYSTEM , EIO , 0 ); <nl>  <nl> else {
static int set_usb_mass_storage_ifsubtype ( char * to , const char * from , size_t len <nl> type = " floppy "; <nl> break ; <nl> case 1 : /* RBC devices */ <nl> + type = " rbc "; <nl> + break ; <nl> case 6 : /* Transparent SPC - 2 devices */ <nl> - type = " disk "; <nl> + type = " scsi "; <nl> break ; <nl> default : <nl> break ;
void _logsys_log_printf ( <nl> subsysid = LOGSYS_MAX_SUBSYS_COUNT ; <nl> } <nl>  <nl> + if (( level > logsys_loggers [ subsysid ]. syslog_priority ) && <nl> + ( level > logsys_loggers [ subsysid ]. logfile_priority )) { <nl> + return ; <nl> + } <nl> + <nl> va_start ( ap , format ); <nl> len = vsprintf ( logsys_print_buffer , format , ap ); <nl> va_end ( ap );
saEvtEventAttributesSet ( <nl> struct event_data_instance * edi ; <nl> int i ; <nl>  <nl> + if ( priority < SA_EVT_HIGHEST_PRIORITY || <nl> + priority > SA_EVT_LOWEST_PRIORITY ) { <nl> + return SA_AIS_ERR_INVALID_PARAM ; <nl> + } <nl> + <nl> error = saHandleInstanceGet (& event_handle_db , eventHandle , <nl> ( void *)& edi ); <nl> if ( error != SA_AIS_OK ) {
poll_handle poll_create ( <nl> poll_instance -> poll_entries = 0 ; <nl> poll_instance -> ufds = 0 ; <nl> poll_instance -> poll_entry_count = 0 ; <nl> - poll_instance -> serialize_lock_fn = serialize_unlock_fn ; <nl> + poll_instance -> serialize_lock_fn = serialize_lock_fn ; <nl> poll_instance -> serialize_unlock_fn = serialize_unlock_fn ; <nl> timerlist_init (& poll_instance -> timerlist ); <nl> 
static void unlink_all_completed ( void ) <nl> * here <nl> */ <nl> serialize_unlock (); <nl> + api -> timer_delete ( corosync_stats_timer_handle ); <nl> poll_stop ( corosync_poll_handle ); <nl> totempg_finalize (); <nl> 
int amfReadNetwork ( char ** error_string , <nl> int res = 0 ; <nl> int line_number = 0 ; <nl>  <nl> + memset ( mcast_addr , 0 , sizeof ( struct sockaddr_in )); <nl> + memset ( bindnet_addr , 0 , sizeof ( struct sockaddr_in )); <nl>  <nl> mcast_addr -> sin_family = AF_INET ; <nl> fp = fopen ("/ etc / ais / network . conf ", " r ");
static int init_nss_hash ( struct crypto_instance * instance ) <nl> } <nl>  <nl> hash_param . type = siBuffer ; <nl> - hash_param . data = 0 ; <nl> - hash_param . len = 0 ; <nl> + hash_param . data = instance -> private_key ; <nl> + hash_param . len = instance -> private_key_len ; <nl>  <nl> hash_slot = PK11_GetBestSlot ( hash_to_nss [ instance -> crypto_hash_type ], NULL ); <nl> if ( hash_slot == NULL ) {
static void memb_state_gather_enter ( <nl>  <nl> instance -> memb_state = MEMB_STATE_GATHER ; <nl> instance -> stats . gather_entered ++; <nl> - instance -> stats . continuous_gather ++; <nl> + <nl> + if ( gather_from == 3 ) { <nl> + /* <nl> + * State 3 means gather , so we are continuously gathering . <nl> + */ <nl> + instance -> stats . continuous_gather ++; <nl> + } <nl>  <nl> if ( instance -> stats . continuous_gather > MAX_NO_CONT_GATHER ) { <nl> log_printf ( instance -> totemsrp_log_level_warning ,
ev_document_misc_get_screen_dpi ( GdkScreen * screen ) <nl>  <nl> /* diagonal in pixels */ <nl> dp = hypot ( gdk_screen_get_width ( screen ), gdk_screen_get_height ( screen )); <nl> + if ( dp == 0 ) <nl> + return 96 ; <nl>  <nl> /* diagonal in inches */ <nl> di = hypot ( gdk_screen_get_width_mm ( screen ), gdk_screen_get_height_mm ( screen )) / 25 . 4 ; <nl> + if ( di == 0 ) <nl> + return 96 ; <nl>  <nl> return ( dp / di ); <nl> }
ev_view_motion_notify_event ( GtkWidget * widget , <nl> } <nl>  <nl> if ( view -> scroll_info . autoscrolling ) { <nl> - view -> scroll_info . last_y = y ; <nl> + if ( y >= 0 ) <nl> + view -> scroll_info . last_y = y ; <nl> return TRUE ; <nl> } <nl> 
ev_view_presentation_transition_start ( EvViewPresentation * pview ) <nl>  <nl> duration = ev_document_transition_get_page_duration ( EV_DOCUMENT_TRANSITION ( pview -> document ), <nl> pview -> current_page ); <nl> - if ( duration > 0 ) { <nl> + if ( duration >= 0 ) { <nl> pview -> trans_timeout_id = <nl> g_timeout_add_seconds ( duration , <nl> ( GSourceFunc ) transition_next_page ,
ev_view_accessible_focus_changed ( GtkWidget * widget , <nl> g_return_val_if_fail ( EV_IS_VIEW ( widget ), FALSE ); <nl> g_return_val_if_fail ( EV_IS_VIEW_ACCESSIBLE ( self ), FALSE ); <nl>  <nl> - if ( self -> priv -> children == NULL ) <nl> + if ( self -> priv -> children == NULL || self -> priv -> children -> len == 0 ) <nl> return FALSE ; <nl>  <nl> page_accessible = g_ptr_array_index ( self -> priv -> children ,
ev_view_select_all ( EvView * view ) <nl> } <nl>  <nl> merge_selection_region ( view , g_list_reverse ( selections )); <nl> - gtk_widget_queue_draw ( GTK_WIDGET ( view )); <nl> } <nl>  <nl> gboolean
gimp_prop_widget_new_from_pspec ( GObject * config , <nl>  <nl> buffer = gimp_prop_text_buffer_new ( config , pspec -> name , - 1 ); <nl> view = gtk_text_view_new_with_buffer ( buffer ); <nl> + g_object_unref ( buffer ); <nl>  <nl> widget = gtk_scrolled_window_new ( NULL , NULL ); <nl> gtk_scrolled_window_set_shadow_type ( GTK_SCROLLED_WINDOW ( widget ),
gimp_device_manager_device_added ( GdkDeviceManager * gdk_manager , <nl> GdkDisplay * display ; <nl> GimpDeviceInfo * device_info ; <nl>  <nl> + if ( gdk_device_get_source ( device ) == GDK_SOURCE_KEYBOARD ) <nl> + return ; <nl> + <nl> display = gdk_device_manager_get_display ( gdk_manager ); <nl>  <nl> device_info =
gimp_config_path_expand_only ( const gchar * path , <nl> s = gimp_plug_in_directory (); <nl> else if ( strcmp ( token , " gimp_sysconf_dir ") == 0 ) <nl> s = gimp_sysconf_directory (); <nl> + else if ( strcmp ( token , " gimp_installation_dir ") == 0 ) <nl> + s = gimp_installation_directory (); <nl>  <nl> if (! s ) <nl> s = g_getenv ( token );
choose_format ( GeglBuffer * buffer , <nl> break ; <nl>  <nl> case GIMP_SELECT_CRITERION_LCH_L : <nl> + format = babl_format (" CIE L alpha float "); <nl> + break ; <nl> + <nl> case GIMP_SELECT_CRITERION_LCH_C : <nl> case GIMP_SELECT_CRITERION_LCH_H : <nl> format = babl_format (" CIE LCH ( ab ) alpha float ");
d_load_object ( gchar * desc , <nl> if ( sscanf ( buf , "% d ", & new_obj -> type_data ) != 1 ) <nl> { <nl> g_message (" Error while loading object ( no type data )"); <nl> + g_free ( new_obj ); <nl> return NULL ; <nl> } <nl>  <nl> d_load_object ( gchar * desc , <nl> if ( strcmp ("</ EXTRA >", buf )) <nl> { <nl> g_message (" Syntax error while loading object "); <nl> + g_free ( new_obj ); <nl> return NULL ; <nl> } <nl> /* Go around and read the last line */
save_layer ( const gchar * filename , <nl> gimp_filename_to_utf8 ( filename )); <nl>  <nl> /* Attempt to open the output file */ <nl> - if (( outfile = g_fopen ( filename , " wb +")) == NULL ) <nl> + if (( outfile = g_fopen ( filename , " w + b ")) == NULL ) <nl> { <nl> g_set_error ( error , G_FILE_ERROR , <nl> g_file_error_from_errno ( errno ),
gimp_container_tree_view_constructor ( GType type , <nl> tree_view -> main_column = gtk_tree_view_column_new (); <nl> gtk_tree_view_insert_column ( tree_view -> view , tree_view -> main_column , 0 ); <nl>  <nl> + gtk_tree_view_set_expander_column ( tree_view -> view , tree_view -> main_column ); <nl> + <nl> tree_view -> renderer_cell = gimp_cell_renderer_viewable_new (); <nl> gtk_tree_view_column_pack_start ( tree_view -> main_column , <nl> tree_view -> renderer_cell ,
gimp_image_map_tool_response ( GtkWidget * widget , <nl>  <nl> gimp_image_flush ( gimp_display_get_image ( tool -> display )); <nl>  <nl> - if ( image_map_tool -> config ) <nl> + if ( image_map_tool -> config && image_map_tool -> settings_box ) <nl> gimp_settings_box_add_current ( GIMP_SETTINGS_BOX ( image_map_tool -> settings_box ), <nl> GIMP_GUI_CONFIG ( tool -> tool_info -> gimp -> config )-> image_map_tool_max_recent ); <nl> }
gimp_tile_backend_plugin_command ( GeglTileSource * tile_store , <nl> break ; <nl>  <nl> default : <nl> - g_assert ( command < GEGL_TILE_LAST_COMMAND && command >= 0 ); <nl> + /* g_assert ( command < GEGL_TILE_LAST_COMMAND && command >= 0 ); */ <nl> + break ; <nl> } <nl>  <nl> return result ;
p_vertical_bend ( BenderDialog * cd , <nl> } <nl> } <nl> } <nl> + <nl> + g_free ( last_arr ); <nl> + g_free ( first_arr ); <nl> } <nl>  <nl> /* ============================================================================
metadata_message_dialog ( GtkMessageType type , <nl> { <nl> GtkWidget * dlg ; <nl>  <nl> - dlg = gtk_message_dialog_new ( parent , 0 , type , GTK_BUTTONS_OK , message ); <nl> + dlg = gtk_message_dialog_new ( parent , 0 , type , GTK_BUTTONS_OK , "% s ", message ); <nl>  <nl> if ( title ) <nl> gtk_window_set_title ( GTK_WINDOW ( dlg ), title );
neon ( GimpDrawable * drawable , <nl> g_free ( val_p ); <nl> g_free ( val_m ); <nl> g_free ( src ); <nl> + g_free ( src2 ); <nl> g_free ( dest ); <nl> } <nl> 
gimp_histogram_view_notify ( GimpHistogram * histogram , <nl> static void <nl> gimp_histogram_view_update_bins ( GimpHistogramView * view ) <nl> { <nl> - gint new_bins ; <nl> + gint new_bins = 256 ; <nl>  <nl> if ( view -> histogram ) <nl> new_bins = gimp_histogram_n_bins ( view -> histogram );
gui_unique_win32_idle_open ( IdleOpenData * data ) <nl> if ( data -> file ) <nl> { <nl> file_open_from_command_line ( unique_gimp , data -> file , <nl> - data -> as_new , NULL , 0 ); <nl> + data -> as_new , NULL ); <nl> } <nl> else <nl> { <nl> gui_unique_quartz_idle_open ( GFile * file ) <nl>  <nl> if ( file ) <nl> { <nl> - file_open_from_command_line ( unique_gimp , file , FALSE , NULL , 0 ); <nl> + file_open_from_command_line ( unique_gimp , file , FALSE , NULL ); <nl> } <nl>  <nl> return FALSE ;
gimp_drawable_sync_fs_filter ( GimpDrawable * drawable , <nl> GeglNode * fs_source ; <nl>  <nl> private -> fs_filter = gimp_filter_new (" Floating Selection "); <nl> + gimp_viewable_set_stock_id ( GIMP_VIEWABLE ( private -> fs_filter ), <nl> + " gimp - floating - selection "); <nl>  <nl> node = gimp_filter_get_node ( private -> fs_filter ); <nl> 
save_dialog ( void ) <nl> g_signal_connect ( toggle , " toggled ", <nl> G_CALLBACK ( gimp_toggle_button_update ), <nl> & jsvals . save_xmp ); <nl> + g_signal_connect ( toggle , " toggled ", <nl> + G_CALLBACK ( make_preview ), <nl> + NULL ); <nl>  <nl> gtk_toggle_button_set_active ( GTK_TOGGLE_BUTTON ( toggle ), <nl> jsvals . save_xmp && has_metadata );
layer_options_dialog_new ( GimpImage * image , <nl> GimpContainer * filters ; <nl> GtkWidget * view ; <nl>  <nl> - frame = gimp_frame_new (" Active Filters "); <nl> + frame = gimp_frame_new ( _ (" Active Filters ")); <nl> gtk_box_pack_start ( GTK_BOX ( left_vbox ), frame , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( frame ); <nl> 
gimp_draw_tool_control ( GimpTool * tool , <nl> static gboolean <nl> gimp_draw_tool_draw_timeout ( GimpDrawTool * draw_tool ) <nl> { <nl> + guint64 now = g_get_monotonic_time (); <nl> + <nl> + /* keep the timeout running if the last drawing just happened */ <nl> + if (( now - draw_tool -> last_draw_time ) <= MINIMUM_DRAW_INTERVAL ) <nl> + return FALSE ; <nl> + <nl> draw_tool -> draw_timeout = 0 ; <nl>  <nl> gimp_draw_tool_draw ( draw_tool );
gimp_rectangle_select_tool_cursor_update ( GimpTool * tool , <nl> { <nl> gimp_tool_widget_get_cursor ( private -> widget , coords , state , <nl> & cursor , NULL , & modifier ); <nl> - <nl> - gimp_tool_control_set_cursor ( tool -> control , cursor ); <nl> - gimp_tool_control_set_cursor_modifier ( tool -> control , modifier ); <nl> } <nl>  <nl> + gimp_tool_control_set_cursor ( tool -> control , cursor ); <nl> + gimp_tool_control_set_cursor_modifier ( tool -> control , modifier ); <nl> + <nl> /* override the previous if shift or ctrl are down */ <nl> if ( state & ( gimp_get_extend_selection_mask () | <nl> gimp_get_modify_selection_mask ()))
gimp_prop_kelvin_presets_new ( GObject * config , <nl> menu = gtk_menu_new (); <nl> gtk_menu_attach_to_widget ( GTK_MENU ( menu ), button , NULL ); <nl>  <nl> + gimp_help_set_help_data ( button , <nl> + _ (" Choose from a list of common " <nl> + " color temperatures "), NULL ); <nl> + <nl> g_signal_connect ( button , " button - press - event ", <nl> G_CALLBACK ( gimp_prop_kelvin_presets_button_press ), <nl> menu );
gimp_transform_tool_commit ( GimpTransformTool * tr_tool ) <nl> if ( tr_tool -> gui ) <nl> gimp_tool_gui_hide ( tr_tool -> gui ); <nl>  <nl> - if ( gimp_matrix3_is_identity (& tr_tool -> transform )) <nl> + if ( GIMP_TRANSFORM_TOOL_GET_CLASS ( tr_tool )-> recalc_matrix && <nl> + gimp_matrix3_is_identity (& tr_tool -> transform )) <nl> { <nl> /* No need to commit an identity transformation ! */ <nl> return ;
unsigned int <nl> hb_ot_layout_table_get_lookup_count ( hb_face_t * face , <nl> hb_tag_t table_tag ) <nl> { <nl> + if ( unlikely (! hb_ot_shaper_face_data_ensure ( face ))) return 0 ; <nl> switch ( table_tag ) <nl> { <nl> case HB_OT_TAG_GSUB :
struct hb_ot_face_metrics_accelerator_t <nl>  <nl> this -> blob = OT :: Sanitizer < OT :: _mtx >:: sanitize ( face -> reference_table ( _mtx_tag )); <nl> if ( unlikely (! this -> num_advances || <nl> - 2 * ( this -> num_advances + this -> num_metrics ) < hb_blob_get_length ( this -> blob ))) <nl> + 2 * ( this -> num_advances + this -> num_metrics ) > hb_blob_get_length ( this -> blob ))) <nl> { <nl> this -> num_metrics = this -> num_advances = 0 ; <nl> hb_blob_destroy ( this -> blob );
static HB_Error Load_Mark2Array ( HB_Mark2Array * m2a , <nl>  <nl> FORGET_Frame (); <nl>  <nl> + if ( new_offset == base_offset ) { <nl> + /* Anchor table not provided . Skip loading . <nl> + * Some versions of FreeSans hit this . */ <nl> + m2an [ n ]. PosFormat = 0 ; <nl> + continue ; <nl> + } <nl> + <nl> cur_offset = FILE_Pos (); <nl> if ( FILE_Seek ( new_offset ) || <nl> ( error = Load_Anchor ( & m2an [ n ], stream ) ) != HB_Err_Ok )
my_bool <nl> my_net_write ( NET * net , const char * packet , ulong len ) <nl> { <nl> uchar buff [ NET_HEADER_SIZE ]; <nl> - if ( unlikely (! net -> vio )) // nowhere to write <nl> + if ( unlikely (! net -> vio )) /* nowhere to write */ <nl> return 0 ; <nl> /* <nl> Big packets are handled by splitting them in packets of MAX_PACKET_LENGTH
referenced_xids_note_snapshot_txn_end_iter ( OMTVALUE live_xidv , u_int32_t UU ( inde <nl> if (-- tuple -> references == 0 ) { <nl> r = toku_omt_delete_at ( referenced_xids , idx ); <nl> lazy_assert_zero ( r ); <nl> + toku_free ( tuple ); <nl> } <nl> done : <nl> return 0 ;
static int connect_assisted_discovery ( handlerton *, THD * thd , <nl> break ; <nl> # if defined ( MONGO_SUPPORT ) <nl> case TAB_MONGO : <nl> + if (! topt -> tabname ) <nl> + topt -> tabname = tab ; <nl> + <nl> ok = true ; <nl> break ; <nl> # endif // MONGO_SUPPORT
btr_search_guess_on_hash ( <nl> } <nl>  <nl> block = buf_block_align ( rec ); <nl> - page = buf_block_get_frame ( block ); <nl> + page = page_align ( rec ); <nl>  <nl> if ( UNIV_LIKELY (! has_search_latch )) { <nl> 
static int run_query ( const char * query , DYNAMIC_STRING * ds_res , <nl> NULL ); <nl>  <nl> my_close ( fd , MYF ( 0 )); <nl> + my_delete ( query_file_path , MYF ( 0 )); <nl>  <nl> DBUG_RETURN ( ret ); <nl> }
lock_update_discard ( <nl> lock_rec_reset_and_release_wait ( block , heap_no ); <nl>  <nl> rec = page + rec_get_next_offs ( rec , TRUE ); <nl> - } while ( heap_no != PAGE_NEW_SUPREMUM ); <nl> + } while ( heap_no != PAGE_HEAP_NO_SUPREMUM ); <nl> } else { <nl> rec = page + PAGE_OLD_INFIMUM ; <nl>  <nl> lock_update_discard ( <nl> lock_rec_reset_and_release_wait ( block , heap_no ); <nl>  <nl> rec = page + rec_get_next_offs ( rec , FALSE ); <nl> - } while ( heap_no != PAGE_OLD_SUPREMUM ); <nl> + } while ( heap_no != PAGE_HEAP_NO_SUPREMUM ); <nl> } <nl>  <nl> lock_rec_free_all_from_discard_page ( block );
int ha_create_table_from_engine ( THD * thd , <nl> } <nl>  <nl> err_end : <nl> - my_free (( char *) frmblob , MYF ( MY_ALLOW_ZERO )); <nl> + my_free (( char *) frmblob , MYF ( MY_ALLOW_ZERO_PTR )); <nl> DBUG_RETURN ( error ); <nl> } <nl> 
int serialize_brtnode_to ( int fd , diskoff off , diskoff size , BRTNODE node ) { <nl>  <nl> // printf ("% s :% d wrote % d bytes for % lld size =% lld \ n ", __FILE__ , __LINE__ , w . ndone , off , size ); <nl> assert ( w . ndone <= size ); <nl> - toku_free ( w . buf ); <nl> return 0 ; <nl> } <nl> 
static void wsrep_mysql_parse ( THD * thd , char * rawbuf , uint length , <nl> } <nl> mysql_mutex_unlock (& thd -> LOCK_wsrep_thd ); <nl> } <nl> + <nl> + /* If retry is requested clean up explain structure */ <nl> + if ( thd -> wsrep_conflict_state == RETRY_AUTOCOMMIT && thd -> lex -> explain ) <nl> + delete_explain_query ( thd -> lex ); <nl> + <nl> } while ( thd -> wsrep_conflict_state == RETRY_AUTOCOMMIT ); <nl>  <nl> if ( thd -> wsrep_retry_query )
ha_innobase :: add_index ( <nl>  <nl> func_exit : <nl> mem_heap_free ( heap ); <nl> + innobase_commit_low ( trx );/* work around a bug in mysql_alter_table () */ <nl>  <nl> /* There might be work for utility threads .*/ <nl> srv_active_wake_master_thread ();
mysql_init ( MYSQL * mysql ) <nl> bzero (( char *) ( mysql ), sizeof (*( mysql ))); <nl> mysql -> options . connect_timeout = CONNECT_TIMEOUT ; <nl> mysql -> last_used_con = mysql -> next_slave = mysql -> master = mysql ; <nl> + mysql -> charset = default_charset_info ; <nl> strmov ( mysql -> net . sqlstate , not_error_sqlstate ); <nl> /* <nl> By default , we are a replication pivot . The caller must reset it
buf_page_init_low ( <nl> /*==============*/ <nl> buf_page_t * bpage ) /* in : block to init */ <nl> { <nl> + bpage -> flush_type = BUF_FLUSH_LRU ; <nl> bpage -> accessed = FALSE ; <nl> bpage -> io_fix = BUF_IO_NONE ; <nl> bpage -> buf_fix_count = 0 ;
sync_array_signal_object ( <nl>  <nl> if ( cell_count == cell_max_count ) { <nl> sync_cell_t ** old_cell_ptr = cell_ptr ; <nl> - size_t old_size = cell_max_count * <nl> + size_t old_size , new_size ; <nl> + <nl> + old_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl> cell_max_count *= 2 ; <nl> - <nl> - size_t new_size = cell_max_count * <nl> + new_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl>  <nl> cell_ptr = malloc ( new_size );
btr_pcur_move_to_prev_page ( <nl>  <nl> page_cur_set_after_last ( prev_block , btr_pcur_get_page_cur ( cursor )); <nl>  <nl> - page_check_dir ( prev_page ); <nl> + ut_d ( page_check_dir ( prev_page )); <nl> } <nl>  <nl> /*********************************************************//**
row_merge_create_temporary_table ( <nl>  <nl> if ( error != DB_SUCCESS ) { <nl> trx -> error_state = error ; <nl> + dict_mem_table_free ( new_table ); <nl> new_table = NULL ; <nl> } <nl> 
row_sel_sec_rec_is_for_clust_rec ( <nl> clust_index -> table ))) { <nl> goto inequal ; <nl> } <nl> + <nl> + continue ; <nl> } <nl> } <nl> 
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
sync_array_signal_object ( <nl>  <nl> if ( cell_count == cell_max_count ) { <nl> sync_cell_t ** old_cell_ptr = cell_ptr ; <nl> - size_t old_size = cell_max_count * <nl> + size_t old_size , new_size ; <nl> + <nl> + old_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl> cell_max_count *= 2 ; <nl> - <nl> - size_t new_size = cell_max_count * <nl> + new_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl>  <nl> cell_ptr = malloc ( new_size );
bool check_change_password ( THD * thd , const char * host , const char * user , <nl> return ( 1 ); <nl> } <nl> uint len = strlen ( new_password ); <nl> - if ( len != SCRAMBLED_PASSWORD_CHAR_LENGTH && <nl> + if ( len && len != SCRAMBLED_PASSWORD_CHAR_LENGTH && <nl> len != SCRAMBLED_PASSWORD_CHAR_LENGTH_323 ) <nl> { <nl> net_printf ( thd , 0 ,
extern void bitmap_set_prefix ( MY_BITMAP * map , uint prefix_size ); <nl> extern void bitmap_intersect ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> extern void bitmap_subtract ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> extern void bitmap_union ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> + extern void bitmap_xor ( MY_BITMAP * map , const MY_BITMAP * map2 ); <nl> + extern void bitmap_invert ( MY_BITMAP * map ); <nl>  <nl> extern uint bitmap_lock_set_next ( MY_BITMAP * map ); <nl> extern void bitmap_lock_clear_bit ( MY_BITMAP * map , uint bitmap_bit );
bool rollback_log_node_cache :: give_rollback_log_node ( TOKUTXN txn , ROLLBACK_LOG_N <nl> if ( m_num_avail < m_max_num_avail ) { <nl> retval = true ; <nl> uint32_t index = m_first + m_num_avail ; <nl> - if ( index > m_max_num_avail ) { <nl> + if ( index >= m_max_num_avail ) { <nl> index -= m_max_num_avail ; <nl> } <nl> m_avail_blocknums [ index ]. b = log -> blocknum . b ;
int create_cachetable ( CACHETABLE * result , int n_entries ) { <nl> int i ; <nl> t -> n_in_table = 0 ; <nl> t -> table_size = n_entries ; <nl> - t -> table = toku_calloc ( t -> table_size , sizeof ( struct ctpair )); <nl> + MALLOC_N ( t -> table_size , t -> table ); <nl> assert ( t -> table ); <nl> t -> head = t -> tail = 0 ; <nl> for ( i = 0 ; i < t -> table_size ; i ++) {
build_index ( DB_INDEXER * indexer ) { <nl> else { <nl> invariant ( prov_info . le ); <nl> invariant ( prov_info . ule ); <nl> - LEAFENTRY le = prov_info . le ; <nl> ULEHANDLE ule = prov_info . ule ; <nl> for ( int which_db = 0 ; ( which_db < indexer -> i -> N ) && ( result == 0 ); which_db ++) { <nl> DB * db = indexer -> i -> dest_dbs [ which_db ];
buf_LRU_block_remove_hashed_page ( <nl> void * data = bpage -> zip . data ; <nl> bpage -> zip . data = NULL ; <nl>  <nl> + ut_ad (! bpage -> in_free_list ); <nl> + ut_ad (! bpage -> in_flush_list ); <nl> + ut_ad (! bpage -> in_LRU_list ); <nl> mutex_exit (&(( buf_block_t *) bpage )-> mutex ); <nl> buf_pool_mutex_exit_forbid (); <nl> buf_buddy_free ( data , page_zip_get_size (& bpage -> zip ));
trx_undo_parse_page_header ( <nl> mtr_t * mtr ) /*!< in : mtr or NULL */ <nl> { <nl> trx_id_t trx_id ; <nl> + /* Silence a GCC warning about possibly uninitialized variable <nl> + when mach_ull_parse_compressed () is not inlined . */ <nl> + ut_d ( trx_id = 0 ); <nl> + /* Declare the variable uninitialized in Valgrind , so that the <nl> + above initialization will not mask any bugs . */ <nl> + UNIV_MEM_INVALID (& trx_id , sizeof trx_id ); <nl>  <nl> ptr = mach_ull_parse_compressed ( ptr , end_ptr , & trx_id ); <nl> 
int ha_tokudb :: delete_all_rows () { <nl> txn <nl> ); <nl> if ( error ) { goto cleanup ; } <nl> + error = share -> key_file [ i ]-> pre_acquire_table_lock ( <nl> + share -> key_file [ i ], <nl> + txn <nl> + ); <nl> + if ( error ) { goto cleanup ; } <nl> } <nl> for ( uint i = 0 ; i < curr_num_DBs ; i ++) { <nl> error = truncate_dictionary ( i , txn );
bool ZIPUTIL :: WildMatch ( PSZ pat , PSZ str ) { <nl> if (!*++ pat ) return TRUE ; <nl> goto loopStart ; <nl> default : <nl> - if ( mapCaseTable [( unsigned )* s ] != mapCaseTable [( unsigned )* p ]) <nl> + if ( mapCaseTable [( uchar )* s ] != mapCaseTable [( uchar )* p ]) <nl> goto starCheck ; <nl> break ; <nl> } /* endswitch */
innobase_next_autoinc ( <nl> } else { <nl> next_value = current + increment ; <nl> } <nl> - } else { <nl> + } else if ( max_value > current ) { <nl> if ( current > offset ) { <nl> next_value = (( current - offset ) / increment ) + 1 ; <nl> } else { <nl> innobase_next_autoinc ( <nl> next_value += offset ; <nl> } <nl> } <nl> + } else { <nl> + next_value = max_value ; <nl> } <nl>  <nl> ut_a ( next_value <= max_value );
int toku_cachefile_fd ( CACHEFILE cf ) { <nl> } <nl>  <nl> int toku_cachefile_truncate0 ( CACHEFILE cf ) { <nl> - int r = toku_graceful_dirty ( cachefile ); <nl> + int r ; <nl> + r = toku_graceful_dirty ( cf ); <nl> if ( r != 0 ) return r ; <nl> - int r = ftruncate ( cf -> fd , 0 ); <nl> + r = ftruncate ( cf -> fd , 0 ); <nl> if ( r != 0 ) <nl> r = errno ; <nl> return r ;
row_create_prebuilt ( <nl> prebuilt -> ins_node = NULL ; <nl>  <nl> prebuilt -> ins_upd_rec_buff = NULL ; <nl> - <nl> + <nl> + prebuilt -> hint_need_to_fetch_extra_cols = 0 ; <nl> + <nl> prebuilt -> upd_node = NULL ; <nl> prebuilt -> ins_graph = NULL ; <nl> prebuilt -> upd_graph = NULL ;
innobase_xa_prepare ( <nl> int error = 0 ; <nl> trx_t * trx = check_trx_exists ( thd ); <nl>  <nl> - if ( thd -> lex -> sql_command != SQLCOM_XA_PREPARE ) { <nl> + if ( thd -> lex -> sql_command != SQLCOM_XA_PREPARE && <nl> + ( all || !( thd -> options & ( OPTION_NOT_AUTOCOMMIT | OPTION_BEGIN )))) <nl> + { <nl>  <nl> /* For ibbackup to work the order of transactions in binlog <nl> and InnoDB must be the same . Consider the situation
btr_cur_optimistic_insert ( <nl> if ( UNIV_UNLIKELY (! btr_page_reorganize ( block , index , mtr ))) { <nl> ut_a ( buf_block_get_page_zip ( block )); <nl>  <nl> + ibuf_reset_free_bits_with_type ( index -> type , block ); <nl> + <nl> goto fail ; <nl> } <nl> 
referenced_xids_note_snapshot_txn_end_iter ( OMTVALUE live_xidv , u_int32_t UU ( inde <nl> if (-- tuple -> references == 0 ) { <nl> r = toku_omt_delete_at ( referenced_xids , idx ); <nl> lazy_assert_zero ( r ); <nl> + toku_free ( tuple ); <nl> } <nl> done : <nl> return 0 ;
dict_index_add_to_cache ( <nl>  <nl> if (! dict_index_find_cols ( table , index )) { <nl>  <nl> + dict_mem_index_free ( index ); <nl> return ( DB_CORRUPTION ); <nl> } <nl> 
int subselect_indexsubquery_engine :: exec () <nl> null_finding = 1 ; <nl> /* Check if there exists a row with a null value in the index */ <nl> if (( error = safe_index_read ( tab ))) <nl> + { <nl> + if ( error < 0 ) <nl> + error = 0 ; // Key not found <nl> break ; <nl> + } <nl> } <nl> } <nl> }
enum tablespace_op_type <nl> e ||+-------------------------+ || <nl> V | neighbor | V | <nl> unit1 . 1 <+==================> unit1 . 2 unit2 . 1 <nl> - fake1 . 1 fake2 . 1 <nl> - select1 . 1 . 1 select 1 . 1 . 2 select1 . 2 . 1 select2 . 1 . 1 select2 . 1 . 2 <nl> + fake1 . 1 <nl> + select1 . 1 . 1 select 1 . 1 . 2 select1 . 2 . 1 select2 . 1 . 1 <nl> |^ <nl> || <nl> V |
void mysql_client_binlog_statement ( THD * thd ) <nl>  <nl> const char * error = 0 ; <nl> char * buf = ( char *) my_malloc ( event_len , MYF ( MY_WME )); <nl> - Log_event * ev ; <nl> + Log_event * ev = 0 ; <nl> int res ; <nl>  <nl> /*
static uint remove_key ( MI_KEYDEF * keyinfo , uint nod_flag , <nl> else <nl> get_key_length ( rest_length , keypos ); <nl>  <nl> - if ( next_length > prev_length ) <nl> + if ( next_length >= prev_length ) <nl> { /* Key after is based on deleted key */ <nl> uint pack_length , tmp ; <nl> bmove_upp (( char *) keypos ,( char *) ( lastkey + next_length ),
class base_list_iterator <nl> * new_list . last = current -> next ; <nl> current -> info = new_list . first -> info ; <nl> current -> next = new_list . first -> next ; <nl> + if (( list -> last == & current -> next ) && ( new_list . elements > 1 )) <nl> + list -> last = new_list . last ; <nl> list -> elements += new_list . elements - 1 ; <nl> } <nl> return ret_value ; // return old element
row_purge_remove_sec_if_poss_low ( <nl>  <nl> mtr_start (& mtr ); <nl>  <nl> - btr_cur -> thr = que_node_get_parent ( node ); <nl> - <nl> row_search_index_entry (& was_buffered , index , entry , <nl> BTR_MODIFY_LEAF | BTR_DELETE , & pcur , <nl> & mtr );
void OPJ_CALLCONV opj_stream_set_skip_function ( opj_stream_t * p_stream , opj_strea <nl> void OPJ_CALLCONV opj_stream_set_user_data ( opj_stream_t * p_stream , void * p_data ) <nl> { <nl> opj_stream_private_t * l_stream = ( opj_stream_private_t *) p_stream ; <nl> + if (! l_stream ) <nl> + return ; <nl> l_stream -> m_user_data = p_data ; <nl> } <nl>  <nl> void OPJ_CALLCONV opj_stream_set_user_data_length ( opj_stream_t * p_stream , OPJ_UINT64 data_length ) <nl> { <nl> opj_stream_private_t * l_stream = ( opj_stream_private_t *) p_stream ; <nl> + if (! l_stream ) <nl> + return ; <nl> l_stream -> m_user_data_length = data_length ; <nl> } <nl> 
static int tga_readheader ( FILE * fp , unsigned int * bits_per_pixel , <nl> if ( fread ( tga , TGA_HEADER_SIZE , 1 , fp ) != 1 ) { <nl> fprintf ( stderr , <nl> "\ nError : fread return a number of element different from the expected .\ n "); <nl> + free ( tga ); <nl> return 0 ; <nl> } <nl> id_len = ( unsigned char ) tga [ 0 ];
static opj_bool t2_read_packet_data ( <nl>  <nl> # endif /* USE_JPWL */ <nl>  <nl> + if (( l_cblk -> len + l_seg -> newlen ) > 8192 ) { <nl> + return OPJ_FALSE ; <nl> + } <nl> + <nl> memcpy ( l_cblk -> data + l_cblk -> len , l_current_data , l_seg -> newlen ); <nl>  <nl> if ( l_seg -> numpasses == 0 ) {
buf_shrink_freelists ( int free_all ) <nl> -- n_to_free ; <nl> } <nl> tor_assert (! n_to_free ); <nl> - freelists [ i ]. lowest_length = freelists [ i ]. cur_length = n_to_skip ; <nl> + freelists [ i ]. cur_length = n_to_skip ; <nl> } <nl> + freelists [ i ]. lowest_length = freelists [ i ]. cur_length ; <nl> assert_freelist_ok (& freelists [ i ]); <nl> } <nl> }
int connection_dns_finished_flushing ( connection_t * conn ) { <nl> int connection_dns_reached_eof ( connection_t * conn ) { <nl> log_fn ( LOG_WARN ," Read eof . Worker died unexpectedly ."); <nl> if ( conn -> state == DNSWORKER_STATE_BUSY ) { <nl> - dns_cancel_pending_resolve ( conn -> address ); <nl> + /* don ' t cancel the resolve here -- it would be cancelled in <nl> + * connection_about_to_close_connection (), since conn is still <nl> + * in state BUSY <nl> + */ <nl> num_dnsworkers_busy --; <nl> } <nl> num_dnsworkers --;
router_reload_networkstatus ( void ) <nl> entries = tor_listdir ( filename ); <nl> SMARTLIST_FOREACH ( entries , const char *, fn , { <nl> char buf [ DIGEST_LEN ]; <nl> + if ( fn [ 0 ] == '.') /* skip . and .. */ <nl> + continue ; <nl> if ( strlen ( fn ) != HEX_DIGEST_LEN || <nl> base16_decode ( buf , sizeof ( buf ), fn , strlen ( fn ))) { <nl> log_fn ( LOG_INFO ,
rend_service_set_connection_addr_port ( connection_t * conn , circuit_t * circ ) <nl> serviceid , circ -> n_circ_id ); <nl> circuit_mark_for_close ( circ ); <nl> connection_mark_for_close ( conn , 0 /* XXX */); <nl> + return - 1 ; <nl> } <nl> for ( i = 0 ; i < smartlist_len ( service -> ports ); ++ i ) { <nl> p = smartlist_get ( service -> ports , i );
extern INLINE double U64_TO_DBL ( uint64_t x ) { <nl> # endif <nl>  <nl> /* GCC has several useful attributes . */ <nl> -# ifdef __GNUC__ <nl> +# ifdef __GNUC__ && __GNUC_MAJOR__ >= 3 <nl> # define ATTR_NORETURN __attribute__ (( noreturn )) <nl> # define ATTR_PURE __attribute__ (( pure )) <nl> # define ATTR_MALLOC __attribute__ (( malloc ))
MOCK_IMPL ( STATIC X509 *, <nl> goto error ; <nl> if (! X509_set_pubkey ( x509 , pkey )) <nl> goto error ; <nl> - if (! X509_sign ( x509 , sign_pkey , EVP_sha1 ())) <nl> + <nl> + if (! X509_sign ( x509 , sign_pkey , EVP_sha256 ())) <nl> goto error ; <nl>  <nl> goto done ;
router_dump_router_to_string ( char * s , size_t maxlen , routerinfo_t * router , <nl> if ( result < 0 ) <nl> return - 1 ; <nl> written += result ; <nl> - if ( written < maxlen + 2 ) <nl> + if ( written + 2 > maxlen ) <nl> return - 1 ; <nl> s [ written ++] = '\ n '; <nl> }
directory_remove_invalid ( void ) <nl> directory_set_dirty (); <nl>  <nl> routerlist_assert_ok ( rl ); <nl> + smartlist_free ( nodes ); <nl> } <nl>  <nl> /** Mark the directory as < b > dirty </ b > -- when we ' re next asked for a
consensus_queue_compression_work ( const char * consensus , <nl> config_line_prepend (& job -> labels_in , LABEL_SIGNATORIES , signers ); <nl> tor_free ( signers ); <nl> SMARTLIST_FOREACH ( hexvoters , char *, cp , tor_free ( cp )); <nl> + smartlist_free ( hexvoters ); <nl> } <nl>  <nl> if ( background_compression ) {
circuit_expire_building ( time_t now ) <nl> /* c_rend_ready circs measure age since timestamp_dirty , <nl> * because that ' s set when they switch purposes <nl> */ <nl> - if (! victim -> rend_query [ 0 ] || victim -> timestamp_dirty <= cutoff ) <nl> + if (! victim -> rend_query [ 0 ] || victim -> timestamp_dirty > cutoff ) <nl> continue ; <nl> break ; <nl> case CIRCUIT_PURPOSE_C_REND_READY_INTRO_ACKED :
add_an_entry_guard ( const node_t * chosen , int reset_status , int prepend , <nl> entry -> can_retry = 1 ; <nl> } <nl> entry -> is_dir_cache = node_is_dir ( node ); <nl> + if ( get_options ()-> UseBridges && node_is_a_configured_bridge ( node )) <nl> + entry -> is_dir_cache = 1 ; <nl> + <nl> return NULL ; <nl> } <nl> } else if (! for_directory ) {
static int parse_redirect_line ( or_options_t * options , <nl> tor_assert ( line ); <nl>  <nl> r = tor_malloc_zero ( sizeof ( exit_redirect_t )); <nl> + elements = smartlist_create (); <nl> smartlist_split_string ( elements , line -> value , " ", <nl> SPLIT_SKIP_SPACE | SPLIT_IGNORE_BLANK , 0 ); <nl> if ( smartlist_len ( elements ) != 2 ) {
tor_spawn_background ( const char * const filename , const char ** argv , <nl> process_environment_t * env , <nl> process_handle_t ** process_handle_out ) <nl> { <nl> - if ( may_spawn_background_process == 0 ) <nl> + if ( BUG ( may_spawn_background_process == 0 )) { <nl> + /* We should never reach this point if we ' re forbidden to spawn <nl> + * processes . Instead we should have caught the attempt earlier . */ <nl> return PROCESS_STATUS_ERROR ; <nl> + } <nl>  <nl> # ifdef _WIN32 <nl> HANDLE stdout_pipe_read = NULL ;
# error " Sorry ; we don ' t support building with NDEBUG ." <nl> # else <nl> # ifdef __GNUC__ <nl> -# define PREDICT_FALSE ( x ) PREDICT (( x ) != (( typeof ( x )) 0 ), 0 ) <nl> +# define PREDICT_FALSE ( x ) PREDICT (( x ) == (( typeof ( x )) 0 ), 0 ) <nl> # else <nl> # define PREDICT_FALSE ( x ) !( x ) <nl> # endif
tor_zlib_process ( tor_zlib_state_t * state , <nl> return Z_OK ; <nl> return TOR_ZLIB_BUF_FULL ; <nl> case Z_OK : <nl> - if ( state -> stream . avail_out == 0 ) <nl> + if ( state -> stream . avail_out == 0 || finish ) <nl> return TOR_ZLIB_BUF_FULL ; <nl> return TOR_ZLIB_OK ; <nl> default :
ECP :: ECP ( BufferedTransformation & bt ) <nl> GetField (). BERDecodeElement ( seq , m_b ); <nl> // skip optional seed <nl> if (! seq . EndReached ()) <nl> - BERDecodeOctetString ( seq , TheBitBucket ()); <nl> + { <nl> + SecByteBlock seed ; <nl> + unsigned int unused ; <nl> + BERDecodeBitString ( seq , seed , unused ); <nl> + } <nl> seq . MessageEnd (); <nl> } <nl> 
size_t jsvGetString ( const JsVar * v , char * str , size_t len ) { <nl> * want to pad the entire buffer with zeros */ <nl> len --; <nl> int l = 0 ; <nl> - while (* s && l < len ) { <nl> + while ( s [ l ] && l < len ) { <nl> str [ l ] = s [ l ]; <nl> l ++; <nl> }
JsVar * jspeFactor () { <nl> return jspeFactorTypeOf (); <nl> } else if ( execInfo . lex -> tk == LEX_R_VOID ) { <nl> JSP_MATCH ( LEX_R_VOID ); <nl> - jsvUnLock ( jspeBase ()); <nl> + jsvUnLock ( jspeFactor ()); <nl> return 0 ; <nl> } <nl> // Nothing we can do here ... just hope it ' s the end ...
void jsiSemiInit ( bool autoLoad ) { <nl> "| __ | _ -| . | _ | | | | | . |\ n " <nl> "| _____ | ___ | _ | _ | | ___ | _ | _ | _ | ___ |\ n " <nl> " | _ | http :// espruino . com \ n " <nl> - " " JS_VERSION " Copyright 2015 G . Williams \ n "); <nl> + " " JS_VERSION " Copyright 2016 G . Williams \ n "); <nl> # ifdef ESP8266 <nl> jshPrintBanner (); <nl> # endif
void jswrap_wifi_restore ( void ) { <nl> ap_config . ssid_hidden = jsvGetInteger ( v ); <nl> jsvUnLock ( v ); <nl>  <nl> - v = jsvObjectGetChild ( o ," ssid ", 0 ); <nl> + v = jsvObjectGetChild ( o ," ssidAP ", 0 ); <nl> jsvGetString ( v , ( char *) ap_config . ssid , sizeof ( ap_config . ssid )); <nl>  <nl> ap_config . ssid_len = jsvGetStringLength ( v );
slurmd_task_info_t * task_info_create ( int taskid , int gtaskid , <nl> static inline slurmd_task_info_t * <nl> job_task_info_by_pid ( slurmd_job_t * job , pid_t pid ) <nl> { <nl> - int i ; <nl> + uint32_t i ; <nl> for ( i = 0 ; i < job -> node_tasks ; i ++) { <nl> if ( job -> task [ i ]-> pid == pid ) <nl> return ( job -> task [ i ]);
static int _move_account ( mysql_conn_t * mysql_conn , uint32_t lft , uint32_t rgt , <nl> } <nl> xfree ( query ); <nl> if (!( row = mysql_fetch_row ( result ))) { <nl> - error (" no row "); <nl> + debug4 (" Can ' t move a none existant association "); <nl> mysql_free_result ( result ); <nl> - return SLURM_ERROR ; <nl> + return SLURM_SUCCESS ; <nl> } <nl> par_left = atoi ( row [ 0 ]); <nl> mysql_free_result ( result );
int slurm_step_launch ( slurm_step_ctx ctx , <nl> char ** env = NULL ; <nl>  <nl> debug (" Entering slurm_step_launch "); <nl> + memset (& launch , 0 , sizeof ( launch )); <nl> + <nl> if ( ctx == NULL || ctx -> magic != STEP_CTX_MAGIC ) { <nl> error (" Not a valid slurm_step_ctx !"); <nl> 
static void log_msg ( log_level_t level , const char * fmt , va_list args ) <nl>  <nl> if ( level > SYSLOG_LEVEL && <nl> level > LOGFILE_LEVEL && <nl> - level > STDERR_LEVEL ) <nl> + level > STDERR_LEVEL ) { <nl> + pthread_mutex_unlock (& log_lock ); <nl> return ; <nl> + } <nl>  <nl> if ( log -> opt . prefix_level || SYSLOG_LEVEL > level ) { <nl> switch ( level ) {
void slurm_step_launch_wait_finish ( slurm_step_ctx_t * ctx ) <nl>  <nl> /* Then shutdown the message handler thread */ <nl> eio_signal_shutdown ( sls -> msg_handle ); <nl> + pthread_mutex_unlock (& sls -> lock ); <nl> pthread_join ( sls -> msg_thread , NULL ); <nl> + pthread_mutex_lock (& sls -> lock ); <nl> eio_handle_destroy ( sls -> msg_handle ); <nl>  <nl> /* Then wait for the IO thread to finish */ <nl> void slurm_step_launch_wait_finish ( slurm_step_ctx_t * ctx ) <nl> } <nl>  <nl> mpi_hook_client_fini ( sls -> mpi_state ); <nl> - <nl> pthread_mutex_unlock (& sls -> lock ); <nl> } <nl> 
main ( int argc , char * argv []) <nl>  <nl> _slurmd_fini (); <nl> _destroy_conf (); <nl> + slurm_crypto_fini (); /* must be after _destroy_conf () */ <nl> + <nl> info (" Slurmd shutdown completing "); <nl> log_fini (); <nl> return 0 ; <nl> _slurmd_fini () <nl> slurm_conf_destroy (); <nl> slurm_proctrack_fini (); <nl> slurm_auth_fini (); <nl> - slurm_crypto_fini (); <nl> slurmd_req ( NULL ); /* purge memory allocated by slurmd_req () */ <nl> return SLURM_SUCCESS ; <nl> }
static int _load_job_state ( Buf buffer ) <nl> jobacct_storage_g_job_complete ( acct_db_conn , job_ptr ); <nl> } <nl>  <nl> - if ( job_ptr -> qos ) { <nl> + if ( job_ptr -> qos && ( accounting_enforce & ACCOUNTING_ENFORCE_ASSOCS )) { <nl> memset (& qos_rec , 0 , sizeof ( acct_qos_rec_t )); <nl> qos_rec . id = job_ptr -> qos ; <nl> if ( _determine_and_validate_qos ( job_ptr , & qos_rec )
extern char * ba_set_ionode_str ( bitstr_t * ionode_bitmap ) <nl> if ( hl ) { <nl> ionode_str = hostlist_ranged_string_xmalloc_dims ( <nl> hl , 5 , 0 ); <nl> - info (" iostring is % s ", ionode_str ); <nl> + // info (" iostring is % s ", ionode_str ); <nl> hostlist_destroy ( hl ); <nl> hl = NULL ; <nl> }
extern int assoc_mgr_fill_in_assoc ( void * db_conn , <nl> ret_assoc = found_assoc ; <nl> debug3 (" found association " <nl> " for no partition "); <nl> + continue ; <nl> } else if ( strcasecmp ( assoc -> partition , <nl> - found_assoc -> partition )) <nl> + found_assoc -> partition )) { <nl> debug3 (" not the right partition "); <nl> - continue ; <nl> + continue ; <nl> + } <nl> } <nl> } <nl> ret_assoc = found_assoc ;
extern void gres_plugin_node_state_log ( List gres_list , char * node_name ) <nl> ListIterator gres_iter ; <nl> gres_node_state_t * gres_ptr ; <nl>  <nl> + if ( gres_list == NULL ) <nl> + return ; <nl> + <nl> ( void ) gres_plugin_init (); <nl>  <nl> slurm_mutex_lock (& gres_context_lock );
extern void addto_qos_char_list ( List char_list , List qos_list , char * names ) <nl> bad : <nl> i ++; <nl> start = i ; <nl> + if (! names [ i ]) { <nl> + info (" There is a problem with " <nl> + " your line . It appears you " <nl> + " have spaces inside your list ."); <nl> + break ; <nl> + } <nl> } <nl> i ++; <nl> }
static uint32_t _update_weighted_freq ( struct jobacctinfo * jobacct , <nl> jobacct -> current_weighted_freq = <nl> jobacct -> current_weighted_freq + <nl> ( uint32_t ) jobacct -> this_sampled_cputime * thisfreq ; <nl> - if ( jobacct -> tot_cpu ) { <nl> + if ( jobacct -> tot_cpu >= 1 ) { <nl> return ( jobacct -> current_weighted_freq / <nl> ( uint32_t ) jobacct -> tot_cpu ); <nl> } else
static void _purge_missing_jobs ( int node_inx , time_t now ) <nl> node_boot_time = node_ptr -> boot_time - ( msg_timeout + 5 ); <nl> } <nl> batch_startup_time = now - batch_start_timeout ; <nl> - batch_startup_time -= msg_timeout ; <nl> + batch_startup_time -= MIN ( DEFAULT_MSG_TIMEOUT , msg_timeout ); <nl>  <nl> job_iterator = list_iterator_create ( job_list ); <nl> while (( job_ptr = ( struct job_record *) list_next ( job_iterator ))) {
void msWriteErrorImage ( mapObj * map , char * filename , int blank ) <nl> ls . size = i ; <nl> MS_INIT_COLOR (* ls . color , 0 , 0 , 0 , 255 ); <nl> MS_INIT_COLOR (* ls . outlinecolor , 255 , 255 , 255 , 255 ); <nl> + ls . outlinewidth = 1 ; <nl> break ; <nl> } <nl> }
styleObj * msRemoveStyle ( classObj * class , int nStyleIndex ) { <nl> return NULL ; <nl> } <nl> msCopyStyle ( style , &( class -> styles [ nStyleIndex ])); <nl> + style -> isachild = MS_FALSE ; <nl> for ( i = nStyleIndex ; i < class -> numstyles - 1 ; i ++) { <nl> msCopyStyle (& class -> styles [ i ], & class -> styles [ i + 1 ]); <nl> }
int loadLayer ( layerObj * layer , mapObj * map ) <nl> if ( getString (& layer -> tileindex ) == MS_FAILURE ) return (- 1 ); <nl> break ; <nl> case ( TILEITEM ): <nl> + free ( layer -> tileitem ); layer -> tileitem = NULL ; // erase default <nl> if ( getString (& layer -> tileitem ) == MS_FAILURE ) return (- 1 ); <nl> break ; <nl> case ( TOLERANCE ):
MS_DLL_EXPORT int freeClass ( classObj * ); <nl> MS_DLL_EXPORT void initLabel ( labelObj * label ); <nl> MS_DLL_EXPORT void resetClassStyle ( classObj * _class ); <nl> MS_DLL_EXPORT int initStyle ( styleObj * style ); <nl> + MS_DLL_EXPORT int freeStyle ( styleObj * style ); <nl> MS_DLL_EXPORT void initReferenceMap ( referenceMapObj * ref ); <nl> MS_DLL_EXPORT void initScalebar ( scalebarObj * scalebar ); <nl> MS_DLL_EXPORT void initGrid ( graticuleObj * pGraticule );
int msGetGDALGeoTransform ( GDALDatasetH hDS , mapObj * map , layerObj * layer , <nl> } <nl> /* fullPath has a filename included , so get the extension */ <nl> else { <nl> - fileExtension = strrchr ( szPath ,'.') + 1 ; <nl> + fileExtension = msStrdup ( strrchr ( szPath ,'.') + 1 ); <nl> } <nl> } <nl> /* common behaviour with worldfile generated from basename + . wld */
static gboolean wsq_tlskey_inited = FALSE ; <nl> void <nl> mono_wsq_init () <nl> { <nl> + if ( wsq_tlskey_inited ) <nl> + return ; <nl> + <nl> mono_native_tls_alloc ( wsq_tlskey , NULL ); <nl> + wsq_tlskey_inited = TRUE ; <nl> } <nl>  <nl> void
mono_threads_summarize ( MonoContext * ctx , gchar ** out , MonoStackHash * hashes ) <nl> int count = 4 ; <nl>  <nl> while ( old_num_summarized == num_threads_summarized && count > 0 ) { <nl> + if ( thread -> state & ( ThreadState_Unstarted | ThreadState_Aborted | ThreadState_Stopped )) <nl> + break ; <nl> + <nl> sleep ( 1 ); <nl> mono_memory_barrier (); <nl> const char * name = thread_summary_state_to_str ( summarizing_thread_state );
mono_handle_stack_free ( HandleStack * stack ) <nl> c = next ; <nl> } <nl> g_free ( c ); <nl> + g_free ( stack ); <nl> } <nl>  <nl> void
mono_local_emulate_ops ( MonoCompile * cfg ) <nl> * at IR level , instead of inlining the icall wrapper . FIXME <nl> */ <nl> if ( inlined_wrapper ) { <nl> - mono_decompose_long_opts ( cfg ); <nl> + if (! COMPILE_LLVM ( cfg )) <nl> + mono_decompose_long_opts ( cfg ); <nl> if ( cfg -> opt & ( MONO_OPT_CONSPROP | MONO_OPT_COPYPROP )) <nl> mono_local_cprop ( cfg ); <nl> }
continuation_store ( MonoContinuation * cont , int state , MonoException ** e ) <nl> /* clear to avoid GC retention */ <nl> if ( num_bytes < cont -> stack_used_size ) { <nl> memset (( char *) cont -> saved_stack + num_bytes , 0 , cont -> stack_used_size - num_bytes ); <nl> - cont -> stack_used_size = num_bytes ; <nl> } <nl> + cont -> stack_used_size = num_bytes ; <nl> } else { <nl> tasklets_lock (); <nl> internal_init ();
mono_profiler_init_log ( const char * desc ) <nl> mono_profiler_set_gc_event_callback ( handle , gc_event ); <nl>  <nl> mono_profiler_set_thread_started_callback ( handle , thread_start ); <nl> - mono_profiler_set_thread_stopped_callback ( handle , thread_end ); <nl> + mono_profiler_set_thread_exited_callback ( handle , thread_end ); <nl> mono_profiler_set_thread_name_callback ( handle , thread_name ); <nl>  <nl> mono_profiler_set_domain_loaded_callback ( handle , domain_loaded );
mono_jit_walk_stack_from_ctx_in_thread ( MonoJitStackWalk func , MonoDomain * domai <nl> /* <nl> * FIXME : These frames show up twice , and ctx could refer to native code . <nl> */ <nl> + ctx = new_ctx ; <nl> continue ; <nl> } <nl> frame . actual_method = get_method_from_stack_frame ( frame . ji , get_generic_info_from_stack_frame ( frame . ji , & ctx ));
emit_method_info ( MonoAotCompile * acfg , MonoCompile * cfg ) <nl>  <nl> encode_patch_list ( acfg , patches , n_patches , cfg -> compile_llvm , first_got_offset , p , & p ); <nl>  <nl> + g_ptr_array_free ( patches , TRUE ); <nl> + <nl> acfg -> stats . info_size += p - buf ; <nl>  <nl> g_assert ( p - buf < buf_size );
 <nl> typedef struct _MSBlockInfo MSBlockInfo ; <nl> struct _MSBlockInfo { <nl> - int obj_size ; <nl> - int obj_size_index ; <nl> + guint16 obj_size ; <nl> + /* <nl> + * FIXME : Do we even need this ? It ' s only used during sweep and might be worth <nl> + * recalculating to save the space . <nl> + */ <nl> + guint16 obj_size_index ; <nl> unsigned int pinned : 1 ; <nl> unsigned int has_references : 1 ; <nl> unsigned int has_pinned : 1 ; /* means cannot evacuate */
acfg_free ( MonoAotCompile * acfg ) <nl> g_hash_table_destroy ( acfg -> image_hash ); <nl> g_hash_table_destroy ( acfg -> unwind_info_offsets ); <nl> g_hash_table_destroy ( acfg -> method_label_hash ); <nl> - if (! acfg -> typespec_classes ) <nl> + if ( acfg -> typespec_classes ) <nl> g_hash_table_destroy ( acfg -> typespec_classes ); <nl> g_hash_table_destroy ( acfg -> export_names ); <nl> g_hash_table_destroy ( acfg -> plt_entry_debug_sym_cache );
mono_llvm_emit_method ( MonoCompile * cfg ) <nl> if ( cfg -> method -> save_lmf && ! cfg -> llvm_only ) <nl> LLVM_FAILURE ( ctx , " lmf "); <nl>  <nl> - if ( sig -> pinvoke && cfg -> method -> wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE ) <nl> + if ( sig -> pinvoke && cfg -> method -> wrapper_type != MONO_WRAPPER_RUNTIME_INVOKE && ! cfg -> llvm_only ) <nl> LLVM_FAILURE ( ctx , " pinvoke signature "); <nl>  <nl> header = cfg -> header ;
coverage_filter ( MonoProfiler * prof , MonoMethod * method ) <nl> MonoLockFreeQueue * image_methods , * class_methods ; <nl> MonoLockFreeQueueNode * node ; <nl>  <nl> - if (! coverage_initialized ) <nl> - return FALSE ; <nl> + g_assert ( coverage_initialized && " Why are we being asked for coverage filter info when we ' re not doing coverage ?"); <nl>  <nl> COVERAGE_DEBUG ( fprintf ( stderr , " Coverage filter for % s \ n ", mono_method_get_name ( method ));) <nl> 
sgen_stop_world ( int generation ) <nl> { <nl> int count , dead ; <nl>  <nl> - /* XXX this is the right stop , thought might not be the nicest place to put it */ <nl> - sgen_process_togglerefs (); <nl> - <nl> mono_profiler_gc_event ( MONO_GC_EVENT_PRE_STOP_WORLD , generation ); <nl> MONO_GC_WORLD_STOP_BEGIN (); <nl> acquire_gc_locks (); <nl>  <nl> + /* We start to scan after locks are taking , this ensures we won ' t be interrupted . */ <nl> + sgen_process_togglerefs (); <nl> + <nl> update_current_thread_stack (& count ); <nl>  <nl> sgen_global_stop_count ++;
mono_gc_pthread_detach ( pthread_t thread ) <nl> void <nl> mono_gc_pthread_exit ( void * retval ) <nl> { <nl> + mono_thread_info_dettach (); <nl> pthread_exit ( retval ); <nl> } <nl> 
mini_method_compile ( MonoMethod * method , guint32 opts , MonoDomain * domain , JitFl <nl> // g_free ( nm ); <nl> } <nl> if ( cfg -> llvm_only ) { <nl> + g_free ( cfg -> exception_message ); <nl> cfg -> disable_aot = TRUE ; <nl> return cfg ; <nl> }
mono_mempool_alloc ( MonoMemPool * pool , guint size ) <nl>  <nl> # ifdef MALLOC_ALLOCATION <nl> { <nl> - Chunk * c = g_malloc ( size ); <nl> + Chunk * c = g_malloc ( size + sizeof ( Chunk )); <nl>  <nl> c -> next = pool -> chunks ; <nl> pool -> chunks = c ;
void <nl> mono_llvm_free_domain_info ( MonoDomain * domain ) <nl> { <nl> /* This is called even when llvm is not enabled */ <nl> - if ( mono_llvm_free_domain_info_fptr ) <nl> + if ( backend . free_domain_info ) <nl> backend . free_domain_info ( domain ); <nl> } <nl> 
add_wrappers ( MonoAotCompile * acfg ) <nl> if ( export_name ) <nl> g_hash_table_insert ( acfg -> export_names , wrapper , export_name ); <nl> } <nl> + g_free ( cattr ); <nl> } <nl>  <nl> if (( method -> flags & METHOD_ATTRIBUTE_PINVOKE_IMPL ) ||
mono_threads_attach_coop_internal ( MonoDomain * domain , gpointer * cookie , MonoSta <nl> { <nl> MonoDomain * orig ; <nl> MonoThreadInfo * info ; <nl> - gboolean external ; <nl> + gboolean external = FALSE ; <nl>  <nl> orig = mono_domain_get (); <nl> 
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> } <nl>  <nl> /* Add a sequence point for method entry / exit events */ <nl> - if ( cfg -> gen_seq_points_debug_data ) { <nl> + if ( seq_points && cfg -> gen_seq_points_debug_data ) { <nl> NEW_SEQ_POINT ( cfg , ins , METHOD_ENTRY_IL_OFFSET , FALSE ); <nl> MONO_ADD_INS ( init_localsbb , ins ); <nl> NEW_SEQ_POINT ( cfg , ins , METHOD_EXIT_IL_OFFSET , FALSE );
decode_exception_debug_info ( MonoAotModule * amodule , MonoDomain * domain , <nl> mono_error_cleanup (& error ); /* FIXME don ' t swallow the error */ <nl> } <nl>  <nl> - gi -> generic_sharing_context = g_new0 ( MonoGenericSharingContext , 1 ); <nl> + gi -> generic_sharing_context = alloc0_jit_info_data ( domain , sizeof ( MonoGenericSharingContext ), async ); <nl> if ( decode_value ( p , & p )) { <nl> /* gsharedvt */ <nl> MonoGenericSharingContext * gsctx = gi -> generic_sharing_context ;
bool isEDNSOptionInOpt ( const std :: string & packet , const size_t optStart , const s <nl> size_t p = optStart + 9 ; <nl> uint16_t rdLen = ( 0x100 * packet . at ( p ) + packet . at ( p + 1 )); <nl> p += sizeof ( rdLen ); <nl> - if ( 11 + rdLen > optLen ) { <nl> + if ( rdLen > ( optLen - 11 )) { <nl> return false ; <nl> } <nl> 
void doClient ( ComboAddress server , const std :: string & command ) <nl> msg . assign ( resp . get (), len ); <nl> msg = sodDecryptSym ( msg , g_key , theirs ); <nl> cout << msg ; <nl> + cout . flush (); <nl> } <nl> } <nl> else {
int main ( int argc , char ** argv ) <nl> if (! dh -> qdcount ) <nl> continue ; <nl>  <nl> + if ( pr . d_len < sizeof ( dnsheader )) <nl> + continue ; <nl> + <nl> uint16_t qtype , qclass ; <nl> DNSName qname ; <nl> try {
vector < std :: function < void ( void )>> setupLua ( bool client , const std :: string & confi <nl> return std :: shared_ptr < DNSAction >( new NoRecurseAction ); <nl> }); <nl>  <nl> + g_lua . writeFunction (" DropAction ", []() { <nl> + return std :: shared_ptr < DNSAction >( new DropAction ); <nl> + }); <nl> + <nl> + <nl> g_lua . writeFunction (" MaxQPSIPRule ", []( unsigned int qps ) { <nl> return std :: shared_ptr < DNSRule >( new MaxQPSIPRule ( qps )); <nl> });
RemoteBackend :: RemoteBackend ( const std :: string & suffix ) <nl> { <nl> setArgPrefix (" remote "+ suffix ); <nl> build ( getArg (" connection - string ")); <nl> + this -> d_result = NULL ; <nl> this -> d_dnssec = mustDo (" dnssec "); <nl> this -> d_index = - 1 ; <nl> this -> d_trxid = 0 ;
M_API M_bool M_queue_remove ( M_queue_t * queue , void * member ); <nl> * \ param member User - supplied queue object ( pointer ) to find . <nl> * \ return M_TRUE if member exists , M_FALSE otherwise . <nl> */ <nl> - M_bool M_queue_exists ( M_queue_t * queue , void * member ); <nl> + M_API M_bool M_queue_exists ( M_queue_t * queue , void * member ); <nl>  <nl>  <nl> /*! Take control of a user - supplied queue object ( pointer ). This will remove the
HTTPserver :: HTTPserver ( const char * _docs_dir , const char * _scripts_dir ) { <nl> bool use_http = true ; <nl> struct stat statsBuf ; <nl> int stat_rc ; <nl> + struct timeval tv ; <nl>  <nl> + /* Randomize data */ <nl> + gettimeofday (& tv , NULL ); <nl> + srand ( tv . tv_sec + tv . tv_usec ); <nl> + <nl> static char * http_options [] = { <nl> ( char *)" listening_ports ", ports , <nl> ( char *)" enable_directory_listing ", ( char *)" no ",
static void handle_cgi_request ( struct mg_connection * conn , const char * prog ) { <nl> struct file fout = STRUCT_FILE_INITIALIZER ; <nl> pid_t pid ; <nl>  <nl> + memset (& ri , 0 , sizeof ( ri )); <nl> prepare_cgi_environment ( conn , prog , & blk ); <nl>  <nl> // CGI must be executed in its own directory . ' dir ' must point to the
int pem_read_buffer ( pem_context * ctx , char * header , char * footer , const unsigne <nl> return ( POLARSSL_ERR_PEM_PASSWORD_MISMATCH ); <nl> } <nl> # else <nl> + free ( buf ); <nl> return ( POLARSSL_ERR_PEM_FEATURE_UNAVAILABLE ); <nl> # endif <nl> }
int ssl_get_ciphersuite_id ( const char * ciphersuite_name ) <nl>  <nl> const char * ssl_get_ciphersuite ( const ssl_context * ssl ) <nl> { <nl> + if ( ssl == NULL || ssl -> session == NULL ) <nl> + return NULL ; <nl> + <nl> return ssl_get_ciphersuite_name ( ssl -> session -> ciphersuite ); <nl> } <nl> 
nautilus_path_bar_scroll_down ( NautilusPathBar * path_bar ) <nl> * from the end , removing buttons until we get all the space we <nl> * need . */ <nl> gtk_widget_get_allocation ( BUTTON_DATA ( up_button -> data )-> button , & button_allocation ); <nl> - while ( space_available < space_needed ) { <nl> + while (( space_available < space_needed ) && <nl> + ( up_button != NULL )) { <nl> space_available += button_allocation . width + path_bar -> spacing ; <nl> up_button = up_button -> prev ; <nl> path_bar -> first_scrolled_button = up_button ;
nautilus_window_slot_switch_new_content_view ( NautilusWindowSlot * slot ) <nl> GtkWidget * widget ; <nl> gboolean reusing_view ; <nl>  <nl> - reusing_view = gtk_widget_get_parent ( GTK_WIDGET ( slot -> details -> new_content_view )) != NULL ; <nl> + reusing_view = slot -> details -> new_content_view && <nl> + gtk_widget_get_parent ( GTK_WIDGET ( slot -> details -> new_content_view )) != NULL ; <nl> /* We are either reusing the view , so new_content_view and content_view <nl> * are the same , or the new_content_view is invalid */ <nl> if ( slot -> details -> new_content_view == NULL || reusing_view )
location_cell_data_func ( GtkTreeViewColumn * column , <nl> NAUTILUS_LIST_MODEL_FILE_COLUMN , & file , <nl> - 1 ); <nl>  <nl> + /* The file might be NULL if we just toggled an expander <nl> + * and we ' re still loading the subdirectory . <nl> + */ <nl> + if ( file == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> if ( show_trash_orig && nautilus_file_is_in_trash ( file )) { <nl> NautilusFile * orig_file ; <nl> 
on_widget_destroyed ( GtkWidget * widget , <nl> self -> details -> change_idle_id = 0 ; <nl> } <nl>  <nl> + free_fade ( self ); <nl> self -> details -> widget = NULL ; <nl> } <nl> 
nautilus_toolbar_dispose ( GObject * obj ) <nl> toolbar_update_appearance , self ); <nl> unschedule_menu_popup_timeout ( self ); <nl>  <nl> + g_clear_object (& self -> priv -> zoom_adjustment_grid ); <nl> + g_clear_object (& self -> priv -> zoom_adjustment_list ); <nl> + <nl> G_OBJECT_CLASS ( nautilus_toolbar_parent_class )-> dispose ( obj ); <nl> } <nl> 
struct _NautilusPathBarClass <nl> { <nl> GtkContainerClass parent_class ; <nl>  <nl> - void (* path_clicked ) ( NautilusPathBar * path_bar , <nl> - GFile * location ); <nl> - void (* path_event ) ( NautilusPathBar * path_bar , <nl> - GdkEventButton * event , <nl> - GFile * location ); <nl> + void (* path_clicked ) ( NautilusPathBar * path_bar , <nl> + GFile * location ); <nl> + gboolean (* path_event ) ( NautilusPathBar * path_bar , <nl> + GdkEventButton * event , <nl> + GFile * location ); <nl> }; <nl>  <nl> GType nautilus_path_bar_get_type ( void ) G_GNUC_CONST ;
nautilus_link_get_link_uri_from_desktop ( GKeyFile * key_file , const char * desktop <nl> g_object_unref ( parent ); <nl> } <nl> } <nl> + g_free ( scheme ); <nl> } <nl>  <nl> return retval ;
nautilus_window_slot_content_view_matches ( NautilusWindowSlot * self , <nl> return FALSE ; <nl> } <nl>  <nl> - if ( id != NAUTILUS_VIEW_INVALID_ID && NAUTILUS_IS_FILES_VIEW ( priv -> content_view )){ <nl> + if ( id != NAUTILUS_VIEW_INVALID_ID && NAUTILUS_IS_FILES_VIEW ( priv -> content_view )) { <nl> return nautilus_files_view_get_view_id ( NAUTILUS_FILES_VIEW ( priv -> content_view )) == id ; <nl> } else { <nl> return FALSE ;
clicked_within_double_click_interval ( NautilusIconContainer * container ) <nl> /* Stash time for next compare */ <nl> last_click_time = current_time ; <nl>  <nl> - return ( click_count > 0 ); <nl> + /* Only allow double click */ <nl> + return ( click_count == 1 ); <nl> } <nl>  <nl> static void
pk_proxy_appeared_cb ( GObject * source , <nl>  <nl> /* show an unhelpful dialog */ <nl> show_unhandled_type_error ( parameters_install ); <nl> - /* The callback wasn ' t started , so we have to free the parameters */ <nl> - activate_parameters_install_free ( parameters_install ); <nl>  <nl> return ; <nl> }
done_loading ( NautilusFilesView * view , <nl> } else if ( pending_selection != NULL && all_files_seen ) { <nl> view -> details -> pending_selection = NULL ; <nl>  <nl> - nautilus_files_view_call_set_selection ( view , selection ); <nl> + nautilus_files_view_call_set_selection ( view , pending_selection ); <nl> do_reveal = TRUE ; <nl> } <nl> 
nautilus_search_hit_compute_scores ( NautilusSearchHit * hit , <nl> proximity_bonus , recent_bonus , match_bonus ); <nl>  <nl> g_date_time_unref ( now ); <nl> + g_free ( query_uri ); <nl> } <nl>  <nl> const char *
eel_canvas_button ( GtkWidget * widget , GdkEventButton * event ) <nl>  <nl> canvas = EEL_CANVAS ( widget ); <nl>  <nl> + /* Don ' t handle extra mouse button events */ <nl> + if ( event -> button > 5 ) <nl> + return FALSE ; <nl> + <nl> /* <nl> * dispatch normally regardless of the event ' s window if an item has <nl> * has a pointer grab in effect
notebook_create_window_cb ( GtkNotebook * notebook , <nl> g_object_set_data ( G_OBJECT ( slot ), " dnd - window - slot ", <nl> GINT_TO_POINTER ( TRUE )); <nl>  <nl> + gtk_window_set_position ( GTK_WINDOW ( new_window ), GTK_WIN_POS_MOUSE ); <nl> + <nl> return GTK_NOTEBOOK ( new_window -> details -> notebook ); <nl> } <nl> 
void gf_mpd_url_free ( void * _item ) <nl> { <nl> GF_MPD_URL * ptr = ( GF_MPD_URL *) _item ; <nl> if ( ptr -> sourceURL ) gf_free ( ptr -> sourceURL ); <nl> + if ( ptr -> byte_range ) gf_free ( ptr -> byte_range ); <nl> gf_free ( ptr ); <nl> } <nl> void gf_mpd_string_free ( void * _item ) {
GF_Err gf_bin128_parse ( const char * string , bin128 value ) <nl> break ; <nl> sprintf ( szV , "% c % c ", string [ j ], string [ j + 1 ]); <nl> sscanf ( szV , "% x ", & v ); <nl> + if ( i > 15 ) { <nl> + // force error check below <nl> + i ++; <nl> + break ; <nl> + } <nl> value [ i ] = v ; <nl> i ++; <nl> + <nl> } <nl> } <nl> if ( i != 16 ) {
static void TraverseCustomTexture ( GF_Node * node , void * rs , Bool is_destroy ) <nl>  <nl> static void CustomTexture_update ( GF_TextureHandler * txh ) <nl> { <nl> +# ifndef GPAC_DISABLE_3D <nl> char data [ 12 ]; <nl> +# endif <nl> CustomTextureStack * stack = gf_node_get_private ( txh -> owner ); <nl> // texture not setup , do it <nl> if (! txh -> tx_io ) {
GF_Err MergeFragment ( GF_MovieFragmentBox * moof , GF_ISOFile * mov ) <nl>  <nl> static void FixTrackID ( GF_ISOFile * mov ) <nl> { <nl> + if (! mov -> moov ) return ; <nl> + <nl> if ( gf_list_count ( mov -> moov -> trackList ) == 1 && gf_list_count ( mov -> moof -> TrackList ) == 1 ) { <nl> GF_TrackFragmentBox * traf = ( GF_TrackFragmentBox *) gf_list_get ( mov -> moof -> TrackList , 0 ); <nl> GF_TrackBox * trak = ( GF_TrackBox *) gf_list_get ( mov -> moov -> trackList , 0 );
static GF_Err gf_media_export_filters ( GF_MediaExporter * dumper ) <nl> } <nl> esd = gf_media_map_esd ( dumper -> file , track_num , 0 ); <nl> sample_count = gf_isom_get_sample_count ( dumper -> file , dumper -> trackID ); <nl> - if ( esd ) { <nl> + if ( esd && esd -> decoderConfig ) { <nl> if ( esd -> decoderConfig -> objectTypeIndication < GF_CODECID_LAST_MPEG4_MAPPING ) { <nl> codec_id = gf_codecid_from_oti ( esd -> decoderConfig -> streamType , esd -> decoderConfig -> objectTypeIndication ); <nl> # ifndef GPAC_DISABLE_AV_PARSERS
GF_Err dump_isom_xml ( GF_ISOFile * file , char * inName , Bool is_final_name , Bool do <nl> if ( e ) { <nl> fprintf ( stderr , " Error dumping ISO structure \ n "); <nl> } <nl> - if ( gf_isom_get_track_count ( file ) == 0 ) <nl> - do_track_dump = GF_FALSE ; <nl>  <nl> if ( do_track_dump ) { <nl> u32 i , j ;
s32 AVC_ReadSeqInfo ( char * sps_data , u32 sps_size , AVCState * avc , u32 subseq_sps , <nl>  <nl> pcomp = gf_bs_read_int ( bs , 8 ); <nl> /* sanity checks */ <nl> - if ( pcomp && 0x3 ) <nl> - goto exit ; <nl> + // JLF commented - breaks SVC import and no time to investigate <nl> +// if ( pcomp && 0x3 ) goto exit ; <nl>  <nl> level_idc = gf_bs_read_int ( bs , 8 ); <nl> 
void IMG_NetIO ( void * cbk , GF_NETIO_Parameter * param ) <nl> e = param -> error ; <nl> /* wait to get the whole file */ <nl> if (! e && ( param -> msg_type != GF_NETIO_DATA_TRANSFERED )) return ; <nl> + if (( e == GF_EOS ) && ( param -> msg_type == GF_NETIO_DATA_EXCHANGE )) return ; <nl>  <nl> if ( param -> msg_type == GF_NETIO_DATA_TRANSFERED ) { <nl> szCache = gf_dm_sess_get_cache_name ( read -> dnload );
static Bool enum_dir_fct ( void * cbck , char * file_name , char * file_path ) <nl> JSObject * obj ; <nl> enum_dir_cbk * cbk = ( enum_dir_cbk *) cbck ; <nl>  <nl> + if ( file_name && ( file_name [ 0 ]=='.')) return 0 ; <nl> + <nl> obj = JS_NewObject ( cbk -> c , 0 , 0 , 0 ); <nl> s = JS_NewStringCopyZ ( cbk -> c , file_name ); <nl> JS_DefineProperty ( cbk -> c , obj , " name ", STRING_TO_JSVAL ( s ), 0 , 0 , JSPROP_READONLY | JSPROP_PERMANENT );
GF_RTPHinter * gf_hinter_track_new ( GF_ISOFile * file , u32 TrackNum , <nl> max_ptime = ( u32 ) ( max_ptime * my_sl . timestampResolution / 1000 ); <nl>  <nl> my_sl . AUSeqNumLength = gf_get_bit_size ( gf_isom_get_sample_count ( file , TrackNum )); <nl> + if ( my_sl . AUSeqNumLength > 16 ) my_sl . AUSeqNumLength = 16 ; <nl> + <nl> my_sl . CUDuration = const_dur ; <nl>  <nl> if ( gf_isom_has_sync_points ( file , TrackNum )) {
GF_Box * gf_isom_box_new ( u32 boxType ) <nl>  <nl> void gf_isom_box_add_for_dump_mode ( GF_Box * parent , GF_Box * a ) <nl> { <nl> - if ( use_dump_mode && (! parent -> other_boxes || ( gf_list_find ( parent -> other_boxes , a )< 0 ) ) ) <nl> + if ( use_dump_mode && a && (! parent -> other_boxes || ( gf_list_find ( parent -> other_boxes , a )< 0 ) ) ) <nl> gf_isom_box_add_default ( parent , a ); <nl> } <nl> 
GF_Err adts_dmx_process ( GF_Filter * filter ) <nl> break ; <nl> } <nl>  <nl> + if ( ctx -> hdr . frame_size < ctx -> hdr . hdr_size ) { <nl> + GF_LOG ( GF_LOG_WARNING , GF_LOG_PARSER , ("[ ADTSDmx ] Corrupted ADTS frame header , resyncing \ n ")); <nl> + ctx -> nb_frames = 0 ; <nl> + goto drop_byte ; <nl> + } <nl> + <nl> adts_dmx_check_pid ( filter , ctx ); <nl>  <nl> if (! ctx -> is_playing ) {
static void parse_sec_attr_44 ( sc_file_t * file , const u8 * buf , size_t len ) <nl> } <nl>  <nl> /* Encryption key present ? */ <nl> - iPinCount = iACLen - 1 ; <nl> + iPinCount = iACLen > 0 ? iACLen - 1 : 0 ; <nl>  <nl> if ( buf [ iOffset ] & 0x20 ) { <nl> int iSC ;
sc_pkcs15_free_pubkey_info ( sc_pkcs15_pubkey_info_t * info ) <nl> { <nl> if ( info -> subject . value ) <nl> free ( info -> subject . value ); <nl> + if ( info -> direct . spki . value ) <nl> + free ( info -> direct . spki . value ); <nl> + if ( info -> direct . raw . value ) <nl> + free ( info -> direct . raw . value ); <nl> sc_pkcs15_free_key_params (& info -> params ); <nl> free ( info ); <nl> }
parse_dir_record ( sc_card_t * card , u8 ** buf , size_t * buflen , int rec_nr ) <nl> else <nl> app -> label = NULL ; <nl>  <nl> - if ( asn1_dirrecord [ 2 ]. flags & SC_ASN1_PRESENT ) { <nl> + if ( asn1_dirrecord [ 2 ]. flags & SC_ASN1_PRESENT && path_len > 0 ) { <nl> /* application path present : ignore AID */ <nl> if ( path_len > SC_MAX_PATH_SIZE ) { <nl> free ( app );
pgp_finish ( sc_card_t * card ) <nl> pgp_iterate_blobs ( priv -> mf , 99 , pgp_free_blob ); <nl>  <nl> free ( priv ); <nl> + card -> drv_data = NULL ; <nl> return 0 ; <nl> } <nl> 
int _sc_add_atr ( sc_context_t * ctx , struct sc_card_driver * driver , struct sc_atr_ <nl> driver -> atr_map = map ; <nl> dst = & driver -> atr_map [ driver -> natrs ++]; <nl> memset ( dst , 0 , sizeof (* dst )); <nl> + memset (& driver -> atr_map [ driver -> natrs ], 0 , sizeof ( struct sc_atr_table )); <nl> + dst -> atr = strdup ( src -> atr ); <nl> dst -> atr = strdup ( src -> atr ); <nl> if (! dst -> atr ) <nl> return SC_ERROR_OUT_OF_MEMORY ;
pkcs15_pubkey_get_attribute ( struct sc_pkcs11_session * session , void * object , CK_ <nl> *( CK_OBJECT_CLASS *) attr -> pValue = CKO_PUBLIC_KEY ; <nl> break ; <nl> case CKA_TOKEN : <nl> + check_attribute_buffer ( attr , sizeof ( CK_BBOOL )); <nl> + *( CK_BBOOL *) attr -> pValue = TRUE ; <nl> + break ; <nl> case CKA_SENSITIVE : <nl> /* By PKCS # 11 v2 . 20 public key cannot have SENSITIVE attr TRUE */ <nl> check_attribute_buffer ( attr , sizeof ( CK_BBOOL ));
coolkey_add_object ( coolkey_private_data_t * priv , unsigned long object_id , const <nl> new_object . id = object_id ; <nl> new_object . length = object_length ; <nl>  <nl> + /* The object ID needs to be unique */ <nl> + if ( coolkey_find_object_by_id (& priv -> objects_list , object_id ) != NULL ) { <nl> + return SC_ERROR_INTERNAL ; <nl> + } <nl> + <nl> if ( object_data ) { <nl> new_object . data = malloc ( object_length + add_v1_record ); <nl> if ( new_object . data == NULL ) {
iso7816_select_file ( struct sc_card * card , const struct sc_path * in_path , struct <nl> pathlen = in_path -> len ; <nl> pathtype = in_path -> type ; <nl>  <nl> + if ( file_out != NULL ) { <nl> + * file_out = NULL ; <nl> + } <nl> if ( in_path -> aid . len ) { <nl> if (! pathlen ) { <nl> memcpy ( path , in_path -> aid . value , in_path -> aid . len );
static int asn1_decode_entry ( sc_context_t * ctx , struct sc_asn1_entry * entry , <nl>  <nl> /* Strip off padding zero */ <nl> if (( entry -> flags & SC_ASN1_UNSIGNED ) <nl> - && obj [ 0 ] == 0x00 && objlen > 1 ) { <nl> + && objlen > 1 && obj [ 0 ] == 0x00 ) { <nl> objlen --; <nl> obj ++; <nl> }
int sc_pkcs15_parse_tokeninfo ( sc_context_t * ctx , <nl> sprintf ( byte , "% 02X ", serial [ ii ]); <nl> strcat ( ti -> serial_number , byte ); <nl> } <nl> + sc_log ( ctx , " TokenInfo . serialNunmber '% s '", ti -> serial_number ); <nl> } <nl>  <nl> if ( ti -> manufacturer_id == NULL ) {
void sc_notify_id ( struct sc_context * ctx , struct sc_atr * atr , <nl>  <nl> switch ( id ) { <nl> case NOTIFY_CARD_INSERTED : <nl> - icon = " dialog - information "; <nl> + icon = " contact - new "; <nl> break ; <nl> case NOTIFY_CARD_REMOVED : <nl> - icon = " media - removed "; <nl> + icon = " media - eject "; <nl> break ; <nl> case NOTIFY_PIN_GOOD : <nl> icon = " changes - allow ";
static int transform_pace_output ( u8 * rbuf , size_t rbuflen , <nl> if ( parsed + 2 > rbuflen ) <nl> return SC_ERROR_UNKNOWN_DATA_RECEIVED ; <nl> pace_output -> mse_set_at_sw1 = rbuf [ parsed + 0 ]; <nl> - pace_output -> mse_set_at_sw1 = rbuf [ parsed + 1 ]; <nl> + pace_output -> mse_set_at_sw2 = rbuf [ parsed + 1 ]; <nl> parsed += 2 ; <nl>  <nl> /* length_CardAccess */
print_aligned_vertical ( const printTableContent * cont , FILE * fout ) <nl> if ( cont -> cells [ 0 ] == NULL && cont -> opt -> start_table && <nl> cont -> opt -> stop_table ) <nl> { <nl> - if (! opt_tuples_only ) <nl> + if (! opt_tuples_only && cont -> opt -> default_footer ) <nl> fprintf ( fout , _ ("( No rows )\ n ")); <nl> return ; <nl> }
initHyperLogLog ( hyperLogLogState * cState , uint8 bwidth ) <nl> elog ( ERROR , " bit width must be between 4 and 16 inclusive "); <nl>  <nl> cState -> registerWidth = bwidth ; <nl> - cState -> nRegisters = 1 << bwidth ; <nl> + cState -> nRegisters = ( Size ) 1 << bwidth ; <nl> cState -> arrSize = sizeof ( uint8 ) * cState -> nRegisters + 1 ; <nl>  <nl> /*
static int exif_scan_JPEG_header ( image_info_type * ImageInfo ) { <nl> case M_SOF13 : <nl> case M_SOF14 : <nl> case M_SOF15 : <nl> + if (( itemlen - 2 ) < 6 ) { <nl> + return 0 ; <nl> + } <nl> + <nl> exif_process_SOFn ( Data , marker , & sof_info ); <nl> ImageInfo -> Width = sof_info . width ; <nl> ImageInfo -> Height = sof_info . height ;
static bool php_mb_parse_encoding ( const Variant & encoding , <nl> } <nl> if (! ret ) { <nl> if ( return_list && * return_list ) { <nl> - free (* return_list ); <nl> + req :: free (* return_list ); <nl> * return_list = nullptr ; <nl> } <nl> return_size = 0 ;
TranslatorX64 :: smash ( X64Assembler & a , TCA src , TCA dest , bool isCall ) { <nl> */ <nl> CodeCursor cg ( a , src ); <nl> assert ( isSmashable ( a . code . frontier , kJmpLen )); <nl> - if ( dest > src && dest - src <= 7 ) { <nl> + if ( dest > src && dest - src <= kJmpLen ) { <nl> assert (! isCall ); <nl> a . emitNop ( dest - src ); <nl> } else if (! isCall ) {
SAL_DLLPUBLIC void SAL_CALL rtl_uString_newReplaceAllAsciiL ( <nl> @ param to pointer to the replacing substring ; must not be null and must <nl> point to memory of at least \ p toLength ASCII bytes <nl>  <nl> - @ param fromLength the length of the \ p to substring ; must be non - negative <nl> + @ param toLength the length of the \ p to substring ; must be non - negative <nl>  <nl> @ since LibreOffice 5 . 1 <nl> */
static LibreOfficeKit * lok_init_2 ( const char * install_path , const char * user_p <nl> LokHookFunction2 * pSym2 ; <nl>  <nl> dlhandle = lok_dlopen ( install_path , & imp_lib ); <nl> + if (! dlhandle ) <nl> + return NULL ; <nl>  <nl> pSym2 = ( LokHookFunction2 *) lok_dlsym ( dlhandle , " libreofficekit_hook_2 "); <nl> if (! pSym2 )
int XrdHttpReq :: PostProcessHTTPReq ( bool final_ ) { <nl>  <nl> } else <nl> for ( int i = 0 ; i < iovN ; i ++) { <nl> - prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len ); <nl> + if ( prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len )) return 1 ; <nl> writtenbytes += iovP [ i ]. iov_len ; <nl> } <nl> 
do { while ( optind < Argc && Legacy ( optind )) {} <nl> switch ( opC ) <nl> { case OpCksum : defCks ( optarg ); <nl> break ; <nl> + case OpCoerce : OpSpec |= DoCoerce ; <nl> + break ; <nl> case OpDebug : OpSpec |= DoDebug ; <nl> if (! a2i ( optarg , & Dlvl , 0 , 3 )) Usage ( 22 ); <nl> break ;
namespace XrdCl <nl> XRDCL_SMART_PTR_T < Message > msgPtr ( pResponse ); <nl> pResponse = 0 ; <nl>  <nl> - if ( rsp -> hdr . dlen < 4 ) <nl> + if ( rsp -> hdr . dlen <= 4 ) <nl> { <nl> log -> Error ( XRootDMsg , "[% s ] Got invalid redirect response .", <nl> pUrl . GetHostId (). c_str () );
void DoIt () { myMutex . Lock (); <nl> virtual void Finished ( XrdSsiRequest & rqstR , <nl> const XrdSsiRespInfo & rInfo , <nl> bool cancel = false ) <nl> - { myMutex . Lock (); <nl> + { UnBindRequest (); <nl> + myMutex . Lock (); <nl> if (! isActive ) delete this ; <nl> else { isActive = false ; <nl> myMutex . UnLock ();
static void do_chanuser_sync ( mychan_t * mc , chanuser_t * cu , chanacs_t * ca , <nl> } <nl>  <nl> try_kick ( chansvs . me -> me , mc -> chan , cu -> user , " You are not authorized to be on this channel "); <nl> + return ; <nl> } <nl> if ( fl & CA_AKICK && !( fl & CA_EXEMPT )) <nl> {
void write_accounts ( void ) <nl> for ( i = 0 ; i < 1024 ; i ++) { <nl> for ( nc = nclists [ i ]; nc ; nc = nc -> next ) { <nl> athemeflags = 0 ; <nl> + if ( nc -> aliases . count == 0 ) <nl> + continue ; <nl> na = nc -> aliases . list [ 0 ]; <nl> registered = na -> time_registered ; <nl> for ( ii = 1 ; ii < nc -> aliases . count ; ii ++)
void help_display_as_subcmd ( sourceinfo_t * si , service_t * service , const char * su <nl> if (! help_file ) <nl> { <nl> command_fail ( si , fault_nosuch_target , _ (" Could not get help file for \ 2 % s \ 2 ."), command ); <nl> + free ( ccommand ); <nl> return ; <nl> } <nl> 
msg_create ( mqueue_t * mq , const char * message ) <nl> } <nl>  <nl> mowgli_node_add ( msg , & msg -> node , & mq -> entries ); <nl> + mq -> last_used = CURRTIME ; <nl>  <nl> return msg ; <nl> }
static void can_register ( hook_channel_register_check_t * req ) <nl>  <nl> return_if_fail ( req != NULL ); <nl>  <nl> + /* no point in moderating registrations from those who have PRIV_CHAN_ADMIN since they can <nl> + * approve them anyway . -- nenolod <nl> + */ <nl> + if ( has_priv ( req -> si , PRIV_CHAN_ADMIN )) <nl> + return ; <nl> + <nl> req -> approved ++; <nl>  <nl> cs = csreq_create ( req -> name , entity ( req -> si -> smu ));
PHP_MINFO_FUNCTION ( swoole ) <nl> # ifdef HAVE_PTHREAD_BARRIER <nl> php_info_print_table_row ( 2 , " pthread_barrier ", " enabled "); <nl> # endif <nl> - <nl> +# ifdef SW_USE_JEMALLOC <nl> + php_info_print_table_row ( 2 , " jemalloc ", " enabled "); <nl> +# endif <nl> +# ifdef SW_USE_TCMALLOC <nl> + php_info_print_table_row ( 2 , " tcmalloc ", " enabled "); <nl> +# endif <nl> +# ifdef SW_USE_HUGEPAGE <nl> + php_info_print_table_row ( 2 , " hugepage ", " enabled "); <nl> +# endif <nl> php_info_print_table_end (); <nl>  <nl> DISPLAY_INI_ENTRIES ();
void swTimer_node_insert ( swTimer_node ** root , swTimer_node * new_node ) <nl> swTimer_node * tmp = * root ; <nl> while ( 1 ) <nl> { <nl> - if ( tmp -> exec_msec >= new_node -> exec_msec ) <nl> + if ( tmp -> exec_msec > new_node -> exec_msec ) <nl> { <nl> new_node -> prev = tmp -> prev ; <nl> new_node -> next = tmp ;
int swServer_master_onAccept ( swReactor * reactor , swEvent * event ) <nl> { <nl> if ( swSSL_create ( conn , 0 ) < 0 ) <nl> { <nl> - conn -> active = 0 ; <nl> + bzero ( conn , sizeof ( swConnection )); <nl> close ( new_fd ); <nl> } <nl> } <nl> int swServer_master_onAccept ( swReactor * reactor , swEvent * event ) <nl>  <nl> if ( ret < 0 ) <nl> { <nl> + bzero ( conn , sizeof ( swConnection )); <nl> close ( new_fd ); <nl> return SW_OK ; <nl> }
class ModuleSSLGnuTLS : public Module <nl>  <nl> // This may be on a large ( once a day or week ) timer eventually . <nl> GenerateDHParams (); <nl> + <nl> + delete Conf ; <nl> } <nl>  <nl> void GenerateDHParams () <nl> class ModuleSSLGnuTLS : public Module <nl>  <nl> virtual ~ ModuleSSLGnuTLS () <nl> { <nl> - delete Conf ; <nl> gnutls_dh_params_deinit ( dh_params ); <nl> gnutls_certificate_free_credentials ( x509_cred ); <nl> gnutls_global_deinit ();
class ModuleSSLGnuTLS : public Module <nl> // once a day , once a week or once a month . Depending on the <nl> // security requirements . <nl>  <nl> + if (! dh_alloc ) <nl> + return ; <nl> + <nl> int ret ; <nl>  <nl> if (( ret = gnutls_dh_params_generate2 ( dh_params , dh_bits )) < 0 )
private : <nl> */ <nl> char ibuf [ 16384 ]; <nl>  <nl> + /** <nl> + * The output buffer for this socket <nl> + */ <nl> + std :: string Buffer ; <nl> + <nl> /** <nl> * The IP address being connected <nl> * to stored in string form for
class SilenceMessage : public ClientProtocol :: Message <nl> : ClientProtocol :: Message (" SILENCE ") <nl> { <nl> PushParam ( mask ); <nl> - PushParamRef ( flags ); <nl> + PushParam ( flags ); <nl> } <nl> }; <nl> 
int InspIRCd :: Run () <nl>  <nl> int main ( int argc , char ** argv ) <nl> { <nl> - InspIRCd TittyBiscuits = new InspIRCd ( argc , argv ); <nl> + InspIRCd * TittyBiscuits = new InspIRCd ( argc , argv ); <nl> TittyBiscuits -> Run (); <nl> delete TittyBiscuits ; <nl> return 0 ;
class CoreExport XLine : public classbase <nl> free ( reason ); <nl> free ( source ); <nl> } <nl> + <nl> + /** Returns true whether or not the given user is covered by this line . <nl> + */ <nl> + virtual bool Matches ( User * u ); <nl> + <nl> /** The time the line was added . <nl> */ <nl> time_t set_time ;
class ModuleNickFlood : public Module <nl>  <nl> virtual int OnUserPreNick ( userrec * user , const std :: string & newnick ) <nl> { <nl> + if ( isdigit ( newnick [ 0 ])) /* allow switches to UID */ <nl> + return 0 ; <nl> + <nl> for ( UCListIter i = user -> chans . begin (); i != user -> chans . end (); i ++) <nl> { <nl> chanrec * channel = i -> first ;
class cmd_shun : public Command <nl>  <nl> if ( pcnt == 1 ) <nl> { <nl> - if ( ServerInstance -> XLines -> DelLine ( parameters [ 0 ], " S ", user )) <nl> + if ( ServerInstance -> XLines -> DelLine ( parameters [ 0 ], " SHUN ", user )) <nl> { <nl> ServerInstance -> SNO -> WriteToSnoMask (' x ',"% s Removed shun on % s .", user -> nick , parameters [ 0 ]); <nl> }
class cmd_spynames : public command_t <nl> return CMD_FAILURE ; <nl> } <nl>  <nl> - if ( ServerInstance -> Parser -> LoopCall ( user , this , parameters , pcnt , 1 )) <nl> + if ( ServerInstance -> Parser -> LoopCall ( user , this , parameters , pcnt , 0 )) <nl> return CMD_FAILURE ; <nl>  <nl> c = ServerInstance -> FindChan ( parameters [ 0 ]);
bool LoadConf ( const char * filename , std :: stringstream * target , std :: stringstream <nl> std :: string newstuff = merge . str (); <nl> * target << newstuff ; <nl> } <nl> + else <nl> + { <nl> + // the error propogates up to its parent recursively <nl> + // causing the config reader to bail at the top level . <nl> + fclose ( conf ); <nl> + return false ; <nl> + } <nl> } <nl> else <nl> {
ModeAction ModeChannelLimit :: OnModeChange ( userrec * source , userrec * dest , chanre <nl> return MODEACTION_DENY ; <nl> } <nl>  <nl> + parameter = ConvToStr ( limit ); <nl> + <nl> /* Set new limit */ <nl> channel -> limit = limit ; <nl> channel -> modes [ CM_LIMIT ] = 1 ;
void TreeSocket :: OnTimeout () <nl>  <nl> void TreeSocket :: Close () <nl> { <nl> - if ( fd != - 1 ) <nl> - ServerInstance -> GlobalCulls . AddItem ( this ); <nl> + if ( fd < 0 ) <nl> + return ; <nl> + <nl> + ServerInstance -> GlobalCulls . AddItem ( this ); <nl> this -> BufferedSocket :: Close (); <nl> SetError (" Remote host closed connection "); <nl> 
void NativeWindow :: TitleWasSet ( content :: NavigationEntry * entry , <nl> FOR_EACH_OBSERVER ( NativeWindowObserver , <nl> observers_ , <nl> OnPageTitleUpdated (& prevent_default , text )); <nl> - if (! prevent_default ) <nl> + if (! prevent_default && ! is_closed_ ) <nl> SetTitle ( text ); <nl> } <nl> 
gfx :: Image Clipboard :: ReadImage ( mate :: Arguments * args ) { <nl> void Clipboard :: WriteImage ( const gfx :: Image & image , mate :: Arguments * args ) { <nl> ui :: ScopedClipboardWriter writer ( GetClipboardType ( args )); <nl> SkBitmap bmp ; <nl> + // TODO ( ferreus ): Replace with sk_tools_utils :: copy_to ( chrome60 ) <nl> if ( image . AsBitmap (). deepCopyTo (& bmp )) { <nl> writer . WriteImage ( bmp ); <nl> } else {
Archive ::~ Archive () { <nl> file_ . TakePlatformFile (); <nl> } <nl> # endif <nl> - base :: PostTaskWithTraits ( <nl> - FROM_HERE , <nl> - { base :: MayBlock (), base :: TaskPriority :: BACKGROUND , <nl> - base :: TaskShutdownBehavior :: CONTINUE_ON_SHUTDOWN }, <nl> - base :: Bind ([]( base :: File file ) { file . Close (); }, Passed (& file_ ))); <nl> + base :: ThreadRestrictions :: ScopedAllowIO allow_io ; <nl> + file_ . Close (); <nl> } <nl>  <nl> bool Archive :: Init () {
/* unzip . c -- IO for uncompress . zip files using zlib <nl> + <nl> + Modified for Quake III Arena to use the Z_Malloc () memory pool ; <nl> + this means a system copy of minizip is not a suitable replacement . <nl> + <nl> + Based on minizip : <nl> + <nl> Version 1 . 01e , February 12th , 2005 <nl>  <nl> Copyright ( C ) 1998 - 2005 Gilles Vollant
void AsmCall ( void ) { <nl> " doret : \ n \ t " \ <nl> " ret \ n \ t " \ <nl> : "= rm " ( callSyscallNum ), "= rm " ( callProgramStack ), "= rm " ( callOpStack ) \ <nl> - : " rm " ( instructionPointers ) \ <nl> + : " m " ( instructionPointers ) \ <nl> : " ax ", " di ", " si ", " cx " \ <nl> ); <nl> }
void compute_subject_downtime_times ( time_t start_time , time_t end_time , avail_su <nl> } <nl> saved_status = temp_as -> entry_type ; <nl> saved_stamp = temp_as -> time_stamp ; <nl> + <nl> + /* check if first time is before schedule downtime */ <nl> + if ( saved_stamp < start_time ) <nl> + saved_stamp = start_time ; <nl> + <nl> } <nl> } <nl> 
rtadv_read ( struct thread * thread ) <nl> /* Register myself . */ <nl> rtadv_event ( zvrf , RTADV_READ , sock ); <nl>  <nl> - len = rtadv_recv_packet ( sock , buf , BUFSIZ , & from , & ifindex , & hoplimit ); <nl> + len = rtadv_recv_packet ( sock , buf , sizeof ( buf ), & from , & ifindex , & hoplimit ); <nl>  <nl> if ( len < 0 ) <nl> {
 <nl> # define STREAM_VERIFY_SANE ( S ) \ <nl> do { \ <nl> - if ( !( GETP_VALID ( S , ( S )-> getp )) && ENDP_VALID ( S , ( S )-> endp ) ) \ <nl> + if ( !( GETP_VALID ( S , ( S )-> getp ) && ENDP_VALID ( S , ( S )-> endp )) ) \ <nl> STREAM_WARN_OFFSETS ( S ); \ <nl> assert ( GETP_VALID ( S , ( S )-> getp ) ); \ <nl> assert ( ENDP_VALID ( S , ( S )-> endp ) ); \
static struct static_route * static_route_alloc () <nl>  <nl> s_route = XCALLOC ( MTYPE_PIM_STATIC_ROUTE , sizeof (* s_route )); <nl> if (! s_route ) { <nl> - zlog_err (" PIM XCALLOC (% u ) failure ", sizeof (* s_route )); <nl> + zlog_err (" PIM XCALLOC (% zu ) failure ", sizeof (* s_route )); <nl> return 0 ; <nl> } <nl> return s_route ;
community_del_val ( struct community * com , u_int32_t * val ) <nl> c = com -> size - i - 1 ; <nl>  <nl> if ( c > 0 ) <nl> - memcpy ( com -> val + i , com -> val + ( i + 1 ), c * sizeof ( val )); <nl> + memcpy ( com -> val + i , com -> val + ( i + 1 ), c * sizeof (* val )); <nl>  <nl> com -> size --; <nl> 
ospf_recv_packet ( int fd , struct interface ** ifp , struct stream * ibuf ) <nl>  <nl> ip_len = iph -> ip_len ; <nl>  <nl> -# if ! defined ( GNU_LINUX ) && ( OpenBSD < 200311 ) <nl> +# if ! defined ( GNU_LINUX ) && ( OpenBSD < 200311 ) && ( __FreeBSD_version < 1000000 ) <nl> /* <nl> * Kernel network code touches incoming IP header parameters , <nl> * before protocol specific processing .
bgp_address_del ( struct prefix * p ) <nl> tmp . addr = p -> u . prefix4 ; <nl>  <nl> addr = hash_lookup ( bgp_address_hash , & tmp ); <nl> + /* may have been deleted earlier by bgp_interface_down () */ <nl> + if ( addr == NULL ) <nl> + return ; <nl> + <nl> addr -> refcnt --; <nl>  <nl> if ( addr -> refcnt == 0 )
static void dump_ctx ( struct ctx * ctx ) { <nl> * Values <nl> */ <nl> static void print_tree ( FILE * out , int indent , struct tree * tree ) { <nl> + if ( tree == NULL ) { <nl> + fprintf ( out , "( null tree )\ n "); <nl> + return ; <nl> + } <nl> list_for_each ( t , tree ) { <nl> for ( int i = 0 ; i < indent ; i ++) fputc (' ', out ); <nl> fprintf ( out , "{ ");
static const char * init_root ( const char * root0 ) { <nl> if ( root0 == NULL ) <nl> root0 = "/"; <nl> root = strdup ( root0 ); <nl> + if ( root == NULL ) <nl> + return NULL ; <nl> if ( root [ strlen ( root )- 1 ] != SEP ) { <nl> if ( REALLOC_N ( root , strlen ( root ) + 2 ) == - 1 ) { <nl> FREE ( root );
int main ( int argc , char ** argv ) { <nl> } <nl> } <nl> putchar ('\ n '); <nl> + free ( rx ); <nl> } <nl>  <nl> return 0 ;
# include " fish_version . h " <nl>  <nl> /** Command used to start fishd */ <nl> -# define FISHD_CMD L " fishd ^ / tmp / fishd . log .% s " <nl> +# define FISHD_CMD L " fishd ^ / dev / null " <nl>  <nl> // Version for easier debugging <nl> //# define FISHD_CMD L " fishd "
universal_notifier_t :: notifier_strategy_t universal_notifier_t :: resolve_default_ <nl> } <nl> # if FISH_NOTIFYD_AVAILABLE <nl> return strategy_notifyd ; <nl> +# elif defined ( __CYGWIN__ ) <nl> + return strategy_shmem_polling ; <nl> # else <nl> return strategy_named_pipe ; <nl> # endif
static int indent ( string_buffer_t * out , wchar_t * in , int flags ) <nl> { <nl> indent --; <nl> } <nl> + /* case should have the same indent level as switch */ <nl> + else if ( wcscmp ( unesc , L " case " ) == 0 ) <nl> + { <nl> + indent --; <nl> + } <nl> else if ( wcscmp ( unesc , L " end " ) == 0 ) <nl> { <nl> indent --;
namespace tnt <nl> { <nl> log_debug (" worker - process "); <nl>  <nl> + stop = false ; <nl> + <nl> if ( listeners . empty ()) <nl> { <nl> unsigned short int port = ( getuid () == 0 ? 80 : 8000 );
mrb_init_class ( mrb_state * mrb ) <nl> mrb_define_method ( mrb , mod , " define_method ", mod_define_method , ARGS_REQ ( 1 )); <nl>  <nl> mrb_define_method ( mrb , mod , "===", mrb_mod_eqq , ARGS_REQ ( 1 )); <nl> + mrb_undef_method ( mrb , cls , " append_features "); <nl> }
main ( int argc , char ** argv ) <nl> } <nl>  <nl> mrb = mrb_open (); <nl> + if ( mrb == NULL ) { <nl> + fputs (" Invalid mrb_state , exiting mruby - strip \ n ", stderr ); <nl> + return EXIT_FAILURE ; <nl> + } <nl>  <nl> ireps = ( mrb_irep **) malloc ( sizeof ( mrb_irep *) * argc ); <nl> for ( i = args_result ; i < argc ; ++ i ) {
main ( int argc , char ** argv ) <nl> char_index = 0 ; <nl> while (( last_char = getchar ()) != '\ n ') { <nl> if ( last_char == EOF ) break ; <nl> - if ( char_index > sizeof ( last_code_line )- 2 ) { <nl> + if ( char_index >= sizeof ( last_code_line )- 2 ) { <nl> fputs (" input string too long \ n ", stderr ); <nl> continue ; <nl> }
new_msym ( codegen_scope * s , mrb_sym sym ) <nl> { <nl> size_t i , len ; <nl>  <nl> + if ( s -> irep == NULL ) return 0 ; <nl> len = s -> irep -> slen ; <nl> + <nl> if ( len > 256 ) len = 256 ; <nl> for ( i = 0 ; i < len ; i ++) { <nl> if ( s -> irep -> syms [ i ] == sym ) return i ;
read_section_debug ( mrb_state * mrb , const uint8_t * start , mrb_irep * irep ) <nl> result = read_debug_record ( mrb , bin , irep , & len , filenames , filenames_len ); <nl> if ( result != MRB_DUMP_OK ) goto debug_exit ; <nl>  <nl> + bin += len ; <nl> if (( bin - start ) != bin_to_uint32 ( header -> section_size )) { <nl> result = MRB_DUMP_GENERAL_FAILURE ; <nl> }
mrb_str_inspect ( mrb_state * mrb , mrb_value str ) <nl> buf [ i ] = p [ i ]; <nl> } <nl> mrb_str_cat ( mrb , result , buf , clen ); <nl> - p += clen ; <nl> + p += clen - 1 ; <nl> continue ; <nl> } <nl> # endif
convert_type ( mrb_state * mrb , mrb_value val , const char * tname , const char * metho <nl> return mrb_nil_value (); <nl> } <nl> } <nl> - return mrb_funcall ( mrb , val , method , 0 ); <nl> + return mrb_funcall_argv ( mrb , val , m , 0 , 0 ); <nl> } <nl>  <nl> mrb_value
void restart_connections_by_peer ( struct connection * const c ) <nl> host_addr = c -> spd . that . host_addr ; <nl> } <nl>  <nl> - if ( hp_next == NULL ) { <nl> + if ( c_kind == CK_INSTANCE && hp_next == NULL ) { <nl> + /* in simple cases this is a dangling hp */ <nl> DBG ( DBG_CONTROL , <nl> DBG_log (" no connection to restart after termination ")); <nl> } else {
aggr_inI1_outR1_continue1 ( struct pluto_crypto_req_cont * pcrc <nl> /* unpack first calculation */ <nl> unpack_KE ( st , r , & st -> st_gr ); <nl>  <nl> + /* unpack nonce too */ <nl> + unpack_nonce (& st -> st_nr , r ); <nl> + <nl> /* NOTE : the " r " reply will get freed by our caller */ <nl>  <nl> /* set up second calculation */
static bool ikev2_check_fragment ( struct msg_digest * md ) <nl> " discarding IKE encrypted fragment - fragmentation not allowed by local policy ( ike_frag = no )")); <nl> return FALSE ; <nl> } <nl> + if (!( st -> st_seen_fragvid )) { <nl> + DBG ( DBG_CONTROL , DBG_log ( <nl> + " discarding IKE encrypted fragment - remote never proposed fragmentation ")); <nl> + return FALSE ; <nl> + } <nl>  <nl> DBG ( DBG_CONTROL , DBG_log ( <nl> " received IKE encrypted fragment number '% u ', total number '% u ', next payload '% u '",
 <nl> # define LSW_NFDBITS ( 8 * sizeof ( long int )) <nl> # define LSW_FDELT ( d ) (( d ) / LSW_NFDBITS ) <nl> -# define LSW_FDMASK ( d ) (( long int ) 1 << (( d ) % LSW_NFDBITS )) <nl> +# define LSW_FDMASK ( d ) (( long int ) ( 1UL << (( d ) % LSW_NFDBITS ))) <nl> # define LSW_FD_SETCOUNT (( LSW_FD_SETSIZE + LSW_NFDBITS - 1 ) / LSW_NFDBITS ) <nl>  <nl> typedef struct {
static stf_status ikev2_child_out_tail ( struct msg_digest * md ) <nl>  <nl> DBG_log (" ikev2_child_sa_respond returned STF_FAIL with % s ", <nl> enum_name (& ikev2_notify_names , v2_notify_num )); <nl> + return ret ; /* abort building the response message */ <nl> } else if ( ret != STF_OK ) { <nl> DBG_log (" ikev2_child_sa_respond returned % s ", <nl> enum_name (& stfstatus_name , ret )); <nl> + return ret ; /* abort building the response message */ <nl> } <nl> + <nl> if (! ikev2_padup_pre_encrypt ( pst , & e_pbs_cipher )) <nl> return STF_INTERNAL_ERROR ; <nl> 
static char ** new_list ( const char * value ) <nl>  <nl> /* avoid damaging original string */ <nl> val = clone_str ( value , " new_list value "); <nl> - end = val + strlen ( val ); <nl> + if ( val != NULL ) /* silence a coverity warning */ <nl> + end = val + strlen ( val ); <nl>  <nl> /* count number of items in string */ <nl> for ( b = val , count = 0 ; b < end ; ) {
static lsw_cert_ret pluto_process_certs ( struct state * st , <nl> add_crl_fetch_request_nss (& fdn , end_cert_dp ); <nl> wake_fetch_thread ( __FUNCTION__ ); <nl> } <nl> + DBGF ( DBG_X509 , " releasing end_cert_dp sent to crl fetch "); <nl> + free_generalNames ( end_cert_dp , false /* shallow */); <nl> } <nl> # endif <nl> 
ipsec_xmit_sanity_check_dev ( struct ipsec_xmit_state * ixs ) <nl> } <nl>  <nl> ixs -> physmtu = ixs -> physdev -> mtu ; <nl> - ixs -> cur_mtu = ixs -> physdev -> mtu ; <nl> + ixs -> cur_mtu = ixs -> dev -> mtu ; <nl>  <nl> ixs -> stats = ( struct net_device_stats *) &( ixs -> prv -> mystats ); <nl> 
static void retransmit_v1_msg ( struct state * st ) <nl> delay_ms = c -> r_interval ; <nl> } <nl>  <nl> - if ( delay_ms != 0 ) { <nl> + if ( delay_ms != 0 ) <nl> delay_ms = retrans_delay ( st , delay_ms ); <nl>  <nl> if ( delay_ms != 0 ) {
int do_pam_authentication ( void * varg ) <nl>  <nl> retval = pam_start (" pluto ", arg -> name . ptr , & conv , & pamh ); <nl>  <nl> + /* Send the remote host address to PAM */ <nl> + if ( retval == PAM_SUCCESS ) <nl> + retval = pam_set_item ( pamh , PAM_RHOST , pluto_ip_str (& arg -> st -> st_remoteaddr )); <nl> /* Two factor authentication - Check that the user is valid , <nl> and then check if they are permitted access */ <nl> if ( retval == PAM_SUCCESS )
parse ( cherokee_handler_ssi_t * hdl , <nl>  <nl> ret = parse ( hdl , & file_content , out ); <nl> if ( unlikely ( ret != ret_ok )) { <nl> + cherokee_buffer_mrproper (& file_content ); <nl> return ret_error ; <nl> } <nl> 
cherokee_config_node_add ( cherokee_config_node_t * conf , const char * key , cheroke <nl> } <nl>  <nl> if ( final ) { <nl> + cherokee_buffer_clean (& child -> val ); <nl> cherokee_buffer_add_buffer (& child -> val , val ); <nl> } <nl> 
entry_free ( cherokee_nonce_table_t * nonces , <nl> { <nl> cherokee_list_del (& entry -> listed ); <nl> cherokee_avl_del (& nonces -> table , & entry -> nonce , NULL ); <nl> + cherokee_buffer_mrproper (& entry -> nonce ); <nl> free ( entry ); <nl> } <nl> 
parse_x_real_ip ( cherokee_logger_t * logger , cherokee_connection_t * conn ) <nl> } <nl>  <nl> p = val ; <nl> - while (* p ) { <nl> + while (* p && ( p - val < len )) { <nl> if ((* p == ' ') || (* p == ',')) { <nl> len = p - val ; <nl> break ;
local_file_exists ( cherokee_rule_extensions_t * rule , <nl> ret = cherokee_io_stat ( srv -> iocache , tmp , rule -> use_iocache , <nl> & nocache_info , & io_entry , & info ); <nl>  <nl> - is_file = S_ISREG ( info -> st_mode ); <nl> + if ( ret == ret_ok ) { <nl> + is_file = S_ISREG ( info -> st_mode ); <nl> + } <nl>  <nl> if ( io_entry ) { <nl> cherokee_iocache_entry_unref (& io_entry );
ExprResolveLhs ( struct xkb_context * ctx , const ExprDef * expr , <nl> * elem_rtrn = NULL ; <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> ident . ident ); <nl> * index_rtrn = NULL ; <nl> - return true ; <nl> + return (* field_rtrn != NULL ); <nl> case EXPR_FIELD_REF : <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> field_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> field_ref . field );
frexpf ( float x , int * eptr ) <nl> } <nl> * eptr += ( ix >> 23 )- 126 ; <nl> hx = ( hx & 0x807fffff )| 0x3f000000 ; <nl> - *( int *)& x = hx ; <nl> + SET_FLOAT_WORD ( x , hx ); <nl> return x ; <nl> }
extern char * ptsname ( int ); <nl> extern int ptsname_r ( int , char *, size_t ); <nl> extern int getpt ( void ); <nl>  <nl> - static __inline__ int grantpt ( int __fd ) <nl> + static __inline__ int grantpt ( int __fd __attribute (( unused ))) <nl> { <nl> ( void ) __fd ; <nl> return 0 ; /* devpts does this all for us ! */
void * mmap64 ( void * addr , size_t size , int prot , int flags , int fd , off64_t offse <nl>  <nl> // prevent allocations large enough for ` end - start ` to overflow <nl> size_t rounded = BIONIC_ALIGN ( size , PAGE_SIZE ); <nl> - if ( rounded < size || size > PTRDIFF_MAX ) { <nl> + if ( rounded < size || rounded > PTRDIFF_MAX ) { <nl> errno = ENOMEM ; <nl> return MAP_FAILED ; <nl> }
void get_malloc_leak_info ( uint8_t ** info , size_t * overallSize , <nl> // debug_log (" info = % p \ n ", info ); <nl> if (* info == NULL ) { <nl> * overallSize = 0 ; <nl> - goto done ; <nl> + goto out_nomem_info ; <nl> } <nl>  <nl> // debug_log (" sorting list ...\ n "); <nl> void get_malloc_leak_info ( uint8_t ** info , size_t * overallSize , <nl> head += * infoSize ; <nl> } <nl>  <nl> + out_nomem_info : <nl> dlfree ( list ); <nl>  <nl> done :
struct android_namespace_link_t { <nl> } <nl>  <nl> bool is_accessible ( const char * soname ) const { <nl> + if ( soname == nullptr ) { <nl> + return false ; <nl> + } <nl> return allow_all_shared_libs_ || shared_lib_sonames_ . find ( soname ) != shared_lib_sonames_ . end (); <nl> } <nl> 
namespace Js <nl> const EquivalentPropertyEntry * refInfo = & properties [ pi ]; <nl> if (! this -> NullTypeHandlerBase :: IsObjTypeSpecEquivalent ( type , refInfo )) <nl> { <nl> + failedPropertyIndex = pi ; <nl> return false ; <nl> } <nl> }
namespace Js <nl> Output :: Print ( _u (" ObjectHeaderInlining : Moving inlined properties out of the object header .\ n ")); <nl> Output :: Flush (); <nl> } <nl> - Var * const newInlineSlots = reinterpret_cast < Var *>( object + 1 ); <nl> + Field ( Var ) * const newInlineSlots = reinterpret_cast < Field ( Var ) *>( object + 1 ); <nl> PropertyIndex i = newInlineSlotCapacity ; <nl> do <nl> {
File * FileRef :: create ( FileName fileName , bool readAudioProperties , <nl> File * file = new Ogg :: FLAC :: File ( fileName , readAudioProperties , audioPropertiesStyle ); <nl> if ( file -> isValid ()) <nl> return file ; <nl> + delete file ; <nl> return new Ogg :: Vorbis :: File ( fileName , readAudioProperties , audioPropertiesStyle ); <nl> } <nl> if ( ext == " FLAC ")
 <nl> int Icon :: stdSize ( int v ) <nl> { <nl> - if ( v < 20 ) { <nl> + if ( v < 19 ) { <nl> return 16 ; <nl> } else if ( v < 28 ) { <nl> return 22 ;
static Image * ReadTIFFImage ( const ImageInfo * image_info , <nl> /* <nl> Convert stripped TIFF image . <nl> */ <nl> - extent = 4 *( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> + extent = 4 *(( image -> depth + 7 )/ 8 )*( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> strip_pixels =( unsigned char *) AcquireQuantumMemory ( extent , <nl> sizeof (* strip_pixels )); <nl> if ( strip_pixels == ( unsigned char *) NULL )
static Image * ReadMPCImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> if ( LocaleCompare ( keyword ," number - meta - channels ") == 0 ) <nl> { <nl> image -> number_meta_channels = StringToUnsignedLong ( options ); <nl> + if ( image -> number_meta_channels > MaxPixelChannels ) <nl> + ThrowReaderException ( CorruptImageError , <nl> + " ImproperImageHeader "); <nl> break ; <nl> } <nl> break ;
static MagickBooleanType DrawStrokePolygon ( Image * image , <nl> for ( p = primitive_info ; p -> primitive != UndefinedPrimitive ; p += p -> coordinates ) <nl> { <nl> stroke_polygon = TraceStrokePolygon ( draw_info , p ); <nl> + if ( stroke_polygon == ( PrimitiveInfo *) NULL ) <nl> + { <nl> + status = 0 ; <nl> + break ; <nl> + } <nl> status &= DrawPolygonPrimitive ( image , clone_info , stroke_polygon , exception ); <nl> if ( status == 0 ) <nl> break ;
static ssize_t DecodePSDPixels ( const size_t number_compact_pixels , <nl> length ++; <nl> for ( j = 0 ; j < ( ssize_t ) length ; j ++) <nl> { <nl> + CheckNumberCompactPixels ; <nl> switch ( depth ) <nl> { <nl> case 1 : <nl> static ssize_t DecodePSDPixels ( const size_t number_compact_pixels , <nl> break ; <nl> } <nl> } <nl> - CheckNumberCompactPixels ; <nl> compact_pixels ++; <nl> } <nl> }
static void WriteTo8BimProfile ( Image * image , const char * name , <nl> count =( ssize_t ) value ; <nl> if (( count & 0x01 ) != 0 ) <nl> count ++; <nl> - if (( p > ( datum + length - count )) || ( count > ( ssize_t ) length )) <nl> + if (( count < 0 ) || ( p > ( datum + length - count )) || <nl> + ( count > ( ssize_t ) length )) <nl> break ; <nl> if ( id != profile_id ) <nl> p += count ;
void Magick :: Image :: read ( MagickCore :: Image * image , <nl> if (! quiet ()) <nl> throwExceptionExplicit ( MagickCore :: ImageWarning , <nl> " No image was loaded ."); <nl> + return ; <nl> } <nl> ThrowImageException ; <nl> }
static void CL_API_CALL DestroyMagickCLCacheInfoAndPixels ( <nl> } <nl> } <nl> pixels = info -> pixels ; <nl> + RelinquishMagickResource ( MemoryResource , info -> length ); <nl> DestroyMagickCLCacheInfo ( info ); <nl> ( void ) RelinquishAlignedMemory ( pixels ); <nl> }
static Image * ReadYCBCRImage ( const ImageInfo * image_info , <nl> if ( status == MagickFalse ) <nl> { <nl> quantum_info = DestroyQuantumInfo ( quantum_info ); <nl> + canvas_image = DestroyImage ( canvas_image ); <nl> return ( DestroyImageList ( image )); <nl> } <nl> SetImageColorspace ( image , YCbCrColorspace , exception );
static MagickBooleanType ReadPSDChannelPixels ( Image * image , <nl> SetPixelIndex ( image ,((( unsigned char ) pixel ) & <nl> ( 0x01 << ( 7 - bit ))) != 0 ? 0 : 255 , q ); <nl> SetPixelViaPixelInfo ( image , image -> colormap +( ssize_t ) <nl> - GetPixelIndex ( image , q ), q ); <nl> + ConstrainColormapIndex ( image , GetPixelIndex ( image , q ), <nl> + exception ), q ); <nl> q += GetPixelChannels ( image ); <nl> x ++; <nl> }
MagickExport Image * CloneImage ( const Image * image , const size_t columns , <nl> sizeof (* clone_image -> colormap )); <nl> if ( clone_image -> colormap == ( PixelInfo *) NULL ) <nl> { <nl> - clone_image = DestroyImage ( clone_image ); <nl> + image =( Image *) RelinquishMagickMemory ( image ); <nl> ThrowImageException ( ResourceLimitError ," MemoryAllocationFailed "); <nl> } <nl> ( void ) CopyMagickMemory ( clone_image -> colormap , image -> colormap , length *
ModuleExport MagickBooleanType ReadPSDLayers ( Image * image , <nl> if ( image -> debug != MagickFalse ) <nl> ( void ) LogMagickEvent ( CoderEvent , GetMagickModule (), <nl> " layer data is empty "); <nl> + if ( layer_info [ i ]. info != ( StringInfo *) NULL ) <nl> + layer_info [ i ]. info = DestroyStringInfo ( layer_info [ i ]. info ); <nl> continue ; <nl> } <nl> 
static Image * ReadPSDImage ( const ImageInfo * image_info , ExceptionInfo * exception ) <nl> image -> alpha_trait = UndefinedPixelTrait ; <nl> } <nl> } <nl> + if (( image -> depth == 1 ) && ( image -> storage_class != PseudoClass )) <nl> + ThrowReaderException ( CorruptImageError , " ImproperImageHeader "); <nl> has_merged_image = MagickTrue ; <nl> length = ReadBlobMSBLong ( image ); <nl> if ( length != 0 )
static inline MagickSizeType GetPSDSize ( const PSDInfo * psd_info , Image * image ) <nl> static inline size_t GetPSDRowSize ( Image * image ) <nl> { <nl> if ( image -> depth == 1 ) <nl> - return (( image -> columns + 7 )/ 8 ); <nl> + return ((( image -> columns + 7 )/ 8 )* GetPSDPacketSize ( image )); <nl> else <nl> return ( image -> columns * GetPSDPacketSize ( image )); <nl> }
private : <nl> } <nl> } <nl>  <nl> + UPDATE_TRACE_POINT (); <nl> APR_BRIGADE_INSERT_TAIL ( bb , b ); <nl>  <nl> b = apr_bucket_eos_create ( r -> connection -> bucket_alloc );
void CCountryFlags :: OnInit () <nl> CCountryFlag DummyEntry ; <nl> DummyEntry . m_CountryCode = - 1 ; <nl> DummyEntry . m_Texture = - 1 ; <nl> + mem_zero ( DummyEntry . m_aCountryCodeString , sizeof ( DummyEntry . m_aCountryCodeString )); <nl> m_aCountryFlags . add ( DummyEntry ); <nl> } <nl> }
evrpc_resume_request ( void * vbase , void * ctx , enum EVRPC_HOOK_RESULT res ) <nl>  <nl> (* pause -> cb )( pause -> ctx , res ); <nl> TAILQ_REMOVE ( head , pause , next ); <nl> + mm_free ( pause ); <nl> return ( 0 ); <nl> } <nl> 
epoll_init ( struct event_base * base ) <nl> fd = epollop -> timerfd = timerfd_create ( CLOCK_MONOTONIC , TFD_NONBLOCK | TFD_CLOEXEC ); <nl> if ( epollop -> timerfd >= 0 ) { <nl> struct epoll_event epev ; <nl> + memset (& epev , 0 , sizeof ( epev )); <nl> epev . data . fd = epollop -> timerfd ; <nl> epev . events = EPOLLIN ; <nl> if ( epoll_ctl ( epollop -> epfd , EPOLL_CTL_ADD , fd , & epev ) < 0 ) {
struct evbuffer_overlapped { <nl> static inline struct evbuffer_overlapped * <nl> upcast_evbuffer ( struct evbuffer * buf ) <nl> { <nl> - if (! buf || buf -> is_overlapped ) <nl> + if (! buf || ! buf -> is_overlapped ) <nl> return NULL ; <nl> return EVUTIL_UPCAST ( buf , struct evbuffer_overlapped , buffer ); <nl> }
encode_int_internal ( ev_uint8_t * data , ev_uint32_t number ) <nl> { <nl> int off = 1 , nibbles = 0 ; <nl>  <nl> - memset ( data , 0 , sizeof ( uint32_t )+ 1 ); <nl> + memset ( data , 0 , sizeof ( ev_uint32_t )+ 1 ); <nl> while ( number ) { <nl> if ( off & 0x1 ) <nl> data [ off / 2 ] = ( data [ off / 2 ] & 0xf0 ) | ( number & 0x0f );
evhttp_parse_response_line ( struct evhttp_request * req , char * line ) <nl> return (- 1 ); <nl> } <nl>  <nl> + if ( req -> response_code_line != NULL ) <nl> + mm_free ( req -> response_code_line ); <nl> if (( req -> response_code_line = mm_strdup ( readable )) == NULL ) { <nl> event_warn ("% s : strdup ", __func__ ); <nl> return (- 1 );
event_set ( struct event * ev , int fd , short events , <nl> ev -> ev_arg = arg ; <nl> ev -> ev_fd = fd ; <nl> ev -> ev_events = events ; <nl> + ev -> ev_res = 0 ; <nl> ev -> ev_flags = EVLIST_INIT ; <nl> ev -> ev_ncalls = 0 ; <nl> ev -> ev_pncalls = NULL ;
int Ftp :: Do () <nl> if ( state != CONNECTED_STATE || Error ()) <nl> return MOVED ; <nl>  <nl> - if ( expect -> Has ( Expect :: FEAT )) <nl> + if ( expect -> Has ( Expect :: FEAT ) || conn -> quit_sent ) <nl> goto usual_return ; <nl>  <nl> # if USE_SSL
int FileCopy :: Do () <nl> } <nl> if ( get -> Error () && get -> Size ()== 0 ) <nl> { <nl> - put -> PutEOF (); <nl> - Roll ( put ); <nl> + if ( put -> GetPos ()> 0 ) <nl> + { <nl> + put -> PutEOF (); <nl> + Roll ( put ); <nl> + } <nl> get_error : <nl> SetError ( get -> ErrorText ()); <nl> return MOVED ;
public : <nl> if ( o || ! n ) <nl> return ; <nl> downloader = new const TorrentPeer *[ blk_count ]; <nl> - for ( int i = 0 ; i < blk_count ; i ++) <nl> + for ( unsigned i = 0 ; i < blk_count ; i ++) <nl> downloader [ i ]= 0 ; <nl> } <nl> const TorrentPeer *& d = downloader [ block ];
_win32_read_file ( void * state , void * data , zip_uint64_t len , zip_source_cmd_t cmd <nl> zip_error_set (& ctx -> error , ZIP_ER_RENAME , _zip_set_win32_error ( GetLastError (), & ctx -> win32err )); <nl> return - 1 ; <nl> } <nl> + free ( ctx -> tmpname ); <nl> + ctx -> tmpname = NULL ; <nl> return 0 ; <nl> } <nl> 
int uv__stdio_create ( uv_loop_t * loop , <nl>  <nl> case FILE_TYPE_PIPE : <nl> CHILD_STDIO_CRT_FLAGS ( buffer , i ) = FOPEN | FPIPE ; <nl> + break ; <nl>  <nl> case FILE_TYPE_CHAR : <nl> case FILE_TYPE_REMOTE :
static void uv_loop_init ( uv_loop_t * loop ) { <nl> loop -> active_udp_streams = 0 ; <nl>  <nl> loop -> last_err = uv_ok_ ; <nl> + <nl> + memset (& loop -> counters , 0 , sizeof loop -> counters ); <nl> } <nl>  <nl> 
int uv_run ( uv_loop_t * loop , uv_run_mode mode ) { <nl>  <nl> uv__update_time ( loop ); <nl> uv__run_timers ( loop ); <nl> + uv__run_pending ( loop ); <nl> uv__run_idle ( loop ); <nl> uv__run_prepare ( loop ); <nl> - uv__run_pending ( loop ); <nl>  <nl> timeout = 0 ; <nl> if (( mode & UV_RUN_NOWAIT ) == 0 )
void uv__pipe_close ( uv_pipe_t * handle ) { <nl> */ <nl> unlink ( handle -> pipe_fname ); <nl> free (( void *) handle -> pipe_fname ); <nl> + handle -> pipe_fname = NULL ; <nl> } <nl>  <nl> uv__stream_close (( uv_stream_t *) handle );
int uv_write ( uv_write_t * req , uv_stream_t * stream , uv_buf_t bufs [], int bufcnt , <nl> req -> type = UV_WRITE ; <nl> ngx_queue_init (& req -> queue ); <nl>  <nl> - if ( bufcnt < UV_REQ_BUFSML_SIZE ) { <nl> + if ( bufcnt <= UV_REQ_BUFSML_SIZE ) { <nl> req -> bufs = req -> bufsml ; <nl> } <nl> else {
static int uv__ifaddr_exclude ( struct ifaddrs * ent ) { <nl> return 1 ; <nl> if ( ent -> ifa_addr == NULL ) <nl> return 1 ; <nl> - if ( ent -> ifa_addr -> sa_family == PF_PACKET ) <nl> + if ( ent -> ifa_addr -> sa_family != AF_INET && <nl> + ent -> ifa_addr -> sa_family != AF_INET6 ) <nl> return 1 ; <nl> return 0 ; <nl> }
int uv_async_init ( uv_async_t * async , uv_async_cb async_cb ) { <nl>  <nl> int uv_async_send ( uv_async_t * async ) { <nl> ev_async_send ( EV_DEFAULT_UC_ & async -> async_watcher ); <nl> + return 0 ; <nl> } <nl>  <nl> 
start : <nl> } <nl>  <nl> if ( n < 0 ) { <nl> - if ( errno != EAGAIN ) { <nl> + if ( errno != EAGAIN && errno != EWOULDBLOCK ) { <nl> /* Error */ <nl> req -> error = errno ; <nl> stream -> write_queue_size -= uv__write_req_size ( req );
static void huffman_decode_row ( x3f_info_t * I , x3f_directory_entry_t * DE , <nl> int col ; <nl> bit_state_t BS ; <nl>  <nl> + if ( HUF -> row_offsets . element [ row ] > ID -> data_size - 1 ) <nl> + throw LIBRAW_EXCEPTION_IO_CORRUPT ; <nl> set_bit_state (& BS , ( uint8_t *) ID -> data + HUF -> row_offsets . element [ row ]); <nl>  <nl> for ( col = 0 ; col < ID -> columns ; col ++)
static const char * static_camera_list [] = { <nl> " FujiFilm X - E1 ", <nl> " FujiFilm X - E2 ", <nl> " FujiFilm X - E2S ", <nl> + " FujiFilm X - E3 ", <nl> " FujiFilm X - M1 ", <nl> " FujiFilm XF1 ", <nl> " FujiFilm X - T1 ",
static void simple_decode_row ( x3f_info_t * I , x3f_directory_entry_t * DE , <nl> x3f_image_data_t * ID = & DEH -> data_subsection . image_data ; <nl> x3f_huffman_t * HUF = ID -> huffman ; <nl>  <nl> + if ( row * row_stride > ID -> data_size - ( ID -> columns * sizeof ( uint32_t ))) <nl> + throw LIBRAW_EXCEPTION_IO_CORRUPT ; <nl> uint32_t * data = ( uint32_t *)(( unsigned char *) ID -> data + row * row_stride ); <nl>  <nl> uint16_t c [ 3 ] = { 0 , 0 , 0 };
int MK_EXPORT _mkp_network_io_create_socket ( int domain , int type , int protocol ); <nl> int MK_EXPORT _mkp_network_io_bind ( int socket_fd , const struct sockaddr * addr , <nl> socklen_t addrlen , int backlog ); <nl> int MK_EXPORT _mkp_network_io_server ( int port , char * listen_addr , int reuse_port ); <nl> + int MK_EXPORT _mkp_network_io_buffer_size (); <nl> int MK_EXPORT _mkp_event_read ( int sockfd ); <nl> int MK_EXPORT _mkp_event_write ( int sockfd ); <nl> int MK_EXPORT _mkp_event_error ( int sockfd );
int sql_dump ( sqlite3 * db , const char * query , sqlite3_stmt ** handle ) <nl> ret = sqlite3_prepare ( db , query , - 1 , handle , NULL ); <nl> if ( ret != SQLITE_OK || ! handle ) { <nl> printf (" Error : sql_dump ()=% d % s \ n ", ret , sqlite3_errmsg ( db )); <nl> + return - 1 ; <nl> } <nl>  <nl> return ret ;
time_t mk_utils_gmt2utime ( char * date ) <nl> { <nl> time_t new_unix_time ; <nl> struct tm t_data ; <nl> + memset (& t_data , 0 , sizeof ( struct tm )); <nl>  <nl> if (! strptime ( date , GMT_DATEFORMAT , ( struct tm *) & t_data )) { <nl> return - 1 ;
int mk_http_parser ( struct mk_http_request * req , struct mk_http_parser * p , <nl> p -> chars += 7 ; <nl>  <nl> request_set (& req -> protocol_p , p , buffer ); <nl> + req -> protocol_p . len = 8 ; <nl> mk_http_set_minor_version ( buffer [ tmp + 7 ]); <nl> continue ; <nl> }
int mk_sched_check_timeouts ( struct sched_list_node * sched ) <nl>  <nl> mk_sched_remove_client ( sched , cs_node -> socket ); <nl> mk_session_remove ( cs_node -> socket ); <nl> + <nl> + /* This removal invalidated our iterator . Start over from the beginning . */ <nl> + node = rb_first ( cs_list ); <nl> + if (! node ) break ; <nl> } <nl> } <nl> }
struct mk_iov * mk_iov_create ( int n , int offset ) <nl> iov = mk_mem_malloc ( sizeof ( struct mk_iov )); <nl> iov -> iov_idx = offset ; <nl> iov -> io = mk_mem_malloc_z ( n * sizeof ( struct iovec )); <nl> - iov -> buf_to_free = mk_mem_malloc_z ( n * sizeof ( char *)); <nl> + iov -> buf_to_free = mk_mem_malloc ( n * sizeof ( char *)); <nl> iov -> buf_idx = 0 ; <nl> iov -> total_len = 0 ; <nl> iov -> size = n ;
static inline void mk_stream_set ( struct mk_stream * stream , int type , <nl> * performance and aim to make things easier . The COPYBUF type is not <nl> * used by Monkey core , at the moment the only caller is the CGI plugin . <nl> */ <nl> - if (! stream && type == MK_STREAM_COPYBUF ) { <nl> + if ( type == MK_STREAM_COPYBUF ) { <nl> stream = mk_mem_malloc ( sizeof ( struct mk_stream )); <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> char * sites_conf_dir = NULL ; <nl> char * plugins_conf_dir = NULL ; <nl> char * mimes_conf_file = NULL ; <nl> + struct mk_server * server ; <nl>  <nl> static const struct option long_opts [] = { <nl> { " configdir ", required_argument , NULL , ' c ' },
update_focus_app ( ShellWindowTracker * self ) <nl> } <nl>  <nl> set_focus_app ( self , new_focus_app ); <nl> + <nl> + g_clear_object (& new_focus_app ); <nl> } <nl>  <nl> static void
shell_doc_system_open ( ShellDocSystem * system , <nl> app_exec_quoted = g_regex_replace ( regex , app_exec , - 1 , 0 , "%%", 0 , NULL ); <nl> g_regex_unref ( regex ); <nl>  <nl> - app_info = g_app_info_create_from_commandline ( app_exec , NULL , 0 , NULL ); <nl> + app_info = g_app_info_create_from_commandline ( app_exec_quoted , NULL , 0 , NULL ); <nl> + g_free ( app_exec_quoted ); <nl>  <nl> /* The point of passing an app launch context to <nl> launch () is mostly to get startup notification and
st_box_layout_get_paint_volume ( ClutterActor * actor , <nl> ClutterActorBox content_box ; <nl> ClutterVertex origin ; <nl>  <nl> + /* Setting the paint volume does not make sense when we don ' t have any allocation */ <nl> + if (! clutter_actor_has_allocation ( actor )) <nl> + return FALSE ; <nl> + <nl> /* When have an adjustment we are clipped to the content box , so base <nl> * our paint volume on that . */ <nl> if ( priv -> hadjustment || priv -> vadjustment )
shell_global_get_runtime_state ( ShellGlobal * global , <nl> else <nl> { <nl> GBytes * bytes = g_mapped_file_get_bytes ( mfile ); <nl> - res = g_variant_new_from_bytes (( GVariantType *) property_type , bytes , TRUE ); <nl> + res = g_variant_new_from_bytes ( G_VARIANT_TYPE ( property_type ), bytes , TRUE ); <nl> g_bytes_unref ( bytes ); <nl> g_mapped_file_unref ( mfile ); <nl> }
static inline int SCSigGetFlowintType ( Signature * sig ) <nl> fi -> modifier == FLOWINT_MODIFIER_NE || <nl> fi -> modifier == FLOWINT_MODIFIER_GE || <nl> fi -> modifier == FLOWINT_MODIFIER_GT || <nl> + fi -> modifier == FLOWINT_MODIFIER_NOTSET || <nl> fi -> modifier == FLOWINT_MODIFIER_ISSET ) { <nl> read ++; <nl> } else {
static int DNP3CheckStartBytes ( const DNP3LinkHeader * header ) <nl> */ <nl> static int DNP3ContainsBanner ( const uint8_t * input , uint32_t len ) <nl> { <nl> - return memmem ( input , len , banner , strlen ( banner )) != NULL ; <nl> + return BasicSearch ( input , len , ( uint8_t *) banner , strlen ( banner )) != NULL ; <nl> } <nl>  <nl> /**
insert : <nl>  <nl> Frag * frag ; <nl> TAILQ_FOREACH ( frag , & tracker -> frags , next ) { <nl> - if ( frag_offset < frag -> offset ) <nl> + if ( new -> offset < frag -> offset ) <nl> break ; <nl> } <nl> if ( frag == NULL ) {
static int StatsOutput ( ThreadVars * tv ) <nl> const StatsCounter * pc = NULL ; <nl> void * td = stats_thread_data ; <nl>  <nl> + if ( counters_global_id == 0 ) <nl> + return - 1 ; <nl> + <nl> if ( stats_table . nstats == 0 ) { <nl> StatsThreadRegister (" Global ", & stats_ctx -> global_counter_ctx ); <nl> 
int DetectHttpHeaderMatch ( ThreadVars * t , DetectEngineThreadCtx * det_ctx , <nl>  <nl> SCMutexLock (& f -> m ); <nl>  <nl> - if ( htp_state == NULL ) { <nl> + if ( htp_state == NULL || htp_state -> connp == NULL || <nl> + htp_state -> connp -> conn == NULL ) { <nl> SCLogDebug (" No htp state , no match at http header data "); <nl> goto end ; <nl> }
static int DetectFlowvarSetup ( DetectEngineCtx * de_ctx , Signature * s , char * raws <nl> fd = SCMalloc ( sizeof ( DetectFlowvarData )); <nl> if ( unlikely ( fd == NULL )) <nl> goto error ; <nl> + memset ( fd , 0x00 , sizeof (* fd )); <nl>  <nl> fd -> content = SCMalloc ( contentlen ); <nl> if ( unlikely ( fd -> content == NULL ))
static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> uint8_t flags , <nl> uint32_t * buffer_len ) <nl> { <nl> + uint8_t * headers_buffer = NULL ; <nl> int index = 0 ; <nl> * buffer_len = 0 ; <nl>  <nl> static uint8_t * DetectEngineHHDGetBufferForTX ( int tx_id , <nl> goto end ; <nl>  <nl> htp_header_t * h = NULL ; <nl> - uint8_t * headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> + headers_buffer = det_ctx -> hhd_buffers [ index ]; <nl> size_t headers_buffer_len = 0 ; <nl>  <nl> table_iterator_reset ( headers );
static int DNSUDPResponseParse ( Flow * f , void * dstate , <nl>  <nl> tx -> replied = 1 ; <nl> } <nl> - if ( dns_state != NULL && f != NULL ) { <nl> + if ( f != NULL ) { <nl> dns_state -> last_resp = f -> lastts ; <nl> } <nl> SCReturnInt ( 1 );
IMPORTED_FUNCTION * pe_parse_import_descriptor ( <nl> // I ' ve seen binaries where OriginalFirstThunk is zero . In this case <nl> // use FirstThunk . <nl>  <nl> - if ( offset < 0 ) <nl> + if ( offset <= 0 ) <nl> offset = pe_rva_to_offset ( pe , import_descriptor -> FirstThunk ); <nl>  <nl> if ( offset < 0 )
SOFTWARE , EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE . <nl> */ <nl>  <nl> +// transition helper <nl> +# ifdef FMT_FORMAT_PROVIDE_PRINTF <nl> +# include " printf . h " <nl> +# endif <nl> + <nl> # ifndef FMT_FORMAT_H_ <nl> # define FMT_FORMAT_H_ <nl> 
void FormatDecimal ( char * buffer , uint64_t value , unsigned num_digits ) { <nl> # ifdef _MSC_VER <nl> int signbit ( double value ) { <nl> if ( value < 0 ) return 1 ; <nl> - if (! isnan ( value )) return 0 ; <nl> + if ( value == value ) return 0 ; <nl> int dec = 0 , sign = 0 ; <nl> ecvt ( value , 0 , & dec , & sign ); <nl> return sign ;
CURLcode Curl_readwrite ( struct connectdata * conn , <nl> time_t secs = time ( NULL ); <nl> k -> timeofdoc = curl_getdate ( k -> p + strlen (" Last - Modified :"), <nl> & secs ); <nl> - if ( data -> set . get_filetime >= 0 ) <nl> + if ( data -> set . get_filetime ) <nl> data -> info . filetime = k -> timeofdoc ; <nl> } <nl> else if (( k -> httpcode >= 300 && k -> httpcode < 400 ) &&
static ssize_t data_source_read_callback ( nghttp2_session * session , <nl> memcpy ( buf , stream -> upload_mem , nread ); <nl> stream -> upload_mem += nread ; <nl> stream -> upload_len -= nread ; <nl> - stream -> upload_left -= nread ; <nl> + if ( data_s -> state . infilesize != - 1 ) <nl> + stream -> upload_left -= nread ; <nl> } <nl>  <nl> if ( stream -> upload_left == 0 )
static CURLcode ssh_block_statemach ( struct connectdata * conn , <nl>  <nl> while (( sshc -> state != SSH_STOP ) && ! result ) { <nl> bool block ; <nl> - time_t left ; <nl> + time_t left = 1000 ; <nl> struct timeval now = Curl_tvnow (); <nl>  <nl> result = ssh_statemach_act ( conn , & block );
static CURLcode pop3_doing ( struct connectdata * conn , bool * dophase_done ) <nl> CURLcode result ; <nl> result = pop3_multi_statemach ( conn , dophase_done ); <nl>  <nl> - if (* dophase_done ) { <nl> + if (! result && * dophase_done ) { <nl> result = pop3_dophase_done ( conn , FALSE /* not connected */); <nl>  <nl> DEBUGF ( infof ( conn -> data , " DO phase is complete \ n "));
static CURLMcode multi_runsingle ( struct Curl_multi * multi , <nl>  <nl> case CURLM_STATE_TOOFAST : /* limit - rate exceeded in either direction */ <nl> /* if both rates are within spec , resume transfer */ <nl> + Curl_pgrsUpdate ( easy -> easy_conn ); <nl> if ( ( ( data -> set . max_send_speed == 0 ) || <nl> ( data -> progress . ulspeed < data -> set . max_send_speed )) && <nl> ( ( data -> set . max_recv_speed == 0 ) ||
static bool imap_endofresp ( struct connectdata * conn , char * line , size_t len , <nl> wordlen ++; <nl>  <nl> /* Does the server support the STARTTLS capability ? */ <nl> - if ( len >= 8 && ! memcmp ( line , " STARTTLS ", 8 )) <nl> + if ( wordlen >= 8 && ! memcmp ( line , " STARTTLS ", 8 )) <nl> imapc -> tls_supported = TRUE ; <nl>  <nl> /* Has the server explicitly disabled clear text authentication ? */
void curl_mime_free ( curl_mime * mime ) <nl> curl_mimepart * part ; <nl>  <nl> if ( mime ) { <nl> + mime_subparts_unbind ( mime ); /* Be sure it ' s not referenced anymore . */ <nl> while ( mime -> firstpart ) { <nl> part = mime -> firstpart ; <nl> mime -> firstpart = part -> nextpart ;
void ourWriteOut ( CURL * curl , struct OutStruct * outs , const char * writeinfo ) <nl> double doubleinfo ; <nl>  <nl> while ( ptr && * ptr ) { <nl> - if ('%' == * ptr ) { <nl> + if ('%' == * ptr && ptr [ 1 ]) { <nl> if ('%' == ptr [ 1 ]) { <nl> /* an escaped %- letter */ <nl> fputc ('%', stream );
CURLcode Curl_proxyCONNECT ( struct connectdata * conn , <nl> else <nl> for ( i = 0 ; i < gotbytes ; ptr ++, i ++) { <nl> perline ++; /* amount of bytes in this line so far */ <nl> - if (* ptr =='\ x0a ') { <nl> + if (* ptr == 0x0a ) { <nl> char letter ; <nl> int writetype ; <nl> 
int main ( int argc , char ** argv ) <nl> curl = curl_easy_init (); <nl> if ( curl ) { <nl> /* what call to write : */ <nl> - curl_easy_setopt ( curl , CURLOPT_URL , " HTTPS :// curl . haxx . se "); <nl> + curl_easy_setopt ( curl , CURLOPT_URL , " HTTPS :// your . favourite . ssl . site "); <nl> curl_easy_setopt ( curl , CURLOPT_WRITEHEADER , headerfile ); <nl>  <nl> while ( 1 ) /* do some ugly short cut ... */
gtls_connect_step3 ( struct connectdata * conn , <nl> infof ( data , "\ t common name : WARNING couldn ' t obtain \ n "); <nl> } <nl>  <nl> - if ( data -> set . ssl . certinfo ) { <nl> + if ( data -> set . ssl . certinfo && chainp ) { <nl> unsigned int i ; <nl>  <nl> result = Curl_ssl_init_certinfo ( data , cert_list_size );
CURLcode Curl_http ( struct connectdata * conn , bool * done ) <nl> if ( http -> writebytecount >= postsize ) { <nl> /* already sent the entire request body , mark the " upload " as <nl> complete */ <nl> - infof ( data , " upload completely sent off : %" FORMAT_OFF_T " out of " <nl> + infof ( data , " upload completely sent off : %" FORMAT_OFF_T " out of " <nl> "%" FORMAT_OFF_T " bytes \ n ", <nl> http -> writebytecount , postsize ); <nl> data -> req . upload_done = TRUE ;
CURLcode Curl_http2_switched ( struct connectdata * conn , <nl> " after upgrade : len =% zu \ n ", <nl> nread ); <nl>  <nl> - memcpy ( httpc -> inbuf , mem , nread ); <nl> + if ( nread ) <nl> + memcpy ( httpc -> inbuf , mem , nread ); <nl> httpc -> inbuflen = nread ; <nl>  <nl> nproc = nghttp2_session_mem_recv ( httpc -> h2 , ( const uint8_t *) httpc -> inbuf ,
static ssize_t send_callback ( nghttp2_session * h2 , <nl> ( void ) h2 ; <nl> ( void ) flags ; <nl>  <nl> + if (! c -> send_underlying ) <nl> + /* called before setup properly ! */ <nl> + return NGHTTP2_ERR_CALLBACK_FAILURE ; <nl> + <nl> written = (( Curl_send *) c -> send_underlying )( conn , FIRSTSOCKET , <nl> data , length , & result ); <nl> 
static void voutf ( struct GlobalConfig * config , <nl> ( void ) fwrite ( ptr , cut + 1 , 1 , config -> errors ); <nl> fputs ("\ n ", config -> errors ); <nl> ptr += cut + 1 ; /* skip the space too */ <nl> - len -= cut ; <nl> + len -= cut + 1 ; <nl> } <nl> else { <nl> fputs ( ptr , config -> errors );
static GlobCode glob_range ( URLGlob * glob , char ** patternp , <nl> } <nl> else <nl> step_n = 1 ; <nl> - if (* endp == ']') { <nl> + if ( endp && (* endp == ']')) { <nl> pattern = endp + 1 ; <nl> } <nl> else
static ParameterError str2double ( double * val , const char * str , long max ) <nl> num = strtod ( str , & endptr ); <nl> if ( errno == ERANGE ) <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> - if (( long ) num > max ) { <nl> + if ( num > max ) { <nl> /* too large */ <nl> return PARAM_NUMBER_TOO_LARGE ; <nl> }
void curl_easy_reset ( struct Curl_easy * data ) <nl>  <nl> data -> progress . flags |= PGRS_HIDE ; <nl> data -> state . current_speed = - 1 ; /* init to negative == impossible */ <nl> + <nl> + /* zero out authentication data : */ <nl> + memset (& data -> state . authhost , 0 , sizeof ( struct auth )); <nl> + memset (& data -> state . authproxy , 0 , sizeof ( struct auth )); <nl> } <nl>  <nl> /*
Ghoul2 Insert End <nl> return 0 ; <nl>  <nl> case CG_OPENJK_MENU_PAINT : <nl> - Menu_Paint ( ( menuDef_t *) VMA ( 1 ), ( intptr_t ) VMA ( 2 ) ); <nl> + Menu_Paint ( ( menuDef_t *) VMA ( 1 ), args [ 2 ] ); <nl> return 0 ; <nl>  <nl> case CG_OPENJK_GETMENU_BYNAME :
qboolean Sys_LowPhysicalMemory () <nl> if (! bAsked ) // just in case it takes a little time for GlobalMemoryStatus () to gather stats on <nl> { // stuff we don ' t care about such as virtual mem etc . <nl> bAsked = qtrue ; <nl> + <nl> + stat . dwLength = sizeof ( stat ); <nl> GlobalMemoryStatusEx (& stat ); <nl> } <nl> if ( sys_lowmem -> integer )
void Field_VariableSizeDraw ( field_t * edit , int x , int y , int width , int size , q <nl> drawLen = len - prestep ; <nl> } <nl>  <nl> + if ( drawLen < 0 ) <nl> + return ; <nl> + <nl> // extract < drawLen > characters from the field at < prestep > <nl> if ( drawLen >= MAX_STRING_CHARS ) { <nl> Com_Error ( ERR_DROP , " drawLen >= MAX_STRING_CHARS " );
void NPC_ChoosePainAnimation ( gentity_t * self , gentity_t * other , vec3_t point , i <nl> || PM_RollingAnim ( self -> client -> ps . legsAnim ) <nl> || ( BG_FlippingAnim ( self -> client -> ps . legsAnim )&&! PM_InCartwheel ( self -> client -> ps . legsAnim )) ) <nl> {// strong attacks , rolls , knockdowns , flips and spins cannot be interrupted by pain <nl> + return ; <nl> } <nl> else <nl> {// play an anim
# define GLOBAL_METHOD_CACHE_SIZE 0x800 <nl> # endif <nl> # define LSB_ONLY ( x ) (( x ) & ~(( x ) - 1 )) <nl> -# define POWOR_OF_2_P ( x ) (( x ) == LSB_ONLY ( x )) <nl> -# if ! POWOR_OF_2_P ( GLOBAL_METHOD_CACHE_SIZE ) <nl> +# define POWER_OF_2_P ( x ) (( x ) == LSB_ONLY ( x )) <nl> +# if ! POWER_OF_2_P ( GLOBAL_METHOD_CACHE_SIZE ) <nl> # error GLOBAL_METHOD_CACHE_SIZE must be power of 2 <nl> # endif <nl> # ifndef GLOBAL_METHOD_CACHE_MASK
onig_vsnprintf_with_pattern ( UChar buf [], int bufsize , OnigEncoding enc , <nl> need = ( pat_end - pat ) * 4 + 4 ; <nl>  <nl> if ( n + need < ( size_t ) bufsize ) { <nl> - xstrcat (( char * ) buf , ": /", bufsize ); <nl> + static const char sep [] = ": /"; <nl> + memcpy (( char * ) buf + n , sep , sizeof ( sep )); <nl> s = buf + onigenc_str_bytelen_null ( ONIG_ENCODING_ASCII , buf ); <nl>  <nl> p = pat ;
syck_hdlr_add_alias ( SyckParser * p , char * a ) <nl> { <nl> SyckNode * n ; <nl>  <nl> - if ( st_lookup ( p -> anchors , ( st_data_t ) a , & n ) ) <nl> + if ( st_lookup ( p -> anchors , ( st_data_t ) a , ( st_data_t *)& n ) ) <nl> { <nl> return n ; <nl> }
zstream_run_func ( void * ptr ) <nl> struct zstream * z = args -> z ; <nl> uInt n ; <nl>  <nl> + err = Z_OK ; <nl> while (! args -> interrupt ) { <nl> n = z -> stream . avail_out ; <nl> err = z -> func -> run (& z -> stream , flush );
unix_recv_io ( int argc , VALUE * argv , VALUE sock ) <nl> enum { <nl> GC_REASON_EMSGSIZE = 0x1 , <nl> GC_REASON_TRUNCATE = 0x2 , <nl> - GC_REASON_ENOMEM = 0x4 , <nl> + GC_REASON_ENOMEM = 0x4 <nl> }; <nl>  <nl> int fd ;
rb_str_cat_conv_enc_opts ( VALUE newstr , long ofs , const char * ptr , long len , <nl> long olen ; <nl>  <nl> olen = RSTRING_LEN ( newstr ); <nl> - if ( ofs < - olen || olen <= ofs ) <nl> + if ( ofs < - olen || olen < ofs ) <nl> rb_raise ( rb_eIndexError , " index % ld out of string ", ofs ); <nl> if ( ofs < 0 ) ofs += olen ; <nl> if (! from ) {
str_byte_substr ( VALUE str , long beg , long len , int empty ) <nl> beg += n ; <nl> if ( beg < 0 ) return Qnil ; <nl> } <nl> - if ( beg + len > n ) <nl> + if ( len > n - beg ) <nl> len = n - beg ; <nl> if ( len <= 0 ) { <nl> if (! empty ) return Qnil ;
condvar_ptr ( VALUE self ) <nl>  <nl> /* forked children can ' t reach into parent thread stacks */ <nl> if ( cv -> fork_gen != fork_gen ) { <nl> + cv -> fork_gen = fork_gen ; <nl> list_head_init (& cv -> waitq ); <nl> } <nl> 
static int me_gcap ( struct Client *, struct Client *, int , const char **); <nl>  <nl> struct Message capab_msgtab = { <nl> " CAPAB ", 0 , 0 , 0 , MFLG_SLOW | MFLG_UNREG , <nl> - {{ mr_capab , 0 }, mg_ignore , mg_ignore , mg_ignore , mg_ignore , mg_ignore } <nl> + {{ mr_capab , 2 }, mg_ignore , mg_ignore , mg_ignore , mg_ignore , mg_ignore } <nl> }; <nl> struct Message gcap_msgtab = { <nl> " GCAP ", 0 , 0 , 0 , MFLG_SLOW ,
valid_hostname ( const char * hostname ) <nl> if ( hostname == NULL ) <nl> return NO ; <nl>  <nl> + if (! strcmp ( hostname , " localhost ")) <nl> + return YES ; <nl> + <nl> if ('.' == * p || ':' == * p || '/' == * p ) <nl> return NO ; <nl> 
load_a_module ( const char * path , int warn , int core ) <nl> } <nl> } <nl> } <nl> + <nl> + break ; <nl> default : <nl> ilog ( L_MAIN , " Module % s has unknown / unsupported MAPI version % d .", <nl> mod_basename , MAPI_VERSION (* mapi_version ));
blacklist_dns_callback ( const char * result , bool status , query_type type , void * d <nl> { <nl> /* Done here */ <nl> notice_client ( auth -> cid , "*** IP not found in DNS blacklist % s ", <nl> - rb_dlink_list_length (& blacklist_list ) > 1 : " s " : ""); <nl> + rb_dlink_list_length (& blacklist_list ) > 1 ? " s " : ""); <nl> rb_free ( bluser ); <nl> auth -> data [ PROVIDER_BLACKLIST ] = NULL ; <nl> provider_done ( auth , PROVIDER_BLACKLIST );
delete_opm_scanner ( const char * key __unused , int parc __unused , const char ** par <nl>  <nl> rb_dlinkDelete (& proxy -> node , & proxy_scanners ); <nl> rb_free ( proxy ); <nl> + <nl> + if (! rb_dlink_list_length ( proxy_scanners )) <nl> + opm_enable = false ; <nl> } <nl>  <nl> static void <nl> delete_opm_scanner_all ( const char * key __unused , int parc __unused , const char * <nl> { <nl> opm_cancel ( auth ); <nl> } <nl> + <nl> + opm_enable = false ; <nl> } <nl>  <nl> 
opj_pi_iterator_t * opj_pi_create_decode ( opj_image_t * p_image , <nl> l_current_pi -> include = 00 ; <nl> if ( l_step_l <= ( SIZE_MAX / ( l_tcp -> numlayers + 1U ))) <nl> { <nl> - l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( l_tcp -> numlayers + 1 ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> + l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( size_t )( l_tcp -> numlayers + 1U ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> } <nl>  <nl> if
int ssl_get_ciphersuite_id ( const char * ciphersuite_name ) <nl>  <nl> const char * ssl_get_ciphersuite ( const ssl_context * ssl ) <nl> { <nl> + if ( ssl == NULL || ssl -> session == NULL ) <nl> + return NULL ; <nl> + <nl> return ssl_get_ciphersuite_name ( ssl -> session -> ciphersuite ); <nl> } <nl> 
void pureftpd_register_simple_auth_callback ( int (* callback )( const char * account , <nl> static AuthResult embedded_simple_pw_check ( const char * account , const char * password ) <nl> { <nl> AuthResult authresult ; <nl> - <nl> + <nl> + memset (& authresult , 0 , sizeof authresult ); <nl> if ( simple_auth_callback == NULL || <nl> account == NULL || * account == 0 || password == NULL ) { <nl> authresult . auth_ok = 0 ;
int init_aliases ( void ) <nl> ( tail -> dir = strdup ( dir )) == NULL ) { <nl> die_mem (); <nl> } <nl> - tail -> next = NULL ; <nl> } else { <nl> DirAlias * curr ; <nl>  <nl> int init_aliases ( void ) <nl> tail -> next = curr ; <nl> tail = curr ; <nl> } <nl> + tail -> next = NULL ; <nl> } <nl> fclose ( fp ); <nl> aliases_up ++;
char ** argv ; <nl>  <nl> void readin () <nl> { <nl> - skelout (); <nl> - <nl> - if ( ddebug ) <nl> - puts ( "# define FLEX_DEBUG " ); <nl> - <nl> if ( csize == 256 ) <nl> puts ( " typedef unsigned char YY_CHAR ;" ); <nl> else <nl> puts ( " typedef char YY_CHAR ;" ); <nl>  <nl> + skelout (); <nl> + <nl> + if ( ddebug ) <nl> + puts ( "# define FLEX_DEBUG " ); <nl> + <nl> line_directive_out ( stdout ); <nl>  <nl> if ( yyparse () )
public : <nl> inline Iterator end () const { return Iterator ( value , count ); } <nl>  <nl> inline size_t size () const { return count ; } <nl> + inline T operator []( ptrdiff_t ) const { return value ; } <nl>  <nl> private : <nl> T value ;
foptoas ( int op , Type * t , int flg ) <nl> { <nl> int et , a ; <nl>  <nl> + a = AGOK ; <nl> et = simtype [ t -> etype ]; <nl>  <nl> if ( use_sse )
Dconv ( Fmt * fp ) <nl> break ; <nl>  <nl> case D_BRANCH : <nl> - snprint ( str , sizeof ( str ), "% d ", a -> u . branch -> loc ); <nl> + if ( a -> u . branch == nil ) <nl> + snprint ( str , sizeof ( str ), "< nil >"); <nl> + else <nl> + snprint ( str , sizeof ( str ), "% d ", a -> u . branch -> loc ); <nl> break ; <nl>  <nl> case D_EXTERN :
pc2line ( uvlong pc ) <nl> if ( pc < currpc || pc > txtend ) <nl> return ~ 0 ; <nl>  <nl> - for ( c = pcline ; c < pclineend && pc <= currpc ; c ++) { <nl> + for ( c = pcline ; c < pclineend && currpc < pc ; c ++) { <nl> u = * c ; <nl> if ( u == 0 ) { <nl> currline += ( c [ 1 ]<< 24 )|( c [ 2 ]<< 16 )|( c [ 3 ]<< 8 )| c [ 4 ];
static struct DWAbbrev { <nl> DW_TAG_subrange_type , DW_CHILDREN_no , <nl> // No name ! <nl> DW_AT_type , DW_FORM_ref_addr , <nl> - DW_AT_upper_bound , DW_FORM_data1 , <nl> + DW_AT_upper_bound , DW_FORM_udata , <nl> 0 , 0 <nl> }, <nl> 
runtime  Goexit ( void ) <nl> rundefer (); <nl> runtime  goexit (); <nl> } <nl> + <nl> + void <nl> + runtime  panicdivide ( void ) <nl> +{ <nl> + runtime  panicstring (" integer divide by zero "); <nl> +}
reswitch : <nl> goto error ; <nl> switch ( t -> etype ) { <nl> default : <nl> - yyerror (" invalid operation : % N ( index of type % T )", n , t ); <nl> + yyerror (" invalid operation : % N ( type % T does not support indexing )", n , t ); <nl> goto error ; <nl>  <nl> 
dumpbv ( BitVector * bv , uintptr offset ) <nl> for ( i = 0 ; i < bv -> n ; i += BitsPerPointer ) { <nl> switch ( bv -> bytedata [ i / 8 ] >> i % 8 & 3 ) { <nl> case BitsDead : <nl> - return ; <nl> + // BitsDead has already been processed in makeheapobjbv . <nl> + // We should only see it in stack maps , in which case we should continue processing . <nl> + break ; <nl> case BitsScalar : <nl> break ; <nl> case BitsPointer :
static void list_type ( FUNC_TYPE ft , int one ) <nl> { <nl> FUNCTION * fp ; <nl> int i = 0 ; <nl> - DISPLAY_COLUMNS dc ; <nl> + DISPLAY_COLUMNS dc = { 0 }; <nl>  <nl> if (! one ) <nl> calculate_columns (& dc );
DECLARE_ASN1_FUNCTIONS ( X509_CINF ) <nl> DECLARE_ASN1_FUNCTIONS ( X509 ) <nl> DECLARE_ASN1_FUNCTIONS ( X509_CERT_AUX ) <nl>  <nl> -# define X509_new_index ( l , p , newf , dupf , freef ) \ <nl> +# define X509_get_ex_new_index ( l , p , newf , dupf , freef ) \ <nl> CRYPTO_get_ex_new_index ( CRYPTO_EX_INDEX_X509 , l , p , newf , dupf , freef ) <nl> int X509_set_ex_data ( X509 * r , int idx , void * arg ); <nl> void * X509_get_ex_data ( X509 * r , int idx );
int tls1_mac ( SSL * ssl , SSL3_RECORD * rec , unsigned char * md , int sending ) <nl> mac_ctx = hash ; <nl> } else { <nl> hmac = EVP_MD_CTX_new (); <nl> - if ( hmac == NULL || ! EVP_MD_CTX_copy ( hmac , hash )) <nl> + if ( hmac == NULL || ! EVP_MD_CTX_copy ( hmac , hash )) { <nl> + EVP_MD_CTX_free ( hmac ); <nl> return 0 ; <nl> + } <nl> mac_ctx = hmac ; <nl> } <nl> 
X509_VERIFY_PARAM * X509_VERIFY_PARAM_new ( void ) <nl>  <nl> void X509_VERIFY_PARAM_free ( X509_VERIFY_PARAM * param ) <nl> { <nl> + if ( param == NULL ) <nl> + return ; <nl> x509_verify_param_zero ( param ); <nl> OPENSSL_free ( param -> id ); <nl> OPENSSL_free ( param );
static int pkey_rsa_ctrl ( EVP_PKEY_CTX * ctx , int type , int p1 , void * p2 ) <nl> *( const EVP_MD **) p2 = rctx -> md ; <nl> } else { <nl> if ( rsa_pss_restricted ( rctx )) { <nl> - if ( EVP_MD_type ( rctx -> md ) == EVP_MD_type ( p2 )) <nl> + if ( EVP_MD_type ( rctx -> mgf1md ) == EVP_MD_type ( p2 )) <nl> return 1 ; <nl> RSAerr ( RSA_F_PKEY_RSA_CTRL , RSA_R_MGF1_DIGEST_NOT_ALLOWED ); <nl> return 0 ;
int OBJ_obj2nid ( const ASN1_OBJECT * a ) <nl> if ( a -> nid != 0 ) <nl> return ( a -> nid ); <nl>  <nl> + if ( a -> length == 0 ) <nl> + return NID_undef ; <nl> + <nl> if ( added != NULL ) { <nl> ad . type = ADDED_DATA ; <nl> ad . obj = ( ASN1_OBJECT *) a ; /* XXX : ugly but harmless */
int ossl_policy_cache_set_mapping ( X509 * x , POLICY_MAPPINGS * maps ) <nl>  <nl> ret = 1 ; <nl> bad_mapping : <nl> - if ( ret == - 1 && CRYPTO_THREAD_write_lock ( x -> lock )) { <nl> - x -> ex_flags |= EXFLAG_INVALID_POLICY ; <nl> - CRYPTO_THREAD_unlock ( x -> lock ); <nl> - } <nl> sk_POLICY_MAPPING_pop_free ( maps , POLICY_MAPPING_free ); <nl> return ret ; <nl> 
int asn1parse_main ( int argc , char ** argv ) <nl> ASN1_TYPE * atmp ; <nl> int typ ; <nl> j = atoi ( sk_OPENSSL_STRING_value ( osk , i )); <nl> - if ( j == 0 ) { <nl> + if ( j <= 0 || j >= tmplen ) { <nl> BIO_printf ( bio_err , "'% s ' is an invalid number \ n ", <nl> sk_OPENSSL_STRING_value ( osk , i )); <nl> continue ;
int PKCS12_PBE_keyivgen ( EVP_CIPHER_CTX * ctx , const char * pass , int passlen , <nl> unsigned char * salt ; <nl> unsigned char key [ EVP_MAX_KEY_LENGTH ], iv [ EVP_MAX_IV_LENGTH ]; <nl>  <nl> + if ( cipher == NULL ) <nl> + return 0 ; <nl> + <nl> /* Extract useful info from parameter */ <nl>  <nl> pbe = ASN1_TYPE_unpack_sequence ( ASN1_ITEM_rptr ( PBEPARAM ), param );
int OBJ_create ( const char * oid , const char * sn , const char * ln ) <nl>  <nl> /* Convert numerical OID string to an ASN1_OBJECT structure */ <nl> tmpoid = OBJ_txt2obj ( oid , 1 ); <nl> + if ( tmpoid == NULL ) <nl> + return 0 ; <nl>  <nl> /* If NID is not NID_undef then object already exists */ <nl> if ( OBJ_obj2nid ( tmpoid ) != NID_undef ) {
static int pkey_gost_ctrl ( EVP_PKEY_CTX * ctx , int type , int p1 , void * p2 ) <nl> return 1 ; <nl> case EVP_PKEY_CTRL_SET_IV : <nl> pctx -> shared_ukm = OPENSSL_malloc (( int ) p1 ); <nl> + if ( pctx -> shared_ukm == NULL ) <nl> + { <nl> + GOSTerr ( GOST_F_PKEY_GOST_CTRL , ERR_R_MALLOC_FAILURE ); <nl> + return 0 ; <nl> + } <nl> memcpy ( pctx -> shared_ukm , p2 ,( int ) p1 ); <nl> return 1 ; <nl> case EVP_PKEY_CTRL_PEER_KEY :
static int check_suiteb_cipher_list ( const SSL_METHOD * meth , CERT * c , <nl> * prule_str = " ECDHE - ECDSA - AES256 - GCM - SHA384 "; <nl> break ; <nl> } <nl> + /* Set auto ECDH parameter determination */ <nl> + c -> ecdh_tmp_auto = 1 ; <nl> return 1 ; <nl> } <nl> 
static void cms_env_set_version ( CMS_EnvelopedData * env ) <nl> env -> version = 2 ; <nl> } <nl> } <nl> - if ( env -> version == 2 ) <nl> - return ; <nl> if ( env -> originatorInfo || env -> unprotectedAttrs ) <nl> env -> version = 2 ; <nl> + if ( env -> version == 2 ) <nl> + return ; <nl> env -> version = 0 ; <nl> } <nl> 
static CAPI_KEY * capi_get_key ( CAPI_CTX * ctx , const TCHAR * contname , TCHAR * provn <nl> CAPI_KEY * key ; <nl> DWORD dwFlags = 0 ; <nl> key = OPENSSL_malloc ( sizeof ( CAPI_KEY )); <nl> + if ( key == NULL ) <nl> + return NULL ; <nl> if ( sizeof ( TCHAR )== sizeof ( char )) <nl> CAPI_trace ( ctx , " capi_get_key , contname =% s , provname =% s , type =% d \ n ", <nl> contname , provname , ptype );
static int init_ssl_connection ( SSL * con ) <nl> BIO_ADDR_free ( client ); <nl> dtlslisten = 0 ; <nl> i = SSL_accept ( con ); <nl> + } else { <nl> + BIO_ADDR_free ( client ); <nl> } <nl> } else <nl> # endif
int tls1_setup_key_block ( SSL * s ) <nl>  <nl> if (( p2 = ( unsigned char *) OPENSSL_malloc ( num )) == NULL ) { <nl> SSLerr ( SSL_F_TLS1_SETUP_KEY_BLOCK , ERR_R_MALLOC_FAILURE ); <nl> + OPENSSL_free ( p1 ); <nl> goto err ; <nl> } <nl> # ifdef TLS_DEBUG
static struct file_st * win32_splitter ( DSO * dso , const char * filename , <nl> DSOerr ( DSO_F_WIN32_SPLITTER , <nl> DSO_R_INCORRECT_FILE_SYNTAX ); <nl> /* goto err ;*/ <nl> + OPENSSL_free ( result ); <nl> return ( NULL ); <nl> } <nl> result -> device = start ; <nl> static char * win32_merger ( DSO * dso , const char * filespec1 , const char * filespec2 <nl>  <nl> merged = win32_joiner ( dso , filespec1_split ); <nl> } <nl> + OPENSSL_free ( filespec1_split ); <nl> + OPENSSL_free ( filespec2_split ); <nl> return ( merged ); <nl> } <nl> 
int PEM_read_bio ( BIO * bp , char ** name , char ** header , unsigned char ** data , <nl> dataB = BUF_MEM_new (); <nl> if (( nameB == NULL ) || ( headerB == NULL ) || ( dataB == NULL )) <nl> { <nl> + BUF_MEM_free ( nameB ); <nl> + BUF_MEM_free ( headerB ); <nl> + BUF_MEM_free ( dataB ); <nl> PEMerr ( PEM_F_PEM_READ_BIO , ERR_R_MALLOC_FAILURE ); <nl> return ( 0 ); <nl> }
# ifndef HEADER_STORE_H <nl> # define HEADER_STORE_H <nl>  <nl> +# include < openssl / opensslconf . h > <nl> + <nl> +# ifdef OPENSSL_NO_STORE <nl> +# error STORE is disabled . <nl> +# endif <nl> + <nl> # include < openssl / ossl_typ . h > <nl> # ifndef OPENSSL_NO_DEPRECATED <nl> # include < openssl / evp . h >
iperf_new_test () <nl> memset ( test , 0 , sizeof ( struct iperf_test )); <nl>  <nl> test -> settings = ( struct iperf_settings *) malloc ( sizeof ( struct iperf_settings )); <nl> + if (! test -> settings ) { <nl> + free ( test ); <nl> + i_errno = IENEWTEST ; <nl> + return NULL ; <nl> + } <nl> memset ( test -> settings , 0 , sizeof ( struct iperf_settings )); <nl>  <nl> return test ;
static void newstats ( struct cgpu_info * cgpu ) <nl> void update_usb_stats ( __maybe_unused struct cgpu_info * cgpu ) <nl> { <nl> # if DO_USB_STATS <nl> + if ( cgpu -> usbstat < 1 ) <nl> + newstats ( cgpu ); <nl> + <nl> // we don ' t know the device_id until after add_cgpu () <nl> usb_stats [ cgpu -> usbstat - 1 ]. device_id = cgpu -> device_id ; <nl> # endif
static bool klondike_get_stats ( struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= slaves ; dev ++) { <nl> char * reply = SendCmdGetReply ( klncgpu , ' S ', dev , 0 , NULL ); <nl> if ( reply != NULL ) <nl> - klninfo -> status [ dev ] = *( WORKSTATUS *)( reply + 2 ); <nl> + memcpy (( void *)(&( klninfo -> status [ dev ])), reply + 2 , sizeof ( klninfo -> status [ dev ])); <nl> } <nl> wr_unlock (&( klninfo -> stat_lock )); <nl> 
retry : <nl> struct stat statbuf ; <nl>  <nl> strcpy ( filename , str ); <nl> + free ( str ); <nl> if (! stat ( filename , & statbuf )) { <nl> wlogprint (" File exists , overwrite ?\ n "); <nl> input = getch (); <nl> retry : <nl> goto retry ; <nl> } <nl> } <nl> + else <nl> + free ( str ); <nl> fcfg = fopen ( filename , " w "); <nl> if (! fcfg ) { <nl> wlogprint (" Cannot open or create file \ n ");
static void * miner_thread ( void * userdata ) <nl> nonce_inc = next_inc ; <nl> } else if (! diff . tv_sec ) <nl> nonce_inc = hashes_done * 2 ; <nl> + if ( nonce_inc < 4 ) <nl> + nonce_inc = 0xffffff ; <nl> max64 = work -> blk . nonce + nonce_inc ; <nl> if ( max64 > 0xfffffffaULL ) <nl> max64 = 0xfffffffaULL ;
CBlock * CreateNewBlock ( CReserveKey & reservekey ) <nl> int64 nValueIn = coins . vout [ txin . prevout . n ]. nValue ; <nl> nTotalIn += nValueIn ; <nl>  <nl> - int nConf = pindexPrev -> nHeight - coins . nHeight ; <nl> + int nConf = pindexPrev -> nHeight - coins . nHeight + 1 ; <nl>  <nl> dPriority += ( double ) nValueIn * nConf ; <nl> }
read_gif ( Gif_Reader * grr , int read_flags , <nl> Gif_DeleteArray ( gfc . suffix ); <nl> Gif_DeleteArray ( gfc . length ); <nl> gfc . gfi = 0 ; <nl> + last_name = 0 ; <nl>  <nl> if ( gfs ) <nl> gfs -> errors = gfc . errors [ 1 ];
static int file_to_data ( const char * path , char ** data , size_t * len ) <nl> path , strerror ( errno )); <nl> goto err ; <nl> } <nl> + if (! sb . st_size ) { <nl> + * len = 0 ; <nl> + return 0 ; <nl> + } <nl>  <nl> * data = mmap ( NULL , sb . st_size , PROT_READ , MAP_PRIVATE , fd , 0 ); <nl> if (* data == MAP_FAILED ) {
char * sepol_av_to_string ( policydb_t * policydbp , uint32_t tclass , <nl> int rc ; <nl> int avlen = 0 , len ; <nl>  <nl> + memset ( avbuf , 0 , sizeof avbuf ); <nl> cladatum = policydbp -> class_val_to_struct [ tclass - 1 ]; <nl> p = avbuf ; <nl> for ( i = 0 ; i < cladatum -> permissions . nprim ; i ++) {
int main ( int argc , char ** argv ) <nl> printf ("% s \ n ", context ); <nl> freecon ( context ); <nl> } <nl> + free ( pc [ i ]); <nl> } <nl>  <nl> printf ("\ nFile contexts :\ n "); <nl> int main ( int argc , char ** argv ) <nl> freecon ( context ); <nl> } <nl> } <nl> + free ( fc [ i ]); <nl> } <nl>  <nl> return 0 ;
static void cil_reset_classpermission ( struct cil_classpermission * cp ) <nl> return ; <nl> } <nl>  <nl> - cil_reset_classperms_list ( cp -> classperms ); <nl> + cil_list_destroy (& cp -> classperms , CIL_FALSE ); <nl> } <nl>  <nl> static void cil_reset_classperms_set ( struct cil_classperms_set * cp_set )
static void cil_reset_classpermission ( struct cil_classpermission * cp ) <nl>  <nl> static void cil_reset_classperms_set ( struct cil_classperms_set * cp_set ) <nl> { <nl> - cil_reset_classpermission ( cp_set -> set ); <nl> + if ( cp_set == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> + cp_set -> set = NULL ; <nl> } <nl>  <nl> static inline void cil_reset_classperms_list ( struct cil_list * cp_list )
rsRetVal actionDestruct ( action_t * pThis ) <nl> pthread_mutex_destroy (& pThis -> mutActExec ); <nl> d_free ( pThis -> pszName ); <nl> d_free ( pThis -> ppTpl ); <nl> + d_free ( pThis -> peParamPassing ); <nl>  <nl> finalize_it : <nl> d_free ( pThis );
BEGINrunInput <nl> # endif <nl>  <nl> CODESTARTrunInput <nl> + CHKmalloc ( pReadfds ); <nl> if ( runModConf -> bOmitLocalLogging && nfd == 1 ) <nl> ABORT_FINALIZE ( RS_RET_OK ); <nl> /* this is an endless loop - it is terminated when the thread is
scriptExec ( struct cnfstmt * root , msg_t * pMsg , wti_t * pWti ) <nl> struct cnfstmt * stmt ; <nl>  <nl> for ( stmt = root ; stmt != NULL ; stmt = stmt -> next ) { <nl> + if (* pWti -> pbShutdownImmediate ) { <nl> + DBGPRINTF (" scriptExec : ShutdownImmediate set , " <nl> + " force terminating \ n "); <nl> + goto done ; <nl> + } <nl> if ( Debug ) { <nl> cnfstmtPrintOnly ( stmt , 2 , 0 ); <nl> }
buildSeverityMapping ( instanceData * pData ) <nl> uchar pszSevCode [ 512 ]; <nl> int sevCode ; <nl> uchar * mapping ; <nl> - struct severMap_s * node ; <nl> + struct severMap_s * node = NULL ; <nl> DEFiRet ; <nl>  <nl> mapping = cs . pszSeverityMapping ; <nl> buildSeverityMapping ( instanceData * pData ) <nl> } <nl>  <nl> finalize_it : <nl> + if ( iRet != RS_RET_OK ) { <nl> + if ( node != NULL ) <nl> + free ( node ); <nl> + } <nl> RETiRet ; <nl> } <nl> 
split_binary_parameters ( uchar ** const szBinary , char *** const __restrict__ aPara <nl> (* aParams )[ iPrm ] = NULL ; /* NULL per argv [] convention */ <nl>  <nl> finalize_it : <nl> + if ( estrBinary != param_binary ) { <nl> + es_deleteStr ( estrBinary ); <nl> + } <nl> + if ( estrParams != NULL ) { <nl> + es_deleteStr ( estrParams ); <nl> + } <nl> RETiRet ; <nl> }
doRetry ( nsd_gtls_t * pNsd ) <nl> break ; <nl> default : <nl> assert ( 0 ); /* this shall not happen ! */ <nl> + dbgprintf (" ERROR : pNsd -> rtryCall invalid in nsdsel_gtls . c :% d \ n ", __LINE__ ); <nl> + gnuRet = 0 ; /* if it happens , we have at least a defined behaviour ... ;) */ <nl> break ; <nl> } <nl> 
CODESTARTfreeWrkrInstance <nl> pWrkrData -> curlHandle = NULL ; <nl> } <nl> free ( pWrkrData -> restURL ); <nl> + es_deleteStr ( pWrkrData -> batch . data ); <nl> ENDfreeWrkrInstance <nl>  <nl> BEGINdbgPrintInstInfo
irc_server_msgq_flush () <nl> /* new_msg = plugin_modifier_exec ( PLUGIN_MODIFIER_IRC_IN , <nl> irc_recv_msgq -> server -> name , <nl> ptr_data );*/ <nl> + new_msg = NULL ; <nl> + <nl> /* no changes in new message */ <nl> if ( new_msg && ( strcmp ( ptr_data , new_msg ) == 0 )) <nl> {
gui_chat_draw ( struct t_gui_buffer * buffer , int clear_chat ) <nl>  <nl> if ( clear_chat ) <nl> { <nl> - snprintf ( format_empty , 32 , "%%-% ds ", ptr_win -> win_chat_width ); <nl> + snprintf ( format_empty , sizeof ( format_empty ), <nl> + "%%-% ds ", ptr_win -> win_chat_width ); <nl> for ( i = 0 ; i < ptr_win -> win_chat_height ; i ++) <nl> { <nl> mvwprintw ( GUI_WINDOW_OBJECTS ( ptr_win )-> win_chat , i , 0 ,
irc_ctcp_dcc_filename_without_quotes ( const char * filename ) <nl> int length ; <nl>  <nl> length = strlen ( filename ); <nl> - if ( length > 0 ) <nl> + if ( length > 1 ) <nl> { <nl> if (( filename [ 0 ] == '\"') && ( filename [ length - 1 ] == '\"')) <nl> return weechat_strndup ( filename + 1 , length - 2 );
relay_irc_recv ( struct t_relay_client * client , const char * data ) <nl> /* server capabilities */ <nl> if ( irc_command && ( weechat_strcasecmp ( irc_command , " cap ") == 0 )) <nl> { <nl> - if (( irc_argc > 0 ) && irc_argv ) <nl> + if ( irc_argc > 0 ) <nl> { <nl> relay_irc_recv_command_capab ( client , <nl> irc_argc , irc_argv , irc_argv_eol );
archive_acl_from_text_l ( struct archive_acl * acl , const char * text , <nl> st = field [ n ]. start + 1 ; <nl> len = field [ n ]. end - field [ n ]. start ; <nl>  <nl> + if ( len == 0 ) { <nl> + ret = ARCHIVE_WARN ; <nl> + continue ; <nl> + } <nl> + <nl> switch (* s ) { <nl> case ' u ': <nl> if ( len == 1 || ( len == 4
parse_codes ( struct archive_read * a ) <nl> new_size = DICTIONARY_MAX_SIZE ; <nl> else <nl> new_size = rar_fls (( unsigned int ) rar -> unp_size ) << 1 ; <nl> + if ( new_size == 0 ) { <nl> + archive_set_error (& a -> archive , ARCHIVE_ERRNO_FILE_FORMAT , <nl> + " Zero window size is invalid ."); <nl> + return ( ARCHIVE_FATAL ); <nl> + } <nl> new_window = realloc ( rar -> lzss . window , new_size ); <nl> if ( new_window == NULL ) { <nl> archive_set_error (& a -> archive , ENOMEM ,
parse_codes ( struct archive_read * a ) <nl> rar -> range_dec . Stream = & rar -> bytein ; <nl> __archive_ppmd7_functions . Ppmd7_Construct (& rar -> ppmd7_context ); <nl>  <nl> + if ( rar -> dictionary_size == 0 ) { <nl> + archive_set_error (& a -> archive , ARCHIVE_ERRNO_FILE_FORMAT , <nl> + " Invalid zero dictionary size "); <nl> + return ( ARCHIVE_FATAL ); <nl> + } <nl> + <nl> if (! __archive_ppmd7_functions . Ppmd7_Alloc (& rar -> ppmd7_context , <nl> rar -> dictionary_size , & g_szalloc )) <nl> {
_warc_read ( struct archive_read * a , const void ** buf , size_t * bsz , int64_t * off ) <nl> return ( ARCHIVE_EOF ); <nl> } <nl>  <nl> + if ( w -> unconsumed ) { <nl> + __archive_read_consume ( a , w -> unconsumed ); <nl> + w -> unconsumed = 0U ; <nl> + } <nl> + <nl> rab = __archive_read_ahead ( a , 1U , & nrd ); <nl> if ( nrd < 0 ) { <nl> * bsz = 0U ;
static void doXPathDump ( xmlXPathObjectPtr cur ) { <nl> # ifdef LIBXML_OUTPUT_ENABLED <nl> xmlSaveCtxtPtr ctxt ; <nl>  <nl> - if ( cur -> nodesetval -> nodeNr <= 0 ) { <nl> + if (( cur -> nodesetval == NULL ) || ( cur -> nodesetval -> nodeNr <= 0 )) { <nl> fprintf ( stderr , " XPath set is empty \ n "); <nl> progresult = XMLLINT_ERR_XPATH ; <nl> break ;
# define HAVE_EC <nl> # endif <nl>  <nl> -// ( test for == 1 . 1 . 1pre8 ) <nl> -# if OPENSSL_VERSION_NUMBER == ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> +// ( test for >= 1 . 1 . 1pre8 ) <nl> +# if OPENSSL_VERSION_NUMBER >= ( PACKED_OPENSSL_VERSION_PLAIN ( 1 , 1 , 1 ) - 7 ) \ <nl> && ! defined ( HAS_LIBRESSL ) \ <nl> && defined ( HAVE_EC ) <nl> // EXPERIMENTAL :
Port * erts_get_heart_port () { <nl>  <nl> for ( ix = 0 ; ix < erts_max_ports ; ix ++) { <nl> port = & erts_port [ ix ]; <nl> - /* immediate compare */ <nl> + /* only examine undead or alive ports */ <nl> + if ( port -> status & ERTS_PORT_SFLGS_DEAD ) <nl> + continue ; <nl> + /* immediate atom compare */ <nl> if ( port -> reg && port -> reg -> name == am_heart_port ) { <nl> return port ; <nl> }
gen_select_val ( LoaderState * stp , GenOpArg S , GenOpArg Fail , <nl> op -> a [ j + size ] = Fail ; <nl>  <nl> # ifdef DEBUG <nl> - for ( i = 0 ; i < size ; i ++) { <nl> + for ( i = 0 ; i < size - 1 ; i ++) { <nl> ASSERT ( op -> a [ i + 3 ]. val <= op -> a [ i + 4 ]. val ); <nl> } <nl> # endif
static size_t add_index_color ( char * buf , size_t buflen , format_flag flags , char <nl> if (!( flags & MUTT_FORMAT_INDEX )) <nl> return 0 ; <nl>  <nl> + /* this item is going to be passed to an external filter */ <nl> + if ( flags & MUTT_FORMAT_NOFILTER ) <nl> + return 0 ; <nl> + <nl> if ( color == MT_COLOR_INDEX ) <nl> { /* buf might be uninitialized other cases */ <nl> len = mutt_strlen ( buf );
bool set_default_value ( const char * name , intptr_t value ) <nl> return false ; <nl>  <nl> int idx = mutt_option_index ( name ); <nl> - if (! idx ) <nl> + if ( idx < 0 ) <nl> return false ; <nl>  <nl> MuttVars [ idx ]. initial = value ;
int mutt_index_menu ( void ) <nl> char buf [ 128 ]; <nl>  <nl> buf [ 0 ] = '\ 0 '; <nl> - if (! mutt_get_field (" Enter macro stroke : ", buf , sizeof ( buf ), <nl> + if (! mutt_get_field ( _ (" Enter macro stroke : "), buf , sizeof ( buf ), <nl> MUTT_CLEAR ) && buf [ 0 ]) <nl> { <nl> snprintf ( str , sizeof ( str ), "% s % s ", MarkMacroPrefix , buf );
int mutt_pattern_func ( int op , char * prompt ) <nl> simple = safe_strdup ( buf ); <nl> mutt_check_simple ( buf , sizeof ( buf ), NONULL ( SimpleSearch )); <nl>  <nl> + memset (& err , 0 , sizeof ( err )); <nl> err . data = error ; <nl> err . dsize = sizeof ( error ); <nl> if (( pat = mutt_pattern_comp ( buf , M_FULL_MSG , & err )) == NULL )
static int label_message ( HEADER * hdr , char * new ) <nl> mutt_free_list (& hdr -> env -> labels ); <nl> } <nl>  <nl> - if ( new == NULL ) <nl> - hdr -> env -> labels = NULL ; <nl> - else <nl> + if (( new != NULL ) && (* new != '\ 0 ')) <nl> { <nl> char * last , * label ; <nl> 
int mutt_index_menu ( void ) <nl> imap_allow_reopen ( Context ); <nl> # endif <nl>  <nl> - index_hint = ( Context -> vcount && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl> + index_hint = ( Context -> vcount && menu -> current >= 0 && menu -> current < Context -> vcount ) ? CURHDR -> index : 0 ; <nl>  <nl> if (( check = mx_check_mailbox ( Context , & index_hint , 0 )) < 0 ) <nl> {
void imap_quote_string ( char * dest , size_t dlen , const char * src , bool quote_back <nl> const char * s = src ; <nl>  <nl> * pt ++ = '"'; <nl> - /* save room for trailing quote - char */ <nl> - dlen -= 2 ; <nl> + /* save room for quote - chars */ <nl> + dlen -= 3 ; <nl>  <nl> for (; * s && dlen ; s ++) <nl> {
serial_open ( const char * devpath , unsigned long baud , signed short timeout , bool p <nl> switch ( baud ) { <nl> case 0 : <nl> break ; <nl> + case 57600 : <nl> + cfsetispeed ( & my_termios , B57600 ); <nl> + cfsetospeed ( & my_termios , B57600 ); <nl> + break ; <nl> case 115200 : <nl> cfsetispeed ( & my_termios , B115200 ); <nl> cfsetospeed ( & my_termios , B115200 );
static void set_work_target ( struct work * work , int diff ) <nl> free ( htarget ); <nl> } <nl> } <nl> - memcpy ( work -> target , target , 256 ); <nl> + memcpy ( work -> target , target , 32 ); <nl> } <nl>  <nl> static void gen_stratum_work ( struct pool * pool , struct work * work )
retry : <nl> struct stat statbuf ; <nl>  <nl> strcpy ( filename , str ); <nl> + free ( str ); <nl> if (! stat ( filename , & statbuf )) { <nl> wlogprint (" File exists , overwrite ?\ n "); <nl> input = getch (); <nl> retry : <nl> goto retry ; <nl> } <nl> } <nl> + else <nl> + free ( str ); <nl> fcfg = fopen ( filename , " w "); <nl> if (! fcfg ) { <nl> wlogprint (" Cannot open or create file \ n ");
void _simplelog ( int prio , const char * str , bool force ) <nl> { <nl> # ifdef HAVE_SYSLOG_H <nl> if ( use_syslog ) { <nl> - syslog ( prio , "% s ", str ); <nl> + syslog ( LOG_LOCAL0 | prio , "% s ", str ); <nl> } <nl> # else <nl> if ( 0 ) {}
struct cg_usb_tmo { <nl> struct cg_usb_info { <nl> uint8_t bus_number ; <nl> uint8_t device_address ; <nl> - int which_intinfo ; <nl> int usbstat ; <nl> bool nodev ; <nl> int nodev_count ;
static bool klondike_get_stats ( struct cgpu_info * klncgpu ) <nl> for ( dev = 0 ; dev <= slaves ; dev ++) { <nl> char * reply = SendCmdGetReply ( klncgpu , ' S ', dev , 0 , NULL ); <nl> if ( reply != NULL ) <nl> - klninfo -> status [ dev ] = *( WORKSTATUS *)( reply + 2 ); <nl> + memcpy (( void *)(&( klninfo -> status [ dev ])), reply + 2 , sizeof ( klninfo -> status [ dev ])); <nl> } <nl> wr_unlock (&( klninfo -> stat_lock )); <nl> 
mm_answer_skeyrespond ( int sock , Buffer * m ) <nl> debug3 ("% s : sending authenticated : % d ", __func__ , authok ); <nl> mm_request_send ( sock , MONITOR_ANS_SKEYRESPOND , m ); <nl>  <nl> - auth_method = " skey "; <nl> + auth_method = " keyboard - interactive "; <nl> + auth_submethod = " skey "; <nl>  <nl> return ( authok != 0 ); <nl> }
bindresvport_sa ( int sd , struct sockaddr * sa ) <nl> if ( sa == NULL ) { <nl> memset (& myaddr , 0 , sizeof ( myaddr )); <nl> sa = ( struct sockaddr *)& myaddr ; <nl> + salen = sizeof ( myaddr ); <nl>  <nl> if ( getsockname ( sd , sa , & salen ) == - 1 ) <nl> return - 1 ; /* errno is correctly set */
static int tcmu_rbd_lock_break ( struct tcmu_device * dev , char ** orig_owner ) <nl> tcmu_dev_err ( dev , " Could not break lock from % s . ( Err % d )\ n ", <nl> owners [ 0 ], ret ); <nl> if ( ret == - ETIMEDOUT ) <nl> - return ret ; <nl> + goto free_owners ; <nl>  <nl> ret = - EAGAIN ; <nl> if (!* orig_owner ) {
struct tcmulib_context * tcmulib_initialize ( <nl> teardown_netlink ( ctx -> nl_sock ); <nl> darray_free ( ctx -> handlers ); <nl> darray_free ( ctx -> devices ); <nl> + genl_unregister_family (& tcmu_ops ); <nl> + free ( ctx ); <nl> return NULL ; <nl> } <nl> 
on_unregister_handler ( TCMUService1HandlerManager1 * interface , <nl> gpointer user_data ) <nl> { <nl> struct tcmur_handler * handler = find_handler_by_subtype ( subtype ); <nl> - struct dbus_info * info = handler -> opaque ; <nl> + struct dbus_info * info = handler ? handler -> opaque : NULL ; <nl>  <nl> if (! handler ) { <nl> g_dbus_method_invocation_return_value ( invocation ,
int main ( int argc , char ** argv ) <nl> darray_foreach ( tmp_r_handler , g_runner_handlers ) { <nl> struct tcmulib_handler tmp_handler ; <nl>  <nl> + memset (& tmp_handler , 0 , sizeof ( tmp_handler )); <nl> tmp_handler . name = (* tmp_r_handler )-> name ; <nl> tmp_handler . subtype = (* tmp_r_handler )-> subtype ; <nl> tmp_handler . cfg_desc = (* tmp_r_handler )-> cfg_desc ;
int main ( int argc , char ** argv ) <nl> { <nl> char * rootdir = get_rootdir ( pid ); <nl>  <nl> - dd_create_basic_files ( dd , fsuid , ( rootdir && strcmp ( rootdir , "/") != 0 ) ? rootdir : NULL ); <nl> + dd_create_basic_files ( dd , fsuid , NULL ); <nl>  <nl> char source_filename [ sizeof ("/ proc /% lu / somewhat_long_name ") + sizeof ( long )* 3 ]; <nl> int source_base_ofs = sprintf ( source_filename , "/ proc /% lu / smaps ", ( long ) pid );
static off_t copyfd_sparse ( int src_fd , int dst_fd1 , int dst_fd2 , off_t size2 ) <nl> size2 -= rd ; <nl> if ( size2 < 0 ) <nl> dst_fd2 = - 1 ; <nl> +// TODO : truncate to 0 or even delete the second file <nl> +//( currently we delete the file later ) <nl> } <nl> out : <nl> 
int main ( int argc , char ** argv ) <nl> int inotify_fd = inotify_init (); <nl> if ( inotify_fd == - 1 ) <nl> perror_msg_and_die (" inotify_init failed "); <nl> + close_on_exec_on ( inotify_fd ); <nl> if ( inotify_add_watch ( inotify_fd , DEBUG_DUMPS_DIR , IN_CREATE | IN_MOVED_TO ) == - 1 ) <nl> perror_msg_and_die (" inotify_add_watch failed on '% s '", DEBUG_DUMPS_DIR ); <nl> 
GtkWidget * create_main_window ( void ) <nl> gtk_container_add ( GTK_CONTAINER ( halign ), hbox_report_delete ); <nl>  <nl> GtkWidget * hbox_help_close = gtk_hbutton_box_new (); <nl> - GtkWidget * btn_online_help = gtk_button_new_with_mnemonic ( _ (" _Online Help ")); <nl> + GtkWidget * btn_online_help = gtk_button_new_with_mnemonic ( _ (" Online _Help ")); <nl> GtkWidget * btn_close = gtk_button_new_from_stock ( GTK_STOCK_CLOSE ); <nl> gtk_box_pack_end ( GTK_BOX ( hbox_help_close ), btn_online_help , false , false , 0 ); <nl> gtk_box_pack_end ( GTK_BOX ( hbox_help_close ), btn_close , false , false , 0 );
static void remote_recv_cb ( EV_P_ ev_io * w , int revents ) { <nl>  <nl> ssize_t r = recv ( remote -> fd , server -> buf , BUF_SIZE , 0 ); <nl>  <nl> - if ( verbose ) { <nl> - LOGD (" remote recv : % d byte ", ( int ) r ); <nl> - } <nl> - <nl> if ( r == 0 ) { <nl> // connection closed <nl> close_and_free_remote ( EV_A_ remote );
void Client :: setDisconnectedFromCore () <nl> _ignoreListManager -> deleteLater (); <nl> _ignoreListManager = 0 ; <nl> } <nl> + <nl> + if ( _transferManager ) { <nl> + _transferManager -> deleteLater (); <nl> + _transferManager = 0 ; <nl> + } <nl> + <nl> // we probably don ' t want to save pending input for reconnect <nl> _userInputBuffer . clear (); <nl> 
void MultiLineEdit :: keyPressEvent ( QKeyEvent * event ) { <nl> case Qt :: Key_Greater : <nl> moveCursor ( QTextCursor :: End ); <nl> return ; <nl> + <nl> + // modify <nl> + case Qt :: Key_D : <nl> + moveCursor ( QTextCursor :: WordRight , QTextCursor :: KeepAnchor ); <nl> + cut (); <nl> + return ; <nl> } <nl> } <nl> }
QString MultiLineEdit :: convertMircCodesToHtml ( const QString & text ) { <nl> words [ i ] = "< span style =\"" + style + "\">" + words [ i ] + "</ span >"; <nl> } <nl> } <nl> - return words . join (""); <nl> + return words . join (""). replace ("\ n ","< br />"); <nl> } <nl>  <nl> void MultiLineEdit :: on_returnPressed () {
# include " common / xX . h " <nl> # include " common / tcpdump . h " <nl> # include " common / timer . h " <nl> + <nl> + extern char SVN_Version []; <nl> + const char * svn_version ( void ); /* svn_version . c */ <nl> + <nl> # endif
exif_subchunk_parse ( SF_PRIVATE * psf , uint32_t length ) <nl> case olym_MARKER : <nl> bytesread += psf_binheader_readf ( psf , " 4 ", & dword ) ; <nl> psf_log_printf ( psf , "% M : % u \ n ", marker , dword ) ; <nl> + if ( bytesread + dword > length ) <nl> + break ; <nl> dword += ( dword & 1 ) ; <nl> bytesread += psf_binheader_readf ( psf , " j ", dword ) ; <nl> break ;
aiff_read_chanmap ( SF_PRIVATE * psf , unsigned dword ) <nl> psf_binheader_readf ( psf , " j ", dword - bytesread ) ; <nl>  <nl> if ( map_info -> channel_map != NULL ) <nl> - { size_t chanmap_size = psf -> sf . channels * sizeof ( psf -> channel_map [ 0 ]) ; <nl> + { size_t chanmap_size = SF_MIN ( psf -> sf . channels , layout_tag & 0xffff ) * sizeof ( psf -> channel_map [ 0 ]) ; <nl>  <nl> free ( psf -> channel_map ) ; <nl> 
wavlike_subchunk_parse ( SF_PRIVATE * psf , int chunk , uint32_t chunk_length ) <nl>  <nl> case exif_MARKER : <nl> psf_log_printf ( psf , " % M \ n ", chunk ) ; <nl> - bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> + if ( chunk_length > bytesread ) <nl> + bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> continue ; <nl>  <nl> case data_MARKER :
aiff_read_header ( SF_PRIVATE * psf , COMM_CHUNK * comm_fmt ) <nl> } ; <nl> } ; <nl>  <nl> + if ( psf -> sf . channels < 1 ) <nl> + return SFE_CHANNEL_COUNT_ZERO ; <nl> + <nl> + if ( psf -> sf . channels >= SF_MAX_CHANNELS ) <nl> + return SFE_CHANNEL_COUNT ; <nl> + <nl> if (! ( found_chunk & HAVE_FORM )) <nl> return SFE_AIFF_NO_FORM ; <nl> 
php_http_info_t * php_http_info_parse ( php_http_info_t * info , const char * pre_head <nl> } else { <nl> PHP_HTTP_INFO ( info ). request . url = php_http_url_parse_authority ( url , http - url , ~ 0 TSRMLS_CC ); <nl> } <nl> + if (! PHP_HTTP_INFO ( info ). request . url ) { <nl> + PTR_SET ( PHP_HTTP_INFO ( info ). request . method , NULL ); <nl> + return NULL ; <nl> + } <nl> } else { <nl> PTR_SET ( PHP_HTTP_INFO ( info ). request . method , NULL ); <nl> return NULL ;
OPJ_BOOL opj_j2k_update_image_data ( opj_tcd_t * p_tcd , OPJ_BYTE * p_data , opj_im <nl> if ( ( l_offset_x0_src < 0 ) || ( l_offset_y0_src < 0 ) || ( l_offset_x1_src < 0 ) || ( l_offset_y1_src < 0 ) ){ <nl> return OPJ_FALSE ; <nl> } <nl> + /* testcase 2977 . pdf . asan . 67 . 2198 */ <nl> + if (( OPJ_INT32 ) l_width_dest < 0 || ( OPJ_INT32 ) l_height_dest < 0 ) { <nl> + return OPJ_FALSE ; <nl> + } <nl> /*-----*/ <nl>  <nl> /* Compute the input buffer offset */
void h2o_reprocess_request ( h2o_req_t * req , h2o_iovec_t method , const h2o_url_sch <nl> req -> path_normalized = h2o_url_normalize_path (& req -> pool , req -> path . base , req -> path . len , & req -> query_at , & req -> norm_indexes ); <nl> req -> overrides = overrides ; <nl> req -> res_is_delegated |= is_delegated ; <nl> + req -> reprocess_if_too_early = 0 ; <nl> reset_response ( req ); <nl>  <nl> /* check the delegation ( or reprocess ) counter */
void h2o_http2_scheduler_rebind ( h2o_http2_scheduler_node_t * parent , h2o_http2_sc <nl> decr_active_cnt ( ref -> super . _parent ); <nl> incr_active_cnt ( parent ); <nl> } <nl> + /* update the backlinks */ <nl> + ref -> super . _parent = parent ; <nl> + ref -> super . _slot = new_slot ; <nl>  <nl> if ( exclusive ) <nl> convert_to_exclusive ( parent , ref );
xmlrpc_value * rhbz_get_member ( const char * member , xmlrpc_value * xml ) <nl> xmlrpc_struct_find_value (& env , xml , member , & value ); <nl> if ( env . fault_occurred ) <nl> abrt_xmlrpc_error (& env ); <nl> + if (! value ) <nl> + error_msg_and_die (" fatal : There is no member named '% s '", member ); <nl>  <nl> return value ; <nl> }
LibreportError save_dump_dir_from_problem_data ( problem_data_t * problem_data , cha <nl> VERB2 log (" Renaming from '% s ' to '% s '", dd -> dd_dirname , new_path ); <nl> if ( dd_rename ( dd , new_path ) != 0 ) <nl> { <nl> + free ( new_path ); <nl> + dd_close ( dd ); <nl> + <nl> free (* problem_id ); <nl> * problem_id = NULL ; <nl> return LR_ERROR ; <nl> } <nl> - free ( new_path ); <nl>  <nl> + free ( new_path ); <nl> dd_close ( dd ); <nl>  <nl> return LR_OK ;
char * strtrimch ( char * str , int ch ) <nl> while (* tmp == ch ) <nl> ++ tmp ; <nl>  <nl> - memmove ( str , tmp , strlen ( str )); <nl> + memmove ( str , tmp , strlen ( tmp )); <nl>  <nl> // Remove trailing spaces . <nl> int i = strlen ( str );
static plist_t parse_bin_node ( struct bplist_data * bplist , const char ** object ) <nl> return parse_string_node ( object , size ); <nl>  <nl> case BPLIST_UNICODE : <nl> + if ( size * 2 < size ) { <nl> + PLIST_BIN_ERR ("% s : Integer overflow when calculating BPLIST_UNICODE data size .\ n ", __func__ ); <nl> + return NULL ; <nl> + } <nl> if (* object + size * 2 > bplist -> offset_table ) { <nl> PLIST_BIN_ERR ("% s : BPLIST_UNICODE data bytes point outside of valid range \ n ", __func__ ); <nl> return NULL ;
static void node_from_xml ( parse_ctx ctx , plist_t * plist ) <nl> return ; <nl> } <nl> if (*( ctx -> pos - 1 ) == '/') { <nl> - tag [ ctx -> pos - p - 1 ] = '\ 0 '; <nl> + int idx = ctx -> pos - p - 1 ; <nl> + if ( idx < taglen ) <nl> + tag [ idx ] = '\ 0 '; <nl> is_empty = 1 ; <nl> } <nl> ctx -> pos ++;
static void write_unicode ( GByteArray * bplist , gunichar2 * val , uint64_t size ) <nl> for ( i = 0 ; i < size ; i ++) <nl> byte_convert ( buff + i * sizeof ( gunichar2 ), sizeof ( gunichar2 )); <nl> write_raw_data ( bplist , BPLIST_UNICODE , buff , size ); <nl> + free ( buff ); <nl> } <nl>  <nl> static void write_array ( GByteArray * bplist , GNode * node , GHashTable * ref_table , uint8_t dict_param_size )
usershape_t * gvusershape_find ( char * name ) <nl> { <nl> usershape_t probe ; <nl>  <nl> + if (! ImageDict ) <nl> + return NULL ; <nl> + <nl> probe . name = name ; <nl> return ( dtsearch ( ImageDict , & probe )); <nl> }
static void mp_resolve_color ( GVJ_t * job , gvcolor_t * color ) <nl> color -> u . rgba [ 2 ]); <nl> color -> u . index = i ; <nl> break ; <nl> + case HSVA_DOUBLE : /* TODO : implement color conversion */ <nl> + color -> u . index = 0 ; <nl> + break ; <nl> default : <nl> assert ( 0 ); /* internal error */ <nl> }
static void view_bookmarks_check ( TEXT_BUFFER_VIEW_REC * view , LINE_REC * line ) <nl> if ( new_line != NULL ) { <nl> g_hash_table_insert ( view -> bookmarks , <nl> tmp -> data , new_line ); <nl> + } else { <nl> + g_free ( tmp -> data ); <nl> } <nl> } <nl> g_slist_free ( rec . remove_list );
static int view_scroll ( TEXT_BUFFER_VIEW_REC * view , GList ** lines , int * subline , <nl> { <nl> int linecount , realcount , scroll_visible ; <nl>  <nl> + if (* lines == NULL ) <nl> + return 0 ; <nl> + <nl> /* scroll down */ <nl> scroll_visible = lines == & view -> startline ; <nl> 
static int sig_autoremove ( void ) <nl>  <nl> /* Close only logs with private messages */ <nl> logitem = log -> items -> data ; <nl> + if ( logitem -> servertag == NULL ) <nl> + continue ; <nl> + <nl> server = server_find_tag ( logitem -> servertag ); <nl> if ( logitem -> type == LOG_ITEM_TARGET && <nl> server != NULL && ! server -> ischannel (* logitem -> name ))
verify_callback ( int preverify_ok , X509_STORE_CTX * ctx ) <nl> if ( opt -> verify_export_cert ) <nl> { <nl> gc = gc_new (); <nl> - if ( tmp_file = get_peer_cert ( ctx , opt -> verify_export_cert ,& gc )) <nl> + if (( tmp_file = get_peer_cert ( ctx , opt -> verify_export_cert ,& gc ))) <nl> { <nl> setenv_str ( opt -> es , " peer_cert ", tmp_file ); <nl> }
* development environment . <nl> */ <nl>  <nl> +/* MSVC headers do not define this macro , so do it here */ <nl> +# ifndef IN6_ARE_ADDR_EQUAL <nl> +# define IN6_ARE_ADDR_EQUAL ( a , b ) \ <nl> + ( memcmp (( const void *)( a ), ( const void *)( b ), sizeof ( struct in6_addr )) == 0 ) <nl> +# endif <nl> + <nl> void init_win32 ( void ); <nl> void uninit_win32 ( void ); <nl> 
ecma_create_arguments_object ( ecma_object_t * func_obj_p , /**< callee function */ <nl> prop_desc , <nl> false ); <nl> JERRY_ASSERT ( ecma_is_completion_value_normal_true ( completion )); <nl> + ecma_deref_ecma_string ( length_magic_string_p ); <nl>  <nl> ecma_dealloc_number ( len_p ); <nl> 
run_int_from_pos ( struct __int_data * int_data ) <nl> { <nl> const OPCODE * curr = & __program [ int_data -> pos ]; <nl> completion = __opfuncs [ curr -> op_idx ](* curr , int_data ); <nl> + <nl> + JERRY_ASSERT ( ! ecma_is_completion_value_normal ( completion ) <nl> + || ecma_is_completion_value_normal_simple_value ( completion , <nl> + ECMA_SIMPLE_VALUE_EMPTY ) ); <nl> } while ( completion . type == ECMA_COMPLETION_TYPE_NORMAL ); <nl>  <nl> if ( completion . type == ECMA_COMPLETION_TYPE_BREAK )
dhcp6opt_print ( const u_char * cp , const u_char * ep ) <nl> if ( ep < cp + sizeof (* dh6o )) <nl> goto trunc ; <nl> dh6o = ( struct dhcp6opt *) cp ; <nl> + TCHECK (* dh6o ); <nl> optlen = EXTRACT_16BITS (& dh6o -> dh6opt_len ); <nl> if ( ep < cp + sizeof (* dh6o ) + optlen ) <nl> goto trunc ;
handle_pap ( netdissect_options * ndo , <nl>  <nl> switch ( code ) { <nl> case PAP_AREQ : <nl> + /* A valid Authenticate - Request is 6 or more octets long . */ <nl> + if ( len < 6 ) <nl> + goto trunc ; <nl> if ( length - ( p - p0 ) < 1 ) <nl> return ; <nl> ND_TCHECK (* p );
public : <nl> // base methods <nl> virtual ~ AP4_AtomParent (); <nl> AP4_List < AP4_Atom >& GetChildren () { return m_Children ; } <nl> + AP4_Result CopyChildren ( AP4_AtomParent & destination ) const ; <nl> virtual AP4_Result AddChild ( AP4_Atom * child , int position = - 1 ); <nl> virtual AP4_Result RemoveChild ( AP4_Atom * child ); <nl> virtual AP4_Result DeleteChild ( AP4_Atom :: Type type , AP4_Ordinal index = 0 );
loop : <nl> client = accept_client ( fd , & addr . in , false ); <nl> } <nl>  <nl> - slog_debug ( client , " P : got connection : % s ", conninfo ( client )); <nl> + if ( client ) <nl> + slog_debug ( client , " P : got connection : % s ", conninfo ( client )); <nl>  <nl> /* <nl> * there may be several clients waiting ,
static void start_auth_request ( PgSocket * client , const char * username ) <nl> int res ; <nl> PktBuf * buf ; <nl>  <nl> - client -> auth_user = client -> db -> auth_user ; <nl> /* have to fetch user info from db */ <nl> client -> pool = get_pool ( client -> db , client -> db -> auth_user ); <nl> if (! find_server ( client )) {
static bool handle_client_startup ( PgSocket * client , PktHdr * pkt ) <nl> } <nl> break ; <nl> case ' p ': /* PasswordMessage */ <nl> + /* too early */ <nl> + if (! client -> auth_user ) { <nl> + disconnect_client ( client , true , " client password pkt before startup packet "); <nl> + return false ; <nl> + } <nl> + <nl> /* haven ' t requested it */ <nl> if ( cf_auth_type <= AUTH_TRUST ) { <nl> disconnect_client ( client , true , " unrequested passwd pkt ");
static void zone_timer ( int fd , short flg , void * arg ) <nl> z = container_of ( el , struct DNSZone , lnode ); <nl> ctx -> zone_state = 1 ; <nl> ctx -> cur_zone = z ; <nl> + ctx -> active ++; <nl> impl_query_soa_serial ( ctx , z -> zonename ); <nl> } <nl> 
static bool check_client_passwd ( PgSocket * client , const char * passwd ) <nl> const char * correct ; <nl> PgUser * user = client -> auth_user ; <nl>  <nl> + /* auth_user may be missing */ <nl> + if (! user ) { <nl> + slog_error ( client , " Password packet before auth packet ?"); <nl> + return false ; <nl> + } <nl> + <nl> /* disallow empty passwords */ <nl> if (!* passwd || !* user -> passwd ) <nl> return false ;
close_uzbl ( WebKitWebView * page , GArray * argv , GString * result ) { <nl> if ( uzbl . gui . main_window ) <nl> gtk_widget_destroy ( uzbl . gui . main_window ); <nl> else if ( uzbl . gui . plug ) <nl> - gtk_widget_destroy ( uzbl . gui . plug ); <nl> + gtk_widget_destroy ( GTK_WIDGET ( uzbl . gui . plug )); <nl>  <nl> gtk_main_quit (); <nl> }
find_existing_file ( gchar * path_list ) { <nl> i ++; <nl> } <nl>  <nl> + g_free ( executable ); <nl> g_strfreev ( split ); <nl> return NULL ; <nl> }
request_starting_cb ( WebKitWebView * view , WebKitWebFrame * frame , WebKitWebResour <nl>  <nl> g_object_ref ( decision -> request ); <nl> request_decision ( uri , decision ); <nl> + <nl> + g_free ( decision ); <nl> } <nl>  <nl> void
static int write_element_to_buffer ( bson_buffer * buffer , int type_byte , PyObject * <nl>  <nl> item_value = PySequence_GetItem ( value , i ); <nl> if (! write_element_to_buffer ( buffer , list_type_byte , item_value , check_keys )) { <nl> + Py_DECREF ( item_value ); <nl> return 0 ; <nl> } <nl> + Py_DECREF ( item_value ); <nl> } <nl>  <nl> /* write null byte and fill in length */
public : <nl> y += secVnc . height (); <nl> secPlain . move ( xPad , y ); <nl> y += secPlain . height (); <nl> + <nl> + xPad -= SECOND_COL_XPAD ; <nl> # endif <nl>  <nl> /* Render " OK " and " Cancel " buttons */
static void handle_propfind ( struct connection * conn , const char * path , <nl> struct dir_entry * de = & arr [ i ]; <nl> mg_url_encode ( de -> file_name , strlen ( de -> file_name ), buf , sizeof ( buf )); <nl> print_props ( conn , buf , & de -> st ); <nl> + free ( de -> file_name ); <nl> } <nl> + free ( arr ); <nl> } <nl> ns_send ( conn -> ns_conn , footer , sizeof ( footer ) - 1 ); <nl> }
enum { <nl>  <nl> // Macros for enabling compiler - specific checks for printf - like arguments . <nl> # undef PRINTF_FORMAT_STRING <nl> -# if _MSC_VER >= 1400 <nl> +# if defined ( _MSC_VER ) && _MSC_VER >= 1400 <nl> # include < sal . h > <nl> -# if _MSC_VER > 1400 <nl> +# if defined ( _MSC_VER ) && _MSC_VER > 1400 <nl> # define PRINTF_FORMAT_STRING ( s ) _Printf_format_string_ s <nl> # else <nl> # define PRINTF_FORMAT_STRING ( s ) __format_string s
inf_text_fixline_buffer_buffer_insert_text ( InfTextBuffer * buffer , <nl>  <nl> priv -> keep = g_realloc ( <nl> priv -> keep , <nl> - priv -> n_keep + inf_text_chunk_get_length ( chunk ) <nl> + ( priv -> n_keep + inf_text_chunk_get_length ( chunk )) * sizeof ( guint ) <nl> ); <nl>  <nl> if ( pos - buf_len < priv -> n_keep )
ImagingNewBlock ( const char * mode , int xsize , int ysize ) <nl> if ( im -> linesize && <nl> im -> ysize > INT_MAX / im -> linesize ) { <nl> /* punt if we ' re going to overflow */ <nl> - return NULL ; <nl> + ImagingDelete ( im ); <nl> + return ( Imaging ) ImagingError_MemoryError (); <nl> } <nl>  <nl> if ( im -> ysize * im -> linesize <= 0 ) {
idevice_error_t idevice_connection_enable_ssl ( idevice_connection_t connection ) <nl> return_me = SSL_do_handshake ( ssl ); <nl> if ( return_me != 1 ) { <nl> debug_info (" ERROR in SSL_do_handshake : % s ", errorstring ( SSL_get_error ( ssl , return_me ))); <nl> + SSL_free ( ssl ); <nl> SSL_CTX_free ( ssl_ctx ); <nl> } else { <nl> ssl_data_t ssl_data_loc = ( ssl_data_t ) malloc ( sizeof ( struct ssl_data_private ));
afc_file_read ( afc_client_t client , uint64_t handle , char * data , int length , uint <nl> const int MAXIMUM_READ_SIZE = 1 << 16 ; <nl> afc_error_t ret = AFC_E_SUCCESS ; <nl>  <nl> - if (! client || ! client -> afc_packet || ! client -> connection || handle == 0 ) <nl> + if (! client || ! client -> afc_packet || ! client -> connection || handle == 0 || ( length < 0 )) <nl> return AFC_E_INVALID_ARGUMENT ; <nl> log_debug_msg ("% s : called for length % i \ n ", __func__ , length ); <nl> 
char * format_string ( const char * buf , int cols , int depth ) <nl> int i = 0 ; <nl> int j = 0 ; <nl>  <nl> - assert ( cols >= 0 ); <nl> + assert ( cols > 0 ); <nl> assert ( depth >= 0 ); <nl>  <nl> // Inserts new lines and tabs at appropriate locations
static int jpc_dec_tileinit ( jpc_dec_t * dec , jpc_dec_tile_t * tile ) <nl> uint_fast32_t tmpxend ; <nl> uint_fast32_t tmpyend ; <nl> jpc_dec_cp_t * cp ; <nl> - jpc_tsfb_band_t bnds [ 64 ]; <nl> + jpc_tsfb_band_t bnds [ JPC_MAXBANDS ]; <nl> jpc_pchg_t * pchg ; <nl> int pchgno ; <nl> jpc_dec_cmpt_t * cmpt ;
int oidc_handle_redirect_uri_request ( request_rec * r , oidc_cfg * c , <nl> /* something went wrong */ <nl> return oidc_util_html_send_error ( r , c -> error_template , " Invalid Request ", <nl> apr_psprintf ( r -> pool , <nl> - " The OpenID Connect callback URL received an invalid request : % s ", <nl> - r -> args ), HTTP_INTERNAL_SERVER_ERROR ); <nl> + " The OpenID Connect callback URL received an invalid request "), <nl> + HTTP_INTERNAL_SERVER_ERROR ); <nl> } <nl>  <nl> /*
namespace SDDM { <nl> } <nl>  <nl> bool PamBackend :: closeSession () { <nl> - if ( m_pam -> isOpen ()) <nl> + if ( m_pam -> isOpen ()) { <nl> + qDebug () << "[ PAM ] Closing session "; <nl> return m_pam -> closeSession (); <nl> + } <nl> + qWarning () << "[ PAM ] Asked to close the session but it wasn ' t previously open "; <nl> return Backend :: closeSession (); <nl> } <nl> 
arr_insert ( PyObject * NPY_UNUSED ( self ), PyObject * args , PyObject * kwdict ) <nl> } else { <nl> Py_XDECREF ( values ); <nl> Py_XDECREF ( mask ); <nl> + Py_XDECREF ( array ); <nl> Py_RETURN_NONE ; <nl> } <nl> }
prepare_index ( PyArrayObject * self , PyObject * index , <nl> if ( indices [ i ]. value != PyArray_DIM ( self , used_ndim )) { <nl> static PyObject * warning ; <nl>  <nl> - char * err_msg [ 174 ]; <nl> - sprintf ( err_msg , <nl> + char err_msg [ 174 ]; <nl> + PyOS_snprintf ( err_msg , sizeof ( err_msg ), <nl> " boolean index did not match indexed array along " <nl> " dimension % d ; dimension is %" NPY_INTP_FMT <nl> " but corresponding boolean dimension is %" NPY_INTP_FMT ,
PyArray_FromString ( char * data , npy_intp slen , PyArray_Descr * dtype , <nl>  <nl> if ( dtype == NULL ) { <nl> dtype = PyArray_DescrFromType ( NPY_DEFAULT_TYPE ); <nl> + if ( dtype == NULL ) { <nl> + return NULL ; <nl> + } <nl> } <nl> if ( PyDataType_FLAGCHK ( dtype , NPY_ITEM_IS_POINTER ) || <nl> PyDataType_REFCHK ( dtype )) {
const TSS2_TCTI_INFO * tpm2_tcti_ldr_getinfo ( void ) { <nl> bool tpm2_tcti_ldr_is_tcti_present ( const char * name ) { <nl>  <nl> char path [ PATH_MAX ]; <nl> - snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so ", name ); <nl> + snprintf ( path , sizeof ( path ), " libtss2 - tcti -% s . so . 0 ", name ); <nl>  <nl> void * handle = dlopen ( path , RTLD_LAZY ); <nl> if ( handle ) {
int createEKHandle ( TSS2_SYS_CONTEXT * sapi_context ) <nl> LOG_INFO (" EK create succ .. Handle : 0x % 8 . 8x ", handle2048ek ); <nl>  <nl> if (! ctx . non_persistent_read ) { <nl> + <nl> + if (! ctx . persistent_handle ) { <nl> + LOG_ERR (" Persistent handle for EK was not provided "); <nl> + return 1 ; <nl> + } <nl> + <nl> /* <nl> * To make EK persistent , use own auth <nl> */
pid_t read_pid ( char * pidfile ) <nl> { <nl> FILE * f ; <nl> - long pid = 0 ; <nl> + long pid ; <nl>  <nl> if (!( f = fopen ( pidfile ," r "))) <nl> return 0 ; <nl> - fscanf ( f ,"% ld ", & pid ); <nl> + if ( fscanf ( f ,"% ld ", & pid ) != 1 ) <nl> + pid = 0 ; <nl> fclose ( f ); <nl> return pid ; <nl> }
bool X86_getInstruction ( csh ud , const uint8_t * code , size_t code_len , <nl> insn . prefixPresent [ 0x64 ] = 0 ; <nl> insn . prefixPresent [ 0x65 ] = 0 ; <nl> insn . prefixPresent [ 0x66 ] = 0 ; <nl> + insn . prefixPresent [ 0x67 ] = 0 ; <nl> insn . prefixPresent [ 0xf0 ] = 0 ; <nl> insn . prefixPresent [ 0xf2 ] = 0 ; <nl> insn . prefixPresent [ 0xf3 ] = 0 ;
int decodeInstruction ( struct InternalInstruction * insn , <nl>  <nl> insn -> length = ( size_t )( insn -> readerCursor - insn -> startLocation ); <nl>  <nl> + if ( insn -> length > 15 ) <nl> + return - 1 ; <nl> + <nl> // dbgprintf ( insn , " Read from 0x % llx to 0x % llx : length % zu ", <nl> // startLoc , insn -> readerCursor , insn -> length ); <nl> 
bool M68K_getInstruction ( csh ud , const uint8_t * code , size_t code_len , MCInst * i <nl> cs_struct * handle = instr -> csh ; <nl> m68k_info * info ; <nl>  <nl> - if ( inst_info == NULL ) { <nl> + if ( handle -> printer_info == NULL ) { <nl> info = cs_mem_malloc ( sizeof ( m68k_info )); <nl> if (! info ) { <nl> handle -> errnum = CS_ERR_MEM ;
static void printPCRelOperand ( MCInst * MI , int OpNum , SStream * O ) <nl> SStream_concat ( O , "% u ", imm ); <nl> } else { <nl> if ( imm < - HEX_THRESHOLD ) <nl> - SStream_concat ( O , "- 0x % x ", - imm ); <nl> + SStream_concat ( O , "- 0x % x ", ( unsigned int )- imm ); <nl> else <nl> SStream_concat ( O , "-% u ", - imm ); <nl> }
static DecodeStatus Mips64Disassembler_getInstruction ( int mode , MCInst * instr , <nl> { <nl> uint32_t Insn ; <nl>  <nl> + if ( code_len < 4 ) <nl> + // not enough data <nl> + return MCDisassembler_Fail ; <nl> + <nl> + if ( instr -> flat_insn -> detail ) { <nl> + memset ( instr -> flat_insn -> detail , 0 , sizeof ( cs_detail )); <nl> + } <nl> + <nl> DecodeStatus Result = readInstruction32 (( unsigned char *) code , & Insn , isBigEndian , false ); <nl> if ( Result == MCDisassembler_Fail ) <nl> return MCDisassembler_Fail ;
CAMLprim value caml_bytes_set ( value str , value index , value newval ) <nl> */ <nl> CAMLprim value caml_string_set ( value str , value index , value newval ) <nl> { <nl> - return caml_string_set ( str , index , newval ); <nl> + return caml_bytes_set ( str , index , newval ); <nl> } <nl>  <nl> 
opfunc_in ( opcode_t opdata __unused , /**< operation data */ <nl> const idx_t left_var_idx = opdata . data . in . var_left ; <nl> const idx_t right_var_idx = opdata . data . in . var_right ; <nl>  <nl> + int_data -> pos ++; <nl> + <nl> ecma_completion_value_t ret_value ; <nl>  <nl> ECMA_TRY_CATCH ( left_value , get_variable_value ( int_data , left_var_idx , false ), ret_value );
static inline void drop_trailing_newlines ( char * s ) <nl> static void dorealloc ( char ** mem , size_t oldlen , size_t newlen ) <nl> { <nl> int batches ; <nl> - if ( newlen % BATCH_SIZE <= oldlen % BATCH_SIZE ) <nl> + if ( newlen <= oldlen ) <nl> return ; <nl> - batches = ( newlen % BATCH_SIZE ) + 1 ; <nl> + batches = ( newlen / BATCH_SIZE ) + 1 ; <nl> if (!* mem ) { <nl> do { <nl> * mem = malloc ( batches * BATCH_SIZE );
HwProbe :: hd2value ( hd_t * hd ) <nl> out -> add ( YCPString (" sub_vendor "), YCPString ( s )); <nl> } <nl>  <nl> + // HAL udi <nl> + <nl> + s = hd -> udi ; <nl> + if ( s ) <nl> + { <nl> + out -> add ( YCPString (" udi "), YCPString ( s )); <nl> + } <nl> + <nl> // unique key <nl>  <nl> s = hd -> unique_id ;
YEBuiltin :: toStream ( std :: ostream & str ) const <nl> std :: ostream & <nl> YEBuiltin :: toXml ( std :: ostream & str , int indent ) const <nl> { <nl> - str << "< builtin name =\"" << m_decl -> name << "\""; <nl> + str << "< builtin name =\"" << StaticDeclaration :: Decl2String ( m_decl ) << "\""; <nl>  <nl> if ( m_parameterblock != 0 ) <nl> {
__no_resolve : <nl> if (( off = lseek ( _state . s_wpafd , 0 , SEEK_CUR )) == ( off_t ) - 1 ) <nl> err ( 1 , " lseek ()"); <nl>  <nl> + if ( lseek ( _state . s_wpafd , 0 , SEEK_SET ) == ( off_t ) - 1 ) <nl> + err ( 1 , " lseek ()"); <nl> + <nl> while ( tot ) { <nl> int l = tot ; <nl> 
static int net_get_nopacket ( struct priv_net * pn , void * arg , int * len ) <nl> while ( 1 ) { <nl> l = sizeof ( buf ); <nl> c = net_get ( pn -> pn_s , buf , & l ); <nl> + if ( c < 0 ) <nl> + return c ; <nl>  <nl> if ( c != NET_PACKET && c > 0 ) <nl> break ;
static int resolve_deps ( const char * src ) <nl> if ( strstr ( buf , " not regular file ")) <nl> break ; <nl>  <nl> + if ( strstr ( buf , " cannot read header ")) <nl> + break ; <nl> + <nl> + if ( strstr ( buf , destrootdir )) <nl> + break ; <nl> + <nl> p = strstr ( buf , "/"); <nl> if ( p ) { <nl> int r ;
int main ( int argc , char ** argv ) <nl> info . subusers . erase ( uiter ); <nl> if ( purge_keys ) { <nl> map < string , RGWAccessKey > * keys_map ; <nl> - access_key = subuser ; <nl> + access_key = info . user_id ; <nl> access_key . append (":"); <nl> access_key . append ( subuser ); <nl> keys_map = & info . swift_keys ;
class MDentryLink : public Message { <nl> dirfrag ( df ), <nl> dn ( n ), <nl> is_primary ( p ) {} <nl> + private : <nl> + ~ MDentryLink () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " dentry_link ";} <nl> void print ( ostream & o ) { <nl> o << " dentry_link (" << dirfrag << " " << dn << ")";
extern " C " int ceph_create ( struct ceph_mount_info ** cmount , const char * const i <nl> extern " C " void ceph_shutdown ( struct ceph_mount_info * cmount ) <nl> { <nl> cmount -> shutdown (); <nl> + delete cmount ; <nl> } <nl>  <nl> extern " C " int ceph_conf_read_file ( struct ceph_mount_info * cmount , const char * path )
bool Monitor :: ms_get_authorizer ( int dest_type , AuthAuthorizer ** authorizer , bool <nl> } <nl>  <nl> CephXTicketBlob blob ; <nl> - ret = cephx_build_service_ticket_blob ( info , blob ); <nl> - if ( ret < 0 ) <nl> + if (! cephx_build_service_ticket_blob ( info , blob )) <nl> return false ; <nl> bufferlist ticket_data ; <nl> :: encode ( blob , ticket_data );
void Server :: handle_client_readdir ( MDRequest * mdr ) <nl> if (! dir -> is_complete ()) { <nl> if ( dir -> is_frozen ()) { <nl> dout ( 7 ) << " dir is frozen " << * dir << dendl ; <nl> + mds -> locker -> drop_locks ( mdr ); <nl> + mdr -> drop_local_auth_pins (); <nl> dir -> add_waiter ( CDir :: WAIT_UNFREEZE , new C_MDS_RetryRequest ( mdcache , mdr )); <nl> return ; <nl> }
private : <nl> item_dirty_dirfrag_nest ( this ), <nl> item_dirty_dirfrag_dirfragtree ( this ), <nl> auth_pins ( 0 ), nested_auth_pins ( 0 ), <nl> + auth_pin_freeze_allowance ( 0 ), <nl> nested_anchors ( 0 ), <nl> pop ( ceph_clock_now ( g_ceph_context )), <nl> versionlock ( this , & versionlock_type ),
void Elector :: start () <nl> return ; <nl> } <nl> dout ( 5 ) << " start -- can i be leader ?" << dendl ; <nl> + <nl> + acked_me . clear (); <nl>  <nl> // start by trying to elect me <nl> if ( epoch % 2 == 0 )
public : <nl> uint64_t get_required_features () const { <nl> return required_features ; <nl> } <nl> + mon_feature_t get_required_mon_features () const { <nl> + return monmap -> get_required_features (); <nl> + } <nl> void apply_quorum_to_compatset_features (); <nl> void apply_compatset_features_to_quorum_requirements (); <nl> 
unsigned ceph_str_hash_rjenkins ( const char * str , unsigned length ) <nl> unsigned ceph_str_hash_linux ( const char * str , unsigned length ) <nl> { <nl> unsigned long hash = 0 ; <nl> - unsigned char c ; <nl>  <nl> while ( length --) { <nl> - c = * str ++; <nl> + unsigned char c = * str ++; <nl> hash = ( hash + ( c << 4 ) + ( c >> 4 )) * 11 ; <nl> } <nl> return hash ;
int validate_pool ( IoCtx & io_ctx , CephContext * cct ) { <nl> snap_name ), <nl> boost :: bind (& ImageWatcher :: notify_snap_remove , <nl> ictx -> image_watcher , snap_name )); <nl> - if ( r < 0 && r != - EEXIST ) { <nl> + if ( r < 0 && r != - ENOENT ) { <nl> return r ; <nl> } <nl> } else {
void librados :: IoCtxImpl :: C_NotifyComplete :: notify ( uint8_t opcode , <nl> uint64_t ver , <nl> bufferlist & bl ) <nl> { <nl> + lock -> Lock (); <nl> * done = true ; <nl> cond -> Signal (); <nl> + lock -> Unlock (); <nl> } <nl>  <nl> /////////////////////////// WatchContext ///////////////////////////////
void PG :: activate ( ObjectStore :: Transaction & t , <nl>  <nl> need_up_thru = false ; <nl>  <nl> + // clear prior set ( and dependency info )... we are done peering ! <nl> + clear_prior (); <nl> + <nl> // write pg info , log <nl> write_info ( t ); <nl> write_log ( t );
ssize_t AsyncConnection :: _try_send ( bool send , bool more ) <nl> // trim already sent for outcoming_bl <nl> if ( sent_bytes ) { <nl> if ( sent_bytes < outcoming_bl . length ()) { <nl> - bufferlist bl ; <nl> - outcoming_bl . splice ( sent_bytes , outcoming_bl . length ()- sent_bytes , & bl ); <nl> - bl . swap ( outcoming_bl ); <nl> + outcoming_bl . splice ( 0 , sent_bytes ); <nl> } else { <nl> outcoming_bl . clear (); <nl> }
class MonMap { <nl> void calc_ranks () { <nl> rank_name . resize ( mon_addr . size ()); <nl> rank_addr . resize ( mon_addr . size ()); <nl> + addr_name . clear (); <nl> for ( map < string , entity_addr_t >:: iterator p = mon_addr . begin (); <nl> p != mon_addr . end (); <nl> p ++) {
int FileStore :: mount () <nl> } <nl>  <nl> dout ( 5 ) << " mount op_seq is " << initial_op_seq << dendl ; <nl> + if ( initial_op_seq == 0 ) { <nl> + derr << " mount initial op seq is 0 ; something is wrong " << dendl ; <nl> + ret = - EINVAL ; <nl> + goto close_current_fd ; <nl> + } <nl>  <nl> if (! btrfs_stable_commits ) { <nl> // mark current / as non - snapshotted so that we don ' t rollback away
void PG :: _compare_scrubmaps ( const map < int , ScrubMap *> & maps , <nl> set < int > cur_missing ; <nl> set < int > cur_inconsistent ; <nl> for ( j = maps . begin (); j != maps . end (); ++ j ) { <nl> + if ( j == auth ) <nl> + continue ; <nl> if ( j -> second -> objects . count (* k )) { <nl> // Compare <nl> stringstream ss ;
public : <nl> */ <nl> void get_leaves_under ( frag_t x , std :: list < frag_t >& ls ) const { <nl> std :: list < frag_t > q ; <nl> - q . push_back ( get_branch ( x )); <nl> + q . push_back ( get_branch_or_leaf ( x )); <nl> while (! q . empty ()) { <nl> frag_t t = q . front (); <nl> q . pop_front ();
void JournalingObjectStore :: _op_journal_transactions ( list < ObjectStore :: Transacti <nl> data_len = t -> get_data_length (); <nl> data_align = ( t -> get_data_alignment () - tbl . length ()) & ~ CEPH_PAGE_MASK ; <nl> } <nl> - t -> encode ( tbl ); <nl> + :: encode (* t , tbl ); <nl> } <nl> journal -> submit_entry ( op , tbl , data_align , onjournal ); <nl> } else if ( onjournal )
void MDS :: replay_done () <nl> } <nl>  <nl> if ( continue_replay ) { <nl> - mdlog -> get_journaler ()-> set_writeable (); <nl> continue_replay = false ; <nl> standby_replay_restart (); <nl> return ; <nl> } <nl>  <nl> + mdlog -> get_journaler ()-> set_writeable (); <nl> + <nl> if ( g_conf . mds_wipe_sessions ) { <nl> dout ( 1 ) << " wiping out client sessions " << dendl ; <nl> sessionmap . wipe ();
void CInode :: validate_disk_state ( CInode :: validated_data * results , <nl> } else if ( fin ) { <nl> fin -> complete ( get_rval ()); <nl> } <nl> + delete this ; <nl> } <nl> }; <nl> 
Context * create_async_context_callback ( I & image_ctx , Context * on_finish ) { <nl> image_ctx . op_work_queue , on_finish ); <nl> } <nl>  <nl> + template < typename WQ > <nl> + Context * create_async_context_callback ( WQ * work_queue , Context * on_finish ) { <nl> + // use async callback to acquire a clean lock context <nl> + return new detail :: C_AsyncCallback < WQ >( work_queue , on_finish ); <nl> +} <nl> + <nl> // TODO : temporary until AioCompletion supports templated ImageCtx <nl> inline ImageCtx * get_image_ctx ( ImageCtx * image_ctx ) { <nl> return image_ctx ;
int aio_bench ( Rados & rados , rados_pool_t pool , int secondsToRun , int concurrenti <nl> time (& initialTime ); <nl> stringstream initialTimeS (""); <nl> initialTimeS << initialTime ; <nl> - const char * iTime = initialTimeS . str (). c_str (); <nl> + char iTime [ 100 ]; <nl> + strcpy ( iTime , initialTimeS . str (). c_str ()); <nl> maxLatency . set_from_double ( 0 ); <nl> // set up writes so I can start them together <nl> for ( int i = 0 ; i < concurrentios ; ++ i ) {
int lockdep_will_lock ( const char * name , int id ) <nl> * _dout << "\ npreviously locked at \ n "; <nl> p -> second -> print (* _dout ); <nl> } <nl> + delete bt ; <nl> * _dout << dendl ; <nl> assert ( 0 ); <nl> }
void PG :: scrub () <nl> ss << ", " << fixed << " fixed "; <nl> osd -> get_logclient ()-> log ( errors ? LOG_ERROR : LOG_INFO , ss ); <nl>  <nl> - if (!( errors - fixed ) && repair ) <nl> + if ( errors == 0 || ( repair && ( errors - fixed ) == 0 )) <nl> state_clear ( PG_STATE_INCONSISTENT ); <nl> state_clear ( PG_STATE_REPAIR ); <nl> 
int ceph_do_lookup ( struct super_block * sb , struct dentry * dentry , int mask ) <nl> struct ceph_mds_request_head * rhead ; <nl> int err ; <nl>  <nl> + if ( dentry -> d_name . len > NAME_MAX ) <nl> + return - ENAMETOOLONG ; <nl> + <nl> dout ( 10 , " do_lookup % p mask % d \ n ", dentry , CEPH_STAT_MASK_INODE_ALL ); <nl> path = ceph_build_dentry_path ( dentry , & pathlen ); <nl> if ( IS_ERR ( path ))
void Locker :: xlock_finish ( SimpleLock * lock , Mutation * mut ) <nl> lock -> get_num_client_lease () == 0 ) { <nl> assert (! lock -> is_stable ()); <nl> lock -> get_parent ()-> auth_unpin ( lock ); <nl> - lock -> set_state ( LOCK_LOCK ); <nl> + if ( lock -> get_type () != CEPH_LOCK_DN && (( CInode *) lock -> get_parent ())-> get_loner () >= 0 ) <nl> + lock -> set_state ( LOCK_EXCL ); <nl> + else <nl> + lock -> set_state ( LOCK_LOCK ); <nl> } <nl>  <nl> // others waiting ?
PG :: RecoveryState :: Peering :: react ( const AdvMap & advmap ) { <nl> dout ( 10 ) << " Peering advmap " << dendl ; <nl> if ( pg -> prior_set_affected (* prior_set . get (), & advmap . osdmap )) { <nl> dout ( 1 ) << " Peering , priors_set_affected , going to Reset " << dendl ; <nl> + pg -> state_clear ( PG_STATE_PEERING ); <nl> post_event ( advmap ); <nl> return transit < Reset >(); <nl> }
namespace ceph { <nl> assert ( s == SECSuccess ); <nl> } <nl> void Update ( const byte * input , size_t length ) { <nl> - SECStatus s ; <nl> - s = PK11_DigestOp ( ctx , input , length ); <nl> - assert ( s == SECSuccess ); <nl> + if ( length ) { <nl> + SECStatus s ; <nl> + s = PK11_DigestOp ( ctx , input , length ); <nl> + assert ( s == SECSuccess ); <nl> + } <nl> } <nl> void Final ( byte * digest ) { <nl> SECStatus s ;
void PrimaryLogPG :: on_shutdown () <nl> cancel_log_updates (); <nl> // we must remove PGRefs , so do this this prior to release_backoffs () callers <nl> clear_backoffs (); <nl> + // clean up snap trim references <nl> + snap_trimmer_machine . process_event ( Reset ()); <nl>  <nl> pgbackend -> on_change (); <nl> 
void MgrMonitor :: drop_active () <nl> pending_map . active_gid = 0 ; <nl> pending_map . available = false ; <nl> pending_map . active_addr = entity_addr_t (); <nl> + <nl> + // So that when new active mgr subscribes to mgrdigest , it will <nl> + // get an immediate response instead of waiting for next timer <nl> + cancel_timer (); <nl> } <nl>  <nl> void MgrMonitor :: drop_standby ( uint64_t gid )
int rgw_bucket_init_index ( cls_method_context_t hctx , bufferlist * in , bufferlist <nl>  <nl> if ( header_bl . length () != 0 ) { <nl> CLS_LOG (" ERROR : index already initialized \ n "); <nl> + return - EINVAL ; <nl> } <nl>  <nl> rgw_bucket_dir dir ;
int RGWPostObj_ObjStore :: read_form_part_header ( struct post_form_part * const part <nl> } <nl>  <nl> r = read_line ( bl , chunk_size , reached_boundary , done ); <nl> + if ( r < 0 ) { <nl> + return r ; <nl> + } <nl> } <nl>  <nl> return 0 ;
int namespace_add ( cls_method_context_t hctx , bufferlist * in , bufferlist * out ) <nl> } <nl>  <nl> /** <nl> - * Add a namespace to the namespace directory . <nl> + * Remove a namespace from the namespace directory . <nl> * <nl> * Input : <nl> * @ param name the name of the namespace
void PG :: start_peering_interval ( <nl> state_clear ( PG_STATE_REMAPPED ); <nl>  <nl> int role = osdmap -> calc_pg_role ( osd -> whoami , acting , acting . size ()); <nl> - if ( role == pg_whoami . shard ) <nl> + if ( pool . info . is_replicated () || role == pg_whoami . shard ) <nl> set_role ( role ); <nl> else <nl> set_role (- 1 );
static int do_disk_usage ( librbd :: RBD & rbd , librados :: IoCtx & io_ctx , <nl> ++ count ; <nl> } <nl> } <nl> - if (! found ) { <nl> + if ( imgname != nullptr && ! found ) { <nl> std :: cerr << " specified image " << imgname << " is not found ." << std :: endl ; <nl> return - ENOENT ; <nl> }
int buffer :: list :: read_file ( const char * fn ) <nl> :: fstat ( fd , & st ); <nl> int s = ROUND_UP_TO ( st . st_size , PAGE_SIZE ); <nl> bufferptr bp = buffer :: create_page_aligned ( s ); <nl> + bp . set_length ( st . st_size ); <nl> append ( bp ); <nl> :: read ( fd , ( void *) c_str (), length ()); <nl> :: close ( fd );
int Journal < I >:: remove ( librados :: IoCtx & io_ctx , const std :: string & image_id ) { <nl> return r ; <nl> } <nl>  <nl> - r = journaler . remove ( false ); <nl> + r = journaler . remove ( true ); <nl> if ( r < 0 ) { <nl> lderr ( cct ) << " failed to remove journal : " << cpp_strerror ( r ) << dendl ; <nl> return r ;
string render_log_object_name ( const string & format , <nl> break ; <nl>  <nl> case ' i ': <nl> - sprintf ( buf , "% lld ", bucket_id ); <nl> + sprintf ( buf , "% lld ", ( long long ) bucket_id ); <nl> break ; <nl> case ' n ': <nl> o += bucket_name ;
extern " C " int rados_conf_parse_argv_remainder ( rados_t cluster , int argc , <nl> conf -> apply_changes ( NULL ); <nl> assert ( args . size () <= ( unsigned int ) argc ); <nl> unsigned int i ; <nl> - for ( i = 0 ; i < argc ; ++ i ) { <nl> + for ( i = 0 ; i < ( unsigned int ) argc ; ++ i ) { <nl> if ( i < args . size ()) <nl> remargv [ i ] = args [ i ]; <nl> else
int OSD :: mkfs ( const char * dev , ceph_fsid fsid , int whoami ) <nl> int OSD :: peek_super ( const char * dev , nstring & magic , ceph_fsid & fsid , int & whoami ) <nl> { <nl> ObjectStore * store = create_object_store ( dev ); <nl> + if (! store ) <nl> + return - ENODEV ; <nl> int err = store -> mount (); <nl> if ( err < 0 ) <nl> return err ;
void PG :: do_peer ( ObjectStore :: Transaction & t , list < Context *>& tfin , <nl> osd -> queue_generate_backlog ( this ); <nl> return ; <nl> } <nl> - for ( unsigned i = 0 ; i < acting . size (); i ++) { <nl> + for ( unsigned i = 1 ; i < acting . size (); i ++) { <nl> int o = acting [ i ]; <nl> Info & pi = peer_info [ o ]; <nl> if ( pi . last_complete < pi . log_tail && ! pi . log_backlog &&
bool OSDMonitor :: prepare_command_impl ( MonOpRequestRef op , <nl> } <nl>  <nl> if ( tunable == " straw_calc_version ") { <nl> - if ( value < 0 || value > 2 ) { <nl> + if ( value < 0 || value > 1 ) { <nl> ss << " value must be 0 or 1 ; got " << value ; <nl> err = - EINVAL ; <nl> goto reply ;
int EventCenter :: process_events ( int timeout_microseconds ) <nl> external_lock . Unlock (); <nl> while (! cur_process . empty ()) { <nl> EventCallbackRef e = cur_process . front (); <nl> - cur_process . pop_front (); <nl> if ( e ) <nl> e -> do_request ( 0 ); <nl> + cur_process . pop_front (); <nl> } <nl> } <nl> return numevents ;
void Server :: _rename_prepare ( MDRequestRef & mdr , <nl> } <nl> if ( tpi ) { <nl> tpi -> ctime = mdr -> get_op_stamp (); <nl> + destdn -> make_path_string ( tpi -> stray_prior_path ); <nl> tpi -> nlink --; <nl> if ( tpi -> nlink == 0 ) <nl> oldin -> state_set ( CInode :: STATE_ORPHAN );
PG :: RecoveryState :: Stray :: Stray ( my_context ctx ) <nl> assert (! pg -> is_active ()); <nl> assert (! pg -> is_peering ()); <nl> assert (! pg -> is_primary ()); <nl> - pg -> state_set ( PG_STATE_PEERING ); <nl> } <nl>  <nl> boost :: statechart :: result
public : <nl> RBD (); <nl> ~ RBD (); <nl>  <nl> + // This must be dynamically allocated with new , and <nl> + // must be released with release (). <nl> + // Do not use delete . <nl> struct AioCompletion { <nl> void * pc ; <nl> AioCompletion ( void * cb_arg , callback_t complete_cb );
bool CInode :: encode_inodestat ( bufferlist & bl , Session * session , <nl> int allowed = get_caps_allowed_for_client ( client ); <nl> int issue = ( cap -> wanted () | likes ) & allowed ; <nl> cap -> issue_norevoke ( issue ); <nl> + issue = cap -> pending (); <nl> cap -> set_last_issue_stamp ( g_clock . recent_now ()); <nl> cap -> touch (); // move to back of session cap LRU <nl> e . cap . caps = issue ;
protected : <nl> public : <nl> Message () { <nl> memset (& header , 0 , sizeof ( header )); <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> }; <nl> Message ( int t ) { <nl> memset (& header , 0 , sizeof ( header )); <nl> header . type = t ; <nl> header . priority = 0 ; // undef <nl> header . data_off = 0 ; <nl> + memset (& footer , 0 , sizeof ( footer )); <nl> } <nl> virtual ~ Message () { } <nl> 
void RGWZoneParams :: decode_json ( JSONObj * obj ) <nl> :: decode_json (" usage_log_pool ", usage_log_pool , obj ); <nl> :: decode_json (" user_keys_pool ", user_keys_pool , obj ); <nl> :: decode_json (" user_email_pool ", user_email_pool , obj ); <nl> + :: decode_json (" user_uid_pool ", user_uid_pool , obj ); <nl> :: decode_json (" user_swift_pool ", user_swift_pool , obj ); <nl> - :: decode_json (" user_uid_pool ", user_uid_pool , obj ); <nl> JSONDecoder :: decode_json (" system_key ", system_key , obj ); <nl> JSONDecoder :: decode_json (" placement_pools ", placement_pools , obj ); <nl> }
struct MOSDScrub : public Message { <nl> MOSDScrub () {} <nl> MOSDScrub ( ceph_fsid & f ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> - fsid ( f ) {} <nl> + fsid ( f ), repair ( false ) {} <nl> MOSDScrub ( ceph_fsid & f , vector < pg_t >& pgs , bool r ) : <nl> Message ( MSG_OSD_SCRUB ), <nl> fsid ( f ), scrub_pgs ( pgs ), repair ( r ) {}
void ReplicatedPG :: handle_watch_timeout ( WatchRef watch ) <nl>  <nl> // obc ref swallowed by repop ! <nl> simple_repop_submit ( repop ); <nl> + <nl> + // apply new object state . <nl> + ctx -> obc -> obs = ctx -> new_obs ; <nl> } <nl>  <nl> ObjectContextRef ReplicatedPG :: create_object_context ( const object_info_t & oi ,
int RGWRados :: flush_read_list ( struct get_obj_data * d ) <nl> bufferlist & bl = * iter ; <nl> r = d -> client_cb -> handle_data ( bl , 0 , bl . length ()); <nl> if ( r < 0 ) { <nl> - dout ( 0 ) << " ERROR : flush_read_list (): d -> client_c -> handle_data () returned " << r << dendl ; <nl> + dout ( 0 ) << " ERROR : flush_read_list (): d -> client_cb -> handle_data () returned " << r << dendl ; <nl> break ; <nl> } <nl> }
public : <nl>  <nl> MLogAck () : Message ( MSG_LOGACK ) {} <nl> MLogAck ( ceph_fsid_t & f , version_t l ) : Message ( MSG_LOGACK ), fsid ( f ), last ( l ) {} <nl> + private : <nl> + ~ MLogAck () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " log_ack "; } <nl> void print ( ostream & out ) { <nl> out << " log ( last " << last << ")";
void OSD :: start_recovery_op ( PG * pg , const hobject_t & soid ) <nl>  <nl> void OSD :: finish_recovery_op ( PG * pg , const hobject_t & soid , bool dequeue ) <nl> { <nl> + recovery_wq . lock (); <nl> dout ( 10 ) << " finish_recovery_op " << * pg << " " << soid <nl> << " dequeue =" << dequeue <nl> << " (" << recovery_ops_active << "/" << g_conf -> osd_recovery_max_active << " rops )" <nl> << dendl ; <nl> - recovery_wq . lock (); <nl>  <nl> // adjust count <nl> recovery_ops_active --;
void ObjectCopyRequest < I >:: send_update_object_map () { <nl> m_local_image_ctx -> snap_lock . put_read (); <nl> finish ( 0 ); <nl> return ; <nl> + } else if ( m_local_image_ctx -> object_map == nullptr ) { <nl> + // possible that exclusive lock was lost in background <nl> + derr << ": object map is not initialized " << dendl ; <nl> + <nl> + m_local_image_ctx -> snap_lock . put_read (); <nl> + finish (- EINVAL ); <nl> + return ; <nl> } <nl>  <nl> assert ( m_local_image_ctx -> object_map != nullptr );
sr_t * CInode :: project_snaprealm ( snapid_t snapid ) <nl> } else { <nl> new_srnode = new sr_t (); <nl> new_srnode -> created = snapid ; <nl> - new_srnode -> current_parent_since = snapid ; <nl> + new_srnode -> current_parent_since = get_oldest_snap (); <nl> } <nl> dout ( 10 ) << " project_snaprealm " << new_srnode << dendl ; <nl> projected_nodes . back ()-> snapnode = new_srnode ;
done : <nl> handler -> put_op ( op ); <nl> rgwstore -> destroy_context ( s -> obj_ctx ); <nl> FCGX_Finish_r ( fcgx ); <nl> - delete req ; <nl>  <nl> dout ( 1 ) << "====== req done req =" << hex << req << dec << " http_status =" << http_ret << " ======" << dendl ; <nl> + delete req ; <nl> } <nl>  <nl> class C_InitTimeout : public Context {
void take_min_markers ( IterIn first , IterIn last , IterOut dest ) <nl> } <nl> } <nl>  <nl> +} // anonymous namespace <nl> + <nl> class DataLogTrimCR : public RGWCoroutine { <nl> RGWRados * store ; <nl> RGWHTTPManager * http ; <nl> int DataLogTrimPollCR :: operate () <nl> return 0 ; <nl> } <nl>  <nl> -} // anonymous namespace <nl> - <nl> RGWCoroutine * create_data_log_trim_cr ( RGWRados * store , <nl> RGWHTTPManager * http , <nl> int num_shards , utime_t interval )
int RGWRados :: prepare_atomic_for_write ( RGWRadosCtx * rctx , rgw_obj & obj , librados <nl> do { <nl> r = prepare_atomic_for_write_impl ( rctx , obj , io_ctx , actual_obj , op , pstate ); <nl> } while ( r == - ECANCELED ); <nl> + <nl> + return r ; <nl> } <nl>  <nl> /**
struct C_MDC_TruncateFinish : public Context { <nl>  <nl> void MDCache :: _truncate_inode ( CInode * in , LogSegment * ls ) <nl> { <nl> - inode_t * pi = in -> get_projected_inode (); <nl> + inode_t * pi = & in -> inode ; <nl> dout ( 10 ) << " _truncate_inode " <nl> << pi -> truncate_from << " -> " << pi -> truncate_size <nl> << " on " << * in << dendl ;
const char * ceph_osd_flag_name ( unsigned flag ) <nl> case CEPH_OSD_FLAG_READ : return " read "; <nl> case CEPH_OSD_FLAG_WRITE : return " write "; <nl> case CEPH_OSD_FLAG_ORDERSNAP : return " ordersnap "; <nl> + case CEPH_OSD_FLAG_PEERSTAT_OLD : return " peerstat_old "; <nl> + case CEPH_OSD_FLAG_BALANCE_READS : return " balance_reads "; <nl> + case CEPH_OSD_FLAG_PARALLELEXEC : return " parallelexec "; <nl> case CEPH_OSD_FLAG_PGOP : return " pgop "; <nl> case CEPH_OSD_FLAG_EXEC : return " exec "; <nl> case CEPH_OSD_FLAG_EXEC_PUBLIC : return " exec_public ";
OSD :: OSD ( int id , Messenger * m , MonMap * mm , const char * dev ) : <nl>  <nl> state = STATE_BOOTING ; <nl>  <nl> + memset (& my_stat , 0 , sizeof ( my_stat )); <nl> + <nl> stat_ops = 0 ; <nl> stat_qlen = 0 ; <nl> stat_rd_ops = stat_rd_ops_shed_in = stat_rd_ops_shed_out = 0 ;
OPTION ( osd_mon_shutdown_timeout , OPT_DOUBLE , 5 ) <nl> OPTION ( osd_max_object_size , OPT_U64 , 100 * 1024L * 1024L * 1024L ) // OSD ' s maximum object size <nl> OPTION ( osd_max_attr_size , OPT_U64 , 0 ) <nl>  <nl> - OPTION ( filestore , OPT_BOOL , false ) <nl> - <nl> /// filestore wb throttle limits <nl> OPTION ( filestore_wbthrottle_enable , OPT_BOOL , true ) <nl> OPTION ( filestore_wbthrottle_btrfs_bytes_start_flusher , OPT_U64 , 41943040 )
void CDir :: _omap_fetched ( bufferlist & hdrbl , map < string , bufferlist >& omap , <nl>  <nl> void CDir :: go_bad () <nl> { <nl> + if ( get_version () == 0 ) <nl> + set_version ( 1 ); <nl> state_set ( STATE_BADFRAG ); <nl> // mark complete , ! fetching <nl> mark_complete (); <nl> state_clear ( STATE_FETCHING ); <nl> auth_unpin ( this ); <nl> - <nl> + <nl> // kick waiters <nl> finish_waiting ( WAIT_COMPLETE , 0 ); <nl> }
class MMDSMap : public Message { <nl> epoch = mm -> get_epoch (); <nl> mm -> encode ( encoded ); <nl> } <nl> + private : <nl> + ~ MMDSMap () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " mdsmap "; } <nl> void print ( ostream & out ) { <nl> out << " mdsmap ( e " << epoch << ")";
int execute ( const po :: variables_map & vm ) { <nl> return r ; <nl> } <nl>  <nl> + io_ctx . set_osdmap_full_try (); <nl> + <nl> librbd :: RBD rbd ; <nl> r = do_delete ( rbd , io_ctx , image_name . c_str (), <nl> vm [ at :: NO_PROGRESS ]. as < bool >());
class FileLock : public SimpleLock { <nl> } <nl> bool can_rdlock_soon () { <nl> if ( parent -> is_auth ()) <nl> - return ( state == LOCK_GLOCKL ); <nl> + return <nl> + ( state == LOCK_GLOCKL ) || <nl> + ( state == LOCK_LOCK && xlock_by ); <nl> else <nl> return false ; <nl> }
void Mgr :: init () <nl> cluster_state . notify_osdmap ( osd_map ); <nl> }); <nl>  <nl> + // Subscribe to OSDMap update to pass on to ClusterState <nl> + objecter -> maybe_request_map (); <nl> + <nl> monc -> sub_want (" mgrdigest ", 0 , 0 ); <nl>  <nl> // Prepare to receive FSMap and request it <nl> bool Mgr :: ms_dispatch ( Message * m ) <nl> m -> put (); <nl> break ; <nl> case CEPH_MSG_OSD_MAP : <nl> - <nl> handle_osd_map (); <nl>  <nl> py_modules . notify_all (" osd_map ", "");
int FileStore :: _detect_fs () <nl> dout ( 0 ) << " mount FIEMAP ioctl is disabled via ' filestore fiemap ' config option " << dendl ; <nl> ioctl_fiemap = false ; <nl> } <nl> + free ( fiemap ); <nl>  <nl> struct statfs st ; <nl> r = :: fstatfs ( fd , & st );
int CrushWrapper :: device_class_clone ( <nl> // pick a new shadow bucket id that is not used by the current map <nl> // * or * any previous shadow buckets . <nl> bno = - 1 ; <nl> - while ( crush -> buckets [- 1 - bno ] || <nl> + while (((- 1 - bno ) < crush -> max_buckets && crush -> buckets [- 1 - bno ]) || <nl> used_ids . count ( bno )) { <nl> -- bno ; <nl> }
public : <nl> param ( NULL ) {} <nl> ~ CryptoAESKeyHandler () { <nl> SECITEM_FreeItem ( param , PR_TRUE ); <nl> - PK11_FreeSymKey ( key ); <nl> - PK11_FreeSlot ( slot ); <nl> + if ( key ) <nl> + PK11_FreeSymKey ( key ); <nl> + if ( slot ) <nl> + PK11_FreeSlot ( slot ); <nl> } <nl>  <nl> int init ( const bufferptr & s , ostringstream & err ) {
int ceph_setxattr ( struct dentry * dentry , const char * name , <nl> if ( strncmp ( name , " user .", 5 ) != 0 ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( _ceph_match_vir_xattr ( name ) != NULL ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> /* copy value into some pages */ <nl> nr_pages = calc_pages_for ( 0 , size ); <nl> if ( nr_pages ) {
static inline int ceph_fsid_compare ( const struct ceph_fsid * a , <nl> return memcmp ( a , b , sizeof (* a )); <nl> } <nl>  <nl> + static inline void ceph_fsid_set ( struct ceph_fsid * d , <nl> + const struct ceph_fsid * s ) <nl> +{ <nl> + memcpy ( d , s , sizeof (* d )); <nl> +} <nl> + <nl> /* <nl> * ino , object , etc . <nl> */
void Server :: handle_client_rename ( MDRequestRef & mdr ) <nl> & remote_wrlocks , auth_pin_freeze )) <nl> return ; <nl>  <nl> + if (! check_access ( mdr , srcdn -> get_dir ()-> get_inode (), MAY_WRITE )) <nl> + return ; <nl> + <nl> + if (! check_access ( mdr , destdn -> get_dir ()-> get_inode (), MAY_WRITE )) <nl> + return ; <nl> + <nl> + if (! check_access ( mdr , srci , MAY_WRITE )) <nl> + return ; <nl> + <nl> if ( oldin && <nl> oldin -> is_dir () && <nl> _dir_is_nonempty ( mdr , oldin )) {
static inline std :: string pg_state_string ( int state ) <nl> if ( state & PG_STATE_SCANNING ) <nl> oss << " scanning +"; <nl> string ret ( oss . str ()); <nl> - return ( ret . length () == 0 ) ? " inactive " : ret ; <nl> + if ( ret . length () > 0 ) <nl> + ret . resize ( ret . length () - 1 ); <nl> + else <nl> + ret = " inactive "; <nl> + return ret ; <nl> } <nl>  <nl> 
struct ObjectOperation { <nl> string mname = " filter "; <nl> :: encode ( cname , osd_op . indata ); <nl> :: encode ( mname , osd_op . indata ); <nl> - :: encode ( cookie , osd_op . indata ); <nl> osd_op . indata . append ( filter ); <nl> + :: encode ( cookie , osd_op . indata ); <nl> } <nl> void add_alloc_hint ( int op , uint64_t expected_object_size , <nl> uint64_t expected_write_size ) {
public : <nl> case CEPH_LOCK_IXATTR : return 8 + 6 * SimpleLock :: WAIT_BITS ; <nl> case CEPH_LOCK_ISNAP : return 8 + 7 * SimpleLock :: WAIT_BITS ; <nl> case CEPH_LOCK_INEST : return 8 + 8 * SimpleLock :: WAIT_BITS ; <nl> + case CEPH_LOCK_IFLOCK : return 8 + 9 * SimpleLock :: WAIT_BITS ; <nl> default : <nl> assert ( 0 ); <nl> }
void send_mds_reconnect ( struct ceph_mds_client * mdsc , int mds ) <nl> session = __get_session ( mdsc , mds ); <nl> if ( session ) { <nl> session -> s_state = CEPH_MDS_SESSION_RECONNECTING ; <nl> + session -> s_cap_seq = 0 ; <nl>  <nl> /* estimate needed space */ <nl> len += session -> s_nr_caps *
void OSD :: build_initial_pg_history ( <nl> h -> last_epoch_split = e ; <nl> } <nl> lastmap = osdmap ; <nl> + up_primary = new_up_primary ; <nl> + acting_primary = new_acting_primary ; <nl> + up = new_up ; <nl> + acting = new_acting ; <nl> } <nl> dout ( 20 ) << __func__ << " " << debug . str () << dendl ; <nl> dout ( 10 ) << __func__ << " " << * h << " " << * pi
void OSD :: handle_osd_map ( MOSDMap * m ) <nl> << " but failed to encode full with correct crc ; requesting " <nl> << dendl ; <nl> clog -> warn () << " failed to encode map e " << e << " with expected crc \ n "; <nl> + delete o ; <nl> MMonGetOSDMap * req = new MMonGetOSDMap ; <nl> req -> request_full ( e , last ); <nl> monc -> send_mon_message ( req );
int main ( int argc , const char ** argv ) <nl> derr << " ERROR : failed initializing frontend " << dendl ; <nl> return - r ; <nl> } <nl> - fe -> run (); <nl> + r = fe -> run (); <nl> + if ( r < 0 ) { <nl> + derr << " ERROR : failed run " << dendl ; <nl> + return - r ; <nl> + } <nl>  <nl> fes . push_back ( fe ); <nl> }
void Monitor :: handle_probe ( MonOpRequestRef op ) <nl> } <nl> } <nl>  <nl> -/** <nl> - * @ todo fix this . This is going to cause trouble . <nl> - */ <nl> void Monitor :: handle_probe_probe ( MonOpRequestRef op ) <nl> { <nl> MMonProbe * m = static_cast < MMonProbe *>( op -> get_req ());
void Server :: handle_client_setlayout ( MDRequest * mdr ) <nl> // validate layout <nl> // FIXME : only set striping parameters , for now . <nl> ceph_file_layout layout ; <nl> + memset (& layout , 0 , sizeof ( layout )); <nl>  <nl> if ( req -> head . args . setlayout . layout . fl_object_size > 0 ) <nl> layout . fl_object_size = req -> head . args . setlayout . layout . fl_object_size ;
extern " C " int rados_pool_list ( rados_t cluster , char * buf , size_t len ) <nl> std :: list < std :: string > pools ; <nl> client -> pool_list ( pools ); <nl>  <nl> + if (! buf ) <nl> + return - EINVAL ; <nl> + <nl> char * b = buf ; <nl> if ( b ) <nl> memset ( b , 0 , len );
struct MClientLease : public Message { <nl> h . first = sf ; <nl> h . last = sl ; <nl> } <nl> + private : <nl> + ~ MClientLease () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " client_lease "; } <nl> void print ( ostream & out ) { <nl> out << " client_lease ( a =" << ceph_lease_op_name ( get_action ())
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> continue ; <nl> ci = ceph_inode ( inode ); <nl> spin_lock (& inode -> i_lock ); <nl> + if (! ci -> i_snap_realm ) <nl> + goto split_skip_inode ; <nl> ceph_put_snap_realm ( mdsc , ci -> i_snap_realm ); <nl> list_add (& ci -> i_snap_realm_item , <nl> & realm -> inodes_with_caps ); <nl> ci -> i_snap_realm = realm ; <nl> realm -> nref ++; <nl> + split_skip_inode : <nl> spin_unlock (& inode -> i_lock ); <nl> iput ( inode ); <nl> }
void SaveVCalendar ( TNEFStruct TNEF , int isMtgReq ) { <nl> if ( isMtgReq ) { <nl> CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " MtgReq ", " ics ", filepath ); <nl> } else { <nl> - CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " calendar ", " vcf ", filepath ); <nl> + CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " calendar ", " ics ", filepath ); <nl> } <nl>  <nl> printf ("% s \ n ", ifilename );
void virtio_delete_queues ( VirtIODevice * vdev ) <nl> struct virtqueue * vq ; <nl> unsigned i ; <nl>  <nl> + if ( vdev -> info == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < vdev -> maxQueues ; i ++) { <nl> vq = vdev -> info [ i ]. vq ; <nl> if ( vq != NULL ) {
extern <nl> /* Sadly we can ' t include < malloc . h > as it causes a redefinition error */ <nl> size_t malloc_usable_size ( void *); <nl> # elif defined ( __APPLE__ ) <nl> - # include < malloc . h > <nl> + # if TARGET_OS_IPHONE <nl> + # include < malloc / malloc . h > <nl> + # else <nl> + # include < malloc . h > <nl> + # endif <nl> # else <nl> # error Do not know what to do here <nl> # endif
int der_decode_raw_bit_string ( const unsigned char * in , unsigned long inlen , <nl> blen = (( dlen - 1 ) << 3 ) - ( in [ x ++] & 7 ); <nl>  <nl> /* too many bits ? */ <nl> - if ( blen > * outlen ) { <nl> + if ( blen / 8 > * outlen ) { <nl> * outlen = blen ; <nl> return CRYPT_BUFFER_OVERFLOW ; <nl> }
ONIG_EXTERN int onigenc_unicode_apply_all_case_fold P_ (( OnigCaseFoldType flag , O <nl> addr = OnigUnicodeFolds2 + ( buk )-> index ;\ <nl> else if (( buk )-> fold_len == 3 )\ <nl> addr = OnigUnicodeFolds3 + ( buk )-> index ;\ <nl> + else \ <nl> + addr = 0 ;\ <nl> } while ( 0 ) <nl>  <nl> extern OnigCodePoint OnigUnicodeFolds1 [];
static int jpeg_size ( unsigned char * data , unsigned int data_size , <nl> return 0 ; <nl> } <nl> i += 2 ; <nl> - block_length = data [ i ] * 256 + data [ i + 1 ]; <nl> + if ( i + 1 < data_size ) <nl> + block_length = data [ i ] * 256 + data [ i + 1 ]; <nl> } <nl> } <nl> }
+/* vim : set tabstop = 8 shiftwidth = 4 softtabstop = 4 smarttab expandtab autoindent : */ <nl> /** <nl> * The following sites have various bits & pieces about PDF document <nl> * generation
uint64 ReadCodedSizeValue ( const binary * InBuffer , uint32 & BufferSize , uint64 & <nl> // ID found <nl> PossibleSizeLength = SizeIdx + 1 ; <nl> SizeBitMask >>= SizeIdx ; <nl> + <nl> + // Guard against invalid memory accesses with incomplete IDs . <nl> + if ( PossibleSizeLength > BufferSize ) <nl> + break ; <nl> + <nl> for ( SizeIdx = 0 ; SizeIdx < PossibleSizeLength ; SizeIdx ++) { <nl> PossibleSize [ SizeIdx ] = InBuffer [ SizeIdx ]; <nl> }
jv jq_next ( jq_state * jq ) { <nl> if ( opcode != ON_BACKTRACK ( DESTRUCTURE_ALT )) { <nl> jv_free ( stack_pop ( jq )); // free the input <nl> stack_push ( jq , jv_invalid_get_msg ( jq -> error )); // push the error ' s message <nl> + } else { <nl> + jv_free ( jq -> error ); <nl> } <nl> jq -> error = jv_null (); <nl> uint16_t offset = * pc ++;
tar_directory_for_file ( GsfInfileTar * dir , const char * name , gboolean last ) <nl> gsf_infile_child_by_name ( GSF_INFILE ( dir ), <nl> dirname ); <nl> if ( subdir ) { <nl> + dir = GSF_IS_INFILE_TAR ( subdir ) <nl> + ? GSF_INFILE_TAR ( subdir ) <nl> + : dir ; <nl> /* Undo the ref . */ <nl> g_object_unref ( subdir ); <nl> - dir = GSF_INFILE_TAR ( subdir ); <nl> } else <nl> dir = tar_create_dir ( dir , dirname ); <nl> }
int32_t ByteArray :: SetFilledLength ( int32_t filled_length ) { <nl> } <nl>  <nl> int32_t ByteArray :: Get ( int32_t index ) { <nl> + if ( index < 0 || index >= Length ()) <nl> + return - 1 ; <nl> return InternalGet ( index ) & 0xff ; <nl> } <nl> 
static char * check_dir_or_file ( const char * name ) { <nl> if ( ptr && strlen ( ptr ) == strlen ("/ firejail ")) { <nl> if ( arg_debug ) <nl> printf (" firejail exec symlink detected \ n "); <nl> + free ( actual_path ); <nl> free ( fname ); <nl> fname = NULL ; <nl> i ++; <nl> continue ; <nl> } <nl> + free ( actual_path ); <nl> } <nl>  <nl> }
void * poller (){ <nl> cmd_stdout = popen ( entry -> command , " r "); <nl> if ( cmd_stdout != NULL ) fgets ( cmd_result , 64 , cmd_stdout ); <nl> if ( is_number ( cmd_result )) result = atoll ( cmd_result ); <nl> - break ; <nl> + pclose ( cmd_stdout ); <nl> + break ; <nl> default : <nl> printf (" Unknown Action !\ n "); <nl> result = 0 ;
rsvg_filter_primitive_free ( gpointer impl ) <nl> { <nl> RsvgFilterPrimitive * primitive = impl ; <nl>  <nl> - g_string_free ( primitive -> in , TRUE ); <nl> - g_string_free ( primitive -> result , TRUE ); <nl> + if ( primitive -> in ) { <nl> + g_string_free ( primitive -> in , TRUE ); <nl> + } <nl> + <nl> + if ( primitive -> result ) { <nl> + g_string_free ( primitive -> result , TRUE ); <nl> + } <nl>  <nl> g_free ( primitive ); <nl> }
rsvg_filter_primitive_gaussian_blur_render ( RsvgFilterPrimitive * self , RsvgFilt <nl> boundarys . x0 , boundarys . y0 , <nl> boundarys . x1 - boundarys . x0 , boundarys . y1 - boundarys . y0 ); <nl> cairo_fill ( cr ); <nl> + cairo_destroy ( cr ); <nl> } <nl>  <nl> op . surface = output ;
int main ( int argc , char * argv []) <nl> " dosfslabel : labels can be no longer than 11 characters \ n "); <nl> exit ( 1 ); <nl> } <nl> - for ( i = 0 ; i < 11 ; i ++) <nl> + for ( i = 0 ; label [ i ] && i < 11 ; i ++) <nl> /* don ' t know if here should be more strict ! uppercase ( label [ i ])*/ <nl> if ( islower ( label [ i ])) { <nl> fprintf ( stderr ,
void set_fat ( DOS_FS * fs , uint32_t cluster , int32_t new ) <nl> data [ 1 ] = new >> 4 ; <nl> } else { <nl> FAT_ENTRY subseqEntry ; <nl> - if ( cluster != fs -> clusters - 1 ) <nl> + if ( cluster != fs -> clusters + 1 ) <nl> get_fat (& subseqEntry , fs -> fat , cluster + 1 , fs ); <nl> else <nl> subseqEntry . value = 0 ;
typedef int int32_t ; <nl>  <nl> # endif /* HAVE_CONFIG_H */ <nl>  <nl> - int utf8_encode ( int codepoint , char * buffer , size_t * size ); <nl> + int utf8_encode ( int32_t codepoint , char * buffer , size_t * size ); <nl>  <nl> size_t utf8_check_first ( char byte ); <nl> size_t utf8_check_full ( const char * buffer , size_t size , int32_t * codepoint );
struct iw_rr_ctx { <nl> }; <nl>  <nl>  <nl> - static IW_INLINE double iw_sinc ( double x ) <nl> + static double iw_sinc ( double x ) <nl> { <nl> - if ( x == 0 . 0 ) return 1 . 0 ; <nl> + if ( x <= 0 . 000000005 ) return 1 . 0 ; <nl> return sin ( M_PI * x )/( M_PI * x ); <nl> } <nl> 
typedef double iw_tmpsample ; <nl>  <nl> # ifdef IW_64BIT <nl> -# define IW_DEFAULT_MAX_DIMENSION 1000000 <nl> -# define IW_DEFAULT_MAX_MALLOC 2000000000000 <nl> +# define IW_DEFAULT_MAX_DIMENSION 40000 <nl> +# define IW_DEFAULT_MAX_MALLOC 2000000000 <nl> # else <nl> # define IW_DEFAULT_MAX_DIMENSION 40000 // Must be less than sqrt ( 2 ^ 31 ). <nl> # define IW_DEFAULT_MAX_MALLOC 2000000000
static int iwgif_read_image ( struct iwgifrcontext * rctx ) <nl>  <nl> rctx -> image_width = ( int ) iw_get_ui16le (& rctx -> rbuf [ 4 ]); <nl> rctx -> image_height = ( int ) iw_get_ui16le (& rctx -> rbuf [ 6 ]); <nl> + if ( rctx -> image_width < 1 || rctx -> image_height < 1 ) { <nl> + iw_set_error ( rctx -> ctx , " Invalid image dimensions "); <nl> + goto done ; <nl> + } <nl>  <nl> rctx -> interlaced = ( int )(( rctx -> rbuf [ 8 ]>> 6 )& 0x01 ); <nl> 
int ares_parse_a_reply ( const unsigned char * abuf , int alen , <nl> rr_class = DNS_RR_CLASS ( aptr ); <nl> rr_len = DNS_RR_LEN ( aptr ); <nl> aptr += RRFIXEDSZ ; <nl> + if ( aptr + rr_len > abuf + alen ) <nl> + { <nl> + free ( rr_name ); <nl> + status = ARES_EBADRESP ; <nl> + break ; <nl> + } <nl>  <nl> if ( rr_class == C_IN && rr_type == T_A <nl> && rr_len == sizeof ( struct in_addr )
int main ( gint argc , gchar ** argv ) { <nl> bind ("/", prefix ); <nl>  <nl> fail_if ( chroot ( prefix )); <nl> + fail_if ( chdir ("/")); <nl> fail_if ( execvp (* argv , argv )); <nl> } <nl> 
char * argv [],* envp []; <nl> else <nl> program_name = argv [ 0 ]; <nl>  <nl> - if (!( eargv = ( char **) malloc (( argc + 3 ) * sizeof ( char *)))) <nl> + if (!( eargv = ( char **) malloc (( argc + 4 ) * sizeof ( char *)))) <nl> { <nl> fprintf ( stderr , "% s : Failed to obtain memory .\ n ", program_name ); <nl> exit ( 1 ); /* Trap core dump ! */
check_user_token ( const char * authfile , <nl> { <nl> if ( verbose ) <nl> D ( debug_file , " Match user / token as % s /% s ", username , otp_id ); <nl> + <nl> + fclose ( opwfile ); <nl> return AUTH_FOUND ; <nl> } <nl> }
static int create_zone_index ( const char * directory , timelib_tzdb * db ) <nl> db_index [ index_next ]. pos = data_size ; <nl> data_size += length ; <nl> free ( tzfile_data ); <nl> + <nl> + index_next ++; <nl> + } else { <nl> + free ( db_index [ index_next ]. id ); <nl> } <nl> } <nl> - <nl> - index_next ++; <nl> } <nl> } <nl> 
exit_on_inactivity ( gpointer user_data ) <nl> extern gboolean in_shutdown ; <nl>  <nl> if (! in_shutdown ) <nl> - exit ( 0 ); <nl> + { <nl> + GDBusConnection * session_bus ; <nl> + <nl> + session_bus = g_bus_get_sync ( G_BUS_TYPE_SESSION , NULL , NULL ); <nl> + g_dbus_connection_flush_sync ( session_bus , NULL , NULL ); <nl> + g_object_unref ( session_bus ); <nl> + <nl> + exit ( 0 ); <nl> + } <nl>  <nl> return FALSE ; <nl> }
retry : <nl> usbmuxd_device_info_t * devinfo = device_info_from_device_record ( dev ); <nl> if (! devinfo ) { <nl> DEBUG ( 1 , "% s : can ' t create device info object \ n ", __func__ ); <nl> - free ( payload ); <nl> return - 1 ; <nl> } <nl> collection_add (& tmpdevs , devinfo );
Perl_re_op_compile ( pTHX_ SV ** const patternp , int pat_count , <nl>  <nl> /* We have that number in RExC_npar */ <nl> RExC_total_parens = RExC_npar ; <nl> + <nl> + /* XXX For backporting , use long jumps if there is any possibility of <nl> + * overflow */ <nl> + if ( RExC_size > U16_MAX && ! RExC_use_BRANCHJ ) { <nl> + RExC_use_BRANCHJ = TRUE ; <nl> + flags |= RESTART_PARSE ; <nl> + } <nl> } <nl> else if (! MUST_RESTART ( flags )) { <nl> ReREFCNT_dec ( Rx );
S_study_chunk ( pTHX_ RExC_state_t * pRExC_state , regnode ** scanp , <nl> RExC_precomp ))); <nl> } <nl>  <nl> + if ( ( minnext > 0 && mincount >= SSize_t_MAX / minnext ) <nl> + || min >= SSize_t_MAX - minnext * mincount ) <nl> + { <nl> + FAIL (" Regexp out of space "); <nl> + } <nl> + <nl> min += minnext * mincount ; <nl> is_inf_internal |= deltanext == SSize_t_MAX <nl> || ( maxcount == REG_INFTY && minnext + deltanext > 0 );
int charconv_buffer ( const char * tocode , const char * fromcode , <nl> * result_len = result_length ; <nl> } <nl> } <nl> + else { <nl> + mmap_string_free ( mmapstr ); <nl> + } <nl> } <nl> + else { <nl> + mmap_string_free ( mmapstr ); <nl> + } <nl> return res ; <nl> } <nl> /* else , let ' s try with iconv , if available */
static Image * ReadTIFFImage ( const ImageInfo * image_info , <nl> /* <nl> Convert stripped TIFF image . <nl> */ <nl> - extent = 4 *( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> + extent = 4 *(( image -> depth + 7 )/ 8 )*( samples_per_pixel + 1 )* TIFFStripSize ( tiff ); <nl> strip_pixels =( unsigned char *) AcquireQuantumMemory ( extent , <nl> sizeof (* strip_pixels )); <nl> if ( strip_pixels == ( unsigned char *) NULL )
void rfbScaledScreenUpdateRect ( rfbScreenInfoPtr screen , rfbScreenInfoPtr ptr , in <nl> default : <nl> /* fixme : endianness problem ? */ <nl> for ( z = 0 ; z < bytesPerPixel ; z ++) <nl> - pixel_value += ( srcptr2 [ z ] << ( 8 * z )); <nl> + pixel_value += (( unsigned long ) srcptr2 [ z ] << ( 8 * z )); <nl> break ; <nl> } <nl> /*
HandleRFBServerMessage ( rfbClient * client ) <nl> return FALSE ; <nl> } <nl>  <nl> - buffer = malloc (( uint64_t ) msg . sct . length + 1 ); <nl> + buffer = malloc ( msg . sct . length + 1 ); <nl>  <nl> if (! ReadFromRFBServer ( client , buffer , msg . sct . length )) { <nl> free ( buffer );
rfbSendServerCutText ( rfbScreenInfoPtr rfbScreen , char * str , int len ) <nl> rfbServerCutTextMsg sct ; <nl> rfbClientIteratorPtr iterator ; <nl>  <nl> + memset (( char *)& sct , 0 , sizeof ( sct )); <nl> + <nl> iterator = rfbGetClientIterator ( rfbScreen ); <nl> while (( cl = rfbClientIteratorNext ( iterator )) != NULL ) { <nl> sct . type = rfbServerCutText ;
ConnectClientToUnixSock ( const char * sockFile ) <nl> int sock ; <nl> struct sockaddr_un addr ; <nl> addr . sun_family = AF_UNIX ; <nl> + if ( strlen ( sockFile ) + 1 > sizeof ( addr . sun_path )) { <nl> + rfbClientErr (" ConnectToUnixSock : socket file name too long \ n "); <nl> + return - 1 ; <nl> + } <nl> strcpy ( addr . sun_path , sockFile ); <nl>  <nl> sock = socket ( AF_UNIX , SOCK_STREAM , 0 );
im_vips2dz ( IMAGE * in , const char * filename ) <nl> * p = '\ 0 '; <nl> im_strncpy ( mode , p + 1 , FILENAME_MAX ); <nl> } <nl> + else <nl> + strcpy ( mode , "" ); <nl>  <nl> strcpy ( buf , mode ); <nl> p = & buf [ 0 ];
static void ctrycatchfinally ( JF , js_Ast * trystm , js_Ast * catchvar , js_Ast * catch <nl> emitstring ( J , F , OP_CATCH , catchvar -> string ); <nl> cstm ( J , F , catchstm ); <nl> emit ( J , F , OP_ENDCATCH ); <nl> + emit ( J , F , OP_ENDTRY ); <nl> L3 = emitjump ( J , F , OP_JUMP ); /* skip past the try block to the finally block */ <nl> } <nl> label ( J , F , L1 );
virDomainGetTime ( virDomainPtr dom , <nl> virResetLastError (); <nl>  <nl> virCheckDomainReturn ( dom , - 1 ); <nl> + virCheckReadOnlyGoto ( dom -> conn -> flags , error ); <nl>  <nl> if ( dom -> conn -> driver -> domainGetTime ) { <nl> int ret = dom -> conn -> driver -> domainGetTime ( dom , seconds ,
qemuProcessHandleMonitorEOF ( qemuMonitorPtr mon , <nl> /* We don ' t want this EOF handler to be called over and over while the <nl> * thread is waiting for a job . <nl> */ <nl> + virObjectLock ( mon ); <nl> qemuMonitorUnregister ( mon ); <nl> + virObjectUnlock ( mon ); <nl>  <nl> /* We don ' t want any cleanup from EOF handler ( or any other <nl> * thread ) to enter qemu namespace . */
ofproto_rule_insert__ ( struct ofproto * ofproto , struct rule * rule ) <nl> const struct rule_actions * actions = rule_get_actions ( rule ); <nl>  <nl> /* A rule may not be reinserted . */ <nl> - ovs_assert ( rule -> state == RULE_INITIALIZED ); <nl> + ovs_assert ( rule -> state != RULE_INSERTED ); <nl>  <nl> if ( rule -> hard_timeout || rule -> idle_timeout ) { <nl> ovs_list_insert (& ofproto -> expirable , & rule -> expirable );
status WAVEFile :: parseFormat ( const Tag & id , uint32_t size ) <nl>  <nl> /* numCoefficients should be at least 7 . */ <nl> assert ( numCoefficients >= 7 && numCoefficients <= 255 ); <nl> + if ( numCoefficients < 7 || numCoefficients > 255 ) <nl> + { <nl> + _af_error ( AF_BAD_HEADER , <nl> + " Bad number of coefficients "); <nl> + return AF_FAIL ; <nl> + } <nl>  <nl> m_msadpcmNumCoefficients = numCoefficients ; <nl> 
uint32_t skip ( Protocol_ & prot , TType arg_type ) { <nl> result += prot . readListEnd (); <nl> return result ; <nl> } <nl> - default : <nl> - return 0 ; <nl> + default : { <nl> + TProtocolException :: throwInvalidSkipType ( arg_type ); <nl> + } <nl> } <nl> } <nl> 
_rsvg_io_get_file_path ( const gchar * filename , <nl> { <nl> gchar * absolute_filename ; <nl>  <nl> - if ( g_file_test ( filename , G_FILE_TEST_EXISTS ) || g_path_is_absolute ( filename )) { <nl> + if ( g_path_is_absolute ( filename )) { <nl> absolute_filename = g_strdup ( filename ); <nl> } else { <nl> gchar * tmpcdir ;
NS_ASSUME_NONNULL_BEGIN <nl> @ interface OTRXMPPMessageYapStroage : XMPPModule <nl>  <nl> @ property ( nonatomic , strong , readonly ) YapDatabaseConnection * databaseConnection ; <nl> +@ property ( nonatomic , readonly ) dispatch_queue_t moduleDelegateQueue ; <nl>  <nl> /** This connection is only used for readWrites */ <nl> - ( instancetype ) initWithDatabaseConnection :( YapDatabaseConnection *) databaseConnection ;
int fs_mount ( spiffs * spf , uint32_t addr , uint32_t size , uint8_t * workbuf , <nl> cfg . hal_erase_f = esp_spiffs_erase ; <nl>  <nl> if ( SPIFFS_mount ( spf , & cfg , workbuf , fds , fds_size , 0 , 0 , 0 ) != SPIFFS_OK ) { <nl> + LOG ( LL_ERROR , (" SPIFFS_mount failed : % d ", SPIFFS_errno ( spf ))); <nl> return SPIFFS_errno ( spf ); <nl> } <nl> 
static const char * stbvox_vertex_program = <nl> " uniform vec3 normal_table [ 32 ];\ n " <nl>  <nl> # ifndef STBVOX_CONFIG_OPENGL_MODELVIEW <nl> - " uniform mat44 model_view ;\ n " <nl> + " uniform mat4x4 model_view ;\ n " <nl> # endif <nl>  <nl> // fragment output data
void stb_leakcheck_free ( void * ptr ) <nl> if ( mi -> next ) <nl> mi -> next -> prev = mi -> prev ; <nl> # endif <nl> + free ( ptr ); <nl> } <nl> } <nl> 
innobase_start_or_create_for_mysql ( void ) <nl> sum_of_new_sizes += srv_data_file_sizes [ i ]; <nl> } <nl>  <nl> - if ( sum_of_new_sizes < 640 ) { <nl> + if ( sum_of_new_sizes < 10485760 / UNIV_PAGE_SIZE ) { <nl> fprintf ( stderr , <nl> " InnoDB : Error : tablespace size must be " <nl> " at least 10 MB \ n ");
static int ndbcluster_reset_logs ( THD * thd ) <nl> if (! ndb_binlog_running ) <nl> return 0 ; <nl>  <nl> + /* only reset master should reset logs */ <nl> + if (!( thd -> lex -> type & REFRESH_MASTER )) <nl> + return 0 ; <nl> + <nl> DBUG_ENTER (" ndbcluster_reset_logs "); <nl>  <nl> /*
innodb_initialize ( <nl>  <nl> my_eng_config = ( eng_config_info_t *) config_str ; <nl>  <nl> + /* If no call back function registered ( InnoDB engine failed to load ), <nl> + load InnoDB Memcached engine should fail too */ <nl> + if (! my_eng_config -> cb_ptr ) { <nl> + return ( ENGINE_TMPFAIL ); <nl> + } <nl> + <nl> /* Register the call back function */ <nl> register_innodb_cb (( void *) my_eng_config -> cb_ptr ); <nl> 
bool st_table_list :: prepare_security ( THD * thd ) <nl> { <nl> List_iterator_fast < TABLE_LIST > tb (* view_tables ); <nl> TABLE_LIST * tbl ; <nl> + DBUG_ENTER (" st_table_list :: prepare_security "); <nl> # ifndef NO_EMBEDDED_ACCESS_CHECKS <nl> Security_context * save_security_ctx = thd -> security_ctx ; <nl> - DBUG_ENTER (" st_table_list :: prepare_security "); <nl>  <nl> DBUG_ASSERT (! prelocking_placeholder ); <nl> if ( prepare_view_securety_context ( thd ))
lock_table_locks_check ( <nl> ut_a ( table != NULL ); <nl> ut_ad ( lock_mutex_own ()); <nl>  <nl> + rw_lock_s_lock (& trx_sys -> lock ); <nl> + <nl> for ( trx = UT_LIST_GET_FIRST ( trx_sys -> trx_list ); <nl> trx != NULL ; <nl> trx = UT_LIST_GET_NEXT ( trx_list , trx )) { <nl> lock_table_locks_check ( <nl> } <nl> } <nl>  <nl> + rw_lock_s_unlock (& trx_sys -> lock ); <nl> + <nl> return ( NULL ); <nl> } <nl> # endif /* UNIV_DEBUG */
multi_delete :: multi_delete ( THD * thd_arg , TABLE_LIST * dt , <nl> table -> used_keys = 0 ; <nl> tempfiles [ counter ] = new Unique ( refposcmp2 , <nl> ( void *) & table -> file -> ref_length , <nl> - table -> file -> ref_length + 1 , <nl> + table -> file -> ref_length , <nl> MEM_STRIP_BUF_SIZE ); <nl> } <nl> }
__weak_alias ( vis , _vis ) <nl> # define MAXEXTRAS 5 <nl>  <nl>  <nl> - char * MAKEEXTRALIST ( uint flag , const char * orig ) <nl> + char * MAKEEXTRALIST ( unsigned int flag , const char * orig ) <nl> { <nl> const char * o = orig ; <nl> char * e , * extra ;
TABLE_COUNTER_TYPE Query_cache :: is_cacheable ( THD * thd , uint32 query_len , <nl> ( tables_used -> db_length == 5 && <nl> # ifdef FN_NO_CASE_SENCE <nl> // TODO : latin1 charset should be replaced with system charset <nl> - my_strncasecmp ( my_charset_latin1 , tables_used -> db ," mysql ", 5 ) == 0 <nl> + my_strncasecmp (& my_charset_latin1 , <nl> + tables_used -> db , <nl> + " mysql ", 5 ) == 0 <nl> # else <nl> tables_used -> db [ 0 ]==' m ' && <nl> tables_used -> db [ 1 ]==' y ' &&
inline void setup_table_map ( TABLE * table , TABLE_LIST * table_list , uint tablenr ) <nl> table -> tablenr = tablenr ; <nl> table -> map = ( table_map ) 1 << tablenr ; <nl> table -> force_index = table_list -> force_index ; <nl> + table -> force_index_order = table -> force_index_group = 0 ; <nl> table -> covering_keys = table -> s -> keys_for_keyread ; <nl> table -> merge_keys . clear_all (); <nl> }
int <nl> Tablespace_client :: get_tablespace_info ( CreateFilegroupImplReq * rep ) <nl> { <nl> Ptr < Tsman :: Tablespace > ts_ptr ; <nl> - if ( m_tsman -> m_tablespace_hash . find ( ts_ptr , m_tablespace_id )); <nl> + if ( m_tsman -> m_tablespace_hash . find ( ts_ptr , m_tablespace_id )) <nl> { <nl> rep -> tablespace . extent_size = ts_ptr . p -> m_extent_size ; <nl> rep -> tablespace . logfile_group_id =
static bool check_user ( THD * thd , enum_server_command command , const char * user , <nl> send_error ( net , ER_OUT_OF_RESOURCES ); <nl> return 1 ; <nl> } <nl> - strcpy ( thd -> priv_host , LOCAL_HOST ); <nl> + strmake ( thd -> priv_host , LOCAL_HOST , sizeof ( thd -> priv_host )- 1 ); <nl> thd -> master_access = acl_getroot ( thd , thd -> host , thd -> ip , thd -> user , <nl> passwd , thd -> scramble , <nl> & thd -> priv_user , thd -> priv_host ,
static void trace_table_dependencies ( Opt_trace_context * trace , <nl> } <nl> Opt_trace_array depends_on ( trace , " depends_on_map_bits "); <nl> // RAND_TABLE_BIT may be in join_tabs [ i ]. dependent , so we test all 64 bits <nl> - compile_time_assert ( sizeof ( TABLE :: map ) <= 64 ); <nl> + compile_time_assert ( sizeof ( table -> map ) <= 64 ); <nl> for ( uint j = 0 ; j < 64 ; j ++) <nl> { <nl> if ( join_tabs [ i ]. dependent & ( 1ULL << j ))
MgmtSrvr :: dumpStateSelf ( const Uint32 args [], Uint32 no ) <nl> if ( no < 1 ) <nl> return - 1 ; <nl>  <nl> +# ifdef ERROR_INSERT <nl> switch ( args [ 0 ]) <nl> { <nl> case 9994 : <nl> MgmtSrvr :: dumpStateSelf ( const Uint32 args [], Uint32 no ) <nl> break ; <nl> } <nl>  <nl> -# ifdef ERROR_INSERT <nl> case 9996 : <nl> { <nl> /* Sendbuffer consumption */
int mysql_create_function ( THD * thd , udf_func * udf ) <nl> } <nl>  <nl> rw_wrlock (& THR_LOCK_udf ); <nl> - if (( hash_search (& udf_hash ,( byte *) & udf -> name . str , udf -> name . length ))) <nl> + if (( hash_search (& udf_hash ,( byte *) udf -> name . str , udf -> name . length ))) <nl> { <nl> net_printf ( thd , ER_UDF_EXISTS , udf -> name ); <nl> goto err ;
ha_innobase :: add_index ( <nl> innodb_table = indexed_table <nl> = dict_table_get ( prebuilt -> table -> name , FALSE ); <nl>  <nl> + if ( UNIV_UNLIKELY (! innodb_table )) { <nl> + error = HA_ERR_NO_SUCH_TABLE ; <nl> + goto err_exit ; <nl> + } <nl> + <nl> /* Check if the index name is reserved . */ <nl> if ( innobase_index_name_is_reserved ( trx , key_info , num_of_keys )) { <nl> error = - 1 ;
int TLSX_ValidateEllipticCurves ( CYASSL * ssl , byte first , byte second ) { <nl> int sig = 0 ; /* valitade signature */ <nl> int key = 0 ; /* validate key */ <nl>  <nl> + ( void ) oid ; <nl> + ( void ) octets ; <nl> + <nl> if (! extension ) <nl> return 1 ; /* no suite restriction */ <nl> 
void wolfSSL_X509_STORE_CTX_set_time ( WOLFSSL_X509_STORE_CTX * ctx , <nl> { <nl> ( void ) flags ; <nl>  <nl> + if ( ctx == NULL ) <nl> + return ; <nl> + <nl> ctx -> param -> check_time = t ; <nl> ctx -> param -> flags |= WOLFSSL_USE_CHECK_TIME ; <nl> }
void AesCtrEncrypt ( Aes * aes , byte * out , const byte * in , word32 sz ) <nl> word32 blocks = sz / AES_BLOCK_SIZE ; <nl>  <nl> while ( blocks --) { <nl> - AesEncrypt ( aes , aes -> reg , out ); <nl> + AesEncrypt ( aes , ( byte *) aes -> reg , out ); <nl> IncrementAesCounter (( byte *) aes -> reg ); <nl> xorbuf ( out , in , AES_BLOCK_SIZE ); <nl> 
static int DoCertificate ( CYASSL * ssl , byte * input , word32 * inOutIdx , <nl> ret = KEYUSE_ENCIPHER_E ; <nl> } <nl> if (( ssl -> specs . sig_algo == rsa_sa_algo || <nl> - ssl -> specs . sig_algo == ecc_dsa_sa_algo ) && <nl> + ( ssl -> specs . sig_algo == ecc_dsa_sa_algo && <nl> + ! ssl -> specs . static_ecdh )) && <nl> ( dCert . extKeyUsage & KEYUSE_DIGITAL_SIG ) == 0 ) { <nl> CYASSL_MSG (" KeyUse Digital Sig not set "); <nl> ret = KEYUSE_SIGNATURE_E ;
int wolfSSL_EVP_MD_type ( const WOLFSSL_EVP_MD * md ) <nl> return WOLFSSL_FAILURE ; <nl> # endif <nl> } <nl> - <nl> +# ifdef SESSION_CERTS <nl> + ssl -> session . chain . count = 0 ; <nl> +# endif <nl> # ifdef KEEP_PEER_CERT <nl> FreeX509 (& ssl -> peerCert ); <nl> InitX509 (& ssl -> peerCert , 0 , ssl -> heap );
void InitX509Name ( WOLFSSL_X509_NAME * name , int dynamicFlag ) <nl> # ifdef OPENSSL_EXTRA <nl> XMEMSET (& name -> fullName , 0 , sizeof ( DecodedName )); <nl> XMEMSET (& name -> cnEntry , 0 , sizeof ( WOLFSSL_X509_NAME_ENTRY )); <nl> + XMEMSET (& name -> extra , 0 , sizeof ( name -> extra )); <nl> name -> cnEntry . value = &( name -> cnEntry . data ); /* point to internal data */ <nl> name -> cnEntry . nid = ASN_COMMON_NAME ; <nl> name -> x509 = NULL ;
static int DecodePolicyOID ( char * out , word32 outSz , byte * in , word32 inSz ) <nl> # endif <nl> } <nl> idx += policy_length ; <nl> - } while (( int ) idx < total_length && cert -> extCertPoliciesNb < MAX_CERTPOL_NB ); <nl> + } while (( int ) idx < total_length <nl> + # if defined ( WOLFSSL_CERT_EXT ) <nl> + && cert -> extCertPoliciesNb < MAX_CERTPOL_NB <nl> + # endif <nl> + ); <nl>  <nl> WOLFSSL_LEAVE (" DecodeCertPolicy ", 0 ); <nl> return 0 ;
int activate_fd ( int irq , int fd , int type , void * dev_id ) <nl> . events = events , <nl> . current_events = 0 } ); <nl>  <nl> + err = - EBUSY ; <nl> spin_lock_irqsave (& irq_lock , flags ); <nl> for ( irq_fd = active_fds ; irq_fd != NULL ; irq_fd = irq_fd -> next ) { <nl> if (( irq_fd -> fd == fd ) && ( irq_fd -> type == type )) {
static int btree_readpage_end_io_hook ( struct page * page , u64 start , u64 end , <nl> goto err ; <nl> } <nl> found_level = btrfs_header_level ( eb ); <nl> + if ( found_level >= BTRFS_MAX_LEVEL ) { <nl> + btrfs_info ( root -> fs_info , " bad tree block level % d \ n ", <nl> + ( int ) btrfs_header_level ( eb )); <nl> + ret = - EIO ; <nl> + goto err ; <nl> + } <nl>  <nl> btrfs_set_buffer_lockdep_class ( btrfs_header_owner ( eb ), <nl> eb , found_level );
static void mga_connector_destroy ( struct drm_connector * connector ) <nl> kfree ( connector ); <nl> } <nl>  <nl> - struct drm_connector_helper_funcs mga_vga_connector_helper_funcs = { <nl> + static const struct drm_connector_helper_funcs mga_vga_connector_helper_funcs = { <nl> . get_modes = mga_vga_get_modes , <nl> . mode_valid = mga_vga_mode_valid , <nl> . best_encoder = mga_connector_best_encoder , <nl> }; <nl>  <nl> - struct drm_connector_funcs mga_vga_connector_funcs = { <nl> + static const struct drm_connector_funcs mga_vga_connector_funcs = { <nl> . dpms = drm_helper_connector_dpms , <nl> . detect = mga_vga_detect , <nl> . fill_modes = drm_helper_probe_single_connector_modes ,
static int tpm_binary_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 5th : delimiter */ <nl> seq_putc ( m , '\ 0 '); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl>  <nl> static int tpm_ascii_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 4th : eventname <= max + \' 0 ' delimiter */ <nl> seq_printf ( m , " % s \ n ", eventname ); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl> 
int drm_fb_helper_init ( struct drm_device * dev , <nl> struct drm_crtc * crtc ; <nl> int i ; <nl>  <nl> + if (! max_conn_count ) <nl> + return - EINVAL ; <nl> + <nl> fb_helper -> dev = dev ; <nl>  <nl> INIT_LIST_HEAD (& fb_helper -> kernel_fb_list );
static int pci_dio_auto_attach ( struct comedi_device * dev , <nl> else <nl> dev -> iobase = pci_resource_start ( pcidev , 2 ); <nl>  <nl> + pci_dio_reset ( dev ); <nl> + <nl> ret = comedi_alloc_subdevices ( dev , board -> nsubdevs ); <nl> if ( ret ) <nl> return ret ; <nl> static int pci_dio_auto_attach ( struct comedi_device * dev , <nl> comedi_8254_subdevice_init ( s , dev -> pacer ); <nl> } <nl>  <nl> - pci_dio_reset ( dev ); <nl> - <nl> return 0 ; <nl> } <nl> 
static ssize_t xenbus_file_write ( struct file * filp , <nl> goto out ; <nl>  <nl> /* Can ' t write a xenbus message larger we can buffer */ <nl> - if (( len + u -> len ) > sizeof ( u -> u . buffer )) { <nl> + if ( len > sizeof ( u -> u . buffer ) - u -> len ) { <nl> /* On error , dump existing buffer */ <nl> u -> len = 0 ; <nl> rc = - EINVAL ;
void ath6kl_destroy ( struct net_device * dev , unsigned int unregister ) <nl>  <nl> wlan_node_table_cleanup (& ar -> scan_table ); <nl>  <nl> + kfree ( ar -> fw_board ); <nl> + kfree ( ar -> fw_otp ); <nl> + kfree ( ar -> fw ); <nl> + kfree ( ar -> fw_patch ); <nl> + <nl> ath6kl_cfg80211_deinit ( ar ); <nl> }
static int xemaclite_send ( struct sk_buff * orig_skb , struct net_device * dev ) <nl> skb_tx_timestamp ( new_skb ); <nl>  <nl> dev -> stats . tx_bytes += len ; <nl> - dev_kfree_skb ( new_skb ); <nl> + dev_consume_skb_any ( new_skb ); <nl>  <nl> return 0 ; <nl> }
static void xiic_fill_tx_fifo ( struct xiic_i2c * i2c ) <nl> /* last message in transfer -> STOP */ <nl> data |= XIIC_TX_DYN_STOP_MASK ; <nl> dev_dbg ( i2c -> adap . dev . parent , "% s TX STOP \ n ", __func__ ); <nl> - <nl> - xiic_setreg16 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> - } else <nl> - xiic_setreg8 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> + } <nl> + xiic_setreg16 ( i2c , XIIC_DTR_REG_OFFSET , data ); <nl> } <nl> } <nl> 
accept_err : <nl> return result ; <nl> } <nl>  <nl> - int sctp_accept_from_sock ( struct connection * con ) <nl> + static int sctp_accept_from_sock ( struct connection * con ) <nl> { <nl> /* Check that the new node is in the lockspace */ <nl> struct sctp_prim prim ;
int __init init_dmars ( void ) <nl> deferred_flush = kzalloc ( g_num_of_iommus * <nl> sizeof ( struct deferred_flush_tables ), GFP_KERNEL ); <nl> if (! deferred_flush ) { <nl> - kfree ( g_iommus ); <nl> ret = - ENOMEM ; <nl> goto error ; <nl> }
static struct usb_function * geth_alloc ( struct usb_function_instance * fi ) <nl> sizeof ( geth -> ethaddr )); <nl> if ( status < 12 ) { <nl> kfree ( geth ); <nl> + mutex_unlock (& opts -> lock ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> geth_string_defs [ 1 ]. s = geth -> ethaddr ;
flush_signal_handlers ( struct task_struct * t , int force_default ) <nl> if ( force_default || ka -> sa . sa_handler != SIG_IGN ) <nl> ka -> sa . sa_handler = SIG_DFL ; <nl> ka -> sa . sa_flags = 0 ; <nl> +# ifdef SA_RESTORER <nl> + ka -> sa . sa_restorer = NULL ; <nl> +# endif <nl> sigemptyset (& ka -> sa . sa_mask ); <nl> ka ++; <nl> }
static const struct file_operations bm_register_operations = { <nl> static ssize_t <nl> bm_status_read ( struct file * file , char __user * buf , size_t nbytes , loff_t * ppos ) <nl> { <nl> - char * s = enabled ? " enabled " : " disabled "; <nl> + char * s = enabled ? " enabled \ n " : " disabled \ n "; <nl>  <nl> return simple_read_from_buffer ( buf , nbytes , ppos , s , strlen ( s )); <nl> }
static int skfp_ioctl ( struct net_device * dev , struct ifreq * rq , int cmd ) <nl> break ; <nl> case SKFP_CLR_STATS : /* Zero out the driver statistics */ <nl> if (! capable ( CAP_NET_ADMIN )) { <nl> - memset (& lp -> MacStat , 0 , sizeof ( lp -> MacStat )); <nl> - } else { <nl> status = - EPERM ; <nl> + } else { <nl> + memset (& lp -> MacStat , 0 , sizeof ( lp -> MacStat )); <nl> } <nl> break ; <nl> default :
static void switched_to_dl ( struct rq * rq , struct task_struct * p ) <nl> if ( unlikely ( p -> dl . dl_throttled )) <nl> return ; <nl>  <nl> - if ( p -> on_rq || rq -> curr != p ) { <nl> + if ( p -> on_rq && rq -> curr != p ) { <nl> # ifdef CONFIG_SMP <nl> if ( rq -> dl . overloaded && push_dl_task ( rq ) && rq != task_rq ( p )) <nl> /* Only reschedule if pushing failed */
static void ati_remote_input_report ( struct urb * urb , struct pt_regs * regs ) <nl> input_regs ( dev , regs ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 1 ); <nl> + input_sync ( dev ); <nl> input_event ( dev , ati_remote_tbl [ index ]. type , <nl> ati_remote_tbl [ index ]. code , 0 ); <nl> input_sync ( dev );
static int __init fcoe_init ( void ) <nl> /* Setup link change notification */ <nl> fcoe_dev_setup (); <nl>  <nl> - fcoe_if_init (); <nl> + rc = fcoe_if_init (); <nl> + if ( rc ) <nl> + goto out_free ; <nl>  <nl> return 0 ; <nl> 
static int mptsas_smp_handler ( struct Scsi_Host * shost , struct sas_rphy * rphy , <nl> smprep = ( SmpPassthroughReply_t *) ioc -> sas_mgmt . reply ; <nl> memcpy ( req -> sense , smprep , sizeof (* smprep )); <nl> req -> sense_len = sizeof (* smprep ); <nl> + req -> data_len = 0 ; <nl> + rsp -> data_len -= smprep -> ResponseDataLength ; <nl> } else { <nl> printk ( MYIOC_s_ERR_FMT "% s : smp passthru reply failed to be returned \ n ", <nl> ioc -> name , __FUNCTION__ );
static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
struct page * kmap_to_page ( void * vaddr ) <nl>  <nl> return virt_to_page ( addr ); <nl> } <nl> + EXPORT_SYMBOL ( kmap_to_page ); <nl>  <nl> static void flush_all_zero_pkmaps ( void ) <nl> {
static ssize_t auerchar_write ( struct file * file , const char __user * buf , size_t <nl> int ret ; <nl> wait_queue_t wait ; <nl>  <nl> - dbg (" auerchar_write % d bytes ", len ); <nl> + dbg (" auerchar_write % zd bytes ", len ); <nl>  <nl> /* Error checking */ <nl> if (! ccp )
static int initialize_usbh1_port ( struct platform_device * pdev ) <nl>  <nl> mdelay ( 10 ); <nl>  <nl> - return mx51_initialize_usb_hw ( 0 , MXC_EHCI_ITC_NO_THRESHOLD ); <nl> + return mx51_initialize_usb_hw ( pdev -> id , MXC_EHCI_ITC_NO_THRESHOLD ); <nl> } <nl>  <nl> static struct mxc_usbh_platform_data usbh1_config = {
void rtl92e_set_key ( struct net_device * dev , u8 EntryNo , u8 KeyIndex , <nl> } <nl> } <nl> priv -> rtllib -> is_set_key = true ; <nl> - if ( EntryNo >= TOTAL_CAM_ENTRY ) <nl> + if ( EntryNo >= TOTAL_CAM_ENTRY ) { <nl> netdev_info ( dev , "% s (): Invalid CAM entry \ n ", __func__ ); <nl> + return ; <nl> + } <nl>  <nl> RT_TRACE ( COMP_SEC , <nl> "====> to rtl92e_set_key (), dev :% p , EntryNo :% d , KeyIndex :% d , KeyType :% d , MacAddr % pM \ n ",
static void * arm64_swiotlb_alloc_coherent ( struct device * dev , size_t size , <nl> if ( IS_ENABLED ( CONFIG_DMA_CMA )) { <nl> struct page * page ; <nl>  <nl> + size = PAGE_ALIGN ( size ); <nl> page = dma_alloc_from_contiguous ( dev , size >> PAGE_SHIFT , <nl> get_order ( size )); <nl> if (! page )
static struct omap2_hsmmc_info mmc [] = { <nl> { <nl> . mmc = 1 , <nl> . caps = MMC_CAP_4_BIT_DATA | MMC_CAP_8_BIT_DATA , <nl> + . gpio_cd = - EINVAL , <nl> . gpio_wp = - EINVAL , <nl> }, <nl> {} /* Terminator */
static inline bool ipv4_is_local_multicast ( __be32 addr ) <nl> static inline bool ipv4_is_lbcast ( __be32 addr ) <nl> { <nl> /* limited broadcast */ <nl> - return addr == INADDR_BROADCAST ; <nl> + return addr == htonl ( INADDR_BROADCAST ); <nl> } <nl>  <nl> static inline bool ipv4_is_zeronet ( __be32 addr )
radeon_atom_encoder_mode_set ( struct drm_encoder * encoder , <nl>  <nl> radeon_encoder -> pixel_clock = adjusted_mode -> clock ; <nl>  <nl> - if ( ASIC_IS_AVIVO ( rdev )) { <nl> + if ( ASIC_IS_AVIVO ( rdev ) && ! ASIC_IS_DCE4 ( rdev )) { <nl> if ( radeon_encoder -> active_device & ( ATOM_DEVICE_CV_SUPPORT | ATOM_DEVICE_TV_SUPPORT )) <nl> atombios_yuv_setup ( encoder , true ); <nl> else
static void kvm_mmu_remove_some_alloc_mmu_pages ( struct kvm * kvm , <nl> { <nl> struct kvm_mmu_page * page ; <nl>  <nl> + if ( list_empty (& kvm -> arch . active_mmu_pages )) <nl> + return ; <nl> + <nl> page = container_of ( kvm -> arch . active_mmu_pages . prev , <nl> struct kvm_mmu_page , link ); <nl> kvm_mmu_prepare_zap_page ( kvm , page , invalid_list );
static int quota_setinfo ( struct super_block * sb , int type , void __user * addr ) <nl>  <nl> static void copy_to_if_dqblk ( struct if_dqblk * dst , struct fs_disk_quota * src ) <nl> { <nl> + memset ( dst , 0 , sizeof (* dst )); <nl> dst -> dqb_bhardlimit = src -> d_blk_hardlimit ; <nl> dst -> dqb_bsoftlimit = src -> d_blk_softlimit ; <nl> dst -> dqb_curspace = src -> d_bcount ;
static int palmas_gpio_probe ( struct platform_device * pdev ) <nl> const struct palmas_device_data * dev_data ; <nl>  <nl> match = of_match_device ( of_palmas_gpio_match , & pdev -> dev ); <nl> + if (! match ) <nl> + return - ENODEV ; <nl> dev_data = match -> data ; <nl> if (! dev_data ) <nl> dev_data = & palmas_dev_data ;
static int __init sm_it87_init ( void ) <nl>  <nl> static void __exit sm_it87_exit ( void ) <nl> { <nl> - i2c_isa_del_driver (& it87_isa_driver ); <nl> + if ( isa_address ) <nl> + i2c_isa_del_driver (& it87_isa_driver ); <nl> i2c_del_driver (& it87_driver ); <nl> } <nl> 
static struct drm_display_mode * drm_mode_detailed ( struct drm_device * dev , <nl> return NULL ; <nl> } <nl>  <nl> + /* Some EDIDs have bogus h / vtotal values */ <nl> + if ( mode -> hsync_end > mode -> htotal ) <nl> + mode -> htotal = mode -> hsync_end + 1 ; <nl> + if ( mode -> vsync_end > mode -> vtotal ) <nl> + mode -> vtotal = mode -> vsync_end + 1 ; <nl> + <nl> drm_mode_set_name ( mode ); <nl>  <nl> if ( pt -> misc & DRM_EDID_PT_INTERLACED )
con3270_init ( void ) <nl> return PTR_ERR ( rp ); <nl>  <nl> condev = kzalloc ( sizeof ( struct con3270 ), GFP_KERNEL | GFP_DMA ); <nl> + if (! condev ) <nl> + return - ENOMEM ; <nl> condev -> view . dev = rp ; <nl>  <nl> condev -> read = raw3270_request_alloc ( 0 );
static int iwl_read_ucode ( struct iwl_priv * priv ) <nl> priv -> ucode_data_backup . len = data_size ; <nl> iwl_alloc_fw_desc ( priv -> pci_dev , & priv -> ucode_data_backup ); <nl>  <nl> + if (! priv -> ucode_code . v_addr || ! priv -> ucode_data . v_addr || <nl> + ! priv -> ucode_data_backup . v_addr ) <nl> + goto err_pci_alloc ; <nl> + <nl> /* Initialization instructions and data */ <nl> if ( init_size && init_data_size ) { <nl> priv -> ucode_init . len = init_size ;
static void br_multicast_port_query_expired ( unsigned long data ) <nl> struct net_bridge * br = port -> br ; <nl>  <nl> spin_lock (& br -> multicast_lock ); <nl> - if ( port && ( port -> state == BR_STATE_DISABLED || <nl> - port -> state == BR_STATE_BLOCKING )) <nl> + if ( port -> state == BR_STATE_DISABLED || <nl> + port -> state == BR_STATE_BLOCKING ) <nl> goto out ; <nl>  <nl> if ( port -> multicast_startup_queries_sent <
int ext4_group_add_blocks ( handle_t * handle , struct super_block * sb , <nl>  <nl> ext4_debug (" Adding block ( s ) % llu -% llu \ n ", block , block + count - 1 ); <nl>  <nl> + if ( count == 0 ) <nl> + return 0 ; <nl> + <nl> ext4_get_group_no_and_offset ( sb , block , & block_group , & bit ); <nl> /* <nl> * Check to see if we are freeing blocks across a group
int regcache_sync_block ( struct regmap * map , void * block , <nl> unsigned int block_base , unsigned int start , <nl> unsigned int end ) <nl> { <nl> - if ( regmap_can_raw_write ( map )) <nl> + if ( regmap_can_raw_write ( map ) && ! map -> use_single_rw ) <nl> return regcache_sync_block_raw ( map , block , cache_present , <nl> block_base , start , end ); <nl> else
int rxrpc_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( copy > len - copied ) <nl> copy = len - copied ; <nl>  <nl> - if ( skb -> ip_summed == CHECKSUM_UNNECESSARY ) { <nl> + if ( skb -> ip_summed == CHECKSUM_UNNECESSARY || <nl> + skb -> ip_summed == CHECKSUM_PARTIAL ) { <nl> ret = skb_copy_datagram_iovec ( skb , offset , <nl> msg -> msg_iov , copy ); <nl> } else {
static void raise_dtr ( struct isi_port * port ) <nl> } <nl>  <nl> /* card -> lock HAS to be held */ <nl> - static inline void drop_dtr ( struct isi_port * port ) <nl> + static void drop_dtr ( struct isi_port * port ) <nl> { <nl> struct isi_board * card = port -> card ; <nl> unsigned long base = card -> base ;
static int lis3l02dq_read_raw ( struct iio_dev * indio_dev , <nl> ret = lis3l02dq_read_reg_s16 ( indio_dev , reg , val ); <nl> } <nl> mutex_unlock (& indio_dev -> mlock ); <nl> + if ( ret < 0 ) <nl> + goto error_ret ; <nl> return IIO_VAL_INT ; <nl> case IIO_CHAN_INFO_SCALE : <nl> * val = 0 ;
static int radeon_surface_free ( DRM_IOCTL_ARGS ) <nl> return DRM_ERR ( EINVAL ); <nl> } <nl>  <nl> - DRM_COPY_FROM_USER_IOCTL ( memfree , ( drm_radeon_mem_free_t __user *) data , <nl> + DRM_COPY_FROM_USER_IOCTL ( memfree , ( drm_radeon_surface_free_t __user *) data , <nl> sizeof ( memfree )); <nl>  <nl> if ( free_surface ( filp , dev_priv , memfree . address ))
static void incoming_packet ( struct sasem_context * context , <nl> } <nl>  <nl> if ( debug ) { <nl> - printk ( KERN_INFO " Incoming data : "); <nl> + pr_info (" Incoming data : "); <nl> for ( i = 0 ; i < 8 ; ++ i ) <nl> - printk ( KERN_CONT "% 02x ", buf [ i ]); <nl> - printk ( KERN_CONT "\ n "); <nl> + pr_cont ("% 02x ", buf [ i ]); <nl> + pr_cont ("\ n "); <nl> } <nl>  <nl> /*
i915_gem_object_finish_gpu ( struct drm_i915_gem_object * obj ) <nl> return ret ; <nl> } <nl>  <nl> + ret = i915_gem_object_wait_rendering ( obj ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* Ensure that we invalidate the GPU ' s caches and TLBs . */ <nl> obj -> base . read_domains &= ~ I915_GEM_GPU_DOMAINS ; <nl> - <nl> - return i915_gem_object_wait_rendering ( obj ); <nl> + return 0 ; <nl> } <nl>  <nl> /**
int dwc2_hcd_init ( struct dwc2_hsotg * hsotg , int irq , <nl> if (! hcd ) <nl> goto error1 ; <nl>  <nl> + if ( hsotg -> core_params -> dma_enable <= 0 ) <nl> + hcd -> self . uses_dma = 0 ; <nl> + <nl> hcd -> has_tt = 1 ; <nl>  <nl> spin_lock_init (& hsotg -> lock );
int __devinit mthca_init_eq_table ( struct mthca_dev * dev ) <nl> dev -> eq_table . clr_mask = <nl> swab32 ( 1 << ( dev -> eq_table . inta_pin & 31 )); <nl> dev -> eq_table . clr_int = dev -> clr_base + <nl> - ( dev -> eq_table . inta_pin < 31 ? 4 : 0 ); <nl> + ( dev -> eq_table . inta_pin < 32 ? 4 : 0 ); <nl> } <nl>  <nl> dev -> eq_table . arm_mask = 0 ;
static void zfcp_erp_rports_del ( struct zfcp_adapter * adapter ) <nl> { <nl> struct zfcp_port * port ; <nl> list_for_each_entry ( port , & adapter -> port_list_head , list ) { <nl> + if (! port -> rport ) <nl> + continue ; <nl> fc_remote_port_delete ( port -> rport ); <nl> port -> rport = NULL ; <nl> }
static int ds1374_probe ( struct i2c_adapter * adap , int addr , int kind ) <nl> client -> driver = & ds1374_driver ; <nl>  <nl> ds1374_workqueue = create_singlethread_workqueue (" ds1374 "); <nl> + if (! ds1374_workqueue ) { <nl> + kfree ( client ); <nl> + return - ENOMEM ; /* most expected reason */ <nl> + } <nl>  <nl> if (( rc = i2c_attach_client ( client )) != 0 ) { <nl> kfree ( client );
static int dw_mci_data_complete ( struct dw_mci * host , struct mmc_data * data ) <nl> data -> error = - EIO ; <nl> } <nl>  <nl> - dev_err ( host -> dev , " data error , status 0x % 08x \ n ", status ); <nl> + dev_dbg ( host -> dev , " data error , status 0x % 08x \ n ", status ); <nl>  <nl> /* <nl> * After an error , there may be data lingering
static struct platform_pwm_backlight_data zoom_backlight_data = { <nl> . max_brightness = 127 , <nl> . dft_brightness = 127 , <nl> . pwm_period_ns = 7812500 , <nl> + . enable_gpio = - 1 , <nl> }; <nl>  <nl> static struct platform_device zoom_backlight_pwm = {
static int __devinit twl_rtc_probe ( struct platform_device * pdev ) <nl> if ( ret < 0 ) <nl> goto out1 ; <nl>  <nl> - ret = request_irq ( irq , twl_rtc_interrupt , <nl> + ret = request_threaded_irq ( irq , NULL , twl_rtc_interrupt , <nl> IRQF_TRIGGER_RISING , <nl> dev_name (& rtc -> dev ), rtc ); <nl> if ( ret < 0 ) {
static LIST_HEAD ( pinctrl_maps ); <nl> list_for_each_entry ( _maps_node_ , & pinctrl_maps , node ) \ <nl> for ( _i_ = 0 , _map_ = & _maps_node_ -> maps [ _i_ ]; \ <nl> _i_ < _maps_node_ -> num_maps ; \ <nl> - i ++, _map_ = & _maps_node_ -> maps [ _i_ ]) <nl> + _i_ ++, _map_ = & _maps_node_ -> maps [ _i_ ]) <nl>  <nl> /** <nl> * pinctrl_provide_dummies () - indicate if pinctrl provides dummy state support
static void um_new_card ( DESCRIPTOR * d ) <nl> } else { <nl> DBG_ERR ((" could not create user mode idi card % d ", <nl> adapter_nr )); <nl> + diva_os_free ( 0 , card ); <nl> } <nl> } <nl> 
static long logger_ioctl ( struct file * file , unsigned int cmd , unsigned long arg ) <nl> ret = - EBADF ; <nl> break ; <nl> } <nl> + if (!( in_egroup_p ( file -> f_dentry -> d_inode -> i_gid ) || <nl> + capable ( CAP_SYSLOG ))) { <nl> + ret = - EPERM ; <nl> + break ; <nl> + } <nl> list_for_each_entry ( reader , & log -> readers , list ) <nl> reader -> r_off = log -> w_off ; <nl> log -> head = log -> w_off ;
qla24xx_chip_diag ( scsi_qla_host_t * ha ) <nl> /* Perform RISC reset . */ <nl> qla24xx_reset_risc ( ha ); <nl>  <nl> - ha -> fw_transfer_size = REQUEST_ENTRY_SIZE * 1024 ; <nl> + ha -> fw_transfer_size = REQUEST_ENTRY_SIZE * ha -> request_q_length ; <nl>  <nl> rval = qla2x00_mbx_reg_test ( ha ); <nl> if ( rval ) {
int of_dma_controller_register ( struct device_node * np , <nl> if (! nbcells ) { <nl> pr_err ("% s : # dma - cells property is missing or invalid \ n ", <nl> __func__ ); <nl> + kfree ( ofdma ); <nl> return - EINVAL ; <nl> } <nl> 
bnx2_init_5709_context ( struct bnx2 * bp ) <nl> for ( i = 0 ; i < bp -> ctx_pages ; i ++) { <nl> int j ; <nl>  <nl> + if ( bp -> ctx_blk [ i ]) <nl> + memset ( bp -> ctx_blk [ i ], 0 , BCM_PAGE_SIZE ); <nl> + else <nl> + return - ENOMEM ; <nl> + <nl> REG_WR ( bp , BNX2_CTX_HOST_PAGE_TBL_DATA0 , <nl> ( bp -> ctx_blk_mapping [ i ] & 0xffffffff ) | <nl> BNX2_CTX_HOST_PAGE_TBL_DATA0_VALID );
enum node_states { <nl> # else <nl> N_HIGH_MEMORY = N_NORMAL_MEMORY , <nl> # endif <nl> + N_MEMORY = N_HIGH_MEMORY , <nl> N_CPU , /* The node has one or more cpus */ <nl> NR_NODE_STATES <nl> };
static void clear_subscriber_list ( struct snd_seq_client * client , <nl> list_del (& subs -> dest_list ); <nl> else <nl> list_del (& subs -> src_list ); <nl> + up_write (& agrp -> list_mutex ); <nl> unsubscribe_port ( c , aport , agrp , & subs -> info , 1 ); <nl> kfree ( subs ); <nl> - up_write (& agrp -> list_mutex ); <nl> snd_seq_port_unlock ( aport ); <nl> snd_seq_client_unlock ( c ); <nl> }
static int pagemap_pte_range ( pmd_t * pmd , unsigned long addr , unsigned long end , <nl>  <nl> /* find the first VMA at or above ' addr ' */ <nl> vma = find_vma ( walk -> mm , addr ); <nl> - if ( pmd_trans_huge_lock ( pmd , vma ) == 1 ) { <nl> + if ( vma && pmd_trans_huge_lock ( pmd , vma ) == 1 ) { <nl> for (; addr != end ; addr += PAGE_SIZE ) { <nl> unsigned long offset ; <nl> 
setup_efi_state ( struct boot_params * params , unsigned long params_load_addr , <nl> if ( efi_enabled ( EFI_OLD_MEMMAP )) <nl> return 0 ; <nl>  <nl> + params -> secure_boot = boot_params . secure_boot ; <nl> ei -> efi_loader_signature = current_ei -> efi_loader_signature ; <nl> ei -> efi_systab = current_ei -> efi_systab ; <nl> ei -> efi_systab_hi = current_ei -> efi_systab_hi ;
ext3_set_acl ( handle_t * handle , struct inode * inode , int type , <nl> return error ; <nl> else { <nl> inode -> i_mode = mode ; <nl> + inode -> i_ctime = CURRENT_TIME_SEC ; <nl> ext3_mark_inode_dirty ( handle , inode ); <nl> if ( error == 0 ) <nl> acl = NULL ;
bool iwl_mvm_bt_coex_is_shared_ant_avail ( struct iwl_mvm * mvm ) <nl> if (!( mvm -> fw -> ucode_capa . api [ 0 ] & IWL_UCODE_TLV_API_BT_COEX_SPLIT )) <nl> return iwl_mvm_bt_coex_is_shared_ant_avail_old ( mvm ); <nl>  <nl> - return le32_to_cpu ( mvm -> last_bt_notif . bt_activity_grading ) == BT_OFF ; <nl> + return le32_to_cpu ( mvm -> last_bt_notif . bt_activity_grading ) < BT_HIGH_TRAFFIC ; <nl> } <nl>  <nl> bool iwl_mvm_bt_coex_is_tpc_allowed ( struct iwl_mvm * mvm ,
static void _ceph_msgr_exit ( void ) <nl> ceph_msgr_slab_exit (); <nl>  <nl> BUG_ON ( zero_page == NULL ); <nl> - kunmap ( zero_page ); <nl> page_cache_release ( zero_page ); <nl> zero_page = NULL ; <nl> }
ext4_move_extents ( struct file * o_filp , struct file * d_filp , <nl> orig_inode -> i_ino , donor_inode -> i_ino ); <nl> return - EINVAL ; <nl> } <nl> - <nl> + /* TODO : This is non obvious task to swap blocks for inodes with full <nl> + jornaling enabled */ <nl> + if ( ext4_should_journal_data ( orig_inode ) || <nl> + ext4_should_journal_data ( donor_inode )) { <nl> + return - EINVAL ; <nl> + } <nl> /* Protect orig and donor inodes against a truncate */ <nl> mext_inode_double_lock ( orig_inode , donor_inode ); <nl> 
static int cfg80211_netdev_notifier_call ( struct notifier_block * nb , <nl> kfree ( wdev -> wext . ie ); <nl> wdev -> wext . ie = NULL ; <nl> wdev -> wext . ie_len = 0 ; <nl> + wdev -> wext . connect . auth_type = NL80211_AUTHTYPE_AUTOMATIC ; <nl> # endif <nl> cfg80211_disconnect ( rdev , dev , <nl> WLAN_REASON_DEAUTH_LEAVING , true );
static unsigned long __meminit compute_pernodesize ( int node ) <nl> pernodesize += node * L1_CACHE_BYTES ; <nl> pernodesize += L1_CACHE_ALIGN ( sizeof ( pg_data_t )); <nl> pernodesize += L1_CACHE_ALIGN ( sizeof ( struct ia64_node_data )); <nl> + pernodesize += L1_CACHE_ALIGN ( sizeof ( pg_data_t )); <nl> pernodesize = PAGE_ALIGN ( pernodesize ); <nl> return pernodesize ; <nl> }
void eth_header_cache_update ( struct hh_cache * hh , struct net_device * dev , <nl> static int eth_mac_addr ( struct net_device * dev , void * p ) <nl> { <nl> struct sockaddr * addr = p ; <nl> + <nl> if ( netif_running ( dev )) <nl> return - EBUSY ; <nl> + if (! is_valid_ether_addr ( addr -> sa_data )) <nl> + return - EADDRNOTAVAIL ; <nl> memcpy ( dev -> dev_addr , addr -> sa_data , dev -> addr_len ); <nl> return 0 ; <nl> }
void arch_release_hugepage ( struct page * page ) <nl> ptep = ( pte_t *) page [ 1 ]. index ; <nl> if (! ptep ) <nl> return ; <nl> + clear_table (( unsigned long *) ptep , _PAGE_TYPE_EMPTY , <nl> + PTRS_PER_PTE * sizeof ( pte_t )); <nl> page_table_free (& init_mm , ( unsigned long *) ptep ); <nl> page [ 1 ]. index = 0 ; <nl> }
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl>  <nl> hw -> wiphy -> max_remain_on_channel_duration = 10000 ; <nl> hw -> max_listen_interval = IWL_CONN_MAX_LISTEN_INTERVAL ; <nl> + /* we can compensate an offset of up to 3 channels = 15 MHz */ <nl> + hw -> wiphy -> max_adj_channel_rssi_comp = 3 * 5 ; <nl>  <nl> /* Extract MAC address */ <nl> memcpy ( mvm -> addresses [ 0 ]. addr , mvm -> nvm_data -> hw_addr , ETH_ALEN );
static ssize_t cxlflash_show_port_status ( struct device * dev , <nl> u64 * fc_regs ; <nl>  <nl> rc = kstrtouint (( attr -> attr . name + 4 ), 10 , & port ); <nl> - if ( rc || ( port > NUM_FC_PORTS )) <nl> + if ( rc || ( port >= NUM_FC_PORTS )) <nl> return 0 ; <nl>  <nl> fc_regs = & afu -> afu_map -> global . fc_regs [ port ][ 0 ];
mext_check_arguments ( struct inode * orig_inode , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( IS_IMMUTABLE ( donor_inode ) || IS_APPEND ( donor_inode )) <nl> + return - EPERM ; <nl> + <nl> /* Ext4 move extent does not support swapfile */ <nl> if ( IS_SWAPFILE ( orig_inode ) || IS_SWAPFILE ( donor_inode )) { <nl> ext4_debug (" ext4 move extent : The argument files should "
static s32 igb_get_invariants_82575 ( struct e1000_hw * hw ) <nl> * for setting word_size . <nl> */ <nl> size += NVM_WORD_SIZE_BASE_SHIFT ; <nl> + <nl> + /* EEPROM access above 16k is unsupported */ <nl> + if ( size > 14 ) <nl> + size = 14 ; <nl> nvm -> word_size = 1 << size ; <nl>  <nl> /* setup PHY parameters */
static int wm9713_soc_probe ( struct snd_soc_codec * codec ) <nl> if ( IS_ERR ( wm9713 -> ac97 )) <nl> return PTR_ERR ( wm9713 -> ac97 ); <nl>  <nl> - regmap = devm_regmap_init_ac97 ( wm9713 -> ac97 , & wm9713_regmap_config ); <nl> + regmap = regmap_init_ac97 ( wm9713 -> ac97 , & wm9713_regmap_config ); <nl> if ( IS_ERR ( regmap )) { <nl> snd_soc_free_ac97_codec ( wm9713 -> ac97 ); <nl> return PTR_ERR ( regmap );
static int iommu_no_mapping ( struct pci_dev * pdev ) <nl> if ( pdev -> dma_mask > DMA_BIT_MASK ( 32 )) { <nl> int ret ; <nl> ret = domain_add_dev_info ( si_domain , pdev ); <nl> + if ( ret ) <nl> + return 0 ; <nl> + ret = domain_context_mapping ( si_domain , pdev , CONTEXT_TT_MULTI_LEVEL ); <nl> if (! ret ) { <nl> printk ( KERN_INFO " 64bit % s uses identity mapping \ n ", <nl> pci_name ( pdev ));
xfrm_state_find ( const xfrm_address_t * daddr , const xfrm_address_t * saddr , <nl> xfrm_state_look_at ( pol , x , fl , encap_family , <nl> & best , & acquire_in_progress , & error ); <nl> } <nl> - if ( best ) <nl> + if ( best || acquire_in_progress ) <nl> goto found ; <nl>  <nl> h_wildcard = xfrm_dst_hash ( net , daddr , & saddr_wildcard , tmpl -> reqid , encap_family );
static int mthca_alloc_qp_common ( struct mthca_dev * dev , <nl> int i ; <nl>  <nl> atomic_set (& qp -> refcount , 1 ); <nl> + init_waitqueue_head (& qp -> wait ); <nl> qp -> state = IB_QPS_RESET ; <nl> qp -> atomic_rd_en = 0 ; <nl> qp -> resp_depth = 0 ;
special_insn : <nl> case 0x88 ... 0x8b : /* mov */ <nl> goto mov ; <nl> case 0x8d : /* lea r16 / r32 , m */ <nl> - c -> dst . val = c -> modrm_val ; <nl> + c -> dst . val = c -> modrm_ea ; <nl> break ; <nl> case 0x8f : /* pop ( sole member of Grp1a ) */ <nl> rc = emulate_grp1a ( ctxt , ops );
static struct k_itimer * __lock_timer ( timer_t timer_id , unsigned long * flags ) <nl> { <nl> struct k_itimer * timr ; <nl>  <nl> + /* <nl> + * timer_t could be any type >= int and we want to make sure any <nl> + * @ timer_id outside positive int range fails lookup . <nl> + */ <nl> + if (( unsigned long long ) timer_id > INT_MAX ) <nl> + return NULL ; <nl> + <nl> rcu_read_lock (); <nl> timr = idr_find (& posix_timers_id , ( int ) timer_id ); <nl> if ( timr ) {
static void set_tracepoint ( struct tracepoint_entry ** entry , <nl> static void disable_tracepoint ( struct tracepoint * elem ) <nl> { <nl> elem -> state = 0 ; <nl> + rcu_assign_pointer ( elem -> funcs , NULL ); <nl> } <nl>  <nl> /**
static int __devinit gen_74x164_probe ( struct spi_device * spi ) <nl> } <nl>  <nl> chip -> gpio_chip . ngpio = GEN_74X164_NUMBER_GPIOS * chip -> registers ; <nl> - chip -> buffer = devm_kzalloc (& spi -> dev , chip -> gpio_chip . ngpio , GFP_KERNEL ); <nl> + chip -> buffer = devm_kzalloc (& spi -> dev , chip -> registers , GFP_KERNEL ); <nl> if (! chip -> buffer ) { <nl> ret = - ENOMEM ; <nl> goto exit_destroy ;
int usb_alloc_streams ( struct usb_interface * interface , <nl> return - EINVAL ; <nl> if ( dev -> speed != USB_SPEED_SUPER ) <nl> return - EINVAL ; <nl> + if ( dev -> state < USB_STATE_CONFIGURED ) <nl> + return - ENODEV ; <nl>  <nl> for ( i = 0 ; i < num_eps ; i ++) { <nl> /* Streams only apply to bulk endpoints . */
static int __devinit corgipm_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if (! machine_is_corgi () && ! machine_is_shepherd () <nl> + && ! machine_is_husky ()) <nl> + return - ENODEV ; <nl> + <nl> corgipm_device = platform_device_alloc (" sharpsl - pm ", - 1 ); <nl> if (! corgipm_device ) <nl> return - ENOMEM ;
static void reg_process_pending_hints ( void ) <nl> } <nl>  <nl> reg_process_hint ( reg_request ); <nl> + <nl> + lr = get_last_request (); <nl> + <nl> + spin_lock (& reg_requests_lock ); <nl> + if (! list_empty (& reg_requests_list ) && lr && lr -> processed ) <nl> + schedule_work (& reg_work ); <nl> + spin_unlock (& reg_requests_lock ); <nl> } <nl>  <nl> /* Processes beacon hints -- this has nothing to do with country IEs */
struct pcmcia_device * pcmcia_device_add ( struct pcmcia_socket * s , unsigned int f <nl> p_dev -> socket = s ; <nl> p_dev -> device_no = ( s -> device_count ++); <nl> p_dev -> func = function ; <nl> - if ( s -> functions < function ) <nl> - s -> functions = function ; <nl> + if ( s -> functions <= function ) <nl> + s -> functions = function + 1 ; <nl>  <nl> p_dev -> dev . bus = & pcmcia_bus_type ; <nl> p_dev -> dev . parent = s -> dev . dev ;
static DEFINE_PCI_DEVICE_TABLE ( rt2800pci_device_table ) = { <nl> { PCI_DEVICE ( 0x1814 , 0x5390 ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x5392 ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x539a ) }, <nl> + { PCI_DEVICE ( 0x1814 , 0x539b ) }, <nl> { PCI_DEVICE ( 0x1814 , 0x539f ) }, <nl> # endif <nl> { 0 , }
static int __init parse_memopt ( char * p ) <nl>  <nl> userdef = 1 ; <nl> mem_size = memparse ( p , & p ); <nl> + /* don ' t remove all of memory when handling " mem ={ invalid }" param */ <nl> + if ( mem_size == 0 ) <nl> + return - EINVAL ; <nl> e820_remove_range ( mem_size , ULLONG_MAX - mem_size , E820_RAM , 1 ); <nl>  <nl> return 0 ;
static enum blk_eh_timer_return mtip_cmd_timeout ( struct request * req , <nl> bool reserved ) <nl> { <nl> struct driver_data * dd = req -> q -> queuedata ; <nl> - int ret = BLK_EH_RESET_TIMER ; <nl>  <nl> if ( reserved ) <nl> goto exit_handler ; <nl> static enum blk_eh_timer_return mtip_cmd_timeout ( struct request * req , <nl>  <nl> wake_up_interruptible (& dd -> port -> svc_wait ); <nl> exit_handler : <nl> - return ret ; <nl> + return BLK_EH_RESET_TIMER ; <nl> } <nl>  <nl> static struct blk_mq_ops mtip_mq_ops = {
static int vpbe_display_g_register ( struct file * file , void * priv , <nl> struct v4l2_dbg_register * reg ) <nl> { <nl> struct v4l2_dbg_match * match = & reg -> match ; <nl> + struct vpbe_fh * fh = file -> private_data ; <nl> + struct vpbe_device * vpbe_dev = fh -> disp_dev -> vpbe_dev ; <nl>  <nl> if ( match -> type >= 2 ) { <nl> v4l2_subdev_call ( vpbe_dev -> venc ,
static void qeth_clear_output_buffer ( struct qeth_qdio_out_q * queue , <nl> buf -> buffer -> element [ i ]. addr = NULL ; <nl> buf -> buffer -> element [ i ]. flags = 0 ; <nl> } <nl> + buf -> buffer -> element [ 15 ]. flags = 0 ; <nl> buf -> next_element_to_fill = 0 ; <nl> atomic_set (& buf -> state , QETH_QDIO_BUF_EMPTY ); <nl> }
static int unqueue_me ( struct futex_q * q ) <nl> /* In the common case we don ' t take the spinlock , which is nice . */ <nl> retry : <nl> lock_ptr = q -> lock_ptr ; <nl> + barrier (); <nl> if ( lock_ptr != 0 ) { <nl> spin_lock ( lock_ptr ); <nl> /*
static void hpet_msi_capability_lookup ( unsigned int start_timer ) <nl> continue ; <nl>  <nl> irq = hpet_assign_irq ( hpet_domain , hdev , hdev -> num ); <nl> - if ( irq < 0 ) <nl> + if ( irq <= 0 ) <nl> continue ; <nl>  <nl> sprintf ( hdev -> name , " hpet % d ", i );
more : <nl> goto out_unlock ; <nl> } <nl> if (! d_unhashed ( dentry ) && dentry -> d_inode && <nl> + ceph_snap ( dentry -> d_inode ) != CEPH_SNAPDIR && <nl> filp -> f_pos <= di -> offset ) <nl> break ; <nl> dout (" skipping % p %.* s at % llu (% llu )% s % s \ n ", dentry ,
int wusbhc_chid_set ( struct wusbhc * wusbhc , const struct wusb_ckhdid * chid ) <nl> { <nl> int result = 0 ; <nl>  <nl> - if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof ( chid )) == 0 ) <nl> + if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof (* chid )) == 0 ) <nl> chid = NULL ; <nl>  <nl> mutex_lock (& wusbhc -> mutex );
xfs_file_last_byte ( <nl> * necessary . <nl> */ <nl> if ( ip -> i_df . if_flags & XFS_IFEXTENTS ) { <nl> + xfs_ilock ( ip , XFS_ILOCK_SHARED ); <nl> error = xfs_bmap_last_offset ( NULL , ip , & last_block , <nl> XFS_DATA_FORK ); <nl> + xfs_iunlock ( ip , XFS_ILOCK_SHARED ); <nl> if ( error ) { <nl> last_block = 0 ; <nl> }
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) { <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) { <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& priv -> lock , flags );
static ssize_t ath6kl_fwlog_block_read ( struct file * file , <nl>  <nl> ret = wait_for_completion_interruptible ( <nl> & ar -> debug . fwlog_completion ); <nl> - if ( ret == - ERESTARTSYS ) <nl> + if ( ret == - ERESTARTSYS ) { <nl> + vfree ( buf ); <nl> return ret ; <nl> + } <nl>  <nl> spin_lock (& ar -> debug . fwlog_queue . lock ); <nl> }
static unsigned int br_nf_post_routing ( unsigned int hook , struct sk_buff * skb , <nl> if (! nf_bridge ) <nl> return NF_ACCEPT ; <nl>  <nl> + if (!( nf_bridge -> mask & ( BRNF_BRIDGED | BRNF_BRIDGED_DNAT ))) <nl> + return NF_ACCEPT ; <nl> + <nl> if (! realoutdev ) <nl> return NF_DROP ; <nl> 
void __init ehv_pic_init ( void ) <nl>  <nl> if (! ehv_pic -> irqhost ) { <nl> of_node_put ( np ); <nl> + kfree ( ehv_pic ); <nl> return ; <nl> } <nl> 
static int do_pages_stat ( struct mm_struct * mm , unsigned long nr_pages , <nl> int err ; <nl>  <nl> for ( i = 0 ; i < nr_pages ; i += chunk_nr ) { <nl> - if ( chunk_nr + i > nr_pages ) <nl> + if ( chunk_nr > nr_pages - i ) <nl> chunk_nr = nr_pages - i ; <nl>  <nl> err = copy_from_user ( chunk_pages , & pages [ i ],
static inline int gred_change_vq ( struct Qdisc * sch , int dp , <nl> struct gred_sched_data * q ; <nl>  <nl> if ( table -> tab [ dp ] == NULL ) { <nl> - table -> tab [ dp ] = kzalloc ( sizeof (* q ), GFP_KERNEL ); <nl> + table -> tab [ dp ] = kzalloc ( sizeof (* q ), GFP_ATOMIC ); <nl> if ( table -> tab [ dp ] == NULL ) <nl> return - ENOMEM ; <nl> }
struct drm_mm_node * drm_mm_search_free_in_range ( const struct drm_mm * mm , <nl> wasted += alignment - tmp ; <nl> } <nl>  <nl> - if ( entry -> size >= size + wasted ) { <nl> + if ( entry -> size >= size + wasted && <nl> + ( entry -> start + wasted + size ) <= end ) { <nl> if (! best_match ) <nl> return entry ; <nl> if ( entry -> size < best_size ) {
static int fuse_rename ( struct inode * olddir , struct dentry * oldent , <nl> fuse_invalidate_attr ( newdir ); <nl>  <nl> /* newent will end up negative */ <nl> - if ( newent -> d_inode ) <nl> + if ( newent -> d_inode ) { <nl> + fuse_invalidate_attr ( newent -> d_inode ); <nl> fuse_invalidate_entry_cache ( newent ); <nl> + } <nl> } else if ( err == - EINTR ) { <nl> /* If request was interrupted , DEITY only knows if the <nl> rename actually took place . If the invalidation
static struct net_device * setup_pre_routing ( struct sk_buff * skb ) <nl> else if ( skb -> protocol == htons ( ETH_P_PPP_SES )) <nl> nf_bridge -> mask |= BRNF_PPPoE ; <nl>  <nl> + /* Must drop socket now because of tproxy . */ <nl> + skb_orphan ( skb ); <nl> return skb -> dev ; <nl> } <nl> 
static int usbduxsub_probe ( struct usb_interface * uinterf , <nl> usbduxsub [ index ]. dux_commands = kzalloc ( SIZEOFDUXBUFFER , GFP_KERNEL ); <nl> if (! usbduxsub [ index ]. dux_commands ) { <nl> dev_err ( dev , " comedi_ : usbdux : " <nl> - " error alloc space for dac commands \ n "); <nl> + " error alloc space for dux commands \ n "); <nl> tidy_up (&( usbduxsub [ index ])); <nl> up (& start_stop_sem ); <nl> return - ENOMEM ;
int radeon_vm_bo_update_pte ( struct radeon_device * rdev , <nl> return - ENOMEM ; <nl>  <nl> r = radeon_ib_get ( rdev , R600_RING_TYPE_DMA_INDEX , & ib , NULL , ndw * 4 ); <nl> + if ( r ) <nl> + return r ; <nl> ib . length_dw = 0 ; <nl>  <nl> r = radeon_vm_update_pdes ( rdev , vm , & ib , bo_va -> soffset , bo_va -> eoffset );
rcu_torture_init ( void ) <nl> writer_task = NULL ; <nl> goto unwind ; <nl> } <nl> - reader_tasks = kmalloc ( nrealreaders * sizeof ( reader_tasks [ 0 ]), <nl> + reader_tasks = kzalloc ( nrealreaders * sizeof ( reader_tasks [ 0 ]), <nl> GFP_KERNEL ); <nl> if ( reader_tasks == NULL ) { <nl> VERBOSE_PRINTK_ERRSTRING (" out of memory ");
int wm8350_device_init ( struct wm8350 * wm8350 , int irq , <nl> return ret ; <nl> } <nl>  <nl> - if ( pdata -> init ) { <nl> + if ( pdata && pdata -> init ) { <nl> ret = pdata -> init ( wm8350 ); <nl> if ( ret != 0 ) { <nl> dev_err ( wm8350 -> dev , " Platform init () failed : % d \ n ",
static inline int fls64 ( unsigned long x ) <nl> { <nl> unsigned long t , a , r ; <nl>  <nl> - t = __kernel_cmpbge ( x , 0x0101010101010101 ); <nl> + t = __kernel_cmpbge ( x , 0x0101010101010101UL ); <nl> a = __flsm1_tab [ t ]; <nl> t = __kernel_extbl ( x , a ); <nl> r = a * 8 + __flsm1_tab [ t ] + ( x != 0 );
intel_fill_fb_ggtt_view ( struct i915_ggtt_view * view , struct drm_framebuffer * fb , <nl> tile_size = intel_tile_size ( dev_priv ); <nl>  <nl> cpp = drm_format_plane_cpp ( fb -> pixel_format , 0 ); <nl> - tile_width = intel_tile_width ( dev_priv , cpp , fb -> modifier [ 0 ]); <nl> + tile_width = intel_tile_width ( dev_priv , fb -> modifier [ 0 ], cpp ); <nl> tile_height = tile_size / tile_width ; <nl>  <nl> info -> width_pages = DIV_ROUND_UP ( fb -> pitches [ 0 ], tile_width );
static void __init read_obp_translations ( void ) <nl> for ( i = 0 ; i < prom_trans_ents ; i ++) <nl> prom_trans [ i ]. data &= ~ 0x0003fe0000000000UL ; <nl> } <nl> + <nl> + /* Force execute bit on . */ <nl> + for ( i = 0 ; i < prom_trans_ents ; i ++) <nl> + prom_trans [ i ]. data |= ( tlb_type == hypervisor ? <nl> + _PAGE_EXEC_4V : _PAGE_EXEC_4U ); <nl> } <nl>  <nl> static void __init hypervisor_tlb_lock ( unsigned long vaddr ,
int sst_block_alloc_scratch ( struct sst_dsp * dsp ) <nl> ret = block_list_prepare ( dsp , & dsp -> scratch_block_list ); <nl> if ( ret < 0 ) { <nl> dev_err ( dsp -> dev , " error : scratch block prepare failed \ n "); <nl> + mutex_unlock (& dsp -> mutex ); <nl> return ret ; <nl> } <nl> 
bfa_ioc_mbox_isr ( struct bfa_ioc_s * ioc ) <nl> return ; <nl> } <nl>  <nl> - if (( mc > BFI_MC_MAX ) || ( mod -> mbhdlr [ mc ]. cbfn == NULL )) <nl> + if (( mc >= BFI_MC_MAX ) || ( mod -> mbhdlr [ mc ]. cbfn == NULL )) <nl> return ; <nl>  <nl> mod -> mbhdlr [ mc ]. cbfn ( mod -> mbhdlr [ mc ]. cbarg , & m );
bool ROUTEbRelay ( PSDevice pDevice , unsigned char * pbySkbData , unsigned int uData <nl> pHeadTD = pHeadTD -> next ; <nl> } <nl>  <nl> - pLastTD -> pTDInfo -> skb = 0 ; <nl> + pLastTD -> pTDInfo -> skb = NULL ; <nl> pLastTD -> pTDInfo -> byFlags = 0 ; <nl>  <nl> pDevice -> apCurrTD [ TYPE_AC0DMA ] = pHeadTD ;
static irqreturn_t pcf8563_irq ( int irq , void * dev_id ) <nl>  <nl> err = pcf8563_get_alarm_mode ( pcf8563 -> client , NULL , & pending ); <nl> if ( err ) <nl> - return err ; <nl> + return IRQ_NONE ; <nl>  <nl> if ( pending ) { <nl> rtc_update_irq ( pcf8563 -> rtc , 1 , RTC_IRQF | RTC_AF );
device_receive_frame ( <nl> } <nl>  <nl> ev . src_addr . sa_family = ARPHRD_ETHER ; <nl> - memcpy ( ev . src_addr . sa_data , pMACHeader -> abyAddr2 , ETH_ALEN ); <nl> + ether_addr_copy ( ev . src_addr . sa_data , <nl> + pMACHeader -> abyAddr2 ); <nl> memset (& wrqu , 0 , sizeof ( wrqu )); <nl> wrqu . data . length = sizeof ( ev ); <nl> wireless_send_event ( pDevice -> dev , IWEVMICHAELMICFAILURE , & wrqu , ( char *)& ev );
static int emac_dev_open ( struct net_device * ndev ) <nl> struct emac_priv * priv = netdev_priv ( ndev ); <nl>  <nl> netif_carrier_off ( ndev ); <nl> - for ( cnt = 0 ; cnt <= ETH_ALEN ; cnt ++) <nl> + for ( cnt = 0 ; cnt < ETH_ALEN ; cnt ++) <nl> ndev -> dev_addr [ cnt ] = priv -> mac_addr [ cnt ]; <nl>  <nl> /* Configuration items */
int mdiobus_register ( struct mii_bus * bus ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> + put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
*/ <nl> typedef union <nl> { <nl> - __u32 a4 ; <nl> - __u32 a6 [ 4 ]; <nl> + __be32 a4 ; <nl> + __be32 a6 [ 4 ]; <nl> } xfrm_address_t ; <nl>  <nl> /* Ident of a specific xfrm_state . It is used on input to lookup
# define CR8_RESERVED_BITS (~( unsigned long ) X86_CR8_TPR ) <nl>  <nl> # define KVM_MAX_MCE_BANKS 32 <nl> -# define KVM_MCE_CAP_SUPPORTED MCG_CTL_P <nl> +# define KVM_MCE_CAP_SUPPORTED ( MCG_CTL_P | MCG_SER_P ) <nl>  <nl> /* EFER defaults : <nl> * - enable syscall per default because its emulated by KVM
static ssize_t enable_store ( <nl> struct timed_output_dev * tdev = dev_get_drvdata ( dev ); <nl> int value ; <nl>  <nl> - sscanf ( buf , "% d ", & value ); <nl> + if ( sscanf ( buf , "% d ", & value ) != 1 ) <nl> + return - EINVAL ; <nl> + <nl> tdev -> enable ( tdev , value ); <nl>  <nl> return size ;
static void rtl8180_beacon_work ( struct work_struct * work ) <nl>  <nl> /* grab a fresh beacon */ <nl> skb = ieee80211_beacon_get ( dev , vif ); <nl> + if (! skb ) <nl> + goto resched ; <nl>  <nl> /* <nl> * update beacon timestamp w / TSF value
vhost_scsi_handle_vq ( struct vhost_scsi * vs , struct vhost_virtqueue * vq ) <nl> break ; <nl> } <nl>  <nl> + /* virtio - scsi spec requires byte 0 of the lun to be 1 */ <nl> + if ( unlikely ( v_req . lun [ 0 ] != 1 )) { <nl> + vhost_scsi_send_bad_target ( vs , vq , head , out ); <nl> + continue ; <nl> + } <nl> + <nl> /* Extract the tpgt */ <nl> target = v_req . lun [ 1 ]; <nl> tpg = ACCESS_ONCE ( vs_tpg [ target ]);
u64 perf_evsel__intval ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> value = *( u32 *) ptr ; <nl> break ; <nl> case 8 : <nl> - value = *( u64 *) ptr ; <nl> + memcpy (& value , ptr , sizeof ( u64 )); <nl> break ; <nl> default : <nl> return 0 ;
MODULE_DEVICE_TABLE ( pnp , smsc_ircc_pnp_table ); <nl> static int pnp_driver_registered ; <nl>  <nl> # ifdef CONFIG_PNP <nl> - static int __init smsc_ircc_pnp_probe ( struct pnp_dev * dev , <nl> + static int __devinit smsc_ircc_pnp_probe ( struct pnp_dev * dev , <nl> const struct pnp_device_id * dev_id ) <nl> { <nl> unsigned int firbase , sirbase ;
skip_msix : <nl> } else <nl> ql_log ( ql_log_warn , vha , 0x0039 , <nl> " MSI - X ; Falling back - to INTa mode -- % d .\ n ", ret ); <nl> + <nl> + /* Skip INTx on ISP82xx . */ <nl> + if (! ha -> flags . msi_enabled && IS_QLA82XX ( ha )) <nl> + return QLA_FUNCTION_FAILED ; <nl> + <nl> skip_msi : <nl>  <nl> ret = request_irq ( ha -> pdev -> irq , ha -> isp_ops -> intr_handler ,
static int exynos_dsi_parse_dt ( struct exynos_dsi * dsi ) <nl>  <nl> ep = of_graph_get_next_endpoint ( node , NULL ); <nl> if (! ep ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl>  <nl> dsi -> bridge_node = of_graph_get_remote_port_parent ( ep ); <nl> if (! dsi -> bridge_node ) { <nl> - ret = - ENXIO ; <nl> + ret = - EINVAL ; <nl> goto end ; <nl> } <nl> end :
static __cpuinit int mce_device_create ( unsigned int cpu ) <nl> if (! mce_available (& boot_cpu_data )) <nl> return - EIO ; <nl>  <nl> - memset (& dev -> kobj , 0 , sizeof ( struct kobject )); <nl> + memset ( dev , 0 , sizeof ( struct device )); <nl> dev -> id = cpu ; <nl> dev -> bus = & mce_subsys ; <nl> 
do_last : <nl> goto exit ; <nl> } <nl>  <nl> + if ( IS_ERR ( nd -> intent . open . file )) { <nl> + mutex_unlock (& dir -> d_inode -> i_mutex ); <nl> + error = PTR_ERR ( nd -> intent . open . file ); <nl> + goto exit_dput ; <nl> + } <nl> + <nl> /* Negative dentry , just create the file */ <nl> if (! path . dentry -> d_inode ) { <nl> if (! IS_POSIXACL ( dir -> d_inode ))
int iwl_mvm_scan_request ( struct iwl_mvm * mvm , <nl> else <nl> cmd -> type = cpu_to_le32 ( SCAN_TYPE_FORCED ); <nl>  <nl> + /* <nl> + * TODO : This is a WA due to a bug in the FW AUX framework that does not <nl> + * properly handle time events that fail to be scheduled <nl> + */ <nl> + cmd -> type = cpu_to_le32 ( SCAN_TYPE_FORCED ); <nl> + <nl> cmd -> repeats = cpu_to_le32 ( 1 ); <nl>  <nl> /*
int mmc_send_if_cond ( struct mmc_host * host , u32 ocr ) <nl> static const u8 test_pattern = 0xAA ; <nl> u8 result_pattern ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( struct mmc_command )); <nl> + <nl> /* <nl> * To support SD 2 . 0 cards , we must always invoke SD_SEND_IF_COND <nl> * before SD_APP_OP_COND . This command will harmlessly fail for
static int sep_construct_dma_tables_from_lli ( <nl> table_data_size ); <nl>  <nl> /* If info entry is null - this is the first table built */ <nl> - if ( info_in_entry_ptr == NULL ) { <nl> + if ( info_in_entry_ptr == NULL || info_out_entry_ptr == NULL ) { <nl> /* Set the output parameters to physical addresses */ <nl> * lli_table_in_ptr = <nl> sep_shared_area_virt_to_bus ( sep , dma_in_lli_table_ptr );
u64 gfs2_ri_total ( struct gfs2_sbd * sdp ) <nl> for ( rgrps = 0 ;; rgrps ++) { <nl> loff_t pos = rgrps * sizeof ( struct gfs2_rindex ); <nl>  <nl> - if ( pos + sizeof ( struct gfs2_rindex ) >= i_size_read ( inode )) <nl> + if ( pos + sizeof ( struct gfs2_rindex ) > i_size_read ( inode )) <nl> break ; <nl> error = gfs2_internal_read ( ip , & ra_state , buf , & pos , <nl> sizeof ( struct gfs2_rindex ));
static void picolcd_debug_out_report ( struct picolcd_data * data , <nl> report -> id , raw_size ); <nl> hid_debug_event ( hdev , buff ); <nl> if ( raw_size + 5 > sizeof ( raw_data )) { <nl> + kfree ( buff ); <nl> hid_debug_event ( hdev , " TOO BIG \ n "); <nl> return ; <nl> } else {
pid_t pid_vnr ( struct pid * pid ); <nl> hlist_for_each_entry_rcu (( task ), pos___ , \ <nl> & pid -> tasks [ type ], pids [ type ]. node ) { <nl>  <nl> + /* <nl> + * Both old and new leaders may be attached to <nl> + * the same pid in the middle of de_thread (). <nl> + */ <nl> # define while_each_pid_task ( pid , type , task ) \ <nl> + if ( type == PIDTYPE_PID ) \ <nl> + break ; \ <nl> } \ <nl> } while ( 0 ) <nl> 
i830_dispatch_execbuffer ( struct intel_engine_cs * ring , <nl> */ <nl> intel_ring_emit ( ring , SRC_COPY_BLT_CMD | BLT_WRITE_RGBA ); <nl> intel_ring_emit ( ring , BLT_DEPTH_32 | BLT_ROP_SRC_COPY | 4096 ); <nl> - intel_ring_emit ( ring , DIV_ROUND_UP ( len , 4096 ) << 16 | 1024 ); <nl> + intel_ring_emit ( ring , DIV_ROUND_UP ( len , 4096 ) << 16 | 4096 ); <nl> intel_ring_emit ( ring , cs_offset ); <nl> intel_ring_emit ( ring , 4096 ); <nl> intel_ring_emit ( ring , offset );
retry : <nl> printk (" locked it .\ n "); <nl>  <nl> do_each_thread ( g , p ) { <nl> + /* <nl> + * It ' s not reliable to print a task ' s held locks <nl> + * if it ' s not sleeping ( or if it ' s not the current <nl> + * task ): <nl> + */ <nl> + if ( p -> state == TASK_RUNNING && p != current ) <nl> + continue ; <nl> if ( p -> lockdep_depth ) <nl> lockdep_print_held_locks ( p ); <nl> if (! unlock )
static int hih6130_probe ( struct i2c_client * client , <nl> hih6130 -> client = client ; <nl> mutex_init (& hih6130 -> lock ); <nl>  <nl> + if (! i2c_check_functionality ( client -> adapter , I2C_FUNC_SMBUS_QUICK )) <nl> + hih6130 -> write_length = 1 ; <nl> + <nl> hwmon_dev = devm_hwmon_device_register_with_groups ( dev , client -> name , <nl> hih6130 , <nl> hih6130_groups );
tvp514x_probe ( struct i2c_client * client , const struct i2c_device_id * id ) <nl> if ( ret < 0 ) { <nl> v4l2_err ( sd , "% s decoder driver failed to register !!\ n ", <nl> sd -> name ); <nl> - kfree ( decoder ); <nl> return ret ; <nl> } <nl> # endif
static int sbus_do_settimeofday ( struct timespec * tv ) <nl> static int set_rtc_mmss ( unsigned long secs ) <nl> { <nl> struct rtc_device * rtc = rtc_class_open (" rtc0 "); <nl> + int err = - 1 ; <nl>  <nl> - if ( rtc ) <nl> - return rtc_set_mmss ( rtc , secs ); <nl> + if ( rtc ) { <nl> + err = rtc_set_mmss ( rtc , secs ); <nl> + rtc_class_close ( rtc ); <nl> + } <nl>  <nl> - return - 1 ; <nl> + return err ; <nl> }
module_i2c_driver ( si2157_driver ); <nl> MODULE_DESCRIPTION (" Silicon Labs Si2157 / Si2158 silicon tuner driver "); <nl> MODULE_AUTHOR (" Antti Palosaari < crope @ iki . fi >"); <nl> MODULE_LICENSE (" GPL "); <nl> + MODULE_FIRMWARE ( SI2158_A20_FIRMWARE );
static int __devinit tg3_get_invariants ( struct tg3 * tp ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5717 && <nl> + ( tp -> pci_chip_rev_id != CHIPREV_ID_5717_A0 || <nl> + ( tp -> tg3_flags2 & TG3_FLG2_MII_SERDES ))) <nl> + return - ENOTSUPP ; <nl> + <nl> /* Initialize data / descriptor byte / word swapping . */ <nl> val = tr32 ( GRC_MODE ); <nl> val &= GRC_MODE_HOST_STACKUP ;
offset_store ( struct md_rdev * rdev , const char * buf , size_t len ) <nl> * can be sane */ <nl> return - EBUSY ; <nl> rdev -> data_offset = offset ; <nl> + rdev -> new_data_offset = offset ; <nl> return len ; <nl> } <nl> 
found : <nl>  <nl> static int openpromfs_readdir ( struct file * filp , void * dirent , filldir_t filldir ) <nl> { <nl> - struct inode * inode = filp -> f_dentry -> d_inode ; <nl> + struct inode * inode = filp -> f_path . dentry -> d_inode ; <nl> struct op_inode_info * oi = OP_I ( inode ); <nl> struct device_node * dp = oi -> u . node ; <nl> struct device_node * child ;
store_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
static inline struct device_node * nand_get_flash_node ( struct nand_chip * chip ) <nl>  <nl> static inline struct nand_chip * mtd_to_nand ( struct mtd_info * mtd ) <nl> { <nl> - return mtd -> priv ; <nl> + return container_of ( mtd , struct nand_chip , mtd ); <nl> } <nl>  <nl> static inline struct mtd_info * nand_to_mtd ( struct nand_chip * chip )
static int bcm2835_dma_terminate_all ( struct dma_chan * chan ) <nl> * c -> desc is NULL and exit .) <nl> */ <nl> if ( c -> desc ) { <nl> + bcm2835_dma_desc_free (& c -> desc -> vd ); <nl> c -> desc = NULL ; <nl> bcm2835_dma_abort ( c -> chan_base ); <nl> 
bool ai_deviceremoved ( struct si_pub * sih ) <nl>  <nl> sii = ( struct si_info *) sih ; <nl>  <nl> + if ( sii -> icbus -> hosttype != BCMA_HOSTTYPE_PCI ) <nl> + return false ; <nl> + <nl> pci_read_config_dword ( sii -> pcibus , PCI_VENDOR_ID , & w ); <nl> if (( w & 0xFFFF ) != PCI_VENDOR_ID_BROADCOM ) <nl> return true ;
ecryptfs_setxattr ( struct dentry * dentry , const char * name , const void * value , <nl> } <nl>  <nl> rc = vfs_setxattr ( lower_dentry , name , value , size , flags ); <nl> - if (! rc ) <nl> + if (! rc && dentry -> d_inode ) <nl> fsstack_copy_attr_all ( dentry -> d_inode , lower_dentry -> d_inode ); <nl> out : <nl> return rc ;
static int tun_chr_open ( struct inode * inode , struct file * file ) <nl> set_bit ( SOCK_EXTERNALLY_ALLOCATED , & tfile -> socket . flags ); <nl> INIT_LIST_HEAD (& tfile -> next ); <nl>  <nl> + sock_set_flag (& tfile -> sk , SOCK_ZEROCOPY ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int setup ( struct spi_device * spi ) <nl> if (( chip -> chip_select_num > 0 ) <nl> && ( chip -> chip_select_num <= spi -> master -> num_chipselect )) <nl> peripheral_request ( ssel [ spi -> master -> bus_num ] <nl> - [ chip -> chip_select_num - 1 ], DRV_NAME ); <nl> + [ chip -> chip_select_num - 1 ], spi -> modalias ); <nl>  <nl> cs_deactive ( drv_data , chip ); <nl> 
EXPORT_SYMBOL ( genphy_config_advert ); <nl> */ <nl> int genphy_setup_forced ( struct phy_device * phydev ) <nl> { <nl> - int ctl = BMCR_RESET ; <nl> + int ctl = 0 ; <nl>  <nl> phydev -> pause = phydev -> asym_pause = 0 ; <nl> 
void uf_send_pkt_to_encrypt ( struct work_struct * work ) <nl>  <nl> if ( pktBulkDataLength > 0 ) { <nl> pktBulkData = kmalloc ( pktBulkDataLength , GFP_KERNEL ); <nl> - memset ( pktBulkData , 0 , pktBulkDataLength ); <nl> } else { <nl> unifi_error ( priv , " uf_send_pkt_to_encrypt () : invalid buffer \ n "); <nl> return ;
bnad_cb_tx_resume ( struct bnad * bnad , struct bna_tcb * tcb ) <nl> static void <nl> bnad_cb_tx_cleanup ( struct bnad * bnad , struct bna_tcb * tcb ) <nl> { <nl> - struct bnad_unmap_q * unmap_q = tcb -> unmap_q ; <nl> + struct bnad_unmap_q * unmap_q ; <nl>  <nl> if (! tcb || (! tcb -> unmap_q )) <nl> return ; <nl>  <nl> + unmap_q = tcb -> unmap_q ; <nl> if (! unmap_q -> unmap_array ) <nl> return ; <nl> 
static int pch_phub_write_gbe_mac_addr ( struct pch_phub_reg * chip , u8 * data ) <nl> int retval ; <nl> int i ; <nl>  <nl> - if ( chip -> ioh_type == 1 ) /* EG20T */ <nl> + if (( chip -> ioh_type == 1 ) || ( chip -> ioh_type == 5 )) /* EG20T or ML7831 */ <nl> retval = pch_phub_gbe_serial_rom_conf ( chip ); <nl> else /* ML7223 */ <nl> retval = pch_phub_gbe_serial_rom_conf_mp ( chip );
void __init qe_ic_init ( struct device_node * node , unsigned int flags ) <nl> return ; <nl>  <nl> memset ( qe_ic , 0 , sizeof ( struct qe_ic )); <nl> - qe_ic -> of_node = node ? of_node_get ( node ) : NULL ; <nl> + qe_ic -> of_node = of_node_get ( node ); <nl>  <nl> qe_ic -> irqhost = irq_alloc_host ( IRQ_HOST_MAP_LINEAR , <nl> NR_QE_IC_INTS , & qe_ic_host_ops , 0 );
static void disable_lapic_nmi_watchdog ( void ) <nl> wrmsr ( MSR_P6_EVNTSEL0 , 0 , 0 ); <nl> break ; <nl> case 15 : <nl> - if ( boot_cpu_data . x86_model > 0x3 ) <nl> + if ( boot_cpu_data . x86_model > 0x4 ) <nl> break ; <nl>  <nl> wrmsr ( MSR_P4_IQ_CCCR0 , 0 , 0 ); <nl> void setup_apic_nmi_watchdog ( void ) <nl> setup_p6_watchdog (); <nl> break ; <nl> case 15 : <nl> - if ( boot_cpu_data . x86_model > 0x3 ) <nl> + if ( boot_cpu_data . x86_model > 0x4 ) <nl> return ; <nl>  <nl> if (! setup_p4_watchdog ())
 <nl> # define DRV_MODULE_NAME " bnx2 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 1 . 6 . 7 " <nl> -# define DRV_MODULE_RELDATE " October 10 , 2007 " <nl> +# define DRV_MODULE_VERSION " 1 . 6 . 8 " <nl> +# define DRV_MODULE_RELDATE " October 17 , 2007 " <nl>  <nl> # define RUN_AT ( x ) ( jiffies + ( x )) <nl> 
xfs_qm_reset_dqcounts ( <nl> */ <nl> xfs_dqcheck ( mp , ddq , id + j , type , XFS_QMOPT_DQREPAIR , <nl> " xfs_quotacheck "); <nl> + /* <nl> + * Reset type in case we are reusing group quota file for <nl> + * project quotas or vice versa <nl> + */ <nl> + ddq -> d_flags = type ; <nl> ddq -> d_bcount = 0 ; <nl> ddq -> d_icount = 0 ; <nl> ddq -> d_rtbcount = 0 ;
static void kfd_process_destroy_delayed ( struct rcu_head * rcu ) <nl> mmdrop ( p -> mm ); <nl>  <nl> work = ( struct kfd_process_release_work *) <nl> - kmalloc ( sizeof ( struct kfd_process_release_work ), GFP_KERNEL ); <nl> + kmalloc ( sizeof ( struct kfd_process_release_work ), GFP_ATOMIC ); <nl>  <nl> if ( work ) { <nl> INIT_WORK (( struct work_struct *) work , kfd_process_wq_release );
static int stmmac_init_phy ( struct net_device * dev ) <nl> interface ); <nl> } <nl>  <nl> - if ( IS_ERR ( phydev )) { <nl> + if ( IS_ERR_OR_NULL ( phydev )) { <nl> pr_err ("% s : Could not attach to PHY \ n ", dev -> name ); <nl> + if (! phydev ) <nl> + return - ENODEV ; <nl> + <nl> return PTR_ERR ( phydev ); <nl> } <nl> 
static void pic_update_irq ( struct kvm_pic * s ) <nl> pic_set_irq1 (& s -> pics [ 0 ], 2 , 0 ); <nl> } <nl> irq = pic_get_irq (& s -> pics [ 0 ]); <nl> - if ( irq >= 0 ) <nl> - pic_irq_request ( s -> kvm , 1 ); <nl> - else <nl> - pic_irq_request ( s -> kvm , 0 ); <nl> + pic_irq_request ( s -> kvm , irq >= 0 ); <nl> } <nl>  <nl> void kvm_pic_update_irq ( struct kvm_pic * s )
static void btrfs_submit_direct ( int rw , struct bio * bio , struct inode * inode , <nl> if (! skip_sum ) { <nl> dip -> csums = kmalloc ( sizeof ( u32 ) * bio -> bi_vcnt , GFP_NOFS ); <nl> if (! dip -> csums ) { <nl> + kfree ( dip ); <nl> ret = - ENOMEM ; <nl> goto free_ordered ; <nl> }
done : <nl> "", ( margins ) ? " with margins " : "", ( interlace ) ? <nl> " interlaced " : ""); <nl>  <nl> + memset (& cvt_mode , 0 , sizeof ( cvt_mode )); <nl> cvt_mode . xres = xres ; <nl> cvt_mode . yres = yres ; <nl> cvt_mode . refresh = ( refresh ) ? refresh : 60 ;
int drm_resctx ( struct inode * inode , struct file * filp , <nl> for ( i = 0 ; i < DRM_RESERVED_CONTEXTS ; i ++ ) { <nl> ctx . handle = i ; <nl> if ( copy_to_user ( & res . contexts [ i ], <nl> - & i , sizeof ( i ) ) ) <nl> + & ctx , sizeof ( ctx ) ) ) <nl> return - EFAULT ; <nl> } <nl> }
static struct snd_soc_codec_driver soc_codec_dev_wm8962 = { <nl> . remove = wm8962_remove , <nl> . resume = wm8962_resume , <nl> . set_bias_level = wm8962_set_bias_level , <nl> - . reg_cache_size = WM8962_MAX_REGISTER , <nl> + . reg_cache_size = WM8962_MAX_REGISTER + 1 , <nl> . reg_word_size = sizeof ( u16 ), <nl> . reg_cache_default = wm8962_reg , <nl> . volatile_register = wm8962_volatile_register ,
mv64xxx_of_config ( struct mv64xxx_i2c_data * drv_data , <nl> } <nl> tclk = clk_get_rate ( drv_data -> clk ); <nl>  <nl> - rc = of_property_read_u32 ( np , " clock - frequency ", & bus_freq ); <nl> - if ( rc ) <nl> + if ( of_property_read_u32 ( np , " clock - frequency ", & bus_freq )) <nl> bus_freq = 100000 ; /* 100kHz by default */ <nl>  <nl> if (! mv64xxx_find_baud_factors ( bus_freq , tclk ,
static int em_gio_probe ( struct platform_device * pdev ) <nl> gpio_chip -> request = em_gio_request ; <nl> gpio_chip -> free = em_gio_free ; <nl> gpio_chip -> label = name ; <nl> + gpio_chip -> dev = & pdev -> dev ; <nl> gpio_chip -> owner = THIS_MODULE ; <nl> gpio_chip -> base = pdata -> gpio_base ; <nl> gpio_chip -> ngpio = pdata -> number_of_pins ;
extern void integrator_secondary_startup ( void ); <nl> * control for which core is the next to come out of the secondary <nl> * boot " holding pen " <nl> */ <nl> - volatile int __initdata pen_release = - 1 ; <nl> - unsigned long __initdata phys_pen_release = 0 ; <nl> + volatile int __cpuinitdata pen_release = - 1 ; <nl> + unsigned long __cpuinitdata phys_pen_release = 0 ; <nl>  <nl> static DEFINE_SPINLOCK ( boot_lock ); <nl> 
# include " myri10ge_mcp . h " <nl> # include " myri10ge_mcp_gen_header . h " <nl>  <nl> -# define MYRI10GE_VERSION_STR " 1 . 0 . 0 " <nl> +# define MYRI10GE_VERSION_STR " 1 . 1 . 0 " <nl>  <nl> MODULE_DESCRIPTION (" Myricom 10G driver ( 10GbE )"); <nl> MODULE_AUTHOR (" Maintainer : help @ myri . com ");
struct ioat_dma_chan { <nl> struct delayed_work work ; <nl>  <nl> int pending ; <nl> - int dmacount ; <nl> - int desccount ; <nl> + u16 dmacount ; <nl> + u16 desccount ; <nl>  <nl> struct ioatdma_device * device ; <nl> struct dma_chan common ;
static int map_sg_data ( struct scsi_cmnd * cmd , <nl> sdev_printk ( KERN_ERR , cmd -> device , <nl> " Can ' t allocate memory " <nl> " for indirect table \ n "); <nl> + scsi_dma_unmap ( cmd ); <nl> return 0 ; <nl> } <nl> }
void read_persistent_clock ( struct timespec * ts ) <nl> year += 100 ; <nl>  <nl> ts -> tv_sec = mktime ( year , mon , day , hour , min , sec ); <nl> + ts -> tv_nsec = 0 ; <nl> } <nl>  <nl> 
static int ams_delta_led_remove ( struct platform_device * pdev ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < ARRAY_SIZE ( ams_delta_leds ); i --) <nl> + for ( i = 0 ; i < ARRAY_SIZE ( ams_delta_leds ); i ++) <nl> led_classdev_unregister (& ams_delta_leds [ i ]. cdev ); <nl>  <nl> return 0 ;
static int fb_deferred_io_mkwrite ( struct vm_area_struct * vma , <nl> deferred framebuffer IO . then if userspace touches a page <nl> again , we repeat the same scheme */ <nl>  <nl> + file_update_time ( vma -> vm_file ); <nl> + <nl> /* protect against the workqueue changing the page list */ <nl> mutex_lock (& fbdefio -> lock ); <nl> 
static void mt_feature_mapping ( struct hid_device * hdev , <nl> td -> is_buttonpad = true ; <nl>  <nl> break ; <nl> + case 0xff0000c5 : <nl> + /* Retrieve the Win8 blob once to enable some devices */ <nl> + if ( usage -> usage_index == 0 ) <nl> + mt_get_feature ( hdev , field -> report ); <nl> + break ; <nl> } <nl> } <nl> 
cifs_ucs2_bytes ( const __le16 * from , int maxbytes , <nl> int maxwords = maxbytes / 2 ; <nl> char tmp [ NLS_MAX_CHARSET_SIZE ]; <nl>  <nl> - for ( i = 0 ; from [ i ] && i < maxwords ; i ++) { <nl> + for ( i = 0 ; i < maxwords && from [ i ]; i ++) { <nl> charlen = codepage -> uni2char ( le16_to_cpu ( from [ i ]), tmp , <nl> NLS_MAX_CHARSET_SIZE ); <nl> if ( charlen > 0 )
struct unw_frame_info { <nl> struct unw_ireg { <nl> unsigned long * loc ; <nl> struct unw_ireg_nat { <nl> - long type : 3 ; /* enum unw_nat_type */ <nl> + unsigned long type : 3 ; /* enum unw_nat_type */ <nl> signed long off : 61 ; /* NaT word is at loc + nat . off */ <nl> } nat ; <nl> } r4 , r5 , r6 , r7 ;
int beiscsi_cmd_mccq_create ( struct beiscsi_hba * phba , <nl> spin_lock (& phba -> ctrl . mbox_lock ); <nl> ctrl = & phba -> ctrl ; <nl> wrb = wrb_from_mbox (& ctrl -> mbox_mem ); <nl> + memset ( wrb , 0 , sizeof (* wrb )); <nl> req = embedded_payload ( wrb ); <nl> ctxt = & req -> context ; <nl> 
struct event_format * trace_find_next_event ( struct pevent * pevent , <nl> { <nl> static int idx ; <nl>  <nl> - if (! pevent -> events ) <nl> + if (! pevent || ! pevent -> events ) <nl> return NULL ; <nl>  <nl> if (! event ) {
static int br_multicast_ipv6_rcv ( struct net_bridge * br , <nl> ip6h -> payload_len == 0 ) <nl> return 0 ; <nl>  <nl> - len = ntohs ( ip6h -> payload_len ); <nl> + len = ntohs ( ip6h -> payload_len ) + sizeof (* ip6h ); <nl> if ( skb -> len < len ) <nl> return - EINVAL ; <nl> 
mapping_unwind : <nl> mapping_error : <nl> if ( net_ratelimit ()) <nl> dev_warn (& hw -> pdev -> dev , "% s : tx mapping error \ n ", dev -> name ); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
asmlinkage long sys_ioprio_set ( int which , int who , int ioprio ) <nl> continue ; <nl> ret = set_task_ioprio ( p , ioprio ); <nl> if ( ret ) <nl> - break ; <nl> + goto free_uid ; <nl> } while_each_thread ( g , p ); <nl> - <nl> + free_uid : <nl> if ( who ) <nl> free_uid ( user ); <nl> break ;
static int iio_channel_read ( struct iio_channel * chan , int * val , int * val2 , <nl> if ( val2 == NULL ) <nl> val2 = & unused ; <nl>  <nl> + if (! iio_channel_has_info ( chan -> channel , info )) <nl> + return - EINVAL ; <nl> + <nl> if ( chan -> indio_dev -> info -> read_raw_multi ) { <nl> ret = chan -> indio_dev -> info -> read_raw_multi ( chan -> indio_dev , <nl> chan -> channel , INDIO_MAX_RAW_ELEMENTS ,
static void ar9003_hw_spur_mitigate_ofdm ( struct ath_hw * ah , <nl>  <nl> ar9003_hw_spur_ofdm_clear ( ah ); <nl>  <nl> - for ( i = 0 ; spurChansPtr [ i ] && i < 5 ; i ++) { <nl> + for ( i = 0 ; i < AR_EEPROM_MODAL_SPURS && spurChansPtr [ i ]; i ++) { <nl> freq_offset = FBIN2FREQ ( spurChansPtr [ i ], mode ) - synth_freq ; <nl> if ( abs ( freq_offset ) < range ) { <nl> ar9003_hw_spur_ofdm_work ( ah , chan , freq_offset );
u16 ieee80211_select_queue ( struct ieee80211_sub_if_data * sdata , <nl> return IEEE80211_AC_BE ; <nl> } <nl>  <nl> + if ( skb -> protocol == sdata -> control_port_protocol ) { <nl> + skb -> priority = 7 ; <nl> + return ieee80211_downgrade_queue ( sdata , skb ); <nl> + } <nl> + <nl> /* use the data classifier to determine what 802 . 1d tag the <nl> * data frame has */ <nl> rcu_read_lock ();
static int lmv_iocontrol ( unsigned int cmd , struct obd_export * exp , <nl> __u32 index ; <nl>  <nl> memcpy (& index , data -> ioc_inlbuf2 , sizeof ( __u32 )); <nl> - if (( index >= count )) <nl> + if ( index >= count ) <nl> return - ENODEV ; <nl>  <nl> if ( lmv -> tgts [ index ] == NULL ||
static void kvm_write_wall_clock ( struct kvm * kvm , gpa_t wall_clock ) <nl> */ <nl> getboottime (& boot ); <nl>  <nl> + if ( kvm -> arch . kvmclock_offset ) { <nl> + struct timespec ts = ns_to_timespec ( kvm -> arch . kvmclock_offset ); <nl> + boot = timespec_sub ( boot , ts ); <nl> + } <nl> wc . sec = boot . tv_sec ; <nl> wc . nsec = boot . tv_nsec ; <nl> wc . version = version ;
bnx2_test_loopback ( struct bnx2 * bp ) <nl>  <nl> pkt_size = 1514 ; <nl> skb = dev_alloc_skb ( pkt_size ); <nl> + if (! skb ) <nl> + return - ENOMEM ; <nl> packet = skb_put ( skb , pkt_size ); <nl> memcpy ( packet , bp -> mac_addr , 6 ); <nl> memset ( packet + 6 , 0x0 , 8 );
static void fsg_unbind ( struct usb_configuration * c , struct usb_function * f ) <nl> fsg_common_put ( common ); <nl> usb_free_descriptors ( fsg -> function . descriptors ); <nl> usb_free_descriptors ( fsg -> function . hs_descriptors ); <nl> + usb_free_descriptors ( fsg -> function . ss_descriptors ); <nl> kfree ( fsg ); <nl> } <nl> 
allocate_trace_buffer ( struct trace_array * tr , struct trace_buffer * buf , int size <nl>  <nl> rb_flags = trace_flags & TRACE_ITER_OVERWRITE ? RB_FL_OVERWRITE : 0 ; <nl>  <nl> + buf -> tr = tr ; <nl> + <nl> buf -> buffer = ring_buffer_alloc ( size , rb_flags ); <nl> if (! buf -> buffer ) <nl> return - ENOMEM ;
int snd_soc_dapm_set_endpoint ( struct snd_soc_codec * codec , <nl> list_for_each_entry ( w , & codec -> dapm_widgets , list ) { <nl> if (! strcmp ( w -> name , endpoint )) { <nl> w -> connected = status ; <nl> + return 0 ; <nl> } <nl> } <nl>  <nl> - return 0 ; <nl> + return - ENODEV ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_set_endpoint ); <nl> 
static int add_std_chmaps ( struct hda_codec * codec ) <nl> struct snd_pcm_chmap * chmap ; <nl> const struct snd_pcm_chmap_elem * elem ; <nl>  <nl> - if (! pcm || ! pcm -> pcm || pcm -> own_chmap || <nl> - ! hinfo -> substreams ) <nl> + if (! pcm -> pcm || pcm -> own_chmap || ! hinfo -> substreams ) <nl> continue ; <nl> elem = hinfo -> chmap ? hinfo -> chmap : snd_pcm_std_chmaps ; <nl> err = snd_pcm_add_chmap_ctls ( pcm -> pcm , str , elem ,
handle_transaction ( struct link_transaction * t ) <nl> struct subaction * sa ; <nl> int i ; <nl>  <nl> + if (! t -> request ) { <nl> + printf (" BUG in handle_transaction \ n "); <nl> + return ; <nl> + } <nl> + <nl> for ( i = 0 ; i < array_length ( protocol_decoders ); i ++) <nl> if ( protocol_decoders [ i ]. decode ( t )) <nl> break ;
static int eql_g_master_cfg ( struct net_device * dev , master_config_t __user * mcp ) <nl> equalizer_t * eql ; <nl> master_config_t mc ; <nl>  <nl> + memset (& mc , 0 , sizeof ( master_config_t )); <nl> + <nl> if ( eql_is_master ( dev )) { <nl> eql = netdev_priv ( dev ); <nl> mc . max_slaves = eql -> max_slaves ;
static cycle_t sb1250_hpt_read ( void ) <nl> } <nl>  <nl> struct clocksource bcm1250_clocksource = { <nl> - . name = " MIPS ", <nl> + . name = " bcm1250 - counter - 3 ", <nl> . rating = 200 , <nl> . read = sb1250_hpt_read , <nl> . mask = CLOCKSOURCE_MASK ( 23 ),
static int match_session ( struct cifs_ses * ses , struct smb_vol * vol ) <nl> vol -> username ? vol -> username : "", <nl> CIFS_MAX_USERNAME_LEN )) <nl> return 0 ; <nl> - if ( strlen ( vol -> username ) != 0 && <nl> + if (( vol -> username && strlen ( vol -> username ) != 0 ) && <nl> ses -> password != NULL && <nl> strncmp ( ses -> password , <nl> vol -> password ? vol -> password : "",
struct fsnotify_group * fsnotify_obtain_group ( unsigned int group_num , __u32 mask , <nl> struct fsnotify_group * group , * tgroup ; <nl>  <nl> /* very low use , simpler locking if we just always alloc */ <nl> - group = kmalloc ( sizeof ( struct fsnotify_group ), GFP_KERNEL ); <nl> + group = kzalloc ( sizeof ( struct fsnotify_group ), GFP_KERNEL ); <nl> if (! group ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
static void do_interrupt_requests ( struct kvm_vcpu * vcpu , <nl> vmx_update_window_states ( vcpu ); <nl>  <nl> if ( vcpu -> arch . nmi_pending && ! vcpu -> arch . nmi_injected ) { <nl> - if ( vcpu -> arch . nmi_window_open ) { <nl> + if ( vcpu -> arch . interrupt . pending ) { <nl> + enable_nmi_window ( vcpu ); <nl> + } else if ( vcpu -> arch . nmi_window_open ) { <nl> vcpu -> arch . nmi_pending = false ; <nl> vcpu -> arch . nmi_injected = true ; <nl> } else {
int tipc_node_get_linkname ( u32 bearer_id , u32 addr , char * linkname , size_t len ) <nl> struct tipc_link * link ; <nl> struct tipc_node * node = tipc_node_find ( addr ); <nl>  <nl> - if (( bearer_id > MAX_BEARERS ) || ! node ) <nl> + if (( bearer_id >= MAX_BEARERS ) || ! node ) <nl> return - EINVAL ; <nl> tipc_node_lock ( node ); <nl> link = node -> links [ bearer_id ];
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> */ <nl> if ( skb_shared ( skb )) { <nl> tmp_skb = skb ; <nl> - skb = skb_copy ( skb , GFP_ATOMIC ); <nl> + skb = skb_clone ( skb , GFP_ATOMIC ); <nl> kfree_skb ( tmp_skb ); <nl>  <nl> if (! skb ) {
static int egalax_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> struct egalax_data * td ; <nl> struct hid_report * report ; <nl>  <nl> - td = kmalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> + td = kzalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> if (! td ) { <nl> dev_err (& hdev -> dev , " cannot allocate eGalax data \ n "); <nl> return - ENOMEM ;
static int sdhci_esdhc_imx_probe ( struct platform_device * pdev ) <nl> * to something insane . Change it back here . <nl> */ <nl> if ( esdhc_is_usdhc ( imx_data )) { <nl> - writel ( 0x08100810 , host -> ioaddr + ESDHC_WTMK_LVL ); <nl> + writel ( 0x10401040 , host -> ioaddr + ESDHC_WTMK_LVL ); <nl> + <nl> host -> quirks2 |= SDHCI_QUIRK2_PRESET_VALUE_BROKEN ; <nl> host -> mmc -> caps |= MMC_CAP_1_8V_DDR ; <nl> 
int i915_vma_unbind ( struct i915_vma * vma ) <nl> * cause memory corruption through use - after - free . <nl> */ <nl>  <nl> + /* Throw away the active reference before moving to the unbound list */ <nl> + i915_gem_object_retire ( obj ); <nl> + <nl> if ( i915_is_ggtt ( vma -> vm )) { <nl> i915_gem_object_finish_gtt ( obj ); <nl> 
int dasd_eer_enable ( struct dasd_device * device ) <nl> cqr -> device = device ; <nl> cqr -> retries = 255 ; <nl> cqr -> expires = 10 * HZ ; <nl> + clear_bit ( DASD_CQR_FLAGS_USE_ERP , & cqr -> flags ); <nl>  <nl> cqr -> cpaddr -> cmd_code = DASD_ECKD_CCW_SNSS ; <nl> cqr -> cpaddr -> count = SNSS_DATA_SIZE ;
static int map_lookup_elem ( union bpf_attr * attr ) <nl> if ( copy_from_user ( key , ukey , map -> key_size ) != 0 ) <nl> goto free_key ; <nl>  <nl> - err = - ESRCH ; <nl> + err = - ENOENT ; <nl> rcu_read_lock (); <nl> value = map -> ops -> map_lookup_elem ( map , key ); <nl> if (! value )
static int mv643xx_eth_stop ( struct net_device * dev ) <nl> struct mv643xx_eth_private * mp = netdev_priv ( dev ); <nl> int i ; <nl>  <nl> + wrlp ( mp , INT_MASK_EXT , 0x00000000 ); <nl> wrlp ( mp , INT_MASK , 0x00000000 ); <nl> rdlp ( mp , INT_MASK ); <nl> 
static int __exit usba_udc_remove ( struct platform_device * pdev ) <nl> usba_ep_cleanup_debugfs (& usba_ep [ i ]); <nl> usba_cleanup_debugfs ( udc ); <nl>  <nl> - if ( gpio_is_valid ( udc -> vbus_pin )) <nl> + if ( gpio_is_valid ( udc -> vbus_pin )) { <nl> + free_irq ( gpio_to_irq ( udc -> vbus_pin ), udc ); <nl> gpio_free ( udc -> vbus_pin ); <nl> + } <nl>  <nl> free_irq ( udc -> irq , udc ); <nl> kfree ( usba_ep );
static void ath_ant_comb_scan ( struct ath_softc * sc , struct ath_rx_status * rs ) <nl> main_ant_conf = ( rs -> rs_rssi_ctl2 >> ATH_ANT_RX_MAIN_SHIFT ) & <nl> ATH_ANT_RX_MASK ; <nl>  <nl> - /* Record packet only when alt_rssi is positive */ <nl> - if ( alt_rssi > 0 ) { <nl> + /* Record packet only when both main_rssi and alt_rssi is positive */ <nl> + if ( main_rssi > 0 && alt_rssi > 0 ) { <nl> antcomb -> total_pkt_count ++; <nl> antcomb -> main_total_rssi += main_rssi ; <nl> antcomb -> alt_total_rssi += alt_rssi ;
static int irda_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> struct sock * sk = sock -> sk ; <nl> struct irda_sock * self = irda_sk ( sk ); <nl>  <nl> + memset (& saddr , 0 , sizeof ( saddr )); <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED ) <nl> return - ENOTCONN ;
static struct svc_xprt * svc_rdma_create ( struct svc_serv * serv , <nl> int ret ; <nl>  <nl> dprintk (" svcrdma : Creating RDMA socket \ n "); <nl> - <nl> + if ( sa -> sa_family != AF_INET ) { <nl> + dprintk (" svcrdma : Address family % d is not supported .\ n ", sa -> sa_family ); <nl> + return ERR_PTR (- EAFNOSUPPORT ); <nl> + } <nl> cma_xprt = rdma_create_xprt ( serv , 1 ); <nl> if (! cma_xprt ) <nl> return ERR_PTR (- ENOMEM );
static void collapse_huge_page ( struct mm_struct * mm , <nl> set_pmd_at ( mm , address , pmd , _pmd ); <nl> spin_unlock (& mm -> page_table_lock ); <nl> anon_vma_unlock ( vma -> anon_vma ); <nl> - mem_cgroup_uncharge_page ( new_page ); <nl> goto out ; <nl> } <nl>  <nl> out_up_write : <nl> return ; <nl>  <nl> out : <nl> + mem_cgroup_uncharge_page ( new_page ); <nl> # ifdef CONFIG_NUMA <nl> put_page ( new_page ); <nl> # endif
static int reiserfs_fill_super ( struct super_block * s , void * data , int silent ) <nl> if ( SB_AP_BITMAP ( s )) <nl> brelse ( SB_AP_BITMAP ( s )[ j ]. bh ); <nl> } <nl> - if ( SB_AP_BITMAP ( s )) <nl> - vfree ( SB_AP_BITMAP ( s )); <nl> + vfree ( SB_AP_BITMAP ( s )); <nl> } <nl> if ( SB_BUFFER_WITH_SB ( s )) <nl> brelse ( SB_BUFFER_WITH_SB ( s ));
static struct pci_controller cobalt_pci_controller = { <nl> . mem_resource = & cobalt_mem_resource , <nl> . io_resource = & cobalt_io_resource , <nl> . io_offset = 0 - GT_DEF_PCI0_IO_BASE , <nl> + . io_map_base = CKSEG1ADDR ( GT_DEF_PCI0_IO_BASE ), <nl> }; <nl>  <nl> static int __init cobalt_pci_init ( void )
static int __devinit cpmac_probe ( struct platform_device * pdev ) <nl> priv -> dev = dev ; <nl> priv -> ring_size = 64 ; <nl> priv -> msg_enable = netif_msg_init ( debug_level , 0xff ); <nl> - memcpy ( dev -> dev_addr , pdata -> dev_addr , sizeof ( dev -> dev_addr )); <nl> + memcpy ( dev -> dev_addr , pdata -> dev_addr , sizeof ( pdata -> dev_addr )); <nl>  <nl> snprintf ( priv -> phy_name , MII_BUS_ID_SIZE , PHY_ID_FMT , mdio_bus_id , phy_id ); <nl> 
static struct usb_request * isp1760_ep_alloc_request ( struct usb_ep * ep , <nl> struct isp1760_request * req ; <nl>  <nl> req = kzalloc ( sizeof (* req ), gfp_flags ); <nl> + if (! req ) <nl> + return NULL ; <nl>  <nl> return & req -> req ; <nl> }
int tcp_disconnect ( struct sock * sk , int flags ) <nl> tp -> snd_ssthresh = TCP_INFINITE_SSTHRESH ; <nl> tp -> snd_cwnd_cnt = 0 ; <nl> tp -> bytes_acked = 0 ; <nl> + tp -> window_clamp = 0 ; <nl> tcp_set_ca_state ( sk , TCP_CA_Open ); <nl> tcp_clear_retrans ( tp ); <nl> inet_csk_delack_init ( sk );
int inet6_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl> */ <nl> v4addr = LOOPBACK4_IPV6 ; <nl> if (!( addr_type & IPV6_ADDR_MULTICAST )) { <nl> - if (! inet -> transparent && <nl> + if (!( inet -> freebind || inet -> transparent ) && <nl> ! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> dev , 0 )) { <nl> err = - EADDRNOTAVAIL ;
__alloc_bootmem_core ( struct bootmem_data * bdata , unsigned long size , <nl> if ( limit && bdata -> node_boot_start >= limit ) <nl> return NULL ; <nl>  <nl> + /* on nodes without memory - bootmem_map is NULL */ <nl> + if (! bdata -> node_bootmem_map ) <nl> + return NULL ; <nl> + <nl> end_pfn = bdata -> node_low_pfn ; <nl> limit = PFN_DOWN ( limit ); <nl> if ( limit && end_pfn > limit )
# define VI6_DISP_IRQ_ENB 0x0078 <nl> # define VI6_DISP_IRQ_ENB_DSTE ( 1 << 8 ) <nl> # define VI6_DISP_IRQ_ENB_MAEE ( 1 << 5 ) <nl> -# define VI6_DISP_IRQ_ENB_LNEE ( n ) ( 1 << (( n ) + 4 )) <nl> +# define VI6_DISP_IRQ_ENB_LNEE ( n ) ( 1 << ( n )) <nl>  <nl> # define VI6_DISP_IRQ_STA 0x007c <nl> # define VI6_DISP_IRQ_STA_DSE ( 1 << 8 )
static int cgroupstats_user_cmd ( struct sk_buff * skb , struct genl_info * info ) <nl> na = nla_reserve ( rep_skb , CGROUPSTATS_TYPE_CGROUP_STATS , <nl> sizeof ( struct cgroupstats )); <nl> if ( na == NULL ) { <nl> + nlmsg_free ( rep_skb ); <nl> rc = - EMSGSIZE ; <nl> goto err ; <nl> }
static int atmel_prepare_rx_dma ( struct uart_port * port ) <nl> BUG_ON (! PAGE_ALIGNED ( ring -> buf )); <nl> sg_set_page (& atmel_port -> sg_rx , <nl> virt_to_page ( ring -> buf ), <nl> - ATMEL_SERIAL_RINGSIZE , <nl> + sizeof ( struct atmel_uart_char ) * ATMEL_SERIAL_RINGSIZE , <nl> ( int ) ring -> buf & ~ PAGE_MASK ); <nl> nent = dma_map_sg ( port -> dev , <nl> & atmel_port -> sg_rx ,
static unsigned long super_cache_scan ( struct shrinker * shrink , <nl> inodes = list_lru_count_node (& sb -> s_inode_lru , sc -> nid ); <nl> dentries = list_lru_count_node (& sb -> s_dentry_lru , sc -> nid ); <nl> total_objects = dentries + inodes + fs_objects + 1 ; <nl> + if (! total_objects ) <nl> + total_objects = 1 ; <nl>  <nl> /* proportion the scan between the caches */ <nl> dentries = mult_frac ( sc -> nr_to_scan , dentries , total_objects );
static inline pte_t pfn_pte ( unsigned long pfn , pgprot_t prot ) <nl> sz_bits = 0UL ; <nl> if ( _PAGE_SZBITS_4U != 0UL || _PAGE_SZBITS_4V != 0UL ) { <nl> __asm__ __volatile__ ( <nl> - "\ n661 : sethi % uhi (% 1 ), % 0 \ n " <nl> + "\ n661 : sethi %% uhi (% 1 ), % 0 \ n " <nl> " sllx % 0 , 32 , % 0 \ n " <nl> " . section . sun4v_2insn_patch , \" ax \"\ n " <nl> " . word 661b \ n "
static struct config_item * uvcg_frame_make ( struct config_group * group , <nl> h -> fmt_type = UVCG_MJPEG ; <nl> } else { <nl> mutex_unlock (& opts -> lock ); <nl> + kfree ( h ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> ++ fmt -> num_frames ;
static int iio_read_first_n_kfifo ( struct iio_buffer * r , <nl> int ret , copied ; <nl> struct iio_kfifo * kf = iio_to_kfifo ( r ); <nl>  <nl> - if ( n < r -> bytes_per_datum ) <nl> + if ( n < r -> bytes_per_datum || r -> bytes_per_datum == 0 ) <nl> return - EINVAL ; <nl>  <nl> ret = kfifo_to_user (& kf -> kf , buf , n , & copied ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> return copied ; <nl> }
nouveau_cli_destroy ( struct nouveau_cli * cli ) <nl> nvkm_vm_ref ( NULL , & nvxx_client (& cli -> base )-> vm , NULL ); <nl> nvif_client_fini (& cli -> base ); <nl> usif_client_fini ( cli ); <nl> + kfree ( cli ); <nl> } <nl>  <nl> static void
asmlinkage void math_emulate ( long arg ) <nl> entry_sel_off . offset = FPU_ORIG_EIP ; <nl> entry_sel_off . selector = FPU_CS ; <nl> entry_sel_off . opcode = ( byte1 << 8 ) | FPU_modrm ; <nl> + entry_sel_off . empty = 0 ; <nl>  <nl> FPU_rm = FPU_modrm & 7 ; <nl> 
void falcon_remove_nic ( struct efx_nic * efx ) <nl> struct falcon_nic_data * nic_data = efx -> nic_data ; <nl> int rc ; <nl>  <nl> + /* Remove I2C adapter and clear it in preparation for a retry */ <nl> rc = i2c_del_adapter (& efx -> i2c_adap ); <nl> BUG_ON ( rc ); <nl> + memset (& efx -> i2c_adap , 0 , sizeof ( efx -> i2c_adap )); <nl>  <nl> falcon_remove_spi_devices ( efx ); <nl> falcon_free_buffer ( efx , & efx -> irq_status );
enum { <nl> ATA_REG_IRQ = ATA_REG_NSECT , <nl>  <nl> /* ATA device commands */ <nl> + ATA_CMD_DEV_RESET = 0x08 , /* ATAPI device reset */ <nl> ATA_CMD_CHK_POWER = 0xE5 , /* check power mode */ <nl> ATA_CMD_STANDBY = 0xE2 , /* place in standby power mode */ <nl> ATA_CMD_IDLE = 0xE3 , /* place in idle power mode */
static int _search_rsb ( struct dlm_ls * ls , char * name , int len , int b , <nl> kref_get (& r -> res_ref ); <nl> goto out ; <nl> } <nl> + if ( error == - ENOTBLK ) <nl> + goto out ; <nl> + <nl> error = dlm_search_rsb_tree (& ls -> ls_rsbtbl [ b ]. toss , name , len , flags , & r ); <nl> if ( error ) <nl> goto out ;
static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
/* <nl> * Literals <nl> */ <nl> -# define IPR_DRIVER_VERSION " 2 . 1 . 2 " <nl> -# define IPR_DRIVER_DATE "( February 8 , 2006 )" <nl> +# define IPR_DRIVER_VERSION " 2 . 1 . 3 " <nl> +# define IPR_DRIVER_DATE "( March 29 , 2006 )" <nl>  <nl> /* <nl> * IPR_MAX_CMD_PER_LUN : This defines the maximum number of outstanding
* differently than other MIPS interrupts . <nl> */ <nl>  <nl> - static void gt64120_irq ( int irq , void * dev_id ) <nl> + static irqreturn_t gt64120_irq ( int irq , void * dev_id ) <nl> { <nl> unsigned int irq_src , int_high_src , irq_src_mask , int_high_src_mask ; <nl> int handled = 0 ; <nl> static void gt64120_irq ( int irq , void * dev_id ) <nl>  <nl> GT_WRITE ( GT_INTRCAUSE_OFS , 0 ); <nl> GT_WRITE ( GT_HINTRCAUSE_OFS , 0 ); <nl> + <nl> + return IRQ_HANDLED ; <nl> } <nl>  <nl> /*
static void __init tegra210_pll_init ( void __iomem * clk_base , <nl>  <nl> /* PLLU_VCO */ <nl> val = readl ( clk_base + pll_u_vco_params . base_reg ); <nl> - val &= ~ BIT ( 24 ); /* disable PLLU_OVERRIDE */ <nl> + val &= ~ PLLU_BASE_OVERRIDE ; /* disable PLLU_OVERRIDE */ <nl> writel ( val , clk_base + pll_u_vco_params . base_reg ); <nl>  <nl> clk = tegra_clk_register_pllre (" pll_u_vco ", " pll_ref ", clk_base , pmc ,
static void __init stp_reset ( void ) <nl>  <nl> stp_page = alloc_bootmem_pages ( PAGE_SIZE ); <nl> rc = chsc_sstpc ( stp_page , STP_OP_CTRL , 0x0000 ); <nl> - if ( rc == 1 ) <nl> + if ( rc == 0 ) <nl> set_bit ( CLOCK_SYNC_HAS_STP , & clock_sync_flags ); <nl> else if ( stp_online ) { <nl> printk ( KERN_WARNING " Running on non STP capable machine .\ n ");
static int hda_reg_write ( void * context , unsigned int reg , unsigned int val ) <nl> unsigned int verb ; <nl> int i , bytes , err ; <nl>  <nl> + if ( codec -> caps_overwriting ) <nl> + return 0 ; <nl> + <nl> reg &= ~ 0x00080000U ; /* drop GET bit */ <nl> reg |= ( codec -> addr << 28 ); <nl> verb = get_verb ( reg );
static void iwl_ucode_callback ( const struct firmware * ucode_raw , void * context ) <nl> const struct iwl_op_mode_ops * ops = op -> ops ; <nl> drv -> op_mode = ops -> start ( drv -> trans , drv -> cfg , & drv -> fw ); <nl>  <nl> - if (! drv -> op_mode ) <nl> + if (! drv -> op_mode ) { <nl> + mutex_unlock (& iwlwifi_opmode_table_mtx ); <nl> goto out_unbind ; <nl> + } <nl> } else { <nl> load_module = true ; <nl> }
int kvm_ioapic_set_irq ( struct kvm_ioapic * ioapic , int irq , int level ) <nl> if (( edge && old_irr != ioapic -> irr ) || <nl> (! edge && ! entry . fields . remote_irr )) <nl> ret = ioapic_service ( ioapic , irq ); <nl> + else <nl> + ret = 0 ; /* report coalesced interrupt */ <nl> } <nl> trace_kvm_ioapic_set_irq ( entry . bits , irq , ret == 0 ); <nl> }
int bond_arp_rcv ( const struct sk_buff * skb , struct bonding * bond , <nl> __be32 sip , tip ; <nl> int alen ; <nl>  <nl> + slave -> last_arp_rx = jiffies ; <nl> + <nl> if ( skb -> protocol != __cpu_to_be16 ( ETH_P_ARP )) <nl> return RX_HANDLER_ANOTHER ; <nl> 
static int perf_tp_filter_match ( struct perf_event * event , <nl> { <nl> void * record = data -> raw -> data ; <nl>  <nl> + /* only top level events have filters set */ <nl> + if ( event -> parent ) <nl> + event = event -> parent ; <nl> + <nl> if ( likely (! event -> filter ) || filter_match_preds ( event -> filter , record )) <nl> return 1 ; <nl> return 0 ;
int wlan_unsetup ( wlandevice_t * wlandev ) <nl>  <nl> if ( wlandev -> netdev ) { <nl> wdev = netdev_priv ( wlandev -> netdev ); <nl> - if ( wdev -> wiphy ) wlan_free_wiphy ( wdev -> wiphy ); <nl> + if ( wdev -> wiphy ) <nl> + wlan_free_wiphy ( wdev -> wiphy ); <nl> free_netdev ( wlandev -> netdev ); <nl> wlandev -> netdev = NULL ; <nl> }
void dump_stack ( void ) <nl> EXPORT_SYMBOL ( dump_stack ); <nl>  <nl> /* Stolen from arch / i386 / kernel / traps . c */ <nl> - static int kstack_depth_to_print = 24 ; <nl> + static const int kstack_depth_to_print = 24 ; <nl>  <nl> /* This recently started being used in arch - independent code too , as in <nl> * kernel / sched . c .*/
static void __cpuinit init_amd ( struct cpuinfo_x86 * c ) <nl> level = cpuid_eax ( 1 ); <nl> if ( c -> x86 == 15 && (( level >= 0x0f48 && level < 0x0f50 ) || level >= 0x0f58 )) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl> - if ( c -> x86 == 0x10 ) <nl> + if ( c -> x86 == 0x10 || c -> x86 == 0x11 ) <nl> set_bit ( X86_FEATURE_REP_GOOD , & c -> x86_capability ); <nl>  <nl> /* Enable workaround for FXSAVE leak */
__setup (" noclflush ", setup_noclflush ); <nl> void __cpuinit print_cpu_info ( struct cpuinfo_x86 * c ) <nl> { <nl> if ( c -> x86_model_id [ 0 ]) <nl> - printk ( KERN_INFO "% s ", c -> x86_model_id ); <nl> + printk ( KERN_CONT "% s ", c -> x86_model_id ); <nl>  <nl> if ( c -> x86_mask || c -> cpuid_level >= 0 ) <nl> printk ( KERN_CONT " stepping % 02x \ n ", c -> x86_mask );
int tpm_open ( struct inode * inode , struct file * file ) <nl> return - EBUSY ; <nl> } <nl>  <nl> - chip -> data_buffer = kmalloc ( TPM_BUFSIZE * sizeof ( u8 ), GFP_KERNEL ); <nl> + chip -> data_buffer = kzalloc ( TPM_BUFSIZE , GFP_KERNEL ); <nl> if ( chip -> data_buffer == NULL ) { <nl> clear_bit ( 0 , & chip -> is_open ); <nl> put_device ( chip -> dev );
 <nl> # define FIRST_VM86_IRQ 3 <nl> # define LAST_VM86_IRQ 15 <nl> -# define invalid_vm86_irq ( irq ) (( irq ) < 3 || ( irq ) > 15 ) <nl> + <nl> +# ifndef __ASSEMBLY__ <nl> + static inline int invalid_vm86_irq ( int irq ) <nl> +{ <nl> + return irq < 3 || irq > 15 ; <nl> +} <nl> +# endif <nl>  <nl> /* <nl> * Size the maximum number of interrupts .
static int cb_pcidas_attach ( struct comedi_device * dev , <nl>  <nl> /* 8255 */ <nl> s = dev -> subdevices + 2 ; <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + ret = subdev_8255_init ( dev , s , NULL , <nl> + devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* serial EEPROM , */ <nl> s = dev -> subdevices + 3 ;
static int stmmac_set_coalesce ( struct net_device * dev , <nl> ( ec -> tx_max_coalesced_frames == 0 )) <nl> return - EINVAL ; <nl>  <nl> - if (( ec -> tx_coalesce_usecs > STMMAC_COAL_TX_TIMER ) || <nl> + if (( ec -> tx_coalesce_usecs > STMMAC_MAX_COAL_TX_TICK ) || <nl> ( ec -> tx_max_coalesced_frames > STMMAC_TX_MAX_FRAMES )) <nl> return - EINVAL ; <nl> 
static int create_i2c_bus ( struct i2c_adapter * adapter , <nl> algo -> setscl = via_i2c_setscl ; <nl> algo -> getsda = via_i2c_getsda ; <nl> algo -> getscl = via_i2c_getscl ; <nl> - algo -> udelay = 40 ; <nl> - algo -> timeout = 20 ; <nl> + algo -> udelay = 10 ; <nl> + algo -> timeout = 2 ; <nl> algo -> data = adap_cfg ; <nl>  <nl> sprintf ( adapter -> name , " viafb i2c io_port idx 0x % 02x ",
void afu_release_irqs ( struct cxl_context * ctx , void * cookie ) <nl>  <nl> afu_irq_name_free ( ctx ); <nl> cxl_release_irq_ranges (& ctx -> irqs , ctx -> afu -> adapter ); <nl> + <nl> + kfree ( ctx -> irq_bitmap ); <nl> + ctx -> irq_bitmap = NULL ; <nl> + ctx -> irq_count = 0 ; <nl> }
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl>  <nl> void oprofile_arch_exit ( void ) <nl> { <nl> - model -> exit (); <nl> + if ( model ) <nl> + model -> exit (); <nl> }
static int get_skb_hdr ( struct sk_buff * skb , void ** iphdr , <nl> * tcph = tcp_hdr ( skb ); <nl>  <nl> /* check if ip header and tcp header are complete */ <nl> - if ( iph -> tot_len < ip_len + tcp_hdrlen ( skb )) <nl> + if ( ntohs ( iph -> tot_len ) < ip_len + tcp_hdrlen ( skb )) <nl> return - 1 ; <nl>  <nl> * hdr_flags = LRO_IPV4 | LRO_TCP ;
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
free_work : <nl> out_hang : <nl> intel_crtc_wait_for_pending_flips ( crtc ); <nl> ret = intel_pipe_set_base ( crtc , crtc -> x , crtc -> y , fb ); <nl> - if ( ret == 0 && event ) <nl> + if ( ret == 0 && event ) { <nl> + spin_lock_irqsave (& dev -> event_lock , flags ); <nl> drm_send_vblank_event ( dev , pipe , event ); <nl> + spin_unlock_irqrestore (& dev -> event_lock , flags ); <nl> + } <nl> } <nl> return ret ; <nl> }
int __init omap_mux_init ( u32 mux_pbase , u32 mux_size , <nl> } <nl>  <nl> # ifdef CONFIG_OMAP_MUX <nl> - omap_mux_package_fixup ( package_subset , superset ); <nl> - omap_mux_package_init_balls ( package_balls , superset ); <nl> + if ( package_subset ) <nl> + omap_mux_package_fixup ( package_subset , superset ); <nl> + if ( package_balls ) <nl> + omap_mux_package_init_balls ( package_balls , superset ); <nl> omap_mux_set_cmdline_signals (); <nl> omap_mux_set_board_signals ( board_mux ); <nl> # endif
static int check_in_drive_lists ( ide_drive_t * drive , const char ** list ) <nl> static u8 svwks_ratemask ( ide_drive_t * drive ) <nl> { <nl> struct pci_dev * dev = HWIF ( drive )-> pci_dev ; <nl> - u8 mode ; <nl> + u8 mode = 0 ; <nl>  <nl> if (! svwks_revision ) <nl> pci_read_config_byte ( dev , PCI_REVISION_ID , & svwks_revision );
static int __devinit ab8500_ponkey_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_free_dbr_irq : <nl> - free_irq ( ponkey -> irq_dbf , ponkey ); <nl> + free_irq ( ponkey -> irq_dbr , ponkey ); <nl> err_free_dbf_irq : <nl> free_irq ( ponkey -> irq_dbf , ponkey ); <nl> err_free_mem :
static void ixgbe_reset_subtask ( struct ixgbe_adapter * adapter ) <nl> netdev_err ( adapter -> netdev , " Reset adapter \ n "); <nl> adapter -> tx_timeout_count ++; <nl>  <nl> + rtnl_lock (); <nl> ixgbe_reinit_locked ( adapter ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> /**
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , u32 features ) <nl> skb -> ip_summed = CHECKSUM_NONE ; <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> - if (( skb_headroom ( skb ) < frag_hdr_sz ) && <nl> + if (( skb_mac_header ( skb ) < skb -> head + frag_hdr_sz ) && <nl> pskb_expand_head ( skb , frag_hdr_sz , 0 , GFP_ATOMIC )) <nl> goto out ; <nl> 
static int __devinit sdhci_probe_slot ( struct pci_dev * pdev , int slot ) <nl>  <nl> version = readw ( host -> ioaddr + SDHCI_HOST_VERSION ); <nl> version = ( version & SDHCI_SPEC_VER_MASK ) >> SDHCI_SPEC_VER_SHIFT ; <nl> - if ( version != 0 ) { <nl> + if ( version > 1 ) { <nl> printk ( KERN_ERR "% s : Unknown controller version (% d ). " <nl> " You may experience problems .\ n ", host -> slot_descr , <nl> version );
dispatch_ioctl ( struct client * client , unsigned int cmd , void __user * arg ) <nl> return - EFAULT ; <nl> } <nl>  <nl> - return 0 ; <nl> + return retval ; <nl> } <nl>  <nl> static long
int genwqe_device_create ( struct genwqe_dev * cd ) <nl> genwqe_attribute_groups , <nl> GENWQE_DEVNAME "% u_card ", <nl> cd -> card_idx ); <nl> - if ( cd -> dev == NULL ) { <nl> - rc = - ENODEV ; <nl> + if ( IS_ERR ( cd -> dev )) { <nl> + rc = PTR_ERR ( cd -> dev ); <nl> goto err_cdev ; <nl> } <nl> 
static struct ath_buf * ath_clone_txbuf ( struct ath_softc * sc , struct ath_buf * bf ) <nl> tbf -> aphy = bf -> aphy ; <nl> tbf -> bf_mpdu = bf -> bf_mpdu ; <nl> tbf -> bf_buf_addr = bf -> bf_buf_addr ; <nl> - *( tbf -> bf_desc ) = *( bf -> bf_desc ); <nl> + memcpy ( tbf -> bf_desc , bf -> bf_desc , sc -> sc_ah -> caps . tx_desc_len ); <nl> tbf -> bf_state = bf -> bf_state ; <nl> tbf -> bf_dmacontext = bf -> bf_dmacontext ; <nl> 
static int imx6q_revision ( void ) <nl> } <nl> } <nl>  <nl> - void imx6q_restart ( char mode , const char * cmd ) <nl> + static void imx6q_restart ( char mode , const char * cmd ) <nl> { <nl> struct device_node * np ; <nl> void __iomem * wdog_base ; <nl> put_node : <nl> of_node_put ( np ); <nl> } <nl>  <nl> - struct platform_device imx6q_cpufreq_pdev = { <nl> + static struct platform_device imx6q_cpufreq_pdev = { <nl> . name = " imx6q - cpufreq ", <nl> }; <nl> 
static int uio_pdrv_remove ( struct platform_device * pdev ) <nl>  <nl> uio_unregister_device ( pdata -> uioinfo ); <nl>  <nl> + kfree ( pdata ); <nl> + <nl> return 0 ; <nl> } <nl> 
mpt_attach ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl> ioc -> msi_enable = 0 ; <nl> break ; <nl> } <nl> + <nl> + ioc -> fw_events_off = 1 ; <nl> + <nl> if ( ioc -> errata_flag_1064 ) <nl> pci_disable_io_access ( pdev ); <nl> 
struct i810fb_par { <nl> struct i810fb_i2c_chan chan [ 3 ]; <nl> struct mutex open_lock ; <nl> unsigned int use_count ; <nl> - u32 pseudo_palette [ 17 ]; <nl> + u32 pseudo_palette [ 16 ]; <nl> unsigned long mmio_start_phys ; <nl> u8 __iomem * mmio_start_virtual ; <nl> u8 * edid ;
int ieee80211_master_start_xmit ( struct sk_buff * skb , <nl> return 0 ; <nl> } <nl>  <nl> + if ( osdata -> vif . type == NL80211_IFTYPE_AP_VLAN ) <nl> + osdata = container_of ( osdata -> bss , <nl> + struct ieee80211_sub_if_data , <nl> + u . ap ); <nl> info -> control . vif = & osdata -> vif ; <nl> ret = ieee80211_tx ( odev , skb ); <nl> dev_put ( odev );
void dump_trace ( struct task_struct * task , struct pt_regs * regs , <nl> if ( UNW_SP (& info ) >= PAGE_OFFSET ) { <nl> ops -> warning ( data , " Leftover inexact backtrace :\ n "); <nl> stack = ( void *) UNW_SP (& info ); <nl> + if (! stack ) <nl> + return ; <nl> } else <nl> ops -> warning ( data , " Full inexact backtrace again :\ n "); <nl> } else if ( call_trace >= 1 )
static int __dma_supported ( struct device * dev , u64 mask , bool warn ) <nl> */ <nl> if ( sizeof ( mask ) != sizeof ( dma_addr_t ) && <nl> mask > ( dma_addr_t )~ 0 && <nl> - dma_to_pfn ( dev , ~ 0 ) < max_pfn ) { <nl> + dma_to_pfn ( dev , ~ 0 ) < max_pfn - 1 ) { <nl> if ( warn ) { <nl> dev_warn ( dev , " Coherent DMA mask %# llx is larger than dma_addr_t allows \ n ", <nl> mask );
static int snd_timer_s_stop ( struct snd_timer * timer ) <nl> timer -> sticks = priv -> last_expires - jiff ; <nl> else <nl> timer -> sticks = 1 ; <nl> + priv -> correction = 0 ; <nl> return 0 ; <nl> } <nl> 
# define gadget_is_musbhsfc ( g ) 0 <nl> # endif <nl>  <nl> -/* Mentor high speed " dual role " controller , peripheral mode */ <nl> -# ifdef CONFIG_USB_GADGET_MUSBHDRC <nl> -# define gadget_is_musbhdrc ( g ) ! strcmp (" musbhdrc_udc ", ( g )-> name ) <nl> +/* Mentor high speed " dual role " controller , in peripheral role */ <nl> +# ifdef CONFIG_USB_GADGET_MUSB_HDRC <nl> +# define gadget_is_musbhdrc ( g ) ! strcmp (" musb_hdrc ", ( g )-> name ) <nl> # else <nl> # define gadget_is_musbhdrc ( g ) 0 <nl> # endif
s32 e1000e_setup_fiber_serdes_link ( struct e1000_hw * hw ) <nl> e_dbg (" No signal detected \ n "); <nl> } <nl>  <nl> - return 0 ; <nl> + return ret_val ; <nl> } <nl>  <nl> /**
static int __devinit adm8211_probe ( struct pci_dev * pdev , <nl>  <nl> priv -> channel = 1 ; <nl>  <nl> + dev -> wiphy -> bands [ IEEE80211_BAND_2GHZ ] = & priv -> band ; <nl> + <nl> err = ieee80211_register_hw ( dev ); <nl> if ( err ) { <nl> printk ( KERN_ERR "% s ( adm8211 ): Cannot register device \ n ",
static void __devinit pci_read_bridge_io ( struct pci_bus * child ) <nl>  <nl> if ( base && base <= limit ) { <nl> res -> flags = ( io_base_lo & PCI_IO_RANGE_TYPE_MASK ) | IORESOURCE_IO ; <nl> + res2 . flags = res -> flags ; <nl> region . start = base ; <nl> region . end = limit + 0xfff ; <nl> pcibios_bus_to_resource ( dev , & res2 , & region );
static int dispatch_procfs_write ( struct file * file , <nl>  <nl> if (! ibm || ! ibm -> write ) <nl> return - EINVAL ; <nl> + if ( count > PAGE_SIZE - 2 ) <nl> + return - EINVAL ; <nl>  <nl> kernbuf = kmalloc ( count + 2 , GFP_KERNEL ); <nl> if (! kernbuf )
ret_orig : <nl> kfree_skb ( clone ); <nl> return skb ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( nf_ct_frag6_gather ); <nl>  <nl> void nf_ct_frag6_consume_orig ( struct sk_buff * skb ) <nl> {
intel_dp_max_link_bw ( struct intel_dp * intel_dp ) <nl> case DP_LINK_BW_1_62 : <nl> case DP_LINK_BW_2_7 : <nl> break ; <nl> + case DP_LINK_BW_5_4 : /* 1 . 2 capable displays may advertise higher bw */ <nl> + max_link_bw = DP_LINK_BW_2_7 ; <nl> + break ; <nl> default : <nl> + WARN ( 1 , " invalid max DP link bw val % x , using 1 . 62Gbps \ n ", <nl> + max_link_bw ); <nl> max_link_bw = DP_LINK_BW_1_62 ; <nl> break ; <nl> }
 <nl> # include < linux / seq_file . h > <nl> # include < linux / list . h > <nl> +# include < linux / vmalloc . h > <nl> # include " debug . h " <nl> # include " ath5k . h " <nl> # include " reg . h "
static int bug_handler ( struct pt_regs * regs , unsigned int esr ) <nl> break ; <nl>  <nl> case BUG_TRAP_TYPE_WARN : <nl> + /* Ideally , report_bug () should backtrace for us ... but no . */ <nl> + dump_backtrace ( regs , NULL ); <nl> break ; <nl>  <nl> default :
void (* mach_beep )( unsigned int , unsigned int ); <nl> # if defined ( CONFIG_ISA ) && defined ( MULTI_ISA ) <nl> int isa_type ; <nl> int isa_sex ; <nl> + EXPORT_SYMBOL ( isa_type ); <nl> + EXPORT_SYMBOL ( isa_sex ); <nl> # endif <nl>  <nl> extern int amiga_parse_bootinfo ( const struct bi_record *);
int setup_one_line ( struct line * lines , int n , char * init , <nl> * error_out = " Failed to allocate memory "; <nl> return - ENOMEM ; <nl> } <nl> - if ( line -> valid ) <nl> + if ( line -> valid ) { <nl> tty_unregister_device ( driver , n ); <nl> + kfree ( line -> init_str ); <nl> + } <nl> line -> init_str = new ; <nl> line -> valid = 1 ; <nl> err = parse_chan_pair ( new , line , n , opts , error_out );
void speakup_remove_virtual_keyboard ( void ) <nl> { <nl> if ( virt_keyboard != NULL ) { <nl> input_unregister_device ( virt_keyboard ); <nl> - input_free_device ( virt_keyboard ); <nl> virt_keyboard = NULL ; <nl> } <nl> }
static u32 __init allocate_aperture ( void ) <nl> printk (" Cannot allocate aperture memory hole (% p ,% uK )\ n ", <nl> p , aper_size >> 10 ); <nl> if ( p ) <nl> - free_bootmem_node ( nd0 , ( unsigned long ) p , aper_size ); <nl> + free_bootmem_node ( nd0 , __pa ( p ), aper_size ); <nl> return 0 ; <nl> } <nl> printk (" Mapping aperture over % d KB of RAM @ % lx \ n ",
static void __init ima_add_boot_aggregate ( void ) <nl> result = ima_calc_boot_aggregate (& hash . hdr ); <nl> if ( result < 0 ) { <nl> audit_cause = " hashing_error "; <nl> - kfree ( entry ); <nl> goto err_out ; <nl> } <nl> }
SYSCALL_DEFINE5 ( perf_event_open , <nl> } <nl> } <nl>  <nl> - if ( pid != - 1 ) <nl> + if ( pid != - 1 ) { <nl> task = find_lively_task_by_vpid ( pid ); <nl> + if ( IS_ERR ( task )) { <nl> + err = PTR_ERR ( task ); <nl> + goto err_group_fd ; <nl> + } <nl> + } <nl>  <nl> /* <nl> * Get the target context ( task or percpu ):
static int clone_backref_node ( struct btrfs_trans_handle * trans , <nl> new_node -> bytenr = dest -> node -> start ; <nl> new_node -> level = node -> level ; <nl> new_node -> lowest = node -> lowest ; <nl> + new_node -> checked = 1 ; <nl> new_node -> root = dest ; <nl>  <nl> if (! node -> lowest ) {
static int omap_pcm_open ( struct snd_pcm_substream * substream ) <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> - prtd = kzalloc ( sizeof ( prtd ), GFP_KERNEL ); <nl> + prtd = kzalloc ( sizeof (* prtd ), GFP_KERNEL ); <nl> if ( prtd == NULL ) { <nl> ret = - ENOMEM ; <nl> goto out ;
int datagram_send_ctl ( struct net * net , struct sock * sk , <nl>  <nl> if ( addr_type != IPV6_ADDR_ANY ) { <nl> int strict = __ipv6_addr_src_scope ( addr_type ) <= IPV6_ADDR_SCOPE_LINKLOCAL ; <nl> - if (! inet_sk ( sk )-> transparent && <nl> + if (!( inet_sk ( sk )-> freebind || inet_sk ( sk )-> transparent ) && <nl> ! ipv6_chk_addr ( net , & src_info -> ipi6_addr , <nl> strict ? dev : NULL , 0 )) <nl> err = - EINVAL ;
iscsi_tcp_state_change ( struct sock * sk ) <nl> conn = ( struct iscsi_conn *) sk -> sk_user_data ; <nl> session = conn -> session ; <nl>  <nl> - if ( sk -> sk_state == TCP_CLOSE_WAIT || <nl> - sk -> sk_state == TCP_CLOSE ) { <nl> + if (( sk -> sk_state == TCP_CLOSE_WAIT || <nl> + sk -> sk_state == TCP_CLOSE ) && <nl> + ! atomic_read (& sk -> sk_rmem_alloc )) { <nl> debug_tcp (" iscsi_tcp_state_change : TCP_CLOSE | TCP_CLOSE_WAIT \ n "); <nl> iscsi_conn_failure ( conn , ISCSI_ERR_CONN_FAILED ); <nl> }
int hid_input_report ( struct hid_device * hid , int type , u8 * data , int size , int i <nl>  <nl> if ( hdrv && hdrv -> raw_event && hid_match_report ( hid , report )) { <nl> ret = hdrv -> raw_event ( hid , report , data , size ); <nl> - if ( ret != 0 ) { <nl> + if ( ret < 0 ) { <nl> ret = ret < 0 ? ret : 0 ; <nl> goto unlock ; <nl> }
static int omap_sham_finup ( struct ahash_request * req ) <nl> ctx -> flags |= FLAGS_FINUP ; <nl>  <nl> err1 = omap_sham_update ( req ); <nl> - if ( err1 == - EINPROGRESS ) <nl> + if ( err1 == - EINPROGRESS || err1 == - EBUSY ) <nl> return err1 ; <nl> /* <nl> * final () has to be always called to cleanup resources
void batadv_gw_node_update ( struct batadv_priv * bat_priv , <nl> * gets dereferenced . <nl> */ <nl> spin_lock_bh (& bat_priv -> gw . list_lock ); <nl> - hlist_del_init_rcu (& gw_node -> list ); <nl> + if (! hlist_unhashed (& gw_node -> list )) { <nl> + hlist_del_init_rcu (& gw_node -> list ); <nl> + batadv_gw_node_free_ref ( gw_node ); <nl> + } <nl> spin_unlock_bh (& bat_priv -> gw . list_lock ); <nl>  <nl> - batadv_gw_node_free_ref ( gw_node ); <nl> - <nl> curr_gw = batadv_gw_get_selected_gw_node ( bat_priv ); <nl> if ( gw_node == curr_gw ) <nl> batadv_gw_reselect ( bat_priv );
static int perf_session_deliver_event ( struct perf_session * session , <nl> dump_sample ( session , event , sample ); <nl> if ( evsel == NULL ) { <nl> ++ session -> hists . stats . nr_unknown_id ; <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl> if ( machine == NULL ) { <nl> ++ session -> hists . stats . nr_unprocessable_samples ; <nl> - return - 1 ; <nl> + return 0 ; <nl> } <nl> return tool -> sample ( tool , event , sample , evsel , machine ); <nl> case PERF_RECORD_MMAP :
static void scsi_sysfs_add_devices ( struct Scsi_Host * shost ) <nl> /* target removed before the device could be added */ <nl> if ( sdev -> sdev_state == SDEV_DEL ) <nl> continue ; <nl> + /* If device is already visible , skip adding it to sysfs */ <nl> + if ( sdev -> is_visible ) <nl> + continue ; <nl> if (! scsi_host_scan_allowed ( shost ) || <nl> scsi_sysfs_add_sdev ( sdev ) != 0 ) <nl> __scsi_remove_device ( sdev );
__rb_reserve_next ( struct ring_buffer_per_cpu * cpu_buffer , <nl> write &= RB_WRITE_MASK ; <nl> tail = write - length ; <nl>  <nl> + /* <nl> + * If this is the first commit on the page , then it has the same <nl> + * timestamp as the page itself . <nl> + */ <nl> + if (! tail ) <nl> + delta = 0 ; <nl> + <nl> /* See if we shot pass the end of this buffer page */ <nl> if ( unlikely ( write > BUF_PAGE_SIZE )) <nl> return rb_move_tail ( cpu_buffer , length , tail ,
alloc_new_skb : <nl> * because we have no idea what fragment will be <nl> * the last . <nl> */ <nl> - if ( datalen == length ) <nl> + if ( datalen == length + fraggap ) <nl> alloclen += rt -> u . dst . trailer_len ; <nl>  <nl> if ( transhdrlen ) {
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( ieee80211_vif_is_mesh (& sdata -> vif ) && <nl> ! sdata -> u . mesh . user_mpm ) <nl> init_timer (& sta -> plink_timer ); <nl> + sta -> nonpeer_pm = NL80211_MESH_POWER_ACTIVE ; <nl> # endif <nl>  <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN );
nfsd4_getdeviceinfo ( struct svc_rqst * rqstp , <nl> nfserr = ops -> proc_getdeviceinfo ( exp -> ex_path . mnt -> mnt_sb , gdp ); <nl>  <nl> gdp -> gd_notify_types &= ops -> notify_types ; <nl> - exp_put ( exp ); <nl> out : <nl> + exp_put ( exp ); <nl> return nfserr ; <nl> } <nl> 
static void paging_new_cr3 ( struct kvm_vcpu * vcpu ) <nl> { <nl> pgprintk ("% s : cr3 % lx \ n ", __FUNCTION__ , vcpu -> cr3 ); <nl> mmu_free_roots ( vcpu ); <nl> + if ( unlikely ( vcpu -> kvm -> n_free_mmu_pages < KVM_MIN_FREE_MMU_PAGES )) <nl> + kvm_mmu_free_some_pages ( vcpu ); <nl> mmu_alloc_roots ( vcpu ); <nl> kvm_mmu_flush_tlb ( vcpu ); <nl> kvm_arch_ops -> set_cr3 ( vcpu , vcpu -> mmu . root_hpa );
int ceph_osdc_create_event ( struct ceph_osd_client * osdc , <nl> event -> data = data ; <nl> event -> osdc = osdc ; <nl> INIT_LIST_HEAD (& event -> osd_node ); <nl> + RB_CLEAR_NODE (& event -> node ); <nl> kref_init (& event -> kref ); /* one ref for us */ <nl> kref_get (& event -> kref ); /* one ref for the caller */ <nl> init_completion (& event -> completion );
static int mwifiex_pcie_event_complete ( struct mwifiex_adapter * adapter , <nl>  <nl> if (! card -> evt_buf_list [ rdptr ]) { <nl> skb_push ( skb , INTF_HEADER_LEN ); <nl> + skb_put ( skb , MAX_EVENT_SIZE - skb -> len ); <nl> + memset ( skb -> data , 0 , MAX_EVENT_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , <nl> MAX_EVENT_SIZE , <nl> PCI_DMA_FROMDEVICE ))
const struct imx_imx_ssi_data imx35_imx_ssi_data [] __initconst = { <nl> # ifdef CONFIG_SOC_IMX51 <nl> const struct imx_imx_ssi_data imx51_imx_ssi_data [] __initconst = { <nl> # define imx51_imx_ssi_data_entry ( _id , _hwid ) \ <nl> - imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_4K ) <nl> + imx_imx_ssi_data_entry ( MX51 , _id , _hwid , SZ_16K ) <nl> imx51_imx_ssi_data_entry ( 0 , 1 ), <nl> imx51_imx_ssi_data_entry ( 1 , 2 ), <nl> imx51_imx_ssi_data_entry ( 2 , 3 ),
static const struct nla_policy ifla_vf_policy [ IFLA_VF_MAX + 1 ] = { <nl> . len = sizeof ( struct ifla_vf_vlan ) }, <nl> [ IFLA_VF_TX_RATE ] = { . type = NLA_BINARY , <nl> . len = sizeof ( struct ifla_vf_tx_rate ) }, <nl> + [ IFLA_VF_SPOOFCHK ] = { . type = NLA_BINARY , <nl> + . len = sizeof ( struct ifla_vf_spoofchk ) }, <nl> }; <nl>  <nl> static const struct nla_policy ifla_port_policy [ IFLA_PORT_MAX + 1 ] = {
static int wanxl_pci_init_one ( struct pci_dev * pdev , <nl> if ( pci_set_consistent_dma_mask ( pdev , DMA_BIT_MASK ( 28 )) || <nl> pci_set_dma_mask ( pdev , DMA_BIT_MASK ( 28 ))) { <nl> pr_err (" No usable DMA configuration \ n "); <nl> + pci_disable_device ( pdev ); <nl> return - EIO ; <nl> } <nl> 
static void put_pages ( struct drm_gem_object * obj ) <nl>  <nl> if ( iommu_present (& platform_bus_type )) <nl> drm_gem_put_pages ( obj , msm_obj -> pages , true , false ); <nl> - else <nl> + else { <nl> drm_mm_remove_node ( msm_obj -> vram_node ); <nl> + drm_free_large ( msm_obj -> pages ); <nl> + } <nl>  <nl> msm_obj -> pages = NULL ; <nl> }
void igb_power_up_link ( struct igb_adapter * adapter ) <nl> igb_power_up_phy_copper (& adapter -> hw ); <nl> else <nl> igb_power_up_serdes_link_82575 (& adapter -> hw ); <nl> + igb_reset_phy (& adapter -> hw ); <nl> } <nl>  <nl> /**
static bool linkwatch_urgent_event ( struct net_device * dev ) <nl> if ( dev -> ifindex != dev -> iflink ) <nl> return true ; <nl>  <nl> + if ( dev -> priv_flags & IFF_TEAM_PORT ) <nl> + return true ; <nl> + <nl> return netif_carrier_ok ( dev ) && qdisc_tx_changing ( dev ); <nl> } <nl> 
static int md_notify_reboot ( struct notifier_block * this , <nl> if ( mddev_trylock ( mddev )) { <nl> if ( mddev -> pers ) <nl> __md_stop_writes ( mddev ); <nl> - mddev -> safemode = 2 ; <nl> + if ( mddev -> persistent ) <nl> + mddev -> safemode = 2 ; <nl> mddev_unlock ( mddev ); <nl> } <nl> need_delay = 1 ;
UNUSUAL_DEV ( 0x090a , 0x1200 , 0x0000 , 0x9999 , <nl> USB_SC_RBC , USB_PR_BULK , NULL , <nl> 0 ), <nl>  <nl> +/* Feiya QDI U2 DISK , reported by Hans de Goede < hdegoede @ redhat . com > */ <nl> + UNUSUAL_DEV ( 0x090c , 0x1000 , 0x0000 , 0xffff , <nl> + " Feiya ", <nl> + " QDI U2 DISK ", <nl> + USB_SC_DEVICE , USB_PR_DEVICE , NULL , <nl> + US_FL_NO_READ_CAPACITY_16 ), <nl> + <nl> /* aeb */ <nl> UNUSUAL_DEV ( 0x090c , 0x1132 , 0x0000 , 0xffff , <nl> " Feiya ",
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> struct oz_app_hdr * app_hdr ; <nl> struct oz_serial_ctx * ctx ; <nl>  <nl> + if ( count > sizeof ( ei -> data ) - sizeof (* elt ) - sizeof (* app_hdr )) <nl> + return - EINVAL ; <nl> + <nl> spin_lock_bh (& g_cdev . lock ); <nl> pd = g_cdev . active_pd ; <nl> if ( pd )
static int wm8994_device_init ( struct wm8994 * wm8994 , int irq ) <nl> struct regmap_config * regmap_config ; <nl> const struct reg_default * regmap_patch = NULL ; <nl> const char * devname ; <nl> - int ret , i , patch_regs ; <nl> + int ret , i , patch_regs = 0 ; <nl> int pulls = 0 ; <nl>  <nl> if ( dev_get_platdata ( wm8994 -> dev )) {
int ath9k_hw_fill_cap_info ( struct ath_hw * ah ) <nl>  <nl> if ( AR_SREV_9485 ( ah ) || AR_SREV_9285 ( ah ) || AR_SREV_9330 ( ah )) <nl> chip_chainmask = 1 ; <nl> + else if ( AR_SREV_9462 ( ah )) <nl> + chip_chainmask = 3 ; <nl> else if (! AR_SREV_9280_20_OR_LATER ( ah )) <nl> chip_chainmask = 7 ; <nl> else if (! AR_SREV_9300_20_OR_LATER ( ah ) || AR_SREV_9340 ( ah ))
int btrfs_drop_snapshot ( struct btrfs_root * root , <nl> int level ; <nl>  <nl> path = btrfs_alloc_path (); <nl> - BUG_ON (! path ); <nl> + if (! path ) <nl> + return - ENOMEM ; <nl>  <nl> wc = kzalloc ( sizeof (* wc ), GFP_NOFS ); <nl> - BUG_ON (! wc ); <nl> + if (! wc ) { <nl> + btrfs_free_path ( path ); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> trans = btrfs_start_transaction ( tree_root , 0 ); <nl> BUG_ON ( IS_ERR ( trans ));
static int __cpuinit iucv_cpu_notify ( struct notifier_block * self , <nl> return NOTIFY_BAD ; <nl> iucv_param [ cpu ] = kmalloc_node ( sizeof ( union iucv_param ), <nl> GFP_KERNEL | GFP_DMA , cpu_to_node ( cpu )); <nl> - if (! iucv_param [ cpu ]) <nl> + if (! iucv_param [ cpu ]) { <nl> + kfree ( iucv_irq_data [ cpu ]); <nl> + iucv_irq_data [ cpu ] = NULL ; <nl> return NOTIFY_BAD ; <nl> + } <nl> break ; <nl> case CPU_UP_CANCELED : <nl> case CPU_UP_CANCELED_FROZEN :
int __scm_send ( struct socket * sock , struct msghdr * msg , struct scm_cookie * p ) <nl> goto error ; <nl>  <nl> cred -> uid = cred -> euid = p -> creds . uid ; <nl> - cred -> gid = cred -> egid = p -> creds . uid ; <nl> + cred -> gid = cred -> egid = p -> creds . gid ; <nl> put_cred ( p -> cred ); <nl> p -> cred = cred ; <nl> }
static ssize_t dgrp_class_pollrate_store ( struct device * c , <nl> struct device_attribute * attr , <nl> const char * buf , size_t count ) <nl> { <nl> - sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ); <nl> + if ( sscanf ( buf , " 0x % x \ n ", & dgrp_poll_tick ) != 1 ) <nl> + return - EINVAL ; <nl> + <nl> return count ; <nl> } <nl> static DEVICE_ATTR ( pollrate , 0600 , dgrp_class_pollrate_show ,
nouveau_abi16_ioctl_channel_alloc ( ABI16_IOCTL_ARGS ) <nl>  <nl> if ( unlikely (! abi16 )) <nl> return - ENOMEM ; <nl> + <nl> + if (! drm -> channel ) <nl> + return nouveau_abi16_put ( abi16 , - ENODEV ); <nl> + <nl> client = nv_client ( abi16 -> client ); <nl>  <nl> if ( init -> fb_ctxdma_handle == ~ 0 || init -> tt_ctxdma_handle == ~ 0 )
static void dmaengine_pcm_request_chan_of ( struct dmaengine_pcm * pcm , <nl> return ; <nl>  <nl> if ( pcm -> flags & SND_DMAENGINE_PCM_FLAG_HALF_DUPLEX ) { <nl> - pcm -> chan [ 0 ] = of_dma_request_slave_channel ( of_node , " tx_rx "); <nl> + pcm -> chan [ 0 ] = of_dma_request_slave_channel ( of_node , " rx - tx "); <nl> pcm -> chan [ 1 ] = pcm -> chan [ 0 ]; <nl> } else { <nl> for ( i = SNDRV_PCM_STREAM_PLAYBACK ; i <= SNDRV_PCM_STREAM_CAPTURE ; i ++) {
int __kvm_set_memory_region ( struct kvm * kvm , <nl> goto out ; <nl> if ( mem -> guest_phys_addr & ( PAGE_SIZE - 1 )) <nl> goto out ; <nl> + if ( mem -> userspace_addr & ( PAGE_SIZE - 1 )) <nl> + goto out ; <nl> if ( mem -> slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS ) <nl> goto out ; <nl> if ( mem -> guest_phys_addr + mem -> memory_size < mem -> guest_phys_addr )
static int snd_sb_csp_ioctl ( struct snd_hwdep * hw , struct file * file , unsigned i <nl> switch ( cmd ) { <nl> /* get information */ <nl> case SNDRV_SB_CSP_IOCTL_INFO : <nl> + memset (& info , 0 , sizeof ( info )); <nl> * info . codec_name = * p -> codec_name ; <nl> info . func_nr = p -> func_nr ; <nl> info . acc_format = p -> acc_format ;
static void vmbus_process_offer ( struct work_struct * work ) <nl> spin_lock_irqsave (& vmbus_connection . channel_lock , flags ); <nl> list_del (& newchannel -> listentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channel_lock , flags ); <nl> + kfree ( newchannel -> device_obj ); <nl>  <nl> free_channel ( newchannel ); <nl> } else {
void iwl_rx_reply_rx ( struct iwl_priv * priv , <nl> iwl_dbg_report_frame ( priv , rx_start , len , header , 1 ); <nl> # endif <nl> IWL_DEBUG_STATS_LIMIT ( priv , " Rssi % d , noise % d , qual % d , TSF % llu \ n ", <nl> - rx_status . signal , rx_status . noise , rx_status . signal , <nl> + rx_status . signal , rx_status . noise , rx_status . qual , <nl> ( unsigned long long ) rx_status . mactime ); <nl>  <nl> /*
static void ovfx2_pkt_scan ( struct gspca_dev * gspca_dev , <nl> gspca_frame_add ( gspca_dev , INTER_PACKET , data , len ); <nl>  <nl> /* A short read signals EOF */ <nl> - if ( len < OVFX2_BULK_SIZE ) { <nl> + if ( len < gspca_dev -> cam . bulk_size ) { <nl> /* If the frame is short , and it is one of the first ones <nl> the sensor and bridge are still syncing , so drop it . */ <nl> if ( sd -> first_frame ) {
nv_printk_ ( struct nouveau_object * object , int level , const char * fmt , ...) <nl> char obuf [ 64 ], * ofmt = ""; <nl>  <nl> if ( object -> engine ) { <nl> - snprintf ( obuf , sizeof ( obuf ), "[ 0x % 08x ][% p ]", <nl> - nv_hclass ( object ), object ); <nl> + snprintf ( obuf , sizeof ( obuf ), "[ 0x % 08x ]", <nl> + nv_hclass ( object )); <nl> ofmt = obuf ; <nl> subdev = object -> engine ; <nl> device = object -> engine ;
EXPORT_SYMBOL_GPL ( usbnet_resume ); <nl>  <nl> static int __init usbnet_init ( void ) <nl> { <nl> - /* compiler should optimize this out */ <nl> - BUILD_BUG_ON ( sizeof ((( struct sk_buff *) 0 )-> cb ) <nl> - < sizeof ( struct skb_data )); <nl> + /* Compiler should optimize this out . */ <nl> + BUILD_BUG_ON ( <nl> + FIELD_SIZEOF ( struct sk_buff , cb ) < sizeof ( struct skb_data )); <nl>  <nl> random_ether_addr ( node_id ); <nl> return 0 ;
restart : <nl> if ( radix_tree_deref_retry ( entry )) <nl> goto restart ; <nl>  <nl> - irq = create_irq (); <nl> + irq = irq_alloc_desc ( numa_node_id ()); <nl> if ( unlikely ( irq < 0 )) { <nl> pr_err (" no more free IRQs , bailing ..\ n "); <nl> break ; <nl> } <nl>  <nl> + activate_irq ( irq ); <nl> + <nl> pr_info (" Setting up a chained VIRQ from % d -> % d \ n ", <nl> irq , entry -> pirq ); <nl> 
build_unc_path_to_root ( const struct smb_vol * vol , <nl> pos = full_path + unc_len ; <nl>  <nl> if ( pplen ) { <nl> - * pos ++ = CIFS_DIR_SEP ( cifs_sb ); <nl> - strncpy ( pos , vol -> prepath , pplen ); <nl> + * pos = CIFS_DIR_SEP ( cifs_sb ); <nl> + strncpy ( pos + 1 , vol -> prepath , pplen ); <nl> pos += pplen ; <nl> } <nl> 
static int __exit usba_udc_remove ( struct platform_device * pdev ) <nl> gpio_free ( udc -> vbus_pin ); <nl>  <nl> free_irq ( udc -> irq , udc ); <nl> + kfree ( usba_ep ); <nl> iounmap ( udc -> fifo ); <nl> iounmap ( udc -> regs ); <nl> clk_put ( udc -> hclk );
static inline bool __rpc_copy_addr6 ( struct sockaddr * dst , <nl>  <nl> dsin6 -> sin6_family = ssin6 -> sin6_family ; <nl> dsin6 -> sin6_addr = ssin6 -> sin6_addr ; <nl> + dsin6 -> sin6_scope_id = ssin6 -> sin6_scope_id ; <nl> return true ; <nl> } <nl> # else /* !( IS_ENABLED ( CONFIG_IPV6 ) */
static int ttm_buffer_object_transfer ( struct ttm_buffer_object * bo , <nl> INIT_LIST_HEAD (& fbo -> lru ); <nl> INIT_LIST_HEAD (& fbo -> swap ); <nl> fbo -> vm_node = NULL ; <nl> + atomic_set (& fbo -> cpu_writers , 0 ); <nl>  <nl> fbo -> sync_obj = driver -> sync_obj_ref ( bo -> sync_obj ); <nl> kref_init (& fbo -> list_kref );
static netdev_tx_t tg3_start_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> struct iphdr * iph ; <nl> u32 tcp_opt_len , hdr_len ; <nl>  <nl> - if ( skb_header_cloned ( skb ) && <nl> - pskb_expand_head ( skb , 0 , 0 , GFP_ATOMIC )) <nl> + if ( skb_cow_head ( skb , 0 )) <nl> goto drop ; <nl>  <nl> iph = ip_hdr ( skb );
out : <nl> full_bio -> bi_private = pe -> full_bio_private ; <nl> atomic_inc (& full_bio -> bi_remaining ); <nl> } <nl> - free_pending_exception ( pe ); <nl> - <nl> increment_pending_exceptions_done_count (); <nl>  <nl> up_write (& s -> lock ); <nl> out : <nl> } <nl>  <nl> retry_origin_bios ( s , origin_bios ); <nl> + <nl> + free_pending_exception ( pe ); <nl> } <nl>  <nl> static void commit_callback ( void * context , int success )
int snd_soc_dapm_stream_event ( struct snd_soc_codec * codec , <nl> { <nl> struct snd_soc_dapm_widget * w ; <nl>  <nl> + if ( stream == NULL ) <nl> + return 0 ; <nl> + <nl> mutex_lock (& codec -> mutex ); <nl> list_for_each_entry ( w , & codec -> dapm_widgets , list ) <nl> {
retry : <nl> * and pretend the write failed ... */ <nl> ext3_truncate_failed_direct_write ( inode ); <nl> ret = PTR_ERR ( handle ); <nl> + if ( inode -> i_nlink ) <nl> + ext3_orphan_del ( NULL , inode ); <nl> goto out ; <nl> } <nl> if ( inode -> i_nlink )
struct mmc_fixup { <nl> # define CID_OEMID_ANY (( unsigned short ) - 1 ) <nl> # define CID_NAME_ANY ( NULL ) <nl>  <nl> -# define END_FIXUP { 0 } <nl> +# define END_FIXUP { NULL } <nl>  <nl> # define _FIXUP_EXT ( _name , _manfid , _oemid , _rev_start , _rev_end , \ <nl> _cis_vendor , _cis_device , \
static int __fimc_md_create_flite_source_links ( struct fimc_md * fmd ) <nl> { <nl> struct media_entity * source , * sink ; <nl> unsigned int flags = MEDIA_LNK_FL_ENABLED ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> for ( i = 0 ; i < FIMC_LITE_MAX_DEVS ; i ++) { <nl> struct fimc_lite * fimc = fmd -> fimc_lite [ i ];
static int gfs2_write_end ( struct file * file , struct address_space * mapping , <nl> } <nl>  <nl> brelse ( dibh ); <nl> - gfs2_trans_end ( sdp ); <nl> failed : <nl> + gfs2_trans_end ( sdp ); <nl> if ( al ) { <nl> gfs2_inplace_release ( ip ); <nl> gfs2_quota_unlock ( ip );
void __init paging_init ( void ) <nl> map_mem (); <nl> fixup_executable (); <nl>  <nl> - /* <nl> - * Finally flush the caches and tlb to ensure that we ' re in a <nl> - * consistent state . <nl> - */ <nl> - flush_cache_all (); <nl> - flush_tlb_all (); <nl> - <nl> /* allocate the zero page . */ <nl> zero_page = early_alloc ( PAGE_SIZE ); <nl> 
static __inline__ struct page * drm_do_vm_shm_nopage ( struct vm_area_struct * vma , <nl>  <nl> offset = address - vma -> vm_start ; <nl> i = ( unsigned long ) map -> handle + offset ; <nl> - page = vmalloc_to_page (( void *) i ); <nl> + page = ( map -> type == _DRM_CONSISTENT ) ? <nl> + virt_to_page (( void *) i ) : vmalloc_to_page (( void *) i ); <nl> if (! page ) <nl> return NOPAGE_OOM ; <nl> get_page ( page );
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static int bnx2x_issue_dmae_with_comp ( struct bnx2x * bp , <nl> struct dmae_command * dmae ) <nl> { <nl> u32 * wb_comp = bnx2x_sp ( bp , wb_comp ); <nl> - int cnt = CHIP_REV_IS_SLOW ( bp ) ? ( 400000 ) : 40 ; <nl> + int cnt = CHIP_REV_IS_SLOW ( bp ) ? ( 400000 ) : 4000 ; <nl> int rc = 0 ; <nl>  <nl> DP ( BNX2X_MSG_OFF , " data before [ 0x % 08x 0x % 08x 0x % 08x 0x % 08x ]\ n ",
void sb1250_time_init ( void ) <nl> /* Disable the timer and set up the count */ <nl> __raw_writeq ( 0 , IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_CFG ))); <nl> # ifdef CONFIG_SIMULATION <nl> - __raw_writeq ( 50000 / HZ , <nl> + __raw_writeq (( 50000 / HZ ) - 1 , <nl> IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_INIT ))); <nl> # else <nl> - __raw_writeq ( 1000000 / HZ , <nl> + __raw_writeq (( V_SCD_TIMER_FREQ / HZ ) - 1 , <nl> IOADDR ( A_SCD_TIMER_REGISTER ( cpu , R_SCD_TIMER_INIT ))); <nl> # endif <nl> 
int usb_sg_init ( struct usb_sg_request * io , struct usb_device * dev , <nl> } <nl>  <nl> /* initialize all the urbs we ' ll use */ <nl> - io -> urbs = kmalloc ( io -> entries * sizeof * io -> urbs , mem_flags ); <nl> + io -> urbs = kmalloc ( io -> entries * sizeof (* io -> urbs ), mem_flags ); <nl> if (! io -> urbs ) <nl> goto nomem ; <nl> 
static int handle_invalid_guest_state ( struct kvm_vcpu * vcpu ) <nl> if ( intr_window_requested && vmx_interrupt_allowed ( vcpu )) <nl> return handle_interrupt_window (& vmx -> vcpu ); <nl>  <nl> + if ( test_bit ( KVM_REQ_EVENT , & vcpu -> requests )) <nl> + return 1 ; <nl> + <nl> err = emulate_instruction ( vcpu , 0 ); <nl>  <nl> if ( err == EMULATE_DO_MMIO ) {
static int __init mbcs_init ( void ) <nl> { <nl> int rv ; <nl>  <nl> + if (! ia64_platform_is (" sn2 ")) <nl> + return - ENODEV ; <nl> + <nl> // Put driver into chrdevs []. Get major number . <nl> rv = register_chrdev ( mbcs_major , DEVICE_NAME , & mbcs_ops ); <nl> if ( rv < 0 ) {
void xenbus_dev_changed ( const char * node , struct xen_bus_type * bus ) <nl>  <nl> kfree ( root ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( xenbus_dev_changed ); <nl>  <nl> static void frontend_changed ( struct xenbus_watch * watch , <nl> const char ** vec , unsigned int len )
static int at76_iw_handler_get_scan ( struct net_device * netdev , <nl> if (! iwe ) <nl> return - ENOMEM ; <nl>  <nl> - if ( priv -> scan_state != SCAN_COMPLETED ) <nl> + if ( priv -> scan_state != SCAN_COMPLETED ) { <nl> /* scan not yet finished */ <nl> + kfree ( iwe ); <nl> return - EAGAIN ; <nl> + } <nl>  <nl> spin_lock_irqsave (& priv -> bss_list_spinlock , flags ); <nl> 
int i915_gpu_idle ( struct drm_device * dev ) <nl> /* Is the device fubar ? */ <nl> if ( WARN_ON (! list_empty (& ring -> gpu_write_list ))) <nl> return - EBUSY ; <nl> + <nl> + ret = i915_switch_context ( ring , NULL , DEFAULT_CONTEXT_ID ); <nl> + if ( ret ) <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static int fuse_dentry_revalidate ( struct dentry * entry , struct nameidata * nd ) <nl> { <nl> struct inode * inode ; <nl>  <nl> - if ( nd -> flags & LOOKUP_RCU ) <nl> + if ( nd && nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl>  <nl> inode = entry -> d_inode ;
static int submit_extent_page ( int rw , struct extent_io_tree * tree , <nl> return 0 ; <nl> } <nl> } <nl> - nr = min_t ( int , max_pages , bio_get_nr_vecs ( bdev )); <nl> + nr = bio_get_nr_vecs ( bdev ); <nl> bio = extent_bio_alloc ( bdev , sector , nr , GFP_NOFS | __GFP_HIGH ); <nl> if (! bio ) { <nl> printk (" failed to allocate bio nr % d \ n ", nr );
int main ( int ac , char ** av ) <nl> single_menu_mode = 1 ; <nl> } <nl>  <nl> + initscr (); <nl> + <nl> getyx ( stdscr , saved_y , saved_x ); <nl> if ( init_dialog ( NULL )) { <nl> fprintf ( stderr , N_ (" Your display is too small to run Menuconfig !\ n "));
static int ext4_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> EXT4_FEATURE_INCOMPAT_FILETYPE )) <nl> new_de -> file_type = old_de -> file_type ; <nl> new_dir -> i_version ++; <nl> + new_dir -> i_ctime = new_dir -> i_mtime = <nl> + ext4_current_time ( new_dir ); <nl> + ext4_mark_inode_dirty ( handle , new_dir ); <nl> BUFFER_TRACE ( new_bh , " call ext4_journal_dirty_metadata "); <nl> ext4_journal_dirty_metadata ( handle , new_bh ); <nl> brelse ( new_bh );
static int ep93xx_gpio_irq_type ( struct irq_data * d , unsigned int type ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - __irq_set_handler_locked ( d -> irq , handler ); <nl> + irq_set_handler_locked ( d , handler ); <nl>  <nl> gpio_int_enabled [ port ] |= port_mask ; <nl> 
out : <nl> return err ; <nl> no_route : <nl> kfree_skb ( nskb ); <nl> - IP_INC_STATS_BH ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl> + IP_INC_STATS ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl>  <nl> /* FIXME : Returning the ' err ' will effect all the associations <nl> * associated with a socket , although only one of the paths of the
extern long __put_user_asm_l ( void *, long ); <nl> extern long __put_user_asm_q ( void *, long ); <nl> extern void __put_user_unknown ( void ); <nl>  <nl> + extern long __strnlen_user ( const char * __s , long __n ); <nl> + <nl> # endif /* __ASM_SH_UACCESS_64_H */
static void notify_ring ( struct drm_device * dev , <nl> struct intel_ring_buffer * ring ) <nl> { <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> - u32 seqno = ring -> get_seqno ( ring ); <nl> + u32 seqno ; <nl> + <nl> + if ( ring -> obj == NULL ) <nl> + return ; <nl>  <nl> + seqno = ring -> get_seqno ( ring ); <nl> trace_i915_gem_request_complete ( dev , seqno ); <nl>  <nl> ring -> irq_seqno = seqno ;
static int hdmi_remove ( struct platform_device * pdev ) <nl>  <nl> pm_runtime_disable ( dev ); <nl>  <nl> - free_irq ( hdata -> irq , hdata ); <nl> + free_irq ( hdata -> irq , ctx ); <nl>  <nl>  <nl> /* hdmiphy i2c driver */
void rt2x00rfkill_allocate ( struct rt2x00_dev * rt2x00dev ) <nl>  <nl> void rt2x00rfkill_free ( struct rt2x00_dev * rt2x00dev ) <nl> { <nl> - if (! test_bit ( RFKILL_STATE_ALLOCATED , & rt2x00dev -> flags )) <nl> + if (! test_bit ( RFKILL_STATE_ALLOCATED , & rt2x00dev -> rfkill_state )) <nl> return ; <nl>  <nl> cancel_delayed_work_sync (& rt2x00dev -> rfkill_work );
int twl4030_madc_conversion ( struct twl4030_madc_request * req ) <nl> u8 ch_msb , ch_lsb ; <nl> int ret ; <nl>  <nl> - if (! req ) <nl> + if (! req || ! twl4030_madc ) <nl> return - EINVAL ; <nl> + <nl> mutex_lock (& twl4030_madc -> lock ); <nl> if ( req -> method < TWL4030_MADC_RT || req -> method > TWL4030_MADC_SW2 ) { <nl> ret = - EINVAL ;
static int clk_fetch_parent_index ( struct clk_core * core , <nl> { <nl> int i ; <nl>  <nl> + if (! parent ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * find index of new parent clock using cached parent ptrs , <nl> * or if not yet cached , use string name comparison and cache
static int soc_post_component_init ( struct snd_soc_card * card , <nl>  <nl> # ifdef CONFIG_DEBUG_FS <nl> /* add DPCM sysfs entries */ <nl> - if (! dai_link -> dynamic ) <nl> + if (! dailess && ! dai_link -> dynamic ) <nl> goto out ; <nl>  <nl> ret = soc_dpcm_debugfs_add ( rtd );
static int gprs_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> dev -> name , err ); <nl> dev -> stats . tx_aborted_errors ++; <nl> dev -> stats . tx_errors ++; <nl> - dev_kfree_skb ( skb ); <nl> } else { <nl> dev -> stats . tx_packets ++; <nl> dev -> stats . tx_bytes += len ;
static int rocker_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl> goto err_probe_ports ; <nl> } <nl>  <nl> - dev_info (& pdev -> dev , " Rocker switch with id % 016llx \ n ", rocker -> hw . id ); <nl> + dev_info (& pdev -> dev , " Rocker switch with id %* phN \ n ", <nl> + ( int ) sizeof ( rocker -> hw . id ), & rocker -> hw . id ); <nl>  <nl> return 0 ; <nl> 
static int __init thermal_init ( void ) <nl> idr_destroy (& thermal_cdev_idr ); <nl> mutex_destroy (& thermal_idr_lock ); <nl> mutex_destroy (& thermal_list_lock ); <nl> + return result ; <nl> } <nl> result = genetlink_init (); <nl> return result ;
SYSCALL_DEFINE3 ( sched_setattr , pid_t , pid , struct sched_attr __user *, uattr , <nl> if ( retval ) <nl> return retval ; <nl>  <nl> + if ( attr . sched_policy < 0 ) <nl> + return - EINVAL ; <nl> + <nl> rcu_read_lock (); <nl> retval = - ESRCH ; <nl> p = find_process_by_pid ( pid );
static int annotate_browser__run ( struct annotate_browser * browser , <nl> nd = browser -> curr_hot ; <nl> break ; <nl> case K_UNTAB : <nl> - if ( nd != NULL ) <nl> + if ( nd != NULL ) { <nl> nd = rb_next ( nd ); <nl> if ( nd == NULL ) <nl> nd = rb_first (& browser -> entries ); <nl> - else <nl> + } else <nl> nd = browser -> curr_hot ; <nl> break ; <nl> case K_F1 :
postchange : <nl> current_multiplier ); <nl> } <nl> # endif <nl> + if ( err ) <nl> + freqs . new = freqs . old ; <nl> + <nl> cpufreq_notify_transition ( policy , & freqs , CPUFREQ_POSTCHANGE ); <nl> return err ; <nl> }
static int wm8731_set_bias_level ( struct snd_soc_codec * codec , <nl>  <nl> switch ( level ) { <nl> case SND_SOC_BIAS_ON : <nl> - if ( wm8731 -> mclk ) <nl> - clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( wm8731 -> mclk ) { <nl> + ret = clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( ret ) <nl> + return ret ; <nl> + } <nl> break ; <nl> case SND_SOC_BIAS_PREPARE : <nl> break ;
int hists__browse ( struct hists * self , const char * helpline , const char * ev_name ) <nl>  <nl> switch ( key ) { <nl> case ' a ': <nl> - if ( browser -> selection -> map == NULL && <nl> + if ( browser -> selection -> map == NULL || <nl> browser -> selection -> map -> dso -> annotate_warned ) <nl> continue ; <nl> goto do_annotate ;
static int snd_pcm_drain ( struct snd_pcm_substream * substream ) <nl>  <nl> snd_pcm_stream_lock_irq ( substream ); <nl> /* resume pause */ <nl> - if ( runtime -> status -> state == SNDRV_PCM_STATE_PAUSED ) <nl> + if ( substream -> runtime -> status -> state == SNDRV_PCM_STATE_PAUSED ) <nl> snd_pcm_pause ( substream , 0 ); <nl>  <nl> /* pre - start / stop - all running streams are changed to DRAINING state */
static void request_key_auth_destroy ( struct key * key ) <nl> kenter ("{% d }", key -> serial ); <nl>  <nl> key_put ( rka -> target_key ); <nl> + kfree ( rka ); <nl>  <nl> } /* end request_key_auth_destroy () */ <nl> 
static int io_subchannel_sch_event ( struct subchannel * sch , int process ) <nl> goto out ; <nl> break ; <nl> case IO_SCH_UNREG_ATTACH : <nl> + if ( cdev -> private -> flags . resuming ) { <nl> + /* Device will be handled later . */ <nl> + rc = 0 ; <nl> + goto out ; <nl> + } <nl> /* Unregister ccw device . */ <nl> - if (! cdev -> private -> flags . resuming ) <nl> - ccw_device_unregister ( cdev ); <nl> + ccw_device_unregister ( cdev ); <nl> break ; <nl> default : <nl> break ;
hpet_ioctl_common ( struct hpet_dev * devp , int cmd , unsigned long arg , <nl> break ; <nl> case HPET_INFO : <nl> { <nl> + memset ( info , 0 , sizeof (* info )); <nl> if ( devp -> hd_ireqfreq ) <nl> info -> hi_ireqfreq = <nl> hpet_time_div ( hpetp , devp -> hd_ireqfreq ); <nl> - else <nl> - info -> hi_ireqfreq = 0 ; <nl> info -> hi_flags = <nl> readq (& timer -> hpet_config ) & Tn_PER_INT_CAP_MASK ; <nl> info -> hi_hpet = hpetp -> hp_which ;
static int __devinit cciss_init_one ( struct pci_dev * pdev , <nl> h -> scatter_list = kmalloc ( h -> max_commands * <nl> sizeof ( struct scatterlist *), <nl> GFP_KERNEL ); <nl> + if (! h -> scatter_list ) <nl> + goto clean4 ; <nl> + <nl> for ( k = 0 ; k < h -> nr_cmds ; k ++) { <nl> h -> scatter_list [ k ] = kmalloc ( sizeof ( struct scatterlist ) * <nl> h -> maxsgentries ,
int ext4_mb_find_by_goal ( struct ext4_allocation_context * ac , <nl> int max ; <nl> int err ; <nl> struct ext4_sb_info * sbi = EXT4_SB ( ac -> ac_sb ); <nl> + struct ext4_group_info * grp = ext4_get_group_info ( ac -> ac_sb , group ); <nl> struct ext4_free_extent ex ; <nl>  <nl> if (!( ac -> ac_flags & EXT4_MB_HINT_TRY_GOAL )) <nl> return 0 ; <nl> + if ( grp -> bb_free == 0 ) <nl> + return 0 ; <nl>  <nl> err = ext4_mb_load_buddy ( ac -> ac_sb , group , e4b ); <nl> if ( err )
static int adp5588_gpio_probe ( struct i2c_client * client , <nl> } <nl>  <nl> dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> - if ( dev == NULL ) { <nl> - dev_err (& client -> dev , " failed to alloc memory \ n "); <nl> + if ( dev == NULL ) <nl> return - ENOMEM ; <nl> - } <nl>  <nl> dev -> client = client ; <nl> 
static int drm_helper_probe_single_connector_modes_merge_bits ( struct drm_connect <nl> mode -> status = MODE_UNVERIFIED ; <nl>  <nl> if ( connector -> force ) { <nl> - if ( connector -> force == DRM_FORCE_ON ) <nl> + if ( connector -> force == DRM_FORCE_ON || <nl> + connector -> force == DRM_FORCE_ON_DIGITAL ) <nl> connector -> status = connector_status_connected ; <nl> else <nl> connector -> status = connector_status_disconnected ;
static int ab8500_usb_probe ( struct platform_device * pdev ) <nl> return err ; <nl> } <nl>  <nl> - /* Phy tuning values for AB8500 */ <nl> - if (! is_ab8500_2p0_or_earlier ( ab -> ab8500 )) { <nl> + /* Phy tuning values for AB8500 > v2 . 0 */ <nl> + if ( is_ab8500 ( ab -> ab8500 ) && ! is_ab8500_2p0_or_earlier ( ab -> ab8500 )) { <nl> /* Enable the PBT / Bank 0x12 access */ <nl> err = abx500_set_register_interruptible ( ab -> dev , <nl> AB8500_DEVELOPMENT , AB8500_BANK12_ACCESS , 0x01 );
static long btrfs_ioctl_qgroup_assign ( struct file * file , void __user * arg ) <nl> sa -> src , sa -> dst ); <nl> } <nl>  <nl> + /* update qgroup status and info */ <nl> + err = btrfs_run_qgroups ( trans , root -> fs_info ); <nl> + if ( err < 0 ) <nl> + btrfs_error ( root -> fs_info , ret , <nl> + " failed to update qgroup status and info \ n "); <nl> err = btrfs_end_transaction ( trans , root ); <nl> if ( err && ! ret ) <nl> ret = err ;
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
int ath10k_wmi_event_mgmt_rx ( struct ath10k * ar , struct sk_buff * skb ) <nl> ret = ath10k_wmi_pull_mgmt_rx ( ar , skb , & arg ); <nl> if ( ret ) { <nl> ath10k_warn ( ar , " failed to parse mgmt rx event : % d \ n ", ret ); <nl> + dev_kfree_skb ( skb ); <nl> return ret ; <nl> } <nl> 
long drm_ioctl ( struct file * filp , <nl> goto err_i1 ; <nl> } <nl> } <nl> + if ( asize > usize ) <nl> + memset ( kdata + usize , 0 , asize - usize ); <nl> } <nl>  <nl> if ( cmd & IOC_IN ) {
static const char * alc_get_line_out_pfx ( struct alc_spec * spec , int ch , <nl> case AUTO_PIN_SPEAKER_OUT : <nl> if ( cfg -> line_outs == 1 ) <nl> return " Speaker "; <nl> + if ( cfg -> line_outs == 2 ) <nl> + return ch ? " Bass Speaker " : " Speaker "; <nl> break ; <nl> case AUTO_PIN_HP_OUT : <nl> /* for multi - io case , only the primary out */
static int machines__deliver_event ( struct machines * machines , <nl>  <nl> switch ( event -> header . type ) { <nl> case PERF_RECORD_SAMPLE : <nl> - dump_sample ( evsel , event , sample ); <nl> if ( evsel == NULL ) { <nl> ++ evlist -> stats . nr_unknown_id ; <nl> return 0 ; <nl> } <nl> + dump_sample ( evsel , event , sample ); <nl> if ( machine == NULL ) { <nl> ++ evlist -> stats . nr_unprocessable_samples ; <nl> return 0 ;
static int da9052_rtc_probe ( struct platform_device * pdev ) <nl> return ret ; <nl> } <nl>  <nl> + device_init_wakeup (& pdev -> dev , true ); <nl> + <nl> rtc -> rtc = devm_rtc_device_register (& pdev -> dev , pdev -> name , <nl> & da9052_rtc_ops , THIS_MODULE ); <nl> return PTR_ERR_OR_ZERO ( rtc -> rtc );
static int pwm_omap_dmtimer_probe ( struct platform_device * pdev ) <nl>  <nl> omap = devm_kzalloc (& pdev -> dev , sizeof (* omap ), GFP_KERNEL ); <nl> if (! omap ) { <nl> - omap -> pdata -> free ( dm_timer ); <nl> + pdata -> free ( dm_timer ); <nl> return - ENOMEM ; <nl> } <nl> 
static int ad198x_build_pcms ( struct hda_codec * codec ) <nl> if ( spec -> multiout . dig_out_nid ) { <nl> info ++; <nl> codec -> num_pcms ++; <nl> + codec -> spdif_status_reset = 1 ; <nl> info -> name = " AD198x Digital "; <nl> info -> pcm_type = HDA_PCM_TYPE_SPDIF ; <nl> info -> stream [ SNDRV_PCM_STREAM_PLAYBACK ] = ad198x_pcm_digital_playback ;
exit_snd_soc : <nl> exit_free_irq : <nl> free_irq ( irq , master ); <nl> exit_fsib : <nl> + pm_runtime_disable (& pdev -> dev ); <nl> fsi_stream_remove (& master -> fsib ); <nl> exit_fsia : <nl> fsi_stream_remove (& master -> fsia ); <nl> exit_iounmap : <nl> iounmap ( master -> base ); <nl> - pm_runtime_disable (& pdev -> dev ); <nl> exit_kfree : <nl> kfree ( master ); <nl> master = NULL ;
static int t7l66xb_probe ( struct platform_device * dev ) <nl> t7l66xb_cells [ T7L66XB_CELL_NAND ]. data_size = <nl> sizeof ( t7l66xb_cells [ T7L66XB_CELL_NAND ]); <nl>  <nl> + t7l66xb_cells [ T7L66XB_CELL_MMC ]. platform_data = <nl> + & t7l66xb_cells [ T7L66XB_CELL_MMC ]; <nl> + t7l66xb_cells [ T7L66XB_CELL_MMC ]. data_size = <nl> + sizeof ( t7l66xb_cells [ T7L66XB_CELL_MMC ]); <nl> + <nl> ret = mfd_add_devices (& dev -> dev , dev -> id , <nl> t7l66xb_cells , ARRAY_SIZE ( t7l66xb_cells ), <nl> iomem , t7l66xb -> irq_base );
static int parse_raid_params ( struct raid_set * rs , char ** argv , <nl> rs -> ti -> error = " write_mostly option is only valid for RAID1 "; <nl> return - EINVAL ; <nl> } <nl> - if ( value > rs -> md . raid_disks ) { <nl> + if ( value >= rs -> md . raid_disks ) { <nl> rs -> ti -> error = " Invalid write_mostly drive index given "; <nl> return - EINVAL ; <nl> }
void __init at91_add_device_serial ( void ) <nl> printk ( KERN_INFO " AT91 : No default serial console defined .\ n "); <nl> } <nl> # else <nl> - void __init __deprecated at91_init_serial ( struct at91_uart_config * config ) {} <nl> void __init at91_register_uart ( unsigned id , unsigned portnr , unsigned pins ) {} <nl> void __init at91_set_serial_console ( unsigned portnr ) {} <nl> void __init at91_add_device_serial ( void ) {}
unsigned int qe_get_num_of_snums ( void ) <nl> if (( num_of_snums < 28 ) || ( num_of_snums > QE_NUM_OF_SNUM )) { <nl> /* No QE ever has fewer than 28 SNUMs */ <nl> pr_err (" QE : number of snum is invalid \ n "); <nl> + of_node_put ( qe ); <nl> return - EINVAL ; <nl> } <nl> }
static netdev_tx_t r6040_start_xmit ( struct sk_buff * skb , <nl> /* Set TX descriptor & Transmit it */ <nl> lp -> tx_free_desc --; <nl> descptr = lp -> tx_insert_ptr ; <nl> - if ( skb -> len < MISR ) <nl> - descptr -> len = MISR ; <nl> + if ( skb -> len < ETH_ZLEN ) <nl> + descptr -> len = ETH_ZLEN ; <nl> else <nl> descptr -> len = skb -> len ; <nl> 
static int __init fusb300_probe ( struct platform_device * pdev ) <nl>  <nl> fusb300 -> ep0_req = fusb300_alloc_request (& fusb300 -> ep [ 0 ]-> ep , <nl> GFP_KERNEL ); <nl> - if ( fusb300 -> ep0_req == NULL ) <nl> + if ( fusb300 -> ep0_req == NULL ) { <nl> + ret = - ENOMEM ; <nl> goto clean_up3 ; <nl> + } <nl>  <nl> init_controller ( fusb300 ); <nl> ret = usb_add_gadget_udc (& pdev -> dev , & fusb300 -> gadget );
static void atombios_crtc_program_ss ( struct drm_crtc * crtc , <nl> case ATOM_PPLL_INVALID : <nl> return ; <nl> } <nl> - args . v2 . ucEnable = enable ; <nl> + args . v3 . ucEnable = enable ; <nl> if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK )) <nl> args . v3 . ucEnable = ATOM_DISABLE ; <nl> } else if ( ASIC_IS_DCE4 ( rdev )) {
static int setup_routing_entry ( struct kvm_irq_routing_table * rt , <nl> */ <nl> hlist_for_each_entry ( ei , n , & rt -> map [ ue -> gsi ], link ) <nl> if ( ei -> type == KVM_IRQ_ROUTING_MSI || <nl> + ue -> type == KVM_IRQ_ROUTING_MSI || <nl> ue -> u . irqchip . irqchip == ei -> irqchip . irqchip ) <nl> return r ; <nl> 
static int dma_set_runtime_config ( struct dma_chan * chan , <nl> u32 cctl = 0 ; <nl> int i ; <nl>  <nl> + if (! plchan -> slave ) <nl> + return - EINVAL ; <nl> + <nl> /* Transfer direction */ <nl> plchan -> runtime_direction = config -> direction ; <nl> if ( config -> direction == DMA_TO_DEVICE ) {
void btrfs_add_ordered_operation ( struct btrfs_trans_handle * trans , <nl> * if this file hasn ' t been changed since the last transaction <nl> * commit , we can safely return without doing anything <nl> */ <nl> - if ( last_mod < root -> fs_info -> last_trans_committed ) <nl> + if ( last_mod <= root -> fs_info -> last_trans_committed ) <nl> return ; <nl>  <nl> spin_lock (& root -> fs_info -> ordered_root_lock );
int __init option_setup ( char * str ) <nl>  <nl> TRACE2 ((" option_setup () str % s \ n ", str ? str :" NULL ")); <nl>  <nl> - while ( cur && isdigit (* cur ) && i <= MAXHA ) { <nl> + while ( cur && isdigit (* cur ) && i < MAXHA ) { <nl> ints [ i ++] = simple_strtoul ( cur , NULL , 0 ); <nl> if (( cur = strchr ( cur , ',')) != NULL ) cur ++; <nl> }
int __init init_dmars ( void ) <nl> deferred_flush = kzalloc ( g_num_of_iommus * <nl> sizeof ( struct deferred_flush_tables ), GFP_KERNEL ); <nl> if (! deferred_flush ) { <nl> - kfree ( g_iommus ); <nl> ret = - ENOMEM ; <nl> goto error ; <nl> }
static int ata_dev_read_id ( struct ata_port * ap , struct ata_device * dev , <nl> err_out : <nl> printk ( KERN_WARNING " ata % u : dev % u failed to IDENTIFY (% s )\ n ", <nl> ap -> id , dev -> devno , reason ); <nl> - kfree ( id ); <nl> return rc ; <nl> } <nl> 
static int __devexit mxcnd_remove ( struct platform_device * pdev ) <nl> static struct platform_driver mxcnd_driver = { <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> + . owner = THIS_MODULE , <nl> }, <nl> . remove = __devexit_p ( mxcnd_remove ), <nl> };
static int arizona_dai_set_sysclk ( struct snd_soc_dai * dai , <nl> routes [ 1 ]. source = arizona_dai_clk_str ( clk_id ); <nl> snd_soc_dapm_add_routes (& codec -> dapm , routes , ARRAY_SIZE ( routes )); <nl>  <nl> + dai_priv -> clk = clk_id ; <nl> + <nl> return snd_soc_dapm_sync (& codec -> dapm ); <nl> } <nl> 
void of_gpiochip_add ( struct gpio_chip * chip ) <nl> void of_gpiochip_remove ( struct gpio_chip * chip ) <nl> { <nl> gpiochip_remove_pin_ranges ( chip ); <nl> - <nl> - if ( chip -> of_node ) <nl> - of_node_put ( chip -> of_node ); <nl> + of_node_put ( chip -> of_node ); <nl> }
static void stop_nop_trace ( struct trace_array * tr ) <nl>  <nl> static void nop_trace_init ( struct trace_array * tr ) <nl> { <nl> + int cpu ; <nl> ctx_trace = tr ; <nl>  <nl> + for_each_online_cpu ( cpu ) <nl> + tracing_reset ( tr -> data [ cpu ]); <nl> + <nl> if ( tr -> ctrl ) <nl> start_nop_trace ( tr ); <nl> }
static void __init sanity_check_meminfo ( void ) <nl> bank -> size = VMALLOC_MIN - __va ( bank -> start ); <nl> } <nl> # else <nl> + bank -> highmem = highmem ; <nl> + <nl> /* <nl> * Check whether this memory bank would entirely overlap <nl> * the vmalloc area .
void handle_ra_miss ( struct address_space * mapping , <nl> { <nl> ra -> flags |= RA_FLAG_MISS ; <nl> ra -> flags &= ~ RA_FLAG_INCACHE ; <nl> + ra -> cache_hit = 0 ; <nl> } <nl>  <nl> /*
static inline pgtable_t pte_alloc_one ( struct mm_struct * mm , unsigned long addres <nl> { <nl> struct page * pte ; <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO , 0 ); <nl> + if (! pte ) <nl> + return NULL ; <nl> pgtable_page_ctor ( pte ); <nl> return pte ; <nl> }
static int mmc_blk_ioctl_cmd ( struct block_device * bdev , <nl> md = mmc_blk_get ( bdev -> bd_disk ); <nl> if (! md ) { <nl> err = - EINVAL ; <nl> - goto cmd_done ; <nl> + goto cmd_err ; <nl> } <nl>  <nl> card = md -> queue . card ; <nl> cmd_rel_host : <nl>  <nl> cmd_done : <nl> mmc_blk_put ( md ); <nl> + cmd_err : <nl> kfree ( idata -> buf ); <nl> kfree ( idata ); <nl> return err ;
static int __init d40_lcla_allocate ( struct d40_base * base ) <nl>  <nl> d40_err ( base -> dev , " Failed to allocate % d pages .\ n ", <nl> base -> lcla_pool . pages ); <nl> + ret = - ENOMEM ; <nl>  <nl> for ( j = 0 ; j < i ; j ++) <nl> free_pages ( page_list [ j ], base -> lcla_pool . pages );
static int gpmc_probe_dt ( struct platform_device * pdev ) <nl> of_node_cmp ( child -> name , " nor ") == 0 ) <nl> ret = gpmc_probe_generic_child ( pdev , child ); <nl>  <nl> - if ( ret < 0 ) { <nl> + if ( WARN ( ret < 0 , "% s : probing gpmc child % s failed \ n ", <nl> + __func__ , child -> full_name )) <nl> of_node_put ( child ); <nl> - return ret ; <nl> - } <nl> } <nl>  <nl> return 0 ;
static int wm8995_probe ( struct snd_soc_codec * codec ) <nl>  <nl> if ( ret != 0x8995 ) { <nl> dev_err ( codec -> dev , " Invalid device ID : %# x \ n ", ret ); <nl> + ret = - EINVAL ; <nl> goto err_reg_enable ; <nl> } <nl> 
static int hpb_dmae_chan_probe ( struct hpb_dmae_device * hpbdev , int id ) <nl> } <nl>  <nl> schan = & new_hpb_chan -> shdma_chan ; <nl> + schan -> max_xfer_len = HPB_DMA_TCR_MAX ; <nl> + <nl> shdma_chan_probe ( sdev , schan , id ); <nl>  <nl> if ( pdev -> id >= 0 )
int rt2800_load_firmware ( struct rt2x00_dev * rt2x00dev , <nl> */ <nl> rt2800_register_write ( rt2x00dev , H2M_BBP_AGENT , 0 ); <nl> rt2800_register_write ( rt2x00dev , H2M_MAILBOX_CSR , 0 ); <nl> + if ( rt2x00_is_usb ( rt2x00dev )) <nl> + rt2800_register_write ( rt2x00dev , H2M_INT_SRC , 0 ); <nl> msleep ( 1 ); <nl>  <nl> return 0 ;
sctp_disposition_t sctp_sf_backbeat_8_3 ( const struct sctp_endpoint * ep , <nl> commands ); <nl>  <nl> hbinfo = ( sctp_sender_hb_info_t *) chunk -> skb -> data ; <nl> + /* Make sure that the length of the parameter is what we expect */ <nl> + if ( ntohs ( hbinfo -> param_hdr . length ) != <nl> + sizeof ( sctp_sender_hb_info_t )) { <nl> + return SCTP_DISPOSITION_DISCARD ; <nl> + } <nl> + <nl> from_addr = hbinfo -> daddr ; <nl> link = sctp_assoc_lookup_paddr ( asoc , & from_addr ); <nl> 
static int pipe_buffer_setting ( struct m66592 * m66592 , <nl> break ; <nl> case M66592_BULK : <nl> /* isochronous pipes may be used as bulk pipes */ <nl> - if ( info -> pipe > M66592_BASE_PIPENUM_BULK ) <nl> + if ( info -> pipe >= M66592_BASE_PIPENUM_BULK ) <nl> bufnum = info -> pipe - M66592_BASE_PIPENUM_BULK ; <nl> else <nl> bufnum = info -> pipe - M66592_BASE_PIPENUM_ISOC ;
struct phy_device * get_phy_device ( struct mii_bus * bus , int addr ) <nl> if ( r ) <nl> return ERR_PTR ( r ); <nl>  <nl> - /* If the phy_id is all Fs , there is no device there */ <nl> - if ( 0xffffffff == phy_id ) <nl> + /* If the phy_id is all Fs or all 0s , there is no device there */ <nl> + if (( 0xffff == phy_id ) || ( 0x00 == phy_id )) <nl> return NULL ; <nl>  <nl> dev = phy_device_create ( bus , addr , phy_id );
int usb_serial_register_drivers ( struct usb_serial_driver * const serial_drivers [] <nl>  <nl> /* we only set the reset_resume field if the serial_driver has one */ <nl> for ( sd = serial_drivers ; * sd ; ++ sd ) { <nl> - if ((* sd )-> reset_resume ) <nl> + if ((* sd )-> reset_resume ) { <nl> udriver -> reset_resume = usb_serial_reset_resume ; <nl> break ; <nl> + } <nl> } <nl>  <nl> rc = usb_register ( udriver );
static int i40evf_request_misc_irq ( struct i40evf_adapter * adapter ) <nl> int err ; <nl>  <nl> snprintf ( adapter -> misc_vector_name , <nl> - sizeof ( adapter -> misc_vector_name ) - 1 , " i40evf : mbx "); <nl> + sizeof ( adapter -> misc_vector_name ) - 1 , " i40evf -% s : mbx ", <nl> + dev_name (& adapter -> pdev -> dev )); <nl> err = request_irq ( adapter -> msix_entries [ 0 ]. vector , <nl> & i40evf_msix_aq , 0 , <nl> adapter -> misc_vector_name , netdev );
struct ieee80211_tx_info { <nl> } control ; <nl> struct { <nl> struct ieee80211_tx_rate rates [ IEEE80211_TX_MAX_RATES ]; <nl> - int ack_signal ; <nl> + s32 ack_signal ; <nl> u8 ampdu_ack_len ; <nl> u8 ampdu_len ; <nl> u8 antenna ; <nl> - /* 21 bytes free */ <nl> + void * status_driver_data [ 21 / sizeof ( void *)]; <nl> } status ; <nl> struct { <nl> struct ieee80211_tx_rate driver_rates [
no_firmware : <nl> "% s : please contact support @ connecttech . com \ n ", <nl> serial -> type -> description ); <nl> kfree ( result ); <nl> + kfree ( command ); <nl> return - ENODEV ; <nl>  <nl> no_command_private :
int ath9k_cmn_rx_skb_preprocess ( struct ath_common * common , <nl> { <nl> struct ath_hw * ah = common -> ah ; <nl>  <nl> + memset ( rx_status , 0 , sizeof ( struct ieee80211_rx_status )); <nl> if (! ath9k_rx_accept ( common , skb , rx_status , rx_stats , decrypt_error )) <nl> return - EINVAL ; <nl> 
static int wl1271_plt_init ( struct wl1271 * wl ) <nl> if ( ret < 0 ) <nl> goto out_free_memmap ; <nl>  <nl> + ret = wl1271_acx_sta_mem_cfg ( wl ); <nl> + if ( ret < 0 ) <nl> + goto out_free_memmap ; <nl> + <nl> /* Default fragmentation threshold */ <nl> ret = wl1271_acx_frag_threshold ( wl , wl -> conf . tx . frag_threshold ); <nl> if ( ret < 0 )
static int ax_probe ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_IRQ , 0 ); <nl> if ( res == NULL ) { <nl> dev_err (& pdev -> dev , " no IRQ specified \ n "); <nl> + ret = - ENXIO ; <nl> goto exit_mem ; <nl> } <nl> 
static int ext4_mb_init_per_dev_proc ( struct super_block * sb ) <nl> struct proc_dir_entry * proc ; <nl> char devname [ 64 ]; <nl>  <nl> + if ( proc_root_ext4 == NULL ) { <nl> + sbi -> s_mb_proc = NULL ; <nl> + return - EINVAL ; <nl> + } <nl> bdevname ( sb -> s_bdev , devname ); <nl> sbi -> s_mb_proc = proc_mkdir ( devname , proc_root_ext4 ); <nl> 
iwl_op_mode_mvm_start ( struct iwl_trans * trans , const struct iwl_cfg * cfg , <nl> } <nl> mvm -> sf_state = SF_UNINIT ; <nl> mvm -> low_latency_agg_frame_limit = 6 ; <nl> + mvm -> cur_ucode = IWL_UCODE_INIT ; <nl>  <nl> mutex_init (& mvm -> mutex ); <nl> mutex_init (& mvm -> d0i3_suspend_mutex );
static int zd1201_resume ( struct usb_interface * interface ) <nl> { <nl> struct zd1201 * zd = usb_get_intfdata ( interface ); <nl>  <nl> + if (! zd || ! zd -> dev ) <nl> + return - ENODEV ; <nl> + <nl> netif_device_attach ( zd -> dev ); <nl>  <nl> if ( zd -> was_enabled )
static const u32 cipher_suites [] = { <nl> }; <nl>  <nl> static const struct ieee80211_txrx_stypes <nl> - wilc_wfi_cfg80211_mgmt_types [ NL80211_IFTYPE_MAX ] = { <nl> + wilc_wfi_cfg80211_mgmt_types [ NUM_NL80211_IFTYPES ] = { <nl> [ NL80211_IFTYPE_STATION ] = { <nl> . tx = 0xffff , <nl> . rx = BIT ( IEEE80211_STYPE_ACTION >> 4 ) |
static int ioctl_send_response ( struct client * client , void * buffer ) <nl> if ( copy_from_user ( r -> data , u64_to_uptr ( request -> data ), <nl> r -> length )) { <nl> ret = - EFAULT ; <nl> + kfree ( r -> request ); <nl> goto out ; <nl> } <nl> fw_send_response ( client -> device -> card , r -> request ,
static int lpc18xx_pconf_set_i2c0 ( struct pinctrl_dev * pctldev , <nl> break ; <nl>  <nl> case PIN_CONFIG_INPUT_SCHMITT_ENABLE : <nl> - if ( param ) <nl> + if ( param_val ) <nl> * reg &= ~( LPC18XX_SCU_I2C0_ZIF << shift ); <nl> else <nl> * reg |= ( LPC18XX_SCU_I2C0_ZIF << shift ); <nl> static int lpc18xx_pconf_set_pin ( struct pinctrl_dev * pctldev , <nl> break ; <nl>  <nl> case PIN_CONFIG_INPUT_SCHMITT_ENABLE : <nl> - if ( param ) <nl> + if ( param_val ) <nl> * reg &= ~ LPC18XX_SCU_PIN_ZIF ; <nl> else <nl> * reg |= LPC18XX_SCU_PIN_ZIF ;
int drm_vblank_get ( struct drm_device * dev , int crtc ) <nl> unsigned long irqflags ; <nl> int ret = 0 ; <nl>  <nl> + if (! dev -> num_crtcs ) <nl> + return - EINVAL ; <nl> + <nl> if ( WARN_ON ( crtc >= dev -> num_crtcs )) <nl> return - EINVAL ; <nl> 
static struct config_item_type printer_func_type = { <nl>  <nl> static inline int gprinter_get_minor ( void ) <nl> { <nl> - return ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + int ret ; <nl> + <nl> + ret = ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + if ( ret >= PRINTER_MINORS ) { <nl> + ida_simple_remove (& printer_ida , ret ); <nl> + ret = - ENODEV ; <nl> + } <nl> + <nl> + return ret ; <nl> } <nl>  <nl> static inline void gprinter_put_minor ( int minor )
static int ath9k_start ( struct ieee80211_hw * hw ) <nl> DPRINTF ( sc , ATH_DBG_CONFIG , "% s : Starting driver with " <nl> " initial channel : % d MHz \ n ", __func__ , curchan -> center_freq ); <nl>  <nl> + memset (& sc -> sc_ht_info , 0 , sizeof ( struct ath_ht_info )); <nl> + <nl> /* setup initial channel */ <nl>  <nl> pos = ath_get_channel ( sc , curchan );
static int resizer_configure_output_win ( struct vpfe_resizer_device * resizer ) <nl>  <nl> outformat = & resizer -> resizer_a . formats [ RESIZER_PAD_SOURCE ]; <nl>  <nl> + memset (& output_specs , 0x0 , sizeof ( struct vpfe_rsz_output_spec )); <nl> output_specs . vst_y = param -> user_config . vst ; <nl> if ( outformat -> code == MEDIA_BUS_FMT_YDYUYDYV8_1X16 ) <nl> output_specs . vst_c = param -> user_config . vst ;
static int __devinit rtsx_probe ( struct pci_dev * pci , <nl> th = kthread_create ( rtsx_scan_thread , dev , " rtsx - scan "); <nl> if ( IS_ERR ( th )) { <nl> printk ( KERN_ERR " Unable to start the device - scanning thread \ n "); <nl> + complete (& dev -> scanning_done ); <nl> quiesce_and_remove_host ( dev ); <nl> err = PTR_ERR ( th ); <nl> goto errout ;
get_a_page : <nl> i_size_write ( inode , size ); <nl> inode -> i_mtime = inode -> i_atime = CURRENT_TIME ; <nl> mark_inode_dirty ( inode ); <nl> + set_bit ( QDF_REFRESH , & qd -> qd_flags ); <nl> return 0 ; <nl>  <nl> unlock_out :
static struct of_device_id octeon_cf_match [] = { <nl> }, <nl> {}, <nl> }; <nl> - MODULE_DEVICE_TABLE ( of , octeon_i2c_match ); <nl> + MODULE_DEVICE_TABLE ( of , octeon_cf_match ); <nl>  <nl> static struct platform_driver octeon_cf_driver = { <nl> . probe = octeon_cf_probe ,
static __init int samsung_gpiolib_init ( void ) <nl> s5p_register_gpioint_bank ( IRQ_GPIO_XA , 0 , IRQ_GPIO1_NR_GROUPS ); <nl> s5p_register_gpioint_bank ( IRQ_GPIO_XB , IRQ_GPIO1_NR_GROUPS , IRQ_GPIO2_NR_GROUPS ); <nl> # endif <nl> + } else { <nl> + WARN ( 1 , " Unknown SoC in gpio - samsung , no GPIOs added \ n "); <nl> + return - ENODEV ; <nl> } <nl>  <nl> return 0 ;
restart : <nl> goto out_free ; <nl> } <nl>  <nl> + if ( sk_filter ( other , skb ) < 0 ) { <nl> + /* Toss the packet but do not return any error to the sender */ <nl> + err = len ; <nl> + goto out_free ; <nl> + } <nl> + <nl> unix_state_lock ( other ); <nl> err = - EPERM ; <nl> if (! unix_may_send ( sk , other ))
int __devinit wl18xx_probe ( struct platform_device * pdev ) <nl> { <nl> struct wl1271 * wl ; <nl> struct ieee80211_hw * hw ; <nl> + struct wl18xx_priv * priv ; <nl>  <nl> - hw = wlcore_alloc_hw ( 0 ); <nl> + hw = wlcore_alloc_hw ( sizeof (* priv )); <nl> if ( IS_ERR ( hw )) { <nl> wl1271_error (" can ' t allocate hw "); <nl> return PTR_ERR ( hw );
static int pvscsi_queue_ring ( struct pvscsi_adapter * adapter , <nl> memcpy ( e -> cdb , cmd -> cmnd , e -> cdbLen ); <nl>  <nl> e -> tag = SIMPLE_QUEUE_TAG ; <nl> - if ( sdev -> tagged_supported && <nl> - ( cmd -> tag == HEAD_OF_QUEUE_TAG || <nl> - cmd -> tag == ORDERED_QUEUE_TAG )) <nl> - e -> tag = cmd -> tag ; <nl>  <nl> if ( cmd -> sc_data_direction == DMA_FROM_DEVICE ) <nl> e -> flags = PVSCSI_FLAG_CMD_DIR_TOHOST ;
int amdgpu_gem_userptr_ioctl ( struct drm_device * dev , void * data , <nl> AMDGPU_GEM_USERPTR_REGISTER )) <nl> return - EINVAL ; <nl>  <nl> - if (!( args -> flags & AMDGPU_GEM_USERPTR_ANONONLY ) || <nl> - !( args -> flags & AMDGPU_GEM_USERPTR_REGISTER )) { <nl> + if (!( args -> flags & AMDGPU_GEM_USERPTR_READONLY ) && ( <nl> + !( args -> flags & AMDGPU_GEM_USERPTR_ANONONLY ) || <nl> + !( args -> flags & AMDGPU_GEM_USERPTR_REGISTER ))) { <nl>  <nl> /* if we want to write to it we must require anonymous <nl> memory and install a MMU notifier */
pca963x_dt_init ( struct i2c_client * client , struct pca963x_chipdef * chip ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> for_each_child_of_node ( np , child ) { <nl> - struct led_info led ; <nl> + struct led_info led = {}; <nl> u32 reg ; <nl> int res ; <nl> 
static int f2fs_xattr_advise_set ( struct dentry * dentry , const char * name , <nl> return - EINVAL ; <nl>  <nl> F2FS_I ( inode )-> i_advise |= *( char *) value ; <nl> + mark_inode_dirty ( inode ); <nl> return 0 ; <nl> } <nl> 
asmlinkage long compat_sys_ppoll ( struct pollfd __user * ufds , <nl> } <nl>  <nl> if ( sigmask ) { <nl> - if ( sigsetsize |= sizeof ( compat_sigset_t )) <nl> + if ( sigsetsize != sizeof ( compat_sigset_t )) <nl> return - EINVAL ; <nl> if ( copy_from_user (& ss32 , sigmask , sizeof ( ss32 ))) <nl> return - EFAULT ;
int __init musb_platform_init ( struct musb * musb , void * board_data ) <nl>  <nl> usb_nop_xceiv_register (); <nl> musb -> xceiv = otg_get_transceiver (); <nl> - if (! musb -> xceiv ) <nl> + if (! musb -> xceiv ) { <nl> + gpio_free ( musb -> config -> gpio_vrsel ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> if ( ANOMALY_05000346 ) { <nl> bfin_write_USB_APHY_CALIB ( ANOMALY_05000346_value );
again : <nl> */ <nl> if ( type != TRANS_JOIN_NOLOCK && <nl> ! __sb_start_write ( root -> fs_info -> sb , SB_FREEZE_FS , false )) { <nl> - if ( type == TRANS_JOIN_FREEZE ) <nl> + if ( type == TRANS_JOIN_FREEZE ) { <nl> + kmem_cache_free ( btrfs_trans_handle_cachep , h ); <nl> return ERR_PTR (- EPERM ); <nl> + } <nl> sb_start_intwrite ( root -> fs_info -> sb ); <nl> } <nl> 
static void ext4_mb_group_or_file ( struct ext4_allocation_context * ac ) <nl> return ; <nl> } <nl>  <nl> + if ( sbi -> s_mb_group_prealloc <= 0 ) { <nl> + ac -> ac_flags |= EXT4_MB_STREAM_ALLOC ; <nl> + return ; <nl> + } <nl> + <nl> /* don ' t use group allocation for large files */ <nl> size = max ( size , isize ); <nl> if ( size > sbi -> s_mb_stream_request ) {
static ssize_t ac_read ( struct file * filp , char __user * buf , size_t count , loff_ <nl> struct mailbox mailbox ; <nl>  <nl> /* Got a packet for us */ <nl> + memset (& st_loc , 0 , sizeof ( st_loc )); <nl> ret = do_ac_read ( i , buf , & st_loc , & mailbox ); <nl> spin_unlock_irqrestore (& apbs [ i ]. mutex , flags ); <nl> set_current_state ( TASK_RUNNING );
static int rcar_vin_videobuf_setup ( struct vb2_queue * vq , <nl> unsigned int bytes_per_line ; <nl> int ret ; <nl>  <nl> + if ( fmt -> fmt . pix . sizeimage < icd -> sizeimage ) <nl> + return - EINVAL ; <nl> + <nl> xlate = soc_camera_xlate_by_fourcc ( icd , <nl> fmt -> fmt . pix . pixelformat ); <nl> if (! xlate )
static void * raid5_takeover ( mddev_t * mddev ) <nl>  <nl> if ( mddev -> level == 1 ) <nl> return raid5_takeover_raid1 ( mddev ); <nl> + if ( mddev -> level == 4 ) { <nl> + mddev -> new_layout = ALGORITHM_PARITY_N ; <nl> + mddev -> new_level = 5 ; <nl> + return setup_conf ( mddev ); <nl> + } <nl>  <nl> return ERR_PTR (- EINVAL ); <nl> }
static int nokia_modem_probe ( struct device * dev ) <nl> return - ENOMEM ; <nl> } <nl> dev_set_drvdata ( dev , modem ); <nl> + modem -> device = dev ; <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> if (! irq ) {
drm_property_create_blob ( struct drm_device * dev , size_t length , <nl> struct drm_property_blob * blob ; <nl> int ret ; <nl>  <nl> - if (! length ) <nl> + if (! length || length > ULONG_MAX - sizeof ( struct drm_property_blob )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> blob = kzalloc ( sizeof ( struct drm_property_blob )+ length , GFP_KERNEL );
nvc0_fifo_init ( struct nouveau_object * object ) <nl> nv_wr32 ( priv , 0x002a00 , 0xffffffff ); /* clears PFIFO . INTR bit 30 */ <nl> nv_wr32 ( priv , 0x002100 , 0xffffffff ); <nl> nv_wr32 ( priv , 0x002140 , 0x3fffffff ); <nl> + nv_wr32 ( priv , 0x002628 , 0x00000001 ); /* makes mthd 0x20 work */ <nl> return 0 ; <nl> } <nl> 
static int ext4_fill_super ( struct super_block * sb , void * data , int silent ) <nl> * of the filesystem . <nl> */ <nl> if ( le32_to_cpu ( es -> s_first_data_block ) >= ext4_blocks_count ( es )) { <nl> - ext4_msg ( sb , KERN_WARNING , " bad geometry : first data " <nl> + ext4_msg ( sb , KERN_WARNING , " bad geometry : first data " <nl> " block % u is beyond end of filesystem (% llu )", <nl> le32_to_cpu ( es -> s_first_data_block ), <nl> ext4_blocks_count ( es ));
int add_extent_mapping ( struct extent_map_tree * tree , <nl> rb = tree_insert (& tree -> map , em -> start , & em -> rb_node ); <nl> if ( rb ) { <nl> ret = - EEXIST ; <nl> - free_extent_map ( merge ); <nl> goto out ; <nl> } <nl> atomic_inc (& em -> refs );
static inline struct ata_link * ata_port_next_link ( struct ata_link * link ) <nl> return ap -> pmp_link ; <nl> } <nl>  <nl> - if (++ link - ap -> pmp_link < ap -> nr_pmp_links ) <nl> + if (++ link < ap -> nr_pmp_links + ap -> pmp_link ) <nl> return link ; <nl> return NULL ; <nl> }
again : <nl> smp_mb (); <nl> if ( cur_trans -> state >= TRANS_STATE_BLOCKED && <nl> may_wait_transaction ( root , type )) { <nl> + current -> journal_info = h ; <nl> btrfs_commit_transaction ( h , root ); <nl> goto again ; <nl> }
static const struct snd_pci_quirk alc269_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x103c , 0x228a , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228b , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228c , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> + SND_PCI_QUIRK ( 0x103c , 0x228d , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x228e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22c5 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22c6 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ),
bte_result_t bte_unaligned_copy ( u64 src , u64 dest , u64 len , u64 mode ) <nl> } <nl>  <nl> /* temporary buffer used during unaligned transfers */ <nl> - bteBlock_unaligned = kmalloc ( len + 3 * L1_CACHE_BYTES , <nl> - GFP_KERNEL | GFP_DMA ); <nl> + bteBlock_unaligned = kmalloc ( len + 3 * L1_CACHE_BYTES , GFP_KERNEL ); <nl> if ( bteBlock_unaligned == NULL ) { <nl> return BTEFAIL_NOTAVAIL ; <nl> }
int snd_hda_create_dig_out_ctls ( struct hda_codec * codec , <nl> return - EBUSY ; <nl> } <nl> spdif = snd_array_new (& codec -> spdif_out ); <nl> + if (! spdif ) <nl> + return - ENOMEM ; <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> if (! kctl )
int wl1271_init_ieee80211 ( struct wl1271 * wl ) <nl> wl -> hw -> wiphy -> max_scan_ie_len = WL1271_CMD_TEMPL_DFLT_SIZE - <nl> sizeof ( struct ieee80211_header ); <nl>  <nl> + wl -> hw -> wiphy -> max_sched_scan_ie_len = WL1271_CMD_TEMPL_DFLT_SIZE - <nl> + sizeof ( struct ieee80211_header ); <nl> + <nl> wl -> hw -> wiphy -> flags |= WIPHY_FLAG_AP_UAPSD ; <nl>  <nl> /* make sure all our channels fit in the scanned_ch bitmask */
static inline void * __alloc_percpu ( size_t size , size_t align ) <nl> * percpu sections on SMP for which this path isn ' t used . <nl> */ <nl> WARN_ON_ONCE ( align > __alignof__ ( unsigned long long )); <nl> - return kzalloc ( size , gfp ); <nl> + return kzalloc ( size , GFP_KERNEL ); <nl> } <nl>  <nl> static inline void free_percpu ( void * p )
static int ir_lirc_unregister ( struct rc_dev * dev ) <nl>  <nl> lirc_unregister_driver ( lirc -> drv -> minor ); <nl> lirc_buffer_free ( lirc -> drv -> rbuf ); <nl> + kfree ( lirc -> drv -> rbuf ); <nl> kfree ( lirc -> drv ); <nl>  <nl> return 0 ;
int kvm_vgic_create ( struct kvm * kvm , u32 type ) <nl> * emulation . So check this here again . KVM_CREATE_DEVICE does <nl> * the proper checks already . <nl> */ <nl> - if ( type == KVM_DEV_TYPE_ARM_VGIC_V2 && ! vgic -> can_emulate_gicv2 ) <nl> - return - ENODEV ; <nl> + if ( type == KVM_DEV_TYPE_ARM_VGIC_V2 && ! vgic -> can_emulate_gicv2 ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl>  <nl> /* <nl> * Any time a vcpu is run , vcpu_load is called which tries to grab the
unsigned int solo_eeprom_ewen ( struct solo_dev * solo_dev , int w_en ) <nl> __be16 solo_eeprom_read ( struct solo_dev * solo_dev , int loc ) <nl> { <nl> int read_cmd = loc | ( EE_READ_CMD << ADDR_LEN ); <nl> - unsigned short retval = 0 ; <nl> + u16 retval = 0 ; <nl> int i ; <nl>  <nl> solo_eeprom_cmd ( solo_dev , read_cmd );
static int snd_asihpi_cmode_info ( struct snd_kcontrol * kcontrol , <nl> valid_modes ++; <nl> } <nl>  <nl> + if (! valid_modes ) <nl> + return - EINVAL ; <nl> + <nl> uinfo -> type = SNDRV_CTL_ELEM_TYPE_ENUMERATED ; <nl> uinfo -> count = 1 ; <nl> uinfo -> value . enumerated . items = valid_modes ;
int __init dc21285_setup ( int nr , struct pci_sys_data * sys ) <nl>  <nl> sys -> mem_offset = DC21285_PCI_MEM ; <nl>  <nl> - pci_ioremap_io ( 0 , DC21285_PCI_IO ); <nl> - <nl> pci_add_resource_offset (& sys -> resources , & res [ 0 ], sys -> mem_offset ); <nl> pci_add_resource_offset (& sys -> resources , & res [ 1 ], sys -> mem_offset ); <nl> 
static int mwifiex_cfg80211_start_ap ( struct wiphy * wiphy , <nl> case NL80211_HIDDEN_SSID_ZERO_CONTENTS : <nl> /* firmware doesn ' t support this type of hidden SSID */ <nl> default : <nl> + kfree ( bss_cfg ); <nl> return - EINVAL ; <nl> } <nl> 
struct simple_xattr * simple_xattr_alloc ( const void * value , size_t size ) <nl>  <nl> /* wrap around ? */ <nl> len = sizeof (* new_xattr ) + size ; <nl> - if ( len <= sizeof (* new_xattr )) <nl> + if ( len < sizeof (* new_xattr )) <nl> return NULL ; <nl>  <nl> new_xattr = kmalloc ( len , GFP_KERNEL );
int rt2x00mac_tx ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> */ <nl> if (! test_bit ( DEVICE_PRESENT , & rt2x00dev -> flags )) { <nl> ieee80211_stop_queues ( hw ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
static int sprom_extract ( struct ssb_bus * bus , struct ssb_sprom * out , <nl> ssb_printk ( KERN_WARNING PFX " Unsupported SPROM " <nl> " revision % d detected . Will extract " <nl> " v1 \ n ", out -> revision ); <nl> + out -> revision = 1 ; <nl> sprom_extract_r123 ( out , in ); <nl> } <nl> }
static int ath10k_hw_scan ( struct ieee80211_hw * hw , <nl> arg . ssids [ i ]. len = req -> ssids [ i ]. ssid_len ; <nl> arg . ssids [ i ]. ssid = req -> ssids [ i ]. ssid ; <nl> } <nl> + } else { <nl> + arg . scan_ctrl_flags |= WMI_SCAN_FLAG_PASSIVE ; <nl> } <nl>  <nl> if ( req -> n_channels ) {
static int changed_cb ( struct btrfs_root * left_root , <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> + /* Ignore non - FS objects */ <nl> + if ( key -> objectid == BTRFS_FREE_INO_OBJECTID || <nl> + key -> objectid == BTRFS_FREE_SPACE_OBJECTID ) <nl> + goto out ; <nl> + <nl> if ( key -> type == BTRFS_INODE_ITEM_KEY ) <nl> ret = changed_inode ( sctx , result ); <nl> else if ( key -> type == BTRFS_INODE_REF_KEY )
static int bnx2x_get_hwinfo ( struct bnx2x * bp ) <nl> } else <nl> BNX2X_DEV_INFO (" illegal OV for SD \ n "); <nl> break ; <nl> + case SHARED_FEAT_CFG_FORCE_SF_MODE_FORCED_SF : <nl> + bp -> mf_config [ vn ] = 0 ; <nl> + break ; <nl> default : <nl> /* Unknown configuration : reset mf_config */ <nl> bp -> mf_config [ vn ] = 0 ;
static int drm_crtc_convert_umode ( struct drm_display_mode * out , <nl> if ( in -> clock > INT_MAX || in -> vrefresh > INT_MAX ) <nl> return - ERANGE ; <nl>  <nl> - /* At most , 1 set bit describing the 3D layout of the mode */ <nl> - if ( hweight32 ( in -> flags & DRM_MODE_FLAG_3D_MASK ) > 1 ) <nl> - return - EINVAL ; <nl> - <nl> out -> clock = in -> clock ; <nl> out -> hdisplay = in -> hdisplay ; <nl> out -> hsync_start = in -> hsync_start ;
static long pmcraid_ioctl_passthrough ( <nl> rc = - EFAULT ; <nl> goto out_free_buffer ; <nl> } <nl> + } else if ( request_size < 0 ) { <nl> + rc = - EINVAL ; <nl> + goto out_free_buffer ; <nl> } <nl>  <nl> /* check if we have any additional command parameters */
static int clk_div16_get_divider ( unsigned long parent_rate , unsigned long rate ) <nl> if ( divider_u16 - 1 < 0 ) <nl> return 0 ; <nl>  <nl> - if ( divider_u16 - 1 > 255 ) <nl> + if ( divider_u16 - 1 > 0xFFFF ) <nl> return - EINVAL ; <nl>  <nl> return divider_u16 - 1 ;
static int blkvsc_do_operation ( struct block_device_context * blkdev , <nl>  <nl> page_buf = alloc_page ( GFP_KERNEL ); <nl> if (! page_buf ) { <nl> - kmem_cache_free ( blkvsc_req -> dev -> request_pool , blkvsc_req ); <nl> + kmem_cache_free ( blkdev -> request_pool , blkvsc_req ); <nl> return - ENOMEM ; <nl> } <nl>  <nl> cleanup : <nl>  <nl> __free_page ( page_buf ); <nl>  <nl> - kmem_cache_free ( blkvsc_req -> dev -> request_pool , blkvsc_req ); <nl> + kmem_cache_free ( blkdev -> request_pool , blkvsc_req ); <nl>  <nl> return ret ; <nl> }
static bool g4x_compute_wm0 ( struct drm_device * dev , <nl> int entries , tlb_miss ; <nl>  <nl> crtc = intel_get_crtc_for_plane ( dev , plane ); <nl> - if ( crtc -> fb == NULL || ! crtc -> enabled ) <nl> + if ( crtc -> fb == NULL || ! crtc -> enabled ) { <nl> + * cursor_wm = cursor -> guard_size ; <nl> + * plane_wm = display -> guard_size ; <nl> return false ; <nl> + } <nl>  <nl> htotal = crtc -> mode . htotal ; <nl> hdisplay = crtc -> mode . hdisplay ;
static void * alloc_qos_entry ( void ) <nl> } <nl> spin_unlock_irqrestore (& qos_free_list . lock , flags ); <nl>  <nl> - entry = kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> - return entry ; <nl> + return kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> } <nl>  <nl> static void free_qos_entry ( void * entry )
static int be_get_config ( struct be_adapter * adapter ) <nl> if ( status ) <nl> return status ; <nl> memcpy ( adapter -> netdev -> dev_addr , mac , ETH_ALEN ); <nl> + memcpy ( adapter -> netdev -> perm_addr , mac , ETH_ALEN ); <nl>  <nl> return 0 ; <nl> }
static int __devinit td_probe ( struct platform_device * pdev ) <nl> pdata -> channels + i ; <nl>  <nl> /* even channels are RX , odd are TX */ <nl> - if ((( i % 2 ) && pchan -> rx ) || (!( i % 2 ) && ! pchan -> rx )) { <nl> + if (( i % 2 ) == pchan -> rx ) { <nl> dev_err (& pdev -> dev , " Wrong channel configuration \ n "); <nl> err = - EINVAL ; <nl> goto err_tasklet_kill ;
static inline void calculate_imbalance ( struct lb_env * env , struct sd_lb_stats * s <nl> * max load less than avg load ( as we skip the groups at or below <nl> * its cpu_power , while calculating max_load ..) <nl> */ <nl> - if ( busiest -> avg_load < sds -> avg_load ) { <nl> + if ( busiest -> avg_load <= sds -> avg_load || <nl> + local -> avg_load >= sds -> avg_load ) { <nl> env -> imbalance = 0 ; <nl> return fix_small_imbalance ( env , sds ); <nl> }
int __kvm_set_memory_region ( struct kvm * kvm , <nl> goto out_free ; <nl> } <nl>  <nl> - kvm_free_physmem_slot (& old , & new ); <nl> + kvm_free_physmem_slot (& old , npages ? & new : NULL ); <nl> + /* Slot deletion case : we have to update the current slot */ <nl> + if (! npages ) <nl> + * memslot = old ; <nl> # ifdef CONFIG_DMAR <nl> /* map the pages in iommu page table */ <nl> r = kvm_iommu_map_pages ( kvm , base_gfn , npages );
static int mlx4_en_complete_rx_desc ( struct mlx4_en_priv * priv , <nl> PCI_DMA_FROMDEVICE ); <nl> } <nl> /* Adjust size of last fragment to match actual length */ <nl> - skb_frags_rx [ nr - 1 ]. size = length - <nl> - priv -> frag_info [ nr - 1 ]. frag_prefix_size ; <nl> + if ( nr > 0 ) <nl> + skb_frags_rx [ nr - 1 ]. size = length - <nl> + priv -> frag_info [ nr - 1 ]. frag_prefix_size ; <nl> return nr ; <nl>  <nl> fail :
int intel_framebuffer_init ( struct drm_device * dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> + /* FIXME need to adjust LINOFF / TILEOFF accordingly . */ <nl> + if ( mode_cmd -> offsets [ 0 ] != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = drm_framebuffer_init ( dev , & intel_fb -> base , & intel_fb_funcs ); <nl> if ( ret ) { <nl> DRM_ERROR (" framebuffer init failed % d \ n ", ret );
static int rt2x00mac_tx_rts_cts ( struct rt2x00_dev * rt2x00dev , <nl> ( struct ieee80211_rts *)( skb -> data )); <nl>  <nl> if ( rt2x00queue_write_tx_frame ( queue , skb )) { <nl> + dev_kfree_skb_any ( skb ); <nl> WARNING ( rt2x00dev , " Failed to send RTS / CTS frame .\ n "); <nl> return NETDEV_TX_BUSY ; <nl> }
static void oz_hcd_complete_set_interface ( struct oz_port * port , struct urb * urb , <nl> struct usb_hcd * hcd = port -> ozhcd -> hcd ; <nl> int rc = 0 ; <nl>  <nl> - if ( rcode == 0 ) { <nl> + if (( rcode == 0 ) && ( port -> config_num > 0 )) { <nl> struct usb_host_config * config ; <nl> struct usb_host_interface * intf ; <nl> oz_dbg ( ON , " Set interface % d alt % d \ n ", if_num , alt );
static int ip_vs_wrr_init_svc ( struct ip_vs_service * svc ) <nl> /* <nl> * Allocate the mark variable for WRR scheduling <nl> */ <nl> - mark = kmalloc ( sizeof ( struct ip_vs_wrr_mark ), GFP_ATOMIC ); <nl> + mark = kmalloc ( sizeof ( struct ip_vs_wrr_mark ), GFP_KERNEL ); <nl> if ( mark == NULL ) <nl> return - ENOMEM ; <nl> 
int cx25821_openfile_audio ( struct cx25821_dev * dev , <nl> vfs_read_retval = <nl> vfs_read ( myfile , mybuf , line_size , & pos ); <nl>  <nl> - if ( vfs_read_retval > 0 <nl> - && vfs_read_retval == line_size <nl> - && dev -> _audiodata_buf_virt_addr != NULL ) { <nl> + if ( vfs_read_retval > 0 && <nl> + vfs_read_retval == line_size && <nl> + dev -> _audiodata_buf_virt_addr != NULL ) { <nl> memcpy (( void *)( dev -> <nl> _audiodata_buf_virt_addr <nl> + offset / 4 ), mybuf ,
restart : <nl> mutex_unlock (& mutex ); <nl> } <nl>  <nl> +/* <nl> + * sync everything . Start out by waking pdflush , because that writes back <nl> + * all queues in parallel . <nl> + */ <nl> SYSCALL_DEFINE0 ( sync ) <nl> { <nl> + wakeup_pdflush ( 0 ); <nl> sync_filesystems ( 0 ); <nl> sync_filesystems ( 1 ); <nl> if ( unlikely ( laptop_mode ))
static int efx_init_lm87 ( struct efx_nic * efx , struct i2c_board_info * info , <nl> if (! client ) <nl> return - EIO ; <nl>  <nl> + /* Read - to - clear alarm / interrupt status */ <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS1 ); <nl> + i2c_smbus_read_byte_data ( client , LM87_REG_ALARMS2 ); <nl> + <nl> rc = efx_poke_lm87 ( client , reg_values ); <nl> if ( rc ) <nl> goto err ;
static ssize_t __iscsi_ ## prefix ## _store_ ## name ( \ <nl> \ <nl> if (! capable ( CAP_SYS_ADMIN )) \ <nl> return - EPERM ; \ <nl> - \ <nl> + if ( count >= sizeof ( auth -> name )) \ <nl> + return - EINVAL ; \ <nl> snprintf ( auth -> name , sizeof ( auth -> name ), "% s ", page ); \ <nl> if (! strncmp (" NULL ", auth -> name , 4 )) \ <nl> auth -> naf_flags &= ~ flags ; \
static int vmw_gmr2_bind ( struct vmw_private * dev_priv , <nl> cmd += sizeof ( remap_cmd ) / sizeof ( uint32 ); <nl>  <nl> for ( i = 0 ; i < num_pages ; ++ i ) { <nl> - if ( VMW_PPN_SIZE > 4 ) <nl> + if ( VMW_PPN_SIZE <= 4 ) <nl> * cmd = page_to_pfn (* pages ++); <nl> else <nl> *(( uint64_t *) cmd ) = page_to_pfn (* pages ++);
good_area : <nl> return handle_mm_fault ( mm , vma , addr & PAGE_MASK , flags ); <nl>  <nl> check_stack : <nl> - if ( vma -> vm_flags & VM_GROWSDOWN && ! expand_stack ( vma , addr )) <nl> + /* Don ' t allow expansion below FIRST_USER_ADDRESS */ <nl> + if ( vma -> vm_flags & VM_GROWSDOWN && <nl> + addr >= FIRST_USER_ADDRESS && ! expand_stack ( vma , addr )) <nl> goto good_area ; <nl> out : <nl> return fault ;
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> int run_pending = 0 ; <nl>  <nl> switch ( event ) { <nl> + case NETDEV_REGISTER : <nl> + if (! idev ) { <nl> + idev = ipv6_add_dev ( dev ); <nl> + if (! idev ) <nl> + printk ( KERN_WARNING " IPv6 : add_dev failed for % s \ n ", <nl> + dev -> name ); <nl> + } <nl> + break ; <nl> case NETDEV_UP : <nl> case NETDEV_CHANGE : <nl> if ( event == NETDEV_UP ) {
static ssize_t set_pwm_mode ( struct device * dev , struct device_attribute * attr , <nl> if (!( val == 0 || val == 1 )) <nl> return - EINVAL ; <nl>  <nl> + /* F75373 does not support DC ( linear voltage ) fan control mode */ <nl> + if ( data -> kind == f75373 && val == 0 ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& data -> update_lock ); <nl> conf = f75375_read8 ( client , F75375_REG_CONFIG1 ); <nl> conf &= ~( 1 << FAN_CTRL_LINEAR ( nr ));
static int do_swap_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> } else if ( PageHWPoison ( page )) { <nl> ret = VM_FAULT_HWPOISON ; <nl> delayacct_clear_flag ( DELAYACCT_PF_SWAPIN ); <nl> - goto out ; <nl> + goto out_release ; <nl> } <nl>  <nl> lock_page ( page ); <nl> out_nomap : <nl> pte_unmap_unlock ( page_table , ptl ); <nl> out_page : <nl> unlock_page ( page ); <nl> + out_release : <nl> page_cache_release ( page ); <nl> return ret ; <nl> }
ath5k_ani_init ( struct ath5k_hw * ah , enum ath5k_ani_mode mode ) <nl> if ( ah -> ah_version < AR5K_AR5212 ) <nl> return ; <nl>  <nl> + if ( mode < ATH5K_ANI_MODE_OFF || mode > ATH5K_ANI_MODE_AUTO ) { <nl> + ATH5K_ERR ( ah -> ah_sc , " ANI mode % d out of range ", mode ); <nl> + return ; <nl> + } <nl> + <nl> /* clear old state information */ <nl> memset (& ah -> ah_sc -> ani_state , 0 , sizeof ( ah -> ah_sc -> ani_state )); <nl> 
out_err : <nl> * errors we try again until the max number of retries is reached . <nl> */ <nl> if ( result != - EHOSTUNREACH && result != - ENETUNREACH && <nl> - result != - ENETDOWN && result != EINVAL <nl> + result != - ENETDOWN && result != - EINVAL <nl> && result != - EPROTONOSUPPORT ) { <nl> lowcomms_connect_sock ( con ); <nl> result = 0 ;
static void batadv_tt_local_event ( struct batadv_priv * bat_priv , <nl> del : <nl> list_del (& entry -> list ); <nl> kfree ( entry ); <nl> + kfree ( tt_change_node ); <nl> event_removed = true ; <nl> goto unlock ; <nl> }
static void ath9k_hw_read_revisions ( struct ath_hw * ah ) <nl> val = REG_READ ( ah , AR_SREV ); <nl> ah -> hw_version . macRev = MS ( val , AR_SREV_REVISION2 ); <nl> return ; <nl> + case AR9300_DEVID_QCA955X : <nl> + ah -> hw_version . macVersion = AR_SREV_VERSION_9550 ; <nl> + return ; <nl> } <nl>  <nl> val = REG_READ ( ah , AR_SREV ) & AR_SREV_ID ;
ssize_t usb_store_new_id ( struct usb_dynids * dynids , <nl> if ( fields > 4 ) { <nl> const struct usb_device_id * id = id_table ; <nl>  <nl> + if (! id ) <nl> + return - ENODEV ; <nl> + <nl> for (; id -> match_flags ; id ++) <nl> if ( id -> idVendor == refVendor && id -> idProduct == refProduct ) <nl> break ;
static int setup_p6_watchdog ( unsigned nmi_hz ) <nl> perfctr_msr = MSR_P6_PERFCTR0 ; <nl> evntsel_msr = MSR_P6_EVNTSEL0 ; <nl>  <nl> - wrmsrl ( perfctr_msr , 0UL ); <nl> + /* KVM doesn ' t implement this MSR */ <nl> + if ( wrmsr_safe ( perfctr_msr , 0 , 0 ) < 0 ) <nl> + return 0 ; <nl>  <nl> evntsel = P6_EVNTSEL_INT <nl> | P6_EVNTSEL_OS
static void setup_frame_info ( struct ieee80211_hw * hw , <nl> fi -> keyix = ATH9K_TXKEYIX_INVALID ; <nl> fi -> keytype = keytype ; <nl> fi -> framelen = framelen ; <nl> + <nl> + if (! rate ) <nl> + return ; <nl> fi -> rtscts_rate = rate -> hw_value ; <nl> if ( short_preamble ) <nl> fi -> rtscts_rate |= rate -> hw_value_short ;
netdev_tx_t ixgbe_xmit_frame_ring ( struct sk_buff * skb , <nl> tx_flags |= IXGBE_TX_FLAGS_SW_VLAN ; <nl> } <nl>  <nl> + skb_tx_timestamp ( skb ); <nl> + <nl> # ifdef CONFIG_IXGBE_PTP <nl> if ( unlikely ( skb_shinfo ( skb )-> tx_flags & SKBTX_HW_TSTAMP )) { <nl> skb_shinfo ( skb )-> tx_flags |= SKBTX_IN_PROGRESS ;
static int ioapic_service ( struct kvm_ioapic * ioapic , int irq , bool line_status ) <nl> BUG_ON ( ioapic -> rtc_status . pending_eoi != 0 ); <nl> ret = kvm_irq_delivery_to_apic ( ioapic -> kvm , NULL , & irqe , <nl> ioapic -> rtc_status . dest_map ); <nl> - ioapic -> rtc_status . pending_eoi = ret ; <nl> + ioapic -> rtc_status . pending_eoi = ( ret < 0 ? 0 : ret ); <nl> } else <nl> ret = kvm_irq_delivery_to_apic ( ioapic -> kvm , NULL , & irqe , NULL ); <nl> 
static const struct file_operations vfio_device_fops = { <nl> */ <nl> static char * vfio_devnode ( struct device * dev , umode_t * mode ) <nl> { <nl> - if ( MINOR ( dev -> devt ) == 0 ) <nl> + if ( mode && ( MINOR ( dev -> devt ) == 0 )) <nl> * mode = S_IRUGO | S_IWUGO ; <nl>  <nl> return kasprintf ( GFP_KERNEL , " vfio /% s ", dev_name ( dev ));
void intel_panel_enable_backlight ( struct intel_connector * connector ) <nl>  <nl> WARN_ON ( panel -> backlight . max == 0 ); <nl>  <nl> - if ( panel -> backlight . level == 0 ) { <nl> + if ( panel -> backlight . level <= panel -> backlight . min ) { <nl> panel -> backlight . level = panel -> backlight . max ; <nl> if ( panel -> backlight . device ) <nl> panel -> backlight . device -> props . brightness =
vxge_probe ( struct pci_dev * pdev , const struct pci_device_id * pre ) <nl> driver_config -> config_dev_cnt = 0 ; <nl> driver_config -> total_dev_cnt = 0 ; <nl> driver_config -> g_no_cpus = 0 ; <nl> - driver_config -> vpath_per_dev = max_config_vpath ; <nl> } <nl>  <nl> + driver_config -> vpath_per_dev = max_config_vpath ; <nl> + <nl> driver_config -> total_dev_cnt ++; <nl> if (++ driver_config -> config_dev_cnt > max_config_dev ) { <nl> ret = 0 ;
static int dwc3_gadget_init_hw_endpoints ( struct dwc3 * dwc , <nl>  <nl> dep -> endpoint . name = dep -> name ; <nl>  <nl> + dev_vdbg ( dwc -> dev , " initializing % s \ n ", dep -> name ); <nl> + <nl> if ( epnum == 0 || epnum == 1 ) { <nl> dep -> endpoint . maxpacket = 512 ; <nl> dep -> endpoint . maxburst = 1 ;
int perf_event__parse_sample ( const union perf_event * event , u64 type , <nl> u32 val32 [ 2 ]; <nl> } u ; <nl>  <nl> - <nl> + memset ( data , 0 , sizeof (* data )); <nl> data -> cpu = data -> pid = data -> tid = - 1 ; <nl> data -> stream_id = data -> id = data -> time = - 1ULL ; <nl> 
static int init_volumes ( struct ubi_device * ubi , <nl>  <nl> /* Static volumes only */ <nl> av = ubi_find_av ( ai , i ); <nl> - if (! av ) { <nl> + if (! av || ! av -> leb_count ) { <nl> /* <nl> * No eraseblocks belonging to this volume found . We <nl> * don ' t actually know whether this static volume is
static void __unregister_request ( struct ceph_osd_client * osdc , <nl> */ <nl> static void __cancel_request ( struct ceph_osd_request * req ) <nl> { <nl> - if ( req -> r_sent ) { <nl> + if ( req -> r_sent && req -> r_osd ) { <nl> ceph_con_revoke (& req -> r_osd -> o_con , req -> r_request ); <nl> req -> r_sent = 0 ; <nl> }
ARIZONA_MIXER_CONTROLS (" DSP2R ", ARIZONA_DSP2RMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP3L ", ARIZONA_DSP3LMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP3R ", ARIZONA_DSP3RMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP4L ", ARIZONA_DSP4LMIX_INPUT_1_SOURCE ), <nl> - ARIZONA_MIXER_CONTROLS (" DSP5R ", ARIZONA_DSP4RMIX_INPUT_1_SOURCE ), <nl> + ARIZONA_MIXER_CONTROLS (" DSP4R ", ARIZONA_DSP4RMIX_INPUT_1_SOURCE ), <nl>  <nl> ARIZONA_MIXER_CONTROLS (" Mic ", ARIZONA_MICMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" Noise ", ARIZONA_NOISEMIX_INPUT_1_SOURCE ),
void i915_debugfs_cleanup ( struct drm_minor * minor ); <nl> int i915_debugfs_connector_add ( struct drm_connector * connector ); <nl> void intel_display_crc_init ( struct drm_device * dev ); <nl> # else <nl> - static inline int i915_debugfs_connector_add ( struct drm_connector * connector ) {} <nl> + static inline int i915_debugfs_connector_add ( struct drm_connector * connector ) <nl> +{ return 0 ; } <nl> static inline void intel_display_crc_init ( struct drm_device * dev ) {} <nl> # endif <nl> 
struct iwl_cfg iwl6000i_2bg_cfg = { <nl> . fw_name_pre = IWL6050_FW_PRE , \ <nl> . ucode_api_max = IWL6050_UCODE_API_MAX , \ <nl> . ucode_api_min = IWL6050_UCODE_API_MIN , \ <nl> + . valid_tx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> + . valid_rx_ant = ANT_AB , /* . cfg overwrite */ \ <nl> . ops = & iwl6050_ops , \ <nl> . eeprom_ver = EEPROM_6050_EEPROM_VERSION , \ <nl> . eeprom_calib_ver = EEPROM_6050_TX_POWER_VERSION , \
static struct tgfx __init * tgfx_probe ( int parport , int * n_buttons , int n_devs ) <nl> if ( n_buttons [ i ] < 1 ) <nl> continue ; <nl>  <nl> - if ( n_buttons [ i ] > 6 ) { <nl> + if ( n_buttons [ i ] > ARRAY_SIZE ( tgfx_buttons )) { <nl> printk ( KERN_ERR " turbografx . c : Invalid number of buttons % d \ n ", n_buttons [ i ]); <nl> err = - EINVAL ; <nl> goto err_unreg_devs ;
rpcrdma_register_frmr_external ( struct rpcrdma_mr_seg * seg , <nl> if ( rc ) { <nl> dprintk (" RPC : % s : failed ib_post_send for register ," <nl> " status % i \ n ", __func__ , rc ); <nl> + ib_update_fast_reg_key ( mr , -- key ); <nl> goto out_err ; <nl> } else { <nl> seg1 -> mr_rkey = mr -> rkey ;
static void cx24120_check_cmd ( struct cx24120_state * state , u8 id ) <nl> case CMD_DISEQC_MSG2 : <nl> case CMD_SETVOLTAGE : <nl> case CMD_SETTONE : <nl> + case CMD_DISEQC_BURST : <nl> cx24120_msg_mpeg_output_global_config ( state , 0 ); <nl> /* Old driver would do a msleep ( 100 ) here */ <nl> default :
ieee80211_rx_h_michael_mic_verify ( struct ieee80211_rx_data * rx ) <nl> if ( status -> flag & RX_FLAG_MMIC_ERROR ) <nl> goto mic_fail ; <nl>  <nl> - if (!( status -> flag & RX_FLAG_IV_STRIPPED )) <nl> + if (!( status -> flag & RX_FLAG_IV_STRIPPED ) && rx -> key ) <nl> goto update_iv ; <nl>  <nl> return RX_CONTINUE ;
static void cyberjack_read_int_callback ( struct urb * urb ) <nl>  <nl> old_rdtodo = priv -> rdtodo ; <nl>  <nl> - if ( old_rdtodo + size < old_rdtodo ) { <nl> + if ( old_rdtodo > SHRT_MAX - size ) { <nl> dev_dbg ( dev , " To many bulk_in urbs to do .\ n "); <nl> spin_unlock (& priv -> lock ); <nl> goto resubmit ;
static void scatterwalk_pagedone ( struct scatter_walk * walk , int out , <nl> struct page * page ; <nl>  <nl> page = sg_page ( walk -> sg ) + (( walk -> offset - 1 ) >> PAGE_SHIFT ); <nl> - flush_dcache_page ( page ); <nl> + if (! PageSlab ( page )) <nl> + flush_dcache_page ( page ); <nl> } <nl>  <nl> if ( more ) {
static int time_cpufreq_notifier ( struct notifier_block * nb , unsigned long val , <nl> tsc_khz = cpufreq_scale ( tsc_khz_ref , ref_freq , freq -> new ); <nl> if (!( freq -> flags & CPUFREQ_CONST_LOOPS )) <nl> mark_tsc_unstable (" cpufreq changes "); <nl> - } <nl>  <nl> - set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + set_cyc2ns_scale ( tsc_khz , freq -> cpu ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void __init mark_kpte_bitmap ( unsigned long start , unsigned long end ) <nl> while ( start < end ) { <nl> long remains ; <nl>  <nl> + remains = end - start ; <nl> + if ( remains < size_256MB ) <nl> + break ; <nl> + <nl> if ( start & mask_256MB ) { <nl> start = ( start + size_256MB ) & ~ mask_256MB ; <nl> continue ; <nl> } <nl>  <nl> - remains = end - start ; <nl> while ( remains >= size_256MB ) { <nl> unsigned long index = start >> shift_256MB ; <nl> 
static void __init create_mapping ( phys_addr_t phys , unsigned long virt , <nl> void __iomem * __init early_io_map ( phys_addr_t phys , unsigned long virt ) <nl> { <nl> unsigned long size , mask ; <nl> - bool page64k = IS_ENABLED ( ARM64_64K_PAGES ); <nl> + bool page64k = IS_ENABLED ( CONFIG_ARM64_64K_PAGES ); <nl> pgd_t * pgd ; <nl> pud_t * pud ; <nl> pmd_t * pmd ;
static void * vb2_dc_alloc ( void * alloc_ctx , unsigned long size ) <nl> if (! buf ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> + /* align image size to PAGE_SIZE */ <nl> + size = PAGE_ALIGN ( size ); <nl> + <nl> buf -> vaddr = dma_alloc_coherent ( dev , size , & buf -> dma_addr , GFP_KERNEL ); <nl> if (! buf -> vaddr ) { <nl> dev_err ( dev , " dma_alloc_coherent of size % ld failed \ n ", size );
int pinconf_generic_parse_dt_config ( struct device_node * np , <nl> ncfg ++; <nl> } <nl>  <nl> + /* no configs found at all */ <nl> + if ( ncfg == 0 ) { <nl> + * configs = NULL ; <nl> + * nconfigs = 0 ; <nl> + return 0 ; <nl> + } <nl> + <nl> /* <nl> * Now limit the number of configs to the real number of <nl> * found properties .
static u32 tcp_yeah_ssthresh ( struct sock * sk ) <nl> yeah -> fast_count = 0 ; <nl> yeah -> reno_count = max ( yeah -> reno_count >> 1 , 2U ); <nl>  <nl> - return tp -> snd_cwnd - reduction ; <nl> + return max_t ( int , tp -> snd_cwnd - reduction , 2 ); <nl> } <nl>  <nl> static struct tcp_congestion_ops tcp_yeah __read_mostly = {
static ssize_t read_vmcore ( struct file * file , char __user * buffer , <nl>  <nl> static const struct file_operations proc_vmcore_operations = { <nl> . read = read_vmcore , <nl> - . llseek = generic_file_llseek , <nl> + . llseek = default_llseek , <nl> }; <nl>  <nl> static struct vmcore * __init get_new_element ( void )
static void kswapd_try_to_sleep ( pg_data_t * pgdat , int order , int classzone_idx ) <nl> * them before going back to sleep . <nl> */ <nl> set_pgdat_percpu_threshold ( pgdat , calculate_normal_threshold ); <nl> - schedule (); <nl> + <nl> + if (! kthread_should_stop ()) <nl> + schedule (); <nl> + <nl> set_pgdat_percpu_threshold ( pgdat , calculate_pressure_threshold ); <nl> } else { <nl> if ( remaining )
static void handle_critical_trips ( struct thermal_zone_device * tz , <nl> tz -> ops -> get_trip_temp ( tz , trip , & trip_temp ); <nl>  <nl> /* If we have not crossed the trip_temp , we do not care . */ <nl> - if ( tz -> temperature < trip_temp ) <nl> + if ( trip_temp <= 0 || tz -> temperature < trip_temp ) <nl> return ; <nl>  <nl> trace_thermal_zone_trip ( tz , trip , trip_type );
static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl>  <nl> mutex_lock (& dev -> lock ); <nl>  <nl> - if ( dev -> open ++) <nl> + if ( dev -> open ) <nl> goto unlock ; <nl>  <nl> kref_get (& dev -> ref ); <nl> static int blktrans_open ( struct block_device * bdev , fmode_t mode ) <nl> goto error_release ; <nl>  <nl> unlock : <nl> + dev -> open ++; <nl> mutex_unlock (& dev -> lock ); <nl> blktrans_dev_put ( dev ); <nl> return ret ;
static struct usb_device_id usb_ids [] = { <nl> { USB_DEVICE ( 0x157e , 0x300d ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x079b , 0x0062 ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x1582 , 0x6003 ), . driver_info = DEVICE_ZD1211B }, <nl> + { USB_DEVICE ( 0x050d , 0x705c ), . driver_info = DEVICE_ZD1211B }, <nl> /* " Driverless " devices that need ejecting */ <nl> { USB_DEVICE ( 0x0ace , 0x2011 ), . driver_info = DEVICE_INSTALLER }, <nl> {}
static int i2o_cfg_gethrt ( unsigned long arg ) <nl> put_user ( len , kcmd . reslen ); <nl> if ( len > reslen ) <nl> ret = - ENOBUFS ; <nl> - if ( copy_to_user ( kcmd . resbuf , ( void *) hrt , len )) <nl> + else if ( copy_to_user ( kcmd . resbuf , ( void *) hrt , len )) <nl> ret = - EFAULT ; <nl>  <nl> return ret ;
static void gc_attach ( struct parport * pp ) <nl> pads = gc_cfg [ port_idx ]. args + 1 ; <nl> n_pads = gc_cfg [ port_idx ]. nargs - 1 ; <nl>  <nl> + memset (& gc_parport_cb , 0 , sizeof ( gc_parport_cb )); <nl> gc_parport_cb . flags = PARPORT_FLAG_EXCL ; <nl>  <nl> pd = parport_register_dev_model ( pp , " gamecon ", & gc_parport_cb ,
void kvm_check_async_pf_completion ( struct kvm_vcpu * vcpu ) <nl> spin_unlock (& vcpu -> async_pf . lock ); <nl>  <nl> kvm_arch_async_page_ready ( vcpu , work ); <nl> - kvm_arch_async_page_present ( vcpu , work ); <nl> + kvm_async_page_present_async ( vcpu , work ); <nl>  <nl> list_del (& work -> queue ); <nl> vcpu -> async_pf . queued --;
static void ipw_handle_data_packet ( struct ipw_priv * priv , <nl> IPW_DEBUG_RX (" Rx packet of % d bytes .\ n ", rxb -> skb -> len ); <nl>  <nl> /* HW decrypt will not clear the WEP bit , MIC , PN , etc . */ <nl> - if (! priv -> ieee -> host_decrypt ) <nl> + if (! priv -> ieee -> host_decrypt && priv -> ieee -> iw_mode != IW_MODE_MONITOR ) <nl> ipw_rebuild_decrypted_skb ( priv , rxb -> skb ); <nl>  <nl> if (! ieee80211_rx ( priv -> ieee , rxb -> skb , stats ))
static int dma40_memcpy_channels [] = { <nl> }; <nl>  <nl> /* Default configuration for physcial memcpy */ <nl> - struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> + static struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> . mode = STEDMA40_MODE_PHYSICAL , <nl> . dir = DMA_MEM_TO_MEM , <nl>  <nl> struct stedma40_chan_cfg dma40_memcpy_conf_phy = { <nl> }; <nl>  <nl> /* Default configuration for logical memcpy */ <nl> - struct stedma40_chan_cfg dma40_memcpy_conf_log = { <nl> + static struct stedma40_chan_cfg dma40_memcpy_conf_log = { <nl> . mode = STEDMA40_MODE_LOGICAL , <nl> . dir = DMA_MEM_TO_MEM , <nl> 
static unsigned int xhci_microframes_to_exponent ( struct usb_device * udev , <nl> static unsigned int xhci_parse_microframe_interval ( struct usb_device * udev , <nl> struct usb_host_endpoint * ep ) <nl> { <nl> + if ( ep -> desc . bInterval == 0 ) <nl> + return 0 ; <nl> return xhci_microframes_to_exponent ( udev , ep , <nl> ep -> desc . bInterval , 0 , 15 ); <nl> }
static void __init atmci_get_cap ( struct atmel_mci * host ) <nl> /* keep only major version number */ <nl> switch ( version & 0xf00 ) { <nl> case 0x100 : <nl> + host -> caps . has_pdc = 1 ; <nl> + break ; <nl> case 0x200 : <nl> host -> caps . has_pdc = 1 ; <nl> host -> caps . has_rwproof = 1 ;
int of_gpio_simple_xlate ( struct gpio_chip * gc , <nl> if ( WARN_ON ( gpiospec -> args_count < gc -> of_gpio_n_cells )) <nl> return - EINVAL ; <nl>  <nl> - if ( gpiospec -> args [ 0 ] > gc -> ngpio ) <nl> + if ( gpiospec -> args [ 0 ] >= gc -> ngpio ) <nl> return - EINVAL ; <nl>  <nl> if ( flags )
void kvm_hv_process_stimers ( struct kvm_vcpu * vcpu ) <nl> for ( i = 0 ; i < ARRAY_SIZE ( hv_vcpu -> stimer ); i ++) <nl> if ( test_and_clear_bit ( i , hv_vcpu -> stimer_pending_bitmap )) { <nl> stimer = & hv_vcpu -> stimer [ i ]; <nl> - stimer_stop ( stimer ); <nl> if ( stimer -> config & HV_STIMER_ENABLE ) { <nl> time_now = get_time_ref_counter ( vcpu -> kvm ); <nl> if ( time_now >= stimer -> exp_time )
static int mxs_dcp_start_dma ( struct dcp_async_ctx * actx ) <nl> struct dcp * sdcp = global_sdcp ; <nl> const int chan = actx -> chan ; <nl> uint32_t stat ; <nl> - int ret ; <nl> + unsigned long ret ; <nl> struct dcp_dma_desc * desc = & sdcp -> coh -> desc [ actx -> chan ]; <nl>  <nl> dma_addr_t desc_phys = dma_map_single ( sdcp -> dev , desc , sizeof (* desc ),
static int rxq_process ( struct ieee80211_hw * hw , int index , int limit ) <nl> rmb (); <nl>  <nl> skb = rxq -> rx_skb [ rxq -> rx_head ]; <nl> + if ( skb == NULL ) <nl> + break ; <nl> rxq -> rx_skb [ rxq -> rx_head ] = NULL ; <nl>  <nl> rxq -> rx_head = ( rxq -> rx_head + 1 ) % MWL8K_RX_DESCS ;
static int amdgpu_vm_clear_bo ( struct amdgpu_device * adev , <nl> if ( r ) <nl> return r ; <nl>  <nl> + r = reservation_object_reserve_shared ( bo -> tbo . resv ); <nl> + if ( r ) <nl> + return r ; <nl> + <nl> r = ttm_bo_validate (& bo -> tbo , & bo -> placement , true , false ); <nl> if ( r ) <nl> goto error_unreserve ;
SYSCALL_DEFINE6 ( sparc_ipc , unsigned int , call , int , first , unsigned long , second <nl> long err ; <nl>  <nl> /* No need for backward compatibility . We can start fresh ... */ <nl> - if ( call <= SEMCTL ) { <nl> + if ( call <= SEMTIMEDOP ) { <nl> switch ( call ) { <nl> case SEMOP : <nl> err = sys_semtimedop ( first , ptr ,
static struct thread * __machine__findnew_thread ( struct machine * machine , <nl> * within thread__init_map_groups to find the thread <nl> * leader and that would screwed the rb tree . <nl> */ <nl> - if ( thread__init_map_groups ( th , machine )) <nl> + if ( thread__init_map_groups ( th , machine )) { <nl> + thread__delete ( th ); <nl> return NULL ; <nl> + } <nl> } <nl>  <nl> return th ;
static void __init exynos_reserve ( void ) <nl> " samsung , mfc - v5 ", <nl> " samsung , mfc - v6 ", <nl> " samsung , mfc - v7 ", <nl> + " samsung , mfc - v8 ", <nl> }; <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( mfc_mem ); i ++)
at86rf230_tx_complete ( void * context ) <nl> { <nl> struct at86rf230_state_change * ctx = context ; <nl> struct at86rf230_local * lp = ctx -> lp ; <nl> - struct sk_buff * skb = lp -> tx_skb ; <nl>  <nl> enable_irq ( lp -> spi -> irq ); <nl>  <nl> - ieee802154_xmit_complete ( lp -> hw , skb , ! lp -> tx_aret ); <nl> + ieee802154_xmit_complete ( lp -> hw , lp -> tx_skb , ! lp -> tx_aret ); <nl> } <nl>  <nl> static void
recheck : <nl> */ <nl> if (! capable ( CAP_SYS_NICE )) { <nl> /* can ' t change policy */ <nl> - if ( policy != p -> policy ) <nl> + if ( policy != p -> policy && <nl> + ! p -> signal -> rlim [ RLIMIT_RTPRIO ]. rlim_cur ) <nl> return - EPERM ; <nl> /* can ' t increase priority */ <nl> if ( policy != SCHED_NORMAL &&
parse_init_table ( struct nvbios * bios , uint16_t offset , struct init_exec * iexec ) <nl> int count = 0 , i , ret ; <nl> uint8_t id ; <nl>  <nl> + /* catch NULL script pointers */ <nl> + if ( offset == 0 ) <nl> + return 0 ; <nl> + <nl> /* <nl> * Loop until INIT_DONE causes us to break out of the loop <nl> * ( or until offset > bios length just in case ... )
static int __devinit adp8870_probe ( struct i2c_client * client , <nl> mutex_init (& data -> lock ); <nl>  <nl> memset (& props , 0 , sizeof ( props )); <nl> + props . type = BACKLIGHT_RAW ; <nl> props . max_brightness = props . brightness = ADP8870_MAX_BRIGHTNESS ; <nl> bl = backlight_device_register ( dev_driver_string (& client -> dev ), <nl> & client -> dev , data , & adp8870_bl_ops , & props );
static enum print_line_t trace_stack_print ( struct trace_iterator * iter , <nl> trace_assign_type ( field , iter -> ent ); <nl>  <nl> for ( i = 0 ; i < FTRACE_STACK_ENTRIES ; i ++) { <nl> + if (! field -> caller [ i ]) <nl> + break ; <nl> if ( i ) { <nl> if (! trace_seq_puts ( s , " <= ")) <nl> goto partial ;
static void tg3_adjust_link ( struct net_device * dev ) <nl>  <nl> if ( phydev -> speed == SPEED_100 || phydev -> speed == SPEED_10 ) <nl> mac_mode |= MAC_MODE_PORT_MODE_MII ; <nl> - else <nl> + else if ( phydev -> speed == SPEED_1000 || <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) != ASIC_REV_5785 ) <nl> mac_mode |= MAC_MODE_PORT_MODE_GMII ; <nl> + else <nl> + mac_mode |= MAC_MODE_PORT_MODE_MII ; <nl>  <nl> if ( phydev -> duplex == DUPLEX_HALF ) <nl> mac_mode |= MAC_MODE_HALF_DUPLEX ;
static void rtl8150_disconnect ( struct usb_interface * intf ) <nl> if ( dev ) { <nl> set_bit ( RTL8150_UNPLUG , & dev -> flags ); <nl> tasklet_disable (& dev -> tl ); <nl> + tasklet_kill (& dev -> tl ); <nl> unregister_netdev ( dev -> netdev ); <nl> unlink_all_urbs ( dev ); <nl> free_all_urbs ( dev );
static int vmw_cotable_unbind ( struct vmw_resource * res , <nl> struct vmw_private * dev_priv = res -> dev_priv ; <nl> struct ttm_buffer_object * bo = val_buf -> bo ; <nl> struct vmw_fence_obj * fence ; <nl> - int ret ; <nl>  <nl> if ( list_empty (& res -> mob_head )) <nl> return 0 ; <nl> static int vmw_cotable_unbind ( struct vmw_resource * res , <nl> if ( likely ( fence != NULL )) <nl> vmw_fence_obj_unreference (& fence ); <nl>  <nl> - return ret ; <nl> + return 0 ; <nl> } <nl>  <nl> /**
static int hostap_set_generic_element ( PSDevice pDevice , <nl> { <nl> PSMgmtObject pMgmt = pDevice -> pMgmt ; <nl>  <nl> + if ( param -> u . generic_elem . len > sizeof ( pMgmt -> abyWPAIE )) <nl> + return - EINVAL ; <nl> + <nl> memcpy ( pMgmt -> abyWPAIE , <nl> param -> u . generic_elem . data , <nl> param -> u . generic_elem . len
static int sh_mdio_release ( struct net_device * ndev ) <nl> /* remove mdio bus info from net_device */ <nl> dev_set_drvdata (& ndev -> dev , NULL ); <nl>  <nl> + /* free interrupts memory */ <nl> + kfree ( bus -> irq ); <nl> + <nl> /* free bitbang info */ <nl> free_mdio_bitbang ( bus ); <nl> 
retry : <nl> handle = ext3_journal_start ( inode , 2 ); <nl> if ( IS_ERR ( handle )) { <nl> /* This is really bad luck . We ' ve written the data <nl> - * but cannot extend i_size . Bail out and pretend <nl> - * the write failed ... */ <nl> + * but cannot extend i_size . Truncate allocated blocks <nl> + * and pretend the write failed ... */ <nl> + ext3_truncate ( inode ); <nl> ret = PTR_ERR ( handle ); <nl> goto out ; <nl> }
void __init tsc_calibrate ( void ) <nl> if ( hpet ) { <nl> printk ( KERN_INFO " TSC calibrated against HPET \ n "); <nl> if ( hpet2 < hpet1 ) <nl> - hpet2 += 0x100000000 ; <nl> + hpet2 += 0x100000000UL ; <nl> hpet2 -= hpet1 ; <nl> tsc1 = ( hpet2 * hpet_readl ( HPET_PERIOD )) / 1000000 ; <nl> } else {
static int do_insnlist_ioctl ( struct comedi_device * dev , <nl> goto error ; <nl> } <nl>  <nl> + if ( sizeof ( struct comedi_insn ) * insnlist . n_insns < insnlist . n_insns ) { <nl> + ret = - EINVAL ; <nl> + goto error ; <nl> + } <nl> + <nl> insns = <nl> kmalloc ( sizeof ( struct comedi_insn ) * insnlist . n_insns , GFP_KERNEL ); <nl> if (! insns ) {
_base_sas_log_info ( struct MPT2SAS_ADAPTER * ioc , u32 log_info ) <nl> return ; <nl>  <nl> /* eat the loginfos associated with task aborts */ <nl> - if ( ioc -> ignore_loginfos && ( log_info == 30050000 || log_info == <nl> + if ( ioc -> ignore_loginfos && ( log_info == 0x30050000 || log_info == <nl> 0x31140000 || log_info == 0x31130000 )) <nl> return ; <nl> 
static SENSOR_DEVICE_ATTR ( temp4_input , S_IRUGO , show_temp , NULL , 3 ); <nl> REG : count of 90kHz pulses / revolution */ <nl> static int fan_from_reg ( u16 reg ) <nl> { <nl> + if ( reg == 0 || reg == 0xffff ) <nl> + return 0 ; <nl> return 90000 * 60 / reg ; <nl> } <nl> 
static int nl80211_start_radar_detection ( struct sk_buff * skb , <nl> struct net_device * dev = info -> user_ptr [ 1 ]; <nl> struct wireless_dev * wdev = dev -> ieee80211_ptr ; <nl> struct cfg80211_chan_def chandef ; <nl> + enum nl80211_dfs_regions dfs_region ; <nl> int err ; <nl>  <nl> + dfs_region = reg_get_dfs_region ( wdev -> wiphy ); <nl> + if ( dfs_region == NL80211_DFS_UNSET ) <nl> + return - EINVAL ; <nl> + <nl> err = nl80211_parse_chandef ( rdev , info , & chandef ); <nl> if ( err ) <nl> return err ;
static int mxs_mmc_probe ( struct platform_device * pdev ) <nl> if (! ssp -> dmach ) { <nl> dev_err ( mmc_dev ( host -> mmc ), <nl> "% s : failed to request dma \ n ", __func__ ); <nl> + ret = - ENODEV ; <nl> goto out_clk_put ; <nl> } <nl> 
static int init_timers_cpu ( int cpu ) <nl> /* <nl> * The APs use this path later in boot <nl> */ <nl> - base = kmalloc_node ( sizeof (* base ), <nl> - GFP_KERNEL | __GFP_ZERO , <nl> - cpu_to_node ( cpu )); <nl> + base = kzalloc_node ( sizeof (* base ), GFP_KERNEL , <nl> + cpu_to_node ( cpu )); <nl> if (! base ) <nl> return - ENOMEM ; <nl> 
asmlinkage long sys_rt_sigreturn ( struct pt_regs * regs ) <nl>  <nl> /* It is more difficult to avoid calling this function than to <nl> call it and ignore errors . */ <nl> - if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> r1 )) <nl> + if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> r1 ) == - EFAULT ) <nl> goto badframe ; <nl>  <nl> return rval ;
iscsi_sendpage ( struct iscsi_conn * conn , struct iscsi_buf * buf , <nl> BUG_ON ( buf -> sent + size > buf -> sg . length ); <nl> if ( size > * count ) <nl> size = * count ; <nl> - if ( buf -> sent + size != buf -> sg . length ) <nl> + if ( buf -> sent + size != buf -> sg . length || * count != size ) <nl> flags |= MSG_MORE ; <nl>  <nl> res = iscsi_send ( sk , buf , size , flags );
static ssize_t iio_write_channel_info ( struct device * dev , <nl> if ( buf [ 0 ] == '-') { <nl> negative = true ; <nl> buf ++; <nl> + } else if ( buf [ 0 ] == '+') { <nl> + buf ++; <nl> } <nl>  <nl> while (* buf ) {
int irq_domain_simple_dt_translate ( struct irq_domain * d , <nl> return - EINVAL ; <nl> if ( intsize < 1 ) <nl> return - EINVAL ; <nl> + if ( d -> nr_irq && (( intspec [ 0 ] < d -> hwirq_base ) || <nl> + ( intspec [ 0 ] >= d -> hwirq_base + d -> nr_irq ))) <nl> + return - EINVAL ; <nl>  <nl> * out_hwirq = intspec [ 0 ]; <nl> * out_type = IRQ_TYPE_NONE ;
static int snd_compr_allocate_buffer ( struct snd_compr_stream * stream , <nl> unsigned int buffer_size ; <nl> void * buffer ; <nl>  <nl> + if ( params -> buffer . fragment_size == 0 || <nl> + params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + return - EINVAL ; <nl> + <nl> buffer_size = params -> buffer . fragment_size * params -> buffer . fragments ; <nl> if ( stream -> ops -> copy ) { <nl> buffer = NULL ;
static void slob_free ( void * block , int size ) <nl> sp -> units += units ; <nl>  <nl> if ( b < sp -> free ) { <nl> + if ( b + units == sp -> free ) { <nl> + units += slob_units ( sp -> free ); <nl> + sp -> free = slob_next ( sp -> free ); <nl> + } <nl> set_slob ( b , units , sp -> free ); <nl> sp -> free = b ; <nl> } else {
retry : <nl> goto found ; <nl> } <nl>  <nl> - while ( addr + size >= first -> va_start && addr + size <= vend ) { <nl> + while ( addr + size > first -> va_start && addr + size <= vend ) { <nl> addr = ALIGN ( first -> va_end + PAGE_SIZE , align ); <nl>  <nl> n = rb_next (& first -> rb_node );
int intel_framebuffer_init ( struct drm_device * dev , <nl> case DRM_FORMAT_UYVY : <nl> case DRM_FORMAT_YVYU : <nl> case DRM_FORMAT_VYUY : <nl> - if ( INTEL_INFO ( dev )-> gen < 6 ) <nl> + if ( INTEL_INFO ( dev )-> gen < 5 ) <nl> return - EINVAL ; <nl> break ; <nl> default :
int iwl_mvm_tof_responder_cmd ( struct iwl_mvm * mvm , <nl> if (! fw_has_capa (& mvm -> fw -> ucode_capa , IWL_UCODE_TLV_CAPA_TOF_SUPPORT )) <nl> return - EINVAL ; <nl>  <nl> - if ( vif -> p2p || vif -> type != NL80211_IFTYPE_AP ) { <nl> + if ( vif -> p2p || vif -> type != NL80211_IFTYPE_AP || <nl> + ! mvmvif -> ap_ibss_active ) { <nl> IWL_ERR ( mvm , " Cannot start responder , not in AP mode \ n "); <nl> return - EIO ; <nl> }
ecryptfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> ecryptfs_copy_attr_all ( old_dir , lower_old_dir_dentry -> d_inode ); <nl> out_lock : <nl> unlock_rename ( lower_old_dir_dentry , lower_new_dir_dentry ); <nl> + dput ( lower_new_dentry -> d_parent ); <nl> + dput ( lower_old_dentry -> d_parent ); <nl> dput ( lower_new_dentry ); <nl> dput ( lower_old_dentry ); <nl> return rc ;
struct fbcon_ops { <nl> # define attr_fgcol ( fgshift , s ) \ <nl> ((( s ) >> ( fgshift )) & 0x0f ) <nl> # define attr_bgcol ( bgshift , s ) \ <nl> - ((( s ) >> ( bgshift )) & 0x0f ) <nl> + ((( s ) >> ( bgshift )) & 0x07 ) <nl>  <nl> /* Monochrome */ <nl> # define attr_bold ( s ) \
struct ipic * __init ipic_init ( struct device_node * node , unsigned int flags ) <nl> ipic -> irqhost = irq_alloc_host ( node , IRQ_HOST_MAP_LINEAR , <nl> NR_IPIC_INTS , <nl> & ipic_host_ops , 0 ); <nl> - if ( ipic -> irqhost == NULL ) <nl> + if ( ipic -> irqhost == NULL ) { <nl> + kfree ( ipic ); <nl> return NULL ; <nl> + } <nl>  <nl> ipic -> regs = ioremap ( res . start , res . end - res . start + 1 ); <nl> 
static int ccmp_encrypt_skb ( struct ieee80211_tx_data * tx , struct sk_buff * skb ) <nl> memmove ( pos , pos + CCMP_HDR_LEN , hdrlen ); <nl>  <nl> /* the HW only needs room for the IV , but not the actual IV */ <nl> - if ( info -> control . hw_key -> flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE ) <nl> + if ( info -> control . hw_key && <nl> + ( info -> control . hw_key -> flags & IEEE80211_KEY_FLAG_PUT_IV_SPACE )) <nl> return 0 ; <nl>  <nl> hdr = ( struct ieee80211_hdr *) pos ;
void __init setup_per_cpu_areas ( void ) <nl> fc = __alloc_bootmem ( unit_size , PAGE_SIZE , __pa ( MAX_DMA_ADDRESS )); <nl> if (! ai || ! fc ) <nl> panic (" Failed to allocate memory for percpu areas ."); <nl> + /* kmemleak tracks the percpu allocations separately */ <nl> + kmemleak_free ( fc ); <nl>  <nl> ai -> dyn_size = unit_size ; <nl> ai -> unit_size = unit_size ;
int l2tp_xmit_skb ( struct l2tp_session * session , struct sk_buff * skb , int hdr_len <nl> headroom = NET_SKB_PAD + sizeof ( struct iphdr ) + <nl> uhlen + hdr_len ; <nl> old_headroom = skb_headroom ( skb ); <nl> - if ( skb_cow_head ( skb , headroom )) <nl> + if ( skb_cow_head ( skb , headroom )) { <nl> + dev_kfree_skb ( skb ); <nl> goto abort ; <nl> + } <nl>  <nl> new_headroom = skb_headroom ( skb ); <nl> skb_orphan ( skb );
int MoxaDriverIoctl ( unsigned int cmd , unsigned long arg , int port ) <nl> case MOXA_FIND_BOARD : <nl> case MOXA_LOAD_C320B : <nl> case MOXA_LOAD_CODE : <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> break ; <nl> } <nl> 
static int exynos_drm_crtc_mode_set_base ( struct drm_crtc * crtc , int x , int y , <nl>  <nl> DRM_DEBUG_KMS ("% s \ n ", __FILE__ ); <nl>  <nl> + /* when framebuffer changing is requested , crtc ' s dpms should be on */ <nl> + if ( exynos_crtc -> dpms > DRM_MODE_DPMS_ON ) { <nl> + DRM_ERROR (" failed framebuffer changing request .\ n "); <nl> + return - EPERM ; <nl> + } <nl> + <nl> crtc_w = crtc -> fb -> width - x ; <nl> crtc_h = crtc -> fb -> height - y ; <nl> 
static int dmx_section_feed_release_filter ( struct dmx_section_feed * feed , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( feed -> is_filtering ) <nl> + if ( feed -> is_filtering ) { <nl> + /* release dvbdmx -> mutex as far as <nl> + it is acquired by stop_filtering () itself */ <nl> + mutex_unlock (& dvbdmx -> mutex ); <nl> feed -> stop_filtering ( feed ); <nl> + mutex_lock (& dvbdmx -> mutex ); <nl> + } <nl>  <nl> spin_lock_irq (& dvbdmx -> lock ); <nl> f = dvbdmxfeed -> filter ;
void usb_serial_generic_process_read_urb ( struct urb * urb ) <nl> char * ch = ( char *) urb -> transfer_buffer ; <nl> int i ; <nl>  <nl> + if (! urb -> actual_length ) <nl> + return ; <nl> + <nl> tty = tty_port_tty_get (& port -> port ); <nl> if (! tty ) <nl> return ;
static int hba_setup_cid_tbls ( struct beiscsi_hba * phba ) <nl> kfree ( phba -> ep_array ); <nl> phba -> ep_array = NULL ; <nl> ret = - ENOMEM ; <nl> + <nl> + goto free_memory ; <nl> } <nl>  <nl> for ( i = 0 ; i < phba -> params . cxns_per_ctrl ; i ++) {
void iscsi_boot_destroy_kset ( struct iscsi_boot_kset * boot_kset ) <nl> iscsi_boot_remove_kobj ( boot_kobj ); <nl>  <nl> kset_unregister ( boot_kset -> kset ); <nl> + kfree ( boot_kset ); <nl> } <nl> EXPORT_SYMBOL_GPL ( iscsi_boot_destroy_kset );
static struct inotify_watch * create_watch ( struct inotify_device * dev , <nl> return ERR_PTR ( ret ); <nl> } <nl>  <nl> - dev -> last_wd = ret ; <nl> + dev -> last_wd = watch -> wd ; <nl> watch -> mask = mask ; <nl> atomic_set (& watch -> count , 0 ); <nl> INIT_LIST_HEAD (& watch -> d_list );
static int __devinit savagefb_probe ( struct pci_dev * dev , <nl> # if defined ( CONFIG_FB_SAVAGE_I2C ) <nl> savagefb_create_i2c_busses ( info ); <nl> savagefb_probe_i2c_connector ( info , & par -> edid ); <nl> - kfree ( par -> edid ); <nl> fb_edid_to_monspecs ( par -> edid , & info -> monspecs ); <nl> + kfree ( par -> edid ); <nl> fb_videomode_to_modelist ( info -> monspecs . modedb , <nl> info -> monspecs . modedb_len , <nl> & info -> modelist );
static void intel_dp_commit ( struct drm_encoder * encoder ) <nl>  <nl> if ( IS_eDP ( intel_dp ) || IS_PCH_eDP ( intel_dp )) <nl> ironlake_edp_backlight_on ( dev ); <nl> + intel_dp -> dpms_mode = DRM_MODE_DPMS_ON ; <nl> } <nl>  <nl> static void
static int __init parse_crashkernel_mem ( char * cmdline , <nl> } while (* cur ++ == ','); <nl>  <nl> if (* crash_size > 0 ) { <nl> - while (* cur != ' ' && * cur != '@') <nl> + while (* cur && * cur != ' ' && * cur != '@') <nl> cur ++; <nl> if (* cur == '@') { <nl> cur ++;
int btrfs_sync_file ( struct file * file , struct dentry * dentry , int datasync ) <nl> } <nl> mutex_lock (& dentry -> d_inode -> i_mutex ); <nl> out : <nl> - return ret > 0 ? EIO : ret ; <nl> + return ret > 0 ? - EIO : ret ; <nl> } <nl>  <nl> static const struct vm_operations_struct btrfs_file_vm_ops = {
extern struct page ** ceph_get_direct_page_vector ( const void __user * data , <nl> bool write_page ); <nl> extern void ceph_put_page_vector ( struct page ** pages , int num_pages , <nl> bool dirty ); <nl> - extern void ceph_release_page_vector ( struct page ** pages , int num_pages ); <nl> extern struct page ** ceph_alloc_page_vector ( int num_pages , gfp_t flags ); <nl> extern int ceph_copy_user_to_page_vector ( struct page ** pages , <nl> const void __user * data ,
int rsnd_dai_connect ( struct rsnd_mod * mod , <nl> if (! mod ) <nl> return - EIO ; <nl>  <nl> + if ( io -> mod [ type ]) <nl> + return - EINVAL ; <nl> + <nl> priv = rsnd_mod_to_priv ( mod ); <nl> dev = rsnd_priv_to_dev ( priv ); <nl> 
static int btrfs_get_sb ( struct file_system_type * fs_type , int flags , <nl> mutex_unlock (& root -> d_inode -> i_mutex ); <nl>  <nl> if ( IS_ERR ( new_root )) { <nl> + dput ( root ); <nl> deactivate_locked_super ( s ); <nl> error = PTR_ERR ( new_root ); <nl> - dput ( root ); <nl> goto error_free_subvol_name ; <nl> } <nl> if (! new_root -> d_inode ) {
static int get_dma_channel ( struct device_node * ssi_np , <nl> * dai -> platform name should already point to an allocated buffer . <nl> */ <nl> ret = of_address_to_resource ( dma_channel_np , 0 , & res ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + of_node_put ( dma_channel_np ); <nl> return ret ; <nl> + } <nl> snprintf (( char *) dai -> platform_name , DAI_NAME_SIZE , "% llx .% s ", <nl> ( unsigned long long ) res . start , dma_channel_np -> name ); <nl> 
static int mmu_topup_memory_cache_page ( struct kvm_mmu_memory_cache * cache , <nl> static void mmu_free_memory_cache_page ( struct kvm_mmu_memory_cache * mc ) <nl> { <nl> while ( mc -> nobjs ) <nl> - __free_page ( mc -> objects [-- mc -> nobjs ]); <nl> + free_page (( unsigned long ) mc -> objects [-- mc -> nobjs ]); <nl> } <nl>  <nl> static int __mmu_topup_memory_caches ( struct kvm_vcpu * vcpu , gfp_t gfp_flags )
static int snapshot_status ( struct dm_target * ti , status_type_t type , <nl> unsigned sz = 0 ; <nl> struct dm_snapshot * snap = ti -> private ; <nl>  <nl> + down_write (& snap -> lock ); <nl> + <nl> switch ( type ) { <nl> case STATUSTYPE_INFO : <nl> if (! snap -> valid ) <nl> static int snapshot_status ( struct dm_target * ti , status_type_t type , <nl> break ; <nl> } <nl>  <nl> + up_write (& snap -> lock ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int __init efi_rtc_probe ( struct platform_device * dev ) <nl> if ( IS_ERR ( rtc )) <nl> return PTR_ERR ( rtc ); <nl>  <nl> + rtc -> uie_unsupported = 1 ; <nl> platform_set_drvdata ( dev , rtc ); <nl>  <nl> return 0 ;
int pci_mmap_page_range ( struct pci_dev * dev , struct vm_area_struct * vma , <nl> */ <nl> prot |= _PAGE_CACHE_UC_MINUS ; <nl>  <nl> + prot |= _PAGE_IOMAP ; /* creating a mapping for IO */ <nl> + <nl> vma -> vm_page_prot = __pgprot ( prot ); <nl>  <nl> if ( io_remap_pfn_range ( vma , vma -> vm_start , vma -> vm_pgoff ,
static struct platform_driver gef_wdt_driver = { <nl> . of_match_table = gef_wdt_ids , <nl> }, <nl> . probe = gef_wdt_probe , <nl> + . remove = gef_wdt_remove , <nl> }; <nl>  <nl> static int __init gef_wdt_init ( void )
struct inode * ovl_d_select_inode ( struct dentry * dentry , unsigned file_flags ) <nl> ovl_path_upper ( dentry , & realpath ); <nl> } <nl>  <nl> + if ( realpath . dentry -> d_flags & DCACHE_OP_SELECT_INODE ) <nl> + return realpath . dentry -> d_op -> d_select_inode ( realpath . dentry , file_flags ); <nl> + <nl> return d_backing_inode ( realpath . dentry ); <nl> } <nl> 
static int cgroup_release_agent_show ( struct seq_file * seq , void * v ) <nl> { <nl> struct cgroup * cgrp = seq_css ( seq )-> cgroup ; <nl>  <nl> - if (! cgroup_lock_live_group ( cgrp )) <nl> - return - ENODEV ; <nl> + spin_lock (& release_agent_path_lock ); <nl> seq_puts ( seq , cgrp -> root -> release_agent_path ); <nl> + spin_unlock (& release_agent_path_lock ); <nl> seq_putc ( seq , '\ n '); <nl> - mutex_unlock (& cgroup_mutex ); <nl> return 0 ; <nl> } <nl> 
void ath9k_htc_rx_msg ( struct htc_target * htc_handle , <nl> return ; <nl> } <nl>  <nl> - if ( epid >= ENDPOINT_MAX ) { <nl> + if ( epid < 0 || epid >= ENDPOINT_MAX ) { <nl> if ( pipe_id != USB_REG_IN_PIPE ) <nl> dev_kfree_skb_any ( skb ); <nl> else
int irlan_extract_param ( __u8 * buf , char * name , char * value , __u16 * len ) <nl> memcpy (& val_len , buf + n , 2 ); /* To avoid alignment problems */ <nl> le16_to_cpus (& val_len ); n += 2 ; <nl>  <nl> - if ( val_len > 1016 ) { <nl> + if ( val_len >= 1016 ) { <nl> IRDA_DEBUG ( 2 , "% s (), parameter length to long \ n ", __func__ ); <nl> return - RSP_INVALID_COMMAND_FORMAT ; <nl> }
static int __devinit snd_cs423x_pnp_init_mpu ( int dev , struct pnp_dev * pdev ) <nl> static int __devinit snd_card_cs4232_pnp ( int dev , struct snd_card_cs4236 * acard , <nl> struct pnp_dev * pdev ) <nl> { <nl> + acard -> wss = pdev ; <nl> if ( snd_cs423x_pnp_init_wss ( dev , acard -> wss ) < 0 ) <nl> return - EBUSY ; <nl> cport [ dev ] = - 1 ;
int read_log ( struct tpm_bios_log * log ) <nl> log -> bios_event_log_end = log -> bios_event_log + len ; <nl>  <nl> virt = acpi_os_map_memory ( start , len ); <nl> + if (! virt ) { <nl> + kfree ( log -> bios_event_log ); <nl> + printk ("% s : ERROR - Unable to map memory \ n ", __func__ ); <nl> + return - EIO ; <nl> + } <nl>  <nl> memcpy ( log -> bios_event_log , virt , len ); <nl> 
static int mmc_sdio_resume ( struct mmc_host * host ) <nl> mmc_claim_host ( host ); <nl> err = mmc_sdio_init_card ( host , host -> ocr , host -> card , <nl> ( host -> pm_flags & MMC_PM_KEEP_POWER )); <nl> + if (! err && host -> sdio_irqs ) <nl> + mmc_signal_sdio_irq ( host ); <nl> mmc_release_host ( host ); <nl>  <nl> /*
static void binder_transaction ( struct binder_proc * proc , <nl> proc -> pid , thread -> pid , <nl> ( u64 ) fp -> binder , node -> debug_id , <nl> ( u64 ) fp -> cookie , ( u64 ) node -> cookie ); <nl> + return_error = BR_FAILED_REPLY ; <nl> goto err_binder_get_ref_for_node_failed ; <nl> } <nl> ref = binder_get_ref_for_node ( target_proc , node );
int ip6_mr_input ( struct sk_buff * skb ) <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> cache = ip6mr_cache_find ( mrt ,
void dump_stack ( void ) <nl>  <nl> show_stack ( current , & stack ); <nl> } <nl> + EXPORT_SYMBOL ( dump_stack ); <nl>  <nl> void show_registers ( struct pt_regs * regs ) <nl> {
static void parse_elf ( void * output ) <nl> default : /* Ignore other PT_ * */ break ; <nl> } <nl> } <nl> + <nl> + free ( phdrs ); <nl> } <nl>  <nl> asmlinkage void decompress_kernel ( void * rmode , memptr heap ,
out_unlock : <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
static struct dmi_system_id __initdata acpisleep_dmi_table [] = { <nl> DMI_MATCH ( DMI_PRODUCT_NAME , " Macmini1 , 1 "), <nl> }, <nl> }, <nl> + { <nl> + . callback = init_old_suspend_ordering , <nl> + . ident = " Asus Pundit P1 - AH2 ( M2N8L motherboard )", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_BOARD_VENDOR , " ASUSTek Computer INC ."), <nl> + DMI_MATCH ( DMI_BOARD_NAME , " M2N8L "), <nl> + }, <nl> + }, <nl> {}, <nl> }; <nl> # endif /* CONFIG_SUSPEND */
i915_gem_object_bind_to_gtt ( struct drm_gem_object * obj , unsigned alignment ) <nl> bool retry_alloc = false ; <nl> int ret ; <nl>  <nl> - if ( dev_priv -> mm . suspended ) <nl> - return - EBUSY ; <nl> - <nl> if ( obj_priv -> madv != I915_MADV_WILLNEED ) { <nl> DRM_ERROR (" Attempting to bind a purgeable object \ n "); <nl> return - EINVAL ;
static int beiscsi_eh_device_reset ( struct scsi_cmnd * sc ) <nl> if (! abrt_task -> sc || abrt_task -> state == ISCSI_TASK_FREE ) <nl> continue ; <nl>  <nl> - if ( abrt_task -> sc -> device -> lun != abrt_task -> sc -> device -> lun ) <nl> + if ( sc -> device -> lun != abrt_task -> sc -> device -> lun ) <nl> continue ; <nl>  <nl> /* Invalidate WRB Posted for this Task */
static struct collection collections [] = { <nl>  <nl> /* Iterate over all benchmarks within a collection : */ <nl> # define for_each_bench ( coll , bench ) \ <nl> - for ( bench = coll -> benchmarks ; bench -> name ; bench ++) <nl> + for ( bench = coll -> benchmarks ; bench && bench -> name ; bench ++) <nl>  <nl> static void dump_benchmarks ( struct collection * coll ) <nl> {
void card_send_command ( struct ft1000_device * ft1000dev , void * ptempbuffer , <nl>  <nl> DEBUG (" card_send_command : enter card_send_command ... size =% d \ n ", size ); <nl>  <nl> - commandbuf = ( unsigned char *) kmalloc ( size + 2 , GFP_KERNEL ); <nl> + commandbuf = kmalloc ( size + 2 , GFP_KERNEL ); <nl> memcpy (( void *) commandbuf + 2 , ( void *) ptempbuffer , size ); <nl>  <nl> ft1000_read_register ( ft1000dev , & temp , FT1000_REG_DOORBELL );
static int hyp_init_cpu_notify ( struct notifier_block * self , <nl> switch ( action ) { <nl> case CPU_STARTING : <nl> case CPU_STARTING_FROZEN : <nl> - cpu_init_hyp_mode ( NULL ); <nl> + if ( __hyp_get_vectors () == hyp_default_vectors ) <nl> + cpu_init_hyp_mode ( NULL ); <nl> break ; <nl> } <nl> 
static int gfar_spauseparam ( struct net_device * dev , <nl> struct gfar __iomem * regs = priv -> gfargrp [ 0 ]. regs ; <nl> u32 oldadv , newadv ; <nl>  <nl> + if (! phydev ) <nl> + return - ENODEV ; <nl> + <nl> if (!( phydev -> supported & SUPPORTED_Pause ) || <nl> (!( phydev -> supported & SUPPORTED_Asym_Pause ) && <nl> ( epause -> rx_pause != epause -> tx_pause )))
int ide_noacpitfs = 1 ; <nl> int ide_noacpionboot = 1 ; <nl> # endif <nl>  <nl> -/* <nl> - * This is declared extern in ide . h , for access by other IDE modules : <nl> - */ <nl> ide_hwif_t ide_hwifs [ MAX_HWIFS ]; /* master data repository */ <nl>  <nl> - EXPORT_SYMBOL ( ide_hwifs ); <nl> - <nl> static void ide_port_init_devices_data ( ide_hwif_t *); <nl>  <nl> /*
static int sg_io ( struct file * file , request_queue_t * q , <nl>  <nl> rq -> cmd_type = REQ_TYPE_BLOCK_PC ; <nl>  <nl> - /* <nl> - * bounce this after holding a reference to the original bio , it ' s <nl> - * needed for proper unmapping <nl> - */ <nl> - if ( rq -> bio ) <nl> - blk_queue_bounce ( q , & rq -> bio ); <nl> - <nl> rq -> timeout = jiffies_to_msecs ( hdr -> timeout ); <nl> if (! rq -> timeout ) <nl> rq -> timeout = q -> sg_timeout ;
dma_addr_t xhci_trb_virt_to_dma ( struct xhci_segment * seg , <nl> return 0 ; <nl> /* offset in TRBs */ <nl> segment_offset = trb - seg -> trbs ; <nl> - if ( segment_offset > TRBS_PER_SEGMENT ) <nl> + if ( segment_offset >= TRBS_PER_SEGMENT ) <nl> return 0 ; <nl> return seg -> dma + ( segment_offset * sizeof (* trb )); <nl> }
void sctp_transport_lower_cwnd ( struct sctp_transport * transport , <nl> transport -> ssthresh = max ( transport -> cwnd / 2 , <nl> 4 * transport -> asoc -> pathmtu ); <nl> transport -> cwnd = transport -> asoc -> pathmtu ; <nl> + <nl> + /* T3 - rtx also clears fast recovery on the transport */ <nl> + transport -> fast_recovery = 0 ; <nl> break ; <nl>  <nl> case SCTP_LOWER_CWND_FAST_RTX :
static void cirrus_connector_destroy ( struct drm_connector * connector ) <nl> kfree ( connector ); <nl> } <nl>  <nl> - struct drm_connector_helper_funcs cirrus_vga_connector_helper_funcs = { <nl> + static const struct drm_connector_helper_funcs cirrus_vga_connector_helper_funcs = { <nl> . get_modes = cirrus_vga_get_modes , <nl> . best_encoder = cirrus_connector_best_encoder , <nl> }; <nl>  <nl> - struct drm_connector_funcs cirrus_vga_connector_funcs = { <nl> + static const struct drm_connector_funcs cirrus_vga_connector_funcs = { <nl> . dpms = drm_helper_connector_dpms , <nl> . detect = cirrus_vga_detect , <nl> . fill_modes = drm_helper_probe_single_connector_modes ,
static unsigned long ramfs_nommu_get_unmapped_area ( struct file * file , <nl>  <nl> /* gang - find the pages */ <nl> ret = - ENOMEM ; <nl> - pages = kzalloc ( lpages * sizeof ( struct page *), GFP_KERNEL ); <nl> + pages = kcalloc ( lpages , sizeof ( struct page *), GFP_KERNEL ); <nl> if (! pages ) <nl> goto out_free ; <nl> 
static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> } <nl> logical = re -> logical ; <nl>  <nl> - reada_extent_put ( fs_info , re ); <nl> - <nl> atomic_inc (& dev -> reada_in_flight ); <nl> ret = reada_tree_block_flagged ( fs_info -> extent_root , logical , <nl> mirror_num , & eb ); <nl> static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> if ( eb ) <nl> free_extent_buffer ( eb ); <nl>  <nl> + reada_extent_put ( fs_info , re ); <nl> + <nl> return 1 ; <nl>  <nl> }
vt6656_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl> hw = ieee80211_alloc_hw ( sizeof ( struct vnt_private ), & vnt_mac_ops ); <nl> if (! hw ) { <nl> dev_err (& udev -> dev , " could not register ieee80211_hw \ n "); <nl> + rc = - ENOMEM ; <nl> goto err_nomem ; <nl> } <nl> 
static long media_device_enum_entities ( struct media_device * mdev , <nl> struct media_entity * ent ; <nl> struct media_entity_desc u_ent ; <nl>  <nl> + memset (& u_ent , 0 , sizeof ( u_ent )); <nl> if ( copy_from_user (& u_ent . id , & uent -> id , sizeof ( u_ent . id ))) <nl> return - EFAULT ; <nl> 
static int ethtool_phys_id ( struct net_device * dev , void __user * useraddr ) <nl> if ( rc == 0 ) { <nl> /* Driver will handle this itself */ <nl> schedule_timeout_interruptible ( <nl> - id . data ? id . data : MAX_SCHEDULE_TIMEOUT ); <nl> + id . data ? ( id . data * HZ ) : MAX_SCHEDULE_TIMEOUT ); <nl> } else { <nl> /* Driver expects to be called periodically */ <nl> do {
static int scsi_eh_completed_normally ( struct scsi_cmnd * scmd ) <nl> scsi_handle_queue_full ( scmd -> device ); <nl> /* fall through */ <nl> case BUSY : <nl> + return NEEDS_RETRY ; <nl> default : <nl> return FAILED ; <nl> }
static int easycap_usb_probe ( struct usb_interface * intf , <nl> /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */ <nl> pdata_urb = kzalloc ( sizeof ( struct data_urb ), GFP_KERNEL ); <nl> if (! pdata_urb ) { <nl> + usb_free_urb ( purb ); <nl> SAM (" ERROR : Could not allocate struct data_urb .\ n "); <nl> return - ENOMEM ; <nl> }
static int econet_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> if ( peer ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + memset ( sec , 0 , sizeof (* sec )); <nl> mutex_lock (& econet_mutex ); <nl>  <nl> sk = sock -> sk ;
void hugetlb_unreserve_pages ( struct inode * inode , long offset , long freed ) <nl> long chg = region_truncate (& inode -> i_mapping -> private_list , offset ); <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> - inode -> i_blocks -= blocks_per_huge_page ( h ); <nl> + inode -> i_blocks -= ( blocks_per_huge_page ( h ) * freed ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> hugetlb_put_quota ( inode -> i_mapping , ( chg - freed ));
static enum i40e_media_type i40e_get_media_type ( struct i40e_hw * hw ) <nl> } <nl>  <nl> # define I40E_PF_RESET_WAIT_COUNT_A0 200 <nl> -# define I40E_PF_RESET_WAIT_COUNT 110 <nl> +# define I40E_PF_RESET_WAIT_COUNT 200 <nl> /** <nl> * i40e_pf_reset - Reset the PF <nl> * @ hw : pointer to the hardware structure
ath_reg_apply_active_scan_flags ( struct wiphy * wiphy , <nl> int r ; <nl>  <nl> sband = wiphy -> bands [ IEEE80211_BAND_2GHZ ]; <nl> + if (! sband ) <nl> + return ; <nl>  <nl> /* <nl> * If no country IE has been received always enable active scan
int do_sigtimedwait ( const sigset_t * which , siginfo_t * info , <nl> recalc_sigpending (); <nl> spin_unlock_irq (& tsk -> sighand -> siglock ); <nl>  <nl> - timeout = schedule_timeout_interruptible ( timeout ); <nl> + timeout = freezable_schedule_timeout_interruptible ( timeout ); <nl>  <nl> spin_lock_irq (& tsk -> sighand -> siglock ); <nl> __set_task_blocked ( tsk , & tsk -> real_blocked );
static int ir_do_setkeycode ( struct input_dev * dev , <nl> break ; <nl> } <nl>  <nl> - if ( old_keycode == KEY_RESERVED ) { <nl> + if ( old_keycode == KEY_RESERVED && keycode != KEY_RESERVED ) { <nl> /* No previous mapping found , we might need to grow the table */ <nl> if ( ir_resize_table ( rc_tab )) <nl> return - ENOMEM ;
int brcmf_fws_hdrpull ( struct brcmf_pub * drvr , int ifidx , s16 signal_len , <nl> if (! signal_len ) <nl> return 0 ; <nl> /* if flow control disabled , skip to packet data and leave */ <nl> - if (! fws -> fw_signals ) { <nl> + if ((! fws ) || (! fws -> fw_signals )) { <nl> skb_pull ( skb , signal_len ); <nl> return 0 ; <nl> }
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 84 " <nl> -# define DRV_MODULE_RELDATE " October 12 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 85 " <nl> +# define DRV_MODULE_RELDATE " October 18 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
int max1363_single_channel_from_ring ( long mask , struct max1363_state * st ) <nl> ret = - EBUSY ; <nl> goto error_ret ; <nl> } <nl> - numvals = hweight_long ( st -> current_mode -> modemask ); <nl>  <nl> - ring_data = kmalloc ( numvals * 2 , GFP_KERNEL ); <nl> + ring_data = kmalloc ( ring -> access . get_bytes_per_datum ( ring ), GFP_KERNEL ); <nl> if ( ring_data == NULL ) { <nl> ret = - ENOMEM ; <nl> goto error_ret ;
void kvm_ioapic_calculate_eoi_exitmap ( struct kvm_vcpu * vcpu , <nl> if (! e -> fields . mask && <nl> ( e -> fields . trig_mode == IOAPIC_LEVEL_TRIG || <nl> kvm_irq_has_notifier ( ioapic -> kvm , KVM_IRQCHIP_IOAPIC , <nl> - index ))) { <nl> + index ) || index == RTC_GSI )) { <nl> if ( kvm_apic_match_dest ( vcpu , NULL , 0 , <nl> e -> fields . dest_id , e -> fields . dest_mode )) <nl> __set_bit ( e -> fields . vector , ( unsigned long *) eoi_exit_bitmap );
DECLARE_EVENT_CLASS ( xhci_log_event , <nl> __field ( u64 , dma ) <nl> __field ( u32 , status ) <nl> __field ( u32 , flags ) <nl> - __dynamic_array ( __le32 , trb , 4 ) <nl> + __dynamic_array ( u8 , trb , sizeof ( struct xhci_generic_trb )) <nl> ), <nl> TP_fast_assign ( <nl> __entry -> va = trb_va ;
static void mv_otg_work ( struct work_struct * work ) <nl> struct usb_otg * otg ; <nl> int old_state ; <nl>  <nl> - mvotg = container_of (( struct delayed_work *) work , struct mv_otg , work ); <nl> + mvotg = container_of ( to_delayed_work ( work ), struct mv_otg , work ); <nl>  <nl> run : <nl> /* work queue is single thread , or we need spin_lock to protect */
retry : <nl> ! atomic_inc_not_zero (& ctx -> refcount )) { <nl> raw_spin_unlock (& ctx -> lock ); <nl> ctx = NULL ; <nl> + } else { <nl> + WARN_ON_ONCE ( ctx -> task != task ); <nl> } <nl> - <nl> - WARN_ON_ONCE ( ctx -> task != task ); <nl> } <nl> rcu_read_unlock (); <nl> if (! ctx )
static void sonic_rx ( struct net_device * dev ) <nl> status = sonic_rda_get ( dev , entry , SONIC_RD_STATUS ); <nl> if ( status & SONIC_RCR_PRX ) { <nl> /* Malloc up new buffer . */ <nl> - new_skb = netdev_alloc_skb ( SONIC_RBSIZE + 2 ); <nl> + new_skb = netdev_alloc_skb ( dev , SONIC_RBSIZE + 2 ); <nl> if ( new_skb == NULL ) { <nl> printk ( KERN_ERR "% s : Memory squeeze , dropping packet .\ n ", dev -> name ); <nl> lp -> stats . rx_dropped ++;
static void tpacket_destruct_skb ( struct sk_buff * skb ) <nl> struct packet_sock * po = pkt_sk ( skb -> sk ); <nl> void * ph ; <nl>  <nl> - BUG_ON ( skb == NULL ); <nl> - <nl> if ( likely ( po -> tx_ring . pg_vec )) { <nl> ph = skb_shinfo ( skb )-> destructor_arg ; <nl> BUG_ON ( __packet_get_status ( po , ph ) != TP_STATUS_SENDING );
unsigned long long sched_clock ( void ) <nl>  <nl> /* jiffies based sched_clock if no clocksource is installed */ <nl> if (! clocksource_sh . rating ) <nl> - return ( unsigned long long ) jiffies * ( NSEC_PER_SEC / HZ ); <nl> + return ( jiffies_64 - INITIAL_JIFFIES ) * ( NSEC_PER_SEC / HZ ); <nl>  <nl> cycles = clocksource_sh . read (& clocksource_sh ); <nl> return cyc2ns (& clocksource_sh , cycles );
static int crypto_ccm_auth ( struct aead_request * req , struct scatterlist * plain , <nl> if ( assoclen ) { <nl> pctx -> ilen = format_adata ( idata , assoclen ); <nl> get_data_to_compute ( cipher , pctx , req -> assoc , req -> assoclen ); <nl> + } else { <nl> + pctx -> ilen = 0 ; <nl> } <nl>  <nl> /* compute plaintext into mac */
probe_fail_cdrom_register : <nl> del_gendisk ( gd . disk ); <nl> probe_fail_no_disk : <nl> kfree ( gd . cd_info ); <nl> + probe_fail_no_mem : <nl> unregister_blkdev ( gdrom_major , GDROM_DEV_NAME ); <nl> gdrom_major = 0 ; <nl> - probe_fail_no_mem : <nl> pr_warning (" Probe failed - error is 0x % X \ n ", err ); <nl> return err ; <nl> }
skip_type : <nl> if ( pmu -> pmu_cpu_context ) <nl> goto got_cpu_context ; <nl>  <nl> + ret = - ENOMEM ; <nl> pmu -> pmu_cpu_context = alloc_percpu ( struct perf_cpu_context ); <nl> if (! pmu -> pmu_cpu_context ) <nl> goto free_dev ;
int vnt_init ( struct vnt_private * priv ) <nl>  <nl> priv -> mac_hw = true ; <nl>  <nl> + vnt_radio_power_off ( priv ); <nl> + <nl> return 0 ; <nl> } <nl> 
int eprintf ( int level , const char * fmt , ...) <nl>  <nl> if ( verbose >= level ) { <nl> va_start ( args , fmt ); <nl> - if ( use_browser > 1 ) <nl> + if ( use_browser >= 1 ) <nl> ui_helpline__vshow ( fmt , args ); <nl> else <nl> ret = vfprintf ( stderr , fmt , args );
static void sbp2_prep_command_orb_sg ( struct sbp2_command_orb * orb , <nl>  <nl> /* loop through and fill out our SBP - 2 page tables <nl> * ( and split up anything too large ) */ <nl> - for ( i = 0 , sg_count = 0 ; i < count ; i ++, sgpnt ++) { <nl> + for ( i = 0 , sg_count = 0 ; i < count ; i ++, sgpnt = sg_next ( sgpnt )) { <nl> sg_len = sg_dma_len ( sgpnt ); <nl> sg_addr = sg_dma_address ( sgpnt ); <nl> while ( sg_len ) {
int scsi_error_handler ( void * data ) <nl> set_current_state ( TASK_INTERRUPTIBLE ); <nl> } <nl>  <nl> + __set_current_state ( TASK_RUNNING ); <nl> + <nl> SCSI_LOG_ERROR_RECOVERY ( 1 , printk (" Error handler scsi_eh_ % d " <nl> " exiting \ n ", shost -> host_no )); <nl> 
static struct scsi_host_template aac_driver_template = { <nl>  <nl> static void __aac_shutdown ( struct aac_dev * aac ) <nl> { <nl> - kthread_stop ( aac -> thread ); <nl> + if ( aac -> aif_thread ) <nl> + kthread_stop ( aac -> thread ); <nl> aac_send_shutdown ( aac ); <nl> aac_adapter_disable_int ( aac ); <nl> free_irq ( aac -> pdev -> irq , aac );
static void ieee80211_enable_ps ( struct ieee80211_local * local , <nl> { <nl> struct ieee80211_conf * conf = & local -> hw . conf ; <nl>  <nl> + /* <nl> + * If we are scanning right now then the parameters will <nl> + * take effect when scan finishes . <nl> + */ <nl> + if ( local -> hw_scanning || local -> sw_scanning ) <nl> + return ; <nl> + <nl> if ( conf -> dynamic_ps_timeout > 0 && <nl> !( local -> hw . flags & IEEE80211_HW_SUPPORTS_DYNAMIC_PS )) { <nl> mod_timer (& local -> dynamic_ps_timer , jiffies +
static ssize_t ci_port_test_write ( struct file * file , const char __user * ubuf , <nl> if ( sscanf ( buf , "% u ", & mode ) != 1 ) <nl> return - EINVAL ; <nl>  <nl> + if ( mode > 255 ) <nl> + return - EBADRQC ; <nl> + <nl> pm_runtime_get_sync ( ci -> dev ); <nl> spin_lock_irqsave (& ci -> lock , flags ); <nl> ret = hw_port_test_set ( ci , mode );
static int change_memory_common ( unsigned long addr , int numpages , <nl> WARN_ON_ONCE ( 1 ); <nl> } <nl>  <nl> + if (! numpages ) <nl> + return 0 ; <nl> + <nl> if ( start < MODULES_VADDR || start >= MODULES_END ) <nl> return - EINVAL ; <nl> 
scsi_internal_device_unblock ( struct scsi_device * sdev , <nl> * Try to transition the scsi device to SDEV_RUNNING or one of the <nl> * offlined states and goose the device queue if successful . <nl> */ <nl> - if ( sdev -> sdev_state == SDEV_BLOCK ) <nl> + if (( sdev -> sdev_state == SDEV_BLOCK ) || <nl> + ( sdev -> sdev_state == SDEV_TRANSPORT_OFFLINE )) <nl> sdev -> sdev_state = new_state ; <nl> else if ( sdev -> sdev_state == SDEV_CREATED_BLOCK ) { <nl> if ( new_state == SDEV_TRANSPORT_OFFLINE ||
radeon_user_framebuffer_create ( struct drm_device * dev , <nl> if ( ret ) { <nl> kfree ( radeon_fb ); <nl> drm_gem_object_unreference_unlocked ( obj ); <nl> - return NULL ; <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> return & radeon_fb -> base ;
static int gen6_drpc_info ( struct seq_file * m ) <nl>  <nl> rpmodectl1 = I915_READ ( GEN6_RP_CONTROL ); <nl> rcctl1 = I915_READ ( GEN6_RC_CONTROL ); <nl> - sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> mutex_unlock (& dev -> struct_mutex ); <nl> + mutex_lock (& dev_priv -> rps . hw_lock ); <nl> + sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> + mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> seq_printf ( m , " Video Turbo Mode : % s \ n ", <nl> yesno ( rpmodectl1 & GEN6_RP_MEDIA_TURBO ));
static int __devinit piix_init_one ( struct pci_dev * pdev , <nl> u8 tmp ; <nl> pci_read_config_byte ( pdev , PIIX_SCC , & tmp ); <nl> if ( tmp == PIIX_AHCI_DEVICE ) { <nl> - int rc = piix_disable_ahci ( pdev ); <nl> + rc = piix_disable_ahci ( pdev ); <nl> if ( rc ) <nl> return rc ; <nl> }
static bool compliance_mode_recovery_timer_quirk_check ( void ) <nl>  <nl> dmi_product_name = dmi_get_system_info ( DMI_PRODUCT_NAME ); <nl> dmi_sys_vendor = dmi_get_system_info ( DMI_SYS_VENDOR ); <nl> + if (! dmi_product_name || ! dmi_sys_vendor ) <nl> + return false ; <nl>  <nl> if (!( strstr ( dmi_sys_vendor , " Hewlett - Packard "))) <nl> return false ;
void ixgbe_update_stats ( struct ixgbe_adapter * adapter ) <nl> u32 i , missed_rx = 0 , mpc , bprc , lxon , lxoff , xon_off_tot ; <nl> u64 non_eop_descs = 0 , restart_queue = 0 ; <nl>  <nl> + if ( test_bit ( __IXGBE_DOWN , & adapter -> state ) || <nl> + test_bit ( __IXGBE_RESETTING , & adapter -> state )) <nl> + return ; <nl> + <nl> if ( adapter -> flags2 & IXGBE_FLAG2_RSC_ENABLED ) { <nl> u64 rsc_count = 0 ; <nl> u64 rsc_flush = 0 ;
DEFINE_PER_CPU_READ_MOSTLY ( cpumask_var_t , cpu_llc_shared_map ); <nl> DEFINE_PER_CPU_SHARED_ALIGNED ( struct cpuinfo_x86 , cpu_info ); <nl> EXPORT_PER_CPU_SYMBOL ( cpu_info ); <nl>  <nl> - static DEFINE_PER_CPU ( struct completion , die_complete ); <nl> - <nl> atomic_t init_deasserted ; <nl>  <nl> /* <nl> void cpu_disable_common ( void ) <nl> fixup_irqs (); <nl> } <nl>  <nl> + static DEFINE_PER_CPU ( struct completion , die_complete ); <nl> + <nl> int native_cpu_disable ( void ) <nl> { <nl> int ret ;
int iwl_mvm_sched_scan_start ( struct iwl_mvm * mvm , <nl> return - EBUSY ; <nl> } <nl>  <nl> + /* we don ' t support " match all " in the firmware */ <nl> + if (! req -> n_match_sets ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> ret = iwl_mvm_check_running_scans ( mvm , type ); <nl> if ( ret ) <nl> return ret ;
static IIO_DEV_ATTR_SAMP_FREQ ( S_IWUSR | S_IRUGO , <nl> adis16400_read_frequency , <nl> adis16400_write_frequency ); <nl>  <nl> - static IIO_CONST_ATTR_SAMP_FREQ_AVAIL (" 409 546 819 1638 "); <nl> - <nl> static const u8 adis16400_addresses [] = { <nl> [ ADIS16400_SCAN_GYRO_X ] = ADIS16400_XGYRO_OFF , <nl> [ ADIS16400_SCAN_GYRO_Y ] = ADIS16400_YGYRO_OFF , <nl> static const struct iio_chan_spec adis16334_channels [] = { <nl>  <nl> static struct attribute * adis16400_attributes [] = { <nl> & iio_dev_attr_sampling_frequency . dev_attr . attr , <nl> - & iio_const_attr_sampling_frequency_available . dev_attr . attr , <nl> NULL <nl> }; <nl> 
static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
static void tg3_timer ( unsigned long __opaque ) <nl> * resets . <nl> */ <nl> if (!-- tp -> asf_counter ) { <nl> - if ( tp -> tg3_flags & TG3_FLAG_ENABLE_ASF ) { <nl> + if (( tp -> tg3_flags & TG3_FLAG_ENABLE_ASF ) && <nl> + !( tp -> tg3_flags3 & TG3_FLG3_ENABLE_APE )) { <nl> u32 val ; <nl>  <nl> tg3_wait_for_event_ack ( tp );
static int zoran_dqbuf ( struct file * file , void * __fh , struct v4l2_buffer * buf ) <nl> res = - EAGAIN ; <nl> goto dqbuf_unlock_and_return ; <nl> } <nl> + bs . frame = 0 ; /* suppress compiler warning */ <nl> res = jpg_sync ( fh , & bs ); <nl> if ( res ) <nl> goto dqbuf_unlock_and_return ;
static irqreturn_t intel_sst_interrupt ( int irq , void * context ) <nl> unsigned int size = 0 , str_id ; <nl> struct stream_info * stream ; <nl>  <nl> + /* Do not handle interrupt in suspended state */ <nl> + if ( drv -> sst_state == SST_SUSPENDED ) <nl> + return IRQ_NONE ; <nl> /* Interrupt arrived , check src */ <nl> isr . full = sst_shim_read ( drv -> shim , SST_ISRX ); <nl> 
static unsigned long iommu_range_alloc ( struct iommu_table * tbl , <nl> /* This allocator was derived from x86_64 ' s bit string search */ <nl>  <nl> /* Sanity check */ <nl> - if ( unlikely ( npages ) == 0 ) { <nl> + if ( unlikely ( npages == 0 )) { <nl> if ( printk_ratelimit ()) <nl> WARN_ON ( 1 ); <nl> return DMA_ERROR_CODE ;
__xfrm4_bundle_create ( struct xfrm_policy * policy , struct xfrm_state ** xfrm , int <nl> afinfo = xfrm_state_get_afinfo ( dst_prev -> xfrm -> props . family ); <nl> if (! afinfo ) { <nl> dst = * dst_p ; <nl> + err = - EAFNOSUPPORT ; <nl> goto error ; <nl> } <nl> dst_prev -> output = afinfo -> output ;
static struct platform_driver omap_hsmmc_driver = { <nl> static int __init omap_hsmmc_init ( void ) <nl> { <nl> /* Register the MMC driver */ <nl> - return platform_driver_register (& omap_hsmmc_driver ); <nl> + return platform_driver_probe (& omap_hsmmc_driver , omap_hsmmc_probe ); <nl> } <nl>  <nl> static void __exit omap_hsmmc_cleanup ( void )
store_priv_session_ ## field ( struct device * dev , \ <nl> # define iscsi_priv_session_rw_attr ( field , format ) \ <nl> iscsi_priv_session_attr_show ( field , format ) \ <nl> iscsi_priv_session_attr_store ( field ) \ <nl> - static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUGO , \ <nl> + static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUSR , \ <nl> show_priv_session_ ## field , \ <nl> store_priv_session_ ## field ) <nl> iscsi_priv_session_rw_attr ( recovery_tmo , "% d ");
static int dr_interception ( struct vcpu_svm * svm ) <nl> kvm_register_write (& svm -> vcpu , reg , val ); <nl> } <nl>  <nl> + skip_emulated_instruction (& svm -> vcpu ); <nl> + <nl> return 1 ; <nl> } <nl> 
EXPORT_SYMBOL ( of_find_node_with_property ); <nl> const struct of_device_id * of_match_node ( const struct of_device_id * matches , <nl> const struct device_node * node ) <nl> { <nl> + if (! matches ) <nl> + return NULL ; <nl> + <nl> while ( matches -> name [ 0 ] || matches -> type [ 0 ] || matches -> compatible [ 0 ]) { <nl> int match = 1 ; <nl> if ( matches -> name [ 0 ])
static struct fileIdentDesc * udf_find_entry ( struct inode * dir , <nl> } <nl>  <nl> if (( cfi -> fileCharacteristics & FID_FILE_CHAR_PARENT ) && <nl> - isdotdot ) { <nl> - brelse ( epos . bh ); <nl> - return fi ; <nl> - } <nl> + isdotdot ) <nl> + goto out_ok ; <nl>  <nl> if (! lfi ) <nl> continue ;
struct led_classdev { <nl>  <nl> extern int led_classdev_register ( struct device * parent , <nl> struct led_classdev * led_cdev ); <nl> - extern void led_classdev_unregister ( struct led_classdev * lcd ); <nl> + extern void led_classdev_unregister ( struct led_classdev * led_cdev ); <nl> extern void led_classdev_suspend ( struct led_classdev * led_cdev ); <nl> extern void led_classdev_resume ( struct led_classdev * led_cdev ); <nl> 
static int mc13783_probe ( struct snd_soc_codec * codec ) <nl> { <nl> struct mc13783_priv * priv = snd_soc_codec_get_drvdata ( codec ); <nl>  <nl> - codec -> control_data = priv -> mc13xxx ; <nl> - <nl> mc13xxx_lock ( priv -> mc13xxx ); <nl>  <nl> /* these are the reset values */
static const struct attribute_group isl29108_group = { <nl> static int isl29018_chip_init ( struct isl29018_chip * chip ) <nl> { <nl> int status ; <nl> - int new_adc_bit ; <nl> + unsigned int new_adc_bit ; <nl> unsigned int new_range ; <nl>  <nl> /* Code added per Intersil Application Note 1534 :
static int ov7670_read ( struct i2c_client * c , unsigned char reg , <nl> int ret ; <nl>  <nl> ret = i2c_smbus_read_byte_data ( c , reg ); <nl> - if ( ret >= 0 ) <nl> + if ( ret >= 0 ) { <nl> * value = ( unsigned char ) ret ; <nl> + ret = 0 ; <nl> + } <nl> return ret ; <nl> } <nl> 
static ssize_t mtd_write ( struct file * file , const char __user * buf , size_t count <nl> ops . mode = MTD_OOB_RAW ; <nl> ops . datbuf = kbuf ; <nl> ops . oobbuf = NULL ; <nl> + ops . ooboffs = 0 ; <nl> ops . len = len ; <nl>  <nl> ret = mtd -> write_oob ( mtd , * ppos , & ops );
static int corsair_input_mapping ( struct hid_device * dev , <nl> { <nl> int gkey ; <nl>  <nl> + if (( usage -> hid & HID_USAGE_PAGE ) != HID_UP_KEYBOARD ) <nl> + return 0 ; <nl> + <nl> gkey = corsair_usage_to_gkey ( usage -> hid & HID_USAGE ); <nl> if ( gkey != 0 ) { <nl> hid_map_usage_clear ( input , usage , bit , max , EV_KEY ,
remove_write ( struct device_driver * drv , const char * buf , size_t count ) <nl> count = IFNAMSIZ - 1 ; <nl>  <nl> for ( i = 0 , p =( char *) buf ; i < count && * p ; i ++, p ++) { <nl> - if ((* p == '\ n ') | (* p == ' ')) { <nl> + if ((* p == '\ n ') || (* p == ' ')) { <nl> /* trailing lf , grr */ <nl> break ; <nl> } else {
static int bnx2x_init_dev ( struct bnx2x * bp , struct pci_dev * pdev , <nl> pci_write_config_dword ( bp -> pdev , PCICFG_GRC_ADDRESS , <nl> PCICFG_VENDOR_ID_OFFSET ); <nl>  <nl> + /* Set PCIe reset type to fundamental for EEH recovery */ <nl> + pdev -> needs_freset = 1 ; <nl> + <nl> /* AER ( Advanced Error reporting ) configuration */ <nl> rc = pci_enable_pcie_error_reporting ( pdev ); <nl> if (! rc )
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
static int intel_crtc_page_flip ( struct drm_crtc * crtc , <nl> to_intel_plane ( primary )-> frontbuffer_bit ); <nl>  <nl> intel_fbc_disable ( dev ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> intel_frontbuffer_flip_prepare ( dev , <nl> to_intel_plane ( primary )-> frontbuffer_bit ); <nl> - mutex_unlock (& dev -> struct_mutex ); <nl>  <nl> trace_i915_flip_request ( intel_crtc -> plane , obj ); <nl> 
befs_fill_super ( struct super_block * sb , void * data , int silent ) <nl> brelse ( bh ); <nl>  <nl> unacquire_priv_sbp : <nl> + kfree ( befs_sb -> mount_opts . iocharset ); <nl> kfree ( sb -> s_fs_info ); <nl>  <nl> unacquire_none :
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static int hdmi_pcm_open ( struct hda_pcm_stream * hinfo , <nl> * codec_pars = * hinfo ; <nl>  <nl> eld = & spec -> sink_eld [ idx ]; <nl> - if ( eld -> sad_count > 0 ) { <nl> + if ( eld -> eld_valid && eld -> sad_count > 0 ) { <nl> hdmi_eld_update_pcm_info ( eld , hinfo , codec_pars ); <nl> if ( hinfo -> channels_min > hinfo -> channels_max || <nl> ! hinfo -> rates || ! hinfo -> formats )
static int do_end_io ( struct multipath * m , struct request * clone , <nl> if (! error && ! clone -> errors ) <nl> return 0 ; /* I / O complete */ <nl>  <nl> - if ( error == - EOPNOTSUPP || error == - EREMOTEIO ) <nl> + if ( error == - EOPNOTSUPP || error == - EREMOTEIO || error == - EILSEQ ) <nl> return error ; <nl>  <nl> if ( mpio -> pgpath )
static int m5mols_s_stream ( struct v4l2_subdev * sd , int enable ) <nl> if ( enable ) { <nl> if ( is_code ( code , M5MOLS_RESTYPE_MONITOR )) <nl> ret = m5mols_start_monitor ( info ); <nl> - if ( is_code ( code , M5MOLS_RESTYPE_CAPTURE )) <nl> + else if ( is_code ( code , M5MOLS_RESTYPE_CAPTURE )) <nl> ret = m5mols_start_capture ( info ); <nl> else <nl> ret = - EINVAL ;
static int __devinit e100_probe ( struct pci_dev * pdev , <nl>  <nl> e100_get_defaults ( nic ); <nl>  <nl> + /* D100 MAC doesn ' t allow rx of vlan packets with normal MTU */ <nl> + if ( nic -> mac < mac_82558_D101_A4 ) <nl> + netdev -> features |= NETIF_F_VLAN_CHALLENGED ; <nl> + <nl> /* locks must be initialized before calling hw_reset */ <nl> spin_lock_init (& nic -> cb_lock ); <nl> spin_lock_init (& nic -> cmd_lock );
again : <nl> key . offset = split ; <nl>  <nl> ret = btrfs_search_slot ( trans , root , & key , path , - 1 , 1 ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> if ( ret > 0 && path -> slots [ 0 ] > 0 ) <nl> path -> slots [ 0 ]--; <nl> 
static int snd_hdsp_get_adat_sync_check ( struct snd_kcontrol * kcontrol , struct sn <nl> struct hdsp * hdsp = snd_kcontrol_chip ( kcontrol ); <nl>  <nl> offset = ucontrol -> id . index - 1 ; <nl> - snd_BUG_ON ( offset < 0 ); <nl> + if ( snd_BUG_ON ( offset < 0 )) <nl> + return - EINVAL ; <nl>  <nl> switch ( hdsp -> io_type ) { <nl> case Digiface :
int snd_hda_parse_pin_def_config ( struct hda_codec * codec , <nl> cfg -> input_pins [ AUTO_PIN_AUX ] = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_OUT : <nl> + case AC_JACK_DIG_OTHER_OUT : <nl> cfg -> dig_out_pin = nid ; <nl> break ; <nl> case AC_JACK_SPDIF_IN : <nl> + case AC_JACK_DIG_OTHER_IN : <nl> cfg -> dig_in_pin = nid ; <nl> break ; <nl> }
static const struct mfd_cell s5m8767_devs [] = { <nl> static const struct mfd_cell s2mps11_devs [] = { <nl> { <nl> . name = " s2mps11 - pmic ", <nl> + }, { <nl> + . name = " s2mps14 - rtc ", <nl> }, { <nl> . name = " s2mps11 - clk ", <nl> . of_compatible = " samsung , s2mps11 - clk ",
extern int scsi_execute_async ( struct scsi_device * sdev , <nl> void (* done )( void *, char *, int , int ), <nl> gfp_t gfp ); <nl>  <nl> - static inline void scsi_device_reprobe ( struct scsi_device * sdev ) <nl> + static inline int __must_check scsi_device_reprobe ( struct scsi_device * sdev ) <nl> { <nl> - device_reprobe (& sdev -> sdev_gendev ); <nl> + return device_reprobe (& sdev -> sdev_gendev ); <nl> } <nl>  <nl> static inline unsigned int sdev_channel ( struct scsi_device * sdev )
int regmap_register_patch ( struct regmap * map , const struct reg_default * regs , <nl> int i , ret ; <nl> bool bypass ; <nl>  <nl> + if ( WARN_ONCE ( num_regs <= 0 , " invalid registers number (% d )\ n ", <nl> + num_regs )) <nl> + return 0 ; <nl> + <nl> map -> lock ( map -> lock_arg ); <nl>  <nl> bypass = map -> cache_bypass ;
int __init ip_rt_init ( void ) <nl> 0 , <nl> & rt_hash_log , <nl> & rt_hash_mask , <nl> - 0 ); <nl> + rhash_entries ? 0 : 512 * 1024 ); <nl> memset ( rt_hash_table , 0 , ( rt_hash_mask + 1 ) * sizeof ( struct rt_hash_bucket )); <nl> rt_hash_lock_init (); <nl> 
static int init_fb_chan ( struct mx3fb_data * mx3fb , struct idmac_channel * ichan ) <nl> init_completion (& mx3fbi -> flip_cmpl ); <nl> disable_irq ( ichan -> eof_irq ); <nl> dev_dbg ( mx3fb -> dev , " disabling irq % d \ n ", ichan -> eof_irq ); <nl> - ret = mx3fb_set_par ( fbi ); <nl> - if ( ret < 0 ) <nl> - goto esetpar ; <nl> - <nl> - mx3fb_blank ( FB_BLANK_UNBLANK , fbi ); <nl>  <nl> dev_info ( dev , " registered , using mode % s \ n ", fb_mode ); <nl> 
static dma_cookie_t imxdma_tx_submit ( struct dma_async_tx_descriptor * tx ) <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& imxdma -> lock , flags ); <nl> + list_move_tail ( imxdmac -> ld_free . next , & imxdmac -> ld_queue ); <nl> cookie = dma_cookie_assign ( tx ); <nl> spin_unlock_irqrestore (& imxdma -> lock , flags ); <nl> 
static void parse_dacl ( struct cifs_acl * pdacl , char * end_of_acl , <nl> umode_t group_mask = S_IRWXG ; <nl> umode_t other_mask = S_IRWXU | S_IRWXG | S_IRWXO ; <nl>  <nl> + if ( num_aces > ULONG_MAX / sizeof ( struct cifs_ace *)) <nl> + return ; <nl> ppace = kmalloc ( num_aces * sizeof ( struct cifs_ace *), <nl> GFP_KERNEL ); <nl> if (! ppace ) {
static int xfrm6_fill_dst ( struct xfrm_dst * xdst , struct net_device * dev , <nl> dev_hold ( dev ); <nl>  <nl> xdst -> u . rt6 . rt6i_idev = in6_dev_get ( dev ); <nl> - if (! xdst -> u . rt6 . rt6i_idev ) <nl> + if (! xdst -> u . rt6 . rt6i_idev ) { <nl> + dev_put ( dev ); <nl> return - ENODEV ; <nl> + } <nl>  <nl> rt6_transfer_peer (& xdst -> u . rt6 , rt ); <nl> 
static int run_perf_stat ( int argc __used , const char ** argv ) <nl> "\ t Consider tweaking " <nl> " / proc / sys / kernel / perf_event_paranoid or running as root .", <nl> system_wide ? " system - wide " : ""); <nl> + } else if ( errno == ENOENT ) { <nl> + error ("% s event is not supported . ", event_name ( counter )); <nl> } else { <nl> error (" open_counter returned with % d (% s ). " <nl> "/ bin / dmesg may provide additional information .\ n ",
out : <nl>  <nl> static int jfs_ci_revalidate ( struct dentry * dentry , struct nameidata * nd ) <nl> { <nl> - if ( nd -> flags & LOOKUP_RCU ) <nl> + if ( nd && nd -> flags & LOOKUP_RCU ) <nl> return - ECHILD ; <nl> /* <nl> * This is not negative dentry . Always valid .
void iio_disable_all_buffers ( struct iio_dev * indio_dev ) <nl> indio_dev -> currentmode = INDIO_DIRECT_MODE ; <nl> if ( indio_dev -> setup_ops -> postdisable ) <nl> indio_dev -> setup_ops -> postdisable ( indio_dev ); <nl> + <nl> + if ( indio_dev -> available_scan_masks == NULL ) <nl> + kfree ( indio_dev -> active_scan_mask ); <nl> } <nl>  <nl> int iio_update_buffers ( struct iio_dev * indio_dev ,
intel_pin_and_fence_fb_obj ( struct drm_device * dev , <nl> u32 alignment ; <nl> int ret ; <nl>  <nl> + WARN_ON (! mutex_is_locked (& dev -> struct_mutex )); <nl> + <nl> switch ( obj -> tiling_mode ) { <nl> case I915_TILING_NONE : <nl> if ( IS_BROADWATER ( dev ) || IS_CRESTLINE ( dev )) <nl> err_interruptible : <nl>  <nl> void intel_unpin_fb_obj ( struct drm_i915_gem_object * obj ) <nl> { <nl> + WARN_ON (! mutex_is_locked (& obj -> base . dev -> struct_mutex )); <nl> + <nl> i915_gem_object_unpin_fence ( obj ); <nl> i915_gem_object_unpin_from_display_plane ( obj ); <nl> }
static long rtc_dev_ioctl ( struct file * file , <nl> err = ops -> ioctl ( rtc -> dev . parent , cmd , arg ); <nl> if ( err == - ENOIOCTLCMD ) <nl> err = - ENOTTY ; <nl> - } <nl> + } else <nl> + err = - ENOTTY ; <nl> break ; <nl> } <nl> 
static void handle_swbp ( struct pt_regs * regs ) <nl> if ( unlikely (! test_bit ( UPROBE_COPY_INSN , & uprobe -> flags ))) <nl> goto out ; <nl>  <nl> + /* Tracing handlers use -> utask to communicate with fetch methods */ <nl> + if (! get_utask ()) <nl> + goto out ; <nl> + <nl> handler_chain ( uprobe , regs ); <nl> if ( can_skip_sstep ( uprobe , regs )) <nl> goto out ;
static int emc1403_probe ( struct i2c_client * client , <nl> res = sysfs_create_group (& client -> dev . kobj , & m_thermal_gr ); <nl> if ( res ) { <nl> dev_warn (& client -> dev , " create group failed \ n "); <nl> - hwmon_device_unregister ( data -> hwmon_dev ); <nl> goto thermal_error1 ; <nl> } <nl> data -> hwmon_dev = hwmon_device_register (& client -> dev );
static ssize_t wm8962_beep_set ( struct device * dev , <nl> { <nl> struct wm8962_priv * wm8962 = dev_get_drvdata ( dev ); <nl> long int time ; <nl> + int ret ; <nl>  <nl> - strict_strtol ( buf , 10 , & time ); <nl> + ret = strict_strtol ( buf , 10 , & time ); <nl> + if ( ret != 0 ) <nl> + return ret ; <nl>  <nl> input_event ( wm8962 -> beep , EV_SND , SND_TONE , time ); <nl> 
static int pcc_cpufreq_target ( struct cpufreq_policy * policy , <nl> return 0 ; <nl>  <nl> cmd_incomplete : <nl> + freqs . new = freqs . old ; <nl> + cpufreq_notify_transition ( policy , & freqs , CPUFREQ_POSTCHANGE ); <nl> iowrite16 ( 0 , & pcch_hdr -> status ); <nl> spin_unlock (& pcc_lock ); <nl> return - EINVAL ;
int snd_pcm_status ( struct snd_pcm_substream * substream , <nl> runtime -> status -> audio_tstamp ; <nl> goto _tstamp_end ; <nl> } <nl> + } else { <nl> + /* get tstamp only in fallback mode and only if enabled */ <nl> + if ( runtime -> tstamp_mode == SNDRV_PCM_TSTAMP_ENABLE ) <nl> + snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> } <nl> - snd_pcm_gettime ( runtime , & status -> tstamp ); <nl> _tstamp_end : <nl> status -> appl_ptr = runtime -> control -> appl_ptr ; <nl> status -> hw_ptr = runtime -> status -> hw_ptr ;
void afs_cache_permit ( struct afs_vnode * vnode , struct key * key , long acl_order ) <nl> if (! permits ) <nl> goto out_unlock ; <nl>  <nl> - memcpy ( permits -> permits , xpermits -> permits , <nl> - count * sizeof ( struct afs_permit )); <nl> + if ( xpermits ) <nl> + memcpy ( permits -> permits , xpermits -> permits , <nl> + count * sizeof ( struct afs_permit )); <nl>  <nl> _debug (" key % x access % x ", <nl> key_serial ( key ), vnode -> status . caller_access );
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> req -> pages [ req -> num_pages ] = page ; <nl> req -> num_pages ++; <nl>  <nl> + offset = 0 ; <nl> num -= this_num ; <nl> total_len += this_num ; <nl> index ++;
static int genwqe_pin_mem ( struct genwqe_file * cfile , struct genwqe_mem * m ) <nl> if ( rc != 0 ) { <nl> dev_err (& pci_dev -> dev , <nl> "[% s ] genwqe_user_vmap rc =% d \ n ", __func__ , rc ); <nl> + kfree ( dma_map ); <nl> return rc ; <nl> } <nl> 
xfs_fs_geometry ( <nl> xfs_fsop_geom_t * geo , <nl> int new_version ) <nl> { <nl> + <nl> + memset ( geo , 0 , sizeof (* geo )); <nl> + <nl> geo -> blocksize = mp -> m_sb . sb_blocksize ; <nl> geo -> rtextsize = mp -> m_sb . sb_rextsize ; <nl> geo -> agblocks = mp -> m_sb . sb_agblocks ;
int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> which_atu = 0 ; <nl> } <nl>  <nl> - if (! which_atu ) <nl> + if (! which_atu ) { <nl> + kfree ( res ); <nl> return 0 ; <nl> + } <nl>  <nl> switch ( which_atu ) { <nl> case IOP13XX_INIT_ATU_ATUX : <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> sys -> map_irq = iop13xx_pcie_map_irq ; <nl> break ; <nl> default : <nl> + kfree ( res ); <nl> return 0 ; <nl> } <nl> 
void start_tty ( struct tty_struct * tty ) <nl>  <nl> /* If we have a running line discipline it may need kicking */ <nl> tty_wakeup ( tty ); <nl> - wake_up_interruptible (& tty -> write_wait ); <nl> } <nl>  <nl> EXPORT_SYMBOL ( start_tty );
SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> if ( which > PRIO_USER || which < PRIO_PROCESS ) <nl> return - EINVAL ; <nl>  <nl> + rcu_read_lock (); <nl> read_lock (& tasklist_lock ); <nl> switch ( which ) { <nl> case PRIO_PROCESS : <nl> SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> } <nl> out_unlock : <nl> read_unlock (& tasklist_lock ); <nl> + rcu_read_unlock (); <nl>  <nl> return retval ; <nl> }
i915_gem_object_create_from_data ( struct drm_device * dev , <nl> i915_gem_object_pin_pages ( obj ); <nl> sg = obj -> pages ; <nl> bytes = sg_copy_from_buffer ( sg -> sgl , sg -> nents , ( void *) data , size ); <nl> + obj -> dirty = 1 ; /* Backing store is now out of date */ <nl> i915_gem_object_unpin_pages ( obj ); <nl>  <nl> if ( WARN_ON ( bytes != size )) {
xfs_qm_freelist_destroy ( xfs_frlist_t * ql ) <nl> xfs_qm_dqdestroy ( dqp ); <nl> dqp = nextdqp ; <nl> } <nl> - /* <nl> - * Don ' t bother about unlocking . <nl> - */ <nl> + mutex_unlock (& ql -> qh_lock ); <nl> mutex_destroy (& ql -> qh_lock ); <nl>  <nl> ASSERT ( ql -> qh_nelems == 0 );
static void ttm_tt_clear_mapping ( struct ttm_tt * ttm ) <nl> pgoff_t i ; <nl> struct page ** page = ttm -> pages ; <nl>  <nl> + if ( ttm -> page_flags & TTM_PAGE_FLAG_SG ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < ttm -> num_pages ; ++ i ) { <nl> (* page )-> mapping = NULL ; <nl> (* page ++)-> index = 0 ;
xfs_ioc_trim ( <nl>  <nl> if (! capable ( CAP_SYS_ADMIN )) <nl> return - XFS_ERROR ( EPERM ); <nl> + if (! blk_queue_discard ( q )) <nl> + return - XFS_ERROR ( EOPNOTSUPP ); <nl> if ( copy_from_user (& range , urange , sizeof ( range ))) <nl> return - XFS_ERROR ( EFAULT ); <nl> 
int get_dnode_of_data ( struct dnode_of_data * dn , pgoff_t index , int mode ) <nl>  <nl> /* if inline_data is set , should not report any block indices */ <nl> if ( f2fs_has_inline_data ( dn -> inode ) && index ) { <nl> - err = - EINVAL ; <nl> + err = - ENOENT ; <nl> f2fs_put_page ( npage [ 0 ], 1 ); <nl> goto release_out ; <nl> }
int vmw_kms_update_layout_ioctl ( struct drm_device * dev , void * data , <nl> ret = copy_from_user ( rects , user_rects , rects_size ); <nl> if ( unlikely ( ret != 0 )) { <nl> DRM_ERROR (" Failed to get rects .\ n "); <nl> + ret = - EFAULT ; <nl> goto out_free ; <nl> } <nl> 
static int amd_xgbe_phy_set_mode ( struct phy_device * phydev , <nl> static enum amd_xgbe_phy_an amd_xgbe_an_tx_training ( struct phy_device * phydev , <nl> enum amd_xgbe_phy_rx * state ) <nl> { <nl> + struct amd_xgbe_phy_priv * priv = phydev -> priv ; <nl> int ad_reg , lp_reg , ret ; <nl>  <nl> * state = AMD_XGBE_RX_COMPLETE ;
brcmf_notify_sched_scan_results ( struct brcmf_if * ifp , <nl> } <nl>  <nl> set_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status ); <nl> + cfg -> escan_info . run = brcmf_run_escan ; <nl> err = brcmf_do_escan ( cfg , wiphy , ifp , request ); <nl> if ( err ) { <nl> clear_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status );
static int ocfs2_initialize_super ( struct super_block * sb , <nl> cbits = le32_to_cpu ( di -> id2 . i_super . s_clustersize_bits ); <nl> bbits = le32_to_cpu ( di -> id2 . i_super . s_blocksize_bits ); <nl> sb -> s_maxbytes = ocfs2_max_file_offset ( bbits , cbits ); <nl> + memcpy ( sb -> s_uuid , di -> id2 . i_super . s_uuid , <nl> + sizeof ( di -> id2 . i_super . s_uuid )); <nl>  <nl> osb -> osb_dx_mask = ( 1 << ( cbits - bbits )) - 1 ; <nl> 
static int __init crossbar_of_init ( struct device_node * node ) <nl> int i , size , max , reserved = 0 , entry ; <nl> const __be32 * irqsr ; <nl>  <nl> - cb = kzalloc ( sizeof ( struct cb_device *), GFP_KERNEL ); <nl> + cb = kzalloc ( sizeof (* cb ), GFP_KERNEL ); <nl>  <nl> if (! cb ) <nl> return - ENOMEM ;
static ssize_t bonding_store_slaves ( struct device * d , <nl>  <nl> if ( command [ 0 ] == '-') { <nl> dev = NULL ; <nl> + original_mtu = 0 ; <nl> bond_for_each_slave ( bond , slave , i ) <nl> if ( strnicmp ( slave -> dev -> name , ifname , IFNAMSIZ ) == 0 ) { <nl> dev = slave -> dev ;
static void mv_chan_set_mode ( struct mv_xor_chan * chan , <nl> config &= ~ 0x7 ; <nl> config |= op_mode ; <nl>  <nl> - if ( IS_ENABLED ( __BIG_ENDIAN )) <nl> - config |= XOR_DESCRIPTOR_SWAP ; <nl> - else <nl> - config &= ~ XOR_DESCRIPTOR_SWAP ; <nl> +# if defined ( __BIG_ENDIAN ) <nl> + config |= XOR_DESCRIPTOR_SWAP ; <nl> +# else <nl> + config &= ~ XOR_DESCRIPTOR_SWAP ; <nl> +# endif <nl>  <nl> writel_relaxed ( config , XOR_CONFIG ( chan )); <nl> chan -> current_type = type ;
static void hsmmc_command_incomplete ( struct omap_hsmmc_host * host , <nl> if ( host -> data ) { <nl> omap_hsmmc_reset_controller_fsm ( host , SRD ); <nl> omap_hsmmc_dma_cleanup ( host , err ); <nl> - } <nl> - <nl> + } else if ( host -> mrq && host -> mrq -> cmd ) <nl> + host -> mrq -> cmd -> error = err ; <nl> } <nl>  <nl> static void omap_hsmmc_do_irq ( struct omap_hsmmc_host * host , int status )
static void ni_660x_handle_gpct_interrupt ( struct comedi_device * dev , <nl> struct ni_gpct * counter = s -> private ; <nl>  <nl> ni_tio_handle_interrupt ( counter , s ); <nl> - cfc_handle_events ( dev , s ); <nl> + comedi_handle_events ( dev , s ); <nl> } <nl>  <nl> static irqreturn_t ni_660x_interrupt ( int irq , void * d )
# ifndef _LINUX_BFS_FS_H <nl> # define _LINUX_BFS_FS_H <nl>  <nl> +# include < linux / types . h > <nl> + <nl> # define BFS_BSIZE_BITS 9 <nl> # define BFS_BSIZE ( 1 << BFS_BSIZE_BITS ) <nl>  <nl> # define BFS_VDIR 2L <nl> # define BFS_VREG 1L <nl>  <nl> - <nl> /* BFS inode layout on disk */ <nl> struct bfs_inode { <nl> __le16 i_ino ;
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static void btrfs_qgroup_rescan_worker ( struct btrfs_work * work ) <nl> out : <nl> kfree ( scratch_leaf ); <nl> ulist_free ( qgroups ); <nl> + ulist_free ( tmp ); <nl> btrfs_free_path ( path ); <nl>  <nl> mutex_lock (& fs_info -> qgroup_rescan_lock );
static int ehci_mxc_drv_remove ( struct platform_device * pdev ) <nl> if ( pdata && pdata -> exit ) <nl> pdata -> exit ( pdev ); <nl>  <nl> - if ( pdata -> otg ) <nl> + if ( pdata && pdata -> otg ) <nl> usb_phy_shutdown ( pdata -> otg ); <nl>  <nl> clk_disable_unprepare ( priv -> usbclk );
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> if ( IS_VALLEYVIEW ( dev )) <nl> bios_reserved = 1024 * 1024 ; /* top 1M on VLV / BYT */ <nl>  <nl> + if ( WARN_ON ( bios_reserved > dev_priv -> gtt . stolen_size )) <nl> + return 0 ; <nl> + <nl> /* Basic memrange allocator for stolen space */ <nl> drm_mm_init (& dev_priv -> mm . stolen , 0 , dev_priv -> gtt . stolen_size - <nl> bios_reserved );
int mthca_init_db_tab ( struct mthca_dev * dev ) <nl>  <nl> init_MUTEX (& dev -> db_tab -> mutex ); <nl>  <nl> - dev -> db_tab -> npages = dev -> uar_table . uarc_size / PAGE_SIZE ; <nl> + dev -> db_tab -> npages = dev -> uar_table . uarc_size / 4096 ; <nl> dev -> db_tab -> max_group1 = 0 ; <nl> dev -> db_tab -> min_group2 = dev -> db_tab -> npages - 1 ; <nl> 
cfq_should_preempt ( struct cfq_data * cfqd , struct cfq_queue * new_cfqq , <nl> if ( cfq_class_idle ( cfqq )) <nl> return true ; <nl>  <nl> + /* <nl> + * Don ' t allow a non - RT request to preempt an ongoing RT cfqq timeslice . <nl> + */ <nl> + if ( cfq_class_rt ( cfqq ) && ! cfq_class_rt ( new_cfqq )) <nl> + return false ; <nl> + <nl> /* <nl> * if the new request is sync , but the currently running queue is <nl> * not , let the sync request have priority .
free_interfaces : <nl> intf -> dev . bus_id , ret ); <nl> continue ; <nl> } <nl> - usb_create_sysfs_intf_files ( intf ); <nl> + <nl> + /* The driver ' s probe method can call usb_set_interface (), <nl> + * which would mean the interface ' s sysfs files are already <nl> + * created . Just in case , we ' ll remove them first . <nl> + */ <nl> + usb_remove_sysfs_intf_files ( intf ); <nl> + usb_create_sysfs_intf_files ( intf ); <nl> } <nl>  <nl> usb_autosuspend_device ( dev );
static int __access_remote_vm ( struct task_struct * tsk , struct mm_struct * mm , <nl> */ <nl> # ifdef CONFIG_HAVE_IOREMAP_PROT <nl> vma = find_vma ( mm , addr ); <nl> - if (! vma ) <nl> + if (! vma || vma -> vm_start > addr ) <nl> break ; <nl> if ( vma -> vm_ops && vma -> vm_ops -> access ) <nl> ret = vma -> vm_ops -> access ( vma , addr , buf ,
static int pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> free_press_irq : <nl> - free_irq ( key_press_irq , NULL ); <nl> + free_irq ( key_press_irq , pwrkey ); <nl> unreg_input_dev : <nl> input_unregister_device ( pwr ); <nl> pwr = NULL ;
static int mcp230xx_probe ( struct i2c_client * client , <nl> pdata = devm_kzalloc (& client -> dev , <nl> sizeof ( struct mcp23s08_platform_data ), <nl> GFP_KERNEL ); <nl> + if (! pdata ) <nl> + return - ENOMEM ; <nl> pdata -> base = - 1 ; <nl> } <nl> }
int __init mem_reserve ( unsigned long start , unsigned long end , int must_exist ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( it && start - it -> start < bank_sz ) { <nl> + if ( it && start - it -> start <= bank_sz ) { <nl> if ( start == it -> start ) { <nl> if ( end - it -> start < bank_sz ) { <nl> it -> start = end ;
void laptop_mode_timer_fn ( unsigned long data ) <nl> if (! bdi_has_dirty_io (& q -> backing_dev_info )) <nl> return ; <nl>  <nl> + rcu_read_lock (); <nl> bdi_for_each_wb ( wb , & q -> backing_dev_info , & iter , 0 ) <nl> if ( wb_has_dirty_io ( wb )) <nl> wb_start_writeback ( wb , nr_pages , true , <nl> WB_REASON_LAPTOP_TIMER ); <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> /*
static int hfsplus_fill_super ( struct super_block * sb , void * data , int silent ) <nl> u64 last_fs_block , last_fs_page ; <nl> int err ; <nl>  <nl> - err = - EINVAL ; <nl> + err = - ENOMEM ; <nl> sbi = kzalloc ( sizeof (* sbi ), GFP_KERNEL ); <nl> if (! sbi ) <nl> goto out ;
static acpi_status intel_menlow_register_sensor ( acpi_handle handle , u32 lvl , <nl> return AE_ERROR ; <nl> } <nl>  <nl> + return AE_OK ; <nl> + <nl> aux1_not_found : <nl> if ( status == AE_NOT_FOUND ) <nl> return AE_OK ;
static struct usb_device_id usb_ids [] = { <nl> { USB_DEVICE ( 0x0411 , 0x00da ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x2019 , 0x5303 ), . driver_info = DEVICE_ZD1211B }, <nl> { USB_DEVICE ( 0x129b , 0x1667 ), . driver_info = DEVICE_ZD1211B }, <nl> + { USB_DEVICE ( 0x0cde , 0x001a ), . driver_info = DEVICE_ZD1211B }, <nl> /* " Driverless " devices that need ejecting */ <nl> { USB_DEVICE ( 0x0ace , 0x2011 ), . driver_info = DEVICE_INSTALLER }, <nl> { USB_DEVICE ( 0x0ace , 0x20ff ), . driver_info = DEVICE_INSTALLER },
static void ppp_cp_parse_cr ( struct net_device * dev , u16 pid , u8 id , <nl> for ( opt = data ; len ; len -= opt [ 1 ], opt += opt [ 1 ]) { <nl> if ( len < 2 || len < opt [ 1 ]) { <nl> dev -> stats . rx_errors ++; <nl> + kfree ( out ); <nl> return ; /* bad packet , drop silently */ <nl> } <nl> 
enum pcpu_fc pcpu_chosen_fc __initdata = PCPU_FC_AUTO ; <nl>  <nl> static int __init percpu_alloc_setup ( char * str ) <nl> { <nl> + if (! str ) <nl> + return - EINVAL ; <nl> + <nl> if ( 0 ) <nl> /* nada */; <nl> # ifdef CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK
void led_trigger_unregister ( struct led_trigger * trigger ) <nl>  <nl> void led_trigger_unregister_simple ( struct led_trigger * trigger ) <nl> { <nl> - led_trigger_unregister ( trigger ); <nl> + if ( trigger ) <nl> + led_trigger_unregister ( trigger ); <nl> kfree ( trigger ); <nl> } <nl> 
 <nl> # define BUILD_IRQ ( nr ) \ <nl> asmlinkage void IRQ_NAME ( nr ); \ <nl> - asm ("\ n . p2align \ n " \ <nl> + asm ("\ n . text \ n . p2align \ n " \ <nl> " IRQ " # nr " _interrupt :\ n \ t " \ <nl> " push $~(" # nr ") ; " \ <nl> " jmp common_interrupt ");
static int pm860x_probe ( struct snd_soc_codec * codec ) <nl> } <nl> } <nl>  <nl> - pm860x_set_bias_level ( codec , SND_SOC_BIAS_STANDBY ); <nl> - <nl> return 0 ; <nl>  <nl> out : <nl> static int pm860x_remove ( struct snd_soc_codec * codec ) <nl>  <nl> for ( i = 3 ; i >= 0 ; i --) <nl> free_irq ( pm860x -> irq [ i ], pm860x ); <nl> - pm860x_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> return 0 ; <nl> } <nl> 
asmlinkage long compat_sys_nfsservctl ( int cmd , struct compat_nfsctl_arg __user * <nl>  <nl> default : <nl> err = - EINVAL ; <nl> - goto done ; <nl> + break ; <nl> } <nl>  <nl> + if ( err ) <nl> + goto done ; <nl> + <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> /* The __user pointer casts are valid because of the set_fs () */
static int tpm_tis_init ( struct device * dev , struct tpm_info * tpm_info , <nl> iowrite32 ( intmask , <nl> chip -> vendor . iobase + <nl> TPM_INT_ENABLE ( chip -> vendor . locality )); <nl> + <nl> + devm_free_irq ( dev , i , chip ); <nl> } <nl> } <nl> if ( chip -> vendor . irq ) {
static int __init byt_gpio_init ( void ) <nl> { <nl> return platform_driver_register (& byt_gpio_driver ); <nl> } <nl> - <nl> subsys_initcall ( byt_gpio_init ); <nl> + <nl> + static void __exit byt_gpio_exit ( void ) <nl> +{ <nl> + platform_driver_unregister (& byt_gpio_driver ); <nl> +} <nl> + module_exit ( byt_gpio_exit );
struct map * machine__new_module ( struct machine * machine , u64 start , <nl> if ( kmod_path__parse_name (& m , filename )) <nl> return NULL ; <nl>  <nl> + map = map_groups__find_by_name (& machine -> kmaps , MAP__FUNCTION , <nl> + m . name ); <nl> + if ( map ) <nl> + goto out ; <nl> + <nl> dso = machine__module_dso ( machine , & m , filename ); <nl> if ( dso == NULL ) <nl> goto out ;
nouveau_bo_move_flips ( struct ttm_buffer_object * bo , bool evict , bool intr , <nl> if ( ret ) <nl> goto out ; <nl>  <nl> - ret = nouveau_bo_move_m2mf ( bo , true , intr , no_wait , new_mem ); <nl> + ret = nouveau_bo_move_m2mf ( bo , evict , intr , no_wait , new_mem ); <nl> if ( ret ) <nl> goto out ; <nl> 
static int add_new_gdb ( handle_t * handle , struct inode * inode , <nl> return err ; <nl>  <nl> exit_inode : <nl> + kfree ( n_group_desc ); <nl> /* ext4_handle_release_buffer ( handle , iloc . bh ); */ <nl> brelse ( iloc . bh ); <nl> exit_dindj :
static int ab8500_fg_get_ext_psy_data ( struct device * dev , void * data ) <nl> case POWER_SUPPLY_PROP_TECHNOLOGY : <nl> switch ( ext -> type ) { <nl> case POWER_SUPPLY_TYPE_BATTERY : <nl> - if (! di -> flags . batt_id_received ) { <nl> + if (! di -> flags . batt_id_received && <nl> + di -> bm -> batt_id != BATTERY_UNKNOWN ) { <nl> const struct abx500_battery_type * b ; <nl>  <nl> b = &( di -> bm -> bat_type [ di -> bm -> batt_id ]);
static int at91_rtc_resume ( struct device * dev ) <nl>  <nl> static SIMPLE_DEV_PM_OPS ( at91_rtc_pm_ops , at91_rtc_suspend , at91_rtc_resume ); <nl>  <nl> +# ifdef CONFIG_OF <nl> static const struct of_device_id at91_rtc_dt_ids [] = { <nl> { . compatible = " atmel , at91rm9200 - rtc " }, <nl> { /* sentinel */ } <nl> }; <nl> MODULE_DEVICE_TABLE ( of , at91_rtc_dt_ids ); <nl> +# endif <nl>  <nl> static struct platform_driver at91_rtc_driver = { <nl> . remove = __exit_p ( at91_rtc_remove ),
int sdma_init ( struct hfi1_devdata * dd , u8 port ) <nl>  <nl> sde -> progress_check_head = 0 ; <nl>  <nl> - init_timer (& sde -> err_progress_check_timer ); <nl> - sde -> err_progress_check_timer . function = <nl> - sdma_err_progress_check ; <nl> - sde -> err_progress_check_timer . data = ( unsigned long ) sde ; <nl> + setup_timer (& sde -> err_progress_check_timer , <nl> + sdma_err_progress_check , ( unsigned long ) sde ); <nl>  <nl> sde -> descq = dma_zalloc_coherent ( <nl> & dd -> pcidev -> dev ,
static int ibmvfc_get_err_result ( struct ibmvfc_cmd * vfc_cmd ) <nl> int fc_rsp_len = rsp -> fcp_rsp_len ; <nl>  <nl> if (( rsp -> flags & FCP_RSP_LEN_VALID ) && <nl> - ((! fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> + (( fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> rsp -> data . info . rsp_code )) <nl> return DID_ERROR << 16 ; <nl> 
static int mwl8k_tx_wait_empty ( struct ieee80211_hw * hw ) <nl>  <nl> rc = - ETIMEDOUT ; <nl> } <nl> + priv -> tx_wait = NULL ; <nl> spin_unlock_bh (& priv -> tx_lock ); <nl>  <nl> return rc ;
static void do_writes ( struct mirror_set * ms , struct bio_list * writes ) <nl> /* <nl> * Dispatch io . <nl> */ <nl> - if ( unlikely ( ms -> log_failure )) { <nl> + if ( unlikely ( ms -> log_failure ) && errors_handled ( ms )) { <nl> spin_lock_irq (& ms -> lock ); <nl> bio_list_merge (& ms -> failures , & sync ); <nl> spin_unlock_irq (& ms -> lock );
intel_crt_load_detect ( struct drm_crtc * crtc , struct intel_encoder * intel_encoder <nl> if ( IS_I9XX ( dev )) { <nl> uint32_t pipeconf = I915_READ ( pipeconf_reg ); <nl> I915_WRITE ( pipeconf_reg , pipeconf | PIPECONF_FORCE_BORDER ); <nl> + POSTING_READ ( pipeconf_reg ); <nl> /* Wait for next Vblank to substitue <nl> * border color for Color info */ <nl> intel_wait_for_vblank ( dev , pipe );
__acquires ( musb -> lock ) <nl> musb -> g . a_alt_hnp_support = 1 ; <nl> break ; <nl> # endif <nl> + case USB_DEVICE_DEBUG_MODE : <nl> + handled = 0 ; <nl> + break ; <nl> stall : <nl> default : <nl> handled = - EINVAL ;
static int pcrypt_aead_init_tfm ( struct crypto_tfm * tfm ) <nl> return PTR_ERR ( cipher ); <nl>  <nl> ctx -> child = cipher ; <nl> - tfm -> crt_aead . reqsize = sizeof ( struct pcrypt_request ) <nl> - + sizeof ( struct aead_givcrypt_request ) <nl> - + crypto_aead_reqsize ( cipher ); <nl> + crypto_aead_set_reqsize ( __crypto_aead_cast ( tfm ), <nl> + sizeof ( struct pcrypt_request ) + <nl> + sizeof ( struct aead_givcrypt_request ) + <nl> + crypto_aead_reqsize ( cipher )); <nl>  <nl> return 0 ; <nl> }
static unsigned int cpg_div6_clock_calc_div ( unsigned long rate , <nl> { <nl> unsigned int div ; <nl>  <nl> + if (! rate ) <nl> + rate = 1 ; <nl> + <nl> div = DIV_ROUND_CLOSEST ( parent_rate , rate ); <nl> return clamp_t ( unsigned int , div , 1 , 64 ); <nl> }
static void reset_connection ( struct ceph_connection * con ) <nl> con -> out_msg = NULL ; <nl> } <nl> con -> in_seq = 0 ; <nl> + con -> in_seq_acked = 0 ; <nl> } <nl>  <nl> /*
static int __devinit fs_enet_mdio_probe ( struct of_device * ofdev , <nl>  <nl> ret = of_address_to_resource ( ofdev -> node , 0 , & res ); <nl> if ( ret ) <nl> - return ret ; <nl> + goto out_res ; <nl>  <nl> snprintf ( new_bus -> id , MII_BUS_ID_SIZE , "% x ", res . start ); <nl>  <nl> out_free_irqs : <nl> kfree ( new_bus -> irq ); <nl> out_unmap_regs : <nl> iounmap ( fec -> fecp ); <nl> + out_res : <nl> out_fec : <nl> kfree ( fec ); <nl> out_mii :
static unsigned int xuartps_set_baud_rate ( struct uart_port * port , <nl> unsigned int baud ) <nl> { <nl> unsigned int calc_baud ; <nl> - u32 cd , bdiv ; <nl> + u32 cd = 0 , bdiv = 0 ; <nl> u32 mreg ; <nl> int div8 ; <nl> struct xuartps * xuartps = port -> private_data ;
hub_port_init ( struct usb_hub * hub , struct usb_device * udev , int port1 , <nl>  <nl> did_new_scheme = true ; <nl> retval = hub_enable_device ( udev ); <nl> - if ( retval < 0 ) <nl> + if ( retval < 0 ) { <nl> + dev_err (& udev -> dev , <nl> + " hub failed to enable device , error % d \ n ", <nl> + retval ); <nl> goto fail ; <nl> + } <nl>  <nl> # define GET_DESCRIPTOR_BUFSIZE 64 <nl> buf = kmalloc ( GET_DESCRIPTOR_BUFSIZE , GFP_NOIO );
intel_sdvo_tv_init ( struct intel_sdvo * intel_sdvo , int type ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl>  <nl> intel_sdvo_lvds_init ( struct intel_sdvo * intel_sdvo , int device ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl> 
static void bmc150_accel_unregister_triggers ( struct bmc150_accel_data * data , <nl> { <nl> int i ; <nl>  <nl> - for ( i = from ; i >= 0 ; i ++) { <nl> + for ( i = from ; i >= 0 ; i --) { <nl> if ( data -> triggers [ i ]. indio_trig ) { <nl> iio_trigger_unregister ( data -> triggers [ i ]. indio_trig ); <nl> data -> triggers [ i ]. indio_trig = NULL ;
static int parse_gfp_flags ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> . size = sample -> raw_size , <nl> }; <nl> struct trace_seq seq ; <nl> - char * str , * pos ; <nl> + char * str , * pos = NULL ; <nl>  <nl> if ( nr_gfps ) { <nl> struct gfp_flag key = {
xlog_recover_do_reg_buffer ( <nl> stale_buf = 1 ; <nl> break ; <nl> } <nl> - if ( be16_to_cpu ( dip -> di_core . di_mode )) <nl> + if ( dip -> di_core . di_mode ) <nl> mode_count ++; <nl> - if ( be16_to_cpu ( dip -> di_core . di_gen )) <nl> + if ( dip -> di_core . di_gen ) <nl> gen_count ++; <nl> } <nl> 
int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
static void unlock_two_stripes ( struct stripe_head * sh1 , struct stripe_head * sh2 ) <nl> /* Only freshly new full stripe normal write stripe can be added to a batch list */ <nl> static bool stripe_can_batch ( struct stripe_head * sh ) <nl> { <nl> + struct r5conf * conf = sh -> raid_conf ; <nl> + <nl> + if ( conf -> log ) <nl> + return false ; <nl> return test_bit ( STRIPE_BATCH_READY , & sh -> state ) && <nl> ! test_bit ( STRIPE_BITMAP_PENDING , & sh -> state ) && <nl> is_full_stripe_write ( sh );
void fuse_put_request ( struct fuse_conn * fc , struct fuse_req * req ) <nl> spin_unlock (& fc -> lock ); <nl> } <nl>  <nl> - if ( req -> waiting ) <nl> + if ( req -> waiting ) { <nl> atomic_dec (& fc -> num_waiting ); <nl> + req -> waiting = 0 ; <nl> + } <nl>  <nl> if ( req -> stolen_file ) <nl> put_reserved_req ( fc , req );
static struct dma_chan * of_dma_sirfsoc_xlate ( struct of_phandle_args * dma_spec , <nl> struct sirfsoc_dma * sdma = ofdma -> of_dma_data ; <nl> unsigned int request = dma_spec -> args [ 0 ]; <nl>  <nl> - if ( request > SIRFSOC_DMA_CHANNELS ) <nl> + if ( request >= SIRFSOC_DMA_CHANNELS ) <nl> return NULL ; <nl>  <nl> return dma_get_slave_channel (& sdma -> channels [ request ]. chan );
static int rt5640_probe ( struct snd_soc_codec * codec ) <nl> rt5639_specific_dapm_routes , <nl> ARRAY_SIZE ( rt5639_specific_dapm_routes )); <nl> break ; <nl> + default : <nl> + dev_err ( codec -> dev , <nl> + " The driver is for RT5639 RT5640 or RT5642 only \ n "); <nl> + return - ENODEV ; <nl> } <nl>  <nl> return 0 ;
static int __devinit ntc_thermistor_probe ( struct platform_device * pdev ) <nl>  <nl> data -> dev = & pdev -> dev ; <nl> data -> pdata = pdata ; <nl> - strncpy ( data -> name , pdev -> id_entry -> name , PLATFORM_NAME_SIZE ); <nl> + strlcpy ( data -> name , pdev -> id_entry -> name , sizeof ( data -> name )); <nl>  <nl> switch ( pdev -> id_entry -> driver_data ) { <nl> case TYPE_NCPXXWB473 :
parahotplug_request_create ( struct controlvm_message * msg ) <nl> { <nl> struct parahotplug_request * req ; <nl>  <nl> - req = kmalloc ( sizeof (* req ), GFP_KERNEL | __GFP_NORETRY ); <nl> + req = kmalloc ( sizeof (* req ), GFP_KERNEL | __GFP_NORETRY ); <nl> if (! req ) <nl> return NULL ; <nl> 
u32 ath_calcrxfilter ( struct ath_softc * sc ) <nl> rfilt |= ATH9K_RX_FILTER_COMP_BAR ; <nl>  <nl> if ( sc -> nvifs > 1 || ( sc -> rx . rxfilter & FIF_OTHER_BSS )) { <nl> - /* The following may also be needed for other older chips */ <nl> - if ( sc -> sc_ah -> hw_version . macVersion == AR_SREV_VERSION_9160 ) <nl> + /* This is needed for older chips */ <nl> + if ( sc -> sc_ah -> hw_version . macVersion <= AR_SREV_VERSION_9160 ) <nl> rfilt |= ATH9K_RX_FILTER_PROM ; <nl> rfilt |= ATH9K_RX_FILTER_MCAST_BCAST_ALL ; <nl> }
static int ieee80211_stop ( struct net_device * dev ) <nl> case IEEE80211_IF_TYPE_STA : <nl> case IEEE80211_IF_TYPE_IBSS : <nl> sdata -> u . sta . state = IEEE80211_DISABLED ; <nl> + memset ( sdata -> u . sta . bssid , 0 , ETH_ALEN ); <nl> del_timer_sync (& sdata -> u . sta . timer ); <nl> /* <nl> * When we get here , the interface is marked down .
void bcm43xx_phy_set_baseband_attenuation ( struct bcm43xx_private * bcm , <nl> return ; <nl> } <nl>  <nl> - if ( phy -> analog > 1 ) { <nl> + if ( phy -> analog == 1 ) { <nl> value = bcm43xx_phy_read ( bcm , 0x0060 ) & ~ 0x003C ; <nl> value |= ( baseband_attenuation << 2 ) & 0x003C ; <nl> } else {
static struct dmi_system_id acer_quirks [] = { <nl> }, <nl> . driver_data = & quirk_lenovo_ideapad_s205 , <nl> }, <nl> + { <nl> + . callback = dmi_matched , <nl> + . ident = " Lenovo Ideapad S205 - 1038DPG ", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " LENOVO "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME , " 1038DPG "), <nl> + }, <nl> + . driver_data = & quirk_lenovo_ideapad_s205 , <nl> + }, <nl> {} <nl> }; <nl> 
static irqreturn_t s6000_pcm_irq ( int irq , void * data ) <nl> substream -> runtime && <nl> snd_pcm_running ( substream )) { <nl> dev_dbg ( pcm -> dev , " xrun \ n "); <nl> + snd_pcm_stream_lock ( substream ); <nl> snd_pcm_stop ( substream , SNDRV_PCM_STATE_XRUN ); <nl> + snd_pcm_stream_unlock ( substream ); <nl> ret = IRQ_HANDLED ; <nl> } <nl> 
acpi_ds_build_internal_package_obj ( struct acpi_walk_state * walk_state , <nl> arg = arg -> common . next ; <nl> } <nl>  <nl> - ACPI_ERROR (( AE_INFO , <nl> + ACPI_WARNING (( AE_INFO , <nl> " Package List length (% X ) larger than NumElements count (% X ), truncated \ n ", <nl> i , element_count )); <nl> } else if ( i < element_count ) {
device_release_WPADEV ( pDevice ); <nl> free_netdev ( pDevice -> dev ); <nl> } <nl>  <nl> - kfree ( pDevice ); <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO " device_disconnect3 .. \ n "); <nl> } <nl> 
static int process_pool_aeb ( struct ubi_device * ubi , struct ubi_attach_info * ai , <nl> av = tmp_av ; <nl> else { <nl> ubi_err (" orphaned volume in fastmap pool !"); <nl> + kmem_cache_free ( ai -> aeb_slab_cache , new_aeb ); <nl> return UBI_BAD_FASTMAP ; <nl> } <nl> 
static int snd_pcm_update_hw_ptr0 ( struct snd_pcm_substream * substream , <nl> * the elapsed time to detect xruns . <nl> */ <nl> jdelta = jiffies - runtime -> hw_ptr_jiffies ; <nl> + if ( jdelta < runtime -> hw_ptr_buffer_jiffies / 2 ) <nl> + goto no_delta_check ; <nl> hdelta = jdelta - delta * HZ / runtime -> rate ; <nl> while ( hdelta > runtime -> hw_ptr_buffer_jiffies / 2 + 1 ) { <nl> delta += runtime -> buffer_size ;
static struct xfrm_policy * xfrm_compile_policy ( u16 family , int opt , <nl> if ( nr > XFRM_MAX_DEPTH ) <nl> return NULL ; <nl>  <nl> + if ( p -> dir > XFRM_POLICY_OUT ) <nl> + return NULL ; <nl> + <nl> xp = xfrm_policy_alloc ( GFP_KERNEL ); <nl> if ( xp == NULL ) { <nl> * dir = - ENOBUFS ;
void dynamic_irq_cleanup ( unsigned int irq ) <nl> desc -> chip_data = NULL ; <nl> desc -> handle_irq = handle_bad_irq ; <nl> desc -> chip = & no_irq_chip ; <nl> + desc -> name = NULL ; <nl> spin_unlock_irqrestore (& desc -> lock , flags ); <nl> } <nl> 
static int special_clk_ctl_put ( struct snd_kcontrol * kctl , <nl> struct special_params * params = bebob -> maudio_special_quirk ; <nl> int err , id ; <nl>  <nl> - mutex_lock (& bebob -> mutex ); <nl> - <nl> id = uval -> value . enumerated . item [ 0 ]; <nl> if ( id >= ARRAY_SIZE ( special_clk_labels )) <nl> return 0 ; <nl>  <nl> + mutex_lock (& bebob -> mutex ); <nl> + <nl> err = avc_maudio_set_special_clk ( bebob , id , <nl> params -> dig_in_fmt , <nl> params -> dig_out_fmt ,
static int cpsw_poll ( struct napi_struct * napi , int budget ) <nl> cpdma_ctlr_eoi ( priv -> dma , CPDMA_EOI_RX ); <nl> prim_cpsw = cpsw_get_slave_priv ( priv , 0 ); <nl> if ( prim_cpsw -> irq_enabled == false ) { <nl> - cpsw_enable_irq ( priv ); <nl> prim_cpsw -> irq_enabled = true ; <nl> + cpsw_enable_irq ( priv ); <nl> } <nl> } <nl> 
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> int bios_reserved = 0 ; <nl>  <nl> + if ( dev_priv -> gtt . stolen_size == 0 ) <nl> + return 0 ; <nl> + <nl> dev_priv -> mm . stolen_base = i915_stolen_to_physical ( dev ); <nl> if ( dev_priv -> mm . stolen_base == 0 ) <nl> return 0 ;
int comedi_device_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> " driver '% s ' does not support attach using comedi_config \ n ", <nl> driv -> driver_name ); <nl> module_put ( driv -> module ); <nl> - ret = - ENOSYS ; <nl> + ret = - EIO ; <nl> goto out ; <nl> } <nl> dev -> driver = driv ;
static int sti_drm_platform_probe ( struct platform_device * pdev ) <nl> master = platform_device_register_resndata ( dev , <nl> DRIVER_NAME " __master ", - 1 , <nl> NULL , 0 , NULL , 0 ); <nl> - if (! master ) <nl> - return - EINVAL ; <nl> + if ( IS_ERR ( master )) <nl> + return PTR_ERR ( master ); <nl>  <nl> platform_set_drvdata ( pdev , master ); <nl> return 0 ;
static int cgroup_attach_proc ( struct cgroup * cgrp , struct task_struct * leader ) <nl> if (! group ) <nl> return - ENOMEM ; <nl> /* pre - allocate to guarantee space while iterating in rcu read - side . */ <nl> - retval = flex_array_prealloc ( group , 0 , group_size - 1 , GFP_KERNEL ); <nl> + retval = flex_array_prealloc ( group , 0 , group_size , GFP_KERNEL ); <nl> if ( retval ) <nl> goto out_free_group_list ; <nl> 
static void tty_audit_buf_push ( struct task_struct * tsk , uid_t loginuid , <nl> get_task_comm ( name , tsk ); <nl> audit_log_untrustedstring ( ab , name ); <nl> audit_log_format ( ab , " data ="); <nl> - audit_log_n_untrustedstring ( ab , buf -> data , buf -> valid ); <nl> + audit_log_n_hex ( ab , buf -> data , buf -> valid ); <nl> audit_log_end ( ab ); <nl> } <nl> buf -> valid = 0 ;
static int amd8111e_rx_poll ( struct napi_struct * napi , int budget ) <nl> int rx_pkt_limit = budget ; <nl> unsigned long flags ; <nl>  <nl> + if ( rx_pkt_limit <= 0 ) <nl> + goto rx_not_empty ; <nl> + <nl> do { <nl> /* process receive packets until we use the quota */ <nl> /* If we own the next entry , it ' s a new packet . Send it up . */
void flush_thread ( void ) <nl>  <nl> void start_thread ( struct pt_regs * regs , unsigned long eip , unsigned long esp ) <nl> { <nl> + get_safe_registers ( regs -> regs . gp , regs -> regs . fp ); <nl> PT_REGS_IP ( regs ) = eip ; <nl> PT_REGS_SP ( regs ) = esp ; <nl> current -> ptrace &= ~ PT_DTRACE ;
int bnx2i_send_iscsi_nopout ( struct bnx2i_conn * bnx2i_conn , <nl> bnx2i_cmd = ( struct bnx2i_cmd *) task -> dd_data ; <nl> nopout_hdr = ( struct iscsi_nopout *) task -> hdr ; <nl> nopout_wqe = ( struct bnx2i_nop_out_request *) ep -> qp . sq_prod_qe ; <nl> + <nl> + memset ( nopout_wqe , 0x00 , sizeof ( struct bnx2i_nop_out_request )); <nl> + <nl> nopout_wqe -> op_code = nopout_hdr -> opcode ; <nl> nopout_wqe -> op_attr = ISCSI_FLAG_CMD_FINAL ; <nl> memcpy ( nopout_wqe -> lun , nopout_hdr -> lun , 8 );
static void cleanup_one_si ( struct smi_info * to_clean ) <nl> if (! to_clean ) <nl> return ; <nl>  <nl> + if ( to_clean -> dev ) <nl> + dev_set_drvdata ( to_clean -> dev , NULL ); <nl> + <nl> list_del (& to_clean -> link ); <nl>  <nl> /* Tell the driver that we are shutting down . */
struct Qdisc_ops <nl>  <nl> int (* enqueue )( struct sk_buff *, struct Qdisc *); <nl> struct sk_buff * (* dequeue )( struct Qdisc *); <nl> + struct sk_buff * (* peek )( struct Qdisc *); <nl> int (* requeue )( struct sk_buff *, struct Qdisc *); <nl> unsigned int (* drop )( struct Qdisc *); <nl> 
irqreturn_t ath_isr ( int irq , void * dev ) <nl>  <nl> if (!( ah -> caps . hw_caps & ATH9K_HW_CAP_AUTOSLEEP )) <nl> if ( status & ATH9K_INT_TIM_TIMER ) { <nl> + if ( ATH_DBG_WARN_ON_ONCE ( sc -> ps_idle )) <nl> + goto chip_reset ; <nl> /* Clear RxAbort bit so that we can <nl> * receive frames */ <nl> ath9k_setpower ( sc , ATH9K_PM_AWAKE );
static int load_segment_descriptor ( struct x86_emulate_ctxt * ctxt , <nl> seg_desc . type = 3 ; <nl> seg_desc . p = 1 ; <nl> seg_desc . s = 1 ; <nl> + if ( ctxt -> mode == X86EMUL_MODE_VM86 ) <nl> + seg_desc . dpl = 3 ; <nl> goto load ; <nl> } <nl> 
static int pcf2123_probe ( struct spi_device * spi ) <nl>  <nl> if (!( rxbuf [ 0 ] & 0x20 )) { <nl> dev_err (& spi -> dev , " chip not found \ n "); <nl> + ret = - ENODEV ; <nl> goto kfree_exit ; <nl> } <nl> 
static int musb_gadget_stop ( struct usb_gadget * g , <nl> dev_dbg ( musb -> controller , " unregistering driver % s \ n ", driver -> function ); <nl>  <nl> musb -> is_active = 0 ; <nl> + musb -> gadget_driver = NULL ; <nl> musb_platform_try_idle ( musb , 0 ); <nl> spin_unlock_irqrestore (& musb -> lock , flags ); <nl> 
static void sc_kref_release ( struct kref * kref ) <nl> sc -> sc_node = NULL ; <nl>  <nl> r2net_debug_del_sc ( sc ); <nl> + <nl> + if ( sc -> sc_page ) <nl> + __free_page ( sc -> sc_page ); <nl> kfree ( sc ); <nl> } <nl> 
struct clk * icst_clk_register ( struct device * dev , <nl>  <nl> pclone = kmemdup ( desc -> params , sizeof (* pclone ), GFP_KERNEL ); <nl> if (! pclone ) { <nl> + kfree ( icst ); <nl> pr_err (" could not clone ICST params \ n "); <nl> return ERR_PTR (- ENOMEM ); <nl> }
static long clk_divider_round_rate ( struct clk_hw * hw , unsigned long rate , <nl> bestdiv = readl ( divider -> reg ) >> divider -> shift ; <nl> bestdiv &= div_mask ( divider -> width ); <nl> bestdiv = _get_div ( divider -> table , bestdiv , divider -> flags ); <nl> - return bestdiv ; <nl> + return DIV_ROUND_UP (* prate , bestdiv ); <nl> } <nl>  <nl> return divider_round_rate ( hw , rate , prate , divider -> table ,
int r100_cs_parse ( struct radeon_cs_parser * p ) <nl> int r ; <nl>  <nl> track = kzalloc ( sizeof (* track ), GFP_KERNEL ); <nl> + if (! track ) <nl> + return - ENOMEM ; <nl> r100_cs_track_clear ( p -> rdev , track ); <nl> p -> track = track ; <nl> do {
static int __init ubi_mtd_param_parse ( const char * val , struct kernel_param * kp ) <nl> char * pbuf = & buf [ 0 ]; <nl> char * tokens [ 3 ] = { NULL , NULL , NULL }; <nl>  <nl> + if (! val ) <nl> + return - EINVAL ; <nl> + <nl> if ( mtd_devs == UBI_MAX_DEVICES ) { <nl> printk (" UBI error : too many parameters , max . is % d \ n ", <nl> UBI_MAX_DEVICES );
int ocfs2_cluster_connect ( const char * stack_name , <nl>  <nl> strlcpy ( new_conn -> cc_name , group , GROUP_NAME_MAX + 1 ); <nl> new_conn -> cc_namelen = grouplen ; <nl> - strlcpy ( new_conn -> cc_cluster_name , cluster_name , CLUSTER_NAME_MAX + 1 ); <nl> + if ( cluster_name_len ) <nl> + strlcpy ( new_conn -> cc_cluster_name , cluster_name , <nl> + CLUSTER_NAME_MAX + 1 ); <nl> new_conn -> cc_cluster_name_len = cluster_name_len ; <nl> new_conn -> cc_recovery_handler = recovery_handler ; <nl> new_conn -> cc_recovery_data = recovery_data ;
static int mxcnd_probe ( struct platform_device * pdev ) <nl> if ( err ) <nl> return err ; <nl>  <nl> - clk_prepare_enable ( host -> clk ); <nl> + err = clk_prepare_enable ( host -> clk ); <nl> + if ( err ) <nl> + return err ; <nl> host -> clk_act = 1 ; <nl>  <nl> /*
static int mtk_spi_remove ( struct platform_device * pdev ) <nl> pm_runtime_disable (& pdev -> dev ); <nl>  <nl> mtk_spi_reset ( mdata ); <nl> - clk_disable_unprepare ( mdata -> spi_clk ); <nl> spi_master_put ( master ); <nl>  <nl> return 0 ;
int svc_create_xprt ( struct svc_serv * serv , const char * xprt_name , <nl> list_add (& newxprt -> xpt_list , & serv -> sv_permsocks ); <nl> spin_unlock_bh (& serv -> sv_lock ); <nl> newport = svc_xprt_local_port ( newxprt ); <nl> - clear_bit ( XPT_BUSY , & newxprt -> xpt_flags ); <nl> + svc_xprt_received ( newxprt ); <nl> return newport ; <nl> } <nl> err :
static int ci_udc_pullup ( struct usb_gadget * _gadget , int is_on ) <nl> { <nl> struct ci_hdrc * ci = container_of ( _gadget , struct ci_hdrc , gadget ); <nl>  <nl> + if (! ci -> vbus_active ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> if ( is_on ) <nl> hw_write ( ci , OP_USBCMD , USBCMD_RS , USBCMD_RS ); <nl> else
void reset_vma_resv_huge_pages ( struct vm_area_struct * vma ) <nl> /* Returns true if the VMA has associated reserve pages */ <nl> static int vma_has_reserves ( struct vm_area_struct * vma ) <nl> { <nl> + if ( vma -> vm_flags & VM_NORESERVE ) <nl> + return 0 ; <nl> if ( vma -> vm_flags & VM_MAYSHARE ) <nl> return 1 ; <nl> if ( is_vma_resv_set ( vma , HPAGE_RESV_OWNER ))
static int readable ( struct pcmcia_socket * s , struct resource * res , <nl> destroy_cis_cache ( s ); <nl> } <nl> s -> cis_mem . res = NULL ; <nl> - if (( ret != 0 ) || ( count == 0 )) <nl> + if (( ret != 0 ) || (* count == 0 )) <nl> return 0 ; <nl> return 1 ; <nl> }
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
void __init init_bsp_APIC ( void ) <nl> /** <nl> * setup_local_APIC - setup the local APIC <nl> */ <nl> - void __devinit setup_local_APIC ( void ) <nl> + void __cpuinit setup_local_APIC ( void ) <nl> { <nl> unsigned long oldvalue , value , maxlvt , integrated ; <nl> int i , j ;
static void vlv_display_power_well_deinit ( struct drm_i915_private * dev_priv ) <nl> valleyview_disable_display_irqs ( dev_priv ); <nl> spin_unlock_irq (& dev_priv -> irq_lock ); <nl>  <nl> + /* make sure we ' re done processing display irqs */ <nl> + synchronize_irq ( dev_priv -> dev -> irq ); <nl> + <nl> vlv_power_sequencer_reset ( dev_priv ); <nl> } <nl> 
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
static int gfar_start_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> kfree_skb ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> + <nl> + /* Steal sock reference for processing TX time stamps */ <nl> + swap ( skb_new -> sk , skb -> sk ); <nl> + swap ( skb_new -> destructor , skb -> destructor ); <nl> kfree_skb ( skb ); <nl> skb = skb_new ; <nl> }
static void tgfx_attach ( struct parport * pp ) <nl> n_buttons = tgfx_cfg [ port_idx ]. args + 1 ; <nl> n_devs = tgfx_cfg [ port_idx ]. nargs - 1 ; <nl>  <nl> + memset (& tgfx_parport_cb , 0 , sizeof ( tgfx_parport_cb )); <nl> tgfx_parport_cb . flags = PARPORT_FLAG_EXCL ; <nl>  <nl> pd = parport_register_dev_model ( pp , " turbografx ", & tgfx_parport_cb ,
struct extent_buffer * alloc_extent_buffer ( struct extent_io_tree * tree , <nl> spin_unlock (& tree -> buffer_lock ); <nl> goto free_eb ; <nl> } <nl> - spin_unlock (& tree -> buffer_lock ); <nl> - <nl> /* add one reference for the tree */ <nl> atomic_inc (& eb -> refs ); <nl> + spin_unlock (& tree -> buffer_lock ); <nl> return eb ; <nl>  <nl> free_eb :
static int twl4030_kp_probe ( struct platform_device * pdev ) <nl> err3 : <nl> /* mask all events - we don ' t care about the result */ <nl> ( void ) twl4030_kpwrite_u8 ( kp , 0xff , KEYP_IMR1 ); <nl> - free_irq ( kp -> irq , NULL ); <nl> + free_irq ( kp -> irq , kp ); <nl> err2 : <nl> input_unregister_device ( input ); <nl> input = NULL ;
static bool ath_rx_edma_buf_link ( struct ath_softc * sc , <nl> static void ath_rx_addbuffer_edma ( struct ath_softc * sc , <nl> enum ath9k_rx_qtype qtype , int size ) <nl> { <nl> - struct ath_rx_edma * rx_edma ; <nl> struct ath_common * common = ath9k_hw_common ( sc -> sc_ah ); <nl> u32 nbuf = 0 ; <nl>  <nl> - rx_edma = & sc -> rx . rx_edma [ qtype ]; <nl> if ( list_empty (& sc -> rx . rxbuf )) { <nl> ath_print ( common , ATH_DBG_QUEUE , " No free rx buf available \ n "); <nl> return ;
int __max730x_remove ( struct device * dev ) <nl> ts -> write ( dev , 0x04 , 0x00 ); <nl> gpiochip_remove (& ts -> chip ); <nl> mutex_destroy (& ts -> lock ); <nl> - kfree ( ts ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( __max730x_remove );
void led_blink_set ( struct led_classdev * led_cdev , <nl> unsigned long * delay_on , <nl> unsigned long * delay_off ) <nl> { <nl> + del_timer_sync (& led_cdev -> blink_timer ); <nl> + <nl> if ( led_cdev -> blink_set && <nl> ! led_cdev -> blink_set ( led_cdev , delay_on , delay_off )) { <nl> led_cdev -> blink_delay_on = * delay_on ;
int madvise_free_huge_pmd ( struct mmu_gather * tlb , struct vm_area_struct * vma , <nl> int ret = 0 ; <nl>  <nl> if (! pmd_trans_huge_lock ( pmd , vma , & ptl )) <nl> - goto out ; <nl> + goto out_unlocked ; <nl>  <nl> orig_pmd = * pmd ; <nl> if ( is_huge_zero_pmd ( orig_pmd )) {
int ath_cmn_process_fft ( struct ath_spec_scan_priv * spec_priv , struct ieee80211_h <nl> } <nl>  <nl> /* Process a normal frame */ <nl> - if ( sample_bytes == sample_len ) { <nl> - memcpy ( sample_buf , sample_start , sample_len ); <nl> - ret = fft_handler ( rs , spec_priv , sample_buf , <nl> + if ( sample_bytes == sample_len ) <nl> + ret = fft_handler ( rs , spec_priv , sample_start , <nl> tsf , freq , chan_type ); <nl> - } <nl>  <nl> /* Short report processed , break out of the <nl> * loop .
static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
static inline void check_for_tasks ( int cpu ) <nl>  <nl> write_lock_irq (& tasklist_lock ); <nl> for_each_process ( p ) { <nl> - if ( task_cpu ( p ) == cpu && <nl> + if ( task_cpu ( p ) == cpu && p -> state == TASK_RUNNING && <nl> (! cputime_eq ( p -> utime , cputime_zero ) || <nl> ! cputime_eq ( p -> stime , cputime_zero ))) <nl> printk ( KERN_WARNING " Task % s ( pid = % d ) is on cpu % d \
static void __devinit check_probe_mask ( struct azx * chip , int dev ) <nl> * white / black - list for enable_msi <nl> */ <nl> static struct snd_pci_quirk msi_black_list [] __devinitdata = { <nl> + SND_PCI_QUIRK ( 0x1043 , 0x81f2 , " ASUS ", 0 ), /* Athlon64 X2 + nvidia */ <nl> {} <nl> }; <nl> 
static void xlr_make_tx_desc ( struct nlm_fmn_msg * msg , unsigned long addr , <nl> (( u64 ) fr_stn_id << 54 ) | /* Free back id */ <nl> ( u64 ) 0 << 40 | /* Set len to 0 */ <nl> (( u64 ) physkb & 0xffffffff )); /* 32bit address */ <nl> - msg -> msg2 = msg -> msg3 = 0 ; <nl> + msg -> msg2 = 0 ; <nl> + msg -> msg3 = 0 ; <nl> } <nl>  <nl> static void __maybe_unused xlr_wakeup_queue ( unsigned long dev )
static ctl_table vm_table [] = { <nl> . extra2 = & one_hundred , <nl> }, <nl> # endif <nl> -# ifdef CONFIG_X86_32 <nl> +# if defined ( CONFIG_X86_32 ) || \ <nl> + ( defined ( CONFIG_SUPERH ) && defined ( CONFIG_VSYSCALL )) <nl> { <nl> . ctl_name = VM_VDSO_ENABLED , <nl> . procname = " vdso_enabled ",
static int iio_device_add_event ( struct iio_dev * indio_dev , <nl> & indio_dev -> event_interface -> dev_attr_list ); <nl> kfree ( postfix ); <nl>  <nl> + if (( ret == - EBUSY ) && ( shared_by != IIO_SEPARATE )) <nl> + continue ; <nl> + <nl> if ( ret ) <nl> return ret ; <nl> 
static irqreturn_t edt_ft5x06_ts_isr ( int irq , void * dev_id ) <nl> if ( type == TOUCH_EVENT_RESERVED ) <nl> continue ; <nl>  <nl> + /* ignore TOUCH_DOWN events , might have bogus coordinates */ <nl> + if ( type == TOUCH_EVENT_DOWN ) <nl> + continue ; <nl> + <nl> x = (( buf [ 0 ] << 8 ) | buf [ 1 ]) & 0x0fff ; <nl> y = (( buf [ 2 ] << 8 ) | buf [ 3 ]) & 0x0fff ; <nl> id = ( buf [ 2 ] >> 4 ) & 0x0f ;
static void ni_write_caldac ( struct comedi_device * dev , int addr , int val ) <nl> addr -= caldacs [ type ]. n_chans ; <nl> } <nl>  <nl> + /* bits will be 0 if there is no caldac for the given addr */ <nl> + if ( bits == 0 ) <nl> + return ; <nl> + <nl> for ( bit = 1 << ( bits - 1 ); bit ; bit >>= 1 ) { <nl> ni_writeb ( dev , (( bit & bitstring ) ? 0x02 : 0 ), Serial_Command ); <nl> udelay ( 1 );
void tune_serdes ( struct hfi1_pportdata * ppd ) <nl> ppd -> driver_link_ready = 0 ; <nl> ppd -> offline_disabled_reason = HFI1_ODR_MASK ( OPA_LINKDOWN_REASON_NONE ); <nl>  <nl> - if ( loopback == LOOPBACK_SERDES || loopback == LOOPBACK_LCB || <nl> + /* Skip the tuning for testing ( loopback != none ) and simulations */ <nl> + if ( loopback != LOOPBACK_NONE || <nl> ppd -> dd -> icode == ICODE_FUNCTIONAL_SIMULATOR || <nl> ! dd -> pcfg_cache . cache_valid ) { <nl> ppd -> driver_link_ready = 1 ;
static const struct of_device_id spinand_dt [] = { <nl> { . compatible = " spinand , mt29f ", }, <nl> {} <nl> }; <nl> + MODULE_DEVICE_TABLE ( of , spinand_dt ); <nl>  <nl> /* <nl> * Device name structure description
static int initializing = 1 ; <nl>  <nl> static int pmac_late_init ( void ) <nl> { <nl> + if (! machine_is ( powermac )) <nl> + return - ENODEV ; <nl> + <nl> initializing = 0 ; <nl> /* this is udbg ( which is __init ) and we can later use it during <nl> * cpu hotplug ( in smp_core99_kick_cpu ) */
MODULE_LICENSE (" GPL v2 "); <nl>  <nl> static int debug ; <nl> module_param ( debug , int , 0644 ); <nl> - MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on \ n "); <nl> + MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on "); <nl>  <nl> /* # define MPX_DEBUG */ <nl> 
wl12xx_scan_sched_scan_ssid_list ( struct wl1271 * wl , <nl> * so they ' re used in probe requests . <nl> */ <nl> for ( i = 0 ; i < req -> n_ssids ; i ++) { <nl> + if (! req -> ssids [ i ]. ssid_len ) <nl> + continue ; <nl> + <nl> for ( j = 0 ; j < cmd -> n_ssids ; j ++) <nl> if (! memcmp ( req -> ssids [ i ]. ssid , <nl> cmd -> ssids [ j ]. ssid ,
asmlinkage long sys_uselib ( const char __user * library ) <nl> if ( error ) <nl> goto out ; <nl>  <nl> + error = - EACCES ; <nl> + if ( nd . mnt -> mnt_flags & MNT_NOEXEC ) <nl> + goto exit ; <nl> error = - EINVAL ; <nl> if (! S_ISREG ( nd . dentry -> d_inode -> i_mode )) <nl> goto exit ;
static void wlcore_op_stop_locked ( struct wl1271 * wl ) <nl>  <nl> /* <nl> * FW channels must be re - calibrated after recovery , <nl> - * clear the last Reg - Domain channel configuration . <nl> + * save current Reg - Domain channel configuration and clear it . <nl> */ <nl> + memcpy ( wl -> reg_ch_conf_pending , wl -> reg_ch_conf_last , <nl> + sizeof ( wl -> reg_ch_conf_pending )); <nl> memset ( wl -> reg_ch_conf_last , 0 , sizeof ( wl -> reg_ch_conf_last )); <nl> } <nl> 
nouveau_pm_profile_set ( struct drm_device * dev , const char * profile ) <nl> return - EPERM ; <nl>  <nl> strncpy ( string , profile , sizeof ( string )); <nl> + string [ sizeof ( string ) - 1 ] = 0 ; <nl> if (( ptr = strchr ( string , '\ n '))) <nl> * ptr = '\ 0 '; <nl> 
struct hda_bus { <nl>  <nl> /* codec linked list */ <nl> struct list_head codec_list ; <nl> - struct hda_codec * caddr_tbl [ HDA_MAX_CODEC_ADDRESS ]; /* caddr -> codec */ <nl> + struct hda_codec * caddr_tbl [ HDA_MAX_CODEC_ADDRESS + 1 ]; /* caddr -> codec */ <nl>  <nl> struct semaphore cmd_mutex ; <nl> 
static inline void ata_tf_init ( struct ata_device * dev , struct ata_taskfile * tf ) <nl>  <nl> static inline void ata_qc_reinit ( struct ata_queued_cmd * qc ) <nl> { <nl> + qc -> dma_dir = DMA_NONE ; <nl> qc -> __sg = NULL ; <nl> qc -> flags = 0 ; <nl> qc -> cursect = qc -> cursg = qc -> cursg_ofs = 0 ;
bool i40e_dcb_need_reconfig ( struct i40e_pf * pf , <nl> /* Check if APP Table has changed */ <nl> if ( memcmp (& new_cfg -> app , <nl> & old_cfg -> app , <nl> - sizeof ( new_cfg -> app ))) <nl> + sizeof ( new_cfg -> app ))) { <nl> need_reconfig = true ; <nl> dev_info (& pf -> pdev -> dev , " APP Table change detected .\ n "); <nl> + } <nl>  <nl> return need_reconfig ; <nl> }
static void intel_pin_eld_notify ( void * audio_ptr , int port ) <nl> */ <nl> if ( snd_power_get_state ( codec -> card ) != SNDRV_CTL_POWER_D0 ) <nl> return ; <nl> + /* ditto during suspend / resume process itself */ <nl> + if ( atomic_read (&( codec )-> core . in_pm )) <nl> + return ; <nl>  <nl> check_presence_and_report ( codec , pin_nid ); <nl> }
# define OP_31_XOP_SLBIA 498 <nl> # define OP_31_XOP_MFSR 595 <nl> # define OP_31_XOP_MFSRIN 659 <nl> +# define OP_31_XOP_DCBA 758 <nl> # define OP_31_XOP_SLBMFEV 851 <nl> # define OP_31_XOP_EIOIO 854 <nl> # define OP_31_XOP_SLBMFEE 915 <nl> int kvmppc_core_emulate_op ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> kvmppc_set_gpr ( vcpu , get_rt ( inst ), t ); <nl> } <nl> break ; <nl> + case OP_31_XOP_DCBA : <nl> + /* Gets treated as NOP */ <nl> + break ; <nl> case OP_31_XOP_DCBZ : <nl> { <nl> ulong rb = kvmppc_get_gpr ( vcpu , get_rb ( inst ));
void __init clocksource_of_init ( void ) <nl> clocksource_of_init_fn init_func ; <nl>  <nl> for_each_matching_node_and_match ( np , __clksrc_of_table , & match ) { <nl> + if (! of_device_is_available ( np )) <nl> + continue ; <nl> + <nl> init_func = match -> data ; <nl> init_func ( np ); <nl> }
int pci_get_new_domain_nr ( void ) <nl> void pci_bus_assign_domain_nr ( struct pci_bus * bus , struct device * parent ) <nl> { <nl> static int use_dt_domains = - 1 ; <nl> - int domain = of_get_pci_domain_nr ( parent -> of_node ); <nl> + int domain = - 1 ; <nl>  <nl> + if ( parent ) <nl> + domain = of_get_pci_domain_nr ( parent -> of_node ); <nl> /* <nl> * Check DT domain and use_dt_domains values . <nl> *
do_open_permission ( struct svc_rqst * rqstp , struct svc_fh * current_fh , struct nfs <nl>  <nl> if ( open -> op_share_access & NFS4_SHARE_ACCESS_READ ) <nl> accmode |= MAY_READ ; <nl> - if ( open -> op_share_deny & NFS4_SHARE_ACCESS_WRITE ) <nl> + if ( open -> op_share_access & NFS4_SHARE_ACCESS_WRITE ) <nl> accmode |= ( MAY_WRITE | MAY_TRUNC ); <nl> + if ( open -> op_share_deny & NFS4_SHARE_DENY_WRITE ) <nl> + accmode |= MAY_WRITE ; <nl>  <nl> status = fh_verify ( rqstp , current_fh , S_IFREG , accmode ); <nl> 
static void __save_error_info ( struct super_block * sb , const char * func , <nl> struct ext4_super_block * es = EXT4_SB ( sb )-> s_es ; <nl>  <nl> EXT4_SB ( sb )-> s_mount_state |= EXT4_ERROR_FS ; <nl> + if ( bdev_read_only ( sb -> s_bdev )) <nl> + return ; <nl> es -> s_state |= cpu_to_le16 ( EXT4_ERROR_FS ); <nl> es -> s_last_error_time = cpu_to_le32 ( get_seconds ()); <nl> strncpy ( es -> s_last_error_func , func , sizeof ( es -> s_last_error_func ));
static int sprom_extract ( struct ssb_bus * bus , struct ssb_sprom * out , <nl> ssb_printk ( KERN_WARNING PFX " Unsupported SPROM " <nl> " revision % d detected . Will extract " <nl> " v1 \ n ", out -> revision ); <nl> + out -> revision = 1 ; <nl> sprom_extract_r123 ( out , in ); <nl> } <nl> }
static void cxl_pci_disable_device ( struct pci_dev * dev ) <nl> dev_err (& dev -> dev , " Default context started \ n "); <nl> return ; <nl> } <nl> + dev -> dev . archdata . cxl_ctx = NULL ; <nl> cxl_release_context ( ctx ); <nl> } <nl> }
static int add_tracepoint_multi ( struct list_head ** list , int * idx , <nl> ret = add_tracepoint ( list , idx , sys_name , evt_ent -> d_name ); <nl> } <nl>  <nl> + closedir ( evt_dir ); <nl> return ret ; <nl> } <nl> 
int __pci_read_base ( struct pci_dev * dev , enum pci_bar_type type , <nl> /* Address above 32 - bit boundary ; disable the BAR */ <nl> pci_write_config_dword ( dev , pos , 0 ); <nl> pci_write_config_dword ( dev , pos + 4 , 0 ); <nl> + res -> flags |= IORESOURCE_UNSET ; <nl> region . start = 0 ; <nl> region . end = sz64 ; <nl> bar_disabled = true ;
void gfs2_inplace_release ( struct gfs2_inode * ip ) <nl> { <nl> struct gfs2_blkreserv * rs = ip -> i_res ; <nl>  <nl> - gfs2_blkrsv_put ( ip ); <nl> if ( rs -> rs_rgd_gh . gh_gl ) <nl> gfs2_glock_dq_uninit (& rs -> rs_rgd_gh ); <nl> + gfs2_blkrsv_put ( ip ); <nl> } <nl>  <nl> /**
static int atomic_open ( struct nameidata * nd , struct dentry * dentry , <nl> dput ( dentry ); <nl> dentry = file -> f_path . dentry ; <nl> } <nl> + if ( create_error && dentry -> d_inode == NULL ) { <nl> + error = create_error ; <nl> + goto out ; <nl> + } <nl> goto looked_up ; <nl> } <nl> 
fail : <nl> kvm_unregister_irq_mask_notifier ( kvm , 0 , & pit -> mask_notifier ); <nl> kvm_unregister_irq_ack_notifier ( kvm , & pit_state -> irq_ack_notifier ); <nl> kvm_free_irq_source_id ( kvm , pit -> irq_source_id ); <nl> - <nl> + destroy_workqueue ( pit -> wq ); <nl> kfree ( pit ); <nl> return NULL ; <nl> }
int radeon_atombios_init ( struct radeon_device * rdev ) <nl>  <nl> void radeon_atombios_fini ( struct radeon_device * rdev ) <nl> { <nl> - kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> - kfree ( rdev -> mode_info . atom_context ); <nl> + if ( rdev -> mode_info . atom_context ) { <nl> + kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> + kfree ( rdev -> mode_info . atom_context ); <nl> + } <nl> kfree ( rdev -> mode_info . atom_card_info ); <nl> } <nl> 
int hmc5843_common_probe ( struct device * dev , struct regmap * regmap , <nl> mutex_init (& data -> lock ); <nl>  <nl> indio_dev -> dev . parent = dev ; <nl> + indio_dev -> name = dev -> driver -> name ; <nl> indio_dev -> info = & hmc5843_info ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl> indio_dev -> channels = data -> variant -> channels ;
static enum dma_status d40_tx_status ( struct dma_chan * chan , <nl> } <nl>  <nl> ret = dma_cookie_status ( chan , cookie , txstate ); <nl> - if ( ret != DMA_SUCCESS ) <nl> + if ( ret != DMA_COMPLETE ) <nl> dma_set_residue ( txstate , stedma40_residue ( chan )); <nl>  <nl> if ( d40_is_paused ( d40c ))
static int ath10k_pci_probe ( struct pci_dev * pdev , <nl> if (! ath10k_pci_chip_is_supported ( pdev -> device , chip_id )) { <nl> ath10k_err ( ar , " device % 04x with chip_id % 08x isn ' t supported \ n ", <nl> pdev -> device , chip_id ); <nl> - goto err_sleep ; <nl> + goto err_free_irq ; <nl> } <nl>  <nl> ret = ath10k_core_register ( ar , chip_id );
struct btrfs_root * btrfs_read_fs_root_no_radix ( struct btrfs_root * tree_root , <nl> } <nl> btrfs_free_path ( path ); <nl> if ( ret ) { <nl> + kfree ( root ); <nl> if ( ret > 0 ) <nl> ret = - ENOENT ; <nl> return ERR_PTR ( ret );
qla2x00_mailbox_command ( scsi_qla_host_t * vha , mbx_cmd_t * mcp ) <nl> " mb [ 0 ] = 0x % x .\ n ", mb0 ); <nl> ql_dump_regs ( ql_dbg_mbx + ql_dbg_buffer , vha , 0x1019 ); <nl>  <nl> + /* <nl> + * Attempt to capture a firmware dump for further analysis <nl> + * of the current firmware state <nl> + */ <nl> + ha -> isp_ops -> fw_dump ( vha , 0 ); <nl> + <nl> rval = QLA_FUNCTION_TIMEOUT ; <nl> } <nl> 
int i915_vma_unbind ( struct i915_vma * vma ) <nl>  <nl> i915_gem_gtt_finish_object ( obj ); <nl>  <nl> - list_del (& vma -> mm_list ); <nl> + list_del_init (& vma -> mm_list ); <nl> /* Avoid an unnecessary call to unbind on rebind . */ <nl> if ( i915_is_ggtt ( vma -> vm )) <nl> obj -> map_and_fenceable = true ;
static int xfrm_get_policy ( struct sk_buff * skb , struct nlmsghdr * nlh , void ** xfr <nl> MSG_DONTWAIT ); <nl> } <nl> } else { <nl> + c . data . byid = p -> index ; <nl> c . event = XFRM_SAP_DELETED ; <nl> c . seq = nlh -> nlmsg_seq ; <nl> c . pid = nlh -> nlmsg_pid ;
int shpchprm_set_hpp ( <nl> pci_bus -> number = func -> bus ; <nl> devfn = PCI_DEVFN ( func -> device , func -> function ); <nl>  <nl> - ab = find_acpi_bridge_by_bus ( acpi_bridges_head , ctrl -> seg , ctrl -> bus ); <nl> + ab = find_acpi_bridge_by_bus ( acpi_bridges_head , ctrl -> seg , ctrl -> slot_bus ); <nl>  <nl> if ( ab ) { <nl> if ( ab -> _hpp ) {
struct vpif_disp_buffer { <nl> }; <nl>  <nl> struct common_obj { <nl> - /* Buffer specific parameters */ <nl> - u8 * fbuffers [ VIDEO_MAX_FRAME ]; /* List of buffer pointers for <nl> - * storing frames */ <nl> u32 numbuffers ; /* number of buffers */ <nl> struct vpif_disp_buffer * cur_frm ; /* Pointer pointing to current <nl> * vb2_buffer */
lnet_parse_reply ( lnet_ni_t * ni , lnet_msg_t * msg ) <nl> LASSERT ( md -> md_offset == 0 ); <nl>  <nl> rlength = hdr -> payload_length ; <nl> - mlength = min_t ( int , rlength , md -> md_length ); <nl> + mlength = min_t ( uint , rlength , md -> md_length ); <nl>  <nl> if ( mlength < rlength && <nl> ( md -> md_options & LNET_MD_TRUNCATE ) == 0 ) {
static noinline_for_stack int ethtool_get_rxnfc ( struct net_device * dev , <nl>  <nl> if ( info . cmd == ETHTOOL_GRXCLSRLALL ) { <nl> if ( info . rule_cnt > 0 ) { <nl> - rule_buf = kmalloc ( info . rule_cnt * sizeof ( u32 ), <nl> - GFP_USER ); <nl> + if ( info . rule_cnt <= KMALLOC_MAX_SIZE / sizeof ( u32 )) <nl> + rule_buf = kmalloc ( info . rule_cnt * sizeof ( u32 ), <nl> + GFP_USER ); <nl> if (! rule_buf ) <nl> return - ENOMEM ; <nl> }
int cfhsi_probe ( struct platform_device * pdev ) <nl> dev_err (& ndev -> dev , "% s : Registration error : % d .\ n ", <nl> __func__ , res ); <nl> free_netdev ( ndev ); <nl> + return - ENODEV ; <nl> } <nl> /* Add CAIF HSI device to list . */ <nl> spin_lock (& cfhsi_list_lock );
# include < linux / module . h > /* Modules */ <nl> # include < linux / init . h > /* Initdata */ <nl> # include < linux / ioport . h > /* request_region */ <nl> +# include < linux / delay . h > /* msleep */ <nl> # include < linux / videodev2 . h > /* kernel radio structs */ <nl> # include < linux / version . h > /* for KERNEL_VERSION MACRO */ <nl> # include < linux / io . h > /* outb , outb_p */
static void qeth_l3_set_ip_addr_list ( struct qeth_card * card ) <nl> spin_unlock_irqrestore (& card -> ip_lock , flags ); <nl> rc = qeth_l3_register_addr_entry ( card , todo ); <nl> spin_lock_irqsave (& card -> ip_lock , flags ); <nl> - if (! rc ) <nl> + if (! rc || ( rc == IPA_RC_LAN_OFFLINE )) <nl> list_add_tail (& todo -> entry , & card -> ip_list ); <nl> else <nl> kfree ( todo );
static int video_open ( struct file * file ) <nl>  <nl> if ( NULL == dev ) { <nl> mutex_unlock (& cx25821_devlist_mutex ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> 
static inline u32 ext4_chksum ( struct ext4_sb_info * sbi , u32 crc , <nl> { <nl> struct { <nl> struct shash_desc shash ; <nl> - char ctx [ crypto_shash_descsize ( sbi -> s_chksum_driver )]; <nl> + char ctx [ 4 ]; <nl> } desc ; <nl> int err ; <nl>  <nl> + BUG_ON ( crypto_shash_descsize ( sbi -> s_chksum_driver )!= sizeof ( desc . ctx )); <nl> + <nl> desc . shash . tfm = sbi -> s_chksum_driver ; <nl> desc . shash . flags = 0 ; <nl> *( u32 *) desc . ctx = crc ;
static hda_nid_t path_power_update ( struct hda_codec * codec , <nl>  <nl> for ( i = 0 ; i < path -> depth ; i ++) { <nl> nid = path -> path [ i ]; <nl> + if (!( get_wcaps ( codec , nid ) & AC_WCAP_POWER )) <nl> + continue ; <nl> if ( nid == codec -> core . afg ) <nl> continue ; <nl> if (! allow_powerdown || is_active_nid_for_any ( codec , nid ))
static void default_idle ( void ) <nl> if ( test_thread_flag ( TIF_MCCK_PENDING )) { <nl> local_mcck_enable (); <nl> local_irq_enable (); <nl> - s390_handle_mcck (); <nl> return ; <nl> } <nl> trace_hardirqs_on (); <nl> void cpu_idle ( void ) <nl> for (;;) { <nl> tick_nohz_idle_enter (); <nl> rcu_idle_enter (); <nl> - while (! need_resched ()) <nl> + while (! need_resched () && ! test_thread_flag ( TIF_MCCK_PENDING )) <nl> default_idle (); <nl> rcu_idle_exit (); <nl> tick_nohz_idle_exit (); <nl> + if ( test_thread_flag ( TIF_MCCK_PENDING )) <nl> + s390_handle_mcck (); <nl> preempt_enable_no_resched (); <nl> schedule (); <nl> preempt_disable ();
int btrfs_orphan_add ( struct btrfs_trans_handle * trans , struct inode * inode ) <nl> /* insert an orphan item to track this unlinked / truncated file */ <nl> if ( insert >= 1 ) { <nl> ret = btrfs_insert_orphan_item ( trans , root , btrfs_ino ( inode )); <nl> - BUG_ON ( ret ); <nl> + BUG_ON ( ret && ret != - EEXIST ); <nl> } <nl>  <nl> /* insert an orphan item to track subvolume contains orphan files */
static void wait_for_writer ( struct btrfs_trans_handle * trans , <nl> mutex_unlock (& root -> log_mutex ); <nl> if ( atomic_read (& root -> log_writers )) <nl> schedule (); <nl> - mutex_lock (& root -> log_mutex ); <nl> finish_wait (& root -> log_writer_wait , & wait ); <nl> + mutex_lock (& root -> log_mutex ); <nl> } <nl> } <nl> 
void atari_kbd_leds ( unsigned int leds ) <nl>  <nl> static int atari_keyb_done = 0 ; <nl>  <nl> - int __init atari_keyb_init ( void ) <nl> + int atari_keyb_init ( void ) <nl> { <nl> if ( atari_keyb_done ) <nl> return 0 ; <nl> int __init atari_keyb_init ( void ) <nl> atari_keyb_done = 1 ; <nl> return 0 ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( atari_keyb_init ); <nl>  <nl> int atari_kbd_translate ( unsigned char keycode , unsigned char * keycodep , char raw_mode ) <nl> {
static void storvsc_handle_error ( struct vmscsi_request * vm_srb , <nl> do_work = true ; <nl> process_err_fn = storvsc_remove_lun ; <nl> break ; <nl> - case ( SRB_STATUS_ABORTED | SRB_STATUS_AUTOSENSE_VALID ): <nl> - if (( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> + case SRB_STATUS_ABORTED : <nl> + if ( vm_srb -> srb_status & SRB_STATUS_AUTOSENSE_VALID && <nl> + ( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> do_work = true ; <nl> process_err_fn = storvsc_device_scan ; <nl> /*
static struct iommu_table * vio_build_iommu_table ( struct vio_dev * dev ) <nl> return NULL ; <nl>  <nl> tbl = kmalloc ( sizeof (* tbl ), GFP_KERNEL ); <nl> + if ( tbl == NULL ) <nl> + return NULL ; <nl>  <nl> of_parse_dma_window ( dev -> dev . archdata . of_node , dma_window , <nl> & tbl -> it_index , & offset , & size );
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) <nl> vnt_mac_set_bssid_addr ( priv , ( u8 *) conf -> bssid ); <nl>  <nl> 
static int nl80211_trigger_scan ( struct sk_buff * skb , struct genl_info * info ) <nl> tmp ) { <nl> enum ieee80211_band band = nla_type ( attr ); <nl>  <nl> - if ( band < 0 || band > IEEE80211_NUM_BANDS ) { <nl> + if ( band < 0 || band >= IEEE80211_NUM_BANDS ) { <nl> err = - EINVAL ; <nl> goto out_free ; <nl> }
int ext3_group_add ( struct super_block * sb , struct ext3_new_group_data * input ) <nl> if ( input -> group != EXT3_SB ( sb )-> s_groups_count ) { <nl> ext3_warning ( sb , __FUNCTION__ , <nl> " multiple resizers run on filesystem !\ n "); <nl> + err = - EBUSY ; <nl> goto exit_journal ; <nl> } <nl> 
/* to align the pointer to the ( next ) page boundary */ <nl> # define PAGE_ALIGN ( addr ) ((( addr )+ PAGE_SIZE - 1 )& PAGE_MASK ) <nl>  <nl> -# define __PHYSICAL_MASK (((( phys_addr_t ) 1 ) << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> +# define __PHYSICAL_MASK (( phys_addr_t )( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> # define __VIRTUAL_MASK (( 1UL << __VIRTUAL_MASK_SHIFT ) - 1 ) <nl>  <nl> # ifndef __ASSEMBLY__
static int pit_ioport_read ( struct kvm_io_device * this , <nl> return - EOPNOTSUPP ; <nl>  <nl> addr &= KVM_PIT_CHANNEL_MASK ; <nl> + if ( addr == 3 ) <nl> + return 0 ; <nl> + <nl> s = & pit_state -> channels [ addr ]; <nl>  <nl> mutex_lock (& pit_state -> lock );
struct sk_buff * ieee80211_beacon_get ( struct ieee80211_hw * hw , <nl> " no rate found \ n ", <nl> wiphy_name ( local -> hw . wiphy )); <nl> } <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> skb = NULL ; <nl> goto out ; <nl> }
int devres_release_all ( struct device * dev ) <nl> { <nl> unsigned long flags ; <nl>  <nl> + /* Looks like an uninitialized device structure */ <nl> + if ( WARN_ON ( dev -> devres_head . next == NULL )) <nl> + return - ENODEV ; <nl> spin_lock_irqsave (& dev -> devres_lock , flags ); <nl> return release_nodes ( dev , dev -> devres_head . next , & dev -> devres_head , <nl> flags );
static int __devinit tc35815_read_plat_dev_addr ( struct net_device * dev ) <nl> return - ENODEV ; <nl> } <nl> # else <nl> - static int __devinit tc35815_read_plat_dev_addr ( struct device * dev ) <nl> + static int __devinit tc35815_read_plat_dev_addr ( struct net_device * dev ) <nl> { <nl> return - ENODEV ; <nl> }
cfq_update_io_seektime ( struct cfq_data * cfqd , struct cfq_io_context * cic , <nl> sector_t sdist ; <nl> u64 total ; <nl>  <nl> - if ( cic -> last_request_pos < rq -> sector ) <nl> + if (! cic -> last_request_pos ) <nl> + sdist = 0 ; <nl> + else if ( cic -> last_request_pos < rq -> sector ) <nl> sdist = rq -> sector - cic -> last_request_pos ; <nl> else <nl> sdist = cic -> last_request_pos - rq -> sector ;
flush_thread ( void ) <nl> ia32_drop_ia64_partial_page_list ( current ); <nl> current -> thread . task_size = IA32_PAGE_OFFSET ; <nl> set_fs ( USER_DS ); <nl> + memset ( current -> thread . tls_array , 0 , sizeof ( current -> thread . tls_array )); <nl> } <nl> # endif <nl> }
DT_MACHINE_START ( R8A7794_DT , " Generic R8A7794 ( Flattened Device Tree )") <nl> . init_early = shmobile_init_delay , <nl> . init_late = shmobile_init_late , <nl> . init_time = rcar_gen2_timer_init , <nl> + . reserve = rcar_gen2_reserve , <nl> . dt_compat = r8a7794_boards_compat_dt , <nl> MACHINE_END
int n_tty_ioctl ( struct tty_struct * tty , struct file * file , <nl> ld = tty_ldisc_ref ( tty ); <nl> switch ( arg ) { <nl> case TCIFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> break ; <nl> case TCIOFLUSH : <nl> - if ( ld -> flush_buffer ) <nl> + if ( ld && ld -> flush_buffer ) <nl> ld -> flush_buffer ( tty ); <nl> /* fall through */ <nl> case TCOFLUSH :
static int biovec_create_pools ( struct bio_set * bs , int pool_entries , int scale ) <nl> struct biovec_slab * bp = bvec_slabs + i ; <nl> mempool_t ** bvp = bs -> bvec_pools + i ; <nl>  <nl> - if ( i >= scale ) <nl> + if ( pool_entries > 1 && i >= scale ) <nl> pool_entries >>= 1 ; <nl>  <nl> * bvp = mempool_create_slab_pool ( pool_entries , bp -> slab );
acpi_ex_load_op ( union acpi_operand_object * obj_desc , <nl>  <nl> /* table_ptr was deallocated above */ <nl>  <nl> + acpi_ut_remove_reference ( ddb_handle ); <nl> return_ACPI_STATUS ( status ); <nl> } <nl> 
static int sixpack_ioctl ( struct tty_struct * tty , struct file * file , <nl> unsigned int cmd , unsigned long arg ) <nl> { <nl> struct sixpack * sp = sp_get ( tty ); <nl> - struct net_device * dev = sp -> dev ; <nl> + struct net_device * dev ; <nl> unsigned int tmp , err ; <nl>  <nl> if (! sp ) <nl> return - ENXIO ; <nl> + dev = sp -> dev ; <nl>  <nl> switch ( cmd ) { <nl> case SIOCGIFNAME :
static int uvc_v4l2_put_xu_mapping ( const struct uvc_xu_control_mapping * kp , <nl> __put_user ( kp -> menu_count , & up -> menu_count )) <nl> return - EFAULT ; <nl>  <nl> - __clear_user ( up -> reserved , sizeof ( up -> reserved )); <nl> + if ( __clear_user ( up -> reserved , sizeof ( up -> reserved ))) <nl> + return - EFAULT ; <nl>  <nl> if ( kp -> menu_count == 0 ) <nl> return 0 ;
static void dbri_debug_read ( struct snd_info_entry * entry , <nl> } <nl> # endif <nl>  <nl> - void __devinit snd_dbri_proc ( struct snd_card * card ) <nl> + static void __devinit snd_dbri_proc ( struct snd_card * card ) <nl> { <nl> struct snd_dbri * dbri = card -> private_data ; <nl> struct snd_info_entry * entry ;
int __devinit of_mtd_parse_partitions ( struct device * dev , <nl> return nr_parts ; <nl> } <nl> EXPORT_SYMBOL ( of_mtd_parse_partitions ); <nl> + <nl> + MODULE_LICENSE (" GPL ");
int vb2_mmap ( struct vb2_queue * q , struct vm_area_struct * vma ) <nl> { <nl> unsigned long off = vma -> vm_pgoff << PAGE_SHIFT ; <nl> struct vb2_buffer * vb ; <nl> - unsigned int buffer , plane ; <nl> + unsigned int buffer = 0 , plane = 0 ; <nl> int ret ; <nl> unsigned long length ; <nl> 
static int __init sh_pm_runtime_init ( void ) <nl> ! of_machine_is_compatible (" renesas , r8a7779 ") && <nl> ! of_machine_is_compatible (" renesas , r8a7790 ") && <nl> ! of_machine_is_compatible (" renesas , r8a7791 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7792 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7793 ") && <nl> + ! of_machine_is_compatible (" renesas , r8a7794 ") && <nl> ! of_machine_is_compatible (" renesas , sh7372 ") && <nl> ! of_machine_is_compatible (" renesas , sh73a0 ")) <nl> return 0 ;
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> - err = do_migrate_pages ( mm , & old , & new , MPOL_MF_MOVE ); <nl> + err = do_migrate_pages ( mm , & old , & new , <nl> + capable ( CAP_SYS_ADMIN ) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE ); <nl> out : <nl> mmput ( mm ); <nl> return err ;
static void parse_system_parameter_string ( struct seq_file * m ) <nl> if (! workbuffer ) { <nl> printk ( KERN_ERR "% s % s kmalloc failure at line % d \ n ", <nl> __FILE__ , __FUNCTION__ , __LINE__ ); <nl> + kfree ( local_buffer ); <nl> return ; <nl> } <nl> # ifdef LPARCFG_DEBUG
qla2x00_do_dpc_vp ( scsi_qla_host_t * vha ) <nl> struct qla_hw_data * ha = vha -> hw ; <nl> scsi_qla_host_t * base_vha = pci_get_drvdata ( ha -> pdev ); <nl>  <nl> + if (!( ha -> current_topology & ISP_CFG_F )) <nl> + return 0 ; <nl> + <nl> if ( test_and_clear_bit ( VP_IDX_ACQUIRED , & vha -> vp_flags )) { <nl> /* VP acquired . complete port configuration */ <nl> if ( atomic_read (& base_vha -> loop_state ) == LOOP_READY ) {
int wl12xx_allocate_link ( struct wl1271 * wl , struct wl12xx_vif * wlvif , u8 * hlid ) <nl> __set_bit ( link , wl -> links_map ); <nl> __set_bit ( link , wlvif -> links_map ); <nl> spin_unlock_irqrestore (& wl -> wl_lock , flags ); <nl> + <nl> + /* take the last " freed packets " value from the current FW status */ <nl> + wl -> links [ link ]. prev_freed_pkts = <nl> + wl -> fw_status_2 -> counters . tx_lnk_free_pkts [ link ]; <nl> * hlid = link ; <nl> return 0 ; <nl> }
static ssize_t eisa_eeprom_read ( struct file * file , <nl> ssize_t ret ; <nl> int i ; <nl>  <nl> - if (* ppos >= HPEE_MAX_LENGTH ) <nl> + if (* ppos < 0 || * ppos >= HPEE_MAX_LENGTH ) <nl> return 0 ; <nl>  <nl> count = * ppos + count < HPEE_MAX_LENGTH ? count : HPEE_MAX_LENGTH - * ppos ;
Configuration options : <nl> # define BCD 0x01 <nl>  <nl> # define ATAO_2_RTSISHFT 0x06 /* W 8 */ <nl> -# define RSI 0x01 <nl> +# define ATAO_RTSISHFT_RSI ( 1 << 0 ) <nl>  <nl> # define ATAO_2_RTSISTRB 0x07 /* W 8 */ <nl> 
static inline void dev_set_cma_area ( struct device * dev , struct cma * cma ) <nl> { <nl> if ( dev ) <nl> dev -> cma_area = cma ; <nl> - if (! dev || ! dma_contiguous_default_area ) <nl> + if (! dev && ! dma_contiguous_default_area ) <nl> dma_contiguous_default_area = cma ; <nl> } <nl> 
static ssize_t w1_seq_show ( struct device * device , <nl> w1_write_8 ( sl -> master , W1_42_COND_READ ); <nl> rv = w1_read_block ( sl -> master , ( u8 *)& rn , 8 ); <nl> reg_num = ( struct w1_reg_num *) & rn ; <nl> - if (( char ) reg_num -> family == W1_42_FINISHED_BYTE ) <nl> + if ( reg_num -> family == W1_42_FINISHED_BYTE ) <nl> break ; <nl> if ( sl -> reg_num . id == reg_num -> id ) <nl> seq = i ;
static void iso_callback ( struct fw_iso_context * context , u32 cycle , <nl> struct client * client = data ; <nl> struct iso_interrupt_event * e ; <nl>  <nl> - e = kzalloc ( sizeof (* e ) + header_length , GFP_ATOMIC ); <nl> + e = kmalloc ( sizeof (* e ) + header_length , GFP_ATOMIC ); <nl> if ( e == NULL ) <nl> return ; <nl> 
static const struct usb_device_id hso_ids [] = { <nl> { USB_DEVICE ( 0x0af0 , 0x8600 )}, <nl> { USB_DEVICE ( 0x0af0 , 0x8800 )}, <nl> { USB_DEVICE ( 0x0af0 , 0x8900 )}, <nl> + { USB_DEVICE ( 0x0af0 , 0x9000 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd035 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd055 )}, <nl> { USB_DEVICE ( 0x0af0 , 0xd155 )},
static void snd_als4000_configure ( struct snd_sb * chip ) <nl> /* SPECS_PAGE : 39 */ <nl> for ( i = ALS4K_GCR91_DMA0_ADDR ; i <= ALS4K_GCR96_DMA3_MODE_COUNT ; ++ i ) <nl> snd_als4k_gcr_write ( chip , i , 0 ); <nl> - <nl> + /* enable burst mode to prevent dropouts during high PCI bus usage */ <nl> snd_als4k_gcr_write ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL , <nl> - snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL )); <nl> + snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL ) | 0x04 ); <nl> spin_unlock_irq (& chip -> reg_lock ); <nl> } <nl> 
static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> int err ; <nl> unsigned int mss ; <nl>  <nl> + if ( packets == 0 ) <nl> + return ; <nl> + <nl> WARN_ON ( packets > tp -> packets_out ); <nl> if ( tp -> lost_skb_hint ) { <nl> skb = tp -> lost_skb_hint ;
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int do_dev_config ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> if (! devs ) { <nl> dev_err ( dev -> class_dev , <nl> " Could not allocate memory . Out of memory ?\ n "); <nl> + kfree ( bdev ); <nl> return - ENOMEM ; <nl> } <nl> devpriv -> devs = devs ;
int prism2_scan ( struct wiphy * wiphy , struct net_device * dev , <nl> msg1 . msgcode = DIDmsg_dot11req_scan ; <nl> msg1 . bsstype . data = P80211ENUM_bsstype_any ; <nl>  <nl> - memset (&( msg1 . bssid . data ), 0xFF , sizeof ( p80211item_pstr6_t )); <nl> + memset (& msg1 . bssid . data , 0xFF , sizeof ( msg1 . bssid . data )); <nl> msg1 . bssid . data . len = 6 ; <nl>  <nl> if ( request -> n_ssids > 0 ) {
static void * i915_gem_dmabuf_vmap ( struct dma_buf * dma_buf ) <nl> goto error ; <nl>  <nl> i = 0 ; <nl> - for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ); <nl> + for_each_sg_page ( obj -> pages -> sgl , & sg_iter , obj -> pages -> nents , 0 ) <nl> pages [ i ++] = sg_page_iter_page (& sg_iter ); <nl>  <nl> obj -> dma_buf_vmapping = vmap ( pages , i , 0 , PAGE_KERNEL );
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
xfs_log_force_umount ( <nl> * before we mark the filesystem SHUTDOWN and wake <nl> * everybody up to tell the bad news . <nl> */ <nl> - spin_lock (& log -> l_grant_lock ); <nl> spin_lock (& log -> l_icloglock ); <nl> + spin_lock (& log -> l_grant_lock ); <nl> mp -> m_flags |= XFS_MOUNT_FS_SHUTDOWN ; <nl> XFS_BUF_DONE ( mp -> m_sb_bp ); <nl> /*
static int ocfs2_truncate_file ( struct inode * inode , <nl> ( unsigned long long ) OCFS2_I ( inode )-> ip_blkno , <nl> ( unsigned long long ) new_i_size ); <nl>  <nl> + unmap_mapping_range ( inode -> i_mapping , new_i_size + PAGE_SIZE - 1 , 0 , 1 ); <nl> truncate_inode_pages ( inode -> i_mapping , new_i_size ); <nl>  <nl> fe = ( struct ocfs2_dinode *) di_bh -> b_data ;
static const __u8 uvc_camera_guid [ 16 ] = UVC_GUID_UVC_CAMERA ; <nl> static const __u8 uvc_media_transport_input_guid [ 16 ] = <nl> UVC_GUID_UVC_MEDIA_TRANSPORT_INPUT ; <nl>  <nl> - static int uvc_entity_match_guid ( struct uvc_entity * entity , __u8 guid [ 16 ]) <nl> + static int uvc_entity_match_guid ( const struct uvc_entity * entity , <nl> + const __u8 guid [ 16 ]) <nl> { <nl> switch ( UVC_ENTITY_TYPE ( entity )) { <nl> case UVC_ITT_CAMERA :
int seq_bitmap ( struct seq_file * m , const unsigned long * bits , <nl> unsigned int nr_bits ); <nl> static inline int seq_cpumask ( struct seq_file * m , const struct cpumask * mask ) <nl> { <nl> - return seq_bitmap ( m , mask -> bits , NR_CPUS ); <nl> + return seq_bitmap ( m , mask -> bits , nr_cpu_ids ); <nl> } <nl>  <nl> static inline int seq_nodemask ( struct seq_file * m , nodemask_t * mask )
static int run ( mddev_t * mddev ) <nl> int i ; <nl>  <nl> conf_t * conf = kmalloc ( sizeof (* conf ), GFP_KERNEL ); <nl> + if (! conf ) <nl> + return - ENOMEM ; <nl>  <nl> for ( i = 0 ; i < Modes ; i ++) { <nl> atomic_set (& conf -> counters [ i ], 0 );
void __init pxa_init_irq_gpio ( int gpio_nr ) <nl> set_irq_flags ( irq , IRQF_VALID | IRQF_PROBE ); <nl> } <nl>  <nl> - for ( irq = IRQ_GPIO ( 2 ); irq <= IRQ_GPIO ( gpio_nr ); irq ++) { <nl> + for ( irq = IRQ_GPIO ( 2 ); irq < IRQ_GPIO ( gpio_nr ); irq ++) { <nl> set_irq_chip ( irq , & pxa_muxed_gpio_chip ); <nl> set_irq_handler ( irq , handle_edge_irq ); <nl> set_irq_flags ( irq , IRQF_VALID | IRQF_PROBE );
static int soc_hw_match_params ( struct snd_pcm_substream * substream , <nl> /* normalise cpu bfs div & codec const mult */ <nl> codec_bfs = soc_bfs_rate_to_div ( codec_dai_mode -> bfs , rate , <nl> mclk , rtd -> codec_dai -> dai_runtime . pcmfmt , chn ); <nl> - if ( codec_dai_mode -> bfs & codec_bfs ) { <nl> + if ( cpu_dai_mode -> bfs & codec_bfs ) { <nl> rtd -> cpu_dai -> dai_runtime . bfs = codec_bfs ; <nl> rtd -> codec_dai -> dai_runtime . bfs = codec_dai_mode -> bfs ; <nl> } else
static int rtl8169_rx_interrupt ( struct net_device * dev , <nl> pkt_size , PCI_DMA_FROMDEVICE ); <nl> rtl8169_mark_to_asic ( desc , tp -> rx_buf_sz ); <nl> } else { <nl> - pci_unmap_single ( pdev , addr , pkt_size , <nl> + pci_unmap_single ( pdev , addr , tp -> rx_buf_sz , <nl> PCI_DMA_FROMDEVICE ); <nl> tp -> Rx_skbuff [ entry ] = NULL ; <nl> }
void __update_cache ( struct vm_area_struct * vma , <nl> return ; <nl>  <nl> page = pfn_to_page ( pfn ); <nl> - if ( pfn_valid ( pfn ) && page_mapping ( page )) { <nl> + if ( pfn_valid ( pfn )) { <nl> int dirty = test_and_clear_bit ( PG_dcache_dirty , & page -> flags ); <nl> if ( dirty ) { <nl> unsigned long addr = ( unsigned long ) page_address ( page );
static void w9966_term ( struct w9966 * cam ) <nl> parport_unregister_device ( cam -> pdev ); <nl> w9966_set_state ( cam , W9966_STATE_PDEV , 0 ); <nl> } <nl> + memset ( cam , 0 , sizeof (* cam )); <nl> } <nl>  <nl> 
static ssize_t cpufv_store ( struct device * dev , <nl> return rv ; <nl> if ( value < 0 || value >= c . num ) <nl> return - EINVAL ; <nl> - set_acpi ( eeepc , CM_ASL_CPUFV , value ); <nl> + rv = set_acpi ( eeepc , CM_ASL_CPUFV , value ); <nl> + if ( rv ) <nl> + return rv ; <nl> return count ; <nl> } <nl> 
static int isp_register_entities ( struct isp_device * isp ) <nl> goto done ; <nl>  <nl> /* Register external entities */ <nl> - for ( subdevs = pdata -> subdevs ; subdevs -> subdevs ; ++ subdevs ) { <nl> + for ( subdevs = pdata -> subdevs ; subdevs && subdevs -> subdevs ; ++ subdevs ) { <nl> struct v4l2_subdev * sensor ; <nl> struct media_entity * input ; <nl> unsigned int flags ;
static __init void nested_vmx_setup_ctls_msrs ( void ) <nl> /* nested EPT : emulate EPT also to L1 */ <nl> nested_vmx_secondary_ctls_high |= SECONDARY_EXEC_ENABLE_EPT ; <nl> nested_vmx_ept_caps = VMX_EPT_PAGE_WALK_4_BIT | <nl> - VMX_EPTP_WB_BIT | VMX_EPT_INVEPT_BIT ; <nl> + VMX_EPTP_WB_BIT | VMX_EPT_2MB_PAGE_BIT | <nl> + VMX_EPT_INVEPT_BIT ; <nl> nested_vmx_ept_caps &= vmx_capability . ept ; <nl> /* <nl> * Since invept is completely emulated we support both global
static int option_inquiry ( struct us_data * us ) <nl>  <nl> result = memcmp ( buffer + 8 , " Option ", 6 ); <nl>  <nl> + if ( result != 0 ) <nl> + result = memcmp ( buffer + 8 , " ZCOPTION ", 8 ); <nl> + <nl> /* Read the CSW */ <nl> usb_stor_bulk_transfer_buf ( us , <nl> us -> recv_bulk_pipe ,
static irqreturn_t snd_hdsp_interrupt ( int irq , void * dev_id ) <nl> midi0status = hdsp_read ( hdsp , HDSP_midiStatusIn0 ) & 0xff ; <nl> midi1status = hdsp_read ( hdsp , HDSP_midiStatusIn1 ) & 0xff ; <nl>  <nl> + if (!( hdsp -> state & HDSP_InitializationComplete )) <nl> + return IRQ_HANDLED ; <nl> + <nl> if ( audio ) { <nl> if ( hdsp -> capture_substream ) <nl> snd_pcm_period_elapsed ( hdsp -> pcm -> streams [ SNDRV_PCM_STREAM_CAPTURE ]. substream );
static void dwc3_endpoint_interrupt ( struct dwc3 * dwc , <nl>  <nl> dep = dwc -> eps [ epnum ]; <nl>  <nl> + if (!( dep -> flags & DWC3_EP_ENABLED )) <nl> + return ; <nl> + <nl> dev_vdbg ( dwc -> dev , "% s : % s \ n ", dep -> name , <nl> dwc3_ep_event_string ( event -> endpoint_event )); <nl> 
retry : <nl> list_add_tail (& cap -> session_caps , & session -> s_caps ); <nl> session -> s_nr_caps ++; <nl> spin_unlock (& session -> s_cap_lock ); <nl> - } <nl> + } else if ( new_cap ) <nl> + ceph_put_cap ( mdsc , new_cap ); <nl>  <nl> if (! ci -> i_snap_realm ) { <nl> /*
struct e1000_nvm_operations { <nl>  <nl> struct e1000_mac_info { <nl> struct e1000_mac_operations ops ; <nl> - <nl> - u8 addr [ 6 ]; <nl> - u8 perm_addr [ 6 ]; <nl> + u8 addr [ ETH_ALEN ]; <nl> + u8 perm_addr [ ETH_ALEN ]; <nl>  <nl> enum e1000_mac_type type ; <nl> 
xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
static unsigned int br_nf_pre_routing ( unsigned int hook , struct sk_buff * skb , <nl>  <nl> pskb_trim_rcsum ( skb , len ); <nl>  <nl> + /* BUG : Should really parse the IP options here . */ <nl> + memset ( IPCB ( skb ), 0 , sizeof ( struct inet_skb_parm )); <nl> + <nl> nf_bridge_put ( skb -> nf_bridge ); <nl> if (! nf_bridge_alloc ( skb )) <nl> return NF_DROP ;
static void das16m1_handler ( struct comedi_device * dev , unsigned int status ) <nl> num_samples = FIFO_SIZE ; <nl> insw ( dev -> iobase , devpriv -> ai_buffer , num_samples ); <nl> munge_sample_array ( devpriv -> ai_buffer , num_samples ); <nl> - cfc_write_array_to_buffer ( s , devpriv -> ai_buffer , <nl> - num_samples * sizeof ( short )); <nl> + comedi_buf_write_samples ( s , devpriv -> ai_buffer , num_samples ); <nl> devpriv -> adc_count += num_samples ; <nl>  <nl> if ( cmd -> stop_src == TRIG_COUNT ) {
void thread__delete ( struct thread * thread ) <nl> { <nl> struct comm * comm , * tmp ; <nl>  <nl> - map_groups__put ( thread -> mg ); <nl> - thread -> mg = NULL ; <nl> + if ( thread -> mg ) { <nl> + map_groups__put ( thread -> mg ); <nl> + thread -> mg = NULL ; <nl> + } <nl> list_for_each_entry_safe ( comm , tmp , & thread -> comm_list , list ) { <nl> list_del (& comm -> list ); <nl> comm__free ( comm );
static inline bool i40e_vc_isvalid_vsi_id ( struct i40e_vf * vf , u8 vsi_id ) <nl> { <nl> struct i40e_pf * pf = vf -> pf ; <nl>  <nl> + if ( vsi_id > pf -> num_alloc_vsi ) <nl> + return false ; <nl> return pf -> vsi [ vsi_id ]-> vf_id == vf -> vf_id ; <nl> } <nl> 
int __init acpi_ec_ecdt_probe ( void ) <nl> saved_ec = kmalloc ( sizeof ( struct acpi_ec ), GFP_KERNEL ); <nl> if (! saved_ec ) <nl> return - ENOMEM ; <nl> - memcpy (& saved_ec , boot_ec , sizeof ( saved_ec )); <nl> + memcpy ( saved_ec , boot_ec , sizeof (* saved_ec )); <nl> /* fall through */ <nl> } <nl> /* This workaround is needed only on some broken machines ,
void * __init alloc_large_system_hash ( const char * tablename , <nl> table = __vmalloc ( size , GFP_ATOMIC , PAGE_KERNEL ); <nl> else { <nl> unsigned long order = get_order ( size ); <nl> - table = ( void *) __get_free_pages ( GFP_ATOMIC , order ); <nl> + <nl> + if ( order < MAX_ORDER ) <nl> + table = ( void *) __get_free_pages ( GFP_ATOMIC , <nl> + order ); <nl> /* <nl> * If bucketsize is not a power - of - two , we may free <nl> * some pages at the end of hash table .
void sctp_transport_burst_limited ( struct sctp_transport * t ) <nl> u32 old_cwnd = t -> cwnd ; <nl> u32 max_burst_bytes ; <nl>  <nl> - if ( t -> burst_limited ) <nl> + if ( t -> burst_limited || asoc -> max_burst == 0 ) <nl> return ; <nl>  <nl> max_burst_bytes = t -> flight_size + ( asoc -> max_burst * asoc -> pathmtu );
int do_sys_settimeofday ( struct timespec * tv , struct timezone * tz ) <nl> static int firsttime = 1 ; <nl> int error = 0 ; <nl>  <nl> + if (! timespec_valid ( tv )) <nl> + return - EINVAL ; <nl> + <nl> error = security_settime ( tv , tz ); <nl> if ( error ) <nl> return error ;
static void vmlinux_path__exit ( void ) <nl> { <nl> while (-- vmlinux_path__nr_entries >= 0 ) <nl> zfree (& vmlinux_path [ vmlinux_path__nr_entries ]); <nl> + vmlinux_path__nr_entries = 0 ; <nl>  <nl> zfree (& vmlinux_path ); <nl> }
static void netlink_rcv_cb ( struct sk_buff * skb ) <nl> if ( skb -> len >= NLMSG_HDRLEN ) { <nl> nlh = ( struct nlmsghdr *) skb -> data ; <nl>  <nl> - if ( skb -> len < nlh -> nlmsg_len || <nl> + if ( nlh -> nlmsg_len < ND_IFINDEX_LEN || <nl> + nlh -> nlmsg_len > skb -> len || <nl> nlh -> nlmsg_len > ND_MAX_MSG_LEN ) { <nl> netdev_err ( skb -> dev , " Invalid length (% d ,% d )\ n ", <nl> skb -> len , nlh -> nlmsg_len );
static void __devexit slic_entry_remove ( struct pci_dev * pcidev ) <nl> } <nl> free_netdev ( dev ); <nl> pci_release_regions ( pcidev ); <nl> + pci_disable_device ( pcidev ); <nl> } <nl>  <nl> static int slic_entry_halt ( struct net_device * dev )
static inline void run_hrtimer_queue ( struct hrtimer_base * base ) <nl> { <nl> struct rb_node * node ; <nl>  <nl> + if (! base -> first ) <nl> + return ; <nl> + <nl> if ( base -> get_softirq_time ) <nl> base -> softirq_time = base -> get_softirq_time (); <nl> 
static int efi_status_to_err ( efi_status_t status ) <nl> err = - EACCES ; <nl> break ; <nl> case EFI_NOT_FOUND : <nl> - err = - ENOENT ; <nl> + err = - EIO ; <nl> break ; <nl> default : <nl> err = - EINVAL ;
xfs_bmapi ( <nl> xfs_fsblock_t abno ; /* allocated block number */ <nl> xfs_extlen_t alen ; /* allocated extent length */ <nl> xfs_fileoff_t aoff ; /* allocated file offset */ <nl> - xfs_bmalloca_t bma ; /* args for xfs_bmap_alloc */ <nl> + xfs_bmalloca_t bma = { 0 }; /* args for xfs_bmap_alloc */ <nl> xfs_btree_cur_t * cur ; /* bmap btree cursor */ <nl> xfs_fileoff_t end ; /* end of mapped file region */ <nl> int eof ; /* we ' ve hit the end of extents */
static struct sk_buff * be_insert_vlan_in_pkt ( struct be_adapter * adapter , <nl>  <nl> if ( vlan_tx_tag_present ( skb )) { <nl> vlan_tag = be_get_tx_vlan_tag ( adapter , skb ); <nl> - __vlan_put_tag ( skb , vlan_tag ); <nl> - skb -> vlan_tci = 0 ; <nl> + skb = __vlan_put_tag ( skb , vlan_tag ); <nl> + if ( skb ) <nl> + skb -> vlan_tci = 0 ; <nl> } <nl>  <nl> return skb ;
int ieee80211_request_sched_scan_start ( struct ieee80211_sub_if_data * sdata , <nl> for ( i = 0 ; i < IEEE80211_NUM_BANDS ; i ++) { <nl> local -> sched_scan_ies . ie [ i ] = kzalloc ( 2 + <nl> IEEE80211_MAX_SSID_LEN + <nl> - local -> scan_ies_len , <nl> + local -> scan_ies_len + <nl> + req -> ie_len , <nl> GFP_KERNEL ); <nl> if (! local -> sched_scan_ies . ie [ i ]) { <nl> ret = - ENOMEM ;
static int pctv452e_read_mac_address ( struct dvb_usb_device * d , u8 mac [ 6 ]) <nl> return 0 ; <nl>  <nl> failed : <nl> - memset ( mac , 0 , 6 ); <nl> + eth_zero_addr ( mac ); <nl>  <nl> return ret ; <nl> }
int spu_acquire_runnable ( struct spu_context * ctx ) <nl>  <nl> if ( ctx -> state == SPU_STATE_SAVED ) { <nl> ret = spu_activate ( ctx , 0 ); <nl> + if ( ret ) <nl> + goto out ; <nl> ctx -> state = SPU_STATE_RUNNABLE ; <nl> } <nl> - if ( ret ) <nl> - goto out ; <nl>  <nl> downgrade_write (& ctx -> state_sema ); <nl> /* On success , we return holding the lock */
nvc0_fb_init ( struct drm_device * dev ) <nl> priv = dev_priv -> engine . fb . priv ; <nl>  <nl> nv_wr32 ( dev , 0x100c10 , priv -> r100c10 >> 8 ); <nl> + nv_mask ( dev , 0x17e820 , 0x00100000 , 0x00000000 ); /* NV_PLTCG_INTR_EN */ <nl> return 0 ; <nl> } <nl> 
int cx25821_video_register ( struct cx25821_dev * dev ) <nl>  <nl> spin_lock_init (& dev -> slock ); <nl>  <nl> - for ( i = 0 ; i < MAX_VID_CHANNEL_NUM - 1 ; ++ i ) { <nl> + for ( i = 0 ; i < VID_CHANNEL_NUM ; ++ i ) { <nl> cx25821_init_controls ( dev , i ); <nl>  <nl> cx25821_risc_stopper ( dev -> pci , & dev -> channels [ i ]. vidq . stopper ,
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> AAC_OPT_NEW_COMM ) ? <nl> ( dev -> scsi_host_ptr -> max_sectors << 9 ) : <nl> 65536 )) { <nl> + kfree ( usg ); <nl> rcode = - EINVAL ; <nl> goto cleanup ; <nl> }
static void __call_console_drivers ( unsigned long start , unsigned long end ) <nl>  <nl> static int __read_mostly ignore_loglevel ; <nl>  <nl> - int __init ignore_loglevel_setup ( char * str ) <nl> + static int __init ignore_loglevel_setup ( char * str ) <nl> { <nl> ignore_loglevel = 1 ; <nl> printk ( KERN_INFO " debug : ignoring loglevel setting .\ n ");
static int snd_rme32_capture_close ( struct snd_pcm_substream * substream ) <nl> spin_lock_irq (& rme32 -> lock ); <nl> rme32 -> capture_substream = NULL ; <nl> rme32 -> capture_periodsize = 0 ; <nl> - spin_unlock (& rme32 -> lock ); <nl> + spin_unlock_irq (& rme32 -> lock ); <nl> return 0 ; <nl> } <nl> 
static void r600_cs_parser_fini ( struct radeon_cs_parser * parser , int error ) <nl> kfree ( parser -> relocs ); <nl> for ( i = 0 ; i < parser -> nchunks ; i ++) { <nl> kfree ( parser -> chunks [ i ]. kdata ); <nl> + kfree ( parser -> chunks [ i ]. kpage [ 0 ]); <nl> + kfree ( parser -> chunks [ i ]. kpage [ 1 ]); <nl> } <nl> kfree ( parser -> chunks ); <nl> kfree ( parser -> chunks_array );
static int pl2303_tiocmset ( struct tty_struct * tty , <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl>  <nl> mutex_lock (& serial -> disc_mutex ); <nl> - if (! serial -> disconnected ) <nl> + if (! serial -> disconnected ) { <nl> ret = pl2303_set_control_lines ( port , control ); <nl> - else <nl> + if ( ret ) <nl> + ret = usb_translate_errors ( ret ); <nl> + } else { <nl> ret = - ENODEV ; <nl> + } <nl> mutex_unlock (& serial -> disc_mutex ); <nl>  <nl> return ret ;
int perf_event_init_context ( struct task_struct * child , int ctxn ) <nl> * swapped under us . <nl> */ <nl> parent_ctx = perf_pin_task_context ( parent , ctxn ); <nl> + if (! parent_ctx ) <nl> + return 0 ; <nl>  <nl> /* <nl> * No need to check if parent_ctx != NULL here ; since we saw
static int ath10k_station_assoc ( struct ath10k * ar , struct ath10k_vif * arvif , <nl> return ret ; <nl> } <nl>  <nl> - if (! sta -> wme ) { <nl> + if (! sta -> wme && ! reassoc ) { <nl> arvif -> num_legacy_stations ++; <nl> ret = ath10k_recalc_rtscts_prot ( arvif ); <nl> if ( ret ) {
recurse : <nl> if ( err ) <nl> goto out_free ; <nl>  <nl> - kobject_uevent (& b -> kobj , KOBJ_ADD ); <nl> + if ( b ) <nl> + kobject_uevent (& b -> kobj , KOBJ_ADD ); <nl>  <nl> return err ; <nl> 
static int get_info ( struct net * net , void __user * user , <nl> private = & tmp ; <nl> } <nl> # endif <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . valid_hooks = t -> valid_hooks ; <nl> memcpy ( info . hook_entry , private -> hook_entry , <nl> sizeof ( info . hook_entry ));
int ehci_suspend ( struct usb_hcd * hcd , bool do_wakeup ) <nl> clear_bit ( HCD_FLAG_HW_ACCESSIBLE , & hcd -> flags ); <nl> spin_unlock_irq (& ehci -> lock ); <nl>  <nl> + synchronize_irq ( hcd -> irq ); <nl> + <nl> + /* Check for race with a wakeup request */ <nl> + if ( do_wakeup && HCD_WAKEUP_PENDING ( hcd )) { <nl> + ehci_resume ( hcd , false ); <nl> + return - EBUSY ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( ehci_suspend );
int lapic_watchdog_init ( unsigned nmi_hz ) <nl> probe_nmi_watchdog (); <nl> if (! wd_ops ) <nl> return - 1 ; <nl> + <nl> + if (! wd_ops -> reserve ()) { <nl> + printk ( KERN_ERR <nl> + " NMI watchdog : cannot reserve perfctrs \ n "); <nl> + return - 1 ; <nl> + } <nl> } <nl>  <nl> if (!( wd_ops -> setup ( nmi_hz ))) {
_xfs_buf_ioapply ( <nl> int size ; <nl> int i ; <nl>  <nl> + /* <nl> + * Make sure we capture only current IO errors rather than stale errors <nl> + * left over from previous use of the buffer ( e . g . failed readahead ). <nl> + */ <nl> + bp -> b_error = 0 ; <nl> + <nl> if ( bp -> b_flags & XBF_WRITE ) { <nl> if ( bp -> b_flags & XBF_SYNCIO ) <nl> rw = WRITE_SYNC ;
static struct crypto_instance * crypto_cts_alloc ( struct rtattr ** tb ) <nl> if (! is_power_of_2 ( alg -> cra_blocksize )) <nl> goto out_put_alg ; <nl>  <nl> + if ( strncmp ( alg -> cra_name , " cbc (", 4 )) <nl> + goto out_put_alg ; <nl> + <nl> inst = crypto_alloc_instance (" cts ", alg ); <nl> if ( IS_ERR ( inst )) <nl> goto out_put_alg ;
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> loff_t file_size ; <nl> unsigned int num ; <nl> unsigned int offset ; <nl> - size_t total_len ; <nl> + size_t total_len = 0 ; <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req ))
proc_dodebug ( ctl_table * table , int write , struct file * file , <nl> left --, s ++; <nl> *( unsigned int *) table -> data = value ; <nl> /* Display the RPC tasks on writing to rpc_debug */ <nl> - if ( table -> ctl_name == CTL_RPCDEBUG ) { <nl> + if ( strcmp ( table -> procname , " rpc_debug ") == 0 ) <nl> rpc_show_tasks (); <nl> - } <nl> } else { <nl> if (! access_ok ( VERIFY_WRITE , buffer , left )) <nl> return - EFAULT ;
struct ieee80211_hw * ieee80211_alloc_hw ( size_t priv_data_len , <nl> local -> uapsd_max_sp_len = IEEE80211_DEFAULT_MAX_SP_LEN ; <nl>  <nl> INIT_LIST_HEAD (& local -> interfaces ); <nl> + <nl> + __hw_addr_init (& local -> mc_list ); <nl> + <nl> mutex_init (& local -> iflist_mtx ); <nl> mutex_init (& local -> scan_mtx ); <nl> 
static int do_ipv6_getsockopt ( struct sock * sk , int level , int optname , <nl> return - EINVAL ; <nl> if ( copy_from_user (& gsf , optval , GROUP_FILTER_SIZE ( 0 ))) <nl> return - EFAULT ; <nl> + if ( gsf . gf_group . ss_family != AF_INET6 ) <nl> + return - EADDRNOTAVAIL ; <nl> lock_sock ( sk ); <nl> err = ip6_mc_msfget ( sk , & gsf , <nl> ( struct group_filter __user *) optval , optlen );
static int sh_eth_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> skb -> len + 2 ); <nl> txdesc -> addr = dma_map_single (& ndev -> dev , skb -> data , skb -> len , <nl> DMA_TO_DEVICE ); <nl> + if ( dma_mapping_error (& ndev -> dev , txdesc -> addr )) { <nl> + kfree_skb ( skb ); <nl> + return NETDEV_TX_OK ; <nl> + } <nl> txdesc -> buffer_length = skb -> len ; <nl>  <nl> if ( entry >= mdp -> num_tx_ring - 1 )
static int snd_ctl_elem_add ( struct snd_ctl_file * file , <nl>  <nl> if ( card -> user_ctl_count >= MAX_USER_CONTROLS ) <nl> return - ENOMEM ; <nl> - if ( info -> count > 1024 ) <nl> + if ( info -> count < 1 ) <nl> return - EINVAL ; <nl> access = info -> access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE : <nl> ( info -> access & ( SNDRV_CTL_ELEM_ACCESS_READWRITE |
static struct w1_master * w1_alloc_dev ( u32 id , int slave_count , int slave_ttl , <nl> memcpy (& dev -> dev , device , sizeof ( struct device )); <nl> dev_set_name (& dev -> dev , " w1_bus_master % u ", dev -> id ); <nl> snprintf ( dev -> name , sizeof ( dev -> name ), " w1_bus_master % u ", dev -> id ); <nl> + dev -> dev . init_name = dev -> name ; <nl>  <nl> dev -> driver = driver ; <nl> 
static int __init lart_flash_init ( void ) <nl> mtd . name = module_name ; <nl> mtd . type = MTD_NORFLASH ; <nl> mtd . writesize = 1 ; <nl> + mtd . writebufsize = 4 ; <nl> mtd . flags = MTD_CAP_NORFLASH ; <nl> mtd . size = FLASH_BLOCKSIZE_PARAM * FLASH_NUMBLOCKS_16m_PARAM + FLASH_BLOCKSIZE_MAIN * FLASH_NUMBLOCKS_16m_MAIN ; <nl> mtd . erasesize = FLASH_BLOCKSIZE_MAIN ;
int radeon_ring_alloc ( struct radeon_device * rdev , struct radeon_ring * ring , unsi <nl> if ( ndw < ring -> ring_free_dw ) { <nl> break ; <nl> } <nl> + mutex_unlock (& ring -> mutex ); <nl> r = radeon_fence_wait_next ( rdev , radeon_ring_index ( rdev , ring )); <nl> + mutex_lock (& ring -> mutex ); <nl> if ( r ) <nl> return r ; <nl> }
void free_initmem ( void ) <nl> if (! have_of ) <nl> FREESEC ( openfirmware ); <nl> printk ("\ n "); <nl> + ppc_md . progress = NULL ; <nl> # undef FREESEC <nl> } <nl> 
static int ixgbe_rcv_msg_from_vf ( struct ixgbe_adapter * adapter , u32 vf ) <nl> } <nl> break ; <nl> case IXGBE_VF_SET_MACVLAN : <nl> + if ( adapter -> vfinfo [ vf ]. pf_set_mac ) { <nl> + e_warn ( drv , " VF % d requested MACVLAN filter but is " <nl> + " administratively denied \ n ", vf ); <nl> + retval = - 1 ; <nl> + break ; <nl> + } <nl> index = ( msgbuf [ 0 ] & IXGBE_VT_MSGINFO_MASK ) >> <nl> IXGBE_VT_MSGINFO_SHIFT ; <nl> /*
again : <nl> case STAC_DELL_M4_3 : <nl> spec -> num_dmics = 1 ; <nl> spec -> num_smuxes = 0 ; <nl> - spec -> num_dmuxes = 0 ; <nl> + spec -> num_dmuxes = 1 ; <nl> break ; <nl> default : <nl> spec -> num_dmics = STAC92HD71BXX_NUM_DMICS ;
vpfe_get_pdata ( struct platform_device * pdev ) <nl> pdata -> asd [ i ] = devm_kzalloc (& pdev -> dev , <nl> sizeof ( struct v4l2_async_subdev ), <nl> GFP_KERNEL ); <nl> + if (! pdata -> asd [ i ]) { <nl> + of_node_put ( rem ); <nl> + pdata = NULL ; <nl> + goto done ; <nl> + } <nl> + <nl> pdata -> asd [ i ]-> match_type = V4L2_ASYNC_MATCH_OF ; <nl> pdata -> asd [ i ]-> match . of . node = rem ; <nl> of_node_put ( endpoint );
again : <nl> } <nl> spin_unlock (& fs_info -> reada_lock ); <nl>  <nl> + kfree ( multi ); <nl> return re ; <nl>  <nl> error : <nl> error : <nl> kref_put (& zone -> refcnt , reada_zone_release ); <nl> spin_unlock (& fs_info -> reada_lock ); <nl> } <nl> + kfree ( multi ); <nl> kfree ( re ); <nl> if ( looped ) <nl> goto again ;
static int msp430_ir_init ( struct budget_ci * budget_ci ) <nl> dev -> input_phys = budget_ci -> ir . phys ; <nl> dev -> input_id . bustype = BUS_PCI ; <nl> dev -> input_id . version = 1 ; <nl> + dev -> scanmask = 0xff ; <nl> if ( saa -> pci -> subsystem_vendor ) { <nl> dev -> input_id . vendor = saa -> pci -> subsystem_vendor ; <nl> dev -> input_id . product = saa -> pci -> subsystem_device ;
static bool snd_soc_set_cache_val ( void * base , unsigned int idx , <nl> static unsigned int snd_soc_get_cache_val ( const void * base , unsigned int idx , <nl> unsigned int word_size ) <nl> { <nl> + if (! base ) <nl> + return - 1 ; <nl> + <nl> switch ( word_size ) { <nl> case 1 : { <nl> const u8 * cache = base ;
static int btrfs_show_devname ( struct seq_file * m , struct dentry * root ) <nl> list_for_each_entry ( dev , head , dev_list ) { <nl> if ( dev -> missing ) <nl> continue ; <nl> + if (! dev -> name ) <nl> + continue ; <nl> if (! first_dev || dev -> devid < first_dev -> devid ) <nl> first_dev = dev ; <nl> }
EXPORT_SYMBOL_GPL ( regmap_irq_chip_get_base ); <nl> */ <nl> int regmap_irq_get_virq ( struct regmap_irq_chip_data * data , int irq ) <nl> { <nl> + /* Handle holes in the IRQ list */ <nl> + if (! data -> chip -> irqs [ irq ]. mask ) <nl> + return - EINVAL ; <nl> + <nl> return irq_create_mapping ( data -> domain , irq ); <nl> } <nl> EXPORT_SYMBOL_GPL ( regmap_irq_get_virq );
struct tracepoint { <nl> do { \ <nl> void ** it_func ; \ <nl> \ <nl> - rcu_read_lock_sched (); \ <nl> + rcu_read_lock_sched_notrace (); \ <nl> it_func = rcu_dereference (( tp )-> funcs ); \ <nl> if ( it_func ) { \ <nl> do { \ <nl> (( void (*)( proto ))(* it_func ))( args ); \ <nl> } while (*(++ it_func )); \ <nl> } \ <nl> - rcu_read_unlock_sched (); \ <nl> + rcu_read_unlock_sched_notrace (); \ <nl> } while ( 0 ) <nl>  <nl> /*
static int drbd_accept ( const char ** what , struct socket * sock , struct socket ** n <nl> goto out ; <nl> } <nl> (* newsock )-> ops = sock -> ops ; <nl> + __module_get ((* newsock )-> ops -> owner ); <nl>  <nl> out : <nl> return err ;
static __inline__ int rt6_check_expired ( const struct rt6_info * rt ) <nl> static inline int rt6_need_strict ( struct in6_addr * daddr ) <nl> { <nl> return ( ipv6_addr_type ( daddr ) & <nl> - ( IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL )); <nl> + ( IPV6_ADDR_MULTICAST | IPV6_ADDR_LINKLOCAL | IPV6_ADDR_LOOPBACK )); <nl> } <nl>  <nl> /*
static u64 bpf_get_current_comm ( u64 r1 , u64 size , u64 r3 , u64 r4 , u64 r5 ) <nl> if (! task ) <nl> return - EINVAL ; <nl>  <nl> - memcpy ( buf , task -> comm , min_t ( size_t , size , sizeof ( task -> comm ))); <nl> + strlcpy ( buf , task -> comm , min_t ( size_t , size , sizeof ( task -> comm ))); <nl> return 0 ; <nl> } <nl> 
static int hist_browser__run ( struct hist_browser * browser , const char * help ) <nl>  <nl> hists__browser_title ( browser -> hists , hbt , title , sizeof ( title )); <nl>  <nl> - if ( ui_browser__show (& browser -> b , title , help ) < 0 ) <nl> + if ( ui_browser__show (& browser -> b , title , "% s ", help ) < 0 ) <nl> return - 1 ; <nl>  <nl> while ( 1 ) {
e1000_configure_tx ( struct e1000_adapter * adapter ) <nl> } <nl>  <nl> /* Set the default values for the Tx Inter Packet Gap timer */ <nl> - <nl> - if ( hw -> media_type == e1000_media_type_fiber || <nl> - hw -> media_type == e1000_media_type_internal_serdes ) <nl> + if ( adapter -> hw . mac_type <= e1000_82547_rev_2 && <nl> + ( hw -> media_type == e1000_media_type_fiber || <nl> + hw -> media_type == e1000_media_type_internal_serdes )) <nl> tipg = DEFAULT_82543_TIPG_IPGT_FIBER ; <nl> else <nl> tipg = DEFAULT_82543_TIPG_IPGT_COPPER ;
cfq_should_preempt ( struct cfq_data * cfqd , struct cfq_queue * new_cfqq , <nl> if ( cfq_class_idle ( cfqq )) <nl> return true ; <nl>  <nl> - if ( cfqd -> serving_type == SYNC_NOIDLE_WORKLOAD <nl> - && new_cfqq -> service_tree == cfqq -> service_tree ) <nl> + if ( cfqd -> serving_type == SYNC_NOIDLE_WORKLOAD && <nl> + cfqq_type ( new_cfqq ) == SYNC_NOIDLE_WORKLOAD && <nl> + new_cfqq -> service_tree -> count == 1 ) <nl> return true ; <nl>  <nl> /*
static int i915_dma_cleanup ( struct drm_device * dev ) <nl> if ( dev -> irq_enabled ) <nl> drm_irq_uninstall ( dev ); <nl>  <nl> + mutex_lock (& dev -> struct_mutex ); <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> render_ring ); <nl> if ( HAS_BSD ( dev )) <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> bsd_ring ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl>  <nl> /* Clear the HWS virtual address at teardown */ <nl> if ( I915_NEED_GFX_HWS ( dev ))
static ssize_t dlpar_cpu_probe ( const char * buf , size_t count ) <nl> return - ENODEV ; <nl>  <nl> dn = dlpar_configure_connector ( cpu_to_be32 ( drc_index ), parent ); <nl> + of_node_put ( parent ); <nl> if (! dn ) <nl> return - EINVAL ; <nl>  <nl> - of_node_put ( parent ); <nl> - <nl> rc = dlpar_attach_node ( dn ); <nl> if ( rc ) { <nl> dlpar_release_drc ( drc_index );
void __init lpc32xx_serial_init ( void ) <nl>  <nl> /* This needs to be done after all UART clocks are setup */ <nl> __raw_writel ( clkmodes , LPC32XX_UARTCTL_CLKMODE ); <nl> - for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ) - 1 ; i ++) { <nl> + for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ); i ++) { <nl> /* Force a flush of the RX FIFOs to work around a HW bug */ <nl> puart = serial_std_platform_data [ i ]. mapbase ; <nl> __raw_writel ( 0xC1 , LPC32XX_UART_IIR_FCR ( puart ));
static void _rtl_usb_tx_preprocess ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> u8 tid = 0 ; <nl> u16 seq_number = 0 ; <nl>  <nl> + memset (& tcb_desc , 0 , sizeof ( struct rtl_tcb_desc )); <nl> if ( ieee80211_is_auth ( fc )) { <nl> RT_TRACE ( rtlpriv , COMP_SEND , DBG_DMESG , (" MAC80211_LINKING \ n ")); <nl> rtl_ips_nic_on ( hw );
static void posix_acl_fix_xattr_userns ( <nl> break ; <nl> case ACL_GROUP : <nl> gid = make_kgid ( from , le32_to_cpu ( entry -> e_id )); <nl> - entry -> e_id = cpu_to_le32 ( from_kuid ( to , uid )); <nl> + entry -> e_id = cpu_to_le32 ( from_kgid ( to , gid )); <nl> break ; <nl> default : <nl> break ;
static int ch7006_encoder_create_resources ( struct drm_encoder * encoder , <nl> drm_mode_create_tv_properties ( dev , NUM_TV_NORMS , ch7006_tv_norm_names ); <nl>  <nl> priv -> scale_property = drm_property_create_range ( dev , 0 , " scale ", 0 , 2 ); <nl> + if (! priv -> scale_property ) <nl> + return - ENOMEM ; <nl>  <nl> drm_object_attach_property (& connector -> base , conf -> tv_select_subconnector_property , <nl> priv -> select_subconnector );
static inline void i2c_pnx_arm_timer ( struct i2c_adapter * adap ) <nl> struct timer_list * timer = & data -> mif . timer ; <nl> int expires = I2C_PNX_TIMEOUT / ( 1000 / HZ ); <nl>  <nl> + if ( expires <= 1 ) <nl> + expires = 2 ; <nl> + <nl> del_timer_sync ( timer ); <nl>  <nl> dev_dbg (& adap -> dev , " Timer armed at % lu plus % u jiffies .\ n ",
static int at86rf230_hw_init ( struct at86rf230_local * lp ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + /* Force setting slotted operation bit to 0 . Sometimes the atben <nl> + * sets this bit and I don ' t know why . We set this always force <nl> + * to zero while probing . <nl> + */ <nl> + rc = at86rf230_write_subreg ( lp , SR_SLOTTED_OPERATION , 0 ); <nl> + if ( rc ) <nl> + return rc ; <nl> + <nl> return 0 ; <nl> } <nl> 
service_in_request ( struct musb * musb , const struct usb_ctrlrequest * ctrlrequest ) <nl> static void musb_g_ep0_giveback ( struct musb * musb , struct usb_request * req ) <nl> { <nl> musb_g_giveback (& musb -> endpoints [ 0 ]. ep_in , req , 0 ); <nl> - musb -> ep0_state = MUSB_EP0_STAGE_SETUP ; <nl> } <nl>  <nl> /*
static const struct usb_device_id usbtouch_devices [] = { <nl>  <nl> # ifdef CONFIG_TOUCHSCREEN_USB_ITM <nl> { USB_DEVICE ( 0x0403 , 0xf9e9 ), . driver_info = DEVTYPE_ITM }, <nl> + { USB_DEVICE ( 0x16e3 , 0xf9e9 ), . driver_info = DEVTYPE_ITM }, <nl> # endif <nl>  <nl> # ifdef CONFIG_TOUCHSCREEN_USB_ETURBO
static int __devinit palmas_i2c_probe ( struct i2c_client * i2c , <nl> goto err ; <nl> } <nl>  <nl> + children [ PALMAS_PMIC_ID ]. platform_data = pdata -> pmic_pdata ; <nl> + children [ PALMAS_PMIC_ID ]. pdata_size = sizeof (* pdata -> pmic_pdata ); <nl> + <nl> ret = mfd_add_devices ( palmas -> dev , - 1 , <nl> children , ARRAY_SIZE ( palmas_children ), <nl> NULL , regmap_irq_chip_get_base ( palmas -> irq_data ));
static int parse_addr ( const struct nf_conn * ct , const char * cp , <nl> int family = ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . l3num ; <nl> int ret = 0 ; <nl>  <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> switch ( family ) { <nl> case AF_INET : <nl> ret = in4_pton ( cp , limit - cp , ( u8 *)& addr -> ip , - 1 , & end );
static void __devinit uvesafb_init_info ( struct fb_info * info , <nl> } <nl>  <nl> info -> flags = FBINFO_FLAG_DEFAULT | <nl> - ( par -> ypan ) ? FBINFO_HWACCEL_YPAN : 0 ; <nl> + ( par -> ypan ? FBINFO_HWACCEL_YPAN : 0 ); <nl>  <nl> if (! par -> ypan ) <nl> info -> fbops -> fb_pan_display = NULL ;
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN ); <nl> sta -> local = local ; <nl> sta -> sdata = sdata ; <nl> + sta -> last_rx = jiffies ; <nl>  <nl> ewma_init (& sta -> avg_signal , 1024 , 8 ); <nl> 
static void batadv_hash_init ( struct batadv_hashtable * hash ) <nl> { <nl> uint32_t i ; <nl>  <nl> - for ( i = 0 ; i < hash -> size ; i ++) { <nl> + for ( i = 0 ; i < hash -> size ; i ++) { <nl> INIT_HLIST_HEAD (& hash -> table [ i ]); <nl> spin_lock_init (& hash -> list_locks [ i ]); <nl> }
out_dio : <nl> * async dio is going to do it in the future or an end_io after an <nl> * error has already done it . <nl> */ <nl> - if ( ret == - EIOCBQUEUED || ! ocfs2_iocb_is_rw_locked ( iocb )) { <nl> + if (( ret == - EIOCBQUEUED ) || (! ocfs2_iocb_is_rw_locked ( iocb ))) { <nl> rw_level = - 1 ; <nl> have_alloc_sem = 0 ; <nl> }
read_again : <nl> skb = xgbe_create_skb ( pdata , rdata , & put_len ); <nl> if (! skb ) { <nl> error = 1 ; <nl> - goto read_again ; <nl> + goto skip_data ; <nl> } <nl> } <nl>  <nl> read_again : <nl> } <nl> } <nl>  <nl> + skip_data : <nl> if ( incomplete || context_next ) <nl> goto read_again ; <nl>  <nl> - /* Stray Context Descriptor ? */ <nl> if (! skb ) <nl> goto next_packet ; <nl> 
void qlcnic_set_multi ( struct net_device * netdev ) <nl> netdev_for_each_mc_addr ( ha , netdev ) { <nl> cur = kzalloc ( sizeof ( struct qlcnic_mac_list_s ), <nl> GFP_ATOMIC ); <nl> + if ( cur == NULL ) <nl> + break ; <nl> memcpy ( cur -> mac_addr , <nl> ha -> addr , ETH_ALEN ); <nl> list_add_tail (& cur -> list , & adapter -> vf_mc_list );
static void exynos5_powerdown_conf ( enum sys_powerdown mode ) <nl> void exynos_sys_powerdown_conf ( enum sys_powerdown mode ) <nl> { <nl> unsigned int i ; <nl> + const struct exynos_pmu_data * pmu_data ; <nl> + <nl> + if (! pmu_context ) <nl> + return ; <nl>  <nl> - const struct exynos_pmu_data * pmu_data = pmu_context -> pmu_data ; <nl> + pmu_data = pmu_context -> pmu_data ; <nl>  <nl> if ( pmu_data -> powerdown_conf ) <nl> pmu_data -> powerdown_conf ( mode );
static inline unsigned int elapsed_jiffies_msecs ( unsigned long start ) <nl> if ( end >= start ) <nl> return jiffies_to_msecs ( end - start ); <nl>  <nl> - return jiffies_to_msecs ( end + ( MAX_JIFFY_OFFSET - start ) + 1 ); <nl> + return jiffies_to_msecs ( end + ( ULONG_MAX - start ) + 1 ); <nl> } <nl>  <nl> void
static int uas_queuecommand_lck ( struct scsi_cmnd * cmnd , <nl> return SCSI_MLQUEUE_DEVICE_BUSY ; <nl> } <nl>  <nl> + memset ( cmdinfo , 0 , sizeof (* cmdinfo )); <nl> + <nl> if ( blk_rq_tagged ( cmnd -> request )) { <nl> cmdinfo -> stream = cmnd -> request -> tag + 2 ; <nl> } else {
luan_setup_hoses ( void ) <nl>  <nl> /* Allocate hoses for PCIX1 and PCIX2 */ <nl> hose1 = pcibios_alloc_controller (); <nl> + if (! hose1 ) <nl> + return ; <nl> + <nl> hose2 = pcibios_alloc_controller (); <nl> - if (! hose1 || ! hose2 ) <nl> + if (! hose2 ) { <nl> + pcibios_free_controller ( hose1 ); <nl> return ; <nl> + } <nl>  <nl> /* Setup PCIX1 */ <nl> hose1 -> first_busno = 0 ;
static struct urb * usbtv_setup_iso_transfer ( struct usbtv * usbtv ) <nl> ip -> transfer_flags = URB_ISO_ASAP ; <nl> ip -> transfer_buffer = kzalloc ( size * USBTV_ISOC_PACKETS , <nl> GFP_KERNEL ); <nl> + if (! ip -> transfer_buffer ) { <nl> + usb_free_urb ( ip ); <nl> + return NULL ; <nl> + } <nl> ip -> complete = usbtv_iso_cb ; <nl> ip -> number_of_packets = USBTV_ISOC_PACKETS ; <nl> ip -> transfer_buffer_length = size * USBTV_ISOC_PACKETS ;
struct request { <nl>  <nl> unsigned short ioprio ; <nl>  <nl> + unsigned int timeout ; <nl> + <nl> void * special ; /* opaque pointer available for LLD use */ <nl>  <nl> int errors ; <nl> struct request { <nl>  <nl> unsigned long deadline ; <nl> struct list_head timeout_list ; <nl> - unsigned int timeout ; <nl>  <nl> /* <nl> * completion callback .
nouveau_gem_object_open ( struct drm_gem_object * gem , struct drm_file * file_priv ) <nl> } <nl>  <nl> ret = pm_runtime_get_sync ( dev ); <nl> - if ( ret < 0 && ret != - EACCES ) <nl> + if ( ret < 0 && ret != - EACCES ) { <nl> + kfree ( vma ); <nl> goto out ; <nl> + } <nl>  <nl> ret = nouveau_bo_vma_add ( nvbo , cli -> vm , vma ); <nl> if ( ret )
static int tpm_binary_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 5th : delimiter */ <nl> seq_putc ( m , '\ 0 '); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl>  <nl> static int tpm_ascii_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 4th : eventname <= max + \' 0 ' delimiter */ <nl> seq_printf ( m , " % s \ n ", eventname ); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl> 
static struct drm_driver driver = { <nl> . minor = DRIVER_MINOR , <nl> . patchlevel = DRIVER_PATCHLEVEL , <nl>  <nl> - . gem_free_object = ast_gem_free_object , <nl> + . gem_free_object_unlocked = ast_gem_free_object , <nl> . dumb_create = ast_dumb_create , <nl> . dumb_map_offset = ast_dumb_mmap_offset , <nl> . dumb_destroy = drm_gem_dumb_destroy ,
qla24xx_reset_adapter ( scsi_qla_host_t * vha ) <nl> WRT_REG_DWORD (& reg -> hccr , HCCRX_REL_RISC_PAUSE ); <nl> RD_REG_DWORD (& reg -> hccr ); <nl> spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> + <nl> + if ( IS_NOPOLLING_TYPE ( ha )) <nl> + ha -> isp_ops -> enable_intrs ( ha ); <nl> } <nl>  <nl> /* On sparc systems , obtain port and node WWN from firmware
int dlpar_detach_node ( struct device_node * dn ) <nl> if ( rc ) <nl> return rc ; <nl>  <nl> - of_node_put ( dn ); /* Must decrement the refcount */ <nl> return 0 ; <nl> } <nl> 
static int atmel_serial_remove ( struct platform_device * pdev ) <nl> struct atmel_uart_port * atmel_port = to_atmel_uart_port ( port ); <nl> int ret = 0 ; <nl>  <nl> + tasklet_kill (& atmel_port -> tasklet ); <nl> + <nl> device_init_wakeup (& pdev -> dev , 0 ); <nl>  <nl> ret = uart_remove_one_port (& atmel_uart , port ); <nl>  <nl> - tasklet_kill (& atmel_port -> tasklet ); <nl> kfree ( atmel_port -> rx_ring . buf ); <nl>  <nl> /* " port " is allocated statically , so we shouldn ' t free it */
static inline int open_arg ( int flags , int mask ) <nl>  <nl> static int audit_match_perm ( struct audit_context * ctx , int mask ) <nl> { <nl> + unsigned n ; <nl> if ( unlikely (! ctx )) <nl> return 0 ; <nl>  <nl> - unsigned n = ctx -> major ; <nl> + n = ctx -> major ; <nl> switch ( audit_classify_syscall ( ctx -> arch , n )) { <nl> case 0 : /* native */ <nl> if (( mask & AUDIT_PERM_WRITE ) &&
static const struct file_operations bm_register_operations = { <nl> static ssize_t <nl> bm_status_read ( struct file * file , char __user * buf , size_t nbytes , loff_t * ppos ) <nl> { <nl> - char * s = enabled ? " enabled " : " disabled "; <nl> + char * s = enabled ? " enabled \ n " : " disabled \ n "; <nl>  <nl> return simple_read_from_buffer ( buf , nbytes , ppos , s , strlen ( s )); <nl> }
static enum dlm_status dlmunlock_common ( struct dlm_ctxt * dlm , <nl> else <nl> status = dlm_get_unlock_actions ( dlm , res , lock , lksb , & actions ); <nl>  <nl> - if ( status != DLM_NORMAL && status != DLM_CANCELGRANT ) <nl> + if ( status != DLM_NORMAL && ( status != DLM_CANCELGRANT || ! master_node )) <nl> goto leave ; <nl>  <nl> /* By now this has been masked out of cancel requests . */
static void isp1760_hub_descriptor ( struct isp1760_hcd * priv , <nl> int ports = HCS_N_PORTS ( priv -> hcs_params ); <nl> u16 temp ; <nl>  <nl> - desc -> bDescriptorType = 0x29 ; <nl> + desc -> bDescriptorType = USB_DT_HUB ; <nl> /* priv 1 . 0 , 2 . 3 . 9 says 20ms max */ <nl> desc -> bPwrOn2PwrGood = 10 ; <nl> desc -> bHubContrCurrent = 0 ;
static int radeon_uvd_cs_reloc ( struct radeon_cs_parser * p , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (( start >> 28 ) != ( end >> 28 )) { <nl> + if (( start >> 28 ) != (( end - 1 ) >> 28 )) { <nl> DRM_ERROR (" reloc % LX -% LX crossing 256MB boundary !\ n ", <nl> start , end ); <nl> return - EINVAL ;
struct page * kmap_to_page ( void * vaddr ) <nl>  <nl> return virt_to_page ( addr ); <nl> } <nl> + EXPORT_SYMBOL ( kmap_to_page ); <nl>  <nl> static void flush_all_zero_pkmaps ( void ) <nl> {
static int siu_dai_prepare ( struct snd_pcm_substream * substream , <nl> ret = siu_dai_spbstart ( port_info ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl> + } else { <nl> + ret = 0 ; <nl> } <nl>  <nl> port_info -> play_cap |= self ;
enum { <nl> # define DM_DEV_SET_GEOMETRY _IOWR ( DM_IOCTL , DM_DEV_SET_GEOMETRY_CMD , struct dm_ioctl ) <nl>  <nl> # define DM_VERSION_MAJOR 4 <nl> -# define DM_VERSION_MINOR 31 <nl> +# define DM_VERSION_MINOR 32 <nl> # define DM_VERSION_PATCHLEVEL 0 <nl> -# define DM_VERSION_EXTRA "- ioctl ( 2015 - 3 - 12 )" <nl> +# define DM_VERSION_EXTRA "- ioctl ( 2015 - 6 - 26 )" <nl>  <nl> /* Status bits */ <nl> # define DM_READONLY_FLAG ( 1 << 0 ) /* In / Out */
static int qat_alg_sgl_to_bufl ( struct qat_crypto_instance * inst , <nl> goto err ; <nl>  <nl> for_each_sg ( assoc , sg , assoc_n , i ) { <nl> + if (! sg -> length ) <nl> + continue ; <nl> bufl -> bufers [ bufs ]. addr = dma_map_single ( dev , <nl> sg_virt ( sg ), <nl> sg -> length ,
static void i9xx_enable_pll ( struct intel_crtc * crtc ) <nl> I915_READ ( DPLL (! crtc -> pipe )) | DPLL_DVO_2X_MODE ); <nl> } <nl>  <nl> + I915_WRITE ( reg , dpll ); <nl> + <nl> /* Wait for the clocks to stabilize . */ <nl> POSTING_READ ( reg ); <nl> udelay ( 150 );
u32 bond_xmit_hash ( struct bonding * bond , struct sk_buff * skb ) <nl> struct flow_keys flow ; <nl> u32 hash ; <nl>  <nl> + if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_ENCAP34 && <nl> + skb -> l4_hash ) <nl> + return skb -> hash ; <nl> + <nl> if ( bond -> params . xmit_policy == BOND_XMIT_POLICY_LAYER2 || <nl> ! bond_flow_dissect ( bond , skb , & flow )) <nl> return bond_eth_hash ( skb );
nfqnl_build_packet_message ( struct nfqnl_instance * queue , <nl> struct net_device * indev ; <nl> struct net_device * outdev ; <nl>  <nl> - size = NLMSG_ALIGN ( sizeof ( struct nfgenmsg )) <nl> + size = NLMSG_SPACE ( sizeof ( struct nfgenmsg )) <nl> + nla_total_size ( sizeof ( struct nfqnl_msg_packet_hdr )) <nl> + nla_total_size ( sizeof ( u_int32_t )) /* ifindex */ <nl> + nla_total_size ( sizeof ( u_int32_t )) /* ifindex */
static size_t jffs2_trusted_listxattr ( struct dentry * dentry , char * list , <nl> { <nl> size_t retlen = XATTR_TRUSTED_PREFIX_LEN + name_len + 1 ; <nl>  <nl> + if (! capable ( CAP_SYS_ADMIN )) <nl> + return 0 ; <nl> + <nl> if ( list && retlen <= list_size ) { <nl> strcpy ( list , XATTR_TRUSTED_PREFIX ); <nl> strcpy ( list + XATTR_TRUSTED_PREFIX_LEN , name );
static int synaptics_board_id ( struct psmouse * psmouse ) <nl> struct synaptics_data * priv = psmouse -> private ; <nl> unsigned char bid [ 3 ]; <nl>  <nl> + /* firmwares prior 7 . 5 have no board_id encoded */ <nl> + if ( SYN_ID_FULL ( priv -> identity ) < 0x705 ) <nl> + return 0 ; <nl> + <nl> if ( synaptics_send_cmd ( psmouse , SYN_QUE_MODES , bid )) <nl> return - 1 ; <nl> priv -> board_id = (( bid [ 0 ] & 0xfc ) << 6 ) | bid [ 1 ];
static const char * intel_pt_err_msgs [] = { <nl>  <nl> int intel_pt__strerror ( int code , char * buf , size_t buflen ) <nl> { <nl> - if ( code < 1 || code > INTEL_PT_ERR_MAX ) <nl> + if ( code < 1 || code >= INTEL_PT_ERR_MAX ) <nl> code = INTEL_PT_ERR_UNK ; <nl> strlcpy ( buf , intel_pt_err_msgs [ code ], buflen ); <nl> return 0 ;
static void conf_message_callback ( const char * fmt , va_list ap ) <nl>  <nl> static void show_help ( struct menu * menu ) <nl> { <nl> - struct gstr help = str_new (); <nl> + struct gstr help ; <nl> + <nl> + if (! menu ) <nl> + return ; <nl> + <nl> + help = str_new (); <nl> menu_get_ext_help ( menu , & help ); <nl> show_scroll_win ( main_window , _ ( menu_get_prompt ( menu )), str_get (& help )); <nl> str_free (& help );
retry : <nl> return 0 ; <nl>  <nl> n_group = ext4_get_group_number ( sb , n_blocks_count - 1 ); <nl> + if ( n_group > ( 0xFFFFFFFFUL / EXT4_INODES_PER_GROUP ( sb ))) { <nl> + ext4_warning ( sb , " resize would cause inodes_count overflow "); <nl> + return - EINVAL ; <nl> + } <nl> ext4_get_group_no_and_offset ( sb , o_blocks_count - 1 , & o_group , & offset ); <nl>  <nl> n_desc_blocks = num_desc_blocks ( sb , n_group + 1 );
int kvmppc_handle_exit ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> } <nl> break ; <nl>  <nl> + case BOOKE_INTERRUPT_FP_UNAVAIL : <nl> + kvmppc_queue_exception ( vcpu , exit_nr ); <nl> + r = RESUME_GUEST ; <nl> + break ; <nl> + <nl> case BOOKE_INTERRUPT_DATA_STORAGE : <nl> vcpu -> arch . dear = vcpu -> arch . fault_dear ; <nl> vcpu -> arch . esr = vcpu -> arch . fault_esr ;
static struct eg20t_port * pch_uart_init_port ( struct pci_dev * pdev , <nl> priv -> port . line = num ++; <nl> priv -> trigger = PCH_UART_HAL_TRIGGER_M ; <nl>  <nl> + spin_lock_init (& priv -> port . lock ); <nl> + <nl> pci_set_drvdata ( pdev , priv ); <nl> pch_uart_hal_request ( pdev , fifosize , base_baud ); <nl> 
void btrfs_apply_pending_changes ( struct btrfs_fs_info * fs_info ) <nl> unsigned long prev ; <nl> unsigned long bit ; <nl>  <nl> - prev = cmpxchg (& fs_info -> pending_changes , 0 , 0 ); <nl> + prev = xchg (& fs_info -> pending_changes , 0 ); <nl> if (! prev ) <nl> return ; <nl> 
static void udf_sb_free_partitions ( struct super_block * sb ) <nl> { <nl> struct udf_sb_info * sbi = UDF_SB ( sb ); <nl> int i ; <nl> - <nl> + if ( sbi -> s_partmaps == NULL ) <nl> + return ; <nl> for ( i = 0 ; i < sbi -> s_partitions ; i ++) <nl> udf_free_partition (& sbi -> s_partmaps [ i ]); <nl> kfree ( sbi -> s_partmaps );
static ssize_t stm_char_write ( struct file * file , const char __user * buf , <nl> char * kbuf ; <nl> int err ; <nl>  <nl> + if ( count + 1 > PAGE_SIZE ) <nl> + count = PAGE_SIZE - 1 ; <nl> + <nl> /* <nl> * if no m / c have been assigned to this writer up to this <nl> * point , use " default " policy entry
void ath9k_btcoex_stop_gen_timer ( struct ath_softc * sc ) <nl> { <nl> struct ath_btcoex * btcoex = & sc -> btcoex ; <nl>  <nl> - ath9k_gen_timer_stop ( sc -> sc_ah , btcoex -> no_stomp_timer ); <nl> + if ( btcoex -> hw_timer_enabled ) <nl> + ath9k_gen_timer_stop ( sc -> sc_ah , btcoex -> no_stomp_timer ); <nl> } <nl>  <nl> u16 ath9k_btcoex_aggr_limit ( struct ath_softc * sc , u32 max_4ms_framelen )
void btrfs_evict_inode ( struct inode * inode ) <nl> btrfs_orphan_del ( NULL , inode ); <nl> goto no_delete ; <nl> } <nl> + rsv -> size = min_size ; <nl>  <nl> btrfs_i_size_write ( inode , 0 ); <nl>  <nl> static int btrfs_truncate ( struct inode * inode ) <nl> rsv = btrfs_alloc_block_rsv ( root ); <nl> if (! rsv ) <nl> return - ENOMEM ; <nl> + rsv -> size = min_size ; <nl>  <nl> /* <nl> * 1 for the truncate slack space
static int set_wep_key ( struct airo_info * ai , u16 index , const char * key , <nl> WepKeyRid wkr ; <nl> int rc ; <nl>  <nl> - WARN_ON ( keylen == 0 ); <nl> + if ( WARN_ON ( keylen == 0 )) <nl> + return - 1 ; <nl>  <nl> memset (& wkr , 0 , sizeof ( wkr )); <nl> wkr . len = cpu_to_le16 ( sizeof ( wkr ));
xfs_destroy_ioend ( <nl> } <nl>  <nl> if ( ioend -> io_iocb ) { <nl> + inode_dio_done ( ioend -> io_inode ); <nl> if ( ioend -> io_isasync ) { <nl> aio_complete ( ioend -> io_iocb , ioend -> io_error ? <nl> ioend -> io_error : ioend -> io_result , 0 ); <nl> } <nl> - inode_dio_done ( ioend -> io_inode ); <nl> } <nl>  <nl> mempool_free ( ioend , xfs_ioend_pool );
static int llc_ui_create ( struct net * net , struct socket * sock , int protocol ) <nl> struct sock * sk ; <nl> int rc = - ESOCKTNOSUPPORT ; <nl>  <nl> + if (! capable ( CAP_NET_RAW )) <nl> + return - EPERM ; <nl> + <nl> if ( net != & init_net ) <nl> return - EAFNOSUPPORT ; <nl> 
int dm_suspend ( struct mapped_device * md , unsigned suspend_flags ) <nl> * requests are being added to md -> deferred list . <nl> */ <nl>  <nl> - dm_table_postsuspend_targets ( map ); <nl> - <nl> set_bit ( DMF_SUSPENDED , & md -> flags ); <nl>  <nl> + dm_table_postsuspend_targets ( map ); <nl> + <nl> out : <nl> dm_table_put ( map ); <nl> 
static int __init scx200_create_isa ( const char * text , unsigned long base , <nl> if ( iface == NULL ) <nl> return - ENOMEM ; <nl>  <nl> - if ( request_region ( base , 8 , iface -> adapter . name ) == 0 ) { <nl> + if (! request_region ( base , 8 , iface -> adapter . name )) { <nl> printk ( KERN_ERR NAME ": can ' t allocate io 0x % lx - 0x % lx \ n ", <nl> base , base + 8 - 1 ); <nl> rc = - EBUSY ;
static void line6_destruct ( struct snd_card * card ) <nl> /* Free buffer memory first . We cannot depend on the existence of private <nl> * data from the ( podhd ) module , it may be gone already during this call <nl> */ <nl> - if ( line6 -> buffer_message ) <nl> - kfree ( line6 -> buffer_message ); <nl> + kfree ( line6 -> buffer_message ); <nl>  <nl> kfree ( line6 -> buffer_listen ); <nl> 
static int wl12xx_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> struct wl12xx * wl = hw -> priv ; <nl> int ret ; <nl>  <nl> + mutex_lock (& wl -> mutex ); <nl> + <nl> ret = wl12xx_acx_rts_threshold ( wl , ( u16 ) value ); <nl>  <nl> if ( ret < 0 ) <nl> wl12xx_warning (" wl12xx_op_set_rts_threshold failed : % d ", ret ); <nl>  <nl> + mutex_unlock (& wl -> mutex ); <nl> + <nl> return ret ; <nl> } <nl> 
struct vp_config_entry_24xx { <nl> uint16_t id ; <nl> uint16_t reserved_4 ; <nl> uint16_t hopct ; <nl> - uint8_t reserved_5 ; <nl> + uint8_t reserved_5 [ 2 ]; <nl> }; <nl>  <nl> # define VP_RPT_ID_IOCB_TYPE 0x32 /* Report ID Acquisition entry . */
static ktime_t tick_nohz_stop_sched_tick ( struct tick_sched * ts , <nl> */ <nl> if ( delta == 0 ) { <nl> tick_nohz_restart ( ts , now ); <nl> + /* <nl> + * Make sure next tick stop doesn ' t get fooled by past <nl> + * clock deadline <nl> + */ <nl> + ts -> next_tick = 0 ; <nl> goto out ; <nl> } <nl> }
int btrfs_kobj_rm_device ( struct btrfs_fs_info * fs_info , <nl> if (! fs_info -> device_dir_kobj ) <nl> return - EINVAL ; <nl>  <nl> - if ( one_device ) { <nl> + if ( one_device && one_device -> bdev ) { <nl> disk = one_device -> bdev -> bd_part ; <nl> disk_kobj = & part_to_dev ( disk )-> kobj ; <nl> 
i915_gem_wait_ioctl ( struct drm_device * dev , void * data , struct drm_file * file ) <nl> u32 seqno = 0 ; <nl> int ret = 0 ; <nl>  <nl> + if ( args -> flags != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = i915_mutex_lock_interruptible ( dev ); <nl> if ( ret ) <nl> return ret ;
static int br_afspec ( struct net_bridge * br , <nl> if ( nla_len ( attr ) != sizeof ( struct bridge_vlan_info )) <nl> return - EINVAL ; <nl> vinfo = nla_data ( attr ); <nl> + if (! vinfo -> vid || vinfo -> vid >= VLAN_VID_MASK ) <nl> + return - EINVAL ; <nl> if ( vinfo -> flags & BRIDGE_VLAN_INFO_RANGE_BEGIN ) { <nl> if ( vinfo_start ) <nl> return - EINVAL ;
static void wacom_i4_parse_pen_report ( struct wacom_data * wdata , <nl>  <nl> switch ( data [ 1 ]) { <nl> case 0x80 : /* Out of proximity report */ <nl> - wdata -> tool = 0 ; <nl> input_report_key ( input , BTN_TOUCH , 0 ); <nl> input_report_abs ( input , ABS_PRESSURE , 0 ); <nl> input_report_key ( input , wdata -> tool , 0 ); <nl> + wdata -> tool = 0 ; <nl> input_sync ( input ); <nl> break ; <nl> case 0xC2 : /* Tool report */
int dw_mci_probe ( struct dw_mci * host ) <nl> } <nl> } <nl>  <nl> - if ( host -> pdata -> num_slots > 1 ) { <nl> + if ( host -> pdata -> num_slots < 1 ) { <nl> dev_err ( host -> dev , <nl> " Platform data must supply num_slots .\ n "); <nl> return - ENODEV ;
static int enum_fmt ( void * priv , struct v4l2_fmtdesc * f , <nl> fmt = & formats [ i ]; <nl> strlcpy ( f -> description , fmt -> name , sizeof ( f -> description )); <nl> f -> pixelformat = fmt -> fourcc ; <nl> + if (! coda_format_is_yuv ( fmt -> fourcc )) <nl> + f -> flags |= V4L2_FMT_FLAG_COMPRESSED ; <nl> return 0 ; <nl> } <nl> 
static struct sock * tcp_fastopen_create_child ( struct sock * sk , <nl> tcp_fastopen_add_skb ( child , skb ); <nl>  <nl> tcp_rsk ( req )-> rcv_nxt = tp -> rcv_nxt ; <nl> + tp -> rcv_wup = tp -> rcv_nxt ; <nl> /* tcp_conn_request () is sending the SYNACK , <nl> * and queues the child into listener accept queue . <nl> */
static void hidinput_configure_usage ( struct hid_input * hidinput , struct hid_fiel <nl> break ; <nl>  <nl> case HID_UP_BUTTON : <nl> - code = (( usage -> hid - 1 ) & 0xf ); <nl> + code = (( usage -> hid - 1 ) & HID_USAGE ); <nl>  <nl> switch ( field -> application ) { <nl> case HID_GD_MOUSE :
int tulip_refill_rx ( struct net_device * dev ) <nl>  <nl> mapping = pci_map_single ( tp -> pdev , skb -> data , PKT_BUF_SZ , <nl> PCI_DMA_FROMDEVICE ); <nl> + if ( dma_mapping_error (& tp -> pdev -> dev , mapping )) { <nl> + dev_kfree_skb ( skb ); <nl> + tp -> rx_buffers [ entry ]. skb = NULL ; <nl> + break ; <nl> + } <nl> + <nl> tp -> rx_buffers [ entry ]. mapping = mapping ; <nl>  <nl> tp -> rx_ring [ entry ]. buffer1 = cpu_to_le32 ( mapping );
ext4_move_extents ( struct file * o_filp , struct file * d_filp , <nl> orig_inode -> i_ino , donor_inode -> i_ino ); <nl> return - EINVAL ; <nl> } <nl> - <nl> + /* TODO : This is non obvious task to swap blocks for inodes with full <nl> + jornaling enabled */ <nl> + if ( ext4_should_journal_data ( orig_inode ) || <nl> + ext4_should_journal_data ( donor_inode )) { <nl> + return - EINVAL ; <nl> + } <nl> /* Protect orig and donor inodes against a truncate */ <nl> mext_inode_double_lock ( orig_inode , donor_inode ); <nl> 
static int jfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> */ <nl> rc = dtSearch ( new_dir , & new_dname , & ino , & btstack , JFS_LOOKUP ); <nl> if (! rc ) { <nl> - if (( new_ip == 0 ) || ( ino != new_ip -> i_ino )) { <nl> + if ((! new_ip ) || ( ino != new_ip -> i_ino )) { <nl> rc = - ESTALE ; <nl> goto out3 ; <nl> }
static int lzo_compress_pages ( struct list_head * ws , <nl> } <nl>  <nl> /* we ' re making it bigger , give up */ <nl> - if ( tot_in > 8192 && tot_in < tot_out ) <nl> + if ( tot_in > 8192 && tot_in < tot_out ) { <nl> + ret = - 1 ; <nl> goto out ; <nl> + } <nl>  <nl> /* we ' re all done */ <nl> if ( tot_in >= len )
void eth_header_cache_update ( struct hh_cache * hh , struct net_device * dev , <nl> static int eth_mac_addr ( struct net_device * dev , void * p ) <nl> { <nl> struct sockaddr * addr = p ; <nl> + <nl> if ( netif_running ( dev )) <nl> return - EBUSY ; <nl> + if (! is_valid_ether_addr ( addr -> sa_data )) <nl> + return - EADDRNOTAVAIL ; <nl> memcpy ( dev -> dev_addr , addr -> sa_data , dev -> addr_len ); <nl> return 0 ; <nl> }
static int knav_setup_queue_range ( struct knav_device * kdev , <nl>  <nl> range -> num_irqs ++; <nl>  <nl> - if ( oirq . args_count == 3 ) <nl> + if ( IS_ENABLED ( CONFIG_SMP ) && oirq . args_count == 3 ) <nl> range -> irqs [ i ]. cpu_map = <nl> ( oirq . args [ 2 ] & 0x0000ff00 ) >> 8 ; <nl> }
static int find_first_block_group ( struct btrfs_root * root , <nl> } else { <nl> ret = 0 ; <nl> } <nl> + free_extent_map ( em ); <nl> goto out ; <nl> } <nl> path -> slots [ 0 ]++;
static int skl_tplg_mixer_dapm_post_pmd_event ( struct snd_soc_dapm_widget * w , <nl> int ret = 0 ; <nl>  <nl> skl_tplg_free_pipe_mcps ( skl , mconfig ); <nl> + skl_tplg_free_pipe_mem ( skl , mconfig ); <nl>  <nl> list_for_each_entry ( w_module , & s_pipe -> w_list , node ) { <nl> dst_module = w_module -> w -> priv ; <nl> static int skl_tplg_mixer_dapm_post_pmd_event ( struct snd_soc_dapm_widget * w , <nl> } <nl>  <nl> ret = skl_delete_pipe ( ctx , mconfig -> pipe ); <nl> - skl_tplg_free_pipe_mem ( skl , mconfig ); <nl>  <nl> return ret ; <nl> }
int __init mxc_register_gpios ( void ) <nl> # ifdef CONFIG_MACH_MX21 <nl> static struct resource mx21_usbhc_resources [] = { <nl> { <nl> - . start = MX21_BASE_ADDR , <nl> - . end = MX21_BASE_ADDR + 0x1FFF , <nl> + . start = MX21_USBOTG_BASE_ADDR , <nl> + . end = MX21_USBOTG_BASE_ADDR + SZ_8K - 1 , <nl> . flags = IORESOURCE_MEM , <nl> }, <nl> {
static s32 igb_get_invariants_82575 ( struct e1000_hw * hw ) <nl> * for setting word_size . <nl> */ <nl> size += NVM_WORD_SIZE_BASE_SHIFT ; <nl> + <nl> + /* EEPROM access above 16k is unsupported */ <nl> + if ( size > 14 ) <nl> + size = 14 ; <nl> nvm -> word_size = 1 << size ; <nl>  <nl> /* setup PHY parameters */
static int p9_virtio_probe ( struct virtio_device * vdev ) <nl> int err ; <nl> struct virtio_chan * chan ; <nl>  <nl> + if (! vdev -> config -> get ) { <nl> + dev_err (& vdev -> dev , "% s failure : config access disabled \ n ", <nl> + __func__ ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> chan = kmalloc ( sizeof ( struct virtio_chan ), GFP_KERNEL ); <nl> if (! chan ) { <nl> pr_err (" Failed to allocate virtio 9P channel \ n ");
static struct page * alloc_misplaced_dst_page ( struct page * page , <nl> __GFP_NOMEMALLOC | __GFP_NORETRY | <nl> __GFP_NOWARN ) & <nl> ~ GFP_IOFS , 0 ); <nl> + if ( newpage ) <nl> + page_xchg_last_nid ( newpage , page_last_nid ( page )); <nl> + <nl> return newpage ; <nl> } <nl> 
static struct packet_type ip_packet_type __read_mostly = { <nl>  <nl> static int __init inet_init ( void ) <nl> { <nl> - struct sk_buff * dummy_skb ; <nl> struct inet_protosw * q ; <nl> struct list_head * r ; <nl> int rc = - EINVAL ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct inet_skb_parm ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct inet_skb_parm ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> sysctl_local_reserved_ports = kzalloc ( 65536 / 8 , GFP_KERNEL ); <nl> if (! sysctl_local_reserved_ports )
static u8 parse_subframe ( struct sk_buff * skb , <nl> # else <nl> /* Allocate new skb for releasing to upper layer */ <nl> sub_skb = dev_alloc_skb ( nSubframe_Length + 12 ); <nl> + if (! sub_skb ) <nl> + return 0 ; <nl> skb_reserve ( sub_skb , 12 ); <nl> data_ptr = ( u8 *) skb_put ( sub_skb , nSubframe_Length ); <nl> memcpy ( data_ptr , skb -> data , nSubframe_Length );
void __attribute__ (( weak )) bust_spinlocks ( int yes ) <nl> { <nl> if ( yes ) { <nl> - oops_in_progress = 1 ; <nl> + ++ oops_in_progress ; <nl> } else { <nl> # ifdef CONFIG_VT <nl> unblank_screen (); <nl> # endif <nl> - oops_in_progress = 0 ; <nl> - wake_up_klogd (); <nl> + if (-- oops_in_progress == 0 ) <nl> + wake_up_klogd (); <nl> } <nl> } <nl> 
static int ads7846_remove ( struct spi_device * spi ) <nl>  <nl> ads784x_hwmon_unregister ( spi , ts ); <nl>  <nl> - regulator_disable ( ts -> reg ); <nl> regulator_put ( ts -> reg ); <nl>  <nl> if (! ts -> get_pendown_state ) {
print_graph_entry ( struct ftrace_graph_ent_entry * field , struct trace_seq * s , <nl>  <nl> /* Proc */ <nl> if ( tracer_flags . val & TRACE_GRAPH_PRINT_PROC ) { <nl> - ret = print_graph_proc ( s , pid ); <nl> + ret = print_graph_proc ( s , ent -> pid ); <nl> if ( ret == TRACE_TYPE_PARTIAL_LINE ) <nl> return TRACE_TYPE_PARTIAL_LINE ; <nl> 
static void __exit cleanup_nsc ( void ) <nl> if ( pdev ) { <nl> tpm_nsc_remove (& pdev -> dev ); <nl> platform_device_unregister ( pdev ); <nl> - kfree ( pdev ); <nl> - pdev = NULL ; <nl> } <nl>  <nl> platform_driver_unregister (& nsc_drv );
sid_to_id ( struct cifs_sb_info * cifs_sb , struct cifs_sid * psid , <nl> * probably a safe assumption but might be better to check based on <nl> * sidtype . <nl> */ <nl> + BUILD_BUG_ON ( sizeof ( uid_t ) != sizeof ( gid_t )); <nl> if ( sidkey -> datalen != sizeof ( uid_t )) { <nl> rc = - EIO ; <nl> cFYI ( 1 , "% s : Downcall contained malformed key "
static int old_capi_manufacturer ( unsigned int cmd , void __user * data ) <nl> return - EFAULT ; <nl> } <nl> card = get_capi_ctr_by_nr ( ldef . contr ); <nl> + if (! card ) <nl> + return - EINVAL ; <nl> card = capi_ctr_get ( card ); <nl> if (! card ) <nl> return - ESRCH ;
static void set_times ( struct tca6507_chip * tca , int bank ) <nl> int result ; <nl>  <nl> result = choose_times ( tca -> bank [ bank ]. ontime , & c1 , & c2 ); <nl> + if ( result < 0 ) <nl> + return ; <nl> dev_dbg (& tca -> client -> dev , <nl> " Chose on times % d (% d ) % d (% d ) for % dms \ n ", <nl> c1 , time_codes [ c1 ],
static void dlm_run_purge_list ( struct dlm_ctxt * dlm , <nl> /* This may drop and reacquire the dlm spinlock if it <nl> * has to do migration . */ <nl> mlog ( 0 , " calling dlm_purge_lockres !\ n "); <nl> + dlm_lockres_get ( lockres ); <nl> if ( dlm_purge_lockres ( dlm , lockres )) <nl> BUG (); <nl> + dlm_lockres_put ( lockres ); <nl> mlog ( 0 , " DONE calling dlm_purge_lockres !\ n "); <nl>  <nl> /* Avoid adding any scheduling latencies */
void mdfld_dbi_dsr_exit ( struct drm_device * dev ) <nl> struct drm_psb_private * dev_priv = dev -> dev_private ; <nl> struct mdfld_dbi_dsr_info * dsr_info = dev_priv -> dbi_dsr_info ; <nl>  <nl> - if (! dsr_info ) { <nl> + if ( dsr_info ) { <nl> del_timer_sync (& dsr_info -> dsr_timer ); <nl> kfree ( dsr_info ); <nl> dev_priv -> dbi_dsr_info = NULL ;
# include < linux / version . h > <nl>  <nl> /* Simplified build - specific string for starting entropy . */ <nl> - static const char * build_str = UTS_RELEASE " (" LINUX_COMPILE_BY "@" <nl> + static const char build_str [] = UTS_RELEASE " (" LINUX_COMPILE_BY "@" <nl> LINUX_COMPILE_HOST ") (" LINUX_COMPILER ") " UTS_VERSION ; <nl>  <nl> # define I8254_PORT_CONTROL 0x43
static int ecryptfs_setattr ( struct dentry * dentry , struct iattr * ia ) <nl> } <nl> } <nl> mutex_unlock (& crypt_stat -> cs_mutex ); <nl> + if ( S_ISREG ( inode -> i_mode )) { <nl> + rc = filemap_write_and_wait ( inode -> i_mapping ); <nl> + if ( rc ) <nl> + goto out ; <nl> + fsstack_copy_attr_all ( inode , lower_inode ); <nl> + } <nl> memcpy (& lower_ia , ia , sizeof ( lower_ia )); <nl> if ( ia -> ia_valid & ATTR_FILE ) <nl> lower_ia . ia_file = ecryptfs_file_to_lower ( ia -> ia_file );
static int __devinit sdhci_esdhc_imx_probe ( struct platform_device * pdev ) <nl> clk_prepare_enable ( clk ); <nl> pltfm_host -> clk = clk ; <nl>  <nl> - if (! is_imx25_esdhc ( imx_data )) <nl> - host -> quirks |= SDHCI_QUIRK_BROKEN_TIMEOUT_VAL ; <nl> + host -> quirks |= SDHCI_QUIRK_BROKEN_TIMEOUT_VAL ; <nl>  <nl> if ( is_imx25_esdhc ( imx_data ) || is_imx35_esdhc ( imx_data )) <nl> /* Fix errata ENGcm07207 present on i . MX25 and i . MX35 */
static int snd_hdspm_playback_open ( struct snd_pcm_substream * substream ) <nl> snd_pcm_hw_constraint_minmax ( runtime , <nl> SNDRV_PCM_HW_PARAM_PERIOD_SIZE , <nl> 64 , 8192 ); <nl> + snd_pcm_hw_constraint_minmax ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS , <nl> + 2 , 2 ); <nl> break ; <nl> } <nl>  <nl> static int snd_hdspm_capture_open ( struct snd_pcm_substream * substream ) <nl> snd_pcm_hw_constraint_minmax ( runtime , <nl> SNDRV_PCM_HW_PARAM_PERIOD_SIZE , <nl> 64 , 8192 ); <nl> + snd_pcm_hw_constraint_minmax ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS , <nl> + 2 , 2 ); <nl> break ; <nl> } <nl> 
static int fd_ioctl ( struct block_device * bdev , fmode_t mode , unsigned int cmd , <nl> (( cmd & 0x80 ) && ! capable ( CAP_SYS_ADMIN ))) <nl> return - EPERM ; <nl>  <nl> + if ( WARN_ON ( size < 0 || size > sizeof ( inparam ))) <nl> + return - EINVAL ; <nl> + <nl> /* copyin */ <nl> CLEARSTRUCT (& inparam ); <nl> if ( _IOC_DIR ( cmd ) & _IOC_WRITE )
static int __split_vma ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> return 0 ; <nl>  <nl> /* Clean everything up if vma_adjust failed . */ <nl> - new -> vm_ops -> close ( new ); <nl> + if ( new -> vm_ops && new -> vm_ops -> close ) <nl> + new -> vm_ops -> close ( new ); <nl> if ( new -> vm_file ) { <nl> if ( vma -> vm_flags & VM_EXECUTABLE ) <nl> removed_exe_file_vma ( mm );
static bool vgic_its_check_device_id ( struct kvm * kvm , struct vgic_its * its , <nl> & indirect_ptr , sizeof ( indirect_ptr ))) <nl> return false ; <nl>  <nl> + indirect_ptr = le64_to_cpu ( indirect_ptr ); <nl> + <nl> /* check the valid bit of the first level entry */ <nl> if (!( indirect_ptr & BIT_ULL ( 63 ))) <nl> return false ;
int arizona_set_fll ( struct arizona_fll * fll , int source , <nl> if ( ena ) <nl> pm_runtime_put_autosuspend ( arizona -> dev ); <nl>  <nl> + fll -> fref = Fref ; <nl> + fll -> fout = Fout ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int of_fsl_spi_get_chipselects ( struct device * dev ) <nl> return 0 ; <nl> } <nl>  <nl> - pinfo -> gpios = kmalloc ( ngpios * sizeof (* pinfo -> gpios ), GFP_KERNEL ); <nl> + pinfo -> gpios = kmalloc_array ( ngpios , sizeof (* pinfo -> gpios ), <nl> + GFP_KERNEL ); <nl> if (! pinfo -> gpios ) <nl> return - ENOMEM ; <nl> memset ( pinfo -> gpios , - 1 , ngpios * sizeof (* pinfo -> gpios ));
static void uniphier_pctl_pin_dbg_show ( struct pinctrl_dev * pctldev , <nl> case UNIPHIER_PIN_PULL_DOWN : <nl> pull_dir = " DOWN "; <nl> break ; <nl> + case UNIPHIER_PIN_PULL_UP_FIXED : <nl> + pull_dir = " UP ( FIXED )"; <nl> + break ; <nl> + case UNIPHIER_PIN_PULL_DOWN_FIXED : <nl> + pull_dir = " DOWN ( FIXED )"; <nl> + break ; <nl> case UNIPHIER_PIN_PULL_NONE : <nl> pull_dir = " NONE "; <nl> break ;
static int mmci_probe ( struct amba_device * dev , <nl> dev_dbg ( mmc_dev ( mmc ), " clocking block at % u Hz \ n ", mmc -> f_max ); <nl>  <nl> /* Get regulators and the supported OCR mask */ <nl> - mmc_regulator_get_supply ( mmc ); <nl> + ret = mmc_regulator_get_supply ( mmc ); <nl> + if ( ret == - EPROBE_DEFER ) <nl> + goto clk_disable ; <nl> + <nl> if (! mmc -> ocr_avail ) <nl> mmc -> ocr_avail = plat -> ocr_mask ; <nl> else if ( plat -> ocr_mask )
static struct print_arg * make_bprint_args ( char * fmt , void * data , int size , struc <nl> goto process_again ; <nl> case '.': <nl> goto process_again ; <nl> + case ' z ': <nl> + case ' Z ': <nl> + ls = 1 ; <nl> + goto process_again ; <nl> case ' p ': <nl> ls = 1 ; <nl> /* fall through */
static int selinux_setprocattr ( struct task_struct * p , <nl> return error ; <nl>  <nl> /* Obtain a SID for the context , if one was specified . */ <nl> - if ( size && str [ 1 ] && str [ 1 ] != '\ n ') { <nl> + if ( size && str [ 0 ] && str [ 0 ] != '\ n ') { <nl> if ( str [ size - 1 ] == '\ n ') { <nl> str [ size - 1 ] = 0 ; <nl> size --;
static int ath10k_start ( struct ieee80211_hw * hw ) <nl> goto err_core_stop ; <nl> } <nl>  <nl> + ret = ath10k_wmi_pdev_set_param ( ar , <nl> + ar -> wmi . pdev_param -> ani_enable , 1 ); <nl> + if ( ret ) { <nl> + ath10k_warn ( ar , " failed to enable ani by default : % d \ n ", <nl> + ret ); <nl> + goto err_core_stop ; <nl> + } <nl> + <nl> ar -> num_started_vdevs = 0 ; <nl> ath10k_regd_update ( ar ); <nl> 
static int pb0100_start ( struct sd * sd ) <nl>  <nl> intf = usb_ifnum_to_if ( sd -> gspca_dev . dev , sd -> gspca_dev . iface ); <nl> alt = usb_altnum_to_altsetting ( intf , sd -> gspca_dev . alt ); <nl> + if (! alt ) <nl> + return - ENODEV ; <nl> packet_size = le16_to_cpu ( alt -> endpoint [ 0 ]. desc . wMaxPacketSize ); <nl>  <nl> /* If we don ' t have enough bandwidth use a lower framerate */
acpi_db_walk_for_execute ( acpi_handle obj_handle , <nl>  <nl> status = acpi_get_object_info ( obj_handle , & obj_info ); <nl> if ( ACPI_FAILURE ( status )) { <nl> + ACPI_FREE ( pathname ); <nl> return ( status ); <nl> } <nl> 
create_hw_context ( struct drm_device * dev , <nl> return ERR_PTR (- ENOMEM ); <nl> } <nl>  <nl> + if ( INTEL_INFO ( dev )-> gen >= 7 ) { <nl> + ret = i915_gem_object_set_cache_level ( ctx -> obj , <nl> + I915_CACHE_LLC_MLC ); <nl> + if ( ret ) <nl> + goto err_out ; <nl> + } <nl> + <nl> /* The ring associated with the context object is handled by the normal <nl> * object tracking code . We give an initial ring value simple to pass an <nl> * assertion in the context switch code .
static int ml26124_hw_params ( struct snd_pcm_substream * substream , <nl> struct ml26124_priv * priv = snd_soc_codec_get_drvdata ( codec ); <nl> int i = get_coeff ( priv -> mclk , params_rate ( hw_params )); <nl>  <nl> + if ( i < 0 ) <nl> + return i ; <nl> priv -> substream = substream ; <nl> priv -> rate = params_rate ( hw_params ); <nl> 
void radeon_compute_pll_avivo ( struct radeon_pll * pll , <nl> ref_div_min = pll -> reference_div ; <nl> else <nl> ref_div_min = pll -> min_ref_div ; <nl> - ref_div_max = pll -> max_ref_div ; <nl> + <nl> + if ( pll -> flags & RADEON_PLL_USE_FRAC_FB_DIV && <nl> + pll -> flags & RADEON_PLL_USE_REF_DIV ) <nl> + ref_div_max = pll -> reference_div ; <nl> + else <nl> + ref_div_max = pll -> max_ref_div ; <nl>  <nl> /* determine allowed post divider range */ <nl> if ( pll -> flags & RADEON_PLL_USE_POST_DIV ) {
int cfg80211_mgd_wext_connect ( struct cfg80211_registered_device * rdev , <nl> if ( wdev -> wext . keys ) { <nl> wdev -> wext . keys -> def = wdev -> wext . default_key ; <nl> wdev -> wext . keys -> defmgmt = wdev -> wext . default_mgmt_key ; <nl> - wdev -> wext . connect . privacy = true ; <nl> + if ( wdev -> wext . default_key != - 1 ) <nl> + wdev -> wext . connect . privacy = true ; <nl> } <nl>  <nl> if (! wdev -> wext . connect . ssid_len )
static void cleanup_single_sta ( struct sta_info * sta ) <nl> * directly by station destruction . <nl> */ <nl> for ( i = 0 ; i < IEEE80211_NUM_TIDS ; i ++) { <nl> + kfree ( sta -> ampdu_mlme . tid_start_tx [ i ]); <nl> tid_tx = rcu_dereference_raw ( sta -> ampdu_mlme . tid_tx [ i ]); <nl> if (! tid_tx ) <nl> continue ;
void btrfs_cleanup_one_transaction ( struct btrfs_transaction * cur_trans , <nl>  <nl> btrfs_destroy_marked_extents ( root , & cur_trans -> dirty_pages , <nl> EXTENT_DIRTY ); <nl> + btrfs_destroy_pinned_extent ( root , <nl> + root -> fs_info -> pinned_extents ); <nl>  <nl> /* <nl> memset ( cur_trans , 0 , sizeof (* cur_trans ));
unsigned long acpi_realmode_flags ; <nl> static unsigned long acpi_realmode ; <nl>  <nl> # if defined ( CONFIG_SMP ) && defined ( CONFIG_64BIT ) <nl> - static char temp_stack [ 10240 ]; <nl> + static char temp_stack [ 4096 ]; <nl> # endif <nl>  <nl> /**
static void sti_crtc_atomic_flush ( struct drm_crtc * crtc , <nl>  <nl> switch ( plane -> status ) { <nl> case STI_PLANE_UPDATED : <nl> + /* ignore update for other CRTC */ <nl> + if ( p -> state -> crtc != crtc ) <nl> + continue ; <nl> + <nl> /* update planes tag as updated */ <nl> DRM_DEBUG_DRIVER (" update plane % s \ n ", <nl> sti_plane_to_str ( plane ));
static void radeon_compute_pll_legacy ( struct radeon_pll * pll , <nl> max_fractional_feed_div = pll -> max_frac_feedback_div ; <nl> } <nl>  <nl> - for ( post_div = min_post_div ; post_div <= max_post_div ; ++ post_div ) { <nl> + for ( post_div = max_post_div ; post_div >= min_post_div ; -- post_div ) { <nl> uint32_t ref_div ; <nl>  <nl> if (( pll -> flags & RADEON_PLL_NO_ODD_POST_DIV ) && ( post_div & 1 ))
static void mce_async_callback ( struct urb * urb , struct pt_regs * regs ) <nl> mceusb_dev_printdata ( ir , urb -> transfer_buffer , 0 , len , true ); <nl> } <nl>  <nl> + /* the transfer buffer and urb were allocated in mce_request_packet */ <nl> + kfree ( urb -> transfer_buffer ); <nl> + usb_free_urb ( urb ); <nl> } <nl>  <nl> /* request incoming or send outgoing usb packet - used to initialize remote */
static void __del_gref ( struct gntalloc_gref * gref ) <nl>  <nl> if (! gnttab_end_foreign_access_ref ( gref -> gref_id , 0 )) <nl> return ; <nl> + <nl> + gnttab_free_grant_reference ( gref -> gref_id ); <nl> } <nl>  <nl> gref_size --;
static int f7188x_gpio_set_single_ended ( struct gpio_chip * chip , <nl> data &= ~ BIT ( offset ); <nl> else <nl> data |= BIT ( offset ); <nl> - superio_outb ( sio -> addr , gpio_data_mode ( bank -> regbase ), data ); <nl> + superio_outb ( sio -> addr , gpio_out_mode ( bank -> regbase ), data ); <nl>  <nl> superio_exit ( sio -> addr ); <nl> return 0 ;
i915_gem_execbuffer2 ( struct drm_device * dev , void * data , <nl> struct drm_i915_gem_exec_object2 * exec2_list = NULL ; <nl> int ret ; <nl>  <nl> - if ( args -> buffer_count < 1 ) { <nl> + if ( args -> buffer_count < 1 || <nl> + args -> buffer_count > UINT_MAX / sizeof (* exec2_list )) { <nl> DRM_DEBUG (" execbuf2 with % d buffers \ n ", args -> buffer_count ); <nl> return - EINVAL ; <nl> }
static int s3c64xx_setparent_clksrc ( struct clk * clk , struct clk * parent ) <nl> clksrc |= src_nr << sclk -> shift ; <nl>  <nl> __raw_writel ( clksrc , S3C_CLK_SRC ); <nl> + <nl> + clk -> parent = parent ; <nl> return 0 ; <nl> } <nl> 
void intel_setup_bios ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl>  <nl> /* Set the Panel Power On / Off timings if uninitialized . */ <nl> - if (( I915_READ ( PP_ON_DELAYS ) == 0 ) && ( I915_READ ( PP_OFF_DELAYS ) == 0 )) { <nl> + if (! HAS_PCH_SPLIT ( dev ) && <nl> + I915_READ ( PP_ON_DELAYS ) == 0 && I915_READ ( PP_OFF_DELAYS ) == 0 ) { <nl> /* Set T2 to 40ms and T5 to 200ms */ <nl> I915_WRITE ( PP_ON_DELAYS , 0x019007d0 ); <nl> 
static void acm_waker ( struct work_struct * waker ) <nl> static int acm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> { <nl> struct acm * acm ; <nl> - int rv = - EINVAL ; <nl> + int rv = - ENODEV ; <nl> int i ; <nl> dbg (" Entering acm_tty_open ."); <nl> 
static void b43_request_firmware ( struct work_struct * work ) <nl> for ( i = 0 ; i < B43_NR_FWTYPES ; i ++) { <nl> errmsg = ctx -> errors [ i ]; <nl> if ( strlen ( errmsg )) <nl> - b43err ( dev -> wl , errmsg ); <nl> + b43err ( dev -> wl , "% s ", errmsg ); <nl> } <nl> b43_print_fw_helptext ( dev -> wl , 1 ); <nl> goto out ;
int __init omap2_clk_provider_init ( struct device_node * parent , int index , <nl> clocks_node_ptr [ index ] = clocks ; <nl>  <nl> io = kzalloc ( sizeof (* io ), GFP_KERNEL ); <nl> + if (! io ) <nl> + return - ENOMEM ; <nl>  <nl> io -> regmap = syscon ; <nl> io -> mem = mem ;
intel_fill_fb_ggtt_view ( struct i915_ggtt_view * view , struct drm_framebuffer * fb , <nl> tile_size = intel_tile_size ( dev_priv ); <nl>  <nl> cpp = drm_format_plane_cpp ( fb -> pixel_format , 0 ); <nl> - tile_width = intel_tile_width ( dev_priv , cpp , fb -> modifier [ 0 ]); <nl> + tile_width = intel_tile_width ( dev_priv , fb -> modifier [ 0 ], cpp ); <nl> tile_height = tile_size / tile_width ; <nl>  <nl> info -> width_pages = DIV_ROUND_UP ( fb -> pitches [ 0 ], tile_width );
_return_of_node_put : <nl> of_node_put ( dev_node ); <nl> _return_dev_set_drvdata : <nl> kfree ( priv -> fixed_link ); <nl> - kfree ( priv ); <nl> dev_set_drvdata ( dev , NULL ); <nl> _return : <nl> return err ;
static int dw_i2c_probe ( struct platform_device * pdev ) <nl> adap = & dev -> adapter ; <nl> i2c_set_adapdata ( adap , dev ); <nl> adap -> owner = THIS_MODULE ; <nl> - adap -> class = I2C_CLASS_HWMON | I2C_CLASS_DEPRECATED ; <nl> + adap -> class = I2C_CLASS_DEPRECATED ; <nl> strlcpy ( adap -> name , " Synopsys DesignWare I2C adapter ", <nl> sizeof ( adap -> name )); <nl> adap -> algo = & i2c_dw_algo ;
static int sa1111_resume ( struct platform_device * dev ) <nl> # define sa1111_resume NULL <nl> # endif <nl>  <nl> - static int sa1111_probe ( struct platform_device * pdev ) <nl> + static int __devinit sa1111_probe ( struct platform_device * pdev ) <nl> { <nl> struct resource * mem ; <nl> int irq ;
static struct intel_iommu * device_to_iommu ( struct device * dev , u8 * bus , u8 * devf <nl> * which we used for the IOMMU lookup . Strictly speaking <nl> * we could do this for all PCI devices ; we only need to <nl> * get the BDF # from the scope table for ACPI matches . */ <nl> - if ( pdev -> is_virtfn ) <nl> + if ( pdev && pdev -> is_virtfn ) <nl> goto got_pdev ; <nl>  <nl> * bus = drhd -> devices [ i ]. bus ;
static long do_ixj_ioctl ( struct file * file_p , unsigned int cmd , unsigned long ar <nl> case IXJCTL_SET_FILTER : <nl> if ( copy_from_user (& jf , argp , sizeof ( jf ))) <nl> retval = - EFAULT ; <nl> - retval = ixj_init_filter ( j , & jf ); <nl> + else <nl> + retval = ixj_init_filter ( j , & jf ); <nl> break ; <nl> case IXJCTL_SET_FILTER_RAW : <nl> if ( copy_from_user (& jfr , argp , sizeof ( jfr )))
static unsigned int bsg_poll ( struct file * file , poll_table * wait ) <nl> spin_lock_irq (& bd -> lock ); <nl> if (! list_empty (& bd -> done_list )) <nl> mask |= POLLIN | POLLRDNORM ; <nl> - if ( bd -> queued_cmds >= bd -> max_queue ) <nl> + if ( bd -> queued_cmds < bd -> max_queue ) <nl> mask |= POLLOUT ; <nl> spin_unlock_irq (& bd -> lock ); <nl> 
static int gfx_v9_0_set_clockgating_state ( void * handle , <nl> { <nl> struct amdgpu_device * adev = ( struct amdgpu_device *) handle ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> switch ( adev -> asic_type ) { <nl> case CHIP_VEGA10 : <nl> gfx_v9_0_update_gfx_clock_gating ( adev ,
static void sky2_restart ( struct work_struct * work ) <nl>  <nl> rtnl_lock (); <nl>  <nl> - napi_disable (& hw -> napi ); <nl> - synchronize_irq ( hw -> pdev -> irq ); <nl> imask = sky2_read32 ( hw , B0_IMSK ); <nl> sky2_write32 ( hw , B0_IMSK , 0 ); <nl> + synchronize_irq ( hw -> pdev -> irq ); <nl> + napi_disable (& hw -> napi ); <nl>  <nl> for ( i = 0 ; i < hw -> ports ; i ++) { <nl> struct net_device * dev = hw -> dev [ i ];
void ath_detach ( struct ath_softc * sc ) <nl>  <nl> ath9k_hw_detach ( sc -> sc_ah ); <nl> ath9k_exit_debug ( sc ); <nl> - ath9k_ps_restore ( sc ); <nl> } <nl>  <nl> static int ath9k_reg_notifier ( struct wiphy * wiphy ,
static void bcm2048_rds_fifo_receive ( struct bcm2048_device * bdev ) <nl> bdev -> rds_info . radio_text , bdev -> fifo_size ); <nl> if ( err != 2 ) { <nl> dev_err (& bdev -> client -> dev , " RDS Read problem \ n "); <nl> + mutex_unlock (& bdev -> mutex ); <nl> return ; <nl> } <nl> 
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> ( rdev -> pdev -> device == 0x6667 )) { <nl> max_sclk = 75000 ; <nl> } <nl> + } else if ( rdev -> family == CHIP_OLAND ) { <nl> + if (( rdev -> pdev -> device == 0x6604 ) && <nl> + ( rdev -> pdev -> subsystem_vendor == 0x1028 ) && <nl> + ( rdev -> pdev -> subsystem_device == 0x066F )) { <nl> + max_sclk = 75000 ; <nl> + } <nl> } <nl>  <nl> if ( rps -> vce_active ) {
static void efx_pci_remove ( struct pci_dev * pci_dev ) <nl> efx_dissociate ( efx ); <nl> dev_close ( efx -> net_dev ); <nl> efx_disable_interrupts ( efx ); <nl> + efx -> state = STATE_UNINIT ; <nl> rtnl_unlock (); <nl>  <nl> if ( efx -> type -> sriov_fini )
static int __devexit gpio_keys_remove ( struct platform_device * pdev ) <nl> } <nl>  <nl> input_unregister_device ( input ); <nl> + kfree ( ddata ); <nl>  <nl> return 0 ; <nl> }
static struct afs_server * afs_alloc_server ( struct afs_cell * cell , <nl>  <nl> memcpy (& server -> addr , addr , sizeof ( struct in_addr )); <nl> server -> addr . s_addr = addr -> s_addr ; <nl> + _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> + } else { <nl> + _leave (" = NULL [ nomem ]"); <nl> } <nl> - <nl> - _leave (" = % p {% d }", server , atomic_read (& server -> usage )); <nl> return server ; <nl> } <nl> 
radeon_add_legacy_encoder ( struct drm_device * dev , uint32_t encoder_id , uint32_t <nl>  <nl> switch ( radeon_encoder -> encoder_id ) { <nl> case ENCODER_OBJECT_ID_INTERNAL_LVDS : <nl> + encoder -> possible_crtcs = 0x1 ; <nl> drm_encoder_init ( dev , encoder , & radeon_legacy_lvds_enc_funcs , DRM_MODE_ENCODER_LVDS ); <nl> drm_encoder_helper_add ( encoder , & radeon_legacy_lvds_helper_funcs ); <nl> if ( rdev -> is_atom_bios )
__ieee80211_get_channel_mode ( struct ieee80211_local * local , <nl> if (! sdata -> u . ap . beacon ) <nl> continue ; <nl> break ; <nl> + case NL80211_IFTYPE_MESH_POINT : <nl> + if (! sdata -> wdev . mesh_id_len ) <nl> + continue ; <nl> + break ; <nl> default : <nl> break ; <nl> }
struct greybus_device * greybus_new_module ( struct device * parent , <nl>  <nl> return gdev ; <nl> error : <nl> + put_device (& gdev -> dev ); <nl> greybus_module_release (& gdev -> dev ); <nl> return NULL ; <nl> }
restore_state : <nl>  <nl> return err ; <nl> } <nl> + EXPORT_SYMBOL ( xfrm_migrate ); <nl> # endif <nl> 
int snd_soc_dapm_stream_event ( struct snd_soc_codec * codec , <nl> } <nl> } <nl> } <nl> - mutex_unlock (& codec -> mutex ); <nl>  <nl> dapm_power_widgets ( codec , event ); <nl> + mutex_unlock (& codec -> mutex ); <nl> dump_dapm ( codec , __func__ ); <nl> return 0 ; <nl> }
nfsd_cross_mnt ( struct svc_rqst * rqstp , struct dentry ** dpp , <nl>  <nl> exp2 = rqst_exp_get_by_name ( rqstp , mnt , mounts ); <nl> if ( IS_ERR ( exp2 )) { <nl> - err = PTR_ERR ( exp2 ); <nl> + if ( PTR_ERR ( exp2 ) != - ENOENT ) <nl> + err = PTR_ERR ( exp2 ); <nl> dput ( mounts ); <nl> mntput ( mnt ); <nl> goto out ;
void ieee80211_tx_status ( struct ieee80211_hw * hw , struct sk_buff * skb ) <nl> if (! netif_running ( sdata -> dev )) <nl> continue ; <nl>  <nl> + if (( sdata -> u . mntr_flags & MONITOR_FLAG_COOK_FRAMES ) && <nl> + !( info -> flags & IEEE80211_TX_CTL_INJECTED ) && <nl> + ( type == IEEE80211_FTYPE_DATA )) <nl> + continue ; <nl> + <nl> if ( prev_dev ) { <nl> skb2 = skb_clone ( skb , GFP_ATOMIC ); <nl> if ( skb2 ) {
journal_t * jbd2_journal_init_dev ( struct block_device * bdev , <nl> __FUNCTION__ ); <nl> kfree ( journal ); <nl> journal = NULL ; <nl> + goto out ; <nl> } <nl> journal -> j_dev = bdev ; <nl> journal -> j_fs_dev = fs_dev ; <nl> journal_t * jbd2_journal_init_dev ( struct block_device * bdev , <nl> J_ASSERT ( bh != NULL ); <nl> journal -> j_sb_buffer = bh ; <nl> journal -> j_superblock = ( journal_superblock_t *) bh -> b_data ; <nl> - <nl> + out : <nl> return journal ; <nl> } <nl> 
cifs_put_tcon ( struct cifsTconInfo * tcon ) <nl> CIFSSMBTDis ( xid , tcon ); <nl> _FreeXid ( xid ); <nl>  <nl> - tconInfoFree ( tcon ); <nl> cifs_fscache_release_super_cookie ( tcon ); <nl> + tconInfoFree ( tcon ); <nl> cifs_put_smb_ses ( ses ); <nl> } <nl> 
int snd_es1688_pcm ( struct snd_card * card , struct snd_es1688 * chip , int device ) <nl>  <nl> pcm -> private_data = chip ; <nl> pcm -> info_flags = SNDRV_PCM_INFO_HALF_DUPLEX ; <nl> - sprintf ( pcm -> name , snd_es1688_chip_id ( chip )); <nl> + strcpy ( pcm -> name , snd_es1688_chip_id ( chip )); <nl> chip -> pcm = pcm ; <nl>  <nl> snd_pcm_lib_preallocate_pages_for_all ( pcm , SNDRV_DMA_TYPE_DEV ,
rerun_vcpu : <nl> if ( rc == SIE_INTERCEPT_RERUNVCPU ) <nl> goto rerun_vcpu ; <nl>  <nl> - if ( signal_pending ( current ) && ! rc ) <nl> + if ( signal_pending ( current ) && ! rc ) { <nl> + kvm_run -> exit_reason = KVM_EXIT_INTR ; <nl> rc = - EINTR ; <nl> + } <nl>  <nl> if ( rc == - ENOTSUPP ) { <nl> /* intercept cannot be handled in - kernel , prepare kvm - run */
 <nl> # define DRV_MODULE_NAME " tg3 " <nl> # define PFX DRV_MODULE_NAME ": " <nl> -# define DRV_MODULE_VERSION " 3 . 81 " <nl> -# define DRV_MODULE_RELDATE " September 5 , 2007 " <nl> +# define DRV_MODULE_VERSION " 3 . 82 " <nl> +# define DRV_MODULE_RELDATE " October 5 , 2007 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static int rose_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> skb_copy_datagram_iovec ( skb , 0 , msg -> msg_iov , copied ); <nl>  <nl> if ( srose != NULL ) { <nl> + memset ( srose , 0 , msg -> msg_namelen ); <nl> srose -> srose_family = AF_ROSE ; <nl> srose -> srose_addr = rose -> dest_addr ; <nl> srose -> srose_call = rose -> dest_call ;
static int __open_dso ( struct dso * dso , struct machine * machine ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if (! is_regular_file ( name )) <nl> + return - EINVAL ; <nl> + <nl> fd = do_open ( name ); <nl> free ( name ); <nl> return fd ;
 <nl> bool rtw_IOL_applied ( struct adapter * adapter ) <nl> { <nl> - if ( 1 == adapter -> registrypriv . fw_iol ) <nl> + if ( adapter -> registrypriv . fw_iol == 1 ) <nl> return true ; <nl>  <nl> - if (( 2 == adapter -> registrypriv . fw_iol ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> + if (( adapter -> registrypriv . fw_iol == 2 ) && (! adapter_to_dvobj ( adapter )-> ishighspeed )) <nl> return true ; <nl> return false ; <nl> }
ssize_t ttm_bo_io ( struct ttm_bo_device * bdev , struct file * filp , <nl> return - EFAULT ; <nl>  <nl> driver = bo -> bdev -> driver ; <nl> - if ( unlikely ( driver -> verify_access )) { <nl> + if ( unlikely (! driver -> verify_access )) { <nl> ret = - EPERM ; <nl> goto out_unref ; <nl> }
read_rtc : <nl> } <nl> ds1307 -> nvram -> attr . name = " nvram "; <nl> ds1307 -> nvram -> attr . mode = S_IRUGO | S_IWUSR ; <nl> + sysfs_bin_attr_init ( ds1307 -> nvram ); <nl> ds1307 -> nvram -> read = ds1307_nvram_read , <nl> ds1307 -> nvram -> write = ds1307_nvram_write , <nl> ds1307 -> nvram -> size = chip -> nvram_size ;
static struct pxamci_platform_data magician_mci_info = { <nl>  <nl> static struct pxaohci_platform_data magician_ohci_info = { <nl> . port_mode = PMM_PERPORT_MODE , <nl> - . flags = ENABLE_PORT1 | ENABLE_PORT3 | POWER_CONTROL_LOW , <nl> + /* port1 : CSR Bluetooth , port2 : OTG with UDC */ <nl> + . flags = ENABLE_PORT1 | ENABLE_PORT2 | POWER_CONTROL_LOW , <nl> . power_budget = 0 , <nl> + . power_on_delay = 100 , <nl> }; <nl>  <nl> /*
struct mapped_device * dm_get_from_kobject ( struct kobject * kobj ) <nl> if (& md -> kobj != kobj ) <nl> return NULL ; <nl>  <nl> + if ( test_bit ( DMF_FREEING , & md -> flags ) || <nl> + test_bit ( DMF_DELETING , & md -> flags )) <nl> + return NULL ; <nl> + <nl> dm_get ( md ); <nl> return md ; <nl> }
static __u32 get_ftdi_divisor ( struct tty_struct * tty , <nl> case FT2232H : /* FT2232H chip */ <nl> case FT4232H : /* FT4232H chip */ <nl> case FT232H : /* FT232H chip */ <nl> - if (( baud <= 12000000 ) & ( baud >= 1200 )) { <nl> + if (( baud <= 12000000 ) && ( baud >= 1200 )) { <nl> div_value = ftdi_2232h_baud_to_divisor ( baud ); <nl> } else if ( baud < 1200 ) { <nl> div_value = ftdi_232bm_baud_to_divisor ( baud );
int extcon_register_interest ( struct extcon_specific_cable_nb * obj , <nl>  <nl> obj -> cable_index = extcon_find_cable_index ( obj -> edev , cable_name ); <nl> if ( obj -> cable_index < 0 ) <nl> - return - ENODEV ; <nl> + return obj -> cable_index ; <nl>  <nl> obj -> user_nb = nb ; <nl> 
static void printl ( const char * fmt , ...) <nl>  <nl> kfifo_put ( tcpw . fifo , tbuf , len ); <nl> wake_up (& tcpw . wait ); <nl> -} <nl> +} __attribute__ (( format ( printf , 1 , 2 ))); <nl> + <nl>  <nl> /* <nl> * Hook inserted to be called before each receive packet .
static u32 __seccomp_phase1_filter ( int this_syscall , struct seccomp_data * sd ) <nl>  <nl> switch ( action ) { <nl> case SECCOMP_RET_ERRNO : <nl> - /* Set the low - order 16 - bits as a errno . */ <nl> + /* Set low - order bits as an errno , capped at MAX_ERRNO . */ <nl> + if ( data > MAX_ERRNO ) <nl> + data = MAX_ERRNO ; <nl> syscall_set_return_value ( current , task_pt_regs ( current ), <nl> - data , 0 ); <nl> goto skip ;
static ssize_t numa_node_store ( struct device * dev , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - if (! node_online ( node )) <nl> + if ( node >= MAX_NUMNODES || ! node_online ( node )) <nl> return - EINVAL ; <nl>  <nl> add_taint ( TAINT_FIRMWARE_WORKAROUND , LOCKDEP_STILL_OK );
static int __init parse_memmap_opt ( char * p ) <nl> char * oldp ; <nl> u64 start_at , mem_size ; <nl>  <nl> + if (! p ) <nl> + return - EINVAL ; <nl> + <nl> if (! strcmp ( p , " exactmap ")) { <nl> # ifdef CONFIG_CRASH_DUMP <nl> /*
static size_t log_output ( int facility , int level , enum log_flags lflags , const c <nl> cont_flush (); <nl> } <nl>  <nl> + /* Skip empty continuation lines that couldn ' t be added - they just flush */ <nl> + if (! text_len && ( lflags & LOG_CONT )) <nl> + return 0 ; <nl> + <nl> /* If it doesn ' t end in a newline , try to buffer the current line */ <nl> if (!( lflags & LOG_NEWLINE )) { <nl> if ( cont_add ( facility , level , lflags , text , text_len ))
static void __init s3c24xx_gpiolib_add_chips ( struct samsung_gpio_chip * chip , <nl> struct gpio_chip * gc = & chip -> chip ; <nl>  <nl> for ( i = 0 ; i < nr_chips ; i ++, chip ++) { <nl> + /* skip banks not present on SoC */ <nl> + if ( chip -> chip . base >= S3C_GPIO_END ) <nl> + continue ; <nl> + <nl> if (! chip -> config ) <nl> chip -> config = & s3c24xx_gpiocfg_default ; <nl> if (! chip -> pm )
static void maxiradio_remove ( struct pci_dev * pdev ) <nl> outb ( 0 , dev -> io ); <nl> v4l2_device_unregister ( v4l2_dev ); <nl> release_region ( pci_resource_start ( pdev , 0 ), pci_resource_len ( pdev , 0 )); <nl> + kfree ( dev ); <nl> } <nl>  <nl> static struct pci_device_id maxiradio_pci_tbl [] = {
void jffs2_rtime_exit ( void ); <nl> int jffs2_zlib_init ( void ); <nl> void jffs2_zlib_exit ( void ); <nl> # endif <nl> +# ifdef CONFIG_JFFS2_LZO <nl> + int jffs2_lzo_init ( void ); <nl> + void jffs2_lzo_exit ( void ); <nl> +# endif <nl>  <nl> # endif /* __JFFS2_COMPR_H__ */
static const char * const cw1200_debug_link_id [] = { <nl> " REQ ", <nl> " SOFT ", <nl> " HARD ", <nl> + " RESET ", <nl> + " RESET_REMAP ", <nl> }; <nl>  <nl> static const char * cw1200_debug_mode ( int mode )
int __kvm_set_memory_region ( struct kvm * kvm , <nl> /* Allocate if a slot is being created */ <nl> # ifndef CONFIG_S390 <nl> if ( npages && ! new . rmap ) { <nl> - new . rmap = vmalloc ( npages * sizeof ( struct page *)); <nl> + new . rmap = vmalloc ( npages * sizeof (* new . rmap )); <nl>  <nl> if (! new . rmap ) <nl> goto out_free ;
# define SCLK_MACREF_OUT 106 <nl> # define SCLK_VOP0_PWM 107 <nl> # define SCLK_VOP1_PWM 108 <nl> -# define SCLK_RGA 109 <nl> +# define SCLK_RGA_CORE 109 <nl> # define SCLK_ISP0 110 <nl> # define SCLK_ISP1 111 <nl> # define SCLK_HDMI_CEC 112
static int vtg_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl> } <nl> vtg -> regs = devm_ioremap_nocache ( dev , res -> start , resource_size ( res )); <nl> + if (! vtg -> regs ) { <nl> + DRM_ERROR (" failed to remap I / O memory \ n "); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> np = of_parse_phandle ( pdev -> dev . of_node , " st , slave ", 0 ); <nl> if ( np ) {
void __init pxa_set_mci_info ( struct pxamci_platform_data * info ) <nl> } <nl>  <nl>  <nl> - static struct pxa2xx_udc_mach_info pxa_udc_info ; <nl> + static struct pxa2xx_udc_mach_info pxa_udc_info = { <nl> + . gpio_pullup = - 1 , <nl> + . gpio_vbus = - 1 , <nl> +}; <nl>  <nl> void __init pxa_set_udc_info ( struct pxa2xx_udc_mach_info * info ) <nl> {
static int __init mod_init ( void ) <nl> if ( err ) { <nl> printk ( KERN_ERR PFX " RNG registering failed (% d )\ n ", <nl> err ); <nl> - goto out ; <nl> + goto err_unmap ; <nl> } <nl> out : <nl> return err ;
static int inet6_rtm_getroute ( struct sk_buff * in_skb , struct nlmsghdr * nlh ) <nl> rt = ( struct rt6_info *) ip6_route_output ( net , NULL , & fl6 ); <nl> } <nl>  <nl> + if ( rt == net -> ipv6 . ip6_null_entry ) { <nl> + err = rt -> dst . error ; <nl> + ip6_rt_put ( rt ); <nl> + goto errout ; <nl> + } <nl> + <nl> skb = alloc_skb ( NLMSG_GOODSIZE , GFP_KERNEL ); <nl> if (! skb ) { <nl> ip6_rt_put ( rt );
static int i915_context_status ( struct seq_file * m , void * unused ) <nl> } <nl>  <nl> list_for_each_entry ( ctx , & dev_priv -> context_list , link ) { <nl> + if ( ctx -> obj == NULL ) <nl> + continue ; <nl> + <nl> seq_puts ( m , " HW context "); <nl> describe_ctx ( m , ctx ); <nl> for_each_ring ( ring , dev_priv , i )
void rtl_8821ae_c2h_command_handle ( struct ieee80211_hw * hw ) <nl> rtl_write_byte ( rtlpriv , 0x1AF , 0x00 ); <nl> return ; <nl> } <nl> - ptmp_buf = ( u8 *) kmalloc ( c2h_event . cmd_len , GFP_KERNEL ); <nl> + ptmp_buf = kmalloc ( c2h_event . cmd_len , GFP_KERNEL ); <nl> if ( ptmp_buf == NULL ) { <nl> RT_TRACE ( COMP_FW , DBG_TRACE , (" malloc cmd buf failed \ n ")); <nl> return ;
static void valleyview_irq_preinstall ( struct drm_device * dev ) <nl> I915_WRITE ( RING_IMR ( GEN6_BSD_RING_BASE ), 0 ); <nl> I915_WRITE ( RING_IMR ( BLT_RING_BASE ), 0 ); <nl>  <nl> - /* and GT */ <nl> - I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> - I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> - <nl> gen5_gt_irq_reset ( dev ); <nl>  <nl> I915_WRITE ( DPINVGTT , DPINVGTT_STATUS_MASK );
# include " dvb_frontend . h " <nl> # include " au8522_priv . h " <nl>  <nl> + MODULE_LICENSE (" GPL "); <nl> + <nl> static int debug ; <nl>  <nl> # define dprintk ( arg ...)\
static int bnx2x_set_pauseparam ( struct net_device * dev , <nl> bp -> link_params . req_flow_ctrl [ cfg_idx ] = <nl> BNX2X_FLOW_CTRL_AUTO ; <nl> } <nl> + bp -> link_params . req_fc_auto_adv = BNX2X_FLOW_CTRL_NONE ; <nl> + if ( epause -> rx_pause ) <nl> + bp -> link_params . req_fc_auto_adv |= BNX2X_FLOW_CTRL_RX ; <nl> + <nl> + if ( epause -> tx_pause ) <nl> + bp -> link_params . req_fc_auto_adv |= BNX2X_FLOW_CTRL_TX ; <nl> } <nl>  <nl> DP ( BNX2X_MSG_ETHTOOL ,
static struct tty_struct * receive_chars ( struct uart_port * port , struct pt_regs * <nl> break ; <nl>  <nl> if ( c == CON_BREAK ) { <nl> + if ( uart_handle_break ( port )) <nl> + continue ; <nl> saw_console_brk = 1 ; <nl> c = 0 ; <nl> }
static void tenxpress_phy_fini ( struct efx_nic * efx ) <nl> { <nl> int reg ; <nl>  <nl> - if ( efx -> phy_type == PHY_TYPE_SFT9001B ) { <nl> + if ( efx -> phy_type == PHY_TYPE_SFT9001B ) <nl> device_remove_file (& efx -> pci_dev -> dev , <nl> & dev_attr_phy_short_reach ); <nl> - } else { <nl> + <nl> + if ( efx -> phy_type == PHY_TYPE_SFX7101 ) { <nl> /* Power down the LNPGA */ <nl> reg = ( 1 << PMA_PMD_LNPGA_POWERDOWN_LBN ); <nl> mdio_clause45_write ( efx , efx -> mii . phy_id , MDIO_MMD_PMAPMD ,
static bool ath9k_hw_chip_reset ( struct ath_hw * ah , <nl> if ( AR_SREV_9330 ( ah )) <nl> ar9003_hw_internal_regulator_apply ( ah ); <nl> ath9k_hw_init_pll ( ah , chan ); <nl> - ath9k_hw_set_rfmode ( ah , chan ); <nl>  <nl> return true ; <nl> } <nl> int ath9k_hw_reset ( struct ath_hw * ah , struct ath9k_channel * chan , <nl> if ( r ) <nl> return r ; <nl>  <nl> + ath9k_hw_set_rfmode ( ah , chan ); <nl> + <nl> if ( ath9k_hw_mci_is_enabled ( ah )) <nl> ar9003_mci_reset ( ah , false , IS_CHAN_2GHZ ( chan ), save_fullsleep ); <nl> 
static void __devexit pm2fb_remove ( struct pci_dev * pdev ) <nl> release_mem_region ( fix -> mmio_start , fix -> mmio_len ); <nl>  <nl> pci_set_drvdata ( pdev , NULL ); <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> kfree ( info -> pixmap . addr ); <nl> kfree ( info ); <nl> }
static void add_pin_to_irq_node ( struct irq_cfg * cfg , int node , int apic , int pin <nl> } <nl>  <nl> entry = get_one_free_irq_2_pin ( node ); <nl> + if (! entry ) { <nl> + printk ( KERN_ERR " can not alloc irq_pin_list \ n "); <nl> + BUG_ON ( 1 ); <nl> + } <nl> entry -> apic = apic ; <nl> entry -> pin = pin ; <nl> 
static int i40e_suspend ( struct pci_dev * pdev , pm_message_t state ) <nl>  <nl> set_bit ( __I40E_SUSPENDED , & pf -> state ); <nl> set_bit ( __I40E_DOWN , & pf -> state ); <nl> + del_timer_sync (& pf -> service_timer ); <nl> + cancel_work_sync (& pf -> service_task ); <nl> rtnl_lock (); <nl> i40e_prep_for_reset ( pf ); <nl> rtnl_unlock ();
static struct irqaction psurge_irqaction = { <nl>  <nl> static void __init smp_psurge_setup_cpu ( int cpu_nr ) <nl> { <nl> - if ( cpu_nr != 0 ) <nl> + if ( cpu_nr != 0 || ! psurge_start ) <nl> return ; <nl>  <nl> /* reset the entry point so if we get another intr we won ' t
xfs_mountfs ( <nl> * Allocate and initialize the per - ag data . <nl> */ <nl> spin_lock_init (& mp -> m_perag_lock ); <nl> - INIT_RADIX_TREE (& mp -> m_perag_tree , GFP_NOFS ); <nl> + INIT_RADIX_TREE (& mp -> m_perag_tree , GFP_ATOMIC ); <nl> error = xfs_initialize_perag ( mp , sbp -> sb_agcount , & mp -> m_maxagi ); <nl> if ( error ) { <nl> cmn_err ( CE_WARN , " XFS : Failed per - ag init : % d ", error );
__cmd_probe ( int argc , const char ** argv , const char * prefix __maybe_unused ) <nl> OPT_CALLBACK (' x ', " exec ", NULL , " executable | path ", <nl> " target executable name or path ", opt_set_target ), <nl> OPT_BOOLEAN ( 0 , " demangle ", & symbol_conf . demangle , <nl> - " Disable symbol demangling "), <nl> + " Enable symbol demangling "), <nl> OPT_BOOLEAN ( 0 , " demangle - kernel ", & symbol_conf . demangle_kernel , <nl> " Enable kernel symbol demangling "), <nl> OPT_END ()
struct uts_namespace ; <nl>  <nl> extern cpumask_var_t cpu_isolated_map ; <nl>  <nl> - extern int runqueue_is_locked ( int cpu ); <nl> - <nl> extern void scheduler_tick ( void ); <nl>  <nl> # define MAX_SCHEDULE_TIMEOUT LONG_MAX
static int cp2112_read ( struct cp2112_device * dev , u8 * data , size_t size ) <nl> struct cp2112_force_read_report report ; <nl> int ret ; <nl>  <nl> + if ( size > sizeof ( dev -> read_data )) <nl> + size = sizeof ( dev -> read_data ); <nl> report . report = CP2112_DATA_READ_FORCE_SEND ; <nl> report . length = cpu_to_be16 ( size ); <nl> 
struct net_device * alloc_rtllib ( int sizeof_priv ) <nl> rtllib_softmac_init ( ieee ); <nl>  <nl> ieee -> pHTInfo = kzalloc ( sizeof ( struct rt_hi_throughput ), GFP_KERNEL ); <nl> - if ( ieee -> pHTInfo == NULL ) <nl> + if (! ieee -> pHTInfo ) <nl> return NULL ; <nl>  <nl> HTUpdateDefaultSetting ( ieee );
static hda_nid_t set_path_power ( struct hda_codec * codec , hda_nid_t nid , <nl>  <nl> for ( n = 0 ; n < spec -> paths . used ; n ++) { <nl> path = snd_array_elem (& spec -> paths , n ); <nl> + if (! path -> depth ) <nl> + continue ; <nl> if ( path -> path [ 0 ] == nid || <nl> path -> path [ path -> depth - 1 ] == nid ) { <nl> bool pin_old = path -> pin_enabled ;
static void o2hb_region_release ( struct config_item * item ) <nl> debugfs_remove ( reg -> hr_debug_dir ); <nl> kfree ( reg -> hr_db_livenodes ); <nl> kfree ( reg -> hr_db_regnum ); <nl> - kfree ( reg -> hr_debug_elapsed_time ); <nl> - kfree ( reg -> hr_debug_pinned ); <nl> + kfree ( reg -> hr_db_elapsed_time ); <nl> + kfree ( reg -> hr_db_pinned ); <nl>  <nl> spin_lock (& o2hb_live_lock ); <nl> list_del (& reg -> hr_all_item );
static int __init atmel_nand_probe ( struct platform_device * pdev ) <nl> } <nl> } <nl> if ( use_dma ) <nl> - dev_info ( host -> dev , " Using DMA for NAND access .\ n "); <nl> + dev_info ( host -> dev , " Using % s for DMA transfers .\ n ", <nl> + dma_chan_name ( host -> dma_chan )); <nl> else <nl> dev_info ( host -> dev , " No DMA support for NAND access .\ n "); <nl> 
# define PAGE_SIZE ( _AC ( 1 , UL ) << PAGE_SHIFT ) <nl> # define PAGE_MASK (~( PAGE_SIZE - 1 )) <nl>  <nl> -# define __PHYSICAL_MASK (( phys_addr_t )( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> +# define __PHYSICAL_MASK (( phys_addr_t )(( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 )) <nl> # define __VIRTUAL_MASK (( 1UL << __VIRTUAL_MASK_SHIFT ) - 1 ) <nl>  <nl> /* Cast PAGE_MASK to a signed type so that it is sign - extended if
int hfsplus_parse_options ( char * input , struct hfsplus_sb_info * sbi ) <nl> return 0 ; <nl> } <nl> p = match_strdup (& args [ 0 ]); <nl> - sbi -> nls = load_nls ( p ); <nl> + if ( p ) <nl> + sbi -> nls = load_nls ( p ); <nl> if (! sbi -> nls ) { <nl> printk ( KERN_ERR " hfs : unable to load nls mapping \"% s \"\ n ", p ); <nl> kfree ( p );
static int map_sg_data ( struct scsi_cmnd * cmd , <nl> sdev_printk ( KERN_ERR , cmd -> device , <nl> " Can ' t allocate memory " <nl> " for indirect table \ n "); <nl> + scsi_dma_unmap ( cmd ); <nl> return 0 ; <nl> } <nl> }
static int fb_deferred_io_mkwrite ( struct vm_area_struct * vma , <nl> deferred framebuffer IO . then if userspace touches a page <nl> again , we repeat the same scheme */ <nl>  <nl> + file_update_time ( vma -> vm_file ); <nl> + <nl> /* protect against the workqueue changing the page list */ <nl> mutex_lock (& fbdefio -> lock ); <nl> 
static void dashtty_timer ( unsigned long ignored ) <nl> if ( channel >= 0 ) <nl> fetch_data ( channel ); <nl>  <nl> - mod_timer_pinned (& poll_timer , jiffies + DA_TTY_POLL ); <nl> + mod_timer (& poll_timer , jiffies + DA_TTY_POLL ); <nl> } <nl>  <nl> static void add_poll_timer ( struct timer_list * poll_timer ) <nl> { <nl> - setup_timer ( poll_timer , dashtty_timer , 0 ); <nl> + setup_pinned_timer ( poll_timer , dashtty_timer , 0 ); <nl> poll_timer -> expires = jiffies + DA_TTY_POLL ; <nl>  <nl> /*
try_offline_again : <nl> */ <nl> ata_msleep ( ap , 1 ); <nl>  <nl> + sata_set_spd ( link ); <nl> + <nl> /* <nl> * Now , bring the host controller online again , this can take time <nl> * as PHY reset and communication establishment , 1st D2H FIS and
static void mt_feature_mapping ( struct hid_device * hdev , <nl> td -> is_buttonpad = true ; <nl>  <nl> break ; <nl> + case 0xff0000c5 : <nl> + /* Retrieve the Win8 blob once to enable some devices */ <nl> + if ( usage -> usage_index == 0 ) <nl> + mt_get_feature ( hdev , field -> report ); <nl> + break ; <nl> } <nl> } <nl> 
static int chsc_ioctl_info_cu ( void __user * user_cd ) <nl> goto out_free ; <nl> } <nl> scucd_area -> request . length = 0x0010 ; <nl> - scucd_area -> request . code = 0x0028 ; <nl> + scucd_area -> request . code = 0x0026 ; <nl> scucd_area -> m = cd -> m ; <nl> scucd_area -> fmt1 = cd -> fmt ; <nl> scucd_area -> cssid = cd -> cssid ;
int mc13xxx_common_init ( struct mc13xxx * mc13xxx , <nl> err_mask : <nl> err_revision : <nl> mc13xxx_unlock ( mc13xxx ); <nl> - kfree ( mc13xxx ); <nl> return ret ; <nl> } <nl> 
static void svm_get_segment ( struct kvm_vcpu * vcpu , <nl> if ( seg == VCPU_SREG_CS ) <nl> var -> g = s -> limit > 0xfffff ; <nl>  <nl> + /* <nl> + * Work around a bug where the busy flag in the tr selector <nl> + * isn ' t exposed <nl> + */ <nl> + if ( seg == VCPU_SREG_TR ) <nl> + var -> type |= 0x2 ; <nl> + <nl> var -> unusable = ! var -> present ; <nl> } <nl> 
static int mt9t112_probe ( struct i2c_client * client , <nl> v4l2_i2c_subdev_init (& priv -> subdev , client , & mt9t112_subdev_ops ); <nl>  <nl> ret = mt9t112_camera_probe ( client ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> kfree ( priv ); <nl> + return ret ; <nl> + } <nl>  <nl> /* Cannot fail : using the default supported pixel code */ <nl> mt9t112_set_params ( priv , & rect , V4L2_MBUS_FMT_UYVY8_2X8 );
int ath5k_hw_attach ( struct ath5k_softc * sc ) <nl> ah -> ah_limit_tx_retries = AR5K_INIT_TX_RETRY ; <nl> ah -> ah_software_retry = false ; <nl> ah -> ah_ant_mode = AR5K_ANTMODE_DEFAULT ; <nl> + ah -> ah_noise_floor = - 95 ; /* until first NF calibration is run */ <nl>  <nl> /* <nl> * Find the mac version
static long privcmd_ioctl ( struct file * file , <nl> # ifndef HAVE_ARCH_PRIVCMD_MMAP <nl> static int privcmd_fault ( struct vm_area_struct * vma , struct vm_fault * vmf ) <nl> { <nl> + printk ( KERN_DEBUG " privcmd_fault : vma =% p % lx -% lx , pgoff =% lx , uv =% p \ n ", <nl> + vma , vma -> vm_start , vma -> vm_end , <nl> + vmf -> pgoff , vmf -> virtual_address ); <nl> + <nl> return VM_FAULT_SIGBUS ; <nl> } <nl> 
__ip_vs_get_dest_entries ( struct net * net , const struct ip_vs_get_dests * get , <nl> struct ip_vs_dest * dest ; <nl> struct ip_vs_dest_entry entry ; <nl>  <nl> + memset (& entry , 0 , sizeof ( entry )); <nl> list_for_each_entry ( dest , & svc -> destinations , n_list ) { <nl> if ( count >= get -> num_dests ) <nl> break ;
static int ixgbe_resume ( struct pci_dev * pdev ) <nl>  <nl> pci_wake_from_d3 ( pdev , false ); <nl>  <nl> + rtnl_lock (); <nl> err = ixgbe_init_interrupt_scheme ( adapter ); <nl> + rtnl_unlock (); <nl> if ( err ) { <nl> e_dev_err (" Cannot initialize interrupts for device \ n "); <nl> return err ;
int snd_usb_caiaq_audio_init ( struct snd_usb_caiaqdev * cdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( cdev -> n_streams < 2 ) { <nl> + dev_err ( dev , " bogus number of streams : % d \ n ", cdev -> n_streams ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> ret = snd_pcm_new ( cdev -> chip . card , cdev -> product_name , 0 , <nl> cdev -> n_audio_out , cdev -> n_audio_in , & cdev -> pcm ); <nl> 
static int ioctl_standard_iw_point ( struct iw_point * iwp , unsigned int cmd , <nl> err = - EFAULT ; <nl> goto out ; <nl> } <nl> + <nl> + if ( cmd == SIOCSIWENCODEEXT ) { <nl> + struct iw_encode_ext * ee = ( void *) extra ; <nl> + <nl> + if ( iwp -> length < sizeof (* ee ) + ee -> key_len ) <nl> + return - EFAULT ; <nl> + } <nl> } <nl>  <nl> err = handler ( dev , info , ( union iwreq_data *) iwp , extra );
struct gb_sdio_get_caps_response { <nl>  <nl> /* see possible values below at vdd */ <nl> __le32 ocr ; <nl> - __le16 max_blk_count ; <nl> - __le16 max_blk_size ; <nl> __le32 f_min ; <nl> __le32 f_max ; <nl> + __le16 max_blk_count ; <nl> + __le16 max_blk_size ; <nl> } __packed ; <nl>  <nl> /* set ios request : response has no payload */
void xhci_mem_cleanup ( struct xhci_hcd * xhci ) <nl>  <nl> if ( xhci -> lpm_command ) <nl> xhci_free_command ( xhci , xhci -> lpm_command ); <nl> + xhci -> lpm_command = NULL ; <nl> if ( xhci -> cmd_ring ) <nl> xhci_ring_free ( xhci , xhci -> cmd_ring ); <nl> xhci -> cmd_ring = NULL ;
static bool radeon_msi_ok ( struct radeon_device * rdev ) <nl> ( rdev -> pdev -> subsystem_device == 0x0185 )) <nl> return true ; <nl>  <nl> + /* try and enable MSIs by default on all RS690s */ <nl> + if ( rdev -> family == CHIP_RS690 ) <nl> + return true ; <nl> + <nl> /* RV515 seems to have MSI issues where it loses <nl> * MSI rearms occasionally . This leads to lockups and freezes . <nl> * disable it by default .
struct spi_message { <nl> void * state ; <nl> }; <nl>  <nl> + static inline void spi_message_init_no_memset ( struct spi_message * m ) <nl> +{ <nl> + INIT_LIST_HEAD (& m -> transfers ); <nl> +} <nl> + <nl> static inline void spi_message_init ( struct spi_message * m ) <nl> { <nl> memset ( m , 0 , sizeof * m ); <nl> - INIT_LIST_HEAD (& m -> transfers ); <nl> + spi_message_init_no_memset ( m ); <nl> } <nl>  <nl> static inline void
static int xfrm_add_pol_expire ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl> if ( err ) <nl> return err ; <nl>  <nl> + err = verify_policy_dir ( p -> dir ); <nl> + if ( err ) <nl> + return err ; <nl> + <nl> if ( p -> index ) <nl> xp = xfrm_policy_byid ( net , mark , type , p -> dir , p -> index , 0 , & err ); <nl> else {
static int usb_audio_probe ( struct usb_interface * intf , <nl> goto __error ; <nl> } <nl> chip = usb_chip [ i ]; <nl> - dev_set_drvdata (& dev -> dev , chip ); <nl> atomic_inc (& chip -> active ); /* avoid autopm */ <nl> break ; <nl> } <nl> static int usb_audio_probe ( struct usb_interface * intf , <nl> goto __error ; <nl> } <nl> } <nl> + dev_set_drvdata (& dev -> dev , chip ); <nl>  <nl> /* <nl> * For devices with more than one control interface , we assume the
static long gpio_ioctl ( struct file * filp , unsigned int cmd , unsigned long arg ) <nl> if ( cmd == GPIO_GET_CHIPINFO_IOCTL ) { <nl> struct gpiochip_info chipinfo ; <nl>  <nl> + memset (& chipinfo , 0 , sizeof ( chipinfo )); <nl> + <nl> strncpy ( chipinfo . name , dev_name (& gdev -> dev ), <nl> sizeof ( chipinfo . name )); <nl> chipinfo . name [ sizeof ( chipinfo . name )- 1 ] = '\ 0 ';
static void fsg_unbind ( struct usb_configuration * c , struct usb_function * f ) <nl> fsg_common_put ( common ); <nl> usb_free_descriptors ( fsg -> function . descriptors ); <nl> usb_free_descriptors ( fsg -> function . hs_descriptors ); <nl> + usb_free_descriptors ( fsg -> function . ss_descriptors ); <nl> kfree ( fsg ); <nl> } <nl> 
extern void __chk_io_ptr ( void __iomem *); <nl> # define __deprecated /* unimplemented */ <nl> # endif <nl>  <nl> +# ifdef MODULE <nl> +# define __deprecated_for_modules __deprecated <nl> +# else <nl> +# define __deprecated_for_modules <nl> +# endif <nl> + <nl> # ifndef __must_check <nl> # define __must_check <nl> # endif
static int __init amd64_edac_init ( void ) <nl> mcis = kzalloc ( amd_nb_num () * sizeof ( mcis [ 0 ]), GFP_KERNEL ); <nl> ecc_stngs = kzalloc ( amd_nb_num () * sizeof ( ecc_stngs [ 0 ]), GFP_KERNEL ); <nl> if (!( mcis && ecc_stngs )) <nl> - goto err_ret ; <nl> + goto err_free ; <nl>  <nl> msrs = msrs_alloc (); <nl> if (! msrs )
struct lgdt3306a_config { <nl> u16 vsb_if_khz ; <nl>  <nl> /* disable i2c repeater - 0 : repeater enabled 1 : repeater disabled */ <nl> - int deny_i2c_rptr : 1 ; <nl> + unsigned int deny_i2c_rptr : 1 ; <nl>  <nl> /* spectral inversion - 0 : disabled 1 : enabled */ <nl> - int spectral_inversion : 1 ; <nl> + unsigned int spectral_inversion : 1 ; <nl>  <nl> enum lgdt3306a_mpeg_mode mpeg_mode ; <nl> enum lgdt3306a_tp_clock_edge tpclk_edge ;
static int pcs_parse_one_pinctrl_entry ( struct pcs_device * pcs , <nl> (* map )-> data . mux . function = np -> name ; <nl>  <nl> if ( pcs -> is_pinconf ) { <nl> - if ( pcs_parse_pinconf ( pcs , np , function , map )) <nl> + res = pcs_parse_pinconf ( pcs , np , function , map ); <nl> + if ( res ) <nl> goto free_pingroups ; <nl> * num_maps = 2 ; <nl> } else {
static int fsl_pcie_check_link ( struct pci_controller * hose ) <nl> if ( hose -> indirect_type & PPC_INDIRECT_TYPE_FSL_CFG_REG_LINK ) { <nl> if ( hose -> ops -> read == fsl_indirect_read_config ) { <nl> struct pci_bus bus ; <nl> - bus . number = 0 ; <nl> + bus . number = hose -> first_busno ; <nl> bus . sysdata = hose ; <nl> bus . ops = hose -> ops ; <nl> indirect_read_config (& bus , 0 , PCIE_LTSSM , 4 , & val );
err_dvb_unregister_frontend : <nl>  <nl> err_dvb_frontend_detach : <nl> for ( i = MAX_NO_OF_FE_PER_ADAP - 1 ; i >= 0 ; i --) { <nl> - if ( adap -> fe [ i ]) <nl> + if ( adap -> fe [ i ]) { <nl> dvb_frontend_detach ( adap -> fe [ i ]); <nl> + adap -> fe [ i ] = NULL ; <nl> + } <nl> } <nl>  <nl> err :
static struct sst_acpi_mach sst_acpi_chv [] = { <nl> & chv_platform_data }, <nl> {" 10EC3276 ", " bytcr_rt5640 ", " intel / fw_sst_22a8 . bin ", " bytcr_rt5640 ", NULL , <nl> & chv_platform_data }, <nl> - <nl> + /* some CHT - T platforms rely on RT5651 , use Baytrail machine driver */ <nl> + {" 10EC5651 ", " bytcr_rt5651 ", " intel / fw_sst_22a8 . bin ", " bytcr_rt5651 ", NULL , <nl> + & chv_platform_data }, <nl> {}, <nl> }; <nl> 
static int egalax_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> struct egalax_data * td ; <nl> struct hid_report * report ; <nl>  <nl> - td = kmalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> + td = kzalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> if (! td ) { <nl> dev_err (& hdev -> dev , " cannot allocate eGalax data \ n "); <nl> return - ENOMEM ;
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> */ <nl> if ( skb_shared ( skb )) { <nl> tmp_skb = skb ; <nl> - skb = skb_copy ( skb , GFP_ATOMIC ); <nl> + skb = skb_clone ( skb , GFP_ATOMIC ); <nl> kfree_skb ( tmp_skb ); <nl>  <nl> if (! skb ) {
static int bcma_get_next_core ( struct bcma_bus * bus , u32 __iomem ** eromptr , <nl> switch ( core -> id . id ) { <nl> case BCMA_CORE_4706_MAC_GBIT_COMMON : <nl> case BCMA_CORE_NS_CHIPCOMMON_B : <nl> + case BCMA_CORE_PMU : <nl> + case BCMA_CORE_GCI : <nl> /* Not used yet : case BCMA_CORE_OOB_ROUTER : */ <nl> break ; <nl> default :
static long kvm_vm_ioctl ( struct file * filp , <nl> routing . nr * sizeof (* entries ))) <nl> goto out_free_irq_routing ; <nl> } <nl> + /* avoid races with KVM_CREATE_IRQCHIP on x86 */ <nl> + mutex_lock (& kvm -> lock ); <nl> r = kvm_set_irq_routing ( kvm , entries , routing . nr , <nl> routing . flags ); <nl> + mutex_unlock (& kvm -> lock ); <nl> out_free_irq_routing : <nl> vfree ( entries ); <nl> break ;
static ssize_t rng_dev_read ( struct file * filp , char __user * buf , size_t size , <nl> if ( filp -> f_flags & O_NONBLOCK ) <nl> return ret ? : - EAGAIN ; <nl>  <nl> - if ( need_resched ()){ <nl> - current -> state = TASK_INTERRUPTIBLE ; <nl> - schedule_timeout ( 1 ); <nl> - } <nl> + if ( need_resched ()) <nl> + schedule_timeout_interruptible ( 1 ); <nl> } <nl> else return n ; <nl> if ( signal_pending ( current ))
static struct omap_board_mux board_mux [] __initdata = { <nl> OMAP4_MUX ( DPM_EMU18 , OMAP_PIN_OUTPUT | OMAP_MUX_MODE5 ), <nl> /* dispc2_data0 */ <nl> OMAP4_MUX ( DPM_EMU19 , OMAP_PIN_OUTPUT | OMAP_MUX_MODE5 ), <nl> + /* NIRQ2 for twl6040 */ <nl> + OMAP4_MUX ( SYS_NIRQ2 , OMAP_MUX_MODE0 | <nl> + OMAP_PIN_INPUT_PULLUP | OMAP_PIN_OFF_WAKEUPENABLE ), <nl> { . reg_offset = OMAP_MUX_TERMINATOR }, <nl> }; <nl> 
static int scsi_check_sense ( struct scsi_cmnd * scmd ) <nl> return SUCCESS ; <nl>  <nl> case MEDIUM_ERROR : <nl> + if ( sshdr . asc == 0x11 || /* UNRECOVERED READ ERR */ <nl> + sshdr . asc == 0x13 || /* AMNF DATA FIELD */ <nl> + sshdr . asc == 0x14 ) { /* RECORD NOT FOUND */ <nl> + return SUCCESS ; <nl> + } <nl> return NEEDS_RETRY ; <nl>  <nl> case HARDWARE_ERROR :
static int lov_add_target ( struct obd_device * obd , struct obd_uuid * uuidp , <nl> struct lov_tgt_desc ** newtgts , ** old = NULL ; <nl> __u32 newsize , oldsize = 0 ; <nl>  <nl> - newsize = max ( lov -> lov_tgt_size , ( __u32 ) 2 ); <nl> + newsize = max_t ( __u32 , lov -> lov_tgt_size , 2 ); <nl> while ( newsize < index + 1 ) <nl> newsize = newsize << 1 ; <nl> OBD_ALLOC ( newtgts , sizeof (* newtgts ) * newsize );
static irqreturn_t s3c64xx_dma_irq ( int irq , void * pw ) <nl>  <nl> s3c64xx_dma_bufffdone ( chan , buff , res ); <nl>  <nl> + /* Free the node and update curr , if non - circular queue */ <nl> + if (!( chan -> flags & S3C2410_DMAF_CIRCULAR )) { <nl> + chan -> curr = buff -> next ; <nl> + s3c64xx_dma_freebuff ( buff ); <nl> + } <nl> + <nl> /* Update ' next ' */ <nl> buff = chan -> next ; <nl> if ( chan -> next == chan -> end ) {
# define OMAP4430_PRM_BASE 0x4a306000 <nl> # define OMAP44XX_GPMC_BASE 0x50000000 <nl> # define OMAP443X_SCM_BASE 0x4a002000 <nl> -# define OMAP443X_CTRL_BASE OMAP443X_SCM_BASE <nl> +# define OMAP443X_CTRL_BASE 0x4a100000 <nl> # define OMAP44XX_IC_BASE 0x48200000 <nl> # define OMAP44XX_IVA_INTC_BASE 0x40000000 <nl> # define IRQ_SIR_IRQ 0x0040
void ieee80211_scan_work ( struct work_struct * work ) <nl>  <nl> rc = __ieee80211_start_scan ( sdata , req ); <nl> if ( rc ) { <nl> + /* need to complete scan in cfg80211 */ <nl> + local -> scan_req = req ; <nl> aborted = true ; <nl> goto out_complete ; <nl> } else
EXPORT_SYMBOL ( get_trigger_mask ); <nl> EXPORT_SYMBOL ( global_trigger_mask ); <nl> # endif <nl>  <nl> + EXPORT_SYMBOL ( clear_page ); <nl> + EXPORT_SYMBOL ( copy_page ); <nl> EXPORT_SYMBOL ( empty_zero_page ); <nl>  <nl> EXPORT_SYMBOL ( pfn_base );
static inline void iounmap ( volatile void __iomem * addr ) <nl> { <nl> } <nl>  <nl> + static inline void __iomem * ioport_map ( unsigned long port , unsigned int nr ) <nl> +{ <nl> + return NULL ; <nl> +} <nl> + <nl> + static inline void ioport_unmap ( void __iomem * p ) <nl> +{ <nl> +} <nl> + <nl> /* <nl> * s390 needs a private implementation of pci_iomap since ioremap with its <nl> * offset parameter isn ' t sufficient . That ' s because BAR spaces are not
static struct usb_request * isp1760_ep_alloc_request ( struct usb_ep * ep , <nl> struct isp1760_request * req ; <nl>  <nl> req = kzalloc ( sizeof (* req ), gfp_flags ); <nl> + if (! req ) <nl> + return NULL ; <nl>  <nl> return & req -> req ; <nl> }
static int ceph_con_in_msg_alloc ( struct ceph_connection * con , int * skip ) <nl> msg = con -> ops -> alloc_msg ( con , hdr , skip ); <nl> mutex_lock (& con -> mutex ); <nl> if ( con -> state != CON_STATE_OPEN ) { <nl> - ceph_msg_put ( msg ); <nl> + if ( msg ) <nl> + ceph_msg_put ( msg ); <nl> return - EAGAIN ; <nl> } <nl> con -> in_msg = msg ;
static int dummy_udc_probe ( struct platform_device * pdev ) <nl> int rc ; <nl>  <nl> dum = *(( void **) dev_get_platdata (& pdev -> dev )); <nl> + /* Clear usb_gadget region for new registration to udc - core */ <nl> + memzero_explicit (& dum -> gadget , sizeof ( struct usb_gadget )); <nl> dum -> gadget . name = gadget_name ; <nl> dum -> gadget . ops = & dummy_ops ; <nl> dum -> gadget . max_speed = USB_SPEED_SUPER ;
struct ti_ohci { <nl>  <nl> static inline int cross_bound ( unsigned long addr , unsigned int size ) <nl> { <nl> + if ( size == 0 ) <nl> + return 0 ; <nl> + <nl> if ( size > PAGE_SIZE ) <nl> return 1 ; <nl> 
static void hw_init ( void ) <nl> break ; <nl> } <nl>  <nl> + /* magic required on VX900 for correct modesetting on IGA1 */ <nl> + via_write_reg_mask ( VIACR , 0x45 , 0x00 , 0x01 ); <nl> + <nl> /* probably this should go to the scaling code one day */ <nl> via_write_reg_mask ( VIACR , 0xFD , 0 , 0x80 ); /* VX900 hw scale on IGA2 */ <nl> viafb_write_regx ( scaling_parameters , ARRAY_SIZE ( scaling_parameters ));
static int piix_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl>  <nl> static void piix_host_stop ( struct ata_host_set * host_set ) <nl> { <nl> + struct piix_host_priv * hpriv = host_set -> private_data ; <nl> + <nl> ata_host_stop ( host_set ); <nl> + <nl> + kfree ( hpriv ); <nl> } <nl>  <nl> static int __init piix_init ( void )
static unsigned long super_cache_scan ( struct shrinker * shrink , <nl> inodes = list_lru_count_node (& sb -> s_inode_lru , sc -> nid ); <nl> dentries = list_lru_count_node (& sb -> s_dentry_lru , sc -> nid ); <nl> total_objects = dentries + inodes + fs_objects + 1 ; <nl> + if (! total_objects ) <nl> + total_objects = 1 ; <nl>  <nl> /* proportion the scan between the caches */ <nl> dentries = mult_frac ( sc -> nr_to_scan , dentries , total_objects );
static struct shash_alg alg = { <nl> . export = md5_export , <nl> . import = md5_import , <nl> . descsize = sizeof ( struct md5_state ), <nl> + . statesize = sizeof ( struct md5_state ), <nl> . base = { <nl> . cra_name = " md5 ", <nl> . cra_flags = CRYPTO_ALG_TYPE_SHASH ,
static int __devinit cpm_i2c_setup ( struct cpm_i2c * cpm ) <nl> init_waitqueue_head (& cpm -> i2c_wait ); <nl>  <nl> cpm -> irq = of_irq_to_resource ( ofdev -> node , 0 , NULL ); <nl> - if ( cpm -> irq == NO_IRQ ) <nl> + if (! cpm -> irq ) <nl> return - EINVAL ; <nl>  <nl> /* Install interrupt handler . */
static int __init ion_dummy_init ( void ) <nl> int i , err ; <nl>  <nl> idev = ion_device_create ( NULL ); <nl> - heaps = kzalloc ( sizeof ( struct ion_heap *) * dummy_ion_pdata . nr , <nl> + heaps = kcalloc ( dummy_ion_pdata . nr , sizeof ( struct ion_heap *), <nl> GFP_KERNEL ); <nl> if (! heaps ) <nl> return - ENOMEM ;
static inline pte_t pfn_pte ( unsigned long pfn , pgprot_t prot ) <nl> sz_bits = 0UL ; <nl> if ( _PAGE_SZBITS_4U != 0UL || _PAGE_SZBITS_4V != 0UL ) { <nl> __asm__ __volatile__ ( <nl> - "\ n661 : sethi % uhi (% 1 ), % 0 \ n " <nl> + "\ n661 : sethi %% uhi (% 1 ), % 0 \ n " <nl> " sllx % 0 , 32 , % 0 \ n " <nl> " . section . sun4v_2insn_patch , \" ax \"\ n " <nl> " . word 661b \ n "
static ssize_t xenbus_file_read ( struct file * filp , <nl> int ret ; <nl>  <nl> mutex_lock (& u -> reply_mutex ); <nl> + again : <nl> while ( list_empty (& u -> read_buffers )) { <nl> mutex_unlock (& u -> reply_mutex ); <nl> if ( filp -> f_flags & O_NONBLOCK ) <nl> static ssize_t xenbus_file_read ( struct file * filp , <nl> struct read_buffer , list ); <nl> } <nl> } <nl> + if ( i == 0 ) <nl> + goto again ; <nl>  <nl> out : <nl> mutex_unlock (& u -> reply_mutex );
void flush_thread ( void ) <nl>  <nl> void start_thread ( struct pt_regs * regs , unsigned long eip , unsigned long esp ) <nl> { <nl> + get_safe_registers ( regs -> regs . gp , regs -> regs . fp ); <nl> PT_REGS_IP ( regs ) = eip ; <nl> PT_REGS_SP ( regs ) = esp ; <nl> current -> ptrace &= ~ PT_DTRACE ;
static int rs_get_tbl_info_from_mcs ( const u32 rate_n_flags , <nl> u8 num_of_ant = get_num_of_ant_from_rate ( rate_n_flags ); <nl> u8 mcs ; <nl>  <nl> - memset ( tbl , 0 , sizeof ( struct iwl_scale_tbl_info )); <nl> + memset ( tbl , 0 , offsetof ( struct iwl_scale_tbl_info , win )); <nl> * rate_idx = iwl_hwrate_to_plcp_idx ( rate_n_flags ); <nl>  <nl> if (* rate_idx == IWL_RATE_INVALID ) {
void falcon_remove_nic ( struct efx_nic * efx ) <nl> struct falcon_nic_data * nic_data = efx -> nic_data ; <nl> int rc ; <nl>  <nl> + /* Remove I2C adapter and clear it in preparation for a retry */ <nl> rc = i2c_del_adapter (& efx -> i2c_adap ); <nl> BUG_ON ( rc ); <nl> + memset (& efx -> i2c_adap , 0 , sizeof ( efx -> i2c_adap )); <nl>  <nl> falcon_remove_spi_devices ( efx ); <nl> falcon_free_buffer ( efx , & efx -> irq_status );
pnfs_layout_bulk_destroy_byserver_locked ( struct nfs_client * clp , <nl> struct inode * inode ; <nl>  <nl> list_for_each_entry_safe ( lo , next , & server -> layouts , plh_layouts ) { <nl> + if ( test_bit ( NFS_LAYOUT_INVALID_STID , & lo -> plh_flags )) <nl> + continue ; <nl> inode = igrab ( lo -> plh_inode ); <nl> if ( inode == NULL ) <nl> continue ;
vpif_enum_dv_timings ( struct file * file , void * priv , <nl> int ret ; <nl>  <nl> ret = v4l2_subdev_call ( ch -> sd , video , enum_dv_timings , timings ); <nl> - if ( ret == - ENOIOCTLCMD && ret == - ENODEV ) <nl> + if ( ret == - ENOIOCTLCMD || ret == - ENODEV ) <nl> return - EINVAL ; <nl> return ret ; <nl> }
static int fman_init ( struct fman * fman ) <nl> /* allocate MURAM for FIFO according to total size */ <nl> fman -> fifo_offset = fman_muram_alloc ( fman -> muram , <nl> fman -> state -> total_fifo_size ); <nl> - if ( IS_ERR_VALUE ( fman -> cam_offset )) { <nl> + if ( IS_ERR_VALUE ( fman -> fifo_offset )) { <nl> free_init_resources ( fman ); <nl> dev_err ( fman -> dev , "% s : MURAM alloc for BMI FIFO failed \ n ", <nl> __func__ );
static int irda_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl>  <nl> err = irda_open_tsap ( self , addr -> sir_lsap_sel , addr -> sir_name ); <nl> if ( err < 0 ) { <nl> - kfree ( self -> ias_obj -> name ); <nl> - kfree ( self -> ias_obj ); <nl> + irias_delete_object ( self -> ias_obj ); <nl> + self -> ias_obj = NULL ; <nl> goto out ; <nl> } <nl> 
mxm_sor_map ( struct nvkm_bios * bios , u8 conn ) <nl> u16 map = nvbios_rd16 ( bios , mxm + 4 ); <nl> if ( map ) { <nl> ver = nvbios_rd08 ( bios , map ); <nl> - if ( ver == 0x10 ) { <nl> + if ( ver == 0x10 || ver == 0x11 ) { <nl> if ( conn < nvbios_rd08 ( bios , map + 3 )) { <nl> map += nvbios_rd08 ( bios , map + 1 ); <nl> map += conn ;
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> } <nl> btrfs_release_path ( root , path ); <nl>  <nl> - if ( key . offset + datal < off || <nl> + if ( key . offset + datal <= off || <nl> key . offset >= off + len ) <nl> goto next ; <nl> 
long do_shmat ( int shmid , char __user * shmaddr , int shmflg , ulong * raddr , <nl> down_write (& current -> mm -> mmap_sem ); <nl> if ( addr && !( shmflg & SHM_REMAP )) { <nl> err = - EINVAL ; <nl> + if ( addr + size < addr ) <nl> + goto invalid ; <nl> + <nl> if ( find_vma_intersection ( current -> mm , addr , addr + size )) <nl> goto invalid ; <nl> /*
void kvm_lapic_reset ( struct kvm_vcpu * vcpu , bool init_event ) <nl> apic_set_reg ( apic , APIC_DFR , 0xffffffffU ); <nl> apic_set_spiv ( apic , 0xff ); <nl> apic_set_reg ( apic , APIC_TASKPRI , 0 ); <nl> - kvm_apic_set_ldr ( apic , 0 ); <nl> + if (! apic_x2apic_mode ( apic )) <nl> + kvm_apic_set_ldr ( apic , 0 ); <nl> apic_set_reg ( apic , APIC_ESR , 0 ); <nl> apic_set_reg ( apic , APIC_ICR , 0 ); <nl> apic_set_reg ( apic , APIC_ICR2 , 0 );
static int da9055_rtc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> alm_irq = platform_get_irq_byname ( pdev , " ALM "); <nl> - alm_irq = regmap_irq_get_virq ( rtc -> da9055 -> irq_data , alm_irq ); <nl> + if ( alm_irq < 0 ) <nl> + return alm_irq ; <nl> + <nl> ret = devm_request_threaded_irq (& pdev -> dev , alm_irq , NULL , <nl> da9055_rtc_alm_irq , <nl> IRQF_TRIGGER_HIGH | IRQF_ONESHOT ,
static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
retry_snap : <nl> goto retry_snap ; <nl> } <nl> } else { <nl> + loff_t old_size = inode -> i_size ; <nl> /* <nl> * No need to acquire the i_truncate_mutex . Because <nl> * the MDS revokes Fwb caps before sending truncate <nl> retry_snap : <nl> written = generic_file_buffered_write ( iocb , iov , nr_segs , <nl> pos , & iocb -> ki_pos , <nl> count , 0 ); <nl> + if ( inode -> i_size > old_size ) <nl> + ceph_fscache_update_objectsize ( inode ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> } <nl> 
static void cpufreq_offline_finish ( unsigned int cpu ) <nl> * since this is a core component , and is essential for the <nl> * subsequent light - weight -> init () to succeed . <nl> */ <nl> - if ( cpufreq_driver -> exit ) <nl> + if ( cpufreq_driver -> exit ) { <nl> cpufreq_driver -> exit ( policy ); <nl> + policy -> freq_table = NULL ; <nl> + } <nl> } <nl>  <nl> /**
static int dgrp_net_release ( struct inode * inode , struct file * file ) <nl>  <nl> spin_unlock_irqrestore (& dgrp_poll_data . poll_lock , lock_flags ); <nl>  <nl> - done : <nl> down (& nd -> nd_net_semaphore ); <nl>  <nl> dgrp_monitor_message ( nd , " Net Close "); <nl>  <nl> up (& nd -> nd_net_semaphore ); <nl>  <nl> + done : <nl> module_put ( THIS_MODULE ); <nl> file -> private_data = NULL ; <nl> return 0 ;
static int cpuhp_invoke_ap_callback ( int cpu , enum cpuhp_state state , <nl> if (! cpu_online ( cpu )) <nl> return 0 ; <nl>  <nl> + /* <nl> + * If we are up and running , use the hotplug thread . For early calls <nl> + * we invoke the thread function directly . <nl> + */ <nl> + if (! st -> thread ) <nl> + return cpuhp_invoke_callback ( cpu , state , cb ); <nl> + <nl> st -> cb_state = state ; <nl> st -> cb = cb ; <nl> /*
static void reparent_leader ( struct task_struct * father , struct task_struct * p , <nl> { <nl> list_move_tail (& p -> sibling , & p -> real_parent -> children ); <nl>  <nl> - if ( task_detached ( p )) <nl> + if ( p -> exit_state == EXIT_DEAD ) <nl> return ; <nl> /* <nl> * If this is a threaded reparent there is no need to
static int usb_serial_probe ( struct usb_interface * interface , <nl> num_ports = type -> num_ports ; <nl> } <nl>  <nl> + if ( num_ports > MAX_NUM_PORTS ) { <nl> + dev_warn ( ddev , " too many ports requested : % d \ n ", num_ports ); <nl> + num_ports = MAX_NUM_PORTS ; <nl> + } <nl> + <nl> serial -> num_ports = num_ports ; <nl> serial -> num_bulk_in = num_bulk_in ; <nl> serial -> num_bulk_out = num_bulk_out ;
lookup_pi_state ( u32 uval , struct futex_hash_bucket * hb , <nl> if (! p ) <nl> return - ESRCH ; <nl>  <nl> + if (! p -> mm ) { <nl> + put_task_struct ( p ); <nl> + return - EPERM ; <nl> + } <nl> + <nl> /* <nl> * We need to look at the task state flags to figure out , <nl> * whether the task is exiting . To protect against the do_exit
struct fib_table * fib_trie_unmerge ( struct fib_table * oldtb ) <nl> local_l = fib_find_node ( lt , & local_tp , l -> key ); <nl>  <nl> if ( fib_insert_alias ( lt , local_tp , local_l , new_fa , <nl> - NULL , l -> key )) <nl> + NULL , l -> key )) { <nl> + kmem_cache_free ( fn_alias_kmem , new_fa ); <nl> goto out ; <nl> + } <nl> } <nl>  <nl> /* stop loop if key wrapped back to 0 */
static int wl1271_init_ieee80211 ( struct wl1271 * wl ) <nl> wl -> hw -> wiphy -> max_sched_scan_ie_len = WL1271_CMD_TEMPL_MAX_SIZE - <nl> sizeof ( struct ieee80211_header ); <nl>  <nl> - wl -> hw -> wiphy -> max_remain_on_channel_duration = 5000 ; <nl> + wl -> hw -> wiphy -> max_remain_on_channel_duration = 30000 ; <nl>  <nl> wl -> hw -> wiphy -> flags |= WIPHY_FLAG_AP_UAPSD | <nl> WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL |
static int tilcdc_irq_postinstall ( struct drm_device * dev ) <nl> struct tilcdc_drm_private * priv = dev -> dev_private ; <nl>  <nl> /* enable FIFO underflow irq : */ <nl> - if ( priv -> rev == 1 ) { <nl> + if ( priv -> rev == 1 ) <nl> tilcdc_set ( dev , LCDC_RASTER_CTRL_REG , LCDC_V1_UNDERFLOW_INT_ENA ); <nl> - } else { <nl> + else <nl> tilcdc_set ( dev , LCDC_INT_ENABLE_SET_REG , LCDC_V2_UNDERFLOW_INT_ENA ); <nl> - } <nl>  <nl> return 0 ; <nl> }
have_busfreq : <nl> static int powernow_k6_cpu_exit ( struct cpufreq_policy * policy ) <nl> { <nl> unsigned int i ; <nl> - for ( i = 0 ; i < 8 ; i ++) { <nl> - if ( i == max_multiplier ) <nl> + <nl> + for ( i = 0 ; ( clock_ratio [ i ]. frequency != CPUFREQ_TABLE_END ); i ++) { <nl> + if ( clock_ratio [ i ]. driver_data == max_multiplier ) <nl> powernow_k6_target ( policy , i ); <nl> } <nl> return 0 ;
int rt28xx_sta_ioctl ( IN struct net_device * net_dev , <nl> Status = <nl> copy_to_user ( erq -> pointer , pAd -> nickname , <nl> erq -> length ); <nl> + if ( Status ) <nl> + Status = - EFAULT ; <nl> break ; <nl> } <nl> case SIOCGIWRATE : /* get default bit rate ( bps ) */
static int snd_line6_pcm_free ( struct snd_device * device ) <nl> */ <nl> static void pcm_disconnect_substream ( struct snd_pcm_substream * substream ) <nl> { <nl> - if ( substream -> runtime && snd_pcm_running ( substream )) <nl> + if ( substream -> runtime && snd_pcm_running ( substream )) { <nl> + snd_pcm_stream_lock_irq ( substream ); <nl> snd_pcm_stop ( substream , SNDRV_PCM_STATE_DISCONNECTED ); <nl> + snd_pcm_stream_unlock_irq ( substream ); <nl> + } <nl> } <nl>  <nl> /*
static void fsl_ssi_config ( struct fsl_ssi_private * ssi_private , bool enable , <nl> * ( online configuration ) <nl> */ <nl> if ( enable ) { <nl> - regmap_update_bits ( regs , CCSR_SSI_SIER , vals -> sier , vals -> sier ); <nl> regmap_update_bits ( regs , CCSR_SSI_SRCR , vals -> srcr , vals -> srcr ); <nl> regmap_update_bits ( regs , CCSR_SSI_STCR , vals -> stcr , vals -> stcr ); <nl> + regmap_update_bits ( regs , CCSR_SSI_SIER , vals -> sier , vals -> sier ); <nl> } else { <nl> u32 sier ; <nl> u32 srcr ;
static inline int free_area ( unsigned long pfn , unsigned long end , char * s ) <nl> static inline void poison_init_mem ( void * s , size_t count ) <nl> { <nl> u32 * p = ( u32 *) s ; <nl> - while (( count = count - 4 )) <nl> + for (; count != 0 ; count -= 4 ) <nl> * p ++ = 0xe7fddef0 ; <nl> } <nl> 
static int set_agc_rf ( struct drxk_state * state , <nl> } <nl>  <nl> /* Set TOP , only if IF - AGC is in AUTO mode */ <nl> - if ( p_if_agc_settings -> ctrl_mode == DRXK_AGC_CTRL_AUTO ) <nl> + if ( p_if_agc_settings -> ctrl_mode == DRXK_AGC_CTRL_AUTO ) { <nl> status = write16 ( state , <nl> SCU_RAM_AGC_IF_IACCU_HI_TGT_MAX__A , <nl> p_agc_cfg -> top ); <nl> if ( status < 0 ) <nl> goto error ; <nl> + } <nl>  <nl> /* Cut - Off current */ <nl> status = write16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A ,
int drbd_merge_bvec ( struct request_queue * q , struct bvec_merge_data * bvm , struct <nl> struct request_queue * const b = <nl> device -> ldev -> backing_bdev -> bd_disk -> queue ; <nl> if ( b -> merge_bvec_fn ) { <nl> + bvm -> bi_bdev = device -> ldev -> backing_bdev ; <nl> backing_limit = b -> merge_bvec_fn ( b , bvm , bvec ); <nl> limit = min ( limit , backing_limit ); <nl> }
static int qat_hal_init_esram ( struct icp_qat_fw_loader_handle * handle ) <nl> unsigned int csr_val ; <nl> int times = 30 ; <nl>  <nl> + if ( handle -> pci_dev -> device == ADF_C3XXX_PCI_DEVICE_ID ) <nl> + return 0 ; <nl> + <nl> csr_val = ADF_CSR_RD ( csr_addr , 0 ); <nl> if (( csr_val & ESRAM_AUTO_TINIT ) && ( csr_val & ESRAM_AUTO_TINIT_DONE )) <nl> return 0 ;
static struct spi_board_info mc13783_dev __initdata = { <nl> . bus_num = 1 , <nl> . chip_select = 0 , <nl> . platform_data = & mc13783_pdata , <nl> + . irq = IOMUX_TO_IRQ ( MX31_PIN_GPIO1_3 ), <nl> }; <nl>  <nl> static int mx31lilly_baseboard ;
static struct phy * tegra_xusb_padctl_xlate ( struct device * dev , <nl> if ( args -> args_count <= 0 ) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> - if ( index > ARRAY_SIZE ( padctl -> phys )) <nl> + if ( index >= ARRAY_SIZE ( padctl -> phys )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> return padctl -> phys [ index ];
static int __devinit w83627ehf_probe ( struct platform_device * pdev ) <nl> mutex_init (& data -> lock ); <nl> mutex_init (& data -> update_lock ); <nl> data -> name = w83627ehf_device_names [ sio_data -> kind ]; <nl> + data -> bank = 0xff ; /* Force initial bank selection */ <nl> platform_set_drvdata ( pdev , data ); <nl>  <nl> /* 627EHG and 627EHF have 10 voltage inputs ; 627DHG and 667HG have 9 */
int btrfs_commit_transaction ( struct btrfs_trans_handle * trans , <nl> spin_unlock (& root -> fs_info -> trans_lock ); <nl> } <nl>  <nl> - if ( now < cur_trans -> start_time || now - cur_trans -> start_time < 1 ) <nl> + if (! btrfs_test_opt ( root , SSD ) && <nl> + ( now < cur_trans -> start_time || now - cur_trans -> start_time < 1 )) <nl> should_grow = 1 ; <nl>  <nl> do {
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
static dma_addr_t intel_map_page ( struct device * dev , struct page * page , <nl> struct dma_attrs * attrs ) <nl> { <nl> return __intel_map_single ( dev , page_to_phys ( page ) + offset , size , <nl> - dir , to_pci_dev ( dev )-> dma_mask ); <nl> + dir , * dev -> dma_mask ); <nl> } <nl>  <nl> static void flush_unmaps ( void )
static void ironlake_irq_uninstall ( struct drm_device * dev ) <nl> I915_WRITE ( GTIMR , 0xffffffff ); <nl> I915_WRITE ( GTIER , 0x0 ); <nl> I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> + <nl> + I915_WRITE ( SDEIMR , 0xffffffff ); <nl> + I915_WRITE ( SDEIER , 0x0 ); <nl> + I915_WRITE ( SDEIIR , I915_READ ( SDEIIR )); <nl> } <nl>  <nl> static void i915_driver_irq_uninstall ( struct drm_device * dev )
static struct pci_driver adv_pci1710_pci_driver = { <nl> module_comedi_pci_driver ( adv_pci1710_driver , adv_pci1710_pci_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi : Advantech PCI - 1710 Series Multifunction DAS Cards "); <nl> MODULE_LICENSE (" GPL ");
EXPORT_SYMBOL ( genphy_read_status ); <nl>  <nl> static int genphy_config_init ( struct phy_device * phydev ) <nl> { <nl> - u32 val ; <nl> + int val ; <nl> u32 features ; <nl>  <nl> /* For now , I ' ll claim that the generic driver supports
int scrub_enumerate_chunks ( struct scrub_ctx * sctx , <nl> btrfs_put_block_group ( cache ); <nl> if ( ret ) <nl> break ; <nl> - if ( atomic64_read (& dev_replace -> num_write_errors ) > 0 ) { <nl> + if ( is_dev_replace && <nl> + atomic64_read (& dev_replace -> num_write_errors ) > 0 ) { <nl> ret = - EIO ; <nl> break ; <nl> }
static int __init init_balloon_drv ( void ) <nl> return vmbus_driver_register (& balloon_drv ); <nl> } <nl>  <nl> - static void exit_balloon_drv ( void ) <nl> -{ <nl> - <nl> - vmbus_driver_unregister (& balloon_drv ); <nl> -} <nl> - <nl> module_init ( init_balloon_drv ); <nl> - module_exit ( exit_balloon_drv ); <nl>  <nl> MODULE_DESCRIPTION (" Hyper - V Balloon "); <nl> MODULE_VERSION ( HV_DRV_VERSION );
void pci_bus_add_device ( struct pci_dev * dev ) <nl>  <nl> dev -> match_driver = true ; <nl> retval = device_attach (& dev -> dev ); <nl> - if ( retval < 0 ) { <nl> + if ( retval < 0 && retval != - EPROBE_DEFER ) { <nl> dev_warn (& dev -> dev , " device attach failed (% d )\ n ", retval ); <nl> pci_proc_detach_device ( dev ); <nl> pci_remove_sysfs_dev_files ( dev );
static irqreturn_t ads7846_irq ( int irq , void * handle ) <nl> msecs_to_jiffies ( TS_POLL_PERIOD )); <nl> } <nl>  <nl> - if ( ts -> pendown ) { <nl> + if ( ts -> pendown && ! ts -> stopped ) { <nl> struct input_dev * input = ts -> input ; <nl>  <nl> input_report_key ( input , BTN_TOUCH , 0 );
nouveau_sgdma_populate ( struct ttm_backend * be , unsigned long num_pages , <nl> return - ENOMEM ; <nl>  <nl> nvbe -> ttm_alloced = kmalloc ( sizeof ( bool ) * num_pages , GFP_KERNEL ); <nl> - if (! nvbe -> ttm_alloced ) <nl> + if (! nvbe -> ttm_alloced ) { <nl> + kfree ( nvbe -> pages ); <nl> + nvbe -> pages = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> nvbe -> nr_pages = 0 ; <nl> while ( num_pages --) {
void irq_domain_free_irqs_common ( struct irq_domain * domain , unsigned int virq , <nl> } <nl> irq_domain_free_irqs_parent ( domain , virq , nr_irqs ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( irq_domain_free_irqs_common ); <nl>  <nl> /** <nl> * irq_domain_free_irqs_top - Clear handler and handler data , clear irqdata and free parent
static int <nl> lpfc_parse_vpd ( struct lpfc_hba * phba , uint8_t * vpd , int len ) <nl> { <nl> uint8_t lenlo , lenhi ; <nl> - uint32_t Length ; <nl> + int Length ; <nl> int i , j ; <nl> int finished = 0 ; <nl> int index = 0 ;
static void ipu_plane_dpms ( struct ipu_plane * ipu_plane , int mode ) <nl>  <nl> ipu_idmac_put ( ipu_plane -> ipu_ch ); <nl> ipu_dmfc_put ( ipu_plane -> dmfc ); <nl> - ipu_dp_put ( ipu_plane -> dp ); <nl> + if ( ipu_plane -> dp ) <nl> + ipu_dp_put ( ipu_plane -> dp ); <nl> } <nl> } <nl> 
static void copy_from_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy <nl>  <nl> static void copy_to_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy_info * p , int dir ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> sel , & xp -> selector , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & xp -> lft , sizeof ( p -> lft )); <nl> memcpy (& p -> curlft , & xp -> curlft , sizeof ( p -> curlft ));
int mei_cl_notify_get ( struct mei_cl * cl , bool block , bool * notify_ev ) <nl>  <nl> dev = cl -> dev ; <nl>  <nl> + if (! dev -> hbm_f_ev_supported ) { <nl> + cl_dbg ( dev , cl , " notifications not supported \ n "); <nl> + return - EOPNOTSUPP ; <nl> + } <nl> + <nl> if (! mei_cl_is_connected ( cl )) <nl> return - ENODEV ; <nl> 
void clear_local_APIC ( void ) <nl> } <nl>  <nl> /* lets not touch this if we didn ' t frob it */ <nl> -# if defined ( CONFIG_X86_MCE_P4THERMAL ) || defined ( X86_MCE_INTEL ) <nl> +# if defined ( CONFIG_X86_MCE_P4THERMAL ) || defined ( CONFIG_X86_MCE_INTEL ) <nl> if ( maxlvt >= 5 ) { <nl> v = apic_read ( APIC_LVTTHMR ); <nl> apic_write ( APIC_LVTTHMR , v | APIC_LVT_MASKED );
static void iwl3945_init_hw_rates ( struct iwl_priv * priv , <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < IWL_RATE_COUNT ; i ++) { <nl> + for ( i = 0 ; i < IWL_RATE_COUNT_LEGACY ; i ++) { <nl> rates [ i ]. bitrate = iwl3945_rates [ i ]. ieee * 5 ; <nl> rates [ i ]. hw_value = i ; /* Rate scaling will work on indexes */ <nl> rates [ i ]. hw_value_short = i ;
static int r820t_signal ( struct dvb_frontend * fe , u16 * strength ) <nl>  <nl> /* A higher gain at LNA means a lower signal strength */ <nl> * strength = ( 45 - rc ) << 4 | 0xff ; <nl> + if (* strength == 0xff ) <nl> + * strength = 0 ; <nl> } else { <nl> * strength = 0 ; <nl> }
static void __wa_populate_buf_in_urb_isoc ( struct wahc * wa , struct wa_xfer * xfer , <nl> wa -> buf_in_urb -> transfer_dma = xfer -> urb -> transfer_dma + <nl> xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. offset ; <nl> wa -> buf_in_urb -> transfer_buffer_length = <nl> - xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. length ; <nl> + xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. actual_length ; <nl> wa -> buf_in_urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> wa -> buf_in_urb -> transfer_buffer = NULL ; <nl> wa -> buf_in_urb -> sg = NULL ;
static void gfs2_write_super ( struct super_block * sb ) <nl> static int gfs2_sync_fs ( struct super_block * sb , int wait ) <nl> { <nl> sb -> s_dirt = 0 ; <nl> - if ( wait ) <nl> + if ( wait && sb -> s_fs_info ) <nl> gfs2_log_flush ( sb -> s_fs_info , NULL ); <nl> return 0 ; <nl> }
static void fc_exch_rrq_resp ( struct fc_seq * sp , struct fc_frame * fp , void * arg ) <nl> if ( IS_ERR ( fp )) { <nl> int err = PTR_ERR ( fp ); <nl>  <nl> - if ( err == - FC_EX_CLOSED ) <nl> + if ( err == - FC_EX_CLOSED || err == - FC_EX_TIMEOUT ) <nl> goto cleanup ; <nl> FC_DBG (" Cannot process RRQ , because of frame error % d \ n ", err ); <nl> return ;
static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
static int uio_pdrv_remove ( struct platform_device * pdev ) <nl>  <nl> uio_unregister_device ( pdata -> uioinfo ); <nl>  <nl> + kfree ( pdata ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int soc15_common_set_clockgating_state ( void * handle , <nl> { <nl> struct amdgpu_device * adev = ( struct amdgpu_device *) handle ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> switch ( adev -> asic_type ) { <nl> case CHIP_VEGA10 : <nl> nbio_v6_1_update_medium_grain_clock_gating ( adev ,
static int ibmvscsis_drop_nexus ( struct ibmvscsis_tport * tport ) <nl> /* <nl> * Release the SCSI I_T Nexus to the emulated ibmvscsis Target Port <nl> */ <nl> + target_wait_for_sess_cmds ( se_sess ); <nl> + transport_deregister_session_configfs ( se_sess ); <nl> transport_deregister_session ( se_sess ); <nl> tport -> ibmv_nexus = NULL ; <nl> kfree ( nexus );
static int davinci_pcm_open ( struct snd_pcm_substream * substream ) <nl> int ret = 0 ; <nl>  <nl> snd_soc_set_runtime_hwparams ( substream , & davinci_pcm_hardware ); <nl> + /* ensure that buffer size is a multiple of period size */ <nl> + ret = snd_pcm_hw_constraint_integer ( runtime , <nl> + SNDRV_PCM_HW_PARAM_PERIODS ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl>  <nl> prtd = kzalloc ( sizeof ( struct davinci_runtime_data ), GFP_KERNEL ); <nl> if ( prtd == NULL )
int kvm_vcpu_ioctl_config_tlb ( struct kvm_vcpu * vcpu , <nl>  <nl> num_pages = DIV_ROUND_UP ( cfg -> array + array_len - 1 , PAGE_SIZE ) - <nl> cfg -> array / PAGE_SIZE ; <nl> - pages = kmalloc ( sizeof ( struct page *) * num_pages , GFP_KERNEL ); <nl> + pages = kmalloc_array ( num_pages , sizeof (* pages ), GFP_KERNEL ); <nl> if (! pages ) <nl> return - ENOMEM ; <nl> 
int radeon_atom_get_mclk_range_table ( struct radeon_device * rdev , <nl> p = ( u8 *) vram_module -> asMemTiming ; <nl> for ( i = 0 ; i < mclk_range_table -> num_entries ; i ++) { <nl> format = ( ATOM_MEMORY_TIMING_FORMAT *) p ; <nl> - mclk_range_table -> mclk [ i ] = format -> ulClkRange ; <nl> + mclk_range_table -> mclk [ i ] = le32_to_cpu ( format -> ulClkRange ); <nl> p += mem_timing_size ; <nl> } <nl> } else
int ocfs2_fiemap ( struct inode * inode , struct fiemap_extent_info * fieinfo , <nl> & hole_size , & rec , & is_last ); <nl> if ( ret ) { <nl> mlog_errno ( ret ); <nl> - goto out ; <nl> + goto out_unlock ; <nl> } <nl>  <nl> if ( rec . e_blkno == 0ULL ) {
struct io_context { <nl> /* <nl> * For request batching <nl> */ <nl> - unsigned long last_waited ; /* Time last woken after wait for request */ <nl> int nr_batch_requests ; /* Number of requests left in the batch */ <nl> + unsigned long last_waited ; /* Time last woken after wait for request */ <nl>  <nl> struct radix_tree_root radix_root ; <nl> struct hlist_head cic_list ;
int udf_ioctl ( struct inode * inode , struct file * filp , unsigned int cmd , <nl> static int udf_release_file ( struct inode * inode , struct file * filp ) <nl> { <nl> if ( filp -> f_mode & FMODE_WRITE ) { <nl> + mutex_lock (& inode -> i_mutex ); <nl> lock_kernel (); <nl> udf_discard_prealloc ( inode ); <nl> unlock_kernel (); <nl> + mutex_unlock (& inode -> i_mutex ); <nl> } <nl> return 0 ; <nl> }
static void lm8333_key_handler ( struct lm8333 * lm8333 ) <nl> return ; <nl> } <nl>  <nl> - for ( i = 0 ; keys [ i ] && i < LM8333_FIFO_TRANSFER_SIZE ; i ++) { <nl> + for ( i = 0 ; i < LM8333_FIFO_TRANSFER_SIZE && keys [ i ]; i ++) { <nl> pressed = keys [ i ] & 0x80 ; <nl> code = keys [ i ] & 0x7f ; <nl> 
static int __init r8a66597_probe ( struct platform_device * pdev ) <nl>  <nl> r8a66597 -> ep0_req = r8a66597_alloc_request (& r8a66597 -> ep [ 0 ]. ep , <nl> GFP_KERNEL ); <nl> - if ( r8a66597 -> ep0_req == NULL ) <nl> + if ( r8a66597 -> ep0_req == NULL ) { <nl> + ret = - ENOMEM ; <nl> goto clean_up3 ; <nl> + } <nl> r8a66597 -> ep0_req -> complete = nop_completion ; <nl>  <nl> ret = usb_add_gadget_udc (& pdev -> dev , & r8a66597 -> gadget );
static inline int unshare_utsname ( unsigned long unshare_flags , <nl>  <nl> static inline int copy_utsname ( int flags , struct task_struct * tsk ) <nl> { <nl> + if ( flags & CLONE_NEWUTS ) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> static inline void put_uts_ns ( struct uts_namespace * ns )
static int spi_qup_remove ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> ret = pm_runtime_get_sync (& pdev -> dev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> return ret ; <nl>  <nl> ret = spi_qup_set_state ( controller , QUP_STATE_RESET );
static struct pci_device_id azx_ids [] = { <nl> { PCI_DEVICE ( 0x10de , 0x044b ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x055c ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x055d ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> + { PCI_DEVICE ( 0x10de , 0x0590 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0774 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0775 ), . driver_data = AZX_DRIVER_NVIDIA }, <nl> { PCI_DEVICE ( 0x10de , 0x0776 ), . driver_data = AZX_DRIVER_NVIDIA },
static int f2fs_write_data_pages ( struct address_space * mapping , <nl> available_free_memory ( sbi , DIRTY_DENTS )) <nl> goto skip_write ; <nl>  <nl> + /* during POR , we don ' t need to trigger writepage at all . */ <nl> + if ( unlikely ( is_sbi_flag_set ( sbi , SBI_POR_DOING ))) <nl> + goto skip_write ; <nl> + <nl> diff = nr_pages_to_write ( sbi , DATA , wbc ); <nl>  <nl> if (! S_ISDIR ( inode -> i_mode )) {
static int cx22700_set_tps ( struct cx22700_state * state , <nl>  <nl> cx22700_writereg ( state , 0x04 , val ); <nl>  <nl> + if ( p -> code_rate_HP - FEC_1_2 >= sizeof ( fec_tab ) || <nl> + p -> code_rate_LP - FEC_1_2 >= sizeof ( fec_tab )) <nl> + return - EINVAL ; <nl> val = fec_tab [ p -> code_rate_HP - FEC_1_2 ] << 3 ; <nl> val |= fec_tab [ p -> code_rate_LP - FEC_1_2 ]; <nl> 
static void cfq_dispatch_insert ( struct request_queue * q , struct request * rq ) <nl>  <nl> cfq_log_cfqq ( cfqd , cfqq , " dispatch_insert "); <nl>  <nl> + cfqq -> next_rq = cfq_find_next_rq ( cfqd , cfqq , rq ); <nl> cfq_remove_request ( rq ); <nl> cfqq -> dispatched ++; <nl> elv_dispatch_sort ( q , rq );
static int st_fdma_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> fdev -> slim_rproc = st_slim_rproc_alloc ( pdev , fdev -> fw_name ); <nl> - if (! fdev -> slim_rproc ) { <nl> + if ( IS_ERR ( fdev -> slim_rproc )) { <nl> ret = PTR_ERR ( fdev -> slim_rproc ); <nl> dev_err (& pdev -> dev , " slim_rproc_alloc failed (% d )\ n ", ret ); <nl> goto err ;
static int vpfe_enum_input ( struct file * file , void * priv , <nl> return - EINVAL ; <nl> } <nl> sdinfo = & vpfe_dev -> cfg -> sub_devs [ subdev ]; <nl> - memcpy ( inp , & sdinfo -> inputs [ index ], sizeof ( struct v4l2_input )); <nl> + * inp = sdinfo -> inputs [ index ]; <nl> return 0 ; <nl> } <nl> 
struct kvm_pit * kvm_create_pit ( struct kvm * kvm , u32 flags ) <nl> pit -> wq = create_singlethread_workqueue (" kvm - pit - wq "); <nl> if (! pit -> wq ) { <nl> mutex_unlock (& pit -> pit_state . lock ); <nl> + kvm_free_irq_source_id ( kvm , pit -> irq_source_id ); <nl> kfree ( pit ); <nl> return NULL ; <nl> }
static int v9fs_vfs_readlink ( struct dentry * dentry , char __user * buffer , <nl> int ret ; <nl> char * link = __getname (); <nl>  <nl> - if ( strlen ( link ) < buflen ) <nl> - buflen = strlen ( link ); <nl> + if ( buflen > PATH_MAX ) <nl> + buflen = PATH_MAX ; <nl>  <nl> dprintk ( DEBUG_VFS , " dentry : % s (% p )\ n ", dentry -> d_iname , dentry ); <nl> 
int datagram_send_ctl ( struct net * net , struct sock * sk , <nl>  <nl> if ( addr_type != IPV6_ADDR_ANY ) { <nl> int strict = __ipv6_addr_src_scope ( addr_type ) <= IPV6_ADDR_SCOPE_LINKLOCAL ; <nl> - if (! inet_sk ( sk )-> transparent && <nl> + if (!( inet_sk ( sk )-> freebind || inet_sk ( sk )-> transparent ) && <nl> ! ipv6_chk_addr ( net , & src_info -> ipi6_addr , <nl> strict ? dev : NULL , 0 )) <nl> err = - EINVAL ;
listxattr ( struct dentry * d , char __user * list , size_t size ) <nl> error = d -> d_inode -> i_op -> listxattr ( d , klist , size ); <nl> } else { <nl> error = security_inode_listsecurity ( d -> d_inode , klist , size ); <nl> - if ( size && error >= size ) <nl> + if ( size && error > size ) <nl> error = - ERANGE ; <nl> } <nl> if ( error > 0 ) {
svc_tcp_accept ( struct svc_sock * svsk ) <nl> serv -> sv_name ); <nl> printk ( KERN_NOTICE <nl> "% s : last TCP connect from % s \ n ", <nl> - serv -> sv_name , buf ); <nl> + serv -> sv_name , __svc_print_addr ( sin , <nl> + buf , sizeof ( buf ))); <nl> } <nl> /* <nl> * Always select the oldest socket . It ' s not fair ,
nfulnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl> ret = - EINVAL ; <nl> break ; <nl> } <nl> + <nl> + if (! inst ) <nl> + goto out ; <nl> } else { <nl> if (! inst ) { <nl> UDEBUG (" no config command , and no instance for " <nl> nfulnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl>  <nl> out_put : <nl> instance_put ( inst ); <nl> + out : <nl> return ret ; <nl> } <nl> 
static int btrfs_set_acl ( struct btrfs_trans_handle * trans , <nl> ret = posix_acl_equiv_mode ( acl , & inode -> i_mode ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + if ( ret == 0 ) <nl> + acl = NULL ; <nl> } <nl> ret = 0 ; <nl> break ;
static void dma_pte_free_level ( struct dmar_domain * domain , int level , <nl>  <nl> /* If range covers entire pagetable , free it */ <nl> if (!( start_pfn > level_pfn || <nl> - last_pfn < level_pfn + level_size ( level ))) { <nl> + last_pfn < level_pfn + level_size ( level ) - 1 )) { <nl> dma_clear_pte ( pte ); <nl> domain_flush_cache ( domain , pte , sizeof (* pte )); <nl> free_pgtable_page ( level_pte );
static int max31790_read_pwm ( struct device * dev , u32 attr , int channel , <nl> long * val ) <nl> { <nl> struct max31790_data * data = max31790_update_device ( dev ); <nl> - u8 fan_config = data -> fan_config [ channel ]; <nl> + u8 fan_config ; <nl>  <nl> if ( IS_ERR ( data )) <nl> return PTR_ERR ( data ); <nl>  <nl> + fan_config = data -> fan_config [ channel ]; <nl> + <nl> switch ( attr ) { <nl> case hwmon_pwm_input : <nl> * val = data -> pwm [ channel ] >> 8 ;
static int logger_release ( struct inode * ignored , struct file * file ) <nl> { <nl> if ( file -> f_mode & FMODE_READ ) { <nl> struct logger_reader * reader = file -> private_data ; <nl> + struct logger_log * log = reader -> log ; <nl> + <nl> + mutex_lock (& log -> mutex ); <nl> list_del (& reader -> list ); <nl> + mutex_unlock (& log -> mutex ); <nl> + <nl> kfree ( reader ); <nl> } <nl> 
struct gen_pool * devm_gen_pool_create ( struct device * dev , int min_alloc_order , <nl> struct gen_pool ** ptr , * pool ; <nl>  <nl> ptr = devres_alloc ( devm_gen_pool_release , sizeof (* ptr ), GFP_KERNEL ); <nl> + if (! ptr ) <nl> + return NULL ; <nl>  <nl> pool = gen_pool_create ( min_alloc_order , nid ); <nl> if ( pool ) {
static int p54_tx ( struct ieee80211_hw * dev , struct sk_buff * skb ) <nl> struct p54_common * priv = dev -> priv ; <nl> struct p54_hdr * hdr ; <nl> struct p54_tx_data * txhdr ; <nl> - size_t padding , len , tim_len ; <nl> + size_t padding , len , tim_len = 0 ; <nl> int i , j , ridx ; <nl> u16 hdr_flags = 0 , aid = 0 ; <nl> u8 rate , queue ;
static int corgi_bl_update_status ( struct backlight_device * bd ) <nl>  <nl> if ( corgibl_flags & CORGIBL_SUSPENDED ) <nl> intensity = 0 ; <nl> - if ( corgibl_flags & CORGIBL_BATTLOW ) <nl> - intensity &= lcd -> limit_mask ; <nl> + <nl> + if (( corgibl_flags & CORGIBL_BATTLOW ) && intensity > lcd -> limit_mask ) <nl> + intensity = lcd -> limit_mask ; <nl>  <nl> return corgi_bl_set_intensity ( lcd , intensity ); <nl> }
static bool check_device_tree ( struct ath6kl * ar ) <nl> board_filename , ret ); <nl> continue ; <nl> } <nl> + of_node_put ( node ); <nl> return true ; <nl> } <nl> return false ;
static void skl_power_well_post_enable ( struct drm_i915_private * dev_priv , <nl> 1 << PIPE_C | 1 << PIPE_B ); <nl> } <nl>  <nl> - if ( power_well -> data == SKL_DISP_PW_1 ) <nl> + if ( power_well -> data == SKL_DISP_PW_1 ) { <nl> + intel_prepare_ddi ( dev ); <nl> gen8_irq_power_well_post_enable ( dev_priv , 1 << PIPE_A ); <nl> + } <nl> } <nl>  <nl> static void hsw_set_power_well ( struct drm_i915_private * dev_priv ,
static void eeh_handle_special_event ( void ) <nl>  <nl> /* Notify all devices to be down */ <nl> eeh_pe_state_clear ( pe , EEH_PE_PRI_BUS ); <nl> + eeh_pe_dev_traverse ( pe , <nl> + eeh_report_failure , NULL ); <nl> bus = eeh_pe_bus_get ( phb_pe ); <nl> if (! bus ) { <nl> pr_err ("% s : Cannot find PCI bus for " <nl> static void eeh_handle_special_event ( void ) <nl> pe -> addr ); <nl> break ; <nl> } <nl> - eeh_pe_dev_traverse ( pe , <nl> - eeh_report_failure , NULL ); <nl> pci_hp_remove_devices ( bus ); <nl> } <nl> pci_unlock_rescan_remove ();
static void b43_sdio_remove ( struct sdio_func * func ) <nl> struct b43_sdio * sdio = sdio_get_drvdata ( func ); <nl>  <nl> ssb_bus_unregister (& sdio -> ssb ); <nl> + sdio_claim_host ( func ); <nl> sdio_disable_func ( func ); <nl> + sdio_release_host ( func ); <nl> kfree ( sdio ); <nl> sdio_set_drvdata ( func , NULL ); <nl> }
static void amdgpu_gem_va_update_vm ( struct amdgpu_device * adev , <nl> if ( domain == AMDGPU_GEM_DOMAIN_CPU ) <nl> goto error_unreserve ; <nl> } <nl> + r = amdgpu_vm_update_page_directory ( adev , bo_va -> vm ); <nl> + if ( r ) <nl> + goto error_unreserve ; <nl>  <nl> r = amdgpu_vm_clear_freed ( adev , bo_va -> vm ); <nl> if ( r )
int dmar_set_interrupt ( struct intel_iommu * iommu ) <nl> return 0 ; <nl>  <nl> irq = create_irq (); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> pr_err (" IOMMU : no free vectors \ n "); <nl> return - EINVAL ; <nl> }
static struct zonelist * zonelist_policy ( unsigned int __nocast gfp , struct mempol <nl> case MPOL_BIND : <nl> /* Lower zones don ' t get a policy applied */ <nl> /* Careful : current -> mems_allowed might have moved */ <nl> - if ( gfp >= policy_zone ) <nl> + if (( gfp & GFP_ZONEMASK ) >= policy_zone ) <nl> if ( cpuset_zonelist_valid_mems_allowed ( policy -> v . zonelist )) <nl> return policy -> v . zonelist ; <nl> /* FALL THROUGH */
 <nl> int intel_sanitize_enable_execlists ( struct drm_device * dev , int enable_execlists ) <nl> { <nl> + WARN_ON ( i915 . enable_ppgtt == - 1 ); <nl> + <nl> if ( enable_execlists == 0 ) <nl> return 0 ; <nl> 
static int p9_mux_poll_start ( struct p9_conn * m ) <nl> } <nl>  <nl> if ( i >= ARRAY_SIZE ( p9_mux_poll_tasks )) { <nl> - if ( vptlast == NULL ) <nl> + if ( vptlast == NULL ) { <nl> + mutex_unlock (& p9_mux_task_lock ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> P9_DPRINTK ( P9_DEBUG_MUX , " put in proc % d \ n ", i ); <nl> list_add (& m -> mux_list , & vptlast -> mux_list );
void __cfg80211_disconnected ( struct net_device * dev , const u8 * ie , <nl> memset (& wrqu , 0 , sizeof ( wrqu )); <nl> wrqu . ap_addr . sa_family = ARPHRD_ETHER ; <nl> wireless_send_event ( dev , SIOCGIWAP , & wrqu , NULL ); <nl> + wdev -> wext . connect . ssid_len = 0 ; <nl> # endif <nl> } <nl> 
static int stb0899_send_diseqc_msg ( struct dvb_frontend * fe , struct dvb_diseqc_ma <nl> struct stb0899_state * state = fe -> demodulator_priv ; <nl> u8 reg , i ; <nl>  <nl> - if ( cmd -> msg_len > 8 ) <nl> + if ( cmd -> msg_len > sizeof ( cmd -> msg )) <nl> return - EINVAL ; <nl>  <nl> /* enable FIFO precharge */
static void ixgbe_free_irq ( struct ixgbe_adapter * adapter ) <nl>  <nl> i --; <nl> for (; i >= 0 ; i --) { <nl> + /* free only the irqs that were actually requested */ <nl> + if (! adapter -> q_vector [ i ]-> rxr_count && <nl> + ! adapter -> q_vector [ i ]-> txr_count ) <nl> + continue ; <nl> + <nl> free_irq ( adapter -> msix_entries [ i ]. vector , <nl> adapter -> q_vector [ i ]); <nl> }
static int xhci_plat_probe ( struct platform_device * pdev ) <nl> if ( ret ) <nl> goto dealloc_usb2_hcd ; <nl>  <nl> + device_enable_async_suspend (& pdev -> dev ); <nl> + <nl> return 0 ; <nl>  <nl> 
static int als_wait_for_data_ready ( struct device * dev ) <nl> ret = i2c_smbus_read_byte_data ( client , 0x86 ); <nl> } while (!( ret & 0x80 ) && retry --); <nl>  <nl> - if (! retry ) { <nl> + if ( retry < 0 ) { <nl> dev_warn ( dev , " timeout waiting for data ready \ n "); <nl> return - ETIMEDOUT ; <nl> }
int i40e_ndo_get_vf_config ( struct net_device * netdev , <nl> else <nl> ivi -> linkstate = IFLA_VF_LINK_STATE_DISABLE ; <nl> ivi -> spoofchk = vf -> spoofchk ; <nl> + ivi -> trusted = vf -> trusted ; <nl> ret = 0 ; <nl>  <nl> error_param :
static int ahash_prepare_alg ( struct ahash_alg * alg ) <nl> struct crypto_alg * base = & alg -> halg . base ; <nl>  <nl> if ( alg -> halg . digestsize > PAGE_SIZE / 8 || <nl> - alg -> halg . statesize > PAGE_SIZE / 8 ) <nl> + alg -> halg . statesize > PAGE_SIZE / 8 || <nl> + alg -> halg . statesize == 0 ) <nl> return - EINVAL ; <nl>  <nl> base -> cra_type = & crypto_ahash_type ;
static ssize_t port_fops_read ( struct file * filp , char __user * ubuf , <nl> if ( ret < 0 ) <nl> return ret ; <nl> } <nl> + /* Port got hot - unplugged . */ <nl> + if (! port -> guest_connected ) <nl> + return - ENODEV ; <nl> /* <nl> * We could ' ve received a disconnection message while we were <nl> * waiting for more data .
struct ifmcaddr6 <nl> struct ip6_sf_list * mca_sources ; <nl> struct ip6_sf_list * mca_tomb ; <nl> unsigned int mca_sfmode ; <nl> + unsigned char mca_crcount ; <nl> unsigned long mca_sfcount [ 2 ]; <nl> struct timer_list mca_timer ; <nl> unsigned mca_flags ; <nl> int mca_users ; <nl> atomic_t mca_refcnt ; <nl> spinlock_t mca_lock ; <nl> - unsigned char mca_crcount ; <nl> unsigned long mca_cstamp ; <nl> unsigned long mca_tstamp ; <nl> };
static ssize_t gpio_keys_attr_store_helper ( struct gpio_keys_drvdata * ddata , <nl> } <nl> } <nl>  <nl> + if ( i == ddata -> pdata -> nbuttons ) { <nl> + error = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> mutex_lock (& ddata -> disable_lock ); <nl>  <nl> for ( i = 0 ; i < ddata -> pdata -> nbuttons ; i ++) {
int ext4_punch_hole ( struct inode * inode , loff_t offset , loff_t length ) <nl> up_write (& EXT4_I ( inode )-> i_data_sem ); <nl> if ( IS_SYNC ( inode )) <nl> ext4_handle_sync ( handle ); <nl> + <nl> + /* Now release the pages again to reduce race window */ <nl> + if ( last_block_offset > first_block_offset ) <nl> + truncate_pagecache_range ( inode , first_block_offset , <nl> + last_block_offset ); <nl> + <nl> inode -> i_mtime = inode -> i_ctime = ext4_current_time ( inode ); <nl> ext4_mark_inode_dirty ( handle , inode ); <nl> out_stop :
static __be32 nfsd_set_fh_dentry ( struct svc_rqst * rqstp , struct svc_fh * fhp ) <nl> * fix that case easily . <nl> */ <nl> struct cred * new = prepare_creds (); <nl> - if (! new ) <nl> - return nfserrno (- ENOMEM ); <nl> + if (! new ) { <nl> + error = nfserrno (- ENOMEM ); <nl> + goto out ; <nl> + } <nl> new -> cap_effective = <nl> cap_raise_nfsd_set ( new -> cap_effective , <nl> new -> cap_permitted );
static int __devinit i2o_pci_probe ( struct pci_dev * pdev , <nl> i2o_pci_free ( c ); <nl>  <nl> free_controller : <nl> - i2o_iop_free ( c ); <nl> put_device ( c -> device . parent ); <nl> + i2o_iop_free ( c ); <nl>  <nl> disable : <nl> pci_disable_device ( pdev );
pptp_inbound_pkt ( struct sk_buff ** pskb , <nl> DEBUGP ("% s but no session \ n ", pptp_msg_name [ msg ]); <nl> break ; <nl> } <nl> - if ( info -> sstate != PPTP_CALL_IN_REP <nl> - && info -> sstate != PPTP_CALL_IN_CONF ) { <nl> + if ( info -> cstate != PPTP_CALL_IN_REP <nl> + && info -> cstate != PPTP_CALL_IN_CONF ) { <nl> DEBUGP ("% s but never sent IN_CALL_REPLY \ n ", <nl> pptp_msg_name [ msg ]); <nl> break ;
pte_t xen_make_pte ( unsigned long long pte ) <nl> if ( pte & 1 ) <nl> pte = phys_to_machine ( XPADDR ( pte )). maddr ; <nl>  <nl> + pte &= ~ _PAGE_PCD ; <nl> + <nl> return ( pte_t ){ pte , pte >> 32 }; <nl> } <nl>  <nl> pte_t xen_make_pte ( unsigned long pte ) <nl> if ( pte & _PAGE_PRESENT ) <nl> pte = phys_to_machine ( XPADDR ( pte )). maddr ; <nl>  <nl> + pte &= ~ _PAGE_PCD ; <nl> + <nl> return ( pte_t ){ pte }; <nl> } <nl> 
static int open ( struct tty_struct * tty , struct file * filp ) <nl> if ( info -> port . count == 1 ) { <nl> /* 1st open on this device , init hardware */ <nl> retval = startup ( info ); <nl> - if ( retval < 0 ) <nl> + if ( retval < 0 ) { <nl> + mutex_unlock (& info -> port . mutex ); <nl> goto cleanup ; <nl> + } <nl> } <nl> mutex_unlock (& info -> port . mutex ); <nl> retval = block_til_ready ( tty , filp , info );
static int i2c_register_adapter ( struct i2c_adapter * adap ) <nl>  <nl> dev_dbg (& adap -> dev , " adapter [% s ] registered \ n ", adap -> name ); <nl>  <nl> + pm_runtime_no_callbacks (& adap -> dev ); <nl> + <nl> # ifdef CONFIG_I2C_COMPAT <nl> res = class_compat_create_link ( i2c_adapter_compat_class , & adap -> dev , <nl> adap -> dev . parent );
static struct sg_table * omap_gem_map_dma_buf ( <nl> /* this should be after _get_paddr () to ensure we have pages attached */ <nl> omap_gem_dma_sync ( obj , dir ); <nl>  <nl> - out : <nl> - if ( ret ) <nl> - return ERR_PTR ( ret ); <nl> return sg ; <nl> + out : <nl> + kfree ( sg ); <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> static void omap_gem_unmap_dma_buf ( struct dma_buf_attachment * attachment ,
static int bsd_compress ( void * state , struct sk_buff * skb_in , struct sk_buff * skb <nl> db -> n_bits ++; <nl>  <nl> /* If output length is too large then this is an incompressible frame . */ <nl> - if (! skb_out || ( skb_out && skb_out -> len >= skb_in -> len )) { <nl> + if (! skb_out || skb_out -> len >= skb_in -> len ) { <nl> ++ db -> incomp_count ; <nl> db -> incomp_bytes += isize ; <nl> return 0 ;
static int ioc4_serial_remove_one ( struct ioc4_driver_data * idd ) <nl> if ( soft ) { <nl> free_irq ( control -> ic_irq , soft ); <nl> if ( soft -> is_ioc4_serial_addr ) { <nl> + iounmap ( soft -> is_ioc4_serial_addr ); <nl> release_region (( unsigned long ) <nl> soft -> is_ioc4_serial_addr , <nl> sizeof ( struct ioc4_serial )); <nl> out4 : <nl> out3 : <nl> kfree ( control ); <nl> out2 : <nl> + if ( serial ) <nl> + iounmap ( serial ); <nl> release_region ( tmp_addr1 , sizeof ( struct ioc4_serial )); <nl> out1 : <nl> 
static int get_strength ( struct drxk_state * state , u64 * strength ) <nl> return status ; <nl>  <nl> /* SCU c . o . c . */ <nl> - read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> + status = read16 ( state , SCU_RAM_AGC_RF_IACCU_HI_CO__A , & scu_coc ); <nl> if ( status < 0 ) <nl> return status ; <nl> 
static int musb_probe ( struct platform_device * pdev ) <nl> struct resource * iomem ; <nl> void __iomem * base ; <nl>  <nl> - iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - if (! iomem || irq <= 0 ) <nl> + if ( irq <= 0 ) <nl> return - ENODEV ; <nl>  <nl> + iomem = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> base = devm_ioremap_resource ( dev , iomem ); <nl> if ( IS_ERR ( base )) <nl> return PTR_ERR ( base );
static int ir_rc5_decode ( struct rc_dev * dev , struct ir_raw_event ev ) <nl> u32 scancode ; <nl> enum rc_type protocol ; <nl>  <nl> - if (!( dev -> enabled_protocols & ( RC_BIT_RC5 | RC_BIT_RC5X ))) <nl> + if (!( dev -> enabled_protocols & ( RC_BIT_RC5 | RC_BIT_RC5X | RC_BIT_RC5_SZ ))) <nl> return 0 ; <nl>  <nl> if (! is_timing_event ( ev )) {
static irqreturn_t interrupt_pcl816 ( int irq , void * d ) <nl> } <nl>  <nl> outb ( 0 , dev -> iobase + PCL816_CLRINT ); /* clear INT request */ <nl> - if (! dev -> irq || ! devpriv -> ai_cmd_running || ! devpriv -> int816_mode ) { <nl> + if (! devpriv -> ai_cmd_running || ! devpriv -> int816_mode ) { <nl> if ( devpriv -> irq_was_now_closed ) { <nl> devpriv -> irq_was_now_closed = 0 ; <nl> /* comedi_error ( dev ," last IRQ .."); */
static int cramfs_readpage ( struct file * file , struct page * page ) <nl> pgdata = kmap ( page ); <nl> if ( compr_len == 0 ) <nl> ; /* hole */ <nl> + else if ( compr_len > ( PAGE_CACHE_SIZE << 1 )) <nl> + printk ( KERN_ERR " cramfs : bad compressed blocksize % u \ n ", compr_len ); <nl> else { <nl> mutex_lock (& read_mutex ); <nl> bytes_filled = cramfs_uncompress_block ( pgdata ,
err_out_unregister : <nl>  <nl> err_unlock_policy : <nl> unlock_policy_rwsem_write ( cpu ); <nl> + free_cpumask_var ( policy -> related_cpus ); <nl> err_free_cpumask : <nl> free_cpumask_var ( policy -> cpus ); <nl> err_free_policy :
nvkm_ltc_tags_clear ( struct nvkm_ltc * ltc , u32 first , u32 count ) <nl>  <nl> BUG_ON (( first > limit ) || ( limit >= ltc -> num_tags )); <nl>  <nl> + mutex_lock (& ltc -> subdev . mutex ); <nl> ltc -> func -> cbc_clear ( ltc , first , limit ); <nl> ltc -> func -> cbc_wait ( ltc ); <nl> + mutex_unlock (& ltc -> subdev . mutex ); <nl> } <nl>  <nl> int
static int sbp2scsi_slave_configure ( struct scsi_device * sdev ) <nl> blk_queue_dma_alignment ( sdev -> request_queue , ( 512 - 1 )); <nl> sdev -> use_10_for_rw = 1 ; <nl>  <nl> + if ( sdev -> type == TYPE_ROM ) <nl> + sdev -> use_10_for_ms = 1 ; <nl> if ( sdev -> type == TYPE_DISK && <nl> lu -> workarounds & SBP2_WORKAROUND_MODE_SENSE_8 ) <nl> sdev -> skip_ms_page_8 = 1 ;
loff_t mem_lseek ( struct file * file , loff_t offset , int orig ) <nl> static int mem_release ( struct inode * inode , struct file * file ) <nl> { <nl> struct mm_struct * mm = file -> private_data ; <nl> - <nl> - mmput ( mm ); <nl> + if ( mm ) <nl> + mmput ( mm ); <nl> return 0 ; <nl> } <nl> 
static void vmbus_process_offer ( struct work_struct * work ) <nl> spin_lock_irqsave (& vmbus_connection . channel_lock , flags ); <nl> list_del (& newchannel -> listentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channel_lock , flags ); <nl> + kfree ( newchannel -> device_obj ); <nl>  <nl> free_channel ( newchannel ); <nl> } else {
void inet_initpeers ( void ) __init ; <nl> static inline void inetpeer_set_addr_v4 ( struct inetpeer_addr * iaddr , __be32 ip ) <nl> { <nl> iaddr -> a4 . addr = ip ; <nl> + iaddr -> a4 . vif = 0 ; <nl> iaddr -> family = AF_INET ; <nl> } <nl> 
rockchip_drm_framebuffer_init ( struct drm_device * dev , <nl>  <nl> rockchip_fb = rockchip_fb_alloc ( dev , mode_cmd , & obj , 1 ); <nl> if ( IS_ERR ( rockchip_fb )) <nl> - return NULL ; <nl> + return ERR_CAST ( rockchip_fb ); <nl>  <nl> return & rockchip_fb -> fb ; <nl> }
EXPORT_SYMBOL_GPL ( usbnet_resume ); <nl>  <nl> static int __init usbnet_init ( void ) <nl> { <nl> - /* compiler should optimize this out */ <nl> - BUILD_BUG_ON ( sizeof ((( struct sk_buff *) 0 )-> cb ) <nl> - < sizeof ( struct skb_data )); <nl> + /* Compiler should optimize this out . */ <nl> + BUILD_BUG_ON ( <nl> + FIELD_SIZEOF ( struct sk_buff , cb ) < sizeof ( struct skb_data )); <nl>  <nl> random_ether_addr ( node_id ); <nl> return 0 ;
static int i915_drm_freeze ( struct drm_device * dev ) <nl> * Disable CRTCs directly since we want to preserve sw state <nl> * for _thaw . <nl> */ <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> list_for_each_entry ( crtc , & dev -> mode_config . crtc_list , head ) <nl> dev_priv -> display . crtc_disable ( crtc ); <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl>  <nl> intel_modeset_suspend_hw ( dev ); <nl> }
static int __devexit i2c_hid_remove ( struct i2c_client * client ) <nl>  <nl> free_irq ( client -> irq , ihid ); <nl>  <nl> + if ( ihid -> bufsize ) <nl> + i2c_hid_free_buffers ( ihid ); <nl> + <nl> kfree ( ihid ); <nl>  <nl> return 0 ;
static ssize_t mdc_kuc_write ( struct file * file , <nl> /* for mockup below */ 2 * cfs_size_round ( sizeof (* hai )); <nl>  <nl> OBD_ALLOC ( lh , len ); <nl> + if (! lh ) <nl> + return - ENOMEM ; <nl>  <nl> lh -> kuc_magic = KUC_MAGIC ; <nl> lh -> kuc_transport = KUC_TRANSPORT_HSM ;
static const struct ich_laptop ich_laptop [] = { <nl> { 0x27DF , 0x0005 , 0x0280 }, /* ICH7 on Acer 5602WLMi */ <nl> { 0x27DF , 0x1025 , 0x0110 }, /* ICH7 on Acer 3682WLMi */ <nl> { 0x27DF , 0x1043 , 0x1267 }, /* ICH7 on Asus W5F */ <nl> + { 0x27DF , 0x103C , 0x30A1 }, /* ICH7 on HP Compaq nc2400 */ <nl> { 0x24CA , 0x1025 , 0x0061 }, /* ICH4 on ACER Aspire 2023WLMi */ <nl> /* end marker */ <nl> { 0 , }
static struct rfkill * amilo_rfkill_dev ; <nl>  <nl> static int __devinit amilo_rfkill_probe ( struct platform_device * device ) <nl> { <nl> + int rc ; <nl> const struct dmi_system_id * system_id = <nl> dmi_first_match ( amilo_rfkill_id_table ); <nl> - int rc ; <nl> + <nl> + if (! system_id ) <nl> + return - ENXIO ; <nl>  <nl> amilo_rfkill_dev = rfkill_alloc ( KBUILD_MODNAME , & device -> dev , <nl> RFKILL_TYPE_WLAN ,
static struct pci_driver cciss_pci_driver = { <nl> */ <nl> static int __init cciss_init ( void ) <nl> { <nl> + /* <nl> + * The hardware requires that commands are aligned on a 64 - bit <nl> + * boundary . Given that we use pci_alloc_consistent () to allocate an <nl> + * array of them , the size must be a multiple of 8 bytes . <nl> + */ <nl> + BUILD_BUG_ON ( sizeof ( CommandList_struct ) % 8 ); <nl> + <nl> printk ( KERN_INFO DRIVER_NAME "\ n "); <nl>  <nl> /* Register for our PCI devices */
xfs_uuid_mount ( <nl> uuid_t * uuid = & mp -> m_sb . sb_uuid ; <nl> int hole , i ; <nl>  <nl> + /* Publish UUID in struct super_block */ <nl> + BUILD_BUG_ON ( sizeof ( mp -> m_super -> s_uuid ) != sizeof ( uuid_t )); <nl> + memcpy (& mp -> m_super -> s_uuid , uuid , sizeof ( uuid_t )); <nl> + <nl> if ( mp -> m_flags & XFS_MOUNT_NOUUID ) <nl> return 0 ; <nl> 
void kvm_arch_vcpu_load ( struct kvm_vcpu * vcpu , int cpu ) <nl> if ( check_tsc_unstable ()) { <nl> kvm_x86_ops -> adjust_tsc_offset ( vcpu , - tsc_delta ); <nl> vcpu -> arch . tsc_catchup = 1 ; <nl> - kvm_make_request ( KVM_REQ_CLOCK_UPDATE , vcpu ); <nl> } <nl> + kvm_make_request ( KVM_REQ_CLOCK_UPDATE , vcpu ); <nl> if ( vcpu -> cpu != cpu ) <nl> kvm_migrate_timers ( vcpu ); <nl> vcpu -> cpu = cpu ;
static int find_variable ( Dwarf_Die * sc_die , struct probe_finder * pf ) <nl> if (! die_find_variable_at (& pf -> cu_die , pf -> pvar -> var , 0 , & vr_die )) <nl> ret = - ENOENT ; <nl> } <nl> - if ( ret == 0 ) <nl> + if ( ret >= 0 ) <nl> ret = convert_variable (& vr_die , pf ); <nl>  <nl> if ( ret < 0 )
static struct scsi_device * scsi_alloc_sdev ( struct scsi_target * starget , <nl> /* release fn is set up in scsi_sysfs_device_initialise , so <nl> * have to free and put manually here */ <nl> put_device (& starget -> dev ); <nl> + kfree ( sdev ); <nl> goto out ; <nl> } <nl> 
EXPORT_SYMBOL ( clk_add_alias ); <nl> */ <nl> void clkdev_drop ( struct clk_lookup * cl ) <nl> { <nl> + struct clk_lookup_alloc * cla = container_of ( cl , struct clk_lookup_alloc , cl ); <nl> + <nl> mutex_lock (& clocks_mutex ); <nl> list_del (& cl -> node ); <nl> mutex_unlock (& clocks_mutex ); <nl> - kfree ( cl ); <nl> + kfree ( cla ); <nl> } <nl> EXPORT_SYMBOL ( clkdev_drop );
static int marvell_pre_reset ( struct ata_port * ap ) <nl> switch ( ap -> port_no ) <nl> { <nl> case 0 : <nl> - /* Might be backward , docs unclear */ <nl> if ( inb ( ap -> ioaddr . bmdma_addr + 1 ) & 1 ) <nl> - ap -> cbl = ATA_CBL_PATA80 ; <nl> - else <nl> ap -> cbl = ATA_CBL_PATA40 ; <nl> + else <nl> + ap -> cbl = ATA_CBL_PATA80 ; <nl> + break ; <nl>  <nl> case 1 : /* Legacy SATA port */ <nl> ap -> cbl = ATA_CBL_SATA ;
static int sha384_neon_final ( struct shash_desc * desc , u8 * hash ) <nl> sha512_neon_final ( desc , D ); <nl>  <nl> memcpy ( hash , D , SHA384_DIGEST_SIZE ); <nl> - memset ( D , 0 , SHA512_DIGEST_SIZE ); <nl> + memzero_explicit ( D , SHA512_DIGEST_SIZE ); <nl>  <nl> return 0 ; <nl> }
static void r8a66597_check_detect_child ( struct r8a66597 * r8a66597 , <nl>  <nl> memset ( now_map , 0 , sizeof ( now_map )); <nl>  <nl> + mutex_lock (& usb_bus_idr_lock ); <nl> bus = idr_find (& usb_bus_idr , hcd -> self . busnum ); <nl> if ( bus && bus -> root_hub ) { <nl> collect_usb_address_map ( bus -> root_hub , now_map ); <nl> update_usb_address_map ( r8a66597 , bus -> root_hub , now_map ); <nl> } <nl> + mutex_unlock (& usb_bus_idr_lock ); <nl> } <nl>  <nl> static int r8a66597_hub_status_data ( struct usb_hcd * hcd , char * buf )
static int usbhsg_ep_disable ( struct usb_ep * ep ) <nl> struct usbhsg_uep * uep = usbhsg_ep_to_uep ( ep ); <nl> struct usbhs_pipe * pipe = usbhsg_uep_to_pipe ( uep ); <nl>  <nl> + if (! pipe ) <nl> + return - EINVAL ; <nl> + <nl> usbhsg_pipe_disable ( uep ); <nl> usbhs_pipe_free ( pipe ); <nl> 
struct device_node * v4l2_of_get_next_endpoint ( const struct device_node * parent , <nl> if (! endpoint ) <nl> pr_err ("% s (): no endpoint nodes specified for % s \ n ", <nl> __func__ , parent -> full_name ); <nl> + of_node_put ( node ); <nl> } else { <nl> port = of_get_parent ( prev ); <nl> if (! port )
static int __devinit tmiofb_probe ( struct platform_device * dev ) <nl> dev_err (& dev -> dev , " NULL platform data !\ n "); <nl> return - EINVAL ; <nl> } <nl> + if ( ccr == NULL || lcr == NULL || vram == NULL || irq < 0 ) { <nl> + dev_err (& dev -> dev , " missing resources \ n "); <nl> + return - EINVAL ; <nl> + } <nl>  <nl> info = framebuffer_alloc ( sizeof ( struct tmiofb_par ), & dev -> dev ); <nl> 
static int malidp_bind ( struct device * dev ) <nl> if ( ret < 0 ) <nl> goto irq_init_fail ; <nl>  <nl> + drm -> irq_enabled = true ; <nl> + <nl> ret = drm_vblank_init ( drm , drm -> mode_config . num_crtc ); <nl> if ( ret < 0 ) { <nl> DRM_ERROR (" failed to initialise vblank \ n "); <nl> fbdev_fail : <nl> vblank_fail : <nl> malidp_se_irq_fini ( drm ); <nl> malidp_de_irq_fini ( drm ); <nl> + drm -> irq_enabled = false ; <nl> irq_init_fail : <nl> component_unbind_all ( dev , drm ); <nl> bind_fail :
dma_alloc_coherent ( struct device * dev , size_t size , dma_addr_t * dma_handle , <nl> goto again ; <nl> } <nl>  <nl> + /* Let low level make its own zone decisions */ <nl> + gfp &= ~( GFP_DMA32 | GFP_DMA ); <nl> + <nl> if ( dma_ops -> alloc_coherent ) <nl> return dma_ops -> alloc_coherent ( dev , size , <nl> dma_handle , gfp );
new_slab : <nl> stat ( s , ALLOC_SLAB ); <nl> c -> node = page_to_nid ( page ); <nl> c -> page = page ; <nl> + <nl> + if ( kmem_cache_debug ( s )) <nl> + goto debug ; <nl> goto load_freelist ; <nl> } <nl> if (!( gfpflags & __GFP_NOWARN ) && printk_ratelimit ())
static int blkpg_ioctl ( struct block_device * bdev , struct blkpg_ioctl_arg __user <nl> || pstart < 0 || plength < 0 || partno > 65535 ) <nl> return - EINVAL ; <nl> } <nl> + /* check if partition is aligned to blocksize */ <nl> + if ( p . start & ( bdev_logical_block_size ( bdev ) - 1 )) <nl> + return - EINVAL ; <nl>  <nl> mutex_lock (& bdev -> bd_mutex ); <nl> 
static int ecryptfs_open ( struct inode * inode , struct file * file ) <nl> file , ecryptfs_inode_to_private ( inode )-> lower_file ); <nl> if ( S_ISDIR ( ecryptfs_dentry -> d_inode -> i_mode )) { <nl> ecryptfs_printk ( KERN_DEBUG , " This is a directory \ n "); <nl> + mutex_lock (& crypt_stat -> cs_mutex ); <nl> crypt_stat -> flags &= ~( ECRYPTFS_ENCRYPTED ); <nl> + mutex_unlock (& crypt_stat -> cs_mutex ); <nl> rc = 0 ; <nl> goto out ; <nl> }
int i965_reset ( struct drm_device * dev , u8 flags ) <nl> } <nl> } else { <nl> DRM_ERROR (" Error occurred . Don ' t know how to reset this chip .\ n "); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return - ENODEV ; <nl> } <nl> 
static int __cmd_record ( struct perf_record * rec , int argc , const char ** argv ) <nl> return err ; <nl> } <nl>  <nl> - if (!! rec -> no_buildid <nl> + if (! rec -> no_buildid <nl> && ! perf_header__has_feat (& session -> header , HEADER_BUILD_ID )) { <nl> - pr_err (" Couldn ' t generating buildids . " <nl> + pr_err (" Couldn ' t generate buildids . " <nl> " Use -- no - buildid to profile anyway .\ n "); <nl> return - 1 ; <nl> }
static int at76_iw_handler_get_scan ( struct net_device * netdev , <nl> if (! iwe ) <nl> return - ENOMEM ; <nl>  <nl> - if ( priv -> scan_state != SCAN_COMPLETED ) <nl> + if ( priv -> scan_state != SCAN_COMPLETED ) { <nl> /* scan not yet finished */ <nl> + kfree ( iwe ); <nl> return - EAGAIN ; <nl> + } <nl>  <nl> spin_lock_irqsave (& priv -> bss_list_spinlock , flags ); <nl> 
int btrfs_commit_transaction ( struct btrfs_trans_handle * trans , <nl> spin_unlock (& root -> fs_info -> trans_lock ); <nl>  <nl> wait_for_commit ( root , prev_trans ); <nl> + ret = prev_trans -> aborted ; <nl>  <nl> btrfs_put_transaction ( prev_trans ); <nl> + if ( ret ) <nl> + goto cleanup_transaction ; <nl> } else { <nl> spin_unlock (& root -> fs_info -> trans_lock ); <nl> }
static int hdmi_display_check_timing ( struct omap_dss_device * dssdev , <nl> { <nl> struct omap_dss_device * out = & hdmi . output ; <nl>  <nl> + /* TODO : proper interlace support */ <nl> + if ( timings -> interlace ) <nl> + return - EINVAL ; <nl> + <nl> if (! dispc_mgr_timings_ok ( out -> dispc_channel , timings )) <nl> return - EINVAL ; <nl> 
static int radeon_atom_pick_pll ( struct drm_crtc * crtc ) <nl> if ( pll != ATOM_PPLL_INVALID ) <nl> return pll ; <nl> } <nl> - } else { <nl> + } else if (! ASIC_IS_DCE41 ( rdev )) { /* Don ' t share PLLs on DCE4 . 1 chips */ <nl> /* use the same PPLL for all monitors with the same clock */ <nl> pll = radeon_get_shared_nondp_ppll ( crtc ); <nl> if ( pll != ATOM_PPLL_INVALID )
static int sanitize_enable_ppgtt ( struct drm_device * dev , int enable_ppgtt ) <nl> } <nl> # endif <nl>  <nl> + /* Early VLV doesn ' t have this */ <nl> + if ( IS_VALLEYVIEW ( dev ) && dev -> pdev -> revision < 0xb ) { <nl> + DRM_DEBUG_DRIVER (" disabling PPGTT on pre - B3 step VLV \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> return HAS_ALIASING_PPGTT ( dev ) ? 1 : 0 ; <nl> } <nl> 
static inline pte_t * pte_alloc_one_kernel ( struct mm_struct * mm , <nl> static inline pgtable_t pte_alloc_one ( struct mm_struct * mm , <nl> unsigned long addr ) <nl> { <nl> + pte_t * pte ; <nl> struct page * page ; <nl>  <nl> - page = virt_to_page ( pte_alloc_one_kernel ( mm , addr )); <nl> + pte = pte_alloc_one_kernel ( mm , addr ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + page = virt_to_page ( pte ); <nl> pgtable_page_ctor ( page ); <nl> return page ; <nl> }
static int pcmcia_device_query ( struct pcmcia_device * p_dev ) <nl> tmp = vers1 -> str + vers1 -> ofs [ i ]; <nl>  <nl> length = strlen ( tmp ) + 1 ; <nl> - if (( length < 3 ) || ( length > 255 )) <nl> + if (( length < 2 ) || ( length > 255 )) <nl> continue ; <nl>  <nl> p_dev -> prod_id [ i ] = kmalloc ( sizeof ( char ) * length ,
long sys_sigreturn ( struct pt_regs regs ) <nl> unsigned long __user * extramask = frame -> extramask ; <nl> int sig_size = ( _NSIG_WORDS - 1 ) * sizeof ( unsigned long ); <nl>  <nl> - if ( copy_from_user (& set . sig [ 0 ], oldmask , sizeof (& set . sig [ 0 ])) || <nl> + if ( copy_from_user (& set . sig [ 0 ], oldmask , sizeof ( set . sig [ 0 ])) || <nl> copy_from_user (& set . sig [ 1 ], extramask , sig_size )) <nl> goto segfault ; <nl> 
static void sdma_add_scripts ( struct sdma_engine * sdma , <nl> s32 * saddr_arr = ( u32 *) sdma -> script_addrs ; <nl> int i ; <nl>  <nl> + /* use the default firmware in ROM if missing external firmware */ <nl> + if (! sdma -> script_number ) <nl> + sdma -> script_number = SDMA_SCRIPT_ADDRS_ARRAY_SIZE_V1 ; <nl> + <nl> for ( i = 0 ; i < sdma -> script_number ; i ++) <nl> if ( addr_arr [ i ] > 0 ) <nl> saddr_arr [ i ] = addr_arr [ i ];
retry : <nl>  <nl> blkcg = bio_blkcg ( bio ); <nl> cfqg = cfq_lookup_create_cfqg ( cfqd , blkcg ); <nl> + if (! cfqg ) { <nl> + cfqq = & cfqd -> oom_cfqq ; <nl> + goto out ; <nl> + } <nl> + <nl> cfqq = cic_to_cfqq ( cic , is_sync ); <nl>  <nl> /* <nl> retry : <nl> } else <nl> cfqq = & cfqd -> oom_cfqq ; <nl> } <nl> - <nl> + out : <nl> if ( new_cfqq ) <nl> kmem_cache_free ( cfq_pool , new_cfqq ); <nl> 
EXPORT_SYMBOL ( smp_num_siblings ); <nl>  <nl> /* Last level cache ID of each logical CPU */ <nl> u8 cpu_llc_id [ NR_CPUS ] __cpuinitdata = {[ 0 ... NR_CPUS - 1 ] = BAD_APICID }; <nl> - EXPORT_SYMBOL ( cpu_llc_id ); <nl>  <nl> /* Bitmask of currently online CPUs */ <nl> cpumask_t cpu_online_map __read_mostly ;
__setup (" vga =", sm712vga_setup ); <nl> * Original init function changed to probe method to be used by pci_drv <nl> * process used to detect chips replaced with kernel process in pci_drv <nl> */ <nl> - static int __init smtcfb_pci_probe ( struct pci_dev * pdev , <nl> + static int __devinit smtcfb_pci_probe ( struct pci_dev * pdev , <nl> const struct pci_device_id * ent ) <nl> { <nl> struct smtcfb_info * sfb ;
intel_user_framebuffer_create ( struct drm_device * dev , <nl>  <nl> ret = intel_framebuffer_create ( dev , mode_cmd , & fb , obj ); <nl> if ( ret ) { <nl> + mutex_lock (& dev -> struct_mutex ); <nl> drm_gem_object_unreference ( obj ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return NULL ; <nl> } <nl> 
int ceph_atomic_open ( struct inode * dir , struct dentry * dentry , <nl> } <nl> err = finish_open ( file , dentry , ceph_open , opened ); <nl> } <nl> - <nl> out_err : <nl> + if (! req -> r_err && req -> r_target_inode ) <nl> + ceph_put_fmode ( ceph_inode ( req -> r_target_inode ), req -> r_fmode ); <nl> ceph_mdsc_put_request ( req ); <nl> dout (" atomic_open result =% d \ n ", err ); <nl> return err ;
int twl4030_madc_conversion ( struct twl4030_madc_request * req ) <nl> u8 ch_msb , ch_lsb ; <nl> int ret ; <nl>  <nl> - if (! req ) <nl> + if (! req || ! twl4030_madc ) <nl> return - EINVAL ; <nl> + <nl> mutex_lock (& twl4030_madc -> lock ); <nl> if ( req -> method < TWL4030_MADC_RT || req -> method > TWL4030_MADC_SW2 ) { <nl> ret = - EINVAL ;
static void radeon_cs_parser_fini ( struct radeon_cs_parser * parser , int error ) <nl> { <nl> unsigned i ; <nl>  <nl> - if ( error ) { <nl> + if ( error && parser -> ib ) { <nl> radeon_bo_list_unvalidate (& parser -> validated , <nl> parser -> ib -> fence ); <nl> } else {
static struct i2c_algorithm osif_algorithm = { <nl> # define USB_OSIF_VENDOR_ID 0x1964 <nl> # define USB_OSIF_PRODUCT_ID 0x0001 <nl>  <nl> - static struct usb_device_id osif_table [] = { <nl> + static const struct usb_device_id osif_table [] = { <nl> { USB_DEVICE ( USB_OSIF_VENDOR_ID , USB_OSIF_PRODUCT_ID ) }, <nl> { } <nl> };
static int lstats_open ( struct inode * inode , struct file * file ) <nl> struct seq_file * m ; <nl> struct task_struct * task = get_proc_task ( inode ); <nl>  <nl> + if (! task ) <nl> + return - ENOENT ; <nl> ret = single_open ( file , lstats_show_proc , NULL ); <nl> if (! ret ) { <nl> m = file -> private_data ;
static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
static long tce_iommu_register_pages ( struct tce_container * container , <nl> return ret ; <nl>  <nl> tcemem = kzalloc ( sizeof (* tcemem ), GFP_KERNEL ); <nl> + if (! tcemem ) { <nl> + mm_iommu_put ( container -> mm , mem ); <nl> + return - ENOMEM ; <nl> + } <nl> + <nl> tcemem -> mem = mem ; <nl> list_add (& tcemem -> next , & container -> prereg_list ); <nl> 
static void zfcp_fc_adisc_handler ( unsigned long data ) <nl> struct zfcp_port * port = adisc -> els . port ; <nl> struct zfcp_ls_adisc_acc * ls_adisc = & adisc -> ls_adisc_acc ; <nl>  <nl> - if (! adisc -> els . status ) { <nl> + if ( adisc -> els . status ) { <nl> /* request rejected or timed out */ <nl> zfcp_erp_port_forced_reopen ( port , 0 , 63 , NULL ); <nl> goto out ;
static int ds1374_probe ( struct i2c_adapter * adap , int addr , int kind ) <nl> client -> driver = & ds1374_driver ; <nl>  <nl> ds1374_workqueue = create_singlethread_workqueue (" ds1374 "); <nl> + if (! ds1374_workqueue ) { <nl> + kfree ( client ); <nl> + return - ENOMEM ; /* most expected reason */ <nl> + } <nl>  <nl> if (( rc = i2c_attach_client ( client )) != 0 ) { <nl> kfree ( client );
int btrfs_ordered_update_i_size ( struct inode * inode , u64 offset , <nl>  <nl> if ( ordered ) <nl> offset = entry_end ( ordered ); <nl> + else <nl> + offset = ALIGN ( offset , BTRFS_I ( inode )-> root -> sectorsize ); <nl>  <nl> mutex_lock (& tree -> mutex ); <nl> disk_i_size = BTRFS_I ( inode )-> disk_i_size ;
void tcp_fastopen_add_skb ( struct sock * sk , struct sk_buff * skb ) <nl> tp -> segs_in = 0 ; <nl> tcp_segs_in ( tp , skb ); <nl> __skb_pull ( skb , tcp_hdrlen ( skb )); <nl> + sk_forced_mem_schedule ( sk , skb -> truesize ); <nl> skb_set_owner_r ( skb , sk ); <nl>  <nl> TCP_SKB_CB ( skb )-> seq ++;
static int fbcon_blank ( struct vc_data * vc , int blank , int mode_switch ) <nl> update_screen ( vc ); <nl> } <nl>  <nl> - if ( fbcon_is_inactive ( vc , info ) || <nl> + if ( mode_switch || fbcon_is_inactive ( vc , info ) || <nl> ops -> blank_state != FB_BLANK_UNBLANK ) <nl> fbcon_del_cursor_timer ( info ); <nl> else
static inline uint8_t elf_sym__type ( const GElf_Sym * sym ) <nl>  <nl> static inline int elf_sym__is_function ( const GElf_Sym * sym ) <nl> { <nl> - return elf_sym__type ( sym ) == STT_FUNC && <nl> + return ( elf_sym__type ( sym ) == STT_FUNC || <nl> + elf_sym__type ( sym ) == STT_GNU_IFUNC ) && <nl> sym -> st_name != 0 && <nl> sym -> st_shndx != SHN_UNDEF ; <nl> }
static int gb_connection_hd_cport_quiesce ( struct gb_connection * connection ) <nl> if ( connection -> mode_switch ) <nl> peer_space += sizeof ( struct gb_operation_msg_hdr ); <nl>  <nl> + if (! hd -> driver -> cport_quiesce ) <nl> + return 0 ; <nl> + <nl> ret = hd -> driver -> cport_quiesce ( hd , connection -> hd_cport_id , <nl> peer_space , <nl> GB_CONNECTION_CPORT_QUIESCE_TIMEOUT );
netdev_tx_t ieee80211_subif_start_xmit ( struct sk_buff * skb , <nl> __le16 fc ; <nl> struct ieee80211_hdr hdr ; <nl> struct ieee80211s_hdr mesh_hdr __maybe_unused ; <nl> - struct mesh_path * mppath = NULL ; <nl> + struct mesh_path __maybe_unused * mppath = NULL ; <nl> const u8 * encaps_data ; <nl> int encaps_len , skip_header_bytes ; <nl> int nh_pos , h_pos ;
static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh , <nl>  <nl> if (! buffer_dirty ( bh )) { <nl> /* bdflush has written it . We can drop it now */ <nl> + __jbd2_journal_remove_checkpoint ( jh ); <nl> goto zap_buffer ; <nl> } <nl>  <nl> static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh , <nl> /* The orphan record ' s transaction has <nl> * committed . We can cleanse this buffer */ <nl> clear_buffer_jbddirty ( bh ); <nl> + __jbd2_journal_remove_checkpoint ( jh ); <nl> goto zap_buffer ; <nl> } <nl> }
static int btree_gc_coalesce ( struct btree * b , struct btree_op * op , <nl> BUG_ON ( btree_bset_first ( new_nodes [ 0 ])-> keys ); <nl> btree_node_free ( new_nodes [ 0 ]); <nl> rw_unlock ( true , new_nodes [ 0 ]); <nl> + new_nodes [ 0 ] = NULL ; <nl>  <nl> for ( i = 0 ; i < nodes ; i ++) { <nl> if ( __bch_keylist_realloc (& keylist , bkey_u64s (& r [ i ]. b -> key )))
static int __devinit snd_card_ad1816a_pnp ( int dev , struct snd_card_ad1816a * acar <nl> struct pnp_resource_table * cfg = kmalloc ( sizeof (* cfg ), GFP_KERNEL ); <nl> int err ; <nl>  <nl> + if (! cfg ) <nl> + return - ENOMEM ; <nl> acard -> dev = pnp_request_card_device ( card , id -> devs [ 0 ]. id , NULL ); <nl> if ( acard -> dev == NULL ) { <nl> kfree ( cfg );
static int __init aha152x_init ( void ) <nl>  <nl> static void __exit aha152x_exit ( void ) <nl> { <nl> - struct aha152x_hostdata * hd ; <nl> + struct aha152x_hostdata * hd , * tmp ; <nl>  <nl> - list_for_each_entry ( hd , & aha152x_host_list , host_list ) { <nl> + list_for_each_entry_safe ( hd , tmp , & aha152x_host_list , host_list ) { <nl> struct Scsi_Host * shost = container_of (( void *) hd , struct Scsi_Host , hostdata ); <nl>  <nl> aha152x_release ( shost );
static struct ip_conntrack_expect * find_expect ( struct ip_conntrack * ct , <nl> tuple . dst . protonum = IPPROTO_TCP ; <nl>  <nl> exp = __ip_conntrack_expect_find (& tuple ); <nl> - if ( exp -> master == ct ) <nl> + if ( exp && exp -> master == ct ) <nl> return exp ; <nl> return NULL ; <nl> }
out : <nl> static int si476x_codec_probe ( struct snd_soc_codec * codec ) <nl> { <nl> codec -> control_data = dev_get_regmap ( codec -> dev -> parent , NULL ); <nl> - return 0 ; <nl> + return snd_soc_codec_set_cache_io ( codec , 0 , 0 , SND_SOC_REGMAP ); <nl> } <nl>  <nl> static struct snd_soc_dai_ops si476x_dai_ops = {
static int __init sdma_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + ret = dma_coerce_mask_and_coherent (& pdev -> dev , DMA_BIT_MASK ( 32 )); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> sdma = kzalloc ( sizeof (* sdma ), GFP_KERNEL ); <nl> if (! sdma ) <nl> return - ENOMEM ;
static int batadv_tp_send ( void * arg ) <nl> primary_if = batadv_primary_if_get_selected ( bat_priv ); <nl> if ( unlikely (! primary_if )) { <nl> err = BATADV_TP_REASON_DST_UNREACHABLE ; <nl> + tp_vars -> reason = err ; <nl> goto out ; <nl> } <nl> 
static int amd64_check_ecc_enabled ( struct amd64_pvt * pvt ) <nl> " ECC is enabled by BIOS , Proceeding " <nl> " with EDAC module initialization \ n "); <nl>  <nl> + /* Signal good ECC status */ <nl> + ret = 0 ; <nl> + <nl> /* CLEAR the override , since BIOS controlled it */ <nl> ecc_enable_override = 0 ; <nl> }
static int ipmmu_domain_init_context ( struct ipmmu_vmsa_domain * domain ) <nl> domain -> cfg . ias = 32 ; <nl> domain -> cfg . oas = 40 ; <nl> domain -> cfg . tlb = & ipmmu_gather_ops ; <nl> + domain -> io_domain . geometry . aperture_end = DMA_BIT_MASK ( 32 ); <nl> + domain -> io_domain . geometry . force_aperture = true ; <nl> /* <nl> * TODO : Add support for coherent walk through CCI with DVM and remove <nl> * cache handling . For now , delegate it to the io - pgtable code .
static void pci_dio_detach ( struct comedi_device * dev ) <nl> if ( devpriv ) { <nl> if ( devpriv -> valid ) <nl> pci_dio_reset ( dev ); <nl> + } <nl> + if ( dev -> subdevices ) { <nl> for ( i = 0 ; i < dev -> n_subdevices ; i ++) { <nl> s = dev -> subdevices + i ; <nl> if ( s -> type == COMEDI_SUBD_DIO )
static struct page * page_chain_del ( struct page ** head , int n ) <nl> BUG_ON (! head ); <nl>  <nl> page = * head ; <nl> + <nl> + if (! page ) <nl> + return NULL ; <nl> + <nl> while ( page ) { <nl> tmp = page_chain_next ( page ); <nl> if (-- n == 0 )
bool intel_enable_ppgtt ( struct drm_device * dev , bool full ) <nl>  <nl> /* Full ppgtt disabled by default for now due to issues . */ <nl> if ( full ) <nl> - return false ; /* HAS_PPGTT ( dev ) */ <nl> + return HAS_PPGTT ( dev ) && ( i915 . enable_ppgtt == 2 ); <nl> else <nl> return HAS_ALIASING_PPGTT ( dev ); <nl> }
static struct usb_driver go7007_usb_driver = { <nl> }; <nl>  <nl> module_usb_driver ( go7007_usb_driver ); <nl> + MODULE_LICENSE (" GPL v2 ");
static int ccw_uevent ( struct device * dev , char ** envp , int num_envp , <nl> snprint_alias ( modalias_buf , sizeof ( modalias_buf ), id , ""); <nl> ret = add_uevent_var ( envp , num_envp , & i , buffer , buffer_size , & len , <nl> " MODALIAS =% s ", modalias_buf ); <nl> - return ret ; <nl> + if ( ret ) <nl> + return ret ; <nl> + envp [ i ] = NULL ; <nl> + return 0 ; <nl> } <nl>  <nl> struct bus_type ccw_bus_type ;
static void gpiodevice_release ( struct device * dev ) <nl> cdev_del (& gdev -> chrdev ); <nl> list_del (& gdev -> list ); <nl> ida_simple_remove (& gpio_ida , gdev -> id ); <nl> + kfree ( gdev ); <nl> } <nl>  <nl> /**
static ssize_t dgap_driver_pollrate_show ( struct device_driver * ddp , char * buf ) <nl>  <nl> static ssize_t dgap_driver_pollrate_store ( struct device_driver * ddp , const char * buf , size_t count ) <nl> { <nl> - sscanf ( buf , "% d \ n ", & dgap_poll_tick ); <nl> + if ( sscanf ( buf , "% d \ n ", & dgap_poll_tick ) != 1 ) <nl> + return - EINVAL ; <nl> return count ; <nl> } <nl> static DRIVER_ATTR ( pollrate , ( S_IRUSR | S_IWUSR ), dgap_driver_pollrate_show , dgap_driver_pollrate_store );
struct sctp_cookie { <nl> __u32 adaptation_ind ; <nl>  <nl> __u8 auth_random [ sizeof ( sctp_paramhdr_t ) + SCTP_AUTH_RANDOM_LENGTH ]; <nl> - __u8 auth_hmacs [ SCTP_AUTH_NUM_HMACS + 2 ]; <nl> + __u8 auth_hmacs [ SCTP_AUTH_NUM_HMACS * sizeof ( __u16 ) + 2 ]; <nl> __u8 auth_chunks [ sizeof ( sctp_paramhdr_t ) + SCTP_AUTH_MAX_CHUNKS ]; <nl>  <nl> /* This is a shim for my peer ' s INIT packet , followed by
static int macronix_quad_enable ( struct spi_nor * nor ) <nl> val = read_sr ( nor ); <nl> if ( val < 0 ) <nl> return val ; <nl> + if ( val & SR_QUAD_EN_MX ) <nl> + return 0 ; <nl> + <nl> write_enable ( nor ); <nl>  <nl> write_sr ( nor , val | SR_QUAD_EN_MX );
ccw_device_offline ( struct ccw_device * cdev ) <nl> ccw_device_done ( cdev , DEV_STATE_NOT_OPER ); <nl> return 0 ; <nl> } <nl> + if ( cdev -> private -> state == DEV_STATE_BOXED ) { <nl> + ccw_device_done ( cdev , DEV_STATE_BOXED ); <nl> + return 0 ; <nl> + } <nl> if ( ccw_device_is_orphan ( cdev )) { <nl> ccw_device_done ( cdev , DEV_STATE_OFFLINE ); <nl> return 0 ;
scsi_reset_provider ( struct scsi_device * dev , int flag ) <nl> } <nl>  <nl> scmd = scsi_get_command ( dev , GFP_KERNEL ); <nl> + if (! scmd ) { <nl> + rtn = FAILED ; <nl> + put_device (& dev -> sdev_gendev ); <nl> + goto out_put_autopm_host ; <nl> + } <nl> + <nl> blk_rq_init ( NULL , & req ); <nl> scmd -> request = & req ; <nl> 
static int __devexit max77693_muic_remove ( struct platform_device * pdev ) <nl> free_irq ( muic_irqs [ i ]. virq , info ); <nl> cancel_work_sync (& info -> irq_work ); <nl> extcon_dev_unregister ( info -> edev ); <nl> + kfree ( info -> edev ); <nl> kfree ( info ); <nl>  <nl> return 0 ;
static void hdmi5_core_audio_config ( struct hdmi_core_data * core , <nl>  <nl> /* Source number */ <nl> val = cfg -> iec60958_cfg -> status [ 2 ] & IEC958_AES2_CON_SOURCE ; <nl> - REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 2 ), val , 3 , 4 ); <nl> + REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 2 ), val , 3 , 0 ); <nl>  <nl> /* Channel number right 0 */ <nl> REG_FLD_MOD ( base , HDMI_CORE_FC_AUDSCHNLS ( 3 ), 2 , 3 , 0 );
int tipc_nl_node_get_monitor ( struct sk_buff * skb , struct genl_info * info ) <nl> int err ; <nl>  <nl> msg . skb = nlmsg_new ( NLMSG_GOODSIZE , GFP_KERNEL ); <nl> + if (! msg . skb ) <nl> + return - ENOMEM ; <nl> msg . portid = info -> snd_portid ; <nl> msg . seq = info -> snd_seq ; <nl> 
static void sahara_decode_status ( struct sahara_dev * dev , unsigned int status ) <nl> if ( status & SAHARA_STATUS_MODE_BATCH ) <nl> dev_dbg ( dev -> device , " - Batch Mode .\ n "); <nl> else if ( status & SAHARA_STATUS_MODE_DEDICATED ) <nl> - dev_dbg ( dev -> device , " - Decidated Mode .\ n "); <nl> + dev_dbg ( dev -> device , " - Dedicated Mode .\ n "); <nl> else if ( status & SAHARA_STATUS_MODE_DEBUG ) <nl> dev_dbg ( dev -> device , " - Debug Mode .\ n "); <nl> 
static int input_open_polled_device ( struct input_dev * input ) <nl> dev -> open ( dev ); <nl>  <nl> /* Only start polling if polling is enabled */ <nl> - if ( dev -> poll_interval > 0 ) <nl> - queue_delayed_work ( system_freezable_wq , & dev -> work , 0 ); <nl> + if ( dev -> poll_interval > 0 ) { <nl> + dev -> poll ( dev ); <nl> + input_polldev_queue_work ( dev ); <nl> + } <nl>  <nl> return 0 ; <nl> }
static void rtl8152_get_drvinfo ( struct net_device * netdev , <nl> { <nl> struct r8152 * tp = netdev_priv ( netdev ); <nl>  <nl> - strncpy ( info -> driver , MODULENAME , ETHTOOL_BUSINFO_LEN ); <nl> - strncpy ( info -> version , DRIVER_VERSION , ETHTOOL_BUSINFO_LEN ); <nl> + strlcpy ( info -> driver , MODULENAME , sizeof ( info -> driver )); <nl> + strlcpy ( info -> version , DRIVER_VERSION , sizeof ( info -> version )); <nl> usb_make_path ( tp -> udev , info -> bus_info , sizeof ( info -> bus_info )); <nl> } <nl> 
void btrfs_rm_dev_replace_srcdev ( struct btrfs_fs_info * fs_info , <nl> fs_devices -> num_devices --; <nl> if ( srcdev -> missing ) { <nl> fs_devices -> missing_devices --; <nl> - fs_devices -> rw_devices ++; <nl> + if (! fs_devices -> seeding ) <nl> + fs_devices -> rw_devices ++; <nl> } <nl> if ( srcdev -> can_discard ) <nl> fs_devices -> num_can_discard --;
static void v4l2_ctrl_del_event ( struct v4l2_subscribed_event * sev ) <nl> { <nl> struct v4l2_ctrl * ctrl = v4l2_ctrl_find ( sev -> fh -> ctrl_handler , sev -> id ); <nl>  <nl> + if ( ctrl == NULL ) <nl> + return ; <nl> + <nl> v4l2_ctrl_lock ( ctrl ); <nl> list_del (& sev -> node ); <nl> v4l2_ctrl_unlock ( ctrl );
static struct regmap_config hmc5843_i2c_regmap_config = { <nl> . cache_type = REGCACHE_RBTREE , <nl> }; <nl>  <nl> - static int hmc5843_i2c_probe ( struct i2c_client * client , <nl> + static int hmc5843_i2c_probe ( struct i2c_client * cli , <nl> const struct i2c_device_id * id ) <nl> { <nl> - return hmc5843_common_probe (& client -> dev , <nl> - devm_regmap_init_i2c ( client , & hmc5843_i2c_regmap_config ), <nl> + return hmc5843_common_probe (& cli -> dev , <nl> + devm_regmap_init_i2c ( cli , & hmc5843_i2c_regmap_config ), <nl> id -> driver_data ); <nl> } <nl> 
static int __mlxsw_sp_port_fid_join ( struct mlxsw_sp_port * mlxsw_sp_port , <nl> { <nl> struct mlxsw_sp_fid * f ; <nl>  <nl> + if ( test_bit ( fid , mlxsw_sp_port -> active_vlans )) <nl> + return 0 ; <nl> + <nl> f = mlxsw_sp_fid_find ( mlxsw_sp_port -> mlxsw_sp , fid ); <nl> if (! f ) { <nl> f = mlxsw_sp_fid_create ( mlxsw_sp_port -> mlxsw_sp , fid );
int cpufreq_update_policy ( unsigned int cpu ) <nl> */ <nl> if ( cpufreq_driver -> get ) { <nl> new_policy . cur = cpufreq_driver -> get ( cpu ); <nl> + if ( WARN_ON (! new_policy . cur )) { <nl> + ret = - EIO ; <nl> + goto no_policy ; <nl> + } <nl> + <nl> if (! policy -> cur ) { <nl> pr_debug (" Driver did not initialize current freq "); <nl> policy -> cur = new_policy . cur ;
static int sendconfirmsleep ( struct lbs_private * priv , u8 * cmdptr , u16 size ) <nl> lbs_deb_hex ( LBS_DEB_HOST , " sleep confirm command ", cmdptr , size ); <nl>  <nl> ret = priv -> hw_host_to_card ( priv , MVMS_CMD , cmdptr , size ); <nl> - priv -> dnld_sent = DNLD_RES_RECEIVED ; <nl>  <nl> spin_lock_irqsave (& priv -> driver_lock , flags ); <nl> if ( priv -> intcounter || priv -> currenttxskb )
int diAlloc ( struct inode * pip , bool dir , struct inode * ip ) <nl> /* mask any prior bits for the starting words of the <nl> * summary map . <nl> */ <nl> - mask = ONES << ( EXTSPERSUM - bitno ); <nl> + mask = ( bitno == 0 ) ? 0 : ( ONES << ( EXTSPERSUM - bitno )); <nl> inosmap = le32_to_cpu ( iagp -> inosmap [ sword ]) | mask ; <nl> extsmap = le32_to_cpu ( iagp -> extsmap [ sword ]) | mask ; <nl> 
static int encode_caps_cb ( struct inode * inode , struct ceph_cap * cap , <nl> spin_lock (& ci -> i_ceph_lock ); <nl> cap -> seq = 0 ; /* reset cap seq */ <nl> cap -> issue_seq = 0 ; /* and issue_seq */ <nl> + cap -> mseq = 0 ; /* and migrate_seq */ <nl>  <nl> if ( recon_state -> flock ) { <nl> rec . v2 . cap_id = cpu_to_le64 ( cap -> cap_id );
static int xenbus_write_transaction ( unsigned msg_type , <nl> return xenbus_command_reply ( u , XS_ERROR , " ENOENT "); <nl>  <nl> rc = xenbus_dev_request_and_reply (& u -> u . msg , u ); <nl> - if ( rc ) <nl> + if ( rc && trans ) { <nl> + list_del (& trans -> list ); <nl> kfree ( trans ); <nl> + } <nl>  <nl> out : <nl> return rc ;
static int pm8001_dev_found_notify ( struct domain_device * dev ) <nl> wait_for_completion (& completion ); <nl> if ( dev -> dev_type == SAS_END_DEV ) <nl> msleep ( 50 ); <nl> - pm8001_ha -> flags = PM8001F_RUN_TIME ; <nl> + pm8001_ha -> flags |= PM8001F_RUN_TIME ; <nl> return 0 ; <nl> found_out : <nl> spin_unlock_irqrestore (& pm8001_ha -> lock , flags );
EXPORT_SYMBOL_GPL ( tcp_slow_start ); <nl> */ <nl> void tcp_cong_avoid_ai ( struct tcp_sock * tp , u32 w , u32 acked ) <nl> { <nl> + /* If credits accumulated at a higher w , apply them gently now . */ <nl> + if ( tp -> snd_cwnd_cnt >= w ) { <nl> + tp -> snd_cwnd_cnt = 0 ; <nl> + tp -> snd_cwnd ++; <nl> + } <nl> + <nl> tp -> snd_cwnd_cnt += acked ; <nl> if ( tp -> snd_cwnd_cnt >= w ) { <nl> u32 delta = tp -> snd_cwnd_cnt / w ;
static void fan_watchdog_reset ( void ) <nl> { <nl> static int fan_watchdog_active = 0 ; <nl>  <nl> + if ( fan_control_access_mode == TPACPI_FAN_WR_NONE ) <nl> + return ; <nl> + <nl> if ( fan_watchdog_active ) <nl> cancel_delayed_work (& fan_watchdog_task ); <nl> 
struct hisi_clock_data * hisi_clk_alloc ( struct platform_device * pdev , <nl> if (! clk_data -> base ) <nl> return NULL ; <nl>  <nl> - clk_table = devm_kmalloc (& pdev -> dev , sizeof ( struct clk *) * nr_clks , <nl> - GFP_KERNEL ); <nl> + clk_table = devm_kmalloc_array (& pdev -> dev , nr_clks , <nl> + sizeof (* clk_table ), <nl> + GFP_KERNEL ); <nl> if (! clk_table ) <nl> return NULL ; <nl> 
static int ak8975_probe ( struct i2c_client * client , <nl> indio_dev -> channels = ak8975_channels ; <nl> indio_dev -> num_channels = ARRAY_SIZE ( ak8975_channels ); <nl> indio_dev -> info = & ak8975_info ; <nl> + indio_dev -> name = id -> name ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl>  <nl> err = iio_device_register ( indio_dev );
STATIC int INIT gunzip ( unsigned char * buf , int len , <nl> strm -> next_in ++; <nl> strm -> next_in ++; <nl> } <nl> - strm -> avail_in = len - 10 ; <nl> + strm -> avail_in = len - ( strm -> next_in - zbuf ); <nl>  <nl> strm -> next_out = out_buf ; <nl> strm -> avail_out = out_len ;
struct flex_array * flex_array_alloc ( int element_size , unsigned int total , <nl> ret -> element_size = element_size ; <nl> ret -> total_nr_elements = total ; <nl> if ( elements_fit_in_base ( ret ) && !( flags & __GFP_ZERO )) <nl> - memset ( ret -> parts [ 0 ], FLEX_ARRAY_FREE , <nl> + memset (& ret -> parts [ 0 ], FLEX_ARRAY_FREE , <nl> FLEX_ARRAY_BASE_BYTES_LEFT ); <nl> return ret ; <nl> }
static int ti_qspi_dma_xfer ( struct ti_qspi * qspi , dma_addr_t dma_dst , <nl> tx -> callback = ti_qspi_dma_callback ; <nl> tx -> callback_param = qspi ; <nl> cookie = tx -> tx_submit ( tx ); <nl> + reinit_completion (& qspi -> transfer_complete ); <nl>  <nl> ret = dma_submit_error ( cookie ); <nl> if ( ret ) {
# include < mach / mmc . h > <nl> # include < mach / ohci . h > <nl> # include < mach / pxa2xx - regs . h > <nl> +# include < mach / audio . h > <nl>  <nl> # include " generic . h " <nl> # include " devices . h " <nl> static void __init csb726_init ( void ) <nl> pxa27x_set_i2c_power_info ( NULL ); <nl> pxa_set_mci_info (& csb726_mci ); <nl> pxa_set_ohci_info (& csb726_ohci_platform_data ); <nl> + pxa_set_ac97_info ( NULL ); <nl>  <nl> platform_add_devices ( devices , ARRAY_SIZE ( devices )); <nl> }
int radeon_sa_bo_new ( struct radeon_device * rdev , <nl> offset = 0 ; <nl> list_for_each_entry ( tmp , & sa_manager -> sa_bo , list ) { <nl> /* room before this object ? */ <nl> - if (( tmp -> offset - offset ) >= size ) { <nl> + if ( offset < tmp -> offset && ( tmp -> offset - offset ) >= size ) { <nl> head = tmp -> list . prev ; <nl> goto out ; <nl> }
int hcd_buffer_create ( struct usb_hcd * hcd ) <nl> char name [ 16 ]; <nl> int i , size ; <nl>  <nl> + if (! hcd -> self . controller -> dma_mask ) <nl> + return 0 ; <nl> + <nl> for ( i = 0 ; i < HCD_BUFFER_POOLS ; i ++) { <nl> if (!( size = pool_max [ i ])) <nl> continue ;
xfs_setattr_nonsize ( <nl> } <nl> if (! gid_eq ( igid , gid )) { <nl> if ( XFS_IS_QUOTA_RUNNING ( mp ) && XFS_IS_GQUOTA_ON ( mp )) { <nl> - ASSERT (! XFS_IS_PQUOTA_ON ( mp )); <nl> + ASSERT ( xfs_sb_version_has_pquotino (& mp -> m_sb ) || <nl> + ! XFS_IS_PQUOTA_ON ( mp )); <nl> ASSERT ( mask & ATTR_GID ); <nl> ASSERT ( gdqp ); <nl> olddquot2 = xfs_qm_vop_chown ( tp , ip ,
static struct config_item_type printer_func_type = { <nl>  <nl> static inline int gprinter_get_minor ( void ) <nl> { <nl> - return ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + int ret ; <nl> + <nl> + ret = ida_simple_get (& printer_ida , 0 , 0 , GFP_KERNEL ); <nl> + if ( ret >= PRINTER_MINORS ) { <nl> + ida_simple_remove (& printer_ida , ret ); <nl> + ret = - ENODEV ; <nl> + } <nl> + <nl> + return ret ; <nl> } <nl>  <nl> static inline void gprinter_put_minor ( int minor )
static int spi_map_buf ( struct spi_master * master , struct device * dev , <nl> } <nl>  <nl> ret = dma_map_sg ( dev , sgt -> sgl , sgt -> nents , dir ); <nl> + if (! ret ) <nl> + ret = - ENOMEM ; <nl> if ( ret < 0 ) { <nl> sg_free_table ( sgt ); <nl> return ret ;
send : <nl> ret = batadv_send_skb_via_tt ( bat_priv , skb , dst_hint , <nl> vid ); <nl> } <nl> - if ( ret == NET_XMIT_DROP ) <nl> + if ( ret != NET_XMIT_SUCCESS ) <nl> goto dropped_freed ; <nl> } <nl> 
static int __devinit rtsx_probe ( struct pci_dev * pci , <nl> th = kthread_create ( rtsx_scan_thread , dev , " rtsx - scan "); <nl> if ( IS_ERR ( th )) { <nl> printk ( KERN_ERR " Unable to start the device - scanning thread \ n "); <nl> + complete (& dev -> scanning_done ); <nl> quiesce_and_remove_host ( dev ); <nl> err = PTR_ERR ( th ); <nl> goto errout ;
cfq_rq_enqueued ( struct cfq_data * cfqd , struct cfq_queue * cfqq , <nl> if ( blk_rq_bytes ( rq ) > PAGE_CACHE_SIZE || <nl> cfqd -> busy_queues > 1 ) { <nl> del_timer (& cfqd -> idle_slice_timer ); <nl> - __blk_run_queue ( cfqd -> queue ); <nl> - } <nl> - cfq_mark_cfqq_must_dispatch ( cfqq ); <nl> + __blk_run_queue ( cfqd -> queue ); <nl> + } else <nl> + cfq_mark_cfqq_must_dispatch ( cfqq ); <nl> } <nl> } else if ( cfq_should_preempt ( cfqd , cfqq , rq )) { <nl> /*
void nf_log_packet ( int pf , <nl> const struct net_device * in , <nl> const struct net_device * out , <nl> const struct nf_loginfo * li , <nl> - const char * fmt , ...); <nl> + const char * fmt , ...) __attribute__ (( format ( printf , 7 , 8 ))); <nl>  <nl> # endif /* _NF_LOG_H */
static void b43_supported_bands ( struct b43_wldev * dev , bool * have_2ghz_phy , <nl> * have_5ghz_phy = true ; <nl> return ; <nl> case 0x4321 : /* BCM4306 */ <nl> + /* There are 14e4 : 4321 PCI devs with 2 . 4 GHz BCM4321 ( N - PHY ) */ <nl> + if ( dev -> phy . type != B43_PHYTYPE_G ) <nl> + break ; <nl> + /* fall through */ <nl> case 0x4313 : /* BCM4311 */ <nl> case 0x431a : /* BCM4318 */ <nl> case 0x432a : /* BCM4321 */
static ssize_t spufs_mfc_write ( struct file * file , const char __user * buffer , <nl> if ( ret ) <nl> goto out ; <nl>  <nl> - spu_acquire_runnable ( ctx , 0 ); <nl> + ret = spu_acquire_runnable ( ctx , 0 ); <nl> + if ( ret ) <nl> + goto out ; <nl> + <nl> if ( file -> f_flags & O_NONBLOCK ) { <nl> ret = ctx -> ops -> send_mfc_command ( ctx , & cmd ); <nl> } else {
static int do_calculate_time ( int status , enum apm_source source ) <nl> return - 1 ; <nl> } <nl>  <nl> + if (! I . intval ) <nl> + return 0 ; <nl> + <nl> switch ( source ) { <nl> case SOURCE_CHARGE : <nl> full_prop = POWER_SUPPLY_PROP_CHARGE_FULL ;
static const struct comedi_lrange * dac_range_table [] = { <nl>  <nl> static const struct comedi_lrange * dac_range_lkup ( int opt ) <nl> { <nl> - if ( opt < 0 || opt > 5 ) <nl> + if ( opt < 0 || opt >= 5 ) <nl> return & range_unknown ; <nl> return dac_range_table [ opt ]; <nl> }
done : <nl>  <nl> out : <nl> if ( unlikely ( frozen_buffer )) /* It ' s usually NULL */ <nl> - kfree ( frozen_buffer ); <nl> + jbd_slab_free ( frozen_buffer , bh -> b_size ); <nl>  <nl> JBUFFER_TRACE ( jh , " exit "); <nl> return error ;
static int mtk_pmx_gpio_request_enable ( struct pinctrl_dev * pctldev , <nl> } <nl>  <nl> mtk_pmx_set_mode ( pctldev , offset , muxval ); <nl> + mtk_pconf_set_ies_smt ( pctl , offset , 1 , PIN_CONFIG_INPUT_ENABLE ); <nl>  <nl> return 0 ; <nl> }
static int __init scm_blk_init ( void ) <nl> scm_major = ret ; <nl> ret = scm_alloc_rqs ( nr_requests ); <nl> if ( ret ) <nl> - goto out_unreg ; <nl> + goto out_free ; <nl>  <nl> scm_debug = debug_register (" scm_log ", 16 , 1 , 16 ); <nl> if (! scm_debug ) { <nl> out_dbf : <nl> debug_unregister ( scm_debug ); <nl> out_free : <nl> scm_free_rqs (); <nl> - out_unreg : <nl> unregister_blkdev ( scm_major , " scm "); <nl> out : <nl> return ret ;
int scsi_dh_activate ( struct request_queue * q , activate_complete fn , void * data ) <nl>  <nl> if (! sdev -> handler ) <nl> goto out_fn ; <nl> + err = SCSI_DH_NOTCONN ; <nl> if ( sdev -> sdev_state == SDEV_CANCEL || <nl> sdev -> sdev_state == SDEV_DEL ) <nl> goto out_fn ;
restart : <nl> } else { <nl> spin_unlock (& gl -> gl_spin ); <nl>  <nl> - new_gh = gfs2_holder_get ( gl , state , LM_FLAG_TRY , GFP_KERNEL ); <nl> + new_gh = gfs2_holder_get ( gl , state , LM_FLAG_TRY , GFP_NOFS ); <nl> if (! new_gh ) <nl> return ; <nl> set_bit ( HIF_DEMOTE , & new_gh -> gh_iflags );
static inline void update_sd_power_savings_stats ( struct sched_group * group , <nl> * capacity but still has some space to pick up some load <nl> * from other group and save more power <nl> */ <nl> - if ( sgs -> sum_nr_running > sgs -> group_capacity - 1 ) <nl> + if ( sgs -> sum_nr_running + 1 > sgs -> group_capacity ) <nl> return ; <nl>  <nl> if ( sgs -> sum_nr_running > sds -> leader_nr_running ||
static int __init early_get_pnodeid ( void ) <nl> break ; <nl> case UV3_HUB_PART_NUMBER : <nl> case UV3_HUB_PART_NUMBER_X : <nl> - uv_min_hub_revision_id += UV3_HUB_REVISION_BASE - 1 ; <nl> + uv_min_hub_revision_id += UV3_HUB_REVISION_BASE ; <nl> break ; <nl> } <nl> 
static int __init gc_setup_pad ( struct gc * gc , int idx , int pad_type ) <nl> int i ; <nl> int err ; <nl>  <nl> - if ( pad_type < 1 || pad_type > GC_MAX ) { <nl> + if ( pad_type < 1 || pad_type >= GC_MAX ) { <nl> pr_err (" Pad type % d unknown \ n ", pad_type ); <nl> return - EINVAL ; <nl> }
static int sd_sdr_tuning ( struct rtsx_chip * chip ) <nl> int retval ; <nl>  <nl> retval = sd_tuning_tx ( chip ); <nl> - if ( retval != STATUS_SUCCESS ) { <nl> + if ( retval != STATUS_SUCCESS ) <nl> TRACE_RET ( chip , STATUS_FAIL ); <nl> - } <nl>  <nl> retval = sd_tuning_rx ( chip ); <nl> - if ( retval != STATUS_SUCCESS ) { <nl> + if ( retval != STATUS_SUCCESS ) <nl> TRACE_RET ( chip , STATUS_FAIL ); <nl> - } <nl>  <nl> return STATUS_SUCCESS ; <nl> }
static int __init gt641xx_timer0_clockevent_init ( void ) <nl>  <nl> cd = & gt641xx_timer0_clockevent ; <nl> cd -> rating = 200 + gt641xx_base_clock / 10000000 ; <nl> + clockevent_set_clock ( cd , gt641xx_base_clock ); <nl> cd -> max_delta_ns = clockevent_delta2ns ( 0x7fffffff , cd ); <nl> cd -> min_delta_ns = clockevent_delta2ns ( 0x300 , cd ); <nl> - clockevent_set_clock ( cd , gt641xx_base_clock ); <nl>  <nl> clockevents_register_device (& gt641xx_timer0_clockevent ); <nl> 
int skb_gro_receive ( struct sk_buff ** head , struct sk_buff * skb ) <nl> * NAPI_GRO_CB ( nskb ) = * NAPI_GRO_CB ( p ); <nl> skb_shinfo ( nskb )-> frag_list = p ; <nl> skb_shinfo ( nskb )-> gso_size = pinfo -> gso_size ; <nl> + pinfo -> gso_size = 0 ; <nl> skb_header_release ( p ); <nl> nskb -> prev = p ; <nl> 
i915_gem_object_get_pages_gtt ( struct drm_i915_gem_object * obj ) <nl>  <nl> page_count = obj -> base . size / PAGE_SIZE ; <nl> if ( sg_alloc_table ( st , page_count , GFP_KERNEL )) { <nl> - sg_free_table ( st ); <nl> kfree ( st ); <nl> return - ENOMEM ; <nl> }
static int rndis_wlan_bind ( struct usbnet * usbdev , struct usb_interface * intf ) <nl>  <nl> /* because rndis_command () sleeps we need to use workqueue */ <nl> priv -> workqueue = create_singlethread_workqueue (" rndis_wlan "); <nl> + if (! priv -> workqueue ) { <nl> + wiphy_free ( wiphy ); <nl> + return - ENOMEM ; <nl> + } <nl> INIT_WORK (& priv -> work , rndis_wlan_worker ); <nl> INIT_DELAYED_WORK (& priv -> dev_poller_work , rndis_device_poller ); <nl> INIT_DELAYED_WORK (& priv -> scan_work , rndis_get_scan_results );
int drm_mode_create_dumb_ioctl ( struct drm_device * dev , <nl> return - EINVAL ; <nl>  <nl> /* overflow checks for 32bit size calculations */ <nl> + /* NOTE : DIV_ROUND_UP () can overflow */ <nl> cpp = DIV_ROUND_UP ( args -> bpp , 8 ); <nl> - if ( cpp > 0xffffffffU / args -> width ) <nl> + if (! cpp || cpp > 0xffffffffU / args -> width ) <nl> return - EINVAL ; <nl> stride = cpp * args -> width ; <nl> if ( args -> height > 0xffffffffU / stride )
void kvm_exit ( void ) <nl> kvm_arch_hardware_unsetup (); <nl> kvm_arch_exit (); <nl> free_cpumask_var ( cpus_hardware_enabled ); <nl> + __free_page ( fault_page ); <nl> __free_page ( hwpoison_page ); <nl> __free_page ( bad_page ); <nl> }
static void qlcnic_get_ethtool_stats ( struct net_device * dev , <nl> tx_ring = & adapter -> tx_ring [ ring ]; <nl> data = qlcnic_fill_tx_queue_stats ( data , tx_ring ); <nl> qlcnic_update_stats ( adapter ); <nl> + } else { <nl> + data += QLCNIC_TX_STATS_LEN ; <nl> } <nl> } <nl> 
static int _sp2d_alloc ( unsigned pages_in_unit , unsigned group_width , <nl> num_a1pa = min_t ( unsigned , PAGE_SIZE / sizeof__a1pa , <nl> pages_in_unit - i ); <nl>  <nl> - __a1pa = kzalloc ( num_a1pa * sizeof__a1pa , GFP_KERNEL ); <nl> + __a1pa = kcalloc ( num_a1pa , sizeof__a1pa , GFP_KERNEL ); <nl> if ( unlikely (! __a1pa )) { <nl> ORE_DBGMSG ("!! Failed to _alloc_1p_arrays =% d \ n ", <nl> num_a1pa );
static struct comedi_driver pcl726_driver = { <nl> module_comedi_driver ( pcl726_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for Advantech PCL - 726 & compatibles "); <nl> MODULE_LICENSE (" GPL ");
static struct drm_driver tegra_drm_driver = { <nl> . debugfs_cleanup = tegra_debugfs_cleanup , <nl> # endif <nl>  <nl> - . gem_free_object = tegra_bo_free_object , <nl> + . gem_free_object_unlocked = tegra_bo_free_object , <nl> . gem_vm_ops = & tegra_bo_vm_ops , <nl>  <nl> . prime_handle_to_fd = drm_gem_prime_handle_to_fd ,
int add_extent_mapping ( struct extent_map_tree * tree , <nl> rb = tree_insert (& tree -> map , em -> start , & em -> rb_node ); <nl> if ( rb ) { <nl> ret = - EEXIST ; <nl> - free_extent_map ( merge ); <nl> goto out ; <nl> } <nl> atomic_inc (& em -> refs );
static void esdhc_writew_le ( struct sdhci_host * host , u16 val , int reg ) <nl> new_val |= ESDHC_VENDOR_SPEC_FRC_SDCLK_ON ; <nl> else <nl> new_val &= ~ ESDHC_VENDOR_SPEC_FRC_SDCLK_ON ; <nl> - writel ( new_val , host -> ioaddr + ESDHC_VENDOR_SPEC ); <nl> + writel ( new_val , host -> ioaddr + ESDHC_VENDOR_SPEC ); <nl> return ; <nl> case SDHCI_HOST_CONTROL2 : <nl> new_val = readl ( host -> ioaddr + ESDHC_VENDOR_SPEC );
int be_cmd_loopback_test ( struct be_adapter * adapter , u32 port_num , <nl>  <nl> be_cmd_hdr_prepare (& req -> hdr , CMD_SUBSYSTEM_LOWLEVEL , <nl> OPCODE_LOWLEVEL_LOOPBACK_TEST , sizeof (* req )); <nl> + req -> hdr . timeout = 4 ; <nl>  <nl> req -> pattern = cpu_to_le64 ( pattern ); <nl> req -> src_port = cpu_to_le32 ( port_num );
static void iwl_mvm_unshare_queue ( struct iwl_mvm * mvm , int queue ) <nl>  <nl> /* If aggs should be turned back on - do it */ <nl> if ( mvmsta -> tid_data [ tid ]. state == IWL_AGG_ON ) { <nl> - struct iwl_mvm_add_sta_cmd cmd ; <nl> + struct iwl_mvm_add_sta_cmd cmd = { 0 }; <nl>  <nl> mvmsta -> tid_disable_agg &= ~ BIT ( tid ); <nl> 
static inline void bio_list_add ( struct bio_list * bl , struct bio * bio ) <nl>  <nl> static inline void bio_list_merge ( struct bio_list * bl , struct bio_list * bl2 ) <nl> { <nl> + if (! bl2 -> head ) <nl> + return ; <nl> + <nl> if ( bl -> tail ) <nl> bl -> tail -> bi_next = bl2 -> head ; <nl> else
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) { <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) { <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& priv -> lock , flags );
static int omap_mcbsp_dai_startup ( struct snd_pcm_substream * substream , <nl> * smaller buffer than the FIFO size to avoid underruns <nl> */ <nl> snd_pcm_hw_rule_add ( substream -> runtime , 0 , <nl> - SNDRV_PCM_HW_PARAM_CHANNELS , <nl> + SNDRV_PCM_HW_PARAM_BUFFER_SIZE , <nl> omap_mcbsp_hwrule_min_buffersize , <nl> mcbsp , <nl> - SNDRV_PCM_HW_PARAM_BUFFER_SIZE , - 1 ); <nl> + SNDRV_PCM_HW_PARAM_CHANNELS , - 1 ); <nl>  <nl> /* Make sure , that the period size is always even */ <nl> snd_pcm_hw_constraint_step ( substream -> runtime , 0 ,
static const struct key_entry eeepc_wmi_keymap [] = { <nl> { KE_KEY , 0xcc , { KEY_SWITCHVIDEOMODE } }, <nl> { KE_KEY , 0xe0 , { KEY_PROG1 } }, <nl> { KE_KEY , 0xe1 , { KEY_F14 } }, <nl> - { KE_KEY , 0xe9 , { KEY_DISPLAY_OFF } }, <nl> + { KE_KEY , 0xe9 , { KEY_BRIGHTNESS_ZERO } }, <nl> { KE_END , 0 }, <nl> }; <nl> 
parse_dcb15_entry ( struct drm_device * dev , struct dcb_table * dcb , <nl> entry -> tvconf . has_component_output = false ; <nl> break ; <nl> case OUTPUT_LVDS : <nl> - if (( conn & 0x00003f00 ) != 0x10 ) <nl> + if (( conn & 0x00003f00 ) >> 8 != 0x10 ) <nl> entry -> lvdsconf . use_straps_for_mode = true ; <nl> entry -> lvdsconf . use_power_scripts = true ; <nl> break ;
struct pci_dev * of_create_pci_dev ( struct pci_pbm_info * pbm , <nl> const char * type ; <nl> u32 class ; <nl>  <nl> - dev = kzalloc ( sizeof ( struct pci_dev ), GFP_KERNEL ); <nl> + dev = alloc_pci_dev (); <nl> if (! dev ) <nl> return NULL ; <nl> 
struct snd_soc_card { <nl> /* <nl> * Card - specific routes and widgets . <nl> */ <nl> - struct snd_soc_dapm_widget * dapm_widgets ; <nl> + const struct snd_soc_dapm_widget * dapm_widgets ; <nl> int num_dapm_widgets ; <nl> - struct snd_soc_dapm_route * dapm_routes ; <nl> + const struct snd_soc_dapm_route * dapm_routes ; <nl> int num_dapm_routes ; <nl>  <nl> struct work_struct deferred_resume_work ;
void ceph_handle_snap ( struct ceph_mds_client * mdsc , <nl> * queued ( again ) by ceph_update_snap_trace () <nl> * below . Queue it _now_ , under the old context . <nl> */ <nl> + spin_lock (& realm -> inodes_with_caps_lock ); <nl> list_del_init (& ci -> i_snap_realm_item ); <nl> + spin_unlock (& realm -> inodes_with_caps_lock ); <nl> spin_unlock (& inode -> i_lock ); <nl>  <nl> ceph_queue_cap_snap ( ci ,
static int mxt_lookup_bootloader_address ( struct mxt_data * data ) <nl> switch ( appmode ) { <nl> case 0x4a : <nl> case 0x4b : <nl> + /* Chips after 1664S use different scheme */ <nl> + if ( data -> info . family_id >= 0xa2 ) { <nl> + bootloader = appmode - 0x24 ; <nl> + break ; <nl> + } <nl> + /* Fall through for normal case */ <nl> case 0x4c : <nl> case 0x4d : <nl> case 0x5a :
xfs_allocbt_free_block ( <nl> xfs_extent_busy_insert ( cur -> bc_tp , be32_to_cpu ( agf -> agf_seqno ), bno , 1 , <nl> XFS_EXTENT_BUSY_SKIP_DISCARD ); <nl> xfs_trans_agbtree_delta ( cur -> bc_tp , - 1 ); <nl> + <nl> + xfs_trans_binval ( cur -> bc_tp , bp ); <nl> return 0 ; <nl> } <nl> 
static int ravb_close ( struct net_device * ndev ) <nl> priv -> phydev = NULL ; <nl> } <nl>  <nl> + if ( priv -> chip_id == RCAR_GEN3 ) <nl> + free_irq ( priv -> emac_irq , ndev ); <nl> free_irq ( ndev -> irq , ndev ); <nl>  <nl> napi_disable (& priv -> napi [ RAVB_NC ]);
card_probe_error : <nl> if ( card -> remove ) <nl> card -> remove ( card ); <nl>  <nl> + snd_soc_dapm_free (& card -> dapm ); <nl> soc_cleanup_card_debugfs ( card ); <nl> snd_card_free ( card -> snd_card ); <nl> 
static ssize_t gt_boost_freq_mhz_store ( struct device * kdev , <nl> { <nl> struct drm_minor * minor = dev_to_drm_minor ( kdev ); <nl> struct drm_device * dev = minor -> dev ; <nl> - struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> + struct drm_i915_private * dev_priv = to_i915 ( dev ); <nl> u32 val ; <nl> ssize_t ret ; <nl> 
static int bnx2x_get_hwinfo ( struct bnx2x * bp ) <nl> } else <nl> BNX2X_DEV_INFO (" illegal OV for SD \ n "); <nl> break ; <nl> + case SHARED_FEAT_CFG_FORCE_SF_MODE_FORCED_SF : <nl> + bp -> mf_config [ vn ] = 0 ; <nl> + break ; <nl> default : <nl> /* Unknown configuration : reset mf_config */ <nl> bp -> mf_config [ vn ] = 0 ;
int w1_process ( void * data ) <nl> jremain = 1 ; <nl> } <nl>  <nl> - try_to_freeze (); <nl> __set_current_state ( TASK_INTERRUPTIBLE ); <nl>  <nl> /* hold list_mutex until after interruptible to prevent loosing
out : <nl>  <nl> static void __init zynq_timer_init ( void ) <nl> { <nl> - zynq_early_slcr_init (); <nl> - <nl> zynq_clock_init (); <nl> of_clk_init ( NULL ); <nl> clocksource_probe (); <nl> static void __init zynq_map_io ( void ) <nl>  <nl> static void __init zynq_irq_init ( void ) <nl> { <nl> + zynq_early_slcr_init (); <nl> irqchip_init (); <nl> } <nl> 
pkttype_mt ( const struct sk_buff * skb , const struct net_device * in , <nl> const struct xt_pkttype_info * info = matchinfo ; <nl>  <nl> if ( skb -> pkt_type == PACKET_LOOPBACK ) <nl> - type = ipv4_is_multicast ( ip_hdr ( skb )-> daddr ) <nl> + type = match -> family == AF_INET && <nl> + ipv4_is_multicast ( ip_hdr ( skb )-> daddr ) <nl> ? PACKET_MULTICAST <nl> : PACKET_BROADCAST ; <nl> else
static ssize_t btrfs_direct_IO ( int rw , struct kiocb * iocb , <nl> btrfs_submit_direct , 0 ); <nl> } <nl>  <nl> +# define BTRFS_FIEMAP_FLAGS ( FIEMAP_FLAG_SYNC ) <nl> + <nl> static int btrfs_fiemap ( struct inode * inode , struct fiemap_extent_info * fieinfo , <nl> __u64 start , __u64 len ) <nl> { <nl> + int ret ; <nl> + <nl> + ret = fiemap_check_flags ( fieinfo , BTRFS_FIEMAP_FLAGS ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> return extent_fiemap ( inode , fieinfo , start , len , btrfs_get_extent_fiemap ); <nl> } <nl> 
get_more_pages : <nl> ci -> i_truncate_seq , <nl> ci -> i_truncate_size , <nl> & inode -> i_mtime , true , 1 , 0 ); <nl> + <nl> + if (! req ) { <nl> + rc = - ENOMEM ; <nl> + unlock_page ( page ); <nl> + break ; <nl> + } <nl> + <nl> max_pages = req -> r_num_pages ; <nl>  <nl> alloc_page_vec ( fsc , req );
static struct page * balloon_next_page ( struct page * page ) <nl>  <nl> static enum bp_state update_schedule ( enum bp_state state ) <nl> { <nl> + if ( state == BP_ECANCELED ) <nl> + return BP_ECANCELED ; <nl> + <nl> if ( state == BP_DONE ) { <nl> balloon_stats . schedule_delay = 1 ; <nl> balloon_stats . retry_count = 1 ;
static int __init hdaps_init ( void ) <nl>  <nl> if (! dmi_check_system ( hdaps_whitelist )) { <nl> printk ( KERN_WARNING " hdaps : supported laptop not found !\ n "); <nl> - ret = - ENXIO ; <nl> + ret = - ENODEV ; <nl> goto out ; <nl> } <nl> 
int mei_cl_connect ( struct mei_cl * cl , struct file * file ) <nl> mutex_lock (& dev -> device_lock ); <nl>  <nl> if ( cl -> state != MEI_FILE_CONNECTED ) { <nl> + cl -> state = MEI_FILE_DISCONNECTED ; <nl> /* something went really wrong */ <nl> if (! cl -> status ) <nl> cl -> status = - EFAULT ;
static void __init _set_omap_chip ( void ) <nl>  <nl> } <nl>  <nl> - void __init omap2_check_revision ( void ) <nl> + void __init omap24xx_check_revision ( void ) <nl> { <nl> int i , j ; <nl> u32 idcode ; <nl> void __init omap2_check_revision ( void ) <nl>  <nl> } <nl>  <nl> + void __init omap2_check_revision ( void ) <nl> +{ <nl> + omap24xx_check_revision (); <nl> +} <nl> + <nl> void __init omap2_set_globals_tap ( struct omap_globals * omap2_globals ) <nl> { <nl> class = omap2_globals -> class ;
static int spidev_release ( struct inode * inode , struct file * filp ) <nl> kfree ( spidev -> rx_buffer ); <nl> spidev -> rx_buffer = NULL ; <nl>  <nl> - spidev -> speed_hz = spidev -> spi -> max_speed_hz ; <nl> + if ( spidev -> spi ) <nl> + spidev -> speed_hz = spidev -> spi -> max_speed_hz ; <nl>  <nl> /* ... after we unbound from the underlying device ? */ <nl> spin_lock_irq (& spidev -> spi_lock );
void rv770_set_uvd_clock_before_set_eng_clock ( struct radeon_device * rdev , <nl> if ( new_state -> high . sclk >= current_state -> high . sclk ) <nl> return ; <nl>  <nl> - radeon_set_uvd_clocks ( rdev , new_ps -> vclk , old_ps -> dclk ); <nl> + radeon_set_uvd_clocks ( rdev , new_ps -> vclk , new_ps -> dclk ); <nl> } <nl>  <nl> void rv770_set_uvd_clock_after_set_eng_clock ( struct radeon_device * rdev ,
retry : <nl> /* <nl> * Recalculate credits when extent tree depth changes . <nl> */ <nl> - if ( depth >= 0 && depth != ext_depth ( inode )) { <nl> + if ( depth != ext_depth ( inode )) { <nl> credits = ext4_chunk_trans_blocks ( inode , len ); <nl> depth = ext_depth ( inode ); <nl> }
static int mv643xx_eth_receive_queue ( struct net_device * dev ) <nl> netif_rx ( skb ); <nl> # endif <nl> } <nl> + dev -> last_rx = jiffies ; <nl> } <nl>  <nl> return received_packets ;
static int shmem_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> i_size_write ( inode , newsize ); <nl> inode -> i_ctime = inode -> i_mtime = CURRENT_TIME ; <nl> } <nl> - if ( newsize < oldsize ) { <nl> + if ( newsize <= oldsize ) { <nl> loff_t holebegin = round_up ( newsize , PAGE_SIZE ); <nl> unmap_mapping_range ( inode -> i_mapping , holebegin , 0 , 1 ); <nl> shmem_truncate_range ( inode , newsize , ( loff_t )- 1 );
# endif <nl> # include " igb . h " <nl>  <nl> -# define DRV_VERSION " 2 . 4 . 13 - k2 " <nl> +# define MAJ 3 <nl> +# define MIN 0 <nl> +# define BUILD 6 <nl> +# define KFIX 2 <nl> +# define DRV_VERSION __stringify ( MAJ ) "." __stringify ( MIN ) "." \ <nl> + __stringify ( BUILD ) "- k " __stringify ( KFIX ) <nl> char igb_driver_name [] = " igb "; <nl> char igb_driver_version [] = DRV_VERSION ; <nl> static const char igb_driver_string [] =
void rt2x00link_update_stats ( struct rt2x00_dev * rt2x00dev , <nl> struct link_ant * ant = & rt2x00dev -> link . ant ; <nl> struct ieee80211_hdr * hdr = ( struct ieee80211_hdr *) skb -> data ; <nl>  <nl> + /* <nl> + * No need to update the stats for != STA interfaces <nl> + */ <nl> + if (! rt2x00dev -> intf_sta_count ) <nl> + return ; <nl> + <nl> /* <nl> * Frame was received successfully since non - succesfull <nl> * frames would have been dropped by the hardware .
static int __diag_ipl_functions ( struct kvm_vcpu * vcpu ) <nl>  <nl> VCPU_EVENT ( vcpu , 5 , " diag ipl functions , subcode % lx ", subcode ); <nl> switch ( subcode ) { <nl> + case 0 : <nl> + case 1 : <nl> + page_table_reset_pgste ( current -> mm , 0 , TASK_SIZE ); <nl> + return - EOPNOTSUPP ; <nl> case 3 : <nl> vcpu -> run -> s390_reset_flags = KVM_S390_RESET_CLEAR ; <nl> page_table_reset_pgste ( current -> mm , 0 , TASK_SIZE );
int ll_fid2path ( struct inode * inode , void __user * arg ) <nl> if ( get_user ( pathlen , & gfin -> gf_pathlen )) <nl> return - EFAULT ; <nl>  <nl> + if ( pathlen > PATH_MAX ) <nl> + return - EINVAL ; <nl> + <nl> outsize = sizeof (* gfout ) + pathlen ; <nl>  <nl> OBD_ALLOC ( gfout , outsize );
static int __devinit isp1761_pci_probe ( struct pci_dev * dev , <nl> hcd = isp1760_register ( pci_mem_phy0 , length , dev -> irq , <nl> IRQF_SHARED | IRQF_DISABLED , & dev -> dev , dev_name (& dev -> dev ), <nl> devflags ); <nl> - pci_set_drvdata ( dev , hcd ); <nl> - if (! hcd ) <nl> + if (! IS_ERR ( hcd )) { <nl> + pci_set_drvdata ( dev , hcd ); <nl> return 0 ; <nl> + } <nl> clean : <nl> status = - ENODEV ; <nl> iounmap ( iobase );
void perf_hpp__column_disable ( unsigned col ) <nl>  <nl> void perf_hpp__cancel_cumulate ( void ) <nl> { <nl> + if ( field_order ) <nl> + return ; <nl> + <nl> perf_hpp__column_disable ( PERF_HPP__OVERHEAD_ACC ); <nl> perf_hpp__format [ PERF_HPP__OVERHEAD ]. header = hpp__header_overhead ; <nl> }
int mccic_irq ( struct mcam_camera * cam , unsigned int irqs ) <nl> if ( irqs & ( IRQ_EOF0 << frame )) { <nl> mcam_frame_complete ( cam , frame ); <nl> handled = 1 ; <nl> + if ( cam -> buffer_mode == B_DMA_sg ) <nl> + break ; <nl> } <nl> /* <nl> * If a frame starts , note that we have DMA active . This
static ssize_t set_pwm_mode ( struct device * dev , struct device_attribute * attr , <nl> if (!( val == 0 || val == 1 )) <nl> return - EINVAL ; <nl>  <nl> + /* F75373 does not support DC ( linear voltage ) fan control mode */ <nl> + if ( data -> kind == f75373 && val == 0 ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& data -> update_lock ); <nl> conf = f75375_read8 ( client , F75375_REG_CONFIG1 ); <nl> conf &= ~( 1 << FAN_CTRL_LINEAR ( nr ));
static u32 mop500_sdi0_vdd_handler ( struct device * dev , unsigned int vdd , <nl> unsigned char power_mode ) <nl> { <nl> if ( power_mode == MMC_POWER_UP ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 1 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 1 ); <nl> else if ( power_mode == MMC_POWER_OFF ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 0 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 0 ); <nl>  <nl> return MCI_FBCLKEN | MCI_CMDDIREN | MCI_DATA0DIREN | <nl> MCI_DATA2DIREN | MCI_DATA31DIREN ;
static void bfin_musb_try_idle ( struct musb * musb , unsigned long timeout ) <nl> mod_timer (& musb_conn_timer , jiffies + TIMER_DELAY ); <nl> } <nl>  <nl> - static int bfin_musb_get_vbus_status ( struct musb * musb ) <nl> + static int bfin_musb_vbus_status ( struct musb * musb ) <nl> { <nl> return 0 ; <nl> }
static int tcp_ack_update_window ( struct sock * sk , struct tcp_sock * tp , <nl> /* Note , it is the only place , where <nl> * fast path is recovered for sending TCP . <nl> */ <nl> + tp -> pred_flags = 0 ; <nl> tcp_fast_path_check ( sk , tp ); <nl>  <nl> if ( nwin > tp -> max_window ) {
static struct dma_chan * zx_of_dma_simple_xlate ( struct of_phandle_args * dma_spec , <nl> struct dma_chan * chan ; <nl> struct zx_dma_chan * c ; <nl>  <nl> - if ( request > d -> dma_requests ) <nl> + if ( request >= d -> dma_requests ) <nl> return NULL ; <nl>  <nl> chan = dma_get_any_slave_channel (& d -> slave );
static int hws_cpu_callback ( struct notifier_block * nfb , <nl> { <nl> /* We do not have sampler space available for all possible CPUs . <nl> All CPUs should be online when hw sampling is activated . */ <nl> - return NOTIFY_BAD ; <nl> + return ( hws_state <= HWS_DEALLOCATED ) ? NOTIFY_OK : NOTIFY_BAD ; <nl> } <nl>  <nl> static struct notifier_block hws_cpu_notifier = {
EXPORT_SYMBOL_GPL ( omap_dm_timer_enable ); <nl>  <nl> void omap_dm_timer_disable ( struct omap_dm_timer * timer ) <nl> { <nl> - pm_runtime_put (& timer -> pdev -> dev ); <nl> + pm_runtime_put_sync (& timer -> pdev -> dev ); <nl> } <nl> EXPORT_SYMBOL_GPL ( omap_dm_timer_disable ); <nl> 
static void free_sa_defrag_extent ( struct new_sa_defrag_extent * new ) <nl> return ; <nl>  <nl> list_for_each_entry_safe ( old , tmp , & new -> head , list ) { <nl> - list_del (& old -> list ); <nl> kfree ( old ); <nl> } <nl> kfree ( new );
static int cxgb_extension_ioctl ( struct net_device * dev , void __user * useraddr ) <nl> case CHELSIO_GET_QSET_NUM :{ <nl> struct ch_reg edata ; <nl>  <nl> + memset (& edata , 0 , sizeof ( struct ch_reg )); <nl> + <nl> edata . cmd = CHELSIO_GET_QSET_NUM ; <nl> edata . val = pi -> nqsets ; <nl> if ( copy_to_user ( useraddr , & edata , sizeof ( edata )))
static int load_twl4030_script ( struct twl4030_script * tscript , <nl> goto out ; <nl> } <nl> if ( tscript -> flags & TWL4030_WAKEUP12_SCRIPT ) { <nl> + /* Reset any existing sleep script to avoid hangs on reboot */ <nl> + err = twl_i2c_write_u8 ( TWL_MODULE_PM_MASTER , END_OF_SCRIPT , <nl> + R_SEQ_ADD_A2S ); <nl> + if ( err ) <nl> + goto out ; <nl> + <nl> err = twl4030_config_wakeup12_sequence ( address ); <nl> if ( err ) <nl> goto out ;
static ssize_t macvtap_put_user ( struct macvtap_queue * q , <nl> if ( copy_to_iter (& vnet_hdr , sizeof ( vnet_hdr ), iter ) != <nl> sizeof ( vnet_hdr )) <nl> return - EFAULT ; <nl> + <nl> + iov_iter_advance ( iter , vnet_hdr_len - sizeof ( vnet_hdr )); <nl> } <nl> total = vnet_hdr_len ; <nl> total += skb -> len ;
static pci_ers_result_t atl1c_io_error_detected ( struct pci_dev * pdev , <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + if ( state == pci_channel_io_perm_failure ) <nl> + return PCI_ERS_RESULT_DISCONNECT ; <nl> + <nl> if ( netif_running ( netdev )) <nl> atl1c_down ( adapter ); <nl> 
lpfc_els_retry ( struct lpfc_hba * phba , struct lpfc_iocbq * cmdiocb , <nl> /* FLOGI retry policy */ <nl> retry = 1 ; <nl> /* retry FLOGI forever */ <nl> - maxretry = 0 ; <nl> + if ( phba -> link_flag != LS_LOOPBACK_MODE ) <nl> + maxretry = 0 ; <nl> + else <nl> + maxretry = 2 ; <nl> + <nl> if ( cmdiocb -> retry >= 100 ) <nl> delay = 5000 ; <nl> else if ( cmdiocb -> retry >= 32 )
static int rx_intr_handler ( struct ring_info * ring_data , int budget ) <nl> struct RxD1 * rxdp1 ; <nl> struct RxD3 * rxdp3 ; <nl>  <nl> + if ( budget <= 0 ) <nl> + return napi_pkts ; <nl> + <nl> get_info = ring_data -> rx_curr_get_info ; <nl> get_block = get_info . block_index ; <nl> memcpy (& put_info , & ring_data -> rx_curr_put_info , sizeof ( put_info ));
static void tilcdc_crtc_destroy ( struct drm_crtc * crtc ) <nl> struct tilcdc_crtc * tilcdc_crtc = to_tilcdc_crtc ( crtc ); <nl> struct tilcdc_drm_private * priv = crtc -> dev -> dev_private ; <nl>  <nl> + drm_modeset_lock_crtc ( crtc , NULL ); <nl> tilcdc_crtc_disable ( crtc ); <nl> + drm_modeset_unlock_crtc ( crtc ); <nl>  <nl> flush_workqueue ( priv -> wq ); <nl> 
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
int __init acpi_parse_mcfg ( unsigned long phys_addr , unsigned long size ) <nl> if ( mcfg -> config [ i ]. base_reserved ) { <nl> printk ( KERN_ERR PREFIX <nl> " MMCONFIG not in low 4GB of memory \ n "); <nl> + kfree ( pci_mmcfg_config ); <nl> + pci_mmcfg_config_num = 0 ; <nl> return - ENODEV ; <nl> } <nl> }
static struct resource wdt_sch_resource = { <nl>  <nl> static struct mfd_cell tunnelcreek_cells [] = { <nl> { <nl> - . name = " tunnelcreek_wdt ", <nl> + . name = " ie6xx_wdt ", <nl> . num_resources = 1 , <nl> . resources = & wdt_sch_resource , <nl> },
static int jr3_pci_auto_attach ( struct comedi_device * dev , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if ( pci_resource_len ( pcidev , 0 ) < board -> n_subdevs * sizeof (* block )) <nl> + return - ENXIO ; <nl> + <nl> dev -> mmio = pci_ioremap_bar ( pcidev , 0 ); <nl> if (! dev -> mmio ) <nl> return - ENOMEM ;
ssize_t uwb_est_get_size ( struct uwb_rc * uwb_rc , struct uwb_est * est , <nl> case UWB_EST_8 : type_size = sizeof ( u8 ); break ; <nl> default : BUG (); <nl> } <nl> - if ( offset + type_size >= rceb_size ) { <nl> + if ( offset + type_size > rceb_size ) { <nl> if ( printk_ratelimit ()) <nl> dev_err ( dev , " EST % p 0x % 04x /% 04x /% 04x [% u ]: " <nl> " not enough data to read extra size \ n ",
static int create_device ( struct ramzswap * rzs , int device_id ) <nl> * or set equal to backing swap device ( if provided ) <nl> */ <nl> set_capacity ( rzs -> disk , 0 ); <nl> + <nl> + blk_queue_physical_block_size ( rzs -> disk -> queue , PAGE_SIZE ); <nl> + blk_queue_logical_block_size ( rzs -> disk -> queue , PAGE_SIZE ); <nl> + <nl> add_disk ( rzs -> disk ); <nl>  <nl> rzs -> init_done = 0 ;
int dns_query ( const char * type , const char * name , size_t namelen , <nl> if (!* _result ) <nl> goto put ; <nl>  <nl> - memcpy (* _result , upayload -> data , len + 1 ); <nl> + memcpy (* _result , upayload -> data , len ); <nl> + * _result [ len ] = '\ 0 '; <nl> + <nl> if ( _expiry ) <nl> * _expiry = rkey -> expiry ; <nl> 
static void gen3_init_clock_gating ( struct drm_device * dev ) <nl> dstate |= DSTATE_PLL_D3_OFF | DSTATE_GFX_CLOCK_GATING | <nl> DSTATE_DOT_CLOCK_GATING ; <nl> I915_WRITE ( D_STATE , dstate ); <nl> + <nl> + if ( IS_PINEVIEW ( dev )) <nl> + I915_WRITE ( ECOSKPD , _MASKED_BIT_ENABLE ( ECO_GATING_CX_ONLY )); <nl> } <nl>  <nl> static void i85x_init_clock_gating ( struct drm_device * dev )
static long vop_ioctl ( struct file * f , unsigned int cmd , unsigned long arg ) <nl> ret = - EFAULT ; <nl> goto free_ret ; <nl> } <nl> + /* Ensure desc has not changed between the two reads */ <nl> + if ( memcmp (& dd , dd_config , sizeof ( dd ))) { <nl> + ret = - EINVAL ; <nl> + goto free_ret ; <nl> + } <nl> mutex_lock (& vdev -> vdev_mutex ); <nl> mutex_lock (& vi -> vop_mutex ); <nl> ret = vop_virtio_add_device ( vdev , dd_config );
static void broxton_phy_init ( struct drm_i915_private * dev_priv , <nl> DRM_DEBUG_DRIVER (" DDI PHY % d already enabled , " <nl> " won ' t reprogram it \ n ", phy ); <nl> /* Still read out the GRC value for state verification */ <nl> - if ( phy == DPIO_PHY1 ) <nl> + if ( phy == DPIO_PHY0 ) <nl> dev_priv -> bxt_phy_grc = broxton_get_grc ( dev_priv , phy ); <nl>  <nl> return ;
static int davinci_spi_setup_transfer ( struct spi_device * spi , <nl> struct davinci_spi * dspi ; <nl> struct davinci_spi_config * spicfg ; <nl> u8 bits_per_word = 0 ; <nl> - u32 hz = 0 , spifmt = 0 , prescale = 0 ; <nl> + u32 hz = 0 , spifmt = 0 ; <nl> + int prescale ; <nl>  <nl> dspi = spi_master_get_devdata ( spi -> master ); <nl> spicfg = ( struct davinci_spi_config *) spi -> controller_data ;
static int evm_protect_xattr ( struct dentry * dentry , const char * xattr_name , <nl> goto out ; <nl> } <nl> evm_status = evm_verify_current_integrity ( dentry ); <nl> + if ( evm_status == INTEGRITY_NOXATTRS ) { <nl> + struct integrity_iint_cache * iint ; <nl> + <nl> + iint = integrity_iint_find ( dentry -> d_inode ); <nl> + if ( iint && ( iint -> flags & IMA_NEW_FILE )) <nl> + return 0 ; <nl> + } <nl> out : <nl> if ( evm_status != INTEGRITY_PASS ) <nl> integrity_audit_msg ( AUDIT_INTEGRITY_METADATA , dentry -> d_inode ,
static int i2s_pll_clk_probe ( struct platform_device * pdev ) <nl> if ( IS_ERR ( pll_clk -> base )) <nl> return PTR_ERR ( pll_clk -> base ); <nl>  <nl> + memset (& init , 0 , sizeof ( init )); <nl> clk_name = node -> name ; <nl> init . name = clk_name ; <nl> init . ops = & i2s_pll_ops ;
static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x23 , { KEY_PROG3 } }, /* P_Key */ <nl> { KE_KEY , 0x24 , { KEY_PROG4 } }, /* Social networking_Key */ <nl> { KE_KEY , 0x64 , { KEY_SWITCHVIDEOMODE } }, /* Display Switch */ <nl> - { KE_KEY , 0x82 , { KEY_F22 } }, /* Touch Pad On / Off */ <nl> + { KE_KEY , 0x82 , { KEY_TOUCHPAD_TOGGLE } }, /* Touch Pad On / Off */ <nl> { KE_END , 0 } <nl> }; <nl> 
int symsrc__init ( struct symsrc * ss , struct dso * dso __maybe_unused , <nl> if (! ss -> name ) <nl> goto out_close ; <nl>  <nl> + ss -> fd = fd ; <nl> ss -> type = type ; <nl>  <nl> return 0 ;
retry : <nl> return UBI_IO_BITFLIPS ; <nl> } <nl>  <nl> - if ( read != len && retries ++ < UBI_IO_RETRIES ) { <nl> + if ( retries ++ < UBI_IO_RETRIES ) { <nl> dbg_io (" error % d % s while reading % d bytes from PEB % d :% d ," <nl> " read only % zd bytes , retry ", <nl> err , errstr , len , pnum , offset , read );
cifs_set_file_info ( struct inode * inode , struct iattr * attrs , int xid , <nl> struct cifsTconInfo * pTcon = cifs_sb -> tcon ; <nl> FILE_BASIC_INFO info_buf ; <nl>  <nl> + if ( attrs == NULL ) <nl> + return - EINVAL ; <nl> + <nl> if ( attrs -> ia_valid & ATTR_ATIME ) { <nl> set_time = true ; <nl> info_buf . LastAccessTime =
static int rt2800_get_gain_calibration_delta ( struct rt2x00_dev * rt2x00dev ) <nl> u8 step ; <nl> int i ; <nl>  <nl> + /* <nl> + * First check if temperature compensation is supported . <nl> + */ <nl> + rt2800_eeprom_read ( rt2x00dev , EEPROM_NIC_CONF1 , & eeprom ); <nl> + if (! rt2x00_get_field16 ( eeprom , EEPROM_NIC_CONF1_EXTERNAL_TX_ALC )) <nl> + return 0 ; <nl> + <nl> /* <nl> * Read TSSI boundaries for temperature compensation from <nl> * the EEPROM .
static void frontend_init ( struct dvb_bt8xx_card * card , u32 type ) <nl> /* DST is not a frontend , attaching the ASIC */ <nl> if ( dvb_attach ( dst_attach , state , & card -> dvb_adapter ) == NULL ) { <nl> pr_err ("% s : Could not find a Twinhan DST \ n ", __func__ ); <nl> + kfree ( state ); <nl> break ; <nl> } <nl> /* Attach other DST peripherals if any */
static void logfs_put_super ( struct super_block * sb ) <nl> { <nl> struct logfs_super * super = logfs_super ( sb ); <nl> /* kill the meta - inodes */ <nl> - iput ( super -> s_master_inode ); <nl> iput ( super -> s_segfile_inode ); <nl> + iput ( super -> s_master_inode ); <nl> iput ( super -> s_mapping_inode ); <nl> } <nl> 
static int si_common_early_init ( void * handle ) <nl> AMD_CG_SUPPORT_HDP_LS | <nl> AMD_CG_SUPPORT_HDP_MGCG ; <nl> adev -> pg_flags = 0 ; <nl> + adev -> external_rev_id = 60 ; <nl> break ; <nl> case CHIP_HAINAN : <nl> adev -> cg_flags =
static int wm8731_register ( struct wm8731_priv * wm8731 ) <nl>  <nl> memcpy ( codec -> reg_cache , wm8731_reg , sizeof ( wm8731_reg )); <nl>  <nl> + ret = wm8731_reset ( codec ); <nl> + if ( ret < 0 ) { <nl> + dev_err ( codec -> dev , " Failed to issue reset \ n "); <nl> + return ret ; <nl> + } <nl> + <nl> wm8731_dai . dev = codec -> dev ; <nl>  <nl> - wm8731_reset ( codec ); <nl> wm8731_set_bias_level ( codec , SND_SOC_BIAS_STANDBY ); <nl>  <nl> /* Latch the update bits */
out_destroy : <nl> out_free : <nl> pr_info ("% s : failed to register PMU devices !\ n ", <nl> of_node_full_name ( node )); <nl> + kfree ( pmu -> irq_affinity ); <nl> kfree ( pmu ); <nl> return ret ; <nl> }
int eeh_dn_check_failure ( struct device_node * dn , struct pci_dev * dev ) <nl> no_dn ++; <nl> return 0 ; <nl> } <nl> + dn = find_device_pe ( dn ); <nl> pdn = PCI_DN ( dn ); <nl>  <nl> /* Access to IO BARs might get this far and still not want checking . */
struct dvb_frontend * dib9000_attach ( struct i2c_adapter * i2c_adap , u8 i2c_addr , c <nl> if ( st == NULL ) <nl> return NULL ; <nl> fe = kzalloc ( sizeof ( struct dvb_frontend ), GFP_KERNEL ); <nl> - if ( fe == NULL ) <nl> + if ( fe == NULL ) { <nl> + kfree ( st ); <nl> return NULL ; <nl> + } <nl>  <nl> memcpy (& st -> chip . d9 . cfg , cfg , sizeof ( struct dib9000_config )); <nl> st -> i2c . i2c_adap = i2c_adap ;
static int assign_guest_irq ( struct kvm * kvm , <nl> dev -> irq_requested_type |= guest_irq_type ; <nl> if ( dev -> ack_notifier . gsi != - 1 ) <nl> kvm_register_irq_ack_notifier ( kvm , & dev -> ack_notifier ); <nl> - } else <nl> + } else { <nl> kvm_free_irq_source_id ( kvm , dev -> irq_source_id ); <nl> + dev -> irq_source_id = - 1 ; <nl> + } <nl>  <nl> return r ; <nl> }
static int get_sig_strength ( struct drx_demod_instance * demod , u16 * sig_strength ) <nl> * sig_strength = ( 20 * if_gain / if_agc_sns ); <nl> } <nl>  <nl> + if (* sig_strength <= 7 ) <nl> + * sig_strength = 0 ; <nl> + <nl> return 0 ; <nl> rw_error : <nl> return - EIO ;
void snd_trident_write_voice_regs ( trident_t * trident , <nl> break ; <nl> default : <nl> snd_BUG (); <nl> + return ; <nl> } <nl>  <nl> outb ( voice -> number , TRID_REG ( trident , T4D_LFO_GC_CIR ));
unsigned int ata_sff_qc_issue ( struct ata_queued_cmd * qc ) <nl> break ; <nl>  <nl> default : <nl> - WARN_ON_ONCE ( 1 ); <nl> return AC_ERR_SYSTEM ; <nl> } <nl> 
static void cyberjack_read_int_callback ( struct urb * urb ) <nl>  <nl> old_rdtodo = priv -> rdtodo ; <nl>  <nl> - if ( old_rdtodo + size < old_rdtodo ) { <nl> + if ( old_rdtodo > SHRT_MAX - size ) { <nl> dev_dbg ( dev , " To many bulk_in urbs to do .\ n "); <nl> spin_unlock (& priv -> lock ); <nl> goto resubmit ;
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> - free ( ptr ); <nl> + kmem_free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
struct wm8994_ldo_pdata { <nl> int enable ; <nl>  <nl> const char * supply ; <nl> - struct regulator_init_data * init_data ; <nl> + const struct regulator_init_data * init_data ; <nl> }; <nl>  <nl> # define WM8994_CONFIGURE_GPIO 0x10000
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> if (!( src_file -> f_mode & FMODE_READ )) <nl> goto out_fput ; <nl>  <nl> + /* don ' t make the dst file partly checksummed */ <nl> + if (( BTRFS_I ( src )-> flags & BTRFS_INODE_NODATASUM ) != <nl> + ( BTRFS_I ( inode )-> flags & BTRFS_INODE_NODATASUM )) <nl> + goto out_fput ; <nl> + <nl> ret = - EISDIR ; <nl> if ( S_ISDIR ( src -> i_mode ) || S_ISDIR ( inode -> i_mode )) <nl> goto out_fput ;
static int get_cac_tdp_table ( <nl>  <nl> hwmgr -> dyn_state . cac_dtp_table = kzalloc ( table_size , GFP_KERNEL ); <nl>  <nl> - if ( NULL == hwmgr -> dyn_state . cac_dtp_table ) <nl> + if ( NULL == hwmgr -> dyn_state . cac_dtp_table ) { <nl> + kfree ( tdp_table ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> memset ( hwmgr -> dyn_state . cac_dtp_table , 0x00 , table_size ); <nl> 
static void get_new_segment ( struct f2fs_sb_info * sbi , <nl> if (! new_sec && ((* newseg + 1 ) % sbi -> segs_per_sec )) { <nl> segno = find_next_zero_bit ( free_i -> free_segmap , <nl> TOTAL_SEGS ( sbi ), * newseg + 1 ); <nl> - if ( segno < TOTAL_SEGS ( sbi )) <nl> + if ( segno - * newseg < sbi -> segs_per_sec - <nl> + (* newseg % sbi -> segs_per_sec )) <nl> goto got_it ; <nl> } <nl> find_other_zone :
static void do_ubd_request ( struct request_queue * q ) <nl> " errno = % d \ n ", - n ); <nl> else if ( list_empty (& dev -> restart )) <nl> list_add (& dev -> restart , & restart ); <nl> + kfree ( io_req ); <nl> return ; <nl> } <nl> 
static int smsc95xx_suspend ( struct usb_interface * intf , pm_message_t message ) <nl> ret = smsc95xx_enter_suspend0 ( dev ); <nl>  <nl> done : <nl> - if ( ret ) <nl> + /* <nl> + * TODO : resume () might need to handle the suspend failure <nl> + * in system sleep <nl> + */ <nl> + if ( ret && PMSG_IS_AUTO ( message )) <nl> usbnet_resume ( intf ); <nl> return ret ; <nl> }
static int imx_thermal_probe ( struct platform_device * pdev ) <nl> data -> tempmon = map ; <nl>  <nl> data -> socdata = of_device_get_match_data (& pdev -> dev ); <nl> + if (! data -> socdata ) { <nl> + dev_err (& pdev -> dev , " no device match found \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> /* make sure the IRQ flag is clear before enabling irq on i . MX6SX */ <nl> if ( data -> socdata -> version == TEMPMON_IMX6SX ) {
static struct platform_device sdhi0_device = { <nl> static struct sh_mobile_sdhi_info sdhi1_info = { <nl> . dma_slave_tx = SHDMA_SLAVE_SDHI1_TX , <nl> . dma_slave_rx = SHDMA_SLAVE_SDHI1_RX , <nl> - . tmio_ocr_mask = MMC_VDD_165_195 , <nl> . tmio_flags = TMIO_MMC_WRPROTECT_DISABLE , <nl> . tmio_caps = MMC_CAP_SD_HIGHSPEED | MMC_CAP_SDIO_IRQ | <nl> MMC_CAP_NEEDS_POLL , <nl> static struct resource sh_mmcif_resources [] = { <nl>  <nl> static struct sh_mmcif_plat_data sh_mmcif_plat = { <nl> . sup_pclk = 0 , <nl> - . ocr = MMC_VDD_165_195 | MMC_VDD_32_33 | MMC_VDD_33_34 , <nl> . caps = MMC_CAP_4_BIT_DATA | <nl> MMC_CAP_8_BIT_DATA | <nl> MMC_CAP_NEEDS_POLL ,
static void sci_io_request_build_ssp_command_iu ( struct isci_request * ireq ) <nl> cmd_iu -> _r_c = 0 ; <nl>  <nl> sci_swab32_cpy (& cmd_iu -> cdb , task -> ssp_task . cmd -> cmnd , <nl> - task -> ssp_task . cmd -> cmd_len / sizeof ( u32 )); <nl> + ( task -> ssp_task . cmd -> cmd_len + 3 ) / sizeof ( u32 )); <nl> } <nl>  <nl> static void sci_task_request_build_ssp_task_iu ( struct isci_request * ireq )
static void kvmppc_fast_vcpu_kick_hv ( struct kvm_vcpu * vcpu ) <nl> ++ vcpu -> stat . halt_wakeup ; <nl> } <nl>  <nl> - if ( kvmppc_ipi_thread ( vcpu -> arch . thread_cpu )) <nl> + cpu = READ_ONCE ( vcpu -> arch . thread_cpu ); <nl> + if ( cpu >= 0 && kvmppc_ipi_thread ( cpu )) <nl> return ; <nl>  <nl> /* CPU points to the first thread of the core */
int fsl_rio_setup_rmu ( struct rio_mport * mport , struct device_node * node ) <nl> if (! msg_addr ) { <nl> pr_err ("% s : unable to find ' reg ' property of message - unit \ n ", <nl> node -> full_name ); <nl> + kfree ( rmu ); <nl> return - ENOMEM ; <nl> } <nl> msg_start = of_read_number ( msg_addr , aw );
static int pcm3168a_set_dai_sysclk ( struct snd_soc_dai * dai , <nl> int clk_id , unsigned int freq , int dir ) <nl> { <nl> struct pcm3168a_priv * pcm3168a = snd_soc_codec_get_drvdata ( dai -> codec ); <nl> + int ret ; <nl>  <nl> if ( freq > PCM1368A_MAX_SYSCLK ) <nl> return - EINVAL ; <nl>  <nl> + ret = clk_set_rate ( pcm3168a -> scki , freq ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> pcm3168a -> sysclk = freq ; <nl>  <nl> return 0 ;
static void snd_card_asihpi_timer_function ( unsigned long data ) <nl> s -> number ); <nl> ds -> drained_count ++; <nl> if ( ds -> drained_count > 20 ) { <nl> + unsigned long flags ; <nl> + snd_pcm_stream_lock_irqsave ( s , flags ); <nl> snd_pcm_stop ( s , SNDRV_PCM_STATE_XRUN ); <nl> + snd_pcm_stream_unlock_irqrestore ( s , flags ); <nl> continue ; <nl> } <nl> } else {
static void proc_flush_task_mnt ( struct vfsmount * mnt , pid_t pid , pid_t tgid ) <nl> dput ( dentry ); <nl> } <nl>  <nl> + if ( pid == tgid ) <nl> + return ; <nl> + <nl> name . name = buf ; <nl> name . len = snprintf ( buf , sizeof ( buf ), "% d ", tgid ); <nl> leader = d_hash_and_lookup ( mnt -> mnt_root , & name );
int gpiochip_add_data ( struct gpio_chip * chip , void * data ) <nl> * First : allocate and populate the internal stat container , and <nl> * set up the struct device . <nl> */ <nl> - gdev = kmalloc ( sizeof (* gdev ), GFP_KERNEL ); <nl> + gdev = kzalloc ( sizeof (* gdev ), GFP_KERNEL ); <nl> if (! gdev ) <nl> return - ENOMEM ; <nl> gdev -> dev . bus = & gpio_bus_type ;
static int lx_pipe_wait_for_state ( struct lx6464es * chip , u32 pipe , <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> - if ( current_state == state ) <nl> + if (! err && current_state == state ) <nl> return 0 ; <nl>  <nl> mdelay ( 1 );
static void lkdtm_do_action ( enum ctype which ) <nl> break ; <nl>  <nl> val = kmalloc ( 1024 , GFP_KERNEL ); <nl> - if (! val ) <nl> + if (! val ) { <nl> + free_page ( p ); <nl> break ; <nl> + } <nl>  <nl> base = ( int *) p ; <nl> 
union acpi_parse_object ; <nl>  <nl> static char * acpi_gbl_mutex_names [ ACPI_NUM_MUTEX ] = { <nl> " ACPI_MTX_Interpreter ", <nl> - " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Namespace ", <nl> + " ACPI_MTX_Tables ", <nl> " ACPI_MTX_Events ", <nl> " ACPI_MTX_Caches ", <nl> " ACPI_MTX_Memory ",
int of_pci_range_to_resource ( struct of_pci_range * range , <nl> } <nl> res -> start = port ; <nl> } else { <nl> + if (( sizeof ( resource_size_t ) < 8 ) && <nl> + upper_32_bits ( range -> cpu_addr )) { <nl> + err = - EINVAL ; <nl> + goto invalid_range ; <nl> + } <nl> + <nl> res -> start = range -> cpu_addr ; <nl> } <nl> res -> end = res -> start + range -> size - 1 ;
static inline void task_state ( struct seq_file * m , struct pid_namespace * ns , <nl> " FDSize :\ t % d \ n " <nl> " Groups :\ t ", <nl> fdt ? fdt -> max_fds : 0 ); <nl> + task_unlock ( p ); <nl> rcu_read_unlock (); <nl>  <nl> group_info = cred -> group_info ; <nl> - task_unlock ( p ); <nl> - <nl> for ( g = 0 ; g < group_info -> ngroups ; g ++) <nl> seq_printf ( m , "% d ", <nl> from_kgid_munged ( user_ns , GROUP_AT ( group_info , g )));
u32 crypto4xx_build_pd ( struct crypto_async_request * req , <nl>  <nl> /* figure how many gd is needed */ <nl> num_gd = sg_nents_for_len ( src , datalen ); <nl> + if (( int ) num_gd < 0 ) { <nl> + dev_err ( dev -> core_dev -> device , " Invalid number of src SG .\ n "); <nl> + return - EINVAL ; <nl> + } <nl> if ( num_gd == 1 ) <nl> num_gd = 0 ; <nl> 
cont : <nl> PAGE_SET_WRITEBACK | <nl> page_error_op | <nl> PAGE_END_WRITEBACK ); <nl> - btrfs_free_reserved_data_space_noquota ( inode , start , <nl> - end - start + 1 ); <nl> + if ( ret == 0 ) <nl> + btrfs_free_reserved_data_space_noquota ( inode , <nl> + start , <nl> + end - start + 1 ); <nl> goto free_pages_out ; <nl> } <nl> }
static int tsi721_rio_map_inb_mem ( struct rio_mport * mport , dma_addr_t lstart , <nl> } else if ( ibw_start < ( ib_win -> rstart + ib_win -> size ) && <nl> ( ibw_start + ibw_size ) > ib_win -> rstart ) { <nl> /* Return error if address translation involved */ <nl> - if ( direct && ib_win -> xlat ) { <nl> + if (! direct || ib_win -> xlat ) { <nl> ret = - EFAULT ; <nl> break ; <nl> }
acpi_parse_lapic ( struct acpi_subtable_header * header , const unsigned long end ) <nl>  <nl> acpi_table_print_madt_entry ( header ); <nl>  <nl> + /* Ignore invalid ID */ <nl> + if ( processor -> id == 0xff ) <nl> + return 0 ; <nl> + <nl> /* <nl> * We need to register disabled CPU as well to permit <nl> * counting disabled CPUs . This allows us to size
static int et131x_phy_mii_read ( struct et131x_adapter * adapter , u8 addr , <nl> mii_indicator ); <nl>  <nl> status = - EIO ; <nl> + goto out ; <nl> } <nl>  <nl> /* If we hit here we were able to read the register and we need to <nl> static int et131x_phy_mii_read ( struct et131x_adapter * adapter , u8 addr , <nl> */ <nl> * value = readl (& mac -> mii_mgmt_stat ) & ET_MAC_MIIMGMT_STAT_PHYCRTL_MASK ; <nl>  <nl> + out : <nl> /* Stop the read operation */ <nl> writel ( 0 , & mac -> mii_mgmt_cmd ); <nl> 
static int fusbh200_hcd_fusbh200_probe ( struct platform_device * pdev ) <nl>  <nl> retval = fusbh200_setup ( hcd ); <nl> if ( retval ) <nl> - return retval ; <nl> + goto fail_add_hcd ; <nl>  <nl> fusbh200_init ( fusbh200 ); <nl> 
static void intel_hdmi_mode_set ( struct intel_encoder * encoder ) <nl> else <nl> hdmi_val |= SDVO_COLOR_FORMAT_8bpc ; <nl>  <nl> - /* Required on CPT */ <nl> - if ( intel_hdmi -> has_hdmi_sink && HAS_PCH_CPT ( dev )) <nl> + if ( intel_hdmi -> has_hdmi_sink && <nl> + ( HAS_PCH_CPT ( dev ) || IS_VALLEYVIEW ( dev ))) <nl> hdmi_val |= HDMI_MODE_SELECT_HDMI ; <nl>  <nl> if ( intel_hdmi -> has_audio ) {
static int t4_sched_queue_bind ( struct port_info * pi , struct ch_sched_queue * p ) <nl>  <nl> /* Unbind queue from any existing class */ <nl> err = t4_sched_queue_unbind ( pi , p ); <nl> - if ( err ) <nl> + if ( err ) { <nl> + t4_free_mem ( qe ); <nl> goto out ; <nl> + } <nl>  <nl> /* Bind queue to specified class */ <nl> memset ( qe , 0 , sizeof (* qe ));
static struct talitos_crypto_alg * talitos_alg_alloc ( struct device * dev , <nl> break ; <nl> default : <nl> dev_err ( dev , " unknown algorithm type % d \ n ", t_alg -> algt . type ); <nl> + kfree ( t_alg ); <nl> return ERR_PTR (- EINVAL ); <nl> } <nl> 
static int uevent_net_init ( struct net * net ) <nl> if (! ue_sk -> sk ) { <nl> printk ( KERN_ERR <nl> " kobject_uevent : unable to create netlink socket !\ n "); <nl> + kfree ( ue_sk ); <nl> return - ENODEV ; <nl> } <nl> mutex_lock (& uevent_sock_mutex );
static int __devinit snd_gusextreme_probe ( struct device * dev , unsigned int n ) <nl> " detected at 0x % lx \ n ", dev -> bus_id , gus -> gf1 . port ); <nl> goto out ; <nl> } <nl> + gus -> codec_flag = 1 ; <nl>  <nl> error = snd_es1688_pcm ( es1688 , 0 , NULL ); <nl> if ( error < 0 )
static int x2apic_acpi_madt_oem_check ( char * oem_id , char * oem_table_id ) <nl> { <nl> if ( x2apic_phys ) <nl> return x2apic_enabled (); <nl> + else if (( acpi_gbl_FADT . header . revision >= FADT2_REVISION_ID ) && <nl> + ( acpi_gbl_FADT . flags & ACPI_FADT_APIC_PHYSICAL ) && <nl> + x2apic_enabled ()) { <nl> + printk ( KERN_DEBUG " System requires x2apic physical mode \ n "); <nl> + return 1 ; <nl> + } <nl> else <nl> return 0 ; <nl> }
ldebugfs_fid_write_common ( const char __user * buffer , size_t count , <nl> rc = sscanf ( kernbuf , "[% llx - % llx ]\ n ", <nl> ( unsigned long long *)& tmp . lsr_start , <nl> ( unsigned long long *)& tmp . lsr_end ); <nl> + if ( rc != 2 ) <nl> + return - EINVAL ; <nl> if (! range_is_sane (& tmp ) || range_is_zero (& tmp ) || <nl> tmp . lsr_start < range -> lsr_start || tmp . lsr_end > range -> lsr_end ) <nl> return - EINVAL ;
xfs_find_handle ( <nl> int hsize ; <nl> xfs_handle_t handle ; <nl> struct inode * inode ; <nl> - struct fd f ; <nl> + struct fd f = { 0 }; <nl> struct path path ; <nl> int error ; <nl> struct xfs_inode * ip ;
ieee80211_sta_process_chanswitch ( struct ieee80211_sub_if_data * sdata , <nl> case - 1 : <nl> cfg80211_chandef_create (& new_chandef , new_chan , <nl> NL80211_CHAN_NO_HT ); <nl> + /* keep width for 5 / 10 MHz channels */ <nl> + switch ( sdata -> vif . bss_conf . chandef . width ) { <nl> + case NL80211_CHAN_WIDTH_5 : <nl> + case NL80211_CHAN_WIDTH_10 : <nl> + new_chandef . width = sdata -> vif . bss_conf . chandef . width ; <nl> + break ; <nl> + default : <nl> + break ; <nl> + } <nl> break ; <nl> } <nl> 
static int sep_register_driver_with_fs ( struct sep_device * sep ) <nl> if ( ret_val ) { <nl> dev_warn (& sep -> pdev -> dev , " sysfs attribute1 fails for SEP % x \ n ", <nl> ret_val ); <nl> + misc_deregister (& sep -> miscdev_sep ); <nl> return ret_val ; <nl> } <nl> 
static int palmas_i2c_probe ( struct i2c_client * i2c , <nl> ret = - ENOMEM ; <nl> goto err ; <nl> } <nl> + palmas -> i2c_clients [ i ]-> dev . of_node = of_node_get ( node ); <nl> } <nl> palmas -> regmap [ i ] = devm_regmap_init_i2c ( palmas -> i2c_clients [ i ], <nl> & palmas_regmap_config [ i ]);
static bool vgic_update_irq_pending ( struct kvm * kvm , int cpuid , <nl> } else { <nl> vgic_dist_irq_clear_pending ( vcpu , irq_num ); <nl> } <nl> + <nl> + ret = false ; <nl> + goto out ; <nl> } <nl>  <nl> enabled = vgic_irq_is_enabled ( vcpu , irq_num );
static int hdlcdrv_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> case HDLCDRVCTL_CALIBRATE : <nl> if (! capable ( CAP_SYS_RAWIO )) <nl> return - EPERM ; <nl> + if ( bi . data . calibrate > INT_MAX / s -> par . bitrate ) <nl> + return - EINVAL ; <nl> s -> hdlctx . calibrate = bi . data . calibrate * s -> par . bitrate / 16 ; <nl> return 0 ; <nl> 
static int snd_ctl_elem_user_tlv ( struct snd_kcontrol * kcontrol , <nl> ue -> tlv_data = new_data ; <nl> ue -> tlv_data_size = size ; <nl> } else { <nl> + if (! ue -> tlv_data_size || ! ue -> tlv_data ) <nl> + return - ENXIO ; <nl> if ( size < ue -> tlv_data_size ) <nl> return - ENOSPC ; <nl> if ( copy_to_user ( tlv , ue -> tlv_data , ue -> tlv_data_size ))
static int fs_open ( struct atm_vcc * atm_vcc ) <nl> /* Docs are vague about this atm_hdr field . By the way , the FS <nl> * chip makes odd errors if lower bits are set .... -- REW */ <nl> tc -> atm_hdr = ( vpi << 20 ) | ( vci << 4 ); <nl> + tmc0 = 0 ; <nl> { <nl> int pcr = atm_pcr_goal ( txtp ); <nl> 
relookup : <nl> secure_ipv6_id ( daddr -> addr . a6 )); <nl> p -> metrics [ RTAX_LOCK - 1 ] = INETPEER_METRICS_NEW ; <nl> p -> rate_tokens = 0 ; <nl> - p -> rate_last = 0 ; <nl> + /* 60 * HZ is arbitrary , but chosen enough high so that the first <nl> + * calculation of tokens is at its maximum . <nl> + */ <nl> + p -> rate_last = jiffies - 60 * HZ ; <nl> INIT_LIST_HEAD (& p -> gc_list ); <nl>  <nl> /* Link the node . */
static int usb_remote_probe ( struct usb_interface * intf , <nl> devnum = dev -> devnum ; <nl> maxp = usb_maxpacket ( dev , pipe , usb_pipeout ( pipe )); <nl>  <nl> - dprintk ( DRIVER_NAME "[% d ]: bytes_in_key =% lu maxp =% d \ n ", <nl> + dprintk ( DRIVER_NAME "[% d ]: bytes_in_key =% zu maxp =% d \ n ", <nl> devnum , CODE_LENGTH , maxp ); <nl>  <nl> 
static void blade_image_blit ( struct tridentfb_par * par , const char * data , <nl> writemmr ( par , DST1 , point ( x , y )); <nl> writemmr ( par , DST2 , point ( x + w - 1 , y + h - 1 )); <nl>  <nl> - memcpy ( par -> io_virt + 0x10000 , data , 4 * size ); <nl> + iowrite32_rep ( par -> io_virt + 0x10000 , data , size ); <nl> } <nl>  <nl> static void blade_copy_rect ( struct tridentfb_par * par ,
* XXX We need to find a better place for these things ... <nl> */ <nl> bool perf_host = true ; <nl> - bool perf_guest = true ; <nl> + bool perf_guest = false ; <nl>  <nl> void event_attr_init ( struct perf_event_attr * attr ) <nl> {
static bool shadow_walk_okay ( struct kvm_shadow_walk_iterator * iterator ) <nl> { <nl> if ( iterator -> level < PT_PAGE_TABLE_LEVEL ) <nl> return false ; <nl> + <nl> + if ( iterator -> level == PT_PAGE_TABLE_LEVEL ) <nl> + if ( is_large_pte (* iterator -> sptep )) <nl> + return false ; <nl> + <nl> iterator -> index = SHADOW_PT_INDEX ( iterator -> addr , iterator -> level ); <nl> iterator -> sptep = (( u64 *) __va ( iterator -> shadow_addr )) + iterator -> index ; <nl> return true ;
static void squashfs_put_super ( struct super_block * sb ) <nl> kfree ( sbi -> id_table ); <nl> kfree ( sbi -> fragment_index ); <nl> kfree ( sbi -> meta_index ); <nl> + kfree ( sbi -> inode_lookup_table ); <nl> kfree ( sb -> s_fs_info ); <nl> sb -> s_fs_info = NULL ; <nl> }
static int t7l66xb_probe ( struct platform_device * dev ) <nl> struct resource * iomem , * rscr ; <nl> int ret ; <nl>  <nl> + if ( pdata == NULL ) <nl> + return - EINVAL ; <nl> + <nl> iomem = platform_get_resource ( dev , IORESOURCE_MEM , 0 ); <nl> if (! iomem ) <nl> return - EINVAL ;
static void omap_dma_free_chan_resources ( struct dma_chan * chan ) <nl> vchan_free_chan_resources (& c -> vc ); <nl> omap_free_dma ( c -> dma_ch ); <nl>  <nl> - dev_dbg ( od -> ddev . dev , " freeing channel for % u \ n ", c -> dma_sig ); <nl> + dev_dbg ( od -> ddev . dev , " freeing channel % u used for % u \ n ", c -> dma_ch , <nl> + c -> dma_sig ); <nl> c -> dma_sig = 0 ; <nl> } <nl> 
static void v9fs_fd_close ( struct v9fs_transport * trans ) <nl> if (! trans ) <nl> return ; <nl>  <nl> - trans -> status = Disconnected ; <nl> - ts = trans -> priv ; <nl> + ts = xchg (& trans -> priv , NULL ); <nl>  <nl> if (! ts ) <nl> return ; <nl>  <nl> + trans -> status = Disconnected ; <nl> if ( ts -> in_file ) <nl> fput ( ts -> in_file ); <nl> 
static bool is_zone_first_populated ( pg_data_t * pgdat , struct zone * zone ) <nl> return zone == compare ; <nl> } <nl>  <nl> - /* The zone must be somewhere ! */ <nl> - WARN_ON_ONCE ( 1 ); <nl> return false ; <nl> } <nl> 
_scsih_error_recovery_delete_devices ( struct MPT2SAS_ADAPTER * ioc ) <nl>  <nl> if ( ioc -> is_driver_loading ) <nl> return ; <nl> + <nl> + fw_event = kzalloc ( sizeof ( struct fw_event_work ), GFP_ATOMIC ); <nl> + if (! fw_event ) <nl> + return ; <nl> + <nl> fw_event -> event = MPT2SAS_REMOVE_UNRESPONDING_DEVICES ; <nl> fw_event -> ioc = ioc ; <nl> _scsih_fw_event_add ( ioc , fw_event );
int module_finalize ( const Elf32_Ehdr * hdr , const Elf_Shdr * sechdrs , <nl> # endif <nl> s = find_mod_section ( hdr , sechdrs , ". alt . smp . init "); <nl> if ( s && ! is_smp ()) <nl> +# ifdef CONFIG_SMP_ON_UP <nl> fixup_smp (( void *) s -> sh_addr , s -> sh_size ); <nl> +# else <nl> + return - EINVAL ; <nl> +# endif <nl> return 0 ; <nl> } <nl> 
static void vmw_fb_dirty_flush ( struct vmw_fb_par * par ) <nl> SVGAFifoCmdUpdate body ; <nl> } * cmd ; <nl>  <nl> + if ( vmw_priv -> suspended ) <nl> + return ; <nl> + <nl> spin_lock_irqsave (& par -> dirty . lock , flags ); <nl> if (! par -> dirty . active ) { <nl> spin_unlock_irqrestore (& par -> dirty . lock , flags );
static const struct pinmux_cfg_reg pinmux_config_regs [] = { <nl> FN_MSIOF0_SCK_B , 0 , <nl> /* IP5_23_21 [ 3 ] */ <nl> FN_WE1_N , FN_IERX , FN_CAN1_RX , FN_VI1_G4 , <nl> - FN_VI1_G4_B , FN_VI2_R6 , FN_SCIFA0_CTS_N_B , <nl> - FN_IERX_C , 0 , <nl> + FN_VI1_G4_B , FN_VI2_R6 , FN_SCIFA0_CTS_N_B , FN_IERX_C , <nl> /* IP5_20_18 [ 3 ] */ <nl> FN_WE0_N , FN_IECLK , FN_CAN_CLK , <nl> FN_VI2_VSYNC_N , FN_SCIFA0_TXD_B , FN_VI2_VSYNC_N_B , 0 , 0 ,
# define IRQ_PXA168_DDR_INT 26 <nl> # define IRQ_PXA168_UART1 27 <nl> # define IRQ_PXA168_UART2 28 <nl> +# define IRQ_PXA168_UART3 29 <nl> # define IRQ_PXA168_WDT 35 <nl> +# define IRQ_PXA168_MAIN_PMU 36 <nl> # define IRQ_PXA168_FRQ_CHANGE 38 <nl> # define IRQ_PXA168_SDH1 39 <nl> # define IRQ_PXA168_SDH2 40 <nl> # define IRQ_PXA168_USB2 51 <nl> # define IRQ_PXA168_AC97 57 <nl> # define IRQ_PXA168_TWSI1 58 <nl> -# define IRQ_PXA168_PMU 60 <nl> +# define IRQ_PXA168_AP_PMU 60 <nl> # define IRQ_PXA168_SM_INT 63 <nl>  <nl> /*
int mmap_min_addr_handler ( struct ctl_table * table , int write , <nl> { <nl> int ret ; <nl>  <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> + <nl> ret = proc_doulongvec_minmax ( table , write , buffer , lenp , ppos ); <nl>  <nl> update_mmap_min_addr ();
static int dmx_section_feed_release_filter ( struct dmx_section_feed * feed , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( feed -> is_filtering ) <nl> + if ( feed -> is_filtering ) { <nl> + /* release dvbdmx -> mutex as far as <nl> + it is acquired by stop_filtering () itself */ <nl> + mutex_unlock (& dvbdmx -> mutex ); <nl> feed -> stop_filtering ( feed ); <nl> + mutex_lock (& dvbdmx -> mutex ); <nl> + } <nl>  <nl> spin_lock_irq (& dvbdmx -> lock ); <nl> f = dvbdmxfeed -> filter ;
static void ext4_free_data ( handle_t * handle , struct inode * inode , <nl> * block pointed to itself , it would have been detached when <nl> * the block was cleared . Check for this instead of OOPSing . <nl> */ <nl> - if ( bh2jh ( this_bh )) <nl> + if (( EXT4_JOURNAL ( inode ) == NULL ) || bh2jh ( this_bh )) <nl> ext4_handle_dirty_metadata ( handle , inode , this_bh ); <nl> else <nl> ext4_error ( inode -> i_sb , __func__ ,
static void ironlake_enable_pch_transcoder ( struct drm_i915_private * dev_priv , <nl> val |= TRANS_PROGRESSIVE ; <nl>  <nl> I915_WRITE ( reg , val | TRANS_ENABLE ); <nl> - if ( wait_for ( I915_READ ( reg ) & TRANS_STATE_ENABLE , 100 )) <nl> + if ( intel_wait_for_register ( dev_priv , <nl> + reg , TRANS_STATE_ENABLE , TRANS_STATE_ENABLE , <nl> + 100 )) <nl> DRM_ERROR (" failed to enable transcoder % c \ n ", pipe_name ( pipe )); <nl> } <nl> 
static struct inotify_watch * create_watch ( struct inotify_device * dev , <nl> return ERR_PTR ( ret ); <nl> } <nl>  <nl> - dev -> last_wd = ret ; <nl> + dev -> last_wd = watch -> wd ; <nl> watch -> mask = mask ; <nl> atomic_set (& watch -> count , 0 ); <nl> INIT_LIST_HEAD (& watch -> d_list );
static void alc283_shutup ( struct hda_codec * codec ) <nl>  <nl> alc_write_coef_idx ( codec , 0x43 , 0x9004 ); <nl>  <nl> + /* depop hp during suspend */ <nl> + alc_write_coef_idx ( codec , 0x06 , 0x2100 ); <nl> + <nl> snd_hda_codec_write ( codec , hp_pin , 0 , <nl> AC_VERB_SET_AMP_GAIN_MUTE , AMP_OUT_MUTE ); <nl> 
static struct iwl_power_vec_entry range_2 [ IWL_POWER_MAX ] = { <nl> /* set card power command */ <nl> static int iwl_set_power ( struct iwl_priv * priv , void * cmd ) <nl> { <nl> - return iwl_send_cmd_pdu_async ( priv , POWER_TABLE_CMD , <nl> - sizeof ( struct iwl_powertable_cmd ), <nl> - cmd , NULL ); <nl> + return iwl_send_cmd_pdu ( priv , POWER_TABLE_CMD , <nl> + sizeof ( struct iwl_powertable_cmd ), cmd ); <nl> } <nl> /* decide the right power level according to association status <nl> * and battery status
static int ata_bus_probe ( struct ata_port * ap ) <nl>  <nl> /* reset */ <nl> if ( ap -> ops -> probe_reset ) { <nl> + for ( i = 0 ; i < ATA_MAX_DEVICES ; i ++) <nl> + classes [ i ] = ATA_DEV_UNKNOWN ; <nl> + <nl> rc = ap -> ops -> probe_reset ( ap , classes ); <nl> if ( rc ) { <nl> printk (" ata % u : reset failed ( errno =% d )\ n ", ap -> id , rc );
int rsnd_dai_connect ( struct rsnd_mod * mod , <nl> if (! mod ) <nl> return - EIO ; <nl>  <nl> + if ( io -> mod [ type ]) <nl> + return - EINVAL ; <nl> + <nl> priv = rsnd_mod_to_priv ( mod ); <nl> dev = rsnd_priv_to_dev ( priv ); <nl> 
static struct async * alloc_async ( unsigned int numisoframes ) <nl> static void free_async ( struct async * as ) <nl> { <nl> put_pid ( as -> pid ); <nl> - put_cred ( as -> cred ); <nl> + if ( as -> cred ) <nl> + put_cred ( as -> cred ); <nl> kfree ( as -> urb -> transfer_buffer ); <nl> kfree ( as -> urb -> setup_packet ); <nl> usb_free_urb ( as -> urb );
static u64 tg_prfill_cpu_rwstat ( struct seq_file * sf , <nl> struct blkg_rwstat rwstat = { }, tmp ; <nl> int i , cpu ; <nl>  <nl> + if ( tg -> stats_cpu == NULL ) <nl> + return 0 ; <nl> + <nl> for_each_possible_cpu ( cpu ) { <nl> struct tg_stats_cpu * sc = per_cpu_ptr ( tg -> stats_cpu , cpu ); <nl> 
int btrfs_reserve_extent ( struct btrfs_root * root , <nl> u64 empty_size , u64 hint_byte , <nl> struct btrfs_key * ins , int is_data , int delalloc ) <nl> { <nl> - bool final_tried = false ; <nl> + bool final_tried = num_bytes == min_alloc_size ; <nl> u64 flags ; <nl> int ret ; <nl> 
unmap_intr_base : <nl> iounmap ( priv -> avs_intr_base ); <nl> unmap_base : <nl> iounmap ( priv -> base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return ret ; <nl> } <nl> static int brcm_avs_cpufreq_remove ( struct platform_device * pdev ) <nl> priv = platform_get_drvdata ( pdev ); <nl> iounmap ( priv -> base ); <nl> iounmap ( priv -> avs_intr_base ); <nl> - platform_set_drvdata ( pdev , NULL ); <nl>  <nl> return 0 ; <nl> }
static void rt6_free_pcpu ( struct rt6_info * non_pcpu_rt ) <nl> } <nl> } <nl>  <nl> + free_percpu ( non_pcpu_rt -> rt6i_pcpu ); <nl> non_pcpu_rt -> rt6i_pcpu = NULL ; <nl> } <nl> 
static int __devinit fealnx_init_one ( struct pci_dev * pdev , <nl> if ( np -> flags == HAS_MII_XCVR ) { <nl> int phy , phy_idx = 0 ; <nl>  <nl> - for ( phy = 1 ; phy < 32 && phy_idx < 4 ; phy ++) { <nl> + for ( phy = 1 ; phy < 32 && phy_idx < ARRAY_SIZE ( np -> phys ); <nl> + phy ++) { <nl> int mii_status = mdio_read ( dev , phy , 1 ); <nl>  <nl> if ( mii_status != 0xffff && mii_status != 0x0000 ) {
int mac802154_llsec_key_del ( struct mac802154_llsec * sec , <nl> mkey = container_of ( pos -> key , struct mac802154_llsec_key , key ); <nl>  <nl> if ( llsec_key_id_equal (& pos -> id , key )) { <nl> + list_del_rcu (& pos -> list ); <nl> llsec_key_put ( mkey ); <nl> return 0 ; <nl> }
static int __devexit powernowk8_cpu_exit ( struct cpufreq_policy * pol ) <nl>  <nl> static unsigned int powernowk8_get ( unsigned int cpu ) <nl> { <nl> - struct powernow_k8_data * data ; <nl> + struct powernow_k8_data * data = per_cpu ( powernow_data , cpu ); <nl> cpumask_t oldmask = current -> cpus_allowed ; <nl> unsigned int khz = 0 ; <nl> - unsigned int first ; <nl> - <nl> - first = cpumask_first ( cpu_core_mask ( cpu )); <nl> - data = per_cpu ( powernow_data , first ); <nl>  <nl> if (! data ) <nl> return - EINVAL ;
static int TSS_authhmac ( unsigned char * digest , const unsigned char * key , <nl> if ( dlen == 0 ) <nl> break ; <nl> data = va_arg ( argp , unsigned char *); <nl> + if (! data ) { <nl> + ret = - EINVAL ; <nl> + va_end ( argp ); <nl> + goto out ; <nl> + } <nl> ret = crypto_shash_update (& sdesc -> shash , data , dlen ); <nl> if ( ret < 0 ) { <nl> va_end ( argp );
int free_stream_context ( struct intel_sst_drv * ctx , unsigned int str_id ) <nl> if ( ret ) <nl> sst_clean_stream (& ctx -> streams [ str_id ]); <nl> return ret ; <nl> + } else { <nl> + dev_err ( ctx -> dev , " we tried to free stream context % d which was freed !!!\ n ", str_id ); <nl> } <nl> return ret ; <nl> }
static int __inode_security_revalidate ( struct inode * inode , <nl>  <nl> might_sleep_if ( may_sleep ); <nl>  <nl> - if ( isec -> initialized != LABEL_INITIALIZED ) { <nl> + if ( ss_initialized && isec -> initialized != LABEL_INITIALIZED ) { <nl> if (! may_sleep ) <nl> return - ECHILD ; <nl> 
irq_handler ( int irq , void * device ) <nl>  <nl> pci_int_status = reg_read ( lynx , PCI_INT_STATUS ); <nl>  <nl> + if ( pci_int_status == ~ 0 ) <nl> + /* Card was ejected . */ <nl> + return IRQ_NONE ; <nl> + <nl> if (( pci_int_status & PCI_INT_INT_PEND ) == 0 ) <nl> /* Not our interrupt , bail out quickly . */ <nl> return IRQ_NONE ;
struct bcm2835_audio_instance { <nl> short peer_version ; <nl> }; <nl>  <nl> - bool force_bulk = false ; <nl> + static bool force_bulk ; <nl>  <nl> /* ---- Private Variables ---------------------------------------------------- */ <nl> 
struct sk_buff * __skb_recv_datagram ( struct sock * sk , unsigned int flags , <nl> skb_queue_walk ( queue , skb ) { <nl> * peeked = skb -> peeked ; <nl> if ( flags & MSG_PEEK ) { <nl> - if (* off >= skb -> len && skb -> len ) { <nl> + if (* off >= skb -> len && ( skb -> len || * off || <nl> + skb -> peeked )) { <nl> * off -= skb -> len ; <nl> continue ; <nl> }
static void gb_tty_set_termios ( struct tty_struct * tty , <nl>  <nl> if ( C_BAUD ( tty ) == B0 ) { <nl> newline . rate = gb_tty -> line_coding . rate ; <nl> - newctrl &= GB_UART_CTRL_DTR ; <nl> + newctrl &= ~ GB_UART_CTRL_DTR ; <nl> } else if ( termios_old && ( termios_old -> c_cflag & CBAUD ) == B0 ) { <nl> newctrl |= GB_UART_CTRL_DTR ; <nl> }
int mlx5_core_access_reg ( struct mlx5_core_dev * dev , void * data_in , <nl> in -> arg = cpu_to_be32 ( arg ); <nl> in -> register_id = cpu_to_be16 ( reg_num ); <nl> err = mlx5_cmd_exec ( dev , in , sizeof (* in ) + size_in , out , <nl> - sizeof ( out ) + size_out ); <nl> + sizeof (* out ) + size_out ); <nl> if ( err ) <nl> goto ex2 ; <nl> 
do_kern_mount ( const char * fstype , int flags , const char * name , void * data ) <nl> mnt -> mnt_parent = mnt ; <nl> mnt -> mnt_namespace = current -> namespace ; <nl> up_write (& sb -> s_umount ); <nl> + free_secdata ( secdata ); <nl> put_filesystem ( type ); <nl> return mnt ; <nl> out_sb :
static bool nested_vmx_exit_handled_msr ( struct kvm_vcpu * vcpu , <nl> u32 msr_index = vcpu -> arch . regs [ VCPU_REGS_RCX ]; <nl> gpa_t bitmap ; <nl>  <nl> - if (! nested_cpu_has ( get_vmcs12 ( vcpu ), CPU_BASED_USE_MSR_BITMAPS )) <nl> + if (! nested_cpu_has ( vmcs12 , CPU_BASED_USE_MSR_BITMAPS )) <nl> return 1 ; <nl>  <nl> /*
radeon_user_framebuffer_create ( struct drm_device * dev , <nl> return ERR_PTR (- ENOENT ); <nl> } <nl>  <nl> + /* Handle is imported dma - buf , so cannot be migrated to VRAM for scanout */ <nl> + if ( obj -> import_attach ) { <nl> + DRM_DEBUG_KMS (" Cannot create framebuffer from imported dma_buf \ n "); <nl> + return ERR_PTR (- EINVAL ); <nl> + } <nl> + <nl> radeon_fb = kzalloc ( sizeof (* radeon_fb ), GFP_KERNEL ); <nl> if ( radeon_fb == NULL ) { <nl> drm_gem_object_unreference_unlocked ( obj );
int ip6_mr_input ( struct sk_buff * skb ) <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> cache = ip6mr_cache_find ( mrt ,
imsttfb_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> default : <nl> printk ( KERN_INFO " imsttfb : Device 0x % x unknown , " <nl> " contact maintainer .\ n ", pdev -> device ); <nl> + release_mem_region ( addr , size ); <nl> + framebuffer_release ( info ); <nl> return - ENODEV ; <nl> } <nl> 
static void ui_browser__hists_seek ( struct ui_browser * browser , <nl> * and stop when we printed enough lines to fill the screen . <nl> */ <nl> do_offset : <nl> + if (! nd ) <nl> + return ; <nl> + <nl> if ( offset > 0 ) { <nl> do { <nl> h = rb_entry ( nd , struct hist_entry , rb_node );
__cmpxchg_u32 ( volatile unsigned int * p , unsigned int old , unsigned int new ) <nl> " bra 2f ; \ n " <nl> " . fillinsn \ n " <nl> " 1 :" <nl> - M32R_UNLOCK " % 2 , @% 1 ; \ n " <nl> + M32R_UNLOCK " % 0 , @% 1 ; \ n " <nl> " . fillinsn \ n " <nl> " 2 :" <nl> : "=& r " ( retval )
void __init smp_prepare_boot_cpu ( void ) <nl>  <nl> static void send_ipi_message ( const struct cpumask * mask , enum ipi_msg_type msg ) <nl> { <nl> - unsigned long flags ; <nl> - <nl> - local_irq_save ( flags ); <nl> - <nl> /* <nl> * Call the platform specific cross - CPU call function . <nl> */ <nl> smp_cross_call ( mask , msg ); <nl> - <nl> - local_irq_restore ( flags ); <nl> } <nl>  <nl> void arch_send_call_function_ipi_mask ( const struct cpumask * mask )
static int si_dpm_init_microcode ( struct amdgpu_device * adev ) <nl> ( adev -> pdev -> revision == 0x80 ) || <nl> ( adev -> pdev -> revision == 0x81 ) || <nl> ( adev -> pdev -> revision == 0x83 ) || <nl> + ( adev -> pdev -> revision == 0x87 ) || <nl> ( adev -> pdev -> device == 0x6604 ) || <nl> ( adev -> pdev -> device == 0x6605 )) <nl> chip_name = " oland_k ";
s32 igb_get_phy_id ( struct e1000_hw * hw ) <nl> u16 phy_id ; <nl>  <nl> /* ensure PHY page selection to fix misconfigured i210 */ <nl> - if ( hw -> mac . type == e1000_i210 ) <nl> + if (( hw -> mac . type == e1000_i210 ) || ( hw -> mac . type == e1000_i211 )) <nl> phy -> ops . write_reg ( hw , I347AT4_PAGE_SELECT , 0 ); <nl>  <nl> ret_val = phy -> ops . read_reg ( hw , PHY_ID1 , & phy_id );
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> if (! dev -> suspend_in_progress ) <nl> b43_rng_init ( wl ); <nl>  <nl> + ieee80211_wake_queues ( dev -> wl -> hw ); <nl> + <nl> b43_set_status ( dev , B43_STAT_INITIALIZED ); <nl>  <nl> if (! dev -> suspend_in_progress )
int fcoe_ctlr_els_send ( struct fcoe_ctlr * fip , struct fc_lport * lport , <nl>  <nl> if ( fip -> state == FIP_ST_NON_FIP ) <nl> return 0 ; <nl> + if (! fip -> sel_fcf ) <nl> + goto drop ; <nl>  <nl> switch ( op ) { <nl> case ELS_FLOGI :
static int i82875p_setup_overfl_dev ( struct pci_dev * pdev , <nl> "% s (): pci_bus_add_device () Failed \ n ", <nl> __func__ ); <nl> } <nl> + pci_bus_assign_resources ( dev -> bus ); <nl> } <nl>  <nl> * ovrfl_pdev = dev ;
static void sppp_lcp_input ( struct sppp * sp , struct sk_buff * skb ) <nl> struct net_device * dev = sp -> pp_if ; <nl> int len = skb -> len ; <nl> u8 * p , opt [ 6 ]; <nl> - u32 rmagic ; <nl> + u32 rmagic = 0 ; <nl>  <nl> if (! pskb_may_pull ( skb , sizeof ( struct lcp_header ))) { <nl> if ( sp -> pp_flags & PP_DEBUG )
static void e1000_set_rx_mode ( struct net_device * netdev ) <nl> e1000_rar_set ( hw , ha -> addr , i ++); <nl> } <nl>  <nl> - WARN_ON ( i == rar_entries ); <nl> - <nl> netdev_for_each_mc_addr ( ha , netdev ) { <nl> if ( i == rar_entries ) { <nl> /* load any remaining addresses into the hash table */
struct usb_sevsegdev { <nl> * if str commands are used , we would assume the end of string <nl> * so mem commands are used . <nl> */ <nl> - inline size_t my_memlen ( const char * buf , size_t count ) <nl> + static inline size_t my_memlen ( const char * buf , size_t count ) <nl> { <nl> if ( count > 0 && buf [ count - 1 ] == '\ n ') <nl> return count - 1 ;
void usb_buffer_unmap_sg ( struct usb_device * dev , unsigned pipe , <nl>  <nl> static int verify_suspended ( struct device * dev , void * unused ) <nl> { <nl> + if ( dev -> driver == NULL ) <nl> + return 0 ; <nl> return ( dev -> power . power_state . event == PM_EVENT_ON ) ? - EBUSY : 0 ; <nl> } <nl> 
deinit : <nl> wl1271_free_ap_keys ( wl , wlvif ); <nl> } <nl>  <nl> + dev_kfree_skb ( wlvif -> probereq ); <nl> + wlvif -> probereq = NULL ; <nl> wl12xx_tx_reset_wlvif ( wl , wlvif ); <nl> if ( wl -> last_wlvif == wlvif ) <nl> wl -> last_wlvif = NULL ;
static int mmpcam_probe ( struct platform_device * pdev ) <nl>  <nl> out_unregister : <nl> mccic_shutdown ( mcam ); <nl> - mmpcam_power_down ( mcam ); <nl> out_gpio2 : <nl> + mmpcam_power_down ( mcam ); <nl> gpio_free ( pdata -> sensor_reset_gpio ); <nl> out_gpio : <nl> gpio_free ( pdata -> sensor_power_gpio );
static int pwm_setup_backlight ( struct intel_connector * connector , <nl> return - ENODEV ; <nl> } <nl>  <nl> + /* <nl> + * FIXME : pwm_apply_args () should be removed when switching to <nl> + * the atomic PWM API . <nl> + */ <nl> + pwm_apply_args ( panel -> backlight . pwm ); <nl> + <nl> retval = pwm_config ( panel -> backlight . pwm , CRC_PMIC_PWM_PERIOD_NS , <nl> CRC_PMIC_PWM_PERIOD_NS ); <nl> if ( retval < 0 ) {
static void ceph_x_destroy ( struct ceph_auth_client * ac ) <nl> remove_ticket_handler ( ac , th ); <nl> } <nl>  <nl> + if ( xi -> auth_authorizer . buf ) <nl> + ceph_buffer_put ( xi -> auth_authorizer . buf ); <nl> + <nl> kfree ( ac -> private ); <nl> ac -> private = NULL ; <nl> }
static void sh_msiof_spi_chipselect ( struct spi_device * spi , int is_on ) <nl> } <nl>  <nl> /* use spi -> controller data for CS ( same strategy as spi_gpio ) */ <nl> - gpio_set_value (( unsigned ) spi -> controller_data , value ); <nl> + gpio_set_value (( uintptr_t ) spi -> controller_data , value ); <nl>  <nl> if ( is_on == BITBANG_CS_INACTIVE ) { <nl> if ( test_and_clear_bit ( 0 , & p -> flags )) {
static int shmem_unuse_inode ( struct shmem_inode_info * info , swp_entry_t entry , s <nl> if ( size > ENTRIES_PER_PAGE ) <nl> size = ENTRIES_PER_PAGE ; <nl> offset = shmem_find_swp ( entry , ptr , ptr + size ); <nl> + shmem_swp_unmap ( ptr ); <nl> if ( offset >= 0 ) { <nl> shmem_dir_unmap ( dir ); <nl> + ptr = shmem_swp_map ( subdir ); <nl> goto found ; <nl> } <nl> - shmem_swp_unmap ( ptr ); <nl> } <nl> } <nl> lost1 :
static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
extern int blk_iopoll_enabled ; <nl> static int sixty = 60 ; <nl> # endif <nl>  <nl> + static int neg_one = - 1 ; <nl> static int zero ; <nl> static int __maybe_unused one = 1 ; <nl> static int __maybe_unused two = 2 ;
static int ath9k_add_interface ( struct ieee80211_hw * hw , <nl> } <nl> } <nl>  <nl> - if (( vif -> type == NL80211_IFTYPE_ADHOC ) && <nl> - sc -> nvifs > 0 ) { <nl> + if (( ah -> opmode == NL80211_IFTYPE_ADHOC ) || <nl> + (( vif -> type == NL80211_IFTYPE_ADHOC ) && <nl> + sc -> nvifs > 0 )) { <nl> ath_err ( common , " Cannot create ADHOC interface when other " <nl> " interfaces already exist .\ n "); <nl> ret = - EINVAL ;
unsigned int create_irq_nr ( unsigned int irq_want , int node ) <nl> continue ; <nl>  <nl> desc_new = move_irq_desc ( desc_new , node ); <nl> + cfg_new = desc_new -> chip_data ; <nl>  <nl> if ( __assign_irq_vector ( new , cfg_new , apic -> target_cpus ()) == 0 ) <nl> irq = new ;
static __be32 encode_cb_sequence_res ( struct svc_rqst * rqstp , <nl> if ( unlikely ( status != 0 )) <nl> goto out ; <nl>  <nl> - encode_sessionid ( xdr , & res -> csr_sessionid ); <nl> + status = encode_sessionid ( xdr , & res -> csr_sessionid ); <nl> + if ( status ) <nl> + goto out ; <nl>  <nl> p = xdr_reserve_space ( xdr , 4 * sizeof ( uint32_t )); <nl> if ( unlikely ( p == NULL ))
static int ltr501_write_event_config ( struct iio_dev * indio_dev , <nl> int ret ; <nl>  <nl> /* only 1 and 0 are valid inputs */ <nl> - if ( state != 1 || state != 0 ) <nl> + if ( state != 1 && state != 0 ) <nl> return - EINVAL ; <nl>  <nl> switch ( chan -> type ) {
int call_usermodehelper_exec ( struct subprocess_info * sub_info , int wait ) <nl> DECLARE_COMPLETION_ONSTACK ( done ); <nl> int retval = 0 ; <nl>  <nl> + if (! sub_info -> path ) { <nl> + call_usermodehelper_freeinfo ( sub_info ); <nl> + return - EINVAL ; <nl> + } <nl> helper_lock (); <nl> if (! khelper_wq || usermodehelper_disabled ) { <nl> retval = - EBUSY ;
static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> } <nl> logical = re -> logical ; <nl>  <nl> - reada_extent_put ( fs_info , re ); <nl> - <nl> atomic_inc (& dev -> reada_in_flight ); <nl> ret = reada_tree_block_flagged ( fs_info -> extent_root , logical , <nl> mirror_num , & eb ); <nl> static int reada_start_machine_dev ( struct btrfs_fs_info * fs_info , <nl> if ( eb ) <nl> free_extent_buffer ( eb ); <nl>  <nl> + reada_extent_put ( fs_info , re ); <nl> + <nl> return 1 ; <nl>  <nl> }
static int start_ap ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> priv = wiphy_priv ( wiphy ); <nl> vif = netdev_priv ( dev ); <nl> - wl = vif -> wilc ; <nl> + wl = vif -> wilc ; <nl> PRINT_D ( HOSTAPD_DBG , " Starting ap \ n "); <nl>  <nl> PRINT_D ( HOSTAPD_DBG , " Interval = % d \ n DTIM period = % d \ n Head length = % zu Tail length = % zu \ n ",
static int spi_imx_setupxfer ( struct spi_device * spi , <nl> config . bpw = t ? t -> bits_per_word : spi -> bits_per_word ; <nl> config . speed_hz = t ? t -> speed_hz : spi -> max_speed_hz ; <nl> config . mode = spi -> mode ; <nl> + config . cs = spi_imx -> chipselect [ spi -> chip_select ]; <nl>  <nl> if (! config . speed_hz ) <nl> config . speed_hz = spi -> max_speed_hz ;
static enum ucode_state request_microcode_fw ( int cpu , struct device * device ) <nl> return UCODE_NFOUND ; <nl> } <nl>  <nl> + if (*( u32 *) firmware -> data != UCODE_MAGIC ) { <nl> + printk ( KERN_ERR " microcode : invalid UCODE_MAGIC ( 0x % 08x )\ n ", <nl> + *( u32 *) firmware -> data ); <nl> + return UCODE_ERROR ; <nl> + } <nl> + <nl> ret = generic_load_microcode ( cpu , firmware -> data , firmware -> size ); <nl>  <nl> release_firmware ( firmware );
static irqreturn_t sdhci_irq ( int irq , void * dev_id ) <nl>  <nl> intmask = readl ( host -> ioaddr + SDHCI_INT_STATUS ); <nl>  <nl> - if (! intmask ) { <nl> + if (! intmask || intmask == 0xffffffff ) { <nl> result = IRQ_NONE ; <nl> goto out ; <nl> }
static int __devinit mei_probe ( struct pci_dev * pdev , <nl> err = request_threaded_irq ( pdev -> irq , <nl> NULL , <nl> mei_interrupt_thread_handler , <nl> - 0 , mei_driver_name , dev ); <nl> + IRQF_ONESHOT , mei_driver_name , dev ); <nl> else <nl> err = request_threaded_irq ( pdev -> irq , <nl> mei_interrupt_quick_handler ,
int c4iw_register_device ( struct c4iw_dev * dev ) <nl> dev -> ibdev . iwcm -> add_ref = c4iw_qp_add_ref ; <nl> dev -> ibdev . iwcm -> rem_ref = c4iw_qp_rem_ref ; <nl> dev -> ibdev . iwcm -> get_qp = c4iw_get_qp ; <nl> + memcpy ( dev -> ibdev . iwcm -> ifname , dev -> rdev . lldi . ports [ 0 ]-> name , <nl> + sizeof ( dev -> ibdev . iwcm -> ifname )); <nl>  <nl> ret = ib_register_device (& dev -> ibdev , NULL ); <nl> if ( ret )
static void get_total_mem ( struct mv64x60_mc_pdata * pdata ) <nl> if (! np ) <nl> return ; <nl>  <nl> - reg = get_property ( np , " reg ", NULL ); <nl> + reg = of_get_property ( np , " reg ", NULL ); <nl>  <nl> pdata -> total_mem = reg [ 1 ]; <nl> }
static void i40iw_cm_disconn_true ( struct i40iw_qp * iwqp ) <nl> /* Flush the queues */ <nl> i40iw_flush_wqes ( iwdev , iwqp ); <nl>  <nl> - if ( qp -> term_flags ) { <nl> + if ( qp -> term_flags && iwqp -> ibqp . event_handler ) { <nl> ibevent . device = iwqp -> ibqp . device ; <nl> ibevent . event = ( qp -> eventtype == TERM_EVENT_QP_FATAL ) ? <nl> IB_EVENT_QP_FATAL : IB_EVENT_QP_ACCESS_ERR ;
static int threshold_create_bank ( unsigned int cpu , unsigned int bank ) <nl> const char * name = get_name ( bank , NULL ); <nl> int err = 0 ; <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( is_shared_bank ( bank )) { <nl> nb = node_to_amd_nb ( amd_get_nb_id ( cpu )); <nl> 
int dwc3_send_gadget_generic_command ( struct dwc3 * dwc , unsigned cmd , u32 param ) <nl> dwc3_trace ( trace_dwc3_gadget , <nl> " Command Complete --> % d ", <nl> DWC3_DGCMD_STATUS ( reg )); <nl> + if ( DWC3_DGCMD_STATUS ( reg )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
void bpf_jit_compile ( struct bpf_prog * fp ) <nl>  <nl> memset (& ctx , 0 , sizeof ( ctx )); <nl>  <nl> - ctx . offsets = kcalloc ( fp -> len , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> + ctx . offsets = kcalloc ( fp -> len + 1 , sizeof (* ctx . offsets ), GFP_KERNEL ); <nl> if ( ctx . offsets == NULL ) <nl> return ; <nl> 
static void igb_reuse_rx_page ( struct igb_ring * rx_ring , <nl> rx_ring -> next_to_alloc = ( nta < rx_ring -> count ) ? nta : 0 ; <nl>  <nl> /* transfer page from old buffer to new buffer */ <nl> - memcpy ( new_buff , old_buff , sizeof ( struct igb_rx_buffer )); <nl> + * new_buff = * old_buff ; <nl>  <nl> /* sync the buffer for use by the device */ <nl> dma_sync_single_range_for_device ( rx_ring -> dev , old_buff -> dma ,
int macvlan_common_newlink ( struct net * src_net , struct net_device * dev , <nl>  <nl> list_add_tail_rcu (& vlan -> list , & port -> vlans ); <nl> netif_stacked_transfer_operstate ( lowerdev , dev ); <nl> + linkwatch_fire_event ( dev ); <nl>  <nl> return 0 ; <nl>  <nl> static int macvlan_device_event ( struct notifier_block * unused , <nl> port = macvlan_port_get_rtnl ( dev ); <nl>  <nl> switch ( event ) { <nl> + case NETDEV_UP : <nl> case NETDEV_CHANGE : <nl> list_for_each_entry ( vlan , & port -> vlans , list ) <nl> netif_stacked_transfer_operstate ( vlan -> lowerdev ,
static struct platform_driver i2c_mux_reg_driver = { <nl> . remove = i2c_mux_reg_remove , <nl> . driver = { <nl> . name = " i2c - mux - reg ", <nl> + . of_match_table = of_match_ptr ( i2c_mux_reg_of_match ), <nl> }, <nl> }; <nl> 
static int __init b44_init ( void ) <nl>  <nl> /* Setup paramaters for syncing RX / TX DMA descriptors */ <nl> dma_desc_align_mask = ~( dma_desc_align_size - 1 ); <nl> - dma_desc_sync_size = max ( dma_desc_align_size , sizeof ( struct dma_desc )); <nl> + dma_desc_sync_size = max_t ( unsigned int , dma_desc_align_size , sizeof ( struct dma_desc )); <nl>  <nl> return pci_module_init (& b44_driver ); <nl> }
static int p54u_probe ( struct usb_interface * intf , <nl> priv -> upload_fw = p54u_upload_firmware_net2280 ; <nl> } <nl> err = p54u_load_firmware ( dev , intf ); <nl> + if ( err ) { <nl> + usb_put_dev ( udev ); <nl> + p54_free_common ( dev ); <nl> + } <nl> return err ; <nl> } <nl> 
static int omap_hdq_remove ( struct platform_device * pdev ) <nl>  <nl> if ( hdq_data -> hdq_usecount ) { <nl> dev_dbg (& pdev -> dev , " removed when use count is not zero \ n "); <nl> + mutex_unlock (& hdq_data -> hdq_mutex ); <nl> return - EBUSY ; <nl> } <nl> 
static int exynos_drm_fbdev_update ( struct drm_fb_helper * helper , <nl>  <nl> fbi -> screen_base = buffer -> kvaddr + offset ; <nl> fbi -> screen_size = size ; <nl> + fbi -> fix . smem_len = size ; <nl>  <nl> return 0 ; <nl> }
static void do_checkpoint ( struct f2fs_sb_info * sbi , bool is_umount ) <nl> /* Here , we only have one bio having CP pack */ <nl> sync_meta_pages ( sbi , META_FLUSH , LONG_MAX ); <nl>  <nl> - if ( unlikely (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG ))) { <nl> + if (! is_set_ckpt_flags ( ckpt , CP_ERROR_FLAG )) { <nl> clear_prefree_segments ( sbi ); <nl> release_dirty_inode ( sbi ); <nl> F2FS_RESET_SB_DIRT ( sbi );
snd_pmac_burgundy_busy_wait ( struct snd_pmac * chip ) <nl> int timeout = 50 ; <nl> while (( in_le32 (& chip -> awacs -> codec_ctrl ) & MASK_NEWECMD ) && timeout --) <nl> udelay ( 1 ); <nl> - if (! timeout ) <nl> + if ( timeout < 0 ) <nl> printk ( KERN_DEBUG " burgundy_busy_wait : timeout \ n "); <nl> } <nl> 
static void tpacket_destruct_skb ( struct sk_buff * skb ) <nl> struct packet_sock * po = pkt_sk ( skb -> sk ); <nl> void * ph ; <nl>  <nl> - BUG_ON ( skb == NULL ); <nl> - <nl> if ( likely ( po -> tx_ring . pg_vec )) { <nl> ph = skb_shinfo ( skb )-> destructor_arg ; <nl> BUG_ON ( __packet_get_status ( po , ph ) != TP_STATUS_SENDING );
void __init exynos5_init_irq ( void ) <nl> * Theses parameters should be NULL and 0 because EXYNOS4 <nl> * uses GIC instead of VIC . <nl> */ <nl> - s5p_init_irq ( NULL , 0 ); <nl> + if (! of_machine_is_compatible (" samsung , exynos5440 ")) <nl> + s5p_init_irq ( NULL , 0 ); <nl>  <nl> gic_arch_extn . irq_set_wake = s3c_irq_wake ; <nl> }
static void vss_on_reset ( void ) <nl> int <nl> hv_vss_init ( struct hv_util_service * srv ) <nl> { <nl> + if ( vmbus_proto_version < VERSION_WIN8_1 ) { <nl> + pr_warn (" Integration service ' Backup ( volume snapshot )'" <nl> + " not supported on this host version .\ n "); <nl> + return - ENOTSUPP ; <nl> + } <nl> recv_buffer = srv -> recv_buffer ; <nl>  <nl> /*
void destroy_preds ( struct ftrace_event_call * call ) <nl> filter_free_pred ( filter -> preds [ i ]); <nl> } <nl> kfree ( filter -> preds ); <nl> + kfree ( filter -> filter_string ); <nl> kfree ( filter ); <nl> call -> filter = NULL ; <nl> }
static const struct of_device_id bcm_kona_i2c_of_match [] = { <nl> {. compatible = " brcm , kona - i2c ",}, <nl> {}, <nl> }; <nl> - MODULE_DEVICE_TABLE ( of , kona_i2c_of_match ); <nl> + MODULE_DEVICE_TABLE ( of , bcm_kona_i2c_of_match ); <nl>  <nl> static struct platform_driver bcm_kona_i2c_driver = { <nl> . driver = {
static int amdgpu_ttm_io_mem_reserve ( struct ttm_bo_device * bdev , struct ttm_mem_ <nl> mem -> bus . addr = <nl> ioremap_nocache ( mem -> bus . base + mem -> bus . offset , <nl> mem -> bus . size ); <nl> + if (! mem -> bus . addr ) <nl> + return - ENOMEM ; <nl>  <nl> /* <nl> * Alpha : Use just the bus offset plus
int snd_soc_dapm_device_event ( struct snd_soc_device * socdev , int event ) <nl> struct snd_soc_machine * machine = socdev -> machine ; <nl>  <nl> if ( machine -> dapm_event ) <nl> - machine -> dapm_event ( machine , event ); <nl> + machine -> dapm_event ( machine , event ); <nl> if ( codec -> dapm_event ) <nl> - codec -> dapm_event ( codec , event ); <nl> + codec -> dapm_event ( codec , event ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_device_event );
static int ceph_con_in_msg_alloc ( struct ceph_connection * con , int * skip ) <nl> msg = con -> ops -> alloc_msg ( con , hdr , skip ); <nl> mutex_lock (& con -> mutex ); <nl> if ( con -> state != CON_STATE_OPEN ) { <nl> - ceph_msg_put ( msg ); <nl> + if ( msg ) <nl> + ceph_msg_put ( msg ); <nl> return - EAGAIN ; <nl> } <nl> con -> in_msg = msg ;
coh901318_tx_status ( struct dma_chan * chan , dma_cookie_t cookie , <nl> enum dma_status ret ; <nl>  <nl> ret = dma_cookie_status ( chan , cookie , txstate ); <nl> - if ( ret == DMA_COMPLETE ) <nl> + if ( ret == DMA_COMPLETE || ! txstate ) <nl> return ret ; <nl>  <nl> dma_set_residue ( txstate , coh901318_get_bytes_left ( chan ));
int ssb_bus_scan ( struct ssb_bus * bus , <nl> /* Ignore PCI cores on PCI - E cards . <nl> * Ignore PCI - E cores on PCI cards . */ <nl> if ( dev -> id . coreid == SSB_DEV_PCI ) { <nl> - if ( bus -> host_pci -> is_pcie ) <nl> + if ( pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } else { <nl> - if (! bus -> host_pci -> is_pcie ) <nl> + if (! pci_is_pcie ( bus -> host_pci )) <nl> continue ; <nl> } <nl> }
TLan_FinishReset ( struct net_device * dev ) <nl> TLan_SetTimer ( dev , ( 10 * HZ ), TLAN_TIMER_FINISH_RESET ); <nl> return ; <nl> } <nl> + TLan_SetMulticastList ( dev ); <nl>  <nl> } /* TLan_FinishReset */ <nl> 
vmw_execbuf_copy_fence_user ( struct vmw_private * dev_priv , <nl> if ( user_fence_rep == NULL ) <nl> return ; <nl>  <nl> + memset (& fence_rep , 0 , sizeof ( fence_rep )); <nl> + <nl> fence_rep . error = ret ; <nl> if ( ret == 0 ) { <nl> BUG_ON ( fence == NULL );
static void xfrm_hash_rebuild ( struct work_struct * work ) <nl>  <nl> /* re - insert all policies by order of creation */ <nl> list_for_each_entry_reverse ( policy , & net -> xfrm . policy_all , walk . all ) { <nl> + if ( xfrm_policy_id2dir ( policy -> index ) >= XFRM_POLICY_MAX ) { <nl> + /* skip socket policies */ <nl> + continue ; <nl> + } <nl> newpos = NULL ; <nl> chain = policy_hash_bysel ( net , & policy -> selector , <nl> policy -> family ,
struct usb_tt { <nl> struct usb_device * hub ; /* upstream highspeed hub */ <nl> int multi ; /* true means one TT per port */ <nl> unsigned think_time ; /* think time in ns */ <nl> + void * hcpriv ; /* HCD private data */ <nl>  <nl> /* for control / bulk error recovery ( CLEAR_TT_BUFFER ) */ <nl> spinlock_t lock ;
static int __pppoe_xmit ( struct sock * sk , struct sk_buff * skb ) <nl> * give dev_queue_xmit something it can free . <nl> */ <nl> skb2 = skb_clone ( skb , GFP_ATOMIC ); <nl> + <nl> + if ( skb2 == NULL ) <nl> + goto abort ; <nl> } <nl>  <nl> ph = ( struct pppoe_hdr *) skb_push ( skb2 , sizeof ( struct pppoe_hdr ));
static void pic_clear_isr ( struct kvm_kpic_state * s , int irq ) <nl> void kvm_pic_clear_isr_ack ( struct kvm * kvm ) <nl> { <nl> struct kvm_pic * s = pic_irqchip ( kvm ); <nl> + pic_lock ( s ); <nl> s -> pics [ 0 ]. isr_ack = 0xff ; <nl> s -> pics [ 1 ]. isr_ack = 0xff ; <nl> + pic_unlock ( s ); <nl> } <nl>  <nl> /*
fastcall void __kprobes do_general_protection ( struct pt_regs * regs , <nl> tss -> io_bitmap_max - thread -> io_bitmap_max ); <nl> tss -> io_bitmap_max = thread -> io_bitmap_max ; <nl> tss -> io_bitmap_base = IO_BITMAP_OFFSET ; <nl> + tss -> io_bitmap_owner = thread ; <nl> put_cpu (); <nl> return ; <nl> }
static struct davinci_nand_pdata davinci_nand_data = { <nl> . nr_parts = ARRAY_SIZE ( davinci_nand_partitions ), <nl> . ecc_mode = NAND_ECC_HW , <nl> . options = NAND_USE_FLASH_BBT , <nl> + . ecc_bits = 4 , <nl> }; <nl>  <nl> static struct resource davinci_nand_resources [] = {
static int __init create_setup_data_nodes ( struct dentry * parent ) <nl> if ( PageHighMem ( pg )) { <nl> data = ioremap_cache ( pa_data , sizeof (* data )); <nl> if (! data ) { <nl> + kfree ( node ); <nl> error = - ENXIO ; <nl> goto err_dir ; <nl> }
static int intel_backlight_device_update_status ( struct backlight_device * bd ) <nl> */ <nl> if ( panel -> backlight . enabled ) { <nl> if ( panel -> backlight_power ) { <nl> - bool enable = bd -> props . power == FB_BLANK_UNBLANK ; <nl> + bool enable = bd -> props . power == FB_BLANK_UNBLANK && <nl> + bd -> props . brightness != 0 ; <nl> panel -> backlight_power ( connector , enable ); <nl> } <nl> } else {
static int cipso_v4_map_cat_rbm_hton ( const struct cipso_v4_doi * doi_def , <nl>  <nl> switch ( doi_def -> type ) { <nl> case CIPSO_V4_MAP_PASS : <nl> - net_spot_max = host_cat_len - 1 ; <nl> - while ( net_spot_max > 0 && host_cat [ net_spot_max ] == 0 ) <nl> + net_spot_max = host_cat_len ; <nl> + while ( net_spot_max > 0 && host_cat [ net_spot_max - 1 ] == 0 ) <nl> net_spot_max --; <nl> if ( net_spot_max > net_cat_len ) <nl> return - EINVAL ;
int main ( int argc , char ** argv ) <nl> read_relocs ( fp ); <nl> if ( show_absolute_syms ) { <nl> print_absolute_symbols (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> if ( show_absolute_relocs ) { <nl> print_absolute_relocs (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> emit_relocs ( as_text , use_real_mode ); <nl> + out : <nl> + fclose ( fp ); <nl> return 0 ; <nl> }
static void das16_interrupt ( struct comedi_device * dev ) <nl> cfc_write_array_to_buffer ( s , <nl> devpriv -> dma_buffer [ buffer_index ], num_bytes ); <nl>  <nl> - cfc_handle_events ( dev , s ); <nl> + comedi_handle_events ( dev , s ); <nl> } <nl>  <nl> static void das16_timer_interrupt ( unsigned long arg )
int oxygen_pci_probe ( struct pci_dev * pci , int index , char * id , <nl> goto err_pci_regions ; <nl>  <nl> if ( chip -> model . model_data_size ) { <nl> - chip -> model_data = kmalloc ( chip -> model . model_data_size , <nl> + chip -> model_data = kzalloc ( chip -> model . model_data_size , <nl> GFP_KERNEL ); <nl> if (! chip -> model_data ) { <nl> err = - ENOMEM ;
static int uvc_v4l2_open ( struct file * file ) <nl> if ( atomic_inc_return (& stream -> dev -> users ) == 1 ) { <nl> ret = uvc_status_start ( stream -> dev ); <nl> if ( ret < 0 ) { <nl> - usb_autopm_put_interface ( stream -> dev -> intf ); <nl> atomic_dec (& stream -> dev -> users ); <nl> + usb_autopm_put_interface ( stream -> dev -> intf ); <nl> kfree ( handle ); <nl> return ret ; <nl> }
static void bnx2x_set_one_mac_e1x ( struct bnx2x * bp , <nl> /* Reset the ramrod data buffer */ <nl> memset ( config , 0 , sizeof (* config )); <nl>  <nl> - bnx2x_vlan_mac_set_rdata_e1x ( bp , o , BNX2X_FILTER_MAC_PENDING , <nl> + bnx2x_vlan_mac_set_rdata_e1x ( bp , o , raw -> state , <nl> cam_offset , add , <nl> elem -> cmd_data . vlan_mac . u . mac . mac , 0 , <nl> ETH_VLAN_FILTER_ANY_VLAN , config );
static ssize_t yurex_write ( struct file * file , const char * user_buffer , size_t co <nl> goto error ; <nl>  <nl> mutex_lock (& dev -> io_mutex ); <nl> - if (! dev -> interface ) { /* alreaday disconnected */ <nl> + if (! dev -> interface ) { /* already disconnected */ <nl> mutex_unlock (& dev -> io_mutex ); <nl> retval = - ENODEV ; <nl> goto error ;
int hfsplus_get_block ( struct inode * inode , sector_t iblock , <nl> goto done ; <nl> } <nl>  <nl> + if ( inode -> i_ino == HFSPLUS_EXT_CNID ) <nl> + return - EIO ; <nl> + <nl> mutex_lock (& HFSPLUS_I ( inode ). extents_lock ); <nl> res = hfsplus_ext_read_extent ( inode , ablock ); <nl> if (! res ) {
struct rxrpc_call * rxrpc_new_incoming_call ( struct rxrpc_local * local , <nl>  <nl> /* Get the socket providing the service */ <nl> rx = rcu_dereference ( local -> service ); <nl> - if ( service_id == rx -> srx . srx_service ) <nl> + if ( rx && service_id == rx -> srx . srx_service ) <nl> goto found_service ; <nl>  <nl> trace_rxrpc_abort (" INV ", sp -> hdr . cid , sp -> hdr . callNumber , sp -> hdr . seq ,
static int atalk_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> return - ENOBUFS ; <nl>  <nl> * uaddr_len = sizeof ( struct sockaddr_at ); <nl> + memset (& sat . sat_zero , 0 , sizeof ( sat . sat_zero )); <nl>  <nl> if ( peer ) { <nl> if ( sk -> sk_state != TCP_ESTABLISHED )
int trace_parser_get_init ( struct trace_parser * parser , int size ) <nl> void trace_parser_put ( struct trace_parser * parser ) <nl> { <nl> kfree ( parser -> buffer ); <nl> + parser -> buffer = NULL ; <nl> } <nl>  <nl> /*
found : <nl>  <nl> if ( codec -> reg_cache ) <nl> kfree ( codec -> reg_cache ); <nl> + kfree ( codec -> name ); <nl> kfree ( codec ); <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_unregister_codec );
int cvmx_usb_initialize ( struct cvmx_usb_state * state , int usb_port_number , <nl> } <nl> } <nl>  <nl> - memset ( usb , 0 , sizeof ( usb )); <nl> + memset ( usb , 0 , sizeof (* usb )); <nl> usb -> init_flags = flags ; <nl>  <nl> /* Initialize the USB state structure */
struct comedi_device * comedi_open ( const char * filename ) <nl> if ( strncmp ( filename , "/ dev / comedi ", 11 ) != 0 ) <nl> return NULL ; <nl>  <nl> - minor = simple_strtoul ( filename + 11 , NULL , 0 ); <nl> + if ( kstrtouint ( filename + 11 , 0 , & minor )) <nl> + return NULL ; <nl>  <nl> if ( minor >= COMEDI_NUM_BOARD_MINORS ) <nl> return NULL ;
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
 <nl> void btrfs_tree_lock ( struct extent_buffer * eb ); <nl> void btrfs_tree_unlock ( struct extent_buffer * eb ); <nl> - int btrfs_try_spin_lock ( struct extent_buffer * eb ); <nl>  <nl> void btrfs_tree_read_lock ( struct extent_buffer * eb ); <nl> void btrfs_tree_read_unlock ( struct extent_buffer * eb );
static struct array_cache ** alloc_alien_cache ( int node , int limit ) <nl> } <nl> ac_ptr [ i ] = alloc_arraycache ( node , limit , 0xbaadf00d ); <nl> if (! ac_ptr [ i ]) { <nl> - for ( i --; i <= 0 ; i --) <nl> + for ( i --; i >= 0 ; i --) <nl> kfree ( ac_ptr [ i ]); <nl> kfree ( ac_ptr ); <nl> return NULL ;
int ttm_bo_pipeline_move ( struct ttm_buffer_object * bo , <nl> */ <nl>  <nl> spin_lock (& from -> move_lock ); <nl> - if (! from -> move || fence_is_later ( from -> move , fence )) { <nl> + if (! from -> move || fence_is_later ( fence , from -> move )) { <nl> fence_put ( from -> move ); <nl> from -> move = fence_get ( fence ); <nl> }
err_iounmap : <nl> err_free_mem_region : <nl> release_mem_region ( res -> start , resource_size ( res )); <nl> err_free_mem : <nl> - input_free_device ( kbc -> idev ); <nl> + input_free_device ( input_dev ); <nl> kfree ( kbc ); <nl>  <nl> return err ;
static int sunxi_pctrl_dt_node_to_map ( struct pinctrl_dev * pctldev , <nl> configlen ++; <nl>  <nl> pinconfig = kzalloc ( configlen * sizeof (* pinconfig ), GFP_KERNEL ); <nl> + if (! pinconfig ) { <nl> + kfree (* map ); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> if (! of_property_read_u32 ( node , " allwinner , drive ", & val )) { <nl> u16 strength = ( val + 1 ) * 10 ;
static int dbgp_control_msg ( unsigned devnum , int requesttype , <nl> int ret ; <nl>  <nl> read = ( requesttype & USB_DIR_IN ) != 0 ; <nl> - if ( size > ( read ? DBGP_MAX_PACKET : 0 )) <nl> + if ( size > ( read ? DBGP_MAX_PACKET : 0 )) <nl> return - 1 ; <nl>  <nl> /* Compute the control message */
static int ad799x_read_event_value ( struct iio_dev * indio_dev , <nl> if ( ret < 0 ) <nl> return ret ; <nl> * val = ( ret >> chan -> scan_type . shift ) & <nl> - GENMASK ( chan -> scan_type . realbits - 1 , 0 ); <nl> + GENMASK ( chan -> scan_type . realbits - 1 , 0 ); <nl>  <nl> return IIO_VAL_INT ; <nl> }
static void __init eva_init ( void ) <nl> platform_add_devices ( eva_devices , <nl> ARRAY_SIZE ( eva_devices )); <nl>  <nl> - eva_clock_init (); <nl> - <nl> rmobile_add_device_to_domain (" A4LC ", & lcdc0_device ); <nl> rmobile_add_device_to_domain (" A4LC ", & hdmi_lcdc_device ); <nl> if ( usb ) <nl> static void __init eva_earlytimer_init ( void ) <nl> { <nl> r8a7740_clock_init ( MD_CK0 | MD_CK2 ); <nl> shmobile_earlytimer_init (); <nl> + <nl> + /* the rate of extal1 clock must be set before late_time_init */ <nl> + eva_clock_init (); <nl> } <nl>  <nl> static void __init eva_add_early_devices ( void )
struct tcp_md5sig_pool * __tcp_get_md5sig_pool ( int cpu ) <nl>  <nl> EXPORT_SYMBOL ( __tcp_get_md5sig_pool ); <nl>  <nl> - void __tcp_put_md5sig_pool ( void ) { <nl> - __tcp_free_md5sig_pool ( tcp_md5sig_pool ); <nl> + void __tcp_put_md5sig_pool ( void ) <nl> +{ <nl> + tcp_free_md5sig_pool (); <nl> } <nl>  <nl> EXPORT_SYMBOL ( __tcp_put_md5sig_pool );
int rtllib_rx ( struct rtllib_device * ieee , struct sk_buff * skb , <nl> return ret ; <nl>  <nl> rx_dropped : <nl> - ieee -> stats . rx_dropped ++; <nl> + if ( ieee ) <nl> + ieee -> stats . rx_dropped ++; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( rtllib_rx );
static void setup_rt_frame ( int sig , struct k_sigaction * ka , siginfo_t * info , <nl>  <nl> set_fs ( USER_DS ); <nl>  <nl> + /* the tracer may want to single - step inside the handler */ <nl> + if ( test_thread_flag ( TIF_SINGLESTEP )) <nl> + ptrace_notify ( SIGTRAP ); <nl> + <nl> # ifdef DEBUG_SIG <nl> printk ( KERN_INFO " SIG deliver (% s :% d ): sp =% p pc =% 08lx \ n ", <nl> current -> comm , current -> pid , frame , regs -> pc );
int cap_bprm_set_creds ( struct linux_binprm * bprm ) <nl> { <nl> const struct cred * old = current_cred (); <nl> struct cred * new = bprm -> cred ; <nl> - bool effective , has_cap ; <nl> + bool effective , has_cap = false ; <nl> int ret ; <nl>  <nl> effective = false ;
void __init iop13xx_platform_init ( void ) <nl>  <nl> # ifdef CONFIG_MTD_PHYSMAP <nl> iq8134x_flash_resource . end = iq8134x_flash_resource . start + <nl> - iq8134x_probe_flash_size (); <nl> + iq8134x_probe_flash_size () - 1 ; <nl> if ( iq8134x_flash_resource . end > iq8134x_flash_resource . start ) <nl> iop13xx_devices [ plat_idx ++] = & iq8134x_flash ; <nl> else
void usb_del_gadget_udc ( struct usb_gadget * gadget ) <nl> flush_work (& gadget -> work ); <nl> device_unregister (& udc -> dev ); <nl> device_unregister (& gadget -> dev ); <nl> + memset (& gadget -> dev , 0x00 , sizeof ( gadget -> dev )); <nl> } <nl> EXPORT_SYMBOL_GPL ( usb_del_gadget_udc ); <nl> 
static ssize_t i40e_dbg_command_write ( struct file * filp , <nl> if (! cmd_buf ) <nl> return count ; <nl> bytes_not_copied = copy_from_user ( cmd_buf , buffer , count ); <nl> - if ( bytes_not_copied < 0 ) <nl> + if ( bytes_not_copied < 0 ) { <nl> + kfree ( cmd_buf ); <nl> return bytes_not_copied ; <nl> + } <nl> if ( bytes_not_copied > 0 ) <nl> count -= bytes_not_copied ; <nl> cmd_buf [ count ] = '\ 0 ';
struct tpm_chip * tpm_register_hardware ( struct device * dev , const struct tpm_vend <nl> put_device ( dev ); <nl> clear_bit ( chip -> dev_num , dev_mask ); <nl> kfree ( chip ); <nl> + kfree ( devname ); <nl> return NULL ; <nl> } <nl> 
static int audio_set_pcm_format ( struct snd_pcm_hardware * pcm_hw , <nl> if ( cfg -> subbuffer_size != 1 ) <nl> goto error ; <nl> pr_info (" PCM format is 8 - bit mono \ n "); <nl> + pcm_hw -> channels_min = 1 ; <nl> + pcm_hw -> channels_max = 1 ; <nl> pcm_hw -> formats = SNDRV_PCM_FMTBIT_S8 ; <nl> } else if (! strcmp ( pcm_format , " 2x16 ")) { <nl> if ( cfg -> subbuffer_size != 4 )
void ceph_mdsc_sync ( struct ceph_mds_client * mdsc ) <nl> { <nl> u64 want_tid , want_flush ; <nl>  <nl> + if ( mdsc -> client -> mount_state == CEPH_MOUNT_SHUTDOWN ) <nl> + return ; <nl> + <nl> dout (" sync \ n "); <nl> mutex_lock (& mdsc -> mutex ); <nl> want_tid = mdsc -> last_tid ;
static inline int dev_hard_header ( struct sk_buff * skb , struct net_device * dev , <nl> const void * daddr , const void * saddr , <nl> unsigned len ) <nl> { <nl> - if (! dev -> header_ops ) <nl> + if (! dev -> header_ops || ! dev -> header_ops -> create ) <nl> return 0 ; <nl>  <nl> return dev -> header_ops -> create ( skb , dev , type , daddr , saddr , len );
static struct rt6_info * ip6_route_redirect ( struct in6_addr * dest , <nl> }, <nl> }, <nl> }, <nl> - . gateway = * gateway , <nl> }; <nl>  <nl> + ipv6_addr_copy (& rdfl . gateway , gateway ); <nl> + <nl> if ( rt6_need_strict ( dest )) <nl> flags |= RT6_LOOKUP_F_IFACE ; <nl> 
static void bnx2x_tpa_stop ( struct bnx2x * bp , struct bnx2x_fastpath * fp , <nl>  <nl> return ; <nl> } <nl> - bnx2x_frag_free ( fp , new_data ); <nl> + if ( new_data ) <nl> + bnx2x_frag_free ( fp , new_data ); <nl> drop : <nl> /* drop the packet and keep the buffer in the bin */ <nl> DP ( NETIF_MSG_RX_STATUS ,
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> /* Device config is special , because it must work on <nl> * an unconfigured device . */ <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> + if ( minor >= COMEDI_NUM_BOARD_MINORS ) { <nl> + /* Device config not appropriate on non - board minors . */ <nl> + rc = - ENOTTY ; <nl> + goto done ; <nl> + } <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> if ( rc == 0 )
static void queue_event ( struct client * client , struct event * event , <nl> event -> v [ 1 ]. size = size1 ; <nl>  <nl> spin_lock_irqsave (& client -> lock , flags ); <nl> - <nl> list_add_tail (& event -> link , & client -> event_list ); <nl> - wake_up_interruptible (& client -> wait ); <nl> - <nl> spin_unlock_irqrestore (& client -> lock , flags ); <nl> + <nl> + wake_up_interruptible (& client -> wait ); <nl> } <nl>  <nl> static int
static int ina2xx_buffer_enable ( struct iio_dev * indio_dev ) <nl> chip -> prev_ns = iio_get_time_ns (); <nl>  <nl> chip -> task = kthread_run ( ina2xx_capture_thread , ( void *) indio_dev , <nl> - " ina2xx -% uus ", sampling_us ); <nl> + "% s :% d -% uus ", indio_dev -> name , indio_dev -> id , <nl> + sampling_us ); <nl>  <nl> return PTR_ERR_OR_ZERO ( chip -> task ); <nl> }
static int emc1403_probe ( struct i2c_client * client , <nl> res = sysfs_create_group (& client -> dev . kobj , & m_thermal_gr ); <nl> if ( res ) { <nl> dev_warn (& client -> dev , " create group failed \ n "); <nl> - hwmon_device_unregister ( data -> hwmon_dev ); <nl> goto thermal_error1 ; <nl> } <nl> data -> hwmon_dev = hwmon_device_register (& client -> dev );
static unsigned long output_ptr = 0 ; <nl> static void * malloc ( int size ); <nl> static void free ( void * where ); <nl>  <nl> + void * memset ( void * s , int c , unsigned n ); <nl> + void * memcpy ( void * dest , const void * src , unsigned n ); <nl> + <nl> static void putstr ( const char *); <nl>  <nl> extern int end ;
static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath_softc * sc = hw -> priv ; <nl> int ret = 0 ; <nl>  <nl> - local_bh_disable (); <nl> + mutex_lock (& sc -> mutex ); <nl>  <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( sc -> sc_ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> - local_bh_enable (); <nl> + mutex_unlock (& sc -> mutex ); <nl>  <nl> return ret ; <nl> }
static int virtnet_probe ( struct virtio_device * vdev ) <nl> dev -> mtu = mtu ; <nl> dev -> max_mtu = mtu ; <nl> } <nl> + <nl> + /* TODO : size buffers correctly in this case . */ <nl> + if ( dev -> mtu > ETH_DATA_LEN ) <nl> + vi -> big_packets = true ; <nl> } <nl>  <nl> if ( vi -> any_header_sg )
int sc_ioctl ( int card , scs_ioctl * data ) <nl>  <nl> case SCIOCSTART : <nl> { <nl> + kfree ( rcvmsg ); <nl> pr_debug ("% s : SCIOSTART : ioctl received \ n ", <nl> sc_adapter [ card ]-> devicename ); <nl> if ( sc_adapter [ card ]-> EngineUp ) {
int load_bpf_file ( char * path ) <nl> Elf_Data * data , * data_prog , * symbols = NULL ; <nl> char * shname , * shname_prog ; <nl>  <nl> + /* reset global variables */ <nl> + kern_version = 0 ; <nl> + memset ( license , 0 , sizeof ( license )); <nl> + memset ( processed_sec , 0 , sizeof ( processed_sec )); <nl> + <nl> if ( elf_version ( EV_CURRENT ) == EV_NONE ) <nl> return 1 ; <nl> 
void usbnet_skb_return ( struct usbnet * dev , struct sk_buff * skb ) <nl> return ; <nl> } <nl>  <nl> - skb -> protocol = eth_type_trans ( skb , dev -> net ); <nl> + /* only update if unset to allow minidriver rx_fixup override */ <nl> + if ( skb -> protocol == 0 ) <nl> + skb -> protocol = eth_type_trans ( skb , dev -> net ); <nl> + <nl> dev -> net -> stats . rx_packets ++; <nl> dev -> net -> stats . rx_bytes += skb -> len ; <nl> 
static inline void ftrace_dump ( enum ftrace_dump_mode oops_dump_mode ) { } <nl> # define COMPACTION_BUILD 0 <nl> # endif <nl>  <nl> +/* This helps us to avoid # ifdef CONFIG_SYMBOL_PREFIX */ <nl> +# ifdef CONFIG_SYMBOL_PREFIX <nl> +# define SYMBOL_PREFIX CONFIG_SYMBOL_PREFIX <nl> +# else <nl> +# define SYMBOL_PREFIX "" <nl> +# endif <nl> + <nl> /* Rebuild everything on CONFIG_FTRACE_MCOUNT_RECORD */ <nl> # ifdef CONFIG_FTRACE_MCOUNT_RECORD <nl> # define REBUILD_DUE_TO_FTRACE_MCOUNT_RECORD
static void __init sanity_check_meminfo ( void ) <nl> * Check whether this memory bank would entirely overlap <nl> * the vmalloc area . <nl> */ <nl> - if ( __va ( bank -> start ) >= VMALLOC_MIN ) { <nl> + if ( __va ( bank -> start ) >= VMALLOC_MIN || <nl> + __va ( bank -> start ) < PAGE_OFFSET ) { <nl> printk ( KERN_NOTICE " Ignoring RAM at %. 8lx -%. 8lx " <nl> "( vmalloc region overlap ).\ n ", <nl> bank -> start , bank -> start + bank -> size - 1 );
static void unfreeze_partials ( struct kmem_cache * s ) <nl>  <nl> new . frozen = 0 ; <nl>  <nl> - if (! new . inuse && (! n || n -> nr_partial < s -> min_partial )) <nl> + if (! new . inuse && (! n || n -> nr_partial > s -> min_partial )) <nl> m = M_FREE ; <nl> else { <nl> struct kmem_cache_node * n2 = get_node ( s ,
EXPORT_SYMBOL ( writeback_in_progress ); <nl>  <nl> struct backing_dev_info * inode_to_bdi ( struct inode * inode ) <nl> { <nl> - struct super_block * sb = inode -> i_sb ; <nl> + struct super_block * sb ; <nl> + <nl> + if (! inode ) <nl> + return & noop_backing_dev_info ; <nl> + <nl> + sb = inode -> i_sb ; <nl> # ifdef CONFIG_BLOCK <nl> if ( sb_is_blkdev_sb ( sb )) <nl> return blk_get_backing_dev_info ( I_BDEV ( inode ));
static inline void __cache_free ( struct kmem_cache * cachep , void * objp ) <nl> check_irq_off (); <nl> objp = cache_free_debugcheck ( cachep , objp , __builtin_return_address ( 0 )); <nl>  <nl> - if ( use_alien_caches && cache_free_alien ( cachep , objp )) <nl> + if ( cache_free_alien ( cachep , objp )) <nl> return ; <nl>  <nl> if ( likely ( ac -> avail < ac -> limit )) {
int aix_partition ( struct parsed_partitions * state ) <nl> numlvs = be16_to_cpu ( p -> numlvs ); <nl> put_dev_sector ( sect ); <nl> } <nl> - lvip = kzalloc ( sizeof ( struct lv_info ) * state -> limit , GFP_KERNEL ); <nl> + lvip = kcalloc ( state -> limit , sizeof ( struct lv_info ), GFP_KERNEL ); <nl> if (! lvip ) <nl> return 0 ; <nl> if ( numlvs && ( d = read_part_sector ( state , vgda_sector + 1 , & sect ))) {
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl> ! iwlwifi_mod_params . sw_crypto ) <nl> hw -> flags |= IEEE80211_HW_MFP_CAPABLE ; <nl>  <nl> - if ( mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> + if ( 0 && mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_UAPSD_SUPPORT ) { <nl> hw -> flags |= IEEE80211_HW_SUPPORTS_UAPSD ; <nl> hw -> uapsd_queues = IWL_UAPSD_AC_INFO ; <nl> hw -> uapsd_max_sp_len = IWL_UAPSD_MAX_SP ;
static int exynos_iommu_add_device ( struct device * dev ) <nl> struct iommu_group * group ; <nl> int ret ; <nl>  <nl> + if (! has_sysmmu ( dev )) <nl> + return - ENODEV ; <nl> + <nl> group = iommu_group_get ( dev ); <nl>  <nl> if (! group ) { <nl> static int exynos_iommu_add_device ( struct device * dev ) <nl>  <nl> static void exynos_iommu_remove_device ( struct device * dev ) <nl> { <nl> + if (! has_sysmmu ( dev )) <nl> + return ; <nl> + <nl> iommu_group_remove_device ( dev ); <nl> } <nl> 
::" a " ( rw ) : " memory ") <nl>  <nl> # define __build_write_lock_const ( rw , helper ) \ <nl> - asm volatile ( LOCK " subl $" RW_LOCK_BIAS_STR ",(% 0 )\ n \ t " \ <nl> + asm volatile ( LOCK " subl $" RW_LOCK_BIAS_STR ",% 0 \ n \ t " \ <nl> " jnz 2f \ n " \ <nl> " 1 :\ n " \ <nl> LOCK_SECTION_START ("") \
static void ieee80211_do_stop ( struct ieee80211_sub_if_data * sdata , <nl> if ( sdata -> vif . txq ) { <nl> struct txq_info * txqi = to_txq_info ( sdata -> vif . txq ); <nl>  <nl> + spin_lock_bh (& txqi -> queue . lock ); <nl> ieee80211_purge_tx_queue (& local -> hw , & txqi -> queue ); <nl> + spin_unlock_bh (& txqi -> queue . lock ); <nl> + <nl> atomic_set (& sdata -> txqs_len [ txqi -> txq . ac ], 0 ); <nl> } <nl> 
SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> if ( which > PRIO_USER || which < PRIO_PROCESS ) <nl> return - EINVAL ; <nl>  <nl> + rcu_read_lock (); <nl> read_lock (& tasklist_lock ); <nl> switch ( which ) { <nl> case PRIO_PROCESS : <nl> SYSCALL_DEFINE2 ( getpriority , int , which , int , who ) <nl> } <nl> out_unlock : <nl> read_unlock (& tasklist_lock ); <nl> + rcu_read_unlock (); <nl>  <nl> return retval ; <nl> }
static int __init init_spkm3_module ( void ) <nl> status = gss_mech_register (& gss_spkm3_mech ); <nl> if ( status ) <nl> printk (" Failed to register spkm3 gss mechanism !\ n "); <nl> - return 0 ; <nl> + return status ; <nl> } <nl>  <nl> static void __exit cleanup_spkm3_module ( void )
static int clip_constructor ( struct neighbour * neigh ) <nl>  <nl> static int clip_encap ( struct atm_vcc * vcc , int mode ) <nl> { <nl> + if (! CLIP_VCC ( vcc )) <nl> + return - EBADFD ; <nl> + <nl> CLIP_VCC ( vcc )-> encap = mode ; <nl> return 0 ; <nl> }
drm_atomic_helper_wait_for_vblanks ( struct drm_device * dev , <nl> for_each_crtc_in_state ( old_state , crtc , old_crtc_state , i ) { <nl> struct drm_crtc_state * new_crtc_state = crtc -> state ; <nl>  <nl> - if (! new_crtc_state -> active ) <nl> - continue ; <nl> - <nl> - if (! drm_atomic_helper_framebuffer_changed ( dev , <nl> - old_state , crtc )) <nl> + if (! new_crtc_state -> active || ! new_crtc_state -> planes_changed ) <nl> continue ; <nl>  <nl> ret = drm_crtc_vblank_get ( crtc );
int __vma_adjust ( struct vm_area_struct * vma , unsigned long start , <nl> * If next doesn ' t have anon_vma , import from vma after <nl> * next , if the vma overlaps with it . <nl> */ <nl> - if ( remove_next == 2 && next && ! next -> anon_vma ) <nl> + if ( remove_next == 2 && ! next -> anon_vma ) <nl> exporter = next -> vm_next ; <nl>  <nl> } else if ( end > next -> vm_start ) {
static int pty_set_pktmode ( struct tty_struct * tty , int __user * arg ) <nl> spin_lock_irq (& tty -> ctrl_lock ); <nl> if ( pktmode ) { <nl> if (! tty -> packet ) { <nl> - tty -> packet = 1 ; <nl> tty -> link -> ctrl_status = 0 ; <nl> + smp_mb (); <nl> + tty -> packet = 1 ; <nl> } <nl> } else <nl> tty -> packet = 0 ;
int qed_resc_alloc ( struct qed_dev * cdev ) <nl> DP_ERR ( p_hwfn , <nl> " Cannot allocate 0x % x EQ elements . The maximum of a u16 chain is 0x % x \ n ", <nl> n_eqes , 0xFFFF ); <nl> + rc = - EINVAL ; <nl> goto alloc_err ; <nl> } <nl> 
long vhost_dev_ioctl ( struct vhost_dev * d , unsigned int ioctl , void __user * argp ) <nl> } <nl> if ( eventfp != d -> log_file ) { <nl> filep = d -> log_file ; <nl> + d -> log_file = eventfp ; <nl> ctx = d -> log_ctx ; <nl> d -> log_ctx = eventfp ? <nl> eventfd_ctx_fileget ( eventfp ) : NULL ;
static int tcp_ack_update_window ( struct sock * sk , struct tcp_sock * tp , <nl> static void tcp_conservative_spur_to_response ( struct tcp_sock * tp ) <nl> { <nl> tp -> snd_cwnd = min ( tp -> snd_cwnd , tp -> snd_ssthresh ); <nl> + tp -> snd_cwnd_cnt = 0 ; <nl> tcp_moderate_cwnd ( tp ); <nl> } <nl> 
static long comedi_unlocked_ioctl ( struct file * file , unsigned int cmd , <nl> if ( cmd == COMEDI_DEVCONFIG ) { <nl> rc = do_devconfig_ioctl ( dev , <nl> ( struct comedi_devconfig __user *) arg ); <nl> + if ( rc == 0 ) <nl> + /* Evade comedi_auto_unconfig (). */ <nl> + dev_file_info -> hardware_device = NULL ; <nl> goto done ; <nl> } <nl> 
static void map_cpu_to_logical_apicid ( void ) <nl> { <nl> int cpu = smp_processor_id (); <nl> int apicid = logical_smp_processor_id (); <nl> + int node = apicid_to_node ( apicid ); <nl> + <nl> + if (! node_online ( node )) <nl> + node = first_online_node ; <nl>  <nl> cpu_2_logical_apicid [ cpu ] = apicid ; <nl> - map_cpu_to_node ( cpu , apicid_to_node ( apicid )); <nl> + map_cpu_to_node ( cpu , node ); <nl> } <nl>  <nl> static void unmap_cpu_to_logical_apicid ( int cpu )
void dlm_user_add_ast ( struct dlm_lkb * lkb , int type ) <nl> spin_unlock (& proc -> asts_spin ); <nl>  <nl> if ( eol ) { <nl> - spin_lock (& ua -> proc -> locks_spin ); <nl> + spin_lock (& proc -> locks_spin ); <nl> if (! list_empty (& lkb -> lkb_ownqueue )) { <nl> list_del_init (& lkb -> lkb_ownqueue ); <nl> dlm_put_lkb ( lkb ); <nl> } <nl> - spin_unlock (& ua -> proc -> locks_spin ); <nl> + spin_unlock (& proc -> locks_spin ); <nl> } <nl> out : <nl> mutex_unlock (& ls -> ls_clear_proc_locks );
static int kvm_dev_ioctl_get_supported_cpuid ( struct kvm_cpuid2 * cpuid , <nl> for ( func = 0x80000001 ; func <= limit && nent < cpuid -> nent ; ++ func ) <nl> do_cpuid_ent (& cpuid_entries [ nent ], func , 0 , <nl> & nent , cpuid -> nent ); <nl> + r = - E2BIG ; <nl> + if ( nent >= cpuid -> nent ) <nl> + goto out_free ; <nl> + <nl> r = - EFAULT ; <nl> if ( copy_to_user ( entries , cpuid_entries , <nl> nent * sizeof ( struct kvm_cpuid_entry2 )))
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static void ieee80211_handle_filtered_frame ( struct ieee80211_local * local , <nl> struct ieee80211_hdr * hdr = ( void *) skb -> data ; <nl> int ac ; <nl>  <nl> + if ( info -> flags & IEEE80211_TX_CTL_NO_PS_BUFFER ) { <nl> + ieee80211_free_txskb (& local -> hw , skb ); <nl> + return ; <nl> + } <nl> + <nl> /* <nl> * This skb ' survived ' a round - trip through the driver , and <nl> * hopefully the driver didn ' t mangle it too badly . However ,
static int dwc3_ep0_set_config ( struct dwc3 * dwc , struct usb_ctrlrequest * ctrl ) <nl> case DWC3_ADDRESS_STATE : <nl> ret = dwc3_ep0_delegate_req ( dwc , ctrl ); <nl> /* if the cfg matches and the cfg is non zero */ <nl> - if (! ret && cfg ) <nl> + if ( cfg && (! ret || ( ret == USB_GADGET_DELAYED_STATUS ))) <nl> dwc -> dev_state = DWC3_CONFIGURED_STATE ; <nl> break ; <nl> 
static int intel_pstate_set_policy ( struct cpufreq_policy * policy ) <nl> if ( policy -> policy == CPUFREQ_POLICY_PERFORMANCE ) { <nl> limits . min_perf_pct = 100 ; <nl> limits . min_perf = int_tofp ( 1 ); <nl> + limits . max_policy_pct = 100 ; <nl> limits . max_perf_pct = 100 ; <nl> limits . max_perf = int_tofp ( 1 ); <nl> limits . no_turbo = limits . turbo_disabled ;
populate_shared_memory : <nl> */ <nl> if ( ret == - EAGAIN && op_state_purged ( new_op )) { <nl> orangefs_bufmap_put ( bufmap , buffer_index ); <nl> + buffer_index = - 1 ; <nl> gossip_debug ( GOSSIP_FILE_DEBUG , <nl> "% s : going to repopulate_shared_memory .\ n ", <nl> __func__ );
static int vmx_set_msr ( struct kvm_vcpu * vcpu , u32 msr_index , u64 data ) <nl> msr = find_msr_entry ( vcpu , msr_index ); <nl> if ( msr ) <nl> msr -> data = data ; <nl> - load_msrs ( vcpu -> guest_msrs , NR_BAD_MSRS ); <nl> + if ( vcpu -> vmx_host_state . loaded ) <nl> + load_msrs ( vcpu -> guest_msrs , NR_BAD_MSRS ); <nl> break ; <nl> # endif <nl> case MSR_IA32_SYSENTER_CS :
static int auerchain_start_wait_urb ( pauerchain_t acp , struct urb * urb , int time <nl> } else <nl> status = urb -> status ; <nl>  <nl> - if ( actual_length ) <nl> + if ( status >= 0 ) <nl> * actual_length = urb -> actual_length ; <nl>  <nl> return status ;
static int mv88e6xxx_set_port_state ( struct dsa_switch * ds , int port , u8 state ) <nl> mutex_lock (& ps -> smi_mutex ); <nl>  <nl> reg = _mv88e6xxx_reg_read ( ds , REG_PORT ( port ), PORT_CONTROL ); <nl> - if ( reg < 0 ) <nl> + if ( reg < 0 ) { <nl> + ret = reg ; <nl> goto abort ; <nl> + } <nl>  <nl> oldstate = reg & PORT_CONTROL_STATE_MASK ; <nl> if ( oldstate != state ) {
static void visual_init ( struct vc_data * vc , int num , int init ) <nl> __module_get ( vc -> vc_sw -> owner ); <nl> vc -> vc_num = num ; <nl> vc -> vc_display_fg = & master_display_fg ; <nl> + if ( vc -> vc_uni_pagedir_loc ) <nl> + con_free_unimap ( vc ); <nl> vc -> vc_uni_pagedir_loc = & vc -> vc_uni_pagedir ; <nl> vc -> vc_uni_pagedir = NULL ; <nl> vc -> vc_hi_font_mask = 0 ;
asmlinkage long sys_socketcall ( int call , unsigned long __user * args ) <nl> if ( copy_from_user ( a , args , nargs [ call ])) <nl> return - EFAULT ; <nl>  <nl> - err = audit_socketcall ( nargs [ call ]/ sizeof ( unsigned long ), args ); <nl> + err = audit_socketcall ( nargs [ call ]/ sizeof ( unsigned long ), a ); <nl> if ( err ) <nl> return err ; <nl> 
static int ide_diag_taskfile ( ide_drive_t * drive , ide_task_t * args , unsigned long <nl> struct request rq ; <nl>  <nl> memset (& rq , 0 , sizeof ( rq )); <nl> + rq . ref_count = 1 ; <nl> rq . cmd_type = REQ_TYPE_ATA_TASKFILE ; <nl> rq . buffer = buf ; <nl> 
static void change ( char * dev , char * what , unsigned char * addr , <nl> " buffer \ n "); <nl>  <nl> pid = change_tramp ( argv , output , output_len ); <nl> - if ( pid < 0 ) return ; <nl> + if ( pid < 0 ) { <nl> + kfree ( output ); <nl> + return ; <nl> + } <nl>  <nl> if ( output != NULL ) { <nl> printk ("% s ", output );
int ath9k_hw_reset ( struct ath_hw * ah , struct ath9k_channel * chan , <nl>  <nl> if ( ah -> hw -> conf . radar_enabled ) { <nl> /* set HW specific DFS configuration */ <nl> + ah -> radar_conf . ext_channel = IS_CHAN_HT40 ( chan ); <nl> ath9k_hw_set_radar_params ( ah ); <nl> } <nl> 
bool i40e_is_vsi_in_vlan ( struct i40e_vsi * vsi ) <nl> * so we have to go through all the list in order to make sure <nl> */ <nl> list_for_each_entry ( f , & vsi -> mac_filter_list , list ) { <nl> - if ( f -> vlan >= 0 ) <nl> + if ( f -> vlan >= 0 || vsi -> info . pvid ) <nl> return true ; <nl> } <nl> 
static struct platform_device * crag6410_devices [] __initdata = { <nl> & s3c_device_fb , <nl> & s3c_device_ohci , <nl> & s3c_device_usb_hsotg , <nl> - & s3c_device_adc , <nl> - & s3c_device_rtc , <nl> - & s3c_device_ts , <nl> & s3c_device_timer [ 0 ], <nl> & s3c64xx_device_iis0 , <nl> & s3c64xx_device_iis1 ,
void __kvm_migrate_pit_timer ( struct kvm_vcpu * vcpu ) <nl> return ; <nl>  <nl> timer = & pit -> pit_state . timer ; <nl> + mutex_lock (& pit -> pit_state . lock ); <nl> if ( hrtimer_cancel ( timer )) <nl> hrtimer_start_expires ( timer , HRTIMER_MODE_ABS ); <nl> + mutex_unlock (& pit -> pit_state . lock ); <nl> } <nl>  <nl> static void destroy_pit_timer ( struct kvm_pit * pit )
static int parse_scriptname ( const struct option * opt __used , <nl> script ++; <nl> } else { <nl> script = str ; <nl> - ext = strchr ( script , '.'); <nl> + ext = strrchr ( script , '.'); <nl> if (! ext ) { <nl> fprintf ( stderr , " invalid script extension "); <nl> return - 1 ;
static int __devinit snd_hdspm_create ( struct snd_card * card , <nl> hdspm -> card_name = " RME AES32 "; <nl> hdspm -> midiPorts = 2 ; <nl> break ; <nl> + default : <nl> + snd_printk ( KERN_ERR " HDSPM : unknown firmware revision % x \ n ", <nl> + hdspm -> firmware_rev ); <nl> + return - ENODEV ; <nl> } <nl>  <nl> err = pci_enable_device ( pci );
static int ks7010_upload_firmware ( struct ks_sdio_card * card ) <nl> unsigned char * rom_buf ; <nl> unsigned char rw_data = 0 ; <nl> int ret ; <nl> - int length ; <nl> + unsigned int length ; <nl> const struct firmware * fw_entry = NULL ; <nl>  <nl> /* buffer allocate */
static void handle_stop_signal ( int sig , struct task_struct * p ) <nl> { <nl> struct task_struct * t ; <nl>  <nl> - if ( p -> flags & SIGNAL_GROUP_EXIT ) <nl> + if ( p -> signal -> flags & SIGNAL_GROUP_EXIT ) <nl> /* <nl> * The process is in the middle of dying already . <nl> */
struct nfs_server * nfs4_create_referral_server ( struct nfs_clone_mount * data , <nl> parent_server -> client -> cl_xprt -> prot , <nl> parent_client -> retrans_timeo , <nl> parent_client -> retrans_count ); <nl> + if ( error < 0 ) <nl> + goto error ; <nl>  <nl> /* Initialise the client representation from the parent server */ <nl> nfs_server_copy_userdata ( server , parent_server );
static int __init mpf_checksum ( unsigned char * mp , int len ) <nl> return sum & 0xFF ; <nl> } <nl>  <nl> - static void __cpuinit MP_processor_info ( struct mpc_config_processor * m ) <nl> + static void __init MP_processor_info ( struct mpc_config_processor * m ) <nl> { <nl> int apicid ; <nl> char * bootup_cpu = "";
struct ipoib_neigh * ipoib_neigh_alloc ( struct neighbour * neighbour , <nl>  <nl> neigh -> neighbour = neighbour ; <nl> neigh -> dev = dev ; <nl> + memset (& neigh -> dgid . raw , 0 , sizeof ( union ib_gid )); <nl> * to_ipoib_neigh ( neighbour ) = neigh ; <nl> skb_queue_head_init (& neigh -> queue ); <nl> ipoib_cm_set ( neigh , NULL );
again : <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> + page = compound_head ( page ); <nl> lock_page ( page ); <nl> if (! page -> mapping ) { <nl> unlock_page ( page );
static inline void rtsx_exclusive_enter_ss ( struct rtsx_chip * chip ) <nl> { <nl> struct rtsx_dev * dev = chip -> rtsx ; <nl>  <nl> - spin_lock (&( dev -> reg_lock )); <nl> + spin_lock (& dev -> reg_lock ); <nl> rtsx_enter_ss ( chip ); <nl> - spin_unlock (&( dev -> reg_lock )); <nl> + spin_unlock (& dev -> reg_lock ); <nl> } <nl>  <nl> static inline void rtsx_reset_detected_cards ( struct rtsx_chip * chip , int flag )
skl_ddi_pll_select ( struct intel_crtc * intel_crtc , <nl> DPLL_CFGCR2_KDIV ( wrpll_params . kdiv ) | <nl> DPLL_CFGCR2_PDIV ( wrpll_params . pdiv ) | <nl> wrpll_params . central_freq ; <nl> - } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT ) { <nl> + } else if ( intel_encoder -> type == INTEL_OUTPUT_DISPLAYPORT || <nl> + intel_encoder -> type == INTEL_OUTPUT_DP_MST ) { <nl> switch ( crtc_state -> port_clock / 2 ) { <nl> case 81000 : <nl> ctrl1 |= DPLL_CTRL1_LINK_RATE ( DPLL_CTRL1_LINK_RATE_810 , 0 );
static int bnx2x_populate_ext_phy ( struct bnx2x * bp , <nl> return - EINVAL ; <nl> default : <nl> * phy = phy_null ; <nl> + /* In case external PHY wasn ' t found */ <nl> + if (( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_DIRECT ) && <nl> + ( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_NOT_CONN )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static int ath9k_tx ( struct ieee80211_hw * hw , <nl> struct ath_tx_control txctl ; <nl> int hdrlen , padsize ; <nl>  <nl> + if ( aphy -> state != ATH_WIPHY_ACTIVE ) { <nl> + printk ( KERN_DEBUG " ath9k : % s : TX in unexpected wiphy state " <nl> + "% d \ n ", wiphy_name ( hw -> wiphy ), aphy -> state ); <nl> + goto exit ; <nl> + } <nl> + <nl> memset (& txctl , 0 , sizeof ( struct ath_tx_control )); <nl>  <nl> /*
static int imx_es8328_dai_init ( struct snd_soc_pcm_runtime * rtd ) <nl>  <nl> /* Headphone jack detection */ <nl> if ( gpio_is_valid ( data -> jack_gpio )) { <nl> - ret = snd_soc_jack_new ( rtd -> codec , " Headphone ", <nl> - SND_JACK_HEADPHONE | SND_JACK_BTN_0 , <nl> - & headset_jack ); <nl> + ret = snd_soc_card_jack_new ( rtd -> card , " Headphone ", <nl> + SND_JACK_HEADPHONE | SND_JACK_BTN_0 , <nl> + & headset_jack , NULL , 0 ); <nl> if ( ret ) <nl> return ret ; <nl> 
long vt_compat_ioctl ( struct tty_struct * tty , struct file * file , <nl>  <nl> case PIO_UNIMAP : <nl> case GIO_UNIMAP : <nl> - ret = do_unimap_ioctl ( cmd , up , perm , vc ); <nl> + ret = compat_unimap_ioctl ( cmd , up , perm , vc ); <nl> break ; <nl>  <nl> /*
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> - counter_config = kmalloc ( sizeof ( struct op_counter_config ) * spec -> num_counters , <nl> + counter_config = kcalloc ( spec -> num_counters , sizeof ( struct op_counter_config ), <nl> GFP_KERNEL ); <nl> if (! counter_config ) <nl> return - ENOMEM ;
static void belkin_sa_set_termios ( struct usb_serial_port * port , struct ktermios <nl> } <nl>  <nl> baud = tty_get_baud_rate ( port -> tty ); <nl> + if ( baud == 0 ) { <nl> + dbg ("% s - tty_get_baud_rate says 0 baud ", __FUNCTION__ ); <nl> + return ; <nl> + } <nl> urb_value = BELKIN_SA_BAUD ( baud ); <nl> /* Clip to maximum speed */ <nl> if ( urb_value == 0 )
get_pll_register ( struct drm_device * dev , enum pll_types type ) <nl> else { <nl> u8 * plim = & bios -> data [ bios -> pll_limit_tbl_ptr ]; <nl>  <nl> - if ( plim [ 0 ] >= 0x40 ) { <nl> + if ( plim [ 0 ] >= 0x30 ) { <nl> u8 * entry = plim + plim [ 1 ]; <nl> for ( i = 0 ; i < plim [ 3 ]; i ++, entry += plim [ 2 ]) { <nl> if ( entry [ 0 ] == type )
int fscrypt_ioctl_set_policy ( struct file * filp , const void __user * arg ) <nl> printk ( KERN_WARNING <nl> "% s : Policy inconsistent with encryption context \ n ", <nl> __func__ ); <nl> - ret = - EINVAL ; <nl> + ret = - EEXIST ; <nl> } <nl>  <nl> inode_unlock ( inode );
void brcmf_fweh_process_event ( struct brcmf_pub * drvr , <nl> alloc_flag = GFP_ATOMIC ; <nl>  <nl> event = kzalloc ( sizeof (* event ) + datalen , alloc_flag ); <nl> + if (! event ) <nl> + return ; <nl> + <nl> event -> code = code ; <nl> event -> ifidx = * ifidx ; <nl> 
static void falcon_handle_rx_event ( struct efx_channel * channel , <nl> * UDP / IPv4 , then we can rely on the hardware checksum . <nl> */ <nl> checksummed = <nl> - rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_TCP || <nl> - rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_UDP ; <nl> + efx -> rx_checksum_enabled && <nl> + ( rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_TCP || <nl> + rx_ev_hdr_type == FSE_AB_RX_EV_HDR_TYPE_IPV4_UDP ); <nl> } else { <nl> falcon_handle_rx_not_ok ( rx_queue , event , & rx_ev_pkt_ok , <nl> & discard );
static ssize_t spufs_mbox_read ( struct file * file , char __user * buf , <nl> udata = ( void __user *) buf ; <nl>  <nl> spu_acquire ( ctx ); <nl> - for ( count = 0 ; count <= len ; count += 4 , udata ++) { <nl> + for ( count = 0 ; ( count + 4 ) <= len ; count += 4 , udata ++) { <nl> int ret ; <nl> ret = ctx -> ops -> mbox_read ( ctx , & mbox_data ); <nl> if ( ret == 0 )
int map_groups__clone ( struct map_groups * mg , <nl> if ( new == NULL ) <nl> goto out_unlock ; <nl> map_groups__insert ( mg , new ); <nl> + map__put ( new ); <nl> } <nl>  <nl> err = 0 ;
static int dax_pmem_probe ( struct device * dev ) <nl> nsio = to_nd_namespace_io (& ndns -> dev ); <nl>  <nl> /* parse the ' pfn ' info block via -> rw_bytes */ <nl> - devm_nsio_enable ( dev , nsio ); <nl> + rc = devm_nsio_enable ( dev , nsio ); <nl> + if ( rc ) <nl> + return rc ; <nl> altmap = nvdimm_setup_pfn ( nd_pfn , & res , & __altmap ); <nl> if ( IS_ERR ( altmap )) <nl> return PTR_ERR ( altmap );
static int be_resume ( struct pci_dev * pdev ) <nl> pci_set_power_state ( pdev , PCI_D0 ); <nl> pci_restore_state ( pdev ); <nl>  <nl> + status = be_fw_wait_ready ( adapter ); <nl> + if ( status ) <nl> + return status ; <nl> + <nl> /* tell fw we ' re ready to fire cmds */ <nl> status = be_cmd_fw_init ( adapter ); <nl> if ( status )
relookup : <nl> p -> rate_last = 0 ; <nl> p -> pmtu_expires = 0 ; <nl> p -> pmtu_orig = 0 ; <nl> + p -> redirect_genid = 0 ; <nl> memset (& p -> redirect_learned , 0 , sizeof ( p -> redirect_learned )); <nl>  <nl> 
int __init init_hw_breakpoint ( void ) <nl> err_alloc : <nl> for_each_possible_cpu ( err_cpu ) { <nl> for ( i = 0 ; i < TYPE_MAX ; i ++) <nl> - kfree ( per_cpu ( nr_task_bp_pinned [ i ], cpu )); <nl> + kfree ( per_cpu ( nr_task_bp_pinned [ i ], err_cpu )); <nl> if ( err_cpu == cpu ) <nl> break ; <nl> }
static int i2s_startup ( struct snd_pcm_substream * substream , <nl> /* Enforce set_sysclk in Master mode */ <nl> i2s -> rclk_srcrate = 0 ; <nl>  <nl> + if (! any_active ( i2s ) && ( i2s -> quirks & QUIRK_NEED_RSTCLR )) <nl> + writel ( CON_RSTCLR , i2s -> addr + I2SCON ); <nl> + <nl> spin_unlock_irqrestore (& lock , flags ); <nl>  <nl> return 0 ;
int mxc_initialize_usb_hw ( int port , unsigned int flags ) <nl> # ifdef CONFIG_ARCH_MX51 <nl> if ( cpu_is_mx51 ()) { <nl> void __iomem * usb_base ; <nl> - u32 usbotg_base ; <nl> - u32 usbother_base ; <nl> + void __iomem * usbotg_base ; <nl> + void __iomem * usbother_base ; <nl> int ret = 0 ; <nl>  <nl> usb_base = ioremap ( MX51_OTG_BASE_ADDR , SZ_4K );
static int digi_read_oob_callback ( struct urb * urb ) <nl> return - 1 ; <nl>  <nl> /* handle each oob command */ <nl> - for ( i = 0 ; i < urb -> actual_length - 4 ; i += 4 ) { <nl> + for ( i = 0 ; i < urb -> actual_length - 3 ; i += 4 ) { <nl> opcode = buf [ i ]; <nl> line = buf [ i + 1 ]; <nl> status = buf [ i + 2 ];
skl_compute_ddb ( struct drm_atomic_state * state ) <nl> ret = skl_allocate_pipe_ddb ( cstate , ddb ); <nl> if ( ret ) <nl> return ret ; <nl> + <nl> + ret = drm_atomic_add_affected_planes ( state , & intel_crtc -> base ); <nl> + if ( ret ) <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static int fsg_setup ( struct usb_function * f , <nl> *( u8 *) req -> buf = fsg -> common -> nluns - 1 ; <nl>  <nl> /* Respond with data / status */ <nl> - req -> length = min ( 1 , w_length ); <nl> + req -> length = min (( u16 ) 1 , w_length ); <nl> fsg -> common -> ep0req_name = <nl> ctrl -> bRequestType & USB_DIR_IN ? " ep0 - in " : " ep0 - out "; <nl> return ep0_queue ( fsg -> common );
void exynos4_jpeg_set_enc_out_fmt ( void __iomem * base , unsigned int out_fmt ) <nl>  <nl> void exynos4_jpeg_set_interrupt ( void __iomem * base ) <nl> { <nl> - unsigned int reg ; <nl> - <nl> - reg = readl ( base + EXYNOS4_INT_EN_REG ) & ~ EXYNOS4_INT_EN_MASK ; <nl> writel ( EXYNOS4_INT_EN_ALL , base + EXYNOS4_INT_EN_REG ); <nl> } <nl> 
static int ibmvfc_get_err_result ( struct ibmvfc_cmd * vfc_cmd ) <nl> int fc_rsp_len = rsp -> fcp_rsp_len ; <nl>  <nl> if (( rsp -> flags & FCP_RSP_LEN_VALID ) && <nl> - ((! fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> + (( fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> rsp -> data . info . rsp_code )) <nl> return DID_ERROR << 16 ; <nl> 
static int receive_DataRequest ( struct drbd_conf * mdev , enum drbd_packets cmd , un <nl> " no local data .\ n "); <nl> drbd_send_ack_rp ( mdev , cmd == P_DATA_REQUEST ? P_NEG_DREPLY : <nl> P_NEG_RS_DREPLY , p ); <nl> - return TRUE ; <nl> + /* drain possibly payload */ <nl> + return drbd_drain_block ( mdev , digest_size ); <nl> } <nl>  <nl> /* GFP_NOIO , because we must not cause arbitrary write - out : in a DRBD
void radeon_atom_backlight_init ( struct radeon_encoder * radeon_encoder , <nl> u8 backlight_level ; <nl> char bl_name [ 16 ]; <nl>  <nl> + /* Mac laptops with multiple GPUs use the gmux driver for backlight <nl> + * so don ' t register a backlight device <nl> + */ <nl> + if (( rdev -> pdev -> subsystem_vendor == PCI_VENDOR_ID_APPLE ) && <nl> + ( rdev -> pdev -> device == 0x6741 )) <nl> + return ; <nl> + <nl> if (! radeon_encoder -> enc_priv ) <nl> return ; <nl> 
static int rt298_i2c_probe ( struct i2c_client * i2c , <nl>  <nl> /* enable jack combo mode on supported devices */ <nl> acpiid = acpi_match_device ( dev -> driver -> acpi_match_table , dev ); <nl> - if ( acpiid ) { <nl> + if ( acpiid && acpiid -> driver_data ) { <nl> rt298 -> pdata = *( struct rt298_platform_data *) <nl> acpiid -> driver_data ; <nl> }
static int dvb_register ( struct cx23885_tsport * port ) <nl>  <nl> fe = dvb_attach ( xc4000_attach , fe0 -> dvb . frontend , <nl> & dev -> i2c_bus [ 1 ]. i2c_adap , & cfg ); <nl> + if (! fe ) { <nl> + printk ( KERN_ERR "% s / 2 : xc4000 attach failed \ n ", <nl> + dev -> name ); <nl> + goto frontend_detach ; <nl> + } <nl> } <nl> break ; <nl> case CX23885_BOARD_TBS_6920 :
static void reset_connection ( struct ceph_connection * con ) <nl> con -> out_msg = NULL ; <nl> } <nl> con -> in_seq = 0 ; <nl> + con -> in_seq_acked = 0 ; <nl> } <nl>  <nl> /*
static void sta_ps_start ( struct sta_info * sta ) <nl> for ( tid = 0 ; tid < ARRAY_SIZE ( sta -> sta . txq ); tid ++) { <nl> struct txq_info * txqi = to_txq_info ( sta -> sta . txq [ tid ]); <nl>  <nl> - if (! txqi -> tin . backlog_packets ) <nl> + if ( txqi -> tin . backlog_packets ) <nl> set_bit ( tid , & sta -> txq_buffered_tids ); <nl> else <nl> clear_bit ( tid , & sta -> txq_buffered_tids );
static unsigned int xuartps_set_baud_rate ( struct uart_port * port , <nl> unsigned int baud ) <nl> { <nl> unsigned int calc_baud ; <nl> - u32 cd , bdiv ; <nl> + u32 cd = 0 , bdiv = 0 ; <nl> u32 mreg ; <nl> int div8 ; <nl> struct xuartps * xuartps = port -> private_data ;
struct fib6_node * fib6_lookup ( struct fib6_node * root , struct in6_addr * daddr , <nl> } <nl> }; <nl>  <nl> - fn = fib6_lookup_1 ( root , args ); <nl> + fn = fib6_lookup_1 ( root , daddr ? args : args + 1 ); <nl>  <nl> if ( fn == NULL || fn -> fn_flags & RTN_TL_ROOT ) <nl> fn = root ;
static int __devinit virtcons_probe ( struct virtio_device * dev ) <nl> struct virtqueue * vqs [ 2 ]; <nl> int err ; <nl>  <nl> + if ( vdev ) { <nl> + dev_warn (& vdev -> dev , <nl> + " Multiple virtio - console devices not supported yet \ n "); <nl> + return - EEXIST ; <nl> + } <nl> vdev = dev ; <nl>  <nl> /* This is the scratch page we use to receive console input */
static int w1_f29_add_slave ( struct w1_slave * sl ) <nl> static void w1_f29_remove_slave ( struct w1_slave * sl ) <nl> { <nl> int i ; <nl> - for ( i = NB_SYSFS_BIN_FILES ; i <= 0 ; -- i ) <nl> + for ( i = NB_SYSFS_BIN_FILES - 1 ; i >= 0 ; -- i ) <nl> sysfs_remove_bin_file (& sl -> dev . kobj , <nl> &( w1_f29_sysfs_bin_files [ i ])); <nl> }
try_mount_again : <nl>  <nl> remote_path_check : <nl> /* check if a whole path ( including prepath ) is not remote */ <nl> - if (! rc && cifs_sb -> prepathlen && tcon ) { <nl> + if (! rc && tcon ) { <nl> /* build_path_to_root works only when we have a valid tcon */ <nl> full_path = cifs_build_path_to_root ( cifs_sb , tcon ); <nl> if ( full_path == NULL ) {
static int btrfs_finish_ordered_io ( struct btrfs_ordered_extent * ordered_extent ) <nl> EXTENT_DEFRAG , 1 , cached_state ); <nl> if ( ret ) { <nl> u64 last_snapshot = btrfs_root_last_snapshot (& root -> root_item ); <nl> - if ( last_snapshot >= BTRFS_I ( inode )-> generation ) <nl> + if ( 0 && last_snapshot >= BTRFS_I ( inode )-> generation ) <nl> /* the inode is shared */ <nl> new = record_old_file_extents ( inode , ordered_extent ); <nl> 
static int alloc_iommu ( struct dmar_drhd_unit * drhd ) <nl> intel_iommu_groups , <nl> "% s ", iommu -> name ); <nl>  <nl> + if ( IS_ERR ( iommu -> iommu_dev )) { <nl> + drhd -> iommu = NULL ; <nl> + err = PTR_ERR ( iommu -> iommu_dev ); <nl> + goto err_unmap ; <nl> + } <nl> + <nl> return 0 ; <nl>  <nl> err_unmap :
static int device_change_notifier ( struct notifier_block * nb , <nl> case BUS_NOTIFY_UNBOUND_DRIVER : <nl> if (! domain ) <nl> goto out ; <nl> + if ( iommu_pass_through ) <nl> + break ; <nl> detach_device ( domain , devid ); <nl> break ; <nl> case BUS_NOTIFY_ADD_DEVICE :
static int __of_iio_channel_get ( struct iio_channel * channel , <nl> channel -> indio_dev = indio_dev ; <nl> index = iiospec . args_count ? iiospec . args [ 0 ] : 0 ; <nl> if ( index >= indio_dev -> num_channels ) { <nl> - return - EINVAL ; <nl> + err = - EINVAL ; <nl> goto err_put ; <nl> } <nl> channel -> channel = & indio_dev -> channels [ index ];
static void i40evf_watchdog_task ( struct work_struct * work ) <nl> watchdog_done : <nl> clear_bit ( __I40EVF_IN_CRITICAL_TASK , & adapter -> crit_section ); <nl> restart_watchdog : <nl> + if ( adapter -> state == __I40EVF_REMOVE ) <nl> + return ; <nl> if ( adapter -> aq_required ) <nl> mod_timer (& adapter -> watchdog_timer , <nl> jiffies + msecs_to_jiffies ( 20 ));
static int uas_find_endpoints ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < n_endpoints ; i ++) { <nl> unsigned char * extra = endpoint [ i ]. extra ; <nl> int len = endpoint [ i ]. extralen ; <nl> - while ( len > 1 ) { <nl> + while ( len >= 3 ) { <nl> if ( extra [ 1 ] == USB_DT_PIPE_USAGE ) { <nl> unsigned pipe_id = extra [ 2 ]; <nl> if ( pipe_id > 0 && pipe_id < 5 )
static int __inject_sigp_stop ( struct kvm_s390_local_interrupt * li , int action ) <nl> inti -> type = KVM_S390_SIGP_STOP ; <nl>  <nl> spin_lock_bh (& li -> lock ); <nl> - if (( atomic_read ( li -> cpuflags ) & CPUSTAT_STOPPED )) <nl> + if (( atomic_read ( li -> cpuflags ) & CPUSTAT_STOPPED )) { <nl> + kfree ( inti ); <nl> goto out ; <nl> + } <nl> list_add_tail (& inti -> list , & li -> list ); <nl> atomic_set (& li -> active , 1 ); <nl> atomic_set_mask ( CPUSTAT_STOP_INT , li -> cpuflags );
static void intel_find_plane_obj ( struct intel_crtc * intel_crtc , <nl> return ; <nl>  <nl> kfree ( intel_crtc -> base . fb ); <nl> + intel_crtc -> base . fb = NULL ; <nl>  <nl> /* <nl> * Failed to alloc the obj , check to see if we should share
i915_error_object_create_sized ( struct drm_i915_private * dev_priv , <nl> goto unwind ; <nl>  <nl> local_irq_save ( flags ); <nl> - if ( reloc_offset < dev_priv -> gtt . mappable_end && <nl> + if ( src -> cache_level == I915_CACHE_NONE && <nl> + reloc_offset < dev_priv -> gtt . mappable_end && <nl> src -> has_global_gtt_mapping && <nl> i915_is_ggtt ( vm )) { <nl> void __iomem * s ;
static int AdvLoadMicrocode ( AdvPortAddr iop_base , unsigned char * buf , int size , <nl> i += 2 ; <nl> len += 2 ; <nl> } else { <nl> - unsigned char off = buf [ i ] * 2 ; <nl> + unsigned int off = buf [ i ] * 2 ; <nl> unsigned short word = ( buf [ off + 1 ] << 8 ) | buf [ off ]; <nl> AdvWriteWordAutoIncLram ( iop_base , word ); <nl> len += 2 ;
static int cciss_bigpassthru ( ctlr_info_t * h , void __user * argp ) <nl> return - EINVAL ; <nl> if (! capable ( CAP_SYS_RAWIO )) <nl> return - EPERM ; <nl> - ioc = ( BIG_IOCTL_Command_struct *) <nl> - kmalloc ( sizeof (* ioc ), GFP_KERNEL ); <nl> + ioc = kmalloc ( sizeof (* ioc ), GFP_KERNEL ); <nl> if (! ioc ) { <nl> status = - ENOMEM ; <nl> goto cleanup1 ;
static int vhost_scsi_set_endpoint ( <nl> /* Flushing the vhost_work acts as synchronize_rcu */ <nl> mutex_lock (& vq -> mutex ); <nl> rcu_assign_pointer ( vq -> private_data , vs_tpg ); <nl> + vhost_init_used ( vq ); <nl> mutex_unlock (& vq -> mutex ); <nl> } <nl> ret = 0 ;
static int dw_msi_setup_irq ( struct msi_controller * chip , struct pci_dev * pdev , <nl> struct msi_msg msg ; <nl> struct pcie_port * pp = sys_to_pcie ( pdev -> bus -> sysdata ); <nl>  <nl> + if ( desc -> msi_attrib . is_msix ) <nl> + return - EINVAL ; <nl> + <nl> irq = assign_irq ( 1 , desc , & pos ); <nl> if ( irq < 0 ) <nl> return irq ;
static void __init vpac270_onenand_init ( void ) {} <nl> # if defined ( CONFIG_MMC_PXA ) || defined ( CONFIG_MMC_PXA_MODULE ) <nl> static struct pxamci_platform_data vpac270_mci_platform_data = { <nl> . ocr_mask = MMC_VDD_32_33 | MMC_VDD_33_34 , <nl> + . gpio_power = - 1 , <nl> . gpio_card_detect = GPIO53_VPAC270_SD_DETECT_N , <nl> . gpio_card_ro = GPIO52_VPAC270_SD_READONLY , <nl> . detect_delay_ms = 200 ,
BYTE byPwr = pDevice -> byCCKPwr ; <nl> return TRUE ; <nl> } <nl>  <nl> + if ( uCH == 0 ) <nl> + return - EINVAL ; <nl> + <nl> switch ( uRATE ) { <nl> case RATE_1M : <nl> case RATE_2M :
static int alc_cap_getput_caller ( struct snd_kcontrol * kcontrol , <nl> { <nl> struct hda_codec * codec = snd_kcontrol_chip ( kcontrol ); <nl> struct alc_spec * spec = codec -> spec ; <nl> - int i , err ; <nl> + int i , err = 0 ; <nl>  <nl> mutex_lock (& codec -> control_mutex ); <nl> if ( check_adc_switch && spec -> dual_adc_switch ) {
static void acm_tty_flush_chars ( struct tty_struct * tty ) <nl> int err ; <nl> unsigned long flags ; <nl>  <nl> + if (! cur ) /* nothing to do */ <nl> + return ; <nl> + <nl> acm -> putbuffer = NULL ; <nl> err = usb_autopm_get_interface_async ( acm -> control ); <nl> spin_lock_irqsave (& acm -> write_lock , flags ); <nl> if ( err < 0 ) { <nl> cur -> use = 0 ; <nl> + acm -> putbuffer = cur ; <nl> goto out ; <nl> } <nl> 
static int digi_read_oob_callback ( struct urb * urb ) <nl> return - 1 ; <nl>  <nl> /* handle each oob command */ <nl> - for ( i = 0 ; i < urb -> actual_length - 4 ; i += 4 ) { <nl> + for ( i = 0 ; i < urb -> actual_length - 3 ; i += 4 ) { <nl> opcode = buf [ i ]; <nl> line = buf [ i + 1 ]; <nl> status = buf [ i + 2 ];
parse_dcb20_entry ( struct drm_device * dev , struct dcb_table * dcb , <nl> case 0 : <nl> entry -> dpconf . link_bw = 162000 ; <nl> break ; <nl> - default : <nl> + case 1 : <nl> entry -> dpconf . link_bw = 270000 ; <nl> break ; <nl> + default : <nl> + entry -> dpconf . link_bw = 540000 ; <nl> + break ; <nl> } <nl> switch (( conf & 0x0f000000 ) >> 24 ) { <nl> case 0xf :
static int econet_sendmsg ( struct kiocb * iocb , struct socket * sock , <nl> udpdest . sin_addr . s_addr = htonl ( network | addr . station ); <nl> } <nl>  <nl> + memset (& ah , 0 , sizeof ( ah )); <nl> ah . port = port ; <nl> ah . cb = cb & 0x7f ; <nl> ah . code = 2 ; /* magic */ <nl> - ah . pad = 0 ; <nl>  <nl> /* tack our header on the front of the iovec */ <nl> size = sizeof ( struct aunhdr );
static void scrub_print_warning ( const char * errstr , struct scrub_block * sblock ) <nl> u64 flags = 0 ; <nl> u64 ref_root ; <nl> u32 item_size ; <nl> - u8 ref_level ; <nl> + u8 ref_level = 0 ; <nl> int ret ; <nl>  <nl> WARN_ON ( sblock -> page_count < 1 );
done : <nl> /* Timeouts occur when the device isn ' t connected , so they ' re <nl> * " normal " -- don ' t fill the kernel log with these */ <nl> if ( status & DP_AUX_CH_CTL_TIME_OUT_ERROR ) { <nl> - DRM_DEBUG_KMS (" dp_aux_ch timeout status 0x % 08x \ n ", status ); <nl> + DRM_DEBUG_KMS_RATELIMITED (" dp_aux_ch timeout status 0x % 08x \ n ", <nl> + status ); <nl> ret = - ETIMEDOUT ; <nl> goto out ; <nl> }
void flush_dcache_page ( struct page * page ) <nl> } <nl> EXPORT_SYMBOL ( flush_dcache_page ); <nl>  <nl> + void flush_kernel_dcache_page ( struct page * page ) <nl> +{ <nl> + __cpuc_flush_dcache_area ( page_address ( page ), PAGE_SIZE ); <nl> +} <nl> + EXPORT_SYMBOL ( flush_kernel_dcache_page ); <nl> + <nl> void copy_to_user_page ( struct vm_area_struct * vma , struct page * page , <nl> unsigned long uaddr , void * dst , const void * src , <nl> unsigned long len )
fill_write_buffer ( struct sysfs_buffer * buffer , const char __user * buf , size_t <nl> count = PAGE_SIZE - 1 ; <nl> error = copy_from_user ( buffer -> page , buf , count ); <nl> buffer -> needs_read_fill = 1 ; <nl> + /* if buf is assumed to contain a string , terminate it by \ 0 , <nl> + so e . g . sscanf () can scan the string easily */ <nl> + buffer -> page [ count ] = 0 ; <nl> return error ? - EFAULT : count ; <nl> } <nl> 
static int sctp_send_asconf_del_ip ( struct sock * sk , <nl> continue ; <nl> asoc -> asconf_addr_del_pending = <nl> kzalloc ( sizeof ( union sctp_addr ), GFP_ATOMIC ); <nl> + if ( asoc -> asconf_addr_del_pending == NULL ) { <nl> + retval = - ENOMEM ; <nl> + goto out ; <nl> + } <nl> asoc -> asconf_addr_del_pending -> sa . sa_family = <nl> addrs -> sa_family ; <nl> asoc -> asconf_addr_del_pending -> v4 . sin_port =
static sint _init_mlme_priv ( struct _adapter * padapter ) <nl> _init_queue (&( pmlmepriv -> scanned_queue )); <nl> set_scanned_network_val ( pmlmepriv , 0 ); <nl> memset (& pmlmepriv -> assoc_ssid , 0 , sizeof ( struct ndis_802_11_ssid )); <nl> - pbuf = kmalloc ( MAX_BSS_CNT * ( sizeof ( struct wlan_network )), <nl> - GFP_ATOMIC ); <nl> + pbuf = kmalloc_array ( MAX_BSS_CNT , sizeof ( struct wlan_network ), <nl> + GFP_ATOMIC ); <nl> if ( pbuf == NULL ) <nl> return _FAIL ; <nl> pmlmepriv -> free_bss_buf = pbuf ;
static void line6_stream_stop ( struct snd_line6_pcm * line6pcm , int direction , <nl> spin_lock_irqsave (& pstr -> lock , flags ); <nl> clear_bit ( type , & pstr -> running ); <nl> if (! pstr -> running ) { <nl> + spin_unlock_irqrestore (& pstr -> lock , flags ); <nl> line6_unlink_audio_urbs ( line6pcm , pstr ); <nl> + spin_lock_irqsave (& pstr -> lock , flags ); <nl> if ( direction == SNDRV_PCM_STREAM_CAPTURE ) { <nl> line6pcm -> prev_fbuf = NULL ; <nl> line6pcm -> prev_fsize = 0 ;
static void rockchip_drm_unbind ( struct device * dev ) <nl> rockchip_drm_fbdev_fini ( drm_dev ); <nl> drm_kms_helper_poll_fini ( drm_dev ); <nl>  <nl> + drm_atomic_helper_shutdown ( drm_dev ); <nl> drm_vblank_cleanup ( drm_dev ); <nl> component_unbind_all ( dev , drm_dev ); <nl> drm_mode_config_cleanup ( drm_dev );
enum mtd_file_modes { <nl> MTD_FILE_MODE_RAW , <nl> }; <nl>  <nl> + static inline int mtd_type_is_nand_user ( const struct mtd_info_user * mtd ) <nl> +{ <nl> + return mtd -> type == MTD_NANDFLASH || mtd -> type == MTD_MLCNANDFLASH ; <nl> +} <nl> + <nl> # endif /* __MTD_ABI_H__ */
int tipc_nl_add_monitor_peer ( struct net * net , struct tipc_nl_msg * msg , <nl> u32 bearer_id , u32 * prev_node ) <nl> { <nl> struct tipc_monitor * mon = tipc_monitor ( net , bearer_id ); <nl> - struct tipc_peer * peer = mon -> self ; <nl> + struct tipc_peer * peer ; <nl>  <nl> if (! mon ) <nl> return - EINVAL ; <nl>  <nl> read_lock_bh (& mon -> lock ); <nl> + peer = mon -> self ; <nl> do { <nl> if (* prev_node ) { <nl> if ( peer -> addr == * prev_node )
qlcnic_setup_netdev ( struct qlcnic_adapter * adapter , struct net_device * netdev , <nl> if ( err ) <nl> return err ; <nl>  <nl> + qlcnic_dcb_init_dcbnl_ops ( adapter -> dcb ); <nl> + <nl> err = register_netdev ( netdev ); <nl> if ( err ) { <nl> dev_err (& pdev -> dev , " failed to register net device \ n "); <nl> return err ; <nl> } <nl>  <nl> - qlcnic_dcb_init_dcbnl_ops ( adapter -> dcb ); <nl> - <nl> return 0 ; <nl> } <nl> 
void sync_timeline_signal ( struct sync_timeline * obj ) <nl> list_for_each_entry_safe ( pt , next , & obj -> active_list_head , <nl> active_list ) { <nl> if ( fence_is_signaled_locked (& pt -> base )) <nl> - list_del (& pt -> active_list ); <nl> + list_del_init (& pt -> active_list ); <nl> } <nl>  <nl> spin_unlock_irqrestore (& obj -> child_list_lock , flags );
u16 hpi_entity_alloc_and_pack ( const enum e_entity_type type , <nl> if ( hE ) <nl> return hE ; <nl>  <nl> - HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_ROLE ); <nl> + HPI_DEBUG_ASSERT ( role > entity_role_null && type < LAST_ENTITY_TYPE ); <nl>  <nl> bytes_to_copy = entity_type_to_size [ type ] * item_count ; <nl> total_size = hpi_entity_header_size (* entity ) + bytes_to_copy ;
static int orinoco_ioctl_set_auth ( struct net_device * dev , <nl> */ <nl> if ( param -> value ) { <nl> priv -> tkip_cm_active = 1 ; <nl> - ret = hermes_enable_port ( hw , 0 ); <nl> + ret = hermes_disable_port ( hw , 0 ); <nl> } else { <nl> priv -> tkip_cm_active = 0 ; <nl> - ret = hermes_disable_port ( hw , 0 ); <nl> + ret = hermes_enable_port ( hw , 0 ); <nl> } <nl> break ; <nl> 
void cfg80211_conn_work ( struct work_struct * work ) <nl> wdev_unlock ( wdev ); <nl> continue ; <nl> } <nl> - if ( wdev -> sme_state != CFG80211_SME_CONNECTING ) { <nl> + if ( wdev -> sme_state != CFG80211_SME_CONNECTING || ! wdev -> conn ) { <nl> wdev_unlock ( wdev ); <nl> continue ; <nl> }
static struct platform_driver td_driver = { <nl> . owner = THIS_MODULE , <nl> }, <nl> . probe = td_probe , <nl> - . remove = __exit_p ( td_remove ), <nl> + . remove = td_remove , <nl> }; <nl>  <nl> module_platform_driver ( td_driver );
int mmc_send_app_op_cond ( struct mmc_host * host , u32 ocr , u32 * rocr ) <nl> mmc_delay ( 10 ); <nl> } <nl>  <nl> + if (! i ) <nl> + pr_err ("% s : card never left busy state \ n ", mmc_hostname ( host )); <nl> + <nl> if ( rocr && ! mmc_host_is_spi ( host )) <nl> * rocr = cmd . resp [ 0 ]; <nl> 
static void carl9170_ps_beacon ( struct ar9170 * ar , void * data , unsigned int len ) <nl> cam = ieee80211_check_tim ( tim_ie , tim_len , ar -> common . curaid ); <nl>  <nl> /* 2 . Maybe the AP wants to send multicast / broadcast data ? */ <nl> - cam = !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl> + cam |= !!( tim_ie -> bitmap_ctrl & 0x01 ); <nl>  <nl> if (! cam ) { <nl> /* back to low - power land . */
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> int bios_reserved = 0 ; <nl>  <nl> + if ( dev_priv -> gtt . stolen_size == 0 ) <nl> + return 0 ; <nl> + <nl> dev_priv -> mm . stolen_base = i915_stolen_to_physical ( dev ); <nl> if ( dev_priv -> mm . stolen_base == 0 ) <nl> return 0 ;
nv130_chipset = { <nl> static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> + . bios = nvkm_bios_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
static struct wiphy * wlan_create_wiphy ( struct device * dev , wlandevice_t * wlandev <nl> wiphy -> n_cipher_suites = PRISM2_NUM_CIPHER_SUITES ; <nl> wiphy -> cipher_suites = prism2_cipher_suites ; <nl>  <nl> - if ( wiphy_register ( wiphy ) < 0 ) <nl> + if ( wiphy_register ( wiphy ) < 0 ) { <nl> + wiphy_free ( wiphy ); <nl> return NULL ; <nl> + } <nl>  <nl> return wiphy ; <nl> }
static int fill_buffer ( struct debug_buffer * buf ) <nl> int ret = 0 ; <nl>  <nl> if (! buf -> output_buf ) <nl> - buf -> output_buf = ( char *) vmalloc ( buf -> alloc_size ); <nl> + buf -> output_buf = vmalloc ( buf -> alloc_size ); <nl>  <nl> if (! buf -> output_buf ) { <nl> ret = - ENOMEM ;
long amdgpu_fence_wait_seq_timeout ( struct amdgpu_device * adev , u64 * target_seq , <nl> bool signaled ; <nl> int i , r ; <nl>  <nl> + if ( timeout == 0 ) { <nl> + return amdgpu_fence_any_seq_signaled ( adev , target_seq ); <nl> + } <nl> + <nl> while (! amdgpu_fence_any_seq_signaled ( adev , target_seq )) { <nl>  <nl> /* Save current sequence values , used to check for GPU lockups */
u32 ft_get_task_tag ( struct se_cmd * se_cmd ) <nl> { <nl> struct ft_cmd * cmd = container_of ( se_cmd , struct ft_cmd , se_cmd ); <nl>  <nl> + if ( cmd -> aborted ) <nl> + return ~ 0 ; <nl> return fc_seq_exch ( cmd -> seq )-> rxid ; <nl> } <nl> 
next_file : <nl> mnt_drop_write_file ( dst_file ); <nl> next_loop : <nl> fdput ( dst_fd ); <nl> + <nl> + if ( fatal_signal_pending ( current )) <nl> + goto out ; <nl> } <nl>  <nl> out :
static int gfx_v9_0_rlc_resume ( struct amdgpu_device * adev ) <nl> { <nl> int r ; <nl>  <nl> + if ( amdgpu_sriov_vf ( adev )) <nl> + return 0 ; <nl> + <nl> gfx_v9_0_rlc_stop ( adev ); <nl>  <nl> /* disable CG */
static void pti_tty_cleanup ( struct tty_struct * tty ) <nl> if ( pti_tty_data == NULL ) <nl> return ; <nl> pti_release_masterchannel ( pti_tty_data -> mc ); <nl> - kfree ( tty -> driver_data ); <nl> + kfree ( pti_tty_data ); <nl> tty -> driver_data = NULL ; <nl> } <nl> 
void __init tegra_add_of_provider ( struct device_node * np ) <nl> of_clk_add_provider ( np , of_clk_src_onecell_get , & clk_data ); <nl>  <nl> rst_ctlr . of_node = np ; <nl> - rst_ctlr . nr_resets = clk_num * 32 ; <nl> + rst_ctlr . nr_resets = periph_banks * 32 ; <nl> reset_controller_register (& rst_ctlr ); <nl> } <nl> 
static int port_fops_open ( struct inode * inode , struct file * filp ) <nl>  <nl> /* We get the port with a kref here */ <nl> port = find_port_by_devt ( cdev -> dev ); <nl> + if (! port ) { <nl> + /* Port was unplugged before we could proceed */ <nl> + return - ENXIO ; <nl> + } <nl> filp -> private_data = port ; <nl>  <nl> /*
static int rbd_get_client ( struct rbd_device * rbd_dev , const char * mon_addr , <nl> rbdc = __rbd_client_find ( opt ); <nl> if ( rbdc ) { <nl> ceph_destroy_options ( opt ); <nl> + kfree ( rbd_opts ); <nl>  <nl> /* using an existing client */ <nl> kref_get (& rbdc -> kref );
static inline void print_ipv6_addr ( struct audit_buffer * ab , <nl> char * name1 , char * name2 ) <nl> { <nl> if (! ipv6_addr_any ( addr )) <nl> - audit_log_format ( ab , " % s =% pI6 ", name1 , addr ); <nl> + audit_log_format ( ab , " % s =% pI6c ", name1 , addr ); <nl> if ( port ) <nl> audit_log_format ( ab , " % s =% d ", name2 , ntohs ( port )); <nl> }
static int __init ubi_mtd_param_parse ( const char * val , struct kernel_param * kp ) <nl> char * pbuf = & buf [ 0 ]; <nl> char * tokens [ 3 ] = { NULL , NULL , NULL }; <nl>  <nl> + if (! val ) <nl> + return - EINVAL ; <nl> + <nl> if ( mtd_devs == UBI_MAX_DEVICES ) { <nl> printk (" UBI error : too many parameters , max . is % d \ n ", <nl> UBI_MAX_DEVICES );
static ssize_t sn2_ptc_proc_write ( struct file * file , const char __user * user , si <nl> int cpu ; <nl> char optstr [ 64 ]; <nl>  <nl> + if ( count > sizeof ( optstr )) <nl> + return - EINVAL ; <nl> if ( copy_from_user ( optstr , user , count )) <nl> return - EFAULT ; <nl> optstr [ count - 1 ] = '\ 0 ';
void __cpuinit generic_processor_info ( int apicid , int version ) <nl> num_processors ++; <nl> cpu = cpumask_next_zero (- 1 , cpu_present_mask ); <nl>  <nl> + if ( version != apic_version [ boot_cpu_physical_apicid ]) <nl> + WARN_ONCE ( 1 , <nl> + " ACPI : apic version mismatch , bootcpu : % x cpu % d : % x \ n ", <nl> + apic_version [ boot_cpu_physical_apicid ], cpu , version ); <nl> + <nl> physid_set ( apicid , phys_cpu_present_map ); <nl> if ( apicid == boot_cpu_physical_apicid ) { <nl> /*
static struct sysfs_dirent * sysfs_new_dirent ( struct sysfs_dirent * parent_sd , <nl>  <nl> memset ( sd , 0 , sizeof (* sd )); <nl> atomic_set (& sd -> s_count , 1 ); <nl> - atomic_set (& sd -> s_event , 0 ); <nl> + atomic_set (& sd -> s_event , 1 ); <nl> INIT_LIST_HEAD (& sd -> s_children ); <nl> list_add (& sd -> s_sibling , & parent_sd -> s_children ); <nl> sd -> s_element = element ;
static int vidioc_s_register ( struct file * file , void * priv , <nl> 0x02 , <nl> ( u16 ) reg -> reg , 1 , <nl> value , 1 , 2 ); <nl> + break ; <nl> case 0x322 : <nl> ret = <nl> cx231xx_write_i2c_master ( dev ,
static int move_tasks ( struct rq * this_rq , int this_cpu , struct rq * busiest , <nl> max_load_move - total_load_moved , <nl> sd , idle , all_pinned , & this_best_prio ); <nl> class = class -> next ; <nl> + <nl> + if ( idle == CPU_NEWLY_IDLE && this_rq -> nr_running ) <nl> + break ; <nl> + <nl> } while ( class && max_load_move > total_load_moved ); <nl>  <nl> return total_load_moved > 0 ;
err_regulator_enable : <nl> err_regulator_get : <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8731 -> supplies ), wm8731 -> supplies ); <nl>  <nl> - kfree ( wm8731 ); <nl> return ret ; <nl> } <nl> 
static int hdsp_dds_offset ( struct hdsp * hdsp ) <nl> unsigned int dds_value = hdsp -> dds_value ; <nl> int system_sample_rate = hdsp -> system_sample_rate ; <nl>  <nl> + if (! dds_value ) <nl> + return 0 ; <nl> + <nl> n = DDS_NUMERATOR ; <nl> /* <nl> * dds_value = n / rate
static bool davinci_spi_can_dma ( struct spi_master * master , <nl>  <nl> if ( spicfg ) <nl> can_dma = ( spicfg -> io_type == SPI_IO_TYPE_DMA ) && <nl> - ( xfer -> len >= DMA_MIN_BYTES ); <nl> + ( xfer -> len >= DMA_MIN_BYTES ) && <nl> + ! is_vmalloc_addr ( xfer -> rx_buf ) && <nl> + ! is_vmalloc_addr ( xfer -> tx_buf ); <nl>  <nl> return can_dma ; <nl> }
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
enum stb0899_status stb0899_dvbs2_algo ( struct stb0899_state * state ) <nl> else <nl> internal -> inversion = IQ_SWAP_OFF ; <nl>  <nl> - offsetfreq *= internal -> inversion ; <nl> - <nl> - internal -> freq = internal -> freq - offsetfreq ; <nl> + internal -> freq = internal -> freq + offsetfreq ; <nl> internal -> srate = stb0899_dvbs2_get_srate ( state ); <nl>  <nl> reg = STB0899_READ_S2REG ( STB0899_S2DEMOD , UWP_STAT2 );
static int ad1836_register ( struct ad1836_priv * ad1836 ) <nl>  <nl> if ( ad1836_codec ) { <nl> dev_err ( codec -> dev , " Another ad1836 is registered \ n "); <nl> + kfree ( ad1836 ); <nl> return - EINVAL ; <nl> } <nl> 
static int get_sb_mtd_aux ( struct file_system_type * fs_type , int flags , <nl> DEBUG ( 1 , " MTDSB : New superblock for device % d (\"% s \")\ n ", <nl> mtd -> index , mtd -> name ); <nl>  <nl> + sb -> s_flags = flags ; <nl> + <nl> ret = fill_super ( sb , data , flags & MS_SILENT ? 1 : 0 ); <nl> if ( ret < 0 ) { <nl> up_write (& sb -> s_umount );
static int do_fault_around ( struct fault_env * fe , pgoff_t start_pgoff ) <nl>  <nl> if ( pmd_none (* fe -> pmd )) { <nl> fe -> prealloc_pte = pte_alloc_one ( fe -> vma -> vm_mm , fe -> address ); <nl> + if (! fe -> prealloc_pte ) <nl> + goto out ; <nl> smp_wmb (); /* See comment in __pte_alloc () */ <nl> } <nl> 
static int __init caam_rng_init ( void ) <nl> rng_ctx = kmalloc ( sizeof ( struct caam_rng_ctx ), GFP_DMA ); <nl> if (! rng_ctx ) <nl> return - ENOMEM ; <nl> - caam_init_rng (& rng_ctx , dev ); <nl> + caam_init_rng ( rng_ctx , dev ); <nl>  <nl> dev_info ( dev , " registering rng - caam \ n "); <nl> return hwrng_register (& caam_rng );
static int snd_timer_user_params ( struct file * file , <nl> if ( tu -> timeri -> flags & SNDRV_TIMER_IFLG_EARLY_EVENT ) { <nl> if ( tu -> tread ) { <nl> struct snd_timer_tread tread ; <nl> + memset (& tread , 0 , sizeof ( tread )); <nl> tread . event = SNDRV_TIMER_EVENT_EARLY ; <nl> tread . tstamp . tv_sec = 0 ; <nl> tread . tstamp . tv_nsec = 0 ;
static int create_trace_kprobe ( int argc , char ** argv ) <nl> pr_info (" Failed to parse symbol .\ n "); <nl> return ret ; <nl> } <nl> + if ( offset && is_return && <nl> + ! arch_function_offset_within_entry ( offset )) { <nl> + pr_info (" Given offset is not valid for return probe .\ n "); <nl> + return - EINVAL ; <nl> + } <nl> } <nl> argc -= 2 ; argv += 2 ; <nl> 
int main ( int argc , char ** argv ) <nl> } <nl> } <nl>  <nl> + if ( device_name == NULL ) <nl> + return - 1 ; <nl> + <nl> /* Find the device requested */ <nl> dev_num = find_type_by_name ( device_name , " device "); <nl> if ( dev_num < 0 ) {
*/ <nl> # define AT91_BASE_SYS 0xffffc000 <nl>  <nl> +# endif <nl> + <nl> /* <nl> * On sama5d4 there is no system controller , we map some needed peripherals <nl> */ <nl> # define AT91_ALT_BASE_SYS 0xfc069000 <nl> -# endif <nl>  <nl> /* <nl> * On all at91 have the Advanced Interrupt Controller starts at address <nl> */ <nl> # define AT91_IO_PHYS_BASE AT91_BASE_SYS <nl> # define AT91_IO_VIRT_BASE IOMEM ( AT91_IO_PHYS_BASE ) <nl> + <nl> +# define AT91_ALT_IO_PHYS_BASE AT91_ALT_BASE_SYS <nl> +# define AT91_ALT_IO_VIRT_BASE IOMEM ( AT91_ALT_BASE_SYS ) <nl> # endif <nl>  <nl> # define AT91_IO_SIZE ( 0xFFFFFFFF - AT91_IO_PHYS_BASE + 1 )
qla2xxx_queuecommand ( struct Scsi_Host * host , struct scsi_cmnd * cmd ) <nl> * Return target busy if we ' ve received a non - zero retry_delay_timer <nl> * in a FCP_RSP . <nl> */ <nl> - if ( time_after ( jiffies , fcport -> retry_delay_timestamp )) <nl> + if ( fcport -> retry_delay_timestamp == 0 ) { <nl> + /* retry delay not set */ <nl> + } else if ( time_after ( jiffies , fcport -> retry_delay_timestamp )) <nl> fcport -> retry_delay_timestamp = 0 ; <nl> else <nl> goto qc24_target_busy ;
static int edma_config_pset ( struct dma_chan * chan , struct edmacc_param * pset , <nl> int absync ; <nl>  <nl> acnt = dev_width ; <nl> + <nl> + /* src / dst_maxburst == 0 is the same case as src / dst_maxburst == 1 */ <nl> + if (! burst ) <nl> + burst = 1 ; <nl> /* <nl> * If the maxburst is equal to the fifo width , use <nl> * A - synced transfers . This allows for large contiguous
static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
int ocfs2_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> } <nl>  <nl> if ( size_change && attr -> ia_size != i_size_read ( inode )) { <nl> - if ( attr -> ia_size > sb -> s_maxbytes ) { <nl> - status = - EFBIG ; <nl> + status = inode_newsize_ok ( inode , attr -> ia_size ); <nl> + if ( status ) <nl> goto bail_unlock ; <nl> - } <nl>  <nl> if ( i_size_read ( inode ) > attr -> ia_size ) { <nl> if ( ocfs2_should_order_data ( inode )) {
static void nvmet_execute_rw ( struct nvmet_req * req ) <nl>  <nl> if ( req -> cmd -> rw . opcode == nvme_cmd_write ) { <nl> op = REQ_OP_WRITE ; <nl> + op_flags = WRITE_ODIRECT ; <nl> if ( req -> cmd -> rw . control & cpu_to_le16 ( NVME_RW_FUA )) <nl> op_flags |= REQ_FUA ; <nl> } else {
static void s6000_pcm_enqueue_dma ( struct snd_pcm_substream * substream ) <nl> return ; <nl> } <nl>  <nl> - BUG_ON ( period_size & 15 ); <nl> + if ( WARN_ON ( period_size & 15 )) <nl> + return ; <nl> s6dmac_put_fifo ( DMA_MASK_DMAC ( channel ), DMA_INDEX_CHNL ( channel ), <nl> src , dst , period_size ); <nl> 
int rxrpc_kernel_send_data ( struct socket * sock , struct rxrpc_call * call , <nl> ret = rxrpc_send_data ( rxrpc_sk ( sock -> sk ), call , msg , len ); <nl> break ; <nl> case RXRPC_CALL_COMPLETE : <nl> - /* It ' s too late for this call */ <nl> - ret = - ESHUTDOWN ; <nl> + read_lock_bh (& call -> state_lock ); <nl> + ret = - call -> error ; <nl> + read_unlock_bh (& call -> state_lock ); <nl> break ; <nl> default : <nl> /* Request phase complete for this client call */
int blkdev_ioctl ( struct block_device * bdev , fmode_t mode , unsigned cmd , <nl> * We need to set the startsect first , the driver may <nl> * want to override it . <nl> */ <nl> + memset (& geo , 0 , sizeof ( geo )); <nl> geo . start = get_start_sect ( bdev ); <nl> ret = disk -> fops -> getgeo ( bdev , & geo ); <nl> if ( ret )
module_exit ( visornic_cleanup ); <nl>  <nl> MODULE_AUTHOR (" Unisys "); <nl> MODULE_LICENSE (" GPL "); <nl> - MODULE_DESCRIPTION (" sPAR nic driver for sparlinux : ver 1 . 0 . 0 . 0 "); <nl> - MODULE_VERSION (" 1 . 0 . 0 . 0 "); <nl> + MODULE_DESCRIPTION (" sPAR nic driver for sparlinux ");
# include < libunwind . h > <nl> # include " perf_regs . h " <nl> # include "../../ util / unwind . h " <nl> +# include "../../ util / debug . h " <nl>  <nl> int libunwind__arch_reg_id ( int regnum ) <nl> {
cntrlEnd : <nl>  <nl> BCM_DEBUG_PRINT ( Adapter , DBG_TYPE_OTHERS , OSAL_DBG , DBG_LVL_ALL , " Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO \ n "); <nl>  <nl> + memset (& DevInfo , 0 , sizeof ( DevInfo )); <nl> DevInfo . MaxRDMBufferSize = BUFFER_4K ; <nl> DevInfo . u32DSDStartOffset = EEPROM_CALPARAM_START ; <nl> DevInfo . u32RxAlignmentCorrection = 0 ;
of_at91_clk_master_setup ( struct device_node * np , struct at91_pmc * pmc , <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> if (! irq ) <nl> - return ; <nl> + goto out_free_characteristics ; <nl>  <nl> clk = at91_clk_register_master ( pmc , irq , name , num_parents , <nl> parent_names , layout ,
err1 : <nl>  <nl> static void treo680_irda_shutdown ( struct device * dev ) <nl> { <nl> - gpio_free ( GPIO_NR_TREO680_AMP_EN ); <nl> + gpio_free ( GPIO_NR_TREO680_IR_EN ); <nl> } <nl>  <nl> static struct pxaficp_platform_data treo680_ficp_info = {
static int tegra_dma_probe ( struct platform_device * pdev ) <nl> if ( ret ) { <nl> dev_err (& pdev -> dev , <nl> " request_irq failed with err % d channel % d \ n ", <nl> - i , ret ); <nl> + ret , i ); <nl> goto err_irq ; <nl> } <nl> 
static int o2cb_cluster_check ( void ) <nl> set_bit ( node_num , netmap ); <nl> if (! memcmp ( hbmap , netmap , sizeof ( hbmap ))) <nl> return 0 ; <nl> - if ( i < O2CB_MAP_STABILIZE_COUNT ) <nl> + if ( i < O2CB_MAP_STABILIZE_COUNT - 1 ) <nl> msleep ( 1000 ); <nl> } <nl> 
static void rtl8169_init_phy ( struct net_device * dev , struct rtl8169_private * tp ) <nl> rtl8169_set_speed ( dev , AUTONEG_ENABLE , SPEED_1000 , DUPLEX_FULL , <nl> ADVERTISED_10baseT_Half | ADVERTISED_10baseT_Full | <nl> ADVERTISED_100baseT_Half | ADVERTISED_100baseT_Full | <nl> - tp -> mii . supports_gmii ? <nl> + ( tp -> mii . supports_gmii ? <nl> ADVERTISED_1000baseT_Half | <nl> - ADVERTISED_1000baseT_Full : 0 ); <nl> + ADVERTISED_1000baseT_Full : 0 )); <nl>  <nl> if ( RTL_R8 ( PHYstatus ) & TBI_Enable ) <nl> netif_info ( tp , link , dev , " TBI auto - negotiating \ n ");
int cdc_ncm_bind_common ( struct usbnet * dev , struct usb_interface * intf , u8 data_ <nl> u8 iface_no ; <nl>  <nl> ctx = kzalloc ( sizeof (* ctx ), GFP_KERNEL ); <nl> - if ( ctx == NULL ) <nl> - return - ENODEV ; <nl> + if (! ctx ) <nl> + return - ENOMEM ; <nl>  <nl> hrtimer_init (& ctx -> tx_timer , CLOCK_MONOTONIC , HRTIMER_MODE_REL ); <nl> ctx -> tx_timer . function = & cdc_ncm_tx_timer_cb ;
EXPORT_SYMBOL ( hwsw_unmap_sg ); <nl> EXPORT_SYMBOL ( hwsw_dma_supported ); <nl> EXPORT_SYMBOL ( hwsw_alloc_coherent ); <nl> EXPORT_SYMBOL ( hwsw_free_coherent ); <nl> + EXPORT_SYMBOL ( hwsw_sync_single_for_cpu ); <nl> + EXPORT_SYMBOL ( hwsw_sync_single_for_device ); <nl> + EXPORT_SYMBOL ( hwsw_sync_sg_for_cpu ); <nl> + EXPORT_SYMBOL ( hwsw_sync_sg_for_device );
static void irq_domain_disassociate_many ( struct irq_domain * domain , <nl> while ( count --) { <nl> int irq = irq_base + count ; <nl> struct irq_data * irq_data = irq_get_irq_data ( irq ); <nl> - irq_hw_number_t hwirq = irq_data -> hwirq ; <nl> + irq_hw_number_t hwirq ; <nl>  <nl> if ( WARN_ON (! irq_data || irq_data -> domain != domain )) <nl> continue ; <nl>  <nl> + hwirq = irq_data -> hwirq ; <nl> irq_set_status_flags ( irq , IRQ_NOREQUEST ); <nl>  <nl> /* remove chip and handler */
int cgroup_path ( const struct cgroup * cgrp , char * buf , int buflen ) <nl> return 0 ; <nl> } <nl>  <nl> - start = buf + buflen ; <nl> + start = buf + buflen - 1 ; <nl>  <nl> - *-- start = '\ 0 '; <nl> + * start = '\ 0 '; <nl> for (;;) { <nl> int len = dentry -> d_name . len ; <nl> 
nv134_chipset = { <nl> . name = " GP104 ", <nl> . bar = gf100_bar_new , <nl> . bios = nvkm_bios_new , <nl> + . bus = gf100_bus_new , <nl> . devinit = gm200_devinit_new , <nl> . fb = gp104_fb_new , <nl> . imem = nv50_instmem_new ,
static int pty_write ( struct tty_struct * tty , const unsigned char * buf , <nl>  <nl> static int pty_write_room ( struct tty_struct * tty ) <nl> { <nl> + if ( tty -> stopped ) <nl> + return 0 ; <nl> return pty_space ( tty -> link ); <nl> } <nl> 
static void free_migration ( struct dm_cache_migration * mg ) <nl> wake_up (& cache -> migration_wait ); <nl>  <nl> mempool_free ( mg , cache -> migration_pool ); <nl> - wake_worker ( cache ); <nl> } <nl>  <nl> static int prealloc_data_structs ( struct cache * cache , struct prealloc * p )
int xhci_halt ( struct xhci_hcd * xhci ) <nl> STS_HALT , STS_HALT , XHCI_MAX_HALT_USEC ); <nl> if (! ret ) <nl> xhci -> xhc_state |= XHCI_STATE_HALTED ; <nl> + else <nl> + xhci_warn ( xhci , " Host not halted after % u microseconds .\ n ", <nl> + XHCI_MAX_HALT_USEC ); <nl> return ret ; <nl> } <nl> 
static int dtl1_hci_send_frame ( struct sk_buff * skb ) <nl> nsh . len = skb -> len ; <nl>  <nl> s = bt_skb_alloc ( NSHL + skb -> len + 1 , GFP_ATOMIC ); <nl> + if (! s ) <nl> + return - ENOMEM ; <nl> + <nl> skb_reserve ( s , NSHL ); <nl> memcpy ( skb_put ( s , skb -> len ), skb -> data , skb -> len ); <nl> if ( skb -> len & 0x0001 )
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> if ( dev -> flags & IFF_SLAVE ) <nl> break ; <nl>  <nl> + if ( idev && idev -> cnf . disable_ipv6 ) <nl> + break ; <nl> + <nl> if ( event == NETDEV_UP ) { <nl> if (! addrconf_qdisc_ok ( dev )) { <nl> /* device is not ready yet . */
static void remove_event_file_dir ( struct ftrace_event_file * file ) <nl>  <nl> list_del (& file -> list ); <nl> remove_subsystem ( file -> system ); <nl> + free_event_filter ( file -> filter ); <nl> kmem_cache_free ( file_cachep , file ); <nl> } <nl> 
void __init tcp_tasklet_init ( void ) <nl> * We cant xmit new skbs from this context , as we might already <nl> * hold qdisc lock . <nl> */ <nl> - void tcp_wfree ( struct sk_buff * skb ) <nl> + static void tcp_wfree ( struct sk_buff * skb ) <nl> { <nl> struct sock * sk = skb -> sk ; <nl> struct tcp_sock * tp = tcp_sk ( sk );
struct gpio_chip * gpiochip_find ( void * data , <nl>  <nl> spin_lock_irqsave (& gpio_lock , flags ); <nl> list_for_each_entry ( gdev , & gpio_devices , list ) <nl> - if ( match ( gdev -> chip , data )) <nl> + if ( gdev -> chip && match ( gdev -> chip , data )) <nl> break ; <nl>  <nl> /* No match ? */
static int revoke_lo_scan_elements ( struct gfs2_jdesc * jd , unsigned int start , <nl> blkno = be64_to_cpu (*( __be64 *)( bh -> b_data + offset )); <nl>  <nl> error = gfs2_revoke_add ( sdp , blkno , start ); <nl> - if ( error < 0 ) <nl> + if ( error < 0 ) { <nl> + brelse ( bh ); <nl> return error ; <nl> + } <nl> else if ( error ) <nl> sdp -> sd_found_revokes ++; <nl> 
static void rtl8150_disconnect ( struct usb_interface * intf ) <nl> usb_set_intfdata ( intf , NULL ); <nl> if ( dev ) { <nl> set_bit ( RTL8150_UNPLUG , & dev -> flags ); <nl> - tasklet_disable (& dev -> tl ); <nl> tasklet_kill (& dev -> tl ); <nl> unregister_netdev ( dev -> netdev ); <nl> unlink_all_urbs ( dev );
v9fs_mux_rpc ( struct v9fs_mux_data * m , struct v9fs_fcall * tc , <nl> r . rcall || r . err ); <nl> } while (! r . rcall && ! r . err && err ==- ERESTARTSYS && <nl> m -> trans -> status == Connected && ! m -> err ); <nl> + <nl> + err = - ERESTARTSYS ; <nl> } <nl> sigpending = 1 ; <nl> }
int symbol__init ( void ) <nl> if ( symbol_conf . initialized ) <nl> return 0 ; <nl>  <nl> + symbol_conf . priv_size = ALIGN ( symbol_conf . priv_size , sizeof ( u64 )); <nl> + <nl> elf_version ( EV_CURRENT ); <nl> if ( symbol_conf . sort_by_name ) <nl> symbol_conf . priv_size += ( sizeof ( struct symbol_name_rb_node ) -
static int xfrm_get_policy ( struct sk_buff * skb , struct nlmsghdr * nlh , void ** xfr <nl> MSG_DONTWAIT ); <nl> } <nl> } else { <nl> + c . data . byid = p -> index ; <nl> c . event = XFRM_SAP_DELETED ; <nl> c . seq = nlh -> nlmsg_seq ; <nl> c . pid = nlh -> nlmsg_pid ;
struct vpif_disp_buffer { <nl> }; <nl>  <nl> struct common_obj { <nl> - /* Buffer specific parameters */ <nl> - u8 * fbuffers [ VIDEO_MAX_FRAME ]; /* List of buffer pointers for <nl> - * storing frames */ <nl> u32 numbuffers ; /* number of buffers */ <nl> struct vpif_disp_buffer * cur_frm ; /* Pointer pointing to current <nl> * vb2_buffer */
void free_reloc_roots ( struct list_head * list ) <nl> while (! list_empty ( list )) { <nl> reloc_root = list_entry ( list -> next , struct btrfs_root , <nl> root_list ); <nl> + free_extent_buffer ( reloc_root -> node ); <nl> + free_extent_buffer ( reloc_root -> commit_root ); <nl> + reloc_root -> node = NULL ; <nl> + reloc_root -> commit_root = NULL ; <nl> __del_reloc_root ( reloc_root ); <nl> } <nl> }
int cfhsi_probe ( struct platform_device * pdev ) <nl> dev_err (& ndev -> dev , "% s : Registration error : % d .\ n ", <nl> __func__ , res ); <nl> free_netdev ( ndev ); <nl> + return - ENODEV ; <nl> } <nl> /* Add CAIF HSI device to list . */ <nl> spin_lock (& cfhsi_list_lock );
drm_display_mode_from_vic_index ( struct drm_connector * connector , <nl> return NULL ; <nl>  <nl> newmode = drm_mode_duplicate ( dev , & edid_cea_modes [ cea_mode ]); <nl> + if (! newmode ) <nl> + return NULL ; <nl> + <nl> newmode -> vrefresh = 0 ; <nl>  <nl> return newmode ;
xfs_check_page_type ( <nl> if ( type == XFS_IO_UNWRITTEN ) <nl> return true ; <nl> } else if ( buffer_delay ( bh )) { <nl> - if ( type == XFS_IO_DELALLOC ); <nl> + if ( type == XFS_IO_DELALLOC ) <nl> return true ; <nl> } else if ( buffer_dirty ( bh ) && buffer_mapped ( bh )) { <nl> - if ( type == XFS_IO_OVERWRITE ); <nl> + if ( type == XFS_IO_OVERWRITE ) <nl> return true ; <nl> } <nl> 
int trace_get_user ( struct trace_parser * parser , const char __user * ubuf , <nl>  <nl> /* read the non - space input */ <nl> while ( cnt && ! isspace ( ch )) { <nl> - if ( parser -> idx < parser -> size ) <nl> + if ( parser -> idx < parser -> size - 1 ) <nl> parser -> buffer [ parser -> idx ++] = ch ; <nl> else { <nl> ret = - EINVAL ;
static int s5h1420_send_master_cmd ( struct dvb_frontend * fe , <nl> int result = 0 ; <nl>  <nl> dprintk (" enter % s \ n ", __func__ ); <nl> - if ( cmd -> msg_len > 8 ) <nl> + if ( cmd -> msg_len > sizeof ( cmd -> msg )) <nl> return - EINVAL ; <nl>  <nl> /* setup for DISEQC */
static int tmio_probe ( struct platform_device * dev ) <nl> nand_chip = & tmio -> chip ; <nl> mtd -> priv = nand_chip ; <nl> mtd -> name = " tmio - nand "; <nl> + mtd -> dev . parent = & dev -> dev ; <nl>  <nl> tmio -> ccr = devm_ioremap (& dev -> dev , ccr -> start , resource_size ( ccr )); <nl> if (! tmio -> ccr )
static void sky2_power_aux ( struct sky2_hw * hw ) <nl> Y2_CLK_GAT_LNK1_DIS | Y2_PCI_CLK_LNK2_DIS | <nl> Y2_COR_CLK_LNK2_DIS | Y2_CLK_GAT_LNK2_DIS ); <nl>  <nl> - /* switch power to VAUX */ <nl> - if ( sky2_read32 ( hw , B0_CTST ) & Y2_VAUX_AVAIL ) <nl> + /* switch power to VAUX if supported and PME from D3cold */ <nl> + if ( ( sky2_read32 ( hw , B0_CTST ) & Y2_VAUX_AVAIL ) && <nl> + pci_pme_capable ( hw -> pdev , PCI_D3cold )) <nl> sky2_write8 ( hw , B0_POWER_CTRL , <nl> ( PC_VAUX_ENA | PC_VCC_ENA | <nl> PC_VAUX_ON | PC_VCC_OFF ));
static int __devinit iic_probe ( struct ocp_device * ocp ){ <nl> strcpy ( adap -> name , " IBM IIC "); <nl> i2c_set_adapdata ( adap , dev ); <nl> adap -> id = I2C_HW_OCP ; <nl> + adap -> class = I2C_CLASS_HWMON ; <nl> adap -> algo = & iic_algo ; <nl> adap -> client_register = NULL ; <nl> adap -> client_unregister = NULL ;
static int tty_fasync ( int fd , struct file * filp , int on ) <nl> pid = task_pid ( current ); <nl> type = PIDTYPE_PID ; <nl> } <nl> - spin_unlock_irqrestore (& tty -> ctrl_lock , flags ); <nl> retval = __f_setown ( filp , pid , type , 0 ); <nl> + spin_unlock_irqrestore (& tty -> ctrl_lock , flags ); <nl> if ( retval ) <nl> goto out ; <nl> } else {
static const struct ethtool_ops dnet_ethtool_ops = { <nl> . set_settings = dnet_set_settings , <nl> . get_drvinfo = dnet_get_drvinfo , <nl> . get_link = ethtool_op_get_link , <nl> + . get_ts_info = ethtool_op_get_ts_info , <nl> }; <nl>  <nl> static const struct net_device_ops dnet_netdev_ops = {
int mesh_allocated ; <nl> static struct kmem_cache * rm_cache ; <nl>  <nl> -# ifdef CONFIG_MAC80211_MESH <nl> bool mesh_action_is_path_sel ( struct ieee80211_mgmt * mgmt ) <nl> { <nl> return ( mgmt -> u . action . u . mesh_action . action_code == <nl> WLAN_MESH_ACTION_HWMP_PATH_SELECTION ); <nl> } <nl> -# else <nl> - bool mesh_action_is_path_sel ( struct ieee80211_mgmt * mgmt ) <nl> -{ return false ; } <nl> -# endif <nl>  <nl> void ieee80211s_init ( void ) <nl> {
retry : <nl> } <nl>  <nl> /* Check info buffer */ <nl> - info = ( void *)& msg [ 1 ]; <nl> + info = ( void *)& bcdc -> buf [ 0 ]; <nl>  <nl> /* Copy info buffer */ <nl> if ( buf ) {
static const struct of_device_id regulator_haptic_dt_match [] = { <nl> { . compatible = " regulator - haptic " }, <nl> { /* sentinel */ }, <nl> }; <nl> + MODULE_DEVICE_TABLE ( of , regulator_haptic_dt_match ); <nl>  <nl> static struct platform_driver regulator_haptic_driver = { <nl> . probe = regulator_haptic_probe ,
/* <nl> * Literals <nl> */ <nl> -# define IPR_DRIVER_VERSION " 2 . 6 . 1 " <nl> -# define IPR_DRIVER_DATE "( March 12 , 2015 )" <nl> +# define IPR_DRIVER_VERSION " 2 . 6 . 2 " <nl> +# define IPR_DRIVER_DATE "( June 11 , 2015 )" <nl>  <nl> /* <nl> * IPR_MAX_CMD_PER_LUN : This defines the maximum number of outstanding
static int __devinit sta2x11_mfd_probe ( struct pci_dev * pdev , <nl> sta2x11_mfd_setup ( pdev , setup_data ); <nl>  <nl> /* Record this pdev before mfd_add_devices : their probe looks for it */ <nl> - sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl> + if (! sta2x11_mfd_find ( pdev )) <nl> + sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl>  <nl> /* Just 2 bars for all mfd ' s at present */ <nl> for ( i = 0 ; i < 2 ; i ++) {
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
static int pfkey_spdadd ( struct sock * sk , struct sk_buff * skb , struct sadb_msg * h <nl> return 0 ; <nl>  <nl> out : <nl> + xp -> dead = 1 ; <nl> xfrm_policy_destroy ( xp ); <nl> return err ; <nl> }
i915_gem_object_pin ( struct drm_i915_gem_object * obj , <nl> uint32_t alignment , <nl> unsigned flags ) <nl> { <nl> + struct drm_i915_private * dev_priv = obj -> base . dev -> dev_private ; <nl> struct i915_vma * vma ; <nl> int ret ; <nl>  <nl> + if ( WARN_ON ( vm == & dev_priv -> mm . aliasing_ppgtt -> base )) <nl> + return - ENODEV ; <nl> + <nl> if ( WARN_ON ( flags & ( PIN_GLOBAL | PIN_MAPPABLE ) && ! i915_is_ggtt ( vm ))) <nl> return - EINVAL ; <nl> 
static void * raid0_takeover_raid1 ( mddev_t * mddev ) <nl> mddev -> new_layout = 0 ; <nl> mddev -> new_chunk_sectors = 128 ; /* by default set chunk size to 64k */ <nl> mddev -> delta_disks = 1 - mddev -> raid_disks ; <nl> + mddev -> raid_disks = 1 ; <nl> /* make sure it will be not marked as dirty */ <nl> mddev -> recovery_cp = MaxSector ; <nl> 
static int nokia_modem_probe ( struct device * dev ) <nl> dev_set_drvdata ( dev , modem ); <nl>  <nl> irq = irq_of_parse_and_map ( np , 0 ); <nl> - if ( irq < 0 ) { <nl> + if (! irq ) { <nl> dev_err ( dev , " Invalid rst_ind interrupt (% d )\ n ", irq ); <nl> - return irq ; <nl> + return - EINVAL ; <nl> } <nl> modem -> nokia_modem_rst_ind_irq = irq ; <nl> pflags = irq_get_trigger_type ( irq );
TRACE_EVENT ( task_rename , <nl> TP_fast_assign ( <nl> __entry -> pid = task -> pid ; <nl> memcpy ( entry -> oldcomm , task -> comm , TASK_COMM_LEN ); <nl> - memcpy ( entry -> newcomm , comm , TASK_COMM_LEN ); <nl> + strlcpy ( entry -> newcomm , comm , TASK_COMM_LEN ); <nl> __entry -> oom_score_adj = task -> signal -> oom_score_adj ; <nl> ), <nl> 
vector_setup_out : <nl> **/ <nl> static struct i40e_vsi * i40e_vsi_reinit_setup ( struct i40e_vsi * vsi ) <nl> { <nl> - struct i40e_pf * pf = vsi -> back ; <nl> + struct i40e_pf * pf ; <nl> u8 enabled_tc ; <nl> int ret ; <nl>  <nl> + if (! vsi ) <nl> + return NULL ; <nl> + <nl> + pf = vsi -> back ; <nl> + <nl> i40e_put_lump ( pf -> qp_pile , vsi -> base_queue , vsi -> idx ); <nl> i40e_vsi_clear_rings ( vsi ); <nl> 
static int __ixgbe_shutdown ( struct pci_dev * pdev , bool * enable_wake ) <nl>  <nl> netif_device_detach ( netdev ); <nl>  <nl> + rtnl_lock (); <nl> if ( netif_running ( netdev )) { <nl> - rtnl_lock (); <nl> ixgbe_down ( adapter ); <nl> ixgbe_free_irq ( adapter ); <nl> ixgbe_free_all_tx_resources ( adapter ); <nl> ixgbe_free_all_rx_resources ( adapter ); <nl> - rtnl_unlock (); <nl> } <nl> + rtnl_unlock (); <nl>  <nl> ixgbe_clear_interrupt_scheme ( adapter ); <nl> 
int __init ks_dw_pcie_host_init ( struct keystone_pcie * ks_pcie , <nl>  <nl> /* Index 0 is the config reg . space address */ <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> - pci -> dbi_base = devm_ioremap_resource ( dev , res ); <nl> + pci -> dbi_base = devm_pci_remap_cfg_resource ( dev , res ); <nl> if ( IS_ERR ( pci -> dbi_base )) <nl> return PTR_ERR ( pci -> dbi_base ); <nl> 
process_filter ( struct event_format * event , struct filter_arg ** parg , <nl> * parg = current_op ; <nl> else <nl> * parg = current_exp ; <nl> + free ( token ); <nl> return PEVENT_ERRNO__UNBALANCED_PAREN ; <nl> } <nl> break ; <nl> process_filter ( struct event_format * event , struct filter_arg ** parg , <nl>  <nl> * parg = current_op ; <nl>  <nl> + free ( token ); <nl> return 0 ; <nl>  <nl> fail_alloc :
static noinline void init_btrfs_i ( struct inode * inode ) <nl> bi -> flags = 0 ; <nl> bi -> index_cnt = ( u64 )- 1 ; <nl> bi -> last_unlink_trans = 0 ; <nl> + bi -> ordered_data_close = 0 ; <nl> extent_map_tree_init (& BTRFS_I ( inode )-> extent_tree , GFP_NOFS ); <nl> extent_io_tree_init (& BTRFS_I ( inode )-> io_tree , <nl> inode -> i_mapping , GFP_NOFS );
static int bcm2835_sdhci_probe ( struct platform_device * pdev ) <nl> goto err ; <nl> } <nl>  <nl> - return sdhci_add_host ( host ); <nl> + ret = sdhci_add_host ( host ); <nl> + if ( ret ) <nl> + goto err ; <nl>  <nl> + return 0 ; <nl> err : <nl> sdhci_pltfm_free ( pdev ); <nl> return ret ;
void blk_mq_free_request ( struct request * rq ) <nl> hctx = q -> mq_ops -> map_queue ( q , ctx -> cpu ); <nl> __blk_mq_free_request ( hctx , ctx , rq ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( blk_mq_free_request ); <nl>  <nl> inline void __blk_mq_end_request ( struct request * rq , int error ) <nl> {
static int ibmvfc_map_sg_data ( struct scsi_cmnd * scmd , <nl> & evt -> ext_list_token ); <nl>  <nl> if (! evt -> ext_list ) { <nl> - scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> + scsi_dma_unmap ( scmd ); <nl> + if ( vhost -> log_level > IBMVFC_DEFAULT_LOG_LEVEL ) <nl> + scmd_printk ( KERN_ERR , scmd , " Can ' t allocate memory for scatterlist \ n "); <nl> return - ENOMEM ; <nl> } <nl> }
static int osc_extent_wait ( const struct lu_env * env , struct osc_extent * ext , <nl> "% s : wait ext to % d timedout , recovery in progress ?\ n ", <nl> osc_export ( obj )-> exp_obd -> obd_name , state ); <nl>  <nl> - lwi = LWI_INTR ( LWI_ON_SIGNAL_NOOP , NULL ); <nl> + lwi = LWI_INTR ( NULL , NULL ); <nl> rc = l_wait_event ( ext -> oe_waitq , extent_wait_cb ( ext , state ), <nl> & lwi ); <nl> }
static void dce3_2_afmt_write_sad_regs ( struct drm_encoder * encoder ) <nl> } <nl>  <nl> sad_count = drm_edid_to_sad ( radeon_connector -> edid , & sads ); <nl> - if ( sad_count < 0 ) { <nl> + if ( sad_count <= 0 ) { <nl> DRM_ERROR (" Couldn ' t read SADs : % d \ n ", sad_count ); <nl> return ; <nl> }
static int doc_write_oob ( struct mtd_info * mtd , loff_t ofs , <nl> oobdelta = mtd -> ecclayout -> oobavail ; <nl> break ; <nl> default : <nl> - oobdelta = 0 ; <nl> + return - EINVAL ; <nl> } <nl> if (( len % DOC_LAYOUT_PAGE_SIZE ) || ( ooblen % oobdelta ) || <nl> ( ofs % DOC_LAYOUT_PAGE_SIZE ))
static int gp8psk_fe_read_signal_strength ( struct dvb_frontend * fe , u16 * strength <nl>  <nl> static int gp8psk_fe_get_tune_settings ( struct dvb_frontend * fe , struct dvb_frontend_tune_settings * tune ) <nl> { <nl> - tune -> min_delay_ms = 200 ; <nl> + tune -> min_delay_ms = 800 ; <nl> return 0 ; <nl> } <nl> 
static int brcmf_sdbrcm_write_vars ( struct brcmf_sdio * bus ) <nl> /* Verify NVRAM bytes */ <nl> brcmf_dbg ( INFO , " Compare NVRAM dl & ul ; varsize =% d \ n ", varsize ); <nl> nvram_ularray = kmalloc ( varsize , GFP_ATOMIC ); <nl> - if (! nvram_ularray ) <nl> + if (! nvram_ularray ) { <nl> + kfree ( vbuffer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> /* Upload image to verify downloaded contents . */ <nl> memset ( nvram_ularray , 0xaa , varsize );
spider_net_prepare_rx_descr ( struct spider_net_card * card , <nl> /* and we need to have it 128 byte aligned , therefore we allocate a <nl> * bit more */ <nl> /* allocate an skb */ <nl> - descr -> skb = dev_alloc_skb ( bufsize + SPIDER_NET_RXBUF_ALIGN - 1 ); <nl> + descr -> skb = netdev_alloc_skb ( card -> netdev , <nl> + bufsize + SPIDER_NET_RXBUF_ALIGN - 1 ); <nl> if (! descr -> skb ) { <nl> if ( netif_msg_rx_err ( card ) && net_ratelimit ()) <nl> pr_err (" Not enough memory to allocate rx buffer \ n ");
trace_print_graph_duration ( unsigned long long duration , struct trace_seq * s ) <nl>  <nl> /* Print nsecs ( we don ' t want to exceed 7 numbers ) */ <nl> if ( len < 7 ) { <nl> - snprintf ( nsecs_str , 8 - len , "% 03lu ", nsecs_rem ); <nl> + snprintf ( nsecs_str , min ( sizeof ( nsecs_str ), 8UL - len ), "% 03lu ", <nl> + nsecs_rem ); <nl> ret = trace_seq_printf ( s , ".% s ", nsecs_str ); <nl> if (! ret ) <nl> return TRACE_TYPE_PARTIAL_LINE ;
static void __init request_standard_resources ( void ) <nl> res = alloc_bootmem_low ( sizeof (* res )); <nl> if ( memblock_is_nomap ( region )) { <nl> res -> name = " reserved "; <nl> - res -> flags = IORESOURCE_MEM | IORESOURCE_BUSY ; <nl> + res -> flags = IORESOURCE_MEM ; <nl> } else { <nl> res -> name = " System RAM "; <nl> res -> flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY ;
static ssize_t display_upd_mode_store ( struct device * dev , <nl> int val , r ; <nl> enum omap_dss_update_mode mode ; <nl>  <nl> + if (! dssdev -> driver -> set_update_mode ) <nl> + return - EINVAL ; <nl> + <nl> val = simple_strtoul ( buf , NULL , 10 ); <nl>  <nl> switch ( val ) {
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> - err = do_migrate_pages ( mm , & old , & new , MPOL_MF_MOVE ); <nl> + err = do_migrate_pages ( mm , & old , & new , <nl> + capable ( CAP_SYS_ADMIN ) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE ); <nl> out : <nl> mmput ( mm ); <nl> return err ;
jme_alloc_and_feed_skb ( struct jme_adapter * jme , int idx ) <nl> jme -> jme_vlan_rx ( skb , jme -> vlgrp , <nl> le16_to_cpu ( rxdesc -> descwb . vlan )); <nl> NET_STAT ( jme ). rx_bytes += 4 ; <nl> + } else { <nl> + dev_kfree_skb ( skb ); <nl> } <nl> } else { <nl> jme -> jme_rx ( skb );
static int macvtap_ioctl_set_queue ( struct file * file , unsigned int flags ) <nl> ret = macvtap_enable_queue ( vlan -> dev , file , q ); <nl> else if ( flags & IFF_DETACH_QUEUE ) <nl> ret = macvtap_disable_queue ( q ); <nl> + else <nl> + ret = - EINVAL ; <nl>  <nl> macvtap_put_vlan ( vlan ); <nl> return ret ;
int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
struct device ; <nl>  <nl> enum led_brightness { <nl> LED_OFF = 0 , <nl> + LED_ON = 1 , <nl> LED_HALF = 127 , <nl> LED_FULL = 255 , <nl> };
__acquires (& pool -> lock ) <nl> * kernels , where a requeueing work item waiting for something to <nl> * happen could deadlock with stop_machine as such work item could <nl> * indefinitely requeue itself while all other CPUs are trapped in <nl> - * stop_machine . <nl> + * stop_machine . At the same time , report a quiescent RCU state so <nl> + * the same condition doesn ' t freeze RCU . <nl> */ <nl> + rcu_note_voluntary_context_switch ( current ); <nl> cond_resched (); <nl>  <nl> spin_lock_irq (& pool -> lock );
ssize_t tcp_splice_read ( struct socket * sock , loff_t * ppos , <nl> ssize_t spliced ; <nl> int ret ; <nl>  <nl> + sock_rps_record_flow ( sk ); <nl> /* <nl> * We can ' t seek on a socket input <nl> */
static void raid10d ( mddev_t * mddev ) <nl> sl --; <nl> d = r10_bio -> devs [ sl ]. devnum ; <nl> rdev = conf -> mirrors [ d ]. rdev ; <nl> - atomic_add ( s , & rdev -> corrected_errors ); <nl> if ( rdev && <nl> test_bit ( In_sync , & rdev -> flags )) { <nl> + atomic_add ( s , & rdev -> corrected_errors ); <nl> if ( sync_page_io ( rdev -> bdev , <nl> r10_bio -> devs [ sl ]. addr + <nl> sect + rdev -> data_offset ,
static ssize_t btrfs_label_store ( struct kobject * kobj , <nl> int ret ; <nl> size_t p_len ; <nl>  <nl> + if ( fs_info -> sb -> s_flags & MS_RDONLY ) <nl> + return - EROFS ; <nl> + <nl> /* <nl> * p_len is the len until the first occurrence of either <nl> * '\ n ' or '\ 0 '
static int arche_platform_probe ( struct platform_device * pdev ) <nl> arche_pdata -> wake_detect_gpio = of_get_named_gpio ( np , " svc , wake - detect - gpio ", 0 ); <nl> if ( arche_pdata -> wake_detect_gpio < 0 ) { <nl> dev_err ( dev , " failed to get wake detect gpio \ n "); <nl> - ret = arche_pdata -> wake_detect_gpio ; <nl> - return ret ; <nl> + return arche_pdata -> wake_detect_gpio ; <nl> } <nl>  <nl> ret = devm_gpio_request ( dev , arche_pdata -> wake_detect_gpio , " wake detect ");
static int dspi_request_dma ( struct fsl_dspi * dspi , phys_addr_t phy_addr ) <nl> return 0 ; <nl>  <nl> err_slave_config : <nl> - devm_kfree ( dev , dma -> rx_dma_buf ); <nl> + dma_free_coherent ( dev , DSPI_DMA_BUFSIZE , <nl> + dma -> rx_dma_buf , dma -> rx_dma_phys ); <nl> err_rx_dma_buf : <nl> - devm_kfree ( dev , dma -> tx_dma_buf ); <nl> + dma_free_coherent ( dev , DSPI_DMA_BUFSIZE , <nl> + dma -> tx_dma_buf , dma -> tx_dma_phys ); <nl> err_tx_dma_buf : <nl> dma_release_channel ( dma -> chan_tx ); <nl> err_tx_channel :
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
int test_range_bit ( struct extent_io_tree * tree , u64 start , u64 end , <nl> bitset = 0 ; <nl> break ; <nl> } <nl> + <nl> + if ( state -> end == ( u64 )- 1 ) <nl> + break ; <nl> + <nl> start = state -> end + 1 ; <nl> if ( start > end ) <nl> break ;
static int fsl_pq_mdio_probe ( struct of_device * ofdev , <nl> dev_set_drvdata (& ofdev -> dev , new_bus ); <nl>  <nl> if ( of_device_is_compatible ( np , " fsl , gianfar - mdio ") || <nl> + of_device_is_compatible ( np , " fsl , gianfar - tbi ") || <nl> of_device_is_compatible ( np , " gianfar ")) { <nl> # ifdef CONFIG_GIANFAR <nl> tbipa = get_gfar_tbipa ( regs );
int serial8250_em485_init ( struct uart_8250_port * p ) <nl> if ( p -> em485 != NULL ) <nl> return 0 ; <nl>  <nl> - p -> em485 = kmalloc ( sizeof ( struct uart_8250_em485 ), GFP_KERNEL ); <nl> + p -> em485 = kmalloc ( sizeof ( struct uart_8250_em485 ), GFP_ATOMIC ); <nl> if ( p -> em485 == NULL ) <nl> return - ENOMEM ; <nl> 
extern void numa_initmem_init ( unsigned long start_pfn , unsigned long end_pfn ); <nl> extern unsigned long numa_free_all_bootmem ( void ); <nl>  <nl> extern void reserve_bootmem_generic ( unsigned long phys , unsigned len ); <nl> - extern void free_bootmem_generic ( unsigned long phys , unsigned len ); <nl>  <nl> extern void load_gs_index ( unsigned gs ); <nl> 
void usbip_stop_eh ( struct usbip_device * ud ) <nl> { <nl> struct usbip_task * eh = & ud -> eh ; <nl>  <nl> + if ( eh -> thread == current ) <nl> + return ; /* do not wait for myself */ <nl> + <nl> wait_for_completion (& eh -> thread_done ); <nl> usbip_dbg_eh (" usbip_eh has finished \ n "); <nl> }
static s32 brcmf_p2p_run_escan ( struct brcmf_cfg80211_info * cfg , <nl> } <nl> err = brcmf_p2p_escan ( p2p , num_nodfs , chanspecs , search_state , <nl> action , P2PAPI_BSSCFG_DEVICE ); <nl> + kfree ( chanspecs ); <nl> } <nl> exit : <nl> if ( err )
void host1x_set_drm_data ( struct device * dev , void * data ) <nl> void * host1x_get_drm_data ( struct device * dev ) <nl> { <nl> struct host1x * host1x = dev_get_drvdata ( dev ); <nl> - return host1x -> drm_data ; <nl> + return host1x ? host1x -> drm_data : NULL ; <nl> } <nl>  <nl> void host1x_sync_writel ( struct host1x * host1x , u32 v , u32 r )
ibx_get_dpll ( struct intel_crtc * crtc , struct intel_crtc_state * crtc_state , <nl> DPLL_ID_PCH_PLL_B ); <nl> } <nl>  <nl> + if (! pll ) <nl> + return NULL ; <nl> + <nl> /* reference the pll */ <nl> intel_reference_shared_dpll ( pll , crtc_state ); <nl> 
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
static int ocfs2_truncate_file ( struct inode * inode , <nl> ( unsigned long long ) OCFS2_I ( inode )-> ip_blkno , <nl> ( unsigned long long ) new_i_size ); <nl>  <nl> + unmap_mapping_range ( inode -> i_mapping , new_i_size + PAGE_SIZE - 1 , 0 , 1 ); <nl> truncate_inode_pages ( inode -> i_mapping , new_i_size ); <nl>  <nl> fe = ( struct ocfs2_dinode *) di_bh -> b_data ;
int switch_ssc_clock ( struct rtsx_chip * chip , int clk ) <nl> return STATUS_FAIL ; <nl> } <nl>  <nl> - mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> + mcu_cnt = ( u8 )( 125 / clk + 3 ); <nl> if ( mcu_cnt > 7 ) <nl> mcu_cnt = 7 ; <nl> 
void __iomem * __arm_ioremap_pfn_caller ( unsigned long pfn , <nl> if (! area ) <nl> return NULL ; <nl> addr = ( unsigned long ) area -> addr ; <nl> + area -> phys_addr = __pfn_to_phys ( pfn ); <nl>  <nl> # if ! defined ( CONFIG_SMP ) && ! defined ( CONFIG_ARM_LPAE ) <nl> if ( DOMAIN_IO == 0 &&
static int run ( mddev_t * mddev ) <nl> int i ; <nl>  <nl> conf_t * conf = kmalloc ( sizeof (* conf ), GFP_KERNEL ); <nl> + if (! conf ) <nl> + return - ENOMEM ; <nl>  <nl> for ( i = 0 ; i < Modes ; i ++) { <nl> atomic_set (& conf -> counters [ i ], 0 );
static void iwl_mvm_stat_iterator ( void * _data , u8 * mac , <nl> if ( vif -> type != NL80211_IFTYPE_STATION ) <nl> return ; <nl>  <nl> + if ( sig == 0 ) { <nl> + IWL_DEBUG_RX ( mvm , " RSSI is 0 - skip signal based decision \ n "); <nl> + return ; <nl> + } <nl> + <nl> mvmvif -> bf_data . ave_beacon_signal = sig ; <nl>  <nl> /* BT Coex */
static int qla4xxx_fw_ready ( struct scsi_qla_host * ha ) <nl> DEBUG2 ( printk (" scsi % ld : % s : FW initialized , but " <nl> " auto - discovery still in process \ n ", <nl> ha -> host_no , __func__ )); <nl> + ready = 1 ; <nl> } <nl>  <nl> return ready ;
static void __cpuinit put_core_offline ( unsigned int cpu ) <nl>  <nl> indx = TO_ATTR_NO ( cpu ); <nl>  <nl> + /* The core id is too big , just return */ <nl> + if ( indx > MAX_CORE_DATA - 1 ) <nl> + return ; <nl> + <nl> if ( pdata -> core_data [ indx ] && pdata -> core_data [ indx ]-> cpu == cpu ) <nl> coretemp_remove_core ( pdata , & pdev -> dev , indx ); <nl> 
static int sysfs_get_sb ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb ) || sb -> s_fs_info != info ) <nl> kfree ( info ); <nl> if ( IS_ERR ( sb )) { <nl> - kfree ( info ); <nl> error = PTR_ERR ( sb ); <nl> goto out ; <nl> }
static int coredump_wait ( int exit_code , struct core_state * core_state ) <nl> core_state -> dumper . task = tsk ; <nl> core_state -> dumper . next = NULL ; <nl>  <nl> - down_write (& mm -> mmap_sem ); <nl> + if ( down_write_killable (& mm -> mmap_sem )) <nl> + return - EINTR ; <nl> + <nl> if (! mm -> core_state ) <nl> core_waiters = zap_threads ( tsk , mm , core_state , exit_code ); <nl> up_write (& mm -> mmap_sem );
static struct usbip_imported_device * imported_device_init ( struct usbip_imported_ <nl> goto err ; <nl>  <nl> memcpy ( new_cdev , cdev , sizeof (* new_cdev )); <nl> - dlist_unshift ( idev -> cdev_list , ( void *) new_cdev ); <nl> + dlist_unshift ( idev -> cdev_list , ( void *) new_cdev ); <nl> } <nl> } <nl> 
static int snd_hdsp_hwdep_ioctl ( struct snd_hwdep * hw , struct file * file , unsigne <nl> if (( err = hdsp_get_iobox_version ( hdsp )) < 0 ) <nl> return err ; <nl> } <nl> + memset (& hdsp_version , 0 , sizeof ( hdsp_version )); <nl> hdsp_version . io_type = hdsp -> io_type ; <nl> hdsp_version . firmware_rev = hdsp -> firmware_rev ; <nl> if (( err = copy_to_user ( argp , & hdsp_version , sizeof ( hdsp_version ))))
static bool attempt_plug_merge ( struct request_queue * q , struct bio * bio , <nl> struct request * rq ; <nl> bool ret = false ; <nl>  <nl> + if ( blk_queue_nomerges ( q )) <nl> + goto out ; <nl> + <nl> plug = current -> plug ; <nl> if (! plug ) <nl> goto out ;
static long pnv_pci_ioda2_table_alloc_pages ( int nid , __u64 bus_offset , <nl> level_shift = entries_shift + 3 ; <nl> level_shift = max_t ( unsigned , level_shift , PAGE_SHIFT ); <nl>  <nl> + if (( level_shift - 3 ) * levels + page_shift >= 60 ) <nl> + return - EINVAL ; <nl> + <nl> /* Allocate TCE table */ <nl> addr = pnv_pci_ioda2_table_do_alloc_pages ( nid , level_shift , <nl> levels , tce_table_size , & offset , & total_allocated );
static void intel_agp_insert_sg_entries ( struct agp_memory * mem , <nl> off_t pg_start , int mask_type ) <nl> { <nl> int i , j ; <nl> + u32 cache_bits = 0 ; <nl> + <nl> + if ( agp_bridge -> dev -> device == PCI_DEVICE_ID_INTEL_SANDYBRIDGE_HB ) { <nl> + cache_bits = I830_PTE_SYSTEM_CACHED ; <nl> + } <nl>  <nl> for ( i = 0 , j = pg_start ; i < mem -> page_count ; i ++, j ++) { <nl> writel ( agp_bridge -> driver -> mask_memory ( agp_bridge ,
static int emac_link_differs ( struct emac_instance * dev ) <nl> static void emac_link_timer ( struct work_struct * work ) <nl> { <nl> struct emac_instance * dev = <nl> - container_of (( struct delayed_work *) work , <nl> + container_of ( to_delayed_work ( work ), <nl> struct emac_instance , link_work ); <nl> int link_poll_interval ; <nl> 
static struct tty_audit_buf * tty_audit_buf_alloc ( int major , int minor , <nl> { <nl> struct tty_audit_buf * buf ; <nl>  <nl> - buf = kmalloc ( sizeof (* buf ), GFP_KERNEL ); <nl> + buf = kmalloc ( sizeof (* buf ), GFP_KERNEL ); <nl> if (! buf ) <nl> goto err ; <nl> if ( PAGE_SIZE != N_TTY_BUF_SIZE )
static void fimd_dp_clock_enable ( struct exynos_drm_crtc * crtc , bool enable ) <nl> * clock . On these SoCs the bootloader may enable it but any <nl> * power domain off / on will reset it to disable state . <nl> */ <nl> - if ( ctx -> driver_data != & exynos5_fimd_driver_data || <nl> + if ( ctx -> driver_data != & exynos5_fimd_driver_data && <nl> ctx -> driver_data != & exynos5420_fimd_driver_data ) <nl> return ; <nl> 
MODULE_DEVICE_TABLE ( spi , mcp320x_id ); <nl> static struct spi_driver mcp320x_driver = { <nl> . driver = { <nl> . name = " mcp320x ", <nl> + . of_match_table = of_match_ptr ( mcp320x_dt_ids ), <nl> . owner = THIS_MODULE , <nl> }, <nl> . probe = mcp320x_probe ,
static int setup_blkring ( struct xenbus_device * dev , <nl> if ( err ) <nl> goto fail ; <nl>  <nl> - err = bind_evtchn_to_irqhandler ( info -> evtchn , <nl> - blkif_interrupt , <nl> - IRQF_SAMPLE_RANDOM , " blkif ", info ); <nl> + err = bind_evtchn_to_irqhandler ( info -> evtchn , blkif_interrupt , 0 , <nl> + " blkif ", info ); <nl> if ( err <= 0 ) { <nl> xenbus_dev_fatal ( dev , err , <nl> " bind_evtchn_to_irqhandler failed ");
static int mxs_gpio_set_wake_irq ( u32 irq , u32 enable ) <nl> } <nl>  <nl> static struct irq_chip gpio_irq_chip = { <nl> + . name = " mxs gpio ", <nl> . ack = mxs_gpio_ack_irq , <nl> . mask = mxs_gpio_mask_irq , <nl> . unmask = mxs_gpio_unmask_irq ,
struct sched_domain * build_sched_domain ( struct sched_domain_topology_level * tl , <nl> sd -> level = child -> level + 1 ; <nl> sched_domain_level_max = max ( sched_domain_level_max , sd -> level ); <nl> child -> parent = sd ; <nl> + sd -> child = child ; <nl> } <nl> - sd -> child = child ; <nl> set_domain_attribute ( sd , attr ); <nl>  <nl> return sd ;
static int s5m87xx_i2c_probe ( struct i2c_client * i2c , <nl> s5m87xx -> rtc = i2c_new_dummy ( i2c -> adapter , RTC_I2C_ADDR ); <nl> i2c_set_clientdata ( s5m87xx -> rtc , s5m87xx ); <nl>  <nl> - if ( pdata -> cfg_pmic_irq ) <nl> + if ( pdata && pdata -> cfg_pmic_irq ) <nl> pdata -> cfg_pmic_irq (); <nl>  <nl> s5m_irq_init ( s5m87xx );
static int zcache_comp_init ( void ) <nl> # else <nl> if (* zcache_comp_name != '\ 0 ') { <nl> ret = crypto_has_comp ( zcache_comp_name , 0 , 0 ); <nl> - if (! ret ) <nl> + if (! ret ) { <nl> pr_info (" zcache : % s not supported \ n ", <nl> zcache_comp_name ); <nl> - goto out ; <nl> + ret = 1 ; <nl> + goto out ; <nl> + } <nl> } <nl> if (! ret ) <nl> strcpy ( zcache_comp_name , " lzo ");
static int __devinit mwl8k_probe ( struct pci_dev * pdev , <nl> if ( rc ) { <nl> printk ( KERN_ERR "% s : Cannot register device \ n ", <nl> wiphy_name ( hw -> wiphy )); <nl> - goto err_free_irq ; <nl> + goto err_free_queues ; <nl> } <nl>  <nl> printk ( KERN_INFO "% s : % s v % d , % pM , % s firmware % u .% u .% u .% u \ n ",
put_clk : <nl>  <nl> static int g2d_remove ( struct platform_device * pdev ) <nl> { <nl> - struct g2d_dev * dev = ( struct g2d_dev *) platform_get_drvdata ( pdev ); <nl> + struct g2d_dev * dev = platform_get_drvdata ( pdev ); <nl>  <nl> v4l2_info (& dev -> v4l2_dev , " Removing " G2D_NAME ); <nl> v4l2_m2m_release ( dev -> m2m_dev );
static int wm8962_readable_register ( unsigned int reg ) <nl>  <nl> static int wm8962_reset ( struct snd_soc_codec * codec ) <nl> { <nl> - return snd_soc_write ( codec , WM8962_SOFTWARE_RESET , 0 ); <nl> + return snd_soc_write ( codec , WM8962_SOFTWARE_RESET , 0x6243 ); <nl> } <nl>  <nl> static const DECLARE_TLV_DB_SCALE ( inpga_tlv , - 2325 , 75 , 0 );
static void tcp_reinit_congestion_control ( struct sock * sk , <nl> icsk -> icsk_ca_ops = ca ; <nl> icsk -> icsk_ca_setsockopt = 1 ; <nl>  <nl> - if ( sk -> sk_state != TCP_CLOSE ) <nl> + if ( sk -> sk_state != TCP_CLOSE ) { <nl> + memset ( icsk -> icsk_ca_priv , 0 , sizeof ( icsk -> icsk_ca_priv )); <nl> tcp_init_congestion_control ( sk ); <nl> + } <nl> } <nl>  <nl> /* Manage refcounts on socket close . */
static int xc5000_release ( struct dvb_frontend * fe ) <nl>  <nl> if ( priv ) { <nl> cancel_delayed_work (& priv -> timer_sleep ); <nl> - hybrid_tuner_release_state ( priv ); <nl> if ( priv -> firmware ) <nl> release_firmware ( priv -> firmware ); <nl> + hybrid_tuner_release_state ( priv ); <nl> } <nl>  <nl> mutex_unlock (& xc5000_list_mutex );
void i40evf_virtchnl_completion ( struct i40evf_adapter * adapter , <nl> sizeof ( struct i40e_virtchnl_vsi_resource ); <nl> memcpy ( adapter -> vf_res , msg , min ( msglen , len )); <nl> i40e_vf_parse_hw_config (& adapter -> hw , adapter -> vf_res ); <nl> + /* restore current mac address */ <nl> + ether_addr_copy ( adapter -> hw . mac . addr , netdev -> dev_addr ); <nl> i40evf_process_config ( adapter ); <nl> } <nl> break ;
static void gb_tty_set_termios ( struct tty_struct * tty , <nl> send_control ( gb_tty , newctrl ); <nl> } <nl>  <nl> - if ( memcpy (& gb_tty -> line_coding , & newline , sizeof ( newline ))) { <nl> + if ( memcmp (& gb_tty -> line_coding , & newline , sizeof ( newline ))) { <nl> memcpy (& gb_tty -> line_coding , & newline , sizeof ( newline )); <nl> send_line_coding ( gb_tty ); <nl> }
static int igb_setup_loopback_test ( struct igb_adapter * adapter ) <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_SERDES ) || <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_BACKPLANE ) || <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_SFP ) || <nl> - ( hw -> device_id == E1000_DEV_ID_I354_SGMII )) { <nl> - <nl> + ( hw -> device_id == E1000_DEV_ID_I354_SGMII ) || <nl> + ( hw -> device_id == E1000_DEV_ID_I354_BACKPLANE_2_5GBPS )) { <nl> /* Enable DH89xxCC MPHY for near end loopback */ <nl> reg = rd32 ( E1000_MPHY_ADDR_CTL ); <nl> reg = ( reg & E1000_MPHY_ADDR_CTL_OFFSET_MASK ) |
static void * slob_alloc ( size_t size , gfp_t gfp , int align , int node ) <nl> /* Improve fragment distribution and reduce our average <nl> * search time by starting our next search here . ( see <nl> * Knuth vol 1 , sec 2 . 5 , pg 449 ) */ <nl> - if ( free_slob_pages . next != prev -> next ) <nl> + if ( prev != free_slob_pages . prev && <nl> + free_slob_pages . next != prev -> next ) <nl> list_move_tail (& free_slob_pages , prev -> next ); <nl> break ; <nl> }
extern unsigned long wall_jiffies ; <nl> */ <nl> unsigned long long sched_clock ( void ) <nl> { <nl> - return (( get_clock () - jiffies_timer_cc ) * 1000 ) >> 12 ; <nl> + return (( get_clock () - jiffies_timer_cc ) * 125 ) >> 9 ; <nl> } <nl>  <nl> void tod_to_timeval ( __u64 todval , struct timespec * xtime )
static void p54_pspoll_workaround ( struct p54_common * priv , struct sk_buff * skb ) <nl> return ; <nl>  <nl> /* only consider beacons from the associated BSSID */ <nl> - if (! ether_addr_equal ( hdr -> addr3 , priv -> bssid )) <nl> + if (! ether_addr_equal_64bits ( hdr -> addr3 , priv -> bssid )) <nl> return ; <nl>  <nl> tim = p54_find_ie ( skb , WLAN_EID_TIM );
static int p54_tx_qos_accounting_alloc ( struct p54_common * priv , <nl> struct p54_tx_queue_stats * queue ; <nl> unsigned long flags ; <nl>  <nl> - if ( WARN_ON ( p54_queue > P54_QUEUE_NUM )) <nl> + if ( WARN_ON ( p54_queue >= P54_QUEUE_NUM )) <nl> return - EINVAL ; <nl>  <nl> queue = & priv -> tx_stats [ p54_queue ];
int invalidate_inode_pages2_range ( struct address_space * mapping , <nl> pagevec_release (& pvec ); <nl> cond_resched (); <nl> } <nl> + WARN_ON_ONCE ( ret ); <nl> return ret ; <nl> } <nl> EXPORT_SYMBOL_GPL ( invalidate_inode_pages2_range );
get_matching_model_microcode ( int cpu , unsigned long start , <nl> unsigned int mc_saved_count = mc_saved_data -> mc_saved_count ; <nl> int i ; <nl>  <nl> - while ( leftover ) { <nl> + while ( leftover && mc_saved_count < ARRAY_SIZE ( mc_saved_tmp )) { <nl> mc_header = ( struct microcode_header_intel *) ucode_ptr ; <nl>  <nl> mc_size = get_totalsize ( mc_header );
acpi_table_parse_entries ( char * id , <nl> unsigned long table_end ; <nl> acpi_size tbl_size ; <nl>  <nl> + if ( acpi_disabled ) <nl> + return - ENODEV ; <nl> + <nl> if (! handler ) <nl> return - EINVAL ; <nl>  <nl> int __init acpi_table_parse ( char * id , acpi_table_handler handler ) <nl> struct acpi_table_header * table = NULL ; <nl> acpi_size tbl_size ; <nl>  <nl> + if ( acpi_disabled ) <nl> + return - ENODEV ; <nl> + <nl> if (! handler ) <nl> return - EINVAL ; <nl> 
void igb_update_stats ( struct igb_adapter * adapter , <nl>  <nl> rcu_read_lock (); <nl> for ( i = 0 ; i < adapter -> num_rx_queues ; i ++) { <nl> - u32 rqdpc = rd32 ( E1000_RQDPC ( i )); <nl> struct igb_ring * ring = adapter -> rx_ring [ i ]; <nl> + u32 rqdpc = rd32 ( E1000_RQDPC ( i )); <nl> + if ( hw -> mac . type >= e1000_i210 ) <nl> + wr32 ( E1000_RQDPC ( i ), 0 ); <nl>  <nl> if ( rqdpc ) { <nl> ring -> rx_stats . drops += rqdpc ;
static int hid_scan_report ( struct hid_device * hid ) <nl> item . type == HID_ITEM_TYPE_MAIN && <nl> item . tag == HID_MAIN_ITEM_TAG_BEGIN_COLLECTION && <nl> ( item_udata (& item ) & 0xff ) == HID_COLLECTION_PHYSICAL && <nl> - hid -> bus == BUS_USB ) <nl> + ( hid -> bus == BUS_USB || hid -> bus == BUS_I2C )) <nl> hid -> group = HID_GROUP_SENSOR_HUB ; <nl> } <nl> 
static void rtl8169_hw_phy_config ( struct net_device * dev ) <nl> return ; <nl> } <nl>  <nl> - /* phy config for RTL8169s mac_version C chip */ <nl> + if (( tp -> mac_version != RTL_GIGA_MAC_VER_02 ) && <nl> + ( tp -> mac_version != RTL_GIGA_MAC_VER_03 )) <nl> + return ; <nl> + <nl> mdio_write ( ioaddr , 31 , 0x0001 ); // w 31 2 0 1 <nl> mdio_write ( ioaddr , 21 , 0x1000 ); // w 21 15 0 1000 <nl> mdio_write ( ioaddr , 24 , 0x65c7 ); // w 24 15 0 65c7
static void magicmouse_setup_input ( struct input_dev * input , struct hid_device * h <nl> __set_bit ( BTN_TOOL_TRIPLETAP , input -> keybit ); <nl> __set_bit ( BTN_TOOL_QUADTAP , input -> keybit ); <nl> __set_bit ( BTN_TOUCH , input -> keybit ); <nl> + __set_bit ( INPUT_PROP_POINTER , input -> propbit ); <nl> + __set_bit ( INPUT_PROP_BUTTONPAD , input -> propbit ); <nl> } <nl>  <nl> if ( report_touches ) {
out0 : <nl>  <nl> static int __devexit nuc900_ac97_drvremove ( struct platform_device * pdev ) <nl> { <nl> - <nl> snd_soc_unregister_dai (& pdev -> dev ); <nl>  <nl> clk_put ( nuc900_ac97_data -> clk ); <nl> static int __devexit nuc900_ac97_drvremove ( struct platform_device * pdev ) <nl> release_mem_region ( nuc900_ac97_data -> res -> start , <nl> resource_size ( nuc900_ac97_data -> res )); <nl>  <nl> + kfree ( nuc900_ac97_data ); <nl> nuc900_ac97_data = NULL ; <nl>  <nl> return 0 ;
void __init m68k_setup_node ( int node ) <nl> */ <nl>  <nl> void * empty_zero_page ; <nl> + EXPORT_SYMBOL ( empty_zero_page ); <nl>  <nl> void show_mem ( void ) <nl> {
static int cgroup_rmdir ( struct kernfs_node * kn ) <nl> cgrp = cgroup_kn_lock_live ( kn ); <nl> if (! cgrp ) <nl> return 0 ; <nl> - cgroup_get ( cgrp ); /* for @ kn -> priv clearing */ <nl>  <nl> ret = cgroup_destroy_locked ( cgrp ); <nl>  <nl> cgroup_kn_unlock ( kn ); <nl> - <nl> - cgroup_put ( cgrp ); <nl> return ret ; <nl> } <nl> 
static int __devinit dwc3_omap_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> /* enable all IRQs */ <nl> - dwc3_writel ( omap -> base , USBOTGSS_IRQENABLE_SET_0 , 0x01 ); <nl> + reg = USBOTGSS_IRQO_COREIRQ_ST ; <nl> + dwc3_writel ( omap -> base , USBOTGSS_IRQENABLE_SET_0 , reg ); <nl>  <nl> reg = ( USBOTGSS_IRQ1_OEVT | <nl> USBOTGSS_IRQ1_DRVVBUS_RISE |
static struct xfrm_state * pfkey_msg2xfrm_state ( struct net * net , <nl> x -> aalg -> alg_key_len = key -> sadb_key_bits ; <nl> memcpy ( x -> aalg -> alg_key , key + 1 , keysize ); <nl> } <nl> + x -> aalg -> alg_trunc_len = a -> uinfo . auth . icv_truncbits ; <nl> x -> props . aalgo = sa -> sadb_sa_auth ; <nl> /* x -> algo . flags = sa -> sadb_sa_flags ; */ <nl> }
static int davinci_spi_bufs_dma ( struct spi_device * spi , struct spi_transfer * t ) <nl>  <nl> data1_reg_val = ioread32 ( davinci_spi -> base + SPIDAT1 ); <nl>  <nl> - INIT_COMPLETION ( davinci_spi -> done ); <nl> - <nl> init_completion (& davinci_spi_dma -> dma_rx_completion ); <nl> init_completion (& davinci_spi_dma -> dma_tx_completion ); <nl> 
static int find_probes ( int fd , struct probe_finder * pf ) <nl> . file = pp -> file , <nl> . cu_die = & pf -> cu_die , <nl> . sp_die = & pf -> sp_die , <nl> + . found = 0 , <nl> }; <nl> struct dwarf_callback_param probe_param = { <nl> . data = pf ,
static int mwl8k_request_firmware ( struct mwl8k_priv * priv ) <nl> return 0 ; <nl> } <nl>  <nl> + MODULE_FIRMWARE (" mwl8k / helper_8687 . fw "); <nl> + MODULE_FIRMWARE (" mwl8k / fmimage_8687 . fw "); <nl> + <nl> struct mwl8k_cmd_pkt { <nl> __le16 code ; <nl> __le16 length ;
struct omap_dss_output * omap_dss_get_output ( enum omap_dss_output_id id ) <nl>  <nl> return NULL ; <nl> } <nl> + EXPORT_SYMBOL ( omap_dss_get_output ); <nl>  <nl> static const struct dss_mgr_ops * dss_mgr_ops ; <nl> 
static int selinux_is_sblabel_mnt ( struct super_block * sb ) <nl> return sbsec -> behavior == SECURITY_FS_USE_XATTR || <nl> sbsec -> behavior == SECURITY_FS_USE_TRANS || <nl> sbsec -> behavior == SECURITY_FS_USE_TASK || <nl> + sbsec -> behavior == SECURITY_FS_USE_NATIVE || <nl> /* Special handling . Genfs but also in - core setxattr handler */ <nl> ! strcmp ( sb -> s_type -> name , " sysfs ") || <nl> ! strcmp ( sb -> s_type -> name , " pstore ") ||
static int asoc_simple_card_dai_link_of ( struct device_node * node , <nl> strlen ( dai_link -> cpu_dai_name ) + <nl> strlen ( dai_link -> codec_dai_name ) + 2 , <nl> GFP_KERNEL ); <nl> + if (! name ) { <nl> + ret = - ENOMEM ; <nl> + goto dai_link_of_err ; <nl> + } <nl> + <nl> sprintf ( name , "% s -% s ", dai_link -> cpu_dai_name , <nl> dai_link -> codec_dai_name ); <nl> dai_link -> name = dai_link -> stream_name = name ;
static void vp_del_vq ( struct virtqueue * vq ) <nl> { <nl> struct virtio_pci_device * vp_dev = to_vp_device ( vq -> vdev ); <nl> struct virtio_pci_vq_info * info = vq -> priv ; <nl> - unsigned long size ; <nl> + unsigned long flags , size ; <nl> + <nl> + spin_lock_irqsave (& vp_dev -> lock , flags ); <nl> + list_del (& info -> node ); <nl> + spin_unlock_irqrestore (& vp_dev -> lock , flags ); <nl>  <nl> iowrite16 ( info -> queue_index , vp_dev -> ioaddr + VIRTIO_PCI_QUEUE_SEL ); <nl> 
static struct irq_chip puv3_low_gpio_chip = { <nl> * irq_controller_lock held , and IRQs disabled . Decode the IRQ <nl> * and call the handler . <nl> */ <nl> - static void <nl> - puv3_gpio_handler ( unsigned int irq , struct irq_desc * desc ) <nl> + static void puv3_gpio_handler ( unsigned int __irq , struct irq_desc * desc ) <nl> { <nl> - unsigned int mask ; <nl> + unsigned int mask , irq ; <nl>  <nl> mask = readl ( GPIO_GEDR ); <nl> do {
snd_m3_enable_ints ( struct snd_m3 * chip ) <nl> val = ASSP_INT_ENABLE /*| MPU401_INT_ENABLE */; <nl> if ( chip -> hv_config & HV_CTRL_ENABLE ) <nl> val |= HV_INT_ENABLE ; <nl> + outb ( val , chip -> iobase + HOST_INT_STATUS ); <nl> outw ( val , io + HOST_INT_CTRL ); <nl> outb ( inb ( io + ASSP_CONTROL_C ) | ASSP_HOST_INT_ENABLE , <nl> io + ASSP_CONTROL_C );
static int trusted_update ( struct key * key , const void * data , size_t datalen ) <nl> ret = datablob_parse ( datablob , new_p , new_o ); <nl> if ( ret != Opt_update ) { <nl> ret = - EINVAL ; <nl> + kfree ( new_p ); <nl> goto out ; <nl> } <nl> /* copy old key values , and reseal with new pcrs */
static ssize_t hugetlb_cgroup_write ( struct kernfs_open_file * of , <nl> ret = res_counter_memparse_write_strategy ( buf , & val ); <nl> if ( ret ) <nl> break ; <nl> + val = ALIGN ( val , 1ULL << huge_page_shift (& hstates [ idx ])); <nl> ret = res_counter_set_limit (& h_cg -> hugepage [ idx ], val ); <nl> break ; <nl> default :
int __init irttp_init ( void ) <nl> if (! irttp -> tsaps ) { <nl> IRDA_ERROR ("% s : can ' t allocate IrTTP hashbin !\ n ", <nl> __FUNCTION__ ); <nl> + kfree ( irttp ); <nl> return - ENOMEM ; <nl> } <nl> 
static int get_info ( struct net * net , void __user * user , <nl> private = & tmp ; <nl> } <nl> # endif <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . valid_hooks = t -> valid_hooks ; <nl> memcpy ( info . hook_entry , private -> hook_entry , <nl> sizeof ( info . hook_entry ));
visorchannel_create_guts ( u64 physaddr , unsigned long channel_bytes , <nl> int err ; <nl> size_t size = sizeof ( struct channel_header ); <nl>  <nl> + if ( physaddr == 0 ) <nl> + return NULL ; <nl> + <nl> channel = kzalloc ( sizeof (* channel ), gfp ); <nl> if (! channel ) <nl> goto cleanup ;
out : <nl> static int wl1271_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> { <nl> struct wl1271 * wl = hw -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> mutex_lock (& wl -> mutex ); <nl>  <nl> + if ( unlikely ( wl -> state == WL1271_STATE_OFF )) <nl> + goto out ; <nl> + <nl> ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> if ( ret < 0 ) <nl> goto out ;
static void i40evf_remove ( struct pci_dev * pdev ) <nl> i40evf_reset_interrupt_capability ( adapter ); <nl> } <nl>  <nl> - del_timer_sync (& adapter -> watchdog_timer ); <nl> + if ( adapter -> watchdog_timer . function ) <nl> + del_timer_sync (& adapter -> watchdog_timer ); <nl> + <nl> flush_scheduled_work (); <nl>  <nl> if ( hw -> aq . asq . count )
static int cache_create ( struct cache_args * ca , struct cache ** result ) <nl> atomic_set (& cache -> nr_migrations , 0 ); <nl> init_waitqueue_head (& cache -> migration_wait ); <nl>  <nl> + r = - ENOMEM ; <nl> cache -> nr_dirty = 0 ; <nl> cache -> dirty_bitset = alloc_bitset ( from_cblock ( cache -> cache_size )); <nl> if (! cache -> dirty_bitset ) {
static int dpcm_add_paths ( struct snd_soc_pcm_runtime * fe , int stream , <nl>  <nl> switch ( list -> widgets [ i ]-> id ) { <nl> case snd_soc_dapm_dai_in : <nl> + if ( stream != SNDRV_PCM_STREAM_PLAYBACK ) <nl> + continue ; <nl> + break ; <nl> case snd_soc_dapm_dai_out : <nl> + if ( stream != SNDRV_PCM_STREAM_CAPTURE ) <nl> + continue ; <nl> break ; <nl> default : <nl> continue ;
static int ath9k_htc_aggr_oper ( struct ath9k_htc_priv * priv , <nl> int ret = 0 ; <nl> u8 cmd_rsp ; <nl>  <nl> - if ( tid > ATH9K_HTC_MAX_TID ) <nl> + if ( tid >= ATH9K_HTC_MAX_TID ) <nl> return - EINVAL ; <nl>  <nl> memset (& aggr , 0 , sizeof ( struct ath9k_htc_target_aggr ));
static void usb_hcd_ppc_soc_remove ( struct usb_hcd * hcd , <nl>  <nl> iounmap ( hcd -> regs ); <nl> release_mem_region ( hcd -> rsrc_start , hcd -> rsrc_len ); <nl> - usb_hcd_put ( hcd ); <nl> + usb_put_hcd ( hcd ); <nl> } <nl>  <nl> static int __devinit
struct btrfs_root * btrfs_create_tree ( struct btrfs_trans_handle * trans , <nl> fail : <nl> if ( leaf ) { <nl> btrfs_tree_unlock ( leaf ); <nl> + free_extent_buffer ( root -> commit_root ); <nl> free_extent_buffer ( leaf ); <nl> } <nl> kfree ( root );
static int sony_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> } <nl>  <nl> ret = sony_set_operational ( hdev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_stop ; <nl>  <nl> return 0 ;
static int vpfe_open ( struct file * file ) <nl> if (! vpfe_dev -> initialized ) { <nl> if ( vpfe_initialize_device ( vpfe_dev )) { <nl> mutex_unlock (& vpfe_dev -> lock ); <nl> + v4l2_fh_exit (& fh -> fh ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> }
static int machine__process_kernel_mmap_event ( struct machine * machine , <nl> if ( __machine__create_kernel_maps ( machine , kernel ) < 0 ) <nl> goto out_problem ; <nl>  <nl> + if ( strstr ( dso -> long_name , " vmlinux ")) <nl> + dso__set_short_name ( dso , "[ kernel . vmlinux ]", false ); <nl> + <nl> machine__set_kernel_mmap_len ( machine , event ); <nl>  <nl> /*
static int fuse_retrieve ( struct fuse_conn * fc , struct inode * inode , <nl> loff_t file_size ; <nl> unsigned int num ; <nl> unsigned int offset ; <nl> - size_t total_len ; <nl> + size_t total_len = 0 ; <nl>  <nl> req = fuse_get_req ( fc ); <nl> if ( IS_ERR ( req ))
void pcibios_free_controller ( struct pci_controller * phb ) <nl> if ( phb -> is_dynamic ) <nl> kfree ( phb ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( pcibios_free_controller ); <nl>  <nl> /* <nl> * The function is used to return the minimal alignment
hauppauge_tuner [] = <nl> { TUNER_ABSENT , " MaxLinear 301 "}, <nl> { TUNER_ABSENT , " Mirics MSi001 "}, <nl> { TUNER_ABSENT , " MaxLinear MxL241SF "}, <nl> - { TUNER_ABSENT , " Xceive XC5000C "}, <nl> + { TUNER_XC5000C , " Xceive XC5000C "}, <nl> { TUNER_ABSENT , " Montage M68TS2020 "}, <nl> }; <nl> 
void * devm_memremap ( struct device * dev , resource_size_t offset , <nl> if ( addr ) { <nl> * ptr = addr ; <nl> devres_add ( dev , ptr ); <nl> - } else <nl> + } else { <nl> devres_free ( ptr ); <nl> + return ERR_PTR (- ENXIO ); <nl> + } <nl>  <nl> return addr ; <nl> }
static int sh_eth_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> skb -> len + 2 ); <nl> txdesc -> addr = dma_map_single (& ndev -> dev , skb -> data , skb -> len , <nl> DMA_TO_DEVICE ); <nl> + if ( dma_mapping_error (& ndev -> dev , txdesc -> addr )) { <nl> + kfree_skb ( skb ); <nl> + return NETDEV_TX_OK ; <nl> + } <nl> txdesc -> buffer_length = skb -> len ; <nl>  <nl> if ( entry >= mdp -> num_tx_ring - 1 )
static void ath9k_hw_get_def_gain_boundaries_pdadcs ( struct ath_hw * ah , <nl> vpdTableI [ i ][ sizeCurrVpdTable - 2 ]); <nl> vpdStep = ( int16_t )(( vpdStep < 1 ) ? 1 : vpdStep ); <nl>  <nl> - if ( tgtIndex > maxIndex ) { <nl> + if ( tgtIndex >= maxIndex ) { <nl> while (( ss <= tgtIndex ) && <nl> ( k < ( AR5416_NUM_PDADC_VALUES - 1 ))) { <nl> tmpVal = ( int16_t )(( vpdTableI [ i ][ sizeCurrVpdTable - 1 ] +
static void alc269_fill_coef ( struct hda_codec * codec ) <nl>  <nl> if ( spec -> codec_variant != ALC269_TYPE_ALC269VB ) <nl> return ; <nl> + /* ALC271X doesn ' t seem to support these COEFs ( bko # 52181 ) */ <nl> + if (! strcmp ( codec -> chip_name , " ALC271X ")) <nl> + return ; <nl>  <nl> if (( alc_get_coef0 ( codec ) & 0x00ff ) < 0x015 ) { <nl> alc_write_coef_idx ( codec , 0xf , 0x960b );
static int stmmac_rx ( struct stmmac_priv * priv , int limit ) <nl>  <nl> frame_len = priv -> hw -> desc -> get_rx_frame_len ( p , coe ); <nl>  <nl> + /* check if frame_len fits the preallocated memory */ <nl> + if ( frame_len > priv -> dma_buf_sz ) { <nl> + priv -> dev -> stats . rx_length_errors ++; <nl> + break ; <nl> + } <nl> + <nl> /* ACS is set ; GMAC core strips PAD / FCS for IEEE 802 . 3 <nl> * Type frames ( LLC / LLC - SNAP ) <nl> */
mwifiex_11n_aggregate_pkt ( struct mwifiex_private * priv , <nl> tx_info_aggr -> flags |= MWIFIEX_BUF_FLAG_TDLS_PKT ; <nl> tx_info_aggr -> flags |= MWIFIEX_BUF_FLAG_AGGR_PKT ; <nl> skb_aggr -> priority = skb_src -> priority ; <nl> + skb_aggr -> tstamp = skb_src -> tstamp ; <nl>  <nl> do_gettimeofday (& tv ); <nl> skb_aggr -> tstamp = timeval_to_ktime ( tv );
static int do_tcp_getsockopt ( struct sock * sk , int level , <nl> val = tp -> mss_cache ; <nl> if (! val && (( 1 << sk -> sk_state ) & ( TCPF_CLOSE | TCPF_LISTEN ))) <nl> val = tp -> rx_opt . user_mss ; <nl> + if ( tp -> repair ) <nl> + val = tp -> rx_opt . mss_clamp ; <nl> break ; <nl> case TCP_NODELAY : <nl> val = !!( tp -> nonagle & TCP_NAGLE_OFF );
static int __init longhaul_cpu_init ( struct cpufreq_policy * policy ) <nl> if ( pr == NULL ) goto err_acpi ; <nl>  <nl> cx = & pr -> power . states [ ACPI_STATE_C3 ]; <nl> - if ( cx == NULL || cx -> latency > 1000 ) goto err_acpi ; <nl> + if ( cx -> address == 0 || cx -> latency > 1000 ) goto err_acpi ; <nl>  <nl> /* Now check what we have on this motherboard */ <nl> switch ( c -> x86_model ) {
ssize_t lirc_dev_fop_read ( struct file * file , <nl> return - ENODEV ; <nl> } <nl>  <nl> + if (! LIRC_CAN_REC ( ir -> d . features )) <nl> + return - EINVAL ; <nl> + <nl> dev_dbg ( ir -> d . dev , LOGHEAD " read called \ n ", ir -> d . name , ir -> d . minor ); <nl>  <nl> buf = kzalloc ( ir -> chunk_size , GFP_KERNEL );
static int pm8001_chip_sata_req ( struct pm8001_hba_info * pm8001_ha , <nl>  <nl> /* Check for read log for failed drive and return */ <nl> if ( sata_cmd . sata_fis . command == 0x2f ) { <nl> - if ( pm8001_ha_dev && (( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> + if ((( pm8001_ha_dev -> id & NCQ_READ_LOG_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_ABORT_ALL_FLAG ) || <nl> ( pm8001_ha_dev -> id & NCQ_2ND_RLE_FLAG ))) { <nl> struct task_status_struct * ts ;
static int wm8750_set_bias_level ( struct snd_soc_codec * codec , <nl> case SND_SOC_BIAS_PREPARE : <nl> break ; <nl> case SND_SOC_BIAS_STANDBY : <nl> - if ( codec -> dapm . bias_level == SND_SOC_BIAS_OFF ) { <nl> + if ( snd_soc_codec_get_bias_level ( codec ) == SND_SOC_BIAS_OFF ) { <nl> snd_soc_cache_sync ( codec ); <nl>  <nl> /* Set VMID to 5k */
again : <nl> } <nl> spin_unlock (& fs_info -> reada_lock ); <nl>  <nl> + kfree ( multi ); <nl> return re ; <nl>  <nl> error : <nl> error : <nl> kref_put (& zone -> refcnt , reada_zone_release ); <nl> spin_unlock (& fs_info -> reada_lock ); <nl> } <nl> + kfree ( multi ); <nl> kfree ( re ); <nl> if ( looped ) <nl> goto again ;
static void usb_alphatrack_disconnect ( struct usb_interface * intf ) <nl> mutex_unlock (& dev -> mtx ); <nl> usb_alphatrack_delete ( dev ); <nl> } else { <nl> + atomic_set (& dev -> writes_pending , 0 ); <nl> dev -> intf = NULL ; <nl> mutex_unlock (& dev -> mtx ); <nl> } <nl>  <nl> - atomic_set (& dev -> writes_pending , 0 ); <nl> mutex_unlock (& disconnect_mutex ); <nl>  <nl> dev_info (& intf -> dev , " Alphatrack Surface #% d now disconnected \ n ",
static int super_written ( struct bio * bio , unsigned int bytes_done , int error ) <nl>  <nl> if ( atomic_dec_and_test (& rdev -> mddev -> pending_writes )) <nl> wake_up (& rdev -> mddev -> sb_wait ); <nl> + bio_put ( bio ); <nl> return 0 ; <nl> } <nl> 
static struct pci_device_id agp_sis_pci_table [] = { <nl> . subvendor = PCI_ANY_ID , <nl> . subdevice = PCI_ANY_ID , <nl> }, <nl> - { <nl> - . class = ( PCI_CLASS_BRIDGE_HOST << 8 ), <nl> - . class_mask = ~ 0 , <nl> - . vendor = PCI_VENDOR_ID_SI , <nl> - . device = PCI_DEVICE_ID_SI_760 , <nl> - . subvendor = PCI_ANY_ID , <nl> - . subdevice = PCI_ANY_ID , <nl> - }, <nl> { } <nl> }; <nl> 
struct virtio_device_id { <nl>  <nl> struct i2c_device_id { <nl> char name [ I2C_NAME_SIZE ]; <nl> - kernel_ulong_t driver_data ; /* Data private to the driver */ <nl> + kernel_ulong_t driver_data /* Data private to the driver */ <nl> + __attribute__ (( aligned ( sizeof ( kernel_ulong_t )))); <nl> }; <nl>  <nl> 
void iwl_irq_tasklet ( struct iwl_trans * trans ) <nl> } <nl> # endif <nl>  <nl> - spin_unlock_irqrestore (& trans -> shrd -> lock , flags ); <nl> - <nl> /* saved interrupt in inta variable now we can reset trans_pcie -> inta */ <nl> trans_pcie -> inta = 0 ; <nl>  <nl> + spin_unlock_irqrestore (& trans -> shrd -> lock , flags ); <nl> + <nl> /* Now service all interrupt bits discovered above . */ <nl> if ( inta & CSR_INT_BIT_HW_ERR ) { <nl> IWL_ERR ( trans , " Hardware error detected . Restarting .\ n ");
static int read_bus_info_block ( struct fw_device * device , int generation ) <nl> return - ENOMEM ; <nl>  <nl> stack = & rom [ READ_BIB_ROM_SIZE ]; <nl> + memset ( rom , 0 , sizeof (* rom ) * READ_BIB_ROM_SIZE ); <nl>  <nl> device -> max_speed = SCODE_100 ; <nl> 
static int onenand_write_ops_nolock ( struct mtd_info * mtd , loff_t to , <nl> } <nl>  <nl> /* Only check verify write turn on */ <nl> - ret = onenand_verify ( mtd , ( u_char *) wbuf , to , thislen ); <nl> + ret = onenand_verify ( mtd , buf , to , thislen ); <nl> if ( ret ) { <nl> printk ( KERN_ERR " onenand_write_ops_nolock : verify failed % d \ n ", ret ); <nl> break ;
static struct machine * machines__find_for_cpumode ( struct machines * machines , <nl>  <nl> machine = machines__find ( machines , pid ); <nl> if (! machine ) <nl> - machine = machines__find ( machines , DEFAULT_GUEST_KERNEL_ID ); <nl> + machine = machines__findnew ( machines , DEFAULT_GUEST_KERNEL_ID ); <nl> return machine ; <nl> } <nl> 
static int update_nodemask ( struct cpuset * cs , struct cpuset * trialcs , <nl> spin_unlock_irq (& callback_lock ); <nl>  <nl> /* use trialcs -> mems_allowed as a temp variable */ <nl> - update_nodemasks_hier ( cs , & cs -> mems_allowed ); <nl> + update_nodemasks_hier ( cs , & trialcs -> mems_allowed ); <nl> done : <nl> return retval ; <nl> }
out : <nl> } <nl>  <nl> cl_env_put ( env , & refcheck ); <nl> - return tot_bytes ? : result ; <nl> + return tot_bytes ? tot_bytes : result ; <nl> } <nl>  <nl> /**
cifs_setlk ( struct file * file , struct file_lock * flock , __u32 type , <nl> rc = server -> ops -> mand_unlock_range ( cfile , flock , xid ); <nl>  <nl> out : <nl> - if ( flock -> fl_flags & FL_POSIX ) <nl> - posix_lock_file_wait ( file , flock ); <nl> + if ( flock -> fl_flags & FL_POSIX && ! rc ) <nl> + rc = posix_lock_file_wait ( file , flock ); <nl> return rc ; <nl> } <nl> 
int p9dirent_read ( char * buf , int len , struct p9_dirent * dirent , <nl> } <nl>  <nl> strcpy ( dirent -> d_name , nameptr ); <nl> + kfree ( nameptr ); <nl>  <nl> out : <nl> return fake_pdu . offset ;
static int socrates_nand_probe ( struct platform_device * ofdev ) <nl> nand_release ( mtd ); <nl>  <nl> out : <nl> - dev_set_drvdata (& ofdev -> dev , NULL ); <nl> iounmap ( host -> io_base ); <nl> kfree ( host ); <nl> return res ; <nl> static int socrates_nand_remove ( struct platform_device * ofdev ) <nl>  <nl> nand_release ( mtd ); <nl>  <nl> - dev_set_drvdata (& ofdev -> dev , NULL ); <nl> iounmap ( host -> io_base ); <nl> kfree ( host ); <nl> 
static void ipsec_esp_decrypt_swauth_done ( struct device * dev , <nl> } else <nl> oicv = ( char *)& edesc -> link_tbl [ 0 ]; <nl>  <nl> - err = memcmp ( oicv , icv , authsize ) ? - EBADMSG : 0 ; <nl> + err = crypto_memneq ( oicv , icv , authsize ) ? - EBADMSG : 0 ; <nl> } <nl>  <nl> kfree ( edesc );
static void si_apply_state_adjust_rules ( struct radeon_device * rdev , <nl> max_sclk = 75000 ; <nl> max_mclk = 80000 ; <nl> } <nl> + /* limit clocks on HD8600 series */ <nl> + if ( rdev -> pdev -> device == 0x6660 && <nl> + rdev -> pdev -> revision == 0x83 ) { <nl> + max_sclk = 75000 ; <nl> + max_mclk = 80000 ; <nl> + } <nl>  <nl> if ( rps -> vce_active ) { <nl> rps -> evclk = rdev -> pm . dpm . vce_states [ rdev -> pm . dpm . vce_level ]. evclk ;
static void _rtl_usb_tx_preprocess ( struct ieee80211_hw * hw , struct sk_buff * skb , <nl> u8 tid = 0 ; <nl> u16 seq_number = 0 ; <nl>  <nl> + memset (& tcb_desc , 0 , sizeof ( struct rtl_tcb_desc )); <nl> if ( ieee80211_is_auth ( fc )) { <nl> RT_TRACE ( rtlpriv , COMP_SEND , DBG_DMESG , (" MAC80211_LINKING \ n ")); <nl> rtl_ips_nic_on ( hw );
SYSCALL_DEFINE2 ( delete_module , const char __user *, name_user , <nl> return - EFAULT ; <nl> name [ MODULE_NAME_LEN - 1 ] = '\ 0 '; <nl>  <nl> + audit_log_kern_module ( name ); <nl> + <nl> if ( mutex_lock_interruptible (& module_mutex ) != 0 ) <nl> return - EINTR ; <nl> 
static int get_exec_file ( struct cfg_devnode * dev_node_obj , <nl> if (! drv_datap || ! drv_datap -> base_img ) <nl> return - EFAULT ; <nl>  <nl> - if ( strlen ( drv_datap -> base_img ) > size ) <nl> + if ( strlen ( drv_datap -> base_img ) >= size ) <nl> return - EINVAL ; <nl>  <nl> strcpy ( exec_file , drv_datap -> base_img );
xlog_recover_add_to_trans ( <nl> " bad number of regions (% d ) in inode log format ", <nl> in_f -> ilf_size ); <nl> ASSERT ( 0 ); <nl> + free ( ptr ); <nl> return XFS_ERROR ( EIO ); <nl> } <nl> 
static int iwlagn_mac_sta_add ( struct ieee80211_hw * hw , <nl> { <nl> struct iwl_priv * priv = hw -> priv ; <nl> struct iwl_station_priv * sta_priv = ( void *) sta -> drv_priv ; <nl> - bool is_ap = priv -> iw_mode == NL80211_IFTYPE_STATION ; <nl> + bool is_ap = vif -> type == NL80211_IFTYPE_STATION ; <nl> int ret ; <nl> u8 sta_id ; <nl> 
struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
int hardif_add_interface ( char * dev , int if_num ) <nl> return 1 ; <nl>  <nl> out : <nl> - if ( batman_if -> packet_buff ) <nl> - kfree ( batman_if -> packet_buff ); <nl> + kfree ( batman_if -> packet_buff ); <nl> kfree ( batman_if ); <nl> kfree ( dev ); <nl> return - 1 ;
static int menu_select ( struct cpuidle_driver * drv , struct cpuidle_device * dev ) <nl> * We want to default to C1 ( hlt ), not to busy polling <nl> * unless the timer is happening really really soon . <nl> */ <nl> - if ( data -> next_timer_us > 20 && <nl> + if ( interactivity_req > 20 && <nl> ! drv -> states [ CPUIDLE_DRIVER_STATE_START ]. disabled && <nl> dev -> states_usage [ CPUIDLE_DRIVER_STATE_START ]. disable == 0 ) <nl> data -> last_state_idx = CPUIDLE_DRIVER_STATE_START ;
static int __init pwrdms_setup ( struct powerdomain * pwrdm ) <nl> if (! pwrdm -> pwrsts ) <nl> return 0 ; <nl>  <nl> - pwrst = kmalloc ( sizeof ( struct power_state ), GFP_KERNEL ); <nl> + pwrst = kmalloc ( sizeof ( struct power_state ), GFP_ATOMIC ); <nl> if (! pwrst ) <nl> return - ENOMEM ; <nl> pwrst -> pwrdm = pwrdm ;
static int parse_addr ( const struct nf_conn * ct , const char * cp , <nl> int family = ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . l3num ; <nl> int ret = 0 ; <nl>  <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> switch ( family ) { <nl> case AF_INET : <nl> ret = in4_pton ( cp , limit - cp , ( u8 *)& addr -> ip , - 1 , & end );
xfs_fs_geometry ( <nl> xfs_fsop_geom_t * geo , <nl> int new_version ) <nl> { <nl> + <nl> + memset ( geo , 0 , sizeof (* geo )); <nl> + <nl> geo -> blocksize = mp -> m_sb . sb_blocksize ; <nl> geo -> rtextsize = mp -> m_sb . sb_rextsize ; <nl> geo -> agblocks = mp -> m_sb . sb_agblocks ;
static void xhci_pci_quirks ( struct device * dev , struct xhci_hcd * xhci ) <nl> xhci -> quirks |= XHCI_RESET_ON_RESUME ; <nl> xhci_dbg ( xhci , " QUIRK : Resetting on resume \ n "); <nl> } <nl> + if ( pdev -> vendor == PCI_VENDOR_ID_VIA ) <nl> + xhci -> quirks |= XHCI_RESET_ON_RESUME ; <nl> } <nl>  <nl> /* called during probe () after chip reset completes */
int ceph_init_dentry ( struct dentry * dentry ) <nl> return - ENOMEM ; /* oh well */ <nl>  <nl> spin_lock (& dentry -> d_lock ); <nl> - if ( dentry -> d_fsdata ) /* lost a race */ <nl> + if ( dentry -> d_fsdata ) { <nl> + /* lost a race */ <nl> + kmem_cache_free ( ceph_dentry_cachep , di ); <nl> goto out_unlock ; <nl> + } <nl> di -> dentry = dentry ; <nl> di -> lease_session = NULL ; <nl> dentry -> d_fsdata = di ;
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
static int i915_drm_thaw ( struct drm_device * dev ) <nl> drm_irq_install ( dev ); <nl>  <nl> /* Resume the modeset for every activated CRTC */ <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> drm_helper_resume_force_mode ( dev ); <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl>  <nl> if ( IS_IRONLAKE_M ( dev )) <nl> ironlake_enable_rc6 ( dev );
static int hgcm_call_preprocess_linaddr ( <nl> if (! bounce_buf ) <nl> return - ENOMEM ; <nl>  <nl> + * bounce_buf_ret = bounce_buf ; <nl> + <nl> if ( copy_in ) { <nl> ret = copy_from_user ( bounce_buf , ( void __user *) buf , len ); <nl> if ( ret ) <nl> static int hgcm_call_preprocess_linaddr ( <nl> memset ( bounce_buf , 0 , len ); <nl> } <nl>  <nl> - * bounce_buf_ret = bounce_buf ; <nl> hgcm_call_add_pagelist_size ( bounce_buf , len , extra ); <nl> return 0 ; <nl> }
static int drm_queue_vblank_event ( struct drm_device * dev , int pipe , <nl> if (( vblwait -> request . type & _DRM_VBLANK_NEXTONMISS ) && <nl> ( seq - vblwait -> request . sequence ) <= ( 1 << 23 )) { <nl> vblwait -> request . sequence = seq + 1 ; <nl> + vblwait -> reply . sequence = vblwait -> request . sequence ; <nl> } <nl>  <nl> DRM_DEBUG (" event on vblank count % d , current % d , crtc % d \ n ",
int mwifiex_ret_wmm_get_status ( struct mwifiex_private * priv , <nl> " WMM Parameter Set Count : % d \ n ", <nl> wmm_param_ie -> qos_info_bitmap & mask ); <nl>  <nl> + if ( wmm_param_ie -> vend_hdr . len + 2 > <nl> + sizeof ( struct ieee_types_wmm_parameter )) <nl> + break ; <nl> + <nl> memcpy (( u8 *) & priv -> curr_bss_params . bss_descriptor . <nl> wmm_ie , wmm_param_ie , <nl> wmm_param_ie -> vend_hdr . len + 2 );
void __iomem * __ioremap ( unsigned long phys_addr , unsigned long size , unsigned l <nl> */ <nl> offset = phys_addr & ~ PAGE_MASK ; <nl> phys_addr &= PAGE_MASK ; <nl> - size = PAGE_ALIGN ( last_addr ) - phys_addr ; <nl> + size = PAGE_ALIGN ( last_addr + 1 ) - phys_addr ; <nl>  <nl> /* <nl> * Ok , go for it ..
static int skl_unload_module ( struct sst_dsp * ctx , u16 mod_id ) <nl> dev_err ( ctx -> dev , " Module bad usage cnt !:% d \ n ", usage_cnt ); <nl> return - EIO ; <nl> } <nl> + <nl> + /* if module is used by others return , no need to unload */ <nl> + if ( usage_cnt > 0 ) <nl> + return 0 ; <nl> + <nl> ret = skl_ipc_unload_modules (& skl -> ipc , <nl> SKL_NUM_MODULES , & mod_id ); <nl> if ( ret < 0 ) {
static char * res_strings [] = { <nl> " reserved 37 ", <nl> " reserved 38 ", <nl> " reserved 39 ", <nl> - " reseverd 40 ", <nl> + " reserved 40 ", <nl> " reserved 41 ", <nl> " reserved 42 ", <nl> " reserved 43 ",
static int ssm4567_set_power ( struct ssm4567 * ssm4567 , bool enable ) <nl> regcache_cache_only ( ssm4567 -> regmap , ! enable ); <nl>  <nl> if ( enable ) { <nl> + ret = regmap_write ( ssm4567 -> regmap , SSM4567_REG_SOFT_RESET , <nl> + 0x00 ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> ret = regmap_update_bits ( ssm4567 -> regmap , <nl> SSM4567_REG_POWER_CTRL , <nl> SSM4567_POWER_SPWDN , 0x00 );
static inline struct kvm_vcpu * kvm_get_vcpu_by_id ( struct kvm * kvm , int id ) <nl> struct kvm_vcpu * vcpu ; <nl> int i ; <nl>  <nl> + if ( id < 0 || id >= KVM_MAX_VCPUS ) <nl> + return NULL ; <nl> + vcpu = kvm_get_vcpu ( kvm , id ); <nl> + if ( vcpu && vcpu -> vcpu_id == id ) <nl> + return vcpu ; <nl> kvm_for_each_vcpu ( i , vcpu , kvm ) <nl> if ( vcpu -> vcpu_id == id ) <nl> return vcpu ;
int megasas_alloc_cmds ( struct megasas_instance * instance ) <nl> if ( megasas_create_frame_pool ( instance )) { <nl> dev_printk ( KERN_DEBUG , & instance -> pdev -> dev , " Error creating frame DMA pool \ n "); <nl> megasas_free_cmds ( instance ); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> return 0 ;
int ext4_collapse_range ( struct inode * inode , loff_t offset , loff_t len ) <nl> if (! S_ISREG ( inode -> i_mode )) <nl> return - EINVAL ; <nl>  <nl> + if ( EXT4_SB ( inode -> i_sb )-> s_cluster_ratio > 1 ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> trace_ext4_collapse_range ( inode , offset , len ); <nl>  <nl> punch_start = offset >> EXT4_BLOCK_SIZE_BITS ( sb );
static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
static int perf_trace_event_perm ( struct ftrace_event_call * tp_event , <nl> { <nl> /* The ftrace function trace is allowed only for root . */ <nl> if ( ftrace_event_is_function ( tp_event ) && <nl> - perf_paranoid_kernel () && ! capable ( CAP_SYS_ADMIN )) <nl> + perf_paranoid_tracepoint_raw () && ! capable ( CAP_SYS_ADMIN )) <nl> return - EPERM ; <nl>  <nl> /* No tracing , just counting , so no obvious leak */
void free_user_ns ( struct kref * kref ) <nl> struct user_namespace * ns ; <nl>  <nl> ns = container_of ( kref , struct user_namespace , kref ); <nl> + free_uid ( ns -> root_user ); <nl> kfree ( ns ); <nl> } <nl> 
void __init paging_init ( void ) <nl>  <nl> mem_map = NODE_DATA ( 0 )-> node_mem_map ; <nl>  <nl> - memset ( zero_page , 0 , PAGE_SIZE ); <nl> empty_zero_page = virt_to_page ( zero_page ); <nl> flush_dcache_page ( empty_zero_page ); <nl> }
static int do_ipv6_setsockopt ( struct sock * sk , int level , int optname , <nl> break ; <nl>  <nl> case IPV6_TRANSPARENT : <nl> + if (! capable ( CAP_NET_ADMIN )) { <nl> + retv = - EPERM ; <nl> + break ; <nl> + } <nl> if ( optlen < sizeof ( int )) <nl> goto e_inval ; <nl> /* we don ' t have a separate transparent bit for IPV6 we use the one in the IPv4 socket */
static noinline int cow_file_range_inline ( struct btrfs_root * root , <nl> data_len = compressed_size ; <nl>  <nl> if ( start > 0 || <nl> - actual_end >= PAGE_CACHE_SIZE || <nl> - data_len >= BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> + actual_end > PAGE_CACHE_SIZE || <nl> + data_len > BTRFS_MAX_INLINE_DATA_SIZE ( root ) || <nl> (! compressed_size && <nl> ( actual_end & ( root -> sectorsize - 1 )) == 0 ) || <nl> end + 1 < isize ||
static ssize_t cifsFYI_proc_write ( struct file * file , const char __user * buffer , <nl> cifsFYI = bv ; <nl> else if (( c [ 0 ] > ' 1 ') && ( c [ 0 ] <= ' 9 ')) <nl> cifsFYI = ( int ) ( c [ 0 ] - ' 0 '); /* see cifs_debug . h for meanings */ <nl> + else <nl> + return - EINVAL ; <nl>  <nl> return count ; <nl> }
static struct kioctx * ioctx_alloc ( unsigned nr_events ) <nl> err_cleanup : <nl> aio_nr_sub ( ctx -> max_reqs ); <nl> err : <nl> - aio_free_ring ( ctx ); <nl> free_percpu ( ctx -> cpu ); <nl> free_percpu ( ctx -> reqs . pcpu_count ); <nl> free_percpu ( ctx -> users . pcpu_count );
void __init init_IRQ ( void ) <nl> struct irq_desc * desc ; <nl> int irq ; <nl>  <nl> - for ( irq = 0 ; irq < nr_irqs ; irq ++) <nl> + for ( irq = 0 ; irq < nr_irqs ; irq ++) { <nl> + desc = irq_to_desc_alloc_node ( irq , 0 ); <nl> desc -> status |= IRQ_NOREQUEST | IRQ_NOPROBE ; <nl> + } <nl>  <nl> init_arch_irq (); <nl> }
static int yam_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> break ; <nl>  <nl> case SIOCYAMGCFG : <nl> + memset (& yi , 0 , sizeof ( yi )); <nl> yi . cfg . mask = 0xffffffff ; <nl> yi . cfg . iobase = yp -> iobase ; <nl> yi . cfg . irq = yp -> irq ;
static struct cpuidle_state s3c64xx_cpuidle_set [] = { <nl> [ 0 ] = { <nl> . enter = s3c64xx_enter_idle , <nl> . exit_latency = 1 , <nl> - . target_residency = 100000 , <nl> + . target_residency = 1 , <nl> . flags = CPUIDLE_FLAG_TIME_VALID , <nl> . name = " IDLE ", <nl> . desc = " System active , ARM gated ",
int ring_buffer_resize ( struct ring_buffer * buffer , unsigned long size ) <nl> list_del_init (& page -> list ); <nl> free_buffer_page ( page ); <nl> } <nl> + mutex_unlock (& buffer -> mutex ); <nl> return - ENOMEM ; <nl> } <nl> 
static void igmp_heard_query ( struct in_device * in_dev , struct sk_buff * skb , <nl> * to be intended in a v3 query . <nl> */ <nl> max_delay = IGMPV3_MRC ( ih3 -> code )*( HZ / IGMP_TIMER_SCALE ); <nl> + if (! max_delay ) <nl> + max_delay = 1 ; /* can ' t mod w / 0 */ <nl> } else { /* v3 */ <nl> if (! pskb_may_pull ( skb , sizeof ( struct igmpv3_query ))) <nl> return ;
static int __devexit ssm2602_i2c_remove ( struct i2c_client * client ) <nl>  <nl> static const struct i2c_device_id ssm2602_i2c_id [] = { <nl> { " ssm2602 ", SSM2602 }, <nl> + { " ssm2603 ", SSM2602 }, <nl> { " ssm2604 ", SSM2604 }, <nl> { } <nl> }; <nl> static void __exit ssm2602_exit ( void ) <nl> } <nl> module_exit ( ssm2602_exit ); <nl>  <nl> - MODULE_DESCRIPTION (" ASoC SSM2602 / SSM2604 driver "); <nl> + MODULE_DESCRIPTION (" ASoC SSM2602 / SSM2603 / SSM2604 driver "); <nl> MODULE_AUTHOR (" Cliff Cai "); <nl> MODULE_LICENSE (" GPL ");
int BcmGetSectionValEndOffset ( struct bcm_mini_adapter * Adapter , enum bcm_flash2x <nl> case CONTROL_SECTION : <nl> /* Not Clear So Putting failure . confirm and fix it . */ <nl> SectEndOffset = STATUS_FAILURE ; <nl> + break ; <nl> case ISO_IMAGE1_PART2 : <nl> if ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End != UNINIT_PTR_IN_CS ) <nl> SectEndOffset = ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End );
static long swap_inode_boot_loader ( struct super_block * sb , <nl> handle = ext4_journal_start ( inode_bl , EXT4_HT_MOVE_EXTENTS , 2 ); <nl> if ( IS_ERR ( handle )) { <nl> err = - EINVAL ; <nl> - goto swap_boot_out ; <nl> + goto journal_err_out ; <nl> } <nl>  <nl> /* Protect extent tree against block allocations via delalloc */ <nl> static long swap_inode_boot_loader ( struct super_block * sb , <nl>  <nl> ext4_double_up_write_data_sem ( inode , inode_bl ); <nl>  <nl> + journal_err_out : <nl> ext4_inode_resume_unlocked_dio ( inode ); <nl> ext4_inode_resume_unlocked_dio ( inode_bl ); <nl> 
qla24xx_report_id_acquisition ( scsi_qla_host_t * vha , <nl> if ( vp_idx == 0 && ( MSB ( stat ) != 1 )) <nl> goto reg_needed ; <nl>  <nl> - if ( MSB ( stat ) == 1 ) { <nl> + if ( MSB ( stat ) != 0 ) { <nl> ql_dbg ( ql_dbg_mbx , vha , 0x10ba , <nl> " Could not acquire ID for VP [% d ].\ n ", vp_idx ); <nl> return ;
static int tridentfb_pan_display ( struct fb_var_screeninfo * var , <nl> unsigned int offset ; <nl>  <nl> debug (" enter \ n "); <nl> - offset = ( var -> xoffset + ( var -> yoffset * var -> xres_virtual )) <nl> - * var -> bits_per_pixel / 32 ; <nl> + offset = ( var -> xoffset + ( var -> yoffset * info -> var . xres_virtual )) <nl> + * info -> var . bits_per_pixel / 32 ; <nl> set_screen_start ( par , offset ); <nl> debug (" exit \ n "); <nl> return 0 ;
static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
struct fb_info * fbtft_framebuffer_alloc ( struct fbtft_display * display , <nl> /* Transmit buffer */ <nl> if ( txbuflen == - 1 ) <nl> txbuflen = vmem_size + 2 ; /* add in case startbyte is used */ <nl> + if ( txbuflen >= vmem_size + 2 ) <nl> + txbuflen = 0 ; <nl>  <nl> # ifdef __LITTLE_ENDIAN <nl> if ((! txbuflen ) && ( bpp > 8 ))
static int ccdc_config_vdfc ( struct ccdc_vertical_dft * dfc ) <nl> */ <nl> static void ccdc_config_csc ( struct ccdc_csc * csc ) <nl> { <nl> - u32 val1 , val2 ; <nl> + u32 val1 = 0 , val2 ; <nl> int i ; <nl>  <nl> if (! csc -> enable )
static struct aead_edesc * aead_giv_edesc_alloc ( struct aead_givcrypt_request <nl> assoc_nents = assoc_nents ? : 1 ; <nl> src_nents = src_nents ? : 1 ; <nl> sec4_sg_len += assoc_nents + 1 + src_nents ; <nl> - if ( likely ( req -> src == req -> dst )) <nl> + if ( req -> src == req -> dst && <nl> + ( src_nents || iv_dma + ivsize != sg_dma_address ( req -> src ))) <nl> contig &= ~ GIV_DST_CONTIG ; <nl> } <nl> 
static int snd_seq_ioctl_remove_events ( struct snd_seq_client * client , <nl> * No restrictions so for a user client we can clear <nl> * the whole fifo <nl> */ <nl> - if ( client -> type == USER_CLIENT ) <nl> + if ( client -> type == USER_CLIENT && client -> data . user . fifo ) <nl> snd_seq_fifo_clear ( client -> data . user . fifo ); <nl> } <nl> 
static int fd_ioctl ( struct block_device * bdev , fmode_t mode , unsigned int cmd , <nl> (( cmd & 0x80 ) && ! capable ( CAP_SYS_ADMIN ))) <nl> return - EPERM ; <nl>  <nl> + if ( WARN_ON ( size < 0 || size > sizeof ( inparam ))) <nl> + return - EINVAL ; <nl> + <nl> /* copyin */ <nl> CLEARSTRUCT (& inparam ); <nl> if ( _IOC_DIR ( cmd ) & _IOC_WRITE )
do_sigbus ( struct pt_regs * regs , unsigned long error_code , unsigned long address , <nl> up_read (& mm -> mmap_sem ); <nl>  <nl> /* Kernel mode ? Handle exceptions or die : */ <nl> - if (!( error_code & PF_USER )) <nl> + if (!( error_code & PF_USER )) { <nl> no_context ( regs , error_code , address ); <nl> + return ; <nl> + } <nl>  <nl> /* User - space => ok to do another page fault : */ <nl> if ( is_prefetch ( regs , error_code , address ))
static struct platform_device_id armpmu_plat_device_ids [] = { <nl>  <nl> static int __devinit armpmu_device_probe ( struct platform_device * pdev ) <nl> { <nl> + if (! cpu_pmu ) <nl> + return - ENODEV ; <nl> + <nl> cpu_pmu -> plat_device = pdev ; <nl> return 0 ; <nl> }
static void e1000_get_ethtool_stats ( struct net_device * netdev , <nl> p = ( char *) adapter + <nl> e1000_gstrings_stats [ i ]. stat_offset ; <nl> break ; <nl> + default : <nl> + data [ i ] = 0 ; <nl> + continue ; <nl> } <nl>  <nl> data [ i ] = ( e1000_gstrings_stats [ i ]. sizeof_stat ==
static int pn533_target_found ( struct pn533 * dev , <nl> if ( resp -> tg != 1 ) <nl> return - EPROTO ; <nl>  <nl> + memset (& nfc_tgt , 0 , sizeof ( struct nfc_target )); <nl> + <nl> target_data_len = resp_len - sizeof ( struct pn533_poll_response ); <nl>  <nl> switch ( dev -> poll_mod_curr ) {
static int cp2112_gpio_direction_input ( struct gpio_chip * chip , unsigned offset ) <nl>  <nl> exit : <nl> mutex_unlock (& dev -> lock ); <nl> - return ret <= 0 ? ret : - EIO ; <nl> + return ret < 0 ? ret : - EIO ; <nl> } <nl>  <nl> static void cp2112_gpio_set ( struct gpio_chip * chip , unsigned offset , int value )
static int ecryptfs_write_begin ( struct file * file , <nl> && ( pos != 0 )) <nl> zero_user ( page , 0 , PAGE_CACHE_SIZE ); <nl> out : <nl> + if ( unlikely ( rc )) { <nl> + unlock_page ( page ); <nl> + page_cache_release ( page ); <nl> + * pagep = NULL ; <nl> + } <nl> return rc ; <nl> } <nl> 
struct ehci_hcd { /* one per controller */ <nl> # ifdef DEBUG <nl> struct dentry * debug_dir ; <nl> # endif <nl> + <nl> + /* platform - specific data -- must come last */ <nl> + unsigned long priv [ 0 ] __aligned ( sizeof ( s64 )); <nl> }; <nl>  <nl> /* convert between an HCD pointer and the corresponding EHCI_HCD */
static int emac_poll_rx ( void * param , int budget ) <nl> goto next ; <nl> } <nl>  <nl> + if ( len < ETH_HLEN ) { <nl> + ++ dev -> estats . rx_dropped_stack ; <nl> + emac_recycle_rx_skb ( dev , slot , len ); <nl> + goto next ; <nl> + } <nl> + <nl> if ( len && len < EMAC_RX_COPY_THRESH ) { <nl> struct sk_buff * copy_skb = <nl> alloc_skb ( len + EMAC_RX_SKB_HEADROOM + 2 , GFP_ATOMIC );
static int scsi_report_lun_scan ( struct scsi_target * starget , int bflags , <nl> out_err : <nl> kfree ( lun_data ); <nl> out : <nl> - scsi_device_put ( sdev ); <nl> if ( scsi_device_created ( sdev )) <nl> /* <nl> * the sdev we used didn ' t appear in the report luns scan <nl> */ <nl> __scsi_remove_device ( sdev ); <nl> + scsi_device_put ( sdev ); <nl> return ret ; <nl> } <nl> 
static int do_pages_stat ( struct mm_struct * mm , unsigned long nr_pages , <nl> int err ; <nl>  <nl> for ( i = 0 ; i < nr_pages ; i += chunk_nr ) { <nl> - if ( chunk_nr + i > nr_pages ) <nl> + if ( chunk_nr > nr_pages - i ) <nl> chunk_nr = nr_pages - i ; <nl>  <nl> err = copy_from_user ( chunk_pages , & pages [ i ],
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
static void recalibrate ( struct dp83640_clock * clock ) <nl> u16 cal_gpio , cfg0 , evnt , ptp_trig , trigger , val ; <nl>  <nl> trigger = CAL_TRIGGER ; <nl> - cal_gpio = gpio_tab [ CALIBRATE_GPIO ]; <nl> + cal_gpio = 1 + ptp_find_pin ( clock -> ptp_clock , PTP_PF_PHYSYNC , 0 ); <nl> + if ( cal_gpio < 1 ) { <nl> + pr_err (" PHY calibration pin not avaible - PHY is not calibrated ."); <nl> + return ; <nl> + } <nl>  <nl> mutex_lock (& clock -> extreg_lock ); <nl> 
static int cdrom_ioctl_select_disc ( struct cdrom_device_info * cdi , <nl> return - ENOSYS ; <nl>  <nl> if ( arg != CDSL_CURRENT && arg != CDSL_NONE ) { <nl> - if (( int ) arg >= cdi -> capacity ) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> } <nl> 
static int wm8731_hw_params ( struct snd_pcm_substream * substream , <nl> case 24 : <nl> iface |= 0x0008 ; <nl> break ; <nl> + case 32 : <nl> + iface |= 0x000c ; <nl> + break ; <nl> } <nl>  <nl> wm8731_set_deemph ( codec ); <nl> static int wm8731_startup ( struct snd_pcm_substream * substream , <nl> # define WM8731_RATES SNDRV_PCM_RATE_8000_96000 <nl>  <nl> # define WM8731_FORMATS ( SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S20_3LE |\ <nl> - SNDRV_PCM_FMTBIT_S24_LE ) <nl> + SNDRV_PCM_FMTBIT_S24_LE | SNDRV_PCM_FMTBIT_S32_LE ) <nl>  <nl> static const struct snd_soc_dai_ops wm8731_dai_ops = { <nl> . startup = wm8731_startup ,
static int __init fm10k_init_module ( void ) <nl> /* create driver workqueue */ <nl> fm10k_workqueue = alloc_workqueue ("% s ", WQ_MEM_RECLAIM , 0 , <nl> fm10k_driver_name ); <nl> + if (! fm10k_workqueue ) <nl> + return - ENOMEM ; <nl>  <nl> fm10k_dbg_init (); <nl> 
static inline void __fpu_invalidate_fpregs_state ( struct fpu * fpu ) <nl>  <nl> static inline int fpregs_state_valid ( struct fpu * fpu , unsigned int cpu ) <nl> { <nl> - return fpu == this_cpu_read_stable ( fpu_fpregs_owner_ctx ) && cpu == fpu -> last_cpu ; <nl> + return fpu == this_cpu_read ( fpu_fpregs_owner_ctx ) && cpu == fpu -> last_cpu ; <nl> } <nl>  <nl> /*
static ssize_t bundle_class_show ( struct device * dev , <nl> { <nl> struct gb_bundle * bundle = to_gb_bundle ( dev ); <nl>  <nl> - return sprintf ( buf , "% d \ n ", bundle -> class ); <nl> + return sprintf ( buf , " 0x % 02x \ n ", bundle -> class ); <nl> } <nl> static DEVICE_ATTR_RO ( bundle_class ); <nl> 
int rtl92ce_hw_init ( struct ieee80211_hw * hw ) <nl> local_save_flags ( flags ); <nl> local_irq_enable (); <nl>  <nl> + rtlhal -> fw_ready = false ; <nl> rtlpriv -> intf_ops -> disable_aspm ( hw ); <nl> rtstatus = _rtl92ce_init_mac ( hw ); <nl> if (! rtstatus ) { <nl> int rtl92ce_hw_init ( struct ieee80211_hw * hw ) <nl> goto exit ; <nl> } <nl>  <nl> + rtlhal -> fw_ready = true ; <nl> rtlhal -> last_hmeboxnum = 0 ; <nl> rtl92c_phy_mac_config ( hw ); <nl> /* because last function modify RCR , so we update
static int set_link_state ( struct ibmvnic_adapter * adapter , u8 link_state ) <nl> /* Partuial success , delay and re - send */ <nl> mdelay ( 1000 ); <nl> resend = true ; <nl> + } else if ( adapter -> init_done_rc ) { <nl> + netdev_warn ( netdev , " Unable to set link state , rc =% d \ n ", <nl> + adapter -> init_done_rc ); <nl> + return adapter -> init_done_rc ; <nl> } <nl> } while ( resend ); <nl> 
static int cdrom_read_cdda_bpc ( struct cdrom_device_info * cdi , __u8 __user * ubuf , <nl> if (! q ) <nl> return - ENXIO ; <nl>  <nl> + if (! blk_queue_scsi_passthrough ( q )) { <nl> + WARN_ONCE ( true , <nl> + " Attempt read CDDA info through a non - SCSI queue \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> cdi -> last_sense = 0 ; <nl>  <nl> while ( nframes ) {
batadv_iv_ogm_orig_get ( struct batadv_priv * bat_priv , const uint8_t * addr ) <nl> free_bcast_own : <nl> kfree ( orig_node -> bat_iv . bcast_own ); <nl> free_orig_node : <nl> + /* free twice , as batadv_orig_node_new sets refcount to 2 */ <nl> + batadv_orig_node_free_ref ( orig_node ); <nl> batadv_orig_node_free_ref ( orig_node ); <nl>  <nl> return NULL ;
static bool hist_browser__toggle_fold ( struct hist_browser * browser ) <nl> struct callchain_list * cl = container_of ( ms , struct callchain_list , ms ); <nl> bool has_children ; <nl>  <nl> + if (! he || ! ms ) <nl> + return false ; <nl> + <nl> if ( ms == & he -> ms ) <nl> has_children = hist_entry__toggle_fold ( he ); <nl> else
predicate_parse ( const char * str , int nr_parens , int nr_preds , <nl>  <nl> switch (* next ) { <nl> case '(': /* # 2 */ <nl> - if ( top - op_stack > nr_parens ) <nl> - return ERR_PTR (- EINVAL ); <nl> + if ( top - op_stack > nr_parens ) { <nl> + ret = - EINVAL ; <nl> + goto out_free ; <nl> + } <nl> *(++ top ) = invert ; <nl> continue ; <nl> case '!': /* # 3 */
int fib_dump_info ( struct sk_buff * skb , u32 portid , u32 seq , int event , <nl> IN_DEV_IGNORE_ROUTES_WITH_LINKDOWN ( in_dev )) <nl> rtm -> rtm_flags |= RTNH_F_DEAD ; <nl> } <nl> + if ( fi -> fib_nh -> nh_flags & RTNH_F_OFFLOAD ) <nl> + rtm -> rtm_flags |= RTNH_F_OFFLOAD ; <nl> # ifdef CONFIG_IP_ROUTE_CLASSID <nl> if ( fi -> fib_nh [ 0 ]. nh_tclassid && <nl> nla_put_u32 ( skb , RTA_FLOW , fi -> fib_nh [ 0 ]. nh_tclassid ))
static struct radio_si4713_platform_data rx51_si4713_data __initdata_or_module = <nl> . subdev_board_info = & rx51_si4713_board_info , <nl> }; <nl>  <nl> - static struct platform_device rx51_si4713_dev = { <nl> + static struct platform_device rx51_si4713_dev __initdata_or_module = { <nl> . name = " radio - si4713 ", <nl> . id = - 1 , <nl> . dev = {
struct cfg80211_bss * cfg80211_get_bss ( struct wiphy * wiphy , <nl> continue ; <nl> if ( channel && bss -> pub . channel != channel ) <nl> continue ; <nl> + if (! is_valid_ether_addr ( bss -> pub . bssid )) <nl> + continue ; <nl> /* Don ' t get expired BSS structs */ <nl> if ( time_after ( now , bss -> ts + IEEE80211_SCAN_RESULT_EXPIRE ) && <nl> ! atomic_read (& bss -> hold ))
static int ci_hdrc_usb2_probe ( struct platform_device * pdev ) <nl>  <nl> if (! ci_pdata ) { <nl> ci_pdata = devm_kmalloc ( dev , sizeof (* ci_pdata ), GFP_KERNEL ); <nl> + if (! ci_pdata ) <nl> + return - ENOMEM ; <nl> * ci_pdata = ci_default_pdata ; /* struct copy */ <nl> } <nl> 
static int storage_probe ( struct usb_interface * intf , <nl> return - ENOMEM ; <nl> } <nl>  <nl> + /* <nl> + * Allow 16 - byte CDBs and thus > 2TB <nl> + */ <nl> + host -> max_cmd_len = 16 ; <nl> us = host_to_us ( host ); <nl> memset ( us , 0 , sizeof ( struct us_data )); <nl> mutex_init (&( us -> dev_mutex ));
static int i2c_hid_hwreset ( struct i2c_client * client ) <nl> static void i2c_hid_get_input ( struct i2c_hid * ihid ) <nl> { <nl> int ret , ret_size ; <nl> - int size = le16_to_cpu ( ihid -> hdesc . wMaxInputLength ); <nl> + int size = ihid -> bufsize ; <nl>  <nl> ret = i2c_master_recv ( ihid -> client , ihid -> inbuf , size ); <nl> if ( ret != size ) {
SYSCALL_DEFINE4 ( epoll_ctl , int , epfd , int , op , int , fd , <nl> if ( op == EPOLL_CTL_ADD ) { <nl> if ( is_file_epoll ( tfile )) { <nl> error = - ELOOP ; <nl> - if ( ep_loop_check ( ep , tfile ) != 0 ) <nl> + if ( ep_loop_check ( ep , tfile ) != 0 ) { <nl> + clear_tfile_check_list (); <nl> goto error_tgt_fput ; <nl> + } <nl> } else <nl> list_add (& tfile -> f_tfile_llink , & tfile_check_list ); <nl> }
static int process_all_refs ( struct send_ctx * sctx , <nl> root = sctx -> parent_root ; <nl> cb = __record_deleted_ref ; <nl> } else { <nl> - BUG (); <nl> + btrfs_err ( sctx -> send_root -> fs_info , <nl> + " Wrong command % d in process_all_refs ", cmd ); <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> } <nl>  <nl> key . objectid = sctx -> cmp_key -> objectid ;
static int mspro_block_read_attributes ( struct memstick_dev * card ) <nl> snprintf ( s_attr -> name , sizeof ( s_attr -> name ), <nl> " attr_x % 02x ", attr -> entries [ cnt ]. id ); <nl>  <nl> + sysfs_attr_init (& s_attr -> dev_attr . attr ); <nl> s_attr -> dev_attr . attr . name = s_attr -> name ; <nl> s_attr -> dev_attr . attr . mode = S_IRUGO ; <nl> s_attr -> dev_attr . show = mspro_block_attr_show ( s_attr -> id );
static int sh_veu_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> * vdev = sh_veu_videodev ; <nl> + vdev -> v4l2_dev = & veu -> v4l2_dev ; <nl> spin_lock_init (& veu -> lock ); <nl> mutex_init (& veu -> fop_lock ); <nl> vdev -> lock = & veu -> fop_lock ;
static int mxs_lradc_probe ( struct platform_device * pdev ) <nl> * of the array . <nl> */ <nl> scale_uv = (( u64 ) lradc -> vref_mv [ i ] * 100000000 ) >> <nl> - ( iio -> channels [ i ]. scan_type . realbits - s ); <nl> + ( LRADC_RESOLUTION - s ); <nl> lradc -> scale_avail [ i ][ s ]. nano = <nl> do_div ( scale_uv , 100000000 ) * 10 ; <nl> lradc -> scale_avail [ i ][ s ]. integer = scale_uv ;
ff_layout_alloc_lseg ( struct pnfs_layout_hdr * lh , <nl> goto out_err_free ; <nl>  <nl> /* fh */ <nl> + rc = - EIO ; <nl> p = xdr_inline_decode (& stream , 4 ); <nl> if (! p ) <nl> goto out_err_free ;
static bool device_init_rings ( struct vnt_private * priv ) <nl> CB_MAX_BUF_SIZE , <nl> & priv -> tx_bufs_dma0 , <nl> GFP_ATOMIC ); <nl> - if ( priv -> tx0_bufs == NULL ) { <nl> + if (! priv -> tx0_bufs ) { <nl> dev_err (& priv -> pcid -> dev , " allocate buf dma memory failed \ n "); <nl>  <nl> dma_free_coherent (& priv -> pcid -> dev ,
struct ahash_request { <nl> void * __ctx [] CRYPTO_MINALIGN_ATTR ; <nl> }; <nl>  <nl> +# define AHASH_REQUEST_ON_STACK ( name , ahash ) \ <nl> + char __ ## name ## _desc [ sizeof ( struct ahash_request ) + \ <nl> + crypto_ahash_reqsize ( ahash )] CRYPTO_MINALIGN_ATTR ; \ <nl> + struct ahash_request * name = ( void *) __ ## name ## _desc <nl> + <nl> /** <nl> * struct ahash_alg - asynchronous message digest definition <nl> * @ init : Initialize the transformation context . Intended only to initialize the
static int ttm_bo_cleanup_refs_and_unlock ( struct ttm_buffer_object * bo , <nl> } <nl>  <nl> ttm_bo_del_from_lru ( bo ); <nl> + if (! list_empty (& bo -> ddestroy ) && ( bo -> resv != & bo -> ttm_resv )) <nl> + reservation_object_fini (& bo -> ttm_resv ); <nl> list_del_init (& bo -> ddestroy ); <nl> kref_put (& bo -> list_kref , ttm_bo_ref_bug ); <nl> 
static int match_session ( struct cifs_ses * ses , struct smb_vol * vol ) <nl> vol -> username ? vol -> username : "", <nl> CIFS_MAX_USERNAME_LEN )) <nl> return 0 ; <nl> - if ( strlen ( vol -> username ) != 0 && <nl> + if (( vol -> username && strlen ( vol -> username ) != 0 ) && <nl> ses -> password != NULL && <nl> strncmp ( ses -> password , <nl> vol -> password ? vol -> password : "",
int __init mon_text_init ( void ) <nl> { <nl> struct dentry * mondir ; <nl>  <nl> - mondir = debugfs_create_dir (" usbmon ", NULL ); <nl> + mondir = debugfs_create_dir (" usbmon ", usb_debug_root ); <nl> if ( IS_ERR ( mondir )) { <nl> printk ( KERN_NOTICE TAG ": debugfs is not available \ n "); <nl> return - ENODEV ;
struct qmp * qmp_get ( struct device * dev ) <nl>  <nl> qmp = platform_get_drvdata ( pdev ); <nl>  <nl> - return qmp ? qmp : ERR_PTR (- EPROBE_DEFER ); <nl> + if (! qmp ) { <nl> + put_device (& pdev -> dev ); <nl> + return ERR_PTR (- EPROBE_DEFER ); <nl> + } <nl> + return qmp ; <nl> } <nl> EXPORT_SYMBOL ( qmp_get ); <nl> 
qlafx00_soc_cpu_reset ( scsi_qla_host_t * vha ) <nl> /* Kick in Core0 to start boot process */ <nl> QLAFX00_SET_HBA_SOC_REG ( ha , SOC_SW_RST_CONTROL_REG_CORE0 , ( 0xF00 )); <nl>  <nl> + spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> + <nl> /* Wait 10secs for soft - reset to complete . */ <nl> for ( cnt = 10 ; cnt ; cnt --) { <nl> msleep ( 1000 ); <nl> barrier (); <nl> } <nl> - spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> } <nl>  <nl> /**
static long kfd_ioctl_create_queue ( struct file * filep , struct kfd_process * p , <nl> p -> pasid , <nl> dev -> id ); <nl>  <nl> - err = pqm_create_queue (& p -> pqm , dev , filep , & q_properties , 0 , <nl> - KFD_QUEUE_TYPE_COMPUTE , & queue_id ); <nl> + err = pqm_create_queue (& p -> pqm , dev , filep , & q_properties , <nl> + 0 , q_properties . type , & queue_id ); <nl> if ( err != 0 ) <nl> goto err_create_queue ; <nl> 
static int ad5686_write_raw ( struct iio_dev * indio_dev , <nl>  <nl> switch ( mask ) { <nl> case 0 : <nl> - if ( val > ( 1 << chan -> scan_type . realbits )) <nl> + if ( val > ( 1 << chan -> scan_type . realbits ) || val < 0 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& indio_dev -> mlock );
static const struct hda_fixup alc269_fixups [] = { <nl> [ ALC269_FIXUP_HEADSET_MODE ] = { <nl> . type = HDA_FIXUP_FUNC , <nl> . v . func = alc_fixup_headset_mode , <nl> + . chained = true , <nl> + . chain_id = ALC255_FIXUP_DELL_WMI_MIC_MUTE_LED <nl> }, <nl> [ ALC269_FIXUP_HEADSET_MODE_NO_HP_MIC ] = { <nl> . type = HDA_FIXUP_FUNC ,
bool dc_stream_set_cursor_position ( <nl> ! pipe_ctx -> ipp || ! pipe_ctx -> surface ) <nl> continue ; <nl>  <nl> + if ( pipe_ctx -> surface -> public . address . type <nl> + == PLN_ADDR_TYPE_VIDEO_PROGRESSIVE ) <nl> + pos_cpy . enable = false ; <nl> + <nl> if ( pipe_ctx -> top_pipe && pipe_ctx -> surface != pipe_ctx -> top_pipe -> surface ) <nl> pos_cpy . enable = false ; <nl> 
nv140_chipset = { <nl> . i2c = gm200_i2c_new , <nl> . ibus = gm200_ibus_new , <nl> . imem = nv50_instmem_new , <nl> + . ltc = gp102_ltc_new , <nl> . mc = gp100_mc_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new ,
int snd_usbmidi_create ( struct snd_card * card , <nl> else <nl> err = snd_usbmidi_create_endpoints ( umidi , endpoints ); <nl> if ( err < 0 ) { <nl> - snd_usbmidi_free ( umidi ); <nl> return err ; <nl> } <nl> 
static void cpufreq_policy_free ( struct cpufreq_policy * policy ) <nl>  <nl> static void update_policy_cpu ( struct cpufreq_policy * policy , unsigned int cpu ) <nl> { <nl> + if ( cpu == policy -> cpu ) <nl> + return ; <nl> + <nl> policy -> last_cpu = policy -> cpu ; <nl> policy -> cpu = cpu ; <nl> 
static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
static int goldfish_tty_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_tty_register_device_failed : <nl> - free_irq ( irq , pdev ); <nl> + free_irq ( irq , qtty ); <nl> err_request_irq_failed : <nl> goldfish_tty_current_line_count --; <nl> if ( goldfish_tty_current_line_count == 0 )
static int strip_open ( struct tty_struct * tty ) <nl> * We need a write method . <nl> */ <nl>  <nl> - if ( tty -> ops -> write == NULL ) <nl> + if ( tty -> ops -> write == NULL || tty -> ops -> set_termios == NULL ) <nl> return - EOPNOTSUPP ; <nl>  <nl> /*
static int _regmap_read ( struct regmap * map , unsigned int reg , <nl> if ( map -> cache_only ) <nl> return - EBUSY ; <nl>  <nl> + if (! regmap_readable ( map , reg )) <nl> + return - EIO ; <nl> + <nl> ret = map -> reg_read ( context , reg , val ); <nl> if ( ret == 0 ) { <nl> # ifdef LOG_DEVICE
static void snd_timer_user_tinterrupt ( struct snd_timer_instance * timeri , <nl> } <nl> if (( tu -> filter & ( 1 << SNDRV_TIMER_EVENT_RESOLUTION )) && <nl> tu -> last_resolution != resolution ) { <nl> + memset (& r1 , 0 , sizeof ( r1 )); <nl> r1 . event = SNDRV_TIMER_EVENT_RESOLUTION ; <nl> r1 . tstamp = tstamp ; <nl> r1 . val = resolution ;
static int device_authorization ( struct hdpvr_device * dev ) <nl> hex_dump_to_buffer ( response , 8 , 16 , 1 , print_buf , 5 * buf_size + 1 , 0 ); <nl> v4l2_dbg ( MSG_INFO , hdpvr_debug , & dev -> v4l2_dev , " response : % s \ n ", <nl> print_buf ); <nl> + kfree ( print_buf ); <nl> # endif <nl>  <nl> msleep ( 100 );
int tpm_open ( struct inode * inode , struct file * file ) <nl> return - EBUSY ; <nl> } <nl>  <nl> - chip -> data_buffer = kmalloc ( TPM_BUFSIZE * sizeof ( u8 ), GFP_KERNEL ); <nl> + chip -> data_buffer = kzalloc ( TPM_BUFSIZE , GFP_KERNEL ); <nl> if ( chip -> data_buffer == NULL ) { <nl> clear_bit ( 0 , & chip -> is_open ); <nl> put_device ( chip -> dev );
static void qusb2_phy_set_tune2_param ( struct qusb2_phy * qphy ) <nl> const struct qusb2_phy_cfg * cfg = qphy -> cfg ; <nl> u8 * val ; <nl>  <nl> + /* efuse register is optional */ <nl> + if (! qphy -> cell ) <nl> + return ; <nl> + <nl> /* <nl> * Read efuse register having TUNE2 / 1 parameter ' s high nibble . <nl> * If efuse register shows value as 0x0 , or if we fail to find
void vmw_kms_helper_resource_finish ( struct vmw_validation_ctx * ctx , <nl> vmw_kms_helper_buffer_finish ( res -> dev_priv , NULL , ctx -> buf , <nl> out_fence , NULL ); <nl>  <nl> + vmw_dmabuf_unreference (& ctx -> buf ); <nl> vmw_resource_unreserve ( res , false , NULL , 0 ); <nl> mutex_unlock (& res -> dev_priv -> cmdbuf_mutex ); <nl> }
static int cb_pcidas_attach ( struct comedi_device * dev , <nl>  <nl> /* 8255 */ <nl> s = dev -> subdevices + 2 ; <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + ret = subdev_8255_init ( dev , s , NULL , <nl> + devpriv -> pacer_counter_dio + DIO_8255 ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* serial EEPROM , */ <nl> s = dev -> subdevices + 3 ;
static int hwsim_new_radio_nl ( struct sk_buff * msg , struct genl_info * info ) <nl> if ( info -> attrs [ HWSIM_ATTR_CHANNELS ]) <nl> param . channels = nla_get_u32 ( info -> attrs [ HWSIM_ATTR_CHANNELS ]); <nl>  <nl> + if ( param . channels < 1 ) { <nl> + GENL_SET_ERR_MSG ( info , " must have at least one channel "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( param . channels > CFG80211_MAX_NUM_DIFFERENT_CHANNELS ) { <nl> GENL_SET_ERR_MSG ( info , " too many channels specified "); <nl> return - EINVAL ;
void efi_bgrt_init ( void ) <nl> if ( ACPI_FAILURE ( status )) <nl> return ; <nl>  <nl> + if ( bgrt_tab -> header . length < sizeof (* bgrt_tab )) <nl> + return ; <nl> if ( bgrt_tab -> version != 1 ) <nl> return ; <nl> if ( bgrt_tab -> image_type != 0 || ! bgrt_tab -> image_address )
snd_ali_playback_pointer ( struct snd_pcm_substream * substream ) <nl> spin_unlock (& codec -> reg_lock ); <nl> dev_dbg ( codec -> card -> dev , " playback pointer returned cso =% xh .\ n ", cso ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl>  <nl> static snd_pcm_uframes_t snd_ali_pointer ( struct snd_pcm_substream * substream ) <nl> cso = inw ( ALI_REG ( codec , ALI_CSO_ALPHA_FMS + 2 )); <nl> spin_unlock (& codec -> reg_lock ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl> 
static int iwl_nvm_read_section ( struct iwl_mvm * mvm , u16 section , <nl> offset += ret ; <nl> } <nl>  <nl> - IWL_INFO ( mvm , " NVM section % d read completed \ n ", section ); <nl> + IWL_DEBUG_EEPROM ( mvm -> trans -> dev , <nl> + " NVM section % d read completed \ n ", section ); <nl> return offset ; <nl> } <nl> 
int __init omap_mux_init ( u32 mux_pbase , u32 mux_size , <nl> } <nl>  <nl> # ifdef CONFIG_OMAP_MUX <nl> - omap_mux_package_fixup ( package_subset , superset ); <nl> - omap_mux_package_init_balls ( package_balls , superset ); <nl> + if ( package_subset ) <nl> + omap_mux_package_fixup ( package_subset , superset ); <nl> + if ( package_balls ) <nl> + omap_mux_package_init_balls ( package_balls , superset ); <nl> omap_mux_set_cmdline_signals (); <nl> omap_mux_set_board_signals ( board_mux ); <nl> # endif
static void sba_process_received_request ( struct sba_device * sba , <nl>  <nl> WARN_ON ( tx -> cookie < 0 ); <nl> if ( tx -> cookie > 0 ) { <nl> + spin_lock_irqsave (& sba -> reqs_lock , flags ); <nl> dma_cookie_complete ( tx ); <nl> + spin_unlock_irqrestore (& sba -> reqs_lock , flags ); <nl> dmaengine_desc_get_callback_invoke ( tx , NULL ); <nl> dma_descriptor_unmap ( tx ); <nl> tx -> callback = NULL ;
kvm_irqfd ( struct kvm * kvm , struct kvm_irqfd * args ) <nl> { <nl> if ( args -> flags & ~( KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE )) <nl> return - EINVAL ; <nl> + if ( args -> gsi >= KVM_MAX_IRQ_ROUTES ) <nl> + return - EINVAL ; <nl>  <nl> if ( args -> flags & KVM_IRQFD_FLAG_DEASSIGN ) <nl> return kvm_irqfd_deassign ( kvm , args );
static int atmel_ssc_set_dai_clkdiv ( struct snd_soc_dai * cpu_dai , <nl> * transmit and receive , so if a value has already <nl> * been set , it must match this value . <nl> */ <nl> - if ( ssc_p -> cmr_div == 0 ) <nl> + if ( ssc_p -> dir_mask != <nl> + ( SSC_DIR_MASK_PLAYBACK | SSC_DIR_MASK_CAPTURE )) <nl> + ssc_p -> cmr_div = div ; <nl> + else if ( ssc_p -> cmr_div == 0 ) <nl> ssc_p -> cmr_div = div ; <nl> else <nl> if ( div != ssc_p -> cmr_div )
static void ip6gre_tnl_link_config ( struct ip6_tnl * t , int set_mtu ) <nl> dev -> mtu = rt -> dst . dev -> mtu - addend ; <nl> if (!( t -> parms . flags & IP6_TNL_F_IGN_ENCAP_LIMIT )) <nl> dev -> mtu -= 8 ; <nl> + if ( dev -> type == ARPHRD_ETHER ) <nl> + dev -> mtu -= ETH_HLEN ; <nl>  <nl> if ( dev -> mtu < IPV6_MIN_MTU ) <nl> dev -> mtu = IPV6_MIN_MTU ;
static void copy_from_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy <nl>  <nl> static void copy_to_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy_info * p , int dir ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> sel , & xp -> selector , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & xp -> lft , sizeof ( p -> lft )); <nl> memcpy (& p -> curlft , & xp -> curlft , sizeof ( p -> curlft ));
static ssize_t rpmsg_eptdev_write_iter ( struct kiocb * iocb , <nl> if (! kbuf ) <nl> return - ENOMEM ; <nl>  <nl> - if (! copy_from_iter_full ( kbuf , len , from )) <nl> - return - EFAULT ; <nl> + if (! copy_from_iter_full ( kbuf , len , from )) { <nl> + ret = - EFAULT ; <nl> + goto free_kbuf ; <nl> + } <nl>  <nl> if ( mutex_lock_interruptible (& eptdev -> ept_lock )) { <nl> ret = - ERESTARTSYS ;
int brcmf_fweh_activate_events ( struct brcmf_if * ifp ) <nl> int i , err ; <nl> s8 eventmask [ BRCMF_EVENTING_MASK_LEN ]; <nl>  <nl> + memset ( eventmask , 0 , sizeof ( eventmask )); <nl> for ( i = 0 ; i < BRCMF_E_LAST ; i ++) { <nl> if ( ifp -> drvr -> fweh . evt_handler [ i ]) { <nl> brcmf_dbg ( EVENT , " enable event % s \ n ",
static int netvsc_start_xmit ( struct sk_buff * skb , struct net_device * net ) <nl> } else { <nl> /* we are shutting down or bus overloaded , just drop packet */ <nl> net -> stats . tx_dropped ++; <nl> - netvsc_xmit_completion ( packet ); <nl> + kfree ( packet ); <nl> + dev_kfree_skb_any ( skb ); <nl> } <nl>  <nl> return NETDEV_TX_OK ;
static int usbdev_open ( struct inode * inode , struct file * file ) <nl> if (! ps ) <nl> goto out ; <nl>  <nl> - ret = - ENOENT ; <nl> + ret = - ENODEV ; <nl>  <nl> /* usbdev device - node */ <nl> if ( imajor ( inode ) == USB_DEVICE_MAJOR )
static int ov5642_set_fmt ( struct v4l2_subdev * sd , <nl> mf -> field = V4L2_FIELD_NONE ; <nl>  <nl> if ( format -> which == V4L2_SUBDEV_FORMAT_ACTIVE ) <nl> - priv -> fmt = ov5642_find_datafmt ( mf -> code ); <nl> + priv -> fmt = fmt ; <nl> else <nl> cfg -> try_fmt = * mf ; <nl> return 0 ;
void blk_execute_rq_nowait ( struct request_queue * q , struct gendisk * bd_disk , <nl> spin_lock_irq ( q -> queue_lock ); <nl> __elv_add_request ( q , rq , where , 1 ); <nl> __generic_unplug_device ( q ); <nl> + /* the queue is stopped so it won ' t be plugged + unplugged */ <nl> + if ( blk_pm_resume_request ( rq )) <nl> + q -> request_fn ( q ); <nl> spin_unlock_irq ( q -> queue_lock ); <nl> } <nl> EXPORT_SYMBOL_GPL ( blk_execute_rq_nowait );
int netvsc_send ( struct hv_device * device , <nl> if (! net_device ) <nl> return - ENODEV ; <nl>  <nl> + /* We may race with netvsc_connect_vsp ()/ netvsc_init_buf () and get <nl> + * here before the negotiation with the host is finished and <nl> + * send_section_map may not be allocated yet . <nl> + */ <nl> + if (! net_device -> send_section_map ) <nl> + return - EAGAIN ; <nl> + <nl> out_channel = net_device -> chn_table [ q_idx ]; <nl>  <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ;
static int ioat_pci_probe ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl>  <nl> device -> version = readb ( device -> reg_base + IOAT_VER_OFFSET ); <nl> if ( device -> version >= IOAT_VER_3_0 ) { <nl> + if ( is_skx_ioat ( pdev )) <nl> + device -> version = IOAT_VER_3_2 ; <nl> err = ioat3_dma_probe ( device , ioat_dca_enabled ); <nl>  <nl> if ( device -> version >= IOAT_VER_3_3 )
int yama_task_prctl ( int option , unsigned long arg2 , unsigned long arg3 , <nl> if ( arg2 == 0 ) { <nl> yama_ptracer_del ( NULL , myself ); <nl> rc = 0 ; <nl> - } else if ( arg2 == PR_SET_PTRACER_ANY ) { <nl> + } else if ( arg2 == PR_SET_PTRACER_ANY || ( int ) arg2 == - 1 ) { <nl> rc = yama_ptracer_add ( NULL , myself ); <nl> } else { <nl> struct task_struct * tracer ;
EXPORT_SYMBOL_GPL ( crypto_init_spawn2 ); <nl>  <nl> void crypto_drop_spawn ( struct crypto_spawn * spawn ) <nl> { <nl> + if (! spawn -> alg ) <nl> + return ; <nl> + <nl> down_write (& crypto_alg_sem ); <nl> list_del (& spawn -> list ); <nl> up_write (& crypto_alg_sem );
static void __devinit pnv_pci_ioda_setup_seg ( void ) <nl> } <nl> } <nl>  <nl> + static void __devinit pnv_pci_ioda_setup_DMA ( void ) <nl> +{ <nl> + struct pci_controller * hose , * tmp ; <nl> + <nl> + list_for_each_entry_safe ( hose , tmp , & hose_list , list_node ) { <nl> + pnv_ioda_setup_dma ( hose -> private_data ); <nl> + } <nl> +} <nl> + <nl> static void __devinit pnv_pci_ioda_fixup ( void ) <nl> { <nl> pnv_pci_ioda_setup_PEs (); <nl> pnv_pci_ioda_setup_seg (); <nl> + pnv_pci_ioda_setup_DMA (); <nl> } <nl>  <nl> /*
int xhci_queue_bulk_tx ( struct xhci_hcd * xhci , gfp_t mem_flags , <nl> send_addr = addr ; <nl>  <nl> /* Queue the TRBs , even if they are zero - length */ <nl> - for ( enqd_len = 0 ; enqd_len < full_len ; enqd_len += trb_buff_len ) { <nl> + for ( enqd_len = 0 ; first_trb || enqd_len < full_len ; <nl> + enqd_len += trb_buff_len ) { <nl> field = TRB_TYPE ( TRB_NORMAL ); <nl>  <nl> /* TRB buffer should not cross 64KB boundaries */
static int usb_hcd_fsl_probe ( const struct hc_driver * driver , <nl> */ <nl> if ( pdata -> init && pdata -> init ( pdev )) { <nl> retval = - ENODEV ; <nl> - goto err3 ; <nl> + goto err4 ; <nl> } <nl>  <nl> /* Enable USB controller , 83xx or 8536 */
static int __open_dso ( struct dso * dso , struct machine * machine ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (! is_regular_file ( name )) <nl> + if (! is_regular_file ( name )) { <nl> + free ( name ); <nl> return - EINVAL ; <nl> + } <nl>  <nl> fd = do_open ( name ); <nl> free ( name );
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( ieee80211_vif_is_mesh (& sdata -> vif ) && <nl> ! sdata -> u . mesh . user_mpm ) <nl> init_timer (& sta -> plink_timer ); <nl> + sta -> nonpeer_pm = NL80211_MESH_POWER_ACTIVE ; <nl> # endif <nl>  <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN );
static int ad7877_rx ( struct ad7877 * ts ) <nl> Rt /= z1 ; <nl> Rt = ( Rt + 2047 ) >> 12 ; <nl>  <nl> + /* <nl> + * Sample found inconsistent , pressure is beyond <nl> + * the maximum . Don ' t report it to user space . <nl> + */ <nl> + if ( Rt > ts -> pressure_max ) <nl> + return - EINVAL ; <nl> + <nl> if (! timer_pending (& ts -> timer )) <nl> input_report_key ( input_dev , BTN_TOUCH , 1 ); <nl> 
static int cdrom_ioctl_drive_status ( struct cdrom_device_info * cdi , <nl> if (! CDROM_CAN ( CDC_SELECT_DISC ) || <nl> ( arg == CDSL_CURRENT || arg == CDSL_NONE )) <nl> return cdi -> ops -> drive_status ( cdi , CDSL_CURRENT ); <nl> - if ((( int ) arg >= cdi -> capacity )) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> return cdrom_slot_status ( cdi , arg ); <nl> }
static struct wlan_bssid_ex * collect_bss_info ( struct rtw_adapter * padapter , <nl> memcpy ( bssid -> Ssid . ssid , p + 2 , p [ 1 ]); <nl> bssid -> Ssid . ssid_len = p [ 1 ]; <nl>  <nl> - memset ( bssid -> SupportedRates , 0 , NDIS_802_11_LENGTH_RATES_EX ); <nl> - <nl> /* checking rate info ... */ <nl> i = 0 ; <nl> p = cfg80211_find_ie ( WLAN_EID_SUPP_RATES , bssid -> IEs + ie_offset ,
int assoc_array_gc ( struct assoc_array * array , <nl> shortcut = assoc_array_ptr_to_shortcut ( ptr ); <nl> slot = shortcut -> parent_slot ; <nl> cursor = shortcut -> back_pointer ; <nl> + if (! cursor ) <nl> + goto gc_complete ; <nl> } else { <nl> slot = node -> parent_slot ; <nl> cursor = ptr ; <nl> } <nl> - BUG_ON (! ptr ); <nl> + BUG_ON (! cursor ); <nl> node = assoc_array_ptr_to_node ( cursor ); <nl> slot ++; <nl> goto continue_node ;
static int virtnet_set_channels ( struct net_device * dev , <nl> if ( channels -> rx_count || channels -> tx_count || channels -> other_count ) <nl> return - EINVAL ; <nl>  <nl> - if ( queue_pairs > vi -> max_queue_pairs ) <nl> + if ( queue_pairs > vi -> max_queue_pairs || queue_pairs == 0 ) <nl> return - EINVAL ; <nl>  <nl> get_online_cpus ();
enum dc_status dce110_apply_ctx_to_hw ( <nl> if ( pipe_ctx -> stream == pipe_ctx_old -> stream ) <nl> continue ; <nl>  <nl> + if ( pipe_ctx -> stream && pipe_ctx_old -> stream <nl> + && ! pipe_need_reprogram ( pipe_ctx_old , pipe_ctx )) <nl> + continue ; <nl> + <nl> if ( pipe_ctx -> top_pipe ) <nl> continue ; <nl> 
static int __devinit rdc321x_wdt_probe ( struct platform_device * pdev ) <nl> struct resource * r ; <nl> struct rdc321x_wdt_pdata * pdata ; <nl>  <nl> - pdata = pdev -> dev . platform_data ; <nl> + pdata = platform_get_drvdata ( pdev ); <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " no platform data supplied \ n "); <nl> return - ENODEV ;
static int als_wait_for_data_ready ( struct device * dev ) <nl> ret = i2c_smbus_read_byte_data ( client , 0x86 ); <nl> } while (!( ret & 0x80 ) && retry --); <nl>  <nl> - if (! retry ) { <nl> + if ( retry < 0 ) { <nl> dev_warn ( dev , " timeout waiting for data ready \ n "); <nl> return - ETIMEDOUT ; <nl> }
static int __init sa1110_clk_init ( void ) <nl> struct sdram_params * sdram ; <nl> const char * name = sdram_name ; <nl>  <nl> + if (! cpu_is_sa1110 ()) <nl> + return - ENODEV ; <nl> + <nl> if (! name [ 0 ]) { <nl> if ( machine_is_assabet ()) <nl> name = " TC59SM716 - CL3 ";
static int validate_user_key ( struct fscrypt_info * crypt_info , <nl> goto out ; <nl> } <nl> ukp = user_key_payload_locked ( keyring_key ); <nl> + if (! ukp ) { <nl> + /* key was revoked before we acquired its semaphore */ <nl> + res = - EKEYREVOKED ; <nl> + goto out ; <nl> + } <nl> if ( ukp -> datalen != sizeof ( struct fscrypt_key )) { <nl> res = - EINVAL ; <nl> goto out ;
static int annotate_browser__run ( struct annotate_browser * self , int evidx , <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' H ': <nl> + case ' h ': <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' S ': <nl> + case ' s ': <nl> if ( annotate_browser__toggle_source ( self )) <nl> ui_helpline__puts ( help ); <nl> continue ;
int ath10k_wmi_start_scan ( struct ath10k * ar , <nl> return ret ; <nl>  <nl> if ( test_bit ( ATH10K_FW_FEATURE_WMI_10X , ar -> fw_features )) <nl> - len = sizeof ( struct wmi_start_scan_cmd ) + <nl> + len = sizeof ( struct wmi_10x_start_scan_cmd ) + <nl> ath10k_wmi_start_scan_tlvs_len ( arg ); <nl> else <nl> - len = sizeof ( struct wmi_10x_start_scan_cmd ) + <nl> + len = sizeof ( struct wmi_start_scan_cmd ) + <nl> ath10k_wmi_start_scan_tlvs_len ( arg ); <nl>  <nl> skb = ath10k_wmi_alloc_skb ( ar , len );
static int saa7134_s_fmt_overlay ( struct file * file , void * priv , <nl> struct saa7134_fh * fh = priv ; <nl> struct saa7134_dev * dev = fh -> dev ; <nl> int err ; <nl> - unsigned int flags ; <nl> + unsigned long flags ; <nl>  <nl> if ( saa7134_no_overlay > 0 ) { <nl> printk ( KERN_ERR " V4L2_BUF_TYPE_VIDEO_OVERLAY : no_overlay \ n ");
static long pmcraid_ioctl_passthrough ( <nl> pmcraid_err (" couldn ' t build passthrough ioadls \ n "); <nl> goto out_free_buffer ; <nl> } <nl> + } else if ( request_size < 0 ) { <nl> + rc = - EINVAL ; <nl> + goto out_free_buffer ; <nl> } <nl>  <nl> /* If data is being written into the device , copy the data from user
static irqreturn_t rtl8169_interrupt ( int irq , void * dev_instance ) <nl> handled = 1 ; <nl>  <nl> rtl_irq_disable ( tp ); <nl> - napi_schedule (& tp -> napi ); <nl> + napi_schedule_irqoff (& tp -> napi ); <nl> } <nl> } <nl> return IRQ_RETVAL ( handled );
static inline int uinput_request_reserve_slot ( struct uinput_device * udev , struct <nl>  <nl> static void uinput_request_done ( struct uinput_device * udev , struct uinput_request * request ) <nl> { <nl> - complete (& request -> done ); <nl> - <nl> /* Mark slot as available */ <nl> udev -> requests [ request -> id ] = NULL ; <nl> wake_up_interruptible (& udev -> requests_waitq ); <nl> + <nl> + complete (& request -> done ); <nl> } <nl>  <nl> static int uinput_request_submit ( struct input_dev * dev , struct uinput_request * request )
void arch_update_cpu_topology ( void ); <nl> . busy_idx = 3 , \ <nl> . idle_idx = 3 , \ <nl> . flags = SD_LOAD_BALANCE \ <nl> - | SD_SERIALIZE , \ <nl> + | SD_BALANCE_NEWIDLE \ <nl> + | SD_WAKE_AFFINE \ <nl> + | SD_SERIALIZE , \ <nl> . last_balance = jiffies , \ <nl> . balance_interval = 64 , \ <nl> }
static int kill_something_info ( int sig , struct siginfo * info , pid_t pid ) <nl> return ret ; <nl> } <nl>  <nl> + /* - INT_MIN is undefined . Exclude this case to avoid a UBSAN warning */ <nl> + if ( pid == INT_MIN ) <nl> + return - ESRCH ; <nl> + <nl> read_lock (& tasklist_lock ); <nl> if ( pid != - 1 ) { <nl> ret = __kill_pgrp_info ( sig , info ,
ufs_extend_tail ( struct inode * inode , u64 writes_to , <nl>  <nl> p = ufs_get_direct_data_ptr ( uspi , ufsi , block ); <nl> tmp = ufs_new_fragments ( inode , p , lastfrag , ufs_data_ptr_to_cpu ( sb , p ), <nl> - new_size , err , locked_page ); <nl> + new_size - ( lastfrag & uspi -> s_fpbmask ), err , <nl> + locked_page ); <nl> return tmp != 0 ; <nl> } <nl> 
static struct dentry * aio_mount ( struct file_system_type * fs_type , <nl> static const struct dentry_operations ops = { <nl> . d_dname = simple_dname , <nl> }; <nl> - return mount_pseudo ( fs_type , " aio :", NULL , & ops , AIO_RING_MAGIC ); <nl> + struct dentry * root = mount_pseudo ( fs_type , " aio :", NULL , & ops , <nl> + AIO_RING_MAGIC ); <nl> + <nl> + if (! IS_ERR ( root )) <nl> + root -> d_sb -> s_iflags |= SB_I_NOEXEC ; <nl> + return root ; <nl> } <nl>  <nl> /* aio_setup
static ssize_t unix_stream_sendpage ( struct socket * socket , struct page * page , <nl> * this - does no harm <nl> */ <nl> consume_skb ( newskb ); <nl> + newskb = NULL ; <nl> } <nl>  <nl> if ( skb_append_pagefrags ( skb , page , offset , size )) {
+// SPDX - License - Identifier : GPL - 2 . 0 <nl> /* <nl> - * linux / drivers / pinctrl / pinctrl - rt2880 . c <nl> - * <nl> - * This program is free software ; you can redistribute it and / or modify <nl> - * it under the terms of the GNU General Public License version 2 as <nl> - * publishhed by the Free Software Foundation . <nl> - * <nl> * Copyright ( C ) 2013 John Crispin < blogic @ openwrt . org > <nl> */ <nl> 
static struct pci_driver jr3_pci_pci_driver = { <nl> module_comedi_pci_driver ( jr3_pci_driver , jr3_pci_pci_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for JR3 / PCI force sensor board "); <nl> MODULE_LICENSE (" GPL "); <nl> MODULE_FIRMWARE (" comedi / jr3pci . idm ");
int sas_smp_get_phy_events ( struct sas_phy * phy ) <nl> phy -> phy_reset_problem_count = scsi_to_u32 (& resp [ 24 ]); <nl>  <nl> out : <nl> + kfree ( req ); <nl> kfree ( resp ); <nl> return res ; <nl> 
static ssize_t rng_dev_read ( struct file * filp , char __user * buf , <nl> err = - EAGAIN ; <nl> if (! bytes_read && ( filp -> f_flags & O_NONBLOCK )) <nl> goto out ; <nl> + if ( bytes_read < 0 ) { <nl> + err = bytes_read ; <nl> + goto out ; <nl> + } <nl>  <nl> err = - EFAULT ; <nl> while ( bytes_read && size ) {
static void rt2800pci_txstatus_interrupt ( struct rt2x00_dev * rt2x00dev ) <nl> * Since we have only one producer and one consumer we don ' t <nl> * need to lock the kfifo . <nl> */ <nl> - for ( i = 0 ; i < rt2x00dev -> ops -> tx -> entry_num ; i ++) { <nl> + for ( i = 0 ; i < rt2x00dev -> tx -> limit ; i ++) { <nl> rt2x00mmio_register_read ( rt2x00dev , TX_STA_FIFO , & status ); <nl>  <nl> if (! rt2x00_get_field32 ( status , TX_STA_FIFO_VALID ))
static void ath9k_hw_ar9300_set_txpower ( struct ath_hw * ah , <nl> regulatory -> max_power_level = targetPowerValT2 [ i ]; <nl> } <nl>  <nl> + ath9k_hw_update_regulatory_maxpower ( ah ); <nl> + <nl> if ( test ) <nl> return ; <nl> 
static int davinci_gpio_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& chips [ i ]. lock ); <nl>  <nl> regs = gpio2regs ( base ); <nl> + if (! regs ) <nl> + return - ENXIO ; <nl> chips [ i ]. regs = regs ; <nl> chips [ i ]. set_data = & regs -> set_data ; <nl> chips [ i ]. clr_data = & regs -> clr_data ;
static int rtl8192eu_parse_efuse ( struct rtl8xxxu_priv * priv ) <nl> raw [ i + 6 ], raw [ i + 7 ]); <nl> } <nl> } <nl> + /* <nl> + * Temporarily disable 8192eu support <nl> + */ <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static int eb_relocate_vma ( struct i915_execbuffer * eb , struct i915_vma * vma ) <nl> * to read . However , if the array is not writable the user loses <nl> * the updated relocation values . <nl> */ <nl> - if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof ( urelocs )))) <nl> + if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof (* urelocs )))) <nl> return - EFAULT ; <nl>  <nl> do {
struct ata_host * ata_host_alloc ( struct device * dev , int max_ports ) <nl> return NULL ; <nl>  <nl> if (! devres_open_group ( dev , NULL , GFP_KERNEL )) <nl> - return NULL ; <nl> + goto err_free ; <nl>  <nl> dr = devres_alloc ( ata_devres_release , 0 , GFP_KERNEL ); <nl> if (! dr ) <nl> struct ata_host * ata_host_alloc ( struct device * dev , int max_ports ) <nl>  <nl> err_out : <nl> devres_release_group ( dev , NULL ); <nl> + err_free : <nl> + kfree ( host ); <nl> return NULL ; <nl> } <nl> 
static int iwl_store_ucode_sec ( struct iwl_firmware_pieces * pieces , <nl>  <nl> sec -> offset = le32_to_cpu ( sec_parse -> offset ); <nl> sec -> data = sec_parse -> data ; <nl> + sec -> size = size - sizeof ( sec_parse -> offset ); <nl>  <nl> ++ img -> sec_counter ; <nl> 
static u32 slic_card_locate ( struct adapter * adapter ) <nl> if (! physcard ) { <nl> /* no structure allocated for this physical card yet */ <nl> physcard = kzalloc ( sizeof ( struct physcard ), GFP_ATOMIC ); <nl> - if (! physcard ) <nl> + if (! physcard ) { <nl> + if ( card_hostid == SLIC_HOSTID_DEFAULT ) <nl> + kfree ( card ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> physcard -> next = slic_global . phys_card ; <nl> slic_global . phys_card = physcard ;
acpi_ps_get_next_arg ( struct acpi_walk_state * walk_state , <nl> ACPI_POSSIBLE_METHOD_CALL ); <nl>  <nl> if ( arg -> common . aml_opcode == AML_INT_METHODCALL_OP ) { <nl> + <nl> + /* Free method call op and corresponding namestring sub - ob */ <nl> + <nl> + acpi_ps_free_op ( arg -> common . value . arg ); <nl> acpi_ps_free_op ( arg ); <nl> arg = NULL ; <nl> walk_state -> arg_count = 1 ;
qla2x00_do_dpc ( void * data ) <nl> } else { <nl> fcport -> login_retry = 0 ; <nl> } <nl> - if ( fcport -> login_retry == 0 ) <nl> + if ( fcport -> login_retry == 0 && status != QLA_SUCCESS ) <nl> fcport -> loop_id = FC_NO_LOOP_ID ; <nl> } <nl> if ( test_bit ( LOOP_RESYNC_NEEDED , & ha -> dpc_flags ))
struct ttm_tt * ttm_tt_create ( struct ttm_bo_device * bdev , unsigned long size , <nl> ttm -> dummy_read_page = dummy_read_page ; <nl>  <nl> ttm_tt_alloc_page_directory ( ttm ); <nl> - if (! ttm -> pages ) { <nl> + if (! ttm -> pages || ! ttm -> dma_address ) { <nl> ttm_tt_destroy ( ttm ); <nl> printk ( KERN_ERR TTM_PFX " Failed allocating page table \ n "); <nl> return NULL ;
struct extent_buffer * read_tree_block ( struct btrfs_root * root , u64 bytenr , <nl> return NULL ; <nl>  <nl> ret = btree_read_extent_buffer_pages ( root , buf , 0 , parent_transid ); <nl> + if ( ret ) { <nl> + free_extent_buffer ( buf ); <nl> + return NULL ; <nl> + } <nl> return buf ; <nl>  <nl> }
int acpi_bus_start ( struct acpi_device * device ) <nl> { <nl> struct acpi_bus_ops ops ; <nl>  <nl> + if (! device ) <nl> + return - EINVAL ; <nl> + <nl> memset (& ops , 0 , sizeof ( ops )); <nl> ops . acpi_op_start = 1 ; <nl> 
static int acpi_battery_technology ( struct acpi_battery * battery ) <nl> return POWER_SUPPLY_TECHNOLOGY_NiMH ; <nl> if (! strcasecmp (" LION ", battery -> type )) <nl> return POWER_SUPPLY_TECHNOLOGY_LION ; <nl> + if (! strcasecmp (" LI - ION ", battery -> type )) <nl> + return POWER_SUPPLY_TECHNOLOGY_LION ; <nl> if (! strcasecmp (" LiP ", battery -> type )) <nl> return POWER_SUPPLY_TECHNOLOGY_LIPO ; <nl> return POWER_SUPPLY_TECHNOLOGY_UNKNOWN ;
static int gb_uart_flush ( struct gb_tty * gb_tty , u8 flags ) <nl> & request , sizeof ( request ), NULL , 0 ); <nl> } <nl>  <nl> - static struct gb_tty * get_gb_by_minor ( unsigned minor ) <nl> + static struct gb_tty * get_gb_by_minor ( unsigned int minor ) <nl> { <nl> struct gb_tty * gb_tty ; <nl> 
static u32 Handle_ListenStateExpired ( struct host_if_drv * hif_drv , <nl> wid . size = 2 ; <nl> wid . val = kmalloc ( wid . size , GFP_KERNEL ); <nl>  <nl> - if (! wid . val ) <nl> + if (! wid . val ) { <nl> PRINT_ER (" Failed to allocate memory \ n "); <nl> + return - ENOMEM ; <nl> + } <nl>  <nl> wid . val [ 0 ] = u8remain_on_chan_flag ; <nl> wid . val [ 1 ] = FALSE_FRMWR_CHANNEL ;
qla2x00_configure_fabric ( scsi_qla_host_t * vha ) <nl> fcport -> d_id . b . domain , <nl> fcport -> d_id . b . area , <nl> fcport -> d_id . b . al_pa ); <nl> - fcport -> loop_id = FC_NO_LOOP_ID ; <nl> + qla2x00_clear_loop_id ( fcport ); <nl> } <nl> } <nl> }
static int ixgbe_xdp ( struct net_device * dev , struct netdev_xdp * xdp ) <nl> return ixgbe_xdp_setup ( dev , xdp -> prog ); <nl> case XDP_QUERY_PROG : <nl> xdp -> prog_attached = !!( adapter -> xdp_prog ); <nl> + xdp -> prog_id = adapter -> xdp_prog ? <nl> + adapter -> xdp_prog -> aux -> id : 0 ; <nl> return 0 ; <nl> default : <nl> return - EINVAL ;
int jbd2_journal_start_reserved ( handle_t * handle , unsigned int type , <nl> */ <nl> ret = start_this_handle ( journal , handle , GFP_NOFS ); <nl> if ( ret < 0 ) { <nl> + handle -> h_journal = journal ; <nl> jbd2_journal_free_reserved ( handle ); <nl> return ret ; <nl> }
static void ax25_kill_by_device ( struct net_device * dev ) <nl> ax25_for_each ( s , & ax25_list ) { <nl> if ( s -> ax25_dev == ax25_dev ) { <nl> sk = s -> sk ; <nl> + if (! sk ) { <nl> + spin_unlock_bh (& ax25_list_lock ); <nl> + s -> ax25_dev = NULL ; <nl> + ax25_disconnect ( s , ENETUNREACH ); <nl> + spin_lock_bh (& ax25_list_lock ); <nl> + goto again ; <nl> + } <nl> sock_hold ( sk ); <nl> spin_unlock_bh (& ax25_list_lock ); <nl> lock_sock ( sk );
int intel_atomic_check ( struct drm_device * dev , <nl> state -> allow_modeset = false ; <nl> for ( i = 0 ; i < ncrtcs ; i ++) { <nl> struct intel_crtc * crtc = to_intel_crtc ( state -> crtcs [ i ]); <nl> + if ( crtc ) <nl> + memset (& crtc -> atomic , 0 , sizeof ( crtc -> atomic )); <nl> if ( crtc && crtc -> pipe != nuclear_pipe ) <nl> not_nuclear = true ; <nl> }
void snd_pcm_period_elapsed ( struct snd_pcm_substream * substream ) <nl> snd_timer_interrupt ( substream -> timer , 1 ); <nl> # endif <nl> _end : <nl> - snd_pcm_stream_unlock_irqrestore ( substream , flags ); <nl> kill_fasync (& runtime -> fasync , SIGIO , POLL_IN ); <nl> + snd_pcm_stream_unlock_irqrestore ( substream , flags ); <nl> } <nl>  <nl> EXPORT_SYMBOL ( snd_pcm_period_elapsed );
static int gic_shared_irq_domain_map ( struct irq_domain * d , unsigned int virq , <nl>  <nl> spin_lock_irqsave (& gic_lock , flags ); <nl> gic_map_to_pin ( intr , gic_cpu_pin ); <nl> - gic_map_to_vpe ( intr , vpe ); <nl> + gic_map_to_vpe ( intr , mips_cm_vp_id ( vpe )); <nl> for ( i = 0 ; i < min ( gic_vpes , NR_CPUS ); i ++) <nl> clear_bit ( intr , pcpu_masks [ i ]. pcpu_mask ); <nl> set_bit ( intr , pcpu_masks [ vpe ]. pcpu_mask );
static int usbdev_open ( struct inode * inode , struct file * file ) <nl> struct dev_state * ps ; <nl> int ret ; <nl>  <nl> + lock_kernel (); <nl> /* Protect against simultaneous removal or release */ <nl> mutex_lock (& usbfs_mutex ); <nl>  <nl> static int usbdev_open ( struct inode * inode , struct file * file ) <nl> if ( ret ) <nl> kfree ( ps ); <nl> mutex_unlock (& usbfs_mutex ); <nl> + unlock_kernel (); <nl> return ret ; <nl> } <nl> 
static int rndis_wlan_bind ( struct usbnet * usbdev , struct usb_interface * intf ) <nl>  <nl> /* because rndis_command () sleeps we need to use workqueue */ <nl> priv -> workqueue = create_singlethread_workqueue (" rndis_wlan "); <nl> + if (! priv -> workqueue ) { <nl> + wiphy_free ( wiphy ); <nl> + return - ENOMEM ; <nl> + } <nl> INIT_WORK (& priv -> work , rndis_wlan_worker ); <nl> INIT_DELAYED_WORK (& priv -> dev_poller_work , rndis_device_poller ); <nl> INIT_DELAYED_WORK (& priv -> scan_work , rndis_get_scan_results );
static int nvm_get_dir_info ( struct net_device * dev , u32 * entries , u32 * length ) <nl>  <nl> static int bnxt_get_eeprom_len ( struct net_device * dev ) <nl> { <nl> + struct bnxt * bp = netdev_priv ( dev ); <nl> + <nl> + if ( BNXT_VF ( bp )) <nl> + return 0 ; <nl> + <nl> /* The - 1 return value allows the entire 32 - bit range of offsets to be <nl> * passed via the ethtool command - line utility . <nl> */
static struct parser_context * parser_init_byte_stream ( u64 addr , u32 bytes , <nl> return ctx ; <nl>  <nl> err_finish_ctx : <nl> - parser_done ( ctx ); <nl> + kfree ( ctx ); <nl> return NULL ; <nl> } <nl> 
int hugetlb_fault ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> * so no worry about deadlock . <nl> */ <nl> page = pte_page ( entry ); <nl> + get_page ( page ); <nl> if ( page != pagecache_page ) <nl> lock_page ( page ); <nl>  <nl> int hugetlb_fault ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> } <nl> if ( page != pagecache_page ) <nl> unlock_page ( page ); <nl> + put_page ( page ); <nl>  <nl> out_mutex : <nl> mutex_unlock (& hugetlb_instantiation_mutex );
asmlinkage long sys_migrate_pages ( pid_t pid , unsigned long maxnode , <nl> goto out ; <nl> } <nl>  <nl> + if (! nodes_subset ( new , node_online_map )) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> err = security_task_movememory ( task ); <nl> if ( err ) <nl> goto out ;
static void gfx_v8_0_ring_emit_vm_flush ( struct amdgpu_ring * ring , <nl>  <nl> amdgpu_ring_write ( ring , PACKET3 ( PACKET3_WAIT_REG_MEM , 5 )); <nl> amdgpu_ring_write ( ring , ( WAIT_REG_MEM_MEM_SPACE ( 1 ) | /* memory */ <nl> - WAIT_REG_MEM_FUNCTION ( 3 ))); /* equal */ <nl> + WAIT_REG_MEM_FUNCTION ( 3 ) | /* equal */ <nl> + WAIT_REG_MEM_ENGINE ( usepfp ))); /* pfp or me */ <nl> amdgpu_ring_write ( ring , addr & 0xfffffffc ); <nl> amdgpu_ring_write ( ring , upper_32_bits ( addr ) & 0xffffffff ); <nl> amdgpu_ring_write ( ring , seq );
static int apparmor_setprocattr ( struct task_struct * task , char * name , <nl> sa . aad . op = OP_SETPROCATTR ; <nl> sa . aad . info = name ; <nl> sa . aad . error = - EINVAL ; <nl> - return aa_audit ( AUDIT_APPARMOR_DENIED , NULL , GFP_KERNEL , <nl> + return aa_audit ( AUDIT_APPARMOR_DENIED , <nl> + __aa_current_profile (), GFP_KERNEL , <nl> & sa , NULL ); <nl> } <nl> } else if ( strcmp ( name , " exec ") == 0 ) {
static void init_once ( void * foo ) <nl>  <nl> static struct inode * befs_iget ( struct super_block * sb , unsigned long ino ) <nl> { <nl> - struct buffer_head * bh = NULL ; <nl> + struct buffer_head * bh ; <nl> befs_inode * raw_inode = NULL ; <nl> struct befs_sb_info * befs_sb = BEFS_SB ( sb ); <nl> struct befs_inode_info * befs_ino = NULL ;
static void t3_io_resume ( struct pci_dev * pdev ) <nl> CH_ALERT ( adapter , " adapter recovering , PEX ERR 0x % x \ n ", <nl> t3_read_reg ( adapter , A_PCIE_PEX_ERR )); <nl>  <nl> + rtnl_lock (); <nl> t3_resume_ports ( adapter ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> static const struct pci_error_handlers t3_err_handler = {
dev_config ( struct file * fd , const char __user * buf , size_t len , loff_t * ptr ) <nl> spin_lock_irq (& dev -> lock ); <nl> value = - EINVAL ; <nl> if ( dev -> buf ) { <nl> + spin_unlock_irq (& dev -> lock ); <nl> kfree ( kbuf ); <nl> - goto fail ; <nl> + return value ; <nl> } <nl> dev -> buf = kbuf ; <nl> 
static unsigned long axi_clkgen_recalc_rate ( struct clk_hw * clk_hw , <nl> tmp = ( unsigned long long )( parent_rate / d ) * m ; <nl> do_div ( tmp , dout ); <nl>  <nl> - if ( tmp > ULONG_MAX ) <nl> - return ULONG_MAX ; <nl> - <nl> - return tmp ; <nl> + return min_t ( unsigned long long , tmp , ULONG_MAX ); <nl> } <nl>  <nl> static int axi_clkgen_enable ( struct clk_hw * clk_hw )
static void process_checks ( struct r1bio * r1_bio ) <nl> struct page ** ppages = get_resync_pages ( pbio )-> pages ; <nl> struct page ** spages = get_resync_pages ( sbio )-> pages ; <nl> struct bio_vec * bi ; <nl> - int page_len [ RESYNC_PAGES ]; <nl> + int page_len [ RESYNC_PAGES ] = { 0 }; <nl>  <nl> if ( sbio -> bi_end_io != end_sync_read ) <nl> continue ;
static int fill_inode ( struct inode * inode , struct page * locked_page , <nl> } <nl>  <nl> /* finally update i_version */ <nl> - ci -> i_version = le64_to_cpu ( info -> version ); <nl> + if ( le64_to_cpu ( info -> version ) > ci -> i_version ) <nl> + ci -> i_version = le64_to_cpu ( info -> version ); <nl>  <nl> inode -> i_mapping -> a_ops = & ceph_aops ; <nl> 
tracing_iter_ctrl_write ( struct file * filp , const char __user * ubuf , <nl> break ; <nl> } <nl> } <nl> + /* <nl> + * If no option could be set , return an error : <nl> + */ <nl> + if (! trace_options [ i ]) <nl> + return - EINVAL ; <nl>  <nl> filp -> f_pos += cnt ; <nl> 
drm_atomic_plane_get_property ( struct drm_plane * plane , <nl> * val = state -> src_w ; <nl> } else if ( property == config -> prop_src_h ) { <nl> * val = state -> src_h ; <nl> + } else if ( property == config -> rotation_property ) { <nl> + * val = state -> rotation ; <nl> } else if ( plane -> funcs -> atomic_get_property ) { <nl> return plane -> funcs -> atomic_get_property ( plane , state , property , val ); <nl> } else {
static int hi3660_stub_clk_probe ( struct platform_device * pdev ) <nl> return PTR_ERR ( stub_clk_chan . mbox ); <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> + if (! res ) <nl> + return - EINVAL ; <nl> freq_reg = devm_ioremap ( dev , res -> start , resource_size ( res )); <nl> if (! freq_reg ) <nl> return - ENOMEM ;
lpfc_els_retry ( struct lpfc_hba * phba , struct lpfc_iocbq * cmdiocb , <nl> /* FLOGI retry policy */ <nl> retry = 1 ; <nl> /* retry FLOGI forever */ <nl> - maxretry = 0 ; <nl> + if ( phba -> link_flag != LS_LOOPBACK_MODE ) <nl> + maxretry = 0 ; <nl> + else <nl> + maxretry = 2 ; <nl> + <nl> if ( cmdiocb -> retry >= 100 ) <nl> delay = 5000 ; <nl> else if ( cmdiocb -> retry >= 32 )
static void tilcdc_crtc_destroy ( struct drm_crtc * crtc ) <nl> struct tilcdc_crtc * tilcdc_crtc = to_tilcdc_crtc ( crtc ); <nl> struct tilcdc_drm_private * priv = crtc -> dev -> dev_private ; <nl>  <nl> + drm_modeset_lock_crtc ( crtc , NULL ); <nl> tilcdc_crtc_disable ( crtc ); <nl> + drm_modeset_unlock_crtc ( crtc ); <nl>  <nl> flush_workqueue ( priv -> wq ); <nl> 
static int __devexit wl1271_remove ( struct spi_device * spi ) <nl>  <nl> static struct spi_driver wl1271_spi_driver = { <nl> . driver = { <nl> - . name = " wl1271 ", <nl> + . name = " wl1271_spi ", <nl> . bus = & spi_bus_type , <nl> . owner = THIS_MODULE , <nl> },
xfs_dialloc_ag_inobt ( <nl>  <nl> /* free inodes to the left ? */ <nl> if ( useleft && trec . ir_freecount ) { <nl> - rec = trec ; <nl> xfs_btree_del_cursor ( cur , XFS_BTREE_NOERROR ); <nl> cur = tcur ; <nl>  <nl> pag -> pagl_leftrec = trec . ir_startino ; <nl> pag -> pagl_rightrec = rec . ir_startino ; <nl> pag -> pagl_pagino = pagino ; <nl> + rec = trec ; <nl> goto alloc_inode ; <nl> } <nl> 
static int journal_unmap_buffer ( journal_t * journal , struct buffer_head * bh ) <nl> clear_buffer_mapped ( bh ); <nl> clear_buffer_req ( bh ); <nl> clear_buffer_new ( bh ); <nl> + clear_buffer_delay ( bh ); <nl> + clear_buffer_unwritten ( bh ); <nl> bh -> b_bdev = NULL ; <nl> return may_free ; <nl> }
int fcoe_ctlr_els_send ( struct fcoe_ctlr * fip , struct fc_lport * lport , <nl> fip -> send ( fip , skb ); <nl> return - EINPROGRESS ; <nl> drop : <nl> - kfree_skb ( skb ); <nl> LIBFCOE_FIP_DBG ( fip , " drop els_send op % u d_id % x \ n ", <nl> op , ntoh24 ( fh -> fh_d_id )); <nl> + kfree_skb ( skb ); <nl> return - EINVAL ; <nl> } <nl> EXPORT_SYMBOL ( fcoe_ctlr_els_send );
static int dvb_ca_ioctl ( struct file * file , unsigned int cmd , void * parg ) <nl> { <nl> ca_slot_info_t * info =( ca_slot_info_t *) parg ; <nl>  <nl> - if ( info -> num > 1 ) <nl> + if ( info -> num < 0 || info -> num > 1 ) <nl> return - EINVAL ; <nl> av7110 -> ci_slot [ info -> num ]. num = info -> num ; <nl> av7110 -> ci_slot [ info -> num ]. type = FW_CI_LL_SUPPORT ( av7110 -> arm_app ) ?
static int __init hp_wmi_input_setup ( void ) <nl> int err ; <nl>  <nl> hp_wmi_input_dev = input_allocate_device (); <nl> + if (! hp_wmi_input_dev ) <nl> + return - ENOMEM ; <nl>  <nl> hp_wmi_input_dev -> name = " HP WMI hotkeys "; <nl> hp_wmi_input_dev -> phys = " wmi / input0 ";
static int alc662_parse_auto_config ( struct hda_codec * codec ) <nl> if ( codec -> vendor_id == 0x10ec0663 ) <nl> spec -> init_verbs [ spec -> num_init_verbs ++] = <nl> alc663_auto_init_verbs ; <nl> + <nl> + err = alc_auto_add_mic_boost ( codec ); <nl> + if ( err < 0 ) <nl> + return err ; <nl> + <nl> spec -> mixers [ spec -> num_mixers ] = alc662_capture_mixer ; <nl> spec -> num_mixers ++; <nl> return 1 ;
int io_msg_ring ( struct io_kiocb * req , unsigned int issue_flags ) <nl> req_set_fail ( req ); <nl> io_req_set_res ( req , ret , 0 ); <nl> /* put file to avoid an attempt to IOPOLL the req */ <nl> - io_put_file ( req -> file ); <nl> + if (!( req -> flags & REQ_F_FIXED_FILE )) <nl> + io_put_file ( req -> file ); <nl> req -> file = NULL ; <nl> return IOU_OK ; <nl> }
void __init mem_init ( void ) <nl>  <nl> pci_iommu_alloc (); <nl>  <nl> - /* clear the zero - page */ <nl> - memset ( empty_zero_page , 0 , PAGE_SIZE ); <nl> + /* clear_bss () already clear the empty_zero_page */ <nl>  <nl> reservedpages = 0 ; <nl> 
static int rtl2832u_get_rc_config ( struct dvb_usb_device * d , <nl> rc -> allowed_protos = RC_BIT_ALL ; <nl> rc -> driver_type = RC_DRIVER_IR_RAW ; <nl> rc -> query = rtl2832u_rc_query ; <nl> - rc -> interval = 400 ; <nl> + rc -> interval = 200 ; <nl>  <nl> return 0 ; <nl> }
nouveau_fbcon_destroy ( struct drm_device * dev , struct nouveau_fbdev * fbcon ) <nl> drm_fb_helper_unregister_fbi (& fbcon -> helper ); <nl> drm_fb_helper_fini (& fbcon -> helper ); <nl>  <nl> - if ( nouveau_fb -> nvbo ) { <nl> + if ( nouveau_fb && nouveau_fb -> nvbo ) { <nl> nouveau_vma_del (& nouveau_fb -> vma ); <nl> nouveau_bo_unmap ( nouveau_fb -> nvbo ); <nl> nouveau_bo_unpin ( nouveau_fb -> nvbo );
static int ucb1400_ts_remove ( struct platform_device * dev ) <nl> # ifdef CONFIG_PM <nl> static int ucb1400_ts_resume ( struct platform_device * dev ) <nl> { <nl> - struct ucb1400_ts * ucb = platform_get_drvdata ( dev ); <nl> + struct ucb1400_ts * ucb = dev -> dev . platform_data ; <nl>  <nl> if ( ucb -> ts_task ) { <nl> /*
struct tpm_chip * tpmm_chip_alloc ( struct device * dev , <nl>  <nl> device_initialize (& chip -> dev ); <nl>  <nl> - chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> cdev_init (& chip -> cdev , & tpm_fops ); <nl> + chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> + chip -> cdev . kobj . parent = & chip -> dev . kobj ; <nl>  <nl> return chip ; <nl> }
static ssize_t fuse_dev_splice_read ( struct file * in , loff_t * ppos , <nl> * code can Oops if the buffer persists after module unload . <nl> */ <nl> bufs [ page_nr ]. ops = & nosteal_pipe_buf_ops ; <nl> + bufs [ page_nr ]. flags = 0 ; <nl> ret = add_to_pipe ( pipe , & bufs [ page_nr ++]); <nl> if ( unlikely ( ret < 0 )) <nl> break ;
void mctp_dev_hold ( struct mctp_dev * mdev ) <nl> void mctp_dev_put ( struct mctp_dev * mdev ) <nl> { <nl> if ( mdev && refcount_dec_and_test (& mdev -> refs )) { <nl> + kfree ( mdev -> addrs ); <nl> dev_put ( mdev -> dev ); <nl> kfree_rcu ( mdev , rcu ); <nl> } <nl> static void mctp_unregister ( struct net_device * dev ) <nl>  <nl> mctp_route_remove_dev ( mdev ); <nl> mctp_neigh_remove_dev ( mdev ); <nl> - kfree ( mdev -> addrs ); <nl>  <nl> mctp_dev_put ( mdev ); <nl> }
void update_mmu_cache ( struct vm_area_struct * vma , unsigned long address , <nl> * called with either mm -> page_table_lock held or ptl lock held <nl> */ <nl> unsigned long access = 0 , trap ; <nl> + if ( radix_enabled ()) <nl> + return ; <nl>  <nl> /* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */ <nl> if (! pte_young (* ptep ) || address >= TASK_SIZE )
static int pcc_get_offset ( int cpu ) <nl> pr = per_cpu ( processors , cpu ); <nl> pcc_cpu_data = per_cpu_ptr ( pcc_cpu_info , cpu ); <nl>  <nl> + if (! pr ) <nl> + return - ENODEV ; <nl> + <nl> status = acpi_evaluate_object ( pr -> handle , " PCCP ", NULL , & buffer ); <nl> if ( ACPI_FAILURE ( status )) <nl> return - ENODEV ;
static void kone_keep_values_up_to_date ( struct kone_device * kone , <nl> { <nl> switch ( event -> event ) { <nl> case kone_mouse_event_switch_profile : <nl> + kone -> actual_dpi = kone -> profiles [ event -> value - 1 ]. <nl> + startup_dpi ; <nl> case kone_mouse_event_osd_profile : <nl> kone -> actual_profile = event -> value ; <nl> - kone -> actual_dpi = kone -> profiles [ kone -> actual_profile - 1 ]. <nl> - startup_dpi ; <nl> break ; <nl> case kone_mouse_event_switch_dpi : <nl> case kone_mouse_event_osd_dpi :
static int ti_pipe3_exit ( struct phy * x ) <nl> u32 val ; <nl> unsigned long timeout ; <nl>  <nl> + /* SATA DPLL can ' t be powered down due to Errata i783 */ <nl> + if ( of_device_is_compatible ( phy -> dev -> of_node , " ti , phy - pipe3 - sata ")) <nl> + return 0 ; <nl> + <nl> /* Put DPLL in IDLE mode */ <nl> val = ti_pipe3_readl ( phy -> pll_ctrl_base , PLL_CONFIGURATION2 ); <nl> val |= PLL_IDLE ;
static inline pte_t huge_ptep_get_and_clear ( struct mm_struct * mm , <nl> static inline void huge_ptep_clear_flush ( struct vm_area_struct * vma , <nl> unsigned long addr , pte_t * ptep ) <nl> { <nl> + ptep_clear_flush ( vma , addr , ptep ); <nl> } <nl>  <nl> static inline int huge_pte_none ( pte_t pte )
static int caif_seqpkt_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> if ( m -> msg_flags & MSG_OOB ) <nl> goto read_error ; <nl>  <nl> + m -> msg_namelen = 0 ; <nl> + <nl> skb = skb_recv_datagram ( sk , flags , 0 , & ret ); <nl> if (! skb ) <nl> goto read_error ;
static int fuse_notify_inval_entry ( struct fuse_conn * fc , unsigned int size , <nl> if ( outarg . namelen > FUSE_NAME_MAX ) <nl> goto err ; <nl>  <nl> + err = - EINVAL ; <nl> + if ( size != sizeof ( outarg ) + outarg . namelen + 1 ) <nl> + goto err ; <nl> + <nl> name . name = buf ; <nl> name . len = outarg . namelen ; <nl> err = fuse_copy_one ( cs , buf , outarg . namelen + 1 );
static void rtsx_pci_shutdown ( struct pci_dev * pcidev ) <nl> rtsx_pci_power_off ( pcr , HOST_ENTER_S1 ); <nl>  <nl> pci_disable_device ( pcidev ); <nl> + free_irq ( pcr -> irq , ( void *) pcr ); <nl> + if ( pcr -> msi_en ) <nl> + pci_disable_msi ( pcr -> pci ); <nl> } <nl>  <nl> # else /* CONFIG_PM */
static int get_dma_channel ( struct device_node * ssi_np , <nl> * dai -> platform name should already point to an allocated buffer . <nl> */ <nl> ret = of_address_to_resource ( dma_channel_np , 0 , & res ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + of_node_put ( dma_channel_np ); <nl> return ret ; <nl> + } <nl> snprintf (( char *) dai -> platform_name , DAI_NAME_SIZE , "% llx .% s ", <nl> ( unsigned long long ) res . start , dma_channel_np -> name ); <nl> 
static enum io_status ccwreq_status ( struct ccw_device * cdev , struct irb * lcirb ) <nl> /* Ask the driver what to do */ <nl> if ( cdev -> drv && cdev -> drv -> uc_handler ) { <nl> todo = cdev -> drv -> uc_handler ( cdev , lcirb ); <nl> + CIO_TRACE_EVENT ( 2 , " uc_response "); <nl> + CIO_HEX_EVENT ( 2 , & todo , sizeof ( todo )); <nl> switch ( todo ) { <nl> case UC_TODO_RETRY : <nl> return IO_STATUS_ERROR ;
static int tegra_io_pad_prepare ( enum tegra_io_pad id , unsigned long * request , <nl> } <nl>  <nl> rate = clk_get_rate ( pmc -> clk ); <nl> + if (! rate ) <nl> + return - ENODEV ; <nl>  <nl> tegra_pmc_writel ( DPD_SAMPLE_ENABLE , DPD_SAMPLE ); <nl> 
static int udf_encode_fh ( struct inode * inode , __u32 * fh , int * lenp , <nl> * lenp = 3 ; <nl> fid -> udf . block = location . logicalBlockNum ; <nl> fid -> udf . partref = location . partitionReferenceNum ; <nl> + fid -> udf . parent_partref = 0 ; <nl> fid -> udf . generation = inode -> i_generation ; <nl>  <nl> if ( parent ) {
int __init acpi_debugfs_init ( void ) <nl> if (! acpi_dir ) <nl> goto err ; <nl>  <nl> - cm_dentry = debugfs_create_file (" custom_method ", S_IWUGO , <nl> + cm_dentry = debugfs_create_file (" custom_method ", S_IWUSR , <nl> acpi_dir , NULL , & cm_fops ); <nl> if (! cm_dentry ) <nl> goto err ;
static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> ctl_dir = container_of ( head , struct ctl_dir , header ); <nl>  <nl> if (! dir_emit_dots ( file , ctx )) <nl> - return 0 ; <nl> + goto out ; <nl>  <nl> pos = 2 ; <nl>  <nl> static int proc_sys_readdir ( struct file * file , struct dir_context * ctx ) <nl> break ; <nl> } <nl> } <nl> + out : <nl> sysctl_head_finish ( head ); <nl> return 0 ; <nl> }
int btrfs_read_sys_array ( struct btrfs_root * root ) <nl> if ( ret ) <nl> break ; <nl> } else { <nl> + printk ( KERN_ERR <nl> + " BTRFS : unexpected item type % u in sys_array at offset % u \ n ", <nl> + ( u32 ) key . type , cur_offset ); <nl> ret = - EIO ; <nl> break ; <nl> }
static void devm_pci_release_host_bridge_dev ( struct device * dev ) <nl>  <nl> if ( bridge -> release_fn ) <nl> bridge -> release_fn ( bridge ); <nl> + <nl> + pci_free_resource_list (& bridge -> windows ); <nl> } <nl>  <nl> static void pci_release_host_bridge_dev ( struct device * dev ) <nl> { <nl> devm_pci_release_host_bridge_dev ( dev ); <nl> - pci_free_host_bridge ( to_pci_host_bridge ( dev )); <nl> + kfree ( to_pci_host_bridge ( dev )); <nl> } <nl>  <nl> struct pci_host_bridge * pci_alloc_host_bridge ( size_t priv )
static int create_encryption_context_from_policy ( struct inode * inode , <nl> int fscrypt_process_policy ( struct inode * inode , <nl> const struct fscrypt_policy * policy ) <nl> { <nl> + if (! inode_owner_or_capable ( inode )) <nl> + return - EACCES ; <nl> + <nl> if ( policy -> version != 0 ) <nl> return - EINVAL ; <nl> 
int nvdimm_namespace_attach_btt ( struct nd_namespace_common * ndns ) <nl> } <nl>  <nl> btt_sb = devm_kzalloc (& nd_btt -> dev , sizeof (* btt_sb ), GFP_KERNEL ); <nl> + if (! btt_sb ) <nl> + return - ENOMEM ; <nl>  <nl> /* <nl> * If this returns < 0 , that is ok as it just means there wasn ' t
s32 igb_get_phy_id ( struct e1000_hw * hw ) <nl> u16 phy_id ; <nl>  <nl> /* ensure PHY page selection to fix misconfigured i210 */ <nl> - if ( hw -> mac . type == e1000_i210 ) <nl> + if (( hw -> mac . type == e1000_i210 ) || ( hw -> mac . type == e1000_i211 )) <nl> phy -> ops . write_reg ( hw , I347AT4_PAGE_SELECT , 0 ); <nl>  <nl> ret_val = phy -> ops . read_reg ( hw , PHY_ID1 , & phy_id );
static void alc_auto_init_extra_out ( struct hda_codec * codec ) <nl> int i ; <nl> hda_nid_t pin , dac ; <nl>  <nl> - for ( i = 0 ; i < spec -> autocfg . speaker_outs ; i ++) { <nl> + for ( i = 0 ; i < spec -> autocfg . hp_outs ; i ++) { <nl> pin = spec -> autocfg . hp_pins [ i ]; <nl> if (! pin ) <nl> break ;
int ath_tx_aggr_start ( struct ath_softc * sc , struct ieee80211_sta * sta , <nl> txtid -> paused = true ; <nl> * ssn = txtid -> seq_start = txtid -> seq_next ; <nl>  <nl> + memset ( txtid -> tx_buf , 0 , sizeof ( txtid -> tx_buf )); <nl> + txtid -> baw_head = txtid -> baw_tail = 0 ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int omap_system_dma_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl>  <nl> if ( dma_omap2plus ()) { <nl> - dma_linked_lch = kzalloc ( sizeof ( struct dma_link_info ) * <nl> - dma_lch_count , GFP_KERNEL ); <nl> + dma_linked_lch = kcalloc ( dma_lch_count , <nl> + sizeof (* dma_linked_lch ), <nl> + GFP_KERNEL ); <nl> if (! dma_linked_lch ) { <nl> ret = - ENOMEM ; <nl> goto exit_dma_lch_fail ;
static void gdm_usb_disconnect ( struct usb_interface * intf ) <nl> { <nl> struct phy_dev * phy_dev ; <nl> struct lte_udev * udev ; <nl> - u16 idVendor , idProduct ; <nl> struct usb_device * usbdev ; <nl>  <nl> usbdev = interface_to_usbdev ( intf ); <nl> - <nl> - idVendor = __le16_to_cpu ( usbdev -> descriptor . idVendor ); <nl> - idProduct = __le16_to_cpu ( usbdev -> descriptor . idProduct ); <nl> - <nl> phy_dev = usb_get_intfdata ( intf ); <nl>  <nl> udev = phy_dev -> priv_dev ;
static int pwm_setup_backlight ( struct intel_connector * connector , <nl> return - ENODEV ; <nl> } <nl>  <nl> + /* <nl> + * FIXME : pwm_apply_args () should be removed when switching to <nl> + * the atomic PWM API . <nl> + */ <nl> + pwm_apply_args ( panel -> backlight . pwm ); <nl> + <nl> retval = pwm_config ( panel -> backlight . pwm , CRC_PMIC_PWM_PERIOD_NS , <nl> CRC_PMIC_PWM_PERIOD_NS ); <nl> if ( retval < 0 ) {
static int check_vmentry_postreqs ( struct kvm_vcpu * vcpu , struct vmcs12 * vmcs12 , <nl> return 1 ; <nl> } <nl>  <nl> + if (( vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS ) && <nl> + ( is_noncanonical_address ( vmcs12 -> guest_bndcfgs & PAGE_MASK , vcpu ) || <nl> + ( vmcs12 -> guest_bndcfgs & MSR_IA32_BNDCFGS_RSVD ))) <nl> + return 1 ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int cciss_ioctl32_big_passthru ( struct block_device * bdev , fmode_t mode , <nl> int err ; <nl> u32 cp ; <nl>  <nl> + memset (& arg64 , 0 , sizeof ( arg64 )); <nl> err = 0 ; <nl> err |= <nl> copy_from_user (& arg64 . LUN_info , & arg32 -> LUN_info ,
static int iterate_dir_item ( struct btrfs_root * root , struct btrfs_path * path , <nl> buf = tmp ; <nl> } <nl> if (! buf ) { <nl> - buf = vmalloc ( buf_len ); <nl> + buf = kvmalloc ( buf_len , GFP_KERNEL ); <nl> if (! buf ) { <nl> ret = - ENOMEM ; <nl> goto out ;
long keyctl_read_key ( key_serial_t keyid , char __user * buffer , size_t buflen ) <nl>  <nl> key = key_ref_to_ptr ( key_ref ); <nl>  <nl> + if ( test_bit ( KEY_FLAG_NEGATIVE , & key -> flags )) { <nl> + ret = - ENOKEY ; <nl> + goto error2 ; <nl> + } <nl> + <nl> /* see if we can read it directly */ <nl> ret = key_permission ( key_ref , KEY_NEED_READ ); <nl> if ( ret == 0 )
static int __inet6_check_established ( struct inet_timewait_death_row * death_row , <nl>  <nl> if ( twp != NULL ) { <nl> * twp = tw ; <nl> - NET_INC_STATS_BH ( twsk_net ( tw ), LINUX_MIB_TIMEWAITRECYCLED ); <nl> + NET_INC_STATS_BH ( net , LINUX_MIB_TIMEWAITRECYCLED ); <nl> } else if ( tw != NULL ) { <nl> /* Silly . Should hash - dance instead ... */ <nl> inet_twsk_deschedule ( tw , death_row ); <nl> - NET_INC_STATS_BH ( twsk_net ( tw ), LINUX_MIB_TIMEWAITRECYCLED ); <nl> + NET_INC_STATS_BH ( net , LINUX_MIB_TIMEWAITRECYCLED ); <nl>  <nl> inet_twsk_put ( tw ); <nl> }
static int threshold_create_bank ( unsigned int cpu , unsigned int bank ) <nl> const char * name = get_name ( bank , NULL ); <nl> int err = 0 ; <nl>  <nl> + if (! dev ) <nl> + return - ENODEV ; <nl> + <nl> if ( is_shared_bank ( bank )) { <nl> nb = node_to_amd_nb ( amd_get_nb_id ( cpu )); <nl> 
int vfio_pci_set_irqs_ioctl ( struct vfio_pci_device * vdev , uint32_t flags , <nl> func = vfio_pci_set_err_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> case VFIO_PCI_REQ_IRQ_INDEX : <nl> switch ( flags & VFIO_IRQ_SET_ACTION_TYPE_MASK ) { <nl> case VFIO_IRQ_SET_ACTION_TRIGGER : <nl> func = vfio_pci_set_req_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> } <nl>  <nl> if (! func )
tegra_xusb_find_port_node ( struct tegra_xusb_padctl * padctl , const char * type , <nl> name = kasprintf ( GFP_KERNEL , "% s -% u ", type , index ); <nl> if (! name ) { <nl> of_node_put ( ports ); <nl> - return ERR_PTR (- ENOMEM ); <nl> + return NULL ; <nl> } <nl> np = of_get_child_by_name ( ports , name ); <nl> kfree ( name );
static int max1363_probe ( struct i2c_client * client , <nl> goto error_out ; <nl> } <nl>  <nl> + indio_dev -> dev . of_node = client -> dev . of_node ; <nl> ret = iio_map_array_register ( indio_dev , client -> dev . platform_data ); <nl> if ( ret < 0 ) <nl> goto error_free_device ;
static int snd_sst_fill_kernel_list ( struct stream_info * stream , <nl> return - ENOMEM ; <nl> if ( copy_from_user (( void *) & rar_handle , <nl> iovec [ index ]. iov_base , <nl> - sizeof ( __u32 ))) <nl> + sizeof ( __u32 ))) { <nl> + kfree ( stream_bufs ); <nl> return - EFAULT ; <nl> + } <nl> stream_bufs -> addr = ( char *) rar_handle ; <nl> stream_bufs -> in_use = false ; <nl> stream_bufs -> size = iovec [ 0 ]. iov_len ;
static int intel_ring_context_pin ( struct intel_engine_cs * engine , <nl> ret = context_pin ( ctx ); <nl> if ( ret ) <nl> goto error ; <nl> + <nl> + ce -> state -> obj -> mm . dirty = true ; <nl> } <nl>  <nl> /* The kernel context is only used as a placeholder for flushing the
void reload_ucode_amd ( void ) <nl> if ( mc && rev < mc -> hdr . patch_id ) { <nl> if (! __apply_microcode_amd ( mc )) { <nl> ucode_new_rev = mc -> hdr . patch_id ; <nl> - pr_info (" microcode : reload patch_level = 0x % 08x \ n ", <nl> - ucode_new_rev ); <nl> + pr_info (" reload patch_level = 0x % 08x \ n ", ucode_new_rev ); <nl> } <nl> } <nl> }
static struct task_struct * producer ; <nl> static struct task_struct * consumer ; <nl> static unsigned long read ; <nl>  <nl> - static int disable_reader ; <nl> + static unsigned int disable_reader ; <nl> module_param ( disable_reader , uint , 0644 ); <nl> MODULE_PARM_DESC ( disable_reader , " only run producer "); <nl>  <nl> - static int write_iteration = 50 ; <nl> + static unsigned int write_iteration = 50 ; <nl> module_param ( write_iteration , uint , 0644 ); <nl> MODULE_PARM_DESC ( write_iteration , "# of writes between timestamp readings "); <nl> 
int iwl_mvm_sched_scan_start ( struct iwl_mvm * mvm , <nl> return - EBUSY ; <nl> } <nl>  <nl> + /* we don ' t support " match all " in the firmware */ <nl> + if (! req -> n_match_sets ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> ret = iwl_mvm_check_running_scans ( mvm , type ); <nl> if ( ret ) <nl> return ret ;
static const struct file_operations evtchn_fops = { <nl>  <nl> static struct miscdevice evtchn_miscdev = { <nl> . minor = MISC_DYNAMIC_MINOR , <nl> - . name = " evtchn ", <nl> + . name = " xen / evtchn ", <nl> . fops = & evtchn_fops , <nl> }; <nl> static int __init evtchn_init ( void )
static irqreturn_t dcon_interrupt ( int irq , void * id ) <nl> return IRQ_HANDLED ; <nl> } <nl>  <nl> - static struct i2c_device_id dcon_idtable [] = { <nl> + static const struct i2c_device_id dcon_idtable [] = { <nl> { " olpc_dcon ", 0 }, <nl> { } <nl> }; <nl> static int __init olpc_dcon_init ( void ) <nl> { <nl> pdata = & dcon_pdata_xo_1 ; <nl>  <nl> - i2c_add_driver (& dcon_driver ); <nl> - return 0 ; <nl> + return i2c_add_driver (& dcon_driver ); <nl> } <nl>  <nl> static void __exit olpc_dcon_exit ( void )
static int __devinit pmic8xxx_pwrkey_probe ( struct platform_device * pdev ) <nl> unsigned int delay ; <nl> u8 pon_cntl ; <nl> struct pmic8xxx_pwrkey * pwrkey ; <nl> - const struct pm8xxx_pwrkey_platform_data * pdata = mfd_get_data ( pdev ); <nl> + const struct pm8xxx_pwrkey_platform_data * pdata = <nl> + dev_get_platdata (& pdev -> dev ); <nl>  <nl> if (! pdata ) { <nl> dev_err (& pdev -> dev , " power key platform data not supplied \ n ");
static int corsair_input_mapping ( struct hid_device * dev , <nl> { <nl> int gkey ; <nl>  <nl> + if (( usage -> hid & HID_USAGE_PAGE ) != HID_UP_KEYBOARD ) <nl> + return 0 ; <nl> + <nl> gkey = corsair_usage_to_gkey ( usage -> hid & HID_USAGE ); <nl> if ( gkey != 0 ) { <nl> hid_map_usage_clear ( input , usage , bit , max , EV_KEY ,
static int dpi_check_timings ( struct omap_dss_device * dssdev , <nl> struct dpi_clk_calc_ctx ctx ; <nl> bool ok ; <nl>  <nl> + if ( timings -> x_res % 8 != 0 ) <nl> + return - EINVAL ; <nl> + <nl> if ( mgr && ! dispc_mgr_timings_ok ( mgr -> id , timings )) <nl> return - EINVAL ; <nl> 
static int omninet_port_remove ( struct usb_serial_port * port ) <nl>  <nl> static int omninet_open ( struct tty_struct * tty , struct usb_serial_port * port ) <nl> { <nl> - struct usb_serial * serial = port -> serial ; <nl> - struct usb_serial_port * wport ; <nl> - <nl> - wport = serial -> port [ 1 ]; <nl> - tty_port_tty_set (& wport -> port , tty ); <nl> - <nl> return usb_serial_generic_open ( tty , port ); <nl> } <nl> 
EXPORT_SYMBOL ( clk_enable ); <nl>  <nl> void clk_disable ( struct clk * clk ) <nl> { <nl> + if (! clk ) <nl> + return ; <nl> + <nl> if ( clk -> ops && clk -> ops -> disable ) <nl> clk -> ops -> disable ( clk ); <nl> }
static int list_devices ( struct file * filp , struct dm_ioctl * param , size_t param_ <nl> * Grab our output buffer . <nl> */ <nl> nl = orig_nl = get_result_buffer ( param , param_size , & len ); <nl> - if ( len < needed ) { <nl> + if ( len < needed || len < sizeof ( nl -> dev )) { <nl> param -> flags |= DM_BUFFER_FULL_FLAG ; <nl> goto out ; <nl> }
static inline void callchain_init ( struct callchain_node * node ) <nl> INIT_LIST_HEAD (& node -> children ); <nl> INIT_LIST_HEAD (& node -> val ); <nl>  <nl> + node -> children_hit = 0 ; <nl> node -> parent = NULL ; <nl> node -> hit = 0 ; <nl> }
static int init_fb_chan ( struct mx3fb_data * mx3fb , struct idmac_channel * ichan ) <nl> init_completion (& mx3fbi -> flip_cmpl ); <nl> disable_irq ( ichan -> eof_irq ); <nl> dev_dbg ( mx3fb -> dev , " disabling irq % d \ n ", ichan -> eof_irq ); <nl> - ret = mx3fb_set_par ( fbi ); <nl> - if ( ret < 0 ) <nl> - goto esetpar ; <nl> - <nl> - mx3fb_blank ( FB_BLANK_UNBLANK , fbi ); <nl>  <nl> dev_info ( dev , " registered , using mode % s \ n ", fb_mode ); <nl> 
struct dst_entry * icmp6_dst_alloc ( struct net_device * dev , <nl> struct net * net = dev_net ( dev ); <nl>  <nl> if ( unlikely (! idev )) <nl> - return NULL ; <nl> + return ERR_PTR (- ENODEV ); <nl>  <nl> rt = ip6_dst_alloc (& net -> ipv6 . ip6_dst_ops , dev , 0 ); <nl> if ( unlikely (! rt )) {
struct sock * inet_csk_clone_lock ( const struct sock * sk , <nl> /* listeners have SOCK_RCU_FREE , not the children */ <nl> sock_reset_flag ( newsk , SOCK_RCU_FREE ); <nl>  <nl> + inet_sk ( newsk )-> mc_list = NULL ; <nl> + <nl> newsk -> sk_mark = inet_rsk ( req )-> ir_mark ; <nl> atomic64_set (& newsk -> sk_cookie , <nl> atomic64_read (& inet_rsk ( req )-> ir_cookie ));
static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
static void __init early_vmalloc ( char ** arg ) <nl> " vmalloc area too small , limiting to % luMB \ n ", <nl> vmalloc_reserve >> 20 ); <nl> } <nl> + <nl> + if ( vmalloc_reserve > VMALLOC_END - ( PAGE_OFFSET + SZ_32M )) { <nl> + vmalloc_reserve = VMALLOC_END - ( PAGE_OFFSET + SZ_32M ); <nl> + printk ( KERN_WARNING <nl> + " vmalloc area is too big , limiting to % luMB \ n ", <nl> + vmalloc_reserve >> 20 ); <nl> + } <nl> } <nl> __early_param (" vmalloc =", early_vmalloc ); <nl> 
static void solo_enc_free ( struct solo_enc_dev * solo_enc ) <nl> if ( solo_enc == NULL ) <nl> return ; <nl>  <nl> + pci_free_consistent ( solo_enc -> solo_dev -> pdev , <nl> + sizeof ( struct solo_p2m_desc ) * solo_enc -> desc_nelts , <nl> + solo_enc -> desc_items , solo_enc -> desc_dma ); <nl> video_unregister_device ( solo_enc -> vfd ); <nl> v4l2_ctrl_handler_free (& solo_enc -> hdl ); <nl> kfree ( solo_enc );
static int s3c24xx_eint_init ( struct samsung_pinctrl_drv_data * d ) <nl> irq = bank -> eint_offset ; <nl> mask = bank -> eint_mask ; <nl> for ( pin = 0 ; mask ; ++ pin , mask >>= 1 ) { <nl> - if ( irq > NUM_EINT ) <nl> + if ( irq >= NUM_EINT ) <nl> break ; <nl> if (!( mask & 1 )) <nl> continue ;
static struct clk * of_clk_gpio_delayed_register_get ( <nl> num_parents = of_clk_get_parent_count ( data -> node ); <nl>  <nl> parent_names = kcalloc ( num_parents , sizeof ( char *), GFP_KERNEL ); <nl> - if (! parent_names ) <nl> - return ERR_PTR (- ENOMEM ); <nl> + if (! parent_names ) { <nl> + clk = ERR_PTR (- ENOMEM ); <nl> + goto out ; <nl> + } <nl>  <nl> for ( i = 0 ; i < num_parents ; i ++) <nl> parent_names [ i ] = of_clk_get_parent_name ( data -> node , i );
void spu_deactivate ( struct spu_context * ctx ) <nl> */ <nl> void spu_yield ( struct spu_context * ctx ) <nl> { <nl> - mutex_lock (& ctx -> state_mutex ); <nl> - __spu_deactivate ( ctx , 0 , MAX_PRIO ); <nl> - mutex_unlock (& ctx -> state_mutex ); <nl> + if (!( ctx -> flags & SPU_CREATE_NOSCHED )) { <nl> + mutex_lock (& ctx -> state_mutex ); <nl> + __spu_deactivate ( ctx , 0 , MAX_PRIO ); <nl> + mutex_unlock (& ctx -> state_mutex ); <nl> + } <nl> } <nl>  <nl> void spu_sched_tick ( struct work_struct * work )
nouveau_connector_set_encoder ( struct drm_connector * connector , <nl> return ; <nl> nv_connector -> detected_encoder = nv_encoder ; <nl>  <nl> + if ( dev_priv -> card_type >= NV_50 ) { <nl> + connector -> interlace_allowed = true ; <nl> + connector -> doublescan_allowed = true ; <nl> + } else <nl> if ( nv_encoder -> dcb -> type == OUTPUT_LVDS || <nl> nv_encoder -> dcb -> type == OUTPUT_TMDS ) { <nl> connector -> doublescan_allowed = false ;
static inline void omap3xxx_restart ( enum reboot_mode mode , const char * cmd ) <nl> } <nl> # endif <nl>  <nl> -# if defined ( CONFIG_ARCH_OMAP4 ) || defined ( CONFIG_SOC_OMAP5 ) <nl> +# if defined ( CONFIG_ARCH_OMAP4 ) || defined ( CONFIG_SOC_OMAP5 ) || \ <nl> + defined ( CONFIG_SOC_DRA7XX ) || defined ( CONFIG_SOC_AM43XX ) <nl> void omap44xx_restart ( enum reboot_mode mode , const char * cmd ); <nl> # else <nl> static inline void omap44xx_restart ( enum reboot_mode mode , const char * cmd )
static int sunxi_pinctrl_probe ( struct platform_device * pdev ) <nl> goto gpiochip_error ; <nl> } <nl>  <nl> - clk_prepare_enable ( clk ); <nl> + ret = clk_prepare_enable ( clk ); <nl> + if ( ret ) <nl> + goto gpiochip_error ; <nl>  <nl> pctl -> irq = irq_of_parse_and_map ( node , 0 ); <nl> if (! pctl -> irq ) {
xfs_qm_freelist_destroy ( xfs_frlist_t * ql ) <nl> xfs_qm_dqdestroy ( dqp ); <nl> dqp = nextdqp ; <nl> } <nl> - /* <nl> - * Don ' t bother about unlocking . <nl> - */ <nl> + mutex_unlock (& ql -> qh_lock ); <nl> mutex_destroy (& ql -> qh_lock ); <nl>  <nl> ASSERT ( ql -> qh_nelems == 0 );
brcmf_notify_sched_scan_results ( struct brcmf_if * ifp , <nl> } <nl>  <nl> set_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status ); <nl> + cfg -> escan_info . run = brcmf_run_escan ; <nl> err = brcmf_do_escan ( cfg , wiphy , ifp , request ); <nl> if ( err ) { <nl> clear_bit ( BRCMF_SCAN_STATUS_BUSY , & cfg -> scan_status );
static int e1000_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> goto err_hw_init ; <nl>  <nl> if (( adapter -> flags & FLAG_IS_ICH ) && <nl> - ( adapter -> flags & FLAG_READ_ONLY_NVM )) <nl> + ( adapter -> flags & FLAG_READ_ONLY_NVM ) && <nl> + ( hw -> mac . type < e1000_pch_spt )) <nl> e1000e_write_protect_nvm_ich8lan (& adapter -> hw ); <nl>  <nl> hw -> mac . ops . get_bus_info (& adapter -> hw );
static void mem_cgroup_usage_unregister_event ( struct cgroup * cgrp , <nl> */ <nl> BUG_ON (! thresholds ); <nl>  <nl> + if (! thresholds -> primary ) <nl> + goto unlock ; <nl> + <nl> usage = mem_cgroup_usage ( memcg , type == _MEMSWAP ); <nl>  <nl> /* Check if a threshold crossed before removing */ <nl> static void mem_cgroup_usage_unregister_event ( struct cgroup * cgrp , <nl>  <nl> /* To be sure that nobody uses thresholds */ <nl> synchronize_rcu (); <nl> - <nl> + unlock : <nl> mutex_unlock (& memcg -> thresholds_lock ); <nl> } <nl> 
void __init setup_arch ( char ** cmdline_p ) <nl> init_mm . brk = ( unsigned long ) & _end ; <nl>  <nl> * cmdline_p = m68k_command_line ; <nl> - memcpy ( saved_command_line , * cmdline_p , CL_SIZE ); <nl> + memcpy ( boot_command_line , * cmdline_p , CL_SIZE ); <nl>  <nl> /* Parse the command line for arch - specific options . <nl> * For the m68k , this is currently only " debug = xxx " to enable printing
static int __exit twl4030_usb_remove ( struct platform_device * pdev ) <nl> /* disable complete OTG block */ <nl> twl4030_usb_clear_bits ( twl , POWER_CTRL , POWER_CTRL_OTG_ENAB ); <nl>  <nl> - twl4030_phy_power ( twl , 0 ); <nl> + if (! twl -> asleep ) <nl> + twl4030_phy_power ( twl , 0 ); <nl> regulator_put ( twl -> usb1v5 ); <nl> regulator_put ( twl -> usb1v8 ); <nl> regulator_put ( twl -> usb3v1 );
static ssize_t wait_for_direct_io ( enum ORANGEFS_io_type type , struct inode * inod <nl> */ <nl> if ( ret == - EAGAIN && op_state_purged ( new_op )) { <nl> orangefs_bufmap_put ( bufmap , buffer_index ); <nl> + buffer_index = - 1 ; <nl> gossip_debug ( GOSSIP_FILE_DEBUG , <nl> "% s : going to repopulate_shared_memory .\ n ", <nl> __func__ );
void * ion_heap_map_kernel ( struct ion_heap * heap , <nl> struct page ** tmp = pages ; <nl>  <nl> if (! pages ) <nl> - return NULL ; <nl> + return ERR_PTR (- ENOMEM ); <nl>  <nl> if ( buffer -> flags & ION_FLAG_CACHED ) <nl> pgprot = PAGE_KERNEL ;
static long aio_read_events_ring ( struct kioctx * ctx , <nl> if ( head == tail ) <nl> goto out ; <nl>  <nl> + head %= ctx -> nr_events ; <nl> + tail %= ctx -> nr_events ; <nl> + <nl> while ( ret < nr ) { <nl> long avail ; <nl> struct io_event * ev ;
MODULE_FIRMWARE (" digiface_firmware_rev11 . bin "); <nl> # define HDSP_DMA_AREA_BYTES (( HDSP_MAX_CHANNELS + 1 ) * HDSP_CHANNEL_BUFFER_BYTES ) <nl> # define HDSP_DMA_AREA_KILOBYTES ( HDSP_DMA_AREA_BYTES / 1024 ) <nl>  <nl> -/* use hotplug firmeare loader ? */ <nl> +/* use hotplug firmware loader ? */ <nl> # if defined ( CONFIG_FW_LOADER ) || defined ( CONFIG_FW_LOADER_MODULE ) <nl> -# if ! defined ( HDSP_USE_HWDEP_LOADER ) && ! defined ( CONFIG_SND_HDSP ) <nl> +# if ! defined ( HDSP_USE_HWDEP_LOADER ) <nl> # define HDSP_FW_LOADER <nl> # endif <nl> # endif
void __kvm_migrate_pit_timer ( struct kvm_vcpu * vcpu ) <nl> return ; <nl>  <nl> timer = & pit -> pit_state . timer ; <nl> + mutex_lock (& pit -> pit_state . lock ); <nl> if ( hrtimer_cancel ( timer )) <nl> hrtimer_start_expires ( timer , HRTIMER_MODE_ABS ); <nl> + mutex_unlock (& pit -> pit_state . lock ); <nl> } <nl>  <nl> static void destroy_pit_timer ( struct kvm_pit * pit )
static int amdgpu_cs_wait_any_fence ( struct amdgpu_device * adev , <nl> wait -> out . status = ( r > 0 ); <nl> wait -> out . first_signaled = first ; <nl>  <nl> - if ( array [ first ]) <nl> + if ( first < fence_count && array [ first ]) <nl> r = array [ first ]-> error ; <nl> else <nl> r = 0 ;
int ext4_orphan_del ( handle_t * handle , struct inode * inode ) <nl> struct ext4_iloc iloc ; <nl> int err = 0 ; <nl>  <nl> - if (! EXT4_SB ( inode -> i_sb )-> s_journal ) <nl> + if ((! EXT4_SB ( inode -> i_sb )-> s_journal ) && <nl> + !( EXT4_SB ( inode -> i_sb )-> s_mount_state & EXT4_ORPHAN_FS )) <nl> return 0 ; <nl>  <nl> mutex_lock (& EXT4_SB ( inode -> i_sb )-> s_orphan_lock );
static int create_queue_cpsch ( struct device_queue_manager * dqm , struct queue * q , <nl> return retval ; <nl> } <nl>  <nl> - int fence_wait_timeout ( unsigned int * fence_addr , unsigned int fence_value , <nl> - unsigned long timeout ) <nl> + static int fence_wait_timeout ( unsigned int * fence_addr , <nl> + unsigned int fence_value , <nl> + unsigned long timeout ) <nl> { <nl> BUG_ON (! fence_addr ); <nl> timeout += jiffies ;
int cx23885_tuner_callback ( void * priv , int component , int command , int arg ) <nl> struct cx23885_dev * dev = port -> dev ; <nl> u32 bitmask = 0 ; <nl>  <nl> - if ( command == XC2028_RESET_CLK ) <nl> + if (( command == XC2028_RESET_CLK ) || ( command == XC2028_I2C_FLUSH )) <nl> return 0 ; <nl>  <nl> if ( command != 0 ) {
static void mmu_set_spte ( struct kvm_vcpu * vcpu , u64 * sptep , <nl>  <nl> child = page_header ( pte & PT64_BASE_ADDR_MASK ); <nl> mmu_page_remove_parent_pte ( child , sptep ); <nl> + __set_spte ( sptep , shadow_trap_nonpresent_pte ); <nl> + kvm_flush_remote_tlbs ( vcpu -> kvm ); <nl> } else if ( pfn != spte_to_pfn (* sptep )) { <nl> pgprintk (" hfn old % lx new % lx \ n ", <nl> spte_to_pfn (* sptep ), pfn );
static int __devinit snd_vt1724_read_eeprom ( struct snd_ice1712 * ice , <nl> static void __devinit snd_vt1724_chip_reset ( struct snd_ice1712 * ice ) <nl> { <nl> outb ( VT1724_RESET , ICEREG1724 ( ice , CONTROL )); <nl> + inb ( ICEREG1724 ( ice , CONTROL )); /* pci posting flush */ <nl> msleep ( 10 ); <nl> outb ( 0 , ICEREG1724 ( ice , CONTROL )); <nl> + inb ( ICEREG1724 ( ice , CONTROL )); /* pci posting flush */ <nl> msleep ( 10 ); <nl> } <nl> 
static const struct regmap_config rt5677_regmap = { <nl> static const struct i2c_device_id rt5677_i2c_id [] = { <nl> { " rt5677 ", RT5677 }, <nl> { " rt5676 ", RT5676 }, <nl> + { " RT5677CE : 00 ", RT5677 }, <nl> { } <nl> }; <nl> MODULE_DEVICE_TABLE ( i2c , rt5677_i2c_id );
static void wl3501_free_tx_buffer ( struct wl3501_card * this , u16 ptr ) <nl>  <nl> static int wl3501_esbq_req_test ( struct wl3501_card * this ) <nl> { <nl> - u8 tmp ; <nl> + u8 tmp = 0 ; <nl>  <nl> wl3501_get_from_wla ( this , this -> esbq_req_head + 3 , & tmp , sizeof ( tmp )); <nl> return tmp & 0x80 ;
static void sw_perf_event_destroy ( struct perf_event * event ) <nl>  <nl> static int perf_swevent_init ( struct perf_event * event ) <nl> { <nl> - int event_id = event -> attr . config ; <nl> + u64 event_id = event -> attr . config ; <nl>  <nl> if ( event -> attr . type != PERF_TYPE_SOFTWARE ) <nl> return - ENOENT ;
void bnx2x_disable_sriov ( struct bnx2x * bp ) <nl> static int bnx2x_vf_ndo_sanity ( struct bnx2x * bp , int vfidx , <nl> struct bnx2x_virtf * vf ) <nl> { <nl> + if ( bp -> state != BNX2X_STATE_OPEN ) { <nl> + BNX2X_ERR (" vf ndo called though PF is down \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if (! IS_SRIOV ( bp )) { <nl> BNX2X_ERR (" vf ndo called though sriov is disabled \ n "); <nl> return - EINVAL ;
static int __init ptp_kvm_init ( void ) <nl> { <nl> long ret ; <nl>  <nl> + if (! kvm_para_available ()) <nl> + return - ENODEV ; <nl> + <nl> clock_pair_gpa = slow_virt_to_phys (& clock_pair ); <nl> hv_clock = pvclock_pvti_cpu0_va (); <nl> 
void gfs2_set_iop ( struct inode * inode ) <nl> inode -> i_op = & gfs2_symlink_iops ; <nl> } else { <nl> inode -> i_op = & gfs2_file_iops ; <nl> + init_special_inode ( inode , inode -> i_mode , inode -> i_rdev ); <nl> } <nl>  <nl> unlock_new_inode ( inode );
static int rds_ib_laddr_check ( __be32 addr ) <nl> ret = rdma_bind_addr ( cm_id , ( struct sockaddr *)& sin ); <nl> /* due to this , we will claim to support iWARP devices unless we <nl> check node_type . */ <nl> - if ( ret || cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> + if ( ret || ! cm_id -> device || <nl> + cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> ret = - EADDRNOTAVAIL ; <nl>  <nl> rdsdebug (" addr % pI4 ret % d node type % d \ n ",
static int i965_do_reset ( struct drm_device * dev ) <nl> { <nl> int ret ; <nl>  <nl> + /* FIXME : i965g / gm need a display save / restore for gpu reset . */ <nl> + return - ENODEV ; <nl> + <nl> /* <nl> * Set the domains we want to reset ( GRDOM / bits 2 and 3 ) as <nl> * well as the reset bit ( GR / bit 0 ). Setting the GR bit
static int uas_find_endpoints ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < n_endpoints ; i ++) { <nl> unsigned char * extra = endpoint [ i ]. extra ; <nl> int len = endpoint [ i ]. extralen ; <nl> - while ( len > 1 ) { <nl> + while ( len >= 3 ) { <nl> if ( extra [ 1 ] == USB_DT_PIPE_USAGE ) { <nl> unsigned pipe_id = extra [ 2 ]; <nl> if ( pipe_id > 0 && pipe_id < 5 )
static struct console udbg_console = { <nl> . index = - 1 , <nl> }; <nl>  <nl> + static int early_console_initialized ; <nl> + <nl> void __init disable_early_printk ( void ) <nl> { <nl> + if (! early_console_initialized ) <nl> + return ; <nl> unregister_console (& udbg_console ); <nl> + early_console_initialized = 0 ; <nl> } <nl>  <nl> /* called by setup_system */ <nl> void register_early_udbg_console ( void ) <nl> { <nl> + early_console_initialized = 1 ; <nl> register_console (& udbg_console ); <nl> } <nl> 
vc4_get_bcl ( struct drm_device * dev , struct vc4_exec_info * exec ) <nl> args -> shader_rec_count ); <nl> struct vc4_bo * bo ; <nl>  <nl> - if ( uniforms_offset < shader_rec_offset || <nl> + if ( shader_rec_offset < args -> bin_cl_size || <nl> + uniforms_offset < shader_rec_offset || <nl> exec_size < uniforms_offset || <nl> args -> shader_rec_count >= ( UINT_MAX / <nl> sizeof ( struct vc4_shader_state )) ||
static int skl_probe ( struct pci_dev * pci , <nl> if ( err < 0 ) <nl> goto out_free ; <nl>  <nl> + skl -> nhlt = skl_nhlt_init ( bus -> dev ); <nl> + <nl> + if ( skl -> nhlt == NULL ) <nl> + goto out_free ; <nl> + <nl> pci_set_drvdata ( skl -> pci , ebus ); <nl>  <nl> /* check if dsp is there */
bool seg6_validate_srh ( struct ipv6_sr_hdr * srh , int len ) <nl> struct sr6_tlv * tlv ; <nl> unsigned int tlv_len ; <nl>  <nl> + if ( trailing < sizeof (* tlv )) <nl> + return false ; <nl> + <nl> tlv = ( struct sr6_tlv *)(( unsigned char *) srh + tlv_offset ); <nl> tlv_len = sizeof (* tlv ) + tlv -> len ; <nl> 
static unsigned int ata_eh_speed_down ( struct ata_device * dev , <nl> */ <nl> static inline int ata_eh_worth_retry ( struct ata_queued_cmd * qc ) <nl> { <nl> - if ( qc -> flags & AC_ERR_MEDIA ) <nl> + if ( qc -> err_mask & AC_ERR_MEDIA ) <nl> return 0 ; /* don ' t retry media errors */ <nl> if ( qc -> flags & ATA_QCFLAG_IO ) <nl> return 1 ; /* otherwise retry anything from fs stack */
struct gb_interface * gb_interface_create ( struct greybus_host_device * hd , <nl>  <nl> free_intf : <nl> put_device (& intf -> dev ); <nl> - kfree ( intf ); <nl> put_module : <nl> put_device (& module -> dev ); <nl> return NULL ;
static int bio_readpage_error ( struct bio * failed_bio , u64 phy_offset , <nl> ret = tree -> ops -> submit_bio_hook ( inode , read_mode , bio , <nl> failrec -> this_mirror , <nl> failrec -> bio_flags , 0 ); <nl> + if ( ret ) { <nl> + free_io_failure ( inode , failrec , 0 ); <nl> + bio_put ( bio ); <nl> + } <nl> + <nl> return ret ; <nl> } <nl> 
static void azx_pcm_free ( struct snd_pcm * pcm ) <nl>  <nl> # define MAX_PREALLOC_SIZE ( 32 * 1024 * 1024 ) <nl>  <nl> - int azx_attach_pcm_stream ( struct hda_bus * bus , struct hda_codec * codec , <nl> - struct hda_pcm * cpcm ) <nl> + static int azx_attach_pcm_stream ( struct hda_bus * bus , struct hda_codec * codec , <nl> + struct hda_pcm * cpcm ) <nl> { <nl> struct azx * chip = bus -> private_data ; <nl> struct snd_pcm * pcm ;
int fix_alignment ( struct pt_regs * regs ) <nl>  <nl> type = op . type & INSTR_TYPE_MASK ; <nl> if (! OP_IS_LOAD_STORE ( type )) { <nl> - if ( type != CACHEOP + DCBZ ) <nl> + if ( op . type != CACHEOP + DCBZ ) <nl> return - EINVAL ; <nl> PPC_WARN_ALIGNMENT ( dcbz , regs ); <nl> r = emulate_dcbz ( op . ea , regs );
static inline void create_debug_files ( struct ehci_hcd * ehci ) <nl> & debug_registers_fops )) <nl> goto file_error ; <nl>  <nl> - if (! debugfs_create_file (" lpm ", S_IRUGO | S_IWUGO , ehci -> debug_dir , bus , <nl> + if (! debugfs_create_file (" lpm ", S_IRUGO | S_IWUSR , ehci -> debug_dir , bus , <nl> & debug_lpm_fops )) <nl> goto file_error ; <nl> 
i915_gem_detect_bit_6_swizzle ( struct drm_device * dev ) <nl> } <nl> } <nl>  <nl> + /* FIXME : check with memory config on IGDNG */ <nl> + if ( IS_IGDNG ( dev )) { <nl> + DRM_ERROR (" disable tiling on IGDNG ...\ n "); <nl> + swizzle_x = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + swizzle_y = I915_BIT_6_SWIZZLE_UNKNOWN ; <nl> + } <nl> + <nl> dev_priv -> mm . bit_6_swizzle_x = swizzle_x ; <nl> dev_priv -> mm . bit_6_swizzle_y = swizzle_y ; <nl> }
bool fw_download_code ( struct net_device * dev , u8 * code_virtual_address , u32 buff <nl> * add 4 to avoid packet appending overflow . <nl> * */ <nl> skb = dev_alloc_skb ( USB_HWDESC_HEADER_LEN + frag_length + 4 ); <nl> + if (! skb ) <nl> + return false ; <nl> memcpy (( unsigned char *)( skb -> cb ),& dev , sizeof ( dev )); <nl> tcb_desc = ( cb_desc *)( skb -> cb + MAX_DEV_ADDR_SIZE ); <nl> tcb_desc -> queue_index = TXCMD_QUEUE ;
static int ipoib_mcast_join_complete ( int status , <nl> } <nl>  <nl> if ( mcast -> logcount ++ < 20 ) { <nl> - if ( status == - ETIMEDOUT ) { <nl> + if ( status == - ETIMEDOUT || status == - EAGAIN ) { <nl> ipoib_dbg_mcast ( priv , " multicast join failed for % pI6 , status % d \ n ", <nl> mcast -> mcmember . mgid . raw , status ); <nl> } else {
static void qlcnic_get_ethtool_stats ( struct net_device * dev , <nl> memset ( data , 0 , stats -> n_stats * sizeof ( u64 )); <nl>  <nl> for ( ring = 0 , index = 0 ; ring < adapter -> drv_tx_rings ; ring ++) { <nl> - if ( test_bit ( __QLCNIC_DEV_UP , & adapter -> state )) { <nl> + if ( adapter -> is_up == QLCNIC_ADAPTER_UP_MAGIC ) { <nl> tx_ring = & adapter -> tx_ring [ ring ]; <nl> data = qlcnic_fill_tx_queue_stats ( data , tx_ring ); <nl> qlcnic_update_stats ( adapter );
parser_name_get ( struct parser_context * ctx ) <nl> struct spar_controlvm_parameters_header * phdr = NULL ; <nl>  <nl> phdr = ( struct spar_controlvm_parameters_header *)( ctx -> data ); <nl> + <nl> + if ( phdr -> name_offset + phdr -> name_length > ctx -> param_bytes ) <nl> + return NULL ; <nl> + <nl> ctx -> curr = ctx -> data + phdr -> name_offset ; <nl> ctx -> bytes_remaining = phdr -> name_length ; <nl> return parser_string_get ( ctx );
static int handle_emulation_failure ( struct kvm_vcpu * vcpu ) <nl>  <nl> ++ vcpu -> stat . insn_emulation_fail ; <nl> trace_kvm_emulate_insn_failed ( vcpu ); <nl> - if (! is_guest_mode ( vcpu )) { <nl> + if (! is_guest_mode ( vcpu ) && kvm_x86_ops -> get_cpl ( vcpu ) == 0 ) { <nl> vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> vcpu -> run -> internal . ndata = 0 ;
struct clk * icst_clk_register ( struct device * dev , <nl>  <nl> pclone = kmemdup ( desc -> params , sizeof (* pclone ), GFP_KERNEL ); <nl> if (! pclone ) { <nl> + kfree ( icst ); <nl> pr_err (" could not clone ICST params \ n "); <nl> return ERR_PTR (- ENOMEM ); <nl> }
static int meson_mmc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> dev_err (& pdev -> dev , " failed to get interrupt resource .\ n "); <nl> ret = - EINVAL ; <nl> goto free_host ;
static int moxart_gpio_probe ( struct platform_device * pdev ) <nl> gc -> parent = dev ; <nl> gc -> owner = THIS_MODULE ; <nl>  <nl> - ret = gpiochip_add_data ( gc , NULL ); <nl> + ret = devm_gpiochip_add_data ( dev , gc , NULL ); <nl> if ( ret ) { <nl> dev_err ( dev , "% s : gpiochip_add failed \ n ", <nl> dev -> of_node -> full_name );
struct io_reg CX700_ModeXregs [] = { { VIASR , SR10 , 0xFF , 0x01 }, <nl> { VIACR , CR96 , 0xFF , 0x00 }, <nl> { VIACR , CR97 , 0xFF , 0x00 }, <nl> { VIACR , CR99 , 0xFF , 0x00 }, <nl> -{ VIACR , CR9B , 0xFF , 0x00 }, <nl> -{ VIACR , CRD2 , 0xFF , 0xFF } /* TMDS / LVDS control register . */ <nl> +{ VIACR , CR9B , 0xFF , 0x00 } <nl> }; <nl>  <nl> /* Video Mode Table */
int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
static void atmel_spi_next_xfer ( struct spi_master * master , <nl> xfer , xfer -> len , xfer -> tx_buf , xfer -> tx_dma , <nl> xfer -> rx_buf , xfer -> rx_dma , spi_readl ( as , IMR )); <nl>  <nl> - spi_writel ( as , TCR , len ); <nl> spi_writel ( as , RCR , len ); <nl> + spi_writel ( as , TCR , len ); <nl> spi_writel ( as , PTCR , SPI_BIT ( TXTEN ) | SPI_BIT ( RXTEN )); <nl> } <nl> 
static int twl4030_kp_probe ( struct platform_device * pdev ) <nl> err3 : <nl> /* mask all events - we don ' t care about the result */ <nl> ( void ) twl4030_kpwrite_u8 ( kp , 0xff , KEYP_IMR1 ); <nl> - free_irq ( kp -> irq , NULL ); <nl> + free_irq ( kp -> irq , kp ); <nl> err2 : <nl> input_unregister_device ( input ); <nl> input = NULL ;
extern int for_each_subchannel ( int (* fn )( struct subchannel_id , void *), void *); <nl> struct channel_subsystem { <nl> u8 cssid ; <nl> int valid ; <nl> - struct channel_path * chps [ __MAX_CHPID ]; <nl> + struct channel_path * chps [ __MAX_CHPID + 1 ]; <nl> struct device device ; <nl> struct pgid global_pgid ; <nl> };
static void atiixp_set_dmamode ( struct ata_port * ap , struct ata_device * adev ) <nl> * We must now look at the PIO mode situation . We may need to <nl> * adjust the PIO mode to keep the timings acceptable <nl> */ <nl> - if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> - wanted_pio = 4 ; <nl> + if ( adev -> dma_mode >= XFER_MW_DMA_2 ) <nl> + wanted_pio = 4 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_1 ) <nl> wanted_pio = 3 ; <nl> else if ( adev -> dma_mode == XFER_MW_DMA_0 )
unsigned long sctp_transport_timeout ( struct sctp_transport * trans ) <nl> trans -> state != SCTP_PF ) <nl> timeout += trans -> hbinterval ; <nl>  <nl> - return timeout ; <nl> + return max_t ( unsigned long , timeout , HZ / 5 ); <nl> } <nl>  <nl> /* Reset transport variables to their initial values */
int dlm_migrate_request_handler ( struct o2net_msg * msg , u32 len , void * data , <nl> migrate -> new_master , <nl> migrate -> master ); <nl>  <nl> + if ( ret < 0 ) <nl> + kmem_cache_free ( dlm_mle_cache , mle ); <nl> + <nl> spin_unlock (& dlm -> master_lock ); <nl> unlock : <nl> spin_unlock (& dlm -> spinlock );
static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
int vbd_create ( blkif_t * blkif , blkif_vdev_t handle , unsigned major , <nl>  <nl> vbd -> pdevice = MKDEV ( major , minor ); <nl>  <nl> - bdev = open_by_devnum ( vbd -> pdevice , <nl> - vbd -> readonly ? FMODE_READ : FMODE_WRITE ); <nl> + bdev = blkdev_get_by_dev ( vbd -> pdevice , vbd -> readonly ? <nl> + FMODE_READ : FMODE_WRITE , NULL ); <nl>  <nl> if ( IS_ERR ( bdev )) { <nl> DPRINTK (" vbd_creat : device % 08x could not be opened .\ n ",
static int tower_probe ( struct usb_interface * interface , const struct usb_device <nl> USB_MAJOR , dev -> minor ); <nl>  <nl> exit : <nl> + kfree ( get_version_reply ); <nl> return retval ; <nl>  <nl> error :
int add_mtd_blktrans_dev ( struct mtd_blktrans_dev * new ) <nl> new -> rq -> queuedata = new ; <nl> blk_queue_logical_block_size ( new -> rq , tr -> blksize ); <nl>  <nl> - if ( tr -> discard ) <nl> - queue_flag_set_unlocked ( QUEUE_FLAG_DISCARD , <nl> - new -> rq ); <nl> + if ( tr -> discard ) { <nl> + queue_flag_set_unlocked ( QUEUE_FLAG_DISCARD , new -> rq ); <nl> + new -> rq -> limits . max_discard_sectors = UINT_MAX ; <nl> + } <nl>  <nl> gd -> queue = new -> rq ; <nl> 
static struct sctp_auth_bytes * sctp_auth_create_key ( __u32 key_len , gfp_t gfp ) <nl> struct sctp_auth_bytes * key ; <nl>  <nl> /* Verify that we are not going to overflow INT_MAX */ <nl> - if (( INT_MAX - key_len ) < sizeof ( struct sctp_auth_bytes )) <nl> + if ( key_len > ( INT_MAX - sizeof ( struct sctp_auth_bytes ))) <nl> return NULL ; <nl>  <nl> /* Allocate the shared key */
static inline void ata_tf_init ( struct ata_device * dev , struct ata_taskfile * tf ) <nl>  <nl> static inline void ata_qc_reinit ( struct ata_queued_cmd * qc ) <nl> { <nl> + qc -> dma_dir = DMA_NONE ; <nl> qc -> __sg = NULL ; <nl> qc -> flags = 0 ; <nl> qc -> cursect = qc -> cursg = qc -> cursg_ofs = 0 ;
static struct sock * sctp_v6_create_accept_sk ( struct sock * sk , <nl> newnp = inet6_sk ( newsk ); <nl>  <nl> memcpy ( newnp , np , sizeof ( struct ipv6_pinfo )); <nl> + newnp -> ipv6_mc_list = NULL ; <nl> + newnp -> ipv6_ac_list = NULL ; <nl> + newnp -> ipv6_fl_list = NULL ; <nl>  <nl> rcu_read_lock (); <nl> opt = rcu_dereference ( np -> opt );
int evtchn_get ( unsigned int evtchn ) <nl> struct irq_info * info ; <nl> int err = - ENOENT ; <nl>  <nl> + if ( evtchn >= NR_EVENT_CHANNELS ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& irq_mapping_update_lock ); <nl>  <nl> irq = evtchn_to_irq [ evtchn ];
static void ieee80211_csa_connection_drop_work ( struct work_struct * work ) <nl> container_of ( work , struct ieee80211_sub_if_data , <nl> u . ibss . csa_connection_drop_work ); <nl>  <nl> + sdata_lock ( sdata ); <nl> + <nl> ieee80211_ibss_disconnect ( sdata ); <nl> synchronize_rcu (); <nl> skb_queue_purge (& sdata -> skb_queue ); <nl>  <nl> /* trigger a scan to find another IBSS network to join */ <nl> ieee80211_queue_work (& sdata -> local -> hw , & sdata -> work ); <nl> + <nl> + sdata_unlock ( sdata ); <nl> } <nl>  <nl> static void ieee80211_ibss_csa_mark_radar ( struct ieee80211_sub_if_data * sdata )
int ptrace_setxregs ( struct task_struct * child , void __user * uregs ) <nl> elf_xtregs_t * xtregs = uregs ; <nl> int ret = 0 ; <nl>  <nl> + if (! access_ok ( VERIFY_READ , uregs , sizeof ( elf_xtregs_t ))) <nl> + return - EFAULT ; <nl> + <nl> # if XTENSA_HAVE_COPROCESSORS <nl> /* Flush all coprocessors before we overwrite them . */ <nl> coprocessor_flush_all ( ti );
int radeon_atombios_init ( struct radeon_device * rdev ) <nl>  <nl> void radeon_atombios_fini ( struct radeon_device * rdev ) <nl> { <nl> - kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> - kfree ( rdev -> mode_info . atom_context ); <nl> + if ( rdev -> mode_info . atom_context ) { <nl> + kfree ( rdev -> mode_info . atom_context -> scratch ); <nl> + kfree ( rdev -> mode_info . atom_context ); <nl> + } <nl> kfree ( rdev -> mode_info . atom_card_info ); <nl> } <nl> 
static int ca8210_probe ( struct spi_device * spi_device ) <nl> goto error ; <nl> } <nl>  <nl> + priv -> spi -> dev . platform_data = pdata ; <nl> ret = ca8210_get_platform_data ( priv -> spi , pdata ); <nl> if ( ret ) { <nl> dev_crit (& spi_device -> dev , " ca8210_get_platform_data failed \ n "); <nl> goto error ; <nl> } <nl> - priv -> spi -> dev . platform_data = pdata ; <nl>  <nl> ret = ca8210_dev_com_init ( priv ); <nl> if ( ret ) {
static inline int gro_cells_init ( struct gro_cells * gcells , struct net_device * de <nl> int i ; <nl>  <nl> gcells -> gro_cells_mask = roundup_pow_of_two ( netif_get_num_default_rss_queues ()) - 1 ; <nl> - gcells -> cells = kcalloc ( sizeof ( struct gro_cell ), <nl> - gcells -> gro_cells_mask + 1 , <nl> + gcells -> cells = kcalloc ( gcells -> gro_cells_mask + 1 , <nl> + sizeof ( struct gro_cell ), <nl> GFP_KERNEL ); <nl> if (! gcells -> cells ) <nl> return - ENOMEM ;
static struct scatterlist * alloc_sgtable ( int size ) <nl> if ( new_page ) <nl> __free_page ( new_page ); <nl> } <nl> + kfree ( table ); <nl> return NULL ; <nl> } <nl> alloc_size = min_t ( int , size , PAGE_SIZE );
static int audit_log_single_execve_arg ( struct audit_context * context , <nl> * so we can be sure nothing was lost . <nl> */ <nl> if (( i == 0 ) && ( too_long )) <nl> - audit_log_format (* ab , " a % d_len =% ld ", arg_num , <nl> + audit_log_format (* ab , " a % d_len =% zu ", arg_num , <nl> has_cntl ? 2 * len : len ); <nl>  <nl> /*
device_create_groups_vargs ( struct class * class , struct device * parent , <nl> goto error ; <nl> } <nl>  <nl> + device_initialize ( dev ); <nl> dev -> devt = devt ; <nl> dev -> class = class ; <nl> dev -> parent = parent ; <nl> device_create_groups_vargs ( struct class * class , struct device * parent , <nl> if ( retval ) <nl> goto error ; <nl>  <nl> - retval = device_register ( dev ); <nl> + retval = device_add ( dev ); <nl> if ( retval ) <nl> goto error ; <nl> 
static int do_video_set_spu_palette ( unsigned int fd , unsigned int cmd , <nl>  <nl> err = get_user ( palp , & up -> palette ); <nl> err |= get_user ( length , & up -> length ); <nl> + if ( err ) <nl> + return - EFAULT ; <nl>  <nl> up_native = compat_alloc_user_space ( sizeof ( struct video_spu_palette )); <nl> err = put_user ( compat_ptr ( palp ), & up_native -> palette );
copy_from_user ( void * to , const void __user * from , unsigned long n ) <nl> __kernel_size_t __copy_size = ( __kernel_size_t ) n ; <nl>  <nl> if ( __copy_size && __access_ok ( __copy_from , __copy_size )) <nl> - return __copy_user ( to , from , __copy_size ); <nl> + __copy_size = __copy_user ( to , from , __copy_size ); <nl> + <nl> + if ( unlikely ( __copy_size )) <nl> + memset ( to + ( n - __copy_size ), 0 , __copy_size ); <nl>  <nl> return __copy_size ; <nl> }
/* to align the pointer to the ( next ) page boundary */ <nl> # define PAGE_ALIGN ( addr ) ((( addr )+ PAGE_SIZE - 1 )& PAGE_MASK ) <nl>  <nl> -# define __PHYSICAL_MASK (((( phys_addr_t ) 1 ) << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> +# define __PHYSICAL_MASK (( phys_addr_t )( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> # define __VIRTUAL_MASK (( 1UL << __VIRTUAL_MASK_SHIFT ) - 1 ) <nl>  <nl> # ifndef __ASSEMBLY__
static int io_files_update_with_index_alloc ( struct io_kiocb * req , <nl> struct file * file ; <nl> int ret , fd ; <nl>  <nl> + if (! req -> ctx -> file_data ) <nl> + return - ENXIO ; <nl> + <nl> for ( done = 0 ; done < req -> rsrc_update . nr_args ; done ++) { <nl> if ( copy_from_user (& fd , & fds [ done ], sizeof ( fd ))) { <nl> ret = - EFAULT ;
static int truncate_data_node ( const struct ubifs_info * c , const struct inode * in <nl> int err , dlen , compr_type , out_len , old_dlen ; <nl>  <nl> out_len = le32_to_cpu ( dn -> size ); <nl> - buf = kmalloc ( out_len * WORST_COMPR_FACTOR , GFP_NOFS ); <nl> + buf = kmalloc_array ( out_len , WORST_COMPR_FACTOR , GFP_NOFS ); <nl> if (! buf ) <nl> return - ENOMEM ; <nl> 
static void xiic_reinit ( struct xiic_i2c * i2c ) <nl> /* Enable interrupts */ <nl> xiic_setreg32 ( i2c , XIIC_DGIER_OFFSET , XIIC_GINTR_ENABLE_MASK ); <nl>  <nl> - xiic_irq_clr_en ( i2c , XIIC_INTR_AAS_MASK | XIIC_INTR_ARB_LOST_MASK ); <nl> + xiic_irq_clr_en ( i2c , XIIC_INTR_ARB_LOST_MASK ); <nl> } <nl>  <nl> static void xiic_deinit ( struct xiic_i2c * i2c )
static int stmmac_pci_probe ( struct pci_dev * pdev , <nl> priv = stmmac_dvr_probe (&( pdev -> dev ), & plat_dat , addr ); <nl> if (! priv ) { <nl> pr_err ("% s : main driver probe failed ", __func__ ); <nl> + ret = - ENODEV ; <nl> goto err_out ; <nl> } <nl> priv -> dev -> irq = pdev -> irq ;
int exynos_eint_wkup_init ( struct samsung_pinctrl_drv_data * d ) <nl> if ( match ) { <nl> irq_chip = kmemdup ( match -> data , <nl> sizeof (* irq_chip ), GFP_KERNEL ); <nl> + if (! irq_chip ) <nl> + return - ENOMEM ; <nl> wkup_np = np ; <nl> break ; <nl> }
static const struct snd_pci_quirk alc662_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x1025 , 0x038b , " Acer Aspire 8943G ", ALC662_FIXUP_ASPIRE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05d8 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05db , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> + SND_PCI_QUIRK ( 0x1028 , 0x0626 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x103c , 0x1632 , " HP RP5800 ", ALC662_FIXUP_HP_RP5800 ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1477 , " ASUS N56VZ ", ALC662_FIXUP_BASS_CHMAP ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1bf3 , " ASUS N76VZ ", ALC662_FIXUP_BASS_CHMAP ),
static int vfio_pci_mmap ( void * device_data , struct vm_area_struct * vma ) <nl> return ret ; <nl>  <nl> vdev -> barmap [ index ] = pci_iomap ( pdev , index , 0 ); <nl> + if (! vdev -> barmap [ index ]) { <nl> + pci_release_selected_regions ( pdev , 1 << index ); <nl> + return - ENOMEM ; <nl> + } <nl> } <nl>  <nl> vma -> vm_private_data = vdev ;
static inline int sctp_frag_point ( const struct sctp_association * asoc , int pmtu ) <nl> if ( asoc -> user_frag ) <nl> frag = min_t ( int , frag , asoc -> user_frag ); <nl>  <nl> - frag = min_t ( int , frag , SCTP_MAX_CHUNK_LEN ); <nl> + frag = WORD_TRUNC ( min_t ( int , frag , SCTP_MAX_CHUNK_LEN )); <nl>  <nl> return frag ; <nl> }
static int sctp_wait_for_sndbuf ( struct sctp_association * asoc , long * timeo_p , <nl> */ <nl> release_sock ( sk ); <nl> current_timeo = schedule_timeout ( current_timeo ); <nl> - BUG_ON ( sk != asoc -> base . sk ); <nl> + if ( sk != asoc -> base . sk ) <nl> + goto do_error ; <nl> lock_sock ( sk ); <nl>  <nl> * timeo_p = current_timeo ;
static int atl2_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl>  <nl> err = - EIO ; <nl>  <nl> - netdev -> hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX ; <nl> + netdev -> hw_features = NETIF_F_HW_VLAN_CTAG_RX ; <nl> netdev -> features |= ( NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX ); <nl>  <nl> /* Init PHY as early as possible due to power saving issue */
static void _tcpm_cc_change ( struct tcpm_port * port , enum typec_cc_status cc1 , <nl> break ; <nl>  <nl> case SRC_TRY : <nl> - tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> + if ( tcpm_port_is_source ( port )) <nl> + tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> break ; <nl> case SRC_TRY_DEBOUNCE : <nl> tcpm_set_state ( port , SRC_TRY , 0 );
int ext4_insert_range ( struct inode * inode , loff_t offset , loff_t len ) <nl> up_write (& EXT4_I ( inode )-> i_data_sem ); <nl> goto out_stop ; <nl> } <nl> + } else { <nl> + ext4_ext_drop_refs ( path ); <nl> + kfree ( path ); <nl> } <nl>  <nl> ret = ext4_es_remove_extent ( inode , offset_lblk ,
struct se_portal_group * tcm_loop_make_naa_tpg ( <nl> tpgt_str += 5 ; /* Skip ahead of " tpgt_ " */ <nl> tpgt = ( unsigned short int ) simple_strtoul ( tpgt_str , & end_ptr , 0 ); <nl>  <nl> - if ( tpgt > TL_TPGS_PER_HBA ) { <nl> + if ( tpgt >= TL_TPGS_PER_HBA ) { <nl> printk ( KERN_ERR " Passed tpgt : % hu exceeds TL_TPGS_PER_HBA :" <nl> " % u \ n ", tpgt , TL_TPGS_PER_HBA ); <nl> return ERR_PTR (- EINVAL );
do { \ <nl> # define this_cpu_generic_read ( pcp ) \ <nl> ({ \ <nl> typeof ( pcp ) __ret ; \ <nl> - preempt_disable (); \ <nl> + preempt_disable_notrace (); \ <nl> __ret = raw_cpu_generic_read ( pcp ); \ <nl> - preempt_enable (); \ <nl> + preempt_enable_notrace (); \ <nl> __ret ; \ <nl> }) <nl> 
smb_proc_setattr_unix ( struct dentry * d , struct iattr * attr , <nl> LSET ( data , 32 , SMB_TIME_NO_CHANGE ); <nl> LSET ( data , 40 , SMB_UID_NO_CHANGE ); <nl> LSET ( data , 48 , SMB_GID_NO_CHANGE ); <nl> - LSET ( data , 56 , smb_filetype_from_mode ( attr -> ia_mode )); <nl> + DSET ( data , 56 , smb_filetype_from_mode ( attr -> ia_mode )); <nl> LSET ( data , 60 , major ); <nl> LSET ( data , 68 , minor ); <nl> LSET ( data , 76 , 0 );
static int set_multi_io ( struct hda_codec * codec , int idx , bool output ) <nl> snd_hda_activate_path ( codec , path , false , true ); <nl> set_pin_target ( codec , nid , spec -> multi_io [ idx ]. ctl_in , true ); <nl> } <nl> + <nl> + /* update jack retasking in case it modifies any of them */ <nl> + snd_hda_gen_hp_automute ( codec , NULL ); <nl> + snd_hda_gen_line_automute ( codec , NULL ); <nl> + snd_hda_gen_mic_autoswitch ( codec , NULL ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int io_rw_init_file ( struct io_kiocb * req , fmode_t mode ) <nl> if (!( kiocb -> ki_flags & IOCB_DIRECT ) || ! file -> f_op -> iopoll ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + kiocb -> private = NULL ; <nl> kiocb -> ki_flags |= IOCB_HIPRI | IOCB_ALLOC_CACHE ; <nl> kiocb -> ki_complete = io_complete_rw_iopoll ; <nl> req -> iopoll_completed = 0 ;
sg_build_indirect ( Sg_scatter_hold * schp , Sg_fd * sfp , int buff_size ) <nl> num = ( rem_sz > scatter_elem_sz_prev ) ? <nl> scatter_elem_sz_prev : rem_sz ; <nl>  <nl> - schp -> pages [ k ] = alloc_pages ( gfp_mask , order ); <nl> + schp -> pages [ k ] = alloc_pages ( gfp_mask | __GFP_ZERO , order ); <nl> if (! schp -> pages [ k ]) <nl> goto out ; <nl> 
int v4l2_async_notifier_register ( struct v4l2_device * v4l2_dev , <nl> struct v4l2_async_subdev * asd ; <nl> int i ; <nl>  <nl> - if (! notifier -> num_subdevs || notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> + if (! v4l2_dev || ! notifier -> num_subdevs || <nl> + notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> return - EINVAL ; <nl>  <nl> notifier -> v4l2_dev = v4l2_dev ;
static int rave_sp_probe ( struct serdev_device * serdev ) <nl> return ret ; <nl>  <nl> serdev_device_set_baudrate ( serdev , baud ); <nl> + serdev_device_set_flow_control ( serdev , false ); <nl> + <nl> + ret = serdev_device_set_parity ( serdev , SERDEV_PARITY_NONE ); <nl> + if ( ret ) { <nl> + dev_err ( dev , " Failed to set parity \ n "); <nl> + return ret ; <nl> + } <nl>  <nl> ret = rave_sp_get_status ( sp ); <nl> if ( ret ) {
int mlxsw_sp_router_init ( struct mlxsw_sp * mlxsw_sp ) <nl> return err ; <nl> mlxsw_sp_lpm_init ( mlxsw_sp ); <nl> mlxsw_sp_vrs_init ( mlxsw_sp ); <nl> - return mlxsw_sp_neigh_init ( mlxsw_sp ); <nl> + err = mlxsw_sp_neigh_init ( mlxsw_sp ); <nl> + if ( err ) <nl> + goto err_neigh_init ; <nl> + return 0 ; <nl> + <nl> + err_neigh_init : <nl> + __mlxsw_sp_router_fini ( mlxsw_sp ); <nl> + return err ; <nl> } <nl>  <nl> void mlxsw_sp_router_fini ( struct mlxsw_sp * mlxsw_sp )
int tcp_rcv_state_process ( struct sock * sk , struct sk_buff * skb , <nl> goto discard ; <nl>  <nl> if ( th -> syn ) { <nl> + if ( th -> fin ) <nl> + goto discard ; <nl> if ( icsk -> icsk_af_ops -> conn_request ( sk , skb ) < 0 ) <nl> return 1 ; <nl> 
static int eb_lookup_vmas ( struct i915_execbuffer * eb ) <nl>  <nl> err = radix_tree_insert ( handles_vma , handle , vma ); <nl> if ( unlikely ( err )) { <nl> - kfree ( lut ); <nl> + kmem_cache_free ( eb -> i915 -> luts , lut ); <nl> goto err_obj ; <nl> } <nl> 
static int __devinit ad7879_probe ( struct spi_device * spi ) <nl> kfree ( ts ); <nl> } <nl>  <nl> - return 0 ; <nl> + return error ; <nl> } <nl>  <nl> static int __devexit ad7879_remove ( struct spi_device * spi ) <nl> static int __devinit ad7879_probe ( struct i2c_client * client , <nl> kfree ( ts ); <nl> } <nl>  <nl> - return 0 ; <nl> + return error ; <nl> } <nl>  <nl> static int __devexit ad7879_remove ( struct i2c_client * client )
static int igb_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> if ( hw -> flash_address ) <nl> iounmap ( hw -> flash_address ); <nl> err_sw_init : <nl> + kfree ( adapter -> shadow_vfta ); <nl> igb_clear_interrupt_scheme ( adapter ); <nl> pci_iounmap ( pdev , hw -> hw_addr ); <nl> err_ioremap :
static void destroy_eps ( struct ci_hdrc * ci ) <nl> for ( i = 0 ; i < ci -> hw_ep_max ; i ++) { <nl> struct ci_hw_ep * hwep = & ci -> ci_hw_ep [ i ]; <nl>  <nl> + if ( hwep -> pending_td ) <nl> + free_pending_td ( hwep ); <nl> dma_pool_free ( ci -> qh_pool , hwep -> qh . ptr , hwep -> qh . dma ); <nl> } <nl> }
static int sco_sock_bind ( struct socket * sock , struct sockaddr * addr , <nl> if (! addr || addr -> sa_family != AF_BLUETOOTH ) <nl> return - EINVAL ; <nl>  <nl> + if ( addr_len < sizeof ( struct sockaddr_sco )) <nl> + return - EINVAL ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state != BT_OPEN ) {
__hwmon_device_register ( struct device * dev , const char * name , void * drvdata , <nl> if ( err ) <nl> goto free_hwmon ; <nl>  <nl> - if ( chip && chip -> ops -> read && <nl> + if ( dev && chip && chip -> ops -> read && <nl> chip -> info [ 0 ]-> type == hwmon_chip && <nl> ( chip -> info [ 0 ]-> config [ 0 ] & HWMON_C_REGISTER_TZ )) { <nl> const struct hwmon_channel_info ** info = chip -> info ;
struct reset_control * of_reset_control_get ( struct device_node * node , <nl> { <nl> int index = 0 ; <nl>  <nl> - if ( id ) <nl> + if ( id ) { <nl> index = of_property_match_string ( node , <nl> " reset - names ", id ); <nl> + if ( index < 0 ) <nl> + return ERR_PTR (- ENOENT ); <nl> + } <nl> return of_reset_control_get_by_index ( node , index ); <nl> } <nl> EXPORT_SYMBOL_GPL ( of_reset_control_get );
static int i40e_setup_macvlans ( struct i40e_vsi * vsi , u16 macvlan_cnt , u16 qcnt , <nl> ch -> num_queue_pairs = qcnt ; <nl> if (! i40e_setup_channel ( pf , vsi , ch )) { <nl> ret = - EINVAL ; <nl> + kfree ( ch ); <nl> goto err_free ; <nl> } <nl> ch -> parent_vsi = vsi ;
int fscrypt_process_policy ( struct inode * inode , <nl> return - EINVAL ; <nl>  <nl> if (! inode_has_encryption_context ( inode )) { <nl> + if (! S_ISDIR ( inode -> i_mode )) <nl> + return - EINVAL ; <nl> if (! inode -> i_sb -> s_cop -> empty_dir ) <nl> return - EOPNOTSUPP ; <nl> if (! inode -> i_sb -> s_cop -> empty_dir ( inode ))
static int tipc_l2_device_event ( struct notifier_block * nb , unsigned long evt , <nl> break ; <nl> case NETDEV_UNREGISTER : <nl> case NETDEV_CHANGENAME : <nl> - bearer_disable ( dev_net ( dev ), b ); <nl> + bearer_disable ( net , b ); <nl> break ; <nl> } <nl> return NOTIFY_OK ;
get_matching_model_microcode ( int cpu , unsigned long start , <nl> unsigned int mc_saved_count = mc_saved_data -> mc_saved_count ; <nl> int i ; <nl>  <nl> - while ( leftover ) { <nl> + while ( leftover && mc_saved_count < ARRAY_SIZE ( mc_saved_tmp )) { <nl> mc_header = ( struct microcode_header_intel *) ucode_ptr ; <nl>  <nl> mc_size = get_totalsize ( mc_header );
static void sixpack_close ( struct tty_struct * tty ) <nl> */ <nl> netif_stop_queue ( sp -> dev ); <nl>  <nl> + unregister_netdev ( sp -> dev ); <nl> + <nl> del_timer_sync (& sp -> tx_t ); <nl> del_timer_sync (& sp -> resync_t ); <nl>  <nl> - unregister_netdev ( sp -> dev ); <nl> - <nl> /* Free all 6pack frame buffers after unreg . */ <nl> kfree ( sp -> rbuff ); <nl> kfree ( sp -> xbuff );
static int snd_imx_open ( struct snd_pcm_substream * substream ) <nl> dma_params = snd_soc_dai_get_dma_data ( rtd -> cpu_dai , substream ); <nl>  <nl> dma_data = kzalloc ( sizeof (* dma_data ), GFP_KERNEL ); <nl> + if (! dma_data ) <nl> + return - ENOMEM ; <nl> + <nl> dma_data -> peripheral_type = dma_params -> shared_peripheral ? <nl> IMX_DMATYPE_SSI_SP : IMX_DMATYPE_SSI ; <nl> dma_data -> priority = DMA_PRIO_HIGH ;
static void imc_common_cpuhp_mem_free ( struct imc_pmu * pmu_ptr ) <nl> } <nl>  <nl> /* Only free the attr_groups which are dynamically allocated */ <nl> - kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]-> attrs ); <nl> + if ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]) <nl> + kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]-> attrs ); <nl> kfree ( pmu_ptr -> attr_groups [ IMC_EVENT_ATTR ]); <nl> kfree ( pmu_ptr ); <nl> return ;
struct drm_i915_file_private { <nl>  <nl> # define HAS_FORCE_WAKE ( dev ) ( INTEL_INFO ( dev )-> has_force_wake ) <nl>  <nl> -# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev )) <nl> +# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev ) || IS_HASWELL ( dev )) <nl>  <nl> # include " i915_trace . h " <nl> 
static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
static struct dm_region * __rh_alloc ( struct dm_region_hash * rh , region_t region ) <nl>  <nl> nreg = mempool_alloc ( rh -> region_pool , GFP_ATOMIC ); <nl> if ( unlikely (! nreg )) <nl> - nreg = kmalloc ( sizeof (* nreg ), GFP_NOIO ); <nl> + nreg = kmalloc ( sizeof (* nreg ), GFP_NOIO | __GFP_NOFAIL ); <nl>  <nl> nreg -> state = rh -> log -> type -> in_sync ( rh -> log , region , 1 ) ? <nl> DM_RH_CLEAN : DM_RH_NOSYNC ;
static int mwifiex_pcie_alloc_cmdrsp_buf ( struct mwifiex_adapter * adapter ) <nl> } <nl> skb_put ( skb , MWIFIEX_UPLD_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MWIFIEX_UPLD_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> return - 1 ; <nl> + } <nl>  <nl> card -> cmdrsp_buf = skb ; <nl> 
static int pl2303_tiocmset ( struct tty_struct * tty , <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl>  <nl> mutex_lock (& serial -> disc_mutex ); <nl> - if (! serial -> disconnected ) <nl> + if (! serial -> disconnected ) { <nl> ret = pl2303_set_control_lines ( port , control ); <nl> - else <nl> + if ( ret ) <nl> + ret = usb_translate_errors ( ret ); <nl> + } else { <nl> ret = - ENODEV ; <nl> + } <nl> mutex_unlock (& serial -> disc_mutex ); <nl>  <nl> return ret ;
static int ac100_rtc_probe ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> chip = devm_kzalloc (& pdev -> dev , sizeof (* chip ), GFP_KERNEL ); <nl> + if (! chip ) <nl> + return - ENOMEM ; <nl> + <nl> platform_set_drvdata ( pdev , chip ); <nl> chip -> dev = & pdev -> dev ; <nl> chip -> regmap = ac100 -> regmap ;
int ip6_xmit ( struct sock * sk , struct sk_buff * skb , struct flowi * fl , <nl> skb_reset_network_header ( skb ); <nl> hdr = ipv6_hdr ( skb ); <nl>  <nl> + /* Allow local fragmentation . */ <nl> + if ( ipfragok ) <nl> + skb -> local_df = 1 ; <nl> + <nl> /* <nl> * Fill in the IPv6 header <nl> */
static long __video_do_ioctl ( struct file * file , <nl> break ; <nl>  <nl> ret = 0 ; <nl> + p -> parm . capture . readbuffers = 2 ; <nl> if ( ops -> vidioc_g_std ) <nl> ret = ops -> vidioc_g_std ( file , fh , & std ); <nl> if ( ret == 0 )
static int do_loopback ( struct path * path , const char * old_name , <nl>  <nl> if ( IS_ERR ( mnt )) { <nl> err = PTR_ERR ( mnt ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> err = graft_tree ( mnt , path );
static void ath9k_hw_get_def_gain_boundaries_pdadcs ( struct ath_hw * ah , <nl> vpdTableI [ i ][ sizeCurrVpdTable - 2 ]); <nl> vpdStep = ( int16_t )(( vpdStep < 1 ) ? 1 : vpdStep ); <nl>  <nl> - if ( tgtIndex > maxIndex ) { <nl> + if ( tgtIndex >= maxIndex ) { <nl> while (( ss <= tgtIndex ) && <nl> ( k < ( AR5416_NUM_PDADC_VALUES - 1 ))) { <nl> tmpVal = ( int16_t )(( vpdTableI [ i ][ sizeCurrVpdTable - 1 ] +
irqreturn_t uic_cascade ( int virq , void * data ) <nl> int subvirq ; <nl>  <nl> msr = mfdcr ( uic -> dcrbase + UIC_MSR ); <nl> + if (! msr ) /* spurious interrupt */ <nl> + return IRQ_HANDLED ; <nl> + <nl> src = 32 - ffs ( msr ); <nl>  <nl> subvirq = irq_linear_revmap ( uic -> irqhost , src );
xfsbufd ( <nl>  <nl> current -> flags |= PF_MEMALLOC ; <nl>  <nl> + set_freezable (); <nl> + <nl> do { <nl> if ( unlikely ( freezing ( current ))) { <nl> set_bit ( XBT_FORCE_SLEEP , & target -> bt_flags );
static bool snd_soc_set_cache_val ( void * base , unsigned int idx , <nl> static unsigned int snd_soc_get_cache_val ( const void * base , unsigned int idx , <nl> unsigned int word_size ) <nl> { <nl> + if (! base ) <nl> + return - 1 ; <nl> + <nl> switch ( word_size ) { <nl> case 1 : { <nl> const u8 * cache = base ;
static void usb_alphatrack_disconnect ( struct usb_interface * intf ) <nl> mutex_unlock (& dev -> mtx ); <nl> usb_alphatrack_delete ( dev ); <nl> } else { <nl> + atomic_set (& dev -> writes_pending , 0 ); <nl> dev -> intf = NULL ; <nl> mutex_unlock (& dev -> mtx ); <nl> } <nl>  <nl> - atomic_set (& dev -> writes_pending , 0 ); <nl> mutex_unlock (& disconnect_mutex ); <nl>  <nl> dev_info (& intf -> dev , " Alphatrack Surface #% d now disconnected \ n ",
static struct irq_chip amd_gpio_irqchip = { <nl> . irq_set_type = amd_gpio_irq_set_type , <nl> }; <nl>  <nl> - static void amd_gpio_irq_handler ( unsigned int irq , struct irq_desc * desc ) <nl> + static void amd_gpio_irq_handler ( unsigned int __irq , struct irq_desc * desc ) <nl> { <nl> + unsigned int irq = irq_desc_get_irq ( desc ); <nl> u32 i ; <nl> u32 off ; <nl> u32 reg ;
unsigned long slice_get_unmapped_area ( unsigned long addr , unsigned long len , <nl> unsigned long high_limit ; <nl>  <nl> high_limit = DEFAULT_MAP_WINDOW ; <nl> - if ( addr >= high_limit ) <nl> + if ( addr >= high_limit || ( fixed && ( addr + len > high_limit ))) <nl> high_limit = TASK_SIZE ; <nl>  <nl> if ( len > high_limit )
static int pfkey_register ( struct sock * sk , struct sk_buff * skb , const struct sad <nl>  <nl> xfrm_probe_algs (); <nl>  <nl> - supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL ); <nl> + supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL | __GFP_ZERO ); <nl> if (! supp_skb ) { <nl> if ( hdr -> sadb_msg_satype != SADB_SATYPE_UNSPEC ) <nl> pfk -> registered &= ~( 1 << hdr -> sadb_msg_satype );
int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl> if ( ret != - EPROBE_DEFER ) <nl> list_del (& driver -> pending ); <nl> if ( ret ) <nl> - goto err4 ; <nl> + goto err5 ; <nl> break ; <nl> } <nl> } <nl> int usb_add_gadget_udc_release ( struct device * parent , struct usb_gadget * gadget , <nl>  <nl> return 0 ; <nl>  <nl> + err5 : <nl> + device_del (& udc -> dev ); <nl> + <nl> err4 : <nl> list_del (& udc -> list ); <nl> mutex_unlock (& udc_lock );
void scsi_io_completion ( struct scsi_cmnd * cmd , unsigned int good_bytes ) <nl> */ <nl> req -> next_rq -> resid_len = scsi_in ( cmd )-> resid ; <nl>  <nl> + scsi_release_buffers ( cmd ); <nl> blk_end_request_all ( req , 0 ); <nl>  <nl> - scsi_release_buffers ( cmd ); <nl> scsi_next_command ( cmd ); <nl> return ; <nl> }
static void flush_tmregs_to_thread ( struct task_struct * tsk ) <nl> * in the appropriate thread structures from live . <nl> */ <nl>  <nl> - if ( tsk != current ) <nl> + if ((! cpu_has_feature ( CPU_FTR_TM )) || ( tsk != current )) <nl> return ; <nl>  <nl> if ( MSR_TM_SUSPENDED ( mfmsr ())) {
int i2400m_msg_check_status ( const struct i2400m_l3l4_hdr * l3l4_hdr , <nl>  <nl> if ( status == 0 ) <nl> return 0 ; <nl> - if ( status > ARRAY_SIZE ( ms_to_errno )) { <nl> + if ( status >= ARRAY_SIZE ( ms_to_errno )) { <nl> str = " unknown status code "; <nl> result = - EBADR ; <nl> } else {
static void rs_free_sta ( void * priv_r , struct ieee80211_sta * sta , <nl> void * priv_sta ) <nl> { <nl> struct iwl_lq_sta * lq_sta = priv_sta ; <nl> - struct iwl_priv * priv = priv_r ; <nl> + struct iwl_priv * priv __maybe_unused = priv_r ; <nl>  <nl> IWL_DEBUG_RATE (" enter \ n "); <nl> kfree ( lq_sta );
static struct urb * usbtv_setup_iso_transfer ( struct usbtv * usbtv ) <nl> ip -> transfer_flags = URB_ISO_ASAP ; <nl> ip -> transfer_buffer = kzalloc ( size * USBTV_ISOC_PACKETS , <nl> GFP_KERNEL ); <nl> + if (! ip -> transfer_buffer ) { <nl> + usb_free_urb ( ip ); <nl> + return NULL ; <nl> + } <nl> ip -> complete = usbtv_iso_cb ; <nl> ip -> number_of_packets = USBTV_ISOC_PACKETS ; <nl> ip -> transfer_buffer_length = size * USBTV_ISOC_PACKETS ;
static int atmel_serial_remove ( struct platform_device * pdev ) <nl> struct atmel_uart_port * atmel_port = to_atmel_uart_port ( port ); <nl> int ret = 0 ; <nl>  <nl> + tasklet_kill (& atmel_port -> tasklet ); <nl> + <nl> device_init_wakeup (& pdev -> dev , 0 ); <nl>  <nl> ret = uart_remove_one_port (& atmel_uart , port ); <nl>  <nl> - tasklet_kill (& atmel_port -> tasklet ); <nl> kfree ( atmel_port -> rx_ring . buf ); <nl>  <nl> /* " port " is allocated statically , so we shouldn ' t free it */
static void load_render_mocs ( struct drm_i915_private * dev_priv ) <nl> }; <nl> int ring_id , i ; <nl>  <nl> - for ( ring_id = 0 ; ring_id < I915_NUM_ENGINES ; ring_id ++) { <nl> + for ( ring_id = 0 ; ring_id < ARRAY_SIZE ( regs ); ring_id ++) { <nl> offset . reg = regs [ ring_id ]; <nl> for ( i = 0 ; i < 64 ; i ++) { <nl> gen9_render_mocs . control_table [ ring_id ][ i ] =
static int cpufreq_governor_dbs ( struct cpufreq_policy * policy , <nl> if ( latency == 0 ) <nl> latency = 1 ; <nl>  <nl> - def_sampling_rate = latency * <nl> + def_sampling_rate = 10 * latency * <nl> DEF_SAMPLING_RATE_LATENCY_MULTIPLIER ; <nl>  <nl> if ( def_sampling_rate < MIN_STAT_SAMPLING_RATE )
qdio_check_ccq ( struct qdio_q * q , unsigned int ccq ) <nl> { <nl> char dbf_text [ 15 ]; <nl>  <nl> - if ( ccq == 0 || ccq == 32 || ccq == 96 ) <nl> + if ( ccq == 0 || ccq == 32 ) <nl> return 0 ; <nl> - if ( ccq == 97 ) <nl> + if ( ccq == 96 || ccq == 97 ) <nl> return 1 ; <nl> /* notify devices immediately */ <nl> sprintf ( dbf_text ,"% d ", ccq );
int btrfs_read_sys_array ( struct btrfs_root * root ) <nl> sb_array_offset += len ; <nl> cur_offset += len ; <nl> } <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return ret ; <nl>  <nl> out_short_read : <nl> printk ( KERN_ERR " BTRFS : sys_array too short to read % u bytes at offset % u \ n ", <nl> len , cur_offset ); <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return - EIO ; <nl> }
static void t3e3_remove_card ( struct pci_dev * pdev ) <nl> struct channel * channel0 = pci_get_drvdata ( pdev ); <nl> struct card * card = channel0 -> card ; <nl>  <nl> - del_timer (& card -> timer ); <nl> + del_timer_sync (& card -> timer ); <nl> if ( has_two_ports ( channel0 -> pdev )) { <nl> t3e3_remove_channel (& card -> channels [ 1 ]); <nl> pci_dev_put ( card -> channels [ 1 ]. pdev );
aoedev_freedev ( struct aoedev * d ) <nl> put_disk ( d -> gd ); <nl> } <nl> kfree ( d -> frames ); <nl> - mempool_destroy ( d -> bufpool ); <nl> + if ( d -> bufpool ) <nl> + mempool_destroy ( d -> bufpool ); <nl> kfree ( d ); <nl> } <nl> 
struct fman_mac * memac_config ( struct fman_mac_params * params ) <nl> /* Save FMan revision */ <nl> fman_get_revision ( memac -> fm , & memac -> fm_rev_info ); <nl>  <nl> - if ( memac -> phy_if == PHY_INTERFACE_MODE_SGMII ) { <nl> + if ( memac -> phy_if == PHY_INTERFACE_MODE_SGMII || <nl> + memac -> phy_if == PHY_INTERFACE_MODE_QSGMII ) { <nl> if (! params -> internal_phy_node ) { <nl> pr_err (" PCS PHY node is not available \ n "); <nl> memac_free ( memac );
static int start_ap ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> priv = wiphy_priv ( wiphy ); <nl> vif = netdev_priv ( dev ); <nl> - wl = vif -> wilc ; <nl> + wl = vif -> wilc ; <nl>  <nl> s32Error = set_channel ( wiphy , & settings -> chandef ); <nl> 
static int bpa10x_probe ( struct usb_interface * intf , const struct usb_device_id * <nl> if ( ignore ) <nl> return - ENODEV ; <nl>  <nl> + if ( intf -> cur_altsetting -> desc . bInterfaceNumber > 0 ) <nl> + return - ENODEV ; <nl> + <nl> data = kmalloc ( sizeof (* data ), GFP_KERNEL ); <nl> if (! data ) { <nl> BT_ERR (" Can ' t allocate data structure ");
static void pptp_expectfn ( struct nf_conn * ct , <nl>  <nl> rcu_read_lock (); <nl> nf_nat_pptp_expectfn = rcu_dereference ( nf_nat_pptp_hook_expectfn ); <nl> - if ( nf_nat_pptp_expectfn && ct -> status & IPS_NAT_MASK ) <nl> + if ( nf_nat_pptp_expectfn && ct -> master -> status & IPS_NAT_MASK ) <nl> nf_nat_pptp_expectfn ( ct , exp ); <nl> else { <nl> struct nf_conntrack_tuple inv_t ;
static int __arm_v7s_map ( struct arm_v7s_io_pgtable * data , unsigned long iova , <nl> pte |= ARM_V7S_ATTR_NS_TABLE ; <nl>  <nl> __arm_v7s_set_pte ( ptep , pte , 1 , cfg ); <nl> - } else { <nl> + } else if ( ARM_V7S_PTE_IS_TABLE ( pte , lvl )) { <nl> cptep = iopte_deref ( pte , lvl ); <nl> + } else { <nl> + /* We require an unmap first */ <nl> + WARN_ON (! selftest_running ); <nl> + return - EEXIST ; <nl> } <nl>  <nl> /* Rinse , repeat */
static int pn_send ( struct sk_buff * skb , struct net_device * dev , <nl> struct phonethdr * ph ; <nl> int err ; <nl>  <nl> - if ( skb -> len + 2 > 0xffff ) { <nl> - /* Phonet length field would overflow */ <nl> + if ( skb -> len + 2 > 0xffff /* Phonet length field limit */ || <nl> + skb -> len + sizeof ( struct phonethdr ) > dev -> mtu ) { <nl> err = - EMSGSIZE ; <nl> goto drop ; <nl> }
static int imx2_wdt_set_timeout ( struct watchdog_device * wdog , <nl> { <nl> struct imx2_wdt_device * wdev = watchdog_get_drvdata ( wdog ); <nl>  <nl> + wdog -> timeout = new_timeout ; <nl> + <nl> regmap_update_bits ( wdev -> regmap , IMX2_WDT_WCR , IMX2_WDT_WCR_WT , <nl> WDOG_SEC_TO_COUNT ( new_timeout )); <nl> return 0 ;
static int device_rx_srv ( struct vnt_private * pDevice , unsigned int uIdx ) <nl> pRD = pRD -> next ) { <nl> if ( works ++ > 15 ) <nl> break ; <nl> + <nl> + if (! pRD -> pRDInfo -> skb ) <nl> + break ; <nl> + <nl> if ( vnt_receive_frame ( pDevice , pRD )) { <nl> if (! device_alloc_rx_buf ( pDevice , pRD )) { <nl> dev_err (& pDevice -> pcid -> dev ,
# include < linux / interrupt . h > <nl> # include < linux / pci . h > <nl> # include < linux / firmware . h > <nl> +# include < linux / vmalloc . h > <nl> # include < asm / io . h > <nl> # include < sound / core . h > <nl> # include " mixart . h "
i915_gem_wait_ioctl ( struct drm_device * dev , void * data , struct drm_file * file ) <nl> u32 seqno = 0 ; <nl> int ret = 0 ; <nl>  <nl> + if ( args -> flags != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = i915_mutex_lock_interruptible ( dev ); <nl> if ( ret ) <nl> return ret ;
void rcu_check_callbacks ( int cpu , int user ) <nl> static void rcu_init_percpu_data ( int cpu , struct rcu_ctrlblk * rcp , <nl> struct rcu_data * rdp ) <nl> { <nl> - long flags ; <nl> + unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& rcp -> lock , flags ); <nl> memset ( rdp , 0 , sizeof (* rdp ));
composite_setup ( struct usb_gadget * gadget , const struct usb_ctrlrequest * ctrl ) <nl> if ( w_index != 0x5 || ( w_value >> 8 )) <nl> break ; <nl> interface = w_value & 0xFF ; <nl> + if ( interface >= MAX_CONFIG_INTERFACES || <nl> + ! os_desc_cfg -> interface [ interface ]) <nl> + break ; <nl> buf [ 6 ] = w_index ; <nl> count = count_ext_prop ( os_desc_cfg , <nl> interface );
int dw_mci_probe ( struct dw_mci * host ) <nl> } <nl> } <nl>  <nl> - if ( host -> pdata -> num_slots > 1 ) { <nl> + if ( host -> pdata -> num_slots < 1 ) { <nl> dev_err ( host -> dev , <nl> " Platform data must supply num_slots .\ n "); <nl> return - ENODEV ;
static int iucv_sock_sendmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> txmsg . class = 0 ; <nl> + memcpy (& txmsg . class , skb -> data , skb -> len >= 4 ? 4 : skb -> len ); <nl> txmsg . tag = iucv -> send_tag ++; <nl> memcpy ( skb -> cb , & txmsg . tag , 4 ); <nl> skb_queue_tail (& iucv -> send_skb_q , skb );
static const struct snd_pci_quirk alc269_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x103c , 0x226e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x2271 , " HP ", ALC286_FIXUP_HP_GPIO_LED ), <nl> SND_PCI_QUIRK ( 0x103c , 0x2272 , " HP ", ALC280_FIXUP_HP_DOCK_PINS ), <nl> + SND_PCI_QUIRK ( 0x103c , 0x2273 , " HP ", ALC280_FIXUP_HP_DOCK_PINS ), <nl> SND_PCI_QUIRK ( 0x103c , 0x229e , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22b2 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ), <nl> SND_PCI_QUIRK ( 0x103c , 0x22b7 , " HP ", ALC269_FIXUP_HP_MUTE_LED_MIC1 ),
int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> + mutex_lock (& gpu -> lock ); <nl> + <nl> fence = etnaviv_gpu_fence_alloc ( gpu ); <nl> if (! fence ) { <nl> event_free ( gpu , event ); <nl> int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> - mutex_lock (& gpu -> lock ); <nl> - <nl> gpu -> event [ event ]. fence = fence ; <nl> submit -> fence = fence -> seqno ; <nl> gpu -> active_fence = submit -> fence ;
struct snd_kcontrol * snd_soc_cnew ( const struct snd_kcontrol_new * _template , <nl>  <nl> if ( prefix ) { <nl> name_len = strlen ( long_name ) + strlen ( prefix ) + 2 ; <nl> - name = kmalloc ( name_len , GFP_ATOMIC ); <nl> + name = kmalloc ( name_len , GFP_KERNEL ); <nl> if (! name ) <nl> return NULL ; <nl> 
xfs_fs_remount ( <nl>  <nl> /* ro -> rw */ <nl> if (( mp -> m_flags & XFS_MOUNT_RDONLY ) && !(* flags & MS_RDONLY )) { <nl> + if ( mp -> m_flags & XFS_MOUNT_NORECOVERY ) { <nl> + xfs_warn ( mp , <nl> + " ro -> rw transition prohibited on norecovery mount "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> mp -> m_flags &= ~ XFS_MOUNT_RDONLY ; <nl>  <nl> /*
struct kvm_s390_float_interrupt { <nl> struct list_head list ; <nl> atomic_t active ; <nl> int next_rr_cpu ; <nl> - unsigned long idle_mask [( 64 + sizeof ( long ) - 1 ) / sizeof ( long )]; <nl> - struct kvm_s390_local_interrupt * local_int [ 64 ]; <nl> + unsigned long idle_mask [( KVM_MAX_VCPUS + sizeof ( long ) - 1 ) <nl> + / sizeof ( long )]; <nl> + struct kvm_s390_local_interrupt * local_int [ KVM_MAX_VCPUS ]; <nl> }; <nl>  <nl> 
static void _qed_iscsi_get_tstats ( struct qed_hwfn * p_hwfn , <nl> HILO_64_REGPAIR ( tstats . iscsi_rx_bytes_cnt ); <nl> p_stats -> iscsi_rx_packet_cnt = <nl> HILO_64_REGPAIR ( tstats . iscsi_rx_packet_cnt ); <nl> + p_stats -> iscsi_rx_new_ooo_isle_events_cnt = <nl> + HILO_64_REGPAIR ( tstats . iscsi_rx_new_ooo_isle_events_cnt ); <nl> p_stats -> iscsi_cmdq_threshold_cnt = <nl> le32_to_cpu ( tstats . iscsi_cmdq_threshold_cnt ); <nl> p_stats -> iscsi_rq_threshold_cnt =
void Dot11d_Init ( struct ieee80211_device * ieee ) <nl>  <nl> pDot11dInfo -> State = DOT11D_STATE_NONE ; <nl> pDot11dInfo -> CountryIeLen = 0 ; <nl> - memset ( pDot11dInfo -> channel_map , 0 , MAX_CHANNEL_NUMBER + 1 ); <nl> + memset ( pDot11dInfo -> channel_map , 0 , MAX_CHANNEL_NUMBER + 1 ); <nl> memset ( pDot11dInfo -> MaxTxPwrDbmList , 0xFF , MAX_CHANNEL_NUMBER + 1 ); <nl> RESET_CIE_WATCHDOG ( ieee ); <nl> 
static const struct labpc_board_struct labpc_cs_boards [] = { <nl> }, <nl> }; <nl>  <nl> -/* <nl> - * Useful for shorthand access to the particular board structure <nl> - */ <nl> -# define thisboard (( const struct labpc_board_struct *) dev -> board_ptr ) <nl> - <nl> static int labpc_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> { <nl> + const struct labpc_board_struct * thisboard = comedi_board ( dev ); <nl> struct labpc_private * devpriv ; <nl> unsigned long iobase = 0 ; <nl> unsigned int irq = 0 ;
static void scsi_finish_async_scan ( struct async_scan_data * data ) <nl> printk ("% s called twice for host % d ", __FUNCTION__ , <nl> shost -> host_no ); <nl> dump_stack (); <nl> + mutex_unlock (& shost -> scan_mutex ); <nl> return ; <nl> } <nl> 
static void ide_complete_power_step ( ide_drive_t * drive , struct request * rq , u8 s <nl>  <nl> switch ( rq -> pm -> pm_step ) { <nl> case ide_pm_flush_cache : /* Suspend step 1 ( flush cache ) complete */ <nl> - if ( rq -> pm -> pm_state == 4 ) <nl> + if ( rq -> pm -> pm_state == PM_EVENT_FREEZE ) <nl> rq -> pm -> pm_step = ide_pm_state_completed ; <nl> else <nl> rq -> pm -> pm_step = idedisk_pm_standby ;
void spuctx_switch_state ( struct spu_context * ctx , <nl> node = spu -> node ; <nl> if ( old_state == SPU_UTIL_USER ) <nl> atomic_dec (& cbe_spu_info [ node ]. busy_spus ); <nl> - if ( new_state == SPU_UTIL_USER ); <nl> + if ( new_state == SPU_UTIL_USER ) <nl> atomic_inc (& cbe_spu_info [ node ]. busy_spus ); <nl> } <nl> }
static int create_filter ( struct trace_event_call * call , <nl> if ( err && set_str ) <nl> append_filter_err ( ps , filter ); <nl> } <nl> + if ( err && ! set_str ) { <nl> + free_event_filter ( filter ); <nl> + filter = NULL ; <nl> + } <nl> create_filter_finish ( ps ); <nl>  <nl> * filterp = filter ;
static int snapshot_open ( struct inode * inode , struct file * filp ) <nl> if ( error ) <nl> pm_notifier_call_chain ( PM_POST_RESTORE ); <nl> } <nl> - if ( error ) <nl> + if ( error ) { <nl> + free_basic_memory_bitmaps (); <nl> atomic_inc (& snapshot_device_available ); <nl> + } <nl> data -> frozen = 0 ; <nl> data -> ready = 0 ; <nl> data -> platform_support = 0 ;
fill_write_buffer ( struct sysfs_buffer * buffer , const char __user * buf , size_t <nl> return - ENOMEM ; <nl>  <nl> if ( count >= PAGE_SIZE ) <nl> - count = PAGE_SIZE ; <nl> + count = PAGE_SIZE - 1 ; <nl> error = copy_from_user ( buffer -> page , buf , count ); <nl> buffer -> needs_read_fill = 1 ; <nl> return error ? - EFAULT : count ;
static int ipw2100_get_firmware ( struct ipw2100_priv * priv , <nl> return 0 ; <nl> } <nl>  <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("- i ")); <nl> +# ifdef CONFIG_IPW2100_MONITOR <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("- p ")); <nl> +# endif <nl> + MODULE_FIRMWARE ( IPW2100_FW_NAME ("")); <nl> + <nl> static void ipw2100_release_firmware ( struct ipw2100_priv * priv , <nl> struct ipw2100_fw * fw ) <nl> {
static long __write_once initfree = 1 ; <nl> static int __init set_initfree ( char * str ) <nl> { <nl> long val ; <nl> - if ( strict_strtol ( str , 0 , & val )) { <nl> + if ( strict_strtol ( str , 0 , & val ) == 0 ) { <nl> initfree = val ; <nl> pr_info (" initfree : % s free init pages \ n ", <nl> initfree ? " will " : " won ' t ");
static int rtnl_fill_ifinfo ( struct sk_buff * skb , struct net_device * dev , <nl> * report anything . <nl> */ <nl> ivi . spoofchk = - 1 ; <nl> + memset ( ivi . mac , 0 , sizeof ( ivi . mac )); <nl> if ( dev -> netdev_ops -> ndo_get_vf_config ( dev , i , & ivi )) <nl> break ; <nl> vf_mac . vf =
set_pte_phys ( unsigned long vaddr , unsigned long phys , pgprot_t prot ) <nl> new_pte = pfn_pte ( phys >> PAGE_SHIFT , prot ); <nl>  <nl> pte = pte_offset_kernel ( pmd , vaddr ); <nl> - if (! pte_none (* pte ) && <nl> + if (! pte_none (* pte ) && pte_val ( new_pte ) && <nl> pte_val (* pte ) != ( pte_val ( new_pte ) & __supported_pte_mask )) <nl> pte_ERROR (* pte ); <nl> set_pte ( pte , new_pte );
void gen8_fbc_sw_flush ( struct drm_device * dev , u32 value ) <nl> if (! IS_GEN8 ( dev )) <nl> return ; <nl>  <nl> + if (! intel_fbc_enabled ( dev )) <nl> + return ; <nl> + <nl> I915_WRITE ( MSG_FBC_REND_STATE , value ); <nl> } <nl> 
static int ax25_release ( struct socket * sock ) <nl> ax25_destroy_socket ( ax25 ); <nl> } <nl> if ( ax25_dev ) { <nl> + del_timer_sync (& ax25 -> timer ); <nl> + del_timer_sync (& ax25 -> t1timer ); <nl> + del_timer_sync (& ax25 -> t2timer ); <nl> + del_timer_sync (& ax25 -> t3timer ); <nl> + del_timer_sync (& ax25 -> idletimer ); <nl> dev_put_track ( ax25_dev -> dev , & ax25_dev -> dev_tracker ); <nl> ax25_dev_put ( ax25_dev ); <nl> }
static int ttm_mem_init_dma32_zone ( struct ttm_mem_global * glob , <nl> * No special dma32 zone needed . <nl> */ <nl>  <nl> - if ( mem <= (( uint64_t ) 1ULL << 32 )) <nl> + if ( mem <= (( uint64_t ) 1ULL << 32 )) { <nl> + kfree ( zone ); <nl> return 0 ; <nl> + } <nl>  <nl> /* <nl> * Limit max dma32 memory to 4GB for now
static void __ipr_remove ( struct pci_dev * pdev ) <nl> spin_unlock_irqrestore ( ioa_cfg -> host -> host_lock , host_lock_flags ); <nl> wait_event ( ioa_cfg -> reset_wait_q , ! ioa_cfg -> in_reset_reload ); <nl> flush_work (& ioa_cfg -> work_q ); <nl> + INIT_LIST_HEAD (& ioa_cfg -> used_res_q ); <nl> spin_lock_irqsave ( ioa_cfg -> host -> host_lock , host_lock_flags ); <nl>  <nl> spin_lock (& ipr_driver_lock );
static int get_port_device_capability ( struct pci_dev * dev ) <nl> if ( reg32 & SLOT_HP_CAPABLE_MASK ) <nl> services |= PCIE_PORT_SERVICE_HP ; <nl> } <nl> - /* PME Capable */ <nl> - pos = pci_find_capability ( dev , PCI_CAP_ID_PME ); <nl> - if ( pos ) <nl> + /* PME Capable - root port capability */ <nl> + if ((( reg16 >> 4 ) & PORT_TYPE_MASK ) == PCIE_RC_PORT ) <nl> services |= PCIE_PORT_SERVICE_PME ; <nl>  <nl> pos = PCI_CFG_SPACE_SIZE ;
struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
int mdiobus_register ( struct mii_bus * bus ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> + put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
xlog_finish_defer_ops ( <nl> 0 , XFS_TRANS_RESERVE , & tp ); <nl> if ( error ) <nl> return error ; <nl> + /* dfops is already populated so assign it manually */ <nl> + tp -> t_dfops = dfops ; <nl>  <nl> error = xfs_defer_finish (& tp , dfops ); <nl> if ( error )
static void do_boot_cpu ( __u8 cpuid ); <nl> static void do_quad_bootstrap ( void ); <nl>  <nl> int hard_smp_processor_id ( void ); <nl> + int safe_smp_processor_id ( void ); <nl>  <nl> /* Inline functions */ <nl> static inline void <nl> hard_smp_processor_id ( void ) <nl> return 0 ; <nl> } <nl>  <nl> + int <nl> + safe_smp_processor_id ( void ) <nl> +{ <nl> + return hard_smp_processor_id (); <nl> +} <nl> + <nl> /* broadcast a halt to all other CPUs */ <nl> void <nl> smp_send_stop ( void )
void e1000e_reset ( struct e1000_adapter * adapter ) <nl> e1000e_reset_adaptive ( hw ); <nl> e1000_get_phy_info ( hw ); <nl>  <nl> - if (!( adapter -> flags & FLAG_SMART_POWER_DOWN )) { <nl> + if (( adapter -> flags & FLAG_HAS_SMART_POWER_DOWN ) && <nl> + !( adapter -> flags & FLAG_SMART_POWER_DOWN )) { <nl> u16 phy_data = 0 ; <nl> /* <nl> * speed up time to link by disabling smart power down , ignore
static ssize_t environ_read ( struct file * file , char __user * buf , <nl> struct mm_struct * mm = file -> private_data ; <nl> unsigned long env_start , env_end ; <nl>  <nl> - if (! mm ) <nl> + /* Ensure the process spawned far enough to have an environment . */ <nl> + if (! mm || ! mm -> env_end ) <nl> return 0 ; <nl>  <nl> page = ( char *) __get_free_page ( GFP_TEMPORARY );
static int rose_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> skb_copy_datagram_iovec ( skb , 0 , msg -> msg_iov , copied ); <nl>  <nl> if ( srose != NULL ) { <nl> + memset ( srose , 0 , msg -> msg_namelen ); <nl> srose -> srose_family = AF_ROSE ; <nl> srose -> srose_addr = rose -> dest_addr ; <nl> srose -> srose_call = rose -> dest_call ;
static long DAC960_gam_ioctl ( struct file * file , unsigned int Request , <nl> else <nl> ErrorCode = 0 ; <nl> } <nl> + break ; <nl> default : <nl> ErrorCode = - ENOTTY ; <nl> }
unsigned long parse_tag_value ( const char * str , struct parse_tag * tags ) <nl> if ( s != endptr ) <nl> break ; <nl>  <nl> + if ( value > ULONG_MAX / i -> mult ) <nl> + break ; <nl> value *= i -> mult ; <nl> return value ; <nl> }
static ssize_t pubek_show ( struct device * dev , struct device_attribute * attr , <nl> ssize_t err ; <nl> int i , rc ; <nl> char * str = buf ; <nl> - <nl> struct tpm_chip * chip = to_tpm_chip ( dev ); <nl>  <nl> + memset (& tpm_cmd , 0 , sizeof ( tpm_cmd )); <nl> + <nl> tpm_cmd . header . in = tpm_readpubek_header ; <nl> err = tpm_transmit_cmd ( chip , NULL , & tpm_cmd , READ_PUBEK_RESULT_SIZE , <nl> READ_PUBEK_RESULT_MIN_BODY_SIZE , 0 ,
static void set_pool_mode ( struct pool * pool , enum pool_mode new_mode ) <nl> case PM_WRITE : <nl> if ( old_mode != new_mode ) <nl> notify_of_pool_mode_change ( pool , " write "); <nl> + pool -> pf . error_if_no_space = pt -> requested_pf . error_if_no_space ; <nl> dm_pool_metadata_read_write ( pool -> pmd ); <nl> pool -> process_bio = process_bio ; <nl> pool -> process_discard = process_discard_bio ;
static int msm_otg_read_dt ( struct platform_device * pdev , struct msm_otg * motg ) <nl> motg -> pdata = pdata ; <nl>  <nl> id = of_match_device ( msm_otg_dt_match , & pdev -> dev ); <nl> - pdata -> phy_type = ( int ) id -> data ; <nl> + pdata -> phy_type = ( enum msm_usb_phy_type ) id -> data ; <nl>  <nl> motg -> link_rst = devm_reset_control_get (& pdev -> dev , " link "); <nl> if ( IS_ERR ( motg -> link_rst ))
int dccp_disconnect ( struct sock * sk , int flags ) <nl> sk -> sk_err = ECONNRESET ; <nl>  <nl> dccp_clear_xmit_timers ( sk ); <nl> + <nl> __skb_queue_purge (& sk -> sk_receive_queue ); <nl> + __skb_queue_purge (& sk -> sk_write_queue ); <nl> if ( sk -> sk_send_head != NULL ) { <nl> __kfree_skb ( sk -> sk_send_head ); <nl> sk -> sk_send_head = NULL ;
static int crypto_report ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
static int sgtl5000_set_clock ( struct snd_soc_codec * codec , int frame_rate ) <nl> } else { <nl> dev_err ( codec -> dev , <nl> " PLL not supported in slave mode \ n "); <nl> + dev_err ( codec -> dev , "% d ratio is not supported . " <nl> + " SYS_MCLK needs to be 256 , 384 or 512 * fs \ n ", <nl> + sgtl5000 -> sysclk / sys_fs ); <nl> return - EINVAL ; <nl> } <nl> }
ecryptfs_decode_from_filename ( unsigned char * dst , size_t * dst_size , <nl> break ; <nl> case 2 : <nl> dst [ dst_byte_offset ++] |= ( src_byte ); <nl> - dst [ dst_byte_offset ] = 0 ; <nl> current_bit_offset = 0 ; <nl> break ; <nl> }
static int ath9k_htc_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath9k_htc_sta * ista ; <nl> int ret = 0 ; <nl>  <nl> + mutex_lock (& priv -> mutex ); <nl> + <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> break ; <nl> static int ath9k_htc_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( priv -> ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> + mutex_unlock (& priv -> mutex ); <nl> + <nl> return ret ; <nl> } <nl> 
xfs_getbmap ( <nl> break ; <nl> } <nl>  <nl> + kmem_free ( out ); <nl> return error ; <nl> } <nl> 
static struct protection_domain * get_domain ( struct device * dev ) <nl> domain = to_pdomain ( io_domain ); <nl> attach_device ( dev , domain ); <nl> } <nl> + if ( domain == NULL ) <nl> + return ERR_PTR (- EBUSY ); <nl> + <nl> if (! dma_ops_domain ( domain )) <nl> return ERR_PTR (- EBUSY ); <nl> 
etrax_ethernet_init ( void ) <nl> * does not share cacheline with any other data ( to avoid cache bug ) <nl> */ <nl> RxDescList [ i ]. skb = dev_alloc_skb ( MAX_MEDIA_DATA_SIZE + 2 * L1_CACHE_BYTES ); <nl> + if (! RxDescList [ i ]. skb ) <nl> + return - ENOMEM ; <nl> RxDescList [ i ]. descr . ctrl = 0 ; <nl> RxDescList [ i ]. descr . sw_len = MAX_MEDIA_DATA_SIZE ; <nl> RxDescList [ i ]. descr . next = virt_to_phys (& RxDescList [ i + 1 ]);
void xhci_mem_cleanup ( struct xhci_hcd * xhci ) <nl>  <nl> if ( xhci -> lpm_command ) <nl> xhci_free_command ( xhci , xhci -> lpm_command ); <nl> + xhci -> lpm_command = NULL ; <nl> if ( xhci -> cmd_ring ) <nl> xhci_ring_free ( xhci , xhci -> cmd_ring ); <nl> xhci -> cmd_ring = NULL ;
int vcc_getsockopt ( struct socket * sock , int level , int optname , <nl>  <nl> if (! vcc -> dev || ! test_bit ( ATM_VF_ADDR , & vcc -> flags )) <nl> return - ENOTCONN ; <nl> + memset (& pvc , 0 , sizeof ( pvc )); <nl> pvc . sap_family = AF_ATMPVC ; <nl> pvc . sap_addr . itf = vcc -> dev -> number ; <nl> pvc . sap_addr . vpi = vcc -> vpi ;
static netdev_tx_t ipip6_tunnel_xmit ( struct sk_buff * skb , <nl> if ( tunnel -> parms . iph . daddr && skb_dst ( skb )) <nl> skb_dst ( skb )-> ops -> update_pmtu ( skb_dst ( skb ), NULL , skb , mtu ); <nl>  <nl> - if ( skb -> len > mtu ) { <nl> + if ( skb -> len > mtu && ! skb_is_gso ( skb )) { <nl> icmpv6_send ( skb , ICMPV6_PKT_TOOBIG , 0 , mtu ); <nl> ip_rt_put ( rt ); <nl> goto tx_error ;
static void pty_close ( struct tty_struct * tty , struct file * filp ) <nl> mutex_unlock (& devpts_mutex ); <nl> } <nl> # endif <nl> + tty_unlock ( tty ); <nl> tty_vhangup ( tty -> link ); <nl> + tty_lock ( tty ); <nl> } <nl> } <nl> 
static int eql_g_master_cfg ( struct net_device * dev , master_config_t __user * mcp ) <nl> equalizer_t * eql ; <nl> master_config_t mc ; <nl>  <nl> + memset (& mc , 0 , sizeof ( master_config_t )); <nl> + <nl> if ( eql_is_master ( dev )) { <nl> eql = netdev_priv ( dev ); <nl> mc . max_slaves = eql -> max_slaves ;
static int translate_desc ( struct vhost_dev * dev , u64 addr , u32 len , <nl> } <nl> _iov = iov + ret ; <nl> size = reg -> memory_size - addr + reg -> guest_phys_addr ; <nl> - _iov -> iov_len = min (( u64 ) len , size ); <nl> + _iov -> iov_len = min (( u64 ) len - s , size ); <nl> _iov -> iov_base = ( void __user *)( unsigned long ) <nl> ( reg -> userspace_addr + addr - reg -> guest_phys_addr ); <nl> s += size ;
i915_gem_create ( struct drm_file * file , <nl> u32 handle ; <nl>  <nl> size = roundup ( size , PAGE_SIZE ); <nl> + if ( size == 0 ) <nl> + return - EINVAL ; <nl>  <nl> /* Allocate the new object */ <nl> obj = i915_gem_alloc_object ( dev , size );
int x86_emulate_instruction ( struct kvm_vcpu * vcpu , <nl> if ( reexecute_instruction ( vcpu , cr2 , write_fault_to_spt , <nl> emulation_type )) <nl> return EMULATE_DONE ; <nl> + if ( ctxt -> have_exception && inject_emulated_exception ( vcpu )) <nl> + return EMULATE_DONE ; <nl> if ( emulation_type & EMULTYPE_SKIP ) <nl> return EMULATE_FAIL ; <nl> return handle_emulation_failure ( vcpu );
brcmf_cfg80211_start_ap ( struct wiphy * wiphy , struct net_device * ndev , <nl> ( u8 *)& settings -> beacon . head [ ie_offset ], <nl> settings -> beacon . head_len - ie_offset , <nl> WLAN_EID_SSID ); <nl> - if (! ssid_ie ) <nl> + if (! ssid_ie || ssid_ie -> len > IEEE80211_MAX_SSID_LEN ) <nl> return - EINVAL ; <nl>  <nl> memcpy ( ssid_le . SSID , ssid_ie -> data , ssid_ie -> len );
static int tipc_connect ( struct socket * sock , struct sockaddr * dest , <nl> if ( dst -> family == AF_UNSPEC ) { <nl> memset (& tsk -> remote , 0 , sizeof ( struct sockaddr_tipc )); <nl> tsk -> connected = 0 ; <nl> + } else if ( destlen != sizeof ( struct sockaddr_tipc )) { <nl> + res = - EINVAL ; <nl> } else { <nl> memcpy (& tsk -> remote , dest , destlen ); <nl> tsk -> connected = 1 ;
static int cpufreq_online ( unsigned int cpu ) <nl> if ( new_policy ) { <nl> /* related_cpus should at least include policy -> cpus . */ <nl> cpumask_copy ( policy -> related_cpus , policy -> cpus ); <nl> - /* Clear mask of registered CPUs */ <nl> - cpumask_clear ( policy -> real_cpus ); <nl> } <nl>  <nl> /*
static int exofs_read_lookup_dev_table ( struct exofs_sb_info ** psbi , <nl> } <nl>  <nl> od = osduld_info_lookup (& odi ); <nl> - if ( unlikely ( IS_ERR ( od ))) { <nl> + if ( IS_ERR ( od )) { <nl> ret = PTR_ERR ( od ); <nl> EXOFS_ERR (" ERROR : device requested is not found " <nl> " osd_name -% s =>% d \ n ", odi . osdname , ret );
static int fs_enet_rx_napi ( struct napi_struct * napi , int budget ) <nl> u16 pkt_len , sc ; <nl> int curidx ; <nl>  <nl> + if ( budget <= 0 ) <nl> + return received ; <nl> + <nl> /* <nl> * First , grab all of the stats for the incoming packet . <nl> * These get messed up if we get called due to a busy condition .
void msm_gem_free_object ( struct drm_gem_object * obj ) <nl> if ( msm_obj -> pages ) <nl> drm_free_large ( msm_obj -> pages ); <nl>  <nl> + drm_prime_gem_destroy ( obj , msm_obj -> sgt ); <nl> } else { <nl> vunmap ( msm_obj -> vaddr ); <nl> put_pages ( obj );
static int uas_slave_configure ( struct scsi_device * sdev ) <nl> if ( devinfo -> flags & US_FL_BROKEN_FUA ) <nl> sdev -> broken_fua = 1 ; <nl>  <nl> + scsi_change_queue_depth ( sdev , devinfo -> qdepth - 2 ); <nl> return 0 ; <nl> } <nl> 
static long kvm_dev_ioctl ( struct file * filp , <nl> num_msrs_to_save * sizeof ( u32 ))) <nl> goto out ; <nl> r = 0 ; <nl> + break ; <nl> } <nl> default : <nl> ;
static void __init tegra210_pll_init ( void __iomem * clk_base , <nl>  <nl> /* PLLU_VCO */ <nl> val = readl ( clk_base + pll_u_vco_params . base_reg ); <nl> - val &= ~ BIT ( 24 ); /* disable PLLU_OVERRIDE */ <nl> + val &= ~ PLLU_BASE_OVERRIDE ; /* disable PLLU_OVERRIDE */ <nl> writel ( val , clk_base + pll_u_vco_params . base_reg ); <nl>  <nl> clk = tegra_clk_register_pllre (" pll_u_vco ", " pll_ref ", clk_base , pmc ,
static void apc_agent_timeout ( unsigned long data ) <nl> configure_phy_mask = ~ port_agent -> phy_configured_mask & port_agent -> phy_ready_mask ; <nl>  <nl> if (! configure_phy_mask ) <nl> - return ; <nl> + goto done ; <nl>  <nl> for ( index = 0 ; index < SCI_MAX_PHYS ; index ++) { <nl> if (( configure_phy_mask & ( 1 << index )) == 0 )
static int smsdvb_hotplug ( struct smscore_device_t * coredev , <nl> switch ( smscore_get_device_mode ( coredev )) { <nl> case DEVICE_MODE_DVBT : <nl> case DEVICE_MODE_DVBT_BDA : <nl> - smsdvb_fe_ops . delsys [ 0 ] = SYS_DVBT ; <nl> + client -> frontend . ops . delsys [ 0 ] = SYS_DVBT ; <nl> break ; <nl> case DEVICE_MODE_ISDBT : <nl> case DEVICE_MODE_ISDBT_BDA : <nl> - smsdvb_fe_ops . delsys [ 0 ] = SYS_ISDBT ; <nl> + client -> frontend . ops . delsys [ 0 ] = SYS_ISDBT ; <nl> break ; <nl> } <nl> 
static ssize_t field ## _show ( struct device * dev , \ <nl> char * buf ) \ <nl> { \ <nl> struct gb_interface * intf = to_gb_interface ( dev ); \ <nl> - return sprintf ( buf , "%"# type "\ n ", intf -> field ); \ <nl> + return scnprintf ( buf , PAGE_SIZE , "%"# type "\ n ", intf -> field ); \ <nl> } \ <nl> static DEVICE_ATTR_RO ( field ) <nl> 
xfs_iext_remove_node ( <nl> node -> ptrs [ nr_entries ] = NULL ; <nl>  <nl> if ( pos == 0 && nr_entries > 0 ) { <nl> - xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , <nl> - node ); <nl> + xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , node ); <nl> offset = node -> keys [ 0 ]; <nl> } <nl> 
static void tg3_skb_error_unmap ( struct tg3_napi * tnapi , <nl> dma_unmap_addr ( txb , mapping ), <nl> skb_headlen ( skb ), <nl> PCI_DMA_TODEVICE ); <nl> - for ( i = 0 ; i <= last ; i ++) { <nl> + for ( i = 0 ; i < last ; i ++) { <nl> skb_frag_t * frag = & skb_shinfo ( skb )-> frags [ i ]; <nl>  <nl> entry = NEXT_TX ( entry );
static int safexcel_probe ( struct platform_device * pdev ) <nl> snprintf ( irq_name , 6 , " ring % d ", i ); <nl> irq = safexcel_request_ring_irq ( pdev , irq_name , safexcel_irq_ring , <nl> ring_irq ); <nl> - <nl> - if ( irq < 0 ) <nl> + if ( irq < 0 ) { <nl> + ret = irq ; <nl> goto err_clk ; <nl> + } <nl>  <nl> priv -> ring [ i ]. work_data . priv = priv ; <nl> priv -> ring [ i ]. work_data . ring = i ;
static u32 apic_get_tmcct ( struct kvm_lapic * apic ) <nl> ASSERT ( apic != NULL ); <nl>  <nl> /* if initial count is 0 , current count should also be 0 */ <nl> - if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 ) <nl> + if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 || <nl> + apic -> lapic_timer . period == 0 ) <nl> return 0 ; <nl>  <nl> remaining = hrtimer_get_remaining (& apic -> lapic_timer . timer );
static void walk_linearmapping ( struct pg_state * st ) <nl> unsigned long psize = 1 << mmu_psize_defs [ mmu_linear_psize ]. shift ; <nl>  <nl> for ( addr = PAGE_OFFSET ; addr < PAGE_OFFSET + <nl> - memblock_phys_mem_size (); addr += psize ) <nl> + memblock_end_of_DRAM (); addr += psize ) <nl> hpte_find ( st , addr , mmu_linear_psize ); <nl> } <nl> 
bool wil_fw_verify_file_exists ( struct wil6210_priv * wil , const char * name ) <nl> rc = request_firmware (& fw , name , wil_to_dev ( wil )); <nl> if (! rc ) <nl> release_firmware ( fw ); <nl> - return rc != - ENOENT ; <nl> + else <nl> + wil_dbg_fw ( wil , "<% s > not available : % d \ n ", name , rc ); <nl> + return ! rc ; <nl> }
static int sun8i_vi_layer_atomic_check ( struct drm_plane * plane , <nl> clip . x2 = crtc_state -> adjusted_mode . hdisplay ; <nl> clip . y2 = crtc_state -> adjusted_mode . vdisplay ; <nl>  <nl> + min_scale = DRM_PLANE_HELPER_NO_SCALING ; <nl> + max_scale = DRM_PLANE_HELPER_NO_SCALING ; <nl> + <nl> if ( layer -> mixer -> cfg -> scaler_mask & BIT ( layer -> channel )) { <nl> min_scale = SUN8I_VI_SCALER_SCALE_MIN ; <nl> max_scale = SUN8I_VI_SCALER_SCALE_MAX ;
static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
int blk_mq_alloc_tag_set ( struct blk_mq_tag_set * set ) <nl> return - EINVAL ; <nl>  <nl>  <nl> - set -> tags = kmalloc_node ( set -> nr_hw_queues * sizeof ( struct blk_mq_tags ), <nl> + set -> tags = kmalloc_node ( set -> nr_hw_queues * <nl> + sizeof ( struct blk_mq_tags *), <nl> GFP_KERNEL , set -> numa_node ); <nl> if (! set -> tags ) <nl> goto out ;
static int coda_try_fmt ( struct coda_ctx * ctx , struct coda_codec * codec , <nl> BUG (); <nl> } <nl>  <nl> + f -> fmt . pix . priv = 0 ; <nl> + <nl> return 0 ; <nl> } <nl> 
void perf_callchain_user ( struct perf_callchain_entry * entry , <nl> return ; <nl> } <nl>  <nl> + perf_callchain_store ( entry , regs -> pc ); <nl> tail = ( struct frame_tail __user *) regs -> regs [ 29 ]; <nl>  <nl> while ( entry -> nr < PERF_MAX_STACK_DEPTH &&
static void etb_update_buffer ( struct coresight_device * csdev , <nl>  <nl> capacity = drvdata -> buffer_depth * ETB_FRAME_SIZE_WORDS ; <nl>  <nl> - CS_UNLOCK ( drvdata -> base ); <nl> etb_disable_hw ( drvdata ); <nl> + CS_UNLOCK ( drvdata -> base ); <nl>  <nl> /* unit is in words , not bytes */ <nl> read_ptr = readl_relaxed ( drvdata -> base + ETB_RAM_READ_POINTER );
static int check_hw_params_convention ( struct snd_usb_substream * subs ) <nl>  <nl> channels = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> rates = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> + if (! channels || ! rates ) <nl> + goto __out ; <nl>  <nl> list_for_each ( p , & subs -> fmt_list ) { <nl> struct audioformat * f ;
static int gbcodec_trigger ( struct snd_pcm_substream * substream , int cmd , <nl> dev_err ( dai -> dev , "% d : Error during % s stream \ n ", ret , <nl> start ? " Start " : " Stop "); <nl>  <nl> + /* in case device removed , return 0 for stop trigger */ <nl> + if ( stop && ( ret == - ENODEV )) <nl> + ret = 0 ; <nl> + <nl> func_exit : <nl> mutex_unlock (& gb -> lock ); <nl> return ret ;
static u32 __init allocate_aperture ( void ) <nl> printk (" Cannot allocate aperture memory hole (% p ,% uK )\ n ", <nl> p , aper_size >> 10 ); <nl> if ( p ) <nl> - free_bootmem_node ( nd0 , ( unsigned long ) p , aper_size ); <nl> + free_bootmem_node ( nd0 , __pa ( p ), aper_size ); <nl> return 0 ; <nl> } <nl> printk (" Mapping aperture over % d KB of RAM @ % lx \ n ",
static int picolcd_raw_event ( struct hid_device * hdev , <nl> if (! data ) <nl> return 1 ; <nl>  <nl> + if ( size > 64 ) { <nl> + hid_warn ( hdev , " invalid size value (% d ) for picolcd raw event \ n ", <nl> + size ); <nl> + return 0 ; <nl> + } <nl> + <nl> if ( report -> id == REPORT_KEY_STATE ) { <nl> if ( data -> input_keys ) <nl> ret = picolcd_raw_keypad ( data , report , raw_data + 1 , size - 1 );
static int omap_pcm_open ( struct snd_pcm_substream * substream ) <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> - prtd = kzalloc ( sizeof ( prtd ), GFP_KERNEL ); <nl> + prtd = kzalloc ( sizeof (* prtd ), GFP_KERNEL ); <nl> if ( prtd == NULL ) { <nl> ret = - ENOMEM ; <nl> goto out ;
+// SPDX - License - Identifier : GPL - 2 . 0 <nl> /* <nl> * Copyright ( C ) Maxime Coquelin 2015 <nl> + * Copyright ( C ) STMicroelectronics 2017 <nl> * Author : Maxime Coquelin < mcoquelin . stm32 @ gmail . com > <nl> - * License terms : GNU General Public License ( GPL ), version 2 <nl> */ <nl>  <nl> # include < linux / kernel . h >
int batadv_recv_unicast_packet ( struct sk_buff * skb , <nl> batadv_dbg ( BATADV_DBG_BLA , bat_priv , <nl> " recv_unicast_packet (): Dropped unicast pkt received from another backbone gw % pM .\ n ", <nl> orig_addr_gw ); <nl> - return NET_RX_DROP ; <nl> + goto free_skb ; <nl> } <nl> } <nl> 
EXPORT_SYMBOL_GPL ( crypto_givcipher_type ); <nl>  <nl> const char * crypto_default_geniv ( const struct crypto_alg * alg ) <nl> { <nl> + if ((( alg -> cra_flags & CRYPTO_ALG_TYPE_MASK ) == <nl> + CRYPTO_ALG_TYPE_BLKCIPHER ? alg -> cra_blkcipher . ivsize : <nl> + alg -> cra_ablkcipher . ivsize ) != <nl> + alg -> cra_blocksize ) <nl> + return " chainiv "; <nl> + <nl> return alg -> cra_flags & CRYPTO_ALG_ASYNC ? <nl> " eseqiv " : skcipher_default_geniv ; <nl> }
int ___ieee80211_stop_tx_ba_session ( struct sta_info * sta , u16 tid , <nl>  <nl> spin_lock_bh (& sta -> lock ); <nl>  <nl> + /* free struct pending for start , if present */ <nl> + tid_tx = sta -> ampdu_mlme . tid_start_tx [ tid ]; <nl> + kfree ( tid_tx ); <nl> + sta -> ampdu_mlme . tid_start_tx [ tid ] = NULL ; <nl> + <nl> tid_tx = rcu_dereference_protected_tid_tx ( sta , tid ); <nl> if (! tid_tx ) { <nl> spin_unlock_bh (& sta -> lock );
static int bnxt_get_nvram_item ( struct net_device * dev , u32 index , u32 offset , <nl> dma_addr_t dma_handle ; <nl> struct hwrm_nvm_read_input req = { 0 }; <nl>  <nl> + if (! length ) <nl> + return - EINVAL ; <nl> + <nl> buf = dma_alloc_coherent (& bp -> pdev -> dev , length , & dma_handle , <nl> GFP_KERNEL ); <nl> if (! buf ) {
int dmar_set_interrupt ( struct intel_iommu * iommu ) <nl> return 0 ; <nl>  <nl> irq = create_irq (); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> pr_err (" IOMMU : no free vectors \ n "); <nl> return - EINVAL ; <nl> }
int virtio_gpu_object_create ( struct virtio_gpu_device * vgdev , <nl> return - ENOMEM ; <nl> size = roundup ( size , PAGE_SIZE ); <nl> ret = drm_gem_object_init ( vgdev -> ddev , & bo -> gem_base , size ); <nl> - if ( ret != 0 ) <nl> + if ( ret != 0 ) { <nl> + kfree ( bo ); <nl> return ret ; <nl> + } <nl> bo -> dumb = false ; <nl> virtio_gpu_init_ttm_placement ( bo , pinned ); <nl> 
brcmf_cfg80211_add_key ( struct wiphy * wiphy , struct net_device * ndev , <nl> if (! check_vif_up ( ifp -> vif )) <nl> return - EIO ; <nl>  <nl> - if ( mac_addr ) { <nl> + if ( mac_addr && <nl> + ( params -> cipher != WLAN_CIPHER_SUITE_WEP40 ) && <nl> + ( params -> cipher != WLAN_CIPHER_SUITE_WEP104 )) { <nl> brcmf_dbg ( TRACE , " Exit "); <nl> return brcmf_add_keyext ( wiphy , ndev , key_idx , mac_addr , params ); <nl> }
int i915_gem_freeze_late ( struct drm_i915_private * dev_priv ) <nl> */ <nl>  <nl> i915_gem_shrink ( dev_priv , - 1UL , I915_SHRINK_UNBOUND ); <nl> + i915_gem_drain_freed_objects ( dev_priv ); <nl>  <nl> mutex_lock (& dev_priv -> drm . struct_mutex ); <nl> for ( p = phases ; * p ; p ++) {
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> struct oz_app_hdr * app_hdr ; <nl> struct oz_serial_ctx * ctx ; <nl>  <nl> + if ( count > sizeof ( ei -> data ) - sizeof (* elt ) - sizeof (* app_hdr )) <nl> + return - EINVAL ; <nl> + <nl> spin_lock_bh (& g_cdev . lock ); <nl> pd = g_cdev . active_pd ; <nl> if ( pd )
int drm_atomic_helper_setup_commit ( struct drm_atomic_state * state , <nl> ! try_wait_for_completion (& old_plane_state -> commit -> flip_done )) <nl> return - EBUSY ; <nl>  <nl> - commit = crtc_or_fake_commit ( state , old_plane_state -> crtc ); <nl> + commit = crtc_or_fake_commit ( state , new_plane_state -> crtc ?: old_plane_state -> crtc ); <nl> if (! commit ) <nl> return - ENOMEM ; <nl> 
struct sched_entity { <nl>  <nl> struct sched_rt_entity { <nl> struct list_head run_list ; <nl> - unsigned int time_slice ; <nl> unsigned long timeout ; <nl> + unsigned int time_slice ; <nl> int nr_cpus_allowed ; <nl>  <nl> struct sched_rt_entity * back ;
static int max30102_probe ( struct i2c_client * client , <nl> dev_err (& client -> dev , " regmap initialization failed \ n "); <nl> return PTR_ERR ( data -> regmap ); <nl> } <nl> - max30102_set_powermode ( data , false ); <nl> + <nl> + ret = max30102_set_powermode ( data , false ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = max30102_chip_init ( data ); <nl> if ( ret )
cifs_put_tcp_session ( struct TCP_Server_Info * server , int from_reconnect ) <nl> server -> session_key . response = NULL ; <nl> server -> session_key . len = 0 ; <nl> kfree ( server -> hostname ); <nl> + server -> hostname = NULL ; <nl>  <nl> task = xchg (& server -> tsk , NULL ); <nl> if ( task )
static int parse_features ( struct dm_arg_set * as , struct flakey_c * fc , <nl> arg_name = dm_shift_arg ( as ); <nl> argc --; <nl>  <nl> + if (! arg_name ) { <nl> + ti -> error = " Insufficient feature arguments "; <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* <nl> * drop_writes <nl> */
int blk_init_allocated_queue ( struct request_queue * q ) <nl> q -> exit_rq_fn ( q , q -> fq -> flush_rq ); <nl> out_free_flush_queue : <nl> blk_free_flush_queue ( q -> fq ); <nl> + q -> fq = NULL ; <nl> return - ENOMEM ; <nl> } <nl> EXPORT_SYMBOL ( blk_init_allocated_queue );
void sctp_association_free ( struct sctp_association * asoc ) <nl> /* Only real associations count against the endpoint , so <nl> * don ' t bother for if this is a temporary association . <nl> */ <nl> - if (! asoc -> temp ) { <nl> + if (! list_empty (& asoc -> asocs )) { <nl> list_del (& asoc -> asocs ); <nl>  <nl> /* Decrement the backlog value for a TCP - style listening
static int visor_thread_start ( struct visor_thread_info * thrinfo , <nl> void * thrcontext , char * name ) <nl> { <nl> /* used to stop the thread */ <nl> - thrinfo -> task = kthread_run ( threadfn , thrcontext , name ); <nl> + thrinfo -> task = kthread_run ( threadfn , thrcontext , "% s ", name ); <nl> if ( IS_ERR ( thrinfo -> task )) { <nl> pr_debug ("% s failed (% ld )\ n ", <nl> __func__ , PTR_ERR ( thrinfo -> task ));
s32 fm10k_disable_queues_generic ( struct fm10k_hw * hw , u16 q_cnt ) <nl> /* clear tx_ready to prevent any false hits for reset */ <nl> hw -> mac . tx_ready = false ; <nl>  <nl> + if ( FM10K_REMOVED ( hw -> hw_addr )) <nl> + return 0 ; <nl> + <nl> /* clear the enable bit for all rings */ <nl> for ( i = 0 ; i < q_cnt ; i ++) { <nl> reg = fm10k_read_reg ( hw , FM10K_TXDCTL ( i ));
xfs_error_get_cfg ( <nl> { <nl> struct xfs_error_cfg * cfg ; <nl>  <nl> + if ( error < 0 ) <nl> + error = - error ; <nl> + <nl> switch ( error ) { <nl> case EIO : <nl> cfg = & mp -> m_error_cfg [ error_class ][ XFS_ERR_EIO ];
struct oz_pd * oz_pd_alloc ( const u8 * mac_addr ) <nl> /* <nl> * Context : softirq or process <nl> */ <nl> - void oz_pd_free ( struct work_struct * work ) <nl> + static void oz_pd_free ( struct work_struct * work ) <nl> { <nl> struct list_head * e ; <nl> struct oz_tx_frame * f ;
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> return - EBUSY ; <nl> } <nl>  <nl> + if ( bond_dev == slave_dev ) { <nl> + pr_err ("% s : cannot enslave bond to itself .\ n ", bond_dev -> name ); <nl> + return - EPERM ; <nl> + } <nl> + <nl> /* vlan challenged mutual exclusion */ <nl> /* no need to lock since we ' re protected by rtnl_lock */ <nl> if ( slave_dev -> features & NETIF_F_VLAN_CHALLENGED ) {
static struct xhci_container_ctx * xhci_alloc_container_ctx ( struct xhci_hcd * xhci <nl> ctx -> size += CTX_SIZE ( xhci -> hcc_params ); <nl>  <nl> ctx -> bytes = dma_pool_alloc ( xhci -> device_pool , flags , & ctx -> dma ); <nl> + if (! ctx -> bytes ) { <nl> + kfree ( ctx ); <nl> + return NULL ; <nl> + } <nl> memset ( ctx -> bytes , 0 , ctx -> size ); <nl> return ctx ; <nl> }
int cpuidle_enter_state ( struct cpuidle_device * dev , struct cpuidle_driver * drv , <nl>  <nl> time_end = ktime_get (); <nl>  <nl> - local_irq_enable (); <nl> + if (! cpuidle_state_is_coupled ( dev , drv , entered_state )) <nl> + local_irq_enable (); <nl>  <nl> diff = ktime_to_us ( ktime_sub ( time_end , time_start )); <nl> if ( diff > INT_MAX )
static int rtc_dev_ioctl ( struct inode * inode , struct file * file , <nl> break ; <nl>  <nl> case RTC_PIE_ON : <nl> - if (! capable ( CAP_SYS_RESOURCE )) <nl> + if ( rtc -> irq_freq > rtc -> max_user_freq && <nl> + ! capable ( CAP_SYS_RESOURCE )) <nl> return - EACCES ; <nl> break ; <nl> }
static int spi_ppc4xx_of_probe ( struct platform_device * op ) <nl> if ( num_gpios > 0 ) { <nl> int i ; <nl>  <nl> - hw -> gpios = kzalloc ( sizeof ( int ) * num_gpios , GFP_KERNEL ); <nl> + hw -> gpios = kcalloc ( num_gpios , sizeof (* hw -> gpios ), GFP_KERNEL ); <nl> if (! hw -> gpios ) { <nl> ret = - ENOMEM ; <nl> goto free_master ;
static void test_acipher_speed ( const char * algo , int enc , unsigned int secs , <nl> goto out_free_req ; <nl> } <nl>  <nl> - sg_init_table ( sg , TVMEMSIZE ); <nl> - <nl> k = * keysize + * b_size ; <nl> + sg_init_table ( sg , DIV_ROUND_UP ( k , PAGE_SIZE )); <nl> + <nl> if ( k > PAGE_SIZE ) { <nl> sg_set_buf ( sg , tvmem [ 0 ] + * keysize , <nl> PAGE_SIZE - * keysize );
static bool fib6_rule_suppress ( struct fib_rule * rule , struct fib_lookup_arg * arg <nl> return false ; <nl>  <nl> suppress_route : <nl> - ip6_rt_put ( rt ); <nl> + if (!( arg -> flags & FIB_LOOKUP_NOREF )) <nl> + ip6_rt_put ( rt ); <nl> return true ; <nl> } <nl> 
static int deprecated_sysctl_warning ( struct __sysctl_args * args ) <nl> int name [ CTL_MAXNAME ]; <nl> int i ; <nl>  <nl> + /* Check args -> nlen . */ <nl> + if ( args -> nlen < 0 || args -> nlen > CTL_MAXNAME ) <nl> + return - ENOTDIR ; <nl> + <nl> /* Read in the sysctl name for better debug message logging */ <nl> for ( i = 0 ; i < args -> nlen ; i ++) <nl> if ( get_user ( name [ i ], args -> name + i ))
static struct domain_device * sas_ex_discover_expander ( <nl>  <nl> res = sas_discover_expander ( child ); <nl> if ( res ) { <nl> + spin_lock_irq (& parent -> port -> dev_list_lock ); <nl> + list_del (& child -> dev_list_node ); <nl> + spin_unlock_irq (& parent -> port -> dev_list_lock ); <nl> kfree ( child ); <nl> return NULL ; <nl> }
static const struct ieee80211_channel ath10k_5ghz_channels [] = { <nl> CHAN5G ( 132 , 5660 , 0 ), <nl> CHAN5G ( 136 , 5680 , 0 ), <nl> CHAN5G ( 140 , 5700 , 0 ), <nl> + CHAN5G ( 144 , 5720 , 0 ), <nl> CHAN5G ( 149 , 5745 , 0 ), <nl> CHAN5G ( 153 , 5765 , 0 ), <nl> CHAN5G ( 157 , 5785 , 0 ),
extern long __put_user_asm_l ( void *, long ); <nl> extern long __put_user_asm_q ( void *, long ); <nl> extern void __put_user_unknown ( void ); <nl>  <nl> + extern long __strnlen_user ( const char * __s , long __n ); <nl> + <nl> # endif /* __ASM_SH_UACCESS_64_H */
static void __init sun6i_rtc_clk_init ( struct device_node * node ) <nl>  <nl> clk_data = kzalloc ( sizeof (* clk_data ) + ( sizeof (* clk_data -> hws ) * 2 ), <nl> GFP_KERNEL ); <nl> - if (! clk_data ) <nl> + if (! clk_data ) { <nl> + kfree ( rtc ); <nl> return ; <nl> + } <nl>  <nl> spin_lock_init (& rtc -> lock ); <nl> 
static void zfcp_fc_adisc_handler ( unsigned long data ) <nl> struct zfcp_port * port = adisc -> els . port ; <nl> struct zfcp_ls_adisc_acc * ls_adisc = & adisc -> ls_adisc_acc ; <nl>  <nl> - if (! adisc -> els . status ) { <nl> + if ( adisc -> els . status ) { <nl> /* request rejected or timed out */ <nl> zfcp_erp_port_forced_reopen ( port , 0 , 63 , NULL ); <nl> goto out ;
int journal_create ( journal_t * journal ) <nl> if ( err ) <nl> return err ; <nl> bh = __getblk ( journal -> j_dev , blocknr , journal -> j_blocksize ); <nl> + if ( unlikely (! bh )) <nl> + return - ENOMEM ; <nl> lock_buffer ( bh ); <nl> memset ( bh -> b_data , 0 , journal -> j_blocksize ); <nl> BUFFER_TRACE ( bh , " marking dirty ");
pxa3xx_gcu_write ( struct file * file , const char * buff , <nl> struct pxa3xx_gcu_batch * buffer ; <nl> struct pxa3xx_gcu_priv * priv = to_pxa3xx_gcu_priv ( file ); <nl>  <nl> - int words = count / 4 ; <nl> + size_t words = count / 4 ; <nl>  <nl> /* Does not need to be atomic . There ' s a lock in user space , <nl> * but anyhow , this is just for statistics . */
static int sti_cpufreq_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if ((! of_machine_is_compatible (" st , stih407 ")) && <nl> + (! of_machine_is_compatible (" st , stih410 "))) <nl> + return - ENODEV ; <nl> + <nl> ddata . cpu = get_cpu_device ( 0 ); <nl> if (! ddata . cpu ) { <nl> dev_err ( ddata . cpu , " Failed to get device for CPU0 \ n ");
static struct glink_channel * qcom_glink_alloc_channel ( struct qcom_glink * glink , <nl>  <nl> init_completion (& channel -> open_req ); <nl> init_completion (& channel -> open_ack ); <nl> + init_completion (& channel -> intent_req_comp ); <nl>  <nl> INIT_LIST_HEAD (& channel -> done_intents ); <nl> INIT_WORK (& channel -> intent_work , qcom_glink_rx_done_work );
static int intel_pt_walk_to_ip ( struct intel_pt_decoder * decoder ) <nl> break ; <nl>  <nl> case INTEL_PT_PSB : <nl> + intel_pt_clear_stack (& decoder -> stack ); <nl> err = intel_pt_walk_psb ( decoder ); <nl> if ( err ) <nl> return err ;
static void mwifiex_recreate_adapter ( struct sdio_mmc_card * card ) <nl> mmc_hw_reset ( func -> card -> host ); <nl> sdio_release_host ( func ); <nl>  <nl> + /* Previous save_adapter won ' t be valid after this . We will cancel <nl> + * pending work requests . <nl> + */ <nl> + clear_bit ( MWIFIEX_IFACE_WORK_DEVICE_DUMP , & iface_work_flags ); <nl> + clear_bit ( MWIFIEX_IFACE_WORK_CARD_RESET , & iface_work_flags ); <nl> + <nl> mwifiex_sdio_probe ( func , device_id ); <nl> } <nl> 
static inline int __is_running ( const struct exynos_mipi_phy_desc * data , <nl> struct exynos_mipi_video_phy * state ) <nl> { <nl> u32 val ; <nl> + int ret ; <nl> + <nl> + ret = regmap_read ( state -> regmaps [ data -> resetn_map ], data -> resetn_reg , & val ); <nl> + if ( ret ) <nl> + return 0 ; <nl>  <nl> - regmap_read ( state -> regmaps [ data -> resetn_map ], data -> resetn_reg , & val ); <nl> return val & data -> resetn_val ; <nl> } <nl> 
static int sharpsl_nand_probe ( struct platform_device * pdev ) <nl> /* Register the partitions */ <nl> mtd -> name = " sharpsl - nand "; <nl>  <nl> - err = mtd_device_parse_register ( mtd , NULL , NULL , <nl> + err = mtd_device_parse_register ( mtd , data -> part_parsers , NULL , <nl> data -> partitions , data -> nr_partitions ); <nl> if ( err ) <nl> goto err_add ;
int i915_gem_init_stolen ( struct drm_i915_private * dev_priv ) <nl>  <nl> mutex_init (& dev_priv -> mm . stolen_lock ); <nl>  <nl> + if ( intel_vgpu_active ( dev_priv )) { <nl> + DRM_INFO (" iGVT - g active , disabling use of stolen memory \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> # ifdef CONFIG_INTEL_IOMMU <nl> if ( intel_iommu_gfx_mapped && INTEL_GEN ( dev_priv ) < 8 ) { <nl> DRM_INFO (" DMAR active , disabling use of stolen memory \ n ");
void handle_ra_miss ( struct address_space * mapping , <nl> { <nl> ra -> flags |= RA_FLAG_MISS ; <nl> ra -> flags &= ~ RA_FLAG_INCACHE ; <nl> + ra -> cache_hit = 0 ; <nl> } <nl>  <nl> /*
static int __devinit snd_miro_probe ( struct snd_card * card ) <nl>  <nl> error = snd_card_miro_aci_detect ( card , miro ); <nl> if ( error < 0 ) { <nl> - snd_card_free ( card ); <nl> snd_printk ( KERN_ERR " unable to detect aci chip \ n "); <nl> return - ENODEV ; <nl> }
# define DRIVER_AUTHOR " Maksim Salau < maksim . salau @ gmail . com >" <nl>  <nl> static const struct usb_device_id id_table [] = { <nl> - { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x0409 , 0x0063 ) }, /* V850ESJX3 - STICK */ <nl> + { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x064B , 0x7825 ) }, /* Analog Devices EVAL - ADXL362Z - DB */ <nl> {} <nl> };
static int mfd_add_device ( struct platform_device * parent , <nl> if ( ret ) <nl> goto fail_device ; <nl>  <nl> - memzero ( res , sizeof ( res )); <nl> + memset ( res , 0 , sizeof ( res )); <nl> for ( r = 0 ; r < cell -> num_resources ; r ++) { <nl> res [ r ]. name = cell -> resources [ r ]. name ; <nl> res [ r ]. flags = cell -> resources [ r ]. flags ;
static int cuse_channel_release ( struct inode * inode , struct file * file ) <nl> unregister_chrdev_region ( cc -> cdev -> dev , 1 ); <nl> cdev_del ( cc -> cdev ); <nl> } <nl> + /* Base reference is now owned by " fud " */ <nl> + fuse_conn_put (& cc -> fc ); <nl>  <nl> rc = fuse_dev_release ( inode , file ); /* puts the base reference */ <nl> 
struct sta_info * ieee80211_ibss_add_sta ( struct ieee80211_sub_if_data * sdata , <nl> if (! sta ) <nl> return NULL ; <nl>  <nl> + sta -> last_rx = jiffies ; <nl> set_sta_flags ( sta , WLAN_STA_AUTHORIZED ); <nl>  <nl> /* make sure mandatory rates are always added */
static int fsi_resume ( struct device * dev ) <nl> return 0 ; <nl> } <nl>  <nl> - static struct dev_pm_ops fsi_pm_ops = { <nl> + static const struct dev_pm_ops fsi_pm_ops = { <nl> . suspend = fsi_suspend , <nl> . resume = fsi_resume , <nl> };
static int spi_map_buf ( struct spi_master * master , struct device * dev , <nl> } <nl>  <nl> ret = dma_map_sg ( dev , sgt -> sgl , sgt -> nents , dir ); <nl> + if (! ret ) <nl> + ret = - ENOMEM ; <nl> if ( ret < 0 ) { <nl> sg_free_table ( sgt ); <nl> return ret ;
static int ext4_ext_journal_restart ( handle_t * handle , int needed ) <nl> if ( handle -> h_buffer_credits > needed ) <nl> return 0 ; <nl> err = ext4_journal_extend ( handle , needed ); <nl> - if ( err ) <nl> + if ( err <= 0 ) <nl> return err ; <nl> return ext4_journal_restart ( handle , needed ); <nl> }
static int btrfs_add_system_chunk ( struct btrfs_root * root , <nl> u8 * ptr ; <nl>  <nl> array_size = btrfs_super_sys_array_size ( super_copy ); <nl> - if ( array_size + item_size > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> + if ( array_size + item_size + sizeof ( disk_key ) <nl> + > BTRFS_SYSTEM_CHUNK_ARRAY_SIZE ) <nl> return - EFBIG ; <nl>  <nl> ptr = super_copy -> sys_chunk_array + array_size ;
static const char * alc_get_line_out_pfx ( const struct auto_pin_cfg * cfg , <nl>  <nl> switch ( cfg -> line_out_type ) { <nl> case AUTO_PIN_SPEAKER_OUT : <nl> - return " Speaker "; <nl> + if ( cfg -> line_outs == 1 ) <nl> + return " Speaker "; <nl> + break ; <nl> case AUTO_PIN_HP_OUT : <nl> return " Headphone "; <nl> default :
struct extent_map * btrfs_get_extent ( struct inode * inode , struct page * page , <nl> goto not_found ; <nl> if ( start + len <= found_key . offset ) <nl> goto not_found ; <nl> + if ( start > found_key . offset ) <nl> + goto next ; <nl> em -> start = start ; <nl> em -> orig_start = start ; <nl> em -> len = found_key . offset - start ;
ssize_t btrfs_listxattr ( struct dentry * dentry , char * buffer , size_t size ) <nl>  <nl> if (! buffer || ( name_len + 1 ) > size_left ) { <nl> ret = - ERANGE ; <nl> - break ; <nl> + goto err ; <nl> } <nl>  <nl> name_ptr = ( unsigned long )( di + 1 );
static struct drm_driver tegra_drm_driver = { <nl> . debugfs_cleanup = tegra_debugfs_cleanup , <nl> # endif <nl>  <nl> - . gem_free_object = tegra_bo_free_object , <nl> + . gem_free_object_unlocked = tegra_bo_free_object , <nl> . gem_vm_ops = & tegra_bo_vm_ops , <nl>  <nl> . prime_handle_to_fd = drm_gem_prime_handle_to_fd ,
enum { none , prepare , done , } __init_state ; <nl> static void init_preload ( void ); <nl> static void try_init_preload ( void ) <nl> { <nl> - if (! __init_state != done ) <nl> + if ( __init_state != done ) <nl> init_preload (); <nl> } <nl> 
static int xfrm6_tunnel_rcv ( struct sk_buff * skb ) <nl> __be32 spi ; <nl>  <nl> spi = xfrm6_tunnel_spi_lookup (( xfrm_address_t *)& iph -> saddr ); <nl> - return xfrm6_rcv_spi ( skb , spi ); <nl> + return xfrm6_rcv_spi ( skb , spi ) > 0 ? : 0 ; <nl> } <nl>  <nl> static int xfrm6_tunnel_err ( struct sk_buff * skb , struct inet6_skb_parm * opt ,
int jbd2__journal_restart ( handle_t * handle , int nblocks , gfp_t gfp_mask ) <nl>  <nl> rwsem_release (& journal -> j_trans_commit_map , 1 , _THIS_IP_ ); <nl> handle -> h_buffer_credits = nblocks ; <nl> + /* <nl> + * Restore the original nofs context because the journal restart <nl> + * is basically the same thing as journal stop and start . <nl> + * start_this_handle will start a new nofs context . <nl> + */ <nl> + memalloc_nofs_restore ( handle -> saved_alloc_context ); <nl> ret = start_this_handle ( journal , handle , gfp_mask ); <nl> return ret ; <nl> }
static int futex_requeue ( u32 __user * uaddr1 , unsigned int flags , <nl> struct futex_q * this , * next ; <nl> DEFINE_WAKE_Q ( wake_q ); <nl>  <nl> + if ( nr_wake < 0 || nr_requeue < 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * When PI not supported : return - ENOSYS if requeue_pi is true , <nl> * consequently the compiler knows requeue_pi is always false past
static int snd_soc_dapm_set_bias_level ( struct snd_soc_dapm_context * dapm , <nl> if ( dapm -> codec -> driver -> set_bias_level ) <nl> ret = dapm -> codec -> driver -> set_bias_level ( dapm -> codec , <nl> level ); <nl> - else <nl> - dapm -> bias_level = level ; <nl> - } <nl> + } else <nl> + dapm -> bias_level = level ; <nl> + <nl> if ( ret != 0 ) <nl> goto out ; <nl> 
static int ipw_load ( struct ipw_priv * priv ) <nl> ipw_rx_queue_reset ( priv , priv -> rxq ); <nl> if (! priv -> rxq ) { <nl> IPW_ERROR (" Unable to initialize Rx queue \ n "); <nl> + rc = - ENOMEM ; <nl> goto error ; <nl> } <nl> 
static void __init kvm_guest_init ( void ) <nl> } <nl>  <nl> if ( kvm_para_has_feature ( KVM_FEATURE_PV_TLB_FLUSH ) && <nl> + ! kvm_para_has_hint ( KVM_HINTS_DEDICATED ) && <nl> ! kvm_para_has_feature ( KVM_FEATURE_STEAL_TIME )) <nl> pv_mmu_ops . flush_tlb_others = kvm_flush_tlb_others ; <nl>  <nl> static __init int kvm_setup_pv_tlb_flush ( void ) <nl> int cpu ; <nl>  <nl> if ( kvm_para_has_feature ( KVM_FEATURE_PV_TLB_FLUSH ) && <nl> + ! kvm_para_has_hint ( KVM_HINTS_DEDICATED ) && <nl> ! kvm_para_has_feature ( KVM_FEATURE_STEAL_TIME )) { <nl> for_each_possible_cpu ( cpu ) { <nl> zalloc_cpumask_var_node ( per_cpu_ptr (& __pv_tlb_mask , cpu ),
static int fuse_fill_super ( struct super_block * sb , void * data , int silent ) <nl> err_put_root : <nl> dput ( root_dentry ); <nl> err_put_conn : <nl> + bdi_destroy (& fc -> bdi ); <nl> fuse_conn_put ( fc ); <nl> err_fput : <nl> fput ( file );
static int adv7511_get_modes ( struct adv7511 * adv7511 , <nl> adv7511_set_config_csc ( adv7511 , connector , adv7511 -> rgb , <nl> drm_detect_hdmi_monitor ( edid )); <nl>  <nl> - kfree ( edid ); <nl> - <nl> cec_s_phys_addr_from_edid ( adv7511 -> cec_adap , edid ); <nl>  <nl> + kfree ( edid ); <nl> + <nl> return count ; <nl> } <nl> 
static int rockchip_i2s_remove ( struct platform_device * pdev ) <nl> if (! pm_runtime_status_suspended (& pdev -> dev )) <nl> i2s_runtime_suspend (& pdev -> dev ); <nl>  <nl> - clk_disable_unprepare ( i2s -> mclk ); <nl> clk_disable_unprepare ( i2s -> hclk ); <nl>  <nl> return 0 ;
static void * alloc_qos_entry ( void ) <nl> } <nl> spin_unlock_irqrestore (& qos_free_list . lock , flags ); <nl>  <nl> - entry = kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> - return entry ; <nl> + return kmalloc ( sizeof (* entry ), GFP_ATOMIC ); <nl> } <nl>  <nl> static void free_qos_entry ( void * entry )
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> + . mmu = gf100_mmu_new , <nl> . pci = gp100_pci_new , <nl> . timer = gk20a_timer_new , <nl> . top = gk104_top_new ,
static int s3c2410_udc_ep_enable ( struct usb_ep * _ep , <nl>  <nl> ep = to_s3c2410_ep ( _ep ); <nl>  <nl> - if (! _ep || ! desc || ep -> ep . desc <nl> + if (! _ep || ! desc <nl> || _ep -> name == ep0name <nl> || desc -> bDescriptorType != USB_DT_ENDPOINT ) <nl> return - EINVAL ;
int ll_fid2path ( struct inode * inode , void __user * arg ) <nl> if ( get_user ( pathlen , & gfin -> gf_pathlen )) <nl> return - EFAULT ; <nl>  <nl> + if ( pathlen > PATH_MAX ) <nl> + return - EINVAL ; <nl> + <nl> outsize = sizeof (* gfout ) + pathlen ; <nl>  <nl> OBD_ALLOC ( gfout , outsize );
static int cxgb4vf_open ( struct net_device * dev ) <nl> if ( err ) <nl> return err ; <nl> set_bit ( pi -> port_id , & adapter -> open_device_map ); <nl> - link_start ( dev ); <nl> + err = link_start ( dev ); <nl> + if ( err ) <nl> + return err ; <nl> netif_tx_start_all_queues ( dev ); <nl> return 0 ; <nl> }
static struct platform_driver mxc_w1_driver = { <nl> . name = " mxc_w1 ", <nl> }, <nl> . probe = mxc_w1_probe , <nl> - . remove = __devexit_p ( mxc_w1_remove ), <nl> + . remove = mxc_w1_remove , <nl> }; <nl> module_platform_driver ( mxc_w1_driver ); <nl> 
static void clear_huge_page ( struct page * page , <nl> { <nl> int i ; <nl>  <nl> - if ( unlikely ( sz > MAX_ORDER_NR_PAGES )) { <nl> + if ( unlikely ( sz / PAGE_SIZE > MAX_ORDER_NR_PAGES )) { <nl> clear_gigantic_page ( page , addr , sz ); <nl> return ; <nl> }
static int new_term ( struct parse_events_term ** _term , int type_val , <nl> term -> val . str = str ; <nl> break ; <nl> default : <nl> + free ( term ); <nl> return - EINVAL ; <nl> } <nl> 
nv134_chipset = { <nl> . top = gk104_top_new , <nl> . disp = gp104_disp_new , <nl> . dma = gf119_dma_new , <nl> + . fifo = gp100_fifo_new , <nl> }; <nl>  <nl> static int
void rxrpc_discard_prealloc ( struct rxrpc_sock * rx ) <nl> tail = b -> call_backlog_tail ; <nl> while ( CIRC_CNT ( head , tail , size ) > 0 ) { <nl> struct rxrpc_call * call = b -> call_backlog [ tail ]; <nl> + call -> socket = rx ; <nl> if ( rx -> discard_new_call ) { <nl> _debug (" discard % lx ", call -> user_call_ID ); <nl> rx -> discard_new_call ( call , call -> user_call_ID );
static void tmu_set_mode ( enum clock_event_mode mode , <nl> { <nl> switch ( mode ) { <nl> case CLOCK_EVT_MODE_PERIODIC : <nl> - ctrl_outl ( ctrl_inl ( TMU0_TCNT ), TMU0_TCOR ); <nl> + ctrl_outl ( tmu_latest_interval [ TMU0 ], TMU0_TCOR ); <nl> break ; <nl> case CLOCK_EVT_MODE_ONESHOT : <nl> ctrl_outl ( 0 , TMU0_TCOR );
static int dwc3_gadget_init_hw_endpoints ( struct dwc3 * dwc , <nl>  <nl> dep -> endpoint . name = dep -> name ; <nl>  <nl> + dev_vdbg ( dwc -> dev , " initializing % s \ n ", dep -> name ); <nl> + <nl> if ( epnum == 0 || epnum == 1 ) { <nl> dep -> endpoint . maxpacket = 512 ; <nl> dep -> endpoint . maxburst = 1 ;
static inline void * kvmalloc_array ( size_t n , size_t size , gfp_t flags ) <nl> return kvmalloc ( bytes , flags ); <nl> } <nl>  <nl> + static inline void * kvcalloc ( size_t n , size_t size , gfp_t flags ) <nl> +{ <nl> + return kvmalloc_array ( n , size , flags | __GFP_ZERO ); <nl> +} <nl> + <nl> extern void kvfree ( const void * addr ); <nl>  <nl> static inline atomic_t * compound_mapcount_ptr ( struct page * page )
static void __exit ams_delta_serio_exit ( void ) <nl> free_irq ( OMAP_GPIO_IRQ ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ), 0 ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_DATA ); <nl> - kfree ( ams_delta_serio ); <nl> } <nl> module_exit ( ams_delta_serio_exit );
static int btrfs_ioctl_setflags ( struct file * file , void __user * arg ) <nl> goto out_drop ; <nl>  <nl> } else { <nl> + ret = btrfs_set_prop ( inode , " btrfs . compression ", NULL , 0 , 0 ); <nl> + if ( ret && ret != - ENODATA ) <nl> + goto out_drop ; <nl> ip -> flags &= ~( BTRFS_INODE_COMPRESS | BTRFS_INODE_NOCOMPRESS ); <nl> } <nl> 
void snd_timer_interrupt ( struct snd_timer * timer , unsigned long ticks_left ) <nl> } else { <nl> ti -> flags &= ~ SNDRV_TIMER_IFLG_RUNNING ; <nl> if (-- timer -> running ) <nl> - list_del (& ti -> active_list ); <nl> + list_del_init (& ti -> active_list ); <nl> } <nl> if (( timer -> hw . flags & SNDRV_TIMER_HW_TASKLET ) || <nl> ( ti -> flags & SNDRV_TIMER_IFLG_FAST ))
int vty_init ( const struct file_operations * console_fops ); <nl>  <nl> static inline bool vt_force_oops_output ( struct vc_data * vc ) <nl> { <nl> - if ( oops_in_progress && vc -> vc_panic_force_write ) <nl> + if ( oops_in_progress && vc -> vc_panic_force_write && panic_timeout >= 0 ) <nl> return true ; <nl> return false ; <nl> }
static const char * const mic_bias_level_text [] = { <nl> }; <nl>  <nl> static const struct soc_enum mic_bias_level_enum = <nl> - SOC_ENUM_SINGLE ( CS42L52_IFACE_CTL1 , 0 , <nl> + SOC_ENUM_SINGLE ( CS42L52_IFACE_CTL2 , 0 , <nl> ARRAY_SIZE ( mic_bias_level_text ), mic_bias_level_text ); <nl>  <nl> static const char * const cs42l52_mic_text [] = { " Single ", " Differential " };
struct thread_struct { <nl> . pgdir = swapper_pg_dir , \ <nl> } <nl>  <nl> -/* Do necessary setup to start up a newly executed thread . */ <nl> - void start_thread ( struct pt_regs * regs , <nl> - unsigned long pc , unsigned long usp ); <nl>  <nl> /* Free all resources held by a thread . */ <nl> extern inline void release_thread ( struct task_struct * dead_task )
static int omap_i2c_init ( struct omap_i2c_dev * dev ) <nl> * to get longer filter period for better noise suppression . <nl> * The filter is iclk ( fclk for HS ) period . <nl> */ <nl> - if ( dev -> speed > 400 || cpu_is_omap_2430 ()) <nl> + if ( dev -> speed > 400 || cpu_is_omap2430 ()) <nl> internal_clk = 19200 ; <nl> else if ( dev -> speed > 100 ) <nl> internal_clk = 9600 ;
static int ieee80211_update_mesh_config ( struct wiphy * wiphy , <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_TTL , mask )) <nl> conf -> dot11MeshTTL = nconf -> dot11MeshTTL ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_ELEMENT_TTL , mask )) <nl> - conf -> dot11MeshTTL = nconf -> element_ttl ; <nl> + conf -> element_ttl = nconf -> element_ttl ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_AUTO_OPEN_PLINKS , mask )) <nl> conf -> auto_open_plinks = nconf -> auto_open_plinks ; <nl> if ( _chg_mesh_attr ( NL80211_MESHCONF_SYNC_OFFSET_MAX_NEIGHBOR , mask ))
typedef struct xfs_attr_list_context { <nl> struct attrlist_cursor_kern * cursor ; /* position in list */ <nl> char * alist ; /* output buffer */ <nl> int seen_enough ; /* T / F : seen enough of list ? */ <nl> - int count ; /* num used entries */ <nl> + ssize_t count ; /* num used entries */ <nl> int dupcnt ; /* count dup hashvals seen */ <nl> int bufsize ; /* total buffer size */ <nl> int firstu ; /* first used byte in buffer */
void ieee80211_beacon_connection_loss_work ( struct work_struct * work ) <nl> struct sta_info * sta ; <nl>  <nl> if ( ifmgd -> associated ) { <nl> + rcu_read_lock (); <nl> sta = sta_info_get ( sdata , ifmgd -> bssid ); <nl> if ( sta ) <nl> sta -> beacon_loss_count ++; <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> if ( sdata -> local -> hw . flags & IEEE80211_HW_CONNECTION_MONITOR )
int __mdiobus_register ( struct mii_bus * bus , struct module * owner ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> - put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
int mwifiex_ret_802_11_scan ( struct mwifiex_private * priv , <nl>  <nl> pmatch = adapter -> nd_info -> matches [ idx ]; <nl>  <nl> - if (! pmatch ) { <nl> + if ( pmatch ) { <nl> memset ( pmatch , 0 , sizeof (* pmatch )); <nl> if ( chan_band_tlv ) { <nl> pmatch -> n_channels = 1 ;
static int mxs_phy_probe ( struct platform_device * pdev ) <nl> mxs_phy -> phy . set_suspend = mxs_phy_suspend ; <nl> mxs_phy -> phy . notify_connect = mxs_phy_on_connect ; <nl> mxs_phy -> phy . notify_disconnect = mxs_phy_on_disconnect ; <nl> + mxs_phy -> phy . type = USB_PHY_TYPE_USB2 ; <nl>  <nl> ATOMIC_INIT_NOTIFIER_HEAD (& mxs_phy -> phy . notifier ); <nl> 
static int __exit fsl_udc_remove ( struct platform_device * pdev ) <nl> if (! udc_controller ) <nl> return - ENODEV ; <nl>  <nl> - usb_del_gadget_udc (& udc_controller -> gadget ); <nl> udc_controller -> done = & done ; <nl> + usb_del_gadget_udc (& udc_controller -> gadget ); <nl>  <nl> fsl_udc_clk_release (); <nl> 
static int tsi721_rio_map_inb_mem ( struct rio_mport * mport , dma_addr_t lstart , <nl> } else if ( ibw_start < ( ib_win -> rstart + ib_win -> size ) && <nl> ( ibw_start + ibw_size ) > ib_win -> rstart ) { <nl> /* Return error if address translation involved */ <nl> - if ( direct && ib_win -> xlat ) { <nl> + if (! direct || ib_win -> xlat ) { <nl> ret = - EFAULT ; <nl> break ; <nl> }
struct md_op_data * ll_prep_md_op_data ( struct md_op_data * op_data , <nl> op_data -> op_default_stripe_offset = - 1 ; <nl> if ( S_ISDIR ( i1 -> i_mode )) { <nl> op_data -> op_mea1 = ll_i2info ( i1 )-> lli_lsm_md ; <nl> - op_data -> op_default_stripe_offset = <nl> - ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> + if ( opc == LUSTRE_OPC_MKDIR ) <nl> + op_data -> op_default_stripe_offset = <nl> + ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> } <nl>  <nl> if ( i2 ) {
static struct snd_soc_dai_driver ipq806x_lpass_cpu_dai_driver = { <nl>  <nl> static int ipq806x_lpass_alloc_dma_channel ( struct lpass_data * drvdata , int dir ) <nl> { <nl> - return IPQ806X_LPAIF_RDMA_CHAN_MI2S ; <nl> + if ( dir == SNDRV_PCM_STREAM_PLAYBACK ) <nl> + return IPQ806X_LPAIF_RDMA_CHAN_MI2S ; <nl> + else /* Capture currently not implemented */ <nl> + return - EINVAL ; <nl> } <nl>  <nl> static int ipq806x_lpass_free_dma_channel ( struct lpass_data * drvdata , int chan )
static int sst_platform_open ( struct snd_pcm_substream * substream ) <nl> ret_val = register_sst_card ( stream -> sstdrv_ops ); <nl> if ( ret_val ) { <nl> pr_err (" sst : sst card registration failed \ n "); <nl> + kfree ( stream -> sstdrv_ops ); <nl> + kfree ( stream ); <nl> return ret_val ; <nl> } <nl> runtime -> private_data = stream ;
static int sep_register_driver_with_fs ( struct sep_device * sep ) <nl> if ( ret_val ) { <nl> dev_warn (& sep -> pdev -> dev , " sysfs attribute1 fails for SEP % x \ n ", <nl> ret_val ); <nl> + misc_deregister (& sep -> miscdev_sep ); <nl> return ret_val ; <nl> } <nl> 
static int palmas_i2c_probe ( struct i2c_client * i2c , <nl> ret = - ENOMEM ; <nl> goto err ; <nl> } <nl> + palmas -> i2c_clients [ i ]-> dev . of_node = of_node_get ( node ); <nl> } <nl> palmas -> regmap [ i ] = devm_regmap_init_i2c ( palmas -> i2c_clients [ i ], <nl> & palmas_regmap_config [ i ]);
static long vbg_misc_device_ioctl ( struct file * filp , unsigned int req , <nl> if (! buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( copy_from_user ( buf , ( void *) arg , hdr . size_in )) { <nl> + *(( struct vbg_ioctl_hdr *) buf ) = hdr ; <nl> + if ( copy_from_user ( buf + sizeof ( hdr ), ( void *) arg + sizeof ( hdr ), <nl> + hdr . size_in - sizeof ( hdr ))) { <nl> ret = - EFAULT ; <nl> goto out ; <nl> }
static int ath10k_add_interface ( struct ieee80211_hw * hw , <nl> goto err_vdev_delete ; <nl> } <nl>  <nl> - if ( ar -> cfg_tx_chainmask ) { <nl> + /* Configuring number of spatial stream for monitor interface is causing <nl> + * target assert in qca9888 and qca6174 . <nl> + */ <nl> + if ( ar -> cfg_tx_chainmask && ( vif -> type != NL80211_IFTYPE_MONITOR )) { <nl> u16 nss = get_nss_from_chainmask ( ar -> cfg_tx_chainmask ); <nl>  <nl> vdev_param = ar -> wmi . vdev_param -> nss ;
cy_put_char ( struct tty_struct * tty , unsigned char ch ) <nl> if ( serial_paranoia_check ( info , tty -> name , " cy_put_char ")) <nl> return ; <nl>  <nl> - if (! tty || ! info -> xmit_buf ) <nl> + if (! info -> xmit_buf ) <nl> return ; <nl>  <nl> local_irq_save ( flags ); <nl> cy_write ( struct tty_struct * tty , <nl> return 0 ; <nl> } <nl>  <nl> - if (! tty || ! info -> xmit_buf ){ <nl> + if (! info -> xmit_buf ){ <nl> return 0 ; <nl> } <nl> 
static int sx150x_regmap_reg_width ( struct sx150x_pinctrl * pctl , <nl> reg == data -> pri . x123 . reg_advanced ) <nl> || <nl> ( data -> model == SX150X_456 && <nl> + data -> pri . x456 . reg_advanced && <nl> reg == data -> pri . x456 . reg_advanced )) { <nl> return 8 ; <nl> } else {
static int gart_map_sg ( struct device * dev , struct scatterlist * sg , int nents , <nl>  <nl> error : <nl> flush_gart (); <nl> - gart_unmap_sg ( dev , sg , nents , dir ); <nl> + gart_unmap_sg ( dev , sg , out , dir ); <nl> /* When it was forced or merged try again in a dumb way */ <nl> if ( force_iommu || iommu_merge ) { <nl> out = dma_map_sg_nonforce ( dev , sg , nents , dir );
static struct pnp_driver tpm_inf_pnp_driver = { <nl> . probe = tpm_inf_pnp_probe , <nl> . suspend = tpm_inf_pnp_suspend , <nl> . resume = tpm_inf_pnp_resume , <nl> - . remove = __devexit_p ( tpm_inf_pnp_remove ) <nl> + . remove = tpm_inf_pnp_remove <nl> }; <nl>  <nl> static int __init init_inf ( void )
static int ima_lsm_rule_init ( struct ima_measure_rule_entry * entry , <nl> result = security_filter_rule_init ( entry -> lsm [ lsm_rule ]. type , <nl> Audit_equal , args , <nl> & entry -> lsm [ lsm_rule ]. rule ); <nl> + if (! entry -> lsm [ lsm_rule ]. rule ) <nl> + return - EINVAL ; <nl> return result ; <nl> } <nl> 
CIFSSMBSetAttrLegacy ( int xid , struct cifsTconInfo * tcon , char * fileName , <nl>  <nl> int <nl> CIFSSMBUnixSetInfo ( const int xid , struct cifsTconInfo * tcon , char * fileName , <nl> - const struct cifs_unix_set_info_args * args , <nl> + const struct cifs_unix_set_info_args * args , <nl> const struct nls_table * nls_codepage , int remap ) <nl> { <nl> TRANSACTION2_SPI_REQ * pSMB = NULL ;
int kvm_arch_vcpu_ioctl_set_sregs ( struct kvm_vcpu * vcpu , <nl> kvm_set_segment ( vcpu , & sregs -> tr , VCPU_SREG_TR ); <nl> kvm_set_segment ( vcpu , & sregs -> ldt , VCPU_SREG_LDTR ); <nl>  <nl> + update_cr8_intercept ( vcpu ); <nl> + <nl> /* Older userspace won ' t unhalt the vcpu on reset . */ <nl> if ( kvm_vcpu_is_bsp ( vcpu ) && kvm_rip_read ( vcpu ) == 0xfff0 && <nl> sregs -> cs . selector == 0xf000 && sregs -> cs . base == 0xffff0000 &&
static int igb_ptp_feature_enable_i210 ( struct ptp_clock_info * ptp , <nl> ts . tv_nsec = rq -> perout . period . nsec ; <nl> ns = timespec64_to_ns (& ts ); <nl> ns = ns >> 1 ; <nl> - if ( on && ns <= 70000000LL ) { <nl> + if ( on && (( ns <= 70000000LL ) || ( ns == 125000000LL ) || <nl> + ( ns == 250000000LL ) || ( ns == 500000000LL ))) { <nl> if ( ns < 8LL ) <nl> return - EINVAL ; <nl> use_freq = 1 ;
static int pci_create_attr ( struct pci_dev * pdev , int num , int write_combine ) <nl> res_attr -> size = pci_resource_len ( pdev , num ); <nl> res_attr -> private = & pdev -> resource [ num ]; <nl> retval = sysfs_create_bin_file (& pdev -> dev . kobj , res_attr ); <nl> + if ( retval ) <nl> + kfree ( res_attr ); <nl> } else <nl> retval = - ENOMEM ; <nl> 
static int __init tegra_pmc_early_init ( void ) <nl> bool invert ; <nl> u32 value ; <nl>  <nl> + mutex_init (& pmc -> powergates_lock ); <nl> + <nl> np = of_find_matching_node_and_match ( NULL , tegra_pmc_match , & match ); <nl> if (! np ) { <nl> /* <nl> static int __init tegra_pmc_early_init ( void ) <nl> return - ENXIO ; <nl> } <nl>  <nl> - mutex_init (& pmc -> powergates_lock ); <nl> - <nl> if ( np ) { <nl> pmc -> soc = match -> data ; <nl> 
static int llc_ui_sendmsg ( struct socket * sock , struct msghdr * msg , size_t len ) <nl> if ( size > llc -> dev -> mtu ) <nl> size = llc -> dev -> mtu ; <nl> copied = size - hdrlen ; <nl> + rc = - EINVAL ; <nl> + if ( copied < 0 ) <nl> + goto release ; <nl> release_sock ( sk ); <nl> skb = sock_alloc_send_skb ( sk , size , noblock , & rc ); <nl> lock_sock ( sk );
static int kvm_vcpu_ioctl_enable_cap ( struct kvm_vcpu * vcpu , <nl>  <nl> switch ( cap -> cap ) { <nl> case KVM_CAP_HYPERV_SYNIC : <nl> + if (! irqchip_in_kernel ( vcpu -> kvm )) <nl> + return - EINVAL ; <nl> return kvm_hv_activate_synic ( vcpu ); <nl> default : <nl> return - EINVAL ;
static void igb_reset_q_vector ( struct igb_adapter * adapter , int v_idx ) <nl> { <nl> struct igb_q_vector * q_vector = adapter -> q_vector [ v_idx ]; <nl>  <nl> + /* Coming from igb_set_interrupt_capability , the vectors are not yet <nl> + * allocated . So , q_vector is NULL so we should stop here . <nl> + */ <nl> + if (! q_vector ) <nl> + return ; <nl> + <nl> if ( q_vector -> tx . ring ) <nl> adapter -> tx_ring [ q_vector -> tx . ring -> queue_index ] = NULL ; <nl> 
void btrfs_update_iflags ( struct inode * inode ) <nl> */ <nl> void btrfs_inherit_iflags ( struct inode * inode , struct inode * dir ) <nl> { <nl> - unsigned int flags = BTRFS_I ( dir )-> flags ; <nl> + unsigned int flags ; <nl> + <nl> + if (! dir ) <nl> + return ; <nl> + <nl> + flags = BTRFS_I ( dir )-> flags ; <nl>  <nl> if ( S_ISREG ( inode -> i_mode )) <nl> flags &= ~ BTRFS_INODE_DIRSYNC ;
static int vidioc_querycap ( struct file * file , void * priv , <nl> strcpy ( cap -> driver , " vivi "); <nl> strcpy ( cap -> card , " vivi "); <nl> strlcpy ( cap -> bus_info , dev -> v4l2_dev . name , sizeof ( cap -> bus_info )); <nl> - cap -> capabilities = V4L2_CAP_VIDEO_CAPTURE | V4L2_CAP_STREAMING | \ <nl> - V4L2_CAP_READWRITE ; <nl> + cap -> capabilities = V4L2_CAP_VIDEO_CAPTURE | V4L2_CAP_STREAMING | <nl> + V4L2_CAP_READWRITE | V4L2_CAP_DEVICE_CAPS ; <nl> + cap -> device_caps = cap -> capabilities ; <nl> return 0 ; <nl> } <nl> 
static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
extern int blk_iopoll_enabled ; <nl> static int sixty = 60 ; <nl> # endif <nl>  <nl> + static int neg_one = - 1 ; <nl> static int zero ; <nl> static int __maybe_unused one = 1 ; <nl> static int __maybe_unused two = 2 ;
int afs_prepare_write ( struct file * file , struct page * page , <nl> _leave (" = % d [ prep ]", ret ); <nl> return ret ; <nl> } <nl> - SetPageUptodate ( page ); <nl> } <nl>  <nl> try_again : <nl> int afs_commit_write ( struct file * file , struct page * page , <nl> spin_unlock (& vnode -> writeback_lock ); <nl> } <nl>  <nl> + SetPageUptodate ( page ); <nl> set_page_dirty ( page ); <nl> - <nl> if ( PageDirty ( page )) <nl> _debug (" dirtied "); <nl> 
int saa7164_cmd_send ( struct saa7164_dev * dev , u8 id , enum tmComResCmd command , <nl> dprintk ( DBGLVL_CMD , <nl> "% s () UNKNOWN OR INVALID CONTROL \ n ", <nl> __func__ ); <nl> + ret = SAA_ERR_NOT_SUPPORTED ; <nl> + break ; <nl> default : <nl> dprintk ( DBGLVL_CMD , "% s () UNKNOWN \ n ", __func__ ); <nl> ret = SAA_ERR_NOT_SUPPORTED ;
int inet6_csk_bind_conflict ( const struct sock * sk , <nl> if ( ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> break ; <nl> } <nl> + if (! relax && reuse && sk2 -> sk_reuse && <nl> + sk2 -> sk_state != TCP_LISTEN && <nl> + ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> + break ; <nl> } <nl> } <nl> 
static void i40iw_cm_disconn_true ( struct i40iw_qp * iwqp ) <nl> /* Flush the queues */ <nl> i40iw_flush_wqes ( iwdev , iwqp ); <nl>  <nl> - if ( qp -> term_flags ) { <nl> + if ( qp -> term_flags && iwqp -> ibqp . event_handler ) { <nl> ibevent . device = iwqp -> ibqp . device ; <nl> ibevent . event = ( qp -> eventtype == TERM_EVENT_QP_FATAL ) ? <nl> IB_EVENT_QP_FATAL : IB_EVENT_QP_ACCESS_ERR ;
static int xen_allocate_irq_gsi ( unsigned gsi ) <nl>  <nl> static void xen_free_irq ( unsigned irq ) <nl> { <nl> + /* Legacy IRQ descriptors are managed by the arch . */ <nl> + if ( irq < NR_IRQS_LEGACY ) <nl> + return ; <nl> + <nl> irq_free_desc ( irq ); <nl> } <nl> 
static int aead_setkey ( struct crypto_aead * tfm , const u8 * key , <nl> ctx -> authkey_len = keys . authkeylen ; <nl> ctx -> enckey_len = keys . enckeylen ; <nl>  <nl> + memzero_explicit (& keys , sizeof ( keys )); <nl> return aead_setup ( tfm , crypto_aead_authsize ( tfm )); <nl> badkey : <nl> crypto_aead_set_flags ( tfm , CRYPTO_TFM_RES_BAD_KEY_LEN ); <nl> + memzero_explicit (& keys , sizeof ( keys )); <nl> return - EINVAL ; <nl> } <nl> 
int snd_timer_stop ( struct snd_timer_instance * timeri ) <nl> if ( err < 0 ) <nl> return err ; <nl> timer = timeri -> timer ; <nl> + if (! timer ) <nl> + return - EINVAL ; <nl> spin_lock_irqsave (& timer -> lock , flags ); <nl> timeri -> cticks = timeri -> ticks ; <nl> timeri -> pticks = 0 ;
int intel_gvt_init ( struct drm_i915_private * dev_priv ) <nl> goto bail ; <nl> } <nl>  <nl> + if (! i915 . enable_execlists ) { <nl> + DRM_INFO (" GPU guest virtualisation [ GVT - g ] disabled due to disabled execlist submission [ i915 . enable_execlists module parameter ]\ n "); <nl> + goto bail ; <nl> + } <nl> + <nl> /* <nl> * We ' re not in host or fail to find a MPT module , disable GVT - g <nl> */
static struct attribute * module_attrs [] = { <nl> }; <nl> ATTRIBUTE_GROUPS ( module ); <nl>  <nl> - static void greybus_module_release ( struct device * dev ) <nl> + static void gb_module_release ( struct device * dev ) <nl> { <nl> struct gb_module * module = to_gb_module ( dev ); <nl>  <nl> static void greybus_module_release ( struct device * dev ) <nl>  <nl> struct device_type greybus_module_type = { <nl> . name = " greybus_module ", <nl> - . release = greybus_module_release , <nl> + . release = gb_module_release , <nl> }; <nl>  <nl> struct module_find {
static int do_replace ( struct net * net , const void __user * user , <nl> if ( tmp . num_counters >= INT_MAX / sizeof ( struct ebt_counter )) <nl> return - ENOMEM ; <nl>  <nl> + tmp . name [ sizeof ( tmp . name ) - 1 ] = 0 ; <nl> + <nl> countersize = COUNTER_OFFSET ( tmp . nentries ) * nr_cpu_ids ; <nl> newinfo = vmalloc ( sizeof (* newinfo ) + countersize ); <nl> if (! newinfo )
static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
int sysfs_move_dir ( struct kobject * kobj , struct kobject * new_parent_kobj ) <nl> /* Remove from old parent ' s list and insert into new parent ' s list . */ <nl> sysfs_unlink_sibling ( sd ); <nl> sysfs_get ( new_parent_sd ); <nl> + drop_nlink ( old_parent -> d_inode ); <nl> sysfs_put ( sd -> s_parent ); <nl> sd -> s_parent = new_parent_sd ; <nl> + inc_nlink ( new_parent -> d_inode ); <nl> sysfs_link_sibling ( sd ); <nl>  <nl> out_unlock :
static struct fileIdentDesc * udf_find_entry ( struct inode * dir , <nl> } <nl>  <nl> if (( cfi -> fileCharacteristics & FID_FILE_CHAR_PARENT ) && <nl> - isdotdot ) { <nl> - brelse ( epos . bh ); <nl> - return fi ; <nl> - } <nl> + isdotdot ) <nl> + goto out_ok ; <nl>  <nl> if (! lfi ) <nl> continue ;
extern pgprot_t phys_mem_access_prot ( struct file * file , unsigned long pfn , <nl> # define pmd_sect ( pmd ) (( pmd_val ( pmd ) & PMD_TYPE_MASK ) == \ <nl> PMD_TYPE_SECT ) <nl>  <nl> -# ifdef CONFIG_ARM64_64K_PAGES <nl> +# if defined ( CONFIG_ARM64_64K_PAGES ) || CONFIG_PGTABLE_LEVELS < 3 <nl> # define pud_sect ( pud ) ( 0 ) <nl> # define pud_table ( pud ) ( 1 ) <nl> # else
void init_tg_cfs_entry ( struct task_group * tg , struct cfs_rq * cfs_rq , <nl> se -> cfs_rq = parent -> my_q ; <nl>  <nl> se -> my_q = cfs_rq ; <nl> - update_load_set (& se -> load , 0 ); <nl> + /* guarantee group entities always have weight */ <nl> + update_load_set (& se -> load , NICE_0_LOAD ); <nl> se -> parent = parent ; <nl> } <nl> 
acpi_status acpi_os_initialize1 ( void ); <nl> int init_acpi_device_notify ( void ); <nl> int acpi_scan_init ( void ); <nl> -# ifdef CONFIG_ACPI_PCI_SLOT <nl> - void acpi_pci_slot_init ( void ); <nl> -# else <nl> - static inline void acpi_pci_slot_init ( void ) { } <nl> -# endif <nl> void acpi_pci_root_init ( void ); <nl> void acpi_pci_link_init ( void ); <nl> void acpi_pci_root_hp_init ( void );
static void acpi_ac_notify ( struct acpi_device * device , u32 event ) <nl> acpi_bus_generate_netlink_event ( device -> pnp . device_class , <nl> dev_name (& device -> dev ), event , <nl> ( u32 ) ac -> state ); <nl> + acpi_notifier_call_chain ( device , event , ( u32 ) ac -> state ); <nl> # ifdef CONFIG_ACPI_SYSFS_POWER <nl> kobject_uevent (& ac -> charger . dev -> kobj , KOBJ_CHANGE ); <nl> # endif
static int wm8753_register ( struct wm8753_priv * wm8753 ) <nl> codec -> reg_cache = & wm8753 -> reg_cache ; <nl> codec -> private_data = wm8753 ; <nl>  <nl> - memcpy ( codec -> reg_cache , wm8753_reg , sizeof ( codec -> reg_cache )); <nl> + memcpy ( codec -> reg_cache , wm8753_reg , sizeof ( wm8753 -> reg_cache )); <nl> INIT_DELAYED_WORK (& codec -> delayed_work , wm8753_work ); <nl>  <nl> ret = wm8753_reset ( codec );
static int __devinit e100_probe ( struct pci_dev * pdev , <nl>  <nl> e100_get_defaults ( nic ); <nl>  <nl> + /* D100 MAC doesn ' t allow rx of vlan packets with normal MTU */ <nl> + if ( nic -> mac < mac_82558_D101_A4 ) <nl> + netdev -> features |= NETIF_F_VLAN_CHALLENGED ; <nl> + <nl> /* locks must be initialized before calling hw_reset */ <nl> spin_lock_init (& nic -> cb_lock ); <nl> spin_lock_init (& nic -> cmd_lock );
int p9_client_remove ( struct p9_fid * fid ) <nl> P9_DPRINTK ( P9_DEBUG_9P , "<<< RREMOVE fid % d \ n ", fid -> fid ); <nl>  <nl> p9_free_req ( clnt , req ); <nl> - p9_fid_destroy ( fid ); <nl> - <nl> error : <nl> + p9_fid_destroy ( fid ); <nl> return err ; <nl> } <nl> EXPORT_SYMBOL ( p9_client_remove );
static void ieee80211_xmit ( struct ieee80211_sub_if_data * sdata , <nl> list ) { <nl> if (! ieee80211_sdata_running ( tmp_sdata )) <nl> continue ; <nl> - if ( tmp_sdata -> vif . type != NL80211_IFTYPE_AP ) <nl> + if ( tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_MONITOR || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_AP_VLAN || <nl> + tmp_sdata -> vif . type == <nl> + NL80211_IFTYPE_WDS ) <nl> continue ; <nl> if ( compare_ether_addr ( tmp_sdata -> vif . addr , <nl> hdr -> addr2 ) == 0 ) {
static int mem_cgroup_hierarchical_reclaim ( struct mem_cgroup * root_mem , <nl> excess = res_counter_soft_limit_excess (& root_mem -> res ) >> PAGE_SHIFT ; <nl>  <nl> /* If memsw_is_minimum == 1 , swap - out is of - no - use . */ <nl> - if ( root_mem -> memsw_is_minimum ) <nl> + if (! check_soft && root_mem -> memsw_is_minimum ) <nl> noswap = true ; <nl>  <nl> while ( 1 ) {
static const struct amdgpu_irq_src_funcs dm_hpd_irq_funcs = { <nl>  <nl> void amdgpu_dm_set_irq_funcs ( struct amdgpu_device * adev ) <nl> { <nl> - if ( adev -> mode_info . num_crtc > 0 ) <nl> - adev -> crtc_irq . num_types = AMDGPU_CRTC_IRQ_VLINE1 + adev -> mode_info . num_crtc ; <nl> - else <nl> - adev -> crtc_irq . num_types = 0 ; <nl> + <nl> + adev -> crtc_irq . num_types = adev -> mode_info . num_crtc ; <nl> adev -> crtc_irq . funcs = & dm_crtc_irq_funcs ; <nl>  <nl> adev -> pageflip_irq . num_types = adev -> mode_info . num_crtc ;
static int intel_crtc_mode_set ( struct drm_crtc * crtc , <nl> } <nl> dpll |= DPLL_DVO_HIGH_SPEED ; <nl> } <nl> - if ( is_dp ) <nl> + if ( is_dp || intel_encoder_is_pch_edp (& has_edp_encoder -> base )) <nl> dpll |= DPLL_DVO_HIGH_SPEED ; <nl>  <nl> /* compute bitmask from p1 value */
int __nvme_submit_sync_cmd ( struct request_queue * q , struct nvme_command * cmd , <nl> return PTR_ERR ( req ); <nl>  <nl> req -> cmd_type = REQ_TYPE_DRV_PRIV ; <nl> + req -> cmd_flags = REQ_FAILFAST_DRIVER ; <nl> req -> __data_len = 0 ; <nl> req -> __sector = ( sector_t ) - 1 ; <nl> req -> bio = req -> biotail = NULL ;
int main ( int argc , char ** argv ) <nl> "% s - dev % d ", device_name , dev_num ); <nl> if ( ret < 0 ) { <nl> ret = - ENOMEM ; <nl> - goto error_ret ; <nl> + goto error_free_dev_dir_name ; <nl> } <nl> } <nl>  <nl> int main ( int argc , char ** argv ) <nl> error_free_triggername : <nl> if ( datardytrigger ) <nl> free ( trigger_name ); <nl> + error_free_dev_dir_name : <nl> + free ( dev_dir_name ); <nl> error_ret : <nl> return ret ; <nl> }
static int f2fs_ioc_defragment ( struct file * filp , unsigned long arg ) <nl> goto out ; <nl> } <nl>  <nl> + if ( unlikely (( range . start + range . len ) >> PAGE_SHIFT > <nl> + sbi -> max_file_blocks )) { <nl> + err = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> err = f2fs_defragment_range ( sbi , filp , & range ); <nl> f2fs_update_time ( sbi , REQ_TIME ); <nl> if ( err < 0 )
__ieee80211_tx_prepare ( struct ieee80211_tx_data * tx , <nl> tx -> local = local ; <nl> tx -> sdata = IEEE80211_DEV_TO_SUB_IF ( dev ); <nl> tx -> channel = local -> hw . conf . channel ; <nl> + tx -> rate_idx = - 1 ; <nl> + tx -> last_frag_rate_idx = - 1 ; <nl> /* <nl> * Set this flag ( used below to indicate " automatic fragmentation "), <nl> * it will be cleared / left by radiotap as desired .
static void bnx2x_tpa_stop ( struct bnx2x * bp , struct bnx2x_fastpath * fp , <nl>  <nl> return ; <nl> } <nl> - bnx2x_frag_free ( fp , new_data ); <nl> + if ( new_data ) <nl> + bnx2x_frag_free ( fp , new_data ); <nl> drop : <nl> /* drop the packet and keep the buffer in the bin */ <nl> DP ( NETIF_MSG_RX_STATUS ,
static int asus_laptop_get_info ( struct asus_laptop * asus ) <nl> } <nl> } <nl> asus -> name = kstrdup ( string , GFP_KERNEL ); <nl> - if (! asus -> name ) <nl> + if (! asus -> name ) { <nl> + kfree ( buffer . pointer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if (* string ) <nl> pr_notice (" % s model detected \ n ", string );
static const struct dmi_system_id min_max_dmi_table [] __initconst = { <nl> }, <nl> . driver_data = ( int []){ 1024 , 5052 , 2258 , 4832 }, <nl> }, <nl> + { <nl> + /* Lenovo ThinkPad X240 */ <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " LENOVO "), <nl> + DMI_MATCH ( DMI_PRODUCT_VERSION , " ThinkPad X240 "), <nl> + }, <nl> + . driver_data = ( int []){ 1232 , 5710 , 1156 , 4696 }, <nl> + }, <nl> { <nl> /* Lenovo ThinkPad T440s */ <nl> . matches = {
static int disable_periodic ( struct ehci_hcd * ehci ) <nl> ehci_writel ( ehci , cmd , & ehci -> regs -> command ); <nl> /* posted write ... */ <nl>  <nl> + free_cached_itd_list ( ehci ); <nl> + <nl> ehci -> next_uframe = - 1 ; <nl> return 0 ; <nl> }
static int __devinit eni_start ( struct atm_dev * dev ) <nl> kfree ( eni_dev -> free_list ); <nl>  <nl> free_irq : <nl> - free_irq ( eni_dev -> irq , eni_dev ); <nl> + free_irq ( eni_dev -> irq , dev ); <nl>  <nl> out : <nl> return error ;
static int intel_atomic_check ( struct drm_device * dev , <nl> struct intel_crtc_state * pipe_config = <nl> to_intel_crtc_state ( crtc_state ); <nl>  <nl> + memset (& to_intel_crtc ( crtc )-> atomic , 0 , <nl> + sizeof ( struct intel_crtc_atomic_commit )); <nl> + <nl> /* Catch I915_MODE_FLAG_INHERITED */ <nl> if ( crtc_state -> mode . private_flags != crtc -> state -> mode . private_flags ) <nl> crtc_state -> mode_changed = true ;
nvmet_fc_find_target_queue ( struct nvmet_fc_tgtport * tgtport , <nl> u16 qid = nvmet_fc_getqueueid ( connection_id ); <nl> unsigned long flags ; <nl>  <nl> + if ( qid > NVMET_NR_QUEUES ) <nl> + return NULL ; <nl> + <nl> spin_lock_irqsave (& tgtport -> lock , flags ); <nl> list_for_each_entry ( assoc , & tgtport -> assoc_list , a_list ) { <nl> if ( association_id == assoc -> association_id ) {
static int wm_adsp_fw_put ( struct snd_kcontrol * kcontrol , <nl> if ( adsp [ e -> shift_l ]. running ) <nl> return - EBUSY ; <nl>  <nl> - adsp -> fw = ucontrol -> value . integer . value [ 0 ]; <nl> + adsp [ e -> shift_l ]. fw = ucontrol -> value . integer . value [ 0 ]; <nl>  <nl> return 0 ; <nl> }
static int greybus_remove ( struct device * dev ) <nl> gb_connection_disable_rx ( connection ); <nl>  <nl> driver -> disconnect ( bundle ); <nl> + <nl> + /* Catch buggy drivers that fail to disable their connections . */ <nl> + list_for_each_entry ( connection , & bundle -> connections , bundle_links ) { <nl> + if ( WARN_ON ( connection -> state != GB_CONNECTION_STATE_DISABLED )) <nl> + gb_connection_disable ( connection ); <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
smb2_query_symlink ( const unsigned int xid , struct cifs_tcon * tcon , <nl> & resp_buftype ); <nl> if (! rc || ! err_iov . iov_base ) { <nl> rc = - ENOENT ; <nl> - goto querty_exit ; <nl> + goto free_path ; <nl> } <nl>  <nl> err_buf = err_iov . iov_base ; <nl> smb2_query_symlink ( const unsigned int xid , struct cifs_tcon * tcon , <nl>  <nl> querty_exit : <nl> free_rsp_buf ( resp_buftype , err_buf ); <nl> + free_path : <nl> kfree ( utf16_path ); <nl> return rc ; <nl> }
static void atomic_switch_perf_msrs ( struct vcpu_vmx * vmx ) <nl> msrs [ i ]. host ); <nl> } <nl>  <nl> - void vmx_arm_hv_timer ( struct kvm_vcpu * vcpu ) <nl> + static void vmx_arm_hv_timer ( struct kvm_vcpu * vcpu ) <nl> { <nl> struct vcpu_vmx * vmx = to_vmx ( vcpu ); <nl> u64 tscl ;
int __vma_adjust ( struct vm_area_struct * vma , unsigned long start , <nl> * If next doesn ' t have anon_vma , import from vma after <nl> * next , if the vma overlaps with it . <nl> */ <nl> - if ( remove_next == 2 && next && ! next -> anon_vma ) <nl> + if ( remove_next == 2 && ! next -> anon_vma ) <nl> exporter = next -> vm_next ; <nl>  <nl> } else if ( end > next -> vm_start ) {
static int snd_echo_resume ( struct pci_dev * pci ) <nl> DE_INIT ((" resume start \ n ")); <nl> pci_restore_state ( pci ); <nl> commpage_bak = kmalloc ( sizeof ( struct echoaudio ), GFP_KERNEL ); <nl> + if ( commpage_bak == NULL ) <nl> + return - ENOMEM ; <nl> commpage = chip -> comm_page ; <nl> memcpy ( commpage_bak , commpage , sizeof ( struct comm_page )); <nl> 
static int imx_ldb_connector_get_modes ( struct drm_connector * connector ) <nl> struct drm_display_mode * mode ; <nl>  <nl> mode = drm_mode_create ( connector -> dev ); <nl> + if (! mode ) <nl> + return - EINVAL ; <nl> drm_mode_copy ( mode , & imx_ldb_ch -> mode ); <nl> mode -> type |= DRM_MODE_TYPE_DRIVER | DRM_MODE_TYPE_PREFERRED ; <nl> drm_mode_probed_add ( connector , mode );
static __init void parse_cmdline_early ( char ** cmdline_p ) <nl> if (! memcmp ( from , " noapic ", 6 )) <nl> skip_ioapic_setup = 1 ; <nl>  <nl> - if (! memcmp ( from , " apic ", 4 )) { <nl> + /* Make sure to not confuse with apic = */ <nl> + if (! memcmp ( from , " apic ", 4 ) && <nl> + ( from [ 4 ] == ' ' || from [ 4 ] == 0 )) { <nl> skip_ioapic_setup = 0 ; <nl> ioapic_force = 1 ; <nl> }
static int __devinit mc13783_led_probe ( struct platform_device * pdev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - led = kzalloc ( sizeof (* led ) * pdata -> num_leds , GFP_KERNEL ); <nl> + led = kcalloc ( pdata -> num_leds , sizeof (* led ), GFP_KERNEL ); <nl> if ( led == NULL ) { <nl> dev_err (& pdev -> dev , " failed to alloc memory \ n "); <nl> return - ENOMEM ;
int kprobe_fault_handler ( struct pt_regs * regs , unsigned long cause ); <nl> void kretprobe_trampoline ( void ); <nl> void trap_is_kprobe ( unsigned long address , struct pt_regs * regs ); <nl> # else <nl> - static void trap_is_kprobe ( unsigned long address , struct pt_regs * regs ) <nl> -{ <nl> -} <nl> +# define trap_is_kprobe ( address , regs ) <nl> # endif /* CONFIG_KPROBES */ <nl>  <nl> # endif /* _ARC_KPROBES_H */
static bool gen8_ppgtt_clear_pt ( struct i915_address_space * vm , <nl> GEM_BUG_ON ( pte_end > GEN8_PTES ); <nl>  <nl> bitmap_clear ( pt -> used_ptes , pte , num_entries ); <nl> - <nl> - if ( bitmap_empty ( pt -> used_ptes , GEN8_PTES )) <nl> - return true ; <nl> + if ( USES_FULL_PPGTT ( vm -> i915 )) { <nl> + if ( bitmap_empty ( pt -> used_ptes , GEN8_PTES )) <nl> + return true ; <nl> + } <nl>  <nl> pt_vaddr = kmap_px ( pt ); <nl> 
void uncore_perf_event_update ( struct intel_uncore_box * box , struct perf_event * e <nl> u64 prev_count , new_count , delta ; <nl> int shift ; <nl>  <nl> - if ( event -> hw . idx >= UNCORE_PMC_IDX_FIXED ) <nl> + if ( event -> hw . idx == UNCORE_PMC_IDX_FIXED ) <nl> shift = 64 - uncore_fixed_ctr_bits ( box ); <nl> else <nl> shift = 64 - uncore_perf_ctr_bits ( box );
static int spi_imx_probe ( struct platform_device * pdev ) <nl> goto out_clk_put ; <nl> } <nl>  <nl> + if (! master -> cs_gpios ) { <nl> + dev_err (& pdev -> dev , " No CS GPIOs available \ n "); <nl> + goto out_clk_put ; <nl> + } <nl> + <nl> for ( i = 0 ; i < master -> num_chipselect ; i ++) { <nl> if (! gpio_is_valid ( master -> cs_gpios [ i ])) <nl> continue ;
static enum dma_status sun6i_dma_tx_status ( struct dma_chan * chan , <nl> size_t bytes = 0 ; <nl>  <nl> ret = dma_cookie_status ( chan , cookie , state ); <nl> - if ( ret == DMA_COMPLETE ) <nl> + if ( ret == DMA_COMPLETE || ! state ) <nl> return ret ; <nl>  <nl> spin_lock_irqsave (& vchan -> vc . lock , flags );
static int bnx2x_populate_ext_phy ( struct bnx2x * bp , <nl> return - EINVAL ; <nl> default : <nl> * phy = phy_null ; <nl> + /* In case external PHY wasn ' t found */ <nl> + if (( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_DIRECT ) && <nl> + ( phy_type != PORT_HW_CFG_XGXS_EXT_PHY_TYPE_NOT_CONN )) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
static void seq_set_overflow ( struct seq_file * m ) <nl>  <nl> static void * seq_buf_alloc ( unsigned long size ) <nl> { <nl> + if ( unlikely ( size > MAX_RW_COUNT )) <nl> + return NULL ; <nl> + <nl> return kvmalloc ( size , GFP_KERNEL_ACCOUNT ); <nl> } <nl> 
static int nvme_nvm_submit_user_cmd ( struct request_queue * q , <nl>  <nl> rq -> timeout = timeout ? timeout : ADMIN_TIMEOUT ; <nl>  <nl> - rq -> cmd_flags &= ~ REQ_FAILFAST_DRIVER ; <nl> - <nl> if ( ppa_buf && ppa_len ) { <nl> ppa_list = dma_pool_alloc ( dev -> dma_pool , GFP_KERNEL , & ppa_dma ); <nl> if (! ppa_list ) {
static int btrfs_get_name ( struct dentry * parent , char * name , <nl> name_len = btrfs_inode_ref_name_len ( leaf , iref ); <nl> } <nl>  <nl> + ret = btrfs_is_name_len_valid ( leaf , path -> slots [ 0 ], name_ptr , name_len ); <nl> + if (! ret ) { <nl> + btrfs_free_path ( path ); <nl> + return - EIO ; <nl> + } <nl> read_extent_buffer ( leaf , name , name_ptr , name_len ); <nl> btrfs_free_path ( path ); <nl> 
static struct dma_chan * k3_of_dma_simple_xlate ( struct of_phandle_args * dma_spec , <nl> struct k3_dma_dev * d = ofdma -> of_dma_data ; <nl> unsigned int request = dma_spec -> args [ 0 ]; <nl>  <nl> - if ( request > d -> dma_requests ) <nl> + if ( request >= d -> dma_requests ) <nl> return NULL ; <nl>  <nl> return dma_get_slave_channel (&( d -> chans [ request ]. vc . chan ));
int wilc_netdev_init ( void ) <nl>  <nl> /* create the common structure */ <nl> g_linux_wlan = kzalloc ( sizeof ( linux_wlan_t ), GFP_KERNEL ); <nl> + if (! g_linux_wlan ) <nl> + return - ENOMEM ; <nl>  <nl> /* Reset interrupt count debug */ <nl> int_rcvdU = 0 ;
int ath9k_wmi_cmd ( struct wmi * wmi , enum wmi_cmd_id cmd_id , <nl> ath_dbg ( common , WMI , " Timeout waiting for WMI command : % s \ n ", <nl> wmi_cmd_to_name ( cmd_id )); <nl> mutex_unlock (& wmi -> op_mutex ); <nl> + kfree_skb ( skb ); <nl> return - ETIMEDOUT ; <nl> } <nl> 
acornfb_pan_display ( struct fb_var_screeninfo * var , struct fb_info * info ) <nl> if (!( var -> vmode & FB_VMODE_YWRAP )) <nl> y_bottom += var -> yres ; <nl>  <nl> - BUG_ON ( y_bottom > var -> yres_virtual ); <nl> + if ( y_bottom > var -> yres_virtual ) <nl> + return - EINVAL ; <nl>  <nl> acornfb_update_dma ( info , var ); <nl> 
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> dev -> name ); <nl> break ; <nl> } <nl> + <nl> + if ( idev ) <nl> + idev -> if_flags |= IF_READY ; <nl> } else { <nl> if (! netif_carrier_ok ( dev )) { <nl> /* device is still not ready . */
static long evdev_do_ioctl ( struct file * file , unsigned int cmd , <nl> return - EFAULT ; <nl>  <nl> error = input_ff_upload ( dev , & effect , file ); <nl> + if ( error ) <nl> + return error ; <nl>  <nl> if ( put_user ( effect . id , &((( struct ff_effect __user *) p )-> id ))) <nl> return - EFAULT ; <nl>  <nl> - return error ; <nl> + return 0 ; <nl> } <nl>  <nl> /* Multi - number variable - length handlers */
static void fbcon_deinit ( struct vc_data * vc ) <nl> finished : <nl>  <nl> fbcon_free_font ( p , free_font ); <nl> + if ( free_font ) <nl> + vc -> vc_font . data = NULL ; <nl>  <nl> if (! con_is_bound (& fb_con )) <nl> fbcon_exit ();
static void qedi_get_boot_tgt_info ( struct nvm_iscsi_block * block , <nl> ipv6_en = !!( block -> generic . ctrl_flags & <nl> NVM_ISCSI_CFG_GEN_IPV6_ENABLED ); <nl>  <nl> - snprintf ( tgt -> iscsi_name , NVM_ISCSI_CFG_ISCSI_NAME_MAX_LEN , "% s \ n ", <nl> + snprintf ( tgt -> iscsi_name , sizeof ( tgt -> iscsi_name ), "% s \ n ", <nl> block -> target [ index ]. target_name . byte ); <nl>  <nl> tgt -> ipv6_en = ipv6_en ;
static void mtk_drm_unbind ( struct device * dev ) <nl> { <nl> struct mtk_drm_private * private = dev_get_drvdata ( dev ); <nl>  <nl> - drm_put_dev ( private -> drm ); <nl> + drm_dev_unregister ( private -> drm ); <nl> + drm_dev_unref ( private -> drm ); <nl> private -> drm = NULL ; <nl> } <nl> 
static int do_umount ( struct mount * mnt , int flags ) <nl> * Special case for " unmounting " root ... <nl> * we just try to remount it readonly . <nl> */ <nl> + if (! capable ( CAP_SYS_ADMIN )) <nl> + return - EPERM ; <nl> down_write (& sb -> s_umount ); <nl> if (!( sb -> s_flags & MS_RDONLY )) <nl> retval = do_remount_sb ( sb , MS_RDONLY , NULL , 0 );
static inline void mtd_erase_callback ( struct erase_info * instr ) <nl> printk ( KERN_INFO args ); \ <nl> } while ( 0 ) <nl> # else /* CONFIG_MTD_DEBUG */ <nl> -# define DEBUG ( n , args ...) do { } while ( 0 ) <nl> +# define DEBUG ( n , args ...) \ <nl> + do { \ <nl> + if ( 0 ) \ <nl> + printk ( KERN_INFO args ); \ <nl> + } while ( 0 ) <nl>  <nl> # endif /* CONFIG_MTD_DEBUG */ <nl> 
megasas_sysfs_set_dbg_lvl ( struct device_driver * dd , const char * buf , size_t coun <nl> return retval ; <nl> } <nl>  <nl> - static DRIVER_ATTR ( dbg_lvl , S_IRUGO | S_IWUGO , megasas_sysfs_show_dbg_lvl , <nl> + static DRIVER_ATTR ( dbg_lvl , S_IRUGO | S_IWUSR , megasas_sysfs_show_dbg_lvl , <nl> megasas_sysfs_set_dbg_lvl ); <nl>  <nl> static ssize_t
ccp_run_sha_cmd ( struct ccp_cmd_queue * cmd_q , struct ccp_cmd * cmd ) <nl> LSB_ITEM_SIZE ); <nl> break ; <nl> default : <nl> + kfree ( hmac_buf ); <nl> ret = - EINVAL ; <nl> - goto e_ctx ; <nl> + goto e_data ; <nl> } <nl>  <nl> memset (& hmac_cmd , 0 , sizeof ( hmac_cmd ));
static u16 xhci_calculate_lpm_timeout ( struct usb_hcd * hcd , <nl> if (! config ) <nl> return timeout ; <nl>  <nl> - for ( i = 0 ; i < USB_MAXINTERFACES ; i ++) { <nl> + for ( i = 0 ; i < config -> desc . bNumInterfaces ; i ++) { <nl> struct usb_driver * driver ; <nl> struct usb_interface * intf = config -> interface [ i ]; <nl> 
static int spi_gpio_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl>  <nl> status = devm_add_action_or_reset (& pdev -> dev , spi_gpio_put , master ); <nl> - if ( status ) <nl> + if ( status ) { <nl> + spi_master_put ( master ); <nl> return status ; <nl> + } <nl>  <nl> if ( of_id ) <nl> status = spi_gpio_probe_dt ( pdev , master );
int i915_gem_init ( struct drm_device * dev ) <nl> i915_gem_init_global_gtt ( dev ); <nl>  <nl> ret = i915_gem_context_init ( dev ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return ret ; <nl> + } <nl>  <nl> ret = i915_gem_init_hw ( dev ); <nl> mutex_unlock (& dev -> struct_mutex );
static int __init con3215_init ( void ) <nl> spin_lock_init (& raw3215_freelist_lock ); <nl> for ( i = 0 ; i < NR_3215_REQ ; i ++) { <nl> req = kzalloc ( sizeof ( struct raw3215_req ), GFP_KERNEL | GFP_DMA ); <nl> + if (! req ) <nl> + return - ENOMEM ; <nl> req -> next = raw3215_freelist ; <nl> raw3215_freelist = req ; <nl> }
int class_process_proc_param ( char * prefix , struct lprocfs_vars * lvars , <nl>  <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> - rc = var -> fops -> write (& fakefile , sval , <nl> + rc = var -> fops -> write (& fakefile , <nl> + ( const char __user *) sval , <nl> vallen , NULL ); <nl> set_fs ( oldfs ); <nl> }
int snd_hda_queue_unsol_event ( struct hda_bus * bus , u32 res , u32 res_ex ) <nl> struct hda_bus_unsolicited * unsol ; <nl> unsigned int wp ; <nl>  <nl> + if (! bus || ! bus -> workq ) <nl> + return 0 ; <nl> + <nl> trace_hda_unsol_event ( bus , res , res_ex ); <nl> unsol = bus -> unsol ; <nl> if (! unsol )
asmlinkage long sys_oabi_semtimedop ( int semid , <nl> long err ; <nl> int i ; <nl>  <nl> - if ( nsops < 1 ) <nl> + if ( nsops < 1 || nsops > SEMOPM ) <nl> return - EINVAL ; <nl> sops = kmalloc ( sizeof (* sops ) * nsops , GFP_KERNEL ); <nl> if (! sops )
static void assert_can_enable_dc9 ( struct drm_i915_private * dev_priv ) <nl> " DC9 already programmed to be enabled .\ n "); <nl> WARN_ONCE ( I915_READ ( DC_STATE_EN ) & DC_STATE_EN_UPTO_DC5 , <nl> " DC5 still not disabled to enable DC9 .\ n "); <nl> - WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ), " Power well on .\ n "); <nl> + WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ) & <nl> + SKL_POWER_WELL_REQ ( SKL_DISP_PW_2 ), <nl> + " Power well 2 on .\ n "); <nl> WARN_ONCE ( intel_irqs_enabled ( dev_priv ), <nl> " Interrupts not disabled yet .\ n "); <nl> 
static int ftrace_function_set_regexp ( struct ftrace_ops * ops , int filter , <nl> static int __ftrace_function_set_filter ( int filter , char * buf , int len , <nl> struct function_filter_data * data ) <nl> { <nl> - int i , re_cnt , ret ; <nl> + int i , re_cnt , ret = - EINVAL ; <nl> int * reset ; <nl> char ** re ; <nl> 
void drm_sysfs_destroy ( void ) <nl> */ <nl> static void drm_sysfs_device_release ( struct device * dev ) <nl> { <nl> + memset ( dev , 0 , sizeof ( struct device )); <nl> return ; <nl> } <nl> 
static ssize_t max_threshold_occ_write ( struct kernfs_open_file * of , <nl>  <nl> intel_cqm_threshold = bytes / r -> mon_scale ; <nl>  <nl> - return ret ?: nbytes ; <nl> + return nbytes ; <nl> } <nl>  <nl> /* rdtgroup information files for one cache resource . */
static struct sysfs_dirent * sysfs_new_dirent ( struct sysfs_dirent * parent_sd , <nl>  <nl> memset ( sd , 0 , sizeof (* sd )); <nl> atomic_set (& sd -> s_count , 1 ); <nl> - atomic_set (& sd -> s_event , 0 ); <nl> + atomic_set (& sd -> s_event , 1 ); <nl> INIT_LIST_HEAD (& sd -> s_children ); <nl> list_add (& sd -> s_sibling , & parent_sd -> s_children ); <nl> sd -> s_element = element ;
static int dgap_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> { <nl> int rc ; <nl>  <nl> + if ( dgap_NumBoards >= MAXBOARDS ) <nl> + return - EPERM ; <nl> + <nl> /* wake up and enable device */ <nl> rc = pci_enable_device ( pdev ); <nl> 
int arm_pmu_acpi_probe ( armpmu_init_fn init_fn ) <nl> ret = armpmu_register ( pmu ); <nl> if ( ret ) { <nl> pr_warn (" Failed to register PMU for CPU % d \ n ", cpu ); <nl> + kfree ( pmu -> name ); <nl> return ret ; <nl> } <nl> }
static int sfb_dump ( struct Qdisc * sch , struct sk_buff * skb ) <nl>  <nl> sch -> qstats . backlog = q -> qdisc -> qstats . backlog ; <nl> opts = nla_nest_start ( skb , TCA_OPTIONS ); <nl> + if ( opts == NULL ) <nl> + goto nla_put_failure ; <nl> if ( nla_put ( skb , TCA_SFB_PARMS , sizeof ( opt ), & opt )) <nl> goto nla_put_failure ; <nl> return nla_nest_end ( skb , opts );
static int skl_manifest_load ( struct snd_soc_component * cmpnt , <nl> struct skl * skl = ebus_to_skl ( ebus ); <nl> int ret = 0 ; <nl>  <nl> + /* proceed only if we have private data defined */ <nl> + if ( manifest -> priv . size == 0 ) <nl> + return 0 ; <nl> + <nl> minfo = & skl -> skl_sst -> manifest ; <nl>  <nl> skl_tplg_get_manifest_data ( manifest , bus -> dev , minfo );
static void s6000_pcm_enqueue_dma ( struct snd_pcm_substream * substream ) <nl> return ; <nl> } <nl>  <nl> - BUG_ON ( period_size & 15 ); <nl> + if ( WARN_ON ( period_size & 15 )) <nl> + return ; <nl> s6dmac_put_fifo ( DMA_MASK_DMAC ( channel ), DMA_INDEX_CHNL ( channel ), <nl> src , dst , period_size ); <nl> 
static void stv0900_set_search_standard ( struct stv0900_internal * intp , <nl> break ; <nl> case STV0900_SEARCH_DSS : <nl> dprintk (" Search Standard = DSS \ n "); <nl> - case STV0900_SEARCH_DVBS2 : <nl> break ; <nl> + case STV0900_SEARCH_DVBS2 : <nl> dprintk (" Search Standard = DVBS2 \ n "); <nl> + break ; <nl> case STV0900_AUTO_SEARCH : <nl> default : <nl> dprintk (" Search Standard = AUTO \ n ");
void rt2x00lib_txdone ( struct queue_entry * entry , <nl> * Update TX statistics . <nl> */ <nl> rt2x00dev -> link . qual . tx_success += success ; <nl> - rt2x00dev -> link . qual . tx_failed += txdesc -> retry + fail ; <nl> + rt2x00dev -> link . qual . tx_failed += fail ; <nl>  <nl> /* <nl> * Initialize TX status
static void ar5008_hw_init_bb ( struct ath_hw * ah , <nl> else <nl> synthDelay /= 10 ; <nl>  <nl> + if ( IS_CHAN_HALF_RATE ( chan )) <nl> + synthDelay *= 2 ; <nl> + else if ( IS_CHAN_QUARTER_RATE ( chan )) <nl> + synthDelay *= 4 ; <nl> + <nl> REG_WRITE ( ah , AR_PHY_ACTIVE , AR_PHY_ACTIVE_EN ); <nl>  <nl> udelay ( synthDelay + BASE_ACTIVATE_DELAY );
void __aa_fs_profile_migrate_dents ( struct aa_profile * old , <nl>  <nl> for ( i = 0 ; i < AAFS_PROF_SIZEOF ; i ++) { <nl> new -> dents [ i ] = old -> dents [ i ]; <nl> + if ( new -> dents [ i ]) <nl> + new -> dents [ i ]-> d_inode -> i_mtime = CURRENT_TIME ; <nl> old -> dents [ i ] = NULL ; <nl> } <nl> }
static int xhci_mtk_probe ( struct platform_device * pdev ) <nl> goto disable_ldos ; <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if ( irq < 0 ) <nl> + if ( irq < 0 ) { <nl> + ret = irq ; <nl> goto disable_clk ; <nl> + } <nl>  <nl> /* Initialize dma_mask and coherent_dma_mask to 32 - bits */ <nl> ret = dma_set_coherent_mask ( dev , DMA_BIT_MASK ( 32 ));
static int perl_start_script ( const char * script , int argc , const char ** argv ) <nl> goto error ; <nl> } <nl>  <nl> - perl_run ( my_perl ); <nl> + if ( perl_run ( my_perl )) { <nl> + err = - 1 ; <nl> + goto error ; <nl> + } <nl> + <nl> if ( SvTRUE ( ERRSV )) { <nl> err = - 1 ; <nl> goto error ;
static int qlcnic_82xx_setup_intr ( struct qlcnic_adapter * adapter ) <nl> qlcnic_disable_multi_tx ( adapter ); <nl>  <nl> err = qlcnic_enable_msi_legacy ( adapter ); <nl> - if (! err ) <nl> + if ( err ) <nl> return err ; <nl> } <nl> }
static int pvc_getname ( struct socket * sock , struct sockaddr * sockaddr , <nl> return - ENOTCONN ; <nl> * sockaddr_len = sizeof ( struct sockaddr_atmpvc ); <nl> addr = ( struct sockaddr_atmpvc *) sockaddr ; <nl> + memset ( addr , 0 , sizeof (* addr )); <nl> addr -> sap_family = AF_ATMPVC ; <nl> addr -> sap_addr . itf = vcc -> dev -> number ; <nl> addr -> sap_addr . vpi = vcc -> vpi ;
static void perf_addr_filters_adjust ( struct vm_area_struct * vma ) <nl> struct perf_event_context * ctx ; <nl> int ctxn ; <nl>  <nl> + /* <nl> + * Data tracing isn ' t supported yet and as such there is no need <nl> + * to keep track of anything that isn ' t related to executable code : <nl> + */ <nl> + if (!( vma -> vm_flags & VM_EXEC )) <nl> + return ; <nl> + <nl> rcu_read_lock (); <nl> for_each_task_context_nr ( ctxn ) { <nl> ctx = rcu_dereference ( current -> perf_event_ctxp [ ctxn ]);
static inline void ttwu_post_activation ( struct task_struct * p , struct rq * rq , <nl> if ( p -> sched_class -> task_woken ) <nl> p -> sched_class -> task_woken ( rq , p ); <nl>  <nl> - if ( unlikely ( rq -> idle_stamp )) { <nl> + if ( rq -> idle_stamp ) { <nl> u64 delta = rq -> clock - rq -> idle_stamp ; <nl> u64 max = 2 * sysctl_sched_migration_cost ; <nl> 
int trace_get_user ( struct trace_parser * parser , const char __user * ubuf , <nl>  <nl> /* read the non - space input */ <nl> while ( cnt && ! isspace ( ch )) { <nl> - if ( parser -> idx < parser -> size ) <nl> + if ( parser -> idx < parser -> size - 1 ) <nl> parser -> buffer [ parser -> idx ++] = ch ; <nl> else { <nl> ret = - EINVAL ;
static int proc_pid_permission ( struct inode * inode , int mask ) <nl> bool has_perms ; <nl>  <nl> task = get_proc_task ( inode ); <nl> + if (! task ) <nl> + return - ESRCH ; <nl> has_perms = has_pid_permissions ( pid , task , 1 ); <nl> put_task_struct ( task ); <nl> 
static __inline__ void atomic64_set_mask ( unsigned long mask , atomic64_t * v ) <nl> __CSG_LOOP ( v , mask , " ogr "); <nl> } <nl>  <nl> +# define atomic64_xchg ( v , new ) ( xchg (&(( v )-> counter ), new )) <nl> + <nl> static __inline__ long long atomic64_cmpxchg ( atomic64_t * v , <nl> long long old , long long new ) <nl> {
static int __devinit sta2x11_mfd_probe ( struct pci_dev * pdev , <nl> sta2x11_mfd_setup ( pdev , setup_data ); <nl>  <nl> /* Record this pdev before mfd_add_devices : their probe looks for it */ <nl> - sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl> + if (! sta2x11_mfd_find ( pdev )) <nl> + sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl>  <nl> /* Just 2 bars for all mfd ' s at present */ <nl> for ( i = 0 ; i < 2 ; i ++) {
static void vnt_bss_info_changed ( struct ieee80211_hw * hw , <nl>  <nl> priv -> current_aid = conf -> aid ; <nl>  <nl> - if ( changed & BSS_CHANGED_BSSID ) <nl> + if ( changed & BSS_CHANGED_BSSID && conf -> bssid ) <nl> vnt_mac_set_bssid_addr ( priv , ( u8 *) conf -> bssid ); <nl>  <nl> 
static void __init early_identify_cpu ( struct cpuinfo_x86 * c ) <nl>  <nl> setup_force_cpu_cap ( X86_FEATURE_ALWAYS ); <nl>  <nl> - /* Assume for now that ALL x86 CPUs are insecure */ <nl> - setup_force_cpu_bug ( X86_BUG_CPU_INSECURE ); <nl> + if ( c -> x86_vendor != X86_VENDOR_AMD ) <nl> + setup_force_cpu_bug ( X86_BUG_CPU_INSECURE ); <nl>  <nl> fpu__init_system ( c ); <nl> 
SYSCALL_DEFINE1 ( inotify_init1 , int , flags ) <nl> if ( ret >= 0 ) <nl> return ret ; <nl>  <nl> + fsnotify_put_group ( group ); <nl> atomic_dec (& user -> inotify_devs ); <nl> out_free_uid : <nl> free_uid ( user );
static int ethtool_get_eeprom ( struct net_device * dev , void __user * useraddr ) <nl> bytes_remaining -= eeprom . len ; <nl> } <nl>  <nl> + eeprom . len = userbuf - ( useraddr + sizeof ( eeprom )); <nl> + eeprom . offset -= eeprom . len ; <nl> + if ( copy_to_user ( useraddr , & eeprom , sizeof ( eeprom ))) <nl> + ret = - EFAULT ; <nl> + <nl> kfree ( data ); <nl> return ret ; <nl> }
static int pci_call_probe ( struct pci_driver * drv , struct pci_dev * dev , <nl> if ( node >= 0 && node != numa_node_id ()) { <nl> int cpu ; <nl>  <nl> - get_online_cpus (); <nl> + cpu_hotplug_disable (); <nl> cpu = cpumask_any_and ( cpumask_of_node ( node ), cpu_online_mask ); <nl> if ( cpu < nr_cpu_ids ) <nl> error = work_on_cpu ( cpu , local_pci_probe , & ddi ); <nl> else <nl> error = local_pci_probe (& ddi ); <nl> - put_online_cpus (); <nl> + cpu_hotplug_enable (); <nl> } else <nl> error = local_pci_probe (& ddi ); <nl> 
int vb2_mmap ( struct vb2_queue * q , struct vm_area_struct * vma ) <nl> { <nl> unsigned long off = vma -> vm_pgoff << PAGE_SHIFT ; <nl> struct vb2_buffer * vb ; <nl> - unsigned int buffer , plane ; <nl> + unsigned int buffer = 0 , plane = 0 ; <nl> int ret ; <nl> unsigned long length ; <nl> 
static const struct file_operations __fops = { \ <nl> . release = simple_attr_release , \ <nl> . read = debugfs_attr_read , \ <nl> . write = debugfs_attr_write , \ <nl> - . llseek = generic_file_llseek , \ <nl> + . llseek = no_llseek , \ <nl> } <nl>  <nl> # if defined ( CONFIG_DEBUG_FS )
static void snd_als4000_configure ( struct snd_sb * chip ) <nl> /* SPECS_PAGE : 39 */ <nl> for ( i = ALS4K_GCR91_DMA0_ADDR ; i <= ALS4K_GCR96_DMA3_MODE_COUNT ; ++ i ) <nl> snd_als4k_gcr_write ( chip , i , 0 ); <nl> - <nl> + /* enable burst mode to prevent dropouts during high PCI bus usage */ <nl> snd_als4k_gcr_write ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL , <nl> - snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL )); <nl> + snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL ) | 0x04 ); <nl> spin_unlock_irq (& chip -> reg_lock ); <nl> } <nl> 
static int sr_init_command ( struct scsi_cmnd * SCpnt ) <nl> static int sr_block_open ( struct inode * inode , struct file * file ) <nl> { <nl> struct gendisk * disk = inode -> i_bdev -> bd_disk ; <nl> - struct scsi_cd * cd = scsi_cd ( inode -> i_bdev -> bd_disk ); <nl> + struct scsi_cd * cd ; <nl> int ret = 0 ; <nl>  <nl> if (!( cd = scsi_cd_get ( disk )))
void ip_tunnel_xmit ( struct sk_buff * skb , struct net_device * dev , <nl> tunnel -> err_time + IPTUNNEL_ERR_TIMEO )) { <nl> tunnel -> err_count --; <nl>  <nl> + memset ( IPCB ( skb ), 0 , sizeof (* IPCB ( skb ))); <nl> dst_link_failure ( skb ); <nl> } else <nl> tunnel -> err_count = 0 ;
int test_range_bit ( struct extent_io_tree * tree , u64 start , u64 end , <nl> bitset = 0 ; <nl> break ; <nl> } <nl> + <nl> + if ( state -> end == ( u64 )- 1 ) <nl> + break ; <nl> + <nl> start = state -> end + 1 ; <nl> if ( start > end ) <nl> break ;
__build_packet_message ( struct nfulnl_instance * inst , <nl> } <nl>  <nl> nlh -> nlmsg_len = inst -> skb -> tail - old_tail ; <nl> + inst -> lastnlh = nlh ; <nl> return 0 ; <nl>  <nl> nlmsg_failure :
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
int evm_update_evmxattr ( struct dentry * dentry , const char * xattr_name , <nl> rc = __vfs_setxattr_noperm ( dentry , XATTR_NAME_EVM , <nl> & xattr_data , <nl> sizeof ( xattr_data ), 0 ); <nl> - } <nl> - else if ( rc == - ENODATA ) <nl> + } else if ( rc == - ENODATA && inode -> i_op -> removexattr ) { <nl> rc = inode -> i_op -> removexattr ( dentry , XATTR_NAME_EVM ); <nl> + } <nl> return rc ; <nl> } <nl> 
static int __devexit wm8753_spi_remove ( struct spi_device * spi ) <nl>  <nl> snd_soc_unregister_codec (& spi -> dev ); <nl> regmap_exit ( wm8753 -> regmap ); <nl> - kfree ( wm8753 ); <nl> return 0 ; <nl> } <nl> 
int vmw_du_crtc_cursor_set ( struct drm_crtc * crtc , struct drm_file * file_priv , <nl> if (! ret ) { <nl> if (! surface -> snooper . image ) { <nl> DRM_ERROR (" surface not suitable for cursor \ n "); <nl> + vmw_surface_unreference (& surface ); <nl> return - EINVAL ; <nl> } <nl> } else {
int sst_hsw_stream_get_volume ( struct sst_hsw * hsw , struct sst_hsw_stream * stream <nl> return - EINVAL ; <nl>  <nl> sst_dsp_read ( hsw -> dsp , volume , <nl> - stream -> reply . volume_register_address [ channel ], sizeof ( volume )); <nl> + stream -> reply . volume_register_address [ channel ], <nl> + sizeof (* volume )); <nl>  <nl> return 0 ; <nl> }
static int __devinit mwl8k_probe ( struct pci_dev * pdev , <nl> if ( rc ) { <nl> printk ( KERN_ERR "% s : Cannot register device \ n ", <nl> wiphy_name ( hw -> wiphy )); <nl> - goto err_free_irq ; <nl> + goto err_free_queues ; <nl> } <nl>  <nl> printk ( KERN_INFO "% s : % s v % d , % pM , % s firmware % u .% u .% u .% u \ n ",
int bxt_sst_dsp_init ( struct device * dev , void __iomem * mmio_base , int irq , <nl> sst_dsp_mailbox_init ( sst , ( BXT_ADSP_SRAM0_BASE + SKL_ADSP_W0_STAT_SZ ), <nl> SKL_ADSP_W0_UP_SZ , BXT_ADSP_SRAM1_BASE , SKL_ADSP_W1_SZ ); <nl>  <nl> + INIT_LIST_HEAD (& sst -> module_list ); <nl> ret = skl_ipc_init ( dev , skl ); <nl> if ( ret ) <nl> return ret ;
static inline bool kvm_apic_vid_enabled ( struct kvm * kvm ) <nl>  <nl> static inline bool kvm_apic_has_events ( struct kvm_vcpu * vcpu ) <nl> { <nl> - return vcpu -> arch . apic -> pending_events ; <nl> + return kvm_vcpu_has_lapic ( vcpu ) && vcpu -> arch . apic -> pending_events ; <nl> } <nl>  <nl> static inline bool kvm_lowest_prio_delivery ( struct kvm_lapic_irq * irq )
int uwb_rsv_find_best_allocation ( struct uwb_rsv * rsv , struct uwb_mas_bm * availab <nl> int bit_index ; <nl>  <nl> ai = kzalloc ( sizeof ( struct uwb_rsv_alloc_info ), GFP_KERNEL ); <nl> - <nl> + if (! ai ) <nl> + return UWB_RSV_ALLOC_NOT_FOUND ; <nl> ai -> min_mas = rsv -> min_mas ; <nl> ai -> max_mas = rsv -> max_mas ; <nl> ai -> max_interval = rsv -> max_interval ;
int xfrm_policy_flush ( u8 type , struct xfrm_audit * audit_info ) <nl> continue ; <nl> hlist_del (& pol -> bydst ); <nl> hlist_del (& pol -> byidx ); <nl> + list_del (& pol -> walk . all ); <nl> write_unlock_bh (& xfrm_policy_lock ); <nl>  <nl> xfrm_audit_policy_delete ( pol , 1 , audit_info -> loginuid ,
static void atmel_aes_get_cap ( struct atmel_aes_dev * dd ) <nl>  <nl> /* keep only major version number */ <nl> switch ( dd -> hw_version & 0xff0 ) { <nl> + case 0x200 : <nl> + dd -> caps . has_dualbuff = 1 ; <nl> + dd -> caps . has_cfb64 = 1 ; <nl> + dd -> caps . max_burst_size = 4 ; <nl> + break ; <nl> case 0x130 : <nl> dd -> caps . has_dualbuff = 1 ; <nl> dd -> caps . has_cfb64 = 1 ;
static ssize_t hiddev_read ( struct file * file , char __user * buffer , size_t coun <nl> } <nl>  <nl> schedule (); <nl> + set_current_state ( TASK_INTERRUPTIBLE ); <nl> } <nl>  <nl> set_current_state ( TASK_RUNNING );
void brcmf_fweh_process_event ( struct brcmf_pub * drvr , <nl> if ( code != BRCMF_E_IF && ! fweh -> evt_handler [ code ]) <nl> return ; <nl>  <nl> - if ( datalen > BRCMF_DCMD_MAXLEN ) <nl> + if ( datalen > BRCMF_DCMD_MAXLEN || <nl> + datalen + sizeof (* event_packet ) > packet_len ) <nl> return ; <nl>  <nl> if ( in_interrupt ())
int rhashtable_walk_start_check ( struct rhashtable_iter * iter ) <nl> skip ++; <nl> if ( list == iter -> list ) { <nl> iter -> p = p ; <nl> - skip = skip ; <nl> + iter -> skip = skip ; <nl> goto found ; <nl> } <nl> }
int skl_bind_modules ( struct skl_sst * ctx , <nl>  <nl> skl_dump_bind_info ( ctx , src_mcfg , dst_mcfg ); <nl>  <nl> - if ( src_mcfg -> m_state < SKL_MODULE_INIT_DONE && <nl> + if ( src_mcfg -> m_state < SKL_MODULE_INIT_DONE || <nl> dst_mcfg -> m_state < SKL_MODULE_INIT_DONE ) <nl> return 0 ; <nl> 
static int s5p64x0_alloc_gc ( void ) <nl> } <nl>  <nl> ct = gc -> chip_types ; <nl> - ct -> chip . irq_ack = irq_gc_ack ; <nl> + ct -> chip . irq_ack = irq_gc_ack_set_bit ; <nl> ct -> chip . irq_mask = irq_gc_mask_set_bit ; <nl> ct -> chip . irq_unmask = irq_gc_mask_clr_bit ; <nl> ct -> chip . irq_set_type = s5p64x0_irq_eint_set_type ;
static void channel_swdemux_tsklet ( unsigned long data ) <nl> writel ( channel -> back_buffer_busaddr , channel -> irec + <nl> DMA_PRDS_BUSRP_TP ( 0 )); <nl> else <nl> - writel ( wp , channel -> irec + DMA_PRDS_BUSWP_TP ( 0 )); <nl> + writel ( wp , channel -> irec + DMA_PRDS_BUSRP_TP ( 0 )); <nl> } <nl>  <nl> static int c8sectpfe_start_feed ( struct dvb_demux_feed * dvbdmxfeed )
struct mmc_host_ops { <nl>  <nl> int (* start_signal_voltage_switch )( struct mmc_host * host , struct mmc_ios * ios ); <nl>  <nl> + /* Check if the card is pulling dat [ 0 : 3 ] low */ <nl> + int (* card_busy )( struct mmc_host * host ); <nl> + <nl> /* The tuning command opcode value is different for SD and eMMC cards */ <nl> int (* execute_tuning )( struct mmc_host * host , u32 opcode ); <nl> void (* enable_preset_value )( struct mmc_host * host , bool enable );
static int wl1271_op_hw_scan ( struct ieee80211_hw * hw , <nl> static int wl1271_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> { <nl> struct wl1271 * wl = hw -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> mutex_lock (& wl -> mutex ); <nl>  <nl> + if ( unlikely ( wl -> state == WL1271_STATE_OFF )) <nl> + goto out ; <nl> + <nl> ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> if ( ret < 0 ) <nl> goto out ;
static void orinoco_process_scan_results ( struct work_struct * work ) <nl>  <nl> spin_lock_irqsave (& priv -> scan_lock , flags ); <nl> list_for_each_entry_safe ( sd , temp , & priv -> scan_list , list ) { <nl> - spin_unlock_irqrestore (& priv -> scan_lock , flags ); <nl>  <nl> buf = sd -> buf ; <nl> len = sd -> len ; <nl> type = sd -> type ; <nl>  <nl> list_del (& sd -> list ); <nl> + spin_unlock_irqrestore (& priv -> scan_lock , flags ); <nl> kfree ( sd ); <nl>  <nl> if ( len > 0 ) {
static int overlay_set_addr ( struct mmp_overlay * overlay , struct mmp_addr * addr ) <nl> struct lcd_regs * regs = path_regs ( overlay -> path ); <nl>  <nl> /* FIXME : assert addr supported */ <nl> - memcpy (& overlay -> addr , addr , sizeof ( struct mmp_win )); <nl> + memcpy (& overlay -> addr , addr , sizeof ( struct mmp_addr )); <nl> writel ( addr -> phys [ 0 ], & regs -> g_0 ); <nl>  <nl> return overlay -> addr . phys [ 0 ];
static int adxrs450_read_raw ( struct iio_dev * indio_dev , <nl> * val = t ; <nl> ret = IIO_VAL_INT ; <nl> break ; <nl> + case IIO_CHAN_INFO_CALIBBIAS : <nl> + ret = adxrs450_spi_read_reg_16 ( indio_dev , ADXRS450_DNC1 , & t ); <nl> + if ( ret ) <nl> + break ; <nl> + * val = t ; <nl> + ret = IIO_VAL_INT ; <nl> + break ; <nl> default : <nl> ret = - EINVAL ; <nl> break ;
const u32_t zcFwImage [] = { <nl> 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 , <nl> }; <nl>  <nl> - const u32_t zcFwImageSize = 15936 ; <nl> + const u32_t zcFwImageSize = 15936 ;
static s32 Handle_Get_InActiveTime ( struct wilc_vif * vif , <nl> wid . type = WID_STR ; <nl> wid . size = ETH_ALEN ; <nl> wid . val = kmalloc ( wid . size , GFP_KERNEL ); <nl> + if (! wid . val ) <nl> + return - ENOMEM ; <nl>  <nl> stamac = wid . val ; <nl> ether_addr_copy ( stamac , strHostIfStaInactiveT -> mac );
static struct fb_ops macfb_ops = { <nl> . fb_imageblit = cfb_imageblit , <nl> }; <nl>  <nl> - void __init macfb_setup ( char * options ) <nl> + static void __init macfb_setup ( char * options ) <nl> { <nl> char * this_opt ; <nl> 
static void sas_discover_domain ( struct work_struct * work ) <nl> case FANOUT_DEV : <nl> error = sas_discover_root_expander ( dev ); <nl> break ; <nl> -# ifdef CONFIG_SCSI_SAS_ATA <nl> case SATA_DEV : <nl> case SATA_PM : <nl> +# ifdef CONFIG_SCSI_SAS_ATA <nl> error = sas_discover_sata ( dev ); <nl> break ; <nl> +# else <nl> + SAS_DPRINTK (" ATA device seen but CONFIG_SCSI_SAS_ATA = N so cannot attach \ n "); <nl> + /* Fall through */ <nl> # endif <nl> default : <nl> error = - ENXIO ;
int debug_log ( struct bat_priv * bat_priv , char * fmt , ...) <nl>  <nl> va_start ( args , fmt ); <nl> vscnprintf ( tmp_log_buf , sizeof ( tmp_log_buf ), fmt , args ); <nl> - fdebug_log ( bat_priv -> debug_log , "[% 10u ] % s ", <nl> + fdebug_log ( bat_priv -> debug_log , "[% 10lu ] % s ", <nl> ( jiffies / HZ ), tmp_log_buf ); <nl> va_end ( args ); <nl> 
static int __sock_diag_rcv_msg ( struct sk_buff * skb , struct nlmsghdr * nlh ) <nl> if ( nlmsg_len ( nlh ) < sizeof (* req )) <nl> return - EINVAL ; <nl>  <nl> + if ( req -> sdiag_family >= AF_MAX ) <nl> + return - EINVAL ; <nl> + <nl> hndl = sock_diag_lock_handler ( req -> sdiag_family ); <nl> if ( hndl == NULL ) <nl> err = - ENOENT ;
static int tipc_nl_compat_link_dump ( struct tipc_nl_compat_msg * msg , <nl>  <nl> link_info . dest = nla_get_flag ( link [ TIPC_NLA_LINK_DEST ]); <nl> link_info . up = htonl ( nla_get_flag ( link [ TIPC_NLA_LINK_UP ])); <nl> - strcpy ( link_info . str , nla_data ( link [ TIPC_NLA_LINK_NAME ])); <nl> + nla_strlcpy ( link_info . str , nla_data ( link [ TIPC_NLA_LINK_NAME ]), <nl> + TIPC_MAX_LINK_NAME ); <nl>  <nl> return tipc_add_tlv ( msg -> rep , TIPC_TLV_LINK_INFO , <nl> & link_info , sizeof ( link_info ));
void __init setup_ioapic_dest ( void ) <nl> mask = apic -> target_cpus (); <nl>  <nl> chip = irq_data_get_irq_chip ( idata ); <nl> - chip -> irq_set_affinity ( idata , mask , false ); <nl> + /* Might be lapic_chip for irq 0 */ <nl> + if ( chip -> irq_set_affinity ) <nl> + chip -> irq_set_affinity ( idata , mask , false ); <nl> } <nl> } <nl> # endif
int asn1_ber_decoder ( const struct asn1_decoder * decoder , <nl> if ( unlikely ( len > datalen - dp )) <nl> goto data_overrun_error ; <nl> } <nl> + } else { <nl> + if ( unlikely ( len > datalen - dp )) <nl> + goto data_overrun_error ; <nl> } <nl>  <nl> if ( flags & FLAG_CONS ) {
scsi_reset_provider ( struct scsi_device * dev , int flag ) <nl> rtn = FAILED ; <nl> } <nl>  <nl> - scsi_delete_timer ( scmd ); <nl> scsi_next_command ( scmd ); <nl> return rtn ; <nl> }
static int mxs_lradc_ts_register ( struct mxs_lradc * lradc ) <nl> __set_bit ( EV_ABS , input -> evbit ); <nl> __set_bit ( EV_KEY , input -> evbit ); <nl> __set_bit ( BTN_TOUCH , input -> keybit ); <nl> + __set_bit ( INPUT_PROP_DIRECT , input -> propbit ); <nl> input_set_abs_params ( input , ABS_X , 0 , LRADC_SINGLE_SAMPLE_MASK , 0 , 0 ); <nl> input_set_abs_params ( input , ABS_Y , 0 , LRADC_SINGLE_SAMPLE_MASK , 0 , 0 ); <nl> input_set_abs_params ( input , ABS_PRESSURE , 0 , LRADC_SINGLE_SAMPLE_MASK ,
cifs_setlk ( struct file * file , struct file_lock * flock , __u32 type , <nl> rc = server -> ops -> mand_unlock_range ( cfile , flock , xid ); <nl>  <nl> out : <nl> - if ( flock -> fl_flags & FL_POSIX ) <nl> - posix_lock_file_wait ( file , flock ); <nl> + if ( flock -> fl_flags & FL_POSIX && ! rc ) <nl> + rc = posix_lock_file_wait ( file , flock ); <nl> return rc ; <nl> } <nl> 
static int fnic_fcpio_fw_reset_cmpl_handler ( struct fnic * fnic , <nl>  <nl> atomic64_set (& fnic -> fnic_stats . fw_stats . active_fw_reqs , 0 ); <nl> atomic64_set (& fnic -> fnic_stats . io_stats . active_ios , 0 ); <nl> + atomic64_set (& fnic -> io_cmpl_skip , 0 ); <nl>  <nl> spin_lock_irqsave (& fnic -> fnic_lock , flags ); <nl> 
static struct rpmsg_device * rpmsg_virtio_add_ctrl_dev ( struct virtio_device * vdev <nl>  <nl> err = rpmsg_ctrldev_register_device ( rpdev_ctrl ); <nl> if ( err ) { <nl> - kfree ( vch ); <nl> + /* vch will be free in virtio_rpmsg_release_device () */ <nl> return ERR_PTR ( err ); <nl> } <nl> 
struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
static int nodemgr_host_thread ( void * data ) <nl> g = get_hpsb_generation ( host ); <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> msleep_interruptible ( 63 ); <nl> + try_to_freeze (); <nl> if ( kthread_should_stop ()) <nl> goto exit ; <nl>  <nl> static int nodemgr_host_thread ( void * data ) <nl> /* Sleep 3 seconds */ <nl> for ( i = 3000 / 200 ; i ; i --) { <nl> msleep_interruptible ( 200 ); <nl> + try_to_freeze (); <nl> if ( kthread_should_stop ()) <nl> goto exit ; <nl> 
static int crypt_alloc_tfms ( struct crypt_config * cc , char * ciphermode ) <nl> unsigned i ; <nl> int err ; <nl>  <nl> - cc -> tfms = kmalloc ( cc -> tfms_count * sizeof ( struct crypto_skcipher *), <nl> + cc -> tfms = kzalloc ( cc -> tfms_count * sizeof ( struct crypto_skcipher *), <nl> GFP_KERNEL ); <nl> if (! cc -> tfms ) <nl> return - ENOMEM ;
static void ixgbe_clean_rx_irq ( struct ixgbe_q_vector * q_vector , <nl> if ( ixgbe_rx_is_fcoe ( adapter , rx_desc )) { <nl> ddp_bytes = ixgbe_fcoe_ddp ( adapter , rx_desc , skb , <nl> staterr ); <nl> - if (! ddp_bytes ) <nl> + if (! ddp_bytes ) { <nl> + dev_kfree_skb_any ( skb ); <nl> goto next_desc ; <nl> + } <nl> } <nl> # endif /* IXGBE_FCOE */ <nl> ixgbe_receive_skb ( q_vector , skb , staterr , rx_ring , rx_desc );
static void intel_i9xx_setup_flush ( void ) <nl> intel_private . ifp_resource . flags = IORESOURCE_MEM ; <nl>  <nl> /* Setup chipset flush for 915 */ <nl> - if ( IS_I965 || IS_G33 ) { <nl> + if ( IS_I965 || IS_G33 || IS_G4X ) { <nl> intel_i965_g33_setup_chipset_flush (); <nl> } else { <nl> intel_i915_setup_chipset_flush ();
static int crypto_authenc_verify ( struct aead_request * req , <nl> unsigned int authsize ; <nl>  <nl> areq_ctx -> complete = authenc_verify_ahash_done ; <nl> - areq_ctx -> complete = authenc_verify_ahash_update_done ; <nl> + areq_ctx -> update_complete = authenc_verify_ahash_update_done ; <nl>  <nl> ohash = authenc_ahash_fn ( req , CRYPTO_TFM_REQ_MAY_SLEEP ); <nl> if ( IS_ERR ( ohash ))
static loff_t fuse_file_llseek ( struct file * file , loff_t offset , int origin ) <nl> mutex_lock (& inode -> i_mutex ); <nl> switch ( origin ) { <nl> case SEEK_END : <nl> + retval = fuse_update_attributes ( inode , NULL , file , NULL ); <nl> + if ( retval ) <nl> + return retval ; <nl> offset += i_size_read ( inode ); <nl> break ; <nl> case SEEK_CUR :
int do_huge_pmd_numa_page ( struct mm_struct * mm , struct vm_area_struct * vma , <nl>  <nl> check_same : <nl> spin_lock (& mm -> page_table_lock ); <nl> - if ( unlikely (! pmd_same ( pmd , * pmdp ))) <nl> + if ( unlikely (! pmd_same ( pmd , * pmdp ))) { <nl> + /* Someone else took our fault */ <nl> + current_nid = - 1 ; <nl> goto out_unlock ; <nl> + } <nl> clear_pmdnuma : <nl> pmd = pmd_mknonnuma ( pmd ); <nl> set_pmd_at ( mm , haddr , pmdp , pmd );
int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
static inline void debugfs_remove_domain_dir ( struct irq_domain * d ) { } <nl> # endif <nl>  <nl> const struct fwnode_operations irqchip_fwnode_ops ; <nl> + EXPORT_SYMBOL_GPL ( irqchip_fwnode_ops ); <nl>  <nl> /** <nl> * irq_domain_alloc_fwnode - Allocate a fwnode_handle suitable for
iscsi_if_rx ( struct sk_buff * skb ) <nl> uint32_t group ; <nl>  <nl> nlh = nlmsg_hdr ( skb ); <nl> - if ( nlh -> nlmsg_len < sizeof (* nlh ) || <nl> + if ( nlh -> nlmsg_len < sizeof (* nlh ) + sizeof (* ev ) || <nl> skb -> len < nlh -> nlmsg_len ) { <nl> break ; <nl> }
static int mv643xx_eth_set_mac_address ( struct net_device * dev , void * addr ) <nl> { <nl> struct sockaddr * sa = addr ; <nl>  <nl> + if (! is_valid_ether_addr ( sa -> sa_data )) <nl> + return - EINVAL ; <nl> + <nl> memcpy ( dev -> dev_addr , sa -> sa_data , ETH_ALEN ); <nl>  <nl> netif_addr_lock_bh ( dev );
static void imx_pinconf_group_dbg_show ( struct pinctrl_dev * pctldev , <nl> const char * name ; <nl> int i , ret ; <nl>  <nl> - if ( group > pctldev -> num_groups ) <nl> + if ( group >= pctldev -> num_groups ) <nl> return ; <nl>  <nl> seq_puts ( s , "\ n ");
static int f2fs_write_data_pages ( struct address_space * mapping , <nl> if (! mapping -> a_ops -> writepage ) <nl> return 0 ; <nl>  <nl> + /* skip writing if there is no dirty page in this inode */ <nl> + if (! get_dirty_pages ( inode ) && wbc -> sync_mode == WB_SYNC_NONE ) <nl> + return 0 ; <nl> + <nl> if ( S_ISDIR ( inode -> i_mode ) && wbc -> sync_mode == WB_SYNC_NONE && <nl> get_dirty_pages ( inode ) < nr_pages_to_skip ( sbi , DATA ) && <nl> available_free_memory ( sbi , DIRTY_DENTS ))
static int __iwl_mvm_suspend ( struct ieee80211_hw * hw , <nl> out : <nl> if ( ret < 0 ) { <nl> iwl_mvm_ref ( mvm , IWL_MVM_REF_UCODE_DOWN ); <nl> - ieee80211_restart_hw ( mvm -> hw ); <nl> + if ( mvm -> restart_fw > 0 ) { <nl> + mvm -> restart_fw --; <nl> + ieee80211_restart_hw ( mvm -> hw ); <nl> + } <nl> iwl_mvm_free_nd ( mvm ); <nl> } <nl> out_noreset :
static int cachefiles_mark_object_active ( struct cachefiles_cache * cache , <nl> /* an old object from a previous incarnation is hogging the slot - we <nl> * need to wait for it to be destroyed */ <nl> wait_for_old_object : <nl> - if ( fscache_object_is_live (& object -> fscache )) { <nl> + if ( fscache_object_is_live (& xobject -> fscache )) { <nl> pr_err ("\ n "); <nl> pr_err (" Error : Unexpected object collision \ n "); <nl> cachefiles_printk_object ( object , xobject );
static int radeon_uvd_cs_reloc ( struct radeon_cs_parser * p , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (( start >> 28 ) != ( end >> 28 )) { <nl> + if (( start >> 28 ) != (( end - 1 ) >> 28 )) { <nl> DRM_ERROR (" reloc % LX -% LX crossing 256MB boundary !\ n ", <nl> start , end ); <nl> return - EINVAL ;
void __init omap_detect_sram ( void ) <nl> if ( cpu_is_omap34xx ()) { <nl> omap_sram_base = OMAP3_SRAM_PUB_VA ; <nl> omap_sram_start = OMAP3_SRAM_PUB_PA ; <nl> - omap_sram_size = 0x8000 ; /* 32K */ <nl> + if (( omap_type () == OMAP2_DEVICE_TYPE_EMU ) || <nl> + ( omap_type () == OMAP2_DEVICE_TYPE_SEC )) { <nl> + omap_sram_size = 0x7000 ; /* 28K */ <nl> + } else { <nl> + omap_sram_size = 0x8000 ; /* 32K */ <nl> + } <nl> } else { <nl> omap_sram_base = OMAP2_SRAM_PUB_VA ; <nl> omap_sram_start = OMAP2_SRAM_PUB_PA ;
static int btrfs_real_readdir ( struct file * filp , void * dirent , <nl>  <nl> /* Reached end of directory / root . Bump pos past the last item . */ <nl> if ( key_type == BTRFS_DIR_INDEX_KEY ) <nl> - filp -> f_pos = INT_LIMIT ( off_t ); <nl> + /* <nl> + * 32 - bit glibc will use getdents64 , but then strtol - <nl> + * so the last number we can serve is this . <nl> + */ <nl> + filp -> f_pos = 0x7fffffff ; <nl> else <nl> filp -> f_pos ++; <nl> nopos :
int snd_soc_register_dais ( struct device * dev , <nl> pr_debug (" Registered DAI '% s '\ n ", dai -> name ); <nl> } <nl>  <nl> + mutex_lock (& client_mutex ); <nl> snd_soc_instantiate_cards (); <nl> + mutex_unlock (& client_mutex ); <nl> return 0 ; <nl>  <nl> err :
static int logi_dj_ll_raw_request ( struct hid_device * hid , <nl> if (! out_buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( count < DJREPORT_SHORT_LENGTH - 2 ) <nl> + if ( count > DJREPORT_SHORT_LENGTH - 2 ) <nl> count = DJREPORT_SHORT_LENGTH - 2 ; <nl>  <nl> out_buf [ 0 ] = REPORT_ID_DJ_SHORT ;
struct el3_private { <nl> spinlock_t lock ; <nl> }; <nl>  <nl> - static const char * if_names [] = { " auto ", " 10baseT ", " 10base2 ", " AUI " }; <nl> + static const char * if_names [] = { " auto ", " 10base2 ", " 10baseT ", " AUI " }; <nl>  <nl> /*====================================================================*/ <nl> 
static void udf_sb_free_partitions ( struct super_block * sb ) <nl> { <nl> struct udf_sb_info * sbi = UDF_SB ( sb ); <nl> int i ; <nl> - <nl> + if ( sbi -> s_partmaps == NULL ) <nl> + return ; <nl> for ( i = 0 ; i < sbi -> s_partitions ; i ++) <nl> udf_free_partition (& sbi -> s_partmaps [ i ]); <nl> kfree ( sbi -> s_partmaps );
int dw_hdmi_probe ( struct platform_device * pdev , <nl> const struct dw_hdmi_plat_data * plat_data ) <nl> { <nl> struct dw_hdmi * hdmi ; <nl> - int ret ; <nl>  <nl> hdmi = __dw_hdmi_probe ( pdev , plat_data ); <nl> if ( IS_ERR ( hdmi )) <nl> return PTR_ERR ( hdmi ); <nl>  <nl> - ret = drm_bridge_add (& hdmi -> bridge ); <nl> - if ( ret < 0 ) { <nl> - __dw_hdmi_remove ( hdmi ); <nl> - return ret ; <nl> - } <nl> + drm_bridge_add (& hdmi -> bridge ); <nl>  <nl> return 0 ; <nl> }
static long gntalloc_ioctl_alloc ( struct gntalloc_file_private_data * priv , <nl> goto out ; <nl> } <nl>  <nl> - gref_ids = kzalloc ( sizeof ( gref_ids [ 0 ]) * op . count , GFP_TEMPORARY ); <nl> + gref_ids = kcalloc ( op . count , sizeof ( gref_ids [ 0 ]), GFP_TEMPORARY ); <nl> if (! gref_ids ) { <nl> rc = - ENOMEM ; <nl> goto out ;
static int __init orion_nand_probe ( struct platform_device * pdev ) <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) { <nl> - err = - ENODEV ; <nl> + ret = - ENODEV ; <nl> goto no_res ; <nl> } <nl> 
static int iucv_sock_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> struct sk_buff * skb , * rskb , * cskb ; <nl> int err = 0 ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> if (( sk -> sk_state == IUCV_DISCONN ) && <nl> skb_queue_empty (& iucv -> backlog_skb_q ) && <nl> skb_queue_empty (& sk -> sk_receive_queue ) &&
static inline int logfs_get_sb_bdev ( struct logfs_super * s , <nl>  <nl> /* dev_mtd . c */ <nl> # ifdef CONFIG_MTD <nl> - int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> + int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ); <nl> # else <nl> static inline int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> {
int kvm_timer_enable ( struct kvm_vcpu * vcpu ) <nl> return ret ; <nl>  <nl> no_vgic : <nl> + preempt_disable (); <nl> timer -> enabled = 1 ; <nl> + kvm_timer_vcpu_load_vgic ( vcpu ); <nl> + preempt_enable (); <nl> + <nl> return 0 ; <nl> } <nl> 
int hfsplus_find_cat ( struct super_block * sb , u32 cnid , <nl> return - EIO ; <nl> } <nl>  <nl> + if ( be16_to_cpu ( tmp . thread . nodeName . length ) > 255 ) { <nl> + printk ( KERN_ERR " hfs : catalog name length corrupted \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> hfsplus_cat_build_key_uni ( fd -> search_key , be32_to_cpu ( tmp . thread . parentID ), <nl> & tmp . thread . nodeName ); <nl> return hfs_brec_find ( fd );
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> goto cleanup ; <nl> } <nl>  <nl> - user_srbcmd = kmalloc ( GFP_KERNEL , fibsize ); <nl> + user_srbcmd = kmalloc ( fibsize , GFP_KERNEL ); <nl> if (! user_srbcmd ) { <nl> dprintk (( KERN_DEBUG " aacraid : Could not make a copy of the srb \ n ")); <nl> rcode = - ENOMEM ;
static void zynqmp_gqspi_selectslave ( struct zynqmp_qspi * instanceptr , <nl> case GQSPI_SELECT_FLASH_CS_BOTH : <nl> instanceptr -> genfifocs = GQSPI_GENFIFO_CS_LOWER | <nl> GQSPI_GENFIFO_CS_UPPER ; <nl> + break ; <nl> case GQSPI_SELECT_FLASH_CS_UPPER : <nl> instanceptr -> genfifocs = GQSPI_GENFIFO_CS_UPPER ; <nl> break ;
static void raise_dtr ( struct isi_port * port ) <nl> } <nl>  <nl> /* card -> lock HAS to be held */ <nl> - static inline void drop_dtr ( struct isi_port * port ) <nl> + static void drop_dtr ( struct isi_port * port ) <nl> { <nl> struct isi_board * card = port -> card ; <nl> unsigned long base = card -> base ;
int btrfs_remove_free_space ( struct btrfs_block_group_cache * block_group , <nl> ctl -> total_bitmaps --; <nl> } <nl> kmem_cache_free ( btrfs_free_space_cachep , info ); <nl> + ret = 0 ; <nl> goto out_lock ; <nl> } <nl>  <nl> int btrfs_remove_free_space ( struct btrfs_block_group_cache * block_group , <nl> unlink_free_space ( ctl , info ); <nl> info -> offset += bytes ; <nl> info -> bytes -= bytes ; <nl> - link_free_space ( ctl , info ); <nl> + ret = link_free_space ( ctl , info ); <nl> + WARN_ON ( ret ); <nl> goto out_lock ; <nl> } <nl> 
static void pvr2_hdw_state_log_state ( struct pvr2_hdw * hdw ) <nl> printk ( KERN_INFO "% s %.* s \ n ", hdw -> name , ccnt , buf ); <nl> } <nl> ccnt = pvr2_hdw_report_clients ( hdw , buf , sizeof ( buf )); <nl> + if ( ccnt >= sizeof ( buf )) <nl> + ccnt = sizeof ( buf ); <nl> + <nl> ucnt = 0 ; <nl> while ( ucnt < ccnt ) { <nl> lcnt = 0 ;
static noinline long btrfs_ioctl_start_sync ( struct file * file , void __user * argp <nl> return PTR_ERR ( trans ); <nl> transid = trans -> transid ; <nl> ret = btrfs_commit_transaction_async ( trans , root , 0 ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + btrfs_end_transaction ( trans , root ); <nl> return ret ; <nl> + } <nl>  <nl> if ( argp ) <nl> if ( copy_to_user ( argp , & transid , sizeof ( transid )))
static noinline int btrfs_ioctl_resize ( struct btrfs_root * root , <nl> } <nl> ret = btrfs_grow_device ( trans , device , new_size ); <nl> btrfs_commit_transaction ( trans , root ); <nl> - } else { <nl> + } else if ( new_size < old_size ) { <nl> ret = btrfs_shrink_device ( device , new_size ); <nl> } <nl> 
static int tegra_pcie_probe ( struct platform_device * pdev ) <nl> struct pci_bus * child ; <nl> int err ; <nl>  <nl> - host = pci_alloc_host_bridge ( sizeof (* pcie )); <nl> + host = devm_pci_alloc_host_bridge ( dev , sizeof (* pcie )); <nl> if (! host ) <nl> return - ENOMEM ; <nl> 
int perf_cpu_time_max_percent_handler ( struct ctl_table * table , int write , <nl> void __user * buffer , size_t * lenp , <nl> loff_t * ppos ) <nl> { <nl> - int ret = proc_dointvec ( table , write , buffer , lenp , ppos ); <nl> + int ret = proc_dointvec_minmax ( table , write , buffer , lenp , ppos ); <nl>  <nl> if ( ret || ! write ) <nl> return ret ;
int kvm_vm_ioctl_check_extension ( struct kvm * kvm , long ext ) <nl> break ; <nl> # endif <nl> case KVM_CAP_PPC_HTM : <nl> - r = cpu_has_feature ( CPU_FTR_TM_COMP ) && <nl> - is_kvmppc_hv_enabled ( kvm ); <nl> + r = cpu_has_feature ( CPU_FTR_TM_COMP ) && hv_enabled ; <nl> break ; <nl> default : <nl> r = 0 ;
static int fn_hash_insert ( struct fib_table * tb , struct fib_config * cfg ) <nl> struct fib_info * fi_drop ; <nl> u8 state ; <nl>  <nl> + if ( fi -> fib_treeref > 1 ) <nl> + goto out ; <nl> + <nl> write_lock_bh (& fib_hash_lock ); <nl> fi_drop = fa -> fa_info ; <nl> fa -> fa_info = fi ;
vc4_get_bcl ( struct drm_device * dev , struct vc4_exec_info * exec ) <nl> sizeof ( struct vc4_shader_state )) || <nl> temp_size < exec_size ) { <nl> DRM_ERROR (" overflow in exec arguments \ n "); <nl> + ret = - EINVAL ; <nl> goto fail ; <nl> } <nl> 
MODULE_DEVICE_TABLE ( pci , epca_pci_tbl ); <nl> int __init init_PCI ( void ) <nl> { /* Begin init_PCI */ <nl> memset (& epca_driver , 0 , sizeof ( epca_driver )); <nl> + epca_driver . owner = THIS_MODULE ; <nl> epca_driver . name = " epca "; <nl> epca_driver . id_table = epca_pci_tbl ; <nl> epca_driver . probe = epca_init_one ;
static int atmel_hlcdc_plane_init_properties ( struct atmel_hlcdc_plane * plane , <nl> drm_object_attach_property (& plane -> base . base , <nl> props -> alpha , 255 ); <nl>  <nl> - if ( desc -> layout . xstride && desc -> layout . pstride ) { <nl> + if ( desc -> layout . xstride [ 0 ] && desc -> layout . pstride [ 0 ]) { <nl> int ret ; <nl>  <nl> ret = drm_plane_create_rotation_property (& plane -> base ,
static int vpbe_display_g_register ( struct file * file , void * priv , <nl> struct v4l2_dbg_register * reg ) <nl> { <nl> struct v4l2_dbg_match * match = & reg -> match ; <nl> + struct vpbe_fh * fh = file -> private_data ; <nl> + struct vpbe_device * vpbe_dev = fh -> disp_dev -> vpbe_dev ; <nl>  <nl> if ( match -> type >= 2 ) { <nl> v4l2_subdev_call ( vpbe_dev -> venc ,
static int m41t80_probe ( struct i2c_client * client , <nl> m41t80_rtc_ops . read_alarm = m41t80_read_alarm ; <nl> m41t80_rtc_ops . set_alarm = m41t80_set_alarm ; <nl> m41t80_rtc_ops . alarm_irq_enable = m41t80_alarm_irq_enable ; <nl> + /* Enable the wakealarm */ <nl> + device_init_wakeup (& client -> dev , true ); <nl> } <nl> } <nl> 
static int stmmac_open ( struct net_device * dev ) <nl> if ( ret ) { <nl> pr_err ("% s : Cannot attach to PHY ( error : % d )\ n ", <nl> __func__ , ret ); <nl> - goto phy_error ; <nl> + return ret ; <nl> } <nl> } <nl>  <nl> static int stmmac_open ( struct net_device * dev ) <nl> dma_desc_error : <nl> if ( priv -> phydev ) <nl> phy_disconnect ( priv -> phydev ); <nl> - phy_error : <nl> - clk_disable_unprepare ( priv -> stmmac_clk ); <nl>  <nl> return ret ; <nl> }
static void hpfs_write_failed ( struct address_space * mapping , loff_t to ) <nl> { <nl> struct inode * inode = mapping -> host ; <nl>  <nl> + hpfs_lock ( inode -> i_sb ); <nl> + <nl> if ( to > inode -> i_size ) { <nl> truncate_pagecache ( inode , to , inode -> i_size ); <nl> hpfs_truncate ( inode ); <nl> } <nl> + <nl> + hpfs_unlock ( inode -> i_sb ); <nl> } <nl>  <nl> static int hpfs_write_begin ( struct file * file , struct address_space * mapping ,
static struct hardwall_info * hardwall_create ( <nl> /* Allocate a new rectangle optimistically . */ <nl> rect = kmalloc ( sizeof ( struct hardwall_info ), <nl> GFP_KERNEL | __GFP_ZERO ); <nl> + if ( rect == NULL ) <nl> + return ERR_PTR (- ENOMEM ); <nl> INIT_LIST_HEAD (& rect -> task_head ); <nl>  <nl> /* Compute the rectangle size and validate that it ' s plausible . */
int host_int_set_wep_default_key ( struct host_if_drv * hif_drv , u8 index ); <nl> * @ date 8 March 2012 <nl> * @ version 1 . 0 <nl> */ <nl> - int host_int_add_wep_key_bss_sta ( struct host_if_drv * hWFIDrv , const u8 * pu8WepKey , u8 u8WepKeylen , u8 u8Keyidx ); <nl> + int host_int_add_wep_key_bss_sta ( struct host_if_drv * hif_drv , const u8 * pu8WepKey , u8 u8WepKeylen , u8 u8Keyidx ); <nl> /** <nl> * @ brief host_int_add_wep_key_bss_ap <nl> * @ details valid only in AP mode if External Supplicant support is enabled .
nvkm_disp_oneinit ( struct nvkm_engine * engine ) <nl> /* Create output path objects for each VBIOS display path . */ <nl> i = - 1 ; <nl> while (( data = dcb_outp_parse ( bios , ++ i , & ver , & hdr , & dcbE ))) { <nl> + if ( ver < 0x40 ) /* No support for chipsets prior to NV50 . */ <nl> + break ; <nl> if ( dcbE . type == DCB_OUTPUT_UNUSED ) <nl> continue ; <nl> if ( dcbE . type == DCB_OUTPUT_EOL )
static int arizona_runtime_resume ( struct device * dev ) <nl> return ret ; <nl> } <nl>  <nl> - regcache_sync ( arizona -> regmap ); <nl> + ret = regcache_sync ( arizona -> regmap ); <nl> + if ( ret != 0 ) { <nl> + dev_err ( arizona -> dev , " Failed to restore register cache \ n "); <nl> + regulator_disable ( arizona -> dcvdd ); <nl> + return ret ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static int find_data_references ( struct reloc_control * rc , <nl> } <nl>  <nl> key . objectid = ref_objectid ; <nl> - key . offset = ref_offset ; <nl> key . type = BTRFS_EXTENT_DATA_KEY ; <nl> + if ( ref_offset > (( u64 )- 1 << 32 )) <nl> + key . offset = 0 ; <nl> + else <nl> + key . offset = ref_offset ; <nl>  <nl> path -> search_commit_root = 1 ; <nl> path -> skip_locking = 1 ;
int iptunnel_xmit ( struct sock * sk , struct rtable * rt , struct sk_buff * skb , <nl> __be32 src , __be32 dst , __u8 proto , <nl> __u8 tos , __u8 ttl , __be16 df , bool xnet ) <nl> { <nl> - int pkt_len = skb -> len ; <nl> + int pkt_len = skb -> len - skb_inner_network_offset ( skb ); <nl> struct iphdr * iph ; <nl> int err ; <nl> 
static inline struct dma_chan <nl> if ( chan ) <nl> return chan ; <nl>  <nl> + if (! fn || ! fn_param ) <nl> + return NULL ; <nl> + <nl> return __dma_request_channel ( mask , fn , fn_param ); <nl> } <nl> # endif /* DMAENGINE_H */
static int sur40_probe ( struct usb_interface * interface , <nl> sur40 -> alloc_ctx = vb2_dma_sg_init_ctx ( sur40 -> dev ); <nl> if ( IS_ERR ( sur40 -> alloc_ctx )) { <nl> dev_err ( sur40 -> dev , " Can ' t allocate buffer context "); <nl> + error = PTR_ERR ( sur40 -> alloc_ctx ); <nl> goto err_unreg_v4l2 ; <nl> } <nl> 
static void _rtl_usb_rx_process_noagg ( struct ieee80211_hw * hw , <nl> ieee80211_rx ( hw , skb ); <nl> else <nl> dev_kfree_skb_any ( skb ); <nl> + } else { <nl> + dev_kfree_skb_any ( skb ); <nl> } <nl> } <nl> 
int mmc_send_if_cond ( struct mmc_host * host , u32 ocr ) <nl> static const u8 test_pattern = 0xAA ; <nl> u8 result_pattern ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( struct mmc_command )); <nl> + <nl> /* <nl> * To support SD 2 . 0 cards , we must always invoke SD_SEND_IF_COND <nl> * before SD_APP_OP_COND . This command will harmlessly fail for
static void picolcd_debug_out_report ( struct picolcd_data * data , <nl> report -> id , raw_size ); <nl> hid_debug_event ( hdev , buff ); <nl> if ( raw_size + 5 > sizeof ( raw_data )) { <nl> + kfree ( buff ); <nl> hid_debug_event ( hdev , " TOO BIG \ n "); <nl> return ; <nl> } else {
void ath6kl_rx ( struct htc_target * target , struct htc_packet * packet ) <nl> /* aggregation code will handle the skb */ <nl> return ; <nl> } <nl> - } <nl> + } else if (! is_broadcast_ether_addr ( datap -> h_dest )) <nl> + vif -> net_stats . multicast ++; <nl>  <nl> ath6kl_deliver_frames_to_nw_stack ( vif -> ndev , skb ); <nl> }
static void unicast_arp_send ( struct sk_buff * skb , struct net_device * dev , <nl> skb_push ( skb , sizeof * phdr ); <nl> __skb_queue_tail (& path -> queue , skb ); <nl>  <nl> - if ( path_rec_start ( dev , path )) { <nl> + if (! path -> query && path_rec_start ( dev , path )) { <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl> path_free ( dev , path ); <nl> return ;
typedef __s64 int64_t ; <nl> # endif <nl>  <nl> /* this is a special 64bit data type that is 8 - byte aligned */ <nl> -# define aligned_u64 unsigned long long __attribute__ (( aligned ( 8 ))) <nl> +# define aligned_u64 __u64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_be64 __be64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_le64 __le64 __attribute__ (( aligned ( 8 ))) <nl> 
void __init kmem_cache_init ( void ) <nl> /* Allocate two kmem_caches from the page allocator */ <nl> kmalloc_size = ALIGN ( kmem_size , cache_line_size ()); <nl> order = get_order ( 2 * kmalloc_size ); <nl> - kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT , order ); <nl> + kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT | __GFP_ZERO , order ); <nl>  <nl> /* <nl> * Must first have the slab cache available for the allocations of the
static int iwl_mvm_free_inactive_queue ( struct iwl_mvm * mvm , int queue , <nl> spin_unlock_bh (& mvm -> queue_info_lock ); <nl>  <nl> mvmsta = iwl_mvm_sta_from_staid_protected ( mvm , sta_id ); <nl> + if ( WARN_ON (! mvmsta )) <nl> + return - EINVAL ; <nl>  <nl> disable_agg_tids = iwl_mvm_remove_sta_queue_marking ( mvm , queue ); <nl> /* Disable the queue */
static long compat_ipmi_ioctl ( struct file * filep , unsigned int cmd , <nl> struct ipmi_recv __user * precv64 ; <nl> struct ipmi_recv recv64 ; <nl>  <nl> + memset (& recv64 , 0 , sizeof ( recv64 )); <nl> if ( get_compat_ipmi_recv (& recv64 , compat_ptr ( arg ))) <nl> return - EFAULT ; <nl> 
static int replace_map_fd_with_map_ptr ( struct verifier_env * env ) <nl> if ( IS_ERR ( map )) { <nl> verbose (" fd % d is not pointing to valid bpf_map \ n ", <nl> insn -> imm ); <nl> - fdput ( f ); <nl> return PTR_ERR ( map ); <nl> } <nl> 
int beiscsi_cmd_mccq_create ( struct beiscsi_hba * phba , <nl> spin_lock (& phba -> ctrl . mbox_lock ); <nl> ctrl = & phba -> ctrl ; <nl> wrb = wrb_from_mbox (& ctrl -> mbox_mem ); <nl> + memset ( wrb , 0 , sizeof (* wrb )); <nl> req = embedded_payload ( wrb ); <nl> ctxt = & req -> context ; <nl> 
static int twl6040_vibra_probe ( struct platform_device * pdev ) <nl>  <nl> info -> input_dev -> name = " twl6040 : vibrator "; <nl> info -> input_dev -> id . version = 1 ; <nl> - info -> input_dev -> dev . parent = pdev -> dev . parent ; <nl> info -> input_dev -> close = twl6040_vibra_close ; <nl> __set_bit ( FF_RUMBLE , info -> input_dev -> ffbit ); <nl> 
static void valleyview_disable_rps ( struct drm_device * dev ) <nl>  <nl> int intel_enable_rc6 ( const struct drm_device * dev ) <nl> { <nl> + /* No RC6 before Ironlake */ <nl> + if ( INTEL_INFO ( dev )-> gen < 5 ) <nl> + return 0 ; <nl> + <nl> /* Respect the kernel parameter if it is set */ <nl> if ( i915_enable_rc6 >= 0 ) <nl> return i915_enable_rc6 ;
static long gpio_ioctl ( struct file * filp , unsigned int cmd , unsigned long arg ) <nl> if ( cmd == GPIO_GET_CHIPINFO_IOCTL ) { <nl> struct gpiochip_info chipinfo ; <nl>  <nl> + memset (& chipinfo , 0 , sizeof ( chipinfo )); <nl> + <nl> strncpy ( chipinfo . name , dev_name (& gdev -> dev ), <nl> sizeof ( chipinfo . name )); <nl> chipinfo . name [ sizeof ( chipinfo . name )- 1 ] = '\ 0 ';
int btrfs_read_chunk_tree ( struct btrfs_root * root ) <nl> key . type = 0 ; <nl> again : <nl> ret = btrfs_search_slot ( NULL , root , & key , path , 0 , 0 ); <nl> + if ( ret < 0 ) <nl> + goto error ; <nl> while ( 1 ) { <nl> leaf = path -> nodes [ 0 ]; <nl> slot = path -> slots [ 0 ];
int fib_nh_match ( struct fib_config * cfg , struct fib_info * fi , <nl> fi -> fib_nh , cfg , extack )) <nl> return 1 ; <nl> } <nl> +# ifdef CONFIG_IP_ROUTE_CLASSID <nl> + if ( cfg -> fc_flow && <nl> + cfg -> fc_flow != fi -> fib_nh -> nh_tclassid ) <nl> + return 1 ; <nl> +# endif <nl> if ((! cfg -> fc_oif || cfg -> fc_oif == fi -> fib_nh -> nh_oif ) && <nl> (! cfg -> fc_gw || cfg -> fc_gw == fi -> fib_nh -> nh_gw )) <nl> return 0 ;
static void ufs_mtk_init_va09_pwr_ctrl ( struct ufs_hba * hba ) <nl> struct ufs_mtk_host * host = ufshcd_get_variant ( hba ); <nl>  <nl> host -> reg_va09 = regulator_get ( hba -> dev , " va09 "); <nl> - if (! host -> reg_va09 ) <nl> + if ( IS_ERR ( host -> reg_va09 )) <nl> dev_info ( hba -> dev , " failed to get va09 "); <nl> else <nl> host -> caps |= UFS_MTK_CAP_VA09_PWR_CTRL ;
static unsigned int xdr_set_page_base ( struct xdr_stream * xdr , <nl> void * kaddr ; <nl>  <nl> maxlen = xdr -> buf -> page_len ; <nl> - if ( base >= maxlen ) { <nl> - base = maxlen ; <nl> - maxlen = 0 ; <nl> - } else <nl> + if ( base >= maxlen ) <nl> + return 0 ; <nl> + else <nl> maxlen -= base ; <nl> if ( len > maxlen ) <nl> len = maxlen ;
lba_legacy_resources ( struct parisc_device * pa_dev , struct lba_device * lba_dev ) <nl> r -> name = " LBA PCI Busses "; <nl> r -> start = lba_num & 0xff ; <nl> r -> end = ( lba_num >> 8 ) & 0xff ; <nl> + r -> flags = IORESOURCE_BUS ; <nl>  <nl> /* Set up local PCI Bus resources - we don ' t need them for <nl> ** Legacy boxes but it ' s nice to see in / proc / iomem .
drm_est3_modes ( struct drm_connector * connector , struct detailed_timing * timing ) <nl> u8 * est = (( u8 *) timing ) + 5 ; <nl>  <nl> for ( i = 0 ; i < 6 ; i ++) { <nl> - for ( j = 7 ; j > 0 ; j --) { <nl> + for ( j = 7 ; j >= 0 ; j --) { <nl> m = ( i * 8 ) + ( 7 - j ); <nl> if ( m >= ARRAY_SIZE ( est3_modes )) <nl> break ;
static int trunc_start ( struct gfs2_inode * ip , u64 size ) <nl>  <nl> if ( gfs2_is_stuffed ( ip )) { <nl> u64 dsize = size + sizeof ( struct gfs2_inode ); <nl> + ip -> i_disksize = size ; <nl> ip -> i_inode . i_mtime = ip -> i_inode . i_ctime = CURRENT_TIME ; <nl> gfs2_trans_add_bh ( ip -> i_gl , dibh , 1 ); <nl> gfs2_dinode_out ( ip , dibh -> b_data );
ips_link_to_i915_driver ( void ) <nl> EXPORT_SYMBOL_GPL ( ips_link_to_i915_driver ); <nl>  <nl> static const struct pci_device_id ips_id_table [] = { <nl> - { PCI_DEVICE ( PCI_VENDOR_ID_INTEL , <nl> - PCI_DEVICE_ID_INTEL_THERMAL_SENSOR ), }, <nl> + { PCI_VDEVICE ( INTEL , PCI_DEVICE_ID_INTEL_THERMAL_SENSOR ), }, <nl> { 0 , } <nl> }; <nl> 
static int __init cell_iommu_fixed_mapping_init ( void ) <nl> fbase = _ALIGN_UP ( fbase , 1 << IO_SEGMENT_SHIFT ); <nl> fsize = lmb_phys_mem_size (); <nl>  <nl> - if (( fbase + fsize ) <= 0x800000000 ) <nl> + if (( fbase + fsize ) <= 0x800000000ul ) <nl> hbase = 0 ; /* use the device tree window */ <nl> else { <nl> /* If we ' re over 32 GB we need to cheat . We can ' t map all of
struct rxrpc_connection * rxrpc_find_service_conn_rcu ( struct rxrpc_peer * peer , <nl> else if ( conn -> proto . index_key > k . index_key ) <nl> p = rcu_dereference_raw ( p -> rb_right ); <nl> else <nl> - goto done ; <nl> + break ; <nl> conn = NULL ; <nl> } <nl> } while ( need_seqretry (& peer -> service_conn_lock , seq )); <nl>  <nl> - done : <nl> done_seqretry (& peer -> service_conn_lock , seq ); <nl> _leave (" = % d ", conn ? conn -> debug_id : - 1 ); <nl> return conn ;
static __u8 * cp_report_fixup ( struct hid_device * hdev , __u8 * rdesc , <nl> if (!( quirks & CP_RDESC_SWAPPED_MIN_MAX )) <nl> return rdesc ; <nl>  <nl> + if (* rsize < 4 ) <nl> + return rdesc ; <nl> + <nl> for ( i = 0 ; i < * rsize - 4 ; i ++) <nl> if ( rdesc [ i ] == 0x29 && rdesc [ i + 2 ] == 0x19 ) { <nl> rdesc [ i ] = 0x19 ;
lpfc_els_flush_cmd ( struct lpfc_vport * vport ) <nl> */ <nl> spin_lock_irq (& phba -> hbalock ); <nl> pring = lpfc_phba_elsring ( phba ); <nl> + <nl> + /* Bail out if we ' ve no ELS wq , like in PCI error recovery case . */ <nl> + if ( unlikely (! pring )) { <nl> + spin_unlock_irq (& phba -> hbalock ); <nl> + return ; <nl> + } <nl> + <nl> if ( phba -> sli_rev == LPFC_SLI_REV4 ) <nl> spin_lock (& pring -> ring_lock ); <nl> 
static void ax25_kill_by_device ( struct net_device * dev ) <nl> lock_sock ( sk ); <nl> s -> ax25_dev = NULL ; <nl> ax25_dev_put ( ax25_dev ); <nl> - release_sock ( sk ); <nl> ax25_disconnect ( s , ENETUNREACH ); <nl> + release_sock ( sk ); <nl> spin_lock_bh (& ax25_list_lock ); <nl> sock_put ( sk ); <nl> /* The entry could have been deleted from the
struct inet_connection_sock { <nl>  <nl> u32 probe_timestamp ; <nl> } icsk_mtup ; <nl> - u32 icsk_ca_priv [ 16 ]; <nl> u32 icsk_user_timeout ; <nl> -# define ICSK_CA_PRIV_SIZE ( 16 * sizeof ( u32 )) <nl> + <nl> + u64 icsk_ca_priv [ 64 / sizeof ( u64 )]; <nl> +# define ICSK_CA_PRIV_SIZE ( 8 * sizeof ( u64 )) <nl> }; <nl>  <nl> # define ICSK_TIME_RETRANS 1 /* Retransmit timer */
int perf_uprobe_init ( struct perf_event * p_event , bool is_retprobe ) <nl> return - ENOMEM ; <nl> ret = strncpy_from_user ( <nl> path , u64_to_user_ptr ( p_event -> attr . uprobe_path ), PATH_MAX ); <nl> + if ( ret == PATH_MAX ) <nl> + return - E2BIG ; <nl> if ( ret < 0 ) <nl> goto out ; <nl> if ( path [ 0 ] == '\ 0 ') {
struct ti_ohci { <nl>  <nl> static inline int cross_bound ( unsigned long addr , unsigned int size ) <nl> { <nl> + if ( size == 0 ) <nl> + return 0 ; <nl> + <nl> if ( size > PAGE_SIZE ) <nl> return 1 ; <nl> 
static struct shash_alg alg = { <nl> . export = md5_export , <nl> . import = md5_import , <nl> . descsize = sizeof ( struct md5_state ), <nl> + . statesize = sizeof ( struct md5_state ), <nl> . base = { <nl> . cra_name = " md5 ", <nl> . cra_flags = CRYPTO_ALG_TYPE_SHASH ,
* under normal circumstances , used to verify that nobody uses <nl> * non - initialized list entries . <nl> */ <nl> -# define LIST_POISON1 (( void *) 0x00100100 + POISON_POINTER_DELTA ) <nl> -# define LIST_POISON2 (( void *) 0x00200200 + POISON_POINTER_DELTA ) <nl> +# define LIST_POISON1 (( void *) 0x100 + POISON_POINTER_DELTA ) <nl> +# define LIST_POISON2 (( void *) 0x200 + POISON_POINTER_DELTA ) <nl>  <nl> /********** include / linux / timer . h **********/ <nl> /*
static int perf_event__repipe_attr ( struct perf_tool * tool , <nl> union perf_event * event , <nl> struct perf_evlist ** pevlist ) <nl> { <nl> + struct perf_inject * inject = container_of ( tool , struct perf_inject , <nl> + tool ); <nl> int ret ; <nl>  <nl> ret = perf_event__process_attr ( tool , event , pevlist ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + if (! inject -> pipe_output ) <nl> + return 0 ; <nl> + <nl> return perf_event__repipe_synth ( tool , event ); <nl> } <nl> 
ath5k_deinit_softc ( struct ath5k_softc * sc ) <nl> * state and potentially want to use them . <nl> */ <nl> ath5k_hw_deinit ( sc -> ah ); <nl> + kfree ( sc -> ah ); <nl> free_irq ( sc -> irq , sc ); <nl> } <nl> 
static int ieee80211_get_key ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> if ( pairwise && key_idx < NUM_DEFAULT_KEYS ) <nl> key = rcu_dereference ( sta -> ptk [ key_idx ]); <nl> - else if (! pairwise && key_idx < NUM_DEFAULT_KEYS ) <nl> + else if (! pairwise && <nl> + key_idx < NUM_DEFAULT_KEYS + NUM_DEFAULT_MGMT_KEYS ) <nl> key = rcu_dereference ( sta -> gtk [ key_idx ]); <nl> } else <nl> key = rcu_dereference ( sdata -> keys [ key_idx ]);
static int poll_select_copy_remaining ( struct timespec * end_time , void __user * p , <nl> rts . tv_sec = rts . tv_nsec = 0 ; <nl>  <nl> if ( timeval ) { <nl> + if ( sizeof ( rtv ) > sizeof ( rtv . tv_sec ) + sizeof ( rtv . tv_usec )) <nl> + memset (& rtv , 0 , sizeof ( rtv )); <nl> rtv . tv_sec = rts . tv_sec ; <nl> rtv . tv_usec = rts . tv_nsec / NSEC_PER_USEC ; <nl> 
static int s5c73m3_probe ( struct i2c_client * client , <nl> state -> oif_pads [ OIF_ISP_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_JPEG_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_SOURCE_PAD ]. flags = MEDIA_PAD_FL_SOURCE ; <nl> - oif_sd -> entity . function = MEDIA_ENT_F_V4L2_SUBDEV_UNKNOWN ; <nl> + oif_sd -> entity . function = MEDIA_ENT_F_PROC_VIDEO_SCALER ; <nl>  <nl> ret = media_entity_pads_init (& oif_sd -> entity , OIF_NUM_PADS , <nl> state -> oif_pads );
void __key_link_end ( struct key * keyring , <nl> if ( index_key -> type == & key_type_keyring ) <nl> up_write (& keyring_serialise_link_sem ); <nl>  <nl> - if ( edit && ! edit -> dead_leaf ) { <nl> - key_payload_reserve ( keyring , <nl> - keyring -> datalen - KEYQUOTA_LINK_BYTES ); <nl> + if ( edit ) { <nl> + if (! edit -> dead_leaf ) { <nl> + key_payload_reserve ( keyring , <nl> + keyring -> datalen - KEYQUOTA_LINK_BYTES ); <nl> + } <nl> assoc_array_cancel_edit ( edit ); <nl> } <nl> up_write (& keyring -> sem );
unsigned long perf_instruction_pointer ( struct pt_regs * regs ) <nl> bool use_siar = regs_use_siar ( regs ); <nl> unsigned long siar = mfspr ( SPRN_SIAR ); <nl>  <nl> - if ( ppmu -> flags & PPMU_P10_DD1 ) { <nl> + if ( ppmu && ( ppmu -> flags & PPMU_P10_DD1 )) { <nl> if ( siar ) <nl> return siar ; <nl> else
int vt_do_kdskled ( int console , int cmd , unsigned long arg , int perm ) <nl> kbd -> default_ledflagstate = (( arg >> 4 ) & 7 ); <nl> set_leds (); <nl> spin_unlock_irqrestore (& kbd_event_lock , flags ); <nl> - break ; <nl> + return 0 ; <nl>  <nl> /* the ioctls below only set the lights , not the functions */ <nl> /* for those , see KDGKBLED and KDSKBLED above */
struct dentry * ovl_lookup ( struct inode * dir , struct dentry * dentry , <nl> } <nl>  <nl> if ( d . redirect ) { <nl> + err = - ENOMEM ; <nl> upperredirect = kstrdup ( d . redirect , GFP_KERNEL ); <nl> if (! upperredirect ) <nl> goto out_put_upper ;
int __init oprofile_arch_init ( struct oprofile_operations * ops ) <nl>  <nl> void oprofile_arch_exit ( void ) <nl> { <nl> - model -> exit (); <nl> + if ( model ) <nl> + model -> exit (); <nl> }
int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> if ( IS_ERR ( device )) { <nl> if ( PTR_ERR ( device ) == - ENOENT && <nl> - strcmp ( device_path , " missing ") == 0 ) <nl> + device_path && strcmp ( device_path , " missing ") == 0 ) <nl> ret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND ; <nl> else <nl> ret = PTR_ERR ( device );
void wil_halp_vote ( struct wil6210_priv * wil ) <nl> wil -> halp . ref_cnt ); <nl>  <nl> if (++ wil -> halp . ref_cnt == 1 ) { <nl> + reinit_completion (& wil -> halp . comp ); <nl> wil6210_set_halp ( wil ); <nl> rc = wait_for_completion_timeout (& wil -> halp . comp , to_jiffies ); <nl> if (! rc ) {
static int ocfs2_remove_inode_range ( struct inode * inode , <nl> ocfs2_truncate_cluster_pages ( inode , byte_start , byte_len ); <nl>  <nl> out : <nl> + ocfs2_free_path ( path ); <nl> ocfs2_schedule_truncate_log_flush ( osb , 1 ); <nl> ocfs2_run_deallocs ( osb , & dealloc ); <nl> 
int ip6_route_add ( struct fib6_config * cfg ) <nl> goto out ; <nl> lwtunnel_state_get ( lwtstate ); <nl> rt -> rt6i_lwtstate = lwtstate ; <nl> + rt -> dst . output = lwtunnel_output6 ; <nl> } <nl>  <nl> ipv6_addr_prefix (& rt -> rt6i_dst . addr , & cfg -> fc_dst , cfg -> fc_dst_len );
uint16_t fixed_point_to_int_frac ( <nl> arg )); <nl>  <nl> if ( d <= ( uint16_t )( 1 << integer_bits ) - ( 1 / ( uint16_t ) divisor )) <nl> - numerator = ( uint16_t ) dal_fixed31_32_floor ( <nl> + numerator = ( uint16_t ) dal_fixed31_32_round ( <nl> dal_fixed31_32_mul_int ( <nl> arg , <nl> divisor ));
static noinline struct module * load_module ( void __user * umod , <nl> free_unload : <nl> module_unload_free ( mod ); <nl> # if defined ( CONFIG_MODULE_UNLOAD ) && defined ( CONFIG_SMP ) <nl> - free_init : <nl> percpu_modfree ( mod -> refptr ); <nl> + free_init : <nl> # endif <nl> module_free ( mod , mod -> module_init ); <nl> free_core :
static void xilinx_msi_teardown_irq ( struct msi_controller * chip , <nl> unsigned int irq ) <nl> { <nl> xilinx_pcie_destroy_msi ( irq ); <nl> + irq_dispose_mapping ( irq ); <nl> } <nl>  <nl> /**
sg_unlink_reserve ( Sg_fd * sfp , Sg_request * srp ) <nl> req_schp -> page_order = 0 ; <nl> req_schp -> sglist_len = 0 ; <nl> srp -> res_used = 0 ; <nl> + /* Called without mutex lock to avoid deadlock */ <nl> + sfp -> res_in_use = 0 ; <nl> } <nl>  <nl> static Sg_request *
static int clk_fetch_parent_index ( struct clk_core * core , <nl> { <nl> int i ; <nl>  <nl> + if (! parent ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * find index of new parent clock using cached parent ptrs , <nl> * or if not yet cached , use string name comparison and cache
static int imx_ssi_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> ssi -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( ssi -> irq < 0 ) { <nl> + dev_err (& pdev -> dev , " Failed to get IRQ : % d \ n ", ssi -> irq ); <nl> + return ssi -> irq ; <nl> + } <nl>  <nl> ssi -> clk = devm_clk_get (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( ssi -> clk )) {
struct dma_buf * exynos_dmabuf_prime_export ( struct drm_device * drm_dev , <nl> struct exynos_drm_gem_obj * exynos_gem_obj = to_exynos_gem_obj ( obj ); <nl>  <nl> return dma_buf_export ( exynos_gem_obj , & exynos_dmabuf_ops , <nl> - exynos_gem_obj -> base . size , 0600 ); <nl> + exynos_gem_obj -> base . size , flags ); <nl> } <nl>  <nl> struct drm_gem_object * exynos_dmabuf_prime_import ( struct drm_device * drm_dev ,
static int soc_tplg_dai_create ( struct soc_tplg * tplg , <nl> set_stream_info ( stream , caps ); <nl> } <nl>  <nl> + if ( pcm -> compress ) <nl> + dai_drv -> compress_new = snd_soc_new_compress ; <nl> + <nl> /* pass control to component driver for optional further init */ <nl> ret = soc_tplg_dai_load ( tplg , dai_drv , pcm , NULL ); <nl> if ( ret < 0 ) {
i915_gem_execbuffer_reserve ( struct intel_ring_buffer * ring , <nl>  <nl> obj -> base . pending_read_domains = 0 ; <nl> obj -> base . pending_write_domain = 0 ; <nl> + obj -> pending_fenced_gpu_access = false ; <nl> } <nl> list_splice (& ordered_objects , objects ); <nl> 
static int spi_qup_remove ( struct platform_device * pdev ) <nl> int ret ; <nl>  <nl> ret = pm_runtime_get_sync (& pdev -> dev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> return ret ; <nl>  <nl> ret = spi_qup_set_state ( controller , QUP_STATE_RESET );
void speakup_remove_virtual_keyboard ( void ) <nl> { <nl> if ( virt_keyboard != NULL ) { <nl> input_unregister_device ( virt_keyboard ); <nl> - input_free_device ( virt_keyboard ); <nl> virt_keyboard = NULL ; <nl> } <nl> }
static void do_pata_set_dmamode ( struct ata_port * ap , struct ata_device * adev , i <nl> u16 master_data ; <nl> u8 speed = adev -> dma_mode ; <nl> int devid = adev -> devno + 2 * ap -> port_no ; <nl> - u8 udma_enable ; <nl> + u8 udma_enable = 0 ; <nl>  <nl> static const /* ISP RTC */ <nl> u8 timings [][ 2 ] = { { 0 , 0 },
static int dpaa2_eth_poll ( struct napi_struct * napi , int budget ) <nl> err = dpaa2_io_service_rearm ( NULL , & ch -> nctx ); <nl> cpu_relax (); <nl> } while ( err == - EBUSY ); <nl> + WARN_ONCE ( err , " CDAN notifications rearm failed on core % d ", <nl> + ch -> nctx . desired_cpu ); <nl> } <nl>  <nl> ch -> stats . frames += cleaned ;
static int das16m1_attach ( struct comedi_device * dev , <nl>  <nl> s = & dev -> subdevices [ 3 ]; <nl> /* 8255 */ <nl> - subdev_8255_init ( dev , s , NULL , devpriv -> extra_iobase ); <nl> + ret = subdev_8255_init ( dev , s , NULL , devpriv -> extra_iobase ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> /* disable upper half of hardware conversion counter so it doesn ' t mess with us */ <nl> outb ( TOTAL_CLEAR , dev -> iobase + DAS16M1_8254_FIRST_CNTRL );
static int __devinit wl1271_probe ( struct sdio_func * func , <nl> goto out_free ; <nl> } <nl>  <nl> + enable_irq_wake ( wl -> irq ); <nl> + <nl> disable_irq ( wl -> irq ); <nl>  <nl> ret = wl1271_init_ieee80211 ( wl ); <nl> static void __devexit wl1271_remove ( struct sdio_func * func ) <nl> pm_runtime_get_noresume (& func -> dev ); <nl>  <nl> wl1271_unregister_hw ( wl ); <nl> + disable_irq_wake ( wl -> irq ); <nl> free_irq ( wl -> irq , wl ); <nl> wl1271_free_hw ( wl ); <nl> }
void drbd_start_resync ( struct drbd_device * device , enum drbd_conns side ) <nl> return ; <nl> } <nl>  <nl> + if (! connection ) { <nl> + drbd_err ( device , " No connection to peer , aborting !\ n "); <nl> + return ; <nl> + } <nl> + <nl> if (! test_bit ( B_RS_H_DONE , & device -> flags )) { <nl> if ( side == C_SYNC_TARGET ) { <nl> /* Since application IO was locked out during C_WF_BITMAP_T and
static struct irq_info * info_for_irq ( unsigned irq ) <nl>  <nl> static unsigned int evtchn_from_irq ( unsigned irq ) <nl> { <nl> + if ( unlikely ( WARN ( irq < 0 || irq >= nr_irqs , " Invalid irq % d !\ n ", irq ))) <nl> + return 0 ; <nl> + <nl> return info_for_irq ( irq )-> evtchn ; <nl> } <nl> 
static void __init smp_init_package_map ( void ) <nl> * primary cores . <nl> */ <nl> ncpus = boot_cpu_data . x86_max_cores ; <nl> + if (! ncpus ) { <nl> + pr_warn (" x86_max_cores == zero !?!?"); <nl> + ncpus = 1 ; <nl> + } <nl> + <nl> __max_logical_packages = DIV_ROUND_UP ( total_cpus , ncpus ); <nl>  <nl> /*
static struct mfd_cell db8500_prcmu_devs [] = { <nl> . pdata_size = sizeof ( db8500_regulators ), <nl> }, <nl> { <nl> - . name = " cpufreq - u8500 ", <nl> - . of_compatible = " stericsson , cpufreq - u8500 ", <nl> + . name = " cpufreq - ux500 ", <nl> + . of_compatible = " stericsson , cpufreq - ux500 ", <nl> . platform_data = & db8500_cpufreq_table , <nl> . pdata_size = sizeof ( db8500_cpufreq_table ), <nl> },
static int __devinit i2o_pci_probe ( struct pci_dev * pdev , <nl> i2o_pci_free ( c ); <nl>  <nl> free_controller : <nl> - i2o_iop_free ( c ); <nl> put_device ( c -> device . parent ); <nl> + i2o_iop_free ( c ); <nl>  <nl> disable : <nl> pci_disable_device ( pdev );
static bool mem_cgroup_out_of_memory ( struct mem_cgroup * memcg , gfp_t gfp_mask , <nl> mem_cgroup_iter_break ( memcg , iter ); <nl> if ( chosen ) <nl> put_task_struct ( chosen ); <nl> + /* Set a dummy value to return " true ". */ <nl> + chosen = ( void *) 1 ; <nl> goto unlock ; <nl> case OOM_SCAN_OK : <nl> break ;
vsock_stream_recvmsg ( struct kiocb * kiocb , <nl> vsk = vsock_sk ( sk ); <nl> err = 0 ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state != SS_CONNECTED ) {
static int amdgpu_dm_atomic_check ( struct drm_device * dev , <nl> } <nl> } else { <nl> for_each_oldnew_crtc_in_state ( state , crtc , old_crtc_state , new_crtc_state , i ) { <nl> - if (! drm_atomic_crtc_needs_modeset ( new_crtc_state )) <nl> + if (! drm_atomic_crtc_needs_modeset ( new_crtc_state ) && <nl> + ! new_crtc_state -> color_mgmt_changed ) <nl> continue ; <nl>  <nl> if (! new_crtc_state -> enable )
int bnxt_re_create_srq ( struct ib_srq * ib_srq , <nl> dev_err ( rdev_to_dev ( rdev ), " SRQ copy to udata failed !"); <nl> bnxt_qplib_destroy_srq (& rdev -> qplib_res , <nl> & srq -> qplib_srq ); <nl> - goto exit ; <nl> + goto fail ; <nl> } <nl> } <nl> if ( nq )
s32 host_int_del_station ( struct host_if_drv * hif_drv , const u8 * pu8MacAddr ) <nl> msg . drv = hif_drv ; <nl>  <nl> if (! pu8MacAddr ) <nl> - memset ( pstrDelStationMsg -> mac_addr , 255 , ETH_ALEN ); <nl> + eth_broadcast_addr ( pstrDelStationMsg -> mac_addr ); <nl> else <nl> memcpy ( pstrDelStationMsg -> mac_addr , pu8MacAddr , ETH_ALEN ); <nl> 
static struct stmmac_axi * stmmac_axi_setup ( struct platform_device * pdev ) <nl> if (! np ) <nl> return NULL ; <nl>  <nl> - axi = kzalloc ( sizeof ( axi ), GFP_KERNEL ); <nl> + axi = kzalloc ( sizeof (* axi ), GFP_KERNEL ); <nl> if (! axi ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
cfg80211_inform_bss_frame ( struct wiphy * wiphy , <nl> return NULL ; <nl>  <nl> if ( WARN_ON ( wiphy -> signal_type == CFG80211_SIGNAL_TYPE_UNSPEC && <nl> - ( signal < 0 || signal > 100 ))) <nl> + ( signal < 0 || signal > 100 ))) <nl> return NULL ; <nl>  <nl> if ( WARN_ON ( len < offsetof ( struct ieee80211_mgmt , u . probe_resp . variable )))
void exit_shm ( struct task_struct * task ) <nl> { <nl> struct ipc_namespace * ns = task -> nsproxy -> ipc_ns ; <nl>  <nl> + if ( shm_ids ( ns ). in_use == 0 ) <nl> + return ; <nl> + <nl> /* Destroy all already created segments , but not mapped yet */ <nl> down_write (& shm_ids ( ns ). rw_mutex ); <nl> if ( shm_ids ( ns ). in_use )
static struct file * do_last ( struct nameidata * nd , struct path * path , <nl> if ( path -> dentry -> d_inode -> i_op -> follow_link ) <nl> return NULL ; <nl> error = - ENOTDIR ; <nl> - if (* want_dir & ! path -> dentry -> d_inode -> i_op -> lookup ) <nl> + if (* want_dir && ! path -> dentry -> d_inode -> i_op -> lookup ) <nl> goto exit_dput ; <nl> path_to_nameidata ( path , nd ); <nl> audit_inode ( pathname , nd -> path . dentry );
static void sdhci_prepare_data ( struct sdhci_host * host , struct mmc_command * cmd ) <nl> int sg_cnt ; <nl>  <nl> sg_cnt = sdhci_pre_dma_transfer ( host , data , NULL ); <nl> - if ( sg_cnt == 0 ) { <nl> + if ( sg_cnt <= 0 ) { <nl> /* <nl> * This only happens when someone fed <nl> * us an invalid request .
void intel_engines_mark_idle ( struct drm_i915_private * i915 ) <nl> for_each_engine ( engine , i915 , id ) { <nl> intel_engine_disarm_breadcrumbs ( engine ); <nl> i915_gem_batch_pool_fini (& engine -> batch_pool ); <nl> + tasklet_kill (& engine -> irq_tasklet ); <nl> engine -> no_priolist = false ; <nl> } <nl> }
int usb_sg_init ( struct usb_sg_request * io , struct usb_device * dev , <nl> } <nl>  <nl> /* initialize all the urbs we ' ll use */ <nl> - io -> urbs = kmalloc ( io -> entries * sizeof * io -> urbs , mem_flags ); <nl> + io -> urbs = kmalloc ( io -> entries * sizeof (* io -> urbs ), mem_flags ); <nl> if (! io -> urbs ) <nl> goto nomem ; <nl> 
int __cpuinit local_timer_setup ( struct clock_event_device * evt ) <nl>  <nl> /* Use existing clock_event for cpu 0 */ <nl> if (! smp_processor_id ()) <nl> - return ; <nl> + return 0 ; <nl>  <nl> writel ( DGT_CLK_CTL_DIV_4 , MSM_TMR_BASE + DGT_CLK_CTL ); <nl> 
static const struct drm_i915_gem_object_ops i915_gem_phys_ops = { <nl> . release = i915_gem_object_release_phys , <nl> }; <nl>  <nl> - int <nl> - i915_gem_object_unbind ( struct drm_i915_gem_object * obj ) <nl> + int i915_gem_object_unbind ( struct drm_i915_gem_object * obj ) <nl> { <nl> struct i915_vma * vma ; <nl> LIST_HEAD ( still_in_list ); <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> /* The vma will only be freed if it is marked as closed , and if we wait <nl> * upon rendering to the vma , we may unbind anything in the list .
int batadv_bla_tx ( struct batadv_priv * bat_priv , struct sk_buff * skb , <nl> if (! atomic_read (& bat_priv -> bridge_loop_avoidance )) <nl> goto allow ; <nl>  <nl> - /* in VLAN case , the mac header might not be set . */ <nl> - skb_reset_mac_header ( skb ); <nl> - <nl> if ( batadv_bla_process_claim ( bat_priv , primary_if , skb )) <nl> goto handled ; <nl> 
static void perf_callchain_user_64 ( struct perf_callchain_entry * entry , <nl> sp = regs -> gpr [ 1 ]; <nl> perf_callchain_store ( entry , next_ip ); <nl>  <nl> - for (;;) { <nl> + while ( entry -> nr < PERF_MAX_STACK_DEPTH ) { <nl> fp = ( unsigned long __user *) sp ; <nl> if (! valid_user_sp ( sp , 1 ) || read_user_stack_64 ( fp , & next_sp )) <nl> return ;
static void hidinput_configure_usage ( struct hid_input * hidinput , struct hid_fiel <nl> case 0x0b6 : map_key_clear ( KEY_PREVIOUSSONG ); break ; <nl> case 0x0b7 : map_key_clear ( KEY_STOPCD ); break ; <nl> case 0x0b8 : map_key_clear ( KEY_EJECTCD ); break ; <nl> + case 0x0bc : map_key_clear ( KEY_MEDIA_REPEAT ); break ; <nl>  <nl> case 0x0cd : map_key_clear ( KEY_PLAYPAUSE ); break ; <nl> case 0x0e0 : map_abs_clear ( ABS_VOLUME ); break ;
static void * ixgbe_fwd_add ( struct net_device * pdev , struct net_device * vdev ) <nl> ( adapter -> num_rx_pools > IXGBE_MAX_MACVLANS )) <nl> return ERR_PTR (- EBUSY ); <nl>  <nl> - fwd_adapter = kcalloc ( 1 , sizeof ( struct ixgbe_fwd_adapter ), GFP_KERNEL ); <nl> + fwd_adapter = kzalloc ( sizeof (* fwd_adapter ), GFP_KERNEL ); <nl> if (! fwd_adapter ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
static inline void nfs4_stateid_downgrade ( struct nfs4_ol_stateid * stp , u32 to_ac <nl> } <nl>  <nl> static void <nl> - reset_union_bmap_deny ( unsigned long deny , struct nfs4_ol_stateid * stp ) <nl> + reset_union_bmap_deny ( u32 deny , struct nfs4_ol_stateid * stp ) <nl> { <nl> int i ; <nl> - for ( i = 0 ; i < 4 ; i ++) { <nl> + <nl> + for ( i = 1 ; i < 4 ; i ++) { <nl> if (( i & deny ) != i ) <nl> clear_deny ( i , stp ); <nl> }
void rds_inc_info_copy ( struct rds_incoming * inc , <nl> minfo . fport = inc -> i_hdr . h_dport ; <nl> } <nl>  <nl> + minfo . flags = 0 ; <nl> + <nl> rds_info_copy ( iter , & minfo , sizeof ( minfo )); <nl> }
static int si21_writeregs ( struct si21xx_state * state , u8 reg1 , <nl> . len = len + 1 <nl> }; <nl>  <nl> + if ( len > sizeof ( buf ) - 1 ) <nl> + return - EINVAL ; <nl> + <nl> msg . buf [ 0 ] = reg1 ; <nl> memcpy ( msg . buf + 1 , data , len ); <nl> 
static void isd200_ata_command ( struct scsi_cmnd * srb , struct us_data * us ) <nl>  <nl> /* Make sure driver was initialized */ <nl>  <nl> - if ( us -> extra == NULL ) <nl> + if ( us -> extra == NULL ) { <nl> usb_stor_dbg ( us , " ERROR Driver not initialized \ n "); <nl> + srb -> result = DID_ERROR << 16 ; <nl> + return ; <nl> + } <nl>  <nl> scsi_set_resid ( srb , 0 ); <nl> /* scsi_bufflen might change in protocol translation to ata */
static struct rds_connection * __rds_conn_create ( struct net * net , <nl> } <nl> } <nl>  <nl> + if ( trans == NULL ) { <nl> + kmem_cache_free ( rds_conn_slab , conn ); <nl> + conn = ERR_PTR (- ENODEV ); <nl> + goto out ; <nl> + } <nl> + <nl> conn -> c_trans = trans ; <nl>  <nl> ret = trans -> conn_alloc ( conn , gfp );
static int ptrace_bts_config ( struct task_struct * child , <nl> if (! cfg . signal ) <nl> return - EINVAL ; <nl>  <nl> - return - EOPNOTSUPP ; <nl> - <nl> child -> thread . bts_ovfl_signal = cfg . signal ; <nl> + return - EOPNOTSUPP ; <nl> } <nl>  <nl> if (( cfg . flags & PTRACE_BTS_O_ALLOC ) &&
static enum page_references page_check_references ( struct page * page , <nl> */ <nl> SetPageReferenced ( page ); <nl>  <nl> - if ( referenced_page ) <nl> + if ( referenced_page || referenced_ptes > 1 ) <nl> return PAGEREF_ACTIVATE ; <nl>  <nl> return PAGEREF_KEEP ;
static int wanxl_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> ifr -> ifr_settings . size = size ; /* data size wanted */ <nl> return - ENOBUFS ; <nl> } <nl> + memset (& line , 0 , sizeof ( line )); <nl> line . clock_type = get_status ( port )-> clocking ; <nl> line . clock_rate = 0 ; <nl> line . loopback = 0 ;
static int tegra_syncpt_wait ( struct drm_device * drm , void * data , <nl> if (! sp ) <nl> return - EINVAL ; <nl>  <nl> - return host1x_syncpt_wait ( sp , args -> thresh , args -> timeout , <nl> + return host1x_syncpt_wait ( sp , args -> thresh , <nl> + msecs_to_jiffies ( args -> timeout ), <nl> & args -> value ); <nl> } <nl> 
isdn_net_setcfg ( isdn_net_ioctl_cfg * cfg ) <nl> char * c , <nl> * e ; <nl>  <nl> + if ( strnlen ( cfg -> drvid , sizeof ( cfg -> drvid )) == <nl> + sizeof ( cfg -> drvid )) <nl> + return - EINVAL ; <nl> drvidx = - 1 ; <nl> chidx = - 1 ; <nl> strcpy ( drvid , cfg -> drvid );
static int yurex_probe ( struct usb_interface * interface , const struct usb_device_ <nl> usb_rcvintpipe ( dev -> udev , dev -> int_in_endpointAddr ), <nl> dev -> int_buffer , YUREX_BUF_SIZE , yurex_interrupt , <nl> dev , 1 ); <nl> - dev -> cntl_urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> + dev -> urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> if ( usb_submit_urb ( dev -> urb , GFP_KERNEL )) { <nl> retval = - EIO ; <nl> err (" Could not submitting URB ");
__init int intel_pmu_init ( void ) <nl> if ( version > 1 ) <nl> x86_pmu . num_counters_fixed = max (( int ) edx . split . num_counters_fixed , 3 ); <nl>  <nl> - /* <nl> - * v2 and above have a perf capabilities MSR <nl> - */ <nl> - if ( version > 1 ) { <nl> + if ( boot_cpu_has ( X86_FEATURE_PDCM )) { <nl> u64 capabilities ; <nl>  <nl> rdmsrl ( MSR_IA32_PERF_CAPABILITIES , capabilities );
typedef struct kl_config_hdr { <nl> /* --- New Macros for the changed kl_config_hdr_t structure --- */ <nl>  <nl> # define PTR_CH_MALLOC_HDR ( _k ) (( klc_malloc_hdr_t *)\ <nl> - ( unsigned long ) _k + ( _k -> ch_malloc_hdr_off ))) <nl> + (( unsigned long ) _k + ( _k -> ch_malloc_hdr_off ))) <nl>  <nl> # define KL_CONFIG_CH_MALLOC_HDR ( _n ) PTR_CH_MALLOC_HDR ( KL_CONFIG_HDR ( _n )) <nl> 
int btrfs_add_inode_defrag ( struct btrfs_trans_handle * trans , <nl> spin_lock (& root -> fs_info -> defrag_inodes_lock ); <nl> if (! BTRFS_I ( inode )-> in_defrag ) <nl> __btrfs_add_inode_defrag ( inode , defrag ); <nl> + else <nl> + kfree ( defrag ); <nl> spin_unlock (& root -> fs_info -> defrag_inodes_lock ); <nl> return 0 ; <nl> }
void btrfs_invalidate_inodes ( struct btrfs_root * root ) <nl> struct inode * inode ; <nl> u64 objectid = 0 ; <nl>  <nl> - WARN_ON ( btrfs_root_refs (& root -> root_item ) != 0 ); <nl> + if (! test_bit ( BTRFS_FS_STATE_ERROR , & root -> fs_info -> fs_state )) <nl> + WARN_ON ( btrfs_root_refs (& root -> root_item ) != 0 ); <nl>  <nl> spin_lock (& root -> inode_lock ); <nl> again :
static int sfi_cpufreq_cpu_init ( struct cpufreq_policy * policy ) <nl> { <nl> policy -> shared_type = CPUFREQ_SHARED_TYPE_HW ; <nl> policy -> cpuinfo . transition_latency = 100000 ; /* 100us */ <nl> + policy -> freq_table = freq_table ; <nl>  <nl> - return cpufreq_table_validate_and_show ( policy , freq_table ); <nl> + return 0 ; <nl> } <nl>  <nl> static struct cpufreq_driver sfi_cpufreq_driver = {
void bochs_fbdev_fini ( struct bochs_device * bochs ) <nl> if ( bochs -> fb . initialized ) <nl> bochs_fbdev_destroy ( bochs ); <nl>  <nl> - drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + if ( bochs -> fb . helper . fbdev ) <nl> + drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + <nl> bochs -> fb . initialized = false ; <nl> }
unsigned int kstat_irqs ( unsigned int irq ) <nl> */ <nl> unsigned int kstat_irqs_usr ( unsigned int irq ) <nl> { <nl> - int sum ; <nl> + unsigned int sum ; <nl>  <nl> irq_lock_sparse (); <nl> sum = kstat_irqs ( irq );
int __init pci_legacy_init ( void ) <nl>  <nl> return 0 ; <nl> } <nl> - EXPORT_SYMBOL_GPL ( pci_legacy_init ); <nl>  <nl> void pcibios_scan_specific_bus ( int busn ) <nl> {
static int dim2_probe ( struct platform_device * pdev ) <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - dev -> disable_platform = pdata ? pdata -> disable : 0 ; <nl> + dev -> disable_platform = pdata ? pdata -> disable : NULL ; <nl>  <nl> dev_info (& pdev -> dev , " sync : num of frames per sub - buffer : % u \ n ", fcnt ); <nl> hal_ret = dim_startup ( dev -> io_base , dev -> clk_speed , fcnt );
static int zd1201_resume ( struct usb_interface * interface ) <nl> { <nl> struct zd1201 * zd = usb_get_intfdata ( interface ); <nl>  <nl> + if (! zd || ! zd -> dev ) <nl> + return - ENODEV ; <nl> + <nl> netif_device_attach ( zd -> dev ); <nl>  <nl> if ( zd -> was_enabled )
int azx_codec_configure ( struct azx * chip ) <nl> list_for_each_codec_safe ( codec , next , & chip -> bus ) { <nl> snd_hda_codec_configure ( codec ); <nl> } <nl> + <nl> + if (! azx_bus ( chip )-> num_codecs ) <nl> + return - ENODEV ; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( azx_codec_configure );
static struct sk_buff * isdn_ppp_decompress ( struct sk_buff * skb , struct ippp_struc <nl> rsparm . maxdlen = IPPP_RESET_MAXDATABYTES ; <nl>  <nl> skb_out = dev_alloc_skb ( is -> mru + PPP_HDRLEN ); <nl> + if (! skb_out ) { <nl> + kfree_skb ( skb ); <nl> + printk ( KERN_ERR " ippp : decomp memory allocation failure \ n "); <nl> + return NULL ; <nl> + } <nl> len = ipc -> decompress ( stat , skb , skb_out , & rsparm ); <nl> kfree_skb ( skb ); <nl> if ( len <= 0 ) {
static void pcie_disable_notification ( struct controller * ctrl ) <nl> u16 mask ; <nl> mask = ( PCI_EXP_SLTCTL_PDCE | PCI_EXP_SLTCTL_ABPE | <nl> PCI_EXP_SLTCTL_MRLSCE | PCI_EXP_SLTCTL_PFDE | <nl> - PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE ); <nl> + PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE | <nl> + PCI_EXP_SLTCTL_DLLSCE ); <nl> if ( pcie_write_cmd ( ctrl , 0 , mask )) <nl> ctrl_warn ( ctrl , " Cannot disable software notification \ n "); <nl> }
static int __init twl4030_configure_resource ( struct twl4030_resconfig * rconfig ) <nl> return err ; <nl> } <nl>  <nl> - if ( rconfig -> remap_off >= 0 ) { <nl> + if ( rconfig -> remap_off != TWL4030_RESCONFIG_UNDEF ) { <nl> remap &= ~ OFF_STATE_MASK ; <nl> remap |= rconfig -> remap_off << OFF_STATE_SHIFT ; <nl> } <nl>  <nl> - if ( rconfig -> remap_sleep >= 0 ) { <nl> + if ( rconfig -> remap_sleep != TWL4030_RESCONFIG_UNDEF ) { <nl> remap &= ~ SLEEP_STATE_MASK ; <nl> remap |= rconfig -> remap_off << SLEEP_STATE_SHIFT ; <nl> }
enum vnt_cmd_state { <nl>  <nl> struct vnt_private ; <nl>  <nl> - void vnt_reset_command_timer ( struct vnt_private *); <nl> + void vnt_reset_command_timer ( struct vnt_private * priv ); <nl>  <nl> - int vnt_schedule_command ( struct vnt_private *, enum vnt_cmd ); <nl> + int vnt_schedule_command ( struct vnt_private * priv , enum vnt_cmd ); <nl>  <nl> void vnt_run_command ( struct work_struct * work ); <nl> 
static struct tnode * tnode_new ( t_key key , int pos , int bits ) <nl> } <nl>  <nl> pr_debug (" AT % p s =% zu % zu \ n ", tn , sizeof ( struct tnode ), <nl> - sizeof ( struct rt_trie_node ) << bits ); <nl> + sizeof ( struct rt_trie_node *) << bits ); <nl> return tn ; <nl> } <nl> 
int i2400m_op_rfkill_sw_toggle ( struct wimax_dev * wimax_dev , <nl> "% d \ n ", result ); <nl> result = 0 ; <nl> error_cmd : <nl> - kfree ( cmd ); <nl> kfree_skb ( ack_skb ); <nl> error_msg_to_dev : <nl> error_alloc : <nl> d_fnend ( 4 , dev , "( wimax_dev % p state % d ) = % d \ n ", <nl> wimax_dev , state , result ); <nl> + kfree ( cmd ); <nl> return result ; <nl> } <nl> 
static int iscsit_build_sendtargets_response ( struct iscsi_cmd * cmd ) <nl> if (! text_ptr ) { <nl> pr_err (" Unable to locate '=' string in text_in :" <nl> " % s \ n ", text_in ); <nl> + kfree ( payload ); <nl> return - EINVAL ; <nl> } <nl> /*
struct inode * reiserfs_iget ( struct super_block * s , const struct cpu_key * key ) <nl>  <nl> args . objectid = key -> on_disk_key . k_objectid ; <nl> args . dirid = key -> on_disk_key . k_dir_id ; <nl> + reiserfs_write_unlock ( s ); <nl> inode = iget5_locked ( s , key -> on_disk_key . k_objectid , <nl> reiserfs_find_actor , reiserfs_init_locked_inode , <nl> ( void *)(& args )); <nl> + reiserfs_write_lock ( s ); <nl> if (! inode ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
int __ieee80211_suspend ( struct ieee80211_hw * hw , struct cfg80211_wowlan * wowlan ) <nl> int err = drv_suspend ( local , wowlan ); <nl> if ( err < 0 ) { <nl> local -> quiescing = false ; <nl> + local -> wowlan = false ; <nl> return err ; <nl> } else if ( err > 0 ) { <nl> WARN_ON ( err != 1 );
bfin_debug_mmrs_dma ( struct dentry * parent , unsigned long base , int num , char mdm <nl> __DMA ( CURR_DESC_PTR , curr_desc_ptr ); <nl> __DMA ( CURR_ADDR , curr_addr ); <nl> __DMA ( IRQ_STATUS , irq_status ); <nl> - __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> + if ( strcmp ( pfx , " IMDMA ") != 0 ) <nl> + __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> __DMA ( CURR_X_COUNT , curr_x_count ); <nl> __DMA ( CURR_Y_COUNT , curr_y_count ); <nl> }
unsigned int solo_eeprom_ewen ( struct solo_dev * solo_dev , int w_en ) <nl> __be16 solo_eeprom_read ( struct solo_dev * solo_dev , int loc ) <nl> { <nl> int read_cmd = loc | ( EE_READ_CMD << ADDR_LEN ); <nl> - unsigned short retval = 0 ; <nl> + u16 retval = 0 ; <nl> int i ; <nl>  <nl> solo_eeprom_cmd ( solo_dev , read_cmd );
static LIST_HEAD ( dev_map_list ); <nl>  <nl> static u64 dev_map_bitmap_size ( const union bpf_attr * attr ) <nl> { <nl> - return BITS_TO_LONGS ( attr -> max_entries ) * sizeof ( unsigned long ); <nl> + return BITS_TO_LONGS (( u64 ) attr -> max_entries ) * sizeof ( unsigned long ); <nl> } <nl>  <nl> static struct bpf_map * dev_map_alloc ( union bpf_attr * attr )
struct simple_xattr * simple_xattr_alloc ( const void * value , size_t size ) <nl>  <nl> /* wrap around ? */ <nl> len = sizeof (* new_xattr ) + size ; <nl> - if ( len <= sizeof (* new_xattr )) <nl> + if ( len < sizeof (* new_xattr )) <nl> return NULL ; <nl>  <nl> new_xattr = kmalloc ( len , GFP_KERNEL );
static int atmel_nand_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> nand_chip -> ecc . mode = host -> board . ecc_mode ; <nl> - nand_chip -> chip_delay = 20 ; /* 20us command delay time */ <nl> + nand_chip -> chip_delay = 40 ; /* 40us command delay time */ <nl>  <nl> if ( host -> board . bus_width_16 ) /* 16 - bit bus width */ <nl> nand_chip -> options |= NAND_BUSWIDTH_16 ;
static int kvm_ioctl_create_device ( struct kvm * kvm , <nl>  <nl> ret = anon_inode_getfd ( ops -> name , & kvm_device_fops , dev , O_RDWR | O_CLOEXEC ); <nl> if ( ret < 0 ) { <nl> - ops -> destroy ( dev ); <nl> mutex_lock (& kvm -> lock ); <nl> list_del (& dev -> vm_node ); <nl> mutex_unlock (& kvm -> lock ); <nl> + ops -> destroy ( dev ); <nl> return ret ; <nl> } <nl> 
enum siginfo_layout siginfo_layout ( int sig , int si_code ) <nl> [ SIGSEGV ] = { NSIGSEGV , SIL_FAULT }, <nl> [ SIGBUS ] = { NSIGBUS , SIL_FAULT }, <nl> [ SIGTRAP ] = { NSIGTRAP , SIL_FAULT }, <nl> -# if defined ( SIGMET ) && defined ( NSIGEMT ) <nl> +# if defined ( SIGEMT ) && defined ( NSIGEMT ) <nl> [ SIGEMT ] = { NSIGEMT , SIL_FAULT }, <nl> # endif <nl> [ SIGCHLD ] = { NSIGCHLD , SIL_CHLD },
static int crb_recv ( struct tpm_chip * chip , u8 * buf , size_t count ) <nl>  <nl> memcpy_fromio ( buf , priv -> rsp , 6 ); <nl> expected = be32_to_cpup (( __be32 *) & buf [ 2 ]); <nl> - <nl> - if ( expected > count ) <nl> + if ( expected > count || expected < 6 ) <nl> return - EIO ; <nl>  <nl> memcpy_fromio (& buf [ 6 ], & priv -> rsp [ 6 ], expected - 6 );
static void chase_port ( struct edgeport_port * port , unsigned long timeout , <nl> wait_queue_t wait ; <nl> unsigned long flags ; <nl>  <nl> + if (! tty ) <nl> + return ; <nl> + <nl> if (! timeout ) <nl> timeout = ( HZ * EDGE_CLOSING_WAIT )/ 100 ; <nl> 
static void task_fork_fair ( struct task_struct * p ) <nl>  <nl> update_rq_clock ( rq ); <nl>  <nl> - if ( unlikely ( task_cpu ( p ) != this_cpu )) <nl> + if ( unlikely ( task_cpu ( p ) != this_cpu )) { <nl> + rcu_read_lock (); <nl> __set_task_cpu ( p , this_cpu ); <nl> + rcu_read_unlock (); <nl> + } <nl>  <nl> update_curr ( cfs_rq ); <nl> 
static int das1800_attach ( struct comedi_device * dev , <nl> if ( dev -> irq & it -> options [ 2 ]) <nl> das1800_init_dma ( dev , it ); <nl>  <nl> - devpriv -> fifo_buf = kmalloc ( FIFO_SIZE * sizeof ( uint16_t ), GFP_KERNEL ); <nl> + devpriv -> fifo_buf = kmalloc_array ( FIFO_SIZE , sizeof ( uint16_t ), GFP_KERNEL ); <nl> if (! devpriv -> fifo_buf ) <nl> return - ENOMEM ; <nl> 
static int i915_context_status ( struct seq_file * m , void * unused ) <nl> } <nl>  <nl> list_for_each_entry ( ctx , & dev_priv -> context_list , link ) { <nl> + if ( ctx -> obj == NULL ) <nl> + continue ; <nl> + <nl> seq_puts ( m , " HW context "); <nl> describe_ctx ( m , ctx ); <nl> for_each_ring ( ring , dev_priv , i )
static int alloc_wbufs ( struct ubifs_info * c ) <nl> { <nl> int i , err ; <nl>  <nl> - c -> jheads = kzalloc ( c -> jhead_cnt * sizeof ( struct ubifs_jhead ), <nl> - GFP_KERNEL ); <nl> + c -> jheads = kcalloc ( c -> jhead_cnt , sizeof ( struct ubifs_jhead ), <nl> + GFP_KERNEL ); <nl> if (! c -> jheads ) <nl> return - ENOMEM ; <nl> 
qla27xx_fwdt_entry_t270 ( struct scsi_qla_host * vha , <nl> qla27xx_write_reg ( reg , 0xc0 , addr | 0x80000000 , buf ); <nl> qla27xx_insert32 ( addr , buf , len ); <nl> qla27xx_read_off ( reg , 0xc4 , buf , len ); <nl> - addr ++; <nl> + addr += sizeof ( uint32_t ); <nl> } <nl>  <nl> return false ;
static int shash_update_unaligned ( struct shash_desc * desc , const u8 * data , <nl> u8 buf [ shash_align_buffer_size ( unaligned_len , alignmask )] <nl> __attribute__ (( aligned )); <nl>  <nl> + if ( unaligned_len > len ) <nl> + unaligned_len = len ; <nl> + <nl> memcpy ( buf , data , unaligned_len ); <nl>  <nl> return shash -> update ( desc , buf , unaligned_len ) ?:
int key_reject_and_link ( struct key * key , <nl>  <nl> mutex_unlock (& key_construction_mutex ); <nl>  <nl> - if ( keyring ) <nl> + if ( keyring && link_ret == 0 ) <nl> __key_link_end ( keyring , & key -> index_key , edit ); <nl>  <nl> /* wake up anyone waiting for a key to be constructed */
static ssize_t dfs_file_write ( struct file * file , const char __user * user_buf , <nl> } else if ( dent == d -> dfs_emulate_power_cut ) { <nl> if ( kstrtoint ( buf , 0 , & val ) != 0 ) <nl> count = - EINVAL ; <nl> - d -> emulate_power_cut = val ; <nl> + else <nl> + d -> emulate_power_cut = val ; <nl> goto out ; <nl> } <nl> 
struct kvm_vcpu * kvm_arch_vcpu_create ( struct kvm * kvm , unsigned int id ) <nl> int err ; <nl> struct kvm_vcpu * vcpu ; <nl>  <nl> + if ( irqchip_in_kernel ( kvm ) && vgic_initialized ( kvm )) { <nl> + err = - EBUSY ; <nl> + goto out ; <nl> + } <nl> + <nl> vcpu = kmem_cache_zalloc ( kvm_vcpu_cache , GFP_KERNEL ); <nl> if (! vcpu ) { <nl> err = - ENOMEM ;
i915_gem_userptr_ioctl ( struct drm_device * dev , <nl> I915_USERPTR_UNSYNCHRONIZED )) <nl> return - EINVAL ; <nl>  <nl> + if (! args -> user_size ) <nl> + return - EINVAL ; <nl> + <nl> if ( offset_in_page ( args -> user_ptr | args -> user_size )) <nl> return - EINVAL ; <nl> 
struct lock_chain { <nl> }; <nl>  <nl> # define MAX_LOCKDEP_KEYS_BITS 11 <nl> -# define MAX_LOCKDEP_KEYS ( 1UL << MAX_LOCKDEP_KEYS_BITS ) <nl> +/* <nl> + * Subtract one because we offset hlock -> class_idx by 1 in order <nl> + * to make 0 mean no class . This avoids overflowing the class_idx <nl> + * bitfield and hitting the BUG in hlock_class (). <nl> + */ <nl> +# define MAX_LOCKDEP_KEYS (( 1UL << MAX_LOCKDEP_KEYS_BITS ) - 1 ) <nl>  <nl> struct held_lock { <nl> /*
static int ipxitf_ioctl ( unsigned int cmd , void __user * arg ) <nl> sipx -> sipx_network = ipxif -> if_netnum ; <nl> memcpy ( sipx -> sipx_node , ipxif -> if_node , <nl> sizeof ( sipx -> sipx_node )); <nl> - rc = - EFAULT ; <nl> + rc = 0 ; <nl> if ( copy_to_user ( arg , & ifr , sizeof ( ifr ))) <nl> - break ; <nl> + rc = - EFAULT ; <nl> ipxitf_put ( ipxif ); <nl> - rc = 0 ; <nl> break ; <nl> } <nl> case SIOCAIPXITFCRT :
void dccp_close ( struct sock * sk , long timeout ) <nl> __kfree_skb ( skb ); <nl> } <nl>  <nl> + /* If socket has been already reset kill it . */ <nl> + if ( sk -> sk_state == DCCP_CLOSED ) <nl> + goto adjudge_to_death ; <nl> + <nl> if ( data_was_unread ) { <nl> /* Unread data was tossed , send an appropriate Reset Code */ <nl> DCCP_WARN (" ABORT with % u bytes unread \ n ", data_was_unread );
static int amd_gpio_remove ( struct platform_device * pdev ) <nl> gpio_dev = platform_get_drvdata ( pdev ); <nl>  <nl> gpiochip_remove (& gpio_dev -> gc ); <nl> - pinctrl_unregister ( gpio_dev -> pctrl ); <nl>  <nl> return 0 ; <nl> }
static struct rds_connection * __rds_conn_create ( struct net * net , <nl> * should end up here , but if it <nl> * does , reset / destroy the connection . <nl> */ <nl> + kfree ( conn -> c_path ); <nl> kmem_cache_free ( rds_conn_slab , conn ); <nl> conn = ERR_PTR (- EOPNOTSUPP ); <nl> goto out ;
static struct clk * __init clkgen_odf_register ( const char * parent_name , <nl> gate -> lock = odf_lock ; <nl>  <nl> div = kzalloc ( sizeof (* div ), GFP_KERNEL ); <nl> - if (! div ) <nl> + if (! div ) { <nl> + kfree ( gate ); <nl> return ERR_PTR (- ENOMEM ); <nl> + } <nl>  <nl> div -> flags = CLK_DIVIDER_ONE_BASED | CLK_DIVIDER_ALLOW_ZERO ; <nl> div -> reg = reg + pll_data -> odf [ odf ]. offset ;
static inline void ipv6_store_devconf ( struct ipv6_devconf * cnf , <nl> # endif <nl> array [ DEVCONF_DISABLE_IPV6 ] = cnf -> disable_ipv6 ; <nl> array [ DEVCONF_ACCEPT_DAD ] = cnf -> accept_dad ; <nl> + array [ DEVCONF_FORCE_TLLAO ] = cnf -> force_tllao ; <nl> } <nl>  <nl> static inline size_t inet6_if_nlmsg_size ( void )
static int ci_hdrc_remove ( struct platform_device * pdev ) <nl> dbg_remove_files ( ci ); <nl> free_irq ( ci -> irq , ci ); <nl> ci_role_destroy ( ci ); <nl> + kfree ( ci -> hw_bank . regmap ); <nl>  <nl> return 0 ; <nl> }
typedef struct { <nl> uint8_t max_lun ; <nl>  <nl> uint32_t unique_id ; <nl> - uint8_t irq ; <nl> + int irq ; <nl> uint8_t ito ; <nl> caddr_t ibuf ; <nl> dma_addr_t ibuf_dma_h ;
static int __unioxx5_subdev_init ( struct comedi_device * dev , <nl> return - ENOMEM ; <nl>  <nl> ret = __comedi_request_region ( dev , iobase , UNIOXX5_SIZE ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + kfree ( usp ); <nl> return ret ; <nl> + } <nl> usp -> usp_iobase = iobase ; <nl>  <nl> /* defining modules types */
static int skcipher_sendmsg ( struct socket * sock , struct msghdr * msg , <nl>  <nl> sgl = list_entry ( ctx -> tsgl . prev , struct skcipher_sg_list , list ); <nl> sg = sgl -> sg ; <nl> - sg_unmark_end ( sg + sgl -> cur ); <nl> + if ( sgl -> cur ) <nl> + sg_unmark_end ( sg + sgl -> cur - 1 ); <nl> do { <nl> i = sgl -> cur ; <nl> plen = min_t ( size_t , len , PAGE_SIZE );
static int das16cs_attach ( struct comedi_device * dev , <nl> dev -> driver -> driver_name , dev -> board_name , <nl> dev -> iobase , dev -> irq ); <nl>  <nl> - return 1 ; <nl> + return 0 ; <nl> } <nl>  <nl> static void das16cs_detach ( struct comedi_device * dev )
fst_get_iface ( struct fst_card_info * card , struct fst_port_info * port , <nl> } <nl>  <nl> i = port -> index ; <nl> + memset (& sync , 0 , sizeof ( sync )); <nl> sync . clock_rate = FST_RDL ( card , portConfig [ i ]. lineSpeed ); <nl> /* Lucky card and linux use same encoding here */ <nl> sync . clock_type = FST_RDB ( card , portConfig [ i ]. internalClock ) ==
# include " tg3 . h " <nl>  <nl> # define DRV_MODULE_NAME " tg3 " <nl> -# define DRV_MODULE_VERSION " 3 . 108 " <nl> -# define DRV_MODULE_RELDATE " February 17 , 2010 " <nl> +# define DRV_MODULE_VERSION " 3 . 109 " <nl> +# define DRV_MODULE_RELDATE " April 2 , 2010 " <nl>  <nl> # define TG3_DEF_MAC_MODE 0 <nl> # define TG3_DEF_RX_MODE 0
static void pmf_gpio_set_ ## name ( struct gpio_runtime * rt , int on )\ <nl> \ <nl> if ( unlikely (! rt )) return ; \ <nl> rc = pmf_call_function ( rt -> node , # name "- mute ", & args ); \ <nl> - if ( rc ) \ <nl> + if ( rc && rc != - ENODEV ) \ <nl> printk ( KERN_WARNING " pmf_gpio_set_ " # name \ <nl> " failed , rc : % d \ n ", rc ); \ <nl> rt -> implementation_private &= ~( 1 << bit ); \
static int perf_copy_attr ( struct perf_event_attr __user * uattr , <nl> if ( ret ) <nl> return - EFAULT ; <nl>  <nl> + attr -> size = size ; <nl> + <nl> if ( attr -> __reserved_1 ) <nl> return - EINVAL ; <nl> 
int vfs_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl> bool new_is_dir = false ; <nl> unsigned max_links = new_dir -> i_sb -> s_max_links ; <nl>  <nl> - if ( source == target ) <nl> + /* <nl> + * Check source == target . <nl> + * On overlayfs need to look at underlying inodes . <nl> + */ <nl> + if ( vfs_select_inode ( old_dentry , 0 ) == vfs_select_inode ( new_dentry , 0 )) <nl> return 0 ; <nl>  <nl> error = may_delete ( old_dir , old_dentry , is_dir );
int ocfs2_xattr_get_nolock ( struct inode * inode , <nl> return - EOPNOTSUPP ; <nl>  <nl> if (!( oi -> ip_dyn_features & OCFS2_HAS_XATTR_FL )) <nl> - ret = - ENODATA ; <nl> + return - ENODATA ; <nl>  <nl> xis . inode_bh = xbs . inode_bh = di_bh ; <nl> di = ( struct ocfs2_dinode *) di_bh -> b_data ;
static void release_one_tty ( struct work_struct * work ) <nl> list_del_init (& tty -> tty_files ); <nl> file_list_unlock (); <nl>  <nl> + put_pid ( tty -> pgrp ); <nl> + put_pid ( tty -> session ); <nl> free_tty_struct ( tty ); <nl> } <nl> 
static int iommu_map_page ( struct protection_domain * dom , <nl> count = PAGE_SIZE_PTE_COUNT ( page_size ); <nl> pte = alloc_pte ( dom , bus_addr , page_size , NULL , GFP_KERNEL ); <nl>  <nl> + if (! pte ) <nl> + return - ENOMEM ; <nl> + <nl> for ( i = 0 ; i < count ; ++ i ) <nl> if ( IOMMU_PTE_PRESENT ( pte [ i ])) <nl> return - EBUSY ;
static int vhci_hub_status ( struct usb_hcd * hcd , char * buf ) <nl>  <nl> pr_info (" changed % d \ n ", changed ); <nl>  <nl> - if ( hcd -> state == HC_STATE_SUSPENDED ) <nl> + if (( hcd -> state == HC_STATE_SUSPENDED ) && ( changed == 1 )) <nl> usb_hcd_resume_root_hub ( hcd ); <nl>  <nl> done :
vfs_removexattr ( struct dentry * dentry , const char * name ) <nl> if ( error ) <nl> return error ; <nl>  <nl> + mutex_lock (& inode -> i_mutex ); <nl> error = security_inode_removexattr ( dentry , name ); <nl> - if ( error ) <nl> + if ( error ) { <nl> + mutex_unlock (& inode -> i_mutex ); <nl> return error ; <nl> + } <nl>  <nl> - mutex_lock (& inode -> i_mutex ); <nl> error = inode -> i_op -> removexattr ( dentry , name ); <nl> mutex_unlock (& inode -> i_mutex ); <nl> 
static long vop_ioctl ( struct file * f , unsigned int cmd , unsigned long arg ) <nl> ret = - EFAULT ; <nl> goto free_ret ; <nl> } <nl> + /* Ensure desc has not changed between the two reads */ <nl> + if ( memcmp (& dd , dd_config , sizeof ( dd ))) { <nl> + ret = - EINVAL ; <nl> + goto free_ret ; <nl> + } <nl> mutex_lock (& vdev -> vdev_mutex ); <nl> mutex_lock (& vi -> vop_mutex ); <nl> ret = vop_virtio_add_device ( vdev , dd_config );
static int gbaudio_codec_probe ( struct gb_connection * connection ) <nl> kfree ( topology ); <nl> base_error : <nl> gbcodec -> mgmt_connection = NULL ; <nl> + gbaudio_free_codec ( dev , gbcodec ); <nl> return ret ; <nl> } <nl> 
static int goldfish_fb_remove ( struct platform_device * pdev ) <nl> dma_free_coherent (& pdev -> dev , framesize , ( void *) fb -> fb . screen_base , <nl> fb -> fb . fix . smem_start ); <nl> iounmap ( fb -> reg_base ); <nl> + kfree ( fb ); <nl> return 0 ; <nl> } <nl> 
static int tty_open ( struct inode * inode , struct file * filp ) <nl> if ( IS_ERR ( tty )) { <nl> tty_unlock (); <nl> mutex_unlock (& tty_mutex ); <nl> + tty_driver_kref_put ( driver ); <nl> return PTR_ERR ( tty ); <nl> } <nl> }
struct iio_channel * iio_channel_get ( const char * name , const char * channel_name ) <nl> if ( c == NULL ) <nl> return ERR_PTR (- ENODEV ); <nl>  <nl> - channel = kmalloc ( sizeof (* channel ), GFP_KERNEL ); <nl> + channel = kzalloc ( sizeof (* channel ), GFP_KERNEL ); <nl> if ( channel == NULL ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
bfad_im_get_stats ( struct Scsi_Host * shost ) <nl> rc = bfa_port_get_stats ( BFA_FCPORT (& bfad -> bfa ), <nl> fcstats , bfad_hcb_comp , & fcomp ); <nl> spin_unlock_irqrestore (& bfad -> bfad_lock , flags ); <nl> - if ( rc != BFA_STATUS_OK ) <nl> + if ( rc != BFA_STATUS_OK ) { <nl> + kfree ( fcstats ); <nl> return NULL ; <nl> + } <nl>  <nl> wait_for_completion (& fcomp . comp ); <nl> 
static int is_valid_state_transition ( struct drbd_conf * mdev , <nl> os . conn < C_CONNECTED ) <nl> rv = SS_NEED_CONNECTION ; <nl>  <nl> + if (( ns . conn == C_SYNC_TARGET || ns . conn == C_SYNC_SOURCE ) <nl> + && os . conn < C_WF_REPORT_PARAMS ) <nl> + rv = SS_NEED_CONNECTION ; /* No NetworkFailure -> SyncTarget etc ... */ <nl> + <nl> return rv ; <nl> } <nl> 
void global_dirty_limits ( unsigned long * pbackground , unsigned long * pdirty ) <nl> { <nl> unsigned long background ; <nl> unsigned long dirty ; <nl> - unsigned long available_memory = determine_dirtyable_memory (); <nl> + unsigned long uninitialized_var ( available_memory ); <nl> struct task_struct * tsk ; <nl>  <nl> + if (! vm_dirty_bytes || ! dirty_background_bytes ) <nl> + available_memory = determine_dirtyable_memory (); <nl> + <nl> if ( vm_dirty_bytes ) <nl> dirty = DIV_ROUND_UP ( vm_dirty_bytes , PAGE_SIZE ); <nl> else
static void intel_hdmi_mode_set ( struct intel_encoder * encoder ) <nl> else <nl> hdmi_val |= SDVO_COLOR_FORMAT_8bpc ; <nl>  <nl> - /* Required on CPT */ <nl> - if ( intel_hdmi -> has_hdmi_sink && HAS_PCH_CPT ( dev )) <nl> + if ( intel_hdmi -> has_hdmi_sink && <nl> + ( HAS_PCH_CPT ( dev ) || IS_VALLEYVIEW ( dev ))) <nl> hdmi_val |= HDMI_MODE_SELECT_HDMI ; <nl>  <nl> if ( intel_hdmi -> has_audio ) {
static int sm501fb_start ( struct sm501fb_info * info , <nl> info -> fbmem_len = resource_size ( res ); <nl>  <nl> /* clear framebuffer memory - avoids garbage data on unused fb */ <nl> - memset ( info -> fbmem , 0 , info -> fbmem_len ); <nl> + memset_io ( info -> fbmem , 0 , info -> fbmem_len ); <nl>  <nl> /* clear palette ram - undefined at power on */ <nl> for ( k = 0 ; k < ( 256 * 3 ); k ++)
static int davinci_wdt_probe ( struct platform_device * pdev ) <nl> return PTR_ERR ( davinci_wdt -> clk ); <nl> } <nl>  <nl> - clk_prepare_enable ( davinci_wdt -> clk ); <nl> + ret = clk_prepare_enable ( davinci_wdt -> clk ); <nl> + if ( ret ) { <nl> + dev_err (& pdev -> dev , " failed to prepare clock \ n "); <nl> + return ret ; <nl> + } <nl>  <nl> platform_set_drvdata ( pdev , davinci_wdt ); <nl> 
EXPORT_SYMBOL ( vio_unregister_driver ); <nl> /* vio_dev refcount hit 0 */ <nl> static void __devinit vio_dev_release ( struct device * dev ) <nl> { <nl> - if ( dev -> archdata . of_node ) { <nl> - /* XXX should free TCE table */ <nl> - of_node_put ( dev -> archdata . of_node ); <nl> - } <nl> + /* XXX should free TCE table */ <nl> + of_node_put ( dev -> archdata . of_node ); <nl> kfree ( to_vio_dev ( dev )); <nl> } <nl> 
static int wil_cfg80211_stop_ap ( struct wiphy * wiphy , <nl> wil6210_bus_request ( wil , WIL_DEFAULT_BUS_REQUEST_KBPS ); <nl> wil_set_recovery_state ( wil , fw_recovery_idle ); <nl>  <nl> + set_bit ( wil_status_resetting , wil -> status ); <nl> + <nl> mutex_lock (& wil -> mutex ); <nl>  <nl> wmi_pcp_stop ( wil );
struct pci_dn * handle_eeh_events ( struct eeh_event * event ) <nl> } <nl>  <nl> /* All devices should claim they have recovered by now . */ <nl> - if ( result != PCI_ERS_RESULT_RECOVERED ) { <nl> + if (( result != PCI_ERS_RESULT_RECOVERED ) && <nl> + ( result != PCI_ERS_RESULT_NONE )) { <nl> printk ( KERN_WARNING " EEH : Not recovered \ n "); <nl> goto hard_fail ; <nl> }
static int drbd_asb_recover_0p ( struct drbd_conf * mdev ) __must_hold ( local ) <nl> break ; <nl> } <nl> /* Else fall through to one of the other strategies ... */ <nl> - dev_warn ( DEV , " Discard younger / older primary did not found a decision \ n " <nl> + dev_warn ( DEV , " Discard younger / older primary did not find a decision \ n " <nl> " Using discard - least - changes instead \ n "); <nl> case ASB_DISCARD_ZERO_CHG : <nl> if ( ch_peer == 0 && ch_self == 0 ) {
static int __build_sched_domains ( const cpumask_t * cpu_map , <nl> error : <nl> free_sched_groups ( cpu_map , tmpmask ); <nl> SCHED_CPUMASK_FREE (( void *) allmasks ); <nl> + kfree ( rd ); <nl> return - ENOMEM ; <nl> # endif <nl> }
static int snd_compr_allocate_buffer ( struct snd_compr_stream * stream , <nl> unsigned int buffer_size ; <nl> void * buffer ; <nl>  <nl> + if ( params -> buffer . fragment_size == 0 || <nl> + params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + return - EINVAL ; <nl> + <nl> buffer_size = params -> buffer . fragment_size * params -> buffer . fragments ; <nl> if ( stream -> ops -> copy ) { <nl> buffer = NULL ;
static void dwc2_hcd_cleanup_channels ( struct dwc2_hsotg * hsotg ) <nl> */ <nl> channel -> qh = NULL ; <nl> } <nl> + /* All channels have been freed , mark them available */ <nl> + if ( hsotg -> core_params -> uframe_sched > 0 ) { <nl> + hsotg -> available_host_channels = <nl> + hsotg -> core_params -> host_channels ; <nl> + } else { <nl> + hsotg -> non_periodic_channels = 0 ; <nl> + hsotg -> periodic_channels = 0 ; <nl> + } <nl> } <nl>  <nl> /**
static void oz_usb_handle_ep_data ( struct oz_usb_ctx * usb_ctx , <nl> struct oz_multiple_fixed * body = <nl> ( struct oz_multiple_fixed *) data_hdr ; <nl> u8 * data = body -> data ; <nl> - int n = ( len - sizeof ( struct oz_multiple_fixed )+ 1 ) <nl> + int n ; <nl> + if (! body -> unit_size ) <nl> + break ; <nl> + n = ( len - sizeof ( struct oz_multiple_fixed )+ 1 ) <nl> / body -> unit_size ; <nl> while ( n --) { <nl> oz_hcd_data_ind ( usb_ctx -> hport , body -> endpoint ,
static const struct pinmux_ops sunxi_pmx_ops = { <nl> . get_function_groups = sunxi_pmx_get_func_groups , <nl> . set_mux = sunxi_pmx_set_mux , <nl> . gpio_set_direction = sunxi_pmx_gpio_set_direction , <nl> + . strict = true , <nl> }; <nl>  <nl> static int sunxi_pinctrl_gpio_direction_input ( struct gpio_chip * chip ,
int MoxaDriverIoctl ( unsigned int cmd , unsigned long arg , int port ) <nl> case MOXA_FIND_BOARD : <nl> case MOXA_LOAD_C320B : <nl> case MOXA_LOAD_CODE : <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> break ; <nl> } <nl> 
static int emc1403_detect ( struct i2c_client * client , <nl> } <nl>  <nl> id = i2c_smbus_read_byte_data ( client , THERMAL_REVISION_REG ); <nl> - if ( id != 0x01 ) <nl> + if ( id < 0x01 || id > 0x04 ) <nl> return - ENODEV ; <nl>  <nl> return 0 ;
static int spear_cpufreq_target ( struct cpufreq_policy * policy , <nl> } <nl>  <nl> newfreq = clk_round_rate ( srcclk , newfreq * mult ); <nl> - if ( newfreq < 0 ) { <nl> + if ( newfreq <= 0 ) { <nl> pr_err (" clk_round_rate failed for cpu src clock \ n "); <nl> return newfreq ; <nl> }
static void compat_input ( struct dlm_write_request * kb , <nl> static void compat_output ( struct dlm_lock_result * res , <nl> struct dlm_lock_result32 * res32 ) <nl> { <nl> + memset ( res32 , 0 , sizeof (* res32 )); <nl> + <nl> res32 -> version [ 0 ] = res -> version [ 0 ]; <nl> res32 -> version [ 1 ] = res -> version [ 1 ]; <nl> res32 -> version [ 2 ] = res -> version [ 2 ];
static inline void intel_ring_emit_wa ( struct intel_engine_cs * ring , <nl> struct drm_device * dev = ring -> dev ; <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl>  <nl> - if ( dev_priv -> num_wa_regs > I915_MAX_WA_REGS ) <nl> + if ( dev_priv -> num_wa_regs >= I915_MAX_WA_REGS ) <nl> return ; <nl>  <nl> intel_ring_emit ( ring , MI_LOAD_REGISTER_IMM ( 1 ));
int vfs_quota_on_mount ( struct super_block * sb , char * qf_name , <nl> if ( IS_ERR ( dentry )) <nl> return PTR_ERR ( dentry ); <nl>  <nl> + if (! dentry -> d_inode ) { <nl> + error = - ENOENT ; <nl> + goto out ; <nl> + } <nl> + <nl> error = security_quota_on ( dentry ); <nl> if (! error ) <nl> error = vfs_quota_on_inode ( dentry -> d_inode , type , format_id ); <nl>  <nl> + out : <nl> dput ( dentry ); <nl> return error ; <nl> }
static void ar9003_hw_prog_ini ( struct ath_hw * ah , <nl> u32 val = INI_RA ( iniArr , i , column ); <nl>  <nl> REG_WRITE ( ah , reg , val ); <nl> - <nl> - /* <nl> - * Determine if this is a shift register value , and insert the <nl> - * configured delay if so . <nl> - */ <nl> - if ( reg >= 0x16000 && reg < 0x17000 <nl> - && ah -> config . analog_shiftreg ) <nl> - udelay ( 100 ); <nl> - <nl> DO_DELAY ( regWrites ); <nl> } <nl> }
static int virtio_gpu_object_shmem_init ( struct virtio_gpu_device * vgdev , <nl> * since virtio_gpu doesn ' t support dma - buf import from other devices . <nl> */ <nl> shmem -> pages = drm_gem_shmem_get_sg_table (& bo -> base ); <nl> - if (! shmem -> pages ) { <nl> + if ( IS_ERR ( shmem -> pages )) { <nl> drm_gem_shmem_unpin (& bo -> base ); <nl> - return - EINVAL ; <nl> + return PTR_ERR ( shmem -> pages ); <nl> } <nl>  <nl> if ( use_dma_api ) {
EXPORT_SYMBOL_GPL ( mnt_clone_write ); <nl> */ <nl> int mnt_want_write_file ( struct file * file ) <nl> { <nl> - if (!( file -> f_mode & FMODE_WRITE )) <nl> + struct inode * inode = file -> f_dentry -> d_inode ; <nl> + if (!( file -> f_mode & FMODE_WRITE ) || special_file ( inode -> i_mode )) <nl> return mnt_want_write ( file -> f_path . mnt ); <nl> else <nl> return mnt_clone_write ( file -> f_path . mnt );
struct edac_pci_ctl_info * edac_pci_create_generic_ctl ( struct device * dev , <nl>  <nl> pci -> mod_name = mod_name ; <nl> pci -> ctl_name = EDAC_PCI_GENCTL_NAME ; <nl> - pci -> edac_check = edac_pci_generic_check ; <nl> + if ( edac_op_state == EDAC_OPSTATE_POLL ) <nl> + pci -> edac_check = edac_pci_generic_check ; <nl>  <nl> pdata -> edac_idx = edac_pci_idx ++; <nl> 
static noinline int __btrfs_cow_block ( struct btrfs_trans_handle * trans , <nl> btrfs_set_node_ptr_generation ( parent , parent_slot , <nl> trans -> transid ); <nl> btrfs_mark_buffer_dirty ( parent ); <nl> - tree_mod_log_free_eb ( root -> fs_info , buf ); <nl> + if ( last_ref ) <nl> + tree_mod_log_free_eb ( root -> fs_info , buf ); <nl> btrfs_free_tree_block ( trans , root , buf , parent_start , <nl> last_ref ); <nl> }
static int do_ip_setsockopt ( struct sock * sk , int level , <nl> * Check the arguments are allowable <nl> */ <nl>  <nl> + if ( optlen < sizeof ( struct in_addr )) <nl> + goto e_inval ; <nl> + <nl> err = - EFAULT ; <nl> if ( optlen >= sizeof ( struct ip_mreqn )) { <nl> if ( copy_from_user (& mreq , optval , sizeof ( mreq )))
struct hid_device * hid_allocate_device ( void ) <nl> device_initialize (& hdev -> dev ); <nl> hdev -> dev . release = hid_device_release ; <nl> hdev -> dev . bus = & hid_bus_type ; <nl> + device_enable_async_suspend (& hdev -> dev ); <nl>  <nl> hid_close_report ( hdev ); <nl> 
do_send_specific ( pid_t tgid , pid_t pid , int sig , struct siginfo * info ) <nl>  <nl> static int do_tkill ( pid_t tgid , pid_t pid , int sig ) <nl> { <nl> - struct siginfo info ; <nl> + struct siginfo info = {}; <nl>  <nl> info . si_signo = sig ; <nl> info . si_errno = 0 ;
int nvdimm_has_flush ( struct nd_region * nd_region ) <nl> { <nl> int i ; <nl>  <nl> - /* no nvdimm == flushing capability unknown */ <nl> - if ( nd_region -> ndr_mappings == 0 ) <nl> + /* no nvdimm or pmem api == flushing capability unknown */ <nl> + if ( nd_region -> ndr_mappings == 0 <nl> + || ! IS_ENABLED ( CONFIG_ARCH_HAS_PMEM_API )) <nl> return - ENXIO ; <nl>  <nl> for ( i = 0 ; i < nd_region -> ndr_mappings ; i ++) {
static struct irq_desc * __real_move_irq_desc ( struct irq_desc * old_desc , <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
static int pulse8_cec_adap_transmit ( struct cec_adapter * adap , u8 attempts , <nl> int err ; <nl>  <nl> cmd [ 0 ] = MSGCODE_TRANSMIT_IDLETIME ; <nl> - cmd [ 1 ] = 3 ; <nl> + cmd [ 1 ] = signal_free_time ; <nl> err = pulse8_send_and_wait ( pulse8 , cmd , 2 , <nl> MSGCODE_COMMAND_ACCEPTED , 1 ); <nl> cmd [ 0 ] = MSGCODE_TRANSMIT_ACK_POLARITY ;
int smb2_tree_disconnect ( struct ksmbd_work * work ) <nl>  <nl> ksmbd_close_tree_conn_fds ( work ); <nl> ksmbd_tree_conn_disconnect ( sess , tcon ); <nl> + work -> tcon = NULL ; <nl> return 0 ; <nl> } <nl> 
static int upgrade_fw ( struct adapter * adap ) <nl> if (! ret ) <nl> dev_info ( dev , " firmware upgraded to version % pI4 from " <nl> FW_FNAME "\ n ", & hdr -> fw_ver ); <nl> + } else { <nl> + /* <nl> + * Tell our caller that we didn ' t upgrade the firmware . <nl> + */ <nl> + ret = - EINVAL ; <nl> } <nl> + <nl> out : release_firmware ( fw ); <nl> return ret ; <nl> }
static void ks_sdio_interrupt ( struct sdio_func * func ) <nl> int ret ; <nl> struct ks_sdio_card * card ; <nl> struct ks_wlan_private * priv ; <nl> - unsigned char status , rsize , byte ; <nl> + u8 status , rsize , byte ; <nl>  <nl> card = sdio_get_drvdata ( func ); <nl> priv = card -> priv ;
static void __ccw_device_pm_restore ( struct ccw_device * cdev ) <nl> * available again . Kick re - detection . <nl> */ <nl> cdev -> private -> flags . resuming = 1 ; <nl> + cdev -> private -> path_new_mask = LPM_ANYPATH ; <nl> css_schedule_eval ( sch -> schid ); <nl> spin_unlock_irq ( sch -> lock ); <nl> css_complete_work ();
static int pcnet32_open ( struct net_device * dev ) <nl> lp -> rx_dma_addr [ i ] = 0 ; <nl> } <nl>  <nl> - pcnet32_free_ring ( dev ); <nl> - <nl> /* <nl> * Switch back to 16bit mode to avoid problems with dumb <nl> * DOS packet driver after a warm reboot
static int __get_data_block ( struct inode * inode , sector_t iblock , <nl> if (! err ) { <nl> map_bh ( bh , inode -> i_sb , map . m_pblk ); <nl> bh -> b_state = ( bh -> b_state & ~ F2FS_MAP_FLAGS ) | map . m_flags ; <nl> - bh -> b_size = map . m_len << inode -> i_blkbits ; <nl> + bh -> b_size = ( u64 ) map . m_len << inode -> i_blkbits ; <nl> } <nl> return err ; <nl> }
void rtw_alloc_hwxmits ( struct adapter * padapter ) <nl>  <nl> pxmitpriv -> hwxmit_entry = HWXMIT_ENTRY ; <nl>  <nl> - pxmitpriv -> hwxmits = kzalloc ( sizeof ( struct hw_xmit ) * pxmitpriv -> hwxmit_entry , GFP_KERNEL ); <nl> + pxmitpriv -> hwxmits = kcalloc ( pxmitpriv -> hwxmit_entry , <nl> + sizeof ( struct hw_xmit ), GFP_KERNEL ); <nl>  <nl> hwxmits = pxmitpriv -> hwxmits ; <nl> 
static inline void activate_mm ( struct mm_struct * old , struct mm_struct * new ) <nl> * possible . <nl> */ <nl> if ( old != new && ( current -> flags & PF_BORROWED_MM )) <nl> - force_flush_all (); <nl> + CHOOSE_MODE ( force_flush_all (), <nl> + switch_mm_skas (& new -> context . skas . id )); <nl> } <nl>  <nl> static inline void switch_mm ( struct mm_struct * prev , struct mm_struct * next ,
static int efx_ef10_probe ( struct efx_nic * efx ) <nl> EFX_MAX_CHANNELS , <nl> resource_size (& efx -> pci_dev -> resource [ EFX_MEM_BAR ]) / <nl> ( EFX_VI_PAGE_SIZE * EFX_TXQ_TYPES )); <nl> - BUG_ON ( efx -> max_channels == 0 ); <nl> + if ( WARN_ON ( efx -> max_channels == 0 )) <nl> + return - EIO ; <nl>  <nl> nic_data = kzalloc ( sizeof (* nic_data ), GFP_KERNEL ); <nl> if (! nic_data )
static void amdgpu_gem_va_update_vm ( struct amdgpu_device * adev , <nl> goto error ; <nl>  <nl> if ( operation == AMDGPU_VA_OP_MAP || <nl> - operation == AMDGPU_VA_OP_REPLACE ) <nl> + operation == AMDGPU_VA_OP_REPLACE ) { <nl> r = amdgpu_vm_bo_update ( adev , bo_va , false ); <nl> + if ( r ) <nl> + goto error ; <nl> + } <nl>  <nl> r = amdgpu_vm_update_directories ( adev , vm ); <nl> - if ( r ) <nl> - goto error ; <nl>  <nl> error : <nl> if ( r && r != - ERESTARTSYS )
static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> + memset ( sax , 0 , sizeof ( sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
u64 snd_usb_interface_dsd_format_quirks ( struct snd_usb_audio * chip , <nl> } <nl> } <nl> break ; <nl> + case USB_ID ( 0x16d0 , 0x0a23 ): <nl> + if ( fp -> altsetting == 2 ) <nl> + return SNDRV_PCM_FMTBIT_DSD_U32_BE ; <nl> + break ; <nl>  <nl> default : <nl> break ;
static int treo_attach ( struct usb_serial * serial ) <nl> ( serial -> num_interrupt_in == 0 )) <nl> return 0 ; <nl>  <nl> + if ( serial -> num_bulk_in < 2 || serial -> num_interrupt_in < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> /* <nl> * It appears that Treos and Kyoceras want to use the <nl> * 1st bulk in endpoint to communicate with the 2nd bulk out endpoint ,
static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
int of_get_fb_videomode ( struct device_node * np , struct fb_videomode * fb , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - fb_videomode_from_videomode (& vm , fb ); <nl> + ret = fb_videomode_from_videomode (& vm , fb ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> pr_debug ("% s : got % dx % d display mode from % s \ n ", <nl> of_node_full_name ( np ), vm . hactive , vm . vactive , np -> name );
static int easycap_usb_probe ( struct usb_interface * intf , <nl> /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */ <nl> pdata_urb = kzalloc ( sizeof ( struct data_urb ), GFP_KERNEL ); <nl> if (! pdata_urb ) { <nl> + usb_free_urb ( purb ); <nl> SAM (" ERROR : Could not allocate struct data_urb .\ n "); <nl> return - ENOMEM ; <nl> }
static const struct { <nl> . bDescriptorType = USB_DT_ENDPOINT , <nl> . bEndpointAddress = 1 | USB_DIR_IN , <nl> . bmAttributes = USB_ENDPOINT_XFER_BULK , <nl> + . wMaxPacketSize = htole16 ( 512 ), <nl> }, <nl> . bulk_source = { <nl> . bLength = sizeof ( descriptors . hs_descs . bulk_source ), <nl> . bDescriptorType = USB_DT_ENDPOINT , <nl> . bEndpointAddress = 2 | USB_DIR_OUT , <nl> . bmAttributes = USB_ENDPOINT_XFER_BULK , <nl> + . wMaxPacketSize = htole16 ( 512 ), <nl> }, <nl> }, <nl> };
void ide_do_drive_cmd ( ide_drive_t * drive , struct request * rq ) <nl>  <nl> spin_lock_irqsave ( q -> queue_lock , flags ); <nl> __elv_add_request ( q , rq , ELEVATOR_INSERT_FRONT , 0 ); <nl> - blk_start_queueing ( q ); <nl> spin_unlock_irqrestore ( q -> queue_lock , flags ); <nl> } <nl> EXPORT_SYMBOL ( ide_do_drive_cmd );
static void kick_tx ( int fd ) <nl> int ret ; <nl>  <nl> ret = sendto ( fd , NULL , 0 , MSG_DONTWAIT , NULL , 0 ); <nl> - if ( ret >= 0 || errno == ENOBUFS || errno == EAGAIN ) <nl> + if ( ret >= 0 || errno == ENOBUFS || errno == EAGAIN || errno == EBUSY ) <nl> return ; <nl> lassert ( 0 ); <nl> }
static int count_fastmap_pebs ( struct ubi_attach_info * ai ) <nl> list_for_each_entry ( aeb , & ai -> free , u . list ) <nl> n ++; <nl>  <nl> - ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> + ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> ubi_rb_for_each_entry ( rb2 , aeb , & av -> root , u . rb ) <nl> n ++; <nl> 
int i915_driver_load ( struct drm_device * dev , unsigned long flags ) <nl>  <nl> intel_irq_init ( dev ); <nl> intel_pm_init ( dev ); <nl> - intel_uncore_sanitize ( dev ); <nl> intel_uncore_init ( dev ); <nl> + intel_uncore_sanitize ( dev ); <nl>  <nl> /* Try to make sure MCHBAR is enabled before poking at it */ <nl> intel_setup_mchbar ( dev );
int snd_soc_dapm_device_event ( struct snd_soc_device * socdev , int event ) <nl> struct snd_soc_machine * machine = socdev -> machine ; <nl>  <nl> if ( machine -> dapm_event ) <nl> - machine -> dapm_event ( machine , event ); <nl> + machine -> dapm_event ( machine , event ); <nl> if ( codec -> dapm_event ) <nl> - codec -> dapm_event ( codec , event ); <nl> + codec -> dapm_event ( codec , event ); <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( snd_soc_dapm_device_event );
static int gen6_drpc_info ( struct seq_file * m ) <nl>  <nl> rpmodectl1 = I915_READ ( GEN6_RP_CONTROL ); <nl> rcctl1 = I915_READ ( GEN6_RC_CONTROL ); <nl> - sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> mutex_unlock (& dev -> struct_mutex ); <nl> + mutex_lock (& dev_priv -> rps . hw_lock ); <nl> + sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> + mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> seq_printf ( m , " Video Turbo Mode : % s \ n ", <nl> yesno ( rpmodectl1 & GEN6_RP_MEDIA_TURBO ));
static struct dentry * proc_mount ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb )) <nl> return ERR_CAST ( sb ); <nl>  <nl> + /* <nl> + * procfs isn ' t actually a stacking filesystem ; however , there is <nl> + * too much magic going on inside it to permit stacking things on <nl> + * top of it <nl> + */ <nl> + sb -> s_stack_depth = FILESYSTEM_MAX_STACK_DEPTH ; <nl> + <nl> if (! proc_parse_options ( options , ns )) { <nl> deactivate_locked_super ( sb ); <nl> return ERR_PTR (- EINVAL );
static int ipmi_fasync ( int fd , struct file * file , int on ) <nl> struct ipmi_file_private * priv = file -> private_data ; <nl> int result ; <nl>  <nl> + lock_kernel (); /* could race against open () otherwise */ <nl> result = fasync_helper ( fd , file , on , & priv -> fasync_queue ); <nl> + unlock_kernel (); <nl>  <nl> return ( result ); <nl> }
int cdc_parse_cdc_header ( struct usb_cdc_parsed_header * hdr , <nl> elength = 1 ; <nl> goto next_desc ; <nl> } <nl> + if (( buflen < elength ) || ( elength < 3 )) { <nl> + dev_err (& intf -> dev , " invalid descriptor buffer length \ n "); <nl> + break ; <nl> + } <nl> if ( buffer [ 1 ] != USB_DT_CS_INTERFACE ) { <nl> dev_err (& intf -> dev , " skipping garbage \ n "); <nl> goto next_desc ;
void __init tegra_super_clk_gen4_init ( void __iomem * clk_base , <nl> ARRAY_SIZE ( cclk_lp_parents ), <nl> CLK_SET_RATE_PARENT , <nl> clk_base + CCLKLP_BURST_POLICY , <nl> - 0 , 4 , 8 , 9 , NULL ); <nl> + TEGRA_DIVIDER_2 , 4 , 8 , 9 , NULL ); <nl> * dt_clk = clk ; <nl> } <nl> 
void * knav_pool_create ( const char * name , <nl> bool slot_found ; <nl> int ret ; <nl>  <nl> + if (! kdev ) <nl> + return ERR_PTR (- EPROBE_DEFER ); <nl> + <nl> if (! kdev -> dev ) <nl> return ERR_PTR (- ENODEV ); <nl> 
static const struct iio_chan_spec st_press_1_channels [] = { <nl> }, <nl> . info_mask_separate = <nl> BIT ( IIO_CHAN_INFO_RAW ) | BIT ( IIO_CHAN_INFO_SCALE ), <nl> + . info_mask_shared_by_all = BIT ( IIO_CHAN_INFO_SAMP_FREQ ), <nl> }, <nl> { <nl> . type = IIO_TEMP , <nl> static const struct iio_chan_spec st_press_1_channels [] = { <nl> BIT ( IIO_CHAN_INFO_RAW ) | <nl> BIT ( IIO_CHAN_INFO_SCALE ) | <nl> BIT ( IIO_CHAN_INFO_OFFSET ), <nl> + . info_mask_shared_by_all = BIT ( IIO_CHAN_INFO_SAMP_FREQ ), <nl> }, <nl> IIO_CHAN_SOFT_TIMESTAMP ( 2 ) <nl> };
void drm_modeset_acquire_init ( struct drm_modeset_acquire_ctx * ctx , <nl> uint32_t flags ) <nl> { <nl> + memset ( ctx , 0 , sizeof (* ctx )); <nl> ww_acquire_init (& ctx -> ww_ctx , & crtc_ww_class ); <nl> INIT_LIST_HEAD (& ctx -> locked ); <nl> }
struct qcom_glink * qcom_glink_native_probe ( struct device * dev , <nl> idr_init (& glink -> rcids ); <nl>  <nl> glink -> mbox_client . dev = dev ; <nl> + glink -> mbox_client . knows_txdone = true ; <nl> glink -> mbox_chan = mbox_request_channel (& glink -> mbox_client , 0 ); <nl> if ( IS_ERR ( glink -> mbox_chan )) { <nl> if ( PTR_ERR ( glink -> mbox_chan ) != - EPROBE_DEFER )
static void release_sub_crqs ( struct ibmvnic_adapter * adapter ) <nl> release_sub_crq_queue ( adapter , <nl> adapter -> tx_scrq [ i ]); <nl> } <nl> + kfree ( adapter -> tx_scrq ); <nl> adapter -> tx_scrq = NULL ; <nl> } <nl>  <nl> static void release_sub_crqs ( struct ibmvnic_adapter * adapter ) <nl> release_sub_crq_queue ( adapter , <nl> adapter -> rx_scrq [ i ]); <nl> } <nl> + kfree ( adapter -> rx_scrq ); <nl> adapter -> rx_scrq = NULL ; <nl> } <nl> }
efx_mcdi_mon_add_attr ( struct efx_nic * efx , const char * name , <nl> attr -> index = index ; <nl> attr -> type = type ; <nl> attr -> limit_value = limit_value ; <nl> + sysfs_attr_init (& attr -> dev_attr . attr ); <nl> attr -> dev_attr . attr . name = attr -> name ; <nl> attr -> dev_attr . attr . mode = S_IRUGO ; <nl> attr -> dev_attr . show = reader ;
static int mxcnd_probe ( struct platform_device * pdev ) <nl> init_completion (& host -> op_completion ); <nl>  <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq < 0 ) <nl> + return host -> irq ; <nl>  <nl> /* <nl> * Use host -> devtype_data -> irq_control () here instead of irq_control ()
static int __net_init __ip_vs_ftp_init ( struct net * net ) <nl> struct ip_vs_app * app ; <nl> struct netns_ipvs * ipvs = net_ipvs ( net ); <nl>  <nl> + if (! ipvs ) <nl> + return - ENOENT ; <nl> app = kmemdup (& ip_vs_ftp , sizeof ( struct ip_vs_app ), GFP_KERNEL ); <nl> if (! app ) <nl> return - ENOMEM ;
int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> write_unlock_bh (& bond -> curr_slave_lock ); <nl> read_unlock (& bond -> lock ); <nl> } <nl> + slave_disable_netpoll ( new_slave ); <nl>  <nl> err_close : <nl> slave_dev -> priv_flags &= ~ IFF_BONDING ;
static struct s3c_camif_drvdata s3c6410_camif_drvdata = { <nl> . bus_clk_freq = 133000000UL , <nl> }; <nl>  <nl> - static struct platform_device_id s3c_camif_driver_ids [] = { <nl> + static const struct platform_device_id s3c_camif_driver_ids [] = { <nl> { <nl> . name = " s3c2440 - camif ", <nl> . driver_data = ( unsigned long )& s3c244x_camif_drvdata ,
static ssize_t i40e_dbg_command_write ( struct file * filp , <nl> if (! cmd_buf ) <nl> return count ; <nl> bytes_not_copied = copy_from_user ( cmd_buf , buffer , count ); <nl> - if ( bytes_not_copied < 0 ) <nl> + if ( bytes_not_copied < 0 ) { <nl> + kfree ( cmd_buf ); <nl> return bytes_not_copied ; <nl> + } <nl> if ( bytes_not_copied > 0 ) <nl> count -= bytes_not_copied ; <nl> cmd_buf [ count ] = '\ 0 ';
static void __init __e820_add_region ( struct e820map * e820x , u64 start , u64 size , <nl> { <nl> int x = e820x -> nr_map ; <nl>  <nl> - if ( x == ARRAY_SIZE ( e820x -> map )) { <nl> + if ( x >= ARRAY_SIZE ( e820x -> map )) { <nl> printk ( KERN_ERR " Ooops ! Too many entries in the memory map !\ n "); <nl> return ; <nl> }
static int64_t _sort__sym_cmp ( struct symbol * sym_l , struct symbol * sym_r ) <nl> if ( sym_l == sym_r ) <nl> return 0 ; <nl>  <nl> + if ( sym_l -> inlined || sym_r -> inlined ) <nl> + return strcmp ( sym_l -> name , sym_r -> name ); <nl> + <nl> if ( sym_l -> start != sym_r -> start ) <nl> return ( int64_t )( sym_r -> start - sym_l -> start ); <nl> 
static int rdma_cma_handler ( struct rdma_cm_id * cma_id , <nl> if ( xprt ) { <nl> set_bit ( XPT_CLOSE , & xprt -> xpt_flags ); <nl> svc_xprt_enqueue ( xprt ); <nl> + svc_xprt_put ( xprt ); <nl> } <nl> break ; <nl> case RDMA_CM_EVENT_DEVICE_REMOVAL :
static void handle_swbp ( struct pt_regs * regs ) <nl> if ( unlikely (! test_bit ( UPROBE_COPY_INSN , & uprobe -> flags ))) <nl> goto out ; <nl>  <nl> + /* Tracing handlers use -> utask to communicate with fetch methods */ <nl> + if (! get_utask ()) <nl> + goto out ; <nl> + <nl> handler_chain ( uprobe , regs ); <nl> if ( can_skip_sstep ( uprobe , regs )) <nl> goto out ;
static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> struct ath_softc * sc = hw -> priv ; <nl> int ret = 0 ; <nl>  <nl> - local_bh_disable (); <nl> + mutex_lock (& sc -> mutex ); <nl>  <nl> switch ( action ) { <nl> case IEEE80211_AMPDU_RX_START : <nl> static int ath9k_ampdu_action ( struct ieee80211_hw * hw , <nl> ath_err ( ath9k_hw_common ( sc -> sc_ah ), " Unknown AMPDU action \ n "); <nl> } <nl>  <nl> - local_bh_enable (); <nl> + mutex_unlock (& sc -> mutex ); <nl>  <nl> return ret ; <nl> }
static int virtnet_probe ( struct virtio_device * vdev ) <nl> dev -> mtu = mtu ; <nl> dev -> max_mtu = mtu ; <nl> } <nl> + <nl> + /* TODO : size buffers correctly in this case . */ <nl> + if ( dev -> mtu > ETH_DATA_LEN ) <nl> + vi -> big_packets = true ; <nl> } <nl>  <nl> if ( vi -> any_header_sg )
int blkcg_init_queue ( struct request_queue * q ) <nl> if ( preloaded ) <nl> radix_tree_preload_end (); <nl>  <nl> - if ( IS_ERR ( blkg )) { <nl> - blkg_free ( new_blkg ); <nl> + if ( IS_ERR ( blkg )) <nl> return PTR_ERR ( blkg ); <nl> - } <nl>  <nl> q -> root_blkg = blkg ; <nl> q -> root_rl . blkg = blkg ;
# include < linux / hdmi . h > <nl> # include " hdmi . h " <nl>  <nl> - <nl> -/* Supported HDMI Audio channels */ <nl> -# define MSM_HDMI_AUDIO_CHANNEL_2 0 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_4 1 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_6 2 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_8 3 <nl> - <nl> /* maps MSM_HDMI_AUDIO_CHANNEL_n consts used by audio driver to # of channels : */ <nl> static int nchannels [] = { 2 , 4 , 6 , 8 }; <nl> 
int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> which_atu = 0 ; <nl> } <nl>  <nl> - if (! which_atu ) <nl> + if (! which_atu ) { <nl> + kfree ( res ); <nl> return 0 ; <nl> + } <nl>  <nl> switch ( which_atu ) { <nl> case IOP13XX_INIT_ATU_ATUX : <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> sys -> map_irq = iop13xx_pcie_map_irq ; <nl> break ; <nl> default : <nl> + kfree ( res ); <nl> return 0 ; <nl> } <nl> 
static int __init cy_detect_isa ( void ) <nl> continue ; <nl> } <nl> # ifdef MODULE <nl> - if ( isparam && irq [ i ]) <nl> + if ( isparam && i < NR_CARDS && irq [ i ]) <nl> cy_isa_irq = irq [ i ]; <nl> else <nl> # endif
static int dlm_add_member ( struct dlm_ls * ls , int nodeid ) <nl> return - ENOMEM ; <nl>  <nl> w = dlm_node_weight ( ls -> ls_name , nodeid ); <nl> - if ( w < 0 ) <nl> + if ( w < 0 ) { <nl> + kfree ( memb ); <nl> return w ; <nl> + } <nl>  <nl> memb -> nodeid = nodeid ; <nl> memb -> weight = w ;
int intel_svm_bind_mm ( struct device * dev , int * pasid , int flags , struct svm_dev_ <nl> pasid_max - 1 , GFP_KERNEL ); <nl> if ( ret < 0 ) { <nl> kfree ( svm ); <nl> + kfree ( sdev ); <nl> goto out ; <nl> } <nl> svm -> pasid = ret ;
static int acm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> else <nl> rv = 0 ; <nl>  <nl> + set_bit ( TTY_NO_WRITE_SPLIT , & tty -> flags ); <nl> tty -> driver_data = acm ; <nl> acm -> tty = tty ; <nl> 
long vhost_dev_ioctl ( struct vhost_dev * d , unsigned int ioctl , void __user * argp ) <nl> } <nl> if ( eventfp != d -> log_file ) { <nl> filep = d -> log_file ; <nl> + d -> log_file = eventfp ; <nl> ctx = d -> log_ctx ; <nl> d -> log_ctx = eventfp ? <nl> eventfd_ctx_fileget ( eventfp ) : NULL ;
static int __cpu_find_by ( int (* compare )( int , int , void *), void * compare_arg , <nl> int err = check_cpu_node ( dp -> node , & cur_inst , <nl> compare , compare_arg , <nl> prom_node , mid ); <nl> - if (! err ) <nl> + if (! err ) { <nl> + of_node_put ( dp ); <nl> return 0 ; <nl> + } <nl> } <nl>  <nl> return - ENODEV ;
bool kvm_vgic_map_is_active ( struct kvm_vcpu * vcpu , struct irq_phys_map * map ) <nl> return true ; <nl> } <nl>  <nl> - return dist_active_irq ( vcpu ); <nl> + return vgic_irq_is_active ( vcpu , map -> virt_irq ); <nl> } <nl>  <nl> /*
int x86_decode_insn ( struct x86_emulate_ctxt * ctxt , void * insn , int insn_len ) <nl> /* Decode and fetch the destination operand : register or memory . */ <nl> rc = decode_operand ( ctxt , & ctxt -> dst , ( ctxt -> d >> DstShift ) & OpMask ); <nl>  <nl> - done : <nl> if ( ctxt -> rip_relative ) <nl> ctxt -> memopp -> addr . mem . ea += ctxt -> _eip ; <nl>  <nl> + done : <nl> return ( rc != X86EMUL_CONTINUE ) ? EMULATION_FAILED : EMULATION_OK ; <nl> } <nl> 
void rtl8188eu_set_hal_ops ( struct adapter * adapt ) <nl>  <nl>  <nl> adapt -> HalData = kzalloc ( sizeof ( struct hal_data_8188e ), GFP_KERNEL ); <nl> - if ( adapt -> HalData == NULL ) <nl> + if (! adapt -> HalData ) <nl> DBG_88E (" cant not alloc memory for HAL DATA \ n "); <nl>  <nl> halfunc -> hal_power_on = rtl8188eu_InitPowerOn ;
static inline struct sk_buff * sk_stream_alloc_pskb ( struct sock * sk , <nl> { <nl> struct sk_buff * skb ; <nl>  <nl> + /* The TCP header must be at least 32 - bit aligned . */ <nl> + size = ALIGN ( size , 4 ); <nl> + <nl> skb = alloc_skb_fclone ( size + sk -> sk_prot -> max_header , gfp ); <nl> if ( skb ) { <nl> skb -> truesize += mem ;
static int __devinit lpc32xx_nand_probe ( struct platform_device * pdev ) <nl> dev_err (& pdev -> dev , " Missing platform data \ n "); <nl> return - ENOENT ; <nl> } <nl> + if ( host -> ncfg -> wp_gpio == - EPROBE_DEFER ) <nl> + return - EPROBE_DEFER ; <nl> if ( gpio_is_valid ( host -> ncfg -> wp_gpio ) && <nl> gpio_request ( host -> ncfg -> wp_gpio , " NAND WP ")) { <nl> dev_err (& pdev -> dev , " GPIO not available \ n ");
static int spear_smi_probe_config_dt ( struct platform_device * pdev , <nl> pdata -> board_flash_info = devm_kzalloc (& pdev -> dev , <nl> sizeof (* pdata -> board_flash_info ), <nl> GFP_KERNEL ); <nl> + if (! pdata -> board_flash_info ) <nl> + return - ENOMEM ; <nl>  <nl> /* Fill structs for each subnode ( flash device ) */ <nl> while (( pp = of_get_next_child ( np , pp ))) {
decompress_fn __init decompress_method ( const unsigned char * inbuf , long len , <nl> { <nl> const struct compress_format * cf ; <nl>  <nl> - if ( len < 2 ) <nl> + if ( len < 2 ) { <nl> + if ( name ) <nl> + * name = NULL ; <nl> return NULL ; /* Need at least this much ... */ <nl> + } <nl>  <nl> pr_debug (" Compressed data magic : %#. 2x %#. 2x \ n ", inbuf [ 0 ], inbuf [ 1 ]); <nl> 
int fscrypt_has_permitted_context ( struct inode * parent , struct inode * child ) <nl> BUG_ON ( 1 ); <nl> } <nl>  <nl> + /* No restrictions on file types which are never encrypted */ <nl> + if (! S_ISREG ( child -> i_mode ) && ! S_ISDIR ( child -> i_mode ) && <nl> + ! S_ISLNK ( child -> i_mode )) <nl> + return 1 ; <nl> + <nl> /* no restrictions if the parent directory is not encrypted */ <nl> if (! parent -> i_sb -> s_cop -> is_encrypted ( parent )) <nl> return 1 ;
static int write_vmem ( struct fbtft_par * par , size_t offset , size_t len ) <nl> signed short * convert_buf = kmalloc ( par -> info -> var . xres * <nl> par -> info -> var . yres * sizeof ( signed short ), GFP_NOIO ); <nl>  <nl> + if (! convert_buf ) <nl> + return - ENOMEM ; <nl> + <nl> fbtft_par_dbg ( DEBUG_WRITE_VMEM , par , "% s ()\ n ", __func__ ); <nl>  <nl> /* converting to grayscale16 */
sctp_disposition_t sctp_sf_eat_auth ( const struct sctp_endpoint * ep , <nl> struct sctp_chunk * err_chunk ; <nl> sctp_ierror_t error ; <nl>  <nl> + /* Make sure that the peer has AUTH capable */ <nl> + if (! asoc -> peer . auth_capable ) <nl> + return sctp_sf_unk_chunk ( ep , asoc , type , arg , commands ); <nl> + <nl> if (! sctp_vtag_verify ( chunk , asoc )) { <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_REPORT_BAD_TAG , <nl> SCTP_NULL ());
void con_protect_unimap ( struct vc_data * vc , int rdonly ); <nl> int con_copy_unimap ( struct vc_data * dst_vc , struct vc_data * src_vc ); <nl>  <nl> # define vc_translate ( vc , c ) (( vc )-> vc_translate [( c ) | \ <nl> - ( vc )-> vc_toggle_meta ? 0x80 : 0 ]) <nl> + (( vc )-> vc_toggle_meta ? 0x80 : 0 )]) <nl> # else <nl> # define con_set_trans_old ( arg ) ( 0 ) <nl> # define con_get_trans_old ( arg ) (- EINVAL )
static int qeth_query_card_info ( struct qeth_card * card , <nl>  <nl> static inline int qeth_get_qdio_q_format ( struct qeth_card * card ) <nl> { <nl> - switch ( card -> info . type ) { <nl> - case QETH_CARD_TYPE_IQD : <nl> - return 2 ; <nl> - default : <nl> - return 0 ; <nl> - } <nl> + if ( card -> info . type == QETH_CARD_TYPE_IQD ) <nl> + return QDIO_IQDIO_QFMT ; <nl> + else <nl> + return QDIO_QETH_QFMT ; <nl> } <nl>  <nl> static void qeth_determine_capabilities ( struct qeth_card * card )
static int perf_sched__process_tracepoint_sample ( struct perf_tool * tool __maybe_ <nl> struct perf_evsel * evsel , <nl> struct machine * machine ) <nl> { <nl> - struct thread * thread = machine__findnew_thread ( machine , sample -> pid ); <nl> + struct thread * thread = machine__findnew_thread ( machine , sample -> tid ); <nl> int err = 0 ; <nl>  <nl> if ( thread == NULL ) {
struct gb_host_device * gb_hd_create ( struct gb_hd_driver * driver , <nl> return ERR_PTR (- EINVAL ); <nl> } <nl>  <nl> - if ( num_cports == 0 || num_cports > CPORT_ID_MAX ) { <nl> + if ( num_cports == 0 || num_cports > CPORT_ID_MAX + 1 ) { <nl> dev_err ( parent , " Invalid number of CPorts : % zu \ n ", num_cports ); <nl> return ERR_PTR (- EINVAL ); <nl> }
static int rsc_parse ( struct cache_detail * cd , <nl> /* number of additional gid ' s */ <nl> if ( get_int (& mesg , & N )) <nl> goto out ; <nl> + if ( N < 0 || N > NGROUPS_MAX ) <nl> + goto out ; <nl> status = - ENOMEM ; <nl> rsci . cred . cr_group_info = groups_alloc ( N ); <nl> if ( rsci . cred . cr_group_info == NULL )
static inline int init_new_context ( struct task_struct * tsk , <nl> mm -> context . execute_only_pkey = - 1 ; <nl> } <nl> # endif <nl> - init_new_context_ldt ( tsk , mm ); <nl> - <nl> - return 0 ; <nl> + return init_new_context_ldt ( tsk , mm ); <nl> } <nl> static inline void destroy_context ( struct mm_struct * mm ) <nl> {
# include " commands . h " <nl> # include " power . h " <nl>  <nl> - static bool force_cam ; <nl> + static bool force_cam = true ; <nl> module_param ( force_cam , bool , 0644 ); <nl> MODULE_PARM_DESC ( force_cam , " force continuously aware mode ( no power saving at all )"); <nl> 
int selinux_task_prlimit ( const struct cred * cred , const struct cred * tcred , <nl> { <nl> u32 av = 0 ; <nl>  <nl> + if (! flags ) <nl> + return 0 ; <nl> if ( flags & LSM_PRLIMIT_WRITE ) <nl> av |= PROCESS__SETRLIMIT ; <nl> if ( flags & LSM_PRLIMIT_READ )
intel_sdvo_tv_init ( struct intel_sdvo * intel_sdvo , int type ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl>  <nl> intel_sdvo_lvds_init ( struct intel_sdvo * intel_sdvo , int device ) <nl> return true ; <nl>  <nl> err : <nl> - intel_sdvo_destroy_enhance_property ( connector ); <nl> - kfree ( intel_sdvo_connector ); <nl> + intel_sdvo_destroy ( connector ); <nl> return false ; <nl> } <nl> 
static void scan_requests ( struct ceph_osd * osd , <nl> list_add_tail (& lreq -> scan_item , need_resend_linger ); <nl> break ; <nl> case CALC_TARGET_POOL_DNE : <nl> + list_del_init (& lreq -> scan_item ); <nl> check_linger_pool_dne ( lreq ); <nl> break ; <nl> }
static int option_probe ( struct usb_serial * serial , <nl> serial -> interface -> cur_altsetting -> desc . bInterfaceNumber , <nl> OPTION_BLACKLIST_RESERVED_IF , <nl> ( const struct option_blacklist_info *) id -> driver_info )) <nl> + return - ENODEV ; <nl>  <nl> /* Don ' t bind network interface on Samsung GT - B3730 , it is handled by a separate module */ <nl> if ( serial -> dev -> descriptor . idVendor == SAMSUNG_VENDOR_ID &&
static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> err = rpmsg_ns_register_device ( rpdev_ns ); <nl> if ( err ) <nl> - goto free_vch ; <nl> + /* vch will be free in virtio_rpmsg_release_device () */ <nl> + goto free_ctrldev ; <nl> } <nl>  <nl> /* <nl> static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> return 0 ; <nl>  <nl> - free_vch : <nl> - kfree ( vch ); <nl> free_ctrldev : <nl> rpmsg_virtio_del_ctrl_dev ( rpdev_ctrl ); <nl> free_coherent :
static void restore_custom_reg_settings ( struct wiphy * wiphy ) <nl> chan -> flags = chan -> orig_flags ; <nl> chan -> max_antenna_gain = chan -> orig_mag ; <nl> chan -> max_power = chan -> orig_mpwr ; <nl> + chan -> beacon_found = false ; <nl> } <nl> } <nl> }
intel_ioapic_set_affinity ( struct irq_data * data , const struct cpumask * mask , <nl>  <nl> err = apic -> cpu_mask_to_apicid_and ( cfg -> domain , mask , & dest ); <nl> if ( err ) { <nl> - if ( assign_irq_vector ( irq , cfg , data -> affinity )); <nl> + if ( assign_irq_vector ( irq , cfg , data -> affinity )) <nl> pr_err (" Failed to recover vector for irq % d \ n ", irq ); <nl> return err ; <nl> }
static int Handle1401Esc ( DEVICE_EXTENSION * pdx , char * pCh , <nl> /* This can never happen , really */ <nl> dev_err (& pdx -> interface -> dev , <nl> " ERROR : DMA setup while transfer still waiting "); <nl> - spin_unlock (& pdx -> stagedLock ); <nl> + spin_unlock (& pdx -> stagedLock ); <nl> } else { <nl> if (( wTransType == TM_EXTTOHOST ) <nl> || ( wTransType == TM_EXTTO1401 )) {
static int nr_listen ( struct socket * sock , int backlog ) <nl> struct sock * sk = sock -> sk ; <nl>  <nl> lock_sock ( sk ); <nl> + if ( sock -> state != SS_UNCONNECTED ) { <nl> + release_sock ( sk ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( sk -> sk_state != TCP_LISTEN ) { <nl> memset (& nr_sk ( sk )-> user_addr , 0 , AX25_ADDR_LEN ); <nl> sk -> sk_max_ack_backlog = backlog ;
int ieee80211_register_hw ( struct ieee80211_hw * hw ) <nl> fail_workqueue : <nl> wiphy_unregister ( local -> hw . wiphy ); <nl> fail_wiphy_register : <nl> - kfree ( local -> int_scan_req -> channels ); <nl> + kfree ( local -> int_scan_req ); <nl> return result ; <nl> } <nl> EXPORT_SYMBOL ( ieee80211_register_hw );
void intel_lrc_irq_handler ( struct intel_engine_cs * ring ) <nl>  <nl> spin_unlock (& ring -> execlist_lock ); <nl>  <nl> - WARN ( submit_contexts > 2 , " More than two context complete events ?\ n "); <nl> + if ( unlikely ( submit_contexts > 2 )) <nl> + DRM_ERROR (" More than two context complete events ?\ n "); <nl> + <nl> ring -> next_context_status_buffer = write_pointer % GEN8_CSB_ENTRIES ; <nl>  <nl> /* Update the read pointer to the old write pointer . Manual ringbuffer
static int kcm_sendmsg ( struct socket * sock , struct msghdr * msg , size_t len ) <nl> } else { <nl> /* Message not complete , save state */ <nl> partial_message : <nl> - kcm -> seq_skb = head ; <nl> - kcm_tx_msg ( head )-> last_skb = skb ; <nl> + if ( head ) { <nl> + kcm -> seq_skb = head ; <nl> + kcm_tx_msg ( head )-> last_skb = skb ; <nl> + } <nl> } <nl>  <nl> KCM_STATS_ADD ( kcm -> stats . tx_bytes , copied );
compat_ptr ( compat_uptr_t uptr ) <nl> return ( void __user *) ( unsigned long ) uptr ; <nl> } <nl>  <nl> + static inline compat_uptr_t <nl> + ptr_to_compat ( void __user * uptr ) <nl> +{ <nl> + return ( u32 )( unsigned long ) uptr ; <nl> +} <nl> + <nl> static __inline__ void __user * <nl> compat_alloc_user_space ( long len ) <nl> {
static struct input_dev * gb_svc_input_create ( struct gb_svc * svc ) <nl> return input_dev ; <nl>  <nl> err_free_input : <nl> - input_free_device ( svc -> input ); <nl> + input_free_device ( input_dev ); <nl> return ERR_PTR (- ENOMEM ); <nl> } <nl> 
static int set_gamma ( struct fbtft_par * par , u32 * curves ) <nl> * The masks are the same for both positive and negative voltage <nl> * gamma curves . <nl> */ <nl> - const u8 gamma_par_mask [] = { <nl> + static const u8 gamma_par_mask [] = { <nl> 0xFF , /* V63 [ 3 : 0 ], V0 [ 3 : 0 ]*/ <nl> 0x3F , /* V1 [ 5 : 0 ] */ <nl> 0x3F , /* V2 [ 5 : 0 ] */
static int ttusbdecfe_dvbs_diseqc_send_master_cmd ( struct dvb_frontend * fe , struc <nl> 0x00 , 0x00 , 0x00 , 0x00 , <nl> 0x00 , 0x00 }; <nl>  <nl> + if ( cmd -> msg_len > sizeof ( b ) - 4 ) <nl> + return - EINVAL ; <nl> + <nl> memcpy (& b [ 4 ], cmd -> msg , cmd -> msg_len ); <nl>  <nl> state -> config -> send_command ( fe , 0x72 ,
xfs_qm_dquot_walk ( <nl> skipped = 0 ; <nl> break ; <nl> } <nl> + /* we ' re done if id overflows back to zero */ <nl> + if (! next_index ) <nl> + break ; <nl> } <nl>  <nl> if ( skipped ) {
EXPORT_SYMBOL ( ipmi_get_smi_info ); <nl> static void free_user ( struct kref * ref ) <nl> { <nl> struct ipmi_user * user = container_of ( ref , struct ipmi_user , refcount ); <nl> + cleanup_srcu_struct (& user -> release_barrier ); <nl> kfree ( user ); <nl> } <nl>  <nl> int ipmi_destroy_user ( struct ipmi_user * user ) <nl> { <nl> _ipmi_destroy_user ( user ); <nl>  <nl> - cleanup_srcu_struct (& user -> release_barrier ); <nl> kref_put (& user -> refcount , free_user ); <nl>  <nl> return 0 ;
int iwlagn_mac_setup_register ( struct iwl_priv * priv , <nl> ARRAY_SIZE ( iwlagn_iface_combinations_dualmode ); <nl> } <nl>  <nl> - hw -> wiphy -> max_remain_on_channel_duration = 1000 ; <nl> + hw -> wiphy -> max_remain_on_channel_duration = 500 ; <nl>  <nl> hw -> wiphy -> flags |= WIPHY_FLAG_CUSTOM_REGULATORY | <nl> WIPHY_FLAG_DISABLE_BEACON_HINTS |
static int rk_hw_params ( struct snd_pcm_substream * substream , <nl> case 96000 : <nl> mclk = 12288000 ; <nl> break ; <nl> + case 192000 : <nl> + mclk = 24576000 ; <nl> + break ; <nl> case 11025 : <nl> case 22050 : <nl> case 44100 :
static int acm_probe ( struct usb_interface * intf , <nl> if ( quirks == NO_UNION_NORMAL ) { <nl> data_interface = usb_ifnum_to_if ( usb_dev , 1 ); <nl> control_interface = usb_ifnum_to_if ( usb_dev , 0 ); <nl> + /* we would crash */ <nl> + if (! data_interface || ! control_interface ) <nl> + return - ENODEV ; <nl> goto skip_normal_probe ; <nl> } <nl> 
static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
static int snd_usb_copy_string_desc ( struct mixer_build * state , <nl> int index , char * buf , int maxlen ) <nl> { <nl> int len = usb_string ( state -> chip -> dev , index , buf , maxlen - 1 ); <nl> + <nl> + if ( len < 0 ) <nl> + return 0 ; <nl> + <nl> buf [ len ] = 0 ; <nl> return len ; <nl> }
static inline clock_t jiffies_delta_to_clock_t ( long delta ) <nl> return jiffies_to_clock_t ( max ( 0L , delta )); <nl> } <nl>  <nl> + static inline unsigned int jiffies_delta_to_msecs ( long delta ) <nl> +{ <nl> + return jiffies_to_msecs ( max ( 0L , delta )); <nl> +} <nl> + <nl> extern unsigned long clock_t_to_jiffies ( unsigned long x ); <nl> extern u64 jiffies_64_to_clock_t ( u64 x ); <nl> extern u64 nsec_to_clock_t ( u64 x );
static struct snd_soc_codec_driver soc_codec_device_ak4104 = { <nl> . probe = ak4104_probe , <nl> . remove = ak4104_remove , <nl> . reg_cache_size = AK4104_NUM_REGS , <nl> - . reg_word_size = sizeof ( u16 ), <nl> + . reg_word_size = sizeof ( u8 ), <nl> }; <nl>  <nl> static int ak4104_spi_probe ( struct spi_device * spi )
static SIMPLE_DEV_PM_OPS ( ds1374_pm , ds1374_suspend , ds1374_resume ); <nl> static struct i2c_driver ds1374_driver = { <nl> . driver = { <nl> . name = " rtc - ds1374 ", <nl> + . of_match_table = of_match_ptr ( ds1374_of_match ), <nl> . pm = & ds1374_pm , <nl> }, <nl> . probe = ds1374_probe ,
static int iio_device_add_event ( struct iio_dev * indio_dev , <nl> & indio_dev -> event_interface -> dev_attr_list ); <nl> kfree ( postfix ); <nl>  <nl> + if (( ret == - EBUSY ) && ( shared_by != IIO_SEPARATE )) <nl> + continue ; <nl> + <nl> if ( ret ) <nl> return ret ; <nl> 
static void finish_csr_load ( const struct firmware * fw , void * context ) <nl> } <nl> csr -> mmio_count = dmc_header -> mmio_count ; <nl> for ( i = 0 ; i < dmc_header -> mmio_count ; i ++) { <nl> - if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE && <nl> + if ( dmc_header -> mmioaddr [ i ] < CSR_MMIO_START_RANGE || <nl> dmc_header -> mmioaddr [ i ] > CSR_MMIO_END_RANGE ) { <nl> DRM_ERROR (" Firmware has wrong mmio address 0x % x \ n ", <nl> dmc_header -> mmioaddr [ i ]);
static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> val ); <nl>  <nl> case LIRC_SET_REC_CARRIER_RANGE : <nl> + if (! dev -> s_rx_carrier_range ) <nl> + return - ENOTTY ; <nl> + <nl> if ( val <= 0 ) <nl> return - EINVAL ; <nl>  <nl> static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> break ; <nl>  <nl> case LIRC_SET_REC_TIMEOUT_REPORTS : <nl> + if (! dev -> timeout ) <nl> + return - ENOTTY ; <nl> + <nl> lirc -> send_timeout_reports = !! val ; <nl> break ; <nl> 
static struct ib_qp * i40iw_create_qp ( struct ib_pd * ibpd , <nl> return & iwqp -> ibqp ; <nl> error : <nl> i40iw_free_qp_resources ( iwdev , iwqp , qp_num ); <nl> - kfree ( mem ); <nl> return ERR_PTR ( err_code ); <nl> } <nl> 
static int nvmet_rdma_queue_connect ( struct rdma_cm_id * cm_id , <nl> } <nl> queue -> port = cm_id -> context ; <nl>  <nl> + if ( queue -> host_qid == 0 ) { <nl> + /* Let inflight controller teardown complete */ <nl> + flush_scheduled_work (); <nl> + } <nl> + <nl> ret = nvmet_rdma_cm_accept ( cm_id , queue , & event -> param . conn ); <nl> if ( ret ) <nl> goto release_queue ;
static netdev_tx_t reg_vif_xmit ( struct sk_buff * skb , <nl> int err ; <nl>  <nl> err = ip6mr_fib_lookup ( net , & fl6 , & mrt ); <nl> - if ( err < 0 ) <nl> + if ( err < 0 ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> read_lock (& mrt_lock ); <nl> dev -> stats . tx_bytes += skb -> len ;
struct hisi_clock_data * hisi_clk_init ( struct device_node * np , <nl> goto err ; <nl> } <nl> clk_data -> base = base ; <nl> - <nl> - clk_table = kzalloc ( sizeof ( struct clk *) * nr_clks , GFP_KERNEL ); <nl> + clk_table = kcalloc ( nr_clks , sizeof (* clk_table ), GFP_KERNEL ); <nl> if (! clk_table ) { <nl> pr_err ("% s : could not allocate clock lookup table \ n ", __func__ ); <nl> goto err_data ;
static int usb_console_setup ( struct console * co , char * options ) <nl> tty_kref_put ( tty ); <nl> reset_open_count : <nl> port -> port . count = 0 ; <nl> + info -> port = NULL ; <nl> usb_autopm_put_interface ( serial -> interface ); <nl> error_get_interface : <nl> usb_serial_put ( serial );
static inline struct crypto_kpp * crypto_kpp_reqtfm ( struct kpp_request * req ) <nl> return __crypto_kpp_tfm ( req -> base . tfm ); <nl> } <nl>  <nl> + static inline u32 crypto_kpp_get_flags ( struct crypto_kpp * tfm ) <nl> +{ <nl> + return crypto_tfm_get_flags ( crypto_kpp_tfm ( tfm )); <nl> +} <nl> + <nl> + static inline void crypto_kpp_set_flags ( struct crypto_kpp * tfm , u32 flags ) <nl> +{ <nl> + crypto_tfm_set_flags ( crypto_kpp_tfm ( tfm ), flags ); <nl> +} <nl> + <nl> /** <nl> * crypto_free_kpp () - free KPP tfm handle <nl> *
xfs_growfs_rt ( <nl> /* <nl> * Initial error checking . <nl> */ <nl> - if ( mp -> m_rtdev_targp || mp -> m_rbmip == NULL || <nl> + if ( mp -> m_rtdev_targp == NULL || mp -> m_rbmip == NULL || <nl> ( nrblocks = in -> newblocks ) <= sbp -> sb_rblocks || <nl> ( sbp -> sb_rblocks && ( in -> extsize != sbp -> sb_rextsize ))) <nl> return XFS_ERROR ( EINVAL );
static int ucode_init ( loader_block * lb , amb_dev * dev ) <nl> const struct firmware * fw ; <nl> unsigned long start_address ; <nl> const struct ihex_binrec * rec ; <nl> - const char * errmsg = 0 ; <nl> + const char * errmsg = NULL ; <nl> int res ; <nl>  <nl> res = request_ihex_firmware (& fw , " atmsar11 . fw ", & dev -> pci_dev -> dev );
static int pty_write ( struct tty_struct * tty , const unsigned char * buf , <nl>  <nl> static int pty_write_room ( struct tty_struct * tty ) <nl> { <nl> + if ( tty -> stopped ) <nl> + return 0 ; <nl> return pty_space ( tty -> link ); <nl> } <nl> 
static inline bool is_new_conn_expected ( const struct ip_vs_conn * cp , <nl> switch ( cp -> protocol ) { <nl> case IPPROTO_TCP : <nl> return ( cp -> state == IP_VS_TCP_S_TIME_WAIT ) || <nl> + ( cp -> state == IP_VS_TCP_S_CLOSE ) || <nl> (( conn_reuse_mode & 2 ) && <nl> ( cp -> state == IP_VS_TCP_S_FIN_WAIT ) && <nl> ( cp -> flags & IP_VS_CONN_F_NOOUTPUT ));
static void ks_wlan_hw_rx ( struct ks_wlan_private * priv , uint16_t size ) <nl> int ret ; <nl> struct rx_device_buffer * rx_buffer ; <nl> struct hostif_hdr * hdr ; <nl> - unsigned short event = 0 ; <nl> + u16 event = 0 ; <nl>  <nl> /* receive data */ <nl> if ( rxq_count ( priv ) >= ( RX_DEVICE_BUFF_SIZE - 1 )) {
static struct kset * ipl_kset ; <nl>  <nl> static void __ipl_run ( void * unused ) <nl> { <nl> + if ( MACHINE_IS_LPAR && ipl_info . type == IPL_TYPE_CCW ) <nl> + diag308 ( DIAG308_LOAD_NORMAL_DUMP , NULL ); <nl> diag308 ( DIAG308_LOAD_CLEAR , NULL ); <nl> if ( MACHINE_IS_VM ) <nl> __cpcmd (" IPL ", NULL , 0 , NULL );
static inline int virtqueue_add ( struct virtqueue * _vq , <nl> * host should service the ring ASAP . */ <nl> if ( out_sgs ) <nl> vq -> notify (& vq -> vq ); <nl> + if ( indirect ) <nl> + kfree ( desc ); <nl> END_USE ( vq ); <nl> return - ENOSPC ; <nl> }
static void after_state_ch ( struct drbd_conf * mdev , union drbd_state os , <nl> drbd_free_bc ( mdev -> ldev ); <nl> mdev -> ldev = NULL ;); <nl>  <nl> - if ( mdev -> md_io_tmpp ) <nl> + if ( mdev -> md_io_tmpp ) { <nl> __free_page ( mdev -> md_io_tmpp ); <nl> + mdev -> md_io_tmpp = NULL ; <nl> + } <nl> } <nl>  <nl> /* Disks got bigger while they were detached */
vbuschannel_itoa ( char * p , int remain , int num ) <nl> } <nl> /* form a backwards decimal ascii string in < s > */ <nl> while ( num > 0 ) { <nl> - if ( digits >= ( int ) sizeof ( s )) <nl> + if ( digits >= ( int ) sizeof ( s )) <nl> return 0 ; <nl> s [ digits ++] = ( num % 10 ) + ' 0 '; <nl> num = num / 10 ;
int ipv6_find_hdr ( const struct sk_buff * skb , unsigned int * offset , <nl> found = ( nexthdr == target ); <nl>  <nl> if ((! ipv6_ext_hdr ( nexthdr )) || nexthdr == NEXTHDR_NONE ) { <nl> - if ( target < 0 ) <nl> + if ( target < 0 || found ) <nl> break ; <nl> return - ENOENT ; <nl> }
int thermal_add_hwmon_sysfs ( struct thermal_zone_device * tz ) <nl>  <nl> INIT_LIST_HEAD (& hwmon -> tz_list ); <nl> strlcpy ( hwmon -> type , tz -> type , THERMAL_NAME_LENGTH ); <nl> - hwmon -> device = hwmon_device_register ( NULL ); <nl> + hwmon -> device = hwmon_device_register (& tz -> device ); <nl> if ( IS_ERR ( hwmon -> device )) { <nl> result = PTR_ERR ( hwmon -> device ); <nl> goto free_mem ;
static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
static int ieee80211_set_cqm_rssi_config ( struct wiphy * wiphy , <nl>  <nl> bss_conf -> cqm_rssi_thold = rssi_thold ; <nl> bss_conf -> cqm_rssi_hyst = rssi_hyst ; <nl> + sdata -> u . mgd . last_cqm_event_signal = 0 ; <nl>  <nl> /* tell the driver upon association , unless already associated */ <nl> if ( sdata -> u . mgd . associated &&
int radeon_suspend_kms ( struct drm_device * dev , bool suspend , <nl> radeon_agp_suspend ( rdev ); <nl>  <nl> pci_save_state ( dev -> pdev ); <nl> - if ( freeze && rdev -> family >= CHIP_R600 ) { <nl> + if ( freeze && rdev -> family >= CHIP_CEDAR ) { <nl> rdev -> asic -> asic_reset ( rdev , true ); <nl> pci_restore_state ( dev -> pdev ); <nl> } else if ( suspend ) {
static int nfs4_stat_to_errno ( int ); <nl> 2 + encode_verifier_maxsz + 5 + \ <nl> nfs4_label_maxsz ) <nl> # define decode_readdir_maxsz ( op_decode_hdr_maxsz + \ <nl> - decode_verifier_maxsz + \ <nl> - nfs4_label_maxsz + nfs4_fattr_maxsz ) <nl> + decode_verifier_maxsz ) <nl> # define encode_readlink_maxsz ( op_encode_hdr_maxsz ) <nl> # define decode_readlink_maxsz ( op_decode_hdr_maxsz + 1 ) <nl> # define encode_write_maxsz ( op_encode_hdr_maxsz + \
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> b43_set_phytxctl_defaults ( dev ); <nl>  <nl> /* Minimum Contention Window */ <nl> - if ( phy -> type == B43_PHYTYPE_B ) { <nl> + if ( phy -> type == B43_PHYTYPE_B ) <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0x1F ); <nl> - } else { <nl> + else <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0xF ); <nl> - } <nl> /* Maximum Contention Window */ <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MAXCONT , 0x3FF ); <nl> 
int reiserfs_acl_chmod ( struct inode * inode ) <nl> return 0 ; <nl> } <nl>  <nl> + reiserfs_write_unlock ( inode -> i_sb ); <nl> acl = reiserfs_get_acl ( inode , ACL_TYPE_ACCESS ); <nl> + reiserfs_write_lock ( inode -> i_sb ); <nl> if (! acl ) <nl> return 0 ; <nl> if ( IS_ERR ( acl ))
static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> mutex_lock (& minors_lock ); <nl> dev = hidraw_table [ minor ]; <nl> + if (! dev ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl>  <nl> switch ( cmd ) { <nl> case HIDIOCGRDESCSIZE : <nl> static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> ret = - ENOTTY ; <nl> } <nl> + out : <nl> mutex_unlock (& minors_lock ); <nl> return ret ; <nl> }
static int sdhci_probe ( struct platform_device * pdev ) <nl> host -> hw_name = " sdhci "; <nl> host -> ops = & sdhci_pltfm_ops ; <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq <= 0 ) { <nl> + ret = - EINVAL ; <nl> + goto err_host ; <nl> + } <nl> host -> quirks = SDHCI_QUIRK_BROKEN_ADMA ; <nl>  <nl> sdhci = sdhci_priv ( host );
static int mlxsw_sp_ageing_set ( struct mlxsw_sp * mlxsw_sp , u32 ageing_time ) <nl>  <nl> static int mlxsw_sp_port_attr_br_ageing_set ( struct mlxsw_sp_port * mlxsw_sp_port , <nl> struct switchdev_trans * trans , <nl> - unsigned long ageing_jiffies ) <nl> + unsigned long ageing_clock_t ) <nl> { <nl> struct mlxsw_sp * mlxsw_sp = mlxsw_sp_port -> mlxsw_sp ; <nl> + unsigned long ageing_jiffies = clock_t_to_jiffies ( ageing_clock_t ); <nl> u32 ageing_time = jiffies_to_msecs ( ageing_jiffies ) / 1000 ; <nl>  <nl> if ( switchdev_trans_ph_prepare ( trans ))
void ieee80211_sta_rx_notify ( struct ieee80211_sub_if_data * sdata , <nl> if ( is_multicast_ether_addr ( hdr -> addr1 )) <nl> return ; <nl>  <nl> + /* <nl> + * In case we receive frames after disassociation . <nl> + */ <nl> + if (! sdata -> u . mgd . associated ) <nl> + return ; <nl> + <nl> ieee80211_sta_reset_conn_monitor ( sdata ); <nl> } <nl> 
struct rchan * relay_open ( const char * base_filename , <nl>  <nl> kref_put (& chan -> kref , relay_destroy_channel ); <nl> mutex_unlock (& relay_channels_mutex ); <nl> + kfree ( chan ); <nl> return NULL ; <nl> } <nl> EXPORT_SYMBOL_GPL ( relay_open );
int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
static int aic3x_set_power ( struct snd_soc_codec * codec , int power ) <nl>  <nl> /* Sync reg_cache with the hardware */ <nl> codec -> cache_only = 0 ; <nl> - for ( i = 0 ; i < ARRAY_SIZE ( aic3x_reg ); i ++) <nl> + for ( i = AIC3X_SAMPLE_RATE_SEL_REG ; i < ARRAY_SIZE ( aic3x_reg ); i ++) <nl> snd_soc_write ( codec , i , cache [ i ]); <nl> if ( aic3x -> model == AIC3X_MODEL_3007 ) <nl> aic3x_init_3007 ( codec );
static void ieee80211_iface_work ( struct work_struct * work ) <nl> if ( sta ) { <nl> u16 last_seq ; <nl>  <nl> - last_seq = le16_to_cpu ( <nl> - sta -> last_seq_ctrl [ rx_agg -> tid ]); <nl> + last_seq = IEEE80211_SEQ_TO_SN ( le16_to_cpu ( <nl> + sta -> last_seq_ctrl [ rx_agg -> tid ])); <nl>  <nl> __ieee80211_start_rx_ba_session ( sta , <nl> 0 , 0 ,
qeth_l3_add_mc_to_hash ( struct qeth_card * card , struct in_device * in4_dev ) <nl>  <nl> tmp -> u . a4 . addr = be32_to_cpu ( im4 -> multiaddr ); <nl> memcpy ( tmp -> mac , buf , sizeof ( tmp -> mac )); <nl> + tmp -> is_multicast = 1 ; <nl>  <nl> ipm = qeth_l3_ip_from_hash ( card , tmp ); <nl> if ( ipm ) {
int snd_ctl_add ( struct snd_card * card , struct snd_kcontrol * kcontrol ) <nl> if ( snd_BUG_ON (! card || ! kcontrol -> info )) <nl> goto error ; <nl> id = kcontrol -> id ; <nl> + if ( id . index > UINT_MAX - kcontrol -> count ) <nl> + goto error ; <nl> + <nl> down_write (& card -> controls_rwsem ); <nl> if ( snd_ctl_find_id ( card , & id )) { <nl> up_write (& card -> controls_rwsem );
static void __ibmvnic_reset ( struct work_struct * work ) <nl>  <nl> if ( rc ) { <nl> free_all_rwi ( adapter ); <nl> + mutex_unlock (& adapter -> reset_lock ); <nl> return ; <nl> } <nl> 
static int parse_output ( struct hda_codec * codec ) <nl> memcpy ( cfg -> speaker_pins , cfg -> line_out_pins , <nl> sizeof ( cfg -> speaker_pins )); <nl> cfg -> line_outs = 0 ; <nl> + memset ( cfg -> line_out_pins , 0 , sizeof ( cfg -> line_out_pins )); <nl> } <nl>  <nl> return 0 ;
int bdi_register ( struct backing_dev_info * bdi , struct device * parent , <nl> int ret = 0 ; <nl> struct device * dev ; <nl>  <nl> + if ( WARN_ON ( bdi -> dev )) <nl> + goto exit ; <nl> + <nl> va_start ( args , fmt ); <nl> dev = device_create_vargs ( bdi_class , parent , MKDEV ( 0 , 0 ), bdi , fmt , args ); <nl> va_end ( args );
static inline int _loop ( unsigned dry_run , u8 buf [], <nl> unsigned lcnt0 , lcnt1 , ljmp0 , ljmp1 ; <nl> struct _arg_LPEND lpend ; <nl>  <nl> + if (* bursts == 1 ) <nl> + return _bursts ( dry_run , buf , pxs , 1 ); <nl> + <nl> /* Max iterations possible in DMALP is 256 */ <nl> if (* bursts >= 256 * 256 ) { <nl> lcnt1 = 256 ;
copy_user_handle_tail ( char * to , char * from , unsigned len , unsigned zerorest ) <nl> char c ; <nl> unsigned zero_len ; <nl>  <nl> - for (; len ; -- len ) { <nl> + for (; len ; -- len , to ++) { <nl> if ( __get_user_nocheck ( c , from ++, sizeof ( char ))) <nl> break ; <nl> - if ( __put_user_nocheck ( c , to ++, sizeof ( char ))) <nl> + if ( __put_user_nocheck ( c , to , sizeof ( char ))) <nl> break ; <nl> } <nl> 
int iio_buffer_register ( struct iio_dev * indio_dev , <nl> if ( channels ) { <nl> /* new magic */ <nl> for ( i = 0 ; i < num_channels ; i ++) { <nl> + if ( channels [ i ]. scan_index < 0 ) <nl> + continue ; <nl> + <nl> /* Establish necessary mask length */ <nl> if ( channels [ i ]. scan_index > <nl> ( int ) indio_dev -> masklength - 1 )
static const struct ich_laptop ich_laptop [] = { <nl> { 0x27DF , 0x0005 , 0x0280 }, /* ICH7 on Acer 5602WLMi */ <nl> { 0x27DF , 0x1025 , 0x0110 }, /* ICH7 on Acer 3682WLMi */ <nl> { 0x27DF , 0x1043 , 0x1267 }, /* ICH7 on Asus W5F */ <nl> + { 0x24CA , 0x1025 , 0x0061 }, /* ICH4 on ACER Aspire 2023WLMi */ <nl> /* end marker */ <nl> { 0 , } <nl> };
static void lut_close ( struct i915_gem_context * ctx ) <nl> kmem_cache_free ( ctx -> i915 -> luts , lut ); <nl> } <nl>  <nl> + rcu_read_lock (); <nl> radix_tree_for_each_slot ( slot , & ctx -> handles_vma , & iter , 0 ) { <nl> struct i915_vma * vma = rcu_dereference_raw (* slot ); <nl> struct drm_i915_gem_object * obj = vma -> obj ; <nl> static void lut_close ( struct i915_gem_context * ctx ) <nl>  <nl> __i915_gem_object_release_unless_active ( obj ); <nl> } <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> static void i915_gem_context_free ( struct i915_gem_context * ctx )
static void tg3_phy_toggle_apd ( struct tg3 * tp , bool enable ) <nl> { <nl> u32 reg ; <nl>  <nl> - if (!( tp -> tg3_flags2 & TG3_FLG2_5705_PLUS )) <nl> + if (!( tp -> tg3_flags2 & TG3_FLG2_5705_PLUS ) || <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5906 ) <nl> return ; <nl>  <nl> reg = MII_TG3_MISC_SHDW_WREN |
static bool malidp_check_pages_threshold ( struct malidp_plane_state * ms , <nl> else <nl> sgt = obj -> funcs -> get_sg_table ( obj ); <nl>  <nl> - if (! sgt ) <nl> + if ( IS_ERR ( sgt )) <nl> return false ; <nl>  <nl> sgl = sgt -> sgl ;
static int mon_bin_mmap ( struct file * filp , struct vm_area_struct * vma ) <nl> { <nl> /* don ' t do anything here : " fault " will set up page table entries */ <nl> vma -> vm_ops = & mon_bin_vm_ops ; <nl> + <nl> + if ( vma -> vm_flags & VM_WRITE ) <nl> + return - EPERM ; <nl> + <nl> + vma -> vm_flags &= ~ VM_MAYWRITE ; <nl> vma -> vm_flags |= VM_DONTEXPAND | VM_DONTDUMP ; <nl> vma -> vm_private_data = filp -> private_data ; <nl> mon_bin_vma_open ( vma );
struct request_queue * __scsi_alloc_queue ( struct Scsi_Host * shost , <nl> request_fn_proc * request_fn ) <nl> { <nl> struct request_queue * q ; <nl> - struct device * dev = shost -> shost_gendev . parent ; <nl> + struct device * dev = shost -> dma_dev ; <nl>  <nl> q = blk_init_queue ( request_fn , NULL ); <nl> if (! q )
int ext4_expand_extra_isize_ea ( struct inode * inode , int new_extra_isize , <nl> goto cleanup ; <nl> kfree ( b_entry_name ); <nl> kfree ( buffer ); <nl> + b_entry_name = NULL ; <nl> + buffer = NULL ; <nl> brelse ( is -> iloc . bh ); <nl> kfree ( is ); <nl> kfree ( bs );
int imx_drm_add_crtc ( struct drm_crtc * crtc , <nl>  <nl> mutex_lock (& imxdrm -> mutex ); <nl>  <nl> + /* <nl> + * The vblank arrays are dimensioned by MAX_CRTC - we can ' t <nl> + * pass IDs greater than this to those functions . <nl> + */ <nl> + if ( imxdrm -> pipes >= MAX_CRTC ) { <nl> + ret = - EINVAL ; <nl> + goto err_busy ; <nl> + } <nl> + <nl> if ( imxdrm -> drm -> open_count ) { <nl> ret = - EBUSY ; <nl> goto err_busy ;
asmlinkage long sys_rt_sigreturn_wrapper ( void ); <nl> * The sys_call_table array must be 4K aligned to be accessible from <nl> * kernel / entry . S . <nl> */ <nl> - void * sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> + void * const sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> [ 0 ... __NR_syscalls - 1 ] = sys_ni_syscall , <nl> # include < asm / unistd . h > <nl> };
static int sony_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> } <nl>  <nl> ret = sony_set_operational ( hdev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_stop ; <nl>  <nl> return 0 ;
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , u32 features ) <nl> skb -> ip_summed = CHECKSUM_NONE ; <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> - if (( skb_headroom ( skb ) < frag_hdr_sz ) && <nl> + if (( skb_mac_header ( skb ) < skb -> head + frag_hdr_sz ) && <nl> pskb_expand_head ( skb , frag_hdr_sz , 0 , GFP_ATOMIC )) <nl> goto out ; <nl> 
long dgnc_mgmt_ioctl ( struct file * file , unsigned int cmd , unsigned long arg ) <nl>  <nl> spin_lock_irqsave (& dgnc_global_lock , flags ); <nl>  <nl> + memset (& ddi , 0 , sizeof ( ddi )); <nl> ddi . dinfo_nboards = dgnc_NumBoards ; <nl> sprintf ( ddi . dinfo_version , "% s ", DG_PART ); <nl> 
static int __devinit snd_interwave_pnp ( int dev , struct snd_interwave * iwcard , <nl> struct pnp_resource_table * cfg = kmalloc ( sizeof ( struct pnp_resource_table ), GFP_KERNEL ); <nl> int err ; <nl>  <nl> + if (! cfg ) <nl> + return - ENOMEM ; <nl> iwcard -> dev = pnp_request_card_device ( card , id -> devs [ 0 ]. id , NULL ); <nl> if ( iwcard -> dev == NULL ) { <nl> kfree ( cfg );
static int init_volumes ( struct ubi_device * ubi , const struct ubi_scan_info * si , <nl> if ( ubi -> autoresize_vol_id != - 1 ) { <nl> ubi_err (" more then one auto - resize volume (% d " <nl> " and % d )", ubi -> autoresize_vol_id , i ); <nl> + kfree ( vol ); <nl> return - EINVAL ; <nl> } <nl> 
static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
static int fastrpc_dma_buf_attach ( struct dma_buf * dmabuf , <nl> FASTRPC_PHYS ( buffer -> phys ), buffer -> size ); <nl> if ( ret < 0 ) { <nl> dev_err ( buffer -> dev , " failed to get scatterlist from DMA API \ n "); <nl> + kfree ( a ); <nl> return - EINVAL ; <nl> } <nl> 
static int rt6_score_route ( struct rt6_info * rt , int oif , <nl> m |= IPV6_DECODE_PREF ( IPV6_EXTRACT_PREF ( rt -> rt6i_flags )) << 2 ; <nl> # endif <nl> n = rt6_check_neigh ( rt ); <nl> - if ( n > 1 ) <nl> - m |= 16 ; <nl> - else if (! n && strict & RT6_LOOKUP_F_REACHABLE ) <nl> + if (! n && ( strict & RT6_LOOKUP_F_REACHABLE )) <nl> return - 1 ; <nl> return m ; <nl> }
static void zynqmp_dma_chan_remove ( struct zynqmp_dma_chan * chan ) <nl> if (! chan ) <nl> return ; <nl>  <nl> - devm_free_irq ( chan -> zdev -> dev , chan -> irq , chan ); <nl> + if ( chan -> irq ) <nl> + devm_free_irq ( chan -> zdev -> dev , chan -> irq , chan ); <nl> tasklet_kill (& chan -> tasklet ); <nl> list_del (& chan -> common . device_node ); <nl> }
static int btrfs_extent_same ( struct inode * src , u64 loff , u64 olen , <nl> inode_lock ( src ); <nl>  <nl> ret = extent_same_check_offsets ( src , loff , & len , olen ); <nl> + if ( ret ) <nl> + goto out_unlock ; <nl> + ret = extent_same_check_offsets ( src , dst_loff , & len , olen ); <nl> if ( ret ) <nl> goto out_unlock ; <nl> 
static int iscsi_handle_reject ( struct iscsi_conn * conn , struct iscsi_hdr * hdr , <nl> if ( opcode != ISCSI_OP_NOOP_OUT ) <nl> return 0 ; <nl>  <nl> - if ( rejected_pdu . itt == cpu_to_be32 ( ISCSI_RESERVED_TAG )) { <nl> + if ( rejected_pdu . itt == cpu_to_be32 ( ISCSI_RESERVED_TAG )) { <nl> /* <nl> * nop - out in response to target ' s nop - out rejected . <nl> * Just resend .
static int e1000_set_ringparam ( struct net_device * netdev , <nl> err_alloc_rx : <nl> kfree ( txdr ); <nl> err_alloc_tx : <nl> - e1000_up ( adapter ); <nl> + if ( netif_running ( adapter -> netdev )) <nl> + e1000_up ( adapter ); <nl> err_setup : <nl> clear_bit ( __E1000_RESETTING , & adapter -> flags ); <nl> return err ;
static int lpc32xx_adc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if (( irq < 0 ) || ( irq >= NR_IRQS )) { <nl> + if ( irq <= 0 ) { <nl> dev_err (& pdev -> dev , " failed getting interrupt resource \ n "); <nl> return - EINVAL ; <nl> }
static long aac_compat_do_ioctl ( struct aac_dev * dev , unsigned cmd , unsigned long <nl> static int aac_compat_ioctl ( struct scsi_device * sdev , int cmd , void __user * arg ) <nl> { <nl> struct aac_dev * dev = ( struct aac_dev *) sdev -> host -> hostdata ; <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> return aac_compat_do_ioctl ( dev , cmd , ( unsigned long ) arg ); <nl> } <nl> 
int tcp_sendmsg ( struct sock * sk , struct msghdr * msg , size_t size ) <nl>  <nl> if (! skb_can_coalesce ( skb , i , pfrag -> page , <nl> pfrag -> offset )) { <nl> - if ( i == sysctl_max_skb_frags || ! sg ) { <nl> + if ( i >= sysctl_max_skb_frags || ! sg ) { <nl> tcp_mark_push ( tp , skb ); <nl> goto new_segment ; <nl> }
static int i40e_set_channels ( struct net_device * dev , <nl> * class queue mapping <nl> */ <nl> new_count = i40e_reconfig_rss_queues ( pf , count ); <nl> - if ( new_count > 1 ) <nl> + if ( new_count > 0 ) <nl> return 0 ; <nl> else <nl> return - EINVAL ;
migration_call ( struct notifier_block * nfb , unsigned long action , void * hcpu ) <nl> migrate_tasks ( rq ); <nl> BUG_ON ( rq -> nr_running != 1 ); /* the migration thread */ <nl> raw_spin_unlock_irqrestore (& rq -> lock , flags ); <nl> - break ; <nl> - <nl> - case CPU_DEAD : <nl> calc_load_migrate ( rq ); <nl> break ; <nl> # endif
void parse_events__free_terms ( struct list_head * terms ) <nl>  <nl> list_for_each_entry_safe ( term , h , terms , list ) <nl> free ( term ); <nl> - <nl> - free ( terms ); <nl> }
static void exynos5_powerdown_conf ( enum sys_powerdown mode ) <nl> void exynos_sys_powerdown_conf ( enum sys_powerdown mode ) <nl> { <nl> unsigned int i ; <nl> + const struct exynos_pmu_data * pmu_data ; <nl> + <nl> + if (! pmu_context ) <nl> + return ; <nl>  <nl> - const struct exynos_pmu_data * pmu_data = pmu_context -> pmu_data ; <nl> + pmu_data = pmu_context -> pmu_data ; <nl>  <nl> if ( pmu_data -> powerdown_conf ) <nl> pmu_data -> powerdown_conf ( mode );
static isolate_migrate_t isolate_migratepages ( struct zone * zone , <nl> low_pfn = isolate_migratepages_block ( cc , low_pfn , end_pfn , <nl> isolate_mode ); <nl>  <nl> - if (! low_pfn || cc -> contended ) <nl> + if (! low_pfn || cc -> contended ) { <nl> + acct_isolated ( zone , cc ); <nl> return ISOLATE_ABORT ; <nl> + } <nl>  <nl> /* <nl> * Either we isolated something and proceed with migration . Or
long ext4_fallocate ( struct file * file , int mode , loff_t offset , loff_t len ) <nl> blkbits ) >> blkbits )) <nl> new_size = offset + len ; <nl> else <nl> - new_size = ( map . m_lblk + ret ) << blkbits ; <nl> + new_size = (( loff_t ) map . m_lblk + ret ) << blkbits ; <nl>  <nl> ext4_falloc_update_inode ( inode , mode , new_size , <nl> ( map . m_flags & EXT4_MAP_NEW ));
static struct sg_table * omap_gem_map_dma_buf ( <nl> /* this should be after _get_paddr () to ensure we have pages attached */ <nl> omap_gem_dma_sync ( obj , dir ); <nl>  <nl> - out : <nl> - if ( ret ) <nl> - return ERR_PTR ( ret ); <nl> return sg ; <nl> + out : <nl> + kfree ( sg ); <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> static void omap_gem_unmap_dma_buf ( struct dma_buf_attachment * attachment ,
char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> * string to the length of the original string to allow for worst case . <nl> */ <nl> md_len = strlen ( sb_mountdata ) + INET6_ADDRSTRLEN ; <nl> - mountdata = kzalloc ( md_len + 1 , GFP_KERNEL ); <nl> + mountdata = kzalloc ( md_len + sizeof (" ip =") + 1 , GFP_KERNEL ); <nl> if ( mountdata == NULL ) { <nl> rc = - ENOMEM ; <nl> goto compose_mount_options_err ;
void ath_init_leds ( struct ath_softc * sc ) <nl> { <nl> int ret ; <nl>  <nl> + if ( AR_SREV_9100 ( sc -> sc_ah )) <nl> + return ; <nl> + <nl> if ( sc -> sc_ah -> led_pin < 0 ) { <nl> if ( AR_SREV_9287 ( sc -> sc_ah )) <nl> sc -> sc_ah -> led_pin = ATH_LED_PIN_9287 ;
static int ocores_i2c_of_probe ( struct platform_device * pdev , <nl> & i2c -> reg_io_width ); <nl>  <nl> match = of_match_node ( ocores_i2c_match , pdev -> dev . of_node ); <nl> - if ( match && ( int ) match -> data == TYPE_GRLIB ) { <nl> + if ( match && ( long ) match -> data == TYPE_GRLIB ) { <nl> dev_dbg (& pdev -> dev , " GRLIB variant of i2c - ocores \ n "); <nl> i2c -> setreg = oc_setreg_grlib ; <nl> i2c -> getreg = oc_getreg_grlib ;
int udp_ioctl ( struct sock * sk , int cmd , unsigned long arg ) <nl> { <nl> unsigned int amount = first_packet_length ( sk ); <nl>  <nl> - if ( amount ) <nl> - /* <nl> - * We will only return the amount <nl> - * of this packet since that is all <nl> - * that will be read . <nl> - */ <nl> return put_user ( amount , ( int __user *) arg ); <nl> } <nl> 
static const char * intel_pt_err_msgs [] = { <nl>  <nl> int intel_pt__strerror ( int code , char * buf , size_t buflen ) <nl> { <nl> - if ( code < 1 || code > INTEL_PT_ERR_MAX ) <nl> + if ( code < 1 || code >= INTEL_PT_ERR_MAX ) <nl> code = INTEL_PT_ERR_UNK ; <nl> strlcpy ( buf , intel_pt_err_msgs [ code ], buflen ); <nl> return 0 ;
static void hub_activate ( struct usb_hub * hub , enum hub_activation_type type ) <nl> PREPARE_DELAYED_WORK (& hub -> init_work , hub_init_func2 ); <nl> schedule_delayed_work (& hub -> init_work , <nl> msecs_to_jiffies ( delay )); <nl> + <nl> + /* Suppress autosuspend until init is done */ <nl> + to_usb_interface ( hub -> intfdev )-> pm_usage_cnt = 1 ; <nl> return ; /* Continues at init2 : below */ <nl> } else { <nl> hub_power_on ( hub , true );
static int nand_default_block_markbad ( struct mtd_info * mtd , loff_t ofs ) <nl> int block , ret ; <nl>  <nl> /* Get block number */ <nl> - block = (( int ) ofs ) >> chip -> bbt_erase_shift ; <nl> + block = ( int )( ofs >> chip -> bbt_erase_shift ); <nl> if ( chip -> bbt ) <nl> chip -> bbt [ block >> 2 ] |= 0x01 << (( block & 0x03 ) << 1 ); <nl> 
int __kprobes arch_prepare_kprobe ( struct kprobe * p ) <nl> if (! ret ) { <nl> memcpy ( p -> ainsn . insn , p -> addr , MAX_INSN_SIZE * sizeof ( kprobe_opcode_t )); <nl> p -> opcode = * p -> addr ; <nl> + flush_icache_range (( unsigned long ) p -> ainsn . insn , <nl> + ( unsigned long ) p -> ainsn . insn + sizeof ( kprobe_opcode_t )); <nl> } <nl>  <nl> return ret ;
static int vivid_fb_ioctl ( struct fb_info * info , unsigned cmd , unsigned long arg ) <nl> case FBIOGET_VBLANK : { <nl> struct fb_vblank vblank ; <nl>  <nl> + memset (& vblank , 0 , sizeof ( vblank )); <nl> vblank . flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT | <nl> FB_VBLANK_HAVE_VSYNC ; <nl> vblank . count = 0 ;
static int meson_sar_adc_lock ( struct iio_dev * indio_dev ) <nl> regmap_read ( priv -> regmap , MESON_SAR_ADC_DELAY , & val ); <nl> } while ( val & MESON_SAR_ADC_DELAY_BL30_BUSY && timeout --); <nl>  <nl> - if ( timeout < 0 ) <nl> + if ( timeout < 0 ) { <nl> + mutex_unlock (& indio_dev -> mlock ); <nl> return - ETIMEDOUT ; <nl> + } <nl> } <nl>  <nl> return 0 ;
int install_user_keyrings ( void ) <nl>  <nl> kenter ("% p {% u }", user , uid ); <nl>  <nl> - if ( user -> uid_keyring ) { <nl> + if ( user -> uid_keyring && user -> session_keyring ) { <nl> kleave (" = 0 [ exist ]"); <nl> return 0 ; <nl> }
static void tcp_mtu_probing ( struct inet_connection_sock * icsk , struct sock * sk ) <nl> struct tcp_sock * tp = tcp_sk ( sk ); <nl> int mss ; <nl>  <nl> - mss = tcp_mtu_to_mss ( sk , icsk -> icsk_mtup . search_low )/ 2 ; <nl> + mss = tcp_mtu_to_mss ( sk , icsk -> icsk_mtup . search_low ) >> 1 ; <nl> mss = min ( sysctl_tcp_base_mss , mss ); <nl> mss = max ( mss , 68 - tp -> tcp_header_len ); <nl> icsk -> icsk_mtup . search_low = tcp_mss_to_mtu ( sk , mss );
static int ems_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> goto err ; <nl> } <nl>  <nl> - emsff_init ( hdev ); <nl> + ret = emsff_init ( hdev ); <nl> + if ( ret ) { <nl> + dev_err (& hdev -> dev , " force feedback init failed \ n "); <nl> + hid_hw_stop ( hdev ); <nl> + goto err ; <nl> + } <nl>  <nl> return 0 ; <nl> err :
static void parse_bsd ( struct parsed_partitions * state , <nl> continue ; <nl> bsd_start = le32_to_cpu ( p -> p_offset ); <nl> bsd_size = le32_to_cpu ( p -> p_size ); <nl> + if ( memcmp ( flavour , " bsd \ 0 ", 4 ) == 0 ) <nl> + bsd_start += offset ; <nl> if ( offset == bsd_start && size == bsd_size ) <nl> /* full parent partition , we have it already */ <nl> continue ;
isofs_export_encode_fh ( struct inode * inode , <nl> len = 3 ; <nl> fh32 [ 0 ] = ei -> i_iget5_block ; <nl> fh16 [ 2 ] = ( __u16 ) ei -> i_iget5_offset ; /* fh16 [ sic ] */ <nl> + fh16 [ 3 ] = 0 ; /* avoid leaking uninitialized data */ <nl> fh32 [ 2 ] = inode -> i_generation ; <nl> if ( parent ) { <nl> struct iso_inode_info * eparent ;
static void uwb_drp_handle_alien_drp ( struct uwb_rc * rc , struct uwb_ie_drp * drp_i <nl>  <nl> /* alloc and initialize new uwb_cnflt_alien */ <nl> cnflt = kzalloc ( sizeof ( struct uwb_cnflt_alien ), GFP_KERNEL ); <nl> - if (! cnflt ) <nl> + if (! cnflt ) { <nl> dev_err ( dev , " failed to alloc uwb_cnflt_alien struct \ n "); <nl> + return ; <nl> + } <nl> + <nl> INIT_LIST_HEAD (& cnflt -> rc_node ); <nl> init_timer (& cnflt -> timer ); <nl> cnflt -> timer . function = uwb_cnflt_timer ;
static int mwifiex_pcie_init_evt_ring ( struct mwifiex_adapter * adapter ) <nl> skb_put ( skb , MAX_EVENT_SIZE ); <nl>  <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MAX_EVENT_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> + kfree ( card -> evtbd_ring_vbase ); <nl> return - 1 ; <nl> + } <nl>  <nl> buf_pa = MWIFIEX_SKB_DMA_ADDR ( skb ); <nl> 
static int tun_set_iff ( struct net * net , struct file * file , struct ifreq * ifr ) <nl>  <nl> err_detach : <nl> tun_detach_all ( dev ); <nl> + /* register_netdevice () already called tun_free_netdev () */ <nl> + goto err_free_dev ; <nl> + <nl> err_free_flow : <nl> tun_flow_uninit ( tun ); <nl> security_tun_dev_free_security ( tun -> security );
int extent_range_uptodate ( struct extent_io_tree * tree , <nl> while ( start <= end ) { <nl> index = start >> PAGE_CACHE_SHIFT ; <nl> page = find_get_page ( tree -> mapping , index ); <nl> + if (! page ) <nl> + return 1 ; <nl> uptodate = PageUptodate ( page ); <nl> page_cache_release ( page ); <nl> if (! uptodate ) {
void * __raw_xsave_addr ( struct xregs_state * xsave , int xstate_feature_mask ) <nl> { <nl> int feature_nr = fls64 ( xstate_feature_mask ) - 1 ; <nl>  <nl> + if (! xfeature_enabled ( feature_nr )) { <nl> + WARN_ON_FPU ( 1 ); <nl> + return NULL ; <nl> + } <nl> + <nl> return ( void *) xsave + xstate_comp_offsets [ feature_nr ]; <nl> } <nl> /*
static int valid_kprobe_addr ( int template , int slot , unsigned long addr ) <nl> addr ); <nl> return - EINVAL ; <nl> } <nl> + <nl> + if ( slot == 1 && bundle_encoding [ template ][ 1 ] != L ) { <nl> + printk ( KERN_WARNING " Inserting kprobes on slot # 1 " <nl> + " is not supported \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static long __validate_layout ( struct ceph_mds_client * mdsc , <nl> /* validate striping parameters */ <nl> if (( l -> object_size & ~ PAGE_MASK ) || <nl> ( l -> stripe_unit & ~ PAGE_MASK ) || <nl> - (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit )) <nl> + ( l -> stripe_unit != 0 && <nl> + (( unsigned ) l -> object_size % ( unsigned ) l -> stripe_unit ))) <nl> return - EINVAL ; <nl>  <nl> /* make sure it ' s a valid data pool */
xmaddr_t arbitrary_virt_to_machine ( void * vaddr ) <nl> } <nl> EXPORT_SYMBOL_GPL ( arbitrary_virt_to_machine ); <nl>  <nl> - void xen_flush_tlb_all ( void ) <nl> + static void xen_flush_tlb_all ( void ) <nl> { <nl> struct mmuext_op * op ; <nl> struct multicall_space mcs ;
int ubi_wl_get_peb ( struct ubi_device * ubi ) <nl> peb = __wl_get_peb ( ubi ); <nl> spin_unlock (& ubi -> wl_lock ); <nl>  <nl> + if ( peb < 0 ) <nl> + return peb ; <nl> + <nl> err = ubi_self_check_all_ff ( ubi , peb , ubi -> vid_hdr_aloffset , <nl> ubi -> peb_size - ubi -> vid_hdr_aloffset ); <nl> if ( err ) {
int tusb6010_platform_retime ( unsigned is_refclk ) <nl> unsigned sysclk_ps ; <nl> int status ; <nl>  <nl> - if (! refclk_psec || sysclk_ps == 0 ) <nl> + if (! refclk_psec || fclk_ps == 0 ) <nl> return - ENODEV ; <nl>  <nl> sysclk_ps = is_refclk ? refclk_psec : TUSB6010_OSCCLK_60 ;
static int __devinit gen_74x164_probe ( struct spi_device * spi ) <nl> } <nl>  <nl> chip -> gpio_chip . ngpio = GEN_74X164_NUMBER_GPIOS * chip -> registers ; <nl> - chip -> buffer = devm_kzalloc (& spi -> dev , chip -> gpio_chip . ngpio , GFP_KERNEL ); <nl> + chip -> buffer = devm_kzalloc (& spi -> dev , chip -> registers , GFP_KERNEL ); <nl> if (! chip -> buffer ) { <nl> ret = - ENOMEM ; <nl> goto exit_destroy ;
int iwl_mvm_rx_card_state_notif ( struct iwl_mvm * mvm , <nl> ( flags & CT_KILL_CARD_DISABLED ) ? <nl> " Reached " : " Not reached "); <nl>  <nl> - if ( flags & CARD_DISABLED_MSK ) <nl> - iwl_write32 ( mvm -> trans , CSR_UCODE_DRV_GP1_SET , <nl> - CSR_UCODE_DRV_GP1_BIT_CMD_BLOCKED ); <nl> - <nl> return 0 ; <nl> } <nl> 
static struct zx_dma_desc_sw * zx_alloc_desc_resource ( int num , <nl> kfree ( ds ); <nl> return NULL ; <nl> } <nl> - memset ( ds -> desc_hw , sizeof ( struct zx_desc_hw ) * num , 0 ); <nl> + memset ( ds -> desc_hw , 0 , sizeof ( struct zx_desc_hw ) * num ); <nl> ds -> desc_num = num ; <nl> return ds ; <nl> }
static byte connect_res ( dword Id , word Number , DIVA_CAPI_ADAPTER * a , <nl> add_ai ( plci , & parms [ 5 ]); <nl> sig_req ( plci , REJECT , 0 ); <nl> } <nl> - else if ( Reject == 1 || Reject > 9 ) <nl> + else if ( Reject == 1 || Reject >= 9 ) <nl> { <nl> add_ai ( plci , & parms [ 5 ]); <nl> sig_req ( plci , HANGUP , 0 );
static int ath10k_usb_hif_tx_sg ( struct ath10k * ar , u8 pipe_id , <nl> ath10k_dbg ( ar , ATH10K_DBG_USB_BULK , <nl> " usb bulk transmit failed : % d \ n ", ret ); <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> ret = - EINVAL ; <nl> goto err_free_urb_to_pipe ; <nl> }
static int __devinit ks8851_probe ( struct spi_device * spi ) <nl>  <nl>  <nl> err_netdev : <nl> - free_irq ( ndev -> irq , ndev ); <nl> + free_irq ( ndev -> irq , ks ); <nl>  <nl> err_id : <nl> err_irq :
ssize_t tomoyo_write_control ( struct tomoyo_io_buffer * head , <nl> return - EFAULT ; <nl> if ( mutex_lock_interruptible (& head -> io_sem )) <nl> return - EINTR ; <nl> + head -> read_user_buf_avail = 0 ; <nl> idx = tomoyo_read_lock (); <nl> /* Read a line and dispatch it to the policy handler . */ <nl> while ( avail_len > 0 ) {
static void f2fs_submit_discard_endio ( struct bio * bio ) <nl>  <nl> dc -> error = bio -> bi_error ; <nl> dc -> state = D_DONE ; <nl> - complete (& dc -> wait ); <nl> + complete_all (& dc -> wait ); <nl> bio_put ( bio ); <nl> } <nl> 
int wusbhc_chid_set ( struct wusbhc * wusbhc , const struct wusb_ckhdid * chid ) <nl> { <nl> int result = 0 ; <nl>  <nl> - if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof ( chid )) == 0 ) <nl> + if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof (* chid )) == 0 ) <nl> chid = NULL ; <nl>  <nl> mutex_lock (& wusbhc -> mutex );
static int annotate_browser__run ( struct annotate_browser * self , int evidx , <nl> timer , arg , delay_secs ); <nl> } <nl> break ; <nl> + case NEWT_KEY_LEFT : <nl> + case NEWT_KEY_ESCAPE : <nl> case ' q ': <nl> case CTRL (' c '): <nl> goto out ;
struct ring_buffer * __ring_buffer_alloc ( unsigned long size , unsigned flags , <nl> buffer -> reader_lock_key = key ; <nl>  <nl> /* need at least two pages */ <nl> - if ( buffer -> pages == 1 ) <nl> - buffer -> pages ++; <nl> + if ( buffer -> pages < 2 ) <nl> + buffer -> pages = 2 ; <nl>  <nl> /* <nl> * In case of non - hotplug cpu , if the ring - buffer is allocated
static int reset_one_sub_crq_queue ( struct ibmvnic_adapter * adapter , <nl> scrq -> irq = 0 ; <nl> } <nl>  <nl> - memset ( scrq -> msgs , 0 , 2 * PAGE_SIZE ); <nl> + memset ( scrq -> msgs , 0 , 4 * PAGE_SIZE ); <nl> scrq -> cur = 0 ; <nl>  <nl> rc = h_reg_sub_crq ( adapter -> vdev -> unit_address , scrq -> msg_token ,
static int hdmi_get_edid ( void * ctx , struct drm_connector * connector , <nl> DRM_DEBUG_KMS ("% s : width [% d ] x height [% d ]\ n ", <nl> ( hdata -> dvi_mode ? " dvi monitor " : " hdmi monitor "), <nl> raw_edid -> width_cm , raw_edid -> height_cm ); <nl> + kfree ( raw_edid ); <nl> } else { <nl> return - ENODEV ; <nl> }
static void qlcnic_83xx_mailbox_worker ( struct work_struct * work ) <nl> __func__ , cmd -> cmd_op , cmd -> type , ahw -> pci_func , <nl> ahw -> op_mode ); <nl> clear_bit ( QLC_83XX_MBX_READY , & mbx -> status ); <nl> + qlcnic_dump_mbx ( adapter , cmd ); <nl> qlcnic_83xx_idc_request_reset ( adapter , <nl> QLCNIC_FORCE_FW_DUMP_KEY ); <nl> cmd -> rsp_opcode = QLCNIC_RCODE_TIMEOUT ;
static void radeon_compute_pll_legacy ( struct radeon_pll * pll , <nl> max_fractional_feed_div = pll -> max_frac_feedback_div ; <nl> } <nl>  <nl> - for ( post_div = min_post_div ; post_div <= max_post_div ; ++ post_div ) { <nl> + for ( post_div = max_post_div ; post_div >= min_post_div ; -- post_div ) { <nl> uint32_t ref_div ; <nl>  <nl> if (( pll -> flags & RADEON_PLL_NO_ODD_POST_DIV ) && ( post_div & 1 ))
static void tcp_cwnd_reduction ( struct sock * sk , const int prior_unsacked , <nl> int newly_acked_sacked = prior_unsacked - <nl> ( tp -> packets_out - tp -> sacked_out ); <nl>  <nl> + if ( newly_acked_sacked <= 0 || WARN_ON_ONCE (! tp -> prior_cwnd )) <nl> + return ; <nl> + <nl> tp -> prr_delivered += newly_acked_sacked ; <nl> if ( delta < 0 ) { <nl> u64 dividend = ( u64 ) tp -> snd_ssthresh * tp -> prr_delivered +
static bool assoc_array_insert_into_terminal_node ( struct assoc_array_edit * edit , <nl> free_slot = i ; <nl> continue ; <nl> } <nl> - if ( ops -> compare_object ( assoc_array_ptr_to_leaf ( ptr ), index_key )) { <nl> + if ( assoc_array_ptr_is_leaf ( ptr ) && <nl> + ops -> compare_object ( assoc_array_ptr_to_leaf ( ptr ), <nl> + index_key )) { <nl> pr_devel (" replace in slot % d \ n ", i ); <nl> edit -> leaf_p = & node -> slots [ i ]; <nl> edit -> dead_leaf = node -> slots [ i ];
void rtl88e_dm_watchdog ( struct ieee80211_hw * hw ) <nl> if ( ppsc -> p2p_ps_info . p2p_ps_mode ) <nl> fw_ps_awake = false ; <nl>  <nl> + spin_lock (& rtlpriv -> locks . rf_ps_lock ); <nl> if (( ppsc -> rfpwr_state == ERFON ) && <nl> ((! fw_current_inpsmode ) && fw_ps_awake ) && <nl> (! ppsc -> rfchange_inprogress )) { <nl> void rtl88e_dm_watchdog ( struct ieee80211_hw * hw ) <nl> rtl88e_dm_check_edca_turbo ( hw ); <nl> rtl88e_dm_antenna_diversity ( hw ); <nl> } <nl> + spin_unlock (& rtlpriv -> locks . rf_ps_lock ); <nl> }
static int l2tp_ip6_recvmsg ( struct kiocb * iocb , struct sock * sk , <nl> lsa -> l2tp_addr = ipv6_hdr ( skb )-> saddr ; <nl> lsa -> l2tp_flowinfo = 0 ; <nl> lsa -> l2tp_scope_id = 0 ; <nl> + lsa -> l2tp_conn_id = 0 ; <nl> if ( ipv6_addr_type (& lsa -> l2tp_addr ) & IPV6_ADDR_LINKLOCAL ) <nl> lsa -> l2tp_scope_id = IP6CB ( skb )-> iif ; <nl> }
int tcp_recvmsg ( struct kiocb * iocb , struct sock * sk , struct msghdr * msg , <nl>  <nl> cleanup_rbuf ( sk , copied ); <nl>  <nl> - if ( tp -> ucopy . task == user_recv ) { <nl> + if (! sysctl_tcp_low_latency && tp -> ucopy . task == user_recv ) { <nl> /* Install new reader */ <nl> if (! user_recv && !( flags & ( MSG_TRUNC | MSG_PEEK ))) { <nl> user_recv = current ;
static ssize_t state_store ( struct subsystem * subsys , const char * buf , size_t n <nl> if (* s && ! strncmp ( buf , * s , len )) <nl> break ; <nl> } <nl> - if (* s ) <nl> + if ( state < PM_SUSPEND_MAX && * s ) <nl> error = enter_state ( state ); <nl> else <nl> error = - EINVAL ;
static void scan_add_host ( struct soc_camera_host * ici ) <nl>  <nl> list_for_each_entry ( icd , & devices , list ) { <nl> if ( icd -> iface == ici -> nr ) { <nl> - int ret ; <nl> - <nl> icd -> parent = ici -> v4l2_dev . dev ; <nl> - ret = soc_camera_probe ( icd ); <nl> + soc_camera_probe ( icd ); <nl> } <nl> } <nl> 
struct bio * bch_bio_split ( struct bio * bio , int sectors , <nl>  <nl> if ( bio -> bi_rw & REQ_DISCARD ) { <nl> ret = bio_alloc_bioset ( gfp , 1 , bs ); <nl> + if (! ret ) <nl> + return NULL ; <nl> idx = 0 ; <nl> goto out ; <nl> }
static inline long snd_ctl_ioctl_compat ( struct file * file , unsigned int cmd , uns <nl> case SNDRV_CTL_IOCTL_POWER_STATE : <nl> case SNDRV_CTL_IOCTL_ELEM_LOCK : <nl> case SNDRV_CTL_IOCTL_ELEM_UNLOCK : <nl> + case SNDRV_CTL_IOCTL_ELEM_REMOVE : <nl> + case SNDRV_CTL_IOCTL_TLV_READ : <nl> + case SNDRV_CTL_IOCTL_TLV_WRITE : <nl> + case SNDRV_CTL_IOCTL_TLV_COMMAND : <nl> return snd_ctl_ioctl ( file , cmd , ( unsigned long ) argp ); <nl> case SNDRV_CTL_IOCTL_ELEM_LIST32 : <nl> return snd_ctl_elem_list_compat ( ctl -> card , argp );
static struct mtd_partition davinci_ntosd2_nandflash_partition [] = { <nl> }; <nl>  <nl> static struct davinci_nand_pdata davinci_ntosd2_nandflash_data = { <nl> + . core_chipsel = 0 , <nl> . parts = davinci_ntosd2_nandflash_partition , <nl> . nr_parts = ARRAY_SIZE ( davinci_ntosd2_nandflash_partition ), <nl> . ecc_mode = NAND_ECC_HW ,
verbose_printk (" btrfs : send_create_inode % llu \ n ", ino ); <nl> TLV_PUT_PATH ( sctx , BTRFS_SEND_A_PATH_LINK , p ); <nl> } else if ( S_ISCHR ( mode ) || S_ISBLK ( mode ) || <nl> S_ISFIFO ( mode ) || S_ISSOCK ( mode )) { <nl> - TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , rdev ); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , new_encode_dev ( rdev )); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_MODE , mode ); <nl> } <nl>  <nl> ret = send_cmd ( sctx );
static int ccs811_start_sensor_application ( struct i2c_client * client ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (( ret & CCS811_STATUS_FW_MODE_APPLICATION )) <nl> + return 0 ; <nl> + <nl> if (( ret & CCS811_STATUS_APP_VALID_MASK ) != <nl> CCS811_STATUS_APP_VALID_LOADED ) <nl> return - EIO ;
static int proc_do_submiturb ( struct usb_dev_state * ps , struct usbdevfs_urb * uurb <nl> u = ( is_in ? URB_DIR_IN : URB_DIR_OUT ); <nl> if ( uurb -> flags & USBDEVFS_URB_ISO_ASAP ) <nl> u |= URB_ISO_ASAP ; <nl> - if ( uurb -> flags & USBDEVFS_URB_SHORT_NOT_OK ) <nl> + if ( uurb -> flags & USBDEVFS_URB_SHORT_NOT_OK && is_in ) <nl> u |= URB_SHORT_NOT_OK ; <nl> if ( uurb -> flags & USBDEVFS_URB_NO_FSBR ) <nl> u |= URB_NO_FSBR ;
static int ni_660x_request_mite_channel ( struct comedi_device * dev , <nl> struct mite_channel * mite_chan ; <nl>  <nl> spin_lock_irqsave (& devpriv -> mite_channel_lock , flags ); <nl> - BUG_ON ( counter -> mite_chan ); <nl> mite_chan = mite_request_channel ( devpriv -> mite , <nl> mite_ring ( devpriv , counter )); <nl> if (! mite_chan ) {
static int start_afu ( struct cxlflash_cfg * cfg ) <nl>  <nl> init_pcr ( cfg ); <nl>  <nl> + /* After an AFU reset , RRQ entries are stale , clear them */ <nl> + memset (& afu -> rrq_entry , 0 , sizeof ( afu -> rrq_entry )); <nl> + <nl> /* Initialize RRQ pointers */ <nl> afu -> hrrq_start = & afu -> rrq_entry [ 0 ]; <nl> afu -> hrrq_end = & afu -> rrq_entry [ NUM_RRQ_ENTRY - 1 ];
static int at_dma_remove ( struct platform_device * pdev ) <nl>  <nl> /* Disable interrupts */ <nl> atc_disable_chan_irq ( atdma , chan -> chan_id ); <nl> - tasklet_disable (& atchan -> tasklet ); <nl>  <nl> tasklet_kill (& atchan -> tasklet ); <nl> list_del (& chan -> device_node );
static void isp_video_buffer_query ( struct isp_video_buffer * buf , <nl> switch ( buf -> state ) { <nl> case ISP_BUF_STATE_ERROR : <nl> vbuf -> flags |= V4L2_BUF_FLAG_ERROR ; <nl> + /* Fallthrough */ <nl> case ISP_BUF_STATE_DONE : <nl> vbuf -> flags |= V4L2_BUF_FLAG_DONE ; <nl> + break ; <nl> case ISP_BUF_STATE_QUEUED : <nl> case ISP_BUF_STATE_ACTIVE : <nl> vbuf -> flags |= V4L2_BUF_FLAG_QUEUED ;
struct rockchip_thermal_data { <nl> # define TSADCV2_HIGHT_TSHUT_DEBOUNCE_COUNT 4 <nl> # define TSADCV2_AUTO_PERIOD_TIME 250 /* 250ms */ <nl> # define TSADCV2_AUTO_PERIOD_HT_TIME 50 /* 50ms */ <nl> -# define TSADCV3_AUTO_PERIOD_TIME 187500 /* 250ms */ <nl> -# define TSADCV3_AUTO_PERIOD_HT_TIME 37500 /* 50ms */ <nl> +# define TSADCV3_AUTO_PERIOD_TIME 1875 /* 2 . 5ms */ <nl> +# define TSADCV3_AUTO_PERIOD_HT_TIME 1875 /* 2 . 5ms */ <nl>  <nl> # define TSADCV2_USER_INTER_PD_SOC 0x340 /* 13 clocks */ <nl> 
qla25xx_process_bidir_status_iocb ( scsi_qla_host_t * vha , void * pkt , <nl> bsg_job -> reply_len = sizeof ( struct fc_bsg_reply ); <nl> /* Always return DID_OK , bsg will send the vendor specific response <nl> * in this case only */ <nl> - sp -> done ( sp , DID_OK << 6 ); <nl> + sp -> done ( sp , DID_OK << 16 ); <nl>  <nl> } <nl> 
static int inet6_rtm_getroute ( struct sk_buff * in_skb , struct nlmsghdr * nlh ) <nl> rt = ( struct rt6_info *) ip6_route_output ( net , NULL , & fl6 ); <nl> } <nl>  <nl> + if ( rt == net -> ipv6 . ip6_null_entry ) { <nl> + err = rt -> dst . error ; <nl> + ip6_rt_put ( rt ); <nl> + goto errout ; <nl> + } <nl> + <nl> skb = alloc_skb ( NLMSG_GOODSIZE , GFP_KERNEL ); <nl> if (! skb ) { <nl> ip6_rt_put ( rt );
void rtl92se_disable_interrupt ( struct ieee80211_hw * hw ) <nl> rtl_write_dword ( rtlpriv , INTA_MASK + 4 , 0 ); <nl>  <nl> rtlpci -> irq_enabled = false ; <nl> + synchronize_irq ( rtlpci -> pdev -> irq ); <nl> } <nl>  <nl> 
# include < linux / vmalloc . h > <nl>  <nl> # include < media / videobuf2 - core . h > <nl> +# include < media / videobuf2 - vmalloc . h > <nl> # include < media / videobuf2 - memops . h > <nl>  <nl> struct vb2_vmalloc_buf {
void fuse_conn_kill ( struct fuse_conn * fc ) <nl> kill_fasync (& fc -> fasync , SIGIO , POLL_IN ); <nl> wake_up_all (& fc -> waitq ); <nl> wake_up_all (& fc -> blocked_waitq ); <nl> - wake_up_all (& fc -> reserved_req_waitq ); <nl> } <nl> EXPORT_SYMBOL_GPL ( fuse_conn_kill ); <nl> 
static void __init offb_init_fb ( const char * name , const char * full_name , <nl> return ; <nl> } <nl>  <nl> - size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 17 ; <nl> + size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 16 ; <nl>  <nl> info = kmalloc ( size , GFP_ATOMIC ); <nl> 
static void ccwgroup_ungroup_callback ( struct device * dev ) <nl> struct ccwgroup_device * gdev = to_ccwgroupdev ( dev ); <nl>  <nl> mutex_lock (& gdev -> reg_mutex ); <nl> - __ccwgroup_remove_symlinks ( gdev ); <nl> - device_unregister ( dev ); <nl> + if ( device_is_registered (& gdev -> dev )) { <nl> + __ccwgroup_remove_symlinks ( gdev ); <nl> + device_unregister ( dev ); <nl> + } <nl> mutex_unlock (& gdev -> reg_mutex ); <nl> } <nl> 
int ext3_sync_file ( struct file * file , struct dentry * dentry , int datasync ) <nl> goto out ; <nl> } <nl>  <nl> + if ( datasync && !( inode -> i_state & I_DIRTY_DATASYNC )) <nl> + goto out ; <nl> + <nl> /* <nl> * The VFS has written the file data . If the inode is unaltered <nl> * then we need not start a commit .
static int si_common_early_init ( void * handle ) <nl> AMD_CG_SUPPORT_HDP_LS | <nl> AMD_CG_SUPPORT_HDP_MGCG ; <nl> adev -> pg_flags = 0 ; <nl> + adev -> external_rev_id = 70 ; <nl> break ; <nl>  <nl> default :
xfs_errortag_add ( int error_tag , xfs_mount_t * mp ) <nl> int len ; <nl> int64_t fsid ; <nl>  <nl> + if ( error_tag >= XFS_ERRTAG_MAX ) <nl> + return - EINVAL ; <nl> + <nl> memcpy (& fsid , mp -> m_fixedfsid , sizeof ( xfs_fsid_t )); <nl>  <nl> for ( i = 0 ; i < XFS_NUM_INJECT_ERROR ; i ++) {
void ath6kl_cfg80211_disconnect_event ( struct ath6kl * ar , u8 reason , <nl> NULL , 0 , <nl> WLAN_STATUS_UNSPECIFIED_FAILURE , <nl> GFP_KERNEL ); <nl> - } else { <nl> + } else if ( ar -> sme_state == SME_CONNECTED ) { <nl> cfg80211_disconnected ( ar -> net_dev , reason , <nl> NULL , 0 , GFP_KERNEL ); <nl> }
static int whiteheat_attach ( struct usb_serial * serial ) <nl> err ("% s : Unable to retrieve firmware version , try replugging \ n ", serial -> type -> description ); <nl> err ("% s : If the firmware is not running ( status led not blinking )\ n ", serial -> type -> description ); <nl> err ("% s : please contact support @ connecttech . com \ n ", serial -> type -> description ); <nl> + kfree ( result ); <nl> return - ENODEV ; <nl>  <nl> no_command_private :
static cycle_t sb1250_hpt_read ( void ) <nl> } <nl>  <nl> struct clocksource bcm1250_clocksource = { <nl> - . name = " MIPS ", <nl> + . name = " bcm1250 - counter - 3 ", <nl> . rating = 200 , <nl> . read = sb1250_hpt_read , <nl> . mask = CLOCKSOURCE_MASK ( 23 ),
static int xive_spapr_get_ipi ( unsigned int cpu , struct xive_cpu * xc ) <nl>  <nl> static void xive_spapr_put_ipi ( unsigned int cpu , struct xive_cpu * xc ) <nl> { <nl> + if (! xc -> hw_ipi ) <nl> + return ; <nl> + <nl> xive_irq_bitmap_free ( xc -> hw_ipi ); <nl> + xc -> hw_ipi = 0 ; <nl> } <nl> # endif /* CONFIG_SMP */ <nl> 
static int bcma_get_next_core ( struct bcma_bus * bus , u32 __iomem ** eromptr , <nl> switch ( core -> id . id ) { <nl> case BCMA_CORE_4706_MAC_GBIT_COMMON : <nl> case BCMA_CORE_NS_CHIPCOMMON_B : <nl> + case BCMA_CORE_PMU : <nl> + case BCMA_CORE_GCI : <nl> /* Not used yet : case BCMA_CORE_OOB_ROUTER : */ <nl> break ; <nl> default :
static void start_apic_timer ( struct kvm_lapic * apic ) <nl> { <nl> ktime_t now = apic -> lapic_timer . timer . base -> get_time (); <nl>  <nl> - apic -> lapic_timer . period = apic_get_reg ( apic , APIC_TMICT ) * <nl> + apic -> lapic_timer . period = ( u64 ) apic_get_reg ( apic , APIC_TMICT ) * <nl> APIC_BUS_CYCLE_NS * apic -> divide_count ; <nl> atomic_set (& apic -> lapic_timer . pending , 0 ); <nl> 
static void fcopy_send_data ( struct work_struct * dummy ) <nl> out_src = smsg_out ; <nl> break ; <nl>  <nl> + case WRITE_TO_FILE : <nl> + out_src = fcopy_transaction . fcopy_msg ; <nl> + out_len = sizeof ( struct hv_do_fcopy ); <nl> + break ; <nl> default : <nl> out_src = fcopy_transaction . fcopy_msg ; <nl> out_len = fcopy_transaction . recv_len ;
static int ll_ioctl_fiemap ( struct inode * inode , unsigned long arg ) <nl> if ( get_user ( extent_count , <nl> &(( struct ll_user_fiemap __user *) arg )-> fm_extent_count )) <nl> return - EFAULT ; <nl> + <nl> + if ( extent_count >= <nl> + ( SIZE_MAX - sizeof (* fiemap_s )) / sizeof ( struct ll_fiemap_extent )) <nl> + return - EINVAL ; <nl> num_bytes = sizeof (* fiemap_s ) + ( extent_count * <nl> sizeof ( struct ll_fiemap_extent )); <nl> 
int __cpufreq_driver_target ( struct cpufreq_policy * policy , <nl>  <nl> pr_debug (" target for CPU % u : % u kHz , relation % u \ n ", policy -> cpu , <nl> target_freq , relation ); <nl> + <nl> + if ( target_freq == policy -> cur ) <nl> + return 0 ; <nl> + <nl> if ( cpu_online ( policy -> cpu ) && cpufreq_driver -> target ) <nl> retval = cpufreq_driver -> target ( policy , target_freq , relation ); <nl> 
void tsb_grow ( struct mm_struct * mm , unsigned long tsb_index , unsigned long rss ) <nl> if ( new_size > ( PAGE_SIZE * 2 )) <nl> gfp_flags = __GFP_NOWARN | __GFP_NORETRY ; <nl>  <nl> - new_tsb = kmem_cache_alloc ( tsb_caches [ new_cache_index ], gfp_flags ); <nl> + new_tsb = kmem_cache_alloc_node ( tsb_caches [ new_cache_index ], <nl> + gfp_flags , numa_node_id ()); <nl> if ( unlikely (! new_tsb )) { <nl> /* Not being able to fork due to a high - order TSB <nl> * allocation failure is very bad behavior . Just back
static int ata_eh_recover ( struct ata_port * ap , ata_prereset_fn_t prereset , <nl> down_xfermask = 0 ; <nl> rc = 0 ; <nl>  <nl> + /* if UNLOADING , finish immediately */ <nl> + if ( ap -> flags & ATA_FLAG_UNLOADING ) <nl> + goto out ; <nl> + <nl> /* skip EH if possible . */ <nl> if ( ata_eh_skip_recovery ( ap )) <nl> ehc -> i . action = 0 ;
acpi_status asmlinkage acpi_enter_sleep_state ( u8 sleep_state ) <nl> /* <nl> * 2 ) Enable all wakeup GPEs <nl> */ <nl> + status = acpi_hw_disable_all_gpes (); <nl> + if ( ACPI_FAILURE ( status )) { <nl> + return_ACPI_STATUS ( status ); <nl> + } <nl> + <nl> acpi_gbl_system_awake_and_running = FALSE ; <nl>  <nl> status = acpi_hw_enable_all_wakeup_gpes ();
struct inode * ext2_new_inode ( struct inode * dir , umode_t mode , <nl>  <nl> for ( i = 0 ; i < sbi -> s_groups_count ; i ++) { <nl> gdp = ext2_get_group_desc ( sb , group , & bh2 ); <nl> + if (! gdp ) { <nl> + if (++ group == sbi -> s_groups_count ) <nl> + group = 0 ; <nl> + continue ; <nl> + } <nl> brelse ( bitmap_bh ); <nl> bitmap_bh = read_inode_bitmap ( sb , group ); <nl> if (! bitmap_bh ) {
static int pppoe_rcv_core ( struct sock * sk , struct sk_buff * skb ) <nl> * can ' t change . <nl> */ <nl>  <nl> + if ( skb -> pkt_type == PACKET_OTHERHOST ) <nl> + goto abort_kfree ; <nl> + <nl> if ( sk -> sk_state & PPPOX_BOUND ) { <nl> ppp_input (& po -> chan , skb ); <nl> } else if ( sk -> sk_state & PPPOX_RELAY ) {
static ssize_t port_fops_write ( struct file * filp , const char __user * ubuf , <nl> if ( ret < 0 ) <nl> return ret ; <nl> } <nl> + /* Port got hot - unplugged . */ <nl> + if (! port -> guest_connected ) <nl> + return - ENODEV ; <nl>  <nl> count = min (( size_t )( 32 * 1024 ), count ); <nl> 
static int uas_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl>  <nl> shost -> max_cmd_len = 16 + 252 ; <nl> shost -> max_id = 1 ; <nl> + shost -> max_lun = 256 ; <nl> + shost -> max_channel = 0 ; <nl> shost -> sg_tablesize = udev -> bus -> sg_tablesize ; <nl>  <nl> devinfo -> intf = intf ;
static int sdhci_acpi_probe ( struct platform_device * pdev ) <nl> host -> hw_name = " ACPI "; <nl> host -> ops = & sdhci_acpi_ops_dflt ; <nl> host -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( host -> irq <= 0 ) { <nl> + err = - EINVAL ; <nl> + goto err_free ; <nl> + } <nl>  <nl> host -> ioaddr = devm_ioremap_nocache ( dev , iomem -> start , <nl> resource_size ( iomem ));
static inline struct page * pte_alloc_one ( struct mm_struct * mm , <nl> struct page * pte ; <nl>  <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT , PTE_ORDER ); <nl> - if ( pte ) { <nl> - clear_highpage ( pte ); <nl> - pgtable_page_ctor ( pte ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + clear_highpage ( pte ); <nl> + if (! pgtable_page_ctor ( pte )) { <nl> + __free_page ( pte ); <nl> + return NULL ; <nl> } <nl> return pte ; <nl> }
static int _search_rsb ( struct dlm_ls * ls , char * name , int len , int b , <nl> kref_get (& r -> res_ref ); <nl> goto out ; <nl> } <nl> + if ( error == - ENOTBLK ) <nl> + goto out ; <nl> + <nl> error = dlm_search_rsb_tree (& ls -> ls_rsbtbl [ b ]. toss , name , len , flags , & r ); <nl> if ( error ) <nl> goto out ;
static int bf5xx_i2s_hw_params ( struct snd_pcm_substream * substream , <nl> bf5xx_i2s -> tcr2 |= 7 ; <nl> bf5xx_i2s -> rcr2 |= 7 ; <nl> sport_handle -> wdsize = 1 ; <nl> + break ; <nl> case SNDRV_PCM_FORMAT_S16_LE : <nl> bf5xx_i2s -> tcr2 |= 15 ; <nl> bf5xx_i2s -> rcr2 |= 15 ;
int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = dev_alloc_name ( ndev , ndev -> name ); <nl> if ( ret < 0 ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl>  <nl> int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = register_netdevice ( ndev ); <nl> if ( ret ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl> }
usba_ep_enable ( struct usb_ep * _ep , const struct usb_endpoint_descriptor * desc ) <nl>  <nl> spin_lock_irqsave (& ep -> udc -> lock , flags ); <nl>  <nl> - if ( ep -> ep . desc ) { <nl> - spin_unlock_irqrestore (& ep -> udc -> lock , flags ); <nl> - DBG ( DBG_ERR , " ep % d already enabled \ n ", ep -> index ); <nl> - return - EBUSY ; <nl> - } <nl> - <nl> ep -> ep . desc = desc ; <nl> ep -> ep . maxpacket = maxpacket ; <nl> 
xfs_attr3_rmt_verify ( <nl> if ( be32_to_cpu ( rmt -> rm_bytes ) > fsbsize - sizeof (* rmt )) <nl> return false ; <nl> if ( be32_to_cpu ( rmt -> rm_offset ) + <nl> - be32_to_cpu ( rmt -> rm_bytes ) >= XATTR_SIZE_MAX ) <nl> + be32_to_cpu ( rmt -> rm_bytes ) > XATTR_SIZE_MAX ) <nl> return false ; <nl> if ( rmt -> rm_owner == 0 ) <nl> return false ;
static s32 ixgbe_setup_kr_x550em ( struct ixgbe_hw * hw ) <nl> if ( hw -> phy . autoneg_advertised & IXGBE_LINK_SPEED_2_5GB_FULL ) <nl> return 0 ; <nl>  <nl> + if ( ixgbe_check_reset_blocked ( hw )) <nl> + return 0 ; <nl> + <nl> return ixgbe_setup_kr_speed_x550em ( hw , hw -> phy . autoneg_advertised ); <nl> } <nl> 
static int get_bitmap_file ( struct mddev * mddev , void __user * arg ) <nl> char * ptr ; <nl> int err ; <nl>  <nl> - file = kmalloc ( sizeof (* file ), GFP_NOIO ); <nl> + file = kzalloc ( sizeof (* file ), GFP_NOIO ); <nl> if (! file ) <nl> return - ENOMEM ; <nl> 
static int vga_video_font_height ; <nl> static int vga_scan_lines __read_mostly ; <nl> static unsigned int vga_rolled_over ; <nl>  <nl> - int vgacon_text_mode_force = 0 ; <nl> + static int vgacon_text_mode_force ; <nl>  <nl> bool vgacon_text_force ( void ) <nl> {
static int bcap_probe ( struct platform_device * pdev ) <nl> q -> mem_ops = & vb2_dma_contig_memops ; <nl> q -> timestamp_type = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC ; <nl>  <nl> - vb2_queue_init ( q ); <nl> + ret = vb2_queue_init ( q ); <nl> + if ( ret ) <nl> + goto err_free_handler ; <nl>  <nl> mutex_init (& bcap_dev -> mutex ); <nl> init_completion (& bcap_dev -> comp );
static int blkcg_print_stat ( struct seq_file * sf , void * v ) <nl> struct cftype blkcg_files [] = { <nl> { <nl> . name = " stat ", <nl> + . flags = CFTYPE_NOT_ON_ROOT , <nl> . seq_show = blkcg_print_stat , <nl> }, <nl> { } /* terminate */
static int i7core_register_mci ( struct i7core_dev * i7core_dev , <nl> } <nl>  <nl> fail : <nl> - edac_mc_free ( mci ); <nl> + if ( rc < 0 ) <nl> + edac_mc_free ( mci ); <nl> return rc ; <nl> } <nl> 
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
struct request_list * __blk_queue_next_rl ( struct request_list * rl , <nl> */ <nl> if ( rl == & q -> root_rl ) { <nl> ent = & q -> blkg_list ; <nl> + /* There are no more block groups , hence no request lists */ <nl> + if ( list_empty ( ent )) <nl> + return NULL ; <nl> } else { <nl> blkg = container_of ( rl , struct blkcg_gq , rl ); <nl> ent = & blkg -> q_node ;
static void __devinit tg3_get_eeprom_hw_cfg ( struct tg3 * tp ) <nl> ( cfg2 & NIC_SRAM_DATA_CFG_2_APD_EN )) <nl> tp -> tg3_flags3 |= TG3_FLG3_PHY_ENABLE_APD ; <nl>  <nl> - if ( tp -> tg3_flags2 & TG3_FLG2_PCI_EXPRESS ) { <nl> + if (( tp -> tg3_flags2 & TG3_FLG2_PCI_EXPRESS ) && <nl> + GET_ASIC_REV ( tp -> pci_chip_rev_id ) != ASIC_REV_5785 && <nl> + !( tp -> tg3_flags3 & TG3_FLG3_5717_PLUS )) { <nl> u32 cfg3 ; <nl>  <nl> tg3_read_mem ( tp , NIC_SRAM_DATA_CFG_3 , & cfg3 );
struct thread_map * thread_map__new_by_uid ( uid_t uid ) <nl> { <nl> DIR * proc ; <nl> int max_threads = 32 , items , i ; <nl> - char path [ 256 ]; <nl> + char path [ NAME_MAX + 1 + 6 ]; <nl> struct dirent * dirent , ** namelist = NULL ; <nl> struct thread_map * threads = thread_map__alloc ( max_threads ); <nl> 
static inline void x86_assign_hw_event ( struct perf_event * event , <nl> hwc -> event_base = 0 ; <nl> } else if ( hwc -> idx >= X86_PMC_IDX_FIXED ) { <nl> hwc -> config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL ; <nl> - hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 ; <nl> + hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + ( hwc -> idx - X86_PMC_IDX_FIXED ); <nl> } else { <nl> hwc -> config_base = x86_pmu_config_addr ( hwc -> idx ); <nl> hwc -> event_base = x86_pmu_event_addr ( hwc -> idx );
static int tun_set_iff ( struct net * net , struct file * file , struct ifreq * ifr ) <nl> if (! dev ) <nl> return - ENOMEM ; <nl> err = dev_get_valid_name ( net , dev , name ); <nl> - if ( err ) <nl> + if ( err < 0 ) <nl> goto err_free_dev ; <nl>  <nl> dev_net_set ( dev , net );
static int load_module ( struct load_info * info , const char __user * uargs , <nl> mod_sysfs_teardown ( mod ); <nl> coming_cleanup : <nl> mod -> state = MODULE_STATE_GOING ; <nl> + destroy_params ( mod -> kp , mod -> num_kp ); <nl> blocking_notifier_call_chain (& module_notify_list , <nl> MODULE_STATE_GOING , mod ); <nl> klp_module_going ( mod );
static int gic_set_affinity ( struct irq_data * d , const struct cpumask * mask_val , <nl> int enabled ; <nl> u64 val ; <nl>  <nl> + if ( cpu >= nr_cpu_ids ) <nl> + return - EINVAL ; <nl> + <nl> if ( gic_irq_in_rdist ( d )) <nl> return - EINVAL ; <nl> 
BPF_PROG_TYPE_FNS ( tracepoint , BPF_PROG_TYPE_TRACEPOINT ); <nl> BPF_PROG_TYPE_FNS ( xdp , BPF_PROG_TYPE_XDP ); <nl> BPF_PROG_TYPE_FNS ( perf_event , BPF_PROG_TYPE_PERF_EVENT ); <nl>  <nl> -# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ), type } <nl> +# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ) - 1 , type } <nl> static const struct { <nl> const char * sec ; <nl> size_t len ;
int policydb_read ( struct policydb * p , void * fp ) <nl> } else <nl> tr -> tclass = p -> process_class ; <nl>  <nl> + rc = - EINVAL ; <nl> if (! policydb_role_isvalid ( p , tr -> role ) || <nl> ! policydb_type_isvalid ( p , tr -> type ) || <nl> ! policydb_class_isvalid ( p , tr -> tclass ) ||
int rtc_irq_set_freq ( struct rtc_device * rtc , struct rtc_task * task , int freq ) <nl> int err = 0 ; <nl> unsigned long flags ; <nl>  <nl> - if ( freq <= 0 ) <nl> + if ( freq <= 0 || freq > 5000 ) <nl> return - EINVAL ; <nl>  <nl> spin_lock_irqsave (& rtc -> irq_task_lock , flags );
static int nokia_modem_gpio_probe ( struct device * dev ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - modem -> gpios = devm_kzalloc ( dev , gpio_count * <nl> - sizeof ( struct nokia_modem_gpio ), GFP_KERNEL ); <nl> + modem -> gpios = devm_kcalloc ( dev , gpio_count , sizeof (* modem -> gpios ), <nl> + GFP_KERNEL ); <nl> if (! modem -> gpios ) { <nl> dev_err ( dev , " Could not allocate memory for gpios \ n "); <nl> return - ENOMEM ;
void __iomem * __ioremap ( unsigned long addr , unsigned long size , <nl> pa = addr & PAGE_MASK ; <nl> size = PAGE_ALIGN ( addr + size ) - pa ; <nl>  <nl> - if ( size == 0 ) <nl> + if (( size == 0 ) || ( pa == 0 )) <nl> return NULL ; <nl>  <nl> if ( mem_init_done ) {
static netdev_tx_t ipgre_tunnel_xmit ( struct sk_buff * skb , struct net_device * dev <nl> if ( skb_headroom ( skb ) < max_headroom || skb_shared ( skb )|| <nl> ( skb_cloned ( skb ) && ! skb_clone_writable ( skb , 0 ))) { <nl> struct sk_buff * new_skb = skb_realloc_headroom ( skb , max_headroom ); <nl> + if ( max_headroom > dev -> needed_headroom ) <nl> + dev -> needed_headroom = max_headroom ; <nl> if (! new_skb ) { <nl> ip_rt_put ( rt ); <nl> dev -> stats . tx_dropped ++;
minstrel_get_rate ( void * priv , struct ieee80211_sta * sta , <nl> return ; <nl> # endif <nl>  <nl> + /* Don ' t use EAPOL frames for sampling on non - mrr hw */ <nl> + if ( mp -> hw -> max_rates == 1 && <nl> + ( info -> control . flags & IEEE80211_TX_CTRL_PORT_CTRL_PROTO )) <nl> + return ; <nl> + <nl> delta = ( mi -> total_packets * sampling_ratio / 100 ) - <nl> ( mi -> sample_packets + mi -> sample_deferred / 2 ); <nl> 
static int wm5102_sysclk_ev ( struct snd_soc_dapm_widget * w , <nl> struct snd_kcontrol * kcontrol , int event ) <nl> { <nl> struct snd_soc_codec * codec = w -> codec ; <nl> - struct arizona * arizona = dev_get_drvdata ( codec -> dev ); <nl> + struct arizona * arizona = dev_get_drvdata ( codec -> dev -> parent ); <nl> struct regmap * regmap = codec -> control_data ; <nl> const struct reg_default * patch = NULL ; <nl> int i , patch_size ;
static bool ironlake_get_pipe_config ( struct intel_crtc * crtc , <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> uint32_t tmp ; <nl>  <nl> + if (! intel_display_power_enabled ( dev_priv , <nl> + POWER_DOMAIN_PIPE ( crtc -> pipe ))) <nl> + return false ; <nl> + <nl> pipe_config -> cpu_transcoder = ( enum transcoder ) crtc -> pipe ; <nl> pipe_config -> shared_dpll = DPLL_ID_PRIVATE ; <nl> 
MODULE_PARM_DESC ( cidmode , " Call - ID mode "); <nl> # define GIGASET_MODULENAME " usb_gigaset " <nl> # define GIGASET_DEVNAME " ttyGU " <nl>  <nl> -# define IF_WRITEBUF 2000 /* arbitrary limit */ <nl> +/* length limit according to Siemens 3070usb - protokoll . doc ch . 2 . 1 */ <nl> +# define IF_WRITEBUF 264 <nl>  <nl> /* Values for the Gigaset M105 Data */ <nl> # define USB_M105_VENDOR_ID 0x0681
void cs46xx_dsp_destroy_pcm_channel ( struct snd_cs46xx * chip , <nl> if (! pcm_channel -> src_scb -> ref_count ) { <nl> cs46xx_dsp_remove_scb ( chip , pcm_channel -> src_scb ); <nl>  <nl> - snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot <= DSP_MAX_SRC_NR , <nl> + snd_assert ( pcm_channel -> src_slot >= 0 && pcm_channel -> src_slot < DSP_MAX_SRC_NR , <nl> return ); <nl>  <nl> ins -> src_scb_slots [ pcm_channel -> src_slot ] = 0 ;
ext4_mb_load_buddy ( struct super_block * sb , ext4_group_t group , <nl> grp = ext4_get_group_info ( sb , group ); <nl>  <nl> e4b -> bd_blkbits = sb -> s_blocksize_bits ; <nl> - e4b -> bd_info = ext4_get_group_info ( sb , group ); <nl> + e4b -> bd_info = grp ; <nl> e4b -> bd_sb = sb ; <nl> e4b -> bd_group = group ; <nl> e4b -> bd_buddy_page = NULL ;
void rt2x00ht_create_tx_descriptor ( struct queue_entry * entry , <nl> txdesc -> mpdu_density = 0 ; <nl>  <nl> txdesc -> ba_size = 7 ; /* FIXME : What value is needed ? */ <nl> - txdesc -> stbc = 0 ; /* FIXME : What value is needed ? */ <nl> + <nl> + txdesc -> stbc = <nl> + ( tx_info -> flags & IEEE80211_TX_CTL_STBC ) >> IEEE80211_TX_CTL_STBC_SHIFT ; <nl>  <nl> txdesc -> mcs = rt2x00_get_rate_mcs ( hwrate -> mcs ); <nl> if ( txrate -> flags & IEEE80211_TX_RC_USE_SHORT_PREAMBLE )
int adf_create_ring ( struct adf_accel_dev * accel_dev , const char * section , <nl> dev_err (& GET_DEV ( accel_dev ), " Can ' t get ring number \ n "); <nl> return - EFAULT ; <nl> } <nl> + if ( ring_num >= ADF_ETR_MAX_RINGS_PER_BANK ) { <nl> + dev_err (& GET_DEV ( accel_dev ), " Invalid ring number \ n "); <nl> + return - EFAULT ; <nl> + } <nl>  <nl> bank = & transport_data -> banks [ bank_num ]; <nl> if ( adf_reserve_ring ( bank , ring_num )) {
static int ieee80211_change_bss ( struct wiphy * wiphy , <nl> sdata -> flags &= ~ IEEE80211_SDATA_DONT_BRIDGE_PACKETS ; <nl> } <nl>  <nl> + if ( params -> ht_opmode >= 0 ) { <nl> + sdata -> vif . bss_conf . ht_operation_mode = <nl> + ( u16 ) params -> ht_opmode ; <nl> + changed |= BSS_CHANGED_HT ; <nl> + } <nl> + <nl> ieee80211_bss_info_change_notify ( sdata , changed ); <nl>  <nl> return 0 ;
int cpqhp_configure_device ( struct controller * ctrl , struct pci_func * func ) <nl> } <nl>  <nl> if ( func -> pci_dev -> hdr_type == PCI_HEADER_TYPE_BRIDGE ) { <nl> + int max ; <nl> pci_read_config_byte ( func -> pci_dev , PCI_SECONDARY_BUS , & bus ); <nl> child = ( struct pci_bus *) pci_add_new_bus ( func -> pci_dev -> bus , ( func -> pci_dev ), bus ); <nl> - pci_do_scan_bus ( child ); <nl> + max = pci_do_scan_bus ( child ); <nl> + pci_bus_update_busn_res_end ( child , max ); <nl> } <nl>  <nl> pci_dev_put ( func -> pci_dev );
static void pl011_dma_probe ( struct uart_amba_port * uap ) <nl> /* Optionally make use of an RX channel as well */ <nl> chan = dma_request_slave_channel ( dev , " rx "); <nl>  <nl> - if (! chan && plat -> dma_rx_param ) { <nl> + if (! chan && plat && plat -> dma_rx_param ) { <nl> chan = dma_request_channel ( mask , plat -> dma_filter , plat -> dma_rx_param ); <nl>  <nl> if (! chan ) {
static struct platform_driver rk_iommu_driver = { <nl>  <nl> static int __init rk_iommu_init ( void ) <nl> { <nl> + struct device_node * np ; <nl> int ret ; <nl>  <nl> + np = of_find_matching_node ( NULL , rk_iommu_dt_ids ); <nl> + if (! np ) <nl> + return 0 ; <nl> + <nl> + of_node_put ( np ); <nl> + <nl> ret = bus_set_iommu (& platform_bus_type , & rk_iommu_ops ); <nl> if ( ret ) <nl> return ret ;
int i40e_ndo_get_vf_config ( struct net_device * netdev , <nl> else <nl> ivi -> linkstate = IFLA_VF_LINK_STATE_DISABLE ; <nl> ivi -> spoofchk = vf -> spoofchk ; <nl> + ivi -> trusted = vf -> trusted ; <nl> ret = 0 ; <nl>  <nl> error_param :
SMB2_tcon ( const unsigned int xid , struct cifs_ses * ses , const char * tree , <nl> tcon_error_exit : <nl> if ( rsp -> hdr . Status == STATUS_BAD_NETWORK_NAME ) { <nl> cifs_dbg ( VFS , " BAD_NETWORK_NAME : % s \ n ", tree ); <nl> - tcon -> bad_network_name = true ; <nl> + if ( tcon ) <nl> + tcon -> bad_network_name = true ; <nl> } <nl> goto tcon_exit ; <nl> }
static int xadc_parse_dt ( struct iio_dev * indio_dev , struct device_node * np , <nl> chan -> address = XADC_REG_VPVN ; <nl> } else { <nl> chan -> scan_index = 15 + reg ; <nl> - chan -> scan_index = XADC_REG_VAUX ( reg - 1 ); <nl> + chan -> address = XADC_REG_VAUX ( reg - 1 ); <nl> } <nl> num_channels ++; <nl> chan ++;
void __init reserve_early_overlap_ok ( u64 start , u64 end , char * name ) <nl> */ <nl> void __init reserve_early ( u64 start , u64 end , char * name ) <nl> { <nl> + if ( start >= end ) <nl> + return ; <nl> + <nl> drop_overlaps_that_are_ok ( start , end ); <nl> __reserve_early ( start , end , name , 0 ); <nl> }
static int __init dw_probe ( struct platform_device * pdev ) <nl> dma_writel ( dw , CFG , DW_CFG_DMA_EN ); <nl>  <nl> printk ( KERN_INFO "% s : DesignWare DMA Controller , % d channels \ n ", <nl> - pdev -> dev . bus_id , dw -> dma . chancnt ); <nl> + dev_name (& pdev -> dev ), dw -> dma . chancnt ); <nl>  <nl> dma_async_device_register (& dw -> dma ); <nl> 
struct dst_entry * fib6_rule_lookup ( struct net * net , struct flowi6 * fl6 , <nl> if ( rt -> dst . error == - EAGAIN ) { <nl> ip6_rt_put_flags ( rt , flags ); <nl> rt = net -> ipv6 . ip6_null_entry ; <nl> - if (!( flags | RT6_LOOKUP_F_DST_NOREF )) <nl> + if (!( flags & RT6_LOOKUP_F_DST_NOREF )) <nl> dst_hold (& rt -> dst ); <nl> } <nl> 
static int key_notify_policy_flush ( const struct km_event * c ) <nl> hdr -> sadb_msg_pid = c -> portid ; <nl> hdr -> sadb_msg_version = PF_KEY_V2 ; <nl> hdr -> sadb_msg_errno = ( uint8_t ) 0 ; <nl> + hdr -> sadb_msg_satype = SADB_SATYPE_UNSPEC ; <nl> hdr -> sadb_msg_len = ( sizeof ( struct sadb_msg ) / sizeof ( uint64_t )); <nl> pfkey_broadcast ( skb_out , GFP_ATOMIC , BROADCAST_ALL , NULL , c -> net ); <nl> return 0 ;
static void mcam_ctlr_image ( struct mcam_camera * cam ) <nl> mcam_reg_write_mask ( cam , REG_CTRL0 , <nl> C0_DF_RGB | C0_RGBF_565 | C0_RGB5_BGGR , C0_DF_MASK ); <nl> break ; <nl> + case V4L2_PIX_FMT_SBGGR8 : <nl> + mcam_reg_write_mask ( cam , REG_CTRL0 , <nl> + C0_DF_RGB | C0_RGB5_GRBG , C0_DF_MASK ); <nl> + break ; <nl> default : <nl> cam_err ( cam , " camera : unknown format : %# x \ n ", fmt -> pixelformat ); <nl> break ;
static int sched_read_attr ( struct sched_attr __user * uattr , <nl> attr -> size = usize ; <nl> } <nl>  <nl> - ret = copy_to_user ( uattr , attr , usize ); <nl> + ret = copy_to_user ( uattr , attr , attr -> size ); <nl> if ( ret ) <nl> return - EFAULT ; <nl> 
struct lcd_device * lcd_device_register ( const char * name , struct device * parent , <nl>  <nl> rc = device_register (& new_ld -> dev ); <nl> if ( rc ) { <nl> - kfree ( new_ld ); <nl> + put_device (& new_ld -> dev ); <nl> return ERR_PTR ( rc ); <nl> } <nl> 
static void disk_seqf_stop ( struct seq_file * seqf , void * v ) <nl> if ( iter ) { <nl> class_dev_iter_exit ( iter ); <nl> kfree ( iter ); <nl> + seqf -> private = NULL ; <nl> } <nl> } <nl> 
u32 __tcp_select_window ( struct sock * sk ) <nl> */ <nl> if ( window <= free_space - mss || window > free_space ) <nl> window = ( free_space / mss )* mss ; <nl> + else if ( mss == full_space && <nl> + free_space > window + full_space / 2 ) <nl> + window = free_space ; <nl> } <nl>  <nl> return window ;
struct g2d_runqueue_node { <nl> struct list_head list ; <nl> struct list_head run_cmdlist ; <nl> struct list_head event_list ; <nl> + pid_t pid ; <nl> struct completion complete ; <nl> int async ; <nl> }; <nl> int exynos_g2d_exec_ioctl ( struct drm_device * drm_dev , void * data , <nl> } <nl>  <nl> mutex_lock (& g2d -> runqueue_mutex ); <nl> + runqueue_node -> pid = current -> pid ; <nl> list_add_tail (& runqueue_node -> list , & g2d -> runqueue ); <nl> if (! g2d -> runqueue_node ) <nl> g2d_exec_runqueue ( g2d );
int sctp_packet_transmit ( struct sctp_packet * packet ) <nl> return err ; <nl> no_route : <nl> kfree_skb ( nskb ); <nl> - IP_INC_STATS_BH ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl> + IP_INC_STATS ( sock_net ( asoc -> base . sk ), IPSTATS_MIB_OUTNOROUTES ); <nl>  <nl> /* FIXME : Returning the ' err ' will effect all the associations <nl> * associated with a socket , although only one of the paths of the
void intel_irq_init ( struct drm_device * dev ) <nl> dev -> driver -> get_vblank_counter = gm45_get_vblank_counter ; <nl> } <nl>  <nl> - <nl> - dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + if ( drm_core_check_feature ( dev , DRIVER_MODESET )) <nl> + dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + else <nl> + dev -> driver -> get_vblank_timestamp = NULL ; <nl> dev -> driver -> get_scanout_position = i915_get_crtc_scanoutpos ; <nl>  <nl> if ( IS_IVYBRIDGE ( dev )) {
static int configure_tda827x_fe ( struct saa7134_dev * dev , <nl> /* Get the first frontend */ <nl> fe0 = videobuf_dvb_get_frontend (& dev -> frontends , 1 ); <nl>  <nl> + if (! fe0 ) <nl> + return - EINVAL ; <nl> + <nl> fe0 -> dvb . frontend = dvb_attach ( tda10046_attach , cdec_conf , & dev -> i2c_adap ); <nl> if ( fe0 -> dvb . frontend ) { <nl> if ( cdec_conf -> i2c_gate )
static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
int hists__browse ( struct hists * self , const char * helpline , const char * ev_name ) <nl>  <nl> switch ( key ) { <nl> case ' a ': <nl> - if ( browser -> selection -> map == NULL && <nl> + if ( browser -> selection -> map == NULL || <nl> browser -> selection -> map -> dso -> annotate_warned ) <nl> continue ; <nl> goto do_annotate ;
static int load_firmware ( struct octeon_device * oct ) <nl> char fw_name [ LIO_MAX_FW_FILENAME_LEN ]; <nl> char * tmp_fw_type ; <nl>  <nl> - if ( fw_type_is_auto ()) <nl> + if ( fw_type_is_auto ()) { <nl> tmp_fw_type = LIO_FW_NAME_TYPE_NIC ; <nl> - else <nl> + strncpy ( fw_type , tmp_fw_type , sizeof ( fw_type )); <nl> + } else { <nl> tmp_fw_type = fw_type ; <nl> + } <nl>  <nl> sprintf ( fw_name , "% s % s % s_ % s % s ", LIO_FW_DIR , LIO_FW_BASE_NAME , <nl> octeon_get_conf ( oct )-> card_name , tmp_fw_type ,
void __detach_mounts ( struct dentry * dentry ) <nl>  <nl> namespace_lock (); <nl> mp = lookup_mountpoint ( dentry ); <nl> - if (! mp ) <nl> + if ( IS_ERR_OR_NULL ( mp )) <nl> goto out_unlock ; <nl>  <nl> lock_mount_hash ();
static int handle_vmcall ( struct kvm_vcpu * vcpu ) <nl> return 1 ; <nl> } <nl>  <nl> - static int handle_vmx_insn ( struct kvm_vcpu * vcpu ) <nl> -{ <nl> - kvm_queue_exception ( vcpu , UD_VECTOR ); <nl> - return 1 ; <nl> -} <nl> - <nl> static int handle_invd ( struct kvm_vcpu * vcpu ) <nl> { <nl> return emulate_instruction ( vcpu , 0 ) == EMULATE_DONE ;
static cycle_t e1000e_cyclecounter_read ( const struct cyclecounter * cc ) <nl> systimeh = er32 ( SYSTIMH ); <nl> systimel_2 = er32 ( SYSTIML ); <nl> /* Check for overflow . If there was no overflow , use the values */ <nl> - if ( systimel_1 < systimel_2 ) { <nl> + if ( systimel_1 <= systimel_2 ) { <nl> systim = ( cycle_t ) systimel_1 ; <nl> systim |= ( cycle_t ) systimeh << 32 ; <nl> } else {
int vmw_get_cap_3d_ioctl ( struct drm_device * dev , void * data , <nl> memcpy_fromio ( bounce , & fifo_mem [ SVGA_FIFO_3D_CAPS ], size ); <nl>  <nl> ret = copy_to_user ( buffer , bounce , size ); <nl> + if ( ret ) <nl> + ret = - EFAULT ; <nl> vfree ( bounce ); <nl>  <nl> if ( unlikely ( ret != 0 ))
static int try_smi_init ( struct smi_info * new_smi ) <nl> */ <nl> new_smi -> pdev = platform_device_alloc (" ipmi_si ", <nl> new_smi -> intf_num ); <nl> - if ( rv ) { <nl> + if (! new_smi -> pdev ) { <nl> printk ( KERN_ERR <nl> " ipmi_si_intf :" <nl> " Unable to allocate platform device \ n ");
static int parse_audio_selector_unit ( struct mixer_build * state , int unitid , unsi <nl> kctl = snd_ctl_new1 (& mixer_selectunit_ctl , cval ); <nl> if (! kctl ) { <nl> snd_printk ( KERN_ERR " cannot malloc kcontrol \ n "); <nl> + kfree ( namelist ); <nl> kfree ( cval ); <nl> return - ENOMEM ; <nl> }
static int handle_emulation_failure ( struct kvm_vcpu * vcpu ) <nl> vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> vcpu -> run -> internal . ndata = 0 ; <nl> - r = EMULATE_FAIL ; <nl> + r = EMULATE_USER_EXIT ; <nl> } <nl> kvm_queue_exception ( vcpu , UD_VECTOR ); <nl> 
ixgb_restore_vlan ( struct ixgb_adapter * adapter ) <nl>  <nl> static void ixgb_netpoll ( struct net_device * dev ) <nl> { <nl> - struct ixgb_adapter * adapter = dev -> priv ; <nl> + struct ixgb_adapter * adapter = netdev_priv ( dev ); <nl>  <nl> disable_irq ( adapter -> pdev -> irq ); <nl> ixgb_intr ( adapter -> pdev -> irq , dev , NULL );
static void i40evf_adminq_task ( struct work_struct * work ) <nl>  <nl> /* check for error indications */ <nl> val = rd32 ( hw , hw -> aq . arq . len ); <nl> + if ( val == 0xdeadbeef ) /* indicates device in reset */ <nl> + goto freedom ; <nl> oldval = val ; <nl> if ( val & I40E_VF_ARQLEN1_ARQVFE_MASK ) { <nl> dev_info (& adapter -> pdev -> dev , " ARQ VF Error detected \ n ");
int afu_register_irqs ( struct cxl_context * ctx , u32 count ) <nl> */ <nl> INIT_LIST_HEAD (& ctx -> irq_names ); <nl> for ( r = 1 ; r < CXL_IRQ_RANGES ; r ++) { <nl> - for ( i = 0 ; i < ctx -> irqs . range [ r ]; hwirq ++, i ++) { <nl> + for ( i = 0 ; i < ctx -> irqs . range [ r ]; i ++) { <nl> irq_name = kmalloc ( sizeof ( struct cxl_irq_name ), <nl> GFP_KERNEL ); <nl> if (! irq_name )
static void __init cpg_mssr_register_mod_clk ( const struct mssr_mod_clk * mod , <nl> # else <nl> dev_dbg ( dev , " Ignoring MSTP % s to prevent disabling \ n ", <nl> mod -> name ); <nl> + kfree ( clock ); <nl> return ; <nl> # endif <nl> }
static ssize_t ci_port_test_write ( struct file * file , const char __user * ubuf , <nl> if ( sscanf ( buf , "% u ", & mode ) != 1 ) <nl> return - EINVAL ; <nl>  <nl> + if ( mode > 255 ) <nl> + return - EBADRQC ; <nl> + <nl> pm_runtime_get_sync ( ci -> dev ); <nl> spin_lock_irqsave (& ci -> lock , flags ); <nl> ret = hw_port_test_set ( ci , mode );
static void __init reset_all_timers ( void ) <nl> * In other cases ( such as with VSAless OpenFirmware ), the system firmware <nl> * leaves timers available for us to use . <nl> */ <nl> - static int __init scan_timers ( struct cs5535_mfgpt_chip * mfgpt ) <nl> + static int __devinit scan_timers ( struct cs5535_mfgpt_chip * mfgpt ) <nl> { <nl> struct cs5535_mfgpt_timer timer = { . chip = mfgpt }; <nl> unsigned long flags ;
static int iscsi_login_zero_tsih_s1 ( <nl> if ( IS_ERR ( sess -> se_sess )) { <nl> iscsit_tx_login_rsp ( conn , ISCSI_STATUS_CLS_TARGET_ERR , <nl> ISCSI_LOGIN_STATUS_NO_RESOURCES ); <nl> + kfree ( sess -> sess_ops ); <nl> kfree ( sess ); <nl> return - ENOMEM ; <nl> }
static int btrfs_extent_same ( struct inode * src , u64 loff , u64 len , <nl> if ( src == dst ) <nl> return - EINVAL ; <nl>  <nl> + if ( len == 0 ) <nl> + return 0 ; <nl> + <nl> btrfs_double_lock ( src , loff , dst , dst_loff , len ); <nl>  <nl> ret = extent_same_check_offsets ( src , loff , len );
static irqreturn_t ab8500_irq ( int irq , void * dev ) <nl> do { <nl> int bit = __ffs ( value ); <nl> int line = i * 8 + bit ; <nl> + int virq = ab8500_irq_get_virq ( ab8500 , line ); <nl>  <nl> - handle_nested_irq ( ab8500 -> irq_base + line ); <nl> + handle_nested_irq ( virq ); <nl> value &= ~( 1 << bit ); <nl>  <nl> } while ( value );
static int rcar_gen2_usb_phy_probe ( struct platform_device * pdev ) <nl> priv -> phy . shutdown = rcar_gen2_usb_phy_shutdown ; <nl> priv -> phy . set_suspend = rcar_gen2_usb_phy_set_suspend ; <nl>  <nl> - retval = usb_add_phy (& priv -> phy , USB_PHY_TYPE_USB2 ); <nl> + retval = usb_add_phy_dev (& priv -> phy ); <nl> if ( retval < 0 ) { <nl> dev_err ( dev , " Failed to add USB phy \ n "); <nl> return retval ;
static inline int stack_map_data_size ( struct bpf_map * map ) <nl>  <nl> static int prealloc_elems_and_freelist ( struct bpf_stack_map * smap ) <nl> { <nl> - u32 elem_size = sizeof ( struct stack_map_bucket ) + smap -> map . value_size ; <nl> + u64 elem_size = sizeof ( struct stack_map_bucket ) + <nl> + ( u64 ) smap -> map . value_size ; <nl> int err ; <nl>  <nl> smap -> elems = bpf_map_area_alloc ( elem_size * smap -> map . max_entries ,
static int f2fs_move_file_range ( struct file * file_in , loff_t pos_in , <nl> if ( f2fs_encrypted_inode ( src ) || f2fs_encrypted_inode ( dst )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( src == dst ) { <nl> + if ( pos_in == pos_out ) <nl> + return 0 ; <nl> + if ( pos_out > pos_in && pos_out < pos_in + len ) <nl> + return - EINVAL ; <nl> + } <nl> + <nl> inode_lock ( src ); <nl> if ( src != dst ) { <nl> if (! inode_trylock ( dst )) {
static inline void tpg_s_bytesperline ( struct tpg_data * tpg , unsigned plane , unsi <nl>  <nl> tpg -> bytesperline [ p ] = plane_w / tpg -> hdownsampling [ p ]; <nl> } <nl> + if ( tpg_g_interleaved ( tpg )) <nl> + tpg -> bytesperline [ 1 ] = tpg -> bytesperline [ 0 ]; <nl> } <nl>  <nl> 
enum nvkm_devidx { <nl> NVKM_SUBDEV_MC , <nl> NVKM_SUBDEV_BUS , <nl> NVKM_SUBDEV_TIMER , <nl> + NVKM_SUBDEV_INSTMEM , <nl> NVKM_SUBDEV_FB , <nl> NVKM_SUBDEV_LTC , <nl> - NVKM_SUBDEV_INSTMEM , <nl> NVKM_SUBDEV_MMU , <nl> NVKM_SUBDEV_BAR , <nl> NVKM_SUBDEV_PMU ,
int rds_cmsg_atomic ( struct rds_sock * rs , struct rds_message * rm , <nl> err : <nl> if ( page ) <nl> put_page ( page ); <nl> + rm -> atomic . op_active = 0 ; <nl> kfree ( rm -> atomic . op_notifier ); <nl>  <nl> return ret ;
static struct page ** __iommu_alloc_buffer ( struct device * dev , size_t size , <nl> int i = 0 ; <nl>  <nl> if ( array_size <= PAGE_SIZE ) <nl> - pages = kzalloc ( array_size , gfp ); <nl> + pages = kzalloc ( array_size , GFP_KERNEL ); <nl> else <nl> pages = vzalloc ( array_size ); <nl> if (! pages )
static int cpufreq_add_dev ( struct sys_device * sys_dev ) <nl>  <nl> spin_lock_irqsave (& cpufreq_driver_lock , flags ); <nl> for_each_cpu ( j , policy -> cpus ) { <nl> + if (! cpu_online ( j )) <nl> + continue ; <nl> per_cpu ( cpufreq_cpu_data , j ) = policy ; <nl> per_cpu ( policy_cpu , j ) = policy -> cpu ; <nl> }
static int find_parent_nodes ( struct btrfs_trans_handle * trans , <nl> } <nl> ret = find_extent_in_eb ( eb , bytenr , <nl> * extent_item_pos , & eie ); <nl> - ref -> inode_list = eie ; <nl> free_extent_buffer ( eb ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + ref -> inode_list = eie ; <nl> } <nl> ret = ulist_add_merge ( refs , ref -> parent , <nl> ( uintptr_t ) ref -> inode_list ,
static acpi_status WMID_set_capabilities ( void ) <nl> devices = *(( u32 *) obj -> buffer . pointer ); <nl> } else if ( obj -> type == ACPI_TYPE_INTEGER ) { <nl> devices = ( u32 ) obj -> integer . value ; <nl> + } else { <nl> + kfree ( out . pointer ); <nl> + return AE_ERROR ; <nl> } <nl> } else { <nl> kfree ( out . pointer );
static inline int __mkroute_input ( struct sk_buff * skb , <nl> # endif <nl> if ( in_dev -> cnf . no_policy ) <nl> rth -> u . dst . flags |= DST_NOPOLICY ; <nl> - if ( in_dev -> cnf . no_xfrm ) <nl> + if ( out_dev -> cnf . no_xfrm ) <nl> rth -> u . dst . flags |= DST_NOXFRM ; <nl> rth -> fl . fl4_dst = daddr ; <nl> rth -> rt_dst = daddr ;
# endif <nl> # include " igb . h " <nl>  <nl> -# define DRV_VERSION " 2 . 4 . 13 - k2 " <nl> +# define MAJ 3 <nl> +# define MIN 0 <nl> +# define BUILD 6 <nl> +# define KFIX 2 <nl> +# define DRV_VERSION __stringify ( MAJ ) "." __stringify ( MIN ) "." \ <nl> + __stringify ( BUILD ) "- k " __stringify ( KFIX ) <nl> char igb_driver_name [] = " igb "; <nl> char igb_driver_version [] = DRV_VERSION ; <nl> static const char igb_driver_string [] =
static bool radeon_atom_apply_quirks ( struct drm_device * dev , <nl> if (( supported_device == ATOM_DEVICE_CRT1_SUPPORT ) || <nl> ( supported_device == ATOM_DEVICE_DFP2_SUPPORT )) <nl> return false ; <nl> + if ( supported_device == ATOM_DEVICE_CRT2_SUPPORT ) <nl> + * line_mux = 0x90 ; <nl> } <nl>  <nl> /* ASUS HD 3600 XT board lists the DVI port as HDMI */
static int futex_unlock_pi ( u32 __user * uaddr , struct rw_semaphore * fshared ) <nl> attempt ); <nl> if ( ret ) <nl> goto out ; <nl> + uval = 0 ; <nl> goto retry_unlocked ; <nl> } <nl> 
static void local_exit ( void ) <nl> DMINFO (" cleaned up "); <nl> } <nl>  <nl> - int (* _inits [])( void ) __initdata = { <nl> + static int (* _inits [])( void ) __initdata = { <nl> local_init , <nl> dm_target_init , <nl> dm_linear_init , <nl> int (* _inits [])( void ) __initdata = { <nl> dm_interface_init , <nl> }; <nl>  <nl> - void (* _exits [])( void ) = { <nl> + static void (* _exits [])( void ) = { <nl> local_exit , <nl> dm_target_exit , <nl> dm_linear_exit ,
static int __ocfs2_move_extent ( handle_t * handle , <nl> } <nl>  <nl> out : <nl> + ocfs2_free_path ( path ); <nl> return ret ; <nl> } <nl> 
static void change_port_settings ( struct tty_struct * tty , <nl> if (! baud ) { <nl> /* pick a default , any default ... */ <nl> baud = 9600 ; <nl> - } else <nl> + } else { <nl> + /* Avoid a zero divisor . */ <nl> + baud = min ( baud , 461550 ); <nl> tty_encode_baud_rate ( tty , baud , baud ); <nl> + } <nl>  <nl> edge_port -> baud_rate = baud ; <nl> config -> wBaudRate = ( __u16 )(( 461550L + baud / 2 ) / baud );
static void setup_frame_info ( struct ieee80211_hw * hw , <nl> fi -> keyix = ATH9K_TXKEYIX_INVALID ; <nl> fi -> keytype = keytype ; <nl> fi -> framelen = framelen ; <nl> + <nl> + if (! rate ) <nl> + return ; <nl> fi -> rtscts_rate = rate -> hw_value ; <nl> if ( short_preamble ) <nl> fi -> rtscts_rate |= rate -> hw_value_short ;
static void __unregister_request ( struct ceph_osd_client * osdc , <nl> */ <nl> static void __cancel_request ( struct ceph_osd_request * req ) <nl> { <nl> - if ( req -> r_sent ) { <nl> + if ( req -> r_sent && req -> r_osd ) { <nl> ceph_con_revoke (& req -> r_osd -> o_con , req -> r_request ); <nl> req -> r_sent = 0 ; <nl> }
int v4l2_m2m_streamoff ( struct file * file , struct v4l2_m2m_ctx * m2m_ctx , <nl> /* Drop queue , since streamoff returns device to the same state as after <nl> * calling reqbufs . */ <nl> INIT_LIST_HEAD (& q_ctx -> rdy_queue ); <nl> + q_ctx -> num_rdy = 0 ; <nl> spin_unlock_irqrestore (& q_ctx -> rdy_spinlock , flags ); <nl>  <nl> if ( m2m_dev -> curr_ctx == m2m_ctx ) {
static const struct key_entry acer_wmi_keymap [] = { <nl> { KE_KEY , 0x23 , { KEY_PROG3 } }, /* P_Key */ <nl> { KE_KEY , 0x24 , { KEY_PROG4 } }, /* Social networking_Key */ <nl> { KE_KEY , 0x64 , { KEY_SWITCHVIDEOMODE } }, /* Display Switch */ <nl> - { KE_KEY , 0x82 , { KEY_F22 } }, /* Touch Pad On / Off */ <nl> + { KE_KEY , 0x82 , { KEY_TOUCHPAD_TOGGLE } }, /* Touch Pad On / Off */ <nl> { KE_END , 0 } <nl> }; <nl> 
static int rt5514_dsp_voice_wake_up_put ( struct snd_kcontrol * kcontrol , <nl> # else <nl> dev_err ( component -> dev , " There is no SPI driver for " <nl> " loading the firmware \ n "); <nl> + memset ( buf , 0 , sizeof ( buf )); <nl> # endif <nl> rt5514 -> pll3_cal_value = buf [ 0 ] | buf [ 1 ] << 8 | <nl> buf [ 2 ] << 16 | buf [ 3 ] << 24 ;
static int alps_enter_command_mode ( struct psmouse * psmouse , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( param [ 0 ] != 0x88 && param [ 1 ] != 0x07 ) { <nl> + if ( param [ 0 ] != 0x88 || ( param [ 1 ] != 0x07 && param [ 1 ] != 0x08 )) { <nl> psmouse_dbg ( psmouse , <nl> " unknown response while entering command mode \ n "); <nl> return - 1 ;
static void __devinit bnx2x_link_settings_supported ( struct bnx2x * bp , <nl> break ; <nl>  <nl> case PORT_HW_CFG_XGXS_EXT_PHY_TYPE_BCM8481 : <nl> - BNX2X_DEV_INFO (" ext_phy_type 0x % x ( BCM8481 )\ n ", <nl> + case PORT_HW_CFG_XGXS_EXT_PHY_TYPE_BCM84823 : <nl> + BNX2X_DEV_INFO (" ext_phy_type 0x % x ( BCM848xx )\ n ", <nl> ext_phy_type ); <nl>  <nl> bp -> port . supported |= ( SUPPORTED_10baseT_Half |
static int xen_add_device ( struct device * dev ) <nl>  <nl> # ifdef CONFIG_ACPI <nl> handle = DEVICE_ACPI_HANDLE (& pci_dev -> dev ); <nl> - if (! handle ) <nl> + if (! handle && pci_dev -> bus -> bridge ) <nl> handle = DEVICE_ACPI_HANDLE ( pci_dev -> bus -> bridge ); <nl> # ifdef CONFIG_PCI_IOV <nl> if (! handle && pci_dev -> is_virtfn )
static int ar9003_hw_set_channel ( struct ath_hw * ah , struct ath9k_channel * chan ) <nl> u32 chan_frac ; <nl>  <nl> channelSel = ( freq * 2 ) / 75 ; <nl> - chan_frac = (( freq % 75 ) * 0x20000 ) / 75 ; <nl> + chan_frac = ((( freq * 2 ) % 75 ) * 0x20000 ) / 75 ; <nl> channelSel = ( channelSel << 17 ) | chan_frac ; <nl> } else { <nl> channelSel = CHANSEL_5G ( freq );
int iomap_fiemap ( struct inode * inode , struct fiemap_extent_info * fi , <nl> while ( len > 0 ) { <nl> ret = iomap_apply ( inode , start , len , 0 , ops , & ctx , <nl> iomap_fiemap_actor ); <nl> + /* inode with no ( attribute ) mapping will give ENOENT */ <nl> + if ( ret == - ENOENT ) <nl> + break ; <nl> if ( ret < 0 ) <nl> return ret ; <nl> if ( ret == 0 )
struct platform_device * __init imx_add_platform_device_dmamask ( <nl> ret = platform_device_add ( pdev ); <nl> if ( ret ) { <nl> err : <nl> + if ( dmamask ) <nl> + kfree ( pdev -> dev . dma_mask ); <nl> platform_device_put ( pdev ); <nl> return ERR_PTR ( ret ); <nl> }
static void mmu_pte_write_zap_pte ( struct kvm_vcpu * vcpu , <nl> mmu_page_remove_parent_pte ( child , spte ); <nl> } <nl> } <nl> - * spte = 0 ; <nl> + set_shadow_pte ( spte , 0 ); <nl> kvm_flush_remote_tlbs ( vcpu -> kvm ); <nl> } <nl> 
static int intel_pstate_set_policy ( struct cpufreq_policy * policy ) <nl> limits -> max_sysfs_pct ); <nl> limits -> max_perf_pct = max ( limits -> min_policy_pct , <nl> limits -> max_perf_pct ); <nl> + limits -> max_perf = round_up ( limits -> max_perf , 8 ); <nl>  <nl> /* Make sure min_perf_pct <= max_perf_pct */ <nl> limits -> min_perf_pct = min ( limits -> max_perf_pct , limits -> min_perf_pct );
static int clie_5_attach ( struct usb_serial * serial ) <nl> */ <nl>  <nl> /* some sanity check */ <nl> - if ( serial -> num_ports < 2 ) <nl> - return - 1 ; <nl> + if ( serial -> num_bulk_out < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing bulk out endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> /* port 0 now uses the modified endpoint Address */ <nl> port = serial -> port [ 0 ];
ecryptfs_setxattr ( struct dentry * dentry , const char * name , const void * value , <nl> } <nl>  <nl> rc = vfs_setxattr ( lower_dentry , name , value , size , flags ); <nl> + if (! rc ) <nl> + fsstack_copy_attr_all ( dentry -> d_inode , lower_dentry -> d_inode ); <nl> out : <nl> return rc ; <nl> }
static void __init atmci_get_cap ( struct atmel_mci * host ) <nl> /* keep only major version number */ <nl> switch ( version & 0xf00 ) { <nl> case 0x100 : <nl> + host -> caps . has_pdc = 1 ; <nl> + break ; <nl> case 0x200 : <nl> host -> caps . has_pdc = 1 ; <nl> host -> caps . has_rwproof = 1 ;
static s32 i2c_smbus_xfer_emulated ( struct i2c_adapter * adapter , u16 addr , <nl> status = i2c_transfer ( adapter , msg , num ); <nl> if ( status < 0 ) <nl> return status ; <nl> + if ( status != num ) <nl> + return - EIO ; <nl>  <nl> /* Check PEC if last message is a read */ <nl> if ( i && ( msg [ num - 1 ]. flags & I2C_M_RD )) {
static void do_config_file ( const char * filename ) <nl> perror ( filename ); <nl> exit ( 2 ); <nl> } <nl> - fstat ( fd , & st ); <nl> + if ( fstat ( fd , & st ) < 0 ) { <nl> + fprintf ( stderr , " fixdep : error fstat ' ing config file : "); <nl> + perror ( filename ); <nl> + exit ( 2 ); <nl> + } <nl> if ( st . st_size == 0 ) { <nl> close ( fd ); <nl> return ;
static int stmmac_hw_init ( struct stmmac_priv * priv ) <nl> struct mac_device_info * mac ; <nl>  <nl> /* Identify the MAC HW device */ <nl> - if ( priv -> plat -> has_gmac ) <nl> + if ( priv -> plat -> has_gmac ) { <nl> + priv -> dev -> priv_flags |= IFF_UNICAST_FLT ; <nl> mac = dwmac1000_setup ( priv -> ioaddr ); <nl> - else <nl> + } else { <nl> mac = dwmac100_setup ( priv -> ioaddr ); <nl> + } <nl> if (! mac ) <nl> return - ENOMEM ; <nl> 
static int msr_open ( struct inode * inode , struct file * file ) <nl> unsigned int cpu ; <nl> struct cpuinfo_x86 * c ; <nl>  <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> + <nl> cpu = iminor ( file -> f_path . dentry -> d_inode ); <nl> if ( cpu >= nr_cpu_ids || ! cpu_online ( cpu )) <nl> return - ENXIO ; /* No such CPU */
static int synic_set_irq ( struct kvm_vcpu_hv_synic * synic , u32 sint ) <nl> struct kvm_lapic_irq irq ; <nl> int ret , vector ; <nl>  <nl> + if ( KVM_BUG_ON (! lapic_in_kernel ( vcpu ), vcpu -> kvm )) <nl> + return - EINVAL ; <nl> + <nl> if ( sint >= ARRAY_SIZE ( synic -> sint )) <nl> return - EINVAL ; <nl> 
void check_and_switch_context ( struct mm_struct * mm , unsigned int cpu ) <nl> raw_spin_unlock_irqrestore (& cpu_asid_lock , flags ); <nl>  <nl> switch_mm_fastpath : <nl> + <nl> + arm64_apply_bp_hardening (); <nl> + <nl> /* <nl> * Defer TTBR0_EL1 setting for user threads to uaccess_enable () when <nl> * emulating PAN . <nl> asmlinkage void post_ttbr_update_workaround ( void ) <nl> " ic iallu ; dsb nsh ; isb ", <nl> ARM64_WORKAROUND_CAVIUM_27456 , <nl> CONFIG_CAVIUM_ERRATUM_27456 )); <nl> - <nl> - arm64_apply_bp_hardening (); <nl> } <nl>  <nl> static int asids_init ( void )
void dma_run_dependencies ( struct dma_async_tx_descriptor * tx ) <nl> if (! dep ) <nl> return ; <nl>  <nl> + /* we ' ll submit tx -> next now , so clear the link */ <nl> + tx -> next = NULL ; <nl> chan = dep -> chan ; <nl>  <nl> /* keep submitting up until a channel switch is detected
static void vmw_fb_dirty_flush ( struct vmw_fb_par * par ) <nl> SVGAFifoCmdUpdate body ; <nl> } * cmd ; <nl>  <nl> + if ( vmw_priv -> suspended ) <nl> + return ; <nl> + <nl> spin_lock_irqsave (& par -> dirty . lock , flags ); <nl> if (! par -> dirty . active ) { <nl> spin_unlock_irqrestore (& par -> dirty . lock , flags );
int rtw_disassoc_cmd23a ( struct rtw_adapter * padapter , u32 deauth_timeout_ms , <nl> } else { <nl> /* no need to enqueue , do the cmd hdl directly and <nl> free cmd parameter */ <nl> - if ( H2C_SUCCESS != disconnect_hdl23a ( padapter , ( u8 *) param )) <nl> + if ( disconnect_hdl23a ( padapter , ( u8 *) param ) != H2C_SUCCESS ) <nl> res = _FAIL ; <nl> kfree ( param ); <nl> }
static void edge_bulk_in_callback ( struct urb * urb ) <nl>  <nl> port_number = edge_port -> port -> port_number ; <nl>  <nl> - if ( edge_port -> lsr_event ) { <nl> + if ( urb -> actual_length > 0 && edge_port -> lsr_event ) { <nl> edge_port -> lsr_event = 0 ; <nl> dev_dbg ( dev , "% s ===== Port % u LSR Status = % 02x , Data = % 02x ======\ n ", <nl> __func__ , port_number , edge_port -> lsr_mask , * data );
static int process_ctrl_td ( struct xhci_hcd * xhci , struct xhci_td * td , <nl> case TRB_NORMAL : <nl> td -> urb -> actual_length = requested - remaining ; <nl> goto finish_td ; <nl> + case TRB_STATUS : <nl> + td -> urb -> actual_length = requested ; <nl> + goto finish_td ; <nl> default : <nl> xhci_warn ( xhci , " WARN : unexpected TRB Type % d \ n ", <nl> trb_type );
int iio_sw_buffer_preenable ( struct iio_dev * indio_dev ) <nl> buffer -> scan_mask ); <nl> else <nl> indio_dev -> active_scan_mask = buffer -> scan_mask ; <nl> + <nl> + if ( indio_dev -> active_scan_mask == NULL ) <nl> + return - EINVAL ; <nl> + <nl> iio_update_demux ( indio_dev ); <nl>  <nl> if ( indio_dev -> info -> update_scan_mode )
int ipu_dp_init ( struct ipu_soc * ipu , struct device * dev , unsigned long base ) <nl> int i ; <nl>  <nl> priv = devm_kzalloc ( dev , sizeof (* priv ), GFP_KERNEL ); <nl> + if (! priv ) <nl> + return - ENOMEM ; <nl> priv -> dev = dev ; <nl> priv -> ipu = ipu ; <nl> 
static int conf_choice ( struct menu * menu ) <nl> } <nl> if (! child ) <nl> continue ; <nl> - if ( line [ strlen ( line ) - 1 ] == '?') { <nl> + if ( line [ 0 ] && line [ strlen ( line ) - 1 ] == '?') { <nl> print_help ( child ); <nl> continue ; <nl> }
static const char * ext4_decode_error ( struct super_block * sb , int errno , <nl> errstr = " Out of memory "; <nl> break ; <nl> case - EROFS : <nl> - if (! sb || EXT4_SB ( sb )-> s_journal -> j_flags & JBD2_ABORT ) <nl> + if (! sb || ( EXT4_SB ( sb )-> s_journal && <nl> + EXT4_SB ( sb )-> s_journal -> j_flags & JBD2_ABORT )) <nl> errstr = " Journal has aborted "; <nl> else <nl> errstr = " Readonly filesystem ";
int free_stream_context ( struct intel_sst_drv * ctx , unsigned int str_id ) <nl> if ( ret ) <nl> sst_clean_stream (& ctx -> streams [ str_id ]); <nl> return ret ; <nl> + } else { <nl> + dev_err ( ctx -> dev , " we tried to free stream context % d which was freed !!!\ n ", str_id ); <nl> } <nl> return ret ; <nl> }
int genl_register_family ( struct genl_family * family ) <nl> start , end + 1 , GFP_KERNEL ); <nl> if ( family -> id < 0 ) { <nl> err = family -> id ; <nl> - goto errout_locked ; <nl> + goto errout_free ; <nl> } <nl>  <nl> err = genl_validate_assign_mc_groups ( family ); <nl> int genl_register_family ( struct genl_family * family ) <nl>  <nl> errout_remove : <nl> idr_remove (& genl_fam_idr , family -> id ); <nl> + errout_free : <nl> kfree ( family -> attrbuf ); <nl> errout_locked : <nl> genl_unlock_all ();
int mwifiex_bss_start ( struct mwifiex_private * priv , struct cfg80211_bss * bss , <nl> goto done ; <nl> } <nl>  <nl> - if ( priv -> bss_mode == NL80211_IFTYPE_STATION ) { <nl> + if ( priv -> bss_mode == NL80211_IFTYPE_STATION || <nl> + priv -> bss_mode == NL80211_IFTYPE_P2P_CLIENT ) { <nl> u8 config_bands ; <nl>  <nl> - /* Infra mode */ <nl> ret = mwifiex_deauthenticate ( priv , NULL ); <nl> if ( ret ) <nl> goto done ;
static int elo_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl>  <nl> return 0 ; <nl> err_free : <nl> + usb_put_dev ( udev ); <nl> kfree ( priv ); <nl> return ret ; <nl> }
static int db8500_prcmu_probe ( struct platform_device * pdev ) <nl> } <nl> tcdm_base = devm_ioremap (& pdev -> dev , res -> start , <nl> resource_size ( res )); <nl> + if (! tcdm_base ) { <nl> + dev_err (& pdev -> dev , <nl> + " failed to ioremap prcmu - tcdm register memory \ n "); <nl> + return - ENOENT ; <nl> + } <nl>  <nl> /* Clean up the mailbox interrupts after pre - kernel code . */ <nl> writel ( ALL_MBOX_BITS , PRCM_ARM_IT1_CLR );
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . fuse = gm107_fuse_new , <nl> . gpio = gk104_gpio_new , <nl> + . i2c = gm200_i2c_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> . mmu = gf100_mmu_new ,
sg_start_req ( Sg_request * srp , unsigned char * cmd ) <nl> md -> from_user = 0 ; <nl> } <nl>  <nl> + if ( unlikely ( iov_count > MAX_UIOVEC )) <nl> + return - EINVAL ; <nl> + <nl> if ( iov_count ) { <nl> int size = sizeof ( struct iovec ) * iov_count ; <nl> struct iovec * iov ;
static int vidioc_try_fmt_vid_cap ( struct file * file , void * priv , <nl> else <nl> f -> fmt . pix . field = dev -> interlaced ? <nl> V4L2_FIELD_INTERLACED : V4L2_FIELD_TOP ; <nl> + f -> fmt . pix . priv = 0 ; <nl>  <nl> return 0 ; <nl> }
static int destroy_queue_nocpsch ( struct device_queue_manager * dqm , <nl> } <nl> dqm -> sdma_queue_count --; <nl> deallocate_sdma_queue ( dqm , q -> sdma_id ); <nl> + } else { <nl> + pr_debug (" q -> properties . type is invalid (% d )\ n ", <nl> + q -> properties . type ); <nl> + retval = - EINVAL ; <nl> + goto out ; <nl> } <nl>  <nl> retval = mqd -> destroy_mqd ( mqd , q -> mqd ,
static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> goto cleanup ; <nl> } <nl>  <nl> - if ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr ))) { <nl> + if (( fibsize < ( sizeof ( struct user_aac_srb ) - sizeof ( struct user_sgentry ))) || <nl> + ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr )))) { <nl> rcode = - EINVAL ; <nl> goto cleanup ; <nl> }
int ath6kl_debug_init_fs ( struct ath6kl * ar ) <nl> void ath6kl_debug_cleanup ( struct ath6kl * ar ) <nl> { <nl> skb_queue_purge (& ar -> debug . fwlog_queue ); <nl> + complete (& ar -> debug . fwlog_completion ); <nl> kfree ( ar -> debug . roam_tbl ); <nl> } <nl> 
int sock_diag_register ( struct sock_diag_handler * hndl ) <nl> { <nl> int err = 0 ; <nl>  <nl> - if ( hndl -> family > AF_MAX ) <nl> + if ( hndl -> family >= AF_MAX ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& sock_diag_table_mutex ); <nl> void sock_diag_unregister ( struct sock_diag_handler * hnld ) <nl> { <nl> int family = hnld -> family ; <nl>  <nl> - if ( family > AF_MAX ) <nl> + if ( family >= AF_MAX ) <nl> return ; <nl>  <nl> mutex_lock (& sock_diag_table_mutex );
int add_mtd_partitions ( struct mtd_info * master , <nl>  <nl> for ( i = 0 ; i < nbparts ; i ++) { <nl> slave = allocate_partition ( master , parts + i , i , cur_offset ); <nl> - if ( IS_ERR ( slave )) <nl> + if ( IS_ERR ( slave )) { <nl> + del_mtd_partitions ( master ); <nl> return PTR_ERR ( slave ); <nl> + } <nl>  <nl> mutex_lock (& mtd_partitions_mutex ); <nl> list_add (& slave -> list , & mtd_partitions );
static void audit_log_feature_change ( int which , u32 old_feature , u32 new_feature <nl> { <nl> struct audit_buffer * ab ; <nl>  <nl> + if ( audit_enabled == AUDIT_OFF ) <nl> + return ; <nl> + <nl> ab = audit_log_start ( NULL , GFP_KERNEL , AUDIT_FEATURE_CHANGE ); <nl> audit_log_format ( ab , " feature =% s old =% d new =% d old_lock =% d new_lock =% d res =% d ", <nl> audit_feature_names [ which ], !! old_feature , !! new_feature ,
static int bcm7038_wdt_probe ( struct platform_device * pdev ) <nl> wdt -> clk = devm_clk_get ( dev , NULL ); <nl> /* If unable to get clock , use default frequency */ <nl> if (! IS_ERR ( wdt -> clk )) { <nl> - clk_prepare_enable ( wdt -> clk ); <nl> + err = clk_prepare_enable ( wdt -> clk ); <nl> + if ( err ) <nl> + return err ; <nl> wdt -> rate = clk_get_rate ( wdt -> clk ); <nl> /* Prevent divide - by - zero exception */ <nl> if (! wdt -> rate )
static int omap_pcm_new ( struct snd_soc_pcm_runtime * rtd ) <nl> } <nl>  <nl> out : <nl> + /* free preallocated buffers in case of error */ <nl> + if ( ret ) <nl> + omap_pcm_free_dma_buffers ( pcm ); <nl> + <nl> return ret ; <nl> } <nl> 
static struct xfrm_state * pfkey_msg2xfrm_state ( struct net * net , <nl> ext_hdrs [ SADB_X_EXT_NAT_T_DPORT - 1 ]; <nl> natt -> encap_dport = n_port -> sadb_x_nat_t_port_port ; <nl> } <nl> + memset (& natt -> encap_oa , 0 , sizeof ( natt -> encap_oa )); <nl> } <nl>  <nl> err = xfrm_init_state ( x );
static void btc8821a2ant_run_coexist_mechanism ( struct btc_coexist * btcoexist ) <nl> return ; <nl> } <nl>  <nl> + if ( coex_sta -> under_ips ) { <nl> + RT_TRACE ( rtlpriv , COMP_BT_COEXIST , DBG_LOUD , <nl> + "[ BTCoex ], wifi is under IPS !!!\ n "); <nl> + return ; <nl> + } <nl> + <nl> algorithm = btc8821a2ant_action_algorithm ( btcoexist ); <nl> if ( coex_sta -> c2h_bt_inquiry_page && <nl> ( BT_8821A_2ANT_COEX_ALGO_PANHS != algorithm )) {
static void __ieee80211_scan_completed ( struct ieee80211_hw * hw , bool aborted , <nl> if ( local -> scan_req != local -> int_scan_req ) <nl> cfg80211_scan_done ( local -> scan_req , aborted ); <nl> local -> scan_req = NULL ; <nl> - local -> scan_sdata = NULL ; <nl> + rcu_assign_pointer ( local -> scan_sdata , NULL ); <nl>  <nl> local -> scanning = 0 ; <nl> local -> scan_channel = NULL ;
static int __init macide_init ( void ) <nl> int irq ; <nl> hw_regs_t hw ; <nl>  <nl> + if (! MACH_IS_MAC ) <nl> + return - ENODEV ; <nl> + <nl> switch ( macintosh_config -> ide_type ) { <nl> case MAC_IDE_QUADRA : <nl> base = IDE_BASE ;
static int crypt_iv_tcw_whitening ( struct crypt_config * cc , <nl> for ( i = 0 ; i < (( 1 << SECTOR_SHIFT ) / 8 ); i ++) <nl> crypto_xor ( data + i * 8 , buf , 8 ); <nl> out : <nl> - memset ( buf , 0 , sizeof ( buf )); <nl> + memzero_explicit ( buf , sizeof ( buf )); <nl> return r ; <nl> } <nl> 
void psb_intel_crtc_init ( struct drm_device * dev , int pipe , <nl> ( struct drm_connector **) ( psb_intel_crtc + 1 ); <nl> psb_intel_crtc -> mode_set . num_connectors = 0 ; <nl> psb_intel_cursor_init ( dev , psb_intel_crtc ); <nl> + <nl> + /* Set to true so that the pipe is forced off on initial config . */ <nl> + psb_intel_crtc -> active = true ; <nl> } <nl>  <nl> int psb_intel_get_pipe_from_crtc_id ( struct drm_device * dev , void * data ,
static void oz_add_farewell ( struct oz_pd * pd , u8 ep_num , u8 index , <nl> return ; <nl> f -> ep_num = ep_num ; <nl> f -> index = index ; <nl> + f -> len = len ; <nl> memcpy ( f -> report , report , len ); <nl> oz_dbg ( ON , " RX : Adding farewell report \ n "); <nl> spin_lock (& g_polling_lock );
asmlinkage int sys_rt_sigreturn ( struct pt_regs * regs ) <nl> if ( restore_sigcontext ( regs , & frame -> uc . uc_mcontext )) <nl> goto badframe ; <nl>  <nl> + if ( do_sigaltstack (& frame -> uc . uc_stack , NULL , regs -> sp ) == - EFAULT ) <nl> + goto badframe ; <nl> + <nl> pr_debug (" Context restored : pc = % 08lx , lr = % 08lx , sp = % 08lx \ n ", <nl> regs -> pc , regs -> lr , regs -> sp ); <nl> 
__setup (" mce =", mcheck_enable ); <nl> static int mce_resume ( struct sys_device * dev ) <nl> { <nl> mce_init ( NULL ); <nl> + mce_cpu_features (& current_cpu_data ); <nl> return 0 ; <nl> } <nl> 
static long ioctl_file_dedupe_range ( struct file * file , void __user * arg ) <nl> goto out ; <nl> } <nl>  <nl> + same -> dest_count = count ; <nl> ret = vfs_dedupe_file_range ( file , same ); <nl> if ( ret ) <nl> goto out ;
long join_session_keyring ( const char * name ) <nl> ret = PTR_ERR ( keyring ); <nl> goto error2 ; <nl> } else if ( keyring == new -> session_keyring ) { <nl> + key_put ( keyring ); <nl> ret = 0 ; <nl> goto error2 ; <nl> }
static int intel_backlight_device_update_status ( struct backlight_device * bd ) <nl> */ <nl> if ( panel -> backlight . enabled ) { <nl> if ( panel -> backlight_power ) { <nl> - bool enable = bd -> props . power == FB_BLANK_UNBLANK ; <nl> + bool enable = bd -> props . power == FB_BLANK_UNBLANK && <nl> + bd -> props . brightness != 0 ; <nl> panel -> backlight_power ( connector , enable ); <nl> } <nl> } else {
int main ( int argc , char ** argv ) <nl> read_relocs ( fp ); <nl> if ( show_absolute_syms ) { <nl> print_absolute_symbols (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> if ( show_absolute_relocs ) { <nl> print_absolute_relocs (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> emit_relocs ( as_text , use_real_mode ); <nl> + out : <nl> + fclose ( fp ); <nl> return 0 ; <nl> }
store_priv_session_ ## field ( struct device * dev , \ <nl> # define iscsi_priv_session_rw_attr ( field , format ) \ <nl> iscsi_priv_session_attr_show ( field , format ) \ <nl> iscsi_priv_session_attr_store ( field ) \ <nl> - static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUGO , \ <nl> + static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUSR , \ <nl> show_priv_session_ ## field , \ <nl> store_priv_session_ ## field ) <nl> iscsi_priv_session_rw_attr ( recovery_tmo , "% d ");
void drm_helper_resume_force_mode ( struct drm_device * dev ) <nl> int encoder_dpms ; <nl> bool ret ; <nl>  <nl> + drm_modeset_lock_all ( dev ); <nl> list_for_each_entry ( crtc , & dev -> mode_config . crtc_list , head ) { <nl>  <nl> if (! crtc -> enabled ) <nl> void drm_helper_resume_force_mode ( struct drm_device * dev ) <nl>  <nl> /* disable the unused connectors while restoring the modesetting */ <nl> __drm_helper_disable_unused_functions ( dev ); <nl> + drm_modeset_unlock_all ( dev ); <nl> } <nl> EXPORT_SYMBOL ( drm_helper_resume_force_mode ); <nl> 
static int asoc_simple_card_probe ( struct platform_device * pdev ) <nl> snd_soc_card_set_drvdata ( card , priv ); <nl>  <nl> ret = devm_snd_soc_register_card ( dev , card ); <nl> - if ( ret >= 0 ) <nl> - return ret ; <nl> + if ( ret < 0 ) <nl> + goto err ; <nl> + <nl> + return 0 ; <nl> err : <nl> asoc_simple_card_clean_reference ( card ); <nl> 
# define USB_REQ_LOOPBACK_DATA_READ 0x16 <nl> # define USB_REQ_SET_INTERFACE_DS 0x17 <nl>  <nl> +/* specific requests for USB Power Delivery */ <nl> +# define USB_REQ_GET_PARTNER_PDO 20 <nl> +# define USB_REQ_GET_BATTERY_STATUS 21 <nl> +# define USB_REQ_SET_PDO 22 <nl> +# define USB_REQ_GET_VDM 23 <nl> +# define USB_REQ_SEND_VDM 24 <nl> + <nl> /* The Link Power Management ( LPM ) ECN defines USB_REQ_TEST_AND_SET command , <nl> * used by hubs to put ports into a new L1 suspend state , except that it <nl> * forgot to define its number ...
static int hpsa_get_pdisk_of_ioaccel2 ( struct ctlr_info * h , <nl>  <nl> /* Get the list of physical devices */ <nl> physicals = kzalloc ( reportsize , GFP_KERNEL ); <nl> + if ( physicals == NULL ) <nl> + return 0 ; <nl> if ( hpsa_scsi_do_report_phys_luns ( h , ( struct ReportLUNdata *) physicals , <nl> reportsize , extended )) { <nl> dev_err (& h -> pdev -> dev ,
static int mem_cgroup_hierarchical_reclaim ( struct mem_cgroup * root_mem , <nl> ret = try_to_free_mem_cgroup_pages ( root_mem , gfp_mask , noswap ); <nl> if ( mem_cgroup_check_under_limit ( root_mem )) <nl> return 0 ; <nl> + if (! root_mem -> use_hierarchy ) <nl> + return ret ; <nl>  <nl> next_mem = mem_cgroup_get_first_node ( root_mem ); <nl> 
ipt_recent_checkentry ( const char * tablename , const void * ip , <nl> GFP_KERNEL ); <nl> if ( t == NULL ) <nl> goto out ; <nl> + t -> refcnt = 1 ; <nl> strcpy ( t -> name , info -> name ); <nl> INIT_LIST_HEAD (& t -> lru_list ); <nl> for ( i = 0 ; i < ip_list_hash_size ; i ++)
xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
static void gb_svc_remove_modules ( struct gb_svc * svc ) <nl>  <nl> void gb_svc_del ( struct gb_svc * svc ) <nl> { <nl> - gb_connection_disable ( svc -> connection ); <nl> + gb_connection_disable_rx ( svc -> connection ); <nl>  <nl> /* <nl> * The SVC device and input device may have been registered <nl> void gb_svc_del ( struct gb_svc * svc ) <nl> flush_workqueue ( svc -> wq ); <nl>  <nl> gb_svc_remove_modules ( svc ); <nl> + <nl> + gb_connection_disable ( svc -> connection ); <nl> } <nl>  <nl> void gb_svc_put ( struct gb_svc * svc )
static int pxad_probe ( struct platform_device * op ) <nl> pdev -> slave . dst_addr_widths = widths ; <nl> pdev -> slave . directions = BIT ( DMA_MEM_TO_DEV ) | BIT ( DMA_DEV_TO_MEM ); <nl> pdev -> slave . residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR ; <nl> + pdev -> slave . descriptor_reuse = true ; <nl>  <nl> pdev -> slave . dev = & op -> dev ; <nl> ret = pxad_init_dmadev ( op , pdev , dma_channels );
static int mmc_ext_csd_open ( struct inode * inode , struct file * filp ) <nl> if ( err ) <nl> goto out_free ; <nl>  <nl> - for ( i = 511 ; i >= 0 ; i --) <nl> + for ( i = 0 ; i < 512 ; i ++) <nl> n += sprintf ( buf + n , "% 02x ", ext_csd [ i ]); <nl> n += sprintf ( buf + n , "\ n "); <nl> BUG_ON ( n != EXT_CSD_STR_LEN );
static int tc6393xb_resume ( struct platform_device * dev ) <nl> int ret ; <nl> int i ; <nl>  <nl> - clk_prepare_enable ( tc6393xb -> clk ); <nl> + ret = clk_prepare_enable ( tc6393xb -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = tcpd -> resume ( dev ); <nl> if ( ret )
static s32 ixgbe_reset_hw_X550em ( struct ixgbe_hw * hw ) <nl> hw -> phy . sfp_setup_needed = false ; <nl> } <nl>  <nl> + if ( status == IXGBE_ERR_SFP_NOT_SUPPORTED ) <nl> + return status ; <nl> + <nl> /* Reset PHY */ <nl> if (! hw -> phy . reset_disable && hw -> phy . ops . reset ) <nl> hw -> phy . ops . reset ( hw );
static int ci_get_platdata ( struct device * dev , <nl> return ret ; <nl> } <nl>  <nl> + if ( of_find_property ( dev -> of_node , " non - zero - ttctrl - ttha ", NULL )) <nl> + platdata -> flags |= CI_HDRC_SET_NON_ZERO_TTHA ; <nl> + <nl> ext_id = ERR_PTR (- ENODEV ); <nl> ext_vbus = ERR_PTR (- ENODEV ); <nl> if ( of_property_read_bool ( dev -> of_node , " extcon ")) {
static int cdrom_ioctl_media_changed ( struct cdrom_device_info * cdi , <nl> if (! CDROM_CAN ( CDC_SELECT_DISC ) || arg == CDSL_CURRENT ) <nl> return media_changed ( cdi , 1 ); <nl>  <nl> - if (( unsigned int ) arg >= cdi -> capacity ) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl>  <nl> info = kmalloc ( sizeof (* info ), GFP_KERNEL );
static int cfs_wi_scheduler ( void * arg ) <nl>  <nl> spin_unlock (& sched -> ws_lock ); <nl> rc = wait_event_interruptible_exclusive ( sched -> ws_waitq , <nl> - ! cfs_wi_sched_cansleep ( sched )); <nl> + ! cfs_wi_sched_cansleep ( sched )); <nl> spin_lock (& sched -> ws_lock ); <nl> } <nl> 
static int alloc_reserved_tree_block ( struct btrfs_trans_handle * trans , <nl> ret = btrfs_insert_empty_item ( trans , fs_info -> extent_root , path , <nl> ins , size ); <nl> if ( ret ) { <nl> + btrfs_free_path ( path ); <nl> btrfs_free_and_pin_reserved_extent ( root , ins -> objectid , <nl> root -> nodesize ); <nl> - btrfs_free_path ( path ); <nl> return ret ; <nl> } <nl> 
void drm_mode_config_reset ( struct drm_device * dev ) <nl> if ( encoder -> funcs -> reset ) <nl> encoder -> funcs -> reset ( encoder ); <nl>  <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> drm_for_each_connector ( connector , dev ) { <nl> connector -> status = connector_status_unknown ; <nl>  <nl> if ( connector -> funcs -> reset ) <nl> connector -> funcs -> reset ( connector ); <nl> } <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl> } <nl> EXPORT_SYMBOL ( drm_mode_config_reset ); <nl> 
static void mtk_plane_atomic_update ( struct drm_plane * plane , <nl> pitch = fb -> pitches [ 0 ]; <nl> format = fb -> pixel_format ; <nl>  <nl> - addr += ( plane -> state -> src . x1 >> 16 ) * 4 ; <nl> + addr += ( plane -> state -> src . x1 >> 16 ) * drm_format_plane_cpp ( format , 0 ); <nl> addr += ( plane -> state -> src . y1 >> 16 ) * pitch ; <nl>  <nl> state -> pending . enable = true ;
batadv_frag_merge_packets ( struct hlist_head * chain , struct sk_buff * skb ) <nl> kfree ( entry ); <nl>  <nl> /* Make room for the rest of the fragments . */ <nl> - if ( pskb_expand_head ( skb_out , 0 , size - skb -> len , GFP_ATOMIC ) < 0 ) { <nl> + if ( pskb_expand_head ( skb_out , 0 , size - skb_out -> len , GFP_ATOMIC ) < 0 ) { <nl> kfree_skb ( skb_out ); <nl> skb_out = NULL ; <nl> goto free ;
static int __devinit bq20z75_probe ( struct i2c_client * client , <nl>  <nl> INIT_DELAYED_WORK (& bq20z75_device -> work , bq20z75_delayed_work ); <nl>  <nl> + bq20z75_device -> enable_detection = true ; <nl> + <nl> return 0 ; <nl>  <nl> exit_psupply :
nvkm_pmu_reset ( struct nvkm_pmu * pmu ) <nl> ); <nl>  <nl> /* Reset . */ <nl> - pmu -> func -> reset ( pmu ); <nl> + if ( pmu -> func -> reset ) <nl> + pmu -> func -> reset ( pmu ); <nl>  <nl> /* Wait for IMEM / DMEM scrubbing to be complete . */ <nl> nvkm_msec ( device , 2000 ,
static int of_platform_serial_setup ( struct platform_device * ofdev , <nl> port -> line = ret ; <nl>  <nl> port -> irq = irq_of_parse_and_map ( np , 0 ); <nl> + if (! port -> irq ) { <nl> + ret = - EPROBE_DEFER ; <nl> + goto err_unprepare ; <nl> + } <nl>  <nl> info -> rst = devm_reset_control_get_optional_shared (& ofdev -> dev , NULL ); <nl> if ( IS_ERR ( info -> rst )) {
static int change_memory_common ( unsigned long addr , int numpages , <nl> if (! size ) <nl> return 0 ; <nl>  <nl> - if (! in_range ( start , size , MODULES_VADDR , MODULES_END )) <nl> + if (! in_range ( start , size , MODULES_VADDR , MODULES_END ) && <nl> + ! in_range ( start , size , VMALLOC_START , VMALLOC_END )) <nl> return - EINVAL ; <nl>  <nl> data . set_mask = set_mask ;
static ssize_t iwl_dbgfs_sram_read ( struct file * file , <nl> const struct fw_img * img ; <nl> size_t bufsz ; <nl>  <nl> + if (! iwl_is_ready_rf ( priv )) <nl> + return - EAGAIN ; <nl> + <nl> /* default is to dump the entire data segment */ <nl> if (! priv -> dbgfs_sram_offset && ! priv -> dbgfs_sram_len ) { <nl> priv -> dbgfs_sram_offset = 0x800000 ;
static int dgnc_found_board ( struct pci_dev * pdev , int id ) <nl> return - ENOMEM ; <nl>  <nl> /* make a temporary message buffer for the boot messages */ <nl> - brd -> msgbuf_head = kzalloc ( sizeof ( u8 ) * 8192 , GFP_KERNEL ); <nl> + brd -> msgbuf_head = kcalloc ( 8192 , sizeof ( u8 ), GFP_KERNEL ); <nl> brd -> msgbuf = brd -> msgbuf_head ; <nl>  <nl> if (! brd -> msgbuf ) {
int wpa_ioctl ( PSDevice pDevice , struct iw_point * p ) <nl> default : <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO " wpa_ioctl : unknown cmd =% d \ n ", <nl> param -> cmd ); <nl> + kfree ( param ); <nl> return - EOPNOTSUPP ; <nl> } <nl> 
int iwl_enqueue_hcmd ( struct iwl_priv * priv , struct iwl_host_cmd * cmd ) <nl> return - EIO ; <nl> } <nl>  <nl> + if (( priv -> ucode_owner == IWL_OWNERSHIP_TM ) && <nl> + !( cmd -> flags & CMD_ON_DEMAND )) { <nl> + IWL_DEBUG_HC ( priv , " tm own the uCode , no regular hcmd send \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> copy_size = sizeof ( out_cmd -> hdr ); <nl> cmd_size = sizeof ( out_cmd -> hdr ); <nl> 
static loff_t mtd_lseek ( struct file * file , loff_t offset , int orig ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( offset >= 0 && offset < mtd -> size ) <nl> + if ( offset >= 0 && offset <= mtd -> size ) <nl> return file -> f_pos = offset ; <nl>  <nl> return - EINVAL ;
static int snd_compress_check_input ( struct snd_compr_params * params ) <nl> { <nl> /* first let ' s check the buffer parameter ' s */ <nl> if ( params -> buffer . fragment_size == 0 || <nl> - params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + params -> buffer . fragments > INT_MAX / params -> buffer . fragment_size ) <nl> return - EINVAL ; <nl>  <nl> /* now codec parameters */
static int mga_vram_init ( struct mga_device * mdev ) <nl> aper -> count = 1 ; <nl>  <nl> remove_conflicting_framebuffers ( aper , " mgafb ", true ); <nl> + kfree ( aper ); <nl>  <nl> if (! request_mem_region ( mdev -> mc . vram_base , mdev -> mc . vram_window , <nl> " mgadrmfb_vram ")) {
static const struct omap_video_timings tpo_td043_timings = { <nl> static int tpo_td043_power_on ( struct tpo_td043_device * tpo_td043 ) <nl> { <nl> int nreset_gpio = tpo_td043 -> nreset_gpio ; <nl> + int r ; <nl>  <nl> if ( tpo_td043 -> powered_on ) <nl> return 0 ; <nl>  <nl> - regulator_enable ( tpo_td043 -> vcc_reg ); <nl> + r = regulator_enable ( tpo_td043 -> vcc_reg ); <nl> + if ( r != 0 ) <nl> + return r ; <nl>  <nl> /* wait for regulator to stabilize */ <nl> msleep ( 160 );
ath5k_set_key ( struct ieee80211_hw * hw , enum set_key_cmd cmd , <nl> if ( modparam_nohwcrypt ) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( sc -> opmode == NL80211_IFTYPE_AP ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> switch ( key -> alg ) { <nl> case ALG_WEP : <nl> case ALG_TKIP :
static int regcache_default_sync ( struct regmap * map , unsigned int min , <nl> unsigned int val ; <nl> int ret ; <nl>  <nl> - if ( regmap_volatile ( map , reg )) <nl> + if ( regmap_volatile ( map , reg ) || <nl> + ! regmap_writeable ( map , reg )) <nl> continue ; <nl>  <nl> ret = regcache_read ( map , reg , & val );
int expand_downwards ( struct vm_area_struct * vma , <nl> { <nl> struct mm_struct * mm = vma -> vm_mm ; <nl> struct vm_area_struct * prev ; <nl> - int error ; <nl> + int error = 0 ; <nl>  <nl> address &= PAGE_MASK ; <nl> - error = security_mmap_addr ( address ); <nl> - if ( error ) <nl> - return error ; <nl> + if ( address < mmap_min_addr ) <nl> + return - EPERM ; <nl>  <nl> /* Enforce stack_guard_gap */ <nl> prev = vma -> vm_prev ;
void radeon_atom_backlight_init ( struct radeon_encoder * radeon_encoder , <nl> u8 backlight_level ; <nl> char bl_name [ 16 ]; <nl>  <nl> + /* Mac laptops with multiple GPUs use the gmux driver for backlight <nl> + * so don ' t register a backlight device <nl> + */ <nl> + if (( rdev -> pdev -> subsystem_vendor == PCI_VENDOR_ID_APPLE ) && <nl> + ( rdev -> pdev -> device == 0x6741 )) <nl> + return ; <nl> + <nl> if (! radeon_encoder -> enc_priv ) <nl> return ; <nl> 
static bool event_compare ( struct fsnotify_event * old , struct fsnotify_event * new <nl> /* remember , after old was put on the wait_q we aren ' t <nl> * allowed to look at the inode any more , only thing <nl> * left to check was if the file_name is the same */ <nl> - if ( old -> name_len && <nl> + if (! old -> name_len || <nl> ! strcmp ( old -> file_name , new -> file_name )) <nl> return true ; <nl> break ;
void dasd_int_handler ( struct ccw_device * cdev , unsigned long intparm , <nl> if ( cqr -> status == DASD_CQR_CLEAR_PENDING && <nl> scsw_fctl (& irb -> scsw ) & SCSW_FCTL_CLEAR_FUNC ) { <nl> cqr -> status = DASD_CQR_CLEARED ; <nl> - if ( cqr -> callback_data == DASD_SLEEPON_START_TAG ) <nl> - cqr -> callback_data = DASD_SLEEPON_END_TAG ; <nl> dasd_device_clear_timer ( device ); <nl> wake_up (& dasd_flush_wq ); <nl> - wake_up (& generic_waitq ); <nl> dasd_schedule_device_bh ( device ); <nl> return ; <nl> }
static void bmc150_accel_unregister_triggers ( struct bmc150_accel_data * data , <nl> { <nl> int i ; <nl>  <nl> - for ( i = from ; i >= 0 ; i ++) { <nl> + for ( i = from ; i >= 0 ; i --) { <nl> if ( data -> triggers [ i ]. indio_trig ) { <nl> iio_trigger_unregister ( data -> triggers [ i ]. indio_trig ); <nl> data -> triggers [ i ]. indio_trig = NULL ;
int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
cifs_mount ( struct super_block * sb , struct cifs_sb_info * cifs_sb , <nl>  <nl> remote_path_check : <nl> /* check if a whole path ( including prepath ) is not remote */ <nl> - if (! rc && cifs_sb -> prepathlen && tcon ) { <nl> + if (! rc && tcon ) { <nl> /* build_path_to_root works only when we have a valid tcon */ <nl> full_path = cifs_build_path_to_root ( cifs_sb , tcon ); <nl> if ( full_path == NULL ) {
static struct snd_seq_queue * queue_new ( int owner , int locked ) <nl> static void queue_delete ( struct snd_seq_queue * q ) <nl> { <nl> /* stop and release the timer */ <nl> + mutex_lock (& q -> timer_mutex ); <nl> snd_seq_timer_stop ( q -> timer ); <nl> snd_seq_timer_close ( q ); <nl> + mutex_unlock (& q -> timer_mutex ); <nl> /* wait until access free */ <nl> snd_use_lock_sync (& q -> use_lock ); <nl> /* release resources ... */
void ping_unhash ( struct sock * sk ) <nl> if ( sk_hashed ( sk )) { <nl> write_lock_bh (& ping_table . lock ); <nl> hlist_nulls_del (& sk -> sk_nulls_node ); <nl> + sk_nulls_node_init (& sk -> sk_nulls_node ); <nl> sock_put ( sk ); <nl> isk -> inet_num = 0 ; <nl> isk -> inet_sport = 0 ;
int iwl_run_init_mvm_ucode ( struct iwl_mvm * mvm , bool read_nvm ) <nl> ret = iwl_nvm_check_version ( mvm -> nvm_data , mvm -> trans ); <nl> WARN_ON ( ret ); <nl>  <nl> + /* Send TX valid antennas before triggering calibrations */ <nl> + ret = iwl_send_tx_ant_cfg ( mvm , mvm -> nvm_data -> valid_tx_ant ); <nl> + if ( ret ) <nl> + goto error ; <nl> + <nl> /* Override the calibrations from TLV and the const of fw */ <nl> iwl_set_default_calib_trigger ( mvm ); <nl> 
event_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_preds ( call ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl>  <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> if ( pred -> clear ) { <nl> filter_free_subsystem_preds ( system ); <nl> + filter_free_pred ( pred ); <nl> return cnt ; <nl> } <nl> 
static int adis_update_scan_mode_burst ( struct iio_dev * indio_dev , <nl> return - ENOMEM ; <nl>  <nl> adis -> buffer = kzalloc ( burst_length + sizeof ( u16 ), GFP_KERNEL ); <nl> - if (! adis -> buffer ) <nl> + if (! adis -> buffer ) { <nl> + kfree ( adis -> xfer ); <nl> + adis -> xfer = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> tx = adis -> buffer + burst_length ; <nl> tx [ 0 ] = ADIS_READ_REG ( adis -> burst -> reg_cmd );
do_ip_vs_get_ctl ( struct sock * sk , int cmd , void __user * user , int * len ) <nl> { <nl> struct ip_vs_timeout_user t ; <nl>  <nl> + memset (& t , 0 , sizeof ( t )); <nl> __ip_vs_get_timeouts ( net , & t ); <nl> if ( copy_to_user ( user , & t , sizeof ( t )) != 0 ) <nl> ret = - EFAULT ;
static const struct mssr_mod_clk r8a7795_mod_clks [] __initconst = { <nl> DEF_MOD (" scif2 ", 310 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" pcie1 ", 318 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" pcie0 ", 319 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if1 ", 327 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if0 ", 328 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" intc - ap ", 408 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" audmac0 ", 502 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" audmac1 ", 501 , R8A7795_CLK_S3D4 ),
static enum print_line_t trace_ctxwake_bin ( struct trace_iterator * iter , <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_pid ); <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_prio ); <nl> SEQ_PUT_FIELD_RET ( s , field -> prev_state ); <nl> + SEQ_PUT_FIELD_RET ( s , field -> next_cpu ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_pid ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_prio ); <nl> SEQ_PUT_FIELD_RET ( s , field -> next_state );
__alloc_pages_slowpath ( gfp_t gfp_mask , unsigned int order , <nl> if ( p -> flags & PF_MEMALLOC ) <nl> goto nopage ; <nl>  <nl> + /* Avoid allocations with no watermarks from looping endlessly */ <nl> + if ( test_thread_flag ( TIF_MEMDIE ) && !( gfp_mask & __GFP_NOFAIL )) <nl> + goto nopage ; <nl> + <nl> /* Try direct reclaim and then allocating */ <nl> page = __alloc_pages_direct_reclaim ( gfp_mask , order , <nl> zonelist , high_zoneidx ,
struct gpio_desc * devm_get_gpiod_from_child ( struct device * dev , <nl> suffixes [ i ]); <nl>  <nl> desc = fwnode_get_named_gpiod ( child , prop_name ); <nl> - if (! IS_ERR ( desc ) || ( PTR_ERR ( desc ) == - EPROBE_DEFER )) <nl> + if (! IS_ERR ( desc ) || ( PTR_ERR ( desc ) != - ENOENT )) <nl> break ; <nl> } <nl> if ( IS_ERR ( desc )) {
int bnx2i_send_iscsi_nopout ( struct bnx2i_conn * bnx2i_conn , <nl> bnx2i_cmd = ( struct bnx2i_cmd *) task -> dd_data ; <nl> nopout_hdr = ( struct iscsi_nopout *) task -> hdr ; <nl> nopout_wqe = ( struct bnx2i_nop_out_request *) ep -> qp . sq_prod_qe ; <nl> + <nl> + memset ( nopout_wqe , 0x00 , sizeof ( struct bnx2i_nop_out_request )); <nl> + <nl> nopout_wqe -> op_code = nopout_hdr -> opcode ; <nl> nopout_wqe -> op_attr = ISCSI_FLAG_CMD_FINAL ; <nl> memcpy ( nopout_wqe -> lun , nopout_hdr -> lun , 8 );
static void ilk_pipe_wm_get_hw_state ( struct drm_crtc * crtc ) <nl> if ( IS_HASWELL ( dev ) || IS_BROADWELL ( dev )) <nl> hw -> wm_linetime [ pipe ] = I915_READ ( PIPE_WM_LINETIME ( pipe )); <nl>  <nl> + memset ( active , 0 , sizeof (* active )); <nl> + <nl> active -> pipe_enabled = intel_crtc -> active ; <nl>  <nl> if ( active -> pipe_enabled ) {
static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> ch . band = BRCMU_CHAN_BAND_2G ; <nl> ch . bw = BRCMU_CHAN_BW_40 ; <nl> + ch . sb = BRCMU_CHAN_SB_NONE ; <nl> ch . chnum = 0 ; <nl> cfg -> d11inf . encchspec (& ch ); <nl>  <nl> static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> brcmf_update_bw40_channel_flag (& band -> channels [ j ], & ch ); <nl> } <nl> + kfree ( pbuf ); <nl> } <nl> return err ; <nl> }
static int prepare_vmcs02 ( struct kvm_vcpu * vcpu , struct vmcs12 * vmcs12 , <nl> if ( exec_control & CPU_BASED_TPR_SHADOW ) { <nl> vmcs_write64 ( VIRTUAL_APIC_PAGE_ADDR , - 1ull ); <nl> vmcs_write32 ( TPR_THRESHOLD , vmcs12 -> tpr_threshold ); <nl> + } else { <nl> +# ifdef CONFIG_X86_64 <nl> + exec_control |= CPU_BASED_CR8_LOAD_EXITING | <nl> + CPU_BASED_CR8_STORE_EXITING ; <nl> +# endif <nl> } <nl>  <nl> /*
nlmsvc_lock ( struct svc_rqst * rqstp , struct nlm_file * file , <nl> goto out ; <nl> case - EAGAIN : <nl> ret = nlm_lck_denied ; <nl> - goto out ; <nl> + break ; <nl> case FILE_LOCK_DEFERRED : <nl> if ( wait ) <nl> break ; <nl> nlmsvc_lock ( struct svc_rqst * rqstp , struct nlm_file * file , <nl> goto out ; <nl> } <nl>  <nl> + ret = nlm_lck_denied ; <nl> + if (! wait ) <nl> + goto out ; <nl> + <nl> ret = nlm_lck_blocked ; <nl>  <nl> /* Append to list of blocked */
static int ip_frag_reasm ( struct ipq * qp , struct sk_buff * prev , <nl> skb_morph ( head , qp -> q . fragments ); <nl> head -> next = qp -> q . fragments -> next ; <nl>  <nl> - kfree_skb ( qp -> q . fragments ); <nl> + consume_skb ( qp -> q . fragments ); <nl> qp -> q . fragments = head ; <nl> } <nl> 
static int fsl_ifc_chip_init ( struct fsl_ifc_mtd * priv ) <nl> chip -> ecc . algo = NAND_ECC_HAMMING ; <nl> } <nl>  <nl> - if ( ctrl -> version == FSL_IFC_VERSION_1_1_0 ) <nl> + if ( ctrl -> version >= FSL_IFC_VERSION_1_1_0 ) <nl> fsl_ifc_sram_init ( priv ); <nl>  <nl> return 0 ;
static int dell_wmi_events_set_enabled ( bool enable ) <nl> int ret ; <nl>  <nl> buffer = kzalloc ( sizeof ( struct calling_interface_buffer ), GFP_KERNEL ); <nl> + if (! buffer ) <nl> + return - ENOMEM ; <nl> buffer -> cmd_class = CLASS_INFO ; <nl> buffer -> cmd_select = SELECT_APP_REGISTRATION ; <nl> buffer -> input [ 0 ] = 0x10000 ;
void radeon_compute_pll_avivo ( struct radeon_pll * pll , <nl>  <nl> /* avoid high jitter with small fractional dividers */ <nl> if ( pll -> flags & RADEON_PLL_USE_FRAC_FB_DIV && ( fb_div % 10 )) { <nl> - fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 60 ); <nl> + fb_div_min = max ( fb_div_min , ( 9 - ( fb_div % 10 )) * 20 + 50 ); <nl> if ( fb_div < fb_div_min ) { <nl> unsigned tmp = DIV_ROUND_UP ( fb_div_min , fb_div ); <nl> fb_div *= tmp ;
static noinline void key_gc_unused_keys ( struct list_head * keys ) <nl> if ( test_bit ( KEY_FLAG_INSTANTIATED , & key -> flags )) <nl> atomic_dec (& key -> user -> nikeys ); <nl>  <nl> - key_user_put ( key -> user ); <nl> - <nl> /* now throw away the key memory */ <nl> if ( key -> type -> destroy ) <nl> key -> type -> destroy ( key ); <nl>  <nl> + key_user_put ( key -> user ); <nl> + <nl> kfree ( key -> description ); <nl>  <nl> # ifdef KEY_DEBUGGING
static struct snd_soc_dai_link da8xx_evm_dai = { <nl> . stream_name = " AIC3X ", <nl> . cpu_dai_name = " davinci - mcasp . 0 ", <nl> . codec_dai_name = " tlv320aic3x - hifi ", <nl> - . codec_name = " tlv320aic3x - codec . 0 - 001a ", <nl> + . codec_name = " tlv320aic3x - codec . 1 - 0018 ", <nl> . platform_name = " davinci - pcm - audio ", <nl> . init = evm_aic3x_init , <nl> . ops = & evm_ops ,
static int sh_pfc_dt_node_to_map ( struct pinctrl_dev * pctldev , <nl> for_each_child_of_node ( np , child ) { <nl> ret = sh_pfc_dt_subnode_to_map ( pctldev , child , map , num_maps , <nl> & index ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + of_node_put ( child ); <nl> goto done ; <nl> + } <nl> } <nl>  <nl> /* If no mapping has been found in child nodes try the config node . */
mlxsw_sp_lpm_tree_get ( struct mlxsw_sp * mlxsw_sp , <nl>  <nl> for ( i = 0 ; i < MLXSW_SP_LPM_TREE_COUNT ; i ++) { <nl> lpm_tree = & mlxsw_sp -> router . lpm_trees [ i ]; <nl> - if ( lpm_tree -> proto == proto && <nl> + if ( lpm_tree -> ref_count != 0 && <nl> + lpm_tree -> proto == proto && <nl> mlxsw_sp_prefix_usage_eq (& lpm_tree -> prefix_usage , <nl> prefix_usage )) <nl> goto inc_ref_count ;
void blk_mq_wake_waiters ( struct request_queue * q ) <nl> queue_for_each_hw_ctx ( q , hctx , i ) <nl> if ( blk_mq_hw_queue_mapped ( hctx )) <nl> blk_mq_tag_wakeup_all ( hctx -> tags , true ); <nl> + <nl> + /* <nl> + * If we are called because the queue has now been marked as <nl> + * dying , we need to ensure that processes currently waiting on <nl> + * the queue are notified as well . <nl> + */ <nl> + wake_up_all (& q -> mq_freeze_wq ); <nl> } <nl>  <nl> bool blk_mq_can_queue ( struct blk_mq_hw_ctx * hctx )
static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
static inline u64 btrfs_ino ( struct inode * inode ) <nl> { <nl> u64 ino = BTRFS_I ( inode )-> location . objectid ; <nl>  <nl> - if ( ino <= BTRFS_FIRST_FREE_OBJECTID ) <nl> + /* <nl> + * ! ino : btree_inode <nl> + * type == BTRFS_ROOT_ITEM_KEY : subvol dir <nl> + */ <nl> + if (! ino || BTRFS_I ( inode )-> location . type == BTRFS_ROOT_ITEM_KEY ) <nl> ino = inode -> i_ino ; <nl> return ino ; <nl> }
ieee80211_deliver_skb ( struct ieee80211_rx_data * rx ) <nl> } <nl>  <nl> if ( xmit_skb ) { <nl> - /* send to wireless media */ <nl> + /* <nl> + * Send to wireless media and increase priority by 256 to <nl> + * keep the received priority instead of reclassifying <nl> + * the frame ( see cfg80211_classify8021d ). <nl> + */ <nl> + xmit_skb -> priority += 256 ; <nl> xmit_skb -> protocol = htons ( ETH_P_802_3 ); <nl> skb_reset_network_header ( xmit_skb ); <nl> skb_reset_mac_header ( xmit_skb );
void __init at91_gpio_irq_setup ( void ) <nl> irq_set_chip_data ( id , this ); <nl> irq_set_chained_handler ( id , gpio_irq_handler ); <nl> } <nl> - pr_info (" AT91 : % d gpio irqs in % d banks \ n ", irq , gpio_banks ); <nl> + pr_info (" AT91 : % d gpio irqs in % d banks \ n ", irq - gpio_to_irq ( 0 ), gpio_banks ); <nl> } <nl>  <nl> /* gpiolib support */
MODULE_LICENSE (" GPL v2 "); <nl>  <nl> static int debug ; <nl> module_param ( debug , int , 0644 ); <nl> - MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on \ n "); <nl> + MODULE_PARM_DESC ( debug , " debug level 0 = off ( default ) 1 = on "); <nl>  <nl> /* # define MPX_DEBUG */ <nl> 
static long bcm_char_ioctl ( struct file * filp , UINT cmd , ULONG arg ) <nl>  <nl> BCM_DEBUG_PRINT ( Adapter , DBG_TYPE_OTHERS , OSAL_DBG , DBG_LVL_ALL , " Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO \ n "); <nl>  <nl> + memset (& DevInfo , 0 , sizeof ( DevInfo )); <nl> DevInfo . MaxRDMBufferSize = BUFFER_4K ; <nl> DevInfo . u32DSDStartOffset = EEPROM_CALPARAM_START ; <nl> DevInfo . u32RxAlignmentCorrection = 0 ;
static void mmc_deselect_cards ( struct mmc_host * host ) <nl>  <nl> static inline void mmc_delay ( unsigned int ms ) <nl> { <nl> - if ( ms < HZ / 1000 ) { <nl> - yield (); <nl> + if ( ms < 1000 / HZ ) { <nl> + cond_resched (); <nl> mdelay ( ms ); <nl> } else { <nl> - msleep_interruptible ( ms ); <nl> + msleep ( ms ); <nl> } <nl> } <nl> 
static int atmci_regs_show ( struct seq_file * s , void * v ) <nl> atmci_show_status_reg ( s , " SR ", buf [ MCI_SR / 4 ]); <nl> atmci_show_status_reg ( s , " IMR ", buf [ MCI_IMR / 4 ]); <nl>  <nl> + kfree ( buf ); <nl> + <nl> return 0 ; <nl> } <nl> 
static void cpu_ready_for_interrupts ( void ) <nl> * If we are not in hypervisor mode the job is done once for <nl> * the whole partition in configure_exceptions (). <nl> */ <nl> - if ( early_cpu_has_feature ( CPU_FTR_HVMODE ) && <nl> - early_cpu_has_feature ( CPU_FTR_ARCH_207S )) { <nl> + if ( cpu_has_feature ( CPU_FTR_HVMODE ) && <nl> + cpu_has_feature ( CPU_FTR_ARCH_207S )) { <nl> unsigned long lpcr = mfspr ( SPRN_LPCR ); <nl> mtspr ( SPRN_LPCR , lpcr | LPCR_AIL_3 ); <nl> }
void cec_received_msg ( struct cec_adapter * adap , struct cec_msg * msg ) <nl> if (! valid_la || msg -> len <= 1 ) <nl> return ; <nl>  <nl> + if ( adap -> log_addrs . log_addr_mask == 0 ) <nl> + return ; <nl> + <nl> /* <nl> * Process the message on the protocol level . If is_reply is true , <nl> * then cec_receive_notify () won ' t pass on the reply to the listener ( s )
int iioutils_get_type ( unsigned * is_signed , <nl> ret = - errno ; <nl> printf (" failed to pass scan type description \ n "); <nl> goto error_close_sysfsfp ; <nl> + } else if ( ret != 5 ) { <nl> + ret = - EIO ; <nl> + printf (" scan type description didn ' t match \ n "); <nl> + goto error_close_sysfsfp ; <nl> } <nl> * be = ( endianchar == ' b '); <nl> * bytes = padint / 8 ;
efi_initialize_iomem_resources ( struct resource * code_resource , <nl> if ( md -> attribute & EFI_MEMORY_WP ) { <nl> name = " System ROM "; <nl> flags |= IORESOURCE_READONLY ; <nl> - } else { <nl> + } else if ( md -> attribute == EFI_MEMORY_UC ) <nl> + name = " Uncached RAM "; <nl> + else <nl> name = " System RAM "; <nl> - } <nl> break ; <nl>  <nl> case EFI_ACPI_MEMORY_NVS :
bool ieee80211_set_channel_type ( struct ieee80211_local * local , <nl> switch ( tmp -> vif . bss_conf . channel_type ) { <nl> case NL80211_CHAN_NO_HT : <nl> case NL80211_CHAN_HT20 : <nl> + if ( superchan > tmp -> vif . bss_conf . channel_type ) <nl> + break ; <nl> + <nl> superchan = tmp -> vif . bss_conf . channel_type ; <nl> break ; <nl> case NL80211_CHAN_HT40PLUS :
static void tce_buildmulti_pSeriesLP ( struct iommu_table * tbl , long tcenum , <nl> union tce_entry tce , * tcep ; <nl> long l , limit ; <nl>  <nl> - if ( npages == 1 ) <nl> + if ( TCE_PAGE_FACTOR == 0 && npages == 1 ) <nl> return tce_build_pSeriesLP ( tbl , tcenum , npages , uaddr , <nl> direction ); <nl> 
static int ipw_wx_set_retry ( struct net_device * dev , <nl> if (!( wrqu -> retry . flags & IW_RETRY_LIMIT )) <nl> return 0 ; <nl>  <nl> - if ( wrqu -> retry . value < 0 || wrqu -> retry . value > 255 ) <nl> + if ( wrqu -> retry . value < 0 || wrqu -> retry . value >= 255 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& priv -> mutex );
static int get_task_ioprio ( struct task_struct * p ) <nl> if ( ret ) <nl> goto out ; <nl> ret = IOPRIO_PRIO_VALUE ( IOPRIO_CLASS_NONE , IOPRIO_NORM ); <nl> + task_lock ( p ); <nl> if ( p -> io_context ) <nl> ret = p -> io_context -> ioprio ; <nl> + task_unlock ( p ); <nl> out : <nl> return ret ; <nl> }
static void mousevsc_on_channel_callback ( void * context ) <nl> static int mousevsc_connect_to_vsp ( struct hv_device * device ) <nl> { <nl> int ret = 0 ; <nl> - int t ; <nl> + unsigned long t ; <nl> struct mousevsc_dev * input_dev = hv_get_drvdata ( device ); <nl> struct mousevsc_prt_msg * request ; <nl> struct mousevsc_prt_msg * response ;
static int kjournald2 ( void * arg ) <nl> goto loop ; <nl>  <nl> end_loop : <nl> - write_unlock (& journal -> j_state_lock ); <nl> del_timer_sync (& journal -> j_commit_timer ); <nl> journal -> j_task = NULL ; <nl> wake_up (& journal -> j_wait_done_commit ); <nl> jbd_debug ( 1 , " Journal thread exiting .\ n "); <nl> + write_unlock (& journal -> j_state_lock ); <nl> return 0 ; <nl> } <nl> 
static int patch_alc269 ( struct hda_codec * codec ) <nl>  <nl> spec = codec -> spec ; <nl> spec -> gen . shared_mic_vref_pin = 0x18 ; <nl> + codec -> power_save_node = 1 ; <nl>  <nl> snd_hda_pick_fixup ( codec , alc269_fixup_models , <nl> alc269_fixup_tbl , alc269_fixups );
extern int debug_locks_off ( void ); <nl> ({ \ <nl> int __ret = 0 ; \ <nl> \ <nl> - if ( unlikely ( c )) { \ <nl> + if (! oops_in_progress && unlikely ( c )) { \ <nl> if ( debug_locks_off () && ! debug_locks_silent ) \ <nl> WARN_ON ( 1 ); \ <nl> __ret = 1 ; \
static int do_dentry_open ( struct file * f , <nl> return 0 ; <nl>  <nl> cleanup_all : <nl> + if ( WARN_ON_ONCE ( error > 0 )) <nl> + error = - EINVAL ; <nl> fops_put ( f -> f_op ); <nl> if ( f -> f_mode & FMODE_WRITER ) { <nl> put_write_access ( inode );
static int check_ptr_alignment ( struct bpf_verifier_env * env , <nl> break ; <nl> case PTR_TO_STACK : <nl> pointer_desc = " stack "; <nl> + /* The stack spill tracking logic in check_stack_write () <nl> + * and check_stack_read () relies on stack accesses being <nl> + * aligned . <nl> + */ <nl> + strict = true ; <nl> break ; <nl> default : <nl> break ;
int crypto_reportstat ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
static void storvsc_handle_error ( struct vmscsi_request * vm_srb , <nl> do_work = true ; <nl> process_err_fn = storvsc_remove_lun ; <nl> break ; <nl> - case ( SRB_STATUS_ABORTED | SRB_STATUS_AUTOSENSE_VALID ): <nl> - if (( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> + case SRB_STATUS_ABORTED : <nl> + if ( vm_srb -> srb_status & SRB_STATUS_AUTOSENSE_VALID && <nl> + ( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> do_work = true ; <nl> process_err_fn = storvsc_device_scan ; <nl> /*
static int l2tp_ip6_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> lsa -> l2tp_family = AF_INET6 ; <nl> lsa -> l2tp_flowinfo = 0 ; <nl> lsa -> l2tp_scope_id = 0 ; <nl> + lsa -> l2tp_unused = 0 ; <nl> if ( peer ) { <nl> if (! lsk -> peer_conn_id ) <nl> return - ENOTCONN ;
static int drbg_fini_sym_kernel ( struct drbg_state * drbg ) <nl> drbg -> ctr_handle = NULL ; <nl>  <nl> if ( drbg -> ctr_req ) <nl> - skcipher_request_free ( drbg -> ctr_req );; <nl> + skcipher_request_free ( drbg -> ctr_req ); <nl> drbg -> ctr_req = NULL ; <nl>  <nl> kfree ( drbg -> ctr_null_value_buf );
static int zfcp_ccw_set_online ( struct ccw_device * ccw_device ) <nl> zfcp_erp_adapter_reopen ( adapter , ZFCP_STATUS_COMMON_ERP_FAILED , 85 , <nl> NULL ); <nl> zfcp_erp_wait ( adapter ); <nl> - goto out ; <nl> + up (& zfcp_data . config_sema ); <nl> + flush_work (& adapter -> scan_work ); <nl> + return 0 ; <nl>  <nl> out_scsi_register : <nl> zfcp_erp_thread_kill ( adapter );
static int ipmmu_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& mmu -> lock ); <nl> bitmap_zero ( mmu -> ctx , IPMMU_CTX_MAX ); <nl> mmu -> features = of_device_get_match_data (& pdev -> dev ); <nl> + dma_set_mask_and_coherent (& pdev -> dev , DMA_BIT_MASK ( 40 )); <nl>  <nl> /* Map I / O memory and request IRQ . */ <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 );
# define crisv10_mask_irq ( irq_nr ) (* R_VECT_MASK_CLR = 1 << ( irq_nr )); <nl> # define crisv10_unmask_irq ( irq_nr ) (* R_VECT_MASK_SET = 1 << ( irq_nr )); <nl>  <nl> + extern void kgdb_init ( void ); <nl> + extern void breakpoint ( void ); <nl> + <nl> /* don ' t use set_int_vector , it bypasses the linux interrupt handlers . it is <nl> * global just so that the kernel gdb can use it . <nl> */
static int special_clk_ctl_put ( struct snd_kcontrol * kctl , <nl> struct special_params * params = bebob -> maudio_special_quirk ; <nl> int err , id ; <nl>  <nl> - mutex_lock (& bebob -> mutex ); <nl> - <nl> id = uval -> value . enumerated . item [ 0 ]; <nl> if ( id >= ARRAY_SIZE ( special_clk_labels )) <nl> return 0 ; <nl>  <nl> + mutex_lock (& bebob -> mutex ); <nl> + <nl> err = avc_maudio_set_special_clk ( bebob , id , <nl> params -> dig_in_fmt , <nl> params -> dig_out_fmt ,
static ssize_t dm_dp_aux_transfer ( struct drm_dp_aux * aux , <nl> enum ddc_result res ; <nl> ssize_t read_bytes ; <nl>  <nl> + if ( WARN_ON ( msg -> size > 16 )) <nl> + return - E2BIG ; <nl> + <nl> switch ( msg -> request & ~ DP_AUX_I2C_MOT ) { <nl> case DP_AUX_NATIVE_READ : <nl> read_bytes = dal_ddc_service_read_dpcd_data (
static int falcon_mtd_probe ( struct efx_nic * efx ) <nl>  <nl> /* Allocate space for maximum number of partitions */ <nl> parts = kcalloc ( 2 , sizeof (* parts ), GFP_KERNEL ); <nl> + if (! parts ) <nl> + return - ENOMEM ; <nl> n_parts = 0 ; <nl>  <nl> spi = & nic_data -> spi_flash ;
void exit_io_context ( void ) <nl> ioc -> aic -> exit ( ioc -> aic ); <nl> cfq_exit ( ioc ); <nl>  <nl> - put_io_context ( ioc ); <nl> } <nl> + put_io_context ( ioc ); <nl> } <nl>  <nl> struct io_context * alloc_io_context ( gfp_t gfp_flags , int node )
static int crypt_iv_essiv_ctr ( struct crypt_config * cc , struct dm_target * ti , <nl>  <nl> if ( err ) { <nl> ti -> error = " Error calculating hash in ESSIV "; <nl> + kfree ( salt ); <nl> return err ; <nl> } <nl> 
static int mt9t112_init_pll ( const struct i2c_client * client ) <nl> * I2C Master Clock Divider <nl> */ <nl> mt9t112_reg_write ( ret , client , 0x0014 , 0x3046 ); <nl> + mt9t112_reg_write ( ret , client , 0x0016 , 0x0400 ); /* JPEG initialization workaround */ <nl> mt9t112_reg_write ( ret , client , 0x0022 , 0x0190 ); <nl> mt9t112_reg_write ( ret , client , 0x3B84 , 0x0212 ); <nl> 
static int ext2_fill_super ( struct super_block * sb , void * data , int silent ) <nl> if ( EXT2_INODE_SIZE ( sb ) == 0 ) <nl> goto cantfind_ext2 ; <nl> sbi -> s_inodes_per_block = sb -> s_blocksize / EXT2_INODE_SIZE ( sb ); <nl> - if ( sbi -> s_inodes_per_block == 0 ) <nl> + if ( sbi -> s_inodes_per_block == 0 || sbi -> s_inodes_per_group == 0 ) <nl> goto cantfind_ext2 ; <nl> sbi -> s_itb_per_group = sbi -> s_inodes_per_group / <nl> sbi -> s_inodes_per_block ;
int mwifiex_del_mgmt_ies ( struct mwifiex_private * priv ) <nl> ar_ie , & priv -> assocresp_idx ); <nl>  <nl> done : <nl> + kfree ( gen_ie ); <nl> kfree ( beacon_ie ); <nl> kfree ( pr_ie ); <nl> kfree ( ar_ie );
void snd_seq_device_load_drivers ( void ) <nl> flush_work (& autoload_work ); <nl> } <nl> EXPORT_SYMBOL ( snd_seq_device_load_drivers ); <nl> +# define cancel_autoload_drivers () cancel_work_sync (& autoload_work ) <nl> # else <nl> # define queue_autoload_drivers () /* NOP */ <nl> +# define cancel_autoload_drivers () /* NOP */ <nl> # endif <nl>  <nl> /* <nl> static int snd_seq_device_dev_free ( struct snd_device * device ) <nl> { <nl> struct snd_seq_device * dev = device -> device_data ; <nl>  <nl> + cancel_autoload_drivers (); <nl> put_device (& dev -> dev ); <nl> return 0 ; <nl> }
static inline struct dentry * ovl_lookup_real ( struct dentry * dir , <nl> { <nl> struct dentry * dentry ; <nl>  <nl> - inode_lock ( dir -> d_inode ); <nl> - dentry = lookup_one_len ( name -> name , dir , name -> len ); <nl> - inode_unlock ( dir -> d_inode ); <nl> + dentry = lookup_hash ( name , dir ); <nl>  <nl> if ( IS_ERR ( dentry )) { <nl> if ( PTR_ERR ( dentry ) == - ENOENT )
cpufreq_stat_notifier_trans ( struct notifier_block * nb , unsigned long val , <nl> return 0 ; <nl> } <nl>  <nl> - static int __cpuinit cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> + static int cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> unsigned long action , void * hcpu ) <nl> { <nl> unsigned int cpu = ( unsigned long ) hcpu ;
static int create_fixed_stream_quirk ( struct snd_usb_audio * chip , <nl> } <nl> alts = & iface -> altsetting [ fp -> altset_idx ]; <nl> altsd = get_iface_desc ( alts ); <nl> + if ( altsd -> bNumEndpoints < 1 ) { <nl> + kfree ( fp ); <nl> + kfree ( rate_table ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> fp -> protocol = altsd -> bInterfaceProtocol ; <nl>  <nl> if ( fp -> datainterval == 0 )
static void iwl_req_fw_callback ( const struct firmware * ucode_raw , void * context ) <nl> op -> name , err ); <nl> # endif <nl> } <nl> + kfree ( pieces ); <nl> return ; <nl>  <nl> try_again :
static int __devinit bnx2x_init_one ( struct pci_dev * pdev , <nl> bp = netdev_priv ( dev ); <nl> bp -> msglevel = debug ; <nl>  <nl> + pci_set_drvdata ( pdev , dev ); <nl> + <nl> rc = bnx2x_init_dev ( pdev , dev ); <nl> if ( rc < 0 ) { <nl> free_netdev ( dev ); <nl> return rc ; <nl> } <nl>  <nl> - pci_set_drvdata ( pdev , dev ); <nl> - <nl> rc = bnx2x_init_bp ( bp ); <nl> if ( rc ) <nl> goto init_one_exit ;
static ssize_t gt_max_freq_mhz_show ( struct device * kdev , struct device_attribute <nl> int ret ; <nl>  <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl> - ret = dev_priv -> rps . hw_max * GT_FREQUENCY_MULTIPLIER ; <nl> + ret = dev_priv -> rps . max_delay * GT_FREQUENCY_MULTIPLIER ; <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> return snprintf ( buf , PAGE_SIZE , "% d \ n ", ret );
static int do_prlimit ( struct task_struct * tsk , unsigned int resource , <nl>  <nl> if ( resource >= RLIM_NLIMITS ) <nl> return - EINVAL ; <nl> + resource = array_index_nospec ( resource , RLIM_NLIMITS ); <nl> + <nl> if ( new_rlim ) { <nl> if ( new_rlim -> rlim_cur > new_rlim -> rlim_max ) <nl> return - EINVAL ;
int snd_midi_event_encode_byte ( struct snd_midi_event * dev , int c , <nl> ev -> type = status_event [ ST_SPECIAL + c - 0xf0 ]. event ; <nl> ev -> flags &= ~ SNDRV_SEQ_EVENT_LENGTH_MASK ; <nl> ev -> flags |= SNDRV_SEQ_EVENT_LENGTH_FIXED ; <nl> - return 1 ; <nl> + return ev -> type != SNDRV_SEQ_EVENT_NONE ; <nl> } <nl>  <nl> spin_lock_irqsave (& dev -> lock , flags );
static const char * nand_usdhc_bus_sel [] = { " osc ", " pll_sys_pfd2_270m_clk ", <nl>  <nl> static const char * ahb_channel_sel [] = { " osc ", " pll_sys_pfd2_270m_clk ", <nl> " pll_dram_533m_clk ", " pll_sys_pfd0_392m_clk ", <nl> - " pll_enet_125m_clk ", " pll_usb_main_clk ", " pll_audio_post_div ", <nl> + " pll_enet_250m_clk ", " pll_usb_main_clk ", " pll_audio_post_div ", <nl> " pll_video_post_div ", }; <nl>  <nl> static const char * dram_phym_sel [] = { " pll_dram_main_clk ",
struct inode { <nl> struct timespec i_atime ; <nl> struct timespec i_mtime ; <nl> struct timespec i_ctime ; <nl> - unsigned int i_blkbits ; <nl> blkcnt_t i_blocks ; <nl> + unsigned int i_blkbits ; <nl> unsigned short i_bytes ; <nl> umode_t i_mode ; <nl> spinlock_t i_lock ; /* i_blocks , i_bytes , maybe i_size */
void btrfs_destroy_workqueue ( struct btrfs_workqueue * wq ) <nl> if ( wq -> high ) <nl> __btrfs_destroy_workqueue ( wq -> high ); <nl> __btrfs_destroy_workqueue ( wq -> normal ); <nl> + kfree ( wq ); <nl> } <nl>  <nl> void btrfs_workqueue_set_max ( struct btrfs_workqueue * wq , int max )
int seq_buf_putmem_hex ( struct seq_buf * s , const void * mem , <nl>  <nl> WARN_ON ( s -> size == 0 ); <nl>  <nl> + BUILD_BUG_ON ( MAX_MEMHEX_BYTES * 2 >= HEX_CHARS ); <nl> + <nl> while ( len ) { <nl> - start_len = min ( len , HEX_CHARS - 1 ); <nl> + start_len = min ( len , MAX_MEMHEX_BYTES ); <nl> # ifdef __BIG_ENDIAN <nl> for ( i = 0 , j = 0 ; i < start_len ; i ++) { <nl> # else
static bool do_propagate_liveness ( const struct bpf_verifier_state * state , <nl> if ( parent -> spilled_regs [ i ]. live & REG_LIVE_READ ) <nl> continue ; <nl> if ( state -> spilled_regs [ i ]. live == REG_LIVE_READ ) { <nl> - parent -> regs [ i ]. live |= REG_LIVE_READ ; <nl> + parent -> spilled_regs [ i ]. live |= REG_LIVE_READ ; <nl> touched = true ; <nl> } <nl> }
static int __rds_rdma_map ( struct rds_sock * rs , struct rds_get_mr_args * args , <nl> long i ; <nl> int ret ; <nl>  <nl> - if ( rs -> rs_bound_addr == 0 ) { <nl> + if ( rs -> rs_bound_addr == 0 || ! rs -> rs_transport ) { <nl> ret = - ENOTCONN ; /* XXX not a great errno */ <nl> goto out ; <nl> }
int inet6_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl> */ <nl> v4addr = LOOPBACK4_IPV6 ; <nl> if (!( addr_type & IPV6_ADDR_MULTICAST )) { <nl> - if (! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> + if (! inet -> transparent && <nl> + ! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> dev , 0 )) { <nl> err = - EADDRNOTAVAIL ; <nl> goto out_unlock ;
int tcp_disconnect ( struct sock * sk , int flags ) <nl> tcp_set_ca_state ( sk , TCP_CA_Open ); <nl> tcp_clear_retrans ( tp ); <nl> inet_csk_delack_init ( sk ); <nl> + /* Initialize rcv_mss to TCP_MIN_MSS to avoid division by 0 <nl> + * issue in __tcp_select_window () <nl> + */ <nl> + icsk -> icsk_ack . rcv_mss = TCP_MIN_MSS ; <nl> tcp_init_send_head ( sk ); <nl> memset (& tp -> rx_opt , 0 , sizeof ( tp -> rx_opt )); <nl> __sk_dst_reset ( sk );
struct tracepoint { <nl> do { \ <nl> void ** it_func ; \ <nl> \ <nl> - rcu_read_lock_sched (); \ <nl> + rcu_read_lock_sched_notrace (); \ <nl> it_func = rcu_dereference (( tp )-> funcs ); \ <nl> if ( it_func ) { \ <nl> do { \ <nl> (( void (*)( proto ))(* it_func ))( args ); \ <nl> } while (*(++ it_func )); \ <nl> } \ <nl> - rcu_read_unlock_sched (); \ <nl> + rcu_read_unlock_sched_notrace (); \ <nl> } while ( 0 ) <nl>  <nl> /*
int dm_kcopyd_copy ( struct dm_kcopyd_client * kc , struct dm_io_region * from , <nl> job -> fn = fn ; <nl> job -> context = context ; <nl>  <nl> - if ( job -> source . count < SUB_JOB_SIZE ) <nl> + if ( job -> source . count <= SUB_JOB_SIZE ) <nl> dispatch_job ( job ); <nl>  <nl> else {
void ath10k_htt_tx_free ( struct ath10k_htt * htt ) <nl> { <nl> int size ; <nl>  <nl> + tasklet_kill (& htt -> txrx_compl_task ); <nl> + <nl> idr_for_each (& htt -> pending_tx , ath10k_htt_tx_clean_up_pending , htt -> ar ); <nl> idr_destroy (& htt -> pending_tx ); <nl> 
int __init igafb_init ( void ) <nl> iounmap ( info -> screen_base ); <nl> kfree ( par -> mmap_map ); <nl> kfree ( info ); <nl> + return - ENODEV ; <nl> } <nl>  <nl> # ifdef CONFIG_SPARC
static int onenand_write_ops_nolock ( struct mtd_info * mtd , loff_t to , <nl> } <nl>  <nl> /* Only check verify write turn on */ <nl> - ret = onenand_verify ( mtd , ( u_char *) wbuf , to , thislen ); <nl> + ret = onenand_verify ( mtd , buf , to , thislen ); <nl> if ( ret ) { <nl> printk ( KERN_ERR " onenand_write_ops_nolock : verify failed % d \ n ", ret ); <nl> break ;
unsigned long hugetlb_get_unmapped_area ( struct file * file , unsigned long addr , <nl> { <nl> struct hstate * hstate = hstate_file ( file ); <nl> int mmu_psize = shift_to_mmu_psize ( huge_page_shift ( hstate )); <nl> + <nl> + if (! mmu_huge_psizes [ mmu_psize ]) <nl> + return - EINVAL ; <nl> return slice_get_unmapped_area ( addr , len , flags , mmu_psize , 1 , 0 ); <nl> } <nl> 
struct vm_area_struct * vb2_get_vma ( struct vm_area_struct * vma ) <nl> { <nl> struct vm_area_struct * vma_copy ; <nl>  <nl> - vma_copy = kmalloc ( sizeof (* vma_copy ), GFP_KERNEL ); <nl> + vma_copy = kmem_cache_alloc ( vm_area_cachep , GFP_KERNEL ); <nl> if ( vma_copy == NULL ) <nl> return NULL ; <nl>  <nl> void vb2_put_vma ( struct vm_area_struct * vma ) <nl> if ( vma -> vm_file ) <nl> fput ( vma -> vm_file ); <nl>  <nl> - kfree ( vma ); <nl> + kmem_cache_free ( vm_area_cachep , vma ); <nl> } <nl> EXPORT_SYMBOL_GPL ( vb2_put_vma ); <nl> 
static struct rpc_task * nfs4_do_unlck ( struct file_lock * fl , <nl> { <nl> struct nfs4_unlockdata * data ; <nl>  <nl> + /* Ensure this is an unlock - when canceling a lock , the <nl> + * canceled lock is passed in , and it won ' t be an unlock . <nl> + */ <nl> + fl -> fl_type = F_UNLCK ; <nl> + <nl> data = nfs4_alloc_unlockdata ( fl , ctx , lsp , seqid ); <nl> if ( data == NULL ) { <nl> nfs_free_seqid ( seqid );
static int yam_siocdevprivate ( struct net_device * dev , struct ifreq * ifr , void __ <nl> ym = memdup_user ( data , sizeof ( struct yamdrv_ioctl_mcs )); <nl> if ( IS_ERR ( ym )) <nl> return PTR_ERR ( ym ); <nl> - if ( ym -> cmd != SIOCYAMSMCS ) <nl> - return - EINVAL ; <nl> - if ( ym -> bitrate > YAM_MAXBITRATE ) { <nl> + if ( ym -> cmd != SIOCYAMSMCS || ym -> bitrate > YAM_MAXBITRATE ) { <nl> kfree ( ym ); <nl> return - EINVAL ; <nl> }
struct platform_device * __init imx_add_mxc_mmc ( <nl> struct resource res [] = { <nl> { <nl> . start = data -> iobase , <nl> - . end = data -> iobase + SZ_4K - 1 , <nl> + . end = data -> iobase + data -> iosize - 1 , <nl> . flags = IORESOURCE_MEM , <nl> }, { <nl> . start = data -> irq ,
int bpf_jit_enable __read_mostly ; <nl>  <nl> static void bpf_jit_fill_ill_insns ( void * area , unsigned int size ) <nl> { <nl> - int * p = area ; <nl> - <nl> - /* Fill whole space with trap instructions */ <nl> - while ( p < ( int *)(( char *) area + size )) <nl> - * p ++ = BREAKPOINT_INSTRUCTION ; <nl> + memset32 ( area , BREAKPOINT_INSTRUCTION , size / 4 ); <nl> } <nl>  <nl> static inline void bpf_flush_icache ( void * start , void * end )
static int acl_permission_check ( struct inode * inode , int mask ) <nl> if ( current_user_ns () != inode_userns ( inode )) <nl> goto other_perms ; <nl>  <nl> - if ( current_fsuid () == inode -> i_uid ) <nl> + if ( likely ( current_fsuid () == inode -> i_uid )) <nl> mode >>= 6 ; <nl> else { <nl> if ( IS_POSIXACL ( inode ) && ( mode & S_IRWXG )) {
static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
static void __init_discard_policy ( struct f2fs_sb_info * sbi , <nl> dpolicy -> min_interval = DEF_MIN_DISCARD_ISSUE_TIME ; <nl> dpolicy -> max_interval = DEF_MAX_DISCARD_ISSUE_TIME ; <nl> dpolicy -> io_aware = true ; <nl> + dpolicy -> sync = false ; <nl> if ( utilization ( sbi ) > DEF_DISCARD_URGENT_UTIL ) { <nl> dpolicy -> granularity = 1 ; <nl> dpolicy -> max_interval = DEF_MIN_DISCARD_ISSUE_TIME ;
static void nvmet_execute_identify_ctrl ( struct nvmet_req * req ) <nl> id -> vid = 0 ; <nl> id -> ssvid = 0 ; <nl>  <nl> + memset ( id -> sn , ' ', sizeof ( id -> sn )); <nl> bin2hex ( id -> sn , & ctrl -> subsys -> serial , <nl> min ( sizeof ( ctrl -> subsys -> serial ), sizeof ( id -> sn ) / 2 )); <nl> memcpy_and_pad ( id -> mn , sizeof ( id -> mn ), model , sizeof ( model ) - 1 , ' ');
static struct obstack obstack_for_string ; <nl> # define STRING_1GROW ( Char ) \ <nl> obstack_1grow (& obstack_for_string , Char ) <nl>  <nl> -# define STRING_FREE () \ <nl> +# ifdef NDEBUG <nl> +# define STRING_FREE () \ <nl> obstack_free (& obstack_for_string , last_string ) <nl> +# else <nl> +# define STRING_FREE () \ <nl> + do { \ <nl> + obstack_free (& obstack_for_string , last_string ); \ <nl> + last_string = NULL ; \ <nl> + } while ( 0 ) <nl> +# endif <nl>  <nl> # endif
static SQInteger thread_call ( HSQUIRRELVM v ) <nl> SQObjectPtr o = stack_get ( v , 1 ); <nl> if ( sq_type ( o ) == OT_THREAD ) { <nl> SQInteger nparams = sq_gettop ( v ); <nl> + sq_reservestack ( _thread ( o ), nparams + 3 ); <nl> _thread ( o )-> Push ( _thread ( o )-> _roottable ); <nl> for ( SQInteger i = 2 ; i <( nparams + 1 ); i ++) <nl> sq_move ( _thread ( o ), v , i );
MultiPartInputFile :: initialize () <nl> // Perform usual check on headers . <nl> // <nl>  <nl> + if ( _data -> _headers . size () == 0 ) <nl> + { <nl> + throw IEX_NAMESPACE :: ArgExc (" Files must contain at least one header "); <nl> + } <nl> + <nl> for ( size_t i = 0 ; i < _data -> _headers . size (); i ++) <nl> { <nl> //
B44Compressor :: B44Compressor <nl> // <nl>  <nl> _tmpBuffer = new unsigned short <nl> - [ checkArraySize ( uiMult ( maxScanLineSize , numScanLines ), <nl> + [ checkArraySize ( uiMult ( maxScanLineSize / sizeof ( unsigned short ), numScanLines ), <nl> sizeof ( unsigned short ))]; <nl>  <nl> const ChannelList & channels = header (). channels ();
class McAsciiParserBase { <nl> const char * posStart , <nl> const char * posEnd ); <nl>  <nl> + // limit the value size . <nl> + static constexpr uint32_t maxValueBytes = 1 * 1024 * 1024 * 1024 ; // 1GB <nl> + <nl> std :: string currentErrorDescription_ ; <nl>  <nl> uint64_t currentUInt_ { 0 };
BaseType_t xQueueGenericReset ( QueueHandle_t xQueue , <nl> /* Check for multiplication overflow . */ <nl> configASSERT ( ( uxItemSize == 0 ) || ( uxQueueLength == ( xQueueSizeInBytes / uxItemSize ) ) ); <nl>  <nl> + /* Check for addition overflow . */ <nl> + configASSERT ( ( sizeof ( Queue_t ) + xQueueSizeInBytes ) > xQueueSizeInBytes ); <nl> + <nl> /* Allocate the queue and storage area . Justification for MISRA <nl> * deviation as follows : pvPortMalloc () always ensures returned memory <nl> * blocks are aligned per the requirements of the MCU stack . In this case
fribidi_cap_rtl_to_unicode ( <nl> } <nl> } <nl> else <nl> - us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + { <nl> + if (( int ) s [ i ] < 0 ) <nl> + us [ j ++] = '?'; <nl> + else <nl> + us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + } <nl> } <nl>  <nl> return j ;
hermesBuiltinApply ( void *, Runtime * runtime , NativeArgs args ) { <nl>  <nl> ScopedNativeCallFrame newFrame { <nl> runtime , len , * fn , isConstructor , thisVal . getHermesValue ()}; <nl> + if ( LLVM_UNLIKELY ( newFrame . overflowed ())) <nl> + return runtime -> raiseStackOverflow ( Runtime :: StackOverflowKind :: NativeStack ); <nl> + <nl> for ( uint32_t i = 0 ; i < len ; ++ i ) { <nl> newFrame -> getArgRef ( i ) = argArray -> at ( runtime , i ); <nl> }
int gnutls_x509_ext_import_crl_dist_points ( const gnutls_datum_t * ext , <nl>  <nl> if ( ret < 0 && ret != GNUTLS_E_REQUESTED_DATA_NOT_AVAILABLE ) { <nl> gnutls_assert (); <nl> - gnutls_free ( san . data ); <nl> goto cleanup ; <nl> } <nl> 
gif_internal_decode_frame ( gif_animation * gif , <nl> unsigned int x , y , decode_y , burst_bytes ; <nl> register unsigned char colour ; <nl>  <nl> + /* If the GIF has no frame data , frame holders will not be allocated in <nl> + * gif_initialise () */ <nl> + if ( gif -> frames == NULL ) { <nl> + return GIF_INSUFFICIENT_DATA ; <nl> + } <nl> + <nl> /* Ensure this frame is supposed to be decoded */ <nl> if ( gif -> frames [ frame ]. display == false ) { <nl> return GIF_OK ;
lzw_result lzw_decode ( struct lzw_ctx * ctx , <nl> /* Code is invalid */ <nl> return LZW_BAD_CODE ; <nl>  <nl> + } else if ( code_new >= 1 << LZW_CODE_MAX ) { <nl> + /* Don ' t access out of bound */ <nl> + return LZW_BAD_CODE ; <nl> + <nl> } else if ( code_new < current_entry ) { <nl> /* Code is in table */ <nl> code_out = code_new ;
static int prctl_set_vma_anon_name ( unsigned long start , unsigned long end , <nl> tmp = end ; <nl>  <nl> /* Here vma -> vm_start <= start < tmp <= ( end | vma -> vm_end ). */ <nl> - error = prctl_update_vma_anon_name ( vma , & prev , start , end , <nl> + error = prctl_update_vma_anon_name ( vma , & prev , start , tmp , <nl> ( const char __user *) arg ); <nl> if ( error ) <nl> return error ;
static int stszin ( int size ) <nl> u32in (); <nl> // Number of entries <nl> mp4config . frame . ents = u32in (); <nl> - // fixme : check atom size <nl> + <nl> + if (!( mp4config . frame . ents + 1 )) <nl> + return ERR_FAIL ; <nl> + <nl> mp4config . frame . data = malloc ( sizeof (* mp4config . frame . data ) <nl> * ( mp4config . frame . ents + 1 )); <nl> 
int pam_sm_acct_mgmt ( pam_handle_t * pamh , int flags , <nl> int tac_fd ; <nl>  <nl> user = tty = r_addr = NULL ; <nl> + memset (& arep , 0 , sizeof ( arep )); <nl>  <nl> /* this also obtains service name for authorization <nl> this should be normally performed by pam_get_item ( PAM_SERVICE )
 <nl> # include " idn2 . h " <nl>  <nl> +# include < sys / types . h > <nl> # include < stdbool . h > <nl>  <nl> # include " bidi . h " <nl> static bool <nl> _isBidi ( const uint32_t * label , size_t llen ) <nl> { <nl> - while ( llen -- > 0 ) { <nl> + for (; ( ssize_t ) llen > 0 ; llen --) { <nl> int bc = uc_bidi_category (* label ++); <nl>  <nl> if ( bc == UC_BIDI_R || bc == UC_BIDI_AL || bc == UC_BIDI_AN )
static bool env_var_checked_p ; <nl>  <nl> # define FIELD_2DD_VECTOR ( nam , size , dxf ) \ <nl> OVERFLOW_CHECK ( nam , _obj -> size ) \ <nl> - FIELD_2RD ( nam [ 0 ], dxf ); \ <nl> + if ( _obj -> size ) \ <nl> + FIELD_2RD ( nam [ 0 ], dxf ); \ <nl> for ( vcount = 1 ; vcount < ( BITCODE_BL ) _obj -> size ; vcount ++) \ <nl> { \ <nl> FIELD_2DD ( nam [ vcount ], FIELD_VALUE ( nam [ vcount - 1 ]. x ), \
static void ProcessRadioRxDone ( void ) <nl> } <nl> } <nl>  <nl> + // Abort on empty radio frames <nl> + if ( size == 0 ) <nl> + { <nl> + MacCtx . McpsIndication . Status = LORAMAC_EVENT_INFO_STATUS_ERROR ; <nl> + PrepareRxDoneAbort ( ); <nl> + return ; <nl> + } <nl> + <nl> macHdr . Value = payload [ pktHeaderLen ++]; <nl>  <nl> // Accept frames of LoRaWAN Major Version 1 only
pspdf_prepare_outpages () <nl> chapter_outstarts [ c ] = num_outpages ; <nl>  <nl> for ( i = chapter_starts [ c ], j = 0 , nup = - 1 , page = pages + i ; <nl> - i <= chapter_ends [ c ]; <nl> + i <= chapter_ends [ c ] && num_outpages < num_pages ; <nl> i ++, page ++) <nl> { <nl> if ( nup != page -> nup )
static int cbor2json ( OSCTXT * pCborCtxt , OSCTXT * pJsonCtxt ) <nl> case OSRTCBOR_UTF8STR : { <nl> OSUTF8CHAR * utf8str ; <nl> ret = rtCborDecDynUTF8Str ( pCborCtxt , ub , ( char **)& utf8str ); <nl> + if ( 0 != ret ) return LOG_RTERR ( pCborCtxt , ret ); <nl>  <nl> ret = rtJsonEncStringValue ( pJsonCtxt , utf8str ); <nl> rtxMemFreePtr ( pCborCtxt , utf8str );
int update_timestamp ( const char * key , const struct efi_time * timestamp , char * la <nl> static uint64_t unpack_timestamp ( const struct efi_time * timestamp ) <nl> { <nl> uint64_t val = 0 ; <nl> - uint16_t year = le32_to_cpu ( timestamp -> year ); <nl> + uint16_t year = le16_to_cpu ( timestamp -> year ); <nl>  <nl> /* pad1 , nanosecond , timezone , daylight and pad2 are meant to be zero */ <nl> val |= (( uint64_t ) timestamp -> pad1 & 0xFF ) << 0 ;
static void DetectRunCleanup ( DetectEngineThreadCtx * det_ctx , <nl>  <nl> if ( pflow != NULL ) { <nl> /* update inspected tracker for raw reassembly */ <nl> - if ( p -> proto == IPPROTO_TCP && pflow -> protoctx != NULL ) { <nl> + if ( p -> proto == IPPROTO_TCP && pflow -> protoctx != NULL && <nl> + ( p -> flags & PKT_STREAM_EST )) <nl> + { <nl> StreamReassembleRawUpdateProgress ( pflow -> protoctx , p , <nl> det_ctx -> raw_stream_progress ); <nl> 
njs_module_path ( njs_vm_t * vm , const njs_str_t * dir , njs_module_info_t * info ) <nl> length = info -> name . length ; <nl>  <nl> if ( dir != NULL ) { <nl> - length = dir -> length ; <nl> + length += dir -> length ; <nl>  <nl> if ( length == 0 ) { <nl> return NJS_DECLINED ;
njs_function_frame_save ( njs_vm_t * vm , njs_frame_t * frame , u_char * pc ) <nl> njs_native_frame_t * active , * native ; <nl>  <nl> * frame = * vm -> active_frame ; <nl> + <nl> frame -> previous_active_frame = NULL ; <nl>  <nl> native = & frame -> native ; <nl> + native -> size = 0 ; <nl> + native -> free = NULL ; <nl> + native -> free_size = 0 ; <nl>  <nl> active = & vm -> active_frame -> native ; <nl> value_count = njs_function_frame_value_count ( active );
static const unsigned char * parse_object ( cJSON * item , const unsigned char * value <nl> fail : <nl> if ( item -> child != NULL ) <nl> { <nl> - cJSON_Delete ( child ); <nl> + cJSON_Delete ( item -> child ); <nl> item -> child = NULL ; <nl> } <nl> 
parse_again : <nl>  <nl> case ':': <nl> switch ( state ){ <nl> - case F_HOST : <nl> case F_IP6HOST : <nl> state = P_IP6HOST ; <nl> break ;
otError Commissioner :: GeneratePskc ( const char * aPassPhrase , <nl> uint16_t saltLen = 0 ; <nl>  <nl> VerifyOrExit (( strlen ( aPassPhrase ) >= OT_COMMISSIONING_PASSPHRASE_MIN_SIZE ) && <nl> - ( strlen ( aPassPhrase ) <= OT_COMMISSIONING_PASSPHRASE_MAX_SIZE ), <nl> + ( strlen ( aPassPhrase ) <= OT_COMMISSIONING_PASSPHRASE_MAX_SIZE ) && <nl> + ( strlen ( aNetworkName ) <= OT_NETWORK_NAME_MAX_SIZE ), <nl> error = OT_ERROR_INVALID_ARGS ); <nl>  <nl> memset ( salt , 0 , sizeof ( salt ));
OGRKMLLayer :: OGRKMLLayer ( const char * pszName , <nl> if ( poSRSIn != nullptr ) <nl> { <nl> poSRS_ -> SetWellKnownGeogCS ( " WGS84 " ); <nl> + poSRS_ -> SetAxisMappingStrategy ( OAMS_TRADITIONAL_GIS_ORDER ); <nl> if ( ! poSRS_ -> IsSame ( poSRSIn ) ) <nl> { <nl> poCT_ = OGRCreateCoordinateTransformation ( poSRSIn , poSRS_ );
WORD32 ih264d_decode_gaps_in_frame_num ( dec_struct_t * ps_dec , <nl>  <nl> ps_cur_slice = ps_dec -> ps_cur_slice ; <nl> ps_pic_params = ps_dec -> ps_cur_pps ; <nl> - ps_cur_slice -> u1_field_pic_flag = 0 ; <nl>  <nl> i4_frame_gaps = 0 ; <nl> ps_dpb_mgr = ps_dec -> ps_dpb_mgr ;
WORD32 ih264d_mark_err_slice_skip ( dec_struct_t * ps_dec , <nl> ih264d_err_pic_dispbuf_mgr ( ps_dec ); <nl> return 0 ; <nl> } <nl> - <nl> + ps_dec -> ps_dpb_cmds -> u1_long_term_reference_flag = 0 ; <nl> if ( prev_slice_err == 1 ) <nl> { <nl> /* first slice - missing / header corruption */
WORD32 ih264d_process_intra_mb ( dec_struct_t * ps_dec , <nl> + ( pu1_rem_intra4x4_pred_mode [ u1_cur_sub_block ] <nl> >= i1_intra_pred ); <nl> } <nl> + i1_intra_pred = CLIP3 ( 0 , 8 , i1_intra_pred ); <nl> { <nl> UWORD8 u1_err_code = pu1_intra_err_codes [ i1_intra_pred ]; <nl>  <nl> WORD32 ih264d_process_intra_mb ( dec_struct_t * ps_dec , <nl> + ( pu1_rem_intra4x4_pred_mode [ u1_sub_mb_num ] <nl> >= i1_intra_pred ); <nl> } <nl> + i1_intra_pred = CLIP3 ( 0 , 8 , i1_intra_pred ); <nl> { <nl> UWORD8 u1_err_code = pu1_intra_err_codes [ i1_intra_pred ]; <nl> 
WORD32 ih264d_start_of_pic ( dec_struct_t * ps_dec , <nl> ps_cur_pic -> pu1_col_zero_flag = ( UWORD8 *) ps_col_mv -> pv_col_zero_flag ; <nl> ps_cur_pic -> ps_mv = ( mv_pred_t *) ps_col_mv -> pv_mv ; <nl> ps_dec -> au1_pic_buf_ref_flag [ cur_pic_buf_id ] = 0 ; <nl> - if ( ps_dec -> u1_first_slice_in_stream ) <nl> + <nl> { <nl> /* make first entry of list0 point to cur pic , so that if first Islice is in error , ref pic struct will have valid entries */ <nl> ps_dec -> ps_ref_pic_buf_lx [ 0 ] = ps_dec -> ps_dpb_mgr -> ps_init_dpb [ 0 ];
status_t BnGraphicBufferProducer :: onTransact ( <nl> QueueBufferOutput * const output = <nl> reinterpret_cast < QueueBufferOutput *>( <nl> reply -> writeInplace ( sizeof ( QueueBufferOutput ))); <nl> + memset ( output , 0 , sizeof ( QueueBufferOutput )); <nl> status_t result = queueBuffer ( buf , input , output ); <nl> reply -> writeInt32 ( result ); <nl> return NO_ERROR ;
status_t BnGraphicBufferConsumer :: onTransact ( <nl> CHECK_INTERFACE ( IGraphicBufferConsumer , data , reply ); <nl> sp < GraphicBuffer > buffer = new GraphicBuffer (); <nl> data . read (* buffer . get ()); <nl> - int slot ; <nl> + int slot = - 1 ; <nl> int result = attachBuffer (& slot , buffer ); <nl> reply -> writeInt32 ( slot ); <nl> reply -> writeInt32 ( result );
uint8_t rfc_parse_data ( tRFC_MCB * p_mcb , MX_FRAME * p_frame , BT_HDR * p_buf ) { <nl>  <nl> eal = *( p_data )& RFCOMM_EA ; <nl> len = *( p_data )++ >> RFCOMM_SHIFT_LENGTH1 ; <nl> - if ( eal == 0 && p_buf -> len < RFCOMM_CTRL_FRAME_LEN ) { <nl> + if ( eal == 0 && p_buf -> len > RFCOMM_CTRL_FRAME_LEN ) { <nl> len += (*( p_data )++ << RFCOMM_SHIFT_LENGTH2 ); <nl> } else if ( eal == 0 ) { <nl> RFCOMM_TRACE_ERROR (" Bad Length when EAL = 0 : % d ", p_buf -> len );
int CGIFFF DGifSlurp ( CGIFFF GifFileType * GifFile ) <nl> memcpy ( ep -> Bytes , ExtData , ep -> ByteCount * sizeof ( char )); <nl> # else <nl> if ( ExtData == NULL ) break ; <nl> - AddExtensionBlock ( sp , ExtData [ 0 ], ExtData + 1 ); <nl> + AddExtensionBlock (& ext , ExtData [ 0 ], ExtData + 1 ); <nl> + ext . ExtensionBlocks [ ext . ExtensionBlockCount - 1 ]. code = ext_code ; <nl> # endif <nl> } <nl> }
void NuPlayer :: GenericSource :: notifyPreparedAndCleanup ( status_t err ) { <nl> { <nl> Mutex :: Autolock _l ( mDisconnectLock ); <nl> mDataSource . clear (); <nl> + mDrmManagerClient = NULL ; <nl> mCachedSource . clear (); <nl> mHttpSource . clear (); <nl> }
int64_t NuPlayer :: GenericSource :: getLastReadPosition () { <nl>  <nl> status_t NuPlayer :: GenericSource :: setBuffers ( <nl> bool audio , Vector < MediaBuffer *> & buffers ) { <nl> - if ( mIsWidevine && ! audio && mVideoTrack . mSource != NULL ) { <nl> + if ( mIsSecure && ! audio && mVideoTrack . mSource != NULL ) { <nl> return mVideoTrack . mSource -> setBuffers ( buffers ); <nl> } <nl> return INVALID_OPERATION ;
status_t OMXNodeInstance :: allocateBufferWithBackup ( <nl> } <nl>  <nl> CHECK_EQ ( header -> pAppPrivate , buffer_meta ); <nl> - memset ( header -> pBuffer , 0 , header -> nAllocLen ); <nl>  <nl> * buffer = makeBufferID ( header ); <nl> 
int modbus_reply ( modbus_t * ctx , const uint8_t * req , <nl> nb_write , nb , MODBUS_MAX_WR_WRITE_REGISTERS , MODBUS_MAX_WR_READ_REGISTERS ); <nl> } else if ( mapping_address < 0 || <nl> ( mapping_address + nb ) > mb_mapping -> nb_registers || <nl> - mapping_address < 0 || <nl> + mapping_address_write < 0 || <nl> ( mapping_address_write + nb_write ) > mb_mapping -> nb_registers ) { <nl> rsp_length = response_exception ( <nl> ctx , & sft , MODBUS_EXCEPTION_ILLEGAL_DATA_ADDRESS , rsp , FALSE ,
static bool GetUpdateFile ( update_t * p_update ) <nl> } <nl>  <nl> const int64_t i_read = stream_Size ( p_stream ); <nl> + <nl> + if ( i_read < 0 || i_read >= UINT16_MAX ) <nl> + { <nl> + msg_Err ( p_update -> p_libvlc , " Status file too large "); <nl> + goto error ; <nl> + } <nl> + <nl> psz_update_data = malloc ( i_read + 1 ); /* terminating '\ 0 ' */ <nl> if ( ! psz_update_data ) <nl> goto error ;
static block_t * Encode ( encoder_t * p_enc , picture_t * p_pic ) <nl> * is appended to the sequence header to allow guard <nl> * against poor streaming servers */ <nl> /* XXX , should this be done using the packetizer ? */ <nl> + <nl> + if ( len > UINT32_MAX - sizeof ( eos ) ) <nl> + return NULL ; <nl> + <nl> p_enc -> fmt_out . p_extra = malloc ( len + sizeof ( eos ) ); <nl> if ( ! p_enc -> fmt_out . p_extra ) <nl> return NULL ;
static int oidc_request_post_preserved_restore ( request_rec * r , <nl> " input . type = \" hidden \";\ n " <nl> " document . forms [ 0 ]. appendChild ( input );\ n " <nl> " }\ n " <nl> - " document . forms [ 0 ]. action = '% s ';\ n " <nl> + " document . forms [ 0 ]. action = \"% s \";\ n " <nl> " document . forms [ 0 ]. submit ();\ n " <nl> " }\ n " <nl> " </ script >\ n ", method , original_url );
int init_result ( RESULT & result , void *& data ) { <nl> log_messages . printf ( MSG_DEBUG , " Check result \ n "); <nl>  <nl> char buff [ 256 ]; <nl> - n = fscanf ( f , "% s ", buff ); <nl> + // n = fscanf ( f , "% s ", buff ); <nl> + fgets ( buff , 256 , f ); <nl> char * pch ; <nl> pch = strtok ( buff , " ,"); <nl> if ( pch != NULL ) {
resolve_iffeature ( struct lys_iffeature * expr ) <nl> { <nl> int index_e = 0 , index_f = 0 ; <nl>  <nl> - if ( expr -> expr ) { <nl> + if ( expr -> expr && expr -> features [ 0 ]) { <nl> return resolve_iffeature_recursive ( expr , & index_e , & index_f ); <nl> } <nl> return 0 ;
class MapStageOp : public OpKernel { <nl> OP_REQUIRES_OK ( ctx , ctx -> input (" key ", & key_tensor )); <nl> OP_REQUIRES_OK ( ctx , ctx -> input (" indices ", & indices_tensor )); <nl> OP_REQUIRES_OK ( ctx , ctx -> input_list (" values ", & values_tensor )); <nl> + OP_REQUIRES ( ctx , key_tensor -> NumElements () > 0 , <nl> + errors :: InvalidArgument (" key must not be empty ")); <nl>  <nl> // Create copy for insertion into Staging Area <nl> Tensor key (* key_tensor );
REGISTER_OP (" Dequantize ") <nl> if (! s . ok () && s . code () != error :: NOT_FOUND ) { <nl> return s ; <nl> } <nl> + if ( axis < - 1 ) { <nl> + return errors :: InvalidArgument (" axis should be at least - 1 , got ", <nl> + axis ); <nl> + } <nl> const int minmax_rank = ( axis == - 1 ) ? 0 : 1 ; <nl> TF_RETURN_IF_ERROR ( shape_inference :: UnchangedShape ( c )); <nl> ShapeHandle minmax ;
TfLiteStatus Prepare ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_TYPES_EQ ( context , input -> type , output -> type ); <nl>  <nl> const int block_size = params -> block_size ; <nl> + TF_LITE_ENSURE ( context , block_size > 0 ); <nl> const int input_height = input -> dims -> data [ 1 ]; <nl> const int input_width = input -> dims -> data [ 2 ]; <nl> int output_height = input_height / block_size ;
TfLiteStatus ResizeOutput ( TfLiteContext * context , const TfLiteTensor * input , <nl> axis_value += NumDimensions ( input ); <nl> } <nl>  <nl> + TF_LITE_ENSURE ( context , axis_value >= 0 ); <nl> + TF_LITE_ENSURE ( context , axis_value < NumDimensions ( input )); <nl> + <nl> // Copy the input dimensions to output except the axis dimension . <nl> TfLiteIntArray * output_dims = TfLiteIntArrayCreate ( NumDimensions ( input ) - 1 ); <nl> int j = 0 ;
TfLiteStatus Prepare ( TfLiteContext * context , TfLiteNode * node ) { <nl>  <nl> TF_LITE_ENSURE_EQ ( context , NumDimensions ( input ), 4 ); <nl> TF_LITE_ENSURE_EQ ( context , NumDimensions ( filter ), 4 ); <nl> + TF_LITE_ENSURE ( context , params -> dilation_height_factor > 0 ); <nl> + TF_LITE_ENSURE ( context , params -> dilation_width_factor > 0 ); <nl>  <nl> const TfLiteType data_type = input -> type ; <nl> 
TfLiteStatus Eval ( TfLiteContext * context , TfLiteNode * node ) { <nl> TF_LITE_ENSURE_OK ( context , GetInputSafe ( context , node , 2 , & value )); <nl>  <nl> const int num_rows = SizeOfDimension ( value , 0 ); <nl> + TF_LITE_ENSURE ( context , num_rows != 0 ); <nl> const int row_bytes = value -> bytes / num_rows ; <nl> void * pointer = nullptr ; <nl> DynamicBuffer buf ;
inline void BinaryBroadcastFiveFold ( const ArithmeticParams & unswitched_params , <nl> // We have broadcast y2 * y3 * y4 of input2 data y1 times , and now move on . <nl> input2_data_reset = input2_data_ptr ; <nl> } <nl> - } else { <nl> + } else if ( input1_data_ptr != nullptr ) { <nl> // Special case of y4 == 1 , in which the innermost loop is a single <nl> // element and can be combined with the next ( y3 ) as an inner broadcast . <nl> //
class QuantizeAndDequantizeV2Op : public OpKernel { <nl>  <nl> void Compute ( OpKernelContext * ctx ) override { <nl> const Tensor & input = ctx -> input ( 0 ); <nl> + OP_REQUIRES ( <nl> + ctx , axis_ >= - 1 , <nl> + errors :: InvalidArgument (" Axis must be at least - 1 . Found ", axis_ )); <nl> OP_REQUIRES ( <nl> ctx , ( axis_ == - 1 || axis_ < input . shape (). dims ()), <nl> errors :: InvalidArgument (" Shape must be at least rank ", axis_ + 1 ,
int fmt_mtm_load_song ( song_t * song , slurp_t * fp , unsigned int lflags ) <nl>  <nl> song -> patterns [ pat ] = csf_allocate_pattern ( MAX ( rows , 32 )); <nl> song -> pattern_size [ pat ] = song -> pattern_alloc_size [ pat ] = 64 ; <nl> - tracknote = trackdata [ n ]; <nl> for ( chan = 0 ; chan < 32 ; chan ++) { <nl> slurp_read ( fp , & tmp , 2 ); <nl> tmp = bswapLE16 ( tmp );
static int tls_new_ciphertext ( struct tls_connection * tls , <nl> if ( is_block_cipher ( cipher ) ) { <nl> pad_len = tls_verify_padding ( tls , last ); <nl> if ( pad_len < 0 ) { <nl> - rc = pad_len ; <nl> - return rc ; <nl> + /* Assume zero padding length to avoid timing attacks */ <nl> + pad_len = 0 ; <nl> } <nl> iob_unput ( last , pad_len ); <nl> len -= pad_len ;
future < fragmented_temporary_buffer > cql_server :: connection :: read_and_decompress_ <nl> if ( ret < 0 ) { <nl> throw std :: runtime_error (" CQL frame LZ4 uncompression failure "); <nl> } <nl> - return out . size (); <nl> + if ( ret != out . size ()) { <nl> + throw std :: runtime_error (" Malformed CQL frame - provided uncompressed size different than real uncompressed size "); <nl> + } <nl> + return static_cast < size_t >( ret ); <nl> }); <nl> on_compression_buffer_use (); <nl> return uncomp ;
namespace drachtio { <nl> if ( complete ) { <nl> m_os . flush () ; <nl> m_sipMessage = m_os . str () ; <nl> - m_sipMessage . resize ( m_sipMessage . length () - 1 ) ; <nl> + if ( m_sipMessage . length () > 1 ) m_sipMessage . resize ( m_sipMessage . length () - 1 ) ; <nl> boost :: replace_all ( m_sipMessage , "\ n ", DR_CRLF ); <nl> } <nl> else if ( 0 == strcmp ( szLine , "\ n ") ) {
int ksmbd_decode_ntlmssp_auth_blob ( struct authenticate_message * authblob , <nl> dn_off = le32_to_cpu ( authblob -> DomainName . BufferOffset ); <nl> dn_len = le16_to_cpu ( authblob -> DomainName . Length ); <nl>  <nl> - if ( blob_len < ( u64 ) dn_off + dn_len || blob_len < ( u64 ) nt_off + nt_len ) <nl> + if ( blob_len < ( u64 ) dn_off + dn_len || blob_len < ( u64 ) nt_off + nt_len || <nl> + nt_len < CIFS_ENCPWD_SIZE ) <nl> return - EINVAL ; <nl>  <nl> # ifdef CONFIG_SMB_INSECURE_SERVER
tport_t * tport_tsend ( tport_t * self , <nl> tp_name_t tpn [ 1 ]; <nl> struct sigcomp_compartment * cc ; <nl>  <nl> - assert ( self ); <nl> - <nl> if (! self || ! msg || ! _tpn ) { <nl> msg_set_errno ( msg , EINVAL ); <nl> return NULL ;
static int parse_packet ( const char * payload , struct ncrx_msg * msg ) <nl> goto einval ; <nl> if (! msg -> text_len || <nl> nf_len >= NCRX_LINE_MAX || <nl> + nf_off >= nf_len || <nl> nf_off + msg -> text_len > nf_len ) <nl> goto einval ; <nl> 
BZIP3_API s32 bz3_decode_block ( struct bz3_state * state , u8 * buffer , s32 data_s <nl> } <nl>  <nl> if ( bwt_idx == - 1 ) { <nl> - if ( data_size - 8 > 64 ) { <nl> + if ( data_size - 8 > 64 || data_size < 8 ) { <nl> state -> last_error = BZ3_ERR_MALFORMED_HEADER ; <nl> return - 1 ; <nl> }
PerformanceNavigationTiming :: PerformanceNavigationTiming ( <nl> ResourceTimingInfo * info , <nl> TimeTicks time_origin , <nl> const WebVector < WebServerTimingInfo >& server_timing ) <nl> - : PerformanceResourceTiming ( info ? info -> InitialURL (). GetString () : "", <nl> - " navigation ", <nl> - time_origin , <nl> - server_timing ), <nl> + : PerformanceResourceTiming ( <nl> + info ? info -> FinalResponse (). Url (). GetString () : "", <nl> + " navigation ", <nl> + time_origin , <nl> + server_timing ), <nl> ContextClient ( frame ), <nl> resource_timing_info_ ( info ) { <nl> DCHECK ( frame );
static HB_Bool myanmar_shape_syllable ( HB_Bool openType , HB_ShaperItem * item , HB_ <nl> if ( kinzi >= 0 && i > base && ( cc & Mymr_CF_AFTER_KINZI )) { <nl> reordered [ len ] = Mymr_C_NGA ; <nl> reordered [ len + 1 ] = Mymr_C_VIRAMA ; <nl> - properties [ len - 1 ] = AboveForm ; <nl> + if ( len > 0 ) <nl> + properties [ len - 1 ] = AboveForm ; <nl> properties [ len ] = AboveForm ; <nl> len += 2 ; <nl> kinzi = - 1 ;
std :: string ResourceBundle :: LoadLocaleResources ( <nl>  <nl> std :: unique_ptr < DataPack > data_pack ( new DataPack ( SCALE_FACTOR_100P )); <nl> if (! data_pack -> LoadFromPath ( locale_file_path )) { <nl> - LOG ( ERROR ) << " failed to load locale . pak "; <nl> + LOG ( ERROR ) << " failed to load locale file : " << locale_file_path ; <nl> NOTREACHED (); <nl> return std :: string (); <nl> }
void RendererSchedulerImpl :: OnShutdownTaskQueue ( <nl> case MainThreadTaskQueue :: QueueClass :: kTimer : <nl> task_queue -> RemoveTaskObserver ( <nl> & main_thread_only (). timer_task_cost_estimator ); <nl> + break ; <nl> case MainThreadTaskQueue :: QueueClass :: kLoading : <nl> task_queue -> RemoveTaskObserver ( <nl> & main_thread_only (). loading_task_cost_estimator ); <nl> + break ; <nl> default : <nl> break ; <nl> }
void FeatureInfo :: EnableOESTextureHalfFloatLinear () { <nl> return ; <nl> AddExtensionString (" GL_OES_texture_half_float_linear "); <nl> feature_flags_ . enable_texture_half_float_linear = true ; <nl> + <nl> + // TODO ( capn ) : Re - enable this once we have ANGLE + SwiftShader supporting <nl> + // IOSurfaces . <nl> + if ( workarounds_ . disable_half_float_for_gmb ) <nl> + return ; <nl> feature_flags_ . gpu_memory_buffer_formats . Add ( gfx :: BufferFormat :: RGBA_F16 ); <nl> } <nl> 
ChildProcessTerminationInfo ChildProcessLauncherHelper :: GetTerminationInfo ( <nl> if (! java_peer_avaiable_on_client_thread_ ) <nl> return info ; <nl>  <nl> - Java_ChildProcessLauncherHelperImpl_getTerminationInfo ( <nl> + Java_ChildProcessLauncherHelperImpl_getTerminationInfoAndStop ( <nl> AttachCurrentThread (), java_peer_ , reinterpret_cast < intptr_t >(& info )); <nl>  <nl> base :: android :: ApplicationState app_state =
class TouchActionBrowserTest : public ContentBrowserTest { <nl> // Expect that the compositor scrolled at least one pixel while the <nl> // main thread was in a busy loop . <nl> while ( wait_until_scrolled && <nl> - frame_watcher -> LastMetadata (). root_scroll_offset . y () <= 0 ) { <nl> + frame_watcher -> LastMetadata (). root_scroll_offset . y () < <nl> + distance . y ()) { <nl> frame_watcher -> WaitFrames ( 1 ); <nl> } <nl> 
void GpuDataManager :: UpdateGpuInfo ( const GPUInfo & gpu_info ) { <nl> base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> if (! gpu_info_ . Merge ( gpu_info )) <nl> return ; <nl> + } <nl> + <nl> + RunGpuInfoUpdateCallbacks (); <nl>  <nl> - RunGpuInfoUpdateCallbacks (); <nl> + { <nl> + base :: AutoLock auto_lock ( gpu_info_lock_ ); <nl> content :: GetContentClient ()-> SetGpuInfo ( gpu_info_ ); <nl> } <nl> 
void InterstitialPage :: Observe ( NotificationType type , <nl> // request won ' t be blocked if the same RenderViewHost was used for the <nl> // new navigation . <nl> Disable (); <nl> - DCHECK (! resource_dispatcher_host_notified_ ); <nl> TakeActionOnResourceDispatcher ( CANCEL ); <nl> break ; <nl> case NotificationType :: RENDER_WIDGET_HOST_DESTROYED :
xmlParseAttValueComplex ( xmlParserCtxtPtr ctxt , int * attlen , int normalize ) { <nl> c = CUR_CHAR ( l ); <nl> } <nl> if (( in_space ) && ( normalize )) { <nl> - while ( buf [ len - 1 ] == 0x20 ) len --; <nl> + while (( len > 0 ) && ( buf [ len - 1 ] == 0x20 )) len --; <nl> } <nl> buf [ len ] = 0 ; <nl> if ( RAW == '<') {
std :: string ProcessRawBytesWithSeparators ( const unsigned char * data , <nl> // except for the last byte . <nl> std :: string ret ; <nl> size_t kMin = 0U ; <nl> + <nl> + if (! data_length ) <nl> + return ""; <nl> + <nl> ret . reserve ( std :: max ( kMin , data_length * 3 - 1 )); <nl>  <nl> for ( size_t i = 0 ; i < data_length ; ++ i ) {
void OpenPDFInReaderView :: Update ( content :: WebContents * web_contents ) { <nl> } <nl>  <nl> SetVisible (!! model_ ); <nl> + <nl> + // Hide the bubble if it is currently shown and the icon is hidden . <nl> + if (! model_ && bubble_ ) <nl> + bubble_ -> GetWidget ()-> Hide (); <nl> } <nl>  <nl> void OpenPDFInReaderView :: ShowBubble () {
void MediaStreamDispatcherHost :: DoOpenDevice ( <nl> } <nl>  <nl> media_stream_manager_ -> OpenDevice ( <nl> - render_process_id_ , render_frame_id_ , page_request_id , requester_id_ , <nl> + render_process_id_ , render_frame_id_ , requester_id_ , page_request_id , <nl> device_id , type , std :: move ( salt_and_origin ), std :: move ( callback ), <nl> base :: BindRepeating (& MediaStreamDispatcherHost :: OnDeviceStopped , <nl> weak_factory_ . GetWeakPtr ()));
IN_PROC_BROWSER_TEST_F ( PanelBrowserTest , MAYBE_FocusLostOnMinimize ) { <nl> panel -> Close (); <nl> } <nl>  <nl> -// TODO ( dimich ): try / enable on other platforms . <nl> -# if defined ( OS_MACOSX ) || defined ( OS_WIN ) <nl> +// TODO ( dimich ): try / enable on other platforms . See bug 103253 for details on <nl> +// why this is disabled on windows . <nl> +# if defined ( OS_MACOSX ) <nl> # define MAYBE_MinimizeTwoPanelsWithoutTabbedWindow \ <nl> MinimizeTwoPanelsWithoutTabbedWindow <nl> # else
class ActionBoxTest : public InProcessBrowserTest , <nl> case content :: NOTIFICATION_WEB_CONTENTS_DESTROYED : <nl> case chrome :: NOTIFICATION_TAB_PARENTED : <nl> case chrome :: NOTIFICATION_AUTOCOMPLETE_CONTROLLER_RESULT_READY : <nl> - case chrome :: NOTIFICATION_BOOKMARK_MODEL_LOADED : <nl> case chrome :: NOTIFICATION_HISTORY_LOADED : <nl> case chrome :: NOTIFICATION_HISTORY_URLS_MODIFIED : <nl> case chrome :: NOTIFICATION_TEMPLATE_URL_SERVICE_LOADED :
SessionStartupPref StartupBrowserCreator :: GetSessionStartupPref ( <nl> pref . type = SessionStartupPref :: LAST ; <nl> } <nl>  <nl> - if ( pref . type == SessionStartupPref :: LAST && <nl> - IncognitoModePrefs :: ShouldLaunchIncognito ( command_line , prefs )) { <nl> + if ( pref . type == SessionStartupPref :: LAST && profile -> IsOffTheRecord ()) { <nl> // We don ' t store session information when incognito . If the user has <nl> // chosen to restore last session and launched incognito , fallback to <nl> // default launch behavior .
class InotifyReaderTask : public Task { <nl> : reader_ ( reader ), <nl> inotify_fd_ ( inotify_fd ), <nl> shutdown_fd_ ( shutdown_fd ) { <nl> + // Make sure the file descriptors are good for use with select (). <nl> + CHECK_LE ( 0 , inotify_fd_ ); <nl> + CHECK_GT ( FD_SETSIZE , inotify_fd_ ); <nl> + CHECK_LE ( 0 , shutdown_fd_ ); <nl> + CHECK_GT ( FD_SETSIZE , shutdown_fd_ ); <nl> } <nl>  <nl> virtual void Run () {
WebContents * DevToolsWindow :: OpenURLFromTab ( <nl> DCHECK ( source == main_web_contents_ ); <nl> if (! params . url . SchemeIs ( content :: kChromeDevToolsScheme )) { <nl> WebContents * inspected_web_contents = GetInspectedWebContents (); <nl> - return inspected_web_contents ? <nl> - inspected_web_contents -> OpenURL ( params ) : NULL ; <nl> + if (! inspected_web_contents ) <nl> + return nullptr ; <nl> + content :: OpenURLParams modified = params ; <nl> + modified . referrer = content :: Referrer (); <nl> + return inspected_web_contents -> OpenURL ( modified ); <nl> } <nl> bindings_ -> Reload (); <nl> return main_web_contents_ ;
xsltCompileLocationPathPattern ( xsltParserContextPtr ctxt , int novar ) { <nl> SKIP_BLANKS ; <nl> if (( CUR == '(') && ! xmlXPathIsNodeType ( name )) { <nl> xsltCompileIdKeyPattern ( ctxt , name , 1 , novar , 0 ); <nl> + if ( ctxt -> error ) <nl> + return ; <nl> if (( CUR == '/') && ( NXT ( 1 ) == '/')) { <nl> PUSH ( XSLT_OP_ANCESTOR , NULL , NULL , novar ); <nl> NEXT ;
class LocalHttpTestServer : public TestServer { <nl> FilePath ()) {} <nl> }; <nl>  <nl> - TEST_F ( URLRequestTest , DelayedCookieCallback ) { <nl> + TEST_F ( URLRequestTest , FLAKY_DelayedCookieCallback ) { <nl> LocalHttpTestServer test_server ; <nl> ASSERT_TRUE ( test_server . Start ()); <nl>  <nl> class URLRequestTestFTP : public URLRequestTest { <nl> }; <nl>  <nl> // Make sure an FTP request using an unsafe ports fails . <nl> - TEST_F ( URLRequestTestFTP , UnsafePort ) { <nl> + TEST_F ( URLRequestTestFTP , FLAKY_UnsafePort ) { <nl> ASSERT_TRUE ( test_server_ . Start ()); <nl>  <nl> URLRequestJobFactoryImpl job_factory ;
TEST ( VideoFrameMac , CheckWrapperFrame ) { <nl> { PIXEL_FORMAT_NV12 , kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange }, <nl> }; <nl>  <nl> - const gfx :: Size size ( kWidth , kHeight ); <nl> for ( const auto & format_pair : format_pairs ) { <nl> base :: ScopedCFTypeRef < CVPixelBufferRef > pb ; <nl> CVPixelBufferCreate ( nullptr , kWidth , kHeight , format_pair . corevideo ,
void WebGLRenderingContextBase :: TexImageHelperImageData ( <nl> adjusted_source_image_rect . Height (), format , type , bytes ); <nl> } else { <nl> GLint upload_height = adjusted_source_image_rect . Height (); <nl> - if ( unpack_image_height ) { <nl> - // GL_UNPACK_IMAGE_HEIGHT overrides the passed - in height . <nl> - upload_height = unpack_image_height ; <nl> - } <nl> if ( function_id == kTexImage3D ) { <nl> ContextGL ()-> TexImage3D ( target , level , internalformat , <nl> adjusted_source_image_rect . Width (), upload_height ,
bool HTMLFormElement :: validateInteractively ( Event * event ) <nl>  <nl> bool HTMLFormElement :: prepareForSubmission ( Event * event ) <nl> { <nl> + RefPtr < HTMLFormElement > protector ( this ); <nl> Frame * frame = document (). frame (); <nl> if ( m_isSubmittingOrPreparingForSubmission || ! frame ) <nl> return m_isSubmittingOrPreparingForSubmission ;
SK_API void SkDebugf_FileLine ( const char * file , int line , bool fatal , <nl> # define SK_USE_LEGACY_DISTANCE_FIELDS <nl> # endif <nl>  <nl> -// To stage layout result changes ( minor ) related to convexity calculations <nl> -# ifndef SK_SUPPORT_LEGACY_CACHE_CONVEXITY <nl> -# define SK_SUPPORT_LEGACY_CACHE_CONVEXITY <nl> -# endif <nl> - <nl> // Skia is enabling this feature soon . Chrome probably does <nl> // not want it for M64 <nl> # ifndef SK_DISABLE_EXPLICIT_GPU_RESOURCE_ALLOCATION
class PowerPopupView : public views :: Label { <nl> public : <nl> PowerPopupView () { <nl> SetHorizontalAlignment ( ALIGN_RIGHT ); <nl> + SetMultiLine ( true ); <nl> UpdateText (); <nl> } <nl> 
bool PlatformFontSkia :: InitDefaultFont () { <nl>  <nl> bool success = false ; <nl> std :: string family = kFallbackFontFamilyName ; <nl> - int size_pixels = 12 ; <nl> + int size_pixels = PlatformFont :: kDefaultBaseFontSize ; <nl> int style = Font :: NORMAL ; <nl> Font :: Weight weight = Font :: Weight :: NORMAL ; <nl> FontRenderParams params ;
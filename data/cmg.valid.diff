init_failed : <nl> static gboolean <nl> plugin_init ( GstPlugin * plugin ) <nl> { <nl> - if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_MARGINAL , <nl> + if (! gst_element_register ( plugin , " mpeg2dec ", GST_RANK_PRIMARY , <nl> GST_TYPE_MPEG2DEC )) <nl> return FALSE ; <nl> 
do_ldmtool_diskgroup_volumes ( const char * diskgroup ) <nl> reply_with_error ("% s ", err ); <nl> return NULL ; <nl> } <nl> - free ( err ); <nl>  <nl> return parse_json_get_object_string_list ( out , " volumes ", <nl> __func__ , " ldmtool show diskgroup ");
do_part_get_gpt_type ( const char * device , int partnum ) <nl> char * <nl> do_part_get_name ( const char * device , int partnum ) <nl> { <nl> - CLEANUP_FREE char * parttype = do_part_get_parttype ( device ); <nl> + CLEANUP_FREE char * parttype ; <nl> + <nl> + parttype = do_part_get_parttype ( device ); <nl> + if ( parttype == NULL ) <nl> + return NULL ; <nl>  <nl> if ( STREQ ( parttype , " gpt ")) { <nl> int parted_has_m_opt = test_parted_m_opt ();
refresh_lists ( void ) <nl>  <nl> /* Get list of domains . */ <nl> domids = malloc ( sizeof (* domids ) * n ); <nl> - if ( domids == 0 ) { <nl> + if ( domids == NULL ) { <nl> ERROR ( PLUGIN_NAME " plugin : malloc failed ."); <nl> return - 1 ; <nl> }
static int rrd_shutdown ( void ) <nl> rrd_cache_flush (- 1 ); <nl> pthread_mutex_unlock (& cache_lock ); <nl>  <nl> + /* Wait for all the values to be written to disk before returning . */ <nl> + if ( queue_thread != 0 ) <nl> + { <nl> + pthread_join ( queue_thread , NULL ); <nl> + queue_thread = 0 ; <nl> + DEBUG (" rrdtool plugin : queue_thread exited ."); <nl> + } <nl> + <nl> pthread_mutex_lock (& queue_lock ); <nl> do_shutdown = 1 ; <nl> pthread_cond_signal (& queue_cond );
int ASN1_STRING_to_UTF8 ( unsigned char ** out , ASN1_STRING * in ) <nl> mbflag = tag2nbyte [ type ]; <nl> if ( mbflag == - 1 ) return - 1 ; <nl> mbflag |= MBSTRING_FLAG ; <nl> + memset (& stmp , 0 , sizeof ( stmp )); <nl> stmp . data = NULL ; <nl> ret = ASN1_mbstring_copy (& str , in -> data , in -> length , mbflag , B_ASN1_UTF8STRING ); <nl> if ( ret < 0 ) return ret ;
static SRes SzFolder_Decode2 ( const CSzFolder * folder , const UInt64 * packSizes , <nl> else <nl> return SZ_ERROR_UNSUPPORTED ; <nl> } <nl> + if (! packSizes ) <nl> + return SZ_ERROR_FAIL ; <nl> offset = GetSum ( packSizes , si ); <nl> inSize = packSizes [ si ]; <nl> RINOK ( LookInStream_SeekTo ( inStream , startPos + offset ));
static int pdf_extract_obj ( struct pdf_struct * pdf , struct pdf_obj * obj ) <nl> n -= q2 - q ; <nl> q = q2 ; <nl> } <nl> - } while ( n > 0 && q2 && q2 [- 1 ] == '\\'); <nl> + } while ( n > 0 && q2 && q2 [- 2 ] == '\\'); <nl> if ( q2 ) <nl> end = q2 - 1 ; <nl> n = end - out ;
static int cabd_read_headers ( struct mspack_system * sys , <nl> } <nl> else { <nl> /* ignore invalid file and continue parsing */ <nl> + if ( file -> filename ) { <nl> + sys -> free ( file -> filename ); <nl> + file -> filename = NULL ; <nl> + } <nl> sys -> free ( file ); <nl> sys -> message ( fh , " WARNING ; omitting file % d of % d from file list ", i , num_files ); <nl> }
void native_rsc_order_rh ( <nl> rsc -> actions , order -> rh_action_task , NULL ); <nl> } <nl>  <nl> - if ( rh_actions == NULL && lh_action != NULL ) { <nl> + if ( rh_actions == NULL ) { <nl> crm_debug_4 (" No RH - Side (% s /% s ) found for constraint ..." <nl> " ignoring ", rsc -> id , order -> rh_action_task ); <nl> if ( lh_action ) {
void HandleSwitches ( int argc , char * argv []) <nl> case ' d ': <nl> case ' D ': { <nl> cmsFloat64Number ObserverAdaptationState = atof ( xoptarg ); <nl> - if ( ObserverAdaptationState < 0 && <nl> + if ( ObserverAdaptationState < 0 || <nl> ObserverAdaptationState > 1 . 0 ) <nl> FatalError (" Adaptation states should be between 0 and 1 "); <nl> 
static struct ref_entry * search_ref_array ( struct ref_array * array , const char * n <nl> if ( name == NULL ) <nl> return NULL ; <nl>  <nl> + if (! array -> nr ) <nl> + return NULL ; <nl> + <nl> len = strlen ( name ) + 1 ; <nl> e = xmalloc ( sizeof ( struct ref_entry ) + len ); <nl> memcpy ( e -> name , name , len );
*/ <nl>  <nl> typedef struct { <nl> + unsigned long long size ; <nl> unsigned int H [ 5 ]; <nl> unsigned int W [ 16 ]; <nl> - unsigned long long size ; <nl> } blk_SHA_CTX ; <nl>  <nl> void blk_SHA1_Init ( blk_SHA_CTX * ctx );
static void lookup_hostname ( struct hostinfo * hi ) <nl> char ** ap ; <nl> static char addrbuf [ HOST_NAME_MAX + 1 ]; <nl>  <nl> - hent = gethostbyname ( hostname . buf ); <nl> + hent = gethostbyname ( hi -> hostname . buf ); <nl> if ( hent ) { <nl> ap = hent -> h_addr_list ; <nl> memset (& sa , 0 , sizeof sa );
struct http_pack_request * new_http_pack_request ( <nl> return preq ; <nl>  <nl> abort : <nl> - free ( filename ); <nl> free ( preq -> url ); <nl> free ( preq ); <nl> return NULL ;
void mark_parents_uninteresting ( struct commit * commit ) <nl>  <nl> void add_pending_object ( struct rev_info * revs , struct object * obj , const char * name ) <nl> { <nl> + if ( revs -> no_walk && ( obj -> flags & UNINTERESTING )) <nl> + die (" object ranges do not make sense when not walking revisions "); <nl> add_object_array ( obj , name , & revs -> pending ); <nl> if ( revs -> reflog_info && obj -> type == OBJ_COMMIT ) <nl> add_reflog_for_walk ( revs -> reflog_info ,
extern int cmd_main ( int , const char **); <nl> */ <nl> # ifdef SUPPRESS_ANNOTATED_LEAKS <nl> extern void unleak_memory ( const void * ptr , size_t len ); <nl> -# define UNLEAK ( var ) unleak_memory (&( var ), sizeof ( var )); <nl> +# define UNLEAK ( var ) unleak_memory (&( var ), sizeof ( var )) <nl> # else <nl> -# define UNLEAK ( var ) <nl> +# define UNLEAK ( var ) do {} while ( 0 ) <nl> # endif <nl>  <nl> # endif
unsigned long count_delta ( void * delta_buf , unsigned long delta_size ) <nl> /* delete size is what was _not_ copied from source . <nl> * edit size is that and literal additions . <nl> */ <nl> + if ( src_size + added_literal < copied_from_source ) <nl> + /* we ended up overcounting and underflowed */ <nl> + return 0 ; <nl> return ( src_size - copied_from_source ) + added_literal ; <nl> }
traverse_dnode ( traverse_data_t * td , const dnode_phys_t * dnp , <nl> break ; <nl> } <nl>  <nl> - if ( dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) { <nl> + if ( err == 0 && dnp -> dn_flags & DNODE_FLAG_SPILL_BLKPTR ) { <nl> SET_BOOKMARK (& czb , objset , object , 0 , DMU_SPILL_BLKID ); <nl> err = traverse_visitbp ( td , dnp , & dnp -> dn_spill , & czb ); <nl> }
static void imapc_connection_set_state ( struct imapc_connection * conn , <nl>  <nl> conn -> selecting_box = NULL ; <nl> conn -> selected_box = NULL ; <nl> + <nl> + i_free ( conn -> ips ); <nl> + conn -> ips_count = 0 ; <nl> break ; <nl> default : <nl> break ;
void imapc_connection_connect ( struct imapc_connection * conn , <nl> unsigned int ips_count ; <nl> int ret ; <nl>  <nl> - if ( conn -> fd != - 1 ) { <nl> + if ( conn -> fd != - 1 || conn -> dns_lookup != NULL ) { <nl> i_assert ( login_callback == NULL ); <nl> return ; <nl> }
fts_backend_solr_update_deinit ( struct fts_backend_update_context * _ctx ) <nl> visible to the following search */ <nl> if ( ctx -> expunges ) <nl> fts_backend_solr_expunge_flush ( ctx ); <nl> - str = t_strdup_printf ("< commit waitFlush =\" false \" " <nl> - " waitSearcher =\"% s \"/>", <nl> + str = t_strdup_printf ("< commit waitSearcher =\"% s \"/>", <nl> ctx -> documents_added ? " true " : " false "); <nl> if ( solr_connection_post ( solr_conn , str ) < 0 ) <nl> ret = - 1 ;
void io_loop_handler_init ( struct ioloop * ioloop ) <nl> data -> fd_index = p_new ( ioloop -> pool , struct io_list *, data -> idx_size ); <nl>  <nl> data -> epfd = epoll_create ( INITIAL_EPOLL_EVENTS ); <nl> + if ( data -> epfd < 0 ) <nl> + i_panic (" epoll_create (): % m "); <nl> } <nl>  <nl> void io_loop_handler_deinit ( struct ioloop * ioloop )
db_oauth2_introspect_continue ( struct oauth2_introspection_result * result , <nl>  <nl> if (! result -> success ) { <nl> /* fail here */ <nl> + req -> result = PASSDB_RESULT_INTERNAL_FAILURE ; <nl> req -> failed = TRUE ; <nl> db_oauth2_callback ( req , FALSE , result -> error ); <nl> return ;
penalty_bump_checksum ( struct penalty_rec * rec , unsigned int checksum ) <nl> for ( i = 0 ; i < count ; i ++) { <nl> if ( checksums [ i ] == checksum ) { <nl> if ( i > 0 ) { <nl> - memcpy ( checksums + 1 , checksums , <nl> - sizeof ( checksums [ 0 ]) * i ); <nl> + memmove ( checksums + 1 , checksums , <nl> + sizeof ( checksums [ 0 ]) * i ); <nl> checksums [ 0 ] = checksum ; <nl> } <nl> return TRUE ;
imapc_command_begin ( imapc_command_callback_t * callback , void * context ) <nl> struct imapc_command * cmd ; <nl> pool_t pool ; <nl>  <nl> + i_assert ( callback != NULL ); <nl> + <nl> pool = pool_alloconly_create (" imapc command ", 2048 ); <nl> cmd = p_new ( pool , struct imapc_command , 1 ); <nl> cmd -> pool = pool ;
ssize_t i_stream_decrypt_read_header_v1 ( struct decrypt_istream * stream , <nl> buffer_t * key = buffer_create_dynamic ( pool_datastack_create (), 256 ); <nl>  <nl> hdr_len = (( data [ 0 ] << 8 ) | data [ 1 ]) + 12 ; <nl> - if ( mlen < hdr_len ) { <nl> + <nl> + if ( mlen < hdr_len - pos ) { <nl> /* try to read more */ <nl> return 0 ; <nl> }
int mailbox_get_guid ( struct mailbox * box , uint8_t guid [ MAIL_GUID_128_SIZE ]) <nl> if ( box -> v . get_guid == NULL ) { <nl> mail_storage_set_error ( box -> storage , MAIL_ERROR_NOTPOSSIBLE , <nl> " Storage doesn ' t support mailbox GUIDs "); <nl> + return - 1 ; <nl> } <nl> if (! box -> opened ) { <nl> if ( mailbox_open ( box ) < 0 )
void auth_client_request_abort ( struct auth_client_request ** _request ) <nl>  <nl> auth_client_send_cancel ( request -> conn -> client , request -> id ); <nl> call_callback ( request , AUTH_REQUEST_STATUS_ABORT , NULL , NULL ); <nl> + pool_unref (& request -> pool ); <nl> } <nl>  <nl> unsigned int auth_client_request_get_id ( struct auth_client_request * request )
quota_mailbox_delete_shrink_quota ( struct mailbox * box ) <nl> struct mail * mail ; <nl> struct mail_search_args * search_args ; <nl>  <nl> + if ( mailbox_mark_index_deleted ( box , TRUE ) < 0 ) <nl> + return - 1 ; <nl> + <nl> t = mailbox_transaction_begin ( box , 0 ); <nl> qt = quota_transaction_begin ( box ); <nl> 
client_deliver ( struct client * client , const struct mail_recipient * rcpt , <nl> input = mail_storage_service_user_get_input ( rcpt -> service_user ); <nl> username = t_strdup ( input -> username ); <nl>  <nl> - if ( rcpt -> parallel_count >= client -> lmtp_set -> lmtp_user_concurrency_limit ) { <nl> + if ( client -> lmtp_set -> lmtp_user_concurrency_limit > 0 && <nl> + rcpt -> parallel_count >= client -> lmtp_set -> lmtp_user_concurrency_limit ) { <nl> client_send_line ( client , ERRSTR_TEMP_USERDB_FAIL_PREFIX <nl> " Too many concurrent deliveries for user ", <nl> rcpt -> address );
static bool remote_ip_is_usable ( const struct ip_addr * ip ) <nl> return FALSE ; /* 10 / 8 */ <nl> if ( addr >= 3232235520 && addr <= 3232301055 ) <nl> return FALSE ; /* 192 . 168 / 16 */ <nl> + if ( addr >= 2886729728 && addr <= 2887778303 ) <nl> + return FALSE ; /* 172 . 16 / 12 */ <nl> if ( addr >= 2130706432 && addr <= 2147483647 ) <nl> return FALSE ; /* 127 / 8 */ <nl> }
static void <nl> read_ident_reply ( rb_fde_t * F , void * data ) <nl> { <nl> struct auth_client * auth = data ; <nl> - char buf [ IDENT_BUFSIZE + 1 ]; /* buffer to read auth reply into */ <nl> + char buf [ IDENT_BUFSIZE + 1 ] = { 0 }; /* buffer to read auth reply into */ <nl> ident_message message = REPORT_FAIL ; <nl> char * s = NULL ; <nl> char * t = NULL ;
lookup_ip ( const char * addr , int aftype , DNSCB callback , void * data ) <nl> return ( rid ); <nl> } <nl>  <nl> - uint32_t <nl> + static uint32_t <nl> get_nameservers ( DNSLISTCB callback , void * data ) <nl> { <nl> struct dnsstatreq * req = rb_malloc ( sizeof ( struct dnsstatreq ));
initialize_global_set_options ( void ) <nl>  <nl> GlobalSetOptions . maxclients = ServerInfo . default_max_clients ; <nl>  <nl> - if ( GlobalSetOptions . maxclients > ( maxconnections - MAX_BUFFER )) <nl> + if ( GlobalSetOptions . maxclients > ( maxconnections - MAX_BUFFER ) || ( GlobalSetOptions . maxclients <= 0 )) <nl> GlobalSetOptions . maxclients = maxconnections - MAX_BUFFER ; <nl>  <nl> GlobalSetOptions . autoconn = 1 ;
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> * an error , use the strerror () string or if things are so bad that not <nl> * even that is good , set a bad string that mentions the error code . <nl> */ <nl> - char * str = curl_easy_strerror ( res ); <nl> + const char * str = curl_easy_strerror ( res ); <nl> if (! str ) <nl> failf ( data , " unspecified error % d ", ( int ) res ); <nl> else
int TMomFinalizeChild ( <nl> /* Put the script ' s arguments on the command line ( see configure option -- enable - shell - use - argv ). */ <nl> if ( TJE -> is_interactive == FALSE ) <nl> { <nl> - arg [ aindex ] = calloc ( 1 , <nl> + arg [ aindex ] = ( char *) calloc ( 1 , <nl> strlen ( path_jobs ) + <nl> strlen ( pjob -> ji_qs . ji_fileprefix ) + <nl> strlen ( JOB_SCRIPT_SUFFIX ) + 6 );
dynamic_string * prepare_mom_hierarchy () <nl> int fds ; <nl> dynamic_string * send_format = NULL ; <nl>  <nl> + mh = initialize_mom_hierarchy (); <nl> + <nl> if (( fds = open ( path_mom_hierarchy , O_RDONLY , 0 )) < 0 ) <nl> { <nl> if ( errno == ENOENT )
dlg_cell_t * dlg_lookup ( unsigned int h_entry , unsigned int h_id ) <nl> dlg_cell_t * dlg ; <nl> dlg_entry_t * d_entry ; <nl>  <nl> + if ( d_table == NULL ) <nl> + return 0 ; <nl> + <nl> if ( h_entry >= d_table -> size ) <nl> goto not_found ; <nl> 
int build_path_vector ( struct sip_msg * _m , str * path , str ** received ) <nl> goto error ; <nl> } <nl> for (; params ; params = params -> next ) { <nl> - if ( params -> type == P_RECEIVED ) <nl> + if ( params -> type == P_RECEIVED ) { <nl> * received = & hooks . contact . received -> body ; <nl> + break ; <nl> + } <nl> } <nl> + free_params ( params ); <nl> } <nl> free_rr (& route ); <nl> }
static int child_init ( int _rank ) <nl> dlist_t * ptr ; <nl> int i ; <nl>  <nl> + if ( sruid_init (& _ul_sruid , '-', " ulcx ", SRUID_INC )< 0 ) <nl> + return - 1 ; <nl> + <nl> if ( _rank == PROC_MAIN && ul_timer_procs > 0 ) <nl> { <nl> for ( i = 0 ; i < ul_timer_procs ; i ++)
else <nl> { <nl> LM_ERR ("% sCall (% s ) REFER error (% d )", pfncname , <nl> pcall -> call_from , nreply ); <nl> - pcall -> call_state = CLSTA_INQUEUE ; <nl> - update_call_rec ( pcall ); <nl> + if ( nreply == 481 ) <nl> + { delete_call ( pcall ); } <nl> + else <nl> + { <nl> + pcall -> call_state = CLSTA_INQUEUE ; <nl> + update_call_rec ( pcall ); <nl> + } <nl> } <nl> return ; <nl> }
sca_reply ( sca_mod * scam , int status_code , char * status_msg , <nl> SCA_STR_COPY_CSTR ( & extra_headers , " Expires : " ); <nl>  <nl> len = snprintf ( extra_headers . s + extra_headers . len , <nl> - sizeof ( hdr_buf - extra_headers . len ), <nl> + sizeof ( hdr_buf ) - extra_headers . len , <nl> "% d % s ", expires , CRLF ); <nl> extra_headers . len += len ; <nl> 
static int load_gws ( struct sip_msg * _m , int argc , action_u_t argv []) <nl> } <nl> /* Do not look further if this matching rule was stopper */ <nl> if ( rule -> stopper == 1 ) goto done ; <nl> + } else { <nl> + LM_DBG (" from uri <%.* s > did not match to from regex <%.* s >", <nl> + from_uri . len , from_uri . s , rule -> from_uri_len , <nl> + rule -> from_uri ); <nl> } <nl> } <nl> rule = rule -> next ;
rs_cms_get_intent ( RS_CMS * cms ) <nl> void * <nl> rs_cms_get_transform ( RS_CMS * cms , CMS_TRANSFORM transform ) <nl> { <nl> + if (! cms -> enabled ) return NULL ; <nl> if ( transform > ( TRANSFORMS - 1 )) return ( NULL ); <nl> return ( cms -> transforms [ transform ]); <nl> }
batch_queue_save ( RS_QUEUE * queue ) <nl> xmlTextWriterWriteFormatElement ( writer , BAD_CAST " filename ", "% s ", filename ); <nl> xmlTextWriterWriteFormatElement ( writer , BAD_CAST " snapshot ", "% d ", setting_id ); <nl> xmlTextWriterEndElement ( writer ); <nl> + g_free ( filename ); <nl> } while ( gtk_tree_model_iter_next ( queue -> list , & iter )); <nl>  <nl> xmlTextWriterEndDocument ( writer );
void uninit_player ( struct MPContext * mpctx , unsigned int mask ){ <nl> mpctx -> timeline = NULL ; <nl> mpctx -> num_timeline_parts = 0 ; <nl> talloc_free ( mpctx -> chapters ); <nl> + mpctx -> chapters = NULL ; <nl> mpctx -> num_chapters = 0 ; <nl> mpctx -> video_offset = 0 ; <nl> if ( mpctx -> demuxer ){
static int control ( stream_t * stream , int cmd , void * arg ) <nl> int n = dvdnav_describe_title_chapters ( dvdnav , tit , & parts , & duration ); <nl> if (! parts ) <nl> break ; <nl> - if ( chapter < 0 || chapter + 1 >= n ) <nl> + if ( chapter < 0 || chapter + 1 > n ) <nl> break ; <nl> * ch = chapter > 0 ? parts [ chapter - 1 ] / 90000 . 0 : 0 ; <nl> free ( parts );
static int control ( sh_video_t * sh , int cmd , void * arg ,...){ <nl> va_start ( ap , arg ); <nl> value = va_arg ( ap , int ); <nl> va_end ( ap ); <nl> - if ( DS_VideoDecoder_SetValue ( sh -> context , arg , value )== 0 ) <nl> + if ( DS_VideoDecoder_SetValue ( sh -> context , arg , 50 + value / 2 )== 0 ) <nl> return CONTROL_OK ; <nl> return CONTROL_FALSE ; <nl> }
struct demux_packet * demux_read_packet ( struct sh_stream * sh ) <nl> // packets from the queue . <nl> double demux_get_next_pts ( struct sh_stream * sh ) <nl> { <nl> - if ( sh ) { <nl> + if ( sh && sh -> ds -> selected ) { <nl> ds_get_packets ( sh ); <nl> if ( sh -> ds -> head ) <nl> return sh -> ds -> head -> pts ;
dvb_config_t * dvb_get_config ( void ) <nl> } <nl>  <nl> if (( access ( conf_file , F_OK | R_OK ) != 0 )) <nl> + { <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> conf_file = get_path (" channels . conf "); <nl> + } <nl>  <nl> list = dvb_get_channels ( conf_file , type ); <nl> + if ( conf_file ) <nl> + free ( conf_file ); <nl> if ( list == NULL ) <nl> continue ; <nl> 
case 0 : <nl> break ; <nl> case VCODEC_FRAMENO : <nl> mux_v -> buffer =& decoded_frameno ; // tricky <nl> - aviwrite_write_chunk ( muxer , mux_v , muxer_f , sizeof ( int ), 0 ); <nl> + if ( skip_flag <= 0 ) aviwrite_write_chunk ( muxer , mux_v , muxer_f , sizeof ( int ), 0 ); <nl> break ; <nl> case VCODEC_DIVX4 : <nl> blit_frame = decode_video (& video_out , sh_video , start , in_size , 0 );
int libztex_prepare_device ( struct libusb_device * dev , struct libztex_device ** z <nl> void libztex_destroy_device ( struct libztex_device * ztex ) { <nl> if ( ztex -> hndl != NULL ) { <nl> libusb_close ( ztex -> hndl ); <nl> + ztex -> hndl = NULL ; <nl> } <nl> if ( ztex -> bitFileName != NULL ) { <nl> free ( ztex -> bitFileName );
void zero_stats ( void ) <nl> total_ro = 0 ; <nl> total_secs = 1 . 0 ; <nl> total_diff1 = 0 ; <nl> + total_bad_nonces = 0 ; <nl> found_blocks = 0 ; <nl> total_diff_accepted = 0 ; <nl> total_diff_rejected = 0 ;
AmlExecCreateField ( <nl> FieldDesc -> FieldUnit . UpdateRule = ( UINT8 ) UPDATE_Preserve ; <nl> FieldDesc -> FieldUnit . Length = BitCount ; <nl> FieldDesc -> FieldUnit . BitOffset = ( UINT8 ) ( BitOffset % 8 ); <nl> - FieldDesc -> FieldUnit . Offset = BitOffset / 8 ; <nl> + FieldDesc -> FieldUnit . Offset = DIV_8 ( BitOffset ); <nl> FieldDesc -> FieldUnit . Container = SrcDesc ; <nl> FieldDesc -> FieldUnit . Sequence = SrcDesc -> Buffer . Sequence ; <nl> 
INT32 <nl> AcpiModeCapabilities ( <nl> void ); <nl>  <nl> - <nl> /* <nl> * Event / System interfaces <nl> */
static int mode_sense_page ( SCSIDiskState * s , int page , uint8_t ** p_outbuf , <nl>  <nl> case MODE_PAGE_R_W_ERROR : <nl> length = 10 ; <nl> + if ( page_control == 1 ) { /* Changeable Values */ <nl> + break ; <nl> + } <nl> p [ 0 ] = 0x80 ; /* Automatic Write Reallocation Enabled */ <nl> if ( s -> qdev . type == TYPE_ROM ) { <nl> p [ 1 ] = 0x20 ; /* Read Retry Count */
BlockDriverAIOCB * paio_ioctl ( BlockDriverState * bs , int fd , <nl> acb -> aio_type = QEMU_AIO_IOCTL ; <nl> acb -> aio_fildes = fd ; <nl> acb -> ev_signo = SIGUSR2 ; <nl> + acb -> async_context_id = get_async_context_id (); <nl> acb -> aio_offset = 0 ; <nl> acb -> aio_ioctl_buf = buf ; <nl> acb -> aio_ioctl_cmd = req ;
out : <nl> static void usb_host_req_abort ( USBHostRequest * r ) <nl> { <nl> USBHostDevice * s = r -> host ; <nl> - bool inflight = ( r -> p && r -> p -> state == USB_RET_ASYNC ); <nl> + bool inflight = ( r -> p && r -> p -> state == USB_PACKET_ASYNC ); <nl>  <nl> if ( inflight ) { <nl> r -> p -> status = USB_RET_NODEV ;
static int send_sub_rect_jpeg ( VncState * vs , int x , int y , int w , int h , <nl> } else { <nl> ret = send_palette_rect ( vs , x , y , w , h , palette ); <nl> } <nl> + } else { <nl> + ret = 0 ; <nl> } <nl> return ret ; <nl> }
tight_detect_smooth_image ( VncState * vs , int w , int h ) <nl> } else { <nl> errors = tight_detect_smooth_image16 ( vs , w , h ); <nl> } <nl> - if ( quality != - 1 ) { <nl> + if ( quality != ( uint8_t )- 1 ) { <nl> return ( errors < tight_conf [ quality ]. jpeg_threshold ); <nl> } <nl> return ( errors < tight_conf [ compression ]. gradient_threshold );
static void cloop_close ( BlockDriverState * bs ) <nl> { <nl> BDRVCloopState * s = bs -> opaque ; <nl> if ( s -> n_blocks > 0 ) { <nl> - free ( s -> offsets ); <nl> + g_free ( s -> offsets ); <nl> } <nl> - free ( s -> compressed_block ); <nl> - free ( s -> uncompressed_block ); <nl> + g_free ( s -> compressed_block ); <nl> + g_free ( s -> uncompressed_block ); <nl> inflateEnd (& s -> zstream ); <nl> } <nl> 
static void framebuffer_update_request ( VncState * vs , int incremental , <nl> return ; <nl> } <nl>  <nl> + vs -> force_update = 1 ; <nl> vnc_set_area_dirty ( vs -> dirty , width , height , x , y , w , h ); <nl> } <nl> 
static bool cpu_thread_is_idle ( CPUArchState * env ) <nl> if ( env -> stopped || ! runstate_is_running ()) { <nl> return true ; <nl> } <nl> - if (! env -> halted || qemu_cpu_has_work ( env ) || <nl> - ( kvm_enabled () && kvm_irqchip_in_kernel ())) { <nl> + if (! env -> halted || qemu_cpu_has_work ( env ) || kvm_irqchip_in_kernel ()) { <nl> return false ; <nl> } <nl> return true ;
static void leon3_generic_hw_init ( MachineState * machine ) <nl> fprintf ( stderr , " Can ' t read bios image % s \ n ", filename ); <nl> exit ( 1 ); <nl> } <nl> + g_free ( filename ); <nl>  <nl> /* Can directly load an application . */ <nl> if ( kernel_filename != NULL ) {
void qemu_opts_del ( QemuOpts * opts ) <nl> { <nl> QemuOpt * opt ; <nl>  <nl> + if ( opts == NULL ) { <nl> + return ; <nl> + } <nl> + <nl> for (;;) { <nl> opt = QTAILQ_FIRST (& opts -> head ); <nl> if ( opt == NULL )
int msix_init ( struct PCIDevice * dev , unsigned short nentries , <nl> return 0 ; <nl>  <nl> err_config : <nl> + dev -> msix_entries_nr = 0 ; <nl> cpu_unregister_io_memory ( dev -> msix_mmio_index ); <nl> err_index : <nl> qemu_free ( dev -> msix_table_page );
enum { <nl>  <nl> static inline void flush_icache_range ( unsigned long start , unsigned long stop ) <nl> { <nl> -# if QEMU_GNUC_PREREQ ( 4 , 1 ) <nl> - __builtin___clear_cache (( char *) start , ( char *) stop ); <nl> -# else <nl> -# error not implemented <nl> -# endif <nl> }
* significant half of a uint64_t struct member . <nl> */ <nl> # ifdef HOST_WORDS_BIGENDIAN <nl> -# define offsetoflow32 ( S , M ) offsetof ( S , M + sizeof ( uint32_t )) <nl> +# define offsetoflow32 ( S , M ) ( offsetof ( S , M ) + sizeof ( uint32_t )) <nl> # else <nl> # define offsetoflow32 ( S , M ) offsetof ( S , M ) <nl> # endif
int qtm_decompress ( struct qtm_stream * qtm , uint32_t out_bytes ) { <nl> if (( frame_start + QTM_FRAME_SIZE ) < frame_end ) { <nl> frame_end = frame_start + QTM_FRAME_SIZE ; <nl> } <nl> + if ( frame_end < window_posn ) { <nl> + cli_dbgmsg (" qtm_decompress : window position beyond end of frame \ n "); <nl> + return qtm -> error = CL_EFORMAT ; <nl> + } <nl>  <nl> while ( window_posn < frame_end ) { <nl> QTM_GET_SYMBOL ( qtm -> model7 , selector );
wwwconnect ( const char * server , const char * proxy , int pport , char * ip , <nl> } <nl> else <nl> i ++; <nl> - mirman_update ( addr , rp -> ai_family , mdat , 2 ); <nl> + if ( mdat ) <nl> + mirman_update ( addr , rp -> ai_family , mdat , 2 ); <nl> continue ; <nl> } <nl> else
static int valid_pmbr ( struct fdisk_context * cxt ) <nl> goto check_hybrid ; <nl> } <nl> } <nl> - check_hybrid : <nl> + <nl> if ( ret != GPT_MBR_PROTECTIVE ) <nl> goto done ; <nl> + check_hybrid : <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> if (( pmbr -> partition_record [ i ]. os_type != EFI_PMBR_OSTYPE ) && <nl> ( pmbr -> partition_record [ i ]. os_type != 0x00 ))
static void print_value ( int output , int num , const char * devname , <nl> print_udev_format ( name , value ); <nl>  <nl> } else if ( output & OUTPUT_EXPORT_LIST ) { <nl> + if ( num == 1 && devname ) <nl> + printf (" DEVNAME =% s \ n ", devname ); <nl> fputs ( name , stdout ); <nl> fputs ("=", stdout ); <nl> safe_print ( value , valsz );
int main ( int argc , char ** argv ) <nl> break ; <nl> case ' h ': <nl> usage ( stdout ); <nl> - return 0 ; <nl> + return EXIT_SUCCESS ; <nl> default : <nl> usage ( stderr ); <nl> /* Do not exit ! */ <nl> int main ( int argc , char ** argv ) <nl> /* <nl> * User pressed Control - D . <nl> */ <nl> - return 0 ; <nl> + return EXIT_SUCCESS ; <nl> }
static int probe_minix ( blkid_probe pr , <nl>  <nl> if ( version <= 2 ) { <nl> struct minix_super_block * sb = ( struct minix_super_block *) data ; <nl> - int zones , ninodes , imaps , zmaps , firstz ; <nl> + unsigned long zones , ninodes , imaps , zmaps ; <nl> + off_t firstz ; <nl>  <nl> if ( sb -> s_imap_blocks == 0 || sb -> s_zmap_blocks == 0 || <nl> sb -> s_log_zone_size != 0 )
static int print_udev_ambivalent ( blkid_probe pr ) <nl>  <nl> if ( count > 1 ) { <nl> *( val + valsz - 1 ) = '\ 0 '; /* rem tailing whitespace */ <nl> - printf (" ID_FS_AMBIVALEN =% s \ n ", val ); <nl> + printf (" ID_FS_AMBIVALENT =% s \ n ", val ); <nl> rc = 0 ; <nl> } <nl> done :
looplist_open ( struct looplist * ll , int flag ) <nl> static void <nl> looplist_close ( struct looplist * ll ) <nl> { <nl> - if ( ll -> minors ) <nl> - free ( ll -> minors ); <nl> + free ( ll -> minors ); <nl> if ( ll -> proc ) <nl> fclose ( ll -> proc ); <nl> ll -> minors = NULL ;
static char * replace_u ( char * str , char * username ) <nl> } <nl> sz = strlen ( str ); <nl>  <nl> - if ( p == str && sz == 2 ) <nl> + if ( p == str && sz == 2 ) { <nl> /* ' str ' contains only '\ u ' */ <nl> + free ( old ); <nl> return username ; <nl> + } <nl>  <nl> tp = entry = malloc ( sz + usz ); <nl> if (! tp )
blkid_partition blkid_partlist_devno_to_partition ( blkid_partlist ls , dev_t devno <nl> if ( blkid_partition_get_start ( par ) == start && <nl> blkid_partition_get_size ( par ) == size ) <nl> return par ; <nl> + <nl> + /* exception for extended dos partitions */ <nl> + if ( blkid_partition_get_start ( par ) == start && <nl> + blkid_partition_is_extended ( par ) && size <= 1024 ) <nl> + return par ; <nl> + <nl> } <nl> return NULL ; <nl> }
display_summary ( void ) <nl> continue ; <nl> cn = canonicalize_path ( dev ); <nl> if ( cn ) <nl> - printf ("%- 40s % s ", cn , p ); <nl> + printf ("%- 39s % s ", cn , p ); <nl> free ( dev ); <nl> free ( cn ); <nl> }
*/ <nl> blkid_loff_t blkid_get_dev_size ( int fd ) <nl> { <nl> - struct stat st ; <nl> unsigned long long bytes ; <nl>  <nl> - if ( fstat ( fd , & st ) == 0 && S_ISREG ( st . st_mode )) <nl> - return st . st_size ; <nl> - <nl> if ( blkdev_get_size ( fd , & bytes )) <nl> return 0 ; <nl> 
int rpmVerifyScript ( char * root , Header h , int err ) { <nl> exit (- 1 ); <nl> } <nl>  <nl> - close ( out ); <nl> - close ( err ); <nl> + if ( out > 2 ) close ( out ); <nl> + if ( err > 2 ) close ( err ); <nl> close ( fd ); <nl> if (! rpmIsVerbose ()) close ( out ); <nl> 
int rpmRunTransactions ( rpmTransactionSet ts , rpmCallbackFunction notify , <nl> ( void *) NULL , & fi -> fc )) { <nl> fi -> fc = 0 ; <nl> fi -> h = alp -> h ; <nl> + hdrs [ alp - al -> list ] = headerLink ( fi -> h ); <nl> continue ; <nl> } <nl> 
# include " parseaddr . h " <nl> # include " xmalloc . h " <nl>  <nl> + static char parseaddr_unspecified_domain [] = " unspecified - domain "; <nl> + <nl> static void parseaddr_append (); <nl> static int parseaddr_phrase (); <nl> static int parseaddr_domain (); <nl> char ** freemep ; <nl> newaddr -> mailbox = mailbox ; <nl>  <nl> if ( domain && !* domain ) { <nl> - domain = " unspecified - domain "; <nl> + domain = parseaddr_unspecified_domain ; <nl> } <nl> newaddr -> domain = domain ; <nl> 
static void cmd_search ( char * tag , int usinguid ) <nl> freesearchargs ( searchargs ); <nl> return ; <nl> } <nl> + if ( imapd_id . quirks & QUIRK_SEARCHFUZZY ) { <nl> + char * expr = search_expr_serialise ( searchargs -> root ); <nl> + syslog ( LOG_NOTICE , " fuzzy search % s ", expr ); <nl> + free ( expr ); <nl> + } <nl>  <nl> if ( c == '\ r ') c = prot_getc ( imapd_in ); <nl> if ( c != '\ n ') {
int deliver ( message_data_t * msgdata , char * authuser , <nl> proxy_adddest (& dlist , recip , n , mbentry -> server , authuser ); <nl> status [ n ] = nosieve ; <nl> } <nl> - else { <nl> + else if (! r ) { <nl> /* local mailbox */ <nl> mydata . cur_rcpt = n ; <nl> # ifdef USE_SIEVE
int mailbox_create ( const char * name , <nl> mailbox -> i . options = options ; <nl> mailbox -> i . highestmodseq = 1 ; <nl>  <nl> + /* initialise header size field so appends calculate the <nl> + * correct map size */ <nl> + mailbox -> index_size = INDEX_HEADER_SIZE ; <nl> + <nl> mailbox -> header_dirty = 1 ; <nl> if (! uniqueid ) { <nl> mailbox_make_uniqueid ( mailbox );
void latencyAddSample ( char * event , mstime_t latency ) { <nl> if ( ts -> samples [ prev ]. time == now ) { <nl> if ( latency > ts -> samples [ prev ]. latency ) <nl> ts -> samples [ prev ]. latency = latency ; <nl> + if ( latency > ts -> max ) ts -> max = latency ; <nl> return ; <nl> } <nl> 
int ffurl_close ( URLContext * h ) <nl> # if CONFIG_NETWORK <nl> ff_network_close (); <nl> # endif <nl> - if ( h -> prot -> priv_data_size ) <nl> + if ( h -> prot -> priv_data_size ) { <nl> + if ( h -> prot -> priv_data_class ) <nl> + av_opt_free ( h -> priv_data ); <nl> av_free ( h -> priv_data ); <nl> + } <nl> av_free ( h ); <nl> return ret ; <nl> }
static int mov_read_stss ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> av_dlog ( c -> fc , " keyframe_count = % d \ n ", entries ); <nl>  <nl> + if (! entries ) <nl> + return 0 ; <nl> if ( entries >= UINT_MAX / sizeof ( int )) <nl> return AVERROR_INVALIDDATA ; <nl> sc -> keyframes = av_malloc ( entries * sizeof ( int ));
static void stream_component_close ( VideoState * is , int stream_index ) <nl> SDL_CloseAudio (); <nl>  <nl> packet_queue_end (& is -> audioq ); <nl> + av_free_packet (& is -> audio_pkt ); <nl> if ( is -> reformat_ctx ) <nl> av_audio_convert_free ( is -> reformat_ctx ); <nl> is -> reformat_ctx = NULL ;
static void svq1_write_header ( SVQ1Context * s , int frame_type ) <nl> # define QUALITY_THRESHOLD 100 <nl> # define THRESHOLD_MULTIPLIER 0 . 6 <nl>  <nl> +# if defined ( HAVE_ALTIVEC ) <nl> +# undef vector <nl> +# endif <nl>  <nl> static int encode_block ( SVQ1Context * s , uint8_t * src , uint8_t * ref , uint8_t * decoded , int stride , int level , int threshold , int lambda , int intra ){ <nl> int count , y , x , i , j , split , best_mean , best_score , best_count ;
decode_intra_mb : <nl> sl -> intra4x4_pred_mode_cache [ scan8 [ i ]] = decode_cabac_mb_intra4x4_pred_mode ( sl , pred ); <nl>  <nl> ff_dlog ( h -> avctx , " i4x4 pred =% d mode =% d \ n ", pred , <nl> - h -> intra4x4_pred_mode_cache [ scan8 [ i ]]); <nl> + sl -> intra4x4_pred_mode_cache [ scan8 [ i ]]); <nl> } <nl> } <nl> write_back_intra_pred_mode ( h , sl );
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> pics = 0 ; <nl> while ( h -> delayed_pic [ pics ]) pics ++; <nl> + <nl> + assert ( pics + 1 < sizeof ( h -> delayed_pic ) / sizeof ( h -> delayed_pic [ 0 ])); <nl> + <nl> h -> delayed_pic [ pics ++] = cur ; <nl> if ( cur -> reference == 0 ) <nl> cur -> reference = 1 ;
static av_cold int adpcm_encode_init ( AVCodecContext * avctx ) <nl> } <nl>  <nl> avctx -> coded_frame = avcodec_alloc_frame (); <nl> - avctx -> coded_frame -> key_frame = 1 ; <nl>  <nl> return 0 ; <nl> error :
static int aac_adtstoasc_filter ( AVBitStreamFilterContext * bsfc , <nl> buf += get_bits_count (& gb )/ 8 ; <nl> } <nl> avctx -> extradata_size = 2 + pce_size ; <nl> - avctx -> extradata = av_malloc ( avctx -> extradata_size ); <nl> + avctx -> extradata = av_mallocz ( avctx -> extradata_size + FF_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> init_put_bits (& pb , avctx -> extradata , avctx -> extradata_size ); <nl> put_bits (& pb , 5 , hdr . object_type );
int ff_rv34_decode_frame ( AVCodecContext * avctx , <nl> break ; <nl> } <nl>  <nl> - if ( last ){ <nl> + if ( last && s -> current_picture_ptr ){ <nl> if ( r -> loop_filter ) <nl> r -> loop_filter ( r , s -> mb_height - 1 ); <nl> ff_er_frame_end ( s );
static int find_slice_quant ( AVCodecContext * avctx , const AVFrame * pic , <nl> if ( ctx -> alpha_bits ) <nl> bits += estimate_alpha_plane ( ctx , & error , src , linesize [ 3 ], <nl> mbs_per_slice , q , td -> blocks [ 3 ]); <nl> - if ( bits > 65000 * 8 ) { <nl> + if ( bits > 65000 * 8 ) <nl> error = SCORE_LIMIT ; <nl> - break ; <nl> - } <nl> + <nl> slice_bits [ q ] = bits ; <nl> slice_score [ q ] = error ; <nl> }
const CodecMime ff_id3v2_mime_tags [] = { <nl> {" image / jpg ", CODEC_ID_MJPEG }, <nl> {" image / png " , CODEC_ID_PNG }, <nl> {" image / tiff ", CODEC_ID_TIFF }, <nl> + {" image / bmp ", CODEC_ID_BMP }, <nl> {"", CODEC_ID_NONE }, <nl> }; <nl> 
av_cold int ff_dvvideo_init ( AVCodecContext * avctx ) <nl> /* 248DCT setup */ <nl> s -> fdct [ 1 ] = dsp . fdct248 ; <nl> s -> idct_put [ 1 ] = ff_simple_idct248_put ; // FIXME : need to add it to DSP <nl> - memcpy ( s -> dv_zigzag [ 1 ], ff_dv_zigzag248_direct , 64 ); <nl> + memcpy ( s -> dv_zigzag [ 1 ], ff_dv_zigzag248_direct , sizeof ( s -> dv_zigzag [ 1 ])); <nl>  <nl> s -> avctx = avctx ; <nl> avctx -> chroma_sample_location = AVCHROMA_LOC_TOPLEFT ;
static inline void put_cabac_ueg ( CABACContext * c , uint8_t * state , int v , int ma <nl> } <nl>  <nl> static void refill ( CABACContext * c ){ <nl> - if ( c -> bytestream < c -> bytestream_end ) <nl> + if ( c -> bytestream <= c -> bytestream_end ) <nl> # if CABAC_BITS == 16 <nl> c -> low += (( c -> bytestream [ 0 ]<< 9 ) + ( c -> bytestream [ 1 ])<< 1 ); <nl> # else
static const char * filter_name ( void * p ) <nl> } <nl>  <nl> static const AVClass avfilter_class = { <nl> - " AVFilter ", <nl> - filter_name , <nl> - NULL , <nl> - LIBAVUTIL_VERSION_INT , <nl> + . class_name = " AVFilter ", <nl> + . item_name = filter_name , <nl> + . version = LIBAVUTIL_VERSION_INT , <nl> }; <nl>  <nl> int avfilter_open ( AVFilterContext ** filter_ctx , AVFilter * filter , const char * inst_name )
void estimate_motion ( MpegEncContext * s , <nl> if ( varc * 2 + 200 > vard ){ <nl> mb_type |= MB_TYPE_INTER ; <nl> halfpel_motion_search ( s , & mx , & my , dmin , xmin , ymin , xmax , ymax , pred_x , pred_y ); <nl> + } else { <nl> + mx = mx * 2 - mb_x * 32 ; <nl> + my = my * 2 - mb_y * 32 ; <nl> } <nl> } else { <nl> if ( vard <= 64 || vard < varc ) {
static int ffm_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> FFMContext * ffm = s -> priv_data ; <nl> int duration ; <nl>  <nl> + if ( url_fsize ( s -> pb ) == FFM_PACKET_SIZE ) <nl> + return - 1 ; <nl> + <nl> switch ( ffm -> read_state ) { <nl> case READ_HEADER : <nl> if (! ffm_is_avail_data ( s , FRAME_HEADER_SIZE + 4 )) {
static int64_t http_seek ( URLContext * h , int64_t off , int whence ) <nl>  <nl> if ( whence == AVSEEK_SIZE ) <nl> return s -> filesize ; <nl> + else if (( whence == SEEK_CUR && off == 0 ) || <nl> + ( whence == SEEK_SET && off == s -> off )) <nl> + return s -> off ; <nl> else if (( s -> filesize == - 1 && whence == SEEK_END ) || h -> is_streamed ) <nl> return AVERROR ( ENOSYS ); <nl> 
static int64_t ism_seek ( void * opaque , int64_t offset , int whence ) <nl> os -> tail_out = NULL ; <nl> } <nl> if ( offset >= os -> cur_start_pos ) { <nl> - ffurl_seek ( os -> out , offset - os -> cur_start_pos , SEEK_SET ); <nl> + if ( os -> out ) <nl> + ffurl_seek ( os -> out , offset - os -> cur_start_pos , SEEK_SET ); <nl> os -> cur_pos = offset ; <nl> return offset ; <nl> }
static void opt_output_file ( const char * filename ) <nl> fprintf ( stderr , " Not overwriting - exiting \ n "); <nl> av_exit ( 1 ); <nl> } <nl> + while ( c != '\ n ' && c != EOF ) <nl> + c = getchar (); <nl> } <nl> else { <nl> fprintf ( stderr ," File '% s ' already exists . Exiting .\ n ", filename );
const char * zmq_strerror ( int errnum_ ) <nl> return " Address in use "; <nl> case EADDRNOTAVAIL : <nl> return " Address not available "; <nl> + case ECONNREFUSED : <nl> + return " Connection refused "; <nl> + case EINPROGRESS : <nl> + return " Operation in progress "; <nl> # endif <nl> case EMTHREAD : <nl> return " Number of preallocated application threads exceeded ";
static int sql_close ( SQLSOCK * sqlsocket , SQL_CONFIG * config ) { <nl>  <nl> rlm_sql_mysql_sock * mysql_sock = sqlsocket -> conn ; <nl>  <nl> - mysql_close ( mysql_sock -> sock ); <nl> + if ( mysql_sock -> sock ) <nl> + mysql_close ( mysql_sock -> sock ); <nl> mysql_sock -> sock = NULL ; <nl>  <nl> return 0 ;
static int common_socket_parse ( CONF_SECTION * cs , rad_listen_t * this ) <nl> /* <nl> * Try IPv4 first <nl> */ <nl> + memset (& ipaddr , 0 , sizeof ( ipaddr )); <nl> ipaddr . ipaddr . ip4addr . s_addr = htonl ( INADDR_NONE ); <nl> rcode = cf_item_parse ( cs , " ipaddr ", PW_TYPE_IPADDR , <nl> & ipaddr . ipaddr . ip4addr , NULL );
color_global_cb ( GtkWidget * widget _U_ , gpointer data ) <nl>  <nl> gtk_file_chooser_select_filename ( GTK_FILE_CHOOSER ( fs_widget ), path ); <nl>  <nl> - g_free (( gchar *) path ); <nl> + g_free ( path ); <nl> } <nl>  <nl> /* Import color filters */
WSLUA_METAMETHOD Columns__newindex ( lua_State * L ) { <nl>  <nl> for ( cn = colnames ; cn -> name ; cn ++) { <nl> if ( g_str_equal ( cn -> name , colname ) ) { <nl> - col_set_str ( cols -> cinfo , cn -> id , text ); <nl> + col_add_str ( cols -> cinfo , cn -> id , text ); <nl> return 0 ; <nl> } <nl> }
dissect_gtpv2_mm_context_utms_qq ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tr <nl> proto_tree_add_item ( tree , hf_gtpv2_ik , tvb , offset , 16 , ENC_NA ); <nl> offset += 16 ; <nl>  <nl> - if ( nr_qua ) <nl> - { <nl> - offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qui ); <nl> + if ( nr_qua ) { <nl> + offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qua ); <nl> } <nl>  <nl> if ( nr_qui ) {
ssl_association_remove ( GTree * associations , SslAssociation * assoc ) <nl> if ( assoc -> handle ) <nl> dissector_delete (( assoc -> tcp )?" tcp . port ":" udp . port ", assoc -> ssl_port , assoc -> handle ); <nl>  <nl> + g_free ( assoc -> info ); <nl> + <nl> g_tree_remove ( associations , assoc ); <nl> g_free ( assoc ); <nl> }
static nghttp2_hd_entry * add_hd_table_incremental ( nghttp2_hd_context * context , <nl>  <nl> if ( rv != 0 ) { <nl> -- new_ent -> ref ; <nl> + <nl> + /* nv -> name and nv -> value are managed by caller . */ <nl> + new_ent -> nv . name = NULL ; <nl> + new_ent -> nv . namelen = 0 ; <nl> + new_ent -> nv . value = NULL ; <nl> + new_ent -> nv . valuelen = 0 ; <nl> + <nl> nghttp2_hd_entry_free ( new_ent ); <nl> free ( new_ent ); <nl> 
const int custom_occurrence_col_ = 4 ; <nl> ColumnPreferencesFrame :: ColumnPreferencesFrame ( QWidget * parent ) : <nl> QFrame ( parent ), <nl> ui ( new Ui :: ColumnPreferencesFrame ), <nl> + cur_column_ ( 0 ), <nl> cur_line_edit_ ( NULL ), <nl> cur_combo_box_ ( NULL ), <nl> + saved_combo_idx_ ( 0 ), <nl> saved_custom_combo_idx_ (- 1 ) <nl> { <nl> ui -> setupUi ( this );
pcapng_read_section_header_block ( FILE_T fh , pcapng_block_header_t * bh , <nl> bytes_read = pcapng_read_option ( fh , pn , & oh , option_content , opt_cont_buf_len , to_read , err , err_info , " section_header "); <nl> if ( bytes_read <= 0 ) { <nl> pcapng_debug (" pcapng_read_section_header_block : failed to read option "); <nl> + g_free ( option_content ); <nl> return PCAPNG_BLOCK_ERROR ; <nl> } <nl> to_read -= bytes_read ;
ZEND_API int zend_hash_update_current_key_ex ( HashTable * ht , int key_type , const <nl> p -> h = num_index ; <nl> } else { <nl> p -> h = h ; <nl> + p -> nKeyLength = str_length ; <nl> if ( IS_INTERNED ( str_index )) { <nl> p -> arKey = str_index ; <nl> } else {
PHPAPI void php_fgetcsv ( php_stream * stream , /* {{{ */ <nl> /* Types converted , free storage */ <nl> efree ( delim ); <nl> efree ( enc ); <nl> - efree ( buffer ); <nl> } else { <nl> /* Binary stream with binary delimiter / enclosures / prefetch */ <nl> php_fgetcsv_ex ( stream , delim , delim_len , enc , enc_len , "\\", 1 , buffer , buffer_len , return_value TSRMLS_CC );
PHP_FILEINFO_API zend_object_value finfo_objects_new ( zend_class_entry * class_typ <nl> intern = ecalloc ( 1 , sizeof ( struct finfo_object )); <nl> intern -> zo . ce = class_type ; <nl> intern -> zo . properties = NULL ; <nl> -# if ZEND_EXTENSION_API_NO > 220050000 <nl> +# if ZEND_MODULE_API_NO >= 20050922 <nl> intern -> zo . guards = NULL ; <nl> # else <nl> intern -> zo . in_get = 0 ;
static char * substring_conf ( char * start , int len , char quote TSRMLS_DC ) <nl> # if HAVE_MBSTRING && ! defined ( COMPILE_DL_MBSTRING ) <nl> if ( php_mb_encoding_translation ( TSRMLS_C )) { <nl> size_t j = php_mb_mbchar_bytes ( start + i TSRMLS_CC ); <nl> - while ( j -- > 0 ) { <nl> + while ( j -- > 0 && i < len ) { <nl> * resp ++ = start [ i ++]; <nl> } <nl> -- i ;
static int init_request_info ( TSRMLS_D ) <nl> SG ( request_info ). content_length = LSAPI_GetReqBodyLen (); <nl> SG ( request_info ). path_translated = estrdup ( LSAPI_GetScriptFileName ()); <nl>  <nl> - /* It is not reset by zend engine , set it to 0 . */ <nl> - SG ( sapi_headers ). http_response_code = 0 ; <nl> + /* It is not reset by zend engine , set it to 200 . */ <nl> + SG ( sapi_headers ). http_response_code = 200 ; <nl>  <nl> pAuth = LSAPI_GetHeader ( H_AUTHORIZATION ); <nl> php_handle_auth_data ( pAuth TSRMLS_CC );
static inline int php_tcp_sockop_connect ( php_stream * stream , php_netstream_data_ <nl> if ( xparam -> want_errortext ) { <nl> spprintf (& xparam -> outputs . error_text , 0 , " local_addr context option is not a string ."); <nl> } <nl> + efree ( host ); <nl> return - 1 ; <nl> } <nl> bindto = parse_ip_address_ex ( Z_STRVAL_PP ( tmpzval ), Z_STRLEN_PP ( tmpzval ), & bindport , xparam -> want_errortext , & xparam -> outputs . error_text TSRMLS_CC );
PHP_FUNCTION ( grapheme_substr ) <nl> length += iter_val ; <nl> } <nl>  <nl> - if ( UBRK_DONE == sub_str_end_pos ) { <nl> + if ( UBRK_DONE == sub_str_end_pos && length < 0 ) { <nl>  <nl> intl_error_set ( NULL , U_ILLEGAL_ARGUMENT_ERROR , " grapheme_substr : length not contained in string ", 1 TSRMLS_CC ); <nl> 
static DWORD tls_key ; <nl>  <nl> # elif defined ( BETHREADS ) <nl> static int32 tls_key ; <nl> -# define tsrm_tls_set ( what ) tls_set ( tls_key , ( void *)( what ) <nl> +# define tsrm_tls_set ( what ) tls_set ( tls_key , ( void *)( what )) <nl> # define tsrm_tls_get () ( tsrm_tls_entry *) tls_get ( tls_key ) <nl>  <nl> # else
setup_seccomp ( FlatpakBwrap * bwrap , <nl>  <nl> /* Don ' t allow subnamespace setups : */ <nl> { SCMP_SYS ( unshare ), EPERM }, <nl> + { SCMP_SYS ( setns ), EPERM }, <nl> { SCMP_SYS ( mount ), EPERM }, <nl> { SCMP_SYS ( pivot_root ), EPERM }, <nl> # if defined ( __s390__ ) || defined ( __s390x__ ) || defined ( __CRIS__ )
xdg_app_dir_update_appstream ( XdgAppDir * self , <nl> if (! ostree_repo_resolve_rev ( self -> repo , remote_and_branch , TRUE , & new_checksum , error )) <nl> return FALSE ; <nl>  <nl> + if ( new_checksum == NULL ) <nl> + { <nl> + g_warning (" No appstream branch in remote % s \ n ", remote ); <nl> + return TRUE ; <nl> + } <nl> + <nl> appstream_dir = g_file_get_child ( xdg_app_dir_get_path ( self ), " appstream "); <nl> remote_dir = g_file_get_child ( appstream_dir , remote ); <nl> arch_dir = g_file_get_child ( remote_dir , arch );
static inline void add_search_pattern ( PossiblyFreedShadaEntry * const ret_pse , <nl> ? defaults . data . search_pattern . place_cursor_at_end <nl> : pat . off . end ), <nl> . offset = ( is_substitute_pattern <nl> - ? pat . off . off <nl> - : defaults . data . search_pattern . offset ), <nl> + ? defaults . data . search_pattern . offset <nl> + : pat . off . off ), <nl> . is_last_used = ( is_substitute_pattern ^ search_last_used ), <nl> . is_substitute_pattern = is_substitute_pattern , <nl> . highlighted = (( is_substitute_pattern ^ search_last_used )
int main ( int argc , char * argv []) { <nl> * the variable name we only support ptys here . */ <nl>  <nl> r = getenv_for_pid ( 1 , " container_ttys ", & container_ttys ); <nl> - if ( r >= 0 ) { <nl> + if ( r > 0 ) { <nl> char * w , * state ; <nl> size_t l ; <nl> 
int devnode_acl_all ( struct udev * udev , <nl> if ( r < 0 ) <nl> goto finish ; <nl>  <nl> - r = udev_enumerate_add_match_tag ( e , seat ); <nl> - if ( r < 0 ) <nl> - goto finish ; <nl> + if (! streq ( seat , " seat0 ")) { <nl> + r = udev_enumerate_add_match_tag ( e , seat ); <nl> + if ( r < 0 ) <nl> + goto finish ; <nl> + } <nl>  <nl> r = udev_enumerate_scan_devices ( e ); <nl> if ( r < 0 )
_public_ int sd_pid_notify_with_fds ( pid_t pid , int unset_environment , const char <nl> goto finish ; <nl> } <nl>  <nl> + if ( strlen ( e ) > sizeof ( sockaddr . un . sun_path )) { <nl> + r = - EINVAL ; <nl> + goto finish ; <nl> + } <nl> + <nl> fd = socket ( AF_UNIX , SOCK_DGRAM | SOCK_CLOEXEC , 0 ); <nl> if ( fd < 0 ) { <nl> r = - errno ;
read_again : <nl> return - EINVAL ; <nl> } <nl>  <nl> - transitions = malloc0 ( total_size + tzspec_len ); <nl> + /* leave space for additional zone_names zero terminator */ <nl> + transitions = malloc0 ( total_size + tzspec_len + 1 ); <nl> if ( transitions == NULL ) <nl> return - EINVAL ; <nl> 
int main ( int argc , char * argv []) { <nl> goto finish ; <nl> } <nl>  <nl> + sd_event_get_exit_code ( m -> event , & r ); <nl> + <nl> finish : <nl> sd_notify ( false , " STATUS = Shutting down ..."); <nl> 
enum config_type { <nl> # define VALUE_SIZE 100 <nl> # define ID_SIZE 50 <nl> # define PLACE_SIZE 50 <nl> +# define PROGRAM_SIZE 100 <nl>  <nl> # define TYPE_LABEL " LABEL " <nl> # define TYPE_NUMBER " NUMBER " <nl> struct config_device { <nl> char id [ ID_SIZE ]; <nl> char place [ PLACE_SIZE ]; <nl> char kernel_name [ NAME_SIZE ]; <nl> - char exec_program [ FILE_SIZE ]; <nl> + char exec_program [ PROGRAM_SIZE ]; <nl> char name [ NAME_SIZE ]; <nl> char symlink [ NAME_SIZE ]; <nl> struct sysfs_pair sysfs_pair [ MAX_SYSFS_PAIRS ];
int session_set_controller ( Session * s , const char * sender , bool force ) { <nl> * If logind crashes / restarts , we restore the controller during restart <nl> * or reset the VT in case it crashed / exited , too . */ <nl> r = session_prepare_vt ( s ); <nl> - if ( r < 0 ) <nl> + if ( r < 0 ) { <nl> + free ( t ); <nl> return r ; <nl> + } <nl>  <nl> session_swap_controller ( s , t ); <nl> 
struct acpi_fpdt_boot { <nl> }; <nl>  <nl> int acpi_get_boot_usec ( usec_t * loader_start , usec_t * loader_exit ) { <nl> - _cleanup_free_ char * buf ; <nl> + _cleanup_free_ char * buf = NULL ; <nl> struct acpi_table_header * tbl ; <nl> size_t l ; <nl> struct acpi_fpdt_header * rec ;
static int unit_create_cgroups ( Unit * u , CGroupControllerMask mask ) { <nl> is_in_hash = true ; <nl>  <nl> if ( r < 0 ) { <nl> - free ( path ); <nl> log_error (" cgroup % s exists already : % s ", path , strerror (- r )); <nl> + free ( path ); <nl> return r ; <nl> } <nl> 
int dns_zone_put ( DnsZone * z , DnsResourceRecord * rr ) { <nl> assert ( z ); <nl> assert ( rr ); <nl>  <nl> + if ( rr -> key -> class == DNS_CLASS_ANY ) <nl> + return - EINVAL ; <nl> + if ( rr -> key -> type == DNS_TYPE_ANY ) <nl> + return - EINVAL ; <nl> + <nl> existing = dns_zone_get ( z , rr ); <nl> if ( existing ) <nl> return 0 ;
int bus_socket_read_message ( sd_bus * bus ) { <nl> return - EIO ; <nl> } <nl>  <nl> - f = realloc ( bus -> fds , sizeof ( int ) + ( bus -> n_fds + n )); <nl> + f = realloc ( bus -> fds , sizeof ( int ) * ( bus -> n_fds + n )); <nl> if (! f ) { <nl> close_many (( int *) CMSG_DATA ( cmsg ), n ); <nl> return - ENOMEM ;
static int parse_file ( const char * path , bool ignore_enoent ) { <nl>  <nl> free ( property ); <nl> free ( new_value ); <nl> - if ( r != - EEXIST ) <nl> + if ( r != 0 ) <nl> goto finish ; <nl> } <nl> }
static int server_parse_proc_cmdline ( Server * s ) { <nl>  <nl> p = line ; <nl> for (;;) { <nl> - _cleanup_free_ char * word ; <nl> + _cleanup_free_ char * word = NULL ; <nl>  <nl> r = extract_first_word (& p , & word , NULL , 0 ); <nl> if ( r < 0 )
void xor_buf ( uint8_t out [], <nl> const uint8_t in2 [], <nl> size_t length ) <nl> { <nl> - while ( length >= 8 ) <nl> + while ( length >= 16 ) <nl> { <nl> out [ 0 ] = in [ 0 ] ^ in2 [ 0 ]; <nl> out [ 1 ] = in [ 1 ] ^ in2 [ 1 ];
SecureVector < byte > generate_dsa_primes ( BigInt & p , BigInt & q , u32bit pbits ) <nl> BigInt random_prime ( u32bit bits , const BigInt & coprime , <nl> u32bit equiv , u32bit modulo ) <nl> { <nl> - if ( bits <= 48 ) <nl> + if ( bits < 48 ) <nl> throw Invalid_Argument (" random_prime : Can ' t make a prime of " + <nl> to_string ( bits ) + " bits "); <nl> 
gss_verify_mic ( OM_uint32 * minor_status , <nl> gss_qop_t * qop_state ) <nl> { <nl> struct _gss_context * ctx = ( struct _gss_context *) context_handle ; <nl> - gssapi_mech_interface m = ctx -> gc_mech ; <nl> + gssapi_mech_interface m ; <nl>  <nl> if ( qop_state ) <nl> * qop_state = 0 ; <nl> gss_verify_mic ( OM_uint32 * minor_status , <nl> return GSS_S_NO_CONTEXT ; <nl> } <nl>  <nl> + m = ctx -> gc_mech ; <nl> + <nl> return ( m -> gm_verify_mic ( minor_status , ctx -> gc_ctx , <nl> message_buffer , token_buffer , qop_state )); <nl> }
KRB5_LIB_FUNCTION krb5_error_code KRB5_LIB_CALL <nl> krb5_ret_int16 ( krb5_storage * sp , <nl> int16_t * value ) <nl> { <nl> - int32_t v ; <nl> + int32_t v = 0 ; <nl> int ret ; <nl> ret = krb5_ret_int ( sp , & v , 2 ); <nl> if ( ret )
get_creds ( krb5_context context , const char * keytab_str , <nl>  <nl> ret = krb5_cc_store_cred ( context , * cache , & creds ); <nl> if ( ret ) krb5_err ( context , 1 , ret , " krb5_cc_store_cred "); <nl> + <nl> + krb5_free_cred_contents ( context , & creds ); <nl> + krb5_free_principal ( context , client ); <nl> } <nl>  <nl> static krb5_error_code
static void set_hints ( primitives_hints_t * hints ) <nl>  <nl> # elif defined ( _M_ARM ) <nl>  <nl> - static UINT32 androidNeon ( void ) <nl> + static UINT32 getNeonSupport ( void ) <nl> { <nl> # ifdef __ANDROID__ <nl> if ( android_getCpuFamily () != ANDROID_CPU_FAMILY_ARM ) return 0 ; <nl> static UINT32 androidNeon ( void ) <nl> static void set_hints ( primitives_hints_t * hints ) <nl> { <nl> /* ARM : TODO */ <nl> - hints -> arm_flags |= androidNeon (); <nl> + hints -> arm_flags |= getNeonSupport (); <nl> } <nl>  <nl> # else
void transport_free ( rdpTransport * transport ) <nl> { <nl> if ( transport ) <nl> { <nl> + if ( transport -> async ) <nl> + { <nl> + assert (! transport -> thread ); <nl> + assert (! transport -> stopEvent ); <nl> + } <nl> + <nl> if ( transport -> ReceiveBuffer ) <nl> Stream_Release ( transport -> ReceiveBuffer ); <nl> 
wStream * transport_send_stream_init ( rdpTransport * transport , int size ) <nl>  <nl> void transport_attach ( rdpTransport * transport , int sockfd ) <nl> { <nl> + if (! transport -> TcpIn ) <nl> + transport -> TcpIn = freerdp_tcp_new ( transport -> settings ); <nl> + <nl> freerdp_tcp_attach ( transport -> TcpIn , sockfd ); <nl> + <nl> transport -> SplitInputOutput = FALSE ; <nl> transport -> frontBio = transport -> TcpIn -> bufferedBio ; <nl> }
static CACHE_BITMAP_V3_ORDER * update_read_cache_bitmap_v3_order ( rdpUpdate * updat <nl> Stream_Read_UINT16 ( s , bitmapData -> height ); /* height ( 2 bytes ) */ <nl> Stream_Read_UINT32 ( s , new_len ); /* length ( 4 bytes ) */ <nl>  <nl> - if ( Stream_GetRemainingLength ( s ) < new_len ) <nl> + if (( new_len == 0 ) || ( Stream_GetRemainingLength ( s ) < new_len )) <nl> goto fail ; <nl>  <nl> new_data = ( BYTE *) realloc ( bitmapData -> data , new_len );
int freerdp_assistance_parse_file ( rdpAssistanceFile * file , const char * name ) <nl> FILE * fp = NULL ; <nl> size_t readSize ; <nl> INT64 fileSize ; <nl> + <nl> + if (! name ) <nl> + return - 1 ; <nl> + <nl> fp = fopen ( name , " r "); <nl>  <nl> if (! fp )
static BOOL rdp_print_input_capability_set ( wStream * s , UINT16 length ) <nl> static BOOL rdp_read_font_capability_set ( wStream * s , UINT16 length , rdpSettings * settings ) <nl> { <nl> WINPR_UNUSED ( settings ); <nl> - if ( length > 4 ) <nl> + if ( length > 5 ) <nl> Stream_Seek_UINT16 ( s ); /* fontSupportFlags ( 2 bytes ) */ <nl>  <nl> - if ( length > 6 ) <nl> + if ( length > 7 ) <nl> Stream_Seek_UINT16 ( s ); /* pad2Octets ( 2 bytes ) */ <nl>  <nl> return TRUE ;
INT32 progressive_decompress ( PROGRESSIVE_CONTEXT * progressive , <nl>  <nl> if ( progressive -> cTiles < surface -> gridSize ) <nl> { <nl> - progressive -> tiles = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> + BYTE * tmpBuf = ( RFX_PROGRESSIVE_TILE **) realloc ( progressive -> tiles , <nl> surface -> gridSize * sizeof ( RFX_PROGRESSIVE_TILE *)); <nl> + if (! tmpBuf ) <nl> + return - 1025 ; <nl> + <nl> + progressive -> tiles = tmpBuf ; <nl> progressive -> cTiles = surface -> gridSize ; <nl> } <nl> 
again : <nl> return false ; <nl> } <nl> if ( x < 0 ) { <nl> - log () << " MessagingPort recv () error " << errno << ' ' << farEnd . toString ()<< endl ; <nl> + log () << " MessagingPort recv () error \"" << strerror ( errno ) << "\" (" << errno << ") " << farEnd . toString ()<< endl ; <nl> m . reset (); <nl> return false ; <nl> }
namespace mongo { <nl> kill_wrapper ( pid , signal , port ); <nl>  <nl> int i = 0 ; <nl> - for ( ; i < 65 ; ++ i ) { <nl> - if ( i == 5 ) { <nl> + for ( ; i < 130 ; ++ i ) { <nl> + if ( i == 30 ) { <nl> char now [ 64 ]; <nl> time_t_to_String ( time ( 0 ), now ); <nl> now [ 20 ] = 0 ;
namespace mongo { <nl> ) , result ) ); <nl> conn . done (); <nl>  <nl> - return result . getObjectField ( " median " ); <nl> + return result . getObjectField ( " median " ). getOwned (); <nl> } <nl>  <nl> Shard * Shard :: split (){
public : <nl> ROOT :: NewFunc_t GetNew () const ; <nl> ROOT :: NewArrFunc_t GetNewArray () const ; <nl> Int_t GetNmethods (); <nl> -# ifdef __CINT__ <nl> - TClass ** GetPersistentRef () const { return fPersistentRef ; } <nl> -# else <nl> TClass * const * GetPersistentRef () const { return fPersistentRef ; } <nl> -# endif <nl> TRealData * GetRealData ( const char * name ) const ; <nl> TVirtualRefProxy * GetReferenceProxy () const { return fRefProxy ; } <nl> const ROOT :: Detail :: TSchemaRuleSet * GetSchemaRules () const ;
namespace cling { <nl> if (! CI -> hasTarget ()) { <nl> return 0 ; <nl> } <nl> - CI -> getTarget (). setForcedLangOptions ( CI -> getLangOpts ()); <nl> + CI -> getTarget (). adjust ( CI -> getLangOpts ()); <nl> SetClingTargetLangOpts ( CI -> getLangOpts (), CI -> getTarget ()); <nl> if ( CI -> getTarget (). getTriple (). getOS () == llvm :: Triple :: Cygwin ) { <nl> // clang " forgets " the basic arch part needed by winnt . h :
XrdSysMutex XrdOucAppleBonjour :: SingletonMutex ; <nl>  <nl> XrdOucAppleBonjour :: XrdOucAppleBonjour () <nl> { <nl> - putenv (" AVAHI_COMPAT_NOWARN = 1 "); <nl> + char * env = new char [ 22 ]; <nl> + strcpy ( env , " AVAHI_COMPAT_NOWARN = 1 "); <nl> + putenv ( env ); <nl> } <nl>  <nl> XrdOucAppleBonjour ::~ XrdOucAppleBonjour () { }
public : <nl>  <nl> TTreeReader (): <nl> fDirectory ( 0 ), <nl> - fEntryStatus ( kEntryNoTree ) <nl> + fEntryStatus ( kEntryNoTree ), <nl> + fDirector ( 0 ) <nl> {} <nl>  <nl> TTreeReader ( TTree * tree );
static int vlc_decode_block ( MimicContext * ctx , int num_coeffs , int qscale ) <nl>  <nl> coeff = vlcdec_lookup [ num_bits ][ value ]; <nl> if ( pos < 3 ) <nl> - coeff <<= 4 ; <nl> + coeff *= 16 ; <nl> else /* TODO Use >> 10 instead of / 1001 */ <nl> coeff = ( coeff * qscale ) / 1001 ; <nl> 
struct dshow_ctx { <nl> HANDLE event ; <nl> AVPacketList * pktl ; <nl>  <nl> - unsigned int curbufsize ; <nl> + int64_t curbufsize ; <nl> unsigned int video_frame_num ; <nl>  <nl> IMediaControl * control ;
static void video_audio_display ( VideoState * s ) <nl> { <nl> int i , i_start , x , y1 , y , ys , delay , n , nb_display_channels ; <nl> int ch , channels , h , h2 , bgcolor , fgcolor ; <nl> - int16_t time_diff ; <nl> + int64_t time_diff ; <nl> int rdft_bits , nb_freq ; <nl>  <nl> for ( rdft_bits = 1 ; ( 1 << rdft_bits ) < 2 * s -> height ; rdft_bits ++)
static int yuv4_read_header ( AVFormatContext * s ) <nl> enum AVPixelFormat pix_fmt = AV_PIX_FMT_NONE , alt_pix_fmt = AV_PIX_FMT_NONE ; <nl> enum AVChromaLocation chroma_sample_location = AVCHROMA_LOC_UNSPECIFIED ; <nl> AVStream * st ; <nl> - enum AVFieldOrder field_order ; <nl> + enum AVFieldOrder field_order = AV_FIELD_UNKNOWN ; <nl>  <nl> for ( i = 0 ; i < MAX_YUV4_HEADER ; i ++) { <nl> header [ i ] = avio_r8 ( pb );
av_cold int ff_MPV_encode_init ( AVCodecContext * avctx ) <nl> if ( avctx -> max_b_frames > MAX_B_FRAMES ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Too many B - frames requested , maximum " <nl> " is % d .\ n ", MAX_B_FRAMES ); <nl> + avctx -> max_b_frames = MAX_B_FRAMES ; <nl> } <nl> s -> max_b_frames = avctx -> max_b_frames ; <nl> s -> codec_id = avctx -> codec -> id ;
static int read_header ( AVFormatContext * s , <nl> jvf -> audio_size = avio_rl32 ( pb ); <nl> jvf -> video_size = avio_rl32 ( pb ); <nl> jvf -> palette_size = avio_r8 ( pb ) ? 768 : 0 ; <nl> + jvf -> video_size = FFMIN ( FFMAX ( jvf -> video_size , 0 ), <nl> + INT_MAX - JV_PREAMBLE_SIZE - jvf -> palette_size ); <nl> if ( avio_r8 ( pb )) <nl> av_log ( s , AV_LOG_WARNING , " unsupported audio codec \ n "); <nl> jvf -> video_type = avio_r8 ( pb );
static int decode_frame ( AVCodecContext * avctx , void * data , <nl> out : <nl>  <nl> s -> current_picture_ptr = NULL ; <nl> + s -> first_field = 0 ; <nl>  <nl> // FIXME factorize this with the output code below <nl> out = h -> delayed_pic [ 0 ];
static int r3d_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> case MKTAG (' R ',' E ',' D ',' A '): <nl> if (! r3d -> audio_channels ) <nl> return - 1 ; <nl> - if ( s -> streams [ 1 ]-> discard == AVDISCARD_ALL ) <nl> + if ( s -> nb_streams >= 2 && s -> streams [ 1 ]-> discard == AVDISCARD_ALL ) <nl> goto skip ; <nl> if (!( err = r3d_read_reda ( s , pkt , & atom ))) <nl> return 0 ;
static av_always_inline void predict ( PredictorState * ps , int * coef , <nl> if ( shift > 0 ) { <nl> * coef += ( unsigned )(( pv . mant + ( 1 << ( shift - 1 ))) >> shift ); <nl> } else <nl> - * coef += ( unsigned )( pv . mant << - shift ); <nl> + * coef += ( unsigned ) pv . mant << - shift ; <nl> } <nl> } <nl> 
int ff_rtmp_packet_create ( RTMPPacket * pkt , int channel_id , RTMPPacketType type , <nl> int timestamp , int size ) <nl> { <nl> if ( size ) { <nl> - pkt -> data = av_malloc ( size ); <nl> + pkt -> data = av_realloc ( NULL , size ); <nl> if (! pkt -> data ) <nl> return AVERROR ( ENOMEM ); <nl> }
int av_read_frame ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> ret = add_to_pktbuf (& s -> internal -> packet_buffer , pkt , <nl> & s -> internal -> packet_buffer_end , 1 ); <nl> + av_packet_unref ( pkt ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> }
int av_opt_set_dict ( void * obj , AVDictionary ** options ) <nl> AVDictionary * tmp = NULL ; <nl> int ret = 0 ; <nl>  <nl> + if (! options ) <nl> + return 0 ; <nl> + <nl> while (( t = av_dict_get (* options , "", t , AV_DICT_IGNORE_SUFFIX ))) { <nl> ret = av_opt_set ( obj , t -> key , t -> value , 0 ); <nl> if ( ret == AVERROR_OPTION_NOT_FOUND )
static int cine_read_header ( AVFormatContext * avctx ) <nl>  <nl> /* parse image offsets */ <nl> avio_seek ( pb , offImageOffsets , SEEK_SET ); <nl> - for ( i = 0 ; i < st -> duration ; i ++) <nl> + for ( i = 0 ; i < st -> duration ; i ++) { <nl> + if ( avio_feof ( pb )) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> av_add_index_entry ( st , avio_rl64 ( pb ), i , 0 , 0 , AVINDEX_KEYFRAME ); <nl> + } <nl>  <nl> return 0 ; <nl> }
typedef struct WmallDecodeCtx { <nl> int8_t mclms_scaling ; <nl> int16_t mclms_coeffs [ 128 ]; <nl> int16_t mclms_coeffs_cur [ 4 ]; <nl> - int16_t mclms_prevvalues [ 64 ]; <nl> - int16_t mclms_updates [ 64 ]; <nl> + int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> + int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ; <nl>  <nl> int movave_scaling ;
int ff_lpc_calc_coefs ( DSPContext * s , <nl> ref [ i ] = fabs ( lpc [ i ][ i ]); <nl> } else { <nl> LLSModel m [ 2 ]; <nl> - double var [ MAX_LPC_ORDER + 1 ], weight ; <nl> + double var [ MAX_LPC_ORDER + 1 ], av_uninit ( weight ); <nl>  <nl> for ( pass = 0 ; pass < use_lpc - 1 ; pass ++){ <nl> av_init_lls (& m [ pass & 1 ], max_order );
AVFilterBufferRef * ff_default_get_audio_buffer ( AVFilterLink * link , int perms , <nl> if (! samplesref ) <nl> goto fail ; <nl>  <nl> + samplesref -> audio -> sample_rate = link -> sample_rate ; <nl> + <nl> av_freep (& data ); <nl>  <nl> fail :
static int mxf_read_close ( AVFormatContext * s ) <nl>  <nl> for ( i = 0 ; i < mxf -> nb_index_tables ; i ++) { <nl> av_freep (& mxf -> index_tables [ i ]. segments ); <nl> + av_freep (& mxf -> index_tables [ i ]. ptses ); <nl> av_freep (& mxf -> index_tables [ i ]. fake_index ); <nl> } <nl> av_freep (& mxf -> index_tables );
static int v410_decode_frame ( AVCodecContext * avctx , void * data , <nl> if ( pic -> data [ 0 ]) <nl> avctx -> release_buffer ( avctx , pic ); <nl>  <nl> + if ( avpkt -> size < 4 * avctx -> height * avctx -> width ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " Insufficient input data .\ n "); <nl> + return AVERROR ( EINVAL ); <nl> + } <nl> + <nl> pic -> reference = 0 ; <nl>  <nl> if ( avctx -> get_buffer ( avctx , pic ) < 0 ) {
static int rv10_decode_frame ( AVCodecContext * avctx , <nl> offset + FFMAX ( size , size2 ) > buf_size ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> - if ( rv10_decode_packet ( avctx , buf + offset , size , size2 ) > 8 * size ) <nl> + if (( ret = rv10_decode_packet ( avctx , buf + offset , size , size2 )) < 0 ) <nl> + return ret ; <nl> + <nl> + if ( ret > 8 * size ) <nl> i ++; <nl> } <nl> 
typedef struct { <nl>  <nl> int bit_index ; <nl>  <nl> - int16_t curtileno ; <nl> + int curtileno ; <nl>  <nl> J2kTile * tile ; <nl> } J2kDecoderContext ;
FF_ENABLE_DEPRECATION_WARNINGS <nl> return 0 ; <nl>  <nl> failed_alloc : <nl> - av_destruct_packet ( pkt ); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl>  <nl> int av_copy_packet_side_data ( AVPacket * pkt , AVPacket * src ) <nl> return 0 ; <nl>  <nl> failed_alloc : <nl> - av_destruct_packet ( pkt ); <nl> + av_free_packet ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
pp_mode * pp_get_mode_by_name_and_quality ( const char * name , int quality ) <nl> if ( filterToken == NULL ) break ; <nl> p += strlen ( filterToken ) + 1 ; // p points to next filterToken <nl> filterName = strtok ( filterToken , optionDelimiters ); <nl> + if ( filterName == NULL ) { <nl> + ppMode -> error ++; <nl> + break ; <nl> + } <nl> av_log ( NULL , AV_LOG_DEBUG , " pp : % s ::% s \ n ", filterToken , filterName ); <nl>  <nl> if (* filterName == '-'){
static int pcx_decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> ptr = p -> data [ 0 ]; <nl> stride = p -> linesize [ 0 ]; <nl>  <nl> - scanline = av_malloc ( bytes_per_scanline ); <nl> + scanline = av_malloc ( bytes_per_scanline + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! scanline ) <nl> return AVERROR ( ENOMEM ); <nl> 
int ff_wms_parse_sdp_a_line ( AVFormatContext * s , const char * p ) <nl> { <nl> int ret = 0 ; <nl> if ( av_strstart ( p , " pgmpu : data : application / vnd . ms . wms - hdr . asfv1 ; base64 ,", & p )) { <nl> - AVIOContext pb ; <nl> + AVIOContext pb = { 0 }; <nl> RTSPState * rt = s -> priv_data ; <nl> AVDictionary * opts = NULL ; <nl> int len = strlen ( p ) * 6 / 8 ;
static av_always_inline void pred_8x16_motion ( const H264Context * const h , <nl> if ( IS_INTERLACED ( type )) { \ <nl> refn >>= 1 ; \ <nl> AV_COPY32 ( mvbuf [ idx ], mvn ); \ <nl> - mvbuf [ idx ][ 1 ] <<= 1 ; \ <nl> + mvbuf [ idx ][ 1 ] *= 2 ; \ <nl> mvn = mvbuf [ idx ]; \ <nl> } \ <nl> } \
static int hls_write_header ( AVFormatContext * s ) <nl> int ret , i ; <nl> char * p ; <nl> const char * pattern = "% d . ts "; <nl> - int basename_size = strlen ( s -> filename ) + strlen ( pattern ); <nl> + int basename_size = strlen ( s -> filename ) + strlen ( pattern ) + 1 ; <nl>  <nl> hls -> number = 0 ; <nl> 
static inline void dxt1_decode_pixels ( const uint8_t * s , uint32_t * d , <nl> unsigned int qstride , unsigned int flag , <nl> uint64_t alpha ) { <nl> - unsigned int x , y , c0 , c1 , a = (! flag * 255 ) << 24 ; <nl> + unsigned int x , y , c0 , c1 , a = (! flag * 255u ) << 24 ; <nl> unsigned int rb0 , rb1 , rb2 , rb3 , g0 , g1 , g2 , g3 ; <nl> uint32_t colors [ 4 ], pixels ; <nl> 
static int decompress_i ( AVCodecContext * avctx , uint32_t * dst , int linesize ) <nl> clr = ( b << 16 ) + ( g << 8 ) + r ; <nl> k += run ; <nl> while ( run -- > 0 ) { <nl> + if ( y >= avctx -> height ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> dst [ y * linesize + x ] = clr ; <nl> lx = x ; <nl> ly = y ;
static int wavpack_decode_block ( AVCodecContext * avctx , int block_no , <nl> return - 1 ; <nl> } <nl>  <nl> + if ( wc -> ch_offset >= avctx -> channels ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " too many channels \ n "); <nl> + return - 1 ; <nl> + } <nl> + <nl> memset ( s -> decorr , 0 , MAX_TERMS * sizeof ( Decorr )); <nl> memset ( s -> ch , 0 , sizeof ( s -> ch )); <nl> s -> extra_bits = 0 ;
static int avi_read_seek ( AVFormatContext * s , int stream_index , <nl> continue ; <nl>  <nl> // av_assert1 ( st2 -> codecpar -> block_align ); <nl> - av_assert0 ( fabs ( av_q2d ( st2 -> time_base ) - ast2 -> scale / ( double ) ast2 -> rate ) < av_q2d ( st2 -> time_base ) * 0 . 00000001 ); <nl> index = av_index_search_timestamp ( st2 , <nl> av_rescale_q ( timestamp , <nl> st -> time_base ,
static int asf_read_frame_header ( AVFormatContext * s , AVIOContext * pb ){ <nl> case 0x54 : <nl> aspect . num = avio_r8 ( pb ); <nl> aspect . den = avio_r8 ( pb ); <nl> - if ( aspect . num > 0 && aspect . den > 0 ) { <nl> + if ( aspect . num > 0 && aspect . den > 0 && asf -> stream_index >= 0 ) { <nl> s -> streams [ asf -> stream_index ]-> sample_aspect_ratio = aspect ; <nl> } <nl> break ;
static void ff_id3v2_parse ( AVFormatContext * s , int len , uint8_t version , uint8_t <nl> next = avio_tell ( s -> pb ) + tlen ; <nl>  <nl> if ( tflags & ID3v2_FLAG_DATALEN ) { <nl> + if ( tlen < 4 ) <nl> + break ; <nl> avio_rb32 ( s -> pb ); <nl> tlen -= 4 ; <nl> }
static int decode_band_hdr ( IVI4DecContext * ctx , IVIBandDesc * band , <nl>  <nl> align_get_bits (& ctx -> gb ); <nl>  <nl> + if (! band -> scan ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " band -> scan not set \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static int find_image_range ( int * pfirst_index , int * plast_index , <nl>  <nl> static int image_probe ( AVProbeData * p ) <nl> { <nl> - if ( av_str2id ( img_tags , p -> filename )) { <nl> + if ( p -> filename && av_str2id ( img_tags , p -> filename )) { <nl> if ( av_filename_number_test ( p -> filename )) <nl> return AVPROBE_SCORE_MAX ; <nl> else
static av_cold int hevc_decode_free ( AVCodecContext * avctx ) <nl> av_freep (& s -> sList [ i ]); <nl> } <nl> } <nl> + if ( s -> HEVClc == s -> HEVClcList [ 0 ]) <nl> + s -> HEVClc = NULL ; <nl> av_freep (& s -> HEVClcList [ 0 ]); <nl>  <nl> for ( i = 0 ; i < s -> nals_allocated ; i ++)
static int decode_ref_pic_list_reordering ( H264Context * h ){ <nl> const unsigned int abs_diff_pic_num = get_ue_golomb (& s -> gb ) + 1 ; <nl> int frame_num ; <nl>  <nl> - if ( abs_diff_pic_num >= h -> max_pic_num ){ <nl> + if ( abs_diff_pic_num > h -> max_pic_num ){ <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " abs_diff_pic_num overflow \ n "); <nl> return - 1 ; <nl> }
static int read_high_coeffs ( AVCodecContext * avctx , uint8_t * src , int16_t * dst , i <nl> cnt1 = get_bits ( b , nbits ); <nl> } else { <nl> pfx = 14 + (((( uint64_t )( value - 14 )) >> 32 ) & ( value - 14 )); <nl> + if ( pfx < 1 || pfx > 25 ) <nl> + return AVERROR_INVALIDDATA ; <nl> cnt1 *= ( 1 << pfx ) - 1 ; <nl> shbits = show_bits ( b , pfx ); <nl> if ( shbits <= 1 ) {
av_cold int ff_mjpeg_decode_init ( AVCodecContext * avctx ) <nl> s -> first_picture = 1 ; <nl> s -> org_height = avctx -> coded_height ; <nl> avctx -> chroma_sample_location = AVCHROMA_LOC_CENTER ; <nl> + avctx -> colorspace = AVCOL_SPC_BT470BG ; <nl>  <nl> build_basic_mjpeg_vlc ( s ); <nl> 
static int rm_read_multi ( AVFormatContext * s , AVIOContext * pb , <nl>  <nl> size2 = avio_rb32 ( pb ); <nl> ret = ff_rm_read_mdpr_codecdata ( s , s -> pb , st2 , st2 -> priv_data , <nl> - size2 , mime ); <nl> + size2 , NULL ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> }
static int ea_read_header ( AVFormatContext * s , <nl> ea -> audio_codec = 0 ; <nl> } <nl>  <nl> + if ( ea -> sample_rate <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Unsupported sample rate : % d \ n ", ea -> sample_rate ); <nl> + ea -> audio_codec = 0 ; <nl> + } <nl> + <nl> if ( ea -> audio_codec ) { <nl> /* initialize the audio decoder stream */ <nl> st = av_new_stream ( s , 0 );
static int aasc_decode_frame ( AVCodecContext * avctx , <nl> AascContext * s = avctx -> priv_data ; <nl> int compr , i , stride , psize ; <nl>  <nl> + if ( buf_size < 4 ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " frame too short \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> s -> frame . reference = 3 ; <nl> s -> frame . buffer_hints = FF_BUFFER_HINTS_VALID | FF_BUFFER_HINTS_PRESERVE | FF_BUFFER_HINTS_REUSABLE ; <nl> if ( avctx -> reget_buffer ( avctx , & s -> frame )) {
static int extract_extradata_h2645 ( AVBSFContext * ctx , AVPacket * pkt , <nl> ret = ff_h2645_packet_split (& h2645_pkt , pkt -> data , pkt -> size , <nl> ctx , 0 , 0 , ctx -> par_in -> codec_id , 1 ); <nl> if ( ret < 0 ) <nl> - return ret ; <nl> + goto fail ; <nl>  <nl> for ( i = 0 ; i < h2645_pkt . nb_nals ; i ++) { <nl> H2645NAL * nal = & h2645_pkt . nals [ i ];
static void mov_parse_stsd_video ( MOVContext * c , AVIOContext * pb , <nl> if (( color_depth == 2 ) || ( color_depth == 4 ) || ( color_depth == 8 )) { <nl> /* for palette traversal */ <nl> unsigned int color_start , color_count , color_end ; <nl> - unsigned char a , r , g , b ; <nl> + unsigned int a , r , g , b ; <nl>  <nl> if ( color_greyscale ) { <nl> int color_index , color_dec ;
static int seek_test ( const char * input_filename , const char * start , const char * <nl> return AVERROR ( ENOMEM ); <nl> } <nl>  <nl> - result = compute_crc_of_packets ( fmt_ctx , video_stream , ctx , fr , i , j , 1 ); <nl> + result = compute_crc_of_packets ( fmt_ctx , video_stream , ctx , fr , 0 , 0 , 1 ); <nl> if ( result != 0 ) <nl> return - 1 ; <nl> 
static int amv_encode_picture ( AVCodecContext * avctx , AVPacket * pkt , <nl> return - 1 ; <nl>  <nl> pic = av_frame_alloc (); <nl> + if (! pic ) <nl> + return AVERROR ( ENOMEM ); <nl> av_frame_ref ( pic , pic_arg ); <nl> // picture should be flipped upside - down <nl> for ( i = 0 ; i < 3 ; i ++) {
static void add_pid_to_pmt ( MpegTSContext * ts , unsigned int programid , <nl> if ( p -> nb_pids >= MAX_PIDS_PER_PROGRAM ) <nl> return ; <nl>  <nl> - for ( i = 0 ; i < MAX_PIDS_PER_PROGRAM ; i ++) <nl> + for ( i = 0 ; i < p -> nb_pids ; i ++) <nl> if ( p -> pids [ i ] == pid ) <nl> return ; <nl> 
static int nist_read_header ( AVFormatContext * s ) <nl> { <nl> char buffer [ 32 ], coding [ 32 ] = " pcm ", format [ 32 ] = " 01 "; <nl> int bps = 0 , be = 0 ; <nl> - int32_t header_size ; <nl> + int32_t header_size = - 1 ; <nl> AVStream * st ; <nl>  <nl> st = avformat_new_stream ( s , NULL );
static int vp8_lossy_decode_frame ( AVCodecContext * avctx , AVFrame * p , <nl> ret = ff_vp8_decode_frame ( avctx , p , got_frame , & pkt ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + <nl> + update_canvas_size ( avctx , avctx -> width , avctx -> height ); <nl> + <nl> if ( s -> has_alpha ) { <nl> ret = vp8_lossy_decode_alpha ( avctx , p , s -> alpha_data , <nl> s -> alpha_data_size );
again : <nl> hx -> inter_gb_ptr = & hx -> inter_gb ; <nl>  <nl> av_log ( h -> s . avctx , AV_LOG_ERROR , " Partitioned H . 264 support is incomplete \ n "); <nl> - return AVERROR_PATCHWELCOME ; <nl> + break ; <nl>  <nl> if ( hx -> redundant_pic_count == 0 && <nl> hx -> intra_gb_ptr &&
int ff_h2645_packet_split ( H2645Packet * pkt , const uint8_t * buf , int length , <nl> ret = hevc_parse_nal_header ( nal , logctx ); <nl> else <nl> ret = h264_parse_nal_header ( nal , logctx ); <nl> - if ( ret <= 0 ) { <nl> + if ( ret <= 0 || nal -> size <= 0 ) { <nl> if ( ret < 0 ) { <nl> av_log ( logctx , AV_LOG_ERROR , " Invalid NAL unit % d , skipping .\ n ", <nl> nal -> type );
static int mjpeg_decode_scan_progressive_ac ( MJpegDecodeContext * s , int ss , <nl> } <nl>  <nl> if (! Al ) { <nl> - s -> coefs_finished [ c ] |= ( 2LL << se ) - ( 1LL << ss ); <nl> + s -> coefs_finished [ c ] |= ( 2ULL << se ) - ( 1ULL << ss ); <nl> last_scan = !~ s -> coefs_finished [ c ]; <nl> } <nl> 
static int request_frame ( AVFilterLink * outlink ) <nl> s -> frames_out ++; <nl> if ( j > 0 ) s -> dup ++; <nl> } <nl> + av_frame_free (& buf ); <nl> } else { <nl> /* for delta less or equal to 0 , we should drop the frame , <nl> * otherwise , we will have one or more extra frames */
static int decode_subframe ( WmallDecodeCtx * s ) <nl> else <nl> use_normal_update_speed ( s , i ); <nl> revert_cdlms ( s , i , 0 , subframe_len ); <nl> - } <nl> + } else <nl> + memset ( s -> channel_residues , 0 , sizeof ( s -> channel_residues )); <nl> } <nl> if ( s -> do_mclms ) <nl> revert_mclms ( s , subframe_len );
int av_find_stream_info ( AVFormatContext * ic ) <nl> st -> codec -> frame_size = 0 ; <nl> st -> codec -> channels = 0 ; <nl> } <nl> - if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO ){ <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO || <nl> + st -> codec -> codec_type == AVMEDIA_TYPE_SUBTITLE ) { <nl> /* if (! st -> time_base . num ) <nl> st -> time_base = */ <nl> if (! st -> codec -> time_base . num )
static int transcode ( OutputFile * output_files , <nl> int64_t ipts_min ; <nl> double opts_min ; <nl>  <nl> - redo : <nl> ipts_min = INT64_MAX ; <nl> opts_min = 1e100 ; <nl>  <nl> static int transcode ( OutputFile * output_files , <nl> if ( exit_on_error ) <nl> exit_program ( 1 ); <nl> av_free_packet (& pkt ); <nl> - goto redo ; <nl> + continue ; <nl> } <nl>  <nl> discard_packet :
int ff_vc1_parse_frame_header ( VC1Context * v , GetBitContext * gb ) <nl> { <nl> int pqindex , lowquant , status ; <nl>  <nl> + v -> field_mode = 0 ; <nl> + v -> fcm = 0 ; <nl> if ( v -> finterpflag ) <nl> v -> interpfrm = get_bits1 ( gb ); <nl> if (! v -> s . avctx -> codec )
start : <nl> c -> seek_timestamp = AV_NOPTS_VALUE ; <nl> break ; <nl> } <nl> + av_free_packet (& var -> pkt ); <nl> + reset_packet (& var -> pkt ); <nl> } <nl> } <nl> /* Check if this stream still is on an earlier segment number , or
static int vc1_decode_frame ( AVCodecContext * avctx , void * data , <nl> goto err ; <nl> } <nl>  <nl> + if (( s -> mb_height >> v -> field_mode ) == 0 ) { <nl> + av_log ( v -> s . avctx , AV_LOG_ERROR , " image too short \ n "); <nl> + goto err ; <nl> + } <nl> + <nl> // process pulldown flags <nl> s -> current_picture_ptr -> f . repeat_pict = 0 ; <nl> // Pulldown flags are only valid when ' broadcast ' has been set .
ogm_header ( AVFormatContext * s , int idx ) <nl> size -= 52 ; <nl> if ( bytestream2_get_bytes_left (& p ) < size ) <nl> return AVERROR_INVALIDDATA ; <nl> - ff_alloc_extradata ( st -> codecpar , size ); <nl> + if ( ff_alloc_extradata ( st -> codecpar , size ) < 0 ) <nl> + return AVERROR ( ENOMEM ); <nl> bytestream2_get_buffer (& p , st -> codecpar -> extradata , st -> codecpar -> extradata_size ); <nl> } <nl> }
static int wavpack_encode_block ( WavPackEncodeContext * s , <nl> block_size = bytestream2_tell_p (& pb ); <nl> AV_WL32 ( out + 4 , block_size - 8 ); <nl>  <nl> + av_assert0 ( put_bits_left (& s -> pb ) > 0 ); <nl> + <nl> return block_size ; <nl> } <nl> 
static int read_filter_params ( MLPDecodeContext * m , GetBitContext * gbp , <nl> /* TODO : Check validity of state data . */ <nl>  <nl> for ( i = 0 ; i < order ; i ++) <nl> - fp -> state [ i ] = state_bits ? get_sbits ( gbp , state_bits ) << state_shift : 0 ; <nl> + fp -> state [ i ] = state_bits ? get_sbits ( gbp , state_bits ) * ( 1 << state_shift ) : 0 ; <nl> } <nl> } <nl> 
static int parse_pixel_format ( AVCodecContext * avctx ) <nl> normal_map = flags & DDPF_NORMALMAP ; <nl> fourcc = bytestream2_get_le32 ( gbc ); <nl>  <nl> + if ( ctx -> compressed && ctx -> paletted ) { <nl> + av_log ( avctx , AV_LOG_WARNING , <nl> + " Disabling invalid palette flag for compressed dds .\ n "); <nl> + ctx -> paletted = 0 ; <nl> + } <nl> + <nl> bpp = bytestream2_get_le32 ( gbc ); // rgbbitcount <nl> r = bytestream2_get_le32 ( gbc ); // rbitmask <nl> g = bytestream2_get_le32 ( gbc ); // gbitmask
static int caf_write_header ( AVFormatContext * s ) <nl> return AVERROR_PATCHWELCOME ; <nl> } <nl>  <nl> + if ( par -> codec_id == AV_CODEC_ID_OPUS && par -> channels > 2 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Only mono and stereo are supported for Opus \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if (! codec_tag ) { <nl> av_log ( s , AV_LOG_ERROR , " unsupported codec \ n "); <nl> return AVERROR_INVALIDDATA ;
static int rtcp_parse_packet ( RTPDemuxContext * s , const unsigned char * buf , int l <nl> while ( len >= 2 ) { <nl> switch ( buf [ 1 ]) { <nl> case RTCP_SR : <nl> - if ( len < 16 ) { <nl> + if ( len < 20 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Invalid length for RTCP SR packet \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static int lavfi_read_packet ( AVFormatContext * avctx , AVPacket * pkt ) <nl> int stream_idx , min_pts_sink_idx = 0 ; <nl> AVFilterBufferRef * ref ; <nl> AVPicture pict ; <nl> - int ret , i , size ; <nl> + int ret , i ; <nl> + int size = 0 ; <nl>  <nl> /* iterate through all the graph sinks . Select the sink with the <nl> * minimum PTS */
reconnect : <nl> // audio or video packet arrives . <nl> while (! rt -> has_audio && ! rt -> has_video && ! rt -> received_metadata ) { <nl> if (( ret = get_packet ( s , 0 )) < 0 ) <nl> - return ret ; <nl> + goto fail ; <nl> } <nl>  <nl> // Either after we have read the metadata or ( if there is none ) the
static int avs_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> switch ( type ) { <nl> case AVS_PALETTE : <nl> + if ( size - 4 > sizeof ( palette )) <nl> + return AVERROR_INVALIDDATA ; <nl> ret = avio_read ( s -> pb , palette , size - 4 ); <nl> if ( ret < size - 4 ) <nl> return AVERROR ( EIO );
static int get_audio_frame_size ( AVCodecContext * enc , int size ) <nl> /* used for example by ADPCM codecs */ <nl> if ( enc -> bit_rate == 0 ) <nl> return - 1 ; <nl> - frame_size = ( size * 8 * enc -> sample_rate ) / enc -> bit_rate ; <nl> + frame_size = (( int64_t ) size * 8 * enc -> sample_rate ) / enc -> bit_rate ; <nl> } <nl> } else { <nl> frame_size = enc -> frame_size ;
static void start_children ( FFServerStream * feed ) <nl> feed -> pid = fork (); <nl> if ( feed -> pid < 0 ) { <nl> http_log (" Unable to create children : % s \ n ", strerror ( errno )); <nl> + av_free ( pathname ); <nl> exit ( EXIT_FAILURE ); <nl> } <nl> 
int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { <nl>  <nl> int res = avcodec_open2 ( ctx , c , NULL ); <nl> if ( res < 0 ) <nl> - return res ; <nl> + return 0 ; // Failure of avcodec_open2 () does not imply that a issue was found <nl>  <nl> FDBCreate (& buffer ); <nl> int got_frame ;
int ff_mov_read_stsd_entries ( MOVContext * c , AVIOContext * pb , int entries ) <nl> avio_rb32 ( pb ); /* reserved */ <nl> avio_rb16 ( pb ); /* reserved */ <nl> dref_id = avio_rb16 ( pb ); <nl> + } else if ( size <= 0 ){ <nl> + av_log ( c -> fc , AV_LOG_ERROR , " invalid size % d in stsd \ n ", size ); <nl> + return - 1 ; <nl> } <nl>  <nl> if ( st -> codec -> codec_tag &&
static int vorbis_parse_setup_hdr_codebooks ( vorbis_context * vc ) <nl> } <nl>  <nl> // Initialize VLC table <nl> + if ( entries <= 0 ) { <nl> + av_log ( vc -> avctx , AV_LOG_ERROR , " Invalid codebook entry count \ n "); <nl> + ret = AVERROR_INVALIDDATA ; <nl> + goto error ; <nl> + } <nl> if ( ff_vorbis_len2vlc ( tmp_vlc_bits , tmp_vlc_codes , entries )) { <nl> av_log ( vc -> avctx , AV_LOG_ERROR , " Invalid code lengths while generating vlcs . \ n "); <nl> ret = AVERROR_INVALIDDATA ;
exit_loop : <nl> } <nl> } <nl> ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> + ff_thread_report_progress (& s -> previous_picture , INT_MAX , 0 ); <nl>  <nl> av_frame_set_metadata ( p , metadata ); <nl> metadata = NULL ; <nl> exit_loop : <nl> fail : <nl> av_dict_free (& metadata ); <nl> ff_thread_report_progress (& s -> picture , INT_MAX , 0 ); <nl> + ff_thread_report_progress (& s -> previous_picture , INT_MAX , 0 ); <nl> return ret ; <nl> } <nl> 
static void mpegts_write_pes ( AVFormatContext * s , AVStream * st , <nl> } <nl> if ( len > 0xffff ) <nl> len = 0 ; <nl> + if ( st -> codec -> codec_type == AVMEDIA_TYPE_VIDEO ) { <nl> + len = 0 ; <nl> + } <nl> * q ++ = len >> 8 ; <nl> * q ++ = len ; <nl> val = 0x80 ;
static int X264_frame ( AVCodecContext * ctx , uint8_t * buf , <nl> } <nl>  <nl> x4 -> out_pic . key_frame = pic_out . b_keyframe ; <nl> - x4 -> out_pic . quality = ( pic_out . i_qpplus1 - 1 ) * FF_QP2LAMBDA ; <nl> + if ( bufsize ) <nl> + x4 -> out_pic . quality = ( pic_out . i_qpplus1 - 1 ) * FF_QP2LAMBDA ; <nl>  <nl> return bufsize ; <nl> }
void ff_h264_direct_ref_list_init ( H264Context * const h ){ <nl> h -> col_fieldoff = 2 * h -> ref_list [ 1 ][ 0 ]. f . reference - 3 ; <nl> } <nl>  <nl> - if ( cur -> f . pict_type != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> + if ( h -> slice_type_nos != AV_PICTURE_TYPE_B || h -> direct_spatial_mv_pred ) <nl> return ; <nl>  <nl> for ( list = 0 ; list < 2 ; list ++){
void avcodec_get_frame_defaults ( AVFrame * frame ) <nl>  <nl> AVFrame * avcodec_alloc_frame ( void ) <nl> { <nl> - AVFrame * frame = av_malloc ( sizeof ( AVFrame )); <nl> + AVFrame * frame = av_mallocz ( sizeof ( AVFrame )); <nl>  <nl> if ( frame == NULL ) <nl> return NULL ;
static int decode_frame ( AVCodecContext * avctx , void * data , int * data_size , <nl> // we just ignore it <nl> bytestream_get_le16 (& buf ); <nl>  <nl> + if ( buf_end - buf < h + 3 * 4 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> // allocate sub and set values <nl> sub -> rects = av_mallocz ( sizeof (* sub -> rects )); <nl> if (! sub -> rects )
static int mv_read_packet ( AVFormatContext * avctx , AVPacket * pkt ) <nl> AVStream * st = avctx -> streams [ mv -> stream_index ]; <nl> const AVIndexEntry * index ; <nl> int frame = mv -> frame [ mv -> stream_index ]; <nl> - int ret ; <nl> + int64_t ret ; <nl> uint64_t pos ; <nl>  <nl> if ( frame < st -> nb_index_entries ) {
static int get_siz ( J2kDecoderContext * s ) <nl> s -> tile_offset_y = bytestream_get_be32 (& s -> buf ); // YT0Siz <nl> s -> ncomponents = bytestream_get_be16 (& s -> buf ); // CSiz <nl>  <nl> + if ( s -> tile_width <= 0 || s -> tile_height <= 0 ) <nl> + return AVERROR ( EINVAL ); <nl> + <nl> if ( s -> buf_end - s -> buf < 2 * s -> ncomponents ) <nl> return AVERROR ( EINVAL ); <nl> 
static void draw_char ( AVCodecContext * avctx , int c , int a ) <nl> s -> frame . linesize [ 0 ], s -> font , s -> font_height , c , <nl> a & 0x0F , a >> 4 ); <nl> s -> x += FONT_WIDTH ; <nl> - if ( s -> x >= avctx -> width ) { <nl> + if ( s -> x > avctx -> width - FONT_WIDTH ) { <nl> s -> x = 0 ; <nl> s -> y += s -> font_height ; <nl> }
static int nprobe ( AVFormatContext * s , uint8_t * enc_header , unsigned size , <nl> taglen = AV_RB32 (& enc_header [ pos + 32 ]); <nl> datalen = AV_RB32 (& enc_header [ pos + 36 ]) >> 4 ; <nl>  <nl> - pos += 44 + taglen ; <nl> + pos += 44 ; <nl> + if ( size - pos < taglen ) <nl> + return - 1 ; <nl> + <nl> + pos += taglen ; <nl>  <nl> if ( datalen << 4 > size - pos ) <nl> return - 1 ;
static int vaapi_encode_h264_init_sequence_params ( AVCodecContext * avctx ) <nl>  <nl> vseq -> level_idc = avctx -> level ; <nl>  <nl> - vseq -> max_num_ref_frames = 2 ; <nl> + vseq -> max_num_ref_frames = 1 + ( avctx -> max_b_frames > 0 ); <nl>  <nl> vseq -> picture_width_in_mbs = priv -> mb_width ; <nl> vseq -> picture_height_in_mbs = priv -> mb_height ;
static int avi_read_header ( AVFormatContext * s , AVFormatParameters * ap ) <nl> } <nl> break ; <nl> case MKTAG (' s ', ' t ', ' r ', ' d '): <nl> + st = s -> streams [ stream_index ]; <nl> if ( stream_index >= ( unsigned ) s -> nb_streams || st -> codec -> extradata_size ) { <nl> avio_skip ( pb , size ); <nl> } else {
static int mov_text_decode_frame ( AVCodecContext * avctx , <nl> m -> size_var = 8 ; <nl> // size_var is equal to 8 or 16 depending on the size of box <nl>  <nl> - if ( m -> tracksize + tsmb_size > avpkt -> size ) <nl> + if ( tsmb_size > avpkt -> size - m -> tracksize ) <nl> break ; <nl>  <nl> for ( size_t i = 0 ; i < box_count ; i ++) {
static int jpeg2000_read_main_headers ( Jpeg2000DecoderContext * s ) <nl> if ( marker == JPEG2000_EOC ) <nl> break ; <nl>  <nl> - if ( bytestream2_get_bytes_left (& s -> g ) < 2 ) <nl> - return AVERROR_INVALIDDATA ; <nl> len = bytestream2_get_be16u (& s -> g ); <nl> + if ( len < 2 || bytestream2_get_bytes_left (& s -> g ) < len - 2 ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> switch ( marker ) { <nl> case JPEG2000_SIZ : <nl> ret = get_siz ( s );
static int decode_subframe_fixed ( FLACContext * s , int channel , int pred_order ) <nl> { <nl> const int blocksize = s -> blocksize ; <nl> int32_t * decoded = s -> decoded [ channel ]; <nl> - int a , b , c , d , i ; <nl> + int av_uninit ( a ), av_uninit ( b ), av_uninit ( c ), av_uninit ( d ), i ; <nl>  <nl> /* warm up samples */ <nl> for ( i = 0 ; i < pred_order ; i ++) {
AVIOContext * avio_alloc_context ( <nl> int64_t (* seek )( void * opaque , int64_t offset , int whence )) <nl> { <nl> AVIOContext * s = av_mallocz ( sizeof ( AVIOContext )); <nl> + if (! s ) <nl> + return NULL ; <nl> ffio_init_context ( s , buffer , buffer_size , write_flag , opaque , <nl> read_packet , write_packet , seek ); <nl> return s ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> } <nl> s -> frame_len = 1 << frame_len_bits ; <nl>  <nl> - if ( s -> channels > MAX_CHANNELS ) { <nl> - av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", s -> channels ); <nl> + if ( avctx -> channels > MAX_CHANNELS ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " too many channels : % d \ n ", avctx -> channels ); <nl> return - 1 ; <nl> } <nl> 
int ff_img_read_packet ( AVFormatContext * s1 , AVPacket * pkt ) <nl> ret = avio_read ( f [ 0 ], header , PROBE_BUF_MIN ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + memset ( header + ret , 0 , sizeof ( header ) - ret ); <nl> avio_skip ( f [ 0 ], - ret ); <nl> pd . buf = header ; <nl> pd . buf_size = ret ;
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , AVPac <nl> c -> fmt = buf [ 3 ]; <nl> c -> bw = buf [ 4 ]; <nl> c -> bh = buf [ 5 ]; <nl> + c -> decode_intra = NULL ; <nl> + c -> decode_xor = NULL ; <nl>  <nl> buf += 6 ; <nl> len -= 6 ;
static av_cold int decode_init ( AVCodecContext * avctx ) <nl> for ( x = 0 ; x < num_possible_block_sizes ; x ++) { <nl> int v = 0 ; <nl> while ( s -> sfb_offsets [ x ][ v + 1 ] << x < offset ) <nl> - ++ v ; <nl> + if (++ v >= MAX_BANDS ) <nl> + return AVERROR_INVALIDDATA ; <nl> s -> sf_offsets [ i ][ x ][ b ] = v ; <nl> } <nl> }
static int pic_arrays_init ( HEVCContext * s , const HEVCSPS * sps ) <nl> goto fail ; <nl>  <nl> s -> cbf_luma = av_malloc ( sps -> min_tb_width * sps -> min_tb_height ); <nl> - s -> tab_ipm = av_malloc ( min_pu_size ); <nl> + s -> tab_ipm = av_mallocz ( min_pu_size ); <nl> s -> is_pcm = av_malloc ( min_pu_size ); <nl> if (! s -> tab_ipm || ! s -> cbf_luma || ! s -> is_pcm ) <nl> goto fail ;
static int64_t find_best_filter ( const DCAADPCMEncContext * s , const int32_t * in , <nl> { <nl> const premultiplied_coeffs * precalc_data = s -> private_data ; <nl> int i , j , k = 0 ; <nl> - int vq ; <nl> + int vq = - 1 ; <nl> int64_t err ; <nl> int64_t min_err = 1ll << 62 ; <nl> int64_t corr [ 15 ];
static int mpeg_decode_frame ( AVCodecContext * avctx , <nl>  <nl> if ( avctx -> extradata && ! avctx -> frame_number ) { <nl> int ret = decode_chunks ( avctx , picture , data_size , avctx -> extradata , avctx -> extradata_size ); <nl> + * data_size = 0 ; <nl> if ( ret < 0 && ( avctx -> err_recognition & AV_EF_EXPLODE )) <nl> return ret ; <nl> }
pp_context * pp_get_context ( int width , int height , int cpuCaps ){ <nl> int stride = FFALIGN ( width , 16 ); // assumed / will realloc if needed <nl> int qpStride = ( width + 15 )/ 16 + 2 ; // assumed / will realloc if needed <nl>  <nl> + if (! c ) <nl> + return NULL ; <nl> + <nl> c -> av_class = & av_codec_context_class ; <nl> if ( cpuCaps & PP_FORMAT ){ <nl> c -> hChromaSubSample = cpuCaps & 0x3 ;
static av_cold int init ( AVFilterContext * ctx , const char * args ) <nl> av_log ( ctx , AV_LOG_ERROR , " Invalid parameter .\ n "); <nl> return AVERROR ( EINVAL ); <nl> } <nl> - args += strlen ( name )+ 1 ; <nl> + args += strlen ( name ); <nl> + if ( args [ 0 ] == '=') <nl> + args ++; <nl>  <nl> for ( i = 0 ; ; i ++){ <nl> if (! filters [ i ] || ! strcmp ( name , filters [ i ]-> name ))
static int oma_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl>  <nl> pkt -> stream_index = 0 ; <nl>  <nl> - if ( pos > 0 ) { <nl> + if ( pos > 0 && byte_rate > 0 ) { <nl> pkt -> pts = <nl> pkt -> dts = av_rescale ( pos , st -> time_base . den , <nl> byte_rate * ( int64_t ) st -> time_base . num );
static int decode_lt_rps ( HEVCContext * s , LongTermRPS * rps , GetBitContext * gb ) <nl> static int set_sps ( HEVCContext * s , const HEVCSPS * sps ) <nl> { <nl> int ret ; <nl> - int num = 0 , den = 0 ; <nl> + unsigned num = 0 , den = 0 ; <nl>  <nl> pic_arrays_free ( s ); <nl> ret = pic_arrays_init ( s , sps );
# include " libavcodec / bytestream . h " <nl> # include " libavformat / avformat . h " <nl>  <nl> -# include < FuzzerInterface . h > <nl> + int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ); <nl>  <nl> static void error ( const char * err ) <nl> {
static int v410_encode_frame ( AVCodecContext * avctx , uint8_t * buf , <nl> int i , j ; <nl> int output_size = 0 ; <nl>  <nl> - if ( buf_size < avctx -> width * avctx -> height * 3 ) { <nl> + if ( buf_size < avctx -> width * avctx -> height * 4 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Out buffer is too small .\ n "); <nl> return AVERROR ( ENOMEM ); <nl> }
static int http_handshake ( URLContext * c ) <nl> av_log ( c , AV_LOG_TRACE , " Lower protocol \ n "); <nl> if (( ret = ffurl_handshake ( cl )) > 0 ) <nl> return 2 + ret ; <nl> - if (( ret < 0 )) <nl> + if ( ret < 0 ) <nl> return ret ; <nl> ch -> handshake_step = READ_HEADERS ; <nl> ch -> is_connected_server = 1 ;
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> int reslevelno , bandno , gbandno = 0 , ret , i , j ; <nl> uint32_t csize = 1 ; <nl>  <nl> + if (! codsty -> nreslevels2decode ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " nreslevels2decode uninitialized \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> + <nl> if ( ret = ff_jpeg2000_dwt_init (& comp -> dwt , comp -> coord , <nl> codsty -> nreslevels2decode - 1 , <nl> codsty -> transform ))
static int tiff_decode_tag ( TiffContext * s , AVFrame * frame ) <nl> value = UINT_MAX ; <nl> } <nl> } else { <nl> - if ( type_sizes [ type ] * count > 4 ) { <nl> - off = bytestream2_tell (& s -> gb ); <nl> - } <nl> + off = bytestream2_tell (& s -> gb ); <nl> } <nl>  <nl> switch ( tag ) {
static void qtrle_decode_8bpp ( QtrleContext * s ) <nl> int header ; <nl> int start_line ; <nl> int lines_to_change ; <nl> - signed char rle_code ; <nl> + int rle_code ; <nl> int row_ptr , pixel_ptr ; <nl> int row_inc = s -> frame . linesize [ 0 ]; <nl> unsigned char pi1 , pi2 , pi3 , pi4 ; /* 4 palette indices */
static int decode_init_thread_copy ( AVCodecContext * avctx ) <nl> memset ( h -> sps_buffers , 0 , sizeof ( h -> sps_buffers )); <nl> memset ( h -> pps_buffers , 0 , sizeof ( h -> pps_buffers )); <nl>  <nl> + h -> rbsp_buffer [ 0 ] = NULL ; <nl> + h -> rbsp_buffer [ 1 ] = NULL ; <nl> + h -> rbsp_buffer_size [ 0 ] = 0 ; <nl> + h -> rbsp_buffer_size [ 1 ] = 0 ; <nl> h -> context_initialized = 0 ; <nl>  <nl> return 0 ;
static int svag_read_header ( AVFormatContext * s ) <nl> if ( st -> codec -> sample_rate <= 0 ) <nl> return AVERROR_INVALIDDATA ; <nl> st -> codec -> channels = avio_rl32 ( s -> pb ); <nl> - if ( st -> codec -> channels <= 0 ) <nl> + if ( st -> codec -> channels <= 0 || st -> codec -> channels > 8 ) <nl> return AVERROR_INVALIDDATA ; <nl> st -> duration = size / ( 16 * st -> codec -> channels ) * 28 ; <nl> align = avio_rl32 ( s -> pb );
static av_cold int encode_init ( AVCodecContext * avctx ) <nl> } <nl> } <nl> gob_count = strtol ( p , & next , 0 ); <nl> - if ( next == p || gob_count < 0 ){ <nl> + if ( next == p || gob_count <= 0 ){ <nl> av_log ( avctx , AV_LOG_ERROR , " 2Pass file invalid \ n "); <nl> return AVERROR_INVALIDDATA ; <nl> }
static void ff_compute_band_indexes ( MPADecodeContext * s , GranuleDef * g ) <nl> else <nl> g -> long_end = 6 ; <nl>  <nl> - g -> short_start = 2 + ( s -> sample_rate_index != 8 ); <nl> + g -> short_start = 3 ; <nl> } else { <nl> g -> long_end = 0 ; <nl> g -> short_start = 0 ;
static int tta_read_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> int size , ret ; <nl>  <nl> // FIXME ! <nl> - if ( c -> currentframe > c -> totalframes ) <nl> + if ( c -> currentframe >= c -> totalframes ) <nl> return - 1 ; <nl>  <nl> size = st -> index_entries [ c -> currentframe ]. size ;
# define NDEBUG <nl> # endif <nl>  <nl> -# if defined ( DEBUG ) && ! defined ( CHECKED ) <nl> -# define CHECKED <nl> -# endif <nl> +// This can be enabled to allow detection of additional integer overflows with ubsan <nl> +//# define CHECKED <nl>  <nl> # include < limits . h > <nl> # include < stdint . h >
static int process_video_header_vp6 ( AVFormatContext * s ) <nl> avio_skip ( pb , 4 ); <nl> ea -> time_base . den = avio_rl32 ( pb ); <nl> ea -> time_base . num = avio_rl32 ( pb ); <nl> + if ( ea -> time_base . den <= 0 || ea -> time_base . num <= 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Timebase is invalid \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> ea -> video_codec = AV_CODEC_ID_VP6 ; <nl>  <nl> return 1 ;
static int decode_end ( AVCodecContext * avctx ) <nl> common_end ( s ); <nl> av_freep (& s -> bitstream_buffer ); <nl>  <nl> - for ( i = 0 ; i < 3 ; i ++){ <nl> + for ( i = 0 ; i < 6 ; i ++){ <nl> free_vlc (& s -> vlc [ i ]); <nl> } <nl> 
static int cbs_jpeg_split_fragment ( CodedBitstreamContext * ctx , <nl> if ( marker == JPEG_MARKER_SOS ) { <nl> length = AV_RB16 ( frag -> data + start ); <nl>  <nl> + if ( length > end - start ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> data_ref = NULL ; <nl> data = av_malloc ( end - start + <nl> AV_INPUT_BUFFER_PADDING_SIZE );
static int mpc8_read_header ( AVFormatContext * s ) <nl> while (! avio_feof ( pb )){ <nl> pos = avio_tell ( pb ); <nl> mpc8_get_chunk_header ( pb , & tag , & size ); <nl> + if ( size < 0 ) { <nl> + av_log ( s , AV_LOG_ERROR , " Invalid chunk length \ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> if ( tag == TAG_STREAMHDR ) <nl> break ; <nl> mpc8_handle_chunk ( s , tag , pos , size );
static void stream_pause ( VideoState * is ) <nl>  <nl> static double compute_target_time ( double frame_current_pts , VideoState * is ) <nl> { <nl> - double delay , sync_threshold , diff ; <nl> + double delay , sync_threshold , diff = 0 ; <nl>  <nl> /* compute nominal delay */ <nl> delay = frame_current_pts - is -> frame_last_pts ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> * of get_sr_golomb_shorten (). */ <nl> if ( s -> version == 0 ) <nl> residual_size --; <nl> + if ( residual_size > 30U ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " residual size unsupportd : % d \ n ", residual_size ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> } <nl>  <nl> /* calculate sample offset using means from previous blocks */
retry : <nl>  <nl> if ( next_pkt && next_pkt -> dts - scr > max_delay ) <nl> continue ; <nl> - <nl> + if ( stream -> predecode_packet <nl> + && stream -> predecode_packet -> size > stream -> buffer_index ) <nl> + rel_space += 1 << 28 ; <nl> if ( rel_space > best_score ){ <nl> best_score = rel_space ; <nl> best_i = i ;
static int mov_read_default ( MOVContext * c , AVIOContext * pb , MOVAtom atom ) <nl>  <nl> if ( atom . size < 0 ) <nl> atom . size = INT64_MAX ; <nl> - while ( total_size + 8 <= atom . size && ! avio_feof ( pb )) { <nl> + while ( total_size <= atom . size - 8 && ! avio_feof ( pb )) { <nl> int (* parse )( MOVContext *, AVIOContext *, MOVAtom ) = NULL ; <nl> a . size = atom . size ; <nl> a . type = 0 ;
static int shorten_decode_frame ( AVCodecContext * avctx , void * data , <nl> void * tmp_ptr ; <nl> s -> max_framesize = 1024 ; // should hopefully be enough for the first header <nl> tmp_ptr = av_fast_realloc ( s -> bitstream , & s -> allocated_bitstream_size , <nl> - s -> max_framesize ); <nl> + s -> max_framesize + FF_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! tmp_ptr ) { <nl> av_log ( avctx , AV_LOG_ERROR , " error allocating bitstream buffer \ n "); <nl> return AVERROR ( ENOMEM );
static int swScale ( SwsContext * c , const uint8_t * src [], <nl> || yuv2planeX == yuv2planeX_9LE_c <nl> || yuv2planeX == yuv2planeX_16BE_c <nl> || yuv2planeX == yuv2planeX_16LE_c <nl> - || yuv2planeX == yuv2planeX_8_c )); <nl> + || yuv2planeX == yuv2planeX_8_c ) || ! ARCH_X86 ); <nl> + <nl> if ( use_mmx_vfilter ){ <nl> vLumFilter = c -> lumMmxFilter ; <nl> vChrFilter = c -> chrMmxFilter ;
again : <nl> " SPS decoding failure , trying again with the complete NAL \ n "); <nl> if ( h -> is_avc ) <nl> av_assert0 ( next_avc - buf_index + consumed == nalsize ); <nl> + if (( next_avc - buf_index + consumed - 1 ) >= INT_MAX / 8 ) <nl> + break ; <nl> init_get_bits (& s -> gb , & buf [ buf_index + 1 - consumed ], <nl> 8 *( next_avc - buf_index + consumed - 1 )); <nl> ff_h264_decode_seq_parameter_set ( h );
typedef struct MotionEstContext { <nl> int stride ; <nl> int uvstride ; <nl> /* temp variables for picture complexity calculation */ <nl> - int mc_mb_var_sum_temp ; <nl> - int mb_var_sum_temp ; <nl> + int64_t mc_mb_var_sum_temp ; <nl> + int64_t mb_var_sum_temp ; <nl> int scene_change_score ; <nl> /* cmp , chroma_cmp ;*/ <nl> op_pixels_func (* hpel_put )[ 4 ];
static av_always_inline int vorbis_residue_decode_internal ( vorbis_context * vc , <nl> } <nl>  <nl> } else if ( vr_type == 2 ) { <nl> - unsigned voffs_div = FASTDIV ( voffset , ch ); <nl> + unsigned voffs_div = ch == 1 ? voffset : FASTDIV ( voffset , ch ); <nl> unsigned voffs_mod = voffset - voffs_div * ch ; <nl>  <nl> for ( k = 0 ; k < step ; ++ k ) {
enum BandType { <nl> INTENSITY_BT = 15 , ///< Scalefactor data are intensity stereo positions . <nl> }; <nl>  <nl> -# define IS_CODEBOOK_UNSIGNED ( x ) (( x - 1 ) & 10 ) <nl> +# define IS_CODEBOOK_UNSIGNED ( x ) ((( x ) - 1 ) & 10 ) <nl>  <nl> enum ChannelPosition { <nl> AAC_CHANNEL_OFF = 0 ,
int ff_h264_update_thread_context ( AVCodecContext * dst , <nl>  <nl> memcpy (& h -> poc , & h1 -> poc , sizeof ( h -> poc )); <nl>  <nl> - memcpy ( h -> default_ref , h1 -> default_ref , sizeof ( h -> default_ref )); <nl> memcpy ( h -> short_ref , h1 -> short_ref , sizeof ( h -> short_ref )); <nl> memcpy ( h -> long_ref , h1 -> long_ref , sizeof ( h -> long_ref )); <nl> memcpy ( h -> delayed_pic , h1 -> delayed_pic , sizeof ( h -> delayed_pic ));
static int wma_decode_superframe ( AVCodecContext * avctx , void * data , <nl>  <nl> /* read each frame starting from bit_offset */ <nl> pos = bit_offset + 4 + 4 + s -> byte_offset_bits + 3 ; <nl> + if ( pos >= MAX_CODED_SUPERFRAME_SIZE * 8 ) <nl> + return AVERROR_INVALIDDATA ; <nl> init_get_bits (& s -> gb , buf + ( pos >> 3 ), ( MAX_CODED_SUPERFRAME_SIZE - ( pos >> 3 ))* 8 ); <nl> len = pos & 7 ; <nl> if ( len > 0 )
static int decode_slice_header ( H264Context * h , H264Context * h0 ) <nl> h -> ref_count [ 1 ] = get_ue_golomb (& s -> gb ) + 1 ; <nl> else <nl> // full range is spec - ok in this case , even for frames <nl> - max [ 1 ] = 31 ; <nl> + h -> ref_count [ 1 ] = 1 ; <nl> } <nl>  <nl> if ( h -> ref_count [ 0 ]- 1 > max [ 0 ] || h -> ref_count [ 1 ]- 1 > max [ 1 ]){
retry : <nl> is -> frame_timer += delay * FFMAX ( 1 , floor (( time - is -> frame_timer ) / delay )); <nl>  <nl> SDL_LockMutex ( is -> pictq_mutex ); <nl> - if (! isnan ( vp -> pts )) <nl> + if (! redisplay && ! isnan ( vp -> pts )) <nl> update_video_pts ( is , vp -> pts , vp -> pos , vp -> serial ); <nl> SDL_UnlockMutex ( is -> pictq_mutex ); <nl> 
static int cinepak_decode_vectors ( CinepakContext * s , cvid_strip * strip , <nl> const uint8_t * eod = ( data + size ); <nl> uint32_t flag , mask ; <nl> uint8_t * cb0 , * cb1 , * cb2 , * cb3 ; <nl> - unsigned int x , y ; <nl> + int x , y ; <nl> char * ip0 , * ip1 , * ip2 , * ip3 ; <nl>  <nl> flag = 0 ;
static int ff_filter_frame_framed ( AVFilterLink * link , AVFilterBufferRef * frame ) <nl> } else <nl> out = frame ; <nl>  <nl> - while ( cmd && cmd -> time <= frame -> pts * av_q2d ( link -> time_base )){ <nl> + while ( cmd && cmd -> time <= out -> pts * av_q2d ( link -> time_base )){ <nl> av_log ( link -> dst , AV_LOG_DEBUG , <nl> " Processing command time :% f command :% s arg :% s \ n ", <nl> cmd -> time , cmd -> command , cmd -> arg );
static void conv411 ( uint8_t * dst , int dst_wrap , <nl> int w , c ; <nl> uint8_t * s1 , * s2 , * d ; <nl>  <nl> + width >>= 1 ; <nl> + <nl> for (; height > 0 ; height --) { <nl> s1 = src ; <nl> s2 = src + src_wrap ;
static int common_end ( AVCodecContext * avctx ){ <nl> PlaneContext * p = & s -> plane [ i ]; <nl>  <nl> av_freep (& p -> state ); <nl> + av_freep (& p -> vlc_state ); <nl> } <nl>  <nl> return 0 ;
static int mxf_read_content_storage ( void * arg , AVIOContext * pb , int tag , int siz <nl> MXFContext * mxf = arg ; <nl> switch ( tag ) { <nl> case 0x1901 : <nl> + if ( mxf -> packages_refs ) <nl> + av_log ( mxf -> fc , AV_LOG_VERBOSE , " Multiple packages_refs \ n "); <nl> + av_free ( mxf -> packages_refs ); <nl> mxf -> packages_count = avio_rb32 ( pb ); <nl> mxf -> packages_refs = av_calloc ( mxf -> packages_count , sizeof ( UID )); <nl> if (! mxf -> packages_refs )
static int decode_rle ( uint8_t * bitmap , int linesize , int w , int h , <nl> if ( start >= buf_size ) <nl> return - 1 ; <nl>  <nl> + if ( w <= 0 || h <= 0 ) <nl> + return - 1 ; <nl> + <nl> bit_len = ( buf_size - start ) * 8 ; <nl> init_get_bits (& gb , buf + start , bit_len ); <nl> 
static int read_header ( FFV1Context * f ) <nl> } else { <nl> const uint8_t * p = c -> bytestream_end ; <nl> for ( f -> slice_count = 0 ; <nl> - f -> slice_count < MAX_SLICES && 3 < p - c -> bytestream_start ; <nl> + f -> slice_count < MAX_SLICES && 3 + 5 *!! f -> ec < p - c -> bytestream_start ; <nl> f -> slice_count ++) { <nl> int trailer = 3 + 5 *!! f -> ec ; <nl> int size = AV_RB24 ( p - trailer );
int main ( int argc , char ** argv ) <nl> printf (" Demuxing audio from file '% s ' into '% s '\ n ", src_filename , audio_dst_filename ); <nl>  <nl> /* read frames from the file */ <nl> - while ( av_read_frame ( fmt_ctx , & pkt ) >= 0 ) <nl> + while ( av_read_frame ( fmt_ctx , & pkt ) >= 0 ) { <nl> decode_packet (& got_frame , 0 ); <nl> + av_free_packet (& pkt ); <nl> + } <nl>  <nl> /* flush cached frames */ <nl> pkt . data = NULL ;
static av_cold void rnd_table_init ( void ) { <nl>  <nl> static av_cold void init_noise_samples ( void ) { <nl> int i ; <nl> - int random_seed = 0 ; <nl> + unsigned random_seed = 0 ; <nl> float delta = 1 . 0 / 16384 . 0 ; <nl> for ( i = 0 ; i < 128 ; i ++) { <nl> random_seed = random_seed * 214013 + 2531011 ;
static int transcode_init ( void ) <nl> AVFormatContext * oc ; <nl> OutputStream * ost ; <nl> InputStream * ist ; <nl> - char error [ 1024 ]; <nl> + char error [ 1024 ] = { 0 }; <nl> int want_sdp = 1 ; <nl>  <nl> for ( i = 0 ; i < nb_filtergraphs ; i ++) {
static int filter_frame ( AVFilterLink * inlink , AVFilterBufferRef * insamples ) <nl> } <nl> if ( showwaves -> buf_idx == showwaves -> w ) <nl> push_frame ( outlink ); <nl> + outpicref = showwaves -> outpicref ; <nl> } <nl>  <nl> avfilter_unref_buffer ( insamples );
static int svq1_decode_frame ( AVCodecContext * avctx , void * data , <nl> } <nl>  <nl> * pict = s -> current_picture . f ; <nl> + pict -> qscale_table = NULL ; <nl>  <nl> ff_MPV_frame_end ( s ); <nl> 
static int dxva2_map_frame ( AVHWFramesContext * ctx , AVFrame * dst , const AVFrame * <nl> } <nl>  <nl> map = av_mallocz ( sizeof (* map )); <nl> - if (! map ) <nl> + if (! map ) { <nl> + err = AVERROR ( ENOMEM ); <nl> goto fail ; <nl> + } <nl>  <nl> err = ff_hwframe_map_create ( src -> hw_frames_ctx , dst , src , <nl> dxva2_unmap_frame , map );
static int decode_header ( SnowContext * s ){ <nl> s -> block_max_depth = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> + if ( FFABS ( s -> qbias ) > 127 ) { <nl> + av_log ( s -> avctx , AV_LOG_ERROR , " qbias % d is too large \ n ", s -> qbias ); <nl> + s -> qbias = 0 ; <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> return 0 ; <nl> }
static int decode_vop_header ( Mpeg4DecContext * ctx , GetBitContext * gb ) <nl> int time_incr , time_increment ; <nl> int64_t pts ; <nl>  <nl> + s -> mcsel = 0 ; <nl> s -> pict_type = get_bits ( gb , 2 ) + AV_PICTURE_TYPE_I ; /* pict type : I = 0 , P = 1 */ <nl> if ( s -> pict_type == AV_PICTURE_TYPE_B && s -> low_delay && <nl> ctx -> vol_control_parameters == 0 && !( s -> avctx -> flags & AV_CODEC_FLAG_LOW_DELAY )) {
static int mxg_update_cache ( AVFormatContext * s , unsigned int cache_size ) <nl> /* reallocate internal buffer */ <nl> if ( current_pos > current_pos + cache_size ) <nl> return AVERROR ( ENOMEM ); <nl> - if ( mxg -> soi_ptr ) soi_pos = mxg -> soi_ptr - mxg -> buffer ; <nl> + soi_pos = mxg -> soi_ptr - mxg -> buffer ; <nl> mxg -> buffer = av_fast_realloc ( mxg -> buffer , & mxg -> buffer_size , <nl> current_pos + cache_size + <nl> FF_INPUT_BUFFER_PADDING_SIZE );
static int FUNC ( extension_data )( CodedBitstreamContext * ctx , RWContext * rw , <nl> BitstreamContext start ; <nl> uint8_t bit ; <nl> start = * rw ; <nl> - for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++); <nl> + for ( k = 0 ; cbs_h2645_read_more_rbsp_data ( rw ); k ++) <nl> + bitstream_skip ( rw , 1 ); <nl> current -> bit_length = k ; <nl> if ( k > 0 ) { <nl> * rw = start ;
static av_cold int svq3_decode_init ( AVCodecContext * avctx ) <nl> int offset = get_bits_count (& gb ) + 7 >> 3 ; <nl> uint8_t * buf ; <nl>  <nl> - if (( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> + if ( watermark_height > 0 && <nl> + ( uint64_t ) watermark_width * 4 > UINT_MAX / watermark_height ) <nl> return - 1 ; <nl>  <nl> buf = av_malloc ( buf_len );
static int decode_slice_header ( H264Context * h , H264Context * h0 ){ <nl>  <nl> } else { <nl> if ( s0 -> current_picture_ptr -> frame_num != h -> frame_num ) { <nl> + ff_thread_report_progress (( AVFrame *) s0 -> current_picture_ptr , INT_MAX , <nl> + s0 -> picture_structure == PICT_BOTTOM_FIELD ); <nl> /* <nl> * This and previous field had <nl> * different frame_nums . Consider this field first in
int ff_jpeg2000_init_component ( Jpeg2000Component * comp , <nl> if (! comp -> i_data ) <nl> return AVERROR ( ENOMEM ); <nl> } <nl> - comp -> reslevel = av_malloc_array ( codsty -> nreslevels , sizeof (* comp -> reslevel )); <nl> + comp -> reslevel = av_calloc ( codsty -> nreslevels , sizeof (* comp -> reslevel )); <nl> if (! comp -> reslevel ) <nl> return AVERROR ( ENOMEM ); <nl> /* LOOP on resolution levels */
static void ac3_bit_alloc_calc_bap_c ( int16_t * mask , int16_t * psd , <nl> static void ac3_update_bap_counts_c ( uint16_t mant_cnt [ 16 ], uint8_t * bap , <nl> int len ) <nl> { <nl> - while ( len -- >= 0 ) <nl> + while ( len -- > 0 ) <nl> mant_cnt [ bap [ len ]]++; <nl> } <nl> 
static int sdp_parse_fmtp_config_h264 ( AVStream * stream , <nl> } <nl> } else if (! strcmp ( attr , " sprop - parameter - sets ")) { <nl> codec -> extradata_size = 0 ; <nl> - codec -> extradata = NULL ; <nl> + av_freep (& codec -> extradata ); <nl>  <nl> while (* value ) { <nl> char base64packet [ 1024 ];
static int decode_audio_specific_config ( AACContext * ac , <nl> * <nl> * @ return Returns a 32 - bit pseudorandom integer <nl> */ <nl> - static av_always_inline int lcg_random ( int previous_val ) <nl> + static av_always_inline int lcg_random ( unsigned previous_val ) <nl> { <nl> return previous_val * 1664525 + 1013904223 ; <nl> }
static int h264_slice_header_parse ( const H264Context * h , H264SliceContext * sl , <nl> } <nl>  <nl> sl -> last_qscale_diff = 0 ; <nl> - tmp = pps -> init_qp + get_se_golomb (& sl -> gb ); <nl> + tmp = pps -> init_qp + ( unsigned ) get_se_golomb (& sl -> gb ); <nl> if ( tmp > 51 + 6 * ( sps -> bit_depth_luma - 8 )) { <nl> av_log ( h -> avctx , AV_LOG_ERROR , " QP % u out of range \ n ", tmp ); <nl> return AVERROR_INVALIDDATA ;
static av_always_inline int coeff_abs_level_remaining_decode ( HEVCContext * s , int <nl> } else { <nl> int prefix_minus3 = prefix - 3 ; <nl>  <nl> - if ( prefix == CABAC_MAX_BIN ) { <nl> + if ( prefix == CABAC_MAX_BIN || prefix_minus3 + rc_rice_param >= 31 ) { <nl> av_log ( s -> avctx , AV_LOG_ERROR , " CABAC_MAX_BIN : % d \ n ", prefix ); <nl> return 0 ; <nl> }
void ff_msmpeg4_encode_mb ( MpegEncContext * s , <nl> static void msmpeg4_encode_dc ( MpegEncContext * s , int level , int n , int * dir_ptr ) <nl> { <nl> int sign , code ; <nl> - int pred , extquant ; <nl> + int pred , av_uninit ( extquant ); <nl> int extrabits = 0 ; <nl>  <nl> int16_t * dc_val ;
static void start_frame ( AVFilterLink * inlink , AVFilterBufferRef * inpicref ) <nl> ) <nl> break ; <nl> } <nl> - pad -> needs_copy = plane < 4 && outpicref -> data [ plane ]; <nl> + pad -> needs_copy = plane < 4 && outpicref -> data [ plane ] || !( outpicref -> perms & AV_PERM_WRITE ); <nl> if ( pad -> needs_copy ){ <nl> av_log ( inlink -> dst , AV_LOG_DEBUG , " Direct padding impossible allocating new frame \ n "); <nl> avfilter_unref_buffer ( outpicref );
static inline int wv_get_value_integer ( WavpackFrameContext * s , uint32_t * crc , in <nl> if ( s -> extra_bits ){ <nl> S <<= s -> extra_bits ; <nl>  <nl> - if ( s -> got_extra_bits ){ <nl> + if ( s -> got_extra_bits && get_bits_left (& s -> gb_extra_bits ) >= s -> extra_bits ){ <nl> S |= get_bits (& s -> gb_extra_bits , s -> extra_bits ); <nl> * crc = * crc * 9 + ( S & 0xffff ) * 3 + (( unsigned ) S >> 16 ); <nl> }
static int transcode_init ( void ) <nl> ost -> filter -> filter -> inputs [ 0 ]-> sample_aspect_ratio ; <nl> codec -> pix_fmt = ost -> filter -> filter -> inputs [ 0 ]-> format ; <nl>  <nl> - if ( codec -> width != icodec -> width || <nl> + if (! icodec || <nl> + codec -> width != icodec -> width || <nl> codec -> height != icodec -> height || <nl> codec -> pix_fmt != icodec -> pix_fmt ) { <nl> codec -> bits_per_raw_sample = frame_bits_per_raw_sample ;
static int libopus_encode ( AVCodecContext * avctx , AVPacket * avpkt , <nl> int ret ; <nl>  <nl> if ( frame ) { <nl> - ff_af_queue_add (& opus -> afq , frame ); <nl> + ret = ff_af_queue_add (& opus -> afq , frame ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> if ( frame -> nb_samples < opus -> opts . packet_size ) { <nl> audio = opus -> samples ; <nl> memcpy ( audio , frame -> data [ 0 ], frame -> nb_samples * sample_size );
av_cold int ffv1_common_init ( AVCodecContext * avctx ) <nl>  <nl> s -> picture . f = avcodec_alloc_frame (); <nl> s -> last_picture . f = av_frame_alloc (); <nl> + if (! s -> picture . f || ! s -> last_picture . f ) <nl> + return AVERROR ( ENOMEM ); <nl> ff_dsputil_init (& s -> dsp , avctx ); <nl>  <nl> s -> width = avctx -> width ;
int ff_srtp_decrypt ( struct SRTPContext * s , uint8_t * buf , int * lenptr ) <nl> { <nl> uint8_t iv [ 16 ] = { 0 }, hmac [ 20 ]; <nl> int len = * lenptr ; <nl> - int ext , seq_largest ; <nl> - uint32_t ssrc , roc ; <nl> + int ext , av_uninit ( seq_largest ); <nl> + uint32_t ssrc , av_uninit ( roc ); <nl> uint64_t index ; <nl> int rtcp ; <nl> 
static int decode_frame_packing ( H264Context * h , int size ){ <nl>  <nl> int ff_h264_decode_sei ( H264Context * h ){ <nl> while ( get_bits_left (& h -> gb ) > 16 ) { <nl> - int size , type ; <nl> + int type ; <nl> + unsigned size ; <nl>  <nl> type = 0 ; <nl> do {
static int decode_audio_block ( AC3DecodeContext * s , int blk ) <nl> for ( ch = 1 ; ch <= s -> channels ; ch ++) { <nl> float gain = s -> mul_bias / 4194304 . 0f ; <nl> if ( s -> channel_mode == AC3_CHMODE_DUALMONO ) { <nl> - gain *= s -> dynamic_range [ ch - 1 ]; <nl> + gain *= s -> dynamic_range [ 2 - ch ]; <nl> } else { <nl> gain *= s -> dynamic_range [ 0 ]; <nl> }
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> int ret ; <nl> int i ; <nl>  <nl> + x265_picture_init ( ctx -> params , & x265pic ); <nl> + <nl> if ( pic ) { <nl> for ( i = 0 ; i < 3 ; i ++) { <nl> x265pic . planes [ i ] = pic -> data [ i ];
static int aac_sync ( uint64_t state , AACAC3ParseContext * hdr_info , <nl> int size ; <nl> union { <nl> uint64_t u64 ; <nl> - uint8_t u8 [ 8 ]; <nl> + uint8_t u8 [ 8 + FF_INPUT_BUFFER_PADDING_SIZE ]; <nl> } tmp ; <nl>  <nl> tmp . u64 = av_be2ne64 ( state );
static av_cold int vc2_encode_frame ( AVCodecContext * avctx , AVPacket * avpkt , <nl> int64_t max_frame_bytes , r_bitrate = avctx -> bit_rate >> ( s -> interlaced ); <nl>  <nl> s -> avctx = avctx ; <nl> - s -> size_scaler = 1 ; <nl> + s -> size_scaler = 2 ; <nl> s -> prefix_bytes = 0 ; <nl> s -> last_parse_code = 0 ; <nl> s -> next_parse_offset = 0 ;
static int kempf_decode_tile ( G2MContext * c , int tile_x , int tile_y , <nl> src += 3 ; <nl> } <nl> npal = * src ++ + 1 ; <nl> + if ( src_end - src < npal * 3 ) <nl> + return AVERROR_INVALIDDATA ; <nl> memcpy ( pal , src , npal * 3 ); src += npal * 3 ; <nl> if ( sub_type != 2 ) { <nl> for ( i = 0 ; i < npal ; i ++) {
int av_find_stream_info ( AVFormatContext * ic ) <nl> } <nl>  <nl> pkt = add_to_pktbuf (& ic -> packet_buffer , & pkt1 ); <nl> - if ( av_dup_packet ( pkt ) < 0 ) <nl> + if ( av_dup_packet ( pkt ) < 0 ) { <nl> + av_free ( duration_error ); <nl> return AVERROR ( ENOMEM ); <nl> + } <nl>  <nl> read_size += pkt -> size ; <nl> 
 <nl> typedef struct Hnm4VideoContext { <nl> uint8_t version ; <nl> - uint16_t width ; <nl> - uint16_t height ; <nl> + int width ; <nl> + int height ; <nl> uint8_t * current ; <nl> uint8_t * previous ; <nl> uint8_t * buffer1 ;
static int decode_frame ( AVCodecContext * avctx , <nl>  <nl> bytestream2_init (& gbc , avpkt -> data , avpkt -> size ); <nl>  <nl> + /* PICT images start with a 512 bytes empty header */ <nl> + if ( bytestream2_peek_be32 (& gbc ) == 0 ) <nl> + bytestream2_skip (& gbc , 512 ); <nl> + <nl> /* smallest PICT header */ <nl> if ( bytestream2_get_bytes_left (& gbc ) < 40 ) { <nl> av_log ( avctx , AV_LOG_ERROR , " Frame is too small % d \ n ",
static void probe_codec ( AVFormatContext * s , AVStream * st , const AVPacket * pkt ) <nl> memset ( pd -> buf + pd -> buf_size , 0 , AVPROBE_PADDING_SIZE ); <nl> } else { <nl> st -> probe_packets = 0 ; <nl> + if (! pd -> buf_size ) { <nl> + av_log ( s , AV_LOG_ERROR , " nothing to probe for stream % d \ n ", <nl> + st -> index ); <nl> + return ; <nl> + } <nl> } <nl>  <nl> if (! st -> probe_packets ||
void avformat_free_context ( AVFormatContext * s ) <nl> av_opt_free ( s ); <nl> if ( s -> iformat && s -> iformat -> priv_class && s -> priv_data ) <nl> av_opt_free ( s -> priv_data ); <nl> + if ( s -> oformat && s -> oformat -> priv_class && s -> priv_data ) <nl> + av_opt_free ( s -> priv_data ); <nl>  <nl> for ( i = s -> nb_streams - 1 ; i >= 0 ; i --) { <nl> ff_free_stream ( s , s -> streams [ i ]);
static void opt_output_file ( const char * filename ) <nl> video_enc -> rc_max_rate = video_rc_max_rate ; <nl> video_enc -> rc_min_rate = video_rc_min_rate ; <nl> video_enc -> rc_buffer_size = video_rc_buffer_size ; <nl> + video_enc -> rc_initial_buffer_occupancy = video_rc_buffer_size * 3 / 4 ; <nl> video_enc -> rc_buffer_aggressivity = video_rc_buffer_aggressivity ; <nl> video_enc -> rc_initial_cplx = video_rc_initial_cplx ; <nl> video_enc -> i_quant_factor = video_i_qfactor ;
static int probe_buf_write ( void * opaque , uint8_t * buf , int buf_size ) <nl> int main ( int argc , char ** argv ) <nl> { <nl> int ret ; <nl> - uint8_t * buffer = av_malloc ( AVP_BUFFSIZE ); <nl> + uint8_t * buffer = av_mallocz ( AVP_BUFFSIZE ); <nl>  <nl> if (! buffer ) <nl> exit ( 1 );
static int cllc_decode_frame ( AVCodecContext * avctx , void * data , <nl> coding_type = ( AV_RL32 ( src ) >> 8 ) & 0xFF ; <nl> av_log ( avctx , AV_LOG_DEBUG , " Frame coding type : % d \ n ", coding_type ); <nl>  <nl> + if ( get_bits_left (& gb ) < avctx -> height * avctx -> width ) <nl> + return AVERROR_INVALIDDATA ; <nl> + <nl> switch ( coding_type ) { <nl> case 0 : <nl> avctx -> pix_fmt = AV_PIX_FMT_YUV422P ;
static int rm_read_audio_stream_info ( AVFormatContext * s , AVIOContext * pb , <nl> sub_packet_h <= 1 || <nl> ast -> coded_framesize * sub_packet_h > ( 2 + ( sub_packet_h & 1 )) * ast -> audio_framesize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( ast -> coded_framesize * sub_packet_h != 2 * ast -> audio_framesize ) { <nl> + avpriv_request_sample ( s , " mismatching interleaver parameters "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> break ; <nl> case DEINT_ID_GENR : <nl> if ( ast -> sub_packet_size <= 0 ||
retry : <nl> es_size -= stream -> premux_packet -> unwritten_size ; <nl> stream -> premux_packet = stream -> premux_packet -> next ; <nl> } <nl> - if ( es_size ) <nl> + if ( stream -> premux_packet && es_size ) <nl> stream -> premux_packet -> unwritten_size -= es_size ; <nl>  <nl> if ( remove_decoded_packets ( ctx , s -> last_scr ) < 0 )
static int mp_decode_frame ( AVCodecContext * avctx , <nl> if ( sz == 0 ) <nl> goto end ; <nl>  <nl> + if ( mp -> max_codes_bits <= 0 ) <nl> + goto end ; <nl> if ( init_vlc (& mp -> vlc , mp -> max_codes_bits , mp -> codes_count , & mp -> codes [ 0 ]. size , sizeof ( HuffCode ), 1 , & mp -> codes [ 0 ]. code , sizeof ( HuffCode ), 4 , 0 )) <nl> goto end ; <nl> mp_decode_frame_helper ( mp , & gb );
int ff_h264_frame_start ( H264Context * h ) <nl>  <nl> if (( ret = alloc_picture ( h , pic )) < 0 ) <nl> return ret ; <nl> - if (! h -> sync ) <nl> + if (! h -> sync && ! h -> avctx -> hwaccel ) <nl> avpriv_color_frame (& pic -> f , c ); <nl>  <nl> h -> cur_pic_ptr = pic ;
typedef struct VP9Context { <nl> DECLARE_ALIGNED ( 32 , int16_t , uvblock )[ 2 ][ 1024 ]; <nl> uint8_t eob [ 256 ]; <nl> uint8_t uveob [ 2 ][ 64 ]; <nl> - VP56mv min_mv , max_mv ; <nl> + struct { int x , y ; } min_mv , max_mv ; <nl> DECLARE_ALIGNED ( 32 , uint8_t , tmp_y )[ 64 * 64 ]; <nl> DECLARE_ALIGNED ( 32 , uint8_t , tmp_uv )[ 2 ][ 32 * 32 ]; <nl> } VP9Context ;
typedef struct WmallDecodeCtx { <nl>  <nl> int8_t mclms_order ; <nl> int8_t mclms_scaling ; <nl> - int16_t mclms_coeffs [ 128 ]; <nl> - int16_t mclms_coeffs_cur [ 4 ]; <nl> + int16_t mclms_coeffs [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS * 32 ]; <nl> + int16_t mclms_coeffs_cur [ WMALL_MAX_CHANNELS * WMALL_MAX_CHANNELS ]; <nl> int16_t mclms_prevvalues [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int16_t mclms_updates [ WMALL_MAX_CHANNELS * 2 * 32 ]; <nl> int mclms_recent ;
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> switch ( c -> codec_id ) { <nl> case AV_CODEC_ID_H264 : { <nl> int mode = 1 ; <nl> - if ( fmt && fmt -> oformat -> priv_class && <nl> + if ( fmt && fmt -> oformat && fmt -> oformat -> priv_class && <nl> av_opt_flag_is_set ( fmt -> priv_data , " rtpflags ", " h264_mode0 ")) <nl> mode = 0 ; <nl> if ( c -> extradata_size ) {
static int matroska_parse_block ( MatroskaDemuxContext * matroska , uint8_t * data , <nl> lace_size [ n ] = lace_size [ n - 1 ] + snum ; <nl> total += lace_size [ n ]; <nl> } <nl> - lace_size [ n ] = size - total ; <nl> + lace_size [ laces - 1 ] = size - total ; <nl> break ; <nl> } <nl> }
static inline void xan_wc3_copy_pixel_run ( XanContext * s , <nl>  <nl> palette_plane = s -> current_frame . data [ 0 ]; <nl> prev_palette_plane = s -> last_frame . data [ 0 ]; <nl> + if (! prev_palette_plane ) <nl> + prev_palette_plane = palette_plane ; <nl> stride = s -> current_frame . linesize [ 0 ]; <nl> line_inc = stride - width ; <nl> curframe_index = y * stride + x ;
static inline void drawbox ( AVFilterBufferRef * picref , unsigned int x , unsigned i <nl> static int draw_glyphs ( DrawTextContext * dtext , AVFilterBufferRef * picref , <nl> int width , int height , const uint8_t rgbcolor [ 4 ], const uint8_t yuvcolor [ 4 ], int x , int y ) <nl> { <nl> - char * text = dtext -> text ; <nl> + char * text = HAVE_LOCALTIME_R ? dtext -> expanded_text : dtext -> text ; <nl> uint32_t code = 0 ; <nl> int i ; <nl> uint8_t * p ;
static int process_ipmovie_chunk ( IPMVEContext * s , AVIOContext * pb , <nl>  <nl> case OPCODE_INIT_AUDIO_BUFFERS : <nl> av_dlog ( NULL , " initialize audio buffers \ n "); <nl> - if (( opcode_version > 1 ) || ( opcode_size > 10 )) { <nl> + if (( opcode_version > 1 ) || ( opcode_size > 10 ) || opcode_size < 6 ) { <nl> av_dlog ( NULL , " bad init_audio_buffers opcode \ n "); <nl> chunk_type = CHUNK_BAD ; <nl> break ;
static int read_ffserver_streams ( OptionsContext * o , AVFormatContext * s , const ch <nl> { <nl> int i , err ; <nl> AVFormatContext * ic = avformat_alloc_context (); <nl> + if (! ic ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> ic -> interrupt_callback = int_cb ; <nl> err = avformat_open_input (& ic , filename , NULL , NULL );
reload : <nl> c -> end_of_segment = 1 ; <nl> c -> cur_seq_no = v -> cur_seq_no ; <nl>  <nl> - if ( v -> ctx ) { <nl> + if ( v -> ctx && v -> ctx -> nb_streams ) { <nl> v -> needed = 0 ; <nl> for ( i = v -> stream_offset ; i < v -> stream_offset + v -> ctx -> nb_streams ; <nl> i ++) {
int ff_wma_end ( AVCodecContext * avctx ) <nl> free_vlc (& s -> coef_vlc [ i ]); <nl> av_free ( s -> run_table [ i ]); <nl> av_free ( s -> level_table [ i ]); <nl> + av_free ( s -> int_table [ i ]); <nl> } <nl>  <nl> return 0 ;
static int mov_read_extradata ( MOVContext * c , AVIOContext * pb , MOVAtom atom , <nl> av_log ( c -> fc , AV_LOG_WARNING , " truncated extradata \ n "); <nl> st -> codec -> extradata_size -= atom . size - err ; <nl> } <nl> + memset ( buf + 8 + err , 0 , FF_INPUT_BUFFER_PADDING_SIZE ); <nl> return 0 ; <nl> } <nl> 
static int read_channel_params ( MLPDecodeContext * m , unsigned int substr , <nl>  <nl> if ( cp -> huff_lsbs > 24 ) { <nl> av_log ( m -> avctx , AV_LOG_ERROR , " Invalid huff_lsbs .\ n "); <nl> + cp -> huff_lsbs = 0 ; <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> 
typedef struct MmContext { <nl> AVCodecContext * avctx ; <nl> AVFrame * frame ; <nl> - int palette [ AVPALETTE_COUNT ]; <nl> + unsigned int palette [ AVPALETTE_COUNT ]; <nl> GetByteContext gb ; <nl> } MmContext ; <nl> 
static av_cold int XAVS_init ( AVCodecContext * avctx ) <nl> if (! x4 -> enc ) <nl> return - 1 ; <nl>  <nl> - if (!( x4 -> pts_buffer = av_mallocz (( avctx -> max_b_frames + 1 ) * sizeof (* x4 -> pts_buffer )))) <nl> + if (!( x4 -> pts_buffer = av_mallocz_array (( avctx -> max_b_frames + 1 ), sizeof (* x4 -> pts_buffer )))) <nl> return AVERROR ( ENOMEM ); <nl>  <nl> avctx -> coded_frame = av_frame_alloc ();
static int set_string_binary ( void * obj , const AVOption * o , const char * val , uint <nl> len /= 2 ; <nl>  <nl> ptr = bin = av_malloc ( len ); <nl> + if (! ptr ) <nl> + return AVERROR ( ENOMEM ); <nl> while (* val ) { <nl> int a = hexchar2int (* val ++); <nl> int b = hexchar2int (* val ++);
static int set_string_number ( void * obj , void * target_obj , const AVOption * o , con <nl> } <nl>  <nl> { <nl> - const AVOption * o_named = av_opt_find ( target_obj , buf , o -> unit , 0 , 0 ); <nl> + const AVOption * o_named = av_opt_find ( target_obj , i ? buf : val , o -> unit , 0 , 0 ); <nl> int res ; <nl> int ci = 0 ; <nl> double const_values [ 64 ];
static void d3d11va_frames_uninit ( AVHWFramesContext * ctx ) <nl> static void free_texture ( void * opaque , uint8_t * data ) <nl> { <nl> ID3D11Texture2D_Release (( ID3D11Texture2D *) opaque ); <nl> + av_free ( data ); <nl> } <nl>  <nl> static AVBufferRef * wrap_texture_buf ( ID3D11Texture2D * tex , int index )
static int create_filter ( AVFilterContext ** filt_ctx , AVFilterGraph * ctx , int ind <nl> return ret ; <nl> } <nl>  <nl> - if (! strcmp ( filt_name , " scale ") && ! strstr ( args , " flags ")) { <nl> + if (! strcmp ( filt_name , " scale ") && args && ! strstr ( args , " flags ")) { <nl> snprintf ( tmp_args , sizeof ( tmp_args ), "% s :% s ", <nl> args , ctx -> scale_sws_opts ); <nl> args = tmp_args ;
static int hds_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> HDSContext * c = s -> priv_data ; <nl> AVStream * st = s -> streams [ pkt -> stream_index ]; <nl> OutputStream * os = & c -> streams [ s -> streams [ pkt -> stream_index ]-> id ]; <nl> - int64_t end_dts = ( os -> fragment_index ) * c -> min_frag_duration ; <nl> + int64_t end_dts = os -> fragment_index * ( int64_t ) c -> min_frag_duration ; <nl> int ret ; <nl>  <nl> if ( st -> first_dts == AV_NOPTS_VALUE )
static av_cold int g726_decode_init ( AVCodecContext * avctx ) <nl> { <nl> G726Context * c = avctx -> priv_data ; <nl>  <nl> + if ( avctx -> channels > 1 ){ <nl> + avpriv_request_sample ( avctx , " Decoding more than one channel "); <nl> + return AVERROR_PATCHWELCOME ; <nl> + } <nl> avctx -> channels = 1 ; <nl> avctx -> channel_layout = AV_CH_LAYOUT_MONO ; <nl> 
* MPEG Audio decoder <nl> */ <nl>  <nl> -# define UNCHECKED_BITSTREAM_READER 1 <nl> - <nl> # include " libavutil / audioconvert . h " <nl> # include " avcodec . h " <nl> # include " get_bits . h "
static int check_opcodes ( MMCO * mmco1 , MMCO * mmco2 , int n_mmcos ) <nl> int ff_generate_sliding_window_mmcos ( H264Context * h , int first_slice ) <nl> { <nl> MMCO mmco_temp [ MAX_MMCO_COUNT ], * mmco = first_slice ? h -> mmco : mmco_temp ; <nl> - int mmco_index = 0 , i ; <nl> + int mmco_index = 0 , i = 0 ; <nl>  <nl> assert ( h -> long_ref_count + h -> short_ref_count <= h -> sps . ref_frame_count ); <nl> 
static int64_t pva_read_timestamp ( struct AVFormatContext * s , int stream_index , <nl> ByteIOContext * pb = s -> pb ; <nl> PVAContext * pvactx = s -> priv_data ; <nl> int length , streamid ; <nl> - int64_t res ; <nl> + int64_t res = AV_NOPTS_VALUE ; <nl>  <nl> pos_limit = FFMIN (* pos + PVA_MAX_PAYLOAD_LENGTH * 8 , ( uint64_t )* pos + pos_limit ); <nl> 
static void rac_normalise ( RangeCoder * c ) <nl> c -> low |= * c -> src ++; <nl> } else if (! c -> low ) { <nl> c -> got_error = 1 ; <nl> - return ; <nl> + c -> low = 1 ; <nl> } <nl> if ( c -> range >= RAC_BOTTOM ) <nl> return ;
static inline int init_get_bits ( GetBitContext * s , const uint8_t * buffer , <nl> int buffer_size ; <nl> int ret = 0 ; <nl>  <nl> - if ( bit_size > INT_MAX - 7 || bit_size <= 0 ) { <nl> + if ( bit_size > INT_MAX - 7 || bit_size < 0 || ! buffer ) { <nl> buffer_size = bit_size = 0 ; <nl> buffer = NULL ; <nl> ret = AVERROR_INVALIDDATA ;
int av_expr_parse ( AVExpr ** expr , const char * s , <nl> goto end ; <nl> } <nl> e -> var = av_mallocz ( sizeof ( double ) * VARS ); <nl> + if (! e -> var ) { <nl> + ret = AVERROR ( ENOMEM ); <nl> + goto end ; <nl> + } <nl> * expr = e ; <nl> e = NULL ; <nl> end :
void ff_af_queue_remove ( AudioFrameQueue * afq , int nb_samples , int64_t * pts , <nl>  <nl> if ( nb_samples ){ <nl> av_assert0 (! afq -> frame_count ); <nl> - if ( afq -> frames [ 0 ]. pts != AV_NOPTS_VALUE ) <nl> + if ( afq -> frames && afq -> frames [ 0 ]. pts != AV_NOPTS_VALUE ) <nl> afq -> frames [ 0 ]. pts += nb_samples ; <nl> av_log ( afq -> avctx , AV_LOG_DEBUG , " Trying to remove % d more samples than are in the que \ n ", nb_samples ); <nl> }
int64_t avio_seek ( AVIOContext * s , int64_t offset , int whence ) <nl> offset1 = pos + ( s -> buf_ptr - s -> buffer ); <nl> if ( offset == 0 ) <nl> return offset1 ; <nl> + if ( offset > INT64_MAX - offset1 ) <nl> + return AVERROR ( EINVAL ); <nl> offset += offset1 ; <nl> } <nl> if ( offset < 0 )
static int cfhd_decode ( AVCodecContext * avctx , void * data , int * got_frame , <nl> output = s -> plane [ plane ]. subband [ 0 ]; <nl> for ( i = 0 ; i < lowpass_height * 2 ; i ++) { <nl> for ( j = 0 ; j < lowpass_width * 2 ; j ++) <nl> - output [ j ] <<= 2 ; <nl> + output [ j ] *= 4 ; <nl>  <nl> output += lowpass_width * 2 ; <nl> }
static int build_table ( VLC * vlc , int table_nb_bits , int nb_codes , <nl> /* note : realloc has been done , so reload tables */ <nl> table = & vlc -> table [ table_index ]; <nl> table [ j ][ 0 ] = index ; // code <nl> + av_assert0 ( table [ j ][ 0 ] == index ); <nl> i = k - 1 ; <nl> } <nl> }
void ff_rtsp_undo_setup ( AVFormatContext * s ) <nl> avformat_free_context ( rtpctx ); <nl> } else if ( rt -> transport == RTSP_TRANSPORT_RDT && CONFIG_RTPDEC ) <nl> ff_rdt_parse_close ( rtsp_st -> transport_priv ); <nl> - else if ( rt -> transport == RTSP_TRANSPORT_RAW && CONFIG_RTPDEC ) <nl> + else if ( rt -> transport == RTSP_TRANSPORT_RTP && CONFIG_RTPDEC ) <nl> ff_rtp_parse_close ( rtsp_st -> transport_priv ); <nl> } <nl> rtsp_st -> transport_priv = NULL ;
fail : <nl> fail_at_end : <nl> av_freep (& pic -> codec_picture_params ); <nl> av_frame_free (& pic -> recon_image ); <nl> + av_buffer_unref (& pic -> output_buffer_ref ); <nl> + pic -> output_buffer = VA_INVALID_ID ; <nl> return err ; <nl> } <nl> 
static void guess_mv ( MpegEncContext * s ){ <nl> fixed [ mb_xy ]= f ; <nl> if ( f == MV_FROZEN ) <nl> num_avail ++; <nl> - else if ( s -> last_picture . data [ 0 ]){ <nl> + else if ( s -> last_picture . data [ 0 ] && s -> last_picture . motion_val [ 0 ]){ <nl> const int mb_y = mb_xy / s -> mb_stride ; <nl> const int mb_x = mb_xy % s -> mb_stride ; <nl> const int mot_index = ( mb_x + mb_y * mot_stride ) * mot_step ;
SchroFrame * ff_create_schro_frame ( AVCodecContext * avccontext , <nl> uv_height = y_height >> ( SCHRO_FRAME_FORMAT_V_SHIFT ( schro_frame_fmt )); <nl>  <nl> p_pic = av_mallocz ( sizeof ( AVPicture )); <nl> - avpicture_alloc ( p_pic , avccontext -> pix_fmt , y_width , y_height ); <nl> + if (! p_pic || avpicture_alloc ( p_pic , avccontext -> pix_fmt , y_width , y_height ) < 0 ) { <nl> + av_free ( p_pic ); <nl> + return NULL ; <nl> + } <nl>  <nl> p_frame = schro_frame_new (); <nl> p_frame -> format = schro_frame_fmt ;
static char * sdp_write_media_attributes ( char * buff , int size , AVCodecContext * c , <nl> payload_type , config ? config : ""); <nl> break ; <nl> case CODEC_ID_AAC : <nl> - if ( fmt && fmt -> oformat -> priv_class && <nl> + if ( fmt && fmt -> oformat && fmt -> oformat -> priv_class && <nl> av_opt_flag_is_set ( fmt -> priv_data , " rtpflags ", " latm ")) { <nl> config = latm_context2config ( c ); <nl> if (! config )
static int fdk_aac_decode_frame ( AVCodecContext * avctx , void * data , <nl> return AVERROR_INVALIDDATA ; <nl> } <nl>  <nl> - err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size , 0 ); <nl> + err = aacDecoder_DecodeFrame ( s -> handle , ( INT_PCM *) s -> decoder_buffer , s -> decoder_buffer_size / sizeof ( INT_PCM ), 0 ); <nl> if ( err == AAC_DEC_NOT_ENOUGH_BITS ) { <nl> ret = avpkt -> size - valid ; <nl> goto end ;
static int dv_write_header ( AVFormatContext * s ) <nl> break ; <nl> } <nl> } <nl> - if ( tcr ) <nl> - return av_timecode_init_from_string (& dvc -> tc , rate , tcr -> value , s ); <nl> + if ( tcr && av_timecode_init_from_string (& dvc -> tc , rate , tcr -> value , s ) >= 0 ) <nl> + return 0 ; <nl> return av_timecode_init (& dvc -> tc , rate , 0 , 0 , s ); <nl> } <nl> 
AVStream * avformat_new_stream ( AVFormatContext * s , const AVCodec * c ) <nl> } <nl>  <nl> st -> codec = avcodec_alloc_context3 ( c ); <nl> + if (! st -> codec ) { <nl> + av_free ( st -> info ); <nl> + av_free ( st ); <nl> + return NULL ; <nl> + } <nl> if ( s -> iformat ) { <nl> /* no default bitrate if decoding */ <nl> st -> codec -> bit_rate = 0 ;
static int libx265_encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> { <nl> libx265Context * ctx = avctx -> priv_data ; <nl> x265_picture x265pic ; <nl> - x265_picture x265pic_out = { { 0 } }; <nl> + x265_picture x265pic_out = { 0 }; <nl> x265_nal * nal ; <nl> uint8_t * dst ; <nl> int payload = 0 ;
static av_cold int encode_init ( AVCodecContext * avc_context ) <nl>  <nl> /* Set up the output AVFrame */ <nl> avc_context -> coded_frame = av_frame_alloc (); <nl> + if (! avc_context -> coded_frame ) <nl> + return AVERROR ( ENOMEM ); <nl>  <nl> return 0 ; <nl> }
int avcodec_default_reget_buffer ( AVCodecContext * s , AVFrame * pic ){ <nl> return s -> get_buffer ( s , pic ); <nl> } <nl>  <nl> + assert ( s -> pix_fmt == pic -> pix_fmt ); <nl> + <nl> /* If internal buffer type return the same buffer */ <nl> if ( pic -> type == FF_BUFFER_TYPE_INTERNAL ) { <nl> if ( s -> pkt ) pic -> pkt_pts = s -> pkt -> pts ;
retry : <nl> if ( snprintf ( str , str_size_alloc , "% f ", val ) >= str_size_alloc ) { <nl> av_log ( c -> fc , AV_LOG_ERROR , <nl> " Failed to store the float32 number (% f ) in string .\ n ", val ); <nl> + av_free ( str ); <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> } else {
static void add_input_streams ( OptionsContext * o , AVFormatContext * ic ) <nl> if (! ist -> hwaccel_device ) <nl> exit_program ( 1 ); <nl> } <nl> + ist -> hwaccel_pix_fmt = AV_PIX_FMT_NONE ; <nl>  <nl> break ; <nl> case AVMEDIA_TYPE_AUDIO :
static av_always_inline SoftFloat autocorr_calc ( int64_t accu ) <nl>  <nl> round = 1U << ( nz - 1 ); <nl> mant = ( int )(( accu + round ) >> nz ); <nl> - mant = ( mant + 0x40 )>> 7 ; <nl> + mant = ( mant + 0x40LL )>> 7 ; <nl> mant *= 64 ; <nl> expo = nz + 15 ; <nl> return av_int2sf ( mant , 30 - expo );
static int qsv_decode ( AVCodecContext * avctx , QSVContext * q , <nl>  <nl> do { <nl> ret = get_surface ( avctx , q , & insurf ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + av_freep (& sync ); <nl> return ret ; <nl> + } <nl>  <nl> ret = MFXVideoDECODE_DecodeFrameAsync ( q -> session , avpkt -> size ? & bs : NULL , <nl> insurf , & outsurf , sync );
static int rtmp_open ( URLContext * s , const char * uri , int flags ) <nl> fail : <nl> if ( filename != s -> filename ) <nl> av_freep (& filename ); <nl> + if ( rc ) <nl> + RTMP_Close ( r ); <nl> + <nl> return rc ; <nl> } <nl> 
static void exit_program ( void ) <nl> /* close files */ <nl> for ( i = 0 ; i < nb_output_files ; i ++) { <nl> AVFormatContext * s = output_files [ i ]-> ctx ; <nl> - if (!( s -> oformat -> flags & AVFMT_NOFILE ) && s -> pb ) <nl> + if ( s && s -> oformat && !( s -> oformat -> flags & AVFMT_NOFILE ) && s -> pb ) <nl> avio_close ( s -> pb ); <nl> avformat_free_context ( s ); <nl> av_dict_free (& output_files [ i ]-> opts );
SwsContext * sws_alloc_context ( void ) <nl> { <nl> SwsContext * c = av_mallocz ( sizeof ( SwsContext )); <nl>  <nl> - c -> av_class = & sws_context_class ; <nl> - av_opt_set_defaults ( c ); <nl> + if ( c ) { <nl> + c -> av_class = & sws_context_class ; <nl> + av_opt_set_defaults ( c ); <nl> + } <nl>  <nl> return c ; <nl> }
const char * av_opencl_errstr ( cl_int status ) <nl> static void free_device_list ( AVOpenCLDeviceList * device_list ) <nl> { <nl> int i , j ; <nl> - if (! device_list ) <nl> + if (! device_list || ! device_list -> platform_node ) <nl> return ; <nl> for ( i = 0 ; i < device_list -> platform_num ; i ++) { <nl> if (! device_list -> platform_node [ i ])
static int mv_read_header ( AVFormatContext * avctx ) <nl> uint32_t pos = avio_rb32 ( pb ); <nl> uint32_t asize = avio_rb32 ( pb ); <nl> uint32_t vsize = avio_rb32 ( pb ); <nl> + if ( avio_feof ( pb )) <nl> + return AVERROR_INVALIDDATA ; <nl> avio_skip ( pb , 8 ); <nl> av_add_index_entry ( ast , pos , timestamp , asize , 0 , AVINDEX_KEYFRAME ); <nl> av_add_index_entry ( vst , pos + asize , i , vsize , 0 , AVINDEX_KEYFRAME );
static int mov_write_packet ( AVFormatContext * s , AVPacket * pkt ) <nl> /* copy frame to create needed atoms */ <nl> trk -> vosLen = size ; <nl> trk -> vosData = av_malloc ( size ); <nl> + if (! trk -> vosData ) <nl> + return AVERROR ( ENOMEM ); <nl> memcpy ( trk -> vosData , pkt -> data , size ); <nl> } <nl> 
static int decode_cell ( Indeo3DecodeContext * ctx , AVCodecContext * avctx , <nl> /* of the predicted cell in order to avoid overflows . */ <nl> if ( vq_index >= 8 && ref_block ) { <nl> for ( x = 0 ; x < cell -> width << 2 ; x ++) <nl> - ref_block [ x ] = requant_tab [ vq_index & 7 ][ ref_block [ x ]]; <nl> + ref_block [ x ] = requant_tab [ vq_index & 7 ][ ref_block [ x ] & 127 ]; <nl> } <nl>  <nl> error = IV3_NOERR ;
static int decode_frame ( AVCodecContext * avctx , void * data , int * got_frame , <nl> AVFrame * const p = data ; <nl> int mb_x , mb_y , ret ; <nl>  <nl> - if ( buf_size * 8LL < a -> mb_height2 * a -> mb_width2 * 13LL ) <nl> + if ( buf_size * 8LL < a -> mb_height * a -> mb_width * 13LL ) <nl> return AVERROR_INVALIDDATA ; <nl>  <nl> if (( ret = ff_get_buffer ( avctx , p , 0 )) < 0 )
read_header : <nl>  <nl> // XXX FIXME factorize , this looks very similar to the EOI code <nl>  <nl> + if (! s -> got_picture ) { <nl> + av_log ( avctx , AV_LOG_WARNING , " no picture \ n "); <nl> + return buf_size ; <nl> + } <nl> + <nl> * picture = * s -> picture_ptr ; <nl> * data_size = sizeof ( AVFrame ); <nl> 
static int zerocodec_decode_frame ( AVCodecContext * avctx , void * data , <nl> pic -> key_frame = 1 ; <nl> pic -> pict_type = AV_PICTURE_TYPE_I ; <nl> } else { <nl> + if ( prev == NULL ) { <nl> + av_log ( avctx , AV_LOG_ERROR , " No previous frame !\ n "); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl> pic -> key_frame = 0 ; <nl> pic -> pict_type = AV_PICTURE_TYPE_P ; <nl> }
void ff_ivi_output_plane ( IVIPlaneDesc * plane , uint8_t * dst , int dst_pitch ) <nl> const int16_t * src = plane -> bands [ 0 ]. buf ; <nl> uint32_t pitch = plane -> bands [ 0 ]. pitch ; <nl>  <nl> + if (! src ) <nl> + return ; <nl> + <nl> for ( y = 0 ; y < plane -> height ; y ++) { <nl> for ( x = 0 ; x < plane -> width ; x ++) <nl> dst [ x ] = av_clip_uint8 ( src [ x ] + 128 );
static int encode_frame ( AVCodecContext * avctx , AVPacket * pkt , <nl> ff_init_range_encoder ( c , pkt -> data , pkt -> size ); <nl> ff_build_rac_states ( c , 0 . 05 * ( 1LL << 32 ), 256 - 8 ); <nl>  <nl> + av_frame_unref ( p ); <nl> av_frame_ref ( p , pict ); <nl> p -> pict_type = AV_PICTURE_TYPE_I ; <nl> 
static av_cold int vc2_encode_init ( AVCodecContext * avctx ) <nl> s -> num_x = s -> plane [ 0 ]. dwt_width / s -> slice_width ; <nl> s -> num_y = s -> plane [ 0 ]. dwt_height / s -> slice_height ; <nl>  <nl> - s -> slice_args = av_malloc ( s -> num_x * s -> num_y * sizeof ( SliceArgs )); <nl> + s -> slice_args = av_calloc ( s -> num_x * s -> num_y , sizeof ( SliceArgs )); <nl> if (! s -> slice_args ) <nl> goto alloc_fail ; <nl> 
retry : <nl> return psize ; <nl> fail : <nl> av_free_packet ( pkt ); <nl> - av_free ( pkt ); <nl> return AVERROR ( ENOMEM ); <nl> } <nl> 
static inline int ape_decode_value_3900 ( APEContext * ctx , APERice * rice ) <nl> return AVERROR_INVALIDDATA ; <nl> } <nl> x = range_decode_bits ( ctx , tmpk ); <nl> - } else if ( tmpk <= 32 ) { <nl> + } else if ( tmpk <= 31 ) { <nl> x = range_decode_bits ( ctx , 16 ); <nl> x |= ( range_decode_bits ( ctx , tmpk - 16 ) << 16 ); <nl> } else {
uint8_t * av_packet_new_side_data ( AVPacket * pkt , enum AVPacketSideDataType type , <nl> data = av_malloc ( size + AV_INPUT_BUFFER_PADDING_SIZE ); <nl> if (! data ) <nl> return NULL ; <nl> + memset ( data + size , 0 , AV_INPUT_BUFFER_PADDING_SIZE ); <nl>  <nl> ret = av_packet_add_side_data ( pkt , type , data , size ); <nl> if ( ret < 0 ) {
int main ( int argc , char ** argv ) <nl>  <nl> if ( got_frame ) { <nl> /* push the audio data from decoded frame into the filtergraph */ <nl> - if ( av_buffersrc_add_frame_flags ( buffersrc_ctx , frame , AV_BUFFERSRC_FLAG_KEEP_REF ) < 0 ) { <nl> + if ( av_buffersrc_add_frame_flags ( buffersrc_ctx , frame , 0 ) < 0 ) { <nl> av_log ( NULL , AV_LOG_ERROR , " Error while feeding the audio filtergraph \ n "); <nl> break ; <nl> }
static int decode_frame ( AVCodecContext * avctx , <nl> if ( i ) { <nl> AVRational q = av_d2q ( av_int2float ( i ), 4096 ); <nl> if ( q . num > 0 && q . den > 0 ) <nl> - avctx -> time_base = av_inv_q ( q ); <nl> + avctx -> framerate = q ; <nl> } <nl> } <nl> 
static int opus_packet ( AVFormatContext * avf , int idx ) <nl>  <nl> if (! os -> psize ) <nl> return AVERROR_INVALIDDATA ; <nl> + if ( os -> granule > INT64_MAX - UINT32_MAX ) { <nl> + av_log ( avf , AV_LOG_ERROR , " Unsupported huge granule pos %" PRId64 "\ n ", os -> granule ); <nl> + return AVERROR_INVALIDDATA ; <nl> + } <nl>  <nl> if ((! os -> lastpts || os -> lastpts == AV_NOPTS_VALUE ) && !( os -> flags & OGG_FLAG_EOS )) { <nl> int seg , d ;
static int try_decode_video_frame ( AVCodecContext * codec_ctx , AVPacket * pkt , int <nl> goto end ; <nl> } <nl>  <nl> - if (! decode && codec_ctx -> codec -> caps_internal & FF_CODEC_CAP_SKIP_FRAME_FILL_PARAM ) { <nl> + if (! decode && avpriv_codec_get_cap_skip_frame_fill_param ( codec_ctx -> codec )) { <nl> codec_ctx -> skip_frame = AVDISCARD_ALL ; <nl> } <nl> 
static int mpc8_decode_frame ( AVCodecContext * avctx , <nl> maxband = c -> last_max_band + get_vlc2 ( gb , band_vlc . table , MPC8_BANDS_BITS , 2 ); <nl> if ( maxband > 32 ) maxband -= 33 ; <nl> } <nl> + if ( maxband > c -> maxbands ) <nl> + return AVERROR_INVALIDDATA ; <nl> c -> last_max_band = maxband ; <nl>  <nl> /* read subband indexes */
static int process_line ( URLContext * h , char * line , int line_count , <nl> # ifdef DEBUG <nl> printf (" http_code =% d \ n ", s -> http_code ); <nl> # endif <nl> + /* error codes are 4xx and 5xx */ <nl> + if ( s -> http_code >= 400 && s -> http_code < 600 ) <nl> + return - 1 ; <nl> } else { <nl> while (* p != '\ 0 ' && * p != ':') <nl> p ++;
static int decode_ics_info ( AACContext * ac , IndividualChannelStream * ics , <nl> if ( aot != AOT_ER_AAC_ELD ) { <nl> if ( get_bits1 ( gb )) { <nl> av_log ( ac -> avctx , AV_LOG_ERROR , " Reserved bit set .\ n "); <nl> - return AVERROR_INVALIDDATA ; <nl> + if ( ac -> avctx -> err_recognition & AV_EF_BITSTREAM ) <nl> + return AVERROR_INVALIDDATA ; <nl> } <nl> ics -> window_sequence [ 1 ] = ics -> window_sequence [ 0 ]; <nl> ics -> window_sequence [ 0 ] = get_bits ( gb , 2 );
static int http_open_cnx ( URLContext * h ) <nl>  <nl> if (! strcmp ( proto , " https ")) { <nl> lower_proto = " tls "; <nl> + use_proxy = 0 ; <nl> if ( port < 0 ) <nl> port = 443 ; <nl> }
static AVFilterContext * create_filter_with_args ( const char * filt , void * opaque ) <nl> av_log ( NULL , AV_LOG_ERROR , <nl> " error creating filter \"% s \" with args \"% s \"\ n ", <nl> name , args ? args : "( none )"); <nl> - return NULL ; <nl> } <nl>  <nl> av_free ( filter );
void ff_mpeg_set_erpic ( ERPicture * dst , Picture * src ) <nl> { <nl> int i ; <nl>  <nl> - if (! src ) <nl> + if (! src ) { <nl> + dst -> f = NULL ; <nl> + dst -> tf = NULL ; <nl> return ; <nl> + } <nl>  <nl> dst -> f = src -> f ; <nl> dst -> tf = & src -> tf ;
static void read_tree ( GetBitContext * gb , Tree * tree ) <nl> tree -> syms [ i ] = get_bits ( gb , 4 ); <nl> tmp1 [ tree -> syms [ i ]] = 1 ; <nl> } <nl> - for ( i = 0 ; i < 16 ; i ++) <nl> + for ( i = 0 ; i < 16 && len < 16 - 1 ; i ++) <nl> if (! tmp1 [ i ]) <nl> tree -> syms [++ len ] = i ; <nl> } else {
output_data ( const char * format , ...) <nl>  <nl> va_start ( args , format ); <nl> remaining = BUFSIZ - ( nfrontp - netobuf ); <nl> + if ( remaining == 0 ) <nl> + return remaining ; <nl> if (( n = vsnprintf ( nfrontp , remaining , format , args )) >= remaining || n < 0 ) <nl> n = strlen ( nfrontp ); <nl> nfrontp += n ;
void feedlist_formaction :: process_operation ( operation op , bool automatic , std :: v <nl> case OP_OPENINBROWSER : { <nl> std :: tr1 :: shared_ptr < rss_feed > feed = v -> get_ctrl ()-> get_feed ( pos ); <nl> if ( feed ) { <nl> + LOG ( LOG_INFO , " feedlist_formaction : opening feed at position `% s ': % s ", feedpos . c_str (), feed -> link (). c_str ()); <nl> v -> open_in_browser ( feed -> link ()); <nl> } <nl> }
static void slavio_check_interrupts ( SLAVIO_INTCTLState * s , int set_irqs ) <nl> CPU_IRQ_TIMER_IN ; <nl> if ( i == s -> target_cpu ) { <nl> for ( j = 0 ; j < 32 ; j ++) { <nl> - if (( s -> intregm_pending & ( 1 << j )) && intbit_to_level [ j ]) { <nl> + if (( s -> intregm_pending & ( 1U << j )) && intbit_to_level [ j ]) { <nl> s -> slaves [ i ]. intreg_pending |= 1 << intbit_to_level [ j ]; <nl> } <nl> }
static inline int cpu_interrupts_enabled ( CPUSPARCState * env1 ) <nl> if ( env1 -> psret != 0 ) <nl> return 1 ; <nl> # else <nl> - if ( env1 -> pstate & PS_IE ) <nl> + if (( env1 -> pstate & PS_IE ) && ! cpu_hypervisor_mode ( env1 )) { <nl> return 1 ; <nl> + } <nl> # endif <nl>  <nl> return 0 ;
static target_ulong disas_insn ( DisasContext * s , target_ulong pc_start ) <nl> tval += s -> pc - s -> cs_base ; <nl> if ( s -> dflag == 0 ) <nl> tval &= 0xffff ; <nl> + else if (! CODE64 ( s )) <nl> + tval &= 0xffffffff ; <nl> gen_jmp ( s , tval ); <nl> break ; <nl> case 0xea : /* ljmp im */
static int elf_core_dump ( int signr , const CPUArchState * env ) <nl> phdr . p_align = ELF_EXEC_PAGESIZE ; <nl>  <nl> bswap_phdr (& phdr , 1 ); <nl> - dump_write ( fd , & phdr , sizeof ( phdr )); <nl> + if ( dump_write ( fd , & phdr , sizeof ( phdr )) != 0 ) { <nl> + goto out ; <nl> + } <nl> } <nl>  <nl> /*
int virtio_load ( VirtIODevice * vdev , QEMUFile * f ) <nl> qemu_get_8s ( f , & vdev -> status ); <nl> qemu_get_8s ( f , & vdev -> isr ); <nl> qemu_get_be16s ( f , & vdev -> queue_sel ); <nl> + if ( vdev -> queue_sel >= VIRTIO_PCI_QUEUE_MAX ) { <nl> + return - 1 ; <nl> + } <nl> qemu_get_be32s ( f , & features ); <nl>  <nl> if ( virtio_set_features ( vdev , features ) < 0 ) {
CPUArchState * cpu_copy ( CPUArchState * env ) <nl> { <nl> CPUState * cpu = ENV_GET_CPU ( env ); <nl> CPUState * new_cpu = cpu_init ( cpu_model ); <nl> - CPUArchState * new_env = cpu -> env_ptr ; <nl> + CPUArchState * new_env = new_cpu -> env_ptr ; <nl> CPUBreakpoint * bp ; <nl> CPUWatchpoint * wp ; <nl> 
static uint32_t vmxnet3_get_interrupt_config ( VMXNET3State * s ) <nl> static void vmxnet3_fill_stats ( VMXNET3State * s ) <nl> { <nl> int i ; <nl> + <nl> + if (! s -> device_active ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < s -> txq_num ; i ++) { <nl> cpu_physical_memory_write ( s -> txq_descr [ i ]. tx_stats_pa , <nl> & s -> txq_descr [ i ]. txq_stats ,
do_check_protect_pse36 : <nl>  <nl> /* the page can be put in the TLB */ <nl> prot = PAGE_READ ; <nl> - if (!( ptep & PG_NX_MASK )) <nl> + if (!( ptep & PG_NX_MASK ) && <nl> + !(( env -> cr [ 4 ] & CR4_SMEP_MASK ) && ( ptep & PG_USER_MASK ))) { <nl> prot |= PAGE_EXEC ; <nl> + } <nl> if ( pte & PG_DIRTY_MASK ) { <nl> /* only set write access if already dirty ... otherwise wait <nl> for dirty access */
static void esp_pci_dma_memory_rw ( PCIESPState * pci , uint8_t * buf , int len , <nl> /* update status registers */ <nl> pci -> dma_regs [ DMA_WBC ] -= len ; <nl> pci -> dma_regs [ DMA_WAC ] += len ; <nl> + if ( pci -> dma_regs [ DMA_WBC ] == 0 ) <nl> + pci -> dma_regs [ DMA_STAT ] |= DMA_STAT_DONE ; <nl> } <nl>  <nl> static void esp_pci_dma_memory_read ( void * opaque , uint8_t * buf , int len )
static void raise_mmu_exception ( CPUMIPSState * env , target_ulong address , <nl> env -> CP0_Context = ( env -> CP0_Context & ~ 0x007fffff ) | <nl> (( address >> 9 ) & 0x007ffff0 ); <nl> env -> CP0_EntryHi = ( env -> CP0_EntryHi & env -> CP0_EntryHi_ASID_mask ) | <nl> + ( env -> CP0_EntryHi & ( 1 << CP0EnHi_EHINV )) | <nl> ( address & ( TARGET_PAGE_MASK << 1 )); <nl> # if defined ( TARGET_MIPS64 ) <nl> env -> CP0_EntryHi &= env -> SEGMask ;
void hmp_drive_add_node ( Monitor * mon , const char * optstr ) <nl> qdict = qemu_opts_to_qdict ( opts , NULL ); <nl>  <nl> if (! qdict_get_try_str ( qdict , " node - name ")) { <nl> + QDECREF ( qdict ); <nl> error_report ("' node - name ' needs to be specified "); <nl> goto out ; <nl> }
static void vnc_dpy_copy ( DisplayChangeListener * dcl , <nl> int i , x , y , pitch , inc , w_lim , s ; <nl> int cmp_bytes ; <nl>  <nl> + if (! vd -> server ) { <nl> + /* no client connected */ <nl> + return ; <nl> + } <nl> + <nl> vnc_refresh_server_surface ( vd ); <nl> QTAILQ_FOREACH_SAFE ( vs , & vd -> clients , next , vn ) { <nl> if ( vnc_has_feature ( vs , VNC_FEATURE_COPYRECT )) {
static void fsl_imx31_class_init ( ObjectClass * oc , void * data ) <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl>  <nl> dc -> realize = fsl_imx31_realize ; <nl> - <nl> dc -> desc = " i . MX31 SOC "; <nl> + /* <nl> + * Reason : uses serial_hds in realize and the kzm board does not <nl> + * support multiple CPUs <nl> + */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo fsl_imx31_type_info = {
int main ( int argc , char ** argv ) <nl>  <nl> path = g_strdup_printf (" e1000 /% s ", models [ i ]); <nl> qtest_add_data_func ( path , models [ i ], test_device ); <nl> + g_free ( path ); <nl> } <nl>  <nl> return g_test_run ();
static uint64_t coroutine_fn mirror_iteration ( MirrorBlockJob * s ) <nl> nb_chunks * sectors_per_chunk ); <nl> bitmap_set ( s -> in_flight_bitmap , sector_num / sectors_per_chunk , nb_chunks ); <nl> while ( nb_chunks > 0 && sector_num < end ) { <nl> - int ret ; <nl> + int64_t ret ; <nl> int io_sectors , io_sectors_acct ; <nl> BlockDriverState * file ; <nl> enum MirrorMethod {
void usb_packet_complete ( USBDevice * dev , USBPacket * p ) <nl> { <nl> /* Note : p -> owner != dev is possible in case dev is a hub */ <nl> assert ( p -> owner != NULL ); <nl> - dev -> port -> ops -> complete ( dev -> port , p ); <nl> p -> owner = NULL ; <nl> + dev -> port -> ops -> complete ( dev -> port , p ); <nl> } <nl>  <nl> /* Cancel an active packet . The packed must have been deferred by
static int vnc_display_listen_addr ( VncDisplay * vd , <nl> qio_channel_set_name ( QIO_CHANNEL ( sioc ), name ); <nl> if ( qio_channel_socket_listen_sync ( <nl> sioc , rawaddrs [ i ], listenerr == NULL ? & listenerr : NULL ) < 0 ) { <nl> + object_unref ( OBJECT ( sioc )); <nl> continue ; <nl> } <nl> listening = true ;
BlockDriverAIOCB * dma_bdrv_io ( <nl> dbs -> sg_cur_index = 0 ; <nl> dbs -> sg_cur_byte = 0 ; <nl> dbs -> dir = dir ; <nl> + dbs -> in_cancel = false ; <nl> dbs -> io_func = io_func ; <nl> dbs -> bh = NULL ; <nl> qemu_iovec_init (& dbs -> iov , sg -> nsg );
static void free_test_data ( test_data * data ) <nl> g_free ( temp -> asl_file ); <nl> } <nl>  <nl> - g_array_free ( data -> tables , false ); <nl> + g_array_free ( data -> tables , true ); <nl> } <nl>  <nl> static uint8_t acpi_checksum ( const uint8_t * data , int len )
static void ahci_reg_init ( AHCIState * s ) <nl> s -> control_regs . cap = ( s -> ports - 1 ) | <nl> ( AHCI_NUM_COMMAND_SLOTS << 8 ) | <nl> ( AHCI_SUPPORTED_SPEED_GEN1 << AHCI_SUPPORTED_SPEED ) | <nl> - HOST_CAP_NCQ | HOST_CAP_AHCI ; <nl> + HOST_CAP_NCQ | HOST_CAP_AHCI | HOST_CAP_64 ; <nl>  <nl> s -> control_regs . impl = ( 1 << s -> ports ) - 1 ; <nl> 
long do_sigreturn ( CPUPPCState * env ) <nl> { <nl> struct target_sigcontext * sc = NULL ; <nl> struct target_mcontext * sr = NULL ; <nl> - target_ulong sr_addr , sc_addr ; <nl> + target_ulong sr_addr = 0 , sc_addr ; <nl> sigset_t blocked ; <nl> target_sigset_t set ; <nl> 
static int timebase_post_load ( void * opaque , int version_id ) <nl> host_ns = qemu_clock_get_ns ( QEMU_CLOCK_HOST ); <nl> ns_diff = MAX ( 0 , host_ns - tb_remote -> time_of_the_day_ns ); <nl> migration_duration_ns = MIN ( NANOSECONDS_PER_SECOND , ns_diff ); <nl> - migration_duration_tb = muldiv64 ( migration_duration_ns , freq , <nl> + migration_duration_tb = muldiv64 ( freq , migration_duration_ns , <nl> NANOSECONDS_PER_SECOND ); <nl> guest_tb = tb_remote -> guest_timebase + MIN ( 0 , migration_duration_tb ); <nl> 
static void vhost_user_cleanup ( NetClientState * nc ) <nl> s -> vhost_net = NULL ; <nl> } <nl> if ( nc -> queue_index == 0 ) { <nl> + if ( s -> watch ) { <nl> + g_source_remove ( s -> watch ); <nl> + s -> watch = 0 ; <nl> + } <nl> qemu_chr_fe_deinit (& s -> chr , true ); <nl> } <nl> 
static int pfpu_decode_insn ( MilkymistPFPUState * s ) <nl> uint32_t reg_b = ( insn >> 11 ) & 0x7f ; <nl> uint32_t op = ( insn >> 7 ) & 0xf ; <nl> uint32_t reg_d = insn & 0x7f ; <nl> - uint32_t r ; <nl> + uint32_t r = 0 ; <nl> int latency = 0 ; <nl>  <nl> switch ( op ) {
static int ppce500_load_device_tree ( MachineState * machine , <nl> } <nl>  <nl> fdt = load_device_tree ( filename , & fdt_size ); <nl> + g_free ( filename ); <nl> if (! fdt ) { <nl> goto out ; <nl> }
int qemu_devtree_add_subnode ( void * fdt , const char * name ) <nl> int retval ; <nl>  <nl> if (! basename ) { <nl> + g_free ( dupname ); <nl> return - 1 ; <nl> } <nl> 
uint16_t pvpanic_port ( void ) <nl> if (! o ) { <nl> return 0 ; <nl> } <nl> - return object_property_get_int ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> + return object_property_get_uint ( o , PVPANIC_IOPORT_PROP , NULL ); <nl> } <nl>  <nl> static Property pvpanic_isa_properties [] = {
static bool ga_open_pidfile ( const char * pidfile ) <nl> pidfd = open ( pidfile , O_CREAT | O_WRONLY , S_IRUSR | S_IWUSR ); <nl> if ( pidfd == - 1 || lockf ( pidfd , F_TLOCK , 0 )) { <nl> g_critical (" Cannot lock pid file , % s ", strerror ( errno )); <nl> + if ( pidfd != - 1 ) { <nl> + close ( pidfd ); <nl> + } <nl> return false ; <nl> } <nl> 
bool sysbus_has_irq ( SysBusDevice * dev , int n ) <nl> ObjectProperty * r ; <nl>  <nl> r = object_property_find ( OBJECT ( dev ), prop , NULL ); <nl> + g_free ( prop ); <nl> + <nl> return ( r != NULL ); <nl> } <nl> 
static bool blit_is_unsafe ( struct CirrusVGAState * s ) <nl> assert ( s -> cirrus_blt_width > 0 ); <nl> assert ( s -> cirrus_blt_height > 0 ); <nl>  <nl> + if ( s -> cirrus_blt_width > CIRRUS_BLTBUFSIZE ) { <nl> + return true ; <nl> + } <nl> + <nl> if ( blit_region_is_unsafe ( s , s -> cirrus_blt_dstpitch , <nl> s -> cirrus_blt_dstaddr & s -> cirrus_addr_mask )) { <nl> return true ;
static void ipmi_init_sensors_from_sdrs ( IPMIBmcSim * s ) <nl> static int ipmi_register_netfn ( IPMIBmcSim * s , unsigned int netfn , <nl> const IPMINetfn * netfnd ) <nl> { <nl> - if (( netfn & 1 ) || ( netfn > MAX_NETFNS ) || ( s -> netfns [ netfn / 2 ])) { <nl> + if (( netfn & 1 ) || ( netfn >= MAX_NETFNS ) || ( s -> netfns [ netfn / 2 ])) { <nl> return - 1 ; <nl> } <nl> s -> netfns [ netfn / 2 ] = netfnd ;
static void vnc_display_close ( VncDisplay * vs ) <nl> vs -> subauth = VNC_AUTH_INVALID ; <nl> if ( vs -> tlscreds ) { <nl> object_unparent ( OBJECT ( vs -> tlscreds )); <nl> + vs -> tlscreds = NULL ; <nl> } <nl> g_free ( vs -> tlsaclname ); <nl> vs -> tlsaclname = NULL ;
static void esp_pci_dma_memory_rw ( PCIESPState * pci , uint8_t * buf , int len , <nl> /* update status registers */ <nl> pci -> dma_regs [ DMA_WBC ] -= len ; <nl> pci -> dma_regs [ DMA_WAC ] += len ; <nl> - if ( pci -> dma_regs [ DMA_WBC ] == 0 ) <nl> + if ( pci -> dma_regs [ DMA_WBC ] == 0 ) { <nl> pci -> dma_regs [ DMA_STAT ] |= DMA_STAT_DONE ; <nl> + } <nl> } <nl>  <nl> static void esp_pci_dma_memory_read ( void * opaque , uint8_t * buf , int len )
static int qemu_rdma_broken_ipv6_kernel ( Error ** errp , struct ibv_context * verbs ) <nl>  <nl> for ( x = 0 ; x < num_devices ; x ++) { <nl> verbs = ibv_open_device ( dev_list [ x ]); <nl> + if (! verbs ) { <nl> + if ( errno == EPERM ) { <nl> + continue ; <nl> + } else { <nl> + return - EINVAL ; <nl> + } <nl> + } <nl>  <nl> if ( ibv_query_port ( verbs , 1 , & port_attr )) { <nl> ibv_close_device ( verbs );
static void spapr_nvram_class_init ( ObjectClass * klass , void * data ) <nl> set_bit ( DEVICE_CATEGORY_MISC , dc -> categories ); <nl> dc -> props = spapr_nvram_properties ; <nl> dc -> vmsd = & vmstate_spapr_nvram ; <nl> + /* Reason : Internal device only , uses spapr_rtas_register () in realize () */ <nl> + dc -> user_creatable = false ; <nl> } <nl>  <nl> static const TypeInfo spapr_nvram_type_info = {
static VncServerInfo * vnc_server_info_get ( VncDisplay * vd ) <nl> VncServerInfo * info ; <nl> Error * err = NULL ; <nl>  <nl> - info = g_malloc ( sizeof (* info )); <nl> + info = g_malloc0 ( sizeof (* info )); <nl> vnc_init_basic_info_from_server_addr ( vd -> lsock , <nl> qapi_VncServerInfo_base ( info ), & err ); <nl> info -> has_auth = true ;
void AUD_del_capture ( CaptureVoiceOut * cap , void * cb_opaque ) <nl> sw = sw1 ; <nl> } <nl> QLIST_REMOVE ( cap , entries ); <nl> + g_free ( cap -> hw . mix_buf ); <nl> + g_free ( cap -> buf ); <nl> g_free ( cap ); <nl> } <nl> return ;
static void ccw_machine_class_init ( ObjectClass * oc , void * data ) <nl> mc -> no_parallel = 1 ; <nl> mc -> no_sdcard = 1 ; <nl> mc -> use_sclp = 1 ; <nl> - mc -> max_cpus = 255 ; <nl> + mc -> max_cpus = 248 ; <nl> mc -> get_hotplug_handler = s390_get_hotplug_handler ; <nl> hc -> plug = s390_machine_device_plug ; <nl> nc -> nmi_monitor_handler = s390_nmi ;
void hid_reset ( HIDState * hs ) <nl> memset ( hs -> kbd . keycodes , 0 , sizeof ( hs -> kbd . keycodes )); <nl> memset ( hs -> kbd . key , 0 , sizeof ( hs -> kbd . key )); <nl> hs -> kbd . keys = 0 ; <nl> + hs -> kbd . modifiers = 0 ; <nl> break ; <nl> case HID_MOUSE : <nl> case HID_TABLET :
void qdev_init_nofail ( DeviceState * dev ) <nl>  <nl> assert (! dev -> realized ); <nl>  <nl> + object_ref ( OBJECT ( dev )); <nl> object_property_set_bool ( OBJECT ( dev ), true , " realized ", & err ); <nl> if ( err ) { <nl> error_reportf_err ( err , " Initialization of device % s failed : ", <nl> object_get_typename ( OBJECT ( dev ))); <nl> exit ( 1 ); <nl> } <nl> + object_unref ( OBJECT ( dev )); <nl> } <nl>  <nl> void qdev_machine_creation_done ( void )
int qemu_chr_be_can_write ( CharDriverState * s ) <nl>  <nl> void qemu_chr_be_write ( CharDriverState * s , uint8_t * buf , int len ) <nl> { <nl> - s -> chr_read ( s -> handler_opaque , buf , len ); <nl> + if ( s -> chr_read ) { <nl> + s -> chr_read ( s -> handler_opaque , buf , len ); <nl> + } <nl> } <nl>  <nl> int qemu_chr_fe_get_msgfd ( CharDriverState * s )
static void cpu_common_initfn ( Object * obj ) <nl>  <nl> cpu -> cpu_index = UNASSIGNED_CPU_INDEX ; <nl> cpu -> gdb_num_regs = cpu -> gdb_num_g_regs = cc -> gdb_num_core_regs ; <nl> + /* *- user doesn ' t have configurable SMP topology */ <nl> + /* the default value is changed by qemu_init_vcpu () for softmmu */ <nl> + cpu -> nr_cores = 1 ; <nl> + cpu -> nr_threads = 1 ; <nl> + <nl> qemu_mutex_init (& cpu -> work_mutex ); <nl> QTAILQ_INIT (& cpu -> breakpoints ); <nl> QTAILQ_INIT (& cpu -> watchpoints );
static void patch_instruction ( VAPICROMState * s , X86CPU * cpu , target_ulong ip ) <nl> CPUX86State * env = & cpu -> env ; <nl> VAPICHandlers * handlers ; <nl> uint8_t opcode [ 2 ]; <nl> - uint32_t imm32 ; <nl> + uint32_t imm32 = 0 ; <nl> target_ulong current_pc = 0 ; <nl> target_ulong current_cs_base = 0 ; <nl> uint32_t current_flags = 0 ;
vvfat_co_pwritev ( BlockDriverState * bs , uint64_t offset , uint64_t bytes , <nl> static int64_t coroutine_fn vvfat_co_get_block_status ( BlockDriverState * bs , <nl> int64_t sector_num , int nb_sectors , int * n , BlockDriverState ** file ) <nl> { <nl> - BDRVVVFATState * s = bs -> opaque ; <nl> - * n = s -> sector_count - sector_num ; <nl> + * n = bs -> total_sectors - sector_num ; <nl> if (* n > nb_sectors ) { <nl> * n = nb_sectors ; <nl> } else if (* n < 0 ) {
pixman_format_code_t qemu_default_pixman_format ( int bpp , bool native_endian ) <nl> break ; <nl> } <nl> } <nl> - g_assert_not_reached (); <nl> + return 0 ; <nl> } <nl>  <nl> int qemu_pixman_get_type ( int rshift , int gshift , int bshift )
void HELPER ( mtspr )( CPUOpenRISCState * env , target_ulong spr , target_ulong rb ) <nl> } <nl> break ; <nl> case TO_SPR ( 9 , 0 ): /* PICMR */ <nl> - env -> picmr |= rb ; <nl> + env -> picmr = rb ; <nl> break ; <nl> case TO_SPR ( 9 , 2 ): /* PICSR */ <nl> env -> picsr &= ~ rb ;
int mips_cpu_gdb_write_register ( CPUState * cs , uint8_t * mem_buf , int n ) <nl> return sizeof ( target_ulong ); <nl> } <nl> if ( env -> CP0_Config1 & ( 1 << CP0C1_FP ) <nl> - && n >= 38 && n < 73 ) { <nl> + && n >= 38 && n < 72 ) { <nl> if ( n < 70 ) { <nl> if ( env -> CP0_Status & ( 1 << CP0St_FR )) { <nl> env -> active_fpu . fpr [ n - 38 ]. d = tmp ;
vmxnet3_indicate_packet ( VMXNET3State * s ) <nl> struct Vmxnet3_RxDesc rxd ; <nl> bool is_head = true ; <nl> uint32_t rxd_idx ; <nl> - uint32_t rx_ridx ; <nl> + uint32_t rx_ridx = 0 ; <nl>  <nl> struct Vmxnet3_RxCompDesc rxcd ; <nl> uint32_t new_rxcd_gen = VMXNET3_INIT_GEN ;
void qpci_device_foreach ( QPCIBus * bus , int vendor_id , int device_id , <nl>  <nl> if ( vendor_id != - 1 && <nl> qpci_config_readw ( dev , PCI_VENDOR_ID ) != vendor_id ) { <nl> + g_free ( dev ); <nl> continue ; <nl> } <nl>  <nl> if ( device_id != - 1 && <nl> qpci_config_readw ( dev , PCI_DEVICE_ID ) != device_id ) { <nl> + g_free ( dev ); <nl> continue ; <nl> } <nl> 
static int mmu_translate_region ( CPUS390XState * env , target_ulong vaddr , <nl> __func__ , origin , offs , new_entry ); <nl>  <nl> if (( new_entry & _REGION_ENTRY_INV ) != 0 ) { <nl> - /* XXX different regions have different faults */ <nl> DPRINTF ("% s : invalid region \ n ", __func__ ); <nl> - trigger_page_fault ( env , vaddr , PGM_SEGMENT_TRANS , asc , rw , exc ); <nl> + trigger_page_fault ( env , vaddr , pchks [ level / 4 ], asc , rw , exc ); <nl> return - 1 ; <nl> } <nl> 
static void gen_add_A0_ds_seg ( DisasContext * s ) <nl> if ( s -> override >= 0 ) { <nl> override = s -> override ; <nl> must_add_seg = 1 ; <nl> - } else { <nl> - override = R_DS ; <nl> } <nl> if ( must_add_seg ) { <nl> # ifdef TARGET_X86_64
static int tpm_passthrough_unix_write ( int fd , const uint8_t * buf , uint32_t len ) <nl> int ret , remain ; <nl>  <nl> remain = len ; <nl> - while ( len > 0 ) { <nl> + while ( remain > 0 ) { <nl> ret = write ( fd , buf , remain ); <nl> if ( ret < 0 ) { <nl> if ( errno != EINTR && errno != EAGAIN ) {
static void ps2_reset_keyboard ( PS2KbdState * s ) <nl> trace_ps2_reset_keyboard ( s ); <nl> s -> scan_enabled = 1 ; <nl> s -> scancode_set = 2 ; <nl> + ps2_reset_queue (& s -> common ); <nl> ps2_set_ledstate ( s , 0 ); <nl> } <nl> 
static void pc_fw_add_pflash_drv ( void ) <nl> return ; <nl> } <nl>  <nl> - drive_init ( opts , machine -> use_scsi ); <nl> + if (! drive_init ( opts , machine -> use_scsi )) { <nl> + qemu_opts_del ( opts ); <nl> + } <nl> } <nl>  <nl> static void pc_system_flash_init ( MemoryRegion * rom_memory ,
static int cpu_x86_find_by_name ( x86_def_t * x86_cpu_def , const char * cpu_model ) <nl>  <nl> tsc_freq = strtosz_suffix_unit ( val , & err , <nl> STRTOSZ_DEFSUFFIX_B , 1000 ); <nl> - if (!* val || * err ) { <nl> + if ( tsc_freq < 0 || * err ) { <nl> fprintf ( stderr , " bad numerical value % s \ n ", val ); <nl> goto error ; <nl> }
static void gd_menu_grab_input ( GtkMenuItem * item , void * opaque ) <nl> VirtualConsole * vc = gd_vc_find_current ( s ); <nl>  <nl> if ( gd_is_grab_active ( s )) { <nl> - gd_grab_keyboard ( vc ); <nl> + if (! gd_grab_on_hover ( s )) { <nl> + gd_grab_keyboard ( vc ); <nl> + } <nl> gd_grab_pointer ( vc ); <nl> } else { <nl> gd_ungrab_keyboard ( s );
static void x86_cpuid_set_model_id ( CPUX86State * env , const char * model_id ) <nl> model_id = ""; <nl> } <nl> len = strlen ( model_id ); <nl> + memset ( env -> cpuid_model , 0 , 48 ); <nl> for ( i = 0 ; i < 48 ; i ++) { <nl> if ( i >= len ) { <nl> c = '\ 0 ';
int qemu_devtree_add_subnode ( void * fdt , const char * name ) <nl> } <nl>  <nl> retval = fdt_add_subnode ( fdt , parent , basename ); <nl> +# if 0 <nl> if ( retval < 0 ) { <nl> fprintf ( stderr , " FDT : Failed to create subnode % s : % s \ n ", name , <nl> fdt_strerror ( retval )); <nl> exit ( 1 ); <nl> } <nl> +# endif <nl>  <nl> g_free ( dupname ); <nl> return retval ;
static void register_subpage ( MemoryRegionSection * section ) <nl> subpage = container_of ( existing -> mr , subpage_t , iomem ); <nl> } <nl> start = section -> offset_within_address_space & ~ TARGET_PAGE_MASK ; <nl> - end = start + section -> size ; <nl> + end = start + section -> size - 1 ; <nl> subpage_register ( subpage , start , end , phys_section_add ( section )); <nl> } <nl> 
abi_long do_syscall ( void * cpu_env , int num , abi_long arg1 , <nl> # endif <nl> # ifdef TARGET_NR_pause /* not on alpha */ <nl> case TARGET_NR_pause : <nl> - ret = get_errno ( pause ()); <nl> + if (! block_signals ()) { <nl> + sigsuspend (&(( TaskState *) cpu -> opaque )-> signal_mask ); <nl> + } <nl> + ret = - TARGET_EINTR ; <nl> break ; <nl> # endif <nl> # ifdef TARGET_NR_utime
static void qxl_destroy_primary ( PCIQXLDevice * d ) <nl> dprint ( d , 1 , "% s \ n ", __FUNCTION__ ); <nl>  <nl> d -> mode = QXL_MODE_UNDEFINED ; <nl> + qemu_mutex_unlock_iothread (); <nl> d -> ssd . worker -> destroy_primary_surface ( d -> ssd . worker , 0 ); <nl> + qemu_mutex_lock_iothread (); <nl> } <nl>  <nl> static void qxl_set_mode ( PCIQXLDevice * d , int modenr , int loadvm )
# define MAX_TOKEN_COUNT ( 2ULL << 20 ) <nl> # define MAX_NESTING ( 1ULL << 10 ) <nl>  <nl> + static void json_message_free_token ( void * token , void * opaque ) <nl> +{ <nl> + g_free ( token ); <nl> +} <nl> + <nl> static void json_message_free_tokens ( JSONMessageParser * parser ) <nl> { <nl> if ( parser -> tokens ) { <nl> + g_queue_foreach ( parser -> tokens , json_message_free_token , NULL ); <nl> g_queue_free ( parser -> tokens ); <nl> parser -> tokens = NULL ; <nl> }
static void prop_get_fdt ( Object * obj , Visitor * v , const char * name , <nl> void * fdt ; <nl>  <nl> if (! drc -> fdt ) { <nl> - visit_start_struct ( v , name , NULL , 0 , & err ); <nl> - if (! err ) { <nl> - visit_end_struct ( v , & err ); <nl> - } <nl> - error_propagate ( errp , err ); <nl> + visit_type_null ( v , NULL , errp ); <nl> return ; <nl> } <nl> 
static int bdrv_check_byte_request ( BlockDriverState * bs , int64_t offset , <nl> static int bdrv_check_request ( BlockDriverState * bs , int64_t sector_num , <nl> int nb_sectors ) <nl> { <nl> + if ( nb_sectors > INT_MAX / BDRV_SECTOR_SIZE ) { <nl> + return - EIO ; <nl> + } <nl> + <nl> return bdrv_check_byte_request ( bs , sector_num * BDRV_SECTOR_SIZE , <nl> nb_sectors * BDRV_SECTOR_SIZE ); <nl> }
static uint64_t pl011_read ( void * opaque , target_phys_addr_t offset , <nl> if ( s -> read_count == s -> read_trigger - 1 ) <nl> s -> int_level &= ~ PL011_INT_RX ; <nl> pl011_update ( s ); <nl> - qemu_chr_accept_input ( s -> chr ); <nl> + if ( s -> chr ) { <nl> + qemu_chr_accept_input ( s -> chr ); <nl> + } <nl> return c ; <nl> case 1 : /* UARTCR */ <nl> return 0 ;
static void replay_save_event ( Event * event , int checkpoint ) <nl> replay_event_char_read_save ( event -> opaque ); <nl> break ; <nl> default : <nl> - error_report (" Unknown ID % d of replay event ", read_event_kind ); <nl> + error_report (" Unknown ID %" PRId64 " of replay event ", event -> id ); <nl> exit ( 1 ); <nl> } <nl> }
void helper_svm_check_intercept_param ( uint32_t type , uint64_t param ) <nl> switch (( uint32_t ) ECX ) { <nl> case 0 ... 0x1fff : <nl> t0 = ( ECX * 2 ) % 8 ; <nl> - t1 = ECX / 8 ; <nl> + t1 = ( ECX * 2 ) / 8 ; <nl> break ; <nl> case 0xc0000000 ... 0xc0001fff : <nl> t0 = ( 8192 + ECX - 0xc0000000 ) * 2 ;
void bdrv_drain_all ( void ) <nl> BlockDriverState * bs ; <nl>  <nl> while ( busy ) { <nl> - /* FIXME : We do not have timer support here , so this is effectively <nl> - * a busy wait . <nl> - */ <nl> QTAILQ_FOREACH ( bs , & bdrv_states , list ) { <nl> - if ( bdrv_start_throttled_reqs ( bs )) { <nl> - busy = true ; <nl> - } <nl> + bdrv_start_throttled_reqs ( bs ); <nl> } <nl>  <nl> busy = bdrv_requests_pending_all ();
static int coroutine_fn copy_sectors ( BlockDriverState * bs , <nl> BLKDBG_EVENT ( bs -> file , BLKDBG_COW_READ ); <nl>  <nl> if (! bs -> drv ) { <nl> - return - ENOMEDIUM ; <nl> + ret = - ENOMEDIUM ; <nl> + goto out ; <nl> } <nl>  <nl> /* Call . bdrv_co_readv () directly instead of using the public block - layer
# define XLNX_ZYNQ_DEVCFG ( obj ) \ <nl> OBJECT_CHECK ( XlnxZynqDevcfg , ( obj ), TYPE_XLNX_ZYNQ_DEVCFG ) <nl>  <nl> -# define XLNX_ZYNQ_DEVCFG_R_MAX 0x118 <nl> +# define XLNX_ZYNQ_DEVCFG_R_MAX ( 0x100 / 4 ) <nl>  <nl> # define XLNX_ZYNQ_DEVCFG_DMA_CMD_FIFO_LEN 10 <nl> 
static ssize_t qcow2_crypto_hdr_init_func ( QCryptoBlock * block , size_t headerlen , <nl> /* Zero fill remaining space in cluster so it has predictable <nl> * content in case of future spec changes */ <nl> clusterlen = size_to_clusters ( s , headerlen ) * s -> cluster_size ; <nl> + assert ( qcow2_pre_write_overlap_check ( bs , 0 , ret , clusterlen ) == 0 ); <nl> ret = bdrv_pwrite_zeroes ( bs -> file , <nl> ret + headerlen , <nl> clusterlen - headerlen , 0 );
static void shpc_interrupt_update ( PCIDevice * d ) <nl> for ( slot = 0 ; slot < shpc -> nslots ; ++ slot ) { <nl> uint8_t event = shpc -> config [ SHPC_SLOT_EVENT_LATCH ( slot )]; <nl> uint8_t disable = shpc -> config [ SHPC_SLOT_EVENT_SERR_INT_DIS ( d , slot )]; <nl> - uint32_t mask = 1 << SHPC_IDX_TO_LOGICAL ( slot ); <nl> + uint32_t mask = 1U << SHPC_IDX_TO_LOGICAL ( slot ); <nl> if ( event & ~ disable ) { <nl> int_locator |= mask ; <nl> }
static int qdev_add_one_global ( QemuOpts * opts , void * opaque ) <nl> g -> driver = qemu_opt_get ( opts , " driver "); <nl> g -> property = qemu_opt_get ( opts , " property "); <nl> g -> value = qemu_opt_get ( opts , " value "); <nl> - oc = object_class_by_name ( g -> driver ); <nl> + oc = object_class_dynamic_cast ( object_class_by_name ( g -> driver ), <nl> + TYPE_DEVICE ); <nl> if ( oc ) { <nl> DeviceClass * dc = DEVICE_CLASS ( oc ); <nl> 
static int alloc_refcount_block ( BlockDriverState * bs , <nl> uint64_t last_table_size ; <nl> uint64_t blocks_clusters ; <nl> do { <nl> - uint64_t table_clusters = size_to_clusters ( s , table_size ); <nl> + uint64_t table_clusters = <nl> + size_to_clusters ( s , table_size * sizeof ( uint64_t )); <nl> blocks_clusters = 1 + <nl> (( table_clusters + refcount_block_clusters - 1 ) <nl> / refcount_block_clusters );
static void vmmouse_reset ( DeviceState * d ) <nl>  <nl> s -> status = 0xffff ; <nl> s -> queue_size = VMMOUSE_QUEUE_SIZE ; <nl> + <nl> + vmmouse_disable ( s ); <nl> } <nl>  <nl> static int vmmouse_initfn ( ISADevice * dev )
static int connect_namedsocket ( const char * path ) <nl> size = strlen ( helper . sun_path ) + sizeof ( helper . sun_family ); <nl> if ( connect ( sockfd , ( struct sockaddr *)& helper , size ) < 0 ) { <nl> fprintf ( stderr , " socket error \ n "); <nl> + close ( sockfd ); <nl> return - 1 ; <nl> } <nl> 
 <nl> static inline bool virtio_access_is_big_endian ( VirtIODevice * vdev ) <nl> { <nl> +# if defined ( TARGET_IS_BIENDIAN ) <nl> + return virtio_is_big_endian ( vdev ); <nl> +# elif defined ( TARGET_WORDS_BIGENDIAN ) <nl> if ( virtio_vdev_has_feature ( vdev , VIRTIO_F_VERSION_1 )) { <nl> /* Devices conforming to VIRTIO 1 . 0 or later are always LE . */ <nl> return false ; <nl> } <nl> -# if defined ( TARGET_IS_BIENDIAN ) <nl> - return virtio_is_big_endian ( vdev ); <nl> -# elif defined ( TARGET_WORDS_BIGENDIAN ) <nl> return true ; <nl> # else <nl> return false ;
static int blkverify_open ( BlockDriverState * bs , QDict * options , int flags , <nl>  <nl> ret = 0 ; <nl> fail : <nl> + qemu_opts_del ( opts ); <nl> return ret ; <nl> } <nl> 
void s390_cpu_do_interrupt ( CPUState * cs ) <nl> do_mchk_interrupt ( env ); <nl> break ; <nl> } <nl> + <nl> + /* WAIT PSW during interrupt injection */ <nl> + if ( cs -> exception_index == EXCP_HLT ) { <nl> + /* don ' t trigger a cpu_loop_exit (), use an interrupt instead */ <nl> + cpu_interrupt ( CPU ( cpu ), CPU_INTERRUPT_HALT ); <nl> + } <nl> cs -> exception_index = - 1 ; <nl>  <nl> /* we might still have pending interrupts , but not deliverable */
static void blk_delete ( BlockBackend * blk ) <nl> assert (! blk -> refcnt ); <nl> assert (! blk -> name ); <nl> assert (! blk -> dev ); <nl> + if ( blk -> public . throttle_state ) { <nl> + blk_io_limits_disable ( blk ); <nl> + } <nl> if ( blk -> root ) { <nl> blk_remove_bs ( blk ); <nl> }
static int htab_load ( QEMUFile * f , void * opaque , int version_id ) <nl> section_hdr = qemu_get_be32 ( f ); <nl>  <nl> if ( section_hdr ) { <nl> - Error * local_err ; <nl> + Error * local_err = NULL ; <nl>  <nl> /* First section gives the htab size */ <nl> spapr_reallocate_hpt ( spapr , section_hdr , & local_err );
void qmp_drive_mirror ( const char * device , const char * target , <nl> return ; <nl> } <nl>  <nl> - if ( sync == MIRROR_SYNC_MODE_FULL && mode != NEW_IMAGE_MODE_EXISTING ) { <nl> + if (( sync == MIRROR_SYNC_MODE_FULL || ! source ) <nl> + && mode != NEW_IMAGE_MODE_EXISTING ) <nl> + { <nl> /* create new image w / o backing file */ <nl> assert ( format && drv ); <nl> bdrv_img_create ( target , format ,
static int qcrypto_ivgen_essiv_calculate ( QCryptoIVGen * ivgen , <nl> uint8_t * data = g_new ( uint8_t , ndata ); <nl>  <nl> sector = cpu_to_le64 ( sector ); <nl> - memcpy ( data , ( uint8_t *)& sector , ndata ); <nl> + memcpy ( data , ( uint8_t *)& sector , MIN ( sizeof ( sector ), ndata )); <nl> if ( sizeof ( sector ) < ndata ) { <nl> memset ( data + sizeof ( sector ), 0 , ndata - sizeof ( sector )); <nl> }
static void process_event ( JSONMessageParser * parser , QList * tokens ) <nl> error_free ( err ); <nl> } <nl> ret = send_response ( s , QOBJECT ( qdict )); <nl> - if ( ret ) { <nl> - g_warning (" error sending error response : % s ", strerror ( ret )); <nl> + if ( ret < 0 ) { <nl> + g_warning (" error sending error response : % s ", strerror (- ret )); <nl> } <nl> } <nl> 
static inline void powerpc_excp ( CPUPPCState * env , int excp_model , int excp ) <nl> if ( asrr1 != - 1 ) <nl> env -> spr [ asrr1 ] = env -> spr [ srr1 ]; <nl> /* If we disactivated any translation , flush TLBs */ <nl> - if ( new_msr & (( 1 << MSR_IR ) | ( 1 << MSR_DR ))) <nl> + if ( msr & (( 1 << MSR_IR ) | ( 1 << MSR_DR ))) <nl> tlb_flush ( env , 1 ); <nl>  <nl> if ( msr_ile ) {
# ifndef BITMAP_H <nl> # define BITMAP_H <nl>  <nl> -# include " qemu - common . h " <nl> +# include < glib . h > <nl> +# include < string . h > <nl> +# include < stdlib . h > <nl> + <nl> +# include " qemu / osdep . h " <nl> # include " qemu / bitops . h " <nl>  <nl> /*
typedef struct QObject { <nl>  <nl> /* High - level interface for qobject_decref () */ <nl> # define QDECREF ( obj ) \ <nl> - qobject_decref ( QOBJECT ( obj )) <nl> + qobject_decref ( obj ? QOBJECT ( obj ) : NULL ) <nl>  <nl> /* Initialize an object to default values */ <nl> # define QOBJECT_INIT ( obj , qtype_type ) \
int bdrv_aio_multiwrite ( BlockDriverState * bs , BlockRequest * reqs , int num_reqs ) <nl> reqs [ i ]. error = - EIO ; <nl> goto fail ; <nl> } else { <nl> - mcb -> error = - EIO ; <nl> + mcb -> num_requests ++; <nl> + multiwrite_cb ( mcb , - EIO ); <nl> break ; <nl> } <nl> } else {
static void tcg_out_qemu_st ( TCGContext * s , const TCGArg * args , int opc ) <nl> # else <nl> tcg_out_mov ( s , TCG_TYPE_I32 , 3 , addr_reg2 ); <nl> tcg_out_mov ( s , TCG_TYPE_I32 , 4 , addr_reg ); <nl> -# ifdef TCG_TARGET_CALL_ALIGN_ARGS <nl> ir = 5 ; <nl> -# else <nl> - ir = 4 ; <nl> -# endif <nl> # endif <nl>  <nl> switch ( opc ) {
static int read_directory ( BDRVVVFATState * s , int mapping_index ) <nl> if ( st . st_size > 0x7fffffff ) { <nl> fprintf ( stderr , " File % s is larger than 2GB \ n ", buffer ); <nl> free ( buffer ); <nl> + closedir ( dir ); <nl> return - 2 ; <nl> } <nl> direntry -> size = cpu_to_le32 ( S_ISDIR ( st . st_mode )? 0 : st . st_size );
static void set_year_20xx ( void ) <nl> g_assert_cmpint ( cmos_read ( RTC_YEAR ), ==, 0x11 ); <nl> g_assert_cmpint ( cmos_read ( RTC_CENTURY ), ==, 0x20 ); <nl>  <nl> + if ( sizeof ( time_t ) == 4 ) { <nl> + return ; <nl> + } <nl> + <nl> /* Set a date in 2080 to ensure there is no year - 2038 overflow . */ <nl> cmos_write ( RTC_REG_A , 0x76 ); <nl> cmos_write ( RTC_YEAR , 0x80 );
static void do_sdl_resize ( int new_width , int new_height , int bpp ) <nl>  <nl> // printf (" resizing to % d % d \ n ", w , h ); <nl>  <nl> - flags = SDL_HWSURFACE | SDL_ASYNCBLIT | SDL_HWACCEL | SDL_RESIZABLE ; <nl> - if ( gui_fullscreen ) <nl> + flags = SDL_HWSURFACE | SDL_ASYNCBLIT | SDL_HWACCEL ; <nl> + if ( gui_fullscreen ) { <nl> flags |= SDL_FULLSCREEN ; <nl> + } else { <nl> + flags |= SDL_RESIZABLE ; <nl> + } <nl> if ( gui_noframe ) <nl> flags |= SDL_NOFRAME ; <nl> 
int spapr_h_cas_compose_response ( sPAPRMachineState * spapr , <nl> return 1 ; <nl> } <nl>  <nl> + if ( size < sizeof ( hdr ) || size > FW_MAX_SIZE ) { <nl> + error_report (" SLOF provided an unexpected CAS buffer size " <nl> + TARGET_FMT_lu " ( min : % zu , max : % u )", <nl> + size , sizeof ( hdr ), FW_MAX_SIZE ); <nl> + exit ( EXIT_FAILURE ); <nl> + } <nl> + <nl> size -= sizeof ( hdr ); <nl>  <nl> /* Create skeleton */
int cpu_get_pic_interrupt ( CPUX86State * env ) <nl> /* Make sure everything is in a consistent state for calling fork (). */ <nl> void fork_start ( void ) <nl> { <nl> - cpu_list_lock (); <nl> - qemu_mutex_lock (& tb_ctx . tb_lock ); <nl> mmap_fork_start (); <nl> + qemu_mutex_lock (& tb_ctx . tb_lock ); <nl> + cpu_list_lock (); <nl> } <nl>  <nl> void fork_end ( int child )
void configure_icount ( const char * option ) <nl>  <nl> void qemu_run_all_timers ( void ) <nl> { <nl> + alarm_timer -> pending = 0 ; <nl> + <nl> /* rearm timer , if not periodic */ <nl> if ( alarm_timer -> expired ) { <nl> alarm_timer -> expired = 0 ; <nl> qemu_rearm_alarm_timer ( alarm_timer ); <nl> } <nl>  <nl> - alarm_timer -> pending = 0 ; <nl> - <nl> /* vm time timers */ <nl> if ( vm_running ) { <nl> qemu_run_timers ( vm_clock );
static GIOStatus ga_channel_write ( GAChannel * c , const char * buf , size_t size , <nl> * count = written ; <nl> } <nl>  <nl> + if ( ov . hEvent ) { <nl> + CloseHandle ( ov . hEvent ); <nl> + ov . hEvent = NULL ; <nl> + } <nl> return status ; <nl> } <nl> 
static int connect_to_ssh ( BDRVSSHState * s , QDict * options , <nl> /* Open the socket and connect . */ <nl> s -> sock = inet_connect ( s -> hostport , errp ); <nl> if ( s -> sock < 0 ) { <nl> - ret = - errno ; <nl> + ret = - EIO ; <nl> goto err ; <nl> } <nl> 
int kvm_cpu_exec ( CPUState * cpu ) <nl> qemu_system_reset_request (); <nl> ret = EXCP_INTERRUPT ; <nl> break ; <nl> + case KVM_SYSTEM_EVENT_CRASH : <nl> + qemu_mutex_lock_iothread (); <nl> + qemu_system_guest_panicked (); <nl> + qemu_mutex_unlock_iothread (); <nl> + ret = 0 ; <nl> + break ; <nl> default : <nl> DPRINTF (" kvm_arch_handle_exit \ n "); <nl> ret = kvm_arch_handle_exit ( cpu , run );
static int usb_device_post_load ( void * opaque , int version_id ) <nl> } else { <nl> dev -> attached = 1 ; <nl> } <nl> - if ( dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> + if ( dev -> setup_index < 0 || <nl> + dev -> setup_len < 0 || <nl> + dev -> setup_index >= sizeof ( dev -> data_buf ) || <nl> dev -> setup_len >= sizeof ( dev -> data_buf )) { <nl> return - EINVAL ; <nl> }
static void set_pixel_format ( VncState * vs , <nl> return ; <nl> } <nl>  <nl> + switch ( bits_per_pixel ) { <nl> + case 8 : <nl> + case 16 : <nl> + case 32 : <nl> + break ; <nl> + default : <nl> + vnc_client_error ( vs ); <nl> + return ; <nl> + } <nl> + <nl> vs -> client_pf . rmax = red_max ; <nl> vs -> client_pf . rbits = hweight_long ( red_max ); <nl> vs -> client_pf . rshift = red_shift ;
uint64_t pc_dimm_get_free_addr ( uint64_t address_space_start , <nl> uint64_t new_addr , ret = 0 ; <nl> uint64_t address_space_end = address_space_start + address_space_size ; <nl>  <nl> - assert ( address_space_end > address_space_size ); <nl> + if (! address_space_size ) { <nl> + error_setg ( errp , " memory hotplug is not enabled , " <nl> + " please add maxmem option "); <nl> + goto out ; <nl> + } <nl> + <nl> + assert ( address_space_end > address_space_start ); <nl> object_child_foreach ( qdev_get_machine (), pc_dimm_built_list , & list ); <nl>  <nl> if ( hint ) {
e1000e_init_msix ( E1000EState * s ) <nl> static void <nl> e1000e_cleanup_msix ( E1000EState * s ) <nl> { <nl> - if ( msix_enabled ( PCI_DEVICE ( s ))) { <nl> + if ( msix_present ( PCI_DEVICE ( s ))) { <nl> e1000e_unuse_msix_vectors ( s , E1000E_MSIX_VEC_NUM ); <nl> msix_uninit ( PCI_DEVICE ( s ), & s -> msix , & s -> msix ); <nl> }
static int slirp_guestfwd ( SlirpState * s , const char * config_str , <nl> } <nl>  <nl> fwd = qemu_malloc ( sizeof ( struct GuestFwd )); <nl> - snprintf ( buf , sizeof ( buf ), " guestfwd . tcp :% d ", port ); <nl> + snprintf ( buf , sizeof ( buf ), " guestfwd . tcp .% d ", port ); <nl> fwd -> hd = qemu_chr_open ( buf , p , NULL ); <nl> if (! fwd -> hd ) { <nl> error_report (" could not open guest forwarding device '% s '", buf );
static void vhost_scsi_stop ( VHostSCSI * s ) <nl> VirtioBusClass * k = VIRTIO_BUS_GET_CLASS ( qbus ); <nl> int ret = 0 ; <nl>  <nl> - if (! k -> set_guest_notifiers ) { <nl> + if ( k -> set_guest_notifiers ) { <nl> ret = k -> set_guest_notifiers ( qbus -> parent , s -> dev . nvqs , false ); <nl> if ( ret < 0 ) { <nl> error_report (" vhost guest notifier cleanup failed : % d \ n ", ret );
static void rtas_ibm_os_term ( PowerPCCPU * cpu , <nl> target_ulong args , <nl> uint32_t nret , target_ulong rets ) <nl> { <nl> - target_ulong ret = 0 ; <nl> + qemu_system_guest_panicked ( NULL ); <nl>  <nl> - qapi_event_send_guest_panicked ( GUEST_PANIC_ACTION_PAUSE , false , NULL , <nl> - & error_abort ); <nl> - <nl> - rtas_st ( rets , 0 , ret ); <nl> + rtas_st ( rets , 0 , RTAS_OUT_SUCCESS ); <nl> } <nl>  <nl> static void rtas_set_power_level ( PowerPCCPU * cpu , sPAPRMachineState * spapr ,
int qdev_simple_unplug_cb ( DeviceState * dev ) <nl> way is somewhat unclean , and best avoided . */ <nl> void qdev_init_nofail ( DeviceState * dev ) <nl> { <nl> + const char * typename = object_get_typename ( OBJECT ( dev )); <nl> + <nl> if ( qdev_init ( dev ) < 0 ) { <nl> - error_report (" Initialization of device % s failed ", <nl> - object_get_typename ( OBJECT ( dev ))); <nl> + error_report (" Initialization of device % s failed ", typename ); <nl> exit ( 1 ); <nl> } <nl> }
ram_addr_t qemu_ram_alloc_from_ptr ( ram_addr_t size , void * host , <nl>  <nl> ram_list . phys_dirty = g_realloc ( ram_list . phys_dirty , <nl> last_ram_offset () >> TARGET_PAGE_BITS ); <nl> + memset ( ram_list . phys_dirty + ( new_block -> offset >> TARGET_PAGE_BITS ), <nl> + 0 , size >> TARGET_PAGE_BITS ); <nl> cpu_physical_memory_set_dirty_range ( new_block -> offset , size , 0xff ); <nl>  <nl> if ( kvm_enabled ())
static bool is_zero_cluster ( BlockDriverState * bs , int64_t start ) <nl> BlockDriverState * file ; <nl> int64_t res = bdrv_get_block_status_above ( bs , NULL , start , <nl> s -> cluster_sectors , & nr , & file ); <nl> - return res >= 0 && (( res & BDRV_BLOCK_ZERO ) || !( res & BDRV_BLOCK_DATA )); <nl> + return res >= 0 && ( res & BDRV_BLOCK_ZERO ); <nl> } <nl>  <nl> static bool is_zero_cluster_top_locked ( BlockDriverState * bs , int64_t start )
static void spapr_tce_table_class_init ( ObjectClass * klass , void * data ) <nl> dc -> init = spapr_tce_table_realize ; <nl> dc -> reset = spapr_tce_reset ; <nl> dc -> unrealize = spapr_tce_table_unrealize ; <nl> + /* Reason : This is just an internal device for handling the hypercalls */ <nl> + dc -> user_creatable = false ; <nl>  <nl> QLIST_INIT (& spapr_tce_tables ); <nl> 
static void ehci_advance_state ( EHCIState * ehci , <nl> fprintf ( stderr , " processing error - resetting ehci HC \ n "); <nl> ehci_reset ( ehci ); <nl> again = 0 ; <nl> - assert ( 0 ); <nl> } <nl> } <nl> while ( again );
static int qcow2_do_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail ; <nl> } <nl>  <nl> + if ( header . refcount_table_clusters == 0 && !( flags & BDRV_O_CHECK )) { <nl> + error_setg ( errp , " Image does not contain a reference count table "); <nl> + ret = - EINVAL ; <nl> + goto fail ; <nl> + } <nl> + <nl> ret = validate_table_offset ( bs , s -> refcount_table_offset , <nl> s -> refcount_table_size , sizeof ( uint64_t )); <nl> if ( ret < 0 ) {
static int ehci_init_transfer ( EHCIPacket * p ) <nl> while ( bytes > 0 ) { <nl> if ( cpage > 4 ) { <nl> fprintf ( stderr , " cpage out of range (% d )\ n ", cpage ); <nl> + qemu_sglist_destroy (& p -> sgl ); <nl> return - 1 ; <nl> } <nl> 
static void pcie_cap_slot_hotplug_common ( PCIDevice * hotplug_dev , <nl> /* the slot is electromechanically locked . <nl> * This error is propagated up to qdev and then to HMP / QMP . <nl> */ <nl> - error_setg_errno ( errp , - EBUSY , " slot is electromechanically locked "); <nl> + error_setg_errno ( errp , EBUSY , " slot is electromechanically locked "); <nl> } <nl> } <nl> 
found : <nl> QTAILQ_REMOVE (& s -> discards , p , next ); <nl> d -> offset = MIN ( d -> offset , p -> offset ); <nl> d -> bytes += p -> bytes ; <nl> + g_free ( p ); <nl> } <nl> } <nl> 
static inline void cpu_physical_memory_set_dirty_lebitmap ( unsigned long * bitmap , <nl> unsigned long page = BIT_WORD ( start >> TARGET_PAGE_BITS ); <nl>  <nl> /* start address is aligned at the start of a word ? */ <nl> - if ((( page * BITS_PER_LONG ) << TARGET_PAGE_BITS ) == start ) { <nl> + if (((( page * BITS_PER_LONG ) << TARGET_PAGE_BITS ) == start ) && <nl> + ( hpratio == 1 )) { <nl> long k ; <nl> long nr = BITS_TO_LONGS ( pages ); <nl> 
static void update_irq ( struct xlx_pic * p ) <nl>  <nl> /* Update the vector register . */ <nl> for ( i = 0 ; i < 32 ; i ++) { <nl> - if ( p -> regs [ R_IPR ] & ( 1 << i )) <nl> + if ( p -> regs [ R_IPR ] & ( 1U << i )) { <nl> break ; <nl> + } <nl> } <nl> if ( i == 32 ) <nl> i = ~ 0 ;
void cpu_x86_cpuid ( CPUX86State * env , uint32_t index , uint32_t count , <nl> index = env -> cpuid_xlevel ; <nl> } <nl> } else { <nl> - index = env -> cpuid_xlevel ; <nl> + /* Intel documentation states that invalid EAX input will <nl> + * return the same information as EAX = cpuid_level <nl> + * ( Intel SDM Vol . 2A - Instruction Set Reference - CPUID ) <nl> + */ <nl> + index = env -> cpuid_level ; <nl> } <nl> } <nl> } else {
static int usb_msd_handle_control ( USBDevice * dev , USBPacket * p , <nl> static void usb_msd_cancel_io ( USBDevice * dev , USBPacket * p ) <nl> { <nl> MSDState * s = DO_UPCAST ( MSDState , dev , dev ); <nl> - scsi_req_cancel ( s -> req ); <nl> + <nl> + if ( s -> req ) { <nl> + scsi_req_cancel ( s -> req ); <nl> + } <nl> } <nl>  <nl> static int usb_msd_handle_data ( USBDevice * dev , USBPacket * p )
float64 HELPER ( recpe_f64 )( float64 input , void * fpstp ) <nl> } else { <nl> return float64_set_sign ( float64_maxnorm , float64_is_neg ( f64 )); <nl> } <nl> - } else if ( f64_exp >= 1023 && fpst -> flush_to_zero ) { <nl> + } else if ( f64_exp >= 2045 && fpst -> flush_to_zero ) { <nl> float_raise ( float_flag_underflow , fpst ); <nl> return float64_set_sign ( float64_zero , float64_is_neg ( f64 )); <nl> }
static BlockDriverAIOCB * bdrv_aio_rw_vector ( BlockDriverState * bs , <nl> acb -> is_write = is_write ; <nl> acb -> qiov = qiov ; <nl> acb -> bounce = qemu_blockalign ( bs , qiov -> size ); <nl> - <nl> - if (! acb -> bh ) <nl> - acb -> bh = qemu_bh_new ( bdrv_aio_bh_cb , acb ); <nl> + acb -> bh = qemu_bh_new ( bdrv_aio_bh_cb , acb ); <nl>  <nl> if ( is_write ) { <nl> qemu_iovec_to_buffer ( acb -> qiov , acb -> bounce );
static int apic_init_common ( SysBusDevice * dev ) <nl>  <nl> sysbus_init_mmio ( dev , & s -> io_memory ); <nl>  <nl> - if (! vapic && s -> vapic_control & VAPIC_ENABLE_MASK ) { <nl> + /* Note : We need at least 1M to map the VAPIC option ROM */ <nl> + if (! vapic && s -> vapic_control & VAPIC_ENABLE_MASK && <nl> + ram_size >= 1024 * 1024 ) { <nl> vapic = sysbus_create_simple (" kvmvapic ", - 1 , NULL ); <nl> } <nl> s -> vapic = vapic ;
const char * bdrv_get_device_name ( BlockDriverState * bs ) <nl>  <nl> void bdrv_flush ( BlockDriverState * bs ) <nl> { <nl> + if (! bs -> drv ) <nl> + return ; <nl> if ( bs -> drv -> bdrv_flush ) <nl> bs -> drv -> bdrv_flush ( bs ); <nl> if ( bs -> backing_hd )
void ahci_uninit ( AHCIState * s ) <nl>  <nl> ide_exit ( s ); <nl> } <nl> + object_unparent ( OBJECT (& ad -> port )); <nl> } <nl>  <nl> g_free ( s -> dev );
static void spapr_add_lmbs ( DeviceState * dev , uint64_t addr , uint64_t size , <nl>  <nl> drck = SPAPR_DR_CONNECTOR_GET_CLASS ( drc ); <nl> drck -> attach ( drc , dev , fdt , fdt_offset , ! dev -> hotplugged , errp ); <nl> - spapr_hotplug_req_add_by_index ( drc ); <nl> addr += SPAPR_MEMORY_BLOCK_SIZE ; <nl> } <nl> + spapr_hotplug_req_add_by_count ( SPAPR_DR_CONNECTOR_TYPE_LMB , nr_lmbs ); <nl> } <nl>  <nl> static void spapr_memory_plug ( HotplugHandler * hotplug_dev , DeviceState * dev ,
void shpc_cleanup ( PCIDevice * d , MemoryRegion * bar ) <nl> SHPCDevice * shpc = d -> shpc ; <nl> d -> cap_present &= ~ QEMU_PCI_CAP_SHPC ; <nl> memory_region_del_subregion ( bar , & shpc -> mmio ); <nl> + object_unparent ( OBJECT (& shpc -> mmio )); <nl> /* TODO : cleanup config space changes ? */ <nl> g_free ( shpc -> config ); <nl> g_free ( shpc -> cmask );
static GuestPCIAddress * get_pci_info ( char * guid , Error ** errp ) <nl> } else { <nl> error_setg_win32 ( errp , GetLastError (), <nl> " failed to get device name "); <nl> - goto out ; <nl> + goto free_dev_info ; <nl> } <nl> } <nl>  <nl> static GuestPCIAddress * get_pci_info ( char * guid , Error ** errp ) <nl> pci -> bus = bus ; <nl> break ; <nl> } <nl> + <nl> + free_dev_info : <nl> + SetupDiDestroyDeviceInfoList ( dev_info ); <nl> out : <nl> g_free ( buffer ); <nl> g_free ( name );
int ppc_compat_max_threads ( PowerPCCPU * cpu ); <nl> # define SPR_601_UDECR ( 0x006 ) <nl> # define SPR_LR ( 0x008 ) <nl> # define SPR_CTR ( 0x009 ) <nl> -# define SPR_UAMR ( 0x00C ) <nl> +# define SPR_UAMR ( 0x00D ) <nl> # define SPR_DSCR ( 0x011 ) <nl> # define SPR_DSISR ( 0x012 ) <nl> # define SPR_DAR ( 0x013 ) /* DAE for PowerPC 601 */
static int iscsi_truncate ( BlockDriverState * bs , int64_t offset ) <nl> if ( iscsilun -> allocationmap != NULL ) { <nl> g_free ( iscsilun -> allocationmap ); <nl> iscsilun -> allocationmap = <nl> - bitmap_new ( DIV_ROUND_UP ( bs -> total_sectors , <nl> + bitmap_new ( DIV_ROUND_UP ( sector_lun2qemu ( iscsilun -> num_blocks , <nl> + iscsilun ), <nl> iscsilun -> cluster_sectors )); <nl> } <nl> 
sofree ( struct socket * so ) <nl> if ( so -> so_next && so -> so_prev ) <nl> remque ( so ); /* crashes if so is not in a queue */ <nl>  <nl> + if ( so -> so_tcpcb ) { <nl> + free ( so -> so_tcpcb ); <nl> + } <nl> free ( so ); <nl> } <nl> 
static void sd_reset ( SDState * sd , BlockDriverState * bdrv ) <nl> } else { <nl> sect = 0 ; <nl> } <nl> - sect <<= 9 ; <nl> - <nl> - size = sect + 1 ; <nl> + size = sect << 9 ; <nl>  <nl> sect = ( size >> ( HWBLOCK_SHIFT + SECTOR_SHIFT + WPGROUP_SHIFT )) + 1 ; <nl> 
int main ( int argc , char ** argv ) <nl> root = tmpfs ; <nl> } <nl>  <nl> - socket_path = g_strdup_printf ("/ tmp / vhost -% d . sock ", getpid ()); <nl> + socket_path = g_strdup_printf ("% s / vhost . sock ", tmpfs ); <nl>  <nl> /* create char dev and add read handlers */ <nl> qemu_add_opts (& qemu_chardev_opts );
static int vmdk_snapshot_create ( const char * filename , const char * backing_file ) <nl> ret = - errno ; <nl> goto fail_rgd ; <nl> } <nl> - qemu_free ( rgd_buf ); <nl>  <nl> /* write GD */ <nl> gd_buf = qemu_malloc ( gd_size ); <nl> static int vmdk_snapshot_create ( const char * filename , const char * backing_file ) <nl> goto fail_gd ; <nl> } <nl> qemu_free ( gd_buf ); <nl> + qemu_free ( rgd_buf ); <nl>  <nl> close ( p_fd ); <nl> close ( snp_fd );
static int parallels_open ( BlockDriverState * bs , QDict * options , int flags , <nl> goto fail_options ; <nl> } <nl>  <nl> - if (!( flags & BDRV_O_RESIZE ) || ! bdrv_has_zero_init ( bs -> file -> bs ) || <nl> - bdrv_truncate ( bs -> file , bdrv_getlength ( bs -> file -> bs ), <nl> - PREALLOC_MODE_OFF , NULL ) != 0 ) { <nl> + if (! bdrv_has_zero_init ( bs -> file -> bs )) { <nl> s -> prealloc_mode = PRL_PREALLOC_MODE_FALLOCATE ; <nl> } <nl> 
uint32_t gic_acknowledge_irq ( GICState * s , int cpu ) <nl> } <nl> s -> last_active [ irq ][ cpu ] = s -> running_irq [ cpu ]; <nl>  <nl> - if ( s -> revision == REV_11MPCORE ) { <nl> + if ( s -> revision == REV_11MPCORE || s -> revision == REV_NVIC ) { <nl> /* Clear pending flags for both level and edge triggered interrupts . <nl> * Level triggered IRQs will be reasserted once they become inactive . <nl> */
GEN_HANDLER ( tlbiva , 0x1F , 0x12 , 0x18 , 0x03FFF801 , PPC_TLBIVA ) <nl> GEN_EXCP_PRIVOPC ( ctx ); <nl> return ; <nl> } <nl> + t0 = tcg_temp_new (); <nl> gen_addr_reg_index ( t0 , ctx ); <nl> # if defined ( TARGET_PPC64 ) <nl> if (! ctx -> sf_mode )
int main ( int argc , char ** argv ) { <nl> datalen = 0 ; <nl> } <nl> code = malloc ( strlen ( p )+ 1 ); <nl> + if (! code ) { <nl> + return 1 ; <nl> + } <nl> codelen = r_hex_str2bin ( p , code ); <nl> if (! arch ) arch = " x86 "; <nl> if (! bits ) bits = 32 ;
static int r_core_search_rop ( RCore * core , ut64 from , ut64 to , int opt , const cha <nl> } <nl> if ( mode == ' j ') <nl> r_cons_printf ("]\ n "); <nl> + free ( buf ); <nl> return R_TRUE ; <nl> } <nl> 
static int cmd_print ( void * data , const char * input ) { <nl> free ( loc_buf ); <nl> } <nl> r_list_foreach ( f -> bbs , locs_it , b ) { <nl> - if (! first ) { <nl> + if ( first ) { <nl> first = false ; <nl> } else { <nl> r_cons_print (",");
static char * r_debug_bf_reg_profile ( RDebug * dbg ) { <nl> ); <nl> } <nl>  <nl> - static int r_debug_bf_breakpoint ( void * bp , RBreakpointItem * b , bool set ) { <nl> + static int r_debug_bf_breakpoint ( struct r_bp_t * bp , RBreakpointItem * b , bool set ) { <nl> // r_io_system ( dbg -> iob . io , " db "); <nl> return false ; <nl> }
static void r_core_debug_kill ( RCore * core , const char * input ) { <nl> // - trace <nl> // - stop <nl> if ( signum < 1 ) signum = r_debug_signal_resolve ( core -> dbg , name ); <nl> - if ( signal > 0 ) { <nl> + if ( signum > 0 ) { <nl> int sigopt = 0 ; <nl> if ( strchr ( p , ' s ')) sigopt |= R_DBG_SIGNAL_SKIP ; <nl> if ( strchr ( p , ' c ')) sigopt |= R_DBG_SIGNAL_CONT ;
parse_new ( struct vcc * tl ) <nl> vcc_ErrWhere ( tl , tl -> t ); <nl> return ; <nl> } <nl> - XXXAZ ( sy1 ); <nl>  <nl> sy1 = VCC_AddSymbolTok ( tl , tl -> t , SYM_NONE ); // XXX : NONE ? <nl> XXXAN ( sy1 );
void team :: team_info :: write ( config & cfg ) const <nl> cfg [" hidden "] = hidden ; <nl> cfg [" suppress_end_turn_confirmation "] = no_turn_confirmation ; <nl> cfg [" scroll_to_leader "] = scroll_to_leader ; <nl> - cfg [" controller "] = controller_string (); <nl> + cfg [" controller "] = ( controller == IDLE ? " human " : controller_string ()); <nl>  <nl> std :: stringstream can_recruit_str ; <nl> for ( std :: set < std :: string >:: const_iterator cr = can_recruit . begin (); cr != can_recruit . end (); ++ cr ) {
void textbox :: set_text ( std :: string text ) <nl> { <nl> text_ = string_to_wstring ( text ); <nl> cursor_ = text_ . size (); <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> } <nl> void textbox :: clear () <nl> cursor_ = 0 ; <nl> cursor_pos_ = 0 ; <nl> text_pos_ = 0 ; <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> }
WML_HANDLER_FUNCTION ( replace_schedule , /* event_info */, cfg ) <nl> ERR_NG << " attempted to to replace ToD schedule with empty schedule \ n "; <nl> } else { <nl> resources :: tod_manager -> replace_schedule ( cfg . get_parsed_config ()); <nl> + resources :: screen -> new_turn (); <nl> LOG_NG << " replaced ToD schedule \ n "; <nl> } <nl> }
namespace game_config <nl> namespace sounds { <nl> const std :: string turn_bell = " bell . wav ", <nl> timer_bell = " timer . wav ", <nl> - receive_message = " chat - 3 . ogg ", <nl> + receive_message = " chat - 1 . ogg , chat - 2 . ogg , chat - 3 . ogg , chat - 4 . ogg ", <nl> receive_message_highlight = " chat - highlight . ogg ", <nl> receive_message_friend = " chat - 4 . ogg ", <nl> receive_message_server = " receive . wav ",
loadscreen :: loadscreen ( CVideo & screen , const int & percent ): <nl> setconfig_counter ( 0 ), <nl> parser_counter ( 0 ), <nl> screen_ ( screen ), <nl> + textarea_ (), <nl> + logo_surface_ ( NULL ), <nl> logo_drawn_ ( false ), <nl> pby_offset_ ( 0 ), <nl> prcnt_ ( percent )
void play_multiplayer ( display & disp , game_data & units_data , config cfg , <nl> } else if ( result < int ( choices . size ()/ 3 )* 2 ) { <nl> controller = " ai "; <nl> result -= choices . size ()/ 3 ; <nl> + sides [ res ]-> values [" description "] = ""; <nl> } else { <nl> controller = " network "; <nl> result -= ( choices . size ()/ 3 )* 2 ;
namespace { <nl> bool terrain_matches_filter ( const gamemap & map , const gamemap :: location & loc , const vconfig & cfg , <nl> const gamestatus & game_status , const unit_map & units , const bool flat_tod , <nl> const size_t max_loop ) <nl> -{ <nl> +{ <nl> + if (! map . on_board ( loc )) return false ; <nl> + <nl> // handle radius <nl> const size_t radius = minimum < size_t >( max_loop , <nl> lexical_cast_default < size_t >( cfg [" radius "], 0 ));
struct tiff { <nl> */ <nl> # ifndef ReadOK <nl> # define ReadOK ( tif , buf , size ) \ <nl> - ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t ) size ) == ( tsize_t ) size ) <nl> + ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t )( size )) == ( tsize_t )( size )) <nl> # endif <nl> # ifndef SeekOK <nl> # define SeekOK ( tif , off ) \
static int lxc_spawn ( struct lxc_handler * handler ) <nl> int preserve_mask = 0 , i ; <nl> int netpipepair [ 2 ], nveths ; <nl>  <nl> + netpipe = - 1 ; <nl> + <nl> for ( i = 0 ; i < LXC_NS_MAX ; i ++) <nl> if ( handler -> conf -> inherit_ns_fd [ i ] != - 1 ) <nl> preserve_mask |= ns_info [ i ]. clone_flag ;
int dhcp_packet_verify_headers ( DHCPPacket * packet , size_t len , bool checksum ) { <nl>  <nl> /* UDP */ <nl>  <nl> + if ( packet -> ip . protocol != IPPROTO_UDP ) { <nl> + log_dhcp_client ( client , " ignoring packet : not UDP "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( len < DHCP_IP_UDP_SIZE ) { <nl> log_dhcp_client ( client , " ignoring packet : packet (% zu bytes ) " <nl> " smaller than IP + UDP header (% u bytes )", len ,
int main ( int argc , char * argv [], char * envp []) <nl>  <nl> reload_config = 1 ; <nl> buf = malloc ( nbytes ); <nl> - if ( buf != NULL ) { <nl> + if ( buf == NULL ) { <nl> err (" error getting buffer for inotify , disable watching "); <nl> close ( inotify_fd ); <nl> inotify_fd = - 1 ;
static void item_free ( Item * i ) { <nl> free ( i -> uid_path ); <nl> free ( i -> gid_path ); <nl> free ( i -> description ); <nl> + free ( i -> home ); <nl> free ( i ); <nl> } <nl> 
static int logind_check_inhibitors ( enum action a ) { <nl> if (! on_tty ()) <nl> return 0 ; <nl>  <nl> + if ( arg_transport != BUS_TRANSPORT_LOCAL ) <nl> + return 0 ; <nl> + <nl> r = acquire_bus ( BUS_FULL , & bus ); <nl> if ( r < 0 ) <nl> return r ;
int main ( int argc , char * argv []) { <nl> } <nl> } <nl>  <nl> + free ( arg_root_what ); <nl> + <nl> return r < 0 ? EXIT_FAILURE : EXIT_SUCCESS ; <nl> }
int bus_socket_read_message ( sd_bus * bus ) { <nl> return - EIO ; <nl> } <nl>  <nl> - f = realloc ( bus -> fds , sizeof ( int ) + ( bus -> n_fds + n )); <nl> + f = realloc ( bus -> fds , sizeof ( int ) * ( bus -> n_fds + n )); <nl> if (! f ) { <nl> close_many (( int *) CMSG_DATA ( cmsg ), n ); <nl> return - ENOMEM ;
static void sigusr2_handler ( int num ) <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; ais_service [ i ]; i ++) { <nl> - if ( ais_service [ i ]-> exec_dump_fn ) { <nl> + for ( i = 0 ; i < SERVICE_HANDLER_MAXIMUM_COUNT ; i ++) { <nl> + if ( ais_service [ i ] && ais_service [ i ]-> exec_dump_fn ) { <nl> ais_service [ i ]-> exec_dump_fn (); <nl> } <nl> }
static int init_nss_hash ( struct crypto_instance * instance ) <nl> } <nl>  <nl> hash_param . type = siBuffer ; <nl> - hash_param . data = 0 ; <nl> - hash_param . len = 0 ; <nl> + hash_param . data = instance -> private_key ; <nl> + hash_param . len = instance -> private_key_len ; <nl>  <nl> hash_slot = PK11_GetBestSlot ( hash_to_nss [ instance -> crypto_hash_type ], NULL ); <nl> if ( hash_slot == NULL ) {
ev_view_motion_notify_event ( GtkWidget * widget , <nl> } <nl>  <nl> if ( view -> scroll_info . autoscrolling ) { <nl> - view -> scroll_info . last_y = y ; <nl> + if ( y >= 0 ) <nl> + view -> scroll_info . last_y = y ; <nl> return TRUE ; <nl> } <nl> 
ev_view_presentation_transition_start ( EvViewPresentation * pview ) <nl>  <nl> duration = ev_document_transition_get_page_duration ( EV_DOCUMENT_TRANSITION ( pview -> document ), <nl> pview -> current_page ); <nl> - if ( duration > 0 ) { <nl> + if ( duration >= 0 ) { <nl> pview -> trans_timeout_id = <nl> g_timeout_add_seconds ( duration , <nl> ( GSourceFunc ) transition_next_page ,
gimp_image_map_tool_response ( GtkWidget * widget , <nl>  <nl> gimp_image_flush ( gimp_display_get_image ( tool -> display )); <nl>  <nl> - if ( image_map_tool -> config ) <nl> + if ( image_map_tool -> config && image_map_tool -> settings_box ) <nl> gimp_settings_box_add_current ( GIMP_SETTINGS_BOX ( image_map_tool -> settings_box ), <nl> GIMP_GUI_CONFIG ( tool -> tool_info -> gimp -> config )-> image_map_tool_max_recent ); <nl> }
neon ( GimpDrawable * drawable , <nl> g_free ( val_p ); <nl> g_free ( val_m ); <nl> g_free ( src ); <nl> + g_free ( src2 ); <nl> g_free ( dest ); <nl> } <nl> 
gimp_draw_tool_control ( GimpTool * tool , <nl> static gboolean <nl> gimp_draw_tool_draw_timeout ( GimpDrawTool * draw_tool ) <nl> { <nl> + guint64 now = g_get_monotonic_time (); <nl> + <nl> + /* keep the timeout running if the last drawing just happened */ <nl> + if (( now - draw_tool -> last_draw_time ) <= MINIMUM_DRAW_INTERVAL ) <nl> + return FALSE ; <nl> + <nl> draw_tool -> draw_timeout = 0 ; <nl>  <nl> gimp_draw_tool_draw ( draw_tool );
gimp_rectangle_select_tool_cursor_update ( GimpTool * tool , <nl> { <nl> gimp_tool_widget_get_cursor ( private -> widget , coords , state , <nl> & cursor , NULL , & modifier ); <nl> - <nl> - gimp_tool_control_set_cursor ( tool -> control , cursor ); <nl> - gimp_tool_control_set_cursor_modifier ( tool -> control , modifier ); <nl> } <nl>  <nl> + gimp_tool_control_set_cursor ( tool -> control , cursor ); <nl> + gimp_tool_control_set_cursor_modifier ( tool -> control , modifier ); <nl> + <nl> /* override the previous if shift or ctrl are down */ <nl> if ( state & ( gimp_get_extend_selection_mask () | <nl> gimp_get_modify_selection_mask ()))
lock_update_discard ( <nl> lock_rec_reset_and_release_wait ( block , heap_no ); <nl>  <nl> rec = page + rec_get_next_offs ( rec , TRUE ); <nl> - } while ( heap_no != PAGE_NEW_SUPREMUM ); <nl> + } while ( heap_no != PAGE_HEAP_NO_SUPREMUM ); <nl> } else { <nl> rec = page + PAGE_OLD_INFIMUM ; <nl>  <nl> lock_update_discard ( <nl> lock_rec_reset_and_release_wait ( block , heap_no ); <nl>  <nl> rec = page + rec_get_next_offs ( rec , FALSE ); <nl> - } while ( heap_no != PAGE_OLD_SUPREMUM ); <nl> + } while ( heap_no != PAGE_HEAP_NO_SUPREMUM ); <nl> } <nl>  <nl> lock_rec_free_all_from_discard_page ( block );
_rl_fix_last_undo_of_type ( type , start , end ) <nl>  <nl> for ( rl = rl_undo_list ; rl ; rl = rl -> next ) <nl> { <nl> - if ( rl -> what == ( uint ) type ) <nl> + if ( rl -> what == ( unsigned int ) type ) <nl> { <nl> rl -> start = start ; <nl> rl -> end = end ;
sync_array_signal_object ( <nl>  <nl> if ( cell_count == cell_max_count ) { <nl> sync_cell_t ** old_cell_ptr = cell_ptr ; <nl> - size_t old_size = cell_max_count * <nl> + size_t old_size , new_size ; <nl> + <nl> + old_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl> cell_max_count *= 2 ; <nl> - <nl> - size_t new_size = cell_max_count * <nl> + new_size = cell_max_count * <nl> sizeof ( sync_cell_t *); <nl>  <nl> cell_ptr = malloc ( new_size );
innobase_next_autoinc ( <nl> } else { <nl> next_value = current + increment ; <nl> } <nl> - } else { <nl> + } else if ( max_value > current ) { <nl> if ( current > offset ) { <nl> next_value = (( current - offset ) / increment ) + 1 ; <nl> } else { <nl> innobase_next_autoinc ( <nl> next_value += offset ; <nl> } <nl> } <nl> + } else { <nl> + next_value = max_value ; <nl> } <nl>  <nl> ut_a ( next_value <= max_value );
int toku_cachefile_fd ( CACHEFILE cf ) { <nl> } <nl>  <nl> int toku_cachefile_truncate0 ( CACHEFILE cf ) { <nl> - int r = toku_graceful_dirty ( cachefile ); <nl> + int r ; <nl> + r = toku_graceful_dirty ( cf ); <nl> if ( r != 0 ) return r ; <nl> - int r = ftruncate ( cf -> fd , 0 ); <nl> + r = ftruncate ( cf -> fd , 0 ); <nl> if ( r != 0 ) <nl> r = errno ; <nl> return r ;
class base_list_iterator <nl> * new_list . last = current -> next ; <nl> current -> info = new_list . first -> info ; <nl> current -> next = new_list . first -> next ; <nl> + if (( list -> last == & current -> next ) && ( new_list . elements > 1 )) <nl> + list -> last = new_list . last ; <nl> list -> elements += new_list . elements - 1 ; <nl> } <nl> return ret_value ; // return old element
void OPJ_CALLCONV opj_stream_set_skip_function ( opj_stream_t * p_stream , opj_strea <nl> void OPJ_CALLCONV opj_stream_set_user_data ( opj_stream_t * p_stream , void * p_data ) <nl> { <nl> opj_stream_private_t * l_stream = ( opj_stream_private_t *) p_stream ; <nl> + if (! l_stream ) <nl> + return ; <nl> l_stream -> m_user_data = p_data ; <nl> } <nl>  <nl> void OPJ_CALLCONV opj_stream_set_user_data_length ( opj_stream_t * p_stream , OPJ_UINT64 data_length ) <nl> { <nl> opj_stream_private_t * l_stream = ( opj_stream_private_t *) p_stream ; <nl> + if (! l_stream ) <nl> + return ; <nl> l_stream -> m_user_data_length = data_length ; <nl> } <nl> 
static int tga_readheader ( FILE * fp , unsigned int * bits_per_pixel , <nl> if ( fread ( tga , TGA_HEADER_SIZE , 1 , fp ) != 1 ) { <nl> fprintf ( stderr , <nl> "\ nError : fread return a number of element different from the expected .\ n "); <nl> + free ( tga ); <nl> return 0 ; <nl> } <nl> id_len = ( unsigned char ) tga [ 0 ];
void opj_lupSolve ( OPJ_FLOAT32 * pResult , <nl> lTmpMatrix = lLineMatrix ; <nl> u = *( lTmpMatrix ++); <nl> lCurrentPtr = lDestPtr --; <nl> - for ( j = k + 1 ; j < nb_compo ; ++ j ) { <nl> + for ( j = ( OPJ_UINT32 )( k + 1 ); j < nb_compo ; ++ j ) { <nl> /* sum += matrix [ k ][ j ] * x [ j ] */ <nl> sum += (*( lTmpMatrix ++)) * (*( lCurrentPtr ++)); <nl> }
int connection_dns_finished_flushing ( connection_t * conn ) { <nl> int connection_dns_reached_eof ( connection_t * conn ) { <nl> log_fn ( LOG_WARN ," Read eof . Worker died unexpectedly ."); <nl> if ( conn -> state == DNSWORKER_STATE_BUSY ) { <nl> - dns_cancel_pending_resolve ( conn -> address ); <nl> + /* don ' t cancel the resolve here -- it would be cancelled in <nl> + * connection_about_to_close_connection (), since conn is still <nl> + * in state BUSY <nl> + */ <nl> num_dnsworkers_busy --; <nl> } <nl> num_dnsworkers --;
router_reload_networkstatus ( void ) <nl> entries = tor_listdir ( filename ); <nl> SMARTLIST_FOREACH ( entries , const char *, fn , { <nl> char buf [ DIGEST_LEN ]; <nl> + if ( fn [ 0 ] == '.') /* skip . and .. */ <nl> + continue ; <nl> if ( strlen ( fn ) != HEX_DIGEST_LEN || <nl> base16_decode ( buf , sizeof ( buf ), fn , strlen ( fn ))) { <nl> log_fn ( LOG_INFO ,
static void log_msg ( log_level_t level , const char * fmt , va_list args ) <nl>  <nl> if ( level > SYSLOG_LEVEL && <nl> level > LOGFILE_LEVEL && <nl> - level > STDERR_LEVEL ) <nl> + level > STDERR_LEVEL ) { <nl> + pthread_mutex_unlock (& log_lock ); <nl> return ; <nl> + } <nl>  <nl> if ( log -> opt . prefix_level || SYSLOG_LEVEL > level ) { <nl> switch ( level ) {
main ( int argc , char * argv []) <nl>  <nl> _slurmd_fini (); <nl> _destroy_conf (); <nl> + slurm_crypto_fini (); /* must be after _destroy_conf () */ <nl> + <nl> info (" Slurmd shutdown completing "); <nl> log_fini (); <nl> return 0 ; <nl> _slurmd_fini () <nl> slurm_conf_destroy (); <nl> slurm_proctrack_fini (); <nl> slurm_auth_fini (); <nl> - slurm_crypto_fini (); <nl> slurmd_req ( NULL ); /* purge memory allocated by slurmd_req () */ <nl> return SLURM_SUCCESS ; <nl> }
void msWriteErrorImage ( mapObj * map , char * filename , int blank ) <nl> ls . size = i ; <nl> MS_INIT_COLOR (* ls . color , 0 , 0 , 0 , 255 ); <nl> MS_INIT_COLOR (* ls . outlinecolor , 255 , 255 , 255 , 255 ); <nl> + ls . outlinewidth = 1 ; <nl> break ; <nl> } <nl> }
emit_method_info ( MonoAotCompile * acfg , MonoCompile * cfg ) <nl>  <nl> encode_patch_list ( acfg , patches , n_patches , cfg -> compile_llvm , first_got_offset , p , & p ); <nl>  <nl> + g_ptr_array_free ( patches , TRUE ); <nl> + <nl> acfg -> stats . info_size += p - buf ; <nl>  <nl> g_assert ( p - buf < buf_size );
mono_method_to_ir ( MonoCompile * cfg , MonoMethod * method , MonoBasicBlock * start_b <nl> token = read32 ( ip + 2 ); <nl> func = mono_method_get_wrapper_data ( method , token ); <nl> info = mono_find_jit_icall_by_addr ( func ); <nl> + if (! info ) <nl> + g_error (" Could not find icall address in wrapper % s ", mono_method_full_name ( method , 1 )); <nl> g_assert ( info ); <nl>  <nl> CHECK_STACK ( info -> sig -> param_count );
is_regsize_var ( MonoType * t ) { <nl> case MONO_TYPE_U4 : <nl> case MONO_TYPE_I : <nl> case MONO_TYPE_U : <nl> + case MONO_TYPE_PTR : <nl> + case MONO_TYPE_FNPTR : <nl> return TRUE ; <nl> case MONO_TYPE_OBJECT : <nl> case MONO_TYPE_STRING :
void GeoIPBackend :: initialize () { <nl> } <nl> } <nl>  <nl> - tmp_domains . push_back ( dom ); <nl> + tmp_domains . push_back ( std :: move ( dom )); <nl> } <nl>  <nl> s_domains . clear ();
nautilus_toolbar_dispose ( GObject * obj ) <nl> toolbar_update_appearance , self ); <nl> unschedule_menu_popup_timeout ( self ); <nl>  <nl> + g_clear_object (& self -> priv -> zoom_adjustment_grid ); <nl> + g_clear_object (& self -> priv -> zoom_adjustment_list ); <nl> + <nl> G_OBJECT_CLASS ( nautilus_toolbar_parent_class )-> dispose ( obj ); <nl> } <nl> 
GF_Err import_file ( GF_ISOFile * dest , char * inName , u32 import_flags , Double forc <nl> } <nl> } <nl> } <nl> + else if (! strnicmp ( ext , " prog_id =", 8 )) { <nl> + prog_id = atoi ( ext + 8 ); <nl> + do_all = 0 ; <nl> + } <nl> else track_id = atoi ( ext ); <nl> } <nl> if ( do_audio || do_video || track_id ) do_all = 0 ;
static void TraverseCustomTexture ( GF_Node * node , void * rs , Bool is_destroy ) <nl>  <nl> static void CustomTexture_update ( GF_TextureHandler * txh ) <nl> { <nl> +# ifndef GPAC_DISABLE_3D <nl> char data [ 12 ]; <nl> +# endif <nl> CustomTextureStack * stack = gf_node_get_private ( txh -> owner ); <nl> // texture not setup , do it <nl> if (! txh -> tx_io ) {
GF_RTPHinter * gf_hinter_track_new ( GF_ISOFile * file , u32 TrackNum , <nl> max_ptime = ( u32 ) ( max_ptime * my_sl . timestampResolution / 1000 ); <nl>  <nl> my_sl . AUSeqNumLength = gf_get_bit_size ( gf_isom_get_sample_count ( file , TrackNum )); <nl> + if ( my_sl . AUSeqNumLength > 16 ) my_sl . AUSeqNumLength = 16 ; <nl> + <nl> my_sl . CUDuration = const_dur ; <nl>  <nl> if ( gf_isom_has_sync_points ( file , TrackNum )) {
GF_Box * gf_isom_box_new ( u32 boxType ) <nl>  <nl> void gf_isom_box_add_for_dump_mode ( GF_Box * parent , GF_Box * a ) <nl> { <nl> - if ( use_dump_mode && (! parent -> other_boxes || ( gf_list_find ( parent -> other_boxes , a )< 0 ) ) ) <nl> + if ( use_dump_mode && a && (! parent -> other_boxes || ( gf_list_find ( parent -> other_boxes , a )< 0 ) ) ) <nl> gf_isom_box_add_default ( parent , a ); <nl> } <nl> 
int _sc_add_atr ( sc_context_t * ctx , struct sc_card_driver * driver , struct sc_atr_ <nl> driver -> atr_map = map ; <nl> dst = & driver -> atr_map [ driver -> natrs ++]; <nl> memset ( dst , 0 , sizeof (* dst )); <nl> + memset (& driver -> atr_map [ driver -> natrs ], 0 , sizeof ( struct sc_atr_table )); <nl> + dst -> atr = strdup ( src -> atr ); <nl> dst -> atr = strdup ( src -> atr ); <nl> if (! dst -> atr ) <nl> return SC_ERROR_OUT_OF_MEMORY ;
sc_keycache_forget_key ( const sc_path_t * path , int type , int ref ) <nl> while (( s = * prev ) != NULL ) { <nl> if ( __match_entry ( s , type , ref , path , 1 )) { <nl> * prev = s -> next ; <nl> + if ( s -> named_pin != - 1 && s -> ref == - 1 ) <nl> + named_pin [ s -> named_pin ] = NULL ; <nl> memset ( s , 0 , sizeof (* s )); <nl> free ( s ); <nl> } else {
void swTimer_node_insert ( swTimer_node ** root , swTimer_node * new_node ) <nl> swTimer_node * tmp = * root ; <nl> while ( 1 ) <nl> { <nl> - if ( tmp -> exec_msec >= new_node -> exec_msec ) <nl> + if ( tmp -> exec_msec > new_node -> exec_msec ) <nl> { <nl> new_node -> prev = tmp -> prev ; <nl> new_node -> next = tmp ;
int swServer_master_onAccept ( swReactor * reactor , swEvent * event ) <nl> { <nl> if ( swSSL_create ( conn , 0 ) < 0 ) <nl> { <nl> - conn -> active = 0 ; <nl> + bzero ( conn , sizeof ( swConnection )); <nl> close ( new_fd ); <nl> } <nl> } <nl> int swServer_master_onAccept ( swReactor * reactor , swEvent * event ) <nl>  <nl> if ( ret < 0 ) <nl> { <nl> + bzero ( conn , sizeof ( swConnection )); <nl> close ( new_fd ); <nl> return SW_OK ; <nl> }
private : <nl> */ <nl> char ibuf [ 16384 ]; <nl>  <nl> + /** <nl> + * The output buffer for this socket <nl> + */ <nl> + std :: string Buffer ; <nl> + <nl> /** <nl> * The IP address being connected <nl> * to stored in string form for
class CoreExport XLine : public classbase <nl> free ( reason ); <nl> free ( source ); <nl> } <nl> + <nl> + /** Returns true whether or not the given user is covered by this line . <nl> + */ <nl> + virtual bool Matches ( User * u ); <nl> + <nl> /** The time the line was added . <nl> */ <nl> time_t set_time ;
class cmd_shun : public Command <nl>  <nl> if ( pcnt == 1 ) <nl> { <nl> - if ( ServerInstance -> XLines -> DelLine ( parameters [ 0 ], " S ", user )) <nl> + if ( ServerInstance -> XLines -> DelLine ( parameters [ 0 ], " SHUN ", user )) <nl> { <nl> ServerInstance -> SNO -> WriteToSnoMask (' x ',"% s Removed shun on % s .", user -> nick , parameters [ 0 ]); <nl> }
void TreeSocket :: OnTimeout () <nl>  <nl> void TreeSocket :: Close () <nl> { <nl> - if ( fd != - 1 ) <nl> - ServerInstance -> GlobalCulls . AddItem ( this ); <nl> + if ( fd < 0 ) <nl> + return ; <nl> + <nl> + ServerInstance -> GlobalCulls . AddItem ( this ); <nl> this -> BufferedSocket :: Close (); <nl> SetError (" Remote host closed connection "); <nl> 
int regexp_nsub ( struct regexp * r ) { <nl> } <nl>  <nl> void regexp_release ( struct regexp * regexp ) { <nl> - if ( regexp -> re != NULL ) { <nl> + if ( regexp != NULL && regexp -> re != NULL ) { <nl> regfree ( regexp -> re ); <nl> FREE ( regexp -> re ); <nl> }
static void dump_ctx ( struct ctx * ctx ) { <nl> * Values <nl> */ <nl> static void print_tree ( FILE * out , int indent , struct tree * tree ) { <nl> + if ( tree == NULL ) { <nl> + fprintf ( out , "( null tree )\ n "); <nl> + return ; <nl> + } <nl> list_for_each ( t , tree ) { <nl> for ( int i = 0 ; i < indent ; i ++) fputc (' ', out ); <nl> fprintf ( out , "{ ");
main ( int argc , char ** argv ) <nl> } <nl>  <nl> mrb = mrb_open (); <nl> + if ( mrb == NULL ) { <nl> + fputs (" Invalid mrb_state , exiting mruby - strip \ n ", stderr ); <nl> + return EXIT_FAILURE ; <nl> + } <nl>  <nl> ireps = ( mrb_irep **) malloc ( sizeof ( mrb_irep *) * argc ); <nl> for ( i = args_result ; i < argc ; ++ i ) {
main ( int argc , char ** argv ) <nl> char_index = 0 ; <nl> while (( last_char = getchar ()) != '\ n ') { <nl> if ( last_char == EOF ) break ; <nl> - if ( char_index > sizeof ( last_code_line )- 2 ) { <nl> + if ( char_index >= sizeof ( last_code_line )- 2 ) { <nl> fputs (" input string too long \ n ", stderr ); <nl> continue ; <nl> }
new_msym ( codegen_scope * s , mrb_sym sym ) <nl> { <nl> size_t i , len ; <nl>  <nl> + if ( s -> irep == NULL ) return 0 ; <nl> len = s -> irep -> slen ; <nl> + <nl> if ( len > 256 ) len = 256 ; <nl> for ( i = 0 ; i < len ; i ++) { <nl> if ( s -> irep -> syms [ i ] == sym ) return i ;
convert_type ( mrb_state * mrb , mrb_value val , const char * tname , const char * metho <nl> return mrb_nil_value (); <nl> } <nl> } <nl> - return mrb_funcall ( mrb , val , method , 0 ); <nl> + return mrb_funcall_argv ( mrb , val , m , 0 , 0 ); <nl> } <nl>  <nl> mrb_value
static bool ikev2_check_fragment ( struct msg_digest * md ) <nl> " discarding IKE encrypted fragment - fragmentation not allowed by local policy ( ike_frag = no )")); <nl> return FALSE ; <nl> } <nl> + if (!( st -> st_seen_fragvid )) { <nl> + DBG ( DBG_CONTROL , DBG_log ( <nl> + " discarding IKE encrypted fragment - remote never proposed fragmentation ")); <nl> + return FALSE ; <nl> + } <nl>  <nl> DBG ( DBG_CONTROL , DBG_log ( <nl> " received IKE encrypted fragment number '% u ', total number '% u ', next payload '% u '",
static char ** new_list ( const char * value ) <nl>  <nl> /* avoid damaging original string */ <nl> val = clone_str ( value , " new_list value "); <nl> - end = val + strlen ( val ); <nl> + if ( val != NULL ) /* silence a coverity warning */ <nl> + end = val + strlen ( val ); <nl>  <nl> /* count number of items in string */ <nl> for ( b = val , count = 0 ; b < end ; ) {
parse_x_real_ip ( cherokee_logger_t * logger , cherokee_connection_t * conn ) <nl> } <nl>  <nl> p = val ; <nl> - while (* p ) { <nl> + while (* p && ( p - val < len )) { <nl> if ((* p == ' ') || (* p == ',')) { <nl> len = p - val ; <nl> break ;
get_controls ( struct xkb_keymap * keymap , xcb_connection_t * conn , <nl> xcb_xkb_get_controls_reply ( conn , cookie , NULL ); <nl>  <nl> FAIL_IF_BAD_REPLY ( reply , " XkbGetControls "); <nl> + FAIL_UNLESS ( reply -> numGroups > 0 && reply -> numGroups <= 4 ); <nl>  <nl> keymap -> enabled_ctrls = translate_controls_mask ( reply -> enabledControls ); <nl> keymap -> num_groups = reply -> numGroups ;
 <nl> int Icon :: stdSize ( int v ) <nl> { <nl> - if ( v < 20 ) { <nl> + if ( v < 19 ) { <nl> return 16 ; <nl> } else if ( v < 28 ) { <nl> return 22 ;
static ssize_t DecodePSDPixels ( const size_t number_compact_pixels , <nl> length ++; <nl> for ( j = 0 ; j < ( ssize_t ) length ; j ++) <nl> { <nl> + CheckNumberCompactPixels ; <nl> switch ( depth ) <nl> { <nl> case 1 : <nl> static ssize_t DecodePSDPixels ( const size_t number_compact_pixels , <nl> break ; <nl> } <nl> } <nl> - CheckNumberCompactPixels ; <nl> compact_pixels ++; <nl> } <nl> }
static Image * ReadYCBCRImage ( const ImageInfo * image_info , <nl> if ( status == MagickFalse ) <nl> { <nl> quantum_info = DestroyQuantumInfo ( quantum_info ); <nl> + canvas_image = DestroyImage ( canvas_image ); <nl> return ( DestroyImageList ( image )); <nl> } <nl> SetImageColorspace ( image , YCbCrColorspace , exception );
ModuleExport MagickBooleanType ReadPSDLayers ( Image * image , <nl> if ( image -> debug != MagickFalse ) <nl> ( void ) LogMagickEvent ( CoderEvent , GetMagickModule (), <nl> " layer data is empty "); <nl> + if ( layer_info [ i ]. info != ( StringInfo *) NULL ) <nl> + layer_info [ i ]. info = DestroyStringInfo ( layer_info [ i ]. info ); <nl> continue ; <nl> } <nl> 
server_request_free_answers ( struct server_request * req ) <nl> free ( victim -> name ); <nl> if ( victim -> data ) <nl> free ( victim -> data ); <nl> - /* XXXX free ( victim ?) - NM */ <nl> + free ( victim ); <nl> victim = next ; <nl> } <nl> * list = NULL ;
evhttp_parse_response_line ( struct evhttp_request * req , char * line ) <nl> return (- 1 ); <nl> } <nl>  <nl> + if ( req -> response_code_line != NULL ) <nl> + mm_free ( req -> response_code_line ); <nl> if (( req -> response_code_line = mm_strdup ( readable )) == NULL ) { <nl> event_warn ("% s : strdup ", __func__ ); <nl> return (- 1 );
void uv__pipe_close ( uv_pipe_t * handle ) { <nl> */ <nl> unlink ( handle -> pipe_fname ); <nl> free (( void *) handle -> pipe_fname ); <nl> + handle -> pipe_fname = NULL ; <nl> } <nl>  <nl> uv__stream_close (( uv_stream_t *) handle );
static void huffman_decode_row ( x3f_info_t * I , x3f_directory_entry_t * DE , <nl> int col ; <nl> bit_state_t BS ; <nl>  <nl> + if ( HUF -> row_offsets . element [ row ] > ID -> data_size - 1 ) <nl> + throw LIBRAW_EXCEPTION_IO_CORRUPT ; <nl> set_bit_state (& BS , ( uint8_t *) ID -> data + HUF -> row_offsets . element [ row ]); <nl>  <nl> for ( col = 0 ; col < ID -> columns ; col ++)
int mk_http_parser ( struct mk_http_request * req , struct mk_http_parser * p , <nl> p -> chars += 7 ; <nl>  <nl> request_set (& req -> protocol_p , p , buffer ); <nl> + req -> protocol_p . len = 8 ; <nl> mk_http_set_minor_version ( buffer [ tmp + 7 ]); <nl> continue ; <nl> }
struct mk_iov * mk_iov_create ( int n , int offset ) <nl> iov = mk_mem_malloc ( sizeof ( struct mk_iov )); <nl> iov -> iov_idx = offset ; <nl> iov -> io = mk_mem_malloc_z ( n * sizeof ( struct iovec )); <nl> - iov -> buf_to_free = mk_mem_malloc_z ( n * sizeof ( char *)); <nl> + iov -> buf_to_free = mk_mem_malloc ( n * sizeof ( char *)); <nl> iov -> buf_idx = 0 ; <nl> iov -> total_len = 0 ; <nl> iov -> size = n ;
int main ( int argc , char ** argv ) <nl> char * sites_conf_dir = NULL ; <nl> char * plugins_conf_dir = NULL ; <nl> char * mimes_conf_file = NULL ; <nl> + struct mk_server * server ; <nl>  <nl> static const struct option long_opts [] = { <nl> { " configdir ", required_argument , NULL , ' c ' },
int DetectHttpHeaderMatch ( ThreadVars * t , DetectEngineThreadCtx * det_ctx , <nl>  <nl> SCMutexLock (& f -> m ); <nl>  <nl> - if ( htp_state == NULL ) { <nl> + if ( htp_state == NULL || htp_state -> connp == NULL || <nl> + htp_state -> connp -> conn == NULL ) { <nl> SCLogDebug (" No htp state , no match at http header data "); <nl> goto end ; <nl> }
CURLcode Curl_http ( struct connectdata * conn , bool * done ) <nl> if ( http -> writebytecount >= postsize ) { <nl> /* already sent the entire request body , mark the " upload " as <nl> complete */ <nl> - infof ( data , " upload completely sent off : %" FORMAT_OFF_T " out of " <nl> + infof ( data , " upload completely sent off : %" FORMAT_OFF_T " out of " <nl> "%" FORMAT_OFF_T " bytes \ n ", <nl> http -> writebytecount , postsize ); <nl> data -> req . upload_done = TRUE ;
CURLcode Curl_perform ( struct SessionHandle * data ) <nl> * an error , use the strerror () string or if things are so bad that not <nl> * even that is good , set a bad string that mentions the error code . <nl> */ <nl> - char * str = curl_easy_strerror ( res ); <nl> + const char * str = curl_easy_strerror ( res ); <nl> if (! str ) <nl> failf ( data , " unspecified error % d ", ( int ) res ); <nl> else
static ssize_t send_callback ( nghttp2_session * h2 , <nl> ( void ) h2 ; <nl> ( void ) flags ; <nl>  <nl> + if (! c -> send_underlying ) <nl> + /* called before setup properly ! */ <nl> + return NGHTTP2_ERR_CALLBACK_FAILURE ; <nl> + <nl> written = (( Curl_send *) c -> send_underlying )( conn , FIRSTSOCKET , <nl> data , length , & result ); <nl> 
static GlobCode glob_range ( URLGlob * glob , char ** patternp , <nl> } <nl> else <nl> step_n = 1 ; <nl> - if (* endp == ']') { <nl> + if ( endp && (* endp == ']')) { <nl> pattern = endp + 1 ; <nl> } <nl> else
void curl_easy_reset ( struct Curl_easy * data ) <nl>  <nl> data -> progress . flags |= PGRS_HIDE ; <nl> data -> state . current_speed = - 1 ; /* init to negative == impossible */ <nl> + <nl> + /* zero out authentication data : */ <nl> + memset (& data -> state . authhost , 0 , sizeof ( struct auth )); <nl> + memset (& data -> state . authproxy , 0 , sizeof ( struct auth )); <nl> } <nl>  <nl> /*
Ghoul2 Insert End <nl> return 0 ; <nl>  <nl> case CG_OPENJK_MENU_PAINT : <nl> - Menu_Paint ( ( menuDef_t *) VMA ( 1 ), ( intptr_t ) VMA ( 2 ) ); <nl> + Menu_Paint ( ( menuDef_t *) VMA ( 1 ), args [ 2 ] ); <nl> return 0 ; <nl>  <nl> case CG_OPENJK_GETMENU_BYNAME :
qboolean Sys_LowPhysicalMemory () <nl> if (! bAsked ) // just in case it takes a little time for GlobalMemoryStatus () to gather stats on <nl> { // stuff we don ' t care about such as virtual mem etc . <nl> bAsked = qtrue ; <nl> + <nl> + stat . dwLength = sizeof ( stat ); <nl> GlobalMemoryStatusEx (& stat ); <nl> } <nl> if ( sys_lowmem -> integer )
void CQuickSpriteSystem :: Add ( float * pointdata , color4ub_t color , vec2_t fog ) <nl> } <nl>  <nl> curcoord = mVerts [ mNextVert ]; <nl> - memcpy ( curcoord , pointdata , 4 * sizeof ( vec4_t )); <nl> + memcpy ( curcoord , pointdata , sizeof ( vec4_t )); <nl>  <nl> // Set up color <nl> curcolor = & mColors [ mNextVert ];
void Field_VariableSizeDraw ( field_t * edit , int x , int y , int width , int size , q <nl> drawLen = len - prestep ; <nl> } <nl>  <nl> + if ( drawLen < 0 ) <nl> + return ; <nl> + <nl> // extract < drawLen > characters from the field at < prestep > <nl> if ( drawLen >= MAX_STRING_CHARS ) { <nl> Com_Error ( ERR_DROP , " drawLen >= MAX_STRING_CHARS " );
str_byte_substr ( VALUE str , long beg , long len , int empty ) <nl> beg += n ; <nl> if ( beg < 0 ) return Qnil ; <nl> } <nl> - if ( beg + len > n ) <nl> + if ( len > n - beg ) <nl> len = n - beg ; <nl> if ( len <= 0 ) { <nl> if (! empty ) return Qnil ;
static int me_gcap ( struct Client *, struct Client *, int , const char **); <nl>  <nl> struct Message capab_msgtab = { <nl> " CAPAB ", 0 , 0 , 0 , MFLG_SLOW | MFLG_UNREG , <nl> - {{ mr_capab , 0 }, mg_ignore , mg_ignore , mg_ignore , mg_ignore , mg_ignore } <nl> + {{ mr_capab , 2 }, mg_ignore , mg_ignore , mg_ignore , mg_ignore , mg_ignore } <nl> }; <nl> struct Message gcap_msgtab = { <nl> " GCAP ", 0 , 0 , 0 , MFLG_SLOW ,
int CloseUnreal ( HWND hWnd ) <nl> return 0 ; <nl> else <nl> { <nl> - DestroyWindow ( hWnd ); <nl> - exit ( 0 ); <nl> + DestroyWindow ( hWnd ); <nl> + TerminateProcess ( GetCurrentProcess (), 0 ); <nl> + exit ( 0 ); /* in case previous fails ( possible ?) */ <nl> } <nl> } <nl> 
GetOutboundPinholeTimeout ( struct upnphttp * h , const char * action , const char * <nl> rem_port = GetValueFromNameValueList (& data , " RemotePort "); <nl> protocol = GetValueFromNameValueList (& data , " Protocol "); <nl>  <nl> - if (! int_port || ! ext_port || ! protocol ) <nl> + if (! int_port || ! rem_port || ! protocol ) <nl> { <nl> ClearNameValueList (& data ); <nl> SoapError ( h , 402 , " Invalid Args ");
pc2line ( uvlong pc ) <nl> if ( pc < currpc || pc > txtend ) <nl> return ~ 0 ; <nl>  <nl> - for ( c = pcline ; c < pclineend && pc <= currpc ; c ++) { <nl> + for ( c = pcline ; c < pclineend && currpc < pc ; c ++) { <nl> u = * c ; <nl> if ( u == 0 ) { <nl> currline += ( c [ 1 ]<< 24 )|( c [ 2 ]<< 16 )|( c [ 3 ]<< 8 )| c [ 4 ];
int OBJ_obj2nid ( const ASN1_OBJECT * a ) <nl> if ( a -> nid != 0 ) <nl> return ( a -> nid ); <nl>  <nl> + if ( a -> length == 0 ) <nl> + return NID_undef ; <nl> + <nl> if ( added != NULL ) { <nl> ad . type = ADDED_DATA ; <nl> ad . obj = ( ASN1_OBJECT *) a ; /* XXX : ugly but harmless */
int PKCS12_PBE_keyivgen ( EVP_CIPHER_CTX * ctx , const char * pass , int passlen , <nl> unsigned char * salt ; <nl> unsigned char key [ EVP_MAX_KEY_LENGTH ], iv [ EVP_MAX_IV_LENGTH ]; <nl>  <nl> + if ( cipher == NULL ) <nl> + return 0 ; <nl> + <nl> /* Extract useful info from parameter */ <nl>  <nl> pbe = ASN1_TYPE_unpack_sequence ( ASN1_ITEM_rptr ( PBEPARAM ), param );
static int check_suiteb_cipher_list ( const SSL_METHOD * meth , CERT * c , <nl> * prule_str = " ECDHE - ECDSA - AES256 - GCM - SHA384 "; <nl> break ; <nl> } <nl> + /* Set auto ECDH parameter determination */ <nl> + c -> ecdh_tmp_auto = 1 ; <nl> return 1 ; <nl> } <nl> 
int RSA_padding_check_PKCS1_OAEP ( unsigned char * to , int tlen , <nl> } <nl>  <nl> lzero = num - flen ; <nl> + if ( lzero < 0 ) <nl> + { <nl> + RSAerr ( RSA_F_RSA_PADDING_CHECK_PKCS1_OAEP , RSA_R_OAEP_DECODING_ERROR ); <nl> + return (- 1 ); <nl> + } <nl> maskeddb = from - lzero + SHA_DIGEST_LENGTH ; <nl>  <nl> MGF1 ( seed , SHA_DIGEST_LENGTH , maskeddb , dblen );
static int init_ssl_connection ( SSL * con ) <nl> BIO_ADDR_free ( client ); <nl> dtlslisten = 0 ; <nl> i = SSL_accept ( con ); <nl> + } else { <nl> + BIO_ADDR_free ( client ); <nl> } <nl> } else <nl> # endif
retry : <nl> struct stat statbuf ; <nl>  <nl> strcpy ( filename , str ); <nl> + free ( str ); <nl> if (! stat ( filename , & statbuf )) { <nl> wlogprint (" File exists , overwrite ?\ n "); <nl> input = getch (); <nl> retry : <nl> goto retry ; <nl> } <nl> } <nl> + else <nl> + free ( str ); <nl> fcfg = fopen ( filename , " w "); <nl> if (! fcfg ) { <nl> wlogprint (" Cannot open or create file \ n ");
char * sepol_av_to_string ( policydb_t * policydbp , uint32_t tclass , <nl> int rc ; <nl> int avlen = 0 , len ; <nl>  <nl> + memset ( avbuf , 0 , sizeof avbuf ); <nl> cladatum = policydbp -> class_val_to_struct [ tclass - 1 ]; <nl> p = avbuf ; <nl> for ( i = 0 ; i < cladatum -> permissions . nprim ; i ++) {
static void mcast () <nl>  <nl> count ++; <nl> came_from_siz = sizeof ( came_from ); <nl> - if ( SOCKETFAIL ( rep = recvfrom ( mcast_sock , buf , sizeof ( buf ), <nl> + if ( SOCKETFAIL ( rep = recvfrom ( mcast_sock , buf , sizeof ( buf ) - 1 , <nl> 0 , ( struct sockaddr *)(& came_from ), & came_from_siz ))) { <nl> applog ( LOG_DEBUG , " API mcast failed count =% d (% s ) (% d )", <nl> count , SOCKERRMSG , ( int ) mcast_sock );
struct tcmulib_context * tcmulib_initialize ( <nl> teardown_netlink ( ctx -> nl_sock ); <nl> darray_free ( ctx -> handlers ); <nl> darray_free ( ctx -> devices ); <nl> + genl_unregister_family (& tcmu_ops ); <nl> + free ( ctx ); <nl> return NULL ; <nl> } <nl> 
int main ( int argc , char ** argv ) <nl> int inotify_fd = inotify_init (); <nl> if ( inotify_fd == - 1 ) <nl> perror_msg_and_die (" inotify_init failed "); <nl> + close_on_exec_on ( inotify_fd ); <nl> if ( inotify_add_watch ( inotify_fd , DEBUG_DUMPS_DIR , IN_CREATE | IN_MOVED_TO ) == - 1 ) <nl> perror_msg_and_die (" inotify_add_watch failed on '% s '", DEBUG_DUMPS_DIR ); <nl> 
void Client :: setDisconnectedFromCore () <nl> _ignoreListManager -> deleteLater (); <nl> _ignoreListManager = 0 ; <nl> } <nl> + <nl> + if ( _transferManager ) { <nl> + _transferManager -> deleteLater (); <nl> + _transferManager = 0 ; <nl> + } <nl> + <nl> // we probably don ' t want to save pending input for reconnect <nl> _userInputBuffer . clear (); <nl> 
wavlike_subchunk_parse ( SF_PRIVATE * psf , int chunk , uint32_t chunk_length ) <nl>  <nl> case exif_MARKER : <nl> psf_log_printf ( psf , " % M \ n ", chunk ) ; <nl> - bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> + if ( chunk_length > bytesread ) <nl> + bytesread += exif_subchunk_parse ( psf , chunk_length - bytesread ) ; <nl> continue ; <nl>  <nl> case data_MARKER :
aiff_read_header ( SF_PRIVATE * psf , COMM_CHUNK * comm_fmt ) <nl> } ; <nl> } ; <nl>  <nl> + if ( psf -> sf . channels < 1 ) <nl> + return SFE_CHANNEL_COUNT_ZERO ; <nl> + <nl> + if ( psf -> sf . channels >= SF_MAX_CHANNELS ) <nl> + return SFE_CHANNEL_COUNT ; <nl> + <nl> if (! ( found_chunk & HAVE_FORM )) <nl> return SFE_AIFF_NO_FORM ; <nl> 
static problem_data_t * load_problem_data_if_not_yet ( problem_data_t * problem_data <nl> struct dump_dir * dd = dd_opendir ( dump_dir_name , /* flags :*/ 0 ); <nl> if (! dd ) <nl> { <nl> - problem_data_free ( problem_data ); <nl> return NULL ; <nl> } <nl> problem_data = create_problem_data_from_dump_dir ( dd );
usershape_t * gvusershape_find ( char * name ) <nl> { <nl> usershape_t probe ; <nl>  <nl> + if (! ImageDict ) <nl> + return NULL ; <nl> + <nl> probe . name = name ; <nl> return ( dtsearch ( ImageDict , & probe )); <nl> }
ieee802_11_print ( netdissect_options * ndo , <nl> hdrlen = roundup2 ( hdrlen , 4 ); <nl> if ( ndo -> ndo_Hflag && FC_TYPE ( fc ) == T_DATA && <nl> DATA_FRAME_IS_QOS ( FC_SUBTYPE ( fc ))) { <nl> + if (! ND_TTEST_1 ( p + hdrlen )) { <nl> + nd_print_trunc ( ndo ); <nl> + return hdrlen ; <nl> + } <nl> meshdrlen = extract_mesh_header_length ( p + hdrlen ); <nl> hdrlen += meshdrlen ; <nl> } else
dhcp6opt_print ( const u_char * cp , const u_char * ep ) <nl> if ( ep < cp + sizeof (* dh6o )) <nl> goto trunc ; <nl> dh6o = ( struct dhcp6opt *) cp ; <nl> + TCHECK (* dh6o ); <nl> optlen = EXTRACT_16BITS (& dh6o -> dh6opt_len ); <nl> if ( ep < cp + sizeof (* dh6o ) + optlen ) <nl> goto trunc ;
icmp6_nodeinfo_print ( netdissect_options * ndo , u_int icmp6len , const u_char * bp , <nl> } else <nl> dnsname_print ( ndo , cp , ep ); <nl> if (( EXTRACT_16BITS (& ni6 -> ni_flags ) & 0x01 ) != 0 ) <nl> - ND_PRINT (( ndo ," [ TTL =% u ]", *( uint32_t *)( ni6 + 1 ))); <nl> + ND_PRINT (( ndo ," [ TTL =% u ]", EXTRACT_32BITS ( ni6 + 1 ))); <nl> break ; <nl> case NI_QTYPE_NODEADDR : <nl> if ( needcomma )
static bool handle_client_startup ( PgSocket * client , PktHdr * pkt ) <nl> } <nl> break ; <nl> case ' p ': /* PasswordMessage */ <nl> + /* too early */ <nl> + if (! client -> auth_user ) { <nl> + disconnect_client ( client , true , " client password pkt before startup packet "); <nl> + return false ; <nl> + } <nl> + <nl> /* haven ' t requested it */ <nl> if ( cf_auth_type <= AUTH_TRUST ) { <nl> disconnect_client ( client , true , " unrequested passwd pkt ");
bool find_server ( PgSocket * client ) <nl> sbuf_pause (& client -> sbuf ); <nl> res = false ; /* don ' t process client data yet */ <nl> server -> setting_vars = 1 ; <nl> + server -> ready = 0 ; <nl> } else <nl> res = true ; <nl> } else {
find_existing_file ( gchar * path_list ) { <nl> i ++; <nl> } <nl>  <nl> + g_free ( executable ); <nl> g_strfreev ( split ); <nl> return NULL ; <nl> }
int oidc_handle_redirect_uri_request ( request_rec * r , oidc_cfg * c , <nl> /* something went wrong */ <nl> return oidc_util_html_send_error ( r , c -> error_template , " Invalid Request ", <nl> apr_psprintf ( r -> pool , <nl> - " The OpenID Connect callback URL received an invalid request : % s ", <nl> - r -> args ), HTTP_INTERNAL_SERVER_ERROR ); <nl> + " The OpenID Connect callback URL received an invalid request "), <nl> + HTTP_INTERNAL_SERVER_ERROR ); <nl> } <nl>  <nl> /*
 <nl> int xerbla_ ( char * srname , integer * info ) <nl> { <nl> - const char * format = " On entry to %.* s " \ <nl> + char * format = " On entry to %.* s " \ <nl> " parameter number % d had an illegal value "; <nl> char buf [ 60 + 6 + 4 ]; /* 6 for name , 4 for param . num . */ <nl> 
bool X86_getInstruction ( csh ud , const uint8_t * code , size_t code_len , <nl> insn . prefixPresent [ 0x64 ] = 0 ; <nl> insn . prefixPresent [ 0x65 ] = 0 ; <nl> insn . prefixPresent [ 0x66 ] = 0 ; <nl> + insn . prefixPresent [ 0x67 ] = 0 ; <nl> insn . prefixPresent [ 0xf0 ] = 0 ; <nl> insn . prefixPresent [ 0xf2 ] = 0 ; <nl> insn . prefixPresent [ 0xf3 ] = 0 ;
int decodeInstruction ( struct InternalInstruction * insn , <nl>  <nl> insn -> length = ( size_t )( insn -> readerCursor - insn -> startLocation ); <nl>  <nl> + if ( insn -> length > 15 ) <nl> + return - 1 ; <nl> + <nl> // dbgprintf ( insn , " Read from 0x % llx to 0x % llx : length % zu ", <nl> // startLoc , insn -> readerCursor , insn -> length ); <nl> 
class MDentryLink : public Message { <nl> dirfrag ( df ), <nl> dn ( n ), <nl> is_primary ( p ) {} <nl> + private : <nl> + ~ MDentryLink () {} <nl>  <nl> + public : <nl> const char * get_type_name () { return " dentry_link ";} <nl> void print ( ostream & o ) { <nl> o << " dentry_link (" << dirfrag << " " << dn << ")";
unsigned ceph_str_hash_rjenkins ( const char * str , unsigned length ) <nl> unsigned ceph_str_hash_linux ( const char * str , unsigned length ) <nl> { <nl> unsigned long hash = 0 ; <nl> - unsigned char c ; <nl>  <nl> while ( length --) { <nl> - c = * str ++; <nl> + unsigned char c = * str ++; <nl> hash = ( hash + ( c << 4 ) + ( c >> 4 )) * 11 ; <nl> } <nl> return hash ;
int validate_pool ( IoCtx & io_ctx , CephContext * cct ) { <nl> snap_name ), <nl> boost :: bind (& ImageWatcher :: notify_snap_remove , <nl> ictx -> image_watcher , snap_name )); <nl> - if ( r < 0 && r != - EEXIST ) { <nl> + if ( r < 0 && r != - ENOENT ) { <nl> return r ; <nl> } <nl> } else {
int FileStore :: mount () <nl> } <nl>  <nl> dout ( 5 ) << " mount op_seq is " << initial_op_seq << dendl ; <nl> + if ( initial_op_seq == 0 ) { <nl> + derr << " mount initial op seq is 0 ; something is wrong " << dendl ; <nl> + ret = - EINVAL ; <nl> + goto close_current_fd ; <nl> + } <nl>  <nl> if (! btrfs_stable_commits ) { <nl> // mark current / as non - snapshotted so that we don ' t rollback away
int lockdep_will_lock ( const char * name , int id ) <nl> * _dout << "\ npreviously locked at \ n "; <nl> p -> second -> print (* _dout ); <nl> } <nl> + delete bt ; <nl> * _dout << dendl ; <nl> assert ( 0 ); <nl> }
extern " C " int rados_conf_parse_argv_remainder ( rados_t cluster , int argc , <nl> conf -> apply_changes ( NULL ); <nl> assert ( args . size () <= ( unsigned int ) argc ); <nl> unsigned int i ; <nl> - for ( i = 0 ; i < argc ; ++ i ) { <nl> + for ( i = 0 ; i < ( unsigned int ) argc ; ++ i ) { <nl> if ( i < args . size ()) <nl> remargv [ i ] = args [ i ]; <nl> else
bool OSDMonitor :: prepare_command_impl ( MonOpRequestRef op , <nl> } <nl>  <nl> if ( tunable == " straw_calc_version ") { <nl> - if ( value < 0 || value > 2 ) { <nl> + if ( value < 0 || value > 1 ) { <nl> ss << " value must be 0 or 1 ; got " << value ; <nl> err = - EINVAL ; <nl> goto reply ;
void RGWCopyObj_ObjStore_S3 :: send_partial_response ( off_t ofs ) <nl> dump_errno ( s ); <nl>  <nl> end_header ( s , this , " application / xml "); <nl> + dump_start ( s ); <nl> if ( op_ret == 0 ) { <nl> s -> formatter -> open_object_section_in_ns (" CopyObjectResult ", XMLNS_AWS_S3 ); <nl> }
static struct errno_http hterrs [] = { <nl> { EEXIST , " 409 ", " BucketAlreadyExists " }, <nl> { ENOTEMPTY , " 409 ", " BucketNotEmpty " }, <nl> { ERANGE , " 416 ", " InvalidRange " }, <nl> - { 0 , NULL }}; <nl> + { 0 , NULL , NULL }}; <nl>  <nl> void dump_errno ( struct req_state * s , int err , struct rgw_err * rgwerr ) <nl> {
void RGWZoneParams :: decode_json ( JSONObj * obj ) <nl> :: decode_json (" usage_log_pool ", usage_log_pool , obj ); <nl> :: decode_json (" user_keys_pool ", user_keys_pool , obj ); <nl> :: decode_json (" user_email_pool ", user_email_pool , obj ); <nl> + :: decode_json (" user_uid_pool ", user_uid_pool , obj ); <nl> :: decode_json (" user_swift_pool ", user_swift_pool , obj ); <nl> - :: decode_json (" user_uid_pool ", user_uid_pool , obj ); <nl> JSONDecoder :: decode_json (" system_key ", system_key , obj ); <nl> JSONDecoder :: decode_json (" placement_pools ", placement_pools , obj ); <nl> }
OutputDataSocket :: OutputDataSocket ( CephContext * cct , uint64_t _backlog ) <nl> m_shutdown_rd_fd (- 1 ), <nl> m_shutdown_wr_fd (- 1 ), <nl> going_down ( false ), <nl> + data_size ( 0 ), <nl> m_lock (" OutputDataSocket :: m_lock ") <nl> { <nl> }
public : <nl> param ( NULL ) {} <nl> ~ CryptoAESKeyHandler () { <nl> SECITEM_FreeItem ( param , PR_TRUE ); <nl> - PK11_FreeSymKey ( key ); <nl> - PK11_FreeSlot ( slot ); <nl> + if ( key ) <nl> + PK11_FreeSymKey ( key ); <nl> + if ( slot ) <nl> + PK11_FreeSlot ( slot ); <nl> } <nl>  <nl> int init ( const bufferptr & s , ostringstream & err ) {
public : <nl> case CEPH_LOCK_IXATTR : return 8 + 6 * SimpleLock :: WAIT_BITS ; <nl> case CEPH_LOCK_ISNAP : return 8 + 7 * SimpleLock :: WAIT_BITS ; <nl> case CEPH_LOCK_INEST : return 8 + 8 * SimpleLock :: WAIT_BITS ; <nl> + case CEPH_LOCK_IFLOCK : return 8 + 9 * SimpleLock :: WAIT_BITS ; <nl> default : <nl> assert ( 0 ); <nl> }
extern " C " int rados_pool_list ( rados_t cluster , char * buf , size_t len ) <nl> if ( r < 0 ) <nl> return r ; <nl>  <nl> - if (! buf ) <nl> + if ( len > 0 && ! buf ) <nl> return - EINVAL ; <nl>  <nl> char * b = buf ;
extern " C " int rados_pool_list ( rados_t cluster , char * buf , size_t len ) <nl> std :: list < std :: string > pools ; <nl> client -> pool_list ( pools ); <nl>  <nl> + if (! buf ) <nl> + return - EINVAL ; <nl> + <nl> char * b = buf ; <nl> if ( b ) <nl> memset ( b , 0 , len );
EbmlElement * EbmlElement :: FindNextElement ( IOCallback & DataStream , const EbmlSe <nl> ReadIndex = SizeIdx - 1 ; <nl> memmove (& PossibleIdNSize [ 0 ], & PossibleIdNSize [ 1 ], ReadIndex ); <nl> UpperLevel = UpperLevel_original ; <nl> - } while ( MaxDataSize > DataStream . getFilePointer () - SizeIdx + PossibleID_Length ); <nl> + } while ( MaxDataSize >= ReadSize ); <nl>  <nl> return NULL ; <nl> }
rsvg_acquire_file_resource ( const char * filename , const char * base_uri , GError * <nl>  <nl> g_byte_array_append ( array , ( guint8 *) data , length ); <nl> g_free ( data ); <nl> + g_free ( path ); <nl>  <nl> return array ; <nl> }
void set_fat ( DOS_FS * fs , uint32_t cluster , int32_t new ) <nl> data [ 1 ] = new >> 4 ; <nl> } else { <nl> FAT_ENTRY subseqEntry ; <nl> - if ( cluster != fs -> clusters - 1 ) <nl> + if ( cluster != fs -> clusters + 1 ) <nl> get_fat (& subseqEntry , fs -> fat , cluster + 1 , fs ); <nl> else <nl> subseqEntry . value = 0 ;
typedef double iw_tmpsample ; <nl>  <nl> # ifdef IW_64BIT <nl> -# define IW_DEFAULT_MAX_DIMENSION 1000000 <nl> -# define IW_DEFAULT_MAX_MALLOC 2000000000000 <nl> +# define IW_DEFAULT_MAX_DIMENSION 40000 <nl> +# define IW_DEFAULT_MAX_MALLOC 2000000000 <nl> # else <nl> # define IW_DEFAULT_MAX_DIMENSION 40000 // Must be less than sqrt ( 2 ^ 31 ). <nl> # define IW_DEFAULT_MAX_MALLOC 2000000000
IW_IMPL ( int ) iw_read_bmp_file ( struct iw_context * ctx , struct iw_iodescr * iodescr <nl> done : <nl> if (! retval ) { <nl> iw_set_error ( ctx ," BMP read failed "); <nl> + // If we didn ' t call iw_set_input_image , ' img ' still belongs to us , <nl> + // so free its contents . <nl> + iw_free ( ctx , img . pixels ); <nl> } <nl> return retval ; <nl> }
HandleRFBServerMessage ( rfbClient * client ) <nl> return FALSE ; <nl> } <nl>  <nl> - buffer = malloc (( uint64_t ) msg . sct . length + 1 ); <nl> + buffer = malloc ( msg . sct . length + 1 ); <nl>  <nl> if (! ReadFromRFBServer ( client , buffer , msg . sct . length )) { <nl> free ( buffer );
hfs_cat_traverse ( HFS_INFO * hfs , <nl> size_t rec_off ; <nl> hfs_btree_key_cat * key ; <nl> uint8_t retval ; <nl> - uint16_t keylen ; <nl> + int keylen ; <nl>  <nl> // get the record offset in the node <nl> rec_off = <nl> hfs_cat_traverse ( HFS_INFO * hfs , <nl> size_t rec_off ; <nl> hfs_btree_key_cat * key ; <nl> uint8_t retval ; <nl> - uint16_t keylen ; <nl> + int keylen ; <nl>  <nl> // get the record offset in the node <nl> rec_off =
bool st_table_list :: prepare_security ( THD * thd ) <nl> { <nl> List_iterator_fast < TABLE_LIST > tb (* view_tables ); <nl> TABLE_LIST * tbl ; <nl> + DBUG_ENTER (" st_table_list :: prepare_security "); <nl> # ifndef NO_EMBEDDED_ACCESS_CHECKS <nl> Security_context * save_security_ctx = thd -> security_ctx ; <nl> - DBUG_ENTER (" st_table_list :: prepare_security "); <nl>  <nl> DBUG_ASSERT (! prelocking_placeholder ); <nl> if ( prepare_view_securety_context ( thd ))
buf_page_init_for_read ( <nl> } <nl> } <nl>  <nl> + ut_a (! block -> page . buf_fix_count ); <nl> + block -> page . buf_fix_count ++;; <nl> rw_lock_x_lock (& block -> lock ); <nl> mutex_exit (& block -> mutex ); <nl> mutex_exit (& buf_pool -> zip_mutex ); <nl> buf_page_init_for_read ( <nl> } <nl>  <nl> buf_zip_decompress ( block , srv_use_checksums ); <nl> + mutex_enter (& block -> mutex ); <nl> + block -> page . buf_fix_count --; <nl> + mutex_exit (& block -> mutex ); <nl> rw_lock_x_unlock (& block -> lock ); <nl>  <nl> return ( NULL );
void wolfSSL_X509_STORE_CTX_set_time ( WOLFSSL_X509_STORE_CTX * ctx , <nl> { <nl> ( void ) flags ; <nl>  <nl> + if ( ctx == NULL ) <nl> + return ; <nl> + <nl> ctx -> param -> check_time = t ; <nl> ctx -> param -> flags |= WOLFSSL_USE_CHECK_TIME ; <nl> }
int wolfSSL_EVP_MD_type ( const WOLFSSL_EVP_MD * md ) <nl> return WOLFSSL_FAILURE ; <nl> # endif <nl> } <nl> - <nl> +# ifdef SESSION_CERTS <nl> + ssl -> session . chain . count = 0 ; <nl> +# endif <nl> # ifdef KEEP_PEER_CERT <nl> FreeX509 (& ssl -> peerCert ); <nl> InitX509 (& ssl -> peerCert , 0 , ssl -> heap );
static int btree_readpage_end_io_hook ( struct page * page , u64 start , u64 end , <nl> goto err ; <nl> } <nl> found_level = btrfs_header_level ( eb ); <nl> + if ( found_level >= BTRFS_MAX_LEVEL ) { <nl> + btrfs_info ( root -> fs_info , " bad tree block level % d \ n ", <nl> + ( int ) btrfs_header_level ( eb )); <nl> + ret = - EIO ; <nl> + goto err ; <nl> + } <nl>  <nl> btrfs_set_buffer_lockdep_class ( btrfs_header_owner ( eb ), <nl> eb , found_level );
static int tpm_binary_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 5th : delimiter */ <nl> seq_putc ( m , '\ 0 '); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl>  <nl> static int tpm_ascii_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 4th : eventname <= max + \' 0 ' delimiter */ <nl> seq_printf ( m , " % s \ n ", eventname ); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl> 
void apply_paravirt ( struct paravirt_patch_site * start , <nl> unsigned int used ; <nl>  <nl> BUG_ON ( p -> len > MAX_PATCH_LEN ); <nl> + /* prep the buffer with the original instructions */ <nl> + memcpy ( insnbuf , p -> instr , p -> len ); <nl> used = paravirt_ops . patch ( p -> instrtype , p -> clobbers , insnbuf , <nl> ( unsigned long ) p -> instr , p -> len ); <nl> 
again : <nl>  <nl> # ifdef CONFIG_MODULE_SIG <nl> mod -> sig_ok = info -> sig_ok ; <nl> - if (! mod -> sig_ok ) <nl> + if (! mod -> sig_ok ) { <nl> + printk_once ( KERN_NOTICE <nl> + "% s : module verification failed : signature and / or " <nl> + " required key missing - tainting kernel \ n ", <nl> + mod -> name ); <nl> add_taint_module ( mod , TAINT_FORCED_MODULE ); <nl> + } <nl> # endif <nl>  <nl> /* Now module is in final location , initialize linked lists , etc . */
static int try_to_unmap_file ( struct page * page , enum ttu_flags flags ) <nl> unsigned long max_nl_size = 0 ; <nl> unsigned int mapcount ; <nl>  <nl> + if ( PageHuge ( page )) <nl> + pgoff = page -> index << compound_order ( page ); <nl> + <nl> mutex_lock (& mapping -> i_mmap_mutex ); <nl> vma_interval_tree_foreach ( vma , & mapping -> i_mmap , pgoff , pgoff ) { <nl> unsigned long address = vma_address ( page , vma );
static int quota_setinfo ( struct super_block * sb , int type , void __user * addr ) <nl>  <nl> static void copy_to_if_dqblk ( struct if_dqblk * dst , struct fs_disk_quota * src ) <nl> { <nl> + memset ( dst , 0 , sizeof (* dst )); <nl> dst -> dqb_bhardlimit = src -> d_blk_hardlimit ; <nl> dst -> dqb_bsoftlimit = src -> d_blk_softlimit ; <nl> dst -> dqb_curspace = src -> d_bcount ;
static int palmas_gpio_probe ( struct platform_device * pdev ) <nl> const struct palmas_device_data * dev_data ; <nl>  <nl> match = of_match_device ( of_palmas_gpio_match , & pdev -> dev ); <nl> + if (! match ) <nl> + return - ENODEV ; <nl> dev_data = match -> data ; <nl> if (! dev_data ) <nl> dev_data = & palmas_dev_data ;
static int omap_hdmi_dai_hw_params ( struct snd_pcm_substream * substream , <nl> /* <nl> * fill the IEC - 60958 channel status word <nl> */ <nl> + /* initialize the word bytes */ <nl> + memset ( iec -> status , 0 , sizeof ( iec -> status )); <nl>  <nl> /* specify IEC - 60958 - 3 ( commercial use ) */ <nl> iec -> status [ 0 ] &= ~ IEC958_AES0_PROFESSIONAL ;
static inline unsigned long long res_counter_margin ( struct res_counter * cnt ) <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& cnt -> lock , flags ); <nl> - margin = cnt -> limit - cnt -> usage ; <nl> + if ( cnt -> limit > cnt -> usage ) <nl> + margin = cnt -> limit - cnt -> usage ; <nl> + else <nl> + margin = 0 ; <nl> spin_unlock_irqrestore (& cnt -> lock , flags ); <nl> return margin ; <nl> }
static unsigned int iwl_hw_get_beacon_cmd ( struct iwl_priv * priv , <nl> sizeof ( frame -> u ) - sizeof (* tx_beacon_cmd )); <nl> if ( WARN_ON_ONCE ( frame_size > MAX_MPDU_SIZE )) <nl> return 0 ; <nl> + if (! frame_size ) <nl> + return 0 ; <nl>  <nl> /* Set up TX command fields */ <nl> tx_beacon_cmd -> tx . len = cpu_to_le16 (( u16 ) frame_size );
i915_gem_object_finish_gpu ( struct drm_i915_gem_object * obj ) <nl> return ret ; <nl> } <nl>  <nl> + ret = i915_gem_object_wait_rendering ( obj ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* Ensure that we invalidate the GPU ' s caches and TLBs . */ <nl> obj -> base . read_domains &= ~ I915_GEM_GPU_DOMAINS ; <nl> - <nl> - return i915_gem_object_wait_rendering ( obj ); <nl> + return 0 ; <nl> } <nl>  <nl> /**
int __devinit mthca_init_eq_table ( struct mthca_dev * dev ) <nl> dev -> eq_table . clr_mask = <nl> swab32 ( 1 << ( dev -> eq_table . inta_pin & 31 )); <nl> dev -> eq_table . clr_int = dev -> clr_base + <nl> - ( dev -> eq_table . inta_pin < 31 ? 4 : 0 ); <nl> + ( dev -> eq_table . inta_pin < 32 ? 4 : 0 ); <nl> } <nl>  <nl> dev -> eq_table . arm_mask = 0 ;
enum node_states { <nl> # else <nl> N_HIGH_MEMORY = N_NORMAL_MEMORY , <nl> # endif <nl> + N_MEMORY = N_HIGH_MEMORY , <nl> N_CPU , /* The node has one or more cpus */ <nl> NR_NODE_STATES <nl> };
setup_efi_state ( struct boot_params * params , unsigned long params_load_addr , <nl> if ( efi_enabled ( EFI_OLD_MEMMAP )) <nl> return 0 ; <nl>  <nl> + params -> secure_boot = boot_params . secure_boot ; <nl> ei -> efi_loader_signature = current_ei -> efi_loader_signature ; <nl> ei -> efi_systab = current_ei -> efi_systab ; <nl> ei -> efi_systab_hi = current_ei -> efi_systab_hi ;
bool iwl_mvm_bt_coex_is_shared_ant_avail ( struct iwl_mvm * mvm ) <nl> if (!( mvm -> fw -> ucode_capa . api [ 0 ] & IWL_UCODE_TLV_API_BT_COEX_SPLIT )) <nl> return iwl_mvm_bt_coex_is_shared_ant_avail_old ( mvm ); <nl>  <nl> - return le32_to_cpu ( mvm -> last_bt_notif . bt_activity_grading ) == BT_OFF ; <nl> + return le32_to_cpu ( mvm -> last_bt_notif . bt_activity_grading ) < BT_HIGH_TRAFFIC ; <nl> } <nl>  <nl> bool iwl_mvm_bt_coex_is_tpc_allowed ( struct iwl_mvm * mvm ,
static ssize_t cxlflash_show_port_status ( struct device * dev , <nl> u64 * fc_regs ; <nl>  <nl> rc = kstrtouint (( attr -> attr . name + 4 ), 10 , & port ); <nl> - if ( rc || ( port > NUM_FC_PORTS )) <nl> + if ( rc || ( port >= NUM_FC_PORTS )) <nl> return 0 ; <nl>  <nl> fc_regs = & afu -> afu_map -> global . fc_regs [ port ][ 0 ];
static int wm9713_soc_probe ( struct snd_soc_codec * codec ) <nl> if ( IS_ERR ( wm9713 -> ac97 )) <nl> return PTR_ERR ( wm9713 -> ac97 ); <nl>  <nl> - regmap = devm_regmap_init_ac97 ( wm9713 -> ac97 , & wm9713_regmap_config ); <nl> + regmap = regmap_init_ac97 ( wm9713 -> ac97 , & wm9713_regmap_config ); <nl> if ( IS_ERR ( regmap )) { <nl> snd_soc_free_ac97_codec ( wm9713 -> ac97 ); <nl> return PTR_ERR ( regmap );
static int ks8695_poll ( struct napi_struct * napi , int budget ) <nl> if ( work_done < budget ) { <nl> unsigned long flags ; <nl> spin_lock_irqsave (& ksp -> rx_lock , flags ); <nl> + __napi_complete ( napi ); <nl> /* enable rx interrupt */ <nl> writel ( isr | mask_bit , KS8695_IRQ_VA + KS8695_INTEN ); <nl> - __napi_complete ( napi ); <nl> spin_unlock_irqrestore (& ksp -> rx_lock , flags ); <nl> } <nl> return work_done ;
static void qeth_clear_output_buffer ( struct qeth_qdio_out_q * queue , <nl> buf -> buffer -> element [ i ]. addr = NULL ; <nl> buf -> buffer -> element [ i ]. flags = 0 ; <nl> } <nl> + buf -> buffer -> element [ 15 ]. flags = 0 ; <nl> buf -> next_element_to_fill = 0 ; <nl> atomic_set (& buf -> state , QETH_QDIO_BUF_EMPTY ); <nl> }
int wusbhc_chid_set ( struct wusbhc * wusbhc , const struct wusb_ckhdid * chid ) <nl> { <nl> int result = 0 ; <nl>  <nl> - if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof ( chid )) == 0 ) <nl> + if ( memcmp ( chid , & wusb_ckhdid_zero , sizeof (* chid )) == 0 ) <nl> chid = NULL ; <nl>  <nl> mutex_lock (& wusbhc -> mutex );
rcu_torture_init ( void ) <nl> writer_task = NULL ; <nl> goto unwind ; <nl> } <nl> - reader_tasks = kmalloc ( nrealreaders * sizeof ( reader_tasks [ 0 ]), <nl> + reader_tasks = kzalloc ( nrealreaders * sizeof ( reader_tasks [ 0 ]), <nl> GFP_KERNEL ); <nl> if ( reader_tasks == NULL ) { <nl> VERBOSE_PRINTK_ERRSTRING (" out of memory ");
int sst_block_alloc_scratch ( struct sst_dsp * dsp ) <nl> ret = block_list_prepare ( dsp , & dsp -> scratch_block_list ); <nl> if ( ret < 0 ) { <nl> dev_err ( dsp -> dev , " error : scratch block prepare failed \ n "); <nl> + mutex_unlock (& dsp -> mutex ); <nl> return ret ; <nl> } <nl> 
int iwl_power_update_mode ( struct iwl_priv * priv , bool force ) <nl> if ( priv -> cfg -> ops -> lib -> update_chain_flags && <nl> update_chains ) <nl> priv -> cfg -> ops -> lib -> update_chain_flags ( priv ); <nl> - else <nl> + else if ( priv -> cfg -> ops -> lib -> update_chain_flags ) <nl> IWL_DEBUG_POWER ( priv , <nl> " Cannot update the power , chain noise " <nl> " calibration running : % d \ n ",
static unsigned int bsg_poll ( struct file * file , poll_table * wait ) <nl> spin_lock_irq (& bd -> lock ); <nl> if (! list_empty (& bd -> done_list )) <nl> mask |= POLLIN | POLLRDNORM ; <nl> - if ( bd -> queued_cmds >= bd -> max_queue ) <nl> + if ( bd -> queued_cmds < bd -> max_queue ) <nl> mask |= POLLOUT ; <nl> spin_unlock_irq (& bd -> lock ); <nl> 
 <nl> # define BOUNCE_SIZE ( 64 * 1024 ) <nl>  <nl> -# define PS3ROM_MAX_SECTORS ( BOUNCE_SIZE / CD_FRAMESIZE ) <nl> +# define PS3ROM_MAX_SECTORS ( BOUNCE_SIZE >> 9 ) <nl>  <nl>  <nl> struct ps3rom_private {
static int cy_put_char ( struct tty_struct * tty , unsigned char ch ) <nl> return 0 ; <nl>  <nl> if (! info -> xmit_buf ) <nl> - return ; <nl> + return 0 ; <nl>  <nl> local_irq_save ( flags ); <nl> if ( info -> xmit_cnt >= PAGE_SIZE - 1 ) {
void ath10k_thermal_set_throttling ( struct ath10k * ar ) <nl>  <nl> lockdep_assert_held (& ar -> conf_mutex ); <nl>  <nl> + if (! ar -> wmi . ops -> gen_pdev_set_quiet_mode ) <nl> + return ; <nl> + <nl> if ( ar -> state != ATH10K_STATE_ON ) <nl> return ; <nl> 
static int __devinit tg3_get_invariants ( struct tg3 * tp ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( GET_ASIC_REV ( tp -> pci_chip_rev_id ) == ASIC_REV_5717 && <nl> + ( tp -> pci_chip_rev_id != CHIPREV_ID_5717_A0 || <nl> + ( tp -> tg3_flags2 & TG3_FLG2_MII_SERDES ))) <nl> + return - ENOTSUPP ; <nl> + <nl> /* Initialize data / descriptor byte / word swapping . */ <nl> val = tr32 ( GRC_MODE ); <nl> val &= GRC_MODE_HOST_STACKUP ;
offset_store ( struct md_rdev * rdev , const char * buf , size_t len ) <nl> * can be sane */ <nl> return - EBUSY ; <nl> rdev -> data_offset = offset ; <nl> + rdev -> new_data_offset = offset ; <nl> return len ; <nl> } <nl> 
static struct file * do_last ( struct nameidata * nd , struct path * path , <nl> if ( error ) <nl> return ERR_PTR ( error ); <nl> error = - EISDIR ; <nl> - if ( S_ISDIR ( nd -> inode -> i_mode )) <nl> + if (( open_flag & O_CREAT ) && S_ISDIR ( nd -> inode -> i_mode )) <nl> goto exit ; <nl> error = - ENOTDIR ; <nl> if (( nd -> flags & LOOKUP_DIRECTORY ) && ! nd -> inode -> i_op -> lookup )
static int setup ( struct spi_device * spi ) <nl> if (( chip -> chip_select_num > 0 ) <nl> && ( chip -> chip_select_num <= spi -> master -> num_chipselect )) <nl> peripheral_request ( ssel [ spi -> master -> bus_num ] <nl> - [ chip -> chip_select_num - 1 ], DRV_NAME ); <nl> + [ chip -> chip_select_num - 1 ], spi -> modalias ); <nl>  <nl> cs_deactive ( drv_data , chip ); <nl> 
static int pch_phub_write_gbe_mac_addr ( struct pch_phub_reg * chip , u8 * data ) <nl> int retval ; <nl> int i ; <nl>  <nl> - if ( chip -> ioh_type == 1 ) /* EG20T */ <nl> + if (( chip -> ioh_type == 1 ) || ( chip -> ioh_type == 5 )) /* EG20T or ML7831 */ <nl> retval = pch_phub_gbe_serial_rom_conf ( chip ); <nl> else /* ML7223 */ <nl> retval = pch_phub_gbe_serial_rom_conf_mp ( chip );
static int fb_deferred_io_mkwrite ( struct vm_area_struct * vma , <nl> deferred framebuffer IO . then if userspace touches a page <nl> again , we repeat the same scheme */ <nl>  <nl> + file_update_time ( vma -> vm_file ); <nl> + <nl> /* protect against the workqueue changing the page list */ <nl> mutex_lock (& fbdefio -> lock ); <nl> 
static void svm_get_segment ( struct kvm_vcpu * vcpu , <nl> if ( seg == VCPU_SREG_CS ) <nl> var -> g = s -> limit > 0xfffff ; <nl>  <nl> + /* <nl> + * Work around a bug where the busy flag in the tr selector <nl> + * isn ' t exposed <nl> + */ <nl> + if ( seg == VCPU_SREG_TR ) <nl> + var -> type |= 0x2 ; <nl> + <nl> var -> unusable = ! var -> present ; <nl> } <nl> 
vxge_hw_device_initialize ( <nl> __vxge_hw_device_pci_e_init ( hldev ); <nl>  <nl> status = __vxge_hw_device_reg_addr_get ( hldev ); <nl> - if ( status != VXGE_HW_OK ) <nl> + if ( status != VXGE_HW_OK ) { <nl> + vfree ( hldev ); <nl> goto exit ; <nl> + } <nl> __vxge_hw_device_id_get ( hldev ); <nl>  <nl> __vxge_hw_device_host_info_get ( hldev );
mapping_unwind : <nl> mapping_error : <nl> if ( net_ratelimit ()) <nl> dev_warn (& hw -> pdev -> dev , "% s : tx mapping error \ n ", dev -> name ); <nl> - dev_kfree_skb ( skb ); <nl> + dev_kfree_skb_any ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
u16 ieee80211_select_queue ( struct ieee80211_sub_if_data * sdata , <nl> return IEEE80211_AC_BE ; <nl> } <nl>  <nl> + if ( skb -> protocol == sdata -> control_port_protocol ) { <nl> + skb -> priority = 7 ; <nl> + return ieee80211_downgrade_queue ( sdata , skb ); <nl> + } <nl> + <nl> /* use the data classifier to determine what 802 . 1d tag the <nl> * data frame has */ <nl> rcu_read_lock ();
static void kvm_write_wall_clock ( struct kvm * kvm , gpa_t wall_clock ) <nl> */ <nl> getboottime (& boot ); <nl>  <nl> + if ( kvm -> arch . kvmclock_offset ) { <nl> + struct timespec ts = ns_to_timespec ( kvm -> arch . kvmclock_offset ); <nl> + boot = timespec_sub ( boot , ts ); <nl> + } <nl> wc . sec = boot . tv_sec ; <nl> wc . nsec = boot . tv_nsec ; <nl> wc . version = version ;
static int match_session ( struct cifs_ses * ses , struct smb_vol * vol ) <nl> vol -> username ? vol -> username : "", <nl> CIFS_MAX_USERNAME_LEN )) <nl> return 0 ; <nl> - if ( strlen ( vol -> username ) != 0 && <nl> + if (( vol -> username && strlen ( vol -> username ) != 0 ) && <nl> ses -> password != NULL && <nl> strncmp ( ses -> password , <nl> vol -> password ? vol -> password : "",
int tipc_node_get_linkname ( u32 bearer_id , u32 addr , char * linkname , size_t len ) <nl> struct tipc_link * link ; <nl> struct tipc_node * node = tipc_node_find ( addr ); <nl>  <nl> - if (( bearer_id > MAX_BEARERS ) || ! node ) <nl> + if (( bearer_id >= MAX_BEARERS ) || ! node ) <nl> return - EINVAL ; <nl> tipc_node_lock ( node ); <nl> link = node -> links [ bearer_id ];
int dasd_eer_enable ( struct dasd_device * device ) <nl> cqr -> device = device ; <nl> cqr -> retries = 255 ; <nl> cqr -> expires = 10 * HZ ; <nl> + clear_bit ( DASD_CQR_FLAGS_USE_ERP , & cqr -> flags ); <nl>  <nl> cqr -> cpaddr -> cmd_code = DASD_ECKD_CCW_SNSS ; <nl> cqr -> cpaddr -> count = SNSS_DATA_SIZE ;
void machine_shutdown ( void ) <nl> */ <nl> void machine_halt ( void ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> local_irq_disable (); <nl> void machine_halt ( void ) <nl> */ <nl> void machine_power_off ( void ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> if ( pm_power_off ) <nl> void machine_power_off ( void ) <nl> */ <nl> void machine_restart ( char * cmd ) <nl> { <nl> + local_irq_disable (); <nl> smp_send_stop (); <nl>  <nl> arm_pm_restart ( reboot_mode , cmd );
ath5k_hw_set_antenna_mode ( struct ath5k_hw * ah , u8 ant_mode ) <nl> u8 def_ant , tx_ant , ee_mode ; <nl> u32 sta_id1 = 0 ; <nl>  <nl> + /* if channel is not initialized yet we can ' t set the antennas <nl> + * so just store the mode . it will be set on the next reset */ <nl> + if ( channel == NULL ) { <nl> + ah -> ah_ant_mode = ant_mode ; <nl> + return ; <nl> + } <nl> + <nl> def_ant = ah -> ah_def_ant ; <nl>  <nl> ATH5K_TRACE ( ah -> ah_sc );
int inet6_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl> */ <nl> v4addr = LOOPBACK4_IPV6 ; <nl> if (!( addr_type & IPV6_ADDR_MULTICAST )) { <nl> - if (! inet -> transparent && <nl> + if (!( inet -> freebind || inet -> transparent ) && <nl> ! ipv6_chk_addr ( net , & addr -> sin6_addr , <nl> dev , 0 )) { <nl> err = - EADDRNOTAVAIL ;
__alloc_bootmem_core ( struct bootmem_data * bdata , unsigned long size , <nl> if ( limit && bdata -> node_boot_start >= limit ) <nl> return NULL ; <nl>  <nl> + /* on nodes without memory - bootmem_map is NULL */ <nl> + if (! bdata -> node_bootmem_map ) <nl> + return NULL ; <nl> + <nl> end_pfn = bdata -> node_low_pfn ; <nl> limit = PFN_DOWN ( limit ); <nl> if ( limit && end_pfn > limit )
static int atmel_prepare_rx_dma ( struct uart_port * port ) <nl> BUG_ON (! PAGE_ALIGNED ( ring -> buf )); <nl> sg_set_page (& atmel_port -> sg_rx , <nl> virt_to_page ( ring -> buf ), <nl> - ATMEL_SERIAL_RINGSIZE , <nl> + sizeof ( struct atmel_uart_char ) * ATMEL_SERIAL_RINGSIZE , <nl> ( int ) ring -> buf & ~ PAGE_MASK ); <nl> nent = dma_map_sg ( port -> dev , <nl> & atmel_port -> sg_rx ,
struct oz_urb_link { <nl> /* Holds state information about a USB endpoint . <nl> */ <nl> # define OZ_EP_BUFFER_SIZE_ISOC ( 1024 * 24 ) <nl> +# define OZ_EP_BUFFER_SIZE_INT 512 <nl> struct oz_endpoint { <nl> struct list_head urb_list ; /* List of oz_urb_link items . */ <nl> struct list_head link ; /* For isoc ep , links in to isoc <nl> static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> buffer_size = OZ_EP_BUFFER_SIZE_ISOC ; <nl> break ; <nl> case USB_ENDPOINT_XFER_INT : <nl> - buffer_size = 128 ; <nl> + buffer_size = OZ_EP_BUFFER_SIZE_INT ; <nl> break ; <nl> } <nl> }
static int FNAME ( fix_write_pf )( struct kvm_vcpu * vcpu , <nl> struct kvm_mmu_page * page ; <nl>  <nl> if ( is_writeble_pte (* shadow_ent )) <nl> - return 0 ; <nl> + return ! user || (* shadow_ent & PT_USER_MASK ); <nl>  <nl> writable_shadow = * shadow_ent & PT_SHADOW_WRITABLE_MASK ; <nl> if ( user ) {
void falcon_remove_nic ( struct efx_nic * efx ) <nl> struct falcon_nic_data * nic_data = efx -> nic_data ; <nl> int rc ; <nl>  <nl> + /* Remove I2C adapter and clear it in preparation for a retry */ <nl> rc = i2c_del_adapter (& efx -> i2c_adap ); <nl> BUG_ON ( rc ); <nl> + memset (& efx -> i2c_adap , 0 , sizeof ( efx -> i2c_adap )); <nl>  <nl> falcon_remove_spi_devices ( efx ); <nl> falcon_free_buffer ( efx , & efx -> irq_status );
static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
static void __init stp_reset ( void ) <nl>  <nl> stp_page = alloc_bootmem_pages ( PAGE_SIZE ); <nl> rc = chsc_sstpc ( stp_page , STP_OP_CTRL , 0x0000 ); <nl> - if ( rc == 1 ) <nl> + if ( rc == 0 ) <nl> set_bit ( CLOCK_SYNC_HAS_STP , & clock_sync_flags ); <nl> else if ( stp_online ) { <nl> printk ( KERN_WARNING " Running on non STP capable machine .\ n ");
static int hda_reg_write ( void * context , unsigned int reg , unsigned int val ) <nl> unsigned int verb ; <nl> int i , bytes , err ; <nl>  <nl> + if ( codec -> caps_overwriting ) <nl> + return 0 ; <nl> + <nl> reg &= ~ 0x00080000U ; /* drop GET bit */ <nl> reg |= ( codec -> addr << 28 ); <nl> verb = get_verb ( reg );
static int perf_tp_filter_match ( struct perf_event * event , <nl> { <nl> void * record = data -> raw -> data ; <nl>  <nl> + /* only top level events have filters set */ <nl> + if ( event -> parent ) <nl> + event = event -> parent ; <nl> + <nl> if ( likely (! event -> filter ) || filter_match_preds ( event -> filter , record )) <nl> return 1 ; <nl> return 0 ;
 <nl> # define FIRST_VM86_IRQ 3 <nl> # define LAST_VM86_IRQ 15 <nl> -# define invalid_vm86_irq ( irq ) (( irq ) < 3 || ( irq ) > 15 ) <nl> + <nl> +# ifndef __ASSEMBLY__ <nl> + static inline int invalid_vm86_irq ( int irq ) <nl> +{ <nl> + return irq < 3 || irq > 15 ; <nl> +} <nl> +# endif <nl>  <nl> /* <nl> * Size the maximum number of interrupts .
static int stmmac_set_coalesce ( struct net_device * dev , <nl> ( ec -> tx_max_coalesced_frames == 0 )) <nl> return - EINVAL ; <nl>  <nl> - if (( ec -> tx_coalesce_usecs > STMMAC_COAL_TX_TIMER ) || <nl> + if (( ec -> tx_coalesce_usecs > STMMAC_MAX_COAL_TX_TICK ) || <nl> ( ec -> tx_max_coalesced_frames > STMMAC_TX_MAX_FRAMES )) <nl> return - EINVAL ; <nl> 
static int cfg80211_netdev_notifier_call ( struct notifier_block * nb , <nl> * Configure power management to the driver here so that its <nl> * correctly set also after interface type changes etc . <nl> */ <nl> - if ( wdev -> iftype == NL80211_IFTYPE_STATION && <nl> + if (( wdev -> iftype == NL80211_IFTYPE_STATION || <nl> + wdev -> iftype == NL80211_IFTYPE_P2P_CLIENT ) && <nl> rdev -> ops -> set_power_mgmt ) <nl> if ( rdev -> ops -> set_power_mgmt ( wdev -> wiphy , dev , <nl> wdev -> ps ,
dispatch_ioctl ( struct client * client , unsigned int cmd , void __user * arg ) <nl> return - EFAULT ; <nl> } <nl>  <nl> - return 0 ; <nl> + return retval ; <nl> } <nl>  <nl> static long
static inline void __init ulite_console_of_find_device ( int id ) <nl> continue ; <nl>  <nl> ulite_ports [ id ]. mapbase = res . start ; <nl> + of_node_put ( np ); <nl> return ; <nl> } <nl> }
mpt_attach ( struct pci_dev * pdev , const struct pci_device_id * id ) <nl> ioc -> msi_enable = 0 ; <nl> break ; <nl> } <nl> + <nl> + ioc -> fw_events_off = 1 ; <nl> + <nl> if ( ioc -> errata_flag_1064 ) <nl> pci_disable_io_access ( pdev ); <nl> 
static int __dma_supported ( struct device * dev , u64 mask , bool warn ) <nl> */ <nl> if ( sizeof ( mask ) != sizeof ( dma_addr_t ) && <nl> mask > ( dma_addr_t )~ 0 && <nl> - dma_to_pfn ( dev , ~ 0 ) < max_pfn ) { <nl> + dma_to_pfn ( dev , ~ 0 ) < max_pfn - 1 ) { <nl> if ( warn ) { <nl> dev_warn ( dev , " Coherent DMA mask %# llx is larger than dma_addr_t allows \ n ", <nl> mask );
# define gadget_is_musbhsfc ( g ) 0 <nl> # endif <nl>  <nl> -/* Mentor high speed " dual role " controller , peripheral mode */ <nl> -# ifdef CONFIG_USB_GADGET_MUSBHDRC <nl> -# define gadget_is_musbhdrc ( g ) ! strcmp (" musbhdrc_udc ", ( g )-> name ) <nl> +/* Mentor high speed " dual role " controller , in peripheral role */ <nl> +# ifdef CONFIG_USB_GADGET_MUSB_HDRC <nl> +# define gadget_is_musbhdrc ( g ) ! strcmp (" musb_hdrc ", ( g )-> name ) <nl> # else <nl> # define gadget_is_musbhdrc ( g ) 0 <nl> # endif
static int bug_handler ( struct pt_regs * regs , unsigned int esr ) <nl> break ; <nl>  <nl> case BUG_TRAP_TYPE_WARN : <nl> + /* Ideally , report_bug () should backtrace for us ... but no . */ <nl> + dump_backtrace ( regs , NULL ); <nl> break ; <nl>  <nl> default :
static int twlreg_disable ( struct regulator_dev * rdev ) <nl> return grp ; <nl>  <nl> if ( twl_class_is_4030 ()) <nl> - grp &= ~ P1_GRP_4030 ; <nl> + grp &= ~( P1_GRP_4030 | P2_GRP_4030 | P3_GRP_4030 ); <nl> else <nl> - grp &= ~ P1_GRP_6030 ; <nl> + grp &= ~( P1_GRP_6030 | P2_GRP_6030 | P3_GRP_6030 ); <nl>  <nl> return twlreg_write ( info , TWL_MODULE_PM_RECEIVER , VREG_GRP , grp ); <nl> }
static int parse_audio_selector_unit ( struct mixer_build * state , int unitid , void <nl> if (! len && check_input_term ( state , desc -> baSourceID [ i ], & iterm ) >= 0 ) <nl> len = get_term_name ( state , & iterm , namelist [ i ], MAX_ITEM_NAME_LEN , 0 ); <nl> if (! len ) <nl> - sprintf ( namelist [ i ], " Input % d ", i ); <nl> + sprintf ( namelist [ i ], " Input % u ", i ); <nl> } <nl>  <nl> kctl = snd_ctl_new1 (& mixer_selectunit_ctl , cval );
struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( ieee80211_vif_is_mesh (& sdata -> vif ) && <nl> ! sdata -> u . mesh . user_mpm ) <nl> init_timer (& sta -> plink_timer ); <nl> + sta -> nonpeer_pm = NL80211_MESH_POWER_ACTIVE ; <nl> # endif <nl>  <nl> memcpy ( sta -> sta . addr , addr , ETH_ALEN );
int ceph_osdc_create_event ( struct ceph_osd_client * osdc , <nl> event -> data = data ; <nl> event -> osdc = osdc ; <nl> INIT_LIST_HEAD (& event -> osd_node ); <nl> + RB_CLEAR_NODE (& event -> node ); <nl> kref_init (& event -> kref ); /* one ref for us */ <nl> kref_get (& event -> kref ); /* one ref for the caller */ <nl> init_completion (& event -> completion );
static int mwifiex_pcie_event_complete ( struct mwifiex_adapter * adapter , <nl>  <nl> if (! card -> evt_buf_list [ rdptr ]) { <nl> skb_push ( skb , INTF_HEADER_LEN ); <nl> + skb_put ( skb , MAX_EVENT_SIZE - skb -> len ); <nl> + memset ( skb -> data , 0 , MAX_EVENT_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , <nl> MAX_EVENT_SIZE , <nl> PCI_DMA_FROMDEVICE ))
void igb_power_up_link ( struct igb_adapter * adapter ) <nl> igb_power_up_phy_copper (& adapter -> hw ); <nl> else <nl> igb_power_up_serdes_link_82575 (& adapter -> hw ); <nl> + igb_reset_phy (& adapter -> hw ); <nl> } <nl>  <nl> /**
static void do_reads ( struct mirror_set * ms , struct bio_list * reads ) <nl> /* <nl> * We can only read balance if the region is in sync . <nl> */ <nl> - if ( rh_in_sync (& ms -> rh , region , 0 )) <nl> + if ( rh_in_sync (& ms -> rh , region , 1 )) <nl> m = choose_mirror ( ms , bio -> bi_sector ); <nl> else <nl> m = ms -> default_mirror ;
static int wm8994_device_init ( struct wm8994 * wm8994 , int irq ) <nl> struct regmap_config * regmap_config ; <nl> const struct reg_default * regmap_patch = NULL ; <nl> const char * devname ; <nl> - int ret , i , patch_regs ; <nl> + int ret , i , patch_regs = 0 ; <nl> int pulls = 0 ; <nl>  <nl> if ( dev_get_platdata ( wm8994 -> dev )) {
static int sky2_rx_start ( struct sky2_port * sky2 ) <nl> rx_set_checksum ( sky2 ); <nl>  <nl> /* Space needed for frame data + headers rounded up */ <nl> - size = ALIGN ( sky2 -> netdev -> mtu + ETH_HLEN + VLAN_HLEN , 8 ) <nl> - + 8 ; <nl> + size = roundup ( sky2 -> netdev -> mtu + ETH_HLEN + VLAN_HLEN , 8 ); <nl>  <nl> /* Stopping point for hardware truncation */ <nl> thresh = ( size - 8 ) / sizeof ( u32 );
int __kvm_set_memory_region ( struct kvm * kvm , <nl> goto out ; <nl> if ( mem -> guest_phys_addr & ( PAGE_SIZE - 1 )) <nl> goto out ; <nl> + if ( mem -> userspace_addr & ( PAGE_SIZE - 1 )) <nl> + goto out ; <nl> if ( mem -> slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS ) <nl> goto out ; <nl> if ( mem -> guest_phys_addr + mem -> memory_size < mem -> guest_phys_addr )
static int snd_sb_csp_ioctl ( struct snd_hwdep * hw , struct file * file , unsigned i <nl> switch ( cmd ) { <nl> /* get information */ <nl> case SNDRV_SB_CSP_IOCTL_INFO : <nl> + memset (& info , 0 , sizeof ( info )); <nl> * info . codec_name = * p -> codec_name ; <nl> info . func_nr = p -> func_nr ; <nl> info . acc_format = p -> acc_format ;
static void vmbus_process_offer ( struct work_struct * work ) <nl> spin_lock_irqsave (& vmbus_connection . channel_lock , flags ); <nl> list_del (& newchannel -> listentry ); <nl> spin_unlock_irqrestore (& vmbus_connection . channel_lock , flags ); <nl> + kfree ( newchannel -> device_obj ); <nl>  <nl> free_channel ( newchannel ); <nl> } else {
restart : <nl> if ( radix_tree_deref_retry ( entry )) <nl> goto restart ; <nl>  <nl> - irq = create_irq (); <nl> + irq = irq_alloc_desc ( numa_node_id ()); <nl> if ( unlikely ( irq < 0 )) { <nl> pr_err (" no more free IRQs , bailing ..\ n "); <nl> break ; <nl> } <nl>  <nl> + activate_irq ( irq ); <nl> + <nl> pr_info (" Setting up a chained VIRQ from % d -> % d \ n ", <nl> irq , entry -> pirq ); <nl> 
build_unc_path_to_root ( const struct smb_vol * vol , <nl> pos = full_path + unc_len ; <nl>  <nl> if ( pplen ) { <nl> - * pos ++ = CIFS_DIR_SEP ( cifs_sb ); <nl> - strncpy ( pos , vol -> prepath , pplen ); <nl> + * pos = CIFS_DIR_SEP ( cifs_sb ); <nl> + strncpy ( pos + 1 , vol -> prepath , pplen ); <nl> pos += pplen ; <nl> } <nl> 
static int ttm_buffer_object_transfer ( struct ttm_buffer_object * bo , <nl> INIT_LIST_HEAD (& fbo -> lru ); <nl> INIT_LIST_HEAD (& fbo -> swap ); <nl> fbo -> vm_node = NULL ; <nl> + atomic_set (& fbo -> cpu_writers , 0 ); <nl>  <nl> fbo -> sync_obj = driver -> sync_obj_ref ( bo -> sync_obj ); <nl> kref_init (& fbo -> list_kref );
static bool igbvf_clean_rx_irq ( struct igbvf_adapter * adapter , <nl> dma_unmap_single (& pdev -> dev , buffer_info -> dma , <nl> adapter -> rx_ps_hdr_size , <nl> DMA_FROM_DEVICE ); <nl> + buffer_info -> dma = 0 ; <nl> skb_put ( skb , hlen ); <nl> } <nl> 
static int gfs2_write_end ( struct file * file , struct address_space * mapping , <nl> } <nl>  <nl> brelse ( dibh ); <nl> - gfs2_trans_end ( sdp ); <nl> failed : <nl> + gfs2_trans_end ( sdp ); <nl> if ( al ) { <nl> gfs2_inplace_release ( ip ); <nl> gfs2_quota_unlock ( ip );
mwifiex_config_scan ( struct mwifiex_private * priv , <nl> wildcard_ssid_tlv -> max_ssid_length = <nl> IEEE80211_MAX_SSID_LEN ; <nl>  <nl> + if (! memcmp ( user_scan_in -> ssid_list [ i ]. ssid , <nl> + " DIRECT -", 7 )) <nl> + wildcard_ssid_tlv -> max_ssid_length = 0xfe ; <nl> + <nl> memcpy ( wildcard_ssid_tlv -> ssid , <nl> user_scan_in -> ssid_list [ i ]. ssid , ssid_len ); <nl> 
void prune_icache_sb ( struct super_block * sb , int nr_to_scan ) <nl> * inode to the back of the list so we don ' t spin on it . <nl> */ <nl> if (! spin_trylock (& inode -> i_lock )) { <nl> - list_move (& inode -> i_lru , & sb -> s_inode_lru ); <nl> + list_move_tail (& inode -> i_lru , & sb -> s_inode_lru ); <nl> continue ; <nl> } <nl> 
void rt2x00rfkill_allocate ( struct rt2x00_dev * rt2x00dev ) <nl>  <nl> void rt2x00rfkill_free ( struct rt2x00_dev * rt2x00dev ) <nl> { <nl> - if (! test_bit ( RFKILL_STATE_ALLOCATED , & rt2x00dev -> flags )) <nl> + if (! test_bit ( RFKILL_STATE_ALLOCATED , & rt2x00dev -> rfkill_state )) <nl> return ; <nl>  <nl> cancel_delayed_work_sync (& rt2x00dev -> rfkill_work );
static int clk_fetch_parent_index ( struct clk_core * core , <nl> { <nl> int i ; <nl>  <nl> + if (! parent ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * find index of new parent clock using cached parent ptrs , <nl> * or if not yet cached , use string name comparison and cache
static int gprs_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> dev -> name , err ); <nl> dev -> stats . tx_aborted_errors ++; <nl> dev -> stats . tx_errors ++; <nl> - dev_kfree_skb ( skb ); <nl> } else { <nl> dev -> stats . tx_packets ++; <nl> dev -> stats . tx_bytes += len ;
static int __init thermal_init ( void ) <nl> idr_destroy (& thermal_cdev_idr ); <nl> mutex_destroy (& thermal_idr_lock ); <nl> mutex_destroy (& thermal_list_lock ); <nl> + return result ; <nl> } <nl> result = genetlink_init (); <nl> return result ;
static int wm8731_set_bias_level ( struct snd_soc_codec * codec , <nl>  <nl> switch ( level ) { <nl> case SND_SOC_BIAS_ON : <nl> - if ( wm8731 -> mclk ) <nl> - clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( wm8731 -> mclk ) { <nl> + ret = clk_prepare_enable ( wm8731 -> mclk ); <nl> + if ( ret ) <nl> + return ret ; <nl> + } <nl> break ; <nl> case SND_SOC_BIAS_PREPARE : <nl> break ;
static int ab8500_usb_probe ( struct platform_device * pdev ) <nl> return err ; <nl> } <nl>  <nl> - /* Phy tuning values for AB8500 */ <nl> - if (! is_ab8500_2p0_or_earlier ( ab -> ab8500 )) { <nl> + /* Phy tuning values for AB8500 > v2 . 0 */ <nl> + if ( is_ab8500 ( ab -> ab8500 ) && ! is_ab8500_2p0_or_earlier ( ab -> ab8500 )) { <nl> /* Enable the PBT / Bank 0x12 access */ <nl> err = abx500_set_register_interruptible ( ab -> dev , <nl> AB8500_DEVELOPMENT , AB8500_BANK12_ACCESS , 0x01 );
static int pwm_omap_dmtimer_probe ( struct platform_device * pdev ) <nl>  <nl> omap = devm_kzalloc (& pdev -> dev , sizeof (* omap ), GFP_KERNEL ); <nl> if (! omap ) { <nl> - omap -> pdata -> free ( dm_timer ); <nl> + pdata -> free ( dm_timer ); <nl> return - ENOMEM ; <nl> } <nl> 
exit_snd_soc : <nl> exit_free_irq : <nl> free_irq ( irq , master ); <nl> exit_fsib : <nl> + pm_runtime_disable (& pdev -> dev ); <nl> fsi_stream_remove (& master -> fsib ); <nl> exit_fsia : <nl> fsi_stream_remove (& master -> fsia ); <nl> exit_iounmap : <nl> iounmap ( master -> base ); <nl> - pm_runtime_disable (& pdev -> dev ); <nl> exit_kfree : <nl> kfree ( master ); <nl> master = NULL ;
static void atombios_crtc_program_ss ( struct drm_crtc * crtc , <nl> case ATOM_PPLL_INVALID : <nl> return ; <nl> } <nl> - args . v2 . ucEnable = enable ; <nl> + args . v3 . ucEnable = enable ; <nl> if (( ss -> percentage == 0 ) || ( ss -> type & ATOM_EXTERNAL_SS_MASK )) <nl> args . v3 . ucEnable = ATOM_DISABLE ; <nl> } else if ( ASIC_IS_DCE4 ( rdev )) {
void of_gpiochip_add ( struct gpio_chip * chip ) <nl> void of_gpiochip_remove ( struct gpio_chip * chip ) <nl> { <nl> gpiochip_remove_pin_ranges ( chip ); <nl> - <nl> - if ( chip -> of_node ) <nl> - of_node_put ( chip -> of_node ); <nl> + of_node_put ( chip -> of_node ); <nl> }
void handle_ra_miss ( struct address_space * mapping , <nl> { <nl> ra -> flags |= RA_FLAG_MISS ; <nl> ra -> flags &= ~ RA_FLAG_INCACHE ; <nl> + ra -> cache_hit = 0 ; <nl> } <nl>  <nl> /*
int rt2800_load_firmware ( struct rt2x00_dev * rt2x00dev , <nl> */ <nl> rt2800_register_write ( rt2x00dev , H2M_BBP_AGENT , 0 ); <nl> rt2800_register_write ( rt2x00dev , H2M_MAILBOX_CSR , 0 ); <nl> + if ( rt2x00_is_usb ( rt2x00dev )) <nl> + rt2800_register_write ( rt2x00dev , H2M_INT_SRC , 0 ); <nl> msleep ( 1 ); <nl>  <nl> return 0 ;
struct phy_device * get_phy_device ( struct mii_bus * bus , int addr ) <nl> if ( r ) <nl> return ERR_PTR ( r ); <nl>  <nl> - /* If the phy_id is all Fs , there is no device there */ <nl> - if ( 0xffffffff == phy_id ) <nl> + /* If the phy_id is all Fs or all 0s , there is no device there */ <nl> + if (( 0xffff == phy_id ) || ( 0x00 == phy_id )) <nl> return NULL ; <nl>  <nl> dev = phy_device_create ( bus , addr , phy_id );
int ip_recv_error ( struct sock * sk , struct msghdr * msg , int len , int * addr_len ) <nl> int err ; <nl> int copied ; <nl>  <nl> + WARN_ON_ONCE ( sk -> sk_family == AF_INET6 ); <nl> + <nl> err = - EAGAIN ; <nl> skb = sock_dequeue_err_skb ( sk ); <nl> if ( skb == NULL )
int snd_pcm_hw_refine ( struct snd_pcm_substream * substream , <nl> snd_mask_max (& params -> masks [ SNDRV_PCM_HW_PARAM_CHANNELS ])) { <nl> changed = substream -> ops -> ioctl ( substream , <nl> SNDRV_PCM_IOCTL1_FIFO_SIZE , params ); <nl> - if ( params < 0 ) <nl> + if ( changed < 0 ) <nl> return changed ; <nl> } <nl> }
get_a_page : <nl> i_size_write ( inode , size ); <nl> inode -> i_mtime = inode -> i_atime = CURRENT_TIME ; <nl> mark_inode_dirty ( inode ); <nl> + set_bit ( QDF_REFRESH , & qd -> qd_flags ); <nl> return 0 ; <nl>  <nl> unlock_out :
int skb_gro_receive ( struct sk_buff ** head , struct sk_buff * skb ) <nl> * NAPI_GRO_CB ( nskb ) = * NAPI_GRO_CB ( p ); <nl> skb_shinfo ( nskb )-> frag_list = p ; <nl> skb_shinfo ( nskb )-> gso_size = pinfo -> gso_size ; <nl> + pinfo -> gso_size = 0 ; <nl> skb_header_release ( p ); <nl> nskb -> prev = p ; <nl> 
static void * raid5_takeover ( mddev_t * mddev ) <nl>  <nl> if ( mddev -> level == 1 ) <nl> return raid5_takeover_raid1 ( mddev ); <nl> + if ( mddev -> level == 4 ) { <nl> + mddev -> new_layout = ALGORITHM_PARITY_N ; <nl> + mddev -> new_level = 5 ; <nl> + return setup_conf ( mddev ); <nl> + } <nl>  <nl> return ERR_PTR (- EINVAL ); <nl> }
drm_property_create_blob ( struct drm_device * dev , size_t length , <nl> struct drm_property_blob * blob ; <nl> int ret ; <nl>  <nl> - if (! length ) <nl> + if (! length || length > ULONG_MAX - sizeof ( struct drm_property_blob )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> blob = kzalloc ( sizeof ( struct drm_property_blob )+ length , GFP_KERNEL );
again : <nl> smp_mb (); <nl> if ( cur_trans -> state >= TRANS_STATE_BLOCKED && <nl> may_wait_transaction ( root , type )) { <nl> + current -> journal_info = h ; <nl> btrfs_commit_transaction ( h , root ); <nl> goto again ; <nl> }
bte_result_t bte_unaligned_copy ( u64 src , u64 dest , u64 len , u64 mode ) <nl> } <nl>  <nl> /* temporary buffer used during unaligned transfers */ <nl> - bteBlock_unaligned = kmalloc ( len + 3 * L1_CACHE_BYTES , <nl> - GFP_KERNEL | GFP_DMA ); <nl> + bteBlock_unaligned = kmalloc ( len + 3 * L1_CACHE_BYTES , GFP_KERNEL ); <nl> if ( bteBlock_unaligned == NULL ) { <nl> return BTEFAIL_NOTAVAIL ; <nl> }
static int ir_lirc_unregister ( struct rc_dev * dev ) <nl>  <nl> lirc_unregister_driver ( lirc -> drv -> minor ); <nl> lirc_buffer_free ( lirc -> drv -> rbuf ); <nl> + kfree ( lirc -> drv -> rbuf ); <nl> kfree ( lirc -> drv ); <nl>  <nl> return 0 ;
static int ath10k_hw_scan ( struct ieee80211_hw * hw , <nl> arg . ssids [ i ]. len = req -> ssids [ i ]. ssid_len ; <nl> arg . ssids [ i ]. ssid = req -> ssids [ i ]. ssid ; <nl> } <nl> + } else { <nl> + arg . scan_ctrl_flags |= WMI_SCAN_FLAG_PASSIVE ; <nl> } <nl>  <nl> if ( req -> n_channels ) {
static int changed_cb ( struct btrfs_root * left_root , <nl> if ( ret < 0 ) <nl> goto out ; <nl>  <nl> + /* Ignore non - FS objects */ <nl> + if ( key -> objectid == BTRFS_FREE_INO_OBJECTID || <nl> + key -> objectid == BTRFS_FREE_SPACE_OBJECTID ) <nl> + goto out ; <nl> + <nl> if ( key -> type == BTRFS_INODE_ITEM_KEY ) <nl> ret = changed_inode ( sctx , result ); <nl> else if ( key -> type == BTRFS_INODE_REF_KEY )
static int sdhci_bcm_kona_probe ( struct platform_device * pdev ) <nl> kona_dev = sdhci_pltfm_priv ( pltfm_priv ); <nl> mutex_init (& kona_dev -> write_lock ); <nl>  <nl> - mmc_of_parse ( host -> mmc ); <nl> + ret = mmc_of_parse ( host -> mmc ); <nl> + if ( ret ) <nl> + goto err_pltfm_free ; <nl>  <nl> if (! host -> mmc -> f_max ) { <nl> dev_err (& pdev -> dev , " Missing max - freq for SDHCI cfg \ n ");
static int vmw_gmr2_bind ( struct vmw_private * dev_priv , <nl> cmd += sizeof ( remap_cmd ) / sizeof ( uint32 ); <nl>  <nl> for ( i = 0 ; i < num_pages ; ++ i ) { <nl> - if ( VMW_PPN_SIZE > 4 ) <nl> + if ( VMW_PPN_SIZE <= 4 ) <nl> * cmd = page_to_pfn (* pages ++); <nl> else <nl> *(( uint64_t *) cmd ) = page_to_pfn (* pages ++);
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> int run_pending = 0 ; <nl>  <nl> switch ( event ) { <nl> + case NETDEV_REGISTER : <nl> + if (! idev ) { <nl> + idev = ipv6_add_dev ( dev ); <nl> + if (! idev ) <nl> + printk ( KERN_WARNING " IPv6 : add_dev failed for % s \ n ", <nl> + dev -> name ); <nl> + } <nl> + break ; <nl> case NETDEV_UP : <nl> case NETDEV_CHANGE : <nl> if ( event == NETDEV_UP ) {
wlc_d11hdrs_mac80211 ( struct wlc_info * wlc , struct ieee80211_hw * hw , <nl>  <nl> /* add Broadcom tx descriptor header */ <nl> txh = ( d11txh_t *) skb_push ( p , D11_TXH_LEN ); <nl> - memset (( char *) txh , 0 , D11_TXH_LEN ); <nl> + memset ( txh , 0 , D11_TXH_LEN ); <nl>  <nl> /* setup frameid */ <nl> if ( tx_info -> flags & IEEE80211_TX_CTL_ASSIGN_SEQ ) {
static void md_update_sb ( mddev_t * mddev , int force_change ) <nl> int sync_req ; <nl> int nospares = 0 ; <nl>  <nl> + if ( mddev -> external ) <nl> + return ; <nl> repeat : <nl> spin_lock_irq (& mddev -> write_lock ); <nl> 
static void batadv_tt_local_event ( struct batadv_priv * bat_priv , <nl> del : <nl> list_del (& entry -> list ); <nl> kfree ( entry ); <nl> + kfree ( tt_change_node ); <nl> event_removed = true ; <nl> goto unlock ; <nl> }
ssize_t usb_store_new_id ( struct usb_dynids * dynids , <nl> if ( fields > 4 ) { <nl> const struct usb_device_id * id = id_table ; <nl>  <nl> + if (! id ) <nl> + return - ENODEV ; <nl> + <nl> for (; id -> match_flags ; id ++) <nl> if ( id -> idVendor == refVendor && id -> idProduct == refProduct ) <nl> break ;
static inline __be16 x25_type_trans ( struct sk_buff * skb , struct net_device * dev ) <nl> { <nl> skb -> mac . raw = skb -> data ; <nl> + skb -> dev = dev ; <nl> skb -> pkt_type = PACKET_HOST ; <nl>  <nl> return htons ( ETH_P_X25 );
ARIZONA_MIXER_CONTROLS (" DSP2R ", ARIZONA_DSP2RMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP3L ", ARIZONA_DSP3LMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP3R ", ARIZONA_DSP3RMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" DSP4L ", ARIZONA_DSP4LMIX_INPUT_1_SOURCE ), <nl> - ARIZONA_MIXER_CONTROLS (" DSP5R ", ARIZONA_DSP4RMIX_INPUT_1_SOURCE ), <nl> + ARIZONA_MIXER_CONTROLS (" DSP4R ", ARIZONA_DSP4RMIX_INPUT_1_SOURCE ), <nl>  <nl> ARIZONA_MIXER_CONTROLS (" Mic ", ARIZONA_MICMIX_INPUT_1_SOURCE ), <nl> ARIZONA_MIXER_CONTROLS (" Noise ", ARIZONA_NOISEMIX_INPUT_1_SOURCE ),
static int __devinit twl4030_madc_probe ( struct platform_device * pdev ) <nl> if (! madc ) <nl> return - ENOMEM ; <nl>  <nl> + madc -> dev = & pdev -> dev ; <nl> + <nl> /* <nl> * Phoenix provides 2 interrupt lines . The first one is connected to <nl> * the OMAP . The other one can be connected to the other processor such
static int virtscsi_queuecommand ( struct Scsi_Host * sh , struct scsi_cmnd * sc ) <nl> sizeof cmd -> req . cmd , sizeof cmd -> resp . cmd , <nl> GFP_ATOMIC ) >= 0 ) <nl> ret = 0 ; <nl> + else <nl> + mempool_free ( cmd , virtscsi_cmd_pool ); <nl>  <nl> out : <nl> return ret ;
static void scatterwalk_pagedone ( struct scatter_walk * walk , int out , <nl> struct page * page ; <nl>  <nl> page = sg_page ( walk -> sg ) + (( walk -> offset - 1 ) >> PAGE_SHIFT ); <nl> - flush_dcache_page ( page ); <nl> + if (! PageSlab ( page )) <nl> + flush_dcache_page ( page ); <nl> } <nl>  <nl> if ( more ) {
static int i2o_cfg_gethrt ( unsigned long arg ) <nl> put_user ( len , kcmd . reslen ); <nl> if ( len > reslen ) <nl> ret = - ENOBUFS ; <nl> - if ( copy_to_user ( kcmd . resbuf , ( void *) hrt , len )) <nl> + else if ( copy_to_user ( kcmd . resbuf , ( void *) hrt , len )) <nl> ret = - EFAULT ; <nl>  <nl> return ret ;
static unsigned int xhci_microframes_to_exponent ( struct usb_device * udev , <nl> static unsigned int xhci_parse_microframe_interval ( struct usb_device * udev , <nl> struct usb_host_endpoint * ep ) <nl> { <nl> + if ( ep -> desc . bInterval == 0 ) <nl> + return 0 ; <nl> return xhci_microframes_to_exponent ( udev , ep , <nl> ep -> desc . bInterval , 0 , 15 ); <nl> }
static int gpio_setup_irq ( struct gpio_desc * desc , struct device * dev , <nl> return 0 ; <nl>  <nl> free_sd : <nl> - sysfs_put ( pdesc -> value_sd ); <nl> + if ( pdesc ) <nl> + sysfs_put ( pdesc -> value_sd ); <nl> free_id : <nl> idr_remove (& pdesc_idr , id ); <nl> desc -> flags &= GPIO_FLAGS_MASK ;
void __init tsc_calibrate ( void ) <nl> if ( hpet ) { <nl> printk ( KERN_INFO " TSC calibrated against HPET \ n "); <nl> if ( hpet2 < hpet1 ) <nl> - hpet2 += 0x100000000 ; <nl> + hpet2 += 0x100000000UL ; <nl> hpet2 -= hpet1 ; <nl> tsc1 = ( hpet2 * hpet_readl ( HPET_PERIOD )) / 1000000 ; <nl> } else {
static ssize_t iio_write_channel_info ( struct device * dev , <nl> if ( buf [ 0 ] == '-') { <nl> negative = true ; <nl> buf ++; <nl> + } else if ( buf [ 0 ] == '+') { <nl> + buf ++; <nl> } <nl>  <nl> while (* buf ) {
i915_gem_set_tiling ( struct drm_device * dev , void * data , <nl> } <nl>  <nl> mutex_lock (& dev -> struct_mutex ); <nl> - if ( i915_gem_obj_is_pinned ( obj ) || obj -> framebuffer_references ) { <nl> + if ( obj -> pin_display || obj -> framebuffer_references ) { <nl> ret = - EBUSY ; <nl> goto err ; <nl> }
struct ipic * __init ipic_init ( struct device_node * node , unsigned int flags ) <nl> ipic -> irqhost = irq_alloc_host ( node , IRQ_HOST_MAP_LINEAR , <nl> NR_IPIC_INTS , <nl> & ipic_host_ops , 0 ); <nl> - if ( ipic -> irqhost == NULL ) <nl> + if ( ipic -> irqhost == NULL ) { <nl> + kfree ( ipic ); <nl> return NULL ; <nl> + } <nl>  <nl> ipic -> regs = ioremap ( res . start , res . end - res . start + 1 ); <nl> 
static int __devinit savagefb_probe ( struct pci_dev * dev , <nl> # if defined ( CONFIG_FB_SAVAGE_I2C ) <nl> savagefb_create_i2c_busses ( info ); <nl> savagefb_probe_i2c_connector ( info , & par -> edid ); <nl> - kfree ( par -> edid ); <nl> fb_edid_to_monspecs ( par -> edid , & info -> monspecs ); <nl> + kfree ( par -> edid ); <nl> fb_videomode_to_modelist ( info -> monspecs . modedb , <nl> info -> monspecs . modedb_len , <nl> & info -> modelist );
int btrfs_sync_file ( struct file * file , struct dentry * dentry , int datasync ) <nl> } <nl> mutex_lock (& dentry -> d_inode -> i_mutex ); <nl> out : <nl> - return ret > 0 ? EIO : ret ; <nl> + return ret > 0 ? - EIO : ret ; <nl> } <nl>  <nl> static const struct vm_operations_struct btrfs_file_vm_ops = {
static int get_dma_channel ( struct device_node * ssi_np , <nl> * dai -> platform name should already point to an allocated buffer . <nl> */ <nl> ret = of_address_to_resource ( dma_channel_np , 0 , & res ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + of_node_put ( dma_channel_np ); <nl> return ret ; <nl> + } <nl> snprintf (( char *) dai -> platform_name , DAI_NAME_SIZE , "% llx .% s ", <nl> ( unsigned long long ) res . start , dma_channel_np -> name ); <nl> 
static void __init h2_init_smc91x ( void ) <nl>  <nl> static struct i2c_board_info __initdata h2_i2c_board_info [] = { <nl> { <nl> + I2C_BOARD_INFO (" tps65010 ", 0x48 ), <nl> + . type = " tps65010 ", <nl> + . irq = OMAP_GPIO_IRQ ( 58 ), <nl> + }, { <nl> I2C_BOARD_INFO (" isp1301_omap ", 0x2d ), <nl> . type = " isp1301_omap ", <nl> . irq = OMAP_GPIO_IRQ ( 2 ),
static struct collection collections [] = { <nl>  <nl> /* Iterate over all benchmarks within a collection : */ <nl> # define for_each_bench ( coll , bench ) \ <nl> - for ( bench = coll -> benchmarks ; bench -> name ; bench ++) <nl> + for ( bench = coll -> benchmarks ; bench && bench -> name ; bench ++) <nl>  <nl> static void dump_benchmarks ( struct collection * coll ) <nl> {
void rtw_alloc_hwxmits ( struct adapter * padapter ) <nl>  <nl> pxmitpriv -> hwxmit_entry = HWXMIT_ENTRY ; <nl>  <nl> - pxmitpriv -> hwxmits = kzalloc ( sizeof ( struct hw_xmit ) * pxmitpriv -> hwxmit_entry , GFP_KERNEL ); <nl> + pxmitpriv -> hwxmits = kcalloc ( pxmitpriv -> hwxmit_entry , <nl> + sizeof ( struct hw_xmit ), GFP_KERNEL ); <nl>  <nl> hwxmits = pxmitpriv -> hwxmits ; <nl> 
vt6656_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl> hw = ieee80211_alloc_hw ( sizeof ( struct vnt_private ), & vnt_mac_ops ); <nl> if (! hw ) { <nl> dev_err (& udev -> dev , " could not register ieee80211_hw \ n "); <nl> + rc = - ENOMEM ; <nl> goto err_nomem ; <nl> } <nl> 
static long media_device_enum_entities ( struct media_device * mdev , <nl> struct media_entity * ent ; <nl> struct media_entity_desc u_ent ; <nl>  <nl> + memset (& u_ent , 0 , sizeof ( u_ent )); <nl> if ( copy_from_user (& u_ent . id , & uent -> id , sizeof ( u_ent . id ))) <nl> return - EFAULT ; <nl> 
static enum i40e_media_type i40e_get_media_type ( struct i40e_hw * hw ) <nl> } <nl>  <nl> # define I40E_PF_RESET_WAIT_COUNT_A0 200 <nl> -# define I40E_PF_RESET_WAIT_COUNT 110 <nl> +# define I40E_PF_RESET_WAIT_COUNT 200 <nl> /** <nl> * i40e_pf_reset - Reset the PF <nl> * @ hw : pointer to the hardware structure
static int ipw_up ( struct ipw_priv * priv ) <nl> if (!( priv -> config & CFG_CUSTOM_MAC )) <nl> eeprom_parse_mac ( priv , priv -> mac_addr ); <nl> memcpy ( priv -> net_dev -> dev_addr , priv -> mac_addr , ETH_ALEN ); <nl> + memcpy ( priv -> net_dev -> perm_addr , priv -> mac_addr , ETH_ALEN ); <nl>  <nl> for ( j = 0 ; j < ARRAY_SIZE ( ipw_geos ); j ++) { <nl> if (! memcmp (& priv -> eeprom [ EEPROM_COUNTRY_CODE ],
int brcmf_fws_hdrpull ( struct brcmf_pub * drvr , int ifidx , s16 signal_len , <nl> if (! signal_len ) <nl> return 0 ; <nl> /* if flow control disabled , skip to packet data and leave */ <nl> - if (! fws -> fw_signals ) { <nl> + if ((! fws ) || (! fws -> fw_signals )) { <nl> skb_pull ( skb , signal_len ); <nl> return 0 ; <nl> }
static void mv_otg_work ( struct work_struct * work ) <nl> struct usb_otg * otg ; <nl> int old_state ; <nl>  <nl> - mvotg = container_of (( struct delayed_work *) work , struct mv_otg , work ); <nl> + mvotg = container_of ( to_delayed_work ( work ), struct mv_otg , work ); <nl>  <nl> run : <nl> /* work queue is single thread , or we need spin_lock to protect */
skip_type : <nl> if ( pmu -> pmu_cpu_context ) <nl> goto got_cpu_context ; <nl>  <nl> + ret = - ENOMEM ; <nl> pmu -> pmu_cpu_context = alloc_percpu ( struct perf_cpu_context ); <nl> if (! pmu -> pmu_cpu_context ) <nl> goto free_dev ;
static struct scsi_host_template aac_driver_template = { <nl>  <nl> static void __aac_shutdown ( struct aac_dev * aac ) <nl> { <nl> - kthread_stop ( aac -> thread ); <nl> + if ( aac -> aif_thread ) <nl> + kthread_stop ( aac -> thread ); <nl> aac_send_shutdown ( aac ); <nl> aac_adapter_disable_int ( aac ); <nl> free_irq ( aac -> pdev -> irq , aac );
static ssize_t ci_port_test_write ( struct file * file , const char __user * ubuf , <nl> if ( sscanf ( buf , "% u ", & mode ) != 1 ) <nl> return - EINVAL ; <nl>  <nl> + if ( mode > 255 ) <nl> + return - EBADRQC ; <nl> + <nl> pm_runtime_get_sync ( ci -> dev ); <nl> spin_lock_irqsave (& ci -> lock , flags ); <nl> ret = hw_port_test_set ( ci , mode );
radeon_user_framebuffer_create ( struct drm_device * dev , <nl> if ( ret ) { <nl> kfree ( radeon_fb ); <nl> drm_gem_object_unreference_unlocked ( obj ); <nl> - return NULL ; <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> return & radeon_fb -> base ;
static int gen6_drpc_info ( struct seq_file * m ) <nl>  <nl> rpmodectl1 = I915_READ ( GEN6_RP_CONTROL ); <nl> rcctl1 = I915_READ ( GEN6_RC_CONTROL ); <nl> - sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> mutex_unlock (& dev -> struct_mutex ); <nl> + mutex_lock (& dev_priv -> rps . hw_lock ); <nl> + sandybridge_pcode_read ( dev_priv , GEN6_PCODE_READ_RC6VIDS , & rc6vids ); <nl> + mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> seq_printf ( m , " Video Turbo Mode : % s \ n ", <nl> yesno ( rpmodectl1 & GEN6_RP_MEDIA_TURBO ));
static struct platform_driver omap_hsmmc_driver = { <nl> static int __init omap_hsmmc_init ( void ) <nl> { <nl> /* Register the MMC driver */ <nl> - return platform_driver_register (& omap_hsmmc_driver ); <nl> + return platform_driver_probe (& omap_hsmmc_driver , omap_hsmmc_probe ); <nl> } <nl>  <nl> static void __exit omap_hsmmc_cleanup ( void )
struct led_classdev { <nl>  <nl> extern int led_classdev_register ( struct device * parent , <nl> struct led_classdev * led_cdev ); <nl> - extern void led_classdev_unregister ( struct led_classdev * lcd ); <nl> + extern void led_classdev_unregister ( struct led_classdev * led_cdev ); <nl> extern void led_classdev_suspend ( struct led_classdev * led_cdev ); <nl> extern void led_classdev_resume ( struct led_classdev * led_cdev ); <nl> 
static int sn_hwperf_op_cpu ( struct sn_hwperf_op_info * op_info ) <nl> else { <nl> /* migrate the task before calling SAL */ <nl> save_allowed = current -> cpus_allowed ; <nl> - set_cpus_allowed ( current , cpumask_of_cpu ( cpu )); <nl> + set_cpus_allowed_ptr ( current , cpumask_of ( cpu )); <nl> sn_hwperf_call_sal ( op_info ); <nl> - set_cpus_allowed ( current , save_allowed ); <nl> + set_cpus_allowed_ptr ( current , & save_allowed ); <nl> } <nl> } <nl> r = op_info -> ret ;
again : <nl> key . offset = split ; <nl>  <nl> ret = btrfs_search_slot ( trans , root , & key , path , - 1 , 1 ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> if ( ret > 0 && path -> slots [ 0 ] > 0 ) <nl> path -> slots [ 0 ]--; <nl> 
static void balloon_up ( struct work_struct * dummy ) <nl> floor = compute_balloon_floor (); <nl>  <nl> /* Refuse to balloon below the floor , keep the 2M granularity . */ <nl> - if ( val . freeram - num_pages < floor ) { <nl> + if ( val . freeram < num_pages || val . freeram - num_pages < floor ) { <nl> num_pages = val . freeram > floor ? ( val . freeram - floor ) : 0 ; <nl> num_pages -= num_pages % PAGES_IN_2M ; <nl> }
intel_pin_and_fence_fb_obj ( struct drm_device * dev , <nl> u32 alignment ; <nl> int ret ; <nl>  <nl> + WARN_ON (! mutex_is_locked (& dev -> struct_mutex )); <nl> + <nl> switch ( obj -> tiling_mode ) { <nl> case I915_TILING_NONE : <nl> if ( IS_BROADWATER ( dev ) || IS_CRESTLINE ( dev )) <nl> err_interruptible : <nl>  <nl> void intel_unpin_fb_obj ( struct drm_i915_gem_object * obj ) <nl> { <nl> + WARN_ON (! mutex_is_locked (& obj -> base . dev -> struct_mutex )); <nl> + <nl> i915_gem_object_unpin_fence ( obj ); <nl> i915_gem_object_unpin_from_display_plane ( obj ); <nl> }
static ssize_t wm8962_beep_set ( struct device * dev , <nl> { <nl> struct wm8962_priv * wm8962 = dev_get_drvdata ( dev ); <nl> long int time ; <nl> + int ret ; <nl>  <nl> - strict_strtol ( buf , 10 , & time ); <nl> + ret = strict_strtol ( buf , 10 , & time ); <nl> + if ( ret != 0 ) <nl> + return ret ; <nl>  <nl> input_event ( wm8962 -> beep , EV_SND , SND_TONE , time ); <nl> 
void afs_cache_permit ( struct afs_vnode * vnode , struct key * key , long acl_order ) <nl> if (! permits ) <nl> goto out_unlock ; <nl>  <nl> - memcpy ( permits -> permits , xpermits -> permits , <nl> - count * sizeof ( struct afs_permit )); <nl> + if ( xpermits ) <nl> + memcpy ( permits -> permits , xpermits -> permits , <nl> + count * sizeof ( struct afs_permit )); <nl>  <nl> _debug (" key % x access % x ", <nl> key_serial ( key ), vnode -> status . caller_access );
static int uevent_net_init ( struct net * net ) <nl> if (! ue_sk -> sk ) { <nl> printk ( KERN_ERR <nl> " kobject_uevent : unable to create netlink socket !\ n "); <nl> + kfree ( ue_sk ); <nl> return - ENODEV ; <nl> } <nl> mutex_lock (& uevent_sock_mutex );
xfs_qm_freelist_destroy ( xfs_frlist_t * ql ) <nl> xfs_qm_dqdestroy ( dqp ); <nl> dqp = nextdqp ; <nl> } <nl> - /* <nl> - * Don ' t bother about unlocking . <nl> - */ <nl> + mutex_unlock (& ql -> qh_lock ); <nl> mutex_destroy (& ql -> qh_lock ); <nl>  <nl> ASSERT ( ql -> qh_nelems == 0 );
int vmw_kms_update_layout_ioctl ( struct drm_device * dev , void * data , <nl> ret = copy_from_user ( rects , user_rects , rects_size ); <nl> if ( unlikely ( ret != 0 )) { <nl> DRM_ERROR (" Failed to get rects .\ n "); <nl> + ret = - EFAULT ; <nl> goto out_free ; <nl> } <nl> 
static int __init crossbar_of_init ( struct device_node * node ) <nl> int i , size , max , reserved = 0 , entry ; <nl> const __be32 * irqsr ; <nl>  <nl> - cb = kzalloc ( sizeof ( struct cb_device *), GFP_KERNEL ); <nl> + cb = kzalloc ( sizeof (* cb ), GFP_KERNEL ); <nl>  <nl> if (! cb ) <nl> return - ENOMEM ;
static void ni_660x_handle_gpct_interrupt ( struct comedi_device * dev , <nl> struct ni_gpct * counter = s -> private ; <nl>  <nl> ni_tio_handle_interrupt ( counter , s ); <nl> - cfc_handle_events ( dev , s ); <nl> + comedi_handle_events ( dev , s ); <nl> } <nl>  <nl> static irqreturn_t ni_660x_interrupt ( int irq , void * d )
# ifndef _LINUX_BFS_FS_H <nl> # define _LINUX_BFS_FS_H <nl>  <nl> +# include < linux / types . h > <nl> + <nl> # define BFS_BSIZE_BITS 9 <nl> # define BFS_BSIZE ( 1 << BFS_BSIZE_BITS ) <nl>  <nl> # define BFS_VDIR 2L <nl> # define BFS_VREG 1L <nl>  <nl> - <nl> /* BFS inode layout on disk */ <nl> struct bfs_inode { <nl> __le16 i_ino ;
static void btrfs_qgroup_rescan_worker ( struct btrfs_work * work ) <nl> out : <nl> kfree ( scratch_leaf ); <nl> ulist_free ( qgroups ); <nl> + ulist_free ( tmp ); <nl> btrfs_free_path ( path ); <nl>  <nl> mutex_lock (& fs_info -> qgroup_rescan_lock );
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> if ( IS_VALLEYVIEW ( dev )) <nl> bios_reserved = 1024 * 1024 ; /* top 1M on VLV / BYT */ <nl>  <nl> + if ( WARN_ON ( bios_reserved > dev_priv -> gtt . stolen_size )) <nl> + return 0 ; <nl> + <nl> /* Basic memrange allocator for stolen space */ <nl> drm_mm_init (& dev_priv -> mm . stolen , 0 , dev_priv -> gtt . stolen_size - <nl> bios_reserved );
static int ide_diag_taskfile ( ide_drive_t * drive , ide_task_t * args , unsigned long <nl> struct request rq ; <nl>  <nl> memset (& rq , 0 , sizeof ( rq )); <nl> + rq . ref_count = 1 ; <nl> rq . cmd_type = REQ_TYPE_ATA_TASKFILE ; <nl> rq . buffer = buf ; <nl> 
enum pcpu_fc pcpu_chosen_fc __initdata = PCPU_FC_AUTO ; <nl>  <nl> static int __init percpu_alloc_setup ( char * str ) <nl> { <nl> + if (! str ) <nl> + return - EINVAL ; <nl> + <nl> if ( 0 ) <nl> /* nada */; <nl> # ifdef CONFIG_NEED_PER_CPU_EMBED_FIRST_CHUNK
static int mga_vram_init ( struct mga_device * mdev ) <nl> aper -> count = 1 ; <nl>  <nl> remove_conflicting_framebuffers ( aper , " mgafb ", true ); <nl> + kfree ( aper ); <nl>  <nl> if (! request_mem_region ( mdev -> mc . vram_base , mdev -> mc . vram_window , <nl> " mgadrmfb_vram ")) {
static int __init byt_gpio_init ( void ) <nl> { <nl> return platform_driver_register (& byt_gpio_driver ); <nl> } <nl> - <nl> subsys_initcall ( byt_gpio_init ); <nl> + <nl> + static void __exit byt_gpio_exit ( void ) <nl> +{ <nl> + platform_driver_unregister (& byt_gpio_driver ); <nl> +} <nl> + module_exit ( byt_gpio_exit );
bool f2fs_may_inline ( struct inode * inode ) <nl> if ( f2fs_is_atomic_file ( inode )) <nl> return false ; <nl>  <nl> - if (! S_ISREG ( inode -> i_mode )) <nl> + if (! S_ISREG ( inode -> i_mode ) && ! S_ISLNK ( inode -> i_mode )) <nl> return false ; <nl>  <nl> if ( i_size_read ( inode ) > MAX_INLINE_DATA )
static int ab8500_fg_get_ext_psy_data ( struct device * dev , void * data ) <nl> case POWER_SUPPLY_PROP_TECHNOLOGY : <nl> switch ( ext -> type ) { <nl> case POWER_SUPPLY_TYPE_BATTERY : <nl> - if (! di -> flags . batt_id_received ) { <nl> + if (! di -> flags . batt_id_received && <nl> + di -> bm -> batt_id != BATTERY_UNKNOWN ) { <nl> const struct abx500_battery_type * b ; <nl>  <nl> b = &( di -> bm -> bat_type [ di -> bm -> batt_id ]);
static int rt5640_probe ( struct snd_soc_codec * codec ) <nl> rt5639_specific_dapm_routes , <nl> ARRAY_SIZE ( rt5639_specific_dapm_routes )); <nl> break ; <nl> + default : <nl> + dev_err ( codec -> dev , <nl> + " The driver is for RT5639 RT5640 or RT5642 only \ n "); <nl> + return - ENODEV ; <nl> } <nl>  <nl> return 0 ;
int i915_gem_init_stolen ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> int bios_reserved = 0 ; <nl>  <nl> + if ( dev_priv -> gtt . stolen_size == 0 ) <nl> + return 0 ; <nl> + <nl> dev_priv -> mm . stolen_base = i915_stolen_to_physical ( dev ); <nl> if ( dev_priv -> mm . stolen_base == 0 ) <nl> return 0 ;
static int __devinit cobalt_raq_led_probe ( struct platform_device * pdev ) <nl> if (! res ) <nl> return - EBUSY ; <nl>  <nl> - led_port = ioremap ( res -> start , res -> end - res -> start + 1 ); <nl> + led_port = ioremap ( res -> start , resource_size ( res )); <nl> if (! led_port ) <nl> return - ENOMEM ; <nl> 
static int amd8111e_rx_poll ( struct napi_struct * napi , int budget ) <nl> int rx_pkt_limit = budget ; <nl> unsigned long flags ; <nl>  <nl> + if ( rx_pkt_limit <= 0 ) <nl> + goto rx_not_empty ; <nl> + <nl> do { <nl> /* process receive packets until we use the quota */ <nl> /* If we own the next entry , it ' s a new packet . Send it up . */
irqreturn_t ath_isr ( int irq , void * dev ) <nl>  <nl> if (!( ah -> caps . hw_caps & ATH9K_HW_CAP_AUTOSLEEP )) <nl> if ( status & ATH9K_INT_TIM_TIMER ) { <nl> + if ( ATH_DBG_WARN_ON_ONCE ( sc -> ps_idle )) <nl> + goto chip_reset ; <nl> /* Clear RxAbort bit so that we can <nl> * receive frames */ <nl> ath9k_setpower ( sc , ATH9K_PM_AWAKE );
static int pcf2123_probe ( struct spi_device * spi ) <nl>  <nl> if (!( rxbuf [ 0 ] & 0x20 )) { <nl> dev_err (& spi -> dev , " chip not found \ n "); <nl> + ret = - ENODEV ; <nl> goto kfree_exit ; <nl> } <nl> 
int pstore_register ( struct pstore_info * psi ) <nl> add_timer (& pstore_timer ); <nl> } <nl>  <nl> + /* <nl> + * Update the module parameter backend , so it is visible <nl> + * through / sys / module / pstore / parameters / backend <nl> + */ <nl> + backend = psi -> name ; <nl> + <nl> pr_info (" Registered % s as persistent store backend \ n ", psi -> name ); <nl>  <nl> return 0 ;
static int dasd_eer_open ( struct inode * inp , struct file * filp ) <nl> unsigned long flags ; <nl>  <nl> eerb = kzalloc ( sizeof ( struct eerbuffer ), GFP_KERNEL ); <nl> + if (! eerb ) <nl> + return - ENOMEM ; <nl> eerb -> buffer_page_count = eer_pages ; <nl> if ( eerb -> buffer_page_count < 1 || <nl> eerb -> buffer_page_count > INT_MAX / PAGE_SIZE ) {
int ocfs2_cluster_connect ( const char * stack_name , <nl>  <nl> strlcpy ( new_conn -> cc_name , group , GROUP_NAME_MAX + 1 ); <nl> new_conn -> cc_namelen = grouplen ; <nl> - strlcpy ( new_conn -> cc_cluster_name , cluster_name , CLUSTER_NAME_MAX + 1 ); <nl> + if ( cluster_name_len ) <nl> + strlcpy ( new_conn -> cc_cluster_name , cluster_name , <nl> + CLUSTER_NAME_MAX + 1 ); <nl> new_conn -> cc_cluster_name_len = cluster_name_len ; <nl> new_conn -> cc_recovery_handler = recovery_handler ; <nl> new_conn -> cc_recovery_data = recovery_data ;
static int mtk_spi_remove ( struct platform_device * pdev ) <nl> pm_runtime_disable (& pdev -> dev ); <nl>  <nl> mtk_spi_reset ( mdata ); <nl> - clk_disable_unprepare ( mdata -> spi_clk ); <nl> spi_master_put ( master ); <nl>  <nl> return 0 ;
void reset_vma_resv_huge_pages ( struct vm_area_struct * vma ) <nl> /* Returns true if the VMA has associated reserve pages */ <nl> static int vma_has_reserves ( struct vm_area_struct * vma ) <nl> { <nl> + if ( vma -> vm_flags & VM_NORESERVE ) <nl> + return 0 ; <nl> if ( vma -> vm_flags & VM_MAYSHARE ) <nl> return 1 ; <nl> if ( is_vma_resv_set ( vma , HPAGE_RESV_OWNER ))
nouveau_bo_move_m2mf ( struct ttm_buffer_object * bo , int evict , bool intr , <nl> bool no_wait_gpu , struct ttm_mem_reg * new_mem ) <nl> { <nl> struct nouveau_drm * drm = nouveau_bdev ( bo -> bdev ); <nl> - struct nouveau_channel * chan = chan = drm -> ttm . chan ; <nl> + struct nouveau_channel * chan = drm -> ttm . chan ; <nl> struct nouveau_bo * nvbo = nouveau_bo ( bo ); <nl> struct ttm_mem_reg * old_mem = & bo -> mem ; <nl> int ret ;
static int twl4030_kp_probe ( struct platform_device * pdev ) <nl> err3 : <nl> /* mask all events - we don ' t care about the result */ <nl> ( void ) twl4030_kpwrite_u8 ( kp , 0xff , KEYP_IMR1 ); <nl> - free_irq ( kp -> irq , NULL ); <nl> + free_irq ( kp -> irq , kp ); <nl> err2 : <nl> input_unregister_device ( input ); <nl> input = NULL ;
static void xlr_make_tx_desc ( struct nlm_fmn_msg * msg , unsigned long addr , <nl> (( u64 ) fr_stn_id << 54 ) | /* Free back id */ <nl> ( u64 ) 0 << 40 | /* Set len to 0 */ <nl> (( u64 ) physkb & 0xffffffff )); /* 32bit address */ <nl> - msg -> msg2 = msg -> msg3 = 0 ; <nl> + msg -> msg2 = 0 ; <nl> + msg -> msg3 = 0 ; <nl> } <nl>  <nl> static void __maybe_unused xlr_wakeup_queue ( unsigned long dev )
void tune_serdes ( struct hfi1_pportdata * ppd ) <nl> ppd -> driver_link_ready = 0 ; <nl> ppd -> offline_disabled_reason = HFI1_ODR_MASK ( OPA_LINKDOWN_REASON_NONE ); <nl>  <nl> - if ( loopback == LOOPBACK_SERDES || loopback == LOOPBACK_LCB || <nl> + /* Skip the tuning for testing ( loopback != none ) and simulations */ <nl> + if ( loopback != LOOPBACK_NONE || <nl> ppd -> dd -> icode == ICODE_FUNCTIONAL_SIMULATOR || <nl> ! dd -> pcfg_cache . cache_valid ) { <nl> ppd -> driver_link_ready = 1 ;
nouveau_pm_profile_set ( struct drm_device * dev , const char * profile ) <nl> return - EPERM ; <nl>  <nl> strncpy ( string , profile , sizeof ( string )); <nl> + string [ sizeof ( string ) - 1 ] = 0 ; <nl> if (( ptr = strchr ( string , '\ n '))) <nl> * ptr = '\ 0 '; <nl> 
match ( const struct sk_buff * skb , <nl> return 0 ; <nl> } <nl>  <nl> + if ( mh -> ip6mh_proto != IPPROTO_NONE ) { <nl> + duprintf (" Dropping invalid MH Payload Proto : % u \ n ", <nl> + mh -> ip6mh_proto ); <nl> + * hotdrop = 1 ; <nl> + return 0 ; <nl> + } <nl> + <nl> return type_match ( mhinfo -> types [ 0 ], mhinfo -> types [ 1 ], mh -> ip6mh_type , <nl> !!( mhinfo -> invflags & IP6T_MH_INV_TYPE )); <nl> }
static void intel_dp_prepare ( struct drm_encoder * encoder ) <nl> uint32_t dp_reg = I915_READ ( intel_dp -> output_reg ); <nl>  <nl> if ( IS_eDP ( intel_dp ) || IS_PCH_eDP ( intel_dp )) { <nl> + ironlake_edp_panel_off ( dev ); <nl> ironlake_edp_backlight_off ( dev ); <nl> ironlake_edp_panel_vdd_on ( dev ); <nl> ironlake_edp_pll_on ( encoder );
int pci_get_new_domain_nr ( void ) <nl> void pci_bus_assign_domain_nr ( struct pci_bus * bus , struct device * parent ) <nl> { <nl> static int use_dt_domains = - 1 ; <nl> - int domain = of_get_pci_domain_nr ( parent -> of_node ); <nl> + int domain = - 1 ; <nl>  <nl> + if ( parent ) <nl> + domain = of_get_pci_domain_nr ( parent -> of_node ); <nl> /* <nl> * Check DT domain and use_dt_domains values . <nl> *
int __pci_read_base ( struct pci_dev * dev , enum pci_bar_type type , <nl> /* Address above 32 - bit boundary ; disable the BAR */ <nl> pci_write_config_dword ( dev , pos , 0 ); <nl> pci_write_config_dword ( dev , pos + 4 , 0 ); <nl> + res -> flags |= IORESOURCE_UNSET ; <nl> region . start = 0 ; <nl> region . end = sz64 ; <nl> bar_disabled = true ;
int xfrm_init_replay ( struct xfrm_state * x ) <nl> replay_esn -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> return - EINVAL ; <nl>  <nl> + if (( x -> props . flags & XFRM_STATE_ESN ) && replay_esn -> replay_window == 0 ) <nl> + return - EINVAL ; <nl> + <nl> if (( x -> props . flags & XFRM_STATE_ESN ) && x -> replay_esn ) <nl> x -> repl = & xfrm_replay_esn ; <nl> else
static bool ixgbe_clean_tx_irq ( struct ixgbe_q_vector * q_vector , <nl> total_packets += tx_buffer -> gso_segs ; <nl>  <nl> /* free the skb */ <nl> - dev_kfree_skb_any ( tx_buffer -> skb ); <nl> + dev_consume_skb_any ( tx_buffer -> skb ); <nl>  <nl> /* unmap skb header data */ <nl> dma_unmap_single ( tx_ring -> dev ,
static enum dma_status d40_tx_status ( struct dma_chan * chan , <nl> } <nl>  <nl> ret = dma_cookie_status ( chan , cookie , txstate ); <nl> - if ( ret != DMA_SUCCESS ) <nl> + if ( ret != DMA_COMPLETE ) <nl> dma_set_residue ( txstate , stedma40_residue ( chan )); <nl>  <nl> if ( d40_is_paused ( d40c ))
lnet_parse_reply ( lnet_ni_t * ni , lnet_msg_t * msg ) <nl> LASSERT ( md -> md_offset == 0 ); <nl>  <nl> rlength = hdr -> payload_length ; <nl> - mlength = min_t ( int , rlength , md -> md_length ); <nl> + mlength = min_t ( uint , rlength , md -> md_length ); <nl>  <nl> if ( mlength < rlength && <nl> ( md -> md_options & LNET_MD_TRUNCATE ) == 0 ) {
static void qeth_l3_set_ip_addr_list ( struct qeth_card * card ) <nl> spin_unlock_irqrestore (& card -> ip_lock , flags ); <nl> rc = qeth_l3_register_addr_entry ( card , todo ); <nl> spin_lock_irqsave (& card -> ip_lock , flags ); <nl> - if (! rc ) <nl> + if (! rc || ( rc == IPA_RC_LAN_OFFLINE )) <nl> list_add_tail (& todo -> entry , & card -> ip_list ); <nl> else <nl> kfree ( todo );
static int video_open ( struct file * file ) <nl>  <nl> if ( NULL == dev ) { <nl> mutex_unlock (& cx25821_devlist_mutex ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> 
static inline u32 ext4_chksum ( struct ext4_sb_info * sbi , u32 crc , <nl> { <nl> struct { <nl> struct shash_desc shash ; <nl> - char ctx [ crypto_shash_descsize ( sbi -> s_chksum_driver )]; <nl> + char ctx [ 4 ]; <nl> } desc ; <nl> int err ; <nl>  <nl> + BUG_ON ( crypto_shash_descsize ( sbi -> s_chksum_driver )!= sizeof ( desc . ctx )); <nl> + <nl> desc . shash . tfm = sbi -> s_chksum_driver ; <nl> desc . shash . flags = 0 ; <nl> *( u32 *) desc . ctx = crc ;
static enum dma_status omap_dma_tx_status ( struct dma_chan * chan , <nl>  <nl> if ( d -> dir == DMA_MEM_TO_DEV ) <nl> pos = omap_dma_get_src_pos ( c ); <nl> - else if ( d -> dir == DMA_DEV_TO_MEM ) <nl> + else if ( d -> dir == DMA_DEV_TO_MEM || d -> dir == DMA_MEM_TO_MEM ) <nl> pos = omap_dma_get_dst_pos ( c ); <nl> else <nl> pos = 0 ;
/* to align the pointer to the ( next ) page boundary */ <nl> # define PAGE_ALIGN ( addr ) ((( addr )+ PAGE_SIZE - 1 )& PAGE_MASK ) <nl>  <nl> -# define __PHYSICAL_MASK (((( phys_addr_t ) 1 ) << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> +# define __PHYSICAL_MASK (( phys_addr_t )( 1ULL << __PHYSICAL_MASK_SHIFT ) - 1 ) <nl> # define __VIRTUAL_MASK (( 1UL << __VIRTUAL_MASK_SHIFT ) - 1 ) <nl>  <nl> # ifndef __ASSEMBLY__
static int pit_ioport_read ( struct kvm_io_device * this , <nl> return - EOPNOTSUPP ; <nl>  <nl> addr &= KVM_PIT_CHANNEL_MASK ; <nl> + if ( addr == 3 ) <nl> + return 0 ; <nl> + <nl> s = & pit_state -> channels [ addr ]; <nl>  <nl> mutex_lock (& pit_state -> lock );
int usb_set_interface ( struct usb_device * dev , int interface , int alternate ) <nl> interface ); <nl> return - EINVAL ; <nl> } <nl> + if ( iface -> unregistering ) <nl> + return - ENODEV ; <nl>  <nl> alt = usb_altnum_to_altsetting ( iface , alternate ); <nl> if (! alt ) {
static const char * get_input_type ( struct hda_gnode * node , unsigned int * pinctl ) <nl> return " Front Aux "; <nl> return " Aux "; <nl> case AC_JACK_MIC_IN : <nl> - if ( node -> pin_caps & <nl> - ( AC_PINCAP_VREF_80 << AC_PINCAP_VREF_SHIFT )) <nl> + if ( pinctl && <nl> + ( node -> pin_caps & <nl> + ( AC_PINCAP_VREF_80 << AC_PINCAP_VREF_SHIFT ))) <nl> * pinctl |= AC_PINCTL_VREF_80 ; <nl> if (( location & 0x0f ) == AC_JACK_LOC_FRONT ) <nl> return " Front Mic ";
mptctl_getiocinfo ( unsigned long arg , unsigned int data_size ) <nl> else <nl> karg -> adapterType = MPT_IOCTL_INTERFACE_SCSI ; <nl>  <nl> - if ( karg -> hdr . port > 1 ) <nl> + if ( karg -> hdr . port > 1 ) { <nl> + kfree ( karg ); <nl> return - EINVAL ; <nl> + } <nl> port = karg -> hdr . port ; <nl>  <nl> karg -> port = port ;
static int biovec_create_pools ( struct bio_set * bs , int pool_entries , int scale ) <nl> struct biovec_slab * bp = bvec_slabs + i ; <nl> mempool_t ** bvp = bs -> bvec_pools + i ; <nl>  <nl> - if ( i >= scale ) <nl> + if ( pool_entries > 1 && i >= scale ) <nl> pool_entries >>= 1 ; <nl>  <nl> * bvp = mempool_create_slab_pool ( pool_entries , bp -> slab );
static int uvc_v4l2_put_xu_mapping ( const struct uvc_xu_control_mapping * kp , <nl> __put_user ( kp -> menu_count , & up -> menu_count )) <nl> return - EFAULT ; <nl>  <nl> - __clear_user ( up -> reserved , sizeof ( up -> reserved )); <nl> + if ( __clear_user ( up -> reserved , sizeof ( up -> reserved ))) <nl> + return - EFAULT ; <nl>  <nl> if ( kp -> menu_count == 0 ) <nl> return 0 ;
static void dbri_debug_read ( struct snd_info_entry * entry , <nl> } <nl> # endif <nl>  <nl> - void __devinit snd_dbri_proc ( struct snd_card * card ) <nl> + static void __devinit snd_dbri_proc ( struct snd_card * card ) <nl> { <nl> struct snd_dbri * dbri = card -> private_data ; <nl> struct snd_info_entry * entry ;
static int ocfs2_truncate_file ( struct inode * inode , <nl> ( unsigned long long ) OCFS2_I ( inode )-> ip_blkno , <nl> ( unsigned long long ) new_i_size ); <nl>  <nl> + unmap_mapping_range ( inode -> i_mapping , new_i_size + PAGE_SIZE - 1 , 0 , 1 ); <nl> truncate_inode_pages ( inode -> i_mapping , new_i_size ); <nl>  <nl> fe = ( struct ocfs2_dinode *) di_bh -> b_data ;
static int isp_register_entities ( struct isp_device * isp ) <nl> goto done ; <nl>  <nl> /* Register external entities */ <nl> - for ( subdevs = pdata -> subdevs ; subdevs -> subdevs ; ++ subdevs ) { <nl> + for ( subdevs = pdata -> subdevs ; subdevs && subdevs -> subdevs ; ++ subdevs ) { <nl> struct v4l2_subdev * sensor ; <nl> struct media_entity * input ; <nl> unsigned int flags ;
retry : <nl> list_add_tail (& cap -> session_caps , & session -> s_caps ); <nl> session -> s_nr_caps ++; <nl> spin_unlock (& session -> s_cap_lock ); <nl> - } <nl> + } else if ( new_cap ) <nl> + ceph_put_cap ( mdsc , new_cap ); <nl>  <nl> if (! ci -> i_snap_realm ) { <nl> /*
fail_put : <nl> ip -> i_gl -> gl_object = NULL ; <nl> gfs2_glock_put ( ip -> i_gl ); <nl> fail : <nl> - iput ( inode ); <nl> + iget_failed ( inode ); <nl> return ERR_PTR ( error ); <nl> } <nl> 
int setup_arg_pages ( struct linux_binprm * bprm , <nl> # else <nl> stack_top = arch_align_stack ( stack_top ); <nl> stack_top = PAGE_ALIGN ( stack_top ); <nl> + <nl> + if ( unlikely ( stack_top < mmap_min_addr ) || <nl> + unlikely ( vma -> vm_end - vma -> vm_start >= stack_top - mmap_min_addr )) <nl> + return - ENOMEM ; <nl> + <nl> stack_shift = vma -> vm_end - stack_top ; <nl>  <nl> bprm -> p -= stack_shift ;
static int efi_status_to_err ( efi_status_t status ) <nl> err = - EACCES ; <nl> break ; <nl> case EFI_NOT_FOUND : <nl> - err = - ENOENT ; <nl> + err = - EIO ; <nl> break ; <nl> default : <nl> err = - EINVAL ;
xfs_bmapi ( <nl> xfs_fsblock_t abno ; /* allocated block number */ <nl> xfs_extlen_t alen ; /* allocated extent length */ <nl> xfs_fileoff_t aoff ; /* allocated file offset */ <nl> - xfs_bmalloca_t bma ; /* args for xfs_bmap_alloc */ <nl> + xfs_bmalloca_t bma = { 0 }; /* args for xfs_bmap_alloc */ <nl> xfs_btree_cur_t * cur ; /* bmap btree cursor */ <nl> xfs_fileoff_t end ; /* end of mapped file region */ <nl> int eof ; /* we ' ve hit the end of extents */
static int pctv452e_read_mac_address ( struct dvb_usb_device * d , u8 mac [ 6 ]) <nl> return 0 ; <nl>  <nl> failed : <nl> - memset ( mac , 0 , 6 ); <nl> + eth_zero_addr ( mac ); <nl>  <nl> return ret ; <nl> }
_xfs_buf_find ( <nl> * have to check that the buffer falls within the filesystem bounds . <nl> */ <nl> eofs = XFS_FSB_TO_BB ( btp -> bt_mount , btp -> bt_mount -> m_sb . sb_dblocks ); <nl> - if ( blkno >= eofs ) { <nl> + if ( blkno < 0 || blkno >= eofs ) { <nl> /* <nl> * XXX ( dgc ): we should really be returning - EFSCORRUPTED here , <nl> * but none of the higher level infrastructure supports
static int tg3_run_loopback ( struct tg3 * tp , int loopback_mode ) <nl> } <nl> mac_mode = ( tp -> mac_mode & ~ MAC_MODE_PORT_MODE_MASK ) | <nl> MAC_MODE_LINK_POLARITY | MAC_MODE_PORT_MODE_GMII ; <nl> - if (( tp -> phy_id & PHY_ID_MASK ) == PHY_ID_BCM5401 ) <nl> + if (( tp -> phy_id & PHY_ID_MASK ) == PHY_ID_BCM5401 ) { <nl> mac_mode &= ~ MAC_MODE_LINK_POLARITY ; <nl> + tg3_writephy ( tp , MII_TG3_EXT_CTRL , <nl> + MII_TG3_EXT_CTRL_LNK3_LED_MODE ); <nl> + } <nl> tw32 ( MAC_MODE , mac_mode ); <nl> } <nl> else
static int mei_irq_thread_write_handler ( struct mei_io_list * cmpl_list , <nl> return 0 ; <nl> } <nl> * slots = mei_count_empty_write_slots ( dev ); <nl> + if (* slots <= 0 ) <nl> + return - EMSGSIZE ; <nl> + <nl> /* complete all waiting for write CB */ <nl> dev_dbg (& dev -> pdev -> dev , " complete all waiting for write cb .\ n "); <nl> 
int bench_sched_messaging ( int argc , const char ** argv , <nl> break ; <nl> } <nl>  <nl> + free ( pth_tab ); <nl> + <nl> return 0 ; <nl> }
static int do_ipv6_getsockopt ( struct sock * sk , int level , int optname , <nl> return - EINVAL ; <nl> if ( copy_from_user (& gsf , optval , GROUP_FILTER_SIZE ( 0 ))) <nl> return - EFAULT ; <nl> + if ( gsf . gf_group . ss_family != AF_INET6 ) <nl> + return - EADDRNOTAVAIL ; <nl> lock_sock ( sk ); <nl> err = ip6_mc_msfget ( sk , & gsf , <nl> ( struct group_filter __user *) optval , optlen );
static struct w1_master * w1_alloc_dev ( u32 id , int slave_count , int slave_ttl , <nl> memcpy (& dev -> dev , device , sizeof ( struct device )); <nl> dev_set_name (& dev -> dev , " w1_bus_master % u ", dev -> id ); <nl> snprintf ( dev -> name , sizeof ( dev -> name ), " w1_bus_master % u ", dev -> id ); <nl> + dev -> dev . init_name = dev -> name ; <nl>  <nl> dev -> driver = driver ; <nl> 
again : <nl> } <nl> spin_unlock (& fs_info -> reada_lock ); <nl>  <nl> + kfree ( multi ); <nl> return re ; <nl>  <nl> error : <nl> error : <nl> kref_put (& zone -> refcnt , reada_zone_release ); <nl> spin_unlock (& fs_info -> reada_lock ); <nl> } <nl> + kfree ( multi ); <nl> kfree ( re ); <nl> if ( looped ) <nl> goto again ;
static int msp430_ir_init ( struct budget_ci * budget_ci ) <nl> dev -> input_phys = budget_ci -> ir . phys ; <nl> dev -> input_id . bustype = BUS_PCI ; <nl> dev -> input_id . version = 1 ; <nl> + dev -> scanmask = 0xff ; <nl> if ( saa -> pci -> subsystem_vendor ) { <nl> dev -> input_id . vendor = saa -> pci -> subsystem_vendor ; <nl> dev -> input_id . product = saa -> pci -> subsystem_device ;
struct tracepoint { <nl> do { \ <nl> void ** it_func ; \ <nl> \ <nl> - rcu_read_lock_sched (); \ <nl> + rcu_read_lock_sched_notrace (); \ <nl> it_func = rcu_dereference (( tp )-> funcs ); \ <nl> if ( it_func ) { \ <nl> do { \ <nl> (( void (*)( proto ))(* it_func ))( args ); \ <nl> } while (*(++ it_func )); \ <nl> } \ <nl> - rcu_read_unlock_sched (); \ <nl> + rcu_read_unlock_sched_notrace (); \ <nl> } while ( 0 ) <nl>  <nl> /*
static int hist_browser__run ( struct hist_browser * browser , const char * help ) <nl>  <nl> hists__browser_title ( browser -> hists , hbt , title , sizeof ( title )); <nl>  <nl> - if ( ui_browser__show (& browser -> b , title , help ) < 0 ) <nl> + if ( ui_browser__show (& browser -> b , title , "% s ", help ) < 0 ) <nl> return - 1 ; <nl>  <nl> while ( 1 ) {
e1000_configure_tx ( struct e1000_adapter * adapter ) <nl> } <nl>  <nl> /* Set the default values for the Tx Inter Packet Gap timer */ <nl> - <nl> - if ( hw -> media_type == e1000_media_type_fiber || <nl> - hw -> media_type == e1000_media_type_internal_serdes ) <nl> + if ( adapter -> hw . mac_type <= e1000_82547_rev_2 && <nl> + ( hw -> media_type == e1000_media_type_fiber || <nl> + hw -> media_type == e1000_media_type_internal_serdes )) <nl> tipg = DEFAULT_82543_TIPG_IPGT_FIBER ; <nl> else <nl> tipg = DEFAULT_82543_TIPG_IPGT_COPPER ;
pvpanic_panic_notify ( struct notifier_block * nb , unsigned long code , <nl>  <nl> static struct notifier_block pvpanic_panic_nb = { <nl> . notifier_call = pvpanic_panic_notify , <nl> + . priority = 1 , /* let this called before broken drm_fb_helper */ <nl> }; <nl>  <nl> 
cfq_should_preempt ( struct cfq_data * cfqd , struct cfq_queue * new_cfqq , <nl> if ( cfq_class_idle ( cfqq )) <nl> return true ; <nl>  <nl> - if ( cfqd -> serving_type == SYNC_NOIDLE_WORKLOAD <nl> - && new_cfqq -> service_tree == cfqq -> service_tree ) <nl> + if ( cfqd -> serving_type == SYNC_NOIDLE_WORKLOAD && <nl> + cfqq_type ( new_cfqq ) == SYNC_NOIDLE_WORKLOAD && <nl> + new_cfqq -> service_tree -> count == 1 ) <nl> return true ; <nl>  <nl> /*
static int i915_dma_cleanup ( struct drm_device * dev ) <nl> if ( dev -> irq_enabled ) <nl> drm_irq_uninstall ( dev ); <nl>  <nl> + mutex_lock (& dev -> struct_mutex ); <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> render_ring ); <nl> if ( HAS_BSD ( dev )) <nl> intel_cleanup_ring_buffer ( dev , & dev_priv -> bsd_ring ); <nl> + mutex_unlock (& dev -> struct_mutex ); <nl>  <nl> /* Clear the HWS virtual address at teardown */ <nl> if ( I915_NEED_GFX_HWS ( dev ))
void __init lpc32xx_serial_init ( void ) <nl>  <nl> /* This needs to be done after all UART clocks are setup */ <nl> __raw_writel ( clkmodes , LPC32XX_UARTCTL_CLKMODE ); <nl> - for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ) - 1 ; i ++) { <nl> + for ( i = 0 ; i < ARRAY_SIZE ( uartinit_data ); i ++) { <nl> /* Force a flush of the RX FIFOs to work around a HW bug */ <nl> puart = serial_std_platform_data [ i ]. mapbase ; <nl> __raw_writel ( 0xC1 , LPC32XX_UART_IIR_FCR ( puart ));
static int at86rf230_hw_init ( struct at86rf230_local * lp ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + /* Force setting slotted operation bit to 0 . Sometimes the atben <nl> + * sets this bit and I don ' t know why . We set this always force <nl> + * to zero while probing . <nl> + */ <nl> + rc = at86rf230_write_subreg ( lp , SR_SLOTTED_OPERATION , 0 ); <nl> + if ( rc ) <nl> + return rc ; <nl> + <nl> return 0 ; <nl> } <nl> 
static void __devinit uvesafb_init_info ( struct fb_info * info , <nl> } <nl>  <nl> info -> flags = FBINFO_FLAG_DEFAULT | <nl> - ( par -> ypan ) ? FBINFO_HWACCEL_YPAN : 0 ; <nl> + ( par -> ypan ? FBINFO_HWACCEL_YPAN : 0 ); <nl>  <nl> if (! par -> ypan ) <nl> info -> fbops -> fb_pan_display = NULL ;
static void * vexpress_sysreg_config_func_get ( struct device * dev , <nl> struct device_node * node ) <nl> { <nl> struct vexpress_sysreg_config_func * config_func ; <nl> - u32 site ; <nl> + u32 site = 0 ; <nl> u32 position = 0 ; <nl> u32 dcc = 0 ; <nl> u32 func_device [ 2 ];
void qlcnic_set_multi ( struct net_device * netdev ) <nl> netdev_for_each_mc_addr ( ha , netdev ) { <nl> cur = kzalloc ( sizeof ( struct qlcnic_mac_list_s ), <nl> GFP_ATOMIC ); <nl> + if ( cur == NULL ) <nl> + break ; <nl> memcpy ( cur -> mac_addr , <nl> ha -> addr , ETH_ALEN ); <nl> list_add_tail (& cur -> list , & adapter -> vf_mc_list );
int __inet_inherit_port ( const struct sock * sk , struct sock * child ) <nl>  <nl> spin_lock (& head -> lock ); <nl> tb = inet_csk ( sk )-> icsk_bind_hash ; <nl> + if ( unlikely (! tb )) { <nl> + spin_unlock (& head -> lock ); <nl> + return - ENOENT ; <nl> + } <nl> if ( tb -> port != port ) { <nl> /* NOTE : using tproxy and redirecting skbs to a proxy <nl> * on a different listener port breaks the assumption
static inline unsigned int elapsed_jiffies_msecs ( unsigned long start ) <nl> if ( end >= start ) <nl> return jiffies_to_msecs ( end - start ); <nl>  <nl> - return jiffies_to_msecs ( end + ( MAX_JIFFY_OFFSET - start ) + 1 ); <nl> + return jiffies_to_msecs ( end + ( ULONG_MAX - start ) + 1 ); <nl> } <nl>  <nl> void
static int tpm_binary_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 5th : delimiter */ <nl> seq_putc ( m , '\ 0 '); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl>  <nl> static int tpm_ascii_bios_measurements_show ( struct seq_file * m , void * v ) <nl> /* 4th : eventname <= max + \' 0 ' delimiter */ <nl> seq_printf ( m , " % s \ n ", eventname ); <nl>  <nl> + kfree ( eventname ); <nl> return 0 ; <nl> } <nl> 
int drm_fb_helper_init ( struct drm_device * dev , <nl> struct drm_crtc * crtc ; <nl> int i ; <nl>  <nl> + if (! max_conn_count ) <nl> + return - EINVAL ; <nl> + <nl> fb_helper -> dev = dev ; <nl>  <nl> INIT_LIST_HEAD (& fb_helper -> kernel_fb_list );
static int siu_dai_prepare ( struct snd_pcm_substream * substream , <nl> ret = siu_dai_spbstart ( port_info ); <nl> if ( ret < 0 ) <nl> goto fail ; <nl> + } else { <nl> + ret = 0 ; <nl> } <nl>  <nl> port_info -> play_cap |= self ;
static const struct x86_cpu_id rapl_ids [] = { <nl> RAPL_CPU ( 0x45 , rapl_defaults_core ),/* Haswell ULT */ <nl> RAPL_CPU ( 0x4C , rapl_defaults_atom ),/* Braswell */ <nl> RAPL_CPU ( 0x4A , rapl_defaults_atom ),/* Tangier */ <nl> + RAPL_CPU ( 0x56 , rapl_defaults_core ),/* Future Xeon */ <nl> RAPL_CPU ( 0x5A , rapl_defaults_atom ),/* Annidale */ <nl> {} <nl> };
static int snd_pcm_oss_open_file ( struct file * file , <nl> for ( idx = 0 ; idx < 2 ; idx ++) { <nl> if ( setup [ idx ]. disable ) <nl> continue ; <nl> + if (! pcm -> streams [ idx ]. substream_count ) <nl> + continue ; /* no matching substream */ <nl> if ( idx == SNDRV_PCM_STREAM_PLAYBACK ) { <nl> if (! ( f_mode & FMODE_WRITE )) <nl> continue ;
static int ff_layout_async_handle_error_v3 ( struct rpc_task * task , <nl> return - NFS4ERR_RESET_TO_PNFS ; <nl> out_retry : <nl> task -> tk_status = 0 ; <nl> - rpc_restart_call ( task ); <nl> + rpc_restart_call_prepare ( task ); <nl> rpc_delay ( task , NFS_JUKEBOX_RETRY_TIME ); <nl> return - EAGAIN ; <nl> }
void ath9k_btcoex_stop_gen_timer ( struct ath_softc * sc ) <nl> { <nl> struct ath_btcoex * btcoex = & sc -> btcoex ; <nl>  <nl> - ath9k_gen_timer_stop ( sc -> sc_ah , btcoex -> no_stomp_timer ); <nl> + if ( btcoex -> hw_timer_enabled ) <nl> + ath9k_gen_timer_stop ( sc -> sc_ah , btcoex -> no_stomp_timer ); <nl> } <nl>  <nl> u16 ath9k_btcoex_aggr_limit ( struct ath_softc * sc , u32 max_4ms_framelen )
static int set_wep_key ( struct airo_info * ai , u16 index , const char * key , <nl> WepKeyRid wkr ; <nl> int rc ; <nl>  <nl> - WARN_ON ( keylen == 0 ); <nl> + if ( WARN_ON ( keylen == 0 )) <nl> + return - 1 ; <nl>  <nl> memset (& wkr , 0 , sizeof ( wkr )); <nl> wkr . len = cpu_to_le16 ( sizeof ( wkr ));
static int wl12xx_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> struct wl12xx * wl = hw -> priv ; <nl> int ret ; <nl>  <nl> + mutex_lock (& wl -> mutex ); <nl> + <nl> ret = wl12xx_acx_rts_threshold ( wl , ( u16 ) value ); <nl>  <nl> if ( ret < 0 ) <nl> wl12xx_warning (" wl12xx_op_set_rts_threshold failed : % d ", ret ); <nl>  <nl> + mutex_unlock (& wl -> mutex ); <nl> + <nl> return ret ; <nl> } <nl> 
drm_do_get_edid ( struct drm_connector * connector , struct i2c_adapter * adapter ) <nl> break ; <nl> } <nl> } <nl> - if ( i == 4 ) <nl> + <nl> + if ( i == 4 && print_bad_edid ) { <nl> dev_warn ( connector -> dev -> dev , <nl> "% s : Ignoring invalid EDID block % d .\ n ", <nl> drm_get_connector_name ( connector ), j ); <nl> + <nl> + connector -> bad_edid_counter ++; <nl> + } <nl> } <nl>  <nl> if ( valid_extensions != block [ 0x7e ]) {
int btrfs_kobj_rm_device ( struct btrfs_fs_info * fs_info , <nl> if (! fs_info -> device_dir_kobj ) <nl> return - EINVAL ; <nl>  <nl> - if ( one_device ) { <nl> + if ( one_device && one_device -> bdev ) { <nl> disk = one_device -> bdev -> bd_part ; <nl> disk_kobj = & part_to_dev ( disk )-> kobj ; <nl> 
struct fimc_fmt * find_format ( struct v4l2_format * f , unsigned int mask ) <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( fimc_formats ); ++ i ) { <nl> fmt = & fimc_formats [ i ]; <nl> - if ( fmt -> fourcc == f -> fmt . pix . pixelformat && <nl> + if ( fmt -> fourcc == f -> fmt . pix_mp . pixelformat && <nl> ( fmt -> flags & mask )) <nl> break ; <nl> }
int tulip_refill_rx ( struct net_device * dev ) <nl>  <nl> mapping = pci_map_single ( tp -> pdev , skb -> data , PKT_BUF_SZ , <nl> PCI_DMA_FROMDEVICE ); <nl> + if ( dma_mapping_error (& tp -> pdev -> dev , mapping )) { <nl> + dev_kfree_skb ( skb ); <nl> + tp -> rx_buffers [ entry ]. skb = NULL ; <nl> + break ; <nl> + } <nl> + <nl> tp -> rx_buffers [ entry ]. mapping = mapping ; <nl>  <nl> tp -> rx_ring [ entry ]. buffer1 = cpu_to_le32 ( mapping );
static u8 parse_subframe ( struct sk_buff * skb , <nl> # else <nl> /* Allocate new skb for releasing to upper layer */ <nl> sub_skb = dev_alloc_skb ( nSubframe_Length + 12 ); <nl> + if (! sub_skb ) <nl> + return 0 ; <nl> skb_reserve ( sub_skb , 12 ); <nl> data_ptr = ( u8 *) skb_put ( sub_skb , nSubframe_Length ); <nl> memcpy ( data_ptr , skb -> data , nSubframe_Length );
void __attribute__ (( weak )) bust_spinlocks ( int yes ) <nl> { <nl> if ( yes ) { <nl> - oops_in_progress = 1 ; <nl> + ++ oops_in_progress ; <nl> } else { <nl> # ifdef CONFIG_VT <nl> unblank_screen (); <nl> # endif <nl> - oops_in_progress = 0 ; <nl> - wake_up_klogd (); <nl> + if (-- oops_in_progress == 0 ) <nl> + wake_up_klogd (); <nl> } <nl> } <nl> 
static int __devinit corgipm_init ( void ) <nl> { <nl> int ret ; <nl>  <nl> + if (! machine_is_corgi () && ! machine_is_shepherd () <nl> + && ! machine_is_husky ()) <nl> + return - ENODEV ; <nl> + <nl> corgipm_device = platform_device_alloc (" sharpsl - pm ", - 1 ); <nl> if (! corgipm_device ) <nl> return - ENOMEM ;
static int old_capi_manufacturer ( unsigned int cmd , void __user * data ) <nl> return - EFAULT ; <nl> } <nl> card = get_capi_ctr_by_nr ( ldef . contr ); <nl> + if (! card ) <nl> + return - EINVAL ; <nl> card = capi_ctr_get ( card ); <nl> if (! card ) <nl> return - ESRCH ;
static void dlm_run_purge_list ( struct dlm_ctxt * dlm , <nl> /* This may drop and reacquire the dlm spinlock if it <nl> * has to do migration . */ <nl> mlog ( 0 , " calling dlm_purge_lockres !\ n "); <nl> + dlm_lockres_get ( lockres ); <nl> if ( dlm_purge_lockres ( dlm , lockres )) <nl> BUG (); <nl> + dlm_lockres_put ( lockres ); <nl> mlog ( 0 , " DONE calling dlm_purge_lockres !\ n "); <nl>  <nl> /* Avoid adding any scheduling latencies */
static int lbs_spi_thread ( void * data ) <nl> up (& card -> spi_thread_terminated ); <nl> do_exit ( 0 ); <nl> } <nl> - } while ( err == EINTR ); <nl> + } while ( err == - EINTR ); <nl>  <nl> /* Read the host interrupt status register to see what we <nl> * can do . */
static inline int rsi_create_kthread ( struct rsi_common * common , <nl> u8 * name ) <nl> { <nl> init_completion (& thread -> completion ); <nl> - thread -> task = kthread_run ( func_ptr , common , name ); <nl> + thread -> task = kthread_run ( func_ptr , common , "% s ", name ); <nl> if ( IS_ERR ( thread -> task )) <nl> return ( int ) PTR_ERR ( thread -> task ); <nl> 
int arizona_set_fll ( struct arizona_fll * fll , int source , <nl> if ( ena ) <nl> pm_runtime_put_autosuspend ( arizona -> dev ); <nl>  <nl> + fll -> fref = Fref ; <nl> + fll -> fout = Fout ; <nl> + <nl> return 0 ; <nl> } <nl> 
static int selinux_setprocattr ( struct task_struct * p , <nl> return error ; <nl>  <nl> /* Obtain a SID for the context , if one was specified . */ <nl> - if ( size && str [ 1 ] && str [ 1 ] != '\ n ') { <nl> + if ( size && str [ 0 ] && str [ 0 ] != '\ n ') { <nl> if ( str [ size - 1 ] == '\ n ') { <nl> str [ size - 1 ] = 0 ; <nl> size --;
static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> spin_unlock_bh (& g_cdev . lock ); <nl> if ( pd == NULL ) <nl> return - ENXIO ; <nl> + if (!( pd -> state & OZ_PD_S_CONNECTED )) <nl> + return - EAGAIN ; <nl> eb = & pd -> elt_buff ; <nl> ei = oz_elt_info_alloc ( eb ); <nl> if ( ei == NULL ) {
create_hw_context ( struct drm_device * dev , <nl> return ERR_PTR (- ENOMEM ); <nl> } <nl>  <nl> + if ( INTEL_INFO ( dev )-> gen >= 7 ) { <nl> + ret = i915_gem_object_set_cache_level ( ctx -> obj , <nl> + I915_CACHE_LLC_MLC ); <nl> + if ( ret ) <nl> + goto err_out ; <nl> + } <nl> + <nl> /* The ring associated with the context object is handled by the normal <nl> * object tracking code . We give an initial ring value simple to pass an <nl> * assertion in the context switch code .
static void radeon_compute_pll_legacy ( struct radeon_pll * pll , <nl> max_fractional_feed_div = pll -> max_frac_feedback_div ; <nl> } <nl>  <nl> - for ( post_div = min_post_div ; post_div <= max_post_div ; ++ post_div ) { <nl> + for ( post_div = max_post_div ; post_div >= min_post_div ; -- post_div ) { <nl> uint32_t ref_div ; <nl>  <nl> if (( pll -> flags & RADEON_PLL_NO_ODD_POST_DIV ) && ( post_div & 1 ))
static int f7188x_gpio_set_single_ended ( struct gpio_chip * chip , <nl> data &= ~ BIT ( offset ); <nl> else <nl> data |= BIT ( offset ); <nl> - superio_outb ( sio -> addr , gpio_data_mode ( bank -> regbase ), data ); <nl> + superio_outb ( sio -> addr , gpio_out_mode ( bank -> regbase ), data ); <nl>  <nl> superio_exit ( sio -> addr ); <nl> return 0 ;
intel_fill_fb_ggtt_view ( struct i915_ggtt_view * view , struct drm_framebuffer * fb , <nl> tile_size = intel_tile_size ( dev_priv ); <nl>  <nl> cpp = drm_format_plane_cpp ( fb -> pixel_format , 0 ); <nl> - tile_width = intel_tile_width ( dev_priv , cpp , fb -> modifier [ 0 ]); <nl> + tile_width = intel_tile_width ( dev_priv , fb -> modifier [ 0 ], cpp ); <nl> tile_height = tile_size / tile_width ; <nl>  <nl> info -> width_pages = DIV_ROUND_UP ( fb -> pitches [ 0 ], tile_width );
_return_of_node_put : <nl> of_node_put ( dev_node ); <nl> _return_dev_set_drvdata : <nl> kfree ( priv -> fixed_link ); <nl> - kfree ( priv ); <nl> dev_set_drvdata ( dev , NULL ); <nl> _return : <nl> return err ;
static int atc_control ( struct dma_chan * chan , enum dma_ctrl_cmd cmd , <nl> list_splice_init (& atchan -> queue , & list ); <nl> list_splice_init (& atchan -> active_list , & list ); <nl>  <nl> - spin_unlock_bh (& atchan -> lock ); <nl> - <nl> /* Flush all pending and queued descriptors */ <nl> list_for_each_entry_safe ( desc , _desc , & list , desc_node ) <nl> atc_chain_complete ( atchan , desc ); <nl>  <nl> + spin_unlock_bh (& atchan -> lock ); <nl> + <nl> return 0 ; <nl> } <nl> 
done : <nl> init_timer (& dpriv -> timer ); <nl> dpriv -> timer . expires = jiffies + 10 * HZ ; <nl> dpriv -> timer . data = ( unsigned long ) dev ; <nl> - dpriv -> timer . function = & dscc4_timer ; <nl> + dpriv -> timer . function = dscc4_timer ; <nl> add_timer (& dpriv -> timer ); <nl> netif_carrier_on ( dev ); <nl> 
static long do_ixj_ioctl ( struct file * file_p , unsigned int cmd , unsigned long ar <nl> case IXJCTL_SET_FILTER : <nl> if ( copy_from_user (& jf , argp , sizeof ( jf ))) <nl> retval = - EFAULT ; <nl> - retval = ixj_init_filter ( j , & jf ); <nl> + else <nl> + retval = ixj_init_filter ( j , & jf ); <nl> break ; <nl> case IXJCTL_SET_FILTER_RAW : <nl> if ( copy_from_user (& jfr , argp , sizeof ( jfr )))
radeon_add_legacy_encoder ( struct drm_device * dev , uint32_t encoder_id , uint32_t <nl>  <nl> switch ( radeon_encoder -> encoder_id ) { <nl> case ENCODER_OBJECT_ID_INTERNAL_LVDS : <nl> + encoder -> possible_crtcs = 0x1 ; <nl> drm_encoder_init ( dev , encoder , & radeon_legacy_lvds_enc_funcs , DRM_MODE_ENCODER_LVDS ); <nl> drm_encoder_helper_add ( encoder , & radeon_legacy_lvds_helper_funcs ); <nl> if ( rdev -> is_atom_bios )
struct greybus_device * greybus_new_module ( struct device * parent , <nl>  <nl> return gdev ; <nl> error : <nl> + put_device (& gdev -> dev ); <nl> greybus_module_release (& gdev -> dev ); <nl> return NULL ; <nl> }
void ieee80211_tx_status ( struct ieee80211_hw * hw , struct sk_buff * skb ) <nl> if (! netif_running ( sdata -> dev )) <nl> continue ; <nl>  <nl> + if (( sdata -> u . mntr_flags & MONITOR_FLAG_COOK_FRAMES ) && <nl> + !( info -> flags & IEEE80211_TX_CTL_INJECTED ) && <nl> + ( type == IEEE80211_FTYPE_DATA )) <nl> + continue ; <nl> + <nl> if ( prev_dev ) { <nl> skb2 = skb_clone ( skb , GFP_ATOMIC ); <nl> if ( skb2 ) {
static int start_afu ( struct cxlflash_cfg * cfg ) <nl>  <nl> init_pcr ( cfg ); <nl>  <nl> + /* After an AFU reset , RRQ entries are stale , clear them */ <nl> + memset (& afu -> rrq_entry , 0 , sizeof ( afu -> rrq_entry )); <nl> + <nl> /* Initialize RRQ pointers */ <nl> afu -> hrrq_start = & afu -> rrq_entry [ 0 ]; <nl> afu -> hrrq_end = & afu -> rrq_entry [ NUM_RRQ_ENTRY - 1 ];
ssize_t ttm_bo_io ( struct ttm_bo_device * bdev , struct file * filp , <nl> return - EFAULT ; <nl>  <nl> driver = bo -> bdev -> driver ; <nl> - if ( unlikely ( driver -> verify_access )) { <nl> + if ( unlikely (! driver -> verify_access )) { <nl> ret = - EPERM ; <nl> goto out_unref ; <nl> }
read_rtc : <nl> } <nl> ds1307 -> nvram -> attr . name = " nvram "; <nl> ds1307 -> nvram -> attr . mode = S_IRUGO | S_IWUSR ; <nl> + sysfs_bin_attr_init ( ds1307 -> nvram ); <nl> ds1307 -> nvram -> read = ds1307_nvram_read , <nl> ds1307 -> nvram -> write = ds1307_nvram_write , <nl> ds1307 -> nvram -> size = chip -> nvram_size ;
static int __init parse_memmap_opt ( char * p ) <nl> char * oldp ; <nl> u64 start_at , mem_size ; <nl>  <nl> + if (! p ) <nl> + return - EINVAL ; <nl> + <nl> if (! strcmp ( p , " exactmap ")) { <nl> # ifdef CONFIG_CRASH_DUMP <nl> /*
static size_t log_output ( int facility , int level , enum log_flags lflags , const c <nl> cont_flush (); <nl> } <nl>  <nl> + /* Skip empty continuation lines that couldn ' t be added - they just flush */ <nl> + if (! text_len && ( lflags & LOG_CONT )) <nl> + return 0 ; <nl> + <nl> /* If it doesn ' t end in a newline , try to buffer the current line */ <nl> if (!( lflags & LOG_NEWLINE )) { <nl> if ( cont_add ( facility , level , lflags , text , text_len ))
void jffs2_rtime_exit ( void ); <nl> int jffs2_zlib_init ( void ); <nl> void jffs2_zlib_exit ( void ); <nl> # endif <nl> +# ifdef CONFIG_JFFS2_LZO <nl> + int jffs2_lzo_init ( void ); <nl> + void jffs2_lzo_exit ( void ); <nl> +# endif <nl>  <nl> # endif /* __JFFS2_COMPR_H__ */
int __kvm_set_memory_region ( struct kvm * kvm , <nl> /* Allocate if a slot is being created */ <nl> # ifndef CONFIG_S390 <nl> if ( npages && ! new . rmap ) { <nl> - new . rmap = vmalloc ( npages * sizeof ( struct page *)); <nl> + new . rmap = vmalloc ( npages * sizeof (* new . rmap )); <nl>  <nl> if (! new . rmap ) <nl> goto out_free ;
void __init pxa_set_mci_info ( struct pxamci_platform_data * info ) <nl> } <nl>  <nl>  <nl> - static struct pxa2xx_udc_mach_info pxa_udc_info ; <nl> + static struct pxa2xx_udc_mach_info pxa_udc_info = { <nl> + . gpio_pullup = - 1 , <nl> + . gpio_vbus = - 1 , <nl> +}; <nl>  <nl> void __init pxa_set_udc_info ( struct pxa2xx_udc_mach_info * info ) <nl> {
static int fn_trie_insert ( struct fib_table * tb , struct fib_config * cfg ) <nl> struct fib_info * fi_drop ; <nl> u8 state ; <nl>  <nl> + if ( fi -> fib_treeref > 1 ) <nl> + goto out ; <nl> + <nl> err = - ENOBUFS ; <nl> new_fa = kmem_cache_alloc ( fn_alias_kmem , GFP_KERNEL ); <nl> if ( new_fa == NULL )
static void valleyview_irq_preinstall ( struct drm_device * dev ) <nl> I915_WRITE ( RING_IMR ( GEN6_BSD_RING_BASE ), 0 ); <nl> I915_WRITE ( RING_IMR ( BLT_RING_BASE ), 0 ); <nl>  <nl> - /* and GT */ <nl> - I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> - I915_WRITE ( GTIIR , I915_READ ( GTIIR )); <nl> - <nl> gen5_gt_irq_reset ( dev ); <nl>  <nl> I915_WRITE ( DPINVGTT , DPINVGTT_STATUS_MASK );
# include " dvb_frontend . h " <nl> # include " au8522_priv . h " <nl>  <nl> + MODULE_LICENSE (" GPL "); <nl> + <nl> static int debug ; <nl>  <nl> # define dprintk ( arg ...)\
static void tenxpress_phy_fini ( struct efx_nic * efx ) <nl> { <nl> int reg ; <nl>  <nl> - if ( efx -> phy_type == PHY_TYPE_SFT9001B ) { <nl> + if ( efx -> phy_type == PHY_TYPE_SFT9001B ) <nl> device_remove_file (& efx -> pci_dev -> dev , <nl> & dev_attr_phy_short_reach ); <nl> - } else { <nl> + <nl> + if ( efx -> phy_type == PHY_TYPE_SFX7101 ) { <nl> /* Power down the LNPGA */ <nl> reg = ( 1 << PMA_PMD_LNPGA_POWERDOWN_LBN ); <nl> mdio_clause45_write ( efx , efx -> mii . phy_id , MDIO_MMD_PMAPMD ,
static bool ath9k_hw_chip_reset ( struct ath_hw * ah , <nl> if ( AR_SREV_9330 ( ah )) <nl> ar9003_hw_internal_regulator_apply ( ah ); <nl> ath9k_hw_init_pll ( ah , chan ); <nl> - ath9k_hw_set_rfmode ( ah , chan ); <nl>  <nl> return true ; <nl> } <nl> int ath9k_hw_reset ( struct ath_hw * ah , struct ath9k_channel * chan , <nl> if ( r ) <nl> return r ; <nl>  <nl> + ath9k_hw_set_rfmode ( ah , chan ); <nl> + <nl> if ( ath9k_hw_mci_is_enabled ( ah )) <nl> ar9003_mci_reset ( ah , false , IS_CHAN_2GHZ ( chan ), save_fullsleep ); <nl> 
__cmd_probe ( int argc , const char ** argv , const char * prefix __maybe_unused ) <nl> OPT_CALLBACK (' x ', " exec ", NULL , " executable | path ", <nl> " target executable name or path ", opt_set_target ), <nl> OPT_BOOLEAN ( 0 , " demangle ", & symbol_conf . demangle , <nl> - " Disable symbol demangling "), <nl> + " Enable symbol demangling "), <nl> OPT_BOOLEAN ( 0 , " demangle - kernel ", & symbol_conf . demangle_kernel , <nl> " Enable kernel symbol demangling "), <nl> OPT_END ()
static void dashtty_timer ( unsigned long ignored ) <nl> if ( channel >= 0 ) <nl> fetch_data ( channel ); <nl>  <nl> - mod_timer_pinned (& poll_timer , jiffies + DA_TTY_POLL ); <nl> + mod_timer (& poll_timer , jiffies + DA_TTY_POLL ); <nl> } <nl>  <nl> static void add_poll_timer ( struct timer_list * poll_timer ) <nl> { <nl> - setup_timer ( poll_timer , dashtty_timer , 0 ); <nl> + setup_pinned_timer ( poll_timer , dashtty_timer , 0 ); <nl> poll_timer -> expires = jiffies + DA_TTY_POLL ; <nl>  <nl> /*
static int chsc_ioctl_info_cu ( void __user * user_cd ) <nl> goto out_free ; <nl> } <nl> scucd_area -> request . length = 0x0010 ; <nl> - scucd_area -> request . code = 0x0028 ; <nl> + scucd_area -> request . code = 0x0026 ; <nl> scucd_area -> m = cd -> m ; <nl> scucd_area -> fmt1 = cd -> fmt ; <nl> scucd_area -> cssid = cd -> cssid ;
static int alloc_spa ( struct cxl_afu * afu ) <nl>  <nl> static void release_spa ( struct cxl_afu * afu ) <nl> { <nl> + cxl_p1n_write ( afu , CXL_PSL_SPAP_An , 0 ); <nl> free_pages (( unsigned long ) afu -> spa , afu -> spa_order ); <nl> } <nl> 
static int ioctl_standard_iw_point ( struct iw_point * iwp , unsigned int cmd , <nl> err = - EFAULT ; <nl> goto out ; <nl> } <nl> + <nl> + if ( cmd == SIOCSIWENCODEEXT ) { <nl> + struct iw_encode_ext * ee = ( void *) extra ; <nl> + <nl> + if ( iwp -> length < sizeof (* ee ) + ee -> key_len ) <nl> + return - EFAULT ; <nl> + } <nl> } <nl>  <nl> err = handler ( dev , info , ( union iwreq_data *) iwp , extra );
struct lgdt3306a_config { <nl> u16 vsb_if_khz ; <nl>  <nl> /* disable i2c repeater - 0 : repeater enabled 1 : repeater disabled */ <nl> - int deny_i2c_rptr : 1 ; <nl> + unsigned int deny_i2c_rptr : 1 ; <nl>  <nl> /* spectral inversion - 0 : disabled 1 : enabled */ <nl> - int spectral_inversion : 1 ; <nl> + unsigned int spectral_inversion : 1 ; <nl>  <nl> enum lgdt3306a_mpeg_mode mpeg_mode ; <nl> enum lgdt3306a_tp_clock_edge tpclk_edge ;
struct vm_area_struct * vb2_get_vma ( struct vm_area_struct * vma ) <nl>  <nl> return vma_copy ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( vb2_get_vma ); <nl>  <nl> /** <nl> * vb2_put_userptr () - release a userspace virtual memory area
static int egalax_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> struct egalax_data * td ; <nl> struct hid_report * report ; <nl>  <nl> - td = kmalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> + td = kzalloc ( sizeof ( struct egalax_data ), GFP_KERNEL ); <nl> if (! td ) { <nl> dev_err (& hdev -> dev , " cannot allocate eGalax data \ n "); <nl> return - ENOMEM ;
x86_emulate_insn ( struct x86_emulate_ctxt * ctxt ) <nl> goto done ; <nl> } <nl>  <nl> + if (( c -> d & SrcMask ) == SrcMemFAddr && c -> src . type != OP_MEM ) { <nl> + emulate_ud ( ctxt ); <nl> + goto done ; <nl> + } <nl> + <nl> /* Privileged instruction can be executed only in CPL = 0 */ <nl> if (( c -> d & Priv ) && ops -> cpl ( ctxt -> vcpu )) { <nl> emulate_gp ( ctxt , 0 );
static int newseg ( struct ipc_namespace * ns , struct ipc_params * params ) <nl> if ( size < SHMMIN || size > ns -> shm_ctlmax ) <nl> return - EINVAL ; <nl>  <nl> + if ( numpages << PAGE_SHIFT < size ) <nl> + return - ENOSPC ; <nl> + <nl> if ( ns -> shm_tot + numpages < ns -> shm_tot || <nl> ns -> shm_tot + numpages > ns -> shm_ctlall ) <nl> return - ENOSPC ;
static int scsi_check_sense ( struct scsi_cmnd * scmd ) <nl> return SUCCESS ; <nl>  <nl> case MEDIUM_ERROR : <nl> + if ( sshdr . asc == 0x11 || /* UNRECOVERED READ ERR */ <nl> + sshdr . asc == 0x13 || /* AMNF DATA FIELD */ <nl> + sshdr . asc == 0x14 ) { /* RECORD NOT FOUND */ <nl> + return SUCCESS ; <nl> + } <nl> return NEEDS_RETRY ; <nl>  <nl> case HARDWARE_ERROR :
int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl> + IEEE80211_ENCRYPT_HEADROOM ; <nl> ndev -> needed_tailroom = IEEE80211_ENCRYPT_TAILROOM ; <nl>  <nl> + ret = dev_alloc_name ( ndev , ndev -> name ); <nl> + if ( ret < 0 ) <nl> + goto fail ; <nl> + <nl> ieee80211_assign_perm_addr ( local , ndev , type ); <nl> memcpy ( ndev -> dev_addr , ndev -> perm_addr , ETH_ALEN ); <nl> SET_NETDEV_DEV ( ndev , wiphy_dev ( local -> hw . wiphy ));
static int irda_bind ( struct socket * sock , struct sockaddr * uaddr , int addr_len ) <nl>  <nl> err = irda_open_tsap ( self , addr -> sir_lsap_sel , addr -> sir_name ); <nl> if ( err < 0 ) { <nl> - kfree ( self -> ias_obj -> name ); <nl> - kfree ( self -> ias_obj ); <nl> + irias_delete_object ( self -> ias_obj ); <nl> + self -> ias_obj = NULL ; <nl> goto out ; <nl> } <nl> 
static inline struct page * pte_alloc_one ( struct mm_struct * mm , <nl> struct page * pte ; <nl>  <nl> pte = alloc_pages ( GFP_KERNEL | __GFP_REPEAT , PTE_ORDER ); <nl> - if ( pte ) { <nl> - clear_highpage ( pte ); <nl> - pgtable_page_ctor ( pte ); <nl> + if (! pte ) <nl> + return NULL ; <nl> + clear_highpage ( pte ); <nl> + if (! pgtable_page_ctor ( pte )) { <nl> + __free_page ( pte ); <nl> + return NULL ; <nl> } <nl> return pte ; <nl> }
static int wl1271_init_ieee80211 ( struct wl1271 * wl ) <nl> wl -> hw -> wiphy -> max_sched_scan_ie_len = WL1271_CMD_TEMPL_MAX_SIZE - <nl> sizeof ( struct ieee80211_header ); <nl>  <nl> - wl -> hw -> wiphy -> max_remain_on_channel_duration = 5000 ; <nl> + wl -> hw -> wiphy -> max_remain_on_channel_duration = 30000 ; <nl>  <nl> wl -> hw -> wiphy -> flags |= WIPHY_FLAG_AP_UAPSD | <nl> WIPHY_FLAG_HAS_REMAIN_ON_CHANNEL |
static inline int free_area ( unsigned long pfn , unsigned long end , char * s ) <nl> static inline void poison_init_mem ( void * s , size_t count ) <nl> { <nl> u32 * p = ( u32 *) s ; <nl> - while (( count = count - 4 )) <nl> + for (; count != 0 ; count -= 4 ) <nl> * p ++ = 0xe7fddef0 ; <nl> } <nl> 
static int clcdfb_of_get_mode ( struct device * dev , struct device_node * endpoint , <nl>  <nl> len = clcdfb_snprintf_mode ( NULL , 0 , mode ); <nl> name = devm_kzalloc ( dev , len + 1 , GFP_KERNEL ); <nl> + if (! name ) <nl> + return - ENOMEM ; <nl> + <nl> clcdfb_snprintf_mode ( name , len + 1 , mode ); <nl> mode -> name = name ; <nl> 
int cmd_top ( int argc , const char ** argv , const char * prefix ) <nl> event_id [ 0 ] = 0 ; <nl> } <nl>  <nl> + if ( delay_secs < 1 ) <nl> + delay_secs = 1 ; <nl> + <nl> for ( counter = 0 ; counter < nr_counters ; counter ++) { <nl> if ( event_count [ counter ]) <nl> continue ;
EXPORT_SYMBOL ( genphy_read_status ); <nl>  <nl> static int genphy_config_init ( struct phy_device * phydev ) <nl> { <nl> - u32 val ; <nl> + int val ; <nl> u32 features ; <nl>  <nl> /* For now , I ' ll claim that the generic driver supports
cfg80211_bss_update ( struct cfg80211_registered_device * dev , <nl> memcpy ( ies , res -> pub . information_elements , ielen ); <nl> found -> ies_allocated = true ; <nl> found -> pub . information_elements = ies ; <nl> + found -> pub . len_information_elements = ielen ; <nl> } <nl> } <nl> }
static void copy_from_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy <nl>  <nl> static void copy_to_user_policy ( struct xfrm_policy * xp , struct xfrm_userpolicy_info * p , int dir ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> sel , & xp -> selector , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & xp -> lft , sizeof ( p -> lft )); <nl> memcpy (& p -> curlft , & xp -> curlft , sizeof ( p -> curlft ));
static void iwl3945_init_hw_rates ( struct iwl_priv * priv , <nl> { <nl> int i ; <nl>  <nl> - for ( i = 0 ; i < IWL_RATE_COUNT ; i ++) { <nl> + for ( i = 0 ; i < IWL_RATE_COUNT_LEGACY ; i ++) { <nl> rates [ i ]. bitrate = iwl3945_rates [ i ]. ieee * 5 ; <nl> rates [ i ]. hw_value = i ; /* Rate scaling will work on indexes */ <nl> rates [ i ]. hw_value_short = i ;
static void __wa_populate_buf_in_urb_isoc ( struct wahc * wa , struct wa_xfer * xfer , <nl> wa -> buf_in_urb -> transfer_dma = xfer -> urb -> transfer_dma + <nl> xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. offset ; <nl> wa -> buf_in_urb -> transfer_buffer_length = <nl> - xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. length ; <nl> + xfer -> urb -> iso_frame_desc [ curr_iso_frame ]. actual_length ; <nl> wa -> buf_in_urb -> transfer_flags |= URB_NO_TRANSFER_DMA_MAP ; <nl> wa -> buf_in_urb -> transfer_buffer = NULL ; <nl> wa -> buf_in_urb -> sg = NULL ;
int kvm_vcpu_ioctl_config_tlb ( struct kvm_vcpu * vcpu , <nl>  <nl> num_pages = DIV_ROUND_UP ( cfg -> array + array_len - 1 , PAGE_SIZE ) - <nl> cfg -> array / PAGE_SIZE ; <nl> - pages = kmalloc ( sizeof ( struct page *) * num_pages , GFP_KERNEL ); <nl> + pages = kmalloc_array ( num_pages , sizeof (* pages ), GFP_KERNEL ); <nl> if (! pages ) <nl> return - ENOMEM ; <nl> 
int radeon_atom_get_mclk_range_table ( struct radeon_device * rdev , <nl> p = ( u8 *) vram_module -> asMemTiming ; <nl> for ( i = 0 ; i < mclk_range_table -> num_entries ; i ++) { <nl> format = ( ATOM_MEMORY_TIMING_FORMAT *) p ; <nl> - mclk_range_table -> mclk [ i ] = format -> ulClkRange ; <nl> + mclk_range_table -> mclk [ i ] = le32_to_cpu ( format -> ulClkRange ); <nl> p += mem_timing_size ; <nl> } <nl> } else
static void lm8333_key_handler ( struct lm8333 * lm8333 ) <nl> return ; <nl> } <nl>  <nl> - for ( i = 0 ; keys [ i ] && i < LM8333_FIFO_TRANSFER_SIZE ; i ++) { <nl> + for ( i = 0 ; i < LM8333_FIFO_TRANSFER_SIZE && keys [ i ]; i ++) { <nl> pressed = keys [ i ] & 0x80 ; <nl> code = keys [ i ] & 0x7f ; <nl> 
static inline int unshare_utsname ( unsigned long unshare_flags , <nl>  <nl> static inline int copy_utsname ( int flags , struct task_struct * tsk ) <nl> { <nl> + if ( flags & CLONE_NEWUTS ) <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> static inline void put_uts_ns ( struct uts_namespace * ns )
struct kvm_pit * kvm_create_pit ( struct kvm * kvm , u32 flags ) <nl> pit -> wq = create_singlethread_workqueue (" kvm - pit - wq "); <nl> if (! pit -> wq ) { <nl> mutex_unlock (& pit -> pit_state . lock ); <nl> + kvm_free_irq_source_id ( kvm , pit -> irq_source_id ); <nl> kfree ( pit ); <nl> return NULL ; <nl> }
static int v9fs_vfs_readlink ( struct dentry * dentry , char __user * buffer , <nl> int ret ; <nl> char * link = __getname (); <nl>  <nl> - if ( strlen ( link ) < buflen ) <nl> - buflen = strlen ( link ); <nl> + if ( buflen > PATH_MAX ) <nl> + buflen = PATH_MAX ; <nl>  <nl> dprintk ( DEBUG_VFS , " dentry : % s (% p )\ n ", dentry -> d_iname , dentry ); <nl> 
static int btrfs_set_acl ( struct btrfs_trans_handle * trans , <nl> ret = posix_acl_equiv_mode ( acl , & inode -> i_mode ); <nl> if ( ret < 0 ) <nl> return ret ; <nl> + if ( ret == 0 ) <nl> + acl = NULL ; <nl> } <nl> ret = 0 ; <nl> break ;
static int p54_tx ( struct ieee80211_hw * dev , struct sk_buff * skb ) <nl> struct p54_common * priv = dev -> priv ; <nl> struct p54_hdr * hdr ; <nl> struct p54_tx_data * txhdr ; <nl> - size_t padding , len , tim_len ; <nl> + size_t padding , len , tim_len = 0 ; <nl> int i , j , ridx ; <nl> u16 hdr_flags = 0 , aid = 0 ; <nl> u8 rate , queue ;
static int corgi_bl_update_status ( struct backlight_device * bd ) <nl>  <nl> if ( corgibl_flags & CORGIBL_SUSPENDED ) <nl> intensity = 0 ; <nl> - if ( corgibl_flags & CORGIBL_BATTLOW ) <nl> - intensity &= lcd -> limit_mask ; <nl> + <nl> + if (( corgibl_flags & CORGIBL_BATTLOW ) && intensity > lcd -> limit_mask ) <nl> + intensity = lcd -> limit_mask ; <nl>  <nl> return corgi_bl_set_intensity ( lcd , intensity ); <nl> }
TRACE_EVENT ( iwlwifi_dev_tx , <nl> __entry -> framelen = buf0_len + buf1_len ; <nl> memcpy ( __get_dynamic_array ( tfd ), tfd , tfdlen ); <nl> memcpy ( __get_dynamic_array ( buf0 ), buf0 , buf0_len ); <nl> - memcpy ( __get_dynamic_array ( buf1 ), buf1 , buf0_len ); <nl> + memcpy ( __get_dynamic_array ( buf1 ), buf1 , buf1_len ); <nl> ), <nl> TP_printk ("[% p ] TX %. 2x (% zu bytes )", <nl> __entry -> priv ,
static void eeh_handle_special_event ( void ) <nl>  <nl> /* Notify all devices to be down */ <nl> eeh_pe_state_clear ( pe , EEH_PE_PRI_BUS ); <nl> + eeh_pe_dev_traverse ( pe , <nl> + eeh_report_failure , NULL ); <nl> bus = eeh_pe_bus_get ( phb_pe ); <nl> if (! bus ) { <nl> pr_err ("% s : Cannot find PCI bus for " <nl> static void eeh_handle_special_event ( void ) <nl> pe -> addr ); <nl> break ; <nl> } <nl> - eeh_pe_dev_traverse ( pe , <nl> - eeh_report_failure , NULL ); <nl> pci_hp_remove_devices ( bus ); <nl> } <nl> pci_unlock_rescan_remove ();
static void b43_sdio_remove ( struct sdio_func * func ) <nl> struct b43_sdio * sdio = sdio_get_drvdata ( func ); <nl>  <nl> ssb_bus_unregister (& sdio -> ssb ); <nl> + sdio_claim_host ( func ); <nl> sdio_disable_func ( func ); <nl> + sdio_release_host ( func ); <nl> kfree ( sdio ); <nl> sdio_set_drvdata ( func , NULL ); <nl> }
static struct zonelist * zonelist_policy ( unsigned int __nocast gfp , struct mempol <nl> case MPOL_BIND : <nl> /* Lower zones don ' t get a policy applied */ <nl> /* Careful : current -> mems_allowed might have moved */ <nl> - if ( gfp >= policy_zone ) <nl> + if (( gfp & GFP_ZONEMASK ) >= policy_zone ) <nl> if ( cpuset_zonelist_valid_mems_allowed ( policy -> v . zonelist )) <nl> return policy -> v . zonelist ; <nl> /* FALL THROUGH */
void __cfg80211_disconnected ( struct net_device * dev , const u8 * ie , <nl> memset (& wrqu , 0 , sizeof ( wrqu )); <nl> wrqu . ap_addr . sa_family = ARPHRD_ETHER ; <nl> wireless_send_event ( dev , SIOCGIWAP , & wrqu , NULL ); <nl> + wdev -> wext . connect . ssid_len = 0 ; <nl> # endif <nl> } <nl> 
static int stb0899_send_diseqc_msg ( struct dvb_frontend * fe , struct dvb_diseqc_ma <nl> struct stb0899_state * state = fe -> demodulator_priv ; <nl> u8 reg , i ; <nl>  <nl> - if ( cmd -> msg_len > 8 ) <nl> + if ( cmd -> msg_len > sizeof ( cmd -> msg )) <nl> return - EINVAL ; <nl>  <nl> /* enable FIFO precharge */
static int als_wait_for_data_ready ( struct device * dev ) <nl> ret = i2c_smbus_read_byte_data ( client , 0x86 ); <nl> } while (!( ret & 0x80 ) && retry --); <nl>  <nl> - if (! retry ) { <nl> + if ( retry < 0 ) { <nl> dev_warn ( dev , " timeout waiting for data ready \ n "); <nl> return - ETIMEDOUT ; <nl> }
static int ahash_prepare_alg ( struct ahash_alg * alg ) <nl> struct crypto_alg * base = & alg -> halg . base ; <nl>  <nl> if ( alg -> halg . digestsize > PAGE_SIZE / 8 || <nl> - alg -> halg . statesize > PAGE_SIZE / 8 ) <nl> + alg -> halg . statesize > PAGE_SIZE / 8 || <nl> + alg -> halg . statesize == 0 ) <nl> return - EINVAL ; <nl>  <nl> base -> cra_type = & crypto_ahash_type ;
int ext4_punch_hole ( struct inode * inode , loff_t offset , loff_t length ) <nl> up_write (& EXT4_I ( inode )-> i_data_sem ); <nl> if ( IS_SYNC ( inode )) <nl> ext4_handle_sync ( handle ); <nl> + <nl> + /* Now release the pages again to reduce race window */ <nl> + if ( last_block_offset > first_block_offset ) <nl> + truncate_pagecache_range ( inode , first_block_offset , <nl> + last_block_offset ); <nl> + <nl> inode -> i_mtime = inode -> i_ctime = ext4_current_time ( inode ); <nl> ext4_mark_inode_dirty ( handle , inode ); <nl> out_stop :
static int __devinit i2o_pci_probe ( struct pci_dev * pdev , <nl> i2o_pci_free ( c ); <nl>  <nl> free_controller : <nl> - i2o_iop_free ( c ); <nl> put_device ( c -> device . parent ); <nl> + i2o_iop_free ( c ); <nl>  <nl> disable : <nl> pci_disable_device ( pdev );
static int sbp2scsi_slave_configure ( struct scsi_device * sdev ) <nl> blk_queue_dma_alignment ( sdev -> request_queue , ( 512 - 1 )); <nl> sdev -> use_10_for_rw = 1 ; <nl>  <nl> + if ( sdev -> type == TYPE_ROM ) <nl> + sdev -> use_10_for_ms = 1 ; <nl> if ( sdev -> type == TYPE_DISK && <nl> lu -> workarounds & SBP2_WORKAROUND_MODE_SENSE_8 ) <nl> sdev -> skip_ms_page_8 = 1 ;
long sys_sigreturn ( struct pt_regs regs ) <nl> unsigned long __user * extramask = frame -> extramask ; <nl> int sig_size = ( _NSIG_WORDS - 1 ) * sizeof ( unsigned long ); <nl>  <nl> - if ( copy_from_user (& set . sig [ 0 ], oldmask , sizeof (& set . sig [ 0 ])) || <nl> + if ( copy_from_user (& set . sig [ 0 ], oldmask , sizeof ( set . sig [ 0 ])) || <nl> copy_from_user (& set . sig [ 1 ], extramask , sig_size )) <nl> goto segfault ; <nl> 
static void radeon_cs_parser_fini ( struct radeon_cs_parser * parser , int error ) <nl> { <nl> unsigned i ; <nl>  <nl> - if ( error ) { <nl> + if ( error && parser -> ib ) { <nl> radeon_bo_list_unvalidate (& parser -> validated , <nl> parser -> ib -> fence ); <nl> } else {
static struct i2c_algorithm osif_algorithm = { <nl> # define USB_OSIF_VENDOR_ID 0x1964 <nl> # define USB_OSIF_PRODUCT_ID 0x0001 <nl>  <nl> - static struct usb_device_id osif_table [] = { <nl> + static const struct usb_device_id osif_table [] = { <nl> { USB_DEVICE ( USB_OSIF_VENDOR_ID , USB_OSIF_PRODUCT_ID ) }, <nl> { } <nl> };
void BSP_gettod ( int * yearp , int * monp , int * dayp , <nl> { <nl> } <nl>  <nl> - int BSP_hwclk ( int op , struct hwclk_time * t ) <nl> + int BSP_hwclk ( int op , struct rtc_time * t ) <nl> { <nl> if (! op ) { <nl> /* read */
static void zfcp_fc_adisc_handler ( unsigned long data ) <nl> struct zfcp_port * port = adisc -> els . port ; <nl> struct zfcp_ls_adisc_acc * ls_adisc = & adisc -> ls_adisc_acc ; <nl>  <nl> - if (! adisc -> els . status ) { <nl> + if ( adisc -> els . status ) { <nl> /* request rejected or timed out */ <nl> zfcp_erp_port_forced_reopen ( port , 0 , 63 , NULL ); <nl> goto out ;
bool intel_enable_ppgtt ( struct drm_device * dev , bool full ) <nl>  <nl> /* Full ppgtt disabled by default for now due to issues . */ <nl> if ( full ) <nl> - return false ; /* HAS_PPGTT ( dev ) */ <nl> + return HAS_PPGTT ( dev ) && ( i915 . enable_ppgtt == 2 ); <nl> else <nl> return HAS_ALIASING_PPGTT ( dev ); <nl> }
static int __init bm2835_mmal_init ( void ) <nl> num_cameras = MAX_BCM2835_CAMERAS ; <nl>  <nl> for ( camera = 0 ; camera < num_cameras ; camera ++) { <nl> - dev = kzalloc ( sizeof ( struct bm2835_mmal_dev ), GFP_KERNEL ); <nl> + dev = kzalloc ( sizeof (* dev ), GFP_KERNEL ); <nl> if (! dev ) { <nl> ret = - ENOMEM ; <nl> goto cleanup_gdev ;
ccw_device_offline ( struct ccw_device * cdev ) <nl> ccw_device_done ( cdev , DEV_STATE_NOT_OPER ); <nl> return 0 ; <nl> } <nl> + if ( cdev -> private -> state == DEV_STATE_BOXED ) { <nl> + ccw_device_done ( cdev , DEV_STATE_BOXED ); <nl> + return 0 ; <nl> + } <nl> if ( ccw_device_is_orphan ( cdev )) { <nl> ccw_device_done ( cdev , DEV_STATE_OFFLINE ); <nl> return 0 ;
static struct regmap_config hmc5843_i2c_regmap_config = { <nl> . cache_type = REGCACHE_RBTREE , <nl> }; <nl>  <nl> - static int hmc5843_i2c_probe ( struct i2c_client * client , <nl> + static int hmc5843_i2c_probe ( struct i2c_client * cli , <nl> const struct i2c_device_id * id ) <nl> { <nl> - return hmc5843_common_probe (& client -> dev , <nl> - devm_regmap_init_i2c ( client , & hmc5843_i2c_regmap_config ), <nl> + return hmc5843_common_probe (& cli -> dev , <nl> + devm_regmap_init_i2c ( cli , & hmc5843_i2c_regmap_config ), <nl> id -> driver_data ); <nl> } <nl> 
static int sendconfirmsleep ( struct lbs_private * priv , u8 * cmdptr , u16 size ) <nl> lbs_deb_hex ( LBS_DEB_HOST , " sleep confirm command ", cmdptr , size ); <nl>  <nl> ret = priv -> hw_host_to_card ( priv , MVMS_CMD , cmdptr , size ); <nl> - priv -> dnld_sent = DNLD_RES_RECEIVED ; <nl>  <nl> spin_lock_irqsave (& priv -> driver_lock , flags ); <nl> if ( priv -> intcounter || priv -> currenttxskb )
EXPORT_SYMBOL_GPL ( tcp_slow_start ); <nl> */ <nl> void tcp_cong_avoid_ai ( struct tcp_sock * tp , u32 w , u32 acked ) <nl> { <nl> + /* If credits accumulated at a higher w , apply them gently now . */ <nl> + if ( tp -> snd_cwnd_cnt >= w ) { <nl> + tp -> snd_cwnd_cnt = 0 ; <nl> + tp -> snd_cwnd ++; <nl> + } <nl> + <nl> tp -> snd_cwnd_cnt += acked ; <nl> if ( tp -> snd_cwnd_cnt >= w ) { <nl> u32 delta = tp -> snd_cwnd_cnt / w ;
struct hisi_clock_data * hisi_clk_alloc ( struct platform_device * pdev , <nl> if (! clk_data -> base ) <nl> return NULL ; <nl>  <nl> - clk_table = devm_kmalloc (& pdev -> dev , sizeof ( struct clk *) * nr_clks , <nl> - GFP_KERNEL ); <nl> + clk_table = devm_kmalloc_array (& pdev -> dev , nr_clks , <nl> + sizeof (* clk_table ), <nl> + GFP_KERNEL ); <nl> if (! clk_table ) <nl> return NULL ; <nl> 
int afu_register_irqs ( struct cxl_context * ctx , u32 count ) <nl> */ <nl> INIT_LIST_HEAD (& ctx -> irq_names ); <nl> for ( r = 1 ; r < CXL_IRQ_RANGES ; r ++) { <nl> - for ( i = 0 ; i < ctx -> irqs . range [ r ]; hwirq ++, i ++) { <nl> + for ( i = 0 ; i < ctx -> irqs . range [ r ]; i ++) { <nl> irq_name = kmalloc ( sizeof ( struct cxl_irq_name ), <nl> GFP_KERNEL ); <nl> if (! irq_name )
# include < mach / mmc . h > <nl> # include < mach / ohci . h > <nl> # include < mach / pxa2xx - regs . h > <nl> +# include < mach / audio . h > <nl>  <nl> # include " generic . h " <nl> # include " devices . h " <nl> static void __init csb726_init ( void ) <nl> pxa27x_set_i2c_power_info ( NULL ); <nl> pxa_set_mci_info (& csb726_mci ); <nl> pxa_set_ohci_info (& csb726_ohci_platform_data ); <nl> + pxa_set_ac97_info ( NULL ); <nl>  <nl> platform_add_devices ( devices , ARRAY_SIZE ( devices )); <nl> }
int radeon_sa_bo_new ( struct radeon_device * rdev , <nl> offset = 0 ; <nl> list_for_each_entry ( tmp , & sa_manager -> sa_bo , list ) { <nl> /* room before this object ? */ <nl> - if (( tmp -> offset - offset ) >= size ) { <nl> + if ( offset < tmp -> offset && ( tmp -> offset - offset ) >= size ) { <nl> head = tmp -> list . prev ; <nl> goto out ; <nl> }
static int ll_dir_setdirstripe ( struct inode * parent , struct lmv_user_md * lump , <nl> PFID ( ll_inode2fid ( parent )), parent , dirname , <nl> ( int ) lump -> lum_stripe_offset , lump -> lum_stripe_count ); <nl>  <nl> + if ( lump -> lum_stripe_count > 1 && <nl> + !( exp_connect_flags ( sbi -> ll_md_exp ) & OBD_CONNECT_DIR_STRIPE )) <nl> + return - EINVAL ; <nl> + <nl> if ( lump -> lum_magic != cpu_to_le32 ( LMV_USER_MAGIC )) <nl> lustre_swab_lmv_user_md ( lump ); <nl> 
static int __devinit rtsx_probe ( struct pci_dev * pci , <nl> th = kthread_create ( rtsx_scan_thread , dev , " rtsx - scan "); <nl> if ( IS_ERR ( th )) { <nl> printk ( KERN_ERR " Unable to start the device - scanning thread \ n "); <nl> + complete (& dev -> scanning_done ); <nl> quiesce_and_remove_host ( dev ); <nl> err = PTR_ERR ( th ); <nl> goto errout ;
void nf_log_packet ( int pf , <nl> const struct net_device * in , <nl> const struct net_device * out , <nl> const struct nf_loginfo * li , <nl> - const char * fmt , ...); <nl> + const char * fmt , ...) __attribute__ (( format ( printf , 7 , 8 ))); <nl>  <nl> # endif /* _NF_LOG_H */
static int __init scm_blk_init ( void ) <nl> scm_major = ret ; <nl> ret = scm_alloc_rqs ( nr_requests ); <nl> if ( ret ) <nl> - goto out_unreg ; <nl> + goto out_free ; <nl>  <nl> scm_debug = debug_register (" scm_log ", 16 , 1 , 16 ); <nl> if (! scm_debug ) { <nl> out_dbf : <nl> debug_unregister ( scm_debug ); <nl> out_free : <nl> scm_free_rqs (); <nl> - out_unreg : <nl> unregister_blkdev ( scm_major , " scm "); <nl> out : <nl> return ret ;
int scsi_dh_activate ( struct request_queue * q , activate_complete fn , void * data ) <nl>  <nl> if (! sdev -> handler ) <nl> goto out_fn ; <nl> + err = SCSI_DH_NOTCONN ; <nl> if ( sdev -> sdev_state == SDEV_CANCEL || <nl> sdev -> sdev_state == SDEV_DEL ) <nl> goto out_fn ;
static int rndis_wlan_bind ( struct usbnet * usbdev , struct usb_interface * intf ) <nl>  <nl> /* because rndis_command () sleeps we need to use workqueue */ <nl> priv -> workqueue = create_singlethread_workqueue (" rndis_wlan "); <nl> + if (! priv -> workqueue ) { <nl> + wiphy_free ( wiphy ); <nl> + return - ENOMEM ; <nl> + } <nl> INIT_WORK (& priv -> work , rndis_wlan_worker ); <nl> INIT_DELAYED_WORK (& priv -> dev_poller_work , rndis_device_poller ); <nl> INIT_DELAYED_WORK (& priv -> scan_work , rndis_get_scan_results );
int be_cmd_loopback_test ( struct be_adapter * adapter , u32 port_num , <nl>  <nl> be_cmd_hdr_prepare (& req -> hdr , CMD_SUBSYSTEM_LOWLEVEL , <nl> OPCODE_LOWLEVEL_LOOPBACK_TEST , sizeof (* req )); <nl> + req -> hdr . timeout = 4 ; <nl>  <nl> req -> pattern = cpu_to_le64 ( pattern ); <nl> req -> src_port = cpu_to_le32 ( port_num );
static void iwl_mvm_unshare_queue ( struct iwl_mvm * mvm , int queue ) <nl>  <nl> /* If aggs should be turned back on - do it */ <nl> if ( mvmsta -> tid_data [ tid ]. state == IWL_AGG_ON ) { <nl> - struct iwl_mvm_add_sta_cmd cmd ; <nl> + struct iwl_mvm_add_sta_cmd cmd = { 0 }; <nl>  <nl> mvmsta -> tid_disable_agg &= ~ BIT ( tid ); <nl> 
int snd_hda_create_dig_out_ctls ( struct hda_codec * codec , <nl> return - EBUSY ; <nl> } <nl> spdif = snd_array_new (& codec -> spdif_out ); <nl> + if (! spdif ) <nl> + return - ENOMEM ; <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> if (! kctl )
static int ravb_close ( struct net_device * ndev ) <nl> priv -> phydev = NULL ; <nl> } <nl>  <nl> + if ( priv -> chip_id == RCAR_GEN3 ) <nl> + free_irq ( priv -> emac_irq , ndev ); <nl> free_irq ( ndev -> irq , ndev ); <nl>  <nl> napi_disable (& priv -> napi [ RAVB_NC ]);
static ssize_t gt_boost_freq_mhz_store ( struct device * kdev , <nl> { <nl> struct drm_minor * minor = dev_to_drm_minor ( kdev ); <nl> struct drm_device * dev = minor -> dev ; <nl> - struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> + struct drm_i915_private * dev_priv = to_i915 ( dev ); <nl> u32 val ; <nl> ssize_t ret ; <nl> 
static int __init hdaps_init ( void ) <nl>  <nl> if (! dmi_check_system ( hdaps_whitelist )) { <nl> printk ( KERN_WARNING " hdaps : supported laptop not found !\ n "); <nl> - ret = - ENXIO ; <nl> + ret = - ENODEV ; <nl> goto out ; <nl> } <nl> 
static int spidev_release ( struct inode * inode , struct file * filp ) <nl> kfree ( spidev -> rx_buffer ); <nl> spidev -> rx_buffer = NULL ; <nl>  <nl> - spidev -> speed_hz = spidev -> spi -> max_speed_hz ; <nl> + if ( spidev -> spi ) <nl> + spidev -> speed_hz = spidev -> spi -> max_speed_hz ; <nl>  <nl> /* ... after we unbound from the underlying device ? */ <nl> spin_lock_irq (& spidev -> spi_lock );
void rt2x00link_update_stats ( struct rt2x00_dev * rt2x00dev , <nl> struct link_ant * ant = & rt2x00dev -> link . ant ; <nl> struct ieee80211_hdr * hdr = ( struct ieee80211_hdr *) skb -> data ; <nl>  <nl> + /* <nl> + * No need to update the stats for != STA interfaces <nl> + */ <nl> + if (! rt2x00dev -> intf_sta_count ) <nl> + return ; <nl> + <nl> /* <nl> * Frame was received successfully since non - succesfull <nl> * frames would have been dropped by the hardware .
int ll_fid2path ( struct inode * inode , void __user * arg ) <nl> if ( get_user ( pathlen , & gfin -> gf_pathlen )) <nl> return - EFAULT ; <nl>  <nl> + if ( pathlen > PATH_MAX ) <nl> + return - EINVAL ; <nl> + <nl> outsize = sizeof (* gfout ) + pathlen ; <nl>  <nl> OBD_ALLOC ( gfout , outsize );
struct nv40_mpeg_priv { <nl> }; <nl>  <nl> struct nv40_mpeg_chan { <nl> - struct nouveau_mpeg base ; <nl> + struct nouveau_mpeg_chan base ; <nl> }; <nl>  <nl> /*******************************************************************************
static u32 mop500_sdi0_vdd_handler ( struct device * dev , unsigned int vdd , <nl> unsigned char power_mode ) <nl> { <nl> if ( power_mode == MMC_POWER_UP ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 1 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 1 ); <nl> else if ( power_mode == MMC_POWER_OFF ) <nl> - gpio_set_value ( GPIO_SDMMC_EN , 0 ); <nl> + gpio_set_value_cansleep ( GPIO_SDMMC_EN , 0 ); <nl>  <nl> return MCI_FBCLKEN | MCI_CMDDIREN | MCI_DATA0DIREN | <nl> MCI_DATA2DIREN | MCI_DATA31DIREN ;
static int hws_cpu_callback ( struct notifier_block * nfb , <nl> { <nl> /* We do not have sampler space available for all possible CPUs . <nl> All CPUs should be online when hw sampling is activated . */ <nl> - return NOTIFY_BAD ; <nl> + return ( hws_state <= HWS_DEALLOCATED ) ? NOTIFY_OK : NOTIFY_BAD ; <nl> } <nl>  <nl> static struct notifier_block hws_cpu_notifier = {
static ssize_t macvtap_put_user ( struct macvtap_queue * q , <nl> if ( copy_to_iter (& vnet_hdr , sizeof ( vnet_hdr ), iter ) != <nl> sizeof ( vnet_hdr )) <nl> return - EFAULT ; <nl> + <nl> + iov_iter_advance ( iter , vnet_hdr_len - sizeof ( vnet_hdr )); <nl> } <nl> total = vnet_hdr_len ; <nl> total += skb -> len ;
static void xmit_common ( struct sk_buff * skb , struct ehea_swqe * swqe ) <nl> { <nl> swqe -> tx_control |= EHEA_SWQE_IMM_DATA_PRESENT | EHEA_SWQE_CRC ; <nl>  <nl> - if ( skb -> protocol != htons ( ETH_P_IP )) <nl> + if ( vlan_get_protocol ( skb ) != htons ( ETH_P_IP )) <nl> return ; <nl>  <nl> if ( skb -> ip_summed == CHECKSUM_PARTIAL )
static struct resource wdt_sch_resource = { <nl>  <nl> static struct mfd_cell tunnelcreek_cells [] = { <nl> { <nl> - . name = " tunnelcreek_wdt ", <nl> + . name = " ie6xx_wdt ", <nl> . num_resources = 1 , <nl> . resources = & wdt_sch_resource , <nl> },
int p54_parse_eeprom ( struct ieee80211_hw * dev , void * eeprom , int len ) <nl> case PDR_END : <nl> i = len ; <nl> break ; <nl> + default : <nl> + printk ( KERN_INFO " p54 : unknown eeprom code : 0x % x \ n ", <nl> + le16_to_cpu ( entry -> code )); <nl> + break ; <nl> } <nl>  <nl> entry = ( void *) entry + ( entry_len + 1 )* 2 ;
ssize_t uwb_est_get_size ( struct uwb_rc * uwb_rc , struct uwb_est * est , <nl> case UWB_EST_8 : type_size = sizeof ( u8 ); break ; <nl> default : BUG (); <nl> } <nl> - if ( offset + type_size >= rceb_size ) { <nl> + if ( offset + type_size > rceb_size ) { <nl> if ( printk_ratelimit ()) <nl> dev_err ( dev , " EST % p 0x % 04x /% 04x /% 04x [% u ]: " <nl> " not enough data to read extra size \ n ",
static long vop_ioctl ( struct file * f , unsigned int cmd , unsigned long arg ) <nl> ret = - EFAULT ; <nl> goto free_ret ; <nl> } <nl> + /* Ensure desc has not changed between the two reads */ <nl> + if ( memcmp (& dd , dd_config , sizeof ( dd ))) { <nl> + ret = - EINVAL ; <nl> + goto free_ret ; <nl> + } <nl> mutex_lock (& vdev -> vdev_mutex ); <nl> mutex_lock (& vi -> vop_mutex ); <nl> ret = vop_virtio_add_device ( vdev , dd_config );
void hid_sensor_remove_trigger ( struct iio_dev * indio_dev ) <nl> { <nl> iio_trigger_unregister ( indio_dev -> trig ); <nl> iio_trigger_free ( indio_dev -> trig ); <nl> + indio_dev -> trig = NULL ; <nl> } <nl> EXPORT_SYMBOL ( hid_sensor_remove_trigger ); <nl> 
static void get_new_segment ( struct f2fs_sb_info * sbi , <nl> if (! new_sec && ((* newseg + 1 ) % sbi -> segs_per_sec )) { <nl> segno = find_next_zero_bit ( free_i -> free_segmap , <nl> TOTAL_SEGS ( sbi ), * newseg + 1 ); <nl> - if ( segno < TOTAL_SEGS ( sbi )) <nl> + if ( segno - * newseg < sbi -> segs_per_sec - <nl> + (* newseg % sbi -> segs_per_sec )) <nl> goto got_it ; <nl> } <nl> find_other_zone :
static int pcm3168a_set_dai_sysclk ( struct snd_soc_dai * dai , <nl> int clk_id , unsigned int freq , int dir ) <nl> { <nl> struct pcm3168a_priv * pcm3168a = snd_soc_codec_get_drvdata ( dai -> codec ); <nl> + int ret ; <nl>  <nl> if ( freq > PCM1368A_MAX_SYSCLK ) <nl> return - EINVAL ; <nl>  <nl> + ret = clk_set_rate ( pcm3168a -> scki , freq ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> pcm3168a -> sysclk = freq ; <nl>  <nl> return 0 ;
static int t4_sched_queue_bind ( struct port_info * pi , struct ch_sched_queue * p ) <nl>  <nl> /* Unbind queue from any existing class */ <nl> err = t4_sched_queue_unbind ( pi , p ); <nl> - if ( err ) <nl> + if ( err ) { <nl> + t4_free_mem ( qe ); <nl> goto out ; <nl> + } <nl>  <nl> /* Bind queue to specified class */ <nl> memset ( qe , 0 , sizeof (* qe ));
static int x2apic_acpi_madt_oem_check ( char * oem_id , char * oem_table_id ) <nl> { <nl> if ( x2apic_phys ) <nl> return x2apic_enabled (); <nl> + else if (( acpi_gbl_FADT . header . revision >= FADT2_REVISION_ID ) && <nl> + ( acpi_gbl_FADT . flags & ACPI_FADT_APIC_PHYSICAL ) && <nl> + x2apic_enabled ()) { <nl> + printk ( KERN_DEBUG " System requires x2apic physical mode \ n "); <nl> + return 1 ; <nl> + } <nl> else <nl> return 0 ; <nl> }
ldebugfs_fid_write_common ( const char __user * buffer , size_t count , <nl> rc = sscanf ( kernbuf , "[% llx - % llx ]\ n ", <nl> ( unsigned long long *)& tmp . lsr_start , <nl> ( unsigned long long *)& tmp . lsr_end ); <nl> + if ( rc != 2 ) <nl> + return - EINVAL ; <nl> if (! range_is_sane (& tmp ) || range_is_zero (& tmp ) || <nl> tmp . lsr_start < range -> lsr_start || tmp . lsr_end > range -> lsr_end ) <nl> return - EINVAL ;
static int hdlcdrv_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> case HDLCDRVCTL_CALIBRATE : <nl> if (! capable ( CAP_SYS_RAWIO )) <nl> return - EPERM ; <nl> + if ( bi . data . calibrate > INT_MAX / s -> par . bitrate ) <nl> + return - EINVAL ; <nl> s -> hdlctx . calibrate = bi . data . calibrate * s -> par . bitrate / 16 ; <nl> return 0 ; <nl> 
static int snd_ctl_elem_user_tlv ( struct snd_kcontrol * kcontrol , <nl> ue -> tlv_data = new_data ; <nl> ue -> tlv_data_size = size ; <nl> } else { <nl> + if (! ue -> tlv_data_size || ! ue -> tlv_data ) <nl> + return - ENXIO ; <nl> if ( size < ue -> tlv_data_size ) <nl> return - ENOSPC ; <nl> if ( copy_to_user ( tlv , ue -> tlv_data , ue -> tlv_data_size ))
static void enqueue_cmd_and_start_io ( ctlr_info_t * h , <nl> spin_lock_irqsave (& h -> lock , flags ); <nl> addQ (& h -> reqQ , c ); <nl> h -> Qdepth ++; <nl> + if ( h -> Qdepth > h -> maxQsinceinit ) <nl> + h -> maxQsinceinit = h -> Qdepth ; <nl> start_io ( h ); <nl> spin_unlock_irqrestore (& h -> lock , flags ); <nl> }
static int usb_remote_probe ( struct usb_interface * intf , <nl> devnum = dev -> devnum ; <nl> maxp = usb_maxpacket ( dev , pipe , usb_pipeout ( pipe )); <nl>  <nl> - dprintk ( DRIVER_NAME "[% d ]: bytes_in_key =% lu maxp =% d \ n ", <nl> + dprintk ( DRIVER_NAME "[% d ]: bytes_in_key =% zu maxp =% d \ n ", <nl> devnum , CODE_LENGTH , maxp ); <nl>  <nl> 
int irq_domain_simple_dt_translate ( struct irq_domain * d , <nl> return - EINVAL ; <nl> if ( intsize < 1 ) <nl> return - EINVAL ; <nl> + if ( d -> nr_irq && (( intspec [ 0 ] < d -> hwirq_base ) || <nl> + ( intspec [ 0 ] >= d -> hwirq_base + d -> nr_irq ))) <nl> + return - EINVAL ; <nl>  <nl> * out_hwirq = intspec [ 0 ]; <nl> * out_type = IRQ_TYPE_NONE ;
static int t7l66xb_probe ( struct platform_device * dev ) <nl> struct resource * iomem , * rscr ; <nl> int ret ; <nl>  <nl> + if ( pdata == NULL ) <nl> + return - EINVAL ; <nl> + <nl> iomem = platform_get_resource ( dev , IORESOURCE_MEM , 0 ); <nl> if (! iomem ) <nl> return - EINVAL ;
static void omap_dma_free_chan_resources ( struct dma_chan * chan ) <nl> vchan_free_chan_resources (& c -> vc ); <nl> omap_free_dma ( c -> dma_ch ); <nl>  <nl> - dev_dbg ( od -> ddev . dev , " freeing channel for % u \ n ", c -> dma_sig ); <nl> + dev_dbg ( od -> ddev . dev , " freeing channel % u used for % u \ n ", c -> dma_ch , <nl> + c -> dma_sig ); <nl> c -> dma_sig = 0 ; <nl> } <nl> 
int module_finalize ( const Elf32_Ehdr * hdr , const Elf_Shdr * sechdrs , <nl> # endif <nl> s = find_mod_section ( hdr , sechdrs , ". alt . smp . init "); <nl> if ( s && ! is_smp ()) <nl> +# ifdef CONFIG_SMP_ON_UP <nl> fixup_smp (( void *) s -> sh_addr , s -> sh_size ); <nl> +# else <nl> + return - EINVAL ; <nl> +# endif <nl> return 0 ; <nl> } <nl> 
int btrfs_reserve_extent ( struct btrfs_root * root , <nl> u64 empty_size , u64 hint_byte , <nl> struct btrfs_key * ins , int is_data , int delalloc ) <nl> { <nl> - bool final_tried = false ; <nl> + bool final_tried = num_bytes == min_alloc_size ; <nl> u64 flags ; <nl> int ret ; <nl> 
static int emi26_load_firmware ( struct usb_device * dev ) <nl>  <nl> /* De - assert reset ( let the CPU run ) */ <nl> err = emi26_set_reset ( dev , 0 ); <nl> + if ( err < 0 ) { <nl> + err ("% s - error loading firmware : error = % d ", __FUNCTION__ , err ); <nl> + goto wraperr ; <nl> + } <nl> msleep ( 250 ); /* let device settle */ <nl>  <nl> /* 2 . We upload the FPGA firmware into the EMI
static int record_connection ( char * host , char * port , char * busid , int rhport ) <nl> char buff [ MAX_BUFF + 1 ]; <nl> int ret ; <nl>  <nl> - mkdir ( VHCI_STATE_PATH , 0700 ); <nl> + ret = mkdir ( VHCI_STATE_PATH , 0700 ); <nl> + if ( ret < 0 ) <nl> + return - 1 ; <nl>  <nl> snprintf ( path , PATH_MAX , VHCI_STATE_PATH "/ port % d ", rhport ); <nl> 
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> if (! dev -> suspend_in_progress ) <nl> b43_rng_init ( wl ); <nl>  <nl> + ieee80211_wake_queues ( dev -> wl -> hw ); <nl> + <nl> b43_set_status ( dev , B43_STAT_INITIALIZED ); <nl>  <nl> if (! dev -> suspend_in_progress )
deinit : <nl> wl1271_free_ap_keys ( wl , wlvif ); <nl> } <nl>  <nl> + dev_kfree_skb ( wlvif -> probereq ); <nl> + wlvif -> probereq = NULL ; <nl> wl12xx_tx_reset_wlvif ( wl , wlvif ); <nl> if ( wl -> last_wlvif == wlvif ) <nl> wl -> last_wlvif = NULL ;
static void sh_msiof_spi_chipselect ( struct spi_device * spi , int is_on ) <nl> } <nl>  <nl> /* use spi -> controller data for CS ( same strategy as spi_gpio ) */ <nl> - gpio_set_value (( unsigned ) spi -> controller_data , value ); <nl> + gpio_set_value (( uintptr_t ) spi -> controller_data , value ); <nl>  <nl> if ( is_on == BITBANG_CS_INACTIVE ) { <nl> if ( test_and_clear_bit ( 0 , & p -> flags )) {
static int start_ap ( struct wiphy * wiphy , struct net_device * dev , <nl>  <nl> priv = wiphy_priv ( wiphy ); <nl> vif = netdev_priv ( dev ); <nl> - wl = vif -> wilc ; <nl> + wl = vif -> wilc ; <nl> PRINT_D ( HOSTAPD_DBG , " Starting ap \ n "); <nl>  <nl> PRINT_D ( HOSTAPD_DBG , " Interval = % d \ n DTIM period = % d \ n Head length = % zu Tail length = % zu \ n ",
static inline struct kmem_cache * cache_from_obj ( struct kmem_cache * s , void * x ) <nl> return cachep ; <nl>  <nl> pr_err ("% s : Wrong slab cache . % s but object is from % s \ n ", <nl> - __FUNCTION__ , cachep -> name , s -> name ); <nl> + __func__ , cachep -> name , s -> name ); <nl> WARN_ON_ONCE ( 1 ); <nl> return s ; <nl> }
struct dnode_of_data { <nl> static inline void set_new_dnode ( struct dnode_of_data * dn , struct inode * inode , <nl> struct page * ipage , struct page * npage , nid_t nid ) <nl> { <nl> + memset ( dn , 0 , sizeof (* dn )); <nl> dn -> inode = inode ; <nl> dn -> inode_page = ipage ; <nl> dn -> node_page = npage ; <nl> dn -> nid = nid ; <nl> - dn -> inode_page_locked = 0 ; <nl> } <nl>  <nl> /*
static int amdgpu_ttm_io_mem_reserve ( struct ttm_bo_device * bdev , struct ttm_mem_ <nl> mem -> bus . addr = <nl> ioremap_nocache ( mem -> bus . base + mem -> bus . offset , <nl> mem -> bus . size ); <nl> + if (! mem -> bus . addr ) <nl> + return - ENOMEM ; <nl>  <nl> /* <nl> * Alpha : Use just the bus offset plus
static void xfrm_hash_rebuild ( struct work_struct * work ) <nl>  <nl> /* re - insert all policies by order of creation */ <nl> list_for_each_entry_reverse ( policy , & net -> xfrm . policy_all , walk . all ) { <nl> + if ( xfrm_policy_id2dir ( policy -> index ) >= XFRM_POLICY_MAX ) { <nl> + /* skip socket policies */ <nl> + continue ; <nl> + } <nl> newpos = NULL ; <nl> chain = policy_hash_bysel ( net , & policy -> selector , <nl> policy -> family ,
static int intel_backlight_device_update_status ( struct backlight_device * bd ) <nl> */ <nl> if ( panel -> backlight . enabled ) { <nl> if ( panel -> backlight_power ) { <nl> - bool enable = bd -> props . power == FB_BLANK_UNBLANK ; <nl> + bool enable = bd -> props . power == FB_BLANK_UNBLANK && <nl> + bd -> props . brightness != 0 ; <nl> panel -> backlight_power ( connector , enable ); <nl> } <nl> } else {
int main ( int argc , char ** argv ) <nl> read_relocs ( fp ); <nl> if ( show_absolute_syms ) { <nl> print_absolute_symbols (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> if ( show_absolute_relocs ) { <nl> print_absolute_relocs (); <nl> - return 0 ; <nl> + goto out ; <nl> } <nl> emit_relocs ( as_text , use_real_mode ); <nl> + out : <nl> + fclose ( fp ); <nl> return 0 ; <nl> }
set_ether_config ( struct eth_dev * dev , gfp_t gfp_flags ) <nl> result = usb_ep_enable ( dev -> out_ep , dev -> out ); <nl> if ( result != 0 ) { <nl> DEBUG ( dev , " enable % s --> % d \ n ", <nl> - dev -> in_ep -> name , result ); <nl> + dev -> out_ep -> name , result ); <nl> goto done ; <nl> } <nl> }
int oxygen_pci_probe ( struct pci_dev * pci , int index , char * id , <nl> goto err_pci_regions ; <nl>  <nl> if ( chip -> model . model_data_size ) { <nl> - chip -> model_data = kmalloc ( chip -> model . model_data_size , <nl> + chip -> model_data = kzalloc ( chip -> model . model_data_size , <nl> GFP_KERNEL ); <nl> if (! chip -> model_data ) { <nl> err = - ENOMEM ;
void psched_ratecfg_precompute ( struct psched_ratecfg * r , u32 rate ) <nl> u64 mult ; <nl> int shift ; <nl>  <nl> - r -> rate_bps = rate << 3 ; <nl> + r -> rate_bps = ( u64 ) rate << 3 ; <nl> r -> shift = 0 ; <nl> r -> mult = 1 ; <nl> /*
static void __init eva_init ( void ) <nl> platform_add_devices ( eva_devices , <nl> ARRAY_SIZE ( eva_devices )); <nl>  <nl> - eva_clock_init (); <nl> - <nl> rmobile_add_device_to_domain (" A4LC ", & lcdc0_device ); <nl> rmobile_add_device_to_domain (" A4LC ", & hdmi_lcdc_device ); <nl> if ( usb ) <nl> static void __init eva_earlytimer_init ( void ) <nl> { <nl> r8a7740_clock_init ( MD_CK0 | MD_CK2 ); <nl> shmobile_earlytimer_init (); <nl> + <nl> + /* the rate of extal1 clock must be set before late_time_init */ <nl> + eva_clock_init (); <nl> } <nl>  <nl> static void __init eva_add_early_devices ( void )
int rtllib_rx ( struct rtllib_device * ieee , struct sk_buff * skb , <nl> return ret ; <nl>  <nl> rx_dropped : <nl> - ieee -> stats . rx_dropped ++; <nl> + if ( ieee ) <nl> + ieee -> stats . rx_dropped ++; <nl> return 0 ; <nl> } <nl> EXPORT_SYMBOL ( rtllib_rx );
static int smc91c92_config ( struct pcmcia_device * link ) <nl> struct net_device * dev = link -> priv ; <nl> struct smc_private * smc = netdev_priv ( dev ); <nl> char * name ; <nl> - int i , j , rev ; <nl> + int i , rev , j = 0 ; <nl> unsigned int ioaddr ; <nl> u_long mir ; <nl> 
void __init iop13xx_platform_init ( void ) <nl>  <nl> # ifdef CONFIG_MTD_PHYSMAP <nl> iq8134x_flash_resource . end = iq8134x_flash_resource . start + <nl> - iq8134x_probe_flash_size (); <nl> + iq8134x_probe_flash_size () - 1 ; <nl> if ( iq8134x_flash_resource . end > iq8134x_flash_resource . start ) <nl> iop13xx_devices [ plat_idx ++] = & iq8134x_flash ; <nl> else
void register_lapic_address ( unsigned long address ); <nl> extern void setup_boot_APIC_clock ( void ); <nl> extern void setup_secondary_APIC_clock ( void ); <nl> extern int APIC_init_uniprocessor ( void ); <nl> + <nl> +# ifdef CONFIG_X86_64 <nl> + static inline int apic_force_enable ( unsigned long addr ) <nl> +{ <nl> + return - 1 ; <nl> +} <nl> +# else <nl> extern int apic_force_enable ( unsigned long addr ); <nl> +# endif <nl>  <nl> extern int apic_bsp_setup ( bool upmode ); <nl> extern void apic_ap_setup ( void );
static unsigned long output_ptr = 0 ; <nl> static void * malloc ( int size ); <nl> static void free ( void * where ); <nl>  <nl> + void * memset ( void * s , int c , unsigned n ); <nl> + void * memcpy ( void * dest , const void * src , unsigned n ); <nl> + <nl> static void putstr ( const char *); <nl>  <nl> extern int end ;
int load_bpf_file ( char * path ) <nl> Elf_Data * data , * data_prog , * symbols = NULL ; <nl> char * shname , * shname_prog ; <nl>  <nl> + /* reset global variables */ <nl> + kern_version = 0 ; <nl> + memset ( license , 0 , sizeof ( license )); <nl> + memset ( processed_sec , 0 , sizeof ( processed_sec )); <nl> + <nl> if ( elf_version ( EV_CURRENT ) == EV_NONE ) <nl> return 1 ; <nl> 
void cpuset_print_task_mems_allowed ( struct task_struct * tsk ) <nl>  <nl> struct cgroup * cgrp = task_cs ( tsk )-> css . cgroup ; <nl>  <nl> + rcu_read_lock (); <nl> spin_lock (& cpuset_buffer_lock ); <nl>  <nl> nodelist_scnprintf ( cpuset_nodelist , CPUSET_NODELIST_LEN , <nl> void cpuset_print_task_mems_allowed ( struct task_struct * tsk ) <nl> tsk -> comm , cgroup_name ( cgrp ), cpuset_nodelist ); <nl>  <nl> spin_unlock (& cpuset_buffer_lock ); <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> /*
static void dm_stat_free ( struct rcu_head * head ) <nl> int cpu ; <nl> struct dm_stat * s = container_of ( head , struct dm_stat , rcu_head ); <nl>  <nl> + kfree ( s -> histogram_boundaries ); <nl> kfree ( s -> program_id ); <nl> kfree ( s -> aux_data ); <nl> for_each_possible_cpu ( cpu ) {
static void ieee80211_do_stop ( struct ieee80211_sub_if_data * sdata , <nl> if ( sdata -> vif . txq ) { <nl> struct txq_info * txqi = to_txq_info ( sdata -> vif . txq ); <nl>  <nl> + spin_lock_bh (& txqi -> queue . lock ); <nl> ieee80211_purge_tx_queue (& local -> hw , & txqi -> queue ); <nl> + spin_unlock_bh (& txqi -> queue . lock ); <nl> + <nl> atomic_set (& sdata -> txqs_len [ txqi -> txq . ac ], 0 ); <nl> } <nl> 
static void mwifiex_unregister_dev ( struct mwifiex_adapter * adapter ) <nl> { <nl> struct pcie_service_card * card = adapter -> card ; <nl> const struct mwifiex_pcie_card_reg * reg ; <nl> - struct pci_dev * pdev = card -> dev ; <nl> + struct pci_dev * pdev ; <nl> int i ; <nl>  <nl> if ( card ) { <nl> + pdev = card -> dev ; <nl> if ( card -> msix_enable ) { <nl> for ( i = 0 ; i < MWIFIEX_NUM_MSIX_VECTORS ; i ++) <nl> synchronize_irq ( card -> msix_entries [ i ]. vector );
static ssize_t store_scaling_governor ( struct cpufreq_policy * policy , <nl> return - EINVAL ; <nl>  <nl> ret = cpufreq_set_policy ( policy , & new_policy ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> policy -> user_policy . policy = policy -> policy ; <nl> policy -> user_policy . governor = policy -> governor ; <nl> - <nl> - if ( ret ) <nl> - return ret ; <nl> - else <nl> - return count ; <nl> + return count ; <nl> } <nl>  <nl> /**
long vhost_dev_ioctl ( struct vhost_dev * d , unsigned int ioctl , void __user * argp ) <nl> } <nl> if ( eventfp != d -> log_file ) { <nl> filep = d -> log_file ; <nl> + d -> log_file = eventfp ; <nl> ctx = d -> log_ctx ; <nl> d -> log_ctx = eventfp ? <nl> eventfd_ctx_fileget ( eventfp ) : NULL ;
void dlm_user_add_ast ( struct dlm_lkb * lkb , int type ) <nl> spin_unlock (& proc -> asts_spin ); <nl>  <nl> if ( eol ) { <nl> - spin_lock (& ua -> proc -> locks_spin ); <nl> + spin_lock (& proc -> locks_spin ); <nl> if (! list_empty (& lkb -> lkb_ownqueue )) { <nl> list_del_init (& lkb -> lkb_ownqueue ); <nl> dlm_put_lkb ( lkb ); <nl> } <nl> - spin_unlock (& ua -> proc -> locks_spin ); <nl> + spin_unlock (& proc -> locks_spin ); <nl> } <nl> out : <nl> mutex_unlock (& ls -> ls_clear_proc_locks );
static int __devinit ci13xxx_pci_probe ( struct pci_dev * pdev , <nl> struct resource res [ 3 ]; <nl> int retval = 0 , nres = 2 ; <nl>  <nl> + if (! driver ) { <nl> + dev_err (& pdev -> dev , " device doesn ' t provide driver data \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> retval = pci_enable_device ( pdev ); <nl> if ( retval ) <nl> goto done ;
static int auerchain_start_wait_urb ( pauerchain_t acp , struct urb * urb , int time <nl> } else <nl> status = urb -> status ; <nl>  <nl> - if ( actual_length ) <nl> + if ( status >= 0 ) <nl> * actual_length = urb -> actual_length ; <nl>  <nl> return status ;
asmlinkage long sys_socketcall ( int call , unsigned long __user * args ) <nl> if ( copy_from_user ( a , args , nargs [ call ])) <nl> return - EFAULT ; <nl>  <nl> - err = audit_socketcall ( nargs [ call ]/ sizeof ( unsigned long ), args ); <nl> + err = audit_socketcall ( nargs [ call ]/ sizeof ( unsigned long ), a ); <nl> if ( err ) <nl> return err ; <nl> 
static void change ( char * dev , char * what , unsigned char * addr , <nl> " buffer \ n "); <nl>  <nl> pid = change_tramp ( argv , output , output_len ); <nl> - if ( pid < 0 ) return ; <nl> + if ( pid < 0 ) { <nl> + kfree ( output ); <nl> + return ; <nl> + } <nl>  <nl> if ( output != NULL ) { <nl> printk ("% s ", output );
static void handle_stop_signal ( int sig , struct task_struct * p ) <nl> { <nl> struct task_struct * t ; <nl>  <nl> - if ( p -> flags & SIGNAL_GROUP_EXIT ) <nl> + if ( p -> signal -> flags & SIGNAL_GROUP_EXIT ) <nl> /* <nl> * The process is in the middle of dying already . <nl> */
static int chap_server_compute_md5 ( <nl> pr_err (" Unable to convert incoming challenge \ n "); <nl> goto out ; <nl> } <nl> + if ( challenge_len > 1024 ) { <nl> + pr_err (" CHAP_C exceeds maximum binary size of 1024 bytes \ n "); <nl> + goto out ; <nl> + } <nl> /* <nl> * During mutual authentication , the CHAP_C generated by the <nl> * initiator must not match the original CHAP_C generated by
void brcmf_fweh_process_event ( struct brcmf_pub * drvr , <nl> alloc_flag = GFP_ATOMIC ; <nl>  <nl> event = kzalloc ( sizeof (* event ) + datalen , alloc_flag ); <nl> + if (! event ) <nl> + return ; <nl> + <nl> event -> code = code ; <nl> event -> ifidx = * ifidx ; <nl> 
skl_compute_ddb ( struct drm_atomic_state * state ) <nl> ret = skl_allocate_pipe_ddb ( cstate , ddb ); <nl> if ( ret ) <nl> return ret ; <nl> + <nl> + ret = drm_atomic_add_affected_planes ( state , & intel_crtc -> base ); <nl> + if ( ret ) <nl> + return ret ; <nl> } <nl>  <nl> return 0 ;
static int ibmvfc_get_err_result ( struct ibmvfc_cmd * vfc_cmd ) <nl> int fc_rsp_len = rsp -> fcp_rsp_len ; <nl>  <nl> if (( rsp -> flags & FCP_RSP_LEN_VALID ) && <nl> - ((! fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> + (( fc_rsp_len && fc_rsp_len != 4 && fc_rsp_len != 8 ) || <nl> rsp -> data . info . rsp_code )) <nl> return DID_ERROR << 16 ; <nl> 
static unsigned int xuartps_set_baud_rate ( struct uart_port * port , <nl> unsigned int baud ) <nl> { <nl> unsigned int calc_baud ; <nl> - u32 cd , bdiv ; <nl> + u32 cd = 0 , bdiv = 0 ; <nl> u32 mreg ; <nl> int div8 ; <nl> struct xuartps * xuartps = port -> private_data ;
static int ac97_write ( struct snd_soc_codec * codec , unsigned int reg , <nl> { <nl> u16 * cache = codec -> reg_cache ; <nl>  <nl> - soc_ac97_ops . write ( codec -> ac97 , reg , val ); <nl> + if ( reg < 0x7c ) <nl> + soc_ac97_ops . write ( codec -> ac97 , reg , val ); <nl> reg = reg >> 1 ; <nl> if ( reg < ( ARRAY_SIZE ( wm9712_reg ))) <nl> cache [ reg ] = val ;
static void i40evf_watchdog_task ( struct work_struct * work ) <nl> watchdog_done : <nl> clear_bit ( __I40EVF_IN_CRITICAL_TASK , & adapter -> crit_section ); <nl> restart_watchdog : <nl> + if ( adapter -> state == __I40EVF_REMOVE ) <nl> + return ; <nl> if ( adapter -> aq_required ) <nl> mod_timer (& adapter -> watchdog_timer , <nl> jiffies + msecs_to_jiffies ( 20 ));
static int AdvLoadMicrocode ( AdvPortAddr iop_base , unsigned char * buf , int size , <nl> i += 2 ; <nl> len += 2 ; <nl> } else { <nl> - unsigned char off = buf [ i ] * 2 ; <nl> + unsigned int off = buf [ i ] * 2 ; <nl> unsigned short word = ( buf [ off + 1 ] << 8 ) | buf [ off ]; <nl> AdvWriteWordAutoIncLram ( iop_base , word ); <nl> len += 2 ;
fill_write_buffer ( struct sysfs_buffer * buffer , const char __user * buf , size_t <nl> count = PAGE_SIZE - 1 ; <nl> error = copy_from_user ( buffer -> page , buf , count ); <nl> buffer -> needs_read_fill = 1 ; <nl> + /* if buf is assumed to contain a string , terminate it by \ 0 , <nl> + so e . g . sscanf () can scan the string easily */ <nl> + buffer -> page [ count ] = 0 ; <nl> return error ? - EFAULT : count ; <nl> } <nl> 
static void pti_tty_cleanup ( struct tty_struct * tty ) <nl> if ( pti_tty_data == NULL ) <nl> return ; <nl> pti_release_masterchannel ( pti_tty_data -> mc ); <nl> - kfree ( tty -> driver_data ); <nl> + kfree ( pti_tty_data ); <nl> tty -> driver_data = NULL ; <nl> } <nl> 
LNetCtl ( unsigned int cmd , void * arg ) <nl> int rc ; <nl> unsigned long secs_passed ; <nl>  <nl> + BUILD_BUG_ON ( LIBCFS_IOC_DATA_MAX < <nl> + sizeof ( struct lnet_ioctl_net_config ) + <nl> + sizeof ( struct lnet_ioctl_config_data )); <nl> + <nl> switch ( cmd ) { <nl> case IOC_LIBCFS_GET_NI : <nl> rc = LNetGetId ( data -> ioc_count , & id );
void __cpuinit generic_processor_info ( int apicid , int version ) <nl> num_processors ++; <nl> cpu = cpumask_next_zero (- 1 , cpu_present_mask ); <nl>  <nl> + if ( version != apic_version [ boot_cpu_physical_apicid ]) <nl> + WARN_ONCE ( 1 , <nl> + " ACPI : apic version mismatch , bootcpu : % x cpu % d : % x \ n ", <nl> + apic_version [ boot_cpu_physical_apicid ], cpu , version ); <nl> + <nl> physid_set ( apicid , phys_cpu_present_map ); <nl> if ( apicid == boot_cpu_physical_apicid ) { <nl> /*
static int vidioc_s_register ( struct file * file , void * priv , <nl> 0x02 , <nl> ( u16 ) reg -> reg , 1 , <nl> value , 1 , 2 ); <nl> + break ; <nl> case 0x322 : <nl> ret = <nl> cx231xx_write_i2c_master ( dev ,
err_regulator_enable : <nl> err_regulator_get : <nl> regulator_bulk_free ( ARRAY_SIZE ( wm8731 -> supplies ), wm8731 -> supplies ); <nl>  <nl> - kfree ( wm8731 ); <nl> return ret ; <nl> } <nl> 
static int ad1836_register ( struct ad1836_priv * ad1836 ) <nl>  <nl> if ( ad1836_codec ) { <nl> dev_err ( codec -> dev , " Another ad1836 is registered \ n "); <nl> + kfree ( ad1836 ); <nl> return - EINVAL ; <nl> } <nl> 
static int snd_timer_user_params ( struct file * file , <nl> if ( tu -> timeri -> flags & SNDRV_TIMER_IFLG_EARLY_EVENT ) { <nl> if ( tu -> tread ) { <nl> struct snd_timer_tread tread ; <nl> + memset (& tread , 0 , sizeof ( tread )); <nl> tread . event = SNDRV_TIMER_EVENT_EARLY ; <nl> tread . tstamp . tv_sec = 0 ; <nl> tread . tstamp . tv_nsec = 0 ;
int rxrpc_kernel_send_data ( struct socket * sock , struct rxrpc_call * call , <nl> ret = rxrpc_send_data ( rxrpc_sk ( sock -> sk ), call , msg , len ); <nl> break ; <nl> case RXRPC_CALL_COMPLETE : <nl> - /* It ' s too late for this call */ <nl> - ret = - ESHUTDOWN ; <nl> + read_lock_bh (& call -> state_lock ); <nl> + ret = - call -> error ; <nl> + read_unlock_bh (& call -> state_lock ); <nl> break ; <nl> default : <nl> /* Request phase complete for this client call */
static int ixgbe_ndo_fdb_add ( struct ndmsg * ndm , struct nlattr * tb [], <nl> { <nl> /* guarantee we can provide a unique filter for the unicast address */ <nl> if ( is_unicast_ether_addr ( addr ) || is_link_local_ether_addr ( addr )) { <nl> - if ( IXGBE_MAX_PF_MACVLANS <= netdev_uc_count ( dev )) <nl> + struct ixgbe_adapter * adapter = netdev_priv ( dev ); <nl> + u16 pool = VMDQ_P ( 0 ); <nl> + <nl> + if ( netdev_uc_count ( dev ) >= ixgbe_available_rars ( adapter , pool )) <nl> return - ENOMEM ; <nl> } <nl> 
static int pty_write ( struct tty_struct * tty , const unsigned char * buf , <nl>  <nl> static int pty_write_room ( struct tty_struct * tty ) <nl> { <nl> + if ( tty -> stopped ) <nl> + return 0 ; <nl> return pty_space ( tty -> link ); <nl> } <nl> 
static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> if ( dev -> flags & IFF_SLAVE ) <nl> break ; <nl>  <nl> + if ( idev && idev -> cnf . disable_ipv6 ) <nl> + break ; <nl> + <nl> if ( event == NETDEV_UP ) { <nl> if (! addrconf_qdisc_ok ( dev )) { <nl> /* device is not ready yet . */
static void remove_event_file_dir ( struct ftrace_event_file * file ) <nl>  <nl> list_del (& file -> list ); <nl> remove_subsystem ( file -> system ); <nl> + free_event_filter ( file -> filter ); <nl> kmem_cache_free ( file_cachep , file ); <nl> } <nl> 
static int revoke_lo_scan_elements ( struct gfs2_jdesc * jd , unsigned int start , <nl> blkno = be64_to_cpu (*( __be64 *)( bh -> b_data + offset )); <nl>  <nl> error = gfs2_revoke_add ( sdp , blkno , start ); <nl> - if ( error < 0 ) <nl> + if ( error < 0 ) { <nl> + brelse ( bh ); <nl> return error ; <nl> + } <nl> else if ( error ) <nl> sdp -> sd_found_revokes ++; <nl> 
xfs_check_page_type ( <nl> if ( type == XFS_IO_UNWRITTEN ) <nl> return true ; <nl> } else if ( buffer_delay ( bh )) { <nl> - if ( type == XFS_IO_DELALLOC ); <nl> + if ( type == XFS_IO_DELALLOC ) <nl> return true ; <nl> } else if ( buffer_dirty ( bh ) && buffer_mapped ( bh )) { <nl> - if ( type == XFS_IO_OVERWRITE ); <nl> + if ( type == XFS_IO_OVERWRITE ) <nl> return true ; <nl> } <nl> 
int trace_get_user ( struct trace_parser * parser , const char __user * ubuf , <nl>  <nl> /* read the non - space input */ <nl> while ( cnt && ! isspace ( ch )) { <nl> - if ( parser -> idx < parser -> size ) <nl> + if ( parser -> idx < parser -> size - 1 ) <nl> parser -> buffer [ parser -> idx ++] = ch ; <nl> else { <nl> ret = - EINVAL ;
static void sky2_power_aux ( struct sky2_hw * hw ) <nl> Y2_CLK_GAT_LNK1_DIS | Y2_PCI_CLK_LNK2_DIS | <nl> Y2_COR_CLK_LNK2_DIS | Y2_CLK_GAT_LNK2_DIS ); <nl>  <nl> - /* switch power to VAUX */ <nl> - if ( sky2_read32 ( hw , B0_CTST ) & Y2_VAUX_AVAIL ) <nl> + /* switch power to VAUX if supported and PME from D3cold */ <nl> + if ( ( sky2_read32 ( hw , B0_CTST ) & Y2_VAUX_AVAIL ) && <nl> + pci_pme_capable ( hw -> pdev , PCI_D3cold )) <nl> sky2_write8 ( hw , B0_POWER_CTRL , <nl> ( PC_VAUX_ENA | PC_VCC_ENA | <nl> PC_VAUX_ON | PC_VCC_OFF ));
static struct dma_async_tx_descriptor * edma_prep_slave_sg ( <nl> edma_alloc_slot ( EDMA_CTLR ( echan -> ch_num ), <nl> EDMA_SLOT_ANY ); <nl> if ( echan -> slot [ i ] < 0 ) { <nl> + kfree ( edesc ); <nl> dev_err ( dev , " Failed to allocate slot \ n "); <nl> kfree ( edesc ); <nl> return NULL ;
static int __devinit sta2x11_mfd_probe ( struct pci_dev * pdev , <nl> sta2x11_mfd_setup ( pdev , setup_data ); <nl>  <nl> /* Record this pdev before mfd_add_devices : their probe looks for it */ <nl> - sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl> + if (! sta2x11_mfd_find ( pdev )) <nl> + sta2x11_mfd_add ( pdev , GFP_ATOMIC ); <nl>  <nl> /* Just 2 bars for all mfd ' s at present */ <nl> for ( i = 0 ; i < 2 ; i ++) {
int of_irq_map_one ( struct device_node * device , int index , struct of_irq * out_irq <nl> intsize = * tmp ; <nl>  <nl> /* Check index */ <nl> - if ( index * intsize >= intlen ) <nl> + if (( index + 1 ) * intsize > intlen ) <nl> return - EINVAL ; <nl>  <nl> /* Get new specifier and map it */
static int btrfs_parse_early_options ( const char * options , fmode_t flags , <nl> token = match_token ( p , tokens , args ); <nl> switch ( token ) { <nl> case Opt_subvol : <nl> + kfree (* subvol_name ); <nl> * subvol_name = match_strdup (& args [ 0 ]); <nl> break ; <nl> case Opt_subvolid :
static int rename_volumes ( struct ubi_device * ubi , <nl> req -> ents [ i ]. name [ req -> ents [ i ]. name_len ] = '\ 0 '; <nl> n = strlen ( req -> ents [ i ]. name ); <nl> if ( n != req -> ents [ i ]. name_len ) <nl> - err = - EINVAL ; <nl> + return - EINVAL ; <nl> } <nl>  <nl> /* Make sure volume IDs and names are unique */
static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> b43_set_phytxctl_defaults ( dev ); <nl>  <nl> /* Minimum Contention Window */ <nl> - if ( phy -> type == B43_PHYTYPE_B ) { <nl> + if ( phy -> type == B43_PHYTYPE_B ) <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0x1F ); <nl> - } else { <nl> + else <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0xF ); <nl> - } <nl> /* Maximum Contention Window */ <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MAXCONT , 0x3FF ); <nl> 
static int doc_write_oob ( struct mtd_info * mtd , loff_t ofs , <nl> oobdelta = mtd -> ecclayout -> oobavail ; <nl> break ; <nl> default : <nl> - oobdelta = 0 ; <nl> + return - EINVAL ; <nl> } <nl> if (( len % DOC_LAYOUT_PAGE_SIZE ) || ( ooblen % oobdelta ) || <nl> ( ofs % DOC_LAYOUT_PAGE_SIZE ))
int max8998_irq_init ( struct max8998_dev * max8998 ) <nl>  <nl> void max8998_irq_exit ( struct max8998_dev * max8998 ) <nl> { <nl> + if ( max8998 -> ono ) <nl> + free_irq ( max8998 -> ono , max8998 ); <nl> + <nl> if ( max8998 -> irq ) <nl> free_irq ( max8998 -> irq , max8998 ); <nl> }
static int falcon_mtd_probe ( struct efx_nic * efx ) <nl>  <nl> /* Allocate space for maximum number of partitions */ <nl> parts = kcalloc ( 2 , sizeof (* parts ), GFP_KERNEL ); <nl> + if (! parts ) <nl> + return - ENOMEM ; <nl> n_parts = 0 ; <nl>  <nl> spi = & nic_data -> spi_flash ;
int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
struct device ; <nl>  <nl> enum led_brightness { <nl> LED_OFF = 0 , <nl> + LED_ON = 1 , <nl> LED_HALF = 127 , <nl> LED_FULL = 255 , <nl> };
static void napi_reuse_skb ( struct napi_struct * napi , struct sk_buff * skb ) <nl> __skb_pull ( skb , skb_headlen ( skb )); <nl> skb_reserve ( skb , NET_IP_ALIGN - skb_headroom ( skb )); <nl> skb -> vlan_tci = 0 ; <nl> + skb -> dev = napi -> dev ; <nl>  <nl> napi -> skb = skb ; <nl> }
void __iomem * __arm_ioremap_pfn_caller ( unsigned long pfn , <nl> if (! area ) <nl> return NULL ; <nl> addr = ( unsigned long ) area -> addr ; <nl> + area -> phys_addr = __pfn_to_phys ( pfn ); <nl>  <nl> # if ! defined ( CONFIG_SMP ) && ! defined ( CONFIG_ARM_LPAE ) <nl> if ( DOMAIN_IO == 0 &&
static int run ( mddev_t * mddev ) <nl> int i ; <nl>  <nl> conf_t * conf = kmalloc ( sizeof (* conf ), GFP_KERNEL ); <nl> + if (! conf ) <nl> + return - ENOMEM ; <nl>  <nl> for ( i = 0 ; i < Modes ; i ++) { <nl> atomic_set (& conf -> counters [ i ], 0 );
nfqnl_recv_config ( struct sock * ctnl , struct sk_buff * skb , <nl>  <nl> if ( nfqa [ NFQA_CFG_PARAMS - 1 ]) { <nl> struct nfqnl_msg_config_params * params ; <nl> - params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl>  <nl> + if (! queue ) { <nl> + ret = - ENOENT ; <nl> + goto out_put ; <nl> + } <nl> + params = NFA_DATA ( nfqa [ NFQA_CFG_PARAMS - 1 ]); <nl> nfqnl_set_mode ( queue , params -> copy_mode , <nl> ntohl ( params -> copy_range )); <nl> }
void __init kmem_cache_init ( void ) <nl> /* Allocate two kmem_caches from the page allocator */ <nl> kmalloc_size = ALIGN ( kmem_size , cache_line_size ()); <nl> order = get_order ( 2 * kmalloc_size ); <nl> - kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT , order ); <nl> + kmem_cache = ( void *) __get_free_pages ( GFP_NOWAIT | __GFP_ZERO , order ); <nl>  <nl> /* <nl> * Must first have the slab cache available for the allocations of the
static int snd_hdsp_hwdep_ioctl ( struct snd_hwdep * hw , struct file * file , unsigne <nl> if (( err = hdsp_get_iobox_version ( hdsp )) < 0 ) <nl> return err ; <nl> } <nl> + memset (& hdsp_version , 0 , sizeof ( hdsp_version )); <nl> hdsp_version . io_type = hdsp -> io_type ; <nl> hdsp_version . firmware_rev = hdsp -> firmware_rev ; <nl> if (( err = copy_to_user ( argp , & hdsp_version , sizeof ( hdsp_version ))))
int vfs_quota_on_mount ( struct super_block * sb , char * qf_name , <nl> struct dentry * dentry ; <nl> int error ; <nl>  <nl> + mutex_lock (& sb -> s_root -> d_inode -> i_mutex ); <nl> dentry = lookup_one_len ( qf_name , sb -> s_root , strlen ( qf_name )); <nl> + mutex_unlock (& sb -> s_root -> d_inode -> i_mutex ); <nl> if ( IS_ERR ( dentry )) <nl> return PTR_ERR ( dentry ); <nl> 
static int emac_link_differs ( struct emac_instance * dev ) <nl> static void emac_link_timer ( struct work_struct * work ) <nl> { <nl> struct emac_instance * dev = <nl> - container_of (( struct delayed_work *) work , <nl> + container_of ( to_delayed_work ( work ), <nl> struct emac_instance , link_work ); <nl> int link_poll_interval ; <nl> 
static int log_one_block ( struct log_writes_c * lc , <nl> goto out ; <nl> sector ++; <nl>  <nl> + atomic_inc (& lc -> io_blocks ); <nl> bio = bio_alloc ( GFP_KERNEL , block -> vec_cnt ); <nl> if (! bio ) { <nl> DMERR (" Couldn ' t alloc log bio "); <nl> goto error ; <nl> } <nl> - atomic_inc (& lc -> io_blocks ); <nl> bio -> bi_iter . bi_size = 0 ; <nl> bio -> bi_iter . bi_sector = sector ; <nl> bio -> bi_bdev = lc -> logdev -> bdev ;
static int igb_setup_loopback_test ( struct igb_adapter * adapter ) <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_SERDES ) || <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_BACKPLANE ) || <nl> ( hw -> device_id == E1000_DEV_ID_DH89XXCC_SFP ) || <nl> - ( hw -> device_id == E1000_DEV_ID_I354_SGMII )) { <nl> - <nl> + ( hw -> device_id == E1000_DEV_ID_I354_SGMII ) || <nl> + ( hw -> device_id == E1000_DEV_ID_I354_BACKPLANE_2_5GBPS )) { <nl> /* Enable DH89xxCC MPHY for near end loopback */ <nl> reg = rd32 ( E1000_MPHY_ADDR_CTL ); <nl> reg = ( reg & E1000_MPHY_ADDR_CTL_OFFSET_MASK ) |
static ssize_t gt_max_freq_mhz_show ( struct device * kdev , struct device_attribute <nl> int ret ; <nl>  <nl> mutex_lock (& dev_priv -> rps . hw_lock ); <nl> - ret = dev_priv -> rps . hw_max * GT_FREQUENCY_MULTIPLIER ; <nl> + ret = dev_priv -> rps . max_delay * GT_FREQUENCY_MULTIPLIER ; <nl> mutex_unlock (& dev_priv -> rps . hw_lock ); <nl>  <nl> return snprintf ( buf , PAGE_SIZE , "% d \ n ", ret );
static int __ext4_ext_check ( const char * function , unsigned int line , <nl> error_msg = " invalid extent entries "; <nl> goto corrupted ; <nl> } <nl> + if ( unlikely ( depth > 32 )) { <nl> + error_msg = " too large eh_depth "; <nl> + goto corrupted ; <nl> + } <nl> /* Verify checksum on non - root extent tree nodes */ <nl> if ( ext_depth ( inode ) != depth && <nl> ! ext4_extent_block_csum_verify ( inode , eh )) {
void igb_update_stats ( struct igb_adapter * adapter , <nl>  <nl> rcu_read_lock (); <nl> for ( i = 0 ; i < adapter -> num_rx_queues ; i ++) { <nl> - u32 rqdpc = rd32 ( E1000_RQDPC ( i )); <nl> struct igb_ring * ring = adapter -> rx_ring [ i ]; <nl> + u32 rqdpc = rd32 ( E1000_RQDPC ( i )); <nl> + if ( hw -> mac . type >= e1000_i210 ) <nl> + wr32 ( E1000_RQDPC ( i ), 0 ); <nl>  <nl> if ( rqdpc ) { <nl> ring -> rx_stats . drops += rqdpc ;
int __init irttp_init ( void ) <nl> if (! irttp -> tsaps ) { <nl> IRDA_ERROR ("% s : can ' t allocate IrTTP hashbin !\ n ", <nl> __FUNCTION__ ); <nl> + kfree ( irttp ); <nl> return - ENOMEM ; <nl> } <nl> 
visorchannel_create_guts ( u64 physaddr , unsigned long channel_bytes , <nl> int err ; <nl> size_t size = sizeof ( struct channel_header ); <nl>  <nl> + if ( physaddr == 0 ) <nl> + return NULL ; <nl> + <nl> channel = kzalloc ( sizeof (* channel ), gfp ); <nl> if (! channel ) <nl> goto cleanup ;
void * devm_memremap ( struct device * dev , resource_size_t offset , <nl> if ( addr ) { <nl> * ptr = addr ; <nl> devres_add ( dev , ptr ); <nl> - } else <nl> + } else { <nl> devres_free ( ptr ); <nl> + return ERR_PTR (- ENXIO ); <nl> + } <nl>  <nl> return addr ; <nl> }
static void ath9k_hw_get_def_gain_boundaries_pdadcs ( struct ath_hw * ah , <nl> vpdTableI [ i ][ sizeCurrVpdTable - 2 ]); <nl> vpdStep = ( int16_t )(( vpdStep < 1 ) ? 1 : vpdStep ); <nl>  <nl> - if ( tgtIndex > maxIndex ) { <nl> + if ( tgtIndex >= maxIndex ) { <nl> while (( ss <= tgtIndex ) && <nl> ( k < ( AR5416_NUM_PDADC_VALUES - 1 ))) { <nl> tmpVal = ( int16_t )(( vpdTableI [ i ][ sizeCurrVpdTable - 1 ] +
static int rtd520_probe_fifo_depth ( comedi_device * dev ) <nl> return - EIO ; <nl> } <nl> RtdAdcClearFifo ( dev ); <nl> - if ( fifo_size != 0x400 || fifo_size != 0x2000 ) <nl> + if ( fifo_size != 0x400 && fifo_size != 0x2000 ) <nl> { <nl> rt_printk ("\ ncomedi : % s : unexpected fifo size of % i , expected 1024 or 8192 .\ n ", <nl> DRV_NAME , fifo_size );
u32 __tcp_select_window ( struct sock * sk ) <nl> int full_space = min_t ( int , tp -> window_clamp , allowed_space ); <nl> int window ; <nl>  <nl> - if ( mss > full_space ) <nl> + if ( unlikely ( mss > full_space )) { <nl> mss = full_space ; <nl> - <nl> + if ( mss <= 0 ) <nl> + return 0 ; <nl> + } <nl> if ( free_space < ( full_space >> 1 )) { <nl> icsk -> icsk_ack . quick = 0 ; <nl> 
static void passdown_endio ( struct bio * bio ) <nl> * to unmap ( we ignore err ). <nl> */ <nl> queue_passdown_pt2 ( bio -> bi_private ); <nl> + bio_put ( bio ); <nl> } <nl>  <nl> static void process_prepared_discard_passdown_pt1 ( struct dm_thin_new_mapping * m )
again : <nl> } <nl> spin_unlock (& fs_info -> reada_lock ); <nl>  <nl> + kfree ( multi ); <nl> return re ; <nl>  <nl> error : <nl> error : <nl> kref_put (& zone -> refcnt , reada_zone_release ); <nl> spin_unlock (& fs_info -> reada_lock ); <nl> } <nl> + kfree ( multi ); <nl> kfree ( re ); <nl> if ( looped ) <nl> goto again ;
int simple_rename ( struct inode * old_dir , struct dentry * old_dentry , <nl>  <nl> if ( new_dentry -> d_inode ) { <nl> simple_unlink ( new_dir , new_dentry ); <nl> - if ( they_are_dirs ) <nl> + if ( they_are_dirs ) { <nl> + drop_nlink ( new_dentry -> d_inode ); <nl> drop_nlink ( old_dir ); <nl> + } <nl> } else if ( they_are_dirs ) { <nl> drop_nlink ( old_dir ); <nl> inc_nlink ( new_dir );
static int update_nodemask ( struct cpuset * cs , struct cpuset * trialcs , <nl> spin_unlock_irq (& callback_lock ); <nl>  <nl> /* use trialcs -> mems_allowed as a temp variable */ <nl> - update_nodemasks_hier ( cs , & cs -> mems_allowed ); <nl> + update_nodemasks_hier ( cs , & trialcs -> mems_allowed ); <nl> done : <nl> return retval ; <nl> }
out : <nl> } <nl>  <nl> cl_env_put ( env , & refcheck ); <nl> - return tot_bytes ? : result ; <nl> + return tot_bytes ? tot_bytes : result ; <nl> } <nl>  <nl> /**
int p9dirent_read ( char * buf , int len , struct p9_dirent * dirent , <nl> } <nl>  <nl> strcpy ( dirent -> d_name , nameptr ); <nl> + kfree ( nameptr ); <nl>  <nl> out : <nl> return fake_pdu . offset ;
static void ipsec_esp_decrypt_swauth_done ( struct device * dev , <nl> } else <nl> oicv = ( char *)& edesc -> link_tbl [ 0 ]; <nl>  <nl> - err = memcmp ( oicv , icv , authsize ) ? - EBADMSG : 0 ; <nl> + err = crypto_memneq ( oicv , icv , authsize ) ? - EBADMSG : 0 ; <nl> } <nl>  <nl> kfree ( edesc );
static int menu_select ( struct cpuidle_driver * drv , struct cpuidle_device * dev ) <nl> * We want to default to C1 ( hlt ), not to busy polling <nl> * unless the timer is happening really really soon . <nl> */ <nl> - if ( data -> next_timer_us > 20 && <nl> + if ( interactivity_req > 20 && <nl> ! drv -> states [ CPUIDLE_DRIVER_STATE_START ]. disabled && <nl> dev -> states_usage [ CPUIDLE_DRIVER_STATE_START ]. disable == 0 ) <nl> data -> last_state_idx = CPUIDLE_DRIVER_STATE_START ;
static void xhci_pci_quirks ( struct device * dev , struct xhci_hcd * xhci ) <nl> xhci -> quirks |= XHCI_RESET_ON_RESUME ; <nl> xhci_dbg ( xhci , " QUIRK : Resetting on resume \ n "); <nl> } <nl> + if ( pdev -> vendor == PCI_VENDOR_ID_VIA ) <nl> + xhci -> quirks |= XHCI_RESET_ON_RESUME ; <nl> } <nl>  <nl> /* called during probe () after chip reset completes */
static int i915_drm_thaw ( struct drm_device * dev ) <nl> drm_irq_install ( dev ); <nl>  <nl> /* Resume the modeset for every activated CRTC */ <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> drm_helper_resume_force_mode ( dev ); <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl>  <nl> if ( IS_IRONLAKE_M ( dev )) <nl> ironlake_enable_rc6 ( dev );
static int usbvision_v4l2_close ( struct file * file ) <nl> usbvision_scratch_free ( usbvision ); <nl>  <nl> usbvision -> user --; <nl> + mutex_unlock (& usbvision -> v4l2_lock ); <nl>  <nl> if ( usbvision -> remove_pending ) { <nl> printk ( KERN_INFO "% s : Final disconnect \ n ", __func__ ); <nl> usbvision_release ( usbvision ); <nl> return 0 ; <nl> } <nl> - mutex_unlock (& usbvision -> v4l2_lock ); <nl>  <nl> PDEBUG ( DBG_IO , " success "); <nl> return v4l2_fh_release ( file );
static struct cpuidle_state s3c64xx_cpuidle_set [] = { <nl> [ 0 ] = { <nl> . enter = s3c64xx_enter_idle , <nl> . exit_latency = 1 , <nl> - . target_residency = 100000 , <nl> + . target_residency = 1 , <nl> . flags = CPUIDLE_FLAG_TIME_VALID , <nl> . name = " IDLE ", <nl> . desc = " System active , ARM gated ",
int ring_buffer_resize ( struct ring_buffer * buffer , unsigned long size ) <nl> list_del_init (& page -> list ); <nl> free_buffer_page ( page ); <nl> } <nl> + mutex_unlock (& buffer -> mutex ); <nl> return - ENOMEM ; <nl> } <nl> 
static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
static int cp2112_gpio_direction_input ( struct gpio_chip * chip , unsigned offset ) <nl>  <nl> exit : <nl> mutex_unlock (& dev -> lock ); <nl> - return ret <= 0 ? ret : - EIO ; <nl> + return ret < 0 ? ret : - EIO ; <nl> } <nl>  <nl> static void cp2112_gpio_set ( struct gpio_chip * chip , unsigned offset , int value )
static int ecryptfs_write_begin ( struct file * file , <nl> && ( pos != 0 )) <nl> zero_user ( page , 0 , PAGE_CACHE_SIZE ); <nl> out : <nl> + if ( unlikely ( rc )) { <nl> + unlock_page ( page ); <nl> + page_cache_release ( page ); <nl> + * pagep = NULL ; <nl> + } <nl> return rc ; <nl> } <nl> 
static int emac_poll_rx ( void * param , int budget ) <nl> goto next ; <nl> } <nl>  <nl> + if ( len < ETH_HLEN ) { <nl> + ++ dev -> estats . rx_dropped_stack ; <nl> + emac_recycle_rx_skb ( dev , slot , len ); <nl> + goto next ; <nl> + } <nl> + <nl> if ( len && len < EMAC_RX_COPY_THRESH ) { <nl> struct sk_buff * copy_skb = <nl> alloc_skb ( len + EMAC_RX_SKB_HEADROOM + 2 , GFP_ATOMIC );
static int cdrom_ioctl_select_disc ( struct cdrom_device_info * cdi , <nl> return - ENOSYS ; <nl>  <nl> if ( arg != CDSL_CURRENT && arg != CDSL_NONE ) { <nl> - if (( int ) arg >= cdi -> capacity ) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> } <nl> 
static int rsi_send_beacon ( struct rsi_common * common ) <nl> skb_pull ( skb , ( 64 - dword_align_bytes )); <nl> if ( rsi_prepare_beacon ( common , skb )) { <nl> rsi_dbg ( ERR_ZONE , " Failed to prepare beacon \ n "); <nl> + dev_kfree_skb ( skb ); <nl> return - EINVAL ; <nl> } <nl> skb_queue_tail (& common -> tx_queue [ MGMT_BEACON_Q ], skb );
static int process_all_refs ( struct send_ctx * sctx , <nl> root = sctx -> parent_root ; <nl> cb = __record_deleted_ref ; <nl> } else { <nl> - BUG (); <nl> + btrfs_err ( sctx -> send_root -> fs_info , <nl> + " Wrong command % d in process_all_refs ", cmd ); <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> } <nl>  <nl> key . objectid = sctx -> cmp_key -> objectid ;
static int mxs_lradc_probe ( struct platform_device * pdev ) <nl> * of the array . <nl> */ <nl> scale_uv = (( u64 ) lradc -> vref_mv [ i ] * 100000000 ) >> <nl> - ( iio -> channels [ i ]. scan_type . realbits - s ); <nl> + ( LRADC_RESOLUTION - s ); <nl> lradc -> scale_avail [ i ][ s ]. nano = <nl> do_div ( scale_uv , 100000000 ) * 10 ; <nl> lradc -> scale_avail [ i ][ s ]. integer = scale_uv ;
static int moxart_probe ( struct platform_device * pdev ) <nl> goto out ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out ; <nl>  <nl> dma_cap_zero ( mask ); <nl> dma_cap_set ( DMA_SLAVE , mask );
static ssize_t sta_ht_capa_read ( struct file * file , char __user * userbuf , <nl> if ( _cond ) \ <nl> p += scnprintf ( p , sizeof ( buf )+ buf - p , "\ t " _str "\ n "); \ <nl> } while ( 0 ) <nl> - char buf [ 1024 ], * p = buf ; <nl> + char buf [ 512 ], * p = buf ; <nl> int i ; <nl> struct sta_info * sta = file -> private_data ; <nl> struct ieee80211_sta_ht_cap * htc = & sta -> sta . ht_cap ;
qlafx00_soc_cpu_reset ( scsi_qla_host_t * vha ) <nl> /* Kick in Core0 to start boot process */ <nl> QLAFX00_SET_HBA_SOC_REG ( ha , SOC_SW_RST_CONTROL_REG_CORE0 , ( 0xF00 )); <nl>  <nl> + spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> + <nl> /* Wait 10secs for soft - reset to complete . */ <nl> for ( cnt = 10 ; cnt ; cnt --) { <nl> msleep ( 1000 ); <nl> barrier (); <nl> } <nl> - spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> } <nl>  <nl> /**
static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
static int netvsc_start_xmit ( struct sk_buff * skb , struct net_device * net ) <nl> } else { <nl> /* we are shutting down or bus overloaded , just drop packet */ <nl> net -> stats . tx_dropped ++; <nl> - netvsc_xmit_completion ( packet ); <nl> + kfree ( packet ); <nl> + dev_kfree_skb_any ( skb ); <nl> } <nl>  <nl> return NETDEV_TX_OK ;
static int musb_urb_enqueue ( <nl> * we only have work to do in the former case . <nl> */ <nl> spin_lock_irqsave (& musb -> lock , flags ); <nl> - if ( hep -> hcpriv ) { <nl> + if ( hep -> hcpriv || ! next_urb ( qh )) { <nl> /* some concurrent activity submitted another urb to hep ... <nl> * odd , rare , error prone , but legal . <nl> */
static const struct soc_device_attribute gen3_soc_whitelist [] = { <nl> /* generic ones */ <nl> { . soc_id = " r8a7795 " }, <nl> { . soc_id = " r8a7796 " }, <nl> + { . soc_id = " r8a77980 " }, <nl> { . soc_id = " r8a77995 " }, <nl> { /* sentinel */ } <nl> };
static int cdrom_ioctl_drive_status ( struct cdrom_device_info * cdi , <nl> if (! CDROM_CAN ( CDC_SELECT_DISC ) || <nl> ( arg == CDSL_CURRENT || arg == CDSL_NONE )) <nl> return cdi -> ops -> drive_status ( cdi , CDSL_CURRENT ); <nl> - if ((( int ) arg >= cdi -> capacity )) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> return cdrom_slot_status ( cdi , arg ); <nl> }
static int mwifiex_pcie_event_complete ( struct mwifiex_adapter * adapter , <nl>  <nl> if (! card -> evt_buf_list [ rdptr ]) { <nl> skb_push ( skb , INTF_HEADER_LEN ); <nl> + skb_put ( skb , MAX_EVENT_SIZE - skb -> len ); <nl> + memset ( skb -> data , 0 , MAX_EVENT_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , <nl> MAX_EVENT_SIZE , <nl> PCI_DMA_FROMDEVICE ))
static int kill_something_info ( int sig , struct siginfo * info , pid_t pid ) <nl> return ret ; <nl> } <nl>  <nl> + /* - INT_MIN is undefined . Exclude this case to avoid a UBSAN warning */ <nl> + if ( pid == INT_MIN ) <nl> + return - ESRCH ; <nl> + <nl> read_lock (& tasklist_lock ); <nl> if ( pid != - 1 ) { <nl> ret = __kill_pgrp_info ( sig , info ,
static ssize_t rng_dev_read ( struct file * filp , char __user * buf , <nl> err = - EAGAIN ; <nl> if (! bytes_read && ( filp -> f_flags & O_NONBLOCK )) <nl> goto out ; <nl> + if ( bytes_read < 0 ) { <nl> + err = bytes_read ; <nl> + goto out ; <nl> + } <nl>  <nl> err = - EFAULT ; <nl> while ( bytes_read && size ) {
static void rt2800pci_txstatus_interrupt ( struct rt2x00_dev * rt2x00dev ) <nl> * Since we have only one producer and one consumer we don ' t <nl> * need to lock the kfifo . <nl> */ <nl> - for ( i = 0 ; i < rt2x00dev -> ops -> tx -> entry_num ; i ++) { <nl> + for ( i = 0 ; i < rt2x00dev -> tx -> limit ; i ++) { <nl> rt2x00mmio_register_read ( rt2x00dev , TX_STA_FIFO , & status ); <nl>  <nl> if (! rt2x00_get_field32 ( status , TX_STA_FIFO_VALID ))
static int rtl8192eu_parse_efuse ( struct rtl8xxxu_priv * priv ) <nl> raw [ i + 6 ], raw [ i + 7 ]); <nl> } <nl> } <nl> + /* <nl> + * Temporarily disable 8192eu support <nl> + */ <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
int intel_atomic_check ( struct drm_device * dev , <nl> state -> allow_modeset = false ; <nl> for ( i = 0 ; i < ncrtcs ; i ++) { <nl> struct intel_crtc * crtc = to_intel_crtc ( state -> crtcs [ i ]); <nl> + if ( crtc ) <nl> + memset (& crtc -> atomic , 0 , sizeof ( crtc -> atomic )); <nl> if ( crtc && crtc -> pipe != nuclear_pipe ) <nl> not_nuclear = true ; <nl> }
static struct parser_context * parser_init_byte_stream ( u64 addr , u32 bytes , <nl> return ctx ; <nl>  <nl> err_finish_ctx : <nl> - parser_done ( ctx ); <nl> + kfree ( ctx ); <nl> return NULL ; <nl> } <nl> 
static int create_encryption_context_from_policy ( struct inode * inode , <nl> int fscrypt_process_policy ( struct inode * inode , <nl> const struct fscrypt_policy * policy ) <nl> { <nl> + if (! inode_owner_or_capable ( inode )) <nl> + return - EACCES ; <nl> + <nl> if ( policy -> version != 0 ) <nl> return - EINVAL ; <nl> 
dcb_outp_parse ( struct nouveau_bios * bios , u8 idx , u8 * ver , u8 * len , <nl> struct dcb_output * outp ) <nl> { <nl> u16 dcb = dcb_outp ( bios , idx , ver , len ); <nl> + memset ( outp , 0x00 , sizeof (* outp )); <nl> if ( dcb ) { <nl> if (* ver >= 0x20 ) { <nl> u32 conn = nv_ro32 ( bios , dcb + 0x00 );
struct task_struct { <nl> int exit_state ; <nl> int exit_code , exit_signal ; <nl> int pdeath_signal ; /* The signal sent when the parent dies */ <nl> - unsigned int jobctl ; /* JOBCTL_ *, siglock protected */ <nl> + unsigned long jobctl ; /* JOBCTL_ *, siglock protected */ <nl>  <nl> /* Used for emulating ABI behavior of previous Linux versions */ <nl> unsigned int personality ;
static irqreturn_t dcon_interrupt ( int irq , void * id ) <nl> return IRQ_HANDLED ; <nl> } <nl>  <nl> - static struct i2c_device_id dcon_idtable [] = { <nl> + static const struct i2c_device_id dcon_idtable [] = { <nl> { " olpc_dcon ", 0 }, <nl> { } <nl> }; <nl> static int __init olpc_dcon_init ( void ) <nl> { <nl> pdata = & dcon_pdata_xo_1 ; <nl>  <nl> - i2c_add_driver (& dcon_driver ); <nl> - return 0 ; <nl> + return i2c_add_driver (& dcon_driver ); <nl> } <nl>  <nl> static void __exit olpc_dcon_exit ( void )
static int corsair_input_mapping ( struct hid_device * dev , <nl> { <nl> int gkey ; <nl>  <nl> + if (( usage -> hid & HID_USAGE_PAGE ) != HID_UP_KEYBOARD ) <nl> + return 0 ; <nl> + <nl> gkey = corsair_usage_to_gkey ( usage -> hid & HID_USAGE ); <nl> if ( gkey != 0 ) { <nl> hid_map_usage_clear ( input , usage , bit , max , EV_KEY ,
static int init_fb_chan ( struct mx3fb_data * mx3fb , struct idmac_channel * ichan ) <nl> init_completion (& mx3fbi -> flip_cmpl ); <nl> disable_irq ( ichan -> eof_irq ); <nl> dev_dbg ( mx3fb -> dev , " disabling irq % d \ n ", ichan -> eof_irq ); <nl> - ret = mx3fb_set_par ( fbi ); <nl> - if ( ret < 0 ) <nl> - goto esetpar ; <nl> - <nl> - mx3fb_blank ( FB_BLANK_UNBLANK , fbi ); <nl>  <nl> dev_info ( dev , " registered , using mode % s \ n ", fb_mode ); <nl> 
static void __init early_vmalloc ( char ** arg ) <nl> " vmalloc area too small , limiting to % luMB \ n ", <nl> vmalloc_reserve >> 20 ); <nl> } <nl> + <nl> + if ( vmalloc_reserve > VMALLOC_END - ( PAGE_OFFSET + SZ_32M )) { <nl> + vmalloc_reserve = VMALLOC_END - ( PAGE_OFFSET + SZ_32M ); <nl> + printk ( KERN_WARNING <nl> + " vmalloc area is too big , limiting to % luMB \ n ", <nl> + vmalloc_reserve >> 20 ); <nl> + } <nl> } <nl> __early_param (" vmalloc =", early_vmalloc ); <nl> 
static int s3c24xx_eint_init ( struct samsung_pinctrl_drv_data * d ) <nl> irq = bank -> eint_offset ; <nl> mask = bank -> eint_mask ; <nl> for ( pin = 0 ; mask ; ++ pin , mask >>= 1 ) { <nl> - if ( irq > NUM_EINT ) <nl> + if ( irq >= NUM_EINT ) <nl> break ; <nl> if (!( mask & 1 )) <nl> continue ;
static struct clk * of_clk_gpio_delayed_register_get ( <nl> num_parents = of_clk_get_parent_count ( data -> node ); <nl>  <nl> parent_names = kcalloc ( num_parents , sizeof ( char *), GFP_KERNEL ); <nl> - if (! parent_names ) <nl> - return ERR_PTR (- ENOMEM ); <nl> + if (! parent_names ) { <nl> + clk = ERR_PTR (- ENOMEM ); <nl> + goto out ; <nl> + } <nl>  <nl> for ( i = 0 ; i < num_parents ; i ++) <nl> parent_names [ i ] = of_clk_get_parent_name ( data -> node , i );
static int find_fsync_dnodes ( struct f2fs_sb_info * sbi , struct list_head * head ) <nl> if ( IS_ERR ( entry -> inode )) { <nl> err = PTR_ERR ( entry -> inode ); <nl> kmem_cache_free ( fsync_entry_slab , entry ); <nl> - if ( err == - ENOENT ) <nl> + if ( err == - ENOENT ) { <nl> + err = 0 ; <nl> goto next ; <nl> + } <nl> break ; <nl> } <nl> list_add_tail (& entry -> list , head );
static int __floppy_read_block_0 ( struct block_device * bdev ) <nl> bio . bi_size = size ; <nl> bio . bi_bdev = bdev ; <nl> bio . bi_sector = 0 ; <nl> + bio . bi_flags = BIO_QUIET ; <nl> init_completion (& complete ); <nl> bio . bi_private = & complete ; <nl> bio . bi_end_io = floppy_rb0_complete ;
static int e1000_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> goto err_hw_init ; <nl>  <nl> if (( adapter -> flags & FLAG_IS_ICH ) && <nl> - ( adapter -> flags & FLAG_READ_ONLY_NVM )) <nl> + ( adapter -> flags & FLAG_READ_ONLY_NVM ) && <nl> + ( hw -> mac . type < e1000_pch_spt )) <nl> e1000e_write_protect_nvm_ich8lan (& adapter -> hw ); <nl>  <nl> hw -> mac . ops . get_bus_info (& adapter -> hw );
static long aio_read_events_ring ( struct kioctx * ctx , <nl> if ( head == tail ) <nl> goto out ; <nl>  <nl> + head %= ctx -> nr_events ; <nl> + tail %= ctx -> nr_events ; <nl> + <nl> while ( ret < nr ) { <nl> long avail ; <nl> struct io_event * ev ;
MODULE_FIRMWARE (" digiface_firmware_rev11 . bin "); <nl> # define HDSP_DMA_AREA_BYTES (( HDSP_MAX_CHANNELS + 1 ) * HDSP_CHANNEL_BUFFER_BYTES ) <nl> # define HDSP_DMA_AREA_KILOBYTES ( HDSP_DMA_AREA_BYTES / 1024 ) <nl>  <nl> -/* use hotplug firmeare loader ? */ <nl> +/* use hotplug firmware loader ? */ <nl> # if defined ( CONFIG_FW_LOADER ) || defined ( CONFIG_FW_LOADER_MODULE ) <nl> -# if ! defined ( HDSP_USE_HWDEP_LOADER ) && ! defined ( CONFIG_SND_HDSP ) <nl> +# if ! defined ( HDSP_USE_HWDEP_LOADER ) <nl> # define HDSP_FW_LOADER <nl> # endif <nl> # endif
void compact_pgdat ( pg_data_t * pgdat , int order ) <nl> . sync = false , <nl> }; <nl>  <nl> + if (! order ) <nl> + return ; <nl> + <nl> __compact_pgdat ( pgdat , & cc ); <nl> } <nl> 
static void imon_incoming_packet ( struct imon_context * ictx , <nl> if ( press_type == 0 ) <nl> rc_keyup ( ictx -> rdev ); <nl> else { <nl> - if ( ictx -> rc_type == RC_BIT_RC6_MCE ) <nl> + if ( ictx -> rc_type == RC_BIT_RC6_MCE || <nl> + ictx -> rc_type == RC_BIT_OTHER ) <nl> rc_keydown ( ictx -> rdev , <nl> ictx -> rc_type == RC_BIT_RC6_MCE ? RC_TYPE_RC6_MCE : RC_TYPE_OTHER , <nl> ictx -> rc_scancode , ictx -> rc_toggle );
static int __devinit snd_vt1724_read_eeprom ( struct snd_ice1712 * ice , <nl> static void __devinit snd_vt1724_chip_reset ( struct snd_ice1712 * ice ) <nl> { <nl> outb ( VT1724_RESET , ICEREG1724 ( ice , CONTROL )); <nl> + inb ( ICEREG1724 ( ice , CONTROL )); /* pci posting flush */ <nl> msleep ( 10 ); <nl> outb ( 0 , ICEREG1724 ( ice , CONTROL )); <nl> + inb ( ICEREG1724 ( ice , CONTROL )); /* pci posting flush */ <nl> msleep ( 10 ); <nl> } <nl> 
static void slic_card_cleanup ( struct sliccard * card ) <nl> { <nl> if ( card -> loadtimerset ) { <nl> card -> loadtimerset = 0 ; <nl> - del_timer (& card -> loadtimer ); <nl> + del_timer_sync (& card -> loadtimer ); <nl> } <nl>  <nl> slic_debug_card_destroy ( card );
vc4_get_bcl ( struct drm_device * dev , struct vc4_exec_info * exec ) <nl> args -> shader_rec_count ); <nl> struct vc4_bo * bo ; <nl>  <nl> - if ( uniforms_offset < shader_rec_offset || <nl> + if ( shader_rec_offset < args -> bin_cl_size || <nl> + uniforms_offset < shader_rec_offset || <nl> exec_size < uniforms_offset || <nl> args -> shader_rec_count >= ( UINT_MAX / <nl> sizeof ( struct vc4_shader_state )) ||
static unsigned int ata_eh_speed_down ( struct ata_device * dev , <nl> */ <nl> static inline int ata_eh_worth_retry ( struct ata_queued_cmd * qc ) <nl> { <nl> - if ( qc -> flags & AC_ERR_MEDIA ) <nl> + if ( qc -> err_mask & AC_ERR_MEDIA ) <nl> return 0 ; /* don ' t retry media errors */ <nl> if ( qc -> flags & ATA_QCFLAG_IO ) <nl> return 1 ; /* otherwise retry anything from fs stack */
bool fw_download_code ( struct net_device * dev , u8 * code_virtual_address , u32 buff <nl> * add 4 to avoid packet appending overflow . <nl> * */ <nl> skb = dev_alloc_skb ( USB_HWDESC_HEADER_LEN + frag_length + 4 ); <nl> + if (! skb ) <nl> + return false ; <nl> memcpy (( unsigned char *)( skb -> cb ),& dev , sizeof ( dev )); <nl> tcb_desc = ( cb_desc *)( skb -> cb + MAX_DEV_ADDR_SIZE ); <nl> tcb_desc -> queue_index = TXCMD_QUEUE ;
static int handle_emulation_failure ( struct kvm_vcpu * vcpu ) <nl>  <nl> ++ vcpu -> stat . insn_emulation_fail ; <nl> trace_kvm_emulate_insn_failed ( vcpu ); <nl> - if (! is_guest_mode ( vcpu )) { <nl> + if (! is_guest_mode ( vcpu ) && kvm_x86_ops -> get_cpl ( vcpu ) == 0 ) { <nl> vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> vcpu -> run -> internal . ndata = 0 ;
static int meson_mmc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> dev_err (& pdev -> dev , " failed to get interrupt resource .\ n "); <nl> ret = - EINVAL ; <nl> goto free_host ;
int vbd_create ( blkif_t * blkif , blkif_vdev_t handle , unsigned major , <nl>  <nl> vbd -> pdevice = MKDEV ( major , minor ); <nl>  <nl> - bdev = open_by_devnum ( vbd -> pdevice , <nl> - vbd -> readonly ? FMODE_READ : FMODE_WRITE ); <nl> + bdev = blkdev_get_by_dev ( vbd -> pdevice , vbd -> readonly ? <nl> + FMODE_READ : FMODE_WRITE , NULL ); <nl>  <nl> if ( IS_ERR ( bdev )) { <nl> DPRINTK (" vbd_creat : device % 08x could not be opened .\ n ",
int add_mtd_blktrans_dev ( struct mtd_blktrans_dev * new ) <nl> new -> rq -> queuedata = new ; <nl> blk_queue_logical_block_size ( new -> rq , tr -> blksize ); <nl>  <nl> - if ( tr -> discard ) <nl> - queue_flag_set_unlocked ( QUEUE_FLAG_DISCARD , <nl> - new -> rq ); <nl> + if ( tr -> discard ) { <nl> + queue_flag_set_unlocked ( QUEUE_FLAG_DISCARD , new -> rq ); <nl> + new -> rq -> limits . max_discard_sectors = UINT_MAX ; <nl> + } <nl>  <nl> gd -> queue = new -> rq ; <nl> 
int ptrace_setxregs ( struct task_struct * child , void __user * uregs ) <nl> elf_xtregs_t * xtregs = uregs ; <nl> int ret = 0 ; <nl>  <nl> + if (! access_ok ( VERIFY_READ , uregs , sizeof ( elf_xtregs_t ))) <nl> + return - EFAULT ; <nl> + <nl> # if XTENSA_HAVE_COPROCESSORS <nl> /* Flush all coprocessors before we overwrite them . */ <nl> coprocessor_flush_all ( ti );
static inline u8 rc5_data ( struct rc_map_table * key ) <nl> return key -> scancode & 0xff ; <nl> } <nl>  <nl> - static inline u8 rc5_scan ( struct rc_map_table * key ) <nl> + static inline u16 rc5_scan ( struct rc_map_table * key ) <nl> { <nl> return key -> scancode & 0xffff ; <nl> }
device_create_groups_vargs ( struct class * class , struct device * parent , <nl> goto error ; <nl> } <nl>  <nl> + device_initialize ( dev ); <nl> dev -> devt = devt ; <nl> dev -> class = class ; <nl> dev -> parent = parent ; <nl> device_create_groups_vargs ( struct class * class , struct device * parent , <nl> if ( retval ) <nl> goto error ; <nl>  <nl> - retval = device_register ( dev ); <nl> + retval = device_add ( dev ); <nl> if ( retval ) <nl> goto error ; <nl> 
static struct page * get_partial ( struct kmem_cache * s , gfp_t flags , int node ) <nl> int searchnode = ( node == NUMA_NO_NODE ) ? numa_node_id () : node ; <nl>  <nl> page = get_partial_node ( get_node ( s , searchnode )); <nl> - if ( page || node != - 1 ) <nl> + if ( page || node != NUMA_NO_NODE ) <nl> return page ; <nl>  <nl> return get_any_partial ( s , flags );
static struct omap_system_dma_plat_info dma_plat_info __initdata = { <nl> . dma_read = dma_read , <nl> }; <nl>  <nl> - static struct platform_device_info omap_dma_dev_info = { <nl> + static struct platform_device_info omap_dma_dev_info __initdata = { <nl> . name = " omap - dma - engine ", <nl> . id = - 1 , <nl> . dma_mask = DMA_BIT_MASK ( 32 ),
static const struct snd_pci_quirk alc662_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x1025 , 0x038b , " Acer Aspire 8943G ", ALC662_FIXUP_ASPIRE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05d8 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05db , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> + SND_PCI_QUIRK ( 0x1028 , 0x0626 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x103c , 0x1632 , " HP RP5800 ", ALC662_FIXUP_HP_RP5800 ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1477 , " ASUS N56VZ ", ALC662_FIXUP_BASS_CHMAP ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1bf3 , " ASUS N76VZ ", ALC662_FIXUP_BASS_CHMAP ),
static int vfio_pci_mmap ( void * device_data , struct vm_area_struct * vma ) <nl> return ret ; <nl>  <nl> vdev -> barmap [ index ] = pci_iomap ( pdev , index , 0 ); <nl> + if (! vdev -> barmap [ index ]) { <nl> + pci_release_selected_regions ( pdev , 1 << index ); <nl> + return - ENOMEM ; <nl> + } <nl> } <nl>  <nl> vma -> vm_private_data = vdev ;
static void _tcpm_cc_change ( struct tcpm_port * port , enum typec_cc_status cc1 , <nl> break ; <nl>  <nl> case SRC_TRY : <nl> - tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> + if ( tcpm_port_is_source ( port )) <nl> + tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> break ; <nl> case SRC_TRY_DEBOUNCE : <nl> tcpm_set_state ( port , SRC_TRY , 0 );
sg_build_indirect ( Sg_scatter_hold * schp , Sg_fd * sfp , int buff_size ) <nl> num = ( rem_sz > scatter_elem_sz_prev ) ? <nl> scatter_elem_sz_prev : rem_sz ; <nl>  <nl> - schp -> pages [ k ] = alloc_pages ( gfp_mask , order ); <nl> + schp -> pages [ k ] = alloc_pages ( gfp_mask | __GFP_ZERO , order ); <nl> if (! schp -> pages [ k ]) <nl> goto out ; <nl> 
__hwmon_device_register ( struct device * dev , const char * name , void * drvdata , <nl> if ( err ) <nl> goto free_hwmon ; <nl>  <nl> - if ( chip && chip -> ops -> read && <nl> + if ( dev && chip && chip -> ops -> read && <nl> chip -> info [ 0 ]-> type == hwmon_chip && <nl> ( chip -> info [ 0 ]-> config [ 0 ] & HWMON_C_REGISTER_TZ )) { <nl> const struct hwmon_channel_info ** info = chip -> info ;
static void kick_requests ( struct ceph_osd_client * osdc , int force_resend ) <nl> __register_request ( osdc , req ); <nl> __unregister_linger_request ( osdc , req ); <nl> } <nl> + reset_changed_osds ( osdc ); <nl> mutex_unlock (& osdc -> request_mutex ); <nl>  <nl> if ( needmap ) { <nl> dout ("% d requests for down osds , need new map \ n ", needmap ); <nl> ceph_monc_request_next_osdmap (& osdc -> client -> monc ); <nl> } <nl> - reset_changed_osds ( osdc ); <nl> } <nl>  <nl> 
static void sixpack_close ( struct tty_struct * tty ) <nl> */ <nl> netif_stop_queue ( sp -> dev ); <nl>  <nl> + unregister_netdev ( sp -> dev ); <nl> + <nl> del_timer_sync (& sp -> tx_t ); <nl> del_timer_sync (& sp -> resync_t ); <nl>  <nl> - unregister_netdev ( sp -> dev ); <nl> - <nl> /* Free all 6pack frame buffers after unreg . */ <nl> kfree ( sp -> rbuff ); <nl> kfree ( sp -> xbuff );
# define MPT2SAS_DRIVER_NAME " mpt2sas " <nl> # define MPT2SAS_AUTHOR " LSI Corporation < DL - MPTFusionLinux @ lsi . com >" <nl> # define MPT2SAS_DESCRIPTION " LSI MPT Fusion SAS 2 . 0 Device Driver " <nl> -# define MPT2SAS_DRIVER_VERSION " 07 . 100 . 00 . 00 " <nl> -# define MPT2SAS_MAJOR_VERSION 07 <nl> +# define MPT2SAS_DRIVER_VERSION " 08 . 100 . 00 . 00 " <nl> +# define MPT2SAS_MAJOR_VERSION 08 <nl> # define MPT2SAS_MINOR_VERSION 100 <nl> # define MPT2SAS_BUILD_VERSION 00 <nl> # define MPT2SAS_RELEASE_VERSION 00
irqreturn_t uic_cascade ( int virq , void * data ) <nl> int subvirq ; <nl>  <nl> msr = mfdcr ( uic -> dcrbase + UIC_MSR ); <nl> + if (! msr ) /* spurious interrupt */ <nl> + return IRQ_HANDLED ; <nl> + <nl> src = 32 - ffs ( msr ); <nl>  <nl> subvirq = irq_linear_revmap ( uic -> irqhost , src );
static int pfkey_register ( struct sock * sk , struct sk_buff * skb , const struct sad <nl>  <nl> xfrm_probe_algs (); <nl>  <nl> - supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL ); <nl> + supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL | __GFP_ZERO ); <nl> if (! supp_skb ) { <nl> if ( hdr -> sadb_msg_satype != SADB_SATYPE_UNSPEC ) <nl> pfk -> registered &= ~( 1 << hdr -> sadb_msg_satype );
static void flush_tmregs_to_thread ( struct task_struct * tsk ) <nl> * in the appropriate thread structures from live . <nl> */ <nl>  <nl> - if ( tsk != current ) <nl> + if ((! cpu_has_feature ( CPU_FTR_TM )) || ( tsk != current )) <nl> return ; <nl>  <nl> if ( MSR_TM_SUSPENDED ( mfmsr ())) {
static void rs_free_sta ( void * priv_r , struct ieee80211_sta * sta , <nl> void * priv_sta ) <nl> { <nl> struct iwl_lq_sta * lq_sta = priv_sta ; <nl> - struct iwl_priv * priv = priv_r ; <nl> + struct iwl_priv * priv __maybe_unused = priv_r ; <nl>  <nl> IWL_DEBUG_RATE (" enter \ n "); <nl> kfree ( lq_sta );
static inline int xfrm_replay_verify_len ( struct xfrm_replay_state_esn * replay_es <nl> if ( nla_len ( rp ) < ulen || xfrm_replay_state_esn_len ( replay_esn ) != ulen ) <nl> return - EINVAL ; <nl>  <nl> + if ( up -> replay_window > up -> bmp_len * sizeof ( __u32 ) * 8 ) <nl> + return - EINVAL ; <nl> + <nl> return 0 ; <nl> } <nl> 
aoedev_freedev ( struct aoedev * d ) <nl> put_disk ( d -> gd ); <nl> } <nl> kfree ( d -> frames ); <nl> - mempool_destroy ( d -> bufpool ); <nl> + if ( d -> bufpool ) <nl> + mempool_destroy ( d -> bufpool ); <nl> kfree ( d ); <nl> } <nl> 
static int imx2_wdt_set_timeout ( struct watchdog_device * wdog , <nl> { <nl> struct imx2_wdt_device * wdev = watchdog_get_drvdata ( wdog ); <nl>  <nl> + wdog -> timeout = new_timeout ; <nl> + <nl> regmap_update_bits ( wdev -> regmap , IMX2_WDT_WCR , IMX2_WDT_WCR_WT , <nl> WDOG_SEC_TO_COUNT ( new_timeout )); <nl> return 0 ;
static int soc_tplg_dapm_widget_create ( struct soc_tplg * tplg , <nl> widget -> dobj . type = SND_SOC_DOBJ_WIDGET ; <nl> widget -> dobj . ops = tplg -> ops ; <nl> widget -> dobj . index = tplg -> index ; <nl> + kfree ( template . sname ); <nl> + kfree ( template . name ); <nl> list_add (& widget -> dobj . list , & tplg -> comp -> dobj_list ); <nl> return 0 ; <nl> 
i915_gem_wait_ioctl ( struct drm_device * dev , void * data , struct drm_file * file ) <nl> u32 seqno = 0 ; <nl> int ret = 0 ; <nl>  <nl> + if ( args -> flags != 0 ) <nl> + return - EINVAL ; <nl> + <nl> ret = i915_mutex_lock_interruptible ( dev ); <nl> if ( ret ) <nl> return ret ;
struct snd_kcontrol * snd_soc_cnew ( const struct snd_kcontrol_new * _template , <nl>  <nl> if ( prefix ) { <nl> name_len = strlen ( long_name ) + strlen ( prefix ) + 2 ; <nl> - name = kmalloc ( name_len , GFP_ATOMIC ); <nl> + name = kmalloc ( name_len , GFP_KERNEL ); <nl> if (! name ) <nl> return NULL ; <nl> 
static const struct labpc_board_struct labpc_cs_boards [] = { <nl> }, <nl> }; <nl>  <nl> -/* <nl> - * Useful for shorthand access to the particular board structure <nl> - */ <nl> -# define thisboard (( const struct labpc_board_struct *) dev -> board_ptr ) <nl> - <nl> static int labpc_attach ( struct comedi_device * dev , struct comedi_devconfig * it ) <nl> { <nl> + const struct labpc_board_struct * thisboard = comedi_board ( dev ); <nl> struct labpc_private * devpriv ; <nl> unsigned long iobase = 0 ; <nl> unsigned int irq = 0 ;
static void ide_complete_power_step ( ide_drive_t * drive , struct request * rq , u8 s <nl>  <nl> switch ( rq -> pm -> pm_step ) { <nl> case ide_pm_flush_cache : /* Suspend step 1 ( flush cache ) complete */ <nl> - if ( rq -> pm -> pm_state == 4 ) <nl> + if ( rq -> pm -> pm_state == PM_EVENT_FREEZE ) <nl> rq -> pm -> pm_step = ide_pm_state_completed ; <nl> else <nl> rq -> pm -> pm_step = idedisk_pm_standby ;
static int _mei_irq_thread_read ( struct mei_device * dev , s32 * slots , <nl> struct mei_cl * cl , <nl> struct mei_io_list * cmpl_list ) <nl> { <nl> - if ((* slots * sizeof ( u32 )) >= ( sizeof ( struct mei_msg_hdr ) + <nl> + if ((* slots * sizeof ( u32 )) < ( sizeof ( struct mei_msg_hdr ) + <nl> sizeof ( struct hbm_flow_control ))) { <nl> /* return the cancel routine */ <nl> list_del (& cb_pos -> cb_list );
void gen8_fbc_sw_flush ( struct drm_device * dev , u32 value ) <nl> if (! IS_GEN8 ( dev )) <nl> return ; <nl>  <nl> + if (! intel_fbc_enabled ( dev )) <nl> + return ; <nl> + <nl> I915_WRITE ( MSG_FBC_REND_STATE , value ); <nl> } <nl> 
static void __ipr_remove ( struct pci_dev * pdev ) <nl> spin_unlock_irqrestore ( ioa_cfg -> host -> host_lock , host_lock_flags ); <nl> wait_event ( ioa_cfg -> reset_wait_q , ! ioa_cfg -> in_reset_reload ); <nl> flush_work (& ioa_cfg -> work_q ); <nl> + INIT_LIST_HEAD (& ioa_cfg -> used_res_q ); <nl> spin_lock_irqsave ( ioa_cfg -> host -> host_lock , host_lock_flags ); <nl>  <nl> spin_lock (& ipr_driver_lock );
static int __devexit gpio_keys_remove ( struct platform_device * pdev ) <nl> } <nl>  <nl> input_unregister_device ( input ); <nl> + kfree ( ddata ); <nl>  <nl> return 0 ; <nl> }
int mdiobus_register ( struct mii_bus * bus ) <nl> err = device_register (& bus -> dev ); <nl> if ( err ) { <nl> pr_err (" mii_bus % s failed to register \ n ", bus -> id ); <nl> + put_device (& bus -> dev ); <nl> return - EINVAL ; <nl> } <nl> 
static void set_pool_mode ( struct pool * pool , enum pool_mode new_mode ) <nl> case PM_WRITE : <nl> if ( old_mode != new_mode ) <nl> notify_of_pool_mode_change ( pool , " write "); <nl> + pool -> pf . error_if_no_space = pt -> requested_pf . error_if_no_space ; <nl> dm_pool_metadata_read_write ( pool -> pmd ); <nl> pool -> process_bio = process_bio ; <nl> pool -> process_discard = process_discard_bio ;
static void * skd_alloc_dma ( struct skd_device * skdev , struct kmem_cache * s , <nl> return NULL ; <nl> * dma_handle = dma_map_single ( dev , buf , s -> size , dir ); <nl> if ( dma_mapping_error ( dev , * dma_handle )) { <nl> - kfree ( buf ); <nl> + kmem_cache_free ( s , buf ); <nl> buf = NULL ; <nl> } <nl> return buf ;
xfs_getbmap ( <nl> break ; <nl> } <nl>  <nl> + kmem_free ( out ); <nl> return error ; <nl> } <nl> 
etrax_ethernet_init ( void ) <nl> * does not share cacheline with any other data ( to avoid cache bug ) <nl> */ <nl> RxDescList [ i ]. skb = dev_alloc_skb ( MAX_MEDIA_DATA_SIZE + 2 * L1_CACHE_BYTES ); <nl> + if (! RxDescList [ i ]. skb ) <nl> + return - ENOMEM ; <nl> RxDescList [ i ]. descr . ctrl = 0 ; <nl> RxDescList [ i ]. descr . sw_len = MAX_MEDIA_DATA_SIZE ; <nl> RxDescList [ i ]. descr . next = virt_to_phys (& RxDescList [ i + 1 ]);
static long kvm_dev_ioctl ( struct file * filp , <nl> num_msrs_to_save * sizeof ( u32 ))) <nl> goto out ; <nl> r = 0 ; <nl> + break ; <nl> } <nl> default : <nl> ;
nfs4_preprocess_seqid_op ( struct svc_fh * current_fh , u32 seqid , stateid_t * statei <nl> printk (" NFSD : preprocess_seqid_op : old stateid !\ n "); <nl> goto out ; <nl> } <nl> - /* XXX renew the client lease here */ <nl> + renew_client ( sop -> so_client ); <nl> status = nfs_ok ; <nl>  <nl> out :
tda998x_encoder_init ( struct i2c_client * client , <nl>  <nl> priv -> current_page = 0xff ; <nl> priv -> cec = i2c_new_dummy ( client -> adapter , 0x34 ); <nl> - if (! priv -> cec ) <nl> + if (! priv -> cec ) { <nl> + kfree ( priv ); <nl> return - ENODEV ; <nl> + } <nl> priv -> dpms = DRM_MODE_DPMS_OFF ; <nl>  <nl> encoder_slave -> slave_priv = priv ;
struct pci_bus * pci_acpi_scan_root ( struct acpi_pci_root * root ) <nl> return NULL ; <nl>  <nl> root_ops = kzalloc_node ( sizeof (* root_ops ), GFP_KERNEL , node ); <nl> - if (! root_ops ) <nl> + if (! root_ops ) { <nl> + kfree ( ri ); <nl> return NULL ; <nl> + } <nl>  <nl> ri -> cfg = pci_acpi_setup_ecam_mapping ( root ); <nl> if (! ri -> cfg ) {
static void tg3_skb_error_unmap ( struct tg3_napi * tnapi , <nl> dma_unmap_addr ( txb , mapping ), <nl> skb_headlen ( skb ), <nl> PCI_DMA_TODEVICE ); <nl> - for ( i = 0 ; i <= last ; i ++) { <nl> + for ( i = 0 ; i < last ; i ++) { <nl> skb_frag_t * frag = & skb_shinfo ( skb )-> frags [ i ]; <nl>  <nl> entry = NEXT_TX ( entry );
static inline void tcp_check_send_head ( struct sock * sk , struct sk_buff * skb_unli <nl> { <nl> if ( sk -> sk_send_head == skb_unlinked ) <nl> sk -> sk_send_head = NULL ; <nl> + if ( tcp_sk ( sk )-> highest_sack == skb_unlinked ) <nl> + tcp_sk ( sk )-> highest_sack = NULL ; <nl> } <nl>  <nl> static inline void tcp_init_send_head ( struct sock * sk )
void perf_callchain_user ( struct perf_callchain_entry * entry , <nl> return ; <nl> } <nl>  <nl> + perf_callchain_store ( entry , regs -> pc ); <nl> tail = ( struct frame_tail __user *) regs -> regs [ 29 ]; <nl>  <nl> while ( entry -> nr < PERF_MAX_STACK_DEPTH &&
static int check_hw_params_convention ( struct snd_usb_substream * subs ) <nl>  <nl> channels = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> rates = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> + if (! channels || ! rates ) <nl> + goto __out ; <nl>  <nl> list_for_each ( p , & subs -> fmt_list ) { <nl> struct audioformat * f ;
static u32 __init allocate_aperture ( void ) <nl> printk (" Cannot allocate aperture memory hole (% p ,% uK )\ n ", <nl> p , aper_size >> 10 ); <nl> if ( p ) <nl> - free_bootmem_node ( nd0 , ( unsigned long ) p , aper_size ); <nl> + free_bootmem_node ( nd0 , __pa ( p ), aper_size ); <nl> return 0 ; <nl> } <nl> printk (" Mapping aperture over % d KB of RAM @ % lx \ n ",
static struct sk_buff * udp6_ufo_fragment ( struct sk_buff * skb , <nl>  <nl> /* Check if there is enough headroom to insert fragment header . */ <nl> tnl_hlen = skb_tnl_header_len ( skb ); <nl> - if ( skb_headroom ( skb ) < ( tnl_hlen + frag_hdr_sz )) { <nl> + if ( skb -> mac_header < ( tnl_hlen + frag_hdr_sz )) { <nl> if ( gso_pskb_expand_head ( skb , tnl_hlen + frag_hdr_sz )) <nl> goto out ; <nl> }
static struct xhci_container_ctx * xhci_alloc_container_ctx ( struct xhci_hcd * xhci <nl> ctx -> size += CTX_SIZE ( xhci -> hcc_params ); <nl>  <nl> ctx -> bytes = dma_pool_alloc ( xhci -> device_pool , flags , & ctx -> dma ); <nl> + if (! ctx -> bytes ) { <nl> + kfree ( ctx ); <nl> + return NULL ; <nl> + } <nl> memset ( ctx -> bytes , 0 , ctx -> size ); <nl> return ctx ; <nl> }
static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> btrfs_wait_ordered_range ( src , off , len ); <nl> } <nl>  <nl> + /* truncate page cache pages from target inode range */ <nl> + truncate_inode_pages_range (& inode -> i_data , off , <nl> + ALIGN ( off + len , PAGE_CACHE_SIZE ) - 1 ); <nl> + <nl> /* clone data */ <nl> key . objectid = btrfs_ino ( src ); <nl> key . type = BTRFS_EXTENT_DATA_KEY ;
static void bpf_tcp_release ( struct sock * sk ) <nl> psock -> cork = NULL ; <nl> } <nl>  <nl> - sk -> sk_prot = psock -> sk_proto ; <nl> - psock -> sk_proto = NULL ; <nl> + if ( psock -> sk_proto ) { <nl> + sk -> sk_prot = psock -> sk_proto ; <nl> + psock -> sk_proto = NULL ; <nl> + } <nl> out : <nl> rcu_read_unlock (); <nl> }
static int rtc_dev_ioctl ( struct inode * inode , struct file * file , <nl> break ; <nl>  <nl> case RTC_PIE_ON : <nl> - if (! capable ( CAP_SYS_RESOURCE )) <nl> + if ( rtc -> irq_freq > rtc -> max_user_freq && <nl> + ! capable ( CAP_SYS_RESOURCE )) <nl> return - EACCES ; <nl> break ; <nl> }
pxa3xx_gcu_write ( struct file * file , const char * buff , <nl> struct pxa3xx_gcu_batch * buffer ; <nl> struct pxa3xx_gcu_priv * priv = to_pxa3xx_gcu_priv ( file ); <nl>  <nl> - int words = count / 4 ; <nl> + size_t words = count / 4 ; <nl>  <nl> /* Does not need to be atomic . There ' s a lock in user space , <nl> * but anyhow , this is just for statistics . */
static inline int __is_running ( const struct exynos_mipi_phy_desc * data , <nl> struct exynos_mipi_video_phy * state ) <nl> { <nl> u32 val ; <nl> + int ret ; <nl> + <nl> + ret = regmap_read ( state -> regmaps [ data -> resetn_map ], data -> resetn_reg , & val ); <nl> + if ( ret ) <nl> + return 0 ; <nl>  <nl> - regmap_read ( state -> regmaps [ data -> resetn_map ], data -> resetn_reg , & val ); <nl> return val & data -> resetn_val ; <nl> } <nl> 
static int macronix_quad_enable ( struct spi_nor * nor ) <nl> val = read_sr ( nor ); <nl> if ( val < 0 ) <nl> return val ; <nl> + if ( val & SR_QUAD_EN_MX ) <nl> + return 0 ; <nl> + <nl> write_enable ( nor ); <nl>  <nl> write_sr ( nor , val | SR_QUAD_EN_MX );
static int mfd_add_device ( struct platform_device * parent , <nl> if ( ret ) <nl> goto fail_device ; <nl>  <nl> - memzero ( res , sizeof ( res )); <nl> + memset ( res , 0 , sizeof ( res )); <nl> for ( r = 0 ; r < cell -> num_resources ; r ++) { <nl> res [ r ]. name = cell -> resources [ r ]. name ; <nl> res [ r ]. flags = cell -> resources [ r ]. flags ;
static int s3c2410_udc_ep_enable ( struct usb_ep * _ep , <nl>  <nl> ep = to_s3c2410_ep ( _ep ); <nl>  <nl> - if (! _ep || ! desc || ep -> ep . desc <nl> + if (! _ep || ! desc <nl> || _ep -> name == ep0name <nl> || desc -> bDescriptorType != USB_DT_ENDPOINT ) <nl> return - EINVAL ;
static int acp_dma_hw_params ( struct snd_pcm_substream * substream , <nl> if ( WARN_ON (! rtd )) <nl> return - EINVAL ; <nl>  <nl> - rtd -> i2s_instance = pinfo -> i2s_instance ; <nl> + if ( pinfo ) <nl> + rtd -> i2s_instance = pinfo -> i2s_instance ; <nl> if ( adata -> asic_type == CHIP_STONEY ) { <nl> val = acp_reg_read ( adata -> acp_mmio , <nl> mmACP_I2S_16BIT_RESOLUTION_EN );
static int dwc3_gadget_init_hw_endpoints ( struct dwc3 * dwc , <nl>  <nl> dep -> endpoint . name = dep -> name ; <nl>  <nl> + dev_vdbg ( dwc -> dev , " initializing % s \ n ", dep -> name ); <nl> + <nl> if ( epnum == 0 || epnum == 1 ) { <nl> dep -> endpoint . maxpacket = 512 ; <nl> dep -> endpoint . maxburst = 1 ;
static inline void * kvmalloc_array ( size_t n , size_t size , gfp_t flags ) <nl> return kvmalloc ( bytes , flags ); <nl> } <nl>  <nl> + static inline void * kvcalloc ( size_t n , size_t size , gfp_t flags ) <nl> +{ <nl> + return kvmalloc_array ( n , size , flags | __GFP_ZERO ); <nl> +} <nl> + <nl> extern void kvfree ( const void * addr ); <nl>  <nl> static inline atomic_t * compound_mapcount_ptr ( struct page * page )
static int btrfs_ioctl_setflags ( struct file * file , void __user * arg ) <nl> goto out_drop ; <nl>  <nl> } else { <nl> + ret = btrfs_set_prop ( inode , " btrfs . compression ", NULL , 0 , 0 ); <nl> + if ( ret && ret != - ENODATA ) <nl> + goto out_drop ; <nl> ip -> flags &= ~( BTRFS_INODE_COMPRESS | BTRFS_INODE_NOCOMPRESS ); <nl> } <nl> 
static const char * const mic_bias_level_text [] = { <nl> }; <nl>  <nl> static const struct soc_enum mic_bias_level_enum = <nl> - SOC_ENUM_SINGLE ( CS42L52_IFACE_CTL1 , 0 , <nl> + SOC_ENUM_SINGLE ( CS42L52_IFACE_CTL2 , 0 , <nl> ARRAY_SIZE ( mic_bias_level_text ), mic_bias_level_text ); <nl>  <nl> static const char * const cs42l52_mic_text [] = { " Single ", " Differential " };
static int omap_i2c_init ( struct omap_i2c_dev * dev ) <nl> * to get longer filter period for better noise suppression . <nl> * The filter is iclk ( fclk for HS ) period . <nl> */ <nl> - if ( dev -> speed > 400 || cpu_is_omap_2430 ()) <nl> + if ( dev -> speed > 400 || cpu_is_omap2430 ()) <nl> internal_clk = 19200 ; <nl> else if ( dev -> speed > 100 ) <nl> internal_clk = 9600 ;
typedef struct xfs_attr_list_context { <nl> struct attrlist_cursor_kern * cursor ; /* position in list */ <nl> char * alist ; /* output buffer */ <nl> int seen_enough ; /* T / F : seen enough of list ? */ <nl> - int count ; /* num used entries */ <nl> + ssize_t count ; /* num used entries */ <nl> int dupcnt ; /* count dup hashvals seen */ <nl> int bufsize ; /* total buffer size */ <nl> int firstu ; /* first used byte in buffer */
struct md_op_data * ll_prep_md_op_data ( struct md_op_data * op_data , <nl> op_data -> op_default_stripe_offset = - 1 ; <nl> if ( S_ISDIR ( i1 -> i_mode )) { <nl> op_data -> op_mea1 = ll_i2info ( i1 )-> lli_lsm_md ; <nl> - op_data -> op_default_stripe_offset = <nl> - ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> + if ( opc == LUSTRE_OPC_MKDIR ) <nl> + op_data -> op_default_stripe_offset = <nl> + ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> } <nl>  <nl> if ( i2 ) {
static struct snd_soc_dai_driver ipq806x_lpass_cpu_dai_driver = { <nl>  <nl> static int ipq806x_lpass_alloc_dma_channel ( struct lpass_data * drvdata , int dir ) <nl> { <nl> - return IPQ806X_LPAIF_RDMA_CHAN_MI2S ; <nl> + if ( dir == SNDRV_PCM_STREAM_PLAYBACK ) <nl> + return IPQ806X_LPAIF_RDMA_CHAN_MI2S ; <nl> + else /* Capture currently not implemented */ <nl> + return - EINVAL ; <nl> } <nl>  <nl> static int ipq806x_lpass_free_dma_channel ( struct lpass_data * drvdata , int chan )
static int palmas_i2c_probe ( struct i2c_client * i2c , <nl> ret = - ENOMEM ; <nl> goto err ; <nl> } <nl> + palmas -> i2c_clients [ i ]-> dev . of_node = of_node_get ( node ); <nl> } <nl> palmas -> regmap [ i ] = devm_regmap_init_i2c ( palmas -> i2c_clients [ i ], <nl> & palmas_regmap_config [ i ]);
int afs_prepare_write ( struct file * file , struct page * page , <nl> _leave (" = % d [ prep ]", ret ); <nl> return ret ; <nl> } <nl> - SetPageUptodate ( page ); <nl> } <nl>  <nl> try_again : <nl> int afs_commit_write ( struct file * file , struct page * page , <nl> spin_unlock (& vnode -> writeback_lock ); <nl> } <nl>  <nl> + SetPageUptodate ( page ); <nl> set_page_dirty ( page ); <nl> - <nl> if ( PageDirty ( page )) <nl> _debug (" dirtied "); <nl> 
int inet6_csk_bind_conflict ( const struct sock * sk , <nl> if ( ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> break ; <nl> } <nl> + if (! relax && reuse && sk2 -> sk_reuse && <nl> + sk2 -> sk_state != TCP_LISTEN && <nl> + ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> + break ; <nl> } <nl> } <nl> 
static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
int btrfs_defrag_file ( struct inode * inode , struct file * file , <nl> i = range -> start >> PAGE_CACHE_SHIFT ; <nl> } <nl> if (! max_to_defrag ) <nl> - max_to_defrag = last_index - 1 ; <nl> + max_to_defrag = last_index ; <nl>  <nl> while ( i <= last_index && defrag_count < max_to_defrag ) { <nl> /*
static int greybus_remove ( struct device * dev ) <nl> gb_connection_disable_rx ( connection ); <nl>  <nl> driver -> disconnect ( bundle ); <nl> + <nl> + /* Catch buggy drivers that fail to disable their connections . */ <nl> + list_for_each_entry ( connection , & bundle -> connections , bundle_links ) { <nl> + if ( WARN_ON ( connection -> state != GB_CONNECTION_STATE_DISABLED )) <nl> + gb_connection_disable ( connection ); <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
static int check_leaf ( struct fib_table * tb , struct trie * t , struct leaf * l , <nl>  <nl> if ( fa -> fa_tos && fa -> fa_tos != flp -> flowi4_tos ) <nl> continue ; <nl> + if ( fi -> fib_dead ) <nl> + continue ; <nl> if ( fa -> fa_info -> fib_scope < flp -> flowi4_scope ) <nl> continue ; <nl> fib_alias_accessed ( fa );
static int snd_echo_resume ( struct pci_dev * pci ) <nl> DE_INIT ((" resume start \ n ")); <nl> pci_restore_state ( pci ); <nl> commpage_bak = kmalloc ( sizeof ( struct echoaudio ), GFP_KERNEL ); <nl> + if ( commpage_bak == NULL ) <nl> + return - ENOMEM ; <nl> commpage = chip -> comm_page ; <nl> memcpy ( commpage_bak , commpage , sizeof ( struct comm_page )); <nl> 
static int falcon_spi_device_init ( struct efx_nic * efx , <nl> struct efx_spi_device * spi_device ; <nl>  <nl> if ( device_type != 0 ) { <nl> - spi_device = kmalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> + spi_device = kzalloc ( sizeof (* spi_device ), GFP_KERNEL ); <nl> if (! spi_device ) <nl> return - ENOMEM ; <nl> spi_device -> device_id = device_id ;
static struct dma_chan * k3_of_dma_simple_xlate ( struct of_phandle_args * dma_spec , <nl> struct k3_dma_dev * d = ofdma -> of_dma_data ; <nl> unsigned int request = dma_spec -> args [ 0 ]; <nl>  <nl> - if ( request > d -> dma_requests ) <nl> + if ( request >= d -> dma_requests ) <nl> return NULL ; <nl>  <nl> return dma_get_slave_channel (&( d -> chans [ request ]. vc . chan ));
static struct mfd_cell max77686_devs [] = { <nl> { . name = " max77686 - pmic ", }, <nl> { . name = " max77686 - rtc ", }, <nl> + { . name = " max77686 - clk ", }, <nl> }; <nl>  <nl> static struct regmap_config max77686_regmap_config = {
static inline void mtd_erase_callback ( struct erase_info * instr ) <nl> printk ( KERN_INFO args ); \ <nl> } while ( 0 ) <nl> # else /* CONFIG_MTD_DEBUG */ <nl> -# define DEBUG ( n , args ...) do { } while ( 0 ) <nl> +# define DEBUG ( n , args ...) \ <nl> + do { \ <nl> + if ( 0 ) \ <nl> + printk ( KERN_INFO args ); \ <nl> + } while ( 0 ) <nl>  <nl> # endif /* CONFIG_MTD_DEBUG */ <nl> 
static u16 xhci_calculate_lpm_timeout ( struct usb_hcd * hcd , <nl> if (! config ) <nl> return timeout ; <nl>  <nl> - for ( i = 0 ; i < USB_MAXINTERFACES ; i ++) { <nl> + for ( i = 0 ; i < config -> desc . bNumInterfaces ; i ++) { <nl> struct usb_driver * driver ; <nl> struct usb_interface * intf = config -> interface [ i ]; <nl> 
static int __init con3215_init ( void ) <nl> spin_lock_init (& raw3215_freelist_lock ); <nl> for ( i = 0 ; i < NR_3215_REQ ; i ++) { <nl> req = kzalloc ( sizeof ( struct raw3215_req ), GFP_KERNEL | GFP_DMA ); <nl> + if (! req ) <nl> + return - ENOMEM ; <nl> req -> next = raw3215_freelist ; <nl> raw3215_freelist = req ; <nl> }
asmlinkage long sys_oabi_semtimedop ( int semid , <nl> long err ; <nl> int i ; <nl>  <nl> - if ( nsops < 1 ) <nl> + if ( nsops < 1 || nsops > SEMOPM ) <nl> return - EINVAL ; <nl> sops = kmalloc ( sizeof (* sops ) * nsops , GFP_KERNEL ); <nl> if (! sops )
static ssize_t max_threshold_occ_write ( struct kernfs_open_file * of , <nl>  <nl> intel_cqm_threshold = bytes / r -> mon_scale ; <nl>  <nl> - return ret ?: nbytes ; <nl> + return nbytes ; <nl> } <nl>  <nl> /* rdtgroup information files for one cache resource . */
static int _write_mirror ( struct ore_io_state * ios , int cur_comp ) <nl> struct bio * bio ; <nl>  <nl> if ( per_dev != master_dev ) { <nl> - bio = bio_clone_kmalloc ( master_dev -> bio , <nl> - GFP_KERNEL ); <nl> + bio = bio_clone_fast ( master_dev -> bio , <nl> + GFP_KERNEL , NULL ); <nl> if ( unlikely (! bio )) { <nl> ORE_DBGMSG ( <nl> " Failed to allocate BIO size =% u \ n ",
xfs_setattr_nonsize ( <nl>  <nl> out_cancel : <nl> xfs_trans_cancel ( tp ); <nl> + xfs_iunlock ( ip , XFS_ILOCK_EXCL ); <nl> out_dqrele : <nl> xfs_qm_dqrele ( udqp ); <nl> xfs_qm_dqrele ( gdqp );
static int xhci_mtk_probe ( struct platform_device * pdev ) <nl> goto disable_ldos ; <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if ( irq < 0 ) <nl> + if ( irq < 0 ) { <nl> + ret = irq ; <nl> goto disable_clk ; <nl> + } <nl>  <nl> /* Initialize dma_mask and coherent_dma_mask to 32 - bits */ <nl> ret = dma_set_coherent_mask ( dev , DMA_BIT_MASK ( 32 ));
int trace_get_user ( struct trace_parser * parser , const char __user * ubuf , <nl>  <nl> /* read the non - space input */ <nl> while ( cnt && ! isspace ( ch )) { <nl> - if ( parser -> idx < parser -> size ) <nl> + if ( parser -> idx < parser -> size - 1 ) <nl> parser -> buffer [ parser -> idx ++] = ch ; <nl> else { <nl> ret = - EINVAL ;
static void __init early_identify_cpu ( struct cpuinfo_x86 * c ) <nl>  <nl> setup_force_cpu_cap ( X86_FEATURE_ALWAYS ); <nl>  <nl> - /* Assume for now that ALL x86 CPUs are insecure */ <nl> - setup_force_cpu_bug ( X86_BUG_CPU_INSECURE ); <nl> + if ( c -> x86_vendor != X86_VENDOR_AMD ) <nl> + setup_force_cpu_bug ( X86_BUG_CPU_INSECURE ); <nl>  <nl> fpu__init_system ( c ); <nl> 
SYSCALL_DEFINE1 ( inotify_init1 , int , flags ) <nl> if ( ret >= 0 ) <nl> return ret ; <nl>  <nl> + fsnotify_put_group ( group ); <nl> atomic_dec (& user -> inotify_devs ); <nl> out_free_uid : <nl> free_uid ( user );
static int pci_call_probe ( struct pci_driver * drv , struct pci_dev * dev , <nl> if ( node >= 0 && node != numa_node_id ()) { <nl> int cpu ; <nl>  <nl> - get_online_cpus (); <nl> + cpu_hotplug_disable (); <nl> cpu = cpumask_any_and ( cpumask_of_node ( node ), cpu_online_mask ); <nl> if ( cpu < nr_cpu_ids ) <nl> error = work_on_cpu ( cpu , local_pci_probe , & ddi ); <nl> else <nl> error = local_pci_probe (& ddi ); <nl> - put_online_cpus (); <nl> + cpu_hotplug_enable (); <nl> } else <nl> error = local_pci_probe (& ddi ); <nl> 
bool drm_i2c_encoder_mode_fixup ( struct drm_encoder * encoder , <nl> const struct drm_display_mode * mode , <nl> struct drm_display_mode * adjusted_mode ) <nl> { <nl> + if (! get_slave_funcs ( encoder )-> mode_fixup ) <nl> + return true ; <nl> + <nl> return get_slave_funcs ( encoder )-> mode_fixup ( encoder , mode , adjusted_mode ); <nl> } <nl> EXPORT_SYMBOL ( drm_i2c_encoder_mode_fixup );
static int __devexit wm8753_spi_remove ( struct spi_device * spi ) <nl>  <nl> snd_soc_unregister_codec (& spi -> dev ); <nl> regmap_exit ( wm8753 -> regmap ); <nl> - kfree ( wm8753 ); <nl> return 0 ; <nl> } <nl> 
int vmw_du_crtc_cursor_set ( struct drm_crtc * crtc , struct drm_file * file_priv , <nl> if (! ret ) { <nl> if (! surface -> snooper . image ) { <nl> DRM_ERROR (" surface not suitable for cursor \ n "); <nl> + vmw_surface_unreference (& surface ); <nl> return - EINVAL ; <nl> } <nl> } else {
static int __devinit mwl8k_probe ( struct pci_dev * pdev , <nl> if ( rc ) { <nl> printk ( KERN_ERR "% s : Cannot register device \ n ", <nl> wiphy_name ( hw -> wiphy )); <nl> - goto err_free_irq ; <nl> + goto err_free_queues ; <nl> } <nl>  <nl> printk ( KERN_INFO "% s : % s v % d , % pM , % s firmware % u .% u .% u .% u \ n ",
static int wl1271_op_hw_scan ( struct ieee80211_hw * hw , <nl> static int wl1271_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> { <nl> struct wl1271 * wl = hw -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> mutex_lock (& wl -> mutex ); <nl>  <nl> + if ( unlikely ( wl -> state == WL1271_STATE_OFF )) <nl> + goto out ; <nl> + <nl> ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> if ( ret < 0 ) <nl> goto out ;
const u32_t zcFwImage [] = { <nl> 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 , <nl> }; <nl>  <nl> - const u32_t zcFwImageSize = 15936 ; <nl> + const u32_t zcFwImageSize = 15936 ;
static struct fb_ops macfb_ops = { <nl> . fb_imageblit = cfb_imageblit , <nl> }; <nl>  <nl> - void __init macfb_setup ( char * options ) <nl> + static void __init macfb_setup ( char * options ) <nl> { <nl> char * this_opt ; <nl> 
void __init setup_ioapic_dest ( void ) <nl> mask = apic -> target_cpus (); <nl>  <nl> chip = irq_data_get_irq_chip ( idata ); <nl> - chip -> irq_set_affinity ( idata , mask , false ); <nl> + /* Might be lapic_chip for irq 0 */ <nl> + if ( chip -> irq_set_affinity ) <nl> + chip -> irq_set_affinity ( idata , mask , false ); <nl> } <nl> } <nl> # endif
struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
static int crypto_authenc_verify ( struct aead_request * req , <nl> unsigned int authsize ; <nl>  <nl> areq_ctx -> complete = authenc_verify_ahash_done ; <nl> - areq_ctx -> complete = authenc_verify_ahash_update_done ; <nl> + areq_ctx -> update_complete = authenc_verify_ahash_update_done ; <nl>  <nl> ohash = authenc_ahash_fn ( req , CRYPTO_TFM_REQ_MAY_SLEEP ); <nl> if ( IS_ERR ( ohash ))
static inline void debugfs_remove_domain_dir ( struct irq_domain * d ) { } <nl> # endif <nl>  <nl> const struct fwnode_operations irqchip_fwnode_ops ; <nl> + EXPORT_SYMBOL_GPL ( irqchip_fwnode_ops ); <nl>  <nl> /** <nl> * irq_domain_alloc_fwnode - Allocate a fwnode_handle suitable for
static int __init orion_nand_probe ( struct platform_device * pdev ) <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) { <nl> - err = - ENODEV ; <nl> + ret = - ENODEV ; <nl> goto no_res ; <nl> } <nl> 
static int iucv_sock_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> struct sk_buff * skb , * rskb , * cskb ; <nl> int err = 0 ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> if (( sk -> sk_state == IUCV_DISCONN ) && <nl> skb_queue_empty (& iucv -> backlog_skb_q ) && <nl> skb_queue_empty (& sk -> sk_receive_queue ) &&
static void efifb_fixup_resources ( struct pci_dev * dev ) <nl> if (! base ) <nl> return ; <nl>  <nl> - for ( i = 0 ; i < PCI_STD_RESOURCE_END ; i ++) { <nl> + for ( i = 0 ; i <= PCI_STD_RESOURCE_END ; i ++) { <nl> struct resource * res = & dev -> resource [ i ]; <nl>  <nl> if (!( res -> flags & IORESOURCE_MEM ))
int btrfs_remove_free_space ( struct btrfs_block_group_cache * block_group , <nl> ctl -> total_bitmaps --; <nl> } <nl> kmem_cache_free ( btrfs_free_space_cachep , info ); <nl> + ret = 0 ; <nl> goto out_lock ; <nl> } <nl>  <nl> int btrfs_remove_free_space ( struct btrfs_block_group_cache * block_group , <nl> unlink_free_space ( ctl , info ); <nl> info -> offset += bytes ; <nl> info -> bytes -= bytes ; <nl> - link_free_space ( ctl , info ); <nl> + ret = link_free_space ( ctl , info ); <nl> + WARN_ON ( ret ); <nl> goto out_lock ; <nl> } <nl> 
static int autofs4_tree_busy ( struct vfsmount * mnt , <nl> struct autofs_info * ino = autofs4_dentry_ino ( p ); <nl> unsigned int ino_count = atomic_read (& ino -> count ); <nl>  <nl> + /* <nl> + * Clean stale dentries below that have not been <nl> + * invalidated after a mount fail during lookup <nl> + */ <nl> + d_invalidate ( p ); <nl> + <nl> /* allow for dget above and top is already dgot */ <nl> if ( p == top ) <nl> ino_count += 2 ;
static struct comedi_driver das16_driver = { <nl> module_comedi_driver ( das16_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for DAS16 compatible boards "); <nl> MODULE_LICENSE (" GPL ");
MODULE_DEVICE_TABLE ( pci , epca_pci_tbl ); <nl> int __init init_PCI ( void ) <nl> { /* Begin init_PCI */ <nl> memset (& epca_driver , 0 , sizeof ( epca_driver )); <nl> + epca_driver . owner = THIS_MODULE ; <nl> epca_driver . name = " epca "; <nl> epca_driver . id_table = epca_pci_tbl ; <nl> epca_driver . probe = epca_init_one ;
static int m41t80_probe ( struct i2c_client * client , <nl> m41t80_rtc_ops . read_alarm = m41t80_read_alarm ; <nl> m41t80_rtc_ops . set_alarm = m41t80_set_alarm ; <nl> m41t80_rtc_ops . alarm_irq_enable = m41t80_alarm_irq_enable ; <nl> + /* Enable the wakealarm */ <nl> + device_init_wakeup (& client -> dev , true ); <nl> } <nl> } <nl> 
nvkm_disp_oneinit ( struct nvkm_engine * engine ) <nl> /* Create output path objects for each VBIOS display path . */ <nl> i = - 1 ; <nl> while (( data = dcb_outp_parse ( bios , ++ i , & ver , & hdr , & dcbE ))) { <nl> + if ( ver < 0x40 ) /* No support for chipsets prior to NV50 . */ <nl> + break ; <nl> if ( dcbE . type == DCB_OUTPUT_UNUSED ) <nl> continue ; <nl> if ( dcbE . type == DCB_OUTPUT_EOL )
static int sur40_probe ( struct usb_interface * interface , <nl> sur40 -> alloc_ctx = vb2_dma_sg_init_ctx ( sur40 -> dev ); <nl> if ( IS_ERR ( sur40 -> alloc_ctx )) { <nl> dev_err ( sur40 -> dev , " Can ' t allocate buffer context "); <nl> + error = PTR_ERR ( sur40 -> alloc_ctx ); <nl> goto err_unreg_v4l2 ; <nl> } <nl> 
struct device * driver_find_device ( struct device_driver * drv , <nl> struct klist_iter i ; <nl> struct device * dev ; <nl>  <nl> - if (! drv ) <nl> + if (! drv || ! drv -> p ) <nl> return NULL ; <nl>  <nl> klist_iter_init_node (& drv -> p -> klist_devices , & i ,
int mmc_send_if_cond ( struct mmc_host * host , u32 ocr ) <nl> static const u8 test_pattern = 0xAA ; <nl> u8 result_pattern ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( struct mmc_command )); <nl> + <nl> /* <nl> * To support SD 2 . 0 cards , we must always invoke SD_SEND_IF_COND <nl> * before SD_APP_OP_COND . This command will harmlessly fail for
static int ieee80211_change_station ( struct wiphy * wiphy , <nl> } <nl>  <nl> if ( params -> vlan -> ieee80211_ptr -> use_4addr ) { <nl> - if ( vlansdata -> u . vlan . sta ) <nl> + if ( vlansdata -> u . vlan . sta ) { <nl> + rcu_read_unlock (); <nl> return - EBUSY ; <nl> + } <nl>  <nl> rcu_assign_pointer ( vlansdata -> u . vlan . sta , sta ); <nl> }
static int iwl_mvm_free_inactive_queue ( struct iwl_mvm * mvm , int queue , <nl> spin_unlock_bh (& mvm -> queue_info_lock ); <nl>  <nl> mvmsta = iwl_mvm_sta_from_staid_protected ( mvm , sta_id ); <nl> + if ( WARN_ON (! mvmsta )) <nl> + return - EINVAL ; <nl>  <nl> disable_agg_tids = iwl_mvm_remove_sta_queue_marking ( mvm , queue ); <nl> /* Disable the queue */
static void valleyview_disable_rps ( struct drm_device * dev ) <nl>  <nl> int intel_enable_rc6 ( const struct drm_device * dev ) <nl> { <nl> + /* No RC6 before Ironlake */ <nl> + if ( INTEL_INFO ( dev )-> gen < 5 ) <nl> + return 0 ; <nl> + <nl> /* Respect the kernel parameter if it is set */ <nl> if ( i915_enable_rc6 >= 0 ) <nl> return i915_enable_rc6 ;
static __u8 * cp_report_fixup ( struct hid_device * hdev , __u8 * rdesc , <nl> if (!( quirks & CP_RDESC_SWAPPED_MIN_MAX )) <nl> return rdesc ; <nl>  <nl> + if (* rsize < 4 ) <nl> + return rdesc ; <nl> + <nl> for ( i = 0 ; i < * rsize - 4 ; i ++) <nl> if ( rdesc [ i ] == 0x29 && rdesc [ i + 2 ] == 0x19 ) { <nl> rdesc [ i ] = 0x19 ;
struct ti_ohci { <nl>  <nl> static inline int cross_bound ( unsigned long addr , unsigned int size ) <nl> { <nl> + if ( size == 0 ) <nl> + return 0 ; <nl> + <nl> if ( size > PAGE_SIZE ) <nl> return 1 ; <nl> 
static int s5c73m3_probe ( struct i2c_client * client , <nl> state -> oif_pads [ OIF_ISP_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_JPEG_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_SOURCE_PAD ]. flags = MEDIA_PAD_FL_SOURCE ; <nl> - oif_sd -> entity . function = MEDIA_ENT_F_V4L2_SUBDEV_UNKNOWN ; <nl> + oif_sd -> entity . function = MEDIA_ENT_F_PROC_VIDEO_SCALER ; <nl>  <nl> ret = media_entity_pads_init (& oif_sd -> entity , OIF_NUM_PADS , <nl> state -> oif_pads );
void __key_link_end ( struct key * keyring , <nl> if ( index_key -> type == & key_type_keyring ) <nl> up_write (& keyring_serialise_link_sem ); <nl>  <nl> - if ( edit && ! edit -> dead_leaf ) { <nl> - key_payload_reserve ( keyring , <nl> - keyring -> datalen - KEYQUOTA_LINK_BYTES ); <nl> + if ( edit ) { <nl> + if (! edit -> dead_leaf ) { <nl> + key_payload_reserve ( keyring , <nl> + keyring -> datalen - KEYQUOTA_LINK_BYTES ); <nl> + } <nl> assoc_array_cancel_edit ( edit ); <nl> } <nl> up_write (& keyring -> sem );
ip_vs_new_dest ( struct ip_vs_service * svc , struct ip_vs_dest_user_kern * udest , <nl> # ifdef CONFIG_IP_VS_IPV6 <nl> if ( svc -> af == AF_INET6 ) { <nl> atype = ipv6_addr_type (& udest -> addr . in6 ); <nl> - if (!( atype & IPV6_ADDR_UNICAST ) && <nl> + if ((!( atype & IPV6_ADDR_UNICAST ) || <nl> + atype & IPV6_ADDR_LINKLOCAL ) && <nl> ! __ip_vs_addr_is_local_v6 (& udest -> addr . in6 )) <nl> return - EINVAL ; <nl> } else
static int alc_mux_select ( struct hda_codec * codec , unsigned int adc_idx , <nl> int i , type , num_conns ; <nl> hda_nid_t nid ; <nl>  <nl> + if (! spec -> input_mux ) <nl> + return 0 ; <nl> + <nl> mux_idx = adc_idx >= spec -> num_mux_defs ? 0 : adc_idx ; <nl> imux = & spec -> input_mux [ mux_idx ]; <nl> if (! imux -> num_items && mux_idx > 0 )
void wil_halp_vote ( struct wil6210_priv * wil ) <nl> wil -> halp . ref_cnt ); <nl>  <nl> if (++ wil -> halp . ref_cnt == 1 ) { <nl> + reinit_completion (& wil -> halp . comp ); <nl> wil6210_set_halp ( wil ); <nl> rc = wait_for_completion_timeout (& wil -> halp . comp , to_jiffies ); <nl> if (! rc ) {
int ip6_route_add ( struct fib6_config * cfg ) <nl> goto out ; <nl> lwtunnel_state_get ( lwtstate ); <nl> rt -> rt6i_lwtstate = lwtstate ; <nl> + rt -> dst . output = lwtunnel_output6 ; <nl> } <nl>  <nl> ipv6_addr_prefix (& rt -> rt6i_dst . addr , & cfg -> fc_dst , cfg -> fc_dst_len );
struct dma_buf * exynos_dmabuf_prime_export ( struct drm_device * drm_dev , <nl> struct exynos_drm_gem_obj * exynos_gem_obj = to_exynos_gem_obj ( obj ); <nl>  <nl> return dma_buf_export ( exynos_gem_obj , & exynos_dmabuf_ops , <nl> - exynos_gem_obj -> base . size , 0600 ); <nl> + exynos_gem_obj -> base . size , flags ); <nl> } <nl>  <nl> struct drm_gem_object * exynos_dmabuf_prime_import ( struct drm_device * drm_dev ,
xfs_btree_simple_query_range ( <nl> if ( error ) <nl> goto out ; <nl>  <nl> + /* Nothing ? See if there ' s anything to the right . */ <nl> + if (! stat ) { <nl> + error = xfs_btree_increment ( cur , 0 , & stat ); <nl> + if ( error ) <nl> + goto out ; <nl> + } <nl> + <nl> while ( stat ) { <nl> /* Find the record . */ <nl> error = xfs_btree_get_rec ( cur , & recp , & stat );
static void do_pata_set_dmamode ( struct ata_port * ap , struct ata_device * adev , i <nl> u16 master_data ; <nl> u8 speed = adev -> dma_mode ; <nl> int devid = adev -> devno + 2 * ap -> port_no ; <nl> - u8 udma_enable ; <nl> + u8 udma_enable = 0 ; <nl>  <nl> static const /* ISP RTC */ <nl> u8 timings [][ 2 ] = { { 0 , 0 },
static struct irq_info * info_for_irq ( unsigned irq ) <nl>  <nl> static unsigned int evtchn_from_irq ( unsigned irq ) <nl> { <nl> + if ( unlikely ( WARN ( irq < 0 || irq >= nr_irqs , " Invalid irq % d !\ n ", irq ))) <nl> + return 0 ; <nl> + <nl> return info_for_irq ( irq )-> evtchn ; <nl> } <nl> 
void recalc_intercepts ( struct vcpu_svm * svm ) <nl> /* If SMI is not intercepted , ignore guest SMI intercept as well */ <nl> if (! intercept_smi ) <nl> vmcb_clr_intercept ( c , INTERCEPT_SMI ); <nl> + <nl> + vmcb_set_intercept ( c , INTERCEPT_VMLOAD ); <nl> + vmcb_set_intercept ( c , INTERCEPT_VMSAVE ); <nl> } <nl>  <nl> static void copy_vmcb_control_area ( struct vmcb_control_area * dst ,
build_unc_path_to_root ( const struct smb_vol * vol , <nl> pos = full_path + unc_len ; <nl>  <nl> if ( pplen ) { <nl> - * pos ++ = CIFS_DIR_SEP ( cifs_sb ); <nl> - strncpy ( pos , vol -> prepath , pplen ); <nl> + * pos = CIFS_DIR_SEP ( cifs_sb ); <nl> + strncpy ( pos + 1 , vol -> prepath , pplen ); <nl> pos += pplen ; <nl> } <nl> 
cfg80211_inform_bss_frame ( struct wiphy * wiphy , <nl> return NULL ; <nl>  <nl> if ( WARN_ON ( wiphy -> signal_type == CFG80211_SIGNAL_TYPE_UNSPEC && <nl> - ( signal < 0 || signal > 100 ))) <nl> + ( signal < 0 || signal > 100 ))) <nl> return NULL ; <nl>  <nl> if ( WARN_ON ( len < offsetof ( struct ieee80211_mgmt , u . probe_resp . variable )))
int usb_sg_init ( struct usb_sg_request * io , struct usb_device * dev , <nl> } <nl>  <nl> /* initialize all the urbs we ' ll use */ <nl> - io -> urbs = kmalloc ( io -> entries * sizeof * io -> urbs , mem_flags ); <nl> + io -> urbs = kmalloc ( io -> entries * sizeof (* io -> urbs ), mem_flags ); <nl> if (! io -> urbs ) <nl> goto nomem ; <nl> 
static int __devinit gab_probe ( struct platform_device * pdev ) <nl> ret = request_any_context_irq ( irq , gab_charged , <nl> IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING , <nl> " battery charged ", adc_bat ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_gpio ; <nl> } <nl> 
static int skge_down ( struct net_device * dev ) <nl> struct skge_hw * hw = skge -> hw ; <nl> int port = skge -> port ; <nl>  <nl> - if ( skge -> mem == NULL ) <nl> + if (! skge -> mem ) <nl> return 0 ; <nl>  <nl> netif_info ( skge , ifdown , skge -> netdev , " disabling interface \ n ");
static inline void nfs4_stateid_downgrade ( struct nfs4_ol_stateid * stp , u32 to_ac <nl> } <nl>  <nl> static void <nl> - reset_union_bmap_deny ( unsigned long deny , struct nfs4_ol_stateid * stp ) <nl> + reset_union_bmap_deny ( u32 deny , struct nfs4_ol_stateid * stp ) <nl> { <nl> int i ; <nl> - for ( i = 0 ; i < 4 ; i ++) { <nl> + <nl> + for ( i = 1 ; i < 4 ; i ++) { <nl> if (( i & deny ) != i ) <nl> clear_deny ( i , stp ); <nl> }
static int tegra_syncpt_wait ( struct drm_device * drm , void * data , <nl> if (! sp ) <nl> return - EINVAL ; <nl>  <nl> - return host1x_syncpt_wait ( sp , args -> thresh , args -> timeout , <nl> + return host1x_syncpt_wait ( sp , args -> thresh , <nl> + msecs_to_jiffies ( args -> timeout ), <nl> & args -> value ); <nl> } <nl> 
int btrfs_add_inode_defrag ( struct btrfs_trans_handle * trans , <nl> spin_lock (& root -> fs_info -> defrag_inodes_lock ); <nl> if (! BTRFS_I ( inode )-> in_defrag ) <nl> __btrfs_add_inode_defrag ( inode , defrag ); <nl> + else <nl> + kfree ( defrag ); <nl> spin_unlock (& root -> fs_info -> defrag_inodes_lock ); <nl> return 0 ; <nl> }
static inline u32 skb_mstamp_us_delta ( const struct skb_mstamp * t1 , <nl> return delta_us ; <nl> } <nl>  <nl> + static inline bool skb_mstamp_after ( const struct skb_mstamp * t1 , <nl> + const struct skb_mstamp * t0 ) <nl> +{ <nl> + s32 diff = t1 -> stamp_jiffies - t0 -> stamp_jiffies ; <nl> + <nl> + if (! diff ) <nl> + diff = t1 -> stamp_us - t0 -> stamp_us ; <nl> + return diff > 0 ; <nl> +} <nl>  <nl> /** <nl> * struct sk_buff - socket buffer
static int sfi_cpufreq_cpu_init ( struct cpufreq_policy * policy ) <nl> { <nl> policy -> shared_type = CPUFREQ_SHARED_TYPE_HW ; <nl> policy -> cpuinfo . transition_latency = 100000 ; /* 100us */ <nl> + policy -> freq_table = freq_table ; <nl>  <nl> - return cpufreq_table_validate_and_show ( policy , freq_table ); <nl> + return 0 ; <nl> } <nl>  <nl> static struct cpufreq_driver sfi_cpufreq_driver = {
bfin_debug_mmrs_dma ( struct dentry * parent , unsigned long base , int num , char mdm <nl> __DMA ( CURR_DESC_PTR , curr_desc_ptr ); <nl> __DMA ( CURR_ADDR , curr_addr ); <nl> __DMA ( IRQ_STATUS , irq_status ); <nl> - __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> + if ( strcmp ( pfx , " IMDMA ") != 0 ) <nl> + __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> __DMA ( CURR_X_COUNT , curr_x_count ); <nl> __DMA ( CURR_Y_COUNT , curr_y_count ); <nl> }
static int kvm_ioctl_create_device ( struct kvm * kvm , <nl>  <nl> ret = anon_inode_getfd ( ops -> name , & kvm_device_fops , dev , O_RDWR | O_CLOEXEC ); <nl> if ( ret < 0 ) { <nl> - ops -> destroy ( dev ); <nl> mutex_lock (& kvm -> lock ); <nl> list_del (& dev -> vm_node ); <nl> mutex_unlock (& kvm -> lock ); <nl> + ops -> destroy ( dev ); <nl> return ret ; <nl> } <nl> 
static bool g4x_compute_wm0 ( struct drm_device * dev , <nl> int entries , tlb_miss ; <nl>  <nl> crtc = intel_get_crtc_for_plane ( dev , plane ); <nl> - if ( crtc -> fb == NULL || ! crtc -> enabled ) <nl> + if ( crtc -> fb == NULL || ! crtc -> enabled ) { <nl> + * cursor_wm = cursor -> guard_size ; <nl> + * plane_wm = display -> guard_size ; <nl> return false ; <nl> + } <nl>  <nl> htotal = crtc -> mode . htotal ; <nl> hdisplay = crtc -> mode . hdisplay ;
static int das1800_attach ( struct comedi_device * dev , <nl> if ( dev -> irq & it -> options [ 2 ]) <nl> das1800_init_dma ( dev , it ); <nl>  <nl> - devpriv -> fifo_buf = kmalloc ( FIFO_SIZE * sizeof ( uint16_t ), GFP_KERNEL ); <nl> + devpriv -> fifo_buf = kmalloc_array ( FIFO_SIZE , sizeof ( uint16_t ), GFP_KERNEL ); <nl> if (! devpriv -> fifo_buf ) <nl> return - ENOMEM ; <nl> 
static int init_phy ( struct net_device * dev ) <nl> if ( priv -> phy_interface == PHY_INTERFACE_MODE_SGMII ) <nl> uec_configure_serdes ( dev ); <nl>  <nl> - phydev -> supported &= ( ADVERTISED_10baseT_Half | <nl> - ADVERTISED_10baseT_Full | <nl> - ADVERTISED_100baseT_Half | <nl> - ADVERTISED_100baseT_Full ); <nl> + phydev -> supported &= ( SUPPORTED_MII | <nl> + SUPPORTED_Autoneg | <nl> + ADVERTISED_10baseT_Half | <nl> + ADVERTISED_10baseT_Full | <nl> + ADVERTISED_100baseT_Half | <nl> + ADVERTISED_100baseT_Full ); <nl>  <nl> if ( priv -> max_speed == SPEED_1000 ) <nl> phydev -> supported |= ADVERTISED_1000baseT_Full ;
static struct clk * __init clkgen_odf_register ( const char * parent_name , <nl> gate -> lock = odf_lock ; <nl>  <nl> div = kzalloc ( sizeof (* div ), GFP_KERNEL ); <nl> - if (! div ) <nl> + if (! div ) { <nl> + kfree ( gate ); <nl> return ERR_PTR (- ENOMEM ); <nl> + } <nl>  <nl> div -> flags = CLK_DIVIDER_ONE_BASED | CLK_DIVIDER_ALLOW_ZERO ; <nl> div -> reg = reg + pll_data -> odf [ odf ]. offset ;
typedef struct { <nl> uint8_t max_lun ; <nl>  <nl> uint32_t unique_id ; <nl> - uint8_t irq ; <nl> + int irq ; <nl> uint8_t ito ; <nl> caddr_t ibuf ; <nl> dma_addr_t ibuf_dma_h ;
static int rfcomm_tty_open ( struct tty_struct * tty , struct file * filp ) <nl> break ; <nl> } <nl>  <nl> + tty_unlock (); <nl> schedule (); <nl> + tty_lock (); <nl> } <nl> set_current_state ( TASK_RUNNING ); <nl> remove_wait_queue (& dev -> wait , & wait );
static int skcipher_sendmsg ( struct socket * sock , struct msghdr * msg , <nl>  <nl> sgl = list_entry ( ctx -> tsgl . prev , struct skcipher_sg_list , list ); <nl> sg = sgl -> sg ; <nl> - sg_unmark_end ( sg + sgl -> cur ); <nl> + if ( sgl -> cur ) <nl> + sg_unmark_end ( sg + sgl -> cur - 1 ); <nl> do { <nl> i = sgl -> cur ; <nl> plen = min_t ( size_t , len , PAGE_SIZE );
int ocfs2_xattr_get_nolock ( struct inode * inode , <nl> return - EOPNOTSUPP ; <nl>  <nl> if (!( oi -> ip_dyn_features & OCFS2_HAS_XATTR_FL )) <nl> - ret = - ENODATA ; <nl> + return - ENODATA ; <nl>  <nl> xis . inode_bh = xbs . inode_bh = di_bh ; <nl> di = ( struct ocfs2_dinode *) di_bh -> b_data ;
static int sm501fb_start ( struct sm501fb_info * info , <nl> info -> fbmem_len = resource_size ( res ); <nl>  <nl> /* clear framebuffer memory - avoids garbage data on unused fb */ <nl> - memset ( info -> fbmem , 0 , info -> fbmem_len ); <nl> + memset_io ( info -> fbmem , 0 , info -> fbmem_len ); <nl>  <nl> /* clear palette ram - undefined at power on */ <nl> for ( k = 0 ; k < ( 256 * 3 ); k ++)
static int drbd_asb_recover_0p ( struct drbd_conf * mdev ) __must_hold ( local ) <nl> break ; <nl> } <nl> /* Else fall through to one of the other strategies ... */ <nl> - dev_warn ( DEV , " Discard younger / older primary did not found a decision \ n " <nl> + dev_warn ( DEV , " Discard younger / older primary did not find a decision \ n " <nl> " Using discard - least - changes instead \ n "); <nl> case ASB_DISCARD_ZERO_CHG : <nl> if ( ch_peer == 0 && ch_self == 0 ) {
mlxreg_hotplug_health_work_helper ( struct mlxreg_hotplug_priv_data * priv , <nl> { <nl> struct mlxreg_core_data * data = item -> data ; <nl> u32 regval ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> for ( i = 0 ; i < item -> count ; i ++, data ++) { <nl> /* Mask event . */
static int virtio_gpu_object_shmem_init ( struct virtio_gpu_device * vgdev , <nl> * since virtio_gpu doesn ' t support dma - buf import from other devices . <nl> */ <nl> shmem -> pages = drm_gem_shmem_get_sg_table (& bo -> base ); <nl> - if (! shmem -> pages ) { <nl> + if ( IS_ERR ( shmem -> pages )) { <nl> drm_gem_shmem_unpin (& bo -> base ); <nl> - return - EINVAL ; <nl> + return PTR_ERR ( shmem -> pages ); <nl> } <nl>  <nl> if ( use_dma_api ) {
static void ks_sdio_interrupt ( struct sdio_func * func ) <nl> int ret ; <nl> struct ks_sdio_card * card ; <nl> struct ks_wlan_private * priv ; <nl> - unsigned char status , rsize , byte ; <nl> + u8 status , rsize , byte ; <nl>  <nl> card = sdio_get_drvdata ( func ); <nl> priv = card -> priv ;
static int pcnet32_open ( struct net_device * dev ) <nl> lp -> rx_dma_addr [ i ] = 0 ; <nl> } <nl>  <nl> - pcnet32_free_ring ( dev ); <nl> - <nl> /* <nl> * Switch back to 16bit mode to avoid problems with dumb <nl> * DOS packet driver after a warm reboot
int of_get_fb_videomode ( struct device_node * np , struct fb_videomode * fb , <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - fb_videomode_from_videomode (& vm , fb ); <nl> + ret = fb_videomode_from_videomode (& vm , fb ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> pr_debug ("% s : got % dx % d display mode from % s \ n ", <nl> of_node_full_name ( np ), vm . hactive , vm . vactive , np -> name );
static void kick_tx ( int fd ) <nl> int ret ; <nl>  <nl> ret = sendto ( fd , NULL , 0 , MSG_DONTWAIT , NULL , 0 ); <nl> - if ( ret >= 0 || errno == ENOBUFS || errno == EAGAIN ) <nl> + if ( ret >= 0 || errno == ENOBUFS || errno == EAGAIN || errno == EBUSY ) <nl> return ; <nl> lassert ( 0 ); <nl> }
int i915_driver_load ( struct drm_device * dev , unsigned long flags ) <nl>  <nl> intel_irq_init ( dev ); <nl> intel_pm_init ( dev ); <nl> - intel_uncore_sanitize ( dev ); <nl> intel_uncore_init ( dev ); <nl> + intel_uncore_sanitize ( dev ); <nl>  <nl> /* Try to make sure MCHBAR is enabled before poking at it */ <nl> intel_setup_mchbar ( dev );
static struct dentry * proc_mount ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb )) <nl> return ERR_CAST ( sb ); <nl>  <nl> + /* <nl> + * procfs isn ' t actually a stacking filesystem ; however , there is <nl> + * too much magic going on inside it to permit stacking things on <nl> + * top of it <nl> + */ <nl> + sb -> s_stack_depth = FILESYSTEM_MAX_STACK_DEPTH ; <nl> + <nl> if (! proc_parse_options ( options , ns )) { <nl> deactivate_locked_super ( sb ); <nl> return ERR_PTR (- EINVAL );
static const struct attribute_group isl29108_group = { <nl> static int isl29018_chip_init ( struct isl29018_chip * chip ) <nl> { <nl> int status ; <nl> - int new_adc_bit ; <nl> + unsigned int new_adc_bit ; <nl> unsigned int new_range ; <nl>  <nl> /* Code added per Intersil Application Note 1534 :
void drm_modeset_acquire_init ( struct drm_modeset_acquire_ctx * ctx , <nl> uint32_t flags ) <nl> { <nl> + memset ( ctx , 0 , sizeof (* ctx )); <nl> ww_acquire_init (& ctx -> ww_ctx , & crtc_ww_class ); <nl> INIT_LIST_HEAD (& ctx -> locked ); <nl> }
static int __net_init __ip_vs_ftp_init ( struct net * net ) <nl> struct ip_vs_app * app ; <nl> struct netns_ipvs * ipvs = net_ipvs ( net ); <nl>  <nl> + if (! ipvs ) <nl> + return - ENOENT ; <nl> app = kmemdup (& ip_vs_ftp , sizeof ( struct ip_vs_app ), GFP_KERNEL ); <nl> if (! app ) <nl> return - ENOMEM ;
static struct s3c_camif_drvdata s3c6410_camif_drvdata = { <nl> . bus_clk_freq = 133000000UL , <nl> }; <nl>  <nl> - static struct platform_device_id s3c_camif_driver_ids [] = { <nl> + static const struct platform_device_id s3c_camif_driver_ids [] = { <nl> { <nl> . name = " s3c2440 - camif ", <nl> . driver_data = ( unsigned long )& s3c244x_camif_drvdata ,
static void handle_swbp ( struct pt_regs * regs ) <nl> if ( unlikely (! test_bit ( UPROBE_COPY_INSN , & uprobe -> flags ))) <nl> goto out ; <nl>  <nl> + /* Tracing handlers use -> utask to communicate with fetch methods */ <nl> + if (! get_utask ()) <nl> + goto out ; <nl> + <nl> handler_chain ( uprobe , regs ); <nl> if ( can_skip_sstep ( uprobe , regs )) <nl> goto out ;
static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
static int __cpu_find_by ( int (* compare )( int , int , void *), void * compare_arg , <nl> int err = check_cpu_node ( dp -> node , & cur_inst , <nl> compare , compare_arg , <nl> prom_node , mid ); <nl> - if (! err ) <nl> + if (! err ) { <nl> + of_node_put ( dp ); <nl> return 0 ; <nl> + } <nl> } <nl>  <nl> return - ENODEV ;
static int twl6040_probe ( struct i2c_client * client , <nl> init_completion (& twl6040 -> ready ); <nl>  <nl> twl6040 -> rev = twl6040_reg_read ( twl6040 , TWL6040_REG_ASICREV ); <nl> + if ( twl6040 -> rev < 0 ) { <nl> + dev_err (& client -> dev , " Failed to read revision register : % d \ n ", <nl> + twl6040 -> rev ); <nl> + goto gpio_err ; <nl> + } <nl>  <nl> /* ERRATA : Automatic power - up is not possible in ES1 . 0 */ <nl> if ( twl6040_get_revid ( twl6040 ) > TWL6040_REV_ES1_0 )
void con_protect_unimap ( struct vc_data * vc , int rdonly ); <nl> int con_copy_unimap ( struct vc_data * dst_vc , struct vc_data * src_vc ); <nl>  <nl> # define vc_translate ( vc , c ) (( vc )-> vc_translate [( c ) | \ <nl> - ( vc )-> vc_toggle_meta ? 0x80 : 0 ]) <nl> + (( vc )-> vc_toggle_meta ? 0x80 : 0 )]) <nl> # else <nl> # define con_set_trans_old ( arg ) ( 0 ) <nl> # define con_get_trans_old ( arg ) (- EINVAL )
static inline int pmd_none_or_trans_huge_or_clear_bad ( pmd_t * pmd ) <nl> # ifdef CONFIG_TRANSPARENT_HUGEPAGE <nl> barrier (); <nl> # endif <nl> - if ( pmd_none ( pmdval )) <nl> + if ( pmd_none ( pmdval ) || pmd_trans_huge ( pmdval )) <nl> return 1 ; <nl> if ( unlikely ( pmd_bad ( pmdval ))) { <nl> - if (! pmd_trans_huge ( pmdval )) <nl> - pmd_clear_bad ( pmd ); <nl> + pmd_clear_bad ( pmd ); <nl> return 1 ; <nl> } <nl> return 0 ;
static int pinconf_dbg_config_write ( struct file * file , <nl> int i ; <nl>  <nl> /* Get userspace string and assure termination */ <nl> - buf_size = min ( count , ( sizeof ( buf )- 1 )); <nl> + buf_size = min ( count , ( size_t )( sizeof ( buf )- 1 )); <nl> if ( copy_from_user ( buf , user_buf , buf_size )) <nl> return - EFAULT ; <nl> buf [ buf_size ] = 0 ;
static int option_probe ( struct usb_serial * serial , <nl> serial -> interface -> cur_altsetting -> desc . bInterfaceNumber , <nl> OPTION_BLACKLIST_RESERVED_IF , <nl> ( const struct option_blacklist_info *) id -> driver_info )) <nl> + return - ENODEV ; <nl>  <nl> /* Don ' t bind network interface on Samsung GT - B3730 , it is handled by a separate module */ <nl> if ( serial -> dev -> descriptor . idVendor == SAMSUNG_VENDOR_ID &&
int iwl_mvm_mac_setup_register ( struct iwl_mvm * mvm ) <nl> else <nl> hw -> wiphy -> flags &= ~ WIPHY_FLAG_PS_ON_BY_DEFAULT ; <nl>  <nl> - if ( mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_SCHED_SCAN ) { <nl> + if ( 0 && mvm -> fw -> ucode_capa . flags & IWL_UCODE_TLV_FLAGS_SCHED_SCAN ) { <nl> hw -> wiphy -> flags |= WIPHY_FLAG_SUPPORTS_SCHED_SCAN ; <nl> hw -> wiphy -> max_sched_scan_ssids = PROBE_OPTION_MAX ; <nl> hw -> wiphy -> max_match_sets = IWL_SCAN_MAX_PROFILES ;
static int Handle1401Esc ( DEVICE_EXTENSION * pdx , char * pCh , <nl> /* This can never happen , really */ <nl> dev_err (& pdx -> interface -> dev , <nl> " ERROR : DMA setup while transfer still waiting "); <nl> - spin_unlock (& pdx -> stagedLock ); <nl> + spin_unlock (& pdx -> stagedLock ); <nl> } else { <nl> if (( wTransType == TM_EXTTOHOST ) <nl> || ( wTransType == TM_EXTTO1401 )) {
static struct dma_page * pool_find_page ( struct dma_pool * pool , dma_addr_t dma ) <nl> list_for_each_entry ( page , & pool -> page_list , page_list ) { <nl> if ( dma < page -> dma ) <nl> continue ; <nl> - if ( dma < ( page -> dma + pool -> allocation )) <nl> + if (( dma - page -> dma ) < pool -> allocation ) <nl> return page ; <nl> } <nl> return NULL ;
static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> val ); <nl>  <nl> case LIRC_SET_REC_CARRIER_RANGE : <nl> + if (! dev -> s_rx_carrier_range ) <nl> + return - ENOTTY ; <nl> + <nl> if ( val <= 0 ) <nl> return - EINVAL ; <nl>  <nl> static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> break ; <nl>  <nl> case LIRC_SET_REC_TIMEOUT_REPORTS : <nl> + if (! dev -> timeout ) <nl> + return - ENOTTY ; <nl> + <nl> lirc -> send_timeout_reports = !! val ; <nl> break ; <nl> 
static struct ib_qp * i40iw_create_qp ( struct ib_pd * ibpd , <nl> return & iwqp -> ibqp ; <nl> error : <nl> i40iw_free_qp_resources ( iwdev , iwqp , qp_num ); <nl> - kfree ( mem ); <nl> return ERR_PTR ( err_code ); <nl> } <nl> 
static inline bool is_new_conn_expected ( const struct ip_vs_conn * cp , <nl> switch ( cp -> protocol ) { <nl> case IPPROTO_TCP : <nl> return ( cp -> state == IP_VS_TCP_S_TIME_WAIT ) || <nl> + ( cp -> state == IP_VS_TCP_S_CLOSE ) || <nl> (( conn_reuse_mode & 2 ) && <nl> ( cp -> state == IP_VS_TCP_S_FIN_WAIT ) && <nl> ( cp -> flags & IP_VS_CONN_F_NOOUTPUT ));
bool kvm_irq_delivery_to_apic_fast ( struct kvm * kvm , struct kvm_lapic * src , <nl> * r = - 1 ; <nl>  <nl> if ( irq -> shorthand == APIC_DEST_SELF ) { <nl> + if ( KVM_BUG_ON (! src , kvm )) { <nl> + * r = 0 ; <nl> + return true ; <nl> + } <nl> * r = kvm_apic_set_irq ( src -> vcpu , irq , dest_map ); <nl> return true ; <nl> }
bool ath9k_hw_eeprom_set_board_values ( struct ath_hal * ah , <nl>  <nl> txRxAttenLocal = IS_CHAN_2GHZ ( chan ) ? 23 : 44 ; <nl>  <nl> - ath9k_hw_get_eeprom_antenna_cfg ( ah , chan , 1 , & ant_config ); <nl> + ath9k_hw_get_eeprom_antenna_cfg ( ah , chan , 0 , & ant_config ); <nl> REG_WRITE ( ah , AR_PHY_SWITCH_COM , ant_config ); <nl>  <nl> for ( i = 0 ; i < AR5416_MAX_CHAINS ; i ++) {
static int copy_to_user_auth ( struct xfrm_algo_auth * auth , struct sk_buff * skb ) <nl> return - EMSGSIZE ; <nl>  <nl> algo = nla_data ( nla ); <nl> - strcpy ( algo -> alg_name , auth -> alg_name ); <nl> + strncpy ( algo -> alg_name , auth -> alg_name , sizeof ( algo -> alg_name )); <nl> memcpy ( algo -> alg_key , auth -> alg_key , ( auth -> alg_key_len + 7 ) / 8 ); <nl> algo -> alg_key_len = auth -> alg_key_len ; <nl> 
int reiserfs_acl_chmod ( struct inode * inode ) <nl> return 0 ; <nl> } <nl>  <nl> + reiserfs_write_unlock ( inode -> i_sb ); <nl> acl = reiserfs_get_acl ( inode , ACL_TYPE_ACCESS ); <nl> + reiserfs_write_lock ( inode -> i_sb ); <nl> if (! acl ) <nl> return 0 ; <nl> if ( IS_ERR ( acl ))
int max8998_irq_init ( struct max8998_dev * max8998 ) <nl>  <nl> void max8998_irq_exit ( struct max8998_dev * max8998 ) <nl> { <nl> + if ( max8998 -> ono ) <nl> + free_irq ( max8998 -> ono , max8998 ); <nl> + <nl> if ( max8998 -> irq ) <nl> free_irq ( max8998 -> irq , max8998 ); <nl> }
static void ieee80211_iface_work ( struct work_struct * work ) <nl> if ( sta ) { <nl> u16 last_seq ; <nl>  <nl> - last_seq = le16_to_cpu ( <nl> - sta -> last_seq_ctrl [ rx_agg -> tid ]); <nl> + last_seq = IEEE80211_SEQ_TO_SN ( le16_to_cpu ( <nl> + sta -> last_seq_ctrl [ rx_agg -> tid ])); <nl>  <nl> __ieee80211_start_rx_ba_session ( sta , <nl> 0 , 0 ,
static struct afs_volume * afs_alloc_volume ( struct afs_mount_params * params , <nl> error_2 : <nl> afs_put_serverlist ( params -> net , slist ); <nl> error_1 : <nl> + afs_put_cell ( params -> net , volume -> cell ); <nl> kfree ( volume ); <nl> error_0 : <nl> return ERR_PTR ( ret );
static void lut_close ( struct i915_gem_context * ctx ) <nl> kmem_cache_free ( ctx -> i915 -> luts , lut ); <nl> } <nl>  <nl> + rcu_read_lock (); <nl> radix_tree_for_each_slot ( slot , & ctx -> handles_vma , & iter , 0 ) { <nl> struct i915_vma * vma = rcu_dereference_raw (* slot ); <nl> struct drm_i915_gem_object * obj = vma -> obj ; <nl> static void lut_close ( struct i915_gem_context * ctx ) <nl>  <nl> __i915_gem_object_release_unless_active ( obj ); <nl> } <nl> + rcu_read_unlock (); <nl> } <nl>  <nl> static void i915_gem_context_free ( struct i915_gem_context * ctx )
static int sony_probe ( struct hid_device * hdev , const struct hid_device_id * id ) <nl> } <nl>  <nl> ret = sony_set_operational ( hdev ); <nl> - if ( ret ) <nl> + if ( ret < 0 ) <nl> goto err_stop ; <nl>  <nl> return 0 ;
int st_sensors_check_device_support ( struct iio_dev * indio_dev , <nl> break ; <nl> } <nl> if ( n == ARRAY_SIZE ( sensor_settings [ i ]. sensors_supported )) { <nl> - dev_err (& indio_dev -> dev , " device name and WhoAmI mismatch .\ n "); <nl> + dev_err (& indio_dev -> dev , " device name \"% s \" and WhoAmI ( 0x % 02x ) mismatch ", <nl> + indio_dev -> name , wai ); <nl> goto sensor_name_mismatch ; <nl> } <nl> 
static int fastrpc_dma_buf_attach ( struct dma_buf * dmabuf , <nl> FASTRPC_PHYS ( buffer -> phys ), buffer -> size ); <nl> if ( ret < 0 ) { <nl> dev_err ( buffer -> dev , " failed to get scatterlist from DMA API \ n "); <nl> + kfree ( a ); <nl> return - EINVAL ; <nl> } <nl> 
char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> compose_mount_options_err : <nl> kfree ( mountdata ); <nl> mountdata = ERR_PTR ( rc ); <nl> + kfree (* devname ); <nl> + * devname = NULL ; <nl> goto compose_mount_options_out ; <nl> } <nl> 
static int iscsi_handle_reject ( struct iscsi_conn * conn , struct iscsi_hdr * hdr , <nl> if ( opcode != ISCSI_OP_NOOP_OUT ) <nl> return 0 ; <nl>  <nl> - if ( rejected_pdu . itt == cpu_to_be32 ( ISCSI_RESERVED_TAG )) { <nl> + if ( rejected_pdu . itt == cpu_to_be32 ( ISCSI_RESERVED_TAG )) { <nl> /* <nl> * nop - out in response to target ' s nop - out rejected . <nl> * Just resend .
int tcp_sendmsg ( struct sock * sk , struct msghdr * msg , size_t size ) <nl>  <nl> if (! skb_can_coalesce ( skb , i , pfrag -> page , <nl> pfrag -> offset )) { <nl> - if ( i == sysctl_max_skb_frags || ! sg ) { <nl> + if ( i >= sysctl_max_skb_frags || ! sg ) { <nl> tcp_mark_push ( tp , skb ); <nl> goto new_segment ; <nl> }
migration_call ( struct notifier_block * nfb , unsigned long action , void * hcpu ) <nl> migrate_tasks ( rq ); <nl> BUG_ON ( rq -> nr_running != 1 ); /* the migration thread */ <nl> raw_spin_unlock_irqrestore (& rq -> lock , flags ); <nl> - break ; <nl> - <nl> - case CPU_DEAD : <nl> calc_load_migrate ( rq ); <nl> break ; <nl> # endif
static const char * intel_pt_err_msgs [] = { <nl>  <nl> int intel_pt__strerror ( int code , char * buf , size_t buflen ) <nl> { <nl> - if ( code < 1 || code > INTEL_PT_ERR_MAX ) <nl> + if ( code < 1 || code >= INTEL_PT_ERR_MAX ) <nl> code = INTEL_PT_ERR_UNK ; <nl> strlcpy ( buf , intel_pt_err_msgs [ code ], buflen ); <nl> return 0 ;
static int vivid_fb_ioctl ( struct fb_info * info , unsigned cmd , unsigned long arg ) <nl> case FBIOGET_VBLANK : { <nl> struct fb_vblank vblank ; <nl>  <nl> + memset (& vblank , 0 , sizeof ( vblank )); <nl> vblank . flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT | <nl> FB_VBLANK_HAVE_VSYNC ; <nl> vblank . count = 0 ;
static void parse_bsd ( struct parsed_partitions * state , <nl> continue ; <nl> bsd_start = le32_to_cpu ( p -> p_offset ); <nl> bsd_size = le32_to_cpu ( p -> p_size ); <nl> + if ( memcmp ( flavour , " bsd \ 0 ", 4 ) == 0 ) <nl> + bsd_start += offset ; <nl> if ( offset == bsd_start && size == bsd_size ) <nl> /* full parent partition , we have it already */ <nl> continue ;
static void uwb_drp_handle_alien_drp ( struct uwb_rc * rc , struct uwb_ie_drp * drp_i <nl>  <nl> /* alloc and initialize new uwb_cnflt_alien */ <nl> cnflt = kzalloc ( sizeof ( struct uwb_cnflt_alien ), GFP_KERNEL ); <nl> - if (! cnflt ) <nl> + if (! cnflt ) { <nl> dev_err ( dev , " failed to alloc uwb_cnflt_alien struct \ n "); <nl> + return ; <nl> + } <nl> + <nl> INIT_LIST_HEAD (& cnflt -> rc_node ); <nl> init_timer (& cnflt -> timer ); <nl> cnflt -> timer . function = uwb_cnflt_timer ;
static int __devinit gen_74x164_probe ( struct spi_device * spi ) <nl> } <nl>  <nl> chip -> gpio_chip . ngpio = GEN_74X164_NUMBER_GPIOS * chip -> registers ; <nl> - chip -> buffer = devm_kzalloc (& spi -> dev , chip -> gpio_chip . ngpio , GFP_KERNEL ); <nl> + chip -> buffer = devm_kzalloc (& spi -> dev , chip -> registers , GFP_KERNEL ); <nl> if (! chip -> buffer ) { <nl> ret = - ENOMEM ; <nl> goto exit_destroy ;
static struct zx_dma_desc_sw * zx_alloc_desc_resource ( int num , <nl> kfree ( ds ); <nl> return NULL ; <nl> } <nl> - memset ( ds -> desc_hw , sizeof ( struct zx_desc_hw ) * num , 0 ); <nl> + memset ( ds -> desc_hw , 0 , sizeof ( struct zx_desc_hw ) * num ); <nl> ds -> desc_num = num ; <nl> return ds ; <nl> }
static void radeon_compute_pll_legacy ( struct radeon_pll * pll , <nl> max_fractional_feed_div = pll -> max_frac_feedback_div ; <nl> } <nl>  <nl> - for ( post_div = min_post_div ; post_div <= max_post_div ; ++ post_div ) { <nl> + for ( post_div = max_post_div ; post_div >= min_post_div ; -- post_div ) { <nl> uint32_t ref_div ; <nl>  <nl> if (( pll -> flags & RADEON_PLL_NO_ODD_POST_DIV ) && ( post_div & 1 ))
static bool assoc_array_insert_into_terminal_node ( struct assoc_array_edit * edit , <nl> free_slot = i ; <nl> continue ; <nl> } <nl> - if ( ops -> compare_object ( assoc_array_ptr_to_leaf ( ptr ), index_key )) { <nl> + if ( assoc_array_ptr_is_leaf ( ptr ) && <nl> + ops -> compare_object ( assoc_array_ptr_to_leaf ( ptr ), <nl> + index_key )) { <nl> pr_devel (" replace in slot % d \ n ", i ); <nl> edit -> leaf_p = & node -> slots [ i ]; <nl> edit -> dead_leaf = node -> slots [ i ];
int tcp_recvmsg ( struct kiocb * iocb , struct sock * sk , struct msghdr * msg , <nl>  <nl> cleanup_rbuf ( sk , copied ); <nl>  <nl> - if ( tp -> ucopy . task == user_recv ) { <nl> + if (! sysctl_tcp_low_latency && tp -> ucopy . task == user_recv ) { <nl> /* Install new reader */ <nl> if (! user_recv && !( flags & ( MSG_TRUNC | MSG_PEEK ))) { <nl> user_recv = current ;
static inline long snd_ctl_ioctl_compat ( struct file * file , unsigned int cmd , uns <nl> case SNDRV_CTL_IOCTL_POWER_STATE : <nl> case SNDRV_CTL_IOCTL_ELEM_LOCK : <nl> case SNDRV_CTL_IOCTL_ELEM_UNLOCK : <nl> + case SNDRV_CTL_IOCTL_ELEM_REMOVE : <nl> + case SNDRV_CTL_IOCTL_TLV_READ : <nl> + case SNDRV_CTL_IOCTL_TLV_WRITE : <nl> + case SNDRV_CTL_IOCTL_TLV_COMMAND : <nl> return snd_ctl_ioctl ( file , cmd , ( unsigned long ) argp ); <nl> case SNDRV_CTL_IOCTL_ELEM_LIST32 : <nl> return snd_ctl_elem_list_compat ( ctl -> card , argp );
store_vrm_reg ( struct device * dev , struct device_attribute * attr , const char * buf <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> data -> vrm = val ; <nl>  <nl> return count ;
static void isp_video_buffer_query ( struct isp_video_buffer * buf , <nl> switch ( buf -> state ) { <nl> case ISP_BUF_STATE_ERROR : <nl> vbuf -> flags |= V4L2_BUF_FLAG_ERROR ; <nl> + /* Fallthrough */ <nl> case ISP_BUF_STATE_DONE : <nl> vbuf -> flags |= V4L2_BUF_FLAG_DONE ; <nl> + break ; <nl> case ISP_BUF_STATE_QUEUED : <nl> case ISP_BUF_STATE_ACTIVE : <nl> vbuf -> flags |= V4L2_BUF_FLAG_QUEUED ;
static int __init parse_options ( struct early_uart_device * device , char * options ) <nl>  <nl> if (( options = strchr ( options , ','))) { <nl> options ++; <nl> - device -> baud = simple_strtoul ( options , 0 , 0 ); <nl> + device -> baud = simple_strtoul ( options , NULL , 0 ); <nl> length = min ( strcspn ( options , " "), sizeof ( device -> options )); <nl> strncpy ( device -> options , options , length ); <nl> } else {
# include < linux / vmalloc . h > <nl>  <nl> # include < media / videobuf2 - core . h > <nl> +# include < media / videobuf2 - vmalloc . h > <nl> # include < media / videobuf2 - memops . h > <nl>  <nl> struct vb2_vmalloc_buf {
void ath6kl_cfg80211_disconnect_event ( struct ath6kl * ar , u8 reason , <nl> NULL , 0 , <nl> WLAN_STATUS_UNSPECIFIED_FAILURE , <nl> GFP_KERNEL ); <nl> - } else { <nl> + } else if ( ar -> sme_state == SME_CONNECTED ) { <nl> cfg80211_disconnected ( ar -> net_dev , reason , <nl> NULL , 0 , GFP_KERNEL ); <nl> }
static int ll_ioctl_fiemap ( struct inode * inode , unsigned long arg ) <nl> if ( get_user ( extent_count , <nl> &(( struct ll_user_fiemap __user *) arg )-> fm_extent_count )) <nl> return - EFAULT ; <nl> + <nl> + if ( extent_count >= <nl> + ( SIZE_MAX - sizeof (* fiemap_s )) / sizeof ( struct ll_fiemap_extent )) <nl> + return - EINVAL ; <nl> num_bytes = sizeof (* fiemap_s ) + ( extent_count * <nl> sizeof ( struct ll_fiemap_extent )); <nl> 
void tsb_grow ( struct mm_struct * mm , unsigned long tsb_index , unsigned long rss ) <nl> if ( new_size > ( PAGE_SIZE * 2 )) <nl> gfp_flags = __GFP_NOWARN | __GFP_NORETRY ; <nl>  <nl> - new_tsb = kmem_cache_alloc ( tsb_caches [ new_cache_index ], gfp_flags ); <nl> + new_tsb = kmem_cache_alloc_node ( tsb_caches [ new_cache_index ], <nl> + gfp_flags , numa_node_id ()); <nl> if ( unlikely (! new_tsb )) { <nl> /* Not being able to fork due to a high - order TSB <nl> * allocation failure is very bad behavior . Just back
acpi_status asmlinkage acpi_enter_sleep_state ( u8 sleep_state ) <nl> /* <nl> * 2 ) Enable all wakeup GPEs <nl> */ <nl> + status = acpi_hw_disable_all_gpes (); <nl> + if ( ACPI_FAILURE ( status )) { <nl> + return_ACPI_STATUS ( status ); <nl> + } <nl> + <nl> acpi_gbl_system_awake_and_running = FALSE ; <nl>  <nl> status = acpi_hw_enable_all_wakeup_gpes ();
static ssize_t port_fops_write ( struct file * filp , const char __user * ubuf , <nl> if ( ret < 0 ) <nl> return ret ; <nl> } <nl> + /* Port got hot - unplugged . */ <nl> + if (! port -> guest_connected ) <nl> + return - ENODEV ; <nl>  <nl> count = min (( size_t )( 32 * 1024 ), count ); <nl> 
static s32 ixgbe_setup_kr_x550em ( struct ixgbe_hw * hw ) <nl> if ( hw -> phy . autoneg_advertised & IXGBE_LINK_SPEED_2_5GB_FULL ) <nl> return 0 ; <nl>  <nl> + if ( ixgbe_check_reset_blocked ( hw )) <nl> + return 0 ; <nl> + <nl> return ixgbe_setup_kr_speed_x550em ( hw , hw -> phy . autoneg_advertised ); <nl> } <nl> 
static int gic_set_affinity ( struct irq_data * d , const struct cpumask * mask_val , <nl> int enabled ; <nl> u64 val ; <nl>  <nl> + if ( cpu >= nr_cpu_ids ) <nl> + return - EINVAL ; <nl> + <nl> if ( gic_irq_in_rdist ( d )) <nl> return - EINVAL ; <nl> 
int rtc_irq_set_freq ( struct rtc_device * rtc , struct rtc_task * task , int freq ) <nl> int err = 0 ; <nl> unsigned long flags ; <nl>  <nl> - if ( freq <= 0 ) <nl> + if ( freq <= 0 || freq > 5000 ) <nl> return - EINVAL ; <nl>  <nl> spin_lock_irqsave (& rtc -> irq_task_lock , flags );
static int fc_lport_els_request ( struct fc_bsg_job * job , <nl> char * pp ; <nl> int len ; <nl>  <nl> - fp = fc_frame_alloc ( lport , sizeof ( struct fc_frame_header ) + <nl> - job -> request_payload . payload_len ); <nl> + fp = fc_frame_alloc ( lport , job -> request_payload . payload_len ); <nl> if (! fp ) <nl> return - ENOMEM ; <nl> 
MODULE_PARM_DESC ( cidmode , " Call - ID mode "); <nl> # define GIGASET_MODULENAME " usb_gigaset " <nl> # define GIGASET_DEVNAME " ttyGU " <nl>  <nl> -# define IF_WRITEBUF 2000 /* arbitrary limit */ <nl> +/* length limit according to Siemens 3070usb - protokoll . doc ch . 2 . 1 */ <nl> +# define IF_WRITEBUF 264 <nl>  <nl> /* Values for the Gigaset M105 Data */ <nl> # define USB_M105_VENDOR_ID 0x0681
static struct platform_driver rk_iommu_driver = { <nl>  <nl> static int __init rk_iommu_init ( void ) <nl> { <nl> + struct device_node * np ; <nl> int ret ; <nl>  <nl> + np = of_find_matching_node ( NULL , rk_iommu_dt_ids ); <nl> + if (! np ) <nl> + return 0 ; <nl> + <nl> + of_node_put ( np ); <nl> + <nl> ret = bus_set_iommu (& platform_bus_type , & rk_iommu_ops ); <nl> if ( ret ) <nl> return ret ;
static noinline int btrfs_search_path_in_tree ( struct btrfs_fs_info * info , <nl> key . objectid = key . offset ; <nl> key . offset = ( u64 )- 1 ; <nl> dirid = key . objectid ; <nl> - <nl> } <nl> if ( ptr < name ) <nl> goto out ; <nl> - memcpy ( name , ptr , total_len ); <nl> + memmove ( name , ptr , total_len ); <nl> name [ total_len ]='\ 0 '; <nl> ret = 0 ; <nl> out :
SMB2_tcon ( const unsigned int xid , struct cifs_ses * ses , const char * tree , <nl> tcon_error_exit : <nl> if ( rsp -> hdr . Status == STATUS_BAD_NETWORK_NAME ) { <nl> cifs_dbg ( VFS , " BAD_NETWORK_NAME : % s \ n ", tree ); <nl> - tcon -> bad_network_name = true ; <nl> + if ( tcon ) <nl> + tcon -> bad_network_name = true ; <nl> } <nl> goto tcon_exit ; <nl> }
static void __init imx6sl_init_late ( void ) <nl> if ( IS_ENABLED ( CONFIG_ARM_IMX6Q_CPUFREQ )) <nl> platform_device_register_simple (" imx6q - cpufreq ", - 1 , NULL , 0 ); <nl>  <nl> - if ( cpu_is_imx6sl ()) <nl> + if ( IS_ENABLED ( CONFIG_SOC_IMX6SL ) && cpu_is_imx6sl ()) <nl> imx6sl_cpuidle_init (); <nl> - else <nl> + else if ( IS_ENABLED ( CONFIG_SOC_IMX6SLL )) <nl> imx6sx_cpuidle_init (); <nl> } <nl> 
struct g2d_runqueue_node { <nl> struct list_head list ; <nl> struct list_head run_cmdlist ; <nl> struct list_head event_list ; <nl> + pid_t pid ; <nl> struct completion complete ; <nl> int async ; <nl> }; <nl> int exynos_g2d_exec_ioctl ( struct drm_device * drm_dev , void * data , <nl> } <nl>  <nl> mutex_lock (& g2d -> runqueue_mutex ); <nl> + runqueue_node -> pid = current -> pid ; <nl> list_add_tail (& runqueue_node -> list , & g2d -> runqueue ); <nl> if (! g2d -> runqueue_node ) <nl> g2d_exec_runqueue ( g2d );
static int configure_tda827x_fe ( struct saa7134_dev * dev , <nl> /* Get the first frontend */ <nl> fe0 = videobuf_dvb_get_frontend (& dev -> frontends , 1 ); <nl>  <nl> + if (! fe0 ) <nl> + return - EINVAL ; <nl> + <nl> fe0 -> dvb . frontend = dvb_attach ( tda10046_attach , cdec_conf , & dev -> i2c_adap ); <nl> if ( fe0 -> dvb . frontend ) { <nl> if ( cdec_conf -> i2c_gate )
void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> bool return_now = false ; <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> + if (! pnfs_layout_is_valid ( lo )) { <nl> + spin_unlock (& inode -> i_lock ); <nl> + return ; <nl> + } <nl> pnfs_set_plh_return_info ( lo , range . iomode , 0 ); <nl> /* Block LAYOUTGET */ <nl> set_bit ( NFS_LAYOUT_RETURN , & lo -> plh_flags );
static struct ip_conntrack_expect * find_expect ( struct ip_conntrack * ct , <nl> tuple . dst . protonum = IPPROTO_TCP ; <nl>  <nl> exp = __ip_conntrack_expect_find (& tuple ); <nl> - if ( exp -> master == ct ) <nl> + if ( exp && exp -> master == ct ) <nl> return exp ; <nl> return NULL ; <nl> }
static int netvsc_init_buf ( struct hv_device * device ) <nl> net_device -> map_words = DIV_ROUND_UP ( net_device -> send_section_cnt , <nl> BITS_PER_LONG ); <nl>  <nl> - net_device -> send_section_map = <nl> - kzalloc ( net_device -> map_words * sizeof ( ulong ), GFP_KERNEL ); <nl> + net_device -> send_section_map = kcalloc ( net_device -> map_words , <nl> + sizeof ( ulong ), GFP_KERNEL ); <nl> if ( net_device -> send_section_map == NULL ) { <nl> ret = - ENOMEM ; <nl> goto cleanup ;
static int lz4_uncompress ( const char * source , char * dest , int osize ) <nl> len = * ip ++; <nl> for (; len == 255 ; length += 255 ) <nl> len = * ip ++; <nl> + if ( unlikely ( length > ( size_t )( length + len ))) <nl> + goto _output_error ; <nl> length += len ; <nl> } <nl> 
ixgb_restore_vlan ( struct ixgb_adapter * adapter ) <nl>  <nl> static void ixgb_netpoll ( struct net_device * dev ) <nl> { <nl> - struct ixgb_adapter * adapter = dev -> priv ; <nl> + struct ixgb_adapter * adapter = netdev_priv ( dev ); <nl>  <nl> disable_irq ( adapter -> pdev -> irq ); <nl> ixgb_intr ( adapter -> pdev -> irq , dev , NULL );
static ssize_t ci_port_test_write ( struct file * file , const char __user * ubuf , <nl> if ( sscanf ( buf , "% u ", & mode ) != 1 ) <nl> return - EINVAL ; <nl>  <nl> + if ( mode > 255 ) <nl> + return - EBADRQC ; <nl> + <nl> pm_runtime_get_sync ( ci -> dev ); <nl> spin_lock_irqsave (& ci -> lock , flags ); <nl> ret = hw_port_test_set ( ci , mode );
static int au_ide_probe ( struct device * dev ) <nl> goto out ; <nl> } <nl>  <nl> - /* FIXME : This might possibly break PCMCIA IDE devices */ <nl> - <nl> - hwif = & ide_hwifs [ pdev -> id ]; <nl> + hwif = ide_find_port (); <nl> + if ( hwif == NULL ) { <nl> + ret = - ENOENT ; <nl> + goto out ; <nl> + } <nl>  <nl> memset (& hw , 0 , sizeof ( hw )); <nl> auide_setup_ports (& hw , ahwif );
enum nvkm_devidx { <nl> NVKM_SUBDEV_MC , <nl> NVKM_SUBDEV_BUS , <nl> NVKM_SUBDEV_TIMER , <nl> + NVKM_SUBDEV_INSTMEM , <nl> NVKM_SUBDEV_FB , <nl> NVKM_SUBDEV_LTC , <nl> - NVKM_SUBDEV_INSTMEM , <nl> NVKM_SUBDEV_MMU , <nl> NVKM_SUBDEV_BAR , <nl> NVKM_SUBDEV_PMU ,
# endif <nl> # include " igb . h " <nl>  <nl> -# define DRV_VERSION " 2 . 4 . 13 - k2 " <nl> +# define MAJ 3 <nl> +# define MIN 0 <nl> +# define BUILD 6 <nl> +# define KFIX 2 <nl> +# define DRV_VERSION __stringify ( MAJ ) "." __stringify ( MIN ) "." \ <nl> + __stringify ( BUILD ) "- k " __stringify ( KFIX ) <nl> char igb_driver_name [] = " igb "; <nl> char igb_driver_version [] = DRV_VERSION ; <nl> static const char igb_driver_string [] =
static int __ocfs2_move_extent ( handle_t * handle , <nl> } <nl>  <nl> out : <nl> + ocfs2_free_path ( path ); <nl> return ret ; <nl> } <nl> 
static int mga_vram_init ( struct mga_device * mdev ) <nl> { <nl> void __iomem * mem ; <nl> struct apertures_struct * aper = alloc_apertures ( 1 ); <nl> + if (! aper ) <nl> + return - ENOMEM ; <nl>  <nl> /* BAR 0 is VRAM */ <nl> mdev -> mc . vram_base = pci_resource_start ( mdev -> dev -> pdev , 0 );
static struct phy * exynos_mipi_video_phy_xlate ( struct device * dev , <nl> { <nl> struct exynos_mipi_video_phy * state = dev_get_drvdata ( dev ); <nl>  <nl> - if ( WARN_ON ( args -> args [ 0 ] > EXYNOS_MIPI_PHYS_NUM )) <nl> + if ( WARN_ON ( args -> args [ 0 ] >= EXYNOS_MIPI_PHYS_NUM )) <nl> return ERR_PTR (- ENODEV ); <nl>  <nl> return state -> phys [ args -> args [ 0 ]]. phy ;
static void __init atmci_get_cap ( struct atmel_mci * host ) <nl> /* keep only major version number */ <nl> switch ( version & 0xf00 ) { <nl> case 0x100 : <nl> + host -> caps . has_pdc = 1 ; <nl> + break ; <nl> case 0x200 : <nl> host -> caps . has_pdc = 1 ; <nl> host -> caps . has_rwproof = 1 ;
static s32 i2c_smbus_xfer_emulated ( struct i2c_adapter * adapter , u16 addr , <nl> status = i2c_transfer ( adapter , msg , num ); <nl> if ( status < 0 ) <nl> return status ; <nl> + if ( status != num ) <nl> + return - EIO ; <nl>  <nl> /* Check PEC if last message is a read */ <nl> if ( i && ( msg [ num - 1 ]. flags & I2C_M_RD )) {
int rtw_disassoc_cmd23a ( struct rtw_adapter * padapter , u32 deauth_timeout_ms , <nl> } else { <nl> /* no need to enqueue , do the cmd hdl directly and <nl> free cmd parameter */ <nl> - if ( H2C_SUCCESS != disconnect_hdl23a ( padapter , ( u8 *) param )) <nl> + if ( disconnect_hdl23a ( padapter , ( u8 *) param ) != H2C_SUCCESS ) <nl> res = _FAIL ; <nl> kfree ( param ); <nl> }
nv134_chipset = { <nl> . fb = gp104_fb_new , <nl> . fuse = gm107_fuse_new , <nl> . gpio = gk104_gpio_new , <nl> + . i2c = gm200_i2c_new , <nl> . imem = nv50_instmem_new , <nl> . mc = gp100_mc_new , <nl> . mmu = gf100_mmu_new ,
int ath6kl_debug_init_fs ( struct ath6kl * ar ) <nl> void ath6kl_debug_cleanup ( struct ath6kl * ar ) <nl> { <nl> skb_queue_purge (& ar -> debug . fwlog_queue ); <nl> + complete (& ar -> debug . fwlog_completion ); <nl> kfree ( ar -> debug . roam_tbl ); <nl> } <nl> 
int sock_diag_register ( struct sock_diag_handler * hndl ) <nl> { <nl> int err = 0 ; <nl>  <nl> - if ( hndl -> family > AF_MAX ) <nl> + if ( hndl -> family >= AF_MAX ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& sock_diag_table_mutex ); <nl> void sock_diag_unregister ( struct sock_diag_handler * hnld ) <nl> { <nl> int family = hnld -> family ; <nl>  <nl> - if ( family > AF_MAX ) <nl> + if ( family >= AF_MAX ) <nl> return ; <nl>  <nl> mutex_lock (& sock_diag_table_mutex );
store_priv_session_ ## field ( struct device * dev , \ <nl> # define iscsi_priv_session_rw_attr ( field , format ) \ <nl> iscsi_priv_session_attr_show ( field , format ) \ <nl> iscsi_priv_session_attr_store ( field ) \ <nl> - static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUGO , \ <nl> + static ISCSI_CLASS_ATTR ( priv_sess , field , S_IRUGO | S_IWUSR , \ <nl> show_priv_session_ ## field , \ <nl> store_priv_session_ ## field ) <nl> iscsi_priv_session_rw_attr ( recovery_tmo , "% d ");
static int cdrom_ioctl_media_changed ( struct cdrom_device_info * cdi , <nl> if (! CDROM_CAN ( CDC_SELECT_DISC ) || arg == CDSL_CURRENT ) <nl> return media_changed ( cdi , 1 ); <nl>  <nl> - if (( unsigned int ) arg >= cdi -> capacity ) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl>  <nl> info = kmalloc ( sizeof (* info ), GFP_KERNEL );
static ssize_t iwl_dbgfs_sram_read ( struct file * file , <nl> const struct fw_img * img ; <nl> size_t bufsz ; <nl>  <nl> + if (! iwl_is_ready_rf ( priv )) <nl> + return - EAGAIN ; <nl> + <nl> /* default is to dump the entire data segment */ <nl> if (! priv -> dbgfs_sram_offset && ! priv -> dbgfs_sram_len ) { <nl> priv -> dbgfs_sram_offset = 0x800000 ;
static int mga_vram_init ( struct mga_device * mdev ) <nl> aper -> count = 1 ; <nl>  <nl> remove_conflicting_framebuffers ( aper , " mgafb ", true ); <nl> + kfree ( aper ); <nl>  <nl> if (! request_mem_region ( mdev -> mc . vram_base , mdev -> mc . vram_window , <nl> " mgadrmfb_vram ")) {
int expand_downwards ( struct vm_area_struct * vma , <nl> { <nl> struct mm_struct * mm = vma -> vm_mm ; <nl> struct vm_area_struct * prev ; <nl> - int error ; <nl> + int error = 0 ; <nl>  <nl> address &= PAGE_MASK ; <nl> - error = security_mmap_addr ( address ); <nl> - if ( error ) <nl> - return error ; <nl> + if ( address < mmap_min_addr ) <nl> + return - EPERM ; <nl>  <nl> /* Enforce stack_guard_gap */ <nl> prev = vma -> vm_prev ;
struct inet_peer * inet_getpeer ( const struct inetpeer_addr * daddr , int create ) <nl> p -> rate_last = 0 ; <nl> p -> pmtu_expires = 0 ; <nl> p -> pmtu_orig = 0 ; <nl> + p -> redirect_genid = 0 ; <nl> memset (& p -> redirect_learned , 0 , sizeof ( p -> redirect_learned )); <nl>  <nl> 
static struct snd_pci_quirk alc883_cfg_tbl [] = { <nl> SND_PCI_QUIRK ( 0x17c0 , 0x4071 , " MEDION MD2 ", ALC883_MEDION_MD2 ), <nl> SND_PCI_QUIRK ( 0x1991 , 0x5625 , " Haier W66 ", ALC883_HAIER_W66 ), <nl> SND_PCI_QUIRK ( 0x17aa , 0x3bfc , " Lenovo NB0763 ", ALC883_LENOVO_NB0763 ), <nl> + SND_PCI_QUIRK ( 0x1043 , 0x8249 , " Asus M2A - VM HDMI ", ALC883_3ST_6ch_DIG ), <nl> + SND_PCI_QUIRK ( 0x147b , 0x1083 , " Abit IP35 - PRO ", ALC883_6ST_DIG ), <nl> {} <nl> }; <nl> 
int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
cifs_mount ( struct super_block * sb , struct cifs_sb_info * cifs_sb , <nl>  <nl> remote_path_check : <nl> /* check if a whole path ( including prepath ) is not remote */ <nl> - if (! rc && cifs_sb -> prepathlen && tcon ) { <nl> + if (! rc && tcon ) { <nl> /* build_path_to_root works only when we have a valid tcon */ <nl> full_path = cifs_build_path_to_root ( cifs_sb , tcon ); <nl> if ( full_path == NULL ) {
static struct snd_seq_queue * queue_new ( int owner , int locked ) <nl> static void queue_delete ( struct snd_seq_queue * q ) <nl> { <nl> /* stop and release the timer */ <nl> + mutex_lock (& q -> timer_mutex ); <nl> snd_seq_timer_stop ( q -> timer ); <nl> snd_seq_timer_close ( q ); <nl> + mutex_unlock (& q -> timer_mutex ); <nl> /* wait until access free */ <nl> snd_use_lock_sync (& q -> use_lock ); <nl> /* release resources ... */
int drm_open ( struct inode * inode , struct file * filp ) <nl> retcode = drm_open_helper ( inode , filp , dev ); <nl> if (! retcode ) { <nl> atomic_inc (& dev -> counts [ _DRM_STAT_OPENS ]); <nl> - if (! dev -> open_count ++) <nl> + if (! dev -> open_count ++) { <nl> retcode = drm_setup ( dev ); <nl> + if ( retcode ) <nl> + dev -> open_count --; <nl> + } <nl> } <nl> if (! retcode ) { <nl> mutex_lock (& dev -> struct_mutex );
static int route4_change ( struct net * net , struct sk_buff * in_skb , <nl> fp = & b -> ht [ h ]; <nl> for ( pfp = rtnl_dereference (* fp ); pfp ; <nl> fp = & pfp -> next , pfp = rtnl_dereference (* fp )) { <nl> - if ( pfp == f ) { <nl> - * fp = f -> next ; <nl> + if ( pfp == fold ) { <nl> + rcu_assign_pointer (* fp , fold -> next ); <nl> break ; <nl> } <nl> }
asmlinkage void __sched schedule ( void ) <nl> } <nl> EXPORT_SYMBOL ( schedule ); <nl>  <nl> -# ifdef CONFIG_SMP <nl> +# ifdef CONFIG_MUTEX_SPIN_ON_OWNER <nl> /* <nl> * Look out ! " owner " is an entirely speculative pointer <nl> * access and not reliable .
static int atmci_regs_show ( struct seq_file * s , void * v ) <nl> atmci_show_status_reg ( s , " SR ", buf [ MCI_SR / 4 ]); <nl> atmci_show_status_reg ( s , " IMR ", buf [ MCI_IMR / 4 ]); <nl>  <nl> + kfree ( buf ); <nl> + <nl> return 0 ; <nl> } <nl> 
static int drbg_fini_sym_kernel ( struct drbg_state * drbg ) <nl> drbg -> ctr_handle = NULL ; <nl>  <nl> if ( drbg -> ctr_req ) <nl> - skcipher_request_free ( drbg -> ctr_req );; <nl> + skcipher_request_free ( drbg -> ctr_req ); <nl> drbg -> ctr_req = NULL ; <nl>  <nl> kfree ( drbg -> ctr_null_value_buf );
static inline struct dentry * ovl_lookup_real ( struct dentry * dir , <nl> { <nl> struct dentry * dentry ; <nl>  <nl> - inode_lock ( dir -> d_inode ); <nl> - dentry = lookup_one_len ( name -> name , dir , name -> len ); <nl> - inode_unlock ( dir -> d_inode ); <nl> + dentry = lookup_hash ( name , dir ); <nl>  <nl> if ( IS_ERR ( dentry )) { <nl> if ( PTR_ERR ( dentry ) == - ENOENT )
static int ax25_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> ax25_address src ; <nl> const unsigned char * mac = skb_mac_header ( skb ); <nl>  <nl> + memset ( sax , 0 , sizeof ( struct full_sockaddr_ax25 )); <nl> ax25_addr_parse ( mac + 1 , skb -> data - mac - 1 , & src , NULL , <nl> & digi , NULL , NULL ); <nl> sax -> sax25_family = AF_AX25 ;
static int do_prlimit ( struct task_struct * tsk , unsigned int resource , <nl>  <nl> if ( resource >= RLIM_NLIMITS ) <nl> return - EINVAL ; <nl> + resource = array_index_nospec ( resource , RLIM_NLIMITS ); <nl> + <nl> if ( new_rlim ) { <nl> if ( new_rlim -> rlim_cur > new_rlim -> rlim_max ) <nl> return - EINVAL ;
struct block_device * bdget ( dev_t dev ) <nl>  <nl> if ( inode -> i_state & I_NEW ) { <nl> bdev -> bd_contains = NULL ; <nl> + bdev -> bd_super = NULL ; <nl> bdev -> bd_inode = inode ; <nl> bdev -> bd_block_size = ( 1 << inode -> i_blkbits ); <nl> bdev -> bd_part_count = 0 ;
static const char * nand_usdhc_bus_sel [] = { " osc ", " pll_sys_pfd2_270m_clk ", <nl>  <nl> static const char * ahb_channel_sel [] = { " osc ", " pll_sys_pfd2_270m_clk ", <nl> " pll_dram_533m_clk ", " pll_sys_pfd0_392m_clk ", <nl> - " pll_enet_125m_clk ", " pll_usb_main_clk ", " pll_audio_post_div ", <nl> + " pll_enet_250m_clk ", " pll_usb_main_clk ", " pll_audio_post_div ", <nl> " pll_video_post_div ", }; <nl>  <nl> static const char * dram_phym_sel [] = { " pll_dram_main_clk ",
static bool do_propagate_liveness ( const struct bpf_verifier_state * state , <nl> if ( parent -> spilled_regs [ i ]. live & REG_LIVE_READ ) <nl> continue ; <nl> if ( state -> spilled_regs [ i ]. live == REG_LIVE_READ ) { <nl> - parent -> regs [ i ]. live |= REG_LIVE_READ ; <nl> + parent -> spilled_regs [ i ]. live |= REG_LIVE_READ ; <nl> touched = true ; <nl> } <nl> }
static int snd_timer_user_tselect ( struct file * file , <nl> if ( err < 0 ) <nl> goto __err ; <nl>  <nl> + tu -> qhead = tu -> qtail = tu -> qused = 0 ; <nl> kfree ( tu -> queue ); <nl> tu -> queue = NULL ; <nl> kfree ( tu -> tqueue );
static netdev_tx_t xgene_enet_start_xmit ( struct sk_buff * skb , <nl> return NETDEV_TX_OK ; <nl> } <nl>  <nl> - pdata -> ring_ops -> wr_cmd ( tx_ring , count ); <nl> skb_tx_timestamp ( skb ); <nl>  <nl> pdata -> stats . tx_packets ++; <nl> pdata -> stats . tx_bytes += skb -> len ; <nl>  <nl> + pdata -> ring_ops -> wr_cmd ( tx_ring , count ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
B44Compressor :: B44Compressor <nl> // <nl>  <nl> _tmpBuffer = new unsigned short <nl> - [ checkArraySize ( uiMult ( maxScanLineSize , numScanLines ), <nl> + [ checkArraySize ( uiMult ( maxScanLineSize / sizeof ( unsigned short ), numScanLines ), <nl> sizeof ( unsigned short ))]; <nl>  <nl> const ChannelList & channels = header (). channels ();
fribidi_cap_rtl_to_unicode ( <nl> } <nl> } <nl> else <nl> - us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + { <nl> + if (( int ) s [ i ] < 0 ) <nl> + us [ j ++] = '?'; <nl> + else <nl> + us [ j ++] = caprtl_to_unicode [( int ) s [ i ]]; <nl> + } <nl> } <nl>  <nl> return j ;
int secure_decrypt ( void * data , unsigned int data_length , int is_signed ) <nl> /* Check the CMAC */ <nl> fixed_length = at91_aes_roundup ( data_length ); <nl> cmac = ( const unsigned int *)(( char *) data + fixed_length ); <nl> - if ( memcmp ( cmac , computed_cmac , AT91_AES_BLOCK_SIZE_BYTE )) <nl> + if (! consttime_memequal ( cmac , computed_cmac , AT91_AES_BLOCK_SIZE_BYTE )) <nl> goto exit ; <nl> } <nl> 
static void singlevar ( LexState * ls , expdesc * var ) { <nl> expdesc key ; <nl> singlevaraux ( fs , ls -> envn , var , 1 ); /* get environment variable */ <nl> lua_assert ( var -> k != VVOID ); /* this one must exist */ <nl> + luaK_exp2anyregup ( fs , var ); /* but could be a constant */ <nl> codestring (& key , varname ); /* key is variable name */ <nl> luaK_indexed ( fs , var , & key ); /* env [ varname ] */ <nl> }
static int cmd_handle_untagged ( IMAP_DATA * idata ) <nl> dprint ( 2 , ( debugfile , " Handling untagged NO \ n ")); <nl>  <nl> /* Display the warning message from the server */ <nl> - mutt_error ("% s ", s + 3 ); <nl> + mutt_error ("% s ", s + 2 ); <nl> mutt_sleep ( 2 ); <nl> } <nl> 
vq_endchains ( struct virtio_vq_info * vq , int used_all_avail ) <nl> uint16_t event_idx , new_idx , old_idx ; <nl> int intr ; <nl>  <nl> + if (! vq || ! vq -> used ) <nl> + return ; <nl> + <nl> /* <nl> * Interrupt generation : if we ' re using EVENT_IDX , <nl> * interrupt if we ' ve crossed the event threshold .
static block_t * Encode ( encoder_t * p_enc , picture_t * p_pic ) <nl> * is appended to the sequence header to allow guard <nl> * against poor streaming servers */ <nl> /* XXX , should this be done using the packetizer ? */ <nl> + <nl> + if ( len > UINT32_MAX - sizeof ( eos ) ) <nl> + return NULL ; <nl> + <nl> p_enc -> fmt_out . p_extra = malloc ( len + sizeof ( eos ) ); <nl> if ( ! p_enc -> fmt_out . p_extra ) <nl> return NULL ;
class MapStageOp : public OpKernel { <nl> OP_REQUIRES_OK ( ctx , ctx -> input (" key ", & key_tensor )); <nl> OP_REQUIRES_OK ( ctx , ctx -> input (" indices ", & indices_tensor )); <nl> OP_REQUIRES_OK ( ctx , ctx -> input_list (" values ", & values_tensor )); <nl> + OP_REQUIRES ( ctx , key_tensor -> NumElements () > 0 , <nl> + errors :: InvalidArgument (" key must not be empty ")); <nl>  <nl> // Create copy for insertion into Staging Area <nl> Tensor key (* key_tensor );
int MatchingArraySize ( const ArrayType1 & array1 , int index1 , <nl> inline int MatchingDim ( const RuntimeShape & shape1 , int index1 , <nl> const RuntimeShape & shape2 , int index2 ) { <nl> TFLITE_DCHECK_EQ ( shape1 . Dims ( index1 ), shape2 . Dims ( index2 )); <nl> - return shape1 . Dims ( index1 ); <nl> + return std :: min ( shape1 . Dims ( index1 ), shape2 . Dims ( index2 )); <nl> } <nl>  <nl> template < typename ... Args >
static HB_Bool myanmar_shape_syllable ( HB_Bool openType , HB_ShaperItem * item , HB_ <nl> if ( kinzi >= 0 && i > base && ( cc & Mymr_CF_AFTER_KINZI )) { <nl> reordered [ len ] = Mymr_C_NGA ; <nl> reordered [ len + 1 ] = Mymr_C_VIRAMA ; <nl> - properties [ len - 1 ] = AboveForm ; <nl> + if ( len > 0 ) <nl> + properties [ len - 1 ] = AboveForm ; <nl> properties [ len ] = AboveForm ; <nl> len += 2 ; <nl> kinzi = - 1 ;